<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Thu, 07 Aug 2025 00:04:55 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[python-poetry/poetry]]></title>
            <link>https://github.com/python-poetry/poetry</link>
            <guid>https://github.com/python-poetry/poetry</guid>
            <pubDate>Thu, 07 Aug 2025 00:04:55 GMT</pubDate>
            <description><![CDATA[Python packaging and dependency management made easy]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/python-poetry/poetry">python-poetry/poetry</a></h1>
            <p>Python packaging and dependency management made easy</p>
            <p>Language: Python</p>
            <p>Stars: 33,604</p>
            <p>Forks: 2,364</p>
            <p>Stars today: 25 stars today</p>
            <h2>README</h2><pre># Poetry: Python packaging and dependency management made easy

[![Poetry](https://img.shields.io/endpoint?url=https://python-poetry.org/badge/v0.json)](https://python-poetry.org/)
[![Stable Version](https://img.shields.io/pypi/v/poetry?label=stable)][PyPI Releases]
[![Pre-release Version](https://img.shields.io/github/v/release/python-poetry/poetry?label=pre-release&amp;include_prereleases&amp;sort=semver)][PyPI Releases]
[![Python Versions](https://img.shields.io/pypi/pyversions/poetry)][PyPI]
[![Download Stats](https://img.shields.io/pypi/dm/poetry)](https://pypistats.org/packages/poetry)
[![Discord](https://img.shields.io/discord/487711540787675139?logo=discord)][Discord]

Poetry helps you declare, manage and install dependencies of Python projects,
ensuring you have the right stack everywhere.

![Poetry Install](https://raw.githubusercontent.com/python-poetry/poetry/main/assets/install.gif)

Poetry replaces `setup.py`, `requirements.txt`, `setup.cfg`, `MANIFEST.in` and `Pipfile` with a simple `pyproject.toml`
based project format.

```toml
[project]
name = &quot;my-package&quot;
version = &quot;0.1.0&quot;
description = &quot;The description of the package&quot;

license = { text = &quot;MIT&quot; }
readme = &quot;README.md&quot;

# No python upper bound for package metadata
requires-python = &quot;&gt;=3.9&quot;

authors = [
    { name = &quot;SeÃÅbastien Eustace&quot;, email = &quot;sebastien@eustace.io&quot; },
]

# Keywords (translated to tags on the package index)
keywords = [&quot;packaging&quot;, &quot;poetry&quot;]

dependencies = [
    # equivalent to ^3.8.1 with semver constraints
    &quot;aiohttp (&gt;=3.8.1,&lt;4.0.0)&quot;,
    # dependency with extras
    &quot;requests[security] (&gt;=2.28,&lt;3.0)&quot;,
    # version-specific dependency with prereleases allowed (see below)
    &quot;tomli (&gt;=2.0.1,&lt;3.0.0) ; python_version &lt; &#039;3.11&#039;&quot;,
    # git dependency with branch specified
    &quot;cleo @ git+https://github.com/python-poetry/cleo.git@main&quot;,
]

[project.urls]
repository = &quot;https://github.com/python-poetry/poetry&quot;
homepage = &quot;https://python-poetry.org&quot;

# Scripts are easily expressed
[project.scripts]
my_package_cli = &#039;my_package.console:run&#039;

[project.optional-dependencies]
# optional dependency to be installed via &#039;poetry install -E my-extra&#039;
my-extra = [&quot;pendulum (&gt;=3.1.0,&lt;4.0.0)&quot;]

[tool.poetry.dependencies]
# Python upper bound for locking
python = &quot;&gt;=3.9,&lt;4.0&quot;
# Version-specific dependencies with prereleases allowed
tomli = { allow-prereleases = true }

# Dependency groups are supported for organizing your dependencies
[tool.poetry.group.dev.dependencies]
pytest = &quot;^7.1.2&quot;
pytest-cov = &quot;^3.0&quot;

# ...and can be installed only when explicitly requested
# via &#039;poetry install --with docs&#039;
[tool.poetry.group.docs]
optional = true
[tool.poetry.group.docs.dependencies]
Sphinx = &quot;^5.1.1&quot;
```

## Installation

Poetry supports multiple installation methods, including a simple script found at [install.python-poetry.org]. For full
installation instructions, including advanced usage of the script, alternate install methods, and CI best practices, see
the full [installation documentation].

## Documentation

[Documentation] for the current version of Poetry (as well as the development branch and recently out of support
versions) is available from the [official website].

## Contribute

Poetry is a large, complex project always in need of contributors. For those new to the project, a list of
[suggested issues] to work on in Poetry and poetry-core is available. The full [contributing documentation] also
provides helpful guidance.

## Resources

* [Releases][PyPI Releases]
* [Official Website]
* [Documentation]
* [Issue Tracker]
* [Discord]

  [PyPI]: https://pypi.org/project/poetry/
  [PyPI Releases]: https://pypi.org/project/poetry/#history
  [Official Website]: https://python-poetry.org
  [Documentation]: https://python-poetry.org/docs/
  [Issue Tracker]: https://github.com/python-poetry/poetry/issues
  [Suggested Issues]: https://github.com/python-poetry/poetry/contribute
  [Contributing Documentation]: https://python-poetry.org/docs/contributing
  [Discord]: https://discord.com/invite/awxPgve
  [install.python-poetry.org]: https://install.python-poetry.org
  [Installation Documentation]: https://python-poetry.org/docs/#installation

## Related Projects

* [poetry-core](https://github.com/python-poetry/poetry-core): PEP 517 build-system for Poetry projects, and
dependency-free core functionality of the Poetry frontend
* [poetry-plugin-export](https://github.com/python-poetry/poetry-plugin-export): Export Poetry projects/lock files to
foreign formats like requirements.txt
* [poetry-plugin-bundle](https://github.com/python-poetry/poetry-plugin-bundle): Install Poetry projects/lock files to
external formats like virtual environments
* [install.python-poetry.org](https://github.com/python-poetry/install.python-poetry.org): The official Poetry
installation script
* [website](https://github.com/python-poetry/website): The official Poetry website and blog

## Supporters

Thanks to [JetBrains](https://www.jetbrains.com) for supporting us with licenses for their tools.

[&lt;img src=&quot;https://resources.jetbrains.com/storage/products/company/brand/logos/jetbrains.svg&quot; width=&quot;150&quot; alt=&quot;JetBrains logo.&quot; /&gt;](https://www.jetbrains.com)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[open-edge-platform/anomalib]]></title>
            <link>https://github.com/open-edge-platform/anomalib</link>
            <guid>https://github.com/open-edge-platform/anomalib</guid>
            <pubDate>Thu, 07 Aug 2025 00:04:54 GMT</pubDate>
            <description><![CDATA[An anomaly detection library comprising state-of-the-art algorithms and features such as experiment management, hyper-parameter optimization, and edge inference.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/open-edge-platform/anomalib">open-edge-platform/anomalib</a></h1>
            <p>An anomaly detection library comprising state-of-the-art algorithms and features such as experiment management, hyper-parameter optimization, and edge inference.</p>
            <p>Language: Python</p>
            <p>Stars: 4,675</p>
            <p>Forks: 783</p>
            <p>Stars today: 17 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;https://raw.githubusercontent.com/open-edge-platform/anomalib/main/docs/source/_static/images/logos/anomalib-wide-blue.png&quot; width=&quot;600px&quot; alt=&quot;Anomalib Logo - A deep learning library for anomaly detection&quot;&gt;

**A library for benchmarking, developing and deploying deep learning anomaly detection algorithms**

---

[Key Features](#key-features) ‚Ä¢
[Docs](https://anomalib.readthedocs.io/en/latest/) ‚Ä¢
[Notebooks](examples/notebooks) ‚Ä¢
[License](LICENSE)

[![python](https://img.shields.io/badge/python-3.10%2B-green)]()
[![pytorch](https://img.shields.io/badge/pytorch-2.0%2B-orange)]()
[![lightning](https://img.shields.io/badge/lightning-2.2%2B-blue)]()
[![openvino](https://img.shields.io/badge/openvino-2024.0%2B-purple)]()

[![Pre-Merge Checks](https://github.com/open-edge-platform/anomalib/actions/workflows/pre_merge.yml/badge.svg)](https://github.com/open-edge-platform/anomalib/actions/workflows/pre_merge.yml)
[![codecov](https://codecov.io/gh/open-edge-platform/anomalib/branch/main/graph/badge.svg?token=Z6A07N1BZK)](https://codecov.io/gh/open-edge-platform/anomalib)
[![Downloads](https://static.pepy.tech/personalized-badge/anomalib?period=total&amp;units=international_system&amp;left_color=grey&amp;right_color=green&amp;left_text=PyPI%20Downloads)](https://pepy.tech/project/anomalib)
[![snyk](https://snyk.io/advisor/python/anomalib/badge.svg)](https://snyk.io/advisor/python/anomalib)
[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/8330/badge)](https://www.bestpractices.dev/projects/8330)

[![ReadTheDocs](https://readthedocs.org/projects/anomalib/badge/?version=latest)](https://anomalib.readthedocs.io/en/latest/?badge=latest)
[![Anomalib - Gurubase docs](https://img.shields.io/badge/Gurubase-Ask%20Anomalib%20Guru-006BFF)](https://gurubase.io/g/anomalib)

&lt;a href=&quot;https://trendshift.io/repositories/6030&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/6030&quot; alt=&quot;open-edge-platform%2Fanomalib | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;

---

&gt; üåü **Announcing v2.1.0 Release!** üåü
&gt;
&gt; We&#039;re excited to announce the release of Anomalib v2.1.0!
&gt; This version brings several state-of-the-art models and anomaly detection datasets. Key features include:
&gt;
&gt; New models :
&gt;
&gt; - **üñºÔ∏è UniNet (CVPR 2025)**: A contrastive learning-guided unified framework with feature selection for anomaly detection.
&gt; - **üñºÔ∏è Dinomaly (CVPR 2025)**: A &#039;less is more philosophy&#039; encoder-decoder architecture model leveraging pre-trained foundational models.
&gt; - **üé• Fuvas (ICASSP 2025)**: Few-shot unsupervised video anomaly segmentation via low-rank factorization of spatio-temporal features.
&gt;
&gt; New datasets:
&gt;
&gt; - **MVTec AD 2** : A new version of the MVTec AD dataset with 8 categories of industrial anomaly detection.
&gt; - **MVTec LOCO AD** : MVTec logical constraints anomaly detection dataset that includes both structural and logical anomalies.
&gt; - **Real-IAD** : A real-world multi-view dataset for benchmarking versatile industrial anomaly detection.
&gt; - **VAD dataset** : Valeo Anomaly Dataset (VAD) showcasing a diverse range of defects, from highly obvious to extremely subtle.
&gt;
&gt; We value your input! Please share feedback via [GitHub Issues](https://github.com/open-edge-platform/anomalib/issues) or our [Discussions](https://github.com/open-edge-platform/anomalib/discussions)

# üëã Introduction

Anomalib is a deep learning library that aims to collect state-of-the-art anomaly detection algorithms for benchmarking on both public and private datasets. Anomalib provides several ready-to-use implementations of anomaly detection algorithms described in the recent literature, as well as a set of tools that facilitate the development and implementation of custom models. The library has a strong focus on visual anomaly detection, where the goal of the algorithm is to detect and/or localize anomalies within images or videos in a dataset. Anomalib is constantly updated with new algorithms and training/inference extensions, so keep checking!

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/open-edge-platform/anomalib/main/docs/source/_static/images/readme.png&quot; width=&quot;1000&quot; alt=&quot;A prediction made by anomalib&quot;&gt;
&lt;/p&gt;

## Key features

- Simple and modular API and CLI for training, inference, benchmarking, and hyperparameter optimization.
- The largest public collection of ready-to-use deep learning anomaly detection algorithms and benchmark datasets.
- [**Lightning**](https://www.lightning.ai/) based model implementations to reduce boilerplate code and limit the implementation efforts to the bare essentials.
- The majority of models can be exported to [**OpenVINO**](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html) Intermediate Representation (IR) for accelerated inference on Intel hardware.
- A set of [inference tools](tools) for quick and easy deployment of the standard or custom anomaly detection models.

# üì¶ Installation

Anomalib can be installed from PyPI. We recommend using a virtual environment and a modern package installer like `uv` or `pip`.

## üöÄ Quick Install

For a standard installation, you can use `uv` or `pip`. This will install the latest version of Anomalib with its core dependencies. PyTorch will be installed based on its default behavior, which usually works for CPU and standard CUDA setups.

```bash
# With uv
uv pip install anomalib

# Or with pip
pip install anomalib
```

For more control over the installation, such as specifying the PyTorch backend (e.g., XPU, CUDA and ROCm) or installing extra dependencies for specific models, see the advanced options below.

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üí° Advanced Installation: Specify Hardware Backend&lt;/strong&gt;&lt;/summary&gt;

To ensure compatibility with your hardware, you can specify a backend during installation. This is the recommended approach for production environments and for hardware other than CPU or standard CUDA.

**Using `uv`:**

```bash
# CPU support (default, works on all platforms)
uv pip install &quot;anomalib[cpu]&quot;

# CUDA 12.4 support (Linux/Windows with NVIDIA GPU)
uv pip install &quot;anomalib[cu124]&quot;

# CUDA 12.1 support (Linux/Windows with NVIDIA GPU)
uv pip install &quot;anomalib[cu121]&quot;

# CUDA 11.8 support (Linux/Windows with NVIDIA GPU)
uv pip install &quot;anomalib[cu118]&quot;

# ROCm support (Linux with AMD GPU)
uv pip install &quot;anomalib[rocm]&quot;

# Intel XPU support (Linux with Intel GPU)
uv pip install &quot;anomalib[xpu]&quot;
```

**Using `pip`:**
The same extras can be used with `pip`:

```bash
pip install &quot;anomalib[cu124]&quot;
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üß© Advanced Installation: Additional Dependencies&lt;/strong&gt;&lt;/summary&gt;

Anomalib includes most dependencies by default. For specialized features, you may need additional optional dependencies. Remember to include your hardware-specific extra.

```bash
# Example: Install with OpenVINO support and CUDA 12.4
uv pip install &quot;anomalib[openvino,cu124]&quot;

# Example: Install all optional dependencies for a CPU-only setup
uv pip install &quot;anomalib[full,cpu]&quot;
```

Here is a list of available optional dependency groups:

| Extra         | Description                              | Purpose                                     |
| :------------ | :--------------------------------------- | :------------------------------------------ |
| `[openvino]`  | Intel OpenVINO optimization              | For accelerated inference on Intel hardware |
| `[clip]`      | Vision-language models                   | `winclip`                                   |
| `[vlm]`       | Vision-language model backends           | Advanced VLM features                       |
| `[loggers]`   | Experiment tracking (wandb, comet, etc.) | For experiment management                   |
| `[notebooks]` | Jupyter notebook support                 | For running example notebooks               |
| `[full]`      | All optional dependencies                | All optional features                       |

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üîß Advanced Installation: Install from Source&lt;/strong&gt;&lt;/summary&gt;

For contributing to `anomalib` or using a development version, you can install from source.

**Using `uv`:**
This is the recommended method for developers as it uses the project&#039;s lock file for reproducible environments.

```bash
git clone https://github.com/open-edge-platform/anomalib.git
cd anomalib

# Create the virtual environment
uv venv

# Sync with the lockfile for a specific backend (e.g., CPU)
uv sync --extra cpu

# Or for a different backend like CUDA 12.4
uv sync --extra cu124

# To set up a full development environment
uv sync --extra dev --extra cpu
```

**Using `pip`:**

```bash
git clone https://github.com/open-edge-platform/anomalib.git
cd anomalib

# Install in editable mode with a specific backend
pip install -e &quot;.[cpu]&quot;

# Install with development dependencies
pip install -e &quot;.[dev,cpu]&quot;
```

&lt;/details&gt;

# üß† Training

Anomalib supports both API and CLI-based training approaches:

## üîå Python API

```python
from anomalib.data import MVTecAD
from anomalib.models import Patchcore
from anomalib.engine import Engine

# Initialize components
datamodule = MVTecAD()
model = Patchcore()
engine = Engine()

# Train the model
engine.fit(datamodule=datamodule, model=model)
```

## ‚å®Ô∏è Command Line

```bash
# Train with default settings
anomalib train --model Patchcore --data anomalib.data.MVTecAD

# Train with custom category
anomalib train --model Patchcore --data anomalib.data.MVTecAD --data.category transistor

# Train with config file
anomalib train --config path/to/config.yaml
```

# ü§ñ Inference

Anomalib provides multiple inference options including Torch, Lightning, Gradio, and OpenVINO. Here&#039;s how to get started:

## üîå Python API

```python
# Load model and make predictions
predictions = engine.predict(
    datamodule=datamodule,
    model=model,
    ckpt_path=&quot;path/to/checkpoint.ckpt&quot;,
)
```

## ‚å®Ô∏è Command Line

```bash
# Basic prediction
anomalib predict --model anomalib.models.Patchcore \
                 --data anomalib.data.MVTecAD \
                 --ckpt_path path/to/model.ckpt

# Prediction with results
anomalib predict --model anomalib.models.Patchcore \
                 --data anomalib.data.MVTecAD \
                 --ckpt_path path/to/model.ckpt \
                 --return_predictions
```

&gt; üìò **Note:** For advanced inference options including Gradio and OpenVINO, check our [Inference Documentation](https://anomalib.readthedocs.io).

# Training on Intel GPUs

&gt; [!Note]
&gt; Currently, only single GPU training is supported on Intel GPUs.
&gt; These commands were tested on Arc 750 and Arc 770.

Ensure that you have PyTorch with XPU support installed. For more information, please refer to the [PyTorch XPU documentation](https://pytorch.org/docs/stable/notes/get_start_xpu.html)

## üîå API

```python
from anomalib.data import MVTecAD
from anomalib.engine import Engine, SingleXPUStrategy, XPUAccelerator
from anomalib.models import Stfpm

engine = Engine(
    strategy=SingleXPUStrategy(),
    accelerator=XPUAccelerator(),
)
engine.train(Stfpm(), datamodule=MVTecAD())
```

## ‚å®Ô∏è CLI

```bash
anomalib train --model Padim --data MVTecAD --trainer.accelerator xpu --trainer.strategy xpu_single
```

# ‚öôÔ∏è Hyperparameter Optimization

Anomalib supports hyperparameter optimization (HPO) using [Weights &amp; Biases](https://wandb.ai/) and [Comet.ml](https://www.comet.com/).

```bash
# Run HPO with Weights &amp; Biases
anomalib hpo --backend WANDB --sweep_config tools/hpo/configs/wandb.yaml
```

&gt; üìò **Note:** For detailed HPO configuration, check our [HPO Documentation](https://open-edge-platform.github.io/anomalib/tutorials/hyperparameter_optimization.html).

# üß™ Experiment Management

Track your experiments with popular logging platforms through [PyTorch Lightning loggers](https://pytorch-lightning.readthedocs.io/en/stable/extensions/logging.html):

- üìä Weights &amp; Biases
- üìà Comet.ml
- üìâ TensorBoard

Enable logging in your config file to track:

- Hyperparameters
- Metrics
- Model graphs
- Test predictions

&gt; üìò **Note:** For logging setup, see our [Logging Documentation](https://open-edge-platform.github.io/anomalib/tutorials/logging.html).

# üìä Benchmarking

Evaluate and compare model performance across different datasets:

```bash
# Run benchmarking with default configuration
anomalib benchmark --config tools/experimental/benchmarking/sample.yaml
```

&gt; üí° **Tip:** Check individual model performance in their respective README files:
&gt;
&gt; - [Patchcore Results](src/anomalib/models/image/patchcore/README.md#mvtec-ad-dataset)
&gt; - [Other Models](src/anomalib/models/)

# ‚úçÔ∏è Reference

If you find Anomalib useful in your research or work, please cite:

```tex
@inproceedings{akcay2022anomalib,
  title={Anomalib: A deep learning library for anomaly detection},
  author={Akcay, Samet and Ameln, Dick and Vaidya, Ashwin and Lakshmanan, Barath and Ahuja, Nilesh and Genc, Utku},
  booktitle={2022 IEEE International Conference on Image Processing (ICIP)},
  pages={1706--1710},
  year={2022},
  organization={IEEE}
}
```

# üë• Contributing

We welcome contributions! Check out our [Contributing Guide](CONTRIBUTING.md) to get started.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/open-edge-platform/anomalib/graphs/contributors&quot;&gt;
    &lt;img src=&quot;https://contrib.rocks/image?repo=open-edge-platform/anomalib&quot; alt=&quot;Contributors to open-edge-platform/anomalib&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;b&gt;Thank you to all our contributors!&lt;/b&gt;
&lt;/p&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[confident-ai/deepeval]]></title>
            <link>https://github.com/confident-ai/deepeval</link>
            <guid>https://github.com/confident-ai/deepeval</guid>
            <pubDate>Thu, 07 Aug 2025 00:04:53 GMT</pubDate>
            <description><![CDATA[The LLM Evaluation Framework]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/confident-ai/deepeval">confident-ai/deepeval</a></h1>
            <p>The LLM Evaluation Framework</p>
            <p>Language: Python</p>
            <p>Stars: 9,912</p>
            <p>Forks: 860</p>
            <p>Stars today: 46 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/confident-ai/deepeval/blob/main/docs/static/img/deepeval.png&quot; alt=&quot;DeepEval Logo&quot; width=&quot;100%&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;h1 align=&quot;center&quot;&gt;The LLM Evaluation Framework&lt;/h1&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/5917&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/5917&quot; alt=&quot;confident-ai%2Fdeepeval | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://discord.gg/3SEyvpgu2f&quot;&gt;
        &lt;img alt=&quot;discord-invite&quot; src=&quot;https://dcbadge.vercel.app/api/server/3SEyvpgu2f?style=flat&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;h4 align=&quot;center&quot;&gt;
    &lt;p&gt;
        &lt;a href=&quot;https://deepeval.com/docs/getting-started?utm_source=GitHub&quot;&gt;Documentation&lt;/a&gt; |
        &lt;a href=&quot;#-metrics-and-features&quot;&gt;Metrics and Features&lt;/a&gt; |
        &lt;a href=&quot;#-quickstart&quot;&gt;Getting Started&lt;/a&gt; |
        &lt;a href=&quot;#-integrations&quot;&gt;Integrations&lt;/a&gt; |
        &lt;a href=&quot;https://confident-ai.com?utm_source=GitHub&quot;&gt;DeepEval Platform&lt;/a&gt;
    &lt;p&gt;
&lt;/h4&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://github.com/confident-ai/deepeval/releases&quot;&gt;
        &lt;img alt=&quot;GitHub release&quot; src=&quot;https://img.shields.io/github/release/confident-ai/deepeval.svg?color=violet&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://colab.research.google.com/drive/1PPxYEBa6eu__LquGoFFJZkhYgWVYE6kh?usp=sharing&quot;&gt;
        &lt;img alt=&quot;Try Quickstart in Colab&quot; src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/confident-ai/deepeval/blob/master/LICENSE.md&quot;&gt;
        &lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/github/license/confident-ai/deepeval.svg?color=yellow&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://x.com/deepeval&quot;&gt;
        &lt;img alt=&quot;Twitter Follow&quot; src=&quot;https://img.shields.io/twitter/follow/deepeval?style=social&amp;logo=x&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=de&quot;&gt;Deutsch&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=es&quot;&gt;Espa√±ol&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=fr&quot;&gt;fran√ßais&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=ja&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=ko&quot;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=pt&quot;&gt;Portugu√™s&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=ru&quot;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=zh&quot;&gt;‰∏≠Êñá&lt;/a&gt;
&lt;/p&gt;

**DeepEval** is a simple-to-use, open-source LLM evaluation framework, for evaluating and testing large-language model systems. It is similar to Pytest but specialized for unit testing LLM outputs. DeepEval incorporates the latest research to evaluate LLM outputs based on metrics such as G-Eval, hallucination, answer relevancy, RAGAS, etc., which uses LLMs and various other NLP models that runs **locally on your machine** for evaluation.

Whether your LLM applications are RAG pipelines, chatbots, AI agents, implemented via LangChain or LlamaIndex, DeepEval has you covered. With it, you can easily determine the optimal models, prompts, and architecture to improve your RAG pipeline, agentic workflows, prevent prompt drifting, or even transition from OpenAI to hosting your own Deepseek R1 with confidence.

&gt; [!IMPORTANT]
&gt; Need a place for your DeepEval testing data to live üè°‚ù§Ô∏è? [Sign up to the DeepEval platform](https://confident-ai.com?utm_source=GitHub) to compare iterations of your LLM app, generate &amp; share testing reports, and more.
&gt;
&gt; ![Demo GIF](assets/demo.gif)

&gt; Want to talk LLM evaluation, need help picking metrics, or just to say hi? [Come join our discord.](https://discord.com/invite/3SEyvpgu2f)

&lt;br /&gt;

# üî• Metrics and Features

&gt; ü•≥ You can now share DeepEval&#039;s test results on the cloud directly on [Confident AI](https://confident-ai.com?utm_source=GitHub)&#039;s infrastructure

- Supports both end-to-end and component-level LLM evaluation.
- Large variety of ready-to-use LLM evaluation metrics (all with explanations) powered by **ANY** LLM of your choice, statistical methods, or NLP models that runs **locally on your machine**:
  - G-Eval
  - DAG ([deep acyclic graph](https://deepeval.com/docs/metrics-dag))
  - **RAG metrics:**
    - Answer Relevancy
    - Faithfulness
    - Contextual Recall
    - Contextual Precision
    - Contextual Relevancy
    - RAGAS
  - **Agentic metrics:**
    - Task Completion
    - Tool Correctness
  - **Others:**
    - Hallucination
    - Summarization
    - Bias
    - Toxicity
  - **Conversational metrics:**
    - Knowledge Retention
    - Conversation Completeness
    - Conversation Relevancy
    - Role Adherence
  - etc.
- Build your own custom metrics that are automatically integrated with DeepEval&#039;s ecosystem.
- Generate synthetic datasets for evaluation.
- Integrates seamlessly with **ANY** CI/CD environment.
- [Red team your LLM application](https://deepeval.com/docs/red-teaming-introduction) for 40+ safety vulnerabilities in a few lines of code, including:
  - Toxicity
  - Bias
  - SQL Injection
  - etc., using advanced 10+ attack enhancement strategies such as prompt injections.
- Easily benchmark **ANY** LLM on popular LLM benchmarks in [under 10 lines of code.](https://deepeval.com/docs/benchmarks-introduction?utm_source=GitHub), which includes:
  - MMLU
  - HellaSwag
  - DROP
  - BIG-Bench Hard
  - TruthfulQA
  - HumanEval
  - GSM8K
- [100% integrated with Confident AI](https://confident-ai.com?utm_source=GitHub) for the full evaluation lifecycle:
  - Curate/annotate evaluation datasets on the cloud
  - Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best
  - Fine-tune metrics for custom results
  - Debug evaluation results via LLM traces
  - Monitor &amp; evaluate LLM responses in product to improve datasets with real-world data
  - Repeat until perfection

&gt; [!NOTE]
&gt; Confident AI is the DeepEval platform. Create an account [here.](https://app.confident-ai.com?utm_source=GitHub)

&lt;br /&gt;

# üîå Integrations

- ü¶Ñ LlamaIndex, to [**unit test RAG applications in CI/CD**](https://www.deepeval.com/integrations/frameworks/llamaindex?utm_source=GitHub)
- ü§ó Hugging Face, to [**enable real-time evaluations during LLM fine-tuning**](https://www.deepeval.com/integrations/frameworks/huggingface?utm_source=GitHub)

&lt;br /&gt;

# üöÄ QuickStart

Let&#039;s pretend your LLM application is a RAG based customer support chatbot; here&#039;s how DeepEval can help test what you&#039;ve built.

## Installation

```
pip install -U deepeval
```

## Create an account (highly recommended)

Using the `deepeval` platform will allow you to generate sharable testing reports on the cloud. It is free, takes no additional code to setup, and we highly recommend giving it a try.

To login, run:

```
deepeval login
```

Follow the instructions in the CLI to create an account, copy your API key, and paste it into the CLI. All test cases will automatically be logged (find more information on data privacy [here](https://deepeval.com/docs/data-privacy?utm_source=GitHub)).

## Writing your first test case

Create a test file:

```bash
touch test_chatbot.py
```

Open `test_chatbot.py` and write your first test case to run an **end-to-end** evaluation using DeepEval, which treats your LLM app as a black-box:

```python
import pytest
from deepeval import assert_test
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams

def test_case():
    correctness_metric = GEval(
        name=&quot;Correctness&quot;,
        criteria=&quot;Determine if the &#039;actual output&#039; is correct based on the &#039;expected output&#039;.&quot;,
        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],
        threshold=0.5
    )
    test_case = LLMTestCase(
        input=&quot;What if these shoes don&#039;t fit?&quot;,
        # Replace this with the actual output from your LLM application
        actual_output=&quot;You have 30 days to get a full refund at no extra cost.&quot;,
        expected_output=&quot;We offer a 30-day full refund at no extra costs.&quot;,
        retrieval_context=[&quot;All customers are eligible for a 30 day full refund at no extra costs.&quot;]
    )
    assert_test(test_case, [correctness_metric])
```

Set your `OPENAI_API_KEY` as an environment variable (you can also evaluate using your own custom model, for more details visit [this part of our docs](https://deepeval.com/docs/metrics-introduction#using-a-custom-llm?utm_source=GitHub)):

```
export OPENAI_API_KEY=&quot;...&quot;
```

And finally, run `test_chatbot.py` in the CLI:

```
deepeval test run test_chatbot.py
```

**Congratulations! Your test case should have passed ‚úÖ** Let&#039;s breakdown what happened.

- The variable `input` mimics a user input, and `actual_output` is a placeholder for what your application&#039;s supposed to output based on this input.
- The variable `expected_output` represents the ideal answer for a given `input`, and [`GEval`](https://deepeval.com/docs/metrics-llm-evals) is a research-backed metric provided by `deepeval` for you to evaluate your LLM output&#039;s on any custom with human-like accuracy.
- In this example, the metric `criteria` is correctness of the `actual_output` based on the provided `expected_output`.
- All metric scores range from 0 - 1, which the `threshold=0.5` threshold ultimately determines if your test have passed or not.

[Read our documentation](https://deepeval.com/docs/getting-started?utm_source=GitHub) for more information on more options to run end-to-end evaluation, how to use additional metrics, create your own custom metrics, and tutorials on how to integrate with other tools like LangChain and LlamaIndex.

&lt;br /&gt;

## Evaluating Nested Components

If you wish to evaluate individual components within your LLM app, you need to run **component-level** evals - a powerful way to evaluate any component within an LLM system.

Simply trace &quot;components&quot; such as LLM calls, retrievers, tool calls, and agents within your LLM application using the `@observe` decorator to apply metrics on a component-level. Tracing with `deepeval` is non-instrusive (learn more [here](https://deepeval.com/docs/evaluation-llm-tracing#dont-be-worried-about-tracing)) and helps you avoid rewriting your codebase just for evals:

```python
from deepeval.tracing import observe, update_current_span
from deepeval.test_case import LLMTestCase
from deepeval.dataset import Golden
from deepeval.metrics import GEval
from deepeval import evaluate

correctness = GEval(name=&quot;Correctness&quot;, criteria=&quot;Determine if the &#039;actual output&#039; is correct based on the &#039;expected output&#039;.&quot;, evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT])

@observe(metrics=[correctness])
def inner_component():
    # Component can be anything from an LLM call, retrieval, agent, tool use, etc.
    update_current_span(test_case=LLMTestCase(input=&quot;...&quot;, actual_output=&quot;...&quot;))
    return

@observe
def llm_app(input: str):
    inner_component()
    return

evaluate(observed_callback=llm_app, goldens=[Golden(input=&quot;Hi!&quot;)])
```

You can learn everything about component-level evaluations [here.](https://www.deepeval.com/docs/evaluation-component-level-llm-evals)

&lt;br /&gt;

## Evaluating Without Pytest Integration

Alternatively, you can evaluate without Pytest, which is more suited for a notebook environment.

```python
from deepeval import evaluate
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)
test_case = LLMTestCase(
    input=&quot;What if these shoes don&#039;t fit?&quot;,
    # Replace this with the actual output from your LLM application
    actual_output=&quot;We offer a 30-day full refund at no extra costs.&quot;,
    retrieval_context=[&quot;All customers are eligible for a 30 day full refund at no extra costs.&quot;]
)
evaluate([test_case], [answer_relevancy_metric])
```

## Using Standalone Metrics

DeepEval is extremely modular, making it easy for anyone to use any of our metrics. Continuing from the previous example:

```python
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)
test_case = LLMTestCase(
    input=&quot;What if these shoes don&#039;t fit?&quot;,
    # Replace this with the actual output from your LLM application
    actual_output=&quot;We offer a 30-day full refund at no extra costs.&quot;,
    retrieval_context=[&quot;All customers are eligible for a 30 day full refund at no extra costs.&quot;]
)

answer_relevancy_metric.measure(test_case)
print(answer_relevancy_metric.score)
# All metrics also offer an explanation
print(answer_relevancy_metric.reason)
```

Note that some metrics are for RAG pipelines, while others are for fine-tuning. Make sure to use our docs to pick the right one for your use case.

## Evaluating a Dataset / Test Cases in Bulk

In DeepEval, a dataset is simply a collection of test cases. Here is how you can evaluate these in bulk:

```python
import pytest
from deepeval import assert_test
from deepeval.dataset import EvaluationDataset, Golden
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

dataset = EvaluationDataset(goldens=[Golden(input=&quot;What&#039;s the weather like today?&quot;)])

for golden in dataset.goldens:
    test_case = LLMTestCase(
        input=golden.input,
        actual_output=your_llm_app(golden.input)
    )
    dataset.add_test_case(test_case)

@pytest.mark.parametrize(
    &quot;test_case&quot;,
    dataset,
)
def test_customer_chatbot(test_case: LLMTestCase):
    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)
    assert_test(test_case, [answer_relevancy_metric])
```

```bash
# Run this in the CLI, you can also add an optional -n flag to run tests in parallel
deepeval test run test_&lt;filename&gt;.py -n 4
```

&lt;br/&gt;

Alternatively, although we recommend using `deepeval test run`, you can evaluate a dataset/test cases without using our Pytest integration:

```python
from deepeval import evaluate
...

evaluate(dataset, [answer_relevancy_metric])
# or
dataset.evaluate([answer_relevancy_metric])
```

# LLM Evaluation With Confident AI

The correct LLM evaluation lifecycle is only achievable with [the DeepEval platform](https://confident-ai.com?utm_source=Github). It allows you to:

1. Curate/annotate evaluation datasets on the cloud
2. Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best
3. Fine-tune metrics for custom results
4. Debug evaluation results via LLM traces
5. Monitor &amp; evaluate LLM responses in product to improve datasets with real-world data
6. Repeat until perfection

Everything on Confident AI, including how to use Confident is available [here](https://documentation.confident-ai.com/docs?utm_source=GitHub).

To begin, login from the CLI:

```bash
deepeval login
```

Follow the instructions to log in, create your account, and paste your API key into the CLI.

Now, run your test file again:

```bash
deepeval test run test_chatbot.py
```

You should see a link displayed in the CLI once the test has finished running. Paste it into your browser to view the results!

![Demo GIF](assets/demo.gif)

&lt;br /&gt;

# Contributing

Please read [CONTRIBUTING.md](https://github.com/confident-ai/deepeval/blob/main/CONTRIBUTING.md) for details on our code of conduct, and the process for submitting pull requests to us.

&lt;br /&gt;

# Roadmap

Features:

- [x] Integration with Confident AI
- [x] Implement G-Eval
- [x] Implement RAG metrics
- [x] Implement Conversational metrics
- [x] Evaluation Dataset Creation
- [x] Red-Teaming
- [ ] DAG custom metrics
- [ ] Guardrails

&lt;br /&gt;

# Authors

Built by the founders of Confident AI. Contact jeffreyip@confident-ai.com for all enquiries.

&lt;br /&gt;

# License

DeepEval is licensed under Apache 2.0 - see the [LICENSE.md](https://github.com/confident-ai/deepeval/blob/main/LICENSE.md) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/mcp-for-beginners]]></title>
            <link>https://github.com/microsoft/mcp-for-beginners</link>
            <guid>https://github.com/microsoft/mcp-for-beginners</guid>
            <pubDate>Thu, 07 Aug 2025 00:04:52 GMT</pubDate>
            <description><![CDATA[This open-source curriculum introduces the fundamentals of Model Context Protocol (MCP) through real-world, cross-language examples in .NET, Java, TypeScript, JavaScript, and Python. Designed for developers, it focuses on practical techniques for building modular, scalable, and secure AI workflows from session setup to service orchestration.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/mcp-for-beginners">microsoft/mcp-for-beginners</a></h1>
            <p>This open-source curriculum introduces the fundamentals of Model Context Protocol (MCP) through real-world, cross-language examples in .NET, Java, TypeScript, JavaScript, and Python. Designed for developers, it focuses on practical techniques for building modular, scalable, and secure AI workflows from session setup to service orchestration.</p>
            <p>Language: Python</p>
            <p>Stars: 7,643</p>
            <p>Forks: 2,070</p>
            <p>Stars today: 671 stars today</p>
            <h2>README</h2><pre>![MCP-for-beginners](./images/mcp-beginners.png) 

[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/mcp-for-beginners.svg)](https://GitHub.com/microsoft/mcp-for-beginners/graphs/contributors)
[![GitHub issues](https://img.shields.io/github/issues/microsoft/mcp-for-beginners.svg)](https://GitHub.com/microsoft/mcp-for-beginners/issues)
[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/mcp-for-beginners.svg)](https://GitHub.com/microsoft/mcp-for-beginners/pulls)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)

[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/mcp-for-beginners.svg?style=social&amp;label=Watch)](https://GitHub.com/microsoft/mcp-for-beginners/watchers)
[![GitHub forks](https://img.shields.io/github/forks/microsoft/mcp-for-beginners.svg?style=social&amp;label=Fork)](https://GitHub.com/microsoft/mcp-for-beginners/fork)
[![GitHub stars](https://img.shields.io/github/stars/microsoft/mcp-for-beginners?style=social&amp;label=Star)](https://GitHub.com/microsoft/mcp-for-beginners/stargazers)


[![Microsoft Azure AI Foundry Discord](https://dcbadge.limes.pink/api/server/ByRwuEEgH4)](https://discord.com/invite/ByRwuEEgH4)

Follow these steps to get started using these resources:
1. **Fork the Repository**: Click [![GitHub forks](https://img.shields.io/github/forks/microsoft/mcp-for-beginners.svg?style=social&amp;label=Fork)](https://GitHub.com/microsoft/mcp-for-beginners/fork)
2. **Clone the Repository**:   `git clone https://github.com/microsoft/mcp-for-beginners.git`
3. [**Join The Azure AI Foundry Discord and meet experts and fellow developers**](https://discord.com/invite/ByRwuEEgH4)


### üåê Multi-Language Support

#### Supported via GitHub Action (Automated &amp; Always Up-to-Date)

[French](./translations/fr/README.md) | [Spanish](./translations/es/README.md) | [German](./translations/de/README.md) | [Russian](./translations/ru/README.md) | [Arabic](./translations/ar/README.md) | [Persian (Farsi)](./translations/fa/README.md) | [Urdu](./translations/ur/README.md) | [Chinese (Simplified)](./translations/zh/README.md) | [Chinese (Traditional, Macau)](./translations/mo/README.md) | [Chinese (Traditional, Hong Kong)](./translations/hk/README.md) | [Chinese (Traditional, Taiwan)](./translations/tw/README.md) | [Japanese](./translations/ja/README.md) | [Korean](./translations/ko/README.md) | [Hindi](./translations/hi/README.md) | [Bengali](./translations/bn/README.md) | [Marathi](./translations/mr/README.md) | [Nepali](./translations/ne/README.md) | [Punjabi (Gurmukhi)](./translations/pa/README.md) | [Portuguese (Portugal)](./translations/pt/README.md) | [Portuguese (Brazil)](./translations/br/README.md) | [Italian](./translations/it/README.md) | [Polish](./translations/pl/README.md) | [Turkish](./translations/tr/README.md) | [Greek](./translations/el/README.md) | [Thai](./translations/th/README.md) | [Swedish](./translations/sv/README.md) | [Danish](./translations/da/README.md) | [Norwegian](./translations/no/README.md) | [Finnish](./translations/fi/README.md) | [Dutch](./translations/nl/README.md) | [Hebrew](./translations/he/README.md) | [Vietnamese](./translations/vi/README.md) | [Indonesian](./translations/id/README.md) | [Malay](./translations/ms/README.md) | [Tagalog (Filipino)](./translations/tl/README.md) | [Swahili](./translations/sw/README.md) | [Hungarian](./translations/hu/README.md) | [Czech](./translations/cs/README.md) | [Slovak](./translations/sk/README.md) | [Romanian](./translations/ro/README.md) | [Bulgarian](./translations/bg/README.md) | [Serbian (Cyrillic)](./translations/sr/README.md) | [Croatian](./translations/hr/README.md) | [Slovenian](./translations/sl/README.md) | [Ukrainian](./translations/uk/README.md) | [Burmese (Myanmar)](./translations/my/README.md)

# üöÄ Model Context Protocol (MCP) Curriculum for Beginners

## **Learn MCP with Hands-on Code Examples in C#, Java, JavaScript, Python, and TypeScript**

## üß† Overview of the Model Context Protocol Curriculum

The **Model Context Protocol (MCP)** is a cutting-edge framework designed to standardize interactions between AI models and client applications. This open-source curriculum offers a structured learning path, complete with practical coding examples and real-world use cases, across popular programming languages including C#, Java, JavaScript, TypeScript, and Python.

Whether you&#039;re an AI developer, system architect, or software engineer, this guide is your comprehensive resource for mastering MCP fundamentals and implementation strategies.

## üîó Official MCP Resources

- üìò [MCP Documentation](https://modelcontextprotocol.io/) ‚Äì Detailed tutorials and user guides  
- üìú [MCP Specification](https://modelcontextprotocol.io/docs/) ‚Äì Protocol architecture and technical references  
- üìú [Original MCP Specification](https://spec.modelcontextprotocol.io/) ‚Äì Legacy technical references (may contain additional details)  
- üßë‚Äçüíª [MCP GitHub Repository](https://github.com/modelcontextprotocol) ‚Äì Open-source SDKs, tools, and code samples
- üåê [MCP Community](https://github.com/orgs/modelcontextprotocol/discussions) ‚Äì Join discussions and contribute to the community

## Join us for MCP Dev Days 29-30th July 2025 

Get ready for two days of deep technical insight, community connection, and hands-on learning at MCP Dev Days, a virtual event dedicated to the Model Context Protocol (MCP) ‚Äî the emerging standard that bridges AI models and the tools they rely on.

‚û°Ô∏è [Register for MCP Dev Days](https://developer.microsoft.com/en-us/reactor/series/S-1563/)

You can watch MCP Dev Days by registering on our event page: https://aka.ms/mcpdevdays. From there, you‚Äôll be able to join a live stream on YouTube or Twitch. All of the content is recorded and will be available afterwards on the Microsoft Developer YouTube channel. Source code for the demos will be available on GitHub too.

### Event Details
- Dates: July 29 (Day 1) &amp; July 30 (Day 2)
- Time: 9:00 AM PST daily
- Where: Online ‚Äì join from anywhere!

#### Day 1: MCP Productivity, DevTools, &amp; Community: 

Is all about empowering developers to use MCP in their developer workflow and celebrating the amazing MCP community. We‚Äôll be joined with community members and partners such as Arcade, Block, Okta, and Neon to see how they are collaborating with Microsoft to shape an open, extensible MCP ecosystem. Real-world demos across VS Code, Visual Studio, GitHub Copilot, and popular community tools
Practical, context-driven dev workflows
Community-led sessions and insights
Whether you‚Äôre just getting started with MCP or already building with it, Day 1 will set the stage with inspiration and actionable takeaways.

#### Day 2: Build MCP Servers with Confidence

Is for MCP builders. We‚Äôll go deep into implementation strategies and best practices for creating MCP servers and integrating MCP into your AI workflows.

### Topics include:

- Building MCP Servers and integrating them into agent experiences
- Prompt-driven development
- Security best practices
- Using building blocks like Functions, ACA, and API Management
- Registry alignment and tooling (1P + 3P)

If you‚Äôre a developer, tool builder, or AI product strategist, this day is packed with the insights you need to build scalable, secure, and future-ready MCP solutions.

## üß≠ MCP Curriculum Overview

### üìö Complete Curriculum Structure

| Module | Topic | Description | Link |
|--------|-------|-------------|------|
| **Module 1-3: Fundamentals** | | | |
| 00 | Introduction to MCP | Overview of the Model Context Protocol and its significance in AI pipelines | [Read more](./00-Introduction/README.md) |
| 01 | Core Concepts Explained | In-depth exploration of core MCP concepts | [Read more](./01-CoreConcepts/README.md) |
| 02 | Security in MCP | Security threats and best practices | [Read more](./02-Security/README.md) |
| 03 | Getting Started with MCP | Environment setup, basic servers/clients, integration | [Read more](./03-GettingStarted/README.md) |
| **Module 3: Building Your First Server &amp; Client** | | | |
| 3.1 | First Server | Create your first MCP server | [Guide](./03-GettingStarted/01-first-server/README.md) |
| 3.2 | First Client | Develop a basic MCP client | [Guide](./03-GettingStarted/02-client/README.md) |
| 3.3 | Client with LLM | Integrate large language models | [Guide](./03-GettingStarted/03-llm-client/README.md) |
| 3.4 | VS Code Integration | Consume MCP servers in VS Code | [Guide](./03-GettingStarted/04-vscode/README.md) |
| 3.5 | SSE Server | Create servers using Server-Sent Events | [Guide](./03-GettingStarted/05-sse-server/README.md) |
| 3.6 | HTTP Streaming | Implement HTTP streaming in MCP | [Guide](./03-GettingStarted/06-http-streaming/README.md) |
| 3.7 | AI Toolkit | Use AI Toolkit with MCP | [Guide](./03-GettingStarted/07-aitk/README.md) |
| 3.8 | Testing | Test your MCP server implementation | [Guide](./03-GettingStarted/08-testing/README.md) |
| 3.9 | Deployment | Deploy MCP servers to production | [Guide](./03-GettingStarted/09-deployment/README.md) |
| **Module 4-5: Practical &amp; Advanced** | | | |
| 04 | Practical Implementation | SDKs, debugging, testing, reusable prompt templates | [Read more](./04-PracticalImplementation/README.md) |
| 05 | Advanced Topics in MCP | Multi-modal AI, scaling, enterprise use | [Read more](./05-AdvancedTopics/README.md) |
| 5.1 | Azure Integration | MCP Integration with Azure | [Guide](./05-AdvancedTopics/mcp-integration/README.md) |
| 5.2 | Multi-modality | Working with multiple modalities | [Guide](./05-AdvancedTopics/mcp-multi-modality/README.md) |
| 5.3 | OAuth2 Demo | Implement OAuth2 authentication | [Guide](./05-AdvancedTopics/mcp-oauth2-demo/README.md) |
| 5.4 | Root Contexts | Understand and implement root contexts | [Guide](./05-AdvancedTopics/mcp-root-contexts/README.md) |
| 5.5 | Routing | MCP routing strategies | [Guide](./05-AdvancedTopics/mcp-routing/README.md) |
| 5.6 | Sampling | Sampling techniques in MCP | [Guide](./05-AdvancedTopics/mcp-sampling/README.md) |
| 5.7 | Scaling | Scale MCP implementations | [Guide](./05-AdvancedTopics/mcp-scaling/README.md) |
| 5.8 | Security | Advanced security considerations | [Guide](./05-AdvancedTopics/mcp-security/README.md) |
| 5.9 | Web Search | Implement web search capabilities | [Guide](./05-AdvancedTopics/web-search-mcp/README.md) |
| 5.10 | Realtime Streaming | Build realtime streaming functionality | [Guide](./05-AdvancedTopics/mcp-realtimestreaming/README.md) |
| 5.11 | Realtime Search | Implement realtime search | [Guide](./05-AdvancedTopics/mcp-realtimesearch/README.md) |
| 5.12 | Entra ID Auth | Authentication with Microsoft Entra ID | [Guide](./05-AdvancedTopics/mcp-security-entra/README.md) |
| 5.13 | Foundry Integration | Integrate with Azure AI Foundry | [Guide](./05-AdvancedTopics/mcp-foundry-agent-integration/README.md) |
| 5.14 | Context Engineering | Techniques for effective context engineering | [Guide](./05-AdvancedTopics/mcp-contextengineering/README.md) |
| **Module 6-10: Community &amp; Best Practices** | | | |
| 06 | Community Contributions | How to contribute to the MCP ecosystem | [Guide](./06-CommunityContributions/README.md) |
| 07 | Insights from Early Adoption | Real-world implementation stories | [Guide](./07-LessonsFromEarlyAdoption/README.md) |
| 08 | Best Practices for MCP | Performance, fault-tolerance, resilience | [Guide](./08-BestPractices/README.md) |
| 09 | MCP Case Studies | Practical implementation examples | [Guide](./09-CaseStudy/README.md) |
| 10 | Hands-on Workshop | Building an MCP Server with AI Toolkit | [Lab](./10-StreamliningAIWorkflowsBuildingAnMCPServerWithAIToolkit/README.md) |

### üíª Sample Code Projects

#### Basic MCP Calculator Samples

| Language | Description | Link |
|----------|-------------|------|
| C# | MCP Server Example | [View Code](./03-GettingStarted/samples/csharp/README.md) |
| Java | MCP Calculator | [View Code](./03-GettingStarted/samples/java/calculator/README.md) |
| JavaScript | MCP Demo | [View Code](./03-GettingStarted/samples/javascript/README.md) |
| Python | MCP Server | [View Code](./03-GettingStarted/samples/python/mcp_calculator_server.py) |
| TypeScript | MCP Example | [View Code](./03-GettingStarted/samples/typescript/README.md) |

#### Advanced MCP Implementations

| Language | Description | Link |
|----------|-------------|------|
| C# | Advanced Sample | [View Code](./04-PracticalImplementation/samples/csharp/README.md) |
| Java | Container App Example | [View Code](./04-PracticalImplementation/samples/java/containerapp/README.md) |
| JavaScript | Advanced Sample | [View Code](./04-PracticalImplementation/samples/javascript/README.md) |
| Python | Complex Implementation | [View Code](./04-PracticalImplementation/samples/python/mcp_sample.py) |
| TypeScript | Container Sample | [View Code](./04-PracticalImplementation/samples/typescript/README.md) |


## üéØ Prerequisites for Learning MCP

To get the most out of this curriculum, you should have:

- Basic knowledge of programming in at least one of the following languages: C#, Java, JavaScript, Python, or TypeScript
- Understanding of client-server model and APIs
- Familiarity with REST and HTTP concepts
- (Optional) Background in AI/ML concepts

- Joining our community discussions for support

## üìö Study Guide &amp; Resources

This repository includes several resources to help you navigate and learn effectively:

### Study Guide

A comprehensive [Study Guide](./study_guide.md) is available to help you navigate this repository effectively. The guide includes:

- A visual curriculum map showing all topics covered
- Detailed breakdown of each repository section
- Guidance on how to use sample projects
- Recommended learning paths for different skill levels
- Additional resources to complement your learning journey

### Changelog

We maintain a detailed [Changelog](./changelog.md) that tracks all significant updates to the curriculum materials, including:

- New content additions
- Structural changes
- Feature improvements
- Documentation updates

## üõ†Ô∏è How to Use This Curriculum Effectively

Each lesson in this guide includes:

1. Clear explanations of MCP concepts  
2. Live code examples in multiple languages  
3. Exercises to build real MCP applications  
4. Extra resources for advanced learners


## üåü Community Thanks

Thanks to Microsoft Valued Professional [Shivam Goyal](https://www.linkedin.com/in/shivam2003/) for contributing important code samples. 

## üìú License Information

This content is licensed under the **MIT License**. For terms and conditions, see the [LICENSE](./LICENSE).

## ü§ù Contribution Guidelines

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit &lt;https://cla.opensource.microsoft.com&gt;.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## üìÇ Repository Structure

The repository is organized as follows:

- **Core Curriculum (00-10)**: The main content organized in ten sequential modules
- **images/**: Diagrams and illustrations used throughout the curriculum
- **translations/**: Multi-language support with automated translations
- **translated_images/**: Localized versions of diagrams and illustrations
- **study_guide.md**: Comprehensive guide to navigating the repository
- **changelog.md**: Record of all significant changes to the curriculum materials
- **mcp.json**: Configuration file for MCP specification
- **CODE_OF_CONDUCT.md, LICENSE, SECURITY.md, SUPPORT.md**: Project governance documents

## üéí Other Courses
Our team produces other courses! Check out:

- [AI Agents For Beginners](https://github.com/microsoft/ai-agents-for-beginners?WT.mc_id=academic-105485-koreyst)
- [Generative AI for Beginners using .NET](https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst)
- [Generative AI for Beginners using JavaScript](https://github.com/microsoft/generative-ai-with-javascript?WT.mc_id=academic-105485-koreyst)
- [Generative AI for Beginners](https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst)
- [ML for Beginners](https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst)
- [Data Science for Beginners](https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst)
- [AI for Beginners](https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst)
- [Cybersecurity for Beginners](https://github.com/microsoft/Security-101??WT.mc_id=academic-96948-sayoung)
- [Web Dev for Beginners](https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst)
- [IoT for Beginners](https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst)
- [XR Development for Beginners](https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst)
- [Mastering GitHub Copilot for AI Paired Programming](https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst)
- [Mastering GitHub Copilot for C#/.NET Developers](https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst)
- [Choose Your Own Copilot Adventure](https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst)


## ‚Ñ¢Ô∏è Trademark Notice

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
[Microsoft&#039;s Trademark &amp; Brand Guidelines](https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos is subject to those third-parties&#039; policies.

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[openai/tiktoken]]></title>
            <link>https://github.com/openai/tiktoken</link>
            <guid>https://github.com/openai/tiktoken</guid>
            <pubDate>Thu, 07 Aug 2025 00:04:51 GMT</pubDate>
            <description><![CDATA[tiktoken is a fast BPE tokeniser for use with OpenAI's models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/openai/tiktoken">openai/tiktoken</a></h1>
            <p>tiktoken is a fast BPE tokeniser for use with OpenAI's models.</p>
            <p>Language: Python</p>
            <p>Stars: 15,395</p>
            <p>Forks: 1,158</p>
            <p>Stars today: 55 stars today</p>
            <h2>README</h2><pre># ‚è≥ tiktoken

tiktoken is a fast [BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding) tokeniser for use with
OpenAI&#039;s models.

```python
import tiktoken
enc = tiktoken.get_encoding(&quot;o200k_base&quot;)
assert enc.decode(enc.encode(&quot;hello world&quot;)) == &quot;hello world&quot;

# To get the tokeniser corresponding to a specific model in the OpenAI API:
enc = tiktoken.encoding_for_model(&quot;gpt-4o&quot;)
```

The open source version of `tiktoken` can be installed from [PyPI](https://pypi.org/project/tiktoken):
```
pip install tiktoken
```

The tokeniser API is documented in `tiktoken/core.py`.

Example code using `tiktoken` can be found in the
[OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb).


## Performance

`tiktoken` is between 3-6x faster than a comparable open source tokeniser:

![image](https://raw.githubusercontent.com/openai/tiktoken/main/perf.svg)

Performance measured on 1GB of text using the GPT-2 tokeniser, using `GPT2TokenizerFast` from
`tokenizers==0.13.2`, `transformers==4.24.0` and `tiktoken==0.2.0`.


## Getting help

Please post questions in the [issue tracker](https://github.com/openai/tiktoken/issues).

If you work at OpenAI, make sure to check the internal documentation or feel free to contact
@shantanu.

## What is BPE anyway?

Language models don&#039;t see text like you and I, instead they see a sequence of numbers (known as tokens).
Byte pair encoding (BPE) is a way of converting text into tokens. It has a couple desirable
properties:
1) It&#039;s reversible and lossless, so you can convert tokens back into the original text
2) It works on arbitrary text, even text that is not in the tokeniser&#039;s training data
3) It compresses the text: the token sequence is shorter than the bytes corresponding to the
   original text. On average, in practice, each token corresponds to about 4 bytes.
4) It attempts to let the model see common subwords. For instance, &quot;ing&quot; is a common subword in
   English, so BPE encodings will often split &quot;encoding&quot; into tokens like &quot;encod&quot; and &quot;ing&quot;
   (instead of e.g. &quot;enc&quot; and &quot;oding&quot;). Because the model will then see the &quot;ing&quot; token again and
   again in different contexts, it helps models generalise and better understand grammar.

`tiktoken` contains an educational submodule that is friendlier if you want to learn more about
the details of BPE, including code that helps visualise the BPE procedure:
```python
from tiktoken._educational import *

# Train a BPE tokeniser on a small amount of text
enc = train_simple_encoding()

# Visualise how the GPT-4 encoder encodes text
enc = SimpleBytePairEncoding.from_tiktoken(&quot;cl100k_base&quot;)
enc.encode(&quot;hello world aaaaaaaaaaaa&quot;)
```


## Extending tiktoken

You may wish to extend `tiktoken` to support new encodings. There are two ways to do this.


**Create your `Encoding` object exactly the way you want and simply pass it around.**

```python
cl100k_base = tiktoken.get_encoding(&quot;cl100k_base&quot;)

# In production, load the arguments directly instead of accessing private attributes
# See openai_public.py for examples of arguments for specific encodings
enc = tiktoken.Encoding(
    # If you&#039;re changing the set of special tokens, make sure to use a different name
    # It should be clear from the name what behaviour to expect.
    name=&quot;cl100k_im&quot;,
    pat_str=cl100k_base._pat_str,
    mergeable_ranks=cl100k_base._mergeable_ranks,
    special_tokens={
        **cl100k_base._special_tokens,
        &quot;&lt;|im_start|&gt;&quot;: 100264,
        &quot;&lt;|im_end|&gt;&quot;: 100265,
    }
)
```

**Use the `tiktoken_ext` plugin mechanism to register your `Encoding` objects with `tiktoken`.**

This is only useful if you need `tiktoken.get_encoding` to find your encoding, otherwise prefer
option 1.

To do this, you&#039;ll need to create a namespace package under `tiktoken_ext`.

Layout your project like this, making sure to omit the `tiktoken_ext/__init__.py` file:
```
my_tiktoken_extension
‚îú‚îÄ‚îÄ tiktoken_ext
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ my_encodings.py
‚îî‚îÄ‚îÄ setup.py
```

`my_encodings.py` should be a module that contains a variable named `ENCODING_CONSTRUCTORS`.
This is a dictionary from an encoding name to a function that takes no arguments and returns
arguments that can be passed to `tiktoken.Encoding` to construct that encoding. For an example, see
`tiktoken_ext/openai_public.py`. For precise details, see `tiktoken/registry.py`.

Your `setup.py` should look something like this:
```python
from setuptools import setup, find_namespace_packages

setup(
    name=&quot;my_tiktoken_extension&quot;,
    packages=find_namespace_packages(include=[&#039;tiktoken_ext*&#039;]),
    install_requires=[&quot;tiktoken&quot;],
    ...
)
```

Then simply `pip install ./my_tiktoken_extension` and you should be able to use your
custom encodings! Make sure **not** to use an editable install.

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hao-ai-lab/FastVideo]]></title>
            <link>https://github.com/hao-ai-lab/FastVideo</link>
            <guid>https://github.com/hao-ai-lab/FastVideo</guid>
            <pubDate>Thu, 07 Aug 2025 00:04:50 GMT</pubDate>
            <description><![CDATA[A unified inference and post-training framework for accelerated video generation.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hao-ai-lab/FastVideo">hao-ai-lab/FastVideo</a></h1>
            <p>A unified inference and post-training framework for accelerated video generation.</p>
            <p>Language: Python</p>
            <p>Stars: 1,786</p>
            <p>Forks: 122</p>
            <p>Stars today: 27 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;img src=assets/logo.png width=&quot;30%&quot;/&gt;
&lt;/div&gt;

**FastVideo is a unified post-training and inference framework for accelerated video generation.**

FastVideo features an end-to-end unified pipeline for accelerating diffusion models, starting from data preprocessing to model training, finetuning, distillation, and inference. FastVideo is designed to be modular and extensible, allowing users to easily add new optimizations and techniques. Whether it is training-free optimizations or post-training optimizations, FastVideo has you covered.

&lt;p align=&quot;center&quot;&gt;
    | &lt;a href=&quot;https://hao-ai-lab.github.io/FastVideo&quot;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://hao-ai-lab.github.io/FastVideo/inference/inference_quick_start.html&quot;&gt;&lt;b&gt; Quick Start&lt;/b&gt;&lt;/a&gt; | ü§ó &lt;a href=&quot;https://huggingface.co/FastVideo/FastWan2.1-T2V-1.3B-Diffusers&quot;  target=&quot;_blank&quot;&gt;&lt;b&gt;FastWan2.1&lt;/b&gt;&lt;/a&gt;  | ü§ó &lt;a href=&quot;https://huggingface.co/FastVideo/FastWan2.2-TI2V-5B-Diffusers&quot; target=&quot;_blank&quot;&gt;&lt;b&gt;FastWan2.2&lt;/b&gt;&lt;/a&gt; | üü£üí¨ &lt;a href=&quot;https://join.slack.com/t/fastvideo/shared_invite/zt-38u6p1jqe-yDI1QJOCEnbtkLoaI5bjZQ&quot; target=&quot;_blank&quot;&gt; &lt;b&gt;Slack&lt;/b&gt; &lt;/a&gt; |  üü£üí¨ &lt;a href=&quot;https://ibb.co/qqPzbrw&quot; target=&quot;_blank&quot;&gt; &lt;b&gt; WeChat &lt;/b&gt; &lt;/a&gt; |
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=assets/fastwan.png width=&quot;90%&quot;/&gt;
&lt;/div&gt;

## NEWS
- ```2025/08/04```: Release [FastWan](https://hao-ai-lab.github.io/FastVideo/distillation/dmd.html) models and [Sparse-Distillation](https://hao-ai-lab.github.io/blogs/fastvideo_post_training/).
- ```2025/06/14```: Release finetuning and inference code for [VSA](https://arxiv.org/pdf/2505.13389)
- ```2025/04/24```: [FastVideo V1](https://hao-ai-lab.github.io/blogs/fastvideo/) is released!
- ```2025/02/18```: Release the inference code for [Sliding Tile Attention](https://hao-ai-lab.github.io/blogs/sta/).

## Key Features

FastVideo has the following features:
- End-to-end post-training support:
  - [Sparse distillation](https://hao-ai-lab.github.io/blogs/fastvideo_post_training/) for Wan2.1 and Wan2.2 to achineve &gt;50x denoising speedup
  - Data preprocessing pipeline for video data
  - Support full finetuning and LoRA finetuning for state-of-the-art open video DiTs
  - Scalable training with FSDP2, sequence parallelism, and selective activation checkpointing, with near linear scaling to 64 GPUs
- State-of-the-art performance optimizations for inference
  - [Video Sparse Attention](https://arxiv.org/pdf/2505.13389)
  - [Sliding Tile Attention](https://arxiv.org/pdf/2502.04507)
  - [TeaCache](https://arxiv.org/pdf/2411.19108)
  - [Sage Attention](https://arxiv.org/abs/2410.02367)
- Diverse hardware and OS support
  - Support H100, A100, 4090
  - Support Linux, Windows, MacOS

## Getting Started
We recommend using an environment manager such as `Conda` to create a clean environment:

```bash
# Create and activate a new conda environment
conda create -n fastvideo python=3.12
conda activate fastvideo

# Install FastVideo
pip install fastvideo
```

Please see our [docs](https://hao-ai-lab.github.io/FastVideo/getting_started/installation.html) for more detailed installation instructions.

## Sparse Distillation
For our sparse distillation techniques, please see our [distillation docs](https://hao-ai-lab.github.io/FastVideo/distillation/dmd.html) and check out our [blog](https://hao-ai-lab.github.io/blogs/fastvideo_post_training/).

See below for recipes and datasets:

|                                            Model                                              |                                               Sparse Distillation                                                 |                                                  Dataset                                                  |
|:-------------------------------------------------------------------------------------------:  |:---------------------------------------------------------------------------------------------------------------:  |:--------------------------------------------------------------------------------------------------------: |
| [FastWan2.1-T2V-1.3B](https://huggingface.co/FastVideo/FastWan2.1-T2V-1.3B-Diffusers)         |    [Recipe](https://github.com/hao-ai-lab/FastVideo/tree/main/examples/distill/Wan2.1-T2V/Wan-Syn-Data-480P)      | [FastVideo Synthetic Wan2.1 480P](https://huggingface.co/datasets/FastVideo/Wan-Syn_77x448x832_600k)      |
| [FastWan2.1-T2V-14B-Preview](https://huggingface.co/FastVideo/FastWan2.1-T2V-14B-Diffusers)   |                                                   Coming soon!                                                    |   [FastVideo Synthetic Wan2.1 720P](https://huggingface.co/datasets/FastVideo/Wan-Syn_77x768x1280_250k)   |
| [FastWan2.2-TI2V-5B](https://huggingface.co/FastVideo/FastWan2.2-TI2V-5B-Diffusers)           | [Recipe](https://github.com/hao-ai-lab/FastVideo/tree/main/examples/distill/Wan2.2-TI2V-5B-Diffusers/Data-free)   | [FastVideo Synthetic Wan2.2 720P](https://huggingface.co/datasets/FastVideo/Wan2.2-Syn-121x704x1280_32k)  |

## Inference
### Generating Your First Video
Here&#039;s a minimal example to generate a video using the default settings. Create a file called `example.py` with the following code:

```python
from fastvideo import VideoGenerator

def main():
    # Create a video generator with a pre-trained model
    generator = VideoGenerator.from_pretrained(
        &quot;FastVideo/FastWan2.1-T2V-1.3B-Diffusers&quot;,
        num_gpus=1,  # Adjust based on your hardware
    )

    # Define a prompt for your video
    prompt = &quot;A curious raccoon peers through a vibrant field of yellow sunflowers, its eyes wide with interest.&quot;

    # Generate the video
    video = generator.generate_video(
        prompt,
        return_frames=True,  # Also return frames from this call (defaults to False)
        output_path=&quot;my_videos/&quot;,  # Controls where videos are saved
        save_video=True
    )

if __name__ == &#039;__main__&#039;:
    main()
```

Run the script with:

```bash
python example.py
```

For a more detailed guide, please see our [inference quick start](https://hao-ai-lab.github.io/FastVideo/inference/inference_quick_start.html).

### Other docs:

- [Design Overview](https://hao-ai-lab.github.io/FastVideo/design/overview.html)
- [Contribution Guide](https://hao-ai-lab.github.io/FastVideo/getting_started/installation.html)

## Distillation and Finetuning
- [Distillation Guide](https://hao-ai-lab.github.io/FastVideo/distillation/dmd.html)
&lt;!-- - [Finetuning Guide](https://hao-ai-lab.github.io/FastVideo/training/finetune.html) --&gt;

## üìë Development Plan
&lt;!-- - More distillation methods --&gt;
  &lt;!-- - [ ] Add Distribution Matching Distillation --&gt;
More FastWan Models Coming Soon!
- [ ] Add FastWan2.1-T2V-14B
- [ ] Add FastWan2.2-T2V-14B
- [ ] Add FastWan2.2-I2V-14B
&lt;!-- - Optimization features
- Code updates --&gt;
  &lt;!-- - [ ] fp8 support --&gt;
  &lt;!-- - [ ] faster load model and save model support --&gt;

See details in [development roadmap](https://github.com/hao-ai-lab/FastVideo/issues/468).

## ü§ù Contributing

We welcome all contributions. Please check out our guide [here](https://hao-ai-lab.github.io/FastVideo/contributing/overview.html)

## Acknowledgement
We learned and reused code from the following projects:
- [Wan-Video](https://github.com/Wan-Video)
- [ThunderKittens](https://github.com/HazyResearch/ThunderKittens)
- [Triton](https://github.com/triton-lang/triton)
- [DMD2](https://github.com/tianweiy/DMD2)
- [diffusers](https://github.com/huggingface/diffusers)
- [xDiT](https://github.com/xdit-project/xDiT)
- [vLLM](https://github.com/vllm-project/vllm)
- [SGLang](https://github.com/sgl-project/sglang)

We thank [MBZUAI](https://ifm.mbzuai.ac.ae/), [Anyscale](https://www.anyscale.com/), and [GMI Cloud](https://www.gmicloud.ai/) for their support throughout this project.

## Citation
If you find FastVideo useful, please considering citing our work:

```bibtex
@software{fastvideo2024,
  title        = {FastVideo: A Unified Framework for Accelerated Video Generation},
  author       = {The FastVideo Team},
  url          = {https://github.com/hao-ai-lab/FastVideo},
  month        = apr,
  year         = {2024},
}

@article{zhang2025vsa,
  title={VSA: Faster Video Diffusion with Trainable Sparse Attention},
  author={Zhang, Peiyuan and Huang, Haofeng and Chen, Yongqi and Lin, Will and Liu, Zhengzhong and Stoica, Ion and Xing, Eric and Zhang, Hao},
  journal={arXiv preprint arXiv:2505.13389},
  year={2025}
}

@article{zhang2025fast,
  title={Fast video generation with sliding tile attention},
  author={Zhang, Peiyuan and Chen, Yongqi and Su, Runlong and Ding, Hangliang and Stoica, Ion and Liu, Zhengzhong and Zhang, Hao},
  journal={arXiv preprint arXiv:2502.04507},
  year={2025}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[souzatharsis/podcastfy]]></title>
            <link>https://github.com/souzatharsis/podcastfy</link>
            <guid>https://github.com/souzatharsis/podcastfy</guid>
            <pubDate>Thu, 07 Aug 2025 00:04:49 GMT</pubDate>
            <description><![CDATA[An Open Source Python alternative to NotebookLM's podcast feature: Transforming Multimodal Content into Captivating Multilingual Audio Conversations with GenAI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/souzatharsis/podcastfy">souzatharsis/podcastfy</a></h1>
            <p>An Open Source Python alternative to NotebookLM's podcast feature: Transforming Multimodal Content into Captivating Multilingual Audio Conversations with GenAI</p>
            <p>Language: Python</p>
            <p>Stars: 4,937</p>
            <p>Forks: 559</p>
            <p>Stars today: 82 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;a href=&quot;https://trendshift.io/repositories/12965&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12965&quot; alt=&quot;Podcastfy.ai | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

# Podcastfy.ai üéôÔ∏èü§ñ
An Open Source API alternative to NotebookLM&#039;s podcast feature: Transforming Multimodal Content into Captivating Multilingual Audio Conversations with GenAI



https://github.com/user-attachments/assets/5d42c106-aabe-44c1-8498-e9c53545ba40



[Paper](https://github.com/souzatharsis/podcastfy/blob/main/paper/paper.pdf) |
[Python Package](https://github.com/souzatharsis/podcastfy/blob/59563ee105a0d1dbb46744e0ff084471670dd725/podcastfy.ipynb) |
[CLI](https://github.com/souzatharsis/podcastfy/blob/59563ee105a0d1dbb46744e0ff084471670dd725/usage/cli.md) |
[Web App](https://openpod.fly.dev/) |
[Feedback](https://github.com/souzatharsis/podcastfy/issues)

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/souzatharsis/podcastfy/blob/main/podcastfy.ipynb)
[![PyPi Status](https://img.shields.io/pypi/v/podcastfy)](https://pypi.org/project/podcastfy/)
![PyPI Downloads](https://static.pepy.tech/badge/podcastfy)
[![Issues](https://img.shields.io/github/issues-raw/souzatharsis/podcastfy)](https://github.com/souzatharsis/podcastfy/issues)
[![Pytest](https://github.com/souzatharsis/podcastfy/actions/workflows/python-app.yml/badge.svg)](https://github.com/souzatharsis/podcastfy/actions/workflows/python-app.yml)
[![Docker](https://github.com/souzatharsis/podcastfy/actions/workflows/docker-publish.yml/badge.svg)](https://github.com/souzatharsis/podcastfy/actions/workflows/docker-publish.yml)
[![Documentation Status](https://readthedocs.org/projects/podcastfy/badge/?version=latest)](https://podcastfy.readthedocs.io/en/latest/?badge=latest)
[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
![GitHub Repo stars](https://img.shields.io/github/stars/souzatharsis/podcastfy)
&lt;/div&gt;

Podcastfy is an open-source Python package that transforms multi-modal content (text, images) into engaging, multi-lingual audio conversations using GenAI. Input content includes websites, PDFs, images, YouTube videos, as well as user provided topics.

Unlike closed-source UI-based tools focused primarily on research synthesis (e.g. NotebookLM ‚ù§Ô∏è), Podcastfy focuses on open source, programmatic and bespoke generation of engaging, conversational content from a multitude of multi-modal sources, enabling customization and scale.

## Testimonials üí¨

&gt; &quot;Love that you casually built an open source version of the most popular product Google built in the last decade&quot;

&gt; &quot;Loving this initiative and the best I have seen so far especially for a &#039;non-techie&#039; user.&quot;

&gt; &quot;Your library was very straightforward to work with. You did Amazing work brother üôè&quot;

&gt; &quot;I think it&#039;s awesome that you were inspired/recognize how hard it is to beat NotebookLM&#039;s quality, but you did an *incredible* job with this! It sounds incredible, and it&#039;s open-source! Thank you for being amazing!&quot;

[![Star History Chart](https://api.star-history.com/svg?repos=souzatharsis/podcastfy&amp;type=Date&amp;theme=dark)](https://api.star-history.com/svg?repos=souzatharsis/podcastfy&amp;type=Date&amp;theme=dark)

## Audio Examples üîä
This sample collection was generated using this [Python Notebook](usage/examples.ipynb).

### Images
Sample 1: Senecio, 1922 (Paul Klee) and Connection of Civilizations (2017) by Gheorghe Virtosu
***
&lt;img src=&quot;data/images/Senecio.jpeg&quot; alt=&quot;Senecio, 1922 (Paul Klee)&quot; width=&quot;20%&quot; height=&quot;auto&quot;&gt; &lt;img src=&quot;data/images/connection.jpg&quot; alt=&quot;Connection of Civilizations (2017) by Gheorghe Virtosu &quot; width=&quot;21.5%&quot; height=&quot;auto&quot;&gt;
&lt;video src=&quot;https://github.com/user-attachments/assets/a4134a0d-138c-4ab4-bc70-0f53b3507e6b&quot;&gt;&lt;/video&gt;  
***
Sample 2: The Great Wave off Kanagawa, 1831 (Hokusai) and Takiyasha the Witch and the Skeleton Spectre, c. 1844 (Kuniyoshi)
***
 &lt;img src=&quot;data/images/japan_1.jpg&quot; alt=&quot;The Great Wave off Kanagawa, 1831 (Hokusai)&quot; width=&quot;20%&quot; height=&quot;auto&quot;&gt; &lt;img src=&quot;data/images/japan2.jpg&quot; alt=&quot;Takiyasha the Witch and the Skeleton Spectre, c. 1844 (Kuniyoshi)&quot; width=&quot;21.5%&quot; height=&quot;auto&quot;&gt; 
&lt;video src=&quot;https://github.com/user-attachments/assets/f6aaaeeb-39d2-4dde-afaf-e2cd212e9fed&quot;&gt;&lt;/video&gt;  
***
Sample 3: Pop culture icon Taylor Swift and Mona Lisa, 1503 (Leonardo da Vinci)
***
&lt;img src=&quot;data/images/taylor.png&quot; alt=&quot;Taylor Swift&quot; width=&quot;28%&quot; height=&quot;auto&quot;&gt; &lt;img src=&quot;data/images/monalisa.jpeg&quot; alt=&quot;Mona Lisa&quot; width=&quot;10.5%&quot; height=&quot;auto&quot;&gt;
&lt;video src=&quot;https://github.com/user-attachments/assets/3b6f7075-159b-4540-946f-3f3907dffbca&quot;&gt;&lt;/video&gt; 


### Text
| Audio | Description  | Source |
|-------|--|--------|
| &lt;video src=&quot;https://github.com/user-attachments/assets/ef41a207-a204-4b60-a11e-06d66a0fbf06&quot;&gt;&lt;/video&gt;  | Personal Website | [Website](https://www.souzatharsis.com) |
| [Audio](https://soundcloud.com/high-lander123/amodei?in=high-lander123/sets/podcastfy-sample-audio-longform&amp;si=b8dfaf4e3ddc4651835e277500384156) (`longform=True`) | Lex Fridman Podcast: 5h interview with Dario Amodei Anthropic&#039;s CEO |  [Youtube](https://www.youtube.com/watch?v=ugvHCXCOmm4) |
| [Audio](https://soundcloud.com/high-lander123/benjamin?in=high-lander123/sets/podcastfy-sample-audio-longform&amp;si=dca7e2eec1c94252be18b8794499959a&amp;utm_source=clipboard&amp;utm_medium=text&amp;utm_campaign=social_sharing) (`longform=True`)| Benjamin Franklin&#039;s Autobiography | [Book](https://www.gutenberg.org/cache/epub/148/pg148.txt) |

### Multi-Lingual Text
| Language | Content Type | Description | Audio | Source |
|----------|--------------|-------------|-------|--------|
| French | Website | Agroclimate research information | [Audio](https://audio.com/thatupiso/audio/podcast-fr-agro) | [Website](https://agroclim.inrae.fr/) |
| Portuguese-BR | News Article | Election polls in S√£o Paulo | [Audio](https://audio.com/thatupiso/audio/podcast-thatupiso-br) | [Website](https://noticias.uol.com.br/eleicoes/2024/10/03/nova-pesquisa-datafolha-quem-subiu-e-quem-caiu-na-disputa-de-sp-03-10.htm) |


## Quickstart üíª

### Prerequisites
- Python 3.11 or higher
- `$ pip install ffmpeg` (for audio processing)

### Setup
1. Install from PyPI
  `$ pip install podcastfy`

2. Set up your [API keys](usage/config.md)

### Python
```python
from podcastfy.client import generate_podcast

audio_file = generate_podcast(urls=[&quot;&lt;url1&gt;&quot;, &quot;&lt;url2&gt;&quot;])
```
### CLI
```
python -m podcastfy.client --url &lt;url1&gt; --url &lt;url2&gt;
```

### Fastapi (Beta for urls)
```
Containerize podcastify and launch the api
Dockerfile_api

Make requests to the api look at the notebook for a clear example
fetch_audio(request_data, ENDPOINT, BASE_URL)
```
  
## Usage üíª

- [Python Package Quickstart](podcastfy.ipynb)

- [How to](usage/how-to.md)

- [Python Package Reference Manual](https://podcastfy.readthedocs.io/en/latest/podcastfy.html)

- [CLI](usage/cli.md)

## Customization üîß

Podcastfy offers a range of customization options to tailor your AI-generated podcasts:
- Customize podcast [conversation](usage/conversation_custom.md) (e.g. format, style, voices)
- Choose to run [Local LLMs](usage/local_llm.md) (156+ HuggingFace models)
- Set other [Configuration Settings](usage/config.md)

## Features ‚ú®

- Generate conversational content from multiple sources and formats (images, text, websites, YouTube, and PDFs).
- Generate shorts (2-5 minutes) or longform (30+ minutes) podcasts.
- Customize transcript and audio generation (e.g., style, language, structure).
- Generate transcripts using 100+ LLM models (OpenAI, Anthropic, Google etc).
- Leverage local LLMs for transcript generation for increased privacy and control.
- Integrate with advanced text-to-speech models (OpenAI, Google, ElevenLabs, and Microsoft Edge).
- Provide multi-language support for global content creation.
- Integrate seamlessly with CLI and Python packages for automated workflows.

## Built with Podcastfy üöÄ

- [OpenNotebook](https://www.open-notebook.ai/)
- [SurfSense](https://www.surfsense.net/)
- [OpenPod](https://openpod.fly.dev/)
- [Podcast-llm](https://github.com/evandempsey/podcast-llm)
- [Podcastfy-HuggingFace App](https://huggingface.co/spaces/thatupiso/Podcastfy.ai_demo)


## Updates üöÄüöÄ

### v0.4.0+ release
- Released new Multi-Speaker TTS model (is it the one NotebookLM uses?!?)
- Generate short or longform podcasts
- Generate podcasts from input topic using grounded real-time web search
- Integrate with 100+ LLM models (OpenAI, Anthropic, Google etc) for transcript generation

See [CHANGELOG](CHANGELOG.md) for more details.


## License

This software is licensed under [Apache 2.0](LICENSE). See [instructions](usage/license-guide.md) if you would like to use podcastfy in your software.

## Contributing ü§ù

We welcome contributions! See [Guidelines](GUIDELINES.md) for more details.

## Example Use Cases üéßüé∂

- **Content Creators** can use `Podcastfy` to convert blog posts, articles, or multimedia content into podcast-style audio, enabling them to reach broader audiences. By transforming content into an audio format, creators can cater to users who prefer listening over reading.

- **Educators** can transform lecture notes, presentations, and visual materials into audio conversations, making educational content more accessible to students with different learning preferences. This is particularly beneficial for students with visual impairments or those who have difficulty processing written information.

- **Researchers** can convert research papers, visual data, and technical content into conversational audio. This makes it easier for a wider audience, including those with disabilities, to consume and understand complex scientific information. Researchers can also create audio summaries of their work to enhance accessibility.

- **Accessibility Advocates** can use `Podcastfy` to promote digital accessibility by providing a tool that converts multimodal content into auditory formats. This helps individuals with visual impairments, dyslexia, or other disabilities that make it challenging to consume written or visual content.
  
## Contributors

&lt;a href=&quot;https://github.com/souzatharsis/podcastfy/graphs/contributors&quot;&gt;
  &lt;img alt=&quot;contributors&quot; src=&quot;https://contrib.rocks/image?repo=souzatharsis/podcastfy&quot;/&gt;
&lt;/a&gt;

&lt;p align=&quot;right&quot; style=&quot;font-size: 14px; color: #555; margin-top: 20px;&quot;&gt;
    &lt;a href=&quot;#readme-top&quot; style=&quot;text-decoration: none; color: #007bff; font-weight: bold;&quot;&gt;
        ‚Üë Back to Top ‚Üë
    &lt;/a&gt;
&lt;/p&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[freqtrade/freqtrade]]></title>
            <link>https://github.com/freqtrade/freqtrade</link>
            <guid>https://github.com/freqtrade/freqtrade</guid>
            <pubDate>Thu, 07 Aug 2025 00:04:48 GMT</pubDate>
            <description><![CDATA[Free, open source crypto trading bot]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/freqtrade/freqtrade">freqtrade/freqtrade</a></h1>
            <p>Free, open source crypto trading bot</p>
            <p>Language: Python</p>
            <p>Stars: 41,201</p>
            <p>Forks: 8,349</p>
            <p>Stars today: 124 stars today</p>
            <h2>README</h2><pre># ![freqtrade](https://raw.githubusercontent.com/freqtrade/freqtrade/develop/docs/assets/freqtrade_poweredby.svg)

[![Freqtrade CI](https://github.com/freqtrade/freqtrade/actions/workflows/ci.yml/badge.svg?branch=develop)](https://github.com/freqtrade/freqtrade/actions/)
[![DOI](https://joss.theoj.org/papers/10.21105/joss.04864/status.svg)](https://doi.org/10.21105/joss.04864)
[![Coverage Status](https://coveralls.io/repos/github/freqtrade/freqtrade/badge.svg?branch=develop&amp;service=github)](https://coveralls.io/github/freqtrade/freqtrade?branch=develop)
[![Documentation](https://readthedocs.org/projects/freqtrade/badge/)](https://www.freqtrade.io)
[![Maintainability](https://api.codeclimate.com/v1/badges/5737e6d668200b7518ff/maintainability)](https://codeclimate.com/github/freqtrade/freqtrade/maintainability)

Freqtrade is a free and open source crypto trading bot written in Python. It is designed to support all major exchanges and be controlled via Telegram or webUI. It contains backtesting, plotting and money management tools as well as strategy optimization by machine learning.

![freqtrade](https://raw.githubusercontent.com/freqtrade/freqtrade/develop/docs/assets/freqtrade-screenshot.png)

## Disclaimer

This software is for educational purposes only. Do not risk money which
you are afraid to lose. USE THE SOFTWARE AT YOUR OWN RISK. THE AUTHORS
AND ALL AFFILIATES ASSUME NO RESPONSIBILITY FOR YOUR TRADING RESULTS.

Always start by running a trading bot in Dry-run and do not engage money
before you understand how it works and what profit/loss you should
expect.

We strongly recommend you to have coding and Python knowledge. Do not
hesitate to read the source code and understand the mechanism of this bot.

## Supported Exchange marketplaces

Please read the [exchange specific notes](docs/exchanges.md) to learn about eventual, special configurations needed for each exchange.

- [X] [Binance](https://www.binance.com/)
- [X] [Bitmart](https://bitmart.com/)
- [X] [BingX](https://bingx.com/invite/0EM9RX)
- [X] [Bybit](https://bybit.com/)
- [X] [Gate.io](https://www.gate.io/ref/6266643)
- [X] [HTX](https://www.htx.com/)
- [X] [Hyperliquid](https://hyperliquid.xyz/) (A decentralized exchange, or DEX)
- [X] [Kraken](https://kraken.com/)
- [X] [OKX](https://okx.com/)
- [X] [MyOKX](https://okx.com/) (OKX EEA)
- [ ] [potentially many others](https://github.com/ccxt/ccxt/). _(We cannot guarantee they will work)_

### Supported Futures Exchanges (experimental)

- [X] [Binance](https://www.binance.com/)
- [X] [Gate.io](https://www.gate.io/ref/6266643)
- [X] [Hyperliquid](https://hyperliquid.xyz/) (A decentralized exchange, or DEX)
- [X] [OKX](https://okx.com/)
- [X] [Bybit](https://bybit.com/)

Please make sure to read the [exchange specific notes](docs/exchanges.md), as well as the [trading with leverage](docs/leverage.md) documentation before diving in.

### Community tested

Exchanges confirmed working by the community:

- [X] [Bitvavo](https://bitvavo.com/)
- [X] [Kucoin](https://www.kucoin.com/)

## Documentation

We invite you to read the bot documentation to ensure you understand how the bot is working.

Please find the complete documentation on the [freqtrade website](https://www.freqtrade.io).

## Features

- [x] **Based on Python 3.11+**: For botting on any operating system - Windows, macOS and Linux.
- [x] **Persistence**: Persistence is achieved through sqlite.
- [x] **Dry-run**: Run the bot without paying money.
- [x] **Backtesting**: Run a simulation of your buy/sell strategy.
- [x] **Strategy Optimization by machine learning**: Use machine learning to optimize your buy/sell strategy parameters with real exchange data.
- [X] **Adaptive prediction modeling**: Build a smart strategy with FreqAI that self-trains to the market via adaptive machine learning methods. [Learn more](https://www.freqtrade.io/en/stable/freqai/)
- [x] **Whitelist crypto-currencies**: Select which crypto-currency you want to trade or use dynamic whitelists.
- [x] **Blacklist crypto-currencies**: Select which crypto-currency you want to avoid.
- [x] **Builtin WebUI**: Builtin web UI to manage your bot.
- [x] **Manageable via Telegram**: Manage the bot with Telegram.
- [x] **Display profit/loss in fiat**: Display your profit/loss in fiat currency.
- [x] **Performance status report**: Provide a performance status of your current trades.

## Quick start

Please refer to the [Docker Quickstart documentation](https://www.freqtrade.io/en/stable/docker_quickstart/) on how to get started quickly.

For further (native) installation methods, please refer to the [Installation documentation page](https://www.freqtrade.io/en/stable/installation/).

## Basic Usage

### Bot commands

```
usage: freqtrade [-h] [-V]
                 {trade,create-userdir,new-config,show-config,new-strategy,download-data,convert-data,convert-trade-data,trades-to-ohlcv,list-data,backtesting,backtesting-show,backtesting-analysis,edge,hyperopt,hyperopt-list,hyperopt-show,list-exchanges,list-markets,list-pairs,list-strategies,list-hyperoptloss,list-freqaimodels,list-timeframes,show-trades,test-pairlist,convert-db,install-ui,plot-dataframe,plot-profit,webserver,strategy-updater,lookahead-analysis,recursive-analysis}
                 ...

Free, open source crypto trading bot

positional arguments:
  {trade,create-userdir,new-config,show-config,new-strategy,download-data,convert-data,convert-trade-data,trades-to-ohlcv,list-data,backtesting,backtesting-show,backtesting-analysis,edge,hyperopt,hyperopt-list,hyperopt-show,list-exchanges,list-markets,list-pairs,list-strategies,list-hyperoptloss,list-freqaimodels,list-timeframes,show-trades,test-pairlist,convert-db,install-ui,plot-dataframe,plot-profit,webserver,strategy-updater,lookahead-analysis,recursive-analysis}
    trade               Trade module.
    create-userdir      Create user-data directory.
    new-config          Create new config
    show-config         Show resolved config
    new-strategy        Create new strategy
    download-data       Download backtesting data.
    convert-data        Convert candle (OHLCV) data from one format to
                        another.
    convert-trade-data  Convert trade data from one format to another.
    trades-to-ohlcv     Convert trade data to OHLCV data.
    list-data           List downloaded data.
    backtesting         Backtesting module.
    backtesting-show    Show past Backtest results
    backtesting-analysis
                        Backtest Analysis module.
    hyperopt            Hyperopt module.
    hyperopt-list       List Hyperopt results
    hyperopt-show       Show details of Hyperopt results
    list-exchanges      Print available exchanges.
    list-markets        Print markets on exchange.
    list-pairs          Print pairs on exchange.
    list-strategies     Print available strategies.
    list-hyperoptloss   Print available hyperopt loss functions.
    list-freqaimodels   Print available freqAI models.
    list-timeframes     Print available timeframes for the exchange.
    show-trades         Show trades.
    test-pairlist       Test your pairlist configuration.
    convert-db          Migrate database to different system
    install-ui          Install FreqUI
    plot-dataframe      Plot candles with indicators.
    plot-profit         Generate plot showing profits.
    webserver           Webserver module.
    strategy-updater    updates outdated strategy files to the current version
    lookahead-analysis  Check for potential look ahead bias.
    recursive-analysis  Check for potential recursive formula issue.

options:
  -h, --help            show this help message and exit
  -V, --version         show program&#039;s version number and exit
```

### Telegram RPC commands

Telegram is not mandatory. However, this is a great way to control your bot. More details and the full command list on the [documentation](https://www.freqtrade.io/en/latest/telegram-usage/)

- `/start`: Starts the trader.
- `/stop`: Stops the trader.
- `/stopentry`: Stop entering new trades.
- `/status &lt;trade_id&gt;|[table]`: Lists all or specific open trades.
- `/profit [&lt;n&gt;]`: Lists cumulative profit from all finished trades, over the last n days.
- `/profit_long [&lt;n&gt;]`: Lists cumulative profit from all finished long trades, over the last n days.
- `/profit_short [&lt;n&gt;]`: Lists cumulative profit from all finished short trades, over the last n days.
- `/forceexit &lt;trade_id&gt;|all`: Instantly exits the given trade (Ignoring `minimum_roi`).
- `/fx &lt;trade_id&gt;|all`: Alias to `/forceexit`
- `/performance`: Show performance of each finished trade grouped by pair
- `/balance`: Show account balance per currency.
- `/daily &lt;n&gt;`: Shows profit or loss per day, over the last n days.
- `/help`: Show help message.
- `/version`: Show version.


## Development branches

The project is currently setup in two main branches:

- `develop` - This branch has often new features, but might also contain breaking changes. We try hard to keep this branch as stable as possible.
- `stable` - This branch contains the latest stable release. This branch is generally well tested.
- `feat/*` - These are feature branches, which are being worked on heavily. Please don&#039;t use these unless you want to test a specific feature.

## Support

### Help / Discord

For any questions not covered by the documentation or for further information about the bot, or to simply engage with like-minded individuals, we encourage you to join the Freqtrade [discord server](https://discord.gg/p7nuUNVfP7).

### [Bugs / Issues](https://github.com/freqtrade/freqtrade/issues?q=is%3Aissue)

If you discover a bug in the bot, please
[search the issue tracker](https://github.com/freqtrade/freqtrade/issues?q=is%3Aissue)
first. If it hasn&#039;t been reported, please
[create a new issue](https://github.com/freqtrade/freqtrade/issues/new/choose) and
ensure you follow the template guide so that the team can assist you as
quickly as possible.

For every [issue](https://github.com/freqtrade/freqtrade/issues/new/choose) created, kindly follow up and mark satisfaction or reminder to close issue when equilibrium ground is reached.

--Maintain github&#039;s [community policy](https://docs.github.com/en/site-policy/github-terms/github-community-code-of-conduct)--

### [Feature Requests](https://github.com/freqtrade/freqtrade/labels/enhancement)

Have you a great idea to improve the bot you want to share? Please,
first search if this feature was not [already discussed](https://github.com/freqtrade/freqtrade/labels/enhancement).
If it hasn&#039;t been requested, please
[create a new request](https://github.com/freqtrade/freqtrade/issues/new/choose)
and ensure you follow the template guide so that it does not get lost
in the bug reports.

### [Pull Requests](https://github.com/freqtrade/freqtrade/pulls)

Feel like the bot is missing a feature? We welcome your pull requests!

Please read the
[Contributing document](https://github.com/freqtrade/freqtrade/blob/develop/CONTRIBUTING.md)
to understand the requirements before sending your pull-requests.

Coding is not a necessity to contribute - maybe start with improving the documentation?
Issues labeled [good first issue](https://github.com/freqtrade/freqtrade/labels/good%20first%20issue) can be good first contributions, and will help get you familiar with the codebase.

**Note** before starting any major new feature work, *please open an issue describing what you are planning to do* or talk to us on [discord](https://discord.gg/p7nuUNVfP7) (please use the #dev channel for this). This will ensure that interested parties can give valuable feedback on the feature, and let others know that you are working on it.

**Important:** Always create your PR against the `develop` branch, not `stable`.

## Requirements

### Up-to-date clock

The clock must be accurate, synchronized to a NTP server very frequently to avoid problems with communication to the exchanges.

### Minimum hardware required

To run this bot we recommend you a cloud instance with a minimum of:

- Minimal (advised) system requirements: 2GB RAM, 1GB disk space, 2vCPU

### Software requirements

- [Python &gt;= 3.11](http://docs.python-guide.org/en/latest/starting/installation/)
- [pip](https://pip.pypa.io/en/stable/installing/)
- [git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)
- [TA-Lib](https://ta-lib.github.io/ta-lib-python/)
- [virtualenv](https://virtualenv.pypa.io/en/stable/installation.html) (Recommended)
- [Docker](https://www.docker.com/products/docker) (Recommended)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[JudgmentLabs/judgeval]]></title>
            <link>https://github.com/JudgmentLabs/judgeval</link>
            <guid>https://github.com/JudgmentLabs/judgeval</guid>
            <pubDate>Thu, 07 Aug 2025 00:04:47 GMT</pubDate>
            <description><![CDATA[The open source post-building layer for agents. Our traces + evals power agent post-training (RL, SFT), monitoring, and regression testing.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/JudgmentLabs/judgeval">JudgmentLabs/judgeval</a></h1>
            <p>The open source post-building layer for agents. Our traces + evals power agent post-training (RL, SFT), monitoring, and regression testing.</p>
            <p>Language: Python</p>
            <p>Stars: 818</p>
            <p>Forks: 79</p>
            <p>Stars today: 69 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;assets/new_lightmode.svg#gh-light-mode-only&quot; alt=&quot;Judgment Logo&quot; width=&quot;400&quot; /&gt;
&lt;img src=&quot;assets/new_darkmode.svg#gh-dark-mode-only&quot; alt=&quot;Judgment Logo&quot; width=&quot;400&quot; /&gt;

&lt;br&gt;
&lt;div style=&quot;font-size: 1.5em;&quot;&gt;
    Enable self-learning agents with traces, evals, and environment data.
&lt;/div&gt;

## [Docs](https://docs.judgmentlabs.ai/)  ‚Ä¢  [Judgment Cloud](https://app.judgmentlabs.ai/register)  ‚Ä¢ [Self-Host](https://docs.judgmentlabs.ai/documentation/self-hosting/get-started)  ‚Ä¢ [Landing Page](https://judgmentlabs.ai/)

 [Demo](https://www.youtube.com/watch?v=1S4LixpVbcc) ‚Ä¢ [Bug Reports](https://github.com/JudgmentLabs/judgeval/issues) ‚Ä¢ [Changelog](https://docs.judgmentlabs.ai/changelog/2025-04-21)

We&#039;re hiring! Join us in our mission to enable self-learning agents by providing the data and signals needed for monitoring and post-training.

[![X](https://img.shields.io/badge/-X/Twitter-000?logo=x&amp;logoColor=white)](https://x.com/JudgmentLabs)
[![LinkedIn](https://custom-icon-badges.demolab.com/badge/LinkedIn%20-0A66C2?logo=linkedin-white&amp;logoColor=fff)](https://www.linkedin.com/company/judgmentlabs)
[![Discord](https://img.shields.io/badge/-Discord-5865F2?logo=discord&amp;logoColor=white)](https://discord.gg/tGVFf8UBUY)

&lt;img src=&quot;assets/product_shot.png&quot; alt=&quot;Judgment Platform&quot; width=&quot;800&quot; /&gt;

&lt;/div&gt;

Judgeval offers **open-source tooling** for tracing and evaluating autonomous, stateful agents. It **provides runtime data from agent-environment interactions** for continuous learning and self-improvement.

## üé¨ See Judgeval in Action

**[Multi-Agent System](https://github.com/JudgmentLabs/judgment-cookbook/tree/main/cookbooks/agents/multi-agent) with complete observability:** (1) A multi-agent system spawns agents to research topics on the internet. (2) With just **3 lines of code**, Judgeval traces every input/output + environment response across all agent tool calls for debugging. (3) After completion, (4) export all interaction data to enable further environment-specific learning and optimization.

&lt;table style=&quot;width: 100%; max-width: 800px; table-layout: fixed;&quot;&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot; style=&quot;padding: 8px; width: 50%;&quot;&gt;
  &lt;img src=&quot;assets/agent.gif&quot; alt=&quot;Agent Demo&quot; style=&quot;width: 100%; max-width: 350px; height: auto;&quot; /&gt;
  &lt;br&gt;&lt;strong&gt;ü§ñ Agents Running&lt;/strong&gt;
&lt;/td&gt;
&lt;td align=&quot;center&quot; style=&quot;padding: 8px; width: 50%;&quot;&gt;
  &lt;img src=&quot;assets/trace.gif&quot; alt=&quot;Trace Demo&quot; style=&quot;width: 100%; max-width: 350px; height: auto;&quot; /&gt;
  &lt;br&gt;&lt;strong&gt;üìä Real-time Tracing&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot; style=&quot;padding: 8px; width: 50%;&quot;&gt;
  &lt;img src=&quot;assets/document.gif&quot; alt=&quot;Agent Completed Demo&quot; style=&quot;width: 100%; max-width: 350px; height: auto;&quot; /&gt;
  &lt;br&gt;&lt;strong&gt;‚úÖ Agents Completed Running&lt;/strong&gt;
&lt;/td&gt;
&lt;td align=&quot;center&quot; style=&quot;padding: 8px; width: 50%;&quot;&gt;
  &lt;img src=&quot;assets/data.gif&quot; alt=&quot;Data Export Demo&quot; style=&quot;width: 100%; max-width: 350px; height: auto;&quot; /&gt;
  &lt;br&gt;&lt;strong&gt;üì§ Exporting Agent Environment Data&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;

&lt;/table&gt;

## üìã Table of Contents
- [üõ†Ô∏è Installation](#Ô∏è-installation)
- [üèÅ Quickstarts](#-quickstarts)
- [‚ú® Features](#-features)
- [üè¢ Self-Hosting](#-self-hosting)
- [üìö Cookbooks](#-cookbooks)
- [üíª Development with Cursor](#-development-with-cursor)

## üõ†Ô∏è Installation

Get started with Judgeval by installing our SDK using pip:

```bash
pip install judgeval
```

Ensure you have your `JUDGMENT_API_KEY` and `JUDGMENT_ORG_ID` environment variables set to connect to the [Judgment Platform](https://app.judgmentlabs.ai/).

```bash
export JUDGMENT_API_KEY=...
export JUDGMENT_ORG_ID=...
```

**If you don&#039;t have keys, [create an account](https://app.judgmentlabs.ai/register) on the platform!**

## üèÅ Quickstarts

### üõ∞Ô∏è Tracing

Create a file named `agent.py` with the following code:

```python
from judgeval.tracer import Tracer, wrap
from openai import OpenAI

client = wrap(OpenAI())  # tracks all LLM calls
judgment = Tracer(project_name=&quot;my_project&quot;)

@judgment.observe(span_type=&quot;tool&quot;)
def format_question(question: str) -&gt; str:
    # dummy tool
    return f&quot;Question : {question}&quot;

@judgment.observe(span_type=&quot;function&quot;)
def run_agent(prompt: str) -&gt; str:
    task = format_question(prompt)
    response = client.chat.completions.create(
        model=&quot;gpt-4.1&quot;,
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: task}]
    )
    return response.choices[0].message.content
    
run_agent(&quot;What is the capital of the United States?&quot;)
```
You&#039;ll see your trace exported to the Judgment Platform:

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;assets/online_eval.png&quot; alt=&quot;Judgment Platform Trace Example&quot; width=&quot;1500&quot; /&gt;&lt;/p&gt;


[Click here](https://docs.judgmentlabs.ai/documentation/tracing/introduction) for a more detailed explanation.


&lt;!-- Created by https://github.com/ekalinin/github-markdown-toc --&gt;


## ‚ú® Features

|  |  |
|:---|:---:|
| &lt;h3&gt;üîç Tracing&lt;/h3&gt;Automatic agent tracing integrated with common frameworks (LangGraph, OpenAI, Anthropic). **Tracks inputs/outputs, agent tool calls, latency, cost, and custom metadata** at every step.&lt;br&gt;&lt;br&gt;**Useful for:**&lt;br&gt;‚Ä¢ üêõ Debugging agent runs &lt;br&gt;‚Ä¢ üìã Collecting agent environment data &lt;br&gt;‚Ä¢ üî¨ Pinpointing performance bottlenecks| &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;assets/agent_trace_example.png&quot; alt=&quot;Tracing visualization&quot; width=&quot;1200&quot;/&gt;&lt;/p&gt; |
| &lt;h3&gt;üß™ Evals&lt;/h3&gt;Build custom evaluators on top of your agents. Judgeval supports LLM-as-a-judge, manual labeling, and code-based evaluators that connect with our metric-tracking infrastructure. &lt;br&gt;&lt;br&gt;**Useful for:**&lt;br&gt;‚Ä¢ ‚ö†Ô∏è Unit-testing &lt;br&gt;‚Ä¢ üî¨ A/B testing &lt;br&gt;‚Ä¢ üõ°Ô∏è Online guardrails | &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;assets/test.png&quot; alt=&quot;Evaluation metrics&quot; width=&quot;800&quot;/&gt;&lt;/p&gt; |
| &lt;h3&gt;üì° Monitoring&lt;/h3&gt;Get Slack alerts for agent failures in production. Add custom hooks to address production regressions.&lt;br&gt;&lt;br&gt; **Useful for:** &lt;br&gt;‚Ä¢ üìâ Identifying degradation early &lt;br&gt;‚Ä¢ üìà Visualizing performance trends across agent versions and time | &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;assets/errors.png&quot; alt=&quot;Monitoring Dashboard&quot; width=&quot;1200&quot;/&gt;&lt;/p&gt; |
| &lt;h3&gt;üìä Datasets&lt;/h3&gt;Export traces and test cases to datasets for scaled analysis and optimization. Move datasets to/from Parquet, S3, etc. &lt;br&gt;&lt;br&gt;Run evals on datasets as unit tests or to A/B test different agent configurations, enabling continuous learning from production interactions. &lt;br&gt;&lt;br&gt; **Useful for:**&lt;br&gt;‚Ä¢ üóÉÔ∏è Agent environment interaction data for optimization&lt;br&gt;‚Ä¢ üîÑ Scaled analysis for A/B tests | &lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;assets/datasets_preview_screenshot.png&quot; alt=&quot;Dataset management&quot; width=&quot;1200&quot;/&gt;&lt;/p&gt; |

## üè¢ Self-Hosting

Run Judgment on your own infrastructure: we provide comprehensive self-hosting capabilities that give you full control over the backend and data plane that Judgeval interfaces with.

### Key Features
* Deploy Judgment on your own AWS account
* Store data in your own Supabase instance
* Access Judgment through your own custom domain

### Getting Started
1. Check out our [self-hosting documentation](https://docs.judgmentlabs.ai/documentation/self-hosting/get-started) for detailed setup instructions, along with how your self-hosted instance can be accessed
2. Use the [Judgment CLI](https://docs.judgmentlabs.ai/documentation/developer-tools/judgment-cli/installation) to deploy your self-hosted environment
3. After your self-hosted instance is setup, make sure the `JUDGMENT_API_URL` environmental variable is set to your self-hosted backend endpoint

## üìö Cookbooks

Have your own? We&#039;re happy to feature it if you create a PR or message us on [Discord](https://discord.gg/tGVFf8UBUY).

You can access our repo of cookbooks [here](https://github.com/JudgmentLabs/judgment-cookbook).

## üíª Development with Cursor
Building agents and LLM workflows in Cursor works best when your coding assistant has the proper context about Judgment integration. The Cursor rules file contains the key information needed for your assistant to implement Judgment features effectively.

Refer to the official [documentation](https://docs.judgmentlabs.ai/documentation/developer-tools/cursor/cursor-rules) for access to the rules file and more information on integrating this rules file with your codebase.

## ‚≠ê Star Us on GitHub

If you find Judgeval useful, please consider giving us a star on GitHub! Your support helps us grow our community and continue improving the repository.

## ‚ù§Ô∏è Contributors

There are many ways to contribute to Judgeval:

- Submit [bug reports](https://github.com/JudgmentLabs/judgeval/issues) and [feature requests](https://github.com/JudgmentLabs/judgeval/issues)
- Review the documentation and submit [Pull Requests](https://github.com/JudgmentLabs/judgeval/pulls) to improve it
- Speaking or writing about Judgment and letting us know!

&lt;!-- Contributors collage --&gt;
[![Contributors](https://contributors-img.web.app/image?repo=JudgmentLabs/judgeval)](https://github.com/JudgmentLabs/judgeval/graphs/contributors)

---

Judgeval is created and maintained by [Judgment Labs](https://judgmentlabs.ai/).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[HKUDS/LightRAG]]></title>
            <link>https://github.com/HKUDS/LightRAG</link>
            <guid>https://github.com/HKUDS/LightRAG</guid>
            <pubDate>Thu, 07 Aug 2025 00:04:46 GMT</pubDate>
            <description><![CDATA["LightRAG: Simple and Fast Retrieval-Augmented Generation"]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/HKUDS/LightRAG">HKUDS/LightRAG</a></h1>
            <p>"LightRAG: Simple and Fast Retrieval-Augmented Generation"</p>
            <p>Language: Python</p>
            <p>Stars: 19,191</p>
            <p>Forks: 2,709</p>
            <p>Stars today: 56 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;div style=&quot;margin: 20px 0;&quot;&gt;
  &lt;img src=&quot;./assets/logo.png&quot; width=&quot;120&quot; height=&quot;120&quot; alt=&quot;LightRAG Logo&quot; style=&quot;border-radius: 20px; box-shadow: 0 8px 32px rgba(0, 217, 255, 0.3);&quot;&gt;
&lt;/div&gt;

# üöÄ LightRAG: Simple and Fast Retrieval-Augmented Generation

&lt;div align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://trendshift.io/repositories/13043&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13043&quot; alt=&quot;HKUDS%2FLightRAG | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;div style=&quot;width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;div style=&quot;background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; padding: 25px; text-align: center;&quot;&gt;
    &lt;p&gt;
      &lt;a href=&#039;https://github.com/HKUDS/LightRAG&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/üî•Project-Page-00d9ff?style=for-the-badge&amp;logo=github&amp;logoColor=white&amp;labelColor=1a1a2e&#039;&gt;&lt;/a&gt;
      &lt;a href=&#039;https://arxiv.org/abs/2410.05779&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/üìÑarXiv-2410.05779-ff6b6b?style=for-the-badge&amp;logo=arxiv&amp;logoColor=white&amp;labelColor=1a1a2e&#039;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://github.com/HKUDS/LightRAG/stargazers&quot;&gt;&lt;img src=&#039;https://img.shields.io/github/stars/HKUDS/LightRAG?color=00d9ff&amp;style=for-the-badge&amp;logo=star&amp;logoColor=white&amp;labelColor=1a1a2e&#039; /&gt;&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;img src=&quot;https://img.shields.io/badge/üêçPython-3.10-4ecdc4?style=for-the-badge&amp;logo=python&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;
      &lt;a href=&quot;https://pypi.org/project/lightrag-hku/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/lightrag-hku.svg?style=for-the-badge&amp;logo=pypi&amp;logoColor=white&amp;labelColor=1a1a2e&amp;color=ff6b6b&quot;&gt;&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://discord.gg/yF2MmDJyGJ&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/üí¨Discord-Community-7289da?style=for-the-badge&amp;logo=discord&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://github.com/HKUDS/LightRAG/issues/285&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/üí¨WeChat-Group-07c160?style=for-the-badge&amp;logo=wechat&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;a href=&quot;README-zh.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/üá®üá≥‰∏≠ÊñáÁâà-1a1a2e?style=for-the-badge&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;README.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/üá∫üá∏English-1a1a2e?style=for-the-badge&quot;&gt;&lt;/a&gt;
    &lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div align=&quot;center&quot; style=&quot;margin: 30px 0;&quot;&gt;
  &lt;img src=&quot;https://user-images.githubusercontent.com/74038190/212284100-561aa473-3905-4a80-b561-0d28506553ee.gif&quot; width=&quot;800&quot;&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot; style=&quot;margin: 30px 0;&quot;&gt;
    &lt;img src=&quot;./README.assets/b2aaf634151b4706892693ffb43d9093.png&quot; width=&quot;800&quot; alt=&quot;LightRAG Diagram&quot;&gt;
&lt;/div&gt;

---
## üéâ News
- [X] [2025.06.16]üéØüì¢Our team has released [RAG-Anything](https://github.com/HKUDS/RAG-Anything) an All-in-One Multimodal RAG System for seamless text, image, table, and equation processing.
- [X] [2025.06.05]üéØüì¢LightRAG now supports comprehensive multimodal data handling through [RAG-Anything](https://github.com/HKUDS/RAG-Anything) integration, enabling seamless document parsing and RAG capabilities across diverse formats including PDFs, images, Office documents, tables, and formulas. Please refer to the new [multimodal section](https://github.com/HKUDS/LightRAG/?tab=readme-ov-file#multimodal-document-processing-rag-anything-integration) for details.
- [X] [2025.03.18]üéØüì¢LightRAG now supports citation functionality, enabling proper source attribution.
- [X] [2025.02.05]üéØüì¢Our team has released [VideoRAG](https://github.com/HKUDS/VideoRAG) understanding extremely long-context videos.
- [X] [2025.01.13]üéØüì¢Our team has released [MiniRAG](https://github.com/HKUDS/MiniRAG) making RAG simpler with small models.
- [X] [2025.01.06]üéØüì¢You can now [use PostgreSQL for Storage](#using-postgresql-for-storage).
- [X] [2024.12.31]üéØüì¢LightRAG now supports [deletion by document ID](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#delete).
- [X] [2024.11.25]üéØüì¢LightRAG now supports seamless integration of [custom knowledge graphs](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#insert-custom-kg), empowering users to enhance the system with their own domain expertise.
- [X] [2024.11.19]üéØüì¢A comprehensive guide to LightRAG is now available on [LearnOpenCV](https://learnopencv.com/lightrag). Many thanks to the blog author.
- [X] [2024.11.11]üéØüì¢LightRAG now supports [deleting entities by their names](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#delete).
- [X] [2024.11.09]üéØüì¢Introducing the [LightRAG Gui](https://lightrag-gui.streamlit.app), which allows you to insert, query, visualize, and download LightRAG knowledge.
- [X] [2024.11.04]üéØüì¢You can now [use Neo4J for Storage](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#using-neo4j-for-storage).
- [X] [2024.10.29]üéØüì¢LightRAG now supports multiple file types, including PDF, DOC, PPT, and CSV via `textract`.
- [X] [2024.10.20]üéØüì¢We&#039;ve added a new feature to LightRAG: Graph Visualization.
- [X] [2024.10.18]üéØüì¢We&#039;ve added a link to a [LightRAG Introduction Video](https://youtu.be/oageL-1I0GE). Thanks to the author!
- [X] [2024.10.17]üéØüì¢We have created a [Discord channel](https://discord.gg/yF2MmDJyGJ)! Welcome to join for sharing and discussions! üéâüéâ
- [X] [2024.10.16]üéØüì¢LightRAG now supports [Ollama models](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#quick-start)!
- [X] [2024.10.15]üéØüì¢LightRAG now supports [Hugging Face models](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#quick-start)!

&lt;details&gt;
  &lt;summary style=&quot;font-size: 1.4em; font-weight: bold; cursor: pointer; display: list-item;&quot;&gt;
    Algorithm Flowchart
  &lt;/summary&gt;

![LightRAG Indexing Flowchart](https://learnopencv.com/wp-content/uploads/2024/11/LightRAG-VectorDB-Json-KV-Store-Indexing-Flowchart-scaled.jpg)
*Figure 1: LightRAG Indexing Flowchart - Img Caption : [Source](https://learnopencv.com/lightrag/)*
![LightRAG Retrieval and Querying Flowchart](https://learnopencv.com/wp-content/uploads/2024/11/LightRAG-Querying-Flowchart-Dual-Level-Retrieval-Generation-Knowledge-Graphs-scaled.jpg)
*Figure 2: LightRAG Retrieval and Querying Flowchart - Img Caption : [Source](https://learnopencv.com/lightrag/)*

&lt;/details&gt;

## Installation

### Install LightRAG Server

The LightRAG Server is designed to provide Web UI and API support. The Web UI facilitates document indexing, knowledge graph exploration, and a simple RAG query interface. LightRAG Server also provide an Ollama compatible interfaces, aiming to emulate LightRAG as an Ollama chat model. This allows AI chat bot, such as Open WebUI, to access LightRAG easily.

* Install from PyPI

```bash
pip install &quot;lightrag-hku[api]&quot;
cp env.example .env
lightrag-server
```

* Installation from Source

```bash
git clone https://github.com/HKUDS/LightRAG.git
cd LightRAG
# create a Python virtual enviroment if neccesary
# Install in editable mode with API support
pip install -e &quot;.[api]&quot;
cp env.example .env
lightrag-server
```

* Launching the LightRAG Server with Docker Compose

```
git clone https://github.com/HKUDS/LightRAG.git
cd LightRAG
cp env.example .env
# modify LLM and Embedding settings in .env
docker compose up
```

&gt; Historical versions of LightRAG docker images can be found here: [LightRAG Docker Images]( https://github.com/HKUDS/LightRAG/pkgs/container/lightrag)

### Install  LightRAG Core

* Install from source (Recommend)

```bash
cd LightRAG
pip install -e .
```

* Install from PyPI

```bash
pip install lightrag-hku
```

## Quick Start

### LLM and Technology Stack Requirements for LightRAG

LightRAG&#039;s demands on the capabilities of Large Language Models (LLMs) are significantly higher than those of traditional RAG, as it requires the LLM to perform entity-relationship extraction tasks from documents. Configuring appropriate Embedding and Reranker models is also crucial for improving query performance.

- **LLM Selection**:
  - It is recommended to use an LLM with at least 32 billion parameters.
  - The context length should be at least 32KB, with 64KB being recommended.
- **Embedding Model**:
  - A high-performance Embedding model is essential for RAG.
  - We recommend using mainstream multilingual Embedding models, such as: `BAAI/bge-m3` and `text-embedding-3-large`.
  - **Important Note**: The Embedding model must be determined before document indexing, and the same model must be used during the document query phase.
- **Reranker Model Configuration**:
  - Configuring a Reranker model can significantly enhance LightRAG&#039;s retrieval performance.
  - When a Reranker model is enabled, it is recommended to set the &quot;mix mode&quot; as the default query mode.
  - We recommend using mainstream Reranker models, such as: `BAAI/bge-reranker-v2-m3` or models provided by services like Jina.

### Quick Start for LightRAG Server

* For more information about LightRAG Server, please refer to [LightRAG Server](./lightrag/api/README.md).

### Quick Start for LightRAG core

To get started with LightRAG core, refer to the sample codes available in the `examples` folder. Additionally, a [video demo](https://www.youtube.com/watch?v=g21royNJ4fw) demonstration is provided to guide you through the local setup process. If you already possess an OpenAI API key, you can run the demo right away:

```bash
### you should run the demo code with project folder
cd LightRAG
### provide your API-KEY for OpenAI
export OPENAI_API_KEY=&quot;sk-...your_opeai_key...&quot;
### download the demo document of &quot;A Christmas Carol&quot; by Charles Dickens
curl https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/tests/mock_data.txt &gt; ./book.txt
### run the demo code
python examples/lightrag_openai_demo.py
```

For a streaming response implementation example, please see `examples/lightrag_openai_compatible_demo.py`. Prior to execution, ensure you modify the sample code&#039;s LLM and embedding configurations accordingly.

**Note 1**: When running the demo program, please be aware that different test scripts may use different embedding models. If you switch to a different embedding model, you must clear the data directory (`./dickens`); otherwise, the program may encounter errors. If you wish to retain the LLM cache, you can preserve the `kv_store_llm_response_cache.json` file while clearing the data directory.

**Note 2**: Only `lightrag_openai_demo.py` and `lightrag_openai_compatible_demo.py` are officially supported sample codes. Other sample files are community contributions that haven&#039;t undergone full testing and optimization.

## Programing with LightRAG Core

&gt; If you would like to integrate LightRAG into your project, we recommend utilizing the REST API provided by the LightRAG Server. LightRAG Core is typically intended for embedded applications or for researchers who wish to conduct studies and evaluations.

### ‚ö†Ô∏è Important: Initialization Requirements

**LightRAG requires explicit initialization before use.** You must call both `await rag.initialize_storages()` and `await initialize_pipeline_status()` after creating a LightRAG instance, otherwise you will encounter errors like:
- `AttributeError: __aenter__` - if storages are not initialized
- `KeyError: &#039;history_messages&#039;` - if pipeline status is not initialized

### A Simple Program

Use the below Python snippet to initialize LightRAG, insert text to it, and perform queries:

```python
import os
import asyncio
from lightrag import LightRAG, QueryParam
from lightrag.llm.openai import gpt_4o_mini_complete, gpt_4o_complete, openai_embed
from lightrag.kg.shared_storage import initialize_pipeline_status
from lightrag.utils import setup_logger

setup_logger(&quot;lightrag&quot;, level=&quot;INFO&quot;)

WORKING_DIR = &quot;./rag_storage&quot;
if not os.path.exists(WORKING_DIR):
    os.mkdir(WORKING_DIR)

async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        embedding_func=openai_embed,
        llm_model_func=gpt_4o_mini_complete,
    )
    # IMPORTANT: Both initialization calls are required!
    await rag.initialize_storages()  # Initialize storage backends
    await initialize_pipeline_status()  # Initialize processing pipeline
    return rag

async def main():
    try:
        # Initialize RAG instance
        rag = await initialize_rag()
        await rag.ainsert(&quot;Your text&quot;)

        # Perform hybrid search
        mode = &quot;hybrid&quot;
        print(
          await rag.aquery(
              &quot;What are the top themes in this story?&quot;,
              param=QueryParam(mode=mode)
          )
        )

    except Exception as e:
        print(f&quot;An error occurred: {e}&quot;)
    finally:
        if rag:
            await rag.finalize_storages()

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

Important notes for the above snippet:

- Export your OPENAI_API_KEY environment variable before running the script.
- This program uses the default storage settings for LightRAG, so all data will be persisted to WORKING_DIR/rag_storage.
- This program demonstrates only the simplest way to initialize a LightRAG object: Injecting the embedding and LLM functions, and initializing storage and pipeline status after creating the LightRAG object.

### LightRAG init parameters

A full list of LightRAG init parameters:

&lt;details&gt;
&lt;summary&gt; Parameters &lt;/summary&gt;

| **Parameter** | **Type** | **Explanation** | **Default** |
|--------------|----------|-----------------|-------------|
| **working_dir** | `str` | Directory where the cache will be stored | `lightrag_cache+timestamp` |
| **workspace** | str | Workspace name for data isolation between different LightRAG Instances |  |
| **kv_storage** | `str` | Storage type for documents and text chunks. Supported types: `JsonKVStorage`,`PGKVStorage`,`RedisKVStorage`,`MongoKVStorage` | `JsonKVStorage` |
| **vector_storage** | `str` | Storage type for embedding vectors. Supported types: `NanoVectorDBStorage`,`PGVectorStorage`,`MilvusVectorDBStorage`,`ChromaVectorDBStorage`,`FaissVectorDBStorage`,`MongoVectorDBStorage`,`QdrantVectorDBStorage` | `NanoVectorDBStorage` |
| **graph_storage** | `str` | Storage type for graph edges and nodes. Supported types: `NetworkXStorage`,`Neo4JStorage`,`PGGraphStorage`,`AGEStorage` | `NetworkXStorage` |
| **doc_status_storage** | `str` | Storage type for documents process status. Supported types: `JsonDocStatusStorage`,`PGDocStatusStorage`,`MongoDocStatusStorage` | `JsonDocStatusStorage` |
| **chunk_token_size** | `int` | Maximum token size per chunk when splitting documents | `1200` |
| **chunk_overlap_token_size** | `int` | Overlap token size between two chunks when splitting documents | `100` |
| **tokenizer** | `Tokenizer` | The function used to convert text into tokens (numbers) and back using .encode() and .decode() functions following `TokenizerInterface` protocol. If you don&#039;t specify one, it will use the default Tiktoken tokenizer. | `TiktokenTokenizer` |
| **tiktoken_model_name** | `str` | If you&#039;re using the default Tiktoken tokenizer, this is the name of the specific Tiktoken model to use. This setting is ignored if you provide your own tokenizer. | `gpt-4o-mini` |
| **entity_extract_max_gleaning** | `int` | Number of loops in the entity extraction process, appending history messages | `1` |
| **node_embedding_algorithm** | `str` | Algorithm for node embedding (currently not used) | `node2vec` |
| **node2vec_params** | `dict` | Parameters for node embedding | `{&quot;dimensions&quot;: 1536,&quot;num_walks&quot;: 10,&quot;walk_length&quot;: 40,&quot;window_size&quot;: 2,&quot;iterations&quot;: 3,&quot;random_seed&quot;: 3,}` |
| **embedding_func** | `EmbeddingFunc` | Function to generate embedding vectors from text | `openai_embed` |
| **embedding_batch_num** | `int` | Maximum batch size for embedding processes (multiple texts sent per batch) | `32` |
| **embedding_func_max_async** | `int` | Maximum number of concurrent asynchronous embedding processes | `16` |
| **llm_model_func** | `callable` | Function for LLM generation | `gpt_4o_mini_complete` |
| **llm_model_name** | `str` | LLM model name for generation | `meta-llama/Llama-3.2-1B-Instruct` |
| **summary_max_tokens** | `int` | Maximum tokens send to LLM to generate entity relation summaries | `32000`Ôºàdefault value changed by env var MAX_TOKENS) |
| **llm_model_max_async** | `int` | Maximum number of concurrent asynchronous LLM processes | `4`Ôºàdefault value changed by env var MAX_ASYNC) |
| **llm_model_kwargs** | `dict` | Additional parameters for LLM generation | |
| **vector_db_storage_cls_kwargs** | `dict` | Additional parameters for vector database, like setting the threshold for nodes and relations retrieval | cosine_better_than_threshold: 0.2Ôºàdefault value changed by env var COSINE_THRESHOLD) |
| **enable_llm_cache** | `bool` | If `TRUE`, stores LLM results in cache; repeated prompts return cached responses | `TRUE` |
| **enable_llm_cache_for_entity_extract** | `bool` | If `TRUE`, stores LLM results in cache for entity extraction; Good for beginners to debug your application | `TRUE` |
| **addon_params** | `dict` | Additional parameters, e.g., `{&quot;example_number&quot;: 1, &quot;language&quot;: &quot;Simplified Chinese&quot;, &quot;entity_types&quot;: [&quot;organization&quot;, &quot;person&quot;, &quot;geo&quot;, &quot;event&quot;]}`: sets example limit, entiy/relation extraction output language | `example_number: all examples, language: English` |
| **embedding_cache_config** | `dict` | Configuration for question-answer caching. Contains three parameters: `enabled`: Boolean value to enable/disable cache lookup functionality. When enabled, the system will check cached responses before generating new answers. `similarity_threshold`: Float value (0-1), similarity threshold. When a new question&#039;s similarity with a cached question exceeds this threshold, the cached answer will be returned directly without calling the LLM. `use_llm_check`: Boolean value to enable/disable LLM similarity verification. When enabled, LLM will be used as a secondary check to verify the similarity between questions before returning cached answers. | Default: `{&quot;enabled&quot;: False, &quot;similarity_threshold&quot;: 0.95, &quot;use_llm_check&quot;: False}` |

&lt;/details&gt;

### Query Param

Use QueryParam to control the behavior your query:

```python
class QueryParam:
    &quot;&quot;&quot;Configuration parameters for query execution in LightRAG.&quot;&quot;&quot;

    mode: Literal[&quot;local&quot;, &quot;global&quot;, &quot;hybrid&quot;, &quot;naive&quot;, &quot;mix&quot;, &quot;bypass&quot;] = &quot;global&quot;
    &quot;&quot;&quot;Specifies the retrieval mode:
    - &quot;local&quot;: Focuses on context-dependent information.
    - &quot;global&quot;: Utilizes global knowledge.
    - &quot;hybrid&quot;: Combines local and global retrieval methods.
    - &quot;naive&quot;: Performs a basic search without advanced techniques.
    - &quot;mix&quot;: Integrates knowledge graph and vector retrieval.
    &quot;&quot;&quot;

    only_need_context: bool = False
    &quot;&quot;&quot;If True, only returns the retrieved context without generating a response.&quot;&quot;&quot;

    only_need_prompt: bool = False
    &quot;&quot;&quot;If True, only returns the generated prompt without producing a response.&quot;&quot;&quot;

    response_type: str = &quot;Multiple Paragraphs&quot;
    &quot;&quot;&quot;Defines the response format. Examples: &#039;Multiple Paragraphs&#039;, &#039;Single Paragraph&#039;, &#039;Bullet Points&#039;.&quot;&quot;&quot;

    stream: bool = False
    &quot;&quot;&quot;If True, enables streaming output for real-time responses.&quot;&quot;&quot;

    top_k: int = int(os.getenv(&quot;TOP_K&quot;, &quot;60&quot;))
    &quot;&quot;&quot;Number of top items to retrieve. Represents entities in &#039;local&#039; mode and relationships in &#039;global&#039; mode.&quot;&quot;&quot;

    chunk_top_k: int = int(os.getenv(&quot;CHUNK_TOP_K&quot;, &quot;10&quot;))
    &quot;&quot;&quot;Number of text chunks to retrieve initially from vector search and keep after reranking.
    If None, defaults to top_k value.
    &quot;&quot;&quot;

    max_entity_tokens: int = int(os.getenv(&quot;MAX_ENTITY_TOKENS&quot;, &quot;10000&quot;))
    &quot;&quot;&quot;Maximum number of tokens allocated for entity context in unified token control system.&quot;&quot;&quot;

    max_relation_tokens: int = int(os.getenv(&quot;MAX_RELATION_TOKENS&quot;, &quot;10000&quot;))
    &quot;&quot;&quot;Maximum number of tokens allocated for relationship context in unified token control system.&quot;&quot;&quot;

    max_total_tokens: int = int(os.getenv(&quot;MAX_TOTAL_TOKENS&quot;, &quot;30000&quot;))
    &quot;&quot;&quot;Maximum total toke

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[yeongpin/cursor-free-vip]]></title>
            <link>https://github.com/yeongpin/cursor-free-vip</link>
            <guid>https://github.com/yeongpin/cursor-free-vip</guid>
            <pubDate>Thu, 07 Aug 2025 00:04:45 GMT</pubDate>
            <description><![CDATA[[Support 0.49.x]ÔºàReset Cursor AI MachineID & Bypass Higher Token LimitÔºâ Cursor Ai ÔºåËá™Âä®ÈáçÁΩÆÊú∫Âô®ID Ôºå ÂÖçË¥πÂçáÁ∫ß‰ΩøÁî®ProÂäüËÉΩ: You've reached your trial request limit. / Too many free trial accounts used on this machine. Please upgrade to pro. We have this limit in place to prevent abuse. Please let us know if you believe this is a mistake.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/yeongpin/cursor-free-vip">yeongpin/cursor-free-vip</a></h1>
            <p>[Support 0.49.x]ÔºàReset Cursor AI MachineID & Bypass Higher Token LimitÔºâ Cursor Ai ÔºåËá™Âä®ÈáçÁΩÆÊú∫Âô®ID Ôºå ÂÖçË¥πÂçáÁ∫ß‰ΩøÁî®ProÂäüËÉΩ: You've reached your trial request limit. / Too many free trial accounts used on this machine. Please upgrade to pro. We have this limit in place to prevent abuse. Please let us know if you believe this is a mistake.</p>
            <p>Language: Python</p>
            <p>Stars: 33,950</p>
            <p>Forks: 4,171</p>
            <p>Stars today: 96 stars today</p>
            <h2>README</h2><pre># ‚û§ Cursor Free VIP

&lt;div align=&quot;center&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./images/logo.png&quot; alt=&quot;Cursor Pro Logo&quot; width=&quot;200&quot; style=&quot;border-radius: 6px;&quot;/&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;

[![Release](https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/release/yeongpin/cursor-free-vip)](https://github.com/yeongpin/cursor-free-vip/releases/latest)
[![License: CC BY-NC-ND 4.0](https://img.shields.io/badge/License-CC_BY--NC--ND_4.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-nd/4.0/)
[![Stars](https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/stars/yeongpin/cursor-free-vip)](https://github.com/yeongpin/cursor-free-vip/stargazers)
[![Downloads](https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/downloads/yeongpin/cursor-free-vip/total)](https://github.com/yeongpin/cursor-free-vip/releases/latest)
&lt;a href=&quot;https://buymeacoffee.com/yeongpin&quot; target=&quot;_blank&quot;&gt;&lt;img alt=&quot;Buy Me a Coffee&quot; src=&quot;https://img.shields.io/badge/Buy%20Me%20a%20Coffee-Support%20Me-FFDA33&quot;&gt;&lt;/a&gt;
 [&lt;img src=&quot;https://devin.ai/assets/deepwiki-badge.png&quot; alt=&quot;Ask DeepWiki.com&quot; height=&quot;20&quot;/&gt;](https://deepwiki.com/yeongpin/cursor-free-vip)

&lt;/p&gt;


&lt;a href=&quot;https://trendshift.io/repositories/13425&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13425&quot; alt=&quot;yeongpin%2Fcursor-free-vip | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;br&gt;
&lt;a href=&quot;https://www.buymeacoffee.com/yeongpin&quot; target=&quot;_blank&quot;&gt;
      &lt;img src=&quot;https://img.buymeacoffee.com/button-api/?text=buy me a coffee&amp;emoji=‚òï&amp;slug=yeongpin&amp;button_colour=ffda33&amp;font_colour=000000&amp;font_family=Bree&amp;outline_colour=000000&amp;coffee_colour=FFDD00&amp;latest=2&quot; width=&quot;160&quot; height=&#039;55&#039; alt=&quot;Buy Me a Coffee&quot;/&gt;
&lt;/a&gt;


&lt;h4&gt;Support Latest 0.49.x Version | ÊîØÊåÅÊúÄÊñ∞ 0.49.x ÁâàÊú¨&lt;/h4&gt;

This tool is for educational purposes, currently the repo does not violate any laws. Please support the original project.
This tool will not generate any fake email accounts and OAuth access.

Supports Windows, macOS and Linux.

For optimal performance, run with privileges and always stay up to date.

ÈÄôÊòØ‰∏ÄÊ¨æÁî®ÊñºÂ≠∏ÁøíÂíåÁ†îÁ©∂ÁöÑÂ∑•ÂÖ∑ÔºåÁõÆÂâç repo Ê≤íÊúâÈÅïÂèç‰ªª‰ΩïÊ≥ïÂæã„ÄÇË´ãÊîØÊåÅÂéü‰ΩúËÄÖ„ÄÇ
ÈÄôÊ¨æÂ∑•ÂÖ∑‰∏çÊúÉÁîüÊàê‰ªª‰ΩïÂÅáÁöÑÈõªÂ≠êÈÉµ‰ª∂Â∏≥Êà∂Âíå OAuth Ë®™Âïè„ÄÇ

ÊîØÊåÅ Windows„ÄÅmacOS Âíå Linux„ÄÇ

Â∞çÊñºÊúÄ‰Ω≥ÊÄßËÉΩÔºåË´ã‰ª•ÁÆ°ÁêÜÂì°Ë∫´‰ªΩÈÅãË°å‰∏¶ÂßãÁµÇ‰øùÊåÅÊúÄÊñ∞„ÄÇ


&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./images/product_2025-04-16_10-40-21.png&quot; alt=&quot;new&quot; width=&quot;800&quot; style=&quot;border-radius: 6px;&quot;/&gt;&lt;br&gt;
&lt;/p&gt;

&lt;/div&gt;

## üîÑ Change Log | Êõ¥Êñ∞Êó•Âøó

[Watch Change Log | Êü•ÁúãÊõ¥Êñ∞Êó•Âøó](CHANGELOG.md)

## ‚ú® Features | ÂäüËÉΩÁâπÈªû

* Support Windows macOS and Linux systems&lt;br&gt;ÊîØÊåÅ Windows„ÄÅmacOS Âíå Linux Á≥ªÁµ±&lt;br&gt;

* Reset Cursor&#039;s configuration&lt;br&gt;ÈáçÁΩÆ Cursor ÁöÑÈÖçÁΩÆ&lt;br&gt;

* Multi-language support (English, ÁÆÄ‰Ωì‰∏≠Êñá, ÁπÅÈ´î‰∏≠Êñá, Vietnamese)&lt;br&gt;Â§öË™ûË®ÄÊîØÊåÅÔºàËã±Êñá„ÄÅÁÆÄ‰Ωì‰∏≠Êñá„ÄÅÁπÅÈ´î‰∏≠Êñá„ÄÅË∂äÂçóË™ûÔºâ&lt;br&gt;

## üíª System Support | Á≥ªÁµ±ÊîØÊåÅ

| Operating System | Architecture      | Supported |
|------------------|-------------------|-----------|
| Windows          | x64, x86          | ‚úÖ         |
| macOS            | Intel, Apple Silicon | ‚úÖ      |
| Linux            | x64, x86, ARM64   | ‚úÖ         |

## üëÄ How to use | Â¶Ç‰Ωï‰ΩøÁî®

&lt;details open&gt;
&lt;summary&gt;&lt;b&gt;‚≠ê Auto Run Script | ËÖ≥Êú¨Ëá™ÂãïÂåñÈÅãË°å&lt;/b&gt;&lt;/summary&gt;

### **Linux/macOS**

```bash
curl -fsSL https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/install.sh -o install.sh &amp;&amp; chmod +x install.sh &amp;&amp; ./install.sh
```

### **Archlinux**

Install via [AUR](https://aur.archlinux.org/packages/cursor-free-vip-git)

```bash
yay -S cursor-free-vip-git
```

### **Windows**

```powershell
irm https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/install.ps1 | iex
```

&lt;/details&gt;

If you want to stop the script, please press Ctrl+C&lt;br&gt;Ë¶ÅÂÅúÊ≠¢ËÖ≥Êú¨ÔºåË´ãÊåâ Ctrl+C

## ‚ùó Note | Ê≥®ÊÑè‰∫ãÈ†Ö

üìù Config | Êñá‰ª∂ÈÖçÁΩÆ
`Win / Macos / Linux Path | Ë∑ØÂæë [Documents/.cursor-free-vip/config.ini]`
&lt;details&gt;
&lt;summary&gt;&lt;b&gt;‚≠ê Config | Êñá‰ª∂ÈÖçÁΩÆ&lt;/b&gt;&lt;/summary&gt;

```
[Chrome]
# Default Google Chrome Path | ÈªòË™çGoogle Chrome ÈÅäË¶ΩÂô®Ë∑ØÂæë
chromepath = C:\Program Files\Google/Chrome/Application/chrome.exe

[Turnstile]
# Handle Turnstile Wait Time | Á≠âÂæÖ‰∫∫Ê©üÈ©óË≠âÊôÇÈñì
handle_turnstile_time = 2
# Handle Turnstile Wait Random Time (must merge 1-3 or 1,3) | Á≠âÂæÖ‰∫∫Ê©üÈ©óË≠âÈö®Ê©üÊôÇÈñìÔºàÂøÖÈ†àÊòØ 1-3 ÊàñËÄÖ 1,3 ÈÄôÊ®£ÁöÑÁµÑÂêàÔºâ
handle_turnstile_random_time = 1-3

[OSPaths]
# Storage Path | Â≠òÂÑ≤Ë∑ØÂæë
storage_path = /Users/username/Library/Application Support/Cursor/User/globalStorage/storage.json
# SQLite Path | SQLiteË∑ØÂæë
sqlite_path = /Users/username/Library/Application Support/Cursor/User/globalStorage/state.vscdb
# Machine ID Path | Ê©üÂô®IDË∑ØÂæë
machine_id_path = /Users/username/Library/Application Support/Cursor/machineId
# For Linux users: ~/.config/cursor/machineid

[Timing]
# Min Random Time | ÊúÄÂ∞èÈö®Ê©üÊôÇÈñì
min_random_time = 0.1
# Max Random Time | ÊúÄÂ§ßÈö®Ê©üÊôÇÈñì
max_random_time = 0.8
# Page Load Wait | È†ÅÈù¢Âä†ËºâÁ≠âÂæÖÊôÇÈñì
page_load_wait = 0.1-0.8
# Input Wait | Ëº∏ÂÖ•Á≠âÂæÖÊôÇÈñì
input_wait = 0.3-0.8
# Submit Wait | Êèê‰∫§Á≠âÂæÖÊôÇÈñì
submit_wait = 0.5-1.5
# Verification Code Input | È©óË≠âÁ¢ºËº∏ÂÖ•Á≠âÂæÖÊôÇÈñì
verification_code_input = 0.1-0.3
# Verification Success Wait | È©óË≠âÊàêÂäüÁ≠âÂæÖÊôÇÈñì
verification_success_wait = 2-3
# Verification Retry Wait | È©óË≠âÈáçË©¶Á≠âÂæÖÊôÇÈñì
verification_retry_wait = 2-3
# Email Check Initial Wait | ÈÉµ‰ª∂Ê™¢Êü•ÂàùÂßãÁ≠âÂæÖÊôÇÈñì
email_check_initial_wait = 4-6
# Email Refresh Wait | ÈÉµ‰ª∂Âà∑Êñ∞Á≠âÂæÖÊôÇÈñì
email_refresh_wait = 2-4
# Settings Page Load Wait | Ë®≠ÁΩÆÈ†ÅÈù¢Âä†ËºâÁ≠âÂæÖÊôÇÈñì
settings_page_load_wait = 1-2
# Failed Retry Time | Â§±ÊïóÈáçË©¶ÊôÇÈñì
failed_retry_time = 0.5-1
# Retry Interval | ÈáçË©¶ÈñìÈöî
retry_interval = 8-12
# Max Timeout | ÊúÄÂ§ßË∂ÖÊôÇÊôÇÈñì
max_timeout = 160

[Utils]
# Check Update | Ê™¢Êü•Êõ¥Êñ∞
check_update = True
# Show Account Info | È°ØÁ§∫Ë≥¨Ëôü‰ø°ÊÅØ
show_account_info = True

[TempMailPlus]
# Enable TempMailPlus | ÂïìÁî® TempMailPlusÔºà‰ªª‰ΩïËΩâÁôºÂà∞TempMailPlusÁöÑÈÉµ‰ª∂ÈÉΩÊîØÊåÅÁç≤ÂèñÈ©óË≠âÁ¢ºÔºå‰æãÂ¶ÇcloudflareÈÉµ‰ª∂Catch-allÔºâ
enabled = false
# TempMailPlus Email | TempMailPlus ÈõªÂ≠êÈÉµ‰ª∂
email = xxxxx@mailto.plus
# TempMailPlus pin | TempMailPlus pinÁ¢º
epin = 

[WindowsPaths]
storage_path = C:\Users\yeongpin\AppData\Roaming\Cursor\User\globalStorage\storage.json
sqlite_path = C:\Users\yeongpin\AppData\Roaming\Cursor\User\globalStorage\state.vscdb
machine_id_path = C:\Users\yeongpin\AppData\Roaming\Cursor\machineId
cursor_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app
updater_path = C:\Users\yeongpin\AppData\Local\cursor-updater
update_yml_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app-update.yml
product_json_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app\product.json

[Browser]
default_browser = opera
chrome_path = C:\Program Files\Google\Chrome\Application\chrome.exe
edge_path = C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe
firefox_path = C:\Program Files\Mozilla Firefox\firefox.exe
brave_path = C:\Program Files\BraveSoftware/Brave-Browser/Application/brave.exe
chrome_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe
edge_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\msedgedriver.exe
firefox_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\geckodriver.exe
brave_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe
opera_path = C:\Users\yeongpin\AppData\Local\Programs\Opera\opera.exe
opera_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe

[OAuth]
show_selection_alert = False
timeout = 120
max_attempts = 3
```

&lt;/details&gt;

* Use administrator privileges to run the script &lt;br&gt;Ë´ã‰ΩøÁî®ÁÆ°ÁêÜÂì°Ë∫´‰ªΩÈÅãË°åËÖ≥Êú¨

* Confirm that Cursor is closed before running the script &lt;br&gt;Ë´ãÁ¢∫‰øùÂú®ÈÅãË°åËÖ≥Êú¨ÂâçÂ∑≤Á∂ìÈóúÈñâ Cursor&lt;br&gt;

* This tool is only for learning and research purposes &lt;br&gt;Ê≠§Â∑•ÂÖ∑ÂÉÖ‰æõÂ≠∏ÁøíÂíåÁ†îÁ©∂‰ΩøÁî®&lt;br&gt;

* Please comply with the relevant software usage terms when using this tool &lt;br&gt;‰ΩøÁî®Êú¨Â∑•ÂÖ∑ÊôÇË´ãÈÅµÂÆàÁõ∏ÈóúËªü‰ª∂‰ΩøÁî®Ê¢ùÊ¨æ

## üö® Common Issues | Â∏∏Ë¶ãÂïèÈ°å

|                   Â¶ÇÊûúÈÅáÂà∞Ê¨äÈôêÂïèÈ°åÔºåË´ãÁ¢∫‰øùÔºö                    |                   Ê≠§ËÖ≥Êú¨‰ª•ÁÆ°ÁêÜÂì°Ë∫´‰ªΩÈÅãË°å                    |
|:--------------------------------------------------:|:------------------------------------------------:|
| If you encounter permission issues, please ensure: | This script is run with administrator privileges |
| Error &#039;User is not authorized&#039; | This means your account was banned for using temporary (disposal) mail. Ensure using a non-temporary mail service |
## ü§© Contribution | Ë≤¢Áçª

Ê≠°ËøéÊèê‰∫§ Issue Âíå Pull RequestÔºÅ


&lt;a href=&quot;https://github.com/yeongpin/cursor-free-vip/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=yeongpin/cursor-free-vip&amp;preview=true&amp;max=&amp;columns=&quot; /&gt;
&lt;/a&gt;
&lt;br /&gt;&lt;br /&gt;

## üì© Disclaimer | ÂÖçË≤¨ËÅ≤Êòé

Êú¨Â∑•ÂÖ∑ÂÉÖ‰æõÂ≠∏ÁøíÂíåÁ†îÁ©∂‰ΩøÁî®Ôºå‰ΩøÁî®Êú¨Â∑•ÂÖ∑ÊâÄÁî¢ÁîüÁöÑ‰ªª‰ΩïÂæåÊûúÁî±‰ΩøÁî®ËÄÖËá™Ë°åÊâøÊìî„ÄÇ &lt;br&gt;

This tool is only for learning and research purposes, and any consequences arising from the use of this tool are borne
by the user.

## üí∞ Buy Me a Coffee | Ë´ãÊàëÂñùÊùØÂíñÂï°

&lt;div align=&quot;center&quot;&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;img src=&quot;./images/provi-code.jpg&quot; alt=&quot;buy_me_a_coffee&quot; width=&quot;280&quot;/&gt;&lt;br&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;img src=&quot;./images/paypal.png&quot; alt=&quot;buy_me_a_coffee&quot; width=&quot;280&quot;/&gt;&lt;br&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;

## ‚≠ê Star History | ÊòüÊòüÊï∏

&lt;div align=&quot;center&quot;&gt;

[![Star History Chart](https://api.star-history.com/svg?repos=yeongpin/cursor-free-vip&amp;type=Date)](https://star-history.com/#yeongpin/cursor-free-vip&amp;Date)

&lt;/div&gt;

## üìù License | ÊéàÊ¨ä

Êú¨È†ÖÁõÆÊé°Áî® [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/) ÊéàÊ¨ä„ÄÇ
Please refer to the [LICENSE](LICENSE.md) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[roboflow/rf-detr]]></title>
            <link>https://github.com/roboflow/rf-detr</link>
            <guid>https://github.com/roboflow/rf-detr</guid>
            <pubDate>Thu, 07 Aug 2025 00:04:44 GMT</pubDate>
            <description><![CDATA[RF-DETR is a real-time object detection model architecture developed by Roboflow, SOTA on COCO and designed for fine-tuning.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/roboflow/rf-detr">roboflow/rf-detr</a></h1>
            <p>RF-DETR is a real-time object detection model architecture developed by Roboflow, SOTA on COCO and designed for fine-tuning.</p>
            <p>Language: Python</p>
            <p>Stars: 2,668</p>
            <p>Forks: 294</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre># RF-DETR: SOTA Real-Time Object Detection Model

[![version](https://badge.fury.io/py/rfdetr.svg)](https://badge.fury.io/py/rfdetr)
[![downloads](https://img.shields.io/pypi/dm/rfdetr)](https://pypistats.org/packages/rfdetr)
[![python-version](https://img.shields.io/pypi/pyversions/rfdetr)](https://badge.fury.io/py/rfdetr)
[![license](https://img.shields.io/badge/license-Apache%202.0-blue)](https://github.com/roboflow/rfdetr/blob/main/LICENSE)

[![hf space](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/SkalskiP/RF-DETR)
[![colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-rf-detr-on-detection-dataset.ipynb)
[![roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/rf-detr)
[![discord](https://img.shields.io/discord/1159501506232451173?logo=discord&amp;label=discord&amp;labelColor=fff&amp;color=5865f2&amp;link=https%3A%2F%2Fdiscord.gg%2FGbfgXGJ8Bk)](https://discord.gg/GbfgXGJ8Bk)

RF-DETR is a real-time, transformer-based object detection model architecture developed by Roboflow and released under the Apache 2.0 license.

RF-DETR is the first real-time model to exceed 60 AP on the [Microsoft COCO benchmark](https://cocodataset.org/#home) alongside competitive performance at base sizes. It also achieves state-of-the-art performance on [RF100-VL](https://github.com/roboflow/rf100-vl), an object detection benchmark that measures model domain adaptability to real world problems. RF-DETR is fastest and most accurate for its size when compared current real-time objection models.

RF-DETR is small enough to run on the edge using [Inference](https://github.com/roboflow/inference), making it an ideal model for deployments that need both strong accuracy and real-time performance.

[Read the documentation to get started training.](https://rfdetr.roboflow.com)

## News

- `2025/07/23`: We release three new checkpoints for RF-DETR: Nano, Small, and Medium.
    - RF-DETR Base is now deprecated. We recommend using RF-DETR Medium which offers subtantially better accuracy at comparable latency.
- `2025/03/20`: We release RF-DETR real-time object detection model. **Code and checkpoint for RF-DETR-large and RF-DETR-base are available.**
- `2025/04/03`: We release early stopping, gradient checkpointing, metrics saving, training resume, TensorBoard and W&amp;B logging support.
- `2025/05/16`: We release an &#039;optimize_for_inference&#039; method which speeds up native PyTorch by up to 2x, depending on platform.

## Results

RF-DETR achieves state-of-the-art performance on both the Microsoft COCO and the RF100-VL benchmarks.

The table below shows the performance of RF-DETR medium, compared to comparable medium models:

![rf-detr-coco-rf100-vl-9](https://media.roboflow.com/rfdetr/pareto1.png)

|family|size  |coco_map50|coco_map50@95|rf100vl_map50|rv100vl_map50@95|latency|
|------|------|----------|------------|-------------|---------------|-------|
|RF-DETR|Nano  |67.6      |48.4        |84.1         |57.1           |2.32   |
|RF-DETR|Small |72.1      |53.0        |85.9         |59.6           |3.52   |
|RF-DETR|Medium|73.6      |54.7        |86.6         |60.6           |4.52   |
|YOLO11|n     |52.0      |37.4        |81.4         |55.3           |2.49   |
|YOLO11|s     |59.7      |44.4        |82.3         |56.2           |3.16   |
|YOLO11|m     |64.1      |48.6        |82.5         |56.5           |5.13   |
|YOLO11|l     |65.3      |50.2        |x            |x              |6.65   |
|YOLO11|x     |66.5      |51.2        |x            |x              |11.92  |
|LW-DETR|Tiny  |60.7      |42.9        |x            |x              |1.91   |
|LW-DETR|Small |66.8      |48.0        |84.5         |58.0           |2.62   |
|LW-DETR|Medium|72.0      |52.6        |85.2         |59.4           |4.49   |
|D-FINE |Nano  |60.2      |42.7        |83.6         |57.7           |2.12   |
|D-FINE |Small |67.6      |50.7        |84.5         |59.9           |3.55   |
|D-FINE |Medium|72.6      |55.1        |84.6         |60.2           |5.68   |

[See our benchmark notes in the RF-DETR documentation.](https://rfdetr.roboflow.com/learn/benchmarks/)

_We are actively working on RF-DETR Large and X-Large models using the same techniques we used to achieve the strong accuracy that RF-DETR Medium attains. This is why RF-DETR Large and X-Large is not yet reported on our pareto charts and why we haven&#039;t benchmarked other models at similar sizes. Check back in the next few weeks for the launch of new RF-DETR Large and X-Large models._

## Installation

To install RF-DETR, install the `rfdetr` package in a [**Python&gt;=3.9**](https://www.python.org/) environment with `pip`:

```bash
pip install rfdetr
```

&lt;details&gt;
&lt;summary&gt;Install from source&lt;/summary&gt;

&lt;br&gt;

By installing RF-DETR from source, you can explore the most recent features and enhancements that have not yet been officially released. Please note that these updates are still in development and may not be as stable as the latest published release.

```bash
pip install git+https://github.com/roboflow/rf-detr.git
```

&lt;/details&gt;

## Inference

The easiest path to deployment is using Roboflow&#039;s [Inference](https://github.com/roboflow/inference) package. 

The code below lets you run `rfdetr-base` on an image:

```python
import os
import supervision as sv
from inference import get_model
from PIL import Image
from io import BytesIO
import requests

url = &quot;https://media.roboflow.com/dog.jpeg&quot;
image = Image.open(BytesIO(requests.get(url).content))

model = get_model(&quot;rfdetr-base&quot;)

predictions = model.infer(image, confidence=0.5)[0]

detections = sv.Detections.from_inference(predictions)

labels = [prediction.class_name for prediction in predictions.predictions]

annotated_image = image.copy()
annotated_image = sv.BoxAnnotator(color=sv.ColorPalette.ROBOFLOW).annotate(annotated_image, detections)
annotated_image = sv.LabelAnnotator(color=sv.ColorPalette.ROBOFLOW).annotate(annotated_image, detections, labels)
```

## Predict

You can also use the .predict method to perform inference during local development. The `.predict()` method accepts various input formats, including file paths, PIL images, NumPy arrays, and torch tensors. Please ensure inputs use RGB channel order. For `torch.Tensor` inputs specifically, they must have a shape of `(3, H, W)` with values normalized to the `[0..1)` range. If you don&#039;t plan to modify the image or batch size dynamically at runtime, you can also use `.optimize_for_inference()` to get up to 2x end-to-end speedup, depending on platform.

```python
import io
import requests
import supervision as sv
from PIL import Image
from rfdetr import RFDETRBase
from rfdetr.util.coco_classes import COCO_CLASSES

model = RFDETRBase()

model = model.optimize_for_inference()

url = &quot;https://media.roboflow.com/notebooks/examples/dog-2.jpeg&quot;

image = Image.open(io.BytesIO(requests.get(url).content))
detections = model.predict(image, threshold=0.5)

labels = [
    f&quot;{COCO_CLASSES[class_id]} {confidence:.2f}&quot;
    for class_id, confidence
    in zip(detections.class_id, detections.confidence)
]

annotated_image = image.copy()
annotated_image = sv.BoxAnnotator().annotate(annotated_image, detections)
annotated_image = sv.LabelAnnotator().annotate(annotated_image, detections, labels)

sv.plot_image(annotated_image)
```

### Train a Model

You can fine-tune an RF-DETR Nano, Small, Medium, and Base model with a custom dataset using the `rfdetr` Python package.

[Read our training tutorial to get started](https://rfdetr.roboflow.com/learn/train/)

## Documentation

Visit our [documentation website](https://rfdetr.roboflow.com) to learn more about how to use RF-DETR.

## License

Both the code and the weights pretrained on the COCO dataset are released under the [Apache 2.0 license](https://github.com/roboflow/r-flow/blob/main/LICENSE).

## Acknowledgements

Our work is built upon [LW-DETR](https://arxiv.org/pdf/2406.03459), [DINOv2](https://arxiv.org/pdf/2304.07193), and [Deformable DETR](https://arxiv.org/pdf/2010.04159). Thanks to their authors for their excellent work!

## Citation

If you find our work helpful for your research, please consider citing the following BibTeX entry.

```bibtex
@software{rf-detr,
  author = {Robinson, Isaac and Robicheaux, Peter and Popov, Matvei},
  license = {Apache-2.0},
  title = {RF-DETR},
  howpublished = {\url{https://github.com/roboflow/rf-detr}},
  year = {2025},
  note = {SOTA Real-Time Object Detection Model}
}
```

## Contribute

We welcome and appreciate all contributions! If you notice any issues or bugs, have questions, or would like to suggest new features, please [open an issue](https://github.com/roboflow/rf-detr/issues/new) or pull request. By sharing your ideas and improvements, you help make RF-DETR better for everyone.

&lt;div align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://youtube.com/roboflow&quot;&gt;
          &lt;img
            src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1672949634652&quot;
            width=&quot;3%&quot;
          /&gt;
      &lt;/a&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&quot; width=&quot;3%&quot;/&gt;
      &lt;a href=&quot;https://roboflow.com&quot;&gt;
          &lt;img
            src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1672949746649&quot;
            width=&quot;3%&quot;
          /&gt;
      &lt;/a&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&quot; width=&quot;3%&quot;/&gt;
      &lt;a href=&quot;https://www.linkedin.com/company/roboflow-ai/&quot;&gt;
          &lt;img
            src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1672949633691&quot;
            width=&quot;3%&quot;
          /&gt;
      &lt;/a&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&quot; width=&quot;3%&quot;/&gt;
      &lt;a href=&quot;https://docs.roboflow.com&quot;&gt;
          &lt;img
            src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1672949634511&quot;
            width=&quot;3%&quot;
          /&gt;
      &lt;/a&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&quot; width=&quot;3%&quot;/&gt;
      &lt;a href=&quot;https://discuss.roboflow.com&quot;&gt;
          &lt;img
            src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1672949633584&quot;
            width=&quot;3%&quot;
          /&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&quot; width=&quot;3%&quot;/&gt;
      &lt;a href=&quot;https://blog.roboflow.com&quot;&gt;
          &lt;img
            src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1672949633605&quot;
            width=&quot;3%&quot;
          /&gt;
      &lt;/a&gt;
      &lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[reflex-dev/reflex]]></title>
            <link>https://github.com/reflex-dev/reflex</link>
            <guid>https://github.com/reflex-dev/reflex</guid>
            <pubDate>Thu, 07 Aug 2025 00:04:43 GMT</pubDate>
            <description><![CDATA[üï∏Ô∏è Web apps in pure Python üêç]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/reflex-dev/reflex">reflex-dev/reflex</a></h1>
            <p>üï∏Ô∏è Web apps in pure Python üêç</p>
            <p>Language: Python</p>
            <p>Stars: 25,256</p>
            <p>Forks: 1,480</p>
            <p>Stars today: 367 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/reflex.svg&quot; alt=&quot;Reflex Logo&quot; width=&quot;300px&quot;&gt;

&lt;hr&gt;

### **‚ú® Performant, customizable web apps in pure Python. Deploy in seconds. ‚ú®**

[![PyPI version](https://badge.fury.io/py/reflex.svg)](https://badge.fury.io/py/reflex)
![versions](https://img.shields.io/pypi/pyversions/reflex.svg)
[![Documentation](https://img.shields.io/badge/Documentation%20-Introduction%20-%20%23007ec6)](https://reflex.dev/docs/getting-started/introduction)
[![PyPI Downloads](https://static.pepy.tech/badge/reflex)](https://pepy.tech/projects/reflex)
[![Discord](https://img.shields.io/discord/1029853095527727165?color=%237289da&amp;label=Discord)](https://discord.gg/T5WSbC2YtQ)
[![Twitter](https://img.shields.io/twitter/follow/getreflex)](https://x.com/getreflex)

&lt;/div&gt;

---

[English](https://github.com/reflex-dev/reflex/blob/main/README.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](https://github.com/reflex-dev/reflex/blob/main/docs/zh/zh_cn/README.md) | [ÁπÅÈ´î‰∏≠Êñá](https://github.com/reflex-dev/reflex/blob/main/docs/zh/zh_tw/README.md) | [T√ºrk√ße](https://github.com/reflex-dev/reflex/blob/main/docs/tr/README.md) | [‡§π‡§ø‡§Ç‡§¶‡•Ä](https://github.com/reflex-dev/reflex/blob/main/docs/in/README.md) | [Portugu√™s (Brasil)](https://github.com/reflex-dev/reflex/blob/main/docs/pt/pt_br/README.md) | [Italiano](https://github.com/reflex-dev/reflex/blob/main/docs/it/README.md) | [Espa√±ol](https://github.com/reflex-dev/reflex/blob/main/docs/es/README.md) | [ÌïúÍµ≠Ïñ¥](https://github.com/reflex-dev/reflex/blob/main/docs/kr/README.md) | [Êó•Êú¨Ë™û](https://github.com/reflex-dev/reflex/blob/main/docs/ja/README.md) | [Deutsch](https://github.com/reflex-dev/reflex/blob/main/docs/de/README.md) | [Persian (Ÿæÿßÿ±ÿ≥€å)](https://github.com/reflex-dev/reflex/blob/main/docs/pe/README.md) | [Ti·∫øng Vi·ªát](https://github.com/reflex-dev/reflex/blob/main/docs/vi/README.md)

---

&gt; [!NOTE]
&gt; üöÄ **Try [Reflex Build](https://build.reflex.dev/)** ‚Äì our AI-powered app builder that generates full-stack Reflex applications in seconds.

---

# Introduction

Reflex is a library to build full-stack web apps in pure Python.

Key features:

- **Pure Python** - Write your app&#039;s frontend and backend all in Python, no need to learn Javascript.
- **Full Flexibility** - Reflex is easy to get started with, but can also scale to complex apps.
- **Deploy Instantly** - After building, deploy your app with a [single command](https://reflex.dev/docs/hosting/deploy-quick-start/) or host it on your own server.

See our [architecture page](https://reflex.dev/blog/2024-03-21-reflex-architecture/#the-reflex-architecture) to learn how Reflex works under the hood.

## ‚öôÔ∏è Installation

Open a terminal and run (Requires Python 3.10+):

```bash
pip install reflex
```

## ü•≥ Create your first app

Installing `reflex` also installs the `reflex` command line tool.

Test that the install was successful by creating a new project. (Replace `my_app_name` with your project name):

```bash
mkdir my_app_name
cd my_app_name
reflex init
```

This command initializes a template app in your new directory.

You can run this app in development mode:

```bash
reflex run
```

You should see your app running at http://localhost:3000.

Now you can modify the source code in `my_app_name/my_app_name.py`. Reflex has fast refreshes so you can see your changes instantly when you save your code.

## ü´ß Example App

Let&#039;s go over an example: creating an image generation UI around [DALL¬∑E](https://platform.openai.com/docs/guides/images/image-generation?context=node). For simplicity, we just call the [OpenAI API](https://platform.openai.com/docs/api-reference/authentication), but you could replace this with an ML model run locally.

&amp;nbsp;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/dalle.gif&quot; alt=&quot;A frontend wrapper for DALL¬∑E, shown in the process of generating an image.&quot; width=&quot;550&quot; /&gt;
&lt;/div&gt;

&amp;nbsp;

Here is the complete code to create this. This is all done in one Python file!

```python
import reflex as rx
import openai

openai_client = openai.OpenAI()


class State(rx.State):
    &quot;&quot;&quot;The app state.&quot;&quot;&quot;

    prompt = &quot;&quot;
    image_url = &quot;&quot;
    processing = False
    complete = False

    def get_image(self):
        &quot;&quot;&quot;Get the image from the prompt.&quot;&quot;&quot;
        if self.prompt == &quot;&quot;:
            return rx.window_alert(&quot;Prompt Empty&quot;)

        self.processing, self.complete = True, False
        yield
        response = openai_client.images.generate(
            prompt=self.prompt, n=1, size=&quot;1024x1024&quot;
        )
        self.image_url = response.data[0].url
        self.processing, self.complete = False, True


def index():
    return rx.center(
        rx.vstack(
            rx.heading(&quot;DALL-E&quot;, font_size=&quot;1.5em&quot;),
            rx.input(
                placeholder=&quot;Enter a prompt..&quot;,
                on_blur=State.set_prompt,
                width=&quot;25em&quot;,
            ),
            rx.button(
                &quot;Generate Image&quot;,
                on_click=State.get_image,
                width=&quot;25em&quot;,
                loading=State.processing
            ),
            rx.cond(
                State.complete,
                rx.image(src=State.image_url, width=&quot;20em&quot;),
            ),
            align=&quot;center&quot;,
        ),
        width=&quot;100%&quot;,
        height=&quot;100vh&quot;,
    )

# Add state and page to the app.
app = rx.App()
app.add_page(index, title=&quot;Reflex:DALL-E&quot;)
```

## Let&#039;s break this down.

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/reflex-dev/reflex/main/docs/images/dalle_colored_code_example.png&quot; alt=&quot;Explaining the differences between backend and frontend parts of the DALL-E app.&quot; width=&quot;900&quot; /&gt;
&lt;/div&gt;

### **Reflex UI**

Let&#039;s start with the UI.

```python
def index():
    return rx.center(
        ...
    )
```

This `index` function defines the frontend of the app.

We use different components such as `center`, `vstack`, `input`, and `button` to build the frontend. Components can be nested within each other
to create complex layouts. And you can use keyword args to style them with the full power of CSS.

Reflex comes with [60+ built-in components](https://reflex.dev/docs/library) to help you get started. We are actively adding more components, and it&#039;s easy to [create your own components](https://reflex.dev/docs/wrapping-react/overview/).

### **State**

Reflex represents your UI as a function of your state.

```python
class State(rx.State):
    &quot;&quot;&quot;The app state.&quot;&quot;&quot;
    prompt = &quot;&quot;
    image_url = &quot;&quot;
    processing = False
    complete = False

```

The state defines all the variables (called vars) in an app that can change and the functions that change them.

Here the state is comprised of a `prompt` and `image_url`. There are also the booleans `processing` and `complete` to indicate when to disable the button (during image generation) and when to show the resulting image.

### **Event Handlers**

```python
def get_image(self):
    &quot;&quot;&quot;Get the image from the prompt.&quot;&quot;&quot;
    if self.prompt == &quot;&quot;:
        return rx.window_alert(&quot;Prompt Empty&quot;)

    self.processing, self.complete = True, False
    yield
    response = openai_client.images.generate(
        prompt=self.prompt, n=1, size=&quot;1024x1024&quot;
    )
    self.image_url = response.data[0].url
    self.processing, self.complete = False, True
```

Within the state, we define functions called event handlers that change the state vars. Event handlers are the way that we can modify the state in Reflex. They can be called in response to user actions, such as clicking a button or typing in a text box. These actions are called events.

Our DALL¬∑E app has an event handler, `get_image` which gets this image from the OpenAI API. Using `yield` in the middle of an event handler will cause the UI to update. Otherwise the UI will update at the end of the event handler.

### **Routing**

Finally, we define our app.

```python
app = rx.App()
```

We add a page from the root of the app to the index component. We also add a title that will show up in the page preview/browser tab.

```python
app.add_page(index, title=&quot;DALL-E&quot;)
```

You can create a multi-page app by adding more pages.

## üìë Resources

&lt;div align=&quot;center&quot;&gt;

üìë [Docs](https://reflex.dev/docs/getting-started/introduction) &amp;nbsp; | &amp;nbsp; üóûÔ∏è [Blog](https://reflex.dev/blog) &amp;nbsp; | &amp;nbsp; üì± [Component Library](https://reflex.dev/docs/library) &amp;nbsp; | &amp;nbsp; üñºÔ∏è [Templates](https://reflex.dev/templates/) &amp;nbsp; | &amp;nbsp; üõ∏ [Deployment](https://reflex.dev/docs/hosting/deploy-quick-start) &amp;nbsp;

&lt;/div&gt;

## ‚úÖ Status

Reflex launched in December 2022 with the name Pynecone.

üöÄ Introducing [Reflex Build](https://build.reflex.dev/) ‚Äî Our AI-Powered Builder
Reflex Build uses AI to generate complete full-stack Python applications. It helps you quickly create, customize, and refine your Reflex apps ‚Äî from frontend components to backend logic ‚Äî so you can focus on your ideas instead of boilerplate code. Whether you‚Äôre prototyping or scaling, Reflex Build accelerates development by intelligently scaffolding and optimizing your app‚Äôs entire stack.

Alongside this, [Reflex Cloud](https://cloud.reflex.dev) launched in 2025 to offer the best hosting experience for your Reflex apps. We‚Äôre continuously improving the platform with new features and capabilities.

Reflex has new releases and features coming every week! Make sure to :star: star and :eyes: watch this repository to stay up to date.

## Contributing

We welcome contributions of any size! Below are some good ways to get started in the Reflex community.

- **Join Our Discord**: Our [Discord](https://discord.gg/T5WSbC2YtQ) is the best place to get help on your Reflex project and to discuss how you can contribute.
- **GitHub Discussions**: A great way to talk about features you want added or things that are confusing/need clarification.
- **GitHub Issues**: [Issues](https://github.com/reflex-dev/reflex/issues) are an excellent way to report bugs. Additionally, you can try and solve an existing issue and submit a PR.

We are actively looking for contributors, no matter your skill level or experience. To contribute check out [CONTRIBUTING.md](https://github.com/reflex-dev/reflex/blob/main/CONTRIBUTING.md)

## All Thanks To Our Contributors:

&lt;a href=&quot;https://github.com/reflex-dev/reflex/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=reflex-dev/reflex&quot; /&gt;
&lt;/a&gt;

## License

Reflex is open-source and licensed under the [Apache License 2.0](https://raw.githubusercontent.com/reflex-dev/reflex/main/LICENSE).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[public-apis/public-apis]]></title>
            <link>https://github.com/public-apis/public-apis</link>
            <guid>https://github.com/public-apis/public-apis</guid>
            <pubDate>Thu, 07 Aug 2025 00:04:42 GMT</pubDate>
            <description><![CDATA[A collective list of free APIs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/public-apis/public-apis">public-apis/public-apis</a></h1>
            <p>A collective list of free APIs</p>
            <p>Language: Python</p>
            <p>Stars: 360,167</p>
            <p>Forks: 37,777</p>
            <p>Stars today: 588 stars today</p>
            <h2>README</h2><pre># Try Public APIs for free
The Public APIs repository is manually curated by community members like you and folks working at [APILayer](https://apilayer.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo). It includes an extensive list of public APIs from many domains that you can use for your own products. Consider it a treasure trove of APIs well-managed by the community over the years.

&lt;br &gt;

&lt;p&gt;
    &lt;a href=&quot;https://apilayer.com&quot;&gt;
        &lt;div&gt;
            &lt;img src=&quot;.github/cs1586-APILayerLogoUpdate2022-LJ_v2-HighRes.png&quot; width=&quot;100%&quot; alt=&quot;APILayer Logo&quot; /&gt;
        &lt;/div&gt;
    &lt;/a&gt;
  &lt;/p&gt;

APILayer is the fastest way to integrate APIs into any product. Explore [APILayer APIs](https://apilayer.com/products/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo) here for your next project.

Join our [Discord server](https://discord.com/invite/hgjA78638n/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo) to get updates, ask questions, get answers, random community calls, and more.

&lt;br &gt;

## APILayer APIs
| API | Description | Call this API |
|:---|:---|:---|
| [IPstack](https://ipstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Locate and Identify Website Visitors by IP Address | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-55145132-244c-448c-8e6f-8780866e4862?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-55145132-244c-448c-8e6f-8780866e4862%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Marketstack](https://marketstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Free, easy-to-use REST API interface delivering worldwide stock market data in JSON format | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Weatherstack](https://weatherstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Retrieve instant, accurate weather information for any location in the world in lightweight JSON format | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Numverify](https://numverify.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers ) | Global Phone Number Validation &amp; Lookup JSON API |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Fixer](https://fixer.io/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Fixer is a simple and lightweight API for current and historical foreign exchange (forex) rates. |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Aviationstack](https://avaitionstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Free, real-time flight status and global Aviation data API |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-72ee0d35-018e-4370-a2b6-a66d3ebd5b5a?action=collection/fork)|

&lt;br &gt;

## Learn more about Public APIs

&lt;strong&gt;Get Involved&lt;/strong&gt;

* [Contributing Guide](CONTRIBUTING.md)
* [API for this project](https://github.com/davemachado/public-api)
* [Issues](https://github.com/public-apis/public-apis/issues)
* [Pull Requests](https://github.com/public-apis/public-apis/pulls)
* [LICENSE](LICENSE) 

&lt;br /&gt;

## Index

* [Animals](#animals)
* [Anime](#anime)
* [Anti-Malware](#anti-malware)
* [Art &amp; Design](#art--design)
* [Authentication &amp; Authorization](#authentication--authorization)
* [Blockchain](#blockchain)
* [Books](#books)
* [Business](#business)
* [Calendar](#calendar)
* [Cloud Storage &amp; File Sharing](#cloud-storage--file-sharing)
* [Continuous Integration](#continuous-integration)
* [Cryptocurrency](#cryptocurrency)
* [Currency Exchange](#currency-exchange)
* [Data Validation](#data-validation)
* [Development](#development)
* [Dictionaries](#dictionaries)
* [Documents &amp; Productivity](#documents--productivity)
* [Email](#email)
* [Entertainment](#entertainment)
* [Environment](#environment)
* [Events](#events)
* [Finance](#finance)
* [Food &amp; Drink](#food--drink)
* [Games &amp; Comics](#games--comics)
* [Geocoding](#geocoding)
* [Government](#government)
* [Health](#health)
* [Jobs](#jobs)
* [Machine Learning](#machine-learning)
* [Music](#music)
* [News](#news)
* [Open Data](#open-data)
* [Open Source Projects](#open-source-projects)
* [Patent](#patent)
* [Personality](#personality)
* [Phone](#phone)
* [Photography](#photography)
* [Programming](#programming)
* [Science &amp; Math](#science--math)
* [Security](#security)
* [Shopping](#shopping)
* [Social](#social)
* [Sports &amp; Fitness](#sports--fitness)
* [Test Data](#test-data)
* [Text Analysis](#text-analysis)
* [Tracking](#tracking)
* [Transportation](#transportation)
* [URL Shorteners](#url-shorteners)
* [Vehicle](#vehicle)
* [Video](#video)
* [Weather](#weather)
&lt;br &gt;

### Animals
API | Description | Auth | HTTPS | CORS 
|:---|:---|:---|:---|:---|
| [AdoptAPet](https://www.adoptapet.com/public/apis/pet_list.html) | Resource to help get pets adopted | `apiKey` | Yes | Yes |
| [Axolotl](https://theaxolotlapi.netlify.app/) | Collection of axolotl pictures and facts | No | Yes | No |
| [Cat Facts](https://alexwohlbruck.github.io/cat-facts/) | Daily cat facts | No | Yes | No | |
| [Cataas](https://cataas.com/) | Cat as a service (cats pictures and gifs) | No | Yes | No |
| [Cats](https://docs.thecatapi.com/) | Pictures of cats from Tumblr | `apiKey` | Yes | No |
| [Dog Facts](https://dukengn.github.io/Dog-facts-API/) | Random dog facts | No | Yes | Yes |
| [Dog Facts](https://kinduff.github.io/dog-api/) | Random facts of Dogs | No | Yes | Yes |
| [Dogs](https://dog.ceo/dog-api/) | Based on the Stanford Dogs Dataset | No | Yes | Yes |
| [eBird](https://documenter.getpostman.com/view/664302/S1ENwy59) | Retrieve recent or notable birding observations within a region | `apiKey` | Yes | No |
| [FishWatch](https://www.fishwatch.gov/developers) | Information and pictures about individual fish species | No | Yes | Yes |
| [HTTP Cat](https://http.cat/) | Cat for every HTTP Status | No | Yes | Yes |
| [HTTP Dog](https://http.dog/) | Dogs for every HTTP response status code | No | Yes | Yes |
| [IUCN](http://apiv3.iucnredlist.org/api/v3/docs) | IUCN Red List of Threatened Species | `apiKey` | No | No |
| [MeowFacts](https://github.com/wh-iterabb-it/meowfacts) | Get random cat facts | No | Yes | No |
| [Movebank](https://github.com/movebank/movebank-api-doc) | Movement and Migration data of animals | No | Yes | Yes |
| [Petfinder](https://www.petfinder.com/developers/) | Petfinder is dedicated to helping pets find homes, another resource to get pets adopted | `apiKey` | Yes | Yes |
| [PlaceBear](https://placebear.com/) | Placeholder bear pictures | No | Yes | Yes |
| [PlaceDog](https://place.dog) | Placeholder Dog pictures | No | Yes | Yes |
| [PlaceKitten](https://placekitten.com/) | Placeholder Kitten pictures | No | Yes | Yes |
| [RandomDog](https://random.dog/woof.json) | Random pictures of dogs | No | Yes | Yes |
| [RandomDuck](https://random-d.uk/api) | Random pictures of ducks | No | Yes | No |
| [RandomFox](https://randomfox.ca/floof/) | Random pictures of foxes | No | Yes | No |
| [RescueGroups](https://userguide.rescuegroups.org/display/APIDG/API+Developers+Guide+Home) | Adoption | No | Yes | Unknown |
| [Shibe.Online](http://shibe.online/) | Random pictures of Shiba Inu, cats or birds | No | Yes | Yes |
| [The Dog](https://thedogapi.com/) | A public service all about Dogs, free to use when making your fancy new App, Website or Service | `apiKey` | Yes | No |
| [xeno-canto](https://xeno-canto.org/explore/api) | Bird recordings | No | Yes | Unknown |
| [Zoo Animals](https://zoo-animal-api.herokuapp.com/) | Facts and pictures of zoo animals | No | Yes | Yes |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;

### Anime
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AniAPI](https://aniapi.com/docs/) | Anime discovery, streaming &amp; syncing with trackers | `OAuth` | Yes | Yes |
| [AniDB](https://wiki.anidb.net/HTTP_API_Definition) | Anime Database | `apiKey` | No | Unknown |
| [AniList](https://github.com/AniList/ApiV2-GraphQL-Docs) | Anime discovery &amp; tracking | `OAuth` | Yes | Unknown |
| [AnimeChan](https://github.com/RocktimSaikia/anime-chan) | Anime quotes (over 10k+) | No | Yes | No |
| [AnimeFacts](https://chandan-02.github.io/anime-facts-rest-api/) | Anime Facts (over 100+) | No | Yes | Yes |
| [AnimeNewsNetwork](https://www.animenewsnetwork.com/encyclopedia/api.php) | Anime industry news | No | Yes | Yes |
| [Catboy](https://catboys.com/api) | Neko images, funny GIFs &amp; more | No | Yes | Yes |
| [Danbooru Anime](https://danbooru.donmai.us/wiki_pages/help:api) | Thousands of anime artist database to find good anime art | `apiKey` | Yes | Yes |
| [Jikan](https://jikan.moe) | Unofficial MyAnimeList API | No | Yes | Yes |
| [Kitsu](https://kitsu.docs.apiary.io/) | Anime discovery platform | `OAuth` | Yes | Yes |
| [MangaDex](https://api.mangadex.org/docs.html) | Manga Database and Community | `apiKey` | Yes | Unknown |
| [Mangapi](https://rapidapi.com/pierre.carcellermeunier/api/mangapi3/) | Translate manga pages from one language to another | `apiKey` | Yes | Unknown |
| [MyAnimeList](https://myanimelist.net/clubs.php?cid=13727) | Anime and Manga Database and Community | `OAuth` | Yes | Unknown |
| [NekosBest](https://docs.nekos.best) | Neko Images &amp; Anime roleplaying GIFs | No | Yes | Yes |
| [Shikimori](https://shikimori.one/api/doc) | Anime discovery, tracking, forum, rates | `OAuth` | Yes | Unknown |
| [Studio Ghibli](https://ghibliapi.herokuapp.com) | Resources from Studio Ghibli films | No | Yes | Yes |
| [Trace Moe](https://soruly.github.io/trace.moe-api/#/) | A useful tool to get the exact scene of an anime from a screenshot | No | Yes | No |
| [Waifu.im](https://waifu.im/docs) | Get waifu pictures from an archive of over 4000 images and multiple tags | No | Yes | Yes |
| [Waifu.pics](https://waifu.pics/docs) | Image sharing platform for anime images | No | Yes | No |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;

### Anti-Malware
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AbuseIPDB](https://docs.abuseipdb.com/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [AlienVault Open Threat Exchange (OTX)](https://otx.alienvault.com/api) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [CAPEsandbox](https://capev2.readthedocs.io/en/latest/usage/api.html) | Malware execution and analysis | `apiKey` | Yes | Unknown |
| [Google Safe Browsing](https://developers.google.com/safe-browsing/) | Google Link/Domain Flagging | `apiKey` | Yes | Unknown |
| [MalDatabase](https://maldatabase.com/api-doc.html) | Provide malware datasets and threat intelligence feeds | `apiKey` | Yes | Unknown |
| [MalShare](https://malshare.com/doc.php) | Malware Archive / file sourcing | `apiKey` | Yes | No |
| [MalwareBazaar](https://bazaar.abuse.ch/api/) | Collect and share malware samples | `apiKey` | Yes | Unknown |
| [Metacert](https://metacert.com/) | Metacert Link Flagging | `apiKey` | Yes | Unknown |
| [NoPhishy](https://rapidapi.com/Amiichu/api/exerra-phishing-check/) | Check links to see if they&#039;re known phishing attempts | `apiKey` | Yes | Yes |
| [Phisherman](https://phisherman.gg/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [Scanii](https://docs.scanii.com/) | Simple REST API that can scan submitted documents/files for the presence of threats | `apiKey` | Yes | Yes |
| [URLhaus](https://urlhaus-api.abuse.ch/) | Bulk queries and Download Malware Samples | No | Yes | Yes |
| [URLScan.io](https://urlscan.io/about-api/) | Scan and Analyse URLs | `apiKey` | Yes | Unknown |
| [VirusTotal](https://www.virustotal.com/en/documentation/public-api/) | VirusTotal File/URL Analysis | `apiKey` | Yes | Unknown |
| [Web of Trust](https://support.mywot.com/hc/en-us/sections/360004477734-API-) | IP/domain/URL reputation | `apiKey` | Yes | Unknown | 

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Art &amp; Design
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [Am√©thyste](https://api.amethyste.moe/) | Generate images for Discord users | `apiKey` | Yes | Unknown |
| [Art Institute of Chicago](https://api.artic.edu/docs/) | Art | No | Yes | Yes |
| [Colormind](http://colormind.io/api-access/) | Color scheme generator | No | No | Unknown |
| [ColourLovers](http://www.colourlovers.com/api) | Get various patterns, palettes and images | No | No | Unknown |
| [Cooper Hewitt](https://collection.cooperhewitt.org/api) | Smithsonian Design Museum | `apiKey` | Yes | Unknown |
| [Dribbble](https://developer.dribbble.com) | Discover the world‚Äôs top designers &amp; creatives | `OAuth` | Yes | Unknown |
| [EmojiHub](https://github.com/cheatsnake/emojihub) | Get emojis by categories and groups | No | Yes | Yes |
| [Europeana](https://pro.europeana.eu/resources/apis/search) | European Museum and Galleries content | `apiKey` | Yes | Unknown |
| [Harvard Art Museums](https://github.com/harvardartmuseums/api-docs) | Art | `apiKey` | No | Unknown |
| [Icon Horse](https://icon.horse) | Favicons for any website, with fallbacks | No | Yes | Yes |
| [Iconfinder](https://developer.iconfinder.com) | Icons | `apiKey` | Yes | Unknown |
| [Icons8](https://img.icons8.com/) | Icons (find &quot;search icon&quot; hyperlink in page) | No | Yes | Unknown |
| [Lordicon](https://lordicon.com/) | Icons with predone Animations | No | Yes | Yes |
| [Metropolitan Museum of Art](https://metmuseum.github.io/) | Met Museum of Art | No | Yes | No |
| [Noun Project](http://api.thenounproject.com/index.html) | Icons | `OAuth` | No | Unknown |
| [PHP-Noise](https://php-noise.com/) | Noise Background Image Generator | No | Yes | Yes |
| [Pixel Encounter](https://pixelencounter.com/api) | SVG Icon Generator | No | Yes | No |
| [Rijksmuseum](https://data.rijksmuseum.nl/object-metadata/api/) | RijksMuseum Data | `apiKey` | Yes | Unknown |
| [Word Cloud](https://wordcloudapi.com/) | Easily create word clouds | `apiKey` | Yes | Unknown |
| [xColors](https://x-colors.herokuapp.com/) | Generate &amp; convert colors | No | Yes | Yes |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Authentication &amp; Authorization
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [Auth0](https://auth0.com) | Easy to implement, adaptable authentication and authorization platform | `apiKey` | Yes | Yes |
| [GetOTP](https://otp.dev/en/docs/) | Implement OTP flow quickly | `apiKey` | Yes | No |
| [Micro User Service](https://m3o.com/user) | User management and authentication | `apiKey` | Yes | No |
| [MojoAuth](https://mojoauth.com) | Secure and modern passwordless authentication platform | `apiKey` | Yes | Yes |
| [SAWO Labs](https://sawolabs.com) | Simplify login and improve user experience by integrating passwordless authentication in your app | `apiKey` | Yes | Yes |
| [Stytch](https://stytch.com/) | User infrastructure for modern applications | `apiKey` | Yes | No |
| [Warrant](https://warrant.dev/) | APIs for authorization and access control | `apiKey` | Yes | Yes |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Blockchain
| API | Description | Auth | HTTPS | CORS |
|---|:---|:---|:---|:---|
| [Bitquery](https://graphql.bitquery.io/ide) | Onchain GraphQL APIs &amp; DEX APIs | `apiKey` | Yes | Yes |
| [Chainlink](https://chain.link/developer-resources) | Build hybrid smart contracts with Chainlink | No | Yes | Unknown |
| [Chainpoint](https://tierion.com/chainpoint/) | Chainpoint is a global network for anchoring data to the Bitcoin blockchain | No | Yes | Unknown |
| [Covalent](https://www.covalenthq.com/docs/api/) | Multi-blockchain data aggregator platform | `apiKey` | Yes | Unknown |
| [Etherscan](https://etherscan.io/apis) | Ethereum explorer API | `apiKey` | Yes | Yes |
| [Helium](https://docs.helium.com/api/blockchain/introduction/) | Helium is a global, distributed network of Hotspots that create public, long-range wireless coverage | No | Yes | Unknown |
| [Nownodes](https://nownodes.io/) | Blockchain-as-a-service solution that provides high-quality connection via API | `apiKey` | Yes | Unknown |
| [Steem](https://developers.steem.io/) | Blockchain-based blogging and social media website | No | No | No |
| [The Graph](https://thegraph.com) | Indexing protocol for querying networks like Ethereum with GraphQL | `apiKey` | Yes | Unknown |
| [Walltime](https://walltime.info/api.html) | To retrieve Walltime&#039;s market info | No | Yes | Unknown |
| [Watchdata](https://docs.watchdata.io) | Provide simple and reliable API access to Ethereum blockchain | `apiKey` | Yes | Unknown |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Books
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [A B√≠blia Digital](https://www.abibliadigital.com.br/en) | Do not worry about managing the multiple versions of the Bible | `apiKey` | Yes | No |
| [Bhagavad Gita](https://docs.bhagavadgitaapi.in) | Open Source Shrimad Bhagavad Gita API including 21+ authors translation in Sanskrit/English/Hindi | `apiKey` | Yes | Yes |
| [Bhagavad Gita](https://bhagavadgita.io/api) | Bhagavad Gita text | `OAuth` | Yes | Yes |
| [Bhagavad Gita telugu](https://gita-api.vercel.app) | Bhagavad Gita API in telugu and odia languages | No | Yes | Yes |
| [Bible-api](https://bible-api.com/) | Free Bible API with multiple languages | No | Yes | Yes |
| [British National Bibliography](http://bnb.data.bl.uk/) | Books | No | No | Unknown |
| [Crossref Metadata Search](https://github.com/CrossRef/rest-api-doc) | Books &amp; Articles Metadata | No | Yes | Unknown |
| [Ganjoor](https://api.ganjoor.net) | Classic Persian poetry works including access to related manuscripts, recitations and music tracks | `OAuth` | Yes | Yes |
| [Google Books](https://developers.google.com/books/) | Books | `OAuth` | Yes | Unknown |
| [GurbaniNow](https://github.com/GurbaniNow/api) | Fast and Accurate Gurbani RESTful API | No | Yes | Unknown |
| [Gutendex](https://gutendex.com/) | Web-API for fetching data from Project Gutenberg Books Library | No | Yes | Unknown |
| [Open Library](https://openlibrary.org/developers/api) | Books, book covers and related data | No | Yes | No |
| [Penguin Publishing](http://www.penguinrandomhouse.biz/webservices/rest/) | Books, book covers and related data | No | Yes | Yes |
| [PoetryDB](https://github.com/thundercomb/poetrydb#readme) | Enables you to get instant data from our vast p

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jianchang512/pyvideotrans]]></title>
            <link>https://github.com/jianchang512/pyvideotrans</link>
            <guid>https://github.com/jianchang512/pyvideotrans</guid>
            <pubDate>Thu, 07 Aug 2025 00:04:41 GMT</pubDate>
            <description><![CDATA[Translate the video from one language to another and add dubbing. Â∞ÜËßÜÈ¢ë‰ªé‰∏ÄÁßçËØ≠Ë®ÄÁøªËØë‰∏∫Âè¶‰∏ÄÁßçËØ≠Ë®ÄÔºåÂêåÊó∂ÊîØÊåÅËØ≠Èü≥ËØÜÂà´ËΩ¨ÂΩï„ÄÅËØ≠Èü≥ÂêàÊàê„ÄÅÂ≠óÂπïÁøªËØë„ÄÇ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jianchang512/pyvideotrans">jianchang512/pyvideotrans</a></h1>
            <p>Translate the video from one language to another and add dubbing. Â∞ÜËßÜÈ¢ë‰ªé‰∏ÄÁßçËØ≠Ë®ÄÁøªËØë‰∏∫Âè¶‰∏ÄÁßçËØ≠Ë®ÄÔºåÂêåÊó∂ÊîØÊåÅËØ≠Èü≥ËØÜÂà´ËΩ¨ÂΩï„ÄÅËØ≠Èü≥ÂêàÊàê„ÄÅÂ≠óÂπïÁøªËØë„ÄÇ</p>
            <p>Language: Python</p>
            <p>Stars: 13,479</p>
            <p>Forks: 1,538</p>
            <p>Stars today: 31 stars today</p>
            <h2>README</h2><pre>ÁÆÄ‰Ωì‰∏≠Êñá | [English](docs/EN/README_EN.md)  | [ÊçêÂä©](docs/about.md) | ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑Ôºö`pyvideotrans`

# ËßÜÈ¢ëÁøªËØëÈÖçÈü≥Â∑•ÂÖ∑

ËøôÊòØ‰∏Ä‰∏™ËßÜÈ¢ëÁøªËØëÈÖçÈü≥Â∑•ÂÖ∑ÔºåÂèØÂ∞Ü‰∏ÄÁßçËØ≠Ë®ÄÁöÑËßÜÈ¢ëÁøªËØë‰∏∫ÊåáÂÆöËØ≠Ë®ÄÁöÑËßÜÈ¢ëÔºåËá™Âä®ÁîüÊàêÂíåÊ∑ªÂä†ËØ•ËØ≠Ë®ÄÁöÑÂ≠óÂπïÂíåÈÖçÈü≥„ÄÇÂπ∂ÊîØÊåÅAPIË∞ÉÁî®(v3.0‰ª•‰∏ãÁâàÊú¨Ôºå‰ª•‰∏ä‰πÖÊú™Êõ¥Êñ∞)


ËØ≠Èü≥ËØÜÂà´ÊîØÊåÅ `faster-whisper/openai-whisper/OpenAI API/Deepgram.com/Gemini/Parakeet/GoogleSpeech/ÈòøÈáåFunasr/Â≠óËäÇÁÅ´Â±±Á≠â`ÔºåÂπ∂ÊîØÊåÅËá™ÂÆö‰πâËØ≠Èü≥ËØÜÂà´api.

ÊñáÂ≠óÁøªËØëÊîØÊåÅ `ÂæÆËΩØÁøªËØë|GoogleÁøªËØë|ÁôæÂ∫¶ÁøªËØë|ËÖæËÆØÁøªËØë|ChatGPT|AzureAI|Gemini|DeepSeek|claude|DeepL|DeepLX|Â≠óËäÇÁÅ´Â±±|Á¶ªÁ∫øÁøªËØëOTT|Ëá™ÂÆö‰πâAPIÁ≠â`

ÊñáÂ≠óÂêàÊàêËØ≠Èü≥ÊîØÊåÅ `Edge tts` `Google tts` `Azure AI TTS` `Openai TTS` `Elevenlabs TTS` `Ëá™ÂÆö‰πâTTSÊúçÂä°Âô®api` `GPT-SoVITS` `F5-TTS` `Index-tts` `ChatterBox` `Gemini-tts` [clone-voice](https://github.com/jianchang512/clone-voice)  [ChatTTS-ui](https://github.com/jianchang512/ChatTTS-ui)  [Fish TTS](https://github.com/fishaudio/fish-speech)  [CosyVoice](https://github.com/FunAudioLLM/CosyVoice) 

ÂÖÅËÆ∏‰øùÁïôËÉåÊôØ‰º¥Â•èÈü≥‰πêÁ≠â(Âü∫‰∫éuvr5)

ÊîØÊåÅÁöÑËØ≠Ë®ÄÔºö‰∏≠ÊñáÁÆÄÁπÅ„ÄÅËã±ËØ≠„ÄÅÈü©ËØ≠„ÄÅÊó•ËØ≠„ÄÅ‰øÑËØ≠„ÄÅÊ≥ïËØ≠„ÄÅÂæ∑ËØ≠„ÄÅÊÑèÂ§ßÂà©ËØ≠„ÄÅË•øÁè≠ÁâôËØ≠„ÄÅËë°ËêÑÁâôËØ≠„ÄÅË∂äÂçóËØ≠„ÄÅÊ≥∞ÂõΩËØ≠„ÄÅÈòøÊãâ‰ºØËØ≠„ÄÅÂúüËÄ≥ÂÖ∂ËØ≠„ÄÅÂåàÁâôÂà©ËØ≠„ÄÅÂç∞Â∫¶ËØ≠„ÄÅ‰πåÂÖãÂÖ∞ËØ≠„ÄÅÂìàËê®ÂÖãËØ≠„ÄÅÂç∞Â∞ºËØ≠„ÄÅÈ©¨Êù•ËØ≠„ÄÅÊç∑ÂÖãËØ≠„ÄÅÊ≥¢ÂÖ∞ËØ≠„ÄÅËç∑ÂÖ∞ËØ≠„ÄÅÁëûÂÖ∏ËØ≠„ÄÅËä¨ÂÖ∞ËØ≠„ÄÅËè≤ÂæãÂÆæËØ≠„ÄÅÂ≠üÂä†ÊãâËØ≠„ÄÅ‰πåÂ∞îÈÉΩËØ≠„ÄÅÂ∏å‰ºØÊù•ËØ≠„ÄÅÁ≤§ËØ≠Á≠âÔºåÂÖ∂‰ªñËØ≠Ë®ÄÂèØÈÄâËá™Âä®Ê£ÄÊµã


&gt; **[ËµûÂä©ÂïÜ]**
&gt; 
&gt; [![](https://github.com/user-attachments/assets/5348c86e-2d5f-44c7-bc1b-3cc5f077e710)](https://gpt302.saaslink.net/teRK8Y)
&gt;  [302.AI](https://gpt302.saaslink.net/teRK8Y)ÊòØ‰∏Ä‰∏™ÊåâÈúÄ‰ªòË¥πÁöÑ‰∏ÄÁ´ôÂºèAIÂ∫îÁî®Âπ≥Âè∞ÔºåÂºÄÊîæÂπ≥Âè∞ÔºåÂºÄÊ∫êÁîüÊÄÅ, [302.AIÂºÄÊ∫êÂú∞ÂùÄ](https://gpt302.saaslink.net/teRK8Y)
&gt; 
&gt; ÈõÜÂêà‰∫ÜÊúÄÊñ∞ÊúÄÂÖ®ÁöÑAIÊ®°ÂûãÂíåÂìÅÁâå/ÊåâÈúÄ‰ªòË¥πÈõ∂ÊúàË¥π/ÁÆ°ÁêÜÂíå‰ΩøÁî®ÂàÜÁ¶ª/ÊâÄÊúâAIËÉΩÂäõÂùáÊèê‰æõAPI/ÊØèÂë®Êé®Âá∫2-3‰∏™Êñ∞Â∫îÁî®


## ‰∏ªË¶ÅÁî®ÈÄîÂíåÂäüËÉΩ

„ÄêËá™Âä®ÁøªËØëËßÜÈ¢ëÂπ∂ÈÖçÈü≥„ÄëÂ∞ÜËßÜÈ¢ë‰∏≠ÁöÑÂ£∞Èü≥ÁøªËØë‰∏∫Âè¶‰∏ÄÁßçËØ≠Ë®ÄÁöÑÈÖçÈü≥ÔºåÂπ∂ÂµåÂÖ•ËØ•ËØ≠Ë®ÄÂ≠óÂπï

„ÄêËØ≠Èü≥ËØÜÂà´/Â∞ÜÈü≥È¢ëËßÜÈ¢ëËΩ¨‰∏∫Â≠óÂπï„ÄëÂèØÊâπÈáèÂ∞ÜÈü≥È¢ë„ÄÅËßÜÈ¢ëÊñá‰ª∂‰∏≠ÁöÑ‰∫∫Á±ªËØ¥ËØùÂ£∞ÔºåËØÜÂà´‰∏∫ÊñáÂ≠óÂπ∂ÂØºÂá∫‰∏∫srtÂ≠óÂπïÊñá‰ª∂

„ÄêËØ≠Èü≥ÂêàÊàê/Â≠óÂπïÈÖçÈü≥„ÄëÊ†πÊçÆÊú¨Âú∞Â∑≤ÊúâÁöÑsrtÂ≠óÂπïÊñá‰ª∂ÂàõÂª∫ÈÖçÈü≥ÔºåÊîØÊåÅÂçï‰∏™ÊàñÊâπÈáèÂ≠óÂπï

„ÄêÂ≠óÂπïÂ§öËßíËâ≤ÈÖçÈü≥„ÄëÂØºÂÖ•Êú¨Âú∞Â∑≤ÊúâÁöÑsrtÂ≠óÂπïÔºå‰∏∫ÊØèÊù°Â≠óÂπïÊåáÂÆö‰∏Ä‰∏™ÈÖçÈü≥ËßíËâ≤ÔºåÂàõÂª∫Â§öËßíËâ≤ÈÖçÈü≥Êñá‰ª∂

„ÄêÁøªËØëÂ≠óÂπïÊñá‰ª∂„ÄëÂ∞Ü‰∏Ä‰∏™ÊàñÂ§ö‰∏™srtÂ≠óÂπïÊñá‰ª∂ÁøªËØë‰∏∫ÂÖ∂‰ªñËØ≠Ë®ÄÁöÑÂ≠óÂπïÊñá‰ª∂

„ÄêÂêàÂπ∂ËßÜÈ¢ëÂíåÈü≥È¢ë„ÄëÊâπÈáèÂ∞ÜËßÜÈ¢ëÊñá‰ª∂ÂíåÈü≥È¢ëÊñá‰ª∂‰∏Ä‰∏ÄÂØπÂ∫îÂêàÂπ∂

„ÄêÂêàÂπ∂ËßÜÈ¢ëÂíåsrtÂ≠óÂπï„ÄëÊâπÈáèÂ∞ÜËßÜÈ¢ëÊñá‰ª∂srtÂ≠óÂπïÊñá‰ª∂‰∏Ä‰∏ÄÂØπÂ∫îÂêàÂπ∂

„Äê‰∏∫ËßÜÈ¢ëÊ∑ªÂä†ÂõæÁâáÊ∞¥Âç∞„ÄëÊâπÈáèÂ∞ÜËßÜÈ¢ëÊñá‰ª∂‰∏≠ÂµåÂÖ•ÂõæÁâáÊ∞¥Âç∞

„Äê‰ªéËßÜÈ¢ë‰∏≠ÊèêÂèñÈü≥È¢ë„Äë‰ªéËßÜÈ¢ë‰∏≠ÂàÜÁ¶ª‰∏∫Èü≥È¢ëÊñá‰ª∂ÂíåÊó†Â£∞ËßÜÈ¢ë

„ÄêÈü≥È¢ëËßÜÈ¢ëÊ†ºÂºèËΩ¨Êç¢„ÄëÊâπÈáèÂ∞ÜÈü≥È¢ëËßÜÈ¢ëËøõË°åÊ†ºÂºèËΩ¨Êç¢

„ÄêÂ≠óÂπïÁºñËæëÂπ∂ÂØºÂá∫Â§öÊ†ºÂºè„ÄëÊîØÊåÅÂØºÂÖ•srt„ÄÅvtt„ÄÅassÊ†ºÂºèÂ≠óÂπïÔºåÁºñËæëÂêéÂèØËÆæÁΩÆÂ≠ó‰ΩìÊ†∑Âºè„ÄÅËâ≤ÂΩ©Á≠âÂØºÂá∫ÂØπÂ∫îÊ†ºÂºèÂ≠óÂπï

„ÄêÂ≠óÂπïÊ†ºÂºèËΩ¨Êç¢„ÄëÊâπÈáèÂ∞ÜÂ≠óÂπïÊñá‰ª∂ËøõË°å srt/ass/vtt Ê†ºÂºè‰∫íËΩ¨

„Äê‰∫∫Â£∞ËÉåÊôØ‰πêÂàÜÁ¶ª„Äë

„ÄêAPIË∞ÉÁî®&lt;v3.0„ÄëÊîØÊåÅ ËØ≠Èü≥ÂêàÊàê„ÄÅËØ≠Ë®ÄËØÜÂà´„ÄÅÂ≠óÂπïÁøªËØë„ÄÅËßÜÈ¢ëÁøªËØëÊé•Âè£Ë∞ÉÁî®

----

![pyvideotrans-home](https://github.com/user-attachments/assets/b2f95a7f-b4e5-4a6d-b2a5-eb6cd22531e0)

[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&amp;logo=googlecolab&amp;color=525252)](https://colab.research.google.com/drive/1kPTeAMz3LnWRnGmabcz4AWW42hiehmfm?usp=sharing)

## È¢ÑÊâìÂåÖÁâàÊú¨(‰ªÖwin10/win11ÂèØÁî®ÔºåMacOS/LinuxÁ≥ªÁªü‰ΩøÁî®Ê∫êÁ†ÅÈÉ®ÁΩ≤)

&gt; ‰ΩøÁî®pyinstallerÊâìÂåÖÔºåÊú™ÂÅöÂÖçÊùÄÂíåÁ≠æÂêçÔºåÊùÄËΩØÂèØËÉΩÊä•ÊØíÔºåËØ∑Âä†ÂÖ•‰ø°‰ªªÂêçÂçïÊàñ‰ΩøÁî®Ê∫êÁ†ÅÈÉ®ÁΩ≤

0. [ÁÇπÂáªÂéª‰∏ãËΩΩÈ¢ÑÊâìÂåÖÁâà,Ëß£ÂéãÂà∞Êó†Á©∫Ê†ºÁöÑËã±ÊñáÁõÆÂΩïÂêéÔºåÂèåÂáª sp.exe (https://github.com/jianchang512/pyvideotrans/releases)

1. Ëß£ÂéãÂà∞Ëã±ÊñáË∑ØÂæÑ‰∏ãÔºåÂπ∂‰∏îË∑ØÂæÑ‰∏≠‰∏çÂê´ÊúâÁ©∫Ê†º„ÄÇËß£ÂéãÂêéÂèåÂáª sp.exe  (Ëã•ÈÅáÂà∞ÊùÉÈôêÈóÆÈ¢òÂèØÂè≥ÈîÆ‰ΩøÁî®ÁÆ°ÁêÜÂëòÊùÉÈôêÊâìÂºÄ)

4. Ê≥®ÊÑèÔºöÂøÖÈ°ªËß£ÂéãÂêé‰ΩøÁî®Ôºå‰∏çÂèØÁõ¥Êé•ÂéãÁº©ÂåÖÂÜÖÂèåÂáª‰ΩøÁî®Ôºå‰πü‰∏çÂèØËß£ÂéãÂêéÁßªÂä®sp.exeÊñá‰ª∂Âà∞ÂÖ∂‰ªñ‰ΩçÁΩÆ



## MacOSÊ∫êÁ†ÅÈÉ®ÁΩ≤

0. ÊâìÂºÄÁªàÁ´ØÁ™óÂè£ÔºåÂàÜÂà´ÊâßË°åÂ¶Ç‰∏ãÂëΩ‰ª§
	
	&gt; ÊâßË°åÂâçÁ°Æ‰øùÂ∑≤ÂÆâË£Ö HomebrewÔºåÂ¶ÇÊûú‰Ω†Ê≤°ÊúâÂÆâË£Ö Homebrew,ÈÇ£‰πàÈúÄË¶ÅÂÖàÂÆâË£Ö
	&gt;
	&gt; ÊâßË°åÂëΩ‰ª§ÂÆâË£Ö HomebrewÔºö  `/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot;`
	&gt;
	&gt; ÂÆâË£ÖÂÆåÊàêÂêéÔºåÊâßË°åÔºö `eval $(brew --config)`
	&gt;

    ```
    brew install libsndfile

    brew install ffmpeg

    brew install git

    brew install python@3.10

    ```

    ÁªßÁª≠ÊâßË°å

    ```
    export PATH=&quot;/usr/local/opt/python@3.10/bin:$PATH&quot;

    source ~/.bash_profile 
	
	source ~/.zshrc

    ```



1. ÂàõÂª∫‰∏çÂê´Á©∫Ê†ºÂíå‰∏≠ÊñáÁöÑÊñá‰ª∂Â§πÔºåÂú®ÁªàÁ´Ø‰∏≠ËøõÂÖ•ËØ•Êñá‰ª∂Â§π„ÄÇ
2. ÁªàÁ´Ø‰∏≠ÊâßË°åÂëΩ‰ª§ `git clone https://github.com/jianchang512/pyvideotrans `
3. ÊâßË°åÂëΩ‰ª§ `cd pyvideotrans`
4. ÁªßÁª≠ÊâßË°å `python -m venv venv`
5. ÁªßÁª≠ÊâßË°åÂëΩ‰ª§ `source ./venv/bin/activate`ÔºåÊâßË°åÂÆåÊØïÊü•ÁúãÁ°ÆËÆ§ÁªàÁ´ØÂëΩ‰ª§ÊèêÁ§∫Á¨¶Â∑≤ÂèòÊàêÂ∑≤`(venv)`ÂºÄÂ§¥,‰ª•‰∏ãÂëΩ‰ª§ÂøÖÈ°ªÁ°ÆÂÆöÁªàÁ´ØÊèêÁ§∫Á¨¶ÊòØ‰ª•`(venv)`ÂºÄÂ§¥
6. ÊâßË°å `pip install -r requirements.txt `ÔºåÂ¶ÇÊûúÊèêÁ§∫Â§±Ë¥•ÔºåÊâßË°åÂ¶Ç‰∏ã2Êù°ÂëΩ‰ª§ÂàáÊç¢pipÈïúÂÉèÂà∞ÈòøÈáåÈïúÂÉè

    ```
    pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/
    pip config set install.trusted-host mirrors.aliyun.com
    ```

    ÁÑ∂ÂêéÈáçÊñ∞ÊâßË°å
    Â¶ÇÊûúÂ∑≤ÂàáÊç¢Âà∞ÈòøÈáåÈïúÂÉèÊ∫êÔºå‰ªçÊèêÁ§∫Â§±Ë¥•ÔºåËØ∑Â∞ùËØïÊâßË°å `pip install -r requirements.txt`

7. `python sp.py` ÊâìÂºÄËΩØ‰ª∂ÁïåÈù¢



## Linux Ê∫êÁ†ÅÈÉ®ÁΩ≤

0. CentOS/RHELÁ≥ª‰æùÊ¨°ÊâßË°åÂ¶Ç‰∏ãÂëΩ‰ª§ÂÆâË£Ö python3.10

```

sudo yum update

sudo yum groupinstall &quot;Development Tools&quot;

sudo yum install openssl-devel bzip2-devel libffi-devel

cd /tmp

wget https://www.python.org/ftp/python/3.10.4/Python-3.10.4.tgz

tar xzf Python-3.10.4.tgz

cd Python-3.10.4

./configure ‚Äî enable-optimizations

sudo make &amp;&amp; sudo make install

sudo alternatives ‚Äî install /usr/bin/python3 python3 /usr/local/bin/python3.10 1

sudo yum install -y ffmpeg

```

1. Ubuntu/DebianÁ≥ªÊâßË°åÂ¶Ç‰∏ãÂëΩ‰ª§ÂÆâË£Öpython3.10

```

apt update &amp;&amp; apt upgrade -y

apt install software-properties-common -y

add-apt-repository ppa:deadsnakes/ppa

apt update

sudo apt-get install libxcb-cursor0

apt install python3.10

curl -sS https://bootstrap.pypa.io/get-pip.py | python3.10

sudo update-alternatives --install /usr/bin/python python /usr/local/bin/python3.10  1

sudo update-alternatives --config python

apt-get install ffmpeg

```


**ÊâìÂºÄ‰ªªÊÑè‰∏Ä‰∏™ÁªàÁ´ØÔºåÊâßË°å `python3 -V`ÔºåÂ¶ÇÊûúÊòæÁ§∫ ‚Äú3.10.4‚ÄùÔºåËØ¥ÊòéÂÆâË£ÖÊàêÂäüÔºåÂê¶ÂàôÂ§±Ë¥•**


1. ÂàõÂª∫‰∏™‰∏çÂê´Á©∫Ê†ºÂíå‰∏≠ÊñáÁöÑÊñá‰ª∂Â§πÔºå ‰ªéÁªàÁ´ØÊâìÂºÄËØ•Êñá‰ª∂Â§π„ÄÇ
3. ÁªàÁ´Ø‰∏≠ÊâßË°åÂëΩ‰ª§ `git clone https://github.com/jianchang512/pyvideotrans`
4. ÁªßÁª≠ÊâßË°åÂëΩ‰ª§ `cd pyvideotrans`
5. ÁªßÁª≠ÊâßË°å `python -m venv venv`
6. ÁªßÁª≠ÊâßË°åÂëΩ‰ª§ `source .\venv\scripts\activate`ÔºåÊâßË°åÂÆåÊØïÊü•ÁúãÁ°ÆËÆ§ÁªàÁ´ØÂëΩ‰ª§ÊèêÁ§∫Á¨¶Â∑≤ÂèòÊàêÂ∑≤`(venv)`ÂºÄÂ§¥,‰ª•‰∏ãÂëΩ‰ª§ÂøÖÈ°ªÁ°ÆÂÆöÁªàÁ´ØÊèêÁ§∫Á¨¶ÊòØ‰ª•`(venv)`ÂºÄÂ§¥
7. ÊâßË°å `pip install -r requirements.txt`ÔºåÂ¶ÇÊûúÊèêÁ§∫Â§±Ë¥•ÔºåÊâßË°åÂ¶Ç‰∏ã2Êù°ÂëΩ‰ª§ÂàáÊç¢pipÈïúÂÉèÂà∞ÈòøÈáåÈïúÂÉè

    ```

    pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/
    pip config set install.trusted-host mirrors.aliyun.com

    ```

    ÁÑ∂ÂêéÈáçÊñ∞ÊâßË°å,Â¶ÇÊûúÂ∑≤ÂàáÊç¢Âà∞ÈòøÈáåÈïúÂÉèÊ∫êÔºå‰ªçÊèêÁ§∫Â§±Ë¥•ÔºåËØ∑Â∞ùËØïÊâßË°å `pip install -r requirements.txt `
8. Â¶ÇÊûúË¶Å‰ΩøÁî®CUDAÂä†ÈÄüÔºåÂàÜÂà´ÊâßË°å

    `pip uninstall -y torch torchaudio`

    `pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu126`

    `pip install nvidia-cublas-cu12 nvidia-cudnn-cu12`

9. linux Â¶ÇÊûúË¶ÅÂêØÁî®cudaÂä†ÈÄüÔºåÂøÖÈ°ªÊúâËã±‰ºüËææÊòæÂç°ÔºåÂπ∂‰∏îÈÖçÁΩÆÂ•Ω‰∫ÜCUDA12+ÁéØÂ¢É,ËØ∑Ëá™Ë°åÊêúÁ¥¢ &quot;Linux CUDA ÂÆâË£Ö&quot;


10. `python sp.py` ÊâìÂºÄËΩØ‰ª∂ÁïåÈù¢


## Window10/11 Ê∫êÁ†ÅÈÉ®ÁΩ≤

0. ÊâìÂºÄ https://www.python.org/downloads/ ‰∏ãËΩΩ windows3.10Ôºå‰∏ãËΩΩÂêéÂèåÂáªÔºå‰∏ÄË∑ØnextÔºåÊ≥®ÊÑèË¶ÅÈÄâ‰∏≠‚ÄúAdd to PATH‚Äù

   **ÊâìÂºÄ‰∏Ä‰∏™cmdÔºåÊâßË°å `python -V`ÔºåÂ¶ÇÊûúËæìÂá∫‰∏çÊòØ `3.10.4`,ËØ¥ÊòéÂÆâË£ÖÂá∫ÈîôÔºåÊàñÊ≤°ÊúâÂä†ÂÖ• `Add to PATH`,ËØ∑ÈáçÊñ∞ÂÆâË£Ö**

1. ÊâìÂºÄ https://github.com/git-for-windows/git/releases/download/v2.45.0.windows.1/Git-2.45.0-64-bit.exe Ôºå‰∏ãËΩΩgitÔºå‰∏ãËΩΩÂêéÂèåÂáª‰∏ÄË∑Ø‰∏ã‰∏ÄÊ≠•„ÄÇ
2. Êâæ‰∏™‰∏çÂê´Á©∫Ê†ºÂíå‰∏≠ÊñáÁöÑÊñá‰ª∂Â§πÔºåÂú∞ÂùÄÊ†è‰∏≠ËæìÂÖ• `cmd`ÂõûËΩ¶ÔºåÊâìÂºÄÁªàÁ´ØÔºå‰ª•‰∏ãÂëΩ‰ª§ÂùáÂú®ËØ•ÁªàÁ´Ø‰∏≠ÊâßË°å
3. ÊâßË°åÂëΩ‰ª§ `git clone https://github.com/jianchang512/pyvideotrans`
4. ÁªßÁª≠ÊâßË°åÂëΩ‰ª§ `cd pyvideotrans`
5. ÁªßÁª≠ÊâßË°å `python -m venv venv`
6. ÁªßÁª≠ÊâßË°åÂëΩ‰ª§ `venv\Scripts\activate`,ÊâßË°åÂêéËØ∑Êü•ÁúãÁ°ÆËÆ§ÂëΩ‰ª§Ë°åÂºÄÂ§¥Â∑≤ÂèòÊàê‰∫Ü`(venv)`,Âê¶ÂàôËØ¥ÊòéÂá∫Èîô
7. ÊâßË°å `pip install -r requirements.txt `ÔºåÂ¶ÇÊûúÊèêÁ§∫Â§±Ë¥•ÔºåÊâßË°åÂ¶Ç‰∏ã2Êù°ÂëΩ‰ª§ÂàáÊç¢pipÈïúÂÉèÂà∞ÈòøÈáåÈïúÂÉè

    ```

    pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/
    pip config set install.trusted-host mirrors.aliyun.com

    ```

    ÁÑ∂ÂêéÈáçÊñ∞ÊâßË°å,Â¶ÇÊûúÂ∑≤ÂàáÊç¢Âà∞ÈòøÈáåÈïúÂÉèÊ∫êÔºå‰ªçÊèêÁ§∫Â§±Ë¥•ÔºåËØ∑Â∞ùËØïÊâßË°å `pip install -r requirements.txt`
8.  Â¶ÇÊûúË¶Å‰ΩøÁî®CUDAÂä†ÈÄüÔºåÂàÜÂà´ÊâßË°å

    `pip uninstall -y torch torchaudio`

    `pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu126`


9. windows  Â¶ÇÊûúË¶ÅÂêØÁî®cudaÂä†ÈÄüÔºåÂøÖÈ°ªÊúâËã±‰ºüËææÊòæÂç°ÔºåÂπ∂‰∏îÈÖçÁΩÆÂ•Ω‰∫ÜCUDA12+ÁéØÂ¢ÉÔºåÂÖ∑‰ΩìÂÆâË£ÖËßÅ [CUDAÂä†ÈÄüÊîØÊåÅ](https://pyvideotrans.com/gpu)

10. Ëß£Âéã ffmpeg.zip Âà∞ÂΩìÂâçÊ∫êÁ†ÅÁõÆÂΩï‰∏ãÔºåÊèêÁ§∫Ë¶ÜÁõñÂàôË¶ÜÁõñÔºåËß£ÂéãÂêéÁ°Æ‰øùÊ∫êÁ†Å‰∏ãÁöÑffmepgÊñá‰ª∂Â§πÂÜÖËÉΩÁúãÂà∞ ffmpeg.exe ffprobe.exe ytwin32.exe,

11. `python sp.py` ÊâìÂºÄËΩØ‰ª∂ÁïåÈù¢


## [‰ΩøÁî® UV ÈÉ®ÁΩ≤ÔºöÂèÇËÄÉÊñáÊ°£](https://pyvideotrans.com/blog/uv)


##  Ê∫êÁ†ÅÈÉ®ÁΩ≤ÈóÆÈ¢òËØ¥Êòé

1. ÈªòËÆ§‰ΩøÁî® ctranslate2ÁöÑ4.xÁâàÊú¨Ôºå‰ªÖÊîØÊåÅCUDA12.xÁâàÊú¨ÔºåÂ¶ÇÊûú‰Ω†ÁöÑcuda‰Ωé‰∫é12ÔºåÂπ∂‰∏îÊó†Ê≥ïÂçáÁ∫ßcudaÂà∞12.xÔºåËØ∑ÊâßË°åÂëΩ‰ª§Âç∏ËΩΩctranslate2ÁÑ∂ÂêéÈáçÊñ∞ÂÆâË£Ö

```

pip uninstall -y ctranslate2

pip install ctranslate2==3.24.0

```

2. ÂèØËÉΩ‰ºöÈÅáÂà∞ `xx module not found ` ‰πãÁ±ªÈîôËØØÔºåËØ∑ÊâìÂºÄ requirements.txtÔºåÊêúÁ¥¢ËØ• xx Ê®°ÂùóÔºåÁÑ∂ÂêéÂ∞ÜxxÂêéÁöÑ ==ÂèäÁ≠â‰ºöÂêéÁöÑÁâàÊú¨Âè∑ÂéªÊéâ




# ‰ΩøÁî®ÊïôÁ®ãÂíåÊñáÊ°£

ËØ∑Êü•Áúã https://pyvideotrans.com


# ËØ≠Èü≥ËØÜÂà´Ê®°Âûã:

   ‰∏ãËΩΩÂú∞ÂùÄÔºö https://pyvideotrans.com/model.html



# ËßÜÈ¢ëÊïôÁ®ã(Á¨¨‰∏âÊñπ)

[Mac‰∏ãÊ∫êÁ†ÅÈÉ®ÁΩ≤/bÁ´ô](https://www.bilibili.com/video/BV1tK421y7rd/)

[Áî®Gemini Api ÁªôËßÜÈ¢ëÁøªËØëËÆæÁΩÆÊñπÊ≥ï/bÁ´ô](https://b23.tv/fED1dS3)

[Â¶Ç‰Ωï‰∏ãËΩΩÂíåÂÆâË£Ö](https://www.bilibili.com/video/BV1Gr421s7cN/)


# ËΩØ‰ª∂È¢ÑËßàÊà™Âõæ

![pyvideotrans-home](https://github.com/user-attachments/assets/b2f95a7f-b4e5-4a6d-b2a5-eb6cd22531e0)

![image](https://github.com/user-attachments/assets/b5d1b5fb-c579-477c-bca4-6c5e9aa14d7d)



# Áõ∏ÂÖ≥ËÅîÈ°πÁõÆ

[ChatTTS-ui:‰ΩøÁî®ChatTTSÂêàÊàêÂ£∞Èü≥ÁöÑUIÁïåÈù¢](https://github.com/jianchang512/ChatTTS-ui)

[OTT:Êú¨Âú∞Á¶ªÁ∫øÊñáÂ≠óÁøªËØëÂ∑•ÂÖ∑](https://github.com/jianchang512/ott)

[Â£∞Èü≥ÂÖãÈöÜÂ∑•ÂÖ∑:Áî®‰ªªÊÑèÈü≥Ëâ≤ÂêàÊàêËØ≠Èü≥](https://github.com/jianchang512/clone-voice)

[ËØ≠Èü≥ËØÜÂà´Â∑•ÂÖ∑:Êú¨Âú∞Á¶ªÁ∫øÁöÑËØ≠Èü≥ËØÜÂà´ËΩ¨ÊñáÂ≠óÂ∑•ÂÖ∑](https://github.com/jianchang512/stt)

[‰∫∫Â£∞ËÉåÊôØ‰πêÂàÜÁ¶ª:‰∫∫Â£∞ÂíåËÉåÊôØÈü≥‰πêÂàÜÁ¶ªÂ∑•ÂÖ∑](https://github.com/jianchang512/vocal-separate)

[GPT-SoVITSÁöÑapi.pyÊîπËâØÁâà](https://github.com/jianchang512/gptsovits-api)

[ÈÄÇÈÖç CosyVoice ÁöÑ api.py](https://github.com/jianchang512/cosyvoice-api)


## Ëá¥Ë∞¢

&gt; Êú¨Á®ãÂ∫è‰∏ªË¶Å‰æùËµñÁöÑÈÉ®ÂàÜÂºÄÊ∫êÈ°πÁõÆ

1. [ffmpeg](https://github.com/FFmpeg/FFmpeg)
2. [PySide6](https://pypi.org/project/PySide6/)
3. [edge-tts](https://github.com/rany2/edge-tts)
4. [faster-whisper](https://github.com/SYSTRAN/faster-whisper)
5. [openai-whisper](https://github.com/openai/whisper)
6. [pydub](https://github.com/jiaaro/pydub)

## ÂÖ≥Ê≥®‰ΩúËÄÖÂæÆ‰ø°ÂÖ¨‰ºóÂè∑

&lt;img width=&quot;200&quot; src=&quot;https://github.com/jianchang512/pyvideotrans/assets/3378335/f9337111-9084-41fe-8840-1fb8fedca92d&quot;&gt;


Â¶ÇÊûúËßâÂæóËØ•È°πÁõÆÂØπ‰Ω†Êúâ‰ª∑ÂÄºÔºåÂπ∂Â∏åÊúõËØ•È°πÁõÆËÉΩ‰∏ÄÁõ¥Á®≥ÂÆöÊåÅÁª≠Áª¥Êä§ÔºåÊ¨¢ËøéÊçêÂä©

&lt;img width=&quot;200&quot; src=&quot;https://github.com/user-attachments/assets/5e8688ef-47c3-4a3c-a016-e60f73ccc4dc&quot;&gt;


&lt;img width=&quot;200&quot; src=&quot;https://github.com/jianchang512/pyvideotrans/assets/3378335/fe1aa29d-c26d-46d3-b7f3-e9c030ef32c7&quot;&gt;

&lt;img width=&quot;200&quot; src=&quot;https://pyvideotrans.com/images/biancn.jpg&quot;&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>