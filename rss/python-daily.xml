<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Fri, 15 Aug 2025 00:04:48 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[pathwaycom/pathway]]></title>
            <link>https://github.com/pathwaycom/pathway</link>
            <guid>https://github.com/pathwaycom/pathway</guid>
            <pubDate>Fri, 15 Aug 2025 00:04:48 GMT</pubDate>
            <description><![CDATA[Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pathwaycom/pathway">pathwaycom/pathway</a></h1>
            <p>Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.</p>
            <p>Language: Python</p>
            <p>Stars: 31,220</p>
            <p>Forks: 864</p>
            <p>Stars today: 287 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pathway.com/&quot;&gt;
    &lt;img src=&quot;https://pathway.com/logo-light.svg&quot;/&gt;
  &lt;/a&gt;
  &lt;br /&gt;&lt;br /&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/10388&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/10388&quot; alt=&quot;pathwaycom%2Fpathway | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
  &lt;br /&gt;&lt;br /&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml&quot;&gt;
        &lt;img src=&quot;https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml/badge.svg&quot; alt=&quot;ubuntu&quot;/&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/actions/workflows/release.yml&quot;&gt;
        &lt;img src=&quot;https://github.com/pathwaycom/pathway/actions/workflows/release.yml/badge.svg&quot; alt=&quot;Last release&quot;/&gt;&lt;/a&gt;
        &lt;a href=&quot;https://badge.fury.io/py/pathway&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/pathway.svg&quot; alt=&quot;PyPI version&quot; height=&quot;18&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://badge.fury.io/py/pathway&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/pathway&quot; alt=&quot;PyPI downloads&quot; height=&quot;18&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/blob/main/LICENSE.txt&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/license-BSL-green&quot; alt=&quot;License: BSL&quot;/&gt;&lt;/a&gt;
      &lt;br&gt;
        &lt;a href=&quot;https://discord.gg/pathway&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/discord/1042405378304004156?logo=discord&quot;
            alt=&quot;chat on Discord&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://twitter.com/intent/follow?screen_name=pathway_com&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/twitter/follow/pathwaycom&quot;
            alt=&quot;follow on Twitter&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://linkedin.com/company/pathway&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/pathway-0077B5?style=social&amp;logo=linkedin&quot; alt=&quot;follow on LinkedIn&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://github.com/dylanhogg/awesome-python/blob/main/README.md&quot;&gt;
      &lt;img src=&quot;https://awesome.re/badge.svg&quot; alt=&quot;Awesome Python&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://gurubase.io/g/pathway&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/badge/Gurubase-Ask%20Pathway%20Guru-006BFF&quot; alt=&quot;Pathway Guru&quot;&gt;&lt;/a&gt;
    &lt;br&gt;
    &lt;a href=&quot;#getting-started&quot;&gt;Getting Started&lt;/a&gt; |
    &lt;a href=&quot;#deployment&quot;&gt;Deployment&lt;/a&gt; |
    &lt;a href=&quot;#resources&quot;&gt;Documentation and Support&lt;/a&gt; |
    &lt;a href=&quot;https://pathway.com/blog/&quot;&gt;Blog&lt;/a&gt; |
    &lt;a href=&quot;#license&quot;&gt;License&lt;/a&gt;

  
&lt;/p&gt;

# Pathway&lt;a id=&quot;pathway&quot;&gt; Live Data Framework&lt;/a&gt;

[Pathway](https://pathway.com) is a Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.

Pathway comes with an **easy-to-use Python API**, allowing you to seamlessly integrate your favorite Python ML libraries.
Pathway code is versatile and robust: **you can use it in both development and production environments, handling both batch and streaming data effectively**.
The same code can be used for local development, CI/CD tests, running batch jobs, handling stream replays, and processing data streams.

Pathway is powered by a **scalable Rust engine** based on Differential Dataflow and performs incremental computation.
Your Pathway code, despite being written in Python, is run by the Rust engine, enabling multithreading, multiprocessing, and distributed computations.
All the pipeline is kept in memory and can be easily deployed with **Docker and Kubernetes**.

You can install Pathway with pip:
```
pip install -U pathway
```

For any questions, you will find the community and team behind the project [on Discord](https://discord.com/invite/pathway).

## Use-cases and templates

Ready to see what Pathway can do?

[Try one of our easy-to-run examples](https://pathway.com/developers/templates)!

Available in both notebook and docker formats, these ready-to-launch examples can be launched in just a few clicks. Pick one and start your hands-on experience with Pathway today!

### Event processing and real-time analytics pipelines
With its unified engine for batch and streaming and its full Python compatibility, Pathway makes data processing as easy as possible. It&#039;s the ideal solution for a wide range of data processing pipelines, including:

- [Showcase: Real-time ETL.](https://pathway.com/developers/templates/kafka-etl)
- [Showcase: Event-driven pipelines with alerting.](https://pathway.com/developers/templates/realtime-log-monitoring)
- [Showcase: Realtime analytics.](https://pathway.com/developers/templates/linear_regression_with_kafka/)
- [Docs: Switch from batch to streaming.](https://pathway.com/developers/user-guide/connecting-to-data/switch-from-batch-to-streaming)



### AI Pipelines

Pathway provides dedicated LLM tooling to build live LLM and RAG pipelines. Wrappers for most common LLM services and utilities are included, making working with LLMs and RAGs pipelines incredibly easy. Check out our [LLM xpack documentation](https://pathway.com/developers/user-guide/llm-xpack/overview).

Don&#039;t hesitate to try one of our runnable examples featuring LLM tooling.
You can find such examples [here](https://pathway.com/developers/user-guide/llm-xpack/llm-examples).

  - [Template: Unstructured data to SQL on-the-fly.](https://pathway.com/developers/templates/unstructured-to-structured/)
  - [Template: Private RAG with Ollama and Mistral AI](https://pathway.com/developers/templates/private-rag-ollama-mistral)
  - [Template: Adaptive RAG](https://pathway.com/developers/templates/adaptive-rag)
  - [Template: Multimodal RAG with gpt-4o](https://pathway.com/developers/templates/multimodal-rag)

## Features

- **A wide range of connectors**: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector.
- **Stateless and stateful transformations**: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data.
- **Persistence**: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!
- **Consistency**: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the &quot;at least once&quot; consistency while the enterprise version provides the &quot;exactly once&quot; consistency.
- **Scalable Rust engine**: with Pathway Rust engine, you are free from the usual limits imposed by Python. You can easily do multithreading, multiprocessing, and distributed computations.
- **LLM helpers**: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents.


## Getting started&lt;a id=&quot;getting-started&quot;&gt;&lt;/a&gt;

### Installation&lt;a id=&quot;installation&quot;&gt;&lt;/a&gt;

Pathway requires Python 3.10 or above.

You can install the current release of Pathway using `pip`:

```
$ pip install -U pathway
```

‚ö†Ô∏è Pathway is available on MacOS and Linux. Users of other systems should run Pathway on a Virtual Machine.


### Example: computing the sum of positive values in real time.&lt;a id=&quot;example&quot;&gt;&lt;/a&gt;

```python
import pathway as pw

# Define the schema of your data (Optional)
class InputSchema(pw.Schema):
  value: int

# Connect to your data using connectors
input_table = pw.io.csv.read(
  &quot;./input/&quot;,
  schema=InputSchema
)

#Define your operations on the data
filtered_table = input_table.filter(input_table.value&gt;=0)
result_table = filtered_table.reduce(
  sum_value = pw.reducers.sum(filtered_table.value)
)

# Load your results to external systems
pw.io.jsonlines.write(result_table, &quot;output.jsonl&quot;)

# Run the computation
pw.run()
```

Run Pathway [in Google Colab](https://colab.research.google.com/drive/1aBIJ2HCng-YEUOMrr0qtj0NeZMEyRz55?usp=sharing).

You can find more examples [here](https://github.com/pathwaycom/pathway/tree/main/examples).


## Deployment&lt;a id=&quot;deployment&quot;&gt;&lt;/a&gt;

### Locally&lt;a id=&quot;running-pathway-locally&quot;&gt;&lt;/a&gt;

To use Pathway, you only need to import it:

```python
import pathway as pw
```

Now, you can easily create your processing pipeline, and let Pathway handle the updates. Once your pipeline is created, you can launch the computation on streaming data with a one-line command:

```python
pw.run()
```

You can then run your Pathway project (say, `main.py`) just like a normal Python script: `$ python main.py`.
Pathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages. 

&lt;img src=&quot;https://d14l3brkh44201.cloudfront.net/pathway-dashboard.png&quot; width=&quot;1326&quot; alt=&quot;Pathway dashboard&quot;/&gt;

Alternatively, you can use the pathway&#039;ish version:

```
$ pathway spawn python main.py
```

Pathway natively supports multithreading.
To launch your application with 3 threads, you can do as follows:
```
$ pathway spawn --threads 3 python main.py
```

To jumpstart a Pathway project, you can use our [cookiecutter template](https://github.com/pathwaycom/cookiecutter-pathway).


### Docker&lt;a id=&quot;docker&quot;&gt;&lt;/a&gt;

You can easily run Pathway using docker.

#### Pathway image

You can use the [Pathway docker image](https://hub.docker.com/r/pathwaycom/pathway), using a Dockerfile:

```dockerfile
FROM pathwaycom/pathway:latest

WORKDIR /app

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD [ &quot;python&quot;, &quot;./your-script.py&quot; ]
```

You can then build and run the Docker image:

```console
docker build -t my-pathway-app .
docker run -it --rm --name my-pathway-app my-pathway-app
```

#### Run a single Python script

When dealing with single-file projects, creating a full-fledged `Dockerfile`
might seem unnecessary. In such scenarios, you can execute a
Python script directly using the Pathway Docker image. For example:

```console
docker run -it --rm --name my-pathway-app -v &quot;$PWD&quot;:/app pathwaycom/pathway:latest python my-pathway-app.py
```

#### Python docker image

You can also use a standard Python image and install Pathway using pip with a Dockerfile:

```dockerfile
FROM --platform=linux/x86_64 python:3.10

RUN pip install -U pathway
COPY ./pathway-script.py pathway-script.py

CMD [&quot;python&quot;, &quot;-u&quot;, &quot;pathway-script.py&quot;]
```

### Kubernetes and cloud&lt;a id=&quot;k8s&quot;&gt;&lt;/a&gt;

Docker containers are ideally suited for deployment on the cloud with Kubernetes.
If you want to scale your Pathway application, you may be interested in our Pathway for Enterprise.
Pathway for Enterprise is specially tailored towards end-to-end data processing and real time intelligent analytics.
It scales using distributed computing on the cloud and supports distributed Kubernetes deployment, with external persistence setup.

You can easily deploy Pathway using services like Render: see [how to deploy Pathway in a few clicks](https://pathway.com/developers/user-guide/deployment/render-deploy/).

If you are interested, don&#039;t hesitate to [contact us](mailto:contact@pathway.com) to learn more.

## Performance&lt;a id=&quot;performance&quot;&gt;&lt;/a&gt;

Pathway is made to outperform state-of-the-art technologies designed for streaming and batch data processing tasks, including: Flink, Spark, and Kafka Streaming. It also makes it possible to implement a lot of algorithms/UDF&#039;s in streaming mode which are not readily supported by other streaming frameworks (especially: temporal joins, iterative graph algorithms, machine learning routines).

If you are curious, here are [some benchmarks to play with](https://github.com/pathwaycom/pathway-benchmarks).

&lt;img src=&quot;https://github.com/pathwaycom/pathway-benchmarks/raw/main/images/bm-wordcount-lineplot.png&quot; width=&quot;1326&quot; alt=&quot;WordCount Graph&quot;/&gt;

## Documentation and Support&lt;a id=&quot;resources&quot;&gt;&lt;/a&gt;

The entire documentation of Pathway is available at [pathway.com/developers/](https://pathway.com/developers/user-guide/introduction/welcome), including the [API Docs](https://pathway.com/developers/api-docs/pathway).

If you have any question, don&#039;t hesitate to [open an issue on GitHub](https://github.com/pathwaycom/pathway/issues), join us on [Discord](https://discord.com/invite/pathway), or send us an email at [contact@pathway.com](mailto:contact@pathway.com).

## License&lt;a id=&quot;license&quot;&gt;&lt;/a&gt;

Pathway is distributed on a [BSL 1.1 License](https://github.com/pathwaycom/pathway/blob/main/LICENSE.txt) which allows for unlimited non-commercial use, as well as use of the Pathway package [for most commercial purposes](https://pathway.com/license/), free of charge. Code in this repository automatically converts to Open Source (Apache 2.0 License) after 4 years. Some [public repos](https://github.com/pathwaycom) which are complementary to this one (examples, libraries, connectors, etc.) are licensed as Open Source, under the MIT license.


## Contribution guidelines&lt;a id=&quot;contribution-guidelines&quot;&gt;&lt;/a&gt;

If you develop a library or connector which you would like to integrate with this repo, we suggest releasing it first as a separate repo on a MIT/Apache 2.0 license. 

For all concerns regarding core Pathway functionalities, Issues are encouraged. For further information, don&#039;t hesitate to engage with Pathway&#039;s [Discord community](https://discord.gg/pathway).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ostris/ai-toolkit]]></title>
            <link>https://github.com/ostris/ai-toolkit</link>
            <guid>https://github.com/ostris/ai-toolkit</guid>
            <pubDate>Fri, 15 Aug 2025 00:04:47 GMT</pubDate>
            <description><![CDATA[The ultimate training toolkit for finetuning diffusion models]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ostris/ai-toolkit">ostris/ai-toolkit</a></h1>
            <p>The ultimate training toolkit for finetuning diffusion models</p>
            <p>Language: Python</p>
            <p>Stars: 5,789</p>
            <p>Forks: 675</p>
            <p>Stars today: 41 stars today</p>
            <h2>README</h2><pre># AI Toolkit by Ostris

AI Toolkit is an all in one training suite for diffusion models. I try to support all the latest models on consumer grade hardware. Image and video models. It can be run as a GUI or CLI. It is designed to be easy to use but still have every feature imaginable.

## Support My Work

If you enjoy my projects or use them commercially, please consider sponsoring me. Every bit helps! üíñ

[Sponsor on GitHub](https://github.com/orgs/ostris) | [Support on Patreon](https://www.patreon.com/ostris) | [Donate on PayPal](https://www.paypal.com/donate/?hosted_button_id=9GEFUKC8T9R9W)

### Current Sponsors

All of these people / organizations are the ones who selflessly make this project possible. Thank you!!

_Last updated: 2025-08-08 17:01 UTC_

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://x.com/NuxZoe&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://pbs.twimg.com/profile_images/1919488160125616128/QAZXTMEj_400x400.png&quot; alt=&quot;a16z&quot; width=&quot;200&quot; height=&quot;200&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/replicate&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/60410876?v=4&quot; alt=&quot;Replicate&quot; width=&quot;200&quot; height=&quot;200&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/huggingface&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/25720743?v=4&quot; alt=&quot;Hugging Face&quot; width=&quot;200&quot; height=&quot;200&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/josephrocca&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/1167575?u=92d92921b4cb5c8c7e225663fed53c4b41897736&amp;v=4&quot; alt=&quot;josephrocca&quot; width=&quot;200&quot; height=&quot;200&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/162524101/81a72689c3754ac5b9e38612ce5ce914/eyJ3IjoyMDB9/1.png?token-hash=JHRjAxd2XxV1aXIUijj-l65pfTnLoefYSvgNPAsw2lI%3D&quot; alt=&quot;Prasanth Veerina&quot; width=&quot;200&quot; height=&quot;200&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;a href=&quot;https://github.com/weights-ai&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/185568492?v=4&quot; alt=&quot;Weights&quot; width=&quot;200&quot; height=&quot;200&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;hr style=&quot;width:100%;border:none;height:2px;background:#ddd;margin:30px 0;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/93304/J&quot; alt=&quot;Joseph Rocca&quot; width=&quot;150&quot; height=&quot;150&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/161471720/dd330b4036d44a5985ed5985c12a5def/eyJ3IjoyMDB9/1.jpeg?token-hash=k1f4Vv7TevzYa9tqlzAjsogYmkZs8nrXQohPCDGJGkc%3D&quot; alt=&quot;Vladimir Sotnikov&quot; width=&quot;150&quot; height=&quot;150&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/33158543/C&quot; alt=&quot;clement Delangue&quot; width=&quot;150&quot; height=&quot;150&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/8654302/b0f5ebedc62a47c4b56222693e1254e9/eyJ3IjoyMDB9/2.jpeg?token-hash=suI7_QjKUgWpdPuJPaIkElkTrXfItHlL8ZHLPT-w_d4%3D&quot; alt=&quot;Misch Strotz&quot; width=&quot;150&quot; height=&quot;150&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/120239481/49b1ce70d3d24704b8ec34de24ec8f55/eyJ3IjoyMDB9/1.jpeg?token-hash=o0y1JqSXqtGvVXnxb06HMXjQXs6OII9yMMx5WyyUqT4%3D&quot; alt=&quot;nitish PNR&quot; width=&quot;150&quot; height=&quot;150&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;/p&gt;
&lt;hr style=&quot;width:100%;border:none;height:2px;background:#ddd;margin:30px 0;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/2298192/1228b69bd7d7481baf3103315183250d/eyJ3IjoyMDB9/1.jpg?token-hash=opN1e4r4Nnvqbtr8R9HI8eyf9m5F50CiHDOdHzb4UcA%3D&quot; alt=&quot;Mohamed Oumoumad&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/548524/S&quot; alt=&quot;Steve Hanff&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/152118848/3b15a43d71714552b5ed1c9f84e66adf/eyJ3IjoyMDB9/1.png?token-hash=MKf3sWHz0MFPm_OAFjdsNvxoBfN5B5l54mn1ORdlRy8%3D&quot; alt=&quot;Kristjan Retter&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/83319230/M&quot; alt=&quot;Miguel Lara&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/8449560/P&quot; alt=&quot;Patron&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;a href=&quot;https://x.com/NuxZoe&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://pbs.twimg.com/profile_images/1916482710069014528/RDLnPRSg_400x400.jpg&quot; alt=&quot;tungsten&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/169502989/220069e79ce745b29237e94c22a729df/eyJ3IjoyMDB9/1.png?token-hash=E8E2JOqx66k2zMtYUw8Gy57dw-gVqA6OPpdCmWFFSFw%3D&quot; alt=&quot;Timothy Bielec&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/34200989/58ae95ebda0640c8b7a91b4fa31357aa/eyJ3IjoyMDB9/1.jpeg?token-hash=4mVDM1kCYGauYa33zLG14_g0oj9_UjDK_-Qp4zk42GE%3D&quot; alt=&quot;Noah Miller&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/27288932/6c35d2d961ee4e14a7a368c990791315/eyJ3IjoyMDB9/1.jpeg?token-hash=TGIto_PGEG2NEKNyqwzEnRStOkhrjb3QlMhHA3raKJY%3D&quot; alt=&quot;David Garrido&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;a href=&quot;https://x.com/RalFingerLP&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://pbs.twimg.com/profile_images/919595465041162241/ZU7X3T5k_400x400.jpg&quot; alt=&quot;RalFinger&quot; width=&quot;100&quot; height=&quot;100&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;hr style=&quot;width:100%;border:none;height:2px;background:#ddd;margin:30px 0;&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;http://www.ir-ltd.net&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://pbs.twimg.com/profile_images/1602579392198283264/6Tm2GYus_400x400.jpg&quot; alt=&quot;IR-Entertainment Ltd&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/9547341/bb35d9a222fd460e862e960ba3eacbaf/eyJ3IjoyMDB9/1.jpeg?token-hash=Q2XGDvkCbiONeWNxBCTeTMOcuwTjOaJ8Z-CAf5xq3Hs%3D&quot; alt=&quot;Travis Harrington&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/98811435/3a3632d1795b4c2b9f8f0270f2f6a650/eyJ3IjoyMDB9/1.jpeg?token-hash=657rzuJ0bZavMRZW3XZ-xQGqm3Vk6FkMZgFJVMCOPdk%3D&quot; alt=&quot;EmmanuelMr18&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/81275465/1e4148fe9c47452b838949d02dd9a70f/eyJ3IjoyMDB9/1.jpeg?token-hash=YAX1ucxybpCIujUCXfdwzUQkttIn3c7pfi59uaFPSwM%3D&quot; alt=&quot;Aaron Amortegui&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/155963250/6f8fd7075c3b4247bfeb054ba49172d6/eyJ3IjoyMDB9/1.png?token-hash=z81EHmdU2cqSrwa9vJmZTV3h0LG-z9Qakhxq34FrYT4%3D&quot; alt=&quot;Un Defined&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/45562978/0de33cf52ec642ae8a2f612cddec4ca6/eyJ3IjoyMDB9/1.jpeg?token-hash=aD4debMD5ZQjqTII6s4zYSgVK2-bdQt9p3eipi0bENs%3D&quot; alt=&quot;Jack English&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/27791680/J&quot; alt=&quot;Jean-Tristan Marin&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/570742/4ceb33453a5a4745b430a216aba9280f/eyJ3IjoyMDB9/1.jpg?token-hash=nPcJ2zj3sloND9jvbnbYnob2vMXRnXdRuujthqDLWlU%3D&quot; alt=&quot;Al H&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/82763/f99cc484361d4b9d94fe4f0814ada303/eyJ3IjoyMDB9/1.jpeg?token-hash=A3JWlBNL0b24FFWb-FCRDAyhs-OAxg-zrhfBXP_axuU%3D&quot; alt=&quot;Doron Adler&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/103077711/bb215761cc004e80bd9cec7d4bcd636d/eyJ3IjoyMDB9/2.jpeg?token-hash=3U8kdZSUpnmeYIDVK4zK9TTXFpnAud_zOwBRXx18018%3D&quot; alt=&quot;John Dopamine&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/99036356/7ae9c4d80e604e739b68cca12ee2ed01/eyJ3IjoyMDB9/3.png?token-hash=ZhsBMoTOZjJ-Y6h5NOmU5MT-vDb2fjK46JDlpEehkVQ%3D&quot; alt=&quot;Noctre&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/141098579/1a9f0a1249d447a7a0df718a57343912/eyJ3IjoyMDB9/2.png?token-hash=_n-AQmPgY0FP9zCGTIEsr5ka4Y7YuaMkt3qL26ZqGg8%3D&quot; alt=&quot;The Local Lab&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/93348210/5c650f32a0bc481d80900d2674528777/eyJ3IjoyMDB9/1.jpeg?token-hash=0jiknRw3jXqYWW6En8bNfuHgVDj4LI_rL7lSS4-_xlo%3D&quot; alt=&quot;Armin Behjati&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/134129880/680c7e14cd1a4d1a9face921fb010f88/eyJ3IjoyMDB9/1.png?token-hash=5fqqHE6DCTbt7gDQL7VRcWkV71jF7FvWcLhpYl5aMXA%3D&quot; alt=&quot;Bharat Prabhakar&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/70218846/C&quot; alt=&quot;Cosmosis&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/30931983/54ab4e4ceab946e79a6418d205f9ed51/eyJ3IjoyMDB9/1.png?token-hash=j2phDrgd6IWuqKqNIDbq9fR2B3fMF-GUCQSdETS1w5Y%3D&quot; alt=&quot;HestoySeghuro .&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/4105384/J&quot; alt=&quot;Jack Blakely&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/4541423/S&quot; alt=&quot;S√∂ren &quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;a href=&quot;https://www.youtube.com/@happyme7055&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://yt3.googleusercontent.com/ytc/AIdro_mFqhIRk99SoEWY2gvSvVp6u1SkCGMkRqYQ1OlBBeoOVp8=s160-c-k-c0x00ffffff-no-rj&quot; alt=&quot;Marcus Rass&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/53077895/M&quot; alt=&quot;Marc&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/157407541/bb9d80cffdab4334ad78366060561520/eyJ3IjoyMDB9/2.png?token-hash=WYz-U_9zabhHstOT5UIa5jBaoFwrwwqyWxWEzIR2m_c%3D&quot; alt=&quot;Tokio Studio srl IT10640050968&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/44568304/a9d83a0e786b41b4bdada150f7c9271c/eyJ3IjoyMDB9/1.jpeg?token-hash=FtxnwrSrknQUQKvDRv2rqPceX2EF23eLq4pNQYM_fmw%3D&quot; alt=&quot;Albert Bukoski&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/5048649/B&quot; alt=&quot;Ben Ward&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/111904990/08b1cf65be6a4de091c9b73b693b3468/eyJ3IjoyMDB9/1.png?token-hash=_Odz6RD3CxtubEHbUxYujcjw6zAajbo3w8TRz249VBA%3D&quot; alt=&quot;Brian Smith&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/494309/J&quot; alt=&quot;Julian Tsependa&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/5602036/K&quot; alt=&quot;Kelevra&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/159203973/36c817f941ac4fa18103a4b8c0cb9cae/eyJ3IjoyMDB9/1.png?token-hash=zkt72HW3EoiIEAn3LSk9gJPBsXfuTVcc4rRBS3CeR8w%3D&quot; alt=&quot;Marko jak&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/24653779/R&quot; alt=&quot;RayHell&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/76566911/6485eaf5ec6249a7b524ee0b979372f0/eyJ3IjoyMDB9/1.jpeg?token-hash=mwCSkTelDBaengG32NkN0lVl5mRjB-cwo6-a47wnOsU%3D&quot; alt=&quot;the biitz&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/32633822/1ab5612efe80417cbebfe91e871fc052/eyJ3IjoyMDB9/1.png?token-hash=pOS_IU3b3RL5-iL96A3Xqoj2bQ-dDo4RUkBylcMED_s%3D&quot; alt=&quot;Zack Abrams&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/97985240/3d1d0e6905d045aba713e8132cab4a30/eyJ3IjoyMDB9/1.png?token-hash=fRavvbO_yqWKA_OsJb5DzjfKZ1Yt-TG-ihMoeVBvlcM%3D&quot; alt=&quot;◊¢◊ï◊û◊® ◊û◊õ◊ú◊ï◊£&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;a href=&quot;https://github.com/julien-blanchon&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/11278197?v=4&quot; alt=&quot;Blanchon&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/11198131/e696d9647feb4318bcf16243c2425805/eyJ3IjoyMDB9/1.jpeg?token-hash=c2c2p1SaiX86iXAigvGRvzm4jDHvIFCg298A49nIfUM%3D&quot; alt=&quot;Nicholas Agranoff&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/785333/bdb9ede5765d42e5a2021a86eebf0d8f/eyJ3IjoyMDB9/2.jpg?token-hash=l_rajMhxTm6wFFPn7YdoKBxeUqhdRXKdy6_8SGCuNsE%3D&quot; alt=&quot;Sapjes &quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/2446176/S&quot; alt=&quot;Scott VanKirk&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/83034/W&quot; alt=&quot;william tatum&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/138787189/2b5662dcb638466282ac758e3ac651b4/eyJ3IjoyMDB9/1.png?token-hash=zwj7MScO18vhDxhKt6s5q4gdeNJM3xCLuhSt8zlqlZs%3D&quot; alt=&quot;–ê–Ω—Ç–æ–Ω –ê–Ω—Ç–æ–Ω–∏–æ&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/30530914/T&quot; alt=&quot;Techer &quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/25209707/36ae876d662d4d85aaf162b6d67d31e7/eyJ3IjoyMDB9/1.png?token-hash=Zows_A6uqlY5jClhfr4Y3QfMnDKVkS3mbxNHUDkVejo%3D&quot; alt=&quot;fjioq8&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/46680573/ee3d99c04a674dd5a8e1ecfb926db6a2/eyJ3IjoyMDB9/1.jpeg?token-hash=cgD4EXyfZMPnXIrcqWQ5jGqzRUfqjPafb9yWfZUPB4Q%3D&quot; alt=&quot;Neil Murray&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg&quot; alt=&quot;Joakim S√§llstr√∂m&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/63510241/A&quot; alt=&quot;Andrew Park&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;a href=&quot;https://github.com/Spikhalskiy&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/532108?u=2464983638afea8caf4cd9f0e4a7bc3e6a63bb0a&amp;v=4&quot; alt=&quot;Dmitry Spikhalsky&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/88567307/E&quot; alt=&quot;el Chavo&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/117569999/55f75c57f95343e58402529cec852b26/eyJ3IjoyMDB9/1.jpeg?token-hash=squblHZH4-eMs3gI46Uqu1oTOK9sQ-0gcsFdZcB9xQg%3D&quot; alt=&quot;James Thompson&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/66157709/6fe70df085e24464995a1a9293a53760/eyJ3IjoyMDB9/1.jpeg?token-hash=eqe0wvg6JfbRUGMKpL_x3YPI5Ppf18aUUJe2EzADU-g%3D&quot; alt=&quot;Joey Santana&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg&quot; alt=&quot;Heikki Rinkinen&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/6175608/B&quot; alt=&quot;Bobbie &quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;a href=&quot;https://github.com/Slartibart23&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/133593860?u=31217adb2522fb295805824ffa7e14e8f0fca6fa&amp;v=4&quot; alt=&quot;Slarti&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;&lt;/a&gt;
&lt;img src=&quot;https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg&quot; alt=&quot;Tommy Falkowski&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/28533016/e8f6044ccfa7483f87eeaa01c894a773/eyJ3IjoyMDB9/2.png?token-hash=ak-h3JWB50hyenCavcs32AAPw6nNhmH2nBFKpdk5hvM%3D&quot; alt=&quot;William Tatum&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://ostris.com/wp-content/uploads/2025/08/supporter_default.jpg&quot; alt=&quot;Karol Stƒôpie≈Ñ&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/156564939/17dbfd45c59d4cf29853d710cb0c5d6f/eyJ3IjoyMDB9/1.png?token-hash=e6wXA_S8cgJeEDI9eJK934eB0TiM8mxJm9zW_VH0gDU%3D&quot; alt=&quot;Hans Untch&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c8.patreon.com/4/200/59408413/B&quot; alt=&quot;ByteC&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/3712451/432e22a355494ec0a1ea1927ff8d452e/eyJ3IjoyMDB9/7.jpeg?token-hash=OpQ9SAfVQ4Un9dSYlGTHuApZo5GlJ797Mo0DtVtMOSc%3D&quot; alt=&quot;David Shorey&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;display: inline-block;&quot;&gt;
&lt;img src=&quot;https://c10.patreonusercontent.com/4/patreon-media/p/user/53634141/c1441f6c605344bbaef885d4272977bb/eyJ3IjoyMDB9/1.JPG?token-hash=Aizd6AxQhY3n6TBE5AwCVeSwEBbjALxQmu6xqc08qBo%3D&quot; alt=&quot;Jana Spacelight&quot; width=&quot;60&quot; height=&quot;60&quot; style=&quot;border-radius:8px;margin:5px;disp

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[oop7/YTSage]]></title>
            <link>https://github.com/oop7/YTSage</link>
            <guid>https://github.com/oop7/YTSage</guid>
            <pubDate>Fri, 15 Aug 2025 00:04:46 GMT</pubDate>
            <description><![CDATA[Modern YouTube downloader with a clean PySide6 interface. Download videos in any quality, extract audio, fetch subtitles, sponserBlock, and view video metadata. Built with yt-dlp for reliable performance.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/oop7/YTSage">oop7/YTSage</a></h1>
            <p>Modern YouTube downloader with a clean PySide6 interface. Download videos in any quality, extract audio, fetch subtitles, sponserBlock, and view video metadata. Built with yt-dlp for reliable performance.</p>
            <p>Language: Python</p>
            <p>Stars: 1,493</p>
            <p>Forks: 95</p>
            <p>Stars today: 205 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# üé• YTSage

&lt;img src=&quot;https://github.com/user-attachments/assets/f95f7bfb-8591-4d32-b795-68e61efd670c&quot; width=&quot;800&quot; alt=&quot;YTSage Interface&quot;/&gt;

[![PyPI version](https://img.shields.io/pypi/v/ytsage?color=dc2626&amp;style=for-the-badge&amp;logo=pypi&amp;logoColor=white)](https://badge.fury.io/py/ytsage)
[![License: MIT](https://img.shields.io/badge/License-MIT-374151?style=for-the-badge&amp;logo=opensource&amp;logoColor=white)](https://opensource.org/licenses/MIT)
[![Python 3.7+](https://img.shields.io/badge/python-3.7+-1f2937?style=for-the-badge&amp;logo=python&amp;logoColor=white)](https://www.python.org/downloads/)
[![Downloads](https://img.shields.io/pypi/dm/ytsage?color=4b5563&amp;style=for-the-badge&amp;logo=download&amp;logoColor=white)](https://pepy.tech/project/ytsage)
[![GitHub Stars](https://img.shields.io/github/stars/oop7/YTSage?color=dc2626&amp;style=for-the-badge&amp;logo=github&amp;logoColor=white)](https://github.com/oop7/YTSage/stargazers)

**A modern YouTube downloader with a clean PySide6 interface.**  
Download videos in any quality, extract audio, fetch subtitles, and more.

[Installation](#installation) ‚Ä¢
[Features](#features) ‚Ä¢
[Usage](#usage) ‚Ä¢
[Screenshots](#screenshots) ‚Ä¢
[Contributing](#contributing)

&lt;/div&gt;

---

&lt;a id=&quot;features&quot;&gt;&lt;/a&gt;
## ‚ú® Features

&lt;div align=&quot;center&quot;&gt;

| Core Features                     | Advanced Features                       | Extra Features                     |
|-----------------------------------|-----------------------------------------|------------------------------------|
| üé• Format Table                   | üö´ SponsorBlock Integration             | üíæ Save Download Path             |
| üéµ Audio Extraction               | üìù Multi-Subtitle Select &amp; Merge        | üîÑ Auto-Update yt-dlp                  |
| ‚ú® Simple UI                      |  üíæ Save Description                    | üõ†Ô∏è FFmpeg/yt-dlp Detection         |
| üìã Playlist Support              |  üñºÔ∏è Save thumbnail                       | ‚öôÔ∏è Custom Commands                 |
| üñºÔ∏è Playlist Selector             | üöÄ Speed Limiter                        | üç™ Login with Cookies              |
|                                   | ‚úÇÔ∏è Trim Video Sections                   |                                    |

&lt;/div&gt;

&lt;a id=&quot;installation&quot;&gt;&lt;/a&gt;
## üöÄ Installation

### Quick Install (Recommended)
```bash
pip install ytsage
```
```bash
# Run the application
ytsage
```

### üì¶ Other Installation Methods

### Pre-built Executables
- ü™ü Windows: `YTSage.exe`
- ü™ü Windows: `YTSage-ffmpeg.exe` (Includes FFmpeg)
- üêß Linux: `YTSage_{version}_amd64.deb`
- üêß Linux: `YTSage-x86_64.AppImage`
- üçé macOS: `YTSage-macOS-app.zip`
- üçé macOS: `YTSage-{version}.dmg`

&lt;details&gt;
&lt;summary&gt;üõ†Ô∏è Manual Installation from Source&lt;/summary&gt;

```bash
# Clone repository
git clone https://github.com/oop7/YTSage.git

# Navigate to directory
cd YTSage

# Install dependencies
pip install -r requirements.txt

# Run application
python main.py
```

&lt;/details&gt;

&lt;a id=&quot;screenshots&quot;&gt;&lt;/a&gt;
## üì∏ Screenshots

&lt;div align=&quot;center&quot;&gt;
&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/f95f7bfb-8591-4d32-b795-68e61efd670c&quot; alt=&quot;Main Interface&quot; width=&quot;400&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/f7b3ebab-3054-4c77-8109-c899a8b10047&quot; alt=&quot;Playlist Download&quot; width=&quot;400&quot;/&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;em&gt;Main Interface&lt;/em&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;em&gt;Playlist Download&lt;/em&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/a80d2ae2-0031-4ed0-bee4-93293634c62a&quot; alt=&quot;Audio Format Selection with Save Thumbnail&quot; width=&quot;400&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/5236e3cc-8a8d-4d85-a660-782a740ef9af&quot; alt=&quot;Subtitle Options merged with Remove Sponsor Segments&quot; width=&quot;400&quot;/&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;em&gt;Audio Format&lt;/em&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;em&gt;Subtitle Options&lt;/em&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;a id=&quot;usage&quot;&gt;&lt;/a&gt;
## üìñ Usage

&lt;details&gt;
&lt;summary&gt;üéØ Basic Usage&lt;/summary&gt;

1. **Launch YTSage**
2. **Paste YouTube URL** (or use &quot;Paste URL&quot; button)
3. **Click &quot;Analyze&quot;**
4. **Select Format:**
   - `Video` for video downloads
   - `Audio Only` for audio extraction
5. **Choose Options:**
   - Enable subtitles &amp; select language
   - Enable subtitle merge
   - Save thumbnail
   - Remove sponsor segments
   - Save description
6. **Select Output Directory**
7. **Click &quot;Download&quot;**

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üìã Playlist Download&lt;/summary&gt;

1. **Paste Playlist URL**
2. **Click &quot;Analyze&quot;**
3. **Select videos from the playlist selector (optional, defaults to all)**
4. **Choose desired format/quality**
5. **Click &quot;Download&quot;**

&gt; üí° The application automatically handles the download queue

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üß∞ Advanced Options&lt;/summary&gt;

- **Quality Selection:** Choose the highest resolution for best quality
- **Subtitle Options:** Filter languages and embed into video
- **Custom Commands:** Access advanced yt-dlp features
- **Save Description:** Save the description of the video
- **Save Thumbnail:** Save the thumbnail of the video
- **Remove Sponsor Segments:** Remove sponsor segments from the video
- **Speed Limiter:** Limit the download speed
- **Login with Cookies:** Login to YouTube using cookies to access private content  
  How to use it:
  1. Extract cookies from your browser using an extension like [cookie-editor](https://github.com/moustachauve/cookie-editor?tab=readme-ov-file)
  2. Copy the cookies in Netscape format
  3. Create a file named `cookies.txt` and paste the cookies into it
  4. Select the `cookies.txt` file in the app
- **Save Download Path:** Save the download path
- **Update yt-dlp:** Update yt-dlp
- **FFmpeg/yt-dlp Detection:** Automatically detect FFmpeg/yt-dlp
- **Custom Commands:** Access advanced yt-dlp features
- **Trim Video:** Download only specific parts of a video by specifying time ranges (HH:MM:SS format)


&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üõ†Ô∏è Troubleshooting&lt;/summary&gt;

- **Format table not displaying:** Update yt-dlp to the latest version
- **Download fails:** Check your internet connection and ensure the video is available
- **Audio extraction issues:** Verify FFmpeg is properly installed

&lt;/details&gt;

## üß© Requirements

- **Python:** 3.7 or higher
- **GUI Framework:** PySide6
- **Download Engine:** yt-dlp  
- **Media Processing:** FFmpeg
- **Additional Libraries:** Pillow, requests, packaging, markdown, pygame

&lt;a id=&quot;contributing&quot;&gt;&lt;/a&gt;
## üë• Contributing

We welcome contributions! Here&#039;s how you can help:

1. üç¥ Fork the repository
2. üåø Create your feature branch:
   ```bash
   git checkout -b feature/AmazingFeature
   ```
3. üíæ Commit your changes:
   ```bash
   git commit -m &#039;Add some AmazingFeature&#039;
   ```
4. üì§ Push to the branch:
   ```bash
   git push origin feature/AmazingFeature
   ```
5. üîÑ Open a Pull Request

## üìä Star History

&lt;div align=&quot;center&quot;&gt;
  
[![Star History Chart](https://api.star-history.com/svg?repos=oop7/YTSage&amp;type=Date)](https://star-history.com/#oop7/YTSage&amp;Date)

&lt;/div&gt;

## üìú License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

&lt;div align=&quot;center&quot;&gt;

| Technology | Purpose |
|------------|---------|
| [yt-dlp](https://github.com/yt-dlp/yt-dlp) | Download Engine |
| [PySide6](https://wiki.qt.io/Qt_for_Python) | GUI Framework |
| [FFmpeg](https://ffmpeg.org/) | Media Processing |
| [Pillow](https://python-pillow.org/) | Image Processing |
| [requests](https://requests.readthedocs.io/) | HTTP Requests |
| [packaging](https://packaging.python.org/) | Packaging |
| [markdown](https://python-markdown.github.io/) | Markdown Processing |
| [pygame](https://www.pygame.org/) | Audio Playback |
| [New Notification 09 by Universfield](https://pixabay.com/sound-effects/new-notification-09-352705/) | Notification Sound |


&lt;/div&gt;

## ‚ö†Ô∏è Disclaimer

This tool is for personal use only. Please respect YouTube&#039;s terms of service and content creators&#039; rights.

---

&lt;div align=&quot;center&quot;&gt;

Made with ‚ù§Ô∏è by [oop7](https://github.com/oop7)

&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[LeCAR-Lab/ASAP]]></title>
            <link>https://github.com/LeCAR-Lab/ASAP</link>
            <guid>https://github.com/LeCAR-Lab/ASAP</guid>
            <pubDate>Fri, 15 Aug 2025 00:04:45 GMT</pubDate>
            <description><![CDATA[Official implementation of [RSS 2025] "ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills"]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/LeCAR-Lab/ASAP">LeCAR-Lab/ASAP</a></h1>
            <p>Official implementation of [RSS 2025] "ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills"</p>
            <p>Language: Python</p>
            <p>Stars: 1,425</p>
            <p>Forks: 137</p>
            <p>Stars today: 25 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt; ASAP: Aligning Simulation and Real-World Physics for 

Learning Agile Humanoid Whole-Body Skills &lt;/h1&gt;

&lt;div align=&quot;center&quot;&gt;

Robotics: Science and Systems (RSS) 2025

[[Website]](https://agile.human2humanoid.com/)
[[Arxiv]](https://arxiv.org/pdf/2502.01143)
[[Video]](https://www.youtube.com/watch?v=tu7LSNYWDTs&amp;ab_channel=LeCARLabatCMU)

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;imgs/CMU-NV-logo-crop-png.png&quot; height=50&quot;&gt; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;
&lt;/p&gt;




[![IsaacGym](https://img.shields.io/badge/IsaacGym-Preview4-b.svg)](https://developer.nvidia.com/isaac-gym) [![IsaacSim](https://img.shields.io/badge/IsaacSim-4.2.0-b.svg)](https://docs.isaacsim.omniverse.nvidia.com/4.2.0/index.html) [![IsaacSim](https://img.shields.io/badge/Genesis-0.2.1-b.svg)](https://docs.isaacsim.omniverse.nvidia.com/4.2.0/index.html) [![Linux platform](https://img.shields.io/badge/Platform-linux--64-orange.svg)](https://ubuntu.com/blog/tag/22-04-lts) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)]()


&lt;img src=&quot;https://agile.human2humanoid.com/static/images/asap-preview-gif-480P.gif&quot; width=&quot;400px&quot;/&gt;

&lt;/div&gt;

&lt;!-- # Table of Contents --&gt;
## üìö Table of Contents

1. **[Overview](#overview)**  
   - Links: [Website](https://agile.human2humanoid.com/) ‚Ä¢ [Arxiv](https://arxiv.org/pdf/2502.01143) ‚Ä¢ [Video](https://www.youtube.com/watch?v=tu7LSNYWDTs&amp;ab_channel=LeCARLabatCMU)  
 

2. **[Installation &amp; Setup](#installation)**  
   2.1 [Base Frameworks](#isaacgym-conda-env)  
   2.2 [IsaacGym Setup](#install-isaacgym)  
   2.3 [HumanoidVerse Setup](#install-humanoidverse)  
   2.4 [IsaacSim + IsaacLab Setup](#isaaclab-environment)  
   2.5 [Genesis Environment Setup](#genesis-environment)  

3. **[Training Pipelines](#motion-tracking-training)**  
   3.1 [Phase-Based Motion Tracking](#motion-tracking-training)  
   3.2 [ASAP Delta Action Model](#asap-delta-action-model-training)  
       - [Train Delta Action Model](#train-delta-action-model)  
       - [Finetune Policy with Delta Action Model](#use-delta-action-model-for-policy-finetuning)  

4. **[Motion Retargeting to Any Humanoid](#motion-retargeting-to-any-humanoid)**  
   4.1 [Step 1: SMPL Shape Preparation](#1-smpl-shape-preparation)  
   4.2 [Step 2: SMPL Motion Preparation (AMASS)](#2-smpl-motion-preparation-amass)  
   4.3 [Step 3: Robot XML &amp; Motion Config](#3-robot-xml-and-motion-config-preparation)  
   4.4 [Step 4: Humanoid-SMPL Shape Fitting](#4-humanoid-smpl-shape-fitting)  
   4.5 [Step 5: Humanoid-SMPL Motion Retargeting](#5-humanoid-smpl-motion-retargeting)  

5. **[Deployment: Sim2Sim &amp; Sim2Real](#sim2simsim2real)**  
   5.1 [Environment Setup](#environment-setup)  
   5.2 [Sim2Sim Deployment](#sim2sim)  
   5.3 [Sim2Real Deployment](#sim2real)  

7. **[Citation](#citation)**  

8. **[License](#license)**


## TODO
- [x] Release code backbone
- [x] Release phase-based motion tracking training pipeline
- [x] Release ASAP motion datasets
- [x] Release motion retargeting pipeline
- [x] Release sim2sim in MuJoCo
- [x] Release sim2real with UnitreeSDK
- [x] Release ASAP delta action model training pipeline


# Installation

ASAP codebase is built on top of [HumanoidVerse](https://github.com/LeCAR-Lab/HumanoidVerse) (a multi-simulator framework for humanoid learning) and [Human2Humanoid](https://github.com/LeCAR-Lab/human2humanoid) (our prior work on humanoid whole-body tracking).

[HumanoidVerse](https://github.com/LeCAR-Lab/HumanoidVerse) allows you to train humanoid skills in multiple simulators, including IsaacGym, IsaacSim, and Genesis. Its key design logic is the separation and modularization of simulators, tasks, and algorithms, which enables smooth transfers between different simulators and the real world with minimum effort (just one line of code change). We leverage this framework to develop [ASAP](https://agile.human2humanoid.com/) and study how to best transfer policies across simulators and the real world.

## IsaacGym Conda Env

Create mamba/conda environment, in the following we use conda for example, but you can use mamba as well.

```bash
conda create -n hvgym python=3.8
conda activate hvgym
```
### Install IsaacGym

Download [IsaacGym](https://developer.nvidia.com/isaac-gym/download) and extract:

```bash
wget https://developer.nvidia.com/isaac-gym-preview-4
tar -xvzf isaac-gym-preview-4
```

Install IsaacGym Python API:

```bash
pip install -e isaacgym/python
```

Test installation:

```bash
python 1080_balls_of_solitude.py  # or
python joint_monkey.py
```

For libpython error:

- Check conda path:
    ```bash
    conda info -e
    ```
- Set LD_LIBRARY_PATH:
    ```bash
    export LD_LIBRARY_PATH=&lt;/path/to/conda/envs/your_env/lib&gt;:$LD_LIBRARY_PATH
    ```

### Install HumanoidVerse

Install dependencies:
```bash
pip install -e .
pip install -e isaac_utils
pip install -r requirements.txt
```

Test with:
```bash
HYDRA_FULL_ERROR=1 python humanoidverse/train_agent.py \
+simulator=isaacgym \
+exp=locomotion \
+domain_rand=NO_domain_rand \
+rewards=loco/reward_g1_locomotion \
+robot=g1/g1_29dof_anneal_23dof \
+terrain=terrain_locomotion_plane \
+obs=loco/leggedloco_obs_singlestep_withlinvel \
num_envs=1 \
project_name=TestIsaacGymInstallation \
experiment_name=G123dof_loco \
headless=False
```
&lt;details&gt;
&lt;summary&gt;Note:&lt;/summary&gt;
This is ONLY for testing, NOT how we train the locomotion policy in the ASAP paper. But still, you can train a locomotion policy by:

```bash
HYDRA_FULL_ERROR=1 python humanoidverse/train_agent.py \
+simulator=isaacgym \
+exp=locomotion \
+domain_rand=NO_domain_rand \
+rewards=loco/reward_g1_locomotion \
+robot=g1/g1_29dof_anneal_23dof \
+terrain=terrain_locomotion_plane \
+obs=loco/leggedloco_obs_singlestep_withlinvel \
num_envs=4096 \
project_name=TestIsaacGymInstallation \
experiment_name=G123dof_loco \
headless=True \
rewards.reward_penalty_curriculum=True \
rewards.reward_initial_penalty_scale=0.1 \
rewards.reward_penalty_degree=0.00003 
```

&lt;/details&gt;

## IsaacLab Environment

### Install IsaacSim
1. Download Omniverse Launcher
2. Install Isaac Sim through launcher
3. Set environment variables:
```bash
export ISAACSIM_PATH=&quot;${HOME}/.local/share/ov/pkg/isaac-sim-4.2.0&quot;
export ISAACSIM_PYTHON_EXE=&quot;${ISAACSIM_PATH}/python.sh&quot;
```

### Install IsaacLab
```bash
git clone https://github.com/isaac-sim/IsaacLab.git
cd IsaacLab &amp;&amp; ./isaaclab.sh --conda hvlab
mamba activate hvlab
sudo apt install cmake build-essential
./isaaclab.sh --install
```

### Setup HumanoidVerse
```bash
pip install -e .
pip install -e isaac_utils
```

## Genesis Environment
```bash
mamba create -n hvgen python=3.10
mamba activate hvgen
pip install genesis-world torch
```
Install dependencies:

```bash
pip install -e .
pip install -e isaac_utils
```


# Motion Tracking Training

Train a phase-based motion tracking policy to imitate Cristiano Ronaldo&#039;s signature Siuuu move

```bash
python humanoidverse/train_agent.py \
+simulator=isaacgym \
+exp=motion_tracking \
+domain_rand=NO_domain_rand \
+rewards=motion_tracking/reward_motion_tracking_dm_2real \
+robot=g1/g1_29dof_anneal_23dof \
+terrain=terrain_locomotion_plane \
+obs=motion_tracking/deepmimic_a2c_nolinvel_LARGEnoise_history \
num_envs=4096 \
project_name=MotionTracking \
experiment_name=MotionTracking_CR7 \
robot.motion.motion_file=&quot;humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-TairanTestbed_TairanTestbed_CR7_video_CR7_level1_filter_amass.pkl&quot; \
rewards.reward_penalty_curriculum=True \
rewards.reward_penalty_degree=0.00001 \
env.config.resample_motion_when_training=False \
env.config.termination.terminate_when_motion_far=True \
env.config.termination_curriculum.terminate_when_motion_far_curriculum=True \
env.config.termination_curriculum.terminate_when_motion_far_threshold_min=0.3 \
env.config.termination_curriculum.terminate_when_motion_far_curriculum_degree=0.000025 \
robot.asset.self_collisions=0
```

After training, you can visualize the policy by:
```bash
python humanoidverse/eval_agent.py \
+checkpoint=logs/MotionTracking/xxxxxxxx_xxxxxxx-MotionTracking_CR7-motion_tracking-g1_29dof_anneal_23dof/model_5800.pt
```

This is the visualization of the policy after traning 5800 iters. The policy is able to imitate the motion of Cristiano Ronaldo&#039;s Siuuu move. With more training, the policy will be more accurate and smooth (see the video in the [paper](https://arxiv.org/pdf/2502.01143)).

&lt;img src=&quot;imgs/motion_tracking_5800.gif&quot; width=&quot;400px&quot;/&gt;


# ASAP delta action model training

Note that the only difference between the delta action model training and naive motion tracking training is that delta action model needs a motion file with extra keyname `&quot;action&quot;` in the motion file, so that the resulting RL policy we are training is able to use the delta action model to `&quot;control the robot&quot;` to match the real-world/sim2sim motions.


## Train delta action model

```
python roboverse/train_agent.py \                                                                                   
  +simulator=isaacgym \
  +exp=train_delta_a_open_loop \
  +domain_rand=NO_domain_rand \
  +rewards=motion_tracking/delta_a/reward_delta_a_openloop \
  +robot=g1/g1_29dof_anneal_23dof \
  +terrain=terrain_locomotion_plane \
  +obs=delta_a/open_loop \
  num_envs=5000 \
  project_name=DeltaA_Training \
  experiment_name=openloopDeltaA_training \
  robot.motion.motion_file=&quot;&lt;PATH_TO_YOUR_MOTION_FILE_WITH_ACTION_KEYNAME&gt;&quot; \
  env.config.max_episode_length_s=1.0 \
  rewards.reward_scales.penalty_minimal_action_norm=-0.1 \
  +device=cuda:0 \
  env.config.resample_motion_when_training=True \
  env.config.resample_time_interval_s=10000
```

## Use delta action model for policy finetuning


```
HYDRA_FULL_ERROR=1 \
python roboverse/train_agent.py \
+simulator=isaacgym \
+exp=train_delta_a_closed_loop \
algo.config.policy_checkpoint=&#039;&lt;PATH_TO_YOUR_DELTA_A_MODEL&gt;&#039; \
+domain_rand=NO_domain_rand_finetune_with_deltaA \
+rewards=motion_tracking/reward_motion_tracking_dm_simfinetuning \
+robot=g1/g1_29dof_anneal_23dof \
+terrain=terrain_locomotion_plane \
+obs=delta_a/train_policy_with_delta_a \
num_envs=4096 \
project_name=DeltaA_Finetune \
experiment_name=finetune_with_deltaA \
robot.motion.motion_file=&quot;&lt;PATH_TO_YOUR_MOTION_FILE&gt;&quot; \
+opt=wandb \
env.config.add_extra_action=True \
+checkpoint=&quot;&lt;PATH_TO_YOUR_POLICY_TO_BE_FINETUNED&gt;&quot; \
domain_rand.push_robots=False \
env.config.noise_to_initial_level=1 \
rewards.reward_penalty_curriculum=True \
+device=cuda:0 \
algo.config.save_interval=5 \
algo.config.num_learning_iterations=1000 

```





# Motion Retargeting to Any Humanoid

&gt; [!IMPORTANT]
&gt; Here we share a generic humanoid motion retargeting pipeline to any humanoid from [PHC](https://github.com/ZhengyiLuo/PHC). 

&gt; [!IMPORTANT]
&gt; We have provided all the SMPL motions (`ASAP/humanoidverse/data/motions/raw_tairantestbed_smpl`) and retargtted G1 motions (`ASAP/humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles`) used in the ASAP paper in this codebase. If you are interested in using these motions G1, you can ignore this section. If you are interested in retargeting other humanoids or other motions, you can follow the steps below to prepare the SMPL shapes and motions.


It has three steps:
1. SMPL Shape preparation
2. SMPL Motion preparation
3. Robot XML and Motion Config preparation
4. Humanoid-SMPL shape fitting
5. Humanoid-SMPL motion retargeting

## 1. SMPL Shape preparation

Download [v1.1.0 SMPL files with pkl format](https://download.is.tue.mpg.de/download.php?domain=smpl&amp;sfile=SMPL_python_v.1.1.0.zip) and put it under `humanoidverse/data/smpl/`, and you should have:

```
|-- ASAP
    |-- humanoidverse
        |-- data
            |-- smpl
                |-- SMPL_python_v.1.1.0.zip
```

Then `cd ASAP/humanoidverse/data/smpl/` and  `unzip SMPL_python_v.1.1.0.zip`, after some copying and moving, you should have:

```
|-- ASAP
    |-- humanoidverse
        |-- data
            |-- smpl
                |-- SMPL_python_v.1.1.0
                |-- models
                    |-- basicmodel_f_lbs_10_207_0_v1.1.0.pkl
                    |-- basicmodel_m_lbs_10_207_0_v1.1.0.pkl
                    |-- basicmodel_neutral_lbs_10_207_0_v1.1.0.pkl
                |-- smpl_webuser
                |-- ...

```

Rename these three pkl files and move it under smpl like this:

```
|-- ASAP
    |-- humanoidverse
        |-- data
            |-- smpl
                |-- SMPL_FEMALE.pkl
                |-- SMPL_MALE.pkl
                |-- SMPL_NEUTRAL.pkl
```


## 2. SMPL Motion preparation (AMASS)
Download [AMASS Dataset](https://amass.is.tue.mpg.de/index.html) with `SMPL + H G format` and put it under `humanoidverse/data/motions/AMASS/AMASS_Complete/`.

```
|-- ASAP
    |-- humanoidverse
        |-- data
            |-- AMASS
                |-- AMASS_Complete
                    |-- ACCAD.tar.bz2
                    |-- BMLhandball.tar.bz2
                    |-- BMLmovi.tar.bz2
                    |-- BMLrub.tar
                    |-- CMU.tar.bz2
                    |-- ...
                    |-- Transitions.tar.bz2
```

And then cd ASAP/humanoidverse/data/motions/AMASS/AMASS_Complete/ and extract all the motion files by running:

```bash
for file in *.tar.bz2; do
    tar -xvjf &quot;$file&quot;
done
```
Then you should have:

```
|-- ASAP
    |-- humanoidverse
        |-- data
            |-- AMASS
                |-- AMASS_Complete
                    |-- ACCAD
                    |-- BioMotionLab_NTroje
                    |-- BMLhandball
                    |-- BMLmovi
                    |-- CMU
                    |-- ...
                    |-- Transitions
```

## 3. Robot XML and Motion Config preparation

Make sure you have robot xml and meshes ready at (G1 as example) `humanoidverse/data/robots/g1/g1_29dof_anneal_23dof_fitmotionONLY.xml`
And add your config for the robot motion in `humanoidverse/config/robot/g1_29dof_anneal_23dof.yaml` with like the following. Remember to link the xml path in the config.


## 4. Humanoid-SMPL shape fitting
Run the following command to fit the SMPL shape to the humanoid.
```bash
python scripts/data_process/fit_smpl_shape.py +robot=g1/g1_29dof_anneal_23dof
```
And you should have you shape file located at `humanoidverse/data/shape/g1_29dof_anneal_23dof/shape_optimized_v1.pkl`

If you want to visualize the shape, you can run with flag `+vis=True`, then you can have visualization of the fitted SMPL body shape and the humanoid body keypoints like this shape. The blue is the humanoid body keypoints and the orange is the fitted SMPL body keypoint. You can tune the `robot motion` in `humanoidverse/config/robot/g1_29dof_anneal_23dof.yaml` to adjust the correspondence, extend links lengths to get better fitted SMPL shape.

&lt;img src=&quot;imgs/g1_29dof_anneal_23dof_shape.png&quot; width=&quot;400px&quot;/&gt;

## 5. Humanoid-SMPL motion retargeting
Run the following command to retarget the motion to the humanoid.
```bash
python scripts/data_process/fit_smpl_motion.py +robot=g1/g1_29dof_anneal_23dof
```

### Visualize motion
Run
```bash
python scripts/vis/vis_q_mj.py +robot=g1/g1_29dof_anneal_23dof +visualize_motion_file=&quot;humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-motions_raw_tairantestbed_smpl_video_side_jump_level4_filter_amass.pkl&quot;
```

To test, and you should have you one single motion file located at `humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-motions_raw_tairantestbed_smpl_video_side_jump_level4_filter_amass.pkl`.

If you want to visualize the motion, you can run
```bash
python scripts/vis/vis_q_mj.py +robot=g1/g1_29dof_anneal_23dof +visualize_motion_file=&quot;humanoidverse/data/motions/g1_29dof_anneal_23dof/TairanTestbed/singles/0-motions_raw_tairantestbed_smpl_video_side_jump_level4_filter_amass.pkl&quot;
```
You should have

&lt;img src=&quot;imgs/g1_29dof_anneal_23dof_motion.gif&quot; width=&quot;400px&quot;/&gt;




# Sim2Sim/Sim2Real

## Environment Setup

Env Installation:
```
mamba create -n asap_deploy python=3.10
mamba activate asap_deploy
```



Install ros2-python

```bash
# this adds the conda-forge channel to the new created environment configuration 
conda config --env --add channels conda-forge
# and the robostack channel
conda config --env --add channels robostack-staging
# remove the defaults channel just in case, this might return an error if it is not in the list which is ok
conda config --env --remove channels defaults
# install the ros2-python package
conda install ros-humble-desktop
```

Test Ros2Installation:

```bash
rviz2
```

You should see the UI like this:

&lt;img src=&quot;imgs/rviz.png&quot; width=&quot;400px&quot;/&gt;



Install Unitree SDK

```bash
git clone git@github.com:unitreerobotics/unitree_sdk2_python.git
cd unitree_sdk2_python
pip install -e .
```

minor issue to fix:

```bash
pip install --upgrade numpy scipy
```

## Sim2Sim



start the simulation in the sim2real folder:
```bash
python sim_env/base_sim.py --config=config/g1_29dof_hist.yaml
```

in another terminal, start the policy:
```bash
python rl_policy/deepmimic_dec_loco_height.py --config=config/g1_29dof_hist.yaml --loco_model_path=./models/dec_loco/20250109_231507-noDR_rand_history_loco_stand_height_noise-decoupled_locomotion-g1_29dof/model_6600.onnx --mimic_model_paths=./models/mimic
```

- click to the policy terminal and press `]` to activate the locomotion policy
- click to the policy terminal and press `[` to activate the asap policy (phase-based motion tracking policy)
- click to the policy terminal and press `;` to switch to the asap policy 
- press `i` to make the robot the initial position
- press `o` to emergence stop the robot
- press `9` in mujoco viewer to release the robostack
- press `=` to switch between tapping and walking for the locomotion policy
- press `w/a/s/d` to control the linear velocity
- press `q/e` to control the angular velocity
- press `z` to set all commands to zero


And you should be able to play around with some checkpoints from the ASAP paper. Have fun!

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;img src=&quot;imgs/asap-sim2sim-clip0-ezgif.com-video-to-gif-converter.gif&quot; width=&quot;300px&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;imgs/asap-sim2sim-clip1-ezgif.com-video-to-gif-converter.gif&quot; width=&quot;300px&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;imgs/asap-sim2sim-clip3-ezgif.com-video-to-gif-converter.gif&quot; width=&quot;300px&quot;/&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;img src=&quot;imgs/asap-sim2sim-clip4-ezgif.com-video-to-gif-converter.gif&quot; width=&quot;300px&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;imgs/asap-sim2sim-clip5-ezgif.com-video-to-gif-converter.gif&quot; width=&quot;300px&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;imgs/asap-sim2sim-clip6-ezgif.com-video-to-gif-converter.gif&quot; width=&quot;300px&quot;/&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;!-- Replace gif1.gif ... gif6.gif with your actual gif filenames and optionally add captions below each if desired --&gt;



## Sim2Real

`Note from Tairan`: make sure to make the G1 robot to 29dof following this [doc](https://support.unitree.com/home/en/G1_developer/waist_fastener) and restart the robot after waist unlocking. If you don&#039;t know how to log into the Unitree Explore APP, contact unitree support.

Enter Low-Level for g1
- Open humanoid and wait until the head blue light is constantly on
- `L2+R2`
- `L2+A`
- `L2+B`
- Connect PC to the G1 by ethernet cable and configure the network following [this document](https://support.unitree.com/home/en/G1_developer/quick_development) 

Before starting the policy, modify the `config/g1_29dof_hist.yaml` to set `INTERFACE` to `eth0` (if you are using linux), basically the network interface that you are using to connect to the robot with your PC&#039;s IP shown as `192.168.123.xxx` in `ifconfig`.

start the policy:
```bash
python rl_policy/deepmimic_dec_loco_height.py --config=config/g1_29dof_hist.yaml --loco_model_path=./models/dec_loco/20250109_231507-noDR_rand_history_loco_stand_height_noise-decoupled_locomotion-g1_29dof/model_6600.onnx --mimic_model_paths=./models/mimic
```



- click to the policy terminal and press `]` to activate the locomotion policy
- click to the pol

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[paperless-ngx/paperless-ngx]]></title>
            <link>https://github.com/paperless-ngx/paperless-ngx</link>
            <guid>https://github.com/paperless-ngx/paperless-ngx</guid>
            <pubDate>Fri, 15 Aug 2025 00:04:44 GMT</pubDate>
            <description><![CDATA[A community-supported supercharged document management system: scan, index and archive all your documents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/paperless-ngx/paperless-ngx">paperless-ngx/paperless-ngx</a></h1>
            <p>A community-supported supercharged document management system: scan, index and archive all your documents</p>
            <p>Language: Python</p>
            <p>Stars: 30,515</p>
            <p>Forks: 1,842</p>
            <p>Stars today: 85 stars today</p>
            <h2>README</h2><pre>[![ci](https://github.com/paperless-ngx/paperless-ngx/workflows/ci/badge.svg)](https://github.com/paperless-ngx/paperless-ngx/actions)
[![Crowdin](https://badges.crowdin.net/paperless-ngx/localized.svg)](https://crowdin.com/project/paperless-ngx)
[![Documentation Status](https://img.shields.io/github/deployments/paperless-ngx/paperless-ngx/github-pages?label=docs)](https://docs.paperless-ngx.com)
[![codecov](https://codecov.io/gh/paperless-ngx/paperless-ngx/branch/main/graph/badge.svg?token=VK6OUPJ3TY)](https://codecov.io/gh/paperless-ngx/paperless-ngx)
[![Chat on Matrix](https://matrix.to/img/matrix-badge.svg)](https://matrix.to/#/%23paperlessngx%3Amatrix.org)
[![demo](https://cronitor.io/badges/ve7ItY/production/W5E_B9jkelG9ZbDiNHUPQEVH3MY.svg)](https://demo.paperless-ngx.com)

&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/paperless-ngx/paperless-ngx/blob/main/resources/logo/web/png/White%20logo%20-%20no%20background.png&quot; width=&quot;50%&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png&quot; width=&quot;50%&quot;&gt;
    &lt;img src=&quot;https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png&quot; width=&quot;50%&quot;&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;!-- omit in toc --&gt;

# Paperless-ngx

Paperless-ngx is a document management system that transforms your physical documents into a searchable online archive so you can keep, well, _less paper_.

Paperless-ngx is the official successor to the original [Paperless](https://github.com/the-paperless-project/paperless) &amp; [Paperless-ng](https://github.com/jonaswinkler/paperless-ng) projects and is designed to distribute the responsibility of advancing and supporting the project among a team of people. [Consider joining us!](#community-support)

Thanks to the generous folks at [DigitalOcean](https://m.do.co/c/8d70b916d462), a demo is available at [demo.paperless-ngx.com](https://demo.paperless-ngx.com) using login `demo` / `demo`. _Note: demo content is reset frequently and confidential information should not be uploaded._

- [Features](#features)
- [Getting started](#getting-started)
- [Contributing](#contributing)
  - [Community Support](#community-support)
  - [Translation](#translation)
  - [Feature Requests](#feature-requests)
  - [Bugs](#bugs)
- [Related Projects](#related-projects)
- [Important Note](#important-note)

&lt;p align=&quot;right&quot;&gt;This project is supported by:&lt;br/&gt;
  &lt;a href=&quot;https://m.do.co/c/8d70b916d462&quot; style=&quot;padding-top: 4px; display: block;&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_white.svg&quot; width=&quot;140px&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg&quot; width=&quot;140px&quot;&gt;
      &lt;img src=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_black_.svg&quot; width=&quot;140px&quot;&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
&lt;/p&gt;

# Features

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards-dark.png&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png&quot;&gt;
&lt;/picture&gt;

A full list of [features](https://docs.paperless-ngx.com/#features) and [screenshots](https://docs.paperless-ngx.com/#screenshots) are available in the [documentation](https://docs.paperless-ngx.com/).

# Getting started

The easiest way to deploy paperless is `docker compose`. The files in the [`/docker/compose` directory](https://github.com/paperless-ngx/paperless-ngx/tree/main/docker/compose) are configured to pull the image from the GitHub container registry.

If you&#039;d like to jump right in, you can configure a `docker compose` environment with our install script:

```bash
bash -c &quot;$(curl -L https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/install-paperless-ngx.sh)&quot;
```

More details and step-by-step guides for alternative installation methods can be found in [the documentation](https://docs.paperless-ngx.com/setup/#installation).

Migrating from Paperless-ng is easy, just drop in the new docker image! See the [documentation on migrating](https://docs.paperless-ngx.com/setup/#migrating-to-paperless-ngx) for more details.

&lt;!-- omit in toc --&gt;

### Documentation

The documentation for Paperless-ngx is available at [https://docs.paperless-ngx.com](https://docs.paperless-ngx.com/).

# Contributing

If you feel like contributing to the project, please do! Bug fixes, enhancements, visual fixes etc. are always welcome. If you want to implement something big: Please start a discussion about that! The [documentation](https://docs.paperless-ngx.com/development/) has some basic information on how to get started.

## Community Support

People interested in continuing the work on paperless-ngx are encouraged to reach out here on github and in the [Matrix Room](https://matrix.to/#/#paperless:matrix.org). If you would like to contribute to the project on an ongoing basis there are multiple [teams](https://github.com/orgs/paperless-ngx/people) (frontend, ci/cd, etc) that could use your help so please reach out!

## Translation

Paperless-ngx is available in many languages that are coordinated on Crowdin. If you want to help out by translating paperless-ngx into your language, please head over to https://crowdin.com/project/paperless-ngx, and thank you! More details can be found in [CONTRIBUTING.md](https://github.com/paperless-ngx/paperless-ngx/blob/main/CONTRIBUTING.md#translating-paperless-ngx).

## Feature Requests

Feature requests can be submitted via [GitHub Discussions](https://github.com/paperless-ngx/paperless-ngx/discussions/categories/feature-requests), you can search for existing ideas, add your own and vote for the ones you care about.

## Bugs

For bugs please [open an issue](https://github.com/paperless-ngx/paperless-ngx/issues) or [start a discussion](https://github.com/paperless-ngx/paperless-ngx/discussions) if you have questions.

# Related Projects

Please see [the wiki](https://github.com/paperless-ngx/paperless-ngx/wiki/Related-Projects) for a user-maintained list of related projects and software that is compatible with Paperless-ngx.

# Important Note

&gt; Document scanners are typically used to scan sensitive documents like your social insurance number, tax records, invoices, etc. **Paperless-ngx should never be run on an untrusted host** because information is stored in clear text without encryption. No guarantees are made regarding security (but we do try!) and you use the app at your own risk.
&gt; **The safest way to run Paperless-ngx is on a local server in your own home with backups in place**.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[datalab-to/marker]]></title>
            <link>https://github.com/datalab-to/marker</link>
            <guid>https://github.com/datalab-to/marker</guid>
            <pubDate>Fri, 15 Aug 2025 00:04:43 GMT</pubDate>
            <description><![CDATA[Convert PDF to markdown + JSON quickly with high accuracy]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/datalab-to/marker">datalab-to/marker</a></h1>
            <p>Convert PDF to markdown + JSON quickly with high accuracy</p>
            <p>Language: Python</p>
            <p>Stars: 27,496</p>
            <p>Forks: 1,803</p>
            <p>Stars today: 126 stars today</p>
            <h2>README</h2><pre># Marker

Marker converts documents to markdown, JSON, chunks, and HTML quickly and accurately.

- Converts PDF, image, PPTX, DOCX, XLSX, HTML, EPUB files in all languages
- Formats tables, forms, equations, inline math, links, references, and code blocks
- Extracts and saves images
- Removes headers/footers/other artifacts
- Extensible with your own formatting and logic
- Does structured extraction, given a JSON schema (beta)
- Optionally boost accuracy with LLMs (and your own prompt)
- Works on GPU, CPU, or MPS

## Performance

&lt;img src=&quot;data/images/overall.png&quot; width=&quot;800px&quot;/&gt;

Marker benchmarks favorably compared to cloud services like Llamaparse and Mathpix, as well as other open source tools.

The above results are running single PDF pages serially.  Marker is significantly faster when running in batch mode, with a projected throughput of 25 pages/second on an H100.

See [below](#benchmarks) for detailed speed and accuracy benchmarks, and instructions on how to run your own benchmarks.

## Hybrid Mode

For the highest accuracy, pass the `--use_llm` flag to use an LLM alongside marker.  This will do things like merge tables across pages, handle inline math, format tables properly, and extract values from forms.  It can use any gemini or ollama model.  By default, it uses `gemini-2.0-flash`.  See [below](#llm-services) for details.

Here is a table benchmark comparing marker, gemini flash alone, and marker with use_llm:

&lt;img src=&quot;data/images/table.png&quot; width=&quot;400px&quot;/&gt;

As you can see, the use_llm mode offers higher accuracy than marker or gemini alone.

## Examples

| PDF | File type | Markdown                                                                                                                     | JSON                                                                                                   |
|-----|-----------|------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|
| [Think Python](https://greenteapress.com/thinkpython/thinkpython.pdf) | Textbook | [View](https://github.com/VikParuchuri/marker/blob/master/data/examples/markdown/thinkpython/thinkpython.md)                 | [View](https://github.com/VikParuchuri/marker/blob/master/data/examples/json/thinkpython.json)         |
| [Switch Transformers](https://arxiv.org/pdf/2101.03961.pdf) | arXiv paper | [View](https://github.com/VikParuchuri/marker/blob/master/data/examples/markdown/switch_transformers/switch_trans.md) | [View](https://github.com/VikParuchuri/marker/blob/master/data/examples/json/switch_trans.json) |
| [Multi-column CNN](https://arxiv.org/pdf/1804.07821.pdf) | arXiv paper | [View](https://github.com/VikParuchuri/marker/blob/master/data/examples/markdown/multicolcnn/multicolcnn.md)                 | [View](https://github.com/VikParuchuri/marker/blob/master/data/examples/json/multicolcnn.json)         |

# Commercial usage

I want marker to be as widely accessible as possible, while still funding my development/training costs.  Research and personal usage is always okay, but there are some restrictions on commercial usage.

The weights for the models are licensed `cc-by-nc-sa-4.0`, but I will waive that for any organization under \$2M USD in gross revenue in the most recent 12-month period AND under \$2M in lifetime VC/angel funding raised. You also must not be competitive with the [Datalab API](https://www.datalab.to/).  If you want to remove the GPL license requirements (dual-license) and/or use the weights commercially over the revenue limit, check out the options [here](https://www.datalab.to).

# Hosted API

There&#039;s a hosted API for marker available [here](https://www.datalab.to/):

- Supports PDF, image, PPT, PPTX, DOC, DOCX, XLS, XLSX, HTML, EPUB files
- 1/4th the price of leading cloud-based competitors
- Fast - ~15s for a 250 page PDF
- Supports LLM mode
- High uptime (99.99%)

# Community

[Discord](https://discord.gg//KuZwXNGnfH) is where we discuss future development.

# Installation

You&#039;ll need python 3.10+ and [PyTorch](https://pytorch.org/get-started/locally/).

Install with:

```shell
pip install marker-pdf
```

If you want to use marker on documents other than PDFs, you will need to install additional dependencies with:

```shell
pip install marker-pdf[full]
```

# Usage

First, some configuration:

- Your torch device will be automatically detected, but you can override this.  For example, `TORCH_DEVICE=cuda`.
- Some PDFs, even digital ones, have bad text in them.  Set `--force_ocr` to force OCR on all lines, or the `strip_existing_ocr` to keep all digital text, and strip out any existing OCR text.
- If you care about inline math, set `force_ocr` to convert inline math to LaTeX.

## Interactive App

I&#039;ve included a streamlit app that lets you interactively try marker with some basic options.  Run it with:

```shell
pip install streamlit streamlit-ace
marker_gui
```

## Convert a single file

```shell
marker_single /path/to/file.pdf
```

You can pass in PDFs or images.

Options:
- `--page_range TEXT`: Specify which pages to process. Accepts comma-separated page numbers and ranges. Example: `--page_range &quot;0,5-10,20&quot;` will process pages 0, 5 through 10, and page 20.
- `--output_format [markdown|json|html|chunks]`: Specify the format for the output results.
- `--output_dir PATH`: Directory where output files will be saved. Defaults to the value specified in settings.OUTPUT_DIR.
- `--paginate_output`: Paginates the output, using `\n\n{PAGE_NUMBER}` followed by `-` * 48, then `\n\n` 
- `--use_llm`: Uses an LLM to improve accuracy.  You will need to configure the LLM backend - see [below](#llm-services).
- `--force_ocr`: Force OCR processing on the entire document, even for pages that might contain extractable text.  This will also format inline math properly.
- `--block_correction_prompt`: if LLM mode is active, an optional prompt that will be used to correct the output of marker.  This is useful for custom formatting or logic that you want to apply to the output.
- `--strip_existing_ocr`: Remove all existing OCR text in the document and re-OCR with surya.
- `--redo_inline_math`: If you want the absolute highest quality inline math conversion, use this along with `--use_llm`.
- `--disable_image_extraction`: Don&#039;t extract images from the PDF.  If you also specify `--use_llm`, then images will be replaced with a description.
- `--debug`: Enable debug mode for additional logging and diagnostic information.
- `--processors TEXT`: Override the default processors by providing their full module paths, separated by commas. Example: `--processors &quot;module1.processor1,module2.processor2&quot;`
- `--config_json PATH`: Path to a JSON configuration file containing additional settings.
- `config --help`: List all available builders, processors, and converters, and their associated configuration.  These values can be used to build a JSON configuration file for additional tweaking of marker defaults.
- `--converter_cls`: One of `marker.converters.pdf.PdfConverter` (default) or `marker.converters.table.TableConverter`.  The `PdfConverter` will convert the whole PDF, the `TableConverter` will only extract and convert tables.
- `--llm_service`: Which llm service to use if `--use_llm` is passed.  This defaults to `marker.services.gemini.GoogleGeminiService`.
- `--help`: see all of the flags that can be passed into marker.  (it supports many more options then are listed above)

The list of supported languages for surya OCR is [here](https://github.com/VikParuchuri/surya/blob/master/surya/recognition/languages.py).  If you don&#039;t need OCR, marker can work with any language.

## Convert multiple files

```shell
marker /path/to/input/folder
```

- `marker` supports all the same options from `marker_single` above.
- `--workers` is the number of conversion workers to run simultaneously.  This is automatically set by default, but you can increase it to increase throughput, at the cost of more CPU/GPU usage.  Marker will use 5GB of VRAM per worker at the peak, and 3.5GB average.

## Convert multiple files on multiple GPUs

```shell
NUM_DEVICES=4 NUM_WORKERS=15 marker_chunk_convert ../pdf_in ../md_out
```

- `NUM_DEVICES` is the number of GPUs to use.  Should be `2` or greater.
- `NUM_WORKERS` is the number of parallel processes to run on each GPU.

## Use from python

See the `PdfConverter` class at `marker/converters/pdf.py` function for additional arguments that can be passed.

```python
from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.output import text_from_rendered

converter = PdfConverter(
    artifact_dict=create_model_dict(),
)
rendered = converter(&quot;FILEPATH&quot;)
text, _, images = text_from_rendered(rendered)
```

`rendered` will be a pydantic basemodel with different properties depending on the output type requested.  With markdown output (default), you&#039;ll have the properties `markdown`, `metadata`, and `images`.  For json output, you&#039;ll have `children`, `block_type`, and `metadata`.

### Custom configuration

You can pass configuration using the `ConfigParser`.  To see all available options, do `marker_single --help`.

```python
from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.config.parser import ConfigParser

config = {
    &quot;output_format&quot;: &quot;json&quot;,
    &quot;ADDITIONAL_KEY&quot;: &quot;VALUE&quot;
}
config_parser = ConfigParser(config)

converter = PdfConverter(
    config=config_parser.generate_config_dict(),
    artifact_dict=create_model_dict(),
    processor_list=config_parser.get_processors(),
    renderer=config_parser.get_renderer(),
    llm_service=config_parser.get_llm_service()
)
rendered = converter(&quot;FILEPATH&quot;)
```

### Extract blocks

Each document consists of one or more pages.  Pages contain blocks, which can themselves contain other blocks.  It&#039;s possible to programmatically manipulate these blocks.  

Here&#039;s an example of extracting all forms from a document:

```python
from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.schema import BlockTypes

converter = PdfConverter(
    artifact_dict=create_model_dict(),
)
document = converter.build_document(&quot;FILEPATH&quot;)
forms = document.contained_blocks((BlockTypes.Form,))
```

Look at the processors for more examples of extracting and manipulating blocks.

## Other converters

You can also use other converters that define different conversion pipelines:

### Extract tables

The `TableConverter` will only convert and extract tables:

```python
from marker.converters.table import TableConverter
from marker.models import create_model_dict
from marker.output import text_from_rendered

converter = TableConverter(
    artifact_dict=create_model_dict(),
)
rendered = converter(&quot;FILEPATH&quot;)
text, _, images = text_from_rendered(rendered)
```

This takes all the same configuration as the PdfConverter.  You can specify the configuration `force_layout_block=Table` to avoid layout detection and instead assume every page is a table.  Set `output_format=json` to also get cell bounding boxes.

You can also run this via the CLI with 
```shell
marker_single FILENAME --use_llm --force_layout_block Table --converter_cls marker.converters.table.TableConverter --output_format json
```

### OCR Only

If you only want to run OCR, you can also do that through the `OCRConverter`.  Set `--keep_chars` to keep individual characters and bounding boxes.

```python
from marker.converters.ocr import OCRConverter
from marker.models import create_model_dict

converter = OCRConverter(
    artifact_dict=create_model_dict(),
)
rendered = converter(&quot;FILEPATH&quot;)
```

This takes all the same configuration as the PdfConverter.

You can also run this via the CLI with 
```shell
marker_single FILENAME --converter_cls marker.converters.ocr.OCRConverter
```

### Structured Extraction (beta)

You can run structured extraction via the `ExtractionConverter`.  This requires an llm service to be setup first (see [here](#llm-services) for details).  You&#039;ll get a JSON output with the extracted values.

```python
from marker.converters.extraction import ExtractionConverter
from marker.models import create_model_dict
from marker.config.parser import ConfigParser
from pydantic import BaseModel

class Links(BaseModel):
    links: list[str]
    
schema = Links.model_json_schema()
config_parser = ConfigParser({
    &quot;page_schema&quot;: schema
})

converter = ExtractionConverter(
    artifact_dict=create_model_dict(),
    config=config_parser.generate_config_dict(),
    llm_service=config_parser.get_llm_service(),
)
rendered = converter(&quot;FILEPATH&quot;)
```

Rendered will have an `original_markdown` field.  If you pass this back in next time you run the converter, as the `existing_markdown` config key, you can skip re-parsing the document.

# Output Formats

## Markdown

Markdown output will include:

- image links (images will be saved in the same folder)
- formatted tables
- embedded LaTeX equations (fenced with `$$`)
- Code is fenced with triple backticks
- Superscripts for footnotes

## HTML

HTML output is similar to markdown output:

- Images are included via `img` tags
- equations are fenced with `&lt;math&gt;` tags
- code is in `pre` tags

## JSON

JSON output will be organized in a tree-like structure, with the leaf nodes being blocks.  Examples of leaf nodes are a single list item, a paragraph of text, or an image.

The output will be a list, with each list item representing a page.  Each page is considered a block in the internal marker schema.  There are different types of blocks to represent different elements.  

Pages have the keys:

- `id` - unique id for the block.
- `block_type` - the type of block. The possible block types can be seen in `marker/schema/__init__.py`.  As of this writing, they are [&quot;Line&quot;, &quot;Span&quot;, &quot;FigureGroup&quot;, &quot;TableGroup&quot;, &quot;ListGroup&quot;, &quot;PictureGroup&quot;, &quot;Page&quot;, &quot;Caption&quot;, &quot;Code&quot;, &quot;Figure&quot;, &quot;Footnote&quot;, &quot;Form&quot;, &quot;Equation&quot;, &quot;Handwriting&quot;, &quot;TextInlineMath&quot;, &quot;ListItem&quot;, &quot;PageFooter&quot;, &quot;PageHeader&quot;, &quot;Picture&quot;, &quot;SectionHeader&quot;, &quot;Table&quot;, &quot;Text&quot;, &quot;TableOfContents&quot;, &quot;Document&quot;]
- `html` - the HTML for the page.  Note that this will have recursive references to children.  The `content-ref` tags must be replaced with the child content if you want the full html.  You can see an example of this at `marker/output.py:json_to_html`.  That function will take in a single block from the json output, and turn it into HTML.
- `polygon` - the 4-corner polygon of the page, in (x1,y1), (x2,y2), (x3, y3), (x4, y4) format.  (x1,y1) is the top left, and coordinates go clockwise.
- `children` - the child blocks.

The child blocks have two additional keys:

- `section_hierarchy` - indicates the sections that the block is part of.  `1` indicates an h1 tag, `2` an h2, and so on.
- `images` - base64 encoded images.  The key will be the block id, and the data will be the encoded image.

Note that child blocks of pages can have their own children as well (a tree structure).

```json
{
      &quot;id&quot;: &quot;/page/10/Page/366&quot;,
      &quot;block_type&quot;: &quot;Page&quot;,
      &quot;html&quot;: &quot;&lt;content-ref src=&#039;/page/10/SectionHeader/0&#039;&gt;&lt;/content-ref&gt;&lt;content-ref src=&#039;/page/10/SectionHeader/1&#039;&gt;&lt;/content-ref&gt;&lt;content-ref src=&#039;/page/10/Text/2&#039;&gt;&lt;/content-ref&gt;&lt;content-ref src=&#039;/page/10/Text/3&#039;&gt;&lt;/content-ref&gt;&lt;content-ref src=&#039;/page/10/Figure/4&#039;&gt;&lt;/content-ref&gt;&lt;content-ref src=&#039;/page/10/SectionHeader/5&#039;&gt;&lt;/content-ref&gt;&lt;content-ref src=&#039;/page/10/SectionHeader/6&#039;&gt;&lt;/content-ref&gt;&lt;content-ref src=&#039;/page/10/TextInlineMath/7&#039;&gt;&lt;/content-ref&gt;&lt;content-ref src=&#039;/page/10/TextInlineMath/8&#039;&gt;&lt;/content-ref&gt;&lt;content-ref src=&#039;/page/10/Table/9&#039;&gt;&lt;/content-ref&gt;&lt;content-ref src=&#039;/page/10/SectionHeader/10&#039;&gt;&lt;/content-ref&gt;&lt;content-ref src=&#039;/page/10/Text/11&#039;&gt;&lt;/content-ref&gt;&quot;,
      &quot;polygon&quot;: [[0.0, 0.0], [612.0, 0.0], [612.0, 792.0], [0.0, 792.0]],
      &quot;children&quot;: [
        {
          &quot;id&quot;: &quot;/page/10/SectionHeader/0&quot;,
          &quot;block_type&quot;: &quot;SectionHeader&quot;,
          &quot;html&quot;: &quot;&lt;h1&gt;Supplementary Material for &lt;i&gt;Subspace Adversarial Training&lt;/i&gt; &lt;/h1&gt;&quot;,
          &quot;polygon&quot;: [
            [217.845703125, 80.630859375], [374.73046875, 80.630859375],
            [374.73046875, 107.0],
            [217.845703125, 107.0]
          ],
          &quot;children&quot;: null,
          &quot;section_hierarchy&quot;: {
            &quot;1&quot;: &quot;/page/10/SectionHeader/1&quot;
          },
          &quot;images&quot;: {}
        },
        ...
        ]
    }


```

## Chunks

Chunks format is similar to JSON, but flattens everything into a single list instead of a tree.  Only the top level blocks from each page show up. It also has the full HTML of each block inside, so you don&#039;t need to crawl the tree to reconstruct it.  This enable flexible and easy chunking for RAG.

## Metadata

All output formats will return a metadata dictionary, with the following fields:

```json
{
    &quot;table_of_contents&quot;: [
      {
        &quot;title&quot;: &quot;Introduction&quot;,
        &quot;heading_level&quot;: 1,
        &quot;page_id&quot;: 0,
        &quot;polygon&quot;: [...]
      }
    ], // computed PDF table of contents
    &quot;page_stats&quot;: [
      {
        &quot;page_id&quot;:  0, 
        &quot;text_extraction_method&quot;: &quot;pdftext&quot;,
        &quot;block_counts&quot;: [(&quot;Span&quot;, 200), ...]
      },
      ...
    ]
}
```

# LLM Services

When running with the `--use_llm` flag, you have a choice of services you can use:

- `Gemini` - this will use the Gemini developer API by default.  You&#039;ll need to pass `--gemini_api_key` to configuration.
- `Google Vertex` - this will use vertex, which can be more reliable.  You&#039;ll need to pass `--vertex_project_id`.  To use it, set `--llm_service=marker.services.vertex.GoogleVertexService`.
- `Ollama` - this will use local models.  You can configure `--ollama_base_url` and `--ollama_model`. To use it, set `--llm_service=marker.services.ollama.OllamaService`.
- `Claude` - this will use the anthropic API.  You can configure `--claude_api_key`, and `--claude_model_name`.  To use it, set `--llm_service=marker.services.claude.ClaudeService`.
- `OpenAI` - this supports any openai-like endpoint. You can configure `--openai_api_key`, `--openai_model`, and `--openai_base_url`. To use it, set `--llm_service=marker.services.openai.OpenAIService`.
- `Azure OpenAI` - this uses the Azure OpenAI service. You can configure `--azure_endpoint`, `--azure_api_key`, and `--deployment_name`. To use it, set `--llm_service=marker.services.azure_openai.AzureOpenAIService`.

These services may have additional optional configuration as well - you can see it by viewing the classes.

# Internals

Marker is easy to extend.  The core units of marker are:

- `Providers`, at `marker/providers`.  These provide information from a source file, like a PDF.
- `Builders`, at `marker/builders`.  These generate the initial document blocks and fill in text, using info from the providers.
- `Processors`, at `marker/processors`.  These process specific blocks, for example the table formatter is a processor.
- `Renderers`, at `marker/renderers`. These use the blocks to render output.
- `Schema`, at `marker/schema`.  The classes for all the block types.
- `Converters`, at `marker/converters`.  They run the whole end to end pipeline.

To customize processing behavior, override the `processors`.  To add new output formats, write a new `renderer`.  For additional input formats, write a new `provider.`

Processors and renderers can be directly passed into the base `PDFConverter`, so you can specify your own custom processing easily.

## API server

There is a very simple API server you can run like this:

```shell
pip install -U uvicorn fastapi python-multipart
marker_server --port 8001
```

This will start a fastapi server that you can access at `localhost:8001`.  You can go to `localhost:8001/docs` to see the endpoint options.

You can send requests like this:

```
import requests
import json

post_data = {
    &#039;filepath&#039;: &#039;FILEPATH&#039;,
    # Add other params here
}

reque

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[PacktPublishing/LLM-Engineers-Handbook]]></title>
            <link>https://github.com/PacktPublishing/LLM-Engineers-Handbook</link>
            <guid>https://github.com/PacktPublishing/LLM-Engineers-Handbook</guid>
            <pubDate>Fri, 15 Aug 2025 00:04:42 GMT</pubDate>
            <description><![CDATA[The LLM's practical guide: From the fundamentals to deploying advanced LLM and RAG apps to AWS using LLMOps best practices]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook">PacktPublishing/LLM-Engineers-Handbook</a></h1>
            <p>The LLM's practical guide: From the fundamentals to deploying advanced LLM and RAG apps to AWS using LLMOps best practices</p>
            <p>Language: Python</p>
            <p>Stars: 3,884</p>
            <p>Forks: 878</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;h1&gt;üë∑ LLM Engineer&#039;s Handbook&lt;/h1&gt;
  &lt;p class=&quot;tagline&quot;&gt;Official repository of the &lt;a href=&quot;https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/&quot;&gt;LLM Engineer&#039;s Handbook&lt;/a&gt; by &lt;a href=&quot;https://github.com/iusztinpaul&quot;&gt;Paul Iusztin&lt;/a&gt; and &lt;a href=&quot;https://github.com/mlabonne&quot;&gt;Maxime Labonne&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/br&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/&quot;&gt;
    &lt;img src=&quot;images/cover_plus.png&quot; alt=&quot;Book cover&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  Find the book on &lt;a href=&quot;https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/&quot;&gt;Amazon&lt;/a&gt; or &lt;a href=&quot;https://www.packtpub.com/en-us/product/llm-engineers-handbook-9781836200062&quot;&gt;Packt&lt;/a&gt;
&lt;/p&gt;

## üåü Features

The goal of this book is to create your own end-to-end LLM-based system using best practices:

- üìù Data collection &amp; generation
- üîÑ LLM training pipeline
- üìä Simple RAG system
- üöÄ Production-ready AWS deployment
- üîç Comprehensive monitoring
- üß™ Testing and evaluation framework

You can download and use the final trained model on [Hugging Face](https://huggingface.co/mlabonne/TwinLlama-3.1-8B-DPO).

&gt; [!IMPORTANT]
&gt; The code in this GitHub repository is actively maintained and may contain updates not reflected in the book. **Always refer to this repository for the latest version of the code.**

## üîó Dependencies

### Local dependencies

To install and run the project locally, you need the following dependencies.

| Tool | Version | Purpose | Installation Link |
|------|---------|---------|------------------|
| pyenv | ‚â•2.3.36 | Multiple Python versions (optional) | [Install Guide](https://github.com/pyenv/pyenv?tab=readme-ov-file#installation) |
| Python | 3.11 | Runtime environment | [Download](https://www.python.org/downloads/) |
| Poetry | &gt;= 1.8.3 and &lt; 2.0 | Package management | [Install Guide](https://python-poetry.org/docs/#installation) |
| Docker | ‚â•27.1.1 | Containerization | [Install Guide](https://docs.docker.com/engine/install/) |
| AWS CLI | ‚â•2.15.42 | Cloud management | [Install Guide](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) |
| Git | ‚â•2.44.0 | Version control | [Download](https://git-scm.com/downloads) |

### Cloud services

The code also uses and depends on the following cloud services. For now, you don&#039;t have to do anything. We will guide you in the installation and deployment sections on how to use them:

| Service | Purpose |
|---------|---------|
| [HuggingFace](https://huggingface.com/) | Model registry |
| [Comet ML](https://www.comet.com/site/products/opik/?utm_source=llm_handbook&amp;utm_medium=github&amp;utm_campaign=opik) | Experiment tracker |
| [Opik](https://www.comet.com/site/products/opik/?utm_source=llm_handbook&amp;utm_medium=github&amp;utm_campaign=opik) | Prompt monitoring |
| [ZenML](https://www.zenml.io/) | Orchestrator and artifacts layer |
| [AWS](https://aws.amazon.com/) | Compute and storage |
| [MongoDB](https://www.mongodb.com/) | NoSQL database |
| [Qdrant](https://qdrant.tech/) | Vector database |
| [GitHub Actions](https://github.com/features/actions) | CI/CD pipeline |

In the [LLM Engineer&#039;s Handbook](https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/), Chapter 2 will walk you through each tool. Chapters 10 and 11 provide step-by-step guides on how to set up everything you need.

## üóÇÔ∏è Project Structure

Here is the directory overview:

```bash
.
‚îú‚îÄ‚îÄ code_snippets/       # Standalone example code
‚îú‚îÄ‚îÄ configs/             # Pipeline configuration files
‚îú‚îÄ‚îÄ llm_engineering/     # Core project package
‚îÇ   ‚îú‚îÄ‚îÄ application/    
‚îÇ   ‚îú‚îÄ‚îÄ domain/         
‚îÇ   ‚îú‚îÄ‚îÄ infrastructure/ 
‚îÇ   ‚îú‚îÄ‚îÄ model/         
‚îú‚îÄ‚îÄ pipelines/           # ML pipeline definitions
‚îú‚îÄ‚îÄ steps/               # Pipeline components
‚îú‚îÄ‚îÄ tests/               # Test examples
‚îú‚îÄ‚îÄ tools/               # Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ run.py
‚îÇ   ‚îú‚îÄ‚îÄ ml_service.py
‚îÇ   ‚îú‚îÄ‚îÄ rag.py
‚îÇ   ‚îú‚îÄ‚îÄ data_warehouse.py
```

`llm_engineering/`  is the main Python package implementing LLM and RAG functionality. It follows Domain-Driven Design (DDD) principles:

- `domain/`: Core business entities and structures
- `application/`: Business logic, crawlers, and RAG implementation
- `model/`: LLM training and inference
- `infrastructure/`: External service integrations (AWS, Qdrant, MongoDB, FastAPI)

The code logic and imports flow as follows: `infrastructure` ‚Üí `model` ‚Üí `application` ‚Üí `domain`

`pipelines/`: Contains the ZenML ML pipelines, which serve as the entry point for all the ML pipelines. Coordinates the data processing and model training stages of the ML lifecycle.

`steps/`: Contains individual ZenML steps, which are reusable components for building and customizing ZenML pipelines. Steps perform specific tasks (e.g., data loading, preprocessing) and can be combined within the ML pipelines.

`tests/`: Covers a few sample tests used as examples within the CI pipeline.

`tools/`: Utility scripts used to call the ZenML pipelines and inference code:
- `run.py`: Entry point script to run ZenML pipelines.
- `ml_service.py`: Starts the REST API inference server.
- `rag.py`: Demonstrates usage of the RAG retrieval module.
- `data_warehouse.py`: Used to export or import data from the MongoDB data warehouse through JSON files.

`configs/`: ZenML YAML configuration files to control the execution of pipelines and steps.

`code_snippets/`: Independent code examples that can be executed independently.

## üíª Installation

&gt; [!NOTE]
&gt; If you are experiencing issues while installing and running the repository, consider checking the [Issues](https://github.com/PacktPublishing/LLM-Engineers-Handbook/issues) GitHub section for other people who solved similar problems or directly asking us for help.

### 1. Clone the Repository

Start by cloning the repository and navigating to the project directory:

```bash
git clone https://github.com/PacktPublishing/LLM-Engineers-Handbook.git
cd LLM-Engineers-Handbook 
```

Next, we have to prepare your Python environment and its adjacent dependencies. 

### 2. Set Up Python Environment

The project requires Python 3.11. You can either use your global Python installation or set up a project-specific version using pyenv.

#### Option A: Using Global Python (if version 3.11 is installed)

Verify your Python version:

```bash
python --version  # Should show Python 3.11.x
```

#### Option B: Using pyenv (recommended)

1. Verify pyenv installation:

```bash
pyenv --version   # Should show pyenv 2.3.36 or later
```

2. Install Python 3.11.8:

```bash
pyenv install 3.11.8
```

3. Verify the installation:

```bash
python --version  # Should show Python 3.11.8
```

4. Confirm Python version in the project directory:

```bash
python --version
# Output: Python 3.11.8
```

&gt; [!NOTE]  
&gt; The project includes a `.python-version` file that automatically sets the correct Python version when you&#039;re in the project directory.

### 3. Install Dependencies

The project uses Poetry for dependency management.

1. Verify Poetry installation:

```bash
poetry --version  # Should show Poetry version 1.8.3 or later
```

2. Set up the project environment and install dependencies:

```bash
poetry env use 3.11
poetry install --without aws
poetry run pre-commit install
```

This will:

- Configure Poetry to use Python 3.11
- Install project dependencies (excluding AWS-specific packages)
- Set up pre-commit hooks for code verification

### 4. Activate the Environment

As our task manager, we run all the scripts using [Poe the Poet](https://poethepoet.natn.io/index.html).

1. Start a Poetry shell:

```bash
poetry shell
```

2. Run project commands using Poe the Poet:

```bash
poetry poe ...
```

&lt;details&gt;
&lt;summary&gt;üîß Troubleshooting Poe the Poet Installation&lt;/summary&gt;

### Alternative Command Execution

If you&#039;re experiencing issues with `poethepoet`, you can still run the project commands directly through Poetry. Here&#039;s how:

1. Look up the command definition in `pyproject.toml`
2. Use `poetry run` with the underlying command

#### Example:
Instead of:
```bash
poetry poe local-infrastructure-up
```
Use the direct command from pyproject.toml:
```bash
poetry run &lt;actual-command-from-pyproject-toml&gt;
```
Note: All project commands are defined in the [tool.poe.tasks] section of pyproject.toml
&lt;/details&gt;

Now, let&#039;s configure our local project with all the necessary credentials and tokens to run the code locally.

### 5. Local Development Setup

After you have installed all the dependencies, you must create and fill a¬†`.env` file with your credentials to appropriately interact with other services and run the project. Setting your sensitive credentials in a `.env` file is a good security practice, as this file won&#039;t be committed to GitHub or shared with anyone else. 

1. First, copy our example by running the following:

```bash
cp .env.example .env # The file must be at your repository&#039;s root!
```

2. Now, let&#039;s understand how to fill in all the essential variables within the `.env` file to get you started. The following are the mandatory settings we must complete when working locally:

#### OpenAI

To authenticate to OpenAI&#039;s API, you must fill out the `OPENAI_API_KEY` env var with an authentication token.

```env
OPENAI_API_KEY=your_api_key_here
```

‚Üí Check out this [tutorial](https://platform.openai.com/docs/quickstart) to learn how to provide one from OpenAI.

#### Hugging Face

To authenticate to Hugging Face, you must fill out the `HUGGINGFACE_ACCESS_TOKEN` env var with an authentication token.

```env
HUGGINGFACE_ACCESS_TOKEN=your_token_here
```

‚Üí Check out this [tutorial](https://huggingface.co/docs/hub/en/security-tokens) to learn how to provide one from Hugging Face.

#### Comet ML &amp; Opik

To authenticate to Comet ML (required only during training) and Opik, you must fill out the `COMET_API_KEY` env var with your authentication token.

```env
COMET_API_KEY=your_api_key_here
```

‚Üí Check out this [tutorial](https://www.comet.com/docs/opik/?utm_source=llm_handbook&amp;utm_medium=github&amp;utm_campaign=opik) to learn how to get started with Opik. You can also access Opik&#039;s dashboard using üîó[this link](https://www.comet.com/opik?utm_source=llm_handbook&amp;utm_medium=github&amp;utm_content=opik).

### 6. Deployment Setup

When deploying the project to the cloud, we must set additional settings for Mongo, Qdrant, and AWS. If you are just working locally, the default values of these env vars will work out of the box. Detailed deployment instructions are available in Chapter 11 of the [LLM Engineer&#039;s Handbook](https://www.amazon.com/LLM-Engineers-Handbook-engineering-production/dp/1836200072/).

#### MongoDB

We must change the `DATABASE_HOST` env var with the URL pointing to your cloud MongoDB cluster.

```env
DATABASE_HOST=your_mongodb_url
```

‚Üí Check out this [tutorial](https://www.mongodb.com/resources/products/fundamentals/mongodb-cluster-setup) to learn how to create and host a MongoDB cluster for free.

#### Qdrant

Change `USE_QDRANT_CLOUD` to `true`, `QDRANT_CLOUD_URL` with the URL point to your cloud Qdrant cluster, and `QDRANT_APIKEY` with its API key.

```env
USE_QDRANT_CLOUD=true
QDRANT_CLOUD_URL=your_qdrant_cloud_url
QDRANT_APIKEY=your_qdrant_api_key
```

‚Üí Check out this [tutorial](https://qdrant.tech/documentation/cloud/create-cluster/) to learn how to create a Qdrant cluster for free

#### AWS

For your AWS set-up to work correctly, you need the AWS CLI installed on your local machine and properly configured with an admin user (or a user with enough permissions to create new SageMaker, ECR, and S3 resources; using an admin user will make everything more straightforward).

Chapter 2 provides step-by-step instructions on how to install the AWS CLI, create an admin user on AWS, and get an access key to set up the `AWS_ACCESS_KEY` and `AWS_SECRET_KEY` environment variables. If you already have an AWS admin user in place, you have to configure the following env vars in your `.env` file:

```bash
AWS_REGION=eu-central-1 # Change it with your AWS region.
AWS_ACCESS_KEY=your_aws_access_key
AWS_SECRET_KEY=your_aws_secret_key
```

AWS credentials are typically stored in `~/.aws/credentials`. You can view this file directly using `cat` or similar commands:

```bash
cat ~/.aws/credentials
```

&gt; [!IMPORTANT]
&gt; Additional configuration options are available in [settings.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/settings.py). Any variable in the `Settings` class can be configured through the `.env` file. 

## üèóÔ∏è Infrastructure

### Local infrastructure (for testing and development)

When running the project locally, we host a MongoDB and Qdrant database using Docker. Also, a testing ZenML server is made available through their Python package.

&gt; [!WARNING]
&gt; You need Docker installed (&gt;= v27.1.1)

For ease of use, you can start the whole local development infrastructure with the following command:
```bash
poetry poe local-infrastructure-up
```

Also, you can stop the ZenML server and all the Docker containers using the following command:
```bash
poetry poe local-infrastructure-down
```

&gt; [!WARNING]  
&gt; When running on MacOS, before starting the server, export the following environment variable:
&gt; `export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES`
&gt; Otherwise, the connection between the local server and pipeline will break. üîó More details in [this issue](https://github.com/zenml-io/zenml/issues/2369).
&gt; This is done by default when using Poe the Poet.

Start the inference real-time RESTful API:
```bash
poetry poe run-inference-ml-service
```

&gt; [!IMPORTANT]
&gt; The LLM microservice, called by the RESTful API, will work only after deploying the LLM to AWS SageMaker.

#### ZenML

Dashboard URL: `localhost:8237`

Default credentials:
  - `username`: default
  - `password`: 

‚Üí Find out more about using and setting up [ZenML](https://docs.zenml.io/).

#### Qdrant

REST API URL: `localhost:6333`

Dashboard URL: `localhost:6333/dashboard`

‚Üí Find out more about using and setting up [Qdrant with Docker](https://qdrant.tech/documentation/quick-start/).

#### MongoDB

Database URI: `mongodb://llm_engineering:llm_engineering@127.0.0.1:27017`

Database name: `twin`

Default credentials:
  - `username`: llm_engineering
  - `password`: llm_engineering

‚Üí Find out more about using and setting up [MongoDB with Docker](https://www.mongodb.com/docs/manual/tutorial/install-mongodb-community-with-docker).

You can search your MongoDB collections using your **IDEs MongoDB plugin** (which you have to install separately), where you have to use the database URI to connect to the MongoDB database hosted within the Docker container: `mongodb://llm_engineering:llm_engineering@127.0.0.1:27017`

&gt; [!IMPORTANT]
&gt; Everything related to training or running the LLMs (e.g., training, evaluation, inference) can only be run if you set up AWS SageMaker, as explained in the next section on cloud infrastructure.

### Cloud infrastructure (for production)

Here we will quickly present how to deploy the project to AWS and other serverless services. We won&#039;t go into the details (as everything is presented in the book) but only point out the main steps you have to go through.

First, reinstall your Python dependencies with the AWS group:
```bash
poetry install --with aws
```

#### AWS SageMaker

&gt; [!NOTE]
&gt; Chapter 10 provides step-by-step instructions in the section &quot;Implementing the LLM microservice using AWS SageMaker&quot;.

By this point, we expect you to have AWS CLI installed and your AWS CLI and project&#039;s env vars (within the `.env` file) properly configured with an AWS admin user.

To ensure best practices, we must create a new AWS user restricted to creating and deleting only resources related to AWS SageMaker. Create it by running:
```bash
poetry poe create-sagemaker-role
```
It will create a `sagemaker_user_credentials.json` file at the root of your repository with your new `AWS_ACCESS_KEY` and `AWS_SECRET_KEY` values. **But before replacing your new AWS credentials, also run the following command to create the execution role (to create it using your admin credentials).**

To create the IAM execution role used by AWS SageMaker to access other AWS resources on our behalf, run the following:
```bash
poetry poe create-sagemaker-execution-role
```
It will create a `sagemaker_execution_role.json` file at the root of your repository with your new `AWS_ARN_ROLE` value. Add it to your `.env` file. 

Once you&#039;ve updated the `AWS_ACCESS_KEY`, `AWS_SECRET_KEY`, and `AWS_ARN_ROLE` values in your `.env` file, you can use AWS SageMaker. **Note that this step is crucial to complete the AWS setup.**

#### Training

We start the training pipeline through ZenML by running the following:
```bash
poetry poe run-training-pipeline
```
This will start the training code using the configs from `configs/training.yaml` directly in SageMaker. You can visualize the results in Comet ML&#039;s dashboard.

We start the evaluation pipeline through ZenML by running the following:
```bash
poetry poe run-evaluation-pipeline
```
This will start the evaluation code using the configs from `configs/evaluating.yaml` directly in SageMaker. You can visualize the results in `*-results` datasets saved to your Hugging Face profile.

#### Inference

To create an AWS SageMaker Inference Endpoint, run:
```bash
poetry poe deploy-inference-endpoint
```
To test it out, run:
```bash
poetry poe test-sagemaker-endpoint
```
To delete it, run:
```bash
poetry poe delete-inference-endpoint
```

#### AWS: ML pipelines, artifacts, and containers

The ML pipelines, artifacts, and containers are deployed to AWS by leveraging ZenML&#039;s deployment features. Thus, you must create an account with ZenML Cloud and follow their guide on deploying a ZenML stack to AWS. Otherwise, we provide step-by-step instructions in **Chapter 11**, section **Deploying the LLM Twin&#039;s pipelines to the cloud** on what you must do.  

#### Qdrant &amp; MongoDB

We leverage Qdrant&#039;s and MongoDB&#039;s serverless options when deploying the project. Thus, you can either follow [Qdrant&#039;s](https://qdrant.tech/documentation/cloud/create-cluster/) and [MongoDB&#039;s](https://www.mongodb.com/resources/products/fundamentals/mongodb-cluster-setup) tutorials on how to create a freemium cluster for each or go through **Chapter 11**, section **Deploying the LLM Twin&#039;s pipelines to the cloud** and follow our step-by-step instructions.

#### GitHub Actions

We use GitHub Actions to implement our CI/CD pipelines. To implement your own, you have to fork our repository and set the following env vars as Actions secrets in your forked repository:
- `AWS_ACCESS_KEY_ID`
- `AWS_SECRET_ACCESS_KEY`
- `AWS_ECR_NAME`
- `AWS_REGION`

Also, we provide instructions on how to set everything up in **Chapter 11**, section **Adding LLMOps to the LLM Twin**.

#### Comet ML &amp; Opik

You can visualize the results on their self-hosted dashboards if you create a Comet account and correctly set the `COMET_API_KEY` env var. As Opik is powered by Comet, you don&#039;t have to set up anything else along Comet:
- [Comet ML (for experiment tracking)](https://www.comet.com/?utm_source=llm_handbook&amp;utm_medium=github&amp;utm_campaign=opik)
- [Opik (for prompt monitoring)](https://www.comet.com/opik?utm_source=llm_handbook&amp;utm_medium=github&amp;utm_campaign=opik)

### üí∞ Running the Project Costs

We will mostly stick to free tiers for all the services except for AWS and OpenAI&#039;s API, which are both pay-as-you-go services. The cost of running the project once, with our default values, will be roughly ~$25 (most of it comes from using AWS SageMaker for training and inference).

## ‚ö° Pipelines

All the ML pipelines will be orchestrated behind the scenes by [ZenML](https://www.zenml.io/). A few exceptions exist when running utility scrips, such as exporting or importing from the data warehouse.

The ZenML pipelines are the entry point for most processe

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/magentic-ui]]></title>
            <link>https://github.com/microsoft/magentic-ui</link>
            <guid>https://github.com/microsoft/magentic-ui</guid>
            <pubDate>Fri, 15 Aug 2025 00:04:41 GMT</pubDate>
            <description><![CDATA[A research prototype of a human-centered web agent]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/magentic-ui">microsoft/magentic-ui</a></h1>
            <p>A research prototype of a human-centered web agent</p>
            <p>Language: Python</p>
            <p>Stars: 7,065</p>
            <p>Forks: 729</p>
            <p>Stars today: 47 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;docs/img/magui-readme-logo.svg&quot; alt=&quot;Magentic-UI Logo&quot;&gt;


_Automate your web tasks while you stay in control_

[![image](https://img.shields.io/pypi/v/magentic_ui.svg)](https://pypi.python.org/pypi/magentic_ui)
[![image](https://img.shields.io/pypi/l/magentic_ui.svg)](https://pypi.python.org/pypi/magentic_ui)
![Python Versions](https://img.shields.io/badge/python-3.10%20%7C%203.11%20%7C%203.12%20%7C%203.13-blue)

&lt;/div&gt;

---

Magentic-UI is a **research prototype** of a human-centered interface powered by a multi-agent system that can browse and perform actions on the web, generate and execute code, and generate and analyze files.

  https://github.com/user-attachments/assets/7975fc26-1a18-4acb-8bf9-321171eeade7

## üöÄ Quick Start

Here&#039;s how you can get started with Magentic-UI:

```bash
# 1. Setup environment
python3 -m venv .venv
source .venv/bin/activate
pip install magentic-ui --upgrade

# 2. Set your API key
export OPENAI_API_KEY=&quot;your-api-key-here&quot;

# 3. Launch Magentic-UI
magentic-ui --port 8081
```

Then open &lt;http://localhost:8081&gt; in your browser to interact with Magentic-UI!

&gt; **Prerequisites**: Requires Docker and Python 3.10+. Windows users should use WSL2. See [detailed installation](#Ô∏è-installation) for more info.

## ‚ú® What&#039;s New

- **File Upload Support**: Upload any file through the UI for analysis or modification
- **MCP Agents**: Extend capabilities with your favorite MCP servers
- **Easier Installation**: We have uploaded our docker containers to GHCR so you no longer need to build any containers! Installation time now is much quicker.

## Alternative Usage Options

**Without Docker** (limited functionality: no code execution):
```bash
magentic-ui --run-without-docker --port 8081
```

**Command Line Interface**:
```bash
magentic-cli --work-dir PATH/TO/STORE/DATA
```

**Custom LLM Clients**:
```bash
# Azure
pip install magentic-ui[azure]

# Ollama (local models)
pip install magentic-ui[ollama]
```

You can then pass a config file to the `magentic-ui` command (&lt;a href=&quot;#model-client-configuration&quot;&gt; client config&lt;/a&gt;) or change the model client inside the UI settings.

For further details on installation please read the   &lt;a href=&quot;#Ô∏è-installation&quot;&gt;üõ†Ô∏è Installation&lt;/a&gt; section. For common installation issues and their solutions, please refer to the [troubleshooting document](TROUBLESHOOTING.md). See advanced usage instructions with the command `magentic-ui --help`. 


## Quick Navigation:
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;#-how-it-works&quot;&gt;üü™ How it Works&lt;/a&gt; &amp;nbsp;|&amp;nbsp;
  &lt;a href=&quot;#Ô∏è-installation&quot;&gt;üõ†Ô∏è Installation&lt;/a&gt; &amp;nbsp;|&amp;nbsp;
  &lt;a href=&quot;#troubleshooting&quot;&gt;‚ö†Ô∏è Troubleshooting&lt;/a&gt; &amp;nbsp;|&amp;nbsp; 
  &lt;a href=&quot;#contributing&quot;&gt;ü§ù Contributing&lt;/a&gt; &amp;nbsp;|&amp;nbsp;
  &lt;a href=&quot;#license&quot;&gt;üìÑ License&lt;/a&gt;
&lt;/p&gt;

---

## üü™ How it Works
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./docs/img/magenticui_running.png&quot; alt=&quot;Magentic-UI&quot; height=&quot;400&quot;&gt;
&lt;/p&gt;

Magentic-UI is especially useful for web tasks that require actions on the web (e.g., filling a form, customizing a food order), deep navigation through websites not indexed by search engines (e.g., filtering flights, finding a link from a personal site) or tasks that need web navigation and code execution (e.g., generate a chart from online data).

The interface of Magentic-UI is displayed in the screenshot above and consists of two panels. The left side panel is the sessions navigator where users can create new sessions to solve new tasks, switch between sessions and check on session progress with the session status indicators (üî¥ needs input, ‚úÖ task done, ‚Ü∫ task in progress).

The right-side panel displays the session selected. This is where you can type your query to Magentic-UI alongside any file attachments and observe detailed task progress as well as  interact with the agents. The session display itself is split in two panels: the left side is where Magentic-UI presents the plan, task progress and asks for action approvals, the right side is a browser view where you can see web agent actions in real time and interact with the browser. Finally, at the top of the session display is a progress bar that updates as Magentic-UI makes progress.


The example below shows a step by step user interaction with Magentic-UI:

&lt;!-- Screenshots --&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/img/magui-landing.png&quot; alt=&quot;Magentic-UI Landing&quot; width=&quot;45%&quot; style=&quot;margin:10px;&quot;&gt;
  &lt;img src=&quot;docs/img/magui-coplanning.png&quot; alt=&quot;Co-Planning UI&quot; width=&quot;45%&quot; style=&quot;margin:10px;&quot;&gt;
  &lt;img src=&quot;docs/img/magui-cotasking.png&quot; alt=&quot;Co-Tasking UI&quot; width=&quot;45%&quot; style=&quot;margin:10px;&quot;&gt;
  &lt;img src=&quot;docs/img/magui-actionguard.png&quot; alt=&quot;Action Guard UI&quot; width=&quot;45%&quot; style=&quot;margin:10px;&quot;&gt;
&lt;/p&gt;


What differentiates Magentic-UI from other browser use offerings is its transparent and controllable interface that allows for efficient human-in-the-loop involvement. Magentic-UI is built using [AutoGen](https://github.com/microsoft/autogen) and provides a platform to study human-agent interaction and experiment with web agents. Key features include:

- üßë‚Äçü§ù‚Äçüßë **Co-Planning**: Collaboratively create and approve step-by-step plans using chat and the plan editor.
- ü§ù **Co-Tasking**: Interrupt and guide the task execution using the web browser directly or through chat. Magentic-UI can also ask for clarifications and help when needed.
- üõ°Ô∏è **Action Guards**: Sensitive actions are only executed with explicit user approvals.
- üß† **Plan Learning and Retrieval**: Learn from previous runs to improve future task automation and save them in a plan gallery. Automatically or manually retrieve saved plans in future tasks.
- üîÄ **Parallel Task Execution**: You can run multiple tasks in parallel and session status indicators will let you know when Magentic-UI needs your input or has completed the task.

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=wOs-5SR8xOc&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.youtube.com/vi/wOs-5SR8xOc/maxresdefault.jpg&quot; alt=&quot;Watch the demo video&quot; width=&quot;600&quot;/&gt;
  &lt;/a&gt;
  &lt;br&gt;
  ‚ñ∂Ô∏è &lt;em&gt; Click to watch a video and learn more about Magentic-UI &lt;/em&gt;
&lt;/div&gt;


### Autonomous Evaluation

To evaluate its autonomous capabilities, Magentic-UI has been tested against several benchmarks when running with o4-mini: [GAIA](https://huggingface.co/datasets/gaia-benchmark/GAIA) test set (42.52%), which assesses general AI assistants across reasoning, tool use, and web interaction tasks ; [AssistantBench](https://huggingface.co/AssistantBench) test set (27.60%), focusing on realistic, time-consuming web tasks; [WebVoyager](https://github.com/MinorJerry/WebVoyager) (82.2%), measuring end-to-end web navigation in real-world scenarios; and [WebGames](https://webgames.convergence.ai/) (45.5%), evaluating general-purpose web-browsing agents through interactive challenges.
To reproduce these experimental results, please see the following [instructions](experiments/eval/README.md).



If you&#039;re interested in reading more checkout our [technical report](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/magentic-ui-report.pdf) and [blog post](https://www.microsoft.com/en-us/research/blog/magentic-ui-an-experimental-human-centered-web-agent/).

## üõ†Ô∏è Installation
### Pre-Requisites

**Note**: If you&#039;re using Windows, we highly recommend using [WSL2](https://docs.microsoft.com/en-us/windows/wsl/install) (Windows Subsystem for Linux).

1. If running on **Windows** or **Mac** you should use [Docker Desktop](https://www.docker.com/products/docker-desktop/) or if inside WSL2 you can install Docker directly inside WSL [docker in WSL2 guide](https://gist.github.com/dehsilvadeveloper/c3bdf0f4cdcc5c177e2fe9be671820c7). If running on **Linux**, you should use [Docker Engine](https://docs.docker.com/engine/install/). 

If using Docker Desktop, make sure it is set up to use WSL2:
    - Go to Settings &gt; Resources &gt; WSL Integration
    - Enable integration with your development distro You can find more detailed instructions about this step [here](https://docs.microsoft.com/en-us/windows/wsl/tutorials/wsl-containers).



2. During the Installation step, you will need to set up your `OPENAI_API_KEY`. To use other models, review the [Model Client Configuration](#model-client-configuration) section below.

3. You need at least [Python 3.10](https://www.python.org/downloads/) installed.


If you are on Windows, we recommend to run Magentic-UI inside [WSL2](https://docs.microsoft.com/en-us/windows/wsl/install) (Windows Subsystem for Linux) for correct Docker and file path compatibility.



### PyPI Installation

Magentic-UI is available on PyPI. We recommend using a virtual environment to avoid conflicts with other packages.

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install magentic-ui
```

Alternatively, if you use [`uv`](https://docs.astral.sh/uv/getting-started/installation/) for dependency management, you can install Magentic-UI with:

```bash
uv venv --python=3.12 .venv
. .venv/bin/activate
uv pip install magentic-ui
```


### Running Magentic-UI

To run Magentic-UI, make sure that Docker is running, then run the following command:

```bash
magentic-ui --port 8081
```

&gt;**Note**: Running this command for the first time will pull two docker images required for the Magentic-UI agents. If you encounter problems, you can build them directly with the following command:
```bash
cd docker
sh build-all.sh
```

If you face issues with Docker, please refer to the [TROUBLESHOOTING.md](TROUBLESHOOTING.md) document.

Once the server is running, you can access the UI at &lt;http://localhost:8081&gt;.


### Configuration

#### Model Client Configuration

If you want to use a different OpenAI key, or if you want to configure use with Azure OpenAI or Ollama, you can do so inside the UI by navigating to settings (top right icon) and changing model configuration. Another option is to pass a yaml config file when you start Magentic-UI which will override any settings in the UI:

```bash
magentic-ui --port 8081 --config config.yaml
```

Where the `config.yaml` should look as follows with an AutoGen model client configuration:

```yaml
gpt4o_client: &amp;gpt4o_client
    provider: OpenAIChatCompletionClient
    config:
      model: gpt-4o-2024-08-06
      api_key: null
      base_url: null
      max_retries: 5

orchestrator_client: *gpt4o_client
coder_client: *gpt4o_client
web_surfer_client: *gpt4o_client
file_surfer_client: *gpt4o_client
action_guard_client: *gpt4o_client
plan_learning_client: *gpt4o_client
```
You can change the client for each of the agents using the config file and use AzureOpenAI (`AzureOpenAIChatCompletionClient`), Ollama and other clients.

#### MCP Server Configuration

You can also extend Magentic-UI&#039;s capabilities by adding custom &quot;McpAgents&quot; to the multi-agent team. Each McpAgent can have access to one or more MCP Servers. You can specify these agents via the `mcp_agent_configs` parameter in your `config.yaml`.

For example, here&#039;s an agent called &quot;airbnb_surfer&quot; that has access to the OpenBnb MCP Server running locally via Stdio.

```yaml
mcp_agent_configs:
  - name: airbnb_surfer
    description: &quot;The airbnb_surfer has direct access to AirBnB.&quot;
    model_client: 
      provider: OpenAIChatCompletionClient
      config:
        model: gpt-4.1-2025-04-14
      max_retries: 10
    system_message: |-
      You are AirBnb Surfer, a helpful digital assistant that can help users acces AirBnB.

      You have access to a suite of tools provided by the AirBnB API. Use those tools to satisfy the users requests.
    reflect_on_tool_use: false
    mcp_servers:
      - server_name: AirBnB
        server_params:
          type: StdioServerParams
          command: npx
          args:
            - -y
            - &quot;@openbnb/mcp-server-airbnb&quot;
            - --ignore-robots-txt
```

Under the hood, each `McpAgent` is just a `autogen_agentchat.agents.AssistantAgent` with the set of MCP Servers exposed as an `AggregateMcpWorkbench` which is simply a named collection of `autogen_ext.tools.mcp.McpWorkbench` objects (one per MCP Server).

Currently the supported MCP Server types are `autogen_ext.tools.mcp.StdioServerParams` and `autogen_ext.tools.mcp.SseServerParams`.

### Building Magentic-UI from source

This step is primarily for users seeking to make modifications to the code, are having trouble with the pypi installation or want the latest code before a pypi version release.

#### 1. Make sure the above prerequisites are installed, and that Docker is running.

#### 2. Clone the repository to your local machine:

```bash
git clone https://github.com/microsoft/magentic-ui.git
cd magentic-ui
```

#### 3. Install Magentic-UI&#039;s dependencies with uv or your favorite package manager:

```bash
# install uv through https://docs.astral.sh/uv/getting-started/installation/
uv venv --python=3.12 .venv
uv sync --all-extras
source .venv/bin/activate
```

#### 4. Build the frontend:

First make sure to install node:

```bash
# install nvm to install node
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.1/install.sh | bash
nvm install node
```

Then install the frontend:

```bash
cd frontend
npm install -g gatsby-cli
npm install --global yarn
yarn install
yarn build
```

#### 5. Run Magentic-UI, as usual.

```bash
magentic-ui --port 8081
```


#### Running the UI from source

If you are making changes to the source code of the UI, you can run the frontend in development mode so that it will automatically update when you make changes for faster development.

1. Open a separate terminal and change directory to the frontend

```bash
cd frontend
```

2. Create a `.env.development` file.

```bash
cp .env.default .env.development
```

3. Launch frontend server

```bash
npm run start
```

4. Then run the UI:

```bash
magentic-ui --port 8081
```

The frontend from source will be available at &lt;http://localhost:8000&gt;, and the compiled frontend will be available at &lt;http://localhost:8081&gt;.




## Troubleshooting


If you were unable to get Magentic-UI running, do not worry! The first step is to make sure you have followed the steps outlined above, particularly with the [pre-requisites](#pre-requisites).

For common issues and their solutions, please refer to the [TROUBLESHOOTING.md](TROUBLESHOOTING.md) file in this repository. If you do not see your problem there, please open a `GitHub Issue`. 

## Contributing

This project welcomes contributions and suggestions. For information about contributing to Magentic-UI, please see our [CONTRIBUTING.md](CONTRIBUTING.md) guide, which includes current issues to be resolved and other forms of contributing.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information, see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## License

Microsoft, and any contributors, grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT). See the [LICENSE](LICENSE) file.

Microsoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation
may be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.
The licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.
Microsoft&#039;s general trademark guidelines can be found at &lt;http://go.microsoft.com/fwlink/?LinkID=254653&gt;.

Any use of third-party trademarks or logos are subject to those third-party&#039;s policies.

Privacy information can be found at &lt;https://go.microsoft.com/fwlink/?LinkId=521839&gt;

Microsoft and any contributors reserve all other rights, whether under their respective copyrights, patents, or trademarks, whether by implication, estoppel, or otherwise.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[dagster-io/dagster]]></title>
            <link>https://github.com/dagster-io/dagster</link>
            <guid>https://github.com/dagster-io/dagster</guid>
            <pubDate>Fri, 15 Aug 2025 00:04:40 GMT</pubDate>
            <description><![CDATA[An orchestration platform for the development, production, and observation of data assets.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/dagster-io/dagster">dagster-io/dagster</a></h1>
            <p>An orchestration platform for the development, production, and observation of data assets.</p>
            <p>Language: Python</p>
            <p>Stars: 13,785</p>
            <p>Forks: 1,783</p>
            <p>Stars today: 14 stars today</p>
            <h2>README</h2><pre>python_modules/dagster/README.md</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[tadata-org/fastapi_mcp]]></title>
            <link>https://github.com/tadata-org/fastapi_mcp</link>
            <guid>https://github.com/tadata-org/fastapi_mcp</guid>
            <pubDate>Fri, 15 Aug 2025 00:04:39 GMT</pubDate>
            <description><![CDATA[Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/tadata-org/fastapi_mcp">tadata-org/fastapi_mcp</a></h1>
            <p>Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!</p>
            <p>Language: Python</p>
            <p>Stars: 8,428</p>
            <p>Forks: 664</p>
            <p>Stars today: 423 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/tadata-org/fastapi_mcp&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/7e44e98b-a0ba-4aff-a68a-4ffee3a6189c&quot; alt=&quot;fastapi-to-mcp&quot; height=100/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;span style=&quot;font-size: 0.85em; font-weight: normal;&quot;&gt;Built by &lt;a href=&quot;https://tadata.com&quot;&gt;Tadata&lt;/a&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;h1 align=&quot;center&quot;&gt;
  FastAPI-MCP
&lt;/h1&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/14064&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14064&quot; alt=&quot;tadata-org%2Ffastapi_mcp | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;

[![PyPI version](https://img.shields.io/pypi/v/fastapi-mcp?color=%2334D058&amp;label=pypi%20package)](https://pypi.org/project/fastapi-mcp/)
[![Python Versions](https://img.shields.io/pypi/pyversions/fastapi-mcp.svg)](https://pypi.org/project/fastapi-mcp/)
[![FastAPI](https://img.shields.io/badge/FastAPI-009485.svg?logo=fastapi&amp;logoColor=white)](#)
[![CI](https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml/badge.svg)](https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml)
[![Coverage](https://codecov.io/gh/tadata-org/fastapi_mcp/branch/main/graph/badge.svg)](https://codecov.io/gh/tadata-org/fastapi_mcp)

&lt;/div&gt;


&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/tadata-org/fastapi_mcp&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/b205adc6-28c0-4e3c-a68b-9c1a80eb7d0c&quot; alt=&quot;fastapi-mcp-usage&quot; height=&quot;400&quot;/&gt;&lt;/a&gt;&lt;/p&gt;


## Features

- **Authentication** built in, using your existing FastAPI dependencies!

- **FastAPI-native:** Not just another OpenAPI -&gt; MCP converter

- **Zero/Minimal configuration** required - just point it at your FastAPI app and it works

- **Preserving schemas** of your request models and response models

- **Preserve documentation** of all your endpoints, just as it is in Swagger

- **Flexible deployment** - Mount your MCP server to the same app, or deploy separately

- **ASGI transport** - Uses FastAPI&#039;s ASGI interface directly for efficient communication


## Hosted Solution

If you prefer a managed hosted solution check out [tadata.com](https://tadata.com).

## Installation

We recommend using [uv](https://docs.astral.sh/uv/), a fast Python package installer:

```bash
uv add fastapi-mcp
```

Alternatively, you can install with pip:

```bash
pip install fastapi-mcp
```

## Basic Usage

The simplest way to use FastAPI-MCP is to add an MCP server directly to your FastAPI application:

```python
from fastapi import FastAPI
from fastapi_mcp import FastApiMCP

app = FastAPI()

mcp = FastApiMCP(app)

# Mount the MCP server directly to your FastAPI app
mcp.mount()
```

That&#039;s it! Your auto-generated MCP server is now available at `https://app.base.url/mcp`.

## Documentation, Examples and Advanced Usage

FastAPI-MCP provides [comprehensive documentation](https://fastapi-mcp.tadata.com/). Additionaly, check out the [examples directory](examples) for code samples demonstrating these features in action.

## FastAPI-first Approach

FastAPI-MCP is designed as a native extension of FastAPI, not just a converter that generates MCP tools from your API. This approach offers several key advantages:

- **Native dependencies**: Secure your MCP endpoints using familiar FastAPI `Depends()` for authentication and authorization

- **ASGI transport**: Communicates directly with your FastAPI app using its ASGI interface, eliminating the need for HTTP calls from the MCP to your API

- **Unified infrastructure**: Your FastAPI app doesn&#039;t need to run separately from the MCP server (though [separate deployment](https://fastapi-mcp.tadata.com/advanced/deploy#deploying-separately-from-original-fastapi-app) is also supported)

This design philosophy ensures minimum friction when adding MCP capabilities to your existing FastAPI services.


## Development and Contributing

Thank you for considering contributing to FastAPI-MCP! We encourage the community to post Issues and create Pull Requests.

Before you get started, please see our [Contribution Guide](CONTRIBUTING.md).

## Community

Join [MCParty Slack community](https://join.slack.com/t/themcparty/shared_invite/zt-30yxr1zdi-2FG~XjBA0xIgYSYuKe7~Xg) to connect with other MCP enthusiasts, ask questions, and share your experiences with FastAPI-MCP.

## Requirements

- Python 3.10+ (Recommended 3.12)
- uv

## License

MIT License. Copyright (c) 2025 Tadata Inc.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[modelcontextprotocol/python-sdk]]></title>
            <link>https://github.com/modelcontextprotocol/python-sdk</link>
            <guid>https://github.com/modelcontextprotocol/python-sdk</guid>
            <pubDate>Fri, 15 Aug 2025 00:04:38 GMT</pubDate>
            <description><![CDATA[The official Python SDK for Model Context Protocol servers and clients]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/modelcontextprotocol/python-sdk">modelcontextprotocol/python-sdk</a></h1>
            <p>The official Python SDK for Model Context Protocol servers and clients</p>
            <p>Language: Python</p>
            <p>Stars: 17,488</p>
            <p>Forks: 2,305</p>
            <p>Stars today: 47 stars today</p>
            <h2>README</h2><pre># MCP Python SDK

&lt;div align=&quot;center&quot;&gt;

&lt;strong&gt;Python implementation of the Model Context Protocol (MCP)&lt;/strong&gt;

[![PyPI][pypi-badge]][pypi-url]
[![MIT licensed][mit-badge]][mit-url]
[![Python Version][python-badge]][python-url]
[![Documentation][docs-badge]][docs-url]
[![Specification][spec-badge]][spec-url]
[![GitHub Discussions][discussions-badge]][discussions-url]

&lt;/div&gt;

&lt;!-- omit in toc --&gt;
## Table of Contents

- [MCP Python SDK](#mcp-python-sdk)
  - [Overview](#overview)
  - [Installation](#installation)
    - [Adding MCP to your python project](#adding-mcp-to-your-python-project)
    - [Running the standalone MCP development tools](#running-the-standalone-mcp-development-tools)
  - [Quickstart](#quickstart)
  - [What is MCP?](#what-is-mcp)
  - [Core Concepts](#core-concepts)
    - [Server](#server)
    - [Resources](#resources)
    - [Tools](#tools)
      - [Structured Output](#structured-output)
    - [Prompts](#prompts)
    - [Images](#images)
    - [Context](#context)
    - [Completions](#completions)
    - [Elicitation](#elicitation)
    - [Sampling](#sampling)
    - [Logging and Notifications](#logging-and-notifications)
    - [Authentication](#authentication)
    - [FastMCP Properties](#fastmcp-properties)
    - [Session Properties](#session-properties-and-methods)
    - [Request Context Properties](#request-context-properties)
  - [Running Your Server](#running-your-server)
    - [Development Mode](#development-mode)
    - [Claude Desktop Integration](#claude-desktop-integration)
    - [Direct Execution](#direct-execution)
    - [Streamable HTTP Transport](#streamable-http-transport)
    - [Mounting to an Existing ASGI Server](#mounting-to-an-existing-asgi-server)
  - [Advanced Usage](#advanced-usage)
    - [Low-Level Server](#low-level-server)
    - [Writing MCP Clients](#writing-mcp-clients)
    - [Client Display Utilities](#client-display-utilities)
    - [OAuth Authentication for Clients](#oauth-authentication-for-clients)
    - [Parsing Tool Results](#parsing-tool-results)
    - [MCP Primitives](#mcp-primitives)
    - [Server Capabilities](#server-capabilities)
  - [Documentation](#documentation)
  - [Contributing](#contributing)
  - [License](#license)

[pypi-badge]: https://img.shields.io/pypi/v/mcp.svg
[pypi-url]: https://pypi.org/project/mcp/
[mit-badge]: https://img.shields.io/pypi/l/mcp.svg
[mit-url]: https://github.com/modelcontextprotocol/python-sdk/blob/main/LICENSE
[python-badge]: https://img.shields.io/pypi/pyversions/mcp.svg
[python-url]: https://www.python.org/downloads/
[docs-badge]: https://img.shields.io/badge/docs-modelcontextprotocol.io-blue.svg
[docs-url]: https://modelcontextprotocol.io
[spec-badge]: https://img.shields.io/badge/spec-spec.modelcontextprotocol.io-blue.svg
[spec-url]: https://spec.modelcontextprotocol.io
[discussions-badge]: https://img.shields.io/github/discussions/modelcontextprotocol/python-sdk
[discussions-url]: https://github.com/modelcontextprotocol/python-sdk/discussions

## Overview

The Model Context Protocol allows applications to provide context for LLMs in a standardized way, separating the concerns of providing context from the actual LLM interaction. This Python SDK implements the full MCP specification, making it easy to:

- Build MCP clients that can connect to any MCP server
- Create MCP servers that expose resources, prompts and tools
- Use standard transports like stdio, SSE, and Streamable HTTP
- Handle all MCP protocol messages and lifecycle events

## Installation

### Adding MCP to your python project

We recommend using [uv](https://docs.astral.sh/uv/) to manage your Python projects.

If you haven&#039;t created a uv-managed project yet, create one:

   ```bash
   uv init mcp-server-demo
   cd mcp-server-demo
   ```

   Then add MCP to your project dependencies:

   ```bash
   uv add &quot;mcp[cli]&quot;
   ```

Alternatively, for projects using pip for dependencies:

```bash
pip install &quot;mcp[cli]&quot;
```

### Running the standalone MCP development tools

To run the mcp command with uv:

```bash
uv run mcp
```

## Quickstart

Let&#039;s create a simple MCP server that exposes a calculator tool and some data:

&lt;!-- snippet-source examples/snippets/servers/fastmcp_quickstart.py --&gt;
```python
&quot;&quot;&quot;
FastMCP quickstart example.

cd to the `examples/snippets/clients` directory and run:
    uv run server fastmcp_quickstart stdio
&quot;&quot;&quot;

from mcp.server.fastmcp import FastMCP

# Create an MCP server
mcp = FastMCP(&quot;Demo&quot;)


# Add an addition tool
@mcp.tool()
def add(a: int, b: int) -&gt; int:
    &quot;&quot;&quot;Add two numbers&quot;&quot;&quot;
    return a + b


# Add a dynamic greeting resource
@mcp.resource(&quot;greeting://{name}&quot;)
def get_greeting(name: str) -&gt; str:
    &quot;&quot;&quot;Get a personalized greeting&quot;&quot;&quot;
    return f&quot;Hello, {name}!&quot;


# Add a prompt
@mcp.prompt()
def greet_user(name: str, style: str = &quot;friendly&quot;) -&gt; str:
    &quot;&quot;&quot;Generate a greeting prompt&quot;&quot;&quot;
    styles = {
        &quot;friendly&quot;: &quot;Please write a warm, friendly greeting&quot;,
        &quot;formal&quot;: &quot;Please write a formal, professional greeting&quot;,
        &quot;casual&quot;: &quot;Please write a casual, relaxed greeting&quot;,
    }

    return f&quot;{styles.get(style, styles[&#039;friendly&#039;])} for someone named {name}.&quot;
```

_Full example: [examples/snippets/servers/fastmcp_quickstart.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/fastmcp_quickstart.py)_
&lt;!-- /snippet-source --&gt;

You can install this server in [Claude Desktop](https://claude.ai/download) and interact with it right away by running:

```bash
uv run mcp install server.py
```

Alternatively, you can test it with the MCP Inspector:

```bash
uv run mcp dev server.py
```

## What is MCP?

The [Model Context Protocol (MCP)](https://modelcontextprotocol.io) lets you build servers that expose data and functionality to LLM applications in a secure, standardized way. Think of it like a web API, but specifically designed for LLM interactions. MCP servers can:

- Expose data through **Resources** (think of these sort of like GET endpoints; they are used to load information into the LLM&#039;s context)
- Provide functionality through **Tools** (sort of like POST endpoints; they are used to execute code or otherwise produce a side effect)
- Define interaction patterns through **Prompts** (reusable templates for LLM interactions)
- And more!

## Core Concepts

### Server

The FastMCP server is your core interface to the MCP protocol. It handles connection management, protocol compliance, and message routing:

&lt;!-- snippet-source examples/snippets/servers/lifespan_example.py --&gt;
```python
&quot;&quot;&quot;Example showing lifespan support for startup/shutdown with strong typing.&quot;&quot;&quot;

from collections.abc import AsyncIterator
from contextlib import asynccontextmanager
from dataclasses import dataclass

from mcp.server.fastmcp import Context, FastMCP
from mcp.server.session import ServerSession


# Mock database class for example
class Database:
    &quot;&quot;&quot;Mock database class for example.&quot;&quot;&quot;

    @classmethod
    async def connect(cls) -&gt; &quot;Database&quot;:
        &quot;&quot;&quot;Connect to database.&quot;&quot;&quot;
        return cls()

    async def disconnect(self) -&gt; None:
        &quot;&quot;&quot;Disconnect from database.&quot;&quot;&quot;
        pass

    def query(self) -&gt; str:
        &quot;&quot;&quot;Execute a query.&quot;&quot;&quot;
        return &quot;Query result&quot;


@dataclass
class AppContext:
    &quot;&quot;&quot;Application context with typed dependencies.&quot;&quot;&quot;

    db: Database


@asynccontextmanager
async def app_lifespan(server: FastMCP) -&gt; AsyncIterator[AppContext]:
    &quot;&quot;&quot;Manage application lifecycle with type-safe context.&quot;&quot;&quot;
    # Initialize on startup
    db = await Database.connect()
    try:
        yield AppContext(db=db)
    finally:
        # Cleanup on shutdown
        await db.disconnect()


# Pass lifespan to server
mcp = FastMCP(&quot;My App&quot;, lifespan=app_lifespan)


# Access type-safe lifespan context in tools
@mcp.tool()
def query_db(ctx: Context[ServerSession, AppContext]) -&gt; str:
    &quot;&quot;&quot;Tool that uses initialized resources.&quot;&quot;&quot;
    db = ctx.request_context.lifespan_context.db
    return db.query()
```

_Full example: [examples/snippets/servers/lifespan_example.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/lifespan_example.py)_
&lt;!-- /snippet-source --&gt;

### Resources

Resources are how you expose data to LLMs. They&#039;re similar to GET endpoints in a REST API - they provide data but shouldn&#039;t perform significant computation or have side effects:

&lt;!-- snippet-source examples/snippets/servers/basic_resource.py --&gt;
```python
from mcp.server.fastmcp import FastMCP

mcp = FastMCP(name=&quot;Resource Example&quot;)


@mcp.resource(&quot;file://documents/{name}&quot;)
def read_document(name: str) -&gt; str:
    &quot;&quot;&quot;Read a document by name.&quot;&quot;&quot;
    # This would normally read from disk
    return f&quot;Content of {name}&quot;


@mcp.resource(&quot;config://settings&quot;)
def get_settings() -&gt; str:
    &quot;&quot;&quot;Get application settings.&quot;&quot;&quot;
    return &quot;&quot;&quot;{
  &quot;theme&quot;: &quot;dark&quot;,
  &quot;language&quot;: &quot;en&quot;,
  &quot;debug&quot;: false
}&quot;&quot;&quot;
```

_Full example: [examples/snippets/servers/basic_resource.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/basic_resource.py)_
&lt;!-- /snippet-source --&gt;

### Tools

Tools let LLMs take actions through your server. Unlike resources, tools are expected to perform computation and have side effects:

&lt;!-- snippet-source examples/snippets/servers/basic_tool.py --&gt;
```python
from mcp.server.fastmcp import FastMCP

mcp = FastMCP(name=&quot;Tool Example&quot;)


@mcp.tool()
def sum(a: int, b: int) -&gt; int:
    &quot;&quot;&quot;Add two numbers together.&quot;&quot;&quot;
    return a + b


@mcp.tool()
def get_weather(city: str, unit: str = &quot;celsius&quot;) -&gt; str:
    &quot;&quot;&quot;Get weather for a city.&quot;&quot;&quot;
    # This would normally call a weather API
    return f&quot;Weather in {city}: 22degrees{unit[0].upper()}&quot;
```

_Full example: [examples/snippets/servers/basic_tool.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/basic_tool.py)_
&lt;!-- /snippet-source --&gt;

Tools can optionally receive a Context object by including a parameter with the `Context` type annotation. This context is automatically injected by the FastMCP framework and provides access to MCP capabilities:

&lt;!-- snippet-source examples/snippets/servers/tool_progress.py --&gt;
```python
from mcp.server.fastmcp import Context, FastMCP
from mcp.server.session import ServerSession

mcp = FastMCP(name=&quot;Progress Example&quot;)


@mcp.tool()
async def long_running_task(task_name: str, ctx: Context[ServerSession, None], steps: int = 5) -&gt; str:
    &quot;&quot;&quot;Execute a task with progress updates.&quot;&quot;&quot;
    await ctx.info(f&quot;Starting: {task_name}&quot;)

    for i in range(steps):
        progress = (i + 1) / steps
        await ctx.report_progress(
            progress=progress,
            total=1.0,
            message=f&quot;Step {i + 1}/{steps}&quot;,
        )
        await ctx.debug(f&quot;Completed step {i + 1}&quot;)

    return f&quot;Task &#039;{task_name}&#039; completed&quot;
```

_Full example: [examples/snippets/servers/tool_progress.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/tool_progress.py)_
&lt;!-- /snippet-source --&gt;

#### Structured Output

Tools will return structured results by default, if their return type
annotation is compatible. Otherwise, they will return unstructured results.

Structured output supports these return types:

- Pydantic models (BaseModel subclasses)
- TypedDicts
- Dataclasses and other classes with type hints
- `dict[str, T]` (where T is any JSON-serializable type)
- Primitive types (str, int, float, bool, bytes, None) - wrapped in `{&quot;result&quot;: value}`
- Generic types (list, tuple, Union, Optional, etc.) - wrapped in `{&quot;result&quot;: value}`

Classes without type hints cannot be serialized for structured output. Only
classes with properly annotated attributes will be converted to Pydantic models
for schema generation and validation.

Structured results are automatically validated against the output schema
generated from the annotation. This ensures the tool returns well-typed,
validated data that clients can easily process.

**Note:** For backward compatibility, unstructured results are also
returned. Unstructured results are provided for backward compatibility
with previous versions of the MCP specification, and are quirks-compatible
with previous versions of FastMCP in the current version of the SDK.

**Note:** In cases where a tool function&#039;s return type annotation
causes the tool to be classified as structured _and this is undesirable_,
the  classification can be suppressed by passing `structured_output=False`
to the `@tool` decorator.

&lt;!-- snippet-source examples/snippets/servers/structured_output.py --&gt;
```python
&quot;&quot;&quot;Example showing structured output with tools.&quot;&quot;&quot;

from typing import TypedDict

from pydantic import BaseModel, Field

from mcp.server.fastmcp import FastMCP

mcp = FastMCP(&quot;Structured Output Example&quot;)


# Using Pydantic models for rich structured data
class WeatherData(BaseModel):
    &quot;&quot;&quot;Weather information structure.&quot;&quot;&quot;

    temperature: float = Field(description=&quot;Temperature in Celsius&quot;)
    humidity: float = Field(description=&quot;Humidity percentage&quot;)
    condition: str
    wind_speed: float


@mcp.tool()
def get_weather(city: str) -&gt; WeatherData:
    &quot;&quot;&quot;Get weather for a city - returns structured data.&quot;&quot;&quot;
    # Simulated weather data
    return WeatherData(
        temperature=72.5,
        humidity=45.0,
        condition=&quot;sunny&quot;,
        wind_speed=5.2,
    )


# Using TypedDict for simpler structures
class LocationInfo(TypedDict):
    latitude: float
    longitude: float
    name: str


@mcp.tool()
def get_location(address: str) -&gt; LocationInfo:
    &quot;&quot;&quot;Get location coordinates&quot;&quot;&quot;
    return LocationInfo(latitude=51.5074, longitude=-0.1278, name=&quot;London, UK&quot;)


# Using dict[str, Any] for flexible schemas
@mcp.tool()
def get_statistics(data_type: str) -&gt; dict[str, float]:
    &quot;&quot;&quot;Get various statistics&quot;&quot;&quot;
    return {&quot;mean&quot;: 42.5, &quot;median&quot;: 40.0, &quot;std_dev&quot;: 5.2}


# Ordinary classes with type hints work for structured output
class UserProfile:
    name: str
    age: int
    email: str | None = None

    def __init__(self, name: str, age: int, email: str | None = None):
        self.name = name
        self.age = age
        self.email = email


@mcp.tool()
def get_user(user_id: str) -&gt; UserProfile:
    &quot;&quot;&quot;Get user profile - returns structured data&quot;&quot;&quot;
    return UserProfile(name=&quot;Alice&quot;, age=30, email=&quot;alice@example.com&quot;)


# Classes WITHOUT type hints cannot be used for structured output
class UntypedConfig:
    def __init__(self, setting1, setting2):  # type: ignore[reportMissingParameterType]
        self.setting1 = setting1
        self.setting2 = setting2


@mcp.tool()
def get_config() -&gt; UntypedConfig:
    &quot;&quot;&quot;This returns unstructured output - no schema generated&quot;&quot;&quot;
    return UntypedConfig(&quot;value1&quot;, &quot;value2&quot;)


# Lists and other types are wrapped automatically
@mcp.tool()
def list_cities() -&gt; list[str]:
    &quot;&quot;&quot;Get a list of cities&quot;&quot;&quot;
    return [&quot;London&quot;, &quot;Paris&quot;, &quot;Tokyo&quot;]
    # Returns: {&quot;result&quot;: [&quot;London&quot;, &quot;Paris&quot;, &quot;Tokyo&quot;]}


@mcp.tool()
def get_temperature(city: str) -&gt; float:
    &quot;&quot;&quot;Get temperature as a simple float&quot;&quot;&quot;
    return 22.5
    # Returns: {&quot;result&quot;: 22.5}
```

_Full example: [examples/snippets/servers/structured_output.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/structured_output.py)_
&lt;!-- /snippet-source --&gt;

### Prompts

Prompts are reusable templates that help LLMs interact with your server effectively:

&lt;!-- snippet-source examples/snippets/servers/basic_prompt.py --&gt;
```python
from mcp.server.fastmcp import FastMCP
from mcp.server.fastmcp.prompts import base

mcp = FastMCP(name=&quot;Prompt Example&quot;)


@mcp.prompt(title=&quot;Code Review&quot;)
def review_code(code: str) -&gt; str:
    return f&quot;Please review this code:\n\n{code}&quot;


@mcp.prompt(title=&quot;Debug Assistant&quot;)
def debug_error(error: str) -&gt; list[base.Message]:
    return [
        base.UserMessage(&quot;I&#039;m seeing this error:&quot;),
        base.UserMessage(error),
        base.AssistantMessage(&quot;I&#039;ll help debug that. What have you tried so far?&quot;),
    ]
```

_Full example: [examples/snippets/servers/basic_prompt.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/basic_prompt.py)_
&lt;!-- /snippet-source --&gt;

### Images

FastMCP provides an `Image` class that automatically handles image data:

&lt;!-- snippet-source examples/snippets/servers/images.py --&gt;
```python
&quot;&quot;&quot;Example showing image handling with FastMCP.&quot;&quot;&quot;

from PIL import Image as PILImage

from mcp.server.fastmcp import FastMCP, Image

mcp = FastMCP(&quot;Image Example&quot;)


@mcp.tool()
def create_thumbnail(image_path: str) -&gt; Image:
    &quot;&quot;&quot;Create a thumbnail from an image&quot;&quot;&quot;
    img = PILImage.open(image_path)
    img.thumbnail((100, 100))
    return Image(data=img.tobytes(), format=&quot;png&quot;)
```

_Full example: [examples/snippets/servers/images.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/images.py)_
&lt;!-- /snippet-source --&gt;

### Context

The Context object is automatically injected into tool and resource functions that request it via type hints. It provides access to MCP capabilities like logging, progress reporting, resource reading, user interaction, and request metadata.

#### Getting Context in Functions

To use context in a tool or resource function, add a parameter with the `Context` type annotation:

```python
from mcp.server.fastmcp import Context, FastMCP

mcp = FastMCP(name=&quot;Context Example&quot;)


@mcp.tool()
async def my_tool(x: int, ctx: Context) -&gt; str:
    &quot;&quot;&quot;Tool that uses context capabilities.&quot;&quot;&quot;
    # The context parameter can have any name as long as it&#039;s type-annotated
    return await process_with_context(x, ctx)
```

#### Context Properties and Methods

The Context object provides the following capabilities:

- `ctx.request_id` - Unique ID for the current request
- `ctx.client_id` - Client ID if available
- `ctx.fastmcp` - Access to the FastMCP server instance (see [FastMCP Properties](#fastmcp-properties))
- `ctx.session` - Access to the underlying session for advanced communication (see [Session Properties and Methods](#session-properties-and-methods))
- `ctx.request_context` - Access to request-specific data and lifespan resources (see [Request Context Properties](#request-context-properties))
- `await ctx.debug(message)` - Send debug log message
- `await ctx.info(message)` - Send info log message  
- `await ctx.warning(message)` - Send warning log message
- `await ctx.error(message)` - Send error log message
- `await ctx.log(level, message, logger_name=None)` - Send log with custom level
- `await ctx.report_progress(progress, total=None, message=None)` - Report operation progress
- `await ctx.read_resource(uri)` - Read a resource by URI
- `await ctx.elicit(message, schema)` - Request additional information from user with validation

&lt;!-- snippet-source examples/snippets/servers/tool_progress.py --&gt;
```python
from mcp.server.fastmcp import Context, FastMCP
from mcp.server.session import ServerSession

mcp = FastMCP(name=&quot;Progress Example&quot;)


@mcp.tool()
async def long_running_task(task_name: str, ctx: Context[ServerSession, None], steps: int = 5) -&gt; str:
    &quot;&quot;&quot;Execute a task with progress updates.&quot;&quot;&quot;
    await ctx.info(f&quot;Starting: {task_name}&quot;)

    for i in range(steps):
        progress = (i + 1) / steps
        await ctx.report_progress(
            progress=progress,
            total=1.0,
            message=f&quot;Step {i + 1}/{steps}&quot;,
        )
        await ctx.debug(f&quot;Completed step {i + 1}&quot;)

    return f&quot;Task &#039;{task_name}&#039; completed&quot;
```

_Full example: [examples/snippets/servers/tool_progress.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/tool_progress.py)_
&lt;!-- /snippet-source --&gt;

### Completions

MCP supports providing completion suggestions for prompt arguments and resource template parameters. With the context parameter, servers can provide completions based on previously resolved values:

Client usage:

&lt;!-- snippet-source examples/snippets/clients/completion_client.p

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[browser-use/browser-use]]></title>
            <link>https://github.com/browser-use/browser-use</link>
            <guid>https://github.com/browser-use/browser-use</guid>
            <pubDate>Fri, 15 Aug 2025 00:04:37 GMT</pubDate>
            <description><![CDATA[üåê Make websites accessible for AI agents. Automate tasks online with ease.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/browser-use/browser-use">browser-use/browser-use</a></h1>
            <p>üåê Make websites accessible for AI agents. Automate tasks online with ease.</p>
            <p>Language: Python</p>
            <p>Stars: 67,623</p>
            <p>Forks: 7,825</p>
            <p>Stars today: 82 stars today</p>
            <h2>README</h2><pre>&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;./static/browser-use-dark.png&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;./static/browser-use.png&quot;&gt;
  &lt;img alt=&quot;Shows a black Browser Use Logo in light color mode and a white one in dark color mode.&quot; src=&quot;./static/browser-use.png&quot;  width=&quot;full&quot;&gt;
&lt;/picture&gt;

&lt;h1 align=&quot;center&quot;&gt;Enable AI to control your browser ü§ñ&lt;/h1&gt;

[![GitHub stars](https://img.shields.io/github/stars/gregpr07/browser-use?style=social)](https://github.com/gregpr07/browser-use/stargazers)
[![Discord](https://img.shields.io/discord/1303749220842340412?color=7289DA&amp;label=Discord&amp;logo=discord&amp;logoColor=white)](https://link.browser-use.com/discord)
[![Cloud](https://img.shields.io/badge/Cloud-‚òÅÔ∏è-blue)](https://cloud.browser-use.com)
[![Documentation](https://img.shields.io/badge/Documentation-üìï-blue)](https://docs.browser-use.com)
[![Twitter Follow](https://img.shields.io/twitter/follow/Gregor?style=social)](https://x.com/intent/user?screen_name=gregpr07)
[![Twitter Follow](https://img.shields.io/twitter/follow/Magnus?style=social)](https://x.com/intent/user?screen_name=mamagnus00)
[![Weave Badge](https://img.shields.io/endpoint?url=https%3A%2F%2Fapp.workweave.ai%2Fapi%2Frepository%2Fbadge%2Forg_T5Pvn3UBswTHIsN1dWS3voPg%2F881458615&amp;labelColor=#EC6341)](https://app.workweave.ai/reports/repository/org_T5Pvn3UBswTHIsN1dWS3voPg/881458615)

üåê Browser-use is the easiest way to connect your AI agents with the browser.

üí° See what others are building and share your projects in our [Discord](https://link.browser-use.com/discord)! Want Swag? Check out our [Merch store](https://browsermerch.com).

üå§Ô∏è Skip the setup - try our &lt;b&gt;hosted version&lt;/b&gt; for instant browser automation! &lt;b&gt;[Try the cloud ‚òÅÔ∏é](https://cloud.browser-use.com)&lt;/b&gt;.

# Quick start

With pip (Python&gt;=3.11):

```bash
pip install browser-use
```

Install the browser:

```bash
playwright install chromium --with-deps --no-shell
```

Spin up your agent:

```python
import asyncio
from dotenv import load_dotenv
load_dotenv()
from browser_use import Agent
from browser_use.llm import ChatOpenAI

async def main():
    agent = Agent(
        task=&quot;Compare the price of gpt-4o and DeepSeek-V3&quot;,
        llm=ChatOpenAI(model=&quot;o4-mini&quot;, temperature=1.0),
    )
    await agent.run()

asyncio.run(main())
```

Add your API keys for the provider you want to use to your `.env` file.

```bash
OPENAI_API_KEY=
ANTHROPIC_API_KEY=
AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_KEY=
GOOGLE_API_KEY=
DEEPSEEK_API_KEY=
GROK_API_KEY=
NOVITA_API_KEY=
```

For other settings, models, and more, check out the [documentation üìï](https://docs.browser-use.com).

### Test with UI

You can test browser-use using its [Web UI](https://github.com/browser-use/web-ui) or [Desktop App](https://github.com/browser-use/desktop).

### Test with an interactive CLI

You can also use our `browser-use` interactive CLI (similar to `claude` code):

```bash
pip install &quot;browser-use[cli]&quot;
browser-use
```

## MCP Integration

Browser-use supports the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/), enabling integration with Claude Desktop and other MCP-compatible clients.

### Use as MCP Server with Claude Desktop

Add browser-use to your Claude Desktop configuration:

```json
{
  &quot;mcpServers&quot;: {
    &quot;browser-use&quot;: {
      &quot;command&quot;: &quot;uvx&quot;,
      &quot;args&quot;: [&quot;browser-use[cli]&quot;, &quot;--mcp&quot;],
      &quot;env&quot;: {
        &quot;OPENAI_API_KEY&quot;: &quot;sk-...&quot;
      }
    }
  }
}
```

This gives Claude Desktop access to browser automation tools for web scraping, form filling, and more.

### Connect External MCP Servers to Browser-Use Agent

Browser-use agents can connect to multiple external MCP servers to extend their capabilities:

```python
import asyncio
from browser_use import Agent, Controller
from browser_use.mcp.client import MCPClient
from browser_use.llm import ChatOpenAI

async def main():
    # Initialize controller
    controller = Controller()
    
    # Connect to multiple MCP servers
    filesystem_client = MCPClient(
        server_name=&quot;filesystem&quot;,
        command=&quot;npx&quot;,
        args=[&quot;-y&quot;, &quot;@modelcontextprotocol/server-filesystem&quot;, &quot;/Users/me/documents&quot;]
    )
    
    github_client = MCPClient(
        server_name=&quot;github&quot;, 
        command=&quot;npx&quot;,
        args=[&quot;-y&quot;, &quot;@modelcontextprotocol/server-github&quot;],
        env={&quot;GITHUB_TOKEN&quot;: &quot;your-github-token&quot;}
    )
    
    # Connect and register tools from both servers
    await filesystem_client.connect()
    await filesystem_client.register_to_controller(controller)
    
    await github_client.connect()
    await github_client.register_to_controller(controller)
    
    # Create agent with MCP-enabled controller
    agent = Agent(
        task=&quot;Find the latest report.pdf in my documents and create a GitHub issue about it&quot;,
        llm=ChatOpenAI(model=&quot;gpt-4o&quot;),
        controller=controller  # Controller has tools from both MCP servers
    )
    
    # Run the agent
    await agent.run()
    
    # Cleanup
    await filesystem_client.disconnect()
    await github_client.disconnect()

asyncio.run(main())
```

See the [MCP documentation](https://docs.browser-use.com/customize/mcp-server) for more details.

# Demos

&lt;br/&gt;&lt;br/&gt;

[Task](https://github.com/browser-use/browser-use/blob/main/examples/use-cases/shopping.py): Add grocery items to cart, and checkout.

[![AI Did My Groceries](https://github.com/user-attachments/assets/a0ffd23d-9a11-4368-8893-b092703abc14)](https://www.youtube.com/watch?v=L2Ya9PYNns8)

&lt;br/&gt;&lt;br/&gt;

Prompt: Add my latest LinkedIn follower to my leads in Salesforce.

![LinkedIn to Salesforce](https://github.com/user-attachments/assets/50d6e691-b66b-4077-a46c-49e9d4707e07)

&lt;br/&gt;&lt;br/&gt;

[Prompt](https://github.com/browser-use/browser-use/blob/main/examples/use-cases/find_and_apply_to_jobs.py): Read my CV &amp; find ML jobs, save them to a file, and then start applying for them in new tabs, if you need help, ask me.&#039;

https://github.com/user-attachments/assets/171fb4d6-0355-46f2-863e-edb04a828d04

&lt;br/&gt;&lt;br/&gt;

[Prompt](https://github.com/browser-use/browser-use/blob/main/examples/browser/real_browser.py): Write a letter in Google Docs to my Papa, thanking him for everything, and save the document as a PDF.

![Letter to Papa](https://github.com/user-attachments/assets/242ade3e-15bc-41c2-988f-cbc5415a66aa)

&lt;br/&gt;&lt;br/&gt;

[Prompt](https://github.com/browser-use/browser-use/blob/main/examples/custom-functions/save_to_file_hugging_face.py): Look up models with a license of cc-by-sa-4.0 and sort by most likes on Hugging face, save top 5 to file.

https://github.com/user-attachments/assets/de73ee39-432c-4b97-b4e8-939fd7f323b3

&lt;br/&gt;&lt;br/&gt;

## More examples

For more examples see the [examples](examples) folder or join the [Discord](https://link.browser-use.com/discord) and show off your project. You can also see our [`awesome-prompts`](https://github.com/browser-use/awesome-prompts) repo for prompting inspiration.

# Vision

Tell your computer what to do, and it gets it done.

## Roadmap

### Agent

- [ ] Improve agent memory to handle +100 steps
- [ ] Enhance planning capabilities (load website specific context)
- [ ] Reduce token consumption (system prompt, DOM state)

### DOM Extraction

- [ ] Enable detection for all possible UI elements
- [ ] Improve state representation for UI elements so that all LLMs can understand what&#039;s on the page

### Workflows

- [ ] Let user record a workflow - which we can rerun with browser-use as a fallback
- [ ] Make rerunning of workflows work, even if pages change

### User Experience

- [ ] Create various templates for tutorial execution, job application, QA testing, social media, etc. which users can just copy &amp; paste.
- [ ] Improve docs
- [ ] Make it faster

### Parallelization

- [ ] Human work is sequential. The real power of a browser agent comes into reality if we can parallelize similar tasks. For example, if you want to find contact information for 100 companies, this can all be done in parallel and reported back to a main agent, which processes the results and kicks off parallel subtasks again.

## Contributing

We love contributions! Feel free to open issues for bugs or feature requests. To contribute to the docs, check out the `/docs` folder.

## üß™ How to make your agents robust?

We offer to run your tasks in our CI‚Äîautomatically, on every update!

- **Add your task:** Add a YAML file in `tests/agent_tasks/` (see the [`README there`](tests/agent_tasks/README.md) for details).
- **Automatic validation:** Every time we push updates, your task will be run by the agent and evaluated using your criteria.

## Local Setup

To learn more about the library, check out the [local setup üìï](https://docs.browser-use.com/development/local-setup).

`main` is the primary development branch with frequent changes. For production use, install a stable [versioned release](https://github.com/browser-use/browser-use/releases) instead.

---

## Swag

Want to show off your Browser-use swag? Check out our [Merch store](https://browsermerch.com). Good contributors will receive swag for free üëÄ.

## Citation

If you use Browser Use in your research or project, please cite:

```bibtex
@software{browser_use2024,
  author = {M√ºller, Magnus and ≈Ωuniƒç, Gregor},
  title = {Browser Use: Enable AI to control your browser},
  year = {2024},
  publisher = {GitHub},
  url = {https://github.com/browser-use/browser-use}
}
```

 &lt;div align=&quot;center&quot;&gt; &lt;img src=&quot;https://github.com/user-attachments/assets/06fa3078-8461-4560-b434-445510c1766f&quot; width=&quot;400&quot;/&gt; 
 
[![Twitter Follow](https://img.shields.io/twitter/follow/Gregor?style=social)](https://x.com/intent/user?screen_name=gregpr07)
[![Twitter Follow](https://img.shields.io/twitter/follow/Magnus?style=social)](https://x.com/intent/user?screen_name=mamagnus00)
 
 &lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
Made with ‚ù§Ô∏è in Zurich and San Francisco
 &lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hiyouga/LLaMA-Factory]]></title>
            <link>https://github.com/hiyouga/LLaMA-Factory</link>
            <guid>https://github.com/hiyouga/LLaMA-Factory</guid>
            <pubDate>Fri, 15 Aug 2025 00:04:36 GMT</pubDate>
            <description><![CDATA[Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hiyouga/LLaMA-Factory">hiyouga/LLaMA-Factory</a></h1>
            <p>Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)</p>
            <p>Language: Python</p>
            <p>Stars: 56,194</p>
            <p>Forks: 6,892</p>
            <p>Stars today: 68 stars today</p>
            <h2>README</h2><pre>![# LLaMA Factory](assets/logo.png)

[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social)](https://github.com/hiyouga/LLaMA-Factory/stargazers)
[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory)](https://github.com/hiyouga/LLaMA-Factory/commits/main)
[![GitHub contributors](https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange)](https://github.com/hiyouga/LLaMA-Factory/graphs/contributors)
[![GitHub workflow](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg)](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml)
[![PyPI](https://img.shields.io/pypi/v/llamafactory)](https://pypi.org/project/llamafactory/)
[![Citation](https://img.shields.io/badge/citation-760-green)](https://scholar.google.com/scholar?cites=12620864006390196564)
[![Docker Pulls](https://img.shields.io/docker/pulls/hiyouga/llamafactory)](https://hub.docker.com/r/hiyouga/llamafactory/tags)

[![Twitter](https://img.shields.io/twitter/follow/llamafactory_ai)](https://twitter.com/llamafactory_ai)
[![Discord](https://dcbadge.vercel.app/api/server/rKfvV9r9FK?compact=true&amp;style=flat)](https://discord.gg/rKfvV9r9FK)
[![GitCode](https://gitcode.com/zhengyaowei/LLaMA-Factory/star/badge.svg)](https://gitcode.com/zhengyaowei/LLaMA-Factory)

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)
[![Open in DSW](https://gallery.pai-ml.com/assets/open-in-dsw.svg)](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory)
[![Open in Alaya](assets/alaya_new.svg)](https://docs.alayanew.com/docs/documents/newActivities/llamafactory/?utm_source=LLaMA-Factory)
[![Open in Spaces](https://img.shields.io/badge/ü§ó-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/hiyouga/LLaMA-Board)
[![Open in Studios](https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue)](https://modelscope.cn/studios/hiyouga/LLaMA-Board)
[![Open in Novita](https://img.shields.io/badge/Novita-Deploy%20Template-blue)](https://novita.ai/templates-library/105981?sharer=88115474-394e-4bda-968e-b88e123d0c47)

### Used by [Amazon](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/), [NVIDIA](https://developer.nvidia.com/rtx/ai-toolkit), [Aliyun](https://help.aliyun.com/zh/pai/use-cases/fine-tune-a-llama-3-model-with-llama-factory), etc.

&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;

### Supporters ‚ù§Ô∏è

| &lt;div style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://warp.dev/llama-factory&quot;&gt;&lt;img alt=&quot;Warp sponsorship&quot; width=&quot;400&quot; src=&quot;assets/warp.jpg&quot;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://warp.dev/llama-factory&quot; style=&quot;font-size:larger;&quot;&gt;Warp, the agentic terminal for developers&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://warp.dev/llama-factory&quot;&gt;Available for MacOS, Linux, &amp; Windows&lt;/a&gt; | &lt;a href=&quot;https://serpapi.com&quot;&gt;&lt;img alt=&quot;SerpAPI sponsorship&quot; width=&quot;250&quot; src=&quot;assets/serpapi.svg&quot;&gt; &lt;/a&gt; |
| ---- | ---- |

----

### Easily fine-tune 100+ large language models with zero-code [CLI](#quickstart) and [Web UI](#fine-tuning-with-llama-board-gui-powered-by-gradio)

![GitHub Trend](https://trendshift.io/api/badge/repositories/4535)

&lt;/div&gt;

üëã Join our [WeChat group](assets/wechat.jpg), [NPU user group](assets/wechat_npu.jpg) or [Alaya NeW user group](assets/wechat_alaya.png).

\[ English | [‰∏≠Êñá](README_zh.md) \]

**Fine-tuning a large language model can be easy as...**

https://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e

Choose your path:

- **Documentation (WIP)**: https://llamafactory.readthedocs.io/en/latest/
- **Documentation (AMD GPU)**: https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html
- **Colab (free)**: https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing
- **Local machine**: Please refer to [usage](#getting-started)
- **PAI-DSW (free trial)**: https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory
- **Alaya NeW (cloud GPU deal)**: https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory

&gt; [!NOTE]
&gt; Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.

## Table of Contents

- [Features](#features)
- [Blogs](#blogs)
- [Changelog](#changelog)
- [Supported Models](#supported-models)
- [Supported Training Approaches](#supported-training-approaches)
- [Provided Datasets](#provided-datasets)
- [Requirement](#requirement)
- [Getting Started](#getting-started)
  - [Installation](#installation)
  - [Data Preparation](#data-preparation)
  - [Quickstart](#quickstart)
  - [Fine-Tuning with LLaMA Board GUI](#fine-tuning-with-llama-board-gui-powered-by-gradio)
  - [Build Docker](#build-docker)
  - [Deploy with OpenAI-style API and vLLM](#deploy-with-openai-style-api-and-vllm)
  - [Download from ModelScope Hub](#download-from-modelscope-hub)
  - [Download from Modelers Hub](#download-from-modelers-hub)
  - [Use W&amp;B Logger](#use-wb-logger)
  - [Use SwanLab Logger](#use-swanlab-logger)
- [Projects using LLaMA Factory](#projects-using-llama-factory)
- [License](#license)
- [Citation](#citation)
- [Acknowledgement](#acknowledgement)

## Features

- **Various models**: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.
- **Integrated methods**: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.
- **Scalable resources**: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.
- **Advanced algorithms**: [GaLore](https://github.com/jiaweizzhao/GaLore), [BAdam](https://github.com/Ledzy/BAdam), [APOLLO](https://github.com/zhuhanqing/APOLLO), [Adam-mini](https://github.com/zyushun/Adam-mini), [Muon](https://github.com/KellerJordan/Muon), DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.
- **Practical tricks**: [FlashAttention-2](https://github.com/Dao-AILab/flash-attention), [Unsloth](https://github.com/unslothai/unsloth), [Liger Kernel](https://github.com/linkedin/Liger-Kernel), RoPE scaling, NEFTune and rsLoRA.
- **Wide tasks**: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.
- **Experiment monitors**: LlamaBoard, TensorBoard, Wandb, MLflow, [SwanLab](https://github.com/SwanHubX/SwanLab), etc.
- **Faster inference**: OpenAI-style API, Gradio UI and CLI with [vLLM worker](https://github.com/vllm-project/vllm) or [SGLang worker](https://github.com/sgl-project/sglang).

### Day-N Support for Fine-Tuning Cutting-Edge Models

| Support Date | Model Name                                                           |
| ------------ | -------------------------------------------------------------------- |
| Day 0        | Qwen3 / Qwen2.5-VL / Gemma 3 / GLM-4.1V / InternLM 3 / MiniCPM-o-2.6 |
| Day 1        | Llama 3 / GLM-4 / Mistral Small / PaliGemma2 / Llama 4               |

## Blogs

- [Fine-tune GPT-OSS for Role-Playing using LLaMA-Factory](https://docs.llamafactory.com.cn/docs/documents/best-practice/gptoss/?utm_source=LLaMA-Factory) (Chinese)
- [Fine-tune Llama3.1-70B for Medical Diagnosis using LLaMA-Factory](https://docs.alayanew.com/docs/documents/bestPractice/bigModel/llama70B/?utm_source=LLaMA-Factory) (Chinese)
- [A One-Stop Code-Free Model Reinforcement Learning and Deployment Platform based on LLaMA-Factory and EasyR1](https://aws.amazon.com/cn/blogs/china/building-llm-model-hub-based-on-llamafactory-and-easyr1/) (Chinese)
- [How Apoidea Group enhances visual information extraction from banking documents with multimodal models using LLaMA-Factory on Amazon SageMaker HyperPod](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/) (English)
- [Easy Dataset √ó LLaMA Factory: Enabling LLMs to Efficiently Learn Domain Knowledge](https://buaa-act.feishu.cn/wiki/GVzlwYcRFiR8OLkHbL6cQpYin7g) (English)

&lt;details&gt;&lt;summary&gt;All Blogs&lt;/summary&gt;

- [Fine-tune Qwen2.5-VL for Autonomous Driving using LLaMA-Factory](https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory) (Chinese)
- [LLaMA Factory: Fine-tuning the DeepSeek-R1-Distill-Qwen-7B Model for News Classifier](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b) (Chinese)
- [A One-Stop Code-Free Model Fine-Tuning \&amp; Deployment Platform based on SageMaker and LLaMA-Factory](https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/) (Chinese)
- [LLaMA Factory Multi-Modal Fine-Tuning Practice: Fine-Tuning Qwen2-VL for Personal Tourist Guide](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl) (Chinese)
- [LLaMA Factory: Fine-tuning Llama3 for Role-Playing](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory) (Chinese)

&lt;/details&gt;

## Changelog

[25/08/06] We supported fine-tuning the **[GPT-OSS](https://github.com/openai/gpt-oss)** models. See [PR #8826](https://github.com/hiyouga/LLaMA-Factory/pull/8826) to get started.

[25/07/02] We supported fine-tuning the **[GLM-4.1V-9B-Thinking](https://github.com/THUDM/GLM-4.1V-Thinking)** model.

[25/04/28] We supported fine-tuning the **[Qwen3](https://qwenlm.github.io/blog/qwen3/)** model family.

&lt;details&gt;&lt;summary&gt;Full Changelog&lt;/summary&gt;

[25/04/21] We supported the **[Muon](https://github.com/KellerJordan/Muon)** optimizer. See [examples](examples/README.md) for usage. Thank [@tianshijing](https://github.com/tianshijing)&#039;s PR.

[25/04/16] We supported fine-tuning the **[InternVL3](https://huggingface.co/OpenGVLab/InternVL3-8B)** model. See [PR #7258](https://github.com/hiyouga/LLaMA-Factory/pull/7258) to get started.

[25/04/14] We supported fine-tuning the **[GLM-Z1](https://huggingface.co/THUDM/GLM-Z1-9B-0414)** and **[Kimi-VL](https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct)** models.

[25/04/06] We supported fine-tuning the **[Llama 4](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)** model. See [PR #7611](https://github.com/hiyouga/LLaMA-Factory/pull/7611) to get started.

[25/03/31] We supported fine-tuning the **[Qwen2.5 Omni](https://qwenlm.github.io/blog/qwen2.5-omni/)** model. See [PR #7537](https://github.com/hiyouga/LLaMA-Factory/pull/7537) to get started.

[25/03/15] We supported **[SGLang](https://github.com/sgl-project/sglang)** as inference backend. Try `infer_backend: sglang` to accelerate inference.

[25/03/12] We supported fine-tuning the **[Gemma 3](https://huggingface.co/blog/gemma3)** model.

[25/02/24] Announcing **[EasyR1](https://github.com/hiyouga/EasyR1)**, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.

[25/02/11] We supported saving the **[Ollama](https://github.com/ollama/ollama)** modelfile when exporting the model checkpoints. See [examples](examples/README.md) for usage.

[25/02/05] We supported fine-tuning the **[Qwen2-Audio](Qwen/Qwen2-Audio-7B-Instruct)** and **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** on audio understanding tasks.

[25/01/31] We supported fine-tuning the **[DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)** and **[Qwen2.5-VL](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)** models.

[25/01/15] We supported **[APOLLO](https://arxiv.org/abs/2412.05270)** optimizer. See [examples](examples/README.md) for usage.

[25/01/14] We supported fine-tuning the **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** and **[MiniCPM-V-2.6](https://huggingface.co/openbmb/MiniCPM-V-2_6)** models. Thank [@BUAADreamer](https://github.com/BUAADreamer)&#039;s PR.

[25/01/14] We supported fine-tuning the **[InternLM 3](https://huggingface.co/collections/internlm/)** models. Thank [@hhaAndroid](https://github.com/hhaAndroid)&#039;s PR.

[25/01/10] We supported fine-tuning the **[Phi-4](https://huggingface.co/microsoft/phi-4)** model.

[24/12/21] We supported using **[SwanLab](https://github.com/SwanHubX/SwanLab)** for experiment tracking and visualization. See [this section](#use-swanlab-logger) for details.

[24/11/27] We supported fine-tuning the **[Skywork-o1](https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B)** model and the **[OpenO1](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)** dataset.

[24/10/09] We supported downloading pre-trained models and datasets from the **[Modelers Hub](https://modelers.cn/models)**. See [this tutorial](#download-from-modelers-hub) for usage.

[24/09/19] We supported fine-tuning the **[Qwen2.5](https://qwenlm.github.io/blog/qwen2.5/)** models.

[24/08/30] We supported fine-tuning the **[Qwen2-VL](https://qwenlm.github.io/blog/qwen2-vl/)** models. Thank [@simonJJJ](https://github.com/simonJJJ)&#039;s PR.

[24/08/27] We supported **[Liger Kernel](https://github.com/linkedin/Liger-Kernel)**. Try `enable_liger_kernel: true` for efficient training.

[24/08/09] We supported **[Adam-mini](https://github.com/zyushun/Adam-mini)** optimizer. See [examples](examples/README.md) for usage. Thank [@relic-yuexi](https://github.com/relic-yuexi)&#039;s PR.

[24/07/04] We supported [contamination-free packed training](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing). Use `neat_packing: true` to activate it. Thank [@chuan298](https://github.com/chuan298)&#039;s PR.

[24/06/16] We supported **[PiSSA](https://arxiv.org/abs/2404.02948)** algorithm. See [examples](examples/README.md) for usage.

[24/06/07] We supported fine-tuning the **[Qwen2](https://qwenlm.github.io/blog/qwen2/)** and **[GLM-4](https://github.com/THUDM/GLM-4)** models.

[24/05/26] We supported **[SimPO](https://arxiv.org/abs/2405.14734)** algorithm for preference learning. See [examples](examples/README.md) for usage.

[24/05/20] We supported fine-tuning the **PaliGemma** series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with `paligemma` template for chat completion.

[24/05/18] We supported **[KTO](https://arxiv.org/abs/2402.01306)** algorithm for preference learning. See [examples](examples/README.md) for usage.

[24/05/14] We supported training and inference on the Ascend NPU devices. Check [installation](#installation) section for details.

[24/04/26] We supported fine-tuning the **LLaVA-1.5** multimodal LLMs. See [examples](examples/README.md) for usage.

[24/04/22] We provided a **[Colab notebook](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)** for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check [Llama3-8B-Chinese-Chat](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat) and [Llama3-Chinese](https://huggingface.co/zhichen/Llama3-Chinese) for details.

[24/04/21] We supported **[Mixture-of-Depths](https://arxiv.org/abs/2404.02258)** according to [AstraMindAI&#039;s implementation](https://github.com/astramind-ai/Mixture-of-depths). See [examples](examples/README.md) for usage.

[24/04/16] We supported **[BAdam](https://arxiv.org/abs/2404.02827)** optimizer. See [examples](examples/README.md) for usage.

[24/04/16] We supported **[unsloth](https://github.com/unslothai/unsloth)**&#039;s long-sequence training (Llama-2-7B-56k within 24GB). It achieves **117%** speed and **50%** memory compared with FlashAttention-2, more benchmarks can be found in [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison).

[24/03/31] We supported **[ORPO](https://arxiv.org/abs/2403.07691)**. See [examples](examples/README.md) for usage.

[24/03/21] Our paper &quot;[LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models](https://arxiv.org/abs/2403.13372)&quot; is available at arXiv!

[24/03/20] We supported **FSDP+QLoRA** that fine-tunes a 70B model on 2x24GB GPUs. See [examples](examples/README.md) for usage.

[24/03/13] We supported **[LoRA+](https://arxiv.org/abs/2402.12354)**. See [examples](examples/README.md) for usage.

[24/03/07] We supported **[GaLore](https://arxiv.org/abs/2403.03507)** optimizer. See [examples](examples/README.md) for usage.

[24/03/07] We integrated **[vLLM](https://github.com/vllm-project/vllm)** for faster and concurrent inference. Try `infer_backend: vllm` to enjoy **270%** inference speed.

[24/02/28] We supported weight-decomposed LoRA (**[DoRA](https://arxiv.org/abs/2402.09353)**). Try `use_dora: true` to activate DoRA training.

[24/02/15] We supported **block expansion** proposed by [LLaMA Pro](https://github.com/TencentARC/LLaMA-Pro). See [examples](examples/README.md) for usage.

[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this [blog post](https://qwenlm.github.io/blog/qwen1.5/) for details.

[24/01/18] We supported **agent tuning** for most models, equipping model with tool using abilities by fine-tuning with `dataset: glaive_toolcall_en`.

[23/12/23] We supported **[unsloth](https://github.com/unslothai/unsloth)**&#039;s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try `use_unsloth: true` argument to activate unsloth patch. It achieves **170%** speed in our benchmark, check [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison) for details.

[23/12/12] We supported fine-tuning the latest MoE model **[Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)** in our framework. See hardware requirement [here](#hardware-requirement).

[23/12/01] We supported downloading pre-trained models and datasets from the **[ModelScope Hub](https://modelscope.cn/models)**. See [this tutorial](#download-from-modelscope-hub) for usage.

[23/10/21] We supported **[NEFTune](https://arxiv.org/abs/2310.05914)** trick for fine-tuning. Try `neftune_noise_alpha: 5` argument to activate NEFTune.

[23/09/27] We supported **$S^2$-Attn** proposed by [LongLoRA](https://github.com/dvlab-research/LongLoRA) for the LLaMA models. Try `shift_attn: true` argument to enable shift short attention.

[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See [examples](examples/README.md) for usage.

[23/09/10] We supported **[FlashAttention-2](https://github.com/Dao-AILab/flash-attention)**. Try `flash_attn: fa2` argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.

[23/08/12] We supported **RoPE scaling** to extend the context length of the LLaMA models. Try `rope_scaling: linear` argument in training and `rope_scaling: dynamic` argument at inference to extrapolate the position embeddings.

[23/08/11] We supported **[DPO training](https://arxiv.org/abs/2305.18290)** for instruction-tuned models. See [examples](examples/README.md) for usage.

[23/07/31] We supported **dataset streaming**. Try `streaming: true` and `max_steps: 10000` arguments to load your dataset in streaming mode.

[23/07/29] We released two instruction-tuned 13B models at Hugging Face. See these Hugging Face Repos ([LLaMA-2](https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat) / [Baichuan](https://huggingface.co/hiyouga/Baichuan-13B-sft)) for details.

[23/07/18] We developed an **all-in-one Web UI** for training, evaluation and inference. Try `train_web.py` to fine-tune models in your Web browser. Thank [@KanadeSiina](https://github.com/KanadeSiina) and [@codemayq](https://github.com/codemayq) for their efforts in the development.

[23/07/09] We released **[FastEdit](https://github.com/hiyouga/FastEdit)** ‚ö°ü©π, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follo

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[martinvigo/email2phonenumber]]></title>
            <link>https://github.com/martinvigo/email2phonenumber</link>
            <guid>https://github.com/martinvigo/email2phonenumber</guid>
            <pubDate>Fri, 15 Aug 2025 00:04:35 GMT</pubDate>
            <description><![CDATA[A OSINT tool to obtain a target's phone number just by having his email address]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/martinvigo/email2phonenumber">martinvigo/email2phonenumber</a></h1>
            <p>A OSINT tool to obtain a target's phone number just by having his email address</p>
            <p>Language: Python</p>
            <p>Stars: 2,441</p>
            <p>Forks: 288</p>
            <p>Stars today: 20 stars today</p>
            <h2>README</h2><pre># email2phonenumber
email2phonenumber is an OSINT tool that allows you to obtain a target&#039;s phone number just by having his email address.

For full details check: [https://www.martinvigo.com/email2phonenumber](https://www.martinvigo.com/email2phonenumber)

Demo: [https://www.youtube.com/watch?v=dfvqhDUn81s](https://www.youtube.com/watch?v=dfvqhDUn81s)

***IMPORTANT:*** *email2phonenumber is a proof-of-concept tool I wrote during my research on new OSINT methodologies to obtain a target&#039;s phone number. The supported services (Ebay, Lastpass, Amazon and Twitter) have long added protections to protect from these type of scraping like having to receive a code over email first or simply adding captchas. There are of course many other sites that are still leaking phone number digits but I am focused on other research projects. Feel free to submit pull request if you want to add support for new sites.

Please check out my newer tool &quot;[Phonerator](https://www.martinvigo.com/tools/phonerator/)&quot;, which is maintained and focuses on the novel aspect of this research, generating valid phone numbers. 
[See more details](https://www.martinvigo.com/phonerator-an-advanced-valid-phone-number-generator/). There is also a small OSINT challenge in there... ;)

## Basic info
This tool helps automate discovering someone&#039;s phone number by abusing password reset design weaknesses and publicly available data. It supports 3 main functions:

* &quot;scrape&quot; - scrapes websites for phone number digits by initiating password reset using the target&#039;s email address
* &quot;generate&quot; - creates a list of valid phone numbers based on the country&#039;s Phone Numbering Plan publicly available information
* &quot;bruteforce&quot; - iterates over a list of phone numbers and initiates password reset on different websites to obtain associated masked emails and correlate it to the victim&#039;s one

## Setup
email2phonenumber was developed on Python 3.x

You will need couple 3rd party libraries: BeautifulSoup and requests. These can be easily installed with pip

```
pip3 install beautifulsoup4 requests
```

## Usage
Scrape websites for phone number digits
```
python3 email2phonenumber.py scrape -e target@email.com
```

Generate a dictionary of valid phone numbers based on a phone number mask
```
python3 email2phonenumber.py generate -m 555XXX1234 -o /tmp/dic.txt
```
Find target&#039;s phone number by resetting passwords on websites that do not alert the target using a phone number mask and proxies to avoid captchas and other abuse protections
```
python3 email2phonenumber.py bruteforce -m 555XXX1234 -e target@email.com -p /tmp/proxies.txt -q
```

## Demo video
[![email2phonenumber demo video](https://img.youtube.com/vi/dfvqhDUn81s/0.jpg)](https://www.youtube.com/watch?v=dfvqhDUn81s)

## Tool presentation at BSides Las Vegas 2019
[![Tool presentation at Bsides Las Vegas 2019](https://img.youtube.com/vi/1zssBR85vDA/0.jpg)](https://www.youtube.com/watch?v=1zssBR85vDA)

## Authors
Martin Vigo - @martin_vigo - [martinvigo.com](https://www.martinvigo.com)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>