<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Fri, 09 May 2025 00:04:26 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[awslabs/agent-squad]]></title>
            <link>https://github.com/awslabs/agent-squad</link>
            <guid>https://github.com/awslabs/agent-squad</guid>
            <pubDate>Fri, 09 May 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[Flexible and powerful framework for managing multiple AI agents and handling complex conversations]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/awslabs/agent-squad">awslabs/agent-squad</a></h1>
            <p>Flexible and powerful framework for managing multiple AI agents and handling complex conversations</p>
            <p>Language: Python</p>
            <p>Stars: 5,378</p>
            <p>Forks: 442</p>
            <p>Stars today: 79 stars today</p>
            <h2>README</h2><pre>&lt;h2 align=&quot;center&quot;&gt;Agent Squad&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;Flexible, lightweight open-source framework for orchestrating multiple AI agents to handle complex conversations.&lt;/p&gt;

---
&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;ğŸ“¢ New Name Alert:&lt;/strong&gt; Multi-Agent Orchestrator is now &lt;strong&gt;Agent Squad!&lt;/strong&gt; ğŸ‰&lt;br&gt;
  Same powerful functionalities, new catchy name. Embrace the squad!
&lt;/p&gt;

---

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/awslabs/agent-squad&quot;&gt;&lt;img alt=&quot;GitHub Repo&quot; src=&quot;https://img.shields.io/badge/GitHub-Repo-green.svg&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.npmjs.com/package/agent-squad&quot;&gt;&lt;img alt=&quot;npm&quot; src=&quot;https://img.shields.io/npm/v/agent-squad.svg?style=flat-square&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/agent-squad/&quot;&gt;&lt;img alt=&quot;PyPI&quot; src=&quot;https://img.shields.io/pypi/v/agent-squad.svg?style=flat-square&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;!-- GitHub Stats --&gt;
  &lt;img src=&quot;https://img.shields.io/github/stars/awslabs/agent-squad?style=social&quot; alt=&quot;GitHub stars&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/github/forks/awslabs/agent-squad?style=social&quot; alt=&quot;GitHub forks&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/github/watchers/awslabs/agent-squad?style=social&quot; alt=&quot;GitHub watchers&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;!-- Repository Info --&gt;
  &lt;img src=&quot;https://img.shields.io/github/last-commit/awslabs/agent-squad&quot; alt=&quot;Last Commit&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/github/issues/awslabs/agent-squad&quot; alt=&quot;Issues&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/github/issues-pr/awslabs/agent-squad&quot; alt=&quot;Pull Requests&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://awslabs.github.io/agent-squad/&quot; style=&quot;display: inline-block; background-color: #0066cc; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold; font-size: 15px; transition: background-color 0.3s;&quot;&gt;
    ğŸ“š Explore Full Documentation
  &lt;/a&gt;
&lt;/p&gt;


## ğŸ”– Features

- ğŸ§  **Intelligent intent classification** â€” Dynamically route queries to the most suitable agent based on context and content.
- ğŸ”¤ **Dual language support** â€” Fully implemented in both **Python** and **TypeScript**.
- ğŸŒŠ **Flexible agent responses** â€” Support for both streaming and non-streaming responses from different agents.
- ğŸ“š **Context management** â€” Maintain and utilize conversation context across multiple agents for coherent interactions.
- ğŸ”§ **Extensible architecture** â€” Easily integrate new agents or customize existing ones to fit your specific needs.
- ğŸŒ **Universal deployment** â€” Run anywhere - from AWS Lambda to your local environment or any cloud platform.
- ğŸ“¦ **Pre-built agents and classifiers** â€” A variety of ready-to-use agents and multiple classifier implementations available.


## What&#039;s the Agent Squad â“

The Agent Squad is a flexible framework for managing multiple AI agents and handling complex conversations. It intelligently routes queries and maintains context across interactions.

The system offers pre-built components for quick deployment, while also allowing easy integration of custom agents and conversation messages storage solutions.

This adaptability makes it suitable for a wide range of applications, from simple chatbots to sophisticated AI systems, accommodating diverse requirements and scaling efficiently.

&lt;hr/&gt;

## ğŸ—ï¸ High-level architecture flow diagram

&lt;br /&gt;&lt;br /&gt;

![High-level architecture flow diagram](https://raw.githubusercontent.com/awslabs/agent-squad/main/img/flow.jpg)

&lt;br /&gt;&lt;br /&gt;

1. The process begins with user input, which is analyzed by a Classifier.
2. The Classifier leverages both Agents&#039; Characteristics and Agents&#039; Conversation history to select the most appropriate agent for the task.
3. Once an agent is selected, it processes the user input.
4. The orchestrator then saves the conversation, updating the Agents&#039; Conversation history, before delivering the response back to the user.


## ![](https://raw.githubusercontent.com/awslabs/agent-squad/main/img/new.png) Introducing SupervisorAgent: Agents Coordination

The Agent Squad now includes a powerful new SupervisorAgent that enables sophisticated team coordination between multiple specialized agents. This new component implements a &quot;agent-as-tools&quot; architecture, allowing a lead agent to coordinate a team of specialized agents in parallel, maintaining context and delivering coherent responses.

![SupervisorAgent flow diagram](https://raw.githubusercontent.com/awslabs/agent-squad/main/img/flow-supervisor.jpg)

Key capabilities:
- ğŸ¤ **Team Coordination** - Coordonate multiple specialized agents working together on complex tasks
- âš¡ **Parallel Processing** - Execute multiple agent queries simultaneously
- ğŸ§  **Smart Context Management** - Maintain conversation history across all team members
- ğŸ”„ **Dynamic Delegation** - Intelligently distribute subtasks to appropriate team members
- ğŸ¤– **Agent Compatibility** - Works with all agent types (Bedrock, Anthropic, Lex, etc.)

The SupervisorAgent can be used in two powerful ways:
1. **Direct Usage** - Call it directly when you need dedicated team coordination for specific tasks
2. **Classifier Integration** - Add it as an agent within the classifier to build complex hierarchical systems with multiple specialized teams

Here are just a few examples where this agent can be used:
- Customer Support Teams with specialized sub-teams
- AI Movie Production Studios
- Travel Planning Services
- Product Development Teams
- Healthcare Coordination Systems


[Learn more about SupervisorAgent â†’](https://awslabs.github.io/agent-squad/agents/built-in/supervisor-agent)


## ğŸ’¬ Demo App

In the screen recording below, we demonstrate an extended version of the demo app that uses 6 specialized agents:
- **Travel Agent**: Powered by an Amazon Lex Bot
- **Weather Agent**: Utilizes a Bedrock LLM Agent with a tool to query the open-meteo API
- **Restaurant Agent**: Implemented as an Amazon Bedrock Agent
- **Math Agent**: Utilizes a Bedrock LLM Agent with two tools for executing mathematical operations
- **Tech Agent**: A Bedrock LLM Agent designed to answer questions on technical topics
- **Health Agent**: A Bedrock LLM Agent focused on addressing health-related queries

Watch as the system seamlessly switches context between diverse topics, from booking flights to checking weather, solving math problems, and providing health information.
Notice how the appropriate agent is selected for each query, maintaining coherence even with brief follow-up inputs.

The demo highlights the system&#039;s ability to handle complex, multi-turn conversations while preserving context and leveraging specialized agents across various domains.

![](https://raw.githubusercontent.com/awslabs/agent-squad/main/img/demo-app.gif?raw=true)


## ğŸ¯ Examples &amp; Quick Start

Get hands-on experience with the Agent Squad through our diverse set of examples:

- **Demo Applications**:
  - [Streamlit Global Demo](https://github.com/awslabs/agent-squad/tree/main/examples/python): A single Streamlit application showcasing multiple demos, including:
    - AI Movie Production Studio
    - AI Travel Planner
  - [Chat Demo App](https://awslabs.github.io/agent-squad/cookbook/examples/chat-demo-app/):
    - Explore multiple specialized agents handling various domains like travel, weather, math, and health
  - [E-commerce Support Simulator](https://awslabs.github.io/agent-squad/cookbook/examples/ecommerce-support-simulator/): Experience AI-powered customer support with:
    - Automated response generation for common queries
    - Intelligent routing of complex issues to human support
    - Real-time chat and email-style communication
    - Human-in-the-loop interactions for complex cases
- **Sample Projects**: Explore our example implementations in the `examples` folder:
  - [`chat-demo-app`](https://github.com/awslabs/agent-squad/tree/main/examples/chat-demo-app): Web-based chat interface with multiple specialized agents
  - [`ecommerce-support-simulator`](https://github.com/awslabs/agent-squad/tree/main/examples/ecommerce-support-simulator): AI-powered customer support system
  - [`chat-chainlit-app`](https://github.com/awslabs/agent-squad/tree/main/examples/chat-chainlit-app): Chat application built with Chainlit
  - [`fast-api-streaming`](https://github.com/awslabs/agent-squad/tree/main/examples/fast-api-streaming): FastAPI implementation with streaming support
  - [`text-2-structured-output`](https://github.com/awslabs/agent-squad/tree/main/examples/text-2-structured-output): Natural Language to Structured Data
  - [`bedrock-inline-agents`](https://github.com/awslabs/agent-squad/tree/main/examples/bedrock-inline-agents): Bedrock Inline Agents sample
  - [`bedrock-prompt-routing`](https://github.com/awslabs/agent-squad/tree/main/examples/bedrock-prompt-routing): Bedrock Prompt Routing sample code


Examples are available in both Python and TypeScript. Check out our [documentation](https://awslabs.github.io/agent-squad/) for comprehensive guides on setting up and using the Agent Squad framework!

## ğŸ“š Deep Dives: Stories, Blogs &amp; Podcasts

Discover creative implementations and diverse applications of the Agent Squad:

- **[From &#039;Bonjour&#039; to &#039;Boarding Pass&#039;: Multilingual AI Chatbot for Flight Reservations](https://community.aws/content/2lCi8jEKydhDm8eE8QFIQ5K23pF/from-bonjour-to-boarding-pass-multilingual-ai-chatbot-for-flight-reservations)**

  This article demonstrates how to build a multilingual chatbot using the Agent Squad framework. The article explains how to use an **Amazon Lex** bot as an agent, along with 2 other new agents to make it work in many languages with just a few lines of code.

- **[Beyond Auto-Replies: Building an AI-Powered E-commerce Support system](https://community.aws/content/2lq6cYYwTYGc7S3Zmz28xZoQNQj/beyond-auto-replies-building-an-ai-powered-e-commerce-support-system)**

  This article demonstrates how to build an AI-driven multi-agent system for automated e-commerce customer email support. It covers the architecture and setup of specialized AI agents using the Agent Squad framework, integrating automated processing with human-in-the-loop oversight. The guide explores email ingestion, intelligent routing, automated response generation, and human verification, providing a comprehensive approach to balancing AI efficiency with human expertise in customer support.

- **[Speak Up, AI: Voicing Your Agents with Amazon Connect, Lex, and Bedrock](https://community.aws/content/2mt7CFG7xg4yw6GRHwH9akhg0oD/speak-up-ai-voicing-your-agents-with-amazon-connect-lex-and-bedrock)**

  This article demonstrates how to build an AI customer call center. It covers the architecture and setup of specialized AI agents using the Agent Squad framework interacting with voice via **Amazon Connect** and **Amazon Lex**.

- **[Unlock Bedrock InvokeInlineAgent API&#039;s Hidden Potential](https://community.aws/content/2pTsHrYPqvAbJBl9ht1XxPOSPjR/unlock-bedrock-invokeinlineagent-api-s-hidden-potential-with-agent-squad)**

  Learn how to scale **Amazon Bedrock Agents** beyond knowledge base limitations using the Agent Squad framework and **InvokeInlineAgent API**. This article demonstrates dynamic agent creation and knowledge base selection for enterprise-scale AI applications.

- **[Supercharging Amazon Bedrock Flows](https://community.aws/content/2phMjQ0bqWMg4PBwejBs1uf4YQE/supercharging-amazon-bedrock-flows-with-aws-agent-squad)**

  Learn how to enhance **Amazon Bedrock Flows** with conversation memory and multi-flow orchestration using the Agent Squad framework. This guide shows how to overcome Bedrock Flows&#039; limitations to build more sophisticated AI workflows with persistent memory and intelligent routing between flows.

### ğŸ™ï¸ Podcast Discussions

- **ğŸ‡«ğŸ‡· Podcast (French)**: L&#039;orchestrateur multi-agents : Un orchestrateur open source pour vos agents IA
  - **Platforms**:
    - [Apple Podcasts](https://podcasts.apple.com/be/podcast/lorchestrateur-multi-agents/id1452118442?i=1000684332612)
    - [Spotify](https://open.spotify.com/episode/4RdMazSRhZUyW2pniG91Vf)


- **ğŸ‡¬ğŸ‡§ Podcast (English)**: An Orchestrator for Your AI Agents
  - **Platforms**:
    - [Apple Podcasts](https://podcasts.apple.com/us/podcast/an-orchestrator-for-your-ai-agents/id1574162669?i=1000677039579)
    - [Spotify](https://open.spotify.com/episode/2a9DBGZn2lVqVMBLWGipHU)


### TypeScript Version

#### Installation

&gt; ğŸ”„ `multi-agent-orchestrator` becomes `agent-squad`

```bash
npm install agent-squad
```

#### Usage

The following example demonstrates how to use the Agent Squad with two different types of agents: a Bedrock LLM Agent with Converse API support and a Lex Bot Agent. This showcases the flexibility of the system in integrating various AI services.

```typescript
import { AgentSquad, BedrockLLMAgent, LexBotAgent } from &quot;agent-squad&quot;;

const orchestrator = new AgentSquad();

// Add a Bedrock LLM Agent with Converse API support
orchestrator.addAgent(
  new BedrockLLMAgent({
      name: &quot;Tech Agent&quot;,
      description:
        &quot;Specializes in technology areas including software development, hardware, AI, cybersecurity, blockchain, cloud computing, emerging tech innovations, and pricing/costs related to technology products and services.&quot;,
      streaming: true
  })
);

// Add a Lex Bot Agent for handling travel-related queries
orchestrator.addAgent(
  new LexBotAgent({
    name: &quot;Travel Agent&quot;,
    description: &quot;Helps users book and manage their flight reservations&quot;,
    botId: process.env.LEX_BOT_ID,
    botAliasId: process.env.LEX_BOT_ALIAS_ID,
    localeId: &quot;en_US&quot;,
  })
);

// Example usage
const response = await orchestrator.routeRequest(
  &quot;I want to book a flight&quot;,
  &#039;user123&#039;,
  &#039;session456&#039;
);

// Handle the response (streaming or non-streaming)
if (response.streaming == true) {
    console.log(&quot;\n** RESPONSE STREAMING ** \n&quot;);
    // Send metadata immediately
    console.log(`&gt; Agent ID: ${response.metadata.agentId}`);
    console.log(`&gt; Agent Name: ${response.metadata.agentName}`);
    console.log(`&gt; User Input: ${response.metadata.userInput}`);
    console.log(`&gt; User ID: ${response.metadata.userId}`);
    console.log(`&gt; Session ID: ${response.metadata.sessionId}`);
    console.log(
      `&gt; Additional Parameters:`,
      response.metadata.additionalParams
    );
    console.log(`\n&gt; Response: `);

    // Stream the content
    for await (const chunk of response.output) {
      if (typeof chunk === &quot;string&quot;) {
        process.stdout.write(chunk);
      } else {
        console.error(&quot;Received unexpected chunk type:&quot;, typeof chunk);
      }
    }

} else {
    // Handle non-streaming response (AgentProcessingResult)
    console.log(&quot;\n** RESPONSE ** \n&quot;);
    console.log(`&gt; Agent ID: ${response.metadata.agentId}`);
    console.log(`&gt; Agent Name: ${response.metadata.agentName}`);
    console.log(`&gt; User Input: ${response.metadata.userInput}`);
    console.log(`&gt; User ID: ${response.metadata.userId}`);
    console.log(`&gt; Session ID: ${response.metadata.sessionId}`);
    console.log(
      `&gt; Additional Parameters:`,
      response.metadata.additionalParams
    );
    console.log(`\n&gt; Response: ${response.output}`);
}
```

### Python Version

&gt; ğŸ”„ `multi-agent-orchestrator` becomes `agent-squad`

```bash
# Optional: Set up a virtual environment
python -m venv venv
source venv/bin/activate  # On Windows use `venv\Scripts\activate`
pip install agent-squad[aws]
```

#### Default Usage

Here&#039;s an equivalent Python example demonstrating the use of the Agent Squad with a Bedrock LLM Agent and a Lex Bot Agent:

```python
import sys
import asyncio
from agent_squad.orchestrator import AgentSquad
from agent_squad.agents import BedrockLLMAgent, BedrockLLMAgentOptions, AgentStreamResponse

orchestrator = AgentSquad()

tech_agent = BedrockLLMAgent(BedrockLLMAgentOptions(
  name=&quot;Tech Agent&quot;,
  streaming=True,
  description=&quot;Specializes in technology areas including software development, hardware, AI, \
  cybersecurity, blockchain, cloud computing, emerging tech innovations, and pricing/costs \
  related to technology products and services.&quot;,
  model_id=&quot;anthropic.claude-3-sonnet-20240229-v1:0&quot;,
))
orchestrator.add_agent(tech_agent)


health_agent = BedrockLLMAgent(BedrockLLMAgentOptions(
  name=&quot;Health Agent&quot;,
  streaming=True,
  description=&quot;Specializes in health and well being&quot;,
))
orchestrator.add_agent(health_agent)

async def main():
    # Example usage
    response = await orchestrator.route_request(
        &quot;What is AWS Lambda?&quot;,
        &#039;user123&#039;,
        &#039;session456&#039;,
        {},
        True
    )

    # Handle the response (streaming or non-streaming)
    if response.streaming:
        print(&quot;\n** RESPONSE STREAMING ** \n&quot;)
        # Send metadata immediately
        print(f&quot;&gt; Agent ID: {response.metadata.agent_id}&quot;)
        print(f&quot;&gt; Agent Name: {response.metadata.agent_name}&quot;)
        print(f&quot;&gt; User Input: {response.metadata.user_input}&quot;)
        print(f&quot;&gt; User ID: {response.metadata.user_id}&quot;)
        print(f&quot;&gt; Session ID: {response.metadata.session_id}&quot;)
        print(f&quot;&gt; Additional Parameters: {response.metadata.additional_params}&quot;)
        print(&quot;\n&gt; Response: &quot;)

        # Stream the content
        async for chunk in response.output:
            async for chunk in response.output:
              if isinstance(chunk, AgentStreamResponse):
                  print(chunk.text, end=&#039;&#039;, flush=True)
              else:
                  print(f&quot;Received unexpected chunk type: {type(chunk)}&quot;, file=sys.stderr)

    else:
        # Handle non-streaming response (AgentProcessingResult)
        print(&quot;\n** RESPONSE ** \n&quot;)
        print(f&quot;&gt; Agent ID: {response.metadata.agent_id}&quot;)
        print(f&quot;&gt; Agent Name: {response.metadata.agent_name}&quot;)
        print(f&quot;&gt; User Input: {response.metadata.user_input}&quot;)
        print(f&quot;&gt; User ID: {response.metadata.user_id}&quot;)
        print(f&quot;&gt; Session ID: {response.metadata.session_id}&quot;)
        print(f&quot;&gt; Additional Parameters: {response.metadata.additional_params}&quot;)
        print(f&quot;\n&gt; Response: {response.output.content}&quot;)

if __name__ == &quot;__main__&quot;:
  asyncio.run(main())
```

These examples showcase:
1. The use of a Bedrock LLM Agent with Converse API support, allowing for multi-turn conversations.
2. Integration of a Lex Bot Agent for specialized tasks (in this case, travel-related queries).
3. The orchestrator&#039;s ability to route requests to the most appropriate agent based on the input.
4. Handling of both streaming and non-streaming responses from different types of agents.


### Modular Installation Options

The Agent Squad is designed with a modular architecture, allowing you to install only the components you need while ensuring you always get the core functionality.

#### Installation Options

**1. AWS Integration**:

  ```bash
   pip install &quot;agent-squad[aws]&quot;
  ```
Includes core orchestration functionality with comprehensive AWS service integrations (`BedrockLLMAgent`, `AmazonBedrockAgent`, `LambdaAgent`, etc.)

**2. Anthropic Integration**:

  ```bash
pip install &quot;agent-squad[anthropic]&quot;
  ```

**3. OpenAI Integration**:

  ```bash
pip install &quot;agent-squad[openai]&quot;
  ```

Adds OpenAI&#039;s GPT models for agents and classification, along with core packages.

**4. Full Installation**:

  ```bash
pip install &quot;agent-squad[all]&quot;
  ```

Includes all optional dependencies for maximum flexibility.


### ğŸ™Œ **We Want to Hear From You!**

Have something to share, discuss, or brainstorm? Weâ€™d love to connect with you and hear about your journey with the **Agent Squad framework**. Hereâ€™s how you can get involved:

- **ğŸ™Œ Show &amp; Tell**: Got a success story, cool project, or creative implementation? Share it with us in the [**Show and Tell**](https://github.com/awslabs/agent-squad/discussions/categories/show-and-tell) section. Your work might inspire the entire community! ğŸ‰

- **ğŸ’¬ General Discussion**: Have questions, feedback, or suggestions? Join the conversation in our [**General Discussions**](https://github.com/awslabs/agent-squad/discussions/categories/general) section. Itâ€™s the perfect place to c

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[521xueweihan/HelloGitHub]]></title>
            <link>https://github.com/521xueweihan/HelloGitHub</link>
            <guid>https://github.com/521xueweihan/HelloGitHub</guid>
            <pubDate>Fri, 09 May 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[åˆ†äº« GitHub ä¸Šæœ‰è¶£ã€å…¥é—¨çº§çš„å¼€æºé¡¹ç›®ã€‚Share interesting, entry-level open source projects on GitHub.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/521xueweihan/HelloGitHub">521xueweihan/HelloGitHub</a></h1>
            <p>åˆ†äº« GitHub ä¸Šæœ‰è¶£ã€å…¥é—¨çº§çš„å¼€æºé¡¹ç›®ã€‚Share interesting, entry-level open source projects on GitHub.</p>
            <p>Language: Python</p>
            <p>Stars: 106,861</p>
            <p>Forks: 9,969</p>
            <p>Stars today: 735 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/readme.gif&quot;/&gt;
  &lt;br&gt;ä¸­æ–‡ | &lt;a href=&quot;README_en.md&quot;&gt;English&lt;/a&gt; | &lt;a href=&quot;README_ja.md&quot;&gt;æ—¥æœ¬èª&lt;/a&gt;
  &lt;br&gt;åˆ†äº« GitHub ä¸Šæœ‰è¶£ã€å…¥é—¨çº§çš„å¼€æºé¡¹ç›®ã€‚
  &lt;br&gt;å…´è¶£æ˜¯æœ€å¥½çš„è€å¸ˆï¼ŒHelloGitHub å¸®ä½ æ‰¾åˆ°å¼€æºçš„ä¹è¶£ï¼
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://hellogithub.com/repository/d4aae58ddbf34f0799bf3e8f965e0d70&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=d4aae58ddbf34f0799bf3e8f965e0d70&amp;claim_uid=8MKvZoxaWt&quot; alt=&quot;Featuredï½œHelloGitHub&quot; style=&quot;width: 250px; height: 54px;&quot; width=&quot;250&quot; height=&quot;54&quot; /&gt;&lt;/a&gt;&lt;br&gt;
  &lt;a href=&quot;https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/weixin.png&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Talk-%E5%BE%AE%E4%BF%A1%E7%BE%A4-brightgreen.svg?style=popout-square&quot; alt=&quot;WeiXin&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/521xueweihan/HelloGitHub/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/521xueweihan/HelloGitHub.svg?style=popout-square&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/521xueweihan/HelloGitHub/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/521xueweihan/HelloGitHub.svg?style=popout-square&quot; alt=&quot;GitHub issues&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://weibo.com/hellogithub&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%E6%96%B0%E6%B5%AA-Weibo-red.svg?style=popout-square&quot; alt=&quot;Sina Weibo&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

## ç®€ä»‹

HelloGitHub åˆ†äº« GitHub ä¸Šæœ‰è¶£ã€å…¥é—¨çº§çš„å¼€æºé¡¹ç›®ã€‚**æ¯æœˆ 28 å·**ä»¥æœˆåˆŠçš„å½¢å¼[æ›´æ–°å‘å¸ƒ](https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzA5MzYyNzQ0MQ==&amp;action=getalbum&amp;album_id=1331197538447310849#wechat_redirect)ï¼Œå†…å®¹åŒ…æ‹¬ï¼š**æœ‰è¶£ã€å…¥é—¨çº§çš„å¼€æºé¡¹ç›®**ã€**å¼€æºä¹¦ç±**ã€**å®æˆ˜é¡¹ç›®**ã€**ä¼ä¸šçº§é¡¹ç›®**ç­‰ï¼Œè®©ä½ ç”¨å¾ˆçŸ­æ—¶é—´æ„Ÿå—åˆ°å¼€æºçš„é­…åŠ›ï¼Œçˆ±ä¸Šå¼€æºï¼

## å†…å®¹
è·å¾—æ›´å¥½çš„é˜…è¯»ä½“éªŒ [å®˜ç½‘](https://hellogithub.com/) æˆ– [HelloGitHub å…¬ä¼—å·](https://cdn.jsdelivr.net/gh/521xueweihan/img_logo@main/logo/weixin.png)

| :card_index: | :jack_o_lantern: | :beer: | :fish_cake: | :octocat: |
| ------- | ----- | ------------ | ------ | --------- |
| [ç¬¬ 109 æœŸ](/content/HelloGitHub109.md) | [ç¬¬ 108 æœŸ](/content/HelloGitHub108.md) | [ç¬¬ 107 æœŸ](/content/HelloGitHub107.md) | [ç¬¬ 106 æœŸ](/content/HelloGitHub106.md) |
| [ç¬¬ 105 æœŸ](/content/HelloGitHub105.md) | [ç¬¬ 104 æœŸ](/content/HelloGitHub104.md) | [ç¬¬ 103 æœŸ](/content/HelloGitHub103.md) | [ç¬¬ 102 æœŸ](/content/HelloGitHub102.md) | [ç¬¬ 101 æœŸ](/content/HelloGitHub101.md) |
| [ç¬¬ 100 æœŸ](/content/HelloGitHub100.md) | [ç¬¬ 99 æœŸ](/content/HelloGitHub99.md) | [ç¬¬ 98 æœŸ](/content/HelloGitHub98.md) | [ç¬¬ 97 æœŸ](/content/HelloGitHub97.md) | [ç¬¬ 96 æœŸ](/content/HelloGitHub96.md) |
| [ç¬¬ 95 æœŸ](/content/HelloGitHub95.md) | [ç¬¬ 94 æœŸ](/content/HelloGitHub94.md) | [ç¬¬ 93 æœŸ](/content/HelloGitHub93.md) | [ç¬¬ 92 æœŸ](/content/HelloGitHub92.md) | [ç¬¬ 91 æœŸ](/content/HelloGitHub91.md) |
| [ç¬¬ 90 æœŸ](/content/HelloGitHub90.md) | [ç¬¬ 89 æœŸ](/content/HelloGitHub89.md) | [ç¬¬ 88 æœŸ](/content/HelloGitHub88.md) | [ç¬¬ 87 æœŸ](/content/HelloGitHub87.md) | [ç¬¬ 86 æœŸ](/content/HelloGitHub86.md) |
| [ç¬¬ 85 æœŸ](/content/HelloGitHub85.md) | [ç¬¬ 84 æœŸ](/content/HelloGitHub84.md) | [ç¬¬ 83 æœŸ](/content/HelloGitHub83.md) | [ç¬¬ 82 æœŸ](/content/HelloGitHub82.md) | [ç¬¬ 81 æœŸ](/content/HelloGitHub81.md) |
| [ç¬¬ 80 æœŸ](/content/HelloGitHub80.md) | [ç¬¬ 79 æœŸ](/content/HelloGitHub79.md) | [ç¬¬ 78 æœŸ](/content/HelloGitHub78.md) | [ç¬¬ 77 æœŸ](/content/HelloGitHub77.md) | [ç¬¬ 76 æœŸ](/content/HelloGitHub76.md) |
| [ç¬¬ 75 æœŸ](/content/HelloGitHub75.md) | [ç¬¬ 74 æœŸ](/content/HelloGitHub74.md) | [ç¬¬ 73 æœŸ](/content/HelloGitHub73.md) | [ç¬¬ 72 æœŸ](/content/HelloGitHub72.md) | [ç¬¬ 71 æœŸ](/content/HelloGitHub71.md) |


æ¬¢è¿[æ¨èæˆ–è‡ªè](https://hellogithub.com/periodical)é¡¹ç›®æˆä¸º **HelloGitHub** çš„[è´¡çŒ®è€…](https://github.com/521xueweihan/HelloGitHub/blob/master/content/contributors.md)

## èµåŠ©


&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th align=&quot;center&quot; style=&quot;width: 80px;&quot;&gt;
        &lt;a href=&quot;https://www.compshare.cn/?utm_term=logo&amp;utm_campaign=hellogithub&amp;utm_source=otherdsp&amp;utm_medium=display&amp;ytag=logo_hellogithub_otherdsp_display&quot;&gt;          &lt;img src=&quot;https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/ucloud.png&quot; width=&quot;60px&quot;&gt;&lt;br&gt;
          &lt;sub&gt;UCloud&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;è¶…å€¼çš„GPUäº‘æœåŠ¡&lt;/sub&gt;
        &lt;/a&gt;
      &lt;/th&gt;
      &lt;th align=&quot;center&quot; style=&quot;width: 80px;&quot;&gt;
        &lt;a href=&quot;https://www.upyun.com/?from=hellogithub&quot;&gt;
          &lt;img src=&quot;https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/upyun.png&quot; width=&quot;60px&quot;&gt;&lt;br&gt;
          &lt;sub&gt;CDN&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;å¼€å¯å…¨ç½‘åŠ é€Ÿ&lt;/sub&gt;
        &lt;/a&gt;
      &lt;/th&gt;
      &lt;th align=&quot;center&quot; style=&quot;width: 80px;&quot;&gt;
        &lt;a href=&quot;https://github.com/OpenIMSDK/Open-IM-Server&quot;&gt;
          &lt;img src=&quot;https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/im.png&quot; width=&quot;60px&quot;&gt;&lt;br&gt;
          &lt;sub&gt;OpenIM&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;å¼€æºIMåŠ›äº‰No.1&lt;/sub&gt;
        &lt;/a&gt;
      &lt;/th&gt;
      &lt;th align=&quot;center&quot; style=&quot;width: 80px;&quot;&gt;
        &lt;a href=&quot;https://apifox.cn/a103hello&quot;&gt;
          &lt;img src=&quot;https://raw.githubusercontent.com/521xueweihan/img_logo/master/logo/apifox.png&quot; width=&quot;60px&quot;&gt;&lt;br&gt;
          &lt;sub&gt;Apifox&lt;/sub&gt;&lt;br&gt;
          &lt;sub&gt;æ¯” Postman æ›´å¼ºå¤§&lt;/sub&gt;
        &lt;/a&gt;
      &lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
&lt;/table&gt;


## å£°æ˜

&lt;a rel=&quot;license&quot; href=&quot;https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh&quot;&gt;&lt;img alt=&quot;çŸ¥è¯†å…±äº«è®¸å¯åè®®&quot; style=&quot;border-width: 0&quot; src=&quot;https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png&quot;&gt;&lt;/a&gt;&lt;br&gt;æœ¬ä½œå“é‡‡ç”¨ &lt;a rel=&quot;license&quot; href=&quot;https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh&quot;&gt;ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç¦æ­¢æ¼”ç» 4.0 å›½é™…&lt;/a&gt; è¿›è¡Œè®¸å¯ã€‚&lt;a href=&quot;mailto:595666367@qq.com&quot;&gt;è”ç³»æˆ‘&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Blaizzy/mlx-audio]]></title>
            <link>https://github.com/Blaizzy/mlx-audio</link>
            <guid>https://github.com/Blaizzy/mlx-audio</guid>
            <pubDate>Fri, 09 May 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[A text-to-speech (TTS), speech-to-text (STT) and speech-to-speech (STS) library built on Apple's MLX framework, providing efficient speech analysis on Apple Silicon.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Blaizzy/mlx-audio">Blaizzy/mlx-audio</a></h1>
            <p>A text-to-speech (TTS), speech-to-text (STT) and speech-to-speech (STS) library built on Apple's MLX framework, providing efficient speech analysis on Apple Silicon.</p>
            <p>Language: Python</p>
            <p>Stars: 1,054</p>
            <p>Forks: 80</p>
            <p>Stars today: 101 stars today</p>
            <h2>README</h2><pre># MLX-Audio

A text-to-speech (TTS) and Speech-to-Speech (STS) library built on Apple&#039;s MLX framework, providing efficient speech synthesis on Apple Silicon.

## Features

- Fast inference on Apple Silicon (M series chips)
- Multiple language support
- Voice customization options
- Adjustable speech speed control (0.5x to 2.0x)
- Interactive web interface with 3D audio visualization
- REST API for TTS generation
- Quantization support for optimized performance
- Direct access to output files via Finder/Explorer integration

## Installation

```bash
# Install the package
pip install mlx-audio

# For web interface and API dependencies
pip install -r requirements.txt
```

### Quick Start

To generate audio with an LLM use:

```bash
# Basic usage
mlx_audio.tts.generate --text &quot;Hello, world&quot;

# Specify prefix for output file
mlx_audio.tts.generate --text &quot;Hello, world&quot; --file_prefix hello

# Adjust speaking speed (0.5-2.0)
mlx_audio.tts.generate --text &quot;Hello, world&quot; --speed 1.4
```

### How to call from python

To generate audio with an LLM use:

```python
from mlx_audio.tts.generate import generate_audio

# Example: Generate an audiobook chapter as mp3 audio
generate_audio(
    text=(&quot;In the beginning, the universe was created...\n&quot;
        &quot;...or the simulation was booted up.&quot;),
    model_path=&quot;prince-canuma/Kokoro-82M&quot;,
    voice=&quot;af_heart&quot;,
    speed=1.2,
    lang_code=&quot;a&quot;, # Kokoro: (a)f_heart, or comment out for auto
    file_prefix=&quot;audiobook_chapter1&quot;,
    audio_format=&quot;wav&quot;,
    sample_rate=24000,
    join_audio=True,
    verbose=True  # Set to False to disable print messages
)

print(&quot;Audiobook chapter successfully generated!&quot;)

```

### Web Interface &amp; API Server

MLX-Audio includes a web interface with a 3D visualization that reacts to audio frequencies. The interface allows you to:

1. Generate TTS with different voices and speed settings
2. Upload and play your own audio files
3. Visualize audio with an interactive 3D orb
4. Automatically saves generated audio files to the outputs directory in the current working folder
5. Open the output folder directly from the interface (when running locally)

#### Features

- **Multiple Voice Options**: Choose from different voice styles (AF Heart, AF Nova, AF Bella, BF Emma)
- **Adjustable Speech Speed**: Control the speed of speech generation with an interactive slider (0.5x to 2.0x)
- **Real-time 3D Visualization**: A responsive 3D orb that reacts to audio frequencies
- **Audio Upload**: Play and visualize your own audio files
- **Auto-play Option**: Automatically play generated audio
- **Output Folder Access**: Convenient button to open the output folder in your system&#039;s file explorer

To start the web interface and API server:

```bash
# Using the command-line interface
mlx_audio.server

# With custom host and port
mlx_audio.server --host 0.0.0.0 --port 9000

# With verbose logging
mlx_audio.server --verbose
```

Available command line arguments:
- `--host`: Host address to bind the server to (default: 127.0.0.1)
- `--port`: Port to bind the server to (default: 8000)

Then open your browser and navigate to:
```
http://127.0.0.1:8000
```

#### API Endpoints

The server provides the following REST API endpoints:

- `POST /tts`: Generate TTS audio
  - Parameters (form data):
    - `text`: The text to convert to speech (required)
    - `voice`: Voice to use (default: &quot;af_heart&quot;)
    - `speed`: Speech speed from 0.5 to 2.0 (default: 1.0)
  - Returns: JSON with filename of generated audio

- `GET /audio/{filename}`: Retrieve generated audio file

- `POST /play`: Play audio directly from the server
  - Parameters (form data):
    - `filename`: The filename of the audio to play (required)
  - Returns: JSON with status and filename

- `POST /stop`: Stop any currently playing audio
  - Returns: JSON with status

- `POST /open_output_folder`: Open the output folder in the system&#039;s file explorer
  - Returns: JSON with status and path
  - Note: This feature only works when running the server locally

&gt; Note: Generated audio files are stored in `~/.mlx_audio/outputs` by default, or in a fallback directory if that location is not writable.

## Models

### Kokoro

Kokoro is a multilingual TTS model that supports various languages and voice styles.

#### Example Usage

```python
from mlx_audio.tts.models.kokoro import KokoroPipeline
from mlx_audio.tts.utils import load_model
from IPython.display import Audio
import soundfile as sf

# Initialize the model
model_id = &#039;prince-canuma/Kokoro-82M&#039;
model = load_model(model_id)

# Create a pipeline with American English
pipeline = KokoroPipeline(lang_code=&#039;a&#039;, model=model, repo_id=model_id)

# Generate audio
text = &quot;The MLX King lives. Let him cook!&quot;
for _, _, audio in pipeline(text, voice=&#039;af_heart&#039;, speed=1, split_pattern=r&#039;\n+&#039;):
    # Display audio in notebook (if applicable)
    display(Audio(data=audio, rate=24000, autoplay=0))

    # Save audio to file
    sf.write(&#039;audio.wav&#039;, audio[0], 24000)
```

#### Language Options

- ğŸ‡ºğŸ‡¸ `&#039;a&#039;` - American English
- ğŸ‡¬ğŸ‡§ `&#039;b&#039;` - British English
- ğŸ‡¯ğŸ‡µ `&#039;j&#039;` - Japanese (requires `pip install misaki[ja]`)
- ğŸ‡¨ğŸ‡³ `&#039;z&#039;` - Mandarin Chinese (requires `pip install misaki[zh]`)

### CSM (Conversational Speech Model)

CSM is a model from Sesame that allows you text-to-speech and to customize voices using reference audio samples.

#### Example Usage

```bash
# Generate speech using CSM-1B model with reference audio
python -m mlx_audio.tts.generate --model mlx-community/csm-1b --text &quot;Hello from Sesame.&quot; --play --ref_audio ./conversational_a.wav
```

You can pass any audio to clone the voice from or download sample audio file from [here](https://huggingface.co/mlx-community/csm-1b/tree/main/prompts).

## Advanced Features

### Quantization

You can quantize models for improved performance:

```python
from mlx_audio.tts.utils import quantize_model, load_model
import json
import mlx.core as mx

model = load_model(repo_id=&#039;prince-canuma/Kokoro-82M&#039;)
config = model.config

# Quantize to 8-bit
group_size = 64
bits = 8
weights, config = quantize_model(model, config, group_size, bits)

# Save quantized model
with open(&#039;./8bit/config.json&#039;, &#039;w&#039;) as f:
    json.dump(config, f)

mx.save_safetensors(&quot;./8bit/kokoro-v1_0.safetensors&quot;, weights, metadata={&quot;format&quot;: &quot;mlx&quot;})
```

## Requirements

- MLX
- Python 3.8+
- Apple Silicon Mac (for optimal performance)
- For the web interface and API:
  - FastAPI
  - Uvicorn
  
## License

[MIT License](LICENSE)

## Acknowledgements

- Thanks to the Apple MLX team for providing a great framework for building TTS and STS models.
- This project uses the Kokoro model architecture for text-to-speech synthesis.
- The 3D visualization uses Three.js for rendering.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[harry0703/MoneyPrinterTurbo]]></title>
            <link>https://github.com/harry0703/MoneyPrinterTurbo</link>
            <guid>https://github.com/harry0703/MoneyPrinterTurbo</guid>
            <pubDate>Fri, 09 May 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[åˆ©ç”¨AIå¤§æ¨¡å‹ï¼Œä¸€é”®ç”Ÿæˆé«˜æ¸…çŸ­è§†é¢‘ Generate short videos with one click using AI LLM.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/harry0703/MoneyPrinterTurbo">harry0703/MoneyPrinterTurbo</a></h1>
            <p>åˆ©ç”¨AIå¤§æ¨¡å‹ï¼Œä¸€é”®ç”Ÿæˆé«˜æ¸…çŸ­è§†é¢‘ Generate short videos with one click using AI LLM.</p>
            <p>Language: Python</p>
            <p>Stars: 27,652</p>
            <p>Forks: 4,085</p>
            <p>Stars today: 610 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;h1 align=&quot;center&quot;&gt;MoneyPrinterTurbo ğŸ’¸&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&quot; alt=&quot;Stargazers&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&quot; alt=&quot;Issues&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/network/members&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&quot; alt=&quot;Forks&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&quot; alt=&quot;License&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;ç®€ä½“ä¸­æ–‡ | &lt;a href=&quot;README-en.md&quot;&gt;English&lt;/a&gt;&lt;/h3&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/8731&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/8731&quot; alt=&quot;harry0703%2FMoneyPrinterTurbo | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;br&gt;
åªéœ€æä¾›ä¸€ä¸ªè§†é¢‘ &lt;b&gt;ä¸»é¢˜&lt;/b&gt; æˆ– &lt;b&gt;å…³é”®è¯&lt;/b&gt; ï¼Œå°±å¯ä»¥å…¨è‡ªåŠ¨ç”Ÿæˆè§†é¢‘æ–‡æ¡ˆã€è§†é¢‘ç´ æã€è§†é¢‘å­—å¹•ã€è§†é¢‘èƒŒæ™¯éŸ³ä¹ï¼Œç„¶ååˆæˆä¸€ä¸ªé«˜æ¸…çš„çŸ­è§†é¢‘ã€‚
&lt;br&gt;

&lt;h4&gt;Webç•Œé¢&lt;/h4&gt;

![](docs/webui.jpg)

&lt;h4&gt;APIç•Œé¢&lt;/h4&gt;

![](docs/api.jpg)

&lt;/div&gt;

## ç‰¹åˆ«æ„Ÿè°¢ ğŸ™

ç”±äºè¯¥é¡¹ç›®çš„ **éƒ¨ç½²** å’Œ **ä½¿ç”¨**ï¼Œå¯¹äºä¸€äº›å°ç™½ç”¨æˆ·æ¥è¯´ï¼Œè¿˜æ˜¯ **æœ‰ä¸€å®šçš„é—¨æ§›**ï¼Œåœ¨æ­¤ç‰¹åˆ«æ„Ÿè°¢
**å½•å’–ï¼ˆAIæ™ºèƒ½ å¤šåª’ä½“æœåŠ¡å¹³å°ï¼‰** ç½‘ç«™åŸºäºè¯¥é¡¹ç›®ï¼Œæä¾›çš„å…è´¹`AIè§†é¢‘ç”Ÿæˆå™¨`æœåŠ¡ï¼Œå¯ä»¥ä¸ç”¨éƒ¨ç½²ï¼Œç›´æ¥åœ¨çº¿ä½¿ç”¨ï¼Œéå¸¸æ–¹ä¾¿ã€‚

- ä¸­æ–‡ç‰ˆï¼šhttps://reccloud.cn
- è‹±æ–‡ç‰ˆï¼šhttps://reccloud.com

![](docs/reccloud.cn.jpg)

## æ„Ÿè°¢èµåŠ© ğŸ™

æ„Ÿè°¢ä½ç³– https://picwish.cn å¯¹è¯¥é¡¹ç›®çš„æ”¯æŒå’ŒèµåŠ©ï¼Œä½¿å¾—è¯¥é¡¹ç›®èƒ½å¤ŸæŒç»­çš„æ›´æ–°å’Œç»´æŠ¤ã€‚

ä½ç³–ä¸“æ³¨äº**å›¾åƒå¤„ç†é¢†åŸŸ**ï¼Œæä¾›ä¸°å¯Œçš„**å›¾åƒå¤„ç†å·¥å…·**ï¼Œå°†å¤æ‚æ“ä½œæè‡´ç®€åŒ–ï¼ŒçœŸæ­£å®ç°è®©å›¾åƒå¤„ç†æ›´ç®€å•ã€‚

![picwish.jpg](docs/picwish.jpg)

## åŠŸèƒ½ç‰¹æ€§ ğŸ¯

- [x] å®Œæ•´çš„ **MVCæ¶æ„**ï¼Œä»£ç  **ç»“æ„æ¸…æ™°**ï¼Œæ˜“äºç»´æŠ¤ï¼Œæ”¯æŒ `API` å’Œ `Webç•Œé¢`
- [x] æ”¯æŒè§†é¢‘æ–‡æ¡ˆ **AIè‡ªåŠ¨ç”Ÿæˆ**ï¼Œä¹Ÿå¯ä»¥**è‡ªå®šä¹‰æ–‡æ¡ˆ**
- [x] æ”¯æŒå¤šç§ **é«˜æ¸…è§†é¢‘** å°ºå¯¸
    - [x] ç«–å± 9:16ï¼Œ`1080x1920`
    - [x] æ¨ªå± 16:9ï¼Œ`1920x1080`
- [x] æ”¯æŒ **æ‰¹é‡è§†é¢‘ç”Ÿæˆ**ï¼Œå¯ä»¥ä¸€æ¬¡ç”Ÿæˆå¤šä¸ªè§†é¢‘ï¼Œç„¶åé€‰æ‹©ä¸€ä¸ªæœ€æ»¡æ„çš„
- [x] æ”¯æŒ **è§†é¢‘ç‰‡æ®µæ—¶é•¿** è®¾ç½®ï¼Œæ–¹ä¾¿è°ƒèŠ‚ç´ æåˆ‡æ¢é¢‘ç‡
- [x] æ”¯æŒ **ä¸­æ–‡** å’Œ **è‹±æ–‡** è§†é¢‘æ–‡æ¡ˆ
- [x] æ”¯æŒ **å¤šç§è¯­éŸ³** åˆæˆï¼Œå¯ **å®æ—¶è¯•å¬** æ•ˆæœ
- [x] æ”¯æŒ **å­—å¹•ç”Ÿæˆ**ï¼Œå¯ä»¥è°ƒæ•´ `å­—ä½“`ã€`ä½ç½®`ã€`é¢œè‰²`ã€`å¤§å°`ï¼ŒåŒæ—¶æ”¯æŒ`å­—å¹•æè¾¹`è®¾ç½®
- [x] æ”¯æŒ **èƒŒæ™¯éŸ³ä¹**ï¼Œéšæœºæˆ–è€…æŒ‡å®šéŸ³ä¹æ–‡ä»¶ï¼Œå¯è®¾ç½®`èƒŒæ™¯éŸ³ä¹éŸ³é‡`
- [x] è§†é¢‘ç´ ææ¥æº **é«˜æ¸…**ï¼Œè€Œä¸” **æ— ç‰ˆæƒ**ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨è‡ªå·±çš„ **æœ¬åœ°ç´ æ**
- [x] æ”¯æŒ **OpenAI**ã€**Moonshot**ã€**Azure**ã€**gpt4free**ã€**one-api**ã€**é€šä¹‰åƒé—®**ã€**Google Gemini**ã€**Ollama**ã€
  **DeepSeek**ã€ **æ–‡å¿ƒä¸€è¨€** ç­‰å¤šç§æ¨¡å‹æ¥å…¥
    - ä¸­å›½ç”¨æˆ·å»ºè®®ä½¿ç”¨ **DeepSeek** æˆ– **Moonshot** ä½œä¸ºå¤§æ¨¡å‹æä¾›å•†ï¼ˆå›½å†…å¯ç›´æ¥è®¿é—®ï¼Œä¸éœ€è¦VPNã€‚æ³¨å†Œå°±é€é¢åº¦ï¼ŒåŸºæœ¬å¤Ÿç”¨ï¼‰

### åæœŸè®¡åˆ’ ğŸ“…

- [ ] GPT-SoVITS é…éŸ³æ”¯æŒ
- [ ] ä¼˜åŒ–è¯­éŸ³åˆæˆï¼Œåˆ©ç”¨å¤§æ¨¡å‹ï¼Œä½¿å…¶åˆæˆçš„å£°éŸ³ï¼Œæ›´åŠ è‡ªç„¶ï¼Œæƒ…ç»ªæ›´åŠ ä¸°å¯Œ
- [ ] å¢åŠ è§†é¢‘è½¬åœºæ•ˆæœï¼Œä½¿å…¶çœ‹èµ·æ¥æ›´åŠ çš„æµç•…
- [ ] å¢åŠ æ›´å¤šè§†é¢‘ç´ ææ¥æºï¼Œä¼˜åŒ–è§†é¢‘ç´ æå’Œæ–‡æ¡ˆçš„åŒ¹é…åº¦
- [ ] å¢åŠ è§†é¢‘é•¿åº¦é€‰é¡¹ï¼šçŸ­ã€ä¸­ã€é•¿
- [ ] æ”¯æŒæ›´å¤šçš„è¯­éŸ³åˆæˆæœåŠ¡å•†ï¼Œæ¯”å¦‚ OpenAI TTS
- [ ] è‡ªåŠ¨ä¸Šä¼ åˆ°YouTubeå¹³å°

## è§†é¢‘æ¼”ç¤º ğŸ“º

### ç«–å± 9:16

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;â–¶ï¸&lt;/g-emoji&gt; ã€Šå¦‚ä½•å¢åŠ ç”Ÿæ´»çš„ä¹è¶£ã€‹&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;â–¶ï¸&lt;/g-emoji&gt; ã€Šé‡‘é’±çš„ä½œç”¨ã€‹&lt;br&gt;æ›´çœŸå®çš„åˆæˆå£°éŸ³&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;â–¶ï¸&lt;/g-emoji&gt; ã€Šç”Ÿå‘½çš„æ„ä¹‰æ˜¯ä»€ä¹ˆã€‹&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/a84d33d5-27a2-4aba-8fd0-9fb2bd91c6a6&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/af2f3b0b-002e-49fe-b161-18ba91c055e8&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/112c9564-d52b-4472-99ad-970b75f66476&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

### æ¨ªå± 16:9

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;â–¶ï¸&lt;/g-emoji&gt;ã€Šç”Ÿå‘½çš„æ„ä¹‰æ˜¯ä»€ä¹ˆã€‹&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;â–¶ï¸&lt;/g-emoji&gt;ã€Šä¸ºä»€ä¹ˆè¦è¿åŠ¨ã€‹&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/346ebb15-c55f-47a9-a653-114f08bb8073&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/271f2fae-8283-44a0-8aa0-0ed8f9a6fa87&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

## é…ç½®è¦æ±‚ ğŸ“¦

- å»ºè®®æœ€ä½ CPU 4æ ¸æˆ–ä»¥ä¸Šï¼Œå†…å­˜ 8G æˆ–ä»¥ä¸Šï¼Œæ˜¾å¡éå¿…é¡»
- Windows 10 æˆ– MacOS 11.0 ä»¥ä¸Šç³»ç»Ÿ

## å¿«é€Ÿå¼€å§‹ ğŸš€

ä¸‹è½½ä¸€é”®å¯åŠ¨åŒ…ï¼Œè§£å‹ç›´æ¥ä½¿ç”¨ï¼ˆè·¯å¾„ä¸è¦æœ‰ **ä¸­æ–‡**ã€**ç‰¹æ®Šå­—ç¬¦**ã€**ç©ºæ ¼**ï¼‰

### Windows
- ç™¾åº¦ç½‘ç›˜ï¼ˆ1.2.1 è€ç‰ˆæœ¬ï¼‰: https://pan.baidu.com/s/1pSNjxTYiVENulTLm6zieMQ?pwd=g36q æå–ç : g36q

ä¸‹è½½åï¼Œå»ºè®®å…ˆ**åŒå‡»æ‰§è¡Œ** `update.bat` æ›´æ–°åˆ°**æœ€æ–°ä»£ç **ï¼Œç„¶ååŒå‡» `start.bat` å¯åŠ¨

å¯åŠ¨åï¼Œä¼šè‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼ˆå¦‚æœæ‰“å¼€æ˜¯ç©ºç™½ï¼Œå»ºè®®æ¢æˆ **Chrome** æˆ–è€… **Edge** æ‰“å¼€ï¼‰

## å®‰è£…éƒ¨ç½² ğŸ“¥

### å‰ææ¡ä»¶

- å°½é‡ä¸è¦ä½¿ç”¨ **ä¸­æ–‡è·¯å¾„**ï¼Œé¿å…å‡ºç°ä¸€äº›æ— æ³•é¢„æ–™çš„é—®é¢˜
- è¯·ç¡®ä¿ä½ çš„ **ç½‘ç»œ** æ˜¯æ­£å¸¸çš„ï¼ŒVPNéœ€è¦æ‰“å¼€`å…¨å±€æµé‡`æ¨¡å¼

#### â‘  å…‹éš†ä»£ç 

```shell
git clone https://github.com/harry0703/MoneyPrinterTurbo.git
```

#### â‘¡ ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œå»ºè®®å¯åŠ¨åä¹Ÿå¯ä»¥åœ¨ WebUI é‡Œé¢é…ç½®ï¼‰

- å°† `config.example.toml` æ–‡ä»¶å¤åˆ¶ä¸€ä»½ï¼Œå‘½åä¸º `config.toml`
- æŒ‰ç…§ `config.toml` æ–‡ä»¶ä¸­çš„è¯´æ˜ï¼Œé…ç½®å¥½ `pexels_api_keys` å’Œ `llm_provider`ï¼Œå¹¶æ ¹æ® llm_provider å¯¹åº”çš„æœåŠ¡å•†ï¼Œé…ç½®ç›¸å…³çš„
  API Key

### Dockeréƒ¨ç½² ğŸ³

#### â‘  å¯åŠ¨Docker

å¦‚æœæœªå®‰è£… Dockerï¼Œè¯·å…ˆå®‰è£… https://www.docker.com/products/docker-desktop/

å¦‚æœæ˜¯Windowsç³»ç»Ÿï¼Œè¯·å‚è€ƒå¾®è½¯çš„æ–‡æ¡£ï¼š

1. https://learn.microsoft.com/zh-cn/windows/wsl/install
2. https://learn.microsoft.com/zh-cn/windows/wsl/tutorials/wsl-containers

```shell
cd MoneyPrinterTurbo
docker-compose up
```

&gt; æ³¨æ„ï¼šæœ€æ–°ç‰ˆçš„dockerå®‰è£…æ—¶ä¼šè‡ªåŠ¨ä»¥æ’ä»¶çš„å½¢å¼å®‰è£…docker composeï¼Œå¯åŠ¨å‘½ä»¤è°ƒæ•´ä¸ºdocker compose up

#### â‘¡ è®¿é—®Webç•Œé¢

æ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—® http://0.0.0.0:8501

#### â‘¢ è®¿é—®APIæ–‡æ¡£

æ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—® http://0.0.0.0:8080/docs æˆ–è€… http://0.0.0.0:8080/redoc

### æ‰‹åŠ¨éƒ¨ç½² ğŸ“¦

&gt; è§†é¢‘æ•™ç¨‹

- å®Œæ•´çš„ä½¿ç”¨æ¼”ç¤ºï¼šhttps://v.douyin.com/iFhnwsKY/
- å¦‚ä½•åœ¨Windowsä¸Šéƒ¨ç½²ï¼šhttps://v.douyin.com/iFyjoW3M

#### â‘  ä¾èµ–å®‰è£…

å»ºè®®ä½¿ç”¨ [pdm](https://pdm-project.org/en/latest/#installation)

```shell
git clone https://github.com/harry0703/MoneyPrinterTurbo.git
cd MoneyPrinterTurbo
pdm sync
```

#### â‘¡ å®‰è£…å¥½ ImageMagick

- Windows:
    - ä¸‹è½½ https://imagemagick.org/script/download.php é€‰æ‹©Windowsç‰ˆæœ¬ï¼Œåˆ‡è®°ä¸€å®šè¦é€‰æ‹© **é™æ€åº“** ç‰ˆæœ¬ï¼Œæ¯”å¦‚
      ImageMagick-7.1.1-32-Q16-x64-**static**.exe
    - å®‰è£…ä¸‹è½½å¥½çš„ ImageMagickï¼Œ**æ³¨æ„ä¸è¦ä¿®æ”¹å®‰è£…è·¯å¾„**
    - ä¿®æ”¹ `é…ç½®æ–‡ä»¶ config.toml` ä¸­çš„ `imagemagick_path` ä¸ºä½ çš„ **å®é™…å®‰è£…è·¯å¾„**

- MacOS:
  ```shell
  brew install imagemagick
  ````
- Ubuntu
  ```shell
  sudo apt-get install imagemagick
  ```
- CentOS
  ```shell
  sudo yum install ImageMagick
  ```

#### â‘¢ å¯åŠ¨Webç•Œé¢ ğŸŒ

æ³¨æ„éœ€è¦åˆ° MoneyPrinterTurbo é¡¹ç›® `æ ¹ç›®å½•` ä¸‹æ‰§è¡Œä»¥ä¸‹å‘½ä»¤

###### Windows

```bat
webui.bat
```

###### MacOS or Linux

```shell
sh webui.sh
```

å¯åŠ¨åï¼Œä¼šè‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼ˆå¦‚æœæ‰“å¼€æ˜¯ç©ºç™½ï¼Œå»ºè®®æ¢æˆ **Chrome** æˆ–è€… **Edge** æ‰“å¼€ï¼‰

#### â‘£ å¯åŠ¨APIæœåŠ¡ ğŸš€

```shell
python main.py
```

å¯åŠ¨åï¼Œå¯ä»¥æŸ¥çœ‹ `APIæ–‡æ¡£` http://127.0.0.1:8080/docs æˆ–è€… http://127.0.0.1:8080/redoc ç›´æ¥åœ¨çº¿è°ƒè¯•æ¥å£ï¼Œå¿«é€Ÿä½“éªŒã€‚

## è¯­éŸ³åˆæˆ ğŸ—£

æ‰€æœ‰æ”¯æŒçš„å£°éŸ³åˆ—è¡¨ï¼Œå¯ä»¥æŸ¥çœ‹ï¼š[å£°éŸ³åˆ—è¡¨](./docs/voice-list.txt)

2024-04-16 v1.1.2 æ–°å¢äº†9ç§Azureçš„è¯­éŸ³åˆæˆå£°éŸ³ï¼Œéœ€è¦é…ç½®API KEYï¼Œè¯¥å£°éŸ³åˆæˆçš„æ›´åŠ çœŸå®ã€‚

## å­—å¹•ç”Ÿæˆ ğŸ“œ

å½“å‰æ”¯æŒ2ç§å­—å¹•ç”Ÿæˆæ–¹å¼ï¼š

- **edge**: ç”Ÿæˆ`é€Ÿåº¦å¿«`ï¼Œæ€§èƒ½æ›´å¥½ï¼Œå¯¹ç”µè„‘é…ç½®æ²¡æœ‰è¦æ±‚ï¼Œä½†æ˜¯è´¨é‡å¯èƒ½ä¸ç¨³å®š
- **whisper**: ç”Ÿæˆ`é€Ÿåº¦æ…¢`ï¼Œæ€§èƒ½è¾ƒå·®ï¼Œå¯¹ç”µè„‘é…ç½®æœ‰ä¸€å®šè¦æ±‚ï¼Œä½†æ˜¯`è´¨é‡æ›´å¯é `ã€‚

å¯ä»¥ä¿®æ”¹ `config.toml` é…ç½®æ–‡ä»¶ä¸­çš„ `subtitle_provider` è¿›è¡Œåˆ‡æ¢

å»ºè®®ä½¿ç”¨ `edge` æ¨¡å¼ï¼Œå¦‚æœç”Ÿæˆçš„å­—å¹•è´¨é‡ä¸å¥½ï¼Œå†åˆ‡æ¢åˆ° `whisper` æ¨¡å¼

&gt; æ³¨æ„ï¼š

1. whisper æ¨¡å¼ä¸‹éœ€è¦åˆ° HuggingFace ä¸‹è½½ä¸€ä¸ªæ¨¡å‹æ–‡ä»¶ï¼Œå¤§çº¦ 3GB å·¦å³ï¼Œè¯·ç¡®ä¿ç½‘ç»œé€šç•…
2. å¦‚æœç•™ç©ºï¼Œè¡¨ç¤ºä¸ç”Ÿæˆå­—å¹•ã€‚

&gt; ç”±äºå›½å†…æ— æ³•è®¿é—® HuggingFaceï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ä¸‹è½½ `whisper-large-v3` çš„æ¨¡å‹æ–‡ä»¶

ä¸‹è½½åœ°å€ï¼š

- ç™¾åº¦ç½‘ç›˜: https://pan.baidu.com/s/11h3Q6tsDtjQKTjUu3sc5cA?pwd=xjs9
- å¤¸å…‹ç½‘ç›˜ï¼šhttps://pan.quark.cn/s/3ee3d991d64b

æ¨¡å‹ä¸‹è½½åè§£å‹ï¼Œæ•´ä¸ªç›®å½•æ”¾åˆ° `.\MoneyPrinterTurbo\models` é‡Œé¢ï¼Œ
æœ€ç»ˆçš„æ–‡ä»¶è·¯å¾„åº”è¯¥æ˜¯è¿™æ ·: `.\MoneyPrinterTurbo\models\whisper-large-v3`

```
MoneyPrinterTurbo  
  â”œâ”€models
  â”‚   â””â”€whisper-large-v3
  â”‚          config.json
  â”‚          model.bin
  â”‚          preprocessor_config.json
  â”‚          tokenizer.json
  â”‚          vocabulary.json
```

## èƒŒæ™¯éŸ³ä¹ ğŸµ

ç”¨äºè§†é¢‘çš„èƒŒæ™¯éŸ³ä¹ï¼Œä½äºé¡¹ç›®çš„ `resource/songs` ç›®å½•ä¸‹ã€‚
&gt; å½“å‰é¡¹ç›®é‡Œé¢æ”¾äº†ä¸€äº›é»˜è®¤çš„éŸ³ä¹ï¼Œæ¥è‡ªäº YouTube è§†é¢‘ï¼Œå¦‚æœ‰ä¾µæƒï¼Œè¯·åˆ é™¤ã€‚

## å­—å¹•å­—ä½“ ğŸ…°

ç”¨äºè§†é¢‘å­—å¹•çš„æ¸²æŸ“ï¼Œä½äºé¡¹ç›®çš„ `resource/fonts` ç›®å½•ä¸‹ï¼Œä½ ä¹Ÿå¯ä»¥æ”¾è¿›å»è‡ªå·±çš„å­—ä½“ã€‚

## å¸¸è§é—®é¢˜ ğŸ¤”

### â“RuntimeError: No ffmpeg exe could be found

é€šå¸¸æƒ…å†µä¸‹ï¼Œffmpeg ä¼šè¢«è‡ªåŠ¨ä¸‹è½½ï¼Œå¹¶ä¸”ä¼šè¢«è‡ªåŠ¨æ£€æµ‹åˆ°ã€‚
ä½†æ˜¯å¦‚æœä½ çš„ç¯å¢ƒæœ‰é—®é¢˜ï¼Œæ— æ³•è‡ªåŠ¨ä¸‹è½½ï¼Œå¯èƒ½ä¼šé‡åˆ°å¦‚ä¸‹é”™è¯¯ï¼š

```
RuntimeError: No ffmpeg exe could be found.
Install ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable.
```

æ­¤æ—¶ä½ å¯ä»¥ä» https://www.gyan.dev/ffmpeg/builds/ ä¸‹è½½ffmpegï¼Œè§£å‹åï¼Œè®¾ç½® `ffmpeg_path` ä¸ºä½ çš„å®é™…å®‰è£…è·¯å¾„å³å¯ã€‚

```toml
[app]
# è¯·æ ¹æ®ä½ çš„å®é™…è·¯å¾„è®¾ç½®ï¼Œæ³¨æ„ Windows è·¯å¾„åˆ†éš”ç¬¦ä¸º \\
ffmpeg_path = &quot;C:\\Users\\harry\\Downloads\\ffmpeg.exe&quot;
```

### â“ImageMagickçš„å®‰å…¨ç­–ç•¥é˜»æ­¢äº†ä¸ä¸´æ—¶æ–‡ä»¶@/tmp/tmpur5hyyto.txtç›¸å…³çš„æ“ä½œ

å¯ä»¥åœ¨ImageMagickçš„é…ç½®æ–‡ä»¶policy.xmlä¸­æ‰¾åˆ°è¿™äº›ç­–ç•¥ã€‚
è¿™ä¸ªæ–‡ä»¶é€šå¸¸ä½äº /etc/ImageMagick-`X`/ æˆ– ImageMagick å®‰è£…ç›®å½•çš„ç±»ä¼¼ä½ç½®ã€‚
ä¿®æ”¹åŒ…å«`pattern=&quot;@&quot;`çš„æ¡ç›®ï¼Œå°†`rights=&quot;none&quot;`æ›´æ”¹ä¸º`rights=&quot;read|write&quot;`ä»¥å…è®¸å¯¹æ–‡ä»¶çš„è¯»å†™æ“ä½œã€‚

### â“OSError: [Errno 24] Too many open files

è¿™ä¸ªé—®é¢˜æ˜¯ç”±äºç³»ç»Ÿæ‰“å¼€æ–‡ä»¶æ•°é™åˆ¶å¯¼è‡´çš„ï¼Œå¯ä»¥é€šè¿‡ä¿®æ”¹ç³»ç»Ÿçš„æ–‡ä»¶æ‰“å¼€æ•°é™åˆ¶æ¥è§£å†³ã€‚

æŸ¥çœ‹å½“å‰é™åˆ¶

```shell
ulimit -n
```

å¦‚æœè¿‡ä½ï¼Œå¯ä»¥è°ƒé«˜ä¸€äº›ï¼Œæ¯”å¦‚

```shell
ulimit -n 10240
```

### â“Whisper æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œå‡ºç°å¦‚ä¸‹é”™è¯¯

LocalEntryNotfoundEror: Cannot find an appropriate cached snapshotfolderfor the specified revision on the local disk and
outgoing trafic has been disabled.
To enablerepo look-ups and downloads online, pass &#039;local files only=False&#039; as input.

æˆ–è€…

An error occured while synchronizing the model Systran/faster-whisper-large-v3 from the Hugging Face Hub:
An error happened while trying to locate the files on the Hub and we cannot find the appropriate snapshot folder for the
specified revision on the local disk. Please check your internet connection and try again.
Trying to load the model directly from the local cache, if it exists.

è§£å†³æ–¹æ³•ï¼š[ç‚¹å‡»æŸ¥çœ‹å¦‚ä½•ä»ç½‘ç›˜æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹](#%E5%AD%97%E5%B9%95%E7%94%9F%E6%88%90-)

## åé¦ˆå»ºè®® ğŸ“¢

- å¯ä»¥æäº¤ [issue](https://github.com/harry0703/MoneyPrinterTurbo/issues)
  æˆ–è€… [pull request](https://github.com/harry0703/MoneyPrinterTurbo/pulls)ã€‚

## å‚è€ƒé¡¹ç›® ğŸ“š

è¯¥é¡¹ç›®åŸºäº https://github.com/FujiwaraChoki/MoneyPrinter é‡æ„è€Œæ¥ï¼Œåšäº†å¤§é‡çš„ä¼˜åŒ–ï¼Œå¢åŠ äº†æ›´å¤šçš„åŠŸèƒ½ã€‚
æ„Ÿè°¢åŸä½œè€…çš„å¼€æºç²¾ç¥ã€‚

## è®¸å¯è¯ ğŸ“

ç‚¹å‡»æŸ¥çœ‹ [`LICENSE`](LICENSE) æ–‡ä»¶

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=harry0703/MoneyPrinterTurbo&amp;type=Date)](https://star-history.com/#harry0703/MoneyPrinterTurbo&amp;Date)</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NVIDIA/NeMo]]></title>
            <link>https://github.com/NVIDIA/NeMo</link>
            <guid>https://github.com/NVIDIA/NeMo</guid>
            <pubDate>Fri, 09 May 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[A scalable generative AI framework built for researchers and developers working on Large Language Models, Multimodal, and Speech AI (Automatic Speech Recognition and Text-to-Speech)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NVIDIA/NeMo">NVIDIA/NeMo</a></h1>
            <p>A scalable generative AI framework built for researchers and developers working on Large Language Models, Multimodal, and Speech AI (Automatic Speech Recognition and Text-to-Speech)</p>
            <p>Language: Python</p>
            <p>Stars: 13,982</p>
            <p>Forks: 2,819</p>
            <p>Stars today: 50 stars today</p>
            <h2>README</h2><pre>[![Project Status: Active -- The project has reached a stable, usable state and is being actively developed.](http://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active)
[![Documentation](https://readthedocs.com/projects/nvidia-nemo/badge/?version=main)](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/)
[![CodeQL](https://github.com/nvidia/nemo/actions/workflows/codeql.yml/badge.svg?branch=main&amp;event=push)](https://github.com/nvidia/nemo/actions/workflows/codeql.yml)
[![NeMo core license and license for collections in this repo](https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg)](https://github.com/NVIDIA/NeMo/blob/master/LICENSE)
[![Release version](https://badge.fury.io/py/nemo-toolkit.svg)](https://badge.fury.io/py/nemo-toolkit)
[![Python version](https://img.shields.io/pypi/pyversions/nemo-toolkit.svg)](https://badge.fury.io/py/nemo-toolkit)
[![PyPi total downloads](https://static.pepy.tech/personalized-badge/nemo-toolkit?period=total&amp;units=international_system&amp;left_color=grey&amp;right_color=brightgreen&amp;left_text=downloads)](https://pepy.tech/project/nemo-toolkit)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

# **NVIDIA NeMo Framework**

## Latest News

&lt;!-- markdownlint-disable --&gt;
&lt;details open&gt;
  &lt;summary&gt;&lt;b&gt;Pretrain and finetune :hugs:Hugging Face models via AutoModel&lt;/b&gt;&lt;/summary&gt;
      Nemo Framework&#039;s latest feature AutoModel enables broad support for :hugs:Hugging Face models, with 25.02 focusing on &lt;a href=https://huggingface.co/transformers/v3.5.1/model_doc/auto.html#automodelforcausallm&gt;AutoModelForCausalLM&lt;a&gt; in the &lt;a href=https://huggingface.co/models?pipeline_tag=text-generation&amp;sort=trending&gt;text generation category&lt;a&gt;. Future releases will enable support for more model families such as Vision Language Model.
&lt;/details&gt;

&lt;details open&gt;
  &lt;summary&gt;&lt;b&gt;Training on Blackwell using Nemo&lt;/b&gt;&lt;/summary&gt;
      NeMo Framework has added Blackwell support, with 25.02 focusing on functional parity for B200. More optimizations to come in the upcoming releases.
&lt;/details&gt;


&lt;details open&gt;
  &lt;summary&gt;&lt;b&gt;NeMo Framework 2.0&lt;/b&gt;&lt;/summary&gt;
      We&#039;ve released NeMo 2.0, an update on the NeMo Framework which prioritizes modularity and ease-of-use. Please refer to the &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/index.html&gt;NeMo Framework User Guide&lt;/a&gt; to get started.
&lt;/details&gt;
&lt;details open&gt;
  &lt;summary&gt;&lt;b&gt;New Cosmos World Foundation Models Support&lt;/b&gt;&lt;/summary&gt;
    &lt;details&gt; 
      &lt;summary&gt; &lt;a href=&quot;https://developer.nvidia.com/blog/advancing-physical-ai-with-nvidia-cosmos-world-foundation-model-platform&quot;&gt;Advancing Physical AI with NVIDIA Cosmos World Foundation Model Platform &lt;/a&gt; (2025-01-09) 
      &lt;/summary&gt; 
        The end-to-end NVIDIA Cosmos platform accelerates world model development for physical AI systems. Built on CUDA, Cosmos combines state-of-the-art world foundation models, video tokenizers, and AI-accelerated data processing pipelines. Developers can accelerate world model development by fine-tuning Cosmos world foundation models or building new ones from the ground up. These models create realistic synthetic videos of environments and interactions, providing a scalable foundation for training complex systems, from simulating humanoid robots performing advanced actions to developing end-to-end autonomous driving models. 
        &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/accelerate-custom-video-foundation-model-pipelines-with-new-nvidia-nemo-framework-capabilities/&quot;&gt;
          Accelerate Custom Video Foundation Model Pipelines with New NVIDIA NeMo Framework Capabilities
        &lt;/a&gt; (2025-01-07)
      &lt;/summary&gt;
        The NeMo Framework now supports training and customizing the &lt;a href=&quot;https://github.com/NVIDIA/Cosmos&quot;&gt;NVIDIA Cosmos&lt;/a&gt; collection of world foundation models. Cosmos leverages advanced text-to-world generation techniques to create fluid, coherent video content from natural language prompts.
        &lt;br&gt;&lt;br&gt;
        You can also now accelerate your video processing step using the &lt;a href=&quot;https://developer.nvidia.com/nemo-curator-video-processing-early-access&quot;&gt;NeMo Curator&lt;/a&gt; library, which provides optimized video processing and captioning features that can deliver up to 89x faster video processing when compared to an unoptimized CPU pipeline.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
&lt;/details&gt;
&lt;details open&gt;
  &lt;summary&gt;&lt;b&gt;Large Language Models and Multimodal Models&lt;/b&gt;&lt;/summary&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/state-of-the-art-multimodal-generative-ai-model-development-with-nvidia-nemo/&quot;&gt;
          State-of-the-Art Multimodal Generative AI Model Development with NVIDIA NeMo
        &lt;/a&gt; (2024-11-06)
      &lt;/summary&gt;
        NVIDIA recently announced significant enhancements to the NeMo platform, focusing on multimodal generative AI models. The update includes NeMo Curator and the Cosmos tokenizer, which streamline the data curation process and enhance the quality of visual data. These tools are designed to handle large-scale data efficiently, making it easier to develop high-quality AI models for various applications, including robotics and autonomous driving. The Cosmos tokenizers, in particular, efficiently map visual data into compact, semantic tokens, which is crucial for training large-scale generative models. The tokenizer is available now on the &lt;a href=http://github.com/NVIDIA/cosmos-tokenizer/NVIDIA/cosmos-tokenizer&gt;NVIDIA/cosmos-tokenizer&lt;/a&gt; GitHub repo and on &lt;a href=https://huggingface.co/nvidia/Cosmos-Tokenizer-CV8x8x8&gt;Hugging Face&lt;/a&gt;.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/llama/index.html#new-llama-3-1-support for more information/&quot;&gt;
        New Llama 3.1 Support
        &lt;/a&gt; (2024-07-23)
      &lt;/summary&gt;
        The NeMo Framework now supports training and customizing the Llama 3.1 collection of LLMs from Meta.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://aws.amazon.com/blogs/machine-learning/accelerate-your-generative-ai-distributed-training-workloads-with-the-nvidia-nemo-framework-on-amazon-eks/&quot;&gt;
          Accelerate your Generative AI Distributed Training Workloads with the NVIDIA NeMo Framework on Amazon EKS
        &lt;/a&gt; (2024-07-16)
      &lt;/summary&gt;
     NVIDIA NeMo Framework now runs distributed training workloads on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. For step-by-step instructions on creating an EKS cluster and running distributed training workloads with NeMo, see the GitHub repository &lt;a href=&quot;https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/2.nemo-launcher/EKS/&quot;&gt; here.&lt;/a&gt;
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-nemo-accelerates-llm-innovation-with-hybrid-state-space-model-support/&quot;&gt;
          NVIDIA NeMo Accelerates LLM Innovation with Hybrid State Space Model Support
        &lt;/a&gt; (2024/06/17)
      &lt;/summary&gt;
     NVIDIA NeMo and Megatron Core now support pre-training and fine-tuning of state space models (SSMs). NeMo also supports training models based on the Griffin architecture as described by Google DeepMind. 
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
      &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://huggingface.co/models?sort=trending&amp;search=nvidia%2Fnemotron-4-340B&quot;&gt;
          NVIDIA releases 340B base, instruct, and reward models pretrained on a total of 9T tokens.
        &lt;/a&gt; (2024-06-18)
      &lt;/summary&gt;
      See documentation and tutorials for SFT, PEFT, and PTQ with 
      &lt;a href=&quot;https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/nemotron/index.html&quot;&gt;
        Nemotron 340B 
      &lt;/a&gt;
      in the NeMo Framework User Guide.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-sets-new-generative-ai-performance-and-scale-records-in-mlperf-training-v4-0/&quot;&gt;
          NVIDIA sets new generative AI performance and scale records in MLPerf Training v4.0
        &lt;/a&gt; (2024/06/12)
      &lt;/summary&gt;
      Using NVIDIA NeMo Framework and NVIDIA Hopper GPUs NVIDIA was able to scale to 11,616 H100 GPUs and achieve near-linear performance scaling on LLM pretraining. 
      NVIDIA also achieved the highest LLM fine-tuning performance and raised the bar for text-to-image training.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
        &lt;summary&gt;
          &lt;a href=&quot;https://cloud.google.com/blog/products/compute/gke-and-nvidia-nemo-framework-to-train-generative-ai-models&quot;&gt;
            Accelerate your generative AI journey with NVIDIA NeMo Framework on GKE
          &lt;/a&gt; (2024/03/16)
        &lt;/summary&gt;
        An end-to-end walkthrough to train generative AI models on the Google Kubernetes Engine (GKE) using the NVIDIA NeMo Framework is available at https://github.com/GoogleCloudPlatform/nvidia-nemo-on-gke. 
        The walkthrough includes detailed instructions on how to set up a Google Cloud Project and pre-train a GPT model using the NeMo Framework.
        &lt;br&gt;&lt;br&gt;
      &lt;/details&gt;
&lt;/details&gt;
&lt;details open&gt;
  &lt;summary&gt;&lt;b&gt;Speech Recognition&lt;/b&gt;&lt;/summary&gt;
  &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/accelerating-leaderboard-topping-asr-models-10x-with-nvidia-nemo/&quot;&gt;
          Accelerating Leaderboard-Topping ASR Models 10x with NVIDIA NeMo
        &lt;/a&gt; (2024/09/24)
      &lt;/summary&gt;
      NVIDIA NeMo team released a number of inference optimizations for CTC, RNN-T, and TDT models that resulted in up to 10x inference speed-up. 
      These models now exceed an inverse real-time factor (RTFx) of 2,000, with some reaching RTFx of even 6,000.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/new-standard-for-speech-recognition-and-translation-from-the-nvidia-nemo-canary-model/&quot;&gt;
          New Standard for Speech Recognition and Translation from the NVIDIA NeMo Canary Model
        &lt;/a&gt; (2024/04/18)
      &lt;/summary&gt;
      The NeMo team just released Canary, a multilingual model that transcribes speech in English, Spanish, German, and French with punctuation and capitalization. 
      Canary also provides bi-directional translation, between English and the three other supported languages.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/pushing-the-boundaries-of-speech-recognition-with-nemo-parakeet-asr-models/&quot;&gt;
          Pushing the Boundaries of Speech Recognition with NVIDIA NeMo Parakeet ASR Models
        &lt;/a&gt; (2024/04/18)
      &lt;/summary&gt;
      NVIDIA NeMo, an end-to-end platform for the development of multimodal generative AI models at scale anywhereâ€”on any cloud and on-premisesâ€”released the Parakeet family of automatic speech recognition (ASR) models. 
      These state-of-the-art ASR models, developed in collaboration with Suno.ai, transcribe spoken English with exceptional accuracy.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
  &lt;details&gt;
    &lt;summary&gt;
      &lt;a href=&quot;https://developer.nvidia.com/blog/turbocharge-asr-accuracy-and-speed-with-nvidia-nemo-parakeet-tdt/&quot;&gt;
        Turbocharge ASR Accuracy and Speed with NVIDIA NeMo Parakeet-TDT
      &lt;/a&gt; (2024/04/18)
    &lt;/summary&gt;
    NVIDIA NeMo, an end-to-end platform for developing multimodal generative AI models at scale anywhereâ€”on any cloud and on-premisesâ€”recently released Parakeet-TDT. 
    This new addition to the â€¯NeMo ASR Parakeet model family boasts better accuracy and 64% greater speed over the previously best model, Parakeet-RNNT-1.1B.
    &lt;br&gt;&lt;br&gt;
  &lt;/details&gt;
&lt;/details&gt;
&lt;!-- markdownlint-enable --&gt;

## Introduction

NVIDIA NeMo Framework is a scalable and cloud-native generative AI
framework built for researchers and PyTorch developers working on Large
Language Models (LLMs), Multimodal Models (MMs), Automatic Speech
Recognition (ASR), Text to Speech (TTS), and Computer Vision (CV)
domains. It is designed to help you efficiently create, customize, and
deploy new generative AI models by leveraging existing code and
pre-trained model checkpoints.

For technical documentation, please see the [NeMo Framework User
Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html).

## What&#039;s New in NeMo 2.0

NVIDIA NeMo 2.0 introduces several significant improvements over its predecessor, NeMo 1.0, enhancing flexibility, performance, and scalability.

- **Python-Based Configuration** - NeMo 2.0 transitions from YAML files to a Python-based configuration, providing more flexibility and control. This shift makes it easier to extend and customize configurations programmatically.

- **Modular Abstractions** - By adopting PyTorch Lightningâ€™s modular abstractions, NeMo 2.0 simplifies adaptation and experimentation. This modular approach allows developers to more easily modify and experiment with different components of their models.

- **Scalability** - NeMo 2.0 seamlessly scaling large-scale experiments across thousands of GPUs using [NeMo-Run](https://github.com/NVIDIA/NeMo-Run), a powerful tool designed to streamline the configuration, execution, and management of machine learning experiments across computing environments.

Overall, these enhancements make NeMo 2.0 a powerful, scalable, and user-friendly framework for AI model development.

&gt; [!IMPORTANT]  
&gt; NeMo 2.0 is currently supported by the LLM (large language model) and VLM (vision language model) collections.

### Get Started with NeMo 2.0

- Refer to the [Quickstart](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/quickstart.html) for examples of using NeMo-Run to launch NeMo 2.0 experiments locally and on a slurm cluster.
- For more information about NeMo 2.0, see the [NeMo Framework User Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/index.html).
- [NeMo 2.0 Recipes](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/llm/recipes) contains additional examples of launching large-scale runs using NeMo 2.0 and NeMo-Run.
- For an in-depth exploration of the main features of NeMo 2.0, see the [Feature Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/features/index.html#feature-guide).
- To transition from NeMo 1.0 to 2.0, see the [Migration Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/migration/index.html#migration-guide) for step-by-step instructions.

### Get Started with Cosmos

NeMo Curator and NeMo Framework support video curation and post-training of the Cosmos World Foundation Models, which are open and available on [NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/cosmos/collections/cosmos) and [Hugging Face](https://huggingface.co/collections/nvidia/cosmos-6751e884dc10e013a0a0d8e6). For more information on video datasets, refer to [NeMo Curator](https://developer.nvidia.com/nemo-curator). To post-train World Foundation Models using the NeMo Framework for your custom physical AI tasks, see the [Cosmos Diffusion models](https://github.com/NVIDIA/Cosmos/blob/main/cosmos1/models/diffusion/nemo/post_training/README.md) and the [Cosmos Autoregressive models](https://github.com/NVIDIA/Cosmos/blob/main/cosmos1/models/autoregressive/nemo/post_training/README.md).

## LLMs and MMs Training, Alignment, and Customization

All NeMo models are trained with
[Lightning](https://github.com/Lightning-AI/lightning). Training is
automatically scalable to 1000s of GPUs. You can check the performance benchmarks using the
latest NeMo Framework container [here](https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance_summary.html).

When applicable, NeMo models leverage cutting-edge distributed training
techniques, incorporating [parallelism
strategies](https://docs.nvidia.com/nemo-framework/user-guide/latest/modeloverview.html)
to enable efficient training of very large models. These techniques
include Tensor Parallelism (TP), Pipeline Parallelism (PP), Fully
Sharded Data Parallelism (FSDP), Mixture-of-Experts (MoE), and Mixed
Precision Training with BFloat16 and FP8, as well as others.

NeMo Transformer-based LLMs and MMs utilize [NVIDIA Transformer
Engine](https://github.com/NVIDIA/TransformerEngine) for FP8 training on
NVIDIA Hopper GPUs, while leveraging [NVIDIA Megatron
Core](https://github.com/NVIDIA/Megatron-LM/tree/main/megatron/core) for
scaling Transformer model training.

NeMo LLMs can be aligned with state-of-the-art methods such as SteerLM,
Direct Preference Optimization (DPO), and Reinforcement Learning from
Human Feedback (RLHF). See [NVIDIA NeMo
Aligner](https://github.com/NVIDIA/NeMo-Aligner) for more information.

In addition to supervised fine-tuning (SFT), NeMo also supports the
latest parameter efficient fine-tuning (PEFT) techniques such as LoRA,
P-Tuning, Adapters, and IA3. Refer to the [NeMo Framework User
Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/sft_peft/index.html)
for the full list of supported models and techniques.

## LLMs and MMs Deployment and Optimization

NeMo LLMs and MMs can be deployed and optimized with [NVIDIA NeMo
Microservices](https://developer.nvidia.com/nemo-microservices-early-access).

## Speech AI

NeMo ASR and TTS models can be optimized for inference and deployed for
production use cases with [NVIDIA Riva](https://developer.nvidia.com/riva).

## NeMo Framework Launcher

&gt; [!IMPORTANT]  
&gt; NeMo Framework Launcher is compatible with NeMo version 1.0 only. [NeMo-Run](https://github.com/NVIDIA/NeMo-Run) is recommended for launching experiments using NeMo 2.0.

[NeMo Framework
Launcher](https://github.com/NVIDIA/NeMo-Megatron-Launcher) is a
cloud-native tool that streamlines the NeMo Framework experience. It is
used for launching end-to-end NeMo Framework training jobs on CSPs and
Slurm clusters.

The NeMo Framework Launcher includes extensive recipes, scripts,
utilities, and documentation for training NeMo LLMs. It also includes
the NeMo Framework [Autoconfigurator](https://github.com/NVIDIA/NeMo-Megatron-Launcher#53-using-autoconfigurator-to-find-the-optimal-configuration),
which is designed to find the optimal model parallel configuration for
training on a specific cluster.

To get started quickly with the NeMo Framework Launcher, please see the
[NeMo Framework
Playbooks](https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html).
The NeMo Framework Launcher does not currently support ASR and TTS
training, but it will soon.

## Get Started with NeMo Framework

Getting started with NeMo Framework is easy. State-of-the-art pretrained
NeMo models are freely available on [Hugging Face
Hub](https://huggingface.co/models?library=nemo&amp;sort=downloads&amp;search=nvidia)
and [NVIDIA
NGC](https://catalog.ngc.nvidia.com/models?query=nemo&amp;orderBy=weightPopularDESC).
These models can be used to generate text or images, transcribe audio,
and synthesize speech in just a few lines of code.

We have extensive
[tutorials](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/starthere/tutorials.html)
that can be run on [Google Colab](https://colab.research.google.com) or
with our [NGC NeMo Framework
Container](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo).
We also have
[playbooks](https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html)
for users who want to train NeMo models with the NeMo Framework
Launcher.

For advanced users who want to train NeMo models from scratch or
fine-tune existing NeMo models, we have a full suite of [example
scripts](https://github.com/NVIDIA/NeMo/tree/main/examples) that support
multi-GPU/multi-node training.

## Key Features

- [Large Language Models](nemo/collections/nlp/README.md)
- [Multimodal](nemo/collections/multimodal/READM

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ranaroussi/yfinance]]></title>
            <link>https://github.com/ranaroussi/yfinance</link>
            <guid>https://github.com/ranaroussi/yfinance</guid>
            <pubDate>Fri, 09 May 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[Download market data from Yahoo! Finance's API]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ranaroussi/yfinance">ranaroussi/yfinance</a></h1>
            <p>Download market data from Yahoo! Finance's API</p>
            <p>Language: Python</p>
            <p>Stars: 17,152</p>
            <p>Forks: 2,645</p>
            <p>Stars today: 36 stars today</p>
            <h2>README</h2><pre>&lt;img src=&quot;./doc/yfinance-gh-logo-dark.webp#gh-dark-mode-only&quot; height=&quot;100&quot;&gt;
&lt;img src=&quot;./doc/yfinance-gh-logo-light.webp#gh-light-mode-only&quot; height=&quot;100&quot;&gt;

# Download market data from Yahoo! Finance&#039;s API

&lt;a target=&quot;new&quot; href=&quot;https://pypi.python.org/pypi/yfinance&quot;&gt;&lt;img border=0 src=&quot;https://img.shields.io/badge/python-2.7,%203.6+-blue.svg?style=flat&quot; alt=&quot;Python version&quot;&gt;&lt;/a&gt;
&lt;a target=&quot;new&quot; href=&quot;https://pypi.python.org/pypi/yfinance&quot;&gt;&lt;img border=0 src=&quot;https://img.shields.io/pypi/v/yfinance.svg?maxAge=60%&quot; alt=&quot;PyPi version&quot;&gt;&lt;/a&gt;
&lt;a target=&quot;new&quot; href=&quot;https://pypi.python.org/pypi/yfinance&quot;&gt;&lt;img border=0 src=&quot;https://img.shields.io/pypi/status/yfinance.svg?maxAge=60&quot; alt=&quot;PyPi status&quot;&gt;&lt;/a&gt;
&lt;a target=&quot;new&quot; href=&quot;https://pypi.python.org/pypi/yfinance&quot;&gt;&lt;img border=0 src=&quot;https://img.shields.io/pypi/dm/yfinance.svg?maxAge=2592000&amp;label=installs&amp;color=%2327B1FF&quot; alt=&quot;PyPi downloads&quot;&gt;&lt;/a&gt;
&lt;a target=&quot;new&quot; href=&quot;https://github.com/ranaroussi/yfinance&quot;&gt;&lt;img border=0 src=&quot;https://img.shields.io/github/stars/ranaroussi/yfinance.svg?style=social&amp;label=Star&amp;maxAge=60&quot; alt=&quot;Star this repo&quot;&gt;&lt;/a&gt;
&lt;a target=&quot;new&quot; href=&quot;https://x.com/intent/follow?screen_name=aroussi&quot;&gt;&lt;img border=0 src=&quot;https://img.shields.io/twitter/follow/aroussi.svg?style=social&amp;label=Follow&amp;maxAge=60&quot; alt=&quot;Follow me on twitter&quot;&gt;&lt;/a&gt;



**yfinance** offers a Pythonic way to fetch financial &amp; market data from [Yahoo!â“‡ finance](https://finance.yahoo.com).

---

&gt; [!IMPORTANT]  
&gt; **Yahoo!, Y!Finance, and Yahoo! finance are registered trademarks of Yahoo, Inc.**
&gt;
&gt; yfinance is **not** affiliated, endorsed, or vetted by Yahoo, Inc. It&#039;s an open-source tool that uses Yahoo&#039;s publicly available APIs, and is intended for research and educational purposes.
&gt; 
&gt; **You should refer to Yahoo!&#039;s terms of use** ([here](https://policies.yahoo.com/us/en/yahoo/terms/product-atos/apiforydn/index.htm), [here](https://legal.yahoo.com/us/en/yahoo/terms/otos/index.html), and [here](https://policies.yahoo.com/us/en/yahoo/terms/index.htm)) **for details on your rights to use the actual data downloaded.
&gt;
&gt; Remember - the Yahoo! finance API is intended for personal use only.**

---

&gt; [!TIP]
&gt; THE NEW DOCUMENTATION WEBSITE IS NOW LIVE! ğŸ¤˜
&gt; 
&gt; Visit [**ranaroussi.github.io/yfinance**](https://ranaroussi.github.io/yfinance)

---

## Main components

- `Ticker`: single ticker data
- `Tickers`: multiple tickers&#039; data
- `download`: download market data for multiple tickers
- `Market`: get information about a market
- `WebSocket` and `AsyncWebSocket`: live streaming data
- `Search`: quotes and news from search
- `Sector` and `Industry`: sector and industry information
- `EquityQuery` and `Screener`: build query to screen market

## Installation

Install `yfinance` from PYPI using `pip`:

``` {.sourceCode .bash}
$ pip install yfinance
```

---

![Star History Chart](https://api.star-history.com/svg?repos=ranaroussi/yfinance)

---

### Legal Stuff

**yfinance** is distributed under the **Apache Software License**. See
the [LICENSE.txt](./LICENSE.txt) file in the release for details.

AGAIN - yfinance is **not** affiliated, endorsed, or vetted by Yahoo, Inc. It&#039;s
an open-source tool that uses Yahoo&#039;s publicly available APIs, and is
intended for research and educational purposes. You should refer to Yahoo!&#039;s terms of use
([here](https://policies.yahoo.com/us/en/yahoo/terms/product-atos/apiforydn/index.htm),
[here](https://legal.yahoo.com/us/en/yahoo/terms/otos/index.html), and
[here](https://policies.yahoo.com/us/en/yahoo/terms/index.htm)) for
details on your rights to use the actual data downloaded.

---

### P.S.

Please drop me a note with any feedback you have.

**Ran Aroussi**

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ultralytics/ultralytics]]></title>
            <link>https://github.com/ultralytics/ultralytics</link>
            <guid>https://github.com/ultralytics/ultralytics</guid>
            <pubDate>Fri, 09 May 2025 00:04:20 GMT</pubDate>
            <description><![CDATA[Ultralytics YOLO11 ğŸš€]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ultralytics/ultralytics">ultralytics/ultralytics</a></h1>
            <p>Ultralytics YOLO11 ğŸš€</p>
            <p>Language: Python</p>
            <p>Stars: 40,347</p>
            <p>Forks: 7,827</p>
            <p>Stars today: 62 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
    &lt;a href=&quot;https://www.ultralytics.com/blog/ultralytics-yolo11-has-arrived-redefine-whats-possible-in-ai&quot; target=&quot;_blank&quot;&gt;
      &lt;img width=&quot;100%&quot; src=&quot;https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png&quot; alt=&quot;Ultralytics YOLO banner&quot;&gt;&lt;/a&gt;
  &lt;/p&gt;

[ä¸­æ–‡](https://docs.ultralytics.com/zh) | [í•œêµ­ì–´](https://docs.ultralytics.com/ko) | [æ—¥æœ¬èª](https://docs.ultralytics.com/ja) | [Ğ ÑƒÑÑĞºĞ¸Ğ¹](https://docs.ultralytics.com/ru) | [Deutsch](https://docs.ultralytics.com/de) | [FranÃ§ais](https://docs.ultralytics.com/fr) | [EspaÃ±ol](https://docs.ultralytics.com/es) | [PortuguÃªs](https://docs.ultralytics.com/pt) | [TÃ¼rkÃ§e](https://docs.ultralytics.com/tr) | [Tiáº¿ng Viá»‡t](https://docs.ultralytics.com/vi) | [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](https://docs.ultralytics.com/ar) &lt;br&gt;

&lt;div&gt;
    &lt;a href=&quot;https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg&quot; alt=&quot;Ultralytics CI&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://pepy.tech/projects/ultralytics&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/ultralytics&quot; alt=&quot;Ultralytics Downloads&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://zenodo.org/badge/latestdoi/264818686&quot;&gt;&lt;img src=&quot;https://zenodo.org/badge/264818686.svg&quot; alt=&quot;Ultralytics YOLO Citation&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://discord.com/invite/ultralytics&quot;&gt;&lt;img alt=&quot;Ultralytics Discord&quot; src=&quot;https://img.shields.io/discord/1089800235347353640?logo=discord&amp;logoColor=white&amp;label=Discord&amp;color=blue&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://community.ultralytics.com/&quot;&gt;&lt;img alt=&quot;Ultralytics Forums&quot; src=&quot;https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&amp;logo=discourse&amp;label=Forums&amp;color=blue&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://www.reddit.com/r/ultralytics/&quot;&gt;&lt;img alt=&quot;Ultralytics Reddit&quot; src=&quot;https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&amp;logo=reddit&amp;logoColor=white&amp;label=Reddit&amp;color=blue&quot;&gt;&lt;/a&gt;
    &lt;br&gt;
    &lt;a href=&quot;https://console.paperspace.com/github/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://assets.paperspace.io/img/gradient-badge.svg&quot; alt=&quot;Run Ultralytics on Gradient&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open Ultralytics In Colab&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://www.kaggle.com/models/ultralytics/yolo11&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg&quot; alt=&quot;Open Ultralytics In Kaggle&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://mybinder.org/v2/gh/ultralytics/ultralytics/HEAD?labpath=examples%2Ftutorial.ipynb&quot;&gt;&lt;img src=&quot;https://mybinder.org/badge_logo.svg&quot; alt=&quot;Open Ultralytics In Binder&quot;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;

[Ultralytics](https://www.ultralytics.com/) creates cutting-edge, state-of-the-art (SOTA) [YOLO models](https://www.ultralytics.com/yolo) built on years of foundational research in computer vision and AI. Constantly updated for performance and flexibility, our models are **fast**, **accurate**, and **easy to use**. They excel at [object detection](https://docs.ultralytics.com/tasks/detect/), [tracking](https://docs.ultralytics.com/modes/track/), [instance segmentation](https://docs.ultralytics.com/tasks/segment/), [image classification](https://docs.ultralytics.com/tasks/classify/), and [pose estimation](https://docs.ultralytics.com/tasks/pose/) tasks.

Find detailed documentation in the [Ultralytics Docs](https://docs.ultralytics.com/). Get support via [GitHub Issues](https://github.com/ultralytics/ultralytics/issues/new/choose). Join discussions on [Discord](https://discord.com/invite/ultralytics), [Reddit](https://www.reddit.com/r/ultralytics/), and the [Ultralytics Community Forums](https://community.ultralytics.com/)!

Request an Enterprise License for commercial use at [Ultralytics Licensing](https://www.ultralytics.com/license).

&lt;a href=&quot;https://docs.ultralytics.com/models/yolo11/&quot; target=&quot;_blank&quot;&gt;
  &lt;img width=&quot;100%&quot; src=&quot;https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/yolo/performance-comparison.png&quot; alt=&quot;YOLO11 performance plots&quot;&gt;
&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/ultralytics&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics GitHub&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;2%&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://www.linkedin.com/company/ultralytics/&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics LinkedIn&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;2%&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://twitter.com/ultralytics&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics Twitter&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;2%&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://youtube.com/ultralytics?sub_confirmation=1&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics YouTube&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;2%&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://www.tiktok.com/@ultralytics&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics TikTok&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;2%&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://ultralytics.com/bilibili&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics BiliBili&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;2%&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://discord.com/invite/ultralytics&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics Discord&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

## ğŸ“„ Documentation

See below for quickstart installation and usage examples. For comprehensive guidance on training, validation, prediction, and deployment, refer to our full [Ultralytics Docs](https://docs.ultralytics.com/).

&lt;details open&gt;
&lt;summary&gt;Install&lt;/summary&gt;

Install the `ultralytics` package, including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml), in a [**Python&gt;=3.8**](https://www.python.org/) environment with [**PyTorch&gt;=1.8**](https://pytorch.org/get-started/locally/).

[![PyPI - Version](https://img.shields.io/pypi/v/ultralytics?logo=pypi&amp;logoColor=white)](https://pypi.org/project/ultralytics/) [![Ultralytics Downloads](https://static.pepy.tech/badge/ultralytics)](https://www.pepy.tech/projects/ultralytics) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ultralytics?logo=python&amp;logoColor=gold)](https://pypi.org/project/ultralytics/)

```bash
pip install ultralytics
```

For alternative installation methods, including [Conda](https://anaconda.org/conda-forge/ultralytics), [Docker](https://hub.docker.com/r/ultralytics/ultralytics), and building from source via Git, please consult the [Quickstart Guide](https://docs.ultralytics.com/quickstart/).

[![Conda Version](https://img.shields.io/conda/vn/conda-forge/ultralytics?logo=condaforge)](https://anaconda.org/conda-forge/ultralytics) [![Docker Image Version](https://img.shields.io/docker/v/ultralytics/ultralytics?sort=semver&amp;logo=docker)](https://hub.docker.com/r/ultralytics/ultralytics) [![Ultralytics Docker Pulls](https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker)](https://hub.docker.com/r/ultralytics/ultralytics)

&lt;/details&gt;

&lt;details open&gt;
&lt;summary&gt;Usage&lt;/summary&gt;

### CLI

You can use Ultralytics YOLO directly from the Command Line Interface (CLI) with the `yolo` command:

```bash
# Predict using a pretrained YOLO model (e.g., YOLO11n) on an image
yolo predict model=yolo11n.pt source=&#039;https://ultralytics.com/images/bus.jpg&#039;
```

The `yolo` command supports various tasks and modes, accepting additional arguments like `imgsz=640`. Explore the YOLO [CLI Docs](https://docs.ultralytics.com/usage/cli/) for more examples.

### Python

Ultralytics YOLO can also be integrated directly into your Python projects. It accepts the same [configuration arguments](https://docs.ultralytics.com/usage/cfg/) as the CLI:

```python
from ultralytics import YOLO

# Load a pretrained YOLO11n model
model = YOLO(&quot;yolo11n.pt&quot;)

# Train the model on the COCO8 dataset for 100 epochs
train_results = model.train(
    data=&quot;coco8.yaml&quot;,  # Path to dataset configuration file
    epochs=100,  # Number of training epochs
    imgsz=640,  # Image size for training
    device=&quot;cpu&quot;,  # Device to run on (e.g., &#039;cpu&#039;, 0, [0,1,2,3])
)

# Evaluate the model&#039;s performance on the validation set
metrics = model.val()

# Perform object detection on an image
results = model(&quot;path/to/image.jpg&quot;)  # Predict on an image
results[0].show()  # Display results

# Export the model to ONNX format for deployment
path = model.export(format=&quot;onnx&quot;)  # Returns the path to the exported model
```

Discover more examples in the YOLO [Python Docs](https://docs.ultralytics.com/usage/python/).

&lt;/details&gt;

## âœ¨ Models

Ultralytics supports a wide range of YOLO models, from early versions like [YOLOv3](https://docs.ultralytics.com/models/yolov3/) to the latest [YOLO11](https://docs.ultralytics.com/models/yolo11/). The tables below showcase YOLO11 models pretrained on the [COCO](https://docs.ultralytics.com/datasets/detect/coco/) dataset for [Detection](https://docs.ultralytics.com/tasks/detect/), [Segmentation](https://docs.ultralytics.com/tasks/segment/), and [Pose Estimation](https://docs.ultralytics.com/tasks/pose/). Additionally, [Classification](https://docs.ultralytics.com/tasks/classify/) models pretrained on the [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet/) dataset are available. [Tracking](https://docs.ultralytics.com/modes/track/) mode is compatible with all Detection, Segmentation, and Pose models. All [Models](https://docs.ultralytics.com/models/) are automatically downloaded from the latest Ultralytics [release](https://github.com/ultralytics/assets/releases) upon first use.

&lt;a href=&quot;https://docs.ultralytics.com/tasks/&quot; target=&quot;_blank&quot;&gt;
    &lt;img width=&quot;100%&quot; src=&quot;https://github.com/ultralytics/docs/releases/download/0/ultralytics-yolov8-tasks-banner.avif&quot; alt=&quot;Ultralytics YOLO supported tasks&quot;&gt;
&lt;/a&gt;
&lt;br&gt;
&lt;br&gt;

&lt;details open&gt;&lt;summary&gt;Detection (COCO)&lt;/summary&gt;

Explore the [Detection Docs](https://docs.ultralytics.com/tasks/detect/) for usage examples. These models are trained on the [COCO dataset](https://cocodataset.org/), featuring 80 object classes.

| Model                                                                                | size&lt;br&gt;&lt;sup&gt;(pixels) | mAP&lt;sup&gt;val&lt;br&gt;50-95 | Speed&lt;br&gt;&lt;sup&gt;CPU ONNX&lt;br&gt;(ms) | Speed&lt;br&gt;&lt;sup&gt;T4 TensorRT10&lt;br&gt;(ms) | params&lt;br&gt;&lt;sup&gt;(M) | FLOPs&lt;br&gt;&lt;sup&gt;(B) |
| ------------------------------------------------------------------------------------ | --------------------- | -------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |
| [YOLO11n](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt) | 640                   | 39.5                 | 56.1 Â± 0.8                     | 1.5 Â± 0.0                           | 2.6                | 6.5               |
| [YOLO11s](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s.pt) | 640                   | 47.0                 | 90.0 Â± 1.2                     | 2.5 Â± 0.0                           | 9.4                | 21.5              |
| [YOLO11m](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt) | 640                   | 51.5                 | 183.2 Â± 2.0                    | 4.7 Â± 0.1                           | 20.1               | 68.0              |
| [YOLO11l](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l.pt) | 640                   | 53.4                 | 238.6 Â± 1.4                    | 6.2 Â± 0.1                           | 25.3               | 86.9              |
| [YOLO11x](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x.pt) | 640                   | 54.7                 | 462.8 Â± 6.7                    | 11.3 Â± 0.2                          | 56.9               | 194.9             |

- **mAP&lt;sup&gt;val&lt;/sup&gt;** values refer to single-model single-scale performance on the [COCO val2017](https://cocodataset.org/) dataset. See [YOLO Performance Metrics](https://docs.ultralytics.com/guides/yolo-performance-metrics/) for details. &lt;br&gt;Reproduce with `yolo val detect data=coco.yaml device=0`
- **Speed** metrics are averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. &lt;br&gt;Reproduce with `yolo val detect data=coco.yaml batch=1 device=0|cpu`

&lt;/details&gt;

&lt;details&gt;&lt;summary&gt;Segmentation (COCO)&lt;/summary&gt;

Refer to the [Segmentation Docs](https://docs.ultralytics.com/tasks/segment/) for usage examples. These models are trained on [COCO-Seg](https://docs.ultralytics.com/datasets/segment/coco/), including 80 classes.

| Model                                                                                        | size&lt;br&gt;&lt;sup&gt;(pixels) | mAP&lt;sup&gt;box&lt;br&gt;50-95 | mAP&lt;sup&gt;mask&lt;br&gt;50-95 | Speed&lt;br&gt;&lt;sup&gt;CPU ONNX&lt;br&gt;(ms) | Speed&lt;br&gt;&lt;sup&gt;T4 TensorRT10&lt;br&gt;(ms) | params&lt;br&gt;&lt;sup&gt;(M) | FLOPs&lt;br&gt;&lt;sup&gt;(B) |
| -------------------------------------------------------------------------------------------- | --------------------- | -------------------- | --------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |
| [YOLO11n-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-seg.pt) | 640                   | 38.9                 | 32.0                  | 65.9 Â± 1.1                     | 1.8 Â± 0.0                           | 2.9                | 10.4              |
| [YOLO11s-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-seg.pt) | 640                   | 46.6                 | 37.8                  | 117.6 Â± 4.9                    | 2.9 Â± 0.0                           | 10.1               | 35.5              |
| [YOLO11m-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-seg.pt) | 640                   | 51.5                 | 41.5                  | 281.6 Â± 1.2                    | 6.3 Â± 0.1                           | 22.4               | 123.3             |
| [YOLO11l-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-seg.pt) | 640                   | 53.4                 | 42.9                  | 344.2 Â± 3.2                    | 7.8 Â± 0.2                           | 27.6               | 142.2             |
| [YOLO11x-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-seg.pt) | 640                   | 54.7                 | 43.8                  | 664.5 Â± 3.2                    | 15.8 Â± 0.7                          | 62.1               | 319.0             |

- **mAP&lt;sup&gt;val&lt;/sup&gt;** values are for single-model single-scale on the [COCO val2017](https://cocodataset.org/) dataset. See [YOLO Performance Metrics](https://docs.ultralytics.com/guides/yolo-performance-metrics/) for details. &lt;br&gt;Reproduce with `yolo val segment data=coco.yaml device=0`
- **Speed** metrics are averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. &lt;br&gt;Reproduce with `yolo val segment data=coco.yaml batch=1 device=0|cpu`

&lt;/details&gt;

&lt;details&gt;&lt;summary&gt;Classification (ImageNet)&lt;/summary&gt;

Consult the [Classification Docs](https://docs.ultralytics.com/tasks/classify/) for usage examples. These models are trained on [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet/), covering 1000 classes.

| Model                                                                                        | size&lt;br&gt;&lt;sup&gt;(pixels) | acc&lt;br&gt;&lt;sup&gt;top1 | acc&lt;br&gt;&lt;sup&gt;top5 | Speed&lt;br&gt;&lt;sup&gt;CPU ONNX&lt;br&gt;(ms) | Speed&lt;br&gt;&lt;sup&gt;T4 TensorRT10&lt;br&gt;(ms) | params&lt;br&gt;&lt;sup&gt;(M) | FLOPs&lt;br&gt;&lt;sup&gt;(B) at 224 |
| -------------------------------------------------------------------------------------------- | --------------------- | ---------------- | ---------------- | ------------------------------ | ----------------------------------- | ------------------ | ------------------------ |
| [YOLO11n-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-cls.pt) | 224                   | 70.0             | 89.4             | 5.0 Â± 0.3                      | 1.1 Â± 0.0                           | 1.6                | 0.5                      |
| [YOLO11s-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-cls.pt) | 224                   | 75.4             | 92.7             | 7.9 Â± 0.2                      | 1.3 Â± 0.0                           | 5.5                | 1.6                      |
| [YOLO11m-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-cls.pt) | 224                   | 77.3             | 93.9             | 17.2 Â± 0.4                     | 2.0 Â± 0.0                           | 10.4               | 5.0                      |
| [YOLO11l-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-cls.pt) | 224                   | 78.3             | 94.3             | 23.2 Â± 0.3                     | 2.8 Â± 0.0                           | 12.9               | 6.2                      |
| [YOLO11x-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-cls.pt) | 224                   | 79.5             | 94.9             | 41.4 Â± 0.9                     | 3.8 Â± 0.0                           | 28.4               | 13.7                     |

- **acc** values represent model accuracy on the [ImageNet](https://www.image-net.org/) dataset validation set. &lt;br&gt;Reproduce with `yolo val classify data=path/to/ImageNet device=0`
- **Speed** metrics are averaged over ImageNet val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. &lt;br&gt;Reproduce with `yolo val classify data=path/to/ImageNet batch=1 device=0|cpu`

&lt;/details&gt;

&lt;details&gt;&lt;summary&gt;Pose (COCO)&lt;/summary&gt;

See the [Pose Estimation Docs](https://docs.ultralytics.com/tasks/pose/) for usage examples. These models are trained on [COCO-Pose](https://docs.ultralytics.com/datasets/pose/coco/), focusing on the &#039;person&#039; class.

| Model                                                                                          | size&lt;br&gt;&lt;sup&gt;(pixels) | mAP&lt;sup&gt;pose&lt;br&gt;50-95 | mAP&lt;sup&gt;pose&lt;br&gt;50 | Speed&lt;br&gt;&lt;sup&gt;CPU ONNX&lt;br&gt;(ms) | Speed&lt;br&gt;&lt;sup&gt;T4 TensorRT10&lt;br&gt;(ms) | params&lt;br&gt;&lt;sup&gt;(M) | FLOPs&lt;br&gt;&lt;sup&gt;(B) |
| ---------------------------------------------------------------------------------------------- | --------------------- | --------------------- | ------------------ | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |
| [YOLO11n-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-pose.pt) | 640                   | 50.0                  | 81.0               | 52.4 Â± 0.5                     | 1.7 Â± 0.0                           | 2.9                | 7.6               |
| [YOLO11s-pos

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[BerriAI/litellm]]></title>
            <link>https://github.com/BerriAI/litellm</link>
            <guid>https://github.com/BerriAI/litellm</guid>
            <pubDate>Fri, 09 May 2025 00:04:19 GMT</pubDate>
            <description><![CDATA[Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq]]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/BerriAI/litellm">BerriAI/litellm</a></h1>
            <p>Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq]</p>
            <p>Language: Python</p>
            <p>Stars: 22,178</p>
            <p>Forks: 2,838</p>
            <p>Stars today: 54 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
        ğŸš… LiteLLM
    &lt;/h1&gt;
    &lt;p align=&quot;center&quot;&gt;
        &lt;p align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://render.com/deploy?repo=https://github.com/BerriAI/litellm&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;https://render.com/images/deploy-to-render-button.svg&quot; alt=&quot;Deploy to Render&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://railway.app/template/HLP0Ub?referralCode=jch2ME&quot;&gt;
          &lt;img src=&quot;https://railway.app/button.svg&quot; alt=&quot;Deploy on Railway&quot;&gt;
        &lt;/a&gt;
        &lt;/p&gt;
        &lt;p align=&quot;center&quot;&gt;Call all LLM APIs using the OpenAI format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.]
        &lt;br&gt;
    &lt;/p&gt;
&lt;h4 align=&quot;center&quot;&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/simple_proxy&quot; target=&quot;_blank&quot;&gt;LiteLLM Proxy Server (LLM Gateway)&lt;/a&gt; | &lt;a href=&quot;https://docs.litellm.ai/docs/hosted&quot; target=&quot;_blank&quot;&gt; Hosted Proxy (Preview)&lt;/a&gt; | &lt;a href=&quot;https://docs.litellm.ai/docs/enterprise&quot;target=&quot;_blank&quot;&gt;Enterprise Tier&lt;/a&gt;&lt;/h4&gt;
&lt;h4 align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://pypi.org/project/litellm/&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/pypi/v/litellm.svg&quot; alt=&quot;PyPI Version&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://www.ycombinator.com/companies/berriai&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/Y%20Combinator-W23-orange?style=flat-square&quot; alt=&quot;Y Combinator W23&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://wa.link/huol9n&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/static/v1?label=Chat%20on&amp;message=WhatsApp&amp;color=success&amp;logo=WhatsApp&amp;style=flat-square&quot; alt=&quot;Whatsapp&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://discord.gg/wuPM9dRgDw&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/static/v1?label=Chat%20on&amp;message=Discord&amp;color=blue&amp;logo=Discord&amp;style=flat-square&quot; alt=&quot;Discord&quot;&gt;
    &lt;/a&gt;
&lt;/h4&gt;

LiteLLM manages:

- Translate inputs to provider&#039;s `completion`, `embedding`, and `image_generation` endpoints
- [Consistent output](https://docs.litellm.ai/docs/completion/output), text responses will always be available at `[&#039;choices&#039;][0][&#039;message&#039;][&#039;content&#039;]`
- Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - [Router](https://docs.litellm.ai/docs/routing)
- Set Budgets &amp; Rate limits per project, api key, model [LiteLLM Proxy Server (LLM Gateway)](https://docs.litellm.ai/docs/simple_proxy)

[**Jump to LiteLLM Proxy (LLM Gateway) Docs**](https://github.com/BerriAI/litellm?tab=readme-ov-file#openai-proxy---docs) &lt;br&gt;
[**Jump to Supported LLM Providers**](https://github.com/BerriAI/litellm?tab=readme-ov-file#supported-providers-docs)

ğŸš¨ **Stable Release:** Use docker images with the `-stable` tag. These have undergone 12 hour load tests, before being published. [More information about the release cycle here](https://docs.litellm.ai/docs/proxy/release_cycle)

Support for more providers. Missing a provider or LLM Platform, raise a [feature request](https://github.com/BerriAI/litellm/issues/new?assignees=&amp;labels=enhancement&amp;projects=&amp;template=feature_request.yml&amp;title=%5BFeature%5D%3A+).

# Usage ([**Docs**](https://docs.litellm.ai/docs/))

&gt; [!IMPORTANT]
&gt; LiteLLM v1.0.0 now requires `openai&gt;=1.0.0`. Migration guide [here](https://docs.litellm.ai/docs/migration)  
&gt; LiteLLM v1.40.14+ now requires `pydantic&gt;=2.0.0`. No changes required.

&lt;a target=&quot;_blank&quot; href=&quot;https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/liteLLM_Getting_Started.ipynb&quot;&gt;
  &lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot;/&gt;
&lt;/a&gt;

```shell
pip install litellm
```

```python
from litellm import completion
import os

## set ENV variables
os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;your-openai-key&quot;
os.environ[&quot;ANTHROPIC_API_KEY&quot;] = &quot;your-anthropic-key&quot;

messages = [{ &quot;content&quot;: &quot;Hello, how are you?&quot;,&quot;role&quot;: &quot;user&quot;}]

# openai call
response = completion(model=&quot;openai/gpt-4o&quot;, messages=messages)

# anthropic call
response = completion(model=&quot;anthropic/claude-3-sonnet-20240229&quot;, messages=messages)
print(response)
```

### Response (OpenAI Format)

```json
{
    &quot;id&quot;: &quot;chatcmpl-565d891b-a42e-4c39-8d14-82a1f5208885&quot;,
    &quot;created&quot;: 1734366691,
    &quot;model&quot;: &quot;claude-3-sonnet-20240229&quot;,
    &quot;object&quot;: &quot;chat.completion&quot;,
    &quot;system_fingerprint&quot;: null,
    &quot;choices&quot;: [
        {
            &quot;finish_reason&quot;: &quot;stop&quot;,
            &quot;index&quot;: 0,
            &quot;message&quot;: {
                &quot;content&quot;: &quot;Hello! As an AI language model, I don&#039;t have feelings, but I&#039;m operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?&quot;,
                &quot;role&quot;: &quot;assistant&quot;,
                &quot;tool_calls&quot;: null,
                &quot;function_call&quot;: null
            }
        }
    ],
    &quot;usage&quot;: {
        &quot;completion_tokens&quot;: 43,
        &quot;prompt_tokens&quot;: 13,
        &quot;total_tokens&quot;: 56,
        &quot;completion_tokens_details&quot;: null,
        &quot;prompt_tokens_details&quot;: {
            &quot;audio_tokens&quot;: null,
            &quot;cached_tokens&quot;: 0
        },
        &quot;cache_creation_input_tokens&quot;: 0,
        &quot;cache_read_input_tokens&quot;: 0
    }
}
```

Call any model supported by a provider, with `model=&lt;provider_name&gt;/&lt;model_name&gt;`. There might be provider-specific details here, so refer to [provider docs for more information](https://docs.litellm.ai/docs/providers)

## Async ([Docs](https://docs.litellm.ai/docs/completion/stream#async-completion))

```python
from litellm import acompletion
import asyncio

async def test_get_response():
    user_message = &quot;Hello, how are you?&quot;
    messages = [{&quot;content&quot;: user_message, &quot;role&quot;: &quot;user&quot;}]
    response = await acompletion(model=&quot;openai/gpt-4o&quot;, messages=messages)
    return response

response = asyncio.run(test_get_response())
print(response)
```

## Streaming ([Docs](https://docs.litellm.ai/docs/completion/stream))

liteLLM supports streaming the model response back, pass `stream=True` to get a streaming iterator in response.  
Streaming is supported for all models (Bedrock, Huggingface, TogetherAI, Azure, OpenAI, etc.)

```python
from litellm import completion
response = completion(model=&quot;openai/gpt-4o&quot;, messages=messages, stream=True)
for part in response:
    print(part.choices[0].delta.content or &quot;&quot;)

# claude 2
response = completion(&#039;anthropic/claude-3-sonnet-20240229&#039;, messages, stream=True)
for part in response:
    print(part)
```

### Response chunk (OpenAI Format)

```json
{
    &quot;id&quot;: &quot;chatcmpl-2be06597-eb60-4c70-9ec5-8cd2ab1b4697&quot;,
    &quot;created&quot;: 1734366925,
    &quot;model&quot;: &quot;claude-3-sonnet-20240229&quot;,
    &quot;object&quot;: &quot;chat.completion.chunk&quot;,
    &quot;system_fingerprint&quot;: null,
    &quot;choices&quot;: [
        {
            &quot;finish_reason&quot;: null,
            &quot;index&quot;: 0,
            &quot;delta&quot;: {
                &quot;content&quot;: &quot;Hello&quot;,
                &quot;role&quot;: &quot;assistant&quot;,
                &quot;function_call&quot;: null,
                &quot;tool_calls&quot;: null,
                &quot;audio&quot;: null
            },
            &quot;logprobs&quot;: null
        }
    ]
}
```

## Logging Observability ([Docs](https://docs.litellm.ai/docs/observability/callbacks))

LiteLLM exposes pre defined callbacks to send data to Lunary, MLflow, Langfuse, DynamoDB, s3 Buckets, Helicone, Promptlayer, Traceloop, Athina, Slack

```python
from litellm import completion

## set env variables for logging tools (when using MLflow, no API key set up is required)
os.environ[&quot;LUNARY_PUBLIC_KEY&quot;] = &quot;your-lunary-public-key&quot;
os.environ[&quot;HELICONE_API_KEY&quot;] = &quot;your-helicone-auth-key&quot;
os.environ[&quot;LANGFUSE_PUBLIC_KEY&quot;] = &quot;&quot;
os.environ[&quot;LANGFUSE_SECRET_KEY&quot;] = &quot;&quot;
os.environ[&quot;ATHINA_API_KEY&quot;] = &quot;your-athina-api-key&quot;

os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;your-openai-key&quot;

# set callbacks
litellm.success_callback = [&quot;lunary&quot;, &quot;mlflow&quot;, &quot;langfuse&quot;, &quot;athina&quot;, &quot;helicone&quot;] # log input/output to lunary, langfuse, supabase, athina, helicone etc

#openai call
response = completion(model=&quot;openai/gpt-4o&quot;, messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hi ğŸ‘‹ - i&#039;m openai&quot;}])
```

# LiteLLM Proxy Server (LLM Gateway) - ([Docs](https://docs.litellm.ai/docs/simple_proxy))

Track spend + Load Balance across multiple projects

[Hosted Proxy (Preview)](https://docs.litellm.ai/docs/hosted)

The proxy provides:

1. [Hooks for auth](https://docs.litellm.ai/docs/proxy/virtual_keys#custom-auth)
2. [Hooks for logging](https://docs.litellm.ai/docs/proxy/logging#step-1---create-your-custom-litellm-callback-class)
3. [Cost tracking](https://docs.litellm.ai/docs/proxy/virtual_keys#tracking-spend)
4. [Rate Limiting](https://docs.litellm.ai/docs/proxy/users#set-rate-limits)

## ğŸ“– Proxy Endpoints - [Swagger Docs](https://litellm-api.up.railway.app/)


## Quick Start Proxy - CLI

```shell
pip install &#039;litellm[proxy]&#039;
```

### Step 1: Start litellm proxy

```shell
$ litellm --model huggingface/bigcode/starcoder

#INFO: Proxy running on http://0.0.0.0:4000
```

### Step 2: Make ChatCompletions Request to Proxy


&gt; [!IMPORTANT]
&gt; ğŸ’¡ [Use LiteLLM Proxy with Langchain (Python, JS), OpenAI SDK (Python, JS) Anthropic SDK, Mistral SDK, LlamaIndex, Instructor, Curl](https://docs.litellm.ai/docs/proxy/user_keys)  

```python
import openai # openai v1.0.0+
client = openai.OpenAI(api_key=&quot;anything&quot;,base_url=&quot;http://0.0.0.0:4000&quot;) # set proxy to base_url
# request sent to model set on litellm proxy, `litellm --model`
response = client.chat.completions.create(model=&quot;gpt-3.5-turbo&quot;, messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;this is a test request, write a short poem&quot;
    }
])

print(response)
```

## Proxy Key Management ([Docs](https://docs.litellm.ai/docs/proxy/virtual_keys))

Connect the proxy with a Postgres DB to create proxy keys

```bash
# Get the code
git clone https://github.com/BerriAI/litellm

# Go to folder
cd litellm

# Add the master key - you can change this after setup
echo &#039;LITELLM_MASTER_KEY=&quot;sk-1234&quot;&#039; &gt; .env

# Add the litellm salt key - you cannot change this after adding a model
# It is used to encrypt / decrypt your LLM API Key credentials
# We recommend - https://1password.com/password-generator/ 
# password generator to get a random hash for litellm salt key
echo &#039;LITELLM_SALT_KEY=&quot;sk-1234&quot;&#039; &gt; .env

source .env

# Start
docker-compose up
```


UI on `/ui` on your proxy server
![ui_3](https://github.com/BerriAI/litellm/assets/29436595/47c97d5e-b9be-4839-b28c-43d7f4f10033)

Set budgets and rate limits across multiple projects
`POST /key/generate`

### Request

```shell
curl &#039;http://0.0.0.0:4000/key/generate&#039; \
--header &#039;Authorization: Bearer sk-1234&#039; \
--header &#039;Content-Type: application/json&#039; \
--data-raw &#039;{&quot;models&quot;: [&quot;gpt-3.5-turbo&quot;, &quot;gpt-4&quot;, &quot;claude-2&quot;], &quot;duration&quot;: &quot;20m&quot;,&quot;metadata&quot;: {&quot;user&quot;: &quot;ishaan@berri.ai&quot;, &quot;team&quot;: &quot;core-infra&quot;}}&#039;
```

### Expected Response

```shell
{
    &quot;key&quot;: &quot;sk-kdEXbIqZRwEeEiHwdg7sFA&quot;, # Bearer token
    &quot;expires&quot;: &quot;2023-11-19T01:38:25.838000+00:00&quot; # datetime object
}
```

## Supported Providers ([Docs](https://docs.litellm.ai/docs/providers))

| Provider                                                                            | [Completion](https://docs.litellm.ai/docs/#basic-usage) | [Streaming](https://docs.litellm.ai/docs/completion/stream#streaming-responses) | [Async Completion](https://docs.litellm.ai/docs/completion/stream#async-completion) | [Async Streaming](https://docs.litellm.ai/docs/completion/stream#async-streaming) | [Async Embedding](https://docs.litellm.ai/docs/embedding/supported_embedding) | [Async Image Generation](https://docs.litellm.ai/docs/image_generation) |
|-------------------------------------------------------------------------------------|---------------------------------------------------------|---------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------|
| [openai](https://docs.litellm.ai/docs/providers/openai)                             | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 | âœ…                                                                             | âœ…                                                                       |
| [azure](https://docs.litellm.ai/docs/providers/azure)                               | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 | âœ…                                                                             | âœ…                                                                       |
| [AI/ML API](https://docs.litellm.ai/docs/providers/aiml)                               | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 | âœ…                                                                             | âœ…                                                                       |
| [aws - sagemaker](https://docs.litellm.ai/docs/providers/aws_sagemaker)             | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 | âœ…                                                                             |                                                                         |
| [aws - bedrock](https://docs.litellm.ai/docs/providers/bedrock)                     | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 | âœ…                                                                             |                                                                         |
| [google - vertex_ai](https://docs.litellm.ai/docs/providers/vertex)                 | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 | âœ…                                                                             | âœ…                                                                       |
| [google - palm](https://docs.litellm.ai/docs/providers/palm)                        | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 |                                                                               |                                                                         |
| [google AI Studio - gemini](https://docs.litellm.ai/docs/providers/gemini)          | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 |                                                                               |                                                                         |
| [mistral ai api](https://docs.litellm.ai/docs/providers/mistral)                    | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 | âœ…                                                                             |                                                                         |
| [cloudflare AI Workers](https://docs.litellm.ai/docs/providers/cloudflare_workers)  | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 |                                                                               |                                                                         |
| [cohere](https://docs.litellm.ai/docs/providers/cohere)                             | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 | âœ…                                                                             |                                                                         |
| [anthropic](https://docs.litellm.ai/docs/providers/anthropic)                       | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 |                                                                               |                                                                         |
| [empower](https://docs.litellm.ai/docs/providers/empower)                    | âœ…                                                      | âœ…                                                                              | âœ…                                                                                  | âœ…                                                                                |
| [huggingface](https://docs.litellm.ai/docs/providers/huggingface)                   | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 | âœ…                                                                             |                                                                         |
| [replicate](https://docs.litellm.ai/docs/providers/replicate)                       | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 |                                                                               |                                                                         |
| [together_ai](https://docs.litellm.ai/docs/p

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[eosphoros-ai/DB-GPT]]></title>
            <link>https://github.com/eosphoros-ai/DB-GPT</link>
            <guid>https://github.com/eosphoros-ai/DB-GPT</guid>
            <pubDate>Fri, 09 May 2025 00:04:18 GMT</pubDate>
            <description><![CDATA[AI Native Data App Development framework with AWEL(Agentic Workflow Expression Language) and Agents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/eosphoros-ai/DB-GPT">eosphoros-ai/DB-GPT</a></h1>
            <p>AI Native Data App Development framework with AWEL(Agentic Workflow Expression Language) and Agents</p>
            <p>Language: Python</p>
            <p>Stars: 16,383</p>
            <p>Forks: 2,226</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre># DB-GPT: AI Native Data App Development framework with AWEL(Agentic Workflow Expression Language) and Agents

&lt;p align=&quot;left&quot;&gt;
  &lt;img src=&quot;./assets/LOGO.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
    &lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT&quot;&gt;
        &lt;img alt=&quot;stars&quot; src=&quot;https://img.shields.io/github/stars/eosphoros-ai/db-gpt?style=social&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT&quot;&gt;
        &lt;img alt=&quot;forks&quot; src=&quot;https://img.shields.io/github/forks/eosphoros-ai/db-gpt?style=social&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;
      &lt;img alt=&quot;License: MIT&quot; src=&quot;https://img.shields.io/badge/License-MIT-yellow.svg&quot; /&gt;
    &lt;/a&gt;
     &lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT/releases&quot;&gt;
      &lt;img alt=&quot;Release Notes&quot; src=&quot;https://img.shields.io/github/release/eosphoros-ai/DB-GPT&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT/issues&quot;&gt;
      &lt;img alt=&quot;Open Issues&quot; src=&quot;https://img.shields.io/github/issues-raw/eosphoros-ai/DB-GPT&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://discord.gg/7uQnPuveTY&quot;&gt;
      &lt;img alt=&quot;Discord&quot; src=&quot;https://dcbadge.vercel.app/api/server/7uQnPuveTY?compact=true&amp;style=flat&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://join.slack.com/t/slack-inu2564/shared_invite/zt-29rcnyw2b-N~ubOD9kFc7b7MDOAM1otA&quot;&gt;
      &lt;img alt=&quot;Slack&quot; src=&quot;https://badgen.net/badge/Slack/Join%20DB-GPT/0abd59?icon=slack&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://codespaces.new/eosphoros-ai/DB-GPT&quot;&gt;
      &lt;img alt=&quot;Open in GitHub Codespaces&quot; src=&quot;https://github.com/codespaces/badge.svg&quot; /&gt;
    &lt;/a&gt;
  &lt;/p&gt;


[**ç®€ä½“ä¸­æ–‡**](README.zh.md) | [**æ—¥æœ¬èª**](README.ja.md) | [**Discord**](https://discord.gg/7uQnPuveTY) | [**Documents**](https://docs.dbgpt.site) | [**å¾®ä¿¡**](https://github.com/eosphoros-ai/DB-GPT/blob/main/README.zh.md#%E8%81%94%E7%B3%BB%E6%88%91%E4%BB%AC) | [**Community**](https://github.com/eosphoros-ai/community) | [**Paper**](https://arxiv.org/pdf/2312.17449.pdf)

&lt;/div&gt;

## What is DB-GPT?

ğŸ¤– **DB-GPT is an open source AI native data app development framework with AWEL(Agentic Workflow Expression Language) and agents**. 

The purpose is to build infrastructure in the field of large models, through the development of multiple technical capabilities such as multi-model management (SMMF), Text2SQL effect optimization, RAG framework and optimization, Multi-Agents framework collaboration, AWEL (agent workflow orchestration), etc. Which makes large model applications with data simpler and more convenient.

ğŸš€ **In the Data 3.0 era, based on models and databases, enterprises and developers can build their own bespoke applications with less code.**

### DISCKAIMER
- [disckaimer](./DISCKAIMER.md)

### AI-Native Data App 
---
- ğŸ”¥ğŸ”¥ğŸ”¥ [Released V0.7.0 | A set of significant upgrades](http://docs.dbgpt.cn/blog/db-gpt-v070-release)
  - [Support MCP Protocol](https://github.com/eosphoros-ai/DB-GPT/pull/2497)
  - [Support DeepSeek R1](https://github.com/deepseek-ai/DeepSeek-R1)
  - [Support QwQ-32B](https://huggingface.co/Qwen/QwQ-32B)
  - [Refactor the basic modules]()
    - [dbgpt-app](./packages/dbgpt-app)
    - [dbgpt-core](./packages/dbgpt-core)
    - [dbgpt-serve](./packages/dbgpt-serve)
    - [dbgpt-client](./packages/dbgpt-client)
    - [dbgpt-accelerator](./packages/dbgpt-accelerator)
    - [dbgpt-ext](./packages/dbgpt-ext)
---

![app_chat_v0 6](https://github.com/user-attachments/assets/a2f0a875-df8c-4f0d-89a3-eed321c02113)

![app_manage_chat_data_v0 6](https://github.com/user-attachments/assets/c8cc85bb-e3c2-4fab-8fb9-7b4b469d0611)

![chat_dashboard_display_v0 6](https://github.com/user-attachments/assets/b15d6ebe-54c4-4527-a16d-02fbbaf20dc9)

![agent_prompt_awel_v0 6](https://github.com/user-attachments/assets/40761507-a1e1-49d4-b49a-3dd9a5ea41cc)

## Contents
- [Introduction](#introduction)
- [Install](#install)
- [Features](#features)
- [Contribution](#contribution)
- [Contact](#contact-information)

## Introduction 
The architecture of DB-GPT is shown in the following figure:

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./assets/dbgpt.png&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

The core capabilities include the following parts:

- **RAG (Retrieval Augmented Generation)**: RAG is currently the most practically implemented and urgently needed domain. DB-GPT has already implemented a framework based on RAG, allowing users to build knowledge-based applications using the RAG capabilities of DB-GPT.

- **GBI (Generative Business Intelligence)**: Generative BI is one of the core capabilities of the DB-GPT project, providing the foundational data intelligence technology to build enterprise report analysis and business insights.

- **Fine-tuning Framework**: Model fine-tuning is an indispensable capability for any enterprise to implement in vertical and niche domains. DB-GPT provides a complete fine-tuning framework that integrates seamlessly with the DB-GPT project. In recent fine-tuning efforts, an accuracy rate based on the Spider dataset has been achieved at 82.5%.

- **Data-Driven Multi-Agents Framework**: DB-GPT offers a data-driven self-evolving multi-agents framework, aiming to continuously make decisions and execute based on data.

- **Data Factory**: The Data Factory is mainly about cleaning and processing trustworthy knowledge and data in the era of large models.

- **Data Sources**: Integrating various data sources to seamlessly connect production business data to the core capabilities of DB-GPT.

### SubModule
- [DB-GPT-Hub](https://github.com/eosphoros-ai/DB-GPT-Hub) Text-to-SQL workflow with high performance by applying Supervised Fine-Tuning (SFT) on Large Language Models (LLMs).

- [dbgpts](https://github.com/eosphoros-ai/dbgpts)  dbgpts is the official repository which contains some data appsã€AWEL operatorsã€AWEL workflow templates and agents which build upon DB-GPT.

#### Text2SQL Finetune
- support llms
  - [x] LLaMA
  - [x] LLaMA-2
  - [x] BLOOM
  - [x] BLOOMZ
  - [x] Falcon
  - [x] Baichuan
  - [x] Baichuan2
  - [x] InternLM
  - [x] Qwen
  - [x] XVERSE
  - [x] ChatGLM2

[More Information about Text2SQL finetune](https://github.com/eosphoros-ai/DB-GPT-Hub)

- [DB-GPT-Plugins](https://github.com/eosphoros-ai/DB-GPT-Plugins) DB-GPT Plugins that can run Auto-GPT plugin directly
- [GPT-Vis](https://github.com/eosphoros-ai/GPT-Vis) Visualization protocol

## Install 
![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&amp;logo=docker&amp;logoColor=white)
![Linux](https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&amp;logo=linux&amp;logoColor=black)
![macOS](https://img.shields.io/badge/mac%20os-000000?style=for-the-badge&amp;logo=macos&amp;logoColor=F0F0F0)
![Windows](https://img.shields.io/badge/Windows-0078D6?style=for-the-badge&amp;logo=windows&amp;logoColor=white)

[**Usage Tutorial**](http://docs.dbgpt.cn/docs/overview)
- [**Install**](http://docs.dbgpt.cn/docs/installation)
  - [Docker](http://docs.dbgpt.cn/docs/installation/docker)
  - [Source Code](http://docs.dbgpt.cn/docs/installation/sourcecode)
- [**Quickstart**](http://docs.dbgpt.cn/docs/quickstart)
- [**Application**](http://docs.dbgpt.cn/docs/operation_manual)
  - [Development Guide](http://docs.dbgpt.cn/docs/cookbook/app/data_analysis_app_develop) 
  - [App Usage](http://docs.dbgpt.cn/docs/application/app_usage)
  - [AWEL Flow Usage](http://docs.dbgpt.cn/docs/application/awel_flow_usage)
- [**Debugging**](http://docs.dbgpt.cn/docs/operation_manual/advanced_tutorial/debugging)
- [**Advanced Usage**](http://docs.dbgpt.cn/docs/application/advanced_tutorial/cli)
  - [SMMF](http://docs.dbgpt.cn/docs/application/advanced_tutorial/smmf)
  - [Finetune](http://docs.dbgpt.cn/docs/application/fine_tuning_manual/dbgpt_hub)
  - [AWEL](http://docs.dbgpt.cn/docs/awel/tutorial)


## Features

At present, we have introduced several key features to showcase our current capabilities:
- **Private Domain Q&amp;A &amp; Data Processing**

  The DB-GPT project offers a range of functionalities designed to improve knowledge base construction and enable efficient storage and retrieval of both structured and unstructured data. These functionalities include built-in support for uploading multiple file formats, the ability to integrate custom data extraction plug-ins, and unified vector storage and retrieval capabilities for effectively managing large volumes of information.

- **Multi-Data Source &amp; GBI(Generative Business intelligence)**

  The DB-GPT project facilitates seamless natural language interaction with diverse data sources, including Excel, databases, and data warehouses. It simplifies the process of querying and retrieving information from these sources, empowering users to engage in intuitive conversations and gain insights. Moreover, DB-GPT supports the generation of analytical reports, providing users with valuable data summaries and interpretations.

- **Multi-Agents&amp;Plugins**

  It offers support for custom plug-ins to perform various tasks and natively integrates the Auto-GPT plug-in model. The Agents protocol adheres to the Agent Protocol standard.

- **Automated Fine-tuning text2SQL**

  We&#039;ve also developed an automated fine-tuning lightweight framework centred on large language models (LLMs), Text2SQL datasets, LoRA/QLoRA/Pturning, and other fine-tuning methods. This framework simplifies Text-to-SQL fine-tuning, making it as straightforward as an assembly line process. [DB-GPT-Hub](https://github.com/eosphoros-ai/DB-GPT-Hub)

- **SMMF(Service-oriented Multi-model Management Framework)**

  We offer extensive model support, including dozens of large language models (LLMs) from both open-source and API agents, such as LLaMA/LLaMA2, Baichuan, ChatGLM, Wenxin, Tongyi, Zhipu, and many more. 

  - News
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen3-235B-A22B](https://huggingface.co/Qwen/Qwen3-235B-A22B)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen3-30B-A3B](https://huggingface.co/Qwen/Qwen3-30B-A3B)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen3-32B](https://huggingface.co/Qwen/Qwen3-32B)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [GLM-Z1-32B-0414](https://huggingface.co/THUDM/GLM-Z1-32B-0414)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [GLM-4-32B-0414](https://huggingface.co/THUDM/GLM-4-32B-0414)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [QwQ-32B](https://huggingface.co/Qwen/QwQ-32B)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [DeepSeek-V3](https://huggingface.co/deepseek-ai/DeepSeek-V3)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [DeepSeek-R1-Distill-Llama-70B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [DeepSeek-R1-Distill-Qwen-32B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [DeepSeek-R1-Distill-Qwen-14B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [DeepSeek-R1-Distill-Llama-8B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [DeepSeek-R1-Distill-Qwen-7B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [DeepSeek-R1-Distill-Qwen-1.5B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-Coder-32B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-Coder-14B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-72B-Instruct](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-32B-Instruct](https://huggingface.co/Qwen/Qwen2.5-32B-Instruct)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-14B-Instruct](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-Coder-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2.5-Coder-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Meta-Llama-3.1-405B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Meta-Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [gemma-2-27b-it](https://huggingface.co/google/gemma-2-27b-it)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [gemma-2-9b-it](https://huggingface.co/google/gemma-2-9b-it)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [DeepSeek-Coder-V2-Instruct](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [DeepSeek-Coder-V2-Lite-Instruct](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2-57B-A14B-Instruct](https://huggingface.co/Qwen/Qwen2-57B-A14B-Instruct)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2-72B-Instruct](https://huggingface.co/Qwen/Qwen2-72B-Instruct)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen2-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2-0.5B-Instruct)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [glm-4-9b-chat](https://huggingface.co/THUDM/glm-4-9b-chat)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Phi-3](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Yi-1.5-34B-Chat](https://huggingface.co/01-ai/Yi-1.5-34B-Chat)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Yi-1.5-9B-Chat](https://huggingface.co/01-ai/Yi-1.5-9B-Chat)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Yi-1.5-6B-Chat](https://huggingface.co/01-ai/Yi-1.5-6B-Chat)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen1.5-110B-Chat](https://huggingface.co/Qwen/Qwen1.5-110B-Chat)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen1.5-MoE-A2.7B-Chat](https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B-Chat)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Meta-Llama-3-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [CodeQwen1.5-7B-Chat](https://huggingface.co/Qwen/CodeQwen1.5-7B-Chat)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen1.5-32B-Chat](https://huggingface.co/Qwen/Qwen1.5-32B-Chat)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Starling-LM-7B-beta](https://huggingface.co/Nexusflow/Starling-LM-7B-beta)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [gemma-7b-it](https://huggingface.co/google/gemma-7b-it)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [gemma-2b-it](https://huggingface.co/google/gemma-2b-it)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [SOLAR-10.7B](https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Mixtral-8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Qwen-72B-Chat](https://huggingface.co/Qwen/Qwen-72B-Chat)
    - ğŸ”¥ğŸ”¥ğŸ”¥  [Yi-34B-Chat](https://huggingface.co/01-ai/Yi-34B-Chat)
  - [More Supported LLMs](http://docs.dbgpt.site/docs/modules/smmf)

- **Privacy and Security**
  
  We ensure the privacy and security of data through the implementation of various technologies, including privatized large models and proxy desensitization.

- Support Datasources
  - [Datasources](http://docs.dbgpt.cn/docs/modules/connections)

## Image
ğŸŒ [AutoDL Image](https://www.codewithgpu.com/i/eosphoros-ai/DB-GPT/dbgpt)


### Language Switching
    In the .env configuration file, modify the LANGUAGE parameter to switch to different languages. The default is English (Chinese: zh, English: en, other languages to be added later).

## Contribution

- To check detailed guidelines for new contributions, please refer [how to contribute](https://github.com/eosphoros-ai/DB-GPT/blob/main/CONTRIBUTING.md)

### Contributors Wall
&lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=eosphoros-ai/DB-GPT&amp;max=200&quot; /&gt;
&lt;/a&gt;


## Licence
The MIT License (MIT)

## Citation
If you want to understand the overall architecture of DB-GPT, please cite &lt;a href=&quot;https://arxiv.org/abs/2312.17449&quot; target=&quot;_blank&quot;&gt;paper&lt;/a&gt; and &lt;a href=&quot;https:// arxiv.org/abs/2404.10209&quot; target=&quot;_blank&quot;&gt;Paper&lt;/a&gt;

If you want to learn about using DB-GPT for Agent development, please cite the &lt;a href=&quot;https://arxiv.org/abs/2412.13520&quot; target=&quot;_blank&quot;&gt;paper&lt;/a&gt;
```bibtex
@article{xue2023dbgpt,
      title={DB-GPT: Empowering Database Interactions with Private Large Language Models}, 
      author={Siqiao Xue and Caigao Jiang and Wenhui Shi and Fangyin Cheng and Keting Chen and Hongjun Yang and Zhiping Zhang and Jianshan He and Hongyang Zhang and Ganglin Wei and Wang Zhao and Fan Zhou and Danrui Qi and Hong Yi and Shaodong Liu and Faqiang Chen},
      year={2023},
      journal={arXiv preprint arXiv:2312.17449},
      url={https://arxiv.org/abs/2312.17449}
}
@misc{huang2024romasrolebasedmultiagentdatabase,
      title={ROMAS: A Role-Based Multi-Agent System for Database monitoring and Planning}, 
      author={Yi Huang and Fangyin Cheng and Fan Zhou and Jiahui Li and Jian Gong and Hongjun Yang and Zhidong Fan and Caigao Jiang and Siqiao Xue and Faqiang Chen},
      year={2024},
      eprint={2412.13520},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2412.13520}, 
}
@inproceedings{xue2024demonstration,
      title={Demonstration of DB-GPT: Next Generation Data Interaction System Empowered by Large Language Models}, 
      author={Siqiao Xue and Danrui Qi and Caigao Jiang and Wenhui Shi and Fangyin Cheng and Keting Chen and Hongjun Yang and Zhiping Zhang and Jianshan He and Hongyang Zhang and Ganglin Wei and Wang Zhao and Fan Zhou and Hong Yi and Shaodong Liu and Hongjun Yang and Faqiang Chen},
      year={2024},
      booktitle = &quot;Proceedings of the VLDB Endowment&quot;,
      url={https://arxiv.org/abs/2404.10209}
}
```


## Contact Information
We are working on building a community, if you have any ideas for building the community, feel free to contact us.
[![](https://dcbadge.vercel.app/api/server/7uQnPuveTY?compact=true&amp;style=flat)](https://discord.gg/7uQnPuveTY)

[![Star History Chart](https://api.star-history.com/svg?repos=csunny/DB-GPT&amp;type=Date)](https://star-history.com/#csunny/DB-GPT)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Byaidu/PDFMathTranslate]]></title>
            <link>https://github.com/Byaidu/PDFMathTranslate</link>
            <guid>https://github.com/Byaidu/PDFMathTranslate</guid>
            <pubDate>Fri, 09 May 2025 00:04:17 GMT</pubDate>
            <description><![CDATA[PDF scientific paper translation with preserved formats - åŸºäº AI å®Œæ•´ä¿ç•™æ’ç‰ˆçš„ PDF æ–‡æ¡£å…¨æ–‡åŒè¯­ç¿»è¯‘ï¼Œæ”¯æŒ Google/DeepL/Ollama/OpenAI ç­‰æœåŠ¡ï¼Œæä¾› CLI/GUI/MCP/Docker/Zotero]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Byaidu/PDFMathTranslate">Byaidu/PDFMathTranslate</a></h1>
            <p>PDF scientific paper translation with preserved formats - åŸºäº AI å®Œæ•´ä¿ç•™æ’ç‰ˆçš„ PDF æ–‡æ¡£å…¨æ–‡åŒè¯­ç¿»è¯‘ï¼Œæ”¯æŒ Google/DeepL/Ollama/OpenAI ç­‰æœåŠ¡ï¼Œæä¾› CLI/GUI/MCP/Docker/Zotero</p>
            <p>Language: Python</p>
            <p>Stars: 22,818</p>
            <p>Forks: 1,954</p>
            <p>Stars today: 162 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

English | [ç®€ä½“ä¸­æ–‡](docs/README_zh-CN.md) | [ç¹é«”ä¸­æ–‡](docs/README_zh-TW.md) | [æ—¥æœ¬èª](docs/README_ja-JP.md) | [í•œêµ­ì–´](docs/README_ko-KR.md)

&lt;img src=&quot;./docs/images/banner.png&quot; width=&quot;320px&quot;  alt=&quot;PDF2ZH&quot;/&gt;

&lt;h2 id=&quot;title&quot;&gt;PDFMathTranslate&lt;/h2&gt;

&lt;p&gt;
  &lt;!-- PyPI --&gt;
  &lt;a href=&quot;https://pypi.org/project/pdf2zh/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/pdf2zh&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pepy.tech/projects/pdf2zh&quot;&gt;
    &lt;img src=&quot;https://static.pepy.tech/badge/pdf2zh&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://hub.docker.com/repository/docker/byaidu/pdf2zh&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/docker/pulls/byaidu/pdf2zh&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://gitcode.com/Byaidu/PDFMathTranslate/overview&quot;&gt;
    &lt;img src=&quot;https://gitcode.com/Byaidu/PDFMathTranslate/star/badge.svg&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://huggingface.co/spaces/reycn/PDFMathTranslate-Docker&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97-Online%20Demo-FF9E0D&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.modelscope.cn/studios/AI-ModelScope/PDFMathTranslate&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/ModelScope-Demo-blue&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/Byaidu/PDFMathTranslate/pulls&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/contributions-welcome-green&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://t.me/+Z9_SgnxmsmA5NzBl&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Telegram-2CA5E0?style=flat-squeare&amp;logo=telegram&amp;logoColor=white&quot;&gt;&lt;/a&gt;
  &lt;!-- License --&gt;
  &lt;a href=&quot;./LICENSE&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/license/Byaidu/PDFMathTranslate&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;a href=&quot;https://trendshift.io/repositories/12424&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12424&quot; alt=&quot;Byaidu%2FPDFMathTranslate | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;

PDF scientific paper translation and bilingual comparison.

- ğŸ“Š Preserve formulas, charts, table of contents, and annotations _([preview](#preview))_.
- ğŸŒ Support [multiple languages](#language), and diverse [translation services](#services).
- ğŸ¤– Provides [commandline tool](#usage), [interactive user interface](#gui), and [Docker](#docker)

Feel free to provide feedback in [GitHub Issues](https://github.com/Byaidu/PDFMathTranslate/issues) or [Telegram Group](https://t.me/+Z9_SgnxmsmA5NzBl).

For details on how to contribute, please consult the [Contribution Guide](https://github.com/Byaidu/PDFMathTranslate/wiki/Contribution-Guide---%E8%B4%A1%E7%8C%AE%E6%8C%87%E5%8D%97).

&lt;h2 id=&quot;updates&quot;&gt;Updates&lt;/h2&gt;

- [Mar. 3, 2025] Experimental support for the new backend [BabelDOC](https://github.com/funstory-ai/BabelDOC) WebUI added as an experimental option (by [@awwaawwa](https://github.com/awwaawwa))
- [Feb. 22 2025] Better release CI and well-packaged windows-amd64 exe (by [@awwaawwa](https://github.com/awwaawwa))
- [Dec. 24 2024] The translator now supports local models on [Xinference](https://github.com/xorbitsai/inference) _(by [@imClumsyPanda](https://github.com/imClumsyPanda))_
- [Dec. 19 2024] Non-PDF/A documents are now supported using `-cp` _(by [@reycn](https://github.com/reycn))_
- [Dec. 13 2024] Additional support for backend by _(by [@YadominJinta](https://github.com/YadominJinta))_
- [Dec. 10 2024] The translator now supports OpenAI models on Azure _(by [@yidasanqian](https://github.com/yidasanqian))_

&lt;h2 id=&quot;preview&quot;&gt;Preview&lt;/h2&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;./docs/images/preview.gif&quot; width=&quot;80%&quot;/&gt;
&lt;/div&gt;

&lt;h2 id=&quot;demo&quot;&gt;Online Service ğŸŒŸ&lt;/h2&gt;

You can try our application out using either of the following demos:

- [Public free service](https://pdf2zh.com/) online without installation _(recommended)_.
- [Immersive Translate - BabelDOC](https://app.immersivetranslate.com/babel-doc/) 1000 free pages per month. _(recommended)_
- [Demo hosted on HuggingFace](https://huggingface.co/spaces/reycn/PDFMathTranslate-Docker)
- [Demo hosted on ModelScope](https://www.modelscope.cn/studios/AI-ModelScope/PDFMathTranslate) without installation.

Note that the computing resources of the demo are limited, so please avoid abusing them.

&lt;h2 id=&quot;install&quot;&gt;Installation and Usage&lt;/h2&gt;

### Methods

For different use cases, we provide distinct methods to use our program:

&lt;details open&gt;
  &lt;summary&gt;1. UV install&lt;/summary&gt;

1. Python installed (3.10 &lt;= version &lt;= 3.12)
2. Install our package:

   ```bash
   pip install uv
   uv tool install --python 3.12 pdf2zh
   ```

3. Execute translation, files generated in [current working directory](https://chatgpt.com/share/6745ed36-9acc-800e-8a90-59204bd13444):

   ```bash
   pdf2zh document.pdf
   ```

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;2. Windows exe&lt;/summary&gt;

1. Download pdf2zh-version-win64.zip from [release page](https://github.com/Byaidu/PDFMathTranslate/releases)

2. Unzip and double-click `pdf2zh.exe` to run.

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;3. Graphic user interface&lt;/summary&gt;
1. Python installed (3.10 &lt;= version &lt;= 3.12)
2. Install our package:

```bash
pip install pdf2zh
```

3. Start using in browser:

   ```bash
   pdf2zh -i
   ```

4. If your browswer has not been started automatically, goto

   ```bash
   http://localhost:7860/
   ```

   &lt;img src=&quot;./docs/images/gui.gif&quot; width=&quot;500&quot;/&gt;

See [documentation for GUI](./docs/README_GUI.md) for more details.

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;4. Docker&lt;/summary&gt;

1. Pull and run:

   ```bash
   docker pull byaidu/pdf2zh
   docker run -d -p 7860:7860 byaidu/pdf2zh
   ```

2. Open in browser:

   ```
   http://localhost:7860/
   ```

For docker deployment on cloud service:

&lt;div&gt;
&lt;a href=&quot;https://www.heroku.com/deploy?template=https://github.com/Byaidu/PDFMathTranslate&quot;&gt;
  &lt;img src=&quot;https://www.herokucdn.com/deploy/button.svg&quot; alt=&quot;Deploy&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://render.com/deploy&quot;&gt;
  &lt;img src=&quot;https://render.com/images/deploy-to-render-button.svg&quot; alt=&quot;Deploy to Koyeb&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://zeabur.com/templates/5FQIGX?referralCode=reycn&quot;&gt;
  &lt;img src=&quot;https://zeabur.com/button.svg&quot; alt=&quot;Deploy on Zeabur&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://app.koyeb.com/deploy?type=git&amp;builder=buildpack&amp;repository=github.com/Byaidu/PDFMathTranslate&amp;branch=main&amp;name=pdf-math-translate&quot;&gt;
  &lt;img src=&quot;https://www.koyeb.com/static/images/deploy/button.svg&quot; alt=&quot;Deploy to Koyeb&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;5. Zotero Plugin&lt;/summary&gt;


See [Zotero PDF2zh](https://github.com/guaguastandup/zotero-pdf2zh) for more details.

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;6. Commandline&lt;/summary&gt;

1. Python installed (3.10 &lt;= version &lt;= 3.12)
2. Install our package:

   ```bash
   pip install pdf2zh
   ```

3. Execute translation, files generated in [current working directory](https://chatgpt.com/share/6745ed36-9acc-800e-8a90-59204bd13444):

   ```bash
   pdf2zh document.pdf
   ```

&lt;/details&gt;

&gt; [!TIP]
&gt;
&gt; - If you&#039;re using Windows and cannot open the file after downloading, please install [vc_redist.x64.exe](https://aka.ms/vs/17/release/vc_redist.x64.exe) and try again.
&gt;
&gt; - If you cannot access Docker Hub, please try the image on [GitHub Container Registry](https://github.com/Byaidu/PDFMathTranslate/pkgs/container/pdfmathtranslate).
&gt; ```bash
&gt; docker pull ghcr.io/byaidu/pdfmathtranslate
&gt; docker run -d -p 7860:7860 ghcr.io/byaidu/pdfmathtranslate
&gt; ```

### Unable to install?

The present program needs an AI model(`wybxc/DocLayout-YOLO-DocStructBench-onnx`) before working and some users are not able to download due to network issues. If you have a problem with downloading this model, we provide a workaround using the following environment variable:

```shell
set HF_ENDPOINT=https://hf-mirror.com
```

For PowerShell user:

```shell
$env:HF_ENDPOINT = https://hf-mirror.com
```

If the solution does not work to you / you encountered other issues, please refer to [frequently asked questions](https://github.com/Byaidu/PDFMathTranslate/wiki#-faq--%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98).

&lt;h2 id=&quot;usage&quot;&gt;Advanced Options&lt;/h2&gt;

Execute the translation command in the command line to generate the translated document `example-mono.pdf` and the bilingual document `example-dual.pdf` in the current working directory. Use Google as the default translation service. More support translation services can find [HERE](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#services).

&lt;img src=&quot;./docs/images/cmd.explained.png&quot; width=&quot;580px&quot;  alt=&quot;cmd&quot;/&gt;

In the following table, we list all advanced options for reference:

| Option                | Function                                                                                                      | Example                                        |
| --------------------- | ------------------------------------------------------------------------------------------------------------- | ---------------------------------------------- |
| files                 | Local files                                                                                                   | `pdf2zh ~/local.pdf`                           |
| links                 | Online files                                                                                                  | `pdf2zh http://arxiv.org/paper.pdf`            |
| `-i`                  | [Enter GUI](#gui)                                                                                             | `pdf2zh -i`                                    |
| `-p`                  | [Partial document translation](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#partial) | `pdf2zh example.pdf -p 1`                      |
| `-li`                 | [Source language](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#languages)            | `pdf2zh example.pdf -li en`                    |
| `-lo`                 | [Target language](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#languages)            | `pdf2zh example.pdf -lo zh`                    |
| `-s`                  | [Translation service](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#services)         | `pdf2zh example.pdf -s deepl`                  |
| `-t`                  | [Multi-threads](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#threads)                | `pdf2zh example.pdf -t 1`                      |
| `-o`                  | Output dir                                                                                                    | `pdf2zh example.pdf -o output`                 |
| `-f`, `-c`            | [Exceptions](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#exceptions)                | `pdf2zh example.pdf -f &quot;(MS.*)&quot;`               |
| `-cp`                 | Compatibility Mode                                                                                            | `pdf2zh example.pdf --compatible`              |
| `--skip-subset-fonts` | [Skip font subset](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#font-subset)         | `pdf2zh example.pdf --skip-subset-fonts`       |
| `--ignore-cache`      | [Ignore translate cache](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#cache)         | `pdf2zh example.pdf --ignore-cache`            |
| `--share`             | Public link                                                                                                   | `pdf2zh -i --share`                            |
| `--authorized`        | [Authorization](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#auth)                   | `pdf2zh -i --authorized users.txt [auth.html]` |
| `--prompt`            | [Custom Prompt](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#prompt)                 | `pdf2zh --prompt [prompt.txt]`                 |
| `--onnx`              | [Use Custom DocLayout-YOLO ONNX model]                                                                        | `pdf2zh --onnx [onnx/model/path]`              |
| `--serverport`        | [Use Custom WebUI port]                                                                                       | `pdf2zh --serverport 7860`                     |
| `--dir`               | [batch translate]                                                                                             | `pdf2zh --dir /path/to/translate/`             |
| `--config`            | [configuration file](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#cofig)             | `pdf2zh --config /path/to/config/config.json`  |
| `--serverport`        | [custom gradio server port]                                                                                   | `pdf2zh --serverport 7860`                     |
| `--babeldoc`          | Use Experimental backend [BabelDOC](https://funstory-ai.github.io/BabelDOC/) to translate                     | `pdf2zh --babeldoc` -s openai example.pdf      |
| `--mcp`               | Enable MCP STDIO mode                                                                                         | `pdf2zh --mcp`                                 |
| `--sse`               | Enable MCP SSE mode                                                                                           | `pdf2zh --mcp --sse`                           |

For detailed explanations, please refer to our document about [Advanced Usage](./docs/ADVANCED.md) for a full list of each option.

&lt;h2 id=&quot;downstream&quot;&gt;Secondary Development (APIs)&lt;/h2&gt;

For downstream applications, please refer to our document about [API Details](./docs/APIS.md) for futher information about:

- [Python API](./docs/APIS.md#api-python), how to use the program in other Python programs
- [HTTP API](./docs/APIS.md#api-http), how to communicate with a server with the program installed

&lt;h2 id=&quot;todo&quot;&gt;TODOs&lt;/h2&gt;

- [ ] Parse layout with DocLayNet based models, [PaddleX](https://github.com/PaddlePaddle/PaddleX/blob/17cc27ac3842e7880ca4aad92358d3ef8555429a/paddlex/repo_apis/PaddleDetection_api/object_det/official_categories.py#L81), [PaperMage](https://github.com/allenai/papermage/blob/9cd4bb48cbedab45d0f7a455711438f1632abebe/README.md?plain=1#L102), [SAM2](https://github.com/facebookresearch/sam2)

- [ ] Fix page rotation, table of contents, format of lists

- [ ] Fix pixel formula in old papers

- [ ] Async retry except KeyboardInterrupt

- [ ] Knuthâ€“Plass algorithm for western languages

- [ ] Support non-PDF/A files

- [ ] Plugins of [Zotero](https://github.com/zotero/zotero) and [Obsidian](https://github.com/obsidianmd/obsidian-releases)

&lt;h2 id=&quot;acknowledgement&quot;&gt;Acknowledgements&lt;/h2&gt;

- [Immersive Translation](https://immersivetranslate.com) sponsors monthly Pro membership redemption codes for active contributors to this project, see details at: [CONTRIBUTOR_REWARD.md](https://github.com/funstory-ai/BabelDOC/blob/main/docs/CONTRIBUTOR_REWARD.md)

- New backend: [BabelDOC](https://github.com/funstory-ai/BabelDOC)

- Document merging: [PyMuPDF](https://github.com/pymupdf/PyMuPDF)

- Document parsing: [Pdfminer.six](https://github.com/pdfminer/pdfminer.six)

- Document extraction: [MinerU](https://github.com/opendatalab/MinerU)

- Document Preview: [Gradio PDF](https://github.com/freddyaboulton/gradio-pdf)

- Multi-threaded translation: [MathTranslate](https://github.com/SUSYUSTC/MathTranslate)

- Layout parsing: [DocLayout-YOLO](https://github.com/opendatalab/DocLayout-YOLO)

- Document standard: [PDF Explained](https://zxyle.github.io/PDF-Explained/), [PDF Cheat Sheets](https://pdfa.org/resource/pdf-cheat-sheets/)

- Multilingual Font: [Go Noto Universal](https://github.com/satbyy/go-noto-universal)

&lt;h2 id=&quot;contrib&quot;&gt;Contributors&lt;/h2&gt;

&lt;a href=&quot;https://github.com/Byaidu/PDFMathTranslate/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://opencollective.com/PDFMathTranslate/contributors.svg?width=890&amp;button=false&quot; /&gt;
&lt;/a&gt;

![Alt](https://repobeats.axiom.co/api/embed/dfa7583da5332a11468d686fbd29b92320a6a869.svg &quot;Repobeats analytics image&quot;)

&lt;h2 id=&quot;star_hist&quot;&gt;Star History&lt;/h2&gt;

&lt;a href=&quot;https://star-history.com/#Byaidu/PDFMathTranslate&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&amp;type=Date&quot;/&gt;
 &lt;/picture&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Lightricks/LTX-Video]]></title>
            <link>https://github.com/Lightricks/LTX-Video</link>
            <guid>https://github.com/Lightricks/LTX-Video</guid>
            <pubDate>Fri, 09 May 2025 00:04:16 GMT</pubDate>
            <description><![CDATA[Official repository for LTX-Video]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Lightricks/LTX-Video">Lightricks/LTX-Video</a></h1>
            <p>Official repository for LTX-Video</p>
            <p>Language: Python</p>
            <p>Stars: 4,045</p>
            <p>Forks: 332</p>
            <p>Stars today: 173 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# LTX-Video

This is the official repository for LTX-Video.

[Website](https://www.lightricks.com/ltxv) |
[Model](https://huggingface.co/Lightricks/LTX-Video) |
[Demo](https://app.ltx.studio/ltx-video) |
[Paper](https://arxiv.org/abs/2501.00103) |
[Discord](https://discord.gg/Mn8BRgUKKy)

&lt;/div&gt;

## Table of Contents

- [Introduction](#introduction)
- [What&#039;s new](#news)
- [Quick Start Guide](#quick-start-guide)
  - [Online demo](#online-demo)
  - [Run locally](#run-locally)
    - [Installation](#installation)
    - [Inference](#inference)
  - [ComfyUI Integration](#comfyui-integration)
  - [Diffusers Integration](#diffusers-integration)
- [Model User Guide](#model-user-guide)
- [Community Contribution](#community-contribution)
- [Training](#trining)
- [Join Us!](#join-us)
- [Acknowledgement](#acknowledgement)

# Introduction

LTX-Video is the first DiT-based video generation model that can generate high-quality videos in *real-time*.
It can generate 30 FPS videos at 1216Ã—704 resolution, faster than it takes to watch them.
The model is trained on a large-scale dataset of diverse videos and can generate high-resolution videos
with realistic and diverse content.

The model supports text-to-image, image-to-video, keyframe-based animation, video extension (both forward and backward), video-to-video transformations, and any combination of these features.

| | | | |
|:---:|:---:|:---:|:---:|
| ![example1](./docs/_static/ltx-video_example_00001.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A woman with long brown hair and light skin smiles at another woman...&lt;/summary&gt;A woman with long brown hair and light skin smiles at another woman with long blonde hair. The woman with brown hair wears a black jacket and has a small, barely noticeable mole on her right cheek. The camera angle is a close-up, focused on the woman with brown hair&#039;s face. The lighting is warm and natural, likely from the setting sun, casting a soft glow on the scene. The scene appears to be real-life footage.&lt;/details&gt; | ![example2](./docs/_static/ltx-video_example_00002.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A woman walks away from a white Jeep parked on a city street at night...&lt;/summary&gt;A woman walks away from a white Jeep parked on a city street at night, then ascends a staircase and knocks on a door. The woman, wearing a dark jacket and jeans, walks away from the Jeep parked on the left side of the street, her back to the camera; she walks at a steady pace, her arms swinging slightly by her sides; the street is dimly lit, with streetlights casting pools of light on the wet pavement; a man in a dark jacket and jeans walks past the Jeep in the opposite direction; the camera follows the woman from behind as she walks up a set of stairs towards a building with a green door; she reaches the top of the stairs and turns left, continuing to walk towards the building; she reaches the door and knocks on it with her right hand; the camera remains stationary, focused on the doorway; the scene is captured in real-life footage.&lt;/details&gt; | ![example3](./docs/_static/ltx-video_example_00003.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A woman with blonde hair styled up, wearing a black dress...&lt;/summary&gt;A woman with blonde hair styled up, wearing a black dress with sequins and pearl earrings, looks down with a sad expression on her face. The camera remains stationary, focused on the woman&#039;s face. The lighting is dim, casting soft shadows on her face. The scene appears to be from a movie or TV show.&lt;/details&gt; | ![example4](./docs/_static/ltx-video_example_00004.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;The camera pans over a snow-covered mountain range...&lt;/summary&gt;The camera pans over a snow-covered mountain range, revealing a vast expanse of snow-capped peaks and valleys.The mountains are covered in a thick layer of snow, with some areas appearing almost white while others have a slightly darker, almost grayish hue. The peaks are jagged and irregular, with some rising sharply into the sky while others are more rounded. The valleys are deep and narrow, with steep slopes that are also covered in snow. The trees in the foreground are mostly bare, with only a few leaves remaining on their branches. The sky is overcast, with thick clouds obscuring the sun. The overall impression is one of peace and tranquility, with the snow-covered mountains standing as a testament to the power and beauty of nature.&lt;/details&gt; |
| ![example5](./docs/_static/ltx-video_example_00005.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A woman with light skin, wearing a blue jacket and a black hat...&lt;/summary&gt;A woman with light skin, wearing a blue jacket and a black hat with a veil, looks down and to her right, then back up as she speaks; she has brown hair styled in an updo, light brown eyebrows, and is wearing a white collared shirt under her jacket; the camera remains stationary on her face as she speaks; the background is out of focus, but shows trees and people in period clothing; the scene is captured in real-life footage.&lt;/details&gt; | ![example6](./docs/_static/ltx-video_example_00006.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A man in a dimly lit room talks on a vintage telephone...&lt;/summary&gt;A man in a dimly lit room talks on a vintage telephone, hangs up, and looks down with a sad expression. He holds the black rotary phone to his right ear with his right hand, his left hand holding a rocks glass with amber liquid. He wears a brown suit jacket over a white shirt, and a gold ring on his left ring finger. His short hair is neatly combed, and he has light skin with visible wrinkles around his eyes. The camera remains stationary, focused on his face and upper body. The room is dark, lit only by a warm light source off-screen to the left, casting shadows on the wall behind him. The scene appears to be from a movie.&lt;/details&gt; | ![example7](./docs/_static/ltx-video_example_00007.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A prison guard unlocks and opens a cell door...&lt;/summary&gt;A prison guard unlocks and opens a cell door to reveal a young man sitting at a table with a woman. The guard, wearing a dark blue uniform with a badge on his left chest, unlocks the cell door with a key held in his right hand and pulls it open; he has short brown hair, light skin, and a neutral expression. The young man, wearing a black and white striped shirt, sits at a table covered with a white tablecloth, facing the woman; he has short brown hair, light skin, and a neutral expression. The woman, wearing a dark blue shirt, sits opposite the young man, her face turned towards him; she has short blonde hair and light skin. The camera remains stationary, capturing the scene from a medium distance, positioned slightly to the right of the guard. The room is dimly lit, with a single light fixture illuminating the table and the two figures. The walls are made of large, grey concrete blocks, and a metal door is visible in the background. The scene is captured in real-life footage.&lt;/details&gt; | ![example8](./docs/_static/ltx-video_example_00008.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A woman with blood on her face and a white tank top...&lt;/summary&gt;A woman with blood on her face and a white tank top looks down and to her right, then back up as she speaks. She has dark hair pulled back, light skin, and her face and chest are covered in blood. The camera angle is a close-up, focused on the woman&#039;s face and upper torso. The lighting is dim and blue-toned, creating a somber and intense atmosphere. The scene appears to be from a movie or TV show.&lt;/details&gt; |
| ![example9](./docs/_static/ltx-video_example_00009.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A man with graying hair, a beard, and a gray shirt...&lt;/summary&gt;A man with graying hair, a beard, and a gray shirt looks down and to his right, then turns his head to the left. The camera angle is a close-up, focused on the man&#039;s face. The lighting is dim, with a greenish tint. The scene appears to be real-life footage. Step&lt;/details&gt; | ![example10](./docs/_static/ltx-video_example_00010.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A clear, turquoise river flows through a rocky canyon...&lt;/summary&gt;A clear, turquoise river flows through a rocky canyon, cascading over a small waterfall and forming a pool of water at the bottom.The river is the main focus of the scene, with its clear water reflecting the surrounding trees and rocks. The canyon walls are steep and rocky, with some vegetation growing on them. The trees are mostly pine trees, with their green needles contrasting with the brown and gray rocks. The overall tone of the scene is one of peace and tranquility.&lt;/details&gt; | ![example11](./docs/_static/ltx-video_example_00011.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A man in a suit enters a room and speaks to two women...&lt;/summary&gt;A man in a suit enters a room and speaks to two women sitting on a couch. The man, wearing a dark suit with a gold tie, enters the room from the left and walks towards the center of the frame. He has short gray hair, light skin, and a serious expression. He places his right hand on the back of a chair as he approaches the couch. Two women are seated on a light-colored couch in the background. The woman on the left wears a light blue sweater and has short blonde hair. The woman on the right wears a white sweater and has short blonde hair. The camera remains stationary, focusing on the man as he enters the room. The room is brightly lit, with warm tones reflecting off the walls and furniture. The scene appears to be from a film or television show.&lt;/details&gt; | ![example12](./docs/_static/ltx-video_example_00012.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;The waves crash against the jagged rocks of the shoreline...&lt;/summary&gt;The waves crash against the jagged rocks of the shoreline, sending spray high into the air.The rocks are a dark gray color, with sharp edges and deep crevices. The water is a clear blue-green, with white foam where the waves break against the rocks. The sky is a light gray, with a few white clouds dotting the horizon.&lt;/details&gt; |
| ![example13](./docs/_static/ltx-video_example_00013.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;The camera pans across a cityscape of tall buildings...&lt;/summary&gt;The camera pans across a cityscape of tall buildings with a circular building in the center. The camera moves from left to right, showing the tops of the buildings and the circular building in the center. The buildings are various shades of gray and white, and the circular building has a green roof. The camera angle is high, looking down at the city. The lighting is bright, with the sun shining from the upper left, casting shadows from the buildings. The scene is computer-generated imagery.&lt;/details&gt; | ![example14](./docs/_static/ltx-video_example_00014.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A man walks towards a window, looks out, and then turns around...&lt;/summary&gt;A man walks towards a window, looks out, and then turns around. He has short, dark hair, dark skin, and is wearing a brown coat over a red and gray scarf. He walks from left to right towards a window, his gaze fixed on something outside. The camera follows him from behind at a medium distance. The room is brightly lit, with white walls and a large window covered by a white curtain. As he approaches the window, he turns his head slightly to the left, then back to the right. He then turns his entire body to the right, facing the window. The camera remains stationary as he stands in front of the window. The scene is captured in real-life footage.&lt;/details&gt; | ![example15](./docs/_static/ltx-video_example_00015.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;Two police officers in dark blue uniforms and matching hats...&lt;/summary&gt;Two police officers in dark blue uniforms and matching hats enter a dimly lit room through a doorway on the left side of the frame. The first officer, with short brown hair and a mustache, steps inside first, followed by his partner, who has a shaved head and a goatee. Both officers have serious expressions and maintain a steady pace as they move deeper into the room. The camera remains stationary, capturing them from a slightly low angle as they enter. The room has exposed brick walls and a corrugated metal ceiling, with a barred window visible in the background. The lighting is low-key, casting shadows on the officers&#039; faces and emphasizing the grim atmosphere. The scene appears to be from a film or television show.&lt;/details&gt; | ![example16](./docs/_static/ltx-video_example_00016.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A woman with short brown hair, wearing a maroon sleeveless top...&lt;/summary&gt;A woman with short brown hair, wearing a maroon sleeveless top and a silver necklace, walks through a room while talking, then a woman with pink hair and a white shirt appears in the doorway and yells. The first woman walks from left to right, her expression serious; she has light skin and her eyebrows are slightly furrowed. The second woman stands in the doorway, her mouth open in a yell; she has light skin and her eyes are wide. The room is dimly lit, with a bookshelf visible in the background. The camera follows the first woman as she walks, then cuts to a close-up of the second woman&#039;s face. The scene is captured in real-life footage.&lt;/details&gt; |

# News

## May, 5th, 2025: New model 13B v0.9.7:
- Release a new 13B model [ltxv-13b-0.9.7-dev](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev.safetensors)
- Release a new quantized model [ltxv-13b-0.9.7-dev-fp8](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev-fp8.safetensors) for faster inference with less VRam (Supported in the [official CompfyUI workflow](https://github.com/Lightricks/ComfyUI-LTXVideo/))
- Release a new upscalers
  * [ltxv-temporal-upscaler-0.9.7](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-temporal-upscaler-0.9.7.safetensors)
  * [ltxv-spatial-upscaler-0.9.7](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-spatial-upscaler-0.9.7.safetensors)
- Breakthrough prompt adherence and physical understanding.
- New Pipeline for multi-scale video rendering for fast and high quality results


## April, 15th, 2025: New checkpoints v0.9.6:
- Release a new checkpoint [ltxv-2b-0.9.6-dev-04-25](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-dev-04-25.safetensors) with improved quality
- Release a new distilled model [ltxv-2b-0.9.6-distilled-04-25](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-distilled-04-25.safetensors)
    * 15x faster inference than non-distilled model.
    * Does not require classifier-free guidance and spatio-temporal guidance.
    * Supports sampling with 8 (recommended), 4, 2 or 1 diffusion steps.
- Improved prompt adherence, motion quality and fine details.
- New default resolution and FPS: 1216 Ã— 704 pixels at 30 FPS
    * Still real time on H100 with the distilled model.
    * Other resolutions and FPS are still supported.
- Support stochastic inference (can improve visual quality when using the distilled model)

## March, 5th, 2025: New checkpoint v0.9.5
- New license for commercial use ([OpenRail-M](https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.5.license.txt))
- Release a new checkpoint v0.9.5 with improved quality
- Support keyframes and video extension
- Support higher resolutions
- Improved prompt understanding
- Improved VAE
- New online web app in [LTX-Studio](https://app.ltx.studio/ltx-video)
- Automatic prompt enhancement

## February, 20th, 2025: More inference options
- Improve STG (Spatiotemporal Guidance) for LTX-Video
- Support MPS on macOS with PyTorch 2.3.0
- Add support for 8-bit model, LTX-VideoQ8
- Add TeaCache for LTX-Video
- Add [ComfyUI-LTXTricks](#comfyui-integration)
- Add Diffusion-Pipe

## December 31st, 2024: Research paper
- Release the [research paper](https://arxiv.org/abs/2501.00103)

## December 20th, 2024: New checkpoint v0.9.1
- Release a new checkpoint v0.9.1 with improved quality
- Support for STG / PAG
- Support loading checkpoints of LTX-Video in Diffusers format (conversion is done on-the-fly)
- Support offloading unused parts to CPU
- Support the new timestep-conditioned VAE decoder
- Reference contributions from the community in the readme file
- Relax transformers dependency

## November 21th, 2024: Initial release v0.9.0
- Initial release of LTX-Video
- Support text-to-video and image-to-video generation


# Models

| Model              | Version | Notes                                                                                      | inference.py config                                                                                                                                      | ComfyUI workflow (Recommended) |
|--------------------|---------|--------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|------------------|
| ltxv-13b           | 0.9.7   | Highest quality, requires more VRAM                                                      | [ltxv-13b-0.9.7-dev.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.7-dev.yaml)                                             | [ltxv-13b-i2v-base.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/ltxv-13b-i2v-base.json)             |
| ltxv-13b-fp8 | 0.9.7   | Quantized version of ltxv-13b | Coming soon | [ltxv-13b-i2v-base-fp8.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/ltxv-13b-i2v-base-fp8.json) |
| ltxv-2b            | 0.9.6   | Good quality, lower VRAM requirement than ltxv-13b                                              | [ltxv-2b-0.9.6-dev.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.6-dev.yaml)                                                 | [ltxvideo-i2v.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/low_level/ltxvideo-i2v.json)             |
| ltxv-2b-distilled  | 0.9.6   | 15Ã— faster, real-time capable, fewer steps needed, no STG/CFG required                     | [ltxv-2b-0.9.6-distilled.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.6-distilled.yaml)                                     | [ltxvideo-i2v-distilled.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/low_level/ltxvideo-i2v-distilled.json)             |


# Quick Start Guide

## Online inference
The model is accessible right away via the following links:
- [LTX-Studio image-to-video](https://app.ltx.studio/ltx-video)
- [Fal.ai text-to-video](https://fal.ai/models/fal-ai/ltx-video)
- [Fal.ai image-to-video](https://fal.ai/models/fal-ai/ltx-video/image-to-video)
- [Replicate text-to-video and image-to-video](https://replicate.com/lightricks/ltx-video)

## Run locally

### Installation
The codebase was tested with Python 3.10.5, CUDA version 12.2, and supports PyTorch &gt;= 2.1.2.
On macos, MPS was tested with PyTorch 2.3.0, and should support PyTorch == 2.3 or &gt;= 2.6.

```bash
git clone https://github.com/Lightricks/LTX-Video.git
cd LTX-Video

# create env
python -m venv env
source env/bin/activate
python -m pip install -e .\[inference-script\]
```

### Inference

ğŸ“ **Note:** For best results, we recommend using our [ComfyUI](#comfyui-integration) workflow. Weâ€™re working on updating the inference.py script to match the hi

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Alvin9999/new-pac]]></title>
            <link>https://github.com/Alvin9999/new-pac</link>
            <guid>https://github.com/Alvin9999/new-pac</guid>
            <pubDate>Fri, 09 May 2025 00:04:15 GMT</pubDate>
            <description><![CDATA[ç¿»å¢™-ç§‘å­¦ä¸Šç½‘ã€è‡ªç”±ä¸Šç½‘ã€å…è´¹ç§‘å­¦ä¸Šç½‘ã€å…è´¹ç¿»å¢™ã€fanqiangã€æ²¹ç®¡youtube/è§†é¢‘ä¸‹è½½ã€è½¯ä»¶ã€VPNã€ä¸€é”®ç¿»å¢™æµè§ˆå™¨ï¼Œvpsä¸€é”®æ­å»ºç¿»å¢™æœåŠ¡å™¨è„šæœ¬/æ•™ç¨‹ï¼Œå…è´¹shadowsocks/ss/ssr/v2ray/goflywayè´¦å·/èŠ‚ç‚¹ï¼Œç¿»å¢™æ¢¯å­ï¼Œç”µè„‘ã€æ‰‹æœºã€iOSã€å®‰å“ã€windowsã€Macã€Linuxã€è·¯ç”±å™¨ç¿»å¢™ã€ç§‘å­¦ä¸Šç½‘ã€youtubeè§†é¢‘ä¸‹è½½ã€youtubeæ²¹ç®¡é•œåƒ/å…ç¿»å¢™ç½‘ç«™ã€ç¾åŒºapple idå…±äº«è´¦å·ã€ç¿»å¢™-ç§‘å­¦ä¸Šç½‘-æ¢¯å­]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Alvin9999/new-pac">Alvin9999/new-pac</a></h1>
            <p>ç¿»å¢™-ç§‘å­¦ä¸Šç½‘ã€è‡ªç”±ä¸Šç½‘ã€å…è´¹ç§‘å­¦ä¸Šç½‘ã€å…è´¹ç¿»å¢™ã€fanqiangã€æ²¹ç®¡youtube/è§†é¢‘ä¸‹è½½ã€è½¯ä»¶ã€VPNã€ä¸€é”®ç¿»å¢™æµè§ˆå™¨ï¼Œvpsä¸€é”®æ­å»ºç¿»å¢™æœåŠ¡å™¨è„šæœ¬/æ•™ç¨‹ï¼Œå…è´¹shadowsocks/ss/ssr/v2ray/goflywayè´¦å·/èŠ‚ç‚¹ï¼Œç¿»å¢™æ¢¯å­ï¼Œç”µè„‘ã€æ‰‹æœºã€iOSã€å®‰å“ã€windowsã€Macã€Linuxã€è·¯ç”±å™¨ç¿»å¢™ã€ç§‘å­¦ä¸Šç½‘ã€youtubeè§†é¢‘ä¸‹è½½ã€youtubeæ²¹ç®¡é•œåƒ/å…ç¿»å¢™ç½‘ç«™ã€ç¾åŒºapple idå…±äº«è´¦å·ã€ç¿»å¢™-ç§‘å­¦ä¸Šç½‘-æ¢¯å­</p>
            <p>Language: Python</p>
            <p>Stars: 61,634</p>
            <p>Forks: 9,942</p>
            <p>Stars today: 127 stars today</p>
            <h2>README</h2><pre>ç§‘å­¦ä¸Šç½‘-ç¿»å¢™ã€å…è´¹ç¿»å¢™ã€å…è´¹ç§‘å­¦ä¸Šç½‘ã€è½¯ä»¶ã€VPNã€ä¸€é”®ç¿»å¢™æµè§ˆå™¨ï¼Œvpsä¸€é”®æ­å»ºç¿»å¢™æœåŠ¡å™¨è„šæœ¬/æ•™ç¨‹ï¼Œå…è´¹shadowsocks/ss/ssr/v2ray/goflywayè´¦å·/èŠ‚ç‚¹ï¼Œå…è´¹è‡ªç”±ä¸Šç½‘ã€fanqiangã€ç¿»å¢™æ¢¯å­ï¼Œç”µè„‘ã€æ‰‹æœºã€iOSã€å®‰å“ã€windowsã€Macã€Linuxã€è·¯ç”±å™¨ç¿»å¢™ã€youtubeè§†é¢‘ä¸‹è½½ã€youtubeæ²¹ç®¡é•œåƒ/å…ç¿»å¢™ç½‘ç«™ã€ç¾åŒºapple idå…±äº«è´¦å·

**https://github.com/Alvin9999/new-pac/wiki**

åŒ—äº¬æ—¶é—´2025å¹´05æœˆ09æ—¥07ç‚¹51åˆ†æ›´æ–°ã€‚
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hiroi-sora/Umi-OCR]]></title>
            <link>https://github.com/hiroi-sora/Umi-OCR</link>
            <guid>https://github.com/hiroi-sora/Umi-OCR</guid>
            <pubDate>Fri, 09 May 2025 00:04:14 GMT</pubDate>
            <description><![CDATA[OCR software, free and offline. å¼€æºã€å…è´¹çš„ç¦»çº¿OCRè½¯ä»¶ã€‚æ”¯æŒæˆªå±/æ‰¹é‡å¯¼å…¥å›¾ç‰‡ï¼ŒPDFæ–‡æ¡£è¯†åˆ«ï¼Œæ’é™¤æ°´å°/é¡µçœ‰é¡µè„šï¼Œæ‰«æ/ç”ŸæˆäºŒç»´ç ã€‚å†…ç½®å¤šå›½è¯­è¨€åº“ã€‚]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hiroi-sora/Umi-OCR">hiroi-sora/Umi-OCR</a></h1>
            <p>OCR software, free and offline. å¼€æºã€å…è´¹çš„ç¦»çº¿OCRè½¯ä»¶ã€‚æ”¯æŒæˆªå±/æ‰¹é‡å¯¼å…¥å›¾ç‰‡ï¼ŒPDFæ–‡æ¡£è¯†åˆ«ï¼Œæ’é™¤æ°´å°/é¡µçœ‰é¡µè„šï¼Œæ‰«æ/ç”ŸæˆäºŒç»´ç ã€‚å†…ç½®å¤šå›½è¯­è¨€åº“ã€‚</p>
            <p>Language: Python</p>
            <p>Stars: 33,379</p>
            <p>Forks: 3,347</p>
            <p>Stars today: 206 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;left&quot;&gt;
    &lt;span&gt;
        &lt;b&gt;ä¸­æ–‡&lt;/b&gt;
    &lt;/span&gt;
    &lt;span&gt; â€¢ &lt;/span&gt;
    &lt;a href=&quot;README_en.md&quot;&gt;
        English
    &lt;/a&gt;
    &lt;span&gt; â€¢ &lt;/span&gt;
    &lt;a href=&quot;README_ja.md&quot;&gt;
        æ—¥æœ¬èª
    &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR&quot;&gt;
    &lt;img width=&quot;200&quot; height=&quot;128&quot; src=&quot;https://tupian.li/images/2022/10/27/icon---256.png&quot; alt=&quot;Umi-OCR&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt;Umi-OCR æ–‡å­—è¯†åˆ«å·¥å…·&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR/releases/latest&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/v/release/hiroi-sora/Umi-OCR?style=flat-square&quot; alt=&quot;Umi-OCR&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR/blob/main/LICENSE&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/license/hiroi-sora/Umi-OCR?style=flat-square&quot; alt=&quot;LICENSE&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;#ä¸‹è½½å‘è¡Œç‰ˆ&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/downloads/hiroi-sora/Umi-OCR/total?style=flat-square&quot; alt=&quot;forks&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://star-history.com/#hiroi-sora/Umi-OCR&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/hiroi-sora/Umi-OCR?style=flat-square&quot; alt=&quot;stars&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR/forks&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/forks/hiroi-sora/Umi-OCR?style=flat-square&quot; alt=&quot;forks&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://hosted.weblate.org/engage/umi-ocr/&quot;&gt;
    &lt;img src=&quot;https://hosted.weblate.org/widget/umi-ocr/svg-badge.svg&quot; alt=&quot;ç¿»è¯‘çŠ¶æ€&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;h3&gt;
    &lt;a href=&quot;#ç›®å½•&quot;&gt;
      ä½¿ç”¨è¯´æ˜
    &lt;/a&gt;
    &lt;span&gt; â€¢ &lt;/span&gt;
    &lt;a href=&quot;#ä¸‹è½½å‘è¡Œç‰ˆ&quot;&gt;
      ä¸‹è½½åœ°å€
    &lt;/a&gt;
    &lt;span&gt; â€¢ &lt;/span&gt;
    &lt;a href=&quot;CHANGE_LOG.md&quot;&gt;
      æ›´æ–°æ—¥å¿—
    &lt;/a&gt;
    &lt;span&gt; â€¢ &lt;/span&gt;
    &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR/issues&quot;&gt;
      æäº¤Bug
    &lt;/a&gt;
  &lt;/h3&gt;
&lt;/div&gt;
&lt;br&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;strong&gt;å…è´¹ï¼Œå¼€æºï¼Œå¯æ‰¹é‡çš„ç¦»çº¿OCRè½¯ä»¶&lt;/strong&gt;&lt;br&gt;
  &lt;sub&gt;é€‚ç”¨äº Windows7 x64 ã€Linux x64
&lt;/div&gt;&lt;br&gt;

- **å…è´¹**ï¼šæœ¬é¡¹ç›®æ‰€æœ‰ä»£ç å¼€æºï¼Œå®Œå…¨å…è´¹ã€‚
- **æ–¹ä¾¿**ï¼šè§£å‹å³ç”¨ï¼Œç¦»çº¿è¿è¡Œï¼Œæ— éœ€ç½‘ç»œã€‚
- **é«˜æ•ˆ**ï¼šè‡ªå¸¦é«˜æ•ˆç‡çš„ç¦»çº¿OCRå¼•æ“ï¼Œå†…ç½®å¤šç§è¯­è¨€è¯†åˆ«åº“ã€‚
- **çµæ´»**ï¼šæ”¯æŒå‘½ä»¤è¡Œã€HTTPæ¥å£ç­‰å¤–éƒ¨è°ƒç”¨æ–¹å¼ã€‚
- **åŠŸèƒ½**ï¼šæˆªå›¾OCR / æ‰¹é‡OCR / PDFè¯†åˆ« / äºŒç»´ç  / å…¬å¼è¯†åˆ«

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/65599097ab5f4.png&quot; alt=&quot;1-æ ‡é¢˜-1.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

![1-æ ‡é¢˜-2.png](https://tupian.li/images/2023/11/19/6559909fdeeba.png)

## ç›®å½•

- [æˆªå›¾è¯†åˆ«](#æˆªå›¾OCR)
  - [æ’ç‰ˆè§£æ](#æ–‡æœ¬åå¤„ç†) - è¯†åˆ«ä¸åŒæ’ç‰ˆï¼ŒæŒ‰æ­£ç¡®é¡ºåºè¾“å‡ºæ–‡å­—
- [æ‰¹é‡è¯†åˆ«](#æ‰¹é‡OCR)
  - [å¿½ç•¥åŒºåŸŸ](#å¿½ç•¥åŒºåŸŸ) - æ’é™¤æˆªå›¾æ°´å°å¤„çš„æ–‡å­—
- [äºŒç»´ç ](#äºŒç»´ç ) æ”¯æŒæ‰«ç æˆ–ç”ŸæˆäºŒç»´ç å›¾ç‰‡
- [æ–‡æ¡£è¯†åˆ«](#æ–‡æ¡£è¯†åˆ«) ä»PDFæ‰«æä»¶ä¸­æå–æ–‡æœ¬ï¼Œæˆ–è½¬ä¸ºåŒå±‚å¯æœç´¢PDF
- [å…¨å±€è®¾ç½®](#å…¨å±€è®¾ç½®)
- [å‘½ä»¤è¡Œè°ƒç”¨](docs/README_CLI.md)
- [HTTPæ¥å£](docs/http/README.md)
- [æ„å»ºé¡¹ç›®ï¼ˆWindowsã€Linuxï¼‰](#æ„å»ºé¡¹ç›®)

## ä½¿ç”¨æºç 

å¼€å‘è€…è¯·åŠ¡å¿…é˜…è¯» [æ„å»ºé¡¹ç›®](#æ„å»ºé¡¹ç›®) ã€‚

## ä¸‹è½½å‘è¡Œç‰ˆ

ä»¥ä¸‹å‘å¸ƒé“¾æ¥å‡é•¿æœŸç»´æŠ¤ï¼Œæä¾›ç¨³å®šç‰ˆæœ¬çš„ä¸‹è½½ã€‚

- **è“å¥äº‘** https://hiroi-sora.lanzoul.com/s/umi-ocr ï¼ˆå›½å†…æ¨èï¼Œå…æ³¨å†Œ/æ— é™é€Ÿï¼‰
- **GitHub** https://github.com/hiroi-sora/Umi-OCR/releases/latest
- **Source Forge** https://sourceforge.net/projects/umi-ocr


&lt;details&gt;
&lt;summary&gt;&lt;b&gt;â€¢&amp;nbsp;&amp;nbsp;Scoop Installer&lt;/b&gt;ï¼ˆç‚¹å‡»å±•å¼€ï¼‰&lt;/summary&gt;

[Scoop](https://scoop.sh/) æ˜¯ä¸€æ¬¾Windowsä¸‹çš„å‘½ä»¤è¡Œå®‰è£…ç¨‹åºï¼Œå¯æ–¹ä¾¿åœ°ç®¡ç†å¤šä¸ªåº”ç”¨ã€‚æ‚¨å¯ä»¥å…ˆå®‰è£… Scoop ï¼Œå†ä½¿ç”¨ä»¥ä¸‹æŒ‡ä»¤å®‰è£… `Umi-OCR` ï¼š

- æ·»åŠ  `extras` æ¡¶ï¼š
```
scoop bucket add extras
```

- ï¼ˆå¯é€‰1ï¼‰å®‰è£… Umi-OCRï¼ˆè‡ªå¸¦ `Rapid-OCR` å¼•æ“ï¼Œå…¼å®¹æ€§å¥½ï¼‰ï¼š
```
scoop install extras/umi-ocr
```

- ï¼ˆå¯é€‰2ï¼‰å®‰è£… Umi-OCRï¼ˆè‡ªå¸¦ `Paddle-OCR` å¼•æ“ï¼Œé€Ÿåº¦ç¨å¿«ï¼‰ï¼š
```
scoop install extras/umi-ocr-paddle
```

- ä¸è¦åŒæ—¶å®‰è£…äºŒè€…ï¼Œå¿«æ·æ–¹å¼å¯èƒ½ä¼šè¢«è¦†ç›–ã€‚ä½†æ‚¨å¯ä»¥é¢å¤–å¯¼å…¥ [æ’ä»¶](https://github.com/hiroi-sora/Umi-OCR_plugins) ï¼Œéšæ—¶åˆ‡æ¢ä¸åŒOCRå¼•æ“ã€‚

&lt;/details&gt;
&lt;/br&gt;

## å¼€å§‹ä½¿ç”¨

è½¯ä»¶å‘å¸ƒåŒ…ä¸‹è½½ä¸º `.7z` å‹ç¼©åŒ…æˆ– `.7z.exe` è‡ªè§£å‹åŒ…ã€‚è‡ªè§£å‹åŒ…å¯åœ¨æ²¡æœ‰å®‰è£…å‹ç¼©è½¯ä»¶çš„ç”µè„‘ä¸Šï¼Œè§£å‹æ–‡ä»¶ã€‚

æœ¬è½¯ä»¶æ— éœ€å®‰è£…ã€‚è§£å‹åï¼Œç‚¹å‡» `Umi-OCR.exe` å³å¯å¯åŠ¨ç¨‹åºã€‚

é‡åˆ°ä»»ä½•é—®é¢˜ï¼Œè¯·æ [Issue](https://github.com/hiroi-sora/Umi-OCR/issues) ï¼Œæˆ‘ä¼šå°½å¯èƒ½å¸®åŠ©ä½ ã€‚

## ç•Œé¢è¯­è¨€

Umi-OCR æ”¯æŒçš„ç•Œé¢å¤šå›½è¯­è¨€ã€‚åœ¨ç¬¬ä¸€æ¬¡æ‰“å¼€è½¯ä»¶æ—¶ï¼Œå°†ä¼šæŒ‰ç…§ä½ çš„ç”µè„‘çš„ç³»ç»Ÿè®¾ç½®ï¼Œè‡ªåŠ¨åˆ‡æ¢è¯­è¨€ã€‚

å¦‚æœéœ€è¦æ‰‹åŠ¨åˆ‡æ¢è¯­è¨€ï¼Œè¯·å‚è€ƒä¸‹å›¾ï¼Œ`å…¨å±€è®¾ç½®`â†’`è¯­è¨€/Language` ã€‚

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/65599c3f9e600.png&quot; alt=&quot;1-æ ‡é¢˜-1.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

## æ ‡ç­¾é¡µ

Umi-OCR v2 ç”±ä¸€ç³»åˆ—çµæ´»å¥½ç”¨çš„**æ ‡ç­¾é¡µ**ç»„æˆã€‚æ‚¨å¯æŒ‰ç…§è‡ªå·±çš„å–œå¥½ï¼Œæ‰“å¼€éœ€è¦çš„æ ‡ç­¾é¡µã€‚

æ ‡ç­¾æ å·¦ä¸Šè§’å¯ä»¥åˆ‡æ¢**çª—å£ç½®é¡¶**ã€‚å³ä¸Šè§’èƒ½å¤Ÿ**é”å®šæ ‡ç­¾é¡µ**ï¼Œä»¥é˜²æ­¢æ—¥å¸¸ä½¿ç”¨ä¸­è¯¯è§¦å…³é—­æ ‡ç­¾é¡µã€‚

### æˆªå›¾OCR

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/65599097aba8e.png&quot; alt=&quot;2-æˆªå›¾-1.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

**æˆªå›¾OCR**ï¼šæ‰“å¼€è¿™ä¸€é¡µåï¼Œå°±å¯ä»¥ç”¨å¿«æ·é”®å”¤èµ·æˆªå›¾ï¼Œè¯†åˆ«å›¾ä¸­çš„æ–‡å­—ã€‚
- å·¦ä¾§çš„å›¾ç‰‡é¢„è§ˆæ ï¼Œå¯ç›´æ¥ç”¨é¼ æ ‡åˆ’é€‰å¤åˆ¶ã€‚
- å³ä¾§çš„è¯†åˆ«è®°å½•æ ï¼Œå¯ä»¥ç¼–è¾‘æ–‡å­—ï¼Œå…è®¸åˆ’é€‰å¤šä¸ªè®°å½•å¤åˆ¶ã€‚
- ä¹Ÿæ”¯æŒåœ¨åˆ«å¤„å¤åˆ¶å›¾ç‰‡ï¼Œç²˜è´´åˆ°Umi-OCRè¿›è¡Œè¯†åˆ«ã€‚
- å…³äº [å…¬å¼è¯†åˆ«](https://github.com/hiroi-sora/Umi-OCR/issues/254) åŠŸèƒ½

#### æ–‡æœ¬åå¤„ç†

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/6559909f3e378.png&quot; alt=&quot;2-æˆªå›¾-2.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

å…³äº **OCRæ–‡æœ¬åå¤„ç† - æ’ç‰ˆè§£ææ–¹æ¡ˆ**ï¼š å¯ä»¥æ•´ç†OCRç»“æœçš„æ’ç‰ˆå’Œé¡ºåºï¼Œä½¿æ–‡æœ¬æ›´é€‚åˆé˜…è¯»å’Œä½¿ç”¨ã€‚é¢„è®¾æ–¹æ¡ˆï¼š
- `å¤šæ -æŒ‰è‡ªç„¶æ®µæ¢è¡Œ`ï¼šé€‚åˆå¤§éƒ¨åˆ†æƒ…æ™¯ï¼Œè‡ªåŠ¨è¯†åˆ«å¤šæ å¸ƒå±€ï¼ŒæŒ‰è‡ªç„¶æ®µè§„åˆ™è¿›è¡Œæ¢è¡Œã€‚
- `å¤šæ -æ€»æ˜¯æ¢è¡Œ`ï¼šæ¯æ®µè¯­å¥éƒ½è¿›è¡Œæ¢è¡Œã€‚
- `å¤šæ -æ— æ¢è¡Œ`ï¼šå¼ºåˆ¶å°†æ‰€æœ‰è¯­å¥åˆå¹¶åˆ°åŒä¸€è¡Œã€‚
- `å•æ -æŒ‰è‡ªç„¶æ®µæ¢è¡Œ`/`æ€»æ˜¯æ¢è¡Œ`/`æ— æ¢è¡Œ`ï¼šä¸ä¸Šè¿°ç±»ä¼¼ï¼Œä¸è¿‡ ä¸åŒºåˆ†å¤šæ å¸ƒå±€ã€‚
- `å•æ -ä¿ç•™ç¼©è¿›`ï¼šé€‚ç”¨äºè§£æä»£ç æˆªå›¾ï¼Œä¿ç•™è¡Œé¦–ç¼©è¿›å’Œè¡Œä¸­ç©ºæ ¼ã€‚
- `ä¸åšå¤„ç†`ï¼šOCRå¼•æ“çš„åŸå§‹è¾“å‡ºï¼Œé»˜è®¤æ¯æ®µè¯­å¥éƒ½è¿›è¡Œæ¢è¡Œã€‚

ä¸Šè¿°æ–¹æ¡ˆï¼Œå‡èƒ½è‡ªåŠ¨å¤„ç†æ¨ªæ’å’Œç«–æ’ï¼ˆä»å³åˆ°å·¦ï¼‰çš„æ’ç‰ˆã€‚ï¼ˆç«–æ’æ–‡å­—è¿˜éœ€è¦OCRå¼•æ“æœ¬èº«æ”¯æŒï¼‰

---

### æ‰¹é‡OCR

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/655990a2511e0.png&quot; alt=&quot;3-æ‰¹é‡-1.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

**æ‰¹é‡OCR**ï¼šè¿™ä¸€é¡µç”¨äºæ‰¹é‡å¯¼å…¥æœ¬åœ°å›¾ç‰‡è¿›è¡Œè¯†åˆ«ã€‚
- æ”¯æŒæ ¼å¼ï¼š`jpg, jpe, jpeg, jfif, png, webp, bmp, tif, tiff`ã€‚
- ä¿å­˜è¯†åˆ«ç»“æœçš„æ”¯æŒæ ¼å¼ï¼š`txt, jsonl, md, csv(Excel)`ã€‚
- ä¸æˆªå›¾OCRä¸€æ ·ï¼Œæ”¯æŒ`æ–‡æœ¬åå¤„ç†`åŠŸèƒ½ï¼Œæ•´ç†OCRæ–‡æœ¬çš„æ’ç‰ˆå’Œé¡ºåºã€‚
- æ²¡æœ‰æ•°é‡ä¸Šé™ï¼Œå¯ä¸€æ¬¡æ€§å¯¼å…¥å‡ ç™¾å¼ å›¾ç‰‡è¿›è¡Œä»»åŠ¡ã€‚
- æ”¯æŒä»»åŠ¡å®Œæˆåè‡ªåŠ¨å…³æœº/å¾…æœºã€‚
- å¦‚æœè¦è¯†åˆ«åƒç´ è¶…å¤§çš„é•¿å›¾æˆ–å¤§å›¾ï¼Œè¯·è°ƒæ•´ï¼š**é¡µé¢çš„è®¾ç½®â†’æ–‡å­—è¯†åˆ«â†’é™åˆ¶å›¾åƒè¾¹é•¿â†’ã€è°ƒé«˜æ•°å€¼ã€‘**ã€‚
- æ‹¥æœ‰ç‰¹æ®ŠåŠŸèƒ½ `å¿½ç•¥åŒºåŸŸ` ã€‚

#### å¿½ç•¥åŒºåŸŸ

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/6559911d28be7.png&quot; alt=&quot;3-æ‰¹é‡-2.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

å…³äº **OCRæ–‡æœ¬åå¤„ç† - å¿½ç•¥åŒºåŸŸ**ï¼š æ‰¹é‡OCRä¸­çš„ä¸€ç§ç‰¹æ®ŠåŠŸèƒ½ï¼Œé€‚ç”¨äºæ’é™¤å›¾ç‰‡ä¸­çš„ä¸æƒ³è¦çš„æ–‡å­—ã€‚
- åœ¨æ‰¹é‡è¯†åˆ«é¡µçš„å³æ è®¾ç½®ä¸­å¯è¿›å…¥å¿½ç•¥åŒºåŸŸç¼–è¾‘å™¨ã€‚
- å¦‚ä¸Šæ–¹æ ·ä¾‹ï¼Œå›¾ç‰‡é¡¶éƒ¨å’Œå³ä¸‹è§’å­˜åœ¨å¤šä¸ªæ°´å° / LOGOã€‚å¦‚æœæ‰¹é‡è¯†åˆ«è¿™ç±»å›¾ç‰‡ï¼Œæ°´å°ä¼šå¯¹è¯†åˆ«ç»“æœé€ æˆå¹²æ‰°ã€‚
- æŒ‰ä½å³é”®ï¼Œç»˜åˆ¶å¤šä¸ªçŸ©å½¢æ¡†ã€‚è¿™äº›åŒºåŸŸå†…çš„æ–‡å­—å°†åœ¨ä»»åŠ¡ä¸­è¢«å¿½ç•¥ã€‚
- è¯·å°½é‡å°†çŸ©å½¢æ¡†ç”»å¾—å¤§ä¸€äº›ï¼Œå®Œå…¨åŒ…è£¹ä½æ°´å°æ‰€æœ‰å¯èƒ½å‡ºç°çš„ä½ç½®ã€‚
- æ³¨æ„ï¼Œåªæœ‰å¤„äºå¿½ç•¥åŒºåŸŸæ¡†å†…éƒ¨çš„æ•´ä¸ªæ–‡æœ¬å—ï¼ˆè€Œä¸æ˜¯å•ä¸ªå­—ç¬¦ï¼‰ä¼šè¢«å¿½ç•¥ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œé»„è‰²è¾¹æ¡†çš„æ·±è‰²çŸ©å½¢æ˜¯ä¸€ä¸ªå¿½ç•¥åŒºåŸŸã€‚é‚£ä¹ˆåªæœ‰`key_mouse`æ‰ä¼šè¢«å¿½ç•¥ã€‚`pubsub_connector.py`ã€`pubsub_service.py` è¿™ä¸¤ä¸ªæ–‡æœ¬å—å¾—ä»¥ä¿ç•™ã€‚
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2024/05/30/66587bf03ae15.png&quot; alt=&quot;å¿½ç•¥åŒºåŸŸèŒƒå›´ç¤ºä¾‹.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

---

### æ–‡æ¡£è¯†åˆ«

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/hiroi-sora/Umi-OCR/assets/56373419/fc2266ee-b9b7-4079-8b10-6610e6da6cf5&quot; alt=&quot;&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

**æ–‡æ¡£è¯†åˆ«**ï¼š
- æ”¯æŒæ ¼å¼ï¼š`pdf, xps, epub, mobi, fb2, cbz`ã€‚
- å¯¹æ‰«æä»¶è¿›è¡ŒOCRï¼Œæˆ–æå–åŸæœ‰æ–‡æœ¬ã€‚å¯è¾“å‡ºä¸º **åŒå±‚å¯æœç´¢PDF** ã€‚
- æ”¯æŒè®¾å®š **å¿½ç•¥åŒºåŸŸ** ï¼Œå¯ç”¨äºæ’é™¤é¡µçœ‰é¡µè„šçš„æ–‡å­—ã€‚
- å¯è®¾ç½®ä»»åŠ¡å®Œæˆå **è‡ªåŠ¨å…³æœº/ä¼‘çœ ** ã€‚

---

### äºŒç»´ç 

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/655991268d6b1.png&quot; alt=&quot;4-äºŒç»´ç -1.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

**æ‰«ç **ï¼š
- æˆªå›¾/ç²˜è´´/æ‹–å…¥æœ¬åœ°å›¾ç‰‡ï¼Œè¯»å–å…¶ä¸­çš„äºŒç»´ç ã€æ¡å½¢ç ã€‚
- æ”¯æŒä¸€å›¾å¤šç ã€‚
- æ”¯æŒ19ç§åè®®ï¼Œå¦‚ä¸‹ï¼š

`Aztec`,`Codabar`,`Code128`,`Code39`,`Code93`,`DataBar`,`DataBarExpanded`,`DataMatrix`,`EAN13`,`EAN8`,`ITF`,`LinearCodes`,`MatrixCodes`,`MaxiCode`,`MicroQRCode`,`PDF417`,`QRCode`,`UPCA`,`UPCE`

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/6559911cda737.png&quot; alt=&quot;4-äºŒç»´ç -2.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

**ç”Ÿæˆç **ï¼š
- è¾“å…¥æ–‡æœ¬ï¼Œç”ŸæˆäºŒç»´ç å›¾ç‰‡ã€‚
- æ”¯æŒ19ç§åè®®å’Œ**çº é”™ç­‰çº§**ç­‰å‚æ•°ã€‚

---

### å…¨å±€è®¾ç½®

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://tupian.li/images/2023/11/19/655991252e780.png&quot; alt=&quot;5-å…¨å±€è®¾ç½®-1.png&quot; style=&quot;width: 80%;&quot;&gt;&lt;/p&gt;

**å…¨å±€è®¾ç½®**ï¼šåœ¨è¿™é‡Œå¯ä»¥è°ƒæ•´è½¯ä»¶çš„å…¨å±€å‚æ•°ã€‚å¸¸ç”¨åŠŸèƒ½å¦‚ä¸‹ï¼š
- ä¸€é”®æ·»åŠ å¿«æ·æ–¹å¼æˆ–è®¾ç½®å¼€æœºè‡ªå¯ã€‚
- æ›´æ”¹ç•Œé¢**è¯­è¨€**ã€‚Umiæ”¯æŒç¹ä¸­ã€è‹±è¯­ã€æ—¥è¯­ç­‰è¯­è¨€ã€‚
- åˆ‡æ¢ç•Œé¢**ä¸»é¢˜**ã€‚Umiæ‹¥æœ‰å¤šä¸ªäº®/æš—ä¸»é¢˜ã€‚
- è°ƒæ•´ç•Œé¢**æ–‡å­—çš„å¤§å°**å’Œ**å­—ä½“**ã€‚
- åˆ‡æ¢OCRæ’ä»¶ã€‚
- **æ¸²æŸ“å™¨**ï¼šè½¯ä»¶ç•Œé¢é»˜è®¤æ”¯æŒæ˜¾å¡åŠ é€Ÿæ¸²æŸ“ã€‚å¦‚æœåœ¨ä½ çš„æœºå™¨ä¸Šå‡ºç°æˆªå±é—ªçƒã€UIé”™ä½çš„æƒ…å†µï¼Œè¯·è°ƒæ•´`ç•Œé¢å’Œå¤–è§‚` â†’ `æ¸²æŸ“å™¨` ï¼Œå°è¯•åˆ‡æ¢åˆ°ä¸åŒæ¸²æŸ“æ–¹æ¡ˆï¼Œæˆ–å…³é—­ç¡¬ä»¶åŠ é€Ÿã€‚

## è°ƒç”¨æ¥å£ï¼š

- [å‘½ä»¤è¡Œæ‰‹å†Œ](docs/README_CLI.md)
- [HTTPæ¥å£æ‰‹å†Œ](docs/http/README.md)

---

## å…³äºé¡¹ç›®ç»“æ„

### å„ä»“åº“ï¼š

- [ä¸»ä»“åº“](https://github.com/hiroi-sora/Umi-OCR) ğŸ‘ˆ
- [æ’ä»¶åº“](https://github.com/hiroi-sora/Umi-OCR_plugins)
- [Windows è¿è¡Œåº“](https://github.com/hiroi-sora/Umi-OCR_runtime_windows)
- [Linux è¿è¡Œåº“](https://github.com/hiroi-sora/Umi-OCR_runtime_linux)

### å·¥ç¨‹ç»“æ„ï¼š

`**` åç¼€è¡¨ç¤ºæœ¬ä»“åº“(`ä¸»ä»“åº“`)åŒ…å«çš„å†…å®¹ã€‚

```
Umi-OCR
â”œâ”€ Umi-OCR.exe
â”œâ”€ umi-ocr.sh
â””â”€ UmiOCR-data
   â”œâ”€ main.py **
   â”œâ”€ version.py **
   â”œâ”€ qt_res **
   â”‚  â””â”€ é¡¹ç›®qtèµ„æºï¼ŒåŒ…æ‹¬å›¾æ ‡å’Œqmlæºç 
   â”œâ”€ py_src **
   â”‚  â””â”€ é¡¹ç›®pythonæºç 
   â”œâ”€ plugins
   â”‚  â””â”€ æ’ä»¶
   â””â”€ i18n **
      â””â”€ ç¿»è¯‘æ–‡ä»¶
```

æ”¯æŒçš„ç¦»çº¿OCRå¼•æ“ï¼š

- [PaddleOCR-json](https://github.com/hiroi-sora/PaddleOCR-json)
- [RapidOCR-json](https://github.com/hiroi-sora/RapidOCR-json)

è¿è¡Œç¯å¢ƒæ¡†æ¶ï¼š

- [PyStand](https://github.com/skywind3000/PyStand) å®šåˆ¶ç‰ˆ

## æ„å»ºé¡¹ç›®

è¯·è·³è½¬ä¸‹è¿°ä»“åº“ï¼Œå®Œæˆå¯¹åº”å¹³å°çš„å¼€å‘/è¿è¡Œç¯å¢ƒéƒ¨ç½²ã€‚

- [Windows](https://github.com/hiroi-sora/Umi-OCR_runtime_windows)
- [Linux](https://github.com/hiroi-sora/Umi-OCR_runtime_linux)

--- 

## è½¯ä»¶æœ¬åœ°åŒ–ç¿»è¯‘ï¼š

æœ¬é¡¹ç›®ä½¿ç”¨ Weblate å¹³å°è¿›è¡ŒUIç•Œé¢çš„æœ¬åœ°åŒ–ç¿»è¯‘åä½œã€‚æˆ‘ä»¬æ¬¢è¿ä»»ä½•è¯‘è€…å‚ä¸ç¿»è¯‘å·¥ä½œï¼Œæ‚¨å¯è¿›å…¥æ­¤é“¾æ¥ [Weblate: Umi-OCR](https://hosted.weblate.org/engage/umi-ocr/) ï¼Œåœ¨çº¿æ ¡å¯¹ã€è¡¥å……ç°æœ‰è¯­è¨€ï¼Œæˆ–æ·»åŠ æ–°è¯­è¨€ã€‚

æ„Ÿè°¢ä»¥ä¸‹è¯‘è€…ï¼Œä¸º Umi-OCR è´¡çŒ®äº†æœ¬åœ°åŒ–ç¿»è¯‘å·¥ä½œï¼š

| è¯‘è€…                                                                                 | è´¡çŒ®è¯­è¨€                  |
| ------------------------------------------------------------------------------------ | ------------------------- |
| [bob](https://hosted.weblate.org/user/q021)                                          | English, ç¹é«”ä¸­æ–‡, æ—¥æœ¬èª |
| [Qingzheng Gao](https://github.com/QZGao)                                            | English, ç¹é«”ä¸­æ–‡         |
| [Weng, Chia-Ling](https://hosted.weblate.org/user/ChiaLingWeng)                      | English, ç¹é«”ä¸­æ–‡         |
| [linzow](https://hosted.weblate.org/user/linzow)                                     | English, ç¹é«”ä¸­æ–‡         |
| [Marcos i](https://hosted.weblate.org/user/ultramarkorj9)                            | English, PortuguÃªs        |
| [Eric Guo](https://hosted.weblate.org/user/qwedc001)                                 | English                   |
| [steven0081](https://hosted.weblate.org/user/steven0081)                             | English                   |
| [Brandon Cagle](https://hosted.weblate.org/user/random4t4x14)                        | English                   |
| [plum7x](https://hosted.weblate.org/user/plum7x)                                     | ç¹é«”ä¸­æ–‡                  |
| [hugoalh](https://hosted.weblate.org/user/hugoalh)                                   | ç¹é«”ä¸­æ–‡                  |
| [Anarkiisto](https://hosted.weblate.org/user/Anarkiisto)                             | ç¹é«”ä¸­æ–‡                  |
| [ãƒ‰ã‚³ãƒ¢å…‰](https://hosted.weblate.org/user/umren190402)                              | æ—¥æœ¬èª                    |
| [æ¨é¹](https://hosted.weblate.org/user/ypf)                                          | PortuguÃªs                 |
| [Ğ’ÑÑ‡ĞµÑĞ»Ğ°Ğ² ĞĞ½Ğ°Ñ‚Ğ¾Ğ»ÑŒĞµĞ²Ğ¸Ñ‡ ĞœĞ°Ğ»Ñ‹ÑˆĞµĞ²](https://hosted.weblate.org/user/1969)                 | Ğ ÑƒÑÑĞºĞ¸Ğ¹                   |
| [Muhammadyusuf Kurbonov](https://hosted.weblate.org/user/muhammadyusuf.kurbonov2002) | Ğ ÑƒÑÑĞºĞ¸Ğ¹                   |
| [à®¤à®®à®¿à®´à¯à®¨à¯‡à®°à®®à¯](https://hosted.weblate.org/user/TamilNeram/)                                | à®¤à®®à®¿à®´à¯                       |

å¦‚æœæœ‰ä¿¡æ¯é”™è¯¯æˆ–äººå‘˜ç¼ºæ¼ï¼Œè¯·åœ¨ [è¿™ä¸ªè®¨è®º](https://github.com/hiroi-sora/Umi-OCR/discussions/449) ä¸­å›å¤ã€‚

---

## èµåŠ©

Umi-OCR é¡¹ç›®ä¸»è¦ç”±ä½œè€… [hiroi-sora](https://github.com/hiroi-sora) ç”¨ä¸šä½™æ—¶é—´åœ¨å¼€å‘å’Œç»´æŠ¤ã€‚å¦‚æœæ‚¨å–œæ¬¢è¿™æ¬¾è½¯ä»¶ï¼Œæ¬¢è¿èµåŠ©ã€‚

- å›½å†…ç”¨æˆ·å¯é€šè¿‡ [çˆ±å‘ç”µ](https://afdian.com/a/hiroi-sora) èµåŠ©ä½œè€…ã€‚

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=hiroi-sora/Umi-OCR&amp;type=Date)](https://star-history.com/#hiroi-sora/Umi-OCR&amp;Date)

## [æ›´æ–°æ—¥å¿—](CHANGE_LOG.md)

## å¼€å‘è®¡åˆ’

&lt;details&gt;
&lt;summary&gt;å·²å®Œæˆçš„å·¥ä½œ&lt;/summary&gt;

- æ ‡ç­¾é¡µæ¡†æ¶ã€‚
- OCR APIæ§åˆ¶å™¨ã€‚
- OCR ä»»åŠ¡æ§åˆ¶å™¨ã€‚
- ä¸»é¢˜ç®¡ç†å™¨ï¼Œæ”¯æŒåˆ‡æ¢æµ…è‰²/æ·±è‰²ä¸»é¢˜ä¸»é¢˜ã€‚
- å®ç° **æ‰¹é‡OCR**ã€‚
- å®ç° **æˆªå›¾OCR**ã€‚
- å¿«æ·é”®æœºåˆ¶ã€‚
- ç³»ç»Ÿæ‰˜ç›˜èœå•ã€‚
- æ–‡æœ¬å—åå¤„ç†ï¼ˆæ’ç‰ˆä¼˜åŒ–ï¼‰ã€‚
- å¼•æ“å†…å­˜æ¸…ç†ã€‚
- è½¯ä»¶ç•Œé¢å¤šå›½è¯­è¨€ã€‚
- å‘½ä»¤è¡Œæ¨¡å¼ã€‚
- Win7å…¼å®¹ã€‚
- Excelï¼ˆcsvï¼‰è¾“å‡ºæ ¼å¼ã€‚
- `Esc`ä¸­æ–­æˆªå›¾æ“ä½œ
- å¤–ç½®ä¸»é¢˜æ–‡ä»¶
- å­—ä½“åˆ‡æ¢
- åŠ è½½åŠ¨ç”»
- å¿½ç•¥åŒºåŸŸã€‚
- äºŒç»´ç è¯†åˆ«ã€‚
- æ‰¹é‡è¯†åˆ«é¡µé¢çš„å›¾ç‰‡é¢„è§ˆçª—å£ã€‚
- PDFè¯†åˆ«ã€‚
- è°ƒç”¨æœ¬åœ°å›¾ç‰‡æµè§ˆå™¨æ‰“å¼€å›¾ç‰‡ã€‚ [#335](https://github.com/hiroi-sora/Umi-OCR/issues/335)
- é‡å¤ä¸Šä¸€æ¬¡æˆªå›¾ã€‚ [#357](https://github.com/hiroi-sora/Umi-OCR/issues/357)
- ä¿®Bugï¼šæ–‡æ¡£è¯†åˆ«åœ¨Windows7ç³»ç»Ÿçš„å…¼å®¹æ€§é—®é¢˜ã€‚
- HTTP/å‘½ä»¤è¡Œæ¥å£æ·»åŠ äºŒç»´ç è¯†åˆ«/ç”ŸæˆåŠŸèƒ½ã€‚ (#423)
- äºŒç»´ç æ¥å£çš„æ–‡æ¡£ã€‚
- Linux å¹³å°ç§»æ¤ã€‚
- HTTP æ–‡æ¡£è¯†åˆ«æ¥å£ã€‚

&lt;/details&gt;

&lt;!-- ##### æ­£åœ¨è¿›è¡Œçš„å·¥ä½œ --&gt;

##### è¿œæœŸè®¡åˆ’

&lt;details&gt;
&lt;summary&gt;å±•å¼€&lt;/summary&gt;

è¿™äº›æ˜¯é¢„æƒ³ä¸­çš„åŠŸèƒ½ï¼Œåœ¨å¼€å‘åˆæœŸå·²é¢„ç•™å¥½æ¥å£ï¼Œå°†åœ¨è¿œæœŸæ…¢æ…¢å®ç°ã€‚

ä½†å¼€å‘é€”ä¸­å—é™äºå®é™…æƒ…å†µï¼Œå¯èƒ½æ›´æ”¹åŠŸèƒ½è®¾è®¡ã€æ–°å¢åŠå–æ¶ˆåŠŸèƒ½ã€‚

- [ ] é‡æ„åº•å±‚æ’ä»¶æœºåˆ¶ã€‚
- [ ] åœ¨çº¿ OCR API æ’ä»¶ã€‚
- [ ] ç‹¬ç«‹çš„æ•°å­¦å…¬å¼è¯†åˆ«æ’ä»¶ã€‚
- [ ] â€œæ•°å­¦å…¬å¼â€æ ‡ç­¾é¡µï¼Œæä¾›ç‹¬ç«‹çš„æ•°å­¦å…¬å¼è¯†åˆ«/Latexæ¸²æŸ“ã€‚
- [ ] æ£€æŸ¥æ›´æ–°æœºåˆ¶ã€‚
- [ ] æ’ç‰ˆè§£æä¹‹å¤–çš„æ–‡æœ¬åå¤„ç†æ¨¡å—ï¼ˆå¦‚ä¿ç•™æ•°å­—ã€åŠå…¨è§’å­—ç¬¦è½¬æ¢ã€æ–‡æœ¬çº é”™ï¼‰ã€‚
- [ ] å…³é”®æ¥å£å‡½æ•°æ·»åŠ äº‹ä»¶è§¦å‘æ–¹å¼ã€‚

- åŸºäºGPUçš„ç¦»çº¿OCRã€‚
- å›¾ç‰‡ç¿»è¯‘
- ç¦»çº¿ç¿»è¯‘ã€‚
- å›ºå®šåŒºåŸŸè¯†åˆ«ã€‚
- è¯†åˆ«è¡¨æ ¼å›¾ç‰‡ï¼Œè¾“å‡ºä¸ºExcelã€‚
- å†å²è®°å½•ç³»ç»Ÿã€‚
- å…¼å®¹ MacOS / Ubuntu ç­‰å¹³å°ã€‚

&lt;/details&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Lightricks/ComfyUI-LTXVideo]]></title>
            <link>https://github.com/Lightricks/ComfyUI-LTXVideo</link>
            <guid>https://github.com/Lightricks/ComfyUI-LTXVideo</guid>
            <pubDate>Fri, 09 May 2025 00:04:13 GMT</pubDate>
            <description><![CDATA[LTX-Video Support for ComfyUI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Lightricks/ComfyUI-LTXVideo">Lightricks/ComfyUI-LTXVideo</a></h1>
            <p>LTX-Video Support for ComfyUI</p>
            <p>Language: Python</p>
            <p>Stars: 1,197</p>
            <p>Forks: 94</p>
            <p>Stars today: 31 stars today</p>
            <h2>README</h2><pre># ComfyUI-LTXVideo

ComfyUI-LTXVideo is a collection of custom nodes for ComfyUI, designed to provide useful tools for working with the LTXV model.
The model itself is supported in the core ComfyUI [code](https://github.com/comfyanonymous/ComfyUI/tree/master/comfy/ldm/lightricks).
The main LTXVideo repository can be found [here](https://github.com/Lightricks/LTX-Video).

# â­ 06.05.2025 â€“ LTXVideo 13B 0.9.7 Release â­

### ğŸš€ What&#039;s New in LTXVideo 13B 0.9.7

1. **LTXV 13B 0.9.7**
   Delivers cinematic-quality videos at unprecedented speed.&lt;br&gt;
   ğŸ‘‰ [Download here](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev.safetensors)

2. **LTXV 13B Quantized 0.9.7**
   Offers reduced memory requirements and even faster inference speeds.
   Ideal for consumer-grade GPUs (e.g., NVIDIA 4090, 5090).
   Delivers outstanding quality with improved performance.&lt;br&gt;
   ***Important:*** In order to run the quantized version please install [LTXVideo-Q8-Kernels](https://github.com/Lightricks/LTXVideo-Q8-Kernels) package and use dedicated flow below. Loading the model in Comfy with LoadCheckpoint node won&#039;t work. &lt;br&gt;
   ğŸ‘‰ [Download here](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev-fp8.safetensors)&lt;br&gt;
   ğŸ§© Example ComfyUI flow available in the [Example Workflows](#example-workflows) section.

3. **Latent Upscaling Models**
   Enables inference across multiple scales by upscaling latent tensors without decoding/encoding.
   Multiscale inference delivers high-quality results in a fraction of the time compared to similar models.&lt;br&gt;
   ***Important:*** Make sure you put the models below in **models/upscale_models** folder.&lt;br&gt;
   ğŸ‘‰ Spatial upscaling: [Download here](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-spatial-upscaler-0.9.7.safetensors).&lt;br&gt;
   ğŸ‘‰ Temporal upscaling: [Download here](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-temporal-upscaler-0.9.7.safetensors).&lt;br&gt;
   ğŸ§© Example ComfyUI flow available in the [Example Workflows](#example-workflows) section.


### Technical Updates

1. ***New simplified flows and nodes***&lt;br&gt;
1.1. Simplified image to video: [Download here](example_workflows/ltxv-13b-i2v-base.json).&lt;br&gt;
1.2. Simplified image to video with extension: [Download here](example_workflows/ltxv-13b-i2v-extend.json).&lt;br&gt;
1.3. Simplified image to video with keyframes: [Download here](example_workflows/ltxv-13b-i2v-keyframes.json).&lt;br&gt;

# 17.04.2025 â­ LTXVideo 0.9.6 Release â­

### LTXVideo 0.9.6 introduces:

1. LTXV 0.9.6 â€“ higher quality, faster, great for final output. Download from [here](https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltxv-2b-0.9.6-dev-04-25.safetensors).
2. LTXV 0.9.6 Distilled â€“ our fastest model yet (only 8 steps for generation), lighter, great for rapid iteration. Download from [here](https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltxv-2b-0.9.6-distilled-04-25.safetensors).

### Technical Updates

We introduce the __STGGuiderAdvanced__ node, which applies different CFG and STG parameters at various diffusion steps. All flows have been updated to use this node and are designed to provide optimal parameters for the best quality.
See the [Example Workflows](#example-workflows) section.

# 5.03.2025 â­ LTXVideo 0.9.5 Release â­

### LTXVideo 0.9.5 introduces:

1. Improved quality with reduced artifacts.
2. Support for higher resolution and longer sequences.
3. Frame and sequence conditioning (beyond the first frame).
4. Enhanced prompt understanding.
5. Commercial license availability.

### Technical Updates

Since LTXVideo is now fully supported in the ComfyUI core, we have removed the custom model implementation. Instead, we provide updated workflows to showcase the new features:

1. **Frame Conditioning** â€“ Enables interpolation between given frames.
2. **Sequence Conditioning** â€“ Allows motion interpolation from a given frame sequence, enabling video extension from the beginning, end, or middle of the original video.
3. **Prompt Enhancer** â€“ A new node that helps generate prompts optimized for the best model performance.
   See the [Example Workflows](#example-workflows) section for more details.

### LTXTricks Update

The LTXTricks code has been integrated into this repository (in the `/tricks` folder) and will be maintained here. The original [repo](https://github.com/logtd/ComfyUI-LTXTricks) is no longer maintained, but all existing workflows should continue to function as expected.

## 22.12.2024

Fixed a bug which caused the model to produce artifacts on short negative prompts when using a native CLIP Loader node.

## 19.12.2024 â­ Update â­

1. Improved model - removes &quot;strobing texture&quot; artifacts and generates better motion. Download from [here](https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltx-video-2b-v0.9.1.safetensors).
2. STG support
3. Integrated image degradation system for improved motion generation.
4. Additional initial latent optional input to chain latents for high res generation.
5. Image captioning in image to video [flow](example_workflows/ltxvideo-i2v.json).

## Installation

Installation via [ComfyUI-Manager](https://github.com/ltdrdata/ComfyUI-Manager) is preferred. Simply search for `ComfyUI-LTXVideo` in the list of nodes and follow installation instructions.

### Manual installation

1. Install ComfyUI
2. Clone this repository to `custom-nodes` folder in your ComfyUI installation directory.
3. Install the required packages:

```bash
cd custom_nodes/ComfyUI-LTXVideo &amp;&amp; pip install -r requirements.txt
```

For portable ComfyUI installations, run

```
.\python_embeded\python.exe -m pip install -r .\ComfyUI\custom_nodes\ComfyUI-LTXVideo\requirements.txt
```

### Models

1. Download [ltx-video-2b-v0.9.1.safetensors](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltx-video-2b-v0.9.1.safetensors) from Hugging Face and place it under `models/checkpoints`.
2. Install one of the t5 text encoders, for example [google_t5-v1_1-xxl_encoderonly](https://huggingface.co/mcmonkey/google_t5-v1_1-xxl_encoderonly/tree/main). You can install it using ComfyUI Model Manager.

## Example workflows

Note that to run the example workflows, you need to have some additional custom nodes, like [ComfyUI-VideoHelperSuite](https://github.com/kosinkadink/ComfyUI-VideoHelperSuite) and others, installed. You can do it by pressing &quot;Install Missing Custom Nodes&quot; button in ComfyUI Manager.

### Easy to use multi scale generation workflows

ğŸ§© [Image to video](example_workflows/ltxv-13b-i2v-base.json)&lt;br&gt;
ğŸ§© [Image to video with keyframes](example_workflows/ltxv-13b-i2v-keyframes.json)&lt;br&gt;
ğŸ§© [Image to video with duration extension](example_workflows/ltxv-13b-i2v-extend.json)&lt;br&gt;
ğŸ§© [Image to video 8b quantized](example_workflows/ltxv-13b-i2v-base-fp8.json)

### Inversion

#### Flow Edit

ğŸ§© [Download workflow](example_workflows/tricks/ltxvideo-flow-edit.json)&lt;br&gt;
![workflow](example_workflows/tricks/ltxvideo-flow-edit.png)

#### RF Edit

ğŸ§© [Download workflow](example_workflows/tricks/ltxvideo-rf-edit.json)&lt;br&gt;
![workflow](example_workflows/tricks/ltxvideo-rf-edit.png)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[yeongpin/cursor-free-vip]]></title>
            <link>https://github.com/yeongpin/cursor-free-vip</link>
            <guid>https://github.com/yeongpin/cursor-free-vip</guid>
            <pubDate>Fri, 09 May 2025 00:04:12 GMT</pubDate>
            <description><![CDATA[[Support 0.49.x]ï¼ˆReset Cursor AI MachineID & Bypass Higher Token Limitï¼‰ Cursor Ai ï¼Œè‡ªåŠ¨é‡ç½®æœºå™¨ID ï¼Œ å…è´¹å‡çº§ä½¿ç”¨ProåŠŸèƒ½: You've reached your trial request limit. / Too many free trial accounts used on this machine. Please upgrade to pro. We have this limit in place to prevent abuse. Please let us know if you believe this is a mistake.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/yeongpin/cursor-free-vip">yeongpin/cursor-free-vip</a></h1>
            <p>[Support 0.49.x]ï¼ˆReset Cursor AI MachineID & Bypass Higher Token Limitï¼‰ Cursor Ai ï¼Œè‡ªåŠ¨é‡ç½®æœºå™¨ID ï¼Œ å…è´¹å‡çº§ä½¿ç”¨ProåŠŸèƒ½: You've reached your trial request limit. / Too many free trial accounts used on this machine. Please upgrade to pro. We have this limit in place to prevent abuse. Please let us know if you believe this is a mistake.</p>
            <p>Language: Python</p>
            <p>Stars: 24,335</p>
            <p>Forks: 3,032</p>
            <p>Stars today: 310 stars today</p>
            <h2>README</h2><pre># â¤ Cursor Free VIP

&lt;div align=&quot;center&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./images/logo.png&quot; alt=&quot;Cursor Pro Logo&quot; width=&quot;200&quot; style=&quot;border-radius: 6px;&quot;/&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;

[![Release](https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/release/yeongpin/cursor-free-vip)](https://github.com/yeongpin/cursor-free-vip/releases/latest)
[![License: CC BY-NC-ND 4.0](https://img.shields.io/badge/License-CC_BY--NC--ND_4.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc-nd/4.0/)
[![Stars](https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/stars/yeongpin/cursor-free-vip)](https://github.com/yeongpin/cursor-free-vip/stargazers)
[![Downloads](https://img.shields.io/endpoint?url=https://api.pinstudios.net/api/badges/downloads/yeongpin/cursor-free-vip/total)](https://github.com/yeongpin/cursor-free-vip/releases/latest)
&lt;a href=&quot;https://buymeacoffee.com/yeongpin&quot; target=&quot;_blank&quot;&gt;&lt;img alt=&quot;Buy Me a Coffee&quot; src=&quot;https://img.shields.io/badge/Buy%20Me%20a%20Coffee-Support%20Me-FFDA33&quot;&gt;&lt;/a&gt;
 [&lt;img src=&quot;https://devin.ai/assets/deepwiki-badge.png&quot; alt=&quot;Ask DeepWiki.com&quot; height=&quot;20&quot;/&gt;](https://deepwiki.com/yeongpin/cursor-free-vip)

&lt;/p&gt;


&lt;a href=&quot;https://trendshift.io/repositories/13425&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13425&quot; alt=&quot;yeongpin%2Fcursor-free-vip | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;br&gt;
&lt;a href=&quot;https://www.buymeacoffee.com/yeongpin&quot; target=&quot;_blank&quot;&gt;
      &lt;img src=&quot;https://img.buymeacoffee.com/button-api/?text=buy me a coffee&amp;emoji=â˜•&amp;slug=yeongpin&amp;button_colour=ffda33&amp;font_colour=000000&amp;font_family=Bree&amp;outline_colour=000000&amp;coffee_colour=FFDD00&amp;latest=2&quot; width=&quot;160&quot; height=&#039;55&#039; alt=&quot;Buy Me a Coffee&quot;/&gt;
&lt;/a&gt;


&lt;h4&gt;Support Latest 0.49.x Version | æ”¯æŒæœ€æ–° 0.49.x ç‰ˆæœ¬&lt;/h4&gt;

This tool is for educational purposes, currently the repo does not violate any laws. Please support the original project.
This tool will not generate any fake email accounts and OAuth access.

Supports Windows, macOS and Linux.

For optimal performance, run with privileges and always stay up to date.

é€™æ˜¯ä¸€æ¬¾ç”¨æ–¼å­¸ç¿’å’Œç ”ç©¶çš„å·¥å…·ï¼Œç›®å‰ repo æ²’æœ‰é•åä»»ä½•æ³•å¾‹ã€‚è«‹æ”¯æŒåŸä½œè€…ã€‚
é€™æ¬¾å·¥å…·ä¸æœƒç”Ÿæˆä»»ä½•å‡çš„é›»å­éƒµä»¶å¸³æˆ¶å’Œ OAuth è¨ªå•ã€‚

æ”¯æŒ Windowsã€macOS å’Œ Linuxã€‚

å°æ–¼æœ€ä½³æ€§èƒ½ï¼Œè«‹ä»¥ç®¡ç†å“¡èº«ä»½é‹è¡Œä¸¦å§‹çµ‚ä¿æŒæœ€æ–°ã€‚


&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./images/product_2025-04-16_10-40-21.png&quot; alt=&quot;new&quot; width=&quot;800&quot; style=&quot;border-radius: 6px;&quot;/&gt;&lt;br&gt;
&lt;/p&gt;

&lt;/div&gt;

## ğŸ”„ Change Log | æ›´æ–°æ—¥å¿—

[Watch Change Log | æŸ¥çœ‹æ›´æ–°æ—¥å¿—](CHANGELOG.md)

## âœ¨ Features | åŠŸèƒ½ç‰¹é»

* Support Windows macOS and Linux systems&lt;br&gt;æ”¯æŒ Windowsã€macOS å’Œ Linux ç³»çµ±&lt;br&gt;

* Reset Cursor&#039;s configuration&lt;br&gt;é‡ç½® Cursor çš„é…ç½®&lt;br&gt;

* Multi-language support (English, ç®€ä½“ä¸­æ–‡, ç¹é«”ä¸­æ–‡, Vietnamese)&lt;br&gt;å¤šèªè¨€æ”¯æŒï¼ˆè‹±æ–‡ã€ç®€ä½“ä¸­æ–‡ã€ç¹é«”ä¸­æ–‡ã€è¶Šå—èªï¼‰&lt;br&gt;

## ğŸ’» System Support | ç³»çµ±æ”¯æŒ

| Operating System | Architecture      | Supported |
|------------------|-------------------|-----------|
| Windows          | x64, x86          | âœ…         |
| macOS            | Intel, Apple Silicon | âœ…      |
| Linux            | x64, x86, ARM64   | âœ…         |

## ğŸ‘€ How to use | å¦‚ä½•ä½¿ç”¨

&lt;details open&gt;
&lt;summary&gt;&lt;b&gt;â­ Auto Run Script | è…³æœ¬è‡ªå‹•åŒ–é‹è¡Œ&lt;/b&gt;&lt;/summary&gt;

### **Linux/macOS**

```bash
curl -fsSL https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/install.sh -o install.sh &amp;&amp; chmod +x install.sh &amp;&amp; ./install.sh
```

### **Archlinux**

Install via [AUR](https://aur.archlinux.org/packages/cursor-free-vip-git)

```bash
yay -S cursor-free-vip-git
```

### **Windows**

```powershell
irm https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/install.ps1 | iex
```

&lt;/details&gt;

If you want to stop the script, please press Ctrl+C&lt;br&gt;è¦åœæ­¢è…³æœ¬ï¼Œè«‹æŒ‰ Ctrl+C

## â— Note | æ³¨æ„äº‹é …

ğŸ“ Config | æ–‡ä»¶é…ç½®
`Win / Macos / Linux Path | è·¯å¾‘ [Documents/.cursor-free-vip/config.ini]`
&lt;details&gt;
&lt;summary&gt;&lt;b&gt;â­ Config | æ–‡ä»¶é…ç½®&lt;/b&gt;&lt;/summary&gt;

```
[Chrome]
# Default Google Chrome Path | é»˜èªGoogle Chrome éŠè¦½å™¨è·¯å¾‘
chromepath = C:\Program Files\Google/Chrome/Application/chrome.exe

[Turnstile]
# Handle Turnstile Wait Time | ç­‰å¾…äººæ©Ÿé©—è­‰æ™‚é–“
handle_turnstile_time = 2
# Handle Turnstile Wait Random Time (must merge 1-3 or 1,3) | ç­‰å¾…äººæ©Ÿé©—è­‰éš¨æ©Ÿæ™‚é–“ï¼ˆå¿…é ˆæ˜¯ 1-3 æˆ–è€… 1,3 é€™æ¨£çš„çµ„åˆï¼‰
handle_turnstile_random_time = 1-3

[OSPaths]
# Storage Path | å­˜å„²è·¯å¾‘
storage_path = /Users/username/Library/Application Support/Cursor/User/globalStorage/storage.json
# SQLite Path | SQLiteè·¯å¾‘
sqlite_path = /Users/username/Library/Application Support/Cursor/User/globalStorage/state.vscdb
# Machine ID Path | æ©Ÿå™¨IDè·¯å¾‘
machine_id_path = /Users/username/Library/Application Support/Cursor/machineId
# For Linux users: ~/.config/cursor/machineid

[Timing]
# Min Random Time | æœ€å°éš¨æ©Ÿæ™‚é–“
min_random_time = 0.1
# Max Random Time | æœ€å¤§éš¨æ©Ÿæ™‚é–“
max_random_time = 0.8
# Page Load Wait | é é¢åŠ è¼‰ç­‰å¾…æ™‚é–“
page_load_wait = 0.1-0.8
# Input Wait | è¼¸å…¥ç­‰å¾…æ™‚é–“
input_wait = 0.3-0.8
# Submit Wait | æäº¤ç­‰å¾…æ™‚é–“
submit_wait = 0.5-1.5
# Verification Code Input | é©—è­‰ç¢¼è¼¸å…¥ç­‰å¾…æ™‚é–“
verification_code_input = 0.1-0.3
# Verification Success Wait | é©—è­‰æˆåŠŸç­‰å¾…æ™‚é–“
verification_success_wait = 2-3
# Verification Retry Wait | é©—è­‰é‡è©¦ç­‰å¾…æ™‚é–“
verification_retry_wait = 2-3
# Email Check Initial Wait | éƒµä»¶æª¢æŸ¥åˆå§‹ç­‰å¾…æ™‚é–“
email_check_initial_wait = 4-6
# Email Refresh Wait | éƒµä»¶åˆ·æ–°ç­‰å¾…æ™‚é–“
email_refresh_wait = 2-4
# Settings Page Load Wait | è¨­ç½®é é¢åŠ è¼‰ç­‰å¾…æ™‚é–“
settings_page_load_wait = 1-2
# Failed Retry Time | å¤±æ•—é‡è©¦æ™‚é–“
failed_retry_time = 0.5-1
# Retry Interval | é‡è©¦é–“éš”
retry_interval = 8-12
# Max Timeout | æœ€å¤§è¶…æ™‚æ™‚é–“
max_timeout = 160

[Utils]
# Check Update | æª¢æŸ¥æ›´æ–°
check_update = True
# Show Account Info | é¡¯ç¤ºè³¬è™Ÿä¿¡æ¯
show_account_info = True

[TempMailPlus]
# Enable TempMailPlus | å•“ç”¨ TempMailPlusï¼ˆä»»ä½•è½‰ç™¼åˆ°TempMailPlusçš„éƒµä»¶éƒ½æ”¯æŒç²å–é©—è­‰ç¢¼ï¼Œä¾‹å¦‚cloudflareéƒµä»¶Catch-allï¼‰
enabled = false
# TempMailPlus Email | TempMailPlus é›»å­éƒµä»¶
email = xxxxx@mailto.plus
# TempMailPlus pin | TempMailPlus pinç¢¼
epin = 

[WindowsPaths]
storage_path = C:\Users\yeongpin\AppData\Roaming\Cursor\User\globalStorage\storage.json
sqlite_path = C:\Users\yeongpin\AppData\Roaming\Cursor\User\globalStorage\state.vscdb
machine_id_path = C:\Users\yeongpin\AppData\Roaming\Cursor\machineId
cursor_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app
updater_path = C:\Users\yeongpin\AppData\Local\cursor-updater
update_yml_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app-update.yml
product_json_path = C:\Users\yeongpin\AppData\Local\Programs\Cursor\resources\app\product.json

[Browser]
default_browser = opera
chrome_path = C:\Program Files\Google\Chrome\Application\chrome.exe
edge_path = C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe
firefox_path = C:\Program Files\Mozilla Firefox\firefox.exe
brave_path = C:\Program Files\BraveSoftware/Brave-Browser/Application/brave.exe
chrome_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe
edge_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\msedgedriver.exe
firefox_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\geckodriver.exe
brave_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe
opera_path = C:\Users\yeongpin\AppData\Local\Programs\Opera\opera.exe
opera_driver_path = D:\VisualCode\cursor-free-vip-new\drivers\chromedriver.exe

[OAuth]
show_selection_alert = False
timeout = 120
max_attempts = 3
```

&lt;/details&gt;

* Use administrator privileges to run the script &lt;br&gt;è«‹ä½¿ç”¨ç®¡ç†å“¡èº«ä»½é‹è¡Œè…³æœ¬

* Confirm that Cursor is closed before running the script &lt;br&gt;è«‹ç¢ºä¿åœ¨é‹è¡Œè…³æœ¬å‰å·²ç¶“é—œé–‰ Cursor&lt;br&gt;

* This tool is only for learning and research purposes &lt;br&gt;æ­¤å·¥å…·åƒ…ä¾›å­¸ç¿’å’Œç ”ç©¶ä½¿ç”¨&lt;br&gt;

* Please comply with the relevant software usage terms when using this tool &lt;br&gt;ä½¿ç”¨æœ¬å·¥å…·æ™‚è«‹éµå®ˆç›¸é—œè»Ÿä»¶ä½¿ç”¨æ¢æ¬¾

## ğŸš¨ Common Issues | å¸¸è¦‹å•é¡Œ

|                   å¦‚æœé‡åˆ°æ¬Šé™å•é¡Œï¼Œè«‹ç¢ºä¿ï¼š                    |                   æ­¤è…³æœ¬ä»¥ç®¡ç†å“¡èº«ä»½é‹è¡Œ                    |
|:--------------------------------------------------:|:------------------------------------------------:|
| If you encounter permission issues, please ensure: | This script is run with administrator privileges |
| Error &#039;User is not authorized&#039; | This means your account was banned for using temporary (disposal) mail. Ensure using a non-temporary mail service |
## ğŸ¤© Contribution | è²¢ç»

æ­¡è¿æäº¤ Issue å’Œ Pull Requestï¼


&lt;a href=&quot;https://github.com/yeongpin/cursor-free-vip/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=yeongpin/cursor-free-vip&amp;preview=true&amp;max=&amp;columns=&quot; /&gt;
&lt;/a&gt;
&lt;br /&gt;&lt;br /&gt;

## ğŸ“© Disclaimer | å…è²¬è²æ˜

æœ¬å·¥å…·åƒ…ä¾›å­¸ç¿’å’Œç ”ç©¶ä½¿ç”¨ï¼Œä½¿ç”¨æœ¬å·¥å…·æ‰€ç”¢ç”Ÿçš„ä»»ä½•å¾Œæœç”±ä½¿ç”¨è€…è‡ªè¡Œæ‰¿æ“”ã€‚ &lt;br&gt;

This tool is only for learning and research purposes, and any consequences arising from the use of this tool are borne
by the user.

## ğŸ’° Buy Me a Coffee | è«‹æˆ‘å–æ¯å’–å•¡

&lt;div align=&quot;center&quot;&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;img src=&quot;./images/provi-code.jpg&quot; alt=&quot;buy_me_a_coffee&quot; width=&quot;280&quot;/&gt;&lt;br&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;img src=&quot;./images/paypal.png&quot; alt=&quot;buy_me_a_coffee&quot; width=&quot;280&quot;/&gt;&lt;br&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;

## â­ Star History | æ˜Ÿæ˜Ÿæ•¸

&lt;div align=&quot;center&quot;&gt;

[![Star History Chart](https://api.star-history.com/svg?repos=yeongpin/cursor-free-vip&amp;type=Date)](https://star-history.com/#yeongpin/cursor-free-vip&amp;Date)

&lt;/div&gt;

## ğŸ“ License | æˆæ¬Š

æœ¬é …ç›®æ¡ç”¨ [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/) æˆæ¬Šã€‚
Please refer to the [LICENSE](LICENSE.md) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ansible/ansible]]></title>
            <link>https://github.com/ansible/ansible</link>
            <guid>https://github.com/ansible/ansible</guid>
            <pubDate>Fri, 09 May 2025 00:04:11 GMT</pubDate>
            <description><![CDATA[Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain. Automate everything from code deployment to network configuration to cloud management, in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ansible/ansible">ansible/ansible</a></h1>
            <p>Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain. Automate everything from code deployment to network configuration to cloud management, in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com.</p>
            <p>Language: Python</p>
            <p>Stars: 64,939</p>
            <p>Forks: 24,017</p>
            <p>Stars today: 17 stars today</p>
            <h2>README</h2><pre>[![PyPI version](https://img.shields.io/pypi/v/ansible-core.svg)](https://pypi.org/project/ansible-core)
[![Docs badge](https://img.shields.io/badge/docs-latest-brightgreen.svg)](https://docs.ansible.com/ansible/latest/)
[![Chat badge](https://img.shields.io/badge/chat-IRC-brightgreen.svg)](https://docs.ansible.com/ansible/devel/community/communication.html)
[![Build Status](https://dev.azure.com/ansible/ansible/_apis/build/status/CI?branchName=devel)](https://dev.azure.com/ansible/ansible/_build/latest?definitionId=20&amp;branchName=devel)
[![Ansible Code of Conduct](https://img.shields.io/badge/code%20of%20conduct-Ansible-silver.svg)](https://docs.ansible.com/ansible/devel/community/code_of_conduct.html)
[![Ansible mailing lists](https://img.shields.io/badge/mailing%20lists-Ansible-orange.svg)](https://docs.ansible.com/ansible/devel/community/communication.html#mailing-list-information)
[![Repository License](https://img.shields.io/badge/license-GPL%20v3.0-brightgreen.svg)](COPYING)
[![Ansible CII Best Practices certification](https://bestpractices.coreinfrastructure.org/projects/2372/badge)](https://bestpractices.coreinfrastructure.org/projects/2372)

# Ansible

Ansible is a radically simple IT automation system. It handles
configuration management, application deployment, cloud provisioning,
ad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complex
changes like zero-downtime rolling updates with load balancers easy. More information on the Ansible [website](https://ansible.com/).

## Design Principles

* Have an extremely simple setup process with a minimal learning curve.
* Manage machines quickly and in parallel.
* Avoid custom-agents and additional open ports, be agentless by
  leveraging the existing SSH daemon.
* Describe infrastructure in a language that is both machine and human
  friendly.
* Focus on security and easy auditability/review/rewriting of content.
* Manage new remote machines instantly, without bootstrapping any
  software.
* Allow module development in any dynamic language, not just Python.
* Be usable as non-root.
* Be the easiest IT automation system to use, ever.

## Use Ansible

You can install a released version of Ansible with `pip` or a package manager. See our
[installation guide](https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html) for details on installing Ansible
on a variety of platforms.

Power users and developers can run the `devel` branch, which has the latest
features and fixes, directly. Although it is reasonably stable, you are more likely to encounter
breaking changes when running the `devel` branch. We recommend getting involved
in the Ansible community if you want to run the `devel` branch.

## Communication

Join the Ansible forum to ask questions, get help, and interact with the
community.

* [Get Help](https://forum.ansible.com/c/help/6): Find help or share your Ansible knowledge to help others.
  Use tags to filter and subscribe to posts, such as the following:
  * Posts tagged with [ansible](https://forum.ansible.com/tag/ansible)
  * Posts tagged with [ansible-core](https://forum.ansible.com/tag/ansible-core)
  * Posts tagged with [playbook](https://forum.ansible.com/tag/playbook)
* [Social Spaces](https://forum.ansible.com/c/chat/4): Meet and interact with fellow enthusiasts.
* [News &amp; Announcements](https://forum.ansible.com/c/news/5): Track project-wide announcements including social events.
* [Bullhorn newsletter](https://docs.ansible.com/ansible/devel/community/communication.html#the-bullhorn): Get release announcements and important changes.

For more ways to get in touch, see [Communicating with the Ansible community](https://docs.ansible.com/ansible/devel/community/communication.html).

## Contribute to Ansible

* Check out the [Contributor&#039;s Guide](./.github/CONTRIBUTING.md).
* Read [Community Information](https://docs.ansible.com/ansible/devel/community) for all
  kinds of ways to contribute to and interact with the project,
  including how to submit bug reports and code to Ansible.
* Submit a proposed code update through a pull request to the `devel` branch.
* Talk to us before making larger changes
  to avoid duplicate efforts. This not only helps everyone
  know what is going on, but it also helps save time and effort if we decide
  some changes are needed.

## Coding Guidelines

We document our Coding Guidelines in the [Developer Guide](https://docs.ansible.com/ansible/devel/dev_guide/). We particularly suggest you review:

* [Contributing your module to Ansible](https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_checklist.html)
* [Conventions, tips, and pitfalls](https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_best_practices.html)

## Branch Info

* The `devel` branch corresponds to the release actively under development.
* The `stable-2.X` branches correspond to stable releases.
* Create a branch based on `devel` and set up a [dev environment](https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_general.html#common-environment-setup) if you want to open a PR.
* See the [Ansible release and maintenance](https://docs.ansible.com/ansible/devel/reference_appendices/release_and_maintenance.html) page for information about active branches.

## Roadmap

Based on team and community feedback, an initial roadmap will be published for a major or minor version (ex: 2.7, 2.8).
The [Ansible Roadmap page](https://docs.ansible.com/ansible/devel/roadmap/) details what is planned and how to influence the roadmap.

## Authors

Ansible was created by [Michael DeHaan](https://github.com/mpdehaan)
and has contributions from over 5000 users (and growing). Thanks everyone!

[Ansible](https://www.ansible.com) is sponsored by [Red Hat, Inc.](https://www.redhat.com)

## License

GNU General Public License v3.0 or later

See [COPYING](COPYING) to see the full text.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[huggingface/smolagents]]></title>
            <link>https://github.com/huggingface/smolagents</link>
            <guid>https://github.com/huggingface/smolagents</guid>
            <pubDate>Fri, 09 May 2025 00:04:10 GMT</pubDate>
            <description><![CDATA[ğŸ¤— smolagents: a barebones library for agents that think in python code.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/huggingface/smolagents">huggingface/smolagents</a></h1>
            <p>ğŸ¤— smolagents: a barebones library for agents that think in python code.</p>
            <p>Language: Python</p>
            <p>Stars: 18,245</p>
            <p>Forks: 1,586</p>
            <p>Stars today: 113 stars today</p>
            <h2>README</h2><pre>&lt;!---
Copyright 2024 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
--&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;!-- Uncomment when CircleCI is set up
    &lt;a href=&quot;https://circleci.com/gh/huggingface/accelerate&quot;&gt;&lt;img alt=&quot;Build&quot; src=&quot;https://img.shields.io/circleci/build/github/huggingface/transformers/master&quot;&gt;&lt;/a&gt;
    --&gt;
    &lt;a href=&quot;https://github.com/huggingface/smolagents/blob/main/LICENSE&quot;&gt;&lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/github/license/huggingface/smolagents.svg?color=blue&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://huggingface.co/docs/smolagents&quot;&gt;&lt;img alt=&quot;Documentation&quot; src=&quot;https://img.shields.io/website/http/huggingface.co/docs/smolagents/index.html.svg?down_color=red&amp;down_message=offline&amp;up_message=online&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/huggingface/smolagents/releases&quot;&gt;&lt;img alt=&quot;GitHub release&quot; src=&quot;https://img.shields.io/github/release/huggingface/smolagents.svg&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/huggingface/smolagents/blob/main/CODE_OF_CONDUCT.md&quot;&gt;&lt;img alt=&quot;Contributor Covenant&quot; src=&quot;https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;h3 align=&quot;center&quot;&gt;
  &lt;div style=&quot;display:flex;flex-direction:row;&quot;&gt;
    &lt;img src=&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/smolagents.png&quot; alt=&quot;Hugging Face mascot as James Bond&quot; width=400px&gt;
    &lt;p&gt;Agents that think in code!&lt;/p&gt;
  &lt;/div&gt;
&lt;/h3&gt;

`smolagents` is a library that enables you to run powerful agents in a few lines of code. It offers:

âœ¨ **Simplicity**: the logic for agents fits in ~1,000 lines of code (see [agents.py](https://github.com/huggingface/smolagents/blob/main/src/smolagents/agents.py)). We kept abstractions to their minimal shape above raw code!

ğŸ§‘â€ğŸ’» **First-class support for Code Agents**. Our [`CodeAgent`](https://huggingface.co/docs/smolagents/reference/agents#smolagents.CodeAgent) writes its actions in code (as opposed to &quot;agents being used to write code&quot;). To make it secure, we support executing in sandboxed environments via [E2B](https://e2b.dev/) or via Docker.

ğŸ¤— **Hub integrations**: you can [share/pull tools or agents to/from the Hub](https://huggingface.co/docs/smolagents/reference/tools#smolagents.Tool.from_hub) for instant sharing of the most efficient agents!

ğŸŒ **Model-agnostic**: smolagents supports any LLM. It can be a local `transformers` or `ollama` model, one of [many providers on the Hub](https://huggingface.co/blog/inference-providers), or any model from OpenAI, Anthropic and many others via our [LiteLLM](https://www.litellm.ai/) integration.

ğŸ‘ï¸ **Modality-agnostic**: Agents support text, vision, video, even audio inputs! Cf [this tutorial](https://huggingface.co/docs/smolagents/examples/web_browser) for vision.

ğŸ› ï¸ **Tool-agnostic**: you can use tools from [LangChain](https://huggingface.co/docs/smolagents/reference/tools#smolagents.Tool.from_langchain), [MCP](https://huggingface.co/docs/smolagents/reference/tools#smolagents.ToolCollection.from_mcp), you can even use a [Hub Space](https://huggingface.co/docs/smolagents/reference/tools#smolagents.Tool.from_space) as a tool.

Full documentation can be found [here](https://huggingface.co/docs/smolagents/index).

&gt; [!NOTE]
&gt; Check the our [launch blog post](https://huggingface.co/blog/smolagents) to learn more about `smolagents`!

## Quick demo

First install the package with a default set of tools:
```bash
pip install smolagents[toolkit]
```
Then define your agent, give it the tools it needs and run it!
```py
from smolagents import CodeAgent, WebSearchTool, InferenceClientModel

model = InferenceClientModel()
agent = CodeAgent(tools=[WebSearchTool()], model=model)

agent.run(&quot;How many seconds would it take for a leopard at full speed to run through Pont des Arts?&quot;)
```

https://github.com/user-attachments/assets/cd0226e2-7479-4102-aea0-57c22ca47884

You can even share your agent to the Hub, as a Space repository:
```py
agent.push_to_hub(&quot;m-ric/my_agent&quot;)

# agent.from_hub(&quot;m-ric/my_agent&quot;) to load an agent from Hub
```

Our library is LLM-agnostic: you could switch the example above to any inference provider.

&lt;details&gt;
&lt;summary&gt; &lt;b&gt;InferenceClientModel, gateway for all &lt;a href=&quot;https://huggingface.co/docs/inference-providers/index&quot;&gt;inference providers&lt;/a&gt; supported on HF&lt;/b&gt;&lt;/summary&gt;

```py
from smolagents import InferenceClientModel

model = InferenceClientModel(
    model_id=&quot;deepseek-ai/DeepSeek-R1&quot;,
    provider=&quot;together&quot;,
)
```
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt; &lt;b&gt;LiteLLM to access 100+ LLMs&lt;/b&gt;&lt;/summary&gt;

```py
from smolagents import LiteLLMModel

model = LiteLLMModel(
    model_id=&quot;anthropic/claude-3-5-sonnet-latest&quot;,
    temperature=0.2,
    api_key=os.environ[&quot;ANTHROPIC_API_KEY&quot;]
)
```
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt; &lt;b&gt;OpenAI-compatible servers&lt;/b&gt;&lt;/summary&gt;

```py
import os
from smolagents import OpenAIServerModel

model = OpenAIServerModel(
    model_id=&quot;deepseek-ai/DeepSeek-R1&quot;,
    api_base=&quot;https://api.together.xyz/v1/&quot;, # Leave this blank to query OpenAI servers.
    api_key=os.environ[&quot;TOGETHER_API_KEY&quot;], # Switch to the API key for the server you&#039;re targeting.
)
```
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt; &lt;b&gt;Local `transformers` model&lt;/b&gt;&lt;/summary&gt;

```py
from smolagents import TransformersModel

model = TransformersModel(
    model_id=&quot;Qwen/Qwen2.5-Coder-32B-Instruct&quot;,
    max_new_tokens=4096,
    device_map=&quot;auto&quot;
)
```
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt; &lt;b&gt;Azure models&lt;/b&gt;&lt;/summary&gt;

```py
import os
from smolagents import AzureOpenAIServerModel

model = AzureOpenAIServerModel(
    model_id = os.environ.get(&quot;AZURE_OPENAI_MODEL&quot;),
    azure_endpoint=os.environ.get(&quot;AZURE_OPENAI_ENDPOINT&quot;),
    api_key=os.environ.get(&quot;AZURE_OPENAI_API_KEY&quot;),
    api_version=os.environ.get(&quot;OPENAI_API_VERSION&quot;)    
)
```
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt; &lt;b&gt;Amazon Bedrock models&lt;/b&gt;&lt;/summary&gt;

```py
import os
from smolagents import AmazonBedrockServerModel

model = AmazonBedrockServerModel(
    model_id = os.environ.get(&quot;AMAZON_BEDROCK_MODEL_ID&quot;) 
)
```
&lt;/details&gt;

## CLI

You can run agents from CLI using two commands: `smolagent` and `webagent`.

`smolagent` is a generalist command to run a multi-step `CodeAgent` that can be equipped with various tools.

```bash
smolagent &quot;Plan a trip to Tokyo, Kyoto and Osaka between Mar 28 and Apr 7.&quot;  --model-type &quot;InferenceClientModel&quot; --model-id &quot;Qwen/Qwen2.5-Coder-32B-Instruct&quot; --imports &quot;pandas numpy&quot; --tools &quot;web_search&quot;
```

Meanwhile `webagent`Â is a specific web-browsing agent using [helium](https://github.com/mherrmann/helium) (read more [here](https://github.com/huggingface/smolagents/blob/main/src/smolagents/vision_web_browser.py)).

For instance:
```bash
webagent &quot;go to xyz.com/men, get to sale section, click the first clothing item you see. Get the product details, and the price, return them. note that I&#039;m shopping from France&quot; --model-type &quot;LiteLLMModel&quot; --model-id &quot;gpt-4o&quot;
```

## How do Code agents work?

Our [`CodeAgent`](https://huggingface.co/docs/smolagents/reference/agents#smolagents.CodeAgent) works mostly like classical ReAct agents - the exception being that the LLM engine writes its actions as Python code snippets.

```mermaid
flowchart TB
    Task[User Task]
    Memory[agent.memory]
    Generate[Generate from agent.model]
    Execute[Execute Code action - Tool calls are written as functions]
    Answer[Return the argument given to &#039;final_answer&#039;]

    Task --&gt;|Add task to agent.memory| Memory

    subgraph ReAct[ReAct loop]
        Memory --&gt;|Memory as chat messages| Generate
        Generate --&gt;|Parse output to extract code action| Execute
        Execute --&gt;|No call to &#039;final_answer&#039; tool =&gt; Store execution logs in memory and keep running| Memory
    end
    
    Execute --&gt;|Call to &#039;final_answer&#039; tool| Answer

    %% Styling
    classDef default fill:#d4b702,stroke:#8b7701,color:#ffffff
    classDef io fill:#4a5568,stroke:#2d3748,color:#ffffff
    
    class Task,Answer io
```

Actions are now Python code snippets. Hence, tool calls will be performed as Python function calls. For instance, here is how the agent can perform web search over several websites in one single action:
```py
requests_to_search = [&quot;gulf of mexico america&quot;, &quot;greenland denmark&quot;, &quot;tariffs&quot;]
for request in requests_to_search:
    print(f&quot;Here are the search results for {request}:&quot;, web_search(request))
```

Writing actions as code snippets is demonstrated to work better than the current industry practice of letting the LLM output a dictionary of the tools it wants to call: [uses 30% fewer steps](https://huggingface.co/papers/2402.01030) (thus 30% fewer LLM calls) and [reaches higher performance on difficult benchmarks](https://huggingface.co/papers/2411.01747). Head to [our high-level intro to agents](https://huggingface.co/docs/smolagents/conceptual_guides/intro_agents) to learn more on that.

Especially, since code execution can be a security concern (arbitrary code execution!), we provide options at runtime:
  - a secure python interpreter to run code more safely in your environment (more secure than raw code execution but still risky)
  - a sandboxed environment using [E2B](https://e2b.dev/) or Docker (removes the risk to your own system).

On top of this [`CodeAgent`](https://huggingface.co/docs/smolagents/reference/agents#smolagents.CodeAgent) class, we still support the standard [`ToolCallingAgent`](https://huggingface.co/docs/smolagents/reference/agents#smolagents.ToolCallingAgent) that writes actions as JSON/text blobs. But we recommend always using `CodeAgent`.

## How smol is this library?

We strived to keep abstractions to a strict minimum: the main code in `agents.py` has &lt;1,000 lines of code.
Still, we implement several types of agents: `CodeAgent` writes its actions as Python code snippets, and the more classic `ToolCallingAgent` leverages built-in tool calling methods. We also have multi-agent hierarchies, import from tool collections, remote code execution, vision models...

By the way, why use a framework at all? Well, because a big part of this stuff is non-trivial. For instance, the code agent has to keep a consistent format for code throughout its system prompt, its parser, the execution. So our framework handles this complexity for you. But of course we still encourage you to hack into the source code and use only the bits that you need, to the exclusion of everything else!

## How strong are open models for agentic workflows?

We&#039;ve created [`CodeAgent`](https://huggingface.co/docs/smolagents/reference/agents#smolagents.CodeAgent) instances with some leading models, and compared them on [this benchmark](https://huggingface.co/datasets/m-ric/agents_medium_benchmark_2) that gathers questions from a few different benchmarks to propose a varied blend of challenges.

[Find the benchmarking code here](https://github.com/huggingface/smolagents/blob/main/examples/smolagents_benchmark/run.py) for more detail on the agentic setup used, and see a comparison of using LLMs code agents compared to vanilla (spoilers: code agents works better).

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/smolagents/benchmark_code_agents.jpeg&quot; alt=&quot;benchmark of different models on agentic workflows. Open model DeepSeek-R1 beats closed-source models.&quot; width=60% max-width=500px&gt;
&lt;/p&gt;

This comparison shows that open-source models can now take on the best closed models!

## Security

Security is a critical consideration when working with code-executing agents. Our library provides:
- Sandboxed execution options using [E2B](https://e2b.dev/) or Docker
- Best practices for running agent code securely

For security policies, vulnerability reporting, and more information on secure agent execution, please see our [Security Policy](SECURITY.md).

## Contribute

Everyone is welcome to contribute, get started with our [contribution guide](https://github.com/huggingface/smolagents/blob/main/CONTRIBUTING.md).

## Cite smolagents

If you use `smolagents` in your publication, please cite it by using the following BibTeX entry.

```bibtex
@Misc{smolagents,
  title =        {`smolagents`: a smol library to build great agentic systems.},
  author =       {Aymeric Roucher and Albert Villanova del Moral and Thomas Wolf and Leandro von Werra and Erik KaunismÃ¤ki},
  howpublished = {\url{https://github.com/huggingface/smolagents}},
  year =         {2025}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>