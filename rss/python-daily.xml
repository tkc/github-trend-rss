<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sun, 08 Jun 2025 00:04:58 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[topoteretes/cognee]]></title>
            <link>https://github.com/topoteretes/cognee</link>
            <guid>https://github.com/topoteretes/cognee</guid>
            <pubDate>Sun, 08 Jun 2025 00:04:58 GMT</pubDate>
            <description><![CDATA[Memory for AI Agents in 5 lines of code]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/topoteretes/cognee">topoteretes/cognee</a></h1>
            <p>Memory for AI Agents in 5 lines of code</p>
            <p>Language: Python</p>
            <p>Stars: 4,063</p>
            <p>Forks: 330</p>
            <p>Stars today: 440 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/topoteretes/cognee&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/dev/assets/cognee-logo-transparent.png&quot; alt=&quot;Cognee Logo&quot; height=&quot;60&quot;&gt;
  &lt;/a&gt;

  &lt;br /&gt;

  cognee - Memory for AI Agents in 5 lines of code

  &lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=1bezuvLwJmw&amp;t=2s&quot;&gt;Demo&lt;/a&gt;
  .
  &lt;a href=&quot;https://cognee.ai&quot;&gt;Learn more&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://discord.gg/NQPKmU5CCg&quot;&gt;Join Discord&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://www.reddit.com/r/AIMemory/&quot;&gt;Join r/AIMemory&lt;/a&gt;
  &lt;/p&gt;


  [![GitHub forks](https://img.shields.io/github/forks/topoteretes/cognee.svg?style=social&amp;label=Fork&amp;maxAge=2592000)](https://GitHub.com/topoteretes/cognee/network/)
  [![GitHub stars](https://img.shields.io/github/stars/topoteretes/cognee.svg?style=social&amp;label=Star&amp;maxAge=2592000)](https://GitHub.com/topoteretes/cognee/stargazers/)
  [![GitHub commits](https://badgen.net/github/commits/topoteretes/cognee)](https://GitHub.com/topoteretes/cognee/commit/)
  [![Github tag](https://badgen.net/github/tag/topoteretes/cognee)](https://github.com/topoteretes/cognee/tags/)
  [![Downloads](https://static.pepy.tech/badge/cognee)](https://pepy.tech/project/cognee)
  [![License](https://img.shields.io/github/license/topoteretes/cognee?colorA=00C586&amp;colorB=000000)](https://github.com/topoteretes/cognee/blob/main/LICENSE)
  [![Contributors](https://img.shields.io/github/contributors/topoteretes/cognee?colorA=00C586&amp;colorB=000000)](https://github.com/topoteretes/cognee/graphs/contributors)

&lt;a href=&quot;https://www.producthunt.com/posts/cognee?embed=true&amp;utm_source=badge-top-post-badge&amp;utm_medium=badge&amp;utm_souce=badge-cognee&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=946346&amp;theme=light&amp;period=daily&amp;t=1744472480704&quot; alt=&quot;cognee - Memory&amp;#0032;for&amp;#0032;AI&amp;#0032;Agents&amp;#0032;&amp;#0032;in&amp;#0032;5&amp;#0032;lines&amp;#0032;of&amp;#0032;code | Product Hunt&quot; style=&quot;width: 250px; height: 54px;&quot; width=&quot;250&quot; height=&quot;54&quot; /&gt;&lt;/a&gt;

&lt;a href=&quot;https://trendshift.io/repositories/13955&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13955&quot; alt=&quot;topoteretes%2Fcognee | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;


Build dynamic memory for Agents and replace RAG using scalable, modular ECL (Extract, Cognify, Load) pipelines.

More on [use-cases](https://docs.cognee.ai/use-cases) and [evals](https://github.com/topoteretes/cognee/tree/main/evals)

  &lt;p align=&quot;center&quot;&gt;
  üåê Available Languages
  :
  &lt;a href=&quot;assets/community/README.pt.md&quot;&gt;üáµüáπ Portugu√™s&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;assets/community/README.zh.md&quot;&gt;üá®üá≥ [‰∏≠Êñá]&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;assets/community/README.ru.md&quot;&gt;üá∑üá∫ –†—É—Å—Å–∫–∏–π&lt;/a&gt;
  &lt;/p&gt;


&lt;div style=&quot;text-align: center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_benefits.png&quot; alt=&quot;Why cognee?&quot; width=&quot;50%&quot; /&gt;
&lt;/div&gt;
&lt;/div&gt;


## Features

- Interconnect and retrieve your past conversations, documents, images and audio transcriptions
- Replaces RAG systems and reduces developer effort, and cost.
- Load data to graph and vector databases using only Pydantic
- Manipulate your data while ingesting from 30+ data sources

## Get Started

Get started quickly with a Google Colab  &lt;a href=&quot;https://colab.research.google.com/drive/1jHbWVypDgCLwjE71GSXhRL3YxYhCZzG1?usp=sharing&quot;&gt;notebook&lt;/a&gt; , &lt;a href=&quot;https://deepnote.com/workspace/cognee-382213d0-0444-4c89-8265-13770e333c02/project/cognee-demo-78ffacb9-5832-4611-bb1a-560386068b30/notebook/Notebook-1-75b24cda566d4c24ab348f7150792601?utm_source=share-modal&amp;utm_medium=product-shared-content&amp;utm_campaign=notebook&amp;utm_content=78ffacb9-5832-4611-bb1a-560386068b30&quot;&gt;Deepnote notebook&lt;/a&gt; or  &lt;a href=&quot;https://github.com/topoteretes/cognee-starter&quot;&gt;starter repo&lt;/a&gt;


## Contributing
Your contributions are at the core of making this a true open source project. Any contributions you make are **greatly appreciated**. See [`CONTRIBUTING.md`](CONTRIBUTING.md) for more information.





## üì¶ Installation

You can install Cognee using either **pip**, **poetry**, **uv** or any other python package manager.
Cognee supports Python 3.8 to 3.12

### With pip

```bash
pip install cognee
```

## Local Cognee installation

You can install the local Cognee repo using **pip**, **poetry** and **uv**.
For local pip installation please make sure your pip version is above version 21.3.

### with UV with all optional dependencies

```bash
uv sync --all-extras
```

## üíª Basic Usage

### Setup

```
import os
os.environ[&quot;LLM_API_KEY&quot;] = &quot;YOUR OPENAI_API_KEY&quot;

```

You can also set the variables by creating .env file, using our &lt;a href=&quot;https://github.com/topoteretes/cognee/blob/main/.env.template&quot;&gt;template.&lt;/a&gt;
To use different LLM providers, for more info check out our &lt;a href=&quot;https://docs.cognee.ai&quot;&gt;documentation&lt;/a&gt;


### Simple example

This script will run the default pipeline:

```python
import cognee
import asyncio


async def main():
    # Add text to cognee
    await cognee.add(&quot;Natural language processing (NLP) is an interdisciplinary subfield of computer science and information retrieval.&quot;)

    # Generate the knowledge graph
    await cognee.cognify()

    # Query the knowledge graph
    results = await cognee.search(&quot;Tell me about NLP&quot;)

    # Display the results
    for result in results:
        print(result)


if __name__ == &#039;__main__&#039;:
    asyncio.run(main())

```
Example output:
```
  Natural Language Processing (NLP) is a cross-disciplinary and interdisciplinary field that involves computer science and information retrieval. It focuses on the interaction between computers and human language, enabling machines to understand and process natural language.

```

## Our paper is out! &lt;a href=&quot;https://arxiv.org/abs/2505.24478&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Read here&lt;/a&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
  &lt;img src=&quot;assets/cognee-paper.png&quot; alt=&quot;cognee paper&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

## Cognee UI

You can also cognify your files and query using cognee UI.

&lt;img src=&quot;assets/cognee-ui-2.webp&quot; width=&quot;100%&quot; alt=&quot;Cognee UI 2&quot;&gt;&lt;/a&gt;

Try cognee UI out locally [here](https://docs.cognee.ai/how-to-guides/cognee-ui).

## Understand our architecture

&lt;div style=&quot;text-align: center&quot;&gt;
  &lt;img src=&quot;assets/cognee_diagram.png&quot; alt=&quot;cognee concept diagram&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;



## Demos

1. What is AI memory:

[Learn about cognee](https://github.com/user-attachments/assets/8b2a0050-5ec4-424c-b417-8269971503f0)

2. Simple GraphRAG demo

[Simple GraphRAG demo](https://github.com/user-attachments/assets/d80b0776-4eb9-4b8e-aa22-3691e2d44b8f)

3. cognee with Ollama

[cognee with local models](https://github.com/user-attachments/assets/8621d3e8-ecb8-4860-afb2-5594f2ee17db)


## Code of Conduct

We are committed to making open source an enjoyable and respectful experience for our community. See &lt;a href=&quot;https://github.com/topoteretes/cognee/blob/main/CODE_OF_CONDUCT.md&quot;&gt;&lt;code&gt;CODE_OF_CONDUCT&lt;/code&gt;&lt;/a&gt; for more information.

## üí´ Contributors

&lt;a href=&quot;https://github.com/topoteretes/cognee/graphs/contributors&quot;&gt;
  &lt;img alt=&quot;contributors&quot; src=&quot;https://contrib.rocks/image?repo=topoteretes/cognee&quot;/&gt;
&lt;/a&gt;


## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=topoteretes/cognee&amp;type=Date)](https://star-history.com/#topoteretes/cognee&amp;Date)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[scrapy/scrapy]]></title>
            <link>https://github.com/scrapy/scrapy</link>
            <guid>https://github.com/scrapy/scrapy</guid>
            <pubDate>Sun, 08 Jun 2025 00:04:57 GMT</pubDate>
            <description><![CDATA[Scrapy, a fast high-level web crawling & scraping framework for Python.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/scrapy/scrapy">scrapy/scrapy</a></h1>
            <p>Scrapy, a fast high-level web crawling & scraping framework for Python.</p>
            <p>Language: Python</p>
            <p>Stars: 56,475</p>
            <p>Forks: 10,895</p>
            <p>Stars today: 54 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jwohlwend/boltz]]></title>
            <link>https://github.com/jwohlwend/boltz</link>
            <guid>https://github.com/jwohlwend/boltz</guid>
            <pubDate>Sun, 08 Jun 2025 00:04:56 GMT</pubDate>
            <description><![CDATA[Official repository for the Boltz biomolecular interaction models]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jwohlwend/boltz">jwohlwend/boltz</a></h1>
            <p>Official repository for the Boltz biomolecular interaction models</p>
            <p>Language: Python</p>
            <p>Stars: 2,023</p>
            <p>Forks: 326</p>
            <p>Stars today: 90 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;div&gt;&amp;nbsp;&lt;/div&gt;
  &lt;img src=&quot;docs/boltz2_title.png&quot; width=&quot;300&quot;/&gt;
  &lt;img src=&quot;https://model-gateway.boltz.bio/a.png?x-pxid=bce1627f-f326-4bff-8a97-45c6c3bc929d&quot; /&gt;

[Boltz-1](https://doi.org/10.1101/2024.11.19.624167) | [Boltz-2](https://bit.ly/boltz2-pdf) |
[Slack](https://boltz-community.slack.com/join/shared_invite/zt-37b5dxiuo-80rPSDp6lXjD1GTC4bxNIw#/shared-invite/email) &lt;br&gt; &lt;br&gt;
&lt;/div&gt;



![](docs/boltz1_pred_figure.png)


## Introduction

Boltz is a family of models for biomolecular interaction prediction. Boltz-1 was the first fully open source model to approach AlphaFold3 accuracy. Our latest work Boltz-2 is a new biomolecular foundation model that goes beyond AlphaFold3 and Boltz-1 by jointly modeling complex structures and binding affinities, a critical component towards accurate molecular design. Boltz-2 is the first deep learning model to approach the accuracy of physics-based free-energy perturbation (FEP) methods, while running 1000x faster ‚Äî making accurate in silico screening practical for early-stage drug discovery.

All the code and weights are provided under MIT license, making them freely available for both academic and commercial uses. For more information about the model, see the [Boltz-1](https://doi.org/10.1101/2024.11.19.624167) and [Boltz-2](https://bit.ly/boltz2-pdf) technical reports. To discuss updates, tools and applications join our [Slack channel](https://join.slack.com/t/boltz-community/shared_invite/zt-34qg8uink-V1LGdRRUf3avAUVaRvv93w).

## Installation

&gt; Note: we recommend installing boltz in a fresh python environment

Install boltz with PyPI (recommended):

```
pip install boltz -U
```

or directly from GitHub for daily updates:

```
git clone https://github.com/jwohlwend/boltz.git
cd boltz; pip install -e .
```

## Inference

You can run inference using Boltz with:

```
boltz predict input_path --use_msa_server
```

`input_path` should point to a YAML file, or a directory of YAML files for batched processing, describing the biomolecules you want to model and the properties you want to predict (e.g. affinity). To see all available options: `boltz predict --help` and for more information on these input formats, see our [prediction instructions](docs/prediction.md). By default, the `boltz` command will run the latest version of the model.

## Evaluation

‚ö†Ô∏è **Coming soon: updated evaluation code for Boltz-2!**

To encourage reproducibility and facilitate comparison with other models, on top of the existing Boltz-1 evaluation pipeline, we will soon provide the evaluation scripts and structural predictions for Boltz-2, Boltz-1, Chai-1 and AlphaFold3 on our test benchmark dataset, and our affinity predictions on the FEP+ benchamark, CASP16 and our MF-PCBA test set.

![Affinity test sets evaluations](docs/pearson_plot.png)
![Test set evaluations](docs/plot_test_boltz2.png)


## Training

‚ö†Ô∏è **Coming soon: updated training code for Boltz-2!**

If you&#039;re interested in retraining the model, currently for Boltz-1 but soon for Boltz-2, see our [training instructions](docs/training.md).


## Contributing

We welcome external contributions and are eager to engage with the community. Connect with us on our [Slack channel](https://join.slack.com/t/boltz-community/shared_invite/zt-34qg8uink-V1LGdRRUf3avAUVaRvv93w) to discuss advancements, share insights, and foster collaboration around Boltz-2.

Boltz also runs on Tenstorrent hardware thanks to a [fork](https://github.com/moritztng/tt-boltz) by Moritz Th√ºning.

## License

Our model and code are released under MIT License, and can be freely used for both academic and commercial purposes.


## Cite

If you use this code or the models in your research, please cite the following papers:

```bibtex
@article{passaro2025boltz2,
  author = {Passaro, Saro and Corso, Gabriele and Wohlwend, Jeremy and Reveiz, Mateo and Thaler, Stephan and Somnath, Vignesh Ram and Getz, Noah and Portnoi, Tally and Roy, Julien and Stark, Hannes and Kwabi-Addo, David and Beaini, Dominique and Jaakkola, Tommi and Barzilay, Regina},
  title = {Boltz-2: Towards Accurate and Efficient Binding Affinity Prediction},
  year = {2025},
  doi = {},
  journal = {}
}

@article{wohlwend2024boltz1,
  author = {Wohlwend, Jeremy and Corso, Gabriele and Passaro, Saro and Getz, Noah and Reveiz, Mateo and Leidal, Ken and Swiderski, Wojtek and Atkinson, Liam and Portnoi, Tally and Chinn, Itamar and Silterra, Jacob and Jaakkola, Tommi and Barzilay, Regina},
  title = {Boltz-1: Democratizing Biomolecular Interaction Modeling},
  year = {2024},
  doi = {10.1101/2024.11.19.624167},
  journal = {bioRxiv}
}
```

In addition if you use the automatic MSA generation, please cite:

```bibtex
@article{mirdita2022colabfold,
  title={ColabFold: making protein folding accessible to all},
  author={Mirdita, Milot and Sch{\&quot;u}tze, Konstantin and Moriwaki, Yoshitaka and Heo, Lim and Ovchinnikov, Sergey and Steinegger, Martin},
  journal={Nature methods},
  year={2022},
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[deepsense-ai/ragbits]]></title>
            <link>https://github.com/deepsense-ai/ragbits</link>
            <guid>https://github.com/deepsense-ai/ragbits</guid>
            <pubDate>Sun, 08 Jun 2025 00:04:55 GMT</pubDate>
            <description><![CDATA[Building blocks for rapid development of GenAI applications]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/deepsense-ai/ragbits">deepsense-ai/ragbits</a></h1>
            <p>Building blocks for rapid development of GenAI applications</p>
            <p>Language: Python</p>
            <p>Stars: 1,023</p>
            <p>Forks: 78</p>
            <p>Stars today: 239 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;h1&gt;üê∞ Ragbits&lt;/h1&gt;

*Building blocks for rapid development of GenAI applications*

[Homepage](https://deepsense.ai/rd-hub/ragbits/) | [Documentation](https://ragbits.deepsense.ai) | [Contact](https://deepsense.ai/contact/)

[![PyPI - License](https://img.shields.io/pypi/l/ragbits)](https://pypi.org/project/ragbits)
[![PyPI - Version](https://img.shields.io/pypi/v/ragbits)](https://pypi.org/project/ragbits)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ragbits)](https://pypi.org/project/ragbits)

&lt;/div&gt;

---

## Features

### üî® Build Reliable &amp; Scalable GenAI Apps

- **Swap LLMs anytime** ‚Äì Switch between [100+ LLMs via LiteLLM](https://ragbits.deepsense.ai/how-to/llms/use_llms/) or run [local models](https://ragbits.deepsense.ai/how-to/llms/use_local_llms/).
- **Type-safe LLM calls** ‚Äì Use Python generics to [enforce strict type safety](https://ragbits.deepsense.ai/how-to/prompts/use_prompting/#how-to-configure-prompts-output-data-type) in model interactions.
- **Bring your own vector store** ‚Äì Connect to [Qdrant](https://ragbits.deepsense.ai/api_reference/core/vector-stores/#ragbits.core.vector_stores.qdrant.QdrantVectorStore), [PgVector](https://ragbits.deepsense.ai/api_reference/core/vector-stores/#ragbits.core.vector_stores.pgvector.PgVectorStore), and more with built-in support.
- **Developer tools included** ‚Äì [Manage vector stores](https://ragbits.deepsense.ai/cli/main/#ragbits-vector-store), query pipelines, and [test prompts from your terminal](https://ragbits.deepsense.ai/quickstart/quickstart1_prompts/#testing-the-prompt-from-the-cli).
- **Modular installation** ‚Äì Install only what you need, reducing dependencies and improving performance.

### üìö Fast &amp; Flexible RAG Processing

- **Ingest 20+ formats** ‚Äì Process PDFs, HTML, spreadsheets, presentations, and more. Process data using [Docling](https://github.com/docling-project/docling), [Unstructured](https://github.com/Unstructured-IO/unstructured) or create a custom parser.
- **Handle complex data** ‚Äì Extract tables, images, and structured content with built-in VLMs support.
- **Connect to any data source** ‚Äì Use prebuilt connectors for S3, GCS, Azure, or implement your own.
- **Scale ingestion** ‚Äì Process large datasets quickly with [Ray-based parallel processing](https://ragbits.deepsense.ai/how-to/document_search/distributed_ingestion/#how-to-ingest-documents-in-a-distributed-fashion).

### üöÄ Deploy &amp; Monitor with Confidence

- **Real-time observability** ‚Äì Track performance with [OpenTelemetry](https://ragbits.deepsense.ai/how-to/project/use_tracing/#opentelemetry-trace-handler) and [CLI insights](https://ragbits.deepsense.ai/how-to/project/use_tracing/#cli-trace-handler).
- **Built-in testing** ‚Äì Validate prompts [with promptfoo](https://ragbits.deepsense.ai/how-to/prompts/promptfoo/) before deployment.
- **Auto-optimization** ‚Äì Continuously evaluate and refine model performance.
- **Chat UI** ‚Äì Deploy [chatbot interface](https://ragbits.deepsense.ai/how-to/chatbots/api/) with API, persistance and user feedback.

## Installation

To get started quickly, you can install with:

```sh
pip install ragbits
```

This is a starter bundle of packages, containing:

- [`ragbits-core`](https://github.com/deepsense-ai/ragbits/tree/main/packages/ragbits-core) - fundamental tools for working with prompts, LLMs and vector databases.
- [`ragbits-agents`](https://github.com/deepsense-ai/ragbits/tree/main/packages/ragbits-agents) - abstractions for building agentic systems.
- [`ragbits-document-search`](https://github.com/deepsense-ai/ragbits/tree/main/packages/ragbits-document-search) - retrieval and ingestion piplines for knowledge bases.
- [`ragbits-evaluate`](https://github.com/deepsense-ai/ragbits/tree/main/packages/ragbits-evaluate) - unified evaluation framework for Ragbits components.
- [`ragbits-chat`](https://github.com/deepsense-ai/ragbits/tree/main/packages/ragbits-chat) - full-stack infrastructure for building conversational AI applications.
- [`ragbits-cli`](https://github.com/deepsense-ai/ragbits/tree/main/packages/ragbits-cli) - `ragbits` shell command for interacting with Ragbits components.

Alternatively, you can use individual components of the stack by installing their respective packages.

## Quickstart

### Basics

To define a prompt and run LLM:

```python
import asyncio
from pydantic import BaseModel
from ragbits.core.llms import LiteLLM
from ragbits.core.prompt import Prompt

class QuestionAnswerPromptInput(BaseModel):
    question: str

class QuestionAnswerPromptOutput(BaseModel):
    answer: str

class QuestionAnswerPrompt(Prompt[QuestionAnswerPromptInput, QuestionAnswerPromptOutput]):
    system_prompt = &quot;&quot;&quot;
    You are a question answering agent. Answer the question to the best of your ability.
    &quot;&quot;&quot;
    user_prompt = &quot;&quot;&quot;
    Question: {{ question }}
    &quot;&quot;&quot;

llm = LiteLLM(model_name=&quot;gpt-4.1-nano&quot;, use_structured_output=True)

async def main() -&gt; None:
    prompt = QuestionAnswerPrompt(QuestionAnswerPromptInput(question=&quot;What are high memory and low memory on linux?&quot;))
    response = await llm.generate(prompt)
    print(response.answer)

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

### Document Search

To build and query a simple vector store index:

```python
import asyncio
from ragbits.core.embeddings import LiteLLMEmbedder
from ragbits.core.vector_stores import InMemoryVectorStore
from ragbits.document_search import DocumentSearch

embedder = LiteLLMEmbedder(model_name=&quot;text-embedding-3-small&quot;)
vector_store = InMemoryVectorStore(embedder=embedder)
document_search = DocumentSearch(vector_store=vector_store)

async def run() -&gt; None:
    await document_search.ingest(&quot;web://https://arxiv.org/pdf/1706.03762&quot;)
    result = await document_search.search(&quot;What are the key findings presented in this paper?&quot;)
    print(result)

if __name__ == &quot;__main__&quot;:
    asyncio.run(run())
```

### Retrieval-Augmented Generation

To build a simple RAG pipeline:

```python
import asyncio
from pydantic import BaseModel
from ragbits.core.embeddings import LiteLLMEmbedder
from ragbits.core.llms import LiteLLM
from ragbits.core.prompt import Prompt
from ragbits.core.vector_stores import InMemoryVectorStore
from ragbits.document_search import DocumentSearch

class QuestionAnswerPromptInput(BaseModel):
    question: str
    context: list[str]

class QuestionAnswerPromptOutput(BaseModel):
    answer: str

class QuestionAnswerPrompt(Prompt[QuestionAnswerPromptInput, QuestionAnswerPromptOutput]):
    system_prompt = &quot;&quot;&quot;
    You are a question answering agent. Answer the question that will be provided using context.
    If in the given context there is not enough information refuse to answer.
    &quot;&quot;&quot;
    user_prompt = &quot;&quot;&quot;
    Question: {{ question }}
    Context: {% for item in context %}
        {{ item }}
    {%- endfor %}
    &quot;&quot;&quot;

embedder = LiteLLMEmbedder(model_name=&quot;text-embedding-3-small&quot;)
vector_store = InMemoryVectorStore(embedder=embedder)
document_search = DocumentSearch(vector_store=vector_store)
llm = LiteLLM(model_name=&quot;gpt-4.1-nano&quot;, use_structured_output=True)

async def run() -&gt; None:
    question = &quot;What are the key findings presented in this paper?&quot;

    await document_search.ingest(&quot;web://https://arxiv.org/pdf/1706.03762&quot;)
    result = await document_search.search(question)

    prompt = QuestionAnswerPrompt(QuestionAnswerPromptInput(
        question=question,
        context=[element.text_representation for element in result],
    ))
    response = await llm.generate(prompt)
    print(response.answer)

if __name__ == &quot;__main__&quot;:
    asyncio.run(run())
```

### Chatbot interface with UI

To expose your RAG application through Ragbits UI:

```python
from collections.abc import AsyncGenerator

from pydantic import BaseModel

from ragbits.chat.api import RagbitsAPI
from ragbits.chat.interface import ChatInterface
from ragbits.chat.interface.types import ChatContext, ChatResponse
from ragbits.core.embeddings import LiteLLMEmbedder
from ragbits.core.llms import LiteLLM
from ragbits.core.prompt import Prompt
from ragbits.core.prompt.base import ChatFormat
from ragbits.core.vector_stores import InMemoryVectorStore
from ragbits.document_search import DocumentSearch


class QuestionAnswerPromptInput(BaseModel):
    question: str
    context: list[str]


class QuestionAnswerPrompt(Prompt[QuestionAnswerPromptInput, str]):
    system_prompt = &quot;&quot;&quot;
    You are a question answering agent. Answer the question that will be provided using context.
    If in the given context there is not enough information refuse to answer.
    &quot;&quot;&quot;
    user_prompt = &quot;&quot;&quot;
    Question: {{ question }}
    Context: {% for item in context %}{{ item }}{%- endfor %}
    &quot;&quot;&quot;


class MyChat(ChatInterface):
    &quot;&quot;&quot;Chat interface for fullapp application.&quot;&quot;&quot;

    async def setup(self) -&gt; None:
        self.embedder = LiteLLMEmbedder(model_name=&quot;text-embedding-3-small&quot;)
        self.vector_store = InMemoryVectorStore(embedder=self.embedder)
        self.document_search = DocumentSearch(vector_store=self.vector_store)
        self.llm = LiteLLM(model_name=&quot;gpt-4.1-nano&quot;, use_structured_output=True)

        await self.document_search.ingest(&quot;web://https://arxiv.org/pdf/1706.03762&quot;)

    async def chat(
        self,
        message: str,
        history: ChatFormat | None = None,
        context: ChatContext | None = None,
    ) -&gt; AsyncGenerator[ChatResponse, None]:
        # Search for relevant documents
        result = await self.document_search.search(message)

        prompt = QuestionAnswerPrompt(
            QuestionAnswerPromptInput(
                question=message,
                context=[element.text_representation for element in result],
            )
        )

        # Stream the response from the LLM
        async for chunk in self.llm.generate_streaming(prompt):
            yield self.create_text_response(chunk)


if __name__ == &quot;__main__&quot;:
    RagbitsAPI(MyChat).run()
```

## Rapid development

Create Ragbits projects from templates:

```sh
uvx create-ragbits-app
```

Explore `create-ragbits-app` repo [here](https://github.com/deepsense-ai/create-ragbits-app). If you have a new idea for a template, feel free to contribute!

## Documentation

- [Quickstart](https://ragbits.deepsense.ai/quickstart/quickstart1_prompts/) - Get started with Ragbits in a few minutes
- [How-to](https://ragbits.deepsense.ai/how-to/prompts/use_prompting/) - Learn how to use Ragbits in your projects
- [CLI](https://ragbits.deepsense.ai/cli/main/) - Learn how to run Ragbits in your terminal
- [API reference](https://ragbits.deepsense.ai/api_reference/core/prompt/) - Explore the underlying Ragbits API

## Contributing

We welcome contributions! Please read [CONTRIBUTING.md](https://github.com/deepsense-ai/ragbits/tree/main/CONTRIBUTING.md) for more information.

## License

Ragbits is licensed under the [MIT License](https://github.com/deepsense-ai/ragbits/tree/main/LICENSE).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[stanfordnlp/dspy]]></title>
            <link>https://github.com/stanfordnlp/dspy</link>
            <guid>https://github.com/stanfordnlp/dspy</guid>
            <pubDate>Sun, 08 Jun 2025 00:04:54 GMT</pubDate>
            <description><![CDATA[DSPy: The framework for programming‚Äînot prompting‚Äîlanguage models]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/stanfordnlp/dspy">stanfordnlp/dspy</a></h1>
            <p>DSPy: The framework for programming‚Äînot prompting‚Äîlanguage models</p>
            <p>Language: Python</p>
            <p>Stars: 24,991</p>
            <p>Forks: 1,939</p>
            <p>Stars today: 154 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img align=&quot;center&quot; src=&quot;docs/docs/static/img/dspy_logo.png&quot; width=&quot;460px&quot; /&gt;
&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;


## DSPy: _Programming_‚Äînot prompting‚ÄîFoundation Models

**Documentation:** [DSPy Docs](https://dspy.ai/)

[![PyPI Downloads](https://static.pepy.tech/badge/dspy/month)](https://pepy.tech/projects/dspy)


----

DSPy is the framework for _programming‚Äîrather than prompting‚Äîlanguage models_. It allows you to iterate fast on **building modular AI systems** and offers algorithms for **optimizing their prompts and weights**, whether you&#039;re building simple classifiers, sophisticated RAG pipelines, or Agent loops.

DSPy stands for Declarative Self-improving Python. Instead of brittle prompts, you write compositional _Python code_ and use DSPy to **teach your LM to deliver high-quality outputs**. Learn more via our [official documentation site](https://dspy.ai/) or meet the community, seek help, or start contributing via this GitHub repo and our [Discord server](https://discord.gg/XCGy2WDCQB).


## Documentation: [dspy.ai](https://dspy.ai)


**Please go to the [DSPy Docs at dspy.ai](https://dspy.ai)**


## Installation


```bash
pip install dspy
```

To install the very latest from `main`:

```bash
pip install git+https://github.com/stanfordnlp/dspy.git
````




## üìú Citation &amp; Reading More

If you&#039;re looking to understand the framework, please go to the [DSPy Docs at dspy.ai](https://dspy.ai).

If you&#039;re looking to understand the underlying research, this is a set of our papers:

**[Jun&#039;24] [Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs](https://arxiv.org/abs/2406.11695)**       
**[Oct&#039;23] [DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines](https://arxiv.org/abs/2310.03714)**     
[Jul&#039;24] [Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together](https://arxiv.org/abs/2407.10930)     
[Jun&#039;24] [Prompts as Auto-Optimized Training Hyperparameters](https://arxiv.org/abs/2406.11706)    
[Feb&#039;24] [Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models](https://arxiv.org/abs/2402.14207)         
[Jan&#039;24] [In-Context Learning for Extreme Multi-Label Classification](https://arxiv.org/abs/2401.12178)       
[Dec&#039;23] [DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines](https://arxiv.org/abs/2312.13382)   
[Dec&#039;22] [Demonstrate-Search-Predict: Composing Retrieval &amp; Language Models for Knowledge-Intensive NLP](https://arxiv.org/abs/2212.14024.pdf)

To stay up to date or learn more, follow [@lateinteraction](https://twitter.com/lateinteraction) on Twitter.

The **DSPy** logo is designed by **Chuyi Zhang**.

If you use DSPy or DSP in a research paper, please cite our work as follows:

```
@inproceedings{khattab2024dspy,
  title={DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines},
  author={Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T. and Moazam, Hanna and Miller, Heather and Zaharia, Matei and Potts, Christopher},
  journal={The Twelfth International Conference on Learning Representations},
  year={2024}
}
@article{khattab2022demonstrate,
  title={Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive {NLP}},
  author={Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
  journal={arXiv preprint arXiv:2212.14024},
  year={2022}
}
```

&lt;!-- You can also read more about the evolution of the framework from Demonstrate-Search-Predict to DSPy:

* [**DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines**](https://arxiv.org/abs/2312.13382)   (Academic Paper, Dec 2023) 
* [**DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines**](https://arxiv.org/abs/2310.03714) (Academic Paper, Oct 2023) 
* [**Releasing DSPy, the latest iteration of the framework**](https://twitter.com/lateinteraction/status/1694748401374490946) (Twitter Thread, Aug 2023)
* [**Releasing the DSP Compiler (v0.1)**](https://twitter.com/lateinteraction/status/1625231662849073160)  (Twitter Thread, Feb 2023)
* [**Introducing DSP**](https://twitter.com/lateinteraction/status/1617953413576425472)  (Twitter Thread, Jan 2023)
* [**Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP**](https://arxiv.org/abs/2212.14024.pdf) (Academic Paper, Dec 2022) --&gt;

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[public-apis/public-apis]]></title>
            <link>https://github.com/public-apis/public-apis</link>
            <guid>https://github.com/public-apis/public-apis</guid>
            <pubDate>Sun, 08 Jun 2025 00:04:53 GMT</pubDate>
            <description><![CDATA[A collective list of free APIs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/public-apis/public-apis">public-apis/public-apis</a></h1>
            <p>A collective list of free APIs</p>
            <p>Language: Python</p>
            <p>Stars: 349,911</p>
            <p>Forks: 36,789</p>
            <p>Stars today: 237 stars today</p>
            <h2>README</h2><pre># Try Public APIs for free
The Public APIs repository is manually curated by community members like you and folks working at [APILayer](https://apilayer.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo). It includes an extensive list of public APIs from many domains that you can use for your own products. Consider it a treasure trove of APIs well-managed by the community over the years.

&lt;br &gt;

&lt;p&gt;
    &lt;a href=&quot;https://apilayer.com&quot;&gt;
        &lt;div&gt;
            &lt;img src=&quot;.github/cs1586-APILayerLogoUpdate2022-LJ_v2-HighRes.png&quot; width=&quot;100%&quot; alt=&quot;APILayer Logo&quot; /&gt;
        &lt;/div&gt;
    &lt;/a&gt;
  &lt;/p&gt;

APILayer is the fastest way to integrate APIs into any product. Explore [APILayer APIs](https://apilayer.com/products/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo) here for your next project.

Join our [Discord server](https://discord.com/invite/hgjA78638n/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo) to get updates, ask questions, get answers, random community calls, and more.

&lt;br &gt;

## APILayer APIs
| API | Description | Call this API |
|:---|:---|:---|
| [IPstack](https://ipstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Locate and Identify Website Visitors by IP Address | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-55145132-244c-448c-8e6f-8780866e4862?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-55145132-244c-448c-8e6f-8780866e4862%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Marketstack](https://marketstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Free, easy-to-use REST API interface delivering worldwide stock market data in JSON format | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Weatherstack](https://weatherstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Retrieve instant, accurate weather information for any location in the world in lightweight JSON format | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Numverify](https://numverify.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers ) | Global Phone Number Validation &amp; Lookup JSON API |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Fixer](https://fixer.io/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Fixer is a simple and lightweight API for current and historical foreign exchange (forex) rates. |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Aviationstack](https://avaitionstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Free, real-time flight status and global Aviation data API |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-72ee0d35-018e-4370-a2b6-a66d3ebd5b5a?action=collection/fork)|

&lt;br &gt;

## Learn more about Public APIs

&lt;strong&gt;Get Involved&lt;/strong&gt;

* [Contributing Guide](CONTRIBUTING.md)
* [API for this project](https://github.com/davemachado/public-api)
* [Issues](https://github.com/public-apis/public-apis/issues)
* [Pull Requests](https://github.com/public-apis/public-apis/pulls)
* [LICENSE](LICENSE) 

&lt;br /&gt;

## Index

* [Animals](#animals)
* [Anime](#anime)
* [Anti-Malware](#anti-malware)
* [Art &amp; Design](#art--design)
* [Authentication &amp; Authorization](#authentication--authorization)
* [Blockchain](#blockchain)
* [Books](#books)
* [Business](#business)
* [Calendar](#calendar)
* [Cloud Storage &amp; File Sharing](#cloud-storage--file-sharing)
* [Continuous Integration](#continuous-integration)
* [Cryptocurrency](#cryptocurrency)
* [Currency Exchange](#currency-exchange)
* [Data Validation](#data-validation)
* [Development](#development)
* [Dictionaries](#dictionaries)
* [Documents &amp; Productivity](#documents--productivity)
* [Email](#email)
* [Entertainment](#entertainment)
* [Environment](#environment)
* [Events](#events)
* [Finance](#finance)
* [Food &amp; Drink](#food--drink)
* [Games &amp; Comics](#games--comics)
* [Geocoding](#geocoding)
* [Government](#government)
* [Health](#health)
* [Jobs](#jobs)
* [Machine Learning](#machine-learning)
* [Music](#music)
* [News](#news)
* [Open Data](#open-data)
* [Open Source Projects](#open-source-projects)
* [Patent](#patent)
* [Personality](#personality)
* [Phone](#phone)
* [Photography](#photography)
* [Programming](#programming)
* [Science &amp; Math](#science--math)
* [Security](#security)
* [Shopping](#shopping)
* [Social](#social)
* [Sports &amp; Fitness](#sports--fitness)
* [Test Data](#test-data)
* [Text Analysis](#text-analysis)
* [Tracking](#tracking)
* [Transportation](#transportation)
* [URL Shorteners](#url-shorteners)
* [Vehicle](#vehicle)
* [Video](#video)
* [Weather](#weather)
&lt;br &gt;

### Animals
API | Description | Auth | HTTPS | CORS 
|:---|:---|:---|:---|:---|
| [AdoptAPet](https://www.adoptapet.com/public/apis/pet_list.html) | Resource to help get pets adopted | `apiKey` | Yes | Yes |
| [Axolotl](https://theaxolotlapi.netlify.app/) | Collection of axolotl pictures and facts | No | Yes | No |
| [Cat Facts](https://alexwohlbruck.github.io/cat-facts/) | Daily cat facts | No | Yes | No | |
| [Cataas](https://cataas.com/) | Cat as a service (cats pictures and gifs) | No | Yes | No |
| [Cats](https://docs.thecatapi.com/) | Pictures of cats from Tumblr | `apiKey` | Yes | No |
| [Dog Facts](https://dukengn.github.io/Dog-facts-API/) | Random dog facts | No | Yes | Yes |
| [Dog Facts](https://kinduff.github.io/dog-api/) | Random facts of Dogs | No | Yes | Yes |
| [Dogs](https://dog.ceo/dog-api/) | Based on the Stanford Dogs Dataset | No | Yes | Yes |
| [eBird](https://documenter.getpostman.com/view/664302/S1ENwy59) | Retrieve recent or notable birding observations within a region | `apiKey` | Yes | No |
| [FishWatch](https://www.fishwatch.gov/developers) | Information and pictures about individual fish species | No | Yes | Yes |
| [HTTP Cat](https://http.cat/) | Cat for every HTTP Status | No | Yes | Yes |
| [HTTP Dog](https://http.dog/) | Dogs for every HTTP response status code | No | Yes | Yes |
| [IUCN](http://apiv3.iucnredlist.org/api/v3/docs) | IUCN Red List of Threatened Species | `apiKey` | No | No |
| [MeowFacts](https://github.com/wh-iterabb-it/meowfacts) | Get random cat facts | No | Yes | No |
| [Movebank](https://github.com/movebank/movebank-api-doc) | Movement and Migration data of animals | No | Yes | Yes |
| [Petfinder](https://www.petfinder.com/developers/) | Petfinder is dedicated to helping pets find homes, another resource to get pets adopted | `apiKey` | Yes | Yes |
| [PlaceBear](https://placebear.com/) | Placeholder bear pictures | No | Yes | Yes |
| [PlaceDog](https://place.dog) | Placeholder Dog pictures | No | Yes | Yes |
| [PlaceKitten](https://placekitten.com/) | Placeholder Kitten pictures | No | Yes | Yes |
| [RandomDog](https://random.dog/woof.json) | Random pictures of dogs | No | Yes | Yes |
| [RandomDuck](https://random-d.uk/api) | Random pictures of ducks | No | Yes | No |
| [RandomFox](https://randomfox.ca/floof/) | Random pictures of foxes | No | Yes | No |
| [RescueGroups](https://userguide.rescuegroups.org/display/APIDG/API+Developers+Guide+Home) | Adoption | No | Yes | Unknown |
| [Shibe.Online](http://shibe.online/) | Random pictures of Shiba Inu, cats or birds | No | Yes | Yes |
| [The Dog](https://thedogapi.com/) | A public service all about Dogs, free to use when making your fancy new App, Website or Service | `apiKey` | Yes | No |
| [xeno-canto](https://xeno-canto.org/explore/api) | Bird recordings | No | Yes | Unknown |
| [Zoo Animals](https://zoo-animal-api.herokuapp.com/) | Facts and pictures of zoo animals | No | Yes | Yes |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;

### Anime
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AniAPI](https://aniapi.com/docs/) | Anime discovery, streaming &amp; syncing with trackers | `OAuth` | Yes | Yes |
| [AniDB](https://wiki.anidb.net/HTTP_API_Definition) | Anime Database | `apiKey` | No | Unknown |
| [AniList](https://github.com/AniList/ApiV2-GraphQL-Docs) | Anime discovery &amp; tracking | `OAuth` | Yes | Unknown |
| [AnimeChan](https://github.com/RocktimSaikia/anime-chan) | Anime quotes (over 10k+) | No | Yes | No |
| [AnimeFacts](https://chandan-02.github.io/anime-facts-rest-api/) | Anime Facts (over 100+) | No | Yes | Yes |
| [AnimeNewsNetwork](https://www.animenewsnetwork.com/encyclopedia/api.php) | Anime industry news | No | Yes | Yes |
| [Catboy](https://catboys.com/api) | Neko images, funny GIFs &amp; more | No | Yes | Yes |
| [Danbooru Anime](https://danbooru.donmai.us/wiki_pages/help:api) | Thousands of anime artist database to find good anime art | `apiKey` | Yes | Yes |
| [Jikan](https://jikan.moe) | Unofficial MyAnimeList API | No | Yes | Yes |
| [Kitsu](https://kitsu.docs.apiary.io/) | Anime discovery platform | `OAuth` | Yes | Yes |
| [MangaDex](https://api.mangadex.org/docs.html) | Manga Database and Community | `apiKey` | Yes | Unknown |
| [Mangapi](https://rapidapi.com/pierre.carcellermeunier/api/mangapi3/) | Translate manga pages from one language to another | `apiKey` | Yes | Unknown |
| [MyAnimeList](https://myanimelist.net/clubs.php?cid=13727) | Anime and Manga Database and Community | `OAuth` | Yes | Unknown |
| [NekosBest](https://docs.nekos.best) | Neko Images &amp; Anime roleplaying GIFs | No | Yes | Yes |
| [Shikimori](https://shikimori.one/api/doc) | Anime discovery, tracking, forum, rates | `OAuth` | Yes | Unknown |
| [Studio Ghibli](https://ghibliapi.herokuapp.com) | Resources from Studio Ghibli films | No | Yes | Yes |
| [Trace Moe](https://soruly.github.io/trace.moe-api/#/) | A useful tool to get the exact scene of an anime from a screenshot | No | Yes | No |
| [Waifu.im](https://waifu.im/docs) | Get waifu pictures from an archive of over 4000 images and multiple tags | No | Yes | Yes |
| [Waifu.pics](https://waifu.pics/docs) | Image sharing platform for anime images | No | Yes | No |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;

### Anti-Malware
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AbuseIPDB](https://docs.abuseipdb.com/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [AlienVault Open Threat Exchange (OTX)](https://otx.alienvault.com/api) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [CAPEsandbox](https://capev2.readthedocs.io/en/latest/usage/api.html) | Malware execution and analysis | `apiKey` | Yes | Unknown |
| [Google Safe Browsing](https://developers.google.com/safe-browsing/) | Google Link/Domain Flagging | `apiKey` | Yes | Unknown |
| [MalDatabase](https://maldatabase.com/api-doc.html) | Provide malware datasets and threat intelligence feeds | `apiKey` | Yes | Unknown |
| [MalShare](https://malshare.com/doc.php) | Malware Archive / file sourcing | `apiKey` | Yes | No |
| [MalwareBazaar](https://bazaar.abuse.ch/api/) | Collect and share malware samples | `apiKey` | Yes | Unknown |
| [Metacert](https://metacert.com/) | Metacert Link Flagging | `apiKey` | Yes | Unknown |
| [NoPhishy](https://rapidapi.com/Amiichu/api/exerra-phishing-check/) | Check links to see if they&#039;re known phishing attempts | `apiKey` | Yes | Yes |
| [Phisherman](https://phisherman.gg/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [Scanii](https://docs.scanii.com/) | Simple REST API that can scan submitted documents/files for the presence of threats | `apiKey` | Yes | Yes |
| [URLhaus](https://urlhaus-api.abuse.ch/) | Bulk queries and Download Malware Samples | No | Yes | Yes |
| [URLScan.io](https://urlscan.io/about-api/) | Scan and Analyse URLs | `apiKey` | Yes | Unknown |
| [VirusTotal](https://www.virustotal.com/en/documentation/public-api/) | VirusTotal File/URL Analysis | `apiKey` | Yes | Unknown |
| [Web of Trust](https://support.mywot.com/hc/en-us/sections/360004477734-API-) | IP/domain/URL reputation | `apiKey` | Yes | Unknown | 

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Art &amp; Design
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [Am√©thyste](https://api.amethyste.moe/) | Generate images for Discord users | `apiKey` | Yes | Unknown |
| [Art Institute of Chicago](https://api.artic.edu/docs/) | Art | No | Yes | Yes |
| [Colormind](http://colormind.io/api-access/) | Color scheme generator | No | No | Unknown |
| [ColourLovers](http://www.colourlovers.com/api) | Get various patterns, palettes and images | No | No | Unknown |
| [Cooper Hewitt](https://collection.cooperhewitt.org/api) | Smithsonian Design Museum | `apiKey` | Yes | Unknown |
| [Dribbble](https://developer.dribbble.com) | Discover the world‚Äôs top designers &amp; creatives | `OAuth` | Yes | Unknown |
| [EmojiHub](https://github.com/cheatsnake/emojihub) | Get emojis by categories and groups | No | Yes | Yes |
| [Europeana](https://pro.europeana.eu/resources/apis/search) | European Museum and Galleries content | `apiKey` | Yes | Unknown |
| [Harvard Art Museums](https://github.com/harvardartmuseums/api-docs) | Art | `apiKey` | No | Unknown |
| [Icon Horse](https://icon.horse) | Favicons for any website, with fallbacks | No | Yes | Yes |
| [Iconfinder](https://developer.iconfinder.com) | Icons | `apiKey` | Yes | Unknown |
| [Icons8](https://img.icons8.com/) | Icons (find &quot;search icon&quot; hyperlink in page) | No | Yes | Unknown |
| [Lordicon](https://lordicon.com/) | Icons with predone Animations | No | Yes | Yes |
| [Metropolitan Museum of Art](https://metmuseum.github.io/) | Met Museum of Art | No | Yes | No |
| [Noun Project](http://api.thenounproject.com/index.html) | Icons | `OAuth` | No | Unknown |
| [PHP-Noise](https://php-noise.com/) | Noise Background Image Generator | No | Yes | Yes |
| [Pixel Encounter](https://pixelencounter.com/api) | SVG Icon Generator | No | Yes | No |
| [Rijksmuseum](https://data.rijksmuseum.nl/object-metadata/api/) | RijksMuseum Data | `apiKey` | Yes | Unknown |
| [Word Cloud](https://wordcloudapi.com/) | Easily create word clouds | `apiKey` | Yes | Unknown |
| [xColors](https://x-colors.herokuapp.com/) | Generate &amp; convert colors | No | Yes | Yes |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Authentication &amp; Authorization
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [Auth0](https://auth0.com) | Easy to implement, adaptable authentication and authorization platform | `apiKey` | Yes | Yes |
| [GetOTP](https://otp.dev/en/docs/) | Implement OTP flow quickly | `apiKey` | Yes | No |
| [Micro User Service](https://m3o.com/user) | User management and authentication | `apiKey` | Yes | No |
| [MojoAuth](https://mojoauth.com) | Secure and modern passwordless authentication platform | `apiKey` | Yes | Yes |
| [SAWO Labs](https://sawolabs.com) | Simplify login and improve user experience by integrating passwordless authentication in your app | `apiKey` | Yes | Yes |
| [Stytch](https://stytch.com/) | User infrastructure for modern applications | `apiKey` | Yes | No |
| [Warrant](https://warrant.dev/) | APIs for authorization and access control | `apiKey` | Yes | Yes |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Blockchain
| API | Description | Auth | HTTPS | CORS |
|---|:---|:---|:---|:---|
| [Bitquery](https://graphql.bitquery.io/ide) | Onchain GraphQL APIs &amp; DEX APIs | `apiKey` | Yes | Yes |
| [Chainlink](https://chain.link/developer-resources) | Build hybrid smart contracts with Chainlink | No | Yes | Unknown |
| [Chainpoint](https://tierion.com/chainpoint/) | Chainpoint is a global network for anchoring data to the Bitcoin blockchain | No | Yes | Unknown |
| [Covalent](https://www.covalenthq.com/docs/api/) | Multi-blockchain data aggregator platform | `apiKey` | Yes | Unknown |
| [Etherscan](https://etherscan.io/apis) | Ethereum explorer API | `apiKey` | Yes | Yes |
| [Helium](https://docs.helium.com/api/blockchain/introduction/) | Helium is a global, distributed network of Hotspots that create public, long-range wireless coverage | No | Yes | Unknown |
| [Nownodes](https://nownodes.io/) | Blockchain-as-a-service solution that provides high-quality connection via API | `apiKey` | Yes | Unknown |
| [Steem](https://developers.steem.io/) | Blockchain-based blogging and social media website | No | No | No |
| [The Graph](https://thegraph.com) | Indexing protocol for querying networks like Ethereum with GraphQL | `apiKey` | Yes | Unknown |
| [Walltime](https://walltime.info/api.html) | To retrieve Walltime&#039;s market info | No | Yes | Unknown |
| [Watchdata](https://docs.watchdata.io) | Provide simple and reliable API access to Ethereum blockchain | `apiKey` | Yes | Unknown |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Books
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [A B√≠blia Digital](https://www.abibliadigital.com.br/en) | Do not worry about managing the multiple versions of the Bible | `apiKey` | Yes | No |
| [Bhagavad Gita](https://docs.bhagavadgitaapi.in) | Open Source Shrimad Bhagavad Gita API including 21+ authors translation in Sanskrit/English/Hindi | `apiKey` | Yes | Yes |
| [Bhagavad Gita](https://bhagavadgita.io/api) | Bhagavad Gita text | `OAuth` | Yes | Yes |
| [Bhagavad Gita telugu](https://gita-api.vercel.app) | Bhagavad Gita API in telugu and odia languages | No | Yes | Yes |
| [Bible-api](https://bible-api.com/) | Free Bible API with multiple languages | No | Yes | Yes |
| [British National Bibliography](http://bnb.data.bl.uk/) | Books | No | No | Unknown |
| [Crossref Metadata Search](https://github.com/CrossRef/rest-api-doc) | Books &amp; Articles Metadata | No | Yes | Unknown |
| [Ganjoor](https://api.ganjoor.net) | Classic Persian poetry works including access to related manuscripts, recitations and music tracks | `OAuth` | Yes | Yes |
| [Google Books](https://developers.google.com/books/) | Books | `OAuth` | Yes | Unknown |
| [GurbaniNow](https://github.com/GurbaniNow/api) | Fast and Accurate Gurbani RESTful API | No | Yes | Unknown |
| [Gutendex](https://gutendex.com/) | Web-API for fetching data from Project Gutenberg Books Library | No | Yes | Unknown |
| [Open Library](https://openlibrary.org/developers/api) | Books, book covers and related data | No | Yes | No |
| [Penguin Publishing](http://www.penguinrandomhouse.biz/webservices/rest/) | Books, book covers and related data | No | Yes | Yes |
| [PoetryDB](https://github.com/thundercomb/poetrydb#readme) | Enables you to get instant data from our vast p

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[vinta/awesome-python]]></title>
            <link>https://github.com/vinta/awesome-python</link>
            <guid>https://github.com/vinta/awesome-python</guid>
            <pubDate>Sun, 08 Jun 2025 00:04:52 GMT</pubDate>
            <description><![CDATA[An opinionated list of awesome Python frameworks, libraries, software and resources.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/vinta/awesome-python">vinta/awesome-python</a></h1>
            <p>An opinionated list of awesome Python frameworks, libraries, software and resources.</p>
            <p>Language: Python</p>
            <p>Stars: 245,875</p>
            <p>Forks: 25,762</p>
            <p>Stars today: 145 stars today</p>
            <h2>README</h2><pre># Awesome Python [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

An opinionated list of awesome Python frameworks, libraries, software and resources.

Inspired by [awesome-php](https://github.com/ziadoz/awesome-php).

- [Awesome Python](#awesome-python)
    - [Admin Panels](#admin-panels)
    - [Algorithms and Design Patterns](#algorithms-and-design-patterns)
    - [ASGI Servers](#asgi-servers)
    - [Asynchronous Programming](#asynchronous-programming)
    - [Audio](#audio)
    - [Authentication](#authentication)
    - [Build Tools](#build-tools)
    - [Built-in Classes Enhancement](#built-in-classes-enhancement)
    - [Caching](#caching)
    - [ChatOps Tools](#chatops-tools)
    - [CMS](#cms)
    - [Code Analysis](#code-analysis)
    - [Command-line Interface Development](#command-line-interface-development)
    - [Command-line Tools](#command-line-tools)
    - [Computer Vision](#computer-vision)
    - [Configuration Files](#configuration-files)
    - [Cryptography](#cryptography)
    - [Data Analysis](#data-analysis)
    - [Data Validation](#data-validation)
    - [Data Visualization](#data-visualization)
    - [Database Drivers](#database-drivers)
    - [Database](#database)
    - [Date and Time](#date-and-time)
    - [Debugging Tools](#debugging-tools)
    - [Deep Learning](#deep-learning)
    - [DevOps Tools](#devops-tools)
    - [Distributed Computing](#distributed-computing)
    - [Distribution](#distribution)
    - [Documentation](#documentation)
    - [Downloader](#downloader)
    - [Editor Plugins and IDEs](#editor-plugins-and-ides)
    - [Email](#email)
    - [Environment Management](#environment-management)
    - [File Manipulation](#file-manipulation)
    - [Functional Programming](#functional-programming)
    - [Game Development](#game-development)
    - [Geolocation](#geolocation)
    - [GUI Development](#gui-development)
    - [Hardware](#hardware)
    - [HTML Manipulation](#html-manipulation)
    - [HTTP Clients](#http-clients)
    - [Image Processing](#image-processing)
    - [Implementations](#implementations)
    - [Interactive Interpreter](#interactive-interpreter)
    - [Internationalization](#internationalization)
    - [Job Scheduler](#job-scheduler)
    - [Logging](#logging)
    - [Machine Learning](#machine-learning)
    - [Miscellaneous](#miscellaneous)
    - [Natural Language Processing](#natural-language-processing)
    - [Network Virtualization](#network-virtualization)
    - [News Feed](#news-feed)
    - [ORM](#orm)
    - [Package Management](#package-management)
    - [Package Repositories](#package-repositories)
    - [Penetration testing](#penetration-testing)
    - [Permissions](#permissions)
    - [Processes](#processes)
    - [Recommender Systems](#recommender-systems)
    - [Refactoring](#refactoring)
    - [RESTful API](#restful-api)
    - [Robotics](#robotics)
    - [RPC Servers](#rpc-servers)
    - [Science](#science)
    - [Search](#search)
    - [Serialization](#serialization)
    - [Serverless Frameworks](#serverless-frameworks)
    - [Shell](#shell)
    - [Specific Formats Processing](#specific-formats-processing)
    - [Static Site Generator](#static-site-generator)
    - [Tagging](#tagging)
    - [Task Queues](#task-queues)
    - [Template Engine](#template-engine)
    - [Testing](#testing)
    - [Text Processing](#text-processing)
    - [Third-party APIs](#third-party-apis)
    - [URL Manipulation](#url-manipulation)
    - [Video](#video)
    - [Web Asset Management](#web-asset-management)
    - [Web Content Extracting](#web-content-extracting)
    - [Web Crawling](#web-crawling)
    - [Web Frameworks](#web-frameworks)
    - [WebSocket](#websocket)
    - [WSGI Servers](#wsgi-servers)
- [Resources](#resources)
    - [Newsletters](#newsletters)
    - [Podcasts](#podcasts)
- [Contributing](#contributing)

---

## Admin Panels

*Libraries for administrative interfaces.*

* [ajenti](https://github.com/ajenti/ajenti) - The admin panel your servers deserve.
* [django-grappelli](https://github.com/sehmaschine/django-grappelli) - A jazzy skin for the Django Admin-Interface.
* [flask-admin](https://github.com/flask-admin/flask-admin) - Simple and extensible administrative interface framework for Flask.
* [flower](https://github.com/mher/flower) - Real-time monitor and web admin for Celery.
* [jet-bridge](https://github.com/jet-admin/jet-bridge) - Admin panel framework for any application with nice UI (ex Jet Django).
* [wooey](https://github.com/wooey/wooey) - A Django app which creates automatic web UIs for Python scripts.
* [streamlit](https://github.com/streamlit/streamlit) - A framework which lets you build dashboards, generate reports, or create chat apps in minutes.

## Algorithms and Design Patterns

*Python implementation of data structures, algorithms and design patterns. Also see [awesome-algorithms](https://github.com/tayllan/awesome-algorithms).*

* Algorithms
    * [algorithms](https://github.com/keon/algorithms) - Minimal examples of data structures and algorithms.
    * [python-ds](https://github.com/prabhupant/python-ds) - A collection of data structure and algorithms for coding interviews.
    * [sortedcontainers](https://github.com/grantjenks/python-sortedcontainers) - Fast and pure-Python implementation of sorted collections.
    * [thealgorithms](https://github.com/TheAlgorithms/Python) - All Algorithms implemented in Python.
* Design Patterns
    * [pypattyrn](https://github.com/tylerlaberge/PyPattyrn) - A simple yet effective library for implementing common design patterns.
    * [python-patterns](https://github.com/faif/python-patterns) - A collection of design patterns in Python.
    * [transitions](https://github.com/pytransitions/transitions) - A lightweight, object-oriented finite state machine implementation.

## ASGI Servers

*[ASGI](https://asgi.readthedocs.io/en/latest/)-compatible web servers.*

* [daphne](https://github.com/django/daphne) - A HTTP, HTTP2 and WebSocket protocol server for ASGI and ASGI-HTTP.
* [uvicorn](https://github.com/encode/uvicorn) - A lightning-fast ASGI server implementation, using uvloop and httptools.
* [hypercorn](https://github.com/pgjones/hypercorn) - An ASGI and WSGI Server based on Hyper libraries and inspired by Gunicorn.

## Asynchronous Programming

*Libraries for asynchronous, concurrent and parallel execution. Also see [awesome-asyncio](https://github.com/timofurrer/awesome-asyncio).*

* [asyncio](https://docs.python.org/3/library/asyncio.html) - (Python standard library) Asynchronous I/O, event loop, coroutines and tasks.
    - [awesome-asyncio](https://github.com/timofurrer/awesome-asyncio)
* [concurrent.futures](https://docs.python.org/3/library/concurrent.futures.html) - (Python standard library) A high-level interface for asynchronously executing callables.
* [multiprocessing](https://docs.python.org/3/library/multiprocessing.html) - (Python standard library) Process-based parallelism.
* [trio](https://github.com/python-trio/trio) - A friendly library for async concurrency and I/O.
* [twisted](https://github.com/twisted/twisted) - An event-driven networking engine.
* [uvloop](https://github.com/MagicStack/uvloop) - Ultra fast asyncio event loop.
* [eventlet](https://github.com/eventlet/eventlet) - Asynchronous framework with WSGI support.
* [gevent](https://github.com/gevent/gevent) - A coroutine-based Python networking library that uses [greenlet](https://github.com/python-greenlet/greenlet).

## Audio

*Libraries for manipulating audio and its metadata.*

* Audio
    * [audioread](https://github.com/beetbox/audioread) - Cross-library (GStreamer + Core Audio + MAD + FFmpeg) audio decoding.
    * [audioFlux](https://github.com/libAudioFlux/audioFlux) - A library for audio and music analysis, feature extraction.
    * [dejavu](https://github.com/worldveil/dejavu) - Audio fingerprinting and recognition.
    * [kapre](https://github.com/keunwoochoi/kapre) - Keras Audio Preprocessors.
    * [librosa](https://github.com/librosa/librosa) - Python library for audio and music analysis.
    * [matchering](https://github.com/sergree/matchering) - A library for automated reference audio mastering.
    * [mingus](http://bspaans.github.io/python-mingus/) - An advanced music theory and notation package with MIDI file and playback support.
    * [pyaudioanalysis](https://github.com/tyiannak/pyAudioAnalysis) - Audio feature extraction, classification, segmentation and applications.
    * [pydub](https://github.com/jiaaro/pydub) - Manipulate audio with a simple and easy high level interface.
    * [timeside](https://github.com/Parisson/TimeSide) - Open web audio processing framework.
* Metadata
    * [beets](https://github.com/beetbox/beets) - A music library manager and [MusicBrainz](https://musicbrainz.org/) tagger.
    * [eyed3](https://github.com/nicfit/eyeD3) - A tool for working with audio files, specifically MP3 files containing ID3 metadata.
    * [mutagen](https://github.com/quodlibet/mutagen) - A Python module to handle audio metadata.
    * [tinytag](https://github.com/devsnd/tinytag) - A library for reading music meta data of MP3, OGG, FLAC and Wave files.

## Authentication

*Libraries for implementing authentications schemes.*

* OAuth
    * [authlib](https://github.com/lepture/authlib) - JavaScript Object Signing and Encryption draft implementation.
    * [django-allauth](https://github.com/pennersr/django-allauth) - Authentication app for Django that &quot;just works.&quot;
    * [django-oauth-toolkit](https://github.com/jazzband/django-oauth-toolkit) - OAuth 2 goodies for Django.
    * [oauthlib](https://github.com/oauthlib/oauthlib) - A generic and thorough implementation of the OAuth request-signing logic.
* JWT
    * [pyjwt](https://github.com/jpadilla/pyjwt) - JSON Web Token implementation in Python.
    * [python-jose](https://github.com/mpdavis/python-jose/) - A JOSE implementation in Python.

## Build Tools

*Compile software from source code.*

* [bitbake](https://github.com/openembedded/bitbake) - A make-like build tool for embedded Linux.
* [buildout](https://github.com/buildout/buildout) - A build system for creating, assembling and deploying applications from multiple parts.
* [platformio](https://github.com/platformio/platformio-core) - A console tool to build code with different development platforms.
* [pybuilder](https://github.com/pybuilder/pybuilder) - A continuous build tool written in pure Python.
* [scons](https://github.com/SCons/scons) - A software construction tool.

## Built-in Classes Enhancement

*Libraries for enhancing Python built-in classes.*

* [attrs](https://github.com/python-attrs/attrs) - Replacement for `__init__`, `__eq__`, `__repr__`, etc. boilerplate in class definitions.
* [bidict](https://github.com/jab/bidict) - Efficient, Pythonic bidirectional map data structures and related functionality..
* [box](https://github.com/cdgriffith/Box) - Python dictionaries with advanced dot notation access.
* [dataclasses](https://docs.python.org/3/library/dataclasses.html) - (Python standard library) Data classes.
* [dotteddict](https://github.com/carlosescri/DottedDict) - A library that provides a method of accessing lists and dicts with a dotted path notation.

## CMS

*Content Management Systems.*

* [feincms](https://github.com/feincms/feincms) - One of the most advanced Content Management Systems built on Django.
* [indico](https://github.com/indico/indico) - A feature-rich event management system, made @ [CERN](https://en.wikipedia.org/wiki/CERN).
* [wagtail](https://github.com/wagtail/wagtail) - A Django content management system.

## Caching

*Libraries for caching data.*

* [beaker](https://github.com/bbangert/beaker) - A WSGI middleware for sessions and caching.
* [django-cache-machine](https://github.com/django-cache-machine/django-cache-machine) - Automatic caching and invalidation for Django models.
* [django-cacheops](https://github.com/Suor/django-cacheops) - A slick ORM cache with automatic granular event-driven invalidation.
* [dogpile.cache](https://github.com/sqlalchemy/dogpile.cache) - dogpile.cache is a next generation replacement for Beaker made by the same authors.
* [hermescache](https://pypi.org/project/HermesCache/) - Python caching library with tag-based invalidation and dogpile effect prevention.
* [pylibmc](https://github.com/lericson/pylibmc) - A Python wrapper around the [libmemcached](https://libmemcached.org/libMemcached.html) interface.
* [python-diskcache](https://github.com/grantjenks/python-diskcache) - SQLite and file backed cache backend with faster lookups than memcached and redis.

## ChatOps Tools

*Libraries for chatbot development.*

* [errbot](https://github.com/errbotio/errbot/) - The easiest and most popular chatbot to implement ChatOps.

## Code Analysis

*Tools of static analysis, linters and code quality checkers. Also see [awesome-static-analysis](https://github.com/mre/awesome-static-analysis).*

* Code Analysis
    * [code2flow](https://github.com/scottrogowski/code2flow) - Turn your Python and JavaScript code into DOT flowcharts.
    * [prospector](https://github.com/PyCQA/prospector) - A tool to analyse Python code.
    * [vulture](https://github.com/jendrikseipp/vulture) - A tool for finding and analysing dead Python code.
* Code Linters
    * [flake8](https://github.com/PyCQA/flake8) - A wrapper around `pycodestyle`, `pyflakes` and McCabe.
        * [awesome-flake8-extensions](https://github.com/DmytroLitvinov/awesome-flake8-extensions)
    * [pylint](https://github.com/pylint-dev/pylint) - A fully customizable source code analyzer.
* Code Formatters
    * [black](https://github.com/psf/black) - The uncompromising Python code formatter.
    * [isort](https://github.com/timothycrosley/isort) - A Python utility / library to sort imports.
    * [yapf](https://github.com/google/yapf) - Yet another Python code formatter from Google.
* Static Type Checkers, also see [awesome-python-typing](https://github.com/typeddjango/awesome-python-typing)
    * [mypy](https://github.com/python/mypy) - Check variable types during compile time.
    * [pyre-check](https://github.com/facebook/pyre-check) - Performant type checking.
    * [typeshed](https://github.com/python/typeshed) - Collection of library stubs for Python, with static types.
* Static Type Annotations Generators
    * [monkeytype](https://github.com/Instagram/MonkeyType) - A system for Python that generates static type annotations by collecting runtime types.
    * [pytype](https://github.com/google/pytype) - Pytype checks and infers types for Python code - without requiring type annotations.

## Command-line Interface Development

*Libraries for building command-line applications.*

* Command-line Application Development
    * [cement](https://github.com/datafolklabs/cement) - CLI Application Framework for Python.
    * [click](https://github.com/pallets/click/) - A package for creating beautiful command line interfaces in a composable way.
    * [cliff](https://github.com/openstack/cliff) - A framework for creating command-line programs with multi-level commands.
    * [python-fire](https://github.com/google/python-fire) - A library for creating command line interfaces from absolutely any Python object.
    * [python-prompt-toolkit](https://github.com/prompt-toolkit/python-prompt-toolkit) - A library for building powerful interactive command lines.
* Terminal Rendering
    * [alive-progress](https://github.com/rsalmei/alive-progress) - A new kind of Progress Bar, with real-time throughput, eta and very cool animations.
    * [asciimatics](https://github.com/peterbrittain/asciimatics) - A package to create full-screen text UIs (from interactive forms to ASCII animations).
    * [bashplotlib](https://github.com/glamp/bashplotlib) - Making basic plots in the terminal.
    * [colorama](https://github.com/tartley/colorama) - Cross-platform colored terminal text.
    * [rich](https://github.com/Textualize/rich) - Python library for rich text and beautiful formatting in the terminal. Also provides a great `RichHandler` log handler.
    * [tqdm](https://github.com/tqdm/tqdm) - Fast, extensible progress bar for loops and CLI.

## Command-line Tools

*Useful CLI-based tools for productivity.*

* Productivity Tools
    * [copier](https://github.com/copier-org/copier) - A library and command-line utility for rendering projects templates.
    * [cookiecutter](https://github.com/cookiecutter/cookiecutter) - A command-line utility that creates projects from cookiecutters (project templates).
    * [doitlive](https://github.com/sloria/doitlive) - A tool for live presentations in the terminal.
    * [howdoi](https://github.com/gleitz/howdoi) - Instant coding answers via the command line.
    * [invoke](https://github.com/pyinvoke/invoke) - A tool for managing shell-oriented subprocesses and organizing executable Python code into CLI-invokable tasks.
    * [pathpicker](https://github.com/facebook/PathPicker) - Select files out of bash output.
    * [thefuck](https://github.com/nvbn/thefuck) - Correcting your previous console command.
    * [tmuxp](https://github.com/tmux-python/tmuxp) - A [tmux](https://github.com/tmux/tmux) session manager.
    * [try](https://github.com/timofurrer/try) - A dead simple CLI to try out python packages - it&#039;s never been easier.
* CLI Enhancements
    * [httpie](https://github.com/httpie/cli) - A command line HTTP client, a user-friendly cURL replacement.
    * [iredis](https://github.com/laixintao/iredis) - Redis CLI with autocompletion and syntax highlighting.
    * [litecli](https://github.com/dbcli/litecli) - SQLite CLI with autocompletion and syntax highlighting.
    * [mycli](https://github.com/dbcli/mycli) - MySQL CLI with autocompletion and syntax highlighting.
    * [pgcli](https://github.com/dbcli/pgcli) - PostgreSQL CLI with autocompletion and syntax highlighting.

## Computer Vision

*Libraries for Computer Vision.*

* [easyocr](https://github.com/JaidedAI/EasyOCR) - Ready-to-use OCR with 40+ languages supported.
* [kornia](https://github.com/kornia/kornia/) - Open Source Differentiable Computer Vision Library for PyTorch.
* [opencv](https://opencv.org/) - Open Source Computer Vision Library.
* [pytesseract](https://github.com/madmaze/pytesseract) - A wrapper for [Google Tesseract OCR](https://github.com/tesseract-ocr).
* [tesserocr](https://github.com/sirfz/tesserocr) - Another simple, Pillow-friendly, wrapper around the `tesseract-ocr` API for OCR.

## Configuration Files

*Libraries for storing and parsing configuration options.*

* [configparser](https://docs.python.org/3/library/configparser.html) - (Python standard library) INI file parser.
* [configobj](https://github.com/DiffSK/configobj) - INI file parser with validation.
* [hydra](https://github.com/facebookresearch/hydra) - Hydra is a framework for elegantly configuring complex applications.
* [python-decouple](https://github.com/HBNetwork/python-decouple) - Strict separation of settings from code.

## Cryptography

* [cryptography](https://github.com/pyca/cryptography) - A package designed to expose cryptographic primitives and recipes to Python developers.
* [paramiko](https://github.com/paramiko/paramiko) - The leading native Python SSHv2 protocol library.
* [pynacl](https://github.com/pyca/pynacl) - Python binding to the Networking and Cryptography (NaCl) library.

## Data Analysis

*Libraries for data analyzing.*

* [pandas](http://pandas.pydata.org/) - A library providing high-performance, easy-to-use data structures and data analysis tools.
* [aws-sdk-pandas](https://github.com/aws/aws-sdk-pandas) - Pandas on AWS.
* [datasette](https://github.com/simonw/datasette) - An open source multi-tool for exploring and publishing data.
* [optimus](https://github.com/hi-primus/optimus) - Agile Data Science Workflows made easy with PySpark.

## Data Validation

*Libraries for validating data. Us

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[haris-musa/excel-mcp-server]]></title>
            <link>https://github.com/haris-musa/excel-mcp-server</link>
            <guid>https://github.com/haris-musa/excel-mcp-server</guid>
            <pubDate>Sun, 08 Jun 2025 00:04:51 GMT</pubDate>
            <description><![CDATA[A Model Context Protocol server for Excel file manipulation]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/haris-musa/excel-mcp-server">haris-musa/excel-mcp-server</a></h1>
            <p>A Model Context Protocol server for Excel file manipulation</p>
            <p>Language: Python</p>
            <p>Stars: 942</p>
            <p>Forks: 113</p>
            <p>Stars today: 19 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/logo.svg&quot; alt=&quot;Excel MCP Server Logo&quot; width=&quot;300&quot;/&gt;
&lt;/p&gt;

[![PyPI version](https://img.shields.io/pypi/v/excel-mcp-server.svg)](https://pypi.org/project/excel-mcp-server/)
[![PyPI downloads](https://img.shields.io/pypi/dm/excel-mcp-server.svg)](https://pypi.org/project/excel-mcp-server/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A Model Context Protocol (MCP) server that lets you manipulate Excel files without needing Microsoft Excel installed. Create, read, and modify Excel workbooks with your AI agent.

## Features

- üìä Create and modify Excel workbooks
- üìù Read and write data
- üé® Apply formatting and styles
- üìà Create charts and visualizations
- üìä Generate pivot tables
- üîÑ Manage worksheets and ranges
- üîå Dual transport support: stdio and SSE

## Quick Start

### Prerequisites

- Python 3.10 or higher

### Running the Server

The server supports two transport modes: stdio and SSE.

#### Using stdio transport

Stdio transport is ideal for direct integration with tools like Cursor Desktop or local development, which can manipulate local files:

```bash
uvx excel-mcp-server stdio
```

#### Using SSE transport

SSE transport is perfect for remote connections, which manipulate remote files:

```bash
uvx excel-mcp-server sse
```

### Add to Cursor

[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=excel-mcp-server&amp;config=eyJjb21tYW5kIjoidXZ4IGV4Y2VsLW1jcC1zZXJ2ZXIgc3RkaW8ifQ%3D%3D)

## Using with AI Tools

1. Add this configuration to your client, choosing the appropriate transport method for your needs:

**Stdio transport connection** (for local integration):
```json
{
   &quot;mcpServers&quot;: {
      &quot;excel-stdio&quot;: {
         &quot;command&quot;: &quot;uvx&quot;,
         &quot;args&quot;: [&quot;excel-mcp-server&quot;, &quot;stdio&quot;]
      }
   }
}
```

**SSE transport connection**:
```json
{
   &quot;mcpServers&quot;: {
      &quot;excel&quot;: {
         &quot;url&quot;: &quot;http://localhost:8000/sse&quot;,
      }
   }
}
```

2. The Excel tools will be available through your AI assistant.

## Environment Variables &amp; File Path Handling

### SSE Transport

When running the server with the **SSE protocol**, you **must set the `EXCEL_FILES_PATH` environment variable on the server side**. This variable tells the server where to read and write Excel files.
- If not set, it defaults to `./excel_files`.

You can also set the `FASTMCP_PORT` environment variable to control the port the server listens on (default is `8000` if not set).
- Example (Windows PowerShell):
  ```powershell
  $env:EXCEL_FILES_PATH=&quot;E:\MyExcelFiles&quot;
  $env:FASTMCP_PORT=&quot;8080&quot;
  uvx excel-mcp-server sse
  ```
- Example (Linux/macOS):
  ```bash
  EXCEL_FILES_PATH=/path/to/excel_files FASTMCP_PORT=8080 uvx excel-mcp-server sse
  ```

### Stdio Transport

When using the **stdio protocol**, the file path is provided with each tool call, so you do **not** need to set `EXCEL_FILES_PATH` on the server. The server will use the path sent by the client for each operation.

## Available Tools

The server provides a comprehensive set of Excel manipulation tools. See [TOOLS.md](TOOLS.md) for complete documentation of all available tools.

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=haris-musa/excel-mcp-server&amp;type=Date)](https://www.star-history.com/#haris-musa/excel-mcp-server&amp;Date)

## License

MIT License - see [LICENSE](LICENSE) for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/autogen]]></title>
            <link>https://github.com/microsoft/autogen</link>
            <guid>https://github.com/microsoft/autogen</guid>
            <pubDate>Sun, 08 Jun 2025 00:04:50 GMT</pubDate>
            <description><![CDATA[A programming framework for agentic AI ü§ñ PyPi: autogen-agentchat Discord: https://aka.ms/autogen-discord Office Hour: https://aka.ms/autogen-officehour]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/autogen">microsoft/autogen</a></h1>
            <p>A programming framework for agentic AI ü§ñ PyPi: autogen-agentchat Discord: https://aka.ms/autogen-discord Office Hour: https://aka.ms/autogen-officehour</p>
            <p>Language: Python</p>
            <p>Stars: 45,645</p>
            <p>Forks: 6,922</p>
            <p>Stars today: 32 stars today</p>
            <h2>README</h2><pre>&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://microsoft.github.io/autogen/0.2/img/ag.svg&quot; alt=&quot;AutoGen Logo&quot; width=&quot;100&quot;&gt;

[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&amp;label=Follow%20%40pyautogen)](https://twitter.com/pyautogen)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Company?style=flat&amp;logo=linkedin&amp;logoColor=white)](https://www.linkedin.com/company/105812540)
[![Discord](https://img.shields.io/badge/discord-chat-green?logo=discord)](https://aka.ms/autogen-discord)
[![Documentation](https://img.shields.io/badge/Documentation-AutoGen-blue?logo=read-the-docs)](https://microsoft.github.io/autogen/)
[![Blog](https://img.shields.io/badge/Blog-AutoGen-blue?logo=blogger)](https://devblogs.microsoft.com/autogen/)

&lt;/div&gt;

&lt;div align=&quot;center&quot; style=&quot;background-color: rgba(255, 235, 59, 0.5); padding: 10px; border-radius: 5px; margin: 20px 0;&quot;&gt;
  &lt;strong&gt;Important:&lt;/strong&gt; This is the official project. We are not affiliated with any fork or startup. See our &lt;a href=&quot;https://x.com/pyautogen/status/1857264760951296210&quot;&gt;statement&lt;/a&gt;.
&lt;/div&gt;

# AutoGen

**AutoGen** is a framework for creating multi-agent AI applications that can act autonomously or work alongside humans.

## Installation

AutoGen requires **Python 3.10 or later**.

```bash
# Install AgentChat and OpenAI client from Extensions
pip install -U &quot;autogen-agentchat&quot; &quot;autogen-ext[openai]&quot;
```

The current stable version is v0.4. If you are upgrading from AutoGen v0.2, please refer to the [Migration Guide](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/migration-guide.html) for detailed instructions on how to update your code and configurations.

```bash
# Install AutoGen Studio for no-code GUI
pip install -U &quot;autogenstudio&quot;
```

## Quickstart

### Hello World

Create an assistant agent using OpenAI&#039;s GPT-4o model. See [other supported models](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html).

```python
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient

async def main() -&gt; None:
    model_client = OpenAIChatCompletionClient(model=&quot;gpt-4o&quot;)
    agent = AssistantAgent(&quot;assistant&quot;, model_client=model_client)
    print(await agent.run(task=&quot;Say &#039;Hello World!&#039;&quot;))
    await model_client.close()

asyncio.run(main())
```

### Web Browsing Agent Team

Create a group chat team with a web surfer agent and a user proxy agent
for web browsing tasks. You need to install [playwright](https://playwright.dev/python/docs/library).

```python
# pip install -U autogen-agentchat autogen-ext[openai,web-surfer]
# playwright install
import asyncio
from autogen_agentchat.agents import UserProxyAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.agents.web_surfer import MultimodalWebSurfer

async def main() -&gt; None:
    model_client = OpenAIChatCompletionClient(model=&quot;gpt-4o&quot;)
    # The web surfer will open a Chromium browser window to perform web browsing tasks.
    web_surfer = MultimodalWebSurfer(&quot;web_surfer&quot;, model_client, headless=False, animate_actions=True)
    # The user proxy agent is used to get user input after each step of the web surfer.
    # NOTE: you can skip input by pressing Enter.
    user_proxy = UserProxyAgent(&quot;user_proxy&quot;)
    # The termination condition is set to end the conversation when the user types &#039;exit&#039;.
    termination = TextMentionTermination(&quot;exit&quot;, sources=[&quot;user_proxy&quot;])
    # Web surfer and user proxy take turns in a round-robin fashion.
    team = RoundRobinGroupChat([web_surfer, user_proxy], termination_condition=termination)
    try:
        # Start the team and wait for it to terminate.
        await Console(team.run_stream(task=&quot;Find information about AutoGen and write a short summary.&quot;))
    finally:
        await web_surfer.close()
        await model_client.close()

asyncio.run(main())
```

### AutoGen Studio

Use AutoGen Studio to prototype and run multi-agent workflows without writing code.

```bash
# Run AutoGen Studio on http://localhost:8080
autogenstudio ui --port 8080 --appdir ./my-app
```

## Why Use AutoGen?

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;autogen-landing.jpg&quot; alt=&quot;AutoGen Landing&quot; width=&quot;500&quot;&gt;
&lt;/div&gt;

The AutoGen ecosystem provides everything you need to create AI agents, especially multi-agent workflows -- framework, developer tools, and applications.

The _framework_ uses a layered and extensible design. Layers have clearly divided responsibilities and build on top of layers below. This design enables you to use the framework at different levels of abstraction, from high-level APIs to low-level components.

- [Core API](./python/packages/autogen-core/) implements message passing, event-driven agents, and local and distributed runtime for flexibility and power. It also support cross-language support for .NET and Python.
- [AgentChat API](./python/packages/autogen-agentchat/) implements a simpler but opinionated¬†API for rapid prototyping. This API is built on top of the Core API and is closest to what users of v0.2 are familiar with and supports common multi-agent patterns such as two-agent chat or group chats.
- [Extensions API](./python/packages/autogen-ext/) enables first- and third-party extensions continuously expanding framework capabilities. It support specific implementation of LLM clients (e.g., OpenAI, AzureOpenAI), and capabilities such as code execution.

The ecosystem also supports two essential _developer tools_:

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://media.githubusercontent.com/media/microsoft/autogen/refs/heads/main/python/packages/autogen-studio/docs/ags_screen.png&quot; alt=&quot;AutoGen Studio Screenshot&quot; width=&quot;500&quot;&gt;
&lt;/div&gt;

- [AutoGen Studio](./python/packages/autogen-studio/) provides a no-code GUI for building multi-agent applications.
- [AutoGen Bench](./python/packages/agbench/) provides a benchmarking suite for evaluating agent performance.

You can use the AutoGen framework and developer tools to create applications for your domain. For example, [Magentic-One](./python/packages/magentic-one-cli/) is a state-of-the-art multi-agent team built using AgentChat API and Extensions API that can handle a variety of tasks that require web browsing, code execution, and file handling.

With AutoGen you get to join and contribute to a thriving ecosystem. We host weekly office hours and talks with maintainers and community. We also have a [Discord server](https://aka.ms/autogen-discord) for real-time chat, GitHub Discussions for Q&amp;A, and a blog for tutorials and updates.

## Where to go next?

&lt;div align=&quot;center&quot;&gt;

|               | [![Python](https://img.shields.io/badge/AutoGen-Python-blue?logo=python&amp;logoColor=white)](./python)                                                                                                                                                                                                                                                                                                                | [![.NET](https://img.shields.io/badge/AutoGen-.NET-green?logo=.net&amp;logoColor=white)](./dotnet) | [![Studio](https://img.shields.io/badge/AutoGen-Studio-purple?logo=visual-studio&amp;logoColor=white)](./python/packages/autogen-studio)                     |
| ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Installation  | [![Installation](https://img.shields.io/badge/Install-blue)](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/installation.html)                                                                                                                                                                                                                                                            | [![Install](https://img.shields.io/badge/Install-green)](https://microsoft.github.io/autogen/dotnet/dev/core/installation.html) | [![Install](https://img.shields.io/badge/Install-purple)](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/installation.html) |
| Quickstart    | [![Quickstart](https://img.shields.io/badge/Quickstart-blue)](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/quickstart.html#)                                                                                                                                                                                                                                                            | [![Quickstart](https://img.shields.io/badge/Quickstart-green)](https://microsoft.github.io/autogen/dotnet/dev/core/index.html) | [![Usage](https://img.shields.io/badge/Quickstart-purple)](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html#)        |
| Tutorial      | [![Tutorial](https://img.shields.io/badge/Tutorial-blue)](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/index.html)                                                                                                                                                                                                                                                            | [![Tutorial](https://img.shields.io/badge/Tutorial-green)](https://microsoft.github.io/autogen/dotnet/dev/core/tutorial.html) | [![Usage](https://img.shields.io/badge/Tutorial-purple)](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html#)        |
| API Reference | [![API](https://img.shields.io/badge/Docs-blue)](https://microsoft.github.io/autogen/stable/reference/index.html#)                                                                                                                                                                                                                                                                                                    | [![API](https://img.shields.io/badge/Docs-green)](https://microsoft.github.io/autogen/dotnet/dev/api/Microsoft.AutoGen.Contracts.html) | [![API](https://img.shields.io/badge/Docs-purple)](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html)               |
| Packages      | [![PyPi autogen-core](https://img.shields.io/badge/PyPi-autogen--core-blue?logo=pypi)](https://pypi.org/project/autogen-core/) &lt;br&gt; [![PyPi autogen-agentchat](https://img.shields.io/badge/PyPi-autogen--agentchat-blue?logo=pypi)](https://pypi.org/project/autogen-agentchat/) &lt;br&gt; [![PyPi autogen-ext](https://img.shields.io/badge/PyPi-autogen--ext-blue?logo=pypi)](https://pypi.org/project/autogen-ext/) | [![NuGet Contracts](https://img.shields.io/badge/NuGet-Contracts-green?logo=nuget)](https://www.nuget.org/packages/Microsoft.AutoGen.Contracts/) &lt;br&gt; [![NuGet Core](https://img.shields.io/badge/NuGet-Core-green?logo=nuget)](https://www.nuget.org/packages/Microsoft.AutoGen.Core/) &lt;br&gt; [![NuGet Core.Grpc](https://img.shields.io/badge/NuGet-Core.Grpc-green?logo=nuget)](https://www.nuget.org/packages/Microsoft.AutoGen.Core.Grpc/) &lt;br&gt; [![NuGet RuntimeGateway.Grpc](https://img.shields.io/badge/NuGet-RuntimeGateway.Grpc-green?logo=nuget)](https://www.nuget.org/packages/Microsoft.AutoGen.RuntimeGateway.Grpc/) | [![PyPi autogenstudio](https://img.shields.io/badge/PyPi-autogenstudio-purple?logo=pypi)](https://pypi.org/project/autogenstudio/)                       |

&lt;/div&gt;


Interested in contributing? See [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines on how to get started. We welcome contributions of all kinds, including bug fixes, new features, and documentation improvements. Join our community and help us make AutoGen better!

Have questions? Check out our [Frequently Asked Questions (FAQ)](./FAQ.md) for answers to common queries. If you don&#039;t find what you&#039;re looking for, feel free to ask in our [GitHub Discussions](https://github.com/microsoft/autogen/discussions) or join our [Discord server](https://aka.ms/autogen-discord) for real-time support. You can also read our [blog](https://devblogs.microsoft.com/autogen/) for updates.

## Legal Notices

Microsoft and any contributors grant you a license to the Microsoft documentation and other content
in this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),
see the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the
[LICENSE-CODE](LICENSE-CODE) file.

Microsoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation
may be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.
The licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.
Microsoft&#039;s general trademark guidelines can be found at &lt;http://go.microsoft.com/fwlink/?LinkID=254653&gt;.

Privacy information can be found at &lt;https://go.microsoft.com/fwlink/?LinkId=521839&gt;

Microsoft and any contributors reserve all other rights, whether under their respective copyrights, patents,
or trademarks, whether by implication, estoppel, or otherwise.

&lt;p align=&quot;right&quot; style=&quot;font-size: 14px; color: #555; margin-top: 20px;&quot;&gt;
  &lt;a href=&quot;#readme-top&quot; style=&quot;text-decoration: none; color: blue; font-weight: bold;&quot;&gt;
    ‚Üë Back to Top ‚Üë
  &lt;/a&gt;
&lt;/p&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[coleam00/Archon]]></title>
            <link>https://github.com/coleam00/Archon</link>
            <guid>https://github.com/coleam00/Archon</guid>
            <pubDate>Sun, 08 Jun 2025 00:04:49 GMT</pubDate>
            <description><![CDATA[Archon is an AI agent that is able to create other AI agents using an advanced agentic coding workflow and framework knowledge base to unlock a new frontier of automated agents.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/coleam00/Archon">coleam00/Archon</a></h1>
            <p>Archon is an AI agent that is able to create other AI agents using an advanced agentic coding workflow and framework knowledge base to unlock a new frontier of automated agents.</p>
            <p>Language: Python</p>
            <p>Stars: 4,800</p>
            <p>Forks: 929</p>
            <p>Stars today: 156 stars today</p>
            <h2>README</h2><pre># Archon - AI Agent Builder

&lt;img src=&quot;public/Archon.png&quot; alt=&quot;Archon Logo&quot; /&gt;

&lt;div align=&quot;center&quot; style=&quot;margin-top: 20px;margin-bottom: 30px&quot;&gt;

&lt;h3&gt;üöÄ **CURRENT VERSION** üöÄ&lt;/h3&gt;

**[ V6 - Tool Library and MCP Integration ]**
*Prebuilt tools, examples, and MCP server integration*

&lt;/div&gt;

&gt; **üîÑ IMPORTANT UPDATE (March 31st)**: Archon now includes a library of prebuilt tools, examples, and MCP server integrations. Archon can now incorporate these resources when building new agents, significantly enhancing capabilities and reducing hallucinations. Note that the examples/tool library for Archon is just starting out. Please feel free to contribute examples, MCP servers, and prebuilt tools!

Archon is the world&#039;s first **&quot;Agenteer&quot;**, an AI agent designed to autonomously build, refine, and optimize other AI agents. 

It serves both as a practical tool for developers and as an educational framework demonstrating the evolution of agentic systems.
Archon will be developed in iterations, starting with just a simple Pydantic AI agent that can build other Pydantic AI agents,
all the way to a full agentic workflow using LangGraph that can build other AI agents with any framework.
Through its iterative development, Archon showcases the power of planning, feedback loops, and domain-specific knowledge in creating robust AI agents.

## Important Links

- The current version of Archon is V6 as mentioned above - see [V6 Documentation](iterations/v6-tool-library-integration/README.md) for details.

- I **just** created the [Archon community](https://thinktank.ottomator.ai/c/archon/30) forum over in the oTTomator Think Tank! Please post any questions you have there!

- [GitHub Kanban board](https://github.com/users/coleam00/projects/1) for feature implementation and bug squashing.

## Vision

Archon demonstrates three key principles in modern AI development:

1. **Agentic Reasoning**: Planning, iterative feedback, and self-evaluation overcome the limitations of purely reactive systems
2. **Domain Knowledge Integration**: Seamless embedding of frameworks like Pydantic AI and LangGraph within autonomous workflows
3. **Scalable Architecture**: Modular design supporting maintainability, cost optimization, and ethical AI practices

## Getting Started with V6 (current version)

Since V6 is the current version of Archon, all the code for V6 is in both the main directory and `archon/iterations/v6-tool-library-integration` directory.

Note that the examples/tool library for Archon is just starting out. Please feel free to contribute examples, MCP servers, and prebuilt tools!

### Prerequisites
- Docker (optional but preferred)
- Python 3.11+
- Supabase account (for vector database)
- OpenAI/Anthropic/OpenRouter API key or Ollama for local LLMs (note that only OpenAI supports streaming in the Streamlit UI currently)

### Installation

#### Option 1: Docker (Recommended)
1. Clone the repository:
```bash
git clone https://github.com/coleam00/archon.git
cd archon
```

2. Run the Docker setup script:
```bash
# This will build both containers and start Archon
python run_docker.py
```

3. Access the Streamlit UI at http://localhost:8501.

&gt; **Note:** `run_docker.py` will automatically:
&gt; - Build the MCP server container
&gt; - Build the main Archon container
&gt; - Run Archon with the appropriate port mappings
&gt; - Use environment variables from `.env` file if it exists

#### Option 2: Local Python Installation
1. Clone the repository:
```bash
git clone https://github.com/coleam00/archon.git
cd archon
```

2. Install dependencies:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

3. Start the Streamlit UI:
```bash
streamlit run streamlit_ui.py
```

4. Access the Streamlit UI at http://localhost:8501.

### Setup Process

After installation, follow the guided setup process in the Intro section of the Streamlit UI:
- **Environment**: Configure your API keys and model settings - all stored in `workbench/env_vars.json`
- **Database**: Set up your Supabase vector database
- **Documentation**: Crawl and index the Pydantic AI documentation
- **Agent Service**: Start the agent service for generating agents
- **Chat**: Interact with Archon to create AI agents
- **MCP** (optional): Configure integration with AI IDEs

The Streamlit interface will guide you through each step with clear instructions and interactive elements.
There are a good amount of steps for the setup but it goes quick!

### Troubleshooting

If you encounter any errors when using Archon, please first check the logs in the &quot;Agent Service&quot; tab.
Logs specifically for MCP are also logged to `workbench/logs.txt` (file is automatically created) so please
check there. The goal is for you to have a clear error message before creating a bug here in the GitHub repo

### Updating Archon

#### Option 1: Docker
To get the latest updates for Archon when using Docker:

```bash
# Pull the latest changes from the repository (from within the archon directory)
git pull

# Rebuild and restart the containers with the latest changes
python run_docker.py
```

The `run_docker.py` script will automatically:
- Detect and remove any existing Archon containers (whether running or stopped)
- Rebuild the containers with the latest code
- Start fresh containers with the updated version

#### Option 2: Local Python Installation
To get the latest updates for Archon when using local Python installation:

```bash
# Pull the latest changes from the repository (from within the archon directory)
git pull

# Install any new dependencies
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt

# Restart the Streamlit UI
# (If you&#039;re already running it, stop with Ctrl+C first)
streamlit run streamlit_ui.py
```

This ensures you&#039;re always running the most recent version of Archon with all the latest features and bug fixes.

## Project Evolution

### V1: Single-Agent Foundation
- Basic RAG-powered agent using Pydantic AI
- Supabase vector database for documentation storage
- Simple code generation without validation
- [Learn more about V1](iterations/v1-single-agent/README.md)

### V2: Agentic Workflow (LangGraph)
- Multi-agent system with planning and execution separation
- Reasoning LLM (O3-mini/R1) for architecture planning
- LangGraph for workflow orchestration
- Support for local LLMs via Ollama
- [Learn more about V2](iterations/v2-agentic-workflow/README.md)

### V3: MCP Support
- Integration with AI IDEs like Windsurf and Cursor
- Automated file creation and dependency management
- FastAPI service for agent generation
- Improved project structure and organization
- [Learn more about V3](iterations/v3-mcp-support/README.md)

### V4: Streamlit UI Overhaul
- Docker support
- Comprehensive Streamlit interface for managing all aspects of Archon
- Guided setup process with interactive tabs
- Environment variable management through the UI
- Database setup and documentation crawling simplified
- Agent service control and monitoring
- MCP configuration through the UI
- [Learn more about V4](iterations/v4-streamlit-ui-overhaul/README.md)

### V5: Multi-Agent Coding Workflow
- Specialized refiner agents for different autonomously improving the initially generated agent
- Prompt refiner agent for optimizing system prompts
- Tools refiner agent for specialized tool implementation
- Agent refiner for optimizing agent configuration and dependencies
- Cohesive initial agent structure before specialized refinement
- Improved workflow orchestration with LangGraph
- [Learn more about V5](iterations/v5-parallel-specialized-agents/README.md)

### V6: Current - Tool Library and MCP Integration
- Comprehensive library of prebuilt tools, examples, and agent templates
- Integration with MCP servers for massive amounts of prebuilt tools
- Advisor agent that recommends relevant tools and examples based on user requirements
- Automatic incorporation of prebuilt components into new agents
- Specialized tools refiner agent also validates and optimizes MCP server configurations
- Streamlined access to external services through MCP integration
- Reduced development time through component reuse
- [Learn more about V6](iterations/v6-tool-library-integration/README.md)

### Future Iterations
- V7: LangGraph Documentation - Allow Archon to build Pydantic AI AND LangGraph agents
- V8: Self-Feedback Loop - Automated validation and error correction
- V9: Self Agent Execution - Testing and iterating on agents in an isolated environment
- V10: Multi-Framework Support - Framework-agnostic agent generation
- V11: Autonomous Framework Learning - Self-updating framework adapters
- V12: Advanced RAG Techniques - Enhanced retrieval and incorporation of framework documentation
- V13: MCP Agent Marketplace - Integrating Archon agents as MCP servers and publishing to marketplaces

### Future Integrations
- LangSmith
- MCP marketplace
- Other frameworks besides Pydantic AI
- Other vector databases besides Supabase
- [Local AI package](https://github.com/coleam00/local-ai-packaged) for the agent environment

## Archon Agents Architecture

The below diagram from the LangGraph studio is a visual representation of the Archon agent graph.

&lt;img src=&quot;public/ArchonGraph.png&quot; alt=&quot;Archon Graph&quot; /&gt;

The flow works like this:

1. You describe the initial AI agent you want to create
2. The reasoner LLM creates the high level scope for the agent
3. The primary coding agent uses the scope and documentation to create the initial agent
4. Control is passed back to you to either give feedback or ask Archon to &#039;refine&#039; the agent autonomously
5. If refining autonomously, the specialized agents are invoked to improve the prompt, tools, and agent configuration
6. The primary coding agent is invoked again with either user or specialized agent feedback
7. The process goes back to step 4 until you say the agent is complete
8. Once the agent is complete, Archon spits out the full code again with instructions for running it

## File Architecture

### Core Files
- `streamlit_ui.py`: Comprehensive web interface for managing all aspects of Archon
- `graph_service.py`: FastAPI service that handles the agentic workflow
- `run_docker.py`: Script to build and run Archon Docker containers
- `Dockerfile`: Container definition for the main Archon application

### MCP Integration
- `mcp/`: Model Context Protocol server implementation
  - `mcp_server.py`: MCP server script for AI IDE integration
  - `Dockerfile`: Container definition for the MCP server

### Archon Package
- `archon/`: Core agent and workflow implementation
  - `archon_graph.py`: LangGraph workflow definition and agent coordination
  - `pydantic_ai_coder.py`: Main coding agent with RAG capabilities
  - `refiner_agents/`: Specialized agents for refining different aspects of the created agent
    - `prompt_refiner_agent.py`: Optimizes system prompts
    - `tools_refiner_agent.py`: Specializes in tool implementation
    - `agent_refiner_agent.py`: Refines agent configuration and dependencies
  - `crawl_pydantic_ai_docs.py`: Documentation crawler and processor

### Utilities
- `utils/`: Utility functions and database setup
  - `utils.py`: Shared utility functions
  - `site_pages.sql`: Database setup commands

### Workbench
- `workbench/`: Created at runtime, files specific to your environment
  - `env_vars.json`: Environment variables defined in the UI are stored here (included in .gitignore, file is created automatically)
  - `logs.txt`: Low level logs for all Archon processes go here
  - `scope.md`: The detailed scope document created by the reasoner model at the start of each Archon execution

## Deployment Options
- **Docker Containers**: Run Archon in isolated containers with all dependencies included
  - Main container: Runs the Streamlit UI and graph service
  - MCP container: Provides MCP server functionality for AI IDEs
- **Local Python**: Run directly on your system with a Python virtual environment

### Docker Architecture
The Docker implementation consists of two containers:
1. **Main Archon Container**:
   - Runs the Streamlit UI on port 8501
   - Hosts the Graph Service on port 8100
   - Built from the root Dockerfile
   - Handles all agent functionality and user interactions

2. **MCP Container**:
   - Implements the Model Context Protocol for AI IDE integration
   - Built from the mcp/Dockerfile
   - Communicates with the main container&#039;s Graph Service
   - Provides a standardized interface for AI IDEs like Windsurf, Cursor, Cline, and Roo Code

When running with Docker, the `run_docker.py` script automates building and starting both containers with the proper configuration.

## Database Setup

The Supabase database uses the following schema:

```sql
CREATE TABLE site_pages (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    url TEXT,
    chunk_number INTEGER,
    title TEXT,
    summary TEXT,
    content TEXT,
    metadata JSONB,
    embedding VECTOR(1536) -- Adjust dimensions as necessary (i.e. 768 for nomic-embed-text)
);
```

The Streamlit UI provides an interface to set up this database structure automatically.

## Contributing

We welcome contributions! Whether you&#039;re fixing bugs, adding features, or improving documentation, please feel free to submit a Pull Request.

## License

[MIT License](LICENSE)

---

For version-specific details:
- [V1 Documentation](iterations/v1-single-agent/README.md)
- [V2 Documentation](iterations/v2-agentic-workflow/README.md)
- [V3 Documentation](iterations/v3-mcp-support/README.md)
- [V4 Documentation](iterations/v4-streamlit-ui-overhaul/README.md)
- [V5 Documentation](iterations/v5-parallel-specialized-agents/README.md)
- [V6 Documentation](iterations/v6-tool-library-integration/README.md)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[karpathy/nanoGPT]]></title>
            <link>https://github.com/karpathy/nanoGPT</link>
            <guid>https://github.com/karpathy/nanoGPT</guid>
            <pubDate>Sun, 08 Jun 2025 00:04:48 GMT</pubDate>
            <description><![CDATA[The simplest, fastest repository for training/finetuning medium-sized GPTs.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/karpathy/nanoGPT">karpathy/nanoGPT</a></h1>
            <p>The simplest, fastest repository for training/finetuning medium-sized GPTs.</p>
            <p>Language: Python</p>
            <p>Stars: 41,719</p>
            <p>Forks: 6,957</p>
            <p>Stars today: 18 stars today</p>
            <h2>README</h2><pre>
# nanoGPT

![nanoGPT](assets/nanogpt.jpg)

The simplest, fastest repository for training/finetuning medium-sized GPTs. It is a rewrite of [minGPT](https://github.com/karpathy/minGPT) that prioritizes teeth over education. Still under active development, but currently the file `train.py` reproduces GPT-2 (124M) on OpenWebText, running on a single 8XA100 40GB node in about 4 days of training. The code itself is plain and readable: `train.py` is a ~300-line boilerplate training loop and `model.py` a ~300-line GPT model definition, which can optionally load the GPT-2 weights from OpenAI. That&#039;s it.

![repro124m](assets/gpt2_124M_loss.png)

Because the code is so simple, it is very easy to hack to your needs, train new models from scratch, or finetune pretrained checkpoints (e.g. biggest one currently available as a starting point would be the GPT-2 1.3B model from OpenAI).

## install

```
pip install torch numpy transformers datasets tiktoken wandb tqdm
```

Dependencies:

- [pytorch](https://pytorch.org) &lt;3
- [numpy](https://numpy.org/install/) &lt;3
-  `transformers` for huggingface transformers &lt;3 (to load GPT-2 checkpoints)
-  `datasets` for huggingface datasets &lt;3 (if you want to download + preprocess OpenWebText)
-  `tiktoken` for OpenAI&#039;s fast BPE code &lt;3
-  `wandb` for optional logging &lt;3
-  `tqdm` for progress bars &lt;3

## quick start

If you are not a deep learning professional and you just want to feel the magic and get your feet wet, the fastest way to get started is to train a character-level GPT on the works of Shakespeare. First, we download it as a single (1MB) file and turn it from raw text into one large stream of integers:

```sh
python data/shakespeare_char/prepare.py
```

This creates a `train.bin` and `val.bin` in that data directory. Now it is time to train your GPT. The size of it very much depends on the computational resources of your system:

**I have a GPU**. Great, we can quickly train a baby GPT with the settings provided in the [config/train_shakespeare_char.py](config/train_shakespeare_char.py) config file:

```sh
python train.py config/train_shakespeare_char.py
```

If you peek inside it, you&#039;ll see that we&#039;re training a GPT with a context size of up to 256 characters, 384 feature channels, and it is a 6-layer Transformer with 6 heads in each layer. On one A100 GPU this training run takes about 3 minutes and the best validation loss is 1.4697. Based on the configuration, the model checkpoints are being written into the `--out_dir` directory `out-shakespeare-char`. So once the training finishes we can sample from the best model by pointing the sampling script at this directory:

```sh
python sample.py --out_dir=out-shakespeare-char
```

This generates a few samples, for example:

```
ANGELO:
And cowards it be strawn to my bed,
And thrust the gates of my threats,
Because he that ale away, and hang&#039;d
An one with him.

DUKE VINCENTIO:
I thank your eyes against it.

DUKE VINCENTIO:
Then will answer him to save the malm:
And what have you tyrannous shall do this?

DUKE VINCENTIO:
If you have done evils of all disposition
To end his power, the day of thrust for a common men
That I leave, to fight with over-liking
Hasting in a roseman.
```

lol  `¬Ø\_(„ÉÑ)_/¬Ø`. Not bad for a character-level model after 3 minutes of training on a GPU. Better results are quite likely obtainable by instead finetuning a pretrained GPT-2 model on this dataset (see finetuning section later).

**I only have a macbook** (or other cheap computer). No worries, we can still train a GPT but we want to dial things down a notch. I recommend getting the bleeding edge PyTorch nightly ([select it here](https://pytorch.org/get-started/locally/) when installing) as it is currently quite likely to make your code more efficient. But even without it, a simple train run could look as follows:

```sh
python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0
```

Here, since we are running on CPU instead of GPU we must set both `--device=cpu` and also turn off PyTorch 2.0 compile with `--compile=False`. Then when we evaluate we get a bit more noisy but faster estimate (`--eval_iters=20`, down from 200), our context size is only 64 characters instead of 256, and the batch size only 12 examples per iteration, not 64. We&#039;ll also use a much smaller Transformer (4 layers, 4 heads, 128 embedding size), and decrease the number of iterations to 2000 (and correspondingly usually decay the learning rate to around max_iters with `--lr_decay_iters`). Because our network is so small we also ease down on regularization (`--dropout=0.0`). This still runs in about ~3 minutes, but gets us a loss of only 1.88 and therefore also worse samples, but it&#039;s still good fun:

```sh
python sample.py --out_dir=out-shakespeare-char --device=cpu
```
Generates samples like this:

```
GLEORKEN VINGHARD III:
Whell&#039;s the couse, the came light gacks,
And the for mought you in Aut fries the not high shee
bot thou the sought bechive in that to doth groan you,
No relving thee post mose the wear
```

Not bad for ~3 minutes on a CPU, for a hint of the right character gestalt. If you&#039;re willing to wait longer, feel free to tune the hyperparameters, increase the size of the network, the context length (`--block_size`), the length of training, etc.

Finally, on Apple Silicon Macbooks and with a recent PyTorch version make sure to add `--device=mps` (short for &quot;Metal Performance Shaders&quot;); PyTorch then uses the on-chip GPU that can *significantly* accelerate training (2-3X) and allow you to use larger networks. See [Issue 28](https://github.com/karpathy/nanoGPT/issues/28) for more.

## reproducing GPT-2

A more serious deep learning professional may be more interested in reproducing GPT-2 results. So here we go - we first tokenize the dataset, in this case the [OpenWebText](https://openwebtext2.readthedocs.io/en/latest/), an open reproduction of OpenAI&#039;s (private) WebText:

```sh
python data/openwebtext/prepare.py
```

This downloads and tokenizes the [OpenWebText](https://huggingface.co/datasets/openwebtext) dataset. It will create a `train.bin` and `val.bin` which holds the GPT2 BPE token ids in one sequence, stored as raw uint16 bytes. Then we&#039;re ready to kick off training. To reproduce GPT-2 (124M) you&#039;ll want at least an 8X A100 40GB node and run:

```sh
torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py
```

This will run for about 4 days using PyTorch Distributed Data Parallel (DDP) and go down to loss of ~2.85. Now, a GPT-2 model just evaluated on OWT gets a val loss of about 3.11, but if you finetune it it will come down to ~2.85 territory (due to an apparent domain gap), making the two models ~match.

If you&#039;re in a cluster environment and you are blessed with multiple GPU nodes you can make GPU go brrrr e.g. across 2 nodes like:

```sh
# Run on the first (master) node with example IP 123.456.123.456:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py
# Run on the worker node:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py
```

It is a good idea to benchmark your interconnect (e.g. iperf3). In particular, if you don&#039;t have Infiniband then also prepend `NCCL_IB_DISABLE=1` to the above launches. Your multinode training will work, but most likely _crawl_. By default checkpoints are periodically written to the `--out_dir`. We can sample from the model by simply `python sample.py`.

Finally, to train on a single GPU simply run the `python train.py` script. Have a look at all of its args, the script tries to be very readable, hackable and transparent. You&#039;ll most likely want to tune a number of those variables depending on your needs.

## baselines

OpenAI GPT-2 checkpoints allow us to get some baselines in place for openwebtext. We can get the numbers as follows:

```sh
$ python train.py config/eval_gpt2.py
$ python train.py config/eval_gpt2_medium.py
$ python train.py config/eval_gpt2_large.py
$ python train.py config/eval_gpt2_xl.py
```

and observe the following losses on train and val:

| model | params | train loss | val loss |
| ------| ------ | ---------- | -------- |
| gpt2 | 124M         | 3.11  | 3.12     |
| gpt2-medium | 350M  | 2.85  | 2.84     |
| gpt2-large | 774M   | 2.66  | 2.67     |
| gpt2-xl | 1558M     | 2.56  | 2.54     |

However, we have to note that GPT-2 was trained on (closed, never released) WebText, while OpenWebText is just a best-effort open reproduction of this dataset. This means there is a dataset domain gap. Indeed, taking the GPT-2 (124M) checkpoint and finetuning on OWT directly for a while reaches loss down to ~2.85. This then becomes the more appropriate baseline w.r.t. reproduction.

## finetuning

Finetuning is no different than training, we just make sure to initialize from a pretrained model and train with a smaller learning rate. For an example of how to finetune a GPT on new text go to `data/shakespeare` and run `prepare.py` to download the tiny shakespeare dataset and render it into a `train.bin` and `val.bin`, using the OpenAI BPE tokenizer from GPT-2. Unlike OpenWebText this will run in seconds. Finetuning can take very little time, e.g. on a single GPU just a few minutes. Run an example finetuning like:

```sh
python train.py config/finetune_shakespeare.py
```

This will load the config parameter overrides in `config/finetune_shakespeare.py` (I didn&#039;t tune them much though). Basically, we initialize from a GPT2 checkpoint with `init_from` and train as normal, except shorter and with a small learning rate. If you&#039;re running out of memory try decreasing the model size (they are `{&#039;gpt2&#039;, &#039;gpt2-medium&#039;, &#039;gpt2-large&#039;, &#039;gpt2-xl&#039;}`) or possibly decreasing the `block_size` (context length). The best checkpoint (lowest validation loss) will be in the `out_dir` directory, e.g. in `out-shakespeare` by default, per the config file. You can then run the code in `sample.py --out_dir=out-shakespeare`:

```
THEODORE:
Thou shalt sell me to the highest bidder: if I die,
I sell thee to the first; if I go mad,
I sell thee to the second; if I
lie, I sell thee to the third; if I slay,
I sell thee to the fourth: so buy or sell,
I tell thee again, thou shalt not sell my
possession.

JULIET:
And if thou steal, thou shalt not sell thyself.

THEODORE:
I do not steal; I sell the stolen goods.

THEODORE:
Thou know&#039;st not what thou sell&#039;st; thou, a woman,
Thou art ever a victim, a thing of no worth:
Thou hast no right, no right, but to be sold.
```

Whoa there, GPT, entering some dark place over there. I didn&#039;t really tune the hyperparameters in the config too much, feel free to try!

## sampling / inference

Use the script `sample.py` to sample either from pre-trained GPT-2 models released by OpenAI, or from a model you trained yourself. For example, here is a way to sample from the largest available `gpt2-xl` model:

```sh
python sample.py \
    --init_from=gpt2-xl \
    --start=&quot;What is the answer to life, the universe, and everything?&quot; \
    --num_samples=5 --max_new_tokens=100
```

If you&#039;d like to sample from a model you trained, use the `--out_dir` to point the code appropriately. You can also prompt the model with some text from a file, e.g. ```python sample.py --start=FILE:prompt.txt```.

## efficiency notes

For simple model benchmarking and profiling, `bench.py` might be useful. It&#039;s identical to what happens in the meat of the training loop of `train.py`, but omits much of the other complexities.

Note that the code by default uses [PyTorch 2.0](https://pytorch.org/get-started/pytorch-2.0/). At the time of writing (Dec 29, 2022) this makes `torch.compile()` available in the nightly release. The improvement from the one line of code is noticeable, e.g. cutting down iteration time from ~250ms / iter to 135ms / iter. Nice work PyTorch team!

## todos

- Investigate and add FSDP instead of DDP
- Eval zero-shot perplexities on standard evals (e.g. LAMBADA? HELM? etc.)
- Finetune the finetuning script, I think the hyperparams are not great
- Schedule for linear batch size increase during training
- Incorporate other embeddings (rotary, alibi)
- Separate out the optim buffers from model params in checkpoints I think
- Additional logging around network health (e.g. gradient clip events, magnitudes)
- Few more investigations around better init etc.

## troubleshooting

Note that by default this repo uses PyTorch 2.0 (i.e. `torch.compile`). This is fairly new and experimental, and not yet available on all platforms (e.g. Windows). If you&#039;re running into related error messages try to disable this by adding `--compile=False` flag. This will slow down the code but at least it will run.

For some context on this repository, GPT, and language modeling it might be helpful to watch my [Zero To Hero series](https://karpathy.ai/zero-to-hero.html). Specifically, the [GPT video](https://www.youtube.com/watch?v=kCc8FmEb1nY) is popular if you have some prior language modeling context.

For more questions/discussions feel free to stop by **#nanoGPT** on Discord:

[![](https://dcbadge.vercel.app/api/server/3zy8kqD9Cp?compact=true&amp;style=flat)](https://discord.gg/3zy8kqD9Cp)

## acknowledgements

All nanoGPT experiments are powered by GPUs on [Lambda labs](https://lambdalabs.com), my favorite Cloud GPU provider. Thank you Lambda labs for sponsoring nanoGPT!
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[dreammis/social-auto-upload]]></title>
            <link>https://github.com/dreammis/social-auto-upload</link>
            <guid>https://github.com/dreammis/social-auto-upload</guid>
            <pubDate>Sun, 08 Jun 2025 00:04:47 GMT</pubDate>
            <description><![CDATA[Ëá™Âä®Âåñ‰∏ä‰º†ËßÜÈ¢ëÂà∞Á§æ‰∫§Â™í‰ΩìÔºöÊäñÈü≥„ÄÅÂ∞èÁ∫¢‰π¶„ÄÅËßÜÈ¢ëÂè∑„ÄÅtiktok„ÄÅyoutube„ÄÅbilibili]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/dreammis/social-auto-upload">dreammis/social-auto-upload</a></h1>
            <p>Ëá™Âä®Âåñ‰∏ä‰º†ËßÜÈ¢ëÂà∞Á§æ‰∫§Â™í‰ΩìÔºöÊäñÈü≥„ÄÅÂ∞èÁ∫¢‰π¶„ÄÅËßÜÈ¢ëÂè∑„ÄÅtiktok„ÄÅyoutube„ÄÅbilibili</p>
            <p>Language: Python</p>
            <p>Stars: 5,174</p>
            <p>Forks: 869</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre># social-auto-upload

`social-auto-upload` ÊòØ‰∏Ä‰∏™Âº∫Â§ßÁöÑËá™Âä®ÂåñÂ∑•ÂÖ∑ÔºåÊó®Âú®Â∏ÆÂä©ÂÜÖÂÆπÂàõ‰ΩúËÄÖÂíåËøêËê•ËÄÖÈ´òÊïàÂú∞Â∞ÜËßÜÈ¢ëÂÜÖÂÆπ‰∏ÄÈîÆÂèëÂ∏ÉÂà∞Â§ö‰∏™ÂõΩÂÜÖÂ§ñ‰∏ªÊµÅÁ§æ‰∫§Â™í‰ΩìÂπ≥Âè∞„ÄÇ
È°πÁõÆÂÆûÁé∞‰∫ÜÂØπ `ÊäñÈü≥`„ÄÅ`Bilibili`„ÄÅ`Â∞èÁ∫¢‰π¶`„ÄÅ`Âø´Êâã`„ÄÅ`ËßÜÈ¢ëÂè∑`„ÄÅ`ÁôæÂÆ∂Âè∑` ‰ª•Âèä `TikTok` Á≠âÂπ≥Âè∞ÁöÑËßÜÈ¢ë‰∏ä‰º†„ÄÅÂÆöÊó∂ÂèëÂ∏ÉÁ≠âÂäüËÉΩ„ÄÇ
ÁªìÂêàÂêÑÂπ≥Âè∞ `uploader` Ê®°ÂùóÔºåÊÇ®ÂèØ‰ª•ËΩªÊùæÈÖçÁΩÆÂíåÊâ©Â±ïÊîØÊåÅÁöÑÂπ≥Âè∞ÔºåÂπ∂ÈÄöËøáÁ§∫‰æãËÑöÊú¨Âø´ÈÄü‰∏äÊâã„ÄÇ

&lt;img src=&quot;media/show/tkupload.gif&quot; alt=&quot;tiktok show&quot; width=&quot;800&quot;/&gt;

## ÁõÆÂΩï

- [üí° ÂäüËÉΩÁâπÊÄß](#üí°ÂäüËÉΩÁâπÊÄß)
- [üöÄ ÊîØÊåÅÁöÑÂπ≥Âè∞](#üöÄÊîØÊåÅÁöÑÂπ≥Âè∞)
- [üíæ ÂÆâË£ÖÊåáÂçó](#üíæÂÆâË£ÖÊåáÂçó)
- [üèÅ Âø´ÈÄüÂºÄÂßã](#üèÅÂø´ÈÄüÂºÄÂßã)
- [üêá È°πÁõÆËÉåÊôØ](#üêáÈ°πÁõÆËÉåÊôØ)
- [üìÉ ËØ¶ÁªÜÊñáÊ°£](#üìÉËØ¶ÁªÜÊñáÊ°£)
- [üêæ ‰∫§ÊµÅ‰∏éÊîØÊåÅ](#üêæ‰∫§ÊµÅ‰∏éÊîØÊåÅ)
- [ü§ù Ë¥°ÁåÆÊåáÂçó](#ü§ùË¥°ÁåÆÊåáÂçó)
- [üìú ËÆ∏ÂèØËØÅ](#üìúËÆ∏ÂèØËØÅ)
- [‚≠ê Star History](#‚≠êStar-History)

## üí°ÂäüËÉΩÁâπÊÄß

### Â∑≤ÊîØÊåÅÂπ≥Âè∞

-   **ÂõΩÂÜÖÂπ≥Âè∞**:
    -   [x] ÊäñÈü≥
    -   [x] ËßÜÈ¢ëÂè∑
    -   [x] Bilibili
    -   [x] Â∞èÁ∫¢‰π¶
    -   [x] Âø´Êâã
    -   [x] ÁôæÂÆ∂Âè∑
-   **ÂõΩÂ§ñÂπ≥Âè∞**:
    -   [x] TikTok

### Ê†∏ÂøÉÂäüËÉΩ

-   [x] ÂÆöÊó∂‰∏ä‰º† (Cron Job / Scheduled Upload)
-   [ ] Cookie ÁÆ°ÁêÜ (ÈÉ®ÂàÜÂÆûÁé∞ÔºåÊåÅÁª≠‰ºòÂåñ‰∏≠)
-   [ ] ÂõΩÂ§ñÂπ≥Âè∞ Proxy ËÆæÁΩÆ (ÈÉ®ÂàÜÂÆûÁé∞)

### ËÆ°ÂàíÊîØÊåÅ‰∏éÂºÄÂèë‰∏≠

-   **Âπ≥Âè∞Êâ©Â±ï**:
    -   [ ] QQËßÜÈ¢ë
    -   [ ] YouTube
-   **ÂäüËÉΩÂ¢ûÂº∫**:
    -   [x] Êõ¥ÊòìÁî®ÁöÑÁâàÊú¨ (GUI / CLI ‰∫§‰∫í‰ºòÂåñ)
    -   [x] API Â∞ÅË£Ö
    -   [ ] Docker ÈÉ®ÁΩ≤
    -   [ ] Ëá™Âä®Âåñ‰∏ä‰º† (Êõ¥Êô∫ËÉΩÁöÑË∞ÉÂ∫¶Á≠ñÁï•)
    -   [ ] Â§öÁ∫øÁ®ã/ÂºÇÊ≠•‰∏ä‰º†‰ºòÂåñ
    -   [ ] Slack/Ê∂àÊÅØÊé®ÈÄÅÈÄöÁü•

---

## üöÄÊîØÊåÅÁöÑÂπ≥Âè∞

Êú¨È°πÁõÆÈÄöËøáÂêÑÂπ≥Âè∞ÂØπÂ∫îÁöÑ `uploader` Ê®°ÂùóÂÆûÁé∞ËßÜÈ¢ë‰∏ä‰º†ÂäüËÉΩ„ÄÇÊÇ®ÂèØ‰ª•Âú® `examples` ÁõÆÂΩï‰∏ãÊâæÂà∞ÂêÑ‰∏™Âπ≥Âè∞ÁöÑ‰ΩøÁî®Á§∫‰æãËÑöÊú¨„ÄÇ

ÊØè‰∏™Á§∫‰æãËÑöÊú¨Â±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÈÖçÁΩÆÂíåË∞ÉÁî®Áõ∏Â∫îÁöÑ uploader„ÄÇ

## üíæÂÆâË£ÖÊåáÂçó

1.  **ÂÖãÈöÜÈ°πÁõÆ**:
    ```bash
    git clone https://github.com/dreammis/social-auto-upload.git
    cd social-auto-upload
    ```

2.  **ÂÆâË£Ö‰æùËµñ**:
    Âª∫ËÆÆÂú®ËôöÊãüÁéØÂ¢É‰∏≠ÂÆâË£Ö‰æùËµñ„ÄÇ
    ```bash
    conda create -n social-auto-upload python=3.10
    conda activate social-auto-upload
    # ÊåÇËΩΩÊ∏ÖÂçéÈïúÂÉè or ÂëΩ‰ª§Ë°å‰ª£ÁêÜ
    pip install -r requirements.txt
    ```

3.  **ÂÆâË£Ö Playwright ÊµèËßàÂô®È©±Âä®**:
    ```bash
    playwright install chromium firefox
    ```
    Ê†πÊçÆÊÇ®ÁöÑÈúÄÊ±ÇÔºåËá≥Â∞ëÈúÄË¶ÅÂÆâË£Ö `chromium`„ÄÇ`firefox` ‰∏ªË¶ÅÁî®‰∫é TikTok ‰∏ä‰º†ÔºàÊóßÁâàÔºâ„ÄÇ

4.  **‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂**:
    Â§çÂà∂ `conf.example.py` Âπ∂ÈáçÂëΩÂêç‰∏∫ `conf.py`„ÄÇ
    Âú® `conf.py` ‰∏≠ÔºåÊÇ®ÈúÄË¶ÅÈÖçÁΩÆ‰ª•‰∏ãÂÜÖÂÆπÔºö
    -   `LOCAL_CHROME_PATH`: Êú¨Âú∞ Chrome ÊµèËßàÂô®ÁöÑË∑ØÂæÑÔºåÊØîÂ¶Ç `C:\Program Files\Google\Chrome\Application\chrome.exe` ‰øùÂ≠ò„ÄÇ
    
    **‰∏¥Êó∂Ëß£ÂÜ≥ÊñπÊ°à**

    ÈúÄË¶ÅÂú®Ê†πÁõÆÂΩïÂàõÂª∫ `cookiesFile` Âíå `videoFile` ‰∏§‰∏™Êñá‰ª∂Â§πÔºåÂàÜÂà´ÊòØ Â≠òÂÇ®cookieÊñá‰ª∂ Âíå Â≠òÂÇ®‰∏ä‰º†Êñá‰ª∂ ÁöÑÊñá‰ª∂Â§π

5.  **ÈÖçÁΩÆÊï∞ÊçÆÂ∫ì**:
    Â¶ÇÊûú db/database.db Êñá‰ª∂‰∏çÂ≠òÂú®ÔºåÊÇ®ÂèØ‰ª•ËøêË°å‰ª•‰∏ãÂëΩ‰ª§Êù•ÂàùÂßãÂåñÊï∞ÊçÆÂ∫ìÔºö
    ```bash
    cd db
    python createTable.py
    ```
    Ê≠§ÂëΩ‰ª§Â∞ÜÂàùÂßãÂåñ SQLite Êï∞ÊçÆÂ∫ì„ÄÇ

6.  **ÂêØÂä®ÂêéÁ´ØÈ°πÁõÆ**:
    ```bash
    python sau_backend.py
    ```
    ÂêéÁ´ØÈ°πÁõÆÂ∞ÜÂú® `http://localhost:5409` ÂêØÂä®„ÄÇ

7.  **ÂêØÂä®ÂâçÁ´ØÈ°πÁõÆ**:
    ```bash
    cd sau_frontend
    npm install
    npm run dev
    ```
    ÂâçÁ´ØÈ°πÁõÆÂ∞ÜÂú® `http://localhost:5173` ÂêØÂä®ÔºåÂú®ÊµèËßàÂô®‰∏≠ÊâìÂºÄÊ≠§ÈìæÊé•Âç≥ÂèØËÆøÈóÆ„ÄÇ


&gt; ÈùûÁ®ãÂ∫èÂëòÁî®Êà∑ÂèØ‰ª•ÂèÇËÄÉÔºö[Êñ∞ÊâãÁ∫ßÊïôÁ®ã](https://juejin.cn/post/7372114027840208911)


## üèÅÂø´ÈÄüÂºÄÂßã

1.  **ÂáÜÂ§á Cookie**: 
    Â§ßÂ§öÊï∞Âπ≥Âè∞ÈúÄË¶ÅÁôªÂΩïÂêéÁöÑ Cookie ‰ø°ÊÅØÊâçËÉΩËøõË°åÊìç‰Ωú„ÄÇËØ∑ÂèÇÁÖß examples ÁõÆÂΩï‰∏ãÂêÑ `get_xxx_cookie.py` ËÑöÊú¨Ôºà‰æãÂ¶Ç get_douyin_cookie.py, get_ks_cookie.pyÔºâÁöÑËØ¥ÊòéÔºåËøêË°åËÑöÊú¨‰ª•ÁîüÊàêÂπ∂‰øùÂ≠ò Cookie Êñá‰ª∂ÔºàÈÄöÂ∏∏Âú® `cookies/[PLATFORM]_uploader/account.json`Ôºâ„ÄÇ

2.  **ÂáÜÂ§áËßÜÈ¢ëÊñá‰ª∂**: 
    Â∞ÜÈúÄË¶Å‰∏ä‰º†ÁöÑËßÜÈ¢ëÊñá‰ª∂ÔºàÈÄöÂ∏∏‰∏∫ `.mp4` Ê†ºÂºèÔºâÊîæÁΩÆÂú® videos ÁõÆÂΩï‰∏ã„ÄÇ
    ÈÉ®ÂàÜÂπ≥Âè∞ÊîØÊåÅËßÜÈ¢ëÂ∞ÅÈù¢ÔºåÂèØ‰ª•Â∞ÜÂ∞ÅÈù¢ÂõæÁâáÔºà‰æãÂ¶Ç `.png` Ê†ºÂºèÔºå‰∏éËßÜÈ¢ëÂêåÂêçÔºâ‰πüÊîæÂú®Ê≠§ÁõÆÂΩï„ÄÇ
    Â¶ÇÊûúÈúÄË¶Å‰∏ä‰º†Ê†áÈ¢òÂèäÊ†áÁ≠æÔºåËØ∑Âú®ËßÜÈ¢ëÊñá‰ª∂ÊóÅËæπÂàõÂª∫‰∏Ä‰∏™ÂêåÂêçÁöÑ `.txt` Êñá‰ª∂ÔºåÂÜÖÂÆπ‰∏∫Ê†áÈ¢òÂíåÊ†áÁ≠æÔºå‰ª•Êç¢Ë°åÂàÜÈöî„ÄÇ

3.  **‰øÆÊîπÂπ∂ËøêË°åÁ§∫‰æãËÑöÊú¨**:
    ÊâìÂºÄ examples ÁõÆÂΩï‰∏≠ÊÇ®ÊÉ≥‰ΩøÁî®ÁöÑÂπ≥Âè∞ÁöÑ‰∏ä‰º†ËÑöÊú¨Ôºà‰æãÂ¶Ç upload_video_to_douyin.pyÔºâ„ÄÇ
    -   Ê†πÊçÆËÑöÊú¨ÂÜÖÁöÑÊ≥®ÈáäÂíåËØ¥ÊòéÔºåÁ°ÆËÆ§ Cookie Êñá‰ª∂Ë∑ØÂæÑ„ÄÅËßÜÈ¢ëÊñá‰ª∂Ë∑ØÂæÑÁ≠âÈÖçÁΩÆÊòØÂê¶Ê≠£Á°Æ„ÄÇ
    -   ÊÇ®ÂèØ‰ª•‰øÆÊîπËÑöÊú¨‰ª•ÈÄÇÂ∫îÊÇ®ÁöÑÂÖ∑‰ΩìÈúÄÊ±ÇÔºå‰æãÂ¶ÇÊâπÈáè‰∏ä‰º†„ÄÅËá™ÂÆö‰πâÊ†áÈ¢ò„ÄÅÊ†áÁ≠æÁ≠â„ÄÇ

4.  **ÊâßË°å‰∏ä‰º†**:
    ËøêË°å‰øÆÊîπÂêéÁöÑÁ§∫‰æãËÑöÊú¨Ôºå‰æãÂ¶ÇÔºö
    ```bash
    python examples/upload_video_to_douyin.py
    ```

## üêáÈ°πÁõÆËÉåÊôØ

ËØ•È°πÁõÆÊúÄÂàùÊòØÊàë‰∏™‰∫∫Áî®‰∫éËá™Âä®ÂåñÁÆ°ÁêÜÁ§æ‰∫§Â™í‰ΩìËßÜÈ¢ëÂèëÂ∏ÉÁöÑÂ∑•ÂÖ∑„ÄÇÊàëÁöÑ‰∏ªË¶ÅÂèëÂ∏ÉÁ≠ñÁï•ÊòØÊèêÂâç‰∏ÄÂ§©ËÆæÁΩÆÂÆöÊó∂ÂèëÂ∏ÉÔºåÂõ†Ê≠§È°πÁõÆ‰∏≠ÂæàÂ§öÂÆöÊó∂ÂèëÂ∏ÉÁõ∏ÂÖ≥ÁöÑÈÄªËæëÊòØÂü∫‰∫é‚ÄúÁ¨¨‰∫åÂ§©‚ÄùÁöÑÊó∂Èó¥ËøõË°åËÆ°ÁÆóÁöÑ„ÄÇ

Â¶ÇÊûúÊÇ®ÈúÄË¶ÅÁ´ãÂç≥ÂèëÂ∏ÉÊàñÂÖ∂‰ªñÂÆöÂà∂ÂåñÁöÑÂèëÂ∏ÉÁ≠ñÁï•ÔºåÊ¨¢ËøéÁ†îÁ©∂Ê∫êÁ†ÅÊàñÂú®Á§æÂå∫ÊèêÈóÆ„ÄÇ

## üìÉËØ¶ÁªÜÊñáÊ°£

Êõ¥ËØ¶ÁªÜÁöÑÊñáÊ°£ÂíåËØ¥ÊòéÔºåËØ∑Êü•ÁúãÔºö[social-auto-upload ÂÆòÊñπÊñáÊ°£](https://sap-doc.nasdaddy.com/)

## üêæ‰∫§ÊµÅ‰∏éÊîØÊåÅ

[‚òï Donate as u like](https://www.buymeacoffee.com/hysn2001m) - Â¶ÇÊûúÊÇ®ËßâÂæóËøô‰∏™È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåÂèØ‰ª•ËÄÉËôëËµûÂä©„ÄÇ

Â¶ÇÊûúÊÇ®‰πüÊòØÁã¨Á´ãÂºÄÂèëËÄÖ„ÄÅÊäÄÊúØÁà±Â•ΩËÄÖÔºåÂØπ #ÊäÄÊúØÂèòÁé∞ #AIÂàõ‰∏ö #Ë∑®Â¢ÉÁîµÂïÜ #Ëá™Âä®ÂåñÂ∑•ÂÖ∑ #ËßÜÈ¢ëÂàõ‰Ωú Á≠âËØùÈ¢òÊÑüÂÖ¥Ë∂£ÔºåÊ¨¢ËøéÂä†ÂÖ•Á§æÁæ§‰∫§ÊµÅ„ÄÇ

### Creator

&lt;table&gt;
    &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://sap-doc.nasdaddy.com/&quot;&gt;
            &lt;img src=&quot;media/mp.jpg&quot; width=&quot;200px&quot; alt=&quot;NasDaddyÂÖ¨‰ºóÂè∑&quot;/&gt;
            &lt;br /&gt;
            &lt;sub&gt;&lt;b&gt;ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑&lt;/b&gt;&lt;/sub&gt;
        &lt;/a&gt;
        &lt;br /&gt;
        &lt;a href=&quot;https://github.com/dreammis/social-auto-upload/commits?author=dreammis&quot; title=&quot;Code&quot;&gt;üíª&lt;/a&gt;
        &lt;br /&gt;
        ÂÖ≥Ê≥®ÂÖ¨‰ºóÂè∑ÔºåÂêéÂè∞ÂõûÂ§ç `‰∏ä‰º†` Ëé∑ÂèñÂä†Áæ§ÊñπÂºè
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://sap-doc.nasdaddy.com/&quot;&gt;
            &lt;img src=&quot;media/QR.png&quot; width=&quot;200px&quot; alt=&quot;ËµûËµèÁ†Å/ÂÖ•Áæ§ÂºïÂØº&quot;/&gt;
            &lt;br /&gt;
            &lt;sub&gt;&lt;b&gt;‰∫§ÊµÅÁæ§ (ÈÄöËøáÂÖ¨‰ºóÂè∑Ëé∑Âèñ)&lt;/b&gt;&lt;/sub&gt;
        &lt;/a&gt;
        &lt;br /&gt;
        &lt;a href=&quot;https://sap-doc.nasdaddy.com/&quot; title=&quot;Documentation&quot;&gt;üìñ&lt;/a&gt;
        &lt;br /&gt;
        Â¶ÇÊûúÊÇ®ËßâÂæóÈ°πÁõÆÊúâÁî®ÔºåÂèØ‰ª•ËÄÉËôëÊâìËµèÊîØÊåÅ‰∏Ä‰∏ã
    &lt;/td&gt;
&lt;/table&gt;

### Active Core Team

&lt;table&gt;
    &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://leedebug.github.io/&quot;&gt;
            &lt;img src=&quot;media/edan-qrcode.png&quot; width=&quot;200px&quot; alt=&quot;Edan Lee&quot;/&gt;
            &lt;br /&gt;
            &lt;sub&gt;&lt;b&gt;Edan Lee&lt;/b&gt;&lt;/sub&gt;
        &lt;/a&gt;
        &lt;br /&gt;
        &lt;a href=&quot;https://github.com/dreammis/social-auto-upload/commits?author=LeeDebug&quot; title=&quot;Code&quot;&gt;üíª&lt;/a&gt;
        &lt;a href=&quot;https://leedebug.github.io/&quot; title=&quot;Documentation&quot;&gt;üìñ&lt;/a&gt;
        &lt;br /&gt;
        Â∞ÅË£Ö‰∫Ü api Êé•Âè£Âíå web ÂâçÁ´ØÁÆ°ÁêÜÁïåÈù¢
        &lt;br /&gt;
        ÔºàËØ∑Ê≥®ÊòéÊù•ÊÑèÔºöËøõÁæ§„ÄÅÂ≠¶‰π†„ÄÅ‰ºÅ‰∏öÂí®ËØ¢Á≠âÔºâ
    &lt;/td&gt;
&lt;/table&gt;

## ü§ùË¥°ÁåÆÊåáÂçó

Ê¨¢ËøéÂêÑÁßçÂΩ¢ÂºèÁöÑË¥°ÁåÆÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÔºö

-   Êèê‰∫§ BugÊä•Âëä Âíå FeatureËØ∑Ê±Ç„ÄÇ
-   ÊîπËøõ‰ª£Á†Å„ÄÅÊñáÊ°£„ÄÇ
-   ÂàÜ‰∫´‰ΩøÁî®ÁªèÈ™åÂíåÊïôÁ®ã„ÄÇ

Â¶ÇÊûúÊÇ®Â∏åÊúõË¥°ÁåÆ‰ª£Á†ÅÔºåËØ∑ÈÅµÂæ™‰ª•‰∏ãÊ≠•È™§Ôºö

1.  Fork Êú¨‰ªìÂ∫ì„ÄÇ
2.  ÂàõÂª∫‰∏Ä‰∏™Êñ∞ÁöÑÂàÜÊîØ (`git checkout -b feature/YourFeature` Êàñ `bugfix/YourBugfix`)„ÄÇ
3.  Êèê‰∫§ÊÇ®ÁöÑÊõ¥Êîπ (`git commit -m &#039;Add some feature&#039;`)„ÄÇ
4.  PushÂà∞ÊÇ®ÁöÑÂàÜÊîØ (`git push origin feature/YourFeature`)„ÄÇ
5.  ÂàõÂª∫‰∏Ä‰∏™ Pull Request„ÄÇ

## üìúËÆ∏ÂèØËØÅ

Êú¨È°πÁõÆÊöÇÊó∂ÈááÁî® [MIT License](LICENSE) ÂºÄÊ∫êËÆ∏ÂèØËØÅ„ÄÇ

## ‚≠êStar-History

&gt; Â¶ÇÊûúËøô‰∏™È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåËØ∑Áªô‰∏Ä‰∏™ ‚≠ê Star ‰ª•Ë°®Á§∫ÊîØÊåÅÔºÅ

[![Star History Chart](https://api.star-history.com/svg?repos=dreammis/social-auto-upload&amp;type=Date)](https://star-history.com/#dreammis/social-auto-upload&amp;Date)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[infiniflow/ragflow]]></title>
            <link>https://github.com/infiniflow/ragflow</link>
            <guid>https://github.com/infiniflow/ragflow</guid>
            <pubDate>Sun, 08 Jun 2025 00:04:46 GMT</pubDate>
            <description><![CDATA[RAGFlow is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/infiniflow/ragflow">infiniflow/ragflow</a></h1>
            <p>RAGFlow is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding.</p>
            <p>Language: Python</p>
            <p>Stars: 54,535</p>
            <p>Forks: 5,289</p>
            <p>Stars today: 47 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://demo.ragflow.io/&quot;&gt;
&lt;img src=&quot;web/src/assets/logo-with-text.png&quot; width=&quot;520&quot; alt=&quot;ragflow logo&quot;&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;./README.md&quot;&gt;&lt;img alt=&quot;README in English&quot; src=&quot;https://img.shields.io/badge/English-DBEDFA&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_tzh.md&quot;&gt;&lt;img alt=&quot;ÁπÅÈ´î‰∏≠ÊñáÊñá‰ª∂&quot; src=&quot;https://img.shields.io/badge/ÁπÅÈ´î‰∏≠Êñá-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_zh.md&quot;&gt;&lt;img alt=&quot;ÁÆÄ‰Ωì‰∏≠ÊñáÁâàËá™Ëø∞Êñá‰ª∂&quot; src=&quot;https://img.shields.io/badge/ÁÆÄ‰Ωì‰∏≠Êñá-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_ja.md&quot;&gt;&lt;img alt=&quot;Êó•Êú¨Ë™û„ÅÆREADME&quot; src=&quot;https://img.shields.io/badge/Êó•Êú¨Ë™û-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_ko.md&quot;&gt;&lt;img alt=&quot;ÌïúÍµ≠Ïñ¥&quot; src=&quot;https://img.shields.io/badge/ÌïúÍµ≠Ïñ¥-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_id.md&quot;&gt;&lt;img alt=&quot;Bahasa Indonesia&quot; src=&quot;https://img.shields.io/badge/Bahasa Indonesia-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_pt_br.md&quot;&gt;&lt;img alt=&quot;Portugu√™s(Brasil)&quot; src=&quot;https://img.shields.io/badge/Portugu√™s(Brasil)-DFE0E5&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://x.com/intent/follow?screen_name=infiniflowai&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/twitter/follow/infiniflow?logo=X&amp;color=%20%23f5f5f5&quot; alt=&quot;follow on X(Twitter)&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://demo.ragflow.io&quot; target=&quot;_blank&quot;&gt;
        &lt;img alt=&quot;Static Badge&quot; src=&quot;https://img.shields.io/badge/Online-Demo-4e6b99&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://hub.docker.com/r/infiniflow/ragflow&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/docker/pulls/infiniflow/ragflow?label=Docker%20Pulls&amp;color=0db7ed&amp;logo=docker&amp;logoColor=white&amp;style=flat-square&quot; alt=&quot;docker pull infiniflow/ragflow:v0.19.0&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/infiniflow/ragflow/releases/latest&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/github/v/release/infiniflow/ragflow?color=blue&amp;label=Latest%20Release&quot; alt=&quot;Latest Release&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/infiniflow/ragflow/blob/main/LICENSE&quot;&gt;
        &lt;img height=&quot;21&quot; src=&quot;https://img.shields.io/badge/License-Apache--2.0-ffffff?labelColor=d4eaf7&amp;color=2e6cc4&quot; alt=&quot;license&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://deepwiki.com/infiniflow/ragflow&quot;&gt;
        &lt;img alt=&quot;Ask DeepWiki&quot; src=&quot;https://deepwiki.com/badge.svg&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;h4 align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://ragflow.io/docs/dev/&quot;&gt;Document&lt;/a&gt; |
  &lt;a href=&quot;https://github.com/infiniflow/ragflow/issues/4214&quot;&gt;Roadmap&lt;/a&gt; |
  &lt;a href=&quot;https://twitter.com/infiniflowai&quot;&gt;Twitter&lt;/a&gt; |
  &lt;a href=&quot;https://discord.gg/NjYzJD3GM3&quot;&gt;Discord&lt;/a&gt; |
  &lt;a href=&quot;https://demo.ragflow.io&quot;&gt;Demo&lt;/a&gt;
&lt;/h4&gt;

#

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/9064&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/9064&quot; alt=&quot;infiniflow%2Fragflow | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;details open&gt;
&lt;summary&gt;&lt;b&gt;üìï Table of Contents&lt;/b&gt;&lt;/summary&gt;

- üí° [What is RAGFlow?](#-what-is-ragflow)
- üéÆ [Demo](#-demo)
- üìå [Latest Updates](#-latest-updates)
- üåü [Key Features](#-key-features)
- üîé [System Architecture](#-system-architecture)
- üé¨ [Get Started](#-get-started)
- üîß [Configurations](#-configurations)
- üîß [Build a docker image without embedding models](#-build-a-docker-image-without-embedding-models)
- üîß [Build a docker image including embedding models](#-build-a-docker-image-including-embedding-models)
- üî® [Launch service from source for development](#-launch-service-from-source-for-development)
- üìö [Documentation](#-documentation)
- üìú [Roadmap](#-roadmap)
- üèÑ [Community](#-community)
- üôå [Contributing](#-contributing)

&lt;/details&gt;

## üí° What is RAGFlow?

[RAGFlow](https://ragflow.io/) is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document
understanding. It offers a streamlined RAG workflow for businesses of any scale, combining LLM (Large Language Models)
to provide truthful question-answering capabilities, backed by well-founded citations from various complex formatted
data.

## üéÆ Demo

Try our demo at [https://demo.ragflow.io](https://demo.ragflow.io).

&lt;div align=&quot;center&quot; style=&quot;margin-top:20px;margin-bottom:20px;&quot;&gt;
&lt;img src=&quot;https://github.com/infiniflow/ragflow/assets/7248/2f6baa3e-1092-4f11-866d-36f6a9d075e5&quot; width=&quot;1200&quot;/&gt;
&lt;img src=&quot;https://github.com/user-attachments/assets/504bbbf1-c9f7-4d83-8cc5-e9cb63c26db6&quot; width=&quot;1200&quot;/&gt;
&lt;/div&gt;

## üî• Latest Updates

- 2025-05-23 Adds a Python/JavaScript code executor component to Agent.
- 2025-05-05 Supports cross-language query.
- 2025-03-19 Supports using a multi-modal model to make sense of images within PDF or DOCX files.
- 2025-02-28 Combined with Internet search (Tavily), supports reasoning like Deep Research for any LLMs.
- 2024-12-18 Upgrades Document Layout Analysis model in DeepDoc.
- 2024-08-22 Support text to SQL statements through RAG.

## üéâ Stay Tuned

‚≠êÔ∏è Star our repository to stay up-to-date with exciting new features and improvements! Get instant notifications for new
releases! üåü

&lt;div align=&quot;center&quot; style=&quot;margin-top:20px;margin-bottom:20px;&quot;&gt;
&lt;img src=&quot;https://github.com/user-attachments/assets/18c9707e-b8aa-4caf-a154-037089c105ba&quot; width=&quot;1200&quot;/&gt;
&lt;/div&gt;

## üåü Key Features

### üç≠ **&quot;Quality in, quality out&quot;**

- [Deep document understanding](./deepdoc/README.md)-based knowledge extraction from unstructured data with complicated
  formats.
- Finds &quot;needle in a data haystack&quot; of literally unlimited tokens.

### üç± **Template-based chunking**

- Intelligent and explainable.
- Plenty of template options to choose from.

### üå± **Grounded citations with reduced hallucinations**

- Visualization of text chunking to allow human intervention.
- Quick view of the key references and traceable citations to support grounded answers.

### üçî **Compatibility with heterogeneous data sources**

- Supports Word, slides, excel, txt, images, scanned copies, structured data, web pages, and more.

### üõÄ **Automated and effortless RAG workflow**

- Streamlined RAG orchestration catered to both personal and large businesses.
- Configurable LLMs as well as embedding models.
- Multiple recall paired with fused re-ranking.
- Intuitive APIs for seamless integration with business.

## üîé System Architecture

&lt;div align=&quot;center&quot; style=&quot;margin-top:20px;margin-bottom:20px;&quot;&gt;
&lt;img src=&quot;https://github.com/infiniflow/ragflow/assets/12318111/d6ac5664-c237-4200-a7c2-a4a00691b485&quot; width=&quot;1000&quot;/&gt;
&lt;/div&gt;

## üé¨ Get Started

### üìù Prerequisites

- CPU &gt;= 4 cores
- RAM &gt;= 16 GB
- Disk &gt;= 50 GB
- Docker &gt;= 24.0.0 &amp; Docker Compose &gt;= v2.26.1
- [gVisor](https://gvisor.dev/docs/user_guide/install/): Required only if you intend to use the code executor (sandbox) feature of RAGFlow.

&gt; [!TIP]
&gt; If you have not installed Docker on your local machine (Windows, Mac, or Linux), see [Install Docker Engine](https://docs.docker.com/engine/install/).

### üöÄ Start up the server

1. Ensure `vm.max_map_count` &gt;= 262144:

   &gt; To check the value of `vm.max_map_count`:
   &gt;
   &gt; ```bash
   &gt; $ sysctl vm.max_map_count
   &gt; ```
   &gt;
   &gt; Reset `vm.max_map_count` to a value at least 262144 if it is not.
   &gt;
   &gt; ```bash
   &gt; # In this case, we set it to 262144:
   &gt; $ sudo sysctl -w vm.max_map_count=262144
   &gt; ```
   &gt;
   &gt; This change will be reset after a system reboot. To ensure your change remains permanent, add or update the
   &gt; `vm.max_map_count` value in **/etc/sysctl.conf** accordingly:
   &gt;
   &gt; ```bash
   &gt; vm.max_map_count=262144
   &gt; ```

2. Clone the repo:

   ```bash
   $ git clone https://github.com/infiniflow/ragflow.git
   ```

3. Start up the server using the pre-built Docker images:

&gt; [!CAUTION]
&gt; All Docker images are built for x86 platforms. We don&#039;t currently offer Docker images for ARM64.
&gt; If you are on an ARM64 platform, follow [this guide](https://ragflow.io/docs/dev/build_docker_image) to build a Docker image compatible with your system.

   &gt; The command below downloads the `v0.19.0-slim` edition of the RAGFlow Docker image. See the following table for descriptions of different RAGFlow editions. To download a RAGFlow edition different from `v0.19.0-slim`, update the `RAGFLOW_IMAGE` variable accordingly in **docker/.env** before using `docker compose` to start the server. For example: set `RAGFLOW_IMAGE=infiniflow/ragflow:v0.19.0` for the full edition `v0.19.0`.

   ```bash
   $ cd ragflow/docker
   # Use CPU for embedding and DeepDoc tasks:
   $ docker compose -f docker-compose.yml up -d

   # To use GPU to accelerate embedding and DeepDoc tasks:
   # docker compose -f docker-compose-gpu.yml up -d
   ```

   | RAGFlow image tag | Image size (GB) | Has embedding models? | Stable?                  |
   |-------------------|-----------------|-----------------------|--------------------------|
   | v0.19.0           | &amp;approx;9       | :heavy_check_mark:    | Stable release           |
   | v0.19.0-slim      | &amp;approx;2       | ‚ùå                   | Stable release            |
   | nightly           | &amp;approx;9       | :heavy_check_mark:    | _Unstable_ nightly build |
   | nightly-slim      | &amp;approx;2       | ‚ùå                   | _Unstable_ nightly build  |

4. Check the server status after having the server up and running:

   ```bash
   $ docker logs -f ragflow-server
   ```

   _The following output confirms a successful launch of the system:_

   ```bash

         ____   ___    ______ ______ __
        / __ \ /   |  / ____// ____// /____  _      __
       / /_/ // /| | / / __ / /_   / // __ \| | /| / /
      / _, _// ___ |/ /_/ // __/  / // /_/ /| |/ |/ /
     /_/ |_|/_/  |_|\____//_/    /_/ \____/ |__/|__/

    * Running on all addresses (0.0.0.0)
   ```

   &gt; If you skip this confirmation step and directly log in to RAGFlow, your browser may prompt a `network anormal`
   &gt; error because, at that moment, your RAGFlow may not be fully initialized.

5. In your web browser, enter the IP address of your server and log in to RAGFlow.
   &gt; With the default settings, you only need to enter `http://IP_OF_YOUR_MACHINE` (**sans** port number) as the default
   &gt; HTTP serving port `80` can be omitted when using the default configurations.
6. In [service_conf.yaml.template](./docker/service_conf.yaml.template), select the desired LLM factory in `user_default_llm` and update
   the `API_KEY` field with the corresponding API key.

   &gt; See [llm_api_key_setup](https://ragflow.io/docs/dev/llm_api_key_setup) for more information.

   _The show is on!_

## üîß Configurations

When it comes to system configurations, you will need to manage the following files:

- [.env](./docker/.env): Keeps the fundamental setups for the system, such as `SVR_HTTP_PORT`, `MYSQL_PASSWORD`, and
  `MINIO_PASSWORD`.
- [service_conf.yaml.template](./docker/service_conf.yaml.template): Configures the back-end services. The environment variables in this file will be automatically populated when the Docker container starts. Any environment variables set within the Docker container will be available for use, allowing you to customize service behavior based on the deployment environment.
- [docker-compose.yml](./docker/docker-compose.yml): The system relies on [docker-compose.yml](./docker/docker-compose.yml) to start up.

&gt; The [./docker/README](./docker/README.md) file provides a detailed description of the environment settings and service
&gt; configurations which can be used as `${ENV_VARS}` in the [service_conf.yaml.template](./docker/service_conf.yaml.template) file.

To update the default HTTP serving port (80), go to [docker-compose.yml](./docker/docker-compose.yml) and change `80:80`
to `&lt;YOUR_SERVING_PORT&gt;:80`.

Updates to the above configurations require a reboot of all containers to take effect:

&gt; ```bash
&gt; $ docker compose -f docker-compose.yml up -d
&gt; ```

### Switch doc engine from Elasticsearch to Infinity

RAGFlow uses Elasticsearch by default for storing full text and vectors. To switch to [Infinity](https://github.com/infiniflow/infinity/), follow these steps:

1. Stop all running containers:

   ```bash
   $ docker compose -f docker/docker-compose.yml down -v
   ```

&gt; [!WARNING]
&gt; `-v` will delete the docker container volumes, and the existing data will be cleared.

2. Set `DOC_ENGINE` in **docker/.env** to `infinity`.

3. Start the containers:

   ```bash
   $ docker compose -f docker-compose.yml up -d
   ```

&gt; [!WARNING]
&gt; Switching to Infinity on a Linux/arm64 machine is not yet officially supported.

## üîß Build a Docker image without embedding models

This image is approximately 2 GB in size and relies on external LLM and embedding services.

```bash
git clone https://github.com/infiniflow/ragflow.git
cd ragflow/
docker build --platform linux/amd64 --build-arg LIGHTEN=1 -f Dockerfile -t infiniflow/ragflow:nightly-slim .
```

## üîß Build a Docker image including embedding models

This image is approximately 9 GB in size. As it includes embedding models, it relies on external LLM services only.

```bash
git clone https://github.com/infiniflow/ragflow.git
cd ragflow/
docker build --platform linux/amd64 -f Dockerfile -t infiniflow/ragflow:nightly .
```

## üî® Launch service from source for development

1. Install uv, or skip this step if it is already installed:

   ```bash
   pipx install uv pre-commit
   ```

2. Clone the source code and install Python dependencies:

   ```bash
   git clone https://github.com/infiniflow/ragflow.git
   cd ragflow/
   uv sync --python 3.10 --all-extras # install RAGFlow dependent python modules
   uv run download_deps.py
   pre-commit install
   ```

3. Launch the dependent services (MinIO, Elasticsearch, Redis, and MySQL) using Docker Compose:

   ```bash
   docker compose -f docker/docker-compose-base.yml up -d
   ```

   Add the following line to `/etc/hosts` to resolve all hosts specified in **docker/.env** to `127.0.0.1`:

   ```
   127.0.0.1       es01 infinity mysql minio redis sandbox-executor-manager
   ```

4. If you cannot access HuggingFace, set the `HF_ENDPOINT` environment variable to use a mirror site:

   ```bash
   export HF_ENDPOINT=https://hf-mirror.com
   ```

5. If your operating system does not have jemalloc, please install it as follows:

   ```bash
   # ubuntu
   sudo apt-get install libjemalloc-dev
   # centos
   sudo yum install jemalloc
   ```
   
6. Launch backend service:

   ```bash
   source .venv/bin/activate
   export PYTHONPATH=$(pwd)
   bash docker/launch_backend_service.sh
   ```

7. Install frontend dependencies:

   ```bash
   cd web
   npm install
   ```

8. Launch frontend service:

   ```bash
   npm run dev
   ```

   _The following output confirms a successful launch of the system:_

   ![](https://github.com/user-attachments/assets/0daf462c-a24d-4496-a66f-92533534e187)

9. Stop RAGFlow front-end and back-end service after development is complete:

   ```bash
   pkill -f &quot;ragflow_server.py|task_executor.py&quot;
   ```


## üìö Documentation

- [Quickstart](https://ragflow.io/docs/dev/)
- [Configuration](https://ragflow.io/docs/dev/configurations)
- [Release notes](https://ragflow.io/docs/dev/release_notes)
- [User guides](https://ragflow.io/docs/dev/category/guides)
- [Developer guides](https://ragflow.io/docs/dev/category/developers)
- [References](https://ragflow.io/docs/dev/category/references)
- [FAQs](https://ragflow.io/docs/dev/faq)

## üìú Roadmap

See the [RAGFlow Roadmap 2025](https://github.com/infiniflow/ragflow/issues/4214)

## üèÑ Community

- [Discord](https://discord.gg/NjYzJD3GM3)
- [Twitter](https://twitter.com/infiniflowai)
- [GitHub Discussions](https://github.com/orgs/infiniflow/discussions)

## üôå Contributing

RAGFlow flourishes via open-source collaboration. In this spirit, we embrace diverse contributions from the community.
If you would like to be a part, review our [Contribution Guidelines](https://ragflow.io/docs/dev/contributing) first.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[onnx/onnx]]></title>
            <link>https://github.com/onnx/onnx</link>
            <guid>https://github.com/onnx/onnx</guid>
            <pubDate>Sun, 08 Jun 2025 00:04:45 GMT</pubDate>
            <description><![CDATA[Open standard for machine learning interoperability]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/onnx/onnx">onnx/onnx</a></h1>
            <p>Open standard for machine learning interoperability</p>
            <p>Language: Python</p>
            <p>Stars: 19,066</p>
            <p>Forks: 3,750</p>
            <p>Stars today: 8 stars today</p>
            <h2>README</h2><pre>&lt;!--
Copyright (c) ONNX Project Contributors

SPDX-License-Identifier: Apache-2.0
--&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img width=&quot;40%&quot; src=&quot;https://github.com/onnx/onnx/raw/main/docs/onnx-horizontal-color.png&quot; /&gt;&lt;/p&gt;

[![PyPI - Version](https://img.shields.io/pypi/v/onnx.svg)](https://pypi.org/project/onnx)
[![CI](https://github.com/onnx/onnx/actions/workflows/main.yml/badge.svg)](https://github.com/onnx/onnx/actions/workflows/main.yml)
[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3313/badge)](https://bestpractices.coreinfrastructure.org/projects/3313)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/onnx/onnx/badge)](https://api.securityscorecards.dev/projects/github.com/onnx/onnx)
[![REUSE compliant](https://api.reuse.software/badge/github.com/onnx/onnx)](https://api.reuse.software/info/github.com/onnx/onnx)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
[![Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

[Open Neural Network Exchange (ONNX)](https://onnx.ai) is an open ecosystem that empowers AI developers
to choose the right tools as their project evolves. ONNX provides an open source format for AI models, both deep learning and traditional ML. It defines an extensible computation graph model, as well as definitions of built-in operators and standard
data types. Currently we focus on the capabilities needed for inferencing (scoring).

ONNX is [widely supported](http://onnx.ai/supported-tools) and can be found in many frameworks, tools, and hardware. Enabling interoperability between different frameworks and streamlining the path from research to production helps increase the speed of innovation in the AI community. We invite the community to join us and further evolve ONNX.

# Use ONNX

* [Documentation of ONNX Python Package](https://onnx.ai/onnx/)
* [Tutorials for creating ONNX models](https://github.com/onnx/tutorials)
* [Pre-trained ONNX models](https://github.com/onnx/models)

# Learn about the ONNX spec

* [Overview](https://github.com/onnx/onnx/blob/main/docs/Overview.md)
* [ONNX intermediate representation spec](https://github.com/onnx/onnx/blob/main/docs/IR.md)
* [Versioning principles of the spec](https://github.com/onnx/onnx/blob/main/docs/Versioning.md)
* [Operators documentation](https://github.com/onnx/onnx/blob/main/docs/Operators.md)
* [Operators documentation](https://onnx.ai/onnx/operators/index.html) (latest release)
* [Python API Overview](https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md)

# Programming utilities for working with ONNX Graphs

* [Shape and Type Inference](https://github.com/onnx/onnx/blob/main/docs/ShapeInference.md)
* [Graph Optimization](https://github.com/onnx/optimizer)
* [Opset Version Conversion](https://github.com/onnx/onnx/blob/main/docs/docsgen/source/api/version_converter.md)

# Contribute

ONNX is a community project and the open governance model is described [here](https://github.com/onnx/onnx/blob/main/community/readme.md). We encourage you to join the effort and contribute feedback, ideas, and code. You can participate in the [Special Interest Groups](https://github.com/onnx/onnx/blob/main/community/sigs.md) and [Working Groups](https://github.com/onnx/onnx/blob/main/community/working-groups.md) to shape the future of ONNX.

Check out our [contribution guide](https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md) to get started.

If you think some operator should be added to ONNX specification, please read
[this document](https://github.com/onnx/onnx/blob/main/docs/AddNewOp.md).

# Community meetings

The schedules of the regular meetings of the Steering Committee, the working groups and the SIGs can be found [here](https://onnx.ai/calendar)

Community Meetups are held at least once a year. Content from previous community meetups are at:

* 2020.04.09 &lt;https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14091402/LF+AI+Day+-ONNX+Community+Virtual+Meetup+-+Silicon+Valley+-+2020+April+9&gt;
* 2020.10.14 &lt;https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14092138/LF+AI+Day+-+ONNX+Community+Workshop+-+2020+October+14&gt;
* 2021.03.24 &lt;https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14092424/Instructions+for+Event+Hosts+-+LF+AI+Data+Day+-+ONNX+Virtual+Community+Meetup+-+March+2021&gt;
* 2021.10.21 &lt;https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14093194/LF+AI+Data+Day+ONNX+Community+Virtual+Meetup+-+October+2021&gt;
* 2022.06.24 &lt;https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14093969/ONNX+Community+Day+-+2022+June+24&gt;
* 2023.06.28 &lt;https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14094507/ONNX+Community+Day+2023+-+June+28&gt;



# Discuss

We encourage you to open [Issues](https://github.com/onnx/onnx/issues), or use [Slack](https://lfaifoundation.slack.com/) (If you have not joined yet, please use this [link](https://join.slack.com/t/lfaifoundation/shared_invite/zt-o65errpw-gMTbwNr7FnNbVXNVFkmyNA) to join the group) for more real-time discussion.

# Follow Us

Stay up to date with the latest ONNX news. [[Facebook](https://www.facebook.com/onnxai/)] [[Twitter](https://twitter.com/onnxai)]

# Roadmap

A roadmap process takes place every year. More details can be found [here](https://github.com/onnx/steering-committee/tree/main/roadmap)

# Installation

ONNX released packages are published in PyPi.

```sh
pip install onnx # or pip install onnx[reference] for optional reference implementation dependencies
```

[ONNX weekly packages](https://pypi.org/project/onnx-weekly/) are published in PyPI to enable experimentation and early testing.

Detailed install instructions, including Common Build Options and Common Errors can be found [here](https://github.com/onnx/onnx/blob/main/INSTALL.md)

# Testing

ONNX uses [pytest](https://docs.pytest.org) as test driver. In order to run tests, you will first need to install `pytest`:

```sh
pip install pytest
```

After installing pytest, use the following command to run tests.

```sh
pytest
```

# Development

Check out the [contributor guide](https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md) for instructions.

# License

[Apache License v2.0](LICENSE)

# Code of Conduct

[ONNX Open Source Code of Conduct](https://onnx.ai/codeofconduct.html)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[lastmile-ai/mcp-agent]]></title>
            <link>https://github.com/lastmile-ai/mcp-agent</link>
            <guid>https://github.com/lastmile-ai/mcp-agent</guid>
            <pubDate>Sun, 08 Jun 2025 00:04:44 GMT</pubDate>
            <description><![CDATA[Build effective agents using Model Context Protocol and simple workflow patterns]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lastmile-ai/mcp-agent">lastmile-ai/mcp-agent</a></h1>
            <p>Build effective agents using Model Context Protocol and simple workflow patterns</p>
            <p>Language: Python</p>
            <p>Stars: 5,411</p>
            <p>Forks: 495</p>
            <p>Stars today: 183 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/c8d059e5-bd56-4ea2-a72d-807fb4897bde&quot; alt=&quot;Logo&quot; width=&quot;300&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;em&gt;Build effective agents with Model Context Protocol using simple, composable patterns.&lt;/em&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/lastmile-ai/mcp-agent/tree/main/examples&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/a&gt;
  |
  &lt;a href=&quot;https://www.anthropic.com/research/building-effective-agents&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Building Effective Agents&lt;/strong&gt;&lt;/a&gt;
  |
  &lt;a href=&quot;https://modelcontextprotocol.io/introduction&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;MCP&lt;/strong&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://pypi.org/project/mcp-agent/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/mcp-agent?color=%2334D058&amp;label=pypi&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/lastmile-ai/mcp-agent/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-raw/lastmile-ai/mcp-agent&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://lmai.link/discord/mcp-agent&quot;&gt;&lt;img src=&quot;https://shields.io/discord/1089284610329952357&quot; alt=&quot;discord&quot; /&gt;&lt;/a&gt;
&lt;img alt=&quot;Pepy Total Downloads&quot; src=&quot;https://img.shields.io/pepy/dt/mcp-agent?label=pypi%20%7C%20downloads&quot;/&gt;
&lt;a href=&quot;https://github.com/lastmile-ai/mcp-agent/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/l/mcp-agent&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;

## Overview

**`mcp-agent`** is a simple, composable framework to build agents using [Model Context Protocol](https://modelcontextprotocol.io/introduction).

**Inspiration**: Anthropic announced 2 foundational updates for AI application developers:

1. [Model Context Protocol](https://www.anthropic.com/news/model-context-protocol) - a standardized interface to let any software be accessible to AI assistants via MCP servers.
2. [Building Effective Agents](https://www.anthropic.com/research/building-effective-agents) - a seminal writeup on simple, composable patterns for building production-ready AI agents.

`mcp-agent` puts these two foundational pieces into an AI application framework:

1. It handles the pesky business of managing the lifecycle of MCP server connections so you don&#039;t have to.
2. It implements every pattern described in Building Effective Agents, and does so in a _composable_ way, allowing you to chain these patterns together.
3. **Bonus**: It implements [OpenAI&#039;s Swarm](https://github.com/openai/swarm) pattern for multi-agent orchestration, but in a model-agnostic way.

Altogether, this is the simplest and easiest way to build robust agent applications. Much like MCP, this project is in early development.
We welcome all kinds of [contributions](/CONTRIBUTING.md), feedback and your help in growing this to become a new standard.

## Get Started

We recommend using [uv](https://docs.astral.sh/uv/) to manage your Python projects:

```bash
uv add &quot;mcp-agent&quot;
```

Alternatively:

```bash
pip install mcp-agent
```

### Quickstart

&gt; [!TIP]
&gt; The [`examples`](/examples) directory has several example applications to get started with.
&gt; To run an example, clone this repo, then:
&gt;
&gt; ```bash
&gt; cd examples/basic/mcp_basic_agent # Or any other example
&gt; cp mcp_agent.secrets.yaml.example mcp_agent.secrets.yaml # Update API keys
&gt; uv run main.py
&gt; ```

Here is a basic &quot;finder&quot; agent that uses the fetch and filesystem servers to look up a file, read a blog and write a tweet. [Example link](./examples/basic/mcp_basic_agent/):

&lt;details open&gt;
&lt;summary&gt;finder_agent.py&lt;/summary&gt;

```python
import asyncio
import os

from mcp_agent.app import MCPApp
from mcp_agent.agents.agent import Agent
from mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM

app = MCPApp(name=&quot;hello_world_agent&quot;)

async def example_usage():
    async with app.run() as mcp_agent_app:
        logger = mcp_agent_app.logger
        # This agent can read the filesystem or fetch URLs
        finder_agent = Agent(
            name=&quot;finder&quot;,
            instruction=&quot;&quot;&quot;You can read local files or fetch URLs.
                Return the requested information when asked.&quot;&quot;&quot;,
            server_names=[&quot;fetch&quot;, &quot;filesystem&quot;], # MCP servers this Agent can use
        )

        async with finder_agent:
            # Automatically initializes the MCP servers and adds their tools for LLM use
            tools = await finder_agent.list_tools()
            logger.info(f&quot;Tools available:&quot;, data=tools)

            # Attach an OpenAI LLM to the agent (defaults to GPT-4o)
            llm = await finder_agent.attach_llm(OpenAIAugmentedLLM)

            # This will perform a file lookup and read using the filesystem server
            result = await llm.generate_str(
                message=&quot;Show me what&#039;s in README.md verbatim&quot;
            )
            logger.info(f&quot;README.md contents: {result}&quot;)

            # Uses the fetch server to fetch the content from URL
            result = await llm.generate_str(
                message=&quot;Print the first two paragraphs from https://www.anthropic.com/research/building-effective-agents&quot;
            )
            logger.info(f&quot;Blog intro: {result}&quot;)

            # Multi-turn interactions by default
            result = await llm.generate_str(&quot;Summarize that in a 128-char tweet&quot;)
            logger.info(f&quot;Tweet: {result}&quot;)

if __name__ == &quot;__main__&quot;:
    asyncio.run(example_usage())

```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;mcp_agent.config.yaml&lt;/summary&gt;

```yaml
execution_engine: asyncio
logger:
  transports: [console] # You can use [file, console] for both
  level: debug
  path: &quot;logs/mcp-agent.jsonl&quot; # Used for file transport
  # For dynamic log filenames:
  # path_settings:
  #   path_pattern: &quot;logs/mcp-agent-{unique_id}.jsonl&quot;
  #   unique_id: &quot;timestamp&quot;  # Or &quot;session_id&quot;
  #   timestamp_format: &quot;%Y%m%d_%H%M%S&quot;

mcp:
  servers:
    fetch:
      command: &quot;uvx&quot;
      args: [&quot;mcp-server-fetch&quot;]
    filesystem:
      command: &quot;npx&quot;
      args:
        [
          &quot;-y&quot;,
          &quot;@modelcontextprotocol/server-filesystem&quot;,
          &quot;&lt;add_your_directories&gt;&quot;,
        ]

openai:
  # Secrets (API keys, etc.) are stored in an mcp_agent.secrets.yaml file which can be gitignored
  default_model: gpt-4o
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Agent output&lt;/summary&gt;
&lt;img width=&quot;2398&quot; alt=&quot;Image&quot; src=&quot;https://github.com/user-attachments/assets/eaa60fdf-bcc6-460b-926e-6fa8534e9089&quot; /&gt;
&lt;/details&gt;

## Table of Contents

- [Why use mcp-agent?](#why-use-mcp-agent)
- [Example Applications](#examples)
  - [Claude Desktop](#claude-desktop)
  - [Streamlit](#streamlit)
    - [Gmail Agent](#gmail-agent)
    - [RAG](#simple-rag-chatbot)
  - [Marimo](#marimo)
  - [Python](#python)
    - [Swarm (CLI)](#swarm)
- [Core Concepts](#core-components)
- [Workflows Patterns](#workflows)
  - [Augmented LLM](#augmentedllm)
  - [Parallel](#parallel)
  - [Router](#router)
  - [Intent-Classifier](#intentclassifier)
  - [Orchestrator-Workers](#orchestrator-workers)
  - [Evaluator-Optimizer](#evaluator-optimizer)
  - [OpenAI Swarm](#swarm-1)
- [Advanced](#advanced)
  - [Composing multiple workflows](#composability)
  - [Signaling and Human input](#signaling-and-human-input)
  - [App Config](#app-config)
  - [MCP Server Management](#mcp-server-management)
- [Contributing](#contributing)
- [Roadmap](#roadmap)
- [FAQs](#faqs)

## Why use `mcp-agent`?

There are too many AI frameworks out there already. But `mcp-agent` is the only one that is purpose-built for a shared protocol - [MCP](https://modelcontextprotocol.io/introduction). It is also the most lightweight, and is closer to an agent pattern library than a framework.

As [more services become MCP-aware](https://github.com/punkpeye/awesome-mcp-servers), you can use mcp-agent to build robust and controllable AI agents that can leverage those services out-of-the-box.

## Examples

Before we go into the core concepts of mcp-agent, let&#039;s show what you can build with it.

In short, you can build any kind of AI application with mcp-agent: multi-agent collaborative workflows, human-in-the-loop workflows, RAG pipelines and more.

### Claude Desktop

You can integrate mcp-agent apps into MCP clients like Claude Desktop.

#### mcp-agent server

This app wraps an mcp-agent application inside an MCP server, and exposes that server to Claude Desktop.
The app exposes agents and workflows that Claude Desktop can invoke to service of the user&#039;s request.

https://github.com/user-attachments/assets/7807cffd-dba7-4f0c-9c70-9482fd7e0699

This demo shows a multi-agent evaluation task where each agent evaluates aspects of an input poem, and
then an aggregator summarizes their findings into a final response.

**Details**: Starting from a user&#039;s request over text, the application:

- dynamically defines agents to do the job
- uses the appropriate workflow to orchestrate those agents (in this case the Parallel workflow)

**Link to code**: [examples/basic/mcp_agent_server](./examples/basic/mcp_agent_server)

&gt; [!NOTE]
&gt; Huge thanks to [Jerron Lim (@StreetLamb)](https://github.com/StreetLamb)
&gt; for developing and contributing this example!

### Streamlit

You can deploy mcp-agent apps using Streamlit.

#### Gmail agent

This app is able to perform read and write actions on gmail using text prompts -- i.e. read, delete, send emails, mark as read/unread, etc.
It uses an MCP server for Gmail.

https://github.com/user-attachments/assets/54899cac-de24-4102-bd7e-4b2022c956e3

**Link to code**: [gmail-mcp-server](https://github.com/jasonsum/gmail-mcp-server/blob/add-mcp-agent-streamlit/streamlit_app.py)

&gt; [!NOTE]
&gt; Huge thanks to [Jason Summer (@jasonsum)](https://github.com/jasonsum)
&gt; for developing and contributing this example!

#### Simple RAG Chatbot

This app uses a Qdrant vector database (via an MCP server) to do Q&amp;A over a corpus of text.

https://github.com/user-attachments/assets/f4dcd227-cae9-4a59-aa9e-0eceeb4acaf4

**Link to code**: [examples/usecases/streamlit_mcp_rag_agent](./examples/usecases/streamlit_mcp_rag_agent/)

&gt; [!NOTE]
&gt; Huge thanks to [Jerron Lim (@StreetLamb)](https://github.com/StreetLamb)
&gt; for developing and contributing this example!

### Marimo

[Marimo](https://github.com/marimo-team/marimo) is a reactive Python notebook that replaces Jupyter and Streamlit.
Here&#039;s the &quot;file finder&quot; agent from [Quickstart](#quickstart) implemented in Marimo:

&lt;img src=&quot;https://github.com/user-attachments/assets/139a95a5-e3ac-4ea7-9c8f-bad6577e8597&quot; width=&quot;400&quot;/&gt;

**Link to code**: [examples/usecases/marimo_mcp_basic_agent](./examples/usecases/marimo_mcp_basic_agent/)

&gt; [!NOTE]
&gt; Huge thanks to [Akshay Agrawal (@akshayka)](https://github.com/akshayka)
&gt; for developing and contributing this example!

### Python

You can write mcp-agent apps as Python scripts or Jupyter notebooks.

#### Swarm

This example demonstrates a multi-agent setup for handling different customer service requests in an airline context using the Swarm workflow pattern. The agents can triage requests, handle flight modifications, cancellations, and lost baggage cases.

https://github.com/user-attachments/assets/b314d75d-7945-4de6-965b-7f21eb14a8bd

**Link to code**: [examples/workflows/workflow_swarm](./examples/workflows/workflow_swarm/)

## Core Components

The following are the building blocks of the mcp-agent framework:

- **[MCPApp](./src/mcp_agent/app.py)**: global state and app configuration
- **MCP server management**: [`gen_client`](./src/mcp_agent/mcp/gen_client.py) and [`MCPConnectionManager`](./src/mcp_agent/mcp/mcp_connection_manager.py) to easily connect to MCP servers.
- **[Agent](./src/mcp_agent/agents/agent.py)**: An Agent is an entity that has access to a set of MCP servers and exposes them to an LLM as tool calls. It has a name and purpose (instruction).
- **[AugmentedLLM](./src/mcp_agent/workflows/llm/augmented_llm.py)**: An LLM that is enhanced with tools provided from a collection of MCP servers. Every Workflow pattern described below is an `AugmentedLLM` itself, allowing you to compose and chain them together.

Everything in the framework is a derivative of these core capabilities.

## Workflows

mcp-agent provides implementations for every pattern in Anthropic‚Äôs [Building Effective Agents](https://www.anthropic.com/research/building-effective-agents), as well as the OpenAI [Swarm](https://github.com/openai/swarm) pattern.
Each pattern is model-agnostic, and exposed as an `AugmentedLLM`, making everything very composable.

### AugmentedLLM

[AugmentedLLM](./src/mcp_agent/workflows/llm/augmented_llm.py) is an LLM that has access to MCP servers and functions via Agents.

LLM providers implement the AugmentedLLM interface to expose 3 functions:

- `generate`: Generate message(s) given a prompt, possibly over multiple iterations and making tool calls as needed.
- `generate_str`: Calls `generate` and returns result as a string output.
- `generate_structured`: Uses [Instructor](https://github.com/instructor-ai/instructor) to return the generated result as a Pydantic model.

Additionally, `AugmentedLLM` has memory, to keep track of long or short-term history.

&lt;details&gt;
&lt;summary&gt;Example&lt;/summary&gt;

```python
from mcp_agent.agents.agent import Agent
from mcp_agent.workflows.llm.augmented_llm_anthropic import AnthropicAugmentedLLM

finder_agent = Agent(
    name=&quot;finder&quot;,
    instruction=&quot;You are an agent with filesystem + fetch access. Return the requested file or URL contents.&quot;,
    server_names=[&quot;fetch&quot;, &quot;filesystem&quot;],
)

async with finder_agent:
   llm = await finder_agent.attach_llm(AnthropicAugmentedLLM)

   result = await llm.generate_str(
      message=&quot;Print the first 2 paragraphs of https://www.anthropic.com/research/building-effective-agents&quot;,
      # Can override model, tokens and other defaults
   )
   logger.info(f&quot;Result: {result}&quot;)

   # Multi-turn conversation
   result = await llm.generate_str(
      message=&quot;Summarize those paragraphs in a 128 character tweet&quot;,
   )
   logger.info(f&quot;Result: {result}&quot;)
```

&lt;/details&gt;

### [Parallel](src/mcp_agent/workflows/parallel/parallel_llm.py)

![Parallel workflow (Image credit: Anthropic)](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F406bb032ca007fd1624f261af717d70e6ca86286-2401x1000.png&amp;w=3840&amp;q=75)

Fan-out tasks to multiple sub-agents and fan-in the results. Each subtask is an AugmentedLLM, as is the overall Parallel workflow, meaning each subtask can optionally be a more complex workflow itself.

&gt; [!NOTE]
&gt;
&gt; **[Link to full example](examples/workflows/workflow_parallel/main.py)**

&lt;details&gt;
&lt;summary&gt;Example&lt;/summary&gt;

```python
proofreader = Agent(name=&quot;proofreader&quot;, instruction=&quot;Review grammar...&quot;)
fact_checker = Agent(name=&quot;fact_checker&quot;, instruction=&quot;Check factual consistency...&quot;)
style_enforcer = Agent(name=&quot;style_enforcer&quot;, instruction=&quot;Enforce style guidelines...&quot;)

grader = Agent(name=&quot;grader&quot;, instruction=&quot;Combine feedback into a structured report.&quot;)

parallel = ParallelLLM(
    fan_in_agent=grader,
    fan_out_agents=[proofreader, fact_checker, style_enforcer],
    llm_factory=OpenAIAugmentedLLM,
)

result = await parallel.generate_str(&quot;Student short story submission: ...&quot;, RequestParams(model=&quot;gpt4-o&quot;))
```

&lt;/details&gt;

### [Router](src/mcp_agent/workflows/router/)

![Router workflow (Image credit: Anthropic)](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F5c0c0e9fe4def0b584c04d37849941da55e5e71c-2401x1000.png&amp;w=3840&amp;q=75)

Given an input, route to the `top_k` most relevant categories. A category can be an Agent, an MCP server or a regular function.

mcp-agent provides several router implementations, including:

- [`EmbeddingRouter`](src/mcp_agent/workflows/router/router_embedding.py): uses embedding models for classification
- [`LLMRouter`](src/mcp_agent/workflows/router/router_llm.py): uses LLMs for classification

&gt; [!NOTE]
&gt;
&gt; **[Link to full example](examples/workflows/workflow_router/main.py)**

&lt;details&gt;
&lt;summary&gt;Example&lt;/summary&gt;

```python
def print_hello_world:
     print(&quot;Hello, world!&quot;)

finder_agent = Agent(name=&quot;finder&quot;, server_names=[&quot;fetch&quot;, &quot;filesystem&quot;])
writer_agent = Agent(name=&quot;writer&quot;, server_names=[&quot;filesystem&quot;])

llm = OpenAIAugmentedLLM()
router = LLMRouter(
    llm=llm,
    agents=[finder_agent, writer_agent],
    functions=[print_hello_world],
)

results = await router.route( # Also available: route_to_agent, route_to_server
    request=&quot;Find and print the contents of README.md verbatim&quot;,
    top_k=1
)
chosen_agent = results[0].result
async with chosen_agent:
    ...
```

&lt;/details&gt;

### [IntentClassifier](src/mcp_agent/workflows/intent_classifier/)

A close sibling of Router, the Intent Classifier pattern identifies the `top_k` Intents that most closely match a given input.
Just like a Router, mcp-agent provides both an [embedding](src/mcp_agent/workflows/intent_classifier/intent_classifier_embedding.py) and [LLM-based](src/mcp_agent/workflows/intent_classifier/intent_classifier_llm.py) intent classifier.

### [Evaluator-Optimizer](src/mcp_agent/workflows/evaluator_optimizer/evaluator_optimizer.py)

![Evaluator-optimizer workflow (Image credit: Anthropic)](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F14f51e6406ccb29e695da48b17017e899a6119c7-2401x1000.png&amp;w=3840&amp;q=75)

One LLM (the ‚Äúoptimizer‚Äù) refines a response, another (the ‚Äúevaluator‚Äù) critiques it until a response exceeds a quality criteria.

&gt; [!NOTE]
&gt;
&gt; **[Link to full example](examples/workflows/workflow_evaluator_optimizer/main.py)**

&lt;details&gt;
&lt;summary&gt;Example&lt;/summary&gt;

```python
optimizer = Agent(name=&quot;cover_letter_writer&quot;, server_names=[&quot;fetch&quot;], instruction=&quot;Generate a cover letter ...&quot;)
evaluator = Agent(name=&quot;critiquer&quot;, instruction=&quot;Evaluate clarity, specificity, relevance...&quot;)

llm = EvaluatorOptimizerLLM(
    optimizer=optimizer,
    evaluator=evaluator,
    llm_factory=OpenAIAugmentedLLM,
    min_rating=QualityRating.EXCELLENT, # Keep iterating until the minimum quality bar is reached
)

result = await eo_llm.generate_str(&quot;Write a job cover letter for an AI framework developer role at LastMile AI.&quot;)
print(&quot;Final refined cover letter:&quot;, result)
```

&lt;/details&gt;

### [Orchestrator-workers](src/mcp_agent/workflows/orchestrator/orchestrator.py)

![Orchestrator workflow (Image credit: Anthropic)](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F8985fc683fae4780fb34eab1365ab78c7e51bc8e-2401x1000.png&amp;w=3840&amp;q=75)

A higher-level LLM generates a plan, then assigns them to sub-agents, and synthesizes the results.
The Orchestrator workflow automatically parallelizes steps that can be done in parallel, and blocks on dependencies.

&gt; [!NOTE]
&gt;
&gt; **[Link to full example](examples/workflows/workflow_orchestrator_worker/main.py)**

&lt;details&gt;
&lt;summary&gt;Example&lt;/summary&gt;

```python
finder_agent = Agent(name=&quot;finder&quot;, server_names=[&quot;fetch&quot;, &quot;filesystem&quot;])
writer_agent = Agent(name=&quot;writer&quot;, server_names=[&quot;filesystem&quot;])
proofreader = Agent(name=&quot;proofreader&quot;, ...)
fact_checker = Agent(name=&quot;fact_checker&quot;, ...)
style_enforcer = Agent(name=&quot;style_enforcer&quot;, instructions=&quot;Use APA style guide from ...&quot;, server_names=[&quot;fetch&quot;])

orchestrator = Orchestrator(
    llm_factory=AnthropicAugmentedLLM,
    available_agents=[finder_agent, writer_agent, proofreader, fact_checker, style_enforcer],
)

task = &quot;Load short_story.md, evaluate it, produce a graded_report.md with multiple feedback aspects.&quot;
result = await orchestrator.generate_str(task, RequestParams(model=&quot;gpt-4o&quot;))
print(result)
```

&lt;/details&gt;

### [Swarm](src/mcp_agent/workflows/swarm/swarm.py)

OpenAI has an experimental multi-agent pattern called [Swarm](https://github.com/openai/swarm), which we provide a model-agnostic reference implementation for in mcp-agent.

&lt;img src=&quot;https://github.com/openai/swarm/blob/main/assets/swarm_diagram.png?raw=true&quot; width=500 /&gt;

The mcp-agent Swarm pattern works seamlessly with MCP servers, and is exposed as an `AugmentedLLM`, allowing for composability with other patterns above

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[huggingface/lerobot]]></title>
            <link>https://github.com/huggingface/lerobot</link>
            <guid>https://github.com/huggingface/lerobot</guid>
            <pubDate>Sun, 08 Jun 2025 00:04:43 GMT</pubDate>
            <description><![CDATA[ü§ó LeRobot: Making AI for Robotics more accessible with end-to-end learning]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/huggingface/lerobot">huggingface/lerobot</a></h1>
            <p>ü§ó LeRobot: Making AI for Robotics more accessible with end-to-end learning</p>
            <p>Language: Python</p>
            <p>Stars: 14,162</p>
            <p>Forks: 1,766</p>
            <p>Stars today: 41 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;media/lerobot-logo-thumbnail.png&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;media/lerobot-logo-thumbnail.png&quot;&gt;
    &lt;img alt=&quot;LeRobot, Hugging Face Robotics Library&quot; src=&quot;media/lerobot-logo-thumbnail.png&quot; style=&quot;max-width: 100%;&quot;&gt;
  &lt;/picture&gt;
  &lt;br/&gt;
  &lt;br/&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

[![Tests](https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml/badge.svg?branch=main)](https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml?query=branch%3Amain)
[![Coverage](https://codecov.io/gh/huggingface/lerobot/branch/main/graph/badge.svg?token=TODO)](https://codecov.io/gh/huggingface/lerobot)
[![Python versions](https://img.shields.io/pypi/pyversions/lerobot)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/huggingface/lerobot/blob/main/LICENSE)
[![Status](https://img.shields.io/pypi/status/lerobot)](https://pypi.org/project/lerobot/)
[![Version](https://img.shields.io/pypi/v/lerobot)](https://pypi.org/project/lerobot/)
[![Examples](https://img.shields.io/badge/Examples-green.svg)](https://github.com/huggingface/lerobot/tree/main/examples)
[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.1%20adopted-ff69b4.svg)](https://github.com/huggingface/lerobot/blob/main/CODE_OF_CONDUCT.md)
[![Discord](https://dcbadge.vercel.app/api/server/C5P34WJ68S?style=flat)](https://discord.gg/s3KuuzsPFb)

&lt;/div&gt;

&lt;h2 align=&quot;center&quot;&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/huggingface/lerobot/blob/main/examples/12_use_so101.md&quot;&gt;
        Build Your Own SO-101 Robot!&lt;/a&gt;&lt;/p&gt;
&lt;/h2&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;div style=&quot;display: flex; gap: 1rem; justify-content: center; align-items: center;&quot; &gt;
    &lt;img
      src=&quot;media/so101/so101.webp?raw=true&quot;
      alt=&quot;SO-101 follower arm&quot;
      title=&quot;SO-101 follower arm&quot;
      style=&quot;width: 40%;&quot;
    /&gt;
    &lt;img
      src=&quot;media/so101/so101-leader.webp?raw=true&quot;
      alt=&quot;SO-101 leader arm&quot;
      title=&quot;SO-101 leader arm&quot;
      style=&quot;width: 40%;&quot;
    /&gt;
  &lt;/div&gt;


  &lt;p&gt;&lt;strong&gt;Meet the updated SO100, the SO-101 ‚Äì Just ‚Ç¨114 per arm!&lt;/strong&gt;&lt;/p&gt;
  &lt;p&gt;Train it in minutes with a few simple moves on your laptop.&lt;/p&gt;
  &lt;p&gt;Then sit back and watch your creation act autonomously! ü§Ø&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;https://github.com/huggingface/lerobot/blob/main/examples/12_use_so101.md&quot;&gt;
      See the full SO-101 tutorial here.&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;Want to take it to the next level? Make your SO-101 mobile by building LeKiwi!&lt;/p&gt;
  &lt;p&gt;Check out the &lt;a href=&quot;https://github.com/huggingface/lerobot/blob/main/examples/11_use_lekiwi.md&quot;&gt;LeKiwi tutorial&lt;/a&gt; and bring your robot to life on wheels.&lt;/p&gt;

  &lt;img src=&quot;media/lekiwi/kiwi.webp?raw=true&quot; alt=&quot;LeKiwi mobile robot&quot; title=&quot;LeKiwi mobile robot&quot; width=&quot;50%&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;

&lt;h3 align=&quot;center&quot;&gt;
    &lt;p&gt;LeRobot: State-of-the-art AI for real-world robotics&lt;/p&gt;
&lt;/h3&gt;

---

ü§ó LeRobot aims to provide models, datasets, and tools for real-world robotics in PyTorch. The goal is to lower the barrier to entry to robotics so that everyone can contribute and benefit from sharing datasets and pretrained models.

ü§ó LeRobot contains state-of-the-art approaches that have been shown to transfer to the real-world with a focus on imitation learning and reinforcement learning.

ü§ó LeRobot already provides a set of pretrained models, datasets with human collected demonstrations, and simulation environments to get started without assembling a robot. In the coming weeks, the plan is to add more and more support for real-world robotics on the most affordable and capable robots out there.

ü§ó LeRobot hosts pretrained models and datasets on this Hugging Face community page: [huggingface.co/lerobot](https://huggingface.co/lerobot)

#### Examples of pretrained models on simulation environments

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;img src=&quot;media/gym/aloha_act.gif&quot; width=&quot;100%&quot; alt=&quot;ACT policy on ALOHA env&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;media/gym/simxarm_tdmpc.gif&quot; width=&quot;100%&quot; alt=&quot;TDMPC policy on SimXArm env&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;media/gym/pusht_diffusion.gif&quot; width=&quot;100%&quot; alt=&quot;Diffusion policy on PushT env&quot;/&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;ACT policy on ALOHA env&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;TDMPC policy on SimXArm env&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;Diffusion policy on PushT env&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

### Acknowledgment

- Thanks to Tony Zhao, Zipeng Fu and colleagues for open sourcing ACT policy, ALOHA environments and datasets. Ours are adapted from [ALOHA](https://tonyzhaozh.github.io/aloha) and [Mobile ALOHA](https://mobile-aloha.github.io).
- Thanks to Cheng Chi, Zhenjia Xu and colleagues for open sourcing Diffusion policy, Pusht environment and datasets, as well as UMI datasets. Ours are adapted from [Diffusion Policy](https://diffusion-policy.cs.columbia.edu) and [UMI Gripper](https://umi-gripper.github.io).
- Thanks to Nicklas Hansen, Yunhai Feng and colleagues for open sourcing TDMPC policy, Simxarm environments and datasets. Ours are adapted from [TDMPC](https://github.com/nicklashansen/tdmpc) and [FOWM](https://www.yunhaifeng.com/FOWM).
- Thanks to Antonio Loquercio and Ashish Kumar for their early support.
- Thanks to [Seungjae (Jay) Lee](https://sjlee.cc/), [Mahi Shafiullah](https://mahis.life/) and colleagues for open sourcing [VQ-BeT](https://sjlee.cc/vq-bet/) policy and helping us adapt the codebase to our repository. The policy is adapted from [VQ-BeT repo](https://github.com/jayLEE0301/vq_bet_official).


## Installation

Download our source code:
```bash
git clone https://github.com/huggingface/lerobot.git
cd lerobot
```

Create a virtual environment with Python 3.10 and activate it, e.g. with [`miniconda`](https://docs.anaconda.com/free/miniconda/index.html):
```bash
conda create -y -n lerobot python=3.10
conda activate lerobot
```

When using `miniconda`, install `ffmpeg` in your environment:
```bash
conda install ffmpeg -c conda-forge
```

&gt; **NOTE:** This usually installs `ffmpeg 7.X` for your platform compiled with the `libsvtav1` encoder. If `libsvtav1` is not supported (check supported encoders with `ffmpeg -encoders`), you can:
&gt;  - _[On any platform]_ Explicitly install `ffmpeg 7.X` using:
&gt;  ```bash
&gt;  conda install ffmpeg=7.1.1 -c conda-forge
&gt;  ```
&gt;  - _[On Linux only]_ Install [ffmpeg build dependencies](https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu#GettheDependencies) and [compile ffmpeg from source with libsvtav1](https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu#libsvtav1), and make sure you use the corresponding ffmpeg binary to your install with `which ffmpeg`.

Install ü§ó LeRobot:
```bash
pip install -e .
```

&gt; **NOTE:** If you encounter build errors, you may need to install additional dependencies (`cmake`, `build-essential`, and `ffmpeg libs`). On Linux, run:
`sudo apt-get install cmake build-essential python3-dev pkg-config libavformat-dev libavcodec-dev libavdevice-dev libavutil-dev libswscale-dev libswresample-dev libavfilter-dev pkg-config`. For other systems, see: [Compiling PyAV](https://pyav.org/docs/develop/overview/installation.html#bring-your-own-ffmpeg)

For simulations, ü§ó LeRobot comes with gymnasium environments that can be installed as extras:
- [aloha](https://github.com/huggingface/gym-aloha)
- [xarm](https://github.com/huggingface/gym-xarm)
- [pusht](https://github.com/huggingface/gym-pusht)

For instance, to install ü§ó LeRobot with aloha and pusht, use:
```bash
pip install -e &quot;.[aloha, pusht]&quot;
```

To use [Weights and Biases](https://docs.wandb.ai/quickstart) for experiment tracking, log in with
```bash
wandb login
```

(note: you will also need to enable WandB in the configuration. See below.)

## Walkthrough

```
.
‚îú‚îÄ‚îÄ examples             # contains demonstration examples, start here to learn about LeRobot
|   ‚îî‚îÄ‚îÄ advanced         # contains even more examples for those who have mastered the basics
‚îú‚îÄ‚îÄ lerobot
|   ‚îú‚îÄ‚îÄ configs          # contains config classes with all options that you can override in the command line
|   ‚îú‚îÄ‚îÄ common           # contains classes and utilities
|   |   ‚îú‚îÄ‚îÄ datasets       # various datasets of human demonstrations: aloha, pusht, xarm
|   |   ‚îú‚îÄ‚îÄ envs           # various sim environments: aloha, pusht, xarm
|   |   ‚îú‚îÄ‚îÄ policies       # various policies: act, diffusion, tdmpc
|   |   ‚îú‚îÄ‚îÄ robot_devices  # various real devices: dynamixel motors, opencv cameras, koch robots
|   |   ‚îî‚îÄ‚îÄ utils          # various utilities
|   ‚îî‚îÄ‚îÄ scripts          # contains functions to execute via command line
|       ‚îú‚îÄ‚îÄ eval.py                 # load policy and evaluate it on an environment
|       ‚îú‚îÄ‚îÄ train.py                # train a policy via imitation learning and/or reinforcement learning
|       ‚îú‚îÄ‚îÄ control_robot.py        # teleoperate a real robot, record data, run a policy
|       ‚îú‚îÄ‚îÄ push_dataset_to_hub.py  # convert your dataset into LeRobot dataset format and upload it to the Hugging Face hub
|       ‚îî‚îÄ‚îÄ visualize_dataset.py    # load a dataset and render its demonstrations
‚îú‚îÄ‚îÄ outputs               # contains results of scripts execution: logs, videos, model checkpoints
‚îî‚îÄ‚îÄ tests                 # contains pytest utilities for continuous integration
```

### Visualize datasets

Check out [example 1](./examples/1_load_lerobot_dataset.py) that illustrates how to use our dataset class which automatically downloads data from the Hugging Face hub.

You can also locally visualize episodes from a dataset on the hub by executing our script from the command line:
```bash
python lerobot/scripts/visualize_dataset.py \
    --repo-id lerobot/pusht \
    --episode-index 0
```

or from a dataset in a local folder with the `root` option and the `--local-files-only` (in the following case the dataset will be searched for in `./my_local_data_dir/lerobot/pusht`)
```bash
python lerobot/scripts/visualize_dataset.py \
    --repo-id lerobot/pusht \
    --root ./my_local_data_dir \
    --local-files-only 1 \
    --episode-index 0
```


It will open `rerun.io` and display the camera streams, robot states and actions, like this:

https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20240505T172924Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;X-Amz-SignedHeaders=host&amp;actor_id=24889239&amp;key_id=0&amp;repo_id=748713144


Our script can also visualize datasets stored on a distant server. See `python lerobot/scripts/visualize_dataset.py --help` for more instructions.

### The `LeRobotDataset` format

A dataset in `LeRobotDataset` format is very simple to use. It can be loaded from a repository on the Hugging Face hub or a local folder simply with e.g. `dataset = LeRobotDataset(&quot;lerobot/aloha_static_coffee&quot;)` and can be indexed into like any Hugging Face and PyTorch dataset. For instance `dataset[0]` will retrieve a single temporal frame from the dataset containing observation(s) and an action as PyTorch tensors ready to be fed to a model.

A specificity of `LeRobotDataset` is that, rather than retrieving a single frame by its index, we can retrieve several frames based on their temporal relationship with the indexed frame, by setting `delta_timestamps` to a list of relative times with respect to the indexed frame. For example, with `delta_timestamps = {&quot;observation.image&quot;: [-1, -0.5, -0.2, 0]}`  one can retrieve, for a given index, 4 frames: 3 &quot;previous&quot; frames 1 second, 0.5 seconds, and 0.2 seconds before the indexed frame, and the indexed frame itself (corresponding to the 0 entry). See example [1_load_lerobot_dataset.py](examples/1_load_lerobot_dataset.py) for more details on `delta_timestamps`.

Under the hood, the `LeRobotDataset` format makes use of several ways to serialize data which can be useful to understand if you plan to work more closely with this format. We tried to make a flexible yet simple dataset format that would cover most type of features and specificities present in reinforcement learning and robotics, in simulation and in real-world, with a focus on cameras and robot states but easily extended to other types of sensory inputs as long as they can be represented by a tensor.

Here are the important details and internal structure organization of a typical `LeRobotDataset` instantiated with `dataset = LeRobotDataset(&quot;lerobot/aloha_static_coffee&quot;)`. The exact features will change from dataset to dataset but not the main aspects:

```
dataset attributes:
  ‚îú hf_dataset: a Hugging Face dataset (backed by Arrow/parquet). Typical features example:
  ‚îÇ  ‚îú observation.images.cam_high (VideoFrame):
  ‚îÇ  ‚îÇ   VideoFrame = {&#039;path&#039;: path to a mp4 video, &#039;timestamp&#039; (float32): timestamp in the video}
  ‚îÇ  ‚îú observation.state (list of float32): position of an arm joints (for instance)
  ‚îÇ  ... (more observations)
  ‚îÇ  ‚îú action (list of float32): goal position of an arm joints (for instance)
  ‚îÇ  ‚îú episode_index (int64): index of the episode for this sample
  ‚îÇ  ‚îú frame_index (int64): index of the frame for this sample in the episode ; starts at 0 for each episode
  ‚îÇ  ‚îú timestamp (float32): timestamp in the episode
  ‚îÇ  ‚îú next.done (bool): indicates the end of an episode ; True for the last frame in each episode
  ‚îÇ  ‚îî index (int64): general index in the whole dataset
  ‚îú episode_data_index: contains 2 tensors with the start and end indices of each episode
  ‚îÇ  ‚îú from (1D int64 tensor): first frame index for each episode ‚Äî shape (num episodes,) starts with 0
  ‚îÇ  ‚îî to: (1D int64 tensor): last frame index for each episode ‚Äî shape (num episodes,)
  ‚îú stats: a dictionary of statistics (max, mean, min, std) for each feature in the dataset, for instance
  ‚îÇ  ‚îú observation.images.cam_high: {&#039;max&#039;: tensor with same number of dimensions (e.g. `(c, 1, 1)` for images, `(c,)` for states), etc.}
  ‚îÇ  ...
  ‚îú info: a dictionary of metadata on the dataset
  ‚îÇ  ‚îú codebase_version (str): this is to keep track of the codebase version the dataset was created with
  ‚îÇ  ‚îú fps (float): frame per second the dataset is recorded/synchronized to
  ‚îÇ  ‚îú video (bool): indicates if frames are encoded in mp4 video files to save space or stored as png files
  ‚îÇ  ‚îî encoding (dict): if video, this documents the main options that were used with ffmpeg to encode the videos
  ‚îú videos_dir (Path): where the mp4 videos or png images are stored/accessed
  ‚îî camera_keys (list of string): the keys to access camera features in the item returned by the dataset (e.g. `[&quot;observation.images.cam_high&quot;, ...]`)
```

A `LeRobotDataset` is serialised using several widespread file formats for each of its parts, namely:
- hf_dataset stored using Hugging Face datasets library serialization to parquet
- videos are stored in mp4 format to save space
- metadata are stored in plain json/jsonl files

Dataset can be uploaded/downloaded from the HuggingFace hub seamlessly. To work on a local dataset, you can specify its location with the `root` argument if it&#039;s not in the default `~/.cache/huggingface/lerobot` location.

### Evaluate a pretrained policy

Check out [example 2](./examples/2_evaluate_pretrained_policy.py) that illustrates how to download a pretrained policy from Hugging Face hub, and run an evaluation on its corresponding environment.

We also provide a more capable script to parallelize the evaluation over multiple environments during the same rollout. Here is an example with a pretrained model hosted on [lerobot/diffusion_pusht](https://huggingface.co/lerobot/diffusion_pusht):
```bash
python lerobot/scripts/eval.py \
    --policy.path=lerobot/diffusion_pusht \
    --env.type=pusht \
    --eval.batch_size=10 \
    --eval.n_episodes=10 \
    --policy.use_amp=false \
    --policy.device=cuda
```

Note: After training your own policy, you can re-evaluate the checkpoints with:

```bash
python lerobot/scripts/eval.py --policy.path={OUTPUT_DIR}/checkpoints/last/pretrained_model
```

See `python lerobot/scripts/eval.py --help` for more instructions.

### Train your own policy

Check out [example 3](./examples/3_train_policy.py) that illustrates how to train a model using our core library in python, and [example 4](./examples/4_train_policy_with_script.md) that shows how to use our training script from command line.

To use wandb for logging training and evaluation curves, make sure you&#039;ve run `wandb login` as a one-time setup step. Then, when running the training command above, enable WandB in the configuration by adding `--wandb.enable=true`.

A link to the wandb logs for the run will also show up in yellow in your terminal. Here is an example of what they look like in your browser. Please also check [here](./examples/4_train_policy_with_script.md#typical-logs-and-metrics) for the explanation of some commonly used metrics in logs.

![](media/wandb.png)

Note: For efficiency, during training every checkpoint is evaluated on a low number of episodes. You may use `--eval.n_episodes=500` to evaluate on more episodes than the default. Or, after training, you may want to re-evaluate your best checkpoints on more episodes or change the evaluation settings. See `python lerobot/scripts/eval.py --help` for more instructions.

#### Reproduce state-of-the-art (SOTA)

We provide some pretrained policies on our [hub page](https://huggingface.co/lerobot) that can achieve state-of-the-art performances.
You can reproduce their training by loading the config from their run. Simply running:
```bash
python lerobot/scripts/train.py --config_path=lerobot/diffusion_pusht
```
reproduces SOTA results for Diffusion Policy on the PushT task.

## Contribute

If you would like to contribute to ü§ó LeRobot, please check out our [contribution guide](https://github.com/huggingface/lerobot/blob/main/CONTRIBUTING.md).

&lt;!-- ### Add a new dataset

To add a dataset to the hub, you need to login using a write-access token, which can be generated from the [Hugging Face settings](https://huggingface.co/settings/tokens):
```bash
huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential
```

Then point to your raw dataset folder (e.g. `data/aloha_static_pingpong_test_raw`), and push your dataset to the hub with:
```bash
python lerobot/scripts/push_dataset_to_hub.py \
--raw-dir data/aloha_static_pingpong_test_raw \
--out-dir data \
--repo-id lerobot/aloha_static_pingpong_test \
--raw-format aloha_hdf5
```

See `python lerobot/scripts/push_dataset_to_hub.py --help` for more instructions.

If your dataset format is not supported, implement your own in `lerobot/common/datasets/push_dataset_to_hub/${raw_format}_format.py` by copying examples like [pusht_zarr](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/pusht_zarr_format.py), [umi_zarr](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/umi_zarr_format.py), [aloha_hdf5](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/aloha_hdf5_format.py), or [xarm_pkl](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/xarm_pkl_format.py). --&gt;


### Add a pretrained policy

Once you have trained a policy you may upload it to the Hugging Face hub using a hub id that looks like `${hf_user}/${repo_name}` (e.g. [lerobot/diffusion_pusht](https://huggingface.co/lerobot/diffusion_pusht)).

You first need to find the checkpoint folder located inside your experiment directory (e.g. `outputs/train/2024-05-05/20-21-12_aloha_act_default/checkpoints/002500`). Within that there is a `pretrained_model` directory which should contain:
- `config.json`: A serialized version of the policy configuration (following the policy&#039;s dataclass config).
- `model.safetensors`: A set of `torch.nn.Module` parameters, saved in [Hugging Face Safetensors](https://huggingfac

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[RVC-Boss/GPT-SoVITS]]></title>
            <link>https://github.com/RVC-Boss/GPT-SoVITS</link>
            <guid>https://github.com/RVC-Boss/GPT-SoVITS</guid>
            <pubDate>Sun, 08 Jun 2025 00:04:42 GMT</pubDate>
            <description><![CDATA[1 min voice data can also be used to train a good TTS model! (few shot voice cloning)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/RVC-Boss/GPT-SoVITS">RVC-Boss/GPT-SoVITS</a></h1>
            <p>1 min voice data can also be used to train a good TTS model! (few shot voice cloning)</p>
            <p>Language: Python</p>
            <p>Stars: 47,298</p>
            <p>Forks: 5,212</p>
            <p>Stars today: 43 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;h1&gt;GPT-SoVITS-WebUI&lt;/h1&gt;
A Powerful Few-shot Voice Conversion and Text-to-Speech WebUI.&lt;br&gt;&lt;br&gt;

[![madewithlove](https://img.shields.io/badge/made_with-%E2%9D%A4-red?style=for-the-badge&amp;labelColor=orange)](https://github.com/RVC-Boss/GPT-SoVITS)

&lt;a href=&quot;https://trendshift.io/repositories/7033&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/7033&quot; alt=&quot;RVC-Boss%2FGPT-SoVITS | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;!-- img src=&quot;https://counter.seku.su/cmoe?name=gptsovits&amp;theme=r34&quot; /&gt;&lt;br&gt; --&gt;

[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&amp;logo=googlecolab&amp;color=525252)](https://colab.research.google.com/github/RVC-Boss/GPT-SoVITS/blob/main/colab_webui.ipynb)
[![License](https://img.shields.io/badge/LICENSE-MIT-green.svg?style=for-the-badge)](https://github.com/RVC-Boss/GPT-SoVITS/blob/main/LICENSE)
[![Huggingface](https://img.shields.io/badge/ü§ó%20-online%20demo-yellow.svg?style=for-the-badge)](https://huggingface.co/spaces/lj1995/GPT-SoVITS-v2)
[![Discord](https://img.shields.io/discord/1198701940511617164?color=%23738ADB&amp;label=Discord&amp;style=for-the-badge)](https://discord.gg/dnrgs5GHfG)

**English** | [**‰∏≠ÊñáÁÆÄ‰Ωì**](./docs/cn/README.md) | [**Êó•Êú¨Ë™û**](./docs/ja/README.md) | [**ÌïúÍµ≠Ïñ¥**](./docs/ko/README.md) | [**T√ºrk√ße**](./docs/tr/README.md)

&lt;/div&gt;

---

## Features:

1. **Zero-shot TTS:** Input a 5-second vocal sample and experience instant text-to-speech conversion.

2. **Few-shot TTS:** Fine-tune the model with just 1 minute of training data for improved voice similarity and realism.

3. **Cross-lingual Support:** Inference in languages different from the training dataset, currently supporting English, Japanese, Korean, Cantonese and Chinese.

4. **WebUI Tools:** Integrated tools include voice accompaniment separation, automatic training set segmentation, Chinese ASR, and text labeling, assisting beginners in creating training datasets and GPT/SoVITS models.

**Check out our [demo video](https://www.bilibili.com/video/BV12g4y1m7Uw) here!**

Unseen speakers few-shot fine-tuning demo:

https://github.com/RVC-Boss/GPT-SoVITS/assets/129054828/05bee1fa-bdd8-4d85-9350-80c060ab47fb

**User guide: [ÁÆÄ‰Ωì‰∏≠Êñá](https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e) | [English](https://rentry.co/GPT-SoVITS-guide#/)**

## Installation

For users in China, you can [click here](https://www.codewithgpu.com/i/RVC-Boss/GPT-SoVITS/GPT-SoVITS-Official) to use AutoDL Cloud Docker to experience the full functionality online.

### Tested Environments

| Python Version | PyTorch Version  | Device        |
| -------------- | ---------------- | ------------- |
| Python 3.10    | PyTorch 2.5.1    | CUDA 12.4     |
| Python 3.11    | PyTorch 2.5.1    | CUDA 12.4     |
| Python 3.11    | PyTorch 2.7.0    | CUDA 12.8     |
| Python 3.9     | PyTorch 2.8.0dev | CUDA 12.8     |
| Python 3.9     | PyTorch 2.5.1    | Apple silicon |
| Python 3.11    | PyTorch 2.7.0    | Apple silicon |
| Python 3.9     | PyTorch 2.2.2    | CPU           |

### Windows

If you are a Windows user (tested with win&gt;=10), you can [download the integrated package](https://huggingface.co/lj1995/GPT-SoVITS-windows-package/resolve/main/GPT-SoVITS-v3lora-20250228.7z?download=true) and double-click on _go-webui.bat_ to start GPT-SoVITS-WebUI.

**Users in China can [download the package here](https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e/dkxgpiy9zb96hob4#KTvnO).**

### Linux

```bash
conda create -n GPTSoVits python=3.10
conda activate GPTSoVits
bash install.sh --device &lt;CU126|CU128|ROCM|CPU&gt; --source &lt;HF|HF-Mirror|ModelScope&gt; [--download-uvr5]
```

### macOS

**Note: The models trained with GPUs on Macs result in significantly lower quality compared to those trained on other devices, so we are temporarily using CPUs instead.**

Install the program by running the following commands:

```bash
conda create -n GPTSoVits python=3.10
conda activate GPTSoVits
bash install.sh --device &lt;MPS|CPU&gt; --source &lt;HF|HF-Mirror|ModelScope&gt; [--download-uvr5]
```

### Install Manually

#### Install Dependences

```bash
conda create -n GPTSoVits python=3.10
conda activate GPTSoVits

pip install -r extra-req.txt --no-deps
pip install -r requirements.txt
```

#### Install FFmpeg

##### Conda Users

```bash
conda activate GPTSoVits
conda install ffmpeg
```

##### Ubuntu/Debian Users

```bash
sudo apt install ffmpeg
sudo apt install libsox-dev
```

##### Windows Users

Download and place [ffmpeg.exe](https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/ffmpeg.exe) and [ffprobe.exe](https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/ffprobe.exe) in the GPT-SoVITS root

Install [Visual Studio 2017](https://aka.ms/vs/17/release/vc_redist.x86.exe)

##### MacOS Users

```bash
brew install ffmpeg
```

### Running GPT-SoVITS with Docker

#### Docker Image Selection

Due to rapid development in the codebase and a slower Docker image release cycle, please:

- Check [Docker Hub](https://hub.docker.com/r/xxxxrt666/gpt-sovits) for the latest available image tags
- Choose an appropriate image tag for your environment
- `Lite` means the Docker image does not include ASR models and UVR5 models. You can manually download the UVR5 models, while the program will automatically download the ASR models as needed
- The appropriate architecture image (amd64/arm64) will be automatically pulled during Docker Compose
- Optionally, build the image locally using the provided Dockerfile for the most up-to-date changes

#### Environment Variables

- `is_half`: Controls whether half-precision (fp16) is enabled. Set to `true` if your GPU supports it to reduce memory usage.

#### Shared Memory Configuration

On Windows (Docker Desktop), the default shared memory size is small and may cause unexpected behavior. Increase `shm_size` (e.g., to `16g`) in your Docker Compose file based on your available system memory.

#### Choosing a Service

The `docker-compose.yaml` defines two services:

- `GPT-SoVITS-CU126` &amp; `GPT-SoVITS-CU128`: Full version with all features.
- `GPT-SoVITS-CU126-Lite` &amp; `GPT-SoVITS-CU128-Lite`: Lightweight version with reduced dependencies and functionality.

To run a specific service with Docker Compose, use:

```bash
docker compose run --service-ports &lt;GPT-SoVITS-CU126-Lite|GPT-SoVITS-CU128-Lite|GPT-SoVITS-CU126|GPT-SoVITS-CU128&gt;
```

#### Building the Docker Image Locally

If you want to build the image yourself, use:

```bash
bash docker_build.sh --cuda &lt;12.6|12.8&gt; [--lite]
```

#### Accessing the Running Container (Bash Shell)

Once the container is running in the background, you can access it using:

```bash
docker exec -it &lt;GPT-SoVITS-CU126-Lite|GPT-SoVITS-CU128-Lite|GPT-SoVITS-CU126|GPT-SoVITS-CU128&gt; bash
```

## Pretrained Models

**If `install.sh` runs successfully, you may skip No.1,2,3**

**Users in China can [download all these models here](https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e/dkxgpiy9zb96hob4#nVNhX).**

1. Download pretrained models from [GPT-SoVITS Models](https://huggingface.co/lj1995/GPT-SoVITS) and place them in `GPT_SoVITS/pretrained_models`.

2. Download G2PW models from [G2PWModel.zip(HF)](https://huggingface.co/XXXXRT/GPT-SoVITS-Pretrained/resolve/main/G2PWModel.zip)| [G2PWModel.zip(ModelScope)](https://www.modelscope.cn/models/XXXXRT/GPT-SoVITS-Pretrained/resolve/master/G2PWModel.zip), unzip and rename to `G2PWModel`, and then place them in `GPT_SoVITS/text`.(Chinese TTS Only)

3. For UVR5 (Vocals/Accompaniment Separation &amp; Reverberation Removal, additionally), download models from [UVR5 Weights](https://huggingface.co/lj1995/VoiceConversionWebUI/tree/main/uvr5_weights) and place them in `tools/uvr5/uvr5_weights`.

   - If you want to use `bs_roformer` or `mel_band_roformer` models for UVR5, you can manually download the model and corresponding configuration file, and put them in `tools/uvr5/uvr5_weights`. **Rename the model file and configuration file, ensure that the model and configuration files have the same and corresponding names except for the suffix**. In addition, the model and configuration file names **must include `roformer`** in order to be recognized as models of the roformer class.

   - The suggestion is to **directly specify the model type** in the model name and configuration file name, such as `mel_mand_roformer`, `bs_roformer`. If not specified, the features will be compared from the configuration file to determine which type of model it is. For example, the model `bs_roformer_ep_368_sdr_12.9628.ckpt` and its corresponding configuration file `bs_roformer_ep_368_sdr_12.9628.yaml` are a pair, `kim_mel_band_roformer.ckpt` and `kim_mel_band_roformer.yaml` are also a pair.

4. For Chinese ASR (additionally), download models from [Damo ASR Model](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/files), [Damo VAD Model](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/files), and [Damo Punc Model](https://modelscope.cn/models/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/files) and place them in `tools/asr/models`.

5. For English or Japanese ASR (additionally), download models from [Faster Whisper Large V3](https://huggingface.co/Systran/faster-whisper-large-v3) and place them in `tools/asr/models`. Also, [other models](https://huggingface.co/Systran) may have the similar effect with smaller disk footprint.

## Dataset Format

The TTS annotation .list file format:

```

vocal_path|speaker_name|language|text

```

Language dictionary:

- &#039;zh&#039;: Chinese
- &#039;ja&#039;: Japanese
- &#039;en&#039;: English
- &#039;ko&#039;: Korean
- &#039;yue&#039;: Cantonese

Example:

```

D:\GPT-SoVITS\xxx/xxx.wav|xxx|en|I like playing Genshin.

```

## Finetune and inference

### Open WebUI

#### Integrated Package Users

Double-click `go-webui.bat`or use `go-webui.ps1`
if you want to switch to V1,then double-click`go-webui-v1.bat` or use `go-webui-v1.ps1`

#### Others

```bash
python webui.py &lt;language(optional)&gt;
```

if you want to switch to V1,then

```bash
python webui.py v1 &lt;language(optional)&gt;
```

Or maunally switch version in WebUI

### Finetune

#### Path Auto-filling is now supported

1. Fill in the audio path
2. Slice the audio into small chunks
3. Denoise(optinal)
4. ASR
5. Proofreading ASR transcriptions
6. Go to the next Tab, then finetune the model

### Open Inference WebUI

#### Integrated Package Users

Double-click `go-webui-v2.bat` or use `go-webui-v2.ps1` ,then open the inference webui at `1-GPT-SoVITS-TTS/1C-inference`

#### Others

```bash
python GPT_SoVITS/inference_webui.py &lt;language(optional)&gt;
```

OR

```bash
python webui.py
```

then open the inference webui at `1-GPT-SoVITS-TTS/1C-inference`

## V2 Release Notes

New Features:

1. Support Korean and Cantonese

2. An optimized text frontend

3. Pre-trained model extended from 2k hours to 5k hours

4. Improved synthesis quality for low-quality reference audio

   [more details](&lt;https://github.com/RVC-Boss/GPT-SoVITS/wiki/GPT%E2%80%90SoVITS%E2%80%90v2%E2%80%90features-(%E6%96%B0%E7%89%B9%E6%80%A7)&gt;)

Use v2 from v1 environment:

1. `pip install -r requirements.txt` to update some packages

2. Clone the latest codes from github.

3. Download v2 pretrained models from [huggingface](https://huggingface.co/lj1995/GPT-SoVITS/tree/main/gsv-v2final-pretrained) and put them into `GPT_SoVITS/pretrained_models/gsv-v2final-pretrained`.

   Chinese v2 additional: [G2PWModel.zip(HF)](https://huggingface.co/XXXXRT/GPT-SoVITS-Pretrained/resolve/main/G2PWModel.zip)| [G2PWModel.zip(ModelScope)](https://www.modelscope.cn/models/XXXXRT/GPT-SoVITS-Pretrained/resolve/master/G2PWModel.zip)(Download G2PW models, unzip and rename to `G2PWModel`, and then place them in `GPT_SoVITS/text`.)

## V3 Release Notes

New Features:

1. The timbre similarity is higher, requiring less training data to approximate the target speaker (the timbre similarity is significantly improved using the base model directly without fine-tuning).

2. GPT model is more stable, with fewer repetitions and omissions, and it is easier to generate speech with richer emotional expression.

   [more details](&lt;https://github.com/RVC-Boss/GPT-SoVITS/wiki/GPT%E2%80%90SoVITS%E2%80%90v3v4%E2%80%90features-(%E6%96%B0%E7%89%B9%E6%80%A7)&gt;)

Use v3 from v2 environment:

1. `pip install -r requirements.txt` to update some packages

2. Clone the latest codes from github.

3. Download v3 pretrained models (s1v3.ckpt, s2Gv3.pth and models--nvidia--bigvgan_v2_24khz_100band_256x folder) from [huggingface](https://huggingface.co/lj1995/GPT-SoVITS/tree/main) and put them into `GPT_SoVITS/pretrained_models`.

   additional: for Audio Super Resolution model, you can read [how to download](./tools/AP_BWE_main/24kto48k/readme.txt)

## V4 Release Notes

New Features:

1. Version 4 fixes the issue of metallic artifacts in Version 3 caused by non-integer multiple upsampling, and natively outputs 48k audio to prevent muffled sound (whereas Version 3 only natively outputs 24k audio). The author considers Version 4 a direct replacement for Version 3, though further testing is still needed.
   [more details](&lt;https://github.com/RVC-Boss/GPT-SoVITS/wiki/GPT%E2%80%90SoVITS%E2%80%90v3v4%E2%80%90features-(%E6%96%B0%E7%89%B9%E6%80%A7)&gt;)

Use v4 from v1/v2/v3 environment:

1. `pip install -r requirements.txt` to update some packages

2. Clone the latest codes from github.

3. Download v4 pretrained models (gsv-v4-pretrained/s2v4.ckpt, and gsv-v4-pretrained/vocoder.pth) from [huggingface](https://huggingface.co/lj1995/GPT-SoVITS/tree/main) and put them into `GPT_SoVITS/pretrained_models`.

## V2Pro Release Notes

New Features:

1. Slightly higher VRAM usage than v2, surpassing v4&#039;s performance, with v2&#039;s hardware cost and speed.
   [more details](https://github.com/RVC-Boss/GPT-SoVITS/wiki/GPT%E2%80%90SoVITS%E2%80%90features-(%E5%90%84%E7%89%88%E6%9C%AC%E7%89%B9%E6%80%A7))

2.v1/v2 and the v2Pro series share the same characteristics, while v3/v4 have similar features. For training sets with average audio quality, v1/v2/v2Pro can deliver decent results, but v3/v4 cannot. Additionally, the synthesized tone and timebre of v3/v4 lean more toward the reference audio rather than the overall training set.

Use v2Pro from v1/v2/v3/v4 environment:

1. `pip install -r requirements.txt` to update some packages

2. Clone the latest codes from github.

3. Download v2Pro pretrained models (v2Pro/s2Dv2Pro.pth, v2Pro/s2Gv2Pro.pth, v2Pro/s2Dv2ProPlus.pth, v2Pro/s2Gv2ProPlus.pth, and sv/pretrained_eres2netv2w24s4ep4.ckpt) from [huggingface](https://huggingface.co/lj1995/GPT-SoVITS/tree/main) and put them into `GPT_SoVITS/pretrained_models`.

## Todo List

- [x] **High Priority:**

  - [x] Localization in Japanese and English.
  - [x] User guide.
  - [x] Japanese and English dataset fine tune training.

- [ ] **Features:**
  - [x] Zero-shot voice conversion (5s) / few-shot voice conversion (1min).
  - [x] TTS speaking speed control.
  - [ ] ~~Enhanced TTS emotion control.~~ Maybe use pretrained finetuned preset GPT models for better emotion.
  - [ ] Experiment with changing SoVITS token inputs to probability distribution of GPT vocabs (transformer latent).
  - [x] Improve English and Japanese text frontend.
  - [ ] Develop tiny and larger-sized TTS models.
  - [x] Colab scripts.
  - [x] Try expand training dataset (2k hours -&gt; 10k hours).
  - [x] better sovits base model (enhanced audio quality)
  - [ ] model mix

## (Additional) Method for running from the command line

Use the command line to open the WebUI for UVR5

```bash
python tools/uvr5/webui.py &quot;&lt;infer_device&gt;&quot; &lt;is_half&gt; &lt;webui_port_uvr5&gt;
```

&lt;!-- If you can&#039;t open a browser, follow the format below for UVR processing,This is using mdxnet for audio processing
```
python mdxnet.py --model --input_root --output_vocal --output_ins --agg_level --format --device --is_half_precision
``` --&gt;

This is how the audio segmentation of the dataset is done using the command line

```bash
python audio_slicer.py \
    --input_path &quot;&lt;path_to_original_audio_file_or_directory&gt;&quot; \
    --output_root &quot;&lt;directory_where_subdivided_audio_clips_will_be_saved&gt;&quot; \
    --threshold &lt;volume_threshold&gt; \
    --min_length &lt;minimum_duration_of_each_subclip&gt; \
    --min_interval &lt;shortest_time_gap_between_adjacent_subclips&gt;
    --hop_size &lt;step_size_for_computing_volume_curve&gt;
```

This is how dataset ASR processing is done using the command line(Only Chinese)

```bash
python tools/asr/funasr_asr.py -i &lt;input&gt; -o &lt;output&gt;
```

ASR processing is performed through Faster_Whisper(ASR marking except Chinese)

(No progress bars, GPU performance may cause time delays)

```bash
python ./tools/asr/fasterwhisper_asr.py -i &lt;input&gt; -o &lt;output&gt; -l &lt;language&gt; -p &lt;precision&gt;
```

A custom list save path is enabled

## Credits

Special thanks to the following projects and contributors:

### Theoretical Research

- [ar-vits](https://github.com/innnky/ar-vits)
- [SoundStorm](https://github.com/yangdongchao/SoundStorm/tree/master/soundstorm/s1/AR)
- [vits](https://github.com/jaywalnut310/vits)
- [TransferTTS](https://github.com/hcy71o/TransferTTS/blob/master/models.py#L556)
- [contentvec](https://github.com/auspicious3000/contentvec/)
- [hifi-gan](https://github.com/jik876/hifi-gan)
- [fish-speech](https://github.com/fishaudio/fish-speech/blob/main/tools/llama/generate.py#L41)
- [f5-TTS](https://github.com/SWivid/F5-TTS/blob/main/src/f5_tts/model/backbones/dit.py)
- [shortcut flow matching](https://github.com/kvfrans/shortcut-models/blob/main/targets_shortcut.py)

### Pretrained Models

- [Chinese Speech Pretrain](https://github.com/TencentGameMate/chinese_speech_pretrain)
- [Chinese-Roberta-WWM-Ext-Large](https://huggingface.co/hfl/chinese-roberta-wwm-ext-large)
- [BigVGAN](https://github.com/NVIDIA/BigVGAN)
- [eresnetv2](https://modelscope.cn/models/iic/speech_eres2netv2w24s4ep4_sv_zh-cn_16k-common)

### Text Frontend for Inference

- [paddlespeech zh_normalization](https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/paddlespeech/t2s/frontend/zh_normalization)
- [split-lang](https://github.com/DoodleBears/split-lang)
- [g2pW](https://github.com/GitYCC/g2pW)
- [pypinyin-g2pW](https://github.com/mozillazg/pypinyin-g2pW)
- [paddlespeech g2pw](https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/paddlespeech/t2s/frontend/g2pw)

### WebUI Tools

- [ultimatevocalremovergui](https://github.com/Anjok07/ultimatevocalremovergui)
- [audio-slicer](https://github.com/openvpi/audio-slicer)
- [SubFix](https://github.com/cronrpc/SubFix)
- [FFmpeg](https://github.com/FFmpeg/FFmpeg)
- [gradio](https://github.com/gradio-app/gradio)
- [faster-whisper](https://github.com/SYSTRAN/faster-whisper)
- [FunASR](https://github.com/alibaba-damo-academy/FunASR)
- [AP-BWE](https://github.com/yxlu-0102/AP-BWE)

Thankful to @Naozumi520 for providing the Cantonese training set and for the guidance on Cantonese-related knowledge.

## Thanks to all contributors for their efforts

&lt;a href=&quot;https://github.com/RVC-Boss/GPT-SoVITS/graphs/contributors&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=RVC-Boss/GPT-SoVITS&quot; /&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[xming521/WeClone]]></title>
            <link>https://github.com/xming521/WeClone</link>
            <guid>https://github.com/xming521/WeClone</guid>
            <pubDate>Sun, 08 Jun 2025 00:04:41 GMT</pubDate>
            <description><![CDATA[üöÄ One-stop solution for creating your digital avatar from chat history üí° Fine-tune LLMs with your chat logs to capture your unique style, then bind to a chatbot to bring your digital self to life. ‰ªéËÅäÂ§©ËÆ∞ÂΩïÂàõÈÄ†Êï∞Â≠óÂàÜË∫´ÁöÑ‰∏ÄÁ´ôÂºèËß£ÂÜ≥ÊñπÊ°à]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/xming521/WeClone">xming521/WeClone</a></h1>
            <p>üöÄ One-stop solution for creating your digital avatar from chat history üí° Fine-tune LLMs with your chat logs to capture your unique style, then bind to a chatbot to bring your digital self to life. ‰ªéËÅäÂ§©ËÆ∞ÂΩïÂàõÈÄ†Êï∞Â≠óÂàÜË∫´ÁöÑ‰∏ÄÁ´ôÂºèËß£ÂÜ≥ÊñπÊ°à</p>
            <p>Language: Python</p>
            <p>Stars: 13,222</p>
            <p>Forks: 987</p>
            <p>Stars today: 69 stars today</p>
            <h2>README</h2><pre>![download](https://github.com/user-attachments/assets/5842e84e-004f-4afd-9373-af64e9575b78)
&lt;h3 align=&quot;center&quot;&gt;üöÄ One-stop solution for creating your digital avatar from chat history üí°&lt;/h3&gt;  
&lt;h3 align=&quot;center&quot;&gt;üöÄ‰ªéËÅäÂ§©ËÆ∞ÂΩïÂàõÈÄ†Êï∞Â≠óÂàÜË∫´ÁöÑ‰∏ÄÁ´ôÂºèËß£ÂÜ≥ÊñπÊ°àüí°&lt;/h3&gt;  


&lt;div align=&quot;center&quot;&gt;

[![GitHub stars](https://img.shields.io/github/stars/xming521/WeClone?style=for-the-badge&amp;logo=github&amp;label=Stars&amp;logoColor=white&amp;color=ffda65)](https://github.com/xming521/WeClone/stargazers)
[![GitHub release](https://img.shields.io/github/v/release/xming521/WeClone?style=for-the-badge&amp;logo=github&amp;label=Release&amp;logoColor=white&amp;color=06d094)](https://github.com/xming521/WeClone/releases)
&lt;a href=&quot;https://qm.qq.com/cgi-bin/qm/qr?k=QXMsXJ_eqeabS0cck0PGjEMyKjcq7J5d&amp;jump_from=webapi&amp;authKey=KHdy31VbSxj34VQVwXtEOYVi1K7SND45vJcNnm1Z5iCCR6IbGiyWEs9UbPqFI8Jc&quot; target=&quot;_blank&quot; style=&quot;text-decoration: none;&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/QQÁæ§-650118277-12B7F5?style=for-the-badge&amp;logo=qq&amp;logoColor=white&quot; alt=&quot;WeClone‚ë†&quot; title=&quot;WeClone‚ë†&quot;&gt;
&lt;/a&gt;
[![Twitter](https://img.shields.io/badge/Twitter-@weclone567-000000?style=for-the-badge&amp;logo=x&amp;logoColor=white)](https://x.com/weclone567)
[![Telegram](https://img.shields.io/badge/Telegram-2CA5E0?style=for-the-badge&amp;logo=telegram&amp;logoColor=white)](https://t.me/+JEdak4m0XEQ3NGNl)

&lt;a href=&quot;https://hellogithub.com/repository/12ab209b56cb4cfd885c8cfd4cfdd53e&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=12ab209b56cb4cfd885c8cfd4cfdd53e&amp;claim_uid=RThlPDoGrFvdMY5&quot; alt=&quot;FeaturedÔΩúHelloGitHub&quot; style=&quot;width: 150px; height: 28px;&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://trendshift.io/repositories/13759&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13759&quot; alt=&quot;xming521%2FWeClone | Trendshift&quot; style=&quot;width: 220px; height: 50px;&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://deepwiki.com/xming521/WeClone&quot;&gt;&lt;img src=&quot;https://deepwiki.com/badge.svg&quot; alt=&quot;Ask DeepWiki&quot;  style=&quot;width: 134px; height: 23px;margin-bottom: 3px;&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.weclone.love/&quot; target=&quot;_blank&quot;&gt; È°πÁõÆ‰∏ªÈ°µ &lt;/a&gt; ÔΩú
  &lt;a href=&quot;https://www.weclone.love/what-is-weclone.html&quot; target=&quot;_blank&quot;&gt; È°πÁõÆÊñáÊ°£ &lt;/a&gt; ÔΩú
  &lt;a href=&quot;https://blog.051088.xyz/2025/05/14/WeClone-%E7%94%A8%E5%BE%AE%E4%BF%A1%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84AI%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB/&quot; target=&quot;_blank&quot;&gt;WindowsÈÉ®ÁΩ≤ÊåáÂçó&lt;/a&gt; ÔΩú
  &lt;a href=&quot;https://blog.051088.xyz/posts/weclone-linux-tutorial/&quot; target=&quot;_blank&quot;&gt; LinuxÈÉ®ÁΩ≤ÊåáÂçó„Äê‰øùÂßÜÁ∫ß„Äë&lt;/a&gt;
&lt;/p&gt;

&gt; [!IMPORTANT]
&gt; ### WhatsApp and Telegram chat logs integration for digital avatar creation is coming !

## ‚ú®Ê†∏ÂøÉÂäüËÉΩ
- üí´ Ê∂µÁõñÊâìÈÄ†Êï∞Â≠óÂàÜË∫´ÁöÑÂÖ®ÈìæË∑ØÊñπÊ°àÔºåÂåÖÊã¨ËÅäÂ§©Êï∞ÊçÆÂØºÂá∫„ÄÅÈ¢ÑÂ§ÑÁêÜ„ÄÅÊ®°ÂûãËÆ≠ÁªÉ„ÄÅÈÉ®ÁΩ≤
- üí¨ ‰ΩøÁî®ÂæÆ‰ø°ËÅäÂ§©ËÆ∞ÂΩïÂæÆË∞ÉLLMÔºåËÆ©Â§ßÊ®°ÂûãÊúâ&quot;ÈÇ£Âë≥ÂÑø&quot;
- üîó ÁªëÂÆöÂà∞ÂæÆ‰ø°„ÄÅQQ„ÄÅTelegram„ÄÅ‰ºÅÂæÆ„ÄÅÈ£û‰π¶Êú∫Âô®‰∫∫ÔºåÂÆûÁé∞Ëá™Â∑±ÁöÑÊï∞Â≠óÂàÜË∫´
- üõ°Ô∏è ÈöêÁßÅ‰ø°ÊÅØËøáÊª§ÔºåÊú¨Âú∞ÂåñÂæÆË∞ÉÈÉ®ÁΩ≤ÔºåÊï∞ÊçÆÂÆâÂÖ®ÂèØÊéß

## üìãÁâπÊÄß‰∏éËØ¥Êòé

&gt; [!IMPORTANT]
&gt; ### WeCloneÁé∞Âú®ÊîØÊåÅÂõæÁâáÊ®°ÊÄÅÊï∞ÊçÆÂæÆË∞É‰∫ÜÔºÅÂπ∂‰∏îÂåÖÂê´‰∫ÜÊõ¥ÂÖ®ÁöÑ‰∏ä‰∏ãÊñá,ËÆ∞ÂæóÊãâÂèñÊúÄÊñ∞‰ª£Á†ÅÂπ∂Êõ¥Êñ∞‰æùËµñ„ÄÇ

&gt; [!IMPORTANT]
&gt; - WeClone‰ªçÂú®Âø´ÈÄüËø≠‰ª£ÊúüÔºåÂΩìÂâçÊïàÊûú‰∏ç‰ª£Ë°®ÊúÄÁªàÊïàÊûú„ÄÇ  
&gt; - ÂæÆË∞ÉLLMÊïàÊûúÂæàÂ§ßÁ®ãÂ∫¶ÂèñÂÜ≥‰∫éÊ®°ÂûãÂ§ßÂ∞è„ÄÅËÅäÂ§©Êï∞ÊçÆÁöÑÊï∞ÈáèÂíåË¥®ÈáèÔºåÁêÜËÆ∫‰∏äÊ®°ÂûãË∂äÂ§ßÔºåÊï∞ÊçÆË∂äÂ§öÔºåÊïàÊûúË∂äÂ•Ω„ÄÇ   
&gt; - WindowsÁéØÂ¢ÉÊú™ËøõË°å‰∏•Ê†ºÊµãËØïÔºåÂèØ‰ª•‰ΩøÁî®WSL‰Ωú‰∏∫ËøêË°åÁéØÂ¢É„ÄÇËØ¶ÁªÜÊïôÁ®ãÂèØÁÇπÂáª[WindowsÈÉ®ÁΩ≤ÊåáÂçó](https://blog.051088.xyz/2025/05/14/WeClone-%E7%94%A8%E5%BE%AE%E4%BF%A1%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84AI%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB/)Êü•Áúã„ÄÇ

### Á°¨‰ª∂Ë¶ÅÊ±Ç

È°πÁõÆÈªòËÆ§‰ΩøÁî®Qwen2.5-7B-InstructÊ®°ÂûãÔºåLoRAÊñπÊ≥ïÂØπsftÈò∂ÊÆµÂæÆË∞ÉÔºåÂ§ßÁ∫¶ÈúÄË¶Å16GBÊòæÂ≠ò„ÄÇ‰πüÂèØ‰ª•‰ΩøÁî®[LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md#%E6%A8%A1%E5%9E%8B)ÊîØÊåÅÁöÑÂÖ∂‰ªñÊ®°ÂûãÂíåÊñπÊ≥ï„ÄÇ

ÈúÄË¶ÅÊòæÂ≠òÁöÑ‰º∞ÁÆóÂÄºÔºö
| ÊñπÊ≥ï                             | Á≤æÂ∫¶ |   7B  |  14B  |  30B  |   70B  |   `x`B  |
| ------------------------------- | ---- | ----- | ----- | ----- | ------ | ------- |
| Full (`bf16` or `fp16`)         |  32  | 120GB | 240GB | 600GB | 1200GB | `18x`GB |
| Full (`pure_bf16`)              |  16  |  60GB | 120GB | 300GB |  600GB |  `8x`GB |
| Freeze/LoRA/GaLore/APOLLO/BAdam |  16  |  16GB |  32GB |  64GB |  160GB |  `2x`GB |
| QLoRA                           |   8  |  10GB |  20GB |  40GB |   80GB |   `x`GB |
| QLoRA                           |   4  |   6GB |  12GB |  24GB |   48GB | `x/2`GB |
| QLoRA                           |   2  |   4GB |   8GB |  16GB |   24GB | `x/4`GB |


## ÁéØÂ¢ÉÊê≠Âª∫
1.cudaÂÆâË£Ö(Â∑≤ÂÆâË£ÖÂèØË∑≥ËøáÔºå**Ë¶ÅÊ±ÇÁâàÊú¨12.4Âèä‰ª•‰∏ä**)Ôºö[LLaMA Factory](https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/installation.html#cuda) 

2.Âª∫ËÆÆ‰ΩøÁî® [uv](https://docs.astral.sh/uv/)ÂÆâË£Ö‰æùËµñÔºåËøôÊòØ‰∏Ä‰∏™ÈùûÂ∏∏Âø´ÈÄüÁöÑ Python ÁéØÂ¢ÉÁÆ°ÁêÜÂô®„ÄÇÂÆâË£ÖuvÂêéÔºåÊÇ®ÂèØ‰ª•‰ΩøÁî®‰ª•‰∏ãÂëΩ‰ª§ÂàõÂª∫‰∏Ä‰∏™Êñ∞ÁöÑPythonÁéØÂ¢ÉÂπ∂ÂÆâË£Ö‰æùËµñÈ°πÔºåÊ≥®ÊÑèËøô‰∏çÂåÖÂê´Èü≥È¢ëÂÖãÈöÜÂäüËÉΩÁöÑ‰æùËµñÔºö
```bash
git clone https://github.com/xming521/WeClone.git
cd WeClone
uv venv .venv --python=3.10
source .venv/bin/activate # windows‰∏ãÊâßË°å .venv\Scripts\activate
uv pip install --group main -e . 
```
&gt; [!TIP]
&gt; Â¶ÇÊûúË¶Å‰ΩøÁî®ÊúÄÊñ∞ÁöÑÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºåÈúÄË¶ÅÊâãÂä®ÂÆâË£ÖÊúÄÊñ∞ÁâàLLaMA FactoryÔºö`uv pip install --upgrade git+https://github.com/hiyouga/LLaMA-Factory.git`,ÂêåÊó∂ÂÖ∂‰ªñ‰æùËµñÁâàÊú¨‰πüÂèØËÉΩÈúÄË¶Å‰øÆÊîπÔºå‰æãÂ¶Çvllm pytorch transforms

3.Â∞ÜÈÖçÁΩÆÊñá‰ª∂Ê®°ÊùøÂ§çÂà∂‰∏Ä‰ªΩÂπ∂ÈáçÂëΩÂêç‰∏∫`settings.jsonc`ÔºåÂêéÁª≠ÈÖçÁΩÆ‰øÆÊîπÂú®Ê≠§Êñá‰ª∂ËøõË°åÔºö
```bash
cp settings.template.jsonc settings.jsonc
```
- ÂæÆË∞É**Â§öÊ®°ÊÄÅÊ®°Âûã**Êó∂ÔºåËØ∑‰ΩøÁî®[examples/mllm.template.jsonc](https://github.com/xming521/WeClone/blob/master/examples/mllm.template.jsonc)‰Ωú‰∏∫ÈÖçÁΩÆÊñá‰ª∂„ÄÇ

&gt; [!NOTE]
&gt; ËÆ≠ÁªÉ‰ª•ÂèäÊé®ÁêÜÁõ∏ÂÖ≥ÈÖçÁΩÆÁªü‰∏ÄÂú®Êñá‰ª∂`settings.jsonc`

4.‰ΩøÁî®‰ª•‰∏ãÂëΩ‰ª§ÊµãËØïCUDAÁéØÂ¢ÉÊòØÂê¶Ê≠£Á°ÆÈÖçÁΩÆÂπ∂ÂèØË¢´PyTorchËØÜÂà´ÔºåMac‰∏çÈúÄË¶ÅÔºö
```bash
python -c &quot;import torch; print(&#039;CUDAÊòØÂê¶ÂèØÁî®:&#039;, torch.cuda.is_available());&quot;
```

5.ÔºàÂèØÈÄâÔºâÂÆâË£ÖFlashAttentionÔºåÂä†ÈÄüËÆ≠ÁªÉÂíåÊé®ÁêÜÔºö`uv pip install flash-attn --no-build-isolation`

## Ê®°Âûã‰∏ãËΩΩ
```bash
git lfs install
git clone https://www.modelscope.cn/Qwen/Qwen2.5-7B-Instruct.git
```
‰∏ãËΩΩÊúâÈóÆÈ¢ò‰ΩøÁî®ÂÖ∂‰ªñÊñπÂºè‰∏ãËΩΩÔºö[Ê®°ÂûãÁöÑ‰∏ãËΩΩ](https://www.modelscope.cn/docs/models/download)


## Êï∞ÊçÆÂáÜÂ§á

ËØ∑‰ΩøÁî®[PyWxDump](https://github.com/xaoyaoo/PyWxDump)ÊèêÂèñÂæÆ‰ø°ËÅäÂ§©ËÆ∞ÂΩïÔºà‰∏çÊîØÊåÅ4.0ÁâàÊú¨ÂæÆ‰ø°Ôºâ„ÄÇÂèØ‰ª•ÂÖàÂ∞ÜÊâãÊú∫ÁöÑËÅäÂ§©ËÆ∞ÂΩïËøÅÁßªÔºàÂ§á‰ªΩÔºâÂà∞ÁîµËÑëÔºåÊï∞ÊçÆÈáèÊõ¥Â§ö‰∏Ä‰∫õ„ÄÇ‰∏ãËΩΩËΩØ‰ª∂Âπ∂Ëß£ÂØÜÊï∞ÊçÆÂ∫ìÂêéÔºåÁÇπÂáªËÅäÂ§©Â§á‰ªΩÔºåÂØºÂá∫Á±ªÂûã‰∏∫CSVÔºåÂèØ‰ª•ÂØºÂá∫Â§ö‰∏™ËÅîÁ≥ª‰∫∫Ôºà‰∏çÂª∫ËÆÆ‰ΩøÁî®Áæ§ËÅäËÆ∞ÂΩïÔºâÔºåÁÑ∂ÂêéÂ∞ÜÂØºÂá∫ÁöÑ‰Ωç‰∫é`wxdump_tmp/export` ÁöÑ `csv` Êñá‰ª∂Â§πÊîæÂú®`./dataset`ÁõÆÂΩïÂç≥ÂèØÔºå‰πüÂ∞±ÊòØ‰∏çÂêå‰∫∫ËÅäÂ§©ËÆ∞ÂΩïÁöÑÊñá‰ª∂Â§π‰∏ÄËµ∑ÊîæÂú® `./dataset/csv`„ÄÇ   

### ÂõæÁâáÊï∞ÊçÆÂáÜÂ§á
Âú®ËÉΩËøõÂÖ•ÂæÆ‰ø°‰∏™‰∫∫Êñá‰ª∂Â§πÁöÑÁéØÂ¢ÉÊâßË°åÔºåÂ¶ÇÊûúÊ≤°ÊúâÁéØÂ¢ÉÂàõÂª∫ÁéØÂ¢ÉÂπ∂ÂÆâË£ÖÂü∫Á°Ä‰æùËµñÂç≥ÂèØÔºà`uv pip install -e .`ÔºâÔºåÁÑ∂ÂêéÊâßË°å‰ª•‰∏ãÂëΩ‰ª§ÔºåÂ∞ÜÂæÆ‰ø°ÂõæÁâáÊï∞ÊçÆ‰øùÂ≠òÂà∞`./dataset/wechat/dat`ÁõÆÂΩï‰∏ã„ÄÇ
```bash
python weclone/data/chat_parsers/wechat_parser.py --wechat-data-dir &quot;ÂæÆ‰ø°‰∏™‰∫∫Êñá‰ª∂Â§πË∑ØÂæÑ ‰æãÂ¶Ç C:\Users\user\Documents\WeChat Files\wxid_d68wiru2zseo22&quot;
```
‰πãÂêé‰ΩøÁî®[ÂæÆ‰ø°ÂõæÁâáËß£ÂØÜÂ∑•ÂÖ∑](https://github.com/Evil0ctal/WeChat-image-decryption)Ëß£ÂØÜÂõæÁâáÊï∞ÊçÆ,Ëß£ÂØÜÂêéÁöÑÂõæÁâáÊï∞ÊçÆ‰øùÂ≠òÂà∞`dataset/media/images`ÁõÆÂΩï‰∏ã„ÄÇ

## Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ

- È°πÁõÆÈªòËÆ§ÂéªÈô§‰∫ÜÊï∞ÊçÆ‰∏≠ÁöÑÊâãÊú∫Âè∑„ÄÅË∫´‰ªΩËØÅÂè∑„ÄÅÈÇÆÁÆ±„ÄÅÁΩëÂùÄ„ÄÇËøòÂú®`settings.jsonc`‰∏≠Êèê‰æõ‰∫Ü‰∏Ä‰∏™Á¶ÅÁî®ËØçËØçÂ∫ì`blocked_words`ÔºåÂèØ‰ª•Ëá™Ë°åÊ∑ªÂä†ÈúÄË¶ÅËøáÊª§ÁöÑËØçÂè•Ôºà‰ºöÈªòËÆ§ÂéªÊéâÂåÖÊã¨Á¶ÅÁî®ËØçÁöÑÊï¥Âè•Ôºâ„ÄÇ
&gt; [!IMPORTANT]
&gt; üö® ËØ∑‰∏ÄÂÆöÊ≥®ÊÑè‰øùÊä§‰∏™‰∫∫ÈöêÁßÅÔºå‰∏çË¶ÅÊ≥ÑÈú≤‰∏™‰∫∫‰ø°ÊÅØÔºÅ


- ÊâßË°å‰ª•‰∏ãÂëΩ‰ª§ÂØπÊï∞ÊçÆËøõË°åÂ§ÑÁêÜÔºåÂèØ‰ª•Ê†πÊçÆËá™Â∑±ÁöÑËÅäÂ§©È£éÊ†º‰øÆÊîπsettings.jsoncÁöÑ`make_dataset_args`„ÄÇ
```bash
weclone-cli make-dataset
```
- ÁõÆÂâç‰ªÖÊîØÊåÅÊó∂Èó¥Á™óÂè£Á≠ñÁï•ÔºåÊ†πÊçÆ`single_combine_time_window`Â∞ÜÂçï‰∫∫ËøûÁª≠Ê∂àÊÅØÈÄöËøáÈÄóÂè∑ËøûÊé•ÂêàÂπ∂‰∏∫‰∏ÄÂè•ÔºåÊ†πÊçÆ`qa_match_time_window`ÂåπÈÖçÈóÆÁ≠îÂØπ„ÄÇ
- Â§öÊ®°ÊÄÅÊï∞ÊçÆÂèØ‰ª•ÈÄöËøá`image_max_pixels`Âíå`max_image_num`ÂèÇÊï∞ÊéßÂà∂ÂõæÁâáÊï∞ÈáèÂíåÂ§ßÂ∞èÔºåÂáèÂ∞ëÊòæÂ≠òÂç†Áî®„ÄÇ
- ÂèØ‰ª•ÂêØÁî®`clean_dataset`‰∏≠ÁöÑ`enable_clean`ÈÄâÈ°πÔºåÂØπÊï∞ÊçÆËøõË°åÊ∏ÖÊ¥óÔºå‰ª•ËææÂà∞Êõ¥Â•ΩÊïàÊûúÔºàÂ§öÊ®°ÊÄÅÊï∞ÊçÆÊöÇ‰∏çÊîØÊåÅÔºâ„ÄÇ* ÂΩìÂâçÁ≥ªÁªüÊîØÊåÅ‰ΩøÁî® `llm judge` ÂØπËÅäÂ§©ËÆ∞ÂΩïËøõË°åÊâìÂàÜÔºåÊèê‰æõ **vllm Á¶ªÁ∫øÊé®ÁêÜ** Âíå **API Âú®Á∫øÊé®ÁêÜ** ‰∏§ÁßçÊñπÂºè„ÄÇÂèØÈÄöËøáÂ∞Ü `settings.jsonc` Êñá‰ª∂‰∏≠ÁöÑ `&quot;online_llm_clear&quot;: false` ‰øÆÊîπ‰∏∫ `true` Êù•ÂêØÁî® API Âú®Á∫øÊé®ÁêÜÊ®°ÂºèÔºåÂπ∂ÈÖçÁΩÆÁõ∏Â∫îÁöÑ `base_url`„ÄÅ`llm_api_key`„ÄÅ`model_name` Á≠âÂèÇÊï∞„ÄÇÊâÄÊúâÂÖºÂÆπ OpenAI Êé•Âè£ÁöÑÊ®°ÂûãÂùáÂèØÊé•ÂÖ•„ÄÇ
- Âú®Ëé∑Âæó `llm ÊâìÂàÜÂàÜÊï∞ÂàÜÂ∏ÉÊÉÖÂÜµ` ÂêéÔºåÂèØÈÄöËøáËÆæÁΩÆ `accept_score` ÂèÇÊï∞Á≠õÈÄâÂèØÊé•ÂèóÁöÑÂàÜÊï∞Âå∫Èó¥ÔºåÂêåÊó∂ÂèØÈÄÇÂΩìÈôç‰Ωé `train_sft_args` ‰∏≠ÁöÑ `lora_dropout` ÂèÇÊï∞Ôºå‰ª•ÊèêÂçáÊ®°ÂûãÁöÑÊãüÂêàÊïàÊûú„ÄÇ

## ÈÖçÁΩÆÂèÇÊï∞Âπ∂ÂæÆË∞ÉÊ®°Âûã

- (ÂèØÈÄâ)‰øÆÊîπ `settings.jsonc` ÁöÑ `model_name_or_path` Âíå `template` ÈÄâÊã©Êú¨Âú∞‰∏ãËΩΩÂ•ΩÁöÑÂÖ∂‰ªñÊ®°Âûã„ÄÇ  
- ‰øÆÊîπ`per_device_train_batch_size`‰ª•Âèä`gradient_accumulation_steps`Êù•Ë∞ÉÊï¥ÊòæÂ≠òÂç†Áî®„ÄÇ  
- ÂèØ‰ª•Ê†πÊçÆËá™Â∑±Êï∞ÊçÆÈõÜÁöÑÊï∞ÈáèÂíåË¥®Èáè‰øÆÊîπ`train_sft_args`ÁöÑ`num_train_epochs`„ÄÅ`lora_rank`„ÄÅ`lora_dropout`Á≠âÂèÇÊï∞„ÄÇ

### ÂçïÂç°ËÆ≠ÁªÉ
```bash
weclone-cli train-sft
```
Â§öÂç°ÁéØÂ¢ÉÂçïÂç°ËÆ≠ÁªÉÔºåÈúÄË¶ÅÂÖàÊâßË°å `export CUDA_VISIBLE_DEVICES=0`

### Â§öÂç°ËÆ≠ÁªÉ
ÂèñÊ∂à`settings.jsonc`‰∏≠`deepspeed`Ë°å‰ª£Á†ÅÊ≥®ÈáäÔºå‰ΩøÁî®‰ª•‰∏ãÂëΩ‰ª§Â§öÂç°ËÆ≠ÁªÉÔºö
```bash
uv pip install deepspeed
deepspeed --num_gpus=‰ΩøÁî®ÊòæÂç°Êï∞Èáè weclone/train/train_sft.py
```

### ‰ΩøÁî®ÊµèËßàÂô®demoÁÆÄÂçïÊé®ÁêÜ
ÂèØ‰ª•Âú®Ëøô‰∏ÄÊ≠•ÊµãËØïÂá∫ÂêàÈÄÇÁöÑtemperature„ÄÅtop_pÂÄºÔºå‰øÆÊîπsettings.jsoncÁöÑ`infer_args`ÂêéÔºå‰æõÂêéÁª≠Êé®ÁêÜÊó∂‰ΩøÁî®„ÄÇ
```bash
weclone-cli webchat-demo
```

### ‰ΩøÁî®Êé•Âè£ËøõË°åÊé®ÁêÜ

```bash
weclone-cli server
```

### ‰ΩøÁî®Â∏∏ËßÅËÅäÂ§©ÈóÆÈ¢òÊµãËØï
‰∏çÂåÖÂê´ËØ¢ÈóÆ‰∏™‰∫∫‰ø°ÊÅØÁöÑÈóÆÈ¢òÔºå‰ªÖÊúâÊó•Â∏∏ËÅäÂ§©„ÄÇÊµãËØïÁªìÊûúÂú®test_result-my.txt„ÄÇ
```bash
weclone-cli server
weclone-cli test-model
```

## üñºÔ∏è ÂæÆË∞ÉÊïàÊûú
‰ΩøÁî®Qwen2.5-14B-InstructÊ®°ÂûãÔºåÂ§ßÊ¶Ç3‰∏áÊù°Â§ÑÁêÜÂêéÁöÑÊúâÊïàÊï∞ÊçÆÔºålossÈôçÂà∞‰∫Ü3.5Â∑¶Âè≥ÁöÑÊïàÊûú„ÄÇ
&lt;details&gt;
&lt;summary&gt;Êà™Âõæ&lt;/summary&gt;
&lt;div style=&quot;display: flex; flex-wrap: wrap; gap: 10px;&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/0775ec52-452b-485f-9785-c6eb7b277132&quot; alt=&quot;alt text&quot; style=&quot;width: 48%; min-width: 150px;&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/8c7628b5-da70-4c37-9e51-fdfb0eadd2df&quot; alt=&quot;alt text&quot; style=&quot;width: 48%; min-width: 150px;&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/523aa742-2aa3-40e9-bd67-b98b336e83a8&quot; alt=&quot;alt text&quot; style=&quot;width: 48%; min-width: 150px;&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/dabf0603-dcc4-4a47-b5c3-2bbc036820d9&quot; alt=&quot;alt text&quot; style=&quot;width: 48%; min-width: 150px;&quot;&gt;
&lt;/div&gt;
&lt;/details&gt;


## ü§ñ ÈÉ®ÁΩ≤Âà∞ËÅäÂ§©Êú∫Âô®‰∫∫

### AstrBot

[AstrBot](https://github.com/AstrBotDevs/AstrBot) ÊòØÊòì‰∏äÊâãÁöÑÂ§öÂπ≥Âè∞ LLM ËÅäÂ§©Êú∫Âô®‰∫∫ÂèäÂºÄÂèëÊ°ÜÊû∂ ‚ú® Âπ≥Âè∞ÊîØÊåÅ QQ„ÄÅQQÈ¢ëÈÅì„ÄÅTelegram„ÄÅÂæÆ‰ø°„ÄÅ‰ºÅÂæÆ„ÄÅÈ£û‰π¶„ÄÇ      

‰ΩøÁî®Ê≠•È™§Ôºö
1. ÈÉ®ÁΩ≤ AstrBot
2. Âú® AstrBot ‰∏≠ÈÉ®ÁΩ≤Ê∂àÊÅØÂπ≥Âè∞
3. ÊâßË°å `weclone-cli server` ÂêØÂä®apiÊúçÂä°
4. Âú® AstrBot ‰∏≠Êñ∞Â¢ûÊúçÂä°Êèê‰æõÂïÜÔºåÁ±ªÂûãÈÄâÊã©OpenAIÔºåAPI Base URL Ê†πÊçÆAstrBotÈÉ®ÁΩ≤ÊñπÂºèÂ°´ÂÜôÔºà‰æãÂ¶ÇdockerÈÉ®ÁΩ≤ÂèØËÉΩ‰∏∫http://172.17.0.1:8005/v1Ôºâ ÔºåÊ®°ÂûãÂ°´ÂÜôgpt-3.5-turbo,API KeyÈöèÊÑèÂ°´ÂÜô‰∏Ä‰∏™
5. ÂæÆË∞ÉÂêé‰∏çÊîØÊåÅÂ∑•ÂÖ∑Ë∞ÉÁî®ÔºåËØ∑ÂÖàÂÖ≥ÊéâÈªòËÆ§ÁöÑÂ∑•ÂÖ∑ÔºåÊ∂àÊÅØÂπ≥Âè∞ÂèëÈÄÅÊåá‰ª§Ôºö `/tool off all`ÔºåÂê¶Âàô‰ºöÊ≤°ÊúâÂæÆË∞ÉÂêéÁöÑÊïàÊûú„ÄÇ 
6. Ê†πÊçÆÂæÆË∞ÉÊó∂‰ΩøÁî®ÁöÑdefault_systemÔºåÂú® AstrBot ‰∏≠ËÆæÁΩÆÁ≥ªÁªüÊèêÁ§∫ËØç„ÄÇ
![5](https://github.com/user-attachments/assets/19de7072-076a-4cdf-8ae6-46b9b89f536a)
&gt; [!IMPORTANT]
&gt; Ê£ÄÊü•api_serviceÁöÑÊó•ÂøóÔºåÂ∞ΩÈáè‰øùËØÅÂ§ßÊ®°ÂûãÊúçÂä°ËØ∑Ê±ÇÁöÑÂèÇÊï∞ÂíåÂæÆË∞ÉÊó∂‰∏ÄËá¥ÔºåtoolÊèí‰ª∂ËÉΩÂäõÈÉΩÂÖ≥Êéâ„ÄÇ
7. Ë∞ÉÊï¥ÈááÊ†∑ÂèÇÊï∞Ôºå‰æãÂ¶Çtemperature„ÄÅtop_p„ÄÅtop_kÁ≠â
[ÈÖçÁΩÆËá™ÂÆö‰πâÁöÑÊ®°ÂûãÂèÇÊï∞](https://astrbot.app/config/model-config.html#%E9%85%8D%E7%BD%AE%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0)

### LangBot

[LangBot](https://github.com/RockChinQ/LangBot) ÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÊé•ÂÖ•ÂÖ®ÁêÉÂ§öÁßçÂç≥Êó∂ÈÄö‰ø°Âπ≥Âè∞ÁöÑ LLM Êú∫Âô®‰∫∫Âπ≥Âè∞ÔºåÈÄÇÂêàÂêÑÁßçÂú∫ÊôØ‰ΩøÁî®„ÄÇ

1. [ÈÉ®ÁΩ≤ LangBot](https://github.com/RockChinQ/LangBot#-%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8)
2. Âú® LangBot ‰∏≠Ê∑ªÂä†‰∏Ä‰∏™Êú∫Âô®‰∫∫
4. Âú®Ê®°ÂûãÈ°µÊ∑ªÂä†Êñ∞Ê®°ÂûãÔºåÂêçÁß∞`gpt-3.5-turbo`Ôºå‰æõÂ∫îÂïÜÈÄâÊã© OpenAIÔºåÂ°´ÂÜô ËØ∑Ê±Ç URL ‰∏∫ WeClone ÁöÑÂú∞ÂùÄÔºåËØ¶ÁªÜËøûÊé•ÊñπÂºèÂèØ‰ª•ÂèÇËÄÉ[ÊñáÊ°£](https://docs.langbot.app/zh/workshop/network-details.html)ÔºåAPI Key ‰ªªÊÑèÂ°´ÂÜô„ÄÇ

&lt;img width=&quot;400px&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/fc167dea-7c93-4d94-9c5f-db709d0320ba&quot; /&gt;

6. Âú®ÊµÅÊ∞¥Á∫øÈÖçÁΩÆ‰∏≠ÈÄâÊã©ÂàöÊâçÊ∑ªÂä†ÁöÑÊ®°ÂûãÔºåÊàñ‰øÆÊîπÊèêÁ§∫ËØçÈÖçÁΩÆ

&lt;img width=&quot;400px&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/dbb0fd0a-f760-42db-acd0-bb99c859b52e&quot; /&gt;

## üìå Ë∑ØÁ∫øÂõæ
- [ ] Êõ¥‰∏∞ÂØåÁöÑ‰∏ä‰∏ãÊñáÔºöÂåÖÊã¨‰∏ä‰∏ãÊñáÂØπËØù„ÄÅËÅäÂ§©ÂØπË±°‰ø°ÊÅØ„ÄÅÊó∂Èó¥Á≠â + ÊÄùËÄÉ
- [ ] Memory ÊîØÊåÅ
- [ ] ÊîØÊåÅÂ§öÊ®°ÊÄÅ
- [ ] Êï∞ÊçÆÂ¢ûÂº∫
- [ ] ÊîØÊåÅGUI

## ÈóÆÈ¢òËß£ÂÜ≥
- ÂæÆË∞ÉÈóÆÈ¢òÔºö[LLaMA-Factory| FAQs | Â∏∏ËßÅÈóÆÈ¢ò](https://github.com/hiyouga/LLaMA-Factory/issues/4614) ÊàñËÄÖÊõ¥Êñπ‰æøÁöÑ [![Êõ¥Êñπ‰æøÁöÑAsk DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/hiyouga/LLaMA-Factory)

## ‚ù§Ô∏è Ë¥°ÁåÆ‰ª£Á†Å

Ê¨¢Ëøé‰ªª‰Ωï Issues/Pull RequestsÔºÅ

‰Ω†ÂèØ‰ª•ÈÄöËøáÊü•ÁúãIssuesÊàñÂ∏ÆÂä©ÂÆ°Ê†∏ PRÔºàÊãâÂèñËØ∑Ê±ÇÔºâÊù•Ë¥°ÁåÆ„ÄÇÂØπ‰∫éÊñ∞ÂäüËÉΩÁöÑÊ∑ªÂä†ÔºåËØ∑ÂÖàÈÄöËøá Issue ËÆ®ËÆ∫„ÄÇ   
ËøêË°å`uv pip install --group dev -e .`ÂÆâË£ÖÂºÄÂèë‰æùËµñ„ÄÇ   
È°πÁõÆ‰ΩøÁî®`pytest`ÊµãËØï(ÊµãËØïËÑöÊú¨ÂæÖÂÆåÂñÑ)Ôºå`pyright`Ê£ÄÊü•Á±ªÂûãÔºå`ruff`Ê£ÄÊü•‰ª£Á†ÅÊ†ºÂºè„ÄÇ


## ‚ö†Ô∏è ÂÖçË¥£Â£∞Êòé
&gt; [!CAUTION]
&gt; ËØ∑ÂãøÁî®‰∫éÈùûÊ≥ïÁî®ÈÄîÔºåÂê¶ÂàôÂêéÊûúËá™Ë¥ü„ÄÇ
&lt;details&gt;
&lt;summary&gt;1. ‰ΩøÁî®ÁõÆÁöÑ&lt;/summary&gt;

* Êú¨È°πÁõÆ‰ªÖ‰æõÂ≠¶‰π†‰∫§ÊµÅ‰ΩøÁî®Ôºå**ËØ∑ÂãøÁî®‰∫éÈùûÊ≥ïÁî®ÈÄî**Ôºå**ËØ∑ÂãøÁî®‰∫éÈùûÊ≥ïÁî®ÈÄî**Ôºå**ËØ∑ÂãøÁî®‰∫éÈùûÊ≥ïÁî®ÈÄî**ÔºåÂê¶ÂàôÂêéÊûúËá™Ë¥ü„ÄÇ
* Áî®Êà∑ÁêÜËß£Âπ∂ÂêåÊÑèÔºå‰ªª‰ΩïËøùÂèçÊ≥ïÂæãÊ≥ïËßÑ„ÄÅ‰æµÁäØ‰ªñ‰∫∫ÂêàÊ≥ïÊùÉÁõäÁöÑË°å‰∏∫ÔºåÂùá‰∏éÊú¨È°πÁõÆÂèäÂÖ∂ÂºÄÂèëËÄÖÊó†ÂÖ≥ÔºåÂêéÊûúÁî±Áî®Êà∑Ëá™Ë°åÊâøÊãÖ„ÄÇ

2. ‰ΩøÁî®ÊúüÈôê

* ÊÇ®Â∫îËØ•Âú®‰∏ãËΩΩ‰øùÂ≠ò‰ΩøÁî®Êú¨È°πÁõÆÁöÑ24Â∞èÊó∂ÂÜÖÔºåÂà†Èô§Êú¨È°πÁõÆÁöÑÊ∫ê‰ª£Á†ÅÂíåÁ®ãÂ∫èÔºõË∂ÖÂá∫Ê≠§ÊúüÈôêÁöÑ‰ªª‰Ωï‰ΩøÁî®Ë°å‰∏∫Ôºå‰∏ÄÊ¶Ç‰∏éÊú¨È°πÁõÆÂèäÂÖ∂ÂºÄÂèëËÄÖÊó†ÂÖ≥„ÄÇ

3. Êìç‰ΩúËßÑËåÉ

* Êú¨È°πÁõÆ‰ªÖÂÖÅËÆ∏Âú®ÊéàÊùÉÊÉÖÂÜµ‰∏ã‰ΩøÁî®Êï∞ÊçÆËÆ≠ÁªÉÔºå‰∏•Á¶ÅÁî®‰∫éÈùûÊ≥ïÁõÆÁöÑÔºåÂê¶ÂàôËá™Ë°åÊâøÊãÖÊâÄÊúâÁõ∏ÂÖ≥Ë¥£‰ªªÔºõÁî®Êà∑Â¶ÇÂõ†ËøùÂèçÊ≠§ËßÑÂÆöËÄåÂºïÂèëÁöÑ‰ªª‰ΩïÊ≥ïÂæãË¥£‰ªªÔºåÂ∞ÜÁî±Áî®Êà∑Ëá™Ë°åÊâøÊãÖÔºå‰∏éÊú¨È°πÁõÆÂèäÂÖ∂ÂºÄÂèëËÄÖÊó†ÂÖ≥„ÄÇ
* ‰∏•Á¶ÅÁî®‰∫éÁ™ÉÂèñ‰ªñ‰∫∫ÈöêÁßÅÔºå‰∏•Á¶ÅÁî®‰∫éÁ™ÉÂèñ‰ªñ‰∫∫ÈöêÁßÅÔºå‰∏•Á¶ÅÁî®‰∫éÁ™ÉÂèñ‰ªñ‰∫∫ÈöêÁßÅÔºåÂê¶ÂàôËá™Ë°åÊâøÊãÖÊâÄÊúâÁõ∏ÂÖ≥Ë¥£‰ªª„ÄÇ

4. ÂÖçË¥£Â£∞ÊòéÊé•Âèó

* ‰∏ãËΩΩ„ÄÅ‰øùÂ≠ò„ÄÅËøõ‰∏ÄÊ≠•ÊµèËßàÊ∫ê‰ª£Á†ÅÊàñËÄÖ‰∏ãËΩΩÂÆâË£Ö„ÄÅÁºñËØë‰ΩøÁî®Êú¨Á®ãÂ∫èÔºåË°®Á§∫‰Ω†ÂêåÊÑèÊú¨Ë≠¶ÂëäÔºåÂπ∂ÊâøËØ∫ÈÅµÂÆàÂÆÉ;

5. Á¶ÅÊ≠¢Áî®‰∫éÈùûÊ≥ïÊµãËØïÊàñÊ∏óÈÄè

* Á¶ÅÊ≠¢Âà©Áî®Êú¨È°πÁõÆÁöÑÁõ∏ÂÖ≥ÊäÄÊúØ‰ªé‰∫ãÈùûÊ≥ïÊµãËØïÊàñÊ∏óÈÄèÔºåÁ¶ÅÊ≠¢Âà©Áî®Êú¨È°πÁõÆÁöÑÁõ∏ÂÖ≥‰ª£Á†ÅÊàñÁõ∏ÂÖ≥ÊäÄÊúØ‰ªé‰∫ã‰ªª‰ΩïÈùûÊ≥ïÂ∑•‰ΩúÔºåÂ¶ÇÂõ†Ê≠§‰∫ßÁîüÁöÑ‰∏ÄÂàá‰∏çËâØÂêéÊûú‰∏éÊú¨È°πÁõÆÂèäÂÖ∂ÂºÄÂèëËÄÖÊó†ÂÖ≥„ÄÇ
* ‰ªª‰ΩïÂõ†Ê≠§‰∫ßÁîüÁöÑ‰∏çËâØÂêéÊûúÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÊï∞ÊçÆÊ≥ÑÈú≤„ÄÅÁ≥ªÁªüÁò´Áó™„ÄÅ‰æµÁäØÈöêÁßÅÁ≠âÔºåÂùá‰∏éÊú¨È°πÁõÆÂèäÂÖ∂ÂºÄÂèëËÄÖÊó†ÂÖ≥ÔºåË¥£‰ªªÁî±Áî®Êà∑Ëá™Ë°åÊâøÊãÖ„ÄÇ

6. ÂÖçË¥£Â£∞Êòé‰øÆÊîπ

* Êú¨ÂÖçË¥£Â£∞ÊòéÂèØËÉΩÊ†πÊçÆÈ°πÁõÆËøêË°åÊÉÖÂÜµÂíåÊ≥ïÂæãÊ≥ïËßÑÁöÑÂèòÂåñËøõË°å‰øÆÊîπÂíåË∞ÉÊï¥„ÄÇÁî®Êà∑Â∫îÂÆöÊúüÊü•ÈòÖÊú¨È°µÈù¢‰ª•Ëé∑ÂèñÊúÄÊñ∞ÁâàÊú¨ÁöÑÂÖçË¥£Â£∞ÊòéÔºå‰ΩøÁî®Êú¨È°πÁõÆÊó∂Â∫îÈÅµÂÆàÊúÄÊñ∞ÁâàÊú¨ÁöÑÂÖçË¥£Â£∞Êòé„ÄÇ

7. ÂÖ∂‰ªñ

* Èô§Êú¨ÂÖçË¥£Â£∞ÊòéËßÑÂÆöÂ§ñÔºåÁî®Êà∑Âú®‰ΩøÁî®Êú¨È°πÁõÆËøáÁ®ã‰∏≠Â∫îÈÅµÂÆàÁõ∏ÂÖ≥ÁöÑÊ≥ïÂæãÊ≥ïËßÑÂíåÈÅìÂæ∑ËßÑËåÉ„ÄÇÂØπ‰∫éÂõ†Áî®Êà∑ËøùÂèçÁõ∏ÂÖ≥ËßÑÂÆöËÄåÂºïÂèëÁöÑ‰ªª‰ΩïÁ∫†Á∫∑ÊàñÊçüÂ§±ÔºåÊú¨È°πÁõÆÂèäÂÖ∂ÂºÄÂèëËÄÖ‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªª„ÄÇ

* ËØ∑Áî®Êà∑ÊÖéÈáçÈòÖËØªÂπ∂ÁêÜËß£Êú¨ÂÖçË¥£Â£∞ÊòéÁöÑÊâÄÊúâÂÜÖÂÆπÔºåÁ°Æ‰øùÂú®‰ΩøÁî®Êú¨È°πÁõÆÊó∂‰∏•Ê†ºÈÅµÂÆàÁõ∏ÂÖ≥ËßÑÂÆö„ÄÇ

&lt;/details&gt;
ËØ∑Áî®Êà∑ÊÖéÈáçÈòÖËØªÂπ∂ÁêÜËß£Êú¨ÂÖçË¥£Â£∞ÊòéÁöÑÊâÄÊúâÂÜÖÂÆπÔºåÁ°Æ‰øùÂú®‰ΩøÁî®Êú¨È°πÁõÆÊó∂‰∏•Ê†ºÈÅµÂÆàÁõ∏ÂÖ≥ËßÑÂÆö„ÄÇ

&lt;br&gt;  
&lt;br&gt;  
&lt;br&gt;  

## ‚≠ê Star History
&gt; [!TIP] 
&gt; Â¶ÇÊûúÊú¨È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåÊàñËÄÖÊÇ®ÂÖ≥Ê≥®Êú¨È°πÁõÆÁöÑÊú™Êù•ÂèëÂ±ïÔºåËØ∑ÁªôÈ°πÁõÆ StarÔºåË∞¢Ë∞¢ 

&lt;div align=&quot;center&quot;&gt;

[![Star History Chart](https://api.star-history.com/svg?repos=xming521/WeClone&amp;type=Date)](https://www.star-history.com/#xming521/WeClone&amp;Date)

&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt; ÂÖãÈöÜÊàë‰ª¨Ôºå‰øùÁïôÁÅµÈ≠ÇÁöÑËä¨Ëä≥ &lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Shubhamsaboo/awesome-llm-apps]]></title>
            <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
            <guid>https://github.com/Shubhamsaboo/awesome-llm-apps</guid>
            <pubDate>Sun, 08 Jun 2025 00:04:40 GMT</pubDate>
            <description><![CDATA[Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Shubhamsaboo/awesome-llm-apps">Shubhamsaboo/awesome-llm-apps</a></h1>
            <p>Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p>
            <p>Language: Python</p>
            <p>Stars: 33,535</p>
            <p>Forks: 3,822</p>
            <p>Stars today: 151 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;http://www.theunwindai.com&quot;&gt;
    &lt;img src=&quot;docs/banner/unwind_black.png&quot; width=&quot;900px&quot; alt=&quot;Unwind AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.linkedin.com/in/shubhamsaboo/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;style=flat-square&quot; alt=&quot;LinkedIn&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/Saboo_Shubham_&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/Shubham_Saboo&quot; alt=&quot;Twitter&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;hr/&gt;

# üåü Awesome LLM Apps

A curated collection of **Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.** This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/9876&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/9876&quot; alt=&quot;Shubhamsaboo%2Fawesome-llm-apps | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## ü§î Why Awesome LLM Apps?

- üí° Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.
- üî• Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp; RAG.
- üéì Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.

## üìÇ Featured AI Projects

### AI Agents

### üå± Starter AI Agents

*   [üéôÔ∏è AI Blog to Podcast Agent](starter_ai_agents/ai_blog_to_podcast_agent/)
*   [‚ù§Ô∏è‚Äçü©π AI Breakup Recovery Agent](starter_ai_agents/ai_breakup_recovery_agent/)
*   [üìä AI Data Analysis Agent](starter_ai_agents/ai_data_analysis_agent/)
*   [ü©ª AI Medical Imaging Agent](starter_ai_agents/ai_medical_imaging_agent/)
*   [üòÇ AI Meme Generator Agent (Browser)](starter_ai_agents/ai_meme_generator_agent_browseruse/)
*   [üéµ AI Music Generator Agent](starter_ai_agents/ai_music_generator_agent/)
*   [üõ´ AI Travel Agent (Local &amp; Cloud)](starter_ai_agents/ai_travel_agent/)
*   [‚ú® Gemini Multimodal Agent](starter_ai_agents/gemini_multimodal_agent_demo/)
*   [üåê Local News Agent (OpenAI Swarm)](starter_ai_agents/local_news_agent_openai_swarm/)
*   [üîÑ Mixture of Agents](starter_ai_agents/mixture_of_agents/)
*   [üìä xAI Finance Agent](starter_ai_agents/xai_finance_agent/)
*   [üîç OpenAI Research Agent](starter_ai_agents/opeani_research_agent/)
*   [üï∏Ô∏è Web Scrapping AI Agent (Local &amp; Cloud)](starter_ai_agents/web_scrapping_ai_agent/)

### üöÄ Advanced AI Agents

*   [üîç AI Deep Research Agent](advanced_ai_agents/single_agent_apps/ai_deep_research_agent/)
*   [üèóÔ∏è AI System Architect Agent](advanced_ai_agents/single_agent_apps/ai_system_architect_r1/)
*   [üéØ AI Lead Generation Agent](advanced_ai_agents/single_agent_apps/ai_lead_generation_agent/)
*   [üí∞ AI Financial Coach Agent](advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/)
*   [üé¨ AI Movie Production Agent](advanced_ai_agents/single_agent_apps/ai_movie_production_agent/)
*   [üìà AI Investment Agent](advanced_ai_agents/single_agent_apps/ai_investment_agent/)
*   [üèãÔ∏è‚Äç‚ôÇÔ∏è AI Health &amp; Fitness Agent](advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/)
*   [üöÄ AI Product Launch Intelligence Agent](advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent)
*   [üóûÔ∏è AI Journalist Agent](advanced_ai_agents/single_agent_apps/ai_journalist_agent/)
*   [üß† AI Mental Wellbeing Agent](advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/)
*   [üìë AI Meeting Agent](advanced_ai_agents/single_agent_apps/ai_meeting_agent/)
*   [üß¨ AI Self-Evolving Agent](advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/)

### üéÆ Autonomous Game Playing Agents

*   [üéÆ AI 3D Pygame Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/)
*   [‚ôú AI Chess Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/)
*   [üé≤ AI Tic-Tac-Toe Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/)

### ü§ù Multi-agent Teams

*   [üß≤ AI Competitor Intelligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/)
*   [üí≤ AI Finance Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/)
*   [üé® AI Game Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/)
*   [üë®‚Äç‚öñÔ∏è AI Legal Agent Team (Cloud &amp; Local)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/)
*   [üíº AI Recruitment Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/)
*   [üë®‚Äçüíº AI Services Agency (CrewAI)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/)
*   [üë®‚Äçüè´ AI Teaching Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/)
*   [üíª Multimodal Coding Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/)
*   [‚ú® Multimodal Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/)

### üó£Ô∏è Voice AI Agents

*   [üó£Ô∏è AI Audio Tour Agent](voice_ai_agents/ai_audio_tour_agent/)
*   [üìû Customer Support Voice Agent](voice_ai_agents/customer_support_voice_agent/)
*   [üîä Voice RAG Agent (OpenAI SDK)](voice_ai_agents/voice_rag_openaisdk/)


### üåê MCP AI Agents

*   [‚ôæÔ∏è Browser MCP Agent](mcp_ai_agents/browser_mcp_agent/)
*   [üêô GitHub MCP Agent](mcp_ai_agents/github_mcp_agent/)
*   [üìë Notion MCP Agent](mcp_ai_agents/notion_mcp_agent) 
*   [üåç AI Travel Planner MCP Agent](mcp_ai_agents/ai_travel_planner_mcp_agent_team)

### üìÄ RAG (Retrieval Augmented Generation)
*   [üîó Agentic RAG](rag_tutorials/agentic_rag/)
*   [üßê Agentic RAG with Reasoning](rag_tutorials/agentic_rag_with_reasoning/)
*   [üì∞ AI Blog Search (RAG)](rag_tutorials/ai_blog_search/)
*   [üîç Autonomous RAG](rag_tutorials/autonomous_rag/)
*   [üîÑ Corrective RAG (CRAG)](rag_tutorials/corrective_rag/)
*   [üêã Deepseek Local RAG Agent](rag_tutorials/deepseek_local_rag_agent/)
*   [ü§î Gemini Agentic RAG](rag_tutorials/gemini_agentic_rag/)
*   [üëÄ Hybrid Search RAG (Cloud)](rag_tutorials/hybrid_search_rag/)
*   [üîÑ Llama 3.1 Local RAG](rag_tutorials/llama3.1_local_rag/)
*   [üñ•Ô∏è Local Hybrid Search RAG](rag_tutorials/local_hybrid_search_rag/)
*   [ü¶ô Local RAG Agent](rag_tutorials/local_rag_agent/)
*   [üß© RAG-as-a-Service](rag_tutorials/rag-as-a-service/)
*   [‚ú® RAG Agent with Cohere](rag_tutorials/rag_agent_cohere/)
*   [‚õìÔ∏è Basic RAG Chain](rag_tutorials/rag_chain/)
*   [üì† RAG with Database Routing](rag_tutorials/rag_database_routing/)
*   [üñºÔ∏è Vision RAG](rag_tutorials/vision_rag/)

### üíæ LLM Apps with Memory Tutorials

*   [üíæ AI ArXiv Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/)
*   [üõ©Ô∏è AI Travel Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/)
*   [üí¨ Llama3 Stateful Chat](advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/)
*   [üìù LLM App with Personalized Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/)
*   [üóÑÔ∏è Local ChatGPT Clone with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/)
*   [üß† Multi-LLM Application with Shared Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/)


### üí¨ Chat with X Tutorials

*   [üí¨ Chat with GitHub (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_github/)
*   [üì® Chat with Gmail](advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/)
*   [üìÑ Chat with PDF (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/)
*   [üìö Chat with Research Papers (ArXiv) (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/)
*   [üìù Chat with Substack](advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/)
*   [üìΩÔ∏è Chat with YouTube Videos](advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/)

### üîß LLM Fine-tuning Tutorials

*   [üîß Llama 3.2 Fine-tuning](advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/)

## üöÄ Getting Started

1. **Clone the repository** 

    ```bash 
    git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
    ```

2. **Navigate to the desired project directory**

    ```bash 
    cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
    ```

3. **Install the required dependencies**

    ```bash
    pip install -r requirements.txt
    ```

4. **Follow the project-specific instructions** in each project&#039;s `README.md` file to set up and run the app.

## ü§ù Contributing to Open Source

Contributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new [GitHub Issue](https://github.com/Shubhamsaboo/awesome-llm-apps/issues) or submit a pull request. Make sure to follow the existing project structure and include a detailed `README.md` for each new app.

### Thank You, Community, for the Support! üôè

[![Star History Chart](https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;type=Date)](https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;Date)

üåü **Don‚Äôt miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>