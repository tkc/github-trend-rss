<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sun, 18 Jan 2026 00:05:37 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[google/langextract]]></title>
            <link>https://github.com/google/langextract</link>
            <guid>https://github.com/google/langextract</guid>
            <pubDate>Sun, 18 Jan 2026 00:05:37 GMT</pubDate>
            <description><![CDATA[A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google/langextract">google/langextract</a></h1>
            <p>A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.</p>
            <p>Language: Python</p>
            <p>Stars: 21,582</p>
            <p>Forks: 1,497</p>
            <p>Stars today: 425 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/google/langextract&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/google/langextract/main/docs/_static/logo.svg&quot; alt=&quot;LangExtract Logo&quot; width=&quot;128&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

# LangExtract

[![PyPI version](https://img.shields.io/pypi/v/langextract.svg)](https://pypi.org/project/langextract/)
[![GitHub stars](https://img.shields.io/github/stars/google/langextract.svg?style=social&amp;label=Star)](https://github.com/google/langextract)
![Tests](https://github.com/google/langextract/actions/workflows/ci.yaml/badge.svg)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.17015089.svg)](https://doi.org/10.5281/zenodo.17015089)

## Table of Contents

- [Introduction](#introduction)
- [Why LangExtract?](#why-langextract)
- [Quick Start](#quick-start)
- [Installation](#installation)
- [API Key Setup for Cloud Models](#api-key-setup-for-cloud-models)
- [Adding Custom Model Providers](#adding-custom-model-providers)
- [Using OpenAI Models](#using-openai-models)
- [Using Local LLMs with Ollama](#using-local-llms-with-ollama)
- [More Examples](#more-examples)
  - [*Romeo and Juliet* Full Text Extraction](#romeo-and-juliet-full-text-extraction)
  - [Medication Extraction](#medication-extraction)
  - [Radiology Report Structuring: RadExtract](#radiology-report-structuring-radextract)
- [Community Providers](#community-providers)
- [Contributing](#contributing)
- [Testing](#testing)
- [Disclaimer](#disclaimer)

## Introduction

LangExtract is a Python library that uses LLMs to extract structured information from unstructured text documents based on user-defined instructions. It processes materials such as clinical notes or reports, identifying and organizing key details while ensuring the extracted data corresponds to the source text.

## Why LangExtract?

1.  **Precise Source Grounding:** Maps every extraction to its exact location in the source text, enabling visual highlighting for easy traceability and verification.
2.  **Reliable Structured Outputs:** Enforces a consistent output schema based on your few-shot examples, leveraging controlled generation in supported models like Gemini to guarantee robust, structured results.
3.  **Optimized for Long Documents:** Overcomes the &quot;needle-in-a-haystack&quot; challenge of large document extraction by using an optimized strategy of text chunking, parallel processing, and multiple passes for higher recall.
4.  **Interactive Visualization:** Instantly generates a self-contained, interactive HTML file to visualize and review thousands of extracted entities in their original context.
5.  **Flexible LLM Support:** Supports your preferred models, from cloud-based LLMs like the Google Gemini family to local open-source models via the built-in Ollama interface.
6.  **Adaptable to Any Domain:** Define extraction tasks for any domain using just a few examples. LangExtract adapts to your needs without requiring any model fine-tuning.
7.  **Leverages LLM World Knowledge:** Utilize precise prompt wording and few-shot examples to influence how the extraction task may utilize LLM knowledge. The accuracy of any inferred information and its adherence to the task specification are contingent upon the selected LLM, the complexity of the task, the clarity of the prompt instructions, and the nature of the prompt examples.

## Quick Start

&gt; **Note:** Using cloud-hosted models like Gemini requires an API key. See the [API Key Setup](#api-key-setup-for-cloud-models) section for instructions on how to get and configure your key.

Extract structured information with just a few lines of code.

### 1. Define Your Extraction Task

First, create a prompt that clearly describes what you want to extract. Then, provide a high-quality example to guide the model.

```python
import langextract as lx
import textwrap

# 1. Define the prompt and extraction rules
prompt = textwrap.dedent(&quot;&quot;&quot;\
    Extract characters, emotions, and relationships in order of appearance.
    Use exact text for extractions. Do not paraphrase or overlap entities.
    Provide meaningful attributes for each entity to add context.&quot;&quot;&quot;)

# 2. Provide a high-quality example to guide the model
examples = [
    lx.data.ExampleData(
        text=&quot;ROMEO. But soft! What light through yonder window breaks? It is the east, and Juliet is the sun.&quot;,
        extractions=[
            lx.data.Extraction(
                extraction_class=&quot;character&quot;,
                extraction_text=&quot;ROMEO&quot;,
                attributes={&quot;emotional_state&quot;: &quot;wonder&quot;}
            ),
            lx.data.Extraction(
                extraction_class=&quot;emotion&quot;,
                extraction_text=&quot;But soft!&quot;,
                attributes={&quot;feeling&quot;: &quot;gentle awe&quot;}
            ),
            lx.data.Extraction(
                extraction_class=&quot;relationship&quot;,
                extraction_text=&quot;Juliet is the sun&quot;,
                attributes={&quot;type&quot;: &quot;metaphor&quot;}
            ),
        ]
    )
]
```

&gt; **Note:** Examples drive model behavior. Each `extraction_text` should ideally be verbatim from the example&#039;s `text` (no paraphrasing), listed in order of appearance. LangExtract raises `Prompt alignment` warnings by default if examples don&#039;t follow this patternâ€”resolve these for best results.

### 2. Run the Extraction

Provide your input text and the prompt materials to the `lx.extract` function.

```python
# The input text to be processed
input_text = &quot;Lady Juliet gazed longingly at the stars, her heart aching for Romeo&quot;

# Run the extraction
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gemini-2.5-flash&quot;,
)
```

&gt; **Model Selection**: `gemini-2.5-flash` is the recommended default, offering an excellent balance of speed, cost, and quality. For highly complex tasks requiring deeper reasoning, `gemini-2.5-pro` may provide superior results. For large-scale or production use, a Tier 2 Gemini quota is suggested to increase throughput and avoid rate limits. See the [rate-limit documentation](https://ai.google.dev/gemini-api/docs/rate-limits#tier-2) for details.
&gt;
&gt; **Model Lifecycle**: Note that Gemini models have a lifecycle with defined retirement dates. Users should consult the [official model version documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions) to stay informed about the latest stable and legacy versions.

### 3. Visualize the Results

The extractions can be saved to a `.jsonl` file, a popular format for working with language model data. LangExtract can then generate an interactive HTML visualization from this file to review the entities in context.

```python
# Save the results to a JSONL file
lx.io.save_annotated_documents([result], output_name=&quot;extraction_results.jsonl&quot;, output_dir=&quot;.&quot;)

# Generate the visualization from the file
html_content = lx.visualize(&quot;extraction_results.jsonl&quot;)
with open(&quot;visualization.html&quot;, &quot;w&quot;) as f:
    if hasattr(html_content, &#039;data&#039;):
        f.write(html_content.data)  # For Jupyter/Colab
    else:
        f.write(html_content)
```

This creates an animated and interactive HTML file:

![Romeo and Juliet Basic Visualization ](https://raw.githubusercontent.com/google/langextract/main/docs/_static/romeo_juliet_basic.gif)

&gt; **Note on LLM Knowledge Utilization:** This example demonstrates extractions that stay close to the text evidence - extracting &quot;longing&quot; for Lady Juliet&#039;s emotional state and identifying &quot;yearning&quot; from &quot;gazed longingly at the stars.&quot; The task could be modified to generate attributes that draw more heavily from the LLM&#039;s world knowledge (e.g., adding `&quot;identity&quot;: &quot;Capulet family daughter&quot;` or `&quot;literary_context&quot;: &quot;tragic heroine&quot;`). The balance between text-evidence and knowledge-inference is controlled by your prompt instructions and example attributes.

### Scaling to Longer Documents

For larger texts, you can process entire documents directly from URLs with parallel processing and enhanced sensitivity:

```python
# Process Romeo &amp; Juliet directly from Project Gutenberg
result = lx.extract(
    text_or_documents=&quot;https://www.gutenberg.org/files/1513/1513-0.txt&quot;,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gemini-2.5-flash&quot;,
    extraction_passes=3,    # Improves recall through multiple passes
    max_workers=20,         # Parallel processing for speed
    max_char_buffer=1000    # Smaller contexts for better accuracy
)
```

This approach can extract hundreds of entities from full novels while maintaining high accuracy. The interactive visualization seamlessly handles large result sets, making it easy to explore hundreds of entities from the output JSONL file. **[See the full *Romeo and Juliet* extraction example â†’](https://github.com/google/langextract/blob/main/docs/examples/longer_text_example.md)** for detailed results and performance insights.

### Vertex AI Batch Processing

Save costs on large-scale tasks by enabling Vertex AI Batch API: `language_model_params={&quot;vertexai&quot;: True, &quot;batch&quot;: {&quot;enabled&quot;: True}}`.

See an example of the Vertex AI Batch API usage in [this example](docs/examples/batch_api_example.md).

## Installation

### From PyPI

```bash
pip install langextract
```

*Recommended for most users. For isolated environments, consider using a virtual environment:*

```bash
python -m venv langextract_env
source langextract_env/bin/activate  # On Windows: langextract_env\Scripts\activate
pip install langextract
```

### From Source

LangExtract uses modern Python packaging with `pyproject.toml` for dependency management:

*Installing with `-e` puts the package in development mode, allowing you to modify the code without reinstalling.*


```bash
git clone https://github.com/google/langextract.git
cd langextract

# For basic installation:
pip install -e .

# For development (includes linting tools):
pip install -e &quot;.[dev]&quot;

# For testing (includes pytest):
pip install -e &quot;.[test]&quot;
```

### Docker

```bash
docker build -t langextract .
docker run --rm -e LANGEXTRACT_API_KEY=&quot;your-api-key&quot; langextract python your_script.py
```

## API Key Setup for Cloud Models

When using LangExtract with cloud-hosted models (like Gemini or OpenAI), you&#039;ll need to
set up an API key. On-device models don&#039;t require an API key. For developers
using local LLMs, LangExtract offers built-in support for Ollama and can be
extended to other third-party APIs by updating the inference endpoints.

### API Key Sources

Get API keys from:

*   [AI Studio](https://aistudio.google.com/app/apikey) for Gemini models
*   [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/sdks/overview) for enterprise use
*   [OpenAI Platform](https://platform.openai.com/api-keys) for OpenAI models

### Setting up API key in your environment

**Option 1: Environment Variable**

```bash
export LANGEXTRACT_API_KEY=&quot;your-api-key-here&quot;
```

**Option 2: .env File (Recommended)**

Add your API key to a `.env` file:

```bash
# Add API key to .env file
cat &gt;&gt; .env &lt;&lt; &#039;EOF&#039;
LANGEXTRACT_API_KEY=your-api-key-here
EOF

# Keep your API key secure
echo &#039;.env&#039; &gt;&gt; .gitignore
```

In your Python code:
```python
import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=&quot;Extract information...&quot;,
    examples=[...],
    model_id=&quot;gemini-2.5-flash&quot;
)
```

**Option 3: Direct API Key (Not Recommended for Production)**

You can also provide the API key directly in your code, though this is not recommended for production use:

```python
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=&quot;Extract information...&quot;,
    examples=[...],
    model_id=&quot;gemini-2.5-flash&quot;,
    api_key=&quot;your-api-key-here&quot;  # Only use this for testing/development
)
```

**Option 4: Vertex AI (Service Accounts)**

Use [Vertex AI](https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform) for authentication with service accounts:

```python
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=&quot;Extract information...&quot;,
    examples=[...],
    model_id=&quot;gemini-2.5-flash&quot;,
    language_model_params={
        &quot;vertexai&quot;: True,
        &quot;project&quot;: &quot;your-project-id&quot;,
        &quot;location&quot;: &quot;global&quot;  # or regional endpoint
    }
)
```

## Adding Custom Model Providers

LangExtract supports custom LLM providers via a lightweight plugin system. You can add support for new models without changing core code.

- Add new model support independently of the core library
- Distribute your provider as a separate Python package
- Keep custom dependencies isolated
- Override or extend built-in providers via priority-based resolution

See the detailed guide in [Provider System Documentation](langextract/providers/README.md) to learn how to:

- Register a provider with `@registry.register(...)`
- Publish an entry point for discovery
- Optionally provide a schema with `get_schema_class()` for structured output
- Integrate with the factory via `create_model(...)`

## Using OpenAI Models

LangExtract supports OpenAI models (requires optional dependency: `pip install langextract[openai]`):

```python
import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gpt-4o&quot;,  # Automatically selects OpenAI provider
    api_key=os.environ.get(&#039;OPENAI_API_KEY&#039;),
    fence_output=True,
    use_schema_constraints=False
)
```

Note: OpenAI models require `fence_output=True` and `use_schema_constraints=False` because LangExtract doesn&#039;t implement schema constraints for OpenAI yet.

## Using Local LLMs with Ollama
LangExtract supports local inference using Ollama, allowing you to run models without API keys:

```python
import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gemma2:2b&quot;,  # Automatically selects Ollama provider
    model_url=&quot;http://localhost:11434&quot;,
    fence_output=False,
    use_schema_constraints=False
)
```

**Quick setup:** Install Ollama from [ollama.com](https://ollama.com/), run `ollama pull gemma2:2b`, then `ollama serve`.

For detailed installation, Docker setup, and examples, see [`examples/ollama/`](examples/ollama/).

## More Examples

Additional examples of LangExtract in action:

### *Romeo and Juliet* Full Text Extraction

LangExtract can process complete documents directly from URLs. This example demonstrates extraction from the full text of *Romeo and Juliet* from Project Gutenberg (147,843 characters), showing parallel processing, sequential extraction passes, and performance optimization for long document processing.

**[View *Romeo and Juliet* Full Text Example â†’](https://github.com/google/langextract/blob/main/docs/examples/longer_text_example.md)**

### Medication Extraction

&gt; **Disclaimer:** This demonstration is for illustrative purposes of LangExtract&#039;s baseline capability only. It does not represent a finished or approved product, is not intended to diagnose or suggest treatment of any disease or condition, and should not be used for medical advice.

LangExtract excels at extracting structured medical information from clinical text. These examples demonstrate both basic entity recognition (medication names, dosages, routes) and relationship extraction (connecting medications to their attributes), showing LangExtract&#039;s effectiveness for healthcare applications.

**[View Medication Examples â†’](https://github.com/google/langextract/blob/main/docs/examples/medication_examples.md)**

### Radiology Report Structuring: RadExtract

Explore RadExtract, a live interactive demo on HuggingFace Spaces that shows how LangExtract can automatically structure radiology reports. Try it directly in your browser with no setup required.

**[View RadExtract Demo â†’](https://huggingface.co/spaces/google/radextract)**

## Community Providers

Extend LangExtract with custom model providers! Check out our [Community Provider Plugins](COMMUNITY_PROVIDERS.md) registry to discover providers created by the community or add your own.

For detailed instructions on creating a provider plugin, see the [Custom Provider Plugin Example](examples/custom_provider_plugin/).

## Contributing

Contributions are welcome! See [CONTRIBUTING.md](https://github.com/google/langextract/blob/main/CONTRIBUTING.md) to get started
with development, testing, and pull requests. You must sign a
[Contributor License Agreement](https://cla.developers.google.com/about)
before submitting patches.



## Testing

To run tests locally from the source:

```bash
# Clone the repository
git clone https://github.com/google/langextract.git
cd langextract

# Install with test dependencies
pip install -e &quot;.[test]&quot;

# Run all tests
pytest tests
```

Or reproduce the full CI matrix locally with tox:

```bash
tox  # runs pylint + pytest on Python 3.10 and 3.11
```

### Ollama Integration Testing

If you have Ollama installed locally, you can run integration tests:

```bash
# Test Ollama integration (requires Ollama running with gemma2:2b model)
tox -e ollama-integration
```

This test will automatically detect if Ollama is available and run real inference tests.

## Development

### Code Formatting

This project uses automated formatting tools to maintain consistent code style:

```bash
# Auto-format all code
./autoformat.sh

# Or run formatters separately
isort langextract tests --profile google --line-length 80
pyink langextract tests --config pyproject.toml
```

### Pre-commit Hooks

For automatic formatting checks:
```bash
pre-commit install  # One-time setup
pre-commit run --all-files  # Manual run
```

### Linting

Run linting before submitting PRs:

```bash
pylint --rcfile=.pylintrc langextract tests
```

See [CONTRIBUTING.md](CONTRIBUTING.md) for full development guidelines.

## Disclaimer

This is not an officially supported Google product. If you use
LangExtract in production or publications, please cite accordingly and
acknowledge usage. Use is subject to the [Apache 2.0 License](https://github.com/google/langextract/blob/main/LICENSE).
For health-related applications, use of LangExtract is also subject to the
[Health AI Developer Foundations Terms of Use](https://developers.google.com/health-ai-developer-foundations/terms).

---

**Happy Extracting!**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[OpenBMB/VoxCPM]]></title>
            <link>https://github.com/OpenBMB/VoxCPM</link>
            <guid>https://github.com/OpenBMB/VoxCPM</guid>
            <pubDate>Sun, 18 Jan 2026 00:05:36 GMT</pubDate>
            <description><![CDATA[VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenBMB/VoxCPM">OpenBMB/VoxCPM</a></h1>
            <p>VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning</p>
            <p>Language: Python</p>
            <p>Stars: 3,808</p>
            <p>Forks: 444</p>
            <p>Stars today: 310 stars today</p>
            <h2>README</h2><pre>## ğŸ™ï¸ VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning


[![Project Page](https://img.shields.io/badge/Project%20Page-GitHub-blue)](https://github.com/OpenBMB/VoxCPM/) [![Technical Report](https://img.shields.io/badge/Technical%20Report-Arxiv-red)](https://arxiv.org/abs/2509.24650)[![Live Playground](https://img.shields.io/badge/Live%20PlayGround-Demo-orange)](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) [![Samples](https://img.shields.io/badge/Audio%20Samples-Page-green)](https://openbmb.github.io/VoxCPM-demopage)

#### VoxCPM1.5 Model Weights

 [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-OpenBMB-yellow)](https://huggingface.co/openbmb/VoxCPM1.5) [![ModelScope](https://img.shields.io/badge/ModelScope-OpenBMB-purple)](https://modelscope.cn/models/OpenBMB/VoxCPM1.5)  



&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/voxcpm_logo.png&quot; alt=&quot;VoxCPM Logo&quot; width=&quot;40%&quot;&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

ğŸ‘‹ Contact us on [WeChat](assets/wechat.png)

&lt;/div&gt;

## News 
* [2025.12.05] ğŸ‰ ğŸ‰ ğŸ‰  We Open Source the VoxCPM1.5 [weights](https://huggingface.co/openbmb/VoxCPM1.5)! The model now supports both full-parameter fine-tuning and efficient LoRA fine-tuning, empowering you to create your own tailored version. See [Release Notes](docs/release_note.md) for details.
* [2025.09.30] ğŸ”¥ ğŸ”¥ ğŸ”¥  We Release VoxCPM [Technical Report](https://arxiv.org/abs/2509.24650)!
* [2025.09.16] ğŸ”¥ ğŸ”¥ ğŸ”¥  We Open Source the VoxCPM-0.5B [weights](https://huggingface.co/openbmb/VoxCPM-0.5B)!
* [2025.09.16] ğŸ‰ ğŸ‰ ğŸ‰  We Provide the [Gradio PlayGround](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) for VoxCPM-0.5B, try it now! 

## Overview

VoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization and enables two flagship capabilities: context-aware speech generation and true-to-life zero-shot voice cloning.

Unlike mainstream approaches that convert speech to discrete tokens, VoxCPM uses an end-to-end diffusion autoregressive architecture that directly generates continuous speech representations from text. Built on [MiniCPM-4](https://huggingface.co/openbmb/MiniCPM4-0.5B) backbone, it achieves implicit semantic-acoustic decoupling through hierachical language modeling and FSQ constraints, greatly enhancing both expressiveness and generation stability.

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/voxcpm_model.png&quot; alt=&quot;VoxCPM Model Architecture&quot; width=&quot;90%&quot;&gt;
&lt;/div&gt;


###  ğŸš€ Key Features
- **Context-Aware, Expressive Speech Generation** - VoxCPM comprehends text to infer and generate appropriate prosody, delivering speech with remarkable expressiveness and natural flow. It spontaneously adapts speaking style based on content, producing highly fitting vocal expression trained on a massive 1.8 million-hour bilingual corpus.
- **True-to-Life Voice Cloning** - With only a short reference audio clip, VoxCPM performs accurate zero-shot voice cloning, capturing not only the speaker&#039;s timbre but also fine-grained characteristics such as accent, emotional tone, rhythm, and pacing to create a faithful and natural replica.
- **High-Efficiency Synthesis** - VoxCPM supports streaming synthesis with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, making it possible for real-time applications.

### ğŸ“¦ Model Versions
See [Release Notes](docs/release_note.md) for details
- **VoxCPM1.5** (Latest): 
  - Model Params: 800M
  - Sampling rate of AudioVAE: 44100
  - Token rate in LM Backbone: 6.25Hz (patch-size=4)
  - RTF in a single NVIDIA-RTX 4090 GPU: ~0.15

- **VoxCPM-0.5B** (Original):
  - Model Params: 640M
  - Sampling rate of AudioVAE: 16000
  - Token rate in LM Backbone: 12.5Hz (patch-size=2)
  - RTF in a single NVIDIA-RTX 4090 GPU: 0.17



##  Quick Start

### ğŸ”§ Install from PyPI
``` sh
pip install voxcpm
```
### 1.  Model Download (Optional)
By default, when you first run the script, the model will be downloaded automatically, but you can also download the model in advance.
- Download VoxCPM1.5
    ```
    from huggingface_hub import snapshot_download
    snapshot_download(&quot;openbmb/VoxCPM1.5&quot;)
    ```

- Or Download VoxCPM-0.5B
    ```
    from huggingface_hub import snapshot_download
    snapshot_download(&quot;openbmb/VoxCPM-0.5B&quot;)
    ```
- Download ZipEnhancer and SenseVoice-Small. We use ZipEnhancer to enhance speech prompts and SenseVoice-Small for speech prompt ASR in the web demo. 
    ```
    from modelscope import snapshot_download
    snapshot_download(&#039;iic/speech_zipenhancer_ans_multiloss_16k_base&#039;)
    snapshot_download(&#039;iic/SenseVoiceSmall&#039;)
    ```

### 2. Basic Usage
```python
import soundfile as sf
import numpy as np
from voxcpm import VoxCPM

model = VoxCPM.from_pretrained(&quot;openbmb/VoxCPM1.5&quot;)

# Non-streaming
wav = model.generate(
    text=&quot;VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.&quot;,
    prompt_wav_path=None,      # optional: path to a prompt speech for voice cloning
    prompt_text=None,          # optional: reference text
    cfg_value=2.0,             # LM guidance on LocDiT, higher for better adherence to the prompt, but maybe worse
    inference_timesteps=10,   # LocDiT inference timesteps, higher for better result, lower for fast speed
    normalize=False,           # enable external TN tool, but will disable native raw text support
    denoise=False,             # enable external Denoise tool, but it may cause some distortion and restrict the sampling rate to 16kHz
    retry_badcase=True,        # enable retrying mode for some bad cases (unstoppable)
    retry_badcase_max_times=3,  # maximum retrying times
    retry_badcase_ratio_threshold=6.0, # maximum length restriction for bad case detection (simple but effective), it could be adjusted for slow pace speech
)

sf.write(&quot;output.wav&quot;, wav, model.tts_model.sample_rate)
print(&quot;saved: output.wav&quot;)

# Streaming
chunks = []
for chunk in model.generate_streaming(
    text = &quot;Streaming text to speech is easy with VoxCPM!&quot;,
    # supports same args as above
):
    chunks.append(chunk)
wav = np.concatenate(chunks)

sf.write(&quot;output_streaming.wav&quot;, wav, model.tts_model.sample_rate)
print(&quot;saved: output_streaming.wav&quot;)
```

### 3. CLI Usage

After installation, the entry point is `voxcpm` (or use `python -m voxcpm.cli`).

```bash
# 1) Direct synthesis (single text)
voxcpm --text &quot;VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.&quot; --output out.wav

# 2) Voice cloning (reference audio + transcript)
voxcpm --text &quot;VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.&quot; \
  --prompt-audio path/to/voice.wav \
  --prompt-text &quot;reference transcript&quot; \
  --output out.wav \
  # --denoise

# (Optinal) Voice cloning (reference audio + transcript file)
voxcpm --text &quot;VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.&quot; \
  --prompt-audio path/to/voice.wav \
  --prompt-file &quot;/path/to/text-file&quot; \
  --output out.wav \
  # --denoise

# 3) Batch processing (one text per line)
voxcpm --input examples/input.txt --output-dir outs
# (optional) Batch + cloning
voxcpm --input examples/input.txt --output-dir outs \
  --prompt-audio path/to/voice.wav \
  --prompt-text &quot;reference transcript&quot; \
  # --denoise

# 4) Inference parameters (quality/speed)
voxcpm --text &quot;...&quot; --output out.wav \
  --cfg-value 2.0 --inference-timesteps 10 --normalize

# 5) Model loading
# Prefer local path
voxcpm --text &quot;...&quot; --output out.wav --model-path /path/to/VoxCPM_model_dir
# Or from Hugging Face (auto download/cache)
voxcpm --text &quot;...&quot; --output out.wav \
  --hf-model-id openbmb/VoxCPM1.5 --cache-dir ~/.cache/huggingface --local-files-only

# 6) Denoiser control
voxcpm --text &quot;...&quot; --output out.wav \
  --no-denoiser --zipenhancer-path iic/speech_zipenhancer_ans_multiloss_16k_base

# 7) Help
voxcpm --help
python -m voxcpm.cli --help
```

### 4. Start web demo

You can start the UI interface by running `python app.py`, which allows you to perform Voice Cloning and Voice Creation.

### 5. Fine-tuning

VoxCPM1.5 supports both full fine-tuning (SFT) and LoRA fine-tuning, allowing you to train personalized voice models on your own data. See the [Fine-tuning Guide](docs/finetune.md) for detailed instructions.

**Quick Start:**
```bash
# Full fine-tuning
python scripts/train_voxcpm_finetune.py \
    --config_path conf/voxcpm_v1.5/voxcpm_finetune_all.yaml

# LoRA fine-tuning
python scripts/train_voxcpm_finetune.py \
    --config_path conf/voxcpm_v1.5/voxcpm_finetune_lora.yaml
```

## ğŸ“š Documentation

- **[Usage Guide](docs/usage_guide.md)** - Detailed guide on how to use VoxCPM effectively, including text input modes, voice cloning tips, and parameter tuning
- **[Fine-tuning Guide](docs/finetune.md)** - Complete guide for fine-tuning VoxCPM models with SFT and LoRA
- **[Release Notes](docs/release_note.md)** - Version history and updates
- **[Performance Benchmarks](docs/performance.md)** - Detailed performance comparisons on public benchmarks

---

## ğŸ“š More Information

###  ğŸŒŸ Community Projects
We&#039;re excited to see the VoxCPM community growing! Here are some amazing projects and features built by our community:
- **[ComfyUI-VoxCPM](https://github.com/wildminder/ComfyUI-VoxCPM)** A VoxCPM extension for ComfyUI.
- **[ComfyUI-VoxCPMTTS](https://github.com/1038lab/ComfyUI-VoxCPMTTS)** A VoxCPM extension for ComfyUI.
- **[WebUI-VoxCPM](https://github.com/rsxdalv/tts_webui_extension.vox_cpm)** A template extension for TTS WebUI.
- **[PR: Streaming API Support (by AbrahamSanders)](https://github.com/OpenBMB/VoxCPM/pull/26)** 
- **[VoxCPM-NanoVLLM](https://github.com/a710128/nanovllm-voxcpm)** NanoVLLM integration for VoxCPM for faster, high-throughput inference on GPU.
- **[VoxCPM-ONNX](https://github.com/bluryar/VoxCPM-ONNX)** ONNX export for VoxCPM supports faster CPU inference.
- **[VoxCPMANE](https://github.com/0seba/VoxCPMANE)** VoxCPM TTS with Apple Neural Engine backend server.
- **[PR: LoRA finetune web UI (by Ayin1412)](https://github.com/OpenBMB/VoxCPM/pull/100)**
- **[voxcpm_rs](https://github.com/madushan1000/voxcpm_rs)** A re-implementation of VoxCPM-0.5B in Rust.

*Note: The projects are not officially maintained by OpenBMB.*



*Have you built something cool with VoxCPM? We&#039;d love to feature it here! Please open an issue or pull request to add your project.*

### ğŸ“Š Performance Highlights

VoxCPM achieves competitive results on public zero-shot TTS benchmarks. See [Performance Benchmarks](docs/performance.md) for detailed comparison tables.



## âš ï¸ Risks and limitations
- General Model Behavior: While VoxCPM has been trained on a large-scale dataset, it may still produce outputs that are unexpected, biased, or contain artifacts.
- Potential for Misuse of Voice Cloning: VoxCPM&#039;s powerful zero-shot voice cloning capability can generate highly realistic synthetic speech. This technology could be misused for creating convincing deepfakes for purposes of impersonation, fraud, or spreading disinformation. Users of this model must not use it to create content that infringes upon the rights of individuals. It is strictly forbidden to use VoxCPM for any illegal or unethical purposes. We strongly recommend that any publicly shared content generated with this model be clearly marked as AI-generated.
- Current Technical Limitations: Although generally stable, the model may occasionally exhibit instability, especially with very long or expressive inputs. Furthermore, the current version offers limited direct control over specific speech attributes like emotion or speaking style.
- Bilingual Model: VoxCPM is trained primarily on Chinese and English data. Performance on other languages is not guaranteed and may result in unpredictable or low-quality audio.
- This model is released for research and development purposes only. We do not recommend its use in production or commercial applications without rigorous testing and safety evaluations. Please use VoxCPM responsibly.

---

## ğŸ“ TO-DO List
Please stay tuned for updates!
- [x] Release the VoxCPM technical report.
- [x] Support higher sampling rate (44.1kHz in VoxCPM-1.5).
- [x] Support SFT and LoRA fine-tuning.
- [] Multilingual Support (besides ZH/EN).
- [] Controllable Speech Generation by Human Instruction.



## ğŸ“„ License
The VoxCPM model weights and code are open-sourced under the [Apache-2.0](LICENSE) license.

## ğŸ™ Acknowledgments

We extend our sincere gratitude to the following works and resources for their inspiration and contributions:

- [DiTAR](https://arxiv.org/abs/2502.03930) for the diffusion autoregressive backbone used in speech generation
- [MiniCPM-4](https://github.com/OpenBMB/MiniCPM) for serving as the language model foundation
- [CosyVoice](https://github.com/FunAudioLLM/CosyVoice) for the implementation of Flow Matching-based LocDiT
- [DAC](https://github.com/descriptinc/descript-audio-codec) for providing the Audio VAE backbone

## Institutions

This project is developed by the following institutions:
- &lt;img src=&quot;assets/modelbest_logo.png&quot; width=&quot;28px&quot;&gt; [ModelBest](https://modelbest.cn/)

- &lt;img src=&quot;assets/thuhcsi_logo.png&quot; width=&quot;28px&quot;&gt; [THUHCSI](https://github.com/thuhcsi)


## â­ Star History
 [![Star History Chart](https://api.star-history.com/svg?repos=OpenBMB/VoxCPM&amp;type=Date)](https://star-history.com/#OpenBMB/VoxCPM&amp;Date)


## ğŸ“š Citation

If you find our model helpful, please consider citing our projects ğŸ“ and staring us â­ï¸ï¼

```bib
@article{voxcpm2025,
  title        = {VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning},
  author       = {Zhou, Yixuan and Zeng, Guoyang and Liu, Xin and Li, Xiang and Yu, Renjie and Wang, Ziyang and Ye, Runchuan and Sun, Weiyue and Gui, Jiancheng and Li, Kehan and Wu, Zhiyong  and Liu, Zhiyuan},
  journal      = {arXiv preprint arXiv:2509.24650},
  year         = {2025},
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sansan0/TrendRadar]]></title>
            <link>https://github.com/sansan0/TrendRadar</link>
            <guid>https://github.com/sansan0/TrendRadar</guid>
            <pubDate>Sun, 18 Jan 2026 00:05:35 GMT</pubDate>
            <description><![CDATA[ğŸ¯ å‘Šåˆ«ä¿¡æ¯è¿‡è½½ï¼Œä½ çš„ AI èˆ†æƒ…ç›‘æ§åŠ©æ‰‹ä¸çƒ­ç‚¹ç­›é€‰å·¥å…·ï¼èšåˆå¤šå¹³å°çƒ­ç‚¹ + RSS è®¢é˜…ï¼Œæ”¯æŒå…³é”®è¯ç²¾å‡†ç­›é€‰ã€‚AI ç¿»è¯‘ + AI åˆ†æç®€æŠ¥ç›´æ¨æ‰‹æœºï¼Œä¹Ÿæ”¯æŒæ¥å…¥ MCP æ¶æ„ï¼Œèµ‹èƒ½ AI è‡ªç„¶è¯­è¨€å¯¹è¯åˆ†æã€æƒ…æ„Ÿæ´å¯Ÿä¸è¶‹åŠ¿é¢„æµ‹ã€‚æ”¯æŒ Docker ä¸€é”®éƒ¨ç½²ï¼Œæ•°æ®æœ¬åœ°/äº‘ç«¯è‡ªæŒã€‚é›†æˆå¾®ä¿¡/é£ä¹¦/é’‰é’‰/Telegram/é‚®ä»¶/ntfy/bark/slack ç­‰æ¸ é“æ™ºèƒ½æ¨é€ã€‚â­]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sansan0/TrendRadar">sansan0/TrendRadar</a></h1>
            <p>ğŸ¯ å‘Šåˆ«ä¿¡æ¯è¿‡è½½ï¼Œä½ çš„ AI èˆ†æƒ…ç›‘æ§åŠ©æ‰‹ä¸çƒ­ç‚¹ç­›é€‰å·¥å…·ï¼èšåˆå¤šå¹³å°çƒ­ç‚¹ + RSS è®¢é˜…ï¼Œæ”¯æŒå…³é”®è¯ç²¾å‡†ç­›é€‰ã€‚AI ç¿»è¯‘ + AI åˆ†æç®€æŠ¥ç›´æ¨æ‰‹æœºï¼Œä¹Ÿæ”¯æŒæ¥å…¥ MCP æ¶æ„ï¼Œèµ‹èƒ½ AI è‡ªç„¶è¯­è¨€å¯¹è¯åˆ†æã€æƒ…æ„Ÿæ´å¯Ÿä¸è¶‹åŠ¿é¢„æµ‹ã€‚æ”¯æŒ Docker ä¸€é”®éƒ¨ç½²ï¼Œæ•°æ®æœ¬åœ°/äº‘ç«¯è‡ªæŒã€‚é›†æˆå¾®ä¿¡/é£ä¹¦/é’‰é’‰/Telegram/é‚®ä»¶/ntfy/bark/slack ç­‰æ¸ é“æ™ºèƒ½æ¨é€ã€‚â­</p>
            <p>Language: Python</p>
            <p>Stars: 43,510</p>
            <p>Forks: 21,596</p>
            <p>Stars today: 104 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; id=&quot;trendradar&quot;&gt;

&lt;a href=&quot;https://github.com/sansan0/TrendRadar&quot; title=&quot;TrendRadar&quot;&gt;
  &lt;img src=&quot;/_image/banner.webp&quot; alt=&quot;TrendRadar Banner&quot; width=&quot;80%&quot;&gt;
&lt;/a&gt;

æœ€å¿«&lt;strong&gt;30ç§’&lt;/strong&gt;éƒ¨ç½²çš„çƒ­ç‚¹åŠ©æ‰‹ â€”â€” å‘Šåˆ«æ— æ•ˆåˆ·å±ï¼Œåªçœ‹çœŸæ­£å…³å¿ƒçš„æ–°é—»èµ„è®¯

&lt;a href=&quot;https://trendshift.io/repositories/14726&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14726&quot; alt=&quot;sansan0%2FTrendRadar | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;a href=&quot;https://shandianshuo.cn&quot; target=&quot;_blank&quot; title=&quot;AI è¯­éŸ³è¾“å…¥ï¼Œæ¯”æ‰“å­—å¿« 4 å€ âš¡&quot;&gt;&lt;img src=&quot;_image/shandianshuo.png&quot; alt=&quot;é—ªç”µè¯´ logo&quot; height=&quot;50&quot;/&gt;&lt;/a&gt;

[![GitHub Stars](https://img.shields.io/github/stars/sansan0/TrendRadar?style=flat-square&amp;logo=github&amp;color=yellow)](https://github.com/sansan0/TrendRadar/stargazers)
[![GitHub Forks](https://img.shields.io/github/forks/sansan0/TrendRadar?style=flat-square&amp;logo=github&amp;color=blue)](https://github.com/sansan0/TrendRadar/network/members)
[![License](https://img.shields.io/badge/license-GPL--3.0-blue.svg?style=flat-square)](LICENSE)
[![Version](https://img.shields.io/badge/version-v5.2.0-blue.svg)](https://github.com/sansan0/TrendRadar)
[![MCP](https://img.shields.io/badge/MCP-v3.1.6-green.svg)](https://github.com/sansan0/TrendRadar)
[![RSS](https://img.shields.io/badge/RSS-è®¢é˜…æºæ”¯æŒ-orange.svg?style=flat-square&amp;logo=rss&amp;logoColor=white)](https://github.com/sansan0/TrendRadar)
[![AIç¿»è¯‘](https://img.shields.io/badge/AI-å¤šè¯­è¨€æ¨é€-purple.svg?style=flat-square)](https://github.com/sansan0/TrendRadar)

[![ä¼ä¸šå¾®ä¿¡é€šçŸ¥](https://img.shields.io/badge/ä¼ä¸šå¾®ä¿¡-é€šçŸ¥-00D4AA?style=flat-square)](https://work.weixin.qq.com/)
[![ä¸ªäººå¾®ä¿¡é€šçŸ¥](https://img.shields.io/badge/ä¸ªäººå¾®ä¿¡-é€šçŸ¥-00D4AA?style=flat-square)](https://weixin.qq.com/)
[![Telegramé€šçŸ¥](https://img.shields.io/badge/Telegram-é€šçŸ¥-00D4AA?style=flat-square)](https://telegram.org/)
[![dingtalké€šçŸ¥](https://img.shields.io/badge/é’‰é’‰-é€šçŸ¥-00D4AA?style=flat-square)](#)
[![é£ä¹¦é€šçŸ¥](https://img.shields.io/badge/é£ä¹¦-é€šçŸ¥-00D4AA?style=flat-square)](https://www.feishu.cn/)
[![é‚®ä»¶é€šçŸ¥](https://img.shields.io/badge/Email-é€šçŸ¥-00D4AA?style=flat-square)](#)
[![ntfyé€šçŸ¥](https://img.shields.io/badge/ntfy-é€šçŸ¥-00D4AA?style=flat-square)](https://github.com/binwiederhier/ntfy)
[![Barké€šçŸ¥](https://img.shields.io/badge/Bark-é€šçŸ¥-00D4AA?style=flat-square)](https://github.com/Finb/Bark)
[![Slacké€šçŸ¥](https://img.shields.io/badge/Slack-é€šçŸ¥-00D4AA?style=flat-square)](https://slack.com/)
[![é€šç”¨Webhook](https://img.shields.io/badge/é€šç”¨-Webhook-607D8B?style=flat-square&amp;logo=webhook&amp;logoColor=white)](#)


[![GitHub Actions](https://img.shields.io/badge/GitHub_Actions-è‡ªåŠ¨åŒ–-2088FF?style=flat-square&amp;logo=github-actions&amp;logoColor=white)](https://github.com/sansan0/TrendRadar)
[![GitHub Pages](https://img.shields.io/badge/GitHub_Pages-éƒ¨ç½²-4285F4?style=flat-square&amp;logo=github&amp;logoColor=white)](https://sansan0.github.io/TrendRadar)
[![Docker](https://img.shields.io/badge/Docker-éƒ¨ç½²-2496ED?style=flat-square&amp;logo=docker&amp;logoColor=white)](https://hub.docker.com/r/wantcat/trendradar)
[![MCP Support](https://img.shields.io/badge/MCP-AIåˆ†ææ”¯æŒ-FF6B6B?style=flat-square&amp;logo=ai&amp;logoColor=white)](https://modelcontextprotocol.io/)
[![AIåˆ†ææ¨é€](https://img.shields.io/badge/AI-åˆ†ææ¨é€-FF6B6B?style=flat-square&amp;logo=openai&amp;logoColor=white)](#)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

**ä¸­æ–‡** | **[English](README-EN.md)**

&lt;/div&gt;

&gt; æœ¬é¡¹ç›®ä»¥è½»é‡ï¼Œæ˜“éƒ¨ç½²ä¸ºç›®æ ‡

&lt;br&gt;

## ğŸ“‘ å¿«é€Ÿå¯¼èˆª

&gt; ğŸ’¡ **ç‚¹å‡»ä¸‹æ–¹é“¾æ¥**å¯å¿«é€Ÿè·³è½¬åˆ°å¯¹åº”ç« èŠ‚ã€‚éƒ¨ç½²æ¨èä»ã€Œ**å¿«é€Ÿå¼€å§‹**ã€å…¥æ‰‹ï¼Œéœ€è¦è¯¦ç»†è‡ªå®šä¹‰è¯·çœ‹ã€Œ**é…ç½®è¯¦è§£**ã€

&lt;div align=&quot;center&quot;&gt;

|   |   |   |
|:---:|:---:|:---:|
| [ğŸš€ **å¿«é€Ÿå¼€å§‹**](#-å¿«é€Ÿå¼€å§‹) | [AI æ™ºèƒ½åˆ†æ](#-ai-æ™ºèƒ½åˆ†æ) | [âš™ï¸ **é…ç½®è¯¦è§£**](#é…ç½®è¯¦è§£) |
| [Dockeréƒ¨ç½²](#6-docker-éƒ¨ç½²) | [MCPå®¢æˆ·ç«¯](#-mcp-å®¢æˆ·ç«¯) | [ğŸ“ **æ›´æ–°æ—¥å¿—**](#-æ›´æ–°æ—¥å¿—) |
| [ğŸ¯ **æ ¸å¿ƒåŠŸèƒ½**](#-æ ¸å¿ƒåŠŸèƒ½) | [â˜• **æ”¯æŒé¡¹ç›®**](#-æ”¯æŒé¡¹ç›®) | [ğŸ“š **é¡¹ç›®ç›¸å…³**](#-é¡¹ç›®ç›¸å…³) |

&lt;/div&gt;

&lt;br&gt;

- æ„Ÿè°¢**ä¸ºé¡¹ç›®ç‚¹ star** çš„è§‚ä¼—ä»¬ï¼Œ**fork** ä½ æ‰€æ¬²ä¹Ÿï¼Œ**star** æˆ‘æ‰€æ¬²ä¹Ÿï¼Œä¸¤è€…å¾—å…¼ğŸ˜æ˜¯å¯¹å¼€æºç²¾ç¥æœ€å¥½çš„æ”¯æŒ

&lt;details&gt;
&lt;summary&gt;ğŸ‘‰ ç‚¹å‡»å±•å¼€ï¼š&lt;strong&gt;è‡´è°¢åå•&lt;/strong&gt; (å¤©ä½¿è½®è£èª‰æ¦œ ğŸ”¥73+ğŸ”¥ ä½)&lt;/summary&gt;

### æ—©æœŸæ”¯æŒè€…è‡´è°¢

&gt; ğŸ’¡ **ç‰¹åˆ«è¯´æ˜**ï¼š
&gt;
&gt; 1. **å…³äºåå•**ï¼šä¸‹æ–¹è¡¨æ ¼è®°å½•äº†é¡¹ç›®èµ·æ­¥é˜¶æ®µï¼ˆå¤©ä½¿è½®ï¼‰çš„æ”¯æŒè€…ã€‚å› æ—©æœŸäººå·¥ç»Ÿè®¡ç¹çï¼Œ**éš¾å…å­˜åœ¨ç–æ¼æˆ–è®°å½•ä¸å…¨çš„æƒ…å†µï¼Œå¦‚æœ‰é—æ¼ï¼Œå®éæœ¬æ„ï¼Œä¸‡æœ›æµ·æ¶µ**ã€‚
&gt; 2. **æœªæ¥è§„åˆ’**ï¼šä¸ºäº†å°†æœ‰é™çš„ç²¾åŠ›å›å½’ä»£ç ä¸åŠŸèƒ½è¿­ä»£ï¼Œ**å³æ—¥èµ·ä¸å†äººå·¥ç»´æŠ¤æ­¤åå•**ã€‚
&gt;
&gt; æ— è®ºåå­—æ˜¯å¦ä¸Šæ¦œï¼Œä½ ä»¬çš„æ¯ä¸€ä»½æ”¯æŒéƒ½æ˜¯ TrendRadar èƒ½å¤Ÿèµ°åˆ°ä»Šå¤©çš„åŸºçŸ³ã€‚ğŸ™

### åŸºç¡€è®¾æ–½æ”¯æŒ

æ„Ÿè°¢ **GitHub** å…è´¹æä¾›çš„åŸºç¡€è®¾æ–½ï¼Œè¿™æ˜¯æœ¬é¡¹ç›®å¾—ä»¥**ä¸€é”® fork**ä¾¿æ·è¿è¡Œçš„æœ€å¤§å‰æã€‚

### æ•°æ®æ”¯æŒ

æœ¬é¡¹ç›®ä½¿ç”¨ [newsnow](https://github.com/ourongxing/newsnow) é¡¹ç›®çš„ API è·å–å¤šå¹³å°æ•°æ®ï¼Œç‰¹åˆ«æ„Ÿè°¢ä½œè€…æä¾›çš„æœåŠ¡ã€‚

ç»è”ç³»ï¼Œä½œè€…è¡¨ç¤ºæ— éœ€æ‹…å¿ƒæœåŠ¡å™¨å‹åŠ›ï¼Œä½†è¿™æ˜¯åŸºäºä»–çš„å–„æ„å’Œä¿¡ä»»ã€‚è¯·å¤§å®¶ï¼š
- **å‰å¾€ [newsnow é¡¹ç›®](https://github.com/ourongxing/newsnow) ç‚¹ star æ”¯æŒ**
- Docker éƒ¨ç½²æ—¶ï¼Œè¯·åˆç†æ§åˆ¶æ¨é€é¢‘ç‡ï¼Œå‹¿ç«­æ³½è€Œæ¸”

### æ¨å¹¿åŠ©åŠ›

&gt; æ„Ÿè°¢ä»¥ä¸‹å¹³å°å’Œä¸ªäººçš„æ¨è(æŒ‰æ—¶é—´æ’åˆ—)

- [å°ä¼—è½¯ä»¶](https://mp.weixin.qq.com/s/fvutkJ_NPUelSW9OGK39aA) - å¼€æºè½¯ä»¶æ¨èå¹³å°
- [LinuxDo ç¤¾åŒº](https://linux.do/) - æŠ€æœ¯çˆ±å¥½è€…çš„èšé›†åœ°
- [é˜®ä¸€å³°å‘¨åˆŠ](https://github.com/ruanyf/weekly) - æŠ€æœ¯åœˆæœ‰å½±å“åŠ›çš„å‘¨åˆŠ

### è§‚ä¼—æ”¯æŒ

&gt; æ„Ÿè°¢**ç»™äºˆèµ„é‡‘æ”¯æŒ**çš„æœ‹å‹ä»¬ï¼Œä½ ä»¬çš„æ…·æ…¨å·²åŒ–èº«ä¸ºé”®ç›˜æ—çš„é›¶é£Ÿé¥®æ–™ï¼Œé™ªä¼´ç€é¡¹ç›®çš„æ¯ä¸€æ¬¡è¿­ä»£ã€‚
&gt;
&gt; **å…³äº&quot;ä¸€å…ƒç‚¹èµ&quot;çš„å›å½’**ï¼š
&gt; éšç€ v5.0.0 ç‰ˆæœ¬çš„å‘å¸ƒï¼Œé¡¹ç›®è¿ˆå…¥äº†ä¸€ä¸ªæ–°çš„é˜¶æ®µã€‚ä¸ºäº†æ”¯æŒæ—¥ç›Šå¢é•¿çš„ API æˆæœ¬å’Œå’–å•¡å› æ¶ˆè€—ï¼Œ&quot;ä¸€å…ƒç‚¹èµ&quot;é€šé“ç°å·²é‡æ–°å¼€å¯ã€‚ä½ çš„æ¯ä¸€ä»½å¿ƒæ„ï¼Œéƒ½å°†è½¬åŒ–ä¸ºä»£ç ä¸–ç•Œé‡Œçš„ Token å’ŒåŠ¨åŠ›ã€‚ğŸš€ [å‰å¾€æ”¯æŒ](#-æ”¯æŒé¡¹ç›®)

|           ç‚¹èµäºº            |  é‡‘é¢  |  æ—¥æœŸ  |             å¤‡æ³¨             |
| :-------------------------: | :----: | :----: | :-----------------------: |
|           D*5          |  1.8 * 3 | 2025.11.24  |    | 
|           *é¬¼          |  1 | 2025.11.17  |    | 
|           *è¶…          |  10 | 2025.11.17  |    | 
|           R*w          |  10 | 2025.11.17  | è¿™ agent åšçš„ç‰›é€¼å•Š,å…„å¼Ÿ    | 
|           J*o          |  1 | 2025.11.17  | æ„Ÿè°¢å¼€æº,ç¥å¤§ä½¬äº‹ä¸šæœ‰æˆ    | 
|           *æ™¨          |  8.88  | 2025.11.16  | é¡¹ç›®ä¸é”™,ç ”ç©¶å­¦ä¹ ä¸­    | 
|           *æµ·          |  1  | 2025.11.15  |    | 
|           *å¾·          |  1.99  | 2025.11.15  |    | 
|           *ç–          |  8.8  | 2025.11.14  |  æ„Ÿè°¢å¼€æºï¼Œé¡¹ç›®å¾ˆæ£’ï¼Œæ”¯æŒä¸€ä¸‹   | 
|           M*e          |  10  | 2025.11.14  |  å¼€æºä¸æ˜“ï¼Œå¤§ä½¬è¾›è‹¦äº†   | 
|           **æŸ¯          |  1  | 2025.11.14  |     | 
|           *äº‘          |  88  | 2025.11.13  |    å¥½é¡¹ç›®ï¼Œæ„Ÿè°¢å¼€æº  | 
|           *W          |  6  | 2025.11.13  |      | 
|           *å‡¯          |  1  | 2025.11.13  |      | 
|           å¯¹*.          |  1  | 2025.11.13  |    Thanks for your TrendRadar  | 
|           s*y          |  1  | 2025.11.13  |      | 
|           **ç¿”          |  10  | 2025.11.13  |   å¥½é¡¹ç›®ï¼Œç›¸è§æ¨æ™šï¼Œæ„Ÿè°¢å¼€æºï¼     | 
|           *éŸ¦          |  9.9  | 2025.11.13  |   TrendRadarè¶…èµï¼Œè¯·è€å¸ˆå–å’–å•¡~     | 
|           h*p          |  5  | 2025.11.12  |   æ”¯æŒä¸­å›½å¼€æºåŠ›é‡ï¼ŒåŠ æ²¹ï¼     | 
|           c*r          |  6  | 2025.11.12  |        | 
|           a*n          |  5  | 2025.11.12  |        | 
|           ã€‚*c          |  1  | 2025.11.12  |    æ„Ÿè°¢å¼€æºåˆ†äº«    | 
|           *è®°          |  1  | 2025.11.11  |        | 
|           *ä¸»          |  1  | 2025.11.10  |        | 
|           *äº†          |  10  | 2025.11.09  |        | 
|           *æ°          |  5  | 2025.11.08  |        | 
|           *ç‚¹          |  8.80  | 2025.11.07  |   å¼€å‘ä¸æ˜“ï¼Œæ”¯æŒä¸€ä¸‹ã€‚     | 
|           Q*Q          |  6.66  | 2025.11.07  |   æ„Ÿè°¢å¼€æºï¼     | 
|           C*e          |  1  | 2025.11.05  |        | 
|           Peter Fan          |  20  | 2025.10.29  |        | 
|           M*n          |  1  | 2025.10.27  |      æ„Ÿè°¢å¼€æº  | 
|           *è®¸          |  8.88  | 2025.10.23  |      è€å¸ˆ å°ç™½ä¸€æšï¼Œæ‘¸äº†å‡ å¤©äº†è¿˜æ²¡æ•´èµ·æ¥ï¼Œæ±‚æ•™  | 
|           Eason           |  1  | 2025.10.22  |      è¿˜æ²¡æ•´æ˜ç™½ï¼Œä½†ä½ åœ¨åšå¥½äº‹  | 
|           P*n           |  1  | 2025.10.20  |          |
|           *æ°           |  1  | 2025.10.19  |          |
|           *å¾           |  1  | 2025.10.18  |          |
|           *å¿—           |  1  | 2025.10.17  |          |
|           *ğŸ˜€           |  10  | 2025.10.16  |     ç‚¹èµ     |
|           **æ°           |  10  | 2025.10.16  |          |
|           *å•¸           |  10  | 2025.10.16  |          |
|           *çºª           |  5  | 2025.10.14  | TrendRadar         |
|           J*d           |  1  | 2025.10.14  | è°¢è°¢ä½ çš„å·¥å…·ï¼Œå¾ˆå¥½ç©...          |
|           *H           |  1  | 2025.10.14  |           |
|           é‚£*O           |  10  | 2025.10.13  |           |
|           *åœ†           |  1  | 2025.10.13  |           |
|           P*g           |  6  | 2025.10.13  |           |
|           Ocean           |  20  | 2025.10.12  |  ...çœŸçš„å¤ªæ£’äº†ï¼ï¼ï¼å°ç™½çº§åˆ«ä¹Ÿèƒ½ç›´æ¥ç”¨...         |
|           **åŸ¹           |  5.2  | 2025.10.2  |  github-yzyf1312:å¼€æºä¸‡å²         |
|           *æ¤¿           |  3  | 2025.9.23  |  åŠ æ²¹ï¼Œå¾ˆä¸é”™         |
|           *ğŸ           |  10  | 2025.9.21  |           |
|           E*f           |  1  | 2025.9.20  |           |
|           *è®°            |  1  | 2025.9.20  |           |
|           z*u            |  2  | 2025.9.19  |           |
|           **æ˜Š            |  5  | 2025.9.17  |           |
|           *å·            |  1  | 2025.9.15  |           |
|           T*T            |  2  | 2025.9.15  |  ç‚¹èµ         |
|           *å®¶            |  10  | 2025.9.10  |           |
|           *X            |  1.11  | 2025.9.3  |           |
|           *é£™            |  20  | 2025.8.31  |  æ¥è‡ªè€ç«¥è°¢è°¢         |
|           *ä¸‹            |  1  | 2025.8.30  |           |
|           2*D            |  88  | 2025.8.13 ä¸‹åˆ |           |
|           2*D            |  1  | 2025.8.13 ä¸Šåˆ |           |
|           S*o            |  1  | 2025.8.05 |   æ”¯æŒä¸€ä¸‹        |
|           *ä¾             |  10  | 2025.8.04 |           |
|           x*x            |  2  | 2025.8.03 |  trendRadar å¥½é¡¹ç›® ç‚¹èµ          |
|           *è¿œ            |  1  | 2025.8.01 |            |
|           *é‚ª            |  5  | 2025.8.01 |            |
|           *æ¢¦            |  0.1  | 2025.7.30 |            |
|           **é¾™            |  10  | 2025.7.29 |      æ”¯æŒä¸€ä¸‹      |


&lt;/details&gt;

&lt;br&gt;

## ğŸª„ èµåŠ©å•†

&gt; æ¯å¤©å†™æŠ¥å‘Šã€å›å¤æ¶ˆæ¯æ˜¯å¦è®©æ‰‹è…•ç–²æƒ«ï¼Ÿè¯•è¯•ã€Œé—ªç”µè¯´ã€AI è¯­éŸ³è¾“å…¥æ³• â€”â€” è¯´è¯ï¼Œæ¯”æ‰“å­—å¿« 4 å€ âš¡ 

&lt;div align=&quot;center&quot;&gt;

[![Macä¸‹è½½](https://img.shields.io/badge/Mac-å…è´¹ä¸‹è½½-FF6B6B?style=for-the-badge&amp;logo=apple&amp;logoColor=white)](https://shandianshuo.cn) [![Windowsä¸‹è½½](https://img.shields.io/badge/Windows-å…è´¹ä¸‹è½½-FF6B6B?style=for-the-badge&amp;logo=lightning&amp;logoColor=white)](https://shandianshuo.cn)
&lt;a href=&quot;https://shandianshuo.cn&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;_image/banner-shandianshuo.png&quot; alt=&quot;é—ªç”µè¯´&quot; width=&quot;600&quot;/&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;br&gt;

## â˜• æ”¯æŒé¡¹ç›®

&gt; å¦‚æœæœ¬é¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œä½ å¯ä»¥é€‰æ‹©ä»¥ä¸‹æ–¹å¼æ”¯æŒï¼š
&gt; 1. **å…¬ç›ŠåŠ©å­¦**ï¼šå¾®ä¿¡æœç´¢**è…¾è®¯å…¬ç›Š**ï¼Œå¯¹é‡Œé¢çš„**åŠ©å­¦**ç›¸å…³çš„é¡¹ç›®éšå¿ƒæã€‚
&gt;
&gt; 2. **èµåŠ©å¼€å‘è€…**ï¼šä½ çš„èµåŠ©å°†ç”¨äºè¡¥å……ç¢³åŸºç”Ÿç‰©çš„å’–å•¡å› å’Œç¡…åŸºç”Ÿç‰©çš„ Token æ¶ˆè€—ã€‚


- **GitHub Issues**ï¼šé€‚åˆé’ˆå¯¹æ€§å¼ºçš„è§£ç­”ã€‚æé—®æ—¶è¯·æä¾›å®Œæ•´ä¿¡æ¯ï¼ˆæˆªå›¾ã€é”™è¯¯æ—¥å¿—ã€ç³»ç»Ÿç¯å¢ƒç­‰ï¼‰ã€‚
- **å…¬ä¼—å·äº¤æµ**ï¼šé€‚åˆå¿«é€Ÿå’¨è¯¢ã€‚å»ºè®®ä¼˜å…ˆåœ¨ç›¸å…³æ–‡ç« ä¸‹çš„å…¬å…±ç•™è¨€åŒºäº¤æµï¼Œå¦‚ç§ä¿¡ï¼Œè¯·æ–‡æ˜ç¤¼è²Œç”¨è¯­ğŸ˜‰
- **è”ç³»æ–¹å¼**ï¼špath@linux.do


|å…¬ä¼—å·å…³æ³¨ |å¾®ä¿¡ç‚¹èµ | æ”¯ä»˜å®ç‚¹èµ |
|:---:|:---:|:---:|
| &lt;img src=&quot;_image/weixin.png&quot; width=&quot;300&quot; title=&quot;ç¡…åŸºèŒ¶æ°´é—´&quot;/&gt; | &lt;img src=&quot;https://cdn-1258574687.cos.ap-shanghai.myqcloud.com/img/%2F2025%2F07%2F17%2F2ae0a88d98079f7e876c2b4dc85233c6-9e8025.JPG&quot; width=&quot;300&quot; title=&quot;å¾®ä¿¡æ”¯ä»˜&quot;/&gt; | &lt;img src=&quot;https://cdn-1258574687.cos.ap-shanghai.myqcloud.com/img/%2F2025%2F07%2F17%2F1ed4f20ab8e35be51f8e84c94e6e239b4-fe4947.JPG&quot; width=&quot;300&quot; title=&quot;æ”¯ä»˜å®æ”¯ä»˜&quot;/&gt; |

&lt;br&gt;

## ğŸ“ æ›´æ–°æ—¥å¿—

&gt; **ğŸ“Œ æŸ¥çœ‹æœ€æ–°æ›´æ–°**ï¼š**[åŸä»“åº“æ›´æ–°æ—¥å¿—](https://github.com/sansan0/TrendRadar?tab=readme-ov-file#-æ›´æ–°æ—¥å¿—)** ï¼š
- **æç¤º**ï¼šå»ºè®®æŸ¥çœ‹ã€å†å²æ›´æ–°ã€‘ï¼Œæ˜ç¡®å…·ä½“çš„ã€åŠŸèƒ½å†…å®¹ã€‘

### 2026/01/17 - v5.2.0

&gt; ä¸»è¦è§ config.yaml æè¿°

**ğŸŒ AI ç¿»è¯‘åŠŸèƒ½**

- **å¤šè¯­è¨€ç¿»è¯‘**ï¼šæ”¯æŒå°†æ¨é€å†…å®¹ç¿»è¯‘ä¸ºä»»æ„è¯­è¨€
- **æ‰¹é‡ç¿»è¯‘**ï¼šæ™ºèƒ½æ‰¹é‡å¤„ç†ï¼Œå‡å°‘ API è°ƒç”¨æ¬¡æ•°
- **è‡ªå®šä¹‰æç¤ºè¯**ï¼šæ”¯æŒè‡ªå®šä¹‰ç¿»è¯‘é£æ ¼

**ğŸ”§ é…ç½®æ¶æ„ä¼˜åŒ–**

- **AI æ¨¡å‹é…ç½®ç‹¬ç«‹**ï¼šåˆ†æå’Œç¿»è¯‘å…±äº«æ¨¡å‹é…ç½®
- **åŒºåŸŸå¼€å…³ç»Ÿä¸€**ï¼šç»Ÿä¸€ç®¡ç†æ¨é€åŒºåŸŸæ˜¾ç¤º
- **åŒºåŸŸæ’åºè‡ªå®šä¹‰**ï¼šæ”¯æŒè‡ªå®šä¹‰å„åŒºåŸŸçš„æ˜¾ç¤ºé¡ºåº

**âœ¨ AI åˆ†æå¢å¼º**

- **AI åˆ†æåµŒå…¥ HTML**ï¼šåˆ†æç»“æœç›´æ¥åµŒå…¥ HTML æŠ¥å‘Šï¼Œé‚®ä»¶é€šçŸ¥ç›´æ¥ä½¿ç”¨
- **å¯Œæ ·å¼ AI åŒºå—**ï¼šæ¸å˜è“è‰²èƒŒæ™¯å¡ç‰‡å¼å¸ƒå±€ï¼Œæ¸…æ™°åˆ†éš”å„åˆ†æç»´åº¦
- **æ’åæ—¶é—´çº¿æ”¯æŒ**ï¼šAI å¯è·å–æ¯æ¡æ–°é—»åœ¨æ¯ä¸ªæŠ“å–æ—¶é—´ç‚¹çš„ç²¾ç¡®æ’å
- **æ¿å—é‡ç»„ (7â†’4)**ï¼šæ•´åˆä¸ºæ ¸å¿ƒçƒ­ç‚¹æ€åŠ¿ã€èˆ†è®ºé£å‘äº‰è®®ã€å¼‚åŠ¨ä¸å¼±ä¿¡å·ã€ç ”åˆ¤ç­–ç•¥å»ºè®®

**ğŸ”§ å¤šæ¨¡å‹é€‚é…**

- **é€šç”¨å‚æ•°é€ä¼ **ï¼šæ”¯æŒå‘ API é€ä¼ ä»»æ„é«˜çº§å‚æ•°
- **Gemini é€‚é…**ï¼šåŸç”Ÿå‚æ•°æ”¯æŒï¼Œå†…ç½®å®‰å…¨ç­–ç•¥æ”¾å®½

**ğŸ› Bug ä¿®å¤**

- ä¿®å¤è‹¥å¹²å·²çŸ¥é—®é¢˜ï¼Œæå‡ç³»ç»Ÿç¨³å®šæ€§


### 2026/01/10 - mcp-v3.0.0~v3.1.5

- **Breaking Change**ï¼šæ‰€æœ‰å·¥å…·è¿”å›å€¼ç»Ÿä¸€ä¸º `{success, summary, data, error}` ç»“æ„
- **å¼‚æ­¥ä¸€è‡´æ€§**ï¼šæ‰€æœ‰ 21 ä¸ªå·¥å…·å‡½æ•°ä½¿ç”¨ `asyncio.to_thread()` åŒ…è£…åŒæ­¥è°ƒç”¨
- **MCP Resources**ï¼šæ–°å¢ 4 ä¸ªèµ„æºï¼ˆplatformsã€rss-feedsã€available-datesã€keywordsï¼‰
- **RSS å¢å¼º**ï¼š`get_latest_rss` æ”¯æŒå¤šæ—¥æŸ¥è¯¢ï¼ˆdays å‚æ•°ï¼‰ï¼Œè·¨æ—¥æœŸ URL å»é‡
- **æ­£åˆ™åŒ¹é…ä¿®å¤**ï¼š`get_trending_topics` æ”¯æŒ `/pattern/` æ­£åˆ™è¯­æ³•å’Œ `display_name`
- **ç¼“å­˜ä¼˜åŒ–**ï¼šæ–°å¢ `make_cache_key()` å‡½æ•°ï¼Œå‚æ•°æ’åº+MD5 å“ˆå¸Œç¡®ä¿ä¸€è‡´æ€§
- **æ–°å¢ check_version å·¥å…·**ï¼šæ”¯æŒåŒæ—¶æ£€æŸ¥ TrendRadar å’Œ MCP Server ç‰ˆæœ¬æ›´æ–°


&lt;details&gt;
&lt;summary&gt;ğŸ‘‰ ç‚¹å‡»å±•å¼€ï¼š&lt;strong&gt;å†å²æ›´æ–°&lt;/strong&gt;&lt;/summary&gt;


### 2026/01/10 - v5.0.0

&gt; **å¼€å‘å°æ’æ›²**ï¼š
&gt; è‡´æ•¬é‚£ä¸ªé™ªä¼´æˆ‘ä¸¤å¹´å¤šã€å´åœ¨åˆšç»­è´¹ååæ‰‹å¼¹å‡º `&quot;This organization has been disabled&quot;` çš„æŸ C å‚æ¨¡å‹

**âœ¨ æ¨é€å†…å®¹&quot;äº”å¤§æ¿å—&quot;é‡æ„**

æœ¬æ¬¡æ›´æ–°å¯¹æ¨é€æ¶ˆæ¯è¿›è¡Œäº†åŒºåŸŸåŒ–é‡æ„ï¼Œç°åœ¨æ¨é€å†…å®¹æ¸…æ™°åœ°åˆ’åˆ†ä¸ºäº”å¤§æ ¸å¿ƒæ¿å—ï¼š

1.  **ğŸ“Š çƒ­æ¦œæ–°é—»**ï¼šæ ¹æ®æ‚¨çš„å…³é”®è¯ç²¾å‡†ç­›é€‰åçš„å…¨ç½‘çƒ­ç‚¹èšåˆã€‚
2.  **ğŸ“° RSS è®¢é˜…**ï¼šæ‚¨çš„ä¸ªæ€§åŒ–è®¢é˜…æºå†…å®¹ï¼Œæ”¯æŒæŒ‰å…³é”®è¯åˆ†ç»„ã€‚
3.  **ğŸ†• æœ¬æ¬¡æ–°å¢**ï¼šå®æ—¶æ•æ‰è‡ªä¸Šæ¬¡è¿è¡Œä»¥æ¥çš„å…¨æ–°çƒ­ç‚¹ï¼ˆå¸¦ ğŸ†• æ ‡è®°ï¼‰ã€‚
4.  **ğŸ“‹ ç‹¬ç«‹å±•ç¤ºåŒº**ï¼šæŒ‡å®šå¹³å°çš„å®Œæ•´çƒ­æ¦œæˆ– RSS æºå±•ç¤ºï¼Œ**å®Œå…¨ä¸å—å…³é”®è¯è¿‡æ»¤é™åˆ¶**ã€‚
5.  **âœ¨ AI åˆ†ææ¿å—**ï¼šç”± AI é©±åŠ¨çš„æ·±åº¦æ´å¯Ÿï¼ŒåŒ…å«è¶‹åŠ¿æ¦‚è¿°ã€çƒ­åº¦èµ°åŠ¿åŠ**æå…¶é‡è¦**çš„æƒ…æ„Ÿå€¾å‘åˆ†æã€‚

**âœ¨ AI æ™ºèƒ½åˆ†ææ¨é€åŠŸèƒ½**

- **AI åˆ†æé›†æˆ**ï¼šä½¿ç”¨ AI å¤§æ¨¡å‹å¯¹æ¨é€å†…å®¹è¿›è¡Œæ·±åº¦åˆ†æï¼Œè‡ªåŠ¨ç”Ÿæˆçƒ­ç‚¹è¶‹åŠ¿æ¦‚è¿°ã€å…³é”®è¯çƒ­åº¦åˆ†æã€è·¨å¹³å°å…³è”ã€æ½œåœ¨å½±å“è¯„ä¼°ç­‰
- **æƒ…æ„Ÿå€¾å‘åˆ†æ**ï¼šæ–°å¢æ·±åº¦æƒ…æ„Ÿè¯†åˆ«ï¼Œç²¾å‡†æ•æ‰èˆ†è®ºçš„æ­£è´Ÿé¢ã€äº‰è®®æˆ–æ‹…å¿§æƒ…ç»ª
- **å¤š AI æä¾›å•†æ”¯æŒ**ï¼šæ”¯æŒ DeepSeekï¼ˆé»˜è®¤ï¼Œæ€§ä»·æ¯”é«˜ï¼‰ã€OpenAIã€Google Gemini åŠä»»æ„ OpenAI å…¼å®¹æ¥å£
- **ä¸¤ç§æ¨é€æ¨¡å¼**ï¼š`only_analysis`ï¼ˆä»… AI åˆ†æï¼‰ã€`both`ï¼ˆä¸¤è€…éƒ½æ¨é€ï¼‰
- **è‡ªå®šä¹‰æç¤ºè¯**ï¼šé€šè¿‡ `config/ai_analysis_prompt.txt` æ–‡ä»¶è‡ªå®šä¹‰ AI åˆ†æè§’è‰²å’Œè¾“å‡ºæ ¼å¼
- **å¤šç»´åº¦æ•°æ®åˆ†æ**ï¼šAI å¯åˆ†ææ’åå˜åŒ–ã€çƒ­åº¦æŒç»­æ—¶é—´ã€è·¨å¹³å°è¡¨ç°ã€è¶‹åŠ¿é¢„æµ‹ç­‰

**ğŸ“‹ ç‹¬ç«‹å±•ç¤ºåŒºåŠŸèƒ½**

- **å®Œæ•´çƒ­æ¦œå±•ç¤º**ï¼šæŒ‡å®šå¹³å°çš„å®Œæ•´çƒ­æ¦œå•ç‹¬å±•ç¤ºï¼Œä¸å—å…³é”®è¯è¿‡æ»¤å½±å“
- **RSS ç‹¬ç«‹å±•ç¤º**ï¼šRSS æºå†…å®¹å¯å®Œæ•´å±•ç¤ºï¼Œé€‚åˆå†…å®¹è¾ƒå°‘çš„è®¢é˜…æº
- **çµæ´»é…ç½®**ï¼šæ”¯æŒé…ç½®å±•ç¤ºå¹³å°åˆ—è¡¨ã€RSS æºåˆ—è¡¨ã€æœ€å¤§å±•ç¤ºæ¡æ•°

**ğŸ“Š æ¨é€ä½“éªŒé‡æ„**

- **æ’ç‰ˆå‡çº§**ï¼šé‡æ–°è®¾è®¡å¹¶ç»Ÿä¸€å„æ¸ é“ç»Ÿè®¡å¤´éƒ¨ï¼Œå¼ºåŒ–åŒºå—ç»„ç»‡ï¼Œæ¶ˆæ¯å±‚æ¬¡ä¸€ç›®äº†ç„¶
- **é…ç½®ç®€åŒ–**ï¼šä¼˜åŒ–é£ä¹¦ç­‰é€šçŸ¥æ¸ é“çš„é…ç½®é€»è¾‘ï¼Œä¸Šæ‰‹æ›´ç®€å•
- **çƒ­åº¦è¶‹åŠ¿ç®­å¤´**ï¼šæ–°å¢ ğŸ”º(ä¸Šå‡)ã€ğŸ”»(ä¸‹é™)ã€â–(æŒå¹³) è¶‹åŠ¿æ ‡è¯†ï¼Œç›´è§‚å±•ç¤ºçƒ­åº¦å˜åŒ–
- **é€šç”¨ Webhook**ï¼šæ”¯æŒè‡ªå®šä¹‰ Webhook URL å’Œ JSON æ¨¡æ¿ï¼Œè½»æ¾é€‚é… Discordã€Matrixã€IFTTT ç­‰ä»»æ„å¹³å°

**ğŸ”§ é…ç½®ä¼˜åŒ–**

- **é¢‘ç‡è¯é…ç½®å¢å¼º**ï¼šæ–°å¢ `[ç»„åˆ«å]` è¯­æ³•ï¼Œæ”¯æŒ `#` æ³¨é‡Šè¡Œï¼Œé…ç½®æ›´æ¸…æ™°ï¼ˆæ„Ÿè°¢ [@songge8](https://github.com/sansan0/TrendRadar/issues/752) æå‡ºçš„å»ºè®®ï¼‰
- **ç¯å¢ƒå˜é‡æ”¯æŒ**ï¼šAI åˆ†æç›¸å…³é…ç½®æ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–ï¼ˆ`AI_API_KEY`ã€`AI_PROVIDER` ç­‰ï¼‰

&gt; ğŸ’¡ è¯¦ç»†é…ç½®æ•™ç¨‹è§ [è®© AI å¸®æˆ‘åˆ†æçƒ­ç‚¹](#12-è®©-ai-å¸®æˆ‘åˆ†æçƒ­ç‚¹)


### 2026/01/02 - v4.7.0

- **ä¿®å¤ RSS HTML æ˜¾ç¤º**ï¼šä¿®å¤ RSS æ•°æ®æ ¼å¼ä¸åŒ¹é…å¯¼è‡´çš„æ¸²æŸ“é—®é¢˜ï¼Œç°åœ¨æŒ‰å…³é”®è¯åˆ†ç»„æ­£ç¡®æ˜¾ç¤º
- **æ–°å¢æ­£åˆ™è¡¨è¾¾å¼è¯­æ³•**ï¼šå…³é”®è¯é…ç½®æ”¯æŒ `/pattern/` æ­£åˆ™è¯­æ³•ï¼Œè§£å†³è‹±æ–‡å­å­—ç¬¦ä¸²è¯¯åŒ¹é…é—®é¢˜ï¼ˆå¦‚ `ai` åŒ¹é… `training`ï¼‰[ğŸ“– æŸ¥çœ‹è¯­æ³•è¯¦è§£](#å…³é”®è¯åŸºç¡€è¯­æ³•)
- **æ–°å¢æ˜¾ç¤ºåç§°è¯­æ³•**ï¼šä½¿ç”¨ `=&gt; å¤‡æ³¨` ç»™å¤æ‚çš„æ­£åˆ™è¡¨è¾¾å¼èµ·ä¸ªå¥½è®°çš„åå­—ï¼Œæ¨é€æ¶ˆæ¯æ˜¾ç¤ºæ›´æ¸…æ™°ï¼ˆå¦‚ `/\bai\b/ =&gt; AIç›¸å…³`ï¼‰
- **ä¸ä¼šå†™æ­£åˆ™ï¼Ÿ** README æ–°å¢ AI ç”Ÿæˆæ­£åˆ™çš„å¼•å¯¼ï¼Œå‘Šè¯‰ ChatGPT/Gemini/DeepSeek ä½ æƒ³åŒ¹é…ä»€ä¹ˆï¼Œè®© AI å¸®ä½ å†™


### 2025/12/30 - mcp-v2.0.0

- **æ¶æ„è°ƒæ•´**ï¼šç§»é™¤ TXT æ”¯æŒï¼Œç»Ÿä¸€ä½¿ç”¨ SQLite æ•°æ®åº“
- **RSS æŸ¥è¯¢**ï¼šæ–°å¢ `get_latest_rss`ã€`search_rss`ã€`get_rss_feeds_status`
- **ç»Ÿä¸€æœç´¢**ï¼š`search_news` æ”¯æŒ `include_rss` å‚æ•°åŒæ—¶æœç´¢çƒ­æ¦œå’Œ RSS


### 2026/01/01 - v4.6.0

- **ä¿®å¤ RSS HTML æ˜¾ç¤º**ï¼šå°† RSS å†…å®¹åˆå¹¶åˆ°çƒ­æ¦œ HTML é¡µé¢ï¼ŒæŒ‰æºåˆ†ç»„æ˜¾ç¤º
- **æ–°å¢ display_mode é…ç½®**ï¼šæ”¯æŒ `keyword`ï¼ˆæŒ‰å…³é”®è¯åˆ†ç»„ï¼‰å’Œ `platform`ï¼ˆæŒ‰å¹³å°åˆ†ç»„ï¼‰ä¸¤ç§æ˜¾ç¤ºæ¨¡å¼


### 2025/12/30 - v4.5.0

- **RSS è®¢é˜…æºæ”¯æŒ**ï¼šæ–°å¢ RSS/Atom æŠ“å–ï¼ŒæŒ‰å…³é”®è¯åˆ†ç»„ç»Ÿè®¡ï¼ˆä¸çƒ­æ¦œæ ¼å¼ä¸€è‡´ï¼‰
- **å­˜å‚¨ç»“æ„é‡æ„**ï¼šæ‰å¹³åŒ–ç›®å½•ç»“æ„ `output/{type}/{date}.db`
- **ç»Ÿä¸€æ’åºé…ç½®**ï¼š`sort_by_position_first` åŒæ—¶å½±å“çƒ­æ¦œå’Œ RSS
- **é…ç½®ç»“æ„é‡æ„**ï¼š`config.yaml` é‡æ–°ç»„ç»‡ä¸º 7 ä¸ªé€»è¾‘åˆ†ç»„ï¼ˆappã€reportã€notificationã€storageã€platformsã€rssã€advancedï¼‰ï¼Œé…ç½®è·¯å¾„æ›´æ¸…æ™°


### 2025/12/26 - mcp-v1.2.0

  **MCP æ¨¡å—æ›´æ–° - ä¼˜åŒ–å·¥å…·é›†ï¼Œæ–°å¢èšåˆå¯¹æ¯”åŠŸèƒ½ï¼Œåˆå¹¶å†—ä½™å·¥å…·:**
  - æ–°å¢ `aggregate_news` å·¥å…· - è·¨å¹³å°æ–°é—»å»é‡èšåˆ
  - æ–°å¢ `compare_periods` å·¥å…· - æ—¶æœŸå¯¹æ¯”åˆ†æï¼ˆå‘¨ç¯æ¯”/æœˆç¯æ¯”ï¼‰
  - åˆå¹¶ `find_similar_news` + `search_related_news_history` â†’ `find_related_news`
  - å¢å¼º `get_trending_topics` - æ–°å¢ `auto_extract` æ¨¡å¼è‡ªåŠ¨æå–çƒ­ç‚¹
  - ä¿®å¤è‹¥å¹²bug
  - åŒæ­¥æ›´æ–° README-MCP-FAQ.md æ–‡æ¡£çš„ä¸­è‹±æ–‡ç‰ˆ (Q1-Q18)


### 2025/12/20 - v4.0.3

- æ–°å¢ URL æ ‡å‡†åŒ–åŠŸèƒ½ï¼Œè§£å†³å¾®åšç­‰å¹³å°å› åŠ¨æ€å‚æ•°ï¼ˆå¦‚ `band_rank`ï¼‰å¯¼è‡´çš„é‡å¤æ¨é€é—®é¢˜
- ä¿®å¤å¢é‡æ¨¡å¼æ£€æµ‹é€»è¾‘ï¼Œæ­£ç¡®è¯†åˆ«å†å²æ ‡é¢˜


### 2025/12/17 - v4.0.1

- StorageManager æ·»åŠ æ¨é€è®°å½•ä»£ç†æ–¹æ³•
- S3 å®¢æˆ·ç«¯åˆ‡æ¢è‡³ virtual-hosted style ä»¥æå‡å…¼å®¹æ€§ï¼ˆæ”¯æŒè…¾è®¯äº‘ COS ç­‰æ›´å¤šæœåŠ¡ï¼‰


### 2025/12/13 - mcp-v1.1.0

  **MCP æ¨¡å—æ›´æ–°:**
  - é€‚é… v4.0.0ï¼ŒåŒæ—¶ä¹Ÿå…¼å®¹ v3.x çš„æ•°æ®
  - æ–°å¢å­˜å‚¨åŒæ­¥å·¥å…·ï¼š`sync_from_remote`ã€`get_storage_status`ã€`list_available_dates`


### 2025/12/13 - v4.0.0

**ğŸ‰ é‡å¤§æ›´æ–°ï¼šå…¨é¢é‡æ„å­˜å‚¨å’Œæ ¸å¿ƒæ¶æ„**

- **å¤šå­˜å‚¨åç«¯æ”¯æŒ**ï¼šå¼•å…¥å…¨æ–°çš„å­˜å‚¨æ¨¡å—ï¼Œæ”¯æŒæœ¬åœ° SQLite å’Œè¿œç¨‹äº‘å­˜å‚¨ï¼ˆS3 å…¼å®¹åè®®ï¼Œä¾‹å¦‚ Cloudflare R2ï¼‰ï¼Œé€‚åº” GitHub Actionsã€Docker å’Œæœ¬åœ°ç¯å¢ƒã€‚
- **æ•°æ®åº“ç»“æ„ä¼˜åŒ–**ï¼šé‡æ„ SQLite æ•°æ®åº“è¡¨ç»“æ„ï¼Œæå‡æ•°æ®æ•ˆç‡å’ŒæŸ¥è¯¢èƒ½åŠ›ã€‚
- **æ ¸å¿ƒä»£ç æ¨¡å—åŒ–**ï¼šå°†ä¸»ç¨‹åºé€»è¾‘æ‹†åˆ†ä¸º trendradar åŒ…çš„å¤šä¸ªæ¨¡å—ï¼Œæ˜¾è‘—æå‡ä»£ç å¯ç»´æŠ¤æ€§ã€‚
- **å¢å¼ºåŠŸèƒ½**ï¼šå®ç°æ—¥æœŸæ ¼å¼æ ‡å‡†åŒ–ã€æ•°æ®ä¿ç•™ç­–ç•¥ã€æ—¶åŒºé…ç½®æ”¯æŒã€æ—¶é—´æ˜¾ç¤ºä¼˜åŒ–ï¼Œå¹¶ä¿®å¤è¿œç¨‹å­˜å‚¨æ•°æ®æŒä¹…åŒ–é—®é¢˜ï¼Œç¡®ä¿æ•°æ®åˆå¹¶çš„å‡†ç¡®æ€§ã€‚
- **æ¸…ç†å’Œå…¼å®¹**ï¼šç§»é™¤äº†å¤§éƒ¨åˆ†å†å²å…¼å®¹ä»£ç ï¼Œç»Ÿä¸€äº†æ•°æ®å­˜å‚¨å’Œè¯»å–æ–¹å¼ã€‚


### 2025/12/03 - v3.5.0

**ğŸ‰ æ ¸å¿ƒåŠŸèƒ½å¢å¼º**

1. **å¤šè´¦å·æ¨é€æ”¯æŒ**
   - æ‰€æœ‰æ¨é€æ¸ é“ï¼ˆé£ä¹¦ã€é’‰é’‰ã€ä¼ä¸šå¾®ä¿¡ã€Telegramã€ntfyã€Barkã€Slackï¼‰æ”¯æŒå¤šè´¦å·é…ç½®
   - ä½¿ç”¨åˆ†å· `;` åˆ†éš”å¤šä¸ªè´¦å·ï¼Œä¾‹å¦‚ï¼š`FEISHU_WEBHOOK_URL=url1;url2`
   - è‡ªåŠ¨éªŒè¯é…å¯¹é…ç½®ï¼ˆå¦‚ Telegram çš„ token å’Œ chat_idï¼‰æ•°é‡ä¸€è‡´æ€§

2. **æ¨é€å†…å®¹é¡ºåºå¯é…ç½®**
   - æ–°å¢ `reverse_content_order` é…ç½®é¡¹
   - æ”¯æŒè‡ªå®šä¹‰çƒ­ç‚¹è¯æ±‡ç»Ÿè®¡ä¸æ–°å¢çƒ­ç‚¹æ–°é—»çš„æ˜¾ç¤ºé¡ºåº

3. **å…¨å±€è¿‡æ»¤å…³é”®è¯**
   - æ–°å¢ `[GLOBAL_FILTER]` åŒºåŸŸæ ‡è®°ï¼Œæ”¯æŒå…¨å±€è¿‡æ»¤ä¸æƒ³çœ‹åˆ°çš„å†…å®¹
   - é€‚ç”¨åœºæ™¯ï¼šè¿‡æ»¤å¹¿å‘Šã€è¥é”€ã€ä½è´¨å†…å®¹ç­‰

**ğŸ³ Docker åŒè·¯å¾„ HTML ç”Ÿæˆä¼˜åŒ–**

- **é—®é¢˜ä¿®å¤**ï¼šè§£å†³ Docker ç¯å¢ƒä¸‹ `index.html` æ— æ³•åŒæ­¥åˆ°å®¿ä¸»æœºçš„é—®é¢˜
- **åŒè·¯å¾„ç”Ÿæˆ**ï¼šå½“æ—¥æ±‡æ€» HTML åŒæ—¶ç”Ÿæˆåˆ°ä¸¤ä¸ªä½ç½®
  - `index.html`ï¼ˆé¡¹ç›®æ ¹ç›®å½•ï¼‰ï¼šä¾› GitHub Pages è®¿é—®
  - `output/index.html`ï¼šé€šè¿‡ Docker Volume æŒ‚è½½ï¼Œå®¿ä¸»æœºå¯ç›´æ¥è®¿é—®
- **å…¼å®¹æ€§**ï¼šç¡®ä¿ Dockerã€GitHub Actionsã€æœ¬åœ°è¿è¡Œç¯å¢ƒå‡èƒ½æ­£å¸¸è®¿é—®ç½‘é¡µç‰ˆæŠ¥å‘Š

**ğŸ³ Docker MCP é•œåƒæ”¯æŒ**

- æ–°å¢ç‹¬ç«‹çš„ MCP æœåŠ¡é•œåƒ `wantcat/trendradar-mcp`
- æ”¯æŒ Docker éƒ¨ç½² AI åˆ†æåŠŸèƒ½ï¼Œé€šè¿‡ HTTP æ¥å£ï¼ˆç«¯å£ 3333ï¼‰æä¾›æœåŠ¡
- åŒå®¹å™¨æ¶æ„ï¼šæ–°é—»æ¨é€æœåŠ¡ä¸ MCP æœåŠ¡ç‹¬ç«‹è¿è¡Œï¼Œå¯åˆ†åˆ«æ‰©å±•å’Œé‡å¯
- è¯¦è§ [Docker éƒ¨ç½² - MCP æœåŠ¡](#6-docker-éƒ¨ç½²)

**ğŸŒ Web æœåŠ¡å™¨æ”¯æŒ**

- æ–°å¢å†…ç½® Web æœåŠ¡å™¨ï¼Œæ”¯æŒé€šè¿‡æµè§ˆå™¨è®¿é—®ç”Ÿæˆçš„æŠ¥å‘Š
- é€šè¿‡ `manage.py` å‘½ä»¤æ§åˆ¶å¯åŠ¨/åœæ­¢ï¼š`docker exec -it trendradar python manage.py start_webserver`
- è®¿é—®åœ°å€ï¼š`http://localhost:8080`ï¼ˆç«¯å£å¯é…ç½®ï¼‰
- å®‰å…¨ç‰¹æ€§ï¼šé™æ€æ–‡ä»¶æœåŠ¡ã€ç›®å½•é™åˆ¶ã€æœ¬åœ°è®¿é—®
- æ”¯æŒè‡ªåŠ¨å¯åŠ¨å’Œæ‰‹åŠ¨æ§åˆ¶ä¸¤ç§æ¨¡å¼

**ğŸ“– æ–‡æ¡£ä¼˜åŒ–**

- æ–°å¢ [æ¨é€å†…å®¹æ€ä¹ˆæ˜¾ç¤ºï¼Ÿ](#7-æ¨é€å†…å®¹æ€ä¹ˆæ˜¾ç¤º) ç« èŠ‚ï¼šè‡ªå®šä¹‰æ¨é€æ ·å¼å’Œå†…å®¹
- æ–°å¢ [ä»€ä¹ˆæ—¶å€™ç»™æˆ‘æ¨é€ï¼Ÿ](#8-ä»€ä¹ˆæ—¶å€™ç»™æˆ‘æ¨é€) ç« èŠ‚ï¼šè®¾ç½®æ¨é€æ—¶é—´æ®µ
- æ–°å¢ [å¤šä¹…è¿è¡Œä¸€æ¬¡ï¼Ÿ](#9-å¤šä¹…è¿è¡Œä¸€æ¬¡) ç« èŠ‚ï¼šè®¾ç½®è‡ªåŠ¨è¿è¡Œé¢‘ç‡
- æ–°å¢ [æ¨é€åˆ°å¤šä¸ªç¾¤/è®¾å¤‡](#10-æ¨é€åˆ°å¤šä¸ªç¾¤è®¾å¤‡) ç« èŠ‚ï¼šåŒæ—¶æ¨é€ç»™å¤šä¸ªæ¥æ”¶è€…
- ä¼˜åŒ–å„é…ç½®ç« èŠ‚ï¼šç»Ÿä¸€æ·»åŠ &quot;é…ç½®ä½ç½®&quot;è¯´æ˜
- ç®€åŒ–å¿«é€Ÿå¼€å§‹é…ç½®è¯´æ˜ï¼šä¸‰ä¸ªæ ¸å¿ƒæ–‡ä»¶ä¸€ç›®äº†ç„¶
- ä¼˜åŒ– [Docker éƒ¨ç½²](#6-docker-éƒ¨ç½²) ç« èŠ‚ï¼šæ–°å¢é•œåƒè¯´æ˜ã€æ¨è git clone éƒ¨ç½²ã€é‡ç»„éƒ¨ç½²æ–¹å¼

**ğŸ”§ å‡çº§è¯´æ˜**ï¼š
- **GitHub Fork ç”¨æˆ·**ï¼šæ›´æ–° `main.py`ã€`config/config.yaml`ï¼ˆæ–°å¢å¤šè´¦å·æ¨é€æ”¯æŒï¼Œæ— éœ€ä¿®æ”¹ç°æœ‰é…ç½®ï¼‰
- **å¤šè´¦å·æ¨é€**ï¼šæ–°åŠŸèƒ½ï¼Œé»˜è®¤ä¸å¯ç”¨ï¼Œç°æœ‰å•è´¦å·é…ç½®ä¸å—å½±å“


### 2025/11/26 - mcp-v1.0.3

  **MCP æ¨¡å—æ›´æ–°:**
  - æ–°å¢æ—¥æœŸè§£æå·¥å…· resolve_date_range,è§£å†³ AI æ¨¡å‹è®¡ç®—æ—¥æœŸä¸ä¸€è‡´çš„é—®é¢˜
  - æ”¯æŒè‡ªç„¶è¯­è¨€æ—¥æœŸè¡¨è¾¾å¼è§£æ(æœ¬å‘¨ã€æœ€è¿‘7å¤©ã€ä¸Šæœˆç­‰)
  - å·¥å…·æ€»æ•°ä» 13 ä¸ªå¢åŠ åˆ° 14 ä¸ª


### 2025/11/28 - v3.4.1

**ğŸ”§ æ ¼å¼ä¼˜åŒ–**

1. **Bark æ¨é€å¢å¼º**
   - Bark ç°æ”¯æŒ Markdown æ¸²æŸ“
   - å¯ç”¨åŸç”Ÿ Markdown æ ¼å¼ï¼šç²—ä½“ã€é“¾æ¥ã€åˆ—è¡¨ã€ä»£ç å—ç­‰
   - ç§»é™¤çº¯æ–‡æœ¬è½¬æ¢ï¼Œå……åˆ†åˆ©ç”¨ Bark åŸç”Ÿæ¸²æŸ“èƒ½åŠ›

2. **Slack æ ¼å¼ç²¾å‡†åŒ–**
   - ä½¿ç”¨ä¸“ç”¨ mrkdwn æ ¼å¼å¤„ç†åˆ†æ‰¹å†…å®¹
   - æå‡å­—èŠ‚å¤§å°ä¼°ç®—å‡†ç¡®æ€§ï¼ˆé¿å…æ¶ˆæ¯è¶…é™ï¼‰
   - ä¼˜åŒ–é“¾æ¥æ ¼å¼ï¼š`&lt;url|text&gt;` å’ŒåŠ ç²—è¯­æ³•ï¼š`*text*`

3. **æ€§èƒ½æå‡**
   - æ ¼å¼è½¬æ¢åœ¨åˆ†æ‰¹è¿‡ç¨‹ä¸­å®Œæˆï¼Œé¿å…äºŒæ¬¡å¤„ç†
   - å‡†ç¡®ä¼°ç®—æ¶ˆæ¯å¤§å°ï¼Œå‡å°‘å‘é€å¤±è´¥ç‡

**ğŸ”§ å‡çº§è¯´æ˜**ï¼š
- **GitHub Fork ç”¨æˆ·**ï¼šæ›´æ–° `main.py`ï¼Œ`config.yaml`


### 2025/11/25 - v3.4.0

**ğŸ‰ æ–°å¢ Slack æ¨é€æ”¯æŒ**

1. **å›¢é˜Ÿåä½œæ¨é€æ¸ é“**
   - æ”¯æŒ Slack Incoming Webhooksï¼ˆå…¨çƒæµè¡Œçš„å›¢é˜Ÿåä½œå·¥å…·ï¼‰
   - æ¶ˆæ¯é›†ä¸­ç®¡ç†ï¼Œé€‚åˆå›¢é˜Ÿå…±äº«çƒ­ç‚¹èµ„è®¯
   - æ”¯æŒ mrkdwn æ ¼å¼ï¼ˆç²—ä½“ã€é“¾æ¥ç­‰ï¼‰

2. **å¤šç§éƒ¨ç½²æ–¹å¼**
   - GitHub Actionsï¼šé…ç½® `SLACK_WEBHOOK_URL` Secret
   - Dockerï¼šç¯å¢ƒå˜é‡ `SLACK_WEBHOOK_URL`
   - æœ¬åœ°è¿è¡Œï¼š`config/config.yaml` é…ç½®æ–‡ä»¶


&gt; ğŸ“– **è¯¦ç»†é…ç½®æ•™ç¨‹**ï¼š[å¿«é€Ÿå¼€å§‹ - Slack æ¨é€](#-å¿«é€Ÿå¼€å§‹)

- ä¼˜åŒ– setup-windows.bat å’Œ setup-windows-en.bat ä¸€é”®å®‰è£… MCP çš„ä½“éªŒ

**ğŸ”§ å‡çº§è¯´æ˜**ï¼š
- **GitHub Fork ç”¨æˆ·**ï¼šæ›´æ–° `main.py`ã€`config/config.yaml`ã€`.github/workflows/crawler.yml`


### 2025/11/24 - v3.3.0

**ğŸ‰ æ–°å¢ Bark æ¨é€æ”¯æŒ**

1. **iOS ä¸“å±æ¨é€æ¸ é“**
   - æ”¯æŒ Bark æ¨é€ï¼ˆåŸºäº APNsï¼ŒiOS å¹³å°ï¼‰
   - å…è´¹å¼€æºï¼Œç®€æ´é«˜æ•ˆï¼Œæ— å¹¿å‘Šå¹²æ‰°
   - æ”¯æŒå®˜æ–¹æœåŠ¡å™¨å’Œè‡ªå»ºæœåŠ¡å™¨ä¸¤ç§æ–¹å¼

2. **å¤šç§éƒ¨ç½²æ–¹å¼**
   - GitHub Actionsï¼šé…ç½® `BARK_URL` Secret
   - Dockerï¼šç¯å¢ƒå˜é‡ `BARK_URL`
   - æœ¬åœ°è¿è¡Œï¼š`config/config.yaml` é…ç½®æ–‡ä»¶

&gt; ğŸ“– **è¯¦ç»†é…ç½®æ•™ç¨‹**ï¼š[å¿«é€Ÿå¼€å§‹ - Bark æ¨é€](#-å¿«é€Ÿå¼€å§‹)

**ğŸ› Bug ä¿®å¤**
- ä¿®å¤ `config.yaml` ä¸­ `ntfy_server_url` é…ç½®ä¸ç”Ÿæ•ˆçš„é—®é¢˜ ([#345](https://github.com/sansan0/TrendRadar/issues/345))

**ğŸ”§ å‡çº§è¯´æ˜**ï¼š
- **GitHub Fork ç”¨æˆ·**ï¼šæ›´æ–° `main.py`ã€`config/config.yaml`ã€`.github/workflows/crawler.yml`

### 2025/11/23 - v3.2.0

**ğŸ¯ æ–°å¢é«˜çº§å®šåˆ¶åŠŸèƒ½**

1. **å…³é”®è¯æ’åºä¼˜å…ˆçº§é…ç½®**
   - æ”¯æŒä¸¤ç§æ’åºç­–ç•¥ï¼šçƒ­åº¦ä¼˜å…ˆ vs é…ç½®é¡ºåºä¼˜å…ˆ
   - æ»¡è¶³ä¸åŒä½¿ç”¨åœºæ™¯ï¼šçƒ­ç‚¹è¿½è¸ª or ä¸ªæ€§åŒ–å…³æ³¨

2. **æ˜¾ç¤ºæ•°é‡ç²¾å‡†æ§åˆ¶**
   - å…¨å±€é…ç½®ï¼šç»Ÿä¸€é™åˆ¶æ‰€æœ‰å…³é”®è¯æ˜¾ç¤ºæ•°é‡
   - å•ç‹¬é…ç½®ï¼šä½¿ç”¨ `@æ•°å­—` è¯­æ³•ä¸ºç‰¹å®šå…³é”®è¯è®¾ç½®é™åˆ¶
   - æœ‰æ•ˆæ§åˆ¶æ¨é€é•¿åº¦ï¼Œçªå‡ºé‡ç‚¹å†…å®¹

&gt; ğŸ“– **è¯¦ç»†é…ç½®æ•™ç¨‹**ï¼š[å…³é”®è¯é…ç½® - é«˜çº§é…ç½®](#å…³é”®è¯é«˜çº§é…ç½®)

**ğŸ”§ å‡çº§è¯´æ˜**ï¼š
- **GitHub Fork ç”¨æˆ·**ï¼šæ›´æ–° `main.py`ã€`config/config.yaml`


### 2025/11/18 - mcp-v1.0.2

  **MCP æ¨¡å—æ›´æ–°:**
  - ä¼˜åŒ–æŸ¥è¯¢ä»Šæ—¥æ–°é—»å´å¯èƒ½é”™è¯¯è¿”å›è¿‡å»æ—¥æœŸçš„æƒ…å†µ


### 2025/11/22 - v3.1.1

- **ä¿®å¤æ•°æ®å¼‚å¸¸å¯¼è‡´çš„å´©æºƒé—®é¢˜**ï¼šè§£å†³éƒ¨åˆ†ç”¨æˆ·åœ¨ GitHub Actions ç¯å¢ƒä¸­é‡åˆ°çš„ `&#039;float&#039; object has no attribute &#039;lower&#039;` é”™è¯¯
- æ–°å¢åŒé‡é˜²æŠ¤æœºåˆ¶ï¼šåœ¨æ•°æ®è·å–é˜¶æ®µè¿‡æ»¤æ— æ•ˆæ ‡é¢˜ï¼ˆNoneã€floatã€ç©ºå­—ç¬¦ä¸²ï¼‰ï¼ŒåŒæ—¶åœ¨å‡½æ•°è°ƒç”¨å¤„æ·»åŠ ç±»å‹æ£€æŸ¥
- æå‡ç³»ç»Ÿç¨³å®šæ€§ï¼Œç¡®ä¿åœ¨æ•°æ®æºè¿”å›å¼‚å¸¸æ ¼å¼æ—¶ä»èƒ½æ­£å¸¸è¿è¡Œ

**å‡çº§è¯´æ˜**ï¼ˆGitHub Fork ç”¨æˆ·ï¼‰ï¼š
- å¿…é¡»æ›´æ–°ï¼š`main.py`
- å»ºè®®ä½¿ç”¨å°ç‰ˆæœ¬å‡çº§æ–¹å¼ï¼šå¤åˆ¶æ›¿æ¢ä¸Šè¿°æ–‡ä»¶


### 2025/11/20 - v3.1.0

- **æ–°å¢ä¸ªäººå¾®ä¿¡æ¨é€æ”¯æŒ**ï¼šä¼ä¸šå¾®ä¿¡åº”ç”¨å¯æ¨é€åˆ°ä¸ªäººå¾®ä¿¡ï¼Œæ— éœ€å®‰è£…ä¼ä¸šå¾®ä¿¡ APP
- æ”¯æŒä¸¤ç§æ¶ˆæ¯æ ¼å¼ï¼š`markdown`ï¼ˆä¼ä¸šå¾®ä¿¡ç¾¤æœºå™¨äººï¼‰å’Œ `text`ï¼ˆä¸ªäººå¾®ä¿¡åº”ç”¨ï¼‰
- æ–°å¢ `WEWORK_MSG_TYPE` ç¯å¢ƒå˜é‡é…ç½®ï¼Œæ”¯æŒ GitHub Actionsã€Dockerã€docker compose ç­‰å¤šç§éƒ¨ç½²æ–¹å¼
- `text` æ¨¡å¼è‡ªåŠ¨æ¸…é™¤ Markdown è¯­æ³•ï¼Œæä¾›çº¯æ–‡æœ¬æ¨é€æ•ˆæœ
- è¯¦è§å¿«é€Ÿå¼€å§‹ä¸­çš„ã€Œä¸ªäººå¾®ä¿¡æ¨é€ã€é…ç½®è¯´æ˜

**å‡çº§è¯´æ˜**ï¼ˆGitHub Fork ç”¨æˆ·ï¼‰ï¼š
- å¿…é¡»æ›´æ–°ï¼š`main.py`ã€`config/config.yaml`
- å¯é€‰æ›´æ–°ï¼š`.github/workflows/crawler.yml`ï¼ˆå¦‚ä½¿ç”¨ GitHub Actions éƒ¨ç½²ï¼‰
- å»ºè®®ä½¿ç”¨å°ç‰ˆæœ¬å‡çº§æ–¹å¼ï¼šå¤åˆ¶æ›¿æ¢ä¸Šè¿°æ–‡ä»¶

### 2025/11/12 - v3.0.5

- ä¿®å¤é‚®ä»¶å‘é€ SSL/TLS ç«¯å£é…ç½®é€»è¾‘é”™è¯¯
- ä¼˜åŒ–é‚®ç®±æœåŠ¡å•†ï¼ˆQQ/163/126ï¼‰é»˜è®¤ä½¿ç”¨ 465 ç«¯å£ï¼ˆSSLï¼‰
- **æ–°å¢ Docker ç¯å¢ƒå˜é‡æ”¯æŒ**ï¼šæ ¸å¿ƒé…ç½®é¡¹ï¼ˆ`enable_crawler`ã€`report_mode`ã€`push_window` ç­‰ï¼‰æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼Œè§£å†³ NAS ç”¨æˆ·ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸ç”Ÿæ•ˆçš„é—®é¢˜ï¼ˆè¯¦è§ [ğŸ³ Docker éƒ¨ç½²](#-docker-éƒ¨ç½²) ç« èŠ‚ï¼‰


### 2025/10/26 - mcp-v1.0.1

  **MCP æ¨¡å—æ›´æ–°:**
  - ä¿®å¤æ—¥æœŸæŸ¥è¯¢å‚æ•°ä¼ é€’é”™è¯¯
  - ç»Ÿä¸€æ‰€æœ‰å·¥å…·çš„æ—¶é—´å‚æ•°æ ¼å¼


### 2025/10/31 - v3.0.4

- è§£å†³é£ä¹¦å› æ¨é€å†…å®¹è¿‡é•¿è€Œäº§ç”Ÿçš„é”™è¯¯ï¼Œå®ç°äº†åˆ†æ‰¹æ¨é€


### 2025/10/23 - v3.0.3

- æ‰©å¤§ ntfy é”™è¯¯ä¿¡æ¯æ˜¾ç¤ºèŒƒå›´


### 2025/10/21 - v3.0.2

- ä¿®å¤ ntfy æ¨é€ç¼–ç é—®é¢˜

### 2025/10/20 - v3.0.0

**é‡å¤§æ›´æ–° - AI åˆ†æåŠŸèƒ½ä¸Šçº¿** âœ¨

- **æ ¸å¿ƒåŠŸèƒ½**ï¼š
  - æ–°å¢åŸºäº MCP (Model Context Protocol) çš„ AI åˆ†ææœåŠ¡å™¨
  - æ”¯æŒ17ç§æ™ºèƒ½åˆ†æå·¥å…·ï¼šåŸºç¡€æŸ¥è¯¢ã€æ™ºèƒ½æ£€ç´¢ã€é«˜çº§åˆ†æã€RSS æŸ¥è¯¢ã€ç³»ç»Ÿç®¡ç†
  - è‡ªç„¶è¯­è¨€äº¤äº’ï¼šé€šè¿‡å¯¹è¯æ–¹å¼æŸ¥è¯¢å’Œåˆ†ææ–°é—»æ•°æ®
  - å¤šå®¢æˆ·ç«¯æ”¯æŒï¼šClaude Desktopã€Cherry Studioã€Cursorã€Cline ç­‰

- **åˆ†æèƒ½åŠ›**ï¼š
  - è¯é¢˜è¶‹åŠ¿åˆ†æï¼ˆçƒ­åº¦è¿½è¸ªã€ç”Ÿå‘½å‘¨æœŸã€çˆ†ç«æ£€æµ‹ã€è¶‹åŠ¿é¢„æµ‹ï¼‰
  - æ•°æ®æ´å¯Ÿï¼ˆå¹³å°å¯¹æ¯”ã€æ´»è·ƒåº¦ç»Ÿè®¡ã€å…³é”®è¯å…±ç°ï¼‰
  - æƒ…æ„Ÿåˆ†æã€ç›¸ä¼¼æ–°é—»æŸ¥æ‰¾ã€æ™ºèƒ½æ‘˜è¦ç”Ÿæˆ
  - å†å²ç›¸å…³æ–°é—»æ£€ç´¢ã€å¤šæ¨¡å¼æœç´¢

- **æ›´æ–°æç¤º**ï¼š
  - è¿™æ˜¯ç‹¬ç«‹çš„ AI åˆ†æåŠŸèƒ½ï¼Œ

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[paperless-ngx/paperless-ngx]]></title>
            <link>https://github.com/paperless-ngx/paperless-ngx</link>
            <guid>https://github.com/paperless-ngx/paperless-ngx</guid>
            <pubDate>Sun, 18 Jan 2026 00:05:34 GMT</pubDate>
            <description><![CDATA[A community-supported supercharged document management system: scan, index and archive all your documents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/paperless-ngx/paperless-ngx">paperless-ngx/paperless-ngx</a></h1>
            <p>A community-supported supercharged document management system: scan, index and archive all your documents</p>
            <p>Language: Python</p>
            <p>Stars: 35,696</p>
            <p>Forks: 2,258</p>
            <p>Stars today: 22 stars today</p>
            <h2>README</h2><pre>[![ci](https://github.com/paperless-ngx/paperless-ngx/workflows/ci/badge.svg)](https://github.com/paperless-ngx/paperless-ngx/actions)
[![Crowdin](https://badges.crowdin.net/paperless-ngx/localized.svg)](https://crowdin.com/project/paperless-ngx)
[![Documentation Status](https://img.shields.io/github/deployments/paperless-ngx/paperless-ngx/github-pages?label=docs)](https://docs.paperless-ngx.com)
[![codecov](https://codecov.io/gh/paperless-ngx/paperless-ngx/branch/main/graph/badge.svg?token=VK6OUPJ3TY)](https://codecov.io/gh/paperless-ngx/paperless-ngx)
[![Chat on Matrix](https://matrix.to/img/matrix-badge.svg)](https://matrix.to/#/%23paperlessngx%3Amatrix.org)
[![demo](https://cronitor.io/badges/ve7ItY/production/W5E_B9jkelG9ZbDiNHUPQEVH3MY.svg)](https://demo.paperless-ngx.com)

&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/paperless-ngx/paperless-ngx/blob/main/resources/logo/web/png/White%20logo%20-%20no%20background.png&quot; width=&quot;50%&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png&quot; width=&quot;50%&quot;&gt;
    &lt;img src=&quot;https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png&quot; width=&quot;50%&quot;&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;!-- omit in toc --&gt;

# Paperless-ngx

Paperless-ngx is a document management system that transforms your physical documents into a searchable online archive so you can keep, well, _less paper_.

Paperless-ngx is the official successor to the original [Paperless](https://github.com/the-paperless-project/paperless) &amp; [Paperless-ng](https://github.com/jonaswinkler/paperless-ng) projects and is designed to distribute the responsibility of advancing and supporting the project among a team of people. [Consider joining us!](#community-support)

Thanks to the generous folks at [DigitalOcean](https://m.do.co/c/8d70b916d462), a demo is available at [demo.paperless-ngx.com](https://demo.paperless-ngx.com) using login `demo` / `demo`. _Note: demo content is reset frequently and confidential information should not be uploaded._

- [Features](#features)
- [Getting started](#getting-started)
- [Contributing](#contributing)
  - [Community Support](#community-support)
  - [Translation](#translation)
  - [Feature Requests](#feature-requests)
  - [Bugs](#bugs)
- [Related Projects](#related-projects)
- [Important Note](#important-note)

&lt;p align=&quot;right&quot;&gt;This project is supported by:&lt;br/&gt;
  &lt;a href=&quot;https://m.do.co/c/8d70b916d462&quot; style=&quot;padding-top: 4px; display: block;&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_white.svg&quot; width=&quot;140px&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg&quot; width=&quot;140px&quot;&gt;
      &lt;img src=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_black_.svg&quot; width=&quot;140px&quot;&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
&lt;/p&gt;

# Features

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards-dark.png&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png&quot;&gt;
&lt;/picture&gt;

A full list of [features](https://docs.paperless-ngx.com/#features) and [screenshots](https://docs.paperless-ngx.com/#screenshots) are available in the [documentation](https://docs.paperless-ngx.com/).

# Getting started

The easiest way to deploy paperless is `docker compose`. The files in the [`/docker/compose` directory](https://github.com/paperless-ngx/paperless-ngx/tree/main/docker/compose) are configured to pull the image from the GitHub container registry.

If you&#039;d like to jump right in, you can configure a `docker compose` environment with our install script:

```bash
bash -c &quot;$(curl -L https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/install-paperless-ngx.sh)&quot;
```

More details and step-by-step guides for alternative installation methods can be found in [the documentation](https://docs.paperless-ngx.com/setup/#installation).

Migrating from Paperless-ng is easy, just drop in the new docker image! See the [documentation on migrating](https://docs.paperless-ngx.com/setup/#migrating-to-paperless-ngx) for more details.

&lt;!-- omit in toc --&gt;

### Documentation

The documentation for Paperless-ngx is available at [https://docs.paperless-ngx.com](https://docs.paperless-ngx.com/).

# Contributing

If you feel like contributing to the project, please do! Bug fixes, enhancements, visual fixes etc. are always welcome. If you want to implement something big: Please start a discussion about that! The [documentation](https://docs.paperless-ngx.com/development/) has some basic information on how to get started.

## Community Support

People interested in continuing the work on paperless-ngx are encouraged to reach out here on github and in the [Matrix Room](https://matrix.to/#/#paperless:matrix.org). If you would like to contribute to the project on an ongoing basis there are multiple [teams](https://github.com/orgs/paperless-ngx/people) (frontend, ci/cd, etc) that could use your help so please reach out!

## Translation

Paperless-ngx is available in many languages that are coordinated on Crowdin. If you want to help out by translating paperless-ngx into your language, please head over to https://crowdin.com/project/paperless-ngx, and thank you! More details can be found in [CONTRIBUTING.md](https://github.com/paperless-ngx/paperless-ngx/blob/main/CONTRIBUTING.md#translating-paperless-ngx).

## Feature Requests

Feature requests can be submitted via [GitHub Discussions](https://github.com/paperless-ngx/paperless-ngx/discussions/categories/feature-requests), you can search for existing ideas, add your own and vote for the ones you care about.

## Bugs

For bugs please [open an issue](https://github.com/paperless-ngx/paperless-ngx/issues) or [start a discussion](https://github.com/paperless-ngx/paperless-ngx/discussions) if you have questions.

# Related Projects

Please see [the wiki](https://github.com/paperless-ngx/paperless-ngx/wiki/Related-Projects) for a user-maintained list of related projects and software that is compatible with Paperless-ngx.

# Important Note

&gt; Document scanners are typically used to scan sensitive documents like your social insurance number, tax records, invoices, etc. **Paperless-ngx should never be run on an untrusted host** because information is stored in clear text without encryption. No guarantees are made regarding security (but we do try!) and you use the app at your own risk.
&gt; **The safest way to run Paperless-ngx is on a local server in your own home with backups in place**.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[anthropics/skills]]></title>
            <link>https://github.com/anthropics/skills</link>
            <guid>https://github.com/anthropics/skills</guid>
            <pubDate>Sun, 18 Jan 2026 00:05:33 GMT</pubDate>
            <description><![CDATA[Public repository for Agent Skills]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/anthropics/skills">anthropics/skills</a></h1>
            <p>Public repository for Agent Skills</p>
            <p>Language: Python</p>
            <p>Stars: 43,833</p>
            <p>Forks: 4,060</p>
            <p>Stars today: 796 stars today</p>
            <h2>README</h2><pre>&gt; **Note:** This repository contains Anthropic&#039;s implementation of skills for Claude. For information about the Agent Skills standard, see [agentskills.io](http://agentskills.io).

# Skills
Skills are folders of instructions, scripts, and resources that Claude loads dynamically to improve performance on specialized tasks. Skills teach Claude how to complete specific tasks in a repeatable way, whether that&#039;s creating documents with your company&#039;s brand guidelines, analyzing data using your organization&#039;s specific workflows, or automating personal tasks.

For more information, check out:
- [What are skills?](https://support.claude.com/en/articles/12512176-what-are-skills)
- [Using skills in Claude](https://support.claude.com/en/articles/12512180-using-skills-in-claude)
- [How to create custom skills](https://support.claude.com/en/articles/12512198-creating-custom-skills)
- [Equipping agents for the real world with Agent Skills](https://anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills)

# About This Repository

This repository contains skills that demonstrate what&#039;s possible with Claude&#039;s skills system. These skills range from creative applications (art, music, design) to technical tasks (testing web apps, MCP server generation) to enterprise workflows (communications, branding, etc.).

Each skill is self-contained in its own folder with a `SKILL.md` file containing the instructions and metadata that Claude uses. Browse through these skills to get inspiration for your own skills or to understand different patterns and approaches.

Many skills in this repo are open source (Apache 2.0). We&#039;ve also included the document creation &amp; editing skills that power [Claude&#039;s document capabilities](https://www.anthropic.com/news/create-files) under the hood in the [`skills/docx`](./skills/docx), [`skills/pdf`](./skills/pdf), [`skills/pptx`](./skills/pptx), and [`skills/xlsx`](./skills/xlsx) subfolders. These are source-available, not open source, but we wanted to share these with developers as a reference for more complex skills that are actively used in a production AI application.

## Disclaimer

**These skills are provided for demonstration and educational purposes only.** While some of these capabilities may be available in Claude, the implementations and behaviors you receive from Claude may differ from what is shown in these skills. These skills are meant to illustrate patterns and possibilities. Always test skills thoroughly in your own environment before relying on them for critical tasks.

# Skill Sets
- [./skills](./skills): Skill examples for Creative &amp; Design, Development &amp; Technical, Enterprise &amp; Communication, and Document Skills
- [./spec](./spec): The Agent Skills specification
- [./template](./template): Skill template

# Try in Claude Code, Claude.ai, and the API

## Claude Code
You can register this repository as a Claude Code Plugin marketplace by running the following command in Claude Code:
```
/plugin marketplace add anthropics/skills
```

Then, to install a specific set of skills:
1. Select `Browse and install plugins`
2. Select `anthropic-agent-skills`
3. Select `document-skills` or `example-skills`
4. Select `Install now`

Alternatively, directly install either Plugin via:
```
/plugin install document-skills@anthropic-agent-skills
/plugin install example-skills@anthropic-agent-skills
```

After installing the plugin, you can use the skill by just mentioning it. For instance, if you install the `document-skills` plugin from the marketplace, you can ask Claude Code to do something like: &quot;Use the PDF skill to extract the form fields from `path/to/some-file.pdf`&quot;

## Claude.ai

These example skills are all already available to paid plans in Claude.ai. 

To use any skill from this repository or upload custom skills, follow the instructions in [Using skills in Claude](https://support.claude.com/en/articles/12512180-using-skills-in-claude#h_a4222fa77b).

## Claude API

You can use Anthropic&#039;s pre-built skills, and upload custom skills, via the Claude API. See the [Skills API Quickstart](https://docs.claude.com/en/api/skills-guide#creating-a-skill) for more.

# Creating a Basic Skill

Skills are simple to create - just a folder with a `SKILL.md` file containing YAML frontmatter and instructions. You can use the **template-skill** in this repository as a starting point:

```markdown
---
name: my-skill-name
description: A clear description of what this skill does and when to use it
---

# My Skill Name

[Add your instructions here that Claude will follow when this skill is active]

## Examples
- Example usage 1
- Example usage 2

## Guidelines
- Guideline 1
- Guideline 2
```

The frontmatter requires only two fields:
- `name` - A unique identifier for your skill (lowercase, hyphens for spaces)
- `description` - A complete description of what the skill does and when to use it

The markdown content below contains the instructions, examples, and guidelines that Claude will follow. For more details, see [How to create custom skills](https://support.claude.com/en/articles/12512198-creating-custom-skills).

# Partner Skills

Skills are a great way to teach Claude how to get better at using specific pieces of software. As we see awesome example skills from partners, we may highlight some of them here:

- **Notion** - [Notion Skills for Claude](https://www.notion.so/notiondevs/Notion-Skills-for-Claude-28da4445d27180c7af1df7d8615723d0)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[neuphonic/neutts]]></title>
            <link>https://github.com/neuphonic/neutts</link>
            <guid>https://github.com/neuphonic/neutts</guid>
            <pubDate>Sun, 18 Jan 2026 00:05:32 GMT</pubDate>
            <description><![CDATA[On-device TTS model by Neuphonic]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/neuphonic/neutts">neuphonic/neutts</a></h1>
            <p>On-device TTS model by Neuphonic</p>
            <p>Language: Python</p>
            <p>Stars: 4,541</p>
            <p>Forks: 483</p>
            <p>Stars today: 43 stars today</p>
            <h2>README</h2><pre># NeuTTS

HuggingFace ğŸ¤—:

- NeuTTS-Air: [Model](https://huggingface.co/neuphonic/neutts-air), [Q8 GGUF](https://huggingface.co/neuphonic/neutts-air-q8-gguf), [Q4 GGUF](https://huggingface.co/neuphonic/neutts-air-q4-gguf), [Spaces](https://huggingface.co/spaces/neuphonic/neutts-air)
- NeuTTS-Nano: [Model](https://huggingface.co/neuphonic/neutts-nano), [Q8 GGUF](https://huggingface.co/neuphonic/neutts-nano-q8-gguf), [Q4 GGUF](https://huggingface.co/neuphonic/neutts-nano-q4-gguf), [Spaces](https://huggingface.co/spaces/neuphonic/neutts-nano)


[NeuTTS-Nano Demo Video](https://github.com/user-attachments/assets/629ec5b2-4818-4fa6-987a-99fcbadc56bc)

_Created by [Neuphonic](http://neuphonic.com/) - building faster, smaller, on-device voice AI_

State-of-the-art Voice AI has been locked behind web APIs for too long. NeuTTS is a collection of open source, on-device, TTS speech language models with instant voice cloning. Built off of LLM backbones, NeuTTS brings natural-sounding speech, real-time performance, built-in security and speaker cloning to your local device - unlocking a new category of embedded voice agents, assistants, toys, and compliance-safe apps.

## Key Features

- ğŸ—£Best-in-class realism for their size - produce natural, ultra-realistic voices that sound human, at the sweet spot between speed, size, and quality for real-world applications
- ğŸ“±Optimised for on-device deployment - provided in GGML format, ready to run on phones, laptops, or even Raspberry Pis
- ğŸ‘«Instant voice cloning - create your own speaker with as little as 3 seconds of audio
- ğŸš„Simple LM + codec architecture - making development and deployment simple

&gt; [!CAUTION]
&gt; Websites like neutts.com are popping up and they&#039;re not affliated with Neuphonic, our github or this repo.
&gt;
&gt; We are on neuphonic.com only. Please be careful out there! ğŸ™

## Model Details



NeuTTS models are built from small LLM backbones - lightweight yet capable language models optimised for text understanding and generation - as well as a powerful combination of technologies designed for efficiency and quality:

- **Supported Languages**: English
- **Audio Codec**: [NeuCodec](https://huggingface.co/neuphonic/neucodec) - our 50hz neural audio codec that achieves exceptional audio quality at low bitrates using a single codebook
- **Context Window**: 2048 tokens, enough for processing ~30 seconds of audio (including prompt duration)
- **Format**: Available in GGML format for efficient on-device inference
- **Responsibility**: Watermarked outputs
- **Inference Speed**: Real-time generation on mid-range devices
- **Power Consumption**: Optimised for mobile and embedded devices


|  | NeuTTSAir | NeuTTSNano |
|---|---:|---:|
| **# Params (Active)** | ~360m | ~120m |
| **# Params (Emb + Active)** | ~552m | ~229m |
| **Cloning** | Yes | Yes |
| **License** | Apache 2.0 | NeuTTS Open License 1.0 |

## Throughput Benchmarking

The two models were benchmarked using the Q4 quantisations [neutts-air-Q4-0](https://huggingface.co/neuphonic/neutts-air-q4-gguf) and [neutts-nano-Q4-0](https://huggingface.co/neuphonic/neutts-nano-q4-gguf).
Benchmarks on CPU were run through llama-bench (llama.cpp) to measure prefill and decode throughput at multiple context sizes.

For GPU&#039;s (specifically RTX 4090), we leverage vLLM to maximise throughput. We run benchmarks using the [vLLM benchmark](https://docs.vllm.ai/en/stable/cli/bench/throughput/).

We include benchmarks on four devices: Galaxy A25 5G, AMD Ryzen 9HX 370, iMac M4 16GB, NVIDIA GeForce RTX 4090.


|  | NeuTTSAir | NeuTTSNano |
|---|---:|---:|
| **Galaxy A25 5G (CPU only)** | 20 tokens/s | 45 tokens/s|
| **AMD Ryzen 9 HX 370 (CPU only)** | 119 tokens/s | 221 tokens/s |
| **iMAc M4 16 GB (CPU only)** | 111 tokens/s | 195 tokens/s |
| **RTX 4090** | 16194 tokens/s | 19268 tokens/s |


&gt; [!NOTE]
&gt;  llama-bench used 14 threads for prefill and 16 threads for decode (as configured in the benchmark run) on AMD Ryzen 9HX 370 and iMac M4 16GB, and 6 threads for each on the Galaxy A25 5G. The tokens/s reported are when having 500 prefill tokens and generating 250 output tokens.

&gt; [!NOTE]
&gt; Please note that these benchmarks only include the Speech Language Model and do not include the Codec which is needed for a full audio generation pipeline.

## Get Started with NeuTTS

&gt; [!NOTE]
&gt; We have added a [streaming example](examples/basic_streaming_example.py) using the `llama-cpp-python` library as well as a [finetuning script](examples/finetune.py). For finetuning, please refer to the [finetune guide](TRAINING.md) for more details.

1. **Clone Git Repo**

   ```bash
   git clone https://github.com/neuphonic/neutts.git
   cd neutts
   ```

2. **Install `espeak` (required dependency)**

   Please refer to the following link for instructions on how to install `espeak`:

   https://github.com/espeak-ng/espeak-ng/blob/master/docs/guide.md

   ```bash
   # Mac OS
   brew install espeak-ng

   # Ubuntu/Debian
   sudo apt install espeak-ng

   # Windows install
   # via chocolatey (https://community.chocolatey.org/packages?page=1&amp;prerelease=False&amp;moderatorQueue=False&amp;tags=espeak)
   choco install espeak-ng
   # via wingit
   winget install -e --id eSpeak-NG.eSpeak-NG
   # via msi (need to add to path or folow the &quot;Windows users who installed via msi&quot; below)
   # find the msi at https://github.com/espeak-ng/espeak-ng/releases
   ```

   Windows users who installed via msi / do not have their install on path need to run the following (see https://github.com/bootphon/phonemizer/issues/163)
   ```pwsh
   $env:PHONEMIZER_ESPEAK_LIBRARY = &quot;c:\Program Files\eSpeak NG\libespeak-ng.dll&quot;
   $env:PHONEMIZER_ESPEAK_PATH = &quot;c:\Program Files\eSpeak NG&quot;
   setx PHONEMIZER_ESPEAK_LIBRARY &quot;c:\Program Files\eSpeak NG\libespeak-ng.dll&quot;
   setx PHONEMIZER_ESPEAK_PATH &quot;c:\Program Files\eSpeak NG&quot;
   ```

3. **Install Python dependencies**

   The requirements file includes the dependencies needed to run the model with PyTorch.
   When using an ONNX decoder or a GGML model, some dependencies (such as PyTorch) are no longer required.

   ```bash
   pip install -r requirements.txt
   ```
   &gt; [!CAUTION]
   &gt; The inference is compatible and tested on `python&quot;&gt;=3.11, &lt;=3.13&quot;`. This is restricted due to pytorch compatibility. [PyTorch Compatibility Matrix](https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-compatibility-matrix)

4. **(Optional) Install Llama-cpp-python to use the `GGUF` models.**

   ```bash
   pip install llama-cpp-python
   ```

   To run llama-cpp with GPU suport (CUDA, MPS) support please refer to:
   https://pypi.org/project/llama-cpp-python/

5. **(Optional) Install onnxruntime to use the `.onnx` decoder.**
   If you want to run the onnxdecoder
   ```bash
   pip install onnxruntime
   ```

## Running the Model

Run the basic example script to synthesize speech:

```bash
python -m examples.basic_example \
  --input_text &quot;My name is Andy. I&#039;m 25 and I just moved to London. The underground is pretty confusing, but it gets me around in no time at all.&quot; \
  --ref_audio samples/jo.wav \
  --ref_text samples/jo.txt
```

To specify a particular model repo for the backbone or codec, add the `--backbone` argument. Available backbones are listed in [NeuTTS-Air](https://huggingface.co/collections/neuphonic/neutts-air) and [NeuTTS-Nano](https://huggingface.co/collections/neuphonic/neutts-nano) huggingface collections.

Several examples are available, including a Jupyter notebook in the `examples` folder.

### One-Code Block Usage

```python
from neutts import NeuTTS
import soundfile as sf

tts = NeuTTS(
   backbone_repo=&quot;neuphonic/neutts-nano&quot;, # or &#039;neutts-nano-q4-gguf&#039; with llama-cpp-python installed
   backbone_device=&quot;cpu&quot;,
   codec_repo=&quot;neuphonic/neucodec&quot;,
   codec_device=&quot;cpu&quot;
)
input_text = &quot;My name is Andy. I&#039;m 25 and I just moved to London. The underground is pretty confusing, but it gets me around in no time at all.&quot;

ref_text = &quot;samples/jo.txt&quot;
ref_audio_path = &quot;samples/jo.wav&quot;

ref_text = open(ref_text, &quot;r&quot;).read().strip()
ref_codes = tts.encode_reference(ref_audio_path)

wav = tts.infer(input_text, ref_codes, ref_text)
sf.write(&quot;test.wav&quot;, wav, 24000)
```

### Streaming

Speech can also be synthesised in _streaming mode_, where audio is generated in chunks and plays as generated. Note that this requires pyaudio to be installed. To do this, run:

```bash
python -m examples.basic_streaming_example \
  --input_text &quot;My name is Andy. I&#039;m 25 and I just moved to London. The underground is pretty confusing, but it gets me around in no time at all.&quot; \
  --ref_codes samples/jo.pt \
  --ref_text samples/jo.txt
```

Again, a particular model repo can be specified with the `--backbone` argument - note that for streaming the model must be in GGUF format.

## Preparing References for Cloning

NeuTTS requires two inputs:

1. A reference audio sample (`.wav` file)
2. A text string

The model then synthesises the text as speech in the style of the reference audio. This is what enables NeuTTS models instant voice cloning capability.

### Example Reference Files

You can find some ready-to-use samples in the `examples` folder:

- `samples/dave.wav`
- `samples/jo.wav`

### Guidelines for Best Results

For optimal performance, reference audio samples should be:

1. **Mono channel**
2. **16-44 kHz sample rate**
3. **3â€“15 seconds in length**
4. **Saved as a `.wav` file**
5. **Clean** â€” minimal to no background noise
6. **Natural, continuous speech** â€” like a monologue or conversation, with few pauses, so the model can capture tone effectively

## Guidelines for minimizing Latency

For optimal performance on-device:

1. Use the GGUF model backbones
2. Pre-encode references
3. Use the [onnx codec decoder](https://huggingface.co/neuphonic/neucodec-onnx-decoder)

Take a look at this example [examples README](examples/README.md###minimal-latency-example) to get started.

## Responsibility

Every audio file generated by NeuTTS includes [Perth (Perceptual Threshold) Watermarker](https://github.com/resemble-ai/perth).

## Disclaimer

Don&#039;t use this model to do bad thingsâ€¦ please.

## Developer Requirements

To run the pre commit hooks to contribute to this project run:

```bash
pip install pre-commit
```

Then:

```bash
pre-commit install
```

## Running Tests

First, install the dev requirements:

```
pip install -r requirements-dev.txt
```

To run the tests:

```
pytest tests/
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ultralytics/ultralytics]]></title>
            <link>https://github.com/ultralytics/ultralytics</link>
            <guid>https://github.com/ultralytics/ultralytics</guid>
            <pubDate>Sun, 18 Jan 2026 00:05:31 GMT</pubDate>
            <description><![CDATA[Ultralytics YOLO ğŸš€]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ultralytics/ultralytics">ultralytics/ultralytics</a></h1>
            <p>Ultralytics YOLO ğŸš€</p>
            <p>Language: Python</p>
            <p>Stars: 51,894</p>
            <p>Forks: 9,930</p>
            <p>Stars today: 227 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
    &lt;a href=&quot;https://platform.ultralytics.com/ultralytics/yolo26&quot; target=&quot;_blank&quot;&gt;
      &lt;img width=&quot;100%&quot; src=&quot;https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png&quot; alt=&quot;Ultralytics YOLO banner&quot;&gt;&lt;/a&gt;
  &lt;/p&gt;

[ä¸­æ–‡](https://docs.ultralytics.com/zh/) | [í•œêµ­ì–´](https://docs.ultralytics.com/ko/) | [æ—¥æœ¬èª](https://docs.ultralytics.com/ja/) | [Ğ ÑƒÑÑĞºĞ¸Ğ¹](https://docs.ultralytics.com/ru/) | [Deutsch](https://docs.ultralytics.com/de/) | [FranÃ§ais](https://docs.ultralytics.com/fr/) | [EspaÃ±ol](https://docs.ultralytics.com/es) | [PortuguÃªs](https://docs.ultralytics.com/pt/) | [TÃ¼rkÃ§e](https://docs.ultralytics.com/tr/) | [Tiáº¿ng Viá»‡t](https://docs.ultralytics.com/vi/) | [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](https://docs.ultralytics.com/ar/) &lt;br&gt;

&lt;div&gt;
    &lt;a href=&quot;https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg&quot; alt=&quot;Ultralytics CI&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://clickpy.clickhouse.com/dashboard/ultralytics&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/ultralytics&quot; alt=&quot;Ultralytics Downloads&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://discord.com/invite/ultralytics&quot;&gt;&lt;img alt=&quot;Ultralytics Discord&quot; src=&quot;https://img.shields.io/discord/1089800235347353640?logo=discord&amp;logoColor=white&amp;label=Discord&amp;color=blue&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://community.ultralytics.com/&quot;&gt;&lt;img alt=&quot;Ultralytics Forums&quot; src=&quot;https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&amp;logo=discourse&amp;label=Forums&amp;color=blue&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://www.reddit.com/r/ultralytics/&quot;&gt;&lt;img alt=&quot;Ultralytics Reddit&quot; src=&quot;https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&amp;logo=reddit&amp;logoColor=white&amp;label=Reddit&amp;color=blue&quot;&gt;&lt;/a&gt;
    &lt;br&gt;
    &lt;a href=&quot;https://console.paperspace.com/github/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://assets.paperspace.io/img/gradient-badge.svg&quot; alt=&quot;Run Ultralytics on Gradient&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open Ultralytics In Colab&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://www.kaggle.com/models/ultralytics/yolo26&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg&quot; alt=&quot;Open Ultralytics In Kaggle&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://mybinder.org/v2/gh/ultralytics/ultralytics/HEAD?labpath=examples%2Ftutorial.ipynb&quot;&gt;&lt;img src=&quot;https://mybinder.org/badge_logo.svg&quot; alt=&quot;Open Ultralytics In Binder&quot;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;br&gt;

[Ultralytics](https://www.ultralytics.com/) creates cutting-edge, state-of-the-art (SOTA) [YOLO models](https://www.ultralytics.com/yolo) built on years of foundational research in computer vision and AI. Constantly updated for performance and flexibility, our models are **fast**, **accurate**, and **easy to use**. They excel at [object detection](https://docs.ultralytics.com/tasks/detect/), [tracking](https://docs.ultralytics.com/modes/track/), [instance segmentation](https://docs.ultralytics.com/tasks/segment/), [image classification](https://docs.ultralytics.com/tasks/classify/), and [pose estimation](https://docs.ultralytics.com/tasks/pose/) tasks.

Find detailed documentation in the [Ultralytics Docs](https://docs.ultralytics.com/). Get support via [GitHub Issues](https://github.com/ultralytics/ultralytics/issues/new/choose). Join discussions on [Discord](https://discord.com/invite/ultralytics), [Reddit](https://www.reddit.com/r/ultralytics/), and the [Ultralytics Community Forums](https://community.ultralytics.com/)!

Request an Enterprise License for commercial use at [Ultralytics Licensing](https://www.ultralytics.com/license).

&lt;a href=&quot;https://platform.ultralytics.com/ultralytics/yolo26&quot; target=&quot;_blank&quot;&gt;
  &lt;img width=&quot;100%&quot; src=&quot;https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/yolo/performance-comparison.png&quot; alt=&quot;YOLO26 performance plots&quot;&gt;
&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/ultralytics&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics GitHub&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;2%&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://www.linkedin.com/company/ultralytics/&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics LinkedIn&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;2%&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://twitter.com/ultralytics&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics Twitter&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;2%&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/ultralytics?sub_confirmation=1&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics YouTube&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;2%&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://www.tiktok.com/@ultralytics&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics TikTok&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;2%&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://ultralytics.com/bilibili&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics BiliBili&quot;&gt;&lt;/a&gt;
  &lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png&quot; width=&quot;2%&quot; alt=&quot;space&quot;&gt;
  &lt;a href=&quot;https://discord.com/invite/ultralytics&quot;&gt;&lt;img src=&quot;https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png&quot; width=&quot;2%&quot; alt=&quot;Ultralytics Discord&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

## ğŸ“„ Documentation

See below for quickstart installation and usage examples. For comprehensive guidance on training, validation, prediction, and deployment, refer to our full [Ultralytics Docs](https://docs.ultralytics.com/).

&lt;details open&gt;
&lt;summary&gt;Install&lt;/summary&gt;

Install the `ultralytics` package, including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml), in a [**Python&gt;=3.8**](https://www.python.org/) environment with [**PyTorch&gt;=1.8**](https://pytorch.org/get-started/locally/).

[![PyPI - Version](https://img.shields.io/pypi/v/ultralytics?logo=pypi&amp;logoColor=white)](https://pypi.org/project/ultralytics/) [![Ultralytics Downloads](https://static.pepy.tech/badge/ultralytics)](https://clickpy.clickhouse.com/dashboard/ultralytics) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ultralytics?logo=python&amp;logoColor=gold)](https://pypi.org/project/ultralytics/)

```bash
pip install ultralytics
```

For alternative installation methods, including [Conda](https://anaconda.org/conda-forge/ultralytics), [Docker](https://hub.docker.com/r/ultralytics/ultralytics), and building from source via Git, please consult the [Quickstart Guide](https://docs.ultralytics.com/quickstart/).

[![Conda Version](https://img.shields.io/conda/vn/conda-forge/ultralytics?logo=condaforge)](https://anaconda.org/conda-forge/ultralytics) [![Docker Image Version](https://img.shields.io/docker/v/ultralytics/ultralytics?sort=semver&amp;logo=docker)](https://hub.docker.com/r/ultralytics/ultralytics) [![Ultralytics Docker Pulls](https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker)](https://hub.docker.com/r/ultralytics/ultralytics)

&lt;/details&gt;

&lt;details open&gt;
&lt;summary&gt;Usage&lt;/summary&gt;

### CLI

You can use Ultralytics YOLO directly from the Command Line Interface (CLI) with the `yolo` command:

```bash
# Predict using a pretrained YOLO model (e.g., YOLO26n) on an image
yolo predict model=yolo26n.pt source=&#039;https://ultralytics.com/images/bus.jpg&#039;
```

The `yolo` command supports various tasks and modes, accepting additional arguments like `imgsz=640`. Explore the YOLO [CLI Docs](https://docs.ultralytics.com/usage/cli/) for more examples.

### Python

Ultralytics YOLO can also be integrated directly into your Python projects. It accepts the same [configuration arguments](https://docs.ultralytics.com/usage/cfg/) as the CLI:

```python
from ultralytics import YOLO

# Load a pretrained YOLO26n model
model = YOLO(&quot;yolo26n.pt&quot;)

# Train the model on the COCO8 dataset for 100 epochs
train_results = model.train(
    data=&quot;coco8.yaml&quot;,  # Path to dataset configuration file
    epochs=100,  # Number of training epochs
    imgsz=640,  # Image size for training
    device=&quot;cpu&quot;,  # Device to run on (e.g., &#039;cpu&#039;, 0, [0,1,2,3])
)

# Evaluate the model&#039;s performance on the validation set
metrics = model.val()

# Perform object detection on an image
results = model(&quot;path/to/image.jpg&quot;)  # Predict on an image
results[0].show()  # Display results

# Export the model to ONNX format for deployment
path = model.export(format=&quot;onnx&quot;)  # Returns the path to the exported model
```

Discover more examples in the YOLO [Python Docs](https://docs.ultralytics.com/usage/python/).

&lt;/details&gt;

## âœ¨ Models

Ultralytics supports a wide range of YOLO models, from early versions like [YOLOv3](https://docs.ultralytics.com/models/yolov3/) to the latest [YOLO26](https://docs.ultralytics.com/models/yolo26/). The tables below showcase YOLO26 models pretrained on the [COCO](https://docs.ultralytics.com/datasets/detect/coco/) dataset for [Detection](https://docs.ultralytics.com/tasks/detect/), [Segmentation](https://docs.ultralytics.com/tasks/segment/), and [Pose Estimation](https://docs.ultralytics.com/tasks/pose/). Additionally, [Classification](https://docs.ultralytics.com/tasks/classify/) models pretrained on the [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet/) dataset are available. [Tracking](https://docs.ultralytics.com/modes/track/) mode is compatible with all Detection, Segmentation, and Pose models. All [Models](https://docs.ultralytics.com/models/) are automatically downloaded from the latest Ultralytics [release](https://github.com/ultralytics/assets/releases) upon first use.

&lt;a href=&quot;https://docs.ultralytics.com/tasks/&quot; target=&quot;_blank&quot;&gt;
    &lt;img width=&quot;100%&quot; src=&quot;https://github.com/ultralytics/docs/releases/download/0/ultralytics-yolov8-tasks-banner.avif&quot; alt=&quot;Ultralytics YOLO supported tasks&quot;&gt;
&lt;/a&gt;
&lt;br&gt;
&lt;br&gt;

&lt;details open&gt;&lt;summary&gt;Detection (COCO)&lt;/summary&gt;

Explore the [Detection Docs](https://docs.ultralytics.com/tasks/detect/) for usage examples. These models are trained on the [COCO dataset](https://cocodataset.org/), featuring 80 object classes.

| Model                                                                                | size&lt;br&gt;&lt;sup&gt;(pixels)&lt;/sup&gt; | mAP&lt;sup&gt;val&lt;br&gt;50-95&lt;/sup&gt; | mAP&lt;sup&gt;val&lt;br&gt;50-95(e2e)&lt;/sup&gt; | Speed&lt;br&gt;&lt;sup&gt;CPU ONNX&lt;br&gt;(ms)&lt;/sup&gt; | Speed&lt;br&gt;&lt;sup&gt;T4 TensorRT10&lt;br&gt;(ms)&lt;/sup&gt; | params&lt;br&gt;&lt;sup&gt;(M)&lt;/sup&gt; | FLOPs&lt;br&gt;&lt;sup&gt;(B)&lt;/sup&gt; |
| ------------------------------------------------------------------------------------ | --------------------------- | -------------------------- | ------------------------------- | ------------------------------------ | ----------------------------------------- | ------------------------ | ----------------------- |
| [YOLO26n](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n.pt) | 640                         | 40.9                       | 40.1                            | 38.9 Â± 0.7                           | 1.7 Â± 0.0                                 | 2.4                      | 5.4                     |
| [YOLO26s](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26s.pt) | 640                         | 48.6                       | 47.8                            | 87.2 Â± 0.9                           | 2.5 Â± 0.0                                 | 9.5                      | 20.7                    |
| [YOLO26m](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26m.pt) | 640                         | 53.1                       | 52.5                            | 220.0 Â± 1.4                          | 4.7 Â± 0.1                                 | 20.4                     | 68.2                    |
| [YOLO26l](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26l.pt) | 640                         | 55.0                       | 54.4                            | 286.2 Â± 2.0                          | 6.2 Â± 0.2                                 | 24.8                     | 86.4                    |
| [YOLO26x](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26x.pt) | 640                         | 57.5                       | 56.9                            | 525.8 Â± 4.0                          | 11.8 Â± 0.2                                | 55.7                     | 193.9                   |

- **mAP&lt;sup&gt;val&lt;/sup&gt;** values refer to single-model single-scale performance on the [COCO val2017](https://cocodataset.org/) dataset. See [YOLO Performance Metrics](https://docs.ultralytics.com/guides/yolo-performance-metrics/) for details. &lt;br&gt;Reproduce with `yolo val detect data=coco.yaml device=0`
- **Speed** metrics are averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. &lt;br&gt;Reproduce with `yolo val detect data=coco.yaml batch=1 device=0|cpu`

&lt;/details&gt;

&lt;details&gt;&lt;summary&gt;Segmentation (COCO)&lt;/summary&gt;

Refer to the [Segmentation Docs](https://docs.ultralytics.com/tasks/segment/) for usage examples. These models are trained on [COCO-Seg](https://docs.ultralytics.com/datasets/segment/coco/), including 80 classes.

| Model                                                                                        | size&lt;br&gt;&lt;sup&gt;(pixels)&lt;/sup&gt; | mAP&lt;sup&gt;box&lt;br&gt;50-95(e2e)&lt;/sup&gt; | mAP&lt;sup&gt;mask&lt;br&gt;50-95(e2e)&lt;/sup&gt; | Speed&lt;br&gt;&lt;sup&gt;CPU ONNX&lt;br&gt;(ms)&lt;/sup&gt; | Speed&lt;br&gt;&lt;sup&gt;T4 TensorRT10&lt;br&gt;(ms)&lt;/sup&gt; | params&lt;br&gt;&lt;sup&gt;(M)&lt;/sup&gt; | FLOPs&lt;br&gt;&lt;sup&gt;(B)&lt;/sup&gt; |
| -------------------------------------------------------------------------------------------- | --------------------------- | ------------------------------- | -------------------------------- | ------------------------------------ | ----------------------------------------- | ------------------------ | ----------------------- |
| [YOLO26n-seg](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n-seg.pt) | 640                         | 39.6                            | 33.9                             | 53.3 Â± 0.5                           | 2.1 Â± 0.0                                 | 2.7                      | 9.1                     |
| [YOLO26s-seg](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26s-seg.pt) | 640                         | 47.3                            | 40.0                             | 118.4 Â± 0.9                          | 3.3 Â± 0.0                                 | 10.4                     | 34.2                    |
| [YOLO26m-seg](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26m-seg.pt) | 640                         | 52.5                            | 44.1                             | 328.2 Â± 2.4                          | 6.7 Â± 0.1                                 | 23.6                     | 121.5                   |
| [YOLO26l-seg](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26l-seg.pt) | 640                         | 54.4                            | 45.5                             | 387.0 Â± 3.7                          | 8.0 Â± 0.1                                 | 28.0                     | 139.8                   |
| [YOLO26x-seg](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26x-seg.pt) | 640                         | 56.5                            | 47.0                             | 787.0 Â± 6.8                          | 16.4 Â± 0.1                                | 62.8                     | 313.5                   |

- **mAP&lt;sup&gt;val&lt;/sup&gt;** values are for single-model single-scale on the [COCO val2017](https://cocodataset.org/) dataset. See [YOLO Performance Metrics](https://docs.ultralytics.com/guides/yolo-performance-metrics/) for details. &lt;br&gt;Reproduce with `yolo val segment data=coco.yaml device=0`
- **Speed** metrics are averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. &lt;br&gt;Reproduce with `yolo val segment data=coco.yaml batch=1 device=0|cpu`

&lt;/details&gt;

&lt;details&gt;&lt;summary&gt;Classification (ImageNet)&lt;/summary&gt;

Consult the [Classification Docs](https://docs.ultralytics.com/tasks/classify/) for usage examples. These models are trained on [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet/), covering 1000 classes.

| Model                                                                                        | size&lt;br&gt;&lt;sup&gt;(pixels)&lt;/sup&gt; | acc&lt;br&gt;&lt;sup&gt;top1&lt;/sup&gt; | acc&lt;br&gt;&lt;sup&gt;top5&lt;/sup&gt; | Speed&lt;br&gt;&lt;sup&gt;CPU ONNX&lt;br&gt;(ms)&lt;/sup&gt; | Speed&lt;br&gt;&lt;sup&gt;T4 TensorRT10&lt;br&gt;(ms)&lt;/sup&gt; | params&lt;br&gt;&lt;sup&gt;(M)&lt;/sup&gt; | FLOPs&lt;br&gt;&lt;sup&gt;(B) at 224&lt;/sup&gt; |
| -------------------------------------------------------------------------------------------- | --------------------------- | ---------------------- | ---------------------- | ------------------------------------ | ----------------------------------------- | ------------------------ | ------------------------------ |
| [YOLO26n-cls](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n-cls.pt) | 224                         | 71.4                   | 90.1                   | 5.0 Â± 0.3                            | 1.1 Â± 0.0                                 | 2.8                      | 0.5                            |
| [YOLO26s-cls](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26s-cls.pt) | 224                         | 76.0                   | 92.9                   | 7.9 Â± 0.2                            | 1.3 Â± 0.0                                 | 6.7                      | 1.6                            |
| [YOLO26m-cls](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26m-cls.pt) | 224                         | 78.1                   | 94.2                   | 17.2 Â± 0.4                           | 2.0 Â± 0.0                                 | 11.6                     | 4.9                            |
| [YOLO26l-cls](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26l-cls.pt) | 224                         | 79.0                   | 94.6                   | 23.2 Â± 0.3                           | 2.8 Â± 0.0                                 | 14.1                     | 6.2                            |
| [YOLO26x-cls](https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26x-cls.pt) | 224                         | 79.9                   | 95.0                   | 41.4 Â± 0.9                           | 3.8 Â± 0.0                                 | 29.6                     | 13.6                           |

- **acc** values represent model accuracy on the [ImageNet](https://www.image-net.org/) dataset validation set. &lt;br&gt;Reproduce with `yolo val classify data=path/to/ImageNet device=0`
- **Speed** metrics are averaged over ImageNet val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. &lt;br&gt;Reproduce with `yolo val classify data=path/to/ImageNet batch=1 device=0|cpu`

&lt;/details&gt;

&lt;details&gt;&lt;summary&gt;Pose (COCO)&lt;/summary&gt;

See the [Pose Estimation Docs](https://docs.ultralytics.com/tasks

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NVIDIA/physicsnemo]]></title>
            <link>https://github.com/NVIDIA/physicsnemo</link>
            <guid>https://github.com/NVIDIA/physicsnemo</guid>
            <pubDate>Sun, 18 Jan 2026 00:05:30 GMT</pubDate>
            <description><![CDATA[Open-source deep-learning framework for building, training, and fine-tuning deep learning models using state-of-the-art Physics-ML methods]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NVIDIA/physicsnemo">NVIDIA/physicsnemo</a></h1>
            <p>Open-source deep-learning framework for building, training, and fine-tuning deep learning models using state-of-the-art Physics-ML methods</p>
            <p>Language: Python</p>
            <p>Stars: 2,295</p>
            <p>Forks: 552</p>
            <p>Stars today: 4 stars today</p>
            <h2>README</h2><pre># NVIDIA PhysicsNeMo

&lt;!-- markdownlint-disable --&gt;

ğŸ“ NVIDIA PhysicsNeMo is undergoing an update to v2.0 - all the features, with easier installation and integration to external packages.  See the [migration guide](https://github.com/NVIDIA/physicsnemo/blob/main/v2.0-MIGRATION-GUIDE.md) for more details!

[![Project Status: Active - The project has reached a stable, usable state and is being actively developed.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)
[![GitHub](https://img.shields.io/github/license/NVIDIA/physicsnemo)](https://github.com/NVIDIA/physicsnemo/blob/master/LICENSE.txt)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Install CI](https://github.com/NVIDIA/physicsnemo/actions/workflows/install-ci.yml/badge.svg?event=schedule)](https://github.com/NVIDIA/physicsnemo/actions/workflows/install-ci.yml)

&lt;!-- markdownlint-enable --&gt;
[**NVIDIA PhysicsNeMo**](#what-is-physicsnemo)
| [**Documentation**](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/index.html)
| [**Install Guide**](#installation)
| [**Getting Started**](#getting-started-with-physicsnemo)
| [**Contributing Guidelines**](#contributing-to-physicsnemo)
| [**Dev blog**](https://nvidia.github.io/physicsnemo/blog/)

## What is PhysicsNeMo?

NVIDIA PhysicsNeMo is an open-source deep-learning framework for building, training,
fine-tuning, and inferring Physics AI models using state-of-the-art SciML methods for
AI4Science and engineering.

PhysicsNeMo provides Python modules to compose scalable and optimized training and
inference pipelines to explore, develop, validate, and deploy AI models that combine
physics knowledge with data, enabling real-time predictions.

Whether you are exploring the use of neural operators, GNNs, or transformers, or are
interested in Physics-Informed Neural Networks or a hybrid approach in between, PhysicsNeMo
provides you with an optimized stack that will enable you to train your models at scale.

&lt;!-- markdownlint-disable --&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/docs/img/value_prop/Knowledge_guided_models.gif alt=&quot;PhysicsNeMo&quot;/&gt;
&lt;/p&gt;
&lt;!-- markdownlint-enable --&gt;

&lt;!-- toc --&gt;

- [More About PhysicsNeMo](#more-about-physicsnemo)
  - [Scalable GPU-Optimized Training Library](#scalable-gpu-optimized-training-library)
  - [A Suite of Physics-Informed ML Models](#a-suite-of-physics-informed-ml-models)
  - [Seamless PyTorch Integration](#seamless-pytorch-integration)
  - [Easy Customization and Extension](#easy-customization-and-extension)
  - [AI4Science Library](#ai4science-library)
    - [Domain-Specific Packages](#domain-specific-packages)
- [Who is Using and Contributing to PhysicsNeMo](#who-is-using-and-contributing-to-physicsnemo)
- [Why Use PhysicsNeMo](#why-are-they-using-physicsnemo)
- [Getting Started](#getting-started-with-physicsnemo)
- [Resources](#resources)
- [Installation](#installation)
- [Contributing](#contributing-to-physicsnemo)
- [Communication](#communication)
- [License](#license)

&lt;!-- tocstop --&gt;

## More About PhysicsNeMo

At a granular level, PhysicsNeMo is developed as modular functionality and therefore
provides built-in composable modules that are packaged into a few key components:

&lt;!-- markdownlint-disable --&gt;
Component | Description |
---- | --- |
[**physicsnemo.models**](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.models.html) | A collection of optimized, customizable, and easy-to-use families of model architectures such as Neural Operators, Graph Neural Networks, Diffusion models, Transformer models, and many more|
[**physicsnemo.datapipes**](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.datapipes.html) | Optimized and scalable built-in data pipelines fine-tuned to handle engineering and scientific data structures like point clouds, meshes, etc.|
[**physicsnemo.distributed**](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.distributed.html) | A distributed computing sub-module built on top of `torch.distributed` to enable parallel training with just a few steps|
[**physicsnemo.curator**](https://github.com/NVIDIA/physicsnemo-curator) | A sub-module to streamline and accelerate the process of data curation for engineering datasets|
[**physicsnemo.sym.geometry**](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-sym/user_guide/features/csg_and_tessellated_module.html) | A sub-module to handle geometry for DL training using Constructive Solid Geometry modeling and CAD files in STL format|
[**physicsnemo.sym.eq**](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-sym/user_guide/features/nodes.html) | A sub-module to use PDEs in your DL training with several implementations of commonly observed equations and easy ways for customization|
&lt;!-- markdownlint-enable --&gt;

For a complete list, refer to the PhysicsNeMo API documentation for
[PhysicsNeMo](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/index.html).

## AI4Science Library

Usually, PhysicsNeMo is used either as:

- A complementary tool to PyTorch when exploring AI for SciML and AI4Science applications.
- A deep learning research platform that provides scale and optimal performance on
NVIDIA GPUs.

### Domain-Specific Packages

The following are packages dedicated to domain experts of specific communities, catering
to their unique exploration needs:

- [PhysicsNeMo CFD](https://github.com/NVIDIA/physicsnemo-cfd): Inference sub-module of PhysicsNeMo
  to enable CFD domain experts to explore, experiment, and validate using pretrained
  AI models for CFD use cases.
- [PhysicsNeMo Curator](https://github.com/NVIDIA/physicsnemo-curator): Inference sub-module
  of PhysicsNeMo to streamline and accelerate the process of data curation for engineering
  datasets.
- [Earth-2 Studio](https://github.com/NVIDIA/earth2studio): Inference sub-module of PhysicsNeMo
  to enable climate researchers and scientists to explore and experiment with pretrained
  AI models for weather and climate.

### Scalable GPU-Optimized Training Library

PhysicsNeMo provides a highly optimized and scalable training library for maximizing the
power of NVIDIA GPUs.
[Distributed computing](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.distributed.html)
utilities allow for efficient scaling from a single GPU to multi-node GPU clusters with
a few lines of code, ensuring that large-scale
physics-informed machine learning (ML) models can be trained quickly and effectively.
The framework includes support for advanced
[optimization utilities](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.utils.html#module-physicsnemo.utils.capture),
[tailor-made datapipes](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.datapipes.html),
and [validation utilities](https://github.com/NVIDIA/physicsnemo-sym/tree/main/physicsnemo/sym/eq)
to enhance end-to-end training speed.

### A Suite of Physics-Informed ML Models

PhysicsNeMo offers a library of state-of-the-art models specifically designed
for Physics-ML applications. Users can build any model architecture by using the underlying
PyTorch layers and combining them with curated PhysicsNeMo layers.

The [Model Zoo](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.models.html#model-zoo)
includes optimized implementations of families of model architectures such as
Neural Operators:

- [Fourier Neural Operators (FNOs)](physicsnemo/models/fno)
- [DeepONet](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-sym/user_guide/neural_operators/deeponet.html)
- [DoMINO](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/examples/cfd/external_aerodynamics/domino/readme.html)
- [Graph Neural Networks (GNNs)](physicsnemo/models/gnn_layers)
- [MeshGraphNet](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/examples/cfd/vortex_shedding_mgn/readme.html)
- [MeshGraphNet for Lagrangian](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/examples/cfd/lagrangian_mgn/readme.html)
- [XAeroNet](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/examples/cfd/external_aerodynamics/xaeronet/readme.html)
- [Diffusion Models](physicsnemo/models/diffusion)
- [Correction Diffusion Model](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/examples/generative/corrdiff/readme.html)
- [DDPM](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/examples/generative/diffusion/readme.html)
- [PhysicsNeMo GraphCast](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/examples/weather/graphcast/readme.html)
- [Transsolver](https://github.com/NVIDIA/physicsnemo/tree/main/examples/cfd/darcy_transolver)
- [RNNs](https://github.com/NVIDIA/physicsnemo/tree/main/physicsnemo/models)
- [SwinVRNN](https://github.com/NVIDIA/physicsnemo/tree/main/physicsnemo/models/swinvrnn)
- [Physics-Informed Neural Networks (PINNs)](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-sym/user_guide/foundational/1d_wave_equation.html)

And many others.

These models are optimized for various physics domains, such as computational fluid
dynamics, structural mechanics, and electromagnetics. Users can download, customize, and
build upon these models to suit their specific needs, significantly reducing the time
required to develop high-fidelity simulations.

### Seamless PyTorch Integration

PhysicsNeMo is built on top of PyTorch, providing a familiar and user-friendly experience
for those already proficient with PyTorch.
This includes a simple Python interface and modular design, making it easy to use
PhysicsNeMo with existing PyTorch workflows.
Users can leverage the extensive PyTorch ecosystem, including its libraries and tools,
while benefiting from PhysicsNeMo&#039;s specialized capabilities for physics-ML. This seamless
integration ensures users can quickly adopt PhysicsNeMo without a steep learning curve.

For more information, refer to [Converting PyTorch Models to PhysicsNeMo Models](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.models.html#converting-pytorch-models-to-physicsnemo-models).

### Easy Customization and Extension

PhysicsNeMo is designed to be highly extensible, allowing users to add new functionality
with minimal effort. The framework provides Pythonic APIs for
defining new physics models, geometries, and constraints, making it easy to extend its
capabilities to new use cases.
The adaptability of PhysicsNeMo is further enhanced by key features such as
[ONNX support](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.deploy.html)
for flexible model deployment,
robust [logging utilities](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.launch.logging.html)
for streamlined error handling,
and efficient
[checkpointing](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.launch.utils.html#module-physicsnemo.launch.utils.checkpoint)
to simplify model loading and saving.

This extensibility ensures that PhysicsNeMo can adapt to the evolving needs of researchers
and engineers, facilitating the development of innovative solutions in the field of physics-ML.

Detailed information on features and capabilities can be found in the [PhysicsNeMo documentation](https://docs.nvidia.com/physicsnemo/index.html#core).

[Reference samples](examples/README.md) cover a broad spectrum of physics-constrained
and data-driven
workflows to suit the diversity of use cases in the science and engineering disciplines.

&gt; [!TIP]
&gt; Have questions about how PhysicsNeMo can assist you? Try our [Experimental] chatbot,
&gt; [PhysicsNeMo Guide](https://chatgpt.com/g/g-PXrBv20SC-modulus-guide), for answers.

### Hello World

You can start using PhysicsNeMo in your PyTorch code as simply as shown here:

```python
&gt;&gt;&gt; import torch
&gt;&gt;&gt; from physicsnemo.models.mlp.fully_connected import FullyConnected
&gt;&gt;&gt; model = FullyConnected(in_features=32, out_features=64)
&gt;&gt;&gt; input = torch.randn(128, 32)
&gt;&gt;&gt; output = model(input)
&gt;&gt;&gt; output.shape
torch.Size([128, 64])
```

To use the distributed module, you can do the following (example for
distributed data parallel training; for a more in-depth tutorial, refer to
[PhysicsNeMo Distributed](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.distributed.html#)):

```python
import torch
from torch.nn.parallel import DistributedDataParallel
from physicsnemo.distributed import DistributedManager
from physicsnemo.models.mlp.fully_connected import FullyConnected

def main():
    DistributedManager.initialize()
    dist = DistributedManager()

    arch = FullyConnected(in_features=32, out_features=64).to(dist.device)

    if dist.distributed:
        ddps = torch.cuda.Stream()
        with torch.cuda.stream(ddps):
            arch = DistributedDataParallel(
                arch,
                device_ids=[dist.local_rank],
                output_device=dist.device,
                broadcast_buffers=dist.broadcast_buffers,
                find_unused_parameters=dist.find_unused_parameters,
            )
        torch.cuda.current_stream().wait_stream(ddps)

    # Set up the optimizer
    optimizer = torch.optim.Adam(
        arch.parameters(),
        lr=0.001,
    )

    def training_step(invar, target):
        pred = arch(invar)
        loss = torch.sum(torch.pow(pred - target, 2))
        loss.backward()
        optimizer.step()
        return loss

    # Sample training loop
    for i in range(20):
        # Random inputs and targets for simplicity
        input = torch.randn(128, 32, device=dist.device)
        target = torch.randn(128, 64, device=dist.device)

        # Training step
        loss = training_step(input, target)

if __name__ == &quot;__main__&quot;:
    main()
```

To use the PDE module, you can do the following:

```python
&gt;&gt;&gt; from physicsnemo.sym.eq.pdes.navier_stokes import NavierStokes
&gt;&gt;&gt; ns = NavierStokes(nu=0.01, rho=1, dim=2)
&gt;&gt;&gt; ns.pprint()
continuity: u__x + v__y
momentum_x: u*u__x + v*u__y + p__x + u__t - 0.01*u__x__x - 0.01*u__y__y
momentum_y: u*v__x + v*v__y + p__y + v__t - 0.01*v__x__x - 0.01*v__y__y
```

## Who is Using and Contributing to PhysicsNeMo

PhysicsNeMo is an open-source project and gets contributions from researchers in
the SciML and AI4Science fields. While the PhysicsNeMo team works on optimizing the
underlying software stack, the community collaborates and contributes model architectures,
datasets, and reference applications so we can innovate in the pursuit of
developing generalizable model architectures and algorithms.

Some recent examples of community contributors are the [HP Labs 3D Printing team](https://developer.nvidia.com/blog/spotlight-hp-3d-printing-and-nvidia-physicsnemo-collaborate-on-open-source-manufacturing-digital-twin/),
[Stanford Cardiovascular research team](https://developer.nvidia.com/blog/enabling-greater-patient-specific-cardiovascular-care-with-ai-surrogates/),
[UIUC team](https://github.com/NVIDIA/physicsnemo/tree/main/examples/cfd/mhd_pino),
[CMU team](https://github.com/NVIDIA/physicsnemo/tree/main/examples/generative/diffusion),
etc.

Recent examples of research teams using PhysicsNeMo are the
[ORNL team](https://arxiv.org/abs/2404.05768),
[TU Munich CFD team](https://www.nvidia.com/en-us/on-demand/session/gtc24-s62237/), etc.

Please navigate to this page for a complete list of research work leveraging PhysicsNeMo.
For a list of enterprises using PhysicsNeMo, refer to the [PhysicsNeMo Webpage](https://developer.nvidia.com/physicsnemo).

Using PhysicsNeMo and interested in showcasing your work on
[NVIDIA Blogs](https://developer.nvidia.com/blog/category/simulation-modeling-design/)?
Fill out this [proposal form](https://forms.gle/XsBdWp3ji67yZAUF7) and we will get back
to you!

## Why Are They Using PhysicsNeMo

Here are some of the key benefits of PhysicsNeMo for SciML model development:

&lt;!-- markdownlint-disable --&gt;
&lt;img src=&quot;docs/img/value_prop/benchmarking.svg&quot; width=&quot;100&quot;&gt; | &lt;img src=&quot;docs/img/value_prop/recipe.svg&quot; width=&quot;100&quot;&gt; | &lt;img src=&quot;docs/img/value_prop/performance.svg&quot; width=&quot;100&quot;&gt;
---|---|---|
|SciML Benchmarking and Validation|Ease of Using Generalized SciML Recipes with Heterogeneous Datasets |Out-of-the-Box Performance and Scalability
|PhysicsNeMo enables researchers to benchmark their AI models against proven architectures for standard benchmark problems with detailed domain-specific validation criteria.|PhysicsNeMo enables researchers to pick from state-of-the-art SciML architectures and use built-in data pipelines for their use case.| PhysicsNeMo provides out-of-the-box performant training pipelines, including optimized ETL pipelines for heterogeneous engineering and scientific datasets and out-of-the-box scaling across multi-GPU and multi-node GPUs.
&lt;!-- markdownlint-enable --&gt;

See what your peer SciML researchers are saying about PhysicsNeMo (coming soon).

## Getting Started with PhysicsNeMo

The following resources will help you learn how to use PhysicsNeMo. The best
way is to start with a reference sample and then update it for your own use case.

- [Using PhysicsNeMo with your PyTorch model](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/tutorials/simple_training_example.html#using-custom-models-in-physicsnemo)
- [Using PhysicsNeMo built-in models](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/tutorials/simple_training_example.html#using-built-in-models)
- [Getting Started Guide](https://docs.nvidia.com/deeplearning/physicsnemo/getting-started/index.html)
- [Reference Samples](https://github.com/NVIDIA/physicsnemo/blob/main/examples/README.md)
- [User Guide Documentation](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/index.html)

## Learning AI Physics

- [Explore Jupyter Notebooks on Hugging Face](https://huggingface.co/collections/nvidia/physicsnemo)
- [AI4Science PhysicsNeMo Bootcamp](https://github.com/openhackathons-org/End-to-End-AI-for-Science)
- [Self-Paced DLI Training](https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+S-OV-04+V1)
- [Deep Learning for Science and Engineering Lecture Series](https://www.nvidia.com/en-us/on-demand/deep-learning-for-science-and-engineering/)
- [Video Tutorials](https://www.nvidia.com/en-us/on-demand/search/?facet.mimetype[]=event%20session&amp;layout=list&amp;page=1&amp;q=physicsnemo&amp;sort=relevance&amp;sortDir=desc)

## Resources

- [Getting Started Webinar](https://www.nvidia.com/en-us/on-demand/session/gtc24-dlit61460/?playlistId=playList-bd07f4dc-1397-4783-a959-65cec79aa985)
- [PhysicsNeMo: Purpose and Usage](https://www.nvidia.com/en-us/on-demand/session/dliteachingkit-setk5002/)
- [AI4Science PhysicsNeMo Bootcamp](https://github.com/openhackathons-org/End-to-End-AI-for-Science)
- [PhysicsNeMo Pretrained Models](https://catalog.ngc.nvidia.com/models?filters=&amp;orderBy=scoreDESC&amp;query=PhysicsNeMo&amp;page=&amp;pageSize=)
- [PhysicsNeMo Datasets and Supplementary Materials](https://catalog.ngc.nvidia.com/resources?filters=&amp;orderBy=scoreDESC&amp;query=PhysicsNeMo&amp;page=&amp;pageSize=)

## Installation

The following instructions help you install the base PhysicsNeMo modules to get
started. In addition to this, optional dependencies can be installed to provide
additional functionality. A complete list of optional dependencies is available
in the [`pyproject.toml`](./pyproject.toml) file.

The [training recipes](./examples) are not packaged into the pip wheels or the
container to keep the footprint low. We recommend users clone the appropriate
training recipes and use them as a starting point. These training recipes may
require additional example-specific dependencies, as indicated through an
assoc

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Wirasm/PRPs-agentic-eng]]></title>
            <link>https://github.com/Wirasm/PRPs-agentic-eng</link>
            <guid>https://github.com/Wirasm/PRPs-agentic-eng</guid>
            <pubDate>Sun, 18 Jan 2026 00:05:29 GMT</pubDate>
            <description><![CDATA[Prompts, workflows and more for agentic engineering]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Wirasm/PRPs-agentic-eng">Wirasm/PRPs-agentic-eng</a></h1>
            <p>Prompts, workflows and more for agentic engineering</p>
            <p>Language: Python</p>
            <p>Stars: 1,906</p>
            <p>Forks: 587</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre>### â˜• Support This Work

**Found value in these resources?**

ğŸ‘‰ **Buy me a coffee:** https://coff.ee/wirasm

I spent a considerable amount of time creating these resources and prompts. If you find value in this project, please consider buying me a coffee to support my work.

That will help me maintain and improve the resources available for free

---

### ğŸ¯ Transform Your Team with AI Engineering Workshops

**Ready to move beyond toy demos to production-ready AI systems?**

ğŸ‘‰ **Book a workshop:** https://www.rasmuswiding.com/

âœ… **What you&#039;ll get:**

- Put your team on a path to become AI power users
- Learn the exact PRP methodology used by top engineering teams
- Hands-on training with Claude Code, PRPs, and real codebases
- From beginner to advanced AI engineering workshops for teams and individuals

ğŸ’¡ **Perfect for:** Engineering teams, Product teams, and developers who want AI that actually works in production

Let&#039;s talk!
Contact me directly at rasmus@widinglabs.com

# AI Engineering Resources for Claude Code

A comprehensive library of assets and context engineering for Agentic Engineering, optimized for Claude Code. This repository provides the Product Requirement Prompt (PRP) methodology, pre-configured commands, and extensive documentation to enable AI-assisted development that delivers production-ready code on the first pass.

## What is PRP?

Product Requirement Prompt (PRP) is a structured prompt methodology first established in summer 2024 with context engineering at heart. A PRP supplies an AI coding agent with everything it needs to deliver a vertical slice of working softwareâ€”no more, no less.

&gt; &quot;Over-specifying what to build while under-specifying the context, and how to build it, is why so many AI-driven coding attempts stall at 80%.&quot;

### How PRP Differs from Traditional PRD

A traditional PRD clarifies what the product must do and why customers need it, but deliberately avoids how it will be built.

A PRP keeps the goal and justification sections of a PRD yet adds three AI-critical layers:

### Context

Precise file paths and content, library versions and library context, code snippets examples. LLMs generate higher-quality code when given direct, in-prompt references instead of broad descriptions. Usage of a ai_docs/ directory to pipe in library and other docs.

### Implementation Details and Strategy

In contrast to a traditional PRD, a PRP explicitly states how the product will be built. This includes the use of API endpoints, test runners, or agent patterns (ReAct, Plan-and-Execute) to use. Usage of typehints, dependencies, architectural patterns and other tools to ensure the code is built correctly.

### Validation Gates

Deterministic checks such as pytest, ruff, or static type passes. &quot;Shift-left&quot; quality controls catch defects early and are cheaper than late re-work. Example: Each new function should be individually tested, Validation gate = all tests pass.

### Why Context is Non-Negotiable

Large-language-model outputs are bounded by their context window; irrelevant or missing context literally squeezes out useful tokens.

The industry mantra &quot;Garbage In â†’ Garbage Out&quot; applies doubly to prompt engineering and especially in agentic engineering: sloppy input yields brittle code.

### In Short

A PRP is **PRD + curated codebase intelligence + agent/runbook**â€”the minimum viable packet an AI needs to ship production-ready code on the first pass.

The PRP can be small and focusing on a single task or large and covering multiple tasks. The true power of PRP is in the ability to chain tasks together in a PRP to build, self-validate and ship complex features.

## Getting Started

### Option 1: Copy Resources to Your Existing Project

1. **Copy the Claude commands** to your project:

   ```bash
   # From your project root
   cp -r /path/to/PRPs-agentic-eng/.claude/commands .claude/
   ```

2. **Copy the PRP templates and runner**:

   ```bash
   cp -r /path/to/PRPs-agentic-eng/PRPs/templates PRPs/
   cp -r /path/to/PRPs-agentic-eng/PRPs/scripts PRPs/
   cp /path/to/PRPs-agentic-eng/PRPs/README.md PRPs/
   ```

3. **Copy AI documentation** (optional but recommended):
   ```bash
   cp -r /path/to/PRPs-agentic-eng/PRPs/ai_docs PRPs/
   ```

### Option 2: Clone and Start a New Project

1. **Clone this repository**:

   ```bash
   git clone https://github.com/Wirasm/PRPs-agentic-eng.git
   cd PRPs-agentic-eng
   ```

2. **Create your project structure**:

   ```bash
   # Example for a Python project
   mkdir -p src/tests
   touch src/__init__.py
   touch pyproject.toml
   touch CLAUDE.md
   ```

3. **Initialize with UV** (for Python projects):
   ```bash
   uv venv
   uv sync
   ```

## Using Claude Commands

The `.claude/commands/` directory contains 12 pre-configured commands that appear as slash commands in Claude Code.

### Available Commands

1. **PRP Creation &amp; Execution**:
   - `/create-base-prp` - Generate comprehensive PRPs with research
   - `/execute-base-prp` - Execute PRPs against codebase
   - `/planning-create` - Create planning documents with diagrams
   - `/spec-create-adv` - Advanced specification creation
   - `/spec-execute` - Execute specifications

2. **Code Review &amp; Refactoring**:
   - `/review-general` - General code review
   - `/review-staged-unstaged` - Review git changes
   - `/refactor-simple` - Simple refactoring tasks

3. **Git &amp; GitHub**:
   - `/create-pr` - Create pull requests

4. **Utilities**:
   - `/prime-core` - Prime Claude with project context
   - `/onboarding` - Onboarding process for new team members
   - `/debug` - Debugging workflow

### How to Use Commands

1. **In Claude Code**, type `/` to see available commands
2. **Select a command** and provide arguments when prompted
3. **Example usage**:
   ```
   /create-base-prp user authentication system with OAuth2
   ```

## Using PRPs

### Creating a PRP

1. **Use the template** as a starting point:

   ```bash
   cp PRPs/templates/prp_base.md PRPs/my-feature.md
   ```

2. **Fill in the sections**:
   - Goal: What needs to be built
   - Why: Business value and user impact
   - Context: Documentation, code examples, gotchas
   - Implementation Blueprint: Tasks and pseudocode
   - Validation Loop: Executable tests

3. **Or use Claude to generate one**:
   ```
   /create-base-prp implement user authentication with JWT tokens
   ```

### Executing a PRP

1. **Using the runner script**:

   ```bash
   # Interactive mode (recommended for development)
   uv run PRPs/scripts/prp_runner.py --prp my-feature --interactive

   # Headless mode (for CI/CD)
   uv run PRPs/scripts/prp_runner.py --prp my-feature --output-format json

   # Streaming JSON (for real-time monitoring)
   uv run PRPs/scripts/prp_runner.py --prp my-feature --output-format stream-json
   ```

2. **Using Claude commands**:
   ```
   /execute-base-prp PRPs/my-feature.md
   ```

### PRP Best Practices

1. **Context is King**: Include ALL necessary documentation, examples, and caveats
2. **Validation Loops**: Provide executable tests/lints the AI can run and fix
3. **Information Dense**: Use keywords and patterns from the codebase
4. **Progressive Success**: Start simple, validate, then enhance

### Example PRP Structure

```markdown
## Goal

Implement user authentication with JWT tokens

## Why

- Enable secure user sessions
- Support API authentication
- Replace basic auth with industry standard

## What

JWT-based authentication system with login, logout, and token refresh

### Success Criteria

- [ ] Users can login with email/password
- [ ] JWT tokens expire after 24 hours
- [ ] Refresh tokens work correctly
- [ ] All endpoints properly secured

## All Needed Context

### Documentation &amp; References

- url: https://jwt.io/introduction/
  why: JWT structure and best practices

- file: src/auth/basic_auth.py
  why: Current auth pattern to replace

- doc: https://fastapi.tiangolo.com/tutorial/security/oauth2-jwt/
  section: OAuth2 with Password and JWT

### Known Gotchas

# CRITICAL: Use RS256 algorithm for production

# CRITICAL: Store refresh tokens in httpOnly cookies

# CRITICAL: Implement token blacklist for logout

## Implementation Blueprint

[... detailed implementation plan ...]

## Validation Loop

### Level 1: Syntax &amp; Style

ruff check src/ --fix
mypy src/

### Level 2: Unit Tests

uv run pytest tests/test_auth.py -v

### Level 3: Integration Test

curl -X POST http://localhost:8000/auth/login \
 -H &quot;Content-Type: application/json&quot; \
 -d &#039;{&quot;email&quot;: &quot;test@example.com&quot;, &quot;password&quot;: &quot;testpass&quot;}&#039;
```

## Project Structure Recommendations

```
your-project/
|-- .claude/
|   |-- commands/          # Claude Code commands
|   `-- settings.json      # Tool permissions
|-- PRPs/
|   |-- templates/         # PRP templates
|   |-- scrips/           # PRP runner
|   |-- ai_docs/          # Library documentation
|   |-- completed/        # Finished PRPs
|   `-- *.md              # Active PRPs
|-- CLAUDE.md             # Project-specific guidelines
|-- src/                  # Your source code
`-- tests/                # Your tests
```

## Setting Up CLAUDE.md

Create a `CLAUDE.md` file in your project root with:

1. **Core Principles**: KISS, YAGNI, etc.
2. **Code Structure**: File size limits, function length
3. **Architecture**: How your project is organized
4. **Testing**: Test patterns and requirements
5. **Style Conventions**: Language-specific guidelines
6. **Development Commands**: How to run tests, lint, etc.

See the example CLAUDE.md in this repository for a comprehensive template.

## Advanced Usage

### Running Multiple Claude Sessions

Use Git worktrees for parallel development:

```bash
git worktree add -b feature-auth ../project-auth
git worktree add -b feature-api ../project-api

# Run Claude in each worktree
cd ../project-auth &amp;&amp; claude
cd ../project-api &amp;&amp; claude
```

### CI/CD Integration

Use the PRP runner in headless mode:

```yaml
# GitHub Actions example
- name: Execute PRP
  run: |
    uv run PRPs/scripts/prp_runner.py \
      --prp implement-feature \
      --output-format json &gt; result.json
```

### Custom Commands

Create your own commands in `.claude/commands/`:

```markdown
# .claude/commands/my-command.md

# My Custom Command

Do something specific to my project.

## Arguments: $ARGUMENTS

[Your command implementation]
```

## Resources Included

### Documentation (PRPs/ai_docs/)

- `cc_base.md` - Core Claude Code documentation
- `cc_actions_sdk.md` - GitHub Actions and SDK integration
- `cc_best_practices.md` - Best practices guide
- `cc_settings.md` - Configuration and security
- `cc_tutorials.md` - Step-by-step tutorials

### Templates (PRPs/templates/)

- `prp_base.md` - Comprehensive PRP template with validation
- `prp_spec.md` - Specification template
- `prp_planning_base.md` - Planning template with diagrams

### Example PRP

- `example-from-workshop-mcp-crawl4ai-refactor-1.md` - Real-world refactoring example

## License

MIT License

## Support

I spent a considerable amount of time creating these resources and prompts. If you find value in this project, please consider buying me a coffee to support my work.

ğŸ‘‰ **Buy me a coffee:** https://coff.ee/wirasm

---

Remember: The goal is one-pass implementation success through comprehensive context. Happy coding with Claude Code!
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[wagtail/wagtail]]></title>
            <link>https://github.com/wagtail/wagtail</link>
            <guid>https://github.com/wagtail/wagtail</guid>
            <pubDate>Sun, 18 Jan 2026 00:05:28 GMT</pubDate>
            <description><![CDATA[A Django content management system focused on flexibility and user experience]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/wagtail/wagtail">wagtail/wagtail</a></h1>
            <p>A Django content management system focused on flexibility and user experience</p>
            <p>Language: Python</p>
            <p>Stars: 20,045</p>
            <p>Forks: 4,369</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
    &lt;picture&gt;
        &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;.github/wagtail.svg&quot;&gt;
        &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;.github/wagtail-inverse.svg&quot;&gt;
        &lt;img width=&quot;343&quot; src=&quot;.github/wagtail.svg&quot; alt=&quot;Wagtail&quot;&gt;
    &lt;/picture&gt;
&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;br&gt;
    &lt;a href=&quot;https://github.com/wagtail/wagtail/actions&quot;&gt;
        &lt;img src=&quot;https://github.com/wagtail/wagtail/workflows/Wagtail%20CI/badge.svg&quot; alt=&quot;Build Status&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://opensource.org/licenses/BSD-3-Clause&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/license-BSD-blue.svg&quot; alt=&quot;License&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://pypi.python.org/pypi/wagtail/&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/pypi/v/wagtail.svg&quot; alt=&quot;Version&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://pypi.python.org/pypi/wagtail/&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/pypi/dm/wagtail?logo=Downloads&quot; alt=&quot;Monthly downloads&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://fosstodon.org/@wagtail&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/mastodon/follow/109308882653647818?domain=https%3A%2F%2Ffosstodon.org&amp;style=social&quot; alt=&quot;Follow @wagtail@fosstodon.org&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

Wagtail is an open source content management system built on Django, with a strong community and commercial support. It&#039;s focused on user experience, and offers precise control for designers and developers.

![Wagtail screenshot](https://cdn.jsdelivr.net/gh/wagtail/wagtail@main/.github/wagtail-screenshot-with-browser.png)

### ğŸ”¥ Features

-   A fast, attractive interface for authors
-   Complete control over front-end design and structure
-   Scales to millions of pages and thousands of editors
-   Fast out of the box, cache-friendly when you need it
-   Content API for &#039;headless&#039; sites with decoupled front-end
-   Runs on a Raspberry Pi or a multi-datacenter cloud platform
-   StreamField encourages flexible content without compromising structure
-   Powerful, integrated search, using Elasticsearch or PostgreSQL
-   Excellent support for images and embedded content
-   Multi-site and multi-language ready
-   Embraces and extends Django

Find out more at [wagtail.org](https://wagtail.org/).

### ğŸ‘‰ Getting started

Wagtail works with [Python 3](https://www.python.org/downloads/), on any platform.

To get started with using Wagtail, run the following in a [virtual environment](https://docs.python.org/3/tutorial/venv.html):

![Installing Wagtail](.github/install-animation.gif)

```sh
pip install wagtail
wagtail start mysite
cd mysite
pip install -r requirements.txt
python manage.py migrate
python manage.py createsuperuser
python manage.py runserver
```

For detailed installation and setup docs, see [the getting started tutorial](https://docs.wagtail.org/en/stable/getting_started/tutorial.html).

### ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ Whoâ€™s using it?

Wagtail is used by [NASA](https://www.nasa.gov/), [Google](https://www.google.com/), [Oxfam](https://www.oxfam.org/en), the [NHS](https://www.nhs.uk/), [Mozilla](https://www.mozilla.org/en-US/), [MIT](https://www.mit.edu/), the [Red Cross](https://www.icrc.org/en), [Salesforce](https://www.salesforce.com/), [NBC](https://www.nbc.com/), [BMW](https://www.bmw.com/en/index.html), and the US and UK governments. Add your own Wagtail site to [madewithwagtail.org](https://madewithwagtail.org).

### ğŸ“– Documentation

[docs.wagtail.org](https://docs.wagtail.org/) is the full reference for Wagtail, and includes guides for developers, designers and editors, alongside [release notes](https://docs.wagtail.org/en/stable/releases/) and our [roadmap](https://wagtail.org/roadmap/).

For those who are **new to Wagtail**, the [Zen of Wagtail](https://docs.wagtail.org/en/stable/getting_started/the_zen_of_wagtail.html) will help you understand what Wagtail is, and what Wagtail is _not_.

**For developers** who are ready to jump in to their first Wagtail website the [Getting Started Tutorial](https://docs.wagtail.org/en/stable/getting_started/tutorial.html) will guide you through creating and editing your first page.

**Do you have an existing Django project?** The [Wagtail Integration documentation](https://docs.wagtail.org/en/stable/getting_started/integrating_into_django.html) is the best place to start.

### ğŸ“Œ Compatibility

_(If you are reading this on GitHub, the details here may not be indicative of the current released version - please see [Compatible Django / Python versions](https://docs.wagtail.org/en/stable/releases/upgrading.html#compatible-django-python-versions) in the Wagtail documentation.)_

Wagtail supports:

-   Django 4.2.x, 5.2.x and 6.0.x
-   Python 3.10, 3.11, 3.12, 3.13, and 3.14
-   PostgreSQL, MySQL, MariaDB and SQLite (with JSON1) as database backends

[Previous versions of Wagtail](https://docs.wagtail.org/en/stable/releases/upgrading.html#compatible-django-python-versions) additionally supported Python 2.7, 3.8 and earlier Django versions.

---

### ğŸ“¢ Community Support

There is an active community of Wagtail users and developers responding to questions on [Stack Overflow](https://stackoverflow.com/questions/tagged/wagtail). When posting questions, please read Stack Overflow&#039;s advice on [how to ask questions](https://stackoverflow.com/help/how-to-ask) and remember to tag your question &quot;wagtail&quot;.

For topics and discussions that do not fit Stack Overflow&#039;s question and answer format we have a [Slack workspace](https://github.com/wagtail/wagtail/wiki/Slack). Please respect the time and effort of volunteers by not asking the same question in multiple places.

[![Join slack community](.github/join-slack-community.png)](https://github.com/wagtail/wagtail/wiki/Slack)

Our [GitHub discussion boards](https://github.com/wagtail/wagtail/discussions) are open for sharing ideas and plans for the Wagtail project.

We maintain a curated list of third party packages, articles and other resources at [Awesome Wagtail](https://github.com/springload/awesome-wagtail).

### ğŸ§‘â€ğŸ’¼ Commercial Support

Wagtail is sponsored by [Torchbox](https://torchbox.com/). If you need help implementing or hosting Wagtail, please contact us: hello@torchbox.com. See also [madewithwagtail.org/developers/](https://madewithwagtail.org/developers/) for expert Wagtail developers around the world.

### ğŸ” Security

We take the security of Wagtail, and related packages we maintain, seriously. If you have found a security issue with any of our projects please email us at [security@wagtail.org](mailto:security@wagtail.org) so we can work together to find and patch the issue. We appreciate responsible disclosure with any security related issues, so please contact us first before creating a GitHub issue.

If you want to send an encrypted email (optional), the public key ID for security@wagtail.org is 0xbed227b4daf93ff9, and this public key is available from most commonly-used keyservers.

### ğŸ•’ Release schedule

Feature releases of Wagtail are released every three months. Selected releases are designated as Long Term Support (LTS) releases, and will receive maintenance updates for an extended period to address any security and data-loss related issues. For dates of past and upcoming releases and support periods, see [Release Schedule](https://github.com/wagtail/wagtail/wiki/Release-schedule).

#### ğŸ•› Nightly releases

To try out the latest features before a release, we also create builds from `main` every night. You can find instructions on how to install the latest nightly release at https://releases.wagtail.org/nightly/index.html

### ğŸ™‹ğŸ½ Contributing

If you&#039;re a Python or Django developer, fork the repo and get stuck in! We have several developer focused channels on the [Slack workspace](https://github.com/wagtail/wagtail/wiki/Slack).

You might like to start by reviewing the [contributing guidelines](https://docs.wagtail.org/en/latest/contributing/index.html) and checking issues with the [good first issue](https://github.com/wagtail/wagtail/labels/good%20first%20issue) label.

We also welcome translations for Wagtail&#039;s interface. Translation work should be submitted through [Transifex](https://explore.transifex.com/torchbox/wagtail/).

### ğŸ”“ License

[BSD](https://github.com/wagtail/wagtail/blob/main/LICENSE) - Free to use and modify for any purpose, including both open and closed-source code.

### ğŸ‘ Thanks

We thank the following organisations for their services used in Wagtail&#039;s development:

[![Browserstack](https://cdn.jsdelivr.net/gh/wagtail/wagtail@main/.github/browserstack-logo.svg)](https://www.browserstack.com/)&lt;br&gt;
[BrowserStack](https://www.browserstack.com/) provides the project with free access to their live web-based browser testing tool, and automated Selenium cloud testing.

[![Assistiv Labs](https://cdn.jsdelivr.net/gh/wagtail/wagtail@main/.github/assistivlabs-logo.png)](https://assistivlabs.com/)&lt;br&gt;
[Assistiv Labs](https://assistivlabs.com/) provides the project with unlimited access to their remote testing with assistive technologies.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[prowler-cloud/prowler]]></title>
            <link>https://github.com/prowler-cloud/prowler</link>
            <guid>https://github.com/prowler-cloud/prowler</guid>
            <pubDate>Sun, 18 Jan 2026 00:05:27 GMT</pubDate>
            <description><![CDATA[Prowler is the worldâ€™s most widely used open-source cloud security platform that automates security and compliance across any cloud environment.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/prowler-cloud/prowler">prowler-cloud/prowler</a></h1>
            <p>Prowler is the worldâ€™s most widely used open-source cloud security platform that automates security and compliance across any cloud environment.</p>
            <p>Language: Python</p>
            <p>Stars: 12,655</p>
            <p>Forks: 1,923</p>
            <p>Stars today: 35 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img align=&quot;center&quot; src=&quot;https://github.com/prowler-cloud/prowler/blob/master/docs/img/prowler-logo-black.png#gh-light-mode-only&quot; width=&quot;50%&quot; height=&quot;50%&quot;&gt;
  &lt;img align=&quot;center&quot; src=&quot;https://github.com/prowler-cloud/prowler/blob/master/docs/img/prowler-logo-white.png#gh-dark-mode-only&quot; width=&quot;50%&quot; height=&quot;50%&quot;&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;b&gt;&lt;i&gt;Prowler&lt;/b&gt; is the Open Cloud Security platform trusted by thousands to automate security and compliance in any cloud environment. With hundreds of ready-to-use checks and compliance frameworks, Prowler delivers real-time, customizable monitoring and seamless integrations, making cloud security simple, scalable, and cost-effective for organizations of any size.
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;b&gt;Secure ANY cloud at AI Speed at &lt;a href=&quot;https://prowler.com&quot;&gt;prowler.com&lt;/i&gt;&lt;/b&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://goto.prowler.com/slack&quot;&gt;&lt;img width=&quot;30&quot; height=&quot;30&quot; alt=&quot;Prowler community on Slack&quot; src=&quot;https://github.com/prowler-cloud/prowler/assets/38561120/3c8b4ec5-6849-41a5-b5e1-52bbb94af73a&quot;&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://goto.prowler.com/slack&quot;&gt;Join our Prowler community!&lt;/a&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://goto.prowler.com/slack&quot;&gt;&lt;img alt=&quot;Slack Shield&quot; src=&quot;https://img.shields.io/badge/slack-prowler-brightgreen.svg?logo=slack&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/prowler/&quot;&gt;&lt;img alt=&quot;Python Version&quot; src=&quot;https://img.shields.io/pypi/v/prowler.svg&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pypi.python.org/pypi/prowler/&quot;&gt;&lt;img alt=&quot;Python Version&quot; src=&quot;https://img.shields.io/pypi/pyversions/prowler.svg&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pypistats.org/packages/prowler&quot;&gt;&lt;img alt=&quot;PyPI Downloads&quot; src=&quot;https://img.shields.io/pypi/dw/prowler.svg?label=downloads&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://hub.docker.com/r/toniblyx/prowler&quot;&gt;&lt;img alt=&quot;Docker Pulls&quot; src=&quot;https://img.shields.io/docker/pulls/toniblyx/prowler&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://gallery.ecr.aws/prowler-cloud/prowler&quot;&gt;&lt;img width=&quot;120&quot; height=19&quot; alt=&quot;AWS ECR Gallery&quot; src=&quot;https://user-images.githubusercontent.com/3985464/151531396-b6535a68-c907-44eb-95a1-a09508178616.png&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://codecov.io/gh/prowler-cloud/prowler&quot;&gt;&lt;img src=&quot;https://codecov.io/gh/prowler-cloud/prowler/graph/badge.svg?token=OflBGsdpDl&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://insights.linuxfoundation.org/project/prowler-cloud-prowler&quot;&gt;&lt;img src=&quot;https://insights.linuxfoundation.org/api/badge/health-score?project=prowler-cloud-prowler&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://github.com/prowler-cloud/prowler/releases&quot;&gt;&lt;img alt=&quot;Version&quot; src=&quot;https://img.shields.io/github/v/release/prowler-cloud/prowler&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/prowler-cloud/prowler/releases&quot;&gt;&lt;img alt=&quot;Version&quot; src=&quot;https://img.shields.io/github/release-date/prowler-cloud/prowler&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/prowler-cloud/prowler&quot;&gt;&lt;img alt=&quot;Contributors&quot; src=&quot;https://img.shields.io/github/contributors-anon/prowler-cloud/prowler&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/prowler-cloud/prowler/issues&quot;&gt;&lt;img alt=&quot;Issues&quot; src=&quot;https://img.shields.io/github/issues/prowler-cloud/prowler&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/prowler-cloud/prowler&quot;&gt;&lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/github/license/prowler-cloud/prowler&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/ToniBlyx&quot;&gt;&lt;img alt=&quot;Twitter&quot; src=&quot;https://img.shields.io/twitter/follow/toniblyx?style=social&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/prowlercloud&quot;&gt;&lt;img alt=&quot;Twitter&quot; src=&quot;https://img.shields.io/twitter/follow/prowlercloud?style=social&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;hr&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img align=&quot;center&quot; src=&quot;/docs/img/prowler-cloud.gif&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;
&lt;/p&gt;

# Description

**Prowler** is the worldâ€™s most widely used _open-source cloud security platform_ that automates security and compliance across **any cloud environment**. With hundreds of ready-to-use security checks, remediation guidance, and compliance frameworks, Prowler is built to _â€œSecure ANY cloud at AI Speedâ€_. Prowler delivers **AI-driven**, **customizable**, and **easy-to-use** assessments, dashboards, reports, and integrations, making cloud security **simple**, **scalable**, and **cost-effective** for organizations of any size.

Prowler includes hundreds of built-in controls to ensure compliance with standards and frameworks, including:

- **Prowler ThreatScore:** Weighted risk prioritization scoring that helps you focus on the most critical security findings first
- **Industry Standards:** CIS, NIST 800, NIST CSF, CISA, and MITRE ATT&amp;CK
- **Regulatory Compliance and Governance:** RBI, FedRAMP, PCI-DSS, and NIS2
- **Frameworks for Sensitive Data and Privacy:** GDPR, HIPAA, and FFIEC
- **Frameworks for Organizational Governance and Quality Control:** SOC2, GXP, and ISO 27001
- **Cloud-Specific Frameworks:** AWS Foundational Technical Review (FTR), AWS Well-Architected Framework, and BSI C5
- **National Security Standards:** ENS (Spanish National Security Scheme) and KISA ISMS-P (Korean)
- **Custom Security Frameworks:** Tailored to your needs

## Prowler App / Prowler Cloud

Prowler App / [Prowler Cloud](https://cloud.prowler.com/) is a web-based application that simplifies running Prowler across your cloud provider accounts. It provides a user-friendly interface to visualize the results and streamline your security assessments.

![Prowler App](docs/images/products/overview.png)
![Risk Pipeline](docs/images/products/risk-pipeline.png)
![Threat Map](docs/images/products/threat-map.png)


&gt;For more details, refer to the [Prowler App Documentation](https://docs.prowler.com/projects/prowler-open-source/en/latest/#prowler-app-installation)

## Prowler CLI

```console
prowler &lt;provider&gt;
```
![Prowler CLI Execution](docs/img/short-display.png)


## Prowler Dashboard

```console
prowler dashboard
```
![Prowler Dashboard](docs/images/products/dashboard.png)


## Attack Paths

Attack Paths automatically extends every completed AWS scan with a Neo4j graph that combines Cartography&#039;s cloud inventory with Prowler findings. The feature runs in the API worker after each scan and therefore requires:

- An accessible Neo4j instance (the Docker Compose files already ships a `neo4j` service).
- The following environment variables so Django and Celery can connect:

  | Variable | Description | Default |
  | --- | --- | --- |
  | `NEO4J_HOST` | Hostname used by the API containers. | `neo4j` |
  | `NEO4J_PORT` | Bolt port exposed by Neo4j. | `7687` |
  | `NEO4J_USER` / `NEO4J_PASSWORD` | Credentials with rights to create per-tenant databases. | `neo4j` / `neo4j_password` |

Every AWS provider scan will enqueue an Attack Paths ingestion job automatically. Other cloud providers will be added in future iterations.


# Prowler at a Glance
&gt; [!Tip]
&gt; For the most accurate and up-to-date information about checks, services, frameworks, and categories, visit [**Prowler Hub**](https://hub.prowler.com).


| Provider | Checks | Services | [Compliance Frameworks](https://docs.prowler.com/projects/prowler-open-source/en/latest/tutorials/compliance/) | [Categories](https://docs.prowler.com/projects/prowler-open-source/en/latest/tutorials/misc/#categories) | Support | Interface |
|---|---|---|---|---|---|---|
| AWS | 584 | 85 | 40 | 17 | Official | UI, API, CLI |
| GCP | 89 | 17 | 14 | 5 | Official | UI, API, CLI |
| Azure | 169 | 22 | 15 | 8 | Official | UI, API, CLI |
| Kubernetes | 84 | 7 | 6 | 9 | Official | UI, API, CLI |
| GitHub | 20 | 2 | 1 | 2 | Official | UI, API, CLI |
| M365 | 70 | 7 | 3 | 2 | Official | UI, API, CLI |
| OCI | 52 | 15 | 1 | 12 | Official | UI, API, CLI |
| Alibaba Cloud | 63 | 10 | 1 | 9 | Official | CLI |
| IaC | [See `trivy` docs.](https://trivy.dev/latest/docs/coverage/iac/) | N/A | N/A | N/A | Official | UI, API, CLI |
| MongoDB Atlas | 10 | 4 | 0 | 3 | Official | UI, API, CLI |
| LLM | [See `promptfoo` docs.](https://www.promptfoo.dev/docs/red-team/plugins/) | N/A | N/A | N/A | Official | CLI |
| NHN | 6 | 2 | 1 | 0 | Unofficial | CLI |

&gt; [!Note]
&gt; The numbers in the table are updated periodically.



&gt; [!Note]
&gt; Use the following commands to list Prowler&#039;s available checks, services, compliance frameworks, and categories:
&gt; - `prowler &lt;provider&gt; --list-checks`
&gt; - `prowler &lt;provider&gt; --list-services`
&gt; - `prowler &lt;provider&gt; --list-compliance`
&gt; - `prowler &lt;provider&gt; --list-categories`

# ğŸ’» Installation

## Prowler App

Prowler App offers flexible installation methods tailored to various environments:

&gt; For detailed instructions on using Prowler App, refer to the [Prowler App Usage Guide](https://docs.prowler.com/projects/prowler-open-source/en/latest/tutorials/prowler-app/).

### Docker Compose

**Requirements**

* `Docker Compose` installed: https://docs.docker.com/compose/install/.

**Commands**

``` console
curl -LO https://raw.githubusercontent.com/prowler-cloud/prowler/refs/heads/master/docker-compose.yml
curl -LO https://raw.githubusercontent.com/prowler-cloud/prowler/refs/heads/master/.env
docker compose up -d
```

&gt; Containers are built for `linux/amd64`.

### Configuring Your Workstation for Prowler App

If your workstation&#039;s architecture is incompatible, you can resolve this by:

- **Setting the environment variable**: `DOCKER_DEFAULT_PLATFORM=linux/amd64`
- **Using the following flag in your Docker command**: `--platform linux/amd64`

&gt; Once configured, access the Prowler App at http://localhost:3000. Sign up using your email and password to get started.

### Common Issues with Docker Pull Installation

&gt; [!Note]
  If you want to use AWS role assumption (e.g., with the &quot;Connect assuming IAM Role&quot; option), you may need to mount your local `.aws` directory into the container as a volume (e.g., `- &quot;${HOME}/.aws:/home/prowler/.aws:ro&quot;`). There are several ways to configure credentials for Docker containers. See the [Troubleshooting](./docs/troubleshooting.mdx) section for more details and examples.

You can find more information in the [Troubleshooting](./docs/troubleshooting.mdx) section.


### From GitHub

**Requirements**

* `git` installed.
* `poetry` v2 installed: [poetry installation](https://python-poetry.org/docs/#installation).
* `pnpm` installed: [pnpm installation](https://pnpm.io/installation).
* `Docker Compose` installed: https://docs.docker.com/compose/install/.

**Commands to run the API**

``` console
git clone https://github.com/prowler-cloud/prowler
cd prowler/api
poetry install
eval $(poetry env activate)
set -a
source .env
docker compose up postgres valkey -d
cd src/backend
python manage.py migrate --database admin
gunicorn -c config/guniconf.py config.wsgi:application
```
&gt; [!IMPORTANT]
&gt; As of Poetry v2.0.0, the `poetry shell` command has been deprecated. Use `poetry env activate` instead for environment activation.
&gt;
&gt; If your Poetry version is below v2.0.0, continue using `poetry shell` to activate your environment.
&gt; For further guidance, refer to the Poetry Environment Activation Guide https://python-poetry.org/docs/managing-environments/#activating-the-environment.

&gt; After completing the setup, access the API documentation at http://localhost:8080/api/v1/docs.

**Commands to run the API Worker**

``` console
git clone https://github.com/prowler-cloud/prowler
cd prowler/api
poetry install
eval $(poetry env activate)
set -a
source .env
cd src/backend
python -m celery -A config.celery worker -l info -E
```

**Commands to run the API Scheduler**

``` console
git clone https://github.com/prowler-cloud/prowler
cd prowler/api
poetry install
eval $(poetry env activate)
set -a
source .env
cd src/backend
python -m celery -A config.celery beat -l info --scheduler django_celery_beat.schedulers:DatabaseScheduler
```

**Commands to run the UI**

``` console
git clone https://github.com/prowler-cloud/prowler
cd prowler/ui
pnpm install
pnpm run build
pnpm start
```

&gt; Once configured, access the Prowler App at http://localhost:3000. Sign up using your email and password to get started.

## Prowler CLI
### Pip package
Prowler CLI is available as a project in [PyPI](https://pypi.org/project/prowler-cloud/). Consequently, it can be installed using pip with Python &gt;3.9.1, &lt;3.13:

```console
pip install prowler
prowler -v
```
&gt;For further guidance, refer to [https://docs.prowler.com](https://docs.prowler.com/projects/prowler-open-source/en/latest/#prowler-cli-installation)

### Containers

**Available Versions of Prowler CLI**

The following versions of Prowler CLI are available, depending on your requirements:

- `latest`: Synchronizes with the `master` branch. Note that this version is not stable.
- `v4-latest`: Synchronizes with the `v4` branch. Note that this version is not stable.
- `v3-latest`: Synchronizes with the `v3` branch. Note that this version is not stable.
- `&lt;x.y.z&gt;` (release): Stable releases corresponding to specific versions. You can find the complete list of releases [here](https://github.com/prowler-cloud/prowler/releases).
- `stable`: Always points to the latest release.
- `v4-stable`: Always points to the latest release for v4.
- `v3-stable`: Always points to the latest release for v3.

The container images are available here:
- Prowler CLI:
    - [DockerHub](https://hub.docker.com/r/prowlercloud/prowler/tags)
    - [AWS Public ECR](https://gallery.ecr.aws/prowler-cloud/prowler)
- Prowler App:
    - [DockerHub - Prowler UI](https://hub.docker.com/r/prowlercloud/prowler-ui/tags)
    - [DockerHub - Prowler API](https://hub.docker.com/r/prowlercloud/prowler-api/tags)

### From GitHub

Python &gt;3.9.1, &lt;3.13 is required with pip and Poetry:

``` console
git clone https://github.com/prowler-cloud/prowler
cd prowler
eval $(poetry env activate)
poetry install
python prowler-cli.py -v
```
&gt; [!IMPORTANT]
&gt; To clone Prowler on Windows, configure Git to support long file paths by running the following command: `git config core.longpaths true`.

&gt; [!IMPORTANT]
&gt; As of Poetry v2.0.0, the `poetry shell` command has been deprecated. Use `poetry env activate` instead for environment activation.
&gt;
&gt; If your Poetry version is below v2.0.0, continue using `poetry shell` to activate your environment.
&gt; For further guidance, refer to the Poetry Environment Activation Guide https://python-poetry.org/docs/managing-environments/#activating-the-environment.

# âœï¸ High level architecture

## Prowler App
**Prowler App** is composed of four key components:

- **Prowler UI**: A web-based interface, built with Next.js, providing a user-friendly experience for executing Prowler scans and visualizing results.
- **Prowler API**: A backend service, developed with Django REST Framework, responsible for running Prowler scans and storing the generated results.
- **Prowler SDK**: A Python SDK designed to extend the functionality of the Prowler CLI for advanced capabilities.
- **Prowler MCP Server**: A Model Context Protocol server that provides AI tools for Lighthouse, the AI-powered security assistant. This is a critical dependency for Lighthouse functionality.

![Prowler App Architecture](docs/products/img/prowler-app-architecture.png)

## Prowler CLI

**Running Prowler**

Prowler can be executed across various environments, offering flexibility to meet your needs. It can be run from:

- Your own workstation

- A Kubernetes Job

- Google Compute Engine

- Azure Virtual Machines (VMs)

- Amazon EC2 instances

- AWS Fargate or other container platforms

- CloudShell

And many more environments.

![Architecture](docs/img/architecture.png)

# ğŸ¤– AI Skills for Development

Prowler includes a comprehensive set of **AI Skills** that help AI coding assistants understand Prowler&#039;s codebase patterns and conventions.

## What are AI Skills?

Skills are structured instructions that give AI assistants the context they need to write code that follows Prowler&#039;s standards. They include:

- **Coding patterns** for each component (SDK, API, UI, MCP Server)
- **Testing conventions** (pytest, Playwright)
- **Architecture guidelines** (Clean Architecture, RLS patterns)
- **Framework-specific rules** (React 19, Next.js 15, Django DRF, Tailwind 4)

## Available Skills

| Category | Skills |
|----------|--------|
| **Generic** | `typescript`, `react-19`, `nextjs-15`, `tailwind-4`, `playwright`, `pytest`, `django-drf`, `zod-4`, `zustand-5`, `ai-sdk-5` |
| **Prowler** | `prowler`, `prowler-api`, `prowler-ui`, `prowler-mcp`, `prowler-sdk-check`, `prowler-test-ui`, `prowler-test-api`, `prowler-test-sdk`, `prowler-compliance`, `prowler-provider`, `prowler-pr`, `prowler-docs` |

## Setup

```bash
./skills/setup.sh
```

This configures skills for AI coding assistants that follow the [agentskills.io](https://agentskills.io) standard:

| Tool | Configuration |
|------|---------------|
| **Claude Code** | `.claude/skills/` (symlink) |
| **OpenCode** | `.claude/skills/` (symlink) |
| **Codex (OpenAI)** | `.codex/skills/` (symlink) |
| **GitHub Copilot** | `.github/skills/` (symlink) |
| **Gemini CLI** | `.gemini/skills/` (symlink) |

&gt; **Note:** Restart your AI coding assistant after running setup to load the skills.
&gt; Gemini CLI requires `experimental.skills` enabled in settings.

# ğŸ“– Documentation

For installation instructions, usage details, tutorials, and the Developer Guide, visit https://docs.prowler.com/

# ğŸ“ƒ License

Prowler is licensed under the Apache License 2.0.

A copy of the License is available at &lt;http://www.apache.org/licenses/LICENSE-2.0&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jumpserver/jumpserver]]></title>
            <link>https://github.com/jumpserver/jumpserver</link>
            <guid>https://github.com/jumpserver/jumpserver</guid>
            <pubDate>Sun, 18 Jan 2026 00:05:26 GMT</pubDate>
            <description><![CDATA[JumpServer is an open-source Privileged Access Management (PAM) platform that provides DevOps and IT teams with on-demand and secure access to SSH, RDP, Kubernetes, Database and RemoteApp endpoints through a web browser.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jumpserver/jumpserver">jumpserver/jumpserver</a></h1>
            <p>JumpServer is an open-source Privileged Access Management (PAM) platform that provides DevOps and IT teams with on-demand and secure access to SSH, RDP, Kubernetes, Database and RemoteApp endpoints through a web browser.</p>
            <p>Language: Python</p>
            <p>Stars: 29,622</p>
            <p>Forks: 5,632</p>
            <p>Stars today: 79 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://jumpserver.com&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://download.jumpserver.org/images/jumpserver-logo.svg&quot; alt=&quot;JumpServer&quot; width=&quot;300&quot; /&gt;&lt;/a&gt;
  
## An open-source PAM platform (Bastion Host)

[![][license-shield]][license-link]
[![][docs-shield]][docs-link]
[![][deepwiki-shield]][deepwiki-link]
[![][discord-shield]][discord-link]
[![][docker-shield]][docker-link]
[![][github-release-shield]][github-release-link]
[![][github-stars-shield]][github-stars-link]

[English](/README.md) Â· [ä¸­æ–‡(ç®€ä½“)](/readmes/README.zh-hans.md) Â· [ä¸­æ–‡(ç¹é«”)](/readmes/README.zh-hant.md) Â· [æ—¥æœ¬èª](/readmes/README.ja.md) Â· [PortuguÃªs (Brasil)](/readmes/README.pt-br.md) Â· [EspaÃ±ol](/readmes/README.es.md) Â· [Ğ ÑƒÑÑĞºĞ¸Ğ¹](/readmes/README.ru.md) Â· [í•œêµ­ì–´](/readmes/README.ko.md)

&lt;/div&gt;
&lt;br/&gt;

## What is JumpServer?

JumpServer is an open-source Privileged Access Management (PAM) platform that provides DevOps and IT teams with on-demand and secure access to SSH, RDP, Kubernetes, Database and RemoteApp endpoints through a web browser.


&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://www.jumpserver.com/images/jumpserver-arch-light.png&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://www.jumpserver.com/images/jumpserver-arch-dark.png&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/dd612f3d-c958-4f84-b164-f31b75454d7f&quot; alt=&quot;Theme-based Image&quot;&gt;
&lt;/picture&gt;


## Quickstart

Prepare a clean Linux Server ( 64 bit, &gt;= 4c8g )

```sh
curl -sSL https://github.com/jumpserver/jumpserver/releases/latest/download/quick_start.sh | bash
```

Access JumpServer in your browser at `http://your-jumpserver-ip/`
- Username: `admin`
- Password: `ChangeMe`

[![JumpServer Quickstart](https://github.com/user-attachments/assets/0f32f52b-9935-485e-8534-336c63389612)](https://www.youtube.com/watch?v=UlGYRbKrpgY &quot;JumpServer Quickstart&quot;)

## Screenshots
&lt;table style=&quot;border-collapse: collapse; border: 1px solid black;&quot;&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/jumpserver/jumpserver/assets/32935519/99fabe5b-0475-4a53-9116-4c370a1426c4&quot; alt=&quot;JumpServer Console&quot;   /&gt;&lt;/td&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/user-attachments/assets/7c1f81af-37e8-4f07-8ac9-182895e1062e&quot; alt=&quot;JumpServer PAM&quot;   /&gt;&lt;/td&gt;Â Â Â Â 
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/jumpserver/jumpserver/assets/32935519/a424d731-1c70-4108-a7d8-5bbf387dda9a&quot; alt=&quot;JumpServer Audits&quot;   /&gt;&lt;/td&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/jumpserver/jumpserver/assets/32935519/393d2c27-a2d0-4dea-882d-00ed509e00c9&quot; alt=&quot;JumpServer Workbench&quot;   /&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/user-attachments/assets/eaa41f66-8cc8-4f01-a001-0d258501f1c9&quot; alt=&quot;JumpServer RBAC&quot;   /&gt;&lt;/td&gt;Â Â Â Â Â 
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/jumpserver/jumpserver/assets/32935519/3a2611cd-8902-49b8-b82b-2a6dac851f3e&quot; alt=&quot;JumpServer Settings&quot;   /&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/jumpserver/jumpserver/assets/32935519/1e236093-31f7-4563-8eb1-e36d865f1568&quot; alt=&quot;JumpServer SSH&quot;   /&gt;&lt;/td&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/jumpserver/jumpserver/assets/32935519/69373a82-f7ab-41e8-b763-bbad2ba52167&quot; alt=&quot;JumpServer RDP&quot;   /&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/jumpserver/jumpserver/assets/32935519/5bed98c6-cbe8-4073-9597-d53c69dc3957&quot; alt=&quot;JumpServer K8s&quot;   /&gt;&lt;/td&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://github.com/jumpserver/jumpserver/assets/32935519/b80ad654-548f-42bc-ba3d-c1cfdf1b46d6&quot; alt=&quot;JumpServer DB&quot;   /&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## Components

JumpServer consists of multiple key components, which collectively form the functional framework of JumpServer, providing users with comprehensive capabilities for operations management and security control.

| Project                                                | Status                                                                                                                                                                 | Description                                                                                             |
|--------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|
| [Lina](https://github.com/jumpserver/lina)             | &lt;a href=&quot;https://github.com/jumpserver/lina/releases&quot;&gt;&lt;img alt=&quot;Lina release&quot; src=&quot;https://img.shields.io/github/release/jumpserver/lina.svg&quot; /&gt;&lt;/a&gt;                   | JumpServer Web UI                                                                                       |
| [Luna](https://github.com/jumpserver/luna)             | &lt;a href=&quot;https://github.com/jumpserver/luna/releases&quot;&gt;&lt;img alt=&quot;Luna release&quot; src=&quot;https://img.shields.io/github/release/jumpserver/luna.svg&quot; /&gt;&lt;/a&gt;                   | JumpServer Web Terminal                                                                                 |
| [KoKo](https://github.com/jumpserver/koko)             | &lt;a href=&quot;https://github.com/jumpserver/koko/releases&quot;&gt;&lt;img alt=&quot;Koko release&quot; src=&quot;https://img.shields.io/github/release/jumpserver/koko.svg&quot; /&gt;&lt;/a&gt;                   | JumpServer Character Protocol Connector                                                                 |
| [Lion](https://github.com/jumpserver/lion)             | &lt;a href=&quot;https://github.com/jumpserver/lion/releases&quot;&gt;&lt;img alt=&quot;Lion release&quot; src=&quot;https://img.shields.io/github/release/jumpserver/lion.svg&quot; /&gt;&lt;/a&gt;                   | JumpServer Graphical Protocol Connector                                                                 |
| [Chen](https://github.com/jumpserver/chen)             | &lt;a href=&quot;https://github.com/jumpserver/chen/releases&quot;&gt;&lt;img alt=&quot;Chen release&quot; src=&quot;https://img.shields.io/github/release/jumpserver/chen.svg&quot; /&gt;                       | JumpServer Web DB 
| [Client](https://github.com/jumpserver/clients)             | &lt;a href=&quot;https://github.com/jumpserver/clients/releases&quot;&gt;&lt;img alt=&quot;Clients release&quot; src=&quot;https://img.shields.io/github/release/jumpserver/clients.svg&quot; /&gt;                       | JumpServer Client     |  
| [Tinker](https://github.com/jumpserver/tinker)         | &lt;img alt=&quot;Tinker&quot; src=&quot;https://img.shields.io/badge/release-private-red&quot; /&gt;                                                                                            | JumpServer Remote Application Connector (Windows)                                                    |
| [Panda](https://github.com/jumpserver/Panda)           | &lt;img alt=&quot;Panda&quot; src=&quot;https://img.shields.io/badge/release-private-red&quot; /&gt;                                                                                             | JumpServer EE Remote Application Connector (Linux)                                                      |
| [Razor](https://github.com/jumpserver/razor)           | &lt;img alt=&quot;Chen&quot; src=&quot;https://img.shields.io/badge/release-private-red&quot; /&gt;                                                                                              | JumpServer EE RDP Proxy Connector                                                                       |
| [Magnus](https://github.com/jumpserver/magnus)         | &lt;img alt=&quot;Magnus&quot; src=&quot;https://img.shields.io/badge/release-private-red&quot; /&gt;                                                                                            | JumpServer EE Database Proxy Connector                                                                  |
| [Nec](https://github.com/jumpserver/nec)               | &lt;img alt=&quot;Nec&quot; src=&quot;https://img.shields.io/badge/release-private-red&quot; /&gt;                                                                                               | JumpServer EE VNC Proxy Connector                                                                       |
| [Facelive](https://github.com/jumpserver/facelive)     | &lt;img alt=&quot;Facelive&quot; src=&quot;https://img.shields.io/badge/release-private-red&quot; /&gt;                                                                                          | JumpServer EE Facial Recognition                                                                        |

## Third-party projects 
- [jumpserver-grafana-dashboard](https://github.com/acerrah/jumpserver-grafana-dashboard)   JumpServer with grafana dashboard

## Contributing

Welcome to submit PR to contribute. Please refer to [CONTRIBUTING.md][contributing-link] for guidelines.

## License

Copyright (c) 2014-2025 FIT2CLOUD, All rights reserved.

Licensed under The GNU General Public License version 3 (GPLv3) (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at

https://www.gnu.org/licenses/gpl-3.0.html

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot; AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.

&lt;!-- JumpServer official link --&gt;
[docs-link]: https://jumpserver.com/docs
[discord-link]: https://discord.com/invite/W6vYXmAQG2
[deepwiki-link]: https://deepwiki.com/jumpserver/jumpserver/
[contributing-link]: https://github.com/jumpserver/jumpserver/blob/dev/CONTRIBUTING.md

&lt;!-- JumpServer Other link--&gt;
[license-link]: https://www.gnu.org/licenses/gpl-3.0.html
[docker-link]: https://hub.docker.com/u/jumpserver
[github-release-link]: https://github.com/jumpserver/jumpserver/releases/latest
[github-stars-link]: https://github.com/jumpserver/jumpserver
[github-issues-link]: https://github.com/jumpserver/jumpserver/issues

&lt;!-- Shield link--&gt;
[docs-shield]: https://img.shields.io/badge/documentation-148F76
[github-release-shield]: https://img.shields.io/github/v/release/jumpserver/jumpserver
[github-stars-shield]: https://img.shields.io/github/stars/jumpserver/jumpserver?color=%231890FF&amp;style=flat-squareÂ Â Â 
[docker-shield]: https://img.shields.io/docker/pulls/jumpserver/jms_all.svg
[license-shield]: https://img.shields.io/github/license/jumpserver/jumpserver
[deepwiki-shield]: https://img.shields.io/badge/deepwiki-devin?color=blue
[discord-shield]: https://img.shields.io/discord/1194233267294052363?style=flat&amp;logo=discord&amp;logoColor=%23f5f5f5&amp;labelColor=%235462eb&amp;color=%235462eb
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>