<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sat, 28 Feb 2026 00:05:16 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[ruvnet/wifi-densepose]]></title>
            <link>https://github.com/ruvnet/wifi-densepose</link>
            <guid>https://github.com/ruvnet/wifi-densepose</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:16 GMT</pubDate>
            <description><![CDATA[Production-ready implementation of InvisPose - a revolutionary WiFi-based dense human pose estimation system that enables real-time full-body tracking through walls using commodity mesh routers]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ruvnet/wifi-densepose">ruvnet/wifi-densepose</a></h1>
            <p>Production-ready implementation of InvisPose - a revolutionary WiFi-based dense human pose estimation system that enables real-time full-body tracking through walls using commodity mesh routers</p>
            <p>Language: Python</p>
            <p>Stars: 9,105</p>
            <p>Forks: 852</p>
            <p>Stars today: 478 stars today</p>
            <h2>README</h2><pre># WiFi DensePose

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.95+-green.svg)](https://fastapi.tiangolo.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PyPI version](https://img.shields.io/pypi/v/wifi-densepose.svg)](https://pypi.org/project/wifi-densepose/)
[![PyPI downloads](https://img.shields.io/pypi/dm/wifi-densepose.svg)](https://pypi.org/project/wifi-densepose/)
[![Test Coverage](https://img.shields.io/badge/coverage-100%25-brightgreen.svg)](https://github.com/ruvnet/wifi-densepose)
[![Docker](https://img.shields.io/badge/docker-ready-blue.svg)](https://hub.docker.com/r/ruvnet/wifi-densepose)

A cutting-edge WiFi-based human pose estimation system that leverages Channel State Information (CSI) data and advanced machine learning to provide real-time, privacy-preserving pose detection without cameras.

## ğŸš€ Key Features

- **Privacy-First**: No cameras required - uses WiFi signals for pose detection
- **Real-Time Processing**: Sub-50ms latency with 30 FPS pose estimation
- **Multi-Person Tracking**: Simultaneous tracking of up to 10 individuals
- **Domain-Specific Optimization**: Healthcare, fitness, smart home, and security applications
- **Enterprise-Ready**: Production-grade API with authentication, rate limiting, and monitoring
- **Hardware Agnostic**: Works with standard WiFi routers and access points
- **Comprehensive Analytics**: Fall detection, activity recognition, and occupancy monitoring
- **WebSocket Streaming**: Real-time pose data streaming for live applications
- **100% Test Coverage**: Thoroughly tested with comprehensive test suite

## ğŸ¦€ Rust Implementation (v2)

A high-performance Rust port is available in `/rust-port/wifi-densepose-rs/`:

### Performance Benchmarks (Validated)

| Operation | Python (v1) | Rust (v2) | Speedup |
|-----------|-------------|-----------|---------|
| CSI Preprocessing (4x64) | ~5ms | **5.19 Âµs** | ~1000x |
| Phase Sanitization (4x64) | ~3ms | **3.84 Âµs** | ~780x |
| Feature Extraction (4x64) | ~8ms | **9.03 Âµs** | ~890x |
| Motion Detection | ~1ms | **186 ns** | ~5400x |
| **Full Pipeline** | ~15ms | **18.47 Âµs** | ~810x |

### Throughput Metrics

| Component | Throughput |
|-----------|------------|
| CSI Preprocessing | 49-66 Melem/s |
| Phase Sanitization | 67-85 Melem/s |
| Feature Extraction | 7-11 Melem/s |
| Full Pipeline | **~54,000 fps** |

### Resource Comparison

| Feature | Python (v1) | Rust (v2) |
|---------|-------------|-----------|
| Memory Usage | ~500MB | ~100MB |
| WASM Support | âŒ | âœ… |
| Binary Size | N/A | ~10MB |
| Test Coverage | 100% | 107 tests |

**Quick Start (Rust):**
```bash
cd rust-port/wifi-densepose-rs
cargo build --release
cargo test --workspace
cargo bench --package wifi-densepose-signal
```

### Validation Tests

Mathematical correctness validated:
- âœ… Phase unwrapping: 0.000000 radians max error
- âœ… Amplitude RMS: Exact match
- âœ… Doppler shift: 33.33 Hz (exact)
- âœ… Correlation: 1.0 for identical signals
- âœ… Phase coherence: 1.0 for coherent signals

See [Rust Port Documentation](/rust-port/wifi-densepose-rs/docs/) for ADRs and DDD patterns.

## ğŸš¨ WiFi-Mat: Disaster Response Module

A specialized extension for **search and rescue operations** - detecting and localizing survivors trapped in rubble, earthquakes, and natural disasters.

### Key Capabilities

| Feature | Description |
|---------|-------------|
| **Vital Signs Detection** | Breathing (4-60 BPM), heartbeat via micro-Doppler |
| **3D Localization** | Position estimation through debris up to 5m depth |
| **START Triage** | Automatic Immediate/Delayed/Minor/Deceased classification |
| **Real-time Alerts** | Priority-based notifications with escalation |

### Use Cases

- Earthquake search and rescue
- Building collapse response
- Avalanche victim location
- Mine collapse detection
- Flood rescue operations

### Quick Example

```rust
use wifi_densepose_mat::{DisasterResponse, DisasterConfig, DisasterType, ScanZone, ZoneBounds};

let config = DisasterConfig::builder()
    .disaster_type(DisasterType::Earthquake)
    .sensitivity(0.85)
    .max_depth(5.0)
    .build();

let mut response = DisasterResponse::new(config);
response.initialize_event(location, &quot;Building collapse&quot;)?;
response.add_zone(ScanZone::new(&quot;North Wing&quot;, ZoneBounds::rectangle(0.0, 0.0, 30.0, 20.0)))?;
response.start_scanning().await?;

// Get survivors prioritized by triage status
let immediate = response.survivors_by_triage(TriageStatus::Immediate);
println!(&quot;{} survivors require immediate rescue&quot;, immediate.len());
```

### Documentation

- **[WiFi-Mat User Guide](docs/wifi-mat-user-guide.md)** - Complete setup, configuration, and field deployment
- **[Architecture Decision Record](docs/adr/ADR-001-wifi-mat-disaster-detection.md)** - Design decisions and rationale
- **[Domain Model](docs/ddd/wifi-mat-domain-model.md)** - DDD bounded contexts and entities

**Build:**
```bash
cd rust-port/wifi-densepose-rs
cargo build --release --package wifi-densepose-mat
cargo test --package wifi-densepose-mat
```

## ğŸ“‹ Table of Contents

&lt;table&gt;
&lt;tr&gt;
&lt;td width=&quot;50%&quot;&gt;

**ğŸš€ Getting Started**
- [Key Features](#-key-features)
- [Rust Implementation (v2)](#-rust-implementation-v2)
- [WiFi-Mat Disaster Response](#-wifi-mat-disaster-response-module)
- [System Architecture](#ï¸-system-architecture)
- [Installation](#-installation)
  - [Using pip (Recommended)](#using-pip-recommended)
  - [From Source](#from-source)
  - [Using Docker](#using-docker)
  - [System Requirements](#system-requirements)
- [Quick Start](#-quick-start)
  - [Basic Setup](#1-basic-setup)
  - [Start the System](#2-start-the-system)
  - [Using the REST API](#3-using-the-rest-api)
  - [Real-time Streaming](#4-real-time-streaming)

**ğŸ–¥ï¸ Usage &amp; Configuration**
- [CLI Usage](#ï¸-cli-usage)
  - [Installation](#cli-installation)
  - [Basic Commands](#basic-commands)
  - [Configuration Commands](#configuration-commands)
  - [Examples](#cli-examples)
- [Documentation](#-documentation)
  - [Core Documentation](#-core-documentation)
  - [Quick Links](#-quick-links)
  - [API Overview](#-api-overview)
- [Hardware Setup](#-hardware-setup)
  - [Supported Hardware](#supported-hardware)
  - [Physical Setup](#physical-setup)
  - [Network Configuration](#network-configuration)
  - [Environment Calibration](#environment-calibration)

&lt;/td&gt;
&lt;td width=&quot;50%&quot;&gt;

**âš™ï¸ Advanced Topics**
- [Configuration](#ï¸-configuration)
  - [Environment Variables](#environment-variables)
  - [Domain-Specific Configurations](#domain-specific-configurations)
  - [Advanced Configuration](#advanced-configuration)
- [Testing](#-testing)
  - [Running Tests](#running-tests)
  - [Test Categories](#test-categories)
  - [Mock Testing](#mock-testing)
  - [Continuous Integration](#continuous-integration)
- [Deployment](#-deployment)
  - [Production Deployment](#production-deployment)
  - [Infrastructure as Code](#infrastructure-as-code)
  - [Monitoring and Logging](#monitoring-and-logging)

**ğŸ“Š Performance &amp; Community**
- [Performance Metrics](#-performance-metrics)
  - [Benchmark Results](#benchmark-results)
  - [Performance Optimization](#performance-optimization)
  - [Load Testing](#load-testing)
- [Contributing](#-contributing)
  - [Development Setup](#development-setup)
  - [Code Standards](#code-standards)
  - [Contribution Process](#contribution-process)
  - [Code Review Checklist](#code-review-checklist)
- [License](#-license)
- [Acknowledgments](#-acknowledgments)
- [Support](#-support)

&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

## ğŸ—ï¸ System Architecture

WiFi DensePose consists of several key components working together:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   WiFi Router   â”‚    â”‚   WiFi Router   â”‚    â”‚   WiFi Router   â”‚
â”‚   (CSI Source)  â”‚    â”‚   (CSI Source)  â”‚    â”‚   (CSI Source)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     CSI Data Collector    â”‚
                    â”‚   (Hardware Interface)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Signal Processor       â”‚
                    â”‚  (Phase Sanitization)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Neural Network Model    â”‚
                    â”‚    (DensePose Head)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Person Tracker          â”‚
                    â”‚  (Multi-Object Tracking)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                       â”‚                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   REST API        â”‚   â”‚  WebSocket API    â”‚   â”‚   Analytics       â”‚
â”‚  (CRUD Operations)â”‚   â”‚ (Real-time Stream)â”‚   â”‚  (Fall Detection) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

- **CSI Processor**: Extracts and processes Channel State Information from WiFi signals
- **Phase Sanitizer**: Removes hardware-specific phase offsets and noise
- **DensePose Neural Network**: Converts CSI data to human pose keypoints
- **Multi-Person Tracker**: Maintains consistent person identities across frames
- **REST API**: Comprehensive API for data access and system control
- **WebSocket Streaming**: Real-time pose data broadcasting
- **Analytics Engine**: Advanced analytics including fall detection and activity recognition

## ğŸ“¦ Installation

### Using pip (Recommended)

WiFi-DensePose is now available on PyPI for easy installation:

```bash
# Install the latest stable version
pip install wifi-densepose

# Install with specific version
pip install wifi-densepose==1.0.0

# Install with optional dependencies
pip install wifi-densepose[gpu]  # For GPU acceleration
pip install wifi-densepose[dev]  # For development
pip install wifi-densepose[all]  # All optional dependencies
```

### From Source

```bash
git clone https://github.com/ruvnet/wifi-densepose.git
cd wifi-densepose
pip install -r requirements.txt
pip install -e .
```

### Using Docker

```bash
docker pull ruvnet/wifi-densepose:latest
docker run -p 8000:8000 ruvnet/wifi-densepose:latest
```

### System Requirements

- **Python**: 3.8 or higher
- **Operating System**: Linux (Ubuntu 18.04+), macOS (10.15+), Windows 10+
- **Memory**: Minimum 4GB RAM, Recommended 8GB+
- **Storage**: 2GB free space for models and data
- **Network**: WiFi interface with CSI capability
- **GPU**: Optional but recommended (NVIDIA GPU with CUDA support)

## ğŸš€ Quick Start

### 1. Basic Setup

```bash
# Install the package
pip install wifi-densepose

# Copy example configuration
cp example.env .env

# Edit configuration (set your WiFi interface)
nano .env
```

### 2. Start the System

```python
from wifi_densepose import WiFiDensePose

# Initialize with default configuration
system = WiFiDensePose()

# Start pose estimation
system.start()

# Get latest pose data
poses = system.get_latest_poses()
print(f&quot;Detected {len(poses)} persons&quot;)

# Stop the system
system.stop()
```

### 3. Using the REST API

```bash
# Start the API server
wifi-densepose start

# Start with custom configuration
wifi-densepose -c /path/to/config.yaml start

# Start with verbose logging
wifi-densepose -v start

# Check server status
wifi-densepose status
```

The API will be available at `http://localhost:8000`

- **API Documentation**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/api/v1/health
- **Latest Poses**: http://localhost:8000/api/v1/pose/latest

### 4. Real-time Streaming

```python
import asyncio
import websockets
import json

async def stream_poses():
    uri = &quot;ws://localhost:8000/ws/pose/stream&quot;
    async with websockets.connect(uri) as websocket:
        while True:
            data = await websocket.recv()
            poses = json.loads(data)
            print(f&quot;Received poses: {len(poses[&#039;persons&#039;])} persons detected&quot;)

# Run the streaming client
asyncio.run(stream_poses())
```

## ğŸ–¥ï¸ CLI Usage

WiFi DensePose provides a comprehensive command-line interface for easy system management, configuration, and monitoring.

### CLI Installation

The CLI is automatically installed with the package:

```bash
# Install WiFi DensePose with CLI
pip install wifi-densepose

# Verify CLI installation
wifi-densepose --help
wifi-densepose version
```

### Basic Commands

The WiFi-DensePose CLI provides the following commands:

```bash
wifi-densepose [OPTIONS] COMMAND [ARGS]...

Options:
  -c, --config PATH  Path to configuration file
  -v, --verbose      Enable verbose logging
  --debug            Enable debug mode
  --help             Show this message and exit.

Commands:
  config   Configuration management commands.
  db       Database management commands.
  start    Start the WiFi-DensePose API server.
  status   Show the status of the WiFi-DensePose API server.
  stop     Stop the WiFi-DensePose API server.
  tasks    Background task management commands.
  version  Show version information.
```

#### Server Management
```bash
# Start the WiFi-DensePose API server
wifi-densepose start

# Start with custom configuration
wifi-densepose -c /path/to/config.yaml start

# Start with verbose logging
wifi-densepose -v start

# Start with debug mode
wifi-densepose --debug start

# Check server status
wifi-densepose status

# Stop the server
wifi-densepose stop

# Show version information
wifi-densepose version
```

### Configuration Commands

#### Configuration Management
```bash
# Configuration management commands
wifi-densepose config [SUBCOMMAND]

# Examples:
# Show current configuration
wifi-densepose config show

# Validate configuration file
wifi-densepose config validate

# Create default configuration
wifi-densepose config init

# Edit configuration
wifi-densepose config edit
```

#### Database Management
```bash
# Database management commands
wifi-densepose db [SUBCOMMAND]

# Examples:
# Initialize database
wifi-densepose db init

# Run database migrations
wifi-densepose db migrate

# Check database status
wifi-densepose db status

# Backup database
wifi-densepose db backup

# Restore database
wifi-densepose db restore
```

#### Background Tasks
```bash
# Background task management commands
wifi-densepose tasks [SUBCOMMAND]

# Examples:
# List running tasks
wifi-densepose tasks list

# Start background tasks
wifi-densepose tasks start

# Stop background tasks
wifi-densepose tasks stop

# Check task status
wifi-densepose tasks status
```

### Command Examples

#### Complete CLI Reference
```bash
# Show help for main command
wifi-densepose --help

# Show help for specific command
wifi-densepose start --help
wifi-densepose config --help
wifi-densepose db --help

# Use global options with commands
wifi-densepose -v status          # Verbose status check
wifi-densepose --debug start      # Start with debug logging
wifi-densepose -c custom.yaml start  # Start with custom config
```

#### Common Usage Patterns
```bash
# Basic server lifecycle
wifi-densepose start              # Start the server
wifi-densepose status             # Check if running
wifi-densepose stop               # Stop the server

# Configuration management
wifi-densepose config show        # View current config
wifi-densepose config validate    # Check config validity

# Database operations
wifi-densepose db init            # Initialize database
wifi-densepose db migrate         # Run migrations
wifi-densepose db status          # Check database health

# Task management
wifi-densepose tasks list         # List background tasks
wifi-densepose tasks status       # Check task status

# Version and help
wifi-densepose version            # Show version info
wifi-densepose --help             # Show help message
```

### CLI Examples

#### Complete Setup Workflow
```bash
# 1. Check version and help
wifi-densepose version
wifi-densepose --help

# 2. Initialize configuration
wifi-densepose config init

# 3. Initialize database
wifi-densepose db init

# 4. Start the server
wifi-densepose start

# 5. Check status
wifi-densepose status
```

#### Development Workflow
```bash
# Start with debug logging
wifi-densepose --debug start

# Use custom configuration
wifi-densepose -c dev-config.yaml start

# Check database status
wifi-densepose db status

# Manage background tasks
wifi-densepose tasks start
wifi-densepose tasks list
```

#### Production Workflow
```bash
# Start with production config
wifi-densepose -c production.yaml start

# Check system status
wifi-densepose status

# Manage database
wifi-densepose db migrate
wifi-densepose db backup

# Monitor tasks
wifi-densepose tasks status
```

#### Troubleshooting
```bash
# Enable verbose logging
wifi-densepose -v status

# Check configuration
wifi-densepose config validate

# Check database health
wifi-densepose db status

# Restart services
wifi-densepose stop
wifi-densepose start
```

## ğŸ“š Documentation

Comprehensive documentation is available to help you get started and make the most of WiFi-DensePose:

### ğŸ“– Core Documentation

- **[User Guide](docs/user_guide.md)** - Complete guide covering installation, setup, basic usage, and examples
- **[API Reference](docs/api_reference.md)** - Detailed documentation of all public classes, methods, and endpoints
- **[Deployment Guide](docs/deployment.md)** - Production deployment, Docker setup, Kubernetes, and scaling strategies
- **[Troubleshooting Guide](docs/troubleshooting.md)** - Common issues, solutions, and diagnostic procedures

### ğŸš€ Quick Links

- **Interactive API Docs**: http://localhost:8000/docs (when running)
- **Health Check**: http://localhost:8000/api/v1/health
- **Latest Poses**: http://localhost:8000/api/v1/pose/latest
- **System Status**: http://localhost:8000/api/v1/system/status

### ğŸ“‹ API Overview

The system provides a comprehensive REST API and WebSocket streaming:

#### Key REST Endpoints
```bash
# Pose estimation
GET /api/v1/pose/latest          # Get latest pose data
GET /api/v1/pose/history         # Get historical data
GET /api/v1/pose/zones/{zone_id} # Get zone-specific data

# System management
GET /api/v1/system/status        # System health and status
POST /api/v1/system/calibrate    # Calibrate environment
GET /api/v1/analytics/summary    # Analytics dashboard data
```

#### WebSocket Streaming
```javascript
// Real-time pose data
ws://localhost:8000/ws/pose/stream

// Analytics events (falls, alerts)
ws://localhost:8000/ws/analytics/events

// System status updates
ws://localhost:8000/ws/system/status
```

#### Python SDK Quick Example
```python
from wifi_densepose import WiFiDensePoseClient

# Initialize client
client = WiFiDensePoseClient(base_url=&quot;http://localhost:8000&quot;)

# Get latest poses with confidence filtering
poses = client.get_latest_poses(min_confidence=0.7)
print(f&quot;Detected {len(poses)} persons&quot;)

# Get zone occupancy
occupancy = client.get_zone_occupancy(&quot;living_room&quot;)
print(f&quot;Living room occupancy: {occupancy.person_count}&quot;)
```

For complete API documentation with examples, see the [API Reference Guide](docs/api_reference.md).

## ğŸ”§ Hardware Setup

### Supported Hardware

WiFi DensePose works with standard WiFi equipment that supports CSI extraction:

#### Recommended Routers
- **ASUS AX6000** (RT-AX88U) - Excellent CSI quality
- **Netgear Nighthawk AX12** - High performance
- **TP-Link Archer AX73** - Budget-friendly option

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[muratcankoylan/Agent-Skills-for-Context-Engineering]]></title>
            <link>https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering</link>
            <guid>https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:15 GMT</pubDate>
            <description><![CDATA[A comprehensive collection of Agent Skills for context engineering, multi-agent architectures, and production agent systems. Use when building, optimizing, or debugging agent systems that require effective context management.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering">muratcankoylan/Agent-Skills-for-Context-Engineering</a></h1>
            <p>A comprehensive collection of Agent Skills for context engineering, multi-agent architectures, and production agent systems. Use when building, optimizing, or debugging agent systems that require effective context management.</p>
            <p>Language: Python</p>
            <p>Stars: 12,361</p>
            <p>Forks: 960</p>
            <p>Stars today: 803 stars today</p>
            <h2>README</h2><pre># Agent Skills for Context Engineering

A comprehensive, open collection of Agent Skills focused on context engineering principles for building production-grade AI agent systems. These skills teach the art and science of curating context to maximize agent effectiveness across any agent platform.

## What is Context Engineering?

Context engineering is the discipline of managing the language model&#039;s context window. Unlike prompt engineering, which focuses on crafting effective instructions, context engineering addresses the holistic curation of all information that enters the model&#039;s limited attention budget: system prompts, tool definitions, retrieved documents, message history, and tool outputs.

The fundamental challenge is that context windows are constrained not by raw token capacity but by attention mechanics. As context length increases, models exhibit predictable degradation patterns: the &quot;lost-in-the-middle&quot; phenomenon, U-shaped attention curves, and attention scarcity. Effective context engineering means finding the smallest possible set of high-signal tokens that maximize the likelihood of desired outcomes.

## Recognition

This repository is cited in academic research as foundational work on static skill architecture:

&gt; &quot;While static skills are well-recognized [Anthropic, 2025b; Muratcan Koylan, 2025], MCE is among the first to dynamically evolve them, bridging manual skill engineering and autonomous self-improvement.&quot;

â€” [Meta Context Engineering via Agentic Skill Evolution](https://arxiv.org/pdf/2601.21557), Peking University State Key Laboratory of General Artificial Intelligence (2026)

## Skills Overview

### Foundational Skills

These skills establish the foundational understanding required for all subsequent context engineering work.

| Skill | Description |
|-------|-------------|
| [context-fundamentals](skills/context-fundamentals/) | Understand what context is, why it matters, and the anatomy of context in agent systems |
| [context-degradation](skills/context-degradation/) | Recognize patterns of context failure: lost-in-middle, poisoning, distraction, and clash |
| [context-compression](skills/context-compression/) | Design and evaluate compression strategies for long-running sessions |

### Architectural Skills

These skills cover the patterns and structures for building effective agent systems.

| Skill | Description |
|-------|-------------|
| [multi-agent-patterns](skills/multi-agent-patterns/) | Master orchestrator, peer-to-peer, and hierarchical multi-agent architectures |
| [memory-systems](skills/memory-systems/) | Design short-term, long-term, and graph-based memory architectures |
| [tool-design](skills/tool-design/) | Build tools that agents can use effectively |
| [filesystem-context](skills/filesystem-context/) | Use filesystems for dynamic context discovery, tool output offloading, and plan persistence |
| [hosted-agents](skills/hosted-agents/) | **NEW** Build background coding agents with sandboxed VMs, pre-built images, multiplayer support, and multi-client interfaces |

### Operational Skills

These skills address the ongoing operation and optimization of agent systems.

| Skill | Description |
|-------|-------------|
| [context-optimization](skills/context-optimization/) | Apply compaction, masking, and caching strategies |
| [evaluation](skills/evaluation/) | Build evaluation frameworks for agent systems |
| [advanced-evaluation](skills/advanced-evaluation/) | Master LLM-as-a-Judge techniques: direct scoring, pairwise comparison, rubric generation, and bias mitigation |

### Development Methodology

These skills cover the meta-level practices for building LLM-powered projects.

| Skill | Description |
|-------|-------------|
| [project-development](skills/project-development/) | Design and build LLM projects from ideation through deployment, including task-model fit analysis, pipeline architecture, and structured output design |

### Cognitive Architecture Skills

These skills cover formal cognitive modeling for rational agent systems.

| Skill | Description |
|-------|-------------|
| [bdi-mental-states](skills/bdi-mental-states/) | **NEW** Transform external RDF context into agent mental states (beliefs, desires, intentions) using formal BDI ontology patterns for deliberative reasoning and explainability |

## Design Philosophy

### Progressive Disclosure

Each skill is structured for efficient context use. At startup, agents load only skill names and descriptions. Full content loads only when a skill is activated for relevant tasks.

### Platform Agnosticism

These skills focus on transferable principles rather than vendor-specific implementations. The patterns work across Claude Code, Cursor, and any agent platform that supports skills or allows custom instructions.

### Conceptual Foundation with Practical Examples

Scripts and examples demonstrate concepts using Python pseudocode that works across environments without requiring specific dependency installations.

## Usage

### Usage with Claude Code

This repository is a **Claude Code Plugin Marketplace** containing context engineering skills that Claude automatically discovers and activates based on your task context.

### Installation

**Step 1: Add the Marketplace**

Run this command in Claude Code to register this repository as a plugin source:

```
/plugin marketplace add muratcankoylan/Agent-Skills-for-Context-Engineering
```

**Step 2: Browse and Install**

Option A - Browse available plugins:
1. Select `Browse and install plugins`
2. Select `context-engineering-marketplace`
3. Choose a plugin (e.g., `context-engineering-fundamentals`, `agent-architecture`)
4. Select `Install now`

Option B - Direct install via command:

```
/plugin install context-engineering-fundamentals@context-engineering-marketplace
/plugin install agent-architecture@context-engineering-marketplace
/plugin install agent-evaluation@context-engineering-marketplace
/plugin install agent-development@context-engineering-marketplace
/plugin install cognitive-architecture@context-engineering-marketplace
```

### Available Plugins

| Plugin | Skills Included |
|--------|-----------------|
| `context-engineering-fundamentals` | context-fundamentals, context-degradation, context-compression, context-optimization |
| `agent-architecture` | multi-agent-patterns, memory-systems, tool-design, filesystem-context, hosted-agents |
| `agent-evaluation` | evaluation, advanced-evaluation |
| `agent-development` | project-development |
| `cognitive-architecture` | bdi-mental-states |

### Skill Triggers

| Skill | Triggers On |
|-------|-------------|
| `context-fundamentals` | &quot;understand context&quot;, &quot;explain context windows&quot;, &quot;design agent architecture&quot; |
| `context-degradation` | &quot;diagnose context problems&quot;, &quot;fix lost-in-middle&quot;, &quot;debug agent failures&quot; |
| `context-compression` | &quot;compress context&quot;, &quot;summarize conversation&quot;, &quot;reduce token usage&quot; |
| `context-optimization` | &quot;optimize context&quot;, &quot;reduce token costs&quot;, &quot;implement KV-cache&quot; |
| `multi-agent-patterns` | &quot;design multi-agent system&quot;, &quot;implement supervisor pattern&quot; |
| `memory-systems` | &quot;implement agent memory&quot;, &quot;build knowledge graph&quot;, &quot;track entities&quot; |
| `tool-design` | &quot;design agent tools&quot;, &quot;reduce tool complexity&quot;, &quot;implement MCP tools&quot; |
| `filesystem-context` | &quot;offload context to files&quot;, &quot;dynamic context discovery&quot;, &quot;agent scratch pad&quot;, &quot;file-based context&quot; |
| `hosted-agents` | &quot;build background agent&quot;, &quot;create hosted coding agent&quot;, &quot;sandboxed execution&quot;, &quot;multiplayer agent&quot;, &quot;Modal sandboxes&quot; |
| `evaluation` | &quot;evaluate agent performance&quot;, &quot;build test framework&quot;, &quot;measure quality&quot; |
| `advanced-evaluation` | &quot;implement LLM-as-judge&quot;, &quot;compare model outputs&quot;, &quot;mitigate bias&quot; |
| `project-development` | &quot;start LLM project&quot;, &quot;design batch pipeline&quot;, &quot;evaluate task-model fit&quot; |
| `bdi-mental-states` | &quot;model agent mental states&quot;, &quot;implement BDI architecture&quot;, &quot;transform RDF to beliefs&quot;, &quot;build cognitive agent&quot; |

&lt;img width=&quot;1014&quot; height=&quot;894&quot; alt=&quot;Screenshot 2025-12-26 at 12 34 47â€¯PM&quot; src=&quot;https://github.com/user-attachments/assets/f79aaf03-fd2d-4c71-a630-7027adeb9bfe&quot; /&gt;

### For Cursor &amp; Codex &amp; IDE

Copy skill content into `.rules` or create project-specific Skills folders. The skills provide the context and guidelines that agent needs for effective context engineering and agent design.

### For Custom Implementations

Extract the principles and patterns from any skill and implement them in your agent framework. The skills are deliberately platform-agnostic.

## Examples

The [examples](examples/) folder contains complete system designs that demonstrate how multiple skills work together in practice.

| Example | Description | Skills Applied |
|---------|-------------|----------------|
| [digital-brain-skill](examples/digital-brain-skill/) | **NEW** Personal operating system for founders and creators. Complete Claude Code skill with 6 modules, 4 automation scripts | context-fundamentals, context-optimization, memory-systems, tool-design, multi-agent-patterns, evaluation, project-development |
| [x-to-book-system](examples/x-to-book-system/) | Multi-agent system that monitors X accounts and generates daily synthesized books | multi-agent-patterns, memory-systems, context-optimization, tool-design, evaluation |
| [llm-as-judge-skills](examples/llm-as-judge-skills/) | Production-ready LLM evaluation tools with TypeScript implementation, 19 passing tests | advanced-evaluation, tool-design, context-fundamentals, evaluation |
| [book-sft-pipeline](examples/book-sft-pipeline/) | Train models to write in any author&#039;s style. Includes Gertrude Stein case study with 70% human score on Pangram, $2 total cost | project-development, context-compression, multi-agent-patterns, evaluation |

Each example includes:
- Complete PRD with architecture decisions
- Skills mapping showing which concepts informed each decision
- Implementation guidance

### Digital Brain Skill Example

The [digital-brain-skill](examples/digital-brain-skill/) example is a complete personal operating system demonstrating comprehensive skills application:

- **Progressive Disclosure**: 3-level loading (SKILL.md â†’ MODULE.md â†’ data files)
- **Module Isolation**: 6 independent modules (identity, content, knowledge, network, operations, agents)
- **Append-Only Memory**: JSONL files with schema-first lines for agent-friendly parsing
- **Automation Scripts**: 4 consolidated tools (weekly_review, content_ideas, stale_contacts, idea_to_draft)

Includes detailed traceability in [HOW-SKILLS-BUILT-THIS.md](examples/digital-brain-skill/HOW-SKILLS-BUILT-THIS.md) mapping every architectural decision to specific skill principles.

### LLM-as-Judge Skills Example

The [llm-as-judge-skills](examples/llm-as-judge-skills/) example is a complete TypeScript implementation demonstrating:

- **Direct Scoring**: Evaluate responses against weighted criteria with rubric support
- **Pairwise Comparison**: Compare responses with position bias mitigation
- **Rubric Generation**: Create domain-specific evaluation standards
- **EvaluatorAgent**: High-level agent combining all evaluation capabilities

### Book SFT Pipeline Example

The [book-sft-pipeline](examples/book-sft-pipeline/) example demonstrates training small models (8B) to write in any author&#039;s style:

- **Intelligent Segmentation**: Two-tier chunking with overlap for maximum training examples
- **Prompt Diversity**: 15+ templates to prevent memorization and force style learning
- **Tinker Integration**: Complete LoRA training workflow with $2 total cost
- **Validation Methodology**: Modern scenario testing proves style transfer vs content memorization

Integrates with context engineering skills: project-development, context-compression, multi-agent-patterns, evaluation.

## Star History
&lt;img width=&quot;3664&quot; height=&quot;2648&quot; alt=&quot;star-history-2026224&quot; src=&quot;https://github.com/user-attachments/assets/b3bdbf23-4b6a-4774-ae85-42ef4d9b2d79&quot; /&gt;

## Structure

Each skill follows the Agent Skills specification:

```
skill-name/
â”œâ”€â”€ SKILL.md              # Required: instructions + metadata
â”œâ”€â”€ scripts/              # Optional: executable code demonstrating concepts
â””â”€â”€ references/           # Optional: additional documentation and resources
```

See the [template](template/) folder for the canonical skill structure.

## Contributing

This repository follows the Agent Skills open development model. Contributions are welcome from the broader ecosystem. When contributing:

1. Follow the skill template structure
2. Provide clear, actionable instructions
3. Include working examples where appropriate
4. Document trade-offs and potential issues
5. Keep SKILL.md under 500 lines for optimal performance

Feel free to contact [Muratcan Koylan](https://x.com/koylanai) for collaboration opportunities or any inquiries.

## License

MIT License - see LICENSE file for details.

## References

The principles in these skills are derived from research and production experience at leading AI labs and framework developers. Each skill includes references to the underlying research and case studies that inform its recommendations.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[datawhalechina/hello-agents]]></title>
            <link>https://github.com/datawhalechina/hello-agents</link>
            <guid>https://github.com/datawhalechina/hello-agents</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:14 GMT</pubDate>
            <description><![CDATA[ğŸ“š ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹â€”â€”ä»é›¶å¼€å§‹çš„æ™ºèƒ½ä½“åŸç†ä¸å®è·µæ•™ç¨‹]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/datawhalechina/hello-agents">datawhalechina/hello-agents</a></h1>
            <p>ğŸ“š ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹â€”â€”ä»é›¶å¼€å§‹çš„æ™ºèƒ½ä½“åŸç†ä¸å®è·µæ•™ç¨‹</p>
            <p>Language: Python</p>
            <p>Stars: 22,995</p>
            <p>Forks: 2,620</p>
            <p>Stars today: 324 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;right&quot;&gt;
  &lt;a href=&quot;./README_EN.md&quot;&gt;English&lt;/a&gt; | ä¸­æ–‡
&lt;/div&gt;

&lt;div align=&#039;center&#039;&gt;
  &lt;img src=&quot;./docs/images/hello-agents.png&quot; alt=&quot;alt text&quot; width=&quot;100%&quot;&gt;
  &lt;h1&gt;Hello-Agents&lt;/h1&gt;
  &lt;h3&gt;ğŸ¤– ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹&lt;/h3&gt;
  &lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/15520&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/15520&quot; alt=&quot;datawhalechina%2Fhello-agents | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
  &lt;/a&gt;
  &lt;/div&gt;
  &lt;p&gt;&lt;em&gt;ä»åŸºç¡€ç†è®ºåˆ°å®é™…åº”ç”¨ï¼Œå…¨é¢æŒæ¡æ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡ä¸å®ç°&lt;/em&gt;&lt;/p&gt;
  &lt;img src=&quot;https://img.shields.io/github/stars/datawhalechina/Hello-Agents?style=flat&amp;logo=github&quot; alt=&quot;GitHub stars&quot;/&gt;
  &lt;img src=&quot;https://img.shields.io/github/forks/datawhalechina/Hello-Agents?style=flat&amp;logo=github&quot; alt=&quot;GitHub forks&quot;/&gt;
  &lt;img src=&quot;https://img.shields.io/badge/language-Chinese-brightgreen?style=flat&quot; alt=&quot;Language&quot;/&gt;
  &lt;a href=&quot;https://github.com/datawhalechina/Hello-Agents&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-Project-blue?style=flat&amp;logo=github&quot; alt=&quot;GitHub Project&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://datawhalechina.github.io/hello-agents/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/åœ¨çº¿é˜…è¯»-Online%20Reading-green?style=flat&amp;logo=gitbook&quot; alt=&quot;Online Reading&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

---

## ğŸ¯ é¡¹ç›®ä»‹ç»

&amp;emsp;&amp;emsp;å¦‚æœè¯´ 2024 å¹´æ˜¯&quot;ç™¾æ¨¡å¤§æˆ˜&quot;çš„å…ƒå¹´ï¼Œé‚£ä¹ˆ 2025 å¹´æ— ç–‘å¼€å¯äº†&quot;Agent å…ƒå¹´&quot;ã€‚æŠ€æœ¯çš„ç„¦ç‚¹æ­£ä»è®­ç»ƒæ›´å¤§çš„åŸºç¡€æ¨¡å‹ï¼Œè½¬å‘æ„å»ºæ›´èªæ˜çš„æ™ºèƒ½ä½“åº”ç”¨ã€‚ç„¶è€Œï¼Œå½“å‰ç³»ç»Ÿæ€§ã€é‡å®è·µçš„æ•™ç¨‹å´æåº¦åŒ®ä¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å‘èµ·äº† Hello-Agents é¡¹ç›®ï¼Œå¸Œæœ›èƒ½ä¸ºç¤¾åŒºæä¾›ä¸€æœ¬ä»é›¶å¼€å§‹ã€ç†è®ºä¸å®æˆ˜å¹¶é‡çš„æ™ºèƒ½ä½“ç³»ç»Ÿæ„å»ºæŒ‡å—ã€‚

&amp;emsp;&amp;emsp;Hello-Agents æ˜¯ Datawhale ç¤¾åŒºçš„&lt;strong&gt;ç³»ç»Ÿæ€§æ™ºèƒ½ä½“å­¦ä¹ æ•™ç¨‹&lt;/strong&gt;ã€‚å¦‚ä»Š Agent æ„å»ºä¸»è¦åˆ†ä¸ºä¸¤æ´¾ï¼Œä¸€æ´¾æ˜¯ Difyï¼ŒCozeï¼Œn8n è¿™ç±»è½¯ä»¶å·¥ç¨‹ç±» Agentï¼Œå…¶æœ¬è´¨æ˜¯æµç¨‹é©±åŠ¨çš„è½¯ä»¶å¼€å‘ï¼ŒLLM ä½œä¸ºæ•°æ®å¤„ç†çš„åç«¯ï¼›å¦ä¸€æ´¾åˆ™æ˜¯ AI åŸç”Ÿçš„ Agentï¼Œå³çœŸæ­£ä»¥ AI é©±åŠ¨çš„ Agentã€‚æœ¬æ•™ç¨‹æ—¨åœ¨å¸¦é¢†å¤§å®¶æ·±å…¥ç†è§£å¹¶æ„å»ºåè€…â€”â€”çœŸæ­£çš„ AI Native Agentã€‚æ•™ç¨‹å°†å¸¦é¢†ä½ ç©¿é€æ¡†æ¶è¡¨è±¡ï¼Œä»æ™ºèƒ½ä½“çš„æ ¸å¿ƒåŸç†å‡ºå‘ï¼Œæ·±å…¥å…¶æ ¸å¿ƒæ¶æ„ï¼Œç†è§£å…¶ç»å…¸èŒƒå¼ï¼Œå¹¶æœ€ç»ˆäº²æ‰‹æ„å»ºèµ·å±äºè‡ªå·±çš„å¤šæ™ºèƒ½ä½“åº”ç”¨ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæœ€å¥½çš„å­¦ä¹ æ–¹å¼å°±æ˜¯åŠ¨æ‰‹å®è·µã€‚å¸Œæœ›è¿™æœ¬æ•™ç¨‹èƒ½æˆä¸ºä½ æ¢ç´¢æ™ºèƒ½ä½“ä¸–ç•Œçš„èµ·ç‚¹ï¼Œèƒ½å¤Ÿä»ä¸€åå¤§è¯­è¨€æ¨¡å‹çš„&quot;ä½¿ç”¨è€…&quot;ï¼Œèœ•å˜ä¸ºä¸€åæ™ºèƒ½ä½“ç³»ç»Ÿçš„&quot;æ„å»ºè€…&quot;ã€‚

## ğŸ“š å¿«é€Ÿå¼€å§‹

### åœ¨çº¿é˜…è¯»
**[ğŸŒ ç‚¹å‡»è¿™é‡Œå¼€å§‹åœ¨çº¿é˜…è¯»](https://datawhalechina.github.io/hello-agents/)** - æ— éœ€ä¸‹è½½ï¼Œéšæ—¶éšåœ°å­¦ä¹ 

**[ğŸ“– Cookbook](https://book.heterocat.com.cn/)**

### æœ¬åœ°é˜…è¯»
å¦‚æœæ‚¨å¸Œæœ›åœ¨æœ¬åœ°é˜…è¯»æˆ–è´¡çŒ®å†…å®¹ï¼Œè¯·å‚è€ƒä¸‹æ–¹çš„å­¦ä¹ æŒ‡å—ã€‚

### âœ¨ ä½ å°†æ”¶è·ä»€ä¹ˆï¼Ÿ

- ğŸ“– &lt;strong&gt;Datawhale å¼€æºå…è´¹&lt;/strong&gt; å®Œå…¨å…è´¹å­¦ä¹ æœ¬é¡¹ç›®æ‰€æœ‰å†…å®¹ï¼Œä¸ç¤¾åŒºå…±åŒæˆé•¿
- ğŸ” &lt;strong&gt;ç†è§£æ ¸å¿ƒåŸç†&lt;/strong&gt; æ·±å…¥ç†è§£æ™ºèƒ½ä½“çš„æ¦‚å¿µã€å†å²ä¸ç»å…¸èŒƒå¼
- ğŸ—ï¸ &lt;strong&gt;äº²æ‰‹å®ç°&lt;/strong&gt; æŒæ¡çƒ­é—¨ä½ä»£ç å¹³å°å’Œæ™ºèƒ½ä½“ä»£ç æ¡†æ¶çš„ä½¿ç”¨
- ğŸ› ï¸ &lt;strong&gt;è‡ªç ”æ¡†æ¶[HelloAgents](https://github.com/jjyaoao/helloagents)&lt;/strong&gt; åŸºäº Openai åŸç”Ÿ API ä»é›¶æ„å»ºä¸€ä¸ªè‡ªå·±çš„æ™ºèƒ½ä½“æ¡†æ¶
- âš™ï¸ &lt;strong&gt;æŒæ¡é«˜çº§æŠ€èƒ½&lt;/strong&gt; ä¸€æ­¥æ­¥å®ç°ä¸Šä¸‹æ–‡å·¥ç¨‹ã€Memoryã€åè®®ã€è¯„ä¼°ç­‰ç³»ç»Ÿæ€§æŠ€æœ¯
- ğŸ¤ &lt;strong&gt;æ¨¡å‹è®­ç»ƒ&lt;/strong&gt; æŒæ¡ Agentic RLï¼Œä» SFT åˆ° GRPO çš„å…¨æµç¨‹å®æˆ˜è®­ç»ƒ LLM
- ğŸš€ &lt;strong&gt;é©±åŠ¨çœŸå®æ¡ˆä¾‹&lt;/strong&gt; å®æˆ˜å¼€å‘æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹ã€èµ›åšå°é•‡ç­‰ç»¼åˆé¡¹ç›®
- ğŸ“– &lt;strong&gt;æ±‚èŒé¢è¯•&lt;/strong&gt; å­¦ä¹ æ™ºèƒ½ä½“æ±‚èŒç›¸å…³é¢è¯•é—®é¢˜

## ğŸ“– å†…å®¹å¯¼èˆª

| ç« èŠ‚                                                                                        | å…³é”®å†…å®¹                                      | çŠ¶æ€ |
| ------------------------------------------------------------------------------------------- | --------------------------------------------- | ---- |
| [å‰è¨€](./docs/å‰è¨€.md)                                                                      | é¡¹ç›®çš„ç¼˜èµ·ã€èƒŒæ™¯åŠè¯»è€…å»ºè®®                    | âœ…    |
| &lt;strong&gt;ç¬¬ä¸€éƒ¨åˆ†ï¼šæ™ºèƒ½ä½“ä¸è¯­è¨€æ¨¡å‹åŸºç¡€&lt;/strong&gt;                                             |                                               |      |
| [ç¬¬ä¸€ç«  åˆè¯†æ™ºèƒ½ä½“](./docs/chapter1/ç¬¬ä¸€ç« %20åˆè¯†æ™ºèƒ½ä½“.md)                                 | æ™ºèƒ½ä½“å®šä¹‰ã€ç±»å‹ã€èŒƒå¼ä¸åº”ç”¨                  | âœ…    |
| [ç¬¬äºŒç«  æ™ºèƒ½ä½“å‘å±•å²](./docs/chapter2/ç¬¬äºŒç« %20æ™ºèƒ½ä½“å‘å±•å².md)                             | ä»ç¬¦å·ä¸»ä¹‰åˆ° LLM é©±åŠ¨çš„æ™ºèƒ½ä½“æ¼”è¿›             | âœ…    |
| [ç¬¬ä¸‰ç«  å¤§è¯­è¨€æ¨¡å‹åŸºç¡€](./docs/chapter3/ç¬¬ä¸‰ç« %20å¤§è¯­è¨€æ¨¡å‹åŸºç¡€.md)                         | Transformerã€æç¤ºã€ä¸»æµ LLM åŠå…¶å±€é™          | âœ…    |
| &lt;strong&gt;ç¬¬äºŒéƒ¨åˆ†ï¼šæ„å»ºä½ çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“&lt;/strong&gt;                                         |                                               |      |
| [ç¬¬å››ç«  æ™ºèƒ½ä½“ç»å…¸èŒƒå¼æ„å»º](./docs/chapter4/ç¬¬å››ç« %20æ™ºèƒ½ä½“ç»å…¸èŒƒå¼æ„å»º.md)                 | æ‰‹æŠŠæ‰‹å®ç° ReActã€Plan-and-Solveã€Reflection  | âœ…    |
| [ç¬¬äº”ç«  åŸºäºä½ä»£ç å¹³å°çš„æ™ºèƒ½ä½“æ­å»º](./docs/chapter5/ç¬¬äº”ç« %20åŸºäºä½ä»£ç å¹³å°çš„æ™ºèƒ½ä½“æ­å»º.md) | äº†è§£ Cozeã€Difyã€n8n ç­‰ä½ä»£ç æ™ºèƒ½ä½“å¹³å°ä½¿ç”¨   | âœ…    |
| [ç¬¬å…­ç«  æ¡†æ¶å¼€å‘å®è·µ](./docs/chapter6/ç¬¬å…­ç« %20æ¡†æ¶å¼€å‘å®è·µ.md)                             | AutoGenã€AgentScopeã€LangGraph ç­‰ä¸»æµæ¡†æ¶åº”ç”¨ | âœ…    |
| [ç¬¬ä¸ƒç«  æ„å»ºä½ çš„Agentæ¡†æ¶](./docs/chapter7/ç¬¬ä¸ƒç« %20æ„å»ºä½ çš„Agentæ¡†æ¶.md)                   | ä» 0 å¼€å§‹æ„å»ºæ™ºèƒ½ä½“æ¡†æ¶                       | âœ…    |
| &lt;strong&gt;ç¬¬ä¸‰éƒ¨åˆ†ï¼šé«˜çº§çŸ¥è¯†æ‰©å±•&lt;/strong&gt;                                                     |                                               |      |
| [ç¬¬å…«ç«  è®°å¿†ä¸æ£€ç´¢](./docs/chapter8/ç¬¬å…«ç« %20è®°å¿†ä¸æ£€ç´¢.md)                                 | è®°å¿†ç³»ç»Ÿï¼ŒRAGï¼Œå­˜å‚¨                           | âœ…    |
| [ç¬¬ä¹ç«  ä¸Šä¸‹æ–‡å·¥ç¨‹](./docs/chapter9/ç¬¬ä¹ç« %20ä¸Šä¸‹æ–‡å·¥ç¨‹.md)                                 | æŒç»­äº¤äº’çš„&quot;æƒ…å¢ƒç†è§£&quot;                          | âœ…    |
| [ç¬¬åç«  æ™ºèƒ½ä½“é€šä¿¡åè®®](./docs/chapter10/ç¬¬åç« %20æ™ºèƒ½ä½“é€šä¿¡åè®®.md)                        | MCPã€A2Aã€ANP ç­‰åè®®è§£æ                      | âœ…    |
| [ç¬¬åä¸€ç«  Agentic-RL](./docs/chapter11/ç¬¬åä¸€ç« %20Agentic-RL.md)                            | ä» SFT åˆ° GRPO çš„ LLM è®­ç»ƒå®æˆ˜                | âœ…    |
| [ç¬¬åäºŒç«  æ™ºèƒ½ä½“æ€§èƒ½è¯„ä¼°](./docs/chapter12/ç¬¬åäºŒç« %20æ™ºèƒ½ä½“æ€§èƒ½è¯„ä¼°.md)                    | æ ¸å¿ƒæŒ‡æ ‡ã€åŸºå‡†æµ‹è¯•ä¸è¯„ä¼°æ¡†æ¶                  | âœ…    |
| &lt;strong&gt;ç¬¬å››éƒ¨åˆ†ï¼šç»¼åˆæ¡ˆä¾‹è¿›é˜¶&lt;/strong&gt;                                                     |                                               |      |
| [ç¬¬åä¸‰ç«  æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹](./docs/chapter13/ç¬¬åä¸‰ç« %20æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹.md)                        | MCP ä¸å¤šæ™ºèƒ½ä½“åä½œçš„çœŸå®ä¸–ç•Œåº”ç”¨              | âœ…    |
| [ç¬¬åå››ç«  è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“](./docs/chapter14/ç¬¬åå››ç« %20è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“.md)        | DeepResearch Agent å¤ç°ä¸è§£æ                 | âœ…    |
| [ç¬¬åäº”ç«  æ„å»ºèµ›åšå°é•‡](./docs/chapter15/ç¬¬åäº”ç« %20æ„å»ºèµ›åšå°é•‡.md)                        | Agent ä¸æ¸¸æˆçš„ç»“åˆï¼Œæ¨¡æ‹Ÿç¤¾ä¼šåŠ¨æ€              | âœ…    |
| &lt;strong&gt;ç¬¬äº”éƒ¨åˆ†ï¼šæ¯•ä¸šè®¾è®¡åŠæœªæ¥å±•æœ›&lt;/strong&gt;                                               |                                               |      |
| [ç¬¬åå…­ç«  æ¯•ä¸šè®¾è®¡](./docs/chapter16/ç¬¬åå…­ç« %20æ¯•ä¸šè®¾è®¡.md)                                | æ„å»ºå±äºä½ çš„å®Œæ•´å¤šæ™ºèƒ½ä½“åº”ç”¨                  | âœ…    |

### ç¤¾åŒºè´¡çŒ®ç²¾é€‰ (Community Blog)

&amp;emsp;&amp;emsp;æ¬¢è¿å¤§å®¶å°†åœ¨å­¦ä¹  Hello-Agents æˆ– Agent ç›¸å…³æŠ€æœ¯ä¸­çš„ç‹¬åˆ°è§è§£ã€å®è·µæ€»ç»“ï¼Œä»¥ PR çš„å½¢å¼è´¡çŒ®åˆ°ç¤¾åŒºç²¾é€‰ã€‚å¦‚æœæ˜¯ç‹¬ç«‹äºæ­£æ–‡çš„å†…å®¹ï¼Œä¹Ÿå¯ä»¥æŠ•ç¨¿è‡³ Extra-Chapterï¼&lt;strong&gt;æœŸå¾…ä½ çš„ç¬¬ä¸€æ¬¡è´¡çŒ®ï¼&lt;/strong&gt;

| ç¤¾åŒºç²¾é€‰                                                                                                                                      | å†…å®¹æ€»ç»“                  |
| --------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------- |
| [00-å…±åˆ›æ¯•ä¸šè®¾è®¡](https://github.com/datawhalechina/hello-agents/blob/main/Co-creation-projects)                                             | ç¤¾åŒºå…±åˆ›æ¯•ä¸šè®¾è®¡é¡¹ç›®      |
| [01-Agenté¢è¯•é¢˜æ€»ç»“](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra01-é¢è¯•é—®é¢˜æ€»ç»“.md)                          | Agent å²—ä½ç›¸å…³é¢è¯•é—®é¢˜    |
| [01-Agenté¢è¯•é¢˜ç­”æ¡ˆ](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra01-å‚è€ƒç­”æ¡ˆ.md)                              | ç›¸å…³é¢è¯•é—®é¢˜ç­”æ¡ˆ          |
| [02-ä¸Šä¸‹æ–‡å·¥ç¨‹å†…å®¹è¡¥å……](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra02-ä¸Šä¸‹æ–‡å·¥ç¨‹è¡¥å……çŸ¥è¯†.md)                 | ä¸Šä¸‹æ–‡å·¥ç¨‹å†…å®¹æ‰©å±•        |
| [03-Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ•™ç¨‹](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra03-Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ“ä½œæµç¨‹.md) | Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ•™ç¨‹  |
| [04-Hello-agentsè¯¾ç¨‹å¸¸è§é—®é¢˜](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra04-DatawhaleFAQ.md)                 | Datawhaleè¯¾ç¨‹å¸¸è§é—®é¢˜     |
| [05-Agent Skillsä¸MCPå¯¹æ¯”è§£è¯»](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra05-AgentSkillsè§£è¯».md)             | Agent Skillsä¸MCPæŠ€æœ¯å¯¹æ¯” |
| [06-GUI Agentç§‘æ™®ä¸å®æˆ˜](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra06-GUIAgentç§‘æ™®ä¸å®æˆ˜.md)                | GUI Agentç§‘æ™®ä¸å¤šåœºæ™¯å®æˆ˜ |
| [07-ç¯å¢ƒé…ç½®](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra07-ç¯å¢ƒé…ç½®.md)                | ç¯å¢ƒé…ç½® |
| [08-å¦‚ä½•å†™å‡ºå¥½çš„Skill](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra08-å¦‚ä½•å†™å‡ºå¥½çš„Skill.md) | Skill å†™ä½œæœ€ä½³å®è·µ |
| [09-Agentåº”ç”¨å¼€å‘å®è·µè¸©å‘ä¸ç»éªŒåˆ†äº«](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra09-Agentåº”ç”¨å¼€å‘å®è·µè¸©å‘ä¸ç»éªŒåˆ†äº«.md) | Code Agent åº”ç”¨å¼€å‘è¸©å‘ä¸ç»éªŒæ€»ç»“ |

### PDF ç‰ˆæœ¬ä¸‹è½½

&amp;emsp;&amp;emsp;*&lt;strong&gt;æœ¬ Hello-Agents PDF æ•™ç¨‹å®Œå…¨å¼€æºå…è´¹ã€‚ä¸ºé˜²æ­¢å„ç±»è¥é”€å·åŠ æ°´å°åè´©å–ç»™å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåˆå­¦è€…ï¼Œæˆ‘ä»¬ç‰¹åœ°åœ¨ PDF æ–‡ä»¶ä¸­é¢„å…ˆæ·»åŠ äº†ä¸å½±å“é˜…è¯»çš„ Datawhale å¼€æºæ ‡å¿—æ°´å°ï¼Œæ•¬è¯·è°…è§£ï½&lt;/strong&gt;*

&gt; *Hello-Agents PDF : https://github.com/datawhalechina/hello-agents/releases/tag/V1.0.0*  
&gt; *Hello-Agents PDF å›½å†…ä¸‹è½½åœ°å€ : https://www.datawhale.cn/learn/summary/239* 

## ğŸ’¡ å¦‚ä½•å­¦ä¹ 

&amp;emsp;&amp;emsp;æ¬¢è¿ä½ ï¼Œæœªæ¥çš„æ™ºèƒ½ç³»ç»Ÿæ„å»ºè€…ï¼åœ¨å¼€å¯è¿™æ®µæ¿€åŠ¨äººå¿ƒçš„æ—…ç¨‹ä¹‹å‰ï¼Œè¯·å…è®¸æˆ‘ä»¬ç»™ä½ ä¸€äº›æ¸…æ™°çš„æŒ‡å¼•ã€‚

&amp;emsp;&amp;emsp;æœ¬é¡¹ç›®å†…å®¹å…¼é¡¾ç†è®ºä¸å®æˆ˜ï¼Œæ—¨åœ¨å¸®åŠ©ä½ ç³»ç»Ÿæ€§åœ°æŒæ¡ä»å•ä¸ªæ™ºèƒ½ä½“åˆ°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡ä¸å¼€å‘å…¨æµç¨‹ã€‚å› æ­¤ï¼Œå°¤å…¶é€‚åˆæœ‰ä¸€å®šç¼–ç¨‹åŸºç¡€çš„ &lt;strong&gt;AI å¼€å‘è€…ã€è½¯ä»¶å·¥ç¨‹å¸ˆã€åœ¨æ ¡å­¦ç”Ÿ&lt;/strong&gt; ä»¥åŠå¯¹å‰æ²¿ AI æŠ€æœ¯æŠ±æœ‰æµ“åšå…´è¶£çš„ &lt;strong&gt;è‡ªå­¦è€…&lt;/strong&gt;ã€‚åœ¨å­¦ä¹ æœ¬é¡¹ç›®ä¹‹å‰ï¼Œæˆ‘ä»¬å¸Œæœ›ä½ å…·å¤‡åŸºç¡€çš„ Python ç¼–ç¨‹èƒ½åŠ›ï¼Œå¹¶å¯¹å¤§è¯­è¨€æ¨¡å‹æœ‰åŸºæœ¬çš„æ¦‚å¿µæ€§äº†è§£ï¼ˆä¾‹å¦‚ï¼ŒçŸ¥é“å¦‚ä½•é€šè¿‡ API è°ƒç”¨ä¸€ä¸ª LLMï¼‰ã€‚é¡¹ç›®çš„é‡ç‚¹æ˜¯åº”ç”¨ä¸æ„å»ºï¼Œå› æ­¤ä½ æ— éœ€å…·å¤‡æ·±åšçš„ç®—æ³•æˆ–æ¨¡å‹è®­ç»ƒèƒŒæ™¯ã€‚

&amp;emsp;&amp;emsp;é¡¹ç›®åˆ†ä¸ºäº”å¤§éƒ¨åˆ†ï¼Œæ¯ä¸€éƒ¨åˆ†éƒ½æ˜¯é€šå¾€ä¸‹ä¸€é˜¶æ®µçš„åšå®é˜¶æ¢¯ï¼š

- &lt;strong&gt;ç¬¬ä¸€éƒ¨åˆ†ï¼šæ™ºèƒ½ä½“ä¸è¯­è¨€æ¨¡å‹åŸºç¡€&lt;/strong&gt;ï¼ˆç¬¬ä¸€ç« ï½ç¬¬ä¸‰ç« ï¼‰ï¼Œæˆ‘ä»¬å°†ä»æ™ºèƒ½ä½“çš„å®šä¹‰ã€ç±»å‹ä¸å‘å±•å†å²è®²èµ·ï¼Œä¸ºä½ æ¢³ç†&quot;æ™ºèƒ½ä½“&quot;è¿™ä¸€æ¦‚å¿µçš„æ¥é¾™å»è„‰ã€‚éšåï¼Œæˆ‘ä»¬ä¼šå¿«é€Ÿå·©å›ºå¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œä¸ºä½ çš„å®è·µä¹‹æ—…æ‰“ä¸‹åšå®çš„ç†è®ºåœ°åŸºã€‚

- &lt;strong&gt;ç¬¬äºŒéƒ¨åˆ†ï¼šæ„å»ºä½ çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“&lt;/strong&gt;ï¼ˆç¬¬å››ç« ï½ç¬¬ä¸ƒç« ï¼‰ï¼Œè¿™æ˜¯ä½ åŠ¨æ‰‹å®è·µçš„èµ·ç‚¹ã€‚ä½ å°†äº²æ‰‹å®ç° ReAct ç­‰ç»å…¸èŒƒå¼ï¼Œä½“éªŒ Coze ç­‰ä½ä»£ç å¹³å°çš„ä¾¿æ·ï¼Œå¹¶æŒæ¡ Langgraph ç­‰ä¸»æµæ¡†æ¶çš„åº”ç”¨ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è¿˜ä¼šå¸¦ä½ ä»é›¶å¼€å§‹æ„å»ºä¸€ä¸ªå±äºè‡ªå·±çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œè®©ä½ å…¼å…·â€œç”¨è½®å­â€ä¸â€œé€ è½®å­â€çš„èƒ½åŠ›ã€‚

- &lt;strong&gt;ç¬¬ä¸‰éƒ¨åˆ†ï¼šé«˜çº§çŸ¥è¯†æ‰©å±•&lt;/strong&gt;ï¼ˆç¬¬å…«ç« ï½ç¬¬åäºŒç« ï¼‰ï¼Œåœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œä½ çš„æ™ºèƒ½ä½“å°†â€œå­¦ä¼šâ€æ€è€ƒä¸åä½œã€‚æˆ‘ä»¬å°†ä½¿ç”¨ç¬¬äºŒéƒ¨åˆ†çš„è‡ªç ”æ¡†æ¶ï¼Œæ·±å…¥æ¢ç´¢è®°å¿†ä¸æ£€ç´¢ã€ä¸Šä¸‹æ–‡å·¥ç¨‹ã€Agent è®­ç»ƒç­‰æ ¸å¿ƒæŠ€æœ¯ï¼Œå¹¶å­¦ä¹ å¤šæ™ºèƒ½ä½“é—´çš„é€šä¿¡åè®®ã€‚æœ€ç»ˆï¼Œä½ å°†æŒæ¡è¯„ä¼°æ™ºèƒ½ä½“ç³»ç»Ÿæ€§èƒ½çš„ä¸“ä¸šæ–¹æ³•ã€‚

- &lt;strong&gt;ç¬¬å››éƒ¨åˆ†ï¼šç»¼åˆæ¡ˆä¾‹è¿›é˜¶&lt;/strong&gt;ï¼ˆç¬¬åä¸‰ç« ï½ç¬¬åäº”ç« ï¼‰ï¼Œè¿™é‡Œæ˜¯ç†è®ºä¸å®è·µçš„äº¤æ±‡ç‚¹ã€‚ä½ å°†æŠŠæ‰€å­¦èä¼šè´¯é€šï¼Œäº²æ‰‹æ‰“é€ æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹ã€è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“ï¼Œä¹ƒè‡³ä¸€ä¸ªæ¨¡æ‹Ÿç¤¾ä¼šåŠ¨æ€çš„èµ›åšå°é•‡ï¼Œåœ¨çœŸå®æœ‰è¶£çš„é¡¹ç›®ä¸­æ·¬ç‚¼ä½ çš„æ„å»ºèƒ½åŠ›ã€‚

- &lt;strong&gt;ç¬¬äº”éƒ¨åˆ†ï¼šæ¯•ä¸šè®¾è®¡åŠæœªæ¥å±•æœ›&lt;/strong&gt;ï¼ˆç¬¬åå…­ç« ï¼‰ï¼Œåœ¨æ—…ç¨‹çš„ç»ˆç‚¹ï¼Œä½ å°†è¿æ¥ä¸€ä¸ªæ¯•ä¸šè®¾è®¡ï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„ã€å±äºä½ è‡ªå·±çš„å¤šæ™ºèƒ½ä½“åº”ç”¨ï¼Œå…¨é¢æ£€éªŒä½ çš„å­¦ä¹ æˆæœã€‚æˆ‘ä»¬è¿˜å°†ä¸ä½ ä¸€åŒå±•æœ›æ™ºèƒ½ä½“çš„æœªæ¥ï¼Œæ¢ç´¢æ¿€åŠ¨äººå¿ƒçš„å‰æ²¿æ–¹å‘ã€‚


&amp;emsp;&amp;emsp;æ™ºèƒ½ä½“æ˜¯ä¸€ä¸ªé£é€Ÿå‘å±•ä¸”æåº¦ä¾èµ–å®è·µçš„é¢†åŸŸã€‚ä¸ºäº†è·å¾—æœ€ä½³çš„å­¦ä¹ æ•ˆæœï¼Œæˆ‘ä»¬åœ¨é¡¹ç›®çš„`code`æ–‡ä»¶å¤¹å†…æä¾›äº†é…å¥—çš„å…¨éƒ¨ä»£ç ï¼Œå¼ºçƒˆå»ºè®®ä½ &lt;strong&gt;å°†ç†è®ºä¸å®è·µç›¸ç»“åˆ&lt;/strong&gt;ã€‚è¯·åŠ¡å¿…äº²æ‰‹è¿è¡Œã€è°ƒè¯•ç”šè‡³ä¿®æ”¹é¡¹ç›®é‡Œæä¾›çš„æ¯ä¸€ä»½ä»£ç ã€‚æ¬¢è¿ä½ éšæ—¶å…³æ³¨ Datawhale ä»¥åŠå…¶ä»– Agent ç›¸å…³ç¤¾åŒºï¼Œå½“é‡åˆ°é—®é¢˜æ—¶ï¼Œä½ å¯ä»¥éšæ—¶åœ¨æœ¬é¡¹ç›®çš„ issue åŒºæé—®ã€‚

&amp;emsp;&amp;emsp;ç°åœ¨ï¼Œå‡†å¤‡å¥½è¿›å…¥æ™ºèƒ½ä½“çš„å¥‡å¦™ä¸–ç•Œäº†å—ï¼Ÿè®©æˆ‘ä»¬å³åˆ»å¯ç¨‹ï¼

## ä¸‹ä¸€æ­¥è§„åˆ’

- è§†é¢‘è¯¾ç¨‹é™†ç»­æ”¾å‡ºï¼ˆå°†ä¼šæ›´åŠ ç»†è‡´ï¼Œå®è·µè¯¾å¸¦é¢†å¤§å®¶ä»è®¾è®¡æ€è·¯åˆ°å®æ–½ï¼Œæˆäººä»¥é±¼ä¹Ÿæˆäººä»¥æ¸”ï¼‰
- å®Œå–„HelloAgentsæ¡†æ¶ï¼Œå¼€å±•Devåˆ†æ”¯ç»§ç»­ç»´æŠ¤ï¼Œå…¼å®¹å­¦ä¹ ç‰ˆæœ¬ã€‚
- æ„Ÿè°¢å¤§å®¶åŠ©åŠ›2W Star! è¾¾åˆ°3W Starå°†ä¼šæ›´æ–°ç»­ä½œï¼Œã€Šä»é›¶å¼€å§‹è®­ç»ƒæ™ºèƒ½ä½“ã€‹ï¼Œå¸®åŠ©æ¯ä¸€ä¸ªå­¦ä¹ è€…æŒæ¡ä»é›¶åˆ°ä¸€è®­ç»ƒè‡ªå®šä¹‰åœºæ™¯æ™ºèƒ½ä½“æ¨¡å‹çš„èƒ½åŠ›ã€‚

## ğŸ¤ å¦‚ä½•è´¡çŒ®

æˆ‘ä»¬æ˜¯ä¸€ä¸ªå¼€æ”¾çš„å¼€æºç¤¾åŒºï¼Œæ¬¢è¿ä»»ä½•å½¢å¼çš„è´¡çŒ®ï¼

- ğŸ› &lt;strong&gt;æŠ¥å‘Š Bug&lt;/strong&gt; - å‘ç°å†…å®¹æˆ–ä»£ç é—®é¢˜ï¼Œè¯·æäº¤ Issue
- ğŸ’¡ &lt;strong&gt;æå‡ºå»ºè®®&lt;/strong&gt; - å¯¹é¡¹ç›®æœ‰å¥½æƒ³æ³•ï¼Œæ¬¢è¿å‘èµ·è®¨è®º
- ğŸ“ &lt;strong&gt;å®Œå–„å†…å®¹&lt;/strong&gt; - å¸®åŠ©æ”¹è¿›æ•™ç¨‹ï¼Œæäº¤ä½ çš„ Pull Request
- âœï¸ &lt;strong&gt;åˆ†äº«å®è·µ&lt;/strong&gt; - åœ¨&quot;ç¤¾åŒºè´¡çŒ®ç²¾é€‰&quot;ä¸­åˆ†äº«ä½ çš„å­¦ä¹ ç¬”è®°å’Œé¡¹ç›®

## ğŸ™ è‡´è°¢

### æ ¸å¿ƒè´¡çŒ®è€…
- [é™ˆæ€å·-é¡¹ç›®è´Ÿè´£äºº](https://github.com/jjyaoao) (Datawhale æˆå‘˜, å…¨æ–‡å†™ä½œå’Œæ ¡å¯¹)
- [å­™éŸ¬-è”åˆå‘èµ·è€…](https://github.com/fengju0213) (Datawhale æˆå‘˜ã€CAMEL-AI, ç¬¬ä¹ç« å†…å®¹å’Œæ ¡å¯¹)  
- [å§œèˆ’å‡¡-è”åˆå‘èµ·è€…](https://github.com/Tsumugii24)ï¼ˆDatawhale æˆå‘˜, ç« èŠ‚ä¹ é¢˜è®¾è®¡å’Œæ ¡å¯¹ï¼‰
- [é»„ä½©æ—-Datawhaleæ„å‘æˆå‘˜](https://github.com/HeteroCat) (Agent å¼€å‘å·¥ç¨‹å¸ˆ, ç¬¬äº”ç« å†…å®¹è´¡çŒ®è€…)
- [æ›¾é‘«æ°‘-Agentå·¥ç¨‹å¸ˆ](https://github.com/fancyboi999) (ç‰›å®¢ç§‘æŠ€, ç¬¬åå››ç« æ¡ˆä¾‹å¼€å‘)
- [æœ±ä¿¡å¿ -æŒ‡å¯¼ä¸“å®¶](https://xinzhongzhu.github.io/) (Datawhaleé¦–å¸­ç§‘å­¦å®¶-æµ™æ±Ÿå¸ˆèŒƒå¤§å­¦æ­å·äººå·¥æ™ºèƒ½ç ”ç©¶é™¢æ•™æˆ)
### Extra-Chapter è´¡çŒ®è€…
- [WH](https://github.com/WHQAQ11) (å†…å®¹è´¡çŒ®è€…)
- [å‘¨å¥¥æ°-DWè´¡çŒ®è€…å›¢é˜Ÿ](https://github.com/thunderbolt-fire) (è¥¿å®‰äº¤é€šå¤§å­¦, Extra02 å†…å®¹è´¡çŒ®)
- [å¼ å®¸æ—­-ä¸ªäººå¼€å‘è€…](https://github.com/Tasselszcx)(å¸å›½ç†å·¥å­¦é™¢, Extra03 å†…å®¹è´¡çŒ®)
- [é»„å®æ™—-DWè´¡çŒ®è€…å›¢é˜Ÿ](https://github.com/XiaoMa-PM) (æ·±åœ³å¤§å­¦, Extra04 å†…å®¹è´¡çŒ®)
- [ç‹å¤§é¹-Datawhaleæˆå‘˜](https://github.com/ditingdapeng) (é«˜çº§ç ”å‘å·¥ç¨‹å¸ˆ, Extra08 å†…å®¹è´¡çŒ®)
- [å°¤é€¸æ™–-ä¸ªäººå¼€å‘è€…](https://github.com/YYHDBL) (å—äº¬ä¿¡æ¯å·¥ç¨‹å¤§å­¦, Extra09 å†…å®¹è´¡çŒ®)

### ç‰¹åˆ«æ„Ÿè°¢
- æ„Ÿè°¢ [@Sm1les](https://github.com/Sm1les) å¯¹æœ¬é¡¹ç›®çš„å¸®åŠ©ä¸æ”¯æŒ
- æ„Ÿè°¢æ‰€æœ‰ä¸ºæœ¬é¡¹ç›®åšå‡ºè´¡çŒ®çš„å¼€å‘è€…ä»¬ â¤ï¸

&lt;div align=center style=&quot;margin-top: 30px;&quot;&gt;
  &lt;a href=&quot;https://github.com/datawhalechina/Hello-Agents/graphs/contributors&quot;&gt;
    &lt;img src=&quot;https://contrib.rocks/image?repo=datawhalechina/Hello-Agents&quot; /&gt;
  &lt;/a&gt;
&lt;/div&gt;

## Star History

&lt;div align=&#039;center&#039;&gt;
    &lt;img src=&quot;./docs/images/star-history-2026210.png&quot; alt=&quot;Datawhale&quot; width=&quot;90%&quot;&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼&lt;/p&gt;
&lt;/div&gt;

## è¯»è€…äº¤æµç¾¤

&lt;div align=&#039;center&#039;&gt;
    &lt;img src=&quot;./è¯»è€…ç¾¤äºŒç»´ç .png&quot; alt=&quot;è¯»è€…ç¾¤äºŒç»´ç &quot; width=&quot;30%&quot;&gt;
    &lt;p&gt;æ‰«æäºŒç»´ç åŠ å…¥è¯»è€…äº¤æµç¾¤ï¼Œä¸æ›´å¤šå­¦ä¹ è€…äº¤æµè®¨è®º&lt;/p&gt;
&lt;/div&gt;

## å…³äº Datawhale

&lt;div align=&#039;center&#039;&gt;
    &lt;img src=&quot;./docs/images/datawhale.png&quot; alt=&quot;Datawhale&quot; width=&quot;30%&quot;&gt;
    &lt;p&gt;æ‰«æäºŒç»´ç å…³æ³¨ Datawhale å…¬ä¼—å·ï¼Œè·å–æ›´å¤šä¼˜è´¨å¼€æºå†…å®¹&lt;/p&gt;
&lt;/div&gt;

---

## ğŸ“œ å¼€æºåè®®

æœ¬ä½œå“é‡‡ç”¨[çŸ¥è¯†å…±äº«ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç›¸åŒæ–¹å¼å…±äº« 4.0 å›½é™…è®¸å¯åè®®](http://creativecommons.org/licenses/by-nc-sa/4.0/)è¿›è¡Œè®¸å¯ã€‚
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[D4Vinci/Scrapling]]></title>
            <link>https://github.com/D4Vinci/Scrapling</link>
            <guid>https://github.com/D4Vinci/Scrapling</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:13 GMT</pubDate>
            <description><![CDATA[ğŸ•·ï¸ An adaptive Web Scraping framework that handles everything from a single request to a full-scale crawl!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/D4Vinci/Scrapling">D4Vinci/Scrapling</a></h1>
            <p>ğŸ•·ï¸ An adaptive Web Scraping framework that handles everything from a single request to a full-scale crawl!</p>
            <p>Language: Python</p>
            <p>Stars: 18,020</p>
            <p>Forks: 1,198</p>
            <p>Stars today: 1,135 stars today</p>
            <h2>README</h2><pre>&lt;!-- mcp-name: io.github.D4Vinci/Scrapling --&gt;

&lt;h1 align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://scrapling.readthedocs.io&quot;&gt;
        &lt;picture&gt;
          &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/D4Vinci/Scrapling/main/docs/assets/cover_dark.svg?sanitize=true&quot;&gt;
          &lt;img alt=&quot;Scrapling Poster&quot; src=&quot;https://raw.githubusercontent.com/D4Vinci/Scrapling/main/docs/assets/cover_light.svg?sanitize=true&quot;&gt;
        &lt;/picture&gt;
    &lt;/a&gt;
    &lt;br&gt;
    &lt;small&gt;Effortless Web Scraping for the Modern Web&lt;/small&gt;
&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://trendshift.io/repositories/14244&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14244&quot; alt=&quot;D4Vinci%2FScrapling | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
    &lt;br/&gt;
    &lt;a href=&quot;https://github.com/D4Vinci/Scrapling/blob/main/docs/README_AR.md&quot;&gt;Ø§Ù„Ø¹Ø±Ø¨ÙŠÙ‡&lt;/a&gt; | &lt;a href=&quot;https://github.com/D4Vinci/Scrapling/blob/main/docs/README_ES.md&quot;&gt;EspaÃ±ol&lt;/a&gt; | &lt;a href=&quot;https://github.com/D4Vinci/Scrapling/blob/main/docs/README_DE.md&quot;&gt;Deutsch&lt;/a&gt; | &lt;a href=&quot;https://github.com/D4Vinci/Scrapling/blob/main/docs/README_CN.md&quot;&gt;ç®€ä½“ä¸­æ–‡&lt;/a&gt; | &lt;a href=&quot;https://github.com/D4Vinci/Scrapling/blob/main/docs/README_JP.md&quot;&gt;æ—¥æœ¬èª&lt;/a&gt; |  &lt;a href=&quot;https://github.com/D4Vinci/Scrapling/blob/main/docs/README_RU.md&quot;&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt;
    &lt;br/&gt;
    &lt;a href=&quot;https://github.com/D4Vinci/Scrapling/actions/workflows/tests.yml&quot; alt=&quot;Tests&quot;&gt;
        &lt;img alt=&quot;Tests&quot; src=&quot;https://github.com/D4Vinci/Scrapling/actions/workflows/tests.yml/badge.svg&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://badge.fury.io/py/Scrapling&quot; alt=&quot;PyPI version&quot;&gt;
        &lt;img alt=&quot;PyPI version&quot; src=&quot;https://badge.fury.io/py/Scrapling.svg&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://pepy.tech/project/scrapling&quot; alt=&quot;PyPI Downloads&quot;&gt;
        &lt;img alt=&quot;PyPI Downloads&quot; src=&quot;https://static.pepy.tech/personalized-badge/scrapling?period=total&amp;units=INTERNATIONAL_SYSTEM&amp;left_color=GREY&amp;right_color=GREEN&amp;left_text=Downloads&quot;&gt;&lt;/a&gt;
    &lt;br/&gt;
    &lt;a href=&quot;https://discord.gg/EMgGbDceNQ&quot; alt=&quot;Discord&quot; target=&quot;_blank&quot;&gt;
      &lt;img alt=&quot;Discord&quot; src=&quot;https://img.shields.io/discord/1360786381042880532?style=social&amp;logo=discord&amp;link=https%3A%2F%2Fdiscord.gg%2FEMgGbDceNQ&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://x.com/Scrapling_dev&quot; alt=&quot;X (formerly Twitter)&quot;&gt;
      &lt;img alt=&quot;X (formerly Twitter) Follow&quot; src=&quot;https://img.shields.io/twitter/follow/Scrapling_dev?style=social&amp;logo=x&amp;link=https%3A%2F%2Fx.com%2FScrapling_dev&quot;&gt;
    &lt;/a&gt;
    &lt;br/&gt;
    &lt;a href=&quot;https://pypi.org/project/scrapling/&quot; alt=&quot;Supported Python versions&quot;&gt;
        &lt;img alt=&quot;Supported Python versions&quot; src=&quot;https://img.shields.io/pypi/pyversions/scrapling.svg&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://scrapling.readthedocs.io/en/latest/parsing/selection/&quot;&gt;&lt;strong&gt;Selection methods&lt;/strong&gt;&lt;/a&gt;
    &amp;middot;
    &lt;a href=&quot;https://scrapling.readthedocs.io/en/latest/fetching/choosing/&quot;&gt;&lt;strong&gt;Fetchers&lt;/strong&gt;&lt;/a&gt;
    &amp;middot;
    &lt;a href=&quot;https://scrapling.readthedocs.io/en/latest/spiders/architecture.html&quot;&gt;&lt;strong&gt;Spiders&lt;/strong&gt;&lt;/a&gt;
    &amp;middot;
    &lt;a href=&quot;https://scrapling.readthedocs.io/en/latest/spiders/proxy-blocking.html&quot;&gt;&lt;strong&gt;Proxy Rotation&lt;/strong&gt;&lt;/a&gt;
    &amp;middot;
    &lt;a href=&quot;https://scrapling.readthedocs.io/en/latest/cli/overview/&quot;&gt;&lt;strong&gt;CLI&lt;/strong&gt;&lt;/a&gt;
    &amp;middot;
    &lt;a href=&quot;https://scrapling.readthedocs.io/en/latest/ai/mcp-server/&quot;&gt;&lt;strong&gt;MCP&lt;/strong&gt;&lt;/a&gt;
&lt;/p&gt;

Scrapling is an adaptive Web Scraping framework that handles everything from a single request to a full-scale crawl.

Its parser learns from website changes and automatically relocates your elements when pages update. Its fetchers bypass anti-bot systems like Cloudflare Turnstile out of the box. And its spider framework lets you scale up to concurrent, multi-session crawls with pause/resume and automatic proxy rotation â€” all in a few lines of Python. One library, zero compromises.

Blazing fast crawls with real-time stats and streaming. Built by Web Scrapers for Web Scrapers and regular users, there&#039;s something for everyone.

```python
from scrapling.fetchers import Fetcher, AsyncFetcher, StealthyFetcher, DynamicFetcher
StealthyFetcher.adaptive = True
p = StealthyFetcher.fetch(&#039;https://example.com&#039;, headless=True, network_idle=True)  # Fetch website under the radar!
products = p.css(&#039;.product&#039;, auto_save=True)                                        # Scrape data that survives website design changes!
products = p.css(&#039;.product&#039;, adaptive=True)                                         # Later, if the website structure changes, pass `adaptive=True` to find them!
```
Or scale up to full crawls
```python
from scrapling.spiders import Spider, Response

class MySpider(Spider):
  name = &quot;demo&quot;
  start_urls = [&quot;https://example.com/&quot;]

  async def parse(self, response: Response):
      for item in response.css(&#039;.product&#039;):
          yield {&quot;title&quot;: item.css(&#039;h2::text&#039;).get()}

MySpider().start()
```

# Platinum Sponsors

&lt;i&gt;&lt;sub&gt;Do you want to be the first company to show up here? Click [here](https://github.com/sponsors/D4Vinci/sponsorships?tier_id=586646)&lt;/sub&gt;&lt;/i&gt;
# Sponsors 

&lt;!-- sponsors --&gt;

&lt;a href=&quot;https://www.thordata.com/?ls=github&amp;lk=github&quot; target=&quot;_blank&quot; title=&quot;Unblockable proxies and scraping infrastructure, delivering real-time, reliable web data to power AI models and workflows.&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/thordata.jpg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://evomi.com?utm_source=github&amp;utm_medium=banner&amp;utm_campaign=d4vinci-scrapling&quot; target=&quot;_blank&quot; title=&quot;Evomi is your Swiss Quality Proxy Provider, starting at $0.49/GB&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/evomi.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://serpapi.com/?utm_source=scrapling&quot; target=&quot;_blank&quot; title=&quot;Scrape Google and other search engines with SerpApi&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/SerpApi.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://visit.decodo.com/Dy6W0b&quot; target=&quot;_blank&quot; title=&quot;Try the Most Efficient Residential Proxies for Free&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/decodo.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://petrosky.io/d4vinci&quot; target=&quot;_blank&quot; title=&quot;PetroSky delivers cutting-edge VPS hosting.&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/petrosky.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://hasdata.com/?utm_source=github&amp;utm_medium=banner&amp;utm_campaign=D4Vinci&quot; target=&quot;_blank&quot; title=&quot;The web scraping service that actually beats anti-bot systems!&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/hasdata.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://proxyempire.io/&quot; target=&quot;_blank&quot; title=&quot;Collect The Data Your Project Needs with the Best Residential Proxies&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/ProxyEmpire.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://hypersolutions.co/?utm_source=github&amp;utm_medium=readme&amp;utm_campaign=scrapling&quot; target=&quot;_blank&quot; title=&quot;Bot Protection Bypass API for Akamai, DataDome, Incapsula &amp; Kasada&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/HyperSolutions.png&quot;&gt;&lt;/a&gt;


&lt;a href=&quot;https://www.swiftproxy.net/&quot; target=&quot;_blank&quot; title=&quot;Unlock Reliable Proxy Services with Swiftproxy!&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/swiftproxy.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.rapidproxy.io/?ref=d4v&quot; target=&quot;_blank&quot; title=&quot;Affordable Access to the Proxy World â€“ bypass CAPTCHAs blocks, and avoid additional costs.&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/rapidproxy.jpg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://browser.cash/?utm_source=D4Vinci&amp;utm_medium=referral&quot; target=&quot;_blank&quot; title=&quot;Browser Automation &amp; AI Browser Agent Platform&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/browserCash.png&quot;&gt;&lt;/a&gt;

&lt;!-- /sponsors --&gt;

&lt;i&gt;&lt;sub&gt;Do you want to show your ad here? Click [here](https://github.com/sponsors/D4Vinci) and choose the tier that suites you!&lt;/sub&gt;&lt;/i&gt;

---

## Key Features

### Spiders â€” A Full Crawling Framework
- ğŸ•·ï¸ **Scrapy-like Spider API**: Define spiders with `start_urls`, async `parse` callbacks, and `Request`/`Response` objects.
- âš¡ **Concurrent Crawling**: Configurable concurrency limits, per-domain throttling, and download delays.
- ğŸ”„ **Multi-Session Support**: Unified interface for HTTP requests, and stealthy headless browsers in a single spider â€” route requests to different sessions by ID.
- ğŸ’¾ **Pause &amp; Resume**: Checkpoint-based crawl persistence. Press Ctrl+C for a graceful shutdown; restart to resume from where you left off.
- ğŸ“¡ **Streaming Mode**: Stream scraped items as they arrive via `async for item in spider.stream()` with real-time stats â€” ideal for UI, pipelines, and long-running crawls.
- ğŸ›¡ï¸ **Blocked Request Detection**: Automatic detection and retry of blocked requests with customizable logic.
- ğŸ“¦ **Built-in Export**: Export results through hooks and your own pipeline or the built-in JSON/JSONL with `result.items.to_json()` / `result.items.to_jsonl()` respectively.

### Advanced Websites Fetching with Session Support
- **HTTP Requests**: Fast and stealthy HTTP requests with the `Fetcher` class. Can impersonate browsers&#039; TLS fingerprint, headers, and use HTTP/3.
- **Dynamic Loading**: Fetch dynamic websites with full browser automation through the `DynamicFetcher` class supporting Playwright&#039;s Chromium and Google&#039;s Chrome.
- **Anti-bot Bypass**: Advanced stealth capabilities with `StealthyFetcher` and fingerprint spoofing. Can easily bypass all types of Cloudflare&#039;s Turnstile/Interstitial with automation.
- **Session Management**: Persistent session support with `FetcherSession`, `StealthySession`, and `DynamicSession` classes for cookie and state management across requests.
- **Proxy Rotation**: Built-in `ProxyRotator` with cyclic or custom rotation strategies across all session types, plus per-request proxy overrides.
- **Domain Blocking**: Block requests to specific domains (and their subdomains) in browser-based fetchers.
- **Async Support**: Complete async support across all fetchers and dedicated async session classes.

### Adaptive Scraping &amp; AI Integration
- ğŸ”„ **Smart Element Tracking**: Relocate elements after website changes using intelligent similarity algorithms.
- ğŸ¯ **Smart Flexible Selection**: CSS selectors, XPath selectors, filter-based search, text search, regex search, and more.
- ğŸ” **Find Similar Elements**: Automatically locate elements similar to found elements.
- ğŸ¤– **MCP Server to be used with AI**: Built-in MCP server for AI-assisted Web Scraping and data extraction. The MCP server features powerful, custom capabilities that leverage Scrapling to extract targeted content before passing it to the AI (Claude/Cursor/etc), thereby speeding up operations and reducing costs by minimizing token usage. ([demo video](https://www.youtube.com/watch?v=qyFk3ZNwOxE))

### High-Performance &amp; battle-tested Architecture
- ğŸš€ **Lightning Fast**: Optimized performance outperforming most Python scraping libraries.
- ğŸ”‹ **Memory Efficient**: Optimized data structures and lazy loading for a minimal memory footprint.
- âš¡ **Fast JSON Serialization**: 10x faster than the standard library.
- ğŸ—ï¸ **Battle tested**: Not only does Scrapling have 92% test coverage and full type hints coverage, but it has been used daily by hundreds of Web Scrapers over the past year.

### Developer/Web Scraper Friendly Experience
- ğŸ¯ **Interactive Web Scraping Shell**: Optional built-in IPython shell with Scrapling integration, shortcuts, and new tools to speed up Web Scraping scripts development, like converting curl requests to Scrapling requests and viewing requests results in your browser.
- ğŸš€ **Use it directly from the Terminal**: Optionally, you can use Scrapling to scrape a URL without writing a single line of code!
- ğŸ› ï¸ **Rich Navigation API**: Advanced DOM traversal with parent, sibling, and child navigation methods.
- ğŸ§¬ **Enhanced Text Processing**: Built-in regex, cleaning methods, and optimized string operations.
- ğŸ“ **Auto Selector Generation**: Generate robust CSS/XPath selectors for any element.
- ğŸ”Œ **Familiar API**: Similar to Scrapy/BeautifulSoup with the same pseudo-elements used in Scrapy/Parsel.
- ğŸ“˜ **Complete Type Coverage**: Full type hints for excellent IDE support and code completion. The entire codebase is automatically scanned with **PyRight** and **MyPy** with each change.
- ğŸ”‹ **Ready Docker image**: With each release, a Docker image containing all browsers is automatically built and pushed.

## Getting Started

Let&#039;s give you a quick glimpse of what Scrapling can do without deep diving.

### Basic Usage
HTTP requests with session support
```python
from scrapling.fetchers import Fetcher, FetcherSession

with FetcherSession(impersonate=&#039;chrome&#039;) as session:  # Use latest version of Chrome&#039;s TLS fingerprint
    page = session.get(&#039;https://quotes.toscrape.com/&#039;, stealthy_headers=True)
    quotes = page.css(&#039;.quote .text::text&#039;).getall()

# Or use one-off requests
page = Fetcher.get(&#039;https://quotes.toscrape.com/&#039;)
quotes = page.css(&#039;.quote .text::text&#039;).getall()
```
Advanced stealth mode
```python
from scrapling.fetchers import StealthyFetcher, StealthySession

with StealthySession(headless=True, solve_cloudflare=True) as session:  # Keep the browser open until you finish
    page = session.fetch(&#039;https://nopecha.com/demo/cloudflare&#039;, google_search=False)
    data = page.css(&#039;#padded_content a&#039;).getall()

# Or use one-off request style, it opens the browser for this request, then closes it after finishing
page = StealthyFetcher.fetch(&#039;https://nopecha.com/demo/cloudflare&#039;)
data = page.css(&#039;#padded_content a&#039;).getall()
```
Full browser automation
```python
from scrapling.fetchers import DynamicFetcher, DynamicSession

with DynamicSession(headless=True, disable_resources=False, network_idle=True) as session:  # Keep the browser open until you finish
    page = session.fetch(&#039;https://quotes.toscrape.com/&#039;, load_dom=False)
    data = page.xpath(&#039;//span[@class=&quot;text&quot;]/text()&#039;).getall()  # XPath selector if you prefer it

# Or use one-off request style, it opens the browser for this request, then closes it after finishing
page = DynamicFetcher.fetch(&#039;https://quotes.toscrape.com/&#039;)
data = page.css(&#039;.quote .text::text&#039;).getall()
```

### Spiders
Build full crawlers with concurrent requests, multiple session types, and pause/resume:
```python
from scrapling.spiders import Spider, Request, Response

class QuotesSpider(Spider):
    name = &quot;quotes&quot;
    start_urls = [&quot;https://quotes.toscrape.com/&quot;]
    concurrent_requests = 10
    
    async def parse(self, response: Response):
        for quote in response.css(&#039;.quote&#039;):
            yield {
                &quot;text&quot;: quote.css(&#039;.text::text&#039;).get(),
                &quot;author&quot;: quote.css(&#039;.author::text&#039;).get(),
            }
            
        next_page = response.css(&#039;.next a&#039;)
        if next_page:
            yield response.follow(next_page[0].attrib[&#039;href&#039;])

result = QuotesSpider().start()
print(f&quot;Scraped {len(result.items)} quotes&quot;)
result.items.to_json(&quot;quotes.json&quot;)
```
Use multiple session types in a single spider:
```python
from scrapling.spiders import Spider, Request, Response
from scrapling.fetchers import FetcherSession, AsyncStealthySession

class MultiSessionSpider(Spider):
    name = &quot;multi&quot;
    start_urls = [&quot;https://example.com/&quot;]
    
    def configure_sessions(self, manager):
        manager.add(&quot;fast&quot;, FetcherSession(impersonate=&quot;chrome&quot;))
        manager.add(&quot;stealth&quot;, AsyncStealthySession(headless=True), lazy=True)
    
    async def parse(self, response: Response):
        for link in response.css(&#039;a::attr(href)&#039;).getall():
            # Route protected pages through the stealth session
            if &quot;protected&quot; in link:
                yield Request(link, sid=&quot;stealth&quot;)
            else:
                yield Request(link, sid=&quot;fast&quot;, callback=self.parse)  # explicit callback
```
Pause and resume long crawls with checkpoints by running the spider like this:
```python
QuotesSpider(crawldir=&quot;./crawl_data&quot;).start()
```
Press Ctrl+C to pause gracefully â€” progress is saved automatically. Later, when you start the spider again, pass the same `crawldir`, and it will resume from where it stopped.

### Advanced Parsing &amp; Navigation
```python
from scrapling.fetchers import Fetcher

# Rich element selection and navigation
page = Fetcher.get(&#039;https://quotes.toscrape.com/&#039;)

# Get quotes with multiple selection methods
quotes = page.css(&#039;.quote&#039;)  # CSS selector
quotes = page.xpath(&#039;//div[@class=&quot;quote&quot;]&#039;)  # XPath
quotes = page.find_all(&#039;div&#039;, {&#039;class&#039;: &#039;quote&#039;})  # BeautifulSoup-style
# Same as
quotes = page.find_all(&#039;div&#039;, class_=&#039;quote&#039;)
quotes = page.find_all([&#039;div&#039;], class_=&#039;quote&#039;)
quotes = page.find_all(class_=&#039;quote&#039;)  # and so on...
# Find element by text content
quotes = page.find_by_text(&#039;quote&#039;, tag=&#039;div&#039;)

# Advanced navigation
quote_text = page.css(&#039;.quote&#039;)[0].css(&#039;.text::text&#039;).get()
quote_text = page.css(&#039;.quote&#039;).css(&#039;.text::text&#039;).getall()  # Chained selectors
first_quote = page.css(&#039;.quote&#039;)[0]
author = first_quote.next_sibling.css(&#039;.author::text&#039;)
parent_container = first_quote.parent

# Element relationships and similarity
similar_elements = first_quote.find_similar()
below_elements = first_quote.below_elements()
```
You can use the parser right away if you don&#039;t want to fetch websites like below:
```python
from scrapling.parser import Selector

page = Selector(&quot;&lt;html&gt;...&lt;/html&gt;&quot;)
```
And it works precisely the same way!

### Async Session Management Examples
```python
import asyncio
from scrapling.fetchers import FetcherSession, AsyncStealthySession, AsyncDynamicSession

async with FetcherSession(http3=True) as session:  # `FetcherSession` is context-aware and can work in both sync/async patterns
    page1 = session.get(&#039;https://quotes.toscrape.com/&#039;)
    page2 = session.get(&#039;https://quotes.toscrape.com/&#039;, impersonate=&#039;firefox135&#039;)

# Async session usage
async with AsyncStealthySession(max_pages=2) as session:
    tasks = []
    urls = [&#039;https://example.com/page1&#039;, &#039;https://example.com/page2&#039;]
    
    for url in urls:
        task = session.fetch(url)
        tasks.append(task)
    
    print(session.get_pool_stats())  # Optional - The status of the browser tabs pool (busy/free/error)
    results = await asyncio.gather(*tasks)
    print(session.get_pool_stats())
```

## CLI &amp; Interactive Shell

Scrapling includes a powerful command-line interface:

[![asciicast](https://asciinema.org/a/736339.svg)](https://asciinema.org/a/736339)

Launch the interactive Web Scraping shell
```bash
scrapling shell
```
Extract pages to a file directly without programming (Extracts the content inside the `body` tag by default). If the output file ends with `.txt`, then the text content of the target will be extracted. If it ends in `.md`, it will be a Markdown representation of the HTML content; if it ends in `.html`, it will be the HTML content itself.
```bash
scrapling extract get &#039;https://example.com&#039; content.md
scrapling extract get &#039;https://example.com&#039; content.txt --css-selector &#039;#fromSkipToProducts&#039; --impersonate &#039;chrome&#039;  # All elements matching the CSS selector &#039;#fromSkipToProducts&#039;
scrapling extract fetch &#039;https://example.com&#039; content.md --css-selector &#039;#fromSkipToProducts&#039; --no-headless
scrapling extract stealthy-fetch &#039;https://nopecha.com/demo/cloudflare&#039; captchas.html --css-selector &#039;#padded_content a&#039; --solve-cloudflare
```

&gt; [!NOTE]
&gt; There are many additional features, but we want to keep this page concise, including the MCP server and the interactive Web Scraping Shell. Check out the full documentation [here](https://scrapling.readthedocs.io/en/latest/)

## Performance Benchmarks

Scrapling isn&#039;t just powerfulâ€”it&#039;s also blazing fast. The following benchmarks compare Scrapling&#039;s parser with the latest versions of other popular libraries.

### Text Extraction Speed Test (5000 nested elements)

| # |      Library      | Time (ms) | vs Scrapling | 
|--

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[alibaba/OpenSandbox]]></title>
            <link>https://github.com/alibaba/OpenSandbox</link>
            <guid>https://github.com/alibaba/OpenSandbox</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:12 GMT</pubDate>
            <description><![CDATA[OpenSandbox is a general-purpose sandbox platform for AI applications, offering multi-language SDKs, unified sandbox APIs, and Docker/Kubernetes runtimes for scenarios like Coding Agents, GUI Agents, Agent Evaluation, AI Code Execution, and RL Training.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/alibaba/OpenSandbox">alibaba/OpenSandbox</a></h1>
            <p>OpenSandbox is a general-purpose sandbox platform for AI applications, offering multi-language SDKs, unified sandbox APIs, and Docker/Kubernetes runtimes for scenarios like Coding Agents, GUI Agents, Agent Evaluation, AI Code Execution, and RL Training.</p>
            <p>Language: Python</p>
            <p>Stars: 1,455</p>
            <p>Forks: 109</p>
            <p>Stars today: 105 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/assets/logo.svg&quot; alt=&quot;OpenSandbox logo&quot; width=&quot;150&quot; /&gt;

  &lt;h1&gt;OpenSandbox&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/alibaba/OpenSandbox&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/alibaba/OpenSandbox.svg?style=social&quot; alt=&quot;GitHub stars&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://deepwiki.com/alibaba/OpenSandbox&quot;&gt;
    &lt;img src=&quot;https://deepwiki.com/badge.svg&quot; alt=&quot;Ask DeepWiki&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/license/alibaba/OpenSandbox.svg&quot; alt=&quot;license&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://badge.fury.io/py/opensandbox&quot;&gt;
    &lt;img src=&quot;https://badge.fury.io/py/opensandbox.svg&quot; alt=&quot;PyPI version&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://badge.fury.io/js/@alibaba-group%2Fopensandbox&quot;&gt;
    &lt;img src=&quot;https://badge.fury.io/js/@alibaba-group%2Fopensandbox.svg&quot; alt=&quot;npm version&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/alibaba/OpenSandbox/actions&quot;&gt;
    &lt;img src=&quot;https://github.com/alibaba/OpenSandbox/actions/workflows/real-e2e.yml/badge.svg?branch=main&quot; alt=&quot;E2E Status&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

  &lt;hr /&gt;
&lt;/div&gt;

English | [ä¸­æ–‡](docs/README_zh.md)

OpenSandbox is a **general-purpose sandbox platform** for AI applications, offering multi-language SDKs, unified sandbox APIs, and Docker/Kubernetes runtimes for scenarios like Coding Agents, GUI Agents, Agent Evaluation, AI Code Execution, and RL Training.

## Features

- **Multi-language SDKs**: Client SDKs for Python, Java/Kotlin, and JavaScript/TypeScript.
- **Sandbox Protocol**: Defines sandbox lifecycle management APIs and sandbox execution APIs so you can extend custom sandbox runtimes.
- **Sandbox Runtime**: Built-in lifecycle management supporting Docker and [high-performance Kubernetes runtime](./kubernetes), enabling both local runs and large-scale distributed scheduling.
- **Sandbox Environments**: Built-in Command, Filesystem, and Code Interpreter implementations. Examples cover Coding Agents (e.g., Claude Code), browser automation (Chrome, Playwright), and desktop environments (VNC, VS Code).
- **Network Policy**: Unified [Ingress Gateway](components/ingress) with multiple routing strategies plus per-sandbox [egress controls](components/egress).

## Examples

### Basic Sandbox Operations

Requirements:

- Docker (required for local execution)
- Python 3.10+ (recommended for examples and local runtime)

#### 1. Install and Configure the Sandbox Server

```bash
uv pip install opensandbox-server
opensandbox-server init-config ~/.sandbox.toml --example docker
```

&gt; If you prefer working from source, you can still clone the repo for development, but server startup no longer requires it.
&gt;
&gt; ```bash
&gt; git clone https://github.com/alibaba/OpenSandbox.git
&gt; cd OpenSandbox/server
&gt; uv sync
&gt; cp example.config.toml ~/.sandbox.toml # Copy configuration file
&gt; uv run python -m src.main # Start the service
&gt; ```

#### 2. Start the Sandbox Server

```bash
opensandbox-server

# Show help
opensandbox-server -h
```

#### 3. Create a Code Interpreter and Execute Commands

Install the Code Interpreter SDK

```bash
uv pip install opensandbox-code-interpreter
```

Create a sandbox and execute commands

```python
import asyncio
from datetime import timedelta

from code_interpreter import CodeInterpreter, SupportedLanguage
from opensandbox import Sandbox
from opensandbox.models import WriteEntry

async def main() -&gt; None:
    # 1. Create a sandbox
    sandbox = await Sandbox.create(
        &quot;opensandbox/code-interpreter:v1.0.1&quot;,
        entrypoint=[&quot;/opt/opensandbox/code-interpreter.sh&quot;],
        env={&quot;PYTHON_VERSION&quot;: &quot;3.11&quot;},
        timeout=timedelta(minutes=10),
    )

    async with sandbox:

        # 2. Execute a shell command
        execution = await sandbox.commands.run(&quot;echo &#039;Hello OpenSandbox!&#039;&quot;)
        print(execution.logs.stdout[0].text)

        # 3. Write a file
        await sandbox.files.write_files([
            WriteEntry(path=&quot;/tmp/hello.txt&quot;, data=&quot;Hello World&quot;, mode=644)
        ])

        # 4. Read a file
        content = await sandbox.files.read_file(&quot;/tmp/hello.txt&quot;)
        print(f&quot;Content: {content}&quot;) # Content: Hello World

        # 5. Create a code interpreter
        interpreter = await CodeInterpreter.create(sandbox)

        # 6. Execute Python code (single-run, pass language directly)
        result = await interpreter.codes.run(
              &quot;&quot;&quot;
                  import sys
                  print(sys.version)
                  result = 2 + 2
                  result
              &quot;&quot;&quot;,
              language=SupportedLanguage.PYTHON,
        )

        print(result.result[0].text) # 4
        print(result.logs.stdout[0].text) # 3.11.14

    # 7. Cleanup the sandbox
    await sandbox.kill()

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

### More Examples

OpenSandbox provides rich examples demonstrating sandbox usage in different scenarios. All example code is located in the `examples/` directory.

#### ğŸ¯ Basic Examples

- **[code-interpreter](examples/code-interpreter/README.md)** - End-to-end Code Interpreter SDK workflow in a sandbox.
- **[aio-sandbox](examples/aio-sandbox/README.md)** - All-in-One sandbox setup using the OpenSandbox SDK.
- **[agent-sandbox](examples/agent-sandbox/README.md)** - Run OpenSandbox on Kubernetes via [kubernetes-sigs/agent-sandbox](https://github.com/kubernetes-sigs/agent-sandbox).

#### ğŸ¤– Coding Agent Integrations

- **[claude-code](examples/claude-code/README.md)** - Run Claude Code inside OpenSandbox.
- **[gemini-cli](examples/gemini-cli/README.md)** - Run Google Gemini CLI inside OpenSandbox.
- **[codex-cli](examples/codex-cli/README.md)** - Run OpenAI Codex CLI inside OpenSandbox.
- **[iflow-cli](examples/iflow-cli/README.md)** - Run iFLow CLI inside OpenSandbox.
- **[langgraph](examples/langgraph/README.md)** - LangGraph state-machine workflow that creates/runs a sandbox job with fallback retry.
- **[google-adk](examples/google-adk/README.md)** - Google ADK agent using OpenSandbox tools to write/read files and run commands.
- **[openclaw](examples/openclaw/README.md)** - Launch an OpenClaw Gateway inside a sandbox.

#### ğŸŒ Browser and Desktop Environments

- **[chrome](examples/chrome/README.md)** - Headless Chromium with VNC and DevTools access for automation/debugging.
- **[playwright](examples/playwright/README.md)** - Playwright + Chromium headless scraping and testing example.
- **[desktop](examples/desktop/README.md)** - Full desktop environment in a sandbox with VNC access.
- **[vscode](examples/vscode/README.md)** - code-server (VS Code Web) running inside a sandbox for remote dev.

#### ğŸ§  ML and Training

- **[rl-training](examples/rl-training/README.md)** - DQN CartPole training in a sandbox with checkpoints and summary output.

For more details, please refer to [examples](examples/README.md) and the README files in each example directory.

## Project Structure

| Directory | Description                                                      |
|-----------|------------------------------------------------------------------|
| [`sdks/`](sdks/) | Multi-language SDKs (Python, Java/Kotlin, TypeScript/JavaScript) |
| [`specs/`](specs/README.md) | OpenAPI specs and lifecycle specifications                      |
| [`server/`](server/README.md) | Python FastAPI sandbox lifecycle server                          |
| [`kubernetes/`](kubernetes/README.md) | Kubernetes deployment and examples                               |
| [`components/execd/`](components/execd/README.md) | Sandbox execution daemon (commands and file operations)          |
| [`components/ingress/`](components/ingress/README.md) | Sandbox traffic ingress proxy                                    |
| [`components/egress/`](components/egress/README.md) | Sandbox network egress control                                   |
| [`sandboxes/`](sandboxes/) | Runtime sandbox implementations                                   |
| [`examples/`](examples/README.md) | Integration examples and use cases                               |
| [`oseps/`](oseps/README.md) | OpenSandbox Enhancement Proposals                                |
| [`docs/`](docs/) | Architecture and design documentation                            |
| [`tests/`](tests/) | Cross-component E2E tests                                        |
| [`scripts/`](scripts/) | Development and maintenance scripts                              |

For detailed architecture, see [docs/architecture.md](docs/architecture.md).

## Documentation

- [docs/architecture.md](docs/architecture.md) â€“ Overall architecture &amp; design philosophy
- SDK
  - Sandbox base SDK ([Java\Kotlin SDK](sdks/sandbox/kotlin/README.md), [Python SDK](sdks/sandbox/python/README.md), [JavaScript/TypeScript SDK](sdks/sandbox/javascript/README.md)) - includes sandbox lifecycle, command execution, file operations
  - Code Interpreter SDK ([Java\Kotlin SDK](sdks/code-interpreter/kotlin/README.md), [Python SDK](sdks/code-interpreter/python/README.md), [JavaScript/TypeScript SDK](sdks/code-interpreter/javascript/README.md)) - code interpreter
- [specs/README.md](specs/README.md) - OpenAPI definitions for sandbox lifecycle API and sandbox execution API
- [server/README.md](server/README.md) - Sandbox server startup and configuration; supports Docker and Kubernetes runtimes

## License

This project is open source under the [Apache 2.0 License](LICENSE).

## Roadmap

### SDK

- [ ] **Go SDK** - Go client SDK for sandbox lifecycle management, command execution, and file operations.

### Sandbox Runtime

- [ ] **Persistent storage** - Mountable persistent storage for sandboxes (see [Proposal 0003](oseps/0003-volume-and-volumebinding-support.md)).
- [ ] **Ingress multi-network strategies** - Multi-Kubernetes provisioning and multi-network modes for the Ingress Gateway.
- [ ] **Local lightweight sandbox** - Lightweight sandbox for AI tools running directly on PCs.

### Deployment

- [ ] **Kubernetes Helm** - Helm charts to deploy all components.

## Contact and Discussion

- Issues: Submit bugs, feature requests, or design discussions through GitHub Issues
## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=alibaba/OpenSandbox&amp;type=date&amp;legend=top-left)](https://www.star-history.com/#alibaba/OpenSandbox&amp;type=date&amp;legend=top-left)

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hiyouga/LlamaFactory]]></title>
            <link>https://github.com/hiyouga/LlamaFactory</link>
            <guid>https://github.com/hiyouga/LlamaFactory</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:11 GMT</pubDate>
            <description><![CDATA[Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hiyouga/LlamaFactory">hiyouga/LlamaFactory</a></h1>
            <p>Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)</p>
            <p>Language: Python</p>
            <p>Stars: 67,648</p>
            <p>Forks: 8,247</p>
            <p>Stars today: 52 stars today</p>
            <h2>README</h2><pre>![# LLaMA Factory](assets/logo.png)

[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social)](https://github.com/hiyouga/LLaMA-Factory/stargazers)
[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory)](https://github.com/hiyouga/LLaMA-Factory/commits/main)
[![GitHub contributors](https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange)](https://github.com/hiyouga/LLaMA-Factory/graphs/contributors)
[![GitHub workflow](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg)](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml)
[![PyPI](https://img.shields.io/pypi/v/llamafactory)](https://pypi.org/project/llamafactory/)
[![Citation](https://img.shields.io/badge/citation-1000+-green)](https://scholar.google.com/scholar?cites=12620864006390196564)
[![Docker Pulls](https://img.shields.io/docker/pulls/hiyouga/llamafactory)](https://hub.docker.com/r/hiyouga/llamafactory/tags)

[![Twitter](https://img.shields.io/twitter/follow/llamafactory_ai)](https://twitter.com/llamafactory_ai)
[![Discord](assets/thirdparty/discord.svg)](https://discord.gg/rKfvV9r9FK)
[![WeChat](https://img.shields.io/badge/WeChat-User%20Group-blue?logo=wechat)](https://github.com/hiyouga/llamafactory-community)
[![Blog](https://img.shields.io/badge/Hugo-Official%20Blog-blue?logo=hugo)](https://blog.llamafactory.net/en/)

[![Open in Colab](assets/thirdparty/colab.svg)](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)
[![Open in DSW](assets/thirdparty/dsw.svg)](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory)
[![Open in Lab4ai](assets/thirdparty/lab4ai.svg)](https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&amp;utm_source=LLaMA-Factory)
[![Open in Online](assets/thirdparty/online.svg)](https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory)
[![Open in Spaces](https://img.shields.io/badge/ğŸ¤—-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/hiyouga/LLaMA-Board)
[![Open in Studios](https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue)](https://modelscope.cn/studios/hiyouga/LLaMA-Board)
[![Open in Novita](https://img.shields.io/badge/Novita-Deploy%20Template-blue)](https://novita.ai/templates-library/105981?sharer=88115474-394e-4bda-968e-b88e123d0c47)

### Used by [Amazon](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/), [NVIDIA](https://developer.nvidia.com/rtx/ai-toolkit), [Aliyun](https://help.aliyun.com/zh/pai/use-cases/fine-tune-a-llama-3-model-with-llama-factory), etc.

&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;

### Supporters â¤ï¸

| &lt;div style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://warp.dev/llama-factory&quot;&gt;&lt;img alt=&quot;Warp sponsorship&quot; width=&quot;400&quot; src=&quot;assets/sponsors/warp.jpg&quot;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://warp.dev/llama-factory&quot; style=&quot;font-size:larger;&quot;&gt;Warp, the agentic terminal for developers&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://warp.dev/llama-factory&quot;&gt;Available for MacOS, Linux, &amp; Windows&lt;/a&gt; | &lt;a href=&quot;https://serpapi.com&quot;&gt;&lt;img alt=&quot;SerpAPI sponsorship&quot; width=&quot;250&quot; src=&quot;assets/sponsors/serpapi.svg&quot;&gt; &lt;/a&gt; |
| ---- | ---- |

----

### Easily fine-tune 100+ large language models with zero-code [CLI](#quickstart) and [Web UI](#fine-tuning-with-llama-board-gui-powered-by-gradio)

![GitHub Trend](https://trendshift.io/api/badge/repositories/4535)

&lt;/div&gt;

ğŸ‘‹ Join our [WeChat](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/main.jpg), [NPU](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/npu.jpg), [Lab4AI](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/lab4ai.jpg), [LLaMA Factory Online](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/online.jpg) user group.

\[ English | [ä¸­æ–‡](README_zh.md) \]

**Fine-tuning a large language model can be easy as...**

https://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e

Start local training:
- Please refer to [usage](#getting-started)

Start cloud training:
- **Colab (free)**: https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing
- **PAI-DSW (free trial)**: https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory
- **LLaMA Factory Online**: https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory
- **Alaya NeW (cloud GPU deal)**: https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory

Read technical notes:
- **Documentation (WIP)**: https://llamafactory.readthedocs.io/en/latest/
- **Documentation (AMD GPU)**: https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html
- **Official Blog**: https://blog.llamafactory.net/en/
- **Official Course**: https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&amp;utm_source=LLaMA-Factory

&gt; [!NOTE]
&gt; Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.

## Table of Contents

- [Features](#features)
- [Blogs](#blogs)
- [Changelog](#changelog)
- [Supported Models](#supported-models)
- [Supported Training Approaches](#supported-training-approaches)
- [Provided Datasets](#provided-datasets)
- [Requirement](#requirement)
- [Getting Started](#getting-started)
  - [Installation](#installation)
  - [Data Preparation](#data-preparation)
  - [Quickstart](#quickstart)
  - [Fine-Tuning with LLaMA Board GUI](#fine-tuning-with-llama-board-gui-powered-by-gradio)
  - [LLaMA Factory Online](#llama-factory-online)
  - [Build Docker](#build-docker)
  - [Deploy with OpenAI-style API and vLLM](#deploy-with-openai-style-api-and-vllm)
  - [Download from ModelScope Hub](#download-from-modelscope-hub)
  - [Download from Modelers Hub](#download-from-modelers-hub)
  - [Use W&amp;B Logger](#use-wb-logger)
  - [Use SwanLab Logger](#use-swanlab-logger)
- [Projects using LLaMA Factory](#projects-using-llama-factory)
- [License](#license)
- [Citation](#citation)
- [Acknowledgement](#acknowledgement)

## Features

- **Various models**: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen3, Qwen3-VL, DeepSeek, Gemma, GLM, Phi, etc.
- **Integrated methods**: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.
- **Scalable resources**: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.
- **Advanced algorithms**: [GaLore](https://github.com/jiaweizzhao/GaLore), [BAdam](https://github.com/Ledzy/BAdam), [APOLLO](https://github.com/zhuhanqing/APOLLO), [Adam-mini](https://github.com/zyushun/Adam-mini), [Muon](https://github.com/KellerJordan/Muon), [OFT](https://github.com/huggingface/peft/tree/main/src/peft/tuners/oft), DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.
- **Practical tricks**: [FlashAttention-2](https://github.com/Dao-AILab/flash-attention), [Unsloth](https://github.com/unslothai/unsloth), [Liger Kernel](https://github.com/linkedin/Liger-Kernel), [KTransformers](https://github.com/kvcache-ai/ktransformers/), RoPE scaling, NEFTune and rsLoRA.
- **Wide tasks**: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.
- **Experiment monitors**: LlamaBoard, TensorBoard, Wandb, MLflow, [SwanLab](https://github.com/SwanHubX/SwanLab), etc.
- **Faster inference**: OpenAI-style API, Gradio UI and CLI with [vLLM worker](https://github.com/vllm-project/vllm) or [SGLang worker](https://github.com/sgl-project/sglang).

### Day-N Support for Fine-Tuning Cutting-Edge Models

| Support Date | Model Name                                                           |
| ------------ | -------------------------------------------------------------------- |
| Day 0        | Qwen3 / Qwen2.5-VL / Gemma 3 / GLM-4.1V / InternLM 3 / MiniCPM-o-2.6 |
| Day 1        | Llama 3 / GLM-4 / Mistral Small / PaliGemma2 / Llama 4               |

## Blogs

&gt; [!TIP]
&gt; Now we have a dedicated blog for LLaMA Factory!
&gt;
&gt; Website: https://blog.llamafactory.net/en/

- ğŸ’¡ [KTransformers Fine-Tuning Ã— LLaMA Factory: Fine-tuning 1000 Billion models with 2 4090-GPU + CPU](https://blog.llamafactory.net/en/posts/ktransformers/) (English)
- ğŸ’¡ [Easy Dataset Ã— LLaMA Factory: Enabling LLMs to Efficiently Learn Domain Knowledge](https://buaa-act.feishu.cn/wiki/GVzlwYcRFiR8OLkHbL6cQpYin7g) (English)
- [Fine-tune a mental health LLM using LLaMA-Factory](https://www.lab4ai.cn/project/detail?id=25cce32ec131497b9e06a93336a0817f&amp;type=project&amp;utm_source=LLaMA-Factory) (Chinese)
- [Fine-tune GPT-OSS for Role-Playing using LLaMA-Factory](https://docs.llamafactory.com.cn/docs/documents/best-practice/gptroleplay/?utm_source=LLaMA-Factory) (Chinese)
- [A One-Stop Code-Free Model Reinforcement Learning and Deployment Platform based on LLaMA-Factory and EasyR1](https://aws.amazon.com/cn/blogs/china/building-llm-model-hub-based-on-llamafactory-and-easyr1/) (Chinese)
- [How Apoidea Group enhances visual information extraction from banking documents with multimodal models using LLaMA-Factory on Amazon SageMaker HyperPod](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/) (English)

&lt;details&gt;&lt;summary&gt;All Blogs&lt;/summary&gt;

- [Fine-tune Llama3.1-70B for Medical Diagnosis using LLaMA-Factory](https://docs.alayanew.com/docs/documents/bestPractice/bigModel/llama70B/?utm_source=LLaMA-Factory) (Chinese)
- [Fine-tune Qwen2.5-VL for Autonomous Driving using LLaMA-Factory](https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory) (Chinese)
- [LLaMA Factory: Fine-tuning the DeepSeek-R1-Distill-Qwen-7B Model for News Classifier](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b) (Chinese)
- [A One-Stop Code-Free Model Fine-Tuning \&amp; Deployment Platform based on SageMaker and LLaMA-Factory](https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/) (Chinese)
- [LLaMA Factory Multi-Modal Fine-Tuning Practice: Fine-Tuning Qwen2-VL for Personal Tourist Guide](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl) (Chinese)
- [LLaMA Factory: Fine-tuning Llama3 for Role-Playing](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory) (Chinese)

&lt;/details&gt;

## Changelog

[25/10/26] We support Megatron-core training backend with [**mcore_adapter**](https://github.com/alibaba/ROLL/tree/main/mcore_adapter). See [PR #9237](https://github.com/hiyouga/LLaMA-Factory/pull/9237) to get started.

[25/08/22] We supported **[OFT](https://arxiv.org/abs/2306.07280)** and **[OFTv2](https://arxiv.org/abs/2506.19847)**. See [examples](examples/README.md) for usage.

[25/08/20] We supported fine-tuning the **[Intern-S1-mini](https://huggingface.co/internlm/Intern-S1-mini)** models. See [PR #8976](https://github.com/hiyouga/LLaMA-Factory/pull/8976) to get started.

[25/08/06] We supported fine-tuning the **[GPT-OSS](https://github.com/openai/gpt-oss)** models. See [PR #8826](https://github.com/hiyouga/LLaMA-Factory/pull/8826) to get started.

&lt;details&gt;&lt;summary&gt;Full Changelog&lt;/summary&gt;

[25/07/02] We supported fine-tuning the **[GLM-4.1V-9B-Thinking](https://github.com/THUDM/GLM-4.1V-Thinking)** model.

[25/04/28] We supported fine-tuning the **[Qwen3](https://qwenlm.github.io/blog/qwen3/)** model family.

[25/04/21] We supported the **[Muon](https://github.com/KellerJordan/Muon)** optimizer. See [examples](examples/README.md) for usage. Thank [@tianshijing](https://github.com/tianshijing)&#039;s PR.

[25/04/16] We supported fine-tuning the **[InternVL3](https://huggingface.co/OpenGVLab/InternVL3-8B)** model. See [PR #7258](https://github.com/hiyouga/LLaMA-Factory/pull/7258) to get started.

[25/04/14] We supported fine-tuning the **[GLM-Z1](https://huggingface.co/THUDM/GLM-Z1-9B-0414)** and **[Kimi-VL](https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct)** models.

[25/04/06] We supported fine-tuning the **[Llama 4](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)** model. See [PR #7611](https://github.com/hiyouga/LLaMA-Factory/pull/7611) to get started.

[25/03/31] We supported fine-tuning the **[Qwen2.5 Omni](https://qwenlm.github.io/blog/qwen2.5-omni/)** model. See [PR #7537](https://github.com/hiyouga/LLaMA-Factory/pull/7537) to get started.

[25/03/15] We supported **[SGLang](https://github.com/sgl-project/sglang)** as inference backend. Try `infer_backend: sglang` to accelerate inference.

[25/03/12] We supported fine-tuning the **[Gemma 3](https://huggingface.co/blog/gemma3)** model.

[25/02/24] Announcing **[EasyR1](https://github.com/hiyouga/EasyR1)**, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.

[25/02/11] We supported saving the **[Ollama](https://github.com/ollama/ollama)** modelfile when exporting the model checkpoints. See [examples](examples/README.md) for usage.

[25/02/05] We supported fine-tuning the **[Qwen2-Audio](Qwen/Qwen2-Audio-7B-Instruct)** and **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** on audio understanding tasks.

[25/01/31] We supported fine-tuning the **[DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)** and **[Qwen2.5-VL](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)** models.

[25/01/15] We supported **[APOLLO](https://arxiv.org/abs/2412.05270)** optimizer. See [examples](examples/README.md) for usage.

[25/01/14] We supported fine-tuning the **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** and **[MiniCPM-V-2.6](https://huggingface.co/openbmb/MiniCPM-V-2_6)** models. Thank [@BUAADreamer](https://github.com/BUAADreamer)&#039;s PR.

[25/01/14] We supported fine-tuning the **[InternLM 3](https://huggingface.co/collections/internlm/)** models. Thank [@hhaAndroid](https://github.com/hhaAndroid)&#039;s PR.

[25/01/10] We supported fine-tuning the **[Phi-4](https://huggingface.co/microsoft/phi-4)** model.

[24/12/21] We supported using **[SwanLab](https://github.com/SwanHubX/SwanLab)** for experiment tracking and visualization. See [this section](#use-swanlab-logger) for details.

[24/11/27] We supported fine-tuning the **[Skywork-o1](https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B)** model and the **[OpenO1](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)** dataset.

[24/10/09] We supported downloading pre-trained models and datasets from the **[Modelers Hub](https://modelers.cn/models)**. See [this tutorial](#download-from-modelers-hub) for usage.

[24/09/19] We supported fine-tuning the **[Qwen2.5](https://qwenlm.github.io/blog/qwen2.5/)** models.

[24/08/30] We supported fine-tuning the **[Qwen2-VL](https://qwenlm.github.io/blog/qwen2-vl/)** models. Thank [@simonJJJ](https://github.com/simonJJJ)&#039;s PR.

[24/08/27] We supported **[Liger Kernel](https://github.com/linkedin/Liger-Kernel)**. Try `enable_liger_kernel: true` for efficient training.

[24/08/09] We supported **[Adam-mini](https://github.com/zyushun/Adam-mini)** optimizer. See [examples](examples/README.md) for usage. Thank [@relic-yuexi](https://github.com/relic-yuexi)&#039;s PR.

[24/07/04] We supported [contamination-free packed training](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing). Use `neat_packing: true` to activate it. Thank [@chuan298](https://github.com/chuan298)&#039;s PR.

[24/06/16] We supported **[PiSSA](https://arxiv.org/abs/2404.02948)** algorithm. See [examples](examples/README.md) for usage.

[24/06/07] We supported fine-tuning the **[Qwen2](https://qwenlm.github.io/blog/qwen2/)** and **[GLM-4](https://github.com/THUDM/GLM-4)** models.

[24/05/26] We supported **[SimPO](https://arxiv.org/abs/2405.14734)** algorithm for preference learning. See [examples](examples/README.md) for usage.

[24/05/20] We supported fine-tuning the **PaliGemma** series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with `paligemma` template for chat completion.

[24/05/18] We supported **[KTO](https://arxiv.org/abs/2402.01306)** algorithm for preference learning. See [examples](examples/README.md) for usage.

[24/05/14] We supported training and inference on the Ascend NPU devices. Check [installation](#installation) section for details.

[24/04/26] We supported fine-tuning the **LLaVA-1.5** multimodal LLMs. See [examples](examples/README.md) for usage.

[24/04/22] We provided a **[Colab notebook](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)** for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check [Llama3-8B-Chinese-Chat](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat) and [Llama3-Chinese](https://huggingface.co/zhichen/Llama3-Chinese) for details.

[24/04/21] We supported **[Mixture-of-Depths](https://arxiv.org/abs/2404.02258)** according to [AstraMindAI&#039;s implementation](https://github.com/astramind-ai/Mixture-of-depths). See [examples](examples/README.md) for usage.

[24/04/16] We supported **[BAdam](https://arxiv.org/abs/2404.02827)** optimizer. See [examples](examples/README.md) for usage.

[24/04/16] We supported **[unsloth](https://github.com/unslothai/unsloth)**&#039;s long-sequence training (Llama-2-7B-56k within 24GB). It achieves **117%** speed and **50%** memory compared with FlashAttention-2, more benchmarks can be found in [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison).

[24/03/31] We supported **[ORPO](https://arxiv.org/abs/2403.07691)**. See [examples](examples/README.md) for usage.

[24/03/21] Our paper &quot;[LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models](https://arxiv.org/abs/2403.13372)&quot; is available at arXiv!

[24/03/20] We supported **FSDP+QLoRA** that fine-tunes a 70B model on 2x24GB GPUs. See [examples](examples/README.md) for usage.

[24/03/13] We supported **[LoRA+](https://arxiv.org/abs/2402.12354)**. See [examples](examples/README.md) for usage.

[24/03/07] We supported **[GaLore](https://arxiv.org/abs/2403.03507)** optimizer. See [examples](examples/README.md) for usage.

[24/03/07] We integrated **[vLLM](https://github.com/vllm-project/vllm)** for faster and concurrent inference. Try `infer_backend: vllm` to enjoy **270%** inference speed.

[24/02/28] We supported weight-decomposed LoRA (**[DoRA](https://arxiv.org/abs/2402.09353)**). Try `use_dora: true` to activate DoRA training.

[24/02/15] We supported **block expansion** proposed by [LLaMA Pro](https://github.com/TencentARC/LLaMA-Pro). See [examples](examples/README.md) for usage.

[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this [blog post](https://qwenlm.github.io/blog/qwen1.5/) for details.

[24/01/18] We supported **agent tuning** for most models, equipping model with tool using abilities by fine-tuning with `dataset: glaive_toolcall_en`.

[23/12/23] We supported **[unsloth](https://github.com/unslothai/unsloth)**&#039;s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try `use_unsloth: true` argument to activate unsloth patch. It achieves **170%** speed in our benchmark, check [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison) for details.

[23/12/12] We supported fine-tuning the latest MoE model **[Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)** in our framework. See hardware requirement [here](#hardware-requirement).

[23/12/01] We supported downloading pre-trained models and datasets from the **[ModelScope Hub](https://modelscope.cn/models)**. See [this tutorial](#download-from-modelscope-hub

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[bytedance/trae-agent]]></title>
            <link>https://github.com/bytedance/trae-agent</link>
            <guid>https://github.com/bytedance/trae-agent</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:10 GMT</pubDate>
            <description><![CDATA[Trae Agent is an LLM-based agent for general purpose software engineering tasks.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/bytedance/trae-agent">bytedance/trae-agent</a></h1>
            <p>Trae Agent is an LLM-based agent for general purpose software engineering tasks.</p>
            <p>Language: Python</p>
            <p>Stars: 10,883</p>
            <p>Forks: 1,162</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre># Trae Agent

[![arXiv:2507.23370](https://img.shields.io/badge/TechReport-arXiv%3A2507.23370-b31a1b)](https://arxiv.org/abs/2507.23370)
[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Pre-commit](https://github.com/bytedance/trae-agent/actions/workflows/pre-commit.yml/badge.svg)](https://github.com/bytedance/trae-agent/actions/workflows/pre-commit.yml)
[![Unit Tests](https://github.com/bytedance/trae-agent/actions/workflows/unit-test.yml/badge.svg)](https://github.com/bytedance/trae-agent/actions/workflows/unit-test.yml)
[![Discord](https://img.shields.io/discord/1320998163615846420?label=Join%20Discord&amp;color=7289DA)](https://discord.gg/VwaQ4ZBHvC)

**Trae Agent** is an LLM-based agent for general purpose software engineering tasks. It provides a powerful CLI interface that can understand natural language instructions and execute complex software engineering workflows using various tools and LLM providers.

For technical details please refer to [our technical report](https://arxiv.org/abs/2507.23370).

**Project Status:** The project is still being actively developed. Please refer to [docs/roadmap.md](docs/roadmap.md) and [CONTRIBUTING](CONTRIBUTING.md) if you are willing to help us improve Trae Agent.

**Difference with Other CLI Agents:** Trae Agent offers a transparent, modular architecture that researchers and developers can easily modify, extend, and analyze, making it an ideal platform for **studying AI agent architectures, conducting ablation studies, and developing novel agent capabilities**. This **_research-friendly design_** enables the academic and open-source communities to contribute to and build upon the foundational agent framework, fostering innovation in the rapidly evolving field of AI agents.

## âœ¨ Features

- ğŸŒŠ **Lakeview**: Provides short and concise summarisation for agent steps
- ğŸ¤– **Multi-LLM Support**: Works with OpenAI, Anthropic, Doubao, Azure, OpenRouter, Ollama and Google Gemini APIs
- ğŸ› ï¸ **Rich Tool Ecosystem**: File editing, bash execution, sequential thinking, and more
- ğŸ¯ **Interactive Mode**: Conversational interface for iterative development
- ğŸ“Š **Trajectory Recording**: Detailed logging of all agent actions for debugging and analysis
- âš™ï¸ **Flexible Configuration**: YAML-based configuration with environment variable support
- ğŸš€ **Easy Installation**: Simple pip-based installation

## ğŸš€ Installation

### Requirements
- UV (https://docs.astral.sh/uv/)
- API key for your chosen provider (OpenAI, Anthropic, Google Gemini, OpenRouter, etc.)

### Setup

```bash
git clone https://github.com/bytedance/trae-agent.git
cd trae-agent
uv sync --all-extras
source .venv/bin/activate
```

## âš™ï¸ Configuration

### YAML Configuration (Recommended)

1. Copy the example configuration file:
   ```bash
   cp trae_config.yaml.example trae_config.yaml
   ```

2. Edit `trae_config.yaml` with your API credentials and preferences:

```yaml
agents:
  trae_agent:
    enable_lakeview: true
    model: trae_agent_model  # the model configuration name for Trae Agent
    max_steps: 200  # max number of agent steps
    tools:  # tools used with Trae Agent
      - bash
      - str_replace_based_edit_tool
      - sequentialthinking
      - task_done

model_providers:  # model providers configuration
  anthropic:
    api_key: your_anthropic_api_key
    provider: anthropic
  openai:
    api_key: your_openai_api_key
    provider: openai

models:
  trae_agent_model:
    model_provider: anthropic
    model: claude-sonnet-4-20250514
    max_tokens: 4096
    temperature: 0.5
```

**Note:** The `trae_config.yaml` file is ignored by git to protect your API keys.

### Using Base URL
In some cases, we need to use a custom URL for the api. Just add the `base_url` field after `provider`, take the following config as an example:

```
openai:
    api_key: your_openrouter_api_key
    provider: openai
    base_url: https://openrouter.ai/api/v1
```
**Note:** For field formatting, use spaces only. Tabs (\t) are not allowed.

### Environment Variables (Alternative)

You can also configure API keys using environment variables and store them in the .env file:

```bash
export OPENAI_API_KEY=&quot;your-openai-api-key&quot;
export OPENAI_BASE_URL=&quot;your-openai-base-url&quot;
export ANTHROPIC_API_KEY=&quot;your-anthropic-api-key&quot;
export ANTHROPIC_BASE_URL=&quot;your-anthropic-base-url&quot;
export GOOGLE_API_KEY=&quot;your-google-api-key&quot;
export GOOGLE_BASE_URL=&quot;your-google-base-url&quot;
export OPENROUTER_API_KEY=&quot;your-openrouter-api-key&quot;
export OPENROUTER_BASE_URL=&quot;https://openrouter.ai/api/v1&quot;
export DOUBAO_API_KEY=&quot;your-doubao-api-key&quot;
export DOUBAO_BASE_URL=&quot;https://ark.cn-beijing.volces.com/api/v3/&quot;
```

### MCP Services (Optional)

To enable Model Context Protocol (MCP) services, add an `mcp_servers` section to your configuration:

```yaml
mcp_servers:
  playwright:
    command: npx
    args:
      - &quot;@playwright/mcp@0.0.27&quot;
```

**Configuration Priority:** Command-line arguments &gt; Configuration file &gt; Environment variables &gt; Default values

**Legacy JSON Configuration:** If using the older JSON format, see [docs/legacy_config.md](docs/legacy_config.md). We recommend migrating to YAML.

## ğŸ“– Usage

### Basic Commands

```bash
# Simple task execution
trae-cli run &quot;Create a hello world Python script&quot;

# Check configuration
trae-cli show-config

# Interactive mode
trae-cli interactive
```

### Provider-Specific Examples

```bash
# OpenAI
trae-cli run &quot;Fix the bug in main.py&quot; --provider openai --model gpt-4o

# Anthropic
trae-cli run &quot;Add unit tests&quot; --provider anthropic --model claude-sonnet-4-20250514

# Google Gemini
trae-cli run &quot;Optimize this algorithm&quot; --provider google --model gemini-2.5-flash

# OpenRouter (access to multiple providers)
trae-cli run &quot;Review this code&quot; --provider openrouter --model &quot;anthropic/claude-3-5-sonnet&quot;
trae-cli run &quot;Generate documentation&quot; --provider openrouter --model &quot;openai/gpt-4o&quot;

# Doubao
trae-cli run &quot;Refactor the database module&quot; --provider doubao --model doubao-seed-1.6

# Ollama (local models)
trae-cli run &quot;Comment this code&quot; --provider ollama --model qwen3
```

### Advanced Options

```bash
# Custom working directory
trae-cli run &quot;Add tests for utils module&quot; --working-dir /path/to/project

# Save execution trajectory
trae-cli run &quot;Debug authentication&quot; --trajectory-file debug_session.json

# Force patch generation
trae-cli run &quot;Update API endpoints&quot; --must-patch

# Interactive mode with custom settings
trae-cli interactive --provider openai --model gpt-4o --max-steps 30
```

## Docker Mode Commands
### Preparation
**Important**: You need to make sure Docker is configured in your environment.

### Usage
```bash
# Specify a Docker image to run the task in a new container
trae-cli run &quot;Add tests for utils module&quot; --docker-image python:3.11

# Specify a Docker image to run the task in a new container and mount the directory
trae-cli run &quot;write a script to print helloworld&quot; --docker-image python:3.12 --working-dir test_workdir/

# Attach to an existing Docker container by ID (`--working-dir` is invalid with `--docker-container-id`)
trae-cli run &quot;Update API endpoints&quot; --docker-container-id 91998a56056c

# Specify an absolute path to a Dockerfile to build an environment
trae-cli run &quot;Debug authentication&quot; --dockerfile-path test_workspace/Dockerfile

# Specify a path to a local Docker image file (tar archive) to load
trae-cli run &quot;Fix the bug in main.py&quot; --docker-image-file test_workspace/trae_agent_custom.tar

# Remove the Docker container after finishing the task (keep default)
trae-cli run &quot;Add tests for utils module&quot; --docker-image python:3.11 --docker-keep false
```

### Interactive Mode Commands

In interactive mode, you can use:
- Type any task description to execute it
- `status` - Show agent information
- `help` - Show available commands
- `clear` - Clear the screen
- `exit` or `quit` - End the session

## ğŸ› ï¸ Advanced Features

### Available Tools

Trae Agent provides a comprehensive toolkit for software engineering tasks including file editing, bash execution, structured thinking, and task completion. For detailed information about all available tools and their capabilities, see [docs/tools.md](docs/tools.md).

### Trajectory Recording

Trae Agent automatically records detailed execution trajectories for debugging and analysis:

```bash
# Auto-generated trajectory file
trae-cli run &quot;Debug the authentication module&quot;
# Saves to: trajectories/trajectory_YYYYMMDD_HHMMSS.json

# Custom trajectory file
trae-cli run &quot;Optimize database queries&quot; --trajectory-file optimization_debug.json
```

Trajectory files contain LLM interactions, agent steps, tool usage, and execution metadata. For more details, see [docs/TRAJECTORY_RECORDING.md](docs/TRAJECTORY_RECORDING.md).

## ğŸ”§ Development

### Contributing

For contribution guidelines, please refer to [CONTRIBUTING.md](CONTRIBUTING.md).

### Troubleshooting

**Import Errors:**
```bash
PYTHONPATH=. trae-cli run &quot;your task&quot;
```

**API Key Issues:**
```bash
# Verify API keys
echo $OPENAI_API_KEY
trae-cli show-config
```

**Command Not Found:**
```bash
uv run trae-cli run &quot;your task&quot;
```

**Permission Errors:**
```bash
chmod +x /path/to/your/project
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## âœï¸ Citation

```bibtex
@article{traeresearchteam2025traeagent,
      title={Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling},
      author={Trae Research Team and Pengfei Gao and Zhao Tian and Xiangxin Meng and Xinchen Wang and Ruida Hu and Yuanan Xiao and Yizhou Liu and Zhao Zhang and Junjie Chen and Cuiyun Gao and Yun Lin and Yingfei Xiong and Chao Peng and Xia Liu},
      year={2025},
      eprint={2507.23370},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2507.23370},
}
```

## ğŸ™ Acknowledgments

We thank Anthropic for building the [anthropic-quickstart](https://github.com/anthropics/anthropic-quickstarts) project that served as a valuable reference for the tool ecosystem.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[anthropics/skills]]></title>
            <link>https://github.com/anthropics/skills</link>
            <guid>https://github.com/anthropics/skills</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:09 GMT</pubDate>
            <description><![CDATA[Public repository for Agent Skills]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/anthropics/skills">anthropics/skills</a></h1>
            <p>Public repository for Agent Skills</p>
            <p>Language: Python</p>
            <p>Stars: 78,459</p>
            <p>Forks: 8,182</p>
            <p>Stars today: 1,405 stars today</p>
            <h2>README</h2><pre>&gt; **Note:** This repository contains Anthropic&#039;s implementation of skills for Claude. For information about the Agent Skills standard, see [agentskills.io](http://agentskills.io).

# Skills
Skills are folders of instructions, scripts, and resources that Claude loads dynamically to improve performance on specialized tasks. Skills teach Claude how to complete specific tasks in a repeatable way, whether that&#039;s creating documents with your company&#039;s brand guidelines, analyzing data using your organization&#039;s specific workflows, or automating personal tasks.

For more information, check out:
- [What are skills?](https://support.claude.com/en/articles/12512176-what-are-skills)
- [Using skills in Claude](https://support.claude.com/en/articles/12512180-using-skills-in-claude)
- [How to create custom skills](https://support.claude.com/en/articles/12512198-creating-custom-skills)
- [Equipping agents for the real world with Agent Skills](https://anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills)

# About This Repository

This repository contains skills that demonstrate what&#039;s possible with Claude&#039;s skills system. These skills range from creative applications (art, music, design) to technical tasks (testing web apps, MCP server generation) to enterprise workflows (communications, branding, etc.).

Each skill is self-contained in its own folder with a `SKILL.md` file containing the instructions and metadata that Claude uses. Browse through these skills to get inspiration for your own skills or to understand different patterns and approaches.

Many skills in this repo are open source (Apache 2.0). We&#039;ve also included the document creation &amp; editing skills that power [Claude&#039;s document capabilities](https://www.anthropic.com/news/create-files) under the hood in the [`skills/docx`](./skills/docx), [`skills/pdf`](./skills/pdf), [`skills/pptx`](./skills/pptx), and [`skills/xlsx`](./skills/xlsx) subfolders. These are source-available, not open source, but we wanted to share these with developers as a reference for more complex skills that are actively used in a production AI application.

## Disclaimer

**These skills are provided for demonstration and educational purposes only.** While some of these capabilities may be available in Claude, the implementations and behaviors you receive from Claude may differ from what is shown in these skills. These skills are meant to illustrate patterns and possibilities. Always test skills thoroughly in your own environment before relying on them for critical tasks.

# Skill Sets
- [./skills](./skills): Skill examples for Creative &amp; Design, Development &amp; Technical, Enterprise &amp; Communication, and Document Skills
- [./spec](./spec): The Agent Skills specification
- [./template](./template): Skill template

# Try in Claude Code, Claude.ai, and the API

## Claude Code
You can register this repository as a Claude Code Plugin marketplace by running the following command in Claude Code:
```
/plugin marketplace add anthropics/skills
```

Then, to install a specific set of skills:
1. Select `Browse and install plugins`
2. Select `anthropic-agent-skills`
3. Select `document-skills` or `example-skills`
4. Select `Install now`

Alternatively, directly install either Plugin via:
```
/plugin install document-skills@anthropic-agent-skills
/plugin install example-skills@anthropic-agent-skills
```

After installing the plugin, you can use the skill by just mentioning it. For instance, if you install the `document-skills` plugin from the marketplace, you can ask Claude Code to do something like: &quot;Use the PDF skill to extract the form fields from `path/to/some-file.pdf`&quot;

## Claude.ai

These example skills are all already available to paid plans in Claude.ai. 

To use any skill from this repository or upload custom skills, follow the instructions in [Using skills in Claude](https://support.claude.com/en/articles/12512180-using-skills-in-claude#h_a4222fa77b).

## Claude API

You can use Anthropic&#039;s pre-built skills, and upload custom skills, via the Claude API. See the [Skills API Quickstart](https://docs.claude.com/en/api/skills-guide#creating-a-skill) for more.

# Creating a Basic Skill

Skills are simple to create - just a folder with a `SKILL.md` file containing YAML frontmatter and instructions. You can use the **template-skill** in this repository as a starting point:

```markdown
---
name: my-skill-name
description: A clear description of what this skill does and when to use it
---

# My Skill Name

[Add your instructions here that Claude will follow when this skill is active]

## Examples
- Example usage 1
- Example usage 2

## Guidelines
- Guideline 1
- Guideline 2
```

The frontmatter requires only two fields:
- `name` - A unique identifier for your skill (lowercase, hyphens for spaces)
- `description` - A complete description of what the skill does and when to use it

The markdown content below contains the instructions, examples, and guidelines that Claude will follow. For more details, see [How to create custom skills](https://support.claude.com/en/articles/12512198-creating-custom-skills).

# Partner Skills

Skills are a great way to teach Claude how to get better at using specific pieces of software. As we see awesome example skills from partners, we may highlight some of them here:

- **Notion** - [Notion Skills for Claude](https://www.notion.so/notiondevs/Notion-Skills-for-Claude-28da4445d27180c7af1df7d8615723d0)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[modelscope/ms-swift]]></title>
            <link>https://github.com/modelscope/ms-swift</link>
            <guid>https://github.com/modelscope/ms-swift</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:08 GMT</pubDate>
            <description><![CDATA[Use PEFT or Full-parameter to CPT/SFT/DPO/GRPO 600+ LLMs (Qwen3.5, DeepSeek-R1, GLM4.5, InternLM3, Llama4, ...) and 300+ MLLMs (Qwen3-VL, Qwen3-Omni, InternVL3.5, Ovis2.5, GLM4.5v, Llava, Phi4, ...) (AAAI 2025).]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/modelscope/ms-swift">modelscope/ms-swift</a></h1>
            <p>Use PEFT or Full-parameter to CPT/SFT/DPO/GRPO 600+ LLMs (Qwen3.5, DeepSeek-R1, GLM4.5, InternLM3, Llama4, ...) and 300+ MLLMs (Qwen3-VL, Qwen3-Omni, InternVL3.5, Ovis2.5, GLM4.5v, Llava, Phi4, ...) (AAAI 2025).</p>
            <p>Language: Python</p>
            <p>Stars: 12,799</p>
            <p>Forks: 1,218</p>
            <p>Stars today: 26 stars today</p>
            <h2>README</h2><pre># SWIFT (Scalable lightWeight Infrastructure for Fine-Tuning)

&lt;p align=&quot;center&quot;&gt;
    &lt;br&gt;
    &lt;img src=&quot;asset/banner.png&quot;/&gt;
    &lt;br&gt;
&lt;p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://modelscope.cn/home&quot;&gt;ModelScope Community Website&lt;/a&gt;
&lt;br&gt;
        &lt;a href=&quot;README_CN.md&quot;&gt;ä¸­æ–‡&lt;/a&gt; &amp;nbsp ï½œ &amp;nbsp English &amp;nbsp
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://img.shields.io/badge/python-3.11-5be.svg&quot;&gt;
&lt;img src=&quot;https://img.shields.io/badge/pytorch-%E2%89%A52.0-orange.svg&quot;&gt;
&lt;a href=&quot;https://github.com/modelscope/modelscope/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/modelscope-%E2%89%A51.23-5D91D4.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://pypi.org/project/ms-swift/&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/ms-swift.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/modelscope/ms-swift/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/modelscope/ms-swift&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://pepy.tech/project/ms-swift&quot;&gt;&lt;img src=&quot;https://pepy.tech/badge/ms-swift&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/modelscope/ms-swift/pulls&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PR-welcome-55EB99.svg&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/6427&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/6427&quot; alt=&quot;modelscope%2Fswift | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://arxiv.org/abs/2408.05517&quot;&gt;Paper&lt;/a&gt; &amp;nbsp ï½œ &lt;a href=&quot;https://swift.readthedocs.io/en/latest/&quot;&gt;English Documentation&lt;/a&gt; &amp;nbsp ï½œ &amp;nbsp &lt;a href=&quot;https://swift.readthedocs.io/zh-cn/latest/&quot;&gt;ä¸­æ–‡æ–‡æ¡£&lt;/a&gt; &amp;nbsp
&lt;/p&gt;

## ğŸ“– Table of Contents
- [Groups](#-Groups)
- [Introduction](#-introduction)
- [News](#-news)
- [Installation](#%EF%B8%8F-installation)
- [Quick Start](#-quick-Start)
- [Usage](#-Usage)
- [License](#-License)
- [Citation](#-citation)


## â˜ Groups

You can contact us and communicate with us by adding our group:


[Discord Group](https://discord.com/invite/D27yfEFVz5)              |  WeChat Group
:-------------------------:|:-------------------------:
&lt;img src=&quot;asset/discord_qr.jpg&quot; width=&quot;200&quot; height=&quot;200&quot;&gt;  |  &lt;img src=&quot;asset/wechat.png&quot; width=&quot;200&quot; height=&quot;200&quot;&gt;


## ğŸ“ Introduction
ğŸ² **ms-swift** is a large model and multimodal large model fine-tuning and deployment framework provided by the ModelScope community. It now supports training (pre-training, fine-tuning, human alignment), inference, evaluation, quantization, and deployment for 600+ text-only large models and 300+ multimodal large models. Large models include: Qwen3, Qwen3-Next, InternLM3, GLM4.5, Mistral, DeepSeek-R1, Llama4, etc. Multimodal large models include: Qwen3-VL, Qwen3-Omni, Llava, InternVL3.5, MiniCPM-V-4, Ovis2.5, GLM4.5-V, DeepSeek-VL2, etc.

ğŸ” In addition, ms-swift integrates the latest training technologies, including Megatron parallelism techniques such as TP, PP, CP, EP to accelerate training, as well as numerous GRPO algorithm family reinforcement learning algorithms including: GRPO, DAPO, GSPO, SAPO, CISPO, RLOO, Reinforce++, etc. to enhance model intelligence. ms-swift supports a wide range of training tasks, including preference learning algorithms such as DPO, KTO, RM, CPO, SimPO, ORPO, as well as Embedding, Reranker, and sequence classification tasks. ms-swift provides full-pipeline support for large model training, including acceleration for inference, evaluation, and deployment modules using vLLM, SGLang, and LMDeploy, as well as model quantization using GPTQ, AWQ, BNB, and FP8 technologies.

**Why Choose ms-swift?**

- ğŸ **Model Types**: Supports **600+ text-only large models**, **300+ multimodal large models**, and All-to-All full modality models from training to deployment full pipeline, with Day-0 support for popular models.
- **Dataset Types**: Built-in 150+ datasets for pre-training, fine-tuning, human alignment, multimodal and various other tasks, with support for custom datasets. Users only need to prepare datasets for one-click training.
- **Hardware Support**: Supports A10/A100/H100, RTX series, T4/V100, CPU, MPS, and domestic hardware Ascend NPU, etc.
- **Lightweight Training**: Supports lightweight fine-tuning methods such as LoRA, QLoRA, DoRA, LoRA+, LLaMAPro, LongLoRA, LoRA-GA, ReFT, RS-LoRA, Adapter, LISA, etc.
- **Quantized Training**: Supports training on BNB, AWQ, GPTQ, AQLM, HQQ, EETQ quantized models, requiring only 9GB training resources for 7B models.
- **Memory Optimization**: GaLore, Q-Galore, UnSloth, Liger-Kernel, Flash-Attention 2/3, and **Ulysses and Ring-Attention sequence parallelism techniques** support, reducing memory consumption for long-text training.
- **Distributed Training**: Supports distributed data parallelism (DDP), device_map simple model parallelism, DeepSpeed ZeRO2 ZeRO3, FSDP/FSDP2, and Megatron distributed training technologies.
- ğŸ“ **Multimodal Training**: Supports multimodal packing technology to improve training speed by 100%+, supports mixed modality data training with text, images, video and audio, and supports independent control of vit/aligner/llm.
- **Agent Training**: Supports Agent templates, allowing one dataset to be used for training different models.
- ğŸŠ **Training Tasks**: Supports pre-training and instruction fine-tuning, as well as training tasks such as DPO, GKD, KTO, RM, CPO, SimPO, ORPO, and supports **Embedding/Reranker** and sequence classification tasks.
- ğŸ¥¥ **Megatron Parallelism**: Provides TP/PP/SP/CP/ETP/EP/VPP parallel strategies, **MoE model acceleration up to 10x**. Supports full-parameter and LoRA training methods for 250+ text-only large models and 100+ multimodal large models. Supports CPT/SFT/GRPO/DPO/KTO/RM training tasks.
- ğŸ‰ **Reinforcement Learning**: Built-in **rich GRPO family algorithms**, including GRPO, DAPO, GSPO, SAPO, CISPO, CHORD, RLOO, Reinforce++, etc. Supports synchronous and asynchronous vLLM engine inference acceleration, with extensible reward functions, multi-turn inference Schedulers, and environments through plugins.
- **Full-Pipeline Capabilities**: Covers the entire workflow of training, inference, evaluation, quantization, and deployment.
- **UI Training**: Provides Web-UI interface for training, inference, evaluation, and quantization, completing the full pipeline for large models.
- **Inference Acceleration**: Supports Transformers, vLLM, SGLang, and LmDeploy inference acceleration engines, providing OpenAI interfaces for accelerating inference, deployment, and evaluation modules.
- **Model Evaluation**: Uses EvalScope as the evaluation backend, supporting 100+ evaluation datasets for evaluating text-only and multimodal models.
- **Model Quantization**: Supports quantization export for AWQ, GPTQ, FP8, and BNB. Exported models support inference acceleration using vLLM/SGLang/LmDeploy.


## ğŸ‰ News
- ğŸ 2026.01.15: **ms-swift v4.0** major version update is in progress. It is recommended to use the stable branch [release/3.12](https://github.com/modelscope/ms-swift/tree/release/3.12). You can provide your feedback in [this issue](https://github.com/modelscope/ms-swift/issues/7250). Thank you for your support.
- ğŸ 2025.11.14: Megatron GRPO is now available!  Check out the [docs](./docs/source_en/Megatron-SWIFT/GRPO.md) and [examples](examples/megatron/grpo).
- ğŸ 2025.11.04: Support for [Mcore-Bridge](docs/source_en/Megatron-SWIFT/Mcore-Bridge.md), making Megatron training as simple and easy to use as transformers.
- ğŸ 2025.10.28: Ray [here](docs/source_en/Instruction/Ray.md).
- ğŸ 2025.09.07: Added support for CHORD training algorithm. See the [documentation](./docs/source_en/Instruction/GRPO/AdvancedResearch/CHORD.md).
- ğŸ 2025.09.06: Ulysses can now be used with ring-attention, allowing sequences to be sharded into any number of chunks (no longer limited by the number of heads). The argument remains `--sequence_parallel_size N`.
- ğŸ 2025.09.02: Megatron-SWIFT now supports multimodal model training. Documentation can be found [here](./docs/source_en/Megatron-SWIFT/Multimodal-Model.md).
- ğŸ 2025.08.12: Support [Dynamic Fine-Tuning](https://arxiv.org/abs/2508.05629)(DFT) in SFT training, use parameter `--enable_dft_loss true`. Training scripts can be found [here](https://github.com/modelscope/ms-swift/blob/main/examples/train/full/dft.sh).
- ğŸ 2025.07.09: Megatron-SWIFT supports LoRA training. Compared to ms-swift, it achieves significant speedup on MoE models. Training scripts can be found [here](https://github.com/modelscope/ms-swift/blob/main/examples/megatron/lora).
- ğŸ 2025.06.23: Fine-tuning of reranker models is supported. Training scripts can be found here: [Reranker](https://github.com/modelscope/ms-swift/blob/main/examples/train/reranker/train_reranker.sh).
- ğŸ 2025.06.15: Support for GKD training on both pure text large models and multimodal models. Training scripts can be found here: [Pure Text](https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/gkd), [Multimodal](https://github.com/modelscope/ms-swift/blob/main/examples/train/multimodal/rlhf/gkd).

&lt;details&gt;&lt;summary&gt;More&lt;/summary&gt;

- ğŸ 2025.06.11: Support for using Megatron parallelism techniques for RLHF training. The training script can be found [here](https://github.com/modelscope/ms-swift/tree/main/examples/megatron/rlhf).
- ğŸ 2025.05.29: Support sequence parallel in pretrain, sft, dpo and grpo, check script [here](https://github.com/modelscope/ms-swift/tree/main/examples/train/sequence_parallel).
- ğŸ 2025.05.11: GRPO now supports custom processing logic for reward models. See the GenRM example [here](./docs/source_en/Instruction/GRPO/DeveloperGuide/reward_model.md).
- ğŸ 2025.04.15: The ms-swift paper has been accepted by AAAI 2025. You can find the paper at [this link](https://ojs.aaai.org/index.php/AAAI/article/view/35383).
- ğŸ 2025.03.23: Multi-round GRPO is now supported for training multi-turn dialogue scenarios (e.g., agent tool calling). Please refer to the [doc](./docs/source_en/Instruction/GRPO/DeveloperGuide/multi_turn.md).
- ğŸ 2025.03.16: Support for Megatron&#039;s parallel training techniques is now available. Please see the [Megatron-SWIFT training documentation](https://swift.readthedocs.io/en/latest/Megatron-SWIFT/Quick-start.html).
- ğŸ 2025.03.15: Fine-tuning of embedding models for both pure text and multimodal models is supported. Please check the [training script](examples/train/embedding).
- ğŸ 2025.03.05: The hybrid mode for GRPO is supported, with a script for training a 72B model on 4 GPUs (4*80G) available [here](examples/train/grpo/internal/vllm_72b_4gpu.sh). Tensor parallelism with vllm is also supported, with the training script available [here](examples/train/grpo/internal).
- ğŸ 2025.02.21: The GRPO algorithm now supports LMDeploy, with the training script available [here](examples/train/grpo/internal/full_lmdeploy.sh). Additionally, the performance of the GRPO algorithm has been tested, achieving a training speed increase of up to 300% using various tricks. Please check the WanDB table [here](https://wandb.ai/tastelikefeet/grpo_perf_test?nw=nwuseryuzezyz).
- ğŸ 2025.02.21: The `swift sample` command is now supported. The reinforcement fine-tuning script can be found [here](docs/source_en/Instruction/Reinforced-Fine-tuning.md), and the large model API distillation sampling script is available [here](examples/sampler/distill/distill.sh).
- ğŸ”¥ 2025.02.12: Support for the GRPO (Group Relative Policy Optimization) training algorithm has been added. Documentation is available [here](docs/source_en/Instruction/GRPO/GetStarted/GRPO.md).
- ğŸ 2024.12.04: Major update to **ms-swift 3.0**. Please refer to the [release notes and changes](docs/source_en/Instruction/ReleaseNote3.0.md).

- ğŸ‰ 2024.08.12: The ms-swift paper has been published on arXiv and can be read [here](https://arxiv.org/abs/2408.05517).
- ğŸ”¥ 2024.08.05: Support for using [evalscope](https://github.com/modelscope/evalscope/) as a backend for evaluating large models and multimodal models.
- ğŸ”¥ 2024.07.29: Support for using [vllm](https://github.com/vllm-project/vllm) and [lmdeploy](https://github.com/InternLM/lmdeploy) to accelerate inference for large models and multimodal models. When performing infer/deploy/eval, you can specify `--infer_backend vllm/lmdeploy`.
- ğŸ”¥ 2024.07.24: Support for human preference alignment training for multimodal large models, including DPO/ORPO/SimPO/CPO/KTO/RM/PPO.
- ğŸ”¥ 2024.02.01: Support for Agent training! The training algorithm is derived from [this paper](https://arxiv.org/pdf/2309.00986.pdf).
&lt;/details&gt;

## ğŸ› ï¸ Installation
To install using pip:
```shell
pip install ms-swift -U
```

To install from source:
```shell
# pip install git+https://github.com/modelscope/ms-swift.git

git clone https://github.com/modelscope/ms-swift.git
cd ms-swift
# The main branch is for swift 4.x. To install swift 3.x, please run the following command:
# git checkout release/3.12
pip install -e .
```

Running Environment:

|              | Range        | Recommended         | Notes                                     |
|--------------|--------------|---------------------|-------------------------------------------|
| python       | &gt;=3.9        | 3.10/3.11                |                                           |
| cuda         |              | cuda12              | No need to install if using CPU, NPU, MPS |
| torch        | &gt;=2.0        | 2.8.0/2.9.1         |   torch2.9 [conv3d slow](https://swift.readthedocs.io/en/latest/BestPractices/Qwen3-VL-Best-Practice.html#environment-setup)       |
| transformers | &gt;=4.33       | 4.57.6              |                                           |
| modelscope   | &gt;=1.23       |                     |                                           |
| peft         | &gt;=0.11,&lt;0.19 |                     |                                           |
| flash_attn   |              | 2.8.3/3.0.0b1 |                                           |
| trl          | &gt;=0.15,&lt;0.29 | 0.28.0              | RLHF                                      |
| deepspeed    | &gt;=0.14       | 0.18.6              | Training                                  |
| vllm         | &gt;=0.5.1      | 0.11.0/0.15.1       | Inference/Deployment                      |
| sglang       | &gt;=0.4.6      |          | Inference/Deployment                      |
| lmdeploy     | &gt;=0.5   | 0.10.1                 | Inference/Deployment                      |
| evalscope    | &gt;=1.0       |                     | Evaluation                                |
| gradio       |              | 5.32.1              | Web-UI/App                                |

For more optional dependencies, you can refer to [here](https://github.com/modelscope/ms-swift/blob/main/requirements/install_all.sh).


## ğŸš€ Quick Start

10 minutes of self-cognition fine-tuning of Qwen2.5-7B-Instruct on a single 3090 GPU:

### Command Line Interface (Recommended)

```shell
# 22GB
CUDA_VISIBLE_DEVICES=0 \
swift sft \
    --model Qwen/Qwen2.5-7B-Instruct \
    --tuner_type lora \
    --dataset &#039;AI-ModelScope/alpaca-gpt4-data-zh#500&#039; \
              &#039;AI-ModelScope/alpaca-gpt4-data-en#500&#039; \
              &#039;swift/self-cognition#500&#039; \
    --torch_dtype bfloat16 \
    --num_train_epochs 1 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --learning_rate 1e-4 \
    --lora_rank 8 \
    --lora_alpha 32 \
    --target_modules all-linear \
    --gradient_accumulation_steps 16 \
    --eval_steps 50 \
    --save_steps 50 \
    --save_total_limit 2 \
    --logging_steps 5 \
    --max_length 2048 \
    --output_dir output \
    --system &#039;You are a helpful assistant.&#039; \
    --warmup_ratio 0.05 \
    --dataloader_num_workers 4 \
    --model_author swift \
    --model_name swift-robot
```

Tips:

- If you want to train with a custom dataset, you can refer to [this guide](https://swift.readthedocs.io/en/latest/Customization/Custom-dataset.html) to organize your dataset format and specify `--dataset &lt;dataset_path&gt;`.
- The `--model_author` and `--model_name` parameters are only effective when the dataset includes `swift/self-cognition`.
- To train with a different model, simply modify `--model &lt;model_id/model_path&gt;`.
- By default, ModelScope is used for downloading models and datasets. If you want to use HuggingFace, simply specify `--use_hf true`.

After training is complete, use the following command to infer with the trained weights:

- Here, `--adapters` should be replaced with the last checkpoint folder generated during training. Since the adapters folder contains the training parameter file `args.json`, there is no need to specify `--model`, `--system` separately; Swift will automatically read these parameters. To disable this behavior, you can set `--load_args false`.

```shell
# Using an interactive command line for inference.
CUDA_VISIBLE_DEVICES=0 \
swift infer \
    --adapters output/vx-xxx/checkpoint-xxx \
    --stream true \
    --temperature 0 \
    --max_new_tokens 2048

# merge-lora and use vLLM for inference acceleration
CUDA_VISIBLE_DEVICES=0 \
swift infer \
    --adapters output/vx-xxx/checkpoint-xxx \
    --stream true \
    --merge_lora true \
    --infer_backend vllm \
    --vllm_max_model_len 8192 \
    --temperature 0 \
    --max_new_tokens 2048
```

Finally, use the following command to push the model to ModelScope:

```shell
CUDA_VISIBLE_DEVICES=0 \
swift export \
    --adapters output/vx-xxx/checkpoint-xxx \
    --push_to_hub true \
    --hub_model_id &#039;&lt;your-model-id&gt;&#039; \
    --hub_token &#039;&lt;your-sdk-token&gt;&#039; \
    --use_hf false
```


### Web-UI
The Web-UI is a **zero-threshold** training and deployment interface solution based on Gradio interface technology. For more details, you can check [here](https://swift.readthedocs.io/en/latest/GetStarted/Web-UI.html).

```shell
SWIFT_UI_LANG=en swift web-ui
```

![image.png](./docs/resources/web-ui-en.jpg)

### Using Python

ms-swift also supports training and inference using Python. Below is pseudocode for training and inference. For more details, you can refer to [here](https://github.com/modelscope/ms-swift/blob/main/examples/notebook/qwen2_5-self-cognition/self-cognition-sft.ipynb).

Training:

```python
from peft import LoraConfig, get_peft_model
from swift import get_model_processor, get_template, load_dataset, EncodePreprocessor
from swift.trainers import Seq2SeqTrainer, Seq2SeqTrainingArguments
# Retrieve the model and template, and add a trainable LoRA module
model, tokenizer = get_model_processor(model_id_or_path, ...)
template = get_template(tokenizer, ...)
lora_config = LoraConfig(...)
model = get_peft_model(model, lora_config)

# Download and load the dataset, and encode the text into tokens
train_dataset, val_dataset = load_dataset(dataset_id_or_path, ...)
train_dataset = EncodePreprocessor(template=template)(train_dataset, num_proc=num_proc)
val_dataset = EncodePreprocessor(template=template)(val_dataset, num_proc=num_proc)

# Train the model
training_args = Seq2SeqTrainingArguments(...)
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    template=template,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)
trainer.train()
```
Inference:

```python
from swift import TransformersEngine, InferRequest, RequestConfig
# Perform inference using the native Transformers engine
engine = TransformersEngine(model_id_or_path, adapters=[lora_checkpoint])
infer_request = InferRequest(messages=[{&#039;role&#039;: &#039;user&#039;, &#039;content&#039;: &#039;who are you?&#039;}])
request_config = RequestConfig(max_tokens=max_new_tokens, temperature=temperature)

resp_list = engine.infer([infer_request], request_config)
print(f&#039;response: {resp_list[0].choices[0].message.content}&#039;)
```

## âœ¨ Usage
Here is a minimal example of training to deployment using ms-swift. For more details, you can check the [examples](https://github.com/modelscope/ms-swift/tree/main/examples).

- If you want to use other models or datasets (including multimodal models and datasets), you only need to modify `--model` to specify the corresponding model&#039;s ID or path, and modify `--dataset` to specify the 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[4thfever/cultivation-world-simulator]]></title>
            <link>https://github.com/4thfever/cultivation-world-simulator</link>
            <guid>https://github.com/4thfever/cultivation-world-simulator</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:07 GMT</pubDate>
            <description><![CDATA[åŸºäº AI Agent å·¥ä½œæµçš„ä¿®ä»™ä¸–ç•Œæ¨¡æ‹Ÿå™¨ï¼Œæ—¨åœ¨è¿˜åŸæ™ºèƒ½ã€å¼€æ”¾çš„ä»™ä¾ ä¸–ç•Œã€‚| An open-source Cultivation World Simulator using Agentic Workflow to create a dynamic, emerging Xianxia world.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/4thfever/cultivation-world-simulator">4thfever/cultivation-world-simulator</a></h1>
            <p>åŸºäº AI Agent å·¥ä½œæµçš„ä¿®ä»™ä¸–ç•Œæ¨¡æ‹Ÿå™¨ï¼Œæ—¨åœ¨è¿˜åŸæ™ºèƒ½ã€å¼€æ”¾çš„ä»™ä¾ ä¸–ç•Œã€‚| An open-source Cultivation World Simulator using Agentic Workflow to create a dynamic, emerging Xianxia world.</p>
            <p>Language: Python</p>
            <p>Stars: 1,151</p>
            <p>Forks: 138</p>
            <p>Stars today: 29 stars today</p>
            <h2>README</h2><pre>&lt;!-- è¯­è¨€ / Language --&gt;
&lt;h3 align=&quot;center&quot;&gt;
  &lt;a href=&quot;README.md&quot;&gt;ğŸ‡¨ğŸ‡³ ä¸­æ–‡&lt;/a&gt; Â· &lt;a href=&quot;EN_README.md&quot;&gt;ğŸ‡ºğŸ‡¸ English&lt;/a&gt;
&lt;/h3&gt;
&lt;blockquote align=&quot;center&quot;&gt;
  This project now supports English. Don&#039;t hesitate to give it a try!
&lt;/blockquote&gt;
&lt;p align=&quot;center&quot;&gt;â€” âœ¦ â€”&lt;/p&gt;

# ä¿®ä»™ä¸–ç•Œæ¨¡æ‹Ÿå™¨ (Cultivation World Simulator)

![GitHub stars](https://img.shields.io/github/stars/4thfever/cultivation-world-simulator?style=social)
[![Bilibili](https://img.shields.io/badge/Bilibili-%E6%9F%A5%E7%9C%8B%E8%A7%86%E9%A2%91-FB7299?logo=bilibili)](https://space.bilibili.com/527346837)
![QQ Group](https://img.shields.io/badge/QQ%E7%BE%A4-1071821688-deepskyblue?logo=tencent-qq&amp;logoColor=white)
[![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289da?logo=discord&amp;logoColor=white)](https://discord.gg/shhRWmZR)
[![License](https://img.shields.io/badge/license-CC%20BY--NC--SA%204.0-lightgrey)](LICENSE)
![Genre: Xianxia](https://img.shields.io/badge/Genre-Xianxia-red)
![AI Agent](https://img.shields.io/badge/AI-Agent-orange)

![Powered by LLM](https://img.shields.io/badge/Powered%20by-LLM-0077B5)
![Python](https://img.shields.io/badge/Python-3.10%2B-blue?style=flat&amp;logo=python&amp;logoColor=white)
![FastAPI](https://img.shields.io/badge/FastAPI-005571?style=flat&amp;logo=fastapi&amp;logoColor=white)
![Vue](https://img.shields.io/badge/Vue.js-3.x-4FC08D?style=flat&amp;logo=vuedotjs&amp;logoColor=white)
![TypeScript](https://img.shields.io/badge/typescript-%23007ACC.svg?style=flat&amp;logo=typescript&amp;logoColor=white)
![Vite](https://img.shields.io/badge/vite-%23646CFF.svg?style=flat&amp;logo=vite&amp;logoColor=white)
![PixiJS](https://img.shields.io/badge/PixiJS-E72264?style=flat&amp;logo=pixijs&amp;logoColor=white)


&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/zh-CN/screenshot.gif&quot; alt=&quot;æ¸¸æˆæ¼”ç¤º&quot; width=&quot;100%&quot;&gt;
&lt;/p&gt;

&gt; **ä¸€ä¸ªAIé©±åŠ¨çš„ä¿®ä»™ä¸–ç•Œæ¨¡æ‹Ÿå™¨ï¼Œæ—¨åœ¨åˆ›é€ ä¸€ä¸ªçœŸæ­£æ´»ç€çš„ã€æœ‰æ²‰æµ¸æ„Ÿçš„ä»™ä¾ ä¸–ç•Œã€‚**

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://hellogithub.com/repository/4thfever/cultivation-world-simulator&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://api.hellogithub.com/v1/widgets/recommend.svg?rid=d0d75240fb95445bba1d7af7574d8420&amp;claim_uid=DogxfCROM1PBL89&quot; alt=&quot;Featuredï½œHelloGitHub&quot; style=&quot;width: 250px; height: 54px;&quot; width=&quot;250&quot; height=&quot;54&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## ğŸ“– ç®€ä»‹

è¿™æ˜¯ä¸€ä¸ª **AI é©±åŠ¨çš„å¼€æ”¾ä¸–ç•Œä¿®ä»™æ¨¡æ‹Ÿå™¨**ã€‚

ä¸åŒäºä¼ ç»Ÿçš„ RPG æ‰®æ¼”æŸä¸ªè§’è‰²ï¼Œåœ¨è¿™é‡Œï¼Œ**ä½ å°†æ‰®æ¼”â€œå¤©é“â€ï¼ˆä¸Šå¸ï¼‰**ã€‚
ä½ ä¸éœ€è¦äº²è‡ªæ‰“æ€ªå‡çº§ï¼Œè€Œæ˜¯ä»¥ä¸Šå¸è§†è§’è§‚å¯Ÿä¼—ç”Ÿï¼Œåœ¨è§„åˆ™ä¸ AI å…±åŒç¼–ç»‡çš„å¼€æ”¾ä¸–ç•Œä¸­ï¼Œè§è¯é—¨æ´¾å…´è¡°ä¸å¤©éª„å´›èµ·ã€‚ä½ å¯ä»¥é™è§‚æ²§æµ·æ¡‘ç”°ï¼Œä¹Ÿå¯ä»¥é™ä¸‹å¤©åŠ«æˆ–é­”æ”¹å¿ƒçµï¼Œå¾®å¦™åœ°å¹²é¢„ä¸–ç•Œè¿›ç¨‹ã€‚

### âœ¨ æ ¸å¿ƒäº®ç‚¹

- ğŸ‘ï¸ **æ‰®æ¼”â€œå¤©é“â€ (ä¸Šå¸è§†è§’)**ï¼šä½ ä¸æ˜¯ä¿®å£«ï¼Œè€Œæ˜¯æŒæ§ä¸–ç•Œè§„åˆ™çš„**å¤©é“**ã€‚è§‚å¯Ÿä¼—ç”Ÿç™¾æ€ï¼Œä½“å‘³è‹¦è¾£é…¸ç”œã€‚
- ğŸ¤– **å…¨å‘˜ AI é©±åŠ¨**ï¼šæ¯ä¸ª NPC éƒ½ç‹¬ç«‹åŸºäºLLMé©±åŠ¨ï¼Œéƒ½æœ‰ç‹¬ç«‹çš„æ€§æ ¼ã€è®°å¿†ã€äººé™…å…³ç³»å’Œè¡Œä¸ºé€»è¾‘ã€‚ä»–ä»¬ä¼šæ ¹æ®å³æ—¶å±€åŠ¿åšå‡ºå†³ç­–ï¼Œä¼šæœ‰çˆ±æ¨æƒ…ä»‡ï¼Œä¼šç»“å…šè¥ç§ï¼Œç”šè‡³ä¼šé€†å¤©æ”¹å‘½ã€‚
- ğŸŒ **è§„åˆ™ä½œä¸ºåŸºçŸ³**ï¼šä¸–ç•ŒåŸºäºçµæ ¹ã€å¢ƒç•Œã€åŠŸæ³•ã€å¯¿å…ƒç­‰ä¸¥è°¨çš„æ•°å€¼ä½“ç³»è¿è¡Œã€‚AI çš„æƒ³è±¡åŠ›è¢«é™åˆ¶åœ¨åˆç†çš„ä¿®ä»™é€»è¾‘æ¡†æ¶å†…ï¼Œç¡®ä¿ä¸–ç•ŒçœŸå®å¯ä¿¡ã€‚
- ğŸ¦‹ **æ¶Œç°å¼å‰§æƒ…**ï¼šå¼€å‘è€…ä¹Ÿä¸çŸ¥é“ä¸‹ä¸€ç§’ä¼šå‘ç”Ÿä»€ä¹ˆã€‚æ²¡æœ‰é¢„è®¾å‰§æœ¬ï¼Œåªæœ‰æ— æ•°å› æœäº¤ç»‡å‡ºçš„ä¸–ç•Œæ¼”å˜ã€‚å®—é—¨å¤§æˆ˜ã€æ­£é­”ä¹‹äº‰ã€å¤©éª„é™¨è½ï¼Œçš†ç”±ä¸–ç•Œé€»è¾‘è‡ªä¸»æ¨æ¼”ã€‚

&lt;table border=&quot;0&quot;&gt;
  &lt;tr&gt;
    &lt;td width=&quot;33%&quot; valign=&quot;top&quot;&gt;
      &lt;h4 align=&quot;center&quot;&gt;å®—é—¨ä½“ç³»&lt;/h4&gt;
      &lt;img src=&quot;assets/zh-CN/screenshots/å®—é—¨.png&quot; width=&quot;100%&quot; /&gt;
      &lt;br/&gt;&lt;br/&gt;
      &lt;h4 align=&quot;center&quot;&gt;åŸå¸‚åŒºåŸŸ&lt;/h4&gt;
      &lt;img src=&quot;assets/zh-CN/screenshots/åŸå¸‚.png&quot; width=&quot;100%&quot; /&gt;
      &lt;br/&gt;&lt;br/&gt;
      &lt;h4 align=&quot;center&quot;&gt;ç”Ÿå¹³ç»å†&lt;/h4&gt;
      &lt;img src=&quot;assets/zh-CN/screenshots/ç»å†.png&quot; width=&quot;100%&quot; /&gt;
    &lt;/td&gt;
    &lt;td width=&quot;33%&quot; valign=&quot;top&quot;&gt;
      &lt;h4 align=&quot;center&quot;&gt;è§’è‰²é¢æ¿&lt;/h4&gt;
      &lt;img src=&quot;assets/zh-CN/screenshots/è§’è‰².png&quot; width=&quot;100%&quot; /&gt;
      &lt;br/&gt;&lt;br/&gt;
      &lt;h4 align=&quot;center&quot;&gt;æ€§æ ¼ç‰¹è´¨&lt;/h4&gt;
      &lt;img src=&quot;assets/zh-CN/screenshots/ç‰¹è´¨.png&quot; width=&quot;100%&quot; /&gt;
      &lt;br/&gt;&lt;br/&gt;
      &lt;h4 align=&quot;center&quot;&gt;è‡ªä¸»æ€è€ƒ&lt;/h4&gt;
      &lt;img src=&quot;assets/zh-CN/screenshots/æ€è€ƒ.png&quot; width=&quot;100%&quot; /&gt;
      &lt;br/&gt;&lt;br/&gt;
      &lt;h4 align=&quot;center&quot;&gt;æ±Ÿæ¹–ç»°å·&lt;/h4&gt;
      &lt;img src=&quot;assets/zh-CN/screenshots/ç»°å·.png&quot; width=&quot;100%&quot; /&gt;
    &lt;/td&gt;
    &lt;td width=&quot;33%&quot; valign=&quot;top&quot;&gt;
      &lt;h4 align=&quot;center&quot;&gt;æ´åºœæ¢ç§˜&lt;/h4&gt;
      &lt;img src=&quot;assets/zh-CN/screenshots/æ´åºœ.png&quot; width=&quot;100%&quot; /&gt;
      &lt;br/&gt;&lt;br/&gt;
      &lt;h4 align=&quot;center&quot;&gt;é•¿çŸ­æœŸç›®æ ‡&lt;/h4&gt;
      &lt;img src=&quot;assets/zh-CN/screenshots/ç›®æ ‡.png&quot; width=&quot;100%&quot; /&gt;
      &lt;br/&gt;&lt;br/&gt;
      &lt;h4 align=&quot;center&quot;&gt;ä¸¹è¯/æ³•å®/æ­¦å™¨&lt;/h4&gt;
      &lt;img src=&quot;assets/zh-CN/screenshots/ä¸¹è¯.png&quot; width=&quot;100%&quot; /&gt;
      &lt;img src=&quot;assets/zh-CN/screenshots/æ³•å®.png&quot; width=&quot;100%&quot; /&gt;
      &lt;img src=&quot;assets/zh-CN/screenshots/æ­¦å™¨.png&quot; width=&quot;100%&quot; /&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

### ğŸ’­ ä¸ºä»€ä¹ˆè¦åšè¿™ä¸ªï¼Ÿ
ä¿®ä»™ç½‘æ–‡ä¸­çš„ä¸–ç•Œå¾ˆç²¾å½©ï¼Œä½†è¯»è€…æ°¸è¿œåªèƒ½è§‚å¯Ÿåˆ°ä¸€éš…ã€‚

ä¿®ä»™å“ç±»æ¸¸æˆè¦ä¹ˆæ˜¯å®Œå…¨çš„é¢„è®¾å‰§æœ¬ï¼Œè¦ä¹ˆä¾é äººå·¥è®¾è®¡çš„ç®€å•è§„åˆ™çŠ¶æ€æœºï¼Œæœ‰è®¸è®¸å¤šå¤šç‰µå¼ºå’Œé™æ™ºçš„è¡¨ç°ã€‚

åœ¨å¤§è¯­è¨€æ¨¡å‹å‡ºç°åï¼Œè®©â€œæ¯ä¸€ä¸ªè§’è‰²éƒ½æ˜¯é²œæ´»çš„â€çš„ç›®æ ‡å˜å¾—ä¼¼ä¹å¯ä»¥è§¦è¾¾äº†ã€‚

å¸Œæœ›èƒ½å¤Ÿåˆ›é€ å‡ºçº¯ç²¹çš„ã€å¿«ä¹çš„ã€ç›´æ¥çš„ã€æ´»ç€çš„ä¿®ä»™ä¸–ç•Œçš„æ²‰æµ¸æ„Ÿã€‚ä¸æ˜¯åƒä¸€äº›æ¸¸æˆå…¬å¸çš„çº¯ç²¹å®£ä¼ å·¥å…·ï¼Œä¹Ÿä¸æ˜¯åƒæ–¯å¦ç¦å°é•‡é‚£æ ·çš„çº¯ç²¹ç ”ç©¶ï¼Œè€Œæ˜¯èƒ½ç»™ç©å®¶æä¾›çœŸå®ä»£å…¥æ„Ÿå’Œæ²‰æµ¸æ„Ÿçš„å®é™…ä¸–ç•Œã€‚

## ğŸ“ è”ç³»æ–¹å¼
å¦‚æœæ‚¨å¯¹é¡¹ç›®æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿æäº¤ Issueã€‚

- **Bilibili**: [ç‚¹å‡»å…³æ³¨](https://space.bilibili.com/527346837)
- **QQç¾¤**: `1071821688` (å…¥ç¾¤ç­”æ¡ˆï¼šè‚¥æ¡¥ä»Šå¤©åƒä»€ä¹ˆ)
- **Discord**: [åŠ å…¥ç¤¾åŒº](https://discord.gg/shhRWmZR)

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼ä¸€ï¼šDocker ä¸€é”®éƒ¨ç½²ï¼ˆæ¨èï¼‰

æ— éœ€é…ç½®ç¯å¢ƒï¼Œç›´æ¥è¿è¡Œå³å¯ï¼š

```bash
git clone https://github.com/AI-Cultivation/cultivation-world-simulator.git
cd cultivation-world-simulator
docker-compose up -d --build
```

è®¿é—®åœ°å€ï¼šå‰ç«¯ `http://localhost:8123` | åç«¯ `http://localhost:8002`

### æ–¹å¼äºŒï¼šæºç éƒ¨ç½²ï¼ˆå¼€å‘æ¨¡å¼ï¼‰

é€‚åˆéœ€è¦ä¿®æ”¹ä»£ç æˆ–è°ƒè¯•çš„å¼€å‘è€…ã€‚

1. **ç¯å¢ƒå‡†å¤‡ä¸å¯åŠ¨**
   ```bash
   # 1. å®‰è£…åç«¯ä¾èµ–
   pip install -r requirements.txt

   # 2. å®‰è£…å‰ç«¯ä¾èµ– (éœ€ Node.js)
   cd web &amp;&amp; npm install &amp;&amp; cd ..

   # 3. å¯åŠ¨æœåŠ¡ (è‡ªåŠ¨æ‹‰èµ·å‰åç«¯)
   python src/server/main.py --dev
   ```

2. **æ¨¡å‹é…ç½®**
   æœåŠ¡å¯åŠ¨åï¼Œæµè§ˆå™¨ä¼šè‡ªåŠ¨æ‰“å¼€ã€‚**æ¨èç›´æ¥åœ¨å‰ç«¯è®¾ç½®é¡µé¢é€‰æ‹©é¢„è®¾ï¼ˆå¦‚ DeepSeek/Ollamaï¼‰**ï¼Œä¹Ÿå¯æ‰‹åŠ¨ä¿®æ”¹ `static/local_config.yml`ã€‚

---

### ğŸ“± é«˜çº§åŠŸèƒ½

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;å±€åŸŸç½‘/æ‰‹æœºè®¿é—®é…ç½® (ç‚¹å‡»å±•å¼€)&lt;/b&gt;&lt;/summary&gt;

&gt; âš ï¸ ç§»åŠ¨ç«¯ UI æš‚æœªå®Œå…¨é€‚é…ï¼Œä»…ä¾›å°é²œã€‚

1. **åç«¯é…ç½®**ï¼šä¿®æ”¹ `static/local_config.yml`ï¼Œæ·»åŠ  `host: &quot;0.0.0.0&quot;`ã€‚
2. **å‰ç«¯é…ç½®**ï¼šä¿®æ”¹ `web/vite.config.ts`ï¼Œåœ¨ server å—ä¸­æ·»åŠ  `host: &#039;0.0.0.0&#039;`ã€‚
3. **è®¿é—®æ–¹å¼**ï¼šç¡®ä¿æ‰‹æœºä¸ç”µè„‘åœ¨åŒä¸€ WiFi ä¸‹ï¼Œè®¿é—® `http://&lt;ç”µè„‘å±€åŸŸç½‘IP&gt;:5173`ã€‚

&lt;/details&gt;


## ğŸ“Š é¡¹ç›®çŠ¶æ€

![Repobeats analytics](https://repobeats.axiom.co/api/embed/91667dce0fca651a7427022b2d819d20dd17c5e3.svg &quot;Repobeats analytics image&quot;)

## â­ Star History

å¦‚æœä½ è§‰å¾—è¿™ä¸ªé¡¹ç›®æœ‰è¶£ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Star â­ï¼è¿™å°†æ¿€åŠ±æˆ‘ä»¬æŒç»­æ”¹è¿›å’Œæ·»åŠ æ–°åŠŸèƒ½ã€‚

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://star-history.com/#4thfever/cultivation-world-simulator&amp;Date&quot;&gt;
    &lt;img src=&quot;https://api.star-history.com/svg?repos=4thfever/cultivation-world-simulator&amp;type=Date&quot; alt=&quot;Star History Chart&quot; width=&quot;600&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

## ğŸ‘¥ è´¡çŒ®è€…

&lt;a href=&quot;https://github.com/4thfever/cultivation-world-simulator/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=4thfever/cultivation-world-simulator&amp;max=100&amp;columns=11&quot; /&gt;
&lt;/a&gt;

æ›´å¤šè´¡çŒ®ç»†èŠ‚è¯·æŸ¥çœ‹ [CONTRIBUTORS.md](CONTRIBUTORS.md)ã€‚

## ğŸ“‹ åŠŸèƒ½å¼€å‘è¿›åº¦

### ğŸ—ï¸ åŸºç¡€ç³»ç»Ÿ
- âœ… åŸºç¡€ä¸–ç•Œåœ°å›¾ã€æ—¶é—´ã€äº‹ä»¶ç³»ç»Ÿ
- âœ… å¤šæ ·åŒ–åœ°å½¢ç±»å‹ï¼ˆå¹³åŸã€å±±è„‰ã€æ£®æ—ã€æ²™æ¼ ã€æ°´åŸŸç­‰ï¼‰
- âœ… åŸºäºWebå‰ç«¯æ˜¾ç¤ºç•Œé¢
- âœ… åŸºç¡€æ¨¡æ‹Ÿå™¨æ¡†æ¶
- âœ… é…ç½®æ–‡ä»¶
- âœ… release ä¸€é”®å³ç©çš„exe
- âœ… èœå•æ  &amp; å­˜æ¡£ &amp; è¯»æ¡£
- âœ… çµæ´»è‡ªå®šä¹‰LLMæ¥å£
- âœ… æ”¯æŒmac os
- âœ… å¤šè¯­è¨€æœ¬åœ°åŒ–
- âœ… å¼€å§‹æ¸¸æˆé¡µ
- âœ… BGM &amp; éŸ³æ•ˆ

### ğŸ—ºï¸ ä¸–ç•Œç³»ç»Ÿ
- âœ… åŸºç¡€tileåœ°å—ç³»ç»Ÿ
- âœ… åŸºç¡€åŒºåŸŸã€ä¿®è¡ŒåŒºåŸŸã€åŸå¸‚åŒºåŸŸã€å®—é—¨åŒºåŸŸ
- âœ… åŒåœ°å—NPCäº¤äº’
- âœ… çµæ°”åˆ†å¸ƒä¸äº§å‡ºè®¾è®¡
- âœ… ä¸–ç•Œäº‹ä»¶
- âœ… å¤©åœ°äººæ¦œ

### ğŸ‘¤ è§’è‰²ç³»ç»Ÿ
- âœ… è§’è‰²åŸºç¡€å±æ€§ç³»ç»Ÿ
- âœ… ä¿®ç‚¼å¢ƒç•Œä½“ç³»
- âœ… çµæ ¹ç³»ç»Ÿ
- âœ… åŸºç¡€ç§»åŠ¨åŠ¨ä½œ
- âœ… è§’è‰²ç‰¹è´¨ä¸æ€§æ ¼
- âœ… å¢ƒç•Œçªç ´æœºåˆ¶
- âœ… è§’è‰²é—´çš„ç›¸äº’å…³ç³»
- âœ… è§’è‰²äº¤äº’èŒƒå›´
- âœ… è§’è‰²Effectsç³»ç»Ÿï¼šå¢ç›Š/å‡ç›Šæ•ˆæœ
- âœ… è§’è‰²åŠŸæ³•
- âœ… è§’è‰²å…µå™¨ &amp; è¾…åŠ©è£…å¤‡
- âœ… ä¸¹è¯
- âœ… è§’è‰²é•¿çŸ­æœŸè®°å¿†
- âœ… è§’è‰²çš„é•¿çŸ­æœŸç›®æ ‡ï¼Œæ”¯æŒç©å®¶ä¸»åŠ¨è®¾å®š
- âœ… è§’è‰²ç»°å·
- [ ] ç”Ÿæ´»æŠ€èƒ½
  - âœ… é“¸é€ 
  - âœ… ç‚¼ä¸¹
  - [ ] ç§æ¤
  - [ ] é¥²å…»
  - [ ] æŠ€èƒ½å¯å‡çº§
- âœ… å‡¡äºº
- [ ] åŒ–ç¥å¢ƒç•Œ

### ğŸ›ï¸ ç»„ç»‡
- âœ… å®—é—¨
  - âœ… è®¾å®šã€åŠŸæ³•ã€ç–—ä¼¤ã€é©»åœ°ã€è¡Œäº‹é£æ ¼
  - âœ… å®—é—¨ç‰¹æ®ŠåŠ¨ä½œï¼šåˆæ¬¢å®—ï¼ˆåŒä¿®ï¼‰ï¼Œç™¾å…½å®—ï¼ˆå¾¡å…½ï¼‰ç­‰
  - âœ… å®—é—¨ç­‰é˜¶
  - âœ… é“ç»Ÿ
- [ ] ä¸–å®¶
- [ ] æœå»·
- [ ] ç»„ç»‡æ„å¿—AI
- [ ] ç»„ç»‡ä»»åŠ¡ã€èµ„æºã€æœºèƒ½
- [ ] ç»„ç»‡é—´å…³ç³»ç½‘ç»œ

### âš¡ åŠ¨ä½œç³»ç»Ÿ
- âœ… åŸºç¡€ç§»åŠ¨åŠ¨ä½œ
- âœ… åŠ¨ä½œæ‰§è¡Œæ¡†æ¶
- âœ… æœ‰æ˜ç¡®è§„åˆ™çš„å®šä¹‰åŠ¨ä½œ
- âœ… é•¿åŠ¨ä½œæ‰§è¡Œå’Œç»“ç®—ç³»ç»Ÿ
  - âœ… æ”¯æŒå¤šæœˆä»½æŒç»­çš„åŠ¨ä½œï¼ˆå¦‚ä¿®ç‚¼ã€çªç ´ã€æ¸¸æˆç­‰ï¼‰
  - âœ… åŠ¨ä½œå®Œæˆæ—¶çš„è‡ªåŠ¨ç»“ç®—æœºåˆ¶
- âœ… å¤šäººåŠ¨ä½œï¼šåŠ¨ä½œå‘èµ·ä¸åŠ¨ä½œå“åº”
- âœ… å½±å“äººé™…å…³ç³»çš„LLMåŠ¨ä½œ
- âœ… ç³»ç»Ÿæ€§çš„åŠ¨ä½œæ³¨å†Œä¸è¿è¡Œé€»è¾‘

### ğŸ­ äº‹ä»¶ç³»ç»Ÿ
- âœ… å¤©åœ°çµæ°”å˜åŠ¨
- âœ… å¤šäººå¤§äº‹ä»¶ï¼š
  - âœ… æ‹å–ä¼š
  - âœ… ç§˜å¢ƒæ¢ç´¢
  - [ ] æ¯”æ­¦å¤§ä¼š
  - âœ… å®—é—¨ä¼ é“å¤§ä¼š
- [ ] çªå‘äº‹ä»¶
  - [ ] å®ç‰©/æ´åºœå‡ºä¸–
- [ ] è‡ªç„¶äº‹ä»¶ï¼š
  - [ ] è‡ªç„¶ç¾å®³
  - [ ] å…½æ½®

### âš”ï¸ æˆ˜æ–—ç³»ç»Ÿ
- âœ… ä¼˜åŠ£äº’å…‹å…³ç³»
- âœ… èƒœç‡è®¡ç®—ç³»ç»Ÿ

### ğŸ’ ç‰©å“ç³»ç»Ÿ
- âœ… åŸºç¡€ç‰©å“ã€çµçŸ³æ¡†æ¶
- âœ… ç‰©å“äº¤æ˜“æœºåˆ¶

### ğŸŒ¿ ç”Ÿæ€ç³»ç»Ÿ
- âœ… åŠ¨æ¤ç‰©
- âœ… ç‹©çŒã€é‡‡é›†ã€ææ–™ç³»ç»Ÿ
- [ ] é­”å…½

### ğŸ¤– AIå¢å¼ºç³»ç»Ÿ
- âœ… LLMæ¥å£é›†æˆ
- âœ… è§’è‰²AIç³»ç»Ÿï¼ˆè§„åˆ™AI + LLM AIï¼‰
- âœ… åç¨‹åŒ–å†³ç­–æœºåˆ¶ï¼Œå¼‚æ­¥è¿è¡Œï¼Œå¤šçº¿ç¨‹åŠ é€Ÿaiå†³ç­–
- âœ… é•¿æœŸè§„åˆ’å’Œç›®æ ‡å¯¼å‘è¡Œä¸º
- âœ… çªå‘åŠ¨ä½œå“åº”ç³»ç»Ÿï¼ˆå¯¹å¤–ç•Œåˆºæ¿€çš„å³æ—¶ååº”ï¼‰
- âœ… LLMé©±åŠ¨çš„NPCå¯¹è¯ã€æ€è€ƒã€äº’åŠ¨
- âœ… LLMç”Ÿæˆå°ç‰‡æ®µå‰§æƒ…
- âœ… æ ¹æ®ä»»åŠ¡éœ€æ±‚åˆ†åˆ«æ¥å…¥max/flashæ¨¡å‹
- âœ… å°å‰§åœº
  - âœ… æˆ˜æ–—å°å‰§åœº
  - âœ… å¯¹è¯å°å‰§åœº
  - âœ… å°å‰§åœºä¸åŒæ–‡å­—é£æ ¼
- âœ… ä¸€æ¬¡æ€§é€‰æ‹©ï¼ˆå¦‚æ˜¯å¦è¦åˆ‡æ¢åŠŸæ³•ï¼‰

### ğŸ›ï¸ ä¸–ç•ŒèƒŒæ™¯ç³»ç»Ÿ
- âœ… æ³¨å…¥åŸºç¡€ä¸–ç•ŒçŸ¥è¯†
- âœ… ç”¨æˆ·è¾“å…¥å†å²ï¼ŒåŠ¨æ€ç”ŸæˆåŠŸæ³•ã€è£…å¤‡ã€å®—é—¨ã€åŒºåŸŸä¿¡æ¯

### âœ¨ ç‰¹æ®Š
- âœ… å¥‡é‡
- âœ… å¤©åŠ« &amp; å¿ƒé­”
- [ ] å¤ºèˆ &amp; é‡ç”Ÿ
- [ ] æœºç¼˜ &amp; å› æœ
- [ ] å åœ &amp; è°¶çº¬
- [ ] è§’è‰²éšç§˜ &amp; é˜´è°‹
- [ ] é£å‡ä¸Šç•Œ
- [ ] é˜µæ³•
- [ ] ä¸–ç•Œç§˜å¯† &amp; ä¸–ç•Œæ³•åˆ™
- [ ] è›Š
- [ ] ç­ä¸–å±æœº
- [ ] å¼€å®—ç«‹æ´¾/è‡ªç«‹ä¸–å®¶

### ğŸ”­ è¿œæœŸå±•æœ›
- [ ] å†å²/äº‹ä»¶çš„å°è¯´åŒ–&amp;å›¾ç‰‡åŒ–&amp;è§†é¢‘åŒ–
- [ ] Skill agentåŒ–ï¼Œä¿®å£«è‡ªè¡Œè§„åˆ’ã€åˆ†æã€è°ƒç”¨å·¥å…·ã€å†³ç­–</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[VectifyAI/PageIndex]]></title>
            <link>https://github.com/VectifyAI/PageIndex</link>
            <guid>https://github.com/VectifyAI/PageIndex</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:06 GMT</pubDate>
            <description><![CDATA[ğŸ“‘ PageIndex: Document Index for Vectorless, Reasoning-based RAG]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/VectifyAI/PageIndex">VectifyAI/PageIndex</a></h1>
            <p>ğŸ“‘ PageIndex: Document Index for Vectorless, Reasoning-based RAG</p>
            <p>Language: Python</p>
            <p>Stars: 18,971</p>
            <p>Forks: 1,405</p>
            <p>Stars today: 644 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  
&lt;a href=&quot;https://vectify.ai/pageindex&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/46201e72-675b-43bc-bfbd-081cc6b65a1d&quot; alt=&quot;PageIndex Banner&quot; /&gt;
&lt;/a&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/14736&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14736&quot; alt=&quot;VectifyAI%2FPageIndex | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

# PageIndex: Vectorless, Reasoning-based RAG

&lt;p align=&quot;center&quot;&gt;&lt;b&gt;Reasoning-based RAG&amp;nbsp; â—¦ &amp;nbsp;No Vector DB&amp;nbsp; â—¦ &amp;nbsp;No Chunking&amp;nbsp; â—¦ &amp;nbsp;Human-like Retrieval&lt;/b&gt;&lt;/p&gt;

&lt;h4 align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://vectify.ai&quot;&gt;ğŸ  Homepage&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp;
  &lt;a href=&quot;https://chat.pageindex.ai&quot;&gt;ğŸ–¥ï¸ Chat Platform&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp;
  &lt;a href=&quot;https://pageindex.ai/mcp&quot;&gt;ğŸ”Œ MCP&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp;
  &lt;a href=&quot;https://docs.pageindex.ai&quot;&gt;ğŸ“š Docs&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp;
  &lt;a href=&quot;https://discord.com/invite/VuXuf29EUj&quot;&gt;ğŸ’¬ Discord&lt;/a&gt;&amp;nbsp; â€¢ &amp;nbsp;
  &lt;a href=&quot;https://ii2abc2jejf.typeform.com/to/tK3AXl8T&quot;&gt;âœ‰ï¸ Contact&lt;/a&gt;&amp;nbsp;
&lt;/h4&gt;
  
&lt;/div&gt;


&lt;details open&gt;
&lt;summary&gt;&lt;h3&gt;ğŸ“¢ Latest Updates&lt;/h3&gt;&lt;/summary&gt;

 **ğŸ”¥ Releases:**
- [**PageIndex Chat**](https://chat.pageindex.ai): The first human-like document-analysis agent [platform](https://chat.pageindex.ai) built for professional long documents. Can also be integrated via [MCP](https://pageindex.ai/mcp) or [API](https://docs.pageindex.ai/quickstart) (beta).
&lt;!-- - [**PageIndex Chat API**](https://docs.pageindex.ai/quickstart): An API that brings PageIndex&#039;s advanced long-document intelligence directly into your applications and workflows. --&gt;
&lt;!-- - [PageIndex MCP](https://pageindex.ai/mcp): Bring PageIndex into Claude, Cursor, or any MCP-enabled agent. Chat with long PDFs in a reasoning-based, human-like way. --&gt;
 
 **ğŸ“ Articles:**
- [**PageIndex Framework**](https://pageindex.ai/blog/pageindex-intro): Introduces the PageIndex framework â€” an *agentic, in-context* *tree index* that enables LLMs to perform *reasoning-based*, *human-like retrieval* over long documents, without vector DB or chunking.
&lt;!-- - [Do We Still Need OCR?](https://pageindex.ai/blog/do-we-need-ocr): Explores how vision-based, reasoning-native RAG challenges the traditional OCR pipeline, and why the future of document AI might be *vectorless* and *vision-based*. --&gt;

 **ğŸ§ª Cookbooks:**
- [Vectorless RAG](https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex): A minimal, hands-on example of reasoning-based RAG using PageIndex. No vectors, no chunking, and human-like retrieval.
- [Vision-based Vectorless RAG](https://docs.pageindex.ai/cookbook/vision-rag-pageindex): OCR-free, vision-only RAG with PageIndex&#039;s reasoning-native retrieval workflow that works directly over PDF page images.
&lt;/details&gt;

---

# ğŸ“‘ Introduction to PageIndex

Are you frustrated with vector database retrieval accuracy for long professional documents? Traditional vector-based RAG relies on semantic *similarity* rather than true *relevance*. But **similarity â‰  relevance** â€” what we truly need in retrieval is **relevance**, and that requires **reasoning**. When working with professional documents that demand domain expertise and multi-step reasoning, similarity search often falls short.

Inspired by AlphaGo, we propose **[PageIndex](https://vectify.ai/pageindex)** â€” a **vectorless**, **reasoning-based RAG** system that builds a **hierarchical tree index** from long documents and uses LLMs to **reason** *over that index* for **agentic, context-aware retrieval**.
It simulates how *human experts* navigate and extract knowledge from complex documents through *tree search*, enabling LLMs to *think* and *reason* their way to the most relevant document sections. PageIndex performs retrieval in two steps:

1. Generate a â€œTable-of-Contentsâ€ **tree structure index** of documents
2. Perform reasoning-based retrieval through **tree search**

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pageindex.ai/blog/pageindex-intro&quot; target=&quot;_blank&quot; title=&quot;The PageIndex Framework&quot;&gt;
    &lt;img src=&quot;https://docs.pageindex.ai/images/cookbook/vectorless-rag.png&quot; width=&quot;70%&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

### ğŸ¯ Core Features 

Compared to traditional vector-based RAG, **PageIndex** features:
- **No Vector DB**: Uses document structure and LLM reasoning for retrieval, instead of vector similarity search.
- **No Chunking**: Documents are organized into natural sections, not artificial chunks.
- **Human-like Retrieval**: Simulates how human experts navigate and extract knowledge from complex documents.
- **Better Explainability and Traceability**: Retrieval is based on reasoning â€” traceable and interpretable, with page and section references. No more opaque, approximate vector search (â€œvibe retrievalâ€).

PageIndex powers a reasoning-based RAG system that achieved **state-of-the-art** [98.7% accuracy](https://github.com/VectifyAI/Mafin2.5-FinanceBench) on FinanceBench, demonstrating superior performance over vector-based RAG solutions in professional document analysis (see our [blog post](https://vectify.ai/blog/Mafin2.5) for details).

### ğŸ“ Explore PageIndex

To learn more, please see a detailed introduction of the [PageIndex framework](https://pageindex.ai/blog/pageindex-intro). Check out this GitHub repo for open-source code, and the [cookbooks](https://docs.pageindex.ai/cookbook), [tutorials](https://docs.pageindex.ai/tutorials), and [blog](https://pageindex.ai/blog) for additional usage guides and examples. 

The PageIndex service is available as a ChatGPT-style [chat platform](https://chat.pageindex.ai), or can be integrated via [MCP](https://pageindex.ai/mcp) or [API](https://docs.pageindex.ai/quickstart).

### ğŸ› ï¸ Deployment Options
- Self-host â€” run locally with this open-source repo.
- Cloud Service â€” try instantly with our [Chat Platform](https://chat.pageindex.ai/), or integrate with [MCP](https://pageindex.ai/mcp) or [API](https://docs.pageindex.ai/quickstart).
- _Enterprise_ â€” private or on-prem deployment. [Contact us](https://ii2abc2jejf.typeform.com/to/tK3AXl8T) or [book a demo](https://calendly.com/pageindex/meet) for more details.

### ğŸ§ª Quick Hands-on

- Try the [**Vectorless RAG**](https://github.com/VectifyAI/PageIndex/blob/main/cookbook/pageindex_RAG_simple.ipynb) notebook â€” a *minimal*, hands-on example of reasoning-based RAG using PageIndex.
- Experiment with [*Vision-based Vectorless RAG*](https://github.com/VectifyAI/PageIndex/blob/main/cookbook/vision_RAG_pageindex.ipynb) â€” no OCR; a minimal, reasoning-native RAG pipeline that works directly over page images.
  
&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/pageindex_RAG_simple.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Open_In_Colab-Vectorless_RAG-orange?style=for-the-badge&amp;logo=googlecolab&quot; alt=&quot;Open in Colab: Vectorless RAG&quot; /&gt;
  &lt;/a&gt;
  &amp;nbsp;&amp;nbsp;
  &lt;a href=&quot;https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/vision_RAG_pageindex.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Open_In_Colab-Vision_RAG-orange?style=for-the-badge&amp;logo=googlecolab&quot; alt=&quot;Open in Colab: Vision RAG&quot; /&gt;
  &lt;/a&gt;
&lt;/div&gt;

---

# ğŸŒ² PageIndex Tree Structure
PageIndex can transform lengthy PDF documents into a semantic **tree structure**, similar to a _&quot;table of contents&quot;_ but optimized for use with Large Language Models (LLMs). It&#039;s ideal for: financial reports, regulatory filings, academic textbooks, legal or technical manuals, and any document that exceeds LLM context limits.

Below is an example PageIndex tree structure. Also see more example [documents](https://github.com/VectifyAI/PageIndex/tree/main/tests/pdfs) and generated [tree structures](https://github.com/VectifyAI/PageIndex/tree/main/tests/results).

```jsonc
...
{
  &quot;title&quot;: &quot;Financial Stability&quot;,
  &quot;node_id&quot;: &quot;0006&quot;,
  &quot;start_index&quot;: 21,
  &quot;end_index&quot;: 22,
  &quot;summary&quot;: &quot;The Federal Reserve ...&quot;,
  &quot;nodes&quot;: [
    {
      &quot;title&quot;: &quot;Monitoring Financial Vulnerabilities&quot;,
      &quot;node_id&quot;: &quot;0007&quot;,
      &quot;start_index&quot;: 22,
      &quot;end_index&quot;: 28,
      &quot;summary&quot;: &quot;The Federal Reserve&#039;s monitoring ...&quot;
    },
    {
      &quot;title&quot;: &quot;Domestic and International Cooperation and Coordination&quot;,
      &quot;node_id&quot;: &quot;0008&quot;,
      &quot;start_index&quot;: 28,
      &quot;end_index&quot;: 31,
      &quot;summary&quot;: &quot;In 2023, the Federal Reserve collaborated ...&quot;
    }
  ]
}
...
```

You can generate the PageIndex tree structure with this open-source repo, or use our [API](https://docs.pageindex.ai/quickstart) 

---

# âš™ï¸ Package Usage

You can follow these steps to generate a PageIndex tree from a PDF document.

### 1. Install dependencies

```bash
pip3 install --upgrade -r requirements.txt
```

### 2. Set your OpenAI API key

Create a `.env` file in the root directory and add your API key:

```bash
CHATGPT_API_KEY=your_openai_key_here
```

### 3. Run PageIndex on your PDF

```bash
python3 run_pageindex.py --pdf_path /path/to/your/document.pdf
```

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Optional parameters&lt;/strong&gt;&lt;/summary&gt;
&lt;br&gt;
You can customize the processing with additional optional arguments:

```
--model                 OpenAI model to use (default: gpt-4o-2024-11-20)
--toc-check-pages       Pages to check for table of contents (default: 20)
--max-pages-per-node    Max pages per node (default: 10)
--max-tokens-per-node   Max tokens per node (default: 20000)
--if-add-node-id        Add node ID (yes/no, default: yes)
--if-add-node-summary   Add node summary (yes/no, default: yes)
--if-add-doc-description Add doc description (yes/no, default: yes)
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Markdown support&lt;/strong&gt;&lt;/summary&gt;
&lt;br&gt;
We also provide markdown support for PageIndex. You can use the `-md_path` flag to generate a tree structure for a markdown file.

```bash
python3 run_pageindex.py --md_path /path/to/your/document.md
```

&gt; Note: in this function, we use &quot;#&quot; to determine node heading and their levels. For example, &quot;##&quot; is level 2, &quot;###&quot; is level 3, etc. Make sure your markdown file is formatted correctly. If your Markdown file was converted from a PDF or HTML, we don&#039;t recommend using this function, since most existing conversion tools cannot preserve the original hierarchy. Instead, use our [PageIndex OCR](https://pageindex.ai/blog/ocr), which is designed to preserve the original hierarchy, to convert the PDF to a markdown file and then use this function.
&lt;/details&gt;

&lt;!-- 
# â˜ï¸ Improved Tree Generation with PageIndex OCR

This repo is designed for generating PageIndex tree structure for simple PDFs, but many real-world use cases involve complex PDFs that are hard to parse by classic Python tools. However, extracting high-quality text from PDF documents remains a non-trivial challenge. Most OCR tools only extract page-level content, losing the broader document context and hierarchy.

To address this, we introduced PageIndex OCR â€” the first long-context OCR model designed to preserve the global structure of documents. PageIndex OCR significantly outperforms other leading OCR tools, such as those from Mistral and Contextual AI, in recognizing true hierarchy and semantic relationships across document pages.

- Experience next-level OCR quality with PageIndex OCR at our [Dashboard](https://dash.pageindex.ai/).
- Integrate PageIndex OCR seamlessly into your stack via our [API](https://docs.pageindex.ai/quickstart).

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/eb35d8ae-865c-4e60-a33b-ebbd00c41732&quot; width=&quot;80%&quot;&gt;
&lt;/p&gt;
--&gt;

---

# ğŸ“ˆ Case Study: PageIndex Leads Finance QA Benchmark

[Mafin 2.5](https://vectify.ai/mafin) is a reasoning-based RAG system for financial document analysis, powered by **PageIndex**. It achieved a state-of-the-art [**98.7% accuracy**](https://vectify.ai/blog/Mafin2.5) on the [FinanceBench](https://arxiv.org/abs/2311.11944) benchmark, significantly outperforming traditional vector-based RAG systems.

PageIndex&#039;s hierarchical indexing and reasoning-driven retrieval enable precise navigation and extraction of relevant context from complex financial reports, such as SEC filings and earnings disclosures.

Explore the full [benchmark results](https://github.com/VectifyAI/Mafin2.5-FinanceBench) and our [blog post](https://vectify.ai/blog/Mafin2.5) for detailed comparisons and performance metrics.

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/VectifyAI/Mafin2.5-FinanceBench&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/571aa074-d803-43c7-80c4-a04254b782a3&quot; width=&quot;70%&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

---

# ğŸ§­ Resources

* ğŸ§ª [Cookbooks](https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex): hands-on, runnable examples and advanced use cases.
* ğŸ“– [Tutorials](https://docs.pageindex.ai/doc-search): practical guides and strategies, including *Document Search* and *Tree Search*.
* ğŸ“ [Blog](https://pageindex.ai/blog): technical articles, research insights, and product updates.
* ğŸ”Œ [MCP setup](https://pageindex.ai/mcp#quick-setup) &amp; [API docs](https://docs.pageindex.ai/quickstart): integration details and configuration options.

---

# â­ Support Us
Please cite this work as:
```
Mingtian Zhang, Yu Tang and PageIndex Team,
&quot;PageIndex: Next-Generation Vectorless, Reasoning-based RAG&quot;,
PageIndex Blog, Sep 2025.
```

Or use the BibTeX citation:

```
@article{zhang2025pageindex,
  author = {Mingtian Zhang and Yu Tang and PageIndex Team},
  title = {PageIndex: Next-Generation Vectorless, Reasoning-based RAG},
  journal = {PageIndex Blog},
  year = {2025},
  month = {September},
  note = {https://pageindex.ai/blog/pageindex-intro},
}
```

Leave us a star ğŸŒŸ if you like our project. Thank you!  

&lt;p&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/eae4ff38-48ae-4a7c-b19f-eab81201d794&quot; width=&quot;80%&quot;&gt;
&lt;/p&gt;

### Connect with Us

[![Twitter](https://img.shields.io/badge/Twitter-000000?style=for-the-badge&amp;logo=x&amp;logoColor=white)](https://x.com/PageIndexAI)&amp;nbsp;
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&amp;logo=linkedin&amp;logoColor=white)](https://www.linkedin.com/company/vectify-ai/)&amp;nbsp;
[![Discord](https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;logo=discord&amp;logoColor=white)](https://discord.com/invite/VuXuf29EUj)&amp;nbsp;
[![Contact Us](https://img.shields.io/badge/Contact_Us-3B82F6?style=for-the-badge&amp;logo=envelope&amp;logoColor=white)](https://ii2abc2jejf.typeform.com/to/tK3AXl8T)

---

Â© 2025 [Vectify AI](https://vectify.ai)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[agentscope-ai/agentscope]]></title>
            <link>https://github.com/agentscope-ai/agentscope</link>
            <guid>https://github.com/agentscope-ai/agentscope</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:05 GMT</pubDate>
            <description><![CDATA[Build and run agents you can see, understand and trust.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/agentscope-ai/agentscope">agentscope-ai/agentscope</a></h1>
            <p>Build and run agents you can see, understand and trust.</p>
            <p>Language: Python</p>
            <p>Stars: 16,555</p>
            <p>Forks: 1,483</p>
            <p>Stars today: 46 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img
    src=&quot;https://img.alicdn.com/imgextra/i1/O1CN01nTg6w21NqT5qFKH1u_!!6000000001621-55-tps-550-550.svg&quot;
    alt=&quot;AgentScope Logo&quot;
    width=&quot;200&quot;
  /&gt;
&lt;/p&gt;

&lt;span align=&quot;center&quot;&gt;

[**ä¸­æ–‡ä¸»é¡µ**](https://github.com/agentscope-ai/agentscope/blob/main/README_zh.md) | [**Tutorial**](https://doc.agentscope.io/) | [**Roadmap (Jan 2026 -)**](https://github.com/agentscope-ai/agentscope/blob/main/docs/roadmap.md) | [**FAQ**](https://doc.agentscope.io/tutorial/faq.html)

&lt;/span&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2402.14034&quot;&gt;
        &lt;img
            src=&quot;https://img.shields.io/badge/cs.MA-2402.14034-B31C1C?logo=arxiv&amp;logoColor=B31C1C&quot;
            alt=&quot;arxiv&quot;
        /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://pypi.org/project/agentscope/&quot;&gt;
        &lt;img
            src=&quot;https://img.shields.io/badge/python-3.10+-blue?logo=python&quot;
            alt=&quot;pypi&quot;
        /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://pypi.org/project/agentscope/&quot;&gt;
        &lt;img
            src=&quot;https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fpypi.org%2Fpypi%2Fagentscope%2Fjson&amp;query=%24.info.version&amp;prefix=v&amp;logo=pypi&amp;label=version&quot;
            alt=&quot;pypi&quot;
        /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://discord.gg/eYMpfnkG8h&quot;&gt;
        &lt;img
            src=&quot;https://img.shields.io/discord/1194846673529213039?label=Discord&amp;logo=discord&quot;
            alt=&quot;discord&quot;
        /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://doc.agentscope.io/&quot;&gt;
        &lt;img
            src=&quot;https://img.shields.io/badge/Docs-English%7C%E4%B8%AD%E6%96%87-blue?logo=markdown&quot;
            alt=&quot;docs&quot;
        /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;./LICENSE&quot;&gt;
        &lt;img
            src=&quot;https://img.shields.io/badge/license-Apache--2.0-black&quot;
            alt=&quot;license&quot;
        /&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://trendshift.io/api/badge/repositories/10079&quot; alt=&quot;modelscope%2Fagentscope | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
&lt;/p&gt;

## What is AgentScope?

AgentScope is a production-ready, easy-to-use agent framework with essential abstractions that work with rising model capability and built-in support for finetuning.

We design for increasingly agentic LLMs.
Our approach leverages the models&#039; reasoning and tool use abilities
rather than constraining them with strict prompts and opinionated orchestrations.

## Why use AgentScope?

- **Simple**: start building your agents in 5 minutes with built-in ReAct agent, tools, skills, human-in-the-loop steering, memory, planning, realtime voice, evaluation and model finetuning
- **Extensible**: large number of ecosystem integrations for tools, memory and observability; built-in support for MCP and A2A; message hub for flexible multi-agent orchestration and workflows
- **Production-ready**: deploy and serve your agents locally, as serverless in the cloud, or on your K8s cluster with built-in OTel support


&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;./assets/images/agentscope_20260120.png&quot; width=&quot;90%&quot; /&gt;
&lt;br/&gt;
The AgentScope Ecosystem
&lt;/p&gt;


## News
&lt;!-- BEGIN NEWS --&gt;
- **[2026-02] `FEAT`:** Realtime Voice Agent support. [Example](https://github.com/agentscope-ai/agentscope/tree/main/examples/agent/realtime_voice_agent) | [Multi-Agent Realtime Example](https://github.com/agentscope-ai/agentscope/tree/main/examples/workflows/multiagent_realtime) | [Tutorial](https://doc.agentscope.io/tutorial/task_realtime.html)
- **[2026-01] `COMM`:** Biweekly Meetings launched to share ecosystem updates and development plans - join us! [Details &amp; Schedule](https://github.com/agentscope-ai/agentscope/discussions/1126)
- **[2026-01] `FEAT`:** Database support &amp; memory compression in memory module. [Example](https://github.com/agentscope-ai/agentscope/tree/main/examples/functionality/short_term_memory/memory_compression) | [Tutorial](https://doc.agentscope.io/tutorial/task_memory.html)
- **[2025-12] `INTG`:** A2A (Agent-to-Agent) protocol support. [Example](https://github.com/agentscope-ai/agentscope/tree/main/examples/agent/a2a_agent) | [Tutorial](https://doc.agentscope.io/tutorial/task_a2a.html)
- **[2025-12] `FEAT`:** TTS (Text-to-Speech) support. [Example](https://github.com/agentscope-ai/agentscope/tree/main/examples/functionality/tts) | [Tutorial](https://doc.agentscope.io/tutorial/task_tts.html)
- **[2025-11] `INTG`:** Anthropic Agent Skill support. [Example](https://github.com/agentscope-ai/agentscope/tree/main/examples/functionality/agent_skill) | [Tutorial](https://doc.agentscope.io/tutorial/task_agent_skill.html)
- **[2025-11] `RELS`:** Alias-Agent for diverse real-world tasks and Data-Juicer Agent for data processing open-sourced. [Alias-Agent](https://github.com/agentscope-ai/agentscope-samples/tree/main/alias) | [Data-Juicer Agent](https://github.com/agentscope-ai/agentscope-samples/tree/main/data_juicer_agent)
- **[2025-11] `INTG`:** Agentic RL via Trinity-RFT library. [Example](https://github.com/agentscope-ai/agentscope/tree/main/examples/tuner/react_agent) | [Trinity-RFT](https://github.com/agentscope-ai/Trinity-RFT)
- **[2025-11] `INTG`:** ReMe for enhanced long-term memory. [Example](https://github.com/agentscope-ai/agentscope/tree/main/examples/functionality/long_term_memory/reme)
- **[2025-11] `RELS`:** agentscope-samples repository launched and agentscope-runtime upgraded with Docker/K8s deployment and VNC-powered GUI sandboxes. [Samples](https://github.com/agentscope-ai/agentscope-samples) | [Runtime](https://github.com/agentscope-ai/agentscope-runtime)
&lt;!-- END NEWS --&gt;

[More news â†’](./docs/NEWS.md)

## Community

Welcome to join our community on

| [Discord](https://discord.gg/eYMpfnkG8h)                                                                                         | DingTalk                                                                  |
|----------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------|
| &lt;img src=&quot;https://gw.alicdn.com/imgextra/i1/O1CN01hhD1mu1Dd3BWVUvxN_!!6000000000238-2-tps-400-400.png&quot; width=&quot;100&quot; height=&quot;100&quot;&gt; | &lt;img src=&quot;./assets/images/dingtalk_qr_code.png&quot; width=&quot;100&quot; height=&quot;100&quot;&gt; |

&lt;!-- START doctoc generated TOC please keep comment here to allow auto update --&gt;
&lt;!-- DON&#039;T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --&gt;
## ğŸ“‘ Table of Contents

- [Quickstart](#quickstart)
  - [Installation](#installation)
    - [From PyPI](#from-pypi)
    - [From source](#from-source)
- [Example](#example)
  - [Hello AgentScope!](#hello-agentscope)
  - [Voice Agent](#voice-agent)
  - [Realtime Voice Agent](#realtime-voice-agent)
  - [Human-in-the-loop](#human-in-the-loop)
  - [Flexible MCP Usage](#flexible-mcp-usage)
  - [Agentic RL](#agentic-rl)
  - [Multi-Agent Workflows](#multi-agent-workflows)
- [Documentation](#documentation)
- [More Examples &amp; Samples](#more-examples--samples)
  - [Functionality](#functionality)
  - [Agent](#agent)
  - [Game](#game)
  - [Workflow](#workflow)
  - [Evaluation](#evaluation)
  - [Tuner](#tuner)
- [Contributing](#contributing)
- [License](#license)
- [Publications](#publications)
- [Contributors](#contributors)

&lt;!-- END doctoc generated TOC please keep comment here to allow auto update --&gt;

## Quickstart

### Installation

&gt; AgentScope requires **Python 3.10** or higher.

#### From PyPI

```bash
pip install agentscope
```

Or with uv:

```bash
uv pip install agentscope
```

#### From source

```bash
# Pull the source code from GitHub
git clone -b main https://github.com/agentscope-ai/agentscope.git

# Install the package in editable mode
cd agentscope

pip install -e .
# or with uv:
# uv pip install -e .
```


## Example

### Hello AgentScope!

Start with a conversation between user and a ReAct agent ğŸ¤– named &quot;Friday&quot;!

```python
from agentscope.agent import ReActAgent, UserAgent
from agentscope.model import DashScopeChatModel
from agentscope.formatter import DashScopeChatFormatter
from agentscope.memory import InMemoryMemory
from agentscope.tool import Toolkit, execute_python_code, execute_shell_command
import os, asyncio


async def main():
    toolkit = Toolkit()
    toolkit.register_tool_function(execute_python_code)
    toolkit.register_tool_function(execute_shell_command)

    agent = ReActAgent(
        name=&quot;Friday&quot;,
        sys_prompt=&quot;You&#039;re a helpful assistant named Friday.&quot;,
        model=DashScopeChatModel(
            model_name=&quot;qwen-max&quot;,
            api_key=os.environ[&quot;DASHSCOPE_API_KEY&quot;],
            stream=True,
        ),
        memory=InMemoryMemory(),
        formatter=DashScopeChatFormatter(),
        toolkit=toolkit,
    )

    user = UserAgent(name=&quot;user&quot;)

    msg = None
    while True:
        msg = await agent(msg)
        msg = await user(msg)
        if msg.get_text_content() == &quot;exit&quot;:
            break

asyncio.run(main())
```

### Voice Agent

Create a voice-enabled ReAct agent that can understand and respond with speech, even playing a multi-agent werewolf game with voice interactions.


https://github.com/user-attachments/assets/c5f05254-aff6-4375-90df-85e8da95d5da


### Realtime Voice Agent

Build a realtime voice agent with web interface that can interact with users via voice input and output.

[Realtime chatbot](https://github.com/agentscope-ai/agentscope/tree/main/examples/agent/realtime_voice_agent) | [Realtime Multi-Agent Example](https://github.com/agentscope-ai/agentscope/tree/main/examples/workflows/multiagent_realtime)

https://github.com/user-attachments/assets/1b7b114b-e995-4586-9b3f-d3bb9fcd2558



### Human-in-the-loop

Support realtime interruption in ReActAgent: conversation can be interrupted via cancellation in realtime and resumed
seamlessly via robust memory preservation.

&lt;img src=&quot;./assets/images/realtime_steering_en.gif&quot; alt=&quot;Realtime Steering&quot; width=&quot;60%&quot;/&gt;

### Flexible MCP Usage

Use individual MCP tools as **local callable functions** to compose toolkits or wrap into a more complex tool.

```python
from agentscope.mcp import HttpStatelessClient
from agentscope.tool import Toolkit
import os

async def fine_grained_mcp_control():
    # Initialize the MCP client
    client = HttpStatelessClient(
        name=&quot;gaode_mcp&quot;,
        transport=&quot;streamable_http&quot;,
        url=f&quot;https://mcp.amap.com/mcp?key={os.environ[&#039;GAODE_API_KEY&#039;]}&quot;,
    )

    # Obtain the MCP tool as a **local callable function**, and use it anywhere
    func = await client.get_callable_function(func_name=&quot;maps_geo&quot;)

    # Option 1: Call directly
    await func(address=&quot;Tiananmen Square&quot;, city=&quot;Beijing&quot;)

    # Option 2: Pass to agent as a tool
    toolkit = Toolkit()
    toolkit.register_tool_function(func)
    # ...

    # Option 3: Wrap into a more complex tool
    # ...
```

### Agentic RL

Train your agentic application seamlessly with Reinforcement Learning integration. We also prepare multiple sample projects covering various scenarios:

| Example                                                                                          | Description                                                 | Model                  | Training Result             |
|--------------------------------------------------------------------------------------------------|-------------------------------------------------------------|------------------------|-----------------------------|
| [Math Agent](https://github.com/agentscope-ai/agentscope-samples/tree/main/tuner/math_agent)     | Tune a math-solving agent with multi-step reasoning.        | Qwen3-0.6B             | Accuracy: 75% â†’ 85%         |
| [Frozen Lake](https://github.com/agentscope-ai/agentscope-samples/tree/main/tuner/frozen_lake)   | Train an agent to navigate the Frozen Lake environment.     | Qwen2.5-3B-Instruct    | Success rate: 15% â†’ 86%     |
| [Learn to Ask](https://github.com/agentscope-ai/agentscope-samples/tree/main/tuner/learn_to_ask) | Tune agents using LLM-as-a-judge for automated feedback.    | Qwen2.5-7B-Instruct    | Accuracy: 47% â†’ 92%         |
| [Email Search](https://github.com/agentscope-ai/agentscope-samples/tree/main/tuner/email_search) | Improve tool-use capabilities without labeled ground truth. | Qwen3-4B-Instruct-2507 | Accuracy: 60%               |
| [Werewolf Game](https://github.com/agentscope-ai/agentscope-samples/tree/main/tuner/werewolves)  | Train agents for strategic multi-agent game interactions.   | Qwen2.5-7B-Instruct    | Werewolf win rate: 50% â†’ 80% |
| [Data Augment](https://github.com/agentscope-ai/agentscope-samples/tree/main/tuner/data_augment) | Generate synthetic training data to enhance tuning results. | Qwen3-0.6B             | AIME-24 accuracy: 20% â†’ 60% |

### Multi-Agent Workflows

AgentScope provides ``MsgHub`` and pipelines to streamline multi-agent conversations, offering efficient message routing and seamless information sharing

```python
from agentscope.pipeline import MsgHub, sequential_pipeline
from agentscope.message import Msg
import asyncio

async def multi_agent_conversation():
    # Create agents
    agent1 = ...
    agent2 = ...
    agent3 = ...
    agent4 = ...

    # Create a message hub to manage multi-agent conversation
    async with MsgHub(
        participants=[agent1, agent2, agent3],
        announcement=Msg(&quot;Host&quot;, &quot;Introduce yourselves.&quot;, &quot;assistant&quot;)
    ) as hub:
        # Speak in a sequential manner
        await sequential_pipeline([agent1, agent2, agent3])
        # Dynamic manage the participants
        hub.add(agent4)
        hub.delete(agent3)
        await hub.broadcast(Msg(&quot;Host&quot;, &quot;Goodbye!&quot;, &quot;assistant&quot;))

asyncio.run(multi_agent_conversation())
```


## Documentation

- [Tutorial](https://doc.agentscope.io/tutorial/)
- [FAQ](https://doc.agentscope.io/tutorial/faq.html)
- [API Docs](https://doc.agentscope.io/api/agentscope.html)

## More Examples &amp; Samples

### Functionality

- [MCP](https://github.com/agentscope-ai/agentscope/tree/main/examples/functionality/mcp)
- [Anthropic Agent Skill](https://github.com/agentscope-ai/agentscope/tree/main/examples/functionality/agent_skill)
- [Plan](https://github.com/agentscope-ai/agentscope/tree/main/examples/functionality/plan)
- [Structured Output](https://github.com/agentscope-ai/agentscope/tree/main/examples/functionality/structured_output)
- [RAG](https://github.com/agentscope-ai/agentscope/tree/main/examples/functionality/rag)
- [Long-Term Memory](https://github.com/agentscope-ai/agentscope/tree/main/examples/functionality/long_term_memory)
- [Session with SQLite](https://github.com/agentscope-ai/agentscope/tree/main/examples/functionality/session_with_sqlite)
- [Stream Printing Messages](https://github.com/agentscope-ai/agentscope/tree/main/examples/functionality/stream_printing_messages)
- [TTS](https://github.com/agentscope-ai/agentscope/tree/main/examples/functionality/tts)
- [Code-first Deployment](https://github.com/agentscope-ai/agentscope/tree/main/examples/deployment/planning_agent)
- [Memory Compression](https://github.com/agentscope-ai/agentscope/tree/main/examples/functionality/short_term_memory/memory_compression)

### Agent

- [ReAct Agent](https://github.com/agentscope-ai/agentscope/tree/main/examples/agent/react_agent)
- [Voice Agent](https://github.com/agentscope-ai/agentscope/tree/main/examples/agent/voice_agent)
- [Deep Research Agent](https://github.com/agentscope-ai/agentscope/tree/main/examples/agent/deep_research_agent)
- [Browser-use Agent](https://github.com/agentscope-ai/agentscope/tree/main/examples/agent/browser_agent)
- [Meta Planner Agent](https://github.com/agentscope-ai/agentscope/tree/main/examples/agent/meta_planner_agent)
- [A2A Agent](https://github.com/agentscope-ai/agentscope/tree/main/examples/agent/a2a_agent)
- [Realtime Voice Agent](https://github.com/agentscope-ai/agentscope/tree/main/examples/agent/realtime_voice_agent)

### Game

- [Nine-player Werewolves](https://github.com/agentscope-ai/agentscope/tree/main/examples/game/werewolves)

### Workflow

- [Multi-agent Debate](https://github.com/agentscope-ai/agentscope/tree/main/examples/workflows/multiagent_debate)
- [Multi-agent Conversation](https://github.com/agentscope-ai/agentscope/tree/main/examples/workflows/multiagent_conversation)
- [Multi-agent Concurrent](https://github.com/agentscope-ai/agentscope/tree/main/examples/workflows/multiagent_concurrent)
- [Multi-agent Realtime Conversation](https://github.com/agentscope-ai/agentscope/tree/main/examples/workflows/multiagent_realtime)

### Evaluation

- [ACEBench](https://github.com/agentscope-ai/agentscope/tree/main/examples/evaluation/ace_bench)

### Tuner

- [Tune ReAct Agent](https://github.com/agentscope-ai/agentscope/tree/main/examples/tuner/react_agent)


## Contributing

We welcome contributions from the community! Please refer to our [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines
on how to contribute.

## License

AgentScope is released under Apache License 2.0.

## Publications

If you find our work helpful for your research or application, please cite our papers.

- [AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications](https://arxiv.org/abs/2508.16279)

- [AgentScope: A Flexible yet Robust Multi-Agent Platform](https://arxiv.org/abs/2402.14034)

```
@article{agentscope_v1,
    author  = {Dawei Gao, Zitao Li, Yuexiang Xie, Weirui Kuang, Liuyi Yao, Bingchen Qian, Zhijian Ma, Yue Cui, Haohao Luo, Shen Li, Lu Yi, Yi Yu, Shiqi He, Zhiling Luo, Wenmeng Zhou, Zhicheng Zhang, Xuguang He, Ziqian Chen, Weikai Liao, Farruh Isakulovich Kushnazarov, Yaliang Li, Bolin Ding, Jingren Zhou}
    title   = {AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications},
    journal = {CoRR},
    volume  = {abs/2508.16279},
    year    = {2025},
}

@article{agentscope,
    author  = {Dawei Gao, Zitao Li, Xuchen Pan, Weirui Kuang, Zhijian Ma, Bingchen Qian, Fei Wei, Wenhao Zhang, Yuexiang Xie, Daoyuan Chen, Liuyi Yao, Hongyi Peng, Zeyu Zhang, Lin Zhu, Chen Cheng, Hongzhu Shi, Yaliang Li, Bolin Ding, Jingren Zhou}
    title   = {AgentScope: A Flexible yet Robust Multi-Agent Platform},
    journal = {CoRR},
    volume  = {abs/2402.14034},
    year    = {2024},
}
```

## Contributors

All thanks to our contributors:

&lt;a href=&quot;https://github.com/agentscope-ai/agentscope/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=agentscope-ai/agentscope&amp;max=999&amp;columns=12&amp;anon=1&quot; /&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NevaMind-AI/memU]]></title>
            <link>https://github.com/NevaMind-AI/memU</link>
            <guid>https://github.com/NevaMind-AI/memU</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:04 GMT</pubDate>
            <description><![CDATA[Memory for 24/7 proactive agents like openclaw (moltbot, clawdbot).]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NevaMind-AI/memU">NevaMind-AI/memU</a></h1>
            <p>Memory for 24/7 proactive agents like openclaw (moltbot, clawdbot).</p>
            <p>Language: Python</p>
            <p>Stars: 11,196</p>
            <p>Forks: 821</p>
            <p>Stars today: 145 stars today</p>
            <h2>README</h2><pre>![MemU Banner](assets/banner.png)

&lt;div align=&quot;center&quot;&gt;

# memU

### 24/7 Always-On Proactive Memory for AI Agents

[![PyPI version](https://badge.fury.io/py/memu-py.svg)](https://badge.fury.io/py/memu-py)
[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python 3.13+](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)
[![Discord](https://img.shields.io/badge/Discord-Join%20Chat-5865F2?logo=discord&amp;logoColor=white)](https://discord.gg/memu)
[![Twitter](https://img.shields.io/badge/Twitter-Follow-1DA1F2?logo=x&amp;logoColor=white)](https://x.com/memU_ai)

&lt;a href=&quot;https://trendshift.io/repositories/17374&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/17374&quot; alt=&quot;NevaMind-AI%2FmemU | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

**[English](readme/README_en.md) | [ä¸­æ–‡](readme/README_zh.md) | [æ—¥æœ¬èª](readme/README_ja.md) | [í•œêµ­ì–´](readme/README_ko.md) | [EspaÃ±ol](readme/README_es.md) | [FranÃ§ais](readme/README_fr.md)**

&lt;/div&gt;

---

memU is a memory framework built for **24/7 proactive agents**.
It is designed for long-running use and greatly **reduces the LLM token cost** of keeping agents always online, making always-on, evolving agents practical in production systems.
memU **continuously captures and understands user intent**. Even without a command, the agent can tell what you are about to do and act on it by itself.

---

## ğŸ¤– [OpenClaw (Moltbot, Clawdbot) Alternative](https://memu.bot)

&lt;img width=&quot;100%&quot; src=&quot;https://github.com/NevaMind-AI/memU/blob/main/assets/memUbot.png&quot; /&gt;

- **Download-and-use and simple** to get started.
- Builds long-term memory to **understand user intent** and act proactively.
- **Cuts LLM token cost** with smaller context.

Try now: [memU bot](https://memu.bot)

---

## ğŸ—ƒï¸ Memory as File System, File System as Memory

memU treats **memory like a file system**â€”structured, hierarchical, and instantly accessible.

| File System | memU Memory |
|-------------|-------------|
| ğŸ“ Folders | ğŸ·ï¸ Categories (auto-organized topics) |
| ğŸ“„ Files | ğŸ§  Memory Items (extracted facts, preferences, skills) |
| ğŸ”— Symlinks | ğŸ”„ Cross-references (related memories linked) |
| ğŸ“‚ Mount points | ğŸ“¥ Resources (conversations, documents, images) |

**Why this matters:**
- **Navigate memories** like browsing directoriesâ€”drill down from broad categories to specific facts
- **Mount new knowledge** instantlyâ€”conversations and documents become queryable memory
- **Cross-link everything**â€”memories reference each other, building a connected knowledge graph
- **Persistent &amp; portable**â€”export, backup, and transfer memory like files

```
memory/
â”œâ”€â”€ preferences/
â”‚   â”œâ”€â”€ communication_style.md
â”‚   â””â”€â”€ topic_interests.md
â”œâ”€â”€ relationships/
â”‚   â”œâ”€â”€ contacts/
â”‚   â””â”€â”€ interaction_history/
â”œâ”€â”€ knowledge/
â”‚   â”œâ”€â”€ domain_expertise/
â”‚   â””â”€â”€ learned_skills/
â””â”€â”€ context/
    â”œâ”€â”€ recent_conversations/
    â””â”€â”€ pending_tasks/
```

Just as a file system turns raw bytes into organized data, memU transforms raw interactions into **structured, searchable, proactive intelligence**.

---

## â­ï¸ Star the repository

&lt;img width=&quot;100%&quot; src=&quot;https://github.com/NevaMind-AI/memU/blob/main/assets/star.gif&quot; /&gt;
If you find memU useful or interesting, a GitHub Star â­ï¸ would be greatly appreciated.

---


## âœ¨ Core Features

| Capability | Description |
|------------|-------------|
| ğŸ¤– **24/7 Proactive Agent** | Always-on memory agent that works continuously in the backgroundâ€”never sleeps, never forgets |
| ğŸ¯ **User Intention Capture** | Understands and remembers user goals, preferences, and context across sessions automatically |
| ğŸ’° **Cost Efficient** | Reduces long-running token costs by caching insights and avoiding redundant LLM calls |
---

## ğŸ”„ How Proactive Memory Works

```bash

cd examples/proactive
python proactive.py

```

---

### Proactive Memory Lifecycle
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                         USER QUERY                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                                                           â”‚
                 â–¼                                                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ğŸ¤– MAIN AGENT                  â”‚         â”‚              ğŸ§  MEMU BOT                       â”‚
â”‚                                        â”‚         â”‚                                                â”‚
â”‚  Handle user queries &amp; execute tasks   â”‚  â—„â”€â”€â”€â–º  â”‚  Monitor, memorize &amp; proactive intelligence   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                        â”‚         â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  1. RECEIVE USER INPUT           â”‚  â”‚         â”‚  â”‚  1. MONITOR INPUT/OUTPUT                 â”‚  â”‚
â”‚  â”‚     Parse query, understand      â”‚  â”‚   â”€â”€â”€â–º  â”‚  â”‚     Observe agent interactions           â”‚  â”‚
â”‚  â”‚     context and intent           â”‚  â”‚         â”‚  â”‚     Track conversation flow              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                 â”‚                      â”‚         â”‚                    â”‚                           â”‚
â”‚                 â–¼                      â”‚         â”‚                    â–¼                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  2. PLAN &amp; EXECUTE               â”‚  â”‚         â”‚  â”‚  2. MEMORIZE &amp; EXTRACT                   â”‚  â”‚
â”‚  â”‚     Break down tasks             â”‚  â”‚   â—„â”€â”€â”€  â”‚  â”‚     Store insights, facts, preferences   â”‚  â”‚
â”‚  â”‚     Call tools, retrieve data    â”‚  â”‚  inject â”‚  â”‚     Extract skills &amp; knowledge           â”‚  â”‚
â”‚  â”‚     Generate responses           â”‚  â”‚  memory â”‚  â”‚     Update user profile                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                 â”‚                      â”‚         â”‚                    â”‚                           â”‚
â”‚                 â–¼                      â”‚         â”‚                    â–¼                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  3. RESPOND TO USER              â”‚  â”‚         â”‚  â”‚  3. PREDICT USER INTENT                  â”‚  â”‚
â”‚  â”‚     Deliver answer/result        â”‚  â”‚   â”€â”€â”€â–º  â”‚  â”‚     Anticipate next steps                â”‚  â”‚
â”‚  â”‚     Continue conversation        â”‚  â”‚         â”‚  â”‚     Identify upcoming needs              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                 â”‚                      â”‚         â”‚                    â”‚                           â”‚
â”‚                 â–¼                      â”‚         â”‚                    â–¼                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  4. LOOP                         â”‚  â”‚         â”‚  â”‚  4. RUN PROACTIVE TASKS                  â”‚  â”‚
â”‚  â”‚     Wait for next user input     â”‚  â”‚   â—„â”€â”€â”€  â”‚  â”‚     Pre-fetch relevant context           â”‚  â”‚
â”‚  â”‚     or proactive suggestions     â”‚  â”‚  suggestâ”‚  â”‚     Prepare recommendations              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â”‚     Update todolist autonomously         â”‚  â”‚
â”‚                                        â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                                                           â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚     CONTINUOUS SYNC LOOP     â”‚
                              â”‚  Agent â—„â”€â”€â–º MemU Bot â—„â”€â”€â–º DB â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Proactive Use Cases

### 1. **Information Recommendation**
*Agent monitors interests and proactively surfaces relevant content*
```python
# User has been researching AI topics
MemU tracks: reading history, saved articles, search queries

# When new content arrives:
Agent: &quot;I found 3 new papers on RAG optimization that align with
        your recent research on retrieval systems. One author
        (Dr. Chen) you&#039;ve cited before published yesterday.&quot;

# Proactive behaviors:
- Learns topic preferences from browsing patterns
- Tracks author/source credibility preferences
- Filters noise based on engagement history
- Times recommendations for optimal attention
```

### 2. **Email Management**
*Agent learns communication patterns and handles routine correspondence*
```python
# MemU observes email patterns over time:
- Response templates for common scenarios
- Priority contacts and urgent keywords
- Scheduling preferences and availability
- Writing style and tone variations

# Proactive email assistance:
Agent: &quot;You have 12 new emails. I&#039;ve drafted responses for 3 routine
        requests and flagged 2 urgent items from your priority contacts.
        Should I also reschedule tomorrow&#039;s meeting based on the
        conflict John mentioned?&quot;

# Autonomous actions:
âœ“ Draft context-aware replies
âœ“ Categorize and prioritize inbox
âœ“ Detect scheduling conflicts
âœ“ Summarize long threads with key decisions
```

### 3. **Trading &amp; Financial Monitoring**
*Agent tracks market context and user investment behavior*
```python
# MemU learns trading preferences:
- Risk tolerance from historical decisions
- Preferred sectors and asset classes
- Response patterns to market events
- Portfolio rebalancing triggers

# Proactive alerts:
Agent: &quot;NVDA dropped 5% in after-hours trading. Based on your past
        behavior, you typically buy tech dips above 3%. Your current
        allocation allows for $2,000 additional exposure while
        maintaining your 70/30 equity-bond target.&quot;

# Continuous monitoring:
- Track price alerts tied to user-defined thresholds
- Correlate news events with portfolio impact
- Learn from executed vs. ignored recommendations
- Anticipate tax-loss harvesting opportunities
```


...

---

## ğŸ—‚ï¸ Hierarchical Memory Architecture

MemU&#039;s three-layer system enables both **reactive queries** and **proactive context loading**:

&lt;img width=&quot;100%&quot; alt=&quot;structure&quot; src=&quot;assets/structure.png&quot; /&gt;

| Layer | Reactive Use | Proactive Use |
|-------|--------------|---------------|
| **Resource** | Direct access to original data | Background monitoring for new patterns |
| **Item** | Targeted fact retrieval | Real-time extraction from ongoing interactions |
| **Category** | Summary-level overview | Automatic context assembly for anticipation |

**Proactive Benefits:**
- **Auto-categorization**: New memories self-organize into topics
- **Pattern Detection**: System identifies recurring themes
- **Context Prediction**: Anticipates what information will be needed next

---

## ğŸš€ Quick Start

### Option 1: Cloud Version

Experience proactive memory instantly:

ğŸ‘‰ **[memu.so](https://memu.so)** - Hosted service with 7Ã—24 continuous learning

For enterprise deployment with custom proactive workflows, contact **info@nevamind.ai**

#### Cloud API (v3)

| Base URL | `https://api.memu.so` |
|----------|----------------------|
| Auth | `Authorization: Bearer YOUR_API_KEY` |

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/v3/memory/memorize` | Register continuous learning task |
| `GET` | `/api/v3/memory/memorize/status/{task_id}` | Check real-time processing status |
| `POST` | `/api/v3/memory/categories` | List auto-generated categories |
| `POST` | `/api/v3/memory/retrieve` | Query memory (supports proactive context loading) |

ğŸ“š **[Full API Documentation](https://memu.pro/docs#cloud-version)**

---

### Option 2: Self-Hosted

#### Installation
```bash
pip install -e .
```

#### Basic Example

&gt; **Requirements**: Python 3.13+ and an OpenAI API key

**Test Continuous Learning** (in-memory):
```bash
export OPENAI_API_KEY=your_api_key
cd tests
python test_inmemory.py
```

**Test with Persistent Storage** (PostgreSQL):
```bash
# Start PostgreSQL with pgvector
docker run -d \
  --name memu-postgres \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_DB=memu \
  -p 5432:5432 \
  pgvector/pgvector:pg16

# Run continuous learning test
export OPENAI_API_KEY=your_api_key
cd tests
python test_postgres.py
```

Both examples demonstrate **proactive memory workflows**:
1. **Continuous Ingestion**: Process multiple files sequentially
2. **Auto-Extraction**: Immediate memory creation
3. **Proactive Retrieval**: Context-aware memory surfacing

See [`tests/test_inmemory.py`](tests/test_inmemory.py) and [`tests/test_postgres.py`](tests/test_postgres.py) for implementation details.

---

### Custom LLM and Embedding Providers

MemU supports custom LLM and embedding providers beyond OpenAI. Configure them via `llm_profiles`:
```python
from memu import MemUService

service = MemUService(
    llm_profiles={
        # Default profile for LLM operations
        &quot;default&quot;: {
            &quot;base_url&quot;: &quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,
            &quot;api_key&quot;: &quot;your_api_key&quot;,
            &quot;chat_model&quot;: &quot;qwen3-max&quot;,
            &quot;client_backend&quot;: &quot;sdk&quot;  # &quot;sdk&quot; or &quot;http&quot;
        },
        # Separate profile for embeddings
        &quot;embedding&quot;: {
            &quot;base_url&quot;: &quot;https://api.voyageai.com/v1&quot;,
            &quot;api_key&quot;: &quot;your_voyage_api_key&quot;,
            &quot;embed_model&quot;: &quot;voyage-3.5-lite&quot;
        }
    },
    # ... other configuration
)
```

---

### OpenRouter Integration

MemU supports [OpenRouter](https://openrouter.ai) as a model provider, giving you access to multiple LLM providers through a single API.

#### Configuration
```python
from memu import MemoryService

service = MemoryService(
    llm_profiles={
        &quot;default&quot;: {
            &quot;provider&quot;: &quot;openrouter&quot;,
            &quot;client_backend&quot;: &quot;httpx&quot;,
            &quot;base_url&quot;: &quot;https://openrouter.ai&quot;,
            &quot;api_key&quot;: &quot;your_openrouter_api_key&quot;,
            &quot;chat_model&quot;: &quot;anthropic/claude-3.5-sonnet&quot;,  # Any OpenRouter model
            &quot;embed_model&quot;: &quot;openai/text-embedding-3-small&quot;,  # Embedding model
        },
    },
    database_config={
        &quot;metadata_store&quot;: {&quot;provider&quot;: &quot;inmemory&quot;},
    },
)
```

#### Environment Variables

| Variable | Description |
|----------|-------------|
| `OPENROUTER_API_KEY` | Your OpenRouter API key from [openrouter.ai/keys](https://openrouter.ai/keys) |

#### Supported Features

| Feature | Status | Notes |
|---------|--------|-------|
| Chat Completions | Supported | Works with any OpenRouter chat model |
| Embeddings | Supported | Use OpenAI embedding models via OpenRouter |
| Vision | Supported | Use vision-capable models (e.g., `openai/gpt-4o`) |

#### Running OpenRouter Tests
```bash
export OPENROUTER_API_KEY=your_api_key

# Full workflow test (memorize + retrieve)
python tests/test_openrouter.py

# Embedding-specific tests
python tests/test_openrouter_embedding.py

# Vision-specific tests
python tests/test_openrouter_vision.py
```

See [`examples/example_4_openrouter_memory.py`](examples/example_4_openrouter_memory.py) for a complete working example.

---

## ğŸ“– Core APIs

### `memorize()` - Continuous Learning Pipeline

Processes inputs in real-time and immediately updates memory:

&lt;img width=&quot;100%&quot; alt=&quot;memorize&quot; src=&quot;assets/memorize.png&quot; /&gt;

```python
result = await service.memorize(
    resource_url=&quot;path/to/file.json&quot;,  # File path or URL
    modality=&quot;conversation&quot;,            # conversation | document | image | video | audio
    user={&quot;user_id&quot;: &quot;123&quot;}             # Optional: scope to a user
)

# Returns immediately with extracted memory:
{
    &quot;resource&quot;: {...},      # Stored resource metadata
    &quot;items&quot;: [...],         # Extracted memory items (available instantly)
    &quot;categories&quot;: [...]     # Auto-updated category structure
}
```

**Proactive Features:**
- Zero-delay processingâ€”memories available immediately
- Automatic categorization without manual tagging
- Cross-reference with existing memories for pattern detection

### `retrieve()` - Dual-Mode Intelligence

MemU supports both **proactive context loading** and **reactive querying**:

&lt;img width=&quot;100%&quot; alt=&quot;retrieve&quot; src=&quot;assets/retrieve.png&quot; /&gt;

#### RAG-based Retrieval (`method=&quot;rag&quot;`)

Fast **proactive context assembly** using embeddings:

- âœ… **Instant context**: Sub-second memory surfacing
- âœ… **Background monitoring**: Can run continuously without LLM costs
- âœ… **Similarity scoring**: Identifies most relevant memories automatically

#### LLM-based Retrieval (`method=&quot;llm&quot;`)

Deep **anticipatory reasoning** for complex contexts:

- âœ… **Intent prediction**: LLM infers what user needs before they ask
- âœ… **Query evolution**: Automatically refines search as context develops
- âœ… **Early termination**: Stops when sufficient context is gathered

#### Comparison

| Aspect | RAG (Fast Context) | LLM (Deep Reasoning) |
|--------|-------------------|---------------------|
| **Speed** | âš¡ Milliseconds | ğŸ¢ Seconds |
| **Cost** | ğŸ’° Embedding only | ğŸ’°ğŸ’° LLM inference |
| **Proactive use** | Continuous monitoring | Triggered context loading |
| **Best for** | Real-time suggestions | Complex anticipation |

#### Usage
```python
# Proactive retrieval with context history
result = await service.retrieve(
    queries=[
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: {&quot;text&quot;: &quot;What are their preferences?&quot;}},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: {&quot;text&quot;: &quot;Tell me about work habits&quot;}}
    ],
    where={&quot;user_id&quot;: &quot;123&quot;},  # Optional: scope filter
    method=&quot;rag&quot;  # or &quot;llm&quot; for deeper reasoning
)

# Returns context-aware results:
{
    &quot;categories&quot;: [...],     # Relevant topic areas (auto-prioritized)
    &quot;items&quot;: [...],          # Specific memory facts
    &quot;resources&quot;: [...],      # Original sources for traceability
    &quot;next_step_query&quot;: &quot;...&quot; # Predicted follow-up context
}
```

**Proactive Filtering**: Use `where` to scope continuous monitoring:
- `where={&quot;user_id&quot;: &quot;123&quot;}` - User-specific context
- `where={&quot;agent_id__in&quot;: [&quot;1&quot;, &quot;2&quot;]}` - Multi-agent coordination
- Omit `where` for global context awareness

---

## ğŸ’¡ Proactive Scenarios

### Example 1: Always-Learning Assistant

Continuously learns from every interaction without explicit memory commands:
```bash
export OPENAI_API_KEY=your_api_key
python examples/example_1_conversation_memory.py
```

**Proactive Behavior:**
- Automatically extracts preferences from casual mentions
- Builds relationship models from interaction patterns
- Surfaces relevant context in future conversations
- Adapts communication style based on learned preferences

**Best for:** Personal AI assistants, customer support that remembers, social chatbots

---

### Example 2: Self-Improving Agent

Learns from execution logs and proactively suggests optimizations:
```bash
export OPENAI_API_KEY=your_api_key
python examples/example_2_skill_extraction.py
```

**Proactive Behavior:**
- Monitors agent actions and outcomes continuously
- Identifies patterns in successes and failures
- Auto-generates skill guides from experience
- Proactively suggests strategies for similar future tasks

**Best for:** DevOps automation, agent self-improvement, knowledge capture

---

### Example 3: Multimodal Context Builder

Unifies memory across different input types for comprehensive context:
```bash
export OPENAI

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[QwenLM/Qwen-Agent]]></title>
            <link>https://github.com/QwenLM/Qwen-Agent</link>
            <guid>https://github.com/QwenLM/Qwen-Agent</guid>
            <pubDate>Sat, 28 Feb 2026 00:05:03 GMT</pubDate>
            <description><![CDATA[Agent framework and applications built upon Qwen>=3.0, featuring Function Calling, MCP, Code Interpreter, RAG, Chrome extension, etc.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/QwenLM/Qwen-Agent">QwenLM/Qwen-Agent</a></h1>
            <p>Agent framework and applications built upon Qwen>=3.0, featuring Function Calling, MCP, Code Interpreter, RAG, Chrome extension, etc.</p>
            <p>Language: Python</p>
            <p>Stars: 13,426</p>
            <p>Forks: 1,265</p>
            <p>Stars today: 21 stars today</p>
            <h2>README</h2><pre>&lt;!---
Copyright 2023 The Qwen team, Alibaba Group. All rights reserved.

Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
--&gt;

[ä¸­æ–‡](https://github.com/QwenLM/Qwen-Agent/blob/main/README_CN.md) ï½œ English

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/logo_qwen_agent.png&quot; width=&quot;400&quot;/&gt;
&lt;p&gt;
&lt;br&gt;

&lt;p align=&quot;center&quot;&gt;
          ğŸ’œ &lt;a href=&quot;https://chat.qwen.ai/&quot;&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbspğŸ¤— &lt;a href=&quot;https://huggingface.co/Qwen&quot;&gt;Hugging Face&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbspğŸ¤– &lt;a href=&quot;https://modelscope.cn/organization/qwen&quot;&gt;ModelScope&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbsp ğŸ“‘ &lt;a href=&quot;https://qwenlm.github.io/&quot;&gt;Blog&lt;/a&gt; &amp;nbsp&amp;nbsp ï½œ &amp;nbsp&amp;nbspğŸ“– &lt;a href=&quot;https://qwenlm.github.io/Qwen-Agent/en/&quot;&gt;Documentation&lt;/a&gt;

&lt;br&gt;
ğŸ“Š &lt;a href=&quot;https://qwenlm.github.io/Qwen-Agent/en/benchmarks/deepplanning/&quot;&gt;Benchmark&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbspğŸ’¬ &lt;a href=&quot;https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png&quot;&gt;WeChat (å¾®ä¿¡)&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbspğŸ«¨ &lt;a href=&quot;https://discord.gg/CV4E9rpNSD&quot;&gt;Discord&lt;/a&gt;&amp;nbsp&amp;nbsp
&lt;/p&gt;


Qwen-Agent is a framework for developing LLM applications based on the instruction following, tool usage, planning, and
memory capabilities of Qwen.
It also comes with example applications such as Browser Assistant, Code Interpreter, and Custom Assistant.
Now Qwen-Agent plays as the backend of [Qwen Chat](https://chat.qwen.ai/).

# News
* ğŸ”¥ğŸ”¥ğŸ”¥Feb 16, 2026: Open-sourced Qwen3.5. For usage examples, refer to [Qwen3.5 Agent Demo](./examples/assistant_qwen3.5.py).
* Jan 27, 2026: Open-sourced agent evaluation benchmark [DeepPlanning](https://qwenlm.github.io/Qwen-Agent/en/benchmarks/deepplanning/) and added Qwen-Agent [documentation](https://qwenlm.github.io/Qwen-Agent/en/guide/).
* Sep 23, 2025: Added [Qwen3-VL Tool-call Demo](./examples/cookbook_think_with_images.ipynb), supporting tools such as zoom in, image search, and web search.
* Jul 23, 2025: Add [Qwen3-Coder Tool-call Demo](./examples/assistant_qwen3_coder.py); Added native API tool call interface support, such as using vLLM&#039;s built-in tool call parsing.
* May 1, 2025: Add [Qwen3 Tool-call Demo](./examples/assistant_qwen3.py), and add [MCP Cookbooks](./examples/).
* Mar 18, 2025: Support for the `reasoning_content` field; adjust the default [Function Call template](./qwen_agent/llm/fncall_prompts/nous_fncall_prompt.py), which is applicable to the Qwen2.5 series general models and QwQ-32B. If you need to use the old version of the template, please refer to the [example](./examples/function_calling.py) for passing parameters.
* Mar 7, 2025: Added [QwQ-32B Tool-call Demo](./examples/assistant_qwq.py). It supports parallel, multi-step, and multi-turn tool calls.
* Dec 3, 2024: Upgrade GUI to Gradio 5 based. Note: GUI requires Python 3.10 or higher.
* Sep 18, 2024: Added [Qwen2.5-Math Demo](./examples/tir_math.py) to showcase the Tool-Integrated Reasoning capabilities of Qwen2.5-Math. Note: The python executor is not sandboxed and is intended for local testing only, not for production use.

# Getting Started

## Installation

- Install the stable version from PyPI:
```bash
pip install -U &quot;qwen-agent[gui,rag,code_interpreter,mcp]&quot;
# Or use `pip install -U qwen-agent` for the minimal requirements.
# The optional requirements, specified in double brackets, are:
#   [gui] for Gradio-based GUI support;
#   [rag] for RAG support;
#   [code_interpreter] for Code Interpreter support;
#   [mcp] for MCP support.
```

- Alternatively, you can install the latest development version from the source:
```bash
git clone https://github.com/QwenLM/Qwen-Agent.git
cd Qwen-Agent
pip install -e ./&quot;[gui,rag,code_interpreter,mcp]&quot;
# Or `pip install -e ./` for minimal requirements.
```

## Preparation: Model Service

You can either use the model service provided by Alibaba
Cloud&#039;s [DashScope](https://help.aliyun.com/zh/dashscope/developer-reference/quick-start), or deploy and use your own
model service using the open-source Qwen models.

- If you choose to use the model service offered by DashScope, please ensure that you set the environment
variable `DASHSCOPE_API_KEY` to your unique DashScope API key.

- Alternatively, if you prefer to deploy and use your own model service, please follow the instructions provided in the README of Qwen2 for deploying an OpenAI-compatible API service.
Specifically, consult the [vLLM](https://github.com/QwenLM/Qwen2?tab=readme-ov-file#vllm) section for high-throughput GPU deployment or the [Ollama](https://github.com/QwenLM/Qwen2?tab=readme-ov-file#ollama) section for local CPU (+GPU) deployment.
For the QwQ and Qwen3 model, it is recommended to **do not** add the `--enable-auto-tool-choice` and `--tool-call-parser hermes` parameters, as Qwen-Agent will parse the tool outputs from vLLM on its own.
For Qwen3-Coder, it is recommended to enable both of the above parameters, use vLLM&#039;s built-in tool parsing, and combine with the `use_raw_api` parameter [usage](#how-to-pass-llm-parameters-to-the-agent).

## Developing Your Own Agent

Qwen-Agent offers atomic components, such as LLMs (which inherit from `class BaseChatModel` and come with [function calling](https://github.com/QwenLM/Qwen-Agent/blob/main/examples/function_calling.py)) and Tools (which inherit
from `class BaseTool`), along with high-level components like Agents (derived from `class Agent`).

The following example illustrates the process of creating an agent capable of reading PDF files and utilizing tools, as
well as incorporating a custom tool:

```py
import pprint
import urllib.parse
import json5
from qwen_agent.agents import Assistant
from qwen_agent.tools.base import BaseTool, register_tool
from qwen_agent.utils.output_beautify import typewriter_print


# Step 1 (Optional): Add a custom tool named `my_image_gen`.
@register_tool(&#039;my_image_gen&#039;)
class MyImageGen(BaseTool):
    # The `description` tells the agent the functionality of this tool.
    description = &#039;AI painting (image generation) service, input text description, and return the image URL drawn based on text information.&#039;
    # The `parameters` tell the agent what input parameters the tool has.
    parameters = [{
        &#039;name&#039;: &#039;prompt&#039;,
        &#039;type&#039;: &#039;string&#039;,
        &#039;description&#039;: &#039;Detailed description of the desired image content, in English&#039;,
        &#039;required&#039;: True
    }]

    def call(self, params: str, **kwargs) -&gt; str:
        # `params` are the arguments generated by the LLM agent.
        prompt = json5.loads(params)[&#039;prompt&#039;]
        prompt = urllib.parse.quote(prompt)
        return json5.dumps(
            {&#039;image_url&#039;: f&#039;https://image.pollinations.ai/prompt/{prompt}&#039;},
            ensure_ascii=False)


# Step 2: Configure the LLM you are using.
llm_cfg = {
    # Use the model service provided by DashScope:
    &#039;model&#039;: &#039;qwen-max-latest&#039;,
    &#039;model_type&#039;: &#039;qwen_dashscope&#039;,
    # &#039;api_key&#039;: &#039;YOUR_DASHSCOPE_API_KEY&#039;,
    # It will use the `DASHSCOPE_API_KEY&#039; environment variable if &#039;api_key&#039; is not set here.

    # Use a model service compatible with the OpenAI API, such as vLLM or Ollama:
    # &#039;model&#039;: &#039;Qwen2.5-7B-Instruct&#039;,
    # &#039;model_server&#039;: &#039;http://localhost:8000/v1&#039;,  # base_url, also known as api_base
    # &#039;api_key&#039;: &#039;EMPTY&#039;,

    # (Optional) LLM hyperparameters for generation:
    &#039;generate_cfg&#039;: {
        &#039;top_p&#039;: 0.8
    }
}

# Step 3: Create an agent. Here we use the `Assistant` agent as an example, which is capable of using tools and reading files.
system_instruction = &#039;&#039;&#039;After receiving the user&#039;s request, you should:
- first draw an image and obtain the image url,
- then run code `request.get(image_url)` to download the image,
- and finally select an image operation from the given document to process the image.
Please show the image using `plt.show()`.&#039;&#039;&#039;
tools = [&#039;my_image_gen&#039;, &#039;code_interpreter&#039;]  # `code_interpreter` is a built-in tool for executing code. For configuration details, please refer to the FAQ.
files = [&#039;./examples/resource/doc.pdf&#039;]  # Give the bot a PDF file to read.
bot = Assistant(llm=llm_cfg,
                system_message=system_instruction,
                function_list=tools,
                files=files)

# Step 4: Run the agent as a chatbot.
messages = []  # This stores the chat history.
while True:
    # For example, enter the query &quot;draw a dog and rotate it 90 degrees&quot;.
    query = input(&#039;\nuser query: &#039;)
    # Append the user query to the chat history.
    messages.append({&#039;role&#039;: &#039;user&#039;, &#039;content&#039;: query})
    response = []
    response_plain_text = &#039;&#039;
    print(&#039;bot response:&#039;)
    for response in bot.run(messages=messages):
        # Streaming output.
        response_plain_text = typewriter_print(response, response_plain_text)
    # Append the bot responses to the chat history.
    messages.extend(response)
```

In addition to using built-in agent implementations such as `class Assistant`, you can also develop your own agent implemetation by inheriting from `class Agent`.

The framework also provides a convenient GUI interface, supporting the rapid deployment of Gradio Demos for Agents.
For example, in the case above, you can quickly launch a Gradio Demo using the following code:

```py
from qwen_agent.gui import WebUI
WebUI(bot).run()  # bot is the agent defined in the above code, we do not repeat the definition here for saving space.
```
Now you can chat with the Agent in the web UI. Please refer to the [examples](https://github.com/QwenLM/Qwen-Agent/blob/main/examples) directory for more usage examples.

# FAQ
## How to Use the Code Interpreter Tool?

We implement a code interpreter tool based on local Docker containers. You can enable the built-in `code interpreter` tool for your agent, allowing it to autonomously write code according to specific scenarios, execute it securely within an isolated sandbox environment, and return the execution results.

âš ï¸ **Note**: Before using this tool, please ensure that Docker is installed and running on your local operating system. The time required to build the container image for the first time depends on your network conditions. For Docker installation and setup instructions, please refer to the [official documentation](https://docs.docker.com/desktop/).


## How to Use MCP?

You can select the required tools on the open-source [MCP server website](https://github.com/modelcontextprotocol/servers) and configure the relevant environment.

Example of MCP invocation format:
```
{
    &quot;mcpServers&quot;: {
        &quot;memory&quot;: {
            &quot;command&quot;: &quot;npx&quot;,
            &quot;args&quot;: [&quot;-y&quot;, &quot;@modelcontextprotocol/server-memory&quot;]
        },
        &quot;filesystem&quot;: {
            &quot;command&quot;: &quot;npx&quot;,
            &quot;args&quot;: [&quot;-y&quot;, &quot;@modelcontextprotocol/server-filesystem&quot;, &quot;/path/to/allowed/files&quot;]
        },
        &quot;sqlite&quot; : {
            &quot;command&quot;: &quot;uvx&quot;,
            &quot;args&quot;: [
                &quot;mcp-server-sqlite&quot;,
                &quot;--db-path&quot;,
                &quot;test.db&quot;
            ]
        }
    }
}
```
For more details, you can refer to the [MCP usage example](./examples/assistant_mcp_sqlite_bot.py)

The dependencies required to run this example are as follows:
```
# Node.js (Download and install the latest version from the Node.js official website)
# uv 0.4.18 or higher (Check with uv --version)
# Git (Check with git --version)
# SQLite (Check with sqlite3 --version)

# For macOS users, you can install these components using Homebrew:
brew install uv git sqlite3

# For Windows users, you can install these components using winget:
winget install --id=astral-sh.uv -e
winget install git.git sqlite.sqlite
```
## Do you have function calling (aka tool calling)?

Yes. The LLM classes provide [function calling](https://github.com/QwenLM/Qwen-Agent/blob/main/examples/function_calling.py). Additionally, some Agent classes also are built upon the function calling capability, e.g., FnCallAgent and ReActChat.

The current default tool calling template natively supports **Parallel Function Calls**.

## How to pass LLM parameters to the Agent?
```py
llm_cfg = {
    # The model name being used:
    &#039;model&#039;: &#039;qwen3-32b&#039;,
    # The model service being used:
    &#039;model_type&#039;: &#039;qwen_dashscope&#039;,
    # If &#039;api_key&#039; is not set here, it will default to reading the `DASHSCOPE_API_KEY` environment variable:
    &#039;api_key&#039;: &#039;YOUR_DASHSCOPE_API_KEY&#039;,

    # Using an OpenAI API compatible model service, such as vLLM or Ollama:
    # &#039;model&#039;: &#039;qwen3-32b&#039;,
    # &#039;model_server&#039;: &#039;http://localhost:8000/v1&#039;,  # base_url, also known as api_base
    # &#039;api_key&#039;: &#039;EMPTY&#039;,

    # (Optional) LLM hyperparameters:
    &#039;generate_cfg&#039;: {
        # This parameter will affect the tool-call parsing logic. Default is False:
          # Set to True: when content is `&lt;think&gt;this is the thought&lt;/think&gt;this is the answer`
          # Set to False: when response consists of reasoning_content and content
        # &#039;thought_in_content&#039;: True,

        # tool-call template: default is nous (recommended for qwen3):
        # &#039;fncall_prompt_type&#039;: &#039;nous&#039;

        # Maximum input length, messages will be truncated if they exceed this length, please adjust according to model API:
        # &#039;max_input_tokens&#039;: 58000

        # Parameters that will be passed directly to the model API, such as top_p, enable_thinking, etc., according to the API specifications:
        # &#039;top_p&#039;: 0.8

        # Using the API&#039;s native tool call interface
        # &#039;use_raw_api&#039;: True,
    }
}
```

## How to do question-answering over super-long documents involving 1M tokens?

We have released [a fast RAG solution](https://github.com/QwenLM/Qwen-Agent/blob/main/examples/assistant_rag.py), as well as [an expensive but competitive agent](https://github.com/QwenLM/Qwen-Agent/blob/main/examples/parallel_doc_qa.py), for doing question-answering over super-long documents. They have managed to outperform native long-context models on two challenging benchmarks while being more efficient, and perform perfectly in the single-needle &quot;needle-in-the-haystack&quot; pressure test involving 1M-token contexts. See the [blog](https://qwenlm.github.io/blog/qwen-agent-2405/) for technical details.

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/qwen_agent/qwen-agent-2405-blog-long-context-results.png&quot; width=&quot;400&quot;/&gt;
&lt;p&gt;

# Application: BrowserQwen

BrowserQwen is a browser assistant built upon Qwen-Agent. Please refer to its [documentation](https://github.com/QwenLM/Qwen-Agent/blob/main/browser_qwen.md) for details.

# Disclaimer

The Docker container-based code interpreter mounts only the specified working directory and implements basic sandbox isolation, but it should still be used with caution in production environments.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>