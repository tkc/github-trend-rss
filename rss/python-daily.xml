<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sun, 07 Dec 2025 00:05:01 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[microsoft/VibeVoice]]></title>
            <link>https://github.com/microsoft/VibeVoice</link>
            <guid>https://github.com/microsoft/VibeVoice</guid>
            <pubDate>Sun, 07 Dec 2025 00:05:01 GMT</pubDate>
            <description><![CDATA[Open-Source Frontier Voice AI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/VibeVoice">microsoft/VibeVoice</a></h1>
            <p>Open-Source Frontier Voice AI</p>
            <p>Language: Python</p>
            <p>Stars: 11,038</p>
            <p>Forks: 1,393</p>
            <p>Stars today: 452 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

## üéôÔ∏è VibeVoice: Open-Source Frontier Voice AI
[![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=microsoft)](https://microsoft.github.io/VibeVoice)
[![Hugging Face](https://img.shields.io/badge/HuggingFace-Collection-orange?logo=huggingface)](https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f)
[![Technical Report](https://img.shields.io/badge/Technical-Report-red?logo=adobeacrobatreader)](https://arxiv.org/pdf/2508.19205)


&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;Figures/VibeVoice_logo_white.png&quot;&gt;
  &lt;img src=&quot;Figures/VibeVoice_logo.png&quot; alt=&quot;VibeVoice Logo&quot; width=&quot;300&quot;&gt;
&lt;/picture&gt;
&lt;/div&gt;

&lt;div align=&quot;left&quot;&gt;

&lt;h3&gt;üì∞ News&lt;/h3&gt;

&lt;img src=&quot;https://img.shields.io/badge/Status-New-brightgreen?style=flat&quot; alt=&quot;New&quot; /&gt;
&lt;img src=&quot;https://img.shields.io/badge/Feature-Realtime_TTS-blue?style=flat&amp;logo=soundcharts&quot; alt=&quot;Realtime TTS&quot; /&gt;

&lt;strong&gt;2025-12-03: üì£ We open-sourced &lt;a href=&quot;docs/vibevoice-realtime-0.5b.md&quot;&gt;&lt;strong&gt;VibeVoice‚ÄëRealtime‚Äë0.5B&lt;/strong&gt;&lt;/a&gt;, a real‚Äëtime text‚Äëto‚Äëspeech model that supports streaming text input and robust long-form speech generation. Try it on [Colab](https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb).&lt;/strong&gt;

To mitigate deepfake risks and ensure low latency for the first speech chunk, voice prompts are provided in an embedded format. For users requiring voice customization, please reach out to our team. We will also be expanding the range of available speakers.
&lt;br&gt;

https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc

&gt; (Launch your own realtime demo via the websocket example in [Usage](docs/vibevoice-realtime-0.5b.md#usage-1-launch-real-time-websocket-demo)).

&lt;/div&gt;

2025-09-05: VibeVoice is an open-source research framework intended to advance collaboration in the speech synthesis community. After release, we discovered instances where the tool was used in ways inconsistent with the stated intent. Since responsible use of AI is one of Microsoft‚Äôs guiding principles, we have disabled this repo until we are confident that out-of-scope use is no longer possible.


### Overview

VibeVoice is a novel framework designed for generating **expressive**, **long-form**, **multi-speaker** conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking.

VibeVoice currently includes two model variants:

- **Long-form multi-speaker model**: Synthesizes conversational/single-speaker speech up to **90 minutes** with up to **4 distinct speakers**, surpassing the typical 1‚Äì2 speaker limits of many prior models.
- **[Realtime streaming TTS model](docs/vibevoice-realtime-0.5b.md)**: Produces initial audible speech in ~**300 ms** and supports **streaming text input** for single-speaker **real-time** speech generation; designed for low-latency generation.

A core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a [next-token diffusion](https://arxiv.org/abs/2412.08635) framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.


&lt;p align=&quot;left&quot;&gt;
  &lt;img src=&quot;Figures/MOS-preference.png&quot; alt=&quot;MOS Preference Results&quot; height=&quot;260px&quot;&gt;
  &lt;img src=&quot;Figures/VibeVoice.jpg&quot; alt=&quot;VibeVoice Overview&quot; height=&quot;250px&quot; style=&quot;margin-right: 10px;&quot;&gt;
&lt;/p&gt;


### üéµ Demo Examples


**Video Demo**

We produced this video with [Wan2.2](https://github.com/Wan-Video/Wan2.2). We sincerely appreciate the Wan-Video team for their great work.

**English**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784

&lt;/div&gt;


**Chinese**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f

&lt;/div&gt;

**Cross-Lingual**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722

&lt;/div&gt;

**Spontaneous Singing**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730

&lt;/div&gt;


**Long Conversation with 4 people**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727

&lt;/div&gt;

For more examples, see the [Project Page](https://microsoft.github.io/VibeVoice).



## Risks and limitations

While efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release).
Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.

English and Chinese only: Transcripts in languages other than English or Chinese may result in unexpected audio outputs.

Non-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects.

Overlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations.

We do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[RosettaCommons/foundry]]></title>
            <link>https://github.com/RosettaCommons/foundry</link>
            <guid>https://github.com/RosettaCommons/foundry</guid>
            <pubDate>Sun, 07 Dec 2025 00:05:00 GMT</pubDate>
            <description><![CDATA[Central repository for biomolecular foundation models with shared trainers and pipeline components]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/RosettaCommons/foundry">RosettaCommons/foundry</a></h1>
            <p>Central repository for biomolecular foundation models with shared trainers and pipeline components</p>
            <p>Language: Python</p>
            <p>Stars: 411</p>
            <p>Forks: 58</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre># Protein design with Foundry

Foundry provides tooling and infrastructure for using and training all classes of models for protein design, including design (RFD3), inverse folding (ProteinMPNN) and protein folding (RF3).

All models within Foundry rely on [AtomWorks](https://github.com/RosettaCommons/atomworks) - a unified framework for manipulating and processing biomolecular structures - for both training and inference. 

## Getting Started
### Quickstart guide
**Installation**
```bash
pip install rc-foundry[all]
```

**Downloading weights** Models can be downloaded to a target folder with:
```
foundry install base-models --checkpoint-dir &lt;path/to/ckpt/dir&gt;
```
where `checkpoint-dir` will be `~/.foundry/checkpoints` by default. Foundry always searches `~/.foundry/checkpoints` plus any colon-separated entries in `$FOUNDRY_CHECKPOINT_DIRS` during inference or subsequent commands to find checkpoints. `base-models` installs the latest RFD3, RF3 and MPNN variants - you can also download all of the models supported (including multiple checkpoints of RF3) with `all`, or by listing the models sequentially (e.g. `foundry install rfd3 rf3 ...`).
To list the registry of available checkpoints:
```
foundry list-available
```
To check what you already have downloaded (searches `~/.foundry/checkpoints` plus `$FOUNDRY_CHECKPOINT_DIRS` if set):
```
foundry list-installed
```

&gt;*See `examples/all.ipynb` for how to run each model and design proteins end-to-end in a notebook.*

### Google Colab
For an interactive Google Colab notebook walking through a basic design pipeline with RFD3, MPNN, and RF3, please see the [IPD Design Pipeline Tutorial](https://colab.research.google.com/drive/1ZwIMV3n9h0ZOnIXX0GyKUuoiahgifBxh?usp=sharing).

### RFdiffusion3 (RFD3)

[RFdiffusion3](https://www.biorxiv.org/content/10.1101/2025.09.18.676967v2) is an all-atom generative model capable of designing protein structures under complex constraints. 

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/_static/cover.png&quot; alt=&quot;RFdiffusion3 generation trajectory.&quot; width=&quot;700&quot;&gt;
&lt;/div&gt;

&gt; *See [models/rfd3/README.md](models/rfd3/README.md) for complete documentation.*

### RosettaFold3 (RF3)

[RF3](https://doi.org/10.1101/2025.08.14.670328) is a structure prediction neural network that narrows the gap between closed-source AF-3 and open-source alternatives.

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/_static/prot_dna.png&quot; alt=&quot;Protein-DNA complex prediction&quot; width=&quot;400&quot;&gt;
&lt;/div&gt;

&gt; *See [models/rf3/README.md](models/rf3/README.md) for complete documentation.*

### ProteinMPNN
[ProteinMPNN](https://www.science.org/doi/10.1126/science.add2187) and [LigandMPNN](https://www.nature.com/articles/s41592-025-02626-1) are lightweight inverse-folding models which can be use to design diverse sequences for backbones under constrained conditions.

&gt; *See [models/mpnn/README.md](models/mpnn/README.md) for complete documentation.*

---

## Development

### Code Organization

**Strict dependency flow:** `foundry` ‚Üí `atomworks`

- **atomworks**: Structure I/O, preprocessing, featurization
- **foundry**: Model architectures, training, inference endpoints
- **models/\&lt;model\&gt;:** Released models.

#### For Core Developers (Multiple Packages)

Install both `foundry` and models in editable mode for development:

```bash
uv pip install -e &#039;.[all,dev]&#039;
```

This approach allows you to:
- Modify `foundry` shared utilities and see changes immediately
- Work on specific models without installing all models
- Add new models as independent packages in `models/`

&gt; [!NOTE]
&gt; Running tests is not currently supported, test files may be missing.

### Adding New Models

To add a new model:

1. Create `models/&lt;model_name&gt;/` directory with its own `pyproject.toml`
2. Add `foundry` as a dependency
3. Implement model-specific code in `models/&lt;model_name&gt;/src/`
4. Users can install with: `uv pip install -e ./models/&lt;model_name&gt;`

### Pre-commit Formatting

We ship a `.pre-commit-config.yaml` that runs `make format` (via `ruff format`) before each commit. Enable it once per clone:

```bash
pip install pre-commit  # if not already installed
pre-commit install
```

After installation the hook automatically formats the repo whenever you `git commit`. Use `pre-commit run --all-files` to apply it manually.

## Citation

If you use this repository code or data in your work, please cite the relavant work as below:

```bibtex
@article{corley2025accelerating,
  title={Accelerating biomolecular modeling with atomworks and rf3},
  author={Corley, Nathaniel and Mathis, Simon and Krishna, Rohith and Bauer, Magnus S and Thompson, Tuscan R and Ahern, Woody and Kazman, Maxwell W and Brent, Rafael I and Didi, Kieran and Kubaney, Andrew and others},
  journal={bioRxiv},
  year={2025}
}

@article {butcher2025_rfdiffusion3,
    author = {Butcher, Jasper and Krishna, Rohith and Mitra, Raktim and Brent, Rafael Isaac and Li, Yanjing and Corley, Nathaniel and Kim, Paul T and Funk, Jonathan and Mathis, Simon Valentin and Salike, Saman and Muraishi, Aiko and Eisenach, Helen and Thompson, Tuscan Rock and Chen, Jie and Politanska, Yuliya and Sehgal, Enisha and Coventry, Brian and Zhang, Odin and Qiang, Bo and Didi, Kieran and Kazman, Maxwell and DiMaio, Frank and Baker, David},
    title = {De novo Design of All-atom Biomolecular Interactions with RFdiffusion3},
    elocation-id = {2025.09.18.676967},
    year = {2025},
    doi = {10.1101/2025.09.18.676967},
    publisher = {Cold Spring Harbor Laboratory},
    URL = {https://www.biorxiv.org/content/early/2025/11/19/2025.09.18.676967},
    eprint = {https://www.biorxiv.org/content/early/2025/11/19/2025.09.18.676967.full.pdf},
    journal = {bioRxiv}
}

@article{dauparas2022robust,
  title={Robust deep learning--based protein sequence design using ProteinMPNN},
  author={Dauparas, Justas and Anishchenko, Ivan and Bennett, Nathaniel and Bai, Hua and Ragotte, Robert J and Milles, Lukas F and Wicky, Basile IM and Courbet, Alexis and de Haas, Rob J and Bethel, Neville and others},
  journal={Science},
  volume={378},
  number={6615},
  pages={49--56},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

@article{dauparas2025atomic,
  title={Atomic context-conditioned protein sequence design using LigandMPNN},
  author={Dauparas, Justas and Lee, Gyu Rie and Pecoraro, Robert and An, Linna and Anishchenko, Ivan and Glasscock, Cameron and Baker, David},
  journal={Nature Methods},
  pages={1--7},
  year={2025},
  publisher={Nature Publishing Group US New York}
}
```
## Acknowledgments
We thank Rachel Clune and Hope Woods from the RosettaCommons for their collaboration on the codebase, documentation, tutorials and examples. 
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sapientinc/HRM]]></title>
            <link>https://github.com/sapientinc/HRM</link>
            <guid>https://github.com/sapientinc/HRM</guid>
            <pubDate>Sun, 07 Dec 2025 00:04:59 GMT</pubDate>
            <description><![CDATA[Hierarchical Reasoning Model Official Release]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sapientinc/HRM">sapientinc/HRM</a></h1>
            <p>Hierarchical Reasoning Model Official Release</p>
            <p>Language: Python</p>
            <p>Stars: 12,058</p>
            <p>Forks: 1,761</p>
            <p>Stars today: 29 stars today</p>
            <h2>README</h2><pre># Hierarchical Reasoning Model

![](./assets/hrm.png)

Reasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI.
Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency.
HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or CoT data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes.
Furthermore, HRM outperforms much larger models with significantly longer context windows on the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial general intelligence capabilities.
These results underscore HRM‚Äôs potential as a transformative advancement toward universal computation and general-purpose reasoning systems.

**Join our Discord Community: [https://discord.gg/sapient](https://discord.gg/sapient)**


## Quick Start Guide üöÄ

### Prerequisites ‚öôÔ∏è

Ensure PyTorch and CUDA are installed. The repo needs CUDA extensions to be built. If not present, run the following commands:

```bash
# Install CUDA 12.6
CUDA_URL=https://developer.download.nvidia.com/compute/cuda/12.6.3/local_installers/cuda_12.6.3_560.35.05_linux.run

wget -q --show-progress --progress=bar:force:noscroll -O cuda_installer.run $CUDA_URL
sudo sh cuda_installer.run --silent --toolkit --override

export CUDA_HOME=/usr/local/cuda-12.6

# Install PyTorch with CUDA 12.6
PYTORCH_INDEX_URL=https://download.pytorch.org/whl/cu126

pip3 install torch torchvision torchaudio --index-url $PYTORCH_INDEX_URL

# Additional packages for building extensions
pip3 install packaging ninja wheel setuptools setuptools-scm
```

Then install FlashAttention. For Hopper GPUs, install FlashAttention 3

```bash
git clone git@github.com:Dao-AILab/flash-attention.git
cd flash-attention/hopper
python setup.py install
```

For Ampere or earlier GPUs, install FlashAttention 2

```bash
pip3 install flash-attn
```

## Install Python Dependencies üêç

```bash
pip install -r requirements.txt
```

## W&amp;B Integration üìà

This project uses [Weights &amp; Biases](https://wandb.ai/) for experiment tracking and metric visualization. Ensure you&#039;re logged in:

```bash
wandb login
```

## Run Experiments

### Quick Demo: Sudoku Solver üíªüó≤

Train a master-level Sudoku AI capable of solving extremely difficult puzzles on a modern laptop GPU. üß©

```bash
# Download and build Sudoku dataset
python dataset/build_sudoku_dataset.py --output-dir data/sudoku-extreme-1k-aug-1000  --subsample-size 1000 --num-aug 1000

# Start training (single GPU, smaller batch size)
OMP_NUM_THREADS=8 python pretrain.py data_path=data/sudoku-extreme-1k-aug-1000 epochs=20000 eval_interval=2000 global_batch_size=384 lr=7e-5 puzzle_emb_lr=7e-5 weight_decay=1.0 puzzle_emb_weight_decay=1.0
```

Runtime: ~10 hours on a RTX 4070 laptop GPU

## Trained Checkpoints üöß

 - [ARC-AGI-2](https://huggingface.co/sapientinc/HRM-checkpoint-ARC-2)
 - [Sudoku 9x9 Extreme (1000 examples)](https://huggingface.co/sapientinc/HRM-checkpoint-sudoku-extreme)
 - [Maze 30x30 Hard (1000 examples)](https://huggingface.co/sapientinc/HRM-checkpoint-maze-30x30-hard)

To use the checkpoints, see Evaluation section below.

## Full-scale Experiments üîµ

Experiments below assume an 8-GPU setup.

### Dataset Preparation

```bash
# Initialize submodules
git submodule update --init --recursive

# ARC-1
python dataset/build_arc_dataset.py  # ARC offical + ConceptARC, 960 examples
# ARC-2
python dataset/build_arc_dataset.py --dataset-dirs dataset/raw-data/ARC-AGI-2/data --output-dir data/arc-2-aug-1000  # ARC-2 official, 1120 examples

# Sudoku-Extreme
python dataset/build_sudoku_dataset.py  # Full version
python dataset/build_sudoku_dataset.py --output-dir data/sudoku-extreme-1k-aug-1000  --subsample-size 1000 --num-aug 1000  # 1000 examples

# Maze
python dataset/build_maze_dataset.py  # 1000 examples
```

### Dataset Visualization

Explore the puzzles visually:

* Open `puzzle_visualizer.html` in your browser.
* Upload the generated dataset folder located in `data/...`.

## Launch experiments

### Small-sample (1K)

ARC-1:

```bash
OMP_NUM_THREADS=8 torchrun --nproc-per-node 8 pretrain.py 
```

*Runtime:* ~24 hours

ARC-2:

```bash
OMP_NUM_THREADS=8 torchrun --nproc-per-node 8 pretrain.py data_path=data/arc-2-aug-1000
```

*Runtime:* ~24 hours (checkpoint after 8 hours is often sufficient)

Sudoku Extreme (1k):

```bash
OMP_NUM_THREADS=8 torchrun --nproc-per-node 8 pretrain.py data_path=data/sudoku-extreme-1k-aug-1000 epochs=20000 eval_interval=2000 lr=1e-4 puzzle_emb_lr=1e-4 weight_decay=1.0 puzzle_emb_weight_decay=1.0
```

*Runtime:* ~10 minutes

Maze 30x30 Hard (1k):

```bash
OMP_NUM_THREADS=8 torchrun --nproc-per-node 8 pretrain.py data_path=data/maze-30x30-hard-1k epochs=20000 eval_interval=2000 lr=1e-4 puzzle_emb_lr=1e-4 weight_decay=1.0 puzzle_emb_weight_decay=1.0
```

*Runtime:* ~1 hour

### Full Sudoku-Hard

```bash
OMP_NUM_THREADS=8 torchrun --nproc-per-node 8 pretrain.py data_path=data/sudoku-hard-full epochs=100 eval_interval=10 lr_min_ratio=0.1 global_batch_size=2304 lr=3e-4 puzzle_emb_lr=3e-4 weight_decay=0.1 puzzle_emb_weight_decay=0.1 arch.loss.loss_type=softmax_cross_entropy arch.L_cycles=8 arch.halt_max_steps=8 arch.pos_encodings=learned
```

*Runtime:* ~2 hours

## Evaluation

Evaluate your trained models:

* Check `eval/exact_accuracy` in W&amp;B.
* For ARC-AGI, follow these additional steps:

```bash
OMP_NUM_THREADS=8 torchrun --nproc-per-node 8 evaluate.py checkpoint=&lt;CHECKPOINT_PATH&gt;
```

* Then use the provided `arc_eval.ipynb` notebook to finalize and inspect your results.

## Notes

 - Small-sample learning typically exhibits accuracy variance of around ¬±2 points.
 - For Sudoku-Extreme (1,000-example dataset), late-stage overfitting may cause numerical instability during training and Q-learning. It is advisable to use early stopping once the training accuracy approaches 100%.

## Citation üìú

```bibtex
@misc{wang2025hierarchicalreasoningmodel,
      title={Hierarchical Reasoning Model}, 
      author={Guan Wang and Jin Li and Yuhao Sun and Xing Chen and Changling Liu and Yue Wu and Meng Lu and Sen Song and Yasin Abbasi Yadkori},
      year={2025},
      eprint={2506.21734},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2506.21734}, 
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[anthropics/claude-quickstarts]]></title>
            <link>https://github.com/anthropics/claude-quickstarts</link>
            <guid>https://github.com/anthropics/claude-quickstarts</guid>
            <pubDate>Sun, 07 Dec 2025 00:04:58 GMT</pubDate>
            <description><![CDATA[A collection of projects designed to help developers quickly get started with building deployable applications using the Claude API]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/anthropics/claude-quickstarts">anthropics/claude-quickstarts</a></h1>
            <p>A collection of projects designed to help developers quickly get started with building deployable applications using the Claude API</p>
            <p>Language: Python</p>
            <p>Stars: 10,852</p>
            <p>Forks: 1,970</p>
            <p>Stars today: 77 stars today</p>
            <h2>README</h2><pre># Claude Quickstarts

Claude Quickstarts is a collection of projects designed to help developers quickly get started with building  applications using the Claude API. Each quickstart provides a foundation that you can easily build upon and customize for your specific needs.

## Getting Started

To use these quickstarts, you&#039;ll need an Claude API key. If you don&#039;t have one yet, you can sign up for free at [console.anthropic.com](https://console.anthropic.com).

## Available Quickstarts

### Customer Support Agent

A customer support agent powered by Claude. This project demonstrates how to leverage Claude&#039;s natural language understanding and generation capabilities to create an AI-assisted customer support system with access to a knowledge base.

[Go to Customer Support Agent Quickstart](./customer-support-agent)

### Financial Data Analyst

A financial data analyst powered by Claude. This project demonstrates how to leverage Claude&#039;s capabilities with interactive data visualization to analyze financial data via chat.

[Go to Financial Data Analyst Quickstart](./financial-data-analyst)

### Computer Use Demo

An environment and tools that Claude can use to control a desktop computer. This project demonstrates how to leverage the computer use capabilities of Claude, including support for the latest `computer_use_20251124` tool version with zoom actions.

[Go to Computer Use Demo Quickstart](./computer-use-demo)

### Autonomous Coding Agent

An autonomous coding agent powered by the Claude Agent SDK. This project demonstrates a two-agent pattern (initializer + coding agent) that can build complete applications over multiple sessions, with progress persisted via git and a feature list that the agent works through incrementally.

[Go to Autonomous Coding Agent Quickstart](./autonomous-coding)

## General Usage

Each quickstart project comes with its own README and setup instructions. Generally, you&#039;ll follow these steps:

1. Clone this repository
2. Navigate to the specific quickstart directory
3. Install the required dependencies
4. Set up your Claude API key as an environment variable
5. Run the quickstart application

## Explore Further

To deepen your understanding of working with Claude and the Claude API, check out these resources:

- [Claude API Documentation](https://docs.claude.com)
- [Claude Cookbooks](https://github.com/anthropics/claude-cookbooks) - A collection of code snippets and guides for common tasks
- [Claude API Fundamentals Course](https://github.com/anthropics/courses/tree/master/anthropic_api_fundamentals)

## Contributing

We welcome contributions to the Claude Quickstarts repository! If you have ideas for new quickstart projects or improvements to existing ones, please open an issue or submit a pull request.

## Community and Support

- Join our [Anthropic Discord community](https://www.anthropic.com/discord) for discussions and support
- Check out the [Anthropic support documentation](https://support.anthropic.com) for additional help

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[zhu-xlab/GlobalBuildingAtlas]]></title>
            <link>https://github.com/zhu-xlab/GlobalBuildingAtlas</link>
            <guid>https://github.com/zhu-xlab/GlobalBuildingAtlas</guid>
            <pubDate>Sun, 07 Dec 2025 00:04:57 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/zhu-xlab/GlobalBuildingAtlas">zhu-xlab/GlobalBuildingAtlas</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 741</p>
            <p>Forks: 73</p>
            <p>Stars today: 23 stars today</p>
            <h2>README</h2><pre># GlobalBuildingAtlas

## Introduction
In this project, we provide the level of detail 1 (LoD1) data of buildings across the globe.

A overview of the dataset is illustrated bellow:

&lt;img src=&quot;figures/overview.png&quot; width=&quot;800&quot;&gt;


## Access to the Data
### Web Feature Service (WFS)
A WFS is provided so that one can access the data using other websites or GIS softwares such as QGIS and ArcGIS.

Url: `https://tubvsig-so2sat-vm1.srv.mwn.de/geoserver/ows?`

### Web Viewer
A web interface for viewing the data is available at: [website](https://tubvsig-so2sat-vm1.srv.mwn.de)

### Full Data Download
The full data can be downloaded from [mediaTUM](https://mediatum.ub.tum.de/1782307)

## Development Code
### Global Building Polygon Generation using Satellite Data (Sec. 4.3)
For codes related to building map extraction, regularization, polygonization, and simplification, i.e., generating building polygons from satellite images (Sec. 4.3.2, Sec. 4.3.3, and Sec. 4.3.4), please refer to `./im2bf`.

### Global Building Height Estimation (Sec. 4.4)
1. For codes related to monocular height estimation using HTC-DC Net (Sec. 4.4.2), please refer to `./im2bh`.
2. For codes related to the global inference and uncertainty quantification (Sec. 4.4.3), please refer to `./infer_height`

### Global LoD1 Building Model Generation (Sec. 4.5)
1. For codes related to quality-guided building polygon fusion (Sec. 4.5.1), please refer to `./fuse_bf`.
2. For codes related to LoD1 building model generation (Sec. 4.5.2), please refer to `./make_lod1`.

## Visualization Code
For codes to reproduce the plots in the manuscript, please refer to `./make_plots`.

## Code License
MIT with Commons Clause (no commercial use allowed). See [LICENSE](https://github.com/zhu-xlab/GlobalBuildingAtlas/blob/main/LICENSE).

## How to cite
If you find this dataset helpful in your work, please cite the following paper.
```
@misc{zhu2025globalbuildingatlasopenglobalcomplete,
      title={GlobalBuildingAtlas: An Open Global and Complete Dataset of Building Polygons, Heights and LoD1 3D Models}, 
      author={Xiao Xiang Zhu and Sining Chen and Fahong Zhang and Yilei Shi and Yuanyuan Wang},
      year={2025},
      eprint={2506.04106},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2506.04106}, 
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[google/langextract]]></title>
            <link>https://github.com/google/langextract</link>
            <guid>https://github.com/google/langextract</guid>
            <pubDate>Sun, 07 Dec 2025 00:04:56 GMT</pubDate>
            <description><![CDATA[A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google/langextract">google/langextract</a></h1>
            <p>A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.</p>
            <p>Language: Python</p>
            <p>Stars: 17,149</p>
            <p>Forks: 1,214</p>
            <p>Stars today: 33 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/google/langextract&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/google/langextract/main/docs/_static/logo.svg&quot; alt=&quot;LangExtract Logo&quot; width=&quot;128&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

# LangExtract

[![PyPI version](https://img.shields.io/pypi/v/langextract.svg)](https://pypi.org/project/langextract/)
[![GitHub stars](https://img.shields.io/github/stars/google/langextract.svg?style=social&amp;label=Star)](https://github.com/google/langextract)
![Tests](https://github.com/google/langextract/actions/workflows/ci.yaml/badge.svg)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.17015089.svg)](https://doi.org/10.5281/zenodo.17015089)

## Table of Contents

- [Introduction](#introduction)
- [Why LangExtract?](#why-langextract)
- [Quick Start](#quick-start)
- [Installation](#installation)
- [API Key Setup for Cloud Models](#api-key-setup-for-cloud-models)
- [Adding Custom Model Providers](#adding-custom-model-providers)
- [Using OpenAI Models](#using-openai-models)
- [Using Local LLMs with Ollama](#using-local-llms-with-ollama)
- [More Examples](#more-examples)
  - [*Romeo and Juliet* Full Text Extraction](#romeo-and-juliet-full-text-extraction)
  - [Medication Extraction](#medication-extraction)
  - [Radiology Report Structuring: RadExtract](#radiology-report-structuring-radextract)
- [Community Providers](#community-providers)
- [Contributing](#contributing)
- [Testing](#testing)
- [Disclaimer](#disclaimer)

## Introduction

LangExtract is a Python library that uses LLMs to extract structured information from unstructured text documents based on user-defined instructions. It processes materials such as clinical notes or reports, identifying and organizing key details while ensuring the extracted data corresponds to the source text.

## Why LangExtract?

1.  **Precise Source Grounding:** Maps every extraction to its exact location in the source text, enabling visual highlighting for easy traceability and verification.
2.  **Reliable Structured Outputs:** Enforces a consistent output schema based on your few-shot examples, leveraging controlled generation in supported models like Gemini to guarantee robust, structured results.
3.  **Optimized for Long Documents:** Overcomes the &quot;needle-in-a-haystack&quot; challenge of large document extraction by using an optimized strategy of text chunking, parallel processing, and multiple passes for higher recall.
4.  **Interactive Visualization:** Instantly generates a self-contained, interactive HTML file to visualize and review thousands of extracted entities in their original context.
5.  **Flexible LLM Support:** Supports your preferred models, from cloud-based LLMs like the Google Gemini family to local open-source models via the built-in Ollama interface.
6.  **Adaptable to Any Domain:** Define extraction tasks for any domain using just a few examples. LangExtract adapts to your needs without requiring any model fine-tuning.
7.  **Leverages LLM World Knowledge:** Utilize precise prompt wording and few-shot examples to influence how the extraction task may utilize LLM knowledge. The accuracy of any inferred information and its adherence to the task specification are contingent upon the selected LLM, the complexity of the task, the clarity of the prompt instructions, and the nature of the prompt examples.

## Quick Start

&gt; **Note:** Using cloud-hosted models like Gemini requires an API key. See the [API Key Setup](#api-key-setup-for-cloud-models) section for instructions on how to get and configure your key.

Extract structured information with just a few lines of code.

### 1. Define Your Extraction Task

First, create a prompt that clearly describes what you want to extract. Then, provide a high-quality example to guide the model.

```python
import langextract as lx
import textwrap

# 1. Define the prompt and extraction rules
prompt = textwrap.dedent(&quot;&quot;&quot;\
    Extract characters, emotions, and relationships in order of appearance.
    Use exact text for extractions. Do not paraphrase or overlap entities.
    Provide meaningful attributes for each entity to add context.&quot;&quot;&quot;)

# 2. Provide a high-quality example to guide the model
examples = [
    lx.data.ExampleData(
        text=&quot;ROMEO. But soft! What light through yonder window breaks? It is the east, and Juliet is the sun.&quot;,
        extractions=[
            lx.data.Extraction(
                extraction_class=&quot;character&quot;,
                extraction_text=&quot;ROMEO&quot;,
                attributes={&quot;emotional_state&quot;: &quot;wonder&quot;}
            ),
            lx.data.Extraction(
                extraction_class=&quot;emotion&quot;,
                extraction_text=&quot;But soft!&quot;,
                attributes={&quot;feeling&quot;: &quot;gentle awe&quot;}
            ),
            lx.data.Extraction(
                extraction_class=&quot;relationship&quot;,
                extraction_text=&quot;Juliet is the sun&quot;,
                attributes={&quot;type&quot;: &quot;metaphor&quot;}
            ),
        ]
    )
]
```

### 2. Run the Extraction

Provide your input text and the prompt materials to the `lx.extract` function.

```python
# The input text to be processed
input_text = &quot;Lady Juliet gazed longingly at the stars, her heart aching for Romeo&quot;

# Run the extraction
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gemini-2.5-flash&quot;,
)
```

&gt; **Model Selection**: `gemini-2.5-flash` is the recommended default, offering an excellent balance of speed, cost, and quality. For highly complex tasks requiring deeper reasoning, `gemini-2.5-pro` may provide superior results. For large-scale or production use, a Tier 2 Gemini quota is suggested to increase throughput and avoid rate limits. See the [rate-limit documentation](https://ai.google.dev/gemini-api/docs/rate-limits#tier-2) for details.
&gt;
&gt; **Model Lifecycle**: Note that Gemini models have a lifecycle with defined retirement dates. Users should consult the [official model version documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions) to stay informed about the latest stable and legacy versions.

### 3. Visualize the Results

The extractions can be saved to a `.jsonl` file, a popular format for working with language model data. LangExtract can then generate an interactive HTML visualization from this file to review the entities in context.

```python
# Save the results to a JSONL file
lx.io.save_annotated_documents([result], output_name=&quot;extraction_results.jsonl&quot;, output_dir=&quot;.&quot;)

# Generate the visualization from the file
html_content = lx.visualize(&quot;extraction_results.jsonl&quot;)
with open(&quot;visualization.html&quot;, &quot;w&quot;) as f:
    if hasattr(html_content, &#039;data&#039;):
        f.write(html_content.data)  # For Jupyter/Colab
    else:
        f.write(html_content)
```

This creates an animated and interactive HTML file:

![Romeo and Juliet Basic Visualization ](https://raw.githubusercontent.com/google/langextract/main/docs/_static/romeo_juliet_basic.gif)

&gt; **Note on LLM Knowledge Utilization:** This example demonstrates extractions that stay close to the text evidence - extracting &quot;longing&quot; for Lady Juliet&#039;s emotional state and identifying &quot;yearning&quot; from &quot;gazed longingly at the stars.&quot; The task could be modified to generate attributes that draw more heavily from the LLM&#039;s world knowledge (e.g., adding `&quot;identity&quot;: &quot;Capulet family daughter&quot;` or `&quot;literary_context&quot;: &quot;tragic heroine&quot;`). The balance between text-evidence and knowledge-inference is controlled by your prompt instructions and example attributes.

### Scaling to Longer Documents

For larger texts, you can process entire documents directly from URLs with parallel processing and enhanced sensitivity:

```python
# Process Romeo &amp; Juliet directly from Project Gutenberg
result = lx.extract(
    text_or_documents=&quot;https://www.gutenberg.org/files/1513/1513-0.txt&quot;,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gemini-2.5-flash&quot;,
    extraction_passes=3,    # Improves recall through multiple passes
    max_workers=20,         # Parallel processing for speed
    max_char_buffer=1000    # Smaller contexts for better accuracy
)
```

This approach can extract hundreds of entities from full novels while maintaining high accuracy. The interactive visualization seamlessly handles large result sets, making it easy to explore hundreds of entities from the output JSONL file. **[See the full *Romeo and Juliet* extraction example ‚Üí](https://github.com/google/langextract/blob/main/docs/examples/longer_text_example.md)** for detailed results and performance insights.

### Vertex AI Batch Processing

Save costs on large-scale tasks by enabling Vertex AI Batch API: `language_model_params={&quot;vertexai&quot;: True, &quot;batch&quot;: {&quot;enabled&quot;: True}}`.

See an example of the Vertex AI Batch API usage in [this example](docs/examples/batch_api_example.md).

## Installation

### From PyPI

```bash
pip install langextract
```

*Recommended for most users. For isolated environments, consider using a virtual environment:*

```bash
python -m venv langextract_env
source langextract_env/bin/activate  # On Windows: langextract_env\Scripts\activate
pip install langextract
```

### From Source

LangExtract uses modern Python packaging with `pyproject.toml` for dependency management:

*Installing with `-e` puts the package in development mode, allowing you to modify the code without reinstalling.*


```bash
git clone https://github.com/google/langextract.git
cd langextract

# For basic installation:
pip install -e .

# For development (includes linting tools):
pip install -e &quot;.[dev]&quot;

# For testing (includes pytest):
pip install -e &quot;.[test]&quot;
```

### Docker

```bash
docker build -t langextract .
docker run --rm -e LANGEXTRACT_API_KEY=&quot;your-api-key&quot; langextract python your_script.py
```

## API Key Setup for Cloud Models

When using LangExtract with cloud-hosted models (like Gemini or OpenAI), you&#039;ll need to
set up an API key. On-device models don&#039;t require an API key. For developers
using local LLMs, LangExtract offers built-in support for Ollama and can be
extended to other third-party APIs by updating the inference endpoints.

### API Key Sources

Get API keys from:

*   [AI Studio](https://aistudio.google.com/app/apikey) for Gemini models
*   [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/sdks/overview) for enterprise use
*   [OpenAI Platform](https://platform.openai.com/api-keys) for OpenAI models

### Setting up API key in your environment

**Option 1: Environment Variable**

```bash
export LANGEXTRACT_API_KEY=&quot;your-api-key-here&quot;
```

**Option 2: .env File (Recommended)**

Add your API key to a `.env` file:

```bash
# Add API key to .env file
cat &gt;&gt; .env &lt;&lt; &#039;EOF&#039;
LANGEXTRACT_API_KEY=your-api-key-here
EOF

# Keep your API key secure
echo &#039;.env&#039; &gt;&gt; .gitignore
```

In your Python code:
```python
import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=&quot;Extract information...&quot;,
    examples=[...],
    model_id=&quot;gemini-2.5-flash&quot;
)
```

**Option 3: Direct API Key (Not Recommended for Production)**

You can also provide the API key directly in your code, though this is not recommended for production use:

```python
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=&quot;Extract information...&quot;,
    examples=[...],
    model_id=&quot;gemini-2.5-flash&quot;,
    api_key=&quot;your-api-key-here&quot;  # Only use this for testing/development
)
```

**Option 4: Vertex AI (Service Accounts)**

Use [Vertex AI](https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform) for authentication with service accounts:

```python
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=&quot;Extract information...&quot;,
    examples=[...],
    model_id=&quot;gemini-2.5-flash&quot;,
    language_model_params={
        &quot;vertexai&quot;: True,
        &quot;project&quot;: &quot;your-project-id&quot;,
        &quot;location&quot;: &quot;global&quot;  # or regional endpoint
    }
)
```

## Adding Custom Model Providers

LangExtract supports custom LLM providers via a lightweight plugin system. You can add support for new models without changing core code.

- Add new model support independently of the core library
- Distribute your provider as a separate Python package
- Keep custom dependencies isolated
- Override or extend built-in providers via priority-based resolution

See the detailed guide in [Provider System Documentation](langextract/providers/README.md) to learn how to:

- Register a provider with `@registry.register(...)`
- Publish an entry point for discovery
- Optionally provide a schema with `get_schema_class()` for structured output
- Integrate with the factory via `create_model(...)`

## Using OpenAI Models

LangExtract supports OpenAI models (requires optional dependency: `pip install langextract[openai]`):

```python
import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gpt-4o&quot;,  # Automatically selects OpenAI provider
    api_key=os.environ.get(&#039;OPENAI_API_KEY&#039;),
    fence_output=True,
    use_schema_constraints=False
)
```

Note: OpenAI models require `fence_output=True` and `use_schema_constraints=False` because LangExtract doesn&#039;t implement schema constraints for OpenAI yet.

## Using Local LLMs with Ollama
LangExtract supports local inference using Ollama, allowing you to run models without API keys:

```python
import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gemma2:2b&quot;,  # Automatically selects Ollama provider
    model_url=&quot;http://localhost:11434&quot;,
    fence_output=False,
    use_schema_constraints=False
)
```

**Quick setup:** Install Ollama from [ollama.com](https://ollama.com/), run `ollama pull gemma2:2b`, then `ollama serve`.

For detailed installation, Docker setup, and examples, see [`examples/ollama/`](examples/ollama/).

## More Examples

Additional examples of LangExtract in action:

### *Romeo and Juliet* Full Text Extraction

LangExtract can process complete documents directly from URLs. This example demonstrates extraction from the full text of *Romeo and Juliet* from Project Gutenberg (147,843 characters), showing parallel processing, sequential extraction passes, and performance optimization for long document processing.

**[View *Romeo and Juliet* Full Text Example ‚Üí](https://github.com/google/langextract/blob/main/docs/examples/longer_text_example.md)**

### Medication Extraction

&gt; **Disclaimer:** This demonstration is for illustrative purposes of LangExtract&#039;s baseline capability only. It does not represent a finished or approved product, is not intended to diagnose or suggest treatment of any disease or condition, and should not be used for medical advice.

LangExtract excels at extracting structured medical information from clinical text. These examples demonstrate both basic entity recognition (medication names, dosages, routes) and relationship extraction (connecting medications to their attributes), showing LangExtract&#039;s effectiveness for healthcare applications.

**[View Medication Examples ‚Üí](https://github.com/google/langextract/blob/main/docs/examples/medication_examples.md)**

### Radiology Report Structuring: RadExtract

Explore RadExtract, a live interactive demo on HuggingFace Spaces that shows how LangExtract can automatically structure radiology reports. Try it directly in your browser with no setup required.

**[View RadExtract Demo ‚Üí](https://huggingface.co/spaces/google/radextract)**

## Community Providers

Extend LangExtract with custom model providers! Check out our [Community Provider Plugins](COMMUNITY_PROVIDERS.md) registry to discover providers created by the community or add your own.

For detailed instructions on creating a provider plugin, see the [Custom Provider Plugin Example](examples/custom_provider_plugin/).

## Contributing

Contributions are welcome! See [CONTRIBUTING.md](https://github.com/google/langextract/blob/main/CONTRIBUTING.md) to get started
with development, testing, and pull requests. You must sign a
[Contributor License Agreement](https://cla.developers.google.com/about)
before submitting patches.



## Testing

To run tests locally from the source:

```bash
# Clone the repository
git clone https://github.com/google/langextract.git
cd langextract

# Install with test dependencies
pip install -e &quot;.[test]&quot;

# Run all tests
pytest tests
```

Or reproduce the full CI matrix locally with tox:

```bash
tox  # runs pylint + pytest on Python 3.10 and 3.11
```

### Ollama Integration Testing

If you have Ollama installed locally, you can run integration tests:

```bash
# Test Ollama integration (requires Ollama running with gemma2:2b model)
tox -e ollama-integration
```

This test will automatically detect if Ollama is available and run real inference tests.

## Development

### Code Formatting

This project uses automated formatting tools to maintain consistent code style:

```bash
# Auto-format all code
./autoformat.sh

# Or run formatters separately
isort langextract tests --profile google --line-length 80
pyink langextract tests --config pyproject.toml
```

### Pre-commit Hooks

For automatic formatting checks:
```bash
pre-commit install  # One-time setup
pre-commit run --all-files  # Manual run
```

### Linting

Run linting before submitting PRs:

```bash
pylint --rcfile=.pylintrc langextract tests
```

See [CONTRIBUTING.md](CONTRIBUTING.md) for full development guidelines.

## Disclaimer

This is not an officially supported Google product. If you use
LangExtract in production or publications, please cite accordingly and
acknowledge usage. Use is subject to the [Apache 2.0 License](https://github.com/google/langextract/blob/main/LICENSE).
For health-related applications, use of LangExtract is also subject to the
[Health AI Developer Foundations Terms of Use](https://developers.google.com/health-ai-developer-foundations/terms).

---

**Happy Extracting!**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[fastapi/fastapi]]></title>
            <link>https://github.com/fastapi/fastapi</link>
            <guid>https://github.com/fastapi/fastapi</guid>
            <pubDate>Sun, 07 Dec 2025 00:04:55 GMT</pubDate>
            <description><![CDATA[FastAPI framework, high performance, easy to learn, fast to code, ready for production]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/fastapi/fastapi">fastapi/fastapi</a></h1>
            <p>FastAPI framework, high performance, easy to learn, fast to code, ready for production</p>
            <p>Language: Python</p>
            <p>Stars: 92,782</p>
            <p>Forks: 8,333</p>
            <p>Stars today: 69 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://fastapi.tiangolo.com&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/logo-margin/logo-teal.png&quot; alt=&quot;FastAPI&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;em&gt;FastAPI framework, high performance, easy to learn, fast to code, ready for production&lt;/em&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/fastapi/fastapi/actions?query=workflow%3ATest+event%3Apush+branch%3Amaster&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://github.com/fastapi/fastapi/actions/workflows/test.yml/badge.svg?event=push&amp;branch=master&quot; alt=&quot;Test&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://coverage-badge.samuelcolvin.workers.dev/redirect/fastapi/fastapi&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://coverage-badge.samuelcolvin.workers.dev/fastapi/fastapi.svg&quot; alt=&quot;Coverage&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://pypi.org/project/fastapi&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/fastapi?color=%2334D058&amp;label=pypi%20package&quot; alt=&quot;Package version&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://pypi.org/project/fastapi&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/pyversions/fastapi.svg?color=%2334D058&quot; alt=&quot;Supported Python versions&quot;&gt;
&lt;/a&gt;
&lt;/p&gt;

---

**Documentation**: &lt;a href=&quot;https://fastapi.tiangolo.com&quot; target=&quot;_blank&quot;&gt;https://fastapi.tiangolo.com&lt;/a&gt;

**Source Code**: &lt;a href=&quot;https://github.com/fastapi/fastapi&quot; target=&quot;_blank&quot;&gt;https://github.com/fastapi/fastapi&lt;/a&gt;

---

FastAPI is a modern, fast (high-performance), web framework for building APIs with Python based on standard Python type hints.

The key features are:

* **Fast**: Very high performance, on par with **NodeJS** and **Go** (thanks to Starlette and Pydantic). [One of the fastest Python frameworks available](#performance).
* **Fast to code**: Increase the speed to develop features by about 200% to 300%. *
* **Fewer bugs**: Reduce about 40% of human (developer) induced errors. *
* **Intuitive**: Great editor support. &lt;abbr title=&quot;also known as auto-complete, autocompletion, IntelliSense&quot;&gt;Completion&lt;/abbr&gt; everywhere. Less time debugging.
* **Easy**: Designed to be easy to use and learn. Less time reading docs.
* **Short**: Minimize code duplication. Multiple features from each parameter declaration. Fewer bugs.
* **Robust**: Get production-ready code. With automatic interactive documentation.
* **Standards-based**: Based on (and fully compatible with) the open standards for APIs: &lt;a href=&quot;https://github.com/OAI/OpenAPI-Specification&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;OpenAPI&lt;/a&gt; (previously known as Swagger) and &lt;a href=&quot;https://json-schema.org/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;JSON Schema&lt;/a&gt;.

&lt;small&gt;* estimation based on tests conducted by an internal development team, building production applications.&lt;/small&gt;

## Sponsors

&lt;!-- sponsors --&gt;
### Keystone Sponsor

&lt;a href=&quot;https://fastapicloud.com&quot; target=&quot;_blank&quot; title=&quot;FastAPI Cloud. By the same team behind FastAPI. You code. We Cloud.&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/fastapicloud.png&quot;&gt;&lt;/a&gt;

### Gold and Silver Sponsors

&lt;a href=&quot;https://blockbee.io?ref=fastapi&quot; target=&quot;_blank&quot; title=&quot;BlockBee Cryptocurrency Payment Gateway&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/blockbee.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/scalar/scalar/?utm_source=fastapi&amp;utm_medium=website&amp;utm_campaign=main-badge&quot; target=&quot;_blank&quot; title=&quot;Scalar: Beautiful Open-Source API References from Swagger/OpenAPI files&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/scalar.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.propelauth.com/?utm_source=fastapi&amp;utm_campaign=1223&amp;utm_medium=mainbadge&quot; target=&quot;_blank&quot; title=&quot;Auth, user management and more for your B2B product&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/propelauth.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://zuplo.link/fastapi-gh&quot; target=&quot;_blank&quot; title=&quot;Zuplo: Deploy, Secure, Document, and Monetize your FastAPI&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/zuplo.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://liblab.com?utm_source=fastapi&quot; target=&quot;_blank&quot; title=&quot;liblab - Generate SDKs from FastAPI&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/liblab.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://docs.render.com/deploy-fastapi?utm_source=deploydoc&amp;utm_medium=referral&amp;utm_campaign=fastapi&quot; target=&quot;_blank&quot; title=&quot;Deploy &amp; scale any full-stack web app on Render. Focus on building apps, not infra.&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/render.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.coderabbit.ai/?utm_source=fastapi&amp;utm_medium=badge&amp;utm_campaign=fastapi&quot; target=&quot;_blank&quot; title=&quot;Cut Code Review Time &amp; Bugs in Half with CodeRabbit&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/coderabbit.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://subtotal.com/?utm_source=fastapi&amp;utm_medium=sponsorship&amp;utm_campaign=open-source&quot; target=&quot;_blank&quot; title=&quot;The Gold Standard in Retail Account Linking&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/subtotal.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://docs.railway.com/guides/fastapi?utm_medium=integration&amp;utm_source=docs&amp;utm_campaign=fastapi&quot; target=&quot;_blank&quot; title=&quot;Deploy enterprise applications at startup speed&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/railway.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://serpapi.com/?utm_source=fastapi_website&quot; target=&quot;_blank&quot; title=&quot;SerpApi: Web Search API&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/serpapi.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.greptile.com/?utm_source=fastapi&amp;utm_medium=sponsorship&amp;utm_campaign=fastapi_sponsor_page&quot; target=&quot;_blank&quot; title=&quot;Greptile: The AI Code Reviewer&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/greptile.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://databento.com/?utm_source=fastapi&amp;utm_medium=sponsor&amp;utm_content=display&quot; target=&quot;_blank&quot; title=&quot;Pay as you go for market data&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/databento.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://speakeasy.com/editor?utm_source=fastapi+repo&amp;utm_medium=github+sponsorship&quot; target=&quot;_blank&quot; title=&quot;SDKs for your API | Speakeasy&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/speakeasy.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.svix.com/&quot; target=&quot;_blank&quot; title=&quot;Svix - Webhooks as a service&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/svix.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.stainlessapi.com/?utm_source=fastapi&amp;utm_medium=referral&quot; target=&quot;_blank&quot; title=&quot;Stainless | Generate best-in-class SDKs&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/stainless.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.permit.io/blog/implement-authorization-in-fastapi?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=fastapi&quot; target=&quot;_blank&quot; title=&quot;Fine-Grained Authorization for FastAPI&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/permit.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.interviewpal.com/?utm_source=fastapi&amp;utm_medium=open-source&amp;utm_campaign=dev-hiring&quot; target=&quot;_blank&quot; title=&quot;InterviewPal - AI Interview Coach for Engineers and Devs&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/interviewpal.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://dribia.com/en/&quot; target=&quot;_blank&quot; title=&quot;Dribia - Data Science within your reach&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/dribia.png&quot;&gt;&lt;/a&gt;

&lt;!-- /sponsors --&gt;

&lt;a href=&quot;https://fastapi.tiangolo.com/fastapi-people/#sponsors&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Other sponsors&lt;/a&gt;

## Opinions

&quot;_[...] I&#039;m using **FastAPI** a ton these days. [...] I&#039;m actually planning to use it for all of my team&#039;s **ML services at Microsoft**. Some of them are getting integrated into the core **Windows** product and some **Office** products._&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Kabir Khan - &lt;strong&gt;Microsoft&lt;/strong&gt; &lt;a href=&quot;https://github.com/fastapi/fastapi/pull/26&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_We adopted the **FastAPI** library to spawn a **REST** server that can be queried to obtain **predictions**. [for Ludwig]_&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Piero Molino, Yaroslav Dudin, and Sai Sumanth Miryala - &lt;strong&gt;Uber&lt;/strong&gt; &lt;a href=&quot;https://eng.uber.com/ludwig-v0-2/&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_**Netflix** is pleased to announce the open-source release of our **crisis management** orchestration framework: **Dispatch**! [built with **FastAPI**]_&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Kevin Glisson, Marc Vilanova, Forest Monsen - &lt;strong&gt;Netflix&lt;/strong&gt; &lt;a href=&quot;https://netflixtechblog.com/introducing-dispatch-da4b8a2a8072&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_I‚Äôm over the moon excited about **FastAPI**. It‚Äôs so fun!_&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Brian Okken - &lt;strong&gt;&lt;a href=&quot;https://pythonbytes.fm/episodes/show/123/time-to-right-the-py-wrongs?time_in_sec=855&quot; target=&quot;_blank&quot;&gt;Python Bytes&lt;/a&gt; podcast host&lt;/strong&gt; &lt;a href=&quot;https://x.com/brianokken/status/1112220079972728832&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_Honestly, what you&#039;ve built looks super solid and polished. In many ways, it&#039;s what I wanted **Hug** to be - it&#039;s really inspiring to see someone build that._&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Timothy Crosley - &lt;strong&gt;&lt;a href=&quot;https://github.com/hugapi/hug&quot; target=&quot;_blank&quot;&gt;Hug&lt;/a&gt; creator&lt;/strong&gt; &lt;a href=&quot;https://news.ycombinator.com/item?id=19455465&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_If you&#039;re looking to learn one **modern framework** for building REST APIs, check out **FastAPI** [...] It&#039;s fast, easy to use and easy to learn [...]_&quot;

&quot;_We&#039;ve switched over to **FastAPI** for our **APIs** [...] I think you&#039;ll like it [...]_&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Ines Montani - Matthew Honnibal - &lt;strong&gt;&lt;a href=&quot;https://explosion.ai&quot; target=&quot;_blank&quot;&gt;Explosion AI&lt;/a&gt; founders - &lt;a href=&quot;https://spacy.io&quot; target=&quot;_blank&quot;&gt;spaCy&lt;/a&gt; creators&lt;/strong&gt; &lt;a href=&quot;https://x.com/_inesmontani/status/1144173225322143744&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt; - &lt;a href=&quot;https://x.com/honnibal/status/1144031421859655680&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_If anyone is looking to build a production Python API, I would highly recommend **FastAPI**. It is **beautifully designed**, **simple to use** and **highly scalable**, it has become a **key component** in our API first development strategy and is driving many automations and services such as our Virtual TAC Engineer._&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Deon Pillsbury - &lt;strong&gt;Cisco&lt;/strong&gt; &lt;a href=&quot;https://www.linkedin.com/posts/deonpillsbury_cisco-cx-python-activity-6963242628536487936-trAp/&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

## **Typer**, the FastAPI of CLIs

&lt;a href=&quot;https://typer.tiangolo.com&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://typer.tiangolo.com/img/logo-margin/logo-margin-vector.svg&quot; style=&quot;width: 20%;&quot;&gt;&lt;/a&gt;

If you are building a &lt;abbr title=&quot;Command Line Interface&quot;&gt;CLI&lt;/abbr&gt; app to be used in the terminal instead of a web API, check out &lt;a href=&quot;https://typer.tiangolo.com/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;**Typer**&lt;/a&gt;.

**Typer** is FastAPI&#039;s little sibling. And it&#039;s intended to be the **FastAPI of CLIs**. ‚å®Ô∏è üöÄ

## Requirements

FastAPI stands on the shoulders of giants:

* &lt;a href=&quot;https://www.starlette.dev/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Starlette&lt;/a&gt; for the web parts.
* &lt;a href=&quot;https://docs.pydantic.dev/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Pydantic&lt;/a&gt; for the data parts.

## Installation

Create and activate a &lt;a href=&quot;https://fastapi.tiangolo.com/virtual-environments/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;virtual environment&lt;/a&gt; and then install FastAPI:

&lt;div class=&quot;termy&quot;&gt;

```console
$ pip install &quot;fastapi[standard]&quot;

---&gt; 100%
```

&lt;/div&gt;

**Note**: Make sure you put `&quot;fastapi[standard]&quot;` in quotes to ensure it works in all terminals.

## Example

### Create it

Create a file `main.py` with:

```Python
from typing import Union

from fastapi import FastAPI

app = FastAPI()


@app.get(&quot;/&quot;)
def read_root():
    return {&quot;Hello&quot;: &quot;World&quot;}


@app.get(&quot;/items/{item_id}&quot;)
def read_item(item_id: int, q: Union[str, None] = None):
    return {&quot;item_id&quot;: item_id, &quot;q&quot;: q}
```

&lt;details markdown=&quot;1&quot;&gt;
&lt;summary&gt;Or use &lt;code&gt;async def&lt;/code&gt;...&lt;/summary&gt;

If your code uses `async` / `await`, use `async def`:

```Python hl_lines=&quot;9  14&quot;
from typing import Union

from fastapi import FastAPI

app = FastAPI()


@app.get(&quot;/&quot;)
async def read_root():
    return {&quot;Hello&quot;: &quot;World&quot;}


@app.get(&quot;/items/{item_id}&quot;)
async def read_item(item_id: int, q: Union[str, None] = None):
    return {&quot;item_id&quot;: item_id, &quot;q&quot;: q}
```

**Note**:

If you don&#039;t know, check the _&quot;In a hurry?&quot;_ section about &lt;a href=&quot;https://fastapi.tiangolo.com/async/#in-a-hurry&quot; target=&quot;_blank&quot;&gt;`async` and `await` in the docs&lt;/a&gt;.

&lt;/details&gt;

### Run it

Run the server with:

&lt;div class=&quot;termy&quot;&gt;

```console
$ fastapi dev main.py

 ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FastAPI CLI - Development mode ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
 ‚îÇ                                                     ‚îÇ
 ‚îÇ  Serving at: http://127.0.0.1:8000                  ‚îÇ
 ‚îÇ                                                     ‚îÇ
 ‚îÇ  API docs: http://127.0.0.1:8000/docs               ‚îÇ
 ‚îÇ                                                     ‚îÇ
 ‚îÇ  Running in development mode, for production use:   ‚îÇ
 ‚îÇ                                                     ‚îÇ
 ‚îÇ  fastapi run                                        ‚îÇ
 ‚îÇ                                                     ‚îÇ
 ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ

INFO:     Will watch for changes in these directories: [&#039;/home/user/code/awesomeapp&#039;]
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [2248755] using WatchFiles
INFO:     Started server process [2248757]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
```

&lt;/div&gt;

&lt;details markdown=&quot;1&quot;&gt;
&lt;summary&gt;About the command &lt;code&gt;fastapi dev main.py&lt;/code&gt;...&lt;/summary&gt;

The command `fastapi dev` reads your `main.py` file, detects the **FastAPI** app in it, and starts a server using &lt;a href=&quot;https://www.uvicorn.dev&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Uvicorn&lt;/a&gt;.

By default, `fastapi dev` will start with auto-reload enabled for local development.

You can read more about it in the &lt;a href=&quot;https://fastapi.tiangolo.com/fastapi-cli/&quot; target=&quot;_blank&quot;&gt;FastAPI CLI docs&lt;/a&gt;.

&lt;/details&gt;

### Check it

Open your browser at &lt;a href=&quot;http://127.0.0.1:8000/items/5?q=somequery&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/items/5?q=somequery&lt;/a&gt;.

You will see the JSON response as:

```JSON
{&quot;item_id&quot;: 5, &quot;q&quot;: &quot;somequery&quot;}
```

You already created an API that:

* Receives HTTP requests in the _paths_ `/` and `/items/{item_id}`.
* Both _paths_ take `GET` &lt;em&gt;operations&lt;/em&gt; (also known as HTTP _methods_).
* The _path_ `/items/{item_id}` has a _path parameter_ `item_id` that should be an `int`.
* The _path_ `/items/{item_id}` has an optional `str` _query parameter_ `q`.

### Interactive API docs

Now go to &lt;a href=&quot;http://127.0.0.1:8000/docs&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/docs&lt;/a&gt;.

You will see the automatic interactive API documentation (provided by &lt;a href=&quot;https://github.com/swagger-api/swagger-ui&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Swagger UI&lt;/a&gt;):

![Swagger UI](https://fastapi.tiangolo.com/img/index/index-01-swagger-ui-simple.png)

### Alternative API docs

And now, go to &lt;a href=&quot;http://127.0.0.1:8000/redoc&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/redoc&lt;/a&gt;.

You will see the alternative automatic documentation (provided by &lt;a href=&quot;https://github.com/Rebilly/ReDoc&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;ReDoc&lt;/a&gt;):

![ReDoc](https://fastapi.tiangolo.com/img/index/index-02-redoc-simple.png)

## Example upgrade

Now modify the file `main.py` to receive a body from a `PUT` request.

Declare the body using standard Python types, thanks to Pydantic.

```Python hl_lines=&quot;4  9-12  25-27&quot;
from typing import Union

from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()


class Item(BaseModel):
    name: str
    price: float
    is_offer: Union[bool, None] = None


@app.get(&quot;/&quot;)
def read_root():
    return {&quot;Hello&quot;: &quot;World&quot;}


@app.get(&quot;/items/{item_id}&quot;)
def read_item(item_id: int, q: Union[str, None] = None):
    return {&quot;item_id&quot;: item_id, &quot;q&quot;: q}


@app.put(&quot;/items/{item_id}&quot;)
def update_item(item_id: int, item: Item):
    return {&quot;item_name&quot;: item.name, &quot;item_id&quot;: item_id}
```

The `fastapi dev` server should reload automatically.

### Interactive API docs upgrade

Now go to &lt;a href=&quot;http://127.0.0.1:8000/docs&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/docs&lt;/a&gt;.

* The interactive API documentation will be automatically updated, including the new body:

![Swagger UI](https://fastapi.tiangolo.com/img/index/index-03-swagger-02.png)

* Click on the button &quot;Try it out&quot;, it allows you to fill the parameters and directly interact with the API:

![Swagger UI interaction](https://fastapi.tiangolo.com/img/index/index-04-swagger-03.png)

* Then click on the &quot;Execute&quot; button, the user interface will communicate with your API, send the parameters, get the results and show them on the screen:

![Swagger UI interaction](https://fastapi.tiangolo.com/img/index/index-05-swagger-04.png)

### Alternative API docs upgrade

And now, go to &lt;a href=&quot;http://127.0.0.1:8000/redoc&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/redoc&lt;/a&gt;.

* The alternative documentation will also reflect the new query parameter and body:

![ReDoc](https://fastapi.tiangolo.com/img/index/index-06-redoc-02.png)

### Recap

In summary, you declare **once** the types of parameters, body, etc. as function parameters.

You do that with standard modern Python types.

You don&#039;t have to learn a new syntax, the methods or classes of a specific library, etc.

Just standard **Python**.

For example, for an `int`:

```Python
item_id: int
```

or for a more complex `Item` model:

```Python
item: Item
```

...and with that single declaration you get:

* Editor support, including:
    * Completion.
    * Type checks.
* Validation of data:
    * Automatic and clear errors when the data is invalid.
    * Validation even for deeply nested JSON objects.
* &lt;abbr title=&quot;also known as: serialization, parsing, marshalling&quot;&gt;Conversion&lt;/abbr&gt; of input data: coming from the network to Python data and types. Reading from:
    * JSON.
    * Path parameters.
    * Query parameters.
    * Cookies.
    * Headers.
    * Forms.
    * Files.
* &lt;abbr title=&quot;also known as: serialization, parsing, marshalling&quot;&gt;Conversion&lt;/abbr&gt; of output data: converting from Python data and types to network data (as JSON):
    * Convert Python types (`str`, `int`, `float`, `bool`, `list`, etc).
    * `datetime` objects.
    * `UUID` objects.
    * Database models.
    * ...and many more.
* Automatic interactive API documentation, including 2 alternative user interfaces:
    * Swagger UI.
    * ReDoc.

---

Coming back to the previous code example, **FastAPI** will:

* Validate that there is an `item_id` in the path for `GET` and `PUT` requests.
* Validate that the `item_id` is of type `int` for `GET` and `PUT` requests.
    * If it is not, the client will see a useful, clear error.
* Check if there is an optional query parameter named `q` (as in `http://127.0.0.1:8000/items/foo?q=somequery`) for `GET` requests.
    * As the `q` parameter is declared with `= None`, it is optional.
    * Without the `None` it would be required (as is the body in the case with `PUT`).
* For `PUT` requests to `/items/{item_id}`, read the body as JSON:
    * Check that it has a required attribute `name` that should be a `str`.
    * Check that it has a required attribute `price` that has to be a `float`.
    * Check that it has an optional attribute `is_offer`, that should be a `bool`, if present.
    * All this would also work for deeply nested JSON objects.
* Convert from and to JSON automatically.
* Document everything with OpenAPI, that can be used by:
    * Interactive documentation systems.
    * Automatic client code generation systems, for many languages.
* Provide 2 interactive documentation web interfaces 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hesreallyhim/awesome-claude-code]]></title>
            <link>https://github.com/hesreallyhim/awesome-claude-code</link>
            <guid>https://github.com/hesreallyhim/awesome-claude-code</guid>
            <pubDate>Sun, 07 Dec 2025 00:04:54 GMT</pubDate>
            <description><![CDATA[A curated list of awesome commands, files, and workflows for Claude Code]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hesreallyhim/awesome-claude-code">hesreallyhim/awesome-claude-code</a></h1>
            <p>A curated list of awesome commands, files, and workflows for Claude Code</p>
            <p>Language: Python</p>
            <p>Stars: 17,697</p>
            <p>Forks: 999</p>
            <p>Stars today: 41 stars today</p>
            <h2>README</h2><pre>&lt;!--lint disable remark-lint:awesome-badge--&gt;

&lt;!-- Responsive Logo with Theme Support --&gt;
&lt;div align=&quot;center&quot;&gt;
  
  &lt;!-- Same ASCII art for all screen sizes, just scales down on mobile --&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/logo-dark.svg&quot;&gt;
    &lt;img src=&quot;assets/logo-light.svg&quot; alt=&quot;Awesome Claude Code&quot; width=&quot;100%&quot; style=&quot;max-width: 900px;&quot;&gt;
  &lt;/picture&gt;
  
&lt;/div&gt;


&lt;!-- Generated with https://github.com/denvercoder1/readme-typing-svg --&gt;

[![Typing SVG](https://readme-typing-svg.demolab.com/?font=Fira+Code&amp;weight=600&amp;duration=3000&amp;pause=100&amp;color=F7080D&amp;width=680&amp;lines=Lollygagging...;Skedaddling...;Bumbershooting...;Widdershinning...;Higgledy-piggledying...;Doodlebugging...;Fiddle-faddling...;Whimwhamming...;Dilly-dallying...;Flapdoodling...;Ballyhooing...;Galumphing...;Razzle-dazzling...;Tiddle-taddling...;Zigzagging...;Twinkletoeing...;Puddle-jumping...;Snicker-snacking...;Jibber-jabbering...;Frabjoussing...;Piffle-puffling...;Whirligigging...;Bibbity-bobbitying...;)](https://git.io/typing-svg)

&lt;!--lint enable remark-lint:awesome-badge--&gt;

[![Awesome](https://awesome.re/badge-flat2.svg)](https://awesome.re) [![FREEDOM FUNDER](/assets/freedom-funder-badge.svg)](https://bailproject.org)


# Awesome Claude Code

&lt;!--lint enable remark-lint:awesome-badge--&gt;

&lt;!--lint disable double-link--&gt;

This is a curated list of slash-commands, `CLAUDE.md` files, CLI tools, and other resources and guides for enhancing your [Claude Code](https://docs.anthropic.com/en/docs/claude-code) workflow, productivity, and vibes.

&lt;!--lint enable double-link--&gt;

Claude Code is a cutting-edge CLI-based coding assistant and agent released by [Anthropic](https://www.anthropic.com/) that you can access in your terminal or IDE. It is a rapidly evolving tool that offers a number of powerful capabilities, and allows for a lot of configuration, in a lot of different ways. Users are actively working out best practices and workflows. It is the hope that this repo will help the community share knowledge and understand how to get the most out of Claude Code.

### Announcements [üîù](#awesome-claude-code)

&lt;details open&gt;
&lt;summary&gt;View Announcements&lt;/summary&gt;

- &lt;details open&gt;
  &lt;summary&gt;2025-11-21 - Claude Code for Web - Breaking the Internet&lt;/summary&gt;

  - I don&#039;t know about you folks, but I&#039;ve been having a romping good time playing around with Claude Code for Web. I&#039;m thinking about adding a category for it, but I&#039;m not sure how many people have access, or if people are interested in that. It&#039;s really pretty awesome (I hired it as a sub-contractor to see if it could spice up the repo a little bit, so it&#039;s been running for like 30 hours, I&#039;m not sure what it&#039;s up to... üëÄ) Anyway, if you&#039;re working with Claude Code on the Web, or you&#039;d like to see it included here, please sound off in the [Discussion thread](https://github.com/hesreallyhim/awesome-claude-code/discussions/308).

  - This was me last week, talking about Output Styles: &quot;I&#039;m going to guarantee that by one week from now, we will have a minimum of **5 Output Styles** in that category.&quot; Well, since then, nobody submitted an Output Style - which makes me almost certain that nobody is reading these announcements, since it was kind of open invitation for anyone to get on the list. _Nevertheless_, I&#039;m a Him of my words, so I went and put together a few of my own, which you can check out below. It&#039;s pretty exciting to be included here, because I&#039;ve been rejected four or five times, which is _really_ embarrassing because it&#039;s my repo...

  &lt;/details&gt;

&lt;/details&gt;

## This Week&#039;s Additions ‚ú® [üîù](#awesome-claude-code)

&gt; Resources added in the past 7 days

[`Claude Code Output Styles - Debugging`](https://github.com/JamieM0/claude-output-styles) &amp;nbsp; by &amp;nbsp; [Jamie Matthews](https://github.com/JamieM0)  &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT  
A small set of well-written output styles, specifically focused on debugging - root cause analysis, systematic, methodical debugging, encouraging a more careful approach to bug-squashing from Claude Code.

&lt;details&gt;
&lt;summary&gt;üìä GitHub Stats&lt;/summary&gt;

![GitHub Stats for claude-output-styles](https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-output-styles&amp;username=JamieM0&amp;all_stats=true&amp;stats_only=true)

&lt;/details&gt;
&lt;br&gt;

[`CCometixLine - Claude Code Statusline`](https://github.com/Haleclipse/CCometixLine) &amp;nbsp; by &amp;nbsp; [Haleclipse](https://github.com/Haleclipse)  &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT  
A high-performance Claude Code statusline tool written in Rust with Git integration, usage tracking, interactive TUI configuration, and Claude Code enhancement utilities.

&lt;details&gt;
&lt;summary&gt;üìä GitHub Stats&lt;/summary&gt;

![GitHub Stats for CCometixLine](https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=CCometixLine&amp;username=Haleclipse&amp;all_stats=true&amp;stats_only=true)

&lt;/details&gt;
&lt;br&gt;

[`Claude Code Handbook`](https://nikiforovall.blog/claude-code-rules/) &amp;nbsp; by &amp;nbsp; [nikiforovall](https://github.com/nikiforovall)  &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT  
Collection of best practices, tips, and techniques for Claude Code development workflows, enhanced with distributable plugins

[`Context Engineering Kit`](https://github.com/NeoLabHQ/context-engineering-kit) &amp;nbsp; by &amp;nbsp; [Vlad Goncharov](https://github.com/LeoVS09)  &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;GPL-3.0  
Hand-crafted collection of advanced context engineering techniques and patterns with minimal token footprint focused on improving agent result quality.

&lt;details&gt;
&lt;summary&gt;üìä GitHub Stats&lt;/summary&gt;

![GitHub Stats for context-engineering-kit](https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=context-engineering-kit&amp;username=NeoLabHQ&amp;all_stats=true&amp;stats_only=true)

&lt;/details&gt;
&lt;br&gt;

[`Claudix - Claude Code for VSCode`](https://github.com/Haleclipse/Claudix) &amp;nbsp; by &amp;nbsp; [Haleclipse](https://github.com/Haleclipse)  &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;AGPL-3.0  
A VSCode extension that brings Claude Code directly into your editor with interactive chat interface, session management, intelligent file operations, terminal execution, and real-time streaming responses. Built with Vue 3, TypeScript.

&lt;details&gt;
&lt;summary&gt;üìä GitHub Stats&lt;/summary&gt;

![GitHub Stats for Claudix](https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=Claudix&amp;username=Haleclipse&amp;all_stats=true&amp;stats_only=true)

&lt;/details&gt;
&lt;br&gt;

[`claude-starter-kit`](https://github.com/serpro69/claude-starter-kit) &amp;nbsp; by &amp;nbsp; [serpro69](https://github.com/serpro69)  &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT  
This is a starter template repository designed to provide a complete development environment for Claude-Code with pre-configured MCP servers and tools for AI-powered development workflows. The repository is intentionally minimal, containing only configuration templates for three primary systems: Claude Code, Serena, and Task Master.

&lt;details&gt;
&lt;summary&gt;üìä GitHub Stats&lt;/summary&gt;

![GitHub Stats for claude-starter-kit](https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-starter-kit&amp;username=serpro69&amp;all_stats=true&amp;stats_only=true)

&lt;/details&gt;
&lt;br&gt;

[`viwo-cli`](https://github.com/OverseedAI/viwo) &amp;nbsp; by &amp;nbsp; [Hal Shin](https://github.com/hal-shin)  &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT  
Run Claude Code in a Docker container with git worktrees as volume mounts to enable safer usage of `--dangerously-skip-permissions` for frictionless one-shotting prompts. Allows users to spin up multiple instances of Claude Code in the background easily with reduced permission fatigue.

&lt;details&gt;
&lt;summary&gt;üìä GitHub Stats&lt;/summary&gt;

![GitHub Stats for viwo](https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=viwo&amp;username=OverseedAI&amp;all_stats=true&amp;stats_only=true)

&lt;/details&gt;
&lt;br&gt;

[`ccflare -&gt; **better-ccflare**`](https://github.com/tombii/better-ccflare/) &amp;nbsp; by &amp;nbsp; [tombii](https://github.com/tombii)  &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT  
A well-maintained and feature-enhanced fork of the glorious `ccflare` usage dashboard by @snipeship (which at the time of writing has not had an update in a few months). `better-ccflare` builds on this foundation with some performance enhancements, extended provider support, bug fixes, Docker deployment, and more.

&lt;details&gt;
&lt;summary&gt;üìä GitHub Stats&lt;/summary&gt;

![GitHub Stats for better-ccflare](https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=better-ccflare&amp;username=tombii&amp;all_stats=true&amp;stats_only=true)

&lt;/details&gt;
&lt;br&gt;

[`Britfix`](https://github.com/Talieisin/britfix) &amp;nbsp; by &amp;nbsp; [Talieisin](https://github.com/Talieisin)  &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT  
Claude outputs American spellings by default, which can have an impact on: professional credibility, compliance, documentation, and more. Britfix converts to British English, with a Claude Code hook for automatic conversion as files are written. Context-aware: handles code files intelligently by only converting comments and docstrings, never identifiers or string literals.

&lt;details&gt;
&lt;summary&gt;üìä GitHub Stats&lt;/summary&gt;

![GitHub Stats for britfix](https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=britfix&amp;username=Talieisin&amp;all_stats=true&amp;stats_only=true)

&lt;/details&gt;
&lt;br&gt;

[`recall`](https://github.com/zippoxer/recall) &amp;nbsp; by &amp;nbsp; [zippoxer](https://github.com/zippoxer)  &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT  
Full-text search your Claude Code sessions. Run `recall` in terminal, type to search, Enter to resume. Alternative to `claude --resume`.

&lt;details&gt;
&lt;summary&gt;üìä GitHub Stats&lt;/summary&gt;

![GitHub Stats for recall](https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=recall&amp;username=zippoxer&amp;all_stats=true&amp;stats_only=true)

&lt;/details&gt;
&lt;br&gt;

[`learn-faster-kit`](https://github.com/cheukyin175/learn-faster-kit) &amp;nbsp; by &amp;nbsp; [Hugo Lau](https://github.com/cheukyin175)  &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT  
A creative educational framework for Claude Code, inspired by the &quot;FASTER&quot; approach to self-teaching. Ships with a variety of agents, slash commands, and tools that enable Claude Code to help you progress at your own pace, employing well-established pedagogical techniques like active learning and spaced repetition.

&lt;details&gt;
&lt;summary&gt;üìä GitHub Stats&lt;/summary&gt;

![GitHub Stats for learn-faster-kit](https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=learn-faster-kit&amp;username=cheukyin175&amp;all_stats=true&amp;stats_only=true)

&lt;/details&gt;
&lt;br&gt;



## Contents [üîù](#awesome-claude-code)

&lt;details open&gt;
&lt;summary&gt;Table of Contents&lt;/summary&gt;

- &lt;details open&gt;
  &lt;summary&gt;&lt;a href=&quot;#agent-skills-&quot;&gt;Agent Skills&lt;/a&gt;&lt;/summary&gt;

  - [General](#general-)

  &lt;/details&gt;

- &lt;details open&gt;
  &lt;summary&gt;&lt;a href=&quot;#workflows--knowledge-guides-&quot;&gt;Workflows &amp; Knowledge Guides&lt;/a&gt;&lt;/summary&gt;

  - [General](#general--1)

  &lt;/details&gt;

- &lt;details open&gt;
  &lt;summary&gt;&lt;a href=&quot;#tooling-&quot;&gt;Tooling&lt;/a&gt;&lt;/summary&gt;

  - [General](#general--2)
  - [IDE Integrations](#ide-integrations-)
  - [Usage Monitors](#usage-monitors-)
  - [Orchestrators](#orchestrators-)

  &lt;/details&gt;

- &lt;details open&gt;
  &lt;summary&gt;&lt;a href=&quot;#status-lines-&quot;&gt;Status Lines&lt;/a&gt;&lt;/summary&gt;

  - [General](#general--3)

  &lt;/details&gt;

- &lt;details open&gt;
  &lt;summary&gt;&lt;a href=&quot;#hooks-&quot;&gt;Hooks&lt;/a&gt;&lt;/summary&gt;

  - [General](#general--4)

  &lt;/details&gt;

- &lt;details open&gt;
  &lt;summary&gt;&lt;a href=&quot;#output-styles-&quot;&gt;Output Styles&lt;/a&gt;&lt;/summary&gt;

  - [General](#general--5)

  &lt;/details&gt;

- &lt;details open&gt;
  &lt;summary&gt;&lt;a href=&quot;#slash-commands-&quot;&gt;Slash-Commands&lt;/a&gt;&lt;/summary&gt;

  - [General](#general--6)
  - [Version Control &amp; Git](#version-control--git-)
  - [Code Analysis &amp; Testing](#code-analysis--testing-)
  - [Context Loading &amp; Priming](#context-loading--priming-)
  - [Documentation &amp; Changelogs](#documentation--changelogs-)
  - [CI / Deployment](#ci--deployment-)
  - [Project &amp; Task Management](#project--task-management-)
  - [Miscellaneous](#miscellaneous-)

  &lt;/details&gt;

- &lt;details open&gt;
  &lt;summary&gt;&lt;a href=&quot;#claudemd-files-&quot;&gt;CLAUDE.md Files&lt;/a&gt;&lt;/summary&gt;

  - [Language-Specific](#language-specific-)
  - [Domain-Specific](#domain-specific-)
  - [Project Scaffolding &amp; MCP](#project-scaffolding--mcp-)

  &lt;/details&gt;

- &lt;details open&gt;
  &lt;summary&gt;&lt;a href=&quot;#alternative-clients-&quot;&gt;Alternative Clients&lt;/a&gt;&lt;/summary&gt;

  - [General](#general--7)

  &lt;/details&gt;

- &lt;details open&gt;
  &lt;summary&gt;&lt;a href=&quot;#official-documentation-&quot;&gt;Official Documentation&lt;/a&gt;&lt;/summary&gt;

  - [General](#general--8)

  &lt;/details&gt;

&lt;/details&gt;

&lt;br&gt;

## Agent Skills ü§ñ [üîù](#awesome-claude-code)

&gt; **Agent skills** are model-controlled configurations (files, scripts, resources, etc.) that enable Claude Code to perform specialized tasks requiring specific knowledge or capabilities.

&lt;details open&gt;
&lt;summary&gt;&lt;h3&gt;General &lt;a href=&quot;#awesome-claude-code&quot;&gt;üîù&lt;/a&gt;&lt;/h3&gt;&lt;/summary&gt;

[`Codex Skill`](https://github.com/skills-directory/skill-codex) &amp;nbsp; by &amp;nbsp; [klaudworks](https://github.com/klaudworks)    
Enables users to prompt codex from claude code. Unlike the raw codex mcp server, this skill infers parameters such as model, reasoning effort, sandboxing from your prompt or asks you to specify them. It also simplifies continuing prior codex sessions so that codex can continue with the prior context.

&lt;details&gt;
&lt;summary&gt;üìä GitHub Stats&lt;/summary&gt;

![GitHub Stats for skill-codex](https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=skill-codex&amp;username=skills-directory&amp;all_stats=true&amp;stats_only=true)

&lt;/details&gt;
&lt;br&gt;

[`Context Engineering Kit`](https://github.com/NeoLabHQ/context-engineering-kit) &amp;nbsp; by &amp;nbsp; [Vlad Goncharov](https://github.com/LeoVS09)  &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;GPL-3.0  
Hand-crafted collection of advanced context engineering techniques and patterns with minimal token footprint focused on improving agent result quality.

&lt;details&gt;
&lt;summary&gt;üìä GitHub Stats&lt;/summary&gt;

![GitHub Stats for context-engineering-kit](https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=context-engineering-kit&amp;username=NeoLabHQ&amp;all_stats=true&amp;stats_only=true)

&lt;/details&gt;
&lt;br&gt;

[`Web Assets Generator Skill`](https://github.com/alonw0/web-asset-generator) &amp;nbsp; by &amp;nbsp; [Alon Wolenitz](https://github.com/alonw0)  &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT  
Easily generate web assets from Claude Code including favicons, app icons (PWA), and social media meta images (Open Graph) for Facebook, Twitter, WhatsApp, and LinkedIn. Handles image resizing, text-to-image generation, emojis, and provides proper HTML meta tags.

&lt;details&gt;
&lt;summary&gt;üìä GitHub Stats&lt;/summary&gt;

![GitHub Stats for web-asset-generator](https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=web-asset-generator&amp;username=alonw0&amp;all_stats=true&amp;stats_only=true)

&lt;/details&gt;
&lt;br&gt;

&lt;/details&gt;

&lt;br&gt;

## Workflows &amp; Knowledge Guides üß† [üîù](#awesome-claude-code)

&gt; A **workflow** is a tightly coupled set of Claude Code-native resources that facilitate specific projects

&lt;details open&gt;
&lt;summary&gt;&lt;h3&gt;General &lt;a href=&quot;#awesome-claude-code&quot;&gt;üîù&lt;/a&gt;&lt;/h3&gt;&lt;/summary&gt;

[`AB Method`](https://github.com/ayoubben18/ab-method) &amp;nbsp; by &amp;nbsp; [Ayoub Bensalah](https://github.com/ayoubben18)  &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT  
A principled, spec-driven workflow that transforms large problems into focused, incremental missions using Claude Code&#039;s specialized sub agents. Includes slash-commands, sub agents, and specialized workflows designed for specific parts of the SDLC.

&lt;details&gt;
&lt;summary&gt;üìä GitHub Stats&lt;/summary&gt;

![GitHub Stats for ab-method](https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=ab-method&amp;username=ayoubben18&amp;all_stats=true&amp;stats_only=true)

&lt;/details&gt;
&lt;br&gt;

[`Agentic Workflow Patterns`](https://github.com/ThibautMelen/agentic-workflow-patterns) &amp;nbsp; by &amp;nbsp; [ThibautMelen](https://github.com/ThibautMelen)  &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT  
A comprehensive and well-documented collection of agentic patterns from Anthropic docs, with colorful Mermaid diagrams and code examples for each pattern. Covers Subagent Orchestration, Progressive Skills, Parallel Tool Calling, Master-Clone Architecture, Wizard Workflows, and more. Also compatible with other providers.

&lt;details&gt;
&lt;summary&gt;üìä GitHub Stats&lt;/summary&gt;

![GitHub Stats for agentic-workflow-patterns](https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=agentic-workflow-patterns&amp;username=ThibautMelen&amp;all_stats=true&amp;stats_only=true)

&lt;/details&gt;
&lt;br&gt;

[`Blogging Platform Instructions`](https://github.com/cloudartisan/cloudartisan.github.io/tree/main/.claude/commands) &amp;nbsp; by &amp;nbsp; [cloudartisan](https://github.com/cloudartisan)  &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;CC-BY-SA-4.0  
Provides a well-structured set of commands for publishing and maintaining a blogging platform, including commands for creating posts, managing categories, and handling media files.

&lt;details&gt;
&lt;summary&gt;üìä GitHub Stats&lt;/summary&gt;

![GitHub Stats for cloudartisan.github.io](https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=cloudartisan.github.io&amp;username=cloudartisan&amp;all_stats=true&amp;stats_only=true)

&lt;/details&gt;
&lt;br&gt;

[`Claude Code Documentation Mirror`](https://github.com/ericbuess/claude-code-docs) &amp;nbsp; by &amp;nbsp; [Eric Buess](https://github.com/ericbuess)  &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;NOASSERTION  
A mirror of the Anthropic &amp;copy; PBC documentation pages for Claude Code, updated every few hours. Can come in handy when trying to stay on top of the ever-expanding feature-set of Dr. Claw D. Code, Ph.D.

&lt;details&gt;
&lt;summary&gt;üìä GitHub Stats&lt;/summary&gt;

![GitHub Stats for claude-code-docs](https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-code-docs&amp;username=ericbuess&amp;all_stats=true&amp;stats_only=true)

&lt;/details&gt;
&lt;br&gt;

[`Claude Code Handbook`](https://nikiforovall.blog/claude-code-rules/) &amp;nbsp; by &amp;nbsp; [nikiforovall](https://github.com/nikiforovall)  &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT  
Collection of best practices, tips, and techniques for Claude Code development workflows, enhanced with distributable plugins

[`Claude Code Infrastructure Showcase`](https://github.com/diet103/claude-code-infrastructure-showcase) &amp;nbsp; by &amp;nbsp; [diet103](https://github.com/diet103)  &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT  
A remarkably innovative approach to working with Skills, the centerpiece of which being a technique that leverages hooks to ensure that Claude intelligently selects and activates the appropriate Skill given the current context. Well-documented and adaptable to different projects and workflows.

&lt;details&gt;
&lt;summary&gt;üìä GitHub Stats&lt;/summary&gt;

![GitHub Stats for claude-code-infrastructure-showcase](https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-code-infrastructure-showcase&amp;username=diet103&amp;all_stats=true&amp;stats_only=true)

&lt;/details&gt;
&lt;br&gt;

[`Claude Code PM`](https://github.com/automazeio/ccpm) &amp;nbsp; by &amp;nbsp; [Ran Aroussi](https://github.com/ranaroussi)  &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT  
Really comprehensive and feature-packed project-management workflow for Claude Code. Numerous specialized agents, slash-commands, and strong documentation.

&lt;details&gt;
&lt;summary&gt;üìä GitHub Stats&lt;/summary&gt;

![GitHub Stats for ccpm](https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=ccpm&amp;username=automazeio&amp;all_stats=true&amp;stats_only=true)

&lt;/details&gt;
&lt;br&gt;

[`claude-code-docs`](https://github.com/costiash/claude-code-docs) &amp;nbsp; by &amp;nbsp; [Constantin Shafranski](https://github.com/costiash)  &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;Mixed  
A mirror of the Anthropic&amp;copy; PBC documentation site for Claude/Code, but with bonus features like full-text search and query-time updates - a nice companion to `claude-code-docs` for up-to-the-minute, fully-indexed information so that Claude Code can read about itself.

&lt;details&gt;
&lt;summary&gt;üìä GitHub Stats&lt;/summary&gt;

![GitHub Stats for claude-code-docs](https://github-readme-stats-plus-theta.vercel.app/api/pin/?repo=claude-code-docs&amp;username=costiash&amp;all_stats=true&amp;stats_only=true)

&lt;/details&gt;
&lt;br&gt;

[`ClaudoPro Directory`](https://github.com/JSONbored/claudepro-directory) &amp;nbsp; by &amp;nbsp; [ghost](https://github.com/JSONbored)  &amp;nbsp;&amp;nbsp;‚öñÔ∏è&amp;nbsp;&amp;nbsp;MIT  


... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[MODSetter/SurfSense]]></title>
            <link>https://github.com/MODSetter/SurfSense</link>
            <guid>https://github.com/MODSetter/SurfSense</guid>
            <pubDate>Sun, 07 Dec 2025 00:04:53 GMT</pubDate>
            <description><![CDATA[Open source alternative to NotebookLM, Perplexity, and Glean. Connects to search engines, Slack, Linear, Jira, ClickUp, Notion, YouTube, GitHub, Discord, and more. Join our Discord: https://discord.gg/ejRNvftDp9]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/MODSetter/SurfSense">MODSetter/SurfSense</a></h1>
            <p>Open source alternative to NotebookLM, Perplexity, and Glean. Connects to search engines, Slack, Linear, Jira, ClickUp, Notion, YouTube, GitHub, Discord, and more. Join our Discord: https://discord.gg/ejRNvftDp9</p>
            <p>Language: Python</p>
            <p>Stars: 11,071</p>
            <p>Forks: 907</p>
            <p>Stars today: 22 stars today</p>
            <h2>README</h2><pre>
![new_header](https://github.com/user-attachments/assets/e236b764-0ddc-42ff-a1f1-8fbb3d2e0e65)


&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://discord.gg/ejRNvftDp9&quot;&gt;
&lt;img src=&quot;https://img.shields.io/discord/1359368468260192417&quot; alt=&quot;Discord&quot;&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

[English](README.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](README.zh-CN.md)

&lt;/div&gt;

# SurfSense
While tools like NotebookLM and Perplexity are impressive and highly effective for conducting research on any topic/query, SurfSense elevates this capability by integrating with your personal knowledge base. It is a highly customizable AI research agent, connected to external sources such as Search Engines (SearxNG, Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar, Luma, Elasticsearch and more to come.

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/13606&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13606&quot; alt=&quot;MODSetter%2FSurfSense | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;


# Video 


https://github.com/user-attachments/assets/d9221908-e0de-4b2f-ac3a-691cf4b202da


## Podcast Sample

https://github.com/user-attachments/assets/a0a16566-6967-4374-ac51-9b3e07fbecd7




## Key Features

### üí° **Idea**: 
- Have your own highly customizable private NotebookLM and Perplexity integrated with external sources.
### üìÅ **Multiple File Format Uploading Support**
- Save content from your own personal files *(Documents, images, videos and supports **50+ file extensions**)* to your own personal knowledge base .
### üîç **Powerful Search**
- Quickly research or find anything in your saved content .
### üí¨ **Chat with your Saved Content**
- Interact in Natural Language and get cited answers.
### üìÑ **Cited Answers**
- Get Cited answers just like Perplexity.
### üîî **Privacy &amp; Local LLM Support**
- Works Flawlessly with Ollama local LLMs.
### üè† **Self Hostable**
- Open source and easy to deploy locally.
### üë• **Team Collaboration with RBAC**
- Role-Based Access Control for Search Spaces
- Invite team members with customizable roles (Owner, Admin, Editor, Viewer)
- Granular permissions for documents, chats, connectors, and settings
- Share knowledge bases securely within your organization
### üéôÔ∏è Podcasts 
- Blazingly fast podcast generation agent. (Creates a 3-minute podcast in under 20 seconds.)
- Convert your chat conversations into engaging audio content
- Support for local TTS providers (Kokoro TTS)
- Support for multiple TTS providers (OpenAI, Azure, Google Vertex AI)

### üìä **Advanced RAG Techniques**
- Supports 100+ LLM&#039;s
- Supports 6000+ Embedding Models.
- Supports all major Rerankers (Pinecode, Cohere, Flashrank etc)
- Uses Hierarchical Indices (2 tiered RAG setup).
- Utilizes Hybrid Search (Semantic + Full Text Search combined with Reciprocal Rank Fusion).

### ‚ÑπÔ∏è **External Sources**
- Search Engines (Tavily, LinkUp)
- SearxNG (self-hosted instances)
- Slack
- Linear
- Jira
- ClickUp
- Confluence
- Notion
- Gmail
- Youtube Videos
- GitHub
- Discord
- Airtable
- Google Calendar
- Luma
- Elasticsearch
- and more to come.....

## üìÑ **Supported File Extensions**

&gt; **Note**: File format support depends on your ETL service configuration. LlamaCloud supports 50+ formats, Unstructured supports 34+ core formats, and Docling (core formats, local processing, privacy-focused, no API key).

### Documents &amp; Text
**LlamaCloud**: `.pdf`, `.doc`, `.docx`, `.docm`, `.dot`, `.dotm`, `.rtf`, `.txt`, `.xml`, `.epub`, `.odt`, `.wpd`, `.pages`, `.key`, `.numbers`, `.602`, `.abw`, `.cgm`, `.cwk`, `.hwp`, `.lwp`, `.mw`, `.mcw`, `.pbd`, `.sda`, `.sdd`, `.sdp`, `.sdw`, `.sgl`, `.sti`, `.sxi`, `.sxw`, `.stw`, `.sxg`, `.uof`, `.uop`, `.uot`, `.vor`, `.wps`, `.zabw`

**Unstructured**: `.doc`, `.docx`, `.odt`, `.rtf`, `.pdf`, `.xml`, `.txt`, `.md`, `.markdown`, `.rst`, `.html`, `.org`, `.epub`

**Docling**: `.pdf`, `.docx`, `.html`, `.htm`, `.xhtml`, `.adoc`, `.asciidoc`

### Presentations
**LlamaCloud**: `.ppt`, `.pptx`, `.pptm`, `.pot`, `.potm`, `.potx`, `.odp`, `.key`

**Unstructured**: `.ppt`, `.pptx`

**Docling**: `.pptx`

### Spreadsheets &amp; Data
**LlamaCloud**: `.xlsx`, `.xls`, `.xlsm`, `.xlsb`, `.xlw`, `.csv`, `.tsv`, `.ods`, `.fods`, `.numbers`, `.dbf`, `.123`, `.dif`, `.sylk`, `.slk`, `.prn`, `.et`, `.uos1`, `.uos2`, `.wk1`, `.wk2`, `.wk3`, `.wk4`, `.wks`, `.wq1`, `.wq2`, `.wb1`, `.wb2`, `.wb3`, `.qpw`, `.xlr`, `.eth`

**Unstructured**: `.xls`, `.xlsx`, `.csv`, `.tsv`

**Docling**: `.xlsx`, `.csv`

### Images
**LlamaCloud**: `.jpg`, `.jpeg`, `.png`, `.gif`, `.bmp`, `.svg`, `.tiff`, `.webp`, `.html`, `.htm`, `.web`

**Unstructured**: `.jpg`, `.jpeg`, `.png`, `.bmp`, `.tiff`, `.heic`

**Docling**: `.jpg`, `.jpeg`, `.png`, `.bmp`, `.tiff`, `.tif`, `.webp`

### Audio &amp; Video *(Always Supported)*
`.mp3`, `.mpga`, `.m4a`, `.wav`, `.mp4`, `.mpeg`, `.webm`

### Email &amp; Communication
**Unstructured**: `.eml`, `.msg`, `.p7s`

### üîñ Cross Browser Extension
- The SurfSense extension can be used to save any webpage you like.
- Its main usecase is to save any webpages protected beyond authentication.



## FEATURE REQUESTS AND FUTURE


**SurfSense is actively being developed.** While it&#039;s not yet production-ready, you can help us speed up the process.

Join the [SurfSense Discord](https://discord.gg/ejRNvftDp9) and help shape the future of SurfSense!

## üöÄ Roadmap

Stay up to date with our development progress and upcoming features!  
Check out our public roadmap and contribute your ideas or feedback:

**View the Roadmap:** [SurfSense Roadmap on GitHub Projects](https://github.com/users/MODSetter/projects/2)


## How to get started?

### Installation Options

SurfSense provides three options to get started:

1. **[SurfSense Cloud](https://www.surfsense.com/login)** - The easiest way to try SurfSense without any setup.
   - No installation required
   - Instant access to all features
   - Perfect for getting started quickly

2. **[Docker Installation (Recommended for Self-Hosting)](https://www.surfsense.net/docs/docker-installation)** - Easy way to get SurfSense up and running with all dependencies containerized.
   - Includes pgAdmin for database management through a web UI
   - Supports environment variable customization via `.env` file
   - Flexible deployment options (full stack or core services only)
   - No need to manually edit configuration files between environments

3. **[Manual Installation](https://www.surfsense.net/docs/manual-installation)** - For users who prefer more control over their setup or need to customize their deployment.

Docker and manual installation guides include detailed OS-specific instructions for Windows, macOS, and Linux.

Before self-hosting installation, make sure to complete the [prerequisite setup steps](https://www.surfsense.net/docs/) including:
- Auth setup
- **File Processing ETL Service** (choose one):
  - Unstructured.io API key (supports 34+ formats)
  - LlamaIndex API key (enhanced parsing, supports 50+ formats)
  - Docling (local processing, no API key required, supports PDF, Office docs, images, HTML, CSV)
- Other required API keys

## Screenshots

**Research Agent** 

![updated_researcher](https://github.com/user-attachments/assets/e22c5d86-f511-4c72-8c50-feba0c1561b4)

**Search Spaces** 

![search_spaces](https://github.com/user-attachments/assets/e254c38c-f937-44b6-9e9d-770db583d099)

**Manage Documents** 
![documents](https://github.com/user-attachments/assets/7001e306-eb06-4009-89c6-8fadfdc3fc4d)

**Podcast Agent** 
![podcasts](https://github.com/user-attachments/assets/6cb82ffd-9e14-4172-bc79-67faf34c4c1c)


**Agent Chat** 

![git_chat](https://github.com/user-attachments/assets/bb352d52-1c6d-4020-926b-722d0b98b491)

**Browser Extension**

![ext1](https://github.com/user-attachments/assets/1f042b7a-6349-422b-94fb-d40d0df16c40)

![ext2](https://github.com/user-attachments/assets/a9b9f1aa-2677-404d-b0a0-c1b2dddf24a7)


## Tech Stack


 ### **BackEnd** 

-  **FastAPI**: Modern, fast web framework for building APIs with Python
  
-  **PostgreSQL with pgvector**: Database with vector search capabilities for similarity searches

-  **SQLAlchemy**: SQL toolkit and ORM (Object-Relational Mapping) for database interactions

-  **Alembic**: A database migrations tool for SQLAlchemy.

-  **FastAPI Users**: Authentication and user management with JWT and OAuth support

-  **LangGraph**: Framework for developing AI-agents.
  
-  **LangChain**: Framework for developing AI-powered applications.

-  **LLM Integration**: Integration with LLM models through LiteLLM

-  **Rerankers**: Advanced result ranking for improved search relevance

-  **Hybrid Search**: Combines vector similarity and full-text search for optimal results using Reciprocal Rank Fusion (RRF)

-  **Vector Embeddings**: Document and text embeddings for semantic search

-  **pgvector**: PostgreSQL extension for efficient vector similarity operations

-  **Redis**: In-memory data structure store used as message broker and result backend for Celery

-  **Celery**: Distributed task queue for handling asynchronous background jobs (document processing, podcast generation, etc.)

-  **Flower**: Real-time monitoring and administration tool for Celery task queues

-  **Chonkie**: Advanced document chunking and embedding library
 - Uses `AutoEmbeddings` for flexible embedding model selection
 -  `LateChunker` for optimized document chunking based on embedding model&#039;s max sequence length


  
---
 ### **FrontEnd**

-  **Next.js 15.2.3**: React framework featuring App Router, server components, automatic code-splitting, and optimized rendering.

-  **React 19.0.0**: JavaScript library for building user interfaces.

-  **TypeScript**: Static type-checking for JavaScript, enhancing code quality and developer experience.
- **Vercel AI SDK Kit UI Stream Protocol**: To create scalable chat UI.

-  **Tailwind CSS 4.x**: Utility-first CSS framework for building custom UI designs.

-  **Shadcn**: Headless components library.

-  **Lucide React**: Icon set implemented as React components.

-  **Framer Motion**: Animation library for React.

-  **Sonner**: Toast notification library.

-  **Geist**: Font family from Vercel.

-  **React Hook Form**: Form state management and validation.

-  **Zod**: TypeScript-first schema validation with static type inference.

-  **@hookform/resolvers**: Resolvers for using validation libraries with React Hook Form.

-  **@tanstack/react-table**: Headless UI for building powerful tables &amp; datagrids.


 ### **DevOps**

-  **Docker**: Container platform for consistent deployment across environments
  
-  **Docker Compose**: Tool for defining and running multi-container Docker applications

-  **pgAdmin**: Web-based PostgreSQL administration tool included in Docker setup


### **Extension** 
 Manifest v3 on Plasmo


## Contribute 

Contributions are very welcome! A contribution can be as small as a ‚≠ê or even finding and creating issues.
Fine-tuning the Backend is always desired.

For detailed contribution guidelines, please see our [CONTRIBUTING.md](CONTRIBUTING.md) file.

## Star History

&lt;a href=&quot;https://www.star-history.com/#MODSetter/SurfSense&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;

---
---
&lt;p align=&quot;center&quot;&gt;
    &lt;img 
      src=&quot;https://github.com/user-attachments/assets/329c9bc2-6005-4aed-a629-700b5ae296b4&quot; 
      alt=&quot;Catalyst Project&quot; 
      width=&quot;200&quot;
    /&gt;
&lt;/p&gt;

---
---
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pytorch/executorch]]></title>
            <link>https://github.com/pytorch/executorch</link>
            <guid>https://github.com/pytorch/executorch</guid>
            <pubDate>Sun, 07 Dec 2025 00:04:52 GMT</pubDate>
            <description><![CDATA[On-device AI across mobile, embedded and edge for PyTorch]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pytorch/executorch">pytorch/executorch</a></h1>
            <p>On-device AI across mobile, embedded and edge for PyTorch</p>
            <p>Language: Python</p>
            <p>Stars: 3,639</p>
            <p>Forks: 748</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/source/_static/img/et-logo.png&quot; alt=&quot;ExecuTorch logo mark&quot; width=&quot;200&quot;&gt;
  &lt;h1&gt;ExecuTorch&lt;/h1&gt;
  &lt;p&gt;&lt;strong&gt;On-device AI inference powered by PyTorch&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pypi.org/project/executorch/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/executorch?style=for-the-badge&amp;color=blue&quot; alt=&quot;PyPI - Version&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/pytorch/executorch/graphs/contributors&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/pytorch/executorch?style=for-the-badge&amp;color=blue&quot; alt=&quot;GitHub - Contributors&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/pytorch/executorch/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/pytorch/executorch?style=for-the-badge&amp;color=blue&quot; alt=&quot;GitHub - Stars&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/Dh43CKSAdc&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20Us-blue?logo=discord&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Discord - Chat with Us&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://docs.pytorch.org/executorch/main/index.html&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Documentation-blue?logo=googledocs&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Documentation&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

**ExecuTorch** is PyTorch&#039;s unified solution for deploying AI models on-device‚Äîfrom smartphones to microcontrollers‚Äîbuilt for privacy, performance, and portability. It powers Meta&#039;s on-device AI across **Instagram, WhatsApp, Quest 3, Ray-Ban Meta Smart Glasses**, and [more](https://docs.pytorch.org/executorch/main/success-stories.html).

Deploy **LLMs, vision, speech, and multimodal models** with the same PyTorch APIs you already know‚Äîaccelerating research to production with seamless model export, optimization, and deployment. No manual C++ rewrites. No format conversions. No vendor lock-in.

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;üìò Table of Contents&lt;/strong&gt;&lt;/summary&gt;

- [Why ExecuTorch?](#why-executorch)
- [How It Works](#how-it-works)
- [Quick Start](#quick-start)
  - [Installation](#installation)
  - [Export and Deploy in 3 Steps](#export-and-deploy-in-3-steps)
  - [Run on Device](#run-on-device)
  - [LLM Example: Llama](#llm-example-llama)
- [Platform &amp; Hardware Support](#platform--hardware-support)
- [Production Deployments](#production-deployments)
- [Examples &amp; Models](#examples--models)
- [Key Features](#key-features)
- [Documentation](#documentation)
- [Community &amp; Contributing](#community--contributing)
- [License](#license)

&lt;/details&gt;

## Why ExecuTorch?

- **üîí Native PyTorch Export** ‚Äî Direct export from PyTorch. No .onnx, .tflite, or intermediate format conversions. Preserve model semantics.
- **‚ö° Production-Proven** ‚Äî Powers billions of users at [Meta with real-time on-device inference](https://engineering.fb.com/2025/07/28/android/executorch-on-device-ml-meta-family-of-apps/).
- **üíæ Tiny Runtime** ‚Äî 50KB base footprint. Runs on microcontrollers to high-end smartphones.
- **üöÄ [12+ Hardware Backends](https://docs.pytorch.org/executorch/main/backends-overview.html)** ‚Äî Open-source acceleration for Apple, Qualcomm, ARM, MediaTek, Vulkan, and more.
- **üéØ One Export, Multiple Backends** ‚Äî Switch hardware targets with a single line change. Deploy the same model everywhere.

## How It Works

ExecuTorch uses **ahead-of-time (AOT) compilation** to prepare PyTorch models for edge deployment:

1. **üß© Export** ‚Äî Capture your PyTorch model graph with `torch.export()`
2. **‚öôÔ∏è Compile** ‚Äî Quantize, optimize, and partition to hardware backends ‚Üí `.pte`
3. **üöÄ Execute** ‚Äî Load `.pte` on-device via lightweight C++ runtime

Models use a standardized [Core ATen operator set](https://docs.pytorch.org/executorch/main/compiler-ir-advanced.html#intermediate-representation). [Partitioners](https://docs.pytorch.org/executorch/main/compiler-delegate-and-partitioner.html) delegate subgraphs to specialized hardware (NPU/GPU) with CPU fallback.

Learn more: [How ExecuTorch Works](https://docs.pytorch.org/executorch/main/intro-how-it-works.html) ‚Ä¢ [Architecture Guide](https://docs.pytorch.org/executorch/main/getting-started-architecture.html)

## Quick Start

### Installation

```bash
pip install executorch
```

For platform-specific setup (Android, iOS, embedded systems), see the [Quick Start](https://docs.pytorch.org/executorch/main/quick-start-section.html) documentation for additional info.

### Export and Deploy in 3 Steps

```python
import torch
from executorch.exir import to_edge_transform_and_lower
from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner

# 1. Export your PyTorch model
model = MyModel().eval()
example_inputs = (torch.randn(1, 3, 224, 224),)
exported_program = torch.export.export(model, example_inputs)

# 2. Optimize for target hardware (switch backends with one line)
program = to_edge_transform_and_lower(
    exported_program,
    partitioner=[XnnpackPartitioner()]  # CPU | CoreMLPartitioner() for iOS | QnnPartitioner() for Qualcomm
).to_executorch()

# 3. Save for deployment
with open(&quot;model.pte&quot;, &quot;wb&quot;) as f:
    f.write(program.buffer)

# Test locally via ExecuTorch runtime&#039;s pybind API (optional)
from executorch.runtime import Runtime
runtime = Runtime.get()
method = runtime.load_program(&quot;model.pte&quot;).load_method(&quot;forward&quot;)
outputs = method.execute([torch.randn(1, 3, 224, 224)])
```

### Run on Device

**[C++](https://docs.pytorch.org/executorch/main/using-executorch-cpp.html)**
```cpp
#include &lt;executorch/extension/module/module.h&gt;
#include &lt;executorch/extension/tensor/tensor.h&gt;

Module module(&quot;model.pte&quot;);
auto tensor = make_tensor_ptr({2, 2}, {1.0f, 2.0f, 3.0f, 4.0f});
auto outputs = module.forward(tensor);
```

**[Swift (iOS)](https://docs.pytorch.org/executorch/main/ios-section.html)**
```swift
import ExecuTorch

let module = Module(filePath: &quot;model.pte&quot;)
let input = Tensor&lt;Float&gt;([1.0, 2.0, 3.0, 4.0], shape: [2, 2])
let outputs = try module.forward(input)
```

**[Kotlin (Android)](https://docs.pytorch.org/executorch/main/android-section.html)**
```kotlin
val module = Module.load(&quot;model.pte&quot;)
val inputTensor = Tensor.fromBlob(floatArrayOf(1.0f, 2.0f, 3.0f, 4.0f), longArrayOf(2, 2))
val outputs = module.forward(EValue.from(inputTensor))
```

### LLM Example: Llama

Export Llama models using the [`export_llm`](https://docs.pytorch.org/executorch/main/llm/export-llm.html) script or [Optimum-ExecuTorch](https://github.com/huggingface/optimum-executorch):

```bash
# Using export_llm
python -m executorch.extension.llm.export.export_llm --model llama3_2 --output llama.pte

# Using Optimum-ExecuTorch
optimum-cli export executorch \
  --model meta-llama/Llama-3.2-1B \
  --task text-generation \
  --recipe xnnpack \
  --output_dir llama_model
```

Run on-device with the LLM runner API:

**[C++](https://docs.pytorch.org/executorch/main/llm/run-with-c-plus-plus.html)**
```cpp
#include &lt;executorch/extension/llm/runner/text_llm_runner.h&gt;

auto runner = create_llama_runner(&quot;llama.pte&quot;, &quot;tiktoken.bin&quot;);
executorch::extension::llm::GenerationConfig config{
    .seq_len = 128, .temperature = 0.8f};
runner-&gt;generate(&quot;Hello, how are you?&quot;, config);
```

**[Swift (iOS)](https://docs.pytorch.org/executorch/main/llm/run-on-ios.html)**
```swift
import ExecuTorchLLM

let runner = TextRunner(modelPath: &quot;llama.pte&quot;, tokenizerPath: &quot;tiktoken.bin&quot;)
try runner.generate(&quot;Hello, how are you?&quot;, Config {
    $0.sequenceLength = 128
}) { token in
    print(token, terminator: &quot;&quot;)
}
```

**Kotlin (Android)** ‚Äî [API Docs](https://docs.pytorch.org/executorch/main/javadoc/org/pytorch/executorch/extension/llm/package-summary.html) ‚Ä¢ [Demo App](https://github.com/meta-pytorch/executorch-examples/tree/main/llm/android/LlamaDemo)
```kotlin
val llmModule = LlmModule(&quot;llama.pte&quot;, &quot;tiktoken.bin&quot;, 0.8f)
llmModule.load()
llmModule.generate(&quot;Hello, how are you?&quot;, 128, object : LlmCallback {
    override fun onResult(result: String) { print(result) }
    override fun onStats(stats: String) { }
})
```

For multimodal models (vision, audio), use the [MultiModal runner API](extension/llm/runner) which extends the LLM runner to handle image and audio inputs alongside text. See [Llava](examples/models/llava/README.md) and [Voxtral](examples/models/voxtral/README.md) examples.

See [examples/models/llama](examples/models/llama/README.md) for complete workflow including quantization, mobile deployment, and advanced options.

**Next Steps:**
- üìñ [Step-by-step tutorial](https://docs.pytorch.org/executorch/main/getting-started.html) ‚Äî Complete walkthrough for your first model
- ‚ö° [Colab notebook](https://colab.research.google.com/drive/1qpxrXC3YdJQzly3mRg-4ayYiOjC6rue3?usp=sharing) ‚Äî Try ExecuTorch instantly in your browser
- ü§ñ [Deploy Llama models](examples/models/llama/README.md) ‚Äî LLM workflow with quantization and mobile demos

## Platform &amp; Hardware Support

| **Platform**     | **Supported Backends**                                   |
|------------------|----------------------------------------------------------|
| Android          | XNNPACK, Vulkan, Qualcomm, MediaTek, Samsung Exynos      |
| iOS              | XNNPACK, MPS, CoreML (Neural Engine)                     |
| Linux / Windows  | XNNPACK, OpenVINO, CUDA *(experimental)*                 |
| macOS            | XNNPACK, MPS, Metal *(experimental)*                     |
| Embedded / MCU   | XNNPACK, ARM Ethos-U, NXP, Cadence DSP                   |

See [Backend Documentation](https://docs.pytorch.org/executorch/main/backends-overview.html) for detailed hardware requirements and optimization guides.

## Production Deployments

ExecuTorch powers on-device AI at scale across Meta&#039;s family of apps, VR/AR devices, and partner deployments. [View success stories ‚Üí](https://docs.pytorch.org/executorch/main/success-stories.html)

## Examples &amp; Models

**LLMs:** [Llama 3.2/3.1/3](examples/models/llama/README.md), [Qwen 3](examples/models/qwen3/README.md), [Phi-4-mini](examples/models/phi_4_mini/README.md), [LiquidAI LFM2](examples/models/lfm2/README.md)

**Multimodal:** [Llava](examples/models/llava/README.md) (vision-language), [Voxtral](examples/models/voxtral/README.md) (audio-language), [Gemma](examples/models/gemma3) (vision-language)

**Vision/Speech:** [MobileNetV2](https://github.com/meta-pytorch/executorch-examples/tree/main/mv2), [DeepLabV3](https://github.com/meta-pytorch/executorch-examples/tree/main/dl3), [Whisper](https://github.com/meta-pytorch/executorch-examples/tree/main/whisper/android/WhisperApp)

**Resources:** [`examples/`](examples/) directory ‚Ä¢ [executorch-examples](https://github.com/meta-pytorch/executorch-examples) out-of-tree demos ‚Ä¢ [Optimum-ExecuTorch](https://github.com/huggingface/optimum-executorch) for HuggingFace models

## Key Features

ExecuTorch provides advanced capabilities for production deployment:

- **Quantization** ‚Äî Built-in support via [torchao](https://docs.pytorch.org/ao) for 8-bit, 4-bit, and dynamic quantization
- **Memory Planning** ‚Äî Optimize memory usage with ahead-of-time allocation strategies
- **Developer Tools** ‚Äî ETDump profiler, ETRecord inspector, and model debugger
- **Selective Build** ‚Äî Strip unused operators to minimize binary size
- **Custom Operators** ‚Äî Extend with domain-specific kernels
- **Dynamic Shapes** ‚Äî Support variable input sizes with bounded ranges

See [Advanced Topics](https://docs.pytorch.org/executorch/main/advanced-topics-section.html) for quantization techniques, custom backends, and compiler passes.

## Documentation

- [**Documentation Home**](https://docs.pytorch.org/executorch/main/index.html) ‚Äî Complete guides and tutorials
- [**API Reference**](https://docs.pytorch.org/executorch/main/api-section.html) ‚Äî Python, C++, Java/Kotlin APIs
- [**Backend Integration**](https://docs.pytorch.org/executorch/main/backend-delegates-integration.html) ‚Äî Build custom hardware backends
- [**Troubleshooting**](https://docs.pytorch.org/executorch/main/support-section.html) ‚Äî Common issues and solutions

## Community &amp; Contributing

We welcome contributions from the community!

- üí¨ [**GitHub Discussions**](https://github.com/pytorch/executorch/discussions) ‚Äî Ask questions and share ideas
- üéÆ [**Discord**](https://discord.gg/Dh43CKSAdc) ‚Äî Chat with the team and community
- üêõ [**Issues**](https://github.com/pytorch/executorch/issues) ‚Äî Report bugs or request features
- ü§ù [**Contributing Guide**](CONTRIBUTING.md) ‚Äî Guidelines and codebase structure

## License

ExecuTorch is BSD licensed, as found in the [LICENSE](LICENSE) file.

&lt;br&gt;&lt;br&gt;

---

&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Part of the PyTorch ecosystem&lt;/strong&gt;&lt;/p&gt;
  &lt;p&gt;
    &lt;a href=&quot;https://github.com/pytorch/executorch&quot;&gt;GitHub&lt;/a&gt; ‚Ä¢
    &lt;a href=&quot;https://docs.pytorch.org/executorch&quot;&gt;Documentation&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[topoteretes/cognee]]></title>
            <link>https://github.com/topoteretes/cognee</link>
            <guid>https://github.com/topoteretes/cognee</guid>
            <pubDate>Sun, 07 Dec 2025 00:04:51 GMT</pubDate>
            <description><![CDATA[Memory for AI Agents in 6 lines of code]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/topoteretes/cognee">topoteretes/cognee</a></h1>
            <p>Memory for AI Agents in 6 lines of code</p>
            <p>Language: Python</p>
            <p>Stars: 10,004</p>
            <p>Forks: 904</p>
            <p>Stars today: 125 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/topoteretes/cognee&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/dev/assets/cognee-logo-transparent.png&quot; alt=&quot;Cognee Logo&quot; height=&quot;60&quot;&gt;
  &lt;/a&gt;

  &lt;br /&gt;

  Cognee - Accurate and Persistent AI Memory

  &lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=1bezuvLwJmw&amp;t=2s&quot;&gt;Demo&lt;/a&gt;
  .
  &lt;a href=&quot;https://docs.cognee.ai/&quot;&gt;Docs&lt;/a&gt;
  .
  &lt;a href=&quot;https://cognee.ai&quot;&gt;Learn More&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://discord.gg/NQPKmU5CCg&quot;&gt;Join Discord&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://www.reddit.com/r/AIMemory/&quot;&gt;Join r/AIMemory&lt;/a&gt;
  .
  &lt;a href=&quot;https://github.com/topoteretes/cognee-community&quot;&gt;Community Plugins &amp; Add-ons&lt;/a&gt;
  &lt;/p&gt;


  [![GitHub forks](https://img.shields.io/github/forks/topoteretes/cognee.svg?style=social&amp;label=Fork&amp;maxAge=2592000)](https://GitHub.com/topoteretes/cognee/network/)
  [![GitHub stars](https://img.shields.io/github/stars/topoteretes/cognee.svg?style=social&amp;label=Star&amp;maxAge=2592000)](https://GitHub.com/topoteretes/cognee/stargazers/)
  [![GitHub commits](https://badgen.net/github/commits/topoteretes/cognee)](https://GitHub.com/topoteretes/cognee/commit/)
  [![GitHub tag](https://badgen.net/github/tag/topoteretes/cognee)](https://github.com/topoteretes/cognee/tags/)
  [![Downloads](https://static.pepy.tech/badge/cognee)](https://pepy.tech/project/cognee)
  [![License](https://img.shields.io/github/license/topoteretes/cognee?colorA=00C586&amp;colorB=000000)](https://github.com/topoteretes/cognee/blob/main/LICENSE)
  [![Contributors](https://img.shields.io/github/contributors/topoteretes/cognee?colorA=00C586&amp;colorB=000000)](https://github.com/topoteretes/cognee/graphs/contributors)
  &lt;a href=&quot;https://github.com/sponsors/topoteretes&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Sponsor-‚ù§Ô∏è-ff69b4.svg&quot; alt=&quot;Sponsor&quot;&gt;&lt;/a&gt;

&lt;p&gt;
  &lt;a href=&quot;https://www.producthunt.com/posts/cognee?embed=true&amp;utm_source=badge-top-post-badge&amp;utm_medium=badge&amp;utm_souce=badge-cognee&quot; target=&quot;_blank&quot; style=&quot;display:inline-block; margin-right:10px;&quot;&gt;
    &lt;img src=&quot;https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=946346&amp;theme=light&amp;period=daily&amp;t=1744472480704&quot; alt=&quot;cognee - Memory&amp;#0032;for&amp;#0032;AI&amp;#0032;Agents&amp;#0032;&amp;#0032;in&amp;#0032;5&amp;#0032;lines&amp;#0032;of&amp;#0032;code | Product Hunt&quot; width=&quot;250&quot; height=&quot;54&quot; /&gt;
  &lt;/a&gt;

  &lt;a href=&quot;https://trendshift.io/repositories/13955&quot; target=&quot;_blank&quot; style=&quot;display:inline-block;&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/13955&quot; alt=&quot;topoteretes%2Fcognee | Trendshift&quot; width=&quot;250&quot; height=&quot;55&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

Use your data to build personalized and dynamic memory for AI Agents. Cognee lets you replace RAG with scalable and modular ECL (Extract, Cognify, Load) pipelines.

  &lt;p align=&quot;center&quot;&gt;
  üåê Available Languages
  :
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://www.readme-i18n.com/topoteretes/cognee?lang=de&quot;&gt;Deutsch&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/topoteretes/cognee?lang=es&quot;&gt;Espa√±ol&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/topoteretes/cognee?lang=fr&quot;&gt;Fran√ßais&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/topoteretes/cognee?lang=ja&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/topoteretes/cognee?lang=ko&quot;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/topoteretes/cognee?lang=pt&quot;&gt;Portugu√™s&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/topoteretes/cognee?lang=ru&quot;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/topoteretes/cognee?lang=zh&quot;&gt;‰∏≠Êñá&lt;/a&gt;
  &lt;/p&gt;


&lt;div style=&quot;text-align: center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_benefits.png&quot; alt=&quot;Why cognee?&quot; width=&quot;50%&quot; /&gt;
&lt;/div&gt;
&lt;/div&gt;

## About Cognee

Cognee is an open-source tool and platform that transforms your raw data into persistent and dynamic AI memory for Agents. It combines vector search with graph databases to make your documents both searchable by meaning and connected by relationships. 

You can use Cognee in two ways:

1. [Self-host Cognee Open Source](https://docs.cognee.ai/getting-started/installation), which stores all data locally by default.
2. [Connect to Cognee Cloud](https://platform.cognee.ai/), and get the same OSS stack on managed infrastructure for easier development and productionization. 

### Cognee Open Source (self-hosted):

- Interconnects any type of data ‚Äî including past conversations, files, images, and audio transcriptions
- Replaces traditional RAG systems with a unified memory layer built on graphs and vectors
- Reduces developer effort and infrastructure cost while improving quality and precision
- Provides Pythonic data pipelines for ingestion from 30+ data sources
- Offers high customizability through user-defined tasks, modular pipelines, and built-in search endpoints

### Cognee Cloud (managed):
- Hosted web UI dashboard 
- Automatic version updates 
- Resource usage analytics
- GDPR compliant, enterprise-grade security

## Basic Usage &amp; Feature Guide

To learn more, [check out this short, end-to-end Colab walkthrough](https://colab.research.google.com/drive/12Vi9zID-M3fpKpKiaqDBvkk98ElkRPWy?usp=sharing) of Cognee&#039;s core features.

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/12Vi9zID-M3fpKpKiaqDBvkk98ElkRPWy?usp=sharing)

## Quickstart

Let‚Äôs try Cognee in just a few lines of code. For detailed setup and configuration, see the [Cognee Docs](https://docs.cognee.ai/getting-started/installation#environment-configuration).

### Prerequisites

- Python 3.10 to 3.13

### Step 1: Install Cognee

You can install Cognee with **pip**, **poetry**, **uv**, or your preferred Python package manager.

```bash
uv pip install cognee
```

### Step 2: Configure the LLM
```python
import os
os.environ[&quot;LLM_API_KEY&quot;] = &quot;YOUR OPENAI_API_KEY&quot;
```
Alternatively, create a `.env` file using our [template](https://github.com/topoteretes/cognee/blob/main/.env.template).

To integrate other LLM providers, see our [LLM Provider Documentation](https://docs.cognee.ai/setup-configuration/llm-providers).

### Step 3: Run the Pipeline

Cognee will take your documents, generate a knowledge graph from them and then query the graph based on combined relationships. 

Now, run a minimal pipeline:

```python
import cognee
import asyncio


async def main():
    # Add text to cognee
    await cognee.add(&quot;Cognee turns documents into AI memory.&quot;)

    # Generate the knowledge graph
    await cognee.cognify()

    # Add memory algorithms to the graph
    await cognee.memify()

    # Query the knowledge graph
    results = await cognee.search(&quot;What does Cognee do?&quot;)

    # Display the results
    for result in results:
        print(result)


if __name__ == &#039;__main__&#039;:
    asyncio.run(main())

```

As you can see, the output is generated from the document we previously stored in Cognee:

```bash
  Cognee turns documents into AI memory.
```

### Use the Cognee CLI 

As an alternative, you can get started with these essential commands:

```bash
cognee-cli add &quot;Cognee turns documents into AI memory.&quot;

cognee-cli cognify

cognee-cli search &quot;What does Cognee do?&quot;
cognee-cli delete --all

```

To open the local UI, run:
```bash
cognee-cli -ui
```

## Demos &amp; Examples

See Cognee in action:

### Persistent Agent Memory

[Cognee Memory for LangGraph Agents](https://github.com/user-attachments/assets/e113b628-7212-4a2b-b288-0be39a93a1c3)

### Simple GraphRAG

[Watch Demo](https://github.com/user-attachments/assets/f2186b2e-305a-42b0-9c2d-9f4473f15df8)

### Cognee with Ollama

[Watch Demo](https://github.com/user-attachments/assets/39672858-f774-4136-b957-1e2de67b8981)


## Community &amp; Support

### Contributing
We welcome contributions from the community! Your input helps make Cognee better for everyone. See [`CONTRIBUTING.md`](CONTRIBUTING.md) to get started.

### Code of Conduct

We&#039;re committed to fostering an inclusive and respectful community. Read our [Code of Conduct](https://github.com/topoteretes/cognee/blob/main/CODE_OF_CONDUCT.md) for guidelines.

## Research &amp; Citation

We recently published a research paper on optimizing knowledge graphs for LLM reasoning:

```bibtex
@misc{markovic2025optimizinginterfaceknowledgegraphs,
      title={Optimizing the Interface Between Knowledge Graphs and LLMs for Complex Reasoning},
      author={Vasilije Markovic and Lazar Obradovic and Laszlo Hajdu and Jovan Pavlovic},
      year={2025},
      eprint={2505.24478},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2505.24478},
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[qxresearch/qxresearch-event-1]]></title>
            <link>https://github.com/qxresearch/qxresearch-event-1</link>
            <guid>https://github.com/qxresearch/qxresearch-event-1</guid>
            <pubDate>Sun, 07 Dec 2025 00:04:50 GMT</pubDate>
            <description><![CDATA[Python hands on tutorial with 50+ Python Application (10 lines of code) By @xiaowuc2]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/qxresearch/qxresearch-event-1">qxresearch/qxresearch-event-1</a></h1>
            <p>Python hands on tutorial with 50+ Python Application (10 lines of code) By @xiaowuc2</p>
            <p>Language: Python</p>
            <p>Stars: 2,291</p>
            <p>Forks: 764</p>
            <p>Stars today: 68 stars today</p>
            <h2>README</h2><pre> &lt;br /&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://qxresearch.github.io/qxresearch-event-1&quot;&gt;
    &lt;img src=&quot;https://github.com/xiaowuc2/ChatGPT-Python-Applications/blob/main/resource/10lines3.gif&quot; alt=&quot;Logo&quot;&gt;
  &lt;/a&gt;

&lt;/p&gt;



Welcome to our GitHub repository featuring 50+ Python applications with only 10 lines of code! In this repository, you&#039;ll find a wide range of topics such as Machine Learning, Deep Learning, GUI, Computer Vision, and API development. Each application is designed to be simple and concise, making it easy to understand and modify. Whether you&#039;re a beginner or an experienced developer, these applications are perfect for learning and experimenting with Python. So dive in, explore, and have fun!

Additionally, we understand that sometimes code can be complex, which is why we&#039;ve created video explanations for each project available on our YouTube channel. With these resources at your disposal, you can quickly gain a deep understanding of the code and easily customize it to suit your needs. Subscribe to the YouTube channel [@qxresearch](https://www.youtube.com/@qxresearch/) to receive updates on new projects! Which also enables you to join a community of like-minded Python enthusiasts and stay connected with a passionate group of learners and experts.


&lt;br&gt;
&lt;br&gt;

&lt;p align=&quot;center&quot;&gt;
About Us : 
  &lt;a href=&quot;https://www.qxresearch.org&quot;&gt;
    @qxresearch
  &lt;/a&gt;

&lt;/p&gt;

qxresearch AI is a research lab focused on Machine Learning, Deep Learning, and Computer Vision. Our team aspires to make discoveries that hold a broad impact, and at the core of our approach lies the sharing of our findings in the field. Our researchers regularly publish in academic journals, release projects as open source on GitHub, and apply these findings in practical applications.

**We are looking for passionate new PhD students, Postdocs, and Master students to join the team!**

- Follow us on [LinkedIn](https://linkedin.com/company/qxresearch) for timely updates regarding new opportunities.
- Kindly email us your research interests and proposal for consideration.
  
```
‚úîÔ∏è If you think this repository has helped you learn something new you can give a star ‚≠ê 
‚ùå If not, point out &#039;why&#039; and spam the issue section üö© 
```

&lt;br&gt;
&lt;br&gt;

### Python Application

* üìº [Voice Recorder](https://github.com/qxresearch/qxresearch-event-1/tree/master/Applications/Voice%20Recorder) : Simple voice recorder with custom time limit 
* üîë [Password Protect PDF](https://github.com/qxresearch/qxresearch-event-1/tree/master/Applications/Password%20Protech%20PDF) : Protect a pdf with custom password 
* üóè [Merge Multiple PDF](https://github.com/qxresearch/qxresearch-event-1/tree/master/Applications/Merge%20Multiple%20PDF) : Merge multiple pdfs with python scripting
* üîî [Windows Notification](https://github.com/qxresearch/qxresearch-event-1/tree/master/Applications/Windows%20Notification) : Custom windows notification maker
* üé¨ [Audio Visualization Tool](https://github.com/qxresearch/qxresearch-event-1/tree/master/Applications/Audio%20Visualization%20Tool) : Awesome audio visualization tool! 
* üìü [Random Password Generator](https://github.com/qxresearch/qxresearch-event-1/tree/master/Applications/Random%20Password%20Generator) : Random secured password generator app
* üé∂ [Extract mp3 from mp4](https://github.com/qxresearch/qxresearch-event-1/tree/master/Applications/Extract%20mp3%20from%20mp4) : Extract audio from video with parsing
* üîó [Link Shortener and Extractor](https://github.com/qxresearch/qxresearch-event-1/tree/master/Applications/Link%20Shortener%20and%20Extractor) : URL shortener and Extractor from terminal
* üîã [Terminal Tricks](https://github.com/qxresearch/qxresearch-event-1/tree/master/Applications/Terminal%20Tricks) : Cool terminal tricks #scripting
* üéÇ [Birthday Reminder](https://github.com/qxresearch/qxresearch-event-1/tree/master/Applications/Birthday%20Reminder) : Birthday reminder for lazy coders
* üìª [Audiobook](https://github.com/qxresearch/qxresearch-event-1/tree/master/Applications/audiobook) : Audiobook creator from text file 
* ‚è∞ [Alarm](https://github.com/qxresearch/qxresearch-event-1/tree/master/Applications/Alarm) : Friendly alarm for programmers to take a break
* ‚è±Ô∏è [Schedule YouTube Video](https://github.com/xiaowuc2/Schedule-YouTube-video-Python/blob/master/python%20code.py) : Python script will play a youtube video at scheduled time
* üìÜ [Calendar](https://github.com/qxresearch/qxresearch-event-1/tree/master/Applications/Calendar) : A tkinter(GUI toolkit) based calendar app 
* ‚úèÔ∏è [Paint](https://github.com/qxresearch/qxresearch-event-1/tree/master/Applications/Paint) : A tkinter(GUI toolkit) based interactive paint clone
* üíª [Screenshot taker](https://github.com/qxresearch/qxresearch-event-1/tree/master/Applications/ScreenShot) : A tkinter based screenshot app with clickable button
* üìñ [Wikipedia Search Engine](https://github.com/qxresearch/qxresearch-event-1/tree/master/Applications/Search%20Engine) : Wekipedia API integrated tkinter based search engine
* üõ†Ô∏è [Cryptographically Secured Random Number Generator](https://github.com/qxresearch/qxresearch-event-1/tree/master/Applications/CSPRNG) : Building a CSRNG from scratch




&lt;br&gt;

### Machine Learning Applications 

`chatGPT`

- ‚úíÔ∏è [email-automation](https://github.com/xiaowuc2/ChatGPT-Python-Applications/tree/main/email-automation) : Tool to automate automate news briefing and blogging from custom senders (mail) 
- ‚≠ê [custom-chatbot](https://github.com/xiaowuc2/ChatGPT-Python-Applications/tree/main/chatbot) : ask chatbot to do custom work on the bases of the task (eg. scriptwriter) 
- üìü [whisper-speech-text](https://github.com/xiaowuc2/ChatGPT-Python-Applications/tree/main/whisper-speech-text) : OpenAI&#039;s another API to convert text from audio
- ‚öôÔ∏è [finetuned-gpt](https://github.com/xiaowuc2/ChatGPT-Python-Applications/tree/main/finetuned-gpt) : Train chatGPT on your custom data &amp; ask queries from that data
- üí† [voice-assistant](https://github.com/xiaowuc2/ChatGPT-Python-Applications/tree/main/voice-assistant) : Voice assistant based on ChatGPT and WhisperAPI (Audio input &amp; output) 
- üêª [web-scraping-summarizer](https://github.com/xiaowuc2/ChatGPT-Python-Applications/tree/main/web-scraping-summarizer) : This tool scrapes a given website and summarizes the main context
- ‚åö [your-prespective](https://raw.githubusercontent.com/xiaowuc2/ChatGPT-Python-Applications/main/resource/git4.png) : You can train ChatGPT to perceive things the way you do, and it will imitate you!
- üìñ [bhagavad-gita-gpt](https://raw.githubusercontent.com/xiaowuc2/ChatGPT-Python-Applications/main/resource/git4.png) : A religious book which contains all the answers to find our purpose and to live it fully
- üèú [vector-databse](https://github.com/xiaowuc2/ChatGPT-Python-Applications/blob/main/vector-database/Vector_Databse.ipynb) : This is how you can send big text files to chatgpt and avoid the token limits


&lt;br&gt;

### Setup 

Refer to this [setup video](https://youtu.be/beEBeQw5tpc) to install the dependencies and generate API keys and incorporate them with our applications. I&#039;ve articulated the steps in text format here : 

- Install the dependencies following these steps : 

  - Star this repository (top right corner) 
  - &lt;&gt;Code &gt; Download ZIP &gt; Open cmd/terminal in that location
  - Run this command : `pip install -r requirements.txt`
  - Replace API keys in `yml` files 

\* The setup for different projects might not be the same. Please refer to the individual setup guides given for each project.

&lt;br&gt;

### Contributing

Any kind of contributions to `qxresearch-event-1` are welcome. Contributions are what make the open source community such an amazing place to learn, inspire, and create.

1. [**Fork**](https://github.com/qxresearch/qxresearch-event-1/fork) the Project
2. Create your Feature Branch
3. Commit your Changes
4. Push to the Branch
5. Open a [**Pull Request**](https://github.com/qxresearch/qxresearch-event-1/pulls)


&lt;br&gt;


### Do you want to join @qxresearch and contribute to new projects? 

* Fill up this [Form](https://forms.gle/tqR8Pa6j27CHaorT6)
* Subscribe to support : [@qxresearch](https://www.youtube.com/qxresearch) 

&lt;br&gt;


&lt;a href=&quot;https://trackgit.com&quot;&gt;
&lt;img src=&quot;https://us-central1-trackgit-analytics.cloudfunctions.net/token/ping/lggxrc0abm2i1s2ok85l&quot; alt=&quot;trackgit-views&quot; /&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[trustedsec/social-engineer-toolkit]]></title>
            <link>https://github.com/trustedsec/social-engineer-toolkit</link>
            <guid>https://github.com/trustedsec/social-engineer-toolkit</guid>
            <pubDate>Sun, 07 Dec 2025 00:04:49 GMT</pubDate>
            <description><![CDATA[The Social-Engineer Toolkit (SET) repository from TrustedSec - All new versions of SET will be deployed here.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/trustedsec/social-engineer-toolkit">trustedsec/social-engineer-toolkit</a></h1>
            <p>The Social-Engineer Toolkit (SET) repository from TrustedSec - All new versions of SET will be deployed here.</p>
            <p>Language: Python</p>
            <p>Stars: 13,388</p>
            <p>Forks: 3,146</p>
            <p>Stars today: 180 stars today</p>
            <h2>README</h2><pre># The Social-Engineer Toolkit (SET)
* Copyright :copyright: 2020
* Written by: David Kennedy (ReL1K) @HackingDave 
* Company: [TrustedSec](https://www.trustedsec.com)

&lt;br/&gt;

## Description
The Social-Engineer Toolkit is an open-source penetration testing framework designed for social engineering. SET has a number of custom attack vectors that allow you to make a believable attack quickly. SET is a product of TrustedSec, LLC ‚Äì an information security consulting firm located in Cleveland, Ohio.

DISCLAIMER: This is *only* for testing purposes and can only be used where strict consent has been given. Do not use this for illegal purposes, period.
Please read the LICENSE under readme/LICENSE for the licensing of SET. 

#### Supported platforms:
* Linux
* Mac OS X (experimental)

# Installation

## Install via requirements.txt

```bash
pip3 install -r requirements.txt
python3 setup.py 
```

## Install SET
=======
#### Mac OS X
You will need to use a virtual environment for the Python install if you are using an M2 Macbook with the following instructions in your CLI within the social-engineer-toolkit directory. 
```bash
    # to install dependencies, run the following:
    python3 -m venv path/to/venv
    source path/to/venv/bin/activate
    python3 -m pip install -r requirements.txt

    # to install SET
    sudo python3 setup.py 
```

&lt;br/&gt;

## Installation
#### Windows 10 WSL/WSL2 Kali Linux
```bash
sudo apt install set -y
```
Kali Linux on Windows 10 is a minimal installation so it doesn&#039;t have any tools installed.
You can easily install Social Engineer Toolkit on WSL/WSL2 without needing pip using the above command.

#### Linux
```bash
git clone https://github.com/trustedsec/social-engineer-toolkit/ setoolkit/
cd setoolkit
pip3 install -r requirements.txt
python setup.py
```
&lt;br/&gt;

## SET Tutorial
For a full document on how to use SET, [visit the SET user manual](https://github.com/trustedsec/social-engineer-toolkit/raw/master/readme/User_Manual.pdf).

&lt;br/&gt;

## Bugs and enhancements
For bug reports or enhancements, please open an [issue](https://github.com/trustedsec/social-engineer-toolkit/issues) here.
&lt;br/&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>