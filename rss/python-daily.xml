<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Fri, 20 Jun 2025 00:04:46 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[vllm-project/vllm]]></title>
            <link>https://github.com/vllm-project/vllm</link>
            <guid>https://github.com/vllm-project/vllm</guid>
            <pubDate>Fri, 20 Jun 2025 00:04:46 GMT</pubDate>
            <description><![CDATA[A high-throughput and memory-efficient inference and serving engine for LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/vllm-project/vllm">vllm-project/vllm</a></h1>
            <p>A high-throughput and memory-efficient inference and serving engine for LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 50,147</p>
            <p>Forks: 8,199</p>
            <p>Stars today: 99 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-dark.png&quot;&gt;
    &lt;img alt=&quot;vLLM&quot; src=&quot;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-light.png&quot; width=55%&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;h3 align=&quot;center&quot;&gt;
Easy, fast, and cheap LLM serving for everyone
&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
| &lt;a href=&quot;https://docs.vllm.ai&quot;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://blog.vllm.ai/&quot;&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://arxiv.org/abs/2309.06180&quot;&gt;&lt;b&gt;Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://x.com/vllm_project&quot;&gt;&lt;b&gt;Twitter/X&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://discuss.vllm.ai&quot;&gt;&lt;b&gt;User Forum&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://slack.vllm.ai&quot;&gt;&lt;b&gt;Developer Slack&lt;/b&gt;&lt;/a&gt; |
&lt;/p&gt;

---

*Latest News* üî•
- [2025/05] We hosted [NYC vLLM Meetup](https://lu.ma/c1rqyf1f)! Please find the meetup slides [here](https://docs.google.com/presentation/d/1_q_aW_ioMJWUImf1s1YM-ZhjXz8cUeL0IJvaquOYBeA/edit?usp=sharing).
- [2025/05] vLLM is now a hosted project under PyTorch Foundation! Please find the announcement [here](https://pytorch.org/blog/pytorch-foundation-welcomes-vllm/).
- [2025/04] We hosted [Asia Developer Day](https://www.sginnovate.com/event/limited-availability-morning-evening-slots-remaining-inaugural-vllm-asia-developer-day)! Please find the meetup slides from the vLLM team [here](https://docs.google.com/presentation/d/19cp6Qu8u48ihB91A064XfaXruNYiBOUKrBxAmDOllOo/edit?usp=sharing).
- [2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post [here](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html).

&lt;details&gt;
&lt;summary&gt;Previous News&lt;/summary&gt;

- [2025/03] We hosted [vLLM x Ollama Inference Night](https://lu.ma/vllm-ollama)! Please find the meetup slides from the vLLM team [here](https://docs.google.com/presentation/d/16T2PDD1YwRnZ4Tu8Q5r6n53c5Lr5c73UV9Vd2_eBo4U/edit?usp=sharing).
- [2025/03] We hosted [the first vLLM China Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg)! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1REHvfQMKGnvz6p3Fd23HhSO4c8j5WPGZV0bKYLwnHyQ/edit?usp=sharing).
- [2025/03] We hosted [the East Coast vLLM Meetup](https://lu.ma/7mu4k4xx)! Please find the meetup slides [here](https://docs.google.com/presentation/d/1NHiv8EUFF1NLd3fEYODm56nDmL26lEeXCaDgyDlTsRs/edit#slide=id.g31441846c39_0_0).
- [2025/02] We hosted [the ninth vLLM meetup](https://lu.ma/h7g3kuj9) with Meta! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1jzC_PZVXrVNSFVCW-V4cFXb6pn7zZ2CyP_Flwo05aqg/edit?usp=sharing) and AMD [here](https://drive.google.com/file/d/1Zk5qEJIkTmlQ2eQcXQZlljAx3m9s7nwn/view?usp=sharing). The slides from Meta will not be posted.
- [2025/01] We hosted [the eighth vLLM meetup](https://lu.ma/zep56hui) with Google Cloud! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing), and Google Cloud team [here](https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing).
- [2024/12] vLLM joins [pytorch ecosystem](https://pytorch.org/blog/vllm-joins-pytorch)! Easy, Fast, and Cheap LLM Serving for Everyone!
- [2024/11] We hosted [the seventh vLLM meetup](https://lu.ma/h0qvrajz) with Snowflake! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing), and Snowflake team [here](https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing).
- [2024/10] We have just created a developer slack ([slack.vllm.ai](https://slack.vllm.ai)) focusing on coordinating contributions and discussing features. Please feel free to join us there!
- [2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team [here](https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing). Learn more from the [talks](https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR) from other vLLM contributors and users!
- [2024/09] We hosted [the sixth vLLM meetup](https://lu.ma/87q3nvnh) with NVIDIA! Please find the meetup slides [here](https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing).
- [2024/07] We hosted [the fifth vLLM meetup](https://lu.ma/lp0gyjqr) with AWS! Please find the meetup slides [here](https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing).
- [2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post [here](https://blog.vllm.ai/2024/07/23/llama31.html).
- [2024/06] We hosted [the fourth vLLM meetup](https://lu.ma/agivllm) with Cloudflare and BentoML! Please find the meetup slides [here](https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing).
- [2024/04] We hosted [the third vLLM meetup](https://robloxandvllmmeetup2024.splashthat.com/) with Roblox! Please find the meetup slides [here](https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing).
- [2024/01] We hosted [the second vLLM meetup](https://lu.ma/ygxbpzhl) with IBM! Please find the meetup slides [here](https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing).
- [2023/10] We hosted [the first vLLM meetup](https://lu.ma/first-vllm-meetup) with a16z! Please find the meetup slides [here](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing).
- [2023/08] We would like to express our sincere gratitude to [Andreessen Horowitz](https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/) (a16z) for providing a generous grant to support the open-source development and research of vLLM.
- [2023/06] We officially released vLLM! FastChat-vLLM integration has powered [LMSYS Vicuna and Chatbot Arena](https://chat.lmsys.org) since mid-April. Check out our [blog post](https://vllm.ai).

&lt;/details&gt;

---
## About

vLLM is a fast and easy-to-use library for LLM inference and serving.

Originally developed in the [Sky Computing Lab](https://sky.cs.berkeley.edu) at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.

vLLM is fast with:

- State-of-the-art serving throughput
- Efficient management of attention key and value memory with [**PagedAttention**](https://blog.vllm.ai/2023/06/20/vllm.html)
- Continuous batching of incoming requests
- Fast model execution with CUDA/HIP graph
- Quantizations: [GPTQ](https://arxiv.org/abs/2210.17323), [AWQ](https://arxiv.org/abs/2306.00978), [AutoRound](https://arxiv.org/abs/2309.05516), INT4, INT8, and FP8
- Optimized CUDA kernels, including integration with FlashAttention and FlashInfer
- Speculative decoding
- Chunked prefill

**Performance benchmark**: We include a performance benchmark at the end of [our blog post](https://blog.vllm.ai/2024/09/05/perf-update.html). It compares the performance of vLLM against other LLM serving engines ([TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM), [SGLang](https://github.com/sgl-project/sglang) and [LMDeploy](https://github.com/InternLM/lmdeploy)). The implementation is under [nightly-benchmarks folder](.buildkite/nightly-benchmarks/) and you can [reproduce](https://github.com/vllm-project/vllm/issues/8176) this benchmark using our one-click runnable script.

vLLM is flexible and easy to use with:

- Seamless integration with popular Hugging Face models
- High-throughput serving with various decoding algorithms, including *parallel sampling*, *beam search*, and more
- Tensor parallelism and pipeline parallelism support for distributed inference
- Streaming outputs
- OpenAI-compatible API server
- Support NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, TPU, and AWS Neuron
- Prefix caching support
- Multi-LoRA support

vLLM seamlessly supports most popular open-source models on HuggingFace, including:
- Transformer-like LLMs (e.g., Llama)
- Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)
- Embedding Models (e.g., E5-Mistral)
- Multi-modal LLMs (e.g., LLaVA)

Find the full list of supported models [here](https://docs.vllm.ai/en/latest/models/supported_models.html).

## Getting Started

Install vLLM with `pip` or [from source](https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source):

```bash
pip install vllm
```

Visit our [documentation](https://docs.vllm.ai/en/latest/) to learn more.
- [Installation](https://docs.vllm.ai/en/latest/getting_started/installation.html)
- [Quickstart](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)
- [List of Supported Models](https://docs.vllm.ai/en/latest/models/supported_models.html)

## Contributing

We welcome and value any contributions and collaborations.
Please check out [Contributing to vLLM](https://docs.vllm.ai/en/latest/contributing/index.html) for how to get involved.

## Sponsors

vLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!

&lt;!-- Note: Please sort them in alphabetical order. --&gt;
&lt;!-- Note: Please keep these consistent with docs/community/sponsors.md --&gt;
Cash Donations:
- a16z
- Dropbox
- Sequoia Capital
- Skywork AI
- ZhenFund

Compute Resources:
- AMD
- Anyscale
- AWS
- Crusoe Cloud
- Databricks
- DeepInfra
- Google Cloud
- Intel
- Lambda Lab
- Nebius
- Novita AI
- NVIDIA
- Replicate
- Roblox
- RunPod
- Trainy
- UC Berkeley
- UC San Diego

Slack Sponsor: Anyscale

We also have an official fundraising venue through [OpenCollective](https://opencollective.com/vllm). We plan to use the fund to support the development, maintenance, and adoption of vLLM.

## Citation

If you use vLLM for your research, please cite our [paper](https://arxiv.org/abs/2309.06180):

```bibtex
@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}
```

## Contact Us

- For technical questions and feature requests, please use GitHub [Issues](https://github.com/vllm-project/vllm/issues) or [Discussions](https://github.com/vllm-project/vllm/discussions)
- For discussing with fellow users, please use the [vLLM Forum](https://discuss.vllm.ai)
- For coordinating contributions and development, please use [Slack](https://slack.vllm.ai)
- For security disclosures, please use GitHub&#039;s [Security Advisories](https://github.com/vllm-project/vllm/security/advisories) feature
- For collaborations and partnerships, please contact us at [vllm-questions@lists.berkeley.edu](mailto:vllm-questions@lists.berkeley.edu)

## Media Kit

- If you wish to use vLLM&#039;s logo, please refer to [our media kit repo](https://github.com/vllm-project/media-kit)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[cyclotruc/gitingest]]></title>
            <link>https://github.com/cyclotruc/gitingest</link>
            <guid>https://github.com/cyclotruc/gitingest</guid>
            <pubDate>Fri, 20 Jun 2025 00:04:45 GMT</pubDate>
            <description><![CDATA[Replace 'hub' with 'ingest' in any github url to get a prompt-friendly extract of a codebase]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/cyclotruc/gitingest">cyclotruc/gitingest</a></h1>
            <p>Replace 'hub' with 'ingest' in any github url to get a prompt-friendly extract of a codebase</p>
            <p>Language: Python</p>
            <p>Stars: 9,327</p>
            <p>Forks: 720</p>
            <p>Stars today: 96 stars today</p>
            <h2>README</h2><pre># Gitingest

[![Image](./docs/frontpage.png &quot;Gitingest main page&quot;)](https://gitingest.com)

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/cyclotruc/gitingest/blob/main/LICENSE)
[![PyPI version](https://badge.fury.io/py/gitingest.svg)](https://badge.fury.io/py/gitingest)
[![GitHub stars](https://img.shields.io/github/stars/cyclotruc/gitingest?style=social.svg)](https://github.com/cyclotruc/gitingest)
[![Downloads](https://pepy.tech/badge/gitingest)](https://pepy.tech/project/gitingest)

[![Discord](https://dcbadge.limes.pink/api/server/https://discord.com/invite/zerRaGK9EC)](https://discord.com/invite/zerRaGK9EC)

Turn any Git repository into a prompt-friendly text ingest for LLMs.

You can also replace `hub` with `ingest` in any GitHub URL to access the corresponding digest.

[gitingest.com](https://gitingest.com) ¬∑ [Chrome Extension](https://chromewebstore.google.com/detail/adfjahbijlkjfoicpjkhjicpjpjfaood) ¬∑ [Firefox Add-on](https://addons.mozilla.org/firefox/addon/gitingest)

## üöÄ Features

- **Easy code context**: Get a text digest from a Git repository URL or a directory
- **Smart Formatting**: Optimized output format for LLM prompts
- **Statistics about**:
  - File and directory structure
  - Size of the extract
  - Token count
- **CLI tool**: Run it as a shell command
- **Python package**: Import it in your code

## üìö Requirements

- Python 3.8+
- For private repositories: A GitHub Personal Access Token (PAT). You can generate one at [https://github.com/settings/personal-access-tokens](https://github.com/settings/personal-access-tokens) (Profile ‚Üí Settings ‚Üí Developer Settings ‚Üí Personal Access Tokens ‚Üí Fine-grained Tokens)

### üì¶ Installation

Gitingest is available on [PyPI](https://pypi.org/project/gitingest/).
You can install it using `pip`:

```bash
pip install gitingest
```

However, it might be a good idea to use `pipx` to install it.
You can install `pipx` using your preferred package manager.

```bash
brew install pipx
apt install pipx
scoop install pipx
...
```

If you are using pipx for the first time, run:

```bash
pipx ensurepath
```

```bash
# install gitingest
pipx install gitingest
```

## üß© Browser Extension Usage

&lt;!-- markdownlint-disable MD033 --&gt;
&lt;a href=&quot;https://chromewebstore.google.com/detail/adfjahbijlkjfoicpjkhjicpjpjfaood&quot; target=&quot;_blank&quot; title=&quot;Get Gitingest Extension from Chrome Web Store&quot;&gt;&lt;img height=&quot;48&quot; src=&quot;https://github.com/user-attachments/assets/20a6e44b-fd46-4e6c-8ea6-aad436035753&quot; alt=&quot;Available in the Chrome Web Store&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://addons.mozilla.org/firefox/addon/gitingest&quot; target=&quot;_blank&quot; title=&quot;Get Gitingest Extension from Firefox Add-ons&quot;&gt;&lt;img height=&quot;48&quot; src=&quot;https://github.com/user-attachments/assets/c0e99e6b-97cf-4af2-9737-099db7d3538b&quot; alt=&quot;Get The Add-on for Firefox&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://microsoftedge.microsoft.com/addons/detail/nfobhllgcekbmpifkjlopfdfdmljmipf&quot; target=&quot;_blank&quot; title=&quot;Get Gitingest Extension from Microsoft Edge Add-ons&quot;&gt;&lt;img height=&quot;48&quot; src=&quot;https://github.com/user-attachments/assets/204157eb-4cae-4c0e-b2cb-db514419fd9e&quot; alt=&quot;Get from the Edge Add-ons&quot; /&gt;&lt;/a&gt;
&lt;!-- markdownlint-enable MD033 --&gt;

The extension is open source at [lcandy2/gitingest-extension](https://github.com/lcandy2/gitingest-extension).

Issues and feature requests are welcome to the repo.

## üí° Command line usage

The `gitingest` command line tool allows you to analyze codebases and create a text dump of their contents.

```bash
# Basic usage (writes to digest.txt by default)
gitingest /path/to/directory

# From URL
gitingest https://github.com/cyclotruc/gitingest
```

For private repositories, use the `--token/-t` option.

```bash
# Get your token from https://github.com/settings/personal-access-tokens
gitingest https://github.com/username/private-repo --token github_pat_...

# Or set it as an environment variable
export GITHUB_TOKEN=github_pat_...
gitingest https://github.com/username/private-repo
```

By default, the digest is written to a text file (`digest.txt`) in your current working directory. You can customize the output in two ways:

- Use `--output/-o &lt;filename&gt;` to write to a specific file.
- Use `--output/-o -` to output directly to `STDOUT` (useful for piping to other tools).

See more options and usage details with:

```bash
gitingest --help
```

## üêç Python package usage

```python
# Synchronous usage
from gitingest import ingest

summary, tree, content = ingest(&quot;path/to/directory&quot;)

# or from URL
summary, tree, content = ingest(&quot;https://github.com/cyclotruc/gitingest&quot;)
```

For private repositories, you can pass a token:

```python
# Using token parameter
summary, tree, content = ingest(&quot;https://github.com/username/private-repo&quot;, token=&quot;github_pat_...&quot;)

# Or set it as an environment variable
import os
os.environ[&quot;GITHUB_TOKEN&quot;] = &quot;github_pat_...&quot;
summary, tree, content = ingest(&quot;https://github.com/username/private-repo&quot;)
```

By default, this won&#039;t write a file but can be enabled with the `output` argument.

```python
# Asynchronous usage
from gitingest import ingest_async
import asyncio

result = asyncio.run(ingest_async(&quot;path/to/directory&quot;))
```

### Jupyter notebook usage

```python
from gitingest import ingest_async

# Use await directly in Jupyter
summary, tree, content = await ingest_async(&quot;path/to/directory&quot;)

```

This is because Jupyter notebooks are asynchronous by default.

## üê≥ Self-host

1. Build the image:

   ``` bash
   docker build -t gitingest .
   ```

2. Run the container:

   ``` bash
   docker run -d --name gitingest -p 8000:8000 gitingest
   ```

The application will be available at `http://localhost:8000`.

If you are hosting it on a domain, you can specify the allowed hostnames via env variable `ALLOWED_HOSTS`.

   ```bash
   # Default: &quot;gitingest.com, *.gitingest.com, localhost, 127.0.0.1&quot;.
   ALLOWED_HOSTS=&quot;example.com, localhost, 127.0.0.1&quot;
   ```

## ü§ù Contributing

### Non-technical ways to contribute

- **Create an Issue**: If you find a bug or have an idea for a new feature, please [create an issue](https://github.com/cyclotruc/gitingest/issues/new) on GitHub. This will help us track and prioritize your request.
- **Spread the Word**: If you like Gitingest, please share it with your friends, colleagues, and on social media. This will help us grow the community and make Gitingest even better.
- **Use Gitingest**: The best feedback comes from real-world usage! If you encounter any issues or have ideas for improvement, please let us know by [creating an issue](https://github.com/cyclotruc/gitingest/issues/new) on GitHub or by reaching out to us on [Discord](https://discord.com/invite/zerRaGK9EC).

### Technical ways to contribute

Gitingest aims to be friendly for first time contributors, with a simple Python and HTML codebase. If you need any help while working with the code, reach out to us on [Discord](https://discord.com/invite/zerRaGK9EC). For detailed instructions on how to make a pull request, see [CONTRIBUTING.md](./CONTRIBUTING.md).

## üõ†Ô∏è Stack

- [Tailwind CSS](https://tailwindcss.com) - Frontend
- [FastAPI](https://github.com/fastapi/fastapi) - Backend framework
- [Jinja2](https://jinja.palletsprojects.com) - HTML templating
- [tiktoken](https://github.com/openai/tiktoken) - Token estimation
- [posthog](https://github.com/PostHog/posthog) - Amazing analytics

### Looking for a JavaScript/FileSystemNode package?

Check out the NPM alternative üì¶ Repomix: &lt;https://github.com/yamadashy/repomix&gt;

## üöÄ Project Growth

[![Star History Chart](https://api.star-history.com/svg?repos=cyclotruc/gitingest&amp;type=Date)](https://star-history.com/#cyclotruc/gitingest&amp;Date)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[netbox-community/netbox]]></title>
            <link>https://github.com/netbox-community/netbox</link>
            <guid>https://github.com/netbox-community/netbox</guid>
            <pubDate>Fri, 20 Jun 2025 00:04:44 GMT</pubDate>
            <description><![CDATA[The premier source of truth powering network automation. Open source under Apache 2. Try NetBox Cloud free: https://netboxlabs.com/products/free-netbox-cloud/]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/netbox-community/netbox">netbox-community/netbox</a></h1>
            <p>The premier source of truth powering network automation. Open source under Apache 2. Try NetBox Cloud free: https://netboxlabs.com/products/free-netbox-cloud/</p>
            <p>Language: Python</p>
            <p>Stars: 17,957</p>
            <p>Forks: 2,736</p>
            <p>Stars today: 19 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/netbox-community/netbox/main/docs/netbox_logo_light.svg&quot; width=&quot;400&quot; alt=&quot;NetBox logo&quot; /&gt;
  &lt;p&gt;&lt;strong&gt;The cornerstone of every automated network&lt;/strong&gt;&lt;/p&gt;
  &lt;a href=&quot;https://github.com/netbox-community/netbox/releases&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/netbox-community/netbox&quot; alt=&quot;Latest release&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/netbox-community/netbox/blob/main/LICENSE.txt&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-Apache_2.0-blue.svg&quot; alt=&quot;License&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/netbox-community/netbox/graphs/contributors&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/netbox-community/netbox?color=blue&quot; alt=&quot;Contributors&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/netbox-community/netbox/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/netbox-community/netbox?style=flat&quot; alt=&quot;GitHub stars&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://explore.transifex.com/netbox-community/netbox/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/languages-15-blue&quot; alt=&quot;Languages supported&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/netbox-community/netbox/actions/workflows/ci.yml&quot;&gt;&lt;img src=&quot;https://github.com/netbox-community/netbox/workflows/CI/badge.svg?branch=main&quot; alt=&quot;CI status&quot; /&gt;&lt;/a&gt;
  &lt;p&gt;
    &lt;strong&gt;&lt;a href=&quot;https://github.com/netbox-community/netbox/&quot;&gt;NetBox Community&lt;/a&gt;&lt;/strong&gt; |
    &lt;strong&gt;&lt;a href=&quot;https://netboxlabs.com/netbox-cloud/&quot;&gt;NetBox Cloud&lt;/a&gt;&lt;/strong&gt; |
    &lt;strong&gt;&lt;a href=&quot;https://netboxlabs.com/netbox-enterprise/&quot;&gt;NetBox Enterprise&lt;/a&gt;&lt;/strong&gt;
  &lt;/p&gt;
&lt;/div&gt;

NetBox exists to empower network engineers. Since its release in 2016, it has become the go-to solution for modeling and documenting network infrastructure for thousands of organizations worldwide. As a successor to legacy IPAM and DCIM applications, NetBox provides a cohesive, extensive, and accessible data model for all things networked. By providing a single robust user interface and programmable APIs for everything from cable maps to device configurations, NetBox serves as the central source of truth for the modern network.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;#netboxs-role&quot;&gt;NetBox&#039;s Role&lt;/a&gt; |
  &lt;a href=&quot;#why-netbox&quot;&gt;Why NetBox?&lt;/a&gt; |
  &lt;a href=&quot;#getting-started&quot;&gt;Getting Started&lt;/a&gt; |
  &lt;a href=&quot;#get-involved&quot;&gt;Get Involved&lt;/a&gt; |
  &lt;a href=&quot;#screenshots&quot;&gt;Screenshots&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/media/screenshots/home-light.png&quot; width=&quot;600&quot; alt=&quot;NetBox user interface screenshot&quot; /&gt;
&lt;/p&gt;

## NetBox&#039;s Role

NetBox functions as the **source of truth** for your network infrastructure. Its job is to define and validate the _intended state_ of all network components and resources. NetBox does not interact with network nodes directly; rather, it makes this data available programmatically to purpose-built automation, monitoring, and assurance tools. This separation of duties enables the construction of a robust yet flexible automation system.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/media/misc/reference_architecture.png&quot; alt=&quot;Reference network automation architecture&quot; /&gt;
&lt;/p&gt;

The diagram above illustrates the recommended deployment architecture for an automated network, leveraging NetBox as the central authority for network state. This approach allows your team to swap out individual tools to meet changing needs while retaining a predictable, modular workflow.

## Why NetBox?

### Comprehensive Data Model

Racks, devices, cables, IP addresses, VLANs, circuits, power, VPNs, and lots more: NetBox is built for networks. Its comprehensive and thoroughly inter-linked data model provides for natural and highly structured modeling of myriad network primitives that just isn&#039;t possible using general-purpose tools. And there&#039;s no need to waste time contemplating how to build out a database: Everything is ready to go upon installation.

### Focused Development

NetBox strives to meet a singular goal: Provide the best available solution for making network infrastructure programmatically accessible. Unlike &quot;all-in-one&quot; tools which awkwardly bolt on half-baked features in an attempt to check every box, NetBox is committed to its core function. NetBox provides the best possible solution for modeling network infrastructure, and provides rich APIs for integrating with tools that excel in other areas of network automation.

### Extensible and Customizable

No two networks are exactly the same. Users are empowered to extend NetBox&#039;s native data model with custom fields and tags to best suit their unique needs. You can even write your own plugins to introduce entirely new objects and functionality!

### Flexible Permissions

NetBox includes a fully customizable permission system, which affords administrators incredible granularity when assigning roles to users and groups. Want to restrict certain users to working only with cabling and not be able to change IP addresses? Or maybe each team should have access only to a particular tenant? NetBox enables you to craft roles as you see fit.

### Custom Validation &amp; Protection Rules

The data you put into NetBox is crucial to network operations. In addition to its robust native validation rules, NetBox provides mechanisms for administrators to define their own custom validation rules for objects. Custom validation can be used both to ensure new or modified objects adhere to a set of rules, and to prevent the deletion of objects which don&#039;t meet certain criteria. (For example, you might want to prevent the deletion of a device with an &quot;active&quot; status.)

### Device Configuration Rendering

NetBox can render user-created Jinja2 templates to generate device configurations from its own data. Configuration templates can be uploaded individually or pulled automatically from an external source, such as a git repository. Rendered configurations can be retrieved via the REST API for application directly to network devices via a provisioning tool such as Ansible or Salt.

### Custom Scripts

Complex workflows, such as provisioning a new branch office, can be tedious to carry out via the user interface. NetBox allows you to write and upload custom scripts that can be run directly from the UI. Scripts prompt users for input and then automate the necessary tasks to greatly simplify otherwise burdensome processes.

### Automated Events

Users can define event rules to automatically trigger a custom script or outbound webhook in response to a NetBox event. For example, you might want to automatically update a network monitoring service whenever a new device is added to NetBox, or update a DHCP server when an IP range is allocated.

### Comprehensive Change Logging

NetBox automatically logs the creation, modification, and deletion of all managed objects, providing a thorough change history. Changes can be attributed to the executing user, and related changes are grouped automatically by request ID.

&gt; [!NOTE]
&gt; A complete list of NetBox&#039;s myriad features can be found in [the introductory documentation](https://docs.netbox.dev/en/stable/introduction/).

## Getting Started

* Just want to explore? Check out [our public demo](https://demo.netbox.dev/) right now!
* The [official documentation](https://docs.netbox.dev) offers a comprehensive introduction.
* Check out [our wiki](https://github.com/netbox-community/netbox/wiki/Community-Contributions) for even more projects to get the most out of NetBox!

## Get Involved

* Follow [@NetBoxOfficial](https://twitter.com/NetBoxOfficial) on Twitter!
* Join the conversation on [the discussion forum](https://github.com/netbox-community/netbox/discussions) and [Slack](https://netdev.chat/)!
* Already a power user? You can [suggest a feature](https://github.com/netbox-community/netbox/issues/new?assignees=&amp;labels=type%3A+feature&amp;template=feature_request.yaml) or [report a bug](https://github.com/netbox-community/netbox/issues/new?assignees=&amp;labels=type%3A+bug&amp;template=bug_report.yaml) on GitHub.
* Contributions from the community are encouraged and appreciated! Check out our [contributing guide](CONTRIBUTING.md) to get started.
* [Share your idea](https://plugin-ideas.netbox.dev/) for a new plugin, or [learn how to build one](https://github.com/netbox-community/netbox-plugin-tutorial) yourself!

## Screenshots

&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;NetBox Dashboard (Light Mode)&lt;/strong&gt;&lt;br /&gt;
  &lt;img src=&quot;docs/media/screenshots/home-light.png&quot; width=&quot;600&quot; alt=&quot;NetBox dashboard (light mode)&quot; /&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;NetBox Dashboard (Dark Mode)&lt;/strong&gt;&lt;br /&gt;
  &lt;img src=&quot;docs/media/screenshots/home-dark.png&quot; width=&quot;600&quot; alt=&quot;NetBox dashboard (dark mode)&quot; /&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;Prefixes List&lt;/strong&gt;&lt;br /&gt;
  &lt;img src=&quot;docs/media/screenshots/prefixes-list.png&quot; width=&quot;600&quot; alt=&quot;Prefixes list&quot; /&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;Rack View&lt;/strong&gt;&lt;br /&gt;
  &lt;img src=&quot;docs/media/screenshots/rack.png&quot; width=&quot;600&quot; alt=&quot;Rack view&quot; /&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;Cable Trace&lt;/strong&gt;&lt;br /&gt;
  &lt;img src=&quot;docs/media/screenshots/cable-trace.png&quot; width=&quot;600&quot; alt=&quot;Cable trace&quot; /&gt;
&lt;/p&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[cookiecutter/cookiecutter-django]]></title>
            <link>https://github.com/cookiecutter/cookiecutter-django</link>
            <guid>https://github.com/cookiecutter/cookiecutter-django</guid>
            <pubDate>Fri, 20 Jun 2025 00:04:43 GMT</pubDate>
            <description><![CDATA[Cookiecutter Django is a framework for jumpstarting production-ready Django projects quickly.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/cookiecutter/cookiecutter-django">cookiecutter/cookiecutter-django</a></h1>
            <p>Cookiecutter Django is a framework for jumpstarting production-ready Django projects quickly.</p>
            <p>Language: Python</p>
            <p>Stars: 12,850</p>
            <p>Forks: 2,986</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre># Cookiecutter Django

[![Build Status](https://img.shields.io/github/actions/workflow/status/cookiecutter/cookiecutter-django/ci.yml?branch=master)](https://github.com/cookiecutter/cookiecutter-django/actions/workflows/ci.yml?query=branch%3Amaster)
[![Documentation Status](https://readthedocs.org/projects/cookiecutter-django/badge/?version=latest)](https://cookiecutter-django.readthedocs.io/en/latest/?badge=latest)
[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/cookiecutter/cookiecutter-django/master.svg)](https://results.pre-commit.ci/latest/github/cookiecutter/cookiecutter-django/master)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)

[![Updates](https://pyup.io/repos/github/cookiecutter/cookiecutter-django/shield.svg)](https://pyup.io/repos/github/cookiecutter/cookiecutter-django/)
[![Join our Discord](https://img.shields.io/badge/Discord-cookiecutter-5865F2?style=flat&amp;logo=discord&amp;logoColor=white)](https://discord.gg/rAWFUP47d2)
[![Code Helpers Badge](https://www.codetriage.com/cookiecutter/cookiecutter-django/badges/users.svg)](https://www.codetriage.com/cookiecutter/cookiecutter-django)

Powered by [Cookiecutter](https://github.com/cookiecutter/cookiecutter), Cookiecutter Django is a framework for jumpstarting
production-ready Django projects quickly.

- Documentation: &lt;https://cookiecutter-django.readthedocs.io/en/latest/&gt;
- See [Troubleshooting](https://cookiecutter-django.readthedocs.io/en/latest/5-help/troubleshooting.html) for common errors and obstacles
- If you have problems with Cookiecutter Django, please open [issues](https://github.com/cookiecutter/cookiecutter-django/issues/new) don&#039;t send
  emails to the maintainers.

## Features

- For Django 5.1
- Works with Python 3.12
- Renders Django projects with 100% starting test coverage
- Twitter [Bootstrap](https://github.com/twbs/bootstrap) v5
- [12-Factor](https://12factor.net) based settings via [django-environ](https://github.com/joke2k/django-environ)
- Secure by default. We believe in SSL.
- Optimized development and production settings
- Registration via [django-allauth](https://github.com/pennersr/django-allauth)
- Comes with custom user model ready to go
- Optional basic ASGI setup for Websockets
- Optional custom static build using Gulp or Webpack
- Send emails via [Anymail](https://github.com/anymail/django-anymail) (using [Mailgun](http://www.mailgun.com/) by default or Amazon SES if AWS is selected cloud provider, but switchable)
- Media storage using Amazon S3, Google Cloud Storage, Azure Storage or nginx
- Docker support using [docker-compose](https://github.com/docker/compose) for development and production (using [Traefik](https://traefik.io/) with [LetsEncrypt](https://letsencrypt.org/) support)
- [Procfile](https://devcenter.heroku.com/articles/procfile) for deploying to Heroku
- Instructions for deploying to [PythonAnywhere](https://www.pythonanywhere.com/)
- Run tests with unittest or pytest
- Customizable PostgreSQL version
- Default integration with [pre-commit](https://github.com/pre-commit/pre-commit) for identifying simple issues before submission to code review

## Optional Integrations

_These features can be enabled during initial project setup._

- Serve static files from Amazon S3, Google Cloud Storage, Azure Storage or [Whitenoise](https://whitenoise.readthedocs.io/)
- Configuration for [Celery](https://docs.celeryq.dev) and [Flower](https://github.com/mher/flower) (the latter in Docker setup only)
- Integration with [Mailpit](https://github.com/axllent/mailpit/) for local email testing
- Integration with [Sentry](https://sentry.io/welcome/) for error logging

## Constraints

- Only maintained 3rd party libraries are used.
- Uses PostgreSQL everywhere: 13 - 17 ([MySQL fork](https://github.com/mabdullahadeel/cookiecutter-django-mysql) also available).
- Environment variables for configuration (This won&#039;t work with Apache/mod_wsgi).

## Support this Project!

This project is an open source project run by volunteers. You can sponsor us via [OpenCollective](https://opencollective.com/cookiecutter-django) or individually via GitHub Sponsors:

- Daniel Roy Greenfeld, Project Lead ([GitHub](https://github.com/pydanny), [Patreon](https://www.patreon.com/danielroygreenfeld)): expertise in Django and AWS ELB.
- Fabio C. Barrionuevo, Core Developer ([GitHub](https://github.com/luzfcb)): expertise in Python/Django, hands-on DevOps and frontend experience.
- Bruno Alla, Core Developer ([GitHub](https://github.com/browniebroke)): expertise in Python/Django and DevOps.
- Nikita Shupeyko, Core Developer ([GitHub](https://github.com/webyneter)): expertise in Python/Django, hands-on DevOps and frontend experience.

Projects that provide financial support to the maintainers:

### Two Scoops of Django

[![Cover of the book &quot;Two Scoops of Django 3.x&quot;](https://f004.backblazeb2.com/file/feldroycom/images/book-TSD3-800.jpg)](https://www.feldroy.com/two-scoops-press#two-scoops-of-django)

Two Scoops of Django 3.x is the best ice cream-themed Django reference in the universe!

### PyUp

[![PyUp Logo](https://pyup.io/static/images/logo.png)](https://pyup.io)

PyUp brings you automated security and dependency updates used by Google and other organizations. Free for open source projects!

## Usage

Let&#039;s pretend you want to create a Django project called &quot;redditclone&quot;. Rather than using `startproject`
and then editing the results to include your name, email, and various configuration issues that always get forgotten until the worst possible moment, get [cookiecutter](https://github.com/cookiecutter/cookiecutter) to do all the work.

First, get Cookiecutter. Trust me, it&#039;s awesome:

    $ pip install &quot;cookiecutter&gt;=1.7.0&quot;

Now run it against this repo:

    $ cookiecutter https://github.com/cookiecutter/cookiecutter-django

You&#039;ll be prompted for some values. Provide them, then a Django project will be created for you.

**Warning**: After this point, change &#039;Daniel Greenfeld&#039;, &#039;pydanny&#039;, etc to your own information.

Answer the prompts with your own desired [options](http://cookiecutter-django.readthedocs.io/en/latest/1-getting-started/project-generation-options.html). For example:

    Cloning into &#039;cookiecutter-django&#039;...
    remote: Counting objects: 550, done.
    remote: Compressing objects: 100% (310/310), done.
    remote: Total 550 (delta 283), reused 479 (delta 222)
    Receiving objects: 100% (550/550), 127.66 KiB | 58 KiB/s, done.
    Resolving deltas: 100% (283/283), done.
    project_name [My Awesome Project]: Reddit Clone
    project_slug [reddit_clone]: reddit
    description [Behold My Awesome Project!]: A reddit clone.
    author_name [Daniel Roy Greenfeld]: Daniel Greenfeld
    domain_name [example.com]: myreddit.com
    email [daniel-greenfeld@example.com]: pydanny@gmail.com
    version [0.1.0]: 0.0.1
    Select open_source_license:
    1 - MIT
    2 - BSD
    3 - GPLv3
    4 - Apache Software License 2.0
    5 - Not open source
    Choose from 1, 2, 3, 4, 5 [1]: 1
    Select username_type:
    1 - username
    2 - email
    Choose from 1, 2 [1]: 1
    timezone [UTC]: America/Los_Angeles
    windows [n]: n
    Select an editor to use. The choices are:
    1 - None
    2 - PyCharm
    3 - VS Code
    Choose from 1, 2, 3 [1]: 1
    use_docker [n]: n
    Select postgresql_version:
    1 - 17
    2 - 16
    3 - 15
    4 - 14
    5 - 13
    Choose from 1, 2, 3, 4, 5 [1]: 1
    Select cloud_provider:
    1 - AWS
    2 - GCP
    3 - None
    Choose from 1, 2, 3 [1]: 1
    Select mail_service:
    1 - Mailgun
    2 - Amazon SES
    3 - Mailjet
    4 - Mandrill
    5 - Postmark
    6 - Sendgrid
    7 - Brevo (formerly SendinBlue)
    8 - SparkPost
    9 - Other SMTP
    Choose from 1, 2, 3, 4, 5, 6, 7, 8, 9 [1]: 1
    use_async [n]: n
    use_drf [n]: y
    Select frontend_pipeline:
    1 - None
    2 - Django Compressor
    3 - Gulp
    4 - Webpack
    Choose from 1, 2, 3, 4 [1]: 1
    use_celery [n]: y
    use_mailpit [n]: n
    use_sentry [n]: y
    use_whitenoise [n]: n
    use_heroku [n]: y
    Select ci_tool:
    1 - None
    2 - Travis
    3 - Gitlab
    4 - Github
    Choose from 1, 2, 3, 4 [1]: 4
    keep_local_envs_in_vcs [y]: y
    debug [n]: n

Enter the project and take a look around:

    $ cd reddit/
    $ ls

Create a git repo and push it there:

    $ git init
    $ git add .
    $ git commit -m &quot;first awesome commit&quot;
    $ git remote add origin git@github.com:pydanny/redditclone.git
    $ git push -u origin master

Now take a look at your repo. Don&#039;t forget to carefully look at the generated README. Awesome, right?

For local development, see the following:

- [Developing locally](https://cookiecutter-django.readthedocs.io/en/latest/2-local-development/developing-locally.html)
- [Developing locally using docker](https://cookiecutter-django.readthedocs.io/en/latest/2-local-development/developing-locally-docker.html)

## Community

- Have questions? **Before you ask questions anywhere else**, please post your question on [Stack Overflow](http://stackoverflow.com/questions/tagged/cookiecutter-django) under the _cookiecutter-django_ tag. We check there periodically for questions.
- If you think you found a bug or want to request a feature, please open an [issue](https://github.com/cookiecutter/cookiecutter-django/issues).
- For anything else, you can chat with us on [Discord](https://discord.gg/uFXweDQc5a).

&lt;img src=&quot;https://opencollective.com/cookiecutter-django/contributors.svg?width=890&amp;button=false&quot; alt=&quot;Contributors&quot;&gt;

## For Readers of Two Scoops of Django

You may notice that some elements of this project do not exactly match what we describe in chapter 3. The reason for that is this project, amongst other things, serves as a test bed for trying out new ideas and concepts. Sometimes they work, sometimes they don&#039;t, but the end result is that it won&#039;t necessarily match precisely what is described in the book I co-authored.

## For PyUp Users

If you are using [PyUp](https://pyup.io) to keep your dependencies updated and secure, use the code _cookiecutter_ during checkout to get 15% off every month.

## &quot;Your Stuff&quot;

Scattered throughout the Python and HTML of this project are places marked with &quot;your stuff&quot;. This is where third-party libraries are to be integrated with your project.

## For MySQL users

To get full MySQL support in addition to the default Postgresql, you can use this fork of the cookiecutter-django:
https://github.com/mabdullahadeel/cookiecutter-django-mysql

## Releases

Need a stable release? You can find them at &lt;https://github.com/cookiecutter/cookiecutter-django/releases&gt;

## Not Exactly What You Want?

This is what I want. _It might not be what you want._ Don&#039;t worry, you have options:

### Fork This

If you have differences in your preferred setup, I encourage you to fork this to create your own version.
Once you have your fork working, let me know and I&#039;ll add it to a &#039;_Similar Cookiecutter Templates_&#039; list here.
It&#039;s up to you whether to rename your fork.

If you do rename your fork, I encourage you to submit it to the following places:

- [cookiecutter](https://github.com/cookiecutter/cookiecutter) so it gets listed in the README as a template.
- The cookiecutter [grid](https://www.djangopackages.com/grids/g/cookiecutters/) on Django Packages.

### Submit a Pull Request

We accept pull requests if they&#039;re small, atomic, and make our own project development
experience better.

## Articles

- [Why cookiecutter-django is Essential for Your Next Django Project](https://medium.com/@millsks/why-cookiecutter-django-is-essential-for-your-next-django-project-7d3c00cdce51) - Aug. 4, 2024
- [How to Make Your Own Django Cookiecutter Template!](https://medium.com/@FatemeFouladkar/how-to-make-your-own-django-cookiecutter-template-a753d4cbb8c2) - Aug. 10, 2023
- [Cookiecutter Django With Amazon RDS](https://haseeburrehman.com/posts/cookiecutter-django-with-amazon-rds/) - Apr, 2, 2021
- [Complete Walkthrough: Blue/Green Deployment to AWS ECS using GitHub actions](https://github.com/Andrew-Chen-Wang/cookiecutter-django-ecs-github) - June 10, 2020
- [Using cookiecutter-django with Google Cloud Storage](https://ahhda.github.io/cloud/gce/django/2019/03/12/using-django-cookiecutter-cloud-storage.html) - Mar. 12, 2019
- [cookiecutter-django with Nginx, Route 53 and ELB](https://msaizar.com/blog/cookiecutter-django-nginx-route-53-and-elb/) - Feb. 12, 2018
- [cookiecutter-django and Amazon RDS](https://msaizar.com/blog/cookiecutter-django-and-amazon-rds/) - Feb. 7, 2018
- [Using Cookiecutter to Jumpstart a Django Project on Windows with PyCharm](https://joshuahunter.com/posts/using-cookiecutter-to-jumpstart-a-django-project-on-windows-with-pycharm/) - May 19, 2017
- [Exploring with Cookiecutter](http://www.snowboardingcoder.com/django/2016/12/03/exploring-with-cookiecutter/) - Dec. 3, 2016
- [Introduction to Cookiecutter-Django](http://krzysztofzuraw.com/blog/2016/django-cookiecutter.html) - Feb. 19, 2016
- [Django and GitLab - Running Continuous Integration and tests with your FREE account](http://dezoito.github.io/2016/05/11/django-gitlab-continuous-integration-phantomjs.html) - May. 11, 2016
- [Development and Deployment of Cookiecutter-Django on Fedora](https://realpython.com/blog/python/development-and-deployment-of-cookiecutter-django-on-fedora/) - Jan. 18, 2016
- [Development and Deployment of Cookiecutter-Django via Docker](https://realpython.com/blog/python/development-and-deployment-of-cookiecutter-django-via-docker/) - Dec. 29, 2015
- [How to create a Django Application using Cookiecutter and Django 1.8](https://www.swapps.io/blog/how-to-create-a-django-application-using-cookiecutter-and-django-1-8/) - Sept. 12, 2015

Have a blog or online publication? Write about your cookiecutter-django tips and tricks, then send us a pull request with the link.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mindsdb/mindsdb]]></title>
            <link>https://github.com/mindsdb/mindsdb</link>
            <guid>https://github.com/mindsdb/mindsdb</guid>
            <pubDate>Fri, 20 Jun 2025 00:04:42 GMT</pubDate>
            <description><![CDATA[AI's query engine - Platform for building AI that can answer questions over large scale federated data. - The only MCP Server you'll ever need]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mindsdb/mindsdb">mindsdb/mindsdb</a></h1>
            <p>AI's query engine - Platform for building AI that can answer questions over large scale federated data. - The only MCP Server you'll ever need</p>
            <p>Language: Python</p>
            <p>Stars: 32,406</p>
            <p>Forks: 5,328</p>
            <p>Stars today: 78 stars today</p>
            <h2>README</h2><pre>

&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
	&lt;a href=&quot;https://pypi.org/project/MindsDB/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/MindsDB.svg&quot; alt=&quot;MindsDB Release&quot;&gt;&lt;/a&gt;
	&lt;a href=&quot;https://www.python.org/downloads/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/python-3.10.x%7C%203.11.x-brightgreen.svg&quot; alt=&quot;Python supported&quot;&gt;&lt;/a&gt;
	&lt;a href=&quot;https://ossrank.com/p/630&quot;&gt;&lt;img src=&quot;https://shields.io/endpoint?url=https://ossrank.com/shield/630&quot;&gt;&lt;/a&gt;
	&lt;img alt=&quot;PyPI - Downloads&quot; src=&quot;https://img.shields.io/pypi/dm/Mindsdb&quot;&gt;
	&lt;a href=&quot;https://hub.docker.com/u/mindsdb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/mindsdb/mindsdb&quot; alt=&quot;Docker pulls&quot;&gt;&lt;/a&gt;

  &lt;br /&gt;
  &lt;br /&gt;

  &lt;a href=&quot;https://github.com/mindsdb/mindsdb&quot;&gt;
    &lt;img src=&quot;/docs/assets/mindsdb_logo.png&quot; alt=&quot;MindsDB&quot; width=&quot;300&quot;&gt;
  &lt;/a&gt;

  &lt;p align=&quot;center&quot;&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://www.mindsdb.com?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Website&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://docs.mindsdb.com?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Docs&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://mdb.ai/register&quot;&gt;Demo&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://mindsdb.com/joincommunity?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Community Slack&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

----------------------------------------


MindsDB enables humans, AI, agents, and applications to get highly accurate answers across sprawled and large scale data sources.

![image](https://github.com/user-attachments/assets/a796276a-2d3e-4aa2-9a52-25bf44cf32e7)


[MindsDB has an MCP server built in](https://docs.mindsdb.com/mcp/overview) that enables your MCP applications to connect, unify and respond to questions over large-scale federated data‚Äîspanning databases, data warehouses, and SaaS applications.

## Minds [Demo](https://mdb.ai/register)
Play with [Minds demo](https://mdb.ai/register), and see the power of MindsDB at answering questions from structured to unstructured data, whether it&#039;s scattered across SaaS applications, databases, or... hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered.
 
## Install MindsDB Server 

MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart&#039;s content.

  * [Using Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop). This is the fastest and recommended way to get started and have it all running.
  * [Using Docker](https://docs.mindsdb.com/setup/self-hosted/docker). This is also simple, but gives you more flexibility on how to further customize your server.
  * [Using PyPI](https://docs.mindsdb.com/contribute/install). This option enables you to contribute to MindsDB.

----------------------------------------

# Core Philosophy: Connect, Unify, Respond

MindsDB&#039;s architecture is built around three fundamental capabilities:

## [Connect](https://docs.mindsdb.com/integrations/data-overview) Your Data

You can connect to hundreds of enterprise [data sources (learn more)](https://docs.mindsdb.com/integrations/data-overview). These integrations allow MindsDB to access data wherever it resides, forming the foundation for all other capabilities.

## [Unify](https://docs.mindsdb.com/mindsdb_sql/overview) Your Data

Once connected, these data sources can be queried using a full SQL dialect, as if they were all part of a single database. MindsDB‚Äôs federated query engine translates your SQL queries and executes them on the appropriate connected data sources.

When working with many data sources, it‚Äôs important to prepare and unify your data before generating responses from it. MindsDB SQL offers virtual tables (views, knowledge bases, ml-models) to allow working with heterogeneous data as if it were unified in a single organized system.

* [**VIEWS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/view) ‚Äì Simplify data access by creating unified views across different sources (no-ETL).
* [**KNOWLEDGE BASES**](https://docs.mindsdb.com/mindsdb_sql/knowledge-bases) ‚Äì Index and organize unstructured data for efficient retrieval.
* [**ML MODELS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/model) ‚Äì Apply AI/ML transformations to gain insights from your data.

Unification of data can be automated using JOBs

* [**JOBS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/jobs) ‚Äì Schedule synchronization and transformation tasks for real-time processing.


## [Respond](https://docs.mindsdb.com/mindsdb_sql/agents/agent) From Your Data

Chat with Your Data

* [**AGENTS**](https://docs.mindsdb.com/mindsdb_sql/agents/agent) ‚Äì Configure built-in agents specialized in answering questions over your connected and unified data.
* [**MCP**](https://docs.mindsdb.com/mcp/overview) ‚Äì Connect to MindsDB through the MCP (Model Context Protocol) for seamless interaction.

----------------------------------------

## ü§ù Contribute

Interested in contributing to MindsDB? Follow our [installation guide for development](https://docs.mindsdb.com/contribute/install?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

You can find our [contribution guide here](https://docs.mindsdb.com/contribute/contribute?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

We welcome suggestions! Feel free to open new issues with your ideas, and we‚Äôll guide you.

This project adheres to a [Contributor Code of Conduct](https://github.com/mindsdb/mindsdb/blob/main/CODE_OF_CONDUCT.md). By participating, you agree to follow its terms.

Also, check out our [community rewards and programs](https://mindsdb.com/community?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

## ü§ç Support

If you find a bug, please submit an [issue on GitHub](https://github.com/mindsdb/mindsdb/issues/new/choose).

Here‚Äôs how you can get community support:

* Ask a question in our [Slack Community](https://mindsdb.com/joincommunity?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).
* Join our [GitHub Discussions](https://github.com/mindsdb/mindsdb/discussions).
* Post on [Stack Overflow](https://stackoverflow.com/questions/tagged/mindsdb) with the MindsDB tag.

For commercial support, please [contact the MindsDB team](https://mindsdb.com/contact?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

## üíö Current Contributors

&lt;a href=&quot;https://github.com/mindsdb/mindsdb/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contributors-img.web.app/image?repo=mindsdb/mindsdb&quot; /&gt;
&lt;/a&gt;

Generated with [contributors-img](https://contributors-img.web.app).

## üîî Subscribe for Updates

Join our [Slack community](https://mindsdb.com/joincommunity)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[getsentry/sentry]]></title>
            <link>https://github.com/getsentry/sentry</link>
            <guid>https://github.com/getsentry/sentry</guid>
            <pubDate>Fri, 20 Jun 2025 00:04:41 GMT</pubDate>
            <description><![CDATA[Developer-first error tracking and performance monitoring]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/getsentry/sentry">getsentry/sentry</a></h1>
            <p>Developer-first error tracking and performance monitoring</p>
            <p>Language: Python</p>
            <p>Stars: 41,138</p>
            <p>Forks: 4,371</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://sentry.io/?utm_source=github&amp;utm_medium=logo&quot; target=&quot;_blank&quot;&gt;
      &lt;img src=&quot;https://sentry-brand.storage.googleapis.com/sentry-wordmark-dark-280x84.png&quot; alt=&quot;Sentry&quot; width=&quot;280&quot; height=&quot;84&quot; /&gt;
    &lt;/a&gt;
  &lt;/p&gt;
  &lt;p align=&quot;center&quot;&gt;
    Users and logs provide clues. Sentry provides answers.
  &lt;/p&gt;
&lt;/p&gt;

# What&#039;s Sentry?

Sentry is a developer-first error tracking and performance monitoring platform that helps developers see what actually matters, solve quicker, and learn continuously about their applications.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/projects.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/issue-details.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/transaction-summary.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/releases.png&quot; width=&quot;270&quot; /&gt;
&lt;/p&gt;

## Official Sentry SDKs

- [JavaScript](https://github.com/getsentry/sentry-javascript)
- [Electron](https://github.com/getsentry/sentry-electron/)
- [React-Native](https://github.com/getsentry/sentry-react-native)
- [Python](https://github.com/getsentry/sentry-python)
- [Ruby](https://github.com/getsentry/sentry-ruby)
- [PHP](https://github.com/getsentry/sentry-php)
- [Laravel](https://github.com/getsentry/sentry-laravel)
- [Go](https://github.com/getsentry/sentry-go)
- [Rust](https://github.com/getsentry/sentry-rust)
- [Java/Kotlin](https://github.com/getsentry/sentry-java)
- [Objective-C/Swift](https://github.com/getsentry/sentry-cocoa)
- [C\#/F\#](https://github.com/getsentry/sentry-dotnet)
- [C/C++](https://github.com/getsentry/sentry-native)
- [Dart](https://github.com/getsentry/sentry-dart)
- [Perl](https://github.com/getsentry/perl-raven)
- [Clojure](https://github.com/getsentry/sentry-clj/)
- [Elixir](https://github.com/getsentry/sentry-elixir)
- [Unity](https://github.com/getsentry/sentry-unity)
- [Unreal Engine](https://github.com/getsentry/sentry-unreal)
- [PowerShell](https://github.com/getsentry/sentry-powershell)

# Resources

- [Documentation](https://docs.sentry.io/)
- [Discussions](https://github.com/getsentry/sentry/discussions) (Bugs, feature requests,
  general questions)
- [Discord](https://discord.gg/PXa5Apfe7K)
- [Contributing](https://docs.sentry.io/internal/contributing/)
- [Bug Tracker](https://github.com/getsentry/sentry/issues)
- [Code](https://github.com/getsentry/sentry)
- [Transifex](https://www.transifex.com/getsentry/sentry/) (Translate
  Sentry\!)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[modelcontextprotocol/python-sdk]]></title>
            <link>https://github.com/modelcontextprotocol/python-sdk</link>
            <guid>https://github.com/modelcontextprotocol/python-sdk</guid>
            <pubDate>Fri, 20 Jun 2025 00:04:40 GMT</pubDate>
            <description><![CDATA[The official Python SDK for Model Context Protocol servers and clients]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/modelcontextprotocol/python-sdk">modelcontextprotocol/python-sdk</a></h1>
            <p>The official Python SDK for Model Context Protocol servers and clients</p>
            <p>Language: Python</p>
            <p>Stars: 14,804</p>
            <p>Forks: 1,825</p>
            <p>Stars today: 89 stars today</p>
            <h2>README</h2><pre># MCP Python SDK

&lt;div align=&quot;center&quot;&gt;

&lt;strong&gt;Python implementation of the Model Context Protocol (MCP)&lt;/strong&gt;

[![PyPI][pypi-badge]][pypi-url]
[![MIT licensed][mit-badge]][mit-url]
[![Python Version][python-badge]][python-url]
[![Documentation][docs-badge]][docs-url]
[![Specification][spec-badge]][spec-url]
[![GitHub Discussions][discussions-badge]][discussions-url]

&lt;/div&gt;

&lt;!-- omit in toc --&gt;
## Table of Contents

- [MCP Python SDK](#mcp-python-sdk)
  - [Overview](#overview)
  - [Installation](#installation)
    - [Adding MCP to your python project](#adding-mcp-to-your-python-project)
    - [Running the standalone MCP development tools](#running-the-standalone-mcp-development-tools)
  - [Quickstart](#quickstart)
  - [What is MCP?](#what-is-mcp)
  - [Core Concepts](#core-concepts)
    - [Server](#server)
    - [Resources](#resources)
    - [Tools](#tools)
    - [Prompts](#prompts)
    - [Images](#images)
    - [Context](#context)
    - [Completions](#completions)
    - [Elicitation](#elicitation)
    - [Authentication](#authentication)
  - [Running Your Server](#running-your-server)
    - [Development Mode](#development-mode)
    - [Claude Desktop Integration](#claude-desktop-integration)
    - [Direct Execution](#direct-execution)
    - [Mounting to an Existing ASGI Server](#mounting-to-an-existing-asgi-server)
  - [Examples](#examples)
    - [Echo Server](#echo-server)
    - [SQLite Explorer](#sqlite-explorer)
  - [Advanced Usage](#advanced-usage)
    - [Low-Level Server](#low-level-server)
    - [Writing MCP Clients](#writing-mcp-clients)
    - [MCP Primitives](#mcp-primitives)
    - [Server Capabilities](#server-capabilities)
  - [Documentation](#documentation)
  - [Contributing](#contributing)
  - [License](#license)

[pypi-badge]: https://img.shields.io/pypi/v/mcp.svg
[pypi-url]: https://pypi.org/project/mcp/
[mit-badge]: https://img.shields.io/pypi/l/mcp.svg
[mit-url]: https://github.com/modelcontextprotocol/python-sdk/blob/main/LICENSE
[python-badge]: https://img.shields.io/pypi/pyversions/mcp.svg
[python-url]: https://www.python.org/downloads/
[docs-badge]: https://img.shields.io/badge/docs-modelcontextprotocol.io-blue.svg
[docs-url]: https://modelcontextprotocol.io
[spec-badge]: https://img.shields.io/badge/spec-spec.modelcontextprotocol.io-blue.svg
[spec-url]: https://spec.modelcontextprotocol.io
[discussions-badge]: https://img.shields.io/github/discussions/modelcontextprotocol/python-sdk
[discussions-url]: https://github.com/modelcontextprotocol/python-sdk/discussions

## Overview

The Model Context Protocol allows applications to provide context for LLMs in a standardized way, separating the concerns of providing context from the actual LLM interaction. This Python SDK implements the full MCP specification, making it easy to:

- Build MCP clients that can connect to any MCP server
- Create MCP servers that expose resources, prompts and tools
- Use standard transports like stdio, SSE, and Streamable HTTP
- Handle all MCP protocol messages and lifecycle events

## Installation

### Adding MCP to your python project

We recommend using [uv](https://docs.astral.sh/uv/) to manage your Python projects.

If you haven&#039;t created a uv-managed project yet, create one:

   ```bash
   uv init mcp-server-demo
   cd mcp-server-demo
   ```

   Then add MCP to your project dependencies:

   ```bash
   uv add &quot;mcp[cli]&quot;
   ```

Alternatively, for projects using pip for dependencies:
```bash
pip install &quot;mcp[cli]&quot;
```

### Running the standalone MCP development tools

To run the mcp command with uv:

```bash
uv run mcp
```

## Quickstart

Let&#039;s create a simple MCP server that exposes a calculator tool and some data:

```python
# server.py
from mcp.server.fastmcp import FastMCP

# Create an MCP server
mcp = FastMCP(&quot;Demo&quot;)


# Add an addition tool
@mcp.tool()
def add(a: int, b: int) -&gt; int:
    &quot;&quot;&quot;Add two numbers&quot;&quot;&quot;
    return a + b


# Add a dynamic greeting resource
@mcp.resource(&quot;greeting://{name}&quot;)
def get_greeting(name: str) -&gt; str:
    &quot;&quot;&quot;Get a personalized greeting&quot;&quot;&quot;
    return f&quot;Hello, {name}!&quot;
```

You can install this server in [Claude Desktop](https://claude.ai/download) and interact with it right away by running:
```bash
mcp install server.py
```

Alternatively, you can test it with the MCP Inspector:
```bash
mcp dev server.py
```

## What is MCP?

The [Model Context Protocol (MCP)](https://modelcontextprotocol.io) lets you build servers that expose data and functionality to LLM applications in a secure, standardized way. Think of it like a web API, but specifically designed for LLM interactions. MCP servers can:

- Expose data through **Resources** (think of these sort of like GET endpoints; they are used to load information into the LLM&#039;s context)
- Provide functionality through **Tools** (sort of like POST endpoints; they are used to execute code or otherwise produce a side effect)
- Define interaction patterns through **Prompts** (reusable templates for LLM interactions)
- And more!

## Core Concepts

### Server

The FastMCP server is your core interface to the MCP protocol. It handles connection management, protocol compliance, and message routing:

```python
# Add lifespan support for startup/shutdown with strong typing
from contextlib import asynccontextmanager
from collections.abc import AsyncIterator
from dataclasses import dataclass

from fake_database import Database  # Replace with your actual DB type

from mcp.server.fastmcp import FastMCP

# Create a named server
mcp = FastMCP(&quot;My App&quot;)

# Specify dependencies for deployment and development
mcp = FastMCP(&quot;My App&quot;, dependencies=[&quot;pandas&quot;, &quot;numpy&quot;])


@dataclass
class AppContext:
    db: Database


@asynccontextmanager
async def app_lifespan(server: FastMCP) -&gt; AsyncIterator[AppContext]:
    &quot;&quot;&quot;Manage application lifecycle with type-safe context&quot;&quot;&quot;
    # Initialize on startup
    db = await Database.connect()
    try:
        yield AppContext(db=db)
    finally:
        # Cleanup on shutdown
        await db.disconnect()


# Pass lifespan to server
mcp = FastMCP(&quot;My App&quot;, lifespan=app_lifespan)


# Access type-safe lifespan context in tools
@mcp.tool()
def query_db() -&gt; str:
    &quot;&quot;&quot;Tool that uses initialized resources&quot;&quot;&quot;
    ctx = mcp.get_context()
    db = ctx.request_context.lifespan_context[&quot;db&quot;]
    return db.query()
```

### Resources

Resources are how you expose data to LLMs. They&#039;re similar to GET endpoints in a REST API - they provide data but shouldn&#039;t perform significant computation or have side effects:

```python
from mcp.server.fastmcp import FastMCP

mcp = FastMCP(&quot;My App&quot;)


@mcp.resource(&quot;config://app&quot;, title=&quot;Application Configuration&quot;)
def get_config() -&gt; str:
    &quot;&quot;&quot;Static configuration data&quot;&quot;&quot;
    return &quot;App configuration here&quot;


@mcp.resource(&quot;users://{user_id}/profile&quot;, title=&quot;User Profile&quot;)
def get_user_profile(user_id: str) -&gt; str:
    &quot;&quot;&quot;Dynamic user data&quot;&quot;&quot;
    return f&quot;Profile data for user {user_id}&quot;
```

### Tools

Tools let LLMs take actions through your server. Unlike resources, tools are expected to perform computation and have side effects:

```python
import httpx
from mcp.server.fastmcp import FastMCP

mcp = FastMCP(&quot;My App&quot;)


@mcp.tool(title=&quot;BMI Calculator&quot;)
def calculate_bmi(weight_kg: float, height_m: float) -&gt; float:
    &quot;&quot;&quot;Calculate BMI given weight in kg and height in meters&quot;&quot;&quot;
    return weight_kg / (height_m**2)


@mcp.tool(title=&quot;Weather Fetcher&quot;)
async def fetch_weather(city: str) -&gt; str:
    &quot;&quot;&quot;Fetch current weather for a city&quot;&quot;&quot;
    async with httpx.AsyncClient() as client:
        response = await client.get(f&quot;https://api.weather.com/{city}&quot;)
        return response.text
```

### Prompts

Prompts are reusable templates that help LLMs interact with your server effectively:

```python
from mcp.server.fastmcp import FastMCP
from mcp.server.fastmcp.prompts import base

mcp = FastMCP(&quot;My App&quot;)


@mcp.prompt(title=&quot;Code Review&quot;)
def review_code(code: str) -&gt; str:
    return f&quot;Please review this code:\n\n{code}&quot;


@mcp.prompt(title=&quot;Debug Assistant&quot;)
def debug_error(error: str) -&gt; list[base.Message]:
    return [
        base.UserMessage(&quot;I&#039;m seeing this error:&quot;),
        base.UserMessage(error),
        base.AssistantMessage(&quot;I&#039;ll help debug that. What have you tried so far?&quot;),
    ]
```

### Images

FastMCP provides an `Image` class that automatically handles image data:

```python
from mcp.server.fastmcp import FastMCP, Image
from PIL import Image as PILImage

mcp = FastMCP(&quot;My App&quot;)


@mcp.tool()
def create_thumbnail(image_path: str) -&gt; Image:
    &quot;&quot;&quot;Create a thumbnail from an image&quot;&quot;&quot;
    img = PILImage.open(image_path)
    img.thumbnail((100, 100))
    return Image(data=img.tobytes(), format=&quot;png&quot;)
```

### Context

The Context object gives your tools and resources access to MCP capabilities:

```python
from mcp.server.fastmcp import FastMCP, Context

mcp = FastMCP(&quot;My App&quot;)


@mcp.tool()
async def long_task(files: list[str], ctx: Context) -&gt; str:
    &quot;&quot;&quot;Process multiple files with progress tracking&quot;&quot;&quot;
    for i, file in enumerate(files):
        ctx.info(f&quot;Processing {file}&quot;)
        await ctx.report_progress(i, len(files))
        data, mime_type = await ctx.read_resource(f&quot;file://{file}&quot;)
    return &quot;Processing complete&quot;
```

### Completions

MCP supports providing completion suggestions for prompt arguments and resource template parameters. With the context parameter, servers can provide completions based on previously resolved values:

Client usage:
```python
from mcp.client.session import ClientSession
from mcp.types import ResourceTemplateReference


async def use_completion(session: ClientSession):
    # Complete without context
    result = await session.complete(
        ref=ResourceTemplateReference(
            type=&quot;ref/resource&quot;, uri=&quot;github://repos/{owner}/{repo}&quot;
        ),
        argument={&quot;name&quot;: &quot;owner&quot;, &quot;value&quot;: &quot;model&quot;},
    )

    # Complete with context - repo suggestions based on owner
    result = await session.complete(
        ref=ResourceTemplateReference(
            type=&quot;ref/resource&quot;, uri=&quot;github://repos/{owner}/{repo}&quot;
        ),
        argument={&quot;name&quot;: &quot;repo&quot;, &quot;value&quot;: &quot;test&quot;},
        context_arguments={&quot;owner&quot;: &quot;modelcontextprotocol&quot;},
    )
```

Server implementation:
```python
from mcp.server import Server
from mcp.types import (
    Completion,
    CompletionArgument,
    CompletionContext,
    PromptReference,
    ResourceTemplateReference,
)

server = Server(&quot;example-server&quot;)


@server.completion()
async def handle_completion(
    ref: PromptReference | ResourceTemplateReference,
    argument: CompletionArgument,
    context: CompletionContext | None,
) -&gt; Completion | None:
    if isinstance(ref, ResourceTemplateReference):
        if ref.uri == &quot;github://repos/{owner}/{repo}&quot; and argument.name == &quot;repo&quot;:
            # Use context to provide owner-specific repos
            if context and context.arguments:
                owner = context.arguments.get(&quot;owner&quot;)
                if owner == &quot;modelcontextprotocol&quot;:
                    repos = [&quot;python-sdk&quot;, &quot;typescript-sdk&quot;, &quot;specification&quot;]
                    # Filter based on partial input
                    filtered = [r for r in repos if r.startswith(argument.value)]
                    return Completion(values=filtered)
    return None
```
### Elicitation

Request additional information from users during tool execution:

```python
from mcp.server.fastmcp import FastMCP, Context
from mcp.server.elicitation import (
    AcceptedElicitation,
    DeclinedElicitation,
    CancelledElicitation,
)
from pydantic import BaseModel, Field

mcp = FastMCP(&quot;Booking System&quot;)


@mcp.tool()
async def book_table(date: str, party_size: int, ctx: Context) -&gt; str:
    &quot;&quot;&quot;Book a table with confirmation&quot;&quot;&quot;

    # Schema must only contain primitive types (str, int, float, bool)
    class ConfirmBooking(BaseModel):
        confirm: bool = Field(description=&quot;Confirm booking?&quot;)
        notes: str = Field(default=&quot;&quot;, description=&quot;Special requests&quot;)

    result = await ctx.elicit(
        message=f&quot;Confirm booking for {party_size} on {date}?&quot;, schema=ConfirmBooking
    )

    match result:
        case AcceptedElicitation(data=data):
            if data.confirm:
                return f&quot;Booked! Notes: {data.notes or &#039;None&#039;}&quot;
            return &quot;Booking cancelled&quot;
        case DeclinedElicitation():
            return &quot;Booking declined&quot;
        case CancelledElicitation():
            return &quot;Booking cancelled&quot;
```

The `elicit()` method returns an `ElicitationResult` with:
- `action`: &quot;accept&quot;, &quot;decline&quot;, or &quot;cancel&quot;
- `data`: The validated response (only when accepted)
- `validation_error`: Any validation error message

### Authentication

Authentication can be used by servers that want to expose tools accessing protected resources.

`mcp.server.auth` implements an OAuth 2.0 server interface, which servers can use by
providing an implementation of the `OAuthAuthorizationServerProvider` protocol.

```python
from mcp import FastMCP
from mcp.server.auth.provider import OAuthAuthorizationServerProvider
from mcp.server.auth.settings import (
    AuthSettings,
    ClientRegistrationOptions,
    RevocationOptions,
)


class MyOAuthServerProvider(OAuthAuthorizationServerProvider):
    # See an example on how to implement at `examples/servers/simple-auth`
    ...


mcp = FastMCP(
    &quot;My App&quot;,
    auth_server_provider=MyOAuthServerProvider(),
    auth=AuthSettings(
        issuer_url=&quot;https://myapp.com&quot;,
        revocation_options=RevocationOptions(
            enabled=True,
        ),
        client_registration_options=ClientRegistrationOptions(
            enabled=True,
            valid_scopes=[&quot;myscope&quot;, &quot;myotherscope&quot;],
            default_scopes=[&quot;myscope&quot;],
        ),
        required_scopes=[&quot;myscope&quot;],
    ),
)
```

See [OAuthAuthorizationServerProvider](src/mcp/server/auth/provider.py) for more details.

## Running Your Server

### Development Mode

The fastest way to test and debug your server is with the MCP Inspector:

```bash
mcp dev server.py

# Add dependencies
mcp dev server.py --with pandas --with numpy

# Mount local code
mcp dev server.py --with-editable .
```

### Claude Desktop Integration

Once your server is ready, install it in Claude Desktop:

```bash
mcp install server.py

# Custom name
mcp install server.py --name &quot;My Analytics Server&quot;

# Environment variables
mcp install server.py -v API_KEY=abc123 -v DB_URL=postgres://...
mcp install server.py -f .env
```

### Direct Execution

For advanced scenarios like custom deployments:

```python
from mcp.server.fastmcp import FastMCP

mcp = FastMCP(&quot;My App&quot;)

if __name__ == &quot;__main__&quot;:
    mcp.run()
```

Run it with:
```bash
python server.py
# or
mcp run server.py
```

Note that `mcp run` or `mcp dev` only supports server using FastMCP and not the low-level server variant.

### Streamable HTTP Transport

&gt; **Note**: Streamable HTTP transport is superseding SSE transport for production deployments.

```python
from mcp.server.fastmcp import FastMCP

# Stateful server (maintains session state)
mcp = FastMCP(&quot;StatefulServer&quot;)

# Stateless server (no session persistence)
mcp = FastMCP(&quot;StatelessServer&quot;, stateless_http=True)

# Stateless server (no session persistence, no sse stream with supported client)
mcp = FastMCP(&quot;StatelessServer&quot;, stateless_http=True, json_response=True)

# Run server with streamable_http transport
mcp.run(transport=&quot;streamable-http&quot;)
```

You can mount multiple FastMCP servers in a FastAPI application:

```python
# echo.py
from mcp.server.fastmcp import FastMCP

mcp = FastMCP(name=&quot;EchoServer&quot;, stateless_http=True)


@mcp.tool(description=&quot;A simple echo tool&quot;)
def echo(message: str) -&gt; str:
    return f&quot;Echo: {message}&quot;
```

```python
# math.py
from mcp.server.fastmcp import FastMCP

mcp = FastMCP(name=&quot;MathServer&quot;, stateless_http=True)


@mcp.tool(description=&quot;A simple add tool&quot;)
def add_two(n: int) -&gt; int:
    return n + 2
```

```python
# main.py
import contextlib
from fastapi import FastAPI
from mcp.echo import echo
from mcp.math import math


# Create a combined lifespan to manage both session managers
@contextlib.asynccontextmanager
async def lifespan(app: FastAPI):
    async with contextlib.AsyncExitStack() as stack:
        await stack.enter_async_context(echo.mcp.session_manager.run())
        await stack.enter_async_context(math.mcp.session_manager.run())
        yield


app = FastAPI(lifespan=lifespan)
app.mount(&quot;/echo&quot;, echo.mcp.streamable_http_app())
app.mount(&quot;/math&quot;, math.mcp.streamable_http_app())
```

For low level server with Streamable HTTP implementations, see:
- Stateful server: [`examples/servers/simple-streamablehttp/`](examples/servers/simple-streamablehttp/)
- Stateless server: [`examples/servers/simple-streamablehttp-stateless/`](examples/servers/simple-streamablehttp-stateless/)

The streamable HTTP transport supports:
- Stateful and stateless operation modes
- Resumability with event stores
- JSON or SSE response formats
- Better scalability for multi-node deployments

### Mounting to an Existing ASGI Server

&gt; **Note**: SSE transport is being superseded by [Streamable HTTP transport](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#streamable-http).

By default, SSE servers are mounted at `/sse` and Streamable HTTP servers are mounted at `/mcp`. You can customize these paths using the methods described below.

You can mount the SSE server to an existing ASGI server using the `sse_app` method. This allows you to integrate the SSE server with other ASGI applications.

```python
from starlette.applications import Starlette
from starlette.routing import Mount, Host
from mcp.server.fastmcp import FastMCP


mcp = FastMCP(&quot;My App&quot;)

# Mount the SSE server to the existing ASGI server
app = Starlette(
    routes=[
        Mount(&#039;/&#039;, app=mcp.sse_app()),
    ]
)

# or dynamically mount as host
app.router.routes.append(Host(&#039;mcp.acme.corp&#039;, app=mcp.sse_app()))
```

When mounting multiple MCP servers under different paths, you can configure the mount path in several ways:

```python
from starlette.applications import Starlette
from starlette.routing import Mount
from mcp.server.fastmcp import FastMCP

# Create multiple MCP servers
github_mcp = FastMCP(&quot;GitHub API&quot;)
browser_mcp = FastMCP(&quot;Browser&quot;)
curl_mcp = FastMCP(&quot;Curl&quot;)
search_mcp = FastMCP(&quot;Search&quot;)

# Method 1: Configure mount paths via settings (recommended for persistent configuration)
github_mcp.settings.mount_path = &quot;/github&quot;
browser_mcp.settings.mount_path = &quot;/browser&quot;

# Method 2: Pass mount path directly to sse_app (preferred for ad-hoc mounting)
# This approach doesn&#039;t modify the server&#039;s settings permanently

# Create Starlette app with multiple mounted servers
app = Starlette(
    routes=[
        # Using settings-based configuration
        Mount(&quot;/github&quot;, app=github_mcp.sse_app()),
        Mount(&quot;/browser&quot;, app=browser_mcp.sse_app()),
        # Using direct mount path parameter
        Mount(&quot;/curl&quot;, app=curl_mcp.sse_app(&quot;/curl&quot;)),
        Mount(&quot;/search&quot;, app=search_mcp.sse_app(&quot;/search&quot;)),
    ]
)

# Method 3: For direct execution, you can also pass the mount path to run()
if __name__ == &quot;__main__&quot;:
    search_mcp.run(transport=&quot;sse&quot;, mount_path=&quot;/search&quot;)
```

For more information on mounting applications in Starlette, see the [Starlette documentation](https://www.starlette.io/routing/#submounting-routes).

## Examples

### Echo Server

A simple server demonstrating resources, tools, and prompts:

```python
from mcp.server.fastmcp import FastMCP

mcp = FastMCP(&quot;Echo&quot;)


@mcp.resource(&quot;echo://{message}&quot;)
def echo_resource(message: str) -&gt; str:
    &quot;&quot;&quot;Echo a message as a resource&quot;&quot;&quot;
    return f&quot;Resource echo: {message}&quot;


@mcp.tool()
def echo_tool(message: str) -&gt; str:
    &quot;&quot;&quot;Echo a message as a tool&quot;&quot;&quot;
    return f&quot;Tool echo: {message}&quot;


@mcp.prompt()
def echo_prompt(message: str) -&gt; str:
    &quot;&quot;&quot;Create an echo prompt&quot;&quot;&quot;
    return f&quot;Please process this message: {message}&quot;
```

### SQLite Explorer

A more complex example show

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[KoljaB/RealtimeSTT]]></title>
            <link>https://github.com/KoljaB/RealtimeSTT</link>
            <guid>https://github.com/KoljaB/RealtimeSTT</guid>
            <pubDate>Fri, 20 Jun 2025 00:04:39 GMT</pubDate>
            <description><![CDATA[A robust, efficient, low-latency speech-to-text library with advanced voice activity detection, wake word activation and instant transcription.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/KoljaB/RealtimeSTT">KoljaB/RealtimeSTT</a></h1>
            <p>A robust, efficient, low-latency speech-to-text library with advanced voice activity detection, wake word activation and instant transcription.</p>
            <p>Language: Python</p>
            <p>Stars: 7,691</p>
            <p>Forks: 632</p>
            <p>Stars today: 69 stars today</p>
            <h2>README</h2><pre># RealtimeSTT
[![PyPI](https://img.shields.io/pypi/v/RealtimeSTT)](https://pypi.org/project/RealtimeSTT/)
[![Downloads](https://static.pepy.tech/badge/RealtimeSTT)](https://www.pepy.tech/projects/realtimestt)
[![GitHub release](https://img.shields.io/github/release/KoljaB/RealtimeSTT.svg)](https://GitHub.com/KoljaB/RealtimeSTT/releases/)
[![GitHub commits](https://badgen.net/github/commits/KoljaB/RealtimeSTT)](https://GitHub.com/Naereen/KoljaB/RealtimeSTT/commit/)
[![GitHub forks](https://img.shields.io/github/forks/KoljaB/RealtimeSTT.svg?style=social&amp;label=Fork&amp;maxAge=2592000)](https://GitHub.com/KoljaB/RealtimeSTT/network/)
[![GitHub stars](https://img.shields.io/github/stars/KoljaB/RealtimeSTT.svg?style=social&amp;label=Star&amp;maxAge=2592000)](https://GitHub.com/KoljaB/RealtimeSTT/stargazers/)

*Easy-to-use, low-latency speech-to-text library for realtime applications*

## New

- AudioToTextRecorderClient class, which automatically starts a server if none is running and connects to it. The class shares the same interface as AudioToTextRecorder, making it easy to upgrade or switch between the two. (Work in progress, most parameters and callbacks of AudioToTextRecorder are already implemented into AudioToTextRecorderClient, but not all. Also the server can not handle concurrent (parallel) requests yet.)
- reworked CLI interface (&quot;stt-server&quot; to start the server, &quot;stt&quot; to start the client, look at &quot;server&quot; folder for more info)

## About the Project

RealtimeSTT listens to the microphone and transcribes voice into text.  

&gt; **Hint:** *&lt;strong&gt;Check out [Linguflex](https://github.com/KoljaB/Linguflex)&lt;/strong&gt;, the original project from which RealtimeSTT is spun off. It lets you control your environment by speaking and is one of the most capable and sophisticated open-source assistants currently available.*

It&#039;s ideal for:

- **Voice Assistants**
- Applications requiring **fast and precise** speech-to-text conversion

https://github.com/user-attachments/assets/797e6552-27cd-41b1-a7f3-e5cbc72094f5  

[CLI demo code (reproduces the video above)](tests/realtimestt_test.py)

### Updates

Latest Version: v0.3.104

See [release history](https://github.com/KoljaB/RealtimeSTT/releases).

&gt; **Hint:** *Since we use the `multiprocessing` module now, ensure to include the `if __name__ == &#039;__main__&#039;:` protection in your code to prevent unexpected behavior, especially on platforms like Windows. For a detailed explanation on why this is important, visit the [official Python documentation on `multiprocessing`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing-programming).*

## Quick Examples

### Print everything being said:

```python
from RealtimeSTT import AudioToTextRecorder

def process_text(text):
    print(text)

if __name__ == &#039;__main__&#039;:
    print(&quot;Wait until it says &#039;speak now&#039;&quot;)
    recorder = AudioToTextRecorder()

    while True:
        recorder.text(process_text)
```

### Type everything being said:

```python
from RealtimeSTT import AudioToTextRecorder
import pyautogui

def process_text(text):
    pyautogui.typewrite(text + &quot; &quot;)

if __name__ == &#039;__main__&#039;:
    print(&quot;Wait until it says &#039;speak now&#039;&quot;)
    recorder = AudioToTextRecorder()

    while True:
        recorder.text(process_text)
```
*Will type everything being said into your selected text box*

### Features

- **Voice Activity Detection**: Automatically detects when you start and stop speaking.
- **Realtime Transcription**: Transforms speech to text in real-time.
- **Wake Word Activation**: Can activate upon detecting a designated wake word.

&gt; **Hint**: *Check out [RealtimeTTS](https://github.com/KoljaB/RealtimeTTS), the output counterpart of this library, for text-to-voice capabilities. Together, they form a powerful realtime audio wrapper around large language models.*

## Tech Stack

This library uses:

- **Voice Activity Detection**
  - [WebRTCVAD](https://github.com/wiseman/py-webrtcvad) for initial voice activity detection.
  - [SileroVAD](https://github.com/snakers4/silero-vad) for more accurate verification.
- **Speech-To-Text**
  - [Faster_Whisper](https://github.com/guillaumekln/faster-whisper) for instant (GPU-accelerated) transcription.
- **Wake Word Detection**
  - [Porcupine](https://github.com/Picovoice/porcupine) or [OpenWakeWord](https://github.com/dscripka/openWakeWord) for wake word detection.


*These components represent the &quot;industry standard&quot; for cutting-edge applications, providing the most modern and effective foundation for building high-end solutions.*

## Installation

```bash
pip install RealtimeSTT
```

This will install all the necessary dependencies, including a **CPU support only** version of PyTorch.

Although it is possible to run RealtimeSTT with a CPU installation only (use a small model like &quot;tiny&quot; or &quot;base&quot; in this case) you will get way better experience using CUDA (please scroll down).

### Linux Installation

Before installing RealtimeSTT please execute:

```bash
sudo apt-get update
sudo apt-get install python3-dev
sudo apt-get install portaudio19-dev
```

### MacOS Installation

Before installing RealtimeSTT please execute:

```bash
brew install portaudio
```

### GPU Support with CUDA (recommended)

### Updating PyTorch for CUDA Support

To upgrade your PyTorch installation to enable GPU support with CUDA, follow these instructions based on your specific CUDA version. This is useful if you wish to enhance the performance of RealtimeSTT with CUDA capabilities.

#### For CUDA 11.8:
To update PyTorch and Torchaudio to support CUDA 11.8, use the following commands:

```bash
pip install torch==2.5.1+cu118 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu118
```

#### For CUDA 12.X:
To update PyTorch and Torchaudio to support CUDA 12.X, execute the following:

```bash
pip install torch==2.5.1+cu121 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121
```

Replace `2.5.1` with the version of PyTorch that matches your system and requirements.

### Steps That Might Be Necessary Before

&gt; **Note**: *To check if your NVIDIA GPU supports CUDA, visit the [official CUDA GPUs list](https://developer.nvidia.com/cuda-gpus).*

If you didn&#039;t use CUDA models before, some additional steps might be needed one time before installation. These steps prepare the system for CUDA support and installation of the **GPU-optimized** installation. This is recommended for those who require **better performance** and have a compatible NVIDIA GPU. To use RealtimeSTT with GPU support via CUDA please also follow these steps:

1. **Install NVIDIA CUDA Toolkit**:
    - select between CUDA 11.8 or CUDA 12.X Toolkit
        - for 12.X visit [NVIDIA CUDA Toolkit Archive](https://developer.nvidia.com/cuda-toolkit-archive) and select latest version.
        - for 11.8 visit [NVIDIA CUDA Toolkit 11.8](https://developer.nvidia.com/cuda-11-8-0-download-archive).
    - Select operating system and version.
    - Download and install the software.

2. **Install NVIDIA cuDNN**:
    - select between CUDA 11.8 or CUDA 12.X Toolkit
        - for 12.X visit [cuDNN Downloads](https://developer.nvidia.com/cudnn-downloads).
            - Select operating system and version.
            - Download and install the software.
        - for 11.8 visit [NVIDIA cuDNN Archive](https://developer.nvidia.com/rdp/cudnn-archive).
            - Click on &quot;Download cuDNN v8.7.0 (November 28th, 2022), for CUDA 11.x&quot;.
            - Download and install the software.
    
3. **Install ffmpeg**:

    &gt; **Note**: *Installation of ffmpeg might not actually be needed to operate RealtimeSTT* &lt;sup&gt; *thanks to jgilbert2017 for pointing this out&lt;/sup&gt;

    You can download an installer for your OS from the [ffmpeg Website](https://ffmpeg.org/download.html).  
    
    Or use a package manager:

    - **On Ubuntu or Debian**:
        ```bash
        sudo apt update &amp;&amp; sudo apt install ffmpeg
        ```

    - **On Arch Linux**:
        ```bash
        sudo pacman -S ffmpeg
        ```

    - **On MacOS using Homebrew** ([https://brew.sh/](https://brew.sh/)):
        ```bash
        brew install ffmpeg
        ```

    - **On Windows using Winget** [official documentation](https://learn.microsoft.com/en-us/windows/package-manager/winget/) :
        ```bash
        winget install Gyan.FFmpeg
        ```
        
    - **On Windows using Chocolatey** ([https://chocolatey.org/](https://chocolatey.org/)):
        ```bash
        choco install ffmpeg
        ```

    - **On Windows using Scoop** ([https://scoop.sh/](https://scoop.sh/)):
        ```bash
        scoop install ffmpeg
        ```    

## Quick Start

Basic usage:

### Manual Recording

Start and stop of recording are manually triggered.

```python
recorder.start()
recorder.stop()
print(recorder.text())
```

#### Standalone Example:

```python
from RealtimeSTT import AudioToTextRecorder

if __name__ == &#039;__main__&#039;:
    recorder = AudioToTextRecorder()
    recorder.start()
    input(&quot;Press Enter to stop recording...&quot;)
    recorder.stop()
    print(&quot;Transcription: &quot;, recorder.text())
```

### Automatic Recording

Recording based on voice activity detection.

```python
with AudioToTextRecorder() as recorder:
    print(recorder.text())
```

#### Standalone Example:

```python
from RealtimeSTT import AudioToTextRecorder

if __name__ == &#039;__main__&#039;:
    with AudioToTextRecorder() as recorder:
        print(&quot;Transcription: &quot;, recorder.text())
```

When running recorder.text in a loop it is recommended to use a callback, allowing the transcription to be run asynchronously:


```python
def process_text(text):
    print (text)
    
while True:
    recorder.text(process_text)
```

#### Standalone Example:

```python
from RealtimeSTT import AudioToTextRecorder

def process_text(text):
    print(text)

if __name__ == &#039;__main__&#039;:
    recorder = AudioToTextRecorder()

    while True:
        recorder.text(process_text)
```

### Wakewords

Keyword activation before detecting voice. Write the comma-separated list of your desired activation keywords into the wake_words parameter. You can choose wake words from these list: alexa, americano, blueberry, bumblebee, computer, grapefruits, grasshopper, hey google, hey siri, jarvis, ok google, picovoice, porcupine, terminator. 

```python
recorder = AudioToTextRecorder(wake_words=&quot;jarvis&quot;)

print(&#039;Say &quot;Jarvis&quot; then speak.&#039;)
print(recorder.text())
```

#### Standalone Example:

```python
from RealtimeSTT import AudioToTextRecorder

if __name__ == &#039;__main__&#039;:
    recorder = AudioToTextRecorder(wake_words=&quot;jarvis&quot;)

    print(&#039;Say &quot;Jarvis&quot; to start recording.&#039;)
    print(recorder.text())
```

### Callbacks

You can set callback functions to be executed on different events (see [Configuration](#configuration)) :

```python
def my_start_callback():
    print(&quot;Recording started!&quot;)

def my_stop_callback():
    print(&quot;Recording stopped!&quot;)

recorder = AudioToTextRecorder(on_recording_start=my_start_callback,
                               on_recording_stop=my_stop_callback)
```

#### Standalone Example:

```python
from RealtimeSTT import AudioToTextRecorder

def start_callback():
    print(&quot;Recording started!&quot;)

def stop_callback():
    print(&quot;Recording stopped!&quot;)

if __name__ == &#039;__main__&#039;:
    recorder = AudioToTextRecorder(on_recording_start=start_callback,
                                   on_recording_stop=stop_callback)
```

### Feed chunks

If you don&#039;t want to use the local microphone set use_microphone parameter to false and provide raw PCM audiochunks in 16-bit mono (samplerate 16000) with this method:

```python
recorder.feed_audio(audio_chunk)
```

#### Standalone Example:

```python
from RealtimeSTT import AudioToTextRecorder

if __name__ == &#039;__main__&#039;:
    recorder = AudioToTextRecorder(use_microphone=False)
    with open(&quot;audio_chunk.pcm&quot;, &quot;rb&quot;) as f:
        audio_chunk = f.read()

    recorder.feed_audio(audio_chunk)
    print(&quot;Transcription: &quot;, recorder.text())
```

### Shutdown

You can shutdown the recorder safely by using the context manager protocol:

```python
with AudioToTextRecorder() as recorder:
    [...]
```


Or you can call the shutdown method manually (if using &quot;with&quot; is not feasible):

```python
recorder.shutdown()
```

#### Standalone Example:

```python
from RealtimeSTT import AudioToTextRecorder

if __name__ == &#039;__main__&#039;:
    with AudioToTextRecorder() as recorder:
        [...]
    # or manually shutdown if &quot;with&quot; is not used
    recorder.shutdown()
```

## Testing the Library

The test subdirectory contains a set of scripts to help you evaluate and understand the capabilities of the RealtimeTTS library.

Test scripts depending on RealtimeTTS library may require you to enter your azure service region within the script. 
When using OpenAI-, Azure- or Elevenlabs-related demo scripts the API Keys should be provided in the environment variables OPENAI_API_KEY, AZURE_SPEECH_KEY and ELEVENLABS_API_KEY (see [RealtimeTTS](https://github.com/KoljaB/RealtimeTTS))

- **simple_test.py**
    - **Description**: A &quot;hello world&quot; styled demonstration of the library&#039;s simplest usage.

- **realtimestt_test.py**
    - **Description**: Showcasing live-transcription.

- **wakeword_test.py**
    - **Description**: A demonstration of the wakeword activation.

- **translator.py**
    - **Dependencies**: Run `pip install openai realtimetts`.
    - **Description**: Real-time translations into six different languages.

- **openai_voice_interface.py**
    - **Dependencies**: Run `pip install openai realtimetts`.
    - **Description**: Wake word activated and voice based user interface to the OpenAI API.

- **advanced_talk.py**
    - **Dependencies**: Run `pip install openai keyboard realtimetts`.
    - **Description**: Choose TTS engine and voice before starting AI conversation.

- **minimalistic_talkbot.py**
    - **Dependencies**: Run `pip install openai realtimetts`.
    - **Description**: A basic talkbot in 20 lines of code.

The example_app subdirectory contains a polished user interface application for the OpenAI API based on PyQt5.

## Configuration

### Initialization Parameters for `AudioToTextRecorder`

When you initialize the `AudioToTextRecorder` class, you have various options to customize its behavior.

#### General Parameters

- **model** (str, default=&quot;tiny&quot;): Model size or path for transcription.
    - Options: &#039;tiny&#039;, &#039;tiny.en&#039;, &#039;base&#039;, &#039;base.en&#039;, &#039;small&#039;, &#039;small.en&#039;, &#039;medium&#039;, &#039;medium.en&#039;, &#039;large-v1&#039;, &#039;large-v2&#039;.
    - Note: If a size is provided, the model will be downloaded from the Hugging Face Hub.

- **language** (str, default=&quot;&quot;): Language code for transcription. If left empty, the model will try to auto-detect the language. Supported language codes are listed in [Whisper Tokenizer library](https://github.com/openai/whisper/blob/main/whisper/tokenizer.py).

- **compute_type** (str, default=&quot;default&quot;): Specifies the type of computation to be used for transcription. See [Whisper Quantization](https://opennmt.net/CTranslate2/quantization.html)

- **input_device_index** (int, default=0): Audio Input Device Index to use.

- **gpu_device_index** (int, default=0): GPU Device Index to use. The model can also be loaded on multiple GPUs by passing a list of IDs (e.g. [0, 1, 2, 3]).

- **device** (str, default=&quot;cuda&quot;): Device for model to use. Can either be &quot;cuda&quot; or &quot;cpu&quot;. 

- **on_recording_start**: A callable function triggered when recording starts.

- **on_recording_stop**: A callable function triggered when recording ends.

- **on_transcription_start**: A callable function triggered when transcription starts.

- **ensure_sentence_starting_uppercase** (bool, default=True): Ensures that every sentence detected by the algorithm starts with an uppercase letter.

- **ensure_sentence_ends_with_period** (bool, default=True): Ensures that every sentence that doesn&#039;t end with punctuation such as &quot;?&quot;, &quot;!&quot; ends with a period

- **use_microphone** (bool, default=True): Usage of local microphone for transcription. Set to False if you want to provide chunks with feed_audio method.

- **spinner** (bool, default=True): Provides a spinner animation text with information about the current recorder state.

- **level** (int, default=logging.WARNING): Logging level.

- **batch_size** (int, default=16): Batch size for the main transcription. Set to 0 to deactivate.

- **init_logging** (bool, default=True): Whether to initialize the logging framework. Set to False to manage this yourself.

- **handle_buffer_overflow** (bool, default=True): If set, the system will log a warning when an input overflow occurs during recording and remove the data from the buffer.

- **beam_size** (int, default=5): The beam size to use for beam search decoding.

- **initial_prompt** (str or iterable of int, default=None): Initial prompt to be fed to the transcription models.

- **suppress_tokens** (list of int, default=[-1]): Tokens to be suppressed from the transcription output.

- **on_recorded_chunk**: A callback function that is triggered when a chunk of audio is recorded. Submits the chunk data as parameter.

- **debug_mode** (bool, default=False): If set, the system prints additional debug information to the console.

- **print_transcription_time** (bool, default=False): Logs the processing time of the main model transcription. This can be useful for performance monitoring and debugging.

- **early_transcription_on_silence** (int, default=0): If set, the system will transcribe audio faster when silence is detected. Transcription will start after the specified milliseconds. Keep this value lower than `post_speech_silence_duration`, ideally around `post_speech_silence_duration` minus the estimated transcription time with the main model. If silence lasts longer than `post_speech_silence_duration`, the recording is stopped, and the transcription is submitted. If voice activity resumes within this period, the transcription is discarded. This results in faster final transcriptions at the cost of additional GPU load due to some unnecessary final transcriptions.

- **allowed_latency_limit** (int, default=100): Specifies the maximum number of unprocessed chunks in the queue before discarding chunks. This helps prevent the system from being overwhelmed and losing responsiveness in real-time applications.

- **no_log_file** (bool, default=False): If set, the system will skip writing the debug log file, reducing disk I/O. Useful if logging to a file is not needed and performance is a priority.

- **start_callback_in_new_thread** (bool, default=False): If set, the system will create a new thread for all callback functions. This can be useful if the callback function is blocking and you want to avoid blocking the realtimestt application thread. 

#### Real-time Transcription Parameters

&gt; **Note**: *When enabling realtime description a GPU installation is strongly advised. Using realtime transcription may create high GPU loads.*

- **enable_realtime_transcription** (bool, default=False): Enables or disables real-time transcription of audio. When set to True, the audio will be transcribed continuously as it is being recorded.

- **use_main_model_for_realtime** (bool, default=False): If set to True, the main transcription model will be used for both regular and real-time transcription. If False, a separate model specified by `realtime_model_type` will be used for real-time transcription. Using a single model can save memory and potentially improve performance, but may not be optimized for real-time processing. Using separate models allows for a smaller, faster model for real-time transcription while keeping a more accurate model for final transcription.

- **realtime_model_type** (str, default=&quot;tiny&quot;): Specifies the size or path of the machine learning model to be used for real-time transcription.
    - Valid options: &#039;tiny&#039;, &#039;tiny.en&#039;, &#039;base&#039;, &#039;base.en&#039;, &#039;small&#039;, &#039;small.en&#039;, &#039;medium&#039;, &#039;medium.en&#039;, &#039;large-v1&#039;, &#039;large-v2&#039;.

- **realtime_processing_pause** (float, default=0.2): Specifies the time interv

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[polarsource/polar]]></title>
            <link>https://github.com/polarsource/polar</link>
            <guid>https://github.com/polarsource/polar</guid>
            <pubDate>Fri, 20 Jun 2025 00:04:38 GMT</pubDate>
            <description><![CDATA[An open source engine for your digital products. Sell SaaS and digital products in minutes.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/polarsource/polar">polarsource/polar</a></h1>
            <p>An open source engine for your digital products. Sell SaaS and digital products in minutes.</p>
            <p>Language: Python</p>
            <p>Stars: 5,548</p>
            <p>Forks: 319</p>
            <p>Stars today: 35 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;

  &lt;a href=&quot;https://polar.sh&quot;&gt;
      &lt;img src=&quot;https://github.com/user-attachments/assets/89a588e5-0c58-429a-8bbe-20f70af41372&quot; /&gt;
  &lt;/a&gt;

&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://www.producthunt.com/posts/polar-5?embed=true&amp;utm_source=badge-top-post-badge&amp;utm_medium=badge&amp;utm_souce=badge-polar&amp;#0045;5&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=484271&amp;theme=dark&amp;period=daily&quot; alt=&quot;Polar - An&amp;#0032;open&amp;#0032;source&amp;#0032;monetization&amp;#0032;platform&amp;#0032;for&amp;#0032;developers | Product Hunt&quot; style=&quot;width: 250px; height: 54px;&quot; width=&quot;250&quot; height=&quot;54&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.producthunt.com/posts/polar-5?embed=true&amp;utm_source=badge-top-post-topic-badge&amp;utm_medium=badge&amp;utm_souce=badge-polar&amp;#0045;5&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://api.producthunt.com/widgets/embed-image/v1/top-post-topic-badge.svg?post_id=484271&amp;theme=dark&amp;period=monthly&amp;topic_id=267&quot; alt=&quot;Polar - An&amp;#0032;open&amp;#0032;source&amp;#0032;monetization&amp;#0032;platform&amp;#0032;for&amp;#0032;developers | Product Hunt&quot; style=&quot;width: 250px; height: 54px;&quot; width=&quot;250&quot; height=&quot;54&quot; /&gt;&lt;/a&gt;

&lt;/div&gt;

&lt;hr /&gt;
&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://polar.sh&quot;&gt;Website&lt;/a&gt;
&lt;span&gt;&amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&lt;/span&gt;
&lt;a href=&quot;https://polar.sh/blog&quot;&gt;Blog&lt;/a&gt;
&lt;span&gt;&amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&lt;/span&gt;
&lt;a href=&quot;https://polar.sh/docs&quot;&gt;Docs&lt;/a&gt;
&lt;span&gt;&amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&lt;/span&gt;
&lt;a href=&quot;https://docs.polar.sh/api-reference&quot;&gt;API Reference&lt;/a&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://discord.gg/Pnhfz3UThd&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/chat-on%20discord-7289DA.svg&quot; alt=&quot;Discord Chat&quot; /&gt;
  &lt;/a&gt;

  &lt;a href=&quot;https://twitter.com/intent/follow?screen_name=polar_sh&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/polar_sh.svg?label=Follow%20@polar_sh&quot; alt=&quot;Follow @polar_sh&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;hr /&gt;

## Polar: Open Source payments infrastructure for the 21st century

Focus on building your passion, while we focus on the infrastructure to get you paid.

- Sell SaaS and digital products in minutes
- All-in-one funding &amp; monetization platform for developers.
- Sell access to GitHub repositories, Discord Support channels, File Downloads, License Keys &amp; much more with Digital Products &amp; Subscriptions.
- We&#039;re the merchant of record handling the...
    - ...boilerplate (billing, receipts, customer accounts etc)
    - ...headaches (sales tax, VAT)

## Pricing

- 4% + 40¬¢
- No fixed monthly costs
- Additional fees may apply. [Read more](https://docs.polar.sh/documentation/polar-as-merchant-of-record/fees)

## Roadmap, Issues &amp; Feature Requests

**üéØ Upcoming milestones.** [Check out what we&#039;re building towards](https://github.com/polarsource/polar/issues/3242)

**üí¨ Shape the future of Polar with us.** [Join our Discord](https://discord.gg/Pnhfz3UThd)

**üêõ Found a bug?** [Submit it here](https://github.com/polarsource/polar/issues)

**üîì Found a security vulnerability?** We greatly appreciate responsible and private disclosures. See [Security](./SECURITY.md)

### Polar API &amp; SDK

You can integrate Polar on your docs, sites or services using our [Public API](https://docs.polar.sh/api-reference) and [Webhook API](https://docs.polar.sh/developers/webhooks).

We also maintain SDKs for the following languages:

- JavaScript (Node.js and browsers): [polarsource/polar-js](https://github.com/polarsource/polar-js)
- Python: [polarsource/polar-python](https://github.com/polarsource/polar-python)

## Contributions

Our [`DEVELOPMENT.md`](./DEVELOPMENT.md) file contains everything you need to know to configure your development environment.

&gt; [!TIP]
&gt; Want to get started quickly? Use GitHub Codespaces.
&gt;
&gt; [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/polarsource/polar?machine=standardLinux32gb)

### Contributors

&lt;a href=&quot;https://github.com/polarsource/polar/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=polarsource/polar&quot; /&gt;
&lt;/a&gt;

## Monorepo

- **[server](./server/README.md)** ‚Äì Python / FastAPI / Dramatiq / SQLAlchemy (PostgreSQL) / Redis
- **[clients](./clients/README.md)** ‚Äì Turborepo
    - [web](./clients/apps/web) (Dashboard) ‚Äì NextJS (TypeScript)
    - [polarkit](./clients/packages/polarkit) - Shared React components

&lt;sub&gt;‚ô•Ô∏èüôè To our `pyproject.toml` friends: [FastAPI](https://github.com/tiangolo/fastapi), [Pydantic](https://github.com/pydantic/pydantic), [Dramatiq](https://github.com/Bogdanp/dramatiq), [SQLAlchemy](https://github.com/sqlalchemy/sqlalchemy), [Githubkit](https://github.com/yanyongyu/githubkit), [sse-starlette](https://github.com/sysid/sse-starlette), [Uvicorn](https://github.com/encode/uvicorn), [httpx-oauth](https://github.com/frankie567/httpx-oauth), [jinja](https://github.com/pallets/jinja), [blinker](https://github.com/pallets-eco/blinker), [pyjwt](https://github.com/jpadilla/pyjwt), [Sentry](https://github.com/getsentry/sentry) + more&lt;/sub&gt;&lt;br /&gt;
&lt;sub&gt;‚ô•Ô∏èüôè To our `package.json` friends: [Next.js](https://github.com/vercel/next.js/), [TanStack Query](https://github.com/TanStack/query), [tailwindcss](https://github.com/tailwindlabs/tailwindcss), [zustand](https://github.com/pmndrs/zustand), [openapi-typescript-codegen](https://github.com/ferdikoomen/openapi-typescript-codegen), [axios](https://github.com/axios/axios), [radix-ui](https://github.com/radix-ui/primitives), [cmdk](https://github.com/pacocoursey/cmdk), [framer-motion](https://github.com/framer/motion) + more&lt;/sub&gt;&lt;br /&gt;
&lt;sub&gt;‚ô•Ô∏èüôè To [IPinfo](https://ipinfo.io) that provides IP address data to help us geolocate customers during checkout.&lt;/sub&gt;

## License

Licensed under [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[All-Hands-AI/OpenHands]]></title>
            <link>https://github.com/All-Hands-AI/OpenHands</link>
            <guid>https://github.com/All-Hands-AI/OpenHands</guid>
            <pubDate>Fri, 20 Jun 2025 00:04:37 GMT</pubDate>
            <description><![CDATA[üôå OpenHands: Code Less, Make More]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/All-Hands-AI/OpenHands">All-Hands-AI/OpenHands</a></h1>
            <p>üôå OpenHands: Code Less, Make More</p>
            <p>Language: Python</p>
            <p>Stars: 58,664</p>
            <p>Forks: 6,754</p>
            <p>Stars today: 170 stars today</p>
            <h2>README</h2><pre>&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./docs/static/img/logo.png&quot; alt=&quot;Logo&quot; width=&quot;200&quot;&gt;
  &lt;h1 align=&quot;center&quot;&gt;OpenHands: Code Less, Make More&lt;/h1&gt;
&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/All-Hands-AI/OpenHands/graphs/contributors&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/All-Hands-AI/OpenHands?style=for-the-badge&amp;color=blue&quot; alt=&quot;Contributors&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/All-Hands-AI/OpenHands/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/All-Hands-AI/OpenHands?style=for-the-badge&amp;color=blue&quot; alt=&quot;Stargazers&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/All-Hands-AI/OpenHands/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/All-Hands-AI/OpenHands?style=for-the-badge&amp;color=blue&quot; alt=&quot;MIT License&quot;&gt;&lt;/a&gt;
  &lt;br/&gt;
  &lt;a href=&quot;https://join.slack.com/t/openhands-ai/shared_invite/zt-34zm4j0gj-Qz5kRHoca8DFCbqXPS~f_A&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Slack-Join%20Us-red?logo=slack&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Join our Slack community&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/ESHStjSjD4&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Join our Discord community&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/All-Hands-AI/OpenHands/blob/main/CREDITS.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Project-Credits-blue?style=for-the-badge&amp;color=FFE165&amp;logo=github&amp;logoColor=white&quot; alt=&quot;Credits&quot;&gt;&lt;/a&gt;
  &lt;br/&gt;
  &lt;a href=&quot;https://docs.all-hands.dev/usage/getting-started&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;logoColor=FFE165&amp;style=for-the-badge&quot; alt=&quot;Check out the documentation&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://arxiv.org/abs/2407.16741&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Paper%20on%20Arxiv-000?logoColor=FFE165&amp;logo=arxiv&amp;style=for-the-badge&quot; alt=&quot;Paper on Arxiv&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1wOUdFCMyY6Nt0AIqF705KN4JKOWgeI4wUGUP60krXXs/edit?gid=0#gid=0&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Benchmark%20score-000?logoColor=FFE165&amp;logo=huggingface&amp;style=for-the-badge&quot; alt=&quot;Evaluation Benchmark Score&quot;&gt;&lt;/a&gt;

  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=de&quot;&gt;Deutsch&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=es&quot;&gt;Espa√±ol&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=fr&quot;&gt;fran√ßais&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=ja&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=ko&quot;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=pt&quot;&gt;Portugu√™s&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=ru&quot;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=zh&quot;&gt;‰∏≠Êñá&lt;/a&gt;

  &lt;hr&gt;
&lt;/div&gt;

Welcome to OpenHands (formerly OpenDevin), a platform for software development agents powered by AI.

OpenHands agents can do anything a human developer can: modify code, run commands, browse the web,
call APIs, and yes‚Äîeven copy code snippets from StackOverflow.

Learn more at [docs.all-hands.dev](https://docs.all-hands.dev), or [sign up for OpenHands Cloud](https://app.all-hands.dev) to get started.

&gt; [!IMPORTANT]
&gt; Using OpenHands for work? We&#039;d love to chat! Fill out
&gt; [this short form](https://docs.google.com/forms/d/e/1FAIpQLSet3VbGaz8z32gW9Wm-Grl4jpt5WgMXPgJ4EDPVmCETCBpJtQ/viewform)
&gt; to join our Design Partner program, where you&#039;ll get early access to commercial features and the opportunity to provide input on our product roadmap.

![App screenshot](./docs/static/img/screenshot.png)

## ‚òÅÔ∏è OpenHands Cloud
The easiest way to get started with OpenHands is on [OpenHands Cloud](https://app.all-hands.dev),
which comes with $50 in free credits for new users.

## üíª Running OpenHands Locally

OpenHands can also run on your local system using Docker.
See the [Running OpenHands](https://docs.all-hands.dev/usage/installation) guide for
system requirements and more information.

&gt; [!WARNING]
&gt; On a public network? See our [Hardened Docker Installation Guide](https://docs.all-hands.dev/usage/runtimes/docker#hardened-docker-installation)
&gt; to secure your deployment by restricting network binding and implementing additional security measures.


```bash
docker pull docker.all-hands.dev/all-hands-ai/runtime:0.44-nikolaik

docker run -it --rm --pull=always \
    -e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.44-nikolaik \
    -e LOG_ALL_EVENTS=true \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -v ~/.openhands:/.openhands \
    -p 3000:3000 \
    --add-host host.docker.internal:host-gateway \
    --name openhands-app \
    docker.all-hands.dev/all-hands-ai/openhands:0.44
```

&gt; **Note**: If you used OpenHands before version 0.44, you may want to run `mv ~/.openhands-state ~/.openhands` to migrate your conversation history to the new location.

You&#039;ll find OpenHands running at [http://localhost:3000](http://localhost:3000)!

When you open the application, you&#039;ll be asked to choose an LLM provider and add an API key.
[Anthropic&#039;s Claude Sonnet 4](https://www.anthropic.com/api) (`anthropic/claude-sonnet-4-20250514`)
works best, but you have [many options](https://docs.all-hands.dev/usage/llms).

## üí° Other ways to run OpenHands

&gt; [!CAUTION]
&gt; OpenHands is meant to be run by a single user on their local workstation.
&gt; It is not appropriate for multi-tenant deployments where multiple users share the same instance. There is no built-in authentication, isolation, or scalability.
&gt;
&gt; If you&#039;re interested in running OpenHands in a multi-tenant environment, please
&gt; [get in touch with us](https://docs.google.com/forms/d/e/1FAIpQLSet3VbGaz8z32gW9Wm-Grl4jpt5WgMXPgJ4EDPVmCETCBpJtQ/viewform)
&gt; for advanced deployment options.

You can also [connect OpenHands to your local filesystem](https://docs.all-hands.dev/usage/runtimes/docker#connecting-to-your-filesystem),
run OpenHands in a scriptable [headless mode](https://docs.all-hands.dev/usage/how-to/headless-mode),
interact with it via a [friendly CLI](https://docs.all-hands.dev/usage/how-to/cli-mode),
or run it on tagged issues with [a github action](https://docs.all-hands.dev/usage/how-to/github-action).

Visit [Running OpenHands](https://docs.all-hands.dev/usage/installation) for more information and setup instructions.

If you want to modify the OpenHands source code, check out [Development.md](https://github.com/All-Hands-AI/OpenHands/blob/main/Development.md).

Having issues? The [Troubleshooting Guide](https://docs.all-hands.dev/usage/troubleshooting) can help.

## üìñ Documentation
  &lt;a href=&quot;https://deepwiki.com/All-Hands-AI/OpenHands&quot;&gt;&lt;img src=&quot;https://deepwiki.com/badge.svg&quot; alt=&quot;Ask DeepWiki&quot; title=&quot;Autogenerated Documentation by DeepWiki&quot;&gt;&lt;/a&gt;

To learn more about the project, and for tips on using OpenHands,
check out our [documentation](https://docs.all-hands.dev/usage/getting-started).

There you&#039;ll find resources on how to use different LLM providers,
troubleshooting resources, and advanced configuration options.

## ü§ù How to Join the Community

OpenHands is a community-driven project, and we welcome contributions from everyone. We do most of our communication
through Slack, so this is the best place to start, but we also are happy to have you contact us on Discord or Github:

- [Join our Slack workspace](https://join.slack.com/t/openhands-ai/shared_invite/zt-34zm4j0gj-Qz5kRHoca8DFCbqXPS~f_A) - Here we talk about research, architecture, and future development.
- [Join our Discord server](https://discord.gg/ESHStjSjD4) - This is a community-run server for general discussion, questions, and feedback.
- [Read or post Github Issues](https://github.com/All-Hands-AI/OpenHands/issues) - Check out the issues we&#039;re working on, or add your own ideas.

See more about the community in [COMMUNITY.md](./COMMUNITY.md) or find details on contributing in [CONTRIBUTING.md](./CONTRIBUTING.md).

## üìà Progress

See the monthly OpenHands roadmap [here](https://github.com/orgs/All-Hands-AI/projects/1) (updated at the maintainer&#039;s meeting at the end of each month).

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://star-history.com/#All-Hands-AI/OpenHands&amp;Date&quot;&gt;
    &lt;img src=&quot;https://api.star-history.com/svg?repos=All-Hands-AI/OpenHands&amp;type=Date&quot; width=&quot;500&quot; alt=&quot;Star History Chart&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

## üìú License

Distributed under the MIT License. See [`LICENSE`](./LICENSE) for more information.

## üôè Acknowledgements

OpenHands is built by a large number of contributors, and every contribution is greatly appreciated! We also build upon other open source projects, and we are deeply thankful for their work.

For a list of open source projects and licenses used in OpenHands, please see our [CREDITS.md](./CREDITS.md) file.

## üìö Cite

```
@inproceedings{
  wang2025openhands,
  title={OpenHands: An Open Platform for {AI} Software Developers as Generalist Agents},
  author={Xingyao Wang and Boxuan Li and Yufan Song and Frank F. Xu and Xiangru Tang and Mingchen Zhuge and Jiayi Pan and Yueqi Song and Bowen Li and Jaskirat Singh and Hoang H. Tran and Fuqiang Li and Ren Ma and Mingzhang Zheng and Bill Qian and Yanjun Shao and Niklas Muennighoff and Yizhe Zhang and Binyuan Hui and Junyang Lin and Robert Brennan and Hao Peng and Heng Ji and Graham Neubig},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=OJd3ayDDoF}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[comet-ml/opik]]></title>
            <link>https://github.com/comet-ml/opik</link>
            <guid>https://github.com/comet-ml/opik</guid>
            <pubDate>Fri, 20 Jun 2025 00:04:36 GMT</pubDate>
            <description><![CDATA[Debug, evaluate, and monitor your LLM applications, RAG systems, and agentic workflows with comprehensive tracing, automated evaluations, and production-ready dashboards.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/comet-ml/opik">comet-ml/opik</a></h1>
            <p>Debug, evaluate, and monitor your LLM applications, RAG systems, and agentic workflows with comprehensive tracing, automated evaluations, and production-ready dashboards.</p>
            <p>Language: Python</p>
            <p>Stars: 9,948</p>
            <p>Forks: 677</p>
            <p>Stars today: 67 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;&lt;b&gt;&lt;a href=&quot;README.md&quot;&gt;English&lt;/a&gt; | &lt;a href=&quot;readme_CN.md&quot;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | &lt;a href=&quot;readme_JP.md&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href=&quot;readme_KO.md&quot;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt;&lt;/b&gt;&lt;/div&gt;

&lt;h1 align=&quot;center&quot; style=&quot;border-bottom: none&quot;&gt;
    &lt;div&gt;
        &lt;a href=&quot;https://www.comet.com/site/products/opik/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=header_img&amp;utm_campaign=opik&quot;&gt;&lt;picture&gt;
            &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/comet-ml/opik/refs/heads/main/apps/opik-documentation/documentation/static/img/logo-dark-mode.svg&quot;&gt;
            &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/comet-ml/opik/refs/heads/main/apps/opik-documentation/documentation/static/img/opik-logo.svg&quot;&gt;
            &lt;img alt=&quot;Comet Opik logo&quot; src=&quot;https://raw.githubusercontent.com/comet-ml/opik/refs/heads/main/apps/opik-documentation/documentation/static/img/opik-logo.svg&quot; width=&quot;200&quot; /&gt;
        &lt;/picture&gt;&lt;/a&gt;
        &lt;br&gt;
        Opik
    &lt;/div&gt;
&lt;/h1&gt;
&lt;h2 align=&quot;center&quot; style=&quot;border-bottom: none&quot;&gt;Open-source LLM evaluation platform&lt;/h2&gt;
&lt;p align=&quot;center&quot;&gt;
Opik helps you build, evaluate, and optimize LLM systems that run better, faster, and cheaper. From RAG chatbots to code assistants to complex agentic pipelines, Opik provides comprehensive tracing, evaluations, dashboards, and powerful features like &lt;b&gt;Opik Agent Optimizer&lt;/b&gt; and &lt;b&gt;Opik Guardrails&lt;/b&gt; to improve and secure your LLM powered applications in production.
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

[![Python SDK](https://img.shields.io/pypi/v/opik)](https://pypi.org/project/opik/)
[![License](https://img.shields.io/github/license/comet-ml/opik)](https://github.com/comet-ml/opik/blob/main/LICENSE)
[![Build](https://github.com/comet-ml/opik/actions/workflows/build_apps.yml/badge.svg)](https://github.com/comet-ml/opik/actions/workflows/build_apps.yml)
[![Bounties](https://img.shields.io/endpoint?url=https%3A%2F%2Falgora.io%2Fapi%2Fshields%2Fcomet-ml%2Fbounties%3Fstatus%3Dopen)](https://algora.io/comet-ml/bounties?status=open)
&lt;!-- [![Quick Start](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/opik_quickstart.ipynb) --&gt;

&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://www.comet.com/site/products/opik/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=website_button&amp;utm_campaign=opik&quot;&gt;&lt;b&gt;Website&lt;/b&gt;&lt;/a&gt; ‚Ä¢
    &lt;a href=&quot;https://chat.comet.com&quot;&gt;&lt;b&gt;Slack Community&lt;/b&gt;&lt;/a&gt; ‚Ä¢
    &lt;a href=&quot;https://x.com/Cometml&quot;&gt;&lt;b&gt;Twitter&lt;/b&gt;&lt;/a&gt; ‚Ä¢
    &lt;a href=&quot;https://www.comet.com/docs/opik/changelog&quot;&gt;&lt;b&gt;Changelog&lt;/b&gt;&lt;/a&gt; ‚Ä¢
    &lt;a href=&quot;https://www.comet.com/docs/opik/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=docs_button&amp;utm_campaign=opik&quot;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot; style=&quot;margin-top: 1em; margin-bottom: 1em;&quot;&gt;
&lt;a href=&quot;#-what-is-opik&quot;&gt;üöÄ What is Opik?&lt;/a&gt; ‚Ä¢ &lt;a href=&quot;#%EF%B8%8F-opik-server-installation&quot;&gt;üõ†Ô∏è Opik Server Installation&lt;/a&gt; ‚Ä¢ &lt;a href=&quot;#-opik-client-sdk&quot;&gt;üíª Opik Client SDK&lt;/a&gt; ‚Ä¢ &lt;a href=&quot;#-logging-traces-with-integrations&quot;&gt;üìù Logging Traces&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;#-llm-as-a-judge-metrics&quot;&gt;üßë‚Äç‚öñÔ∏è LLM as a Judge&lt;/a&gt; ‚Ä¢ &lt;a href=&quot;#-evaluating-your-llm-application&quot;&gt;üîç Evaluating your Application&lt;/a&gt; ‚Ä¢ &lt;a href=&quot;#-star-us-on-github&quot;&gt;‚≠ê Star Us&lt;/a&gt; ‚Ä¢ &lt;a href=&quot;#-contributing&quot;&gt;ü§ù Contributing&lt;/a&gt;
&lt;/div&gt;

&lt;br&gt;

[![Opik platform screenshot (thumbnail)](readme-thumbnail-new.png)](https://www.comet.com/signup?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=readme_banner&amp;utm_campaign=opik)

## üöÄ What is Opik?

Opik (built by [Comet](https://www.comet.com?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=what_is_opik_link&amp;utm_campaign=opik)) is an open-source platform designed to streamline the entire lifecycle of LLM applications. It empowers developers to evaluate, test, monitor, and optimize their models and agentic systems. Key offerings include:
* **Comprehensive Observability**: Deep tracing of LLM calls, conversation logging, and agent activity.
* **Advanced Evaluation**: Robust prompt evaluation, LLM-as-a-judge, and experiment management.
* **Production-Ready**: Scalable monitoring dashboards and online evaluation rules for production.
* **Opik Agent Optimizer**: Dedicated SDK and set of optimizers to enhance prompts and agents.
* **Opik Guardrails**: Features to help you implement safe and responsible AI practices.

&lt;br&gt;

Key capabilities include:
* **Development &amp; Tracing:**
    * Track all LLM calls and traces with detailed context during development and in production ([Quickstart](https://www.comet.com/docs/opik/quickstart/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=quickstart_link&amp;utm_campaign=opik)).
    * Extensive 3rd-party integrations for easy observability: Seamlessly integrate with a growing list of frameworks, supporting many of the largest and most popular ones natively (including recent additions like **Google ADK**, **Autogen**, and **Flowise AI**). ([Integrations](https://www.comet.com/docs/opik/tracing/integrations/overview/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=integrations_link&amp;utm_campaign=opik))
    * Annotate traces and spans with feedback scores via the [Python SDK](https://www.comet.com/docs/opik/tracing/annotate_traces/#annotating-traces-and-spans-using-the-sdk?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=sdk_link&amp;utm_campaign=opik) or the [UI](https://www.comet.com/docs/opik/tracing/annotate_traces/#annotating-traces-through-the-ui?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=ui_link&amp;utm_campaign=opik).
    * Experiment with prompts and models in the [Prompt Playground](https://www.comet.com/docs/opik/prompt_engineering/playground).

* **Evaluation &amp; Testing**:
    * Automate your LLM application evaluation with [Datasets](https://www.comet.com/docs/opik/evaluation/manage_datasets/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=datasets_link&amp;utm_campaign=opik) and [Experiments](https://www.comet.com/docs/opik/evaluation/evaluate_your_llm/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=eval_link&amp;utm_campaign=opik).
    * Leverage powerful LLM-as-a-judge metrics for complex tasks like [hallucination detection](https://www.comet.com/docs/opik/evaluation/metrics/hallucination/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=hallucination_link&amp;utm_campaign=opik), [moderation](https://www.comet.com/docs/opik/evaluation/metrics/moderation/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=moderation_link&amp;utm_campaign=opik), and RAG assessment ([Answer Relevance](https://www.comet.com/docs/opik/evaluation/metrics/answer_relevance/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=alex_link&amp;utm_campaign=opik), [Context Precision](https://www.comet.com/docs/opik/evaluation/metrics/context_precision/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=context_link&amp;utm_campaign=opik)).
    * Integrate evaluations into your CI/CD pipeline with our [PyTest integration](https://www.comet.com/docs/opik/testing/pytest_integration/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=pytest_link&amp;utm_campaign=opik).

* **Production Monitoring &amp; Optimization**:
    * Log high volumes of production traces: Opik is designed for scale (40M+ traces/day).
    * Monitor feedback scores, trace counts, and token usage over time in the [Opik Dashboard](https://www.comet.com/docs/opik/production/production_monitoring/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=dashboard_link&amp;utm_campaign=opik).
    * Utilize [Online Evaluation Rules](https://www.comet.com/docs/opik/production/rules/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=dashboard_link&amp;utm_campaign=opik) with LLM-as-a-Judge metrics to identify production issues.
    * Leverage **Opik Agent Optimizer** and **Opik Guardrails** to continuously improve and secure your LLM applications in production.

&gt; [!TIP]
&gt; If you are looking for features that Opik doesn&#039;t have today, please raise a new [Feature request](https://github.com/comet-ml/opik/issues/new/choose) üöÄ

&lt;br&gt;

## üõ†Ô∏è Opik Server Installation

Get your Opik server running in minutes. Choose the option that best suits your needs:

### Option 1: Comet.com Cloud (Easiest &amp; Recommended)
Access Opik instantly without any setup. Ideal for quick starts and hassle-free maintenance.

üëâ [Create your free Comet account](https://www.comet.com/signup?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=install_create_link&amp;utm_campaign=opik)

### Option 2: Self-Host Opik for Full Control
Deploy Opik in your own environment. Choose between Docker for local setups or Kubernetes for scalability.

#### Self-Hosting with Docker Compose (for Local Development &amp; Testing)
This is the simplest way to get a local Opik instance running. Note the new `.opik.sh` installation script:

On Linux or Mac Enviroment:
```bash
# Clone the Opik repository
git clone https://github.com/comet-ml/opik.git

# Navigate to the repository
cd opik

# Start the Opik platform
./opik.sh
```

On Windows Enviroment:
```powershell
# Clone the Opik repository
git clone https://github.com/comet-ml/opik.git

# Navigate to the repository
cd opik

# Start the Opik platform
powershell -ExecutionPolicy ByPass -c &quot;.\\opik.ps1&quot;
```

Use the `--help` or `--info` options to troubleshoot issues. Dockerfiles now ensure containers run as non-root users for enhanced security. Once all is up and running, you can now visit [localhost:5173](http://localhost:5173) on your browser! For detailed instructions, see the [Local Deployment Guide](https://www.comet.com/docs/opik/self-host/local_deployment?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=self_host_link&amp;utm_campaign=opik).

#### Self-Hosting with Kubernetes &amp; Helm (for Scalable Deployments)
For production or larger-scale self-hosted deployments, Opik can be installed on a Kubernetes cluster using our Helm chart. Click the badge for the full [Kubernetes Installation Guide using Helm](https://www.comet.com/docs/opik/self-host/kubernetes/#kubernetes-installation?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=kubernetes_link&amp;utm_campaign=opik).

[![Kubernetes](https://img.shields.io/badge/Kubernetes-%23326ce5.svg?&amp;logo=kubernetes&amp;logoColor=white)](https://www.comet.com/docs/opik/self-host/kubernetes/#kubernetes-installation?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=kubernetes_link&amp;utm_campaign=opik)

&gt; [!IMPORTANT]
&gt; **Version 1.7.0 Changes**: Please check the [changelog](https://github.com/comet-ml/opik/blob/main/CHANGELOG.md) for important updates and breaking changes.

## üíª Opik Client SDK

Opik provides a suite of client libraries and a REST API to interact with the Opik server. This includes SDKs for Python, TypeScript, and Ruby (via OpenTelemetry), allowing for seamless integration into your workflows. For detailed API and SDK references, see the [Opik Client Reference Documentation](apps/opik-documentation/documentation/fern/docs/reference/overview.mdx).

### Python SDK Quick Start
To get started with the Python SDK:

Install the package:
```bash
# install using pip
pip install opik

# or install with uv
uv pip install opik
```

Configure the python SDK by running the `opik configure` command, which will prompt you for your Opik server address (for self-hosted instances) or your API key and workspace (for Comet.com):
```bash
opik configure
```

&gt; [!TIP]
&gt; You can also call `opik.configure(use_local=True)` from your Python code to configure the SDK to run on a local self-hosted installation, or provide API key and workspace details directly for Comet.com. Refer to the [Python SDK documentation](apps/opik-documentation/documentation/fern/docs/reference/python-sdk/) for more configuration options.

You are now ready to start logging traces using the [Python SDK](https://www.comet.com/docs/opik/python-sdk-reference/?from=llm&amp;utm_source=opik&amp;utm_medium=github&amp;utm_content=sdk_link2&amp;utm_campaign=opik).

### üìù Logging Traces with Integrations

The easiest way to log traces is to use one of our direct integrations. Opik supports a wide array of frameworks, including recent additions like **Google ADK**, **Autogen**, and **Flowise AI**:

| Integration    | Description                                                         | Documentation                                                                                                                                                        | Try in Colab                                                                                                                                                                                                                       |
|----------------|---------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| AG2            | Log traces for AG2 LLM calls                                        | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/ag2?utm_source=opik&amp;utm_medium=github&amp;utm_content=anthropic_link&amp;utm_campaign=opik)             | (*Coming Soon*)                                                                                                                                                                                                                    |
| aisuite        | Log traces for aisuite LLM calls                                    | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/aisuite?utm_source=opik&amp;utm_medium=github&amp;utm_content=anthropic_link&amp;utm_campaign=opik)         | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/aisuite.ipynb)    |
| Anthropic      | Log traces for Anthropic LLM calls                                  | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/anthropic?utm_source=opik&amp;utm_medium=github&amp;utm_content=anthropic_link&amp;utm_campaign=opik)       | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/anthropic.ipynb)    |
| Autogen        | Log traces for Autogen agentic workflows                            | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/autogen?utm_source=opik&amp;utm_medium=github&amp;utm_content=autogen_link&amp;utm_campaign=opik)           | (*Coming Soon*)                                                                                                                                                                                                                    |
| Bedrock        | Log traces for Amazon Bedrock LLM calls                             | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/bedrock?utm_source=opik&amp;utm_medium=github&amp;utm_content=bedrock_link&amp;utm_campaign=opik)           | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/bedrock.ipynb)      |
| CrewAI         | Log traces for CrewAI calls                                         | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/crewai?utm_source=opik&amp;utm_medium=github&amp;utm_content=crewai_link&amp;utm_campaign=opik)             | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/crewai.ipynb)       |
| DeepSeek       | Log traces for DeepSeek LLM calls                                   | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/deepseek?utm_source=opik&amp;utm_medium=github&amp;utm_content=deepseek_link&amp;utm_campaign=opik)         | (*Coming Soon*)                                                                                                                                                                                                                    |
| Dify           | Log traces for Dify agent runs                                      | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/dify?utm_source=opik&amp;utm_medium=github&amp;utm_content=dspy_link&amp;utm_campaign=opik)                 | (*Coming Soon*)                                                                                                                                                                                                                    |
| DSPy           | Log traces for DSPy runs                                            | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/dspy?utm_source=opik&amp;utm_medium=github&amp;utm_content=dspy_link&amp;utm_campaign=opik)                 | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/dspy.ipynb)         |
| Flowise AI     | Log traces for Flowise AI visual LLM builder                        | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/flowise?utm_source=opik&amp;utm_medium=github&amp;utm_content=flowise_link&amp;utm_campaign=opik)           | (*Native UI intergration, see documentation*)                                                                                                                                                                                      |
| Gemini         | Log traces for Google Gemini LLM calls                              | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/gemini?utm_source=opik&amp;utm_medium=github&amp;utm_content=gemini_link&amp;utm_campaign=opik)             | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/gemini.ipynb)       |
| Google ADK     | Log traces for Google Agent Development Kit (ADK)                   | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/google_adk?utm_source=opik&amp;utm_medium=github&amp;utm_content=google_adk_link&amp;utm_campaign=opik)     | (*Coming Soon*)                                                                                                                                                                                                                    |
| Groq           | Log traces for Groq LLM calls                                       | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/groq?utm_source=opik&amp;utm_medium=github&amp;utm_content=groq_link&amp;utm_campaign=opik)                 | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/groq.ipynb)         |
| Guardrails     | Log traces for Guardrails AI validations                            | [Documentation](https://www.comet.com/docs/opik/tracing/integrations/guardrails/?utm_source=opik&amp;utm_medium=github&amp;utm_content=guardrails_link&amp;utm_campaign=opik)    | [![Open Quickstart In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/opik/blob/master/apps/opik-documentation/documentation/docs/cookbook/guard

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[rougier/numpy-100]]></title>
            <link>https://github.com/rougier/numpy-100</link>
            <guid>https://github.com/rougier/numpy-100</guid>
            <pubDate>Fri, 20 Jun 2025 00:04:35 GMT</pubDate>
            <description><![CDATA[100 numpy exercises (with solutions)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/rougier/numpy-100">rougier/numpy-100</a></h1>
            <p>100 numpy exercises (with solutions)</p>
            <p>Language: Python</p>
            <p>Stars: 12,935</p>
            <p>Forks: 6,061</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre>## 100 numpy exercises

[![Binder](http://mybinder.org/badge.svg)](http://mybinder.org:/repo/rougier/numpy-100/notebooks/100%20Numpy%20exercises.ipynb)

This is a collection of numpy exercises from numpy mailing list, stack overflow, and numpy documentation. I&#039;ve also created some problems myself to reach the 100 limit. The goal of this collection is to offer a quick reference for both old and new users but also to provide a set of exercises for those who teach. For extended exercises, make sure to read [From Python to NumPy](http://www.labri.fr/perso/nrougier/from-python-to-numpy/).

‚Üí [Test them on Binder](http://mybinder.org:/repo/rougier/numpy-100/notebooks/100_Numpy_exercises.ipynb)  
‚Üí [Read them on GitHub](100_Numpy_exercises.md)  

Note: markdown and ipython notebook are created programmatically from the source data in `source/exercises.ktx`.
To modify the content of these files, please change the text in the source and run the `generators.py` module with a python
interpreter with the libraries under `requirements.txt` installed.

The keyed text format (`ktx`) is a minimal human readable key-values to store text (markdown or others) indexed by keys. 

This work is licensed under the MIT license.  
[![DOI](https://zenodo.org/badge/10173/rougier/numpy-100.svg)](https://zenodo.org/badge/latestdoi/10173/rougier/numpy-100)


### Variants in Other Languages

 - **Julia**: [100 Julia Exercises](https://github.com/RoyiAvital/Julia100Exercises).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sqlmapproject/sqlmap]]></title>
            <link>https://github.com/sqlmapproject/sqlmap</link>
            <guid>https://github.com/sqlmapproject/sqlmap</guid>
            <pubDate>Fri, 20 Jun 2025 00:04:34 GMT</pubDate>
            <description><![CDATA[Automatic SQL injection and database takeover tool]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sqlmapproject/sqlmap">sqlmapproject/sqlmap</a></h1>
            <p>Automatic SQL injection and database takeover tool</p>
            <p>Language: Python</p>
            <p>Stars: 34,518</p>
            <p>Forks: 5,929</p>
            <p>Stars today: 55 stars today</p>
            <h2>README</h2><pre># sqlmap ![](https://i.imgur.com/fe85aVR.png)

[![.github/workflows/tests.yml](https://github.com/sqlmapproject/sqlmap/actions/workflows/tests.yml/badge.svg)](https://github.com/sqlmapproject/sqlmap/actions/workflows/tests.yml) [![Python 2.6|2.7|3.x](https://img.shields.io/badge/python-2.6|2.7|3.x-yellow.svg)](https://www.python.org/) [![License](https://img.shields.io/badge/license-GPLv2-red.svg)](https://raw.githubusercontent.com/sqlmapproject/sqlmap/master/LICENSE) [![x](https://img.shields.io/badge/x-@sqlmap-blue.svg)](https://x.com/sqlmap)

sqlmap is an open source penetration testing tool that automates the process of detecting and exploiting SQL injection flaws and taking over of database servers. It comes with a powerful detection engine, many niche features for the ultimate penetration tester, and a broad range of switches including database fingerprinting, over data fetching from the database, accessing the underlying file system, and executing commands on the operating system via out-of-band connections.

Screenshots
----

![Screenshot](https://raw.github.com/wiki/sqlmapproject/sqlmap/images/sqlmap_screenshot.png)

You can visit the [collection of screenshots](https://github.com/sqlmapproject/sqlmap/wiki/Screenshots) demonstrating some of the features on the wiki.

Installation
----

You can download the latest tarball by clicking [here](https://github.com/sqlmapproject/sqlmap/tarball/master) or latest zipball by clicking [here](https://github.com/sqlmapproject/sqlmap/zipball/master).

Preferably, you can download sqlmap by cloning the [Git](https://github.com/sqlmapproject/sqlmap) repository:

    git clone --depth 1 https://github.com/sqlmapproject/sqlmap.git sqlmap-dev

sqlmap works out of the box with [Python](https://www.python.org/download/) version **2.6**, **2.7** and **3.x** on any platform.

Usage
----

To get a list of basic options and switches use:

    python sqlmap.py -h

To get a list of all options and switches use:

    python sqlmap.py -hh

You can find a sample run [here](https://asciinema.org/a/46601).
To get an overview of sqlmap capabilities, a list of supported features, and a description of all options and switches, along with examples, you are advised to consult the [user&#039;s manual](https://github.com/sqlmapproject/sqlmap/wiki/Usage).

Links
----

* Homepage: https://sqlmap.org
* Download: [.tar.gz](https://github.com/sqlmapproject/sqlmap/tarball/master) or [.zip](https://github.com/sqlmapproject/sqlmap/zipball/master)
* Commits RSS feed: https://github.com/sqlmapproject/sqlmap/commits/master.atom
* Issue tracker: https://github.com/sqlmapproject/sqlmap/issues
* User&#039;s manual: https://github.com/sqlmapproject/sqlmap/wiki
* Frequently Asked Questions (FAQ): https://github.com/sqlmapproject/sqlmap/wiki/FAQ
* X: [@sqlmap](https://x.com/sqlmap)
* Demos: [https://www.youtube.com/user/inquisb/videos](https://www.youtube.com/user/inquisb/videos)
* Screenshots: https://github.com/sqlmapproject/sqlmap/wiki/Screenshots

Translations
----

* [Arabic](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-ar-AR.md)
* [Bulgarian](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-bg-BG.md)
* [Chinese](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-zh-CN.md)
* [Croatian](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-hr-HR.md)
* [Dutch](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-nl-NL.md)
* [French](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-fr-FR.md)
* [Georgian](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-ka-GE.md)
* [German](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-de-DE.md)
* [Greek](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-gr-GR.md)
* [Hindi](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-in-HI.md)
* [Indonesian](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-id-ID.md)
* [Italian](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-it-IT.md)
* [Japanese](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-ja-JP.md)
* [Korean](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-ko-KR.md)
* [Kurdish (Central)](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-ckb-KU.md)
* [Persian](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-fa-IR.md)
* [Polish](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-pl-PL.md)
* [Portuguese](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-pt-BR.md)
* [Russian](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-ru-RU.md)
* [Serbian](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-rs-RS.md)
* [Slovak](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-sk-SK.md)
* [Spanish](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-es-MX.md)
* [Turkish](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-tr-TR.md)
* [Ukrainian](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-uk-UA.md)
* [Vietnamese](https://github.com/sqlmapproject/sqlmap/blob/master/doc/translations/README-vi-VN.md)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NVIDIA/TransformerEngine]]></title>
            <link>https://github.com/NVIDIA/TransformerEngine</link>
            <guid>https://github.com/NVIDIA/TransformerEngine</guid>
            <pubDate>Fri, 20 Jun 2025 00:04:33 GMT</pubDate>
            <description><![CDATA[A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit floating point (FP8) precision on Hopper, Ada and Blackwell GPUs, to provide better performance with lower memory utilization in both training and inference.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NVIDIA/TransformerEngine">NVIDIA/TransformerEngine</a></h1>
            <p>A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit floating point (FP8) precision on Hopper, Ada and Blackwell GPUs, to provide better performance with lower memory utilization in both training and inference.</p>
            <p>Language: Python</p>
            <p>Stars: 2,495</p>
            <p>Forks: 434</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>