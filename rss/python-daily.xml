<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Mon, 05 Jan 2026 00:05:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[OpenBB-finance/OpenBB]]></title>
            <link>https://github.com/OpenBB-finance/OpenBB</link>
            <guid>https://github.com/OpenBB-finance/OpenBB</guid>
            <pubDate>Mon, 05 Jan 2026 00:05:02 GMT</pubDate>
            <description><![CDATA[Financial data platform for analysts, quants and AI agents.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenBB-finance/OpenBB">OpenBB-finance/OpenBB</a></h1>
            <p>Financial data platform for analysts, quants and AI agents.</p>
            <p>Language: Python</p>
            <p>Stars: 56,774</p>
            <p>Forks: 5,520</p>
            <p>Stars today: 436 stars today</p>
            <h2>README</h2><pre>&lt;br /&gt;
&lt;img src=&quot;https://github.com/OpenBB-finance/OpenBB/blob/develop/images/odp-light.svg?raw=true#gh-light-mode-only&quot; alt=&quot;Open Data Platform by OpenBB logo&quot; width=&quot;600&quot;&gt;
&lt;img src=&quot;https://github.com/OpenBB-finance/OpenBB/blob/develop/images/odp-dark.svg?raw=true#gh-dark-mode-only&quot; alt=&quot;Open Data Platform by OpenBB logo&quot; width=&quot;600&quot;&gt;
&lt;br /&gt;
&lt;br /&gt;

[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&amp;label=Follow%20%40openbb_finance)](https://x.com/openbb_finance)
[![Discord Shield](https://img.shields.io/discord/831165782750789672)](https://discord.com/invite/xPHTuHCmuV)
[![Open in Dev Containers](https://img.shields.io/static/v1?label=Dev%20Containers&amp;message=Open&amp;color=blue&amp;logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB)
&lt;a href=&quot;https://codespaces.new/OpenBB-finance/OpenBB&quot;&gt;
  &lt;img src=&quot;https://github.com/codespaces/badge.svg&quot; height=&quot;20&quot; /&gt;
&lt;/a&gt;
&lt;a target=&quot;_blank&quot; href=&quot;https://colab.research.google.com/github/OpenBB-finance/OpenBB/blob/develop/examples/googleColab.ipynb&quot;&gt;
  &lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot;/&gt;
&lt;/a&gt;
[![PyPI](https://img.shields.io/pypi/v/openbb?color=blue&amp;label=PyPI%20Package)](https://pypi.org/project/openbb/)

Open Data Platform by OpenBB (ODP) is the open-source toolset that helps data engineers integrate proprietary, licensed, and public data sources into downstream applications like AI copilots and research dashboards.

ODP operates as the &quot;connect once, consume everywhere&quot; infrastructure layer that consolidates and exposes data to multiple surfaces at once: Python environments for quants, OpenBB Workspace and Excel for analysts, MCP servers for AI agents, and REST APIs for other applications.

&lt;a href=&quot;https://pro.openbb.co&quot;&gt;
  &lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://openbb-cms.directus.app/assets/70b971ef-7a7e-486e-b5ae-1cc602f2162c.png&quot; alt=&quot;Logo&quot; width=&quot;1000&quot;&gt;
  &lt;/div&gt;
&lt;/a&gt;

Get started with: `pip install openbb`

```python
from openbb import obb
output = obb.equity.price.historical(&quot;AAPL&quot;)
df = output.to_dataframe()
```

Data integrations available can be found here: &lt;https://docs.openbb.co/python/reference&gt;

---

## OpenBB Workspace

While the Open Data Platform provides the open-source data integration foundation, **OpenBB Workspace** offers the enterprise UI for analysts to visualize datasets and leverage AI agents. The platform&#039;s &quot;connect once, consume everywhere&quot; architecture enables seamless integration between the two.

You can find OpenBB Workspace at &lt;https://pro.openbb.co&gt;.
&lt;a href=&quot;https://pro.openbb.co&quot;&gt;
  &lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://openbb-cms.directus.app/assets/f69b6aaf-0821-4bc8-a43c-715e03a924ef.png&quot; alt=&quot;Logo&quot; width=&quot;1000&quot;&gt;
  &lt;/div&gt;
&lt;/a&gt;

Data integration:

- You can learn more about adding data to the OpenBB workspace from the [docs](https://docs.openbb.co/workspace) or [this open source repository](https://github.com/OpenBB-finance/backends-for-openbb).

AI Agents integration:

- You can learn more about adding AI agents to the OpenBB workspace from [this open source repository](https://github.com/OpenBB-finance/agents-for-openbb).

### Integrating Open Data Platform to the OpenBB Workspace

Connect this library to the OpenBB Workspace with a few simple commands, in a Python (3.9.21 - 3.12) environment.

#### Run an ODP backend

- Install the packages.

```sh
pip install &quot;openbb[all]&quot;
```

- Start the API server over localhost.

```sh
openbb-api
```

This will launch a FastAPI server, via Uvicorn, at `127.0.0.1:6900`.

You can check that it works by going to &lt;http://127.0.0.1:6900&gt;.

#### Integrate the ODP Backend to OpenBB Workspace

Sign-in to the [OpenBB Workspace](https://pro.openbb.co/), and follow the following steps:

![CleanShot 2025-05-17 at 09 51 56@2x](https://github.com/user-attachments/assets/75cffb4a-5e95-470a-b9d0-6ffd4067e069)

1. Go to the &quot;Apps&quot; tab
2. Click on &quot;Connect backend&quot;
3. Fill in the form with:
   Name: Open Data Platform
   URL: &lt;http://127.0.0.1:6900&gt;
4. Click on &quot;Test&quot;. You should get a &quot;Test successful&quot; with the number of apps found.
5. Click on &quot;Add&quot;.

That&#039;s it.

---

&lt;!-- TABLE OF CONTENTS --&gt;
&lt;details closed=&quot;closed&quot;&gt;
  &lt;summary&gt;&lt;h2 style=&quot;display: inline-block&quot;&gt;Table of Contents&lt;/h2&gt;&lt;/summary&gt;
  &lt;ol&gt;
    &lt;li&gt;&lt;a href=&quot;#1-installation&quot;&gt;Installation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#2-contributing&quot;&gt;Contributing&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#3-license&quot;&gt;License&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#4-disclaimer&quot;&gt;Disclaimer&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#5-contacts&quot;&gt;Contacts&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#6-star-history&quot;&gt;Star History&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#7-contributors&quot;&gt;Contributors&lt;/a&gt;&lt;/li&gt;
  &lt;/ol&gt;
&lt;/details&gt;

## 1. Installation

The ODP Python Package can be installed from [PyPI package](https://pypi.org/project/openbb/) by running `pip install openbb`

or by cloning the repository directly with `git clone https://github.com/OpenBB-finance/OpenBB.git`.

Please find more about the installation process, in the [OpenBB Documentation](https://docs.openbb.co/python/installation).

### ODP CLI installation

The ODP CLI is a command-line interface that allows you to access the ODP directly from your command line.

It can be installed by running `pip install openbb-cli`

or by cloning the repository directly with  `git clone https://github.com/OpenBB-finance/OpenBB.git`.

Please find more about the installation process in the [OpenBB Documentation](https://docs.openbb.co/cli/installation).

## 2. Contributing

There are three main ways of contributing to this project. (Hopefully you have starred the project by now ‚≠êÔ∏è)

### Become a Contributor

- More information on our [Developer Documentation](https://docs.openbb.co/python/developer).

### Create a GitHub ticket

Before creating a ticket make sure the one you are creating doesn&#039;t exist already [among the existing issues](https://github.com/OpenBB-finance/OpenBB/issues)

- [Report bug](https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;labels=bug&amp;template=bug_report.md&amp;title=%5BBug%5D)
- [Suggest improvement](https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;labels=enhancement&amp;template=enhancement.md&amp;title=%5BIMPROVE%5D)
- [Request a feature](https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;labels=new+feature&amp;template=feature_request.md&amp;title=%5BFR%5D)

### Provide feedback

We are most active on [our Discord](https://openbb.co/discord), but feel free to reach out to us in any of [our social media](https://openbb.co/links) for feedback.

## 3. License

Distributed under the AGPLv3 License. See
[LICENSE](https://github.com/OpenBB-finance/OpenBB/blob/main/LICENSE) for more information.

## 4. Disclaimer

Trading in financial instruments involves high risks including the risk of losing some, or all, of your investment
amount, and may not be suitable for all investors.

Before deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.

The data contained in the Open Data Platform is not necessarily accurate.

OpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.

All names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.

Our use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.

## 5. Contacts

If you have any questions about the platform or anything OpenBB, feel free to email us at `support@openbb.co`

If you want to say hi, or are interested in partnering with us, feel free to reach us at `hello@openbb.co`

Any of our social media platforms: [openbb.co/links](https://openbb.co/links)

## 6. Star History

This is a proxy of our growth and that we are just getting started.

But for more metrics important to us check [openbb.co/open](https://openbb.co/open).

[![Star History Chart](https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;type=Date&amp;theme=dark)](https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;type=Date&amp;theme=dark)

## 7. Contributors

OpenBB wouldn&#039;t be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.

&lt;a href=&quot;https://github.com/OpenBB-finance/OpenBB/graphs/contributors&quot;&gt;
   &lt;img src=&quot;https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBB&quot; width=&quot;800&quot;/&gt;
&lt;/a&gt;

&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt;
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;

[contributors-shield]: https://img.shields.io/github/contributors/OpenBB-finance/OpenBB.svg?style=for-the-badge
[contributors-url]: https://github.com/OpenBB-finance/OpenBB/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/OpenBB-finance/OpenBB.svg?style=for-the-badge
[forks-url]: https://github.com/OpenBB-finance/OpenBB/network/members
[stars-shield]: https://img.shields.io/github/stars/OpenBB-finance/OpenBB.svg?style=for-the-badge
[stars-url]: https://github.com/OpenBB-finance/OpenBB/stargazers
[issues-shield]: https://img.shields.io/github/issues/OpenBB-finance/OpenBB.svg?style=for-the-badge&amp;color=blue
[issues-url]: https://github.com/OpenBB-finance/OpenBB/issues
[bugs-open-shield]: https://img.shields.io/github/issues/OpenBB-finance/OpenBB/bug.svg?style=for-the-badge&amp;color=yellow
[bugs-open-url]: https://github.com/OpenBB-finance/OpenBB/issues?q=is%3Aissue+label%3Abug+is%3Aopen
[bugs-closed-shield]: https://img.shields.io/github/issues-closed/OpenBB-finance/OpenBB/bug.svg?style=for-the-badge&amp;color=success
[bugs-closed-url]: https://github.com/OpenBB-finance/OpenBB/issues?q=is%3Aissue+label%3Abug+is%3Aclosed
[license-shield]: https://img.shields.io/github/license/OpenBB-finance/OpenBB.svg?style=for-the-badge
[license-url]: https://github.com/OpenBB-finance/OpenBB/blob/main/LICENSE.txt
[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&amp;logo=linkedin&amp;colorB=555
[linkedin-url]: https://linkedin.com/in/DidierRLopes
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[virattt/ai-hedge-fund]]></title>
            <link>https://github.com/virattt/ai-hedge-fund</link>
            <guid>https://github.com/virattt/ai-hedge-fund</guid>
            <pubDate>Mon, 05 Jan 2026 00:05:01 GMT</pubDate>
            <description><![CDATA[An AI Hedge Fund Team]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/virattt/ai-hedge-fund">virattt/ai-hedge-fund</a></h1>
            <p>An AI Hedge Fund Team</p>
            <p>Language: Python</p>
            <p>Stars: 44,503</p>
            <p>Forks: 7,855</p>
            <p>Stars today: 99 stars today</p>
            <h2>README</h2><pre># AI Hedge Fund

This is a proof of concept for an AI-powered hedge fund.  The goal of this project is to explore the use of AI to make trading decisions.  This project is for **educational** purposes only and is not intended for real trading or investment.

This system employs several agents working together:

1. Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation
2. Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety
3. Bill Ackman Agent - An activist investor, takes bold positions and pushes for change
4. Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption
5. Charlie Munger Agent - Warren Buffett&#039;s partner, only buys wonderful businesses at fair prices
6. Michael Burry Agent - The Big Short contrarian who hunts for deep value
7. Mohnish Pabrai Agent - The Dhandho investor, who looks for doubles at low risk
8. Peter Lynch Agent - Practical investor who seeks &quot;ten-baggers&quot; in everyday businesses
9. Phil Fisher Agent - Meticulous growth investor who uses deep &quot;scuttlebutt&quot; research 
10. Rakesh Jhunjhunwala Agent - The Big Bull of India
11. Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential
12. Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price
13. Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals
14. Sentiment Agent - Analyzes market sentiment and generates trading signals
15. Fundamentals Agent - Analyzes fundamental data and generates trading signals
16. Technicals Agent - Analyzes technical indicators and generates trading signals
17. Risk Manager - Calculates risk metrics and sets position limits
18. Portfolio Manager - Makes final trading decisions and generates orders

&lt;img width=&quot;1042&quot; alt=&quot;Screenshot 2025-03-22 at 6 19 07 PM&quot; src=&quot;https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4&quot; /&gt;

Note: the system does not actually make any trades.

[![Twitter Follow](https://img.shields.io/twitter/follow/virattt?style=social)](https://twitter.com/virattt)

## Disclaimer

This project is for **educational and research purposes only**.

- Not intended for real trading or investment
- No investment advice or guarantees provided
- Creator assumes no liability for financial losses
- Consult a financial advisor for investment decisions
- Past performance does not indicate future results

By using this software, you agree to use it solely for learning purposes.

## Table of Contents
- [How to Install](#how-to-install)
- [How to Run](#how-to-run)
  - [‚å®Ô∏è Command Line Interface](#Ô∏è-command-line-interface)
  - [üñ•Ô∏è Web Application](#Ô∏è-web-application)
- [How to Contribute](#how-to-contribute)
- [Feature Requests](#feature-requests)
- [License](#license)

## How to Install

Before you can run the AI Hedge Fund, you&#039;ll need to install it and set up your API keys. These steps are common to both the full-stack web application and command line interface.

### 1. Clone the Repository

```bash
git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
```

### 2. Set up API keys

Create a `.env` file for your API keys:
```bash
# Create .env file for your API keys (in the root directory)
cp .env.example .env
```

Open and edit the `.env` file to add your API keys:
```bash
# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
OPENAI_API_KEY=your-openai-api-key

# For getting financial data to power the hedge fund
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
```

**Important**: You must set at least one LLM API key (e.g. `OPENAI_API_KEY`, `GROQ_API_KEY`, `ANTHROPIC_API_KEY`, or `DEEPSEEK_API_KEY`) for the hedge fund to work. 

**Financial Data**: Data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key. For any other ticker, you will need to set the `FINANCIAL_DATASETS_API_KEY` in the .env file.

## How to Run

### ‚å®Ô∏è Command Line Interface

You can run the AI Hedge Fund directly via terminal. This approach offers more granular control and is useful for automation, scripting, and integration purposes.

&lt;img width=&quot;992&quot; alt=&quot;Screenshot 2025-01-06 at 5 50 17 PM&quot; src=&quot;https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b&quot; /&gt;

#### Quick Start

1. Install Poetry (if not already installed):
```bash
curl -sSL https://install.python-poetry.org | python3 -
```

2. Install dependencies:
```bash
poetry install
```

#### Run the AI Hedge Fund
```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA
```

You can also specify a `--ollama` flag to run the AI hedge fund using local LLMs.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama
```

You can optionally specify the start and end dates to make decisions over a specific time period.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
```

#### Run the Backtester
```bash
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
```

**Example Output:**
&lt;img width=&quot;941&quot; alt=&quot;Screenshot 2025-01-06 at 5 47 52 PM&quot; src=&quot;https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47&quot; /&gt;


Note: The `--ollama`, `--start-date`, and `--end-date` flags work for the backtester, as well!

### üñ•Ô∏è Web Application

The new way to run the AI Hedge Fund is through our web application that provides a user-friendly interface. This is recommended for users who prefer visual interfaces over command line tools.

Please see detailed instructions on how to install and run the web application [here](https://github.com/virattt/ai-hedge-fund/tree/main/app).

&lt;img width=&quot;1721&quot; alt=&quot;Screenshot 2025-06-28 at 6 41 03‚ÄØPM&quot; src=&quot;https://github.com/user-attachments/assets/b95ab696-c9f4-416c-9ad1-51feb1f5374b&quot; /&gt;


## How to Contribute

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

**Important**: Please keep your pull requests small and focused.  This will make it easier to review and merge.

## Feature Requests

If you have a feature request, please open an [issue](https://github.com/virattt/ai-hedge-fund/issues) and make sure it is tagged with `enhancement`.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[python/cpython]]></title>
            <link>https://github.com/python/cpython</link>
            <guid>https://github.com/python/cpython</guid>
            <pubDate>Mon, 05 Jan 2026 00:05:00 GMT</pubDate>
            <description><![CDATA[The Python programming language]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/python/cpython">python/cpython</a></h1>
            <p>The Python programming language</p>
            <p>Language: Python</p>
            <p>Stars: 70,689</p>
            <p>Forks: 33,820</p>
            <p>Stars today: 37 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/VibeVoice]]></title>
            <link>https://github.com/microsoft/VibeVoice</link>
            <guid>https://github.com/microsoft/VibeVoice</guid>
            <pubDate>Mon, 05 Jan 2026 00:04:59 GMT</pubDate>
            <description><![CDATA[Open-Source Frontier Voice AI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/VibeVoice">microsoft/VibeVoice</a></h1>
            <p>Open-Source Frontier Voice AI</p>
            <p>Language: Python</p>
            <p>Stars: 19,514</p>
            <p>Forks: 2,171</p>
            <p>Stars today: 105 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

## üéôÔ∏è VibeVoice: Open-Source Frontier Voice AI
[![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=microsoft)](https://microsoft.github.io/VibeVoice)
[![Hugging Face](https://img.shields.io/badge/HuggingFace-Collection-orange?logo=huggingface)](https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f)
[![Technical Report](https://img.shields.io/badge/Technical-Report-red?logo=adobeacrobatreader)](https://arxiv.org/pdf/2508.19205)


&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;Figures/VibeVoice_logo_white.png&quot;&gt;
  &lt;img src=&quot;Figures/VibeVoice_logo.png&quot; alt=&quot;VibeVoice Logo&quot; width=&quot;300&quot;&gt;
&lt;/picture&gt;
&lt;/div&gt;

&lt;div align=&quot;left&quot;&gt;

&lt;h3&gt;üì∞ News&lt;/h3&gt;

&lt;img src=&quot;https://img.shields.io/badge/Status-New-brightgreen?style=flat&quot; alt=&quot;New&quot; /&gt;
&lt;img src=&quot;https://img.shields.io/badge/Feature-Realtime_TTS-blue?style=flat&amp;logo=soundcharts&quot; alt=&quot;Realtime TTS&quot; /&gt;


&lt;strong&gt;2025-12-16: üì£ We added more experimental speakers for exploration, including multilingual voices and 11 distinct English style voices. [Try it](docs/vibevoice-realtime-0.5b.md#optional-more-experimental-voices). More speaker types will be added over time.&lt;/strong&gt;

2025-12-09: üì£ We added experimental speakers in nine languages (DE, FR, IT, JP, KR, NL, PL, PT, ES) for exploration‚Äîwelcome to try them out and share your feedback.

2025-12-03: üì£ We open-sourced &lt;a href=&quot;docs/vibevoice-realtime-0.5b.md&quot;&gt;&lt;strong&gt;VibeVoice‚ÄëRealtime‚Äë0.5B&lt;/strong&gt;&lt;/a&gt;, a real‚Äëtime text‚Äëto‚Äëspeech model that supports streaming text input and robust long-form speech generation. Try it on [Colab](https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb).

To mitigate deepfake risks and ensure low latency for the first speech chunk, voice prompts are provided in an embedded format. For users requiring voice customization, please reach out to our team. We will also be expanding the range of available speakers.
&lt;br&gt;

https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc

&gt; (Launch your own realtime demo via the websocket example in [Usage](docs/vibevoice-realtime-0.5b.md#usage-1-launch-real-time-websocket-demo)).

&lt;/div&gt;

2025-09-05: VibeVoice is an open-source research framework intended to advance collaboration in the speech synthesis community. After release, we discovered instances where the tool was used in ways inconsistent with the stated intent. Since responsible use of AI is one of Microsoft‚Äôs guiding principles, we have disabled this repo until we are confident that out-of-scope use is no longer possible.


### Overview

VibeVoice is a novel framework designed for generating **expressive**, **long-form**, **multi-speaker** conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking.

VibeVoice currently includes two model variants:

- **Long-form multi-speaker model**: Synthesizes conversational/single-speaker speech up to **90 minutes** with up to **4 distinct speakers**, surpassing the typical 1‚Äì2 speaker limits of many prior models.
- **[Realtime streaming TTS model](docs/vibevoice-realtime-0.5b.md)**: Produces initial audible speech in ~**300 ms** and supports **streaming text input** for single-speaker **real-time** speech generation; designed for low-latency generation.

A core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a [next-token diffusion](https://arxiv.org/abs/2412.08635) framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.


&lt;p align=&quot;left&quot;&gt;
  &lt;img src=&quot;Figures/MOS-preference.png&quot; alt=&quot;MOS Preference Results&quot; height=&quot;260px&quot;&gt;
  &lt;img src=&quot;Figures/VibeVoice.jpg&quot; alt=&quot;VibeVoice Overview&quot; height=&quot;250px&quot; style=&quot;margin-right: 10px;&quot;&gt;
&lt;/p&gt;


### üéµ Demo Examples


**Video Demo**

We produced this video with [Wan2.2](https://github.com/Wan-Video/Wan2.2). We sincerely appreciate the Wan-Video team for their great work.

**English**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784

&lt;/div&gt;


**Chinese**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f

&lt;/div&gt;

**Cross-Lingual**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722

&lt;/div&gt;

**Spontaneous Singing**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730

&lt;/div&gt;


**Long Conversation with 4 people**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727

&lt;/div&gt;

For more examples, see the [Project Page](https://microsoft.github.io/VibeVoice).



## Risks and limitations

While efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release).
Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.

English and Chinese only: Transcripts in languages other than English or Chinese may result in unexpected audio outputs.

Non-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects.

Overlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations.

We do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.

## Star History

![Star History Chart](https://api.star-history.com/svg?repos=Microsoft/vibevoice&amp;type=date&amp;legend=top-left)</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ladaapp/lada]]></title>
            <link>https://github.com/ladaapp/lada</link>
            <guid>https://github.com/ladaapp/lada</guid>
            <pubDate>Mon, 05 Jan 2026 00:04:58 GMT</pubDate>
            <description><![CDATA[Restore videos with pixelated/mosaic regions]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ladaapp/lada">ladaapp/lada</a></h1>
            <p>Restore videos with pixelated/mosaic regions</p>
            <p>Language: Python</p>
            <p>Stars: 2,563</p>
            <p>Forks: 337</p>
            <p>Stars today: 96 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
  &lt;img src=&quot;packaging/flatpak/share/io.github.ladaapp.lada.png&quot; alt=&quot;Lada Icon&quot; style=&quot;display: block; width: 64px; height: 64px;&quot;&gt;
  &lt;br&gt;
  Lada
&lt;/h1&gt;

*Lada* is a tool designed to recover pixelated adult videos (JAV). It helps restore the visual quality of such content, making it more enjoyable to watch.

## Features

- **Recover Pixelated Videos**: Restore pixelated or mosaic scenes in adult videos.
- **Watch/Export Videos**: Use either the CLI or GUI to watch or export your restored videos.

## Usage

### GUI

After opening a file, you can either watch the restored via in realtime or export it to a new file to watch it later:

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/screenshot_gui_1_dark.png&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/screenshot_gui_1_light.png&quot;&gt;
  &lt;img alt=&quot;Screenshot showing video preview&quot; src=&quot;assets/screenshot_gui_1_dark.png&quot; width=&quot;36%&quot;&gt;
&lt;/picture&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/screenshot_gui_2_dark.png&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/screenshot_gui_2_light.png&quot;&gt;
  &lt;img alt=&quot;Screenshot showing video export&quot; src=&quot;assets/screenshot_gui_2_dark.png&quot; width=&quot;45%&quot;&gt;
&lt;/picture&gt;

Additional settings can be found in the left sidebar.

### CLI

You can also use the command-line interface (CLI) to restore video(s):

```shell
lada-cli --input &lt;input video path&gt;
```
&lt;img src=&quot;assets/screenshot_cli_1.png&quot; alt=&quot;screenshot showing video export&quot; width=&quot;60%&quot;&gt;

For more information about additional options, use the `--help` argument.

## Performance and hardware requirements
Don&#039;t expect this to work perfectly, some scenes can be pretty good and close to the real thing. Other scenes can be rather meh and show worse artifacts than the original mosaics.

You&#039;ll need a GPU and some patience to run the app. If your card has at least 4-6GB of VRAM then it should work out of the box.

The CPU is used for encoding the restored video so shouldn&#039;t be too slow either. But you can also use GPU encoding and run both the restoration and encoding tasks on the GPU.

The app also needs quite a bit of RAM for buffering to increase throughput. For 1080p content you should be fine with 6-8GB RAM, 4K will need a lot more.

To watch the restored video in realtime you&#039;ll need a pretty beefy machine or you&#039;ll see the player pausing and buffering until next restored frames are computed.
When viewing the video no encoding is done but it will use more additional RAM for buffering.

If your GPU is not fast enough to watch the video in real-time you&#039;ll have to export it first and watch it later with your favorite media player (available in GUI and CLI).

Technically running the app on your CPU is also supported but it will be so slow that it&#039;s not really practical.

Here are some speed performance numbers using Lada v0.7.0 on my available hardware to give you an idea what to expect (used libx264/CPU codec with default settings; RTX 3090 results are limited by CPU encoding and could be a lot faster by switching to NVENC/GPU encoder):

| Video name | Video description                                                                                    | Video&lt;br&gt;duration / resolution / FPS | Lada&lt;br&gt;runtime / FPS&lt;br&gt;Nvidia RTX 3050&lt;br&gt;(*Laptop GPU*) | Lada&lt;br&gt;runtime / FPS&lt;br&gt;Nvidia RTX 3090&lt;br&gt;(Desktop GPU) |
|------------|------------------------------------------------------------------------------------------------------|--------------------------------------|------------------------------------------------------------|-----------------------------------------------------------|
| vid1       | multiple mosaic regions present on all frames                                                        | 1m30s / 10920x1080 / 30 FPS          | 3m36s / 12 FPS                                             | 1m33s / 30 FPS                                            |
| vid2       | single mosaic region present on all frames                                                           | 3m0s / 1920x1080 / 30 FPS            | 4m11s / 21 FPS                                             | 2m16s / 39 FPS                                            |
| vid3       | half of the video doesn&#039;t have any mosaics present,&lt;br&gt;the other half mostly single mosaic per frame | 41m16s / 852x480 / 30 FPS            | 26m30s / 46 FPS                                            | 10m20s / 119 FPS                                          |

## Installation
### Using Flatpak
The easiest way to install the app (CLI and GUI) on Linux is via Flathub:

&lt;a href=&#039;https://flathub.org/apps/details/io.github.ladaapp.lada&#039;&gt;&lt;img width=&#039;200&#039; alt=&#039;Download from Flathub&#039; src=&#039;https://flathub.org/api/badge?svg&amp;locale=en&#039;/&gt;&lt;/a&gt;

&gt; [!NOTE]
&gt; The Flatpak only works with x86_64 CPUs and Nvidia/CUDA GPUs (Turing or newer: RTX 20xx up to including RTX 50xx). Ensure your NVIDIA GPU driver is up-to-date.
&gt; It can also be used without a GPU but it will be very slow.

&gt; [!TIP]
&gt; After installation you should find Lada in your application launcher to start the GUI. You can also run it via `flatpak run io.github.ladaapp.lada`.

&gt; [!TIP]
&gt; When using the CLI via Flatpak we need to make the file/directory available by giving it permission to the file system so it can access the input/output files
&gt;  ```shell
&gt;  flatpak run --filesystem=host --command=lada-cli io.github.ladaapp.lada --input &lt;input video path&gt;
&gt;  ```
&gt; You may want to set an alias to make it easier to use
&gt; ```shell
&gt; alias lada-cli=&quot;flatpak run --filesystem=host --command=lada-cli io.github.ladaapp.lada&quot;
&gt;  ```
&gt; You could also give the filesystem permission permanently via [Flatseal](https://flathub.org/apps/com.github.tchx84.Flatseal) 

&gt; [!TIP]
&gt; If you want to use the Post-export action feature to run a command/script after export has finished you&#039;ll need to give the Flatpak additional permissions.
&gt; Add the `--talk-name=org.freedesktop.Flatpak` permission and then run your command via `flatpak-spawn`. For example: If the script you want to run is /home/user/myscript.sh then set custom command as `flatpak-spawn --host /home/user/myscript.sh`

&gt; [!TIP]
&gt; If you installed Lada from Flathub and drag-and-drop doesn&#039;t work, your file browser might not support [File Transfer Portal](https://flatpak.github.io/xdg-desktop-portal/docs/doc-org.freedesktop.portal.FileTransfer.html).
&gt; You can fix this by:
&gt;  1) Switching or updating your file browser to one that supports it.
&gt;  2) Granting the app filesystem permissions (e.g., via [Flatseal](https://flathub.org/apps/com.github.tchx84.Flatseal) so it can read files directly).
&gt;  3)  Using the &#039;Open&#039; button to select the file instead of drag-and-drop.

### Using Docker

The app is also available via Docker (CLI only). You can get the image `ladaapp/lada` from [Docker Hub](https://hub.docker.com/r/ladaapp/lada) with this command:

```shell
docker pull ladaapp/lada:latest
````

&gt; [!NOTE]
&gt; The Docker image only works with x86_64 CPUs and Nvidia/CUDA GPUs (Turing or newer: RTX 20xx up to including RTX 50xx). Ensure your NVIDIA GPU driver is up-to-date.
&gt; It can also be used without a GPU but it will be very slow.

&gt; [!TIP]
&gt; Make sure that you have installed the [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) on your system so Docker can pass through the GPU

&gt; [!TIP]
&gt; When using Docker you&#039;ll need to make the file/directory available to the container as well as the GPU:
&gt;  ```shell
&gt; docker run --rm --gpus all --mount type=bind,src=&lt;input video path&gt;,dst=/mnt ladaapp/lada:latest --input &quot;/mnt/&lt;input video file&gt;&quot;
&gt; ```

&gt; [!TIP]
&gt; If you want to use hardware encoders like `hevc_nvenc` you have to provide the container with `video` capability.
&gt; 
&gt; With docker run you can use `--gpus &#039;all,&quot;capabilities=compute,video&quot;&#039;`. Learn more [here](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/docker-specialized.html).

### Using Windows

For Windows users, the app (CLI and GUI) is packaged as a standalone .7z archive file.
You&#039;ll need [7-zip](https://7-zip.org/) to unpack the files. It is recommended to validate the file after downloading. See the Tip below.

Get the latest release from the [Releases Page](https://codeberg.org/ladaapp/lada/releases).

You&#039;ll find `lada.exe` and `lada-cli.exe` after extracting the archive.

&gt; [!NOTE]
&gt; The Windows release only works with x86_64 CPUs and Nvidia/CUDA GPUs (Turing or newer: RTX 20xx up to including RTX 50xx). Ensure your NVIDIA GPU driver is up-to-date.
&gt; It can also be used without a GPU but it will be very slow.

&gt; [!NOTE]
&gt; Be aware that the first start of lada.exe or lada-cli.exe could take a while before Windows Defender or your AV has scanned it. The next time you open the program it should start fast.

&gt; [!TIP]
&gt; It is recommended to compare the checksum of the downloaded file against the value you&#039;ll find in the release announcement.
&gt; This makes sure that you got the correct and unaltered file, especially important if you got the file from an unofficial source.
&gt; 
&gt; Calculate the checksum of the downloaded file on your computer and compare it against the `SHA256` value you&#039;ll find in the release announcement. They must be the same!
&gt; 
&gt; You can do this with Powershell `Get-FileHash /path/to/file.7z` or [QuickHash-GUI](https://www.quickhash-gui.org/).

### Alternative Installation Methods

If the packages above don&#039;t work for you then you&#039;ll have to follow the [Build](#build) steps to set up the project.

Note that these instructions are mostly intended for developers to set up their environment to start working on the source code. But you should hopefully be able
to follow the instructions even if you aren&#039;t a developer.

All packages currently only work with Nvidia cards (or CPU) but there have been reports that, following the Build instructions, newer Intel Xe GPUs and AMD ROCm-compatible cards work as well.

Reach out if you can support packaging the app for other operating systems or hardware.

## Contribute

You can find the Lada project [on GitHub](https://github.com/ladaapp/lada) and [on Codeberg](https://codeberg.org/ladaapp/lada).

The home of the project is on Codeberg. GitHub is set up only as a mirror so it&#039;s code will stay in sync with the main branch on Codeberg.

For contributing code, ideas or bug reports use [Pull requests](https://codeberg.org/ladaapp/lada/pulls) and the [Issue tracker](https://codeberg.org/ladaapp/lada/issues) on Codeberg.

If you want to help translating the app you can contribute to existing translations or set up a new language over at [Codeberg Translate](https://translate.codeberg.org/projects/lada/lada/).

[![Translation status](https://translate.codeberg.org/widget/lada/lada/multi-auto.svg)](https://translate.codeberg.org/engage/lada/)

## Releases

New releases will be published on both [GitHub Releases](https://github.com/ladaapp/lada/releases) and [Codeberg Releases](https://codeberg.org/ladaapp/lada/releases). You should get a notification about new releases if you star the project on either platform.

## Build

If you want to start hacking on this project you&#039;ll need to install the app from source. Check out the detailed installation guides for [Linux](docs/linux_install.md) and [Windows](docs/windows_install.md).

## Training and dataset creation

For instructions on training your own models and datasets, refer to [Training and dataset creation](docs/training_and_dataset_creation.md).

## License

Source code and models are licensed under AGPL-3.0. See the [LICENSE.md](LICENSE.md) file for full details.

## Acknowledgement
This project builds upon work done by these fantastic individuals and projects:

* [DeepMosaics](https://github.com/HypoX64/DeepMosaics): Provided code for mosaic dataset creation. Also inspired me to start this project.
* [BasicVSR++](https://ckkelvinchan.github.io/projects/BasicVSR++) / [MMagic](https://github.com/open-mmlab/mmagic): Used as the base model for mosaic removal.
* [YOLO/Ultralytics](https://github.com/ultralytics/ultralytics): Used for training mosaic and NSFW detection models.
* [DOVER](https://github.com/VQAssessment/DOVER):  Used to assess video quality of created clips during the dataset creation process to filter out low-quality clips.
* [DNN Watermark / PITA Dataset](https://github.com/tgenlis83/dnn-watermark): Used most of its code for creating a watermark detection dataset used to filter out scenes obstructed by text/watermarks/logos.
* [NudeNet](https://github.com/notAI-tech/NudeNet/): Used as an additional NSFW classifier to filter out false positives by our own NSFW segmentation model
* [Twitter Emoji](https://github.com/twitter/twemoji): Provided eggplant emoji as base for the app icon.
* [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN): Used their image degradation model design for our mosaic detection model degradation pipeline.
* [BPJDet](https://github.com/hnuzhy/BPJDet): Model for detecting human body and head. Used for creating SFW mosaics so that mosaic detection model can be trained so skip such material. 
* [CenterFace](https://github.com/Star-Clouds/CenterFace): Model for detecting human faces. Used for creating SFW mosaics so that mosaic detection model can be trained so skip such material. 
* PyTorch, FFmpeg, GStreamer, GTK and [all other folks building our ecosystem](https://xkcd.com/2347/)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[PennyLaneAI/pennylane]]></title>
            <link>https://github.com/PennyLaneAI/pennylane</link>
            <guid>https://github.com/PennyLaneAI/pennylane</guid>
            <pubDate>Mon, 05 Jan 2026 00:04:57 GMT</pubDate>
            <description><![CDATA[PennyLane is a cross-platform Python library for quantum computing, quantum machine learning, and quantum chemistry. Built by researchers, for research.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/PennyLaneAI/pennylane">PennyLaneAI/pennylane</a></h1>
            <p>PennyLane is a cross-platform Python library for quantum computing, quantum machine learning, and quantum chemistry. Built by researchers, for research.</p>
            <p>Language: Python</p>
            <p>Stars: 2,981</p>
            <p>Forks: 727</p>
            <p>Stars today: 12 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;!-- Tests (GitHub actions) --&gt;
  &lt;a href=&quot;https://github.com/PennyLaneAI/pennylane/actions?query=workflow%3ATests&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/PennyLaneAI/PennyLane/tests.yml?branch=master&amp;style=flat-square&quot; /&gt;
  &lt;/a&gt;
  &lt;!-- CodeCov --&gt;
  &lt;a href=&quot;https://codecov.io/gh/PennyLaneAI/pennylane&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/codecov/c/github/PennyLaneAI/pennylane/master.svg?logo=codecov&amp;style=flat-square&quot; /&gt;
  &lt;/a&gt;
  &lt;!-- ReadTheDocs --&gt;
  &lt;a href=&quot;https://docs.pennylane.ai/en/latest&quot;&gt;
    &lt;img src=&quot;https://readthedocs.com/projects/xanaduai-pennylane/badge/?version=latest&amp;style=flat-square&quot; /&gt;
  &lt;/a&gt;
  &lt;!-- PyPI --&gt;
  &lt;a href=&quot;https://pypi.org/project/PennyLane&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/PennyLane.svg?style=flat-square&quot; /&gt;
  &lt;/a&gt;
  &lt;!-- Forum --&gt;
  &lt;a href=&quot;https://discuss.pennylane.ai&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/discourse/https/discuss.pennylane.ai/posts.svg?logo=discourse&amp;style=flat-square&quot; /&gt;
  &lt;/a&gt;
  &lt;!-- License --&gt;
  &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/l/PennyLane.svg?logo=apache&amp;style=flat-square&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pennylane.ai&quot;&gt;PennyLane&lt;/a&gt; is a cross-platform Python library for
  &lt;a href=&quot;https://pennylane.ai/qml/quantum-computing/&quot;&gt;quantum computing&lt;/a&gt;,
  &lt;a href=&quot;https://pennylane.ai/qml/quantum-machine-learning/&quot;&gt;quantum machine learning&lt;/a&gt;,
  and
  &lt;a href=&quot;https://pennylane.ai/qml/quantum-chemistry/&quot;&gt;quantum chemistry&lt;/a&gt;.
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  The definitive open-source framework for quantum programming. Built by researchers, for research.
  &lt;img src=&quot;https://raw.githubusercontent.com/PennyLaneAI/pennylane/master/doc/_static/readme/pl-logo-lightmode.png#gh-light-mode-only&quot; width=&quot;700px&quot;&gt;
    &lt;!--
    Use a relative import for the dark mode image. When loading on PyPI, this
    will fail automatically and show nothing.
    --&gt;
    &lt;img src=&quot;./doc/_static/readme/pl-logo-darkmode.png#gh-dark-mode-only&quot; width=&quot;700px&quot; onerror=&quot;this.style.display=&#039;none&#039;&quot; alt=&quot;&quot;/&gt;
&lt;/p&gt;

## Key Features

&lt;img src=&quot;https://raw.githubusercontent.com/PennyLaneAI/pennylane/master/doc/_static/code.png&quot; width=&quot;400px&quot; align=&quot;right&quot;&gt;

- &lt;strong&gt;*Program quantum computers*&lt;/strong&gt;. Build quantum circuits with a wide range of state preparations, gates, and measurements. Run on [high-performance simulators](https://pennylane.ai/performance/) or [various hardware devices](https://pennylane.ai/plugins/), with advanced features like mid-circuit measurements and error mitigation.

- &lt;strong&gt;*Master quantum algorithms*&lt;/strong&gt;. From NISQ to fault-tolerant quantum computing, unlock algorithms for research and application. Analyze performance, visualize circuits, and access tools for [quantum chemistry](https://docs.pennylane.ai/en/stable/introduction/chemistry.html) and [algorithm development](https://pennylane.ai/search/?contentType=DEMO&amp;categories=algorithms&amp;sort=publication_date).

- &lt;strong&gt;*Machine learning with quantum hardware and simulators*&lt;/strong&gt;. Integrate with **PyTorch**, **TensorFlow**, **JAX**, **Keras**, or **NumPy** to define and train hybrid models using quantum-aware optimizers and hardware-compatible gradients for advanced research tasks. [Quantum machine learning quickstart](https://docs.pennylane.ai/en/stable/introduction/interfaces.html).


- &lt;strong&gt;*Quantum datasets*&lt;/strong&gt;. Access high-quality, pre-simulated datasets to decrease time-to-research and accelerate algorithm development. [Browse the datasets](https://pennylane.ai/datasets/) or contribute your own data.


- &lt;strong&gt;*Compilation and performance*&lt;/strong&gt;. Experimental support for just-in-time
  compilation. Compile your entire hybrid workflow, with support for 
  advanced features such as adaptive circuits, real-time measurement 
  feedback, and unbounded loops. See
  [Catalyst](https://github.com/pennylaneai/catalyst) for more details.

For more details and additional features, please see the [PennyLane website](https://pennylane.ai/features/).

## Installation

PennyLane requires Python version 3.11 and above. Installation of PennyLane, as well as all
dependencies, can be done using pip:

```console
python -m pip install pennylane
```

## Docker support

Docker images are found on the [PennyLane Docker Hub page](https://hub.docker.com/u/pennylaneai), where there is also a detailed description about PennyLane Docker support. [See description here](https://docs.pennylane.ai/projects/lightning/en/stable/dev/docker.html) for more information.

## Getting started

Get up and running quickly with PennyLane by following our [quickstart guide](https://docs.pennylane.ai/en/stable/introduction/pennylane.html), designed to introduce key features and help you start building quantum circuits right away.

Whether you&#039;re exploring quantum machine learning (QML), quantum computing, or quantum chemistry, PennyLane offers a wide range of tools and resources to support your research:

&lt;img src=&quot;https://raw.githubusercontent.com/PennyLaneAI/pennylane/master/doc/_static/readme/research.png&quot; align=&quot;right&quot; width=&quot;350px&quot;&gt;

### Key Resources:

* [Research-oriented Demos](https://pennylane.ai/qml/demonstrations)
* [Learn Quantum Programming](https://pennylane.ai/qml/) with the [Codebook](https://pennylane.ai/codebook/) and [Coding Challenges](https://pennylane.ai/challenges/)
* [Frequently Asked Questions](https://pennylane.ai/faq)
* [Glossary](https://pennylane.ai/qml/glossary)
* [Videos](https://pennylane.ai/qml/videos)


You can also check out our [documentation](https://pennylane.readthedocs.io) for [quickstart
guides](https://pennylane.readthedocs.io/en/stable/introduction/pennylane.html) to using PennyLane,
and detailed developer guides on [how to write your
own](https://pennylane.readthedocs.io/en/stable/development/plugins.html) PennyLane-compatible
quantum device.

## Demos

Take a deeper dive into quantum computing by exploring cutting-edge algorithms using PennyLane and quantum hardware. [Explore PennyLane demos](https://pennylane.ai/qml/demonstrations).

&lt;a href=&quot;https://pennylane.ai/qml/demonstrations&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/PennyLaneAI/pennylane/master/doc/_static/readme/demos.png&quot; width=&quot;900px&quot;&gt;
&lt;/a&gt;

If you would like to contribute your own demo, see our [demo submission
guide](https://pennylane.ai/qml/demos_submission).

## Research Applications

PennyLane is at the forefront of research in quantum computing, quantum machine learning, and quantum chemistry. Explore how PennyLane is used for research in the following publications:

- **Quantum Computing**: [Fast quantum circuit cutting with randomized measurements](https://quantum-journal.org/papers/q-2023-03-02-934/)

- **Quantum Machine Learning**: [Better than classical? The subtle art of benchmarking quantum machine learning models](https://arxiv.org/abs/2403.07059)

- **Quantum Chemistry**: [Accelerating Quantum Computations of Chemistry Through Regularized Compressed Double Factorization](https://quantum-journal.org/papers/q-2024-06-13-1371/)

Impactful research drives PennyLane. Let us know what features you need for your research on [GitHub](https://github.com/PennyLaneAI/pennylane/issues/new?assignees=&amp;labels=enhancement+%3Asparkles%3A&amp;projects=&amp;template=feature_request.yml) or on our [website](https://pennylane.ai/research).



## Contributing to PennyLane

We welcome contributions‚Äîsimply fork the PennyLane repository, and then make a [pull
request](https://help.github.com/articles/about-pull-requests/) containing your contribution. All
contributors to PennyLane will be listed as authors on the releases. All users who contribute
significantly to the code (new plugins, new functionality, etc.) will be listed on the PennyLane
arXiv paper.

We also encourage bug reports, suggestions for new features and enhancements, and even links to cool
projects or applications built on PennyLane.

See our [contributions
page](https://github.com/PennyLaneAI/pennylane/blob/master/.github/CONTRIBUTING.md) and our
[Development guide](https://pennylane.readthedocs.io/en/stable/development/guide.html) for more
details.

## Support

- **Source Code:** https://github.com/PennyLaneAI/pennylane
- **Issue Tracker:** https://github.com/PennyLaneAI/pennylane/issues

If you are having issues, please let us know by posting the issue on our GitHub issue tracker.

Join the [PennyLane Discussion Forum](https://discuss.pennylane.ai/) to connect with the quantum community, get support, and engage directly with our team. It‚Äôs the perfect place to share ideas, ask questions, and collaborate with fellow researchers and developers!

Note that we are committed to providing a friendly, safe, and welcoming environment for all.
Please read and respect the [Code of Conduct](.github/CODE_OF_CONDUCT.md).

## Authors

PennyLane is the work of [many contributors](https://github.com/PennyLaneAI/pennylane/graphs/contributors).

If you are doing research using PennyLane, please cite [our paper](https://arxiv.org/abs/1811.04968):

&gt; Ville Bergholm et al. *PennyLane: Automatic differentiation of hybrid quantum-classical
&gt; computations.* 2018. arXiv:1811.04968

## License

PennyLane is **free** and **open source**, released under the Apache License, Version 2.0.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[PrimeIntellect-ai/verifiers]]></title>
            <link>https://github.com/PrimeIntellect-ai/verifiers</link>
            <guid>https://github.com/PrimeIntellect-ai/verifiers</guid>
            <pubDate>Mon, 05 Jan 2026 00:04:56 GMT</pubDate>
            <description><![CDATA[Our library for RL environments + evals]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/PrimeIntellect-ai/verifiers">PrimeIntellect-ai/verifiers</a></h1>
            <p>Our library for RL environments + evals</p>
            <p>Language: Python</p>
            <p>Stars: 3,695</p>
            <p>Forks: 463</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/user-attachments/assets/40c36e38-c5bd-4c5a-9cb3-f7b902cd155d&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/user-attachments/assets/6414bc9b-126b-41ca-9307-9e982430cde8&quot;&gt;
    &lt;img alt=&quot;Prime Intellect&quot; src=&quot;https://github.com/user-attachments/assets/6414bc9b-126b-41ca-9307-9e982430cde8&quot; width=&quot;312&quot; style=&quot;max-width: 100%;&quot;&gt;
  &lt;/picture&gt;
&lt;/p&gt;

---

&lt;h3 align=&quot;center&quot;&gt;
Verifiers: Environments for LLM Reinforcement Learning
&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://docs.primeintellect.ai/verifiers&quot;&gt;Documentation&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://app.primeintellect.ai/dashboard/environments?ex_sort=most_stars&quot;&gt;Environments Hub&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://github.com/PrimeIntellect-ai/prime-rl&quot;&gt;PRIME-RL&lt;/a&gt;
&lt;/p&gt;

---

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/PrimeIntellect-ai/verifiers/actions/workflows/style.yml&quot;&gt;
    &lt;img src=&quot;https://github.com/PrimeIntellect-ai/verifiers/actions/workflows/style.yml/badge.svg&quot; alt=&quot;Style&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/PrimeIntellect-ai/verifiers/actions/workflows/test.yml&quot;&gt;
    &lt;img src=&quot;https://github.com/PrimeIntellect-ai/verifiers/actions/workflows/test.yml/badge.svg&quot; alt=&quot;Test&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/PrimeIntellect-ai/verifiers/actions/workflows/publish-envs.yml&quot;&gt;
    &lt;img src=&quot;https://github.com/PrimeIntellect-ai/verifiers/actions/workflows/publish-envs.yml/badge.svg&quot; alt=&quot;Envs&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

&gt; Note: This repository hosts community-contributed, lightly reviewed environment implementations. Environments maintained by the Prime Intellect research team are available at [research-environments](https://github.com/PrimeIntellect-ai/research-environments/tree/main).

## News &amp; Updates

- [11/19/25] v0.1.8 is released, featuring a major refactor of the rollout system to use trajectory-based tracking for token-in token-out training across turns, as well as support for truncated or branching rollouts.
- [11/07/25] Verifiers v0.1.7 is released! This includes an improved quickstart configuration for training with [prime-rl], a new included &quot;nano&quot; trainer (`vf.RLTrainer`, replacing `vf.GRPOTrainer`), and a number of bug fixes and improvements to the documentation.
- [10/27/25] A new iteration of the Prime Intellect [Environments Program](https://docs.google.com/spreadsheets/d/13UDfRDjgIZXsMI2s9-Lmn8KSMMsgk2_zsfju6cx_pNU/edit?gid=0#gid=0) is live!  

## Overview

Verifiers is a library of modular components for creating RL environments and training LLM agents. Environments built with Verifiers can be used directly as LLM evaluations, synthetic data pipelines, or agent harnesses for any OpenAI-compatible model endpoint, in addition to RL training. Verifiers is supported by `prime-rl` for large-scale performance-optimized async RL training, includes a minimal `transformers`-based trainer (`vf.RLTrainer`) for simple algorithmic experiments, and can easily be integrated into any RL training stack which exposes an OpenAI-compatible inference client.

Full documentation is available [here](https://docs.primeintellect.ai/verifiers). 

Verifiers is the native library used by Prime Intellect&#039;s [Environments Hub](https://app.primeintellect.ai/dashboard/environments?ex_sort=most_stars); see [here](https://docs.primeintellect.ai/tutorials-environments/environments) for information about publishing your Environments to the Hub, and [here](https://github.com/PrimeIntellect-ai/prime-environments) for a collection of Environments built with Verifiers.

## Quick Start

Verifiers supports CPU-based environment development and evaluation with API models, as well as large-scale GPU-based RL training with [`prime-rl`](https://github.com/PrimeIntellect-ai/prime-rl) and several other trainers. Environments built with Verifiers are standalone Python packages that can be installed and used in your own projects, or shared with the community through the [Environments Hub](https://app.primeintellect.ai/dashboard/environments?ex_sort=most_stars).


To get started, install `uv` and the `prime` [CLI](https://github.com/PrimeIntellect-ai/prime-cli), and add `verifiers` to your project:
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
uv init &amp;&amp; uv venv --python 3.12    # to create a new project if needed
uv tool install prime
uv add verifiers
```

Select an environment from the [Environments Hub](https://app.primeintellect.ai/dashboard/environments?ex_sort=most_stars) to install:
```bash
prime env install will/wiki-search
```

Or install an environment from this repo:
```bash
uv run vf-install wordle --from-repo
```

Run a quick evaluation with OpenAI models:
```bash
uv run vf-eval wordle -m gpt-5-nano
```

For advanced evaluation configurations with the `prime` [CLI](https://github.com/PrimeIntellect-ai/prime-cli), see [here](https://docs.primeintellect.ai/tutorials-environments/evaluating)

## RL Training

### `prime-rl`
We recommend using the [`prime-rl`](https://github.com/PrimeIntellect-ai/prime-rl) trainer, and provide a basic setup guide below. See the [prime-rl documentation](https://docs.primeintellect.ai/prime-rl) for more information.

To get started, do: 

```bash
uv run vf-setup
```

This will clone and install the `prime-rl` trainer and its dependencies, and set up a default configuration for training with the included `wiki-search` Environment.

Then, you can start training with:

```bash
uv run prime-rl @ configs/prime-rl/wiki-search.toml
```

This will launch a tmux session with separate panes for the trainer, orchestrator, and inference server.

### vf.RLTrainer

The included `RLTrainer` is a minimal, hackable training loop based on `transformers.Trainer` that supports both full-parameter finetuning and LoRA training. `RLTrainer` can be viewed as a &quot;baby&quot; `prime-rl` that adopts a similar default training recipe (async CISPO with one-step off-policy overlap), intended for single-node test runs with dense models. The primary files (`trainer.py` and `orchestrator.py`, located in `verifiers/rl/trainer/`) are under 1000 lines of code, and are designed to be a convenient starting point for writing your own training loop.

The feature set is intentionally kept minimal and focused. Users seeking maximum performance, MoE support, multi-node training, multidimensional parallelism, and other advanced features should use the `prime-rl` trainer. 

To use `vf.RLTrainer` in your own project, install with RL extras:
```bash
uv add &#039;verifiers[rl]&#039;
```

Then, create a training configuration file, e.g. `configs/vf-rl/wiki-search.toml`, and do:

```bash
uv run vf-rl @ configs/vf-rl/wiki-search.toml
```

Example configuration files can be created in your project by running `uv run vf-setup`.

### Other Trainers

`verifiers` is intended to be largely trainer-agnostic. It is supported by [SkyRL] and [Tinker], and is straightforward to support for any trainer which can expose an OpenAI-compatible inference client for rollouts. See the [integrations](https://github.com/PrimeIntellect-ai/verifiers/tree/main/integrations) directory for more information.


## Development

To install `verifiers` from source for core library development, or to use the latest `main` branch, install with:
```bash
curl -sSL https://raw.githubusercontent.com/PrimeIntellect-ai/verifiers/main/scripts/install.sh | bash
```

If you want to develop with RL extras enabled in this repo, do:
```bash
uv sync --extra rl
```

Please use the [Environments Hub](https://app.primeintellect.ai/dashboard/environments?ex_sort=most_stars) to share your Environments with the community, rather than PRs to this repo. If you find yourself needing to clone and modify the core library in order to implement key functionality for your project, please open an issue or PR so that we can help you.


## Environments

Environments in Verifiers are installable Python modules which can specify dependencies in a `pyproject.toml`, and which expose a `load_environment` function for instantiation by downstream applications (e.g. trainers). See `environments/` for examples. 

To initialize a blank Environment module template, do:
```bash
uv run vf-init environment-name # -p /path/to/environments (defaults to &quot;./environments&quot;)
```

To install an Environment module into your project, do:
```bash
uv run vf-install environment-name # -p /path/to/environments (defaults to &quot;./environments&quot;) 
```

To install an Environment module from the [Environments Hub](https://app.primeintellect.ai/dashboard/environments?ex_sort=most_stars), do:
```bash
prime env install user/environment-name
```

To install an Environment module from this repo&#039;s `environments` folder, do:
```bash
uv run vf-install math-python --from-repo # -b branch_or_commit (defaults to &quot;main&quot;)
```

Once an Environment module is installed, you can create an instance of the Environment using `load_environment`, passing any necessary args:
```python
import verifiers as vf
vf_env = vf.load_environment(&quot;environment-name&quot;, **env_args)
```

To run a quick evaluation of your Environment with an API-based model, do:
```bash
uv run vf-eval environment-name -s # run and save eval results locally
# vf-eval -h for config options; defaults to gpt-4.1-mini, 5 prompts, 3 rollouts for each
```

If you&#039;re using Prime Intellect infrastructure, the [`prime` CLI](https://github.com/PrimeIntellect-ai/prime-cli) provides first-class commands for working with Verifiers environments through the [Environments Hub](https://docs.primeintellect.ai/tutorials-environments/environments). Install it with `uv tool install prime`, authenticate via `prime login`, then use `prime env push` to publish your package and `prime env install owner/name` (optionally pinning a version) to consume it from pods or local machines.

The core elements of Environments are:
- Datasets: a Hugging Face `Dataset` with a `prompt` column for inputs, and optionally `answer (str)` or `info (dict)` columns for evaluation (both can be omitted for environments that evaluate based solely on completion quality)
- Rollout logic: interactions between models and the environment (e.g. `env_response` + `is_completed` for any `MultiTurnEnv`)
- Rubrics: an encapsulation for one or more reward functions
- Parsers: optional; an encapsulation for reusable parsing logic

We support both `/v1/chat/completions`-style and `/v1/completions`-style inference via OpenAI clients, though we generally recommend `/v1/chat/completions`-style inference for the vast majority of applications. Both  `prime-rl` as well as the included `vf.RLTrainer` support the full set of [SamplingParams](https://docs.vllm.ai/en/stable/api/vllm/sampling_params.html#vllm.sampling_params.SamplingParams) exposed by vLLM (via their OpenAI-compatible [server](https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html) interface), and leveraging this will often be the appropriate way to implement rollout strategies requiring finer-grained control, such as interrupting and resuming generations for interleaved tool use, or enforcing reasoning budgets.


### SingleTurnEnv

For tasks requiring only a single response from a model for each prompt, you can use `SingleTurnEnv` directly by specifying a Dataset and a Rubric. Rubrics are sets of reward functions, which can be either sync or async.

```python
from datasets import load_dataset
import verifiers as vf

dataset = load_dataset(&quot;my-account/my-dataset&quot;, split=&quot;train&quot;)

def reward_A(prompt, completion, info) -&gt; float:
	# reward fn, e.g. correctness
	...

def reward_B(parser, completion) -&gt; float:
	# auxiliary reward fn, e.g. format
	...

async def metric(completion) -&gt; float:
	# non-reward metric, e.g. proper noun count
	...

rubric = vf.Rubric(funcs=[reward_A, reward_B, metric], weights=[1.0, 0.5, 0.0])

vf_env = vf.SingleTurnEnv(
	dataset=dataset,
	rubric=rubric
)

# Async evaluation (recommended)
from openai import AsyncOpenAI
results = await vf_env.evaluate(client=AsyncOpenAI(), model=&quot;gpt-4.1-mini&quot;, num_examples=100, rollouts_per_example=1)

# Sync evaluation
from openai import OpenAI
results = vf_env.evaluate_sync(client=OpenAI(), model=&quot;gpt-4.1-mini&quot;, num_examples=100, rollouts_per_example=1)

vf_env.make_dataset(results) # HF dataset format
```

Datasets should be formatted with columns for:
- `&#039;prompt&#039; (List[ChatMessage])` OR `&#039;question&#039; (str)` fields
	- `ChatMessage` = e.g. `{&#039;role&#039;: &#039;user&#039;, &#039;content&#039;: &#039;...&#039;}`
	- if `question` is set instead of `prompt`, you can also pass `system_prompt (str)` and/or `few_shot (List[ChatMessage])`
- `answer (str)` AND/OR `info (dict)` (both optional, can be omitted entirely)
- `task (str)`: optional, used by `EnvGroup` and `RubricGroup` for orchestrating composition of Environments and Rubrics

The following named attributes available for use by reward functions in your Rubric:
- `prompt`: sequence of input messages
- `completion`: sequence of messages generated during rollout by model and Environment
- `answer`: primary answer column, optional (defaults to empty string if omitted)
- `state`: can be modified during rollout to accumulate any metadata (`state[&#039;trajectory&#039;]` includes the full list of `TrajectoryStep` objects by default)
- `info`: auxiliary info needed for reward computation (e.g. test cases), optional (defaults to empty dict if omitted)
- `task`: tag for task type (used by `EnvGroup` and `RubricGroup`)
- `parser`: the parser object declared. Note: `vf.Parser().get_format_reward_func()` is a no-op (always 1.0); use `vf.ThinkParser` or a custom parser if you want a real format adherence reward.

**Note**: Some environments can fully evaluate using only `prompt`, `completion`, and `state` without requiring ground truth `answer` or `info` data. Examples include format compliance checking, completion quality assessment, or length-based rewards.

For tasks involving LLM judges, you may wish to use `vf.JudgeRubric()` for managing requests to auxiliary models.


### ToolEnv

For many applications involving tool use, you can use `ToolEnv` to leverage models&#039; native tool/function-calling capabilities in an agentic loop. Tools must be stateless and idempotent‚Äîeach call should be fully determined by the provided arguments‚Äîbecause the environment will automatically terminate once the assistant responds without tool calls. Tools can be specified as generic Python functions (with type hints and docstrings), which will then be passed in JSON schema form to each inference request.


```python
import verifiers as vf
vf_env = vf.ToolEnv(
	dataset= ... # HF Dataset with &#039;prompt&#039;/&#039;question&#039; and optionally &#039;answer&#039;/&#039;info&#039; columns
	rubric= ... # Rubric object; vf.ToolRubric() can be optionally used for counting tool invocations in each rollout
	tools=[search_tool, read_article_tool, python_tool], # python functions with type hints + docstrings
	max_turns=10
)
```

In cases where your tools require heavy computational resources, we recommend hosting your tools as standalone servers (e.g. MCP servers) and creating lightweight wrapper functions to pass to `ToolEnv`. Parallel tool call support is enabled by default. If you need to inject per-rollout or cross-call state (IDs, credentials, cached resources), promote the environment to `StatefulToolEnv` and populate that state through `setup_state`/`update_tool_args` instead of hiding globals.

#### StatefulToolEnv

`StatefulToolEnv` extends `ToolEnv` for workflows where tool calls must incorporate dynamic state (for example, sandbox handles or per-user secrets). Implement `setup_state` to seed the state dict and override `update_tool_args` to merge state into each tool invocation. Any arguments you strip from the OpenAI schema via `args_to_skip` should be tracked in `skipped_args` so the model never sees sensitive parameters. Avoid storing global state; keep everything in the provided `state` dict.

#### SandboxEnv &amp; PythonEnv

`SandboxEnv` builds on `StatefulToolEnv` to coordinate long-running sandboxes. Queue heavyweight provisioning inside `setup_state` (without awaiting) and gate tool execution on readiness inside `update_tool_args` or the tools themselves. `PythonEnv` is a concrete sandboxed executor that demonstrates the pattern: it spins up a Prime sandbox, injects the sandbox ID into each tool call, and tears down resources when the rollout finishes. Treat both environments as references when building similar stateful tool workflows.

For training, or self-hosted endpoints, you&#039;ll want to enable auto tool choice in [vLLM](https://docs.vllm.ai/en/stable/features/tool_calling.html#automatic-function-calling) with the appropriate parser. If your model does not support native tool calling, you may find the `XMLParser` abstraction useful for rolling your own tool call parsing on top of `MultiTurnEnv`; see `environments/xml_tool_env` for an example.

### MultiTurnEnv

Both `SingleTurnEnv` and `ToolEnv` are instances of `MultiTurnEnv`, which exposes an interface for writing custom Environment interaction protocols. To implement a custom protocol, define an `env_response` method and use `@vf.stop` decorators for termination conditions.

```python
import verifiers as vf
from verifiers.types import Messages, State

class YourMultiTurnEnv(vf.MultiTurnEnv):
    def __init__(self,
                 dataset: Dataset,
                 rubric: Rubric,
				 max_turns: int,
                 **kwargs):

    async def env_response(self, messages: Messages, state: State, **kwargs) -&gt; Messages:
        # return new environment message(s); state can be updated in-place
        return [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;feedback&quot;}]

    @vf.stop
    async def task_complete(self, state: State) -&gt; bool:
        # return whether or not a rollout is completed
        return state.get(&quot;task_complete&quot;, False)
```

If your application requires more fine-grained control than is allowed by `MultiTurnEnv`, you may want to inherit from the base `Environment` functionality directly and override the `rollout` method.


### Troubleshooting 
- Ensure your `wandb` and `huggingface-cli` logins are set up (or set `report_to=None` in `training_args`). You should also have something set as your `OPENAI_API_KEY` in your environment (can be a dummy key for vLLM). 
- If using high max concurrency, increase the number of allowed open sockets (e.g. `ulimit -n 4096`)
- On some setups, inter-GPU communication can [hang](https://github.com/huggingface/trl/issues/2923) or crash during vLLM weight syncing. This can usually be alleviated by setting (or unsetting) `NCCL_P2P_DISABLE=1` in your environment (or potentially `NCCL_CUMEM_ENABLE=1`). Try this as your first step if you experience NCCL-related issues.
- If problems persist, please open an [issue](https://github.com/PrimeIntellect-ai/verifiers/issues).

### Resource Requirements
`prime-rl` can be run on a single GPU by allocating only a fraction of the available memory to the inference server (see [here](https://github.com/PrimeIntellect-ai/prime-rl/tree/main/examples/alphabet_sort) for an example configuration), and can also be scaled to hundreds of GPUs for large-scale training. A wide range of competitively-priced cluster configurations are available on [Prime Intellect](https://app.primeintellect.ai/dashboard/create-cluster?image=ubuntu_22_cuda_12).


## Citation

Originally created by Will Brown ([@willccbb](https://github.com/willccbb)).

If you use this code in your research, please cite:

```bibtex
@misc{brown_verifiers_2025,
  author       = {William¬†Brown},
  title        = {{Verifiers}: Environments for LLM Reinforcement Learning},
  howpublished = {\url{https://github.com/PrimeIntellect-ai/verifiers}},
  note         = {Commit abcdefg ‚Ä¢ accessed DD‚ÄØMon‚ÄØYYYY},
  year         = {2025}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[OpenMind/OM1]]></title>
            <link>https://github.com/OpenMind/OM1</link>
            <guid>https://github.com/OpenMind/OM1</guid>
            <pubDate>Mon, 05 Jan 2026 00:04:55 GMT</pubDate>
            <description><![CDATA[Modular AI runtime for robots]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenMind/OM1">OpenMind/OM1</a></h1>
            <p>Modular AI runtime for robots</p>
            <p>Language: Python</p>
            <p>Stars: 2,373</p>
            <p>Forks: 666</p>
            <p>Stars today: 23 stars today</p>
            <h2>README</h2><pre>![OM_Banner_X2 (1)](https://github.com/user-attachments/assets/853153b7-351a-433d-9e1a-d257b781f93c)

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://arxiv.org/abs/2412.18588&quot;&gt;Technical Paper&lt;/a&gt; |
&lt;a href=&quot;https://docs.openmind.org/&quot;&gt;Documentation&lt;/a&gt; |
&lt;a href=&quot;https://x.com/openmind_agi&quot;&gt;X&lt;/a&gt; |
&lt;a href=&quot;https://discord.gg/openmind&quot;&gt;Discord&lt;/a&gt;
&lt;/p&gt;

**OpenMind&#039;s OM1 is a modular AI runtime that empowers developers to create and deploy multimodal AI agents across digital environments and physical robots**, including Humanoids, Phone Apps, websites, Quadrupeds, and educational robots such as TurtleBot 4. OM1 agents can process diverse inputs like web data, social media, camera feeds, and LIDAR, while enabling physical actions including motion, autonomous navigation, and natural conversations. The goal of OM1 is to make it easy to create highly capable human-focused robots, that are easy to upgrade and (re)configure to accommodate different physical form factors.

## Capabilities of OM1

* **Modular Architecture**: Designed with Python for simplicity and seamless integration.
* **Data Input**: Easily handles new data and sensors.
* **Hardware Support via Plugins**: Supports new hardware through plugins for API endpoints and specific robot hardware connections to `ROS2`, `Zenoh`, and `CycloneDDS`. (We recommend `Zenoh` for all new development).
* **Web-Based Debugging Display**: Monitor the system in action with WebSim (available at http://localhost:8000/) for easy visual debugging.
* **Pre-configured Endpoints**: Supports Text-to-Speech, multiple LLMs from OpenAI, xAI, DeepSeek, Anthropic, Meta, Gemini, NearAI and multiple Visual Language Models (VLMs) with pre-configured endpoints for each service.

## Architecture Overview
![Artboard 1@4x 1 (1)](https://github.com/user-attachments/assets/dd91457d-010f-43d8-960e-d1165834aa58)


## Getting Started

To get started with OM1, let&#039;s run the Spot agent. Spot uses your webcam to capture and label objects. These text captions are then sent to the LLM, which returns `movement`, `speech` and `face` action commands. These commands are displayed on WebSim along with basic timing and other debugging information.

### Package Management and VENV

You will need the [`uv` package manager](https://docs.astral.sh/uv/getting-started/installation/).

### Clone the Repo

```bash
git clone https://github.com/OpenMind/OM1.git
cd OM1
git submodule update --init
uv venv
```

### Install Dependencies

For MacOS
```bash
brew install portaudio ffmpeg
```

For Linux
```bash
sudo apt-get update
sudo apt-get install portaudio19-dev python-dev ffmpeg
```

### Obtain an OpenMind API Key

Obtain your API Key at [OpenMind Portal](https://portal.openmind.org/). Copy it to `config/spot.json5`, replacing the `openmind_free` placeholder. Or, `cp env.example .env` and add your key to the `.env`.

### Launching OM1

Run
```bash
uv run src/run.py spot
```

After launching OM1, the Spot agent will interact with you and perform (simulated) actions. For more help connecting OM1 to your robot hardware, see [getting started](https://docs.openmind.org/developing/1_get-started).

Note: This is just an example agent configuration.
If you want to interact with the agent and see how it works, make sure ASR and TTS are configured in spot.json5.

## What&#039;s Next?

* Try out some [examples](https://docs.openmind.org/examples)
* Add new `inputs` and `actions`.
* Design custom agents and robots by creating your own `json5` config files with custom combinations of inputs and actions.
* Change the system prompts in the configuration files (located in `/config/`) to create new behaviors.

## Interfacing with New Robot Hardware

OM1 assumes that robot hardware provides a high-level SDK that accepts elemental movement and action commands such as `backflip`, `run`, `gently pick up the red apple`, `move(0.37, 0, 0)`, and `smile`. An example is provided in `src/actions/move/connector/ros2.py`:

```python
...
elif output_interface.action == &quot;shake paw&quot;:
    if self.sport_client:
        self.sport_client.Hello()
...
```

If your robot hardware does not yet provide a suitable HAL (hardware abstraction layer), traditional robotics approaches such as RL (reinforcement learning) in concert with suitable simulation environments (Unity, Gazebo), sensors (such as hand mounted ZED depth cameras), and custom VLAs will be needed for you to create one. It is further assumed that your HAL accepts motion trajectories, provides battery and thermal management/monitoring, and calibrates and tunes sensors such as IMUs, LIDARs, and magnetometers.

OM1 can interface with your HAL via USB, serial, ROS2, CycloneDDS, Zenoh, or websockets. For an example of an advanced humanoid HAL, please see [Unitree&#039;s C++ SDK](https://github.com/unitreerobotics/unitree_sdk2/blob/adee312b081c656ecd0bb4e936eed96325546296/example/g1/high_level/g1_loco_client_example.cpp#L159). Frequently, a HAL, especially ROS2 code, will be dockerized and can then interface with OM1 through DDS middleware or websockets.

## Recommended Development Platforms

OM1 is developed on:

* Nvidia Thor (running JetPak 7.0) - full support
* Jetson AGX Orin 64GB (running Ubuntu 22.04 and JetPack 6.1) - limited support
* Mac Studio with Apple M2 Ultra with 48 GB unified memory (running MacOS Sequoia)
* Mac Mini with Apple M4 Pro with 48 GB unified memory (running MacOS Sequoia)
* Generic Linux machines (running Ubuntu 22.04)

OM1 _should_ run on other platforms (such as Windows) and microcontrollers such as the Raspberry Pi 5 16GB.


## Full Autonomy Guidance

We&#039;re excited to introduce **full autonomy** for Unitree Go2 and G1. Full autonomy has four services that work together in a loop without manual intervention:

- **om1**
- **unitree_sdk** ‚Äì A ROS 2 package that provides SLAM (Simultaneous Localization and Mapping) capabilities for the Unitree Go2 robot using an RPLiDAR sensor, the SLAM Toolbox and the Nav2 stack.
- **om1-avatar** ‚Äì A modern React-based frontend application that provides the user interface and avatar display system for OM1 robotics software.
- **om1-video-processor** - The OM1 Video Processor is a Docker-based solution that enables real-time video streaming, face recognition, and audio capture for OM1 robots.

## Intro to BrainPack?
From research to real-world autonomy, a platform that learns, moves, and builds with you.
We&#039;ll shortly be releasing the **BOM** and details on **DIY** for it.
Stay tuned!

Clone the following repos -
- https://github.com/OpenMind/OM1.git
- https://github.com/OpenMind/unitree-sdk.git
- https://github.com/OpenMind/OM1-avatar.git
- https://github.com/OpenMind/OM1-video-processor.git

## Starting the system
To start all services, run the following commands:
- For OM1

Setup the API key

For Bash: vim ~/.bashrc or ~/.bash_profile.

For Zsh: vim ~/.zshrc.

Add

```bash
export OM_API_KEY=&quot;your_api_key&quot;
```

Update the docker-compose file. Replace &quot;unitree_go2_autonomy_advance&quot; with the agent you want to run.
```bash
command: [&quot;unitree_go2_autonomy_advance&quot;]
```

```bash
cd OM1
docker compose up om1 -d --no-build
```

- For unitree_sdk
```bash
cd unitree_sdk
docker compose up orchestrator -d --no-build
docker compose up om1_sensor -d --no-build
docker compose up watchdog -d --no-build
docker compose up zenoh_bridge -d --no-build
```

- For OM1-avatar
```bash
cd OM1-avatar
docker compose up om1_avatar -d --no-build
```

- For OM1-video-processor
```bash
cd OM1-video-processor
docker compose up -d
```

## Detailed Documentation

More detailed documentation can be accessed at [docs.openmind.org](https://docs.openmind.org/).

## Contributing

Please make sure to read the [Contributing Guide](./CONTRIBUTING.md) before making a pull request.

## License

This project is licensed under the terms of the MIT License, which is a permissive free software license that allows users to freely use, modify, and distribute the software. The MIT License is a widely used and well-established license that is known for its simplicity and flexibility. By using the MIT License, this project aims to encourage collaboration, modification, and distribution of the software.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sherlock-project/sherlock]]></title>
            <link>https://github.com/sherlock-project/sherlock</link>
            <guid>https://github.com/sherlock-project/sherlock</guid>
            <pubDate>Mon, 05 Jan 2026 00:04:54 GMT</pubDate>
            <description><![CDATA[Hunt down social media accounts by username across social networks]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sherlock-project/sherlock">sherlock-project/sherlock</a></h1>
            <p>Hunt down social media accounts by username across social networks</p>
            <p>Language: Python</p>
            <p>Stars: 71,403</p>
            <p>Forks: 8,437</p>
            <p>Stars today: 32 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[wasmerio/Python-Scripts]]></title>
            <link>https://github.com/wasmerio/Python-Scripts</link>
            <guid>https://github.com/wasmerio/Python-Scripts</guid>
            <pubDate>Mon, 05 Jan 2026 00:04:53 GMT</pubDate>
            <description><![CDATA[A curated list of python scripts for automating your tasks]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/wasmerio/Python-Scripts">wasmerio/Python-Scripts</a></h1>
            <p>A curated list of python scripts for automating your tasks</p>
            <p>Language: Python</p>
            <p>Stars: 1,218</p>
            <p>Forks: 440</p>
            <p>Stars today: 80 stars today</p>
            <h2>README</h2><pre># Table of Contents

- [Contributing](#contributing)
  - [Adding a New Script](#adding-a-new-script)
- [List of Scripts in Repo](#list-of-scripts-in-repo)
- [Gitpod](#gitpod)
- [Wall of Contributors](#wall-of-contributors)

 
[![Join Our Discord](https://img.shields.io/badge/Discord-Join%20Server-blue?logo=discord&amp;style=for-the-badge)](https://discord.com/invite/Yn9g6KuWyA)
[![Subscribe on YouTube](https://img.shields.io/badge/YouTube-Subscribe-red?logo=youtube&amp;style=for-the-badge)](https://www.youtube.com/@dhanushnehru?sub_confirmation=1)
[![Subscribe to Newsletter](https://img.shields.io/badge/Newsletter-Subscribe-orange?style=for-the-badge)](https://dhanushn.substack.com/)

# Python Scripts

This repository consists of a list of more than 60 Python scripts, primarily those which automate a specific task. Each folder contains one or more .py files and a README to explain what that specific Python script needs to run. These scripts are free to use as long as long as the original contributor is credited.

## Contributing

We encourage contributions from the community to make this repository even more valuable. Whether you want to add a new Python script or enhance an existing one, we welcome your input. Here&#039;s how you can contribute:

_**Note: Please follow the maintainer of the repository for quick approval of the PR&#039;s via [Twitter](https://twitter.com/Dhanush_Nehru), [Instagram](https://www.instagram.com/dhanush_nehru/), [Youtube](https://www.youtube.com/@dhanushnehru?sub_confirmation=1), [Github](https://github.com/DhanushNehru)**_

### Adding a New Script

**1. Create an issue:** Start by creating an issue in this repository. Describe the new script you&#039;d like to contribute, its purpose, and any specific features it will provide.

**2. Fork the repository:** Fork this repository to your own GitHub account to create a copy you can work on.

**3. Create a Pull Request (PR):** In your forked repository, develop the Python script and include a `README` file explaining how to use it. Ensure you credit the original contributor if you&#039;re building upon an existing script.

**4. Update the List:** If you&#039;re adding a new script, make sure to include it in the &quot;List of Python Scripts&quot; section below, providing a title, a link to the script&#039;s folder, and a brief description.

**NOTE:** Remember to close your issues once your PR has been merged! They can be reopened if needed.

More information on contributing and the general code of conduct for discussion and feedback in this repo can be found here: [CONTRIBUTIONS.md](https://github.com/DhanushNehru/Python-Scripts/blob/main/CONTRIBUTIONS.md)

## List of Scripts in Repo

| Script                                   | Link                                                                                                                                                     | Description                                                                                                                                                       |
| ---------------------------------------- |----------------------------------------------------------------------------------------------------------------------------------------------------------| ----------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Arrange It                               | [Arrange It](https://github.com/DhanushNehru/Python-Scripts/tree/main/Arrange%20It)                                                                    | A Python script that can automatically move files into corresponding folders based on their extensions.                                                           |
| Auto WiFi Check                          | [Auto WiFi Check](https://github.com/DhanushNehru/Python-Scripts/tree/main/Auto%20WiFi%20Check)                                                        | A Python script to monitor if the WiFi connection is active or not                                                                                                |
| AutoCert                                 | [AutoCert](https://github.com/DhanushNehru/Python-Scripts/tree/main/AutoCert)                                                                          | A Python script to auto-generate e-certificates in bulk.                                                                                                          |
| Autocomplete Notes App                   | [Autocomplete Notes App](https://github.com/DhanushNehru/Python-Scripts/tree/main/Autocomplete%20Notes%20App)                                                        | A Python script that provides a notes application with autocomplete features.                                                                                                          |
| Automated Emails                         | [Automated Emails](https://github.com/DhanushNehru/Python-Scripts/tree/main/Automate%20Emails%20Daily)                                                 | A Python script to send out personalized emails by reading a CSV file.                                                                                            |
| Black Hat Python                         | [Black Hat Python](https://github.com/DhanushNehru/Python-Scripts/tree/main/Black%20Hat%20Python)                                                      | Source code from the book Black Hat Python                                                                                                                        |
| Blackjack                                | [Blackjack](https://github.com/DhanushNehru/Python-Scripts/tree/main/Blackjack)                                                                        | A game of Blackjack - let&#039;s get a 21.                                                                                                                             |
| Chessboard                               | [Chessboard](https://github.com/DhanushNehru/Python-Scripts/tree/main/Chess%20Board)                                                                   | Creates a chessboard using matplotlib.                                                                                                                            |
| Compound Interest Calculator             | [Compound Interest Calculator](https://github.com/DhanushNehru/Python-Scripts/tree/main/Calculate%20Compound%20Interest)                               | A Python script to calculate compound interest.                                                                                                                   |
| Convert Temperature                      | [Convert Temperature](https://github.com/DhanushNehru/Python-Scripts/tree/main/Convert%20Temperature)                                                  | A python script to convert temperature between Fahreheit, Celsius and Kelvin                                                                                      |
| Connect Four                             | [Connect Four](https://github.com/DhanushNehru/Python-Scripts/tree/main/Connect%20Four)                                                                | A Python script for the board game Connect Four, which can be played by 1-2 players                                                                               |
| Countdown Timer                          | [Countdown Timer](https://github.com/DhanushNehru/Python-Scripts/tree/main/Countdown%20Timer)                                                          | Displays a message when the Input time elapses.                                                                                                                   |
| Crop Images                              | [Crop Images](https://github.com/DhanushNehru/Python-Scripts/tree/main/Crop%20Images)                                                                  | A Python script to crop a given image.                                                                                                                            |
| CSV to Excel                             | [CSV to Excel](https://github.com/DhanushNehru/Python-Scripts/tree/main/CSV%20to%20Excel)                                                              | A Python script to convert a CSV to an Excel file.                                                                                                                |
| CSV_TO_NDJSON                             | [CSV to Excel](https://github.com/DhanushNehru/Python-Scripts/tree/main/CSV_TO_NDJSON)                                                              | A Python script to convert a CSV to an NDJSON files file.                                                                                                                |
| Currency Script                          | [Currency Script](https://github.com/DhanushNehru/Python-Scripts/tree/main/Currency%20Script)                                                          | A Python script to convert the currency of one country to that of another.                                                                                        |
| Digital Clock                            | [Digital Clock](https://github.com/DhanushNehru/Python-Scripts/tree/main/Digital%20Clock)                                                              | A Python script to preview a digital clock in the terminal.                                                                                                       |
| Disk Usage Visualizer                    | [Disk Usage Visualizer](https://github.com/DhanushNehru/Python-Scripts/tree/main/Disk%20Usage%20Visualizer) | A Python script to display the top N directories taking up space in your disk.
| Display Popup Window                     | [Display Popup Window](https://github.com/DhanushNehru/Python-Scripts/tree/main/Display%20Popup%20Window)                                              | A Python script to preview a GUI interface to the user.                                                                                                           |
| Distance Calculator                      | [Distance Calculator](https://github.com/Mathdallas-code/Python-Scripts/tree/main/Distance%20Calculator)                                               | A Python script to calculate the distance between two points.
| Duplicate Finder                         | [Duplicate Finder](https://github.com/DhanushNehru/Python-Scripts/tree/main/Duplicate%Fnder)                                                           | The script identifies duplicate files by MD5 hash and allows deletion or relocation.                                                                              |
| Emoji                                    | [Emoji](https://github.com/DhanushNehru/Python-Scripts/tree/main/Emoji)                                                                                | The script generates a PDF with an emoji using a custom TrueType font.                                                                                            |
| Emoji to PDF                             | [Emoji to PDF](https://github.com/DhanushNehru/Python-Scripts/tree/main/Emoji%20To%20Pdf)                                                              | A Python Script to view Emoji in PDF.                                                                                                                             |
| Expense Tracker                          | [Expense Tracker](https://github.com/DhanushNehru/Python-Scripts/tree/main/Expense%20Tracker)                                                          | A Python script which can track expenses.                                                                                                                         |
| Face Reaction                            | [Face Reaction](https://github.com/DhanushNehru/Python-Scripts/tree/main/Face%20Reaction)                                                              | A script which attempts to detect facial expressions.                                                                                                             |
| Fake Profiles                            | [Fake Profiles](https://github.com/DhanushNehru/Python-Scripts/tree/main/Fake%20Profile)                                                               | Creates fake profiles.                                                                                                                                            |
| File Encryption Decryption               | [File Encryption Decryption](https://github.com/DhanushNehru/Python-Scripts/tree/main/File%20Encryption%20Decryption)                                  | Encrypts and Decrypts files using AES Algorithms for Security purposes.                                                                                           |
| File Search                              | [File_search](https://github.com/debojit11/Python-Scripts/tree/main/File_Search)                                                                       | A python script that searches a specified folder for all files, regardless of file type, within its directory and subdirectories.                                 |
| Font Art                                 | [Font Art](https://github.com/DhanushNehru/Python-Scripts/tree/main/Font%20Art)                                                                        | Displays a font art using Python.                                                                                                                                 |
| File Organizer                           | [FileOrganizer](https://github.com/DhanushNehru/Python-Scripts/tree/main/FileOrganizer)                                                                | Organizes files into different folders according to their file type                                                                                               |
| File Renamer                             | [FileRenamer](https://github.com/DhanushNehru/Python-Scripts/tree/main/FileRenamer)                                                                    | Bulk renames files with the same start/end                                                                                                                        |
| File Text Search                         | [FileTextSearch](https://github.com/DhanushNehru/Python-Scripts/tree/main/FileTextSearch)                                                              | Searches for a keyword/phrase accross different files                                                                                                             |
| Freelance Helper Program                 | [freelance-helper](https://github.com/DhanushNehru/Python-Scripts/tree/main/freelance-help-program)                                                    | Takes an Excel file with working hours and calculates the payment.                                                                                                |
| Get Hexcodes From Websites               | [Get Hexcodes From Websites](https://github.com/DhanushNehru/Python-Scripts/tree/main/Get%20Hexcodes%20From%20Websites)                                | Generates a Python list containing Hexcodes from a website.                                                                                                       |
| Hand_Volume                              | [Hand_Volume](https://github.com/DhanushNehru/Python-Scripts/tree/main/Hand%20Volume)                                                                  | Detects and tracks hand movements to control volume.                                                                                                              |
| Hangman Game                             | [Hangman](https://github.com/DhanushNehru/Python-Scripts/tree/main/Hangman%20Game)                                                                     | A classic word-guessing game where players guess letters to find the hidden word                                                                                  |
| Hardware Detector                        | [Hardware Detector](https://github.com/DhanushNehru/Python-Scripts/tree/main/Hardware%20Detector)                                                      | A Python script that detects and displays hardware specifications and resource usage, including CPU, RAM, disk space, and GPU information.                        |
| Harvest Predictor                        | [Harvest Predictor](https://github.com/DhanushNehru/Python-Scripts/tree/main/Harvest%20Predictor)                                                      | Takes some necessary input parameters and predicts harvest according to it.                                                                                       |
| Html-to-images                           | [html-to-images](https://github.com/DhanushNehru/Python-Scripts/tree/main/HTML%20to%20Images)                                                          | Converts HTML documents to image files.                                                                                                                           |
| Image Capture                            | [Image Capture](https://github.com/DhanushNehru/Python-Scripts/tree/main/Image%20Capture)                                                              | Captures image from your webcam and saves it on your local device.                                                                                                |
| Image Compress                           | [Image Compress](https://github.com/DhanushNehru/Python-Scripts/tree/main/Image%20Compress)                                                            | Takes an image and compresses it.                                                                                                                                 |
| Image Manipulation without libraries     | [Image Manipulation without libraries](https://github.com/DhanushNehru/Python-Scripts/tree/main/Image%20Manipulation%20without%20libraries)            | Manipulates images without using any external libraries.                                                                                                          |
| Image Text                               | [Image Text](https://github.com/DhanushNehru/Python-Scripts/tree/main/Image%20Text)                                                                    | Extracts text from the image.                                                                                                                                     |
| Image Text to PDF                        | [Image Text to PDF](https://github.com/DhanushNehru/Python-Scripts/tree/main/Image%20Text%20to%20PDF)                                                  | Adds an image and text to a PDF.                                                                                                                                  |
| Image Uploader                           | [Image Uploader](https://github.com/DhanushNehru/Python-Scripts/tree/main/Image%20Uploader)                                                            | Uploads images to Imgur using a keyboard shortcut.                                                                                                                |
| Image Watermarker                        | [Image Watermarker](https://github.com/DhanushNehru/Python-Scripts/tree/main/Image%20Watermarker)                                                      | Adds a watermark to an image.                                                                                                                                     |
| Image to ASCII                           | [Image to ASCII](https://github.com/DhanushNehru/Python-Scripts/tree/main/Image%20to%20ASCII)             

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[chidiwilliams/buzz]]></title>
            <link>https://github.com/chidiwilliams/buzz</link>
            <guid>https://github.com/chidiwilliams/buzz</guid>
            <pubDate>Mon, 05 Jan 2026 00:04:52 GMT</pubDate>
            <description><![CDATA[Buzz transcribes and translates audio offline on your personal computer. Powered by OpenAI's Whisper.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/chidiwilliams/buzz">chidiwilliams/buzz</a></h1>
            <p>Buzz transcribes and translates audio offline on your personal computer. Powered by OpenAI's Whisper.</p>
            <p>Language: Python</p>
            <p>Stars: 16,081</p>
            <p>Forks: 1,201</p>
            <p>Stars today: 27 stars today</p>
            <h2>README</h2><pre>[[ÁÆÄ‰Ωì‰∏≠Êñá](readme/README.zh_CN.md)] &lt;- ÁÇπÂáªÊü•Áúã‰∏≠ÊñáÈ°µÈù¢„ÄÇ

# Buzz

[Documentation](https://chidiwilliams.github.io/buzz/)

Transcribe and translate audio offline on your personal computer. Powered by
OpenAI&#039;s [Whisper](https://github.com/openai/whisper).

![MIT License](https://img.shields.io/badge/license-MIT-green)
[![CI](https://github.com/chidiwilliams/buzz/actions/workflows/ci.yml/badge.svg)](https://github.com/chidiwilliams/buzz/actions/workflows/ci.yml)
[![codecov](https://codecov.io/github/chidiwilliams/buzz/branch/main/graph/badge.svg?token=YJSB8S2VEP)](https://codecov.io/github/chidiwilliams/buzz)
![GitHub release (latest by date)](https://img.shields.io/github/v/release/chidiwilliams/buzz)
[![Github all releases](https://img.shields.io/github/downloads/chidiwilliams/buzz/total.svg)](https://GitHub.com/chidiwilliams/buzz/releases/)

## Installation

### macOS

Download the `.dmg` from the [SourceForge](https://sourceforge.net/projects/buzz-captions/files/).

### Windows

Get the installation files from the [SourceForge](https://sourceforge.net/projects/buzz-captions/files/).

App is not signed, you will get a warning when you install it. Select `More info` -&gt; `Run anyway`.

**Alternatively, install with [winget](https://learn.microsoft.com/en-us/windows/package-manager/winget/)**

```shell
winget install ChidiWilliams.Buzz
```

### Linux

Buzz is available as a [Flatpak](https://flathub.org/apps/io.github.chidiwilliams.Buzz) or a [Snap](https://snapcraft.io/buzz). 

To install flatpak, run:
```shell
flatpak install flathub io.github.chidiwilliams.Buzz
```

To install snap, run:
```shell
sudo apt-get install libportaudio2 libcanberra-gtk-module libcanberra-gtk3-module
sudo snap install buzz
sudo snap connect buzz:password-manager-service
```

### PyPI

Install [ffmpeg](https://www.ffmpeg.org/download.html)

Ensure you use Python 3.12 environment.

Install Buzz

```shell
pip install buzz-captions
python -m buzz
```

**GPU support for PyPI**

To have GPU support for Nvidia GPUS on Windows, for PyPI installed version ensure, CUDA support for [torch](https://pytorch.org/get-started/locally/) 

```
pip3 install -U torch==2.8.0+cu129 torchaudio==2.8.0+cu129 --index-url https://download.pytorch.org/whl/cu129
pip3 install nvidia-cublas-cu12==12.9.1.4 nvidia-cuda-cupti-cu12==12.9.79 nvidia-cuda-runtime-cu12==12.9.79 --extra-index-url https://pypi.ngc.nvidia.com
```

### Latest development version

For info on how to get latest development version with latest features and bug fixes see [FAQ](https://chidiwilliams.github.io/buzz/docs/faq#9-where-can-i-get-latest-development-version).

### Screenshots

&lt;div style=&quot;display: flex; flex-wrap: wrap;&quot;&gt;
    &lt;img alt=&quot;File import&quot; src=&quot;share/screenshots/buzz-1-import.png&quot; style=&quot;max-width: 18%; margin-right: 1%;&quot; /&gt;
    &lt;img alt=&quot;Main screen&quot; src=&quot;share/screenshots/buzz-2-main_screen.png&quot; style=&quot;max-width: 18%; margin-right: 1%; height:auto;&quot; /&gt;
    &lt;img alt=&quot;Preferences&quot; src=&quot;share/screenshots/buzz-3-preferences.png&quot; style=&quot;max-width: 18%; margin-right: 1%; height:auto;&quot; /&gt;
    &lt;img alt=&quot;Model preferences&quot; src=&quot;share/screenshots/buzz-3.2-model-preferences.png&quot; style=&quot;max-width: 18%; margin-right: 1%; height:auto;&quot; /&gt;
    &lt;img alt=&quot;Transcript&quot; src=&quot;share/screenshots/buzz-4-transcript.png&quot; style=&quot;max-width: 18%; margin-right: 1%; height:auto;&quot; /&gt;
    &lt;img alt=&quot;Live recording&quot; src=&quot;share/screenshots/buzz-5-live_recording.png&quot; style=&quot;max-width: 18%; margin-right: 1%; height:auto;&quot; /&gt;
    &lt;img alt=&quot;Resize&quot; src=&quot;share/screenshots/buzz-6-resize.png&quot; style=&quot;max-width: 18%;&quot; /&gt;
&lt;/div&gt;

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[GodsScion/Auto_job_applier_linkedIn]]></title>
            <link>https://github.com/GodsScion/Auto_job_applier_linkedIn</link>
            <guid>https://github.com/GodsScion/Auto_job_applier_linkedIn</guid>
            <pubDate>Mon, 05 Jan 2026 00:04:51 GMT</pubDate>
            <description><![CDATA[Make your job hunt easy by automating your application process with this Auto Applier]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/GodsScion/Auto_job_applier_linkedIn">GodsScion/Auto_job_applier_linkedIn</a></h1>
            <p>Make your job hunt easy by automating your application process with this Auto Applier</p>
            <p>Language: Python</p>
            <p>Stars: 1,300</p>
            <p>Forks: 356</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre># LinkedIn AI Auto Job Applier ü§ñ
This is an web scraping bot that automates the process of job applications on LinkedIn. It searches for jobs relevant to you, answers all questions in application form, customizes your resume based on the collected job information, such as skills required, description, about company, etc. and applies to the job. Can apply 100+ jobs in less than 1 hour. 


## üìΩÔ∏è See it in Action
[![Auto Job Applier demo video](https://github.com/GodsScion/Auto_job_applier_linkedIn/assets/100998531/429f7753-ebb0-499b-bc5e-5b4ee28c4f69)](https://youtu.be/gMbB1fWZDHw)
Click on above image to watch the demo or use this link https://youtu.be/gMbB1fWZDHw


## ‚ú® Content
- [Introduction](#linkedin-ai-auto-job-applier-)
- [Demo Video](#%EF%B8%8F-see-it-in-action)
- [Index](#-content)
- [Install](#%EF%B8%8F-how-to-install)
- [Configure](#-how-to-configure)
- [Contributor Guidelines](#‚Äç-contributor-guidelines)
- [Updates](%EF%B8%8F-major-updates-history)
- [Disclaimer](#-disclaimer)
- [Terms and Conditions](#%EF%B8%8F-terms-and-conditions)
- [License](#%EF%B8%8F-license)
- [Socials](#-socials)
- [Support and Discussions](#-community-support-and-discussions)

&lt;br&gt;

## ‚öôÔ∏è How to install

[![Auto Job Applier setup tutorial video](https://github.com/user-attachments/assets/9e876187-ed3e-4fbf-bd87-4acc145880a2)](https://youtu.be/f9rdz74e1lM?si=4fRBcte0nuvr6tEH)
Click on above image to watch the tutorial for installation and configuration or use this link https://youtu.be/f9rdz74e1lM (Recommended to watch it in 2x speed)

1. [Python 3.10](https://www.python.org/) or above. Visit https://www.python.org/downloads/ to download and install Python, or for windows you could visit Microsoft Store and search for &quot;Python&quot;. **Please make sure Python is added to Path in System Environment Variables**.
2. Install necessary [Undetected Chromedriver](https://pypi.org/project/undetected-chromedriver/), [PyAutoGUI](https://pypi.org/project/PyAutoGUI/) and [Setuptools](https://pypi.org/project/setuptools/) packages. After Python is installed, OPEN a console/terminal or shell, Use below command that uses the [pip](https://pip.pypa.io/en/stable) command-line tool to install these 3 package.
  ```
  pip install undetected-chromedriver pyautogui setuptools openai flask-cors flask
  ```
3. Download and install latest version of [Google Chrome](https://www.google.com/chrome) in it&#039;s default location, visit https://www.google.com/chrome to download it&#039;s installer.
4. Clone the current git repo or download it as a zip file, url to the latest update https://github.com/GodsScion/Auto_job_applier_linkedIn.
5. (Not needed if you set `stealth_mode = True` in `config/settings.py` ) Download and install the appropriate [Chrome Driver](https://googlechromelabs.github.io/chrome-for-testing/) for Google Chrome and paste it in the location Chrome was installed, visit https://googlechromelabs.github.io/chrome-for-testing/ to download.
  &lt;br&gt; &lt;br&gt;
  ***OR*** 
  &lt;br&gt; &lt;br&gt;
  If you are using Windows, click on `windows-setup.bat` available in the `/setup` folder, this will install the latest chromedriver automatically.
6. If you have questions or need help setting it up or to talk in general, join the github server: https://discord.gg/fFp7uUzWCY

[back to index](#-content)

&lt;br&gt;

## üîß How to configure
1. Open `personals.py` file in `/config` folder and enter your details like name, phone number, address, etc. Whatever you want to fill in your applications.
2. Open `questions.py` file in `/config` folder and enter your answers for application questions, configure wether you want the bot to pause before submission or pause if it can&#039;t answer unknown questions.
3. Open `search.py` file in `/config` folder and enter your search preferences, job filters, configure the bot as per your needs (these settings decide which jobs to apply for or skip).
4. Open `secrets.py` file in `/config` folder and enter your LinkedIn username, password to login and OpenAI API Key for generation of job tailored resumes and cover letters (This entire step is optional). If you do not provide username or password or leave them as default, it will login with saved profile in browser, if failed will ask you to login manually.
5. Open `settings.py` file in `/config` folder to configure the bot settings like, keep screen awake, click intervals (click intervals are randomized to seem like human behavior), run in background, stealth mode (to avoid bot detection), etc. as per your needs.
6. (Optional) Don&#039;t forget to add you default resume in the location you mentioned in `default_resume_path = &quot;all resumes/default/resume.pdf&quot;` given in `/config/questions.py`. If one is not provided, it will use your previous resume submitted in LinkedIn or (In Development) generate custom resume if OpenAI APT key is provided!
7. Run `runAiBot.py` and see the magic happen.
8. To run the Applied Jobs history UI, run `app.py` and open web browser on `http://localhost:5000`.
8. If you have questions or need help setting it up or to talk in general, join the github server: https://discord.gg/fFp7uUzWCY

[back to index](#-content)

&lt;br&gt;


## üßë‚Äçüíª Contributor Guidelines
Thank you for your efforts and being a part of the community. All contributions are appreciated no matter how small or big. Once you contribute to the code base, your work will be remembered forever.

NOTE: Only Pull request to `community-version` branch will be accepted. Any other requests will be declined by default, especially to main branch.
Once your code is tested, your changes will be merged to the `main` branch in next cycle.

### Code Guidelines
  #### Functions:
  1. All functions or methods are named lower case and snake case
  2. Must have explanation of their purpose. Write explanation surrounded in `&#039;&#039;&#039; Explanation &#039;&#039;&#039;` under the definition `def function() -&gt; None:`. Example:
      ```python
      def function() -&gt; None:
        &#039;&#039;&#039;
        This function does nothing, it&#039;s just an example for explanation placement!
        &#039;&#039;&#039;
      ```
  4. The Types `(str, list, int, list[str], int | float)` for the parameters and returns must be given. Example:
      ```python
      def function(param1: str, param2: list[str], param3: int) -&gt; str:
      ```
  5. Putting all that together some valid examples for function or method declarations would be as follows.
      ```python
      def function_name_in_camel_case(parameter1: driver, parameter2: str) -&gt; list[str] | ValueError:
        &#039;&#039;&#039;
        This function is an example for code guidelines
        &#039;&#039;&#039;
        return [parameter2, parameter2.lower()]
      ```
  6. The hashtag comments on top of functions are optional, which are intended for developers `# Comments for developers`.
      ```python
      # Enter input text function
      def text_input_by_ID(driver: WebDriver, id: str, value: str, time: float=5.0) -&gt; None | Exception:
          &#039;&#039;&#039;
          Enters `value` into the input field with the given `id` if found, else throws NotFoundException.
          - `time` is the max time to wait for the element to be found.
          &#039;&#039;&#039;
          username_field = WebDriverWait(driver, time).until(EC.presence_of_element_located((By.ID, id)))
          username_field.send_keys(Keys.CONTROL + &quot;a&quot;)
          username_field.send_keys(value)
      
      ```
   
  #### Variables
  1. All variables must start with lower case, must be in explainable full words. If someone reads the variable name, it should be easy to understand what the variable stores.
  2. All local variables are camel case. Examples:
      ```python
      jobListingsElement = None
      ```
      ```python
      localBufferTime = 5.5
      ```
  3. All global variables are snake case. Example:
      ```
      total_runs = 1
      ```
  4. Mentioning types are optional.
      ```python
      localBufferTime: float | int = 5.5
      ```
  
  #### Configuration variables
  1. All config variables are treated as global variables. They have some extra guidelines.
  2. Must have variable setting explanation, and examples of valid values. Examples:
      ```python
      # Explanation of what this setting will do, and instructions to enter it correctly
      config_variable = &quot;value1&quot;    #  &lt;Valid values examples, and NOTES&gt; &quot;value1&quot;, &quot;value2&quot;, etc. Don&#039;t forget quotes (&quot;&quot;)
      ```
      ```python
      # Do you want to randomize the search order for search_terms?
      randomize_search_order = False     # True of False, Note: True or False are case-sensitive
      ```
      ```python
      # Avoid applying to jobs if their required experience is above your current_experience. (Set value as -1 if you want to apply to all ignoring their required experience...)
      current_experience = 5             # Integers &gt; -2 (Ex: -1, 0, 1, 2, 3, 4...)
      ```
      ```python
      # Search location, this will be filled in &quot;City, state, or zip code&quot; search box. If left empty as &quot;&quot;, tool will not fill it.
      search_location = &quot;United States&quot;               # Some valid examples: &quot;&quot;, &quot;United States&quot;, &quot;India&quot;, &quot;Chicago, Illinois, United States&quot;, &quot;90001, Los Angeles, California, United States&quot;, &quot;Bengaluru, Karnataka, India&quot;, etc.

      ```
  4. Add the config variable in appropriate `/config/file`.
  5. Every config variable must be validated. Go to `/modules/validator.py` and add it over there. Example:
      For config variable `search_location = &quot;&quot;` found in `/config/search.py`, string validation is added in file `/modules/validator.py` under the method `def validate_search()`.
      ```python
      def validate_search() -&gt; None | ValueError | TypeError:
          &#039;&#039;&#039;
          Validates all variables in the `/config/search.py` file.
          &#039;&#039;&#039;
          check_string(search_location, &quot;search_location&quot;)
      ```

  [back to index](#-content)
  
  ### Attestation
  1. All contributions require proper attestion. Format for attestation:
  ```python
  ##&gt; ------ &lt;Your full name&gt; : &lt;github id&gt; OR &lt;email&gt; - &lt;Type of change&gt; ------
      print(&quot;My contributions üòç&quot;) # Your code
  ##&lt;
  ```
  2. Examples for proper attestation:
  New feature example
  ```python
  ##&gt; ------ Sai Vignesh Golla : godsscion - Feature ------
  def alert_box(title: str, message: str) -&gt; None:
    &#039;&#039;&#039;
    Shows an alert box with the given `title` and `message`.
    &#039;&#039;&#039;
    from pyautogui import alert
    return alert(title, message)

  ##&lt;
  ```
  
  Bug fix example
  ```python
  def alert_box(title: str, message: str) -&gt; None:
    &#039;&#039;&#039;
    Shows an alert box with the given `title` and `message`.
    &#039;&#039;&#039;
    from pyautogui import alert

  ##&gt; ------ Sai Vignesh Golla : saivigneshgolla@outlook.com - Bug fix ------
    return alert(message, title)
  ##&lt;
  ```

[back to index](#-content)

## üóìÔ∏è Major Updates History:
### Jul 20, 2024
- Contributions from community have been added
- Better AI support, minor bug fixes

### Nov 28, 2024
- Patched to work for latest changes in Linkedin.
- Users can now select to follow or not follow companies when submitting application.
- Frameworks for future AI Developments have been added.
- AI can now be used to extract skills from job description. 

### Oct 16, 2024
- Framework for OpenAI API and Local LLMs
- Framework for RAG

### Sep 09, 2024
- Smarter Auto-fill for salaries and notice periods
- Robust Search location filter, will work in window mode (No need for full screen)
- Better logic for Select and Radio type questions
- Proper functioning of Decline to answer questions in Equal Employment opportunity questions
- Checkbox questions select fail bug fixed
- Annotations are clearer in instructions for setup

### Sep 07, 2024
- Annotations for developers
- Robust input validations
- Restructured config file
- Fixed pagination bug

### Aug 21, 2024
- Performance improvements (skip clicking on applied jobs and blacklisted companies)
- Stop when easy apply application limit is reached
- Added ability to discard from pause at submission dialogue box
- Added support for address input
- Bug fixed radio questions, added support for physical disability questions
- Added framework for future config file updates

### June 19, 2024
- Major Bug fixes (Text Area type questions)
- Made uploading default resume as not required

### May 15, 2024
- Added functionality for textarea type questions `summary`, `cover_letter`(Summary, Cover letter); checkbox type questions (acknowledgements)
- Added feature to skip irrelevant jobs based on `bad_words` 
- Improved performance for answering questions
- Logic change for masters students skipping
- Change variable names `blacklist_exceptions` -&gt; `about_company_good_words` and `blacklist_words` -&gt; `about_company_bad_words`
- Added session summary for logs
- Added option to turn off &quot;Pause before Submit&quot; until next run

### May 05, 2024
- For questions similar to &quot;What is your current location?&quot;, City posted in Job description will be posted as the answer if `current_city` is left empty in the configuration
- Added option to over write previously saved answers for a question `overwrite_previous_answers`
- Tool will now save previous answer of a question
- Tool will now collect all available options for a Radio type or Select type question
- Major update in answering logic for Easy Apply Application questions
- Added Safe mode option for quick stable launches `safe_mode`

### May 04, 2024
- Added option to fill in &quot;City, state, or zip code&quot; search box `search_location`
- Bug fixes in answering City or location question


[back to index](#-content)

&lt;br&gt;

## üìú Disclaimer

**This program is for educational purposes only. By downloading, using, copying, replicating, or interacting with this program or its code, you acknowledge and agree to abide by all the Terms, Conditions, Policies, and Licenses mentioned, which are subject to modification without prior notice. The responsibility of staying informed of any changes or updates bears upon yourself. For the latest Terms &amp; Conditions, Licenses, or Policies, please refer to [Auto Job Applier](https://github.com/GodsScion/Auto_job_applier_linkedIn). Additionally, kindly adhere to and comply with LinkedIn&#039;s terms of service and policies pertaining to web scraping. Usage is at your own risk. The creators and contributors of this program emphasize that they bear no responsibility or liability for any misuse, damages, or legal consequences resulting from its usage.**


## üèõÔ∏è Terms and Conditions

Please consider the following:

- **LinkedIn Policies**: LinkedIn has specific policies regarding web scraping and data collection. The responsibility to review and comply with these policies before engaging, interacting, or undertaking any actions with this program bears upon yourself. Be aware of the limitations and restrictions imposed by LinkedIn to avoid any potential violation(s).

- **No Warranties or Guarantees**: This program is provided as-is, without any warranties or guarantees of any kind. The accuracy, reliability, and effectiveness of the program cannot be guaranteed. Use it at your own risk.

- **Disclaimer of Liability**: The creators and contributors of this program shall not be held responsible or liable for any damages or consequences arising from the direct or indirect use, interaction, or actions performed with this program. This includes but is not limited to any legal issues, loss of data, or other damages incurred.

- **Use at Your Own Risk**: It is important to exercise caution and ensure that your usage, interactions, and actions with this program comply with the applicable laws and regulations. Understand the potential risks and consequences associated with web scraping and data collection activities.

- **Chrome Driver**: This program utilizes the Chrome Driver for web scraping. Please review and comply with the terms and conditions specified for [Chrome Driver](https://chromedriver.chromium.org/home).


## ‚öñÔ∏è License

Copyright (C) 2024 Sai Vignesh Golla  &lt;saivigneshgolla@outlook.com&gt;

This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.

This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details.

You should have received a copy of the GNU Affero General Public License along with this program. If not, see &lt;https://www.gnu.org/licenses/&gt;.

See [AGPLv3 LICENSE](LICENSE) for more info.


&lt;br&gt;

[back to index](#-content)

&lt;br&gt;

## üêß Socials
- **LinkedIn** : https://www.linkedin.com/in/saivigneshgolla/
- **Email**    : saivigneshgolla@outlook.com
- **X/Twitter**: https://x.com/saivigneshgolla
- **Discord**  : godsscion


## üôå Community Support and Discussions
- **Discord Server** : https://discord.gg/fFp7uUzWCY
alternate link: https://discord.gg/ykfDjRFB
- **GitHub**
    - [All Discussions](https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions)
    - [Announcements](https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions/categories/announcements)
    - [General](https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions/categories/general)
    - [Feature requests or Ideas](https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions/categories/feature-requests-or-ideas)
    - [Polls](https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions/categories/polls)
    - [Community Flex](https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions/categories/community-flex)
    - [Support Q&amp;A](https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions/categories/support-q-a)


#### ‚ÑπÔ∏è Version: 25.07.20.9.30 Community Alpha

---

[back to the top](#linkedin-ai-auto-job-applier-)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[beancount/beancount]]></title>
            <link>https://github.com/beancount/beancount</link>
            <guid>https://github.com/beancount/beancount</guid>
            <pubDate>Mon, 05 Jan 2026 00:04:50 GMT</pubDate>
            <description><![CDATA[Beancount: Double-Entry Accounting from Text Files.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/beancount/beancount">beancount/beancount</a></h1>
            <p>Beancount: Double-Entry Accounting from Text Files.</p>
            <p>Language: Python</p>
            <p>Stars: 5,033</p>
            <p>Forks: 395</p>
            <p>Stars today: 132 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[google-research/timesfm]]></title>
            <link>https://github.com/google-research/timesfm</link>
            <guid>https://github.com/google-research/timesfm</guid>
            <pubDate>Mon, 05 Jan 2026 00:04:49 GMT</pubDate>
            <description><![CDATA[TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google-research/timesfm">google-research/timesfm</a></h1>
            <p>TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.</p>
            <p>Language: Python</p>
            <p>Stars: 7,520</p>
            <p>Forks: 661</p>
            <p>Stars today: 41 stars today</p>
            <h2>README</h2><pre># TimesFM

TimesFM (Time Series Foundation Model) is a pretrained time-series foundation
model developed by Google Research for time-series forecasting.

*   Paper:
    [A decoder-only foundation model for time-series forecasting](https://arxiv.org/abs/2310.10688),
    ICML 2024.
*   All checkpoints:
    [TimesFM Hugging Face Collection](https://huggingface.co/collections/google/timesfm-release-66e4be5fdb56e960c1e482a6).
*   [Google Research blog](https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/).
*   [TimesFM in BigQuery](https://cloud.google.com/bigquery/docs/timesfm-model):
    an official Google product.

This open version is not an officially supported Google product.

**Latest Model Version:** TimesFM 2.5

**Archived Model Versions:**

-   1.0 and 2.0: relevant code archived in the sub directory `v1`. You can `pip
    install timesfm==1.3.0` to install an older version of this package to load
    them.

## Update - Oct. 29, 2025

Added back the covariate support through XReg for TimesFM 2.5.


## Update - Sept. 15, 2025

TimesFM 2.5 is out!

Comparing to TimesFM 2.0, this new 2.5 model:

-   uses 200M parameters, down from 500M.
-   supports up to 16k context length, up from 2048.
-   supports continuous quantile forecast up to 1k horizon via an optional 30M
    quantile head.
-   gets rid of the `frequency` indicator.
-   has a couple of new forecasting flags.

Along with the model upgrade we have also upgraded the inference API. This repo
will be under construction over the next few weeks to

1.  add support for an upcoming Flax version of the model (faster inference).
2.  add back covariate support.
3.  populate more docstrings, docs and notebook.

### Install

1.  Clone the repository:
    ```shell
    git clone https://github.com/google-research/timesfm.git
    cd timesfm
    ```

2.  Create a virtual environment and install dependencies using `uv`:
    ```shell
    # Create a virtual environment
    uv venv
    
    # Activate the environment
    source .venv/bin/activate
    
    # Install the package in editable mode with torch
    uv pip install -e .[torch]
    # Or with flax
    uv pip install -e .[flax]
    # Or XReg is needed
    uv pip install -e .[xreg]
    ```

3. [Optional] Install your preferred `torch` / `jax` backend based on your OS and accelerators
(CPU, GPU, TPU or Apple Silicon).:

-   [Install PyTorch](https://pytorch.org/get-started/locally/).
-   [Install Jax](https://docs.jax.dev/en/latest/installation.html#installation)
    for Flax.

### Code Example

```python
import torch
import numpy as np
import timesfm

torch.set_float32_matmul_precision(&quot;high&quot;)

model = timesfm.TimesFM_2p5_200M_torch.from_pretrained(&quot;google/timesfm-2.5-200m-pytorch&quot;)

model.compile(
    timesfm.ForecastConfig(
        max_context=1024,
        max_horizon=256,
        normalize_inputs=True,
        use_continuous_quantile_head=True,
        force_flip_invariance=True,
        infer_is_positive=True,
        fix_quantile_crossing=True,
    )
)
point_forecast, quantile_forecast = model.forecast(
    horizon=12,
    inputs=[
        np.linspace(0, 1, 100),
        np.sin(np.linspace(0, 20, 67)),
    ],  # Two dummy inputs
)
point_forecast.shape  # (2, 12)
quantile_forecast.shape  # (2, 12, 10): mean, then 10th to 90th quantiles.
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>