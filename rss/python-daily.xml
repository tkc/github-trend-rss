<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Wed, 28 May 2025 00:04:37 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[Fosowl/agenticSeek]]></title>
            <link>https://github.com/Fosowl/agenticSeek</link>
            <guid>https://github.com/Fosowl/agenticSeek</guid>
            <pubDate>Wed, 28 May 2025 00:04:37 GMT</pubDate>
            <description><![CDATA[Fully Local Manus AI. No APIs, No $200 monthly bills. Enjoy an autonomous agent that thinks, browses the web, and code for the sole cost of electricity.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Fosowl/agenticSeek">Fosowl/agenticSeek</a></h1>
            <p>Fully Local Manus AI. No APIs, No $200 monthly bills. Enjoy an autonomous agent that thinks, browses the web, and code for the sole cost of electricity.</p>
            <p>Language: Python</p>
            <p>Stars: 9,458</p>
            <p>Forks: 792</p>
            <p>Stars today: 2,444 stars today</p>
            <h2>README</h2><pre># AgenticSeek: Private, Local Manus Alternative.

&lt;p align=&quot;center&quot;&gt;
&lt;img align=&quot;center&quot; src=&quot;./media/agentic_seek_logo.png&quot; width=&quot;300&quot; height=&quot;300&quot; alt=&quot;Agentic Seek Logo&quot;&gt;
&lt;p&gt;

  English | [‰∏≠Êñá](./README_CHS.md) | [ÁπÅÈ´î‰∏≠Êñá](./README_CHT.md) | [Fran√ßais](./README_FR.md) | [Êó•Êú¨Ë™û](./README_JP.md)

*A **100% local alternative to Manus AI**, this voice-enabled AI assistant autonomously browses the web, writes code, and plans tasks while keeping all data on your device. Tailored for local reasoning models, it runs entirely on your hardware, ensuring complete privacy and zero cloud dependency.*

[![Visit AgenticSeek](https://img.shields.io/static/v1?label=Website&amp;message=AgenticSeek&amp;color=blue&amp;style=flat-square)](https://fosowl.github.io/agenticSeek.html) ![License](https://img.shields.io/badge/license-GPL--3.0-green) [![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&amp;logoColor=white)](https://discord.gg/8hGDaME3TC) [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&amp;label=Update%20%40Fosowl)](https://x.com/Martin993886460) [![GitHub stars](https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social)](https://github.com/Fosowl/agenticSeek/stargazers)

### Why AgenticSeek ?

* üîí Fully Local &amp; Private - Everything runs on your machine ‚Äî no cloud, no data sharing. Your files, conversations, and searches stay private.

* üåê Smart Web Browsing - AgenticSeek can browse the internet by itself ‚Äî search, read, extract info, fill web form ‚Äî all hands-free.

* üíª Autonomous Coding Assistant - Need code? It can write, debug, and run programs in Python, C, Go, Java, and more ‚Äî all without supervision.

* üß† Smart Agent Selection - You ask, it figures out the best agent for the job automatically. Like having a team of experts ready to help.

* üìã Plans &amp; Executes Complex Tasks - From trip planning to complex projects ‚Äî it can split big tasks into steps and get things done using multiple AI agents.

* üéôÔ∏è Voice-Enabled - Clean, fast, futuristic voice and speech to text allowing you to talk to it like it&#039;s your personal AI from a sci-fi movie

### **Demo**

&gt; *Can you search for the agenticSeek project, learn what skills are required, then open the CV_candidates.zip and then tell me which match best the project*

https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316

Disclaimer: This demo, including all the files that appear (e.g: CV_candidates.zip), are entirely fictional. We are not a corporation, we seek open-source contributors not candidates.

&gt; üõ†‚ö†Ô∏èÔ∏è **Active Work in Progress** ‚Äì Please note that Code/Bash is not dockerized yet but will be soon (see docker_deployement branch) - Do not deploy over network or production.

&gt; üôè Please also understand that this project began as a side experiment, with no roadmap and no expectations, we didn&#039;t expect to end in Github trending. Financial backing is exactly $1/month (shoutout to my single sponsor). Contributions, feedback, and patience are deeply appreciated.

## Installation

Make sure you have chrome driver, docker and python3.10 installed.

We highly advice you use exactly python3.10 for the setup. Dependencies error might happen otherwise.

For issues related to chrome driver, see the **Chromedriver** section.

### 1Ô∏è‚É£ **Clone the repository and setup**

```sh
git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
```

### 2Ô∏è **Create a virtual env**

```sh
python3 -m venv agentic_seek_env
source agentic_seek_env/bin/activate
# On Windows: agentic_seek_env\Scripts\activate
```

### 3Ô∏è‚É£ **Install package**

Ensure Python, Docker and docker compose, and Google chrome are installed.

We recommand Python 3.10.0.

**Automatic Installation (Recommanded):**

For Linux/Macos:
```sh
./install.sh
```

For windows:

```sh
./install.bat
```

**Manually:**

**Note: For any OS, ensure the ChromeDriver you install matches your installed Chrome version. Run `google-chrome --version`. See known issues if you have chrome &gt;135**

- *Linux*: 

Update Package List: `sudo apt update`

Install Dependencies: `sudo apt install -y alsa-utils portaudio19-dev python3-pyaudio libgtk-3-dev libnotify-dev libgconf-2-4 libnss3 libxss1`

Install ChromeDriver matching your Chrome browser version:
`sudo apt install -y chromium-chromedriver`

Install requirements: `pip3 install -r requirements.txt`

- *Macos*:

Update brew : `brew update`

Install chromedriver : `brew install --cask chromedriver`

Install portaudio: `brew install portaudio`

Upgrade pip : `python3 -m pip install --upgrade pip`

Upgrade wheel : : `pip3 install --upgrade setuptools wheel`

Install requirements: `pip3 install -r requirements.txt`

- *Windows*:

Install pyreadline3 `pip install pyreadline3`

Install portaudio manually (e.g., via vcpkg or prebuilt binaries) and then run: `pip install pyaudio`

Download and install chromedriver manually from: https://sites.google.com/chromium.org/driver/getting-started

Place chromedriver in a directory included in your PATH.

Install requirements: `pip3 install -r requirements.txt`

---

## Setup for running LLM locally on your machine

**Hardware Requirements:**

To run LLMs locally, you&#039;ll need sufficient hardware. At a minimum, a GPU capable of running Qwen/Deepseek 14B is required. See the FAQ for detailed model/performance recommendations.

**Setup your local provider**  

Start your local provider, for example with ollama:

```sh
ollama serve
```

See below for a list of local supported provider.

**Update the config.ini**

Change the config.ini file to set the provider_name to a supported provider and provider_model to a LLM supported by your provider. We recommand reasoning model such as *Qwen* or *Deepseek*.

See the **FAQ** at the end of the README for required hardware.

```sh
[MAIN]
is_local = True # Whenever you are running locally or with remote provider.
provider_name = ollama # or lm-studio, openai, etc..
provider_model = deepseek-r1:14b # choose a model that fit your hardware
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # name of your AI
recover_last_session = True # whenever to recover the previous session
save_session = True # whenever to remember the current session
speak = True # text to speech
listen = False # Speech to text, only for CLI
work_dir =  /Users/mlg/Documents/workspace # The workspace for AgenticSeek.
jarvis_personality = False # Whenever to use a more &quot;Jarvis&quot; like personality (experimental)
languages = en zh # The list of languages, Text to speech will default to the first language on the list
[BROWSER]
headless_browser = True # Whenever to use headless browser, recommanded only if you use web interface.
stealth_mode = True # Use undetected selenium to reduce browser detection
```

Warning: Do *NOT* set provider_name to `openai` if using LM-studio for running LLMs. Set it to `lm-studio`.

Note: Some provider (eg: lm-studio) require you to have `http://` in front of the IP. For example `http://127.0.0.1:1234`

**List of local providers**

| Provider  | Local? | Description                                               |
|-----------|--------|-----------------------------------------------------------|
| ollama    | Yes    | Run LLMs locally with ease using ollama as a LLM provider |
| lm-studio  | Yes    | Run LLM locally with LM studio (set `provider_name` to `lm-studio`)|
| openai    | Yes     |  Use openai compatible API (eg: llama.cpp server)  |

Next step: [Start services and run AgenticSeek](#Start-services-and-Run)  

*See the **Known issues** section if you are having issues*

*See the **Run with an API** section if your hardware can&#039;t run deepseek locally*

*See the **Config** section for detailled config file explanation.*

---

## Setup to run with an API

Set the desired provider in the `config.ini`. See below for a list of API providers.

```sh
[MAIN]
is_local = False
provider_name = google
provider_model = gemini-2.0-flash
provider_server_address = 127.0.0.1:5000 # doesn&#039;t matter
```
Warning: Make sure there is not trailing space in the config.

Export your API key: `export &lt;&lt;PROVIDER&gt;&gt;_API_KEY=&quot;xxx&quot;`

Example: export `TOGETHER_API_KEY=&quot;xxxxx&quot;`

**List of API providers**
  
| Provider  | Local? | Description                                               |
|-----------|--------|-----------------------------------------------------------|
| openai    | Depends  | Use ChatGPT API  |
| deepseek  | No     | Deepseek API (non-private)                            |
| huggingface| No    | Hugging-Face API (non-private)                            |
| togetherAI | No    | Use together AI API (non-private)                         |
| google | No    | Use google gemini API (non-private)                         |

*We advice against using gpt-4o or other closedAI models*, performance are poor for web browsing and task planning.

Please also note that coding/bash might fail with gemini, it seem to ignore our prompt for format to respect, which are optimized for deepseek r1.

Next step: [Start services and run AgenticSeek](#Start-services-and-Run)

*See the **Known issues** section if you are having issues*

*See the **Config** section for detailled config file explanation.*

---

## Start services and Run

Activate your python env if needed.
```sh
source agentic_seek_env/bin/activate
```

Start required services. This will start all services from the docker-compose.yml, including:
    - searxng
    - redis (required by searxng)
    - frontend

```sh
sudo ./start_services.sh # MacOS
start ./start_services.cmd # Window
```

**Options 1:** Run with the CLI interface.

```sh
python3 cli.py
```

We advice you set `headless_browser` to False in the config.ini for CLI mode.

**Options 2:** Run with the Web interface.

Start the backend.

```sh
python3 api.py
```

Go to `http://localhost:3000/` and you should see the web interface.

---

## Usage

Make sure the services are up and running with `./start_services.sh` and run the AgenticSeek with `python3 cli.py` for CLI mode or `python3 api.py` then go to `localhost:3000` for web interface.

You can also use speech to text by setting `listen = True` in the config. Only for CLI mode.

To exit, simply say/type `goodbye`.

Here are some example usage:

&gt; *Make a snake game in python!*

&gt; *Search the web for top cafes in Rennes, France, and save a list of three with their addresses in rennes_cafes.txt.*

&gt; *Write a Go program to calculate the factorial of a number, save it as factorial.go in your workspace*

&gt; *Search my summer_pictures folder for all JPG files, rename them with today‚Äôs date, and save a list of renamed files in photos_list.txt*

&gt; *Search online for popular sci-fi movies from 2024 and pick three to watch tonight. Save the list in movie_night.txt.*

&gt; *Search the web for the latest AI news articles from 2025, select three, and write a Python script to scrape their titles and summaries. Save the script as news_scraper.py and the summaries in ai_news.txt in /home/projects*

&gt; *Friday, search the web for a free stock price API, register with supersuper7434567@gmail.com then write a Python script to fetch using the API daily prices for Tesla, and save the results in stock_prices.csv*

*Note that form filling capabilities are still experimental and might fail.*



After you type your query, AgenticSeek will allocate the best agent for the task.

Because this is an early prototype, the agent routing system might not always allocate the right agent based on your query.

Therefore, you should be very explicit in what you want and how the AI might proceed for example if you want it to conduct a web search, do not say:

`Do you know some good countries for solo-travel?`

Instead, ask:

`Do a web search and find out which are the best country for solo-travel`

---

## **Setup to run the LLM on your own server**  

If you have a powerful computer or a server that you can use, but you want to use it from your laptop you have the options to run the LLM on a remote server using our custom llm server. 

On your &quot;server&quot; that will run the AI model, get the ip address

```sh
ip a | grep &quot;inet &quot; | grep -v 127.0.0.1 | awk &#039;{print $2}&#039; | cut -d/ -f1 # local ip
curl https://ipinfo.io/ip # public ip
```

Note: For Windows or macOS, use ipconfig or ifconfig respectively to find the IP address.

Clone the repository and enter the `server/`folder.


```sh
git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
```

Install server specific requirements:

```sh
pip3 install -r requirements.txt
```

Run the server script.

```sh
python3 app.py --provider ollama --port 3333
```

You have the choice between using `ollama` and `llamacpp` as a LLM service.


Now on your personal computer:

Change the `config.ini` file to set the `provider_name` to `server` and `provider_model` to `deepseek-r1:xxb`.
Set the `provider_server_address` to the ip address of the machine that will run the model.

```sh
[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
```


Next step: [Start services and run AgenticSeek](#Start-services-and-Run)  

---

## Speech to Text

Please note that currently speech to text only work in english.

The speech-to-text functionality is disabled by default. To enable it, set the listen option to True in the config.ini file:

```
listen = True
```

When enabled, the speech-to-text feature listens for a trigger keyword, which is the agent&#039;s name, before it begins processing your input. You can customize the agent&#039;s name by updating the `agent_name` value in the *config.ini* file:

```
agent_name = Friday
```

For optimal recognition, we recommend using a common English name like &quot;John&quot; or &quot;Emma&quot; as the agent name

Once you see the transcript start to appear, say the agent&#039;s name aloud to wake it up (e.g., &quot;Friday&quot;).

Speak your query clearly.

End your request with a confirmation phrase to signal the system to proceed. Examples of confirmation phrases include:
```
&quot;do it&quot;, &quot;go ahead&quot;, &quot;execute&quot;, &quot;run&quot;, &quot;start&quot;, &quot;thanks&quot;, &quot;would ya&quot;, &quot;please&quot;, &quot;okay?&quot;, &quot;proceed&quot;, &quot;continue&quot;, &quot;go on&quot;, &quot;do that&quot;, &quot;go it&quot;, &quot;do you understand?&quot;
```

## Config

Example config:
```
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = 127.0.0.1:11434
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False
work_dir =  /Users/mlg/Documents/ai_folder
jarvis_personality = False
languages = en zh
[BROWSER]
headless_browser = False
stealth_mode = False
```

**Explanation**:

- is_local -&gt; Runs the agent locally (True) or on a remote server (False).

- provider_name -&gt; The provider to use (one of: `ollama`, `server`, `lm-studio`, `deepseek-api`)

- provider_model -&gt; The model used, e.g., deepseek-r1:32b.

- provider_server_address -&gt; Server address, e.g., 127.0.0.1:11434 for local. Set to anything for non-local API.

- agent_name -&gt; Name of the agent, e.g., Friday. Used as a trigger word for TTS.

- recover_last_session -&gt; Restarts from last session (True) or not (False).

- save_session -&gt; Saves session data (True) or not (False).

- speak -&gt; Enables voice output (True) or not (False).

- listen -&gt; listen to voice input (True) or not (False).

- work_dir -&gt; Folder the AI will have access to. eg: /Users/user/Documents/.

- jarvis_personality -&gt; Uses a JARVIS-like personality (True) or not (False). This simply change the prompt file.

- languages -&gt; The list of supported language, needed for the llm router to work properly, avoid putting too many or too similar languages.

- headless_browser -&gt; Runs browser without a visible window (True) or not (False).

- stealth_mode -&gt; Make bot detector time harder. Only downside is you have to manually install the anticaptcha extension.

- languages -&gt; List of supported languages. Required for agent routing system. The longer the languages list the more model will be downloaded.

## Providers

The table below show the available providers:

| Provider  | Local? | Description                                               |
|-----------|--------|-----------------------------------------------------------|
| ollama    | Yes    | Run LLMs locally with ease using ollama as a LLM provider |
| server    | Yes    | Host the model on another machine, run your local machine |
| lm-studio  | Yes    | Run LLM locally with LM studio (`lm-studio`)             |
| openai    | Depends  | Use ChatGPT API (non-private) or openai compatible API  |
| deepseek-api  | No     | Deepseek API (non-private)                            |
| huggingface| No    | Hugging-Face API (non-private)                            |
| togetherAI | No    | Use together AI API (non-private)                         |
| google | No    | Use google gemini API (non-private)                         |

To select a provider change the config.ini:

```
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = 127.0.0.1:5000
```
`is_local`: should be True for any locally running LLM, otherwise False.

`provider_name`: Select the provider to use by it&#039;s name, see the provider list above.

`provider_model`: Set the model to use by the agent.

`provider_server_address`: can be set to anything if you are not using the server provider.

# Known issues

## Chromedriver Issues

**Known error #1:** *chromedriver mismatch*

`Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113
Current browser version is 134.0.6998.89 with binary path`

This happen if there is a mismatch between your browser and chromedriver version.

You need to navigate to download the latest version:

https://developer.chrome.com/docs/chromedriver/downloads

If you&#039;re using Chrome version 115 or newer go to:

https://googlechromelabs.github.io/chrome-for-testing/

And download the chromedriver version matching your OS.

![alt text](./media/chromedriver_readme.png)

If this section is incomplete please raise an issue.

##  connection adapters Issues

```
Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for &#039;127.0.0.1:11434/v1/chat/completions&#039;
```

Make sure you have `http://` in front of the provider IP address :

`provider_server_address = http://127.0.0.1:11434`

## SearxNG base URL must be provided

```
raise ValueError(&quot;SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.&quot;)
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.
```

Maybe you didn&#039;t move `.env.example` as `.env` ? You can also export SEARXNG_BASE_URL:

`export  SEARXNG_BASE_URL=&quot;http://127.0.0.1:8080&quot;`

## FAQ

**Q: What hardware do I need?**  

| Model Size  | GPU  | Comment                                               |
|-----------|--------|-----------------------------------------------------------|
| 7B        | 8GB Vram | ‚ö†Ô∏è Not recommended. Performance is poor, frequent hallucinations, and planner agents will likely fail. |
| 14B        | 12 GB VRAM (e.g. RTX 3060) | ‚úÖ Usable for simple tasks. May struggle with web browsing and planning tasks. |
| 32B        | 24+ GB VRAM (e.g. RTX 4090) | üöÄ Success with most tasks, might still struggle with task planning |
| 70B+        | 48+ GB Vram (eg. mac studio) | üí™ Excellent. Recommended for advanced use cases. |

**Q: Why Deepseek R1 over other models?**  

Deepseek R1 excels at reasoning and tool use for its size. We think it‚Äôs a solid fit for our needs other models work fine, but Deepseek is our primary pick.

**Q: I get an error running `cli.py`. What do I do?**  

Ensure local is running (`ollama serve`), your `config.ini` matches your provider, and dependencies are installed. If none work feel free to raise an issue.

**Q: Can it really run 100% locally?**  

Yes with Ollama, lm-studio or serv

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/qlib]]></title>
            <link>https://github.com/microsoft/qlib</link>
            <guid>https://github.com/microsoft/qlib</guid>
            <pubDate>Wed, 28 May 2025 00:04:36 GMT</pubDate>
            <description><![CDATA[Qlib is an AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms. including supervised learning, market dynamics modeling, and RL.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/qlib">microsoft/qlib</a></h1>
            <p>Qlib is an AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms. including supervised learning, market dynamics modeling, and RL.</p>
            <p>Language: Python</p>
            <p>Stars: 22,600</p>
            <p>Forks: 3,522</p>
            <p>Stars today: 660 stars today</p>
            <h2>README</h2><pre>[![Python Versions](https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&amp;logoColor=white)](https://pypi.org/project/pyqlib/#files)
[![Platform](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey)](https://pypi.org/project/pyqlib/#files)
[![PypI Versions](https://img.shields.io/pypi/v/pyqlib)](https://pypi.org/project/pyqlib/#history)
[![Upload Python Package](https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/pyqlib/)
[![Github Actions Test Status](https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main)](https://github.com/microsoft/qlib/actions)
[![Documentation Status](https://readthedocs.org/projects/qlib/badge/?version=latest)](https://qlib.readthedocs.io/en/latest/?badge=latest)
[![License](https://img.shields.io/pypi/l/pyqlib)](LICENSE)
[![Join the chat at https://gitter.im/Microsoft/qlib](https://badges.gitter.im/Microsoft/qlib.svg)](https://gitter.im/Microsoft/qlib?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge)

## :newspaper: **What&#039;s NEW!** &amp;nbsp;   :sparkling_heart: 

Recent released features

### Introducing &lt;a href=&quot;https://github.com/microsoft/RD-Agent&quot;&gt;&lt;img src=&quot;docs/_static/img/rdagent_logo.png&quot; alt=&quot;RD_Agent&quot; style=&quot;height: 2em&quot;&gt;&lt;/a&gt;: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&amp;D

We are excited to announce the release of **RD-Agent**üì¢, a powerful tool that supports automated factor mining and model optimization in quant investment R&amp;D.

RD-Agent is now available on [GitHub](https://github.com/microsoft/RD-Agent), and we welcome your starüåü!

To learn more, please visit our [‚ôæÔ∏èDemo page](https://rdagent.azurewebsites.net/). Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.

We have prepared several demo videos for you:
| Scenario | Demo video (English) | Demo video (‰∏≠Êñá) |
| --                      | ------    | ------    |
| Quant Factor Mining | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=zh) |
| Quant Factor Mining from reports | [Link](https://rdagent.azurewebsites.net/report_factor?lang=en) | [Link](https://rdagent.azurewebsites.net/report_factor?lang=zh) |
| Quant Model Optimization | [Link](https://rdagent.azurewebsites.net/model_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/model_loop?lang=zh) |

- üìÉ**Paper**: [R&amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization](https://arxiv.org/abs/2505.15155)
- üëæ**Code**: https://github.com/microsoft/RD-Agent/
```BibTeX
@misc{li2025rdagentquant,
    title={R\&amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},
    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},
    year={2025},
    eprint={2505.15155},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
```
![image](https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d)

***

| Feature | Status |
| --                      | ------    |
| [R&amp;D-Agent-Quant](https://arxiv.org/abs/2505.15155) Published | Apply R&amp;D-Agent to Qlib for quant trading | 
| BPQP for End-to-end learning | üìàComing soon!([Under review](https://github.com/microsoft/qlib/pull/1863)) |
| üî•LLM-driven Auto Quant Factoryüî• | üöÄ Released in [‚ôæÔ∏èRD-Agent](https://github.com/microsoft/RD-Agent) on Aug 8, 2024 |
| KRNN and Sandwich models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1414/) on May 26, 2023 |
| Release Qlib v0.9.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.9.0) on Dec 9, 2022 |
| RL Learning Framework | :hammer: :chart_with_upwards_trend: Released on Nov 10, 2022. [#1332](https://github.com/microsoft/qlib/pull/1332), [#1322](https://github.com/microsoft/qlib/pull/1322), [#1316](https://github.com/microsoft/qlib/pull/1316),[#1299](https://github.com/microsoft/qlib/pull/1299),[#1263](https://github.com/microsoft/qlib/pull/1263), [#1244](https://github.com/microsoft/qlib/pull/1244), [#1169](https://github.com/microsoft/qlib/pull/1169), [#1125](https://github.com/microsoft/qlib/pull/1125), [#1076](https://github.com/microsoft/qlib/pull/1076)|
| HIST and IGMTF models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1040) on Apr 10, 2022 |
| Qlib [notebook tutorial](https://github.com/microsoft/qlib/tree/main/examples/tutorial) | üìñ [Released](https://github.com/microsoft/qlib/pull/1037) on Apr 7, 2022 | 
| Ibovespa index data | :rice: [Released](https://github.com/microsoft/qlib/pull/990) on Apr 6, 2022 |
| Point-in-Time database | :hammer: [Released](https://github.com/microsoft/qlib/pull/343) on Mar 10, 2022 |
| Arctic Provider Backend &amp; Orderbook data example | :hammer: [Released](https://github.com/microsoft/qlib/pull/744) on Jan 17, 2022 |
| Meta-Learning-based framework &amp; DDG-DA  | :chart_with_upwards_trend:  :hammer: [Released](https://github.com/microsoft/qlib/pull/743) on Jan 10, 2022 | 
| Planning-based portfolio optimization | :hammer: [Released](https://github.com/microsoft/qlib/pull/754) on Dec 28, 2021 | 
| Release Qlib v0.8.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.8.0) on Dec 8, 2021 |
| ADD model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/704) on Nov 22, 2021 |
| ADARNN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/689) on Nov 14, 2021 |
| TCN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/668) on Nov 4, 2021 |
| Nested Decision Framework | :hammer: [Released](https://github.com/microsoft/qlib/pull/438) on Oct 1, 2021. [Example](https://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/workflow.py) and [Doc](https://qlib.readthedocs.io/en/latest/component/highfreq.html) |
| Temporal Routing Adaptor (TRA) | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/531) on July 30, 2021 |
| Transformer &amp; Localformer | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/508) on July 22, 2021 |
| Release Qlib v0.7.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.7.0) on July 12, 2021 |
| TCTS Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/491) on July 1, 2021 |
| Online serving and automatic model rolling | :hammer:  [Released](https://github.com/microsoft/qlib/pull/290) on May 17, 2021 | 
| DoubleEnsemble Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/286) on Mar 2, 2021 | 
| High-frequency data processing example | :hammer: [Released](https://github.com/microsoft/qlib/pull/257) on Feb 5, 2021  |
| High-frequency trading example | :chart_with_upwards_trend: [Part of code released](https://github.com/microsoft/qlib/pull/227) on Jan 28, 2021  | 
| High-frequency data(1min) | :rice: [Released](https://github.com/microsoft/qlib/pull/221) on Jan 27, 2021 |
| Tabnet Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/205) on Jan 22, 2021 |

Features released before 2021 are not listed here.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/_static/img/logo/1.png&quot; /&gt;
&lt;/p&gt;

Qlib is an open-source, AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms, including supervised learning, market dynamics modeling, and reinforcement learning.

An increasing number of SOTA Quant research works/papers in diverse paradigms are being released in Qlib to collaboratively solve key challenges in quantitative investment. For example, 1) using supervised learning to mine the market&#039;s complex non-linear patterns from rich and heterogeneous financial data, 2) modeling the dynamic nature of the financial market using adaptive concept drift technology, and 3) using reinforcement learning to model continuous investment decisions and assist investors in optimizing their trading strategies.

It contains the full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution. 
For more details, please refer to our paper [&quot;Qlib: An AI-oriented Quantitative Investment Platform&quot;](https://arxiv.org/abs/2009.11189).


&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Frameworks, Tutorial, Data &amp; DevOps&lt;/th&gt;
      &lt;th&gt;Main Challenges &amp; Solutions in Quant Research&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;li&gt;&lt;a href=&quot;#plans&quot;&gt;&lt;strong&gt;Plans&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#framework-of-qlib&quot;&gt;Framework of Qlib&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#quick-start&quot;&gt;Quick Start&lt;/a&gt;&lt;/li&gt;
          &lt;ul dir=&quot;auto&quot;&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#installation&quot;&gt;Installation&lt;/a&gt; &lt;/li&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#data-preparation&quot;&gt;Data Preparation&lt;/a&gt;&lt;/li&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#auto-quant-research-workflow&quot;&gt;Auto Quant Research Workflow&lt;/a&gt;&lt;/li&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#building-customized-quant-research-workflow-by-code&quot;&gt;Building Customized Quant Research Workflow by Code&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
        &lt;li&gt;&lt;a href=&quot;#quant-dataset-zoo&quot;&gt;&lt;strong&gt;Quant Dataset Zoo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#learning-framework&quot;&gt;Learning Framework&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#more-about-qlib&quot;&gt;More About Qlib&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#offline-mode-and-online-mode&quot;&gt;Offline Mode and Online Mode&lt;/a&gt;
        &lt;ul&gt;
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#performance-of-qlib-data-server&quot;&gt;Performance of Qlib Data Server&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
        &lt;li&gt;&lt;a href=&quot;#related-reports&quot;&gt;Related Reports&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#contact-us&quot;&gt;Contact Us&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#contributing&quot;&gt;Contributing&lt;/a&gt;&lt;/li&gt;
      &lt;/td&gt;
      &lt;td valign=&quot;baseline&quot;&gt;
        &lt;li&gt;&lt;a href=&quot;#main-challenges--solutions-in-quant-research&quot;&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/a&gt;
          &lt;ul&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#forecasting-finding-valuable-signalspatterns&quot;&gt;Forecasting: Finding Valuable Signals/Patterns&lt;/a&gt;
              &lt;ul&gt;
                &lt;li type=&quot;disc&quot;&gt;&lt;a href=&quot;#quant-model-paper-zoo&quot;&gt;&lt;strong&gt;Quant Model (Paper) Zoo&lt;/strong&gt;&lt;/a&gt;
                  &lt;ul&gt;
                    &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#run-a-single-model&quot;&gt;Run a Single Model&lt;/a&gt;&lt;/li&gt;
                    &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#run-multiple-models&quot;&gt;Run Multiple Models&lt;/a&gt;&lt;/li&gt;
                  &lt;/ul&gt;
                &lt;/li&gt;
              &lt;/ul&gt;
            &lt;/li&gt;
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#adapting-to-market-dynamics&quot;&gt;Adapting to Market Dynamics&lt;/a&gt;&lt;/li&gt;
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#reinforcement-learning-modeling-continuous-decisions&quot;&gt;Reinforcement Learning: modeling continuous decisions&lt;/a&gt;&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

# Plans
New features under development(order by estimated release time).
Your feedbacks about the features are very important.
&lt;!-- | Feature                        | Status      | --&gt;
&lt;!-- | --                      | ------    | --&gt;

# Framework of Qlib

&lt;div style=&quot;align: center&quot;&gt;
&lt;img src=&quot;docs/_static/img/framework-abstract.jpg&quot; /&gt;
&lt;/div&gt;

The high-level framework of Qlib can be found above(users can find the [detailed framework](https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework) of Qlib&#039;s design when getting into nitty gritty).
The components are designed as loose-coupled modules, and each component could be used stand-alone.

Qlib provides a strong infrastructure to support Quant research. [Data](https://qlib.readthedocs.io/en/latest/component/data.html) is always an important part.
A strong learning framework is designed to support diverse learning paradigms (e.g. [reinforcement learning](https://qlib.readthedocs.io/en/latest/component/rl.html), [supervised learning](https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section)) and patterns at different levels(e.g. [market dynamic modeling](https://qlib.readthedocs.io/en/latest/component/meta.html)).
By modeling the market, [trading strategies](https://qlib.readthedocs.io/en/latest/component/strategy.html) will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be [nested to be optimized and run together](https://qlib.readthedocs.io/en/latest/component/highfreq.html).
At last, a comprehensive [analysis](https://qlib.readthedocs.io/en/latest/component/report.html) will be provided and the model can be [served online](https://qlib.readthedocs.io/en/latest/component/online.html) in a low cost.


# Quick Start

This quick start guide tries to demonstrate
1. It&#039;s very easy to build a complete Quant research workflow and try your ideas with _Qlib_.
2. Though with *public data* and *simple models*, machine learning technologies **work very well** in practical Quant investment.

Here is a quick **[demo](https://terminalizer.com/view/3f24561a4470)** shows how to install ``Qlib``, and run LightGBM with ``qrun``. **But**, please make sure you have already prepared the data following the [instruction](#data-preparation).


## Installation

This table demonstrates the supported Python version of `Qlib`:
|               | install with pip      | install from source  |        plot        |
| ------------- |:---------------------:|:--------------------:|:------------------:|
| Python 3.8    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.9    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.10   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.11   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.12   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |

**Note**: 
1. **Conda** is suggested for managing your Python environment. In some cases, using Python outside of a `conda` environment may result in missing header files, causing the installation failure of certain packages.
2. Please pay attention that installing cython in Python 3.6 will raise some error when installing ``Qlib`` from source. If users use Python 3.6 on their machines, it is recommended to *upgrade* Python to version 3.8 or higher, or use `conda`&#039;s Python to install ``Qlib`` from source.

### Install with pip
Users can easily install ``Qlib`` by pip according to the following command.

```bash
  pip install pyqlib
```

**Note**: pip will install the latest stable qlib. However, the main branch of qlib is in active development. If you want to test the latest scripts or functions in the main branch. Please install qlib with the methods below.

### Install from source
Also, users can install the latest dev version ``Qlib`` by the source code according to the following steps:

* Before installing ``Qlib`` from source, users need to install some dependencies:

  ```bash
  pip install numpy
  pip install --upgrade cython
  ```

* Clone the repository and install ``Qlib`` as follows.
    ```bash
    git clone https://github.com/microsoft/qlib.git &amp;&amp; cd qlib
    pip install .  # `pip install -e .[dev]` is recommended for development. check details in docs/developer/code_standard_and_dev_guide.rst
    ```

**Tips**: If you fail to install `Qlib` or run the examples in your environment,  comparing your steps and the [CI workflow](.github/workflows/test_qlib_from_source.yml) may help you find the problem.

**Tips for Mac**: If you are using Mac with M1, you might encounter issues in building the wheel for LightGBM, which is due to missing dependencies from OpenMP. To solve the problem, install openmp first with ``brew install libomp`` and then run ``pip install .`` to build it successfully. 

## Data Preparation
‚ùó Due to more restrict data security policy. The offical dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.
Here is an example to download the latest data.
```bash
wget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz
mkdir -p ~/.qlib/qlib_data/cn_data
tar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1
rm -f qlib_bin.tar.gz
```

The official dataset below will resume in short future.


----

Load and prepare data by running the following code:

### Get with module
  ```bash
  # get 1d data
  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

  # get 1min data
  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

  ```

### Get from source

  ```bash
  # get 1d data
  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

  # get 1min data
  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

  ```

This dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in
the same repository.
Users could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)

*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.
We recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.

### Automatic update of daily frequency data (from yahoo finance)
  &gt; This step is *Optional* if users only want to try their models and strategies on history data.
  &gt; 
  &gt; It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.
  &gt;
  &gt; **NOTE**: Users can&#039;t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.
  &gt; 
  &gt; For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)

  * Automatic update of data to the &quot;qlib&quot; directory each trading day(Linux)
      * use *crontab*: `crontab -e`
      * set up timed tasks:

        ```
        * * * * 1-5 python &lt;script path&gt; update_data_to_bin --qlib_data_1d_dir &lt;user data dir&gt;
        ```
        * **script path**: *scripts/data_collector/yahoo/collector.py*

  * Manual update of data
      ```
      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir &lt;user data dir&gt; --trading_date &lt;start date&gt; --end_date &lt;end date&gt;
      ```
      * *trading_date*: start of trading day
      * *end_date*: end of trading day(not included)

### Checking the health of the data
  * We provide a script to check the health of the data, you can run the following commands to check whether the data is healthy or not.
    ```
    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data
    ```
  * Of course, you can also add some parameters to adjust the test results, such as this.
    ```
    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pathwaycom/pathway]]></title>
            <link>https://github.com/pathwaycom/pathway</link>
            <guid>https://github.com/pathwaycom/pathway</guid>
            <pubDate>Wed, 28 May 2025 00:04:35 GMT</pubDate>
            <description><![CDATA[Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pathwaycom/pathway">pathwaycom/pathway</a></h1>
            <p>Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.</p>
            <p>Language: Python</p>
            <p>Stars: 26,145</p>
            <p>Forks: 574</p>
            <p>Stars today: 236 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pathway.com/&quot;&gt;
    &lt;img src=&quot;https://pathway.com/logo-light.svg&quot;/&gt;
  &lt;/a&gt;
  &lt;br /&gt;&lt;br /&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/10388&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/10388&quot; alt=&quot;pathwaycom%2Fpathway | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
  &lt;br /&gt;&lt;br /&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml&quot;&gt;
        &lt;img src=&quot;https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml/badge.svg&quot; alt=&quot;ubuntu&quot;/&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/actions/workflows/release.yml&quot;&gt;
        &lt;img src=&quot;https://github.com/pathwaycom/pathway/actions/workflows/release.yml/badge.svg&quot; alt=&quot;Last release&quot;/&gt;&lt;/a&gt;
        &lt;a href=&quot;https://badge.fury.io/py/pathway&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/pathway.svg&quot; alt=&quot;PyPI version&quot; height=&quot;18&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://badge.fury.io/py/pathway&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/pathway&quot; alt=&quot;PyPI downloads&quot; height=&quot;18&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/blob/main/LICENSE.txt&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/license-BSL-green&quot; alt=&quot;License: BSL&quot;/&gt;&lt;/a&gt;
      &lt;br&gt;
        &lt;a href=&quot;https://discord.gg/pathway&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/discord/1042405378304004156?logo=discord&quot;
            alt=&quot;chat on Discord&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://twitter.com/intent/follow?screen_name=pathway_com&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/twitter/follow/pathwaycom&quot;
            alt=&quot;follow on Twitter&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://linkedin.com/company/pathway&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/pathway-0077B5?style=social&amp;logo=linkedin&quot; alt=&quot;follow on LinkedIn&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://github.com/dylanhogg/awesome-python/blob/main/README.md&quot;&gt;
      &lt;img src=&quot;https://awesome.re/badge.svg&quot; alt=&quot;Awesome Python&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://gurubase.io/g/pathway&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/badge/Gurubase-Ask%20Pathway%20Guru-006BFF&quot; alt=&quot;Pathway Guru&quot;&gt;&lt;/a&gt;
    &lt;br&gt;
    &lt;a href=&quot;#getting-started&quot;&gt;Getting Started&lt;/a&gt; |
    &lt;a href=&quot;#deployment&quot;&gt;Deployment&lt;/a&gt; |
    &lt;a href=&quot;#resources&quot;&gt;Documentation and Support&lt;/a&gt; |
    &lt;a href=&quot;https://pathway.com/blog/&quot;&gt;Blog&lt;/a&gt; |
    &lt;a href=&quot;#license&quot;&gt;License&lt;/a&gt;

  
&lt;/p&gt;

# Pathway&lt;a id=&quot;pathway&quot;&gt; Live Data Framework&lt;/a&gt;

[Pathway](https://pathway.com) is a Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.

Pathway comes with an **easy-to-use Python API**, allowing you to seamlessly integrate your favorite Python ML libraries.
Pathway code is versatile and robust: **you can use it in both development and production environments, handling both batch and streaming data effectively**.
The same code can be used for local development, CI/CD tests, running batch jobs, handling stream replays, and processing data streams.

Pathway is powered by a **scalable Rust engine** based on Differential Dataflow and performs incremental computation.
Your Pathway code, despite being written in Python, is run by the Rust engine, enabling multithreading, multiprocessing, and distributed computations.
All the pipeline is kept in memory and can be easily deployed with **Docker and Kubernetes**.

You can install Pathway with pip:
```
pip install -U pathway
```

For any questions, you will find the community and team behind the project [on Discord](https://discord.com/invite/pathway).

## Use-cases and templates

Ready to see what Pathway can do?

[Try one of our easy-to-run examples](https://pathway.com/developers/templates)!

Available in both notebook and docker formats, these ready-to-launch examples can be launched in just a few clicks. Pick one and start your hands-on experience with Pathway today!

### Event processing and real-time analytics pipelines
With its unified engine for batch and streaming and its full Python compatibility, Pathway makes data processing as easy as possible. It&#039;s the ideal solution for a wide range of data processing pipelines, including:

- [Showcase: Real-time ETL.](https://pathway.com/developers/templates/kafka-etl)
- [Showcase: Event-driven pipelines with alerting.](https://pathway.com/developers/templates/realtime-log-monitoring)
- [Showcase: Realtime analytics.](https://pathway.com/developers/templates/linear_regression_with_kafka/)
- [Docs: Switch from batch to streaming.](https://pathway.com/developers/user-guide/connecting-to-data/switch-from-batch-to-streaming)



### AI Pipelines

Pathway provides dedicated LLM tooling to build live LLM and RAG pipelines. Wrappers for most common LLM services and utilities are included, making working with LLMs and RAGs pipelines incredibly easy. Check out our [LLM xpack documentation](https://pathway.com/developers/user-guide/llm-xpack/overview).

Don&#039;t hesitate to try one of our runnable examples featuring LLM tooling.
You can find such examples [here](https://pathway.com/developers/user-guide/llm-xpack/llm-examples).

  - [Template: Unstructured data to SQL on-the-fly.](https://pathway.com/developers/templates/unstructured-to-structured/)
  - [Template: Private RAG with Ollama and Mistral AI](https://pathway.com/developers/templates/private-rag-ollama-mistral)
  - [Template: Adaptive RAG](https://pathway.com/developers/templates/adaptive-rag)
  - [Template: Multimodal RAG with gpt-4o](https://pathway.com/developers/templates/multimodal-rag)

## Features

- **A wide range of connectors**: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector.
- **Stateless and stateful transformations**: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data.
- **Persistence**: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!
- **Consistency**: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the &quot;at least once&quot; consistency while the enterprise version provides the &quot;exactly once&quot; consistency.
- **Scalable Rust engine**: with Pathway Rust engine, you are free from the usual limits imposed by Python. You can easily do multithreading, multiprocessing, and distributed computations.
- **LLM helpers**: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents.


## Getting started&lt;a id=&quot;getting-started&quot;&gt;&lt;/a&gt;

### Installation&lt;a id=&quot;installation&quot;&gt;&lt;/a&gt;

Pathway requires Python 3.10 or above.

You can install the current release of Pathway using `pip`:

```
$ pip install -U pathway
```

‚ö†Ô∏è Pathway is available on MacOS and Linux. Users of other systems should run Pathway on a Virtual Machine.


### Example: computing the sum of positive values in real time.&lt;a id=&quot;example&quot;&gt;&lt;/a&gt;

```python
import pathway as pw

# Define the schema of your data (Optional)
class InputSchema(pw.Schema):
  value: int

# Connect to your data using connectors
input_table = pw.io.csv.read(
  &quot;./input/&quot;,
  schema=InputSchema
)

#Define your operations on the data
filtered_table = input_table.filter(input_table.value&gt;=0)
result_table = filtered_table.reduce(
  sum_value = pw.reducers.sum(filtered_table.value)
)

# Load your results to external systems
pw.io.jsonlines.write(result_table, &quot;output.jsonl&quot;)

# Run the computation
pw.run()
```

Run Pathway [in Google Colab](https://colab.research.google.com/drive/1aBIJ2HCng-YEUOMrr0qtj0NeZMEyRz55?usp=sharing).

You can find more examples [here](https://github.com/pathwaycom/pathway/tree/main/examples).


## Deployment&lt;a id=&quot;deployment&quot;&gt;&lt;/a&gt;

### Locally&lt;a id=&quot;running-pathway-locally&quot;&gt;&lt;/a&gt;

To use Pathway, you only need to import it:

```python
import pathway as pw
```

Now, you can easily create your processing pipeline, and let Pathway handle the updates. Once your pipeline is created, you can launch the computation on streaming data with a one-line command:

```python
pw.run()
```

You can then run your Pathway project (say, `main.py`) just like a normal Python script: `$ python main.py`.
Pathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages. 

&lt;img src=&quot;https://d14l3brkh44201.cloudfront.net/pathway-dashboard.png&quot; width=&quot;1326&quot; alt=&quot;Pathway dashboard&quot;/&gt;

Alternatively, you can use the pathway&#039;ish version:

```
$ pathway spawn python main.py
```

Pathway natively supports multithreading.
To launch your application with 3 threads, you can do as follows:
```
$ pathway spawn --threads 3 python main.py
```

To jumpstart a Pathway project, you can use our [cookiecutter template](https://github.com/pathwaycom/cookiecutter-pathway).


### Docker&lt;a id=&quot;docker&quot;&gt;&lt;/a&gt;

You can easily run Pathway using docker.

#### Pathway image

You can use the [Pathway docker image](https://hub.docker.com/r/pathwaycom/pathway), using a Dockerfile:

```dockerfile
FROM pathwaycom/pathway:latest

WORKDIR /app

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD [ &quot;python&quot;, &quot;./your-script.py&quot; ]
```

You can then build and run the Docker image:

```console
docker build -t my-pathway-app .
docker run -it --rm --name my-pathway-app my-pathway-app
```

#### Run a single Python script

When dealing with single-file projects, creating a full-fledged `Dockerfile`
might seem unnecessary. In such scenarios, you can execute a
Python script directly using the Pathway Docker image. For example:

```console
docker run -it --rm --name my-pathway-app -v &quot;$PWD&quot;:/app pathwaycom/pathway:latest python my-pathway-app.py
```

#### Python docker image

You can also use a standard Python image and install Pathway using pip with a Dockerfile:

```dockerfile
FROM --platform=linux/x86_64 python:3.10

RUN pip install -U pathway
COPY ./pathway-script.py pathway-script.py

CMD [&quot;python&quot;, &quot;-u&quot;, &quot;pathway-script.py&quot;]
```

### Kubernetes and cloud&lt;a id=&quot;k8s&quot;&gt;&lt;/a&gt;

Docker containers are ideally suited for deployment on the cloud with Kubernetes.
If you want to scale your Pathway application, you may be interested in our Pathway for Enterprise.
Pathway for Enterprise is specially tailored towards end-to-end data processing and real time intelligent analytics.
It scales using distributed computing on the cloud and supports distributed Kubernetes deployment, with external persistence setup.

You can easily deploy Pathway using services like Render: see [how to deploy Pathway in a few clicks](https://pathway.com/developers/user-guide/deployment/render-deploy/).

If you are interested, don&#039;t hesitate to [contact us](mailto:contact@pathway.com) to learn more.

## Performance&lt;a id=&quot;performance&quot;&gt;&lt;/a&gt;

Pathway is made to outperform state-of-the-art technologies designed for streaming and batch data processing tasks, including: Flink, Spark, and Kafka Streaming. It also makes it possible to implement a lot of algorithms/UDF&#039;s in streaming mode which are not readily supported by other streaming frameworks (especially: temporal joins, iterative graph algorithms, machine learning routines).

If you are curious, here are [some benchmarks to play with](https://github.com/pathwaycom/pathway-benchmarks).

&lt;img src=&quot;https://github.com/pathwaycom/pathway-benchmarks/raw/main/images/bm-wordcount-lineplot.png&quot; width=&quot;1326&quot; alt=&quot;WordCount Graph&quot;/&gt;

## Documentation and Support&lt;a id=&quot;resources&quot;&gt;&lt;/a&gt;

The entire documentation of Pathway is available at [pathway.com/developers/](https://pathway.com/developers/user-guide/introduction/welcome), including the [API Docs](https://pathway.com/developers/api-docs/pathway).

If you have any question, don&#039;t hesitate to [open an issue on GitHub](https://github.com/pathwaycom/pathway/issues), join us on [Discord](https://discord.com/invite/pathway), or send us an email at [contact@pathway.com](mailto:contact@pathway.com).

## License&lt;a id=&quot;license&quot;&gt;&lt;/a&gt;

Pathway is distributed on a [BSL 1.1 License](https://github.com/pathwaycom/pathway/blob/main/LICENSE.txt) which allows for unlimited non-commercial use, as well as use of the Pathway package [for most commercial purposes](https://pathway.com/license/), free of charge. Code in this repository automatically converts to Open Source (Apache 2.0 License) after 4 years. Some [public repos](https://github.com/pathwaycom) which are complementary to this one (examples, libraries, connectors, etc.) are licensed as Open Source, under the MIT license.


## Contribution guidelines&lt;a id=&quot;contribution-guidelines&quot;&gt;&lt;/a&gt;

If you develop a library or connector which you would like to integrate with this repo, we suggest releasing it first as a separate repo on a MIT/Apache 2.0 license. 

For all concerns regarding core Pathway functionalities, Issues are encouraged. For further information, don&#039;t hesitate to engage with Pathway&#039;s [Discord community](https://discord.gg/pathway).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/RD-Agent]]></title>
            <link>https://github.com/microsoft/RD-Agent</link>
            <guid>https://github.com/microsoft/RD-Agent</guid>
            <pubDate>Wed, 28 May 2025 00:04:34 GMT</pubDate>
            <description><![CDATA[Research and development (R&D) is crucial for the enhancement of industrial productivity, especially in the AI era, where the core aspects of R&D are mainly focused on data and models. We are committed to automating these high-value generic R&D processes through R&D-Agent, which lets AI drive data-driven AI. üîóhttps://aka.ms/RD-Agent-Tech-Report]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/RD-Agent">microsoft/RD-Agent</a></h1>
            <p>Research and development (R&D) is crucial for the enhancement of industrial productivity, especially in the AI era, where the core aspects of R&D are mainly focused on data and models. We are committed to automating these high-value generic R&D processes through R&D-Agent, which lets AI drive data-driven AI. üîóhttps://aka.ms/RD-Agent-Tech-Report</p>
            <p>Language: Python</p>
            <p>Stars: 4,972</p>
            <p>Forks: 444</p>
            <p>Stars today: 187 stars today</p>
            <h2>README</h2><pre>&lt;h4 align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/_static/logo.png&quot; alt=&quot;RA-Agent logo&quot; style=&quot;width:70%; &quot;&gt;
  
  &lt;a href=&quot;https://rdagent.azurewebsites.net&quot; target=&quot;_blank&quot;&gt;üñ•Ô∏è Live Demo&lt;/a&gt; |
  &lt;a href=&quot;https://rdagent.azurewebsites.net/factor_loop&quot; target=&quot;_blank&quot;&gt;üé• Demo Video&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=JJ4JYO3HscM&amp;list=PLALmKB0_N3_i52fhUmPQiL4jsO354uopR&quot; target=&quot;_blank&quot;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;   |
  &lt;a href=&quot;https://rdagent.readthedocs.io/en/latest/index.html&quot; target=&quot;_blank&quot;&gt;üìñ Documentation&lt;/a&gt; |
  &lt;a href=&quot;https://aka.ms/RD-Agent-Tech-Report&quot; target=&quot;_blank&quot;&gt;üìÑ Tech Report&lt;/a&gt; |
  &lt;a href=&quot;#-paperwork-list&quot;&gt; üìÉ Papers &lt;/a&gt;
&lt;/h3&gt;


[![CI](https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml)
[![CodeQL](https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql)
[![Dependabot Updates](https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates)
[![Lint PR Title](https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml)
[![Release.yml](https://github.com/microsoft/RD-Agent/actions/workflows/release.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/release.yml)
[![Platform](https://img.shields.io/badge/platform-Linux-blue)](https://pypi.org/project/rdagent/#files)
[![PyPI](https://img.shields.io/pypi/v/rdagent)](https://pypi.org/project/rdagent/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/rdagent)](https://pypi.org/project/rdagent/)
[![Release](https://img.shields.io/github/v/release/microsoft/RD-Agent)](https://github.com/microsoft/RD-Agent/releases)
[![GitHub](https://img.shields.io/github/license/microsoft/RD-Agent)](https://github.com/microsoft/RD-Agent/blob/main/LICENSE)
[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit)](https://github.com/pre-commit/pre-commit)
[![Checked with mypy](https://www.mypy-lang.org/static/mypy_badge.svg)](http://mypy-lang.org/)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
[![Chat](https://img.shields.io/badge/chat-discord-blue)](https://discord.gg/ybQ97B6Jjy)
[![Documentation Status](https://readthedocs.org/projects/rdagent/badge/?version=latest)](https://rdagent.readthedocs.io/en/latest/?badge=latest)
[![Readthedocs Preview](https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml) &lt;!-- this badge is too long, please place it in the last one to make it pretty --&gt; 
[![arXiv](https://img.shields.io/badge/arXiv-2505.14738-00ff00.svg)](https://arxiv.org/abs/2505.14738)



# üèÜ The Best Machine Learning Engineering Agent!

[MLE-bench](https://github.com/openai/mle-bench) is a comprehensive benchmark evaluating the performance of AI agents on machine learning engineering tasks. Utilizing datasets from 75 Kaggle competitions, MLE-bench provides robust assessments of AI systems&#039; capabilities in real-world ML engineering scenarios.

R&amp;D-Agent currently leads as the top-performing machine learning engineering agent on MLE-bench:

| Agent | Low == Lite (%) | Medium (%) | High (%) | All (%) |
|---------|--------|-----------|---------|----------|
| R&amp;D-Agent o1-preview | 48.18 ¬± 2.49 | 8.95 ¬± 2.36 | 18.67 ¬± 2.98 | 22.4 ¬± 1.1 |
| R&amp;D-Agent o3(R)+GPT-4.1(D) | 51.52 ¬± 6.21 | 7.89 ¬± 3.33 | 16.67 ¬± 3.65 | 22.45 ¬± 2.45 |
| AIDE o1-preview | 34.3 ¬± 2.4 | 8.8 ¬± 1.1 | 10.0 ¬± 1.9 | 16.9 ¬± 1.1 |

**Notes:**
- **O3(R)+GPT-4.1(D)**: This version is designed to both reduce average time per loop and leverage a cost-effective combination of backend LLMs by seamlessly integrating Research Agent (o3) with Development Agent (GPT-4.1).
- **AIDE o1-preview**: Represents the previously best public result on MLE-bench as reported in the original MLE-bench paper.
- Average and standard deviation results for R&amp;D-Agent o1-preview is based on a independent of 5 seeds and for R&amp;D-Agent o3(R)+GPT-4.1(D) is based on 6 seeds.
- According to MLE-Bench, the 75 competitions are categorized into three levels of complexity: **Low==Lite** if we estimate that an experienced ML engineer can produce a sensible solution in under 2 hours, excluding the time taken to train any models; **Medium** if it takes between 2 and 10 hours; and **High** if it takes more than 10 hours.

You can inspect the detailed runs of the above results online.
- [R&amp;D-Agent o1-preview detailed runs](https://aka.ms/RD-Agent_MLE-Bench_O1-preview)
- [R&amp;D-Agent o3(R)+GPT-4.1(D) detailed runs](https://aka.ms/RD-Agent_MLE-Bench_O3_GPT41)

For running R&amp;D-Agent on MLE-bench, refer to **[MLE-bench Guide: Running ML Engineering via MLE-bench](https://rdagent.readthedocs.io/en/latest/scens/data_science.html)**


# üì∞ News
| üóûÔ∏è News        | üìù Description                 |
| --            | ------      |
| [Technical Report Release](#overall-technical-report) | Overall framework description and results on MLE-bench | 
| [R&amp;D-Agent-Quant Release](#deep-application-in-diverse-scenarios) | Apply R&amp;D-Agent to quant trading | 
| MLE-Bench Results Released | R&amp;D-Agent currently leads as the [top-performing machine learning engineering agent](#-the-best-machine-learning-engineering-agent) on MLE-bench |
| Support LiteLLM Backend | We now fully support **[LiteLLM](https://github.com/BerriAI/litellm)** as a backend for integration with multiple LLM providers. |
| More General Data Science Agent | üöÄComing soon! |
| Kaggle Scenario release | We release **[Kaggle Agent](https://rdagent.readthedocs.io/en/latest/scens/kaggle_agent.html)**, try the new features!                  |
| Official WeChat group release  | We created a WeChat group, welcome to join! (üó™[QR Code](https://github.com/microsoft/RD-Agent/issues/880)) |
| Official Discord release  | We launch our first chatting channel in Discord (üó™[![Chat](https://img.shields.io/badge/chat-discord-blue)](https://discord.gg/ybQ97B6Jjy)) |
| First release | **R&amp;D-Agent** is released on GitHub |



# Data Science Agent Preview
Check out our demo video showcasing the current progress of our Data Science Agent under development:

https://github.com/user-attachments/assets/3eccbecb-34a4-4c81-bce4-d3f8862f7305

# üåü Introduction
&lt;div align=&quot;center&quot;&gt;
      &lt;img src=&quot;docs/_static/scen.png&quot; alt=&quot;Our focused scenario&quot; style=&quot;width:80%; &quot;&gt;
&lt;/div&gt;

R&amp;D-Agent aims to automate the most critical and valuable aspects of the industrial R&amp;D process, and we begin with focusing on the data-driven scenarios to streamline the development of models and data. 
Methodologically, we have identified a framework with two key components: &#039;R&#039; for proposing new ideas and &#039;D&#039; for implementing them.
We believe that the automatic evolution of R&amp;D will lead to solutions of significant industrial value.


&lt;!-- Tag Cloud --&gt;
R&amp;D is a very general scenario. The advent of R&amp;D-Agent can be your
- üí∞ **Automatic Quant Factory** ([üé•Demo Video](https://rdagent.azurewebsites.net/factor_loop)|[‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=X4DK2QZKaKY&amp;t=6s))
- ü§ñ **Data Mining Agent:** Iteratively proposing data &amp; models ([üé•Demo Video 1](https://rdagent.azurewebsites.net/model_loop)|[‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=dm0dWL49Bc0&amp;t=104s)) ([üé•Demo Video 2](https://rdagent.azurewebsites.net/dmm)|[‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=VIaSTZuoZg4))  and implementing them by gaining knowledge from data.
- ü¶æ **Research Copilot:** Auto read research papers ([üé•Demo Video](https://rdagent.azurewebsites.net/report_model)|[‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=BiA2SfdKQ7o)) / financial reports ([üé•Demo Video](https://rdagent.azurewebsites.net/report_factor)|[‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=ECLTXVcSx-c)) and implement model structures or building datasets.
- ü§ñ **Kaggle Agent:** Auto Model Tuning and Feature Engineering([üé•Demo Video Coming Soon...]()) and implementing them to achieve more in competitions.
- ...

You can click the links above to view the demo. We&#039;re continuously adding more methods and scenarios to the project to enhance your R&amp;D processes and boost productivity. 

Additionally, you can take a closer look at the examples in our **[üñ•Ô∏è Live Demo](https://rdagent.azurewebsites.net/)**.

&lt;div align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://rdagent.azurewebsites.net/&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;docs/_static/demo.png&quot; alt=&quot;Watch the demo&quot; width=&quot;80%&quot;&gt;
    &lt;/a&gt;
&lt;/div&gt;


# ‚ö° Quick start

You can try above demos by running the following command:

### üê≥ Docker installation.
Users must ensure Docker is installed before attempting most scenarios. Please refer to the [official üê≥Docker page](https://docs.docker.com/engine/install/) for installation instructions.
Ensure the current user can run Docker commands **without using sudo**. You can verify this by executing `docker run hello-world`.

### üêç Create a Conda Environment
- Create a new conda environment with Python (3.10 and 3.11 are well-tested in our CI):
  ```sh
  conda create -n rdagent python=3.10
  ```
- Activate the environment:
  ```sh
  conda activate rdagent
  ```

### üõ†Ô∏è Install the R&amp;D-Agent
- You can directly install the R&amp;D-Agent package from PyPI:
  ```sh
  pip install rdagent
  ```

### üíä Health check
- rdagent provides a health check that currently checks two things.
  - whether the docker installation was successful.
  - whether the default port used by the [rdagent ui](https://github.com/microsoft/RD-Agent?tab=readme-ov-file#%EF%B8%8F-monitor-the-application-results) is occupied.
  ```sh
  rdagent health_check
  ```


### ‚öôÔ∏è Configuration
- The demos requires following ability:
  - ChatCompletion
  - json_mode
  - embedding query

- For example: If you are using the `OpenAI API`, you have to configure your GPT model in the `.env` file like this.
  ```bash
  cat &lt;&lt; EOF  &gt; .env
  OPENAI_API_KEY=&lt;replace_with_your_openai_api_key&gt;
  # EMBEDDING_MODEL=text-embedding-3-small
  CHAT_MODEL=gpt-4-turbo
  EOF
  ```
- However, not every API services support these features by default. For example: `AZURE OpenAI`, you have to configure your GPT model in the `.env` file like this.
  ```bash
  cat &lt;&lt; EOF  &gt; .env
  USE_AZURE=True
  EMBEDDING_OPENAI_API_KEY=&lt;replace_with_your_azure_openai_api_key&gt;
  EMBEDDING_AZURE_API_BASE=&lt;replace_with_your_azure_endpoint&gt;
  EMBEDDING_AZURE_API_VERSION=&lt;replace_with_the_version_of_your_azure_openai_api&gt;
  EMBEDDING_MODEL=text-embedding-3-small
  CHAT_OPENAI_API_KEY=&lt;replace_with_your_azure_openai_api_key&gt;
  CHAT_AZURE_API_BASE=&lt;replace_with_your_azure_endpoint&gt;
  CHAT_AZURE_API_VERSION=&lt;replace_with_the_version_of_your_azure_openai_api&gt;
  CHAT_MODEL=&lt;replace_it_with_the_name_of_your_azure_chat_model&gt;
  EOF
  ```

- We now support LiteLLM as a backend for integration with multiple LLM providers. If you use LiteLLM Backend to use models, you can configure as follows:
  ```bash
  cat &lt;&lt; EOF  &gt; .env
  BACKEND=rdagent.oai.backend.LiteLLMAPIBackend
  # It can be modified to any model supported by LiteLLM.
  CHAT_MODEL=gpt-4o
  EMBEDDING_MODEL=text-embedding-3-small
  # The backend api_key fully follow the convention of litellm.
  OPENAI_API_KEY=&lt;replace_with_your_openai_api_key&gt;
  ```
  
- For more configuration information, please refer to the [documentation](https://rdagent.readthedocs.io/en/latest/installation_and_configuration.html).

### üöÄ Run the Application

The **[üñ•Ô∏è Live Demo](https://rdagent.azurewebsites.net/)** is implemented by the following commands(each item represents one demo, you can select the one you prefer):

- Run the **Automated Quantitative Trading &amp; Iterative Factors Evolution**:  [Qlib](http://github.com/microsoft/qlib) self-loop factor proposal and implementation application
  ```sh
  rdagent fin_factor
  ```

- Run the **Automated Quantitative Trading &amp; Iterative Model Evolution**: [Qlib](http://github.com/microsoft/qlib) self-loop model proposal and implementation application
  ```sh
  rdagent fin_model
  ```

- Run the **Automated Medical Prediction Model Evolution**: Medical self-loop model proposal and implementation application
  &gt;(1) Apply for an account at [PhysioNet](https://physionet.org/). &lt;br /&gt; (2) Request access to FIDDLE preprocessed data: [FIDDLE Dataset](https://physionet.org/content/mimic-eicu-fiddle-feature/1.0.0/). &lt;br /&gt;
  (3) Place your username and password in `.env`.
  ```bash
  cat &lt;&lt; EOF  &gt;&gt; .env
  DM_USERNAME=&lt;your_username&gt;
  DM_PASSWORD=&lt;your_password&gt;
  EOF
  ```
  ```sh
  rdagent med_model
  ```

- Run the **Automated Quantitative Trading &amp; Factors Extraction from Financial Reports**:  Run the [Qlib](http://github.com/microsoft/qlib) factor extraction and implementation application based on financial reports
  ```sh
  # 1. Generally, you can run this scenario using the following command:
  rdagent fin_factor_report --report_folder=&lt;Your financial reports folder path&gt;

  # 2. Specifically, you need to prepare some financial reports first. You can follow this concrete example:
  wget https://github.com/SunsetWolf/rdagent_resource/releases/download/reports/all_reports.zip
  unzip all_reports.zip -d git_ignore_folder/reports
  rdagent fin_factor_report --report_folder=git_ignore_folder/reports
  ```

- Run the **Automated Model Research &amp; Development Copilot**: model extraction and implementation application
  ```sh
  # 1. Generally, you can run your own papers/reports with the following command:
  rdagent general_model &lt;Your paper URL&gt;

  # 2. Specifically, you can do it like this. For more details and additional paper examples, use `rdagent general_model -h`:
  rdagent general_model  &quot;https://arxiv.org/pdf/2210.09789&quot;
  ```

- Run the **Automated Kaggle Model Tuning &amp; Feature Engineering**:  self-loop model proposal and feature engineering implementation application &lt;br /&gt;
  &gt; Using **sf-crime** *(San Francisco Crime Classification)* as an example. &lt;br /&gt;
  &gt; 1. Register and login on the [Kaggle](https://www.kaggle.com/) website. &lt;br /&gt;
  &gt; 2. Configuring the Kaggle API. &lt;br /&gt;
  &gt; (1) Click on the avatar (usually in the top right corner of the page) -&gt; `Settings` -&gt; `Create New Token`, A file called `kaggle.json` will be downloaded. &lt;br /&gt;
  &gt; (2) Move `kaggle.json` to `~/.config/kaggle/` &lt;br /&gt;
  &gt; (3) Modify the permissions of the kaggle.json file. Reference command: `chmod 600 ~/.config/kaggle/kaggle.json` &lt;br /&gt;
  &gt; 3. Join the competition: Click `Join the competition` -&gt; `I Understand and Accept` at the bottom of the [competition details page](https://www.kaggle.com/competitions/sf-crime/data).
  ```bash
  # Generally, you can run the Kaggle competition program with the following command:
  rdagent kaggle --competition &lt;your competition name&gt;

  # Specifically, you need to create a folder for storing competition files (e.g., competition description file, competition datasets, etc.), and configure the path to the folder in your environment. In addition, you need to use chromedriver when you download the competition descriptors, which you can follow for this specific example:
  
  # 1. Install chromedriver.

  # 2. Add the competition description file path to the `.env` file.
  mkdir -p ./git_ignore_folder/kaggle_data
  dotenv set KG_LOCAL_DATA_PATH &quot;$(pwd)/git_ignore_folder/kaggle_data&quot;

  # 3. run the application
  rdagent kaggle --competition sf-crime
  ```
  &gt; **Description of the above example:** &lt;br /&gt;
  &gt; - Kaggle competition data is roughly divided into three sections: competition description file (json file) and complete dataset for the competition and simplified dataset for the competition. &lt;br /&gt;
  &gt; - The Kaggle competition data will be downloaded automatically, the download process depends on `chromedriver`, installation instructions can be found in the [documentation](https://rdagent.readthedocs.io/en/latest/scens/kaggle_agent.html#example-guide). &lt;br /&gt;

### üñ•Ô∏è Monitor the Application Results
- You can run the following command for our demo program to see the run logs.

  ```sh
  rdagent ui --port 19899 --log_dir &lt;your log folder like &quot;log/&quot;&gt;
  ```

  **Note:** Although port 19899 is not commonly used, but before you run this demo, you need to check if port 19899 is occupied. If it is, please change it to another port that is not occupied.

  You can check if a port is occupied by running the following command.

  ```sh
  rdagent health_check
  ```

# üè≠ Scenarios

We have applied R&amp;D-Agent to multiple valuable data-driven industrial scenarios.


## üéØ Goal: Agent for Data-driven R&amp;D

In this project, we are aiming to build an Agent to automate Data-Driven R\&amp;D that can
+ üìÑ Read real-world material (reports, papers, etc.) and **extract** key formulas, descriptions of interested **features** and **models**, which are the key components of data-driven R&amp;D .
+ üõ†Ô∏è **Implement** the extracted formulas (e.g., features, factors, and models) in runnable codes.
   + Due to the limited ability of LLM in implementing at once, build an evolving process for the agent to improve performance by learning from feedback and knowledge.
+ üí° Propose **new ideas** based on current knowledge and observations.

&lt;!-- ![Data-Centric R&amp;D Overview](docs/_static/overview.png) --&gt;

## üìà Scenarios/Demos

In the two key areas of data-driven scenarios, model implementation and data building, our system aims to serve two main roles: ü¶æCopilot and ü§ñAgent. 
- The ü¶æCopilot follows human instructions to automate repetitive tasks. 
- The ü§ñAgent, being more autonomous, actively proposes ideas for better results in the future.

The supported scenarios are listed below:

| Scenario/Target | Model Implementation                   | Data Building                                                                      |
| --              | --                                     | --                                                                                 |
| **üíπ Finance**      | ü§ñ [Iteratively Proposing Ideas &amp; Evolving](https://rdagent.azurewebsites.net/model_loop)[‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=dm0dWL49Bc0&amp;t=104s) |  ü§ñ [Iteratively Proposing Ideas &amp; Evolving](https://rdagent.azurewebsites.net/factor_loop) [‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=X4DK2QZKaKY&amp;t=6s) &lt;br/&gt;   ü¶æ [Auto reports reading &amp; implementation](https://rdagent.azurewebsites.net/report_factor)[‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=ECLTXVcSx-c)  |
| **ü©∫ Medical**      | ü§ñ [Iteratively Proposing Ideas &amp; Evolving](https://rdagent.azurewebsites.net/dmm)[‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=VIaSTZuoZg4) | -                                                                                  |
| **üè≠ General**      | ü¶æ [Auto paper reading &amp; implementation](https://rdagent.azurewebsites.net/report_model)[‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=BiA2SfdKQ7o) &lt;br/&gt; ü§ñ Auto Kaggle Model Tuning   | ü§ñAuto Kaggle feature Engineering |

- **[RoadMap](https://rdagent.readthedocs.io/en/latest/scens/kaggle_agent.html#roadmap)**: Currently, we are working hard to add new features to the Kaggle scenario.

Different scenarios vary in entrance and configuration. Please check the detailed setup tutorial in the scenarios documents.

Here is a gallery of [successful explorations](https://github.com/SunsetWolf/rdagent_resource/releases/download/demo_traces/demo_traces.zip) (5 traces showed in **[üñ•Ô∏è Live Demo](https://rdagent.azurewebsites.net/)**). You can download and view the execution trace using [this command](https://github.com/microsoft/RD-Agent?tab=readme-ov-file#%EF%B8%8F-monitor-the-application-results) from the documentation.

Please refer to **[üìñreadthedocs_scen](https://rdagent.readthedocs.io/en/latest/scens/catalog.htm

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mindsdb/mindsdb]]></title>
            <link>https://github.com/mindsdb/mindsdb</link>
            <guid>https://github.com/mindsdb/mindsdb</guid>
            <pubDate>Wed, 28 May 2025 00:04:33 GMT</pubDate>
            <description><![CDATA[AI's query engine - Platform for building AI that can answer questions over large scale federated data. - The only MCP Server you'll ever need]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mindsdb/mindsdb">mindsdb/mindsdb</a></h1>
            <p>AI's query engine - Platform for building AI that can answer questions over large scale federated data. - The only MCP Server you'll ever need</p>
            <p>Language: Python</p>
            <p>Stars: 30,389</p>
            <p>Forks: 5,144</p>
            <p>Stars today: 300 stars today</p>
            <h2>README</h2><pre>

&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
	&lt;a href=&quot;https://pypi.org/project/MindsDB/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/MindsDB.svg&quot; alt=&quot;MindsDB Release&quot;&gt;&lt;/a&gt;
	&lt;a href=&quot;https://www.python.org/downloads/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/python-3.10.x%7C%203.11.x-brightgreen.svg&quot; alt=&quot;Python supported&quot;&gt;&lt;/a&gt;
	&lt;a href=&quot;https://ossrank.com/p/630&quot;&gt;&lt;img src=&quot;https://shields.io/endpoint?url=https://ossrank.com/shield/630&quot;&gt;&lt;/a&gt;
	&lt;img alt=&quot;PyPI - Downloads&quot; src=&quot;https://img.shields.io/pypi/dm/Mindsdb&quot;&gt;
	&lt;a href=&quot;https://hub.docker.com/u/mindsdb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/mindsdb/mindsdb&quot; alt=&quot;Docker pulls&quot;&gt;&lt;/a&gt;

  &lt;br /&gt;
  &lt;br /&gt;

  &lt;a href=&quot;https://github.com/mindsdb/mindsdb&quot;&gt;
    &lt;img src=&quot;/docs/assets/mindsdb_logo.png&quot; alt=&quot;MindsDB&quot; width=&quot;300&quot;&gt;
  &lt;/a&gt;

  &lt;p align=&quot;center&quot;&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://www.mindsdb.com?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Website&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://docs.mindsdb.com?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Docs&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://mdb.ai/register&quot;&gt;Demo&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://mindsdb.com/joincommunity?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Community Slack&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

----------------------------------------


MindsDB enables humans, AI, agents, and applications to get highly accurate answers across sprawled and large scale data sources.

![image](https://github.com/user-attachments/assets/a796276a-2d3e-4aa2-9a52-25bf44cf32e7)


[MindsDB has an MCP server built in](https://docs.mindsdb.com/mcp/overview) that enables your MCP applications to connect, unify and respond to questions over large-scale federated data‚Äîspanning databases, data warehouses, and SaaS applications.

## Minds [Demo](https://mdb.ai/register)
Play with [Minds demo](https://mdb.ai/register), and see the power of MindsDB at answering questions from structured to unstructured data, whether it&#039;s scattered across SaaS applications, databases, or... hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered.
 
## Install MindsDB Server 

MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart&#039;s content.

  * [Using Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop). This is the fastest and recommended way to get started and have it all running.
  * [Using Docker](https://docs.mindsdb.com/setup/self-hosted/docker). This is also simple, but gives you more flexibility on how to further customize your server.
  * [Using PyPI](https://docs.mindsdb.com/contribute/install). This option enables you to contribute to MindsDB.

----------------------------------------

# Core Philosophy: Connect, Unify, Respond

MindsDB&#039;s architecture is built around three fundamental capabilities:

## [Connect](https://docs.mindsdb.com/integrations/data-overview) Your Data

You can connect to hundreds of enterprise [data sources (learn more)](https://docs.mindsdb.com/integrations/data-overview). These integrations allow MindsDB to access data wherever it resides, forming the foundation for all other capabilities.

## [Unify](https://docs.mindsdb.com/mindsdb_sql/overview) Your Data

Once connected, these data sources can be queried using a full SQL dialect, as if they were all part of a single database. MindsDB‚Äôs federated query engine translates your SQL queries and executes them on the appropriate connected data sources.

When working with many data sources, it‚Äôs important to prepare and unify your data before generating responses from it. MindsDB SQL offers virtual tables (views, knowledge bases, ml-models) to allow working with heterogeneous data as if it were unified in a single organized system.

* [**VIEWS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/view) ‚Äì Simplify data access by creating unified views across different sources (no-ETL).
* [**KNOWLEDGE BASES**](https://docs.mindsdb.com/mindsdb_sql/knowledge-bases) ‚Äì Index and organize unstructured data for efficient retrieval.
* [**ML MODELS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/model) ‚Äì Apply AI/ML transformations to gain insights from your data.

Unification of data can be automated using JOBs

* [**JOBS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/jobs) ‚Äì Schedule synchronization and transformation tasks for real-time processing.


## [Respond](https://docs.mindsdb.com/mindsdb_sql/agents/agent) From Your Data

Chat with Your Data

* [**AGENTS**](https://docs.mindsdb.com/mindsdb_sql/agents/agent) ‚Äì Configure built-in agents specialized in answering questions over your connected and unified data.
* [**MCP**](https://docs.mindsdb.com/mcp/overview) ‚Äì Connect to MindsDB through the MCP (Model Context Protocol) for seamless interaction.

----------------------------------------

## ü§ù Contribute

Interested in contributing to MindsDB? Follow our [installation guide for development](https://docs.mindsdb.com/contribute/install?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

You can find our [contribution guide here](https://docs.mindsdb.com/contribute/contribute?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

We welcome suggestions! Feel free to open new issues with your ideas, and we‚Äôll guide you.

This project adheres to a [Contributor Code of Conduct](https://github.com/mindsdb/mindsdb/blob/main/CODE_OF_CONDUCT.md). By participating, you agree to follow its terms.

Also, check out our [community rewards and programs](https://mindsdb.com/community?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

## ü§ç Support

If you find a bug, please submit an [issue on GitHub](https://github.com/mindsdb/mindsdb/issues/new/choose).

Here‚Äôs how you can get community support:

* Ask a question in our [Slack Community](https://mindsdb.com/joincommunity?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).
* Join our [GitHub Discussions](https://github.com/mindsdb/mindsdb/discussions).
* Post on [Stack Overflow](https://stackoverflow.com/questions/tagged/mindsdb) with the MindsDB tag.

For commercial support, please [contact the MindsDB team](https://mindsdb.com/contact?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

## üíö Current Contributors

&lt;a href=&quot;https://github.com/mindsdb/mindsdb/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contributors-img.web.app/image?repo=mindsdb/mindsdb&quot; /&gt;
&lt;/a&gt;

Generated with [contributors-img](https://contributors-img.web.app).

## üîî Subscribe for Updates

Join our [Slack community](https://mindsdb.com/joincommunity)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sktime/sktime]]></title>
            <link>https://github.com/sktime/sktime</link>
            <guid>https://github.com/sktime/sktime</guid>
            <pubDate>Wed, 28 May 2025 00:04:32 GMT</pubDate>
            <description><![CDATA[A unified framework for machine learning with time series]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sktime/sktime">sktime/sktime</a></h1>
            <p>A unified framework for machine learning with time series</p>
            <p>Language: Python</p>
            <p>Stars: 8,599</p>
            <p>Forks: 1,574</p>
            <p>Stars today: 67 stars today</p>
            <h2>README</h2><pre>
## Welcome to sktime

&lt;a href=&quot;https://www.sktime.net&quot;&gt;&lt;img src=&quot;https://github.com/sktime/sktime/blob/main/docs/source/images/sktime-logo.svg&quot; width=&quot;175&quot; align=&quot;right&quot; /&gt;&lt;/a&gt;

&gt; A unified interface for machine learning with time series

:rocket: **Version 0.37.0 out now!** [Check out the release notes here](https://www.sktime.net/en/latest/changelog.html).

sktime is a library for time series analysis in Python. It provides a unified interface for multiple time series learning tasks. Currently, this includes forecasting, time series classification, clustering, anomaly/changepoint detection, and other tasks. It comes with [time series algorithms](https://www.sktime.net/en/stable/estimator_overview.html) and [scikit-learn] compatible tools to build, tune, and validate time series models.

[scikit-learn]: https://scikit-learn.org/stable/

|  | **[Documentation](https://www.sktime.net/en/stable/users.html)** ¬∑ **[Tutorials](https://www.sktime.net/en/stable/examples.html)** ¬∑ **[Release Notes](https://www.sktime.net/en/stable/changelog.html)** |
|---|---|
| **Open&amp;#160;Source** | [![BSD 3-clause](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](https://github.com/sktime/sktime/blob/main/LICENSE) |
| **Tutorials** | [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/sktime/sktime/main?filepath=examples) [![!youtube](https://img.shields.io/static/v1?logo=youtube&amp;label=YouTube&amp;message=tutorials&amp;color=red)](https://www.youtube.com/playlist?list=PLKs3UgGjlWHqNzu0LEOeLKvnjvvest2d0) |
| **Community** | [![!discord](https://img.shields.io/static/v1?logo=discord&amp;label=discord&amp;message=chat&amp;color=lightgreen)](https://discord.com/invite/54ACzaFsn7) [![!slack](https://img.shields.io/static/v1?logo=linkedin&amp;label=LinkedIn&amp;message=news&amp;color=lightblue)](https://www.linkedin.com/company/scikit-time/)  |
| **CI/CD** | [![github-actions](https://img.shields.io/github/actions/workflow/status/sktime/sktime/wheels.yml?logo=github)](https://github.com/sktime/sktime/actions/workflows/wheels.yml) [![readthedocs](https://img.shields.io/readthedocs/sktime?logo=readthedocs)](https://www.sktime.net/en/latest/?badge=latest) [![platform](https://img.shields.io/conda/pn/conda-forge/sktime)](https://github.com/sktime/sktime) |
| **Code** |  [![!pypi](https://img.shields.io/pypi/v/sktime?color=orange)](https://pypi.org/project/sktime/) [![!conda](https://img.shields.io/conda/vn/conda-forge/sktime)](https://anaconda.org/conda-forge/sktime) [![!python-versions](https://img.shields.io/pypi/pyversions/sktime)](https://www.python.org/) [![!black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)  |
| **Downloads** | ![PyPI - Downloads](https://img.shields.io/pypi/dw/sktime) ![PyPI - Downloads](https://img.shields.io/pypi/dm/sktime) [![Downloads](https://static.pepy.tech/personalized-badge/sktime?period=total&amp;units=international_system&amp;left_color=grey&amp;right_color=blue&amp;left_text=cumulative%20(pypi))](https://pepy.tech/project/sktime) |
| **Citation** | [![!zenodo](https://zenodo.org/badge/DOI/10.5281/zenodo.3749000.svg)](https://doi.org/10.5281/zenodo.3749000) |

## :books: Documentation

| Documentation                        |                                                                |
|--------------------------------------| -------------------------------------------------------------- |
| :star: **[Tutorials]**               | New to sktime? Here&#039;s everything you need to know!              |
| :clipboard: **[Binder Notebooks]**   | Example notebooks to play with in your browser.              |
| :woman_technologist: **[Examples]**  | How to use sktime and its features.                             |
| :scissors: **[Extension Templates]** | How to build your own estimator using sktime&#039;s API.            |
| :control_knobs: **[API Reference]**  | The detailed reference for sktime&#039;s API.                        |
| :tv: **[Video Tutorial]**            | Our video tutorial from 2021 PyData Global.      |
| :hammer_and_wrench: **[Changelog]**  | Changes and version history.                                   |
| :deciduous_tree: **[Roadmap]**       | sktime&#039;s software and community development plan.                                   |
| :pencil: **[Related Software]**      | A list of related software. |

[tutorials]: https://www.sktime.net/en/latest/tutorials.html
[binder notebooks]: https://mybinder.org/v2/gh/sktime/sktime/main?filepath=examples
[examples]: https://www.sktime.net/en/latest/examples.html
[video tutorial]: https://github.com/sktime/sktime-tutorial-pydata-global-2021
[api reference]: https://www.sktime.net/en/latest/api_reference.html
[changelog]: https://www.sktime.net/en/latest/changelog.html
[roadmap]: https://www.sktime.net/en/latest/roadmap.html
[related software]: https://www.sktime.net/en/latest/related_software.html

## :speech_balloon: Where to ask questions

Questions and feedback are extremely welcome! We strongly believe in the value of sharing help publicly, as it allows a wider audience to benefit from it.

| Type                            | Platforms                               |
| ------------------------------- | --------------------------------------- |
| :bug: **Bug Reports**              | [GitHub Issue Tracker]                  |
| :sparkles: **Feature Requests &amp; Ideas** | [GitHub Issue Tracker]                       |
| :woman_technologist: **Usage Questions**          | [GitHub Discussions] ¬∑ [Stack Overflow] |
| :speech_balloon: **General Discussion**        | [GitHub Discussions] |
| :factory: **Contribution &amp; Development** | `dev-chat` channel ¬∑ [Discord] |
| :globe_with_meridians: **Meet-ups and collaboration sessions** | [Discord] - Fridays 13 UTC, dev/meet-ups channel |

[github issue tracker]: https://github.com/sktime/sktime/issues
[github discussions]: https://github.com/sktime/sktime/discussions
[stack overflow]: https://stackoverflow.com/questions/tagged/sktime
[discord]: https://discord.com/invite/54ACzaFsn7

## :dizzy: Features
Our objective is to enhance the interoperability and usability of the time series analysis ecosystem in its entirety. sktime provides a __unified interface for distinct but related time series learning tasks__. It features [__dedicated time series algorithms__](https://www.sktime.net/en/stable/estimator_overview.html) and __tools for composite model building__,  such as pipelining, ensembling, tuning, and reduction, empowering users to apply algorithms designed for one task to another.

sktime also provides **interfaces to related libraries**, for example [scikit-learn], [statsmodels], [tsfresh], [PyOD], and [fbprophet], among others.

[statsmodels]: https://www.statsmodels.org/stable/index.html
[tsfresh]: https://tsfresh.readthedocs.io/en/latest/
[pyod]: https://pyod.readthedocs.io/en/latest/
[fbprophet]: https://facebook.github.io/prophet/

| Module | Status | Links |
|---|---|---|
| **[Forecasting]** | stable | [Tutorial](https://www.sktime.net/en/latest/examples/01_forecasting.html) ¬∑ [API Reference](https://www.sktime.net/en/latest/api_reference/forecasting.html) ¬∑ [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/forecasting.py)  |
| **[Time Series Classification]** | stable | [Tutorial](https://github.com/sktime/sktime/blob/main/examples/02_classification.ipynb) ¬∑ [API Reference](https://www.sktime.net/en/latest/api_reference/classification.html) ¬∑ [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/classification.py) |
| **[Time Series Regression]** | stable | [API Reference](https://www.sktime.net/en/latest/api_reference/regression.html) |
| **[Transformations]** | stable | [Tutorial](https://github.com/sktime/sktime/blob/main/examples/03_transformers.ipynb) ¬∑ [API Reference](https://www.sktime.net/en/latest/api_reference/transformations.html) ¬∑ [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/transformer.py)  |
| **[Detection tasks]** | maturing | [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/detection.py) |
| **[Parameter fitting]** | maturing | [API Reference](https://www.sktime.net/en/latest/api_reference/param_est.html) ¬∑ [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/transformer.py)  |
| **[Time Series Clustering]** | maturing | [API Reference](https://www.sktime.net/en/latest/api_reference/clustering.html) ¬∑  [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/clustering.py) |
| **[Time Series Distances/Kernels]** | maturing | [Tutorial](https://github.com/sktime/sktime/blob/main/examples/03_transformers.ipynb) ¬∑ [API Reference](https://www.sktime.net/en/latest/api_reference/dists_kernels.html) ¬∑ [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/dist_kern_panel.py) |
| **[Time Series Alignment]** | experimental | [API Reference](https://www.sktime.net/en/latest/api_reference/alignment.html) ¬∑ [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/alignment.py) |
| **[Time Series Splitters]** | maturing | [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/split.py) | |
| **[Distributions and simulation]** | experimental |  |

[forecasting]: https://github.com/sktime/sktime/tree/main/sktime/forecasting
[time series classification]: https://github.com/sktime/sktime/tree/main/sktime/classification
[time series regression]: https://github.com/sktime/sktime/tree/main/sktime/regression
[time series clustering]: https://github.com/sktime/sktime/tree/main/sktime/clustering
[detection tasks]: https://github.com/sktime/sktime/tree/main/sktime/detection
[time series distances/kernels]: https://github.com/sktime/sktime/tree/main/sktime/dists_kernels
[time series alignment]: https://github.com/sktime/sktime/tree/main/sktime/alignment
[transformations]: https://github.com/sktime/sktime/tree/main/sktime/transformations
[distributions and simulation]: https://github.com/sktime/sktime/tree/main/sktime/proba
[time series splitters]: https://github.com/sktime/sktime/tree/main/sktime/split
[parameter fitting]: https://github.com/sktime/sktime/tree/main/sktime/param_est


## :hourglass_flowing_sand: Install sktime
For troubleshooting and detailed installation instructions, see the [documentation](https://www.sktime.net/en/latest/installation.html).

- **Operating system**: macOS X ¬∑ Linux ¬∑ Windows 8.1 or higher
- **Python version**: Python 3.8, 3.9, 3.10, 3.11, and 3.12 (only 64-bit)
- **Package managers**: [pip] ¬∑ [conda] (via `conda-forge`)

[pip]: https://pip.pypa.io/en/stable/
[conda]: https://docs.conda.io/en/latest/

### pip
Using pip, sktime releases are available as source packages and binary wheels.
Available wheels are listed [here](https://pypi.org/simple/sktime/).

```bash
pip install sktime
```

or, with maximum dependencies,

```bash
pip install sktime[all_extras]
```

For curated sets of soft dependencies for specific learning tasks:

```bash
pip install sktime[forecasting]  # for selected forecasting dependencies
pip install sktime[forecasting,transformations]  # forecasters and transformers
```

or similar. Valid sets are:

* `forecasting`
* `transformations`
* `classification`
* `regression`
* `clustering`
* `param_est`
* `networks`
* `detection`
* `alignment`

Cave: in general, not all soft dependencies for a learning task are installed,
only a curated selection.

### conda
You can also install sktime from `conda` via the `conda-forge` channel.
The feedstock including the build recipe and configuration is maintained
in [this conda-forge repository](https://github.com/conda-forge/sktime-feedstock).

```bash
conda install -c conda-forge sktime
```

or, with maximum dependencies,

```bash
conda install -c conda-forge sktime-all-extras
```

(as `conda` does not support dependency sets,
flexible choice of soft dependencies is unavailable via `conda`)

## :zap: Quickstart

### Forecasting

``` python
from sktime.datasets import load_airline
from sktime.forecasting.base import ForecastingHorizon
from sktime.forecasting.theta import ThetaForecaster
from sktime.split import temporal_train_test_split
from sktime.performance_metrics.forecasting import mean_absolute_percentage_error

y = load_airline()
y_train, y_test = temporal_train_test_split(y)
fh = ForecastingHorizon(y_test.index, is_relative=False)
forecaster = ThetaForecaster(sp=12)  # monthly seasonal periodicity
forecaster.fit(y_train)
y_pred = forecaster.predict(fh)
mean_absolute_percentage_error(y_test, y_pred)
&gt;&gt;&gt; 0.08661467738190656
```

### Time Series Classification

```python
from sktime.classification.interval_based import TimeSeriesForestClassifier
from sktime.datasets import load_arrow_head
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X, y = load_arrow_head()
X_train, X_test, y_train, y_test = train_test_split(X, y)
classifier = TimeSeriesForestClassifier()
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
accuracy_score(y_test, y_pred)
&gt;&gt;&gt; 0.8679245283018868
```

## :wave: How to get involved

There are many ways to join the sktime community. We follow the [all-contributors](https://github.com/all-contributors/all-contributors) specification: all kinds of contributions are welcome - not just code.

| Documentation              |                                                                |
| -------------------------- | --------------------------------------------------------------        |
| :gift_heart: **[Contribute]**        | How to contribute to sktime.          |
| :school_satchel:  **[Mentoring]** | New to open source? Apply to our mentoring program! |
| :date: **[Meetings]** | Join our discussions, tutorials, workshops, and sprints! |
| :woman_mechanic:  **[Developer Guides]**      | How to further develop sktime&#039;s code base.                             |
| :construction: **[Enhancement Proposals]** | Design a new feature for sktime. |
| :medal_sports: **[Contributors]** | A list of all contributors. |
| :raising_hand: **[Roles]** | An overview of our core community roles. |
| :money_with_wings: **[Donate]** | Fund sktime maintenance and development. |
| :classical_building: **[Governance]** | How and by whom decisions are made in sktime&#039;s community.   |

[contribute]: https://www.sktime.net/en/latest/get_involved/contributing.html
[donate]: https://opencollective.com/sktime
[extension templates]: https://github.com/sktime/sktime/tree/main/extension_templates
[developer guides]: https://www.sktime.net/en/latest/developer_guide.html
[contributors]: https://github.com/sktime/sktime/blob/main/CONTRIBUTORS.md
[governance]: https://www.sktime.net/en/latest/get_involved/governance.html
[mentoring]: https://github.com/sktime/mentoring
[meetings]: https://calendar.google.com/calendar/u/0/embed?src=sktime.toolbox@gmail.com&amp;ctz=UTC
[enhancement proposals]: https://github.com/sktime/enhancement-proposals
[roles]: https://www.sktime.net/en/latest/about/team.html

## :trophy: Hall of fame

Thanks to all our community for all your wonderful contributions, PRs, issues, ideas.

&lt;a href=&quot;https://github.com/sktime/sktime/graphs/contributors&quot;&gt;
&lt;img src=&quot;https://opencollective.com/sktime/contributors.svg?width=600&amp;button=false&quot; /&gt;
&lt;/a&gt;
&lt;br&gt;

## :bulb: Project vision

* **By the community, for the community** -- developed by a friendly and collaborative community.
* The **right tool for the right task** -- helping users to diagnose their learning problem and suitable scientific model types.
* **Embedded in state-of-art ecosystems** and **provider of interoperable interfaces** -- interoperable with [scikit-learn], [statsmodels], [tsfresh], and other community favorites.
* **Rich model composition and reduction functionality** -- build tuning and feature extraction pipelines, solve forecasting tasks with [scikit-learn] regressors.
* **Clean, descriptive specification syntax** -- based on modern object-oriented design principles for data science.
* **Fair model assessment and benchmarking** -- build your models, inspect your models, check your models, and avoid pitfalls.
* **Easily extensible** -- easy extension templates to add your own algorithms compatible with sktime&#039;s API.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[vllm-project/vllm]]></title>
            <link>https://github.com/vllm-project/vllm</link>
            <guid>https://github.com/vllm-project/vllm</guid>
            <pubDate>Wed, 28 May 2025 00:04:31 GMT</pubDate>
            <description><![CDATA[A high-throughput and memory-efficient inference and serving engine for LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/vllm-project/vllm">vllm-project/vllm</a></h1>
            <p>A high-throughput and memory-efficient inference and serving engine for LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 48,263</p>
            <p>Forks: 7,631</p>
            <p>Stars today: 115 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-dark.png&quot;&gt;
    &lt;img alt=&quot;vLLM&quot; src=&quot;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-light.png&quot; width=55%&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;h3 align=&quot;center&quot;&gt;
Easy, fast, and cheap LLM serving for everyone
&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
| &lt;a href=&quot;https://docs.vllm.ai&quot;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://blog.vllm.ai/&quot;&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://arxiv.org/abs/2309.06180&quot;&gt;&lt;b&gt;Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://x.com/vllm_project&quot;&gt;&lt;b&gt;Twitter/X&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://discuss.vllm.ai&quot;&gt;&lt;b&gt;User Forum&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://slack.vllm.ai&quot;&gt;&lt;b&gt;Developer Slack&lt;/b&gt;&lt;/a&gt; |
&lt;/p&gt;

---

*Latest News* üî•
- [2025/05] We hosted [NYC vLLM Meetup](https://lu.ma/c1rqyf1f)! Please find the meetup slides [here](https://docs.google.com/presentation/d/1_q_aW_ioMJWUImf1s1YM-ZhjXz8cUeL0IJvaquOYBeA/edit?usp=sharing).
- [2025/05] vLLM is now a hosted project under PyTorch Foundation! Please find the announcement [here](https://pytorch.org/blog/pytorch-foundation-welcomes-vllm/).
- [2025/04] We hosted [Asia Developer Day](https://www.sginnovate.com/event/limited-availability-morning-evening-slots-remaining-inaugural-vllm-asia-developer-day)! Please find the meetup slides from the vLLM team [here](https://docs.google.com/presentation/d/19cp6Qu8u48ihB91A064XfaXruNYiBOUKrBxAmDOllOo/edit?usp=sharing).
- [2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post [here](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html).

&lt;details&gt;
&lt;summary&gt;Previous News&lt;/summary&gt;

- [2025/03] We hosted [vLLM x Ollama Inference Night](https://lu.ma/vllm-ollama)! Please find the meetup slides from the vLLM team [here](https://docs.google.com/presentation/d/16T2PDD1YwRnZ4Tu8Q5r6n53c5Lr5c73UV9Vd2_eBo4U/edit?usp=sharing).
- [2025/03] We hosted [the first vLLM China Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg)! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1REHvfQMKGnvz6p3Fd23HhSO4c8j5WPGZV0bKYLwnHyQ/edit?usp=sharing).
- [2025/03] We hosted [the East Coast vLLM Meetup](https://lu.ma/7mu4k4xx)! Please find the meetup slides [here](https://docs.google.com/presentation/d/1NHiv8EUFF1NLd3fEYODm56nDmL26lEeXCaDgyDlTsRs/edit#slide=id.g31441846c39_0_0).
- [2025/02] We hosted [the ninth vLLM meetup](https://lu.ma/h7g3kuj9) with Meta! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1jzC_PZVXrVNSFVCW-V4cFXb6pn7zZ2CyP_Flwo05aqg/edit?usp=sharing) and AMD [here](https://drive.google.com/file/d/1Zk5qEJIkTmlQ2eQcXQZlljAx3m9s7nwn/view?usp=sharing). The slides from Meta will not be posted.
- [2025/01] We hosted [the eighth vLLM meetup](https://lu.ma/zep56hui) with Google Cloud! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing), and Google Cloud team [here](https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing).
- [2024/12] vLLM joins [pytorch ecosystem](https://pytorch.org/blog/vllm-joins-pytorch)! Easy, Fast, and Cheap LLM Serving for Everyone!
- [2024/11] We hosted [the seventh vLLM meetup](https://lu.ma/h0qvrajz) with Snowflake! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing), and Snowflake team [here](https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing).
- [2024/10] We have just created a developer slack ([slack.vllm.ai](https://slack.vllm.ai)) focusing on coordinating contributions and discussing features. Please feel free to join us there!
- [2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team [here](https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing). Learn more from the [talks](https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR) from other vLLM contributors and users!
- [2024/09] We hosted [the sixth vLLM meetup](https://lu.ma/87q3nvnh) with NVIDIA! Please find the meetup slides [here](https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing).
- [2024/07] We hosted [the fifth vLLM meetup](https://lu.ma/lp0gyjqr) with AWS! Please find the meetup slides [here](https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing).
- [2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post [here](https://blog.vllm.ai/2024/07/23/llama31.html).
- [2024/06] We hosted [the fourth vLLM meetup](https://lu.ma/agivllm) with Cloudflare and BentoML! Please find the meetup slides [here](https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing).
- [2024/04] We hosted [the third vLLM meetup](https://robloxandvllmmeetup2024.splashthat.com/) with Roblox! Please find the meetup slides [here](https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing).
- [2024/01] We hosted [the second vLLM meetup](https://lu.ma/ygxbpzhl) with IBM! Please find the meetup slides [here](https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing).
- [2023/10] We hosted [the first vLLM meetup](https://lu.ma/first-vllm-meetup) with a16z! Please find the meetup slides [here](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing).
- [2023/08] We would like to express our sincere gratitude to [Andreessen Horowitz](https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/) (a16z) for providing a generous grant to support the open-source development and research of vLLM.
- [2023/06] We officially released vLLM! FastChat-vLLM integration has powered [LMSYS Vicuna and Chatbot Arena](https://chat.lmsys.org) since mid-April. Check out our [blog post](https://vllm.ai).

&lt;/details&gt;

---
## About

vLLM is a fast and easy-to-use library for LLM inference and serving.

Originally developed in the [Sky Computing Lab](https://sky.cs.berkeley.edu) at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.

vLLM is fast with:

- State-of-the-art serving throughput
- Efficient management of attention key and value memory with [**PagedAttention**](https://blog.vllm.ai/2023/06/20/vllm.html)
- Continuous batching of incoming requests
- Fast model execution with CUDA/HIP graph
- Quantizations: [GPTQ](https://arxiv.org/abs/2210.17323), [AWQ](https://arxiv.org/abs/2306.00978), [AutoRound](https://arxiv.org/abs/2309.05516),INT4, INT8, and FP8.
- Optimized CUDA kernels, including integration with FlashAttention and FlashInfer.
- Speculative decoding
- Chunked prefill

**Performance benchmark**: We include a performance benchmark at the end of [our blog post](https://blog.vllm.ai/2024/09/05/perf-update.html). It compares the performance of vLLM against other LLM serving engines ([TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM), [SGLang](https://github.com/sgl-project/sglang) and [LMDeploy](https://github.com/InternLM/lmdeploy)). The implementation is under [nightly-benchmarks folder](.buildkite/nightly-benchmarks/) and you can [reproduce](https://github.com/vllm-project/vllm/issues/8176) this benchmark using our one-click runnable script.

vLLM is flexible and easy to use with:

- Seamless integration with popular Hugging Face models
- High-throughput serving with various decoding algorithms, including *parallel sampling*, *beam search*, and more
- Tensor parallelism and pipeline parallelism support for distributed inference
- Streaming outputs
- OpenAI-compatible API server
- Support NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, TPU, and AWS Neuron.
- Prefix caching support
- Multi-LoRA support

vLLM seamlessly supports most popular open-source models on HuggingFace, including:
- Transformer-like LLMs (e.g., Llama)
- Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)
- Embedding Models (e.g. E5-Mistral)
- Multi-modal LLMs (e.g., LLaVA)

Find the full list of supported models [here](https://docs.vllm.ai/en/latest/models/supported_models.html).

## Getting Started

Install vLLM with `pip` or [from source](https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source):

```bash
pip install vllm
```

Visit our [documentation](https://docs.vllm.ai/en/latest/) to learn more.
- [Installation](https://docs.vllm.ai/en/latest/getting_started/installation.html)
- [Quickstart](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)
- [List of Supported Models](https://docs.vllm.ai/en/latest/models/supported_models.html)

## Contributing

We welcome and value any contributions and collaborations.
Please check out [Contributing to vLLM](https://docs.vllm.ai/en/latest/contributing/index.html) for how to get involved.

## Sponsors

vLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!

&lt;!-- Note: Please sort them in alphabetical order. --&gt;
&lt;!-- Note: Please keep these consistent with docs/community/sponsors.md --&gt;
Cash Donations:
- a16z
- Dropbox
- Sequoia Capital
- Skywork AI
- ZhenFund

Compute Resources:
- AMD
- Anyscale
- AWS
- Crusoe Cloud
- Databricks
- DeepInfra
- Google Cloud
- Intel
- Lambda Lab
- Nebius
- Novita AI
- NVIDIA
- Replicate
- Roblox
- RunPod
- Trainy
- UC Berkeley
- UC San Diego

Slack Sponsor: Anyscale

We also have an official fundraising venue through [OpenCollective](https://opencollective.com/vllm). We plan to use the fund to support the development, maintenance, and adoption of vLLM.

## Citation

If you use vLLM for your research, please cite our [paper](https://arxiv.org/abs/2309.06180):

```bibtex
@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}
```

## Contact Us

- For technical questions and feature requests, please use GitHub [Issues](https://github.com/vllm-project/vllm/issues) or [Discussions](https://github.com/vllm-project/vllm/discussions)
- For discussing with fellow users, please use the [vLLM Forum](https://discuss.vllm.ai)
- coordinating contributions and development, please use [Slack](https://slack.vllm.ai)
- For security disclosures, please use GitHub&#039;s [Security Advisories](https://github.com/vllm-project/vllm/security/advisories) feature
- For collaborations and partnerships, please contact us at [vllm-questions@lists.berkeley.edu](mailto:vllm-questions@lists.berkeley.edu)

## Media Kit

- If you wish to use vLLM&#039;s logo, please refer to [our media kit repo](https://github.com/vllm-project/media-kit).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[open-mmlab/mmsegmentation]]></title>
            <link>https://github.com/open-mmlab/mmsegmentation</link>
            <guid>https://github.com/open-mmlab/mmsegmentation</guid>
            <pubDate>Wed, 28 May 2025 00:04:30 GMT</pubDate>
            <description><![CDATA[OpenMMLab Semantic Segmentation Toolbox and Benchmark.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/open-mmlab/mmsegmentation">open-mmlab/mmsegmentation</a></h1>
            <p>OpenMMLab Semantic Segmentation Toolbox and Benchmark.</p>
            <p>Language: Python</p>
            <p>Stars: 8,918</p>
            <p>Forks: 2,714</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;resources/mmseg-logo.png&quot; width=&quot;600&quot;/&gt;
  &lt;div&gt;&amp;nbsp;&lt;/div&gt;
  &lt;div align=&quot;center&quot;&gt;
    &lt;b&gt;&lt;font size=&quot;5&quot;&gt;OpenMMLab website&lt;/font&gt;&lt;/b&gt;
    &lt;sup&gt;
      &lt;a href=&quot;https://openmmlab.com&quot;&gt;
        &lt;i&gt;&lt;font size=&quot;4&quot;&gt;HOT&lt;/font&gt;&lt;/i&gt;
      &lt;/a&gt;
    &lt;/sup&gt;
    &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
    &lt;b&gt;&lt;font size=&quot;5&quot;&gt;OpenMMLab platform&lt;/font&gt;&lt;/b&gt;
    &lt;sup&gt;
      &lt;a href=&quot;https://platform.openmmlab.com&quot;&gt;
        &lt;i&gt;&lt;font size=&quot;4&quot;&gt;TRY IT OUT&lt;/font&gt;&lt;/i&gt;
      &lt;/a&gt;
    &lt;/sup&gt;
  &lt;/div&gt;
  &lt;div&gt;&amp;nbsp;&lt;/div&gt;

[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/mmsegmentation)](https://pypi.org/project/mmsegmentation/)
[![PyPI](https://img.shields.io/pypi/v/mmsegmentation)](https://pypi.org/project/mmsegmentation)
[![docs](https://img.shields.io/badge/docs-latest-blue)](https://mmsegmentation.readthedocs.io/en/latest/)
[![badge](https://github.com/open-mmlab/mmsegmentation/workflows/build/badge.svg)](https://github.com/open-mmlab/mmsegmentation/actions)
[![codecov](https://codecov.io/gh/open-mmlab/mmsegmentation/branch/master/graph/badge.svg)](https://codecov.io/gh/open-mmlab/mmsegmentation)
[![license](https://img.shields.io/github/license/open-mmlab/mmsegmentation.svg)](https://github.com/open-mmlab/mmsegmentation/blob/main/LICENSE)
[![issue resolution](https://isitmaintained.com/badge/resolution/open-mmlab/mmsegmentation.svg)](https://github.com/open-mmlab/mmsegmentation/issues)
[![open issues](https://isitmaintained.com/badge/open/open-mmlab/mmsegmentation.svg)](https://github.com/open-mmlab/mmsegmentation/issues)
[![Open in OpenXLab](https://cdn-static.openxlab.org.cn/app-center/openxlab_demo.svg)](https://openxlab.org.cn/apps?search=mmseg)

Documentation: &lt;https://mmsegmentation.readthedocs.io/en/latest/&gt;

English | [ÁÆÄ‰Ωì‰∏≠Êñá](README_zh-CN.md)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://openmmlab.medium.com/&quot; style=&quot;text-decoration:none;&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/25839884/219255827-67c1a27f-f8c5-46a9-811d-5e57448c61d1.png&quot; width=&quot;3%&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;
  &lt;img src=&quot;https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png&quot; width=&quot;3%&quot; alt=&quot;&quot; /&gt;
  &lt;a href=&quot;https://discord.gg/raweFPmdzG&quot; style=&quot;text-decoration:none;&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/25839884/218347213-c080267f-cbb6-443e-8532-8e1ed9a58ea9.png&quot; width=&quot;3%&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;
  &lt;img src=&quot;https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png&quot; width=&quot;3%&quot; alt=&quot;&quot; /&gt;
  &lt;a href=&quot;https://twitter.com/OpenMMLab&quot; style=&quot;text-decoration:none;&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/25839884/218346637-d30c8a0f-3eba-4699-8131-512fb06d46db.png&quot; width=&quot;3%&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;
  &lt;img src=&quot;https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png&quot; width=&quot;3%&quot; alt=&quot;&quot; /&gt;
  &lt;a href=&quot;https://www.youtube.com/openmmlab&quot; style=&quot;text-decoration:none;&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/25839884/218346691-ceb2116a-465a-40af-8424-9f30d2348ca9.png&quot; width=&quot;3%&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;
  &lt;img src=&quot;https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png&quot; width=&quot;3%&quot; alt=&quot;&quot; /&gt;
  &lt;a href=&quot;https://space.bilibili.com/1293512903&quot; style=&quot;text-decoration:none;&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/25839884/219026751-d7d14cce-a7c9-4e82-9942-8375fca65b99.png&quot; width=&quot;3%&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;
  &lt;img src=&quot;https://user-images.githubusercontent.com/25839884/218346358-56cc8e2f-a2b8-487f-9088-32480cceabcf.png&quot; width=&quot;3%&quot; alt=&quot;&quot; /&gt;
  &lt;a href=&quot;https://www.zhihu.com/people/openmmlab&quot; style=&quot;text-decoration:none;&quot;&gt;
    &lt;img src=&quot;https://user-images.githubusercontent.com/25839884/219026120-ba71e48b-6e94-4bd4-b4e9-b7d175b5e362.png&quot; width=&quot;3%&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;
&lt;/div&gt;

## Introduction

MMSegmentation is an open source semantic segmentation toolbox based on PyTorch.
It is a part of the OpenMMLab project.

The [main](https://github.com/open-mmlab/mmsegmentation/tree/main) branch works with PyTorch 1.6+.

### üéâ Introducing MMSegmentation v1.0.0 üéâ

We are thrilled to announce the official release of MMSegmentation&#039;s latest version! For this new release, the [main](https://github.com/open-mmlab/mmsegmentation/tree/main) branch serves as the primary branch, while the development branch is [dev-1.x](https://github.com/open-mmlab/mmsegmentation/tree/dev-1.x). The stable branch for the previous release remains as the [0.x](https://github.com/open-mmlab/mmsegmentation/tree/0.x) branch. Please note that the [master](https://github.com/open-mmlab/mmsegmentation/tree/master) branch will only be maintained for a limited time before being removed. We encourage you to be mindful of branch selection and updates during use. Thank you for your unwavering support and enthusiasm, and let&#039;s work together to make MMSegmentation even more robust and powerful! üí™

MMSegmentation v1.x brings remarkable improvements over the 0.x release, offering a more flexible and feature-packed experience. To utilize the new features in v1.x, we kindly invite you to consult our detailed [üìö migration guide](https://mmsegmentation.readthedocs.io/en/latest/migration/interface.html), which will help you seamlessly transition your projects. Your support is invaluable, and we eagerly await your feedback!

![demo image](resources/seg_demo.gif)

### Major features

- **Unified Benchmark**

  We provide a unified benchmark toolbox for various semantic segmentation methods.

- **Modular Design**

  We decompose the semantic segmentation framework into different components and one can easily construct a customized semantic segmentation framework by combining different modules.

- **Support of multiple methods out of box**

  The toolbox directly supports popular and contemporary semantic segmentation frameworks, *e.g.* PSPNet, DeepLabV3, PSANet, DeepLabV3+, etc.

- **High efficiency**

  The training speed is faster than or comparable to other codebases.

## What&#039;s New

v1.2.0 was released on 10/12/2023, from 1.1.0 to 1.2.0, we have added or updated the following features:

### Highlights

- Support for the open-vocabulary semantic segmentation algorithm [SAN](configs/san/README.md)

- Support monocular depth estimation task, please refer to [VPD](configs/vpd/README.md) and [Adabins](projects/Adabins/README.md) for more details.

  ![depth estimation](https://github.com/open-mmlab/mmsegmentation/assets/15952744/07afd0e9-8ace-4a00-aa1e-5bf0ca92dcbc)

- Add new projects: open-vocabulary semantic segmentation algorithm [CAT-Seg](projects/CAT-Seg/README.md), real-time semantic segmentation algofithm [PP-MobileSeg](projects/pp_mobileseg/README.md)

## Installation

Please refer to [get_started.md](docs/en/get_started.md#installation) for installation and [dataset_prepare.md](docs/en/user_guides/2_dataset_prepare.md#prepare-datasets) for dataset preparation.

## Get Started

Please see [Overview](docs/en/overview.md) for the general introduction of MMSegmentation.

Please see [user guides](https://mmsegmentation.readthedocs.io/en/latest/user_guides/index.html#) for the basic usage of MMSegmentation.
There are also [advanced tutorials](https://mmsegmentation.readthedocs.io/en/latest/advanced_guides/index.html) for in-depth understanding of mmseg design and implementation .

A Colab tutorial is also provided. You may preview the notebook [here](demo/MMSegmentation_Tutorial.ipynb) or directly [run](https://colab.research.google.com/github/open-mmlab/mmsegmentation/blob/main/demo/MMSegmentation_Tutorial.ipynb) on Colab.

To migrate from MMSegmentation 0.x, please refer to [migration](docs/en/migration).

## Tutorial

&lt;div align=&quot;center&quot;&gt;
  &lt;b&gt;MMSegmentation Tutorials&lt;/b&gt;
&lt;/div&gt;
&lt;table align=&quot;center&quot;&gt;
  &lt;tbody&gt;
    &lt;tr align=&quot;center&quot; valign=&quot;center&quot;&gt;
      &lt;td&gt;
        &lt;b&gt;Get Started&lt;/b&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;b&gt;MMSeg Basic Tutorial&lt;/b&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;b&gt;MMSeg Detail Tutorial&lt;/b&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;b&gt;MMSeg Development Tutorial&lt;/b&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr valign=&quot;top&quot;&gt;
      &lt;td&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/overview.md&quot;&gt;MMSeg overview&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/get_started.md&quot;&gt;MMSeg Installation&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/notes/faq.md&quot;&gt;FAQ&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/user_guides/1_config.md&quot;&gt;Tutorial 1: Learn about Configs&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/user_guides/2_dataset_prepare.md&quot;&gt;Tutorial 2: Prepare datasets&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/user_guides/3_inference.md&quot;&gt;Tutorial 3: Inference with existing models&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/user_guides/4_train_test.md&quot;&gt;Tutorial 4: Train and test with existing models&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/user_guides/5_deployment.md&quot;&gt;Tutorial 5: Model deployment&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/zh_cn/user_guides/deploy_jetson.md&quot;&gt;Deploy mmsegmentation on Jetson platform&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/user_guides/useful_tools.md&quot;&gt;Useful Tools&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/user_guides/visualization_feature_map.md&quot;&gt;Feature Map Visualization&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/user_guides/visualization.md&quot;&gt;Visualization&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/advanced_guides/datasets.md&quot;&gt;MMSeg Dataset&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/advanced_guides/models.md&quot;&gt;MMSeg Models&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/advanced_guides/structures.md&quot;&gt;MMSeg Dataset Structures&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/advanced_guides/transforms.md&quot;&gt;MMSeg Data Transforms&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/advanced_guides/data_flow.md&quot;&gt;MMSeg Dataflow&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/advanced_guides/engine.md&quot;&gt;MMSeg Training Engine&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/advanced_guides/evaluation.md&quot;&gt;MMSeg Evaluation&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/advanced_guides/add_datasets.md&quot;&gt;Add New Datasets&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/advanced_guides/add_metrics.md&quot;&gt;Add New Metrics&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/advanced_guides/add_models.md&quot;&gt;Add New Modules&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/advanced_guides/add_transforms.md&quot;&gt;Add New Data Transforms&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/advanced_guides/customize_runtime.md&quot;&gt;Customize Runtime Settings&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/advanced_guides/training_tricks.md&quot;&gt;Training Tricks&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;.github/CONTRIBUTING.md&quot;&gt;Contribute code to MMSeg&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/zh_cn/advanced_guides/contribute_dataset.md&quot;&gt;Contribute a standard dataset in projects&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/device/npu.md&quot;&gt;NPU (HUAWEI Ascend)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/migration/interface.md&quot;&gt;0.x ‚Üí 1.x migration&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;docs/en/migration/package.md&quot;&gt;0.x ‚Üí 1.x package&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

## Benchmark and model zoo

Results and models are available in the [model zoo](docs/en/model_zoo.md).

&lt;div align=&quot;center&quot;&gt;
  &lt;b&gt;Overview&lt;/b&gt;
&lt;/div&gt;
&lt;table align=&quot;center&quot;&gt;
  &lt;tbody&gt;
    &lt;tr align=&quot;center&quot; valign=&quot;center&quot;&gt;
      &lt;td&gt;
        &lt;b&gt;Supported backbones&lt;/b&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;b&gt;Supported methods&lt;/b&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;b&gt;Supported Head&lt;/b&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;b&gt;Supported datasets&lt;/b&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;b&gt;Other&lt;/b&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr valign=&quot;top&quot;&gt;
      &lt;td&gt;
        &lt;ul&gt;
        &lt;li&gt;&lt;a href=&quot;mmseg/models/backbones/resnet.py&quot;&gt;ResNet(CVPR&#039;2016)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;mmseg/models/backbones/resnext.py&quot;&gt;ResNeXt (CVPR&#039;2017)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;configs/hrnet&quot;&gt;HRNet (CVPR&#039;2019)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;configs/resnest&quot;&gt;ResNeSt (ArXiv&#039;2020)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;configs/mobilenet_v2&quot;&gt;MobileNetV2 (CVPR&#039;2018)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;configs/mobilenet_v3&quot;&gt;MobileNetV3 (ICCV&#039;2019)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;configs/vit&quot;&gt;Vision Transformer (ICLR&#039;2021)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;configs/swin&quot;&gt;Swin Transformer (ICCV&#039;2021)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;configs/twins&quot;&gt;Twins (NeurIPS&#039;2021)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;configs/beit&quot;&gt;BEiT (ICLR&#039;2022)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;configs/convnext&quot;&gt;ConvNeXt (CVPR&#039;2022)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;configs/mae&quot;&gt;MAE (CVPR&#039;2022)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;configs/poolformer&quot;&gt;PoolFormer (CVPR&#039;2022)&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;configs/segnext&quot;&gt;SegNeXt (NeurIPS&#039;2022)&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;configs/san/&quot;&gt;SAN (CVPR&#039;2023)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/vpd&quot;&gt;VPD (ICCV&#039;2023)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/ddrnet&quot;&gt;DDRNet (T-ITS&#039;2022)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/pidnet&quot;&gt;PIDNet (ArXiv&#039;2022)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/mask2former&quot;&gt;Mask2Former (CVPR&#039;2022)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/maskformer&quot;&gt;MaskFormer (NeurIPS&#039;2021)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/knet&quot;&gt;K-Net (NeurIPS&#039;2021)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/segformer&quot;&gt;SegFormer (NeurIPS&#039;2021)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/segmenter&quot;&gt;Segmenter (ICCV&#039;2021)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/dpt&quot;&gt;DPT (ArXiv&#039;2021)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/setr&quot;&gt;SETR (CVPR&#039;2021)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/stdc&quot;&gt;STDC (CVPR&#039;2021)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/bisenetv2&quot;&gt;BiSeNetV2 (IJCV&#039;2021)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/cgnet&quot;&gt;CGNet (TIP&#039;2020)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/point_rend&quot;&gt;PointRend (CVPR&#039;2020)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/dnlnet&quot;&gt;DNLNet (ECCV&#039;2020)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/ocrnet&quot;&gt;OCRNet (ECCV&#039;2020)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/isanet&quot;&gt;ISANet (ArXiv&#039;2019/IJCV&#039;2021)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/fastscnn&quot;&gt;Fast-SCNN (ArXiv&#039;2019)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/fastfcn&quot;&gt;FastFCN (ArXiv&#039;2019)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/gcnet&quot;&gt;GCNet (ICCVW&#039;2019/TPAMI&#039;2020)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/ann&quot;&gt;ANN (ICCV&#039;2019)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/emanet&quot;&gt;EMANet (ICCV&#039;2019)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/ccnet&quot;&gt;CCNet (ICCV&#039;2019)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/dmnet&quot;&gt;DMNet (ICCV&#039;2019)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/sem_fpn&quot;&gt;Semantic FPN (CVPR&#039;2019)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/danet&quot;&gt;DANet (CVPR&#039;2019)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/apcnet&quot;&gt;APCNet (CVPR&#039;2019)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/nonlocal_net&quot;&gt;NonLocal Net (CVPR&#039;2018)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/encnet&quot;&gt;EncNet (CVPR&#039;2018)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/deeplabv3plus&quot;&gt;DeepLabV3+ (CVPR&#039;2018)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/upernet&quot;&gt;UPerNet (ECCV&#039;2018)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/icnet&quot;&gt;ICNet (ECCV&#039;2018)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/psanet&quot;&gt;PSANet (ECCV&#039;2018)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/bisenetv1&quot;&gt;BiSeNetV1 (ECCV&#039;2018)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/deeplabv3&quot;&gt;DeepLabV3 (ArXiv&#039;2017)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/pspnet&quot;&gt;PSPNet (CVPR&#039;2017)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/erfnet&quot;&gt;ERFNet (T-ITS&#039;2017)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/unet&quot;&gt;UNet (MICCAI&#039;2016/Nat. Methods&#039;2019)&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;configs/fcn&quot;&gt;FCN (CVPR&#039;2015/TPAMI&#039;2017)&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/ann_head.py&quot;&gt;ANN_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/apc_head.py&quot;&gt;APC_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/aspp_head.py&quot;&gt;ASPP_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/cc_head.py&quot;&gt;CC_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/da_head.py&quot;&gt;DA_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/ddr_head.py&quot;&gt;DDR_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/dm_head.py&quot;&gt;DM_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/dnl_head.py&quot;&gt;DNL_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/dpt_head.py&quot;&gt;DPT_HEAD&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/ema_head.py&quot;&gt;EMA_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/enc_head.py&quot;&gt;ENC_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/fcn_head.py&quot;&gt;FCN_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/fpn_head.py&quot;&gt;FPN_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/gc_head.py&quot;&gt;GC_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/ham_head.py&quot;&gt;LightHam_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/isa_head.py&quot;&gt;ISA_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/knet_head.py&quot;&gt;Knet_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/lraspp_head.py&quot;&gt;LRASPP_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/mask2former_head.py&quot;&gt;mask2former_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/maskformer_head.py&quot;&gt;maskformer_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/nl_head.py&quot;&gt;NL_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/ocr_head.py&quot;&gt;OCR_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/pid_head.py&quot;&gt;PID_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/point_head.py&quot;&gt;point_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/psa_head.py&quot;&gt;PSA_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/psp_head.py&quot;&gt;PSP_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/san_head.py&quot;&gt;SAN_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/segformer_head.py&quot;&gt;segformer_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/segmenter_mask_head.py&quot;&gt;segmenter_mask_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/sep_aspp_head.py&quot;&gt;SepASPP_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/sep_fcn_head.py&quot;&gt;SepFCN_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/setr_mla_head.py&quot;&gt;SETRMLAHead_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/setr_up_head.py&quot;&gt;SETRUP_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/stdc_head.py&quot;&gt;STDC_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/uper_head.py&quot;&gt;Uper_Head&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;mmseg/models/decode_heads/vpd_depth_head.py&quot;&gt;VPDDepth_Head&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/td&gt;
      &lt;td&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://github.com/open-mmlab/mmsegmentation/blob/main/docs/en/user_guides/2_dataset_prepare.md#cityscapes&quot;&gt;Cityscapes&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://github.com/open-mmlab/mmsegmentation/blob/main/docs/en/user_guides/2_dataset_prepare.md#pascal-voc&quot;&gt;PASCAL VOC&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://github.com/open-mmlab/mmsegmentation/blob/main/docs/en/user_guides/2_dataset_prepare.md#ade20k&quot;&gt;ADE20K&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://github.com/open-mmlab/mmsegmentation/blob/main/docs/en/user_guides/2_dataset_prepare.md#pascal-context&quot;&gt;Pascal Context&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://github.com/open-mmlab/mmsegmentation/blob/main/docs/en/user_guides/2_dataset_prepare.md#coco-stuff-10k&quot;&gt;COCO-Stuff 10k&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://github.com/open-mmlab/mmsegmentation/blob/main/docs/en/user_guides/2_dataset_prepare.md#coco-stuff-164k&quot;&gt;COCO-Stuff 164k&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://github.com/open-mmlab/mmsegmentation/blob/main/docs/en/user_guides/2_dataset_prepare.md#chase-db1&quot;&gt;CHASE_DB1&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://github.com/open-mmlab/mmsegmentation/blob/main/docs/en/user_guides/2_dataset_prepare.md#drive&quot;&gt;DRIVE&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://github.com/open-mmlab/mmsegmentation/blob/main/docs/en/user_guides/2_dataset_prepare.md#hrf&quot;&gt;HRF&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://github.com/open-mmlab/mmsegmentation/blob/main/docs/en/user_guides/2_dataset_prepare.md#stare&quot;&gt;STARE&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://github.com/open-mmlab/mmsegmentation/blob/main/docs/en/user_guides/2_dataset_prepare.

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[langflow-ai/langflow]]></title>
            <link>https://github.com/langflow-ai/langflow</link>
            <guid>https://github.com/langflow-ai/langflow</guid>
            <pubDate>Wed, 28 May 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[Langflow is a powerful tool for building and deploying AI-powered agents and workflows.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/langflow-ai/langflow">langflow-ai/langflow</a></h1>
            <p>Langflow is a powerful tool for building and deploying AI-powered agents and workflows.</p>
            <p>Language: Python</p>
            <p>Stars: 64,718</p>
            <p>Forks: 6,590</p>
            <p>Stars today: 791 stars today</p>
            <h2>README</h2><pre>&lt;!-- markdownlint-disable MD030 --&gt;

![Langflow logo](./docs/static/img/langflow-logo-color-black-solid.svg)


[![Release Notes](https://img.shields.io/github/release/langflow-ai/langflow?style=flat-square)](https://github.com/langflow-ai/langflow/releases)
[![PyPI - License](https://img.shields.io/badge/license-MIT-orange)](https://opensource.org/licenses/MIT)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/langflow?style=flat-square)](https://pypistats.org/packages/langflow)
[![GitHub star chart](https://img.shields.io/github/stars/langflow-ai/langflow?style=flat-square)](https://star-history.com/#langflow-ai/langflow)
[![Open Issues](https://img.shields.io/github/issues-raw/langflow-ai/langflow?style=flat-square)](https://github.com/langflow-ai/langflow/issues)
[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langflow-ai.svg?style=social&amp;label=Follow%20%40Langflow)](https://twitter.com/langflow_ai)
[![YouTube Channel](https://img.shields.io/youtube/channel/subscribers/UCn2bInQrjdDYKEEmbpwblLQ?label=Subscribe)](https://www.youtube.com/@Langflow)
[![Discord Server](https://img.shields.io/discord/1116803230643527710?logo=discord&amp;style=social&amp;label=Join)](https://discord.gg/EqksyE2EX9)


[Langflow](https://langflow.org) is a powerful tool for building and deploying AI-powered agents and workflows. It provides developers with both a visual authoring experience and a built-in API server that turns every agent into an API endpoint that can be integrated into applications built on any framework or stack. Langflow comes with batteries included and supports all major LLMs, vector databases and a growing library of AI tools.

## ‚ú® Highlight features

1. **Visual Builder** to get started quickly and iterate. 
1. **Access to Code** so developers can tweak any component using Python.
1. **Playground** to immediately test and iterate on their flows with step-by-step control.
1. **Multi-agent** orchestration and conversation management and retrieval.
1. **Deploy as an API** or export as JSON for Python apps.
1. **Observability** with LangSmith, LangFuse and other integrations.
1. **Enterprise-ready** security and scalability.

## ‚ö°Ô∏è Quickstart

Langflow works with Python 3.10 to 3.13.

Install with uv **(recommended)** 

```shell
uv pip install langflow
```

Install with pip

```shell
pip install langflow
```

## üì¶ Deployment

### Self-managed

Langflow is completely open source and you can deploy it to all major deployment clouds. Follow this [guide](https://docs.langflow.org/deployment-docker) to learn how to use Docker to deploy Langflow.

### Fully-managed by DataStax

DataStax Langflow is a full-managed environment with zero setup. Developers can [sign up for a free account](https://astra.datastax.com/signup?type=langflow) to get started.

## ‚≠ê Stay up-to-date

Star Langflow on GitHub to be instantly notified of new releases.

![Star Langflow](https://github.com/user-attachments/assets/03168b17-a11d-4b2a-b0f7-c1cce69e5a2c)

## üëã Contribute

We welcome contributions from developers of all levels. If you&#039;d like to contribute, please check our [contributing guidelines](./CONTRIBUTING.md) and help make Langflow more accessible.

---

[![Star History Chart](https://api.star-history.com/svg?repos=langflow-ai/langflow&amp;type=Timeline)](https://star-history.com/#langflow-ai/langflow&amp;Date)

## ‚ù§Ô∏è Contributors

[![langflow contributors](https://contrib.rocks/image?repo=langflow-ai/langflow)](https://github.com/langflow-ai/langflow/graphs/contributors)

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Byaidu/PDFMathTranslate]]></title>
            <link>https://github.com/Byaidu/PDFMathTranslate</link>
            <guid>https://github.com/Byaidu/PDFMathTranslate</guid>
            <pubDate>Wed, 28 May 2025 00:04:28 GMT</pubDate>
            <description><![CDATA[PDF scientific paper translation with preserved formats - Âü∫‰∫é AI ÂÆåÊï¥‰øùÁïôÊéíÁâàÁöÑ PDF ÊñáÊ°£ÂÖ®ÊñáÂèåËØ≠ÁøªËØëÔºåÊîØÊåÅ Google/DeepL/Ollama/OpenAI Á≠âÊúçÂä°ÔºåÊèê‰æõ CLI/GUI/MCP/Docker/Zotero]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Byaidu/PDFMathTranslate">Byaidu/PDFMathTranslate</a></h1>
            <p>PDF scientific paper translation with preserved formats - Âü∫‰∫é AI ÂÆåÊï¥‰øùÁïôÊéíÁâàÁöÑ PDF ÊñáÊ°£ÂÖ®ÊñáÂèåËØ≠ÁøªËØëÔºåÊîØÊåÅ Google/DeepL/Ollama/OpenAI Á≠âÊúçÂä°ÔºåÊèê‰æõ CLI/GUI/MCP/Docker/Zotero</p>
            <p>Language: Python</p>
            <p>Stars: 24,110</p>
            <p>Forks: 2,067</p>
            <p>Stars today: 54 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

English | [ÁÆÄ‰Ωì‰∏≠Êñá](docs/README_zh-CN.md) | [ÁπÅÈ´î‰∏≠Êñá](docs/README_zh-TW.md) | [Êó•Êú¨Ë™û](docs/README_ja-JP.md) | [ÌïúÍµ≠Ïñ¥](docs/README_ko-KR.md)

&lt;img src=&quot;./docs/images/banner.png&quot; width=&quot;320px&quot;  alt=&quot;PDF2ZH&quot;/&gt;

&lt;h2 id=&quot;title&quot;&gt;PDFMathTranslate&lt;/h2&gt;

&lt;p&gt;
  &lt;!-- PyPI --&gt;
  &lt;a href=&quot;https://pypi.org/project/pdf2zh/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/pdf2zh&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pepy.tech/projects/pdf2zh&quot;&gt;
    &lt;img src=&quot;https://static.pepy.tech/badge/pdf2zh&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://hub.docker.com/repository/docker/byaidu/pdf2zh&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/docker/pulls/byaidu/pdf2zh&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://gitcode.com/Byaidu/PDFMathTranslate/overview&quot;&gt;
    &lt;img src=&quot;https://gitcode.com/Byaidu/PDFMathTranslate/star/badge.svg&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://huggingface.co/spaces/reycn/PDFMathTranslate-Docker&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97-Online%20Demo-FF9E0D&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.modelscope.cn/studios/AI-ModelScope/PDFMathTranslate&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/ModelScope-Demo-blue&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/Byaidu/PDFMathTranslate/pulls&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/contributions-welcome-green&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://t.me/+Z9_SgnxmsmA5NzBl&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Telegram-2CA5E0?style=flat-squeare&amp;logo=telegram&amp;logoColor=white&quot;&gt;&lt;/a&gt;
  &lt;!-- License --&gt;
  &lt;a href=&quot;./LICENSE&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/license/Byaidu/PDFMathTranslate&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;a href=&quot;https://trendshift.io/repositories/12424&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12424&quot; alt=&quot;Byaidu%2FPDFMathTranslate | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;

PDF scientific paper translation and bilingual comparison.

- üìä Preserve formulas, charts, table of contents, and annotations _([preview](#preview))_.
- üåê Support [multiple languages](#language), and diverse [translation services](#services).
- ü§ñ Provides [commandline tool](#usage), [interactive user interface](#gui), and [Docker](#docker)

Feel free to provide feedback in [GitHub Issues](https://github.com/Byaidu/PDFMathTranslate/issues) or [Telegram Group](https://t.me/+Z9_SgnxmsmA5NzBl).

For details on how to contribute, please consult the [Contribution Guide](https://github.com/Byaidu/PDFMathTranslate/wiki/Contribution-Guide---%E8%B4%A1%E7%8C%AE%E6%8C%87%E5%8D%97).

&lt;h2 id=&quot;updates&quot;&gt;Updates&lt;/h2&gt;

- [May 9, 2025] pdf2zh 2.0 Preview Version [#586](https://github.com/Byaidu/PDFMathTranslate/issues/586): The Windows ZIP file and Docker image are now available.

&gt; [!CAUTION]
&gt;
&gt; The current maintainer of this project is developing pdf2zh v2.0-rc at [aw/pdf2zh v2-rc](https://github.com/awwaawwa/PDFMathTranslate/tree/v2-rc).
&gt;
&gt; v2.0-rc can now accept some small PRs. Please discuss with the maintainer [@awwaawwa](https://github.com/awwaawwa) in the [main repository issue](https://github.com/Byaidu/PDFMathTranslate/issues) before submitting a PR to [aw/pdf2zh v2-rc](https://github.com/awwaawwa/PDFMathTranslate/tree/v2-rc)~

- [Mar. 3, 2025] Experimental support for the new backend [BabelDOC](https://github.com/funstory-ai/BabelDOC) WebUI added as an experimental option (by [@awwaawwa](https://github.com/awwaawwa))
- [Feb. 22 2025] Better release CI and well-packaged windows-amd64 exe (by [@awwaawwa](https://github.com/awwaawwa))
- [Dec. 24 2024] The translator now supports local models on [Xinference](https://github.com/xorbitsai/inference) _(by [@imClumsyPanda](https://github.com/imClumsyPanda))_
- [Dec. 19 2024] Non-PDF/A documents are now supported using `-cp` _(by [@reycn](https://github.com/reycn))_
- [Dec. 13 2024] Additional support for backend by _(by [@YadominJinta](https://github.com/YadominJinta))_
- [Dec. 10 2024] The translator now supports OpenAI models on Azure _(by [@yidasanqian](https://github.com/yidasanqian))_

&lt;h2 id=&quot;preview&quot;&gt;Preview&lt;/h2&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;./docs/images/preview.gif&quot; width=&quot;80%&quot;/&gt;
&lt;/div&gt;

&lt;h2 id=&quot;demo&quot;&gt;Online Service üåü&lt;/h2&gt;

You can try our application out using either of the following demos:

- [Public free service](https://pdf2zh.com/) online without installation _(recommended)_.
- [Immersive Translate - BabelDOC](https://app.immersivetranslate.com/babel-doc/) 1000 free pages per month. _(recommended)_
- [Demo hosted on HuggingFace](https://huggingface.co/spaces/reycn/PDFMathTranslate-Docker)
- [Demo hosted on ModelScope](https://www.modelscope.cn/studios/AI-ModelScope/PDFMathTranslate) without installation.

Note that the computing resources of the demo are limited, so please avoid abusing them.

&lt;h2 id=&quot;install&quot;&gt;Installation and Usage&lt;/h2&gt;

### Methods

For different use cases, we provide distinct methods to use our program:

&lt;details open&gt;
  &lt;summary&gt;1. UV install&lt;/summary&gt;

1. Python installed (3.10 &lt;= version &lt;= 3.12)

2. Install our package:

   ```bash
   pip install uv
   uv tool install --python 3.12 pdf2zh
   ```

3. Execute translation, files generated in [current working directory](https://chatgpt.com/share/6745ed36-9acc-800e-8a90-59204bd13444):

   ```bash
   pdf2zh document.pdf
   ```

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;2. Windows exe&lt;/summary&gt;

1. Download pdf2zh-version-win64.zip from [release page](https://github.com/Byaidu/PDFMathTranslate/releases)

2. Unzip and double-click `pdf2zh.exe` to run.

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;3. Graphic user interface&lt;/summary&gt;

1. Python installed (3.10 &lt;= version &lt;= 3.12)

2. Install our package:

```bash
pip install pdf2zh
```

3. Start using in browser:

   ```bash
   pdf2zh -i
   ```

4. If your browser has not been started automatically, goto

   ```bash
   http://localhost:7860/
   ```

   &lt;img src=&quot;./docs/images/gui.gif&quot; width=&quot;500&quot;/&gt;

See [documentation for GUI](./docs/README_GUI.md) for more details.

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;4. Docker&lt;/summary&gt;

1. Pull and run:

   ```bash
   docker pull byaidu/pdf2zh
   docker run -d -p 7860:7860 byaidu/pdf2zh
   ```

2. Open in browser:

   ```
   http://localhost:7860/
   ```

For docker deployment on cloud service:

&lt;div&gt;
&lt;a href=&quot;https://www.heroku.com/deploy?template=https://github.com/Byaidu/PDFMathTranslate&quot;&gt;
  &lt;img src=&quot;https://www.herokucdn.com/deploy/button.svg&quot; alt=&quot;Deploy&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://render.com/deploy&quot;&gt;
  &lt;img src=&quot;https://render.com/images/deploy-to-render-button.svg&quot; alt=&quot;Deploy to Koyeb&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://zeabur.com/templates/5FQIGX?referralCode=reycn&quot;&gt;
  &lt;img src=&quot;https://zeabur.com/button.svg&quot; alt=&quot;Deploy on Zeabur&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://template.sealos.io/deploy?templateName=pdf2zh&quot;&gt;
  &lt;img src=&quot;https://sealos.io/Deploy-on-Sealos.svg&quot; alt=&quot;Deploy on Sealos&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://app.koyeb.com/deploy?type=git&amp;builder=buildpack&amp;repository=github.com/Byaidu/PDFMathTranslate&amp;branch=main&amp;name=pdf-math-translate&quot;&gt;
  &lt;img src=&quot;https://www.koyeb.com/static/images/deploy/button.svg&quot; alt=&quot;Deploy to Koyeb&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;5. Zotero Plugin&lt;/summary&gt;


See [Zotero PDF2zh](https://github.com/guaguastandup/zotero-pdf2zh) for more details.

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;6. Commandline&lt;/summary&gt;

1. Python installed (3.10 &lt;= version &lt;= 3.12)
2. Install our package:

   ```bash
   pip install pdf2zh
   ```

3. Execute translation, files generated in [current working directory](https://chatgpt.com/share/6745ed36-9acc-800e-8a90-59204bd13444):

   ```bash
   pdf2zh document.pdf
   ```

&lt;/details&gt;

&gt; [!TIP]
&gt;
&gt; - If you&#039;re using Windows and cannot open the file after downloading, please install [vc_redist.x64.exe](https://aka.ms/vs/17/release/vc_redist.x64.exe) and try again.
&gt;
&gt; - If you cannot access Docker Hub, please try the image on [GitHub Container Registry](https://github.com/Byaidu/PDFMathTranslate/pkgs/container/pdfmathtranslate).
&gt; ```bash
&gt; docker pull ghcr.io/byaidu/pdfmathtranslate
&gt; docker run -d -p 7860:7860 ghcr.io/byaidu/pdfmathtranslate
&gt; ```

### Unable to install?

The present program needs an AI model(`wybxc/DocLayout-YOLO-DocStructBench-onnx`) before working and some users are not able to download due to network issues. If you have a problem with downloading this model, we provide a workaround using the following environment variable:

```shell
set HF_ENDPOINT=https://hf-mirror.com
```

For PowerShell user:

```shell
$env:HF_ENDPOINT = https://hf-mirror.com
```

If the solution does not work to you / you encountered other issues, please refer to [frequently asked questions](https://github.com/Byaidu/PDFMathTranslate/wiki#-faq--%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98).

&lt;h2 id=&quot;usage&quot;&gt;Advanced Options&lt;/h2&gt;

Execute the translation command in the command line to generate the translated document `example-mono.pdf` and the bilingual document `example-dual.pdf` in the current working directory. Use Google as the default translation service. More support translation services can find [HERE](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#services).

&lt;img src=&quot;./docs/images/cmd.explained.png&quot; width=&quot;580px&quot;  alt=&quot;cmd&quot;/&gt;

In the following table, we list all advanced options for reference:

| Option                | Function                                                                                                      | Example                                        |
| --------------------- | ------------------------------------------------------------------------------------------------------------- | ---------------------------------------------- |
| files                 | Local files                                                                                                   | `pdf2zh ~/local.pdf`                           |
| links                 | Online files                                                                                                  | `pdf2zh http://arxiv.org/paper.pdf`            |
| `-i`                  | [Enter GUI](#gui)                                                                                             | `pdf2zh -i`                                    |
| `-p`                  | [Partial document translation](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#partial) | `pdf2zh example.pdf -p 1`                      |
| `-li`                 | [Source language](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#languages)            | `pdf2zh example.pdf -li en`                    |
| `-lo`                 | [Target language](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#languages)            | `pdf2zh example.pdf -lo zh`                    |
| `-s`                  | [Translation service](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#services)         | `pdf2zh example.pdf -s deepl`                  |
| `-t`                  | [Multi-threads](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#threads)                | `pdf2zh example.pdf -t 1`                      |
| `-o`                  | Output dir                                                                                                    | `pdf2zh example.pdf -o output`                 |
| `-f`, `-c`            | [Exceptions](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#exceptions)                | `pdf2zh example.pdf -f &quot;(MS.*)&quot;`               |
| `-cp`                 | Compatibility Mode                                                                                            | `pdf2zh example.pdf --compatible`              |
| `--skip-subset-fonts` | [Skip font subset](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#font-subset)         | `pdf2zh example.pdf --skip-subset-fonts`       |
| `--ignore-cache`      | [Ignore translate cache](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#cache)         | `pdf2zh example.pdf --ignore-cache`            |
| `--share`             | Public link                                                                                                   | `pdf2zh -i --share`                            |
| `--authorized`        | [Authorization](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#auth)                   | `pdf2zh -i --authorized users.txt [auth.html]` |
| `--prompt`            | [Custom Prompt](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#prompt)                 | `pdf2zh --prompt [prompt.txt]`                 |
| `--onnx`              | [Use Custom DocLayout-YOLO ONNX model]                                                                        | `pdf2zh --onnx [onnx/model/path]`              |
| `--serverport`        | [Use Custom WebUI port]                                                                                       | `pdf2zh --serverport 7860`                     |
| `--dir`               | [batch translate]                                                                                             | `pdf2zh --dir /path/to/translate/`             |
| `--config`            | [configuration file](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#cofig)             | `pdf2zh --config /path/to/config/config.json`  |
| `--serverport`        | [custom gradio server port]                                                                                   | `pdf2zh --serverport 7860`                     |
| `--babeldoc`          | Use Experimental backend [BabelDOC](https://funstory-ai.github.io/BabelDOC/) to translate                     | `pdf2zh --babeldoc` -s openai example.pdf      |
| `--mcp`               | Enable MCP STDIO mode                                                                                         | `pdf2zh --mcp`                                 |
| `--sse`               | Enable MCP SSE mode                                                                                           | `pdf2zh --mcp --sse`                           |

For detailed explanations, please refer to our document about [Advanced Usage](./docs/ADVANCED.md) for a full list of each option.

&lt;h2 id=&quot;downstream&quot;&gt;Secondary Development (APIs)&lt;/h2&gt;

For downstream applications, please refer to our document about [API Details](./docs/APIS.md) for further information about:

- [Python API](./docs/APIS.md#api-python), how to use the program in other Python programs
- [HTTP API](./docs/APIS.md#api-http), how to communicate with a server with the program installed

&lt;h2 id=&quot;todo&quot;&gt;TODOs&lt;/h2&gt;

- [ ] Parse layout with DocLayNet based models, [PaddleX](https://github.com/PaddlePaddle/PaddleX/blob/17cc27ac3842e7880ca4aad92358d3ef8555429a/paddlex/repo_apis/PaddleDetection_api/object_det/official_categories.py#L81), [PaperMage](https://github.com/allenai/papermage/blob/9cd4bb48cbedab45d0f7a455711438f1632abebe/README.md?plain=1#L102), [SAM2](https://github.com/facebookresearch/sam2)

- [ ] Fix page rotation, table of contents, format of lists

- [ ] Fix pixel formula in old papers

- [ ] Async retry except KeyboardInterrupt

- [ ] Knuth‚ÄìPlass algorithm for western languages

- [ ] Support non-PDF/A files

- [ ] Plugins of [Zotero](https://github.com/zotero/zotero) and [Obsidian](https://github.com/obsidianmd/obsidian-releases)

&lt;h2 id=&quot;acknowledgement&quot;&gt;Acknowledgements&lt;/h2&gt;

- [Immersive Translation](https://immersivetranslate.com) sponsors monthly Pro membership redemption codes for active contributors to this project, see details at: [CONTRIBUTOR_REWARD.md](https://github.com/funstory-ai/BabelDOC/blob/main/docs/CONTRIBUTOR_REWARD.md)

- New backend: [BabelDOC](https://github.com/funstory-ai/BabelDOC)

- Document merging: [PyMuPDF](https://github.com/pymupdf/PyMuPDF)

- Document parsing: [Pdfminer.six](https://github.com/pdfminer/pdfminer.six)

- Document extraction: [MinerU](https://github.com/opendatalab/MinerU)

- Document Preview: [Gradio PDF](https://github.com/freddyaboulton/gradio-pdf)

- Multi-threaded translation: [MathTranslate](https://github.com/SUSYUSTC/MathTranslate)

- Layout parsing: [DocLayout-YOLO](https://github.com/opendatalab/DocLayout-YOLO)

- Document standard: [PDF Explained](https://zxyle.github.io/PDF-Explained/), [PDF Cheat Sheets](https://pdfa.org/resource/pdf-cheat-sheets/)

- Multilingual Font: [Go Noto Universal](https://github.com/satbyy/go-noto-universal)

&lt;h2 id=&quot;contrib&quot;&gt;Contributors&lt;/h2&gt;

&lt;a href=&quot;https://github.com/Byaidu/PDFMathTranslate/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://opencollective.com/PDFMathTranslate/contributors.svg?width=890&amp;button=false&quot; /&gt;
&lt;/a&gt;

![Alt](https://repobeats.axiom.co/api/embed/dfa7583da5332a11468d686fbd29b92320a6a869.svg &quot;Repobeats analytics image&quot;)

&lt;h2 id=&quot;star_hist&quot;&gt;Star History&lt;/h2&gt;

&lt;a href=&quot;https://star-history.com/#Byaidu/PDFMathTranslate&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&amp;type=Date&quot;/&gt;
 &lt;/picture&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[trycua/cua]]></title>
            <link>https://github.com/trycua/cua</link>
            <guid>https://github.com/trycua/cua</guid>
            <pubDate>Wed, 28 May 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[c/ua is the Docker Container for Computer-Use AI Agents.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/trycua/cua">trycua/cua</a></h1>
            <p>c/ua is the Docker Container for Computer-Use AI Agents.</p>
            <p>Language: Python</p>
            <p>Stars: 8,098</p>
            <p>Forks: 337</p>
            <p>Stars today: 63 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; alt=&quot;Cua logo&quot; height=&quot;150&quot; srcset=&quot;img/logo_white.png&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; alt=&quot;Cua logo&quot; height=&quot;150&quot; srcset=&quot;img/logo_black.png&quot;&gt;
    &lt;img alt=&quot;Cua logo&quot; height=&quot;150&quot; src=&quot;img/logo_black.png&quot;&gt;
  &lt;/picture&gt;

  [![Python](https://img.shields.io/badge/Python-333333?logo=python&amp;logoColor=white&amp;labelColor=333333)](#)
  [![Swift](https://img.shields.io/badge/Swift-F05138?logo=swift&amp;logoColor=white)](#)
  [![macOS](https://img.shields.io/badge/macOS-000000?logo=apple&amp;logoColor=F0F0F0)](#)
  [![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?&amp;logo=discord&amp;logoColor=white)](https://discord.com/invite/mVnXXpdE85)
  &lt;br&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/13685&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13685&quot; alt=&quot;trycua%2Fcua | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

**c/ua** (pronounced &quot;koo-ah&quot;) enables AI agents to control full operating systems in high-performance virtual containers with near-native speed on Apple Silicon.

&lt;div align=&quot;center&quot;&gt;
  &lt;video src=&quot;https://github.com/user-attachments/assets/c619b4ea-bb8e-4382-860e-f3757e36af20&quot; width=&quot;800&quot; controls&gt;&lt;/video&gt;
&lt;/div&gt;

# üöÄ Quick Start

Get started with a Computer-Use Agent UI and a VM with a single command:


```bash
/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/trycua/cua/main/scripts/playground.sh)&quot;
```


This script will:
- Install Lume CLI for VM management (if needed)
- Pull the latest macOS CUA image (if needed)
- Set up Python environment and install/update required packages
- Launch the Computer-Use Agent UI

#### Supported [Agent Loops](https://github.com/trycua/cua/blob/main/libs/agent/README.md#agent-loops)
- [UITARS-1.5](https://github.com/trycua/cua/blob/main/libs/agent/README.md#agent-loops) - Run locally on Apple Silicon with MLX, or use cloud providers
- [OpenAI CUA](https://github.com/trycua/cua/blob/main/libs/agent/README.md#agent-loops) - Use OpenAI&#039;s Computer-Use Preview model
- [Anthropic CUA](https://github.com/trycua/cua/blob/main/libs/agent/README.md#agent-loops) - Use Anthropic&#039;s Computer-Use capabilities
- [OmniParser-v2.0](https://github.com/trycua/cua/blob/main/libs/agent/README.md#agent-loops) - Control UI with [Set-of-Marks prompting](https://som-gpt4v.github.io/) using any vision model

### System Requirements

- Mac with Apple Silicon (M1/M2/M3/M4 series)
- macOS 15 (Sequoia) or newer
- Disk space for VM images (30GB+ recommended)


# üíª For Developers

### Step 1: Install Lume CLI

```bash
/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/trycua/cua/main/libs/lume/scripts/install.sh)&quot;
```

Lume CLI manages high-performance macOS/Linux VMs with near-native speed on Apple Silicon.

### Step 2: Pull the macOS CUA Image

```bash
lume pull macos-sequoia-cua:latest
```

The macOS CUA image contains the default Mac apps and the Computer Server for easy automation.

### Step 3: Install Python SDK

```bash
pip install &quot;cua-computer[all]&quot; &quot;cua-agent[all]&quot;
```

Alternatively, see the [Developer Guide](./docs/Developer-Guide.md) for building from source.

### Step 4: Use in Your Code

```python
from computer import Computer
from agent import ComputerAgent, LLM

async def main():
    # Start a local macOS VM with a 1024x768 display
    async with Computer(os_type=&quot;macos&quot;, display=&quot;1024x768&quot;) as computer:

        # Example: Direct control of a macOS VM with Computer
        await computer.interface.left_click(100, 200)
        await computer.interface.type_text(&quot;Hello, world!&quot;)
        screenshot_bytes = await computer.interface.screenshot()
        
        # Example: Create and run an agent locally using mlx-community/UI-TARS-1.5-7B-6bit
        agent = ComputerAgent(
          computer=computer,
          loop=&quot;UITARS&quot;,
          model=LLM(provider=&quot;MLXVLM&quot;, name=&quot;mlx-community/UI-TARS-1.5-7B-6bit&quot;)
        )
        await agent.run(&quot;Find the trycua/cua repository on GitHub and follow the quick start guide&quot;)

main()
```

For ready-to-use examples, check out our [Notebooks](./notebooks/) collection.

### Lume CLI Reference

```bash
# Install Lume CLI and background service
curl -fsSL https://raw.githubusercontent.com/trycua/cua/main/libs/lume/scripts/install.sh | bash

# List all VMs
lume ls

# Pull a VM image
lume pull macos-sequoia-cua:latest

# Create a new VM
lume create my-vm --os macos --cpu 4 --memory 8GB --disk-size 50GB

# Run a VM (creates and starts if it doesn&#039;t exist)
lume run macos-sequoia-cua:latest

# Stop a VM
lume stop macos-sequoia-cua_latest

# Delete a VM
lume delete macos-sequoia-cua_latest
```

### Lumier CLI Reference

For advanced container-like virtualization, check out [Lumier](./libs/lumier/README.md) - a Docker interface for macOS and Linux VMs.

```bash
# Install Lume CLI and background service
curl -fsSL https://raw.githubusercontent.com/trycua/cua/main/libs/lume/scripts/install.sh | bash

# Run macOS in a Docker container
docker run -it --rm \
    --name lumier-vm \
    -p 8006:8006 \
    -v $(pwd)/storage:/storage \
    -v $(pwd)/shared:/shared \
    -e VM_NAME=lumier-vm \
    -e VERSION=ghcr.io/trycua/macos-sequoia-cua:latest \
    -e CPU_CORES=4 \
    -e RAM_SIZE=8192 \
    -e HOST_STORAGE_PATH=$(pwd)/storage \
    -e HOST_SHARED_PATH=$(pwd)/shared \
    trycua/lumier:latest
```

## Resources

- [How to use the MCP Server with Claude Desktop or other MCP clients](./libs/mcp-server/README.md) - One of the easiest ways to get started with C/ua
- [How to use OpenAI Computer-Use, Anthropic, OmniParser, or UI-TARS for your Computer-Use Agent](./libs/agent/README.md)
- [How to use Lume CLI for managing desktops](./libs/lume/README.md)
- [Training Computer-Use Models: Collecting Human Trajectories with C/ua (Part 1)](https://www.trycua.com/blog/training-computer-use-models-trajectories-1)
- [Build Your Own Operator on macOS (Part 1)](https://www.trycua.com/blog/build-your-own-operator-on-macos-1)

## Modules

| Module | Description | Installation |
|--------|-------------|---------------|
| [**Lume**](./libs/lume/README.md) | VM management for macOS/Linux using Apple&#039;s Virtualization.Framework | `curl -fsSL https://raw.githubusercontent.com/trycua/cua/main/libs/lume/scripts/install.sh \| bash` |
| [**Lumier**](./libs/lumier/README.md) | Docker interface for macOS and Linux VMs | `docker pull trycua/lumier:latest` |
| [**Computer**](./libs/computer/README.md) | Interface for controlling virtual machines | `pip install &quot;cua-computer[all]&quot;` |
| [**Agent**](./libs/agent/README.md) | AI agent framework for automating tasks | `pip install &quot;cua-agent[all]&quot;` |
| [**MCP Server**](./libs/mcp-server/README.md) | MCP server for using CUA with Claude Desktop | `pip install cua-mcp-server` |
| [**SOM**](./libs/som/README.md) | Self-of-Mark library for Agent | `pip install cua-som` |
| [**PyLume**](./libs/pylume/README.md) | Python bindings for Lume | `pip install pylume` |
| [**Computer Server**](./libs/computer-server/README.md) | Server component for Computer | `pip install cua-computer-server` |
| [**Core**](./libs/core/README.md) | Core utilities | `pip install cua-core` |

## Computer Interface Reference

For complete examples, see [computer_examples.py](./examples/computer_examples.py) or [computer_nb.ipynb](./notebooks/computer_nb.ipynb)

```python
# Mouse Actions
await computer.interface.left_click(x, y)       # Left click at coordinates
await computer.interface.right_click(x, y)      # Right click at coordinates
await computer.interface.double_click(x, y)     # Double click at coordinates
await computer.interface.move_cursor(x, y)      # Move cursor to coordinates
await computer.interface.drag_to(x, y, duration)  # Drag to coordinates
await computer.interface.get_cursor_position()  # Get current cursor position

# Keyboard Actions
await computer.interface.type_text(&quot;Hello&quot;)     # Type text
await computer.interface.press_key(&quot;enter&quot;)     # Press a single key
await computer.interface.hotkey(&quot;command&quot;, &quot;c&quot;) # Press key combination

# Screen Actions
await computer.interface.screenshot()           # Take a screenshot
await computer.interface.get_screen_size()      # Get screen dimensions

# Clipboard Actions
await computer.interface.set_clipboard(text)    # Set clipboard content
await computer.interface.copy_to_clipboard()    # Get clipboard content

# File System Operations
await computer.interface.file_exists(path)      # Check if file exists
await computer.interface.directory_exists(path) # Check if directory exists
await computer.interface.run_command(cmd)       # Run shell command

# Accessibility
await computer.interface.get_accessibility_tree() # Get accessibility tree
```

## ComputerAgent Reference

For complete examples, see [agent_examples.py](./examples/agent_examples.py) or [agent_nb.ipynb](./notebooks/agent_nb.ipynb)

```python
# Import necessary components
from agent import ComputerAgent, LLM, AgentLoop, LLMProvider

# UI-TARS-1.5 agent for local execution with MLX
ComputerAgent(loop=AgentLoop.UITARS, model=LLM(provider=LLMProvider.MLXVLM, name=&quot;mlx-community/UI-TARS-1.5-7B-6bit&quot;))   
# OpenAI Computer-Use agent using OPENAI_API_KEY  
ComputerAgent(loop=AgentLoop.OPENAI, model=LLM(provider=LLMProvider.OPENAI, name=&quot;computer-use-preview&quot;))
# Anthropic Claude agent using ANTHROPIC_API_KEY
ComputerAgent(loop=AgentLoop.ANTHROPIC, model=LLM(provider=LLMProvider.ANTHROPIC))

# OmniParser loop for UI control using Set-of-Marks (SOM) prompting and any vision LLM
ComputerAgent(loop=AgentLoop.OMNI, model=LLM(provider=LLMProvider.OLLAMA, name=&quot;gemma3:12b-it-q4_K_M&quot;))      
# OpenRouter example using OAICOMPAT provider
ComputerAgent(
    loop=AgentLoop.OMNI,
    model=LLM(
        provider=LLMProvider.OAICOMPAT, 
        name=&quot;openai/gpt-4o-mini&quot;,
        provider_base_url=&quot;https://openrouter.ai/api/v1&quot;
    ),
    api_key=&quot;your-openrouter-api-key&quot;
)
```

## Demos

Check out these demos of the Computer-Use Agent in action:

&lt;details open&gt;
&lt;summary&gt;&lt;b&gt;MCP Server: Work with Claude Desktop and Tableau&lt;/b&gt;&lt;/summary&gt;
&lt;br&gt;
&lt;div align=&quot;center&quot;&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/9f573547-5149-493e-9a72-396f3cff29df&quot; width=&quot;800&quot; controls&gt;&lt;/video&gt;
&lt;/div&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;AI-Gradio: Multi-app workflow with browser, VS Code and terminal&lt;/b&gt;&lt;/summary&gt;
&lt;br&gt;
&lt;div align=&quot;center&quot;&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/723a115d-1a07-4c8e-b517-88fbdf53ed0f&quot; width=&quot;800&quot; controls&gt;&lt;/video&gt;
&lt;/div&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Notebook: Fix GitHub issue in Cursor&lt;/b&gt;&lt;/summary&gt;
&lt;br&gt;
&lt;div align=&quot;center&quot;&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/f67f0107-a1e1-46dc-aa9f-0146eb077077&quot; width=&quot;800&quot; controls&gt;&lt;/video&gt;
&lt;/div&gt;
&lt;/details&gt;

## Community

Join our [Discord community](https://discord.com/invite/mVnXXpdE85) to discuss ideas, get assistance, or share your demos!

## License

Cua is open-sourced under the MIT License - see the [LICENSE](LICENSE) file for details.

Microsoft&#039;s OmniParser, which is used in this project, is licensed under the Creative Commons Attribution 4.0 International License (CC-BY-4.0) - see the [OmniParser LICENSE](https://github.com/microsoft/OmniParser/blob/master/LICENSE) file for details.

## Contributing

We welcome contributions to CUA! Please refer to our [Contributing Guidelines](CONTRIBUTING.md) for details.

## Trademarks

Apple, macOS, and Apple Silicon are trademarks of Apple Inc. Ubuntu and Canonical are registered trademarks of Canonical Ltd. Microsoft is a registered trademark of Microsoft Corporation. This project is not affiliated with, endorsed by, or sponsored by Apple Inc., Canonical Ltd., or Microsoft Corporation.

## Stargazers

Thank you to all our supporters!

[![Stargazers over time](https://starchart.cc/trycua/cua.svg?variant=adaptive)](https://starchart.cc/trycua/cua)

## Contributors

&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt;
&lt;!-- prettier-ignore-start --&gt;
&lt;!-- markdownlint-disable --&gt;
&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/f-trycua&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/195596869?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;f-trycua&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;f-trycua&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-f-trycua&quot; title=&quot;Code&quot;&gt;üíª&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;http://pepicrft.me&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/663605?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Pedro Pi√±era Buend√≠a&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Pedro Pi√±era Buend√≠a&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-pepicrft&quot; title=&quot;Code&quot;&gt;üíª&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://iamit.in&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/5647941?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Amit Kumar&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Amit Kumar&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-aktech&quot; title=&quot;Code&quot;&gt;üíª&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://productsway.com/&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/870029?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Dung Duc Huynh (Kaka)&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Dung Duc Huynh (Kaka)&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-jellydn&quot; title=&quot;Code&quot;&gt;üíª&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;http://zaydkrunz.com&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/70227235?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Zayd Krunz&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Zayd Krunz&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-ShrootBuck&quot; title=&quot;Code&quot;&gt;üíª&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/PrashantRaj18198&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/23168997?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Prashant Raj&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Prashant Raj&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-PrashantRaj18198&quot; title=&quot;Code&quot;&gt;üíª&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://www.mobile.dev&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/847683?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Leland Takamine&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Leland Takamine&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-Leland-Takamine&quot; title=&quot;Code&quot;&gt;üíª&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/ddupont808&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/3820588?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;ddupont&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;ddupont&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-ddupont808&quot; title=&quot;Code&quot;&gt;üíª&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/Lizzard1123&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/46036335?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Ethan Gutierrez&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ethan Gutierrez&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-Lizzard1123&quot; title=&quot;Code&quot;&gt;üíª&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://ricterz.me&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/5282759?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Ricter Zheng&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ricter Zheng&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-RicterZ&quot; title=&quot;Code&quot;&gt;üíª&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://www.trytruffle.ai/&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/50844303?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Rahul Karajgikar&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Rahul Karajgikar&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-rahulkarajgikar&quot; title=&quot;Code&quot;&gt;üíª&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/trospix&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/81363696?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;trospix&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;trospix&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-trospix&quot; title=&quot;Code&quot;&gt;üíª&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://wavee.world/invitation/b96d00e6-b802-4a1b-8a66-2e3854a01ffd&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/22633385?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Ikko Eltociear Ashimine&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ikko Eltociear Ashimine&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-eltociear&quot; title=&quot;Code&quot;&gt;üíª&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/dp221125&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/10572119?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;ÌïúÏÑùÌò∏(MilKyo)&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;ÌïúÏÑùÌò∏(MilKyo)&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-dp221125&quot; title=&quot;Code&quot;&gt;üíª&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://www.encona.com/&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/891558?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Rahim Nathwani&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Rahim Nathwani&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-rahimnathwani&quot; title=&quot;Code&quot;&gt;üíª&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://mjspeck.github.io/&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/20689127?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Matt Speck&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Matt Speck&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-mjspeck&quot; title=&quot;Code&quot;&gt;üíª&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/FinnBorge&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/9272726?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;FinnBorge&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;FinnBorge&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-FinnBorge&quot; title=&quot;Code&quot;&gt;üíª&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/jklapacz&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/5343758?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Jakub Klapacz&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jakub Klapacz&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-jklapacz&quot; title=&quot;Code&quot;&gt;üíª&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/evnsnclr&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/139897548?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Evan smith&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Evan smith&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-evnsnclr&quot; title=&quot;Code&quot;&gt;üíª&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;!-- markdownlint-restore --&gt;
&lt;!-- prettier-ignore-end --&gt;

&lt;!-- ALL-CONTRIBUTORS-LIST:END --&gt;</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[seleniumbase/SeleniumBase]]></title>
            <link>https://github.com/seleniumbase/SeleniumBase</link>
            <guid>https://github.com/seleniumbase/SeleniumBase</guid>
            <pubDate>Wed, 28 May 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[Python APIs for web automation, testing, and bypassing bot-detection.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/seleniumbase/SeleniumBase">seleniumbase/SeleniumBase</a></h1>
            <p>Python APIs for web automation, testing, and bypassing bot-detection.</p>
            <p>Language: Python</p>
            <p>Stars: 10,530</p>
            <p>Forks: 1,321</p>
            <p>Stars today: 125 stars today</p>
            <h2>README</h2><pre>&lt;!-- SeleniumBase Docs --&gt;

&lt;meta property=&quot;og:site_name&quot; content=&quot;SeleniumBase&quot;&gt;
&lt;meta property=&quot;og:title&quot; content=&quot;SeleniumBase: Python Web Automation and E2E Testing&quot; /&gt;
&lt;meta property=&quot;og:description&quot; content=&quot;Fast, easy, and reliable Web/UI testing with Python.&quot; /&gt;
&lt;meta property=&quot;og:keywords&quot; content=&quot;Python, pytest, selenium, webdriver, testing, automation, seleniumbase, framework, dashboard, recorder, reports, screenshots&quot;&gt;
&lt;meta property=&quot;og:image&quot; content=&quot;https://seleniumbase.github.io/cdn/img/mac_sb_logo_5b.png&quot; /&gt;
&lt;link rel=&quot;icon&quot; href=&quot;https://seleniumbase.github.io/img/logo7.png&quot; /&gt;

&lt;h1&gt;SeleniumBase&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/&quot;&gt;&lt;img src=&quot;https://seleniumbase.github.io/cdn/img/super_logo_sb3.png&quot; alt=&quot;SeleniumBase&quot; title=&quot;SeleniumBase&quot; width=&quot;350&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot; class=&quot;hero__title&quot;&gt;&lt;b&gt;All-in-one Browser Automation Framework:&lt;br /&gt;Web Crawling / Testing / Scraping / Stealth&lt;/b&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://pypi.python.org/pypi/seleniumbase&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/seleniumbase.svg?color=3399EE&quot; alt=&quot;PyPI version&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/releases&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/seleniumbase/SeleniumBase.svg?color=22AAEE&quot; alt=&quot;GitHub version&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://seleniumbase.io&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/docs-seleniumbase.io-11BBAA.svg&quot; alt=&quot;SeleniumBase Docs&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/actions&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://github.com/seleniumbase/SeleniumBase/workflows/CI%20build/badge.svg&quot; alt=&quot;SeleniumBase GitHub Actions&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/EdhQTn3EyE&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/727927627830001734?color=7289DA&amp;label=Discord&amp;logo=discord&amp;logoColor=white&quot;/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;#python_installation&quot;&gt;üöÄ Start&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/features_list.md&quot;&gt;üè∞ Features&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/customizing_test_runs.md&quot;&gt;üéõÔ∏è Options&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/ReadMe.md&quot;&gt;üìö Examples&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/seleniumbase/console_scripts/ReadMe.md&quot;&gt;üå† Scripts&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/mobile_testing.md&quot;&gt;üì± Mobile&lt;/a&gt;
&lt;br /&gt;
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/method_summary.md&quot;&gt;üìò APIs&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/syntax_formats.md&quot;&gt; üî† Formats&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/recorder_mode.md&quot;&gt;üî¥ Recorder&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/example_logs/ReadMe.md&quot;&gt;üìä Dashboard&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/locale_codes.md&quot;&gt;üóæ Locales&lt;/a&gt; |
&lt;a href=&quot;https://seleniumbase.io/devices/?url=seleniumbase.com&quot;&gt;üíª Farm&lt;/a&gt;
&lt;br /&gt;
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/commander.md&quot;&gt;üéñÔ∏è GUI&lt;/a&gt; |
&lt;a href=&quot;https://seleniumbase.io/demo_page&quot;&gt;üì∞ TestPage&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/uc_mode.md&quot;&gt;üë§ UC Mode&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/cdp_mode/ReadMe.md&quot;&gt;üêô CDP Mode&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/chart_maker/ReadMe.md&quot;&gt;üì∂ Charts&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/seleniumbase/utilities/selenium_grid/ReadMe.md&quot;&gt;üåê Grid&lt;/a&gt;
&lt;br /&gt;
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/how_it_works.md&quot;&gt;üëÅÔ∏è How&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/tree/master/examples/migration/raw_selenium&quot;&gt;üöù Migrate&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/case_plans.md&quot;&gt;üóÇÔ∏è CasePlans&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/tree/master/examples/boilerplates&quot;&gt;‚ôªÔ∏è Template&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/master_qa/ReadMe.md&quot;&gt;üß¨ Hybrid&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/tour_examples/ReadMe.md&quot;&gt;üöé Tours&lt;/a&gt;
&lt;br /&gt;
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/integrations/github/workflows/ReadMe.md&quot;&gt;ü§ñ CI/CD&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/js_package_manager.md&quot;&gt;üïπÔ∏è JSMgr&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/translations.md&quot;&gt;üåè Translator&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/presenter/ReadMe.md&quot;&gt;üéûÔ∏è Presenter&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/dialog_boxes/ReadMe.md&quot;&gt;üõÇ Dialog&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/visual_testing/ReadMe.md&quot;&gt;üñºÔ∏è Visual&lt;/a&gt;
&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;SeleniumBase is the professional toolkit for web automation activities. Built for testing websites, bypassing CAPTCHAs, enhancing productivity, completing tasks, and scaling your business.&lt;/p&gt;

--------

üìö Learn from [**over 200 examples** in the **SeleniumBase/examples/** folder](https://github.com/seleniumbase/SeleniumBase/tree/master/examples).

üêô Note that &lt;a translate=&quot;no&quot; href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/uc_mode.md&quot;&gt;&lt;b&gt;UC Mode&lt;/b&gt;&lt;/a&gt; / &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/cdp_mode/ReadMe.md&quot;&gt;&lt;b&gt;CDP Mode&lt;/b&gt;&lt;/a&gt; (Stealth Mode) have their own ReadMe files.

‚ÑπÔ∏è Most scripts run with raw &lt;code translate=&quot;no&quot;&gt;&lt;b&gt;python&lt;/b&gt;&lt;/code&gt;, although some scripts use &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/syntax_formats.md&quot;&gt;Syntax Formats&lt;/a&gt; that expect &lt;a href=&quot;https://docs.pytest.org/en/latest/how-to/usage.html&quot; translate=&quot;no&quot;&gt;&lt;b&gt;pytest&lt;/b&gt;&lt;/a&gt; (a Python unit-testing framework included with SeleniumBase that can discover, collect, and run tests automatically).

--------

&lt;p align=&quot;left&quot;&gt;üìó Here&#039;s &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/raw_google.py&quot;&gt;raw_google.py&lt;/a&gt;, which performs a Google search:&lt;/p&gt;

```python
from seleniumbase import SB

with SB(test=True, uc=True) as sb:
    sb.open(&quot;https://google.com/ncr&quot;)
    sb.type(&#039;[title=&quot;Search&quot;]&#039;, &quot;SeleniumBase GitHub page\n&quot;)
    sb.click(&#039;[href*=&quot;github.com/seleniumbase/&quot;]&#039;)
    sb.save_screenshot_to_logs()  # ./latest_logs/
    print(sb.get_page_title())
```

&gt; `python raw_google.py`

&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/raw_google.py&quot;&gt;&lt;img src=&quot;https://seleniumbase.github.io/cdn/gif/google_search.gif&quot; alt=&quot;SeleniumBase Test&quot; title=&quot;SeleniumBase Test&quot; width=&quot;480&quot; /&gt;&lt;/a&gt;

--------

&lt;p align=&quot;left&quot;&gt;üìó Here&#039;s an example of bypassing Cloudflare&#039;s challenge page: &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/cdp_mode/raw_gitlab.py&quot;&gt;SeleniumBase/examples/cdp_mode/raw_gitlab.py&lt;/a&gt;&lt;/p&gt;

```python
from seleniumbase import SB

with SB(uc=True, test=True, locale=&quot;en&quot;) as sb:
    url = &quot;https://gitlab.com/users/sign_in&quot;
    sb.activate_cdp_mode(url)
    sb.uc_gui_click_captcha()
    sb.sleep(2)
```

&lt;img src=&quot;https://seleniumbase.github.io/other/cf_sec.jpg&quot; title=&quot;SeleniumBase&quot; width=&quot;332&quot;&gt; &lt;img src=&quot;https://seleniumbase.github.io/other/gitlab_bypass.png&quot; title=&quot;SeleniumBase&quot; width=&quot;288&quot;&gt;

--------

&lt;p align=&quot;left&quot;&gt;üìó Here&#039;s &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/test_get_swag.py&quot;&gt;test_get_swag.py&lt;/a&gt;, which tests an e-commerce site:&lt;/p&gt;

```python
from seleniumbase import BaseCase
BaseCase.main(__name__, __file__)  # Call pytest

class MyTestClass(BaseCase):
    def test_swag_labs(self):
        self.open(&quot;https://www.saucedemo.com&quot;)
        self.type(&quot;#user-name&quot;, &quot;standard_user&quot;)
        self.type(&quot;#password&quot;, &quot;secret_sauce\n&quot;)
        self.assert_element(&quot;div.inventory_list&quot;)
        self.click(&#039;button[name*=&quot;backpack&quot;]&#039;)
        self.click(&quot;#shopping_cart_container a&quot;)
        self.assert_text(&quot;Backpack&quot;, &quot;div.cart_item&quot;)
        self.click(&quot;button#checkout&quot;)
        self.type(&quot;input#first-name&quot;, &quot;SeleniumBase&quot;)
        self.type(&quot;input#last-name&quot;, &quot;Automation&quot;)
        self.type(&quot;input#postal-code&quot;, &quot;77123&quot;)
        self.click(&quot;input#continue&quot;)
        self.click(&quot;button#finish&quot;)
        self.assert_text(&quot;Thank you for your order!&quot;)
```

&gt; `pytest test_get_swag.py`

&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/test_get_swag.py&quot;&gt;&lt;img src=&quot;https://seleniumbase.github.io/cdn/gif/fast_swag_2.gif&quot; alt=&quot;SeleniumBase Test&quot; title=&quot;SeleniumBase Test&quot; width=&quot;480&quot; /&gt;&lt;/a&gt;

&gt; (The default browser is ``--chrome`` if not set.)

--------

&lt;p align=&quot;left&quot;&gt;üìó Here&#039;s &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/test_coffee_cart.py&quot; target=&quot;_blank&quot;&gt;test_coffee_cart.py&lt;/a&gt;, which verifies an e-commerce site:&lt;/p&gt;

```bash
pytest test_coffee_cart.py --demo
```

&lt;p align=&quot;left&quot;&gt;&lt;a href=&quot;https://seleniumbase.io/coffee/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://seleniumbase.github.io/cdn/gif/coffee_cart.gif&quot; width=&quot;480&quot; alt=&quot;SeleniumBase Coffee Cart Test&quot; title=&quot;SeleniumBase Coffee Cart Test&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&gt; &lt;p&gt;(&lt;code translate=&quot;no&quot;&gt;--demo&lt;/code&gt; mode slows down tests and highlights actions)&lt;/p&gt;

--------

&lt;a id=&quot;multiple_examples&quot;&gt;&lt;/a&gt;

&lt;p align=&quot;left&quot;&gt;üìó Here&#039;s &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/test_demo_site.py&quot; target=&quot;_blank&quot;&gt;test_demo_site.py&lt;/a&gt;, which covers several actions:&lt;/p&gt;

```bash
pytest test_demo_site.py
```

&lt;p align=&quot;left&quot;&gt;&lt;a href=&quot;https://seleniumbase.io/demo_page&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://seleniumbase.github.io/cdn/gif/demo_page_5.gif&quot; width=&quot;480&quot; alt=&quot;SeleniumBase Example&quot; title=&quot;SeleniumBase Example&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&gt; Easy to type, click, select, toggle, drag &amp; drop, and more.

(For more examples, see the &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/ReadMe.md&quot;&gt;SeleniumBase/examples/&lt;/a&gt; folder.)

--------

&lt;p align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/&quot;&gt;&lt;img src=&quot;https://seleniumbase.github.io/cdn/img/super_logo_sb3.png&quot; alt=&quot;SeleniumBase&quot; title=&quot;SeleniumBase&quot; width=&quot;232&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p dir=&quot;auto&quot;&gt;&lt;strong&gt;Explore the README:&lt;/strong&gt;&lt;/p&gt;
&lt;ul dir=&quot;auto&quot;&gt;
&lt;li&gt;&lt;a href=&quot;#install_seleniumbase&quot;   &gt;&lt;strong&gt;Get Started / Installation&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#basic_example_and_usage&quot;&gt;&lt;strong&gt;Basic Example / Usage&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#common_methods&quot;         &gt;&lt;strong&gt;Common Test Methods&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#fun_facts&quot;              &gt;&lt;strong&gt;Fun Facts / Learn More&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#demo_mode_and_debugging&quot;&gt;&lt;strong&gt;Demo Mode / Debugging&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#command_line_options&quot;   &gt;&lt;strong&gt;Command-line Options&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#directory_configuration&quot;&gt;&lt;strong&gt;Directory Configuration&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#seleniumbase_dashboard&quot; &gt;&lt;strong&gt;SeleniumBase Dashboard&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#creating_visual_reports&quot;&gt;&lt;strong&gt;Generating Test Reports&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

--------

&lt;details&gt;
&lt;summary&gt; ‚ñ∂Ô∏è How is &lt;b&gt;SeleniumBase&lt;/b&gt; different from raw Selenium? (&lt;b&gt;click to expand&lt;/b&gt;)&lt;/summary&gt;
&lt;div&gt;

&lt;p&gt;üí° SeleniumBase is a Python framework for browser automation and testing. SeleniumBase uses &lt;a href=&quot;https://www.w3.org/TR/webdriver2/#endpoints&quot; target=&quot;_blank&quot;&gt;Selenium/WebDriver&lt;/a&gt; APIs and incorporates test-runners such as &lt;code translate=&quot;no&quot;&gt;pytest&lt;/code&gt;, &lt;code translate=&quot;no&quot;&gt;pynose&lt;/code&gt;, and &lt;code translate=&quot;no&quot;&gt;behave&lt;/code&gt; to provide organized structure, test discovery, test execution, test state (&lt;i&gt;eg. passed, failed, or skipped&lt;/i&gt;), and command-line options for changing default settings (&lt;i&gt;eg. browser selection&lt;/i&gt;). With raw Selenium, you would need to set up your own options-parser for configuring tests from the command-line.&lt;/p&gt;

&lt;p&gt;üí° SeleniumBase&#039;s driver manager gives you more control over automatic driver downloads. (Use &lt;code translate=&quot;no&quot;&gt;--driver-version=VER&lt;/code&gt; with your &lt;code translate=&quot;no&quot;&gt;pytest&lt;/code&gt; run command to specify the version.) By default, SeleniumBase will download a driver version that matches your major browser version if not set.&lt;/p&gt;

&lt;p&gt;üí° SeleniumBase automatically detects between CSS Selectors and XPath, which means you don&#039;t need to specify the type of selector in your commands (&lt;i&gt;but optionally you could&lt;/i&gt;).&lt;/p&gt;

&lt;p&gt;üí° SeleniumBase methods often perform multiple actions in a single method call. For example, &lt;code translate=&quot;no&quot;&gt;self.type(selector, text)&lt;/code&gt; does the following:&lt;br /&gt;1. Waits for the element to be visible.&lt;br /&gt;2. Waits for the element to be interactive.&lt;br /&gt;3. Clears the text field.&lt;br /&gt;4. Types in the new text.&lt;br /&gt;5. Presses Enter/Submit if the text ends in &lt;code translate=&quot;no&quot;&gt;&quot;\n&quot;&lt;/code&gt;.&lt;br /&gt;With raw Selenium, those actions require multiple method calls.&lt;/p&gt;

&lt;p&gt;üí° SeleniumBase uses default timeout values when not set:&lt;br /&gt;
‚úÖ &lt;code translate=&quot;no&quot;&gt;self.click(&quot;button&quot;)&lt;/code&gt;&lt;br /&gt;
With raw Selenium, methods would fail instantly (&lt;i&gt;by default&lt;/i&gt;) if an element needed more time to load:&lt;br /&gt;
‚ùå &lt;code translate=&quot;no&quot;&gt;self.driver.find_element(by=&quot;css selector&quot;, value=&quot;button&quot;).click()&lt;/code&gt;&lt;br /&gt;
(Reliable code is better than unreliable code.)&lt;/p&gt;

&lt;p&gt;üí° SeleniumBase lets you change the explicit timeout values of methods:&lt;br /&gt;
‚úÖ &lt;code translate=&quot;no&quot;&gt;self.click(&quot;button&quot;, timeout=10)&lt;/code&gt;&lt;br /&gt;
With raw Selenium, that requires more code:&lt;br /&gt;
‚ùå &lt;code translate=&quot;no&quot;&gt;WebDriverWait(driver, 10).until(EC.element_to_be_clickable(&quot;css selector&quot;, &quot;button&quot;)).click()&lt;/code&gt;&lt;br /&gt;
(Simple code is better than complex code.)&lt;/p&gt;

&lt;p&gt;üí° SeleniumBase gives you clean error output when a test fails. With raw Selenium, error messages can get very messy.&lt;/p&gt;

&lt;p&gt;üí° SeleniumBase gives you the option to generate a dashboard and reports for tests. It also saves screenshots from failing tests to the &lt;code translate=&quot;no&quot;&gt;./latest_logs/&lt;/code&gt; folder. Raw &lt;a href=&quot;https://www.selenium.dev/documentation/webdriver/&quot; translate=&quot;no&quot; target=&quot;_blank&quot;&gt;Selenium&lt;/a&gt; does not have these options out-of-the-box.&lt;/p&gt;

&lt;p&gt;üí° SeleniumBase includes desktop GUI apps for running tests, such as &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/commander.md&quot; translate=&quot;no&quot;&gt;SeleniumBase Commander&lt;/a&gt; for &lt;code translate=&quot;no&quot;&gt;pytest&lt;/code&gt; and &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/behave_bdd/ReadMe.md&quot; translate=&quot;no&quot;&gt;SeleniumBase Behave GUI&lt;/a&gt; for &lt;code translate=&quot;no&quot;&gt;behave&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;üí° SeleniumBase has its own &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/recorder_mode.md&quot;&gt;Recorder / Test Generator&lt;/a&gt; for creating tests from manual browser actions.&lt;/p&gt;

&lt;p&gt;üí° SeleniumBase comes with &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/case_plans.md&quot;&gt;test case management software, (&quot;CasePlans&quot;)&lt;/a&gt;, for organizing tests and step descriptions.&lt;/p&gt;

&lt;p&gt;üí° SeleniumBase includes tools for &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/chart_maker/ReadMe.md&quot;&gt;building data apps, (&quot;ChartMaker&quot;)&lt;/a&gt;, which can generate JavaScript from Python.&lt;/p&gt;

&lt;/div&gt;
&lt;/details&gt;

--------

&lt;p&gt;üìö &lt;b&gt;Learn about different ways of writing tests:&lt;/b&gt;&lt;/p&gt;

&lt;p align=&quot;left&quot;&gt;üìóüìù Here&#039;s &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/test_simple_login.py&quot;&gt;test_simple_login.py&lt;/a&gt;, which uses &lt;code translate=&quot;no&quot;&gt;&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/seleniumbase/fixtures/base_case.py&quot;&gt;BaseCase&lt;/a&gt;&lt;/code&gt; class inheritance, and runs with &lt;a href=&quot;https://docs.pytest.org/en/latest/how-to/usage.html&quot;&gt;pytest&lt;/a&gt; or &lt;a href=&quot;https://github.com/mdmintz/pynose&quot;&gt;pynose&lt;/a&gt;. (Use &lt;code translate=&quot;no&quot;&gt;self.driver&lt;/code&gt; to access Selenium&#039;s raw &lt;code translate=&quot;no&quot;&gt;driver&lt;/code&gt;.)&lt;/p&gt;

```python
from seleniumbase import BaseCase
BaseCase.main(__name__, __file__)

class TestSimpleLogin(BaseCase):
    def test_simple_login(self):
        self.open(&quot;seleniumbase.io/simple/login&quot;)
        self.type(&quot;#username&quot;, &quot;demo_user&quot;)
        self.type(&quot;#password&quot;, &quot;secret_pass&quot;)
        self.click(&#039;a:contains(&quot;Sign in&quot;)&#039;)
        self.assert_exact_text(&quot;Welcome!&quot;, &quot;h1&quot;)
        self.assert_element(&quot;img#image1&quot;)
        self.highlight(&quot;#image1&quot;)
        self.click_link(&quot;Sign out&quot;)
        self.assert_text(&quot;signed out&quot;, &quot;#top_message&quot;)
```

&lt;p align=&quot;left&quot;&gt;üìòüìù Here&#039;s &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/raw_login_sb.py&quot;&gt;raw_login_sb.py&lt;/a&gt;, which uses the &lt;b&gt;&lt;code translate=&quot;no&quot;&gt;SB&lt;/code&gt;&lt;/b&gt; Context Manager. Runs with pure &lt;code translate=&quot;no&quot;&gt;python&lt;/code&gt;. (Use &lt;code translate=&quot;no&quot;&gt;sb.driver&lt;/code&gt; to access Selenium&#039;s raw &lt;code translate=&quot;no&quot;&gt;driver&lt;/code&gt;.)&lt;/p&gt;

```python
from seleniumbase import SB

with SB() as sb:
    sb.open(&quot;seleniumbase.io/simple/login&quot;)
    sb.type(&quot;#username&quot;, &quot;demo_user&quot;)
    sb.type(&quot;#password&quot;, &quot;secret_pass&quot;)
    sb.click(&#039;a:contains(&quot;Sign in&quot;)&#039;)
    sb.assert_exact_text(&quot;Welcome!&quot;, &quot;h1&quot;)
    sb.assert_element(&quot;img#image1&quot;)
    sb.highlight(&quot;#image1&quot;)
    sb.click_link(&quot;Sign out&quot;)
    sb.assert_text(&quot;signed out&quot;, &quot;#top_message&quot;)
```

&lt;p align=&quot;left&quot;&gt;üìôüìù Here&#039;s &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/raw_login_driver.py&quot;&gt;raw_login_driver.py&lt;/a&gt;, which uses the &lt;b&gt;&lt;code translate=&quot;no&quot;&gt;Driver&lt;/code&gt;&lt;/b&gt; Manager. Runs with pure &lt;code translate=&quot;no&quot;&gt;python&lt;/code&gt;. (The &lt;code&gt;driver&lt;/code&gt; is an improved version of Selenium&#039;s raw &lt;code translate=&quot;no&quot;&gt;driver&lt;/code&gt;, with more methods.)&lt;/p&gt;

```python
from seleniumbase import Driver

driver = Driver()
try:
    driver.open(&quot;seleniumbase.io/simple/login&quot;)
    driver.type(&quot;#username&quot;, &quot;demo_user&quot;)
    driver.type(&quot;#password&quot;, &quot;secret_pass&quot;)
    driver.click(&#039;a:contains(&quot;Sign in&quot;)&#039;)
    driver.assert_exact_text(&quot;Welcome!&quot;, &quot;h1&quot;)
    driver.assert_element(&quot;img#image1&quot;)
    driver.highlight(&quot;#image1&quot;)
    driver.click_link(&quot;Sign out&quot;)
    driver.assert_text(&quot;signed out&quot;, &quot;#top_message&quot;)
finally:
    driver.quit()
```

--------

&lt;a id=&quot;python_installation&quot;&gt;&lt;/a&gt;
&lt;h2&gt;&lt;img src=&quot;https://seleniumbase.github.io/cdn/img/python_logo.png&quot; title=&quot;SeleniumBase&quot; width=&quot;42&quot; /&gt; Set up Python &amp; Git:&lt;/h2&gt;

&lt;a href=&quot;https://www.python.org/downloads/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/pyversions/seleniumbase.svg?color=FACE42&quot; title=&quot;Supported Python Versions&quot; /&gt;&lt;/a&gt;

üîµ Add &lt;b&gt;&lt;a href=&quot;https://www.python.org/downloads/&quot;&gt;Python&lt;/a&gt;&lt;/b&gt; and &lt;b&gt;&lt;a href=&quot;https://git-scm.com/&quot;&gt;Git&lt;/a&gt;&lt;/b&gt; to your System PATH.

üîµ Using a &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/virtualenv_instructions.md&quot;&gt;Python virtual env&lt;/a&gt; is recommended.

&lt;a id=&quot;install_seleniumbase&quot;&gt;&lt;/a&gt;
&lt;h2&gt;&lt;img src=&quot;https://seleniumbase.github.io/img/logo7.png&quot; title=&quot;SeleniumBase&quot; width=&quot;32&quot; /&gt; Install SeleniumBase:&lt;/h2&gt;

**You can install ``seleniumbase`` from [PyPI](https://pypi.org/project/seleniumbase/) or [GitHub](https://github.com/seleniumbase/SeleniumBase):**

üîµ **How to install ``seleniumbase`` from PyPI:**

```bash
pip install seleniumbase
```

* (Add ``--upgrade`` OR ``-U`` to upgrade SeleniumBase.)
* (Add ``--force-reinstall`` to upgrade indirect packages.)
* (Use ``pip3`` if multiple versions of Python are present.)

üîµ **How to install ``seleniumbase`` from a GitHub clone:**

```bash
git clone https://github.com/seleniumbase/SeleniumBase.git
cd SeleniumBase/
pip install -e .
```

üîµ **How to upgrade an existing install from a GitHub clone:**

```bash
git pull
pip install -e .
```

üîµ **Type ``seleniumbase`` or ``sbase`` to verify that SeleniumBase was installed successfully:**



... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[willccbb/verifiers]]></title>
            <link>https://github.com/willccbb/verifiers</link>
            <guid>https://github.com/willccbb/verifiers</guid>
            <pubDate>Wed, 28 May 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[Verifiers for LLM Reinforcement Learning]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/willccbb/verifiers">willccbb/verifiers</a></h1>
            <p>Verifiers for LLM Reinforcement Learning</p>
            <p>Language: Python</p>
            <p>Stars: 1,038</p>
            <p>Forks: 121</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre># Verifiers: Reinforcement Learning with LLMs in Verifiable Environments

This repository contains a set of tools for reinforcement learning with LLMs in verifiable environments.

**WARNING:** This repository in its current state should be viewed as **in-progress research code**, and is not guaranteed to yield stable or optimal training results. Best results will likely be found on reasonable timescales when using 7B+ models, and at least 8 GPUs.

**Note:** If you don&#039;t need multi-turn tool calling or agentic interactions, you should probably just use TRL (or Unsloth/Axolotl) for GRPO. This is mostly a multi-turn LLM RL repo with some other bells and whistles.


## Setup

PyPI [coming soon](https://pypi.org/project/verifiers/), for now just do:
```bash
git clone https://github.com/willccbb/verifiers.git
cd verifiers
uv sync
uv pip install flash-attn --no-build-isolation
source .venv/bin/activate
```
Ensure your `wandb` and `huggingface-cli` logins are set up (or set `report_to=None` in `training_args`).

If you encounter version issues, please confirm that you are able to run basic TRL training in your environment before opening an issue (see `verifiers/examples/trl_grpo.py` as a reference).

## Usage (Multi-GPU)

### Training with Multi-Turn GRPO

See `verifiers/examples/math_train.py` for an example with the ToolEnv environment + a Python tool.

To run on a 8-GPU node with 4 inference GPUs and 4 training GPUs:
```bash
# Launch vLLM inference server from verifiers/, with .venv active
CUDA_VISIBLE_DEVICES=0,1,2,3 python verifiers/inference/vllm_serve.py --model &quot;Qwen/Qwen2.5-7B-Instruct&quot; --tensor_parallel_size 4 --max_model_len 8192  --gpu_memory_utilization 0.9 --enable_prefix_caching True
```

```bash
# Run training script from verifiers/, with .venv active
CUDA_VISIBLE_DEVICES=4,5,6,7 accelerate launch --num-processes 4 --config-file configs/zero3.yaml verifiers/examples/math_train.py
```

Multi-node training setups are supported as well; you can specify the host IP + port of your inference as an argument in the `GRPOConfig` in your training script. See the TRL [docs](https://huggingface.co/docs/trl/main/en/grpo_trainer#trl.GRPOTrainer) for info on multi-node training via SLURM.

### Evaluation

You can also use environment classes to evaluate models with multi-turn tool use offline, i.e. without RL training. See `verifiers/examples/math_eval.py` for an example.

### Custom Environments

To create your own multi-turn environment, inherit from `MultiTurnEnv` and implement:
```python
def is_completed(self, messages: List[Dict[str, str]], **kwargs: Any) -&gt; bool:
    pass

def env_response(self, messages: List[Dict[str, str]], **kwargs: Any) -&gt; Dict[str, str]:
    pass
```

## Features
- [X] Environments (`MultiTurnEnv`): `DoubleCheckEnv`, `CodeEnv`, `ToolEnv`, `SmolaToolEnv`
- [X] Multi-turn tool use in `ToolEnv`, `SmolaToolEnv`, `CodeEnv`
- [X] Dataset formatting + XML parsers
- [X] Basic rubrics for math/code correctness + formatting
- [X] Defaults for GRPO, model, tokenizer, etc.

## Citation

If you use this code in your research, please cite:

```bibtex
@article{brown2025verifiers,
  title={Verifiers: Reinforcement Learning with LLMs in Verifiable Environments},
  author={Brown, William},
  year={2025}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hiyouga/LLaMA-Factory]]></title>
            <link>https://github.com/hiyouga/LLaMA-Factory</link>
            <guid>https://github.com/hiyouga/LLaMA-Factory</guid>
            <pubDate>Wed, 28 May 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hiyouga/LLaMA-Factory">hiyouga/LLaMA-Factory</a></h1>
            <p>Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)</p>
            <p>Language: Python</p>
            <p>Stars: 50,917</p>
            <p>Forks: 6,155</p>
            <p>Stars today: 257 stars today</p>
            <h2>README</h2><pre>![# LLaMA Factory](assets/logo.png)

[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social)](https://github.com/hiyouga/LLaMA-Factory/stargazers)
[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory)](https://github.com/hiyouga/LLaMA-Factory/commits/main)
[![GitHub contributors](https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange)](https://github.com/hiyouga/LLaMA-Factory/graphs/contributors)
[![GitHub workflow](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg)](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml)
[![PyPI](https://img.shields.io/pypi/v/llamafactory)](https://pypi.org/project/llamafactory/)
[![Citation](https://img.shields.io/badge/citation-476-green)](https://scholar.google.com/scholar?cites=12620864006390196564)
[![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)](https://github.com/hiyouga/LLaMA-Factory/pulls)

[![Twitter](https://img.shields.io/twitter/follow/llamafactory_ai)](https://twitter.com/llamafactory_ai)
[![Discord](https://dcbadge.vercel.app/api/server/rKfvV9r9FK?compact=true&amp;style=flat)](https://discord.gg/rKfvV9r9FK)
[![GitCode](https://gitcode.com/zhengyaowei/LLaMA-Factory/star/badge.svg)](https://gitcode.com/zhengyaowei/LLaMA-Factory)

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)
[![Open in DSW](https://gallery.pai-ml.com/assets/open-in-dsw.svg)](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory)
[![Spaces](https://img.shields.io/badge/ü§ó-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/hiyouga/LLaMA-Board)
[![Studios](https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue)](https://modelscope.cn/studios/hiyouga/LLaMA-Board)
[![SageMaker](https://img.shields.io/badge/SageMaker-Open%20in%20AWS-blue)](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/)

### Used by [Amazon](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/), [NVIDIA](https://developer.nvidia.com/rtx/ai-toolkit), [Aliyun](https://help.aliyun.com/zh/pai/use-cases/fine-tune-a-llama-3-model-with-llama-factory), etc.

&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;

### Supporters ‚ù§Ô∏è

&lt;a href=&quot;https://warp.dev/llama-factory&quot;&gt;
    &lt;img alt=&quot;Warp sponsorship&quot; width=&quot;400&quot; src=&quot;https://github.com/user-attachments/assets/ab8dd143-b0fd-4904-bdc5-dd7ecac94eae&quot;&gt;
&lt;/a&gt;

#### [Warp, the agentic terminal for developers](https://warp.dev/llama-factory)

[Available for MacOS, Linux, &amp; Windows](https://warp.dev/llama-factory)

----

### Easily fine-tune 100+ large language models with zero-code [CLI](#quickstart) and [Web UI](#fine-tuning-with-llama-board-gui-powered-by-gradio)

![GitHub Trend](https://trendshift.io/api/badge/repositories/4535)

&lt;/div&gt;

üëã Join our [WeChat](assets/wechat.jpg) or [NPU user group](assets/wechat_npu.jpg).

\[ English | [‰∏≠Êñá](README_zh.md) \]

**Fine-tuning a large language model can be easy as...**

https://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e

Choose your path:

- **Documentation**: https://llamafactory.readthedocs.io/en/latest/
- **Colab (free)**: https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing
- **Local machine**: Please refer to [usage](#getting-started)
- **PAI-DSW (free trial)**: https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory

&gt; [!NOTE]
&gt; Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.

## Table of Contents

- [Features](#features)
- [Blogs](#blogs)
- [Changelog](#changelog)
- [Supported Models](#supported-models)
- [Supported Training Approaches](#supported-training-approaches)
- [Provided Datasets](#provided-datasets)
- [Requirement](#requirement)
- [Getting Started](#getting-started)
  - [Installation](#installation)
  - [Data Preparation](#data-preparation)
  - [Quickstart](#quickstart)
  - [Fine-Tuning with LLaMA Board GUI](#fine-tuning-with-llama-board-gui-powered-by-gradio)
  - [Build Docker](#build-docker)
  - [Deploy with OpenAI-style API and vLLM](#deploy-with-openai-style-api-and-vllm)
  - [Download from ModelScope Hub](#download-from-modelscope-hub)
  - [Download from Modelers Hub](#download-from-modelers-hub)
  - [Use W&amp;B Logger](#use-wb-logger)
  - [Use SwanLab Logger](#use-swanlab-logger)
- [Projects using LLaMA Factory](#projects-using-llama-factory)
- [License](#license)
- [Citation](#citation)
- [Acknowledgement](#acknowledgement)

## Features

- **Various models**: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.
- **Integrated methods**: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.
- **Scalable resources**: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.
- **Advanced algorithms**: [GaLore](https://github.com/jiaweizzhao/GaLore), [BAdam](https://github.com/Ledzy/BAdam), [APOLLO](https://github.com/zhuhanqing/APOLLO), [Adam-mini](https://github.com/zyushun/Adam-mini), [Muon](https://github.com/KellerJordan/Muon), DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.
- **Practical tricks**: [FlashAttention-2](https://github.com/Dao-AILab/flash-attention), [Unsloth](https://github.com/unslothai/unsloth), [Liger Kernel](https://github.com/linkedin/Liger-Kernel), RoPE scaling, NEFTune and rsLoRA.
- **Wide tasks**: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.
- **Experiment monitors**: LlamaBoard, TensorBoard, Wandb, MLflow, [SwanLab](https://github.com/SwanHubX/SwanLab), etc.
- **Faster inference**: OpenAI-style API, Gradio UI and CLI with [vLLM worker](https://github.com/vllm-project/vllm) or [SGLang worker](https://github.com/sgl-project/sglang).

### Day-N Support for Fine-Tuning Cutting-Edge Models

| Support Date | Model Name                                                   |
| ------------ | ------------------------------------------------------------ |
| Day 0        | Qwen3 / Qwen2.5-VL / Gemma 3 / InternLM 3 / MiniCPM-o-2.6    |
| Day 1        | Llama 3 / GLM-4 / Mistral Small / PaliGemma2 / Llama 4       |

## Blogs

- [How Apoidea Group enhances visual information extraction from banking documents with multimodal models using LLaMA-Factory on Amazon SageMaker HyperPod](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/) (English)
- [Easy Dataset √ó LLaMA Factory: Enabling LLMs to Efficiently Learn Domain Knowledge](https://buaa-act.feishu.cn/wiki/GVzlwYcRFiR8OLkHbL6cQpYin7g) (English)
- [LLaMA Factory: Fine-tuning the DeepSeek-R1-Distill-Qwen-7B Model for News Classifier](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b) (Chinese)

&lt;details&gt;&lt;summary&gt;All Blogs&lt;/summary&gt;

- [A One-Stop Code-Free Model Fine-Tuning \&amp; Deployment Platform based on SageMaker and LLaMA-Factory](https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/) (Chinese)
- [LLaMA Factory Multi-Modal Fine-Tuning Practice: Fine-Tuning Qwen2-VL for Personal Tourist Guide](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl) (Chinese)
- [LLaMA Factory: Fine-tuning the LLaMA3 Model for Role-Playing](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory) (Chinese)

&lt;/details&gt;

## Changelog

[25/04/28] We supported fine-tuning the **[Qwen3](https://qwenlm.github.io/blog/qwen3/)** model family.

[25/04/21] We supported the **[Muon](https://github.com/KellerJordan/Muon)** optimizer. See [examples](examples/README.md) for usage. Thank [@tianshijing](https://github.com/tianshijing)&#039;s PR.

[25/04/16] We supported fine-tuning the **[InternVL3](https://huggingface.co/OpenGVLab/InternVL3-8B)** model. See [PR #7258](https://github.com/hiyouga/LLaMA-Factory/pull/7258) to get started.

[25/04/14] We supported fine-tuning the **[GLM-Z1](https://huggingface.co/THUDM/GLM-Z1-9B-0414)** and **[Kimi-VL](https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct)** models.

[25/04/06] We supported fine-tuning the **[Llama 4](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)** model. See [PR #7611](https://github.com/hiyouga/LLaMA-Factory/pull/7611) to get started.

&lt;details&gt;&lt;summary&gt;Full Changelog&lt;/summary&gt;

[25/03/31] We supported fine-tuning the **[Qwen2.5 Omni](https://qwenlm.github.io/blog/qwen2.5-omni/)** model. See [PR #7537](https://github.com/hiyouga/LLaMA-Factory/pull/7537) to get started.

[25/03/15] We supported **[SGLang](https://github.com/sgl-project/sglang)** as inference backend. Try `infer_backend: sglang` to accelerate inference.

[25/03/12] We supported fine-tuning the **[Gemma 3](https://huggingface.co/blog/gemma3)** model.

[25/02/24] Announcing **[EasyR1](https://github.com/hiyouga/EasyR1)**, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.

[25/02/11] We supported saving the **[Ollama](https://github.com/ollama/ollama)** modelfile when exporting the model checkpoints. See [examples](examples/README.md) for usage.

[25/02/05] We supported fine-tuning the **[Qwen2-Audio](Qwen/Qwen2-Audio-7B-Instruct)** and **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** on audio understanding tasks.

[25/01/31] We supported fine-tuning the **[DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)** and **[Qwen2.5-VL](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)** models.

[25/01/15] We supported **[APOLLO](https://arxiv.org/abs/2412.05270)** optimizer. See [examples](examples/README.md) for usage.

[25/01/14] We supported fine-tuning the **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** and **[MiniCPM-V-2.6](https://huggingface.co/openbmb/MiniCPM-V-2_6)** models. Thank [@BUAADreamer](https://github.com/BUAADreamer)&#039;s PR.

[25/01/14] We supported fine-tuning the **[InternLM 3](https://huggingface.co/collections/internlm/)** models. Thank [@hhaAndroid](https://github.com/hhaAndroid)&#039;s PR.

[25/01/10] We supported fine-tuning the **[Phi-4](https://huggingface.co/microsoft/phi-4)** model.

[24/12/21] We supported using **[SwanLab](https://github.com/SwanHubX/SwanLab)** for experiment tracking and visualization. See [this section](#use-swanlab-logger) for details.

[24/11/27] We supported fine-tuning the **[Skywork-o1](https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B)** model and the **[OpenO1](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)** dataset.

[24/10/09] We supported downloading pre-trained models and datasets from the **[Modelers Hub](https://modelers.cn/models)**. See [this tutorial](#download-from-modelers-hub) for usage.

[24/09/19] We supported fine-tuning the **[Qwen2.5](https://qwenlm.github.io/blog/qwen2.5/)** models.

[24/08/30] We supported fine-tuning the **[Qwen2-VL](https://qwenlm.github.io/blog/qwen2-vl/)** models. Thank [@simonJJJ](https://github.com/simonJJJ)&#039;s PR.

[24/08/27] We supported **[Liger Kernel](https://github.com/linkedin/Liger-Kernel)**. Try `enable_liger_kernel: true` for efficient training.

[24/08/09] We supported **[Adam-mini](https://github.com/zyushun/Adam-mini)** optimizer. See [examples](examples/README.md) for usage. Thank [@relic-yuexi](https://github.com/relic-yuexi)&#039;s PR.

[24/07/04] We supported [contamination-free packed training](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing). Use `neat_packing: true` to activate it. Thank [@chuan298](https://github.com/chuan298)&#039;s PR.

[24/06/16] We supported **[PiSSA](https://arxiv.org/abs/2404.02948)** algorithm. See [examples](examples/README.md) for usage.

[24/06/07] We supported fine-tuning the **[Qwen2](https://qwenlm.github.io/blog/qwen2/)** and **[GLM-4](https://github.com/THUDM/GLM-4)** models.

[24/05/26] We supported **[SimPO](https://arxiv.org/abs/2405.14734)** algorithm for preference learning. See [examples](examples/README.md) for usage.

[24/05/20] We supported fine-tuning the **PaliGemma** series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with `paligemma` template for chat completion.

[24/05/18] We supported **[KTO](https://arxiv.org/abs/2402.01306)** algorithm for preference learning. See [examples](examples/README.md) for usage.

[24/05/14] We supported training and inference on the Ascend NPU devices. Check [installation](#installation) section for details.

[24/04/26] We supported fine-tuning the **LLaVA-1.5** multimodal LLMs. See [examples](examples/README.md) for usage.

[24/04/22] We provided a **[Colab notebook](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)** for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check [Llama3-8B-Chinese-Chat](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat) and [Llama3-Chinese](https://huggingface.co/zhichen/Llama3-Chinese) for details.

[24/04/21] We supported **[Mixture-of-Depths](https://arxiv.org/abs/2404.02258)** according to [AstraMindAI&#039;s implementation](https://github.com/astramind-ai/Mixture-of-depths). See [examples](examples/README.md) for usage.

[24/04/16] We supported **[BAdam](https://arxiv.org/abs/2404.02827)** optimizer. See [examples](examples/README.md) for usage.

[24/04/16] We supported **[unsloth](https://github.com/unslothai/unsloth)**&#039;s long-sequence training (Llama-2-7B-56k within 24GB). It achieves **117%** speed and **50%** memory compared with FlashAttention-2, more benchmarks can be found in [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison).

[24/03/31] We supported **[ORPO](https://arxiv.org/abs/2403.07691)**. See [examples](examples/README.md) for usage.

[24/03/21] Our paper &quot;[LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models](https://arxiv.org/abs/2403.13372)&quot; is available at arXiv!

[24/03/20] We supported **FSDP+QLoRA** that fine-tunes a 70B model on 2x24GB GPUs. See [examples](examples/README.md) for usage.

[24/03/13] We supported **[LoRA+](https://arxiv.org/abs/2402.12354)**. See [examples](examples/README.md) for usage.

[24/03/07] We supported **[GaLore](https://arxiv.org/abs/2403.03507)** optimizer. See [examples](examples/README.md) for usage.

[24/03/07] We integrated **[vLLM](https://github.com/vllm-project/vllm)** for faster and concurrent inference. Try `infer_backend: vllm` to enjoy **270%** inference speed.

[24/02/28] We supported weight-decomposed LoRA (**[DoRA](https://arxiv.org/abs/2402.09353)**). Try `use_dora: true` to activate DoRA training.

[24/02/15] We supported **block expansion** proposed by [LLaMA Pro](https://github.com/TencentARC/LLaMA-Pro). See [examples](examples/README.md) for usage.

[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this [blog post](https://qwenlm.github.io/blog/qwen1.5/) for details.

[24/01/18] We supported **agent tuning** for most models, equipping model with tool using abilities by fine-tuning with `dataset: glaive_toolcall_en`.

[23/12/23] We supported **[unsloth](https://github.com/unslothai/unsloth)**&#039;s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try `use_unsloth: true` argument to activate unsloth patch. It achieves **170%** speed in our benchmark, check [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison) for details.

[23/12/12] We supported fine-tuning the latest MoE model **[Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)** in our framework. See hardware requirement [here](#hardware-requirement).

[23/12/01] We supported downloading pre-trained models and datasets from the **[ModelScope Hub](https://modelscope.cn/models)**. See [this tutorial](#download-from-modelscope-hub) for usage.

[23/10/21] We supported **[NEFTune](https://arxiv.org/abs/2310.05914)** trick for fine-tuning. Try `neftune_noise_alpha: 5` argument to activate NEFTune.

[23/09/27] We supported **$S^2$-Attn** proposed by [LongLoRA](https://github.com/dvlab-research/LongLoRA) for the LLaMA models. Try `shift_attn: true` argument to enable shift short attention.

[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See [examples](examples/README.md) for usage.

[23/09/10] We supported **[FlashAttention-2](https://github.com/Dao-AILab/flash-attention)**. Try `flash_attn: fa2` argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.

[23/08/12] We supported **RoPE scaling** to extend the context length of the LLaMA models. Try `rope_scaling: linear` argument in training and `rope_scaling: dynamic` argument at inference to extrapolate the position embeddings.

[23/08/11] We supported **[DPO training](https://arxiv.org/abs/2305.18290)** for instruction-tuned models. See [examples](examples/README.md) for usage.

[23/07/31] We supported **dataset streaming**. Try `streaming: true` and `max_steps: 10000` arguments to load your dataset in streaming mode.

[23/07/29] We released two instruction-tuned 13B models at Hugging Face. See these Hugging Face Repos ([LLaMA-2](https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat) / [Baichuan](https://huggingface.co/hiyouga/Baichuan-13B-sft)) for details.

[23/07/18] We developed an **all-in-one Web UI** for training, evaluation and inference. Try `train_web.py` to fine-tune models in your Web browser. Thank [@KanadeSiina](https://github.com/KanadeSiina) and [@codemayq](https://github.com/codemayq) for their efforts in the development.

[23/07/09] We released **[FastEdit](https://github.com/hiyouga/FastEdit)** ‚ö°ü©π, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow [FastEdit](https://github.com/hiyouga/FastEdit) if you are interested.

[23/06/29] We provided a **reproducible example** of training a chat model using instruction-following datasets, see [Baichuan-7B-sft](https://huggingface.co/hiyouga/Baichuan-7B-sft) for details.

[23/06/22] We aligned the [demo API](src/api_demo.py) with the [OpenAI&#039;s](https://platform.openai.com/docs/api-reference/chat) format where you can insert the fine-tuned model in **arbitrary ChatGPT-based applications**.

[23/06/03] We supported quantized training and inference (aka **[QLoRA](https://github.com/artidoro/qlora)**). See [examples](examples/README.md) for usage.

&lt;/details&gt;

&gt; [!TIP]
&gt; If you cannot use the latest feature, please pull the latest code and install LLaMA-Factory again.

## Supported Models

| Model                                                             | Model size                       | Template            |
| ----------------------------------------------------------------- | -------------------------------- | ------------------- |
| [Baichuan 2](https://huggingface.co/baichuan-inc)                 | 7B/13B                           | baichuan2           |
| [BLOOM/BLOOMZ](https://huggingface.co/bigscience)                 | 560M/1.1B/1.7B/3B/7.1B/176B      | -                   |
| [ChatGLM3](https://huggingface.co/THUDM)                          | 6B                               | chatglm3            |
| [Command R](https://huggingface.co/CohereForAI)                   | 35B/104B                         | cohere              |
| [DeepSeek (Code/

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>