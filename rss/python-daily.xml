<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Thu, 02 Oct 2025 00:04:16 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[harry0703/MoneyPrinterTurbo]]></title>
            <link>https://github.com/harry0703/MoneyPrinterTurbo</link>
            <guid>https://github.com/harry0703/MoneyPrinterTurbo</guid>
            <pubDate>Thu, 02 Oct 2025 00:04:16 GMT</pubDate>
            <description><![CDATA[åˆ©ç”¨AIå¤§æ¨¡å‹ï¼Œä¸€é”®ç”Ÿæˆé«˜æ¸…çŸ­è§†é¢‘ Generate short videos with one click using AI LLM.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/harry0703/MoneyPrinterTurbo">harry0703/MoneyPrinterTurbo</a></h1>
            <p>åˆ©ç”¨AIå¤§æ¨¡å‹ï¼Œä¸€é”®ç”Ÿæˆé«˜æ¸…çŸ­è§†é¢‘ Generate short videos with one click using AI LLM.</p>
            <p>Language: Python</p>
            <p>Stars: 44,882</p>
            <p>Forks: 6,302</p>
            <p>Stars today: 1,355 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;h1 align=&quot;center&quot;&gt;MoneyPrinterTurbo ğŸ’¸&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&quot; alt=&quot;Stargazers&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&quot; alt=&quot;Issues&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/network/members&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&quot; alt=&quot;Forks&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&quot; alt=&quot;License&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;ç®€ä½“ä¸­æ–‡ | &lt;a href=&quot;README-en.md&quot;&gt;English&lt;/a&gt;&lt;/h3&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/8731&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/8731&quot; alt=&quot;harry0703%2FMoneyPrinterTurbo | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;br&gt;
åªéœ€æä¾›ä¸€ä¸ªè§†é¢‘ &lt;b&gt;ä¸»é¢˜&lt;/b&gt; æˆ– &lt;b&gt;å…³é”®è¯&lt;/b&gt; ï¼Œå°±å¯ä»¥å…¨è‡ªåŠ¨ç”Ÿæˆè§†é¢‘æ–‡æ¡ˆã€è§†é¢‘ç´ æã€è§†é¢‘å­—å¹•ã€è§†é¢‘èƒŒæ™¯éŸ³ä¹ï¼Œç„¶ååˆæˆä¸€ä¸ªé«˜æ¸…çš„çŸ­è§†é¢‘ã€‚
&lt;br&gt;

&lt;h4&gt;Webç•Œé¢&lt;/h4&gt;

![](docs/webui.jpg)

&lt;h4&gt;APIç•Œé¢&lt;/h4&gt;

![](docs/api.jpg)

&lt;/div&gt;

## ç‰¹åˆ«æ„Ÿè°¢ ğŸ™

ç”±äºè¯¥é¡¹ç›®çš„ **éƒ¨ç½²** å’Œ **ä½¿ç”¨**ï¼Œå¯¹äºä¸€äº›å°ç™½ç”¨æˆ·æ¥è¯´ï¼Œè¿˜æ˜¯ **æœ‰ä¸€å®šçš„é—¨æ§›**ï¼Œåœ¨æ­¤ç‰¹åˆ«æ„Ÿè°¢
**å½•å’–ï¼ˆAIæ™ºèƒ½ å¤šåª’ä½“æœåŠ¡å¹³å°ï¼‰** ç½‘ç«™åŸºäºè¯¥é¡¹ç›®ï¼Œæä¾›çš„å…è´¹`AIè§†é¢‘ç”Ÿæˆå™¨`æœåŠ¡ï¼Œå¯ä»¥ä¸ç”¨éƒ¨ç½²ï¼Œç›´æ¥åœ¨çº¿ä½¿ç”¨ï¼Œéå¸¸æ–¹ä¾¿ã€‚

- ä¸­æ–‡ç‰ˆï¼šhttps://reccloud.cn
- è‹±æ–‡ç‰ˆï¼šhttps://reccloud.com

![](docs/reccloud.cn.jpg)

## æ„Ÿè°¢èµåŠ© ğŸ™

æ„Ÿè°¢ä½ç³– https://picwish.cn å¯¹è¯¥é¡¹ç›®çš„æ”¯æŒå’ŒèµåŠ©ï¼Œä½¿å¾—è¯¥é¡¹ç›®èƒ½å¤ŸæŒç»­çš„æ›´æ–°å’Œç»´æŠ¤ã€‚

ä½ç³–ä¸“æ³¨äº**å›¾åƒå¤„ç†é¢†åŸŸ**ï¼Œæä¾›ä¸°å¯Œçš„**å›¾åƒå¤„ç†å·¥å…·**ï¼Œå°†å¤æ‚æ“ä½œæè‡´ç®€åŒ–ï¼ŒçœŸæ­£å®ç°è®©å›¾åƒå¤„ç†æ›´ç®€å•ã€‚

![picwish.jpg](docs/picwish.jpg)

## åŠŸèƒ½ç‰¹æ€§ ğŸ¯

- [x] å®Œæ•´çš„ **MVCæ¶æ„**ï¼Œä»£ç  **ç»“æ„æ¸…æ™°**ï¼Œæ˜“äºç»´æŠ¤ï¼Œæ”¯æŒ `API` å’Œ `Webç•Œé¢`
- [x] æ”¯æŒè§†é¢‘æ–‡æ¡ˆ **AIè‡ªåŠ¨ç”Ÿæˆ**ï¼Œä¹Ÿå¯ä»¥**è‡ªå®šä¹‰æ–‡æ¡ˆ**
- [x] æ”¯æŒå¤šç§ **é«˜æ¸…è§†é¢‘** å°ºå¯¸
    - [x] ç«–å± 9:16ï¼Œ`1080x1920`
    - [x] æ¨ªå± 16:9ï¼Œ`1920x1080`
- [x] æ”¯æŒ **æ‰¹é‡è§†é¢‘ç”Ÿæˆ**ï¼Œå¯ä»¥ä¸€æ¬¡ç”Ÿæˆå¤šä¸ªè§†é¢‘ï¼Œç„¶åé€‰æ‹©ä¸€ä¸ªæœ€æ»¡æ„çš„
- [x] æ”¯æŒ **è§†é¢‘ç‰‡æ®µæ—¶é•¿** è®¾ç½®ï¼Œæ–¹ä¾¿è°ƒèŠ‚ç´ æåˆ‡æ¢é¢‘ç‡
- [x] æ”¯æŒ **ä¸­æ–‡** å’Œ **è‹±æ–‡** è§†é¢‘æ–‡æ¡ˆ
- [x] æ”¯æŒ **å¤šç§è¯­éŸ³** åˆæˆï¼Œå¯ **å®æ—¶è¯•å¬** æ•ˆæœ
- [x] æ”¯æŒ **å­—å¹•ç”Ÿæˆ**ï¼Œå¯ä»¥è°ƒæ•´ `å­—ä½“`ã€`ä½ç½®`ã€`é¢œè‰²`ã€`å¤§å°`ï¼ŒåŒæ—¶æ”¯æŒ`å­—å¹•æè¾¹`è®¾ç½®
- [x] æ”¯æŒ **èƒŒæ™¯éŸ³ä¹**ï¼Œéšæœºæˆ–è€…æŒ‡å®šéŸ³ä¹æ–‡ä»¶ï¼Œå¯è®¾ç½®`èƒŒæ™¯éŸ³ä¹éŸ³é‡`
- [x] è§†é¢‘ç´ ææ¥æº **é«˜æ¸…**ï¼Œè€Œä¸” **æ— ç‰ˆæƒ**ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨è‡ªå·±çš„ **æœ¬åœ°ç´ æ**
- [x] æ”¯æŒ **OpenAI**ã€**Moonshot**ã€**Azure**ã€**gpt4free**ã€**one-api**ã€**é€šä¹‰åƒé—®**ã€**Google Gemini**ã€**Ollama**ã€**DeepSeek**ã€ **æ–‡å¿ƒä¸€è¨€**, **Pollinations** ç­‰å¤šç§æ¨¡å‹æ¥å…¥
    - ä¸­å›½ç”¨æˆ·å»ºè®®ä½¿ç”¨ **DeepSeek** æˆ– **Moonshot** ä½œä¸ºå¤§æ¨¡å‹æä¾›å•†ï¼ˆå›½å†…å¯ç›´æ¥è®¿é—®ï¼Œä¸éœ€è¦VPNã€‚æ³¨å†Œå°±é€é¢åº¦ï¼ŒåŸºæœ¬å¤Ÿç”¨ï¼‰


### åæœŸè®¡åˆ’ ğŸ“…

- [ ] GPT-SoVITS é…éŸ³æ”¯æŒ
- [ ] ä¼˜åŒ–è¯­éŸ³åˆæˆï¼Œåˆ©ç”¨å¤§æ¨¡å‹ï¼Œä½¿å…¶åˆæˆçš„å£°éŸ³ï¼Œæ›´åŠ è‡ªç„¶ï¼Œæƒ…ç»ªæ›´åŠ ä¸°å¯Œ
- [ ] å¢åŠ è§†é¢‘è½¬åœºæ•ˆæœï¼Œä½¿å…¶çœ‹èµ·æ¥æ›´åŠ çš„æµç•…
- [ ] å¢åŠ æ›´å¤šè§†é¢‘ç´ ææ¥æºï¼Œä¼˜åŒ–è§†é¢‘ç´ æå’Œæ–‡æ¡ˆçš„åŒ¹é…åº¦
- [ ] å¢åŠ è§†é¢‘é•¿åº¦é€‰é¡¹ï¼šçŸ­ã€ä¸­ã€é•¿
- [ ] æ”¯æŒæ›´å¤šçš„è¯­éŸ³åˆæˆæœåŠ¡å•†ï¼Œæ¯”å¦‚ OpenAI TTS
- [ ] è‡ªåŠ¨ä¸Šä¼ åˆ°YouTubeå¹³å°

## è§†é¢‘æ¼”ç¤º ğŸ“º

### ç«–å± 9:16

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;â–¶ï¸&lt;/g-emoji&gt; ã€Šå¦‚ä½•å¢åŠ ç”Ÿæ´»çš„ä¹è¶£ã€‹&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;â–¶ï¸&lt;/g-emoji&gt; ã€Šé‡‘é’±çš„ä½œç”¨ã€‹&lt;br&gt;æ›´çœŸå®çš„åˆæˆå£°éŸ³&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;â–¶ï¸&lt;/g-emoji&gt; ã€Šç”Ÿå‘½çš„æ„ä¹‰æ˜¯ä»€ä¹ˆã€‹&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/a84d33d5-27a2-4aba-8fd0-9fb2bd91c6a6&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/af2f3b0b-002e-49fe-b161-18ba91c055e8&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/112c9564-d52b-4472-99ad-970b75f66476&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

### æ¨ªå± 16:9

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;â–¶ï¸&lt;/g-emoji&gt;ã€Šç”Ÿå‘½çš„æ„ä¹‰æ˜¯ä»€ä¹ˆã€‹&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;â–¶ï¸&lt;/g-emoji&gt;ã€Šä¸ºä»€ä¹ˆè¦è¿åŠ¨ã€‹&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/346ebb15-c55f-47a9-a653-114f08bb8073&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/271f2fae-8283-44a0-8aa0-0ed8f9a6fa87&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

## é…ç½®è¦æ±‚ ğŸ“¦

- å»ºè®®æœ€ä½ CPU **4æ ¸** æˆ–ä»¥ä¸Šï¼Œå†…å­˜ **4G** æˆ–ä»¥ä¸Šï¼Œæ˜¾å¡éå¿…é¡»
- Windows 10 æˆ– MacOS 11.0 ä»¥ä¸Šç³»ç»Ÿ


## å¿«é€Ÿå¼€å§‹ ğŸš€

### åœ¨ Google Colab ä¸­è¿è¡Œ
å…å»æœ¬åœ°ç¯å¢ƒé…ç½®ï¼Œç‚¹å‡»ç›´æ¥åœ¨ Google Colab ä¸­å¿«é€Ÿä½“éªŒ MoneyPrinterTurbo

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harry0703/MoneyPrinterTurbo/blob/main/docs/MoneyPrinterTurbo.ipynb)


### Windowsä¸€é”®å¯åŠ¨åŒ…

ä¸‹è½½ä¸€é”®å¯åŠ¨åŒ…ï¼Œè§£å‹ç›´æ¥ä½¿ç”¨ï¼ˆè·¯å¾„ä¸è¦æœ‰ **ä¸­æ–‡**ã€**ç‰¹æ®Šå­—ç¬¦**ã€**ç©ºæ ¼**ï¼‰

- ç™¾åº¦ç½‘ç›˜ï¼ˆv1.2.6ï¼‰: https://pan.baidu.com/s/1wg0UaIyXpO3SqIpaq790SQ?pwd=sbqx æå–ç : sbqx
- Google Drive (v1.2.6): https://drive.google.com/file/d/1HsbzfT7XunkrCrHw5ncUjFX8XX4zAuUh/view?usp=sharing

ä¸‹è½½åï¼Œå»ºè®®å…ˆ**åŒå‡»æ‰§è¡Œ** `update.bat` æ›´æ–°åˆ°**æœ€æ–°ä»£ç **ï¼Œç„¶ååŒå‡» `start.bat` å¯åŠ¨

å¯åŠ¨åï¼Œä¼šè‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼ˆå¦‚æœæ‰“å¼€æ˜¯ç©ºç™½ï¼Œå»ºè®®æ¢æˆ **Chrome** æˆ–è€… **Edge** æ‰“å¼€ï¼‰

## å®‰è£…éƒ¨ç½² ğŸ“¥

### å‰ææ¡ä»¶

- å°½é‡ä¸è¦ä½¿ç”¨ **ä¸­æ–‡è·¯å¾„**ï¼Œé¿å…å‡ºç°ä¸€äº›æ— æ³•é¢„æ–™çš„é—®é¢˜
- è¯·ç¡®ä¿ä½ çš„ **ç½‘ç»œ** æ˜¯æ­£å¸¸çš„ï¼ŒVPNéœ€è¦æ‰“å¼€`å…¨å±€æµé‡`æ¨¡å¼

#### â‘  å…‹éš†ä»£ç 

```shell
git clone https://github.com/harry0703/MoneyPrinterTurbo.git
```

#### â‘¡ ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œå»ºè®®å¯åŠ¨åä¹Ÿå¯ä»¥åœ¨ WebUI é‡Œé¢é…ç½®ï¼‰

- å°† `config.example.toml` æ–‡ä»¶å¤åˆ¶ä¸€ä»½ï¼Œå‘½åä¸º `config.toml`
- æŒ‰ç…§ `config.toml` æ–‡ä»¶ä¸­çš„è¯´æ˜ï¼Œé…ç½®å¥½ `pexels_api_keys` å’Œ `llm_provider`ï¼Œå¹¶æ ¹æ® llm_provider å¯¹åº”çš„æœåŠ¡å•†ï¼Œé…ç½®ç›¸å…³çš„
  API Key

### Dockeréƒ¨ç½² ğŸ³

#### â‘  å¯åŠ¨Docker

å¦‚æœæœªå®‰è£… Dockerï¼Œè¯·å…ˆå®‰è£… https://www.docker.com/products/docker-desktop/

å¦‚æœæ˜¯Windowsç³»ç»Ÿï¼Œè¯·å‚è€ƒå¾®è½¯çš„æ–‡æ¡£ï¼š

1. https://learn.microsoft.com/zh-cn/windows/wsl/install
2. https://learn.microsoft.com/zh-cn/windows/wsl/tutorials/wsl-containers

```shell
cd MoneyPrinterTurbo
docker-compose up
```

&gt; æ³¨æ„ï¼šæœ€æ–°ç‰ˆçš„dockerå®‰è£…æ—¶ä¼šè‡ªåŠ¨ä»¥æ’ä»¶çš„å½¢å¼å®‰è£…docker composeï¼Œå¯åŠ¨å‘½ä»¤è°ƒæ•´ä¸ºdocker compose up

#### â‘¡ è®¿é—®Webç•Œé¢

æ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—® http://0.0.0.0:8501

#### â‘¢ è®¿é—®APIæ–‡æ¡£

æ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—® http://0.0.0.0:8080/docs æˆ–è€… http://0.0.0.0:8080/redoc

### æ‰‹åŠ¨éƒ¨ç½² ğŸ“¦

&gt; è§†é¢‘æ•™ç¨‹

- å®Œæ•´çš„ä½¿ç”¨æ¼”ç¤ºï¼šhttps://v.douyin.com/iFhnwsKY/
- å¦‚ä½•åœ¨Windowsä¸Šéƒ¨ç½²ï¼šhttps://v.douyin.com/iFyjoW3M

#### â‘  åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

å»ºè®®ä½¿ç”¨ [conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html) åˆ›å»º python è™šæ‹Ÿç¯å¢ƒ

```shell
git clone https://github.com/harry0703/MoneyPrinterTurbo.git
cd MoneyPrinterTurbo
conda create -n MoneyPrinterTurbo python=3.11
conda activate MoneyPrinterTurbo
pip install -r requirements.txt
```

#### â‘¡ å®‰è£…å¥½ ImageMagick

- Windows:
    - ä¸‹è½½ https://imagemagick.org/script/download.php é€‰æ‹©Windowsç‰ˆæœ¬ï¼Œåˆ‡è®°ä¸€å®šè¦é€‰æ‹© **é™æ€åº“** ç‰ˆæœ¬ï¼Œæ¯”å¦‚
      ImageMagick-7.1.1-32-Q16-x64-**static**.exe
    - å®‰è£…ä¸‹è½½å¥½çš„ ImageMagickï¼Œ**æ³¨æ„ä¸è¦ä¿®æ”¹å®‰è£…è·¯å¾„**
    - ä¿®æ”¹ `é…ç½®æ–‡ä»¶ config.toml` ä¸­çš„ `imagemagick_path` ä¸ºä½ çš„ **å®é™…å®‰è£…è·¯å¾„**

- MacOS:
  ```shell
  brew install imagemagick
  ````
- Ubuntu
  ```shell
  sudo apt-get install imagemagick
  ```
- CentOS
  ```shell
  sudo yum install ImageMagick
  ```

#### â‘¢ å¯åŠ¨Webç•Œé¢ ğŸŒ

æ³¨æ„éœ€è¦åˆ° MoneyPrinterTurbo é¡¹ç›® `æ ¹ç›®å½•` ä¸‹æ‰§è¡Œä»¥ä¸‹å‘½ä»¤

###### Windows

```bat
webui.bat
```

###### MacOS or Linux

```shell
sh webui.sh
```

å¯åŠ¨åï¼Œä¼šè‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼ˆå¦‚æœæ‰“å¼€æ˜¯ç©ºç™½ï¼Œå»ºè®®æ¢æˆ **Chrome** æˆ–è€… **Edge** æ‰“å¼€ï¼‰

#### â‘£ å¯åŠ¨APIæœåŠ¡ ğŸš€

```shell
python main.py
```

å¯åŠ¨åï¼Œå¯ä»¥æŸ¥çœ‹ `APIæ–‡æ¡£` http://127.0.0.1:8080/docs æˆ–è€… http://127.0.0.1:8080/redoc ç›´æ¥åœ¨çº¿è°ƒè¯•æ¥å£ï¼Œå¿«é€Ÿä½“éªŒã€‚

## è¯­éŸ³åˆæˆ ğŸ—£

æ‰€æœ‰æ”¯æŒçš„å£°éŸ³åˆ—è¡¨ï¼Œå¯ä»¥æŸ¥çœ‹ï¼š[å£°éŸ³åˆ—è¡¨](./docs/voice-list.txt)

2024-04-16 v1.1.2 æ–°å¢äº†9ç§Azureçš„è¯­éŸ³åˆæˆå£°éŸ³ï¼Œéœ€è¦é…ç½®API KEYï¼Œè¯¥å£°éŸ³åˆæˆçš„æ›´åŠ çœŸå®ã€‚

## å­—å¹•ç”Ÿæˆ ğŸ“œ

å½“å‰æ”¯æŒ2ç§å­—å¹•ç”Ÿæˆæ–¹å¼ï¼š

- **edge**: ç”Ÿæˆ`é€Ÿåº¦å¿«`ï¼Œæ€§èƒ½æ›´å¥½ï¼Œå¯¹ç”µè„‘é…ç½®æ²¡æœ‰è¦æ±‚ï¼Œä½†æ˜¯è´¨é‡å¯èƒ½ä¸ç¨³å®š
- **whisper**: ç”Ÿæˆ`é€Ÿåº¦æ…¢`ï¼Œæ€§èƒ½è¾ƒå·®ï¼Œå¯¹ç”µè„‘é…ç½®æœ‰ä¸€å®šè¦æ±‚ï¼Œä½†æ˜¯`è´¨é‡æ›´å¯é `ã€‚

å¯ä»¥ä¿®æ”¹ `config.toml` é…ç½®æ–‡ä»¶ä¸­çš„ `subtitle_provider` è¿›è¡Œåˆ‡æ¢

å»ºè®®ä½¿ç”¨ `edge` æ¨¡å¼ï¼Œå¦‚æœç”Ÿæˆçš„å­—å¹•è´¨é‡ä¸å¥½ï¼Œå†åˆ‡æ¢åˆ° `whisper` æ¨¡å¼

&gt; æ³¨æ„ï¼š

1. whisper æ¨¡å¼ä¸‹éœ€è¦åˆ° HuggingFace ä¸‹è½½ä¸€ä¸ªæ¨¡å‹æ–‡ä»¶ï¼Œå¤§çº¦ 3GB å·¦å³ï¼Œè¯·ç¡®ä¿ç½‘ç»œé€šç•…
2. å¦‚æœç•™ç©ºï¼Œè¡¨ç¤ºä¸ç”Ÿæˆå­—å¹•ã€‚

&gt; ç”±äºå›½å†…æ— æ³•è®¿é—® HuggingFaceï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ä¸‹è½½ `whisper-large-v3` çš„æ¨¡å‹æ–‡ä»¶

ä¸‹è½½åœ°å€ï¼š

- ç™¾åº¦ç½‘ç›˜: https://pan.baidu.com/s/11h3Q6tsDtjQKTjUu3sc5cA?pwd=xjs9
- å¤¸å…‹ç½‘ç›˜ï¼šhttps://pan.quark.cn/s/3ee3d991d64b

æ¨¡å‹ä¸‹è½½åè§£å‹ï¼Œæ•´ä¸ªç›®å½•æ”¾åˆ° `.\MoneyPrinterTurbo\models` é‡Œé¢ï¼Œ
æœ€ç»ˆçš„æ–‡ä»¶è·¯å¾„åº”è¯¥æ˜¯è¿™æ ·: `.\MoneyPrinterTurbo\models\whisper-large-v3`

```
MoneyPrinterTurbo  
  â”œâ”€models
  â”‚   â””â”€whisper-large-v3
  â”‚          config.json
  â”‚          model.bin
  â”‚          preprocessor_config.json
  â”‚          tokenizer.json
  â”‚          vocabulary.json
```

## èƒŒæ™¯éŸ³ä¹ ğŸµ

ç”¨äºè§†é¢‘çš„èƒŒæ™¯éŸ³ä¹ï¼Œä½äºé¡¹ç›®çš„ `resource/songs` ç›®å½•ä¸‹ã€‚
&gt; å½“å‰é¡¹ç›®é‡Œé¢æ”¾äº†ä¸€äº›é»˜è®¤çš„éŸ³ä¹ï¼Œæ¥è‡ªäº YouTube è§†é¢‘ï¼Œå¦‚æœ‰ä¾µæƒï¼Œè¯·åˆ é™¤ã€‚

## å­—å¹•å­—ä½“ ğŸ…°

ç”¨äºè§†é¢‘å­—å¹•çš„æ¸²æŸ“ï¼Œä½äºé¡¹ç›®çš„ `resource/fonts` ç›®å½•ä¸‹ï¼Œä½ ä¹Ÿå¯ä»¥æ”¾è¿›å»è‡ªå·±çš„å­—ä½“ã€‚

## å¸¸è§é—®é¢˜ ğŸ¤”

### â“RuntimeError: No ffmpeg exe could be found

é€šå¸¸æƒ…å†µä¸‹ï¼Œffmpeg ä¼šè¢«è‡ªåŠ¨ä¸‹è½½ï¼Œå¹¶ä¸”ä¼šè¢«è‡ªåŠ¨æ£€æµ‹åˆ°ã€‚
ä½†æ˜¯å¦‚æœä½ çš„ç¯å¢ƒæœ‰é—®é¢˜ï¼Œæ— æ³•è‡ªåŠ¨ä¸‹è½½ï¼Œå¯èƒ½ä¼šé‡åˆ°å¦‚ä¸‹é”™è¯¯ï¼š

```
RuntimeError: No ffmpeg exe could be found.
Install ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable.
```

æ­¤æ—¶ä½ å¯ä»¥ä» https://www.gyan.dev/ffmpeg/builds/ ä¸‹è½½ffmpegï¼Œè§£å‹åï¼Œè®¾ç½® `ffmpeg_path` ä¸ºä½ çš„å®é™…å®‰è£…è·¯å¾„å³å¯ã€‚

```toml
[app]
# è¯·æ ¹æ®ä½ çš„å®é™…è·¯å¾„è®¾ç½®ï¼Œæ³¨æ„ Windows è·¯å¾„åˆ†éš”ç¬¦ä¸º \\
ffmpeg_path = &quot;C:\\Users\\harry\\Downloads\\ffmpeg.exe&quot;
```

### â“ImageMagickçš„å®‰å…¨ç­–ç•¥é˜»æ­¢äº†ä¸ä¸´æ—¶æ–‡ä»¶@/tmp/tmpur5hyyto.txtç›¸å…³çš„æ“ä½œ

å¯ä»¥åœ¨ImageMagickçš„é…ç½®æ–‡ä»¶policy.xmlä¸­æ‰¾åˆ°è¿™äº›ç­–ç•¥ã€‚
è¿™ä¸ªæ–‡ä»¶é€šå¸¸ä½äº /etc/ImageMagick-`X`/ æˆ– ImageMagick å®‰è£…ç›®å½•çš„ç±»ä¼¼ä½ç½®ã€‚
ä¿®æ”¹åŒ…å«`pattern=&quot;@&quot;`çš„æ¡ç›®ï¼Œå°†`rights=&quot;none&quot;`æ›´æ”¹ä¸º`rights=&quot;read|write&quot;`ä»¥å…è®¸å¯¹æ–‡ä»¶çš„è¯»å†™æ“ä½œã€‚

### â“OSError: [Errno 24] Too many open files

è¿™ä¸ªé—®é¢˜æ˜¯ç”±äºç³»ç»Ÿæ‰“å¼€æ–‡ä»¶æ•°é™åˆ¶å¯¼è‡´çš„ï¼Œå¯ä»¥é€šè¿‡ä¿®æ”¹ç³»ç»Ÿçš„æ–‡ä»¶æ‰“å¼€æ•°é™åˆ¶æ¥è§£å†³ã€‚

æŸ¥çœ‹å½“å‰é™åˆ¶

```shell
ulimit -n
```

å¦‚æœè¿‡ä½ï¼Œå¯ä»¥è°ƒé«˜ä¸€äº›ï¼Œæ¯”å¦‚

```shell
ulimit -n 10240
```

### â“Whisper æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œå‡ºç°å¦‚ä¸‹é”™è¯¯

LocalEntryNotfoundEror: Cannot find an appropriate cached snapshotfolderfor the specified revision on the local disk and
outgoing trafic has been disabled.
To enablerepo look-ups and downloads online, pass &#039;local files only=False&#039; as input.

æˆ–è€…

An error occured while synchronizing the model Systran/faster-whisper-large-v3 from the Hugging Face Hub:
An error happened while trying to locate the files on the Hub and we cannot find the appropriate snapshot folder for the
specified revision on the local disk. Please check your internet connection and try again.
Trying to load the model directly from the local cache, if it exists.

è§£å†³æ–¹æ³•ï¼š[ç‚¹å‡»æŸ¥çœ‹å¦‚ä½•ä»ç½‘ç›˜æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹](#%E5%AD%97%E5%B9%95%E7%94%9F%E6%88%90-)

## åé¦ˆå»ºè®® ğŸ“¢

- å¯ä»¥æäº¤ [issue](https://github.com/harry0703/MoneyPrinterTurbo/issues)
  æˆ–è€… [pull request](https://github.com/harry0703/MoneyPrinterTurbo/pulls)ã€‚

## è®¸å¯è¯ ğŸ“

ç‚¹å‡»æŸ¥çœ‹ [`LICENSE`](LICENSE) æ–‡ä»¶

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=harry0703/MoneyPrinterTurbo&amp;type=Date)](https://star-history.com/#harry0703/MoneyPrinterTurbo&amp;Date)</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[anthropics/claude-agent-sdk-python]]></title>
            <link>https://github.com/anthropics/claude-agent-sdk-python</link>
            <guid>https://github.com/anthropics/claude-agent-sdk-python</guid>
            <pubDate>Thu, 02 Oct 2025 00:04:15 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/anthropics/claude-agent-sdk-python">anthropics/claude-agent-sdk-python</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 1,801</p>
            <p>Forks: 235</p>
            <p>Stars today: 298 stars today</p>
            <h2>README</h2><pre># Claude Agent SDK for Python

Python SDK for Claude Agent. See the [Claude Agent SDK documentation](https://docs.anthropic.com/en/docs/claude-code/sdk/sdk-python) for more information.

## Installation

```bash
pip install claude-agent-sdk
```

**Prerequisites:**
- Python 3.10+
- Node.js
- Claude Code: `npm install -g @anthropic-ai/claude-code`

## Quick Start

```python
import anyio
from claude_agent_sdk import query

async def main():
    async for message in query(prompt=&quot;What is 2 + 2?&quot;):
        print(message)

anyio.run(main)
```

## Basic Usage: query()

`query()` is an async function for querying Claude Code. It returns an `AsyncIterator` of response messages. See [src/claude_agent_sdk/query.py](src/claude_agent_sdk/query.py).

```python
from claude_agent_sdk import query, ClaudeAgentOptions, AssistantMessage, TextBlock

# Simple query
async for message in query(prompt=&quot;Hello Claude&quot;):
    if isinstance(message, AssistantMessage):
        for block in message.content:
            if isinstance(block, TextBlock):
                print(block.text)

# With options
options = ClaudeAgentOptions(
    system_prompt=&quot;You are a helpful assistant&quot;,
    max_turns=1
)

async for message in query(prompt=&quot;Tell me a joke&quot;, options=options):
    print(message)
```

### Using Tools

```python
options = ClaudeAgentOptions(
    allowed_tools=[&quot;Read&quot;, &quot;Write&quot;, &quot;Bash&quot;],
    permission_mode=&#039;acceptEdits&#039;  # auto-accept file edits
)

async for message in query(
    prompt=&quot;Create a hello.py file&quot;,
    options=options
):
    # Process tool use and results
    pass
```

### Working Directory

```python
from pathlib import Path

options = ClaudeAgentOptions(
    cwd=&quot;/path/to/project&quot;  # or Path(&quot;/path/to/project&quot;)
)
```

## ClaudeSDKClient

`ClaudeSDKClient` supports bidirectional, interactive conversations with Claude
Code. See [src/claude_agent_sdk/client.py](src/claude_agent_sdk/client.py).

Unlike `query()`, `ClaudeSDKClient` additionally enables **custom tools** and **hooks**, both of which can be defined as Python functions.

### Custom Tools (as In-Process SDK MCP Servers)

A **custom tool** is a Python function that you can offer to Claude, for Claude to invoke as needed.

Custom tools are implemented in-process MCP servers that run directly within your Python application, eliminating the need for separate processes that regular MCP servers require.

For an end-to-end example, see [MCP Calculator](examples/mcp_calculator.py).

#### Creating a Simple Tool

```python
from claude_agent_sdk import tool, create_sdk_mcp_server, ClaudeAgentOptions, ClaudeSDKClient

# Define a tool using the @tool decorator
@tool(&quot;greet&quot;, &quot;Greet a user&quot;, {&quot;name&quot;: str})
async def greet_user(args):
    return {
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: f&quot;Hello, {args[&#039;name&#039;]}!&quot;}
        ]
    }

# Create an SDK MCP server
server = create_sdk_mcp_server(
    name=&quot;my-tools&quot;,
    version=&quot;1.0.0&quot;,
    tools=[greet_user]
)

# Use it with Claude
options = ClaudeAgentOptions(
    mcp_servers={&quot;tools&quot;: server},
    allowed_tools=[&quot;mcp__tools__greet&quot;]
)

async with ClaudeSDKClient(options=options) as client:
    await client.query(&quot;Greet Alice&quot;)

    # Extract and print response
    async for msg in client.receive_response():
        print(msg)
```

#### Benefits Over External MCP Servers

- **No subprocess management** - Runs in the same process as your application
- **Better performance** - No IPC overhead for tool calls
- **Simpler deployment** - Single Python process instead of multiple
- **Easier debugging** - All code runs in the same process
- **Type safety** - Direct Python function calls with type hints

#### Migration from External Servers

```python
# BEFORE: External MCP server (separate process)
options = ClaudeAgentOptions(
    mcp_servers={
        &quot;calculator&quot;: {
            &quot;type&quot;: &quot;stdio&quot;,
            &quot;command&quot;: &quot;python&quot;,
            &quot;args&quot;: [&quot;-m&quot;, &quot;calculator_server&quot;]
        }
    }
)

# AFTER: SDK MCP server (in-process)
from my_tools import add, subtract  # Your tool functions

calculator = create_sdk_mcp_server(
    name=&quot;calculator&quot;,
    tools=[add, subtract]
)

options = ClaudeAgentOptions(
    mcp_servers={&quot;calculator&quot;: calculator}
)
```

#### Mixed Server Support

You can use both SDK and external MCP servers together:

```python
options = ClaudeAgentOptions(
    mcp_servers={
        &quot;internal&quot;: sdk_server,      # In-process SDK server
        &quot;external&quot;: {                # External subprocess server
            &quot;type&quot;: &quot;stdio&quot;,
            &quot;command&quot;: &quot;external-server&quot;
        }
    }
)
```

### Hooks

A **hook** is a Python function that the Claude Code *application* (*not* Claude) invokes at specific points of the Claude agent loop. Hooks can provide deterministic processing and automated feedback for Claude. Read more in [Claude Code Hooks Reference](https://docs.anthropic.com/en/docs/claude-code/hooks).

For more examples, see examples/hooks.py.

#### Example

```python
from claude_agent_sdk import ClaudeAgentOptions, ClaudeSDKClient, HookMatcher

async def check_bash_command(input_data, tool_use_id, context):
    tool_name = input_data[&quot;tool_name&quot;]
    tool_input = input_data[&quot;tool_input&quot;]
    if tool_name != &quot;Bash&quot;:
        return {}
    command = tool_input.get(&quot;command&quot;, &quot;&quot;)
    block_patterns = [&quot;foo.sh&quot;]
    for pattern in block_patterns:
        if pattern in command:
            return {
                &quot;hookSpecificOutput&quot;: {
                    &quot;hookEventName&quot;: &quot;PreToolUse&quot;,
                    &quot;permissionDecision&quot;: &quot;deny&quot;,
                    &quot;permissionDecisionReason&quot;: f&quot;Command contains invalid pattern: {pattern}&quot;,
                }
            }
    return {}

options = ClaudeAgentOptions(
    allowed_tools=[&quot;Bash&quot;],
    hooks={
        &quot;PreToolUse&quot;: [
            HookMatcher(matcher=&quot;Bash&quot;, hooks=[check_bash_command]),
        ],
    }
)

async with ClaudeSDKClient(options=options) as client:
    # Test 1: Command with forbidden pattern (will be blocked)
    await client.query(&quot;Run the bash command: ./foo.sh --help&quot;)
    async for msg in client.receive_response():
        print(msg)

    print(&quot;\n&quot; + &quot;=&quot; * 50 + &quot;\n&quot;)

    # Test 2: Safe command that should work
    await client.query(&quot;Run the bash command: echo &#039;Hello from hooks example!&#039;&quot;)
    async for msg in client.receive_response():
        print(msg)
```


## Types

See [src/claude_agent_sdk/types.py](src/claude_agent_sdk/types.py) for complete type definitions:
- `ClaudeAgentOptions` - Configuration options
- `AssistantMessage`, `UserMessage`, `SystemMessage`, `ResultMessage` - Message types
- `TextBlock`, `ToolUseBlock`, `ToolResultBlock` - Content blocks

## Error Handling

```python
from claude_agent_sdk import (
    ClaudeSDKError,      # Base error
    CLINotFoundError,    # Claude Code not installed
    CLIConnectionError,  # Connection issues
    ProcessError,        # Process failed
    CLIJSONDecodeError,  # JSON parsing issues
)

try:
    async for message in query(prompt=&quot;Hello&quot;):
        pass
except CLINotFoundError:
    print(&quot;Please install Claude Code&quot;)
except ProcessError as e:
    print(f&quot;Process failed with exit code: {e.exit_code}&quot;)
except CLIJSONDecodeError as e:
    print(f&quot;Failed to parse response: {e}&quot;)
```

See [src/claude_agent_sdk/_errors.py](src/claude_agent_sdk/_errors.py) for all error types.

## Available Tools

See the [Claude Code documentation](https://docs.anthropic.com/en/docs/claude-code/settings#tools-available-to-claude) for a complete list of available tools.

## Examples

See [examples/quick_start.py](examples/quick_start.py) for a complete working example.

See [examples/streaming_mode.py](examples/streaming_mode.py) for comprehensive examples involving `ClaudeSDKClient`. You can even run interactive examples in IPython from [examples/streaming_mode_ipython.py](examples/streaming_mode_ipython.py).

## Migrating from Claude Code SDK

If you&#039;re upgrading from the Claude Code SDK (versions &lt; 0.1.0), please see the [CHANGELOG.md](CHANGELOG.md#010) for details on breaking changes and new features, including:

- `ClaudeCodeOptions` â†’ `ClaudeAgentOptions` rename
- Merged system prompt configuration
- Settings isolation and explicit control
- New programmatic subagents and session forking features

## License

MIT
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[lukas-blecher/LaTeX-OCR]]></title>
            <link>https://github.com/lukas-blecher/LaTeX-OCR</link>
            <guid>https://github.com/lukas-blecher/LaTeX-OCR</guid>
            <pubDate>Thu, 02 Oct 2025 00:04:14 GMT</pubDate>
            <description><![CDATA[pix2tex: Using a ViT to convert images of equations into LaTeX code.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lukas-blecher/LaTeX-OCR">lukas-blecher/LaTeX-OCR</a></h1>
            <p>pix2tex: Using a ViT to convert images of equations into LaTeX code.</p>
            <p>Language: Python</p>
            <p>Stars: 15,476</p>
            <p>Forks: 1,243</p>
            <p>Stars today: 34 stars today</p>
            <h2>README</h2><pre># pix2tex - LaTeX OCR

[![GitHub](https://img.shields.io/github/license/lukas-blecher/LaTeX-OCR)](https://github.com/lukas-blecher/LaTeX-OCR) [![Documentation Status](https://readthedocs.org/projects/pix2tex/badge/?version=latest)](https://pix2tex.readthedocs.io/en/latest/?badge=latest) [![PyPI](https://img.shields.io/pypi/v/pix2tex?logo=pypi)](https://pypi.org/project/pix2tex) [![PyPI - Downloads](https://img.shields.io/pypi/dm/pix2tex?logo=pypi)](https://pypi.org/project/pix2tex) [![GitHub all releases](https://img.shields.io/github/downloads/lukas-blecher/LaTeX-OCR/total?color=blue&amp;logo=github)](https://github.com/lukas-blecher/LaTeX-OCR/releases) [![Docker Pulls](https://img.shields.io/docker/pulls/lukasblecher/pix2tex?logo=docker)](https://hub.docker.com/r/lukasblecher/pix2tex) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lukas-blecher/LaTeX-OCR/blob/main/notebooks/LaTeX_OCR_test.ipynb) [![Hugging Face Spaces](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/lukbl/LaTeX-OCR)

The goal of this project is to create a learning based system that takes an image of a math formula and returns corresponding LaTeX code. 

![header](https://user-images.githubusercontent.com/55287601/109183599-69431f00-778e-11eb-9809-d42b9451e018.png)

## Using the model
To run the model you need Python 3.7+

If you don&#039;t have PyTorch installed. Follow their instructions [here](https://pytorch.org/get-started/locally/).

Install the package `pix2tex`: 

```
pip install &quot;pix2tex[gui]&quot;
```

Model checkpoints will be downloaded automatically.

There are three ways to get a prediction from an image. 
1. You can use the command line tool by calling `pix2tex`. Here you can parse already existing images from the disk and images in your clipboard.

2. Thanks to [@katie-lim](https://github.com/katie-lim), you can use a nice user interface as a quick way to get the model prediction. Just call the GUI with `latexocr`. From here you can take a screenshot and the predicted latex code is rendered using [MathJax](https://www.mathjax.org/) and copied to your clipboard.

    Under linux, it is possible to use the GUI with `gnome-screenshot` (which comes with multiple monitor support). For other Wayland compositers, `grim` and `slurp` will be used for wlroots-based Wayland compositers and `spectacle` for KDE Plasma. Note that `gnome-screenshot` is not compatible with wlroots or Qt based compositers. Since `gnome-screenshot` will be preferred when available, you may have to set the environment variable `SCREENSHOT_TOOL` to `grim` or `spectacle` in these cases (other available values are `gnome-screenshot` and `pil`).

    ![demo](https://user-images.githubusercontent.com/55287601/117812740-77b7b780-b262-11eb-81f6-fc19766ae2ae.gif)

    If the model is unsure about the what&#039;s in the image it might output a different prediction every time you click &quot;Retry&quot;. With the `temperature` parameter you can control this behavior (low temperature will produce the same result).

3. You can use an API. This has additional dependencies. Install via `pip install -U &quot;pix2tex[api]&quot;` and run
    ```bash
    python -m pix2tex.api.run
    ```
    to start a [Streamlit](https://streamlit.io/) demo that connects to the API at port 8502. There is also a docker image  available for the API: https://hub.docker.com/r/lukasblecher/pix2tex [![Docker Image Size (latest by date)](https://img.shields.io/docker/image-size/lukasblecher/pix2tex?logo=docker)](https://hub.docker.com/r/lukasblecher/pix2tex)

    ```
    docker pull lukasblecher/pix2tex:api
    docker run --rm -p 8502:8502 lukasblecher/pix2tex:api
    ```
    To also run the streamlit demo run
    ```
    docker run --rm -it -p 8501:8501 --entrypoint python lukasblecher/pix2tex:api pix2tex/api/run.py
    ```
    and navigate to http://localhost:8501/

4. Use from within Python
    ```python
    from PIL import Image
    from pix2tex.cli import LatexOCR
    
    img = Image.open(&#039;path/to/image.png&#039;)
    model = LatexOCR()
    print(model(img))
    ```

The model works best with images of smaller resolution. That&#039;s why I added a preprocessing step where another neural network predicts the optimal resolution of the input image. This model will automatically resize the custom image to best resemble the training data and thus increase performance of images found in the wild. Still it&#039;s not perfect and might not be able to handle huge images optimally, so don&#039;t zoom in all the way before taking a picture. 

Always double check the result carefully. You can try to redo the prediction with an other resolution if the answer was wrong.

**Want to use the package?**

I&#039;m trying to compile a documentation right now. 

Visit here: https://pix2tex.readthedocs.io/ 


## Training the model [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lukas-blecher/LaTeX-OCR/blob/main/notebooks/LaTeX_OCR_training.ipynb)

Install a couple of dependencies `pip install &quot;pix2tex[train]&quot;`.
1. First we need to combine the images with their ground truth labels. I wrote a dataset class (which needs further improving) that saves the relative paths to the images with the LaTeX code they were rendered with. To generate the dataset pickle file run 

```
python -m pix2tex.dataset.dataset --equations path_to_textfile --images path_to_images --out dataset.pkl
```
To use your own tokenizer pass it via `--tokenizer` (See below).

You can find my generated training data on the [Google Drive](https://drive.google.com/drive/folders/13CA4vAmOmD_I_dSbvLp-Lf0s6KiaNfuO) as well (formulae.zip - images, math.txt - labels). Repeat the step for the validation and test data. All use the same label text file.

2. Edit the `data` (and `valdata`) entry in the config file to the newly generated `.pkl` file. Change other hyperparameters if you want to. See `pix2tex/model/settings/config.yaml` for a template.
3. Now for the actual training run 
```
python -m pix2tex.train --config path_to_config_file
```

If you want to use your own data you might be interested in creating your own tokenizer with
```
python -m pix2tex.dataset.dataset --equations path_to_textfile --vocab-size 8000 --out tokenizer.json
```
Don&#039;t forget to update the path to the tokenizer in the config file and set `num_tokens` to your vocabulary size.

## Model
The model consist of a ViT [[1](#References)] encoder with a ResNet backbone and a Transformer [[2](#References)] decoder.

### Performance
| BLEU score | normed edit distance | token accuracy |
| ---------- | -------------------- | -------------- |
| 0.88       | 0.10                 | 0.60           |

## Data
We need paired data for the network to learn. Luckily there is a lot of LaTeX code on the internet, e.g. [wikipedia](https://www.wikipedia.org), [arXiv](https://www.arxiv.org). We also use the formulae from the [im2latex-100k](https://zenodo.org/record/56198#.V2px0jXT6eA) [[3](#References)] dataset.
All of it can be found [here](https://drive.google.com/drive/folders/13CA4vAmOmD_I_dSbvLp-Lf0s6KiaNfuO)

### Dataset Requirements
In order to render the math in many different fonts we use  XeLaTeX, generate a PDF and finally convert it to a PNG. For the last step we need to use some third party tools: 
* [XeLaTeX](https://www.ctan.org/pkg/xetex)
* [ImageMagick](https://imagemagick.org/) with [Ghostscript](https://www.ghostscript.com/index.html). (for converting pdf to png)
* [Node.js](https://nodejs.org/) to run [KaTeX](https://github.com/KaTeX/KaTeX) (for normalizing Latex code)
* Python 3.7+ &amp; dependencies (specified in `setup.py`)

### Fonts
Latin Modern Math, GFSNeohellenicMath.otf, Asana Math, XITS Math, Cambria Math


## TODO
- [x] add more evaluation metrics
- [x] create a GUI
- [ ] add beam search
- [ ] support handwritten formulae (kinda done, see training colab notebook)
- [ ] reduce model size (distillation)
- [ ] find optimal hyperparameters
- [ ] tweak model structure
- [ ] fix data scraping and scrape more data
- [ ] trace the model ([#2](https://github.com/lukas-blecher/LaTeX-OCR/issues/2))


## Contribution
Contributions of any kind are welcome.

## Acknowledgment
Code taken and modified from [lucidrains](https://github.com/lucidrains), [rwightman](https://github.com/rwightman/pytorch-image-models), [im2markup](https://github.com/harvardnlp/im2markup), [arxiv_leaks](https://github.com/soskek/arxiv_leaks), [pkra: Mathjax](https://github.com/pkra/MathJax-single-file), [harupy: snipping tool](https://github.com/harupy/snipping-tool)

## References
[1] [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929)

[2] [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

[3] [Image-to-Markup Generation with Coarse-to-Fine Attention](https://arxiv.org/abs/1609.04938v2)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[commaai/openpilot]]></title>
            <link>https://github.com/commaai/openpilot</link>
            <guid>https://github.com/commaai/openpilot</guid>
            <pubDate>Thu, 02 Oct 2025 00:04:13 GMT</pubDate>
            <description><![CDATA[openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/commaai/openpilot">commaai/openpilot</a></h1>
            <p>openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.</p>
            <p>Language: Python</p>
            <p>Stars: 58,087</p>
            <p>Forks: 10,279</p>
            <p>Stars today: 165 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; style=&quot;text-align: center;&quot;&gt;

&lt;h1&gt;openpilot&lt;/h1&gt;

&lt;p&gt;
  &lt;b&gt;openpilot is an operating system for robotics.&lt;/b&gt;
  &lt;br&gt;
  Currently, it upgrades the driver assistance system in 300+ supported cars.
&lt;/p&gt;

&lt;h3&gt;
  &lt;a href=&quot;https://docs.comma.ai&quot;&gt;Docs&lt;/a&gt;
  &lt;span&gt; Â· &lt;/span&gt;
  &lt;a href=&quot;https://docs.comma.ai/contributing/roadmap/&quot;&gt;Roadmap&lt;/a&gt;
  &lt;span&gt; Â· &lt;/span&gt;
  &lt;a href=&quot;https://github.com/commaai/openpilot/blob/master/docs/CONTRIBUTING.md&quot;&gt;Contribute&lt;/a&gt;
  &lt;span&gt; Â· &lt;/span&gt;
  &lt;a href=&quot;https://discord.comma.ai&quot;&gt;Community&lt;/a&gt;
  &lt;span&gt; Â· &lt;/span&gt;
  &lt;a href=&quot;https://comma.ai/shop&quot;&gt;Try it on a comma 3X&lt;/a&gt;
&lt;/h3&gt;

Quick start: `bash &lt;(curl -fsSL openpilot.comma.ai)`

[![openpilot tests](https://github.com/commaai/openpilot/actions/workflows/selfdrive_tests.yaml/badge.svg)](https://github.com/commaai/openpilot/actions/workflows/selfdrive_tests.yaml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![X Follow](https://img.shields.io/twitter/follow/comma_ai)](https://x.com/comma_ai)
[![Discord](https://img.shields.io/discord/469524606043160576)](https://discord.comma.ai)

&lt;/div&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/NmBfgOanCyk&quot; title=&quot;Video By Greer Viau&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/2f7112ae-f748-4f39-b617-fabd689c3772&quot;&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/VHKyqZ7t8Gw&quot; title=&quot;Video By Logan LeGrand&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/92351544-2833-40d7-9e0b-7ef7ae37ec4c&quot;&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/SUIZYzxtMQs&quot; title=&quot;A drive to Taco Bell&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/05ceefc5-2628-439c-a9b2-89ce77dc6f63&quot;&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;


Using openpilot in a car
------

To use openpilot in a car, you need four things:
1. **Supported Device:** a comma 3X, available at [comma.ai/shop](https://comma.ai/shop/comma-3x).
2. **Software:** The setup procedure for the comma 3X allows users to enter a URL for custom software. Use the URL `openpilot.comma.ai` to install the release version.
3. **Supported Car:** Ensure that you have one of [the 275+ supported cars](docs/CARS.md).
4. **Car Harness:** You will also need a [car harness](https://comma.ai/shop/car-harness) to connect your comma 3X to your car.

We have detailed instructions for [how to install the harness and device in a car](https://comma.ai/setup). Note that it&#039;s possible to run openpilot on [other hardware](https://blog.comma.ai/self-driving-car-for-free/), although it&#039;s not plug-and-play.

### Branches
| branch           | URL                                    | description                                                                         |
|------------------|----------------------------------------|-------------------------------------------------------------------------------------|
| `release3`         | openpilot.comma.ai                      | This is openpilot&#039;s release branch.                                                 |
| `release3-staging` | openpilot-test.comma.ai                | This is the staging branch for releases. Use it to get new releases slightly early. |
| `nightly`          | openpilot-nightly.comma.ai             | This is the bleeding edge development branch. Do not expect this to be stable.      |
| `nightly-dev`      | installer.comma.ai/commaai/nightly-dev | Same as nightly, but includes experimental development features for some cars.      |

To start developing openpilot
------

openpilot is developed by [comma](https://comma.ai/) and by users like you. We welcome both pull requests and issues on [GitHub](http://github.com/commaai/openpilot).

* Join the [community Discord](https://discord.comma.ai)
* Check out [the contributing docs](docs/CONTRIBUTING.md)
* Check out the [openpilot tools](tools/)
* Code documentation lives at https://docs.comma.ai
* Information about running openpilot lives on the [community wiki](https://github.com/commaai/openpilot/wiki)

Want to get paid to work on openpilot? [comma is hiring](https://comma.ai/jobs#open-positions) and offers lots of [bounties](https://comma.ai/bounties) for external contributors.

Safety and Testing
----

* openpilot observes [ISO26262](https://en.wikipedia.org/wiki/ISO_26262) guidelines, see [SAFETY.md](docs/SAFETY.md) for more details.
* openpilot has software-in-the-loop [tests](.github/workflows/selfdrive_tests.yaml) that run on every commit.
* The code enforcing the safety model lives in panda and is written in C, see [code rigor](https://github.com/commaai/panda#code-rigor) for more details.
* panda has software-in-the-loop [safety tests](https://github.com/commaai/panda/tree/master/tests/safety).
* Internally, we have a hardware-in-the-loop Jenkins test suite that builds and unit tests the various processes.
* panda has additional hardware-in-the-loop [tests](https://github.com/commaai/panda/blob/master/Jenkinsfile).
* We run the latest openpilot in a testing closet containing 10 comma devices continuously replaying routes.

&lt;details&gt;
&lt;summary&gt;MIT Licensed&lt;/summary&gt;

openpilot is released under the MIT license. Some parts of the software are released under other licenses as specified.

Any user of this software shall indemnify and hold harmless Comma.ai, Inc. and its directors, officers, employees, agents, stockholders, affiliates, subcontractors and customers from and against all allegations, claims, actions, suits, demands, damages, liabilities, obligations, losses, settlements, judgments, costs and expenses (including without limitation attorneysâ€™ fees and costs) which arise out of, relate to or result from any use of this software by user.

**THIS IS ALPHA QUALITY SOFTWARE FOR RESEARCH PURPOSES ONLY. THIS IS NOT A PRODUCT.
YOU ARE RESPONSIBLE FOR COMPLYING WITH LOCAL LAWS AND REGULATIONS.
NO WARRANTY EXPRESSED OR IMPLIED.**
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;User Data and comma Account&lt;/summary&gt;

By default, openpilot uploads the driving data to our servers. You can also access your data through [comma connect](https://connect.comma.ai/). We use your data to train better models and improve openpilot for everyone.

openpilot is open source software: the user is free to disable data collection if they wish to do so.

openpilot logs the road-facing cameras, CAN, GPS, IMU, magnetometer, thermal sensors, crashes, and operating system logs.
The driver-facing camera and microphone are only logged if you explicitly opt-in in settings.

By using openpilot, you agree to [our Privacy Policy](https://comma.ai/privacy). You understand that use of this software or its related services will generate certain types of user data, which may be logged and stored at the sole discretion of comma. By accepting this agreement, you grant an irrevocable, perpetual, worldwide right to comma for the use of this data.
&lt;/details&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[YILING0013/AI_NovelGenerator]]></title>
            <link>https://github.com/YILING0013/AI_NovelGenerator</link>
            <guid>https://github.com/YILING0013/AI_NovelGenerator</guid>
            <pubDate>Thu, 02 Oct 2025 00:04:12 GMT</pubDate>
            <description><![CDATA[ä½¿ç”¨aiç”Ÿæˆå¤šç« èŠ‚çš„é•¿ç¯‡å°è¯´ï¼Œè‡ªåŠ¨è¡”æ¥ä¸Šä¸‹æ–‡ã€ä¼ç¬”]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/YILING0013/AI_NovelGenerator">YILING0013/AI_NovelGenerator</a></h1>
            <p>ä½¿ç”¨aiç”Ÿæˆå¤šç« èŠ‚çš„é•¿ç¯‡å°è¯´ï¼Œè‡ªåŠ¨è¡”æ¥ä¸Šä¸‹æ–‡ã€ä¼ç¬”</p>
            <p>Language: Python</p>
            <p>Stars: 2,180</p>
            <p>Forks: 443</p>
            <p>Stars today: 131 stars today</p>
            <h2>README</h2><pre># ğŸ“– è‡ªåŠ¨å°è¯´ç”Ÿæˆå·¥å…·

&gt;- å½“å‰æ²¡æœ‰ä»€ä¹ˆç²¾åŠ›ç»´æŠ¤è¯¥é¡¹ç›®ï¼Œæœ¬èº«è¯¥é¡¹ç›®å¹¶æ— ä»»ä½•æ”¶ç›Šï¼Œä»¥åŠä¸´è¿‘æ¯•ä¸šï¼Œæœ‰å¾ˆå¤šå†…å®¹è¦å¿™ï¼Œå¦‚æœåé¢æœ‰æ—¶é—´çš„è¯ï¼Œå†è€ƒè™‘åŸºäºæ›´æ–°çš„æŠ€æœ¯å»é‡æ„å§ã€‚â€”â€”2025/9/24

&lt;div align=&quot;center&quot;&gt;
  
âœ¨ **æ ¸å¿ƒåŠŸèƒ½** âœ¨

| åŠŸèƒ½æ¨¡å—          | å…³é”®èƒ½åŠ›                          |
|-------------------|----------------------------------|
| ğŸ¨ å°è¯´è®¾å®šå·¥åŠ    | ä¸–ç•Œè§‚æ¶æ„ / è§’è‰²è®¾å®š / å‰§æƒ…è“å›¾   |
| ğŸ“– æ™ºèƒ½ç« èŠ‚ç”Ÿæˆ    | å¤šé˜¶æ®µç”Ÿæˆä¿éšœå‰§æƒ…è¿è´¯æ€§           |
| ğŸ§  çŠ¶æ€è¿½è¸ªç³»ç»Ÿ    | è§’è‰²å‘å±•è½¨è¿¹ / ä¼ç¬”ç®¡ç†ç³»ç»Ÿ         |
| ğŸ” è¯­ä¹‰æ£€ç´¢å¼•æ“    | åŸºäºå‘é‡çš„é•¿ç¨‹ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ç»´æŠ¤      |
| ğŸ“š çŸ¥è¯†åº“é›†æˆ      | æ”¯æŒæœ¬åœ°æ–‡æ¡£å‚è€ƒ         |
| âœ… è‡ªåŠ¨å®¡æ ¡æœºåˆ¶    | æ£€æµ‹å‰§æƒ…çŸ›ç›¾ä¸é€»è¾‘å†²çª          |
| ğŸ–¥ å¯è§†åŒ–å·¥ä½œå°    | å…¨æµç¨‹GUIæ“ä½œï¼Œé…ç½®/ç”Ÿæˆ/å®¡æ ¡ä¸€ä½“åŒ– |

&lt;/div&gt;

&gt; ä¸€æ¬¾åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å¤šåŠŸèƒ½å°è¯´ç”Ÿæˆå™¨ï¼ŒåŠ©æ‚¨é«˜æ•ˆåˆ›ä½œé€»è¾‘ä¸¥è°¨ã€è®¾å®šç»Ÿä¸€çš„é•¿ç¯‡æ•…äº‹

---

## ğŸ“‘ ç›®å½•å¯¼èˆª
1. [ç¯å¢ƒå‡†å¤‡](#-ç¯å¢ƒå‡†å¤‡)  
2. [é¡¹ç›®æ¶æ„](#-é¡¹ç›®æ¶æ„)  
3. [é…ç½®æŒ‡å—](#âš™ï¸-é…ç½®æŒ‡å—)  
4. [è¿è¡Œè¯´æ˜](#ğŸš€-è¿è¡Œè¯´æ˜)  
5. [ä½¿ç”¨æ•™ç¨‹](#ğŸ“˜-ä½¿ç”¨æ•™ç¨‹)  
6. [ç–‘éš¾è§£ç­”](#â“-ç–‘éš¾è§£ç­”)  

---

## ğŸ›  ç¯å¢ƒå‡†å¤‡
ç¡®ä¿æ»¡è¶³ä»¥ä¸‹è¿è¡Œæ¡ä»¶ï¼š
- **Python 3.9+** è¿è¡Œç¯å¢ƒï¼ˆæ¨è3.10-3.12ä¹‹é—´ï¼‰
- **pip** åŒ…ç®¡ç†å·¥å…·
- æœ‰æ•ˆAPIå¯†é’¥ï¼š
  - äº‘ç«¯æœåŠ¡ï¼šOpenAI / DeepSeek ç­‰
  - æœ¬åœ°æœåŠ¡ï¼šOllama ç­‰å…¼å®¹ OpenAI çš„æ¥å£

---


## ğŸ“¥ å®‰è£…è¯´æ˜
1. **ä¸‹è½½é¡¹ç›®**  
   - é€šè¿‡ [GitHub](https://github.com) ä¸‹è½½é¡¹ç›® ZIP æ–‡ä»¶ï¼Œæˆ–ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å…‹éš†æœ¬é¡¹ç›®ï¼š
     ```bash
     git clone https://github.com/YILING0013/AI_NovelGenerator
     ```

2. **å®‰è£…ç¼–è¯‘å·¥å…·ï¼ˆå¯é€‰ï¼‰**  
   - å¦‚æœå¯¹æŸäº›åŒ…æ— æ³•æ­£å¸¸å®‰è£…ï¼Œè®¿é—® [Visual Studio Build Tools](https://visualstudio.microsoft.com/zh-hans/visual-cpp-build-tools/) ä¸‹è½½å¹¶å®‰è£…C++ç¼–è¯‘å·¥å…·ï¼Œç”¨äºæ„å»ºéƒ¨åˆ†æ¨¡å—åŒ…ï¼›
   - å®‰è£…æ—¶ï¼Œé»˜è®¤åªåŒ…å« MSBuild å·¥å…·ï¼Œéœ€æ‰‹åŠ¨å‹¾é€‰å·¦ä¸Šè§’åˆ—è¡¨æ ä¸­çš„ **C++ æ¡Œé¢å¼€å‘** é€‰é¡¹ã€‚

3. **å®‰è£…ä¾èµ–å¹¶è¿è¡Œ**  
   - æ‰“å¼€ç»ˆç«¯ï¼Œè¿›å…¥é¡¹ç›®æºæ–‡ä»¶ç›®å½•ï¼š
     ```bash
     cd AI_NovelGenerator
     ```
   - å®‰è£…é¡¹ç›®ä¾èµ–ï¼š
     ```bash
     pip install -r requirements.txt
     ```
   - å®‰è£…å®Œæˆåï¼Œè¿è¡Œä¸»ç¨‹åºï¼š
     ```bash
     python main.py
     ```

&gt;å¦‚æœç¼ºå¤±éƒ¨åˆ†ä¾èµ–ï¼Œåç»­**æ‰‹åŠ¨æ‰§è¡Œ**
&gt;```bash
&gt;pip install XXX
&gt;```
&gt;è¿›è¡Œå®‰è£…å³å¯

## ğŸ—‚ é¡¹ç›®æ¶æ„
```
novel-generator/
â”œâ”€â”€ main.py                      # å…¥å£æ–‡ä»¶, è¿è¡Œ GUI
â”œâ”€â”€ consistency_checker.py       # ä¸€è‡´æ€§æ£€æŸ¥, é˜²æ­¢å‰§æƒ…å†²çª
|â€”â€” chapter_directory_parser.py  # ç›®å½•è§£æ
|â€”â€” embedding_adapters.py        # Embedding æ¥å£å°è£…
|â€”â€” llm_adapters.py              # LLM æ¥å£å°è£…
â”œâ”€â”€ prompt_definitions.py        # å®šä¹‰ AI æç¤ºè¯
â”œâ”€â”€ utils.py                     # å¸¸ç”¨å·¥å…·å‡½æ•°, æ–‡ä»¶æ“ä½œ
â”œâ”€â”€ config_manager.py            # ç®¡ç†é…ç½® (API Key, Base URL)
â”œâ”€â”€ config.json                  # ç”¨æˆ·é…ç½®æ–‡ä»¶ (å¯é€‰)
â”œâ”€â”€ novel_generator/             # ç« èŠ‚ç”Ÿæˆæ ¸å¿ƒé€»è¾‘
â”œâ”€â”€ ui/                          # å›¾å½¢ç•Œé¢
â””â”€â”€ vectorstore/                 # (å¯é€‰) æœ¬åœ°å‘é‡æ•°æ®åº“å­˜å‚¨
```

---

## âš™ï¸ é…ç½®æŒ‡å—
### ğŸ“Œ åŸºç¡€é…ç½®ï¼ˆconfig.jsonï¼‰
```json
{
    &quot;api_key&quot;: &quot;sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;base_url&quot;: &quot;https://api.openai.com/v1&quot;,
    &quot;interface_format&quot;: &quot;OpenAI&quot;,
    &quot;model_name&quot;: &quot;gpt-4o-mini&quot;,
    &quot;temperature&quot;: 0.7,
    &quot;max_tokens&quot;: 4096,
    &quot;embedding_api_key&quot;: &quot;sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX&quot;,
    &quot;embedding_interface_format&quot;: &quot;OpenAI&quot;,
    &quot;embedding_url&quot;: &quot;https://api.openai.com/v1&quot;,
    &quot;embedding_model_name&quot;: &quot;text-embedding-ada-002&quot;,
    &quot;embedding_retrieval_k&quot;: 4,
    &quot;topic&quot;: &quot;æ˜Ÿç©¹é“é“ä¸»è§’æ˜Ÿç©¿è¶Šåˆ°åŸç¥æç“¦ç‰¹å¤§é™†ï¼Œæ‹¯æ•‘æç“¦ç‰¹å¤§é™†ï¼Œå¹¶ä¸å…¶ä¸­çš„è§’è‰²å±•å¼€çˆ±æ¨æƒ…ä»‡çš„å°è¯´&quot;,
    &quot;genre&quot;: &quot;ç„å¹»&quot;,
    &quot;num_chapters&quot;: 120,
    &quot;word_number&quot;: 4000,
    &quot;filepath&quot;: &quot;D:/AI_NovelGenerator/filepath&quot;
}
```

### ğŸ”§ é…ç½®è¯´æ˜
1. **ç”Ÿæˆæ¨¡å‹é…ç½®**
   - `api_key`: å¤§æ¨¡å‹æœåŠ¡çš„APIå¯†é’¥
   - `base_url`: APIç»ˆç«¯åœ°å€ï¼ˆæœ¬åœ°æœåŠ¡å¡«Ollamaç­‰åœ°å€ï¼‰
   - `interface_format`: æ¥å£æ¨¡å¼
   - `model_name`: ä¸»ç”Ÿæˆæ¨¡å‹åç§°ï¼ˆå¦‚gpt-4, claude-3ç­‰ï¼‰
   - `temperature`: åˆ›æ„åº¦å‚æ•°ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šæœ‰åˆ›é€ æ€§ï¼‰
   - `max_tokens`: æ¨¡å‹æœ€å¤§å›å¤é•¿åº¦

2. **Embeddingæ¨¡å‹é…ç½®**
   - `embedding_model_name`: æ¨¡å‹åç§°ï¼ˆå¦‚Ollamaçš„nomic-embed-textï¼‰
   - `embedding_url`: æœåŠ¡åœ°å€
   - `embedding_retrieval_k`: 

3. **å°è¯´å‚æ•°é…ç½®**
   - `topic`: æ ¸å¿ƒæ•…äº‹ä¸»é¢˜
   - `genre`: ä½œå“ç±»å‹
   - `num_chapters`: æ€»ç« èŠ‚æ•°
   - `word_number`: å•ç« ç›®æ ‡å­—æ•°
   - `filepath`: ç”Ÿæˆæ–‡ä»¶å­˜å‚¨è·¯å¾„

---

## ğŸš€ è¿è¡Œè¯´æ˜
### **æ–¹å¼ 1ï¼šä½¿ç”¨ Python è§£é‡Šå™¨**
```bash
python main.py
```
æ‰§è¡Œåï¼ŒGUI å°†ä¼šå¯åŠ¨ï¼Œä½ å¯ä»¥åœ¨å›¾å½¢ç•Œé¢ä¸­è¿›è¡Œå„é¡¹æ“ä½œã€‚

### **æ–¹å¼ 2ï¼šæ‰“åŒ…ä¸ºå¯æ‰§è¡Œæ–‡ä»¶**
å¦‚æœä½ æƒ³åœ¨æ—  Python ç¯å¢ƒçš„æœºå™¨ä¸Šä½¿ç”¨æœ¬å·¥å…·ï¼Œå¯ä»¥ä½¿ç”¨ **PyInstaller** è¿›è¡Œæ‰“åŒ…ï¼š

```bash
pip install pyinstaller
pyinstaller main.spec
```
æ‰“åŒ…å®Œæˆåï¼Œä¼šåœ¨ `dist/` ç›®å½•ä¸‹ç”Ÿæˆå¯æ‰§è¡Œæ–‡ä»¶ï¼ˆå¦‚ Windows ä¸‹çš„ `main.exe`ï¼‰ã€‚

---

## ğŸ“˜ ä½¿ç”¨æ•™ç¨‹
1. **å¯åŠ¨åï¼Œå…ˆå®ŒæˆåŸºæœ¬å‚æ•°è®¾ç½®ï¼š**  
   - **API Key &amp; Base URL**ï¼ˆå¦‚ `https://api.openai.com/v1`ï¼‰  
   - **æ¨¡å‹åç§°**ï¼ˆå¦‚ `gpt-3.5-turbo`ã€`gpt-4o` ç­‰ï¼‰  
   - **Temperature** (0~1ï¼Œå†³å®šæ–‡å­—åˆ›æ„ç¨‹åº¦)  
   - **ä¸»é¢˜(Topic)**ï¼ˆå¦‚ â€œåºŸåœŸä¸–ç•Œçš„ AI å›ä¹±â€ï¼‰  
   - **ç±»å‹(Genre)**ï¼ˆå¦‚ â€œç§‘å¹»â€/â€œé­”å¹»â€/â€œéƒ½å¸‚å¹»æƒ³â€ï¼‰  
   - **ç« èŠ‚æ•°**ã€**æ¯ç« å­—æ•°**ï¼ˆå¦‚ 10 ç« ï¼Œæ¯ç« çº¦ 3000 å­—ï¼‰  
   - **ä¿å­˜è·¯å¾„**ï¼ˆå»ºè®®åˆ›å»ºä¸€ä¸ªæ–°çš„è¾“å‡ºæ–‡ä»¶å¤¹ï¼‰

2. **ç‚¹å‡»ã€ŒStep1. ç”Ÿæˆè®¾å®šã€**  
   - ç³»ç»Ÿå°†åŸºäºä¸»é¢˜ã€ç±»å‹ã€ç« èŠ‚æ•°ç­‰ä¿¡æ¯ï¼Œç”Ÿæˆï¼š  
     - `Novel_setting.txt`ï¼šåŒ…å«ä¸–ç•Œè§‚ã€è§’è‰²ä¿¡æ¯ã€é›·ç‚¹æš—çº¿ç­‰ã€‚  
   - å¯ä»¥åœ¨ç”Ÿæˆåçš„ `Novel_setting.txt` ä¸­æŸ¥çœ‹æˆ–ä¿®æ”¹è®¾å®šå†…å®¹ã€‚

3. **ç‚¹å‡»ã€ŒStep2. ç”Ÿæˆç›®å½•ã€**  
   - ç³»ç»Ÿä¼šæ ¹æ®å·²å®Œæˆçš„ `Novel_setting.txt` å†…å®¹ï¼Œä¸ºå…¨éƒ¨ç« èŠ‚ç”Ÿæˆï¼š  
     - `Novel_directory.txt`ï¼šåŒ…æ‹¬æ¯ç« æ ‡é¢˜å’Œç®€è¦æç¤ºã€‚  
   - å¯ä»¥åœ¨ç”Ÿæˆåçš„æ–‡ä»¶ä¸­æŸ¥çœ‹ã€ä¿®æ”¹æˆ–è¡¥å……ç« èŠ‚æ ‡é¢˜å’Œæè¿°ã€‚

4. **ç‚¹å‡»ã€ŒStep3. ç”Ÿæˆç« èŠ‚è‰ç¨¿ã€**  
   - åœ¨ç”Ÿæˆç« èŠ‚ä¹‹å‰ï¼Œä½ å¯ä»¥ï¼š  
     - **è®¾ç½®ç« èŠ‚å·**ï¼ˆå¦‚å†™ç¬¬ 1 ç« ï¼Œå°±å¡« `1`ï¼‰  
     - **åœ¨â€œæœ¬ç« æŒ‡å¯¼â€è¾“å…¥æ¡†**ä¸­æä¾›å¯¹æœ¬ç« å‰§æƒ…çš„ä»»ä½•æœŸæœ›æˆ–æç¤º  
   - ç‚¹å‡»æŒ‰é’®åï¼Œç³»ç»Ÿå°†ï¼š  
     - è‡ªåŠ¨è¯»å–å‰æ–‡è®¾å®šã€`Novel_directory.txt`ã€ä»¥åŠå·²å®šç¨¿ç« èŠ‚  
     - è°ƒç”¨å‘é‡æ£€ç´¢å›é¡¾å‰§æƒ…ï¼Œä¿è¯ä¸Šä¸‹æ–‡è¿è´¯  
     - ç”Ÿæˆæœ¬ç« å¤§çº² (`outline_X.txt`) åŠæ­£æ–‡ (`chapter_X.txt`)  
   - ç”Ÿæˆå®Œæˆåï¼Œä½ å¯åœ¨å·¦ä¾§çš„æ–‡æœ¬æ¡†æŸ¥çœ‹ã€ç¼–è¾‘æœ¬ç« è‰ç¨¿å†…å®¹ã€‚

5. **ç‚¹å‡»ã€ŒStep4. å®šç¨¿å½“å‰ç« èŠ‚ã€**  
   - ç³»ç»Ÿå°†ï¼š  
     - **æ›´æ–°å…¨å±€æ‘˜è¦**ï¼ˆå†™å…¥ `global_summary.txt`ï¼‰  
     - **æ›´æ–°è§’è‰²çŠ¶æ€**ï¼ˆå†™å…¥ `character_state.txt`ï¼‰  
     - **æ›´æ–°å‘é‡æ£€ç´¢åº“**ï¼ˆä¿è¯åç»­ç« èŠ‚å¯ä»¥è°ƒç”¨æœ€æ–°ä¿¡æ¯ï¼‰  
     - **æ›´æ–°å‰§æƒ…è¦ç‚¹**ï¼ˆå¦‚ `plot_arcs.txt`ï¼‰  
   - å®šç¨¿å®Œæˆåï¼Œä½ å¯ä»¥åœ¨ `chapter_X.txt` ä¸­çœ‹åˆ°å®šç¨¿åçš„æ–‡æœ¬ã€‚

6. **ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆå¯é€‰ï¼‰**  
   - ç‚¹å‡»ã€Œ[å¯é€‰] ä¸€è‡´æ€§å®¡æ ¡ã€æŒ‰é’®ï¼Œå¯¹æœ€æ–°ç« èŠ‚è¿›è¡Œå†²çªæ£€æµ‹ï¼Œå¦‚è§’è‰²é€»è¾‘ã€å‰§æƒ…å‰åçŸ›ç›¾ç­‰ã€‚  
   - è‹¥æœ‰å†²çªï¼Œä¼šåœ¨æ—¥å¿—åŒºè¾“å‡ºè¯¦ç»†æç¤ºã€‚

7. **é‡å¤ç¬¬ 4-6 æ­¥** ç›´åˆ°æ‰€æœ‰ç« èŠ‚ç”Ÿæˆå¹¶å®šç¨¿ï¼

&gt; **å‘é‡æ£€ç´¢é…ç½®æç¤º**  
&gt; 1. embeddingæ¨¡å‹éœ€è¦æ˜¾ç¤ºæŒ‡å®šæ¥å£å’Œæ¨¡å‹åç§°ï¼›
&gt; 2. ä½¿ç”¨**æœ¬åœ°Ollama**çš„**Embedding**æ—¶éœ€æå‰å¯åŠ¨OllamaæœåŠ¡ï¼š  
&gt;    ```bash
&gt;    ollama serve  # å¯åŠ¨æœåŠ¡
&gt;    ollama pull nomic-embed-text  # ä¸‹è½½/å¯ç”¨æ¨¡å‹
&gt;    ```
&gt; 3. åˆ‡æ¢ä¸åŒEmbeddingæ¨¡å‹åå»ºè®®æ¸…ç©ºvectorstoreç›®å½•
&gt; 4. äº‘ç«¯Embeddingéœ€ç¡®ä¿å¯¹åº”APIæƒé™å·²å¼€é€š

---

## â“ ç–‘éš¾è§£ç­”
### Q1: Expecting value: line 1 column 1 (char 0)

è¯¥é—®é¢˜å¤§æ¦‚ç‡ç”±äºAPIæœªæ­£ç¡®å“åº”é€ æˆï¼Œä¹Ÿè®¸å“åº”äº†ä¸€ä¸ªhtmlï¼Ÿå…¶å®ƒå†…å®¹ï¼Œå¯¼è‡´å‡ºç°è¯¥æŠ¥é”™ï¼›


### Q2: HTTP/1.1 504 Gateway Timeoutï¼Ÿ
ç¡®è®¤æ¥å£æ˜¯å¦ç¨³å®šï¼›

### Q3: å¦‚ä½•åˆ‡æ¢ä¸åŒçš„Embeddingæä¾›å•†ï¼Ÿ
åœ¨GUIç•Œé¢ä¸­å¯¹åº”è¾“å…¥å³å¯ã€‚

---

å¦‚æœ‰æ›´å¤šé—®é¢˜æˆ–éœ€æ±‚ï¼Œæ¬¢è¿åœ¨**é¡¹ç›® Issues** ä¸­æå‡ºã€‚
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hacksider/Deep-Live-Cam]]></title>
            <link>https://github.com/hacksider/Deep-Live-Cam</link>
            <guid>https://github.com/hacksider/Deep-Live-Cam</guid>
            <pubDate>Thu, 02 Oct 2025 00:04:11 GMT</pubDate>
            <description><![CDATA[real time face swap and one-click video deepfake with only a single image]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hacksider/Deep-Live-Cam">hacksider/Deep-Live-Cam</a></h1>
            <p>real time face swap and one-click video deepfake with only a single image</p>
            <p>Language: Python</p>
            <p>Stars: 73,668</p>
            <p>Forks: 10,715</p>
            <p>Stars today: 70 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;Deep-Live-Cam&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  Real-time face swap and video deepfake with a single click and only a single image.
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/11395&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11395&quot; alt=&quot;hacksider%2FDeep-Live-Cam | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/demo.gif&quot; alt=&quot;Demo GIF&quot; width=&quot;800&quot;&gt;
&lt;/p&gt;

##  Disclaimer

This deepfake software is designed to be a productive tool for the AI-generated media industry. It can assist artists in animating custom characters, creating engaging content, and even using models for clothing design.

We are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to the law and ethics. We may shut down the project or add watermarks if legally required.

- Ethical Use: Users are expected to use this software responsibly and legally. If using a real person&#039;s face, obtain their consent and clearly label any output as a deepfake when sharing online.

- Content Restrictions: The software includes built-in checks to prevent processing inappropriate media, such as nudity, graphic content, or sensitive material.

- Legal Compliance: We adhere to all relevant laws and ethical guidelines. If legally required, we may shut down the project or add watermarks to the output.

- User Responsibility: We are not responsible for end-user actions. Users must ensure their use of the software aligns with ethical standards and legal requirements.

By using this software, you agree to these terms and commit to using it in a manner that respects the rights and dignity of others.

Users are expected to use this software responsibly and legally. If using a real person&#039;s face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user actions.

## Exclusive v2.2 Quick Start - Pre-built (Windows/Mac Silicon)

  &lt;a href=&quot;https://deeplivecam.net/index.php/quickstart&quot;&gt; &lt;img src=&quot;media/Download.png&quot; width=&quot;285&quot; height=&quot;77&quot; /&gt;

##### This is the fastest build you can get if you have a discrete NVIDIA or AMD GPU or Mac Silicon, And you&#039;ll receive special priority support.
 
###### These Pre-builts are perfect for non-technical users or those who don&#039;t have time to, or can&#039;t manually install all the requirements. Just a heads-up: this is an open-source project, so you can also install it manually. 

## TLDR; Live Deepfake in just 3 Clicks
![easysteps](https://github.com/user-attachments/assets/af825228-852c-411b-b787-ffd9aac72fc6)
1. Select a face
2. Select which camera to use
3. Press live!

## Features &amp; Uses - Everything is in real-time

### Mouth Mask

**Retain your original mouth for accurate movement using Mouth Mask**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/ludwig.gif&quot; alt=&quot;resizable-gif&quot;&gt;
&lt;/p&gt;

### Face Mapping

**Use different faces on multiple subjects simultaneously**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/streamers.gif&quot; alt=&quot;face_mapping_source&quot;&gt;
&lt;/p&gt;

### Your Movie, Your Face

**Watch movies with any face in real-time**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/movie.gif&quot; alt=&quot;movie&quot;&gt;
&lt;/p&gt;

### Live Show

**Run Live shows and performances**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/live_show.gif&quot; alt=&quot;show&quot;&gt;
&lt;/p&gt;

### Memes

**Create Your Most Viral Meme Yet**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/meme.gif&quot; alt=&quot;show&quot; width=&quot;450&quot;&gt; 
  &lt;br&gt;
  &lt;sub&gt;Created using Many Faces feature in Deep-Live-Cam&lt;/sub&gt;
&lt;/p&gt;

### Omegle

**Surprise people on Omegle**

&lt;p align=&quot;center&quot;&gt;
  &lt;video src=&quot;https://github.com/user-attachments/assets/2e9b9b82-fa04-4b70-9f56-b1f68e7672d0&quot; width=&quot;450&quot; controls&gt;&lt;/video&gt;
&lt;/p&gt;

## Installation (Manual)

**Please be aware that the installation requires technical skills and is not for beginners. Consider downloading the quickstart version.**

&lt;details&gt;
&lt;summary&gt;Click to see the process&lt;/summary&gt;

### Installation

This is more likely to work on your computer but will be slower as it utilizes the CPU.

**1. Set up Your Platform**

-   Python (3.11 recommended)
-   pip
-   git
-   [ffmpeg](https://www.youtube.com/watch?v=OlNWCpFdVMA) - ```iex (irm ffmpeg.tc.ht)```
-   [Visual Studio 2022 Runtimes (Windows)](https://visualstudio.microsoft.com/visual-cpp-build-tools/)

**2. Clone the Repository**

```bash
git clone https://github.com/hacksider/Deep-Live-Cam.git
cd Deep-Live-Cam
```

**3. Download the Models**

1. [GFPGANv1.4](https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth)
2. [inswapper\_128\_fp16.onnx](https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx)

Place these files in the &quot;**models**&quot; folder.

**4. Install Dependencies**

We highly recommend using a `venv` to avoid issues.


For Windows:
```bash
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
```
For Linux:
```bash
# Ensure you use the installed Python 3.10
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

**For macOS:**

Apple Silicon (M1/M2/M3) requires specific setup:

```bash
# Install Python 3.11 (specific version is important)
brew install python@3.11

# Install tkinter package (required for the GUI)
brew install python-tk@3.10

# Create and activate virtual environment with Python 3.11
python3.11 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

** In case something goes wrong and you need to reinstall the virtual environment **

```bash
# Deactivate the virtual environment
rm -rf venv

# Reinstall the virtual environment
python -m venv venv
source venv/bin/activate

# install the dependencies again
pip install -r requirements.txt

# gfpgan and basicsrs issue fix
pip install git+https://github.com/xinntao/BasicSR.git@master
pip uninstall gfpgan -y
pip install git+https://github.com/TencentARC/GFPGAN.git@master
```

**Run:** If you don&#039;t have a GPU, you can run Deep-Live-Cam using `python run.py`. Note that initial execution will download models (~300MB).

### GPU Acceleration

**CUDA Execution Provider (Nvidia)**

1. Install [CUDA Toolkit 12.8.0](https://developer.nvidia.com/cuda-12-8-0-download-archive)
2. Install [cuDNN v8.9.7 for CUDA 12.x](https://developer.nvidia.com/rdp/cudnn-archive) (required for onnxruntime-gpu):
   - Download cuDNN v8.9.7 for CUDA 12.x
   - Make sure the cuDNN bin directory is in your system PATH
3. Install dependencies:

```bash
pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu==1.21.0
```

3. Usage:

```bash
python run.py --execution-provider cuda
```

**CoreML Execution Provider (Apple Silicon)**

Apple Silicon (M1/M2/M3) specific installation:

1. Make sure you&#039;ve completed the macOS setup above using Python 3.10.
2. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-silicon
pip install onnxruntime-silicon==1.13.1
```

3. Usage (important: specify Python 3.10):

```bash
python3.10 run.py --execution-provider coreml
```

**Important Notes for macOS:**
- You **must** use Python 3.10, not newer versions like 3.11 or 3.13
- Always run with `python3.10` command not just `python` if you have multiple Python versions installed
- If you get error about `_tkinter` missing, reinstall the tkinter package: `brew reinstall python-tk@3.10`
- If you get model loading errors, check that your models are in the correct folder
- If you encounter conflicts with other Python versions, consider uninstalling them:
  ```bash
  # List all installed Python versions
  brew list | grep python
  
  # Uninstall conflicting versions if needed
  brew uninstall --ignore-dependencies python@3.11 python@3.13
  
  # Keep only Python 3.11
  brew cleanup
  ```

**CoreML Execution Provider (Apple Legacy)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-coreml
pip install onnxruntime-coreml==1.21.0
```

2. Usage:

```bash
python run.py --execution-provider coreml
```

**DirectML Execution Provider (Windows)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-directml
pip install onnxruntime-directml==1.21.0
```

2. Usage:

```bash
python run.py --execution-provider directml
```

**OpenVINOâ„¢ Execution Provider (Intel)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-openvino
pip install onnxruntime-openvino==1.21.0
```

2. Usage:

```bash
python run.py --execution-provider openvino
```
&lt;/details&gt;

## Usage

**1. Image/Video Mode**

-   Execute `python run.py`.
-   Choose a source face image and a target image/video.
-   Click &quot;Start&quot;.
-   The output will be saved in a directory named after the target video.

**2. Webcam Mode**

-   Execute `python run.py`.
-   Select a source face image.
-   Click &quot;Live&quot;.
-   Wait for the preview to appear (10-30 seconds).
-   Use a screen capture tool like OBS to stream.
-   To change the face, select a new source image.

## Command Line Arguments (Unmaintained)

```
options:
  -h, --help                                               show this help message and exit
  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image
  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video
  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory
  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)
  --keep-fps                                               keep original fps
  --keep-audio                                             keep original audio
  --keep-frames                                            keep temporary frames
  --many-faces                                             process every face
  --map-faces                                              map source target faces
  --mouth-mask                                             mask the mouth region
  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder
  --video-quality [0-51]                                   adjust output video quality
  --live-mirror                                            the live camera display as you see it in the front-facing camera frame
  --live-resizable                                         the live camera frame is resizable
  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB
  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)
  --execution-threads EXECUTION_THREADS                    number of execution threads
  -v, --version                                            show program&#039;s version number and exit
```

Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.

## Press

**We are always open to criticism and are ready to improve, that&#039;s why we didn&#039;t cherry-pick anything.**

 - [*&quot;Deep-Live-Cam goes viral, allowing anyone to become a digital doppelganger&quot;*](https://arstechnica.com/information-technology/2024/08/new-ai-tool-enables-real-time-face-swapping-on-webcams-raising-fraud-concerns/) - Ars Technica
 - [*&quot;Thanks Deep Live Cam, shapeshifters are among us now&quot;*](https://dataconomy.com/2024/08/15/what-is-deep-live-cam-github-deepfake/) - Dataconomy
 - [*&quot;This free AI tool lets you become anyone during video-calls&quot;*](https://www.newsbytesapp.com/news/science/deep-live-cam-ai-impersonation-tool-goes-viral/story) - NewsBytes
 - [*&quot;OK, this viral AI live stream software is truly terrifying&quot;*](https://www.creativebloq.com/ai/ok-this-viral-ai-live-stream-software-is-truly-terrifying) - Creative Bloq
 - [*&quot;Deepfake AI Tool Lets You Become Anyone in a Video Call With Single Photo&quot;*](https://petapixel.com/2024/08/14/deep-live-cam-deepfake-ai-tool-lets-you-become-anyone-in-a-video-call-with-single-photo-mark-zuckerberg-jd-vance-elon-musk/) - PetaPixel
 - [*&quot;Deep-Live-Cam Uses AI to Transform Your Face in Real-Time, Celebrities Included&quot;*](https://www.techeblog.com/deep-live-cam-ai-transform-face/) - TechEBlog
 - [*&quot;An AI tool that &quot;makes you look like anyone&quot; during a video call is going viral online&quot;*](https://telegrafi.com/en/a-tool-that-makes-you-look-like-anyone-during-a-video-call-is-going-viral-on-the-Internet/) - Telegrafi
 - [*&quot;This Deepfake Tool Turning Images Into Livestreams is Topping the GitHub Charts&quot;*](https://decrypt.co/244565/this-deepfake-tool-turning-images-into-livestreams-is-topping-the-github-charts) - Emerge
 - [*&quot;New Real-Time Face-Swapping AI Allows Anyone to Mimic Famous Faces&quot;*](https://www.digitalmusicnews.com/2024/08/15/face-swapping-ai-real-time-mimic/) - Digital Music News
 - [*&quot;This real-time webcam deepfake tool raises alarms about the future of identity theft&quot;*](https://www.diyphotography.net/this-real-time-webcam-deepfake-tool-raises-alarms-about-the-future-of-identity-theft/) - DIYPhotography
 - [*&quot;That&#039;s Crazy, Oh God. That&#039;s Fucking Freaky Dude... That&#039;s So Wild Dude&quot;*](https://www.youtube.com/watch?time_continue=1074&amp;v=py4Tc-Y8BcY) - SomeOrdinaryGamers
 - [*&quot;Alright look look look, now look chat, we can do any face we want to look like chat&quot;*](https://www.youtube.com/live/mFsCe7AIxq8?feature=shared&amp;t=2686) - IShowSpeed
 - [*&quot;They do a pretty good job matching poses, expression and even the lighting&quot;*](https://www.youtube.com/watch?v=wnCghLjqv3s&amp;t=551s) - TechLinked (LTT)
 - [*&quot;Als Sean Connery an der Redaktionskonferenz teilnahm&quot;*](https://www.golem.de/news/deepfakes-als-sean-connery-an-der-redaktionskonferenz-teilnahm-2408-188172.html) - Golem.de (German)


## Credits

-   [ffmpeg](https://ffmpeg.org/): for making video-related operations easy
-   [deepinsight](https://github.com/deepinsight): for their [insightface](https://github.com/deepinsight/insightface) project which provided a well-made library and models. Please be reminded that the [use of the model is for non-commercial research purposes only](https://github.com/deepinsight/insightface?tab=readme-ov-file#license).
-   [havok2-htwo](https://github.com/havok2-htwo): for sharing the code for webcam
-   [GosuDRM](https://github.com/GosuDRM): for the open version of roop
-   [pereiraroland26](https://github.com/pereiraroland26): Multiple faces support
-   [vic4key](https://github.com/vic4key): For supporting/contributing to this project
-   [kier007](https://github.com/kier007): for improving the user experience
-   [qitianai](https://github.com/qitianai): for multi-lingual support
-   and [all developers](https://github.com/hacksider/Deep-Live-Cam/graphs/contributors) behind libraries used in this project.
-   Footnote: Please be informed that the base author of the code is [s0md3v](https://github.com/s0md3v/roop)
-   All the wonderful users who helped make this project go viral by starring the repo â¤ï¸

[![Stargazers](https://reporoster.com/stars/hacksider/Deep-Live-Cam)](https://github.com/hacksider/Deep-Live-Cam/stargazers)

## Contributions

![Alt](https://repobeats.axiom.co/api/embed/fec8e29c45dfdb9c5916f3a7830e1249308d20e1.svg &quot;Repobeats analytics image&quot;)

## Stars to the Moon ğŸš€

&lt;a href=&quot;https://star-history.com/#hacksider/deep-live-cam&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Byaidu/PDFMathTranslate]]></title>
            <link>https://github.com/Byaidu/PDFMathTranslate</link>
            <guid>https://github.com/Byaidu/PDFMathTranslate</guid>
            <pubDate>Thu, 02 Oct 2025 00:04:10 GMT</pubDate>
            <description><![CDATA[PDF scientific paper translation with preserved formats - åŸºäº AI å®Œæ•´ä¿ç•™æ’ç‰ˆçš„ PDF æ–‡æ¡£å…¨æ–‡åŒè¯­ç¿»è¯‘ï¼Œæ”¯æŒ Google/DeepL/Ollama/OpenAI ç­‰æœåŠ¡ï¼Œæä¾› CLI/GUI/MCP/Docker/Zotero]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Byaidu/PDFMathTranslate">Byaidu/PDFMathTranslate</a></h1>
            <p>PDF scientific paper translation with preserved formats - åŸºäº AI å®Œæ•´ä¿ç•™æ’ç‰ˆçš„ PDF æ–‡æ¡£å…¨æ–‡åŒè¯­ç¿»è¯‘ï¼Œæ”¯æŒ Google/DeepL/Ollama/OpenAI ç­‰æœåŠ¡ï¼Œæä¾› CLI/GUI/MCP/Docker/Zotero</p>
            <p>Language: Python</p>
            <p>Stars: 28,261</p>
            <p>Forks: 2,488</p>
            <p>Stars today: 191 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

English | [ç®€ä½“ä¸­æ–‡](docs/README_zh-CN.md) | [ç¹é«”ä¸­æ–‡](docs/README_zh-TW.md) | [æ—¥æœ¬èª](docs/README_ja-JP.md) | [í•œêµ­ì–´](docs/README_ko-KR.md)

&lt;img src=&quot;./docs/images/banner.png&quot; width=&quot;320px&quot;  alt=&quot;PDF2ZH&quot;/&gt;

&lt;h2 id=&quot;title&quot;&gt;PDFMathTranslate&lt;/h2&gt;

&lt;p&gt;
  &lt;!-- PyPI --&gt;
  &lt;a href=&quot;https://pypi.org/project/pdf2zh/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/pdf2zh&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pepy.tech/projects/pdf2zh&quot;&gt;
    &lt;img src=&quot;https://static.pepy.tech/badge/pdf2zh&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://hub.docker.com/r/byaidu/pdf2zh&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/docker/pulls/byaidu/pdf2zh&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://hellogithub.com/repository/8ec2cfd3ef744762bf531232fa32bc47&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://api.hellogithub.com/v1/widgets/recommend.svg?rid=8ec2cfd3ef744762bf531232fa32bc47&amp;claim_uid=JQ0yfeBNjaTuqDU&amp;theme=small&quot; alt=&quot;Featuredï½œHelloGitHub&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://gitcode.com/Byaidu/PDFMathTranslate/overview&quot;&gt;
    &lt;img src=&quot;https://gitcode.com/Byaidu/PDFMathTranslate/star/badge.svg&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://huggingface.co/spaces/reycn/PDFMathTranslate-Docker&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97-Online%20Demo-FF9E0D&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.modelscope.cn/studios/AI-ModelScope/PDFMathTranslate&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/ModelScope-Demo-blue&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/Byaidu/PDFMathTranslate/pulls&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/contributions-welcome-green&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://t.me/+Z9_SgnxmsmA5NzBl&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Telegram-2CA5E0?style=flat-squeare&amp;logo=telegram&amp;logoColor=white&quot;&gt;&lt;/a&gt;
  &lt;!-- License --&gt;
  &lt;a href=&quot;./LICENSE&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/license/Byaidu/PDFMathTranslate&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;a href=&quot;https://trendshift.io/repositories/12424&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12424&quot; alt=&quot;Byaidu%2FPDFMathTranslate | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;

&lt;h2 id=&quot;updates&quot;&gt;1. What does this do?&lt;/h2&gt;

Scientific PDF document translation preserving layouts.

- ğŸ“Š Preserve formulas, charts, table of contents, and annotations.
- ğŸŒ Support [multiple languages](#usage), and diverse [translation services](#usage).
- ğŸ¤– Provides [commandline tool](#usage), [interactive user interface](#install), and [Docker](#install)

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;./docs/images/preview.gif&quot; width=&quot;80%&quot;/&gt;
&lt;/div&gt;

&lt;h2 id=&quot;updates&quot;&gt;2. Recent Updates&lt;/h2&gt;

- [May 9, 2025] pdf2zh 2.0 Preview Version [#586](https://github.com/Byaidu/PDFMathTranslate/issues/586): The Windows ZIP file and Docker image are now available.

  &gt; [!NOTE]
  &gt;
  &gt; 2.0 Moved to a new repository under the organization: [PDFMathTranslate/PDFMathTranslate-next](https://github.com/PDFMathTranslate/PDFMathTranslate-next)
  &gt; 
  &gt; Version 2.0 official release has been published.

- [Mar. 3, 2025] Experimental support for the new backend [BabelDOC](https://github.com/funstory-ai/BabelDOC) WebUI added as an experimental option (by [@awwaawwa](https://github.com/awwaawwa))
- [Feb. 22 2025] Better release CI and well-packaged windows-amd64 exe (by [@awwaawwa](https://github.com/awwaawwa))


&lt;h2 id=&quot;use-section&quot;&gt;3. Use ğŸŒŸ&lt;/h2&gt;
&lt;h3 id=&quot;demo&quot;&gt;3.1 Online Service ğŸŒŸ&lt;/h3&gt;

You can try our application out using either of the following demos:

- [Public free service](https://pdf2zh.com/) online without installation _(recommended)_.
- [Immersive Translate - BabelDOC](https://app.immersivetranslate.com/babel-doc/) 1000 free pages per month. _(recommended)_
- [Demo hosted on HuggingFace](https://huggingface.co/spaces/reycn/PDFMathTranslate-Docker)
- [Demo hosted on ModelScope](https://www.modelscope.cn/studios/AI-ModelScope/PDFMathTranslate) without installation.

Note that the computing resources of the demo are limited, so please avoid abusing them.

&lt;h3 id=&quot;install&quot;&gt;3.2 Local Installation&lt;/h3&gt;

For different use cases, we provide distinct methods to use our program:

&lt;details open&gt;
  &lt;summary&gt;3.2.1 Python: Install using uv&lt;/summary&gt;

1. Python installed (3.10 &lt;= version &lt;= 3.12)

2. Install our package:

   ```bash
   pip install uv
   uv tool install --python 3.12 pdf2zh
   ```

3. Execute translation, files generated in [current working directory](https://chatgpt.com/share/6745ed36-9acc-800e-8a90-59204bd13444):

   ```bash
   pdf2zh document.pdf
   ```

&lt;/details&gt;
&lt;details&gt;
  &lt;summary&gt;3.2.2 Python: Install using pip&lt;/summary&gt;

1. Python installed (3.10 &lt;= version &lt;= 3.12)
2. Install our package:

   ```bash
   pip install pdf2zh
   ```

3. Execute translation, files generated in [current working directory](https://chatgpt.com/share/6745ed36-9acc-800e-8a90-59204bd13444):

   ```bash
   pdf2zh document.pdf
   ```

&lt;/details&gt;
&lt;details&gt;
  &lt;summary&gt;3.3.3 Python: Graphic user interface&lt;/summary&gt;

1. Python installed (3.10 &lt;= version &lt;= 3.12)

2. Install our package:

  ```bash
  pip install pdf2zh
  ```

3. Start using in browser:

   ```bash
   pdf2zh -i
   ```

4. If your browser has not been started automatically, goto

   ```bash
   http://localhost:7860/
   ```

   &lt;img src=&quot;./docs/images/gui.gif&quot; width=&quot;500&quot;/&gt;

See [documentation for GUI](./docs/README_GUI.md) for more details.

&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;3.2.4 Application: On Windows&lt;/summary&gt;

1. Download pdf2zh-version-win64.zip from [release page](https://github.com/Byaidu/PDFMathTranslate/releases)

2. Unzip and double-click `pdf2zh.exe` to run.


  &gt; [!TIP]
  &gt;
  &gt; - If you&#039;re using Windows and cannot open the file after downloading, please install [vc_redist.x64.exe](https://aka.ms/vs/17/release/vc_redist.x64.exe) and try again.
  &gt; 
&lt;/details&gt;


&lt;details&gt;

&lt;summary&gt;3.2.5 Reference manager: Zotero Plugin&lt;/summary&gt;


See [Zotero PDF2zh](https://github.com/guaguastandup/zotero-pdf2zh) for more details.

&lt;/details&gt;


&lt;details&gt;
  &lt;summary&gt;3.2.6 Docker: Containerized Deployment&lt;/summary&gt;

1. Pull and run:

   ```bash
   docker pull byaidu/pdf2zh
   docker run -d -p 7860:7860 byaidu/pdf2zh
   ```

2. Open in browser:

   ```
   http://localhost:7860/
   ```

For docker deployment on cloud service:

&lt;div&gt;
&lt;a href=&quot;https://www.heroku.com/deploy?template=https://github.com/Byaidu/PDFMathTranslate&quot;&gt;
  &lt;img src=&quot;https://www.herokucdn.com/deploy/button.svg&quot; alt=&quot;Deploy&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://render.com/deploy&quot;&gt;
  &lt;img src=&quot;https://render.com/images/deploy-to-render-button.svg&quot; alt=&quot;Deploy to Koyeb&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://zeabur.com/templates/5FQIGX?referralCode=reycn&quot;&gt;
  &lt;img src=&quot;https://zeabur.com/button.svg&quot; alt=&quot;Deploy on Zeabur&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://template.sealos.io/deploy?templateName=pdf2zh&quot;&gt;
  &lt;img src=&quot;https://sealos.io/Deploy-on-Sealos.svg&quot; alt=&quot;Deploy on Sealos&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://app.koyeb.com/deploy?type=git&amp;builder=buildpack&amp;repository=github.com/Byaidu/PDFMathTranslate&amp;branch=main&amp;name=pdf-math-translate&quot;&gt;
  &lt;img src=&quot;https://www.koyeb.com/static/images/deploy/button.svg&quot; alt=&quot;Deploy to Koyeb&quot; height=&quot;26&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

&gt; [!TIP]
&gt;
&gt; - If you cannot access Docker Hub, please try the image on [GitHub Container Registry](https://github.com/Byaidu/PDFMathTranslate/pkgs/container/pdfmathtranslate).
&gt; ```bash
&gt; docker pull ghcr.io/byaidu/pdfmathtranslate
&gt; docker run -d -p 7860:7860 ghcr.io/byaidu/pdfmathtranslate
&gt; ```
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;3.2.* Solutions for network issues in installation&lt;/summary&gt;

  Users in specific regions may encounter network difficulties when loading the AI model. The current program relies on the AI model (`wybxc/DocLayout-YOLO-DocStructBench-onnx`), and some users are unable to download it due to these network issues.

  To address issues with downloading this model, use the following environment variable as a workaround:

  ```shell
  set HF_ENDPOINT=https://hf-mirror.com
  ```

  For PowerShell user:

  ```shell
  $env:HF_ENDPOINT = https://hf-mirror.com
  ```

  If the solution does not work to you / you encountered other issues, please refer to [Frequently Asked Questions](https://github.com/Byaidu/PDFMathTranslate/wiki#-faq--%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98).
&lt;/details&gt;


&lt;h2 id=&quot;usage&quot;&gt;4. Technical Details&lt;/h2&gt;

### 4.1 Advanced options

Execute the translation command in the command line to generate the translated document `example-mono.pdf` and the bilingual document `example-dual.pdf` in the current working directory. Use Google as the default translation service. More support translation services can find [HERE](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#services).

&lt;img src=&quot;./docs/images/cmd.explained.png&quot; width=&quot;580px&quot;  alt=&quot;cmd&quot;/&gt;

In the following table, we list all advanced options for reference:

| Option                | Function                                                                                                      | Example                                        |
| --------------------- | ------------------------------------------------------------------------------------------------------------- | ---------------------------------------------- |
| files                 | Local files                                                                                                   | `pdf2zh ~/local.pdf`                           |
| links                 | Online files                                                                                                  | `pdf2zh http://arxiv.org/paper.pdf`            |
| `-i`                  | [Enter GUI](#gui)                                                                                             | `pdf2zh -i`                                    |
| `-p`                  | [Partial document translation](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#partial) | `pdf2zh example.pdf -p 1`                      |
| `-li`                 | [Source language](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#languages)            | `pdf2zh example.pdf -li en`                    |
| `-lo`                 | [Target language](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#languages)            | `pdf2zh example.pdf -lo zh`                    |
| `-s`                  | [Translation service](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#services)         | `pdf2zh example.pdf -s deepl`                  |
| `-t`                  | [Multi-threads](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#threads)                | `pdf2zh example.pdf -t 1`                      |
| `-o`                  | Output dir                                                                                                    | `pdf2zh example.pdf -o output`                 |
| `-f`, `-c`            | [Exceptions](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#exceptions)                | `pdf2zh example.pdf -f &quot;(MS.*)&quot;`               |
| `-cp`                 | Compatibility Mode                                                                                            | `pdf2zh example.pdf --compatible`              |
| `--skip-subset-fonts` | [Skip font subset](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#font-subset)         | `pdf2zh example.pdf --skip-subset-fonts`       |
| `--ignore-cache`      | [Ignore translate cache](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#cache)         | `pdf2zh example.pdf --ignore-cache`            |
| `--share`             | Public link                                                                                                   | `pdf2zh -i --share`                            |
| `--authorized`        | [Authorization](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#auth)                   | `pdf2zh -i --authorized users.txt [auth.html]` |
| `--prompt`            | [Custom Prompt](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#prompt)                 | `pdf2zh --prompt [prompt.txt]`                 |
| `--onnx`              | [Use Custom DocLayout-YOLO ONNX model]                                                                        | `pdf2zh --onnx [onnx/model/path]`              |
| `--serverport`        | [Use Custom WebUI port]                                                                                       | `pdf2zh --serverport 7860`                     |
| `--dir`               | [batch translate]                                                                                             | `pdf2zh --dir /path/to/translate/`             |
| `--config`            | [configuration file](https://github.com/Byaidu/PDFMathTranslate/blob/main/docs/ADVANCED.md#cofig)             | `pdf2zh --config /path/to/config/config.json`  |
| `--serverport`        | [custom gradio server port]                                                                                   | `pdf2zh --serverport 7860`                     |
| `--babeldoc`          | Use Experimental backend [BabelDOC](https://funstory-ai.github.io/BabelDOC/) to translate                     | `pdf2zh --babeldoc` -s openai example.pdf      |
| `--mcp`               | Enable MCP STDIO mode                                                                                         | `pdf2zh --mcp`                                 |
| `--sse`               | Enable MCP SSE mode                                                                                           | `pdf2zh --mcp --sse`                           |

For detailed explanations, please refer to our document about [Advanced Usage](./docs/ADVANCED.md) for a full list of each option.

&lt;h3 id=&quot;downstream&quot;&gt;4.2 Downstream Development&lt;/h3&gt;
For downstream applications, please refer to our document about [API Details](./docs/APIS.md) for further information about:

- [Python API](./docs/APIS.md#api-python), how to use the program in other Python programs
- [HTTP API](./docs/APIS.md#api-http), how to communicate with a server with the program installed

&lt;h3 id=&quot;downstream&quot;&gt;4.3 Differences between two major forks&lt;/h3&gt;

- [Byaidu/PDFMathTranslate](https://github.com/Byaidu/PDFMathTranslate): The present and the original project for stable release.

- [PDFMathTranslate/PDFMathTranslate-next](https://github.com/PDFMathTranslate/PDFMathTranslate-next): A fork with web-ui and additional features. This fork handles a large number of marginal cases, improves PDF compatibility, and optimizes cross-column and cross-page semantic consistency, dynamic scaling, and dynamic scaling consistency, among many other translation quality improvements. However, this fork is intended solely for development and does not address compatibility issues and is not designed for community-contributions.

&lt;h2 id=&quot;information&quot;&gt;5. Project Information&lt;/h2&gt;
&lt;h3 id=&quot;citation&quot;&gt;5.1 Citation&lt;/h3&gt;

This work has been accepted by the *Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations* (EMNLP 2025). 

- Pre-print version: [PDFMathTranslate: Scientific Document Translation Preserving Layouts](https://arxiv.org/abs/2507.03009)

  ```
  @online{ouyang2025pdfmathtranslate,
    title = {{{PDFMathTranslate}}: {{Scientific Document Translation Preserving Layouts}}},
    shorttitle = {{{PDFMathTranslate}}},
    author = {Ouyang, Rongxin and Chu, Chang and Xin, Zhikuang and Ma, Xiangyao},
    date = {2025-07-08},
    eprint = {2507.03009},
    eprinttype = {arXiv},
    eprintclass = {cs},
    doi = {10.48550/arXiv.2507.03009},
    url = {http://arxiv.org/abs/2507.03009},
    urldate = {2025-08-27},
    pubstate = {prepublished}
  }
  ```

- The citation for the EMNLP proceedings will be provided upon release.
&lt;!-- ```
@inproceedings{zheng-etal-2024-openresearcher,
    title = &quot;{O}pen{R}esearcher: Unleashing {AI} for Accelerated Scientific Research&quot;,
    author = &quot;Ouyang, Rongxin  and
      Chu, Chang and
      Xin, Zhikuang and
      Ma, Xiangyao&quot;,
    editor = &quot;TBD&quot;,
    booktitle = &quot;Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations&quot;,
    month = nov,
    year = &quot;2025&quot;,
    address = &quot;Miami, Florida, USA&quot;,
    publisher = &quot;Association for Computational Linguistics&quot;,
    url = &quot;https://aclanthology.org/TBD/&quot;,
    doi = &quot;TBD&quot;,
    pages = &quot;TBD&quot;,
    abstract = &quot;Language barriers in scientific documents hinder the diffusion and development of science and technologies. However, prior efforts in translating such documents largely overlooked the information in layouts. To bridge the gap, we introduce PDFMathTranslate, the worldâ€™s first open-source software for translating scientific documents while preserving layouts. Leveraging the most recent advances in large language models and precise layout detection, we contribute to the community with key improvements in precision, flexibility, and efficiency. The work is open-sourced at https://github.com/byaidu/pdfmathtranslate with more than 222k downloads.&quot;
}
``` --&gt;

&lt;h3 id=&quot;acknowledgement&quot;&gt;5.2 Acknowledgement&lt;/h3&gt;

- [Immersive Translation](https://immersivetranslate.com) sponsors monthly Pro membership redemption codes for active contributors to this project, see details at: [CONTRIBUTOR_REWARD.md](https://github.com/funstory-ai/BabelDOC/blob/main/docs/CONTRIBUTOR_REWARD.md)

- New backend: [BabelDOC](https://github.com/funstory-ai/BabelDOC)

- Document merging: [PyMuPDF](https://github.com/pymupdf/PyMuPDF)

- Document parsing: [Pdfminer.six](https://github.com/pdfminer/pdfminer.six)

- Document extraction: [MinerU](https://github.com/opendatalab/MinerU)

- Document Preview: [Gradio PDF](https://github.com/freddyaboulton/gradio-pdf)

- Multi-threaded translation: [MathTranslate](https://github.com/SUSYUSTC/MathTranslate)

- Layout parsing: [DocLayout-YOLO](https://github.com/opendatalab/DocLayout-YOLO)

- Document standard: [PDF Explained](https://zxyle.github.io/PDF-Explained/), [PDF Cheat Sheets](https://pdfa.org/resource/pdf-cheat-sheets/)

- Multilingual Font: [Go Noto Universal](https://github.com/satbyy/go-noto-universal)

&lt;h3 id=&quot;contrib&quot;&gt;5.3 Contributors&lt;/h3&gt;

&lt;a href=&quot;https://github.com/Byaidu/PDFMathTranslate/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://opencollective.com/PDFMathTranslate/contributors.svg?width=890&amp;button=false&quot; /&gt;
&lt;/a&gt;

![Alt](https://repobeats.axiom.co/api/embed/dfa7583da5332a11468d686fbd29b92320a6a869.svg &quot;Repobeats analytics image&quot;)

For details on how to contribute, please consult the [Contribution Guide](https://github.com/Byaidu/PDFMathTranslate/wiki/Contribution-Guide---%E8%B4%A1%E7%8C%AE%E6%8C%87%E5%8D%97).


&lt;h3 id=&quot;star_hist&quot;&gt;5.4 Star History&lt;/h3&gt;

&lt;a href=&quot;https://star-history.com/#Byaidu/PDFMathTranslate&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=Byaidu/PDFMathTranslate&amp;type=Date&quot;/&gt;
 &lt;/picture&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[PostHog/posthog]]></title>
            <link>https://github.com/PostHog/posthog</link>
            <guid>https://github.com/PostHog/posthog</guid>
            <pubDate>Thu, 02 Oct 2025 00:04:09 GMT</pubDate>
            <description><![CDATA[ğŸ¦” PostHog provides open-source web & product analytics, session recording, feature flagging and A/B testing that you can self-host. Get started - free.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/PostHog/posthog">PostHog/posthog</a></h1>
            <p>ğŸ¦” PostHog provides open-source web & product analytics, session recording, feature flagging and A/B testing that you can self-host. Get started - free.</p>
            <p>Language: Python</p>
            <p>Stars: 29,488</p>
            <p>Forks: 1,960</p>
            <p>Stars today: 23 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img alt=&quot;posthoglogo&quot; src=&quot;https://user-images.githubusercontent.com/65415371/205059737-c8a4f836-4889-4654-902e-f302b187b6a0.png&quot;&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&#039;https://posthog.com/contributors&#039;&gt;&lt;img alt=&quot;GitHub contributors&quot; src=&quot;https://img.shields.io/github/contributors/posthog/posthog&quot;/&gt;&lt;/a&gt;
  &lt;a href=&#039;http://makeapullrequest.com&#039;&gt;&lt;img alt=&#039;PRs Welcome&#039; src=&#039;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=shields&#039;/&gt;&lt;/a&gt;
  &lt;img alt=&quot;Docker Pulls&quot; src=&quot;https://img.shields.io/docker/pulls/posthog/posthog&quot;/&gt;
  &lt;a href=&quot;https://github.com/PostHog/posthog/commits/master&quot;&gt;&lt;img alt=&quot;GitHub commit activity&quot; src=&quot;https://img.shields.io/github/commit-activity/m/posthog/posthog&quot;/&gt; &lt;/a&gt;
  &lt;a href=&quot;https://github.com/PostHog/posthog/issues?q=is%3Aissue%20state%3Aclosed&quot;&gt;&lt;img alt=&quot;GitHub closed issues&quot; src=&quot;https://img.shields.io/github/issues-closed/posthog/posthog&quot;/&gt; &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://posthog.com/docs&quot;&gt;Docs&lt;/a&gt; - &lt;a href=&quot;https://posthog.com/community&quot;&gt;Community&lt;/a&gt; - &lt;a href=&quot;https://posthog.com/roadmap&quot;&gt;Roadmap&lt;/a&gt; - &lt;a href=&quot;https://posthog.com/why&quot;&gt;Why PostHog?&lt;/a&gt; - &lt;a href=&quot;https://posthog.com/changelog&quot;&gt;Changelog&lt;/a&gt; - &lt;a href=&quot;https://github.com/PostHog/posthog/issues/new?assignees=&amp;labels=bug&amp;template=bug_report.md&quot;&gt;Bug reports&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=2jQco8hEvTI&quot;&gt;
    &lt;img src=&quot;https://res.cloudinary.com/dmukukwp6/image/upload/demo_thumb_68d0d8d56d&quot; alt=&quot;PostHog Demonstration&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

## PostHog is an all-in-one, open source platform for building successful products

[PostHog](https://posthog.com/) provides every tool you need to build a successful product including:

- [Product analytics](https://posthog.com/product-analytics): Autocapture or manually instrument event-based analytics to understand user behavior and analyze data with visualization or SQL.
- [Web analytics](https://posthog.com/web-analytics): Monitor web traffic and user sessions with a GA-like dashboard. Easily monitor conversion, web vitals, and revenue.
- [Session replays](https://posthog.com/session-replay): Watch real user sessions of interactions with your website or mobile app to diagnose issues and understand user behavior.
- [Feature flags](https://posthog.com/feature-flags): Safely roll out features to select users or cohorts with feature flags.
- [Experiments](https://posthog.com/experiments): Test changes and measure their statistical impact on goal metrics. Set up experiments with no-code too.
- [Error tracking](https://posthog.com/error-tracking): Track errors, get alerts, and resolve issues to improve your product.
- [Surveys](https://posthog.com/surveys): Ask anything with our collection of no-code survey templates, or build custom surveys with our survey builder.
- [Data warehouse](https://posthog.com/data-warehouse): Sync data from external tools like Stripe, Hubspot, your data warehouse, and more. Query it alongside your product data.
- [Data pipelines](https://posthog.com/cdp): Run custom filters and transformations on your incoming data. Send it to 25+ tools or any webhook in real time or batch export large amounts to your warehouse.
- [LLM analytics](https://posthog.com/docs/llm-analytics): Capture traces, generations, latency, and cost for your LLM-powered app.

Best of all, all of this is free to use with a [generous monthly free tier](https://posthog.com/pricing) for each product. Get started by signing up for [PostHog Cloud US](https://us.posthog.com/signup) or [PostHog Cloud EU](https://eu.posthog.com/signup).

## Table of Contents

- [PostHog is an all-in-one, open source platform for building successful products](#posthog-is-an-all-in-one-open-source-platform-for-building-successful-products)
- [Table of Contents](#table-of-contents)
- [Getting started with PostHog](#getting-started-with-posthog)
    - [PostHog Cloud (Recommended)](#posthog-cloud-recommended)
    - [Self-hosting the open-source hobby deploy (Advanced)](#self-hosting-the-open-source-hobby-deploy-advanced)
- [Setting up PostHog](#setting-up-posthog)
- [Learning more about PostHog](#learning-more-about-posthog)
- [Contributing](#contributing)
- [Open-source vs. paid](#open-source-vs-paid)
- [Weâ€™re hiring!](#were-hiring)

## Getting started with PostHog

### PostHog Cloud (Recommended)

The fastest and most reliable way to get started with PostHog is signing up for free toÂ [PostHog Cloud](https://us.posthog.com/signup) or [PostHog Cloud EU](https://eu.posthog.com/signup). Your first 1 million events, 5k recordings, 1M flag requests, 100k exceptions, and 1500 survey responses are free every month, after which you pay based on usage.

### Self-hosting the open-source hobby deploy (Advanced)

If you want to self-host PostHog, you can deploy a hobby instance in one line on Linux with Docker (recommended 4GB memory):

```bash
/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/posthog/posthog/HEAD/bin/deploy-hobby)&quot;
```

Open source deployments should scale to approximately 100k events per month, after which we recommend [migrating to a PostHog Cloud](https://posthog.com/docs/migrate/migrate-to-cloud).

We _do not_ provide customer support or offer guarantees for open source deployments. See our [self-hosting docs](https://posthog.com/docs/self-host), [troubleshooting guide](https://posthog.com/docs/self-host/deploy/troubleshooting), and [disclaimer](https://posthog.com/docs/self-host/open-source/disclaimer) for more info.

## Setting up PostHog

Once you&#039;ve got a PostHog instance, you can set it up by installing our [JavaScript web snippet](https://posthog.com/docs/getting-started/install?tab=snippet), one of [our SDKs](https://posthog.com/docs/getting-started/install?tab=sdks), or by [using our API](https://posthog.com/docs/getting-started/install?tab=api).

We have SDKs and libraries for popular languages and frameworks like:

| Frontend                                              | Mobile                                                          | Backend                                             |
| ----------------------------------------------------- | --------------------------------------------------------------- | --------------------------------------------------- |
| [JavaScript](https://posthog.com/docs/libraries/js)   | [React Native](https://posthog.com/docs/libraries/react-native) | [Python](https://posthog.com/docs/libraries/python) |
| [Next.js](https://posthog.com/docs/libraries/next-js) | [Android](https://posthog.com/docs/libraries/android)           | [Node](https://posthog.com/docs/libraries/node)     |
| [React](https://posthog.com/docs/libraries/react)     | [iOS](https://posthog.com/docs/libraries/ios)                   | [PHP](https://posthog.com/docs/libraries/php)       |
| [Vue](https://posthog.com/docs/libraries/vue-js)      | [Flutter](https://posthog.com/docs/libraries/flutter)           | [Ruby](https://posthog.com/docs/libraries/ruby)     |

Beyond this, we have docs and guides for [Go](https://posthog.com/docs/libraries/go), [.NET/C#](https://posthog.com/docs/libraries/dotnet), [Django](https://posthog.com/docs/libraries/django), [Angular](https://posthog.com/docs/libraries/angular), [WordPress](https://posthog.com/docs/libraries/wordpress), [Webflow](https://posthog.com/docs/libraries/webflow), and more.

Once you&#039;ve installed PostHog, see our [product docs](https://posthog.com/docs/product-os) for more information on how to set up [product analytics](https://posthog.com/docs/product-analytics/capture-events), [web analytics](https://posthog.com/docs/web-analytics/getting-started), [session replays](https://posthog.com/docs/session-replay/how-to-watch-recordings), [feature flags](https://posthog.com/docs/feature-flags/creating-feature-flags), [experiments](https://posthog.com/docs/experiments/creating-an-experiment), [error tracking](https://posthog.com/docs/error-tracking/installation#setting-up-exception-autocapture), [surveys](https://posthog.com/docs/surveys/installation), [data warehouse](https://posthog.com/docs/cdp/sources), and more.

## Learning more about PostHog

Our code isn&#039;t the only thing that&#039;s open source ğŸ˜³. We also open source our [company handbook](https://posthog.com/handbook) which details our [strategy](https://posthog.com/handbook/why-does-posthog-exist), [ways of working](https://posthog.com/handbook/company/culture), and [processes](https://posthog.com/handbook/team-structure).

Curious about how to make the most of PostHog? We wrote a guide to [winning with PostHog](https://posthog.com/docs/new-to-posthog/getting-hogpilled) which walks you through the basics of [measuring activation](https://posthog.com/docs/new-to-posthog/activation), [tracking retention](https://posthog.com/docs/new-to-posthog/retention), and [capturing revenue](https://posthog.com/docs/new-to-posthog/revenue).

## Contributing

We &lt;3 contributions big and small:

- Vote on features or get early access to beta functionality in our [roadmap](https://posthog.com/roadmap)
- Open a PR (see our instructions on [developing PostHog locally](https://posthog.com/handbook/engineering/developing-locally))
- Submit a [feature request](https://github.com/PostHog/posthog/issues/new?assignees=&amp;labels=enhancement%2C+feature&amp;template=feature_request.md) or [bug report](https://github.com/PostHog/posthog/issues/new?assignees=&amp;labels=bug&amp;template=bug_report.md)

## Open-source vs. paid

This repo is available under the [MIT expat license](https://github.com/PostHog/posthog/blob/master/LICENSE), except for the `ee` directory (which has its [license here](https://github.com/PostHog/posthog/blob/master/ee/LICENSE)) if applicable.

Need _absolutely ğŸ’¯% FOSS_? Check out our [posthog-foss](https://github.com/PostHog/posthog-foss) repository, which is purged of all proprietary code and features.

The pricing for our paid plan is completely transparent and available on [our pricing page](https://posthog.com/pricing).

## Weâ€™re hiring!

&lt;img src=&quot;https://res.cloudinary.com/dmukukwp6/image/upload/v1/posthog.com/src/components/Home/images/mission-control-hog&quot; alt=&quot;Hedgehog working on a Mission Control Center&quot; width=&quot;350px&quot;/&gt;

Hey! If you&#039;re reading this, you&#039;ve proven yourself as a dedicated README reader.

You might also make a great addition to our team. We&#039;re growing fast [and would love for you to join us](https://posthog.com/careers).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[frappe/erpnext]]></title>
            <link>https://github.com/frappe/erpnext</link>
            <guid>https://github.com/frappe/erpnext</guid>
            <pubDate>Thu, 02 Oct 2025 00:04:08 GMT</pubDate>
            <description><![CDATA[Free and Open Source Enterprise Resource Planning (ERP)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/frappe/erpnext">frappe/erpnext</a></h1>
            <p>Free and Open Source Enterprise Resource Planning (ERP)</p>
            <p>Language: Python</p>
            <p>Stars: 29,351</p>
            <p>Forks: 9,485</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/frappe/design/blob/master/logos/logo-2019/erpnext-logo.png&quot; height=&quot;128&quot;&gt;
    &lt;h2&gt;ERPNext&lt;/h2&gt;
    &lt;p align=&quot;center&quot;&gt;
        &lt;p&gt;ERP made simple&lt;/p&gt;
    &lt;/p&gt;

[![Build Status](https://travis-ci.com/frappe/erpnext.png)](https://travis-ci.com/frappe/erpnext)
[![Open Source Helpers](https://www.codetriage.com/frappe/erpnext/badges/users.svg)](https://www.codetriage.com/frappe/erpnext)
[![Coverage Status](https://coveralls.io/repos/github/frappe/erpnext/badge.svg?branch=develop)](https://coveralls.io/github/frappe/erpnext?branch=develop)

[https://erpnext.com](https://erpnext.com)

&lt;/div&gt;

Includes: Accounting, Inventory, Manufacturing, CRM, Sales, Purchase, Project Management, HRMS. Requires MariaDB.

ERPNext is built on the [Frappe](https://github.com/frappe/frappe) Framework, a full-stack web app framework in Python &amp; JavaScript.

- [User Guide](https://erpnext.com/docs/user)
- [Discussion Forum](https://discuss.erpnext.com/)

---

### Full Install

The Easy Way: our install script for bench will install all dependencies (e.g. MariaDB). See https://github.com/frappe/bench for more details.

New passwords will be created for the ERPNext &quot;Administrator&quot; user, the MariaDB root user, and the frappe user (the script displays the passwords and saves them to ~/frappe_passwords.txt).

### Virtual Image

You can download a virtual image to run ERPNext in a virtual machine on your local system.

- [ERPNext Download](http://erpnext.com/download)

System and user credentials are listed on the download page.

---

## License

GNU/General Public License (see [license.txt](license.txt))

The ERPNext code is licensed as GNU General Public License (v3) and the Documentation is licensed as Creative Commons (CC-BY-SA-3.0) and the copyright is owned by Frappe Technologies Pvt Ltd (Frappe) and Contributors.

---

## Contributing

1. [Issue Guidelines](https://github.com/frappe/erpnext/wiki/Issue-Guidelines)
1. [Report Security Vulnerabilities](https://erpnext.com/report)
1. [Pull Request Requirements](https://github.com/frappe/erpnext/wiki/Contribution-Guidelines)
1. [Translations](https://translate.erpnext.com)
1. [Chart of Accounts](https://charts.erpnext.com)

---

## Logo and Trademark

The brand name ERPNext and the logo are trademarks of Frappe Technologies Pvt. Ltd.

### Introduction

Frappe Technologies Pvt. Ltd. (Frappe) owns and oversees the trademarks for the ERPNext name and logos. We have developed this trademark usage policy with the following goals in mind:

- Weâ€™d like to make it easy for anyone to use the ERPNext name or logo for community-oriented efforts that help spread and improve ERPNext.
- Weâ€™d like to make it clear how ERPNext-related businesses and projects can (and cannot) use the ERPNext name and logo.
- Weâ€™d like to make it hard for anyone to use the ERPNext name and logo to unfairly profit from, trick or confuse people who are looking for official ERPNext resources.

### Frappe Trademark Usage Policy

Permission from Frappe is required to use the ERPNext name or logo as part of any project, product, service, domain or company name.

We will grant permission to use the ERPNext name and logo for projects that meet the following criteria:

- The primary purpose of your project is to promote the spread and improvement of the ERPNext software.
- Your project is non-commercial in nature (it can make money to cover its costs or contribute to non-profit entities, but it cannot be run as a for-profit project or business).
Your project neither promotes nor is associated with entities that currently fail to comply with the GPL license under which ERPNext is distributed.
- If your project meets these criteria, you will be permitted to use the ERPNext name and logo to promote your project in any way you see fit with one exception: Please do not use ERPNext as part of a domain name.

Use of the ERPNext name and logo is additionally allowed in the following situations:

All other ERPNext-related businesses or projects can use the ERPNext name and logo to refer to and explain their services, but they cannot use them as part of a product, project, service, domain, or company name and they cannot use them in any way that suggests an affiliation with or endorsement by ERPNext or Frappe Technologies or the ERPNext open source project. For example, a consulting company can describe its business as â€œ123 Web Services, offering ERPNext consulting for small businesses,â€ but cannot call its business â€œThe ERPNext Consulting Company.â€

Similarly, itâ€™s OK to use the ERPNext logo as part of a page that describes your products or services, but it is not OK to use it as part of your company or product logo or branding itself. Under no circumstances is it permitted to use ERPNext as part of a top-level domain name.

We do not allow the use of the trademark in advertising, including AdSense/AdWords.

Please note that it is not the goal of this policy to limit commercial activity around ERPNext. We encourage ERPNext-based businesses, and we would love to see hundreds of them.

When in doubt about your use of the ERPNext name or logo, please contact Frappe Technologies for clarification.

(inspired by WordPress)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jsvine/pdfplumber]]></title>
            <link>https://github.com/jsvine/pdfplumber</link>
            <guid>https://github.com/jsvine/pdfplumber</guid>
            <pubDate>Thu, 02 Oct 2025 00:04:07 GMT</pubDate>
            <description><![CDATA[Plumb a PDF for detailed information about each char, rectangle, line, et cetera â€”Â and easily extract text and tables.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jsvine/pdfplumber">jsvine/pdfplumber</a></h1>
            <p>Plumb a PDF for detailed information about each char, rectangle, line, et cetera â€”Â and easily extract text and tables.</p>
            <p>Language: Python</p>
            <p>Stars: 8,761</p>
            <p>Forks: 796</p>
            <p>Stars today: 44 stars today</p>
            <h2>README</h2><pre># pdfplumber

[![Version](https://img.shields.io/pypi/v/pdfplumber.svg)](https://pypi.python.org/pypi/pdfplumber) ![Tests](https://github.com/jsvine/pdfplumber/workflows/Tests/badge.svg) [![Code coverage](https://codecov.io/gh/jsvine/pdfplumber/branch/stable/graph/badge.svg)](https://codecov.io/gh/jsvine/pdfplumber/branch/stable) [![Support Python versions](https://img.shields.io/pypi/pyversions/pdfplumber.svg)](https://pypi.python.org/pypi/pdfplumber)

Plumb a PDF for detailed information about each text character, rectangle, and line. Plus: Table extraction and visual debugging.

Works best on machine-generated, rather than scanned, PDFs. Built on [`pdfminer.six`](https://github.com/goulu/pdfminer). 

Currently [tested](tests/) on [Python 3.8, 3.9, 3.10, 3.11](.github/workflows/tests.yml).

Translations of this document are available in: [Chinese (by @hbh112233abc)](https://github.com/hbh112233abc/pdfplumber/blob/stable/README-CN.md).

__To report a bug__ or request a feature, please [file an issue](https://github.com/jsvine/pdfplumber/issues/new/choose). __To ask a question__ or request assistance with a specific PDF, please [use the discussions forum](https://github.com/jsvine/pdfplumber/discussions).

## Table of Contents

- [Installation](#installation)
- [Command line interface](#command-line-interface)
- [Python library](#python-library)
- [Visual debugging](#visual-debugging)
- [Extracting text](#extracting-text)
- [Extracting tables](#extracting-tables)
- [Extracting form values](#extracting-form-values)
- [Demonstrations](#demonstrations)
- [Comparison to other libraries](#comparison-to-other-libraries)
- [Acknowledgments / Contributors](#acknowledgments--contributors)
- [Contributing](#contributing)

## Installation

```sh
pip install pdfplumber
```

## Command line interface

### Basic example

```sh
curl &quot;https://raw.githubusercontent.com/jsvine/pdfplumber/stable/examples/pdfs/background-checks.pdf&quot; &gt; background-checks.pdf
pdfplumber background-checks.pdf &gt; background-checks.csv
```

The output will be a CSV containing info about every character, line, and rectangle in the PDF.

### Options

| Argument | Description |
|----------|-------------|
|`--format [format]`| `csv`, `json`, or `text`. The `csv` and `json` formats return information about each object. Of those two, the `json` format returns more information; it includes PDF-level and page-level metadata, plus dictionary-nested attributes. The `text` option returns a plain-text representation of the PDF, using `Page.extract_text(layout=True)`.|
|`--pages [list of pages]`| A space-delimited, `1`-indexed list of pages or hyphenated page ranges. E.g., `1, 11-15`, which would return data for pages 1, 11, 12, 13, 14, and 15.|
|`--types [list of object types to extract]`| Choices are `char`, `rect`, `line`, `curve`, `image`, `annot`, et cetera. Defaults to all available.|
|`--laparams`| A JSON-formatted string (e.g., `&#039;{&quot;detect_vertical&quot;: true}&#039;`) to pass to `pdfplumber.open(..., laparams=...)`.|
|`--precision [integer]`| The number of decimal places to round floating-point numbers. Defaults to no rounding.|

## Python library

### Basic example

```python
import pdfplumber

with pdfplumber.open(&quot;path/to/file.pdf&quot;) as pdf:
    first_page = pdf.pages[0]
    print(first_page.chars[0])
```

### Loading a PDF

To start working with a PDF, call `pdfplumber.open(x)`, where `x` can be a:

- path to your PDF file
- file object, loaded as bytes
- file-like object, loaded as bytes

The `open` method returns an instance of the `pdfplumber.PDF` class.

To load a password-protected PDF, pass the `password` keyword argument, e.g., `pdfplumber.open(&quot;file.pdf&quot;, password = &quot;test&quot;)`.

To set layout analysis parameters to `pdfminer.six`&#039;s layout engine, pass the `laparams` keyword argument, e.g., `pdfplumber.open(&quot;file.pdf&quot;, laparams = { &quot;line_overlap&quot;: 0.7 })`.

To [pre-normalize Unicode text](https://unicode.org/reports/tr15/), pass `unicode_norm=...`, where `...` is one of the [four Unicode normalization forms](https://unicode.org/reports/tr15/#Normalization_Forms_Table): `&quot;NFC&quot;`, `&quot;NFD&quot;`, `&quot;NFKC&quot;`, or `&quot;NFKD&quot;`.

Invalid metadata values are treated as a warning by default. If that is not intended, pass `strict_metadata=True` to the `open` method and `pdfplumber.open` will raise an exception if it is unable to parse the metadata.

### The `pdfplumber.PDF` class

The top-level `pdfplumber.PDF` class represents a single PDF and has two main properties:

| Property | Description |
|----------|-------------|
|`.metadata`| A dictionary of metadata key/value pairs, drawn from the PDF&#039;s `Info` trailers. Typically includes &quot;CreationDate,&quot; &quot;ModDate,&quot; &quot;Producer,&quot; et cetera.|
|`.pages`| A list containing one `pdfplumber.Page` instance per page loaded.|

... and also has the following method:

| Method | Description |
|--------|-------------|
|`.close()`| Calling this method calls `Page.close()` on each page, and also closes the file stream (except in cases when the stream is external, i.e., already opened and passed directly to `pdfplumber`). |

### The `pdfplumber.Page` class

The `pdfplumber.Page` class is at the core of `pdfplumber`. Most things you&#039;ll do with `pdfplumber` will revolve around this class. It has these main properties:

| Property | Description |
|----------|-------------|
|`.page_number`| The sequential page number, starting with `1` for the first page, `2` for the second, and so on.|
|`.width`| The page&#039;s width.|
|`.height`| The page&#039;s height.|
|`.objects` / `.chars` / `.lines` / `.rects` / `.curves` / `.images`| Each of these properties is a list, and each list contains one dictionary for each such object embedded on the page. For more detail, see &quot;[Objects](#objects)&quot; below.|

... and these main methods:

| Method | Description |
|--------|-------------|
|`.crop(bounding_box, relative=False, strict=True)`| Returns a version of the page cropped to the bounding box, which should be expressed as 4-tuple with the values `(x0, top, x1, bottom)`. Cropped pages retain objects that fall at least partly within the bounding box. If an object falls only partly within the box, its dimensions are sliced to fit the bounding box. If `relative=True`, the bounding box is calculated as an offset from the top-left of the page&#039;s bounding box, rather than an absolute positioning. (See [Issue #245](https://github.com/jsvine/pdfplumber/issues/245) for a visual example and explanation.) When `strict=True` (the default), the crop&#039;s bounding box must fall entirely within the page&#039;s bounding box.|
|`.within_bbox(bounding_box, relative=False, strict=True)`| Similar to `.crop`, but only retains objects that fall *entirely within* the bounding box.|
|`.outside_bbox(bounding_box, relative=False, strict=True)`| Similar to `.crop` and `.within_bbox`, but only retains objects that fall *entirely outside* the bounding box.|
|`.filter(test_function)`| Returns a version of the page with only the `.objects` for which `test_function(obj)` returns `True`.|

... and also has the following method:

| Method | Description |
|--------|-------------|
|`.close()`| By default, `Page` objects cache their layout and object information to avoid having to reprocess it. When parsing large PDFs, however, these cached properties can require a lot of memory. You can use this method to flush the cache and release the memory.|

Additional methods are described in the sections below:

- [Visual debugging](#visual-debugging)
- [Extracting text](#extracting-text)
- [Extracting tables](#extracting-tables)

### Objects

Each instance of `pdfplumber.PDF` and `pdfplumber.Page` provides access to several types of PDF objects, all derived from [`pdfminer.six`](https://github.com/pdfminer/pdfminer.six/) PDF parsing. The following properties each return a Python list of the matching objects:

- `.chars`, each representing a single text character.
- `.lines`, each representing a single 1-dimensional line.
- `.rects`, each representing a single 2-dimensional rectangle.
- `.curves`, each representing any series of connected points that `pdfminer.six` does not recognize as a line or rectangle.
- `.images`, each representing an image.
- `.annots`, each representing a single PDF annotation (cf. Section 8.4 of the [official PDF specification](https://www.adobe.com/content/dam/acom/en/devnet/acrobat/pdfs/pdf_reference_1-7.pdf) for details)
- `.hyperlinks`, each representing a single PDF annotation of the subtype `Link` and having an `URI` action attribute

Each object is represented as a simple Python `dict`, with the following properties:

#### `char` properties

| Property | Description |
|----------|-------------|
|`page_number`| Page number on which this character was found.|
|`text`| E.g., &quot;z&quot;, or &quot;Z&quot; or &quot; &quot;.|
|`fontname`| Name of the character&#039;s font face.|
|`size`| Font size.|
|`adv`| Equal to text width * the font size * scaling factor.|
|`upright`| Whether the character is upright.|
|`height`| Height of the character.|
|`width`| Width of the character.|
|`x0`| Distance of left side of character from left side of page.|
|`x1`| Distance of right side of character from left side of page.|
|`y0`| Distance of bottom of character from bottom of page.|
|`y1`| Distance of top of character from bottom of page.|
|`top`| Distance of top of character from top of page.|
|`bottom`| Distance of bottom of the character from top of page.|
|`doctop`| Distance of top of character from top of document.|
|`matrix`| The &quot;current transformation matrix&quot; for this character. (See below for details.)|
|`mcid`| The [marked content](https://ghostscript.com/~robin/pdf_reference17.pdf#page=850) section ID for this character if any (otherwise `None`). *Experimental attribute.*|
|`tag`| The [marked content](https://ghostscript.com/~robin/pdf_reference17.pdf#page=850) section tag for this character if any (otherwise `None`). *Experimental attribute.*|
|`ncs`|TKTK|
|`stroking_pattern`|TKTK|
|`non_stroking_pattern`|TKTK|
|`stroking_color`|The color of the character&#039;s outline (i.e., stroke). See [docs/colors.md](docs/colors.md) for details.|
|`non_stroking_color`|The character&#039;s interior color. See [docs/colors.md](docs/colors.md) for details.|
|`object_type`| &quot;char&quot;|

__Note__: A characterâ€™s `matrix` property represents the â€œcurrent transformation matrix,â€ as described in Section 4.2.2 of the [PDF Reference](https://ghostscript.com/~robin/pdf_reference17.pdf) (6th Ed.). The matrix controls the characterâ€™s scale, skew, and positional translation. Rotation is a combination of scale and skew, but in most cases can be considered equal to the x-axis skew. The `pdfplumber.ctm` submodule defines a class, `CTM`, that assists with these calculations. For instance:

```python
from pdfplumber.ctm import CTM
my_char = pdf.pages[0].chars[3]
my_char_ctm = CTM(*my_char[&quot;matrix&quot;])
my_char_rotation = my_char_ctm.skew_x
```

#### `line` properties

| Property | Description |
|----------|-------------|
|`page_number`| Page number on which this line was found.|
|`height`| Height of line.|
|`width`| Width of line.|
|`x0`| Distance of left-side extremity from left side of page.|
|`x1`| Distance of right-side extremity from left side of page.|
|`y0`| Distance of bottom extremity from bottom of page.|
|`y1`| Distance of top extremity bottom of page.|
|`top`| Distance of top of line from top of page.|
|`bottom`| Distance of bottom of the line from top of page.|
|`doctop`| Distance of top of line from top of document.|
|`linewidth`| Thickness of line.|
|`stroking_color`|The color of the line. See [docs/colors.md](docs/colors.md) for details.|
|`non_stroking_color`|The non-stroking color specified for the lineâ€™s path. See [docs/colors.md](docs/colors.md) for details.|
|`mcid`| The [marked content](https://ghostscript.com/~robin/pdf_reference17.pdf#page=850) section ID for this line if any (otherwise `None`). *Experimental attribute.*|
|`tag`| The [marked content](https://ghostscript.com/~robin/pdf_reference17.pdf#page=850) section tag for this line if any (otherwise `None`). *Experimental attribute.*|
|`object_type`| &quot;line&quot;|

#### `rect` properties

| Property | Description |
|----------|-------------|
|`page_number`| Page number on which this rectangle was found.|
|`height`| Height of rectangle.|
|`width`| Width of rectangle.|
|`x0`| Distance of left side of rectangle from left side of page.|
|`x1`| Distance of right side of rectangle from left side of page.|
|`y0`| Distance of bottom of rectangle from bottom of page.|
|`y1`| Distance of top of rectangle from bottom of page.|
|`top`| Distance of top of rectangle from top of page.|
|`bottom`| Distance of bottom of the rectangle from top of page.|
|`doctop`| Distance of top of rectangle from top of document.|
|`linewidth`| Thickness of line.|
|`stroking_color`|The color of the rectangle&#039;s outline. See [docs/colors.md](docs/colors.md) for details.|
|`non_stroking_color`|The rectangleâ€™s fill color. See [docs/colors.md](docs/colors.md) for details.|
|`mcid`| The [marked content](https://ghostscript.com/~robin/pdf_reference17.pdf#page=850) section ID for this rect if any (otherwise `None`). *Experimental attribute.*|
|`tag`| The [marked content](https://ghostscript.com/~robin/pdf_reference17.pdf#page=850) section tag for this rect if any (otherwise `None`). *Experimental attribute.*|
|`object_type`| &quot;rect&quot;|

#### `curve` properties

| Property | Description |
|----------|-------------|
|`page_number`| Page number on which this curve was found.|
|`pts`| A list of `(x, top)` tuples indicating the *points on the curve*.|
|`path`| A list of `(cmd, *(x, top))` tuples *describing the full path description*, including (for example) control points used in Bezier curves.|
|`height`| Height of curve&#039;s bounding box.|
|`width`| Width of curve&#039;s bounding box.|
|`x0`| Distance of curve&#039;s left-most point from left side of page.|
|`x1`| Distance of curve&#039;s right-most point from left side of the page.|
|`y0`| Distance of curve&#039;s lowest point from bottom of page.|
|`y1`| Distance of curve&#039;s highest point from bottom of page.|
|`top`| Distance of curve&#039;s highest point from top of page.|
|`bottom`| Distance of curve&#039;s lowest point from top of page.|
|`doctop`| Distance of curve&#039;s highest point from top of document.|
|`linewidth`| Thickness of line.|
|`fill`| Whether the shape defined by the curve&#039;s path is filled.|
|`stroking_color`|The color of the curve&#039;s outline. See [docs/colors.md](docs/colors.md) for details.|
|`non_stroking_color`|The curveâ€™s fill color. See [docs/colors.md](docs/colors.md) for details.|
|`dash`|A `([dash_array], dash_phase)` tuple describing the curve&#039;s dash style. See [Table 4.6 of the PDF specification](https://ghostscript.com/~robin/pdf_reference17.pdf#page=218) for details.|
|`mcid`| The [marked content](https://ghostscript.com/~robin/pdf_reference17.pdf#page=850) section ID for this curve if any (otherwise `None`). *Experimental attribute.*|
|`tag`| The [marked content](https://ghostscript.com/~robin/pdf_reference17.pdf#page=850) section tag for this curve if any (otherwise `None`). *Experimental attribute.*|
|`object_type`| &quot;curve&quot;|

#### Derived properties

Additionally, both `pdfplumber.PDF` and `pdfplumber.Page` provide access to several derived lists of objects: `.rect_edges` (which decomposes each rectangle into its four lines), `.curve_edges` (which does the same for `curve` objects), and `.edges` (which combines `.rect_edges`, `.curve_edges`, and `.lines`). 

#### `image` properties

*Note: Although the positioning and characteristics of `image` objects are available via `pdfplumber`, this library does not provide direct support for reconstructing image content. For that, please see [this suggestion](https://github.com/jsvine/pdfplumber/discussions/496#discussioncomment-1259772).*

| Property | Description |
|----------|-------------|
|`page_number`| Page number on which the image was found.|
|`height`| Height of the image.|
|`width`| Width of the image.|
|`x0`| Distance of left side of the image from left side of page.|
|`x1`| Distance of right side of the image from left side of page.|
|`y0`| Distance of bottom of the image from bottom of page.|
|`y1`| Distance of top of the image from bottom of page.|
|`top`| Distance of top of the image from top of page.|
|`bottom`| Distance of bottom of the image from top of page.|
|`doctop`| Distance of top of rectangle from top of document.|
|`srcsize`| The image original dimensions, as a `(width, height)` tuple.|
|`colorspace`| Color domain of the image (e.g., RGB).|
|`bits`| The number of bits per color component; e.g., 8 corresponds to 255 possible values for each color component (R, G, and B in an RGB color space).|
|`stream`| Pixel values of the image, as a `pdfminer.pdftypes.PDFStream` object.|
|`imagemask`| A nullable boolean; if `True`, &quot;specifies that the image data is to be used as a stencil mask for painting in the current color.&quot;|
|`name`| &quot;The name by which this image XObject is referenced in the XObject subdictionary of the current resource dictionary.&quot; [ğŸ”—](https://ghostscript.com/~robin/pdf_reference17.pdf#page=340) |
|`mcid`| The [marked content](https://ghostscript.com/~robin/pdf_reference17.pdf#page=850) section ID for this image if any (otherwise `None`). *Experimental attribute.*|
|`tag`| The [marked content](https://ghostscript.com/~robin/pdf_reference17.pdf#page=850) section tag for this image if any (otherwise `None`). *Experimental attribute.*|
|`object_type`| &quot;image&quot;|

### Obtaining higher-level layout objects via `pdfminer.six`

If you pass the `pdfminer.six`-handling `laparams` parameter to `pdfplumber.open(...)`, then each page&#039;s `.objects` dictionary will also contain `pdfminer.six`&#039;s higher-level layout objects, such as `&quot;textboxhorizontal&quot;`.


## Visual debugging

`pdfplumber`&#039;s visual debugging tools can be helpful in understanding the structure of a PDF and the objects that have been extracted from it.


### Creating a `PageImage` with `.to_image()`

To turn any page (including cropped pages) into an `PageImage` object, call `my_page.to_image()`. You can optionally pass *one* of the  following keyword arguments:

- `resolution`: The desired number pixels per inch. Default: `72`. Type: `int`.
- `width`: The desired image width in pixels. Default: unset, determined by `resolution`. Type: `int`.
- `height`: The desired image width in pixels. Default: unset, determined by `resolution`. Type: `int`.
- `antialias`: Whether to use antialiasing when creating the image. Setting to `True` creates images with less-jagged text and graphics, but with larger file sizes. Default: `False`. Type: `bool`.
- `force_mediabox`: Use the page&#039;s `.mediabox` dimensions, rather than the `.cropbox` dimensions. Default: `False`. Type: `bool`.

For instance:

```python
im = my_pdf.pages[0].to_image(resolution=150)
```

From a script or REPL, `im.show()` will open the image in your local image viewer. But `PageImage` objects also play nicely with Jupyter notebooks; they automatically render as cell outputs. For example:

![Visual debugging in Jupyter](examples/screenshots/visual-debugging-in-jupyter.png &quot;Visual debugging in Jupyter&quot;)

*Note*: `.to_image(...)` works as expected with `Page.crop(...)`/`CroppedPage` instances, but is unable to incorporate changes made via `Page.filter(...)`/`FilteredPage` instances.


### Basic `PageImage` methods

| Method | Description |
|--------|-------------|
|`im.reset()`| Clears anything you&#039;ve drawn so far.|
|`im.copy()`| Copies the image to a new `PageImage` object.|
|`im.show()`| Opens the image in your local image viewer.|
|`im.save(path_or_fileobject, format=&quot;PNG&quot;, quantize=True, colors=256, bits=8)`| Saves the annotated image as a PNG file. The default arguments quantize the image to a palette of 256 colors, saving the PNG with 8-bit color depth. You can disable quantization by passing `quantize=False` or adjust the size of the color palette by passing `colors=N`.|

### Drawing methods

You can pass explicit coordinates or any `pdfplu

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[volatilityfoundation/volatility3]]></title>
            <link>https://github.com/volatilityfoundation/volatility3</link>
            <guid>https://github.com/volatilityfoundation/volatility3</guid>
            <pubDate>Thu, 02 Oct 2025 00:04:06 GMT</pubDate>
            <description><![CDATA[Volatility 3.0 development]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/volatilityfoundation/volatility3">volatilityfoundation/volatility3</a></h1>
            <p>Volatility 3.0 development</p>
            <p>Language: Python</p>
            <p>Stars: 3,537</p>
            <p>Forks: 579</p>
            <p>Stars today: 20 stars today</p>
            <h2>README</h2><pre># Volatility 3: The volatile memory extraction framework

Volatility is the world&#039;s most widely used framework for extracting digital
artifacts from volatile memory (RAM) samples. The extraction techniques are
performed completely independent of the system being investigated but offer
visibility into the runtime state of the system. The framework is intended
to introduce people to the techniques and complexities associated with
extracting digital artifacts from volatile memory samples and provide a
platform for further work into this exciting area of research.

In 2019, the Volatility Foundation released a complete rewrite of the
framework, Volatility 3. The project was intended to address many of the
technical and performance challenges associated with the original
code base that became apparent over the previous 10 years. Another benefit
of the rewrite is that Volatility 3 could be released under a custom
license that was more aligned with the goals of the Volatility community,
the Volatility Software License (VSL). See the
[LICENSE](https://www.volatilityfoundation.org/license/vsl-v1.0) file for
more details.

## Quick Start

1. Install the required dependencies:

    ```shell
    pip install --user -e &quot;.[full]&quot;
    ```

2. See available options:

    ```shell
    vol -h
    ```

3. To get more information on a Windows memory sample and to make sure Volatility supports that sample type, run `vol -f &lt;imagepath&gt; windows.info`:

    ```shell
    vol -f /home/user/samples/stuxnet.vmem windows.info
    ```

4. Run some other plugins. The `-f` or `--single-location` is not strictly required, but most plugins expect a single sample.
Some also require/accept other options.  Run `vol &lt;plugin&gt; -h` for more information on a particular command.

## Installing

Volatility 3 requires Python 3.8.0 or later and is published on the [PyPi registry](https://pypi.org/project/volatility3).

```shell
pip install volatility3
```

If you want to use the latest development version of Volatility 3 we recommend you manually clone this repository and install an editable version of the project.
We recommend you use a virtual environment to keep installed dependencies separate from system packages.

The latest stable version of Volatility will always be the `stable` branch of the GitHub repository. The default branch is `develop`.

```shell
git clone https://github.com/volatilityfoundation/volatility3.git
cd volatility3/
python3 -m venv venv &amp;&amp; . venv/bin/activate
pip install -e &quot;.[dev]&quot;
```

## Symbol Tables

Symbol table packs for the various operating systems are available for download at:

&lt;https://downloads.volatilityfoundation.org/volatility3/symbols/windows.zip&gt;
&lt;https://downloads.volatilityfoundation.org/volatility3/symbols/mac.zip&gt;
&lt;https://downloads.volatilityfoundation.org/volatility3/symbols/linux.zip&gt;

The hashes to verify whether any of the symbol pack files have downloaded successfully or have changed can be found at:

&lt;https://downloads.volatilityfoundation.org/volatility3/symbols/SHA256SUMS&gt;
&lt;https://downloads.volatilityfoundation.org/volatility3/symbols/SHA1SUMS&gt;
&lt;https://downloads.volatilityfoundation.org/volatility3/symbols/MD5SUMS&gt;

Symbol tables zip files must be placed, as named, into the `volatility3/symbols` directory (or just the symbols directory next to the executable file).

Windows symbols that cannot be found will be queried, downloaded, generated and cached.  Mac and Linux symbol tables must be manually produced by a tool such as [dwarf2json](https://github.com/volatilityfoundation/dwarf2json).

Important: The first run of volatility with new symbol files will require the cache to be updated.  The symbol packs contain a large number of symbol files and so may take some time to update!
However, this process only needs to be run once on each new symbol file, so assuming the pack stays in the same location will not need to be done again.  Please also note it can be interrupted and next run will restart itself.

Please note: These are representative and are complete up to the point of creation for Windows and Mac.  Due to the ease of compiling Linux kernels and the inability to uniquely distinguish them, an exhaustive set of Linux symbol tables cannot easily be supplied.

## Documentation

The framework is documented through doc strings and can be built using sphinx.

The latest generated copy of the documentation can be found at: &lt;https://volatility3.readthedocs.io/en/latest/&gt;

## Licensing and Copyright

Copyright (C) 2007-2025 Volatility Foundation

All Rights Reserved

&lt;https://www.volatilityfoundation.org/license/vsl-v1.0&gt;

## Bugs and Support

If you think you&#039;ve found a bug, please report it at:

&lt;https://github.com/volatilityfoundation/volatility3/issues&gt;

In order to help us solve your issues as quickly as possible,
please include the following information when filing a bug:

- The version of Volatility you&#039;re using
- The operating system used to run Volatility
- The version of Python used to run Volatility
- The suspected operating system of the memory sample
- The complete command line you used to run Volatility

For community support, please join us on Slack:

&lt;https://www.volatilityfoundation.org/slack&gt;

## Contact

For information or requests, contact:

Volatility Foundation

Web: &lt;https://www.volatilityfoundation.org&gt;

Blog:     &lt;https://volatility-labs.blogspot.com&gt;

Email: volatility (at) volatilityfoundation (dot) org

Twitter: [@volatility](https://twitter.com/volatility)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[OpenBB-finance/OpenBB]]></title>
            <link>https://github.com/OpenBB-finance/OpenBB</link>
            <guid>https://github.com/OpenBB-finance/OpenBB</guid>
            <pubDate>Thu, 02 Oct 2025 00:04:05 GMT</pubDate>
            <description><![CDATA[Financial data platform for analysts, quants and AI agents.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenBB-finance/OpenBB">OpenBB-finance/OpenBB</a></h1>
            <p>Financial data platform for analysts, quants and AI agents.</p>
            <p>Language: Python</p>
            <p>Stars: 52,827</p>
            <p>Forks: 5,043</p>
            <p>Stars today: 156 stars today</p>
            <h2>README</h2><pre>&lt;br /&gt;
&lt;img src=&quot;https://github.com/OpenBB-finance/OpenBB/blob/develop/images/platform-light.svg?raw=true#gh-light-mode-only&quot; alt=&quot;OpenBB Platform logo&quot; width=&quot;600&quot;&gt;
&lt;img src=&quot;https://github.com/OpenBB-finance/OpenBB/blob/develop/images/platform-dark.svg?raw=true#gh-dark-mode-only&quot; alt=&quot;OpenBB Platform logo&quot; width=&quot;600&quot;&gt;
&lt;br /&gt;
&lt;br /&gt;

[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&amp;label=Follow%20%40openbb_finance)](https://x.com/openbb_finance)
[![Discord Shield](https://img.shields.io/discord/831165782750789672)](https://discord.com/invite/xPHTuHCmuV)
[![Open in Dev Containers](https://img.shields.io/static/v1?label=Dev%20Containers&amp;message=Open&amp;color=blue&amp;logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB)
&lt;a href=&quot;https://codespaces.new/OpenBB-finance/OpenBB&quot;&gt;
  &lt;img src=&quot;https://github.com/codespaces/badge.svg&quot; height=&quot;20&quot; /&gt;
&lt;/a&gt;
&lt;a target=&quot;_blank&quot; href=&quot;https://colab.research.google.com/github/OpenBB-finance/OpenBB/blob/develop/examples/googleColab.ipynb&quot;&gt;
  &lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot;/&gt;
&lt;/a&gt;
[![PyPI](https://img.shields.io/pypi/v/openbb?color=blue&amp;label=PyPI%20Package)](https://pypi.org/project/openbb/)

The first financial Platform that is open source.

The OpenBB Platform offers access to equity, options, crypto, forex, macro economy, fixed income, and more while also offering a broad range of extensions to enhance the user experience according to their needs.

Get started with: `pip install openbb`

```python
from openbb import obb
output = obb.equity.price.historical(&quot;AAPL&quot;)
df = output.to_dataframe()
```

You can sign up to the [OpenBB Hub](https://my.openbb.co/login) to get the most out of the OpenBB ecosystem.

Data integrations available can be found here: &lt;https://docs.openbb.co/platform/reference&gt;

---

## OpenBB Workspace

While the OpenBB Platform is all about an integration to dozens of different data vendors, the interface is either Python or a CLI.

If you want an enterprise UI to visualize this datasets and use AI agents on top, you can find OpenBB Workspace at &lt;https://pro.openbb.co&gt;.

&lt;a href=&quot;https://pro.openbb.co&quot;&gt;
  &lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://openbb-cms.directus.app/assets/f69b6aaf-0821-4bc8-a43c-715e03a924ef.png&quot; alt=&quot;Logo&quot; width=&quot;1000&quot;&gt;
  &lt;/div&gt;
&lt;/a&gt;

Data integration:

- You can learn more about adding data to the OpenBB workspace from the [docs](https://docs.openbb.co/workspace) or [this open source repository](https://github.com/OpenBB-finance/backends-for-openbb).

AI Agents integration:

- You can learn more about adding AI agents to the OpenBB workspace from [this open source repository](https://github.com/OpenBB-finance/agents-for-openbb).

### Integrating OpenBB Platform to the OpenBB Workspace

Connect this library to the OpenBB Workspace with a few simple commands, in a Python (3.9.21 - 3.12) environment.

#### Run OpenBB Platform backend

- Install the packages.

```sh
pip install &quot;openbb[all]&quot;
```

- Start the API server over localhost.

```sh
openbb-api
```

This will launch a FastAPI server, via Uvicorn, at `127.0.0.1:6900`.

You can check that it works by going to &lt;http://127.0.0.1:6900&gt;.

#### Integrate OpenBB Platform backend to OpenBB Workspace

Sign-in to the [OpenBB Workspace](https://pro.openbb.co/), and follow the following steps:

![CleanShot 2025-05-17 at 09 51 56@2x](https://github.com/user-attachments/assets/75cffb4a-5e95-470a-b9d0-6ffd4067e069)

1. Go to the &quot;Apps&quot; tab
2. Click on &quot;Connect backend&quot;
3. Fill in the form with:
   Name: OpenBB Platform
   URL: &lt;http://127.0.0.1:6900&gt;
4. Click on &quot;Test&quot;. You should get a &quot;Test successful&quot; with the number of apps found.
5. Click on &quot;Add&quot;.

That&#039;s it.

---

&lt;!-- TABLE OF CONTENTS --&gt;
&lt;details closed=&quot;closed&quot;&gt;
  &lt;summary&gt;&lt;h2 style=&quot;display: inline-block&quot;&gt;Table of Contents&lt;/h2&gt;&lt;/summary&gt;
  &lt;ol&gt;
    &lt;li&gt;&lt;a href=&quot;#1-installation&quot;&gt;Installation&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#2-contributing&quot;&gt;Contributing&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#3-license&quot;&gt;License&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#4-disclaimer&quot;&gt;Disclaimer&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#5-contacts&quot;&gt;Contacts&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#6-star-history&quot;&gt;Star History&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#7-contributors&quot;&gt;Contributors&lt;/a&gt;&lt;/li&gt;
  &lt;/ol&gt;
&lt;/details&gt;

## 1. Installation

The OpenBB Platform can be installed as a [PyPI package](https://pypi.org/project/openbb/) by running `pip install openbb`

or by cloning the repository directly with `git clone https://github.com/OpenBB-finance/OpenBB.git`.

Please find more about the installation process, in the [OpenBB Documentation](https://docs.openbb.co/platform/installation).

### OpenBB Platform CLI installation

The OpenBB Platform CLI is a command-line interface that allows you to access the OpenBB Platform directly from your command line.

It can be installed by running `pip install openbb-cli`

or by cloning the repository directly with  `git clone https://github.com/OpenBB-finance/OpenBB.git`.

Please find more about the installation process in the [OpenBB Documentation](https://docs.openbb.co/cli/installation).

## 2. Contributing

There are three main ways of contributing to this project. (Hopefully you have starred the project by now â­ï¸)

### Become a Contributor

- More information on our [Contributing Documentation](https://docs.openbb.co/platform/developer_guide/misc/contributing).

### Create a GitHub ticket

Before creating a ticket make sure the one you are creating doesn&#039;t exist already [here](https://github.com/OpenBB-finance/OpenBB/issues)

- [Report bug](https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;labels=bug&amp;template=bug_report.md&amp;title=%5BBug%5D)
- [Suggest improvement](https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;labels=enhancement&amp;template=enhancement.md&amp;title=%5BIMPROVE%5D)
- [Request a feature](https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&amp;labels=new+feature&amp;template=feature_request.md&amp;title=%5BFR%5D)

### Provide feedback

We are most active on [our Discord](https://openbb.co/discord), but feel free to reach out to us in any of [our social media](https://openbb.co/links) for feedback.

## 3. License

Distributed under the AGPLv3 License. See
[LICENSE](https://github.com/OpenBB-finance/OpenBB/blob/main/LICENSE) for more information.

## 4. Disclaimer

Trading in financial instruments involves high risks including the risk of losing some, or all, of your investment
amount, and may not be suitable for all investors.

Before deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.

The data contained in the OpenBB Platform is not necessarily accurate.

OpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.

All names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.

Our use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.

## 5. Contacts

If you have any questions about the platform or anything OpenBB, feel free to email us at `support@openbb.co`

If you want to say hi, or are interested in partnering with us, feel free to reach us at `hello@openbb.co`

Any of our social media platforms: [openbb.co/links](https://openbb.co/links)

## 6. Star History

This is a proxy of our growth and that we are just getting started.

But for more metrics important to us check [openbb.co/open](https://openbb.co/open).

[![Star History Chart](https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;type=Date&amp;theme=dark)](https://api.star-history.com/svg?repos=openbb-finance/OpenBB&amp;type=Date&amp;theme=dark)

## 7. Contributors

OpenBB wouldn&#039;t be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.

&lt;a href=&quot;https://github.com/OpenBB-finance/OpenBB/graphs/contributors&quot;&gt;
   &lt;img src=&quot;https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBB&quot; width=&quot;800&quot;/&gt;
&lt;/a&gt;

&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt;
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;

[contributors-shield]: https://img.shields.io/github/contributors/OpenBB-finance/OpenBB.svg?style=for-the-badge
[contributors-url]: https://github.com/OpenBB-finance/OpenBB/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/OpenBB-finance/OpenBB.svg?style=for-the-badge
[forks-url]: https://github.com/OpenBB-finance/OpenBB/network/members
[stars-shield]: https://img.shields.io/github/stars/OpenBB-finance/OpenBB.svg?style=for-the-badge
[stars-url]: https://github.com/OpenBB-finance/OpenBB/stargazers
[issues-shield]: https://img.shields.io/github/issues/OpenBB-finance/OpenBB.svg?style=for-the-badge&amp;color=blue
[issues-url]: https://github.com/OpenBB-finance/OpenBB/issues
[bugs-open-shield]: https://img.shields.io/github/issues/OpenBB-finance/OpenBB/bug.svg?style=for-the-badge&amp;color=yellow
[bugs-open-url]: https://github.com/OpenBB-finance/OpenBB/issues?q=is%3Aissue+label%3Abug+is%3Aopen
[bugs-closed-shield]: https://img.shields.io/github/issues-closed/OpenBB-finance/OpenBB/bug.svg?style=for-the-badge&amp;color=success
[bugs-closed-url]: https://github.com/OpenBB-finance/OpenBB/issues?q=is%3Aissue+label%3Abug+is%3Aclosed
[license-shield]: https://img.shields.io/github/license/OpenBB-finance/OpenBB.svg?style=for-the-badge
[license-url]: https://github.com/OpenBB-finance/OpenBB/blob/main/LICENSE.txt
[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&amp;logo=linkedin&amp;colorB=555
[linkedin-url]: https://linkedin.com/in/DidierRLopes
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[google-deepmind/mujoco_warp]]></title>
            <link>https://github.com/google-deepmind/mujoco_warp</link>
            <guid>https://github.com/google-deepmind/mujoco_warp</guid>
            <pubDate>Thu, 02 Oct 2025 00:04:04 GMT</pubDate>
            <description><![CDATA[GPU-optimized version of the MuJoCo physics simulator, designed for NVIDIA hardware.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google-deepmind/mujoco_warp">google-deepmind/mujoco_warp</a></h1>
            <p>GPU-optimized version of the MuJoCo physics simulator, designed for NVIDIA hardware.</p>
            <p>Language: Python</p>
            <p>Stars: 783</p>
            <p>Forks: 81</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre>&lt;p&gt;
  &lt;a href=&quot;https://github.com/google-deepmind/mujoco_warp/actions/workflows/ci.yml?query=branch%3Amain&quot; alt=&quot;GitHub Actions&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/google-deepmind/mujoco_warp/ci.yml?branch=main&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/google-deepmind/mujoco_warp/blob/main/LICENSE&quot; alt=&quot;License&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/license/google-deepmind/mujoco_warp&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

# MuJoCo Warp (MJWarp)

MJWarp is a GPU-optimized version of the [MuJoCo](https://github.com/google-deepmind/mujoco) physics simulator, designed for NVIDIA hardware.

&gt; [!NOTE]
&gt; MJWarp is in Beta and under active development:
&gt; * MJWarp developers will triage and respond to [bug report and feature requests](https://github.com/google-deepmind/mujoco_warp/issues).
&gt; * MJWarp is mostly feature complete but requires performance optimization, documentation, and testing.
&gt; * The intended audience during Beta are physics engine enthusiasts and learning framework integrators.
&gt; * Machine learning / robotics researchers who just want to train policies should wait for the [MJX](https://mujoco.readthedocs.io/en/stable/mjx.html) or [Isaac](https://isaac-sim.github.io/IsaacLab/main/index.html)/[Newton](https://github.com/newton-physics/newton) integrations, which are coming soon.

MJWarp uses [NVIDIA Warp](https://github.com/NVIDIA/warp) to circumvent many of the [sharp bits](https://mujoco.readthedocs.io/en/stable/mjx.html#mjx-the-sharp-bits) in [MuJoCo MJX](https://mujoco.readthedocs.io/en/stable/mjx.html#). MJWarp is integrated into both [MJX](https://mujoco.readthedocs.io/en/stable/mjx.html) and [Newton](https://github.com/newton-physics/newton).

MJWarp is maintained by [Google DeepMind](https://deepmind.google/) and [NVIDIA](https://www.nvidia.com/).

# Installing for development

MuJoCo Warp is currently supported on Windows or Linux on x86-64 architecture (to be expanded to more platforms and architectures soon).

**Linux**
```bash
git clone https://github.com/google-deepmind/mujoco_warp.git
cd mujoco_warp
python3 -m venv env
source env/bin/activate
pip install --upgrade pip
pip install uv
```
**Windows**
(native Python only, not MSYS2 or WSL)

```powershell
git clone https://github.com/google-deepmind/mujoco_warp.git
cd mujoco_warp
python -m venv env
.\env\Scripts\Activate.ps1  # For MSYS2 Python: env\bin\activate
pip install --upgrade pip
pip install uv
```


Then install MJWarp in editable mode for local development:

```
uv pip install -e .[dev,cuda]
```

Now make sure everything is working:

```bash
pytest
```

Should print out something like `XX passed in XX.XXs` at the end!

If you plan to write Warp kernels for MJWarp, please use the `kernel_analyzer` vscode plugin located in `contrib/kernel_analyzer`.
Please see the `README.md` there for details on how to install it and use it.  The same kernel analyzer will be run on any PR
you open, so it&#039;s important to fix any issues it reports.

# Compatibility

The following features are implemented:

| Category           | Feature                                                                                                 |
| ------------------ | --------------------------------------------------------------------------------------------------------|
| Dynamics           | Forward, Inverse                                                                                        |
| Transmission       | All                                                                                                     |
| Actuator Dynamics  | All except `USER`                                                                                       |
| Actuator Gain      | All except `USER`                                                                                       |
| Actuator Bias      | All except `USER`                                                                                       |
| Geom               | All                                                                                                     |
| Constraint         | All                                                                                                     |
| Equality           | All except `FLEX`, `DISTANCE`                                                                           |
| Integrator         | All except `IMPLICIT`                                                                                   |
| Cone               | All                                                                                                     |
| Condim             | All                                                                                                     |
| Solver             | All except `PGS`, `noslip`                                                                              |
| Fluid Model        | `BOX` only                                                                                              |
| Tendon Wrap        | All                                                                                                     |
| Sensors            | All except `GEOMDIST`, `GEOMNORMAL`, `GEOMFROMTO`, `PLUGIN`, `USER`                                     |
| Flex               | `VERTCOLLIDE`, `ELASTICITY`                                                                             |
| Mass matrix format | Sparse and Dense                                                                                        |
| Jacobian format    | `DENSE` only (row-sparse, no islanding yet)                                                             |

[Differentiability via Warp](https://nvidia.github.io/warp/modules/differentiability.html#differentiability) is not currently
available.

# Viewing simulations

Explore MuJoCo Warp simulations using an interactive viewer:

```bash
mjwarp-viewer benchmark/humanoid/humanoid.xml
```

This will open a window on your local machine that uses the [MuJoCo native visualizer](https://mujoco.readthedocs.io/en/stable/programming/visualization.html).

# Benchmarking

Benchmark as follows:

```bash
mjwarp-testspeed benchmark/humanoid/humanoid.xml
```

To get a full trace of the physics steps (e.g. timings of the subcomponents) run the following:

```bash
mjwarp-testspeed benchmark/humanoid/humanoid.xml --event_trace=True
```

`mjwarp-testspeed` has many configuration options, see ```mjwarp-testspeed --help``` for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[isaac-sim/IsaacLab]]></title>
            <link>https://github.com/isaac-sim/IsaacLab</link>
            <guid>https://github.com/isaac-sim/IsaacLab</guid>
            <pubDate>Thu, 02 Oct 2025 00:04:03 GMT</pubDate>
            <description><![CDATA[Unified framework for robot learning built on NVIDIA Isaac Sim]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/isaac-sim/IsaacLab">isaac-sim/IsaacLab</a></h1>
            <p>Unified framework for robot learning built on NVIDIA Isaac Sim</p>
            <p>Language: Python</p>
            <p>Stars: 5,018</p>
            <p>Forks: 2,429</p>
            <p>Stars today: 17 stars today</p>
            <h2>README</h2><pre>![Isaac Lab](docs/source/_static/isaaclab.jpg)

---

# Isaac Lab

[![IsaacSim](https://img.shields.io/badge/IsaacSim-5.0.0-silver.svg)](https://docs.isaacsim.omniverse.nvidia.com/latest/index.html)
[![Python](https://img.shields.io/badge/python-3.11-blue.svg)](https://docs.python.org/3/whatsnew/3.11.html)
[![Linux platform](https://img.shields.io/badge/platform-linux--64-orange.svg)](https://releases.ubuntu.com/22.04/)
[![Windows platform](https://img.shields.io/badge/platform-windows--64-orange.svg)](https://www.microsoft.com/en-us/)
[![pre-commit](https://img.shields.io/github/actions/workflow/status/isaac-sim/IsaacLab/pre-commit.yaml?logo=pre-commit&amp;logoColor=white&amp;label=pre-commit&amp;color=brightgreen)](https://github.com/isaac-sim/IsaacLab/actions/workflows/pre-commit.yaml)
[![docs status](https://img.shields.io/github/actions/workflow/status/isaac-sim/IsaacLab/docs.yaml?label=docs&amp;color=brightgreen)](https://github.com/isaac-sim/IsaacLab/actions/workflows/docs.yaml)
[![License](https://img.shields.io/badge/license-BSD--3-yellow.svg)](https://opensource.org/licenses/BSD-3-Clause)
[![License](https://img.shields.io/badge/license-Apache--2.0-yellow.svg)](https://opensource.org/license/apache-2-0)


**Isaac Lab** is a GPU-accelerated, open-source framework designed to unify and simplify robotics research workflows,
such as reinforcement learning, imitation learning, and motion planning. Built on [NVIDIA Isaac Sim](https://docs.isaacsim.omniverse.nvidia.com/latest/index.html),
it combines fast and accurate physics and sensor simulation, making it an ideal choice for sim-to-real
transfer in robotics.

Isaac Lab provides developers with a range of essential features for accurate sensor simulation, such as RTX-based
cameras, LIDAR, or contact sensors. The framework&#039;s GPU acceleration enables users to run complex simulations and
computations faster, which is key for iterative processes like reinforcement learning and data-intensive tasks.
Moreover, Isaac Lab can run locally or be distributed across the cloud, offering flexibility for large-scale deployments.


## Key Features

Isaac Lab offers a comprehensive set of tools and environments designed to facilitate robot learning:

- **Robots**: A diverse collection of robots, from manipulators, quadrupeds, to humanoids, with 16 commonly available models.
- **Environments**: Ready-to-train implementations of more than 30 environments, which can be trained with popular reinforcement learning frameworks such as RSL RL, SKRL, RL Games, or Stable Baselines. We also support multi-agent reinforcement learning.
- **Physics**: Rigid bodies, articulated systems, deformable objects
- **Sensors**: RGB/depth/segmentation cameras, camera annotations, IMU, contact sensors, ray casters.


## Getting Started

### Documentation

Our [documentation page](https://isaac-sim.github.io/IsaacLab) provides everything you need to get started, including
detailed tutorials and step-by-step guides. Follow these links to learn more about:

- [Installation steps](https://isaac-sim.github.io/IsaacLab/main/source/setup/installation/index.html#local-installation)
- [Reinforcement learning](https://isaac-sim.github.io/IsaacLab/main/source/overview/reinforcement-learning/rl_existing_scripts.html)
- [Tutorials](https://isaac-sim.github.io/IsaacLab/main/source/tutorials/index.html)
- [Available environments](https://isaac-sim.github.io/IsaacLab/main/source/overview/environments.html)


## Isaac Sim Version Dependency

Isaac Lab is built on top of Isaac Sim and requires specific versions of Isaac Sim that are compatible with each
release of Isaac Lab. Below, we outline the recent Isaac Lab releases and GitHub branches and their corresponding
dependency versions for Isaac Sim.

| Isaac Lab Version             | Isaac Sim Version   |
| ----------------------------- | ------------------- |
| `main` branch                 | Isaac Sim 4.5 / 5.0 |
| `v2.2.X`                      | Isaac Sim 4.5 / 5.0 |
| `v2.1.X`                      | Isaac Sim 4.5       |
| `v2.0.X`                      | Isaac Sim 4.5       |


## Contributing to Isaac Lab

We wholeheartedly welcome contributions from the community to make this framework mature and useful for everyone.
These may happen as bug reports, feature requests, or code contributions. For details, please check our
[contribution guidelines](https://isaac-sim.github.io/IsaacLab/main/source/refs/contributing.html).

## Show &amp; Tell: Share Your Inspiration

We encourage you to utilize our [Show &amp; Tell](https://github.com/isaac-sim/IsaacLab/discussions/categories/show-and-tell)
area in the `Discussions` section of this repository. This space is designed for you to:

* Share the tutorials you&#039;ve created
* Showcase your learning content
* Present exciting projects you&#039;ve developed

By sharing your work, you&#039;ll inspire others and contribute to the collective knowledge
of our community. Your contributions can spark new ideas and collaborations, fostering
innovation in robotics and simulation.

## Troubleshooting

Please see the [troubleshooting](https://isaac-sim.github.io/IsaacLab/main/source/refs/troubleshooting.html) section for
common fixes or [submit an issue](https://github.com/isaac-sim/IsaacLab/issues).

For issues related to Isaac Sim, we recommend checking its [documentation](https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/overview.html)
or opening a question on its [forums](https://forums.developer.nvidia.com/c/agx-autonomous-machines/isaac/67).

## Support

* Please use GitHub [Discussions](https://github.com/isaac-sim/IsaacLab/discussions) for discussing ideas,
  asking questions, and requests for new features.
* Github [Issues](https://github.com/isaac-sim/IsaacLab/issues) should only be used to track executable pieces of
  work with a definite scope and a clear deliverable. These can be fixing bugs, documentation issues, new features,
  or general updates.

## Connect with the NVIDIA Omniverse Community

Do you have a project or resource you&#039;d like to share more widely? We&#039;d love to hear from you!
Reach out to the NVIDIA Omniverse Community team at OmniverseCommunity@nvidia.com to explore opportunities
to spotlight your work.

You can also join the conversation on the [Omniverse Discord](https://discord.com/invite/nvidiaomniverse) to
connect with other developers, share your projects, and help grow a vibrant, collaborative ecosystem
where creativity and technology intersect. Your contributions can make a meaningful impact on the Isaac Lab
community and beyond!

## License

The Isaac Lab framework is released under [BSD-3 License](LICENSE). The `isaaclab_mimic` extension and its
corresponding standalone scripts are released under [Apache 2.0](LICENSE-mimic). The license files of its
dependencies and assets are present in the [`docs/licenses`](docs/licenses) directory.

Note that Isaac Lab requires Isaac Sim, which includes components under proprietary licensing terms. Please see the [Isaac Sim license](docs/licenses/dependencies/isaacsim-license.txt) for information on Isaac Sim licensing.

Note that the `isaaclab_mimic` extension requires cuRobo, which has proprietary licensing terms that can be found in [`docs/licenses/dependencies/cuRobo-license.txt`](docs/licenses/dependencies/cuRobo-license.txt).

## Acknowledgement

Isaac Lab development initiated from the [Orbit](https://isaac-orbit.github.io/) framework. We would appreciate if
you would cite it in academic publications as well:

```
@article{mittal2023orbit,
   author={Mittal, Mayank and Yu, Calvin and Yu, Qinxi and Liu, Jingzhou and Rudin, Nikita and Hoeller, David and Yuan, Jia Lin and Singh, Ritvik and Guo, Yunrong and Mazhar, Hammad and Mandlekar, Ajay and Babich, Buck and State, Gavriel and Hutter, Marco and Garg, Animesh},
   journal={IEEE Robotics and Automation Letters},
   title={Orbit: A Unified Simulation Framework for Interactive Robot Learning Environments},
   year={2023},
   volume={8},
   number={6},
   pages={3740-3747},
   doi={10.1109/LRA.2023.3270034}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[apify/crawlee-python]]></title>
            <link>https://github.com/apify/crawlee-python</link>
            <guid>https://github.com/apify/crawlee-python</guid>
            <pubDate>Thu, 02 Oct 2025 00:04:02 GMT</pubDate>
            <description><![CDATA[Crawleeâ€”A web scraping and browser automation library for Python to build reliable crawlers. Extract data for AI, LLMs, RAG, or GPTs. Download HTML, PDF, JPG, PNG, and other files from websites. Works with BeautifulSoup, Playwright, and raw HTTP. Both headful and headless mode. With proxy rotation.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/apify/crawlee-python">apify/crawlee-python</a></h1>
            <p>Crawleeâ€”A web scraping and browser automation library for Python to build reliable crawlers. Extract data for AI, LLMs, RAG, or GPTs. Download HTML, PDF, JPG, PNG, and other files from websites. Works with BeautifulSoup, Playwright, and raw HTTP. Both headful and headless mode. With proxy rotation.</p>
            <p>Language: Python</p>
            <p>Stars: 6,648</p>
            <p>Forks: 472</p>
            <p>Stars today: 231 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://crawlee.dev&quot;&gt;
        &lt;picture&gt;
          &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/apify/crawlee-python/master/website/static/img/crawlee-dark.svg?sanitize=true&quot;&gt;
          &lt;img alt=&quot;Crawlee&quot; src=&quot;https://raw.githubusercontent.com/apify/crawlee-python/master/website/static/img/crawlee-light.svg?sanitize=true&quot; width=&quot;500&quot;&gt;
        &lt;/picture&gt;
    &lt;/a&gt;
    &lt;br&gt;
    &lt;small&gt;A web scraping and browser automation library&lt;/small&gt;
&lt;/h1&gt;

&lt;p align=center&gt;
    &lt;a href=&quot;https://trendshift.io/repositories/11169&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11169&quot; alt=&quot;apify%2Fcrawlee-python | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=center&gt;
    &lt;a href=&quot;https://badge.fury.io/py/crawlee&quot; rel=&quot;nofollow&quot;&gt;
        &lt;img src=&quot;https://badge.fury.io/py/crawlee.svg&quot; alt=&quot;PyPI version&quot; style=&quot;max-width: 100%;&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://pypi.org/project/crawlee/&quot; rel=&quot;nofollow&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/pypi/dm/crawlee&quot; alt=&quot;PyPI - Downloads&quot; style=&quot;max-width: 100%;&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://pypi.org/project/crawlee/&quot; rel=&quot;nofollow&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/pypi/pyversions/crawlee&quot; alt=&quot;PyPI - Python Version&quot; style=&quot;max-width: 100%;&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://discord.gg/jyEM2PRvMU&quot; rel=&quot;nofollow&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/discord/801163717915574323?label=discord&quot; alt=&quot;Chat on discord&quot; style=&quot;max-width: 100%;&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

Crawlee covers your crawling and scraping end-to-end and **helps you build reliable scrapers. Fast.**

Your crawlers will appear almost human-like and fly under the radar of modern bot protections even with the default configuration. Crawlee gives you the tools to crawl the web for links, scrape data and persistently store it in machine-readable formats, without having to worry about the technical details. And thanks to rich configuration options, you can tweak almost any aspect of Crawlee to suit your project&#039;s needs if the default settings don&#039;t cut it.

&gt; ğŸ‘‰ **View full documentation, guides and examples on the [Crawlee project website](https://crawlee.dev/python/)** ğŸ‘ˆ

We also have a TypeScript implementation of the Crawlee, which you can explore and utilize for your projects. Visit our GitHub repository for more information [Crawlee for JS/TS on GitHub](https://github.com/apify/crawlee).

## Installation

We recommend visiting the [Introduction tutorial](https://crawlee.dev/python/docs/introduction) in Crawlee documentation for more information.

Crawlee is available as [`crawlee`](https://pypi.org/project/crawlee/) package on PyPI. This package includes the core functionality, while additional features are available as optional extras to keep dependencies and package size minimal.

To install Crawlee with all features, run the following command:

```sh
python -m pip install &#039;crawlee[all]&#039;
```

Then, install the [Playwright](https://playwright.dev/) dependencies:

```sh
playwright install
```

Verify that Crawlee is successfully installed:

```sh
python -c &#039;import crawlee; print(crawlee.__version__)&#039;
```

For detailed installation instructions see the [Setting up](https://crawlee.dev/python/docs/introduction/setting-up) documentation page.

### With Crawlee CLI

The quickest way to get started with Crawlee is by using the Crawlee CLI and selecting one of the prepared templates. First, ensure you have [uv](https://pypi.org/project/uv/) installed:

```sh
uv --help
```

If [uv](https://pypi.org/project/uv/) is not installed, follow the official [installation guide](https://docs.astral.sh/uv/getting-started/installation/).

Then, run the CLI and choose from the available templates:

```sh
uvx &#039;crawlee[cli]&#039; create my-crawler
```

If you already have `crawlee` installed, you can spin it up by running:

```sh
crawlee create my-crawler
```

## Examples

Here are some practical examples to help you get started with different types of crawlers in Crawlee. Each example demonstrates how to set up and run a crawler for specific use cases, whether you need to handle simple HTML pages or interact with JavaScript-heavy sites. A crawler run will create a `storage/` directory in your current working directory.

### BeautifulSoupCrawler

The [`BeautifulSoupCrawler`](https://crawlee.dev/python/api/class/BeautifulSoupCrawler) downloads web pages using an HTTP library and provides HTML-parsed content to the user. By default it uses [`HttpxHttpClient`](https://crawlee.dev/python/api/class/HttpxHttpClient) for HTTP communication and [BeautifulSoup](https://pypi.org/project/beautifulsoup4/) for parsing HTML. It is ideal for projects that require efficient extraction of data from HTML content. This crawler has very good performance since it does not use a browser. However, if you need to execute client-side JavaScript, to get your content, this is not going to be enough and you will need to use [`PlaywrightCrawler`](https://crawlee.dev/python/api/class/PlaywrightCrawler). Also if you want to use this crawler, make sure you install `crawlee` with `beautifulsoup` extra.

```python
import asyncio

from crawlee.crawlers import BeautifulSoupCrawler, BeautifulSoupCrawlingContext


async def main() -&gt; None:
    crawler = BeautifulSoupCrawler(
        # Limit the crawl to max requests. Remove or increase it for crawling all links.
        max_requests_per_crawl=10,
    )

    # Define the default request handler, which will be called for every request.
    @crawler.router.default_handler
    async def request_handler(context: BeautifulSoupCrawlingContext) -&gt; None:
        context.log.info(f&#039;Processing {context.request.url} ...&#039;)

        # Extract data from the page.
        data = {
            &#039;url&#039;: context.request.url,
            &#039;title&#039;: context.soup.title.string if context.soup.title else None,
        }

        # Push the extracted data to the default dataset.
        await context.push_data(data)

        # Enqueue all links found on the page.
        await context.enqueue_links()

    # Run the crawler with the initial list of URLs.
    await crawler.run([&#039;https://crawlee.dev&#039;])


if __name__ == &#039;__main__&#039;:
    asyncio.run(main())
```

### PlaywrightCrawler

The [`PlaywrightCrawler`](https://crawlee.dev/python/api/class/PlaywrightCrawler) uses a headless browser to download web pages and provides an API for data extraction. It is built on [Playwright](https://playwright.dev/), an automation library designed for managing headless browsers. It excels at retrieving web pages that rely on client-side JavaScript for content generation, or tasks requiring interaction with JavaScript-driven content. For scenarios where JavaScript execution is unnecessary or higher performance is required, consider using the [`BeautifulSoupCrawler`](https://crawlee.dev/python/api/class/BeautifulSoupCrawler). Also if you want to use this crawler, make sure you install `crawlee` with `playwright` extra.

```python
import asyncio

from crawlee.crawlers import PlaywrightCrawler, PlaywrightCrawlingContext


async def main() -&gt; None:
    crawler = PlaywrightCrawler(
        # Limit the crawl to max requests. Remove or increase it for crawling all links.
        max_requests_per_crawl=10,
    )

    # Define the default request handler, which will be called for every request.
    @crawler.router.default_handler
    async def request_handler(context: PlaywrightCrawlingContext) -&gt; None:
        context.log.info(f&#039;Processing {context.request.url} ...&#039;)

        # Extract data from the page.
        data = {
            &#039;url&#039;: context.request.url,
            &#039;title&#039;: await context.page.title(),
        }

        # Push the extracted data to the default dataset.
        await context.push_data(data)

        # Enqueue all links found on the page.
        await context.enqueue_links()

    # Run the crawler with the initial list of requests.
    await crawler.run([&#039;https://crawlee.dev&#039;])


if __name__ == &#039;__main__&#039;:
    asyncio.run(main())
```

### More examples

Explore our [Examples](https://crawlee.dev/python/docs/examples) page in the Crawlee documentation for a wide range of additional use cases and demonstrations.

## Features

Why Crawlee is the preferred choice for web scraping and crawling?

### Why use Crawlee instead of just a random HTTP library with an HTML parser?

- Unified interface for **HTTP &amp; headless browser** crawling.
- Automatic **parallel crawling** based on available system resources.
- Written in Python with **type hints** - enhances DX (IDE autocompletion) and reduces bugs (static type checking).
- Automatic **retries** on errors or when youâ€™re getting blocked.
- Integrated **proxy rotation** and session management.
- Configurable **request routing** - direct URLs to the appropriate handlers.
- Persistent **queue for URLs** to crawl.
- Pluggable **storage** of both tabular data and files.
- Robust **error handling**.

### Why to use Crawlee rather than Scrapy?

- **Asyncio-based** â€“ Leveraging the standard [Asyncio](https://docs.python.org/3/library/asyncio.html) library, Crawlee delivers better performance and seamless compatibility with other modern asynchronous libraries.
- **Type hints** â€“ Newer project built with modern Python, and complete type hint coverage for a better developer experience.
- **Simple integration** â€“ Crawlee crawlers are regular Python scripts, requiring no additional launcher executor. This flexibility allows to integrate a crawler directly into other applications.
- **State persistence** â€“ Supports state persistence during interruptions, saving time and costs by avoiding the need to restart scraping pipelines from scratch after an issue.
- **Organized data storages** â€“ Allows saving of multiple types of results in a single scraping run. Offers several storing options (see [datasets](https://crawlee.dev/python/api/class/Dataset) &amp; [key-value stores](https://crawlee.dev/python/api/class/KeyValueStore)).

## Running on the Apify platform

Crawlee is open-source and runs anywhere, but since it&#039;s developed by [Apify](https://apify.com), it&#039;s easy to set up on the Apify platform and run in the cloud. Visit the [Apify SDK website](https://docs.apify.com/sdk/python/) to learn more about deploying Crawlee to the Apify platform.

## Support

If you find any bug or issue with Crawlee, please [submit an issue on GitHub](https://github.com/apify/crawlee-python/issues). For questions, you can ask on [Stack Overflow](https://stackoverflow.com/questions/tagged/apify), in GitHub Discussions or you can join our [Discord server](https://discord.com/invite/jyEM2PRvMU).

## Contributing

Your code contributions are welcome, and you&#039;ll be praised for eternity! If you have any ideas for improvements, either submit an issue or create a pull request. For contribution guidelines and the code of conduct, see [CONTRIBUTING.md](https://github.com/apify/crawlee-python/blob/master/CONTRIBUTING.md).

## License

This project is licensed under the Apache License 2.0 - see the [LICENSE](https://github.com/apify/crawlee-python/blob/master/LICENSE) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[opendatalab/MinerU]]></title>
            <link>https://github.com/opendatalab/MinerU</link>
            <guid>https://github.com/opendatalab/MinerU</guid>
            <pubDate>Thu, 02 Oct 2025 00:04:01 GMT</pubDate>
            <description><![CDATA[Transforms complex documents like PDFs into LLM-ready markdown/JSON for your Agentic workflows.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/opendatalab/MinerU">opendatalab/MinerU</a></h1>
            <p>Transforms complex documents like PDFs into LLM-ready markdown/JSON for your Agentic workflows.</p>
            <p>Language: Python</p>
            <p>Stars: 45,140</p>
            <p>Forks: 3,742</p>
            <p>Stars today: 76 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; xmlns=&quot;http://www.w3.org/1999/html&quot;&gt;
&lt;!-- logo --&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/images/MinerU-logo.png&quot; width=&quot;300px&quot; style=&quot;vertical-align:middle;&quot;&gt;
&lt;/p&gt;

&lt;!-- icon --&gt;

[![stars](https://img.shields.io/github/stars/opendatalab/MinerU.svg)](https://github.com/opendatalab/MinerU)
[![forks](https://img.shields.io/github/forks/opendatalab/MinerU.svg)](https://github.com/opendatalab/MinerU)
[![open issues](https://img.shields.io/github/issues-raw/opendatalab/MinerU)](https://github.com/opendatalab/MinerU/issues)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/opendatalab/MinerU)](https://github.com/opendatalab/MinerU/issues)
[![PyPI version](https://img.shields.io/pypi/v/mineru)](https://pypi.org/project/mineru/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/mineru)](https://pypi.org/project/mineru/)
[![Downloads](https://static.pepy.tech/badge/mineru)](https://pepy.tech/project/mineru)
[![Downloads](https://static.pepy.tech/badge/mineru/month)](https://pepy.tech/project/mineru)
[![OpenDataLab](https://img.shields.io/badge/webapp_on_mineru.net-blue?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTM0IiBoZWlnaHQ9IjEzNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMTIyLDljMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0idXJsKCNhKSIvPjxwYXRoIGQ9Im0xMjIsOWMwLDUtNCw5LTksOXMtOS00LTktOSw0LTksOS05LDksNCw5LDl6IiBmaWxsPSIjMDEwMTAxIi8+PHBhdGggZD0ibTkxLDE4YzAsNS00LDktOSw5cy05LTQtOS05LDQtOSw5LTksOSw0LDksOXoiIGZpbGw9InVybCgjYikiLz48cGF0aCBkPSJtOTEsMThjMCw1LTQsOS05LDlzLTktNC05LTksNC05LDktOSw5LDQsOSw5eiIgZmlsbD0iIzAxMDEwMSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0idXJsKCNjKSIvPjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJtMzksNjJjMCwxNiw4LDMwLDIwLDM4LDctNiwxMi0xNiwxMi0yNlY0OWMwLTQsMy03LDYtOGw0Ni0xMmM1LTEsMTEsMywxMSw4djMxYzAsMzctMzAsNjYtNjYsNjYtMzcsMC02Ni0zMC02Ni02NlY0NmMwLTQsMy03LDYtOGwyMC02YzUtMSwxMSwzLDExLDh2MjF6bS0yOSw2YzAsMTYsNiwzMCwxNyw0MCwzLDEsNSwxLDgsMSw1LDAsMTAtMSwxNS0zQzM3LDk1LDI5LDc5LDI5LDYyVjQybC0xOSw1djIweiIgZmlsbD0iIzAxMDEwMSIvPjxkZWZzPjxsaW5lYXJHcmFkaWVudCBpZD0iYSIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYiIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjxsaW5lYXJHcmFkaWVudCBpZD0iYyIgeDE9Ijg0IiB5MT0iNDEiIHgyPSI3NSIgeTI9IjEyMCIgZ3JhZGllbnRVbml0cz0idXNlclNwYWNlT25Vc2UiPjxzdG9wIHN0b3AtY29sb3I9IiNmZmYiLz48c3RvcCBvZmZzZXQ9IjEiIHN0b3AtY29sb3I9IiMyZTJlMmUiLz48L2xpbmVhckdyYWRpZW50PjwvZGVmcz48L3N2Zz4=&amp;labelColor=white)](https://mineru.net/OpenSourceTools/Extractor?source=github)
[![HuggingFace](https://img.shields.io/badge/Demo_on_HuggingFace-yellow.svg?logo=data:image/png;base64,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&amp;labelColor=white)](https://huggingface.co/spaces/opendatalab/MinerU)
[![ModelScope](https://img.shields.io/badge/Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;labelColor=white)](https://www.modelscope.cn/studios/OpenDataLab/MinerU)
[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/gist/myhloli/a3cb16570ab3cfeadf9d8f0ac91b4fca/mineru_demo.ipynb)
[![arXiv](https://img.shields.io/badge/MinerU-Technical%20Report-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2409.18839)
[![arXiv](https://img.shields.io/badge/MinerU2.5-Technical%20Report-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2509.22186)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/opendatalab/MinerU)


&lt;a href=&quot;https://trendshift.io/repositories/11174&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11174&quot; alt=&quot;opendatalab%2FMinerU | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;!-- language --&gt;

[English](README.md) | [ç®€ä½“ä¸­æ–‡](README_zh-CN.md)

&lt;!-- hot link --&gt;

&lt;p align=&quot;center&quot;&gt;
ğŸš€&lt;a href=&quot;https://mineru.net/?source=github&quot;&gt;Access MinerU Nowâ†’âœ… Zero-Install Web Version âœ… Full-Featured Desktop Client âœ… Instant API Access; Skip deployment headaches â€“ get all product formats in one click. Developers, dive in!&lt;/a&gt;
&lt;/p&gt;

&lt;!-- join us --&gt;

&lt;p align=&quot;center&quot;&gt;
    ğŸ‘‹ join us on &lt;a href=&quot;https://discord.gg/Tdedn9GTXq&quot; target=&quot;_blank&quot;&gt;Discord&lt;/a&gt; and &lt;a href=&quot;https://mineru.net/community-portal/?aliasId=3c430f94&quot; target=&quot;_blank&quot;&gt;WeChat&lt;/a&gt;
&lt;/p&gt;

&lt;/div&gt;

# Changelog

- 2025/09/26 2.5.4 released
  - ğŸ‰ğŸ‰ The MinerU2.5 [Technical Report](https://arxiv.org/abs/2509.22186) is now available! We welcome you to read it for a comprehensive overview of its model architecture, training strategy, data engineering and evaluation results.
  - Fixed an issue where some `PDF` files were mistakenly identified as `AI` files, causing parsing failures

- 2025/09/20 2.5.3 Released
  - Dependency version range adjustment to enable Turing and earlier architecture GPUs to use vLLM acceleration for MinerU2.5 model inference.
  - `pipeline` backend compatibility fixes for torch 2.8.0.
  - Reduced default concurrency for vLLM async backend to lower server pressure and avoid connection closure issues caused by high load.
  - More compatibility-related details can be found in the [announcement](https://github.com/opendatalab/MinerU/discussions/3548)

- 2025/09/19 2.5.2 Released

  We are officially releasing MinerU2.5, currently the most powerful multimodal large model for document parsing.
  With only 1.2B parameters, MinerU2.5&#039;s accuracy on the OmniDocBench benchmark comprehensively surpasses top-tier multimodal models like Gemini 2.5 Pro, GPT-4o, and Qwen2.5-VL-72B. It also significantly outperforms leading specialized models such as dots.ocr, MonkeyOCR, and PP-StructureV3.
  The model has been released on [HuggingFace](https://huggingface.co/opendatalab/MinerU2.5-2509-1.2B) and [ModelScope](https://modelscope.cn/models/opendatalab/MinerU2.5-2509-1.2B) platforms. Welcome to download and use!
  - Core Highlights:
    - SOTA Performance with Extreme Efficiency: As a 1.2B model, it achieves State-of-the-Art (SOTA) results that exceed models in the 10B and 100B+ classes, redefining the performance-per-parameter standard in document AI.
    - Advanced Architecture for Across-the-Board Leadership: By combining a two-stage inference pipeline (decoupling layout analysis from content recognition) with a native high-resolution architecture, it achieves SOTA performance across five key areas: layout analysis, text recognition, formula recognition, table recognition, and reading order.
  - Key Capability Enhancements:
    - Layout Detection: Delivers more complete results by accurately covering non-body content like headers, footers, and page numbers. It also provides more precise element localization and natural format reconstruction for lists and references.
    - Table Parsing: Drastically improves parsing for challenging cases, including rotated tables, borderless/semi-structured tables, and long/complex tables.
    - Formula Recognition: Significantly boosts accuracy for complex, long-form, and hybrid Chinese-English formulas, greatly enhancing the parsing capability for mathematical documents.

  Additionally, with the release of vlm 2.5, we have made some adjustments to the repository:
  - The vlm backend has been upgraded to version 2.5, supporting the MinerU2.5 model and no longer compatible with the MinerU2.0-2505-0.9B model. The last version supporting the 2.0 model is mineru-2.2.2.
  - VLM inference-related code has been moved to [mineru_vl_utils](https://github.com/opendatalab/mineru-vl-utils), reducing coupling with the main mineru repository and facilitating independent iteration in the future.
  - The vlm accelerated inference framework has been switched from `sglang` to `vllm`, achieving full compatibility with the vllm ecosystem, allowing users to use the MinerU2.5 model and accelerated inference on any platform that supports the vllm framework.
  - Due to major upgrades in the vlm model supporting more layout types, we have made some adjustments to the structure of the parsing intermediate file `middle.json` and result file `content_list.json`. Please refer to the [documentation](https://opendatalab.github.io/MinerU/reference/output_files/) for details.

  Other repository optimizations:
  - Removed file extension whitelist validation for input files. When input files are PDF documents or images, there are no longer requirements for file extensions, improving usability.

&lt;details&gt;
  &lt;summary&gt;History Log&lt;/summary&gt;

  &lt;details&gt;
    &lt;summary&gt;2025/09/10 2.2.2 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;Fixed the issue where the new table recognition model would affect the overall parsing task when some table parsing failed&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/09/08 2.2.1 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;Fixed the issue where some newly added models were not downloaded when using the model download command.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/09/05 2.2.0 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;
        Major Updates
        &lt;ul&gt;
          &lt;li&gt;In this version, we focused on improving table parsing accuracy by introducing a new &lt;a href=&quot;https://github.com/RapidAI/TableStructureRec&quot;&gt;wired table recognition model&lt;/a&gt; and a brand-new hybrid table structure parsing algorithm, significantly enhancing the table recognition capabilities of the &lt;code&gt;pipeline&lt;/code&gt; backend.&lt;/li&gt;
          &lt;li&gt;We also added support for cross-page table merging, which is supported by both &lt;code&gt;pipeline&lt;/code&gt; and &lt;code&gt;vlm&lt;/code&gt; backends, further improving the completeness and accuracy of table parsing.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        Other Updates
        &lt;ul&gt;
          &lt;li&gt;The &lt;code&gt;pipeline&lt;/code&gt; backend now supports 270-degree rotated table parsing, bringing support for table parsing in 0/90/270-degree orientations&lt;/li&gt;
          &lt;li&gt;&lt;code&gt;pipeline&lt;/code&gt; added OCR capability support for Thai and Greek, and updated the English OCR model to the latest version. English recognition accuracy improved by 11%, Thai recognition model accuracy is 82.68%, and Greek recognition model accuracy is 89.28% (by PPOCRv5)&lt;/li&gt;
          &lt;li&gt;Added &lt;code&gt;bbox&lt;/code&gt; field (mapped to 0-1000 range) in the output &lt;code&gt;content_list.json&lt;/code&gt;, making it convenient for users to directly obtain position information for each content block&lt;/li&gt;
          &lt;li&gt;Removed the &lt;code&gt;pipeline_old_linux&lt;/code&gt; installation option, no longer supporting legacy Linux systems such as &lt;code&gt;CentOS 7&lt;/code&gt;, to provide better support for &lt;code&gt;uv&lt;/code&gt;&#039;s &lt;code&gt;sync&lt;/code&gt;/&lt;code&gt;run&lt;/code&gt; commands&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;

  &lt;details&gt;
    &lt;summary&gt;2025/08/01 2.1.10 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;Fixed an issue in the &lt;code&gt;pipeline&lt;/code&gt; backend where block overlap caused the parsing results to deviate from expectations #3232&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/07/30 2.1.9 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt; 4.54.1 version adaptation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/07/28 2.1.8 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;sglang&lt;/code&gt; 0.4.9.post5 version adaptation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/07/27 2.1.7 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;transformers&lt;/code&gt; 4.54.0 version adaptation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/07/26 2.1.6 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;Fixed table parsing issues in handwritten documents when using &lt;code&gt;vlm&lt;/code&gt; backend&lt;/li&gt;
      &lt;li&gt;Fixed visualization box position drift issue when document is rotated #3175&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/07/24 2.1.5 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code&gt;sglang&lt;/code&gt; 0.4.9 version adaptation, synchronously upgrading the dockerfile base image to sglang 0.4.9.post3&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/07/23 2.1.4 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Bug Fixes&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Fixed the issue of excessive memory consumption during the &lt;code&gt;MFR&lt;/code&gt; step in the &lt;code&gt;pipeline&lt;/code&gt; backend under certain scenarios #2771&lt;/li&gt;
          &lt;li&gt;Fixed the inaccurate matching between &lt;code&gt;image&lt;/code&gt;/&lt;code&gt;table&lt;/code&gt; and &lt;code&gt;caption&lt;/code&gt;/&lt;code&gt;footnote&lt;/code&gt; under certain conditions #3129&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/07/16 2.1.1 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Bug fixes&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Fixed text block content loss issue that could occur in certain &lt;code&gt;pipeline&lt;/code&gt; scenarios #3005&lt;/li&gt;
          &lt;li&gt;Fixed issue where &lt;code&gt;sglang-client&lt;/code&gt; required unnecessary packages like &lt;code&gt;torch&lt;/code&gt; #2968&lt;/li&gt;
          &lt;li&gt;Updated &lt;code&gt;dockerfile&lt;/code&gt; to fix incomplete text content parsing due to missing fonts in Linux #2915&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Usability improvements&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Updated &lt;code&gt;compose.yaml&lt;/code&gt; to facilitate direct startup of &lt;code&gt;sglang-server&lt;/code&gt;, &lt;code&gt;mineru-api&lt;/code&gt;, and &lt;code&gt;mineru-gradio&lt;/code&gt; services&lt;/li&gt;
          &lt;li&gt;Launched brand new &lt;a href=&quot;https://opendatalab.github.io/MinerU/&quot;&gt;online documentation site&lt;/a&gt;, simplified readme, providing better documentation experience&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/details&gt;  

  &lt;details&gt;
    &lt;summary&gt;2025/07/05 2.1.0 Released&lt;/summary&gt;
    &lt;ul&gt;
      &lt;li&gt;This is the first major update of MinerU 2, which includes a large number of new features and improvements, covering significant performance optimizations, user experience enhancements, and bug fixes. The detailed update contents are as follows:&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Performance Optimizations:&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Significantly improved preprocessing speed for documents with specific resolutions (around 2000 pixels on the long side).&lt;/li&gt;
          &lt;li&gt;Greatly enhanced post-processing speed when the &lt;code&gt;pipeline&lt;/code&gt; backend handles batch processing of documents with fewer pages (&amp;lt;10 pages).&lt;/li&gt;
          &lt;li&gt;Layout analysis speed of the &lt;code&gt;pipeline&lt;/code&gt; backend has been increased by approximately 20%.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Experience Enhancements:&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;Built-in ready-to-use &lt;code&gt;fastapi service&lt;/code&gt; and &lt;code&gt;gradio webui&lt;/code&gt;. For detailed usage instructions, please refer to &lt;a href=&quot;https://opendatalab.github.io/MinerU/usage/quick_usage/#advanced-usage-via-api-webui-sglang-clientserver&quot;&gt;Documentation&lt;/a&gt;.&lt;/li&gt;
          &lt;li&gt;Adapted to &lt;code&gt;sglang&lt;/code&gt; version &lt;code&gt;0.4.8&lt;/code&gt;, significantly reducing the GPU memory requirements for the &lt;code&gt;vlm-sglang&lt;/code&gt; backend. It can now run on graphics cards with as little as &lt;code&gt;8GB GPU memory&lt;/code&gt; (Turing architecture or newer).&lt;/li&gt;
          &lt;li&gt;Added transparent parameter passing for all commands related to &lt;code&gt;sglang&lt;/code&gt;, allowing the &lt;code&gt;sglang-engine&lt;/code&gt; backend to receive all &lt;code&gt;sglang&lt;/code&gt; parameters consistently with the &lt;code&gt;sglang-server&lt;/code&gt;.&lt;/li&gt;
          &lt;li&gt;Supports feature extensions based on configuration files, including &lt;code&gt;custom formula delimiters&lt;/code&gt;, &lt;code&gt;enabling heading classification&lt;/code&gt;, and &lt;code&gt;customizing local model directories&lt;/code&gt;. For detailed usage instructions, please refer to &lt;a href=&quot;https://opendatalab.github.io/MinerU/us

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>