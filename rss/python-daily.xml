<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Fri, 16 May 2025 00:04:26 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[xming521/WeClone]]></title>
            <link>https://github.com/xming521/WeClone</link>
            <guid>https://github.com/xming521/WeClone</guid>
            <pubDate>Fri, 16 May 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[🚀从聊天记录创造数字分身的一站式解决方案💡 使用聊天记录微调大语言模型，让大模型有“那味儿”，并绑定到聊天机器人，实现自己的数字分身。 数字克隆/数字分身/数字永生/LLM/聊天机器人/LoRA]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/xming521/WeClone">xming521/WeClone</a></h1>
            <p>🚀从聊天记录创造数字分身的一站式解决方案💡 使用聊天记录微调大语言模型，让大模型有“那味儿”，并绑定到聊天机器人，实现自己的数字分身。 数字克隆/数字分身/数字永生/LLM/聊天机器人/LoRA</p>
            <p>Language: Python</p>
            <p>Stars: 8,376</p>
            <p>Forks: 657</p>
            <p>Stars today: 1,006 stars today</p>
            <h2>README</h2><pre>![download](https://github.com/user-attachments/assets/5842e84e-004f-4afd-9373-af64e9575b78)
&lt;h3 align=&quot;center&quot;&gt;🚀从聊天记录创造数字分身的一站式解决方案💡&lt;/h3&gt;  

&lt;div align=&quot;center&quot;&gt;

[![GitHub stars](https://img.shields.io/github/stars/xming521/WeClone?style=for-the-badge&amp;logo=github&amp;label=Stars&amp;logoColor=white&amp;color=ffda65)](https://github.com/xming521/WeClone/stargazers)
[![GitHub release](https://img.shields.io/github/v/release/xming521/WeClone?style=for-the-badge&amp;logo=github&amp;label=Release&amp;logoColor=white&amp;color=06d094)](https://github.com/xming521/WeClone/releases)
&lt;a href=&quot;https://qm.qq.com/cgi-bin/qm/qr?k=wNdgbOVT6oFOJ2wlMLsolUXErW9ESLpk&amp;jump_from=webapi&amp;authKey=z/reOp6YLyvR4Tl2k2nYMsLoMC3w9/99ucgKMX0oRGlxDV/WbYnvq2QxODoIkfxn&quot; target=&quot;_blank&quot; style=&quot;text-decoration: none;&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/QQ群-708067078-12B7F5?style=for-the-badge&amp;logo=qq&amp;logoColor=white&quot; alt=&quot;WeClone①&quot; title=&quot;WeClone①&quot;&gt;
&lt;/a&gt;
[![Telegram](https://img.shields.io/badge/Telegram-2CA5E0?style=for-the-badge&amp;logo=telegram&amp;logoColor=white)](https://t.me/+JEdak4m0XEQ3NGNl)

&lt;a href=&quot;https://hellogithub.com/repository/12ab209b56cb4cfd885c8cfd4cfdd53e&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=12ab209b56cb4cfd885c8cfd4cfdd53e&amp;claim_uid=RThlPDoGrFvdMY5&quot; alt=&quot;Featured｜HelloGitHub&quot; style=&quot;width: 150px; height: 28px;&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://trendshift.io/repositories/13759&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13759&quot; alt=&quot;xming521%2FWeClone | Trendshift&quot; style=&quot;width: 220px; height: 50px;&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://deepwiki.com/xming521/WeClone&quot;&gt;&lt;img src=&quot;https://deepwiki.com/badge.svg&quot; alt=&quot;Ask DeepWiki&quot;  style=&quot;width: 134px; height: 23px;margin-bottom: 3px;&quot;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://blog.051088.xyz/2025/05/14/WeClone-%E7%94%A8%E5%BE%AE%E4%BF%A1%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84AI%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB/&quot; target=&quot;_blank&quot;&gt;
    Windows部署指南
  &lt;/a&gt;
&lt;/p&gt;

## ✨核心功能
- 💫 涵盖打造数字分身的全链路方案，包括聊天数据导出、预处理、模型训练、部署
- 💬 使用微信聊天记录微调LLM，让大模型有&quot;那味儿&quot;
- 🔗 绑定到微信、QQ、Telegram、企微、飞书机器人，实现自己的数字分身
- 🛡️ 隐私信息过滤，本地化微调部署，数据安全可控

## 📋特性与说明

&gt; [!IMPORTANT]
&gt; ### 0.2.1版本支持了命令行工具，使用前需要重新执行 `uv pip install -e .` 

&gt; [!IMPORTANT]
&gt; 0.2.0版本进行了全面重构，数据集目录和脚本路径全部进行了修改，拉取新代码后，`csv`文件夹放在`dataset`下，并且需要重新安装依赖。

&gt; [!IMPORTANT]
&gt; - WeClone仍在快速迭代期，当前效果不代表最终效果。  
&gt; - 微调LLM效果很大程度取决于模型大小、聊天数据的数量和质量，理论上模型越大，数据越多，效果越好。   
&gt; - Windows环境未进行严格测试，可以使用WSL作为运行环境。详细教程可点击[Windows部署指南](https://blog.051088.xyz/2025/05/14/WeClone-%E7%94%A8%E5%BE%AE%E4%BF%A1%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84AI%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB/)查看。

### 硬件要求

项目默认使用Qwen2.5-7B-Instruct模型，LoRA方法对sft阶段微调，大约需要16GB显存。也可以使用[LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md#%E6%A8%A1%E5%9E%8B)支持的其他模型和方法。

需要显存的估算值：
| 方法                             | 精度 |   7B  |  14B  |  30B  |   70B  |   `x`B  |
| ------------------------------- | ---- | ----- | ----- | ----- | ------ | ------- |
| Full (`bf16` or `fp16`)         |  32  | 120GB | 240GB | 600GB | 1200GB | `18x`GB |
| Full (`pure_bf16`)              |  16  |  60GB | 120GB | 300GB |  600GB |  `8x`GB |
| Freeze/LoRA/GaLore/APOLLO/BAdam |  16  |  16GB |  32GB |  64GB |  160GB |  `2x`GB |
| QLoRA                           |   8  |  10GB |  20GB |  40GB |   80GB |   `x`GB |
| QLoRA                           |   4  |   6GB |  12GB |  24GB |   48GB | `x/2`GB |
| QLoRA                           |   2  |   4GB |   8GB |  16GB |   24GB | `x/4`GB |


## 环境搭建
1.cuda安装(已安装可跳过，**要求版本12.4及以上**)：[LLaMA Factory](https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/installation.html#cuda) 

2.建议使用 [uv](https://docs.astral.sh/uv/)安装依赖，这是一个非常快速的 Python 环境管理器。安装uv后，您可以使用以下命令创建一个新的Python环境并安装依赖项，注意这不包含音频克隆功能的依赖：
```bash
git clone https://github.com/xming521/WeClone.git
cd WeClone
uv venv .venv --python=3.10
source .venv/bin/activate # windows下执行 .venv\Scripts\activate
uv pip install --group main -e . 
```
&gt; [!TIP]
&gt; 如果要使用最新的模型进行微调，需要手动安装最新版LLaMA Factory：`uv pip install --upgrade git+https://github.com/hiyouga/LLaMA-Factory.git`,同时其他依赖版本也可能需要修改，例如vllm pytorch transforms

3.将配置文件模板复制一份并重命名为`settings.jsonc`，后续配置修改在此文件进行：
```bash
cp settings.template.jsonc settings.jsonc
```
&gt; [!NOTE]
&gt; 训练以及推理相关配置统一在文件`settings.jsonc`

4.使用以下命令测试CUDA环境是否正确配置并可被PyTorch识别，Mac不需要：
```bash
python -c &quot;import torch; print(&#039;CUDA是否可用:&#039;, torch.cuda.is_available());&quot;
```

5.（可选）安装FlashAttention，加速训练和推理：`uv pip install flash-attn --no-build-isolation`

## 模型下载
```bash
git lfs install
git clone https://www.modelscope.cn/Qwen/Qwen2.5-7B-Instruct.git
```
下载有问题使用其他方式下载：[模型的下载](https://www.modelscope.cn/docs/models/download)


## 数据准备

请使用[PyWxDump](https://github.com/xaoyaoo/PyWxDump)提取微信聊天记录（不支持4.0版本微信）。可以先将手机的聊天记录迁移（备份）到电脑，数据量更多一些。下载软件并解密数据库后，点击聊天备份，导出类型为CSV，可以导出多个联系人（不建议使用群聊记录），然后将导出的位于`wxdump_tmp/export` 的 `csv` 文件夹放在`./dataset`目录即可，也就是不同人聊天记录的文件夹一起放在 `./dataset/csv`。   

## 数据预处理

- 项目默认去除了数据中的手机号、身份证号、邮箱、网址。还在`settings.jsonc`中提供了一个禁用词词库`blocked_words`，可以自行添加需要过滤的词句（会默认去掉包括禁用词的整句）。
&gt; [!IMPORTANT]
&gt; 🚨 请一定注意保护个人隐私，不要泄露个人信息！

- 执行以下命令对数据进行处理，可以根据自己的聊天风格修改settings.jsonc的`make_dataset_args`。
```bash
weclone-cli make-dataset
```
- 目前仅支持时间窗口策略，根据`single_combine_time_window`将单人连续消息通过逗号连接合并为一句，根据`qa_match_time_window`匹配问答对。
- 可以启用`clean_dataset`中的`enable_clean`选项，对数据进行清洗，以达到更好效果。当前使用llm judge对聊天记录进行打分，使用vllm进行离线推理。在得到`llm打分分数分布情况`后，调整`accept_score`选择可以接受的分数，再适当降低`train_sft_args`的`lora_dropout`参数提升拟合效果。

## 配置参数并微调模型

- (可选)修改 `settings.jsonc` 的 `model_name_or_path` 和 `template` 选择本地下载好的其他模型。  
- 修改`per_device_train_batch_size`以及`gradient_accumulation_steps`来调整显存占用。  
- 可以根据自己数据集的数量和质量修改`train_sft_args`的`num_train_epochs`、`lora_rank`、`lora_dropout`等参数。

### 单卡训练
```bash
weclone-cli train-sft
```
多卡环境单卡训练，需要先执行 `export CUDA_VISIBLE_DEVICES=0`

### 多卡训练
取消`settings.jsonc`中`deepspeed`行代码注释，使用以下命令多卡训练：
```bash
uv pip install deepspeed
deepspeed --num_gpus=使用显卡数量 weclone/train/train_sft.py
```

### 使用浏览器demo简单推理
可以在这一步测试出合适的temperature、top_p值，修改settings.jsonc的`infer_args`后，供后续推理时使用。
```bash
weclone-cli webchat-demo
```

### 使用接口进行推理

```bash
weclone-cli server
```

### 使用常见聊天问题测试
不包含询问个人信息的问题，仅有日常聊天。测试结果在test_result-my.txt。
```bash
weclone-cli server
weclone-cli test-model
```

## 🖼️ 微调效果
使用Qwen2.5-14B-Instruct模型，大概3万条处理后的有效数据，loss降到了3.5左右的效果。
&lt;details&gt;
&lt;summary&gt;截图&lt;/summary&gt;
&lt;div style=&quot;display: flex; flex-wrap: wrap; gap: 10px;&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/0775ec52-452b-485f-9785-c6eb7b277132&quot; alt=&quot;alt text&quot; style=&quot;width: 48%; min-width: 150px;&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/8c7628b5-da70-4c37-9e51-fdfb0eadd2df&quot; alt=&quot;alt text&quot; style=&quot;width: 48%; min-width: 150px;&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/523aa742-2aa3-40e9-bd67-b98b336e83a8&quot; alt=&quot;alt text&quot; style=&quot;width: 48%; min-width: 150px;&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/dabf0603-dcc4-4a47-b5c3-2bbc036820d9&quot; alt=&quot;alt text&quot; style=&quot;width: 48%; min-width: 150px;&quot;&gt;
&lt;/div&gt;
&lt;/details&gt;


## 🤖 部署到聊天机器人

### AstrBot

[AstrBot](https://github.com/AstrBotDevs/AstrBot) 是易上手的多平台 LLM 聊天机器人及开发框架 ✨ 平台支持 QQ、QQ频道、Telegram、微信、企微、飞书。      

使用步骤：
1. 部署 AstrBot
2. 在 AstrBot 中部署消息平台
3. 执行 `weclone-cli server` 启动api服务
4. 在 AstrBot 中新增服务提供商，类型选择OpenAI，API Base URL 根据AstrBot部署方式填写（例如docker部署可能为http://172.17.0.1:8005/v1） ，模型填写gpt-3.5-turbo,API Key随意填写一个
5. 微调后不支持工具调用，请先关掉默认的工具，消息平台发送指令： `/tool off all`，否则会没有微调后的效果。 
6. 根据微调时使用的default_system，在 AstrBot 中设置系统提示词。
![5](https://github.com/user-attachments/assets/19de7072-076a-4cdf-8ae6-46b9b89f536a)
&gt; [!IMPORTANT]
&gt; 检查api_service的日志，尽量保证大模型服务请求的参数和微调时一致，tool插件能力都关掉。
7. 调整采样参数，例如temperature、top_p、top_k等
[配置自定义的模型参数](https://astrbot.app/config/model-config.html#%E9%85%8D%E7%BD%AE%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0)

### LangBot

[LangBot](https://github.com/RockChinQ/LangBot) 是一个开源的接入全球多种即时通信平台的 LLM 机器人平台，适合各种场景使用。

1. [部署 LangBot](https://github.com/RockChinQ/LangBot#-%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8)
2. 在 LangBot 中添加一个机器人
4. 在模型页添加新模型，名称`gpt-3.5-turbo`，供应商选择 OpenAI，填写 请求 URL 为 WeClone 的地址，详细连接方式可以参考[文档](https://docs.langbot.app/zh/workshop/network-details.html)，API Key 任意填写。

&lt;img width=&quot;400px&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/fc167dea-7c93-4d94-9c5f-db709d0320ba&quot; /&gt;

6. 在流水线配置中选择刚才添加的模型，或修改提示词配置

&lt;img width=&quot;400px&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/dbb0fd0a-f760-42db-acd0-bb99c859b52e&quot; /&gt;

## 📌 路线图
- [ ] 更丰富的上下文：包括上下文对话、聊天对象信息、时间等 + 思考
- [ ] Memory 支持
- [ ] 支持多模态
- [ ] 数据增强
- [ ] 支持GUI

## 问题解决
- 微调问题：[LLaMA-Factory| FAQs | 常见问题](https://github.com/hiyouga/LLaMA-Factory/issues/4614) 或者更方便的 [![更方便的Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/hiyouga/LLaMA-Factory)

## ❤️ 贡献代码

欢迎任何 Issues/Pull Requests！

你可以通过查看Issues或帮助审核 PR（拉取请求）来贡献。对于新功能的添加，请先通过 Issue 讨论。   
运行`uv pip install --group dev -e .`安装开发依赖。   
项目使用`pytest`测试(测试脚本待完善)，`pyright`检查类型，`ruff`检查代码格式。


## ⚠️ 免责声明
&gt; [!CAUTION]
&gt; 请勿用于非法用途，否则后果自负。
&lt;details&gt;
&lt;summary&gt;1. 使用目的&lt;/summary&gt;

* 本项目仅供学习交流使用，**请勿用于非法用途**，**请勿用于非法用途**，**请勿用于非法用途**，否则后果自负。
* 用户理解并同意，任何违反法律法规、侵犯他人合法权益的行为，均与本项目及其开发者无关，后果由用户自行承担。

2. 使用期限

* 您应该在下载保存使用本项目的24小时内，删除本项目的源代码和程序；超出此期限的任何使用行为，一概与本项目及其开发者无关。

3. 操作规范

* 本项目仅允许在授权情况下使用数据训练，严禁用于非法目的，否则自行承担所有相关责任；用户如因违反此规定而引发的任何法律责任，将由用户自行承担，与本项目及其开发者无关。
* 严禁用于窃取他人隐私，严禁用于窃取他人隐私，严禁用于窃取他人隐私，否则自行承担所有相关责任。

4. 免责声明接受

* 下载、保存、进一步浏览源代码或者下载安装、编译使用本程序，表示你同意本警告，并承诺遵守它;

5. 禁止用于非法测试或渗透

* 禁止利用本项目的相关技术从事非法测试或渗透，禁止利用本项目的相关代码或相关技术从事任何非法工作，如因此产生的一切不良后果与本项目及其开发者无关。
* 任何因此产生的不良后果，包括但不限于数据泄露、系统瘫痪、侵犯隐私等，均与本项目及其开发者无关，责任由用户自行承担。

6. 免责声明修改

* 本免责声明可能根据项目运行情况和法律法规的变化进行修改和调整。用户应定期查阅本页面以获取最新版本的免责声明，使用本项目时应遵守最新版本的免责声明。

7. 其他

* 除本免责声明规定外，用户在使用本项目过程中应遵守相关的法律法规和道德规范。对于因用户违反相关规定而引发的任何纠纷或损失，本项目及其开发者不承担任何责任。

* 请用户慎重阅读并理解本免责声明的所有内容，确保在使用本项目时严格遵守相关规定。

&lt;/details&gt;
请用户慎重阅读并理解本免责声明的所有内容，确保在使用本项目时严格遵守相关规定。

&lt;br&gt;  
&lt;br&gt;  
&lt;br&gt;  

## ⭐ Star History
&gt; [!TIP] 
&gt; 如果本项目对您有帮助，或者您关注本项目的未来发展，请给项目 Star，谢谢 

&lt;div align=&quot;center&quot;&gt;

[![Star History Chart](https://api.star-history.com/svg?repos=xming521/WeClone&amp;type=Date)](https://www.star-history.com/#xming521/WeClone&amp;Date)

&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt; 克隆我们，保留灵魂的芬芳 &lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mem0ai/mem0]]></title>
            <link>https://github.com/mem0ai/mem0</link>
            <guid>https://github.com/mem0ai/mem0</guid>
            <pubDate>Fri, 16 May 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[Memory for AI Agents; SOTA in AI Agent Memory; Announcing OpenMemory MCP - local and secure memory management.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mem0ai/mem0">mem0ai/mem0</a></h1>
            <p>Memory for AI Agents; SOTA in AI Agent Memory; Announcing OpenMemory MCP - local and secure memory management.</p>
            <p>Language: Python</p>
            <p>Stars: 30,432</p>
            <p>Forks: 2,935</p>
            <p>Stars today: 679 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/mem0ai/mem0&quot;&gt;
    &lt;img src=&quot;docs/images/banner-sm.png&quot; width=&quot;800px&quot; alt=&quot;Mem0 - The Memory Layer for Personalized AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot; style=&quot;display: flex; justify-content: center; gap: 20px; align-items: center;&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/11194&quot; target=&quot;blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/11194&quot; alt=&quot;mem0ai%2Fmem0 | Trendshift&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://mem0.ai&quot;&gt;Learn more&lt;/a&gt;
  ·
  &lt;a href=&quot;https://mem0.dev/DiG&quot;&gt;Join Discord&lt;/a&gt;
  ·
  &lt;a href=&quot;https://mem0.dev/demo&quot;&gt;Demo&lt;/a&gt;
  ·
  &lt;a href=&quot;https://mem0.dev/openmemory&quot;&gt;OpenMemory&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://mem0.dev/DiG&quot;&gt;
    &lt;img src=&quot;https://dcbadge.vercel.app/api/server/6PzXDgEjG5?style=flat&quot; alt=&quot;Mem0 Discord&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://pepy.tech/project/mem0ai&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/dm/mem0ai&quot; alt=&quot;Mem0 PyPI - Downloads&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/mem0ai/mem0&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/commit-activity/m/mem0ai/mem0?style=flat-square&quot; alt=&quot;GitHub commit activity&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/mem0ai&quot; target=&quot;blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/mem0ai?color=%2334D058&amp;label=pypi%20package&quot; alt=&quot;Package version&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.npmjs.com/package/mem0ai&quot; target=&quot;blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/npm/v/mem0ai&quot; alt=&quot;Npm package&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.ycombinator.com/companies/mem0&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Y%20Combinator-S24-orange?style=flat-square&quot; alt=&quot;Y Combinator S24&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://mem0.ai/research&quot;&gt;&lt;strong&gt;📄 Building Production-Ready AI Agents with Scalable Long-Term Memory →&lt;/strong&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;⚡ +26% Accuracy vs. OpenAI Memory • 🚀 91% Faster • 💰 90% Fewer Tokens&lt;/strong&gt;
&lt;/p&gt;

##  🔥 Research Highlights
- **+26% Accuracy** over OpenAI Memory on the LOCOMO benchmark
- **91% Faster Responses** than full-context, ensuring low-latency at scale
- **90% Lower Token Usage** than full-context, cutting costs without compromise
- [Read the full paper](https://mem0.ai/research)

# Introduction

[Mem0](https://mem0.ai) (&quot;mem-zero&quot;) enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions. It remembers user preferences, adapts to individual needs, and continuously learns over time—ideal for customer support chatbots, AI assistants, and autonomous systems.

### Key Features &amp; Use Cases

**Core Capabilities:**
- **Multi-Level Memory**: Seamlessly retains User, Session, and Agent state with adaptive personalization
- **Developer-Friendly**: Intuitive API, cross-platform SDKs, and a fully managed service option

**Applications:**
- **AI Assistants**: Consistent, context-rich conversations
- **Customer Support**: Recall past tickets and user history for tailored help
- **Healthcare**: Track patient preferences and history for personalized care
- **Productivity &amp; Gaming**: Adaptive workflows and environments based on user behavior

## 🚀 Quickstart Guide &lt;a name=&quot;quickstart&quot;&gt;&lt;/a&gt;

Choose between our hosted platform or self-hosted package:

### Hosted Platform

Get up and running in minutes with automatic updates, analytics, and enterprise security.

1. Sign up on [Mem0 Platform](https://app.mem0.ai)
2. Embed the memory layer via SDK or API keys

### Self-Hosted (Open Source)

Install the sdk via pip:

```bash
pip install mem0ai
```

Install sdk via npm:
```bash
npm install mem0ai
```

### Basic Usage

Mem0 requires an LLM to function, with `gpt-4o-mini` from OpenAI as the default. However, it supports a variety of LLMs; for details, refer to our [Supported LLMs documentation](https://docs.mem0.ai/components/llms/overview).

First step is to instantiate the memory:

```python
from openai import OpenAI
from mem0 import Memory

openai_client = OpenAI()
memory = Memory()

def chat_with_memories(message: str, user_id: str = &quot;default_user&quot;) -&gt; str:
    # Retrieve relevant memories
    relevant_memories = memory.search(query=message, user_id=user_id, limit=3)
    memories_str = &quot;\n&quot;.join(f&quot;- {entry[&#039;memory&#039;]}&quot; for entry in relevant_memories[&quot;results&quot;])

    # Generate Assistant response
    system_prompt = f&quot;You are a helpful AI. Answer the question based on query and memories.\nUser Memories:\n{memories_str}&quot;
    messages = [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_prompt}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: message}]
    response = openai_client.chat.completions.create(model=&quot;gpt-4o-mini&quot;, messages=messages)
    assistant_response = response.choices[0].message.content

    # Create new memories from the conversation
    messages.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: assistant_response})
    memory.add(messages, user_id=user_id)

    return assistant_response

def main():
    print(&quot;Chat with AI (type &#039;exit&#039; to quit)&quot;)
    while True:
        user_input = input(&quot;You: &quot;).strip()
        if user_input.lower() == &#039;exit&#039;:
            print(&quot;Goodbye!&quot;)
            break
        print(f&quot;AI: {chat_with_memories(user_input)}&quot;)

if __name__ == &quot;__main__&quot;:
    main()
```

For detailed integration steps, see the [Quickstart](https://docs.mem0.ai/quickstart) and [API Reference](https://docs.mem0.ai/api-reference).

## 🔗 Integrations &amp; Demos

- **ChatGPT with Memory**: Personalized chat powered by Mem0 ([Live Demo](https://mem0.dev/demo))
- **Browser Extension**: Store memories across ChatGPT, Perplexity, and Claude ([Chrome Extension](https://chromewebstore.google.com/detail/onihkkbipkfeijkadecaafbgagkhglop?utm_source=item-share-cb))
- **Langgraph Support**: Build a customer bot with Langgraph + Mem0 ([Guide](https://docs.mem0.ai/integrations/langgraph))
- **CrewAI Integration**: Tailor CrewAI outputs with Mem0 ([Example](https://docs.mem0.ai/integrations/crewai))

## 📚 Documentation &amp; Support

- Full docs: https://docs.mem0.ai
- Community: [Discord](https://mem0.dev/DiG) · [Twitter](https://x.com/mem0ai)
- Contact: founders@mem0.ai

## Citation

We now have a paper you can cite:

```bibtex
@article{mem0,
  title={Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory},
  author={Chhikara, Prateek and Khant, Dev and Aryan, Saket and Singh, Taranjeet and Yadav, Deshraj},
  journal={arXiv preprint arXiv:2504.19413},
  year={2025}
}
```

## ⚖️ License

Apache 2.0 — see the [LICENSE](LICENSE) file for details.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mikumifa/biliTickerBuy]]></title>
            <link>https://github.com/mikumifa/biliTickerBuy</link>
            <guid>https://github.com/mikumifa/biliTickerBuy</guid>
            <pubDate>Fri, 16 May 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[b站 会员购 抢票 漫展 脚本 bilibili 图形化 纯接口 验证码预演练习]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mikumifa/biliTickerBuy">mikumifa/biliTickerBuy</a></h1>
            <p>b站 会员购 抢票 漫展 脚本 bilibili 图形化 纯接口 验证码预演练习</p>
            <p>Language: Python</p>
            <p>Stars: 1,482</p>
            <p>Forks: 221</p>
            <p>Stars today: 89 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/mikumifa/biliTickerBuy&quot; target=&quot;_blank&quot;&gt;
    &lt;img width=&quot;160&quot; src=&quot;assets/icon.ico&quot; alt=&quot;logo&quot;&gt;
  &lt;/a&gt;
  &lt;h2 id=&quot;koishi&quot;&gt;biliTickerBuy&lt;/h1&gt;

&lt;p&gt;
  &lt;!-- GitHub Downloads --&gt;
  &lt;a href=&quot;https://github.com/mikumifa/biliTickerBuy/releases&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/downloads/mikumifa/biliTickerBuy/total&quot; alt=&quot;GitHub all releases&quot;&gt;
  &lt;/a&gt;
  &lt;!-- GitHub Release Version --&gt;
  &lt;a href=&quot;https://github.com/mikumifa/biliTickerBuy/releases&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/v/release/mikumifa/biliTickerBuy&quot; alt=&quot;GitHub release (with filter)&quot;&gt;
  &lt;/a&gt;
  &lt;!-- GitHub Issues --&gt;
  &lt;a href=&quot;https://github.com/mikumifa/biliTickerBuy/issues&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/issues/mikumifa/biliTickerBuy&quot; alt=&quot;GitHub issues&quot;&gt;
  &lt;/a&gt;
  &lt;!-- GitHub Stars --&gt;
  &lt;a href=&quot;https://github.com/mikumifa/biliTickerBuy/stargazers&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/mikumifa/biliTickerBuy&quot; alt=&quot;GitHub Repo stars&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;a href=&quot;https://trendshift.io/repositories/11145&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11145&quot; alt=&quot;mikumifa%2FbiliTickerBuy | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

这是一个开源免费，简单易用的B站会员购辅助工具
&lt;/div&gt;






## 💻 快速安装

[下载链接](https://github.com/mikumifa/biliTickerBuy/releases) 

## 👀 使用说明书
前往飞书： https://n1x87b5cqay.feishu.cn/wiki/Eg4xwt3Dbiah02k1WqOcVk2YnMd

## ❗ 项目问题

程序使用问题： [点此链接前往discussions](https://github.com/mikumifa/biliTickerBuy/discussions)

反馈程序BUG或者提新功能建议： [点此链接向项目提出反馈BUG](https://github.com/mikumifa/biliTickerBuy/issues/new/choose)



## 🤩 项目贡献者

&lt;a href=&quot;https://github.com/mikumifa/biliTickerBuy/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=mikumifa/biliTickerBuy&amp;preview=true&amp;max=&amp;columns=&quot; /&gt;
&lt;/a&gt;
&lt;br /&gt;&lt;br /&gt;

## ⭐️ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=mikumifa/biliTickerBuy&amp;type=Date)](https://www.star-history.com/#mikumifa/biliTickerBuy&amp;Date)

## 📩 免责声明

详见[MIT License](./LICENSE)，切勿进行盈利，所造成的后果与本人无关。

## 💰 捐赠

如果你想支持这个项目的话 [爱发电](https://afdian.com/a/mikumifa)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[airweave-ai/airweave]]></title>
            <link>https://github.com/airweave-ai/airweave</link>
            <guid>https://github.com/airweave-ai/airweave</guid>
            <pubDate>Fri, 16 May 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[Airweave lets agents search any app]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/airweave-ai/airweave">airweave-ai/airweave</a></h1>
            <p>Airweave lets agents search any app</p>
            <p>Language: Python</p>
            <p>Stars: 1,947</p>
            <p>Forks: 195</p>
            <p>Stars today: 399 stars today</p>
            <h2>README</h2><pre>&lt;img width=&quot;1673&quot; alt=&quot;airweave-lettermark&quot; style=&quot;padding-bottom: 12px;&quot; src=&quot;https://github.com/user-attachments/assets/e79a9af7-2e93-4888-9cf4-0f700f19fe05&quot;/&gt;


&lt;div align=&quot;center&quot;&gt;

[![Ruff](https://github.com/airweave-ai/airweave/actions/workflows/ruff.yml/badge.svg)](https://github.com/airweave-ai/airweave/actions/workflows/ruff.yml)
[![ESLint](https://github.com/airweave-ai/airweave/actions/workflows/eslint.yml/badge.svg)](https://github.com/airweave-ai/airweave/actions/workflows/eslint.yml)
[![Backend Tests](https://github.com/airweave-ai/airweave/actions/workflows/tests.yml/badge.svg?branch=main)](https://github.com/airweave-ai/airweave/actions/workflows/tests.yml)
[![Codecov](https://codecov.io/gh/airweave-ai/airweave/branch/main/graph/badge.svg)](https://codecov.io/gh/airweave-ai/airweave)
[![Discord](https://img.shields.io/discord/1323415085011701870?label=Discord&amp;logo=discord&amp;logoColor=white&amp;style=flat-square)](https://discord.com/invite/484HY9Ehxt)

&lt;/div&gt;

# Airweave

**Airweave is a tool that lets agents semantically search any app.** It&#039;s MCP compatible and seamlessly connects any app, database, or API, to transform their contents into agent-ready knowledge.

&lt;div align=&quot;center&quot;&gt;
  
### 🎥 Watch Demo

https://github.com/user-attachments/assets/abdf85cb-a8f5-4b6c-b5a3-d4b5177e6bda

&lt;/div&gt;

## Overview

Airweave simplifies the process of making information retrievable for your agent. Whether you have structured or unstructured data, Airweave helps you break it into processable entities, store the data and make it retrievable through REST and MCP endpoints.

## Table of Contents

- [Airweave](#airweave)
    - [🎥 Watch Demo](#-watch-demo)
  - [Overview](#overview)
  - [Table of Contents](#table-of-contents)
  - [🚀 Quick Start](#-quick-start)
  - [🔌 Supported Integrations](#-supported-integrations)
  - [💻 Usage](#-usage)
    - [Frontend](#frontend)
    - [API](#api)
  - [📦 SDKs](#-sdks)
    - [Python](#python)
    - [TypeScript/JavaScript](#typescriptjavascript)
  - [🔑 Key Features](#-key-features)
  - [🔧 Technology Stack](#-technology-stack)
  - [🛣️ Roadmap](#️-roadmap)
  - [👥 Contributing](#-contributing)
  - [📄 License](#-license)
  - [🔗 Connect](#-connect)

## 🚀 Quick Start

Make sure docker and docker-compose are installed, then...

```bash
# 1. Clone the repository
git clone https://github.com/airweave-ai/airweave.git
cd airweave

# 2. Build and run
chmod +x start.sh
./start.sh
```

That&#039;s it! Access the dashboard at http://localhost:8080

## 🔌 Supported Integrations

&lt;!-- START_APP_GRID --&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;div style=&quot;display: inline-block; text-align: center; padding: 4px;&quot;&gt;
    &lt;img src=&quot;frontend/src/components/icons/apps/asana.svg&quot; alt=&quot;Asana&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/calendly.svg&quot; alt=&quot;Calendly&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/chat-gpt.svg&quot; alt=&quot;Chat-gpt&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/clickup.svg&quot; alt=&quot;Clickup&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/confluence.svg&quot; alt=&quot;Confluence&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/dropbox.svg&quot; alt=&quot;Dropbox&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/elasticsearch.svg&quot; alt=&quot;Elasticsearch&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/facebook.svg&quot; alt=&quot;Facebook&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;
    &lt;img src=&quot;frontend/src/components/icons/apps/github.svg&quot; alt=&quot;Github&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/gmail.svg&quot; alt=&quot;Gmail&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/google_calendar.svg&quot; alt=&quot;Google Calendar&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/google_drive.svg&quot; alt=&quot;Google Drive&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/hubspot.svg&quot; alt=&quot;Hubspot&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/intercom.svg&quot; alt=&quot;Intercom&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/jira.svg&quot; alt=&quot;Jira&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/linear.svg&quot; alt=&quot;Linear&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;
    &lt;img src=&quot;frontend/src/components/icons/apps/linkedin.svg&quot; alt=&quot;Linkedin&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/mailchimp.svg&quot; alt=&quot;Mailchimp&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/monday.svg&quot; alt=&quot;Monday&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/mysql.svg&quot; alt=&quot;Mysql&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/notion.svg&quot; alt=&quot;Notion&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/onedrive.svg&quot; alt=&quot;Onedrive&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/oracle.svg&quot; alt=&quot;Oracle&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/outlook_calendar.svg&quot; alt=&quot;Outlook Calendar&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;
    &lt;img src=&quot;frontend/src/components/icons/apps/outlook_mail.svg&quot; alt=&quot;Outlook Mail&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/perplexity.svg&quot; alt=&quot;Perplexity&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/postgresql.svg&quot; alt=&quot;Postgresql&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/salesforce.svg&quot; alt=&quot;Salesforce&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/slack.svg&quot; alt=&quot;Slack&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/sql_server.svg&quot; alt=&quot;Sql Server&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/sqlite.svg&quot; alt=&quot;Sqlite&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/stripe.svg&quot; alt=&quot;Stripe&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;
    &lt;span style=&quot;width: 40px; display: inline-block; margin: 4px;&quot;&gt;&lt;/span&gt;&lt;span style=&quot;width: 40px; display: inline-block; margin: 4px;&quot;&gt;&lt;/span&gt;&lt;img src=&quot;frontend/src/components/icons/apps/todoist.svg&quot; alt=&quot;Todoist&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/trello.svg&quot; alt=&quot;Trello&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/whatsapp.svg&quot; alt=&quot;Whatsapp&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;&lt;img src=&quot;frontend/src/components/icons/apps/zendesk.svg&quot; alt=&quot;Zendesk&quot; width=&quot;40&quot; height=&quot;40&quot; style=&quot;margin: 4px; padding: 2px;&quot; /&gt;
  &lt;/div&gt;
&lt;/p&gt;

&lt;!-- END_APP_GRID --&gt;

## 💻 Usage

### Frontend
- Access the UI at `http://localhost:8080`
- Connect sources, configure syncs, and query data

### API
- Swagger docs: `http://localhost:8001/docs`
- Create connections, trigger syncs, and search data

## 📦 SDKs

### Python

```bash
pip install airweave-sdk
```

```python
from airweave import AirweaveClient

client = AirweaveClient(api_key=&quot;your-api-key&quot;)

# List all sources
sources = client.sources.list()

# Create a sync job
job = client.sync.create_sync(
  name=&quot;My first sync&quot;,
  source_connection_id=source_id,
  run_immediately=True
)
```

### TypeScript/JavaScript

```bash
npm install @airweave/sdk
# or
yarn add @airweave/sdk
```

```typescript
import { AirweaveClient } from &quot;@airweave/sdk&quot;;

const client = new AirweaveClient({
  apiKey: &quot;your-api-key&quot;,
});

// List sources
const sources = await client.sources.list();

// Create a sync job
const job = await client.sync.create_sync({
  name: &quot;My first sync&quot;,
  source_connection_id: sourceId,
  run_immediately: true,
});
```

## 🔑 Key Features

- **Data synchronization** from 25+ sources with minimal config
- **Entity extraction** and transformation pipeline
- **Multi-tenant** architecture with OAuth2
- **Incremental updates** using content hashing
- **Semantic search** for agent queries
- **Versioning** for data changes
- **White-labeling** support for SaaS builders

## 🔧 Technology Stack

- **Frontend**: React/TypeScript with ShadCN
- **Backend**: FastAPI (Python)
- **Databases**: PostgreSQL (metadata), Qdrant (vectors)
- **Deployment**: Docker Compose (dev), Kubernetes (prod)

## 🛣️ Roadmap

- Additional source integrations
- Redis worker queues for large-scale syncs
- Webhooks for event-driven syncs
- Kubernetes support via Helm charts

## 👥 Contributing

We welcome contributions! Please check [CONTRIBUTING.md](https://github.com/airweave-ai/airweave/blob/main/CONTRIBUTING.md) for details.

## 📄 License

Airweave is released under the [MIT](LICENSE) license.

## 🔗 Connect

- **[Discord](https://discord.com/invite/484HY9Ehxt)** - Get help and discuss features
- **[GitHub Issues](https://github.com/airweave-ai/airweave/issues)** - Report bugs or request features
- **[Twitter](https://x.com/airweave_ai)** - Follow for updates
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[openai/simple-evals]]></title>
            <link>https://github.com/openai/simple-evals</link>
            <guid>https://github.com/openai/simple-evals</guid>
            <pubDate>Fri, 16 May 2025 00:04:22 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/openai/simple-evals">openai/simple-evals</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 3,351</p>
            <p>Forks: 325</p>
            <p>Stars today: 205 stars today</p>
            <h2>README</h2><pre># Overview
This repository contains a lightweight library for evaluating language models.
We are open sourcing it so we can be transparent about the accuracy numbers we&#039;re publishing alongside our latest models.

## Benchmark Results

| Model                        | Prompt        | MMLU   | GPQA [^8]   | MATH [^6]| HumanEval | MGSM[^5] | DROP[^5]&lt;br&gt;(F1, 3-shot) | SimpleQA
|:----------------------------:|:-------------:|:------:|:------:|:--------:|:---------:|:------:|:--------------------------:|:---------:|
| **o3**                         |               |        |        |          |           |        |                             |                      |           |
| o3-high [^10]                | n/a [^7]      |  93.3  |  83.4  |   98.1   |  88.4     |  92.0  |  89.8                      |  48.6     |
| o3 [^9] [^10]                | n/a           |  92.9  |  82.8  |   97.8   |  87.4     |  92.3  |  80.6                      |  49.4     |
| o3-low [^10]                 | n/a           |  92.8  |  78.6  |   96.9   |  87.3     |  91.9  |  82.3                      |  49.4     |
| **o4-mini**                    |               |        |        |          |           |        |                             |                      |
| o4-mini-high [^9] [^10]      | n/a           |  90.3  |  81.3  |   98.2   |  99.3     |  93.5  |  78.1                      |  19.3     |
| o4-mini [^9] [^10]           | n/a           |  90.0  |  77.6  |   97.5   |  97.3     |  93.7  |  77.7                      |  20.2     |
| o4-mini-low [^10]            | n/a           |  89.5  |  73.6  |   96.2   |  95.9     |  93.0  |  76.0                      |  20.2     |
| **o3-mini**                    |               |        |        |          |           |        |                             |                      |           |
| o3-mini-high                 | n/a           |  86.9  |  77.2  |   97.9   |  97.6     |  92.0  |  80.6                      |  13.8     |
| o3-mini                      | n/a           |  85.9  |  74.9  |   97.3   |  96.3     |  90.8  |  79.2                      |  13.4     |
| o3-mini-low                  | n/a           |  84.9  |  67.6  |   95.8   |  94.5     |  89.4  |  77.6                      |  13.0     |
| **o1**                         |               |        |        |          |           |        |                             |                      |
|  o1                          | n/a           |  91.8  |  75.7  |   96.4   |    -      |  89.3  |  90.2                      |  42.6     |
| o1-preview                   | n/a           |  90.8  |  73.3  |   85.5   |  92.4     |  90.8  |  74.8                      |  42.4     |
| o1-mini                      | n/a           |  85.2  |  60.0  |   90.0   |  92.4     |  89.9  |  83.9                      |  07.6     |
| **GPT-4.1**                            |               |        |        |          |           |        |                             |                      |           |
| gpt-4.1-2025-04-14           | assistant [^2]|  90.2  |  66.3  |   82.1   |   94.5    |  86.9  |  79.4                      | 41.6      |
| gpt-4.1-mini-2025-04-14      | assistant     |  87.5  |  65.0  |   81.4   |   93.8    |  88.2  |  81.0                      | 16.8      |
| gpt-4.1-nano-2025-04-14      | assistant     |  80.1  |  50.3  |   62.3   |   87.0    |  73.0  |  82.2                      | 07.6      |
| **GPT-4o**                     |               |        |        |          |           |        |                             |                      |           |
| gpt-4o-2024-11-20            | assistant     |  85.7  |  46.0  |   68.5   |   90.2    |  90.3  |  81.5                      | 38.8      |
| gpt-4o-2024-08-06            | assistant     |  88.7  |  53.1  |   75.9   |   90.2    |  90.0  |  79.8                      | 40.1      |
| gpt-4o-2024-05-13            | assistant     |  87.2  |  49.9  |   76.6   |   91.0    |  89.9  |  83.7                      | 39.0      |
| gpt-4o-mini-2024-07-18       | assistant     |  82.0  |  40.2  |   70.2   |   87.2    |  87.0  |  79.7                      | 09.5      |
| **GPT-4.5-preview**          |               |        |        |          |           |        |                            |           |
| gpt-4.5-preview-2025-02-27   | assistant     |  90.8  |  69.5  |   87.1   |   88.6    |  86.9  |  83.4                      | 62.5      |
| **GPT-4 Turbo and GPT-4**    |               |        |        |          |           |        |                            |           |
| gpt-4-turbo-2024-04-09       | assistant     |  86.7  |  49.3  |   73.4   |   88.2    |  89.6  |  86.0                      | 24.2      |
| gpt-4-0125-preview           | assistant     |  85.4  |  41.4  |   64.5   |   86.6    |  85.1  |  81.5                      | n/a       |
| gpt-4-1106-preview           | assistant     |  84.7  |  42.5  |   64.3   |   83.7    |  87.1  |  83.2                      | n/a       |
| **Other Models (Reported)**   |               |        |        |        |           |        |                           |
| [Claude 3.5 Sonnet](https://www.anthropic.com/news/claude-3-5-sonnet) | unknown |  88.3  |  59.4  |  71.1  |   92.0    | 91.6 | 87.1 |  28.9 |
| [Claude 3 Opus](https://www.anthropic.com/news/claude-3-family) | unknown |  86.8  |  50.4  |  60.1  |   84.9    |   90.7   |  83.1 |  23.5 |
| [Llama 3.1 405b](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md) | unknown |  88.6  |  50.7  |  73.8  |   89.0    | 91.6 |  84.8                   | n/a
| [Llama 3.1 70b](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md) | unknown |  82.0  |  41.7  |  68.0  |   80.5    |  86.9  |  79.6                   | n/a
| [Llama 3.1 8b](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md) | unknown |  68.4  |  30.4  |  51.9  |   72.6    |  68.9  |  59.5                   | n/a
| [Grok 2](https://x.ai/blog/grok-2) | unknown | 87.5 | 56.0 | 76.1 | 88.4 | n/a | n/a | n/a
| [Grok 2 mini](https://x.ai/blog/grok-2) | unknown | 86.2 | 51.0 | 73.0 | 85.7 | n/a | n/a | n/a
| [Gemini 1.0 Ultra](https://goo.gle/GeminiV1-5) | unknown | 83.7 | n/a | 53.2 | 74.4 | 79.0 | 82.4 | n/a
| [Gemini 1.5 Pro](https://goo.gle/GeminiV1-5) | unknown | 81.9 | n/a | 58.5 | 71.9 | 88.7 | 78.9 | n/a
| [Gemini 1.5 Flash](https://goo.gle/GeminiV1-5) | unknown | 77.9 | 38.6 | 40.9 | 71.5 | 75.5 | 78.4 | n/a

## Background

Evals are sensitive to prompting, and there&#039;s significant variation in the formulations used in recent publications and libraries.
Some use few-shot prompts or role playing prompts (&quot;You are an expert software programmer...&quot;).
These approaches are carryovers from evaluating *base models* (rather than instruction/chat-tuned models) and from models that were worse at following instructions.

For this library, we are emphasizing the *zero-shot, chain-of-thought* setting, with simple instructions like &quot;Solve the following multiple choice problem&quot;. We believe that this prompting technique is a better reflection of the models&#039; performance in realistic usage.

**We will not be actively maintaining this repository and monitoring PRs and Issues.** In particular, we&#039;re not accepting new evals. Here are the changes we might accept.
- Bug fixes (hopefully not needed!)
- Adding adapters for new models
- Adding new rows to the table below with eval results, given new models and new system prompts.

This repository is NOT intended as a replacement for https://github.com/openai/evals, which is designed to be a comprehensive collection of a large number of evals.

## Evals

This repository currently contains the following evals:

- MMLU: Measuring Massive Multitask Language Understanding, reference: https://arxiv.org/abs/2009.03300, https://github.com/hendrycks/test, [MIT License](https://github.com/hendrycks/test/blob/master/LICENSE)
- MATH: Measuring Mathematical Problem Solving With the MATH Dataset, reference: https://arxiv.org/abs/2103.03874, https://github.com/hendrycks/math, [MIT License](https://github.com/idavidrein/gpqa/blob/main/LICENSE)
- GPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark, reference: https://arxiv.org/abs/2311.12022, https://github.com/idavidrein/gpqa/,  [MIT License](https://github.com/idavidrein/gpqa/blob/main/LICENSE)
- DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs, reference: https://arxiv.org/abs/1903.00161, https://allenai.org/data/drop, [Apache License 2.0](https://github.com/allenai/allennlp-models/blob/main/LICENSE)
- MGSM: Multilingual Grade School Math Benchmark (MGSM), Language Models are Multilingual Chain-of-Thought Reasoners, reference: https://arxiv.org/abs/2210.03057, https://github.com/google-research/url-nlp, [Creative Commons Attribution 4.0 International Public License (CC-BY)](https://github.com/google-research/url-nlp/blob/main/LICENSE)
- HumanEval: Evaluating Large Language Models Trained on Code, reference https://arxiv.org/abs/2107.03374, https://github.com/openai/human-eval, [MIT License](https://github.com/openai/human-eval/blob/master/LICENSE)
- SimpleQA: Measuring short-form factuality in large language models, reference: https://openai.com/index/introducing-simpleqa, [MIT License](https://github.com/openai/simple-evals/blob/main/LICENSE)
- BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents, reference: https://openai.com/index/browsecomp, [MIT License](https://github.com/openai/simple-evals/blob/main/LICENSE)
- HealthBench: Evaluating Large Language Models Towards Improved Human Health, reference: https://openai.com/index/healthbench, [MIT License](https://github.com/openai/simple-evals/blob/main/LICENSE)

## Samplers

We have implemented sampling interfaces for the following language model APIs:

- OpenAI: https://platform.openai.com/docs/overview
- Claude: https://www.anthropic.com/api

Make sure to set the `*_API_KEY` environment variables before using these APIs.

## Setup

Due to the optional dependencies, we&#039;re not providing a unified setup mechanism. Instead, we&#039;re providing instructions for each eval and sampler.

For [HumanEval](https://github.com/openai/human-eval/) (python programming)
```bash
git clone https://github.com/openai/human-eval
pip install -e human-eval
```

For the [OpenAI API](https://pypi.org/project/openai/):
```bash
pip install openai
```

For the [Anthropic API](https://docs.anthropic.com/claude/docs/quickstart-guide):
```bash
pip install anthropic
```

## Running the evals
```bash
python -m simple-evals.simple_evals --list-models
```
This will list all the models that you can evaluate.

To run the evaluations, you can use the following command:
```bash
python -m simple-evals.simple_evals --model &lt;model_name&gt; --examples &lt;num_examples&gt;
```
This will launch evaluations through the OpenAI API.

## Notes

[^1]:chatgpt system message: &quot;You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture.\nKnowledge cutoff: 2023-12\nCurrent date: 2024-04-01&quot;
[^2]:assistant system message in [OpenAI API doc](https://platform.openai.com/docs/api-reference/introduction): &quot;You are a helpful assistant.&quot; .
[^3]:claude-3 empty system message: suggested by Anthropic API doc, and we have done limited experiments due to [rate limit](https://docs.anthropic.com/claude/reference/rate-limits) issues, but we welcome PRs with alternative choices.
[^4]:claude-3 lmsys system message: system message in LMSYS [Fast-chat open source code](https://github.com/lm-sys/FastChat/blob/7899355ebe32117fdae83985cf8ee476d2f4243f/fastchat/conversation.py#L894): &quot;The assistant is Claude, created by Anthropic. The current date is {{currentDateTime}}. Claude&#039;s knowledge base was last updated ... &quot;. We have done limited experiments due to [rate limit](https://docs.anthropic.com/claude/reference/rate-limits) issues, but we welcome PRs with alternative choices.
[^5]:We believe these evals are saturated for our newer models, but are reporting them for completeness.
[^6]:For newer models (anything on or after o1) we evaluate on [MATH-500](https://github.com/openai/prm800k/tree/main/prm800k/math_splits), which is a newer, IID version of MATH.
[^7]:o-series models do not support using a system prompt.
[^8]:Includes an answer regex tweak for GPQA benchmark.
[^9]:The default reasoning level for o3-mini is &quot;medium&quot;.
[^10]:These results are with no tools enabled for o3 or o4-mini

## Legal Stuff
By contributing to evals, you are agreeing to make your evaluation logic and data under the same MIT license as this repository. You must have adequate rights to upload any data used in an eval. OpenAI reserves the right to use this data in future service improvements to our product. Contributions to OpenAI evals will be subject to our usual Usage Policies: https://platform.openai.com/docs/usage-policies.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[facebookresearch/fairchem]]></title>
            <link>https://github.com/facebookresearch/fairchem</link>
            <guid>https://github.com/facebookresearch/fairchem</guid>
            <pubDate>Fri, 16 May 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[FAIR Chemistry's library of machine learning methods for chemistry]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/facebookresearch/fairchem">facebookresearch/fairchem</a></h1>
            <p>FAIR Chemistry's library of machine learning methods for chemistry</p>
            <p>Language: Python</p>
            <p>Stars: 1,210</p>
            <p>Forks: 306</p>
            <p>Stars today: 73 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt; &lt;code&gt;fairchem&lt;/code&gt; by FAIR Chemistry &lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;559&quot; height=&quot;200&quot; src=&quot;https://github.com/FAIR-Chem/fairchem/assets/45150244/5872c21c-8f39-41af-b703-af9817f0affe&quot;?
&lt;/p&gt;


&lt;h4 align=&quot;center&quot;&gt;

![tests](https://github.com/FAIR-Chem/fairchem/actions/workflows/test.yml/badge.svg?branch=main)
![PyPI - Version](https://img.shields.io/pypi/v/fairchem-core)
![Static Badge](https://img.shields.io/badge/python-3.10%2B-blue)

[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://github.com/codespaces/new/FAIR-Chem/fairchem?quickstart=1)

`fairchem` is the [FAIR](https://ai.meta.com/research/) Chemistry&#039;s centralized repository of all its data, models,
demos, and application efforts for materials science and quantum chemistry.

&gt; :warning: **FAIRChem version 2 is a breaking change from version 1 and is not compatible with our previous pretrained models and code.**
&gt; If you want to use an older model or code from version 1 you will need to install [version 1](https://pypi.org/project/fairchem-core/1.10.0/),
&gt; as detailed [here](#looking-for-fairchem-v1-models-and-code).

### Read our latest release post!
Read about the [UMA model and dataset](https://ai.meta.com/blog/meta-fair-science-new-open-source-releases/) release.

[![Meta FAIR Science Release](https://github.com/user-attachments/assets/acddd09b-ed6f-4d05-9a4b-9ba5e2301150)](https://ai.meta.com/blog/meta-fair-science-new-open-source-releases/?ref=shareable)

### Try the demo!
If you want to explore model capabilities check out our
[educational demo](https://facebook-fairchem-uma-demo.hf.space/)

[![Educational Demo](https://github.com/user-attachments/assets/7005d1bb-4459-403d-b299-d41fdd8c48ec)](https://facebook-fairchem-uma-demo.hf.space/)


### Installation
Install fairchem-core using pip,
```bash
pip install git+https://github.com/facebookresearch/fairchem.git@fairchem_core-2.0.0#subdirectory=packages/fairchem-core
```
**The PyPI install (pip install fairchem-core) is not available right now as we are waiting for a few dependencies to release their PyPI packages, will update this soon when it&#039;s available!**

### Quick Start
The easiest way to use pretrained models is via the [ASE](https://wiki.fysik.dtu.dk/ase/) `FAIRChemCalculator`.
A single uma model can be used for a wide range of applications in chemistry and materials science by picking the
appropriate task name for domain specific prediction.

#### Instantiate a calculator from a pretrained model
Make sure you have a Hugging Face account, have already applied for model access to the 
[UMA model repository](https://huggingface.co/facebook/UMA), and have logged in to Hugging Face using an access token.

#### Set the task for your application and calculate

- **oc20:** use this for catalysis
- **omat:** use this for inorganic materials
- **omol:** use this for molecules
- **odac:** use this for MOFs
- **omc:** use this for molecular crystals

Relax adsorbate on a catalytic surface,
```python
from ase.build import fcc100, add_adsorbate, molecule
from ase.optimize import LBFGS
from fairchem.core import FAIRChemCalculator

calc = FAIRChemCalculator(hf_hub_filename=&quot;uma_sm.pt&quot;, device=&quot;cuda&quot;, task_name=&quot;oc20&quot;)

# Set up your system as an ASE atoms object
slab = fcc100(&quot;Cu&quot;, (3, 3, 3), vacuum=8, periodic=True)
adsorbate = molecule(&quot;CO&quot;)
add_adsorbate(slab, adsorbate, 2.0, &quot;bridge&quot;)

slab.calc = calc

# Set up LBFGS dynamics object
opt = LBFGS(slab)
opt.run(0.05, 100)
```

Or relax an inorganic crystal,
```python
from ase.build import bulk
from ase.optimize import FIRE
from ase.filters import FrechetCellFilter
from fairchem.core import FAIRChemCalculator

calc = FAIRChemCalculator(hf_hub_filename=&quot;uma_sm.pt&quot;, device=&quot;cuda&quot;, task_name=&quot;omat&quot;)

atoms = bulk(&quot;Fe&quot;)
atoms.calc = calc

opt = LBFGS(FrechetCellFilter(atoms))
opt.run(0.05, 100)
```

Run molecular MD,
```python
from ase import units
from ase.io import Trajectory
from ase.md.langevin import Langevin
from ase.build import molecule
from fairchem.core import FAIRChemCalculator

calc = FAIRChemCalculator(hf_hub_filename=&quot;uma_sm.pt&quot;, device=&quot;cuda&quot;, task_name=&quot;omol&quot;)

atoms = molecule(&quot;H2O&quot;)
atoms.calc = calc

dyn = Langevin(
    atoms,
    timestep=0.1 * units.fs,
    temperature_K=400,
    friction=0.001 / units.fs,
)
trajectory = Trajectory(&quot;my_md.traj&quot;, &quot;w&quot;, atoms)
dyn.attach(trajectory.write, interval=1)
dyn.run(steps=1000)
```


### Looking for Fairchem V1, models and code?
Fairchem V2 is a major upgrade and we completely rewrote the trainer, fine-tuning, models and calculators. 

We plan to bring back the following models compatible with Fairchem V2 soon:
* Gemnet-OC
* EquiformersV2
* ESEN

We will also be releasing more detailed documentation on how to use Fairchem V2, stay tuned! 

The old OCPCalculator, trainer code will NOT be revived. We apologize for the inconvenience and please raise Issues if you need help!
In the meantime, you can still use models from fairchem version 1, by installing version 1,

```bash
pip install fairchem-core==1.10
```

And using the `OCPCalculator`
```python
from fairchem.core import OCPCalculator

calc = OCPCalculator(
    model_name=&quot;EquiformerV2-31M-S2EF-OC20-All+MD&quot;,
    local_cache=&quot;pretrained_models&quot;,
    cpu=False,
)
```

### LICENSE
`fairchem` is available under a [MIT License](LICENSE.md).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[happycola233/tchMaterial-parser]]></title>
            <link>https://github.com/happycola233/tchMaterial-parser</link>
            <guid>https://github.com/happycola233/tchMaterial-parser</guid>
            <pubDate>Fri, 16 May 2025 00:04:20 GMT</pubDate>
            <description><![CDATA[国家中小学智慧教育平台 电子课本下载工具，帮助您从智慧教育平台中获取电子课本的 PDF 文件网址并进行下载，让您更方便地获取课本内容。]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/happycola233/tchMaterial-parser">happycola233/tchMaterial-parser</a></h1>
            <p>国家中小学智慧教育平台 电子课本下载工具，帮助您从智慧教育平台中获取电子课本的 PDF 文件网址并进行下载，让您更方便地获取课本内容。</p>
            <p>Language: Python</p>
            <p>Stars: 1,041</p>
            <p>Forks: 119</p>
            <p>Stars today: 261 stars today</p>
            <h2>README</h2><pre># [国家中小学智慧教育平台](https://basic.smartedu.cn/tchMaterial/) 电子课本下载工具

![Python Version](https://img.shields.io/badge/Python-3.x-blue.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Made With Love❤️](https://img.shields.io/badge/Made_With-%E2%9D%A4-red.svg)

&gt; [!TIP]
&gt; 🚀最新版本 v3.0 现已发布，欢迎体验！

本工具可以帮助您从[**国家中小学智慧教育平台**](https://basic.smartedu.cn/tchMaterial/)获取电子课本的 PDF 文件网址并进行下载，让您更方便地获取课本内容。

&gt; [!NOTE]
&gt;
&gt; 自**2025 年 2 月**起，国家中小学智慧教育平台**需要登录**才能访问电子课本资源，用户需提供 **Access Token**（即登录凭据）才可正常使用本工具的下载功能。
&gt;
&gt; **👉请先按照[下方指南](#2-设置-access-token)设置 Access Token，否则程序将无法解析资源！**

## ✨工具特点

- **支持 Access Token 登录**🔑：支持用户手动输入 Access Token，在 Windows 操作系统下会存入注册表，下次启动可自动加载。
- **支持批量下载**📚：一次输入多个电子课本预览页面网址，即可批量下载 PDF 课本文件。
- **自动文件命名**📂：程序会自动使用教材名称作为文件名，方便管理下载的课本文件。
- **高 DPI 适配**🖥️：优化 UI 以适配高分辨率屏幕，避免界面模糊问题。
- **下载进度可视化**📊：实时显示下载进度，支持暂停/恢复操作。
- **跨平台支持**💻：支持 Windows、Linux、macOS 等操作系统（需要图形界面）。

![程序截图](./res/PixPin_2025-03-14_23-44-26.png)

## ⬇️下载与安装方法

### GitHub Releases 页面

由于我们的精力有限，本项目的 [GitHub Releases 页面](https://github.com/happycola233/tchMaterial-parser/releases)**仅会发布适用于 Windows 与 Linux 操作系统的 x64 架构**的程序。

在下载完成之后，即可运行本程序，不需要额外的安装步骤。

### Arch 用户软件仓库（AUR）

对于 **Arch Linux** 操作系统，本程序已发布至[Arch 用户软件仓库](https://aur.archlinux.org/packages/tchmaterial-parser)，因此您还可以通过在终端中输入以下命令安装：

```sh
yay -S tchmaterial-parser
```

感谢 [@iamzhz](https://github.com/iamzhz) 制作了本工具的发行包（[#26](../../issues/26)）！

## 🛠️使用方法

### 1. 输入教材链接📥

将电子课本的**预览页面网址**粘贴到程序文本框中，支持多个 URL（每行一个）。

**示例网址**：

```text
https://basic.smartedu.cn/tchMaterial/detail?contentType=assets_document&amp;contentId=XXXXXX&amp;catalogType=tchMaterial&amp;subCatalog=tchMaterial
```

### 2. 设置 Access Token🔑

若您第一次使用本程序，需点击 “**设置 Token**” 按钮，粘贴 Access Token 并保存。

1. **打开浏览器**，访问[国家中小学智慧教育平台](https://auth.smartedu.cn/uias/login)并**登录账号**。
2. 按下 **F12** 或 **Ctrl+Shift+I**，或右键——检查（审查元素）打开**开发者工具**，选择**控制台（Console）**。
3. 在控制台粘贴以下代码后回车（Enter）：

   ```js
   (function() {
     const authKey = Object.keys(localStorage).find(key =&gt; key.startsWith(&quot;ND_UC_AUTH&quot;));
     if (!authKey) {
       console.error(&quot;未找到 Access Token，请确保已登录！&quot;);
       return;
     }
     const tokenData = JSON.parse(localStorage.getItem(authKey));
     const accessToken = JSON.parse(tokenData.value).access_token;
     console.log(&quot;%cAccess Token: &quot;, &quot;color: green; font-weight: bold&quot;, accessToken);
   })();
   ```
  
4. 复制控制台输出的 **Access Token**，然后在本程序中点击 “**设置 Token**” 按钮，粘贴并保存 Token。

&gt; [!NOTE]
&gt; Access Token 可能会过期，若下载失败提示 **401 Unauthorized**，请重新获取并设置新的 Token。

### 3. 开始下载🚀

点击 “**下载**” 按钮，程序将自动解析并下载 PDF 课本。

本工具支持**批量下载**，所有 PDF 文件会自动按课本名称命名并保存在选定目录中。

## ❓常见问题

### 1. 为什么下载失败？⚠️

- 检查是否已[**正确设置 Access Token**](#2-设置-access-token)🔑，且没有过期。
- **确认网络连接是否正常**🌐，有时网络不稳定可能导致下载失败。
- **确保输入的网址有效**🔗，部分旧资源可能已被移除。

### 2. Access Token 保存在哪里？💾

- **Windows 操作系统**：Token 会存储在**注册表** `HKEY_CURRENT_USER\Software\tchMaterial-parser` 项中的 `AccessToken` 值。
- **Linux 操作系统**: Token 会存储在 `~/.config/tchMaterial-parser/data.json` 的文件中。
- **macOS 等操作系统**：Token 仅在运行时临时存储于内存，不会自动保存，程序重启后需重新输入，目前我们正在努力改进该功能。

### 3. Token 会不会泄露？🔐

- 本程序**不会上传** Token，也不会存储在云端，仅用于本地请求授权。
- **请勿在公开场合分享 Token**，以免您的账号被他人使用，造成严重后果。

## ⭐Star History

[![Star History Chart](https://api.star-history.com/svg?repos=happycola233/tchMaterial-parser&amp;type=Date)](https://star-history.com/#happycola233/tchMaterial-parser&amp;Date)

## 🤝贡献指南

如果您发现 Bug 或有改进建议，欢迎提交 **Issue** 或 **Pull Request**，让我们一起完善本工具！

## 📜许可证

本项目基于 [MIT 许可证](LICENSE)，欢迎自由使用和二次开发。

## 💌友情链接

- 📚您也可以在 [ChinaTextbook](https://github.com/TapXWorld/ChinaTextbook) 项目中下载归档的教材 PDF。
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[trycua/cua]]></title>
            <link>https://github.com/trycua/cua</link>
            <guid>https://github.com/trycua/cua</guid>
            <pubDate>Fri, 16 May 2025 00:04:19 GMT</pubDate>
            <description><![CDATA[c/ua is the Docker Container for Computer-Use AI Agents.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/trycua/cua">trycua/cua</a></h1>
            <p>c/ua is the Docker Container for Computer-Use AI Agents.</p>
            <p>Language: Python</p>
            <p>Stars: 6,817</p>
            <p>Forks: 269</p>
            <p>Stars today: 304 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; alt=&quot;Cua logo&quot; height=&quot;150&quot; srcset=&quot;img/logo_white.png&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; alt=&quot;Cua logo&quot; height=&quot;150&quot; srcset=&quot;img/logo_black.png&quot;&gt;
    &lt;img alt=&quot;Cua logo&quot; height=&quot;150&quot; src=&quot;img/logo_black.png&quot;&gt;
  &lt;/picture&gt;

  [![Python](https://img.shields.io/badge/Python-333333?logo=python&amp;logoColor=white&amp;labelColor=333333)](#)
  [![Swift](https://img.shields.io/badge/Swift-F05138?logo=swift&amp;logoColor=white)](#)
  [![macOS](https://img.shields.io/badge/macOS-000000?logo=apple&amp;logoColor=F0F0F0)](#)
  [![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?&amp;logo=discord&amp;logoColor=white)](https://discord.com/invite/mVnXXpdE85)
  &lt;br&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/13685&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13685&quot; alt=&quot;trycua%2Fcua | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

**c/ua** (pronounced &quot;koo-ah&quot;) enables AI agents to control full operating systems in high-performance virtual containers with near-native speed on Apple Silicon.

&lt;div align=&quot;center&quot;&gt;
  &lt;video src=&quot;https://github.com/user-attachments/assets/c619b4ea-bb8e-4382-860e-f3757e36af20&quot; width=&quot;800&quot; controls&gt;&lt;/video&gt;
&lt;/div&gt;

# 🚀 Quick Start

Get started with a Computer-Use Agent UI and a VM with a single command:


```bash
/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/trycua/cua/main/scripts/playground.sh)&quot;
```


This script will:
- Install Lume CLI for VM management (if needed)
- Pull the latest macOS CUA image (if needed)
- Set up Python environment and install/update required packages
- Launch the Computer-Use Agent UI

#### Supported [Agent Loops](https://github.com/trycua/cua/blob/main/libs/agent/README.md#agent-loops)
- [UITARS-1.5](https://github.com/trycua/cua/blob/main/libs/agent/README.md#agent-loops) - Run locally on Apple Silicon with MLX, or use cloud providers
- [OpenAI CUA](https://github.com/trycua/cua/blob/main/libs/agent/README.md#agent-loops) - Use OpenAI&#039;s Computer-Use Preview model
- [Anthropic CUA](https://github.com/trycua/cua/blob/main/libs/agent/README.md#agent-loops) - Use Anthropic&#039;s Computer-Use capabilities
- [OmniParser-v2.0](https://github.com/trycua/cua/blob/main/libs/agent/README.md#agent-loops) - Control UI with [Set-of-Marks prompting](https://som-gpt4v.github.io/) using any vision model

### System Requirements

- Mac with Apple Silicon (M1/M2/M3/M4 series)
- macOS 15 (Sequoia) or newer
- Disk space for VM images (30GB+ recommended)


# 💻 For Developers

### Step 1: Install Lume CLI

```bash
/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/trycua/cua/main/libs/lume/scripts/install.sh)&quot;
```

Lume CLI manages high-performance macOS/Linux VMs with near-native speed on Apple Silicon.

### Step 2: Pull the macOS CUA Image

```bash
lume pull macos-sequoia-cua:latest
```

The macOS CUA image contains the default Mac apps and the Computer Server for easy automation.

### Step 3: Install Python SDK

```bash
pip install &quot;cua-computer[all]&quot; &quot;cua-agent[all]&quot;
```

Alternatively, see the [Developer Guide](./docs/Developer-Guide.md) for building from source.

### Step 4: Use in Your Code

```python
from computer import Computer
from agent import ComputerAgent, LLM

async def main():
    # Start a local macOS VM with a 1024x768 display
    async with Computer(os_type=&quot;macos&quot;, display=&quot;1024x768&quot;) as computer:

        # Example: Direct control of a macOS VM with Computer
        await computer.interface.left_click(100, 200)
        await computer.interface.type_text(&quot;Hello, world!&quot;)
        screenshot_bytes = await computer.interface.screenshot()
        
        # Example: Create and run an agent locally using mlx-community/UI-TARS-1.5-7B-6bit
        agent = ComputerAgent(
          computer=computer,
          loop=&quot;UITARS&quot;,
          model=LLM(provider=&quot;MLXVLM&quot;, name=&quot;mlx-community/UI-TARS-1.5-7B-6bit&quot;)
        )
        await agent.run(&quot;Find the trycua/cua repository on GitHub and follow the quick start guide&quot;)

main()
```

For ready-to-use examples, check out our [Notebooks](./notebooks/) collection.

### Lume CLI Reference

```bash
# Install Lume CLI and background service
curl -fsSL https://raw.githubusercontent.com/trycua/cua/main/libs/lume/scripts/install.sh | bash

# List all VMs
lume ls

# Pull a VM image
lume pull macos-sequoia-cua:latest

# Create a new VM
lume create my-vm --os macos --cpu 4 --memory 8GB --disk-size 50GB

# Run a VM (creates and starts if it doesn&#039;t exist)
lume run macos-sequoia-cua:latest

# Stop a VM
lume stop macos-sequoia-cua_latest

# Delete a VM
lume delete macos-sequoia-cua_latest
```

### Lumier CLI Reference

For advanced container-like virtualization, check out [Lumier](./libs/lumier/README.md) - a Docker interface for macOS and Linux VMs.

```bash
# Install Lume CLI and background service
curl -fsSL https://raw.githubusercontent.com/trycua/cua/main/libs/lume/scripts/install.sh | bash

# Run macOS in a Docker container
docker run -it --rm \
    --name lumier-vm \
    -p 8006:8006 \
    -v $(pwd)/storage:/storage \
    -v $(pwd)/shared:/shared \
    -e VM_NAME=lumier-vm \
    -e VERSION=ghcr.io/trycua/macos-sequoia-cua:latest \
    -e CPU_CORES=4 \
    -e RAM_SIZE=8192 \
    -e HOST_STORAGE_PATH=$(pwd)/storage \
    -e HOST_SHARED_PATH=$(pwd)/shared \
    trycua/lumier:latest
```

## Resources

- [How to use the MCP Server with Claude Desktop or other MCP clients](./libs/mcp-server/README.md) - One of the easiest ways to get started with C/ua
- [How to use OpenAI Computer-Use, Anthropic, OmniParser, or UI-TARS for your Computer-Use Agent](./libs/agent/README.md)
- [How to use Lume CLI for managing desktops](./libs/lume/README.md)
- [Training Computer-Use Models: Collecting Human Trajectories with C/ua (Part 1)](https://www.trycua.com/blog/training-computer-use-models-trajectories-1)
- [Build Your Own Operator on macOS (Part 1)](https://www.trycua.com/blog/build-your-own-operator-on-macos-1)

## Modules

| Module | Description | Installation |
|--------|-------------|---------------|
| [**Lume**](./libs/lume/README.md) | VM management for macOS/Linux using Apple&#039;s Virtualization.Framework | `curl -fsSL https://raw.githubusercontent.com/trycua/cua/main/libs/lume/scripts/install.sh \| bash` |
| [**Lumier**](./libs/lumier/README.md) | Docker interface for macOS and Linux VMs | `docker pull trycua/lumier:latest` |
| [**Computer**](./libs/computer/README.md) | Interface for controlling virtual machines | `pip install &quot;cua-computer[all]&quot;` |
| [**Agent**](./libs/agent/README.md) | AI agent framework for automating tasks | `pip install &quot;cua-agent[all]&quot;` |
| [**MCP Server**](./libs/mcp-server/README.md) | MCP server for using CUA with Claude Desktop | `pip install cua-mcp-server` |
| [**SOM**](./libs/som/README.md) | Self-of-Mark library for Agent | `pip install cua-som` |
| [**PyLume**](./libs/pylume/README.md) | Python bindings for Lume | `pip install pylume` |
| [**Computer Server**](./libs/computer-server/README.md) | Server component for Computer | `pip install cua-computer-server` |
| [**Core**](./libs/core/README.md) | Core utilities | `pip install cua-core` |

## Computer Interface Reference

For complete examples, see [computer_examples.py](./examples/computer_examples.py) or [computer_nb.ipynb](./notebooks/computer_nb.ipynb)

```python
# Mouse Actions
await computer.interface.left_click(x, y)       # Left click at coordinates
await computer.interface.right_click(x, y)      # Right click at coordinates
await computer.interface.double_click(x, y)     # Double click at coordinates
await computer.interface.move_cursor(x, y)      # Move cursor to coordinates
await computer.interface.drag_to(x, y, duration)  # Drag to coordinates
await computer.interface.get_cursor_position()  # Get current cursor position

# Keyboard Actions
await computer.interface.type_text(&quot;Hello&quot;)     # Type text
await computer.interface.press_key(&quot;enter&quot;)     # Press a single key
await computer.interface.hotkey(&quot;command&quot;, &quot;c&quot;) # Press key combination

# Screen Actions
await computer.interface.screenshot()           # Take a screenshot
await computer.interface.get_screen_size()      # Get screen dimensions

# Clipboard Actions
await computer.interface.set_clipboard(text)    # Set clipboard content
await computer.interface.copy_to_clipboard()    # Get clipboard content

# File System Operations
await computer.interface.file_exists(path)      # Check if file exists
await computer.interface.directory_exists(path) # Check if directory exists
await computer.interface.run_command(cmd)       # Run shell command

# Accessibility
await computer.interface.get_accessibility_tree() # Get accessibility tree
```

## ComputerAgent Reference

For complete examples, see [agent_examples.py](./examples/agent_examples.py) or [agent_nb.ipynb](./notebooks/agent_nb.ipynb)

```python
# Import necessary components
from agent import ComputerAgent, LLM, AgentLoop, LLMProvider

# UI-TARS-1.5 agent for local execution with MLX
ComputerAgent(loop=AgentLoop.UITARS, model=LLM(provider=LLMProvider.MLXVLM, name=&quot;mlx-community/UI-TARS-1.5-7B-6bit&quot;))   
# OpenAI Computer-Use agent using OPENAI_API_KEY  
ComputerAgent(loop=AgentLoop.OPENAI, model=LLM(provider=LLMProvider.OPENAI, name=&quot;computer-use-preview&quot;))
# Anthropic Claude agent using ANTHROPIC_API_KEY
ComputerAgent(loop=AgentLoop.ANTHROPIC, model=LLM(provider=LLMProvider.ANTHROPIC))

# OmniParser loop for UI control using Set-of-Marks (SOM) prompting and any vision LLM
ComputerAgent(loop=AgentLoop.OMNI, model=LLM(provider=LLMProvider.OLLAMA, name=&quot;gemma3:12b-it-q4_K_M&quot;))      
# OpenRouter example using OAICOMPAT provider
ComputerAgent(
    loop=AgentLoop.OMNI,
    model=LLM(
        provider=LLMProvider.OAICOMPAT, 
        name=&quot;openai/gpt-4o-mini&quot;,
        provider_base_url=&quot;https://openrouter.ai/api/v1&quot;
    ),
    api_key=&quot;your-openrouter-api-key&quot;
)
```

## Demos

Check out these demos of the Computer-Use Agent in action:

&lt;details open&gt;
&lt;summary&gt;&lt;b&gt;MCP Server: Work with Claude Desktop and Tableau&lt;/b&gt;&lt;/summary&gt;
&lt;br&gt;
&lt;div align=&quot;center&quot;&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/9f573547-5149-493e-9a72-396f3cff29df&quot; width=&quot;800&quot; controls&gt;&lt;/video&gt;
&lt;/div&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;AI-Gradio: Multi-app workflow with browser, VS Code and terminal&lt;/b&gt;&lt;/summary&gt;
&lt;br&gt;
&lt;div align=&quot;center&quot;&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/723a115d-1a07-4c8e-b517-88fbdf53ed0f&quot; width=&quot;800&quot; controls&gt;&lt;/video&gt;
&lt;/div&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Notebook: Fix GitHub issue in Cursor&lt;/b&gt;&lt;/summary&gt;
&lt;br&gt;
&lt;div align=&quot;center&quot;&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/f67f0107-a1e1-46dc-aa9f-0146eb077077&quot; width=&quot;800&quot; controls&gt;&lt;/video&gt;
&lt;/div&gt;
&lt;/details&gt;

## Community

Join our [Discord community](https://discord.com/invite/mVnXXpdE85) to discuss ideas, get assistance, or share your demos!

## License

Cua is open-sourced under the MIT License - see the [LICENSE](LICENSE) file for details.

Microsoft&#039;s OmniParser, which is used in this project, is licensed under the Creative Commons Attribution 4.0 International License (CC-BY-4.0) - see the [OmniParser LICENSE](https://github.com/microsoft/OmniParser/blob/master/LICENSE) file for details.

## Contributing

We welcome contributions to CUA! Please refer to our [Contributing Guidelines](CONTRIBUTING.md) for details.

## Trademarks

Apple, macOS, and Apple Silicon are trademarks of Apple Inc. Ubuntu and Canonical are registered trademarks of Canonical Ltd. Microsoft is a registered trademark of Microsoft Corporation. This project is not affiliated with, endorsed by, or sponsored by Apple Inc., Canonical Ltd., or Microsoft Corporation.

## Stargazers

Thank you to all our supporters!

[![Stargazers over time](https://starchart.cc/trycua/cua.svg?variant=adaptive)](https://starchart.cc/trycua/cua)

## Contributors

&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt;
&lt;!-- prettier-ignore-start --&gt;
&lt;!-- markdownlint-disable --&gt;
&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/f-trycua&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/195596869?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;f-trycua&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;f-trycua&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-f-trycua&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;http://pepicrft.me&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/663605?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Pedro Piñera Buendía&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Pedro Piñera Buendía&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-pepicrft&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://iamit.in&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/5647941?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Amit Kumar&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Amit Kumar&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-aktech&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://productsway.com/&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/870029?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Dung Duc Huynh (Kaka)&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Dung Duc Huynh (Kaka)&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-jellydn&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;http://zaydkrunz.com&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/70227235?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Zayd Krunz&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Zayd Krunz&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-ShrootBuck&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/PrashantRaj18198&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/23168997?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Prashant Raj&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Prashant Raj&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-PrashantRaj18198&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://www.mobile.dev&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/847683?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Leland Takamine&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Leland Takamine&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-Leland-Takamine&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/ddupont808&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/3820588?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;ddupont&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;ddupont&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-ddupont808&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/Lizzard1123&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/46036335?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Ethan Gutierrez&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ethan Gutierrez&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-Lizzard1123&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://ricterz.me&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/5282759?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Ricter Zheng&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ricter Zheng&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-RicterZ&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://www.trytruffle.ai/&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/50844303?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Rahul Karajgikar&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Rahul Karajgikar&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-rahulkarajgikar&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/trospix&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/81363696?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;trospix&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;trospix&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-trospix&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://wavee.world/invitation/b96d00e6-b802-4a1b-8a66-2e3854a01ffd&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/22633385?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Ikko Eltociear Ashimine&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ikko Eltociear Ashimine&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-eltociear&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/dp221125&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/10572119?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;한석호(MilKyo)&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;한석호(MilKyo)&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-dp221125&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://www.encona.com/&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/891558?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Rahim Nathwani&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Rahim Nathwani&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-rahimnathwani&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://mjspeck.github.io/&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/20689127?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Matt Speck&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Matt Speck&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-mjspeck&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/FinnBorge&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/9272726?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;FinnBorge&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;FinnBorge&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-FinnBorge&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/jklapacz&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/5343758?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Jakub Klapacz&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jakub Klapacz&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-jklapacz&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;!-- markdownlint-restore --&gt;
&lt;!-- prettier-ignore-end --&gt;

&lt;!-- ALL-CONTRIBUTORS-LIST:END --&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[xaoyaoo/PyWxDump]]></title>
            <link>https://github.com/xaoyaoo/PyWxDump</link>
            <guid>https://github.com/xaoyaoo/PyWxDump</guid>
            <pubDate>Fri, 16 May 2025 00:04:18 GMT</pubDate>
            <description><![CDATA[获取微信信息；读取数据库，本地查看聊天记录并导出为csv、html等格式用于AI训练，自动回复等。支持多账户信息获取，支持所有微信版本。]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/xaoyaoo/PyWxDump">xaoyaoo/PyWxDump</a></h1>
            <p>获取微信信息；读取数据库，本地查看聊天记录并导出为csv、html等格式用于AI训练，自动回复等。支持多账户信息获取，支持所有微信版本。</p>
            <p>Language: Python</p>
            <p>Stars: 7,920</p>
            <p>Forks: 1,234</p>
            <p>Stars today: 252 stars today</p>
            <h2>README</h2><pre>[![中文](https://img.shields.io/badge/README-中文-494cad.svg)](https://github.com/xaoyaoo/PyWxDump/tree/master/doc/README_CN.md) [![English](https://img.shields.io/badge/README-English-494cad.svg)](https://github.com/xaoyaoo/PyWxDump/tree/master/doc/README_EN.md)

# &lt;center&gt;PyWxDump&lt;/center&gt;

[![Python](https://img.shields.io/badge/Python-3-blue.svg)](https://www.python.org/)
[![GitHub code size in bytes](https://img.shields.io/github/languages/code-size/xaoyaoo/pywxdump)](https://github.com/xaoyaoo/PyWxDump)
[![GitHub all releases](https://img.shields.io/github/downloads/xaoyaoo/pywxdump/total)](https://github.com/xaoyaoo/PyWxDump)
[![GitHub stars](https://img.shields.io/github/stars/xaoyaoo/PyWxDump.svg)](https://github.com/xaoyaoo/PyWxDump)
[![GitHub forks](https://img.shields.io/github/forks/xaoyaoo/PyWxDump.svg)](https://github.com/xaoyaoo/PyWxDump/fork)
[![GitHub issues](https://img.shields.io/github/issues/xaoyaoo/PyWxDump)](https://github.com/xaoyaoo/PyWxDump/issues)

[![PyPI](https://img.shields.io/pypi/v/pywxdump)](https://pypi.org/project/pywxdump/)
[![Wheel](https://img.shields.io/pypi/wheel/pywxdump)](https://pypi.org/project/pywxdump/)
[![PyPI-Downloads](https://img.shields.io/pypi/dm/pywxdump)](https://pypistats.org/packages/pywxdump)
[![GitHub license](https://img.shields.io/pypi/l/pywxdump)](https://github.com/xaoyaoo/PyWxDump/blob/master/LICENSE)

* Welcome to provide more ideas or code to improve this project together.

### If you are a novice, please pay attention to the Official Accounts: `逍遥之芯` (the QR code is below), and reply: `PyWxDump` to get a picture text tutorial.

### If you have any questions, please check first: [FAQ](https://github.com/xaoyaoo/PyWxDump/tree/master/doc/FAQ.md) Whether there is an answer, or follow the Official Accounts to reply: `FAQ`.

QQ GROUP：[276392799](https://s.xaoyo.top/gOLUDl) or [276392799](https://s.xaoyo.top/bgNcRa)（PASSWORD,please read:[UserGuide.md](https://github.com/xaoyaoo/PyWxDump/tree/master/doc/UserGuide.md)）.

&lt;div&gt;
  &lt;img align=&quot;&quot; width=&quot;200&quot;  src=&quot;https://github.com/xaoyaoo/PyWxDump/blob/master/doc/img/qrcode_gh.jpg&quot; alt=&quot;the Official Accounts&quot; title=&quot;the Official Accounts&quot; height=&quot;200&quot;/&gt;
&lt;/div&gt;

# I. Project Introduction

## 1. Brief Introduction

[PyWxDump](https://github.com/xaoyaoo/PyWxDump) is a tool for obtaining wx account information (nicknames/accounts/phones/emails/database keys), decrypting databases, viewing wx chat, and exporting chat as html backups.

* &lt;strong&gt;&lt;big&gt;Super eager for stars, if you&#039;ve come across this project, please give me a [![Star](https://img.shields.io/github/stars/xaoyaoo/PyWxDump.svg?style=social&amp;label=Star)](https://github.com/xaoyaoo/PyWxDump/)! Thank you so much~ &lt;/big&gt;&lt;/strong&gt;

## 2. Feature

#### 2.1 Core

* (1) Get the **base address offset** of WeChat nickname, WeChat account, WeChat phone number, WeChat email, and WeChat KEY
* (2) Get the WeChat nickname, WeChat account, WeChat phone number, WeChat email, WeChat KEY, WeChat original ID (wxid_******), and WeChat folder path of the currently logged-in WeChat
* (3) Decrypt WeChat database based on key
* (4) Combine multiple types of databases for unified viewing

#### 2.2 Extend Function

* (1) View chat history through the web
* (2) Support exporting chat logs as html, csv, and backing up WeChat chat logs
* (3) Remote viewing of WeChat chat history (must be network accessible, such as a local area network)

#### 2.3 Document Class

* (1) Provide descriptions of some fields in the database
* (2) Provide CE to obtain the base address offset method
* (3) Provide a decryption method for MAC database

#### 2.4 Other functions

* (1) Added a minimalist version of [pywxdumpmini](https://github.com/xaoyaoo/pywxdumpmini), which provides only the ability to obtain database keys and database locations
* (2) Support multiple WeChat opening scenarios, obtain multiple user information, etc.

**Utilize the scene**

1. Network security...
2. Daily backup archiving
3. View chat history remotely (view chat history through the web)
4. Wait...............

## 3. Update plan

* 1.Analyze chat logs of each person and generate word clouds.
* ~~2.Analyze the number of chats per person per day and generate a line chart (day-number of chats)~~
* ~~3.Analyze the monthly and annual chat volume of different people and generate a line chart~~
* ~~4.Generate annual visualization reports~~
* 8.Increase support for enterprise WeChat
* 12.Viewing and backing up of the circle of friends
* ~~13.Clean up WeChat storage space and reduce the space occupied by WeChat (hopefully by selecting a person or group and finding out the media files involved in the chat logs of this group, such as pictures, videos, files, voice recordings, etc., and selectively (such as time periods) or batch-wise clearing them from the computer&#039;s cache by group conversation.)~~
* 14.Automatically send messages to specified people through UI control

## 4. Other

[PyWxDump](https://github.com/xaoyaoo/PyWxDump) is a refactored python language version of [SharpWxDump](https://github.com/AdminTest0/SharpWxDump), with many new features added.

* Project address: https://github.com/xaoyaoo/PyWxDump
* Currently tested only under Windows, there may be issues under mac and Linux.
* If you find any missing or incorrect information, bugs, or suggestions for improvement in the [WX_OFFS.json](https://github.com/xaoyaoo/PyWxDump/tree/master/pywxdump/WX_OFFS.json), please submit an issue on GitHub.
* For common issues, please refer to [FAQ](https://github.com/xaoyaoo/PyWxDump/tree/master/doc/FAQ.md), and for the update log, please refer to [CHANGELOG](https://github.com/xaoyaoo/PyWxDump/tree/master/doc/CHANGELOG.md)
* Web UI repository location [wxdump_web](https://github.com/xaoyaoo/wxdump_web )
* If you are interested in the implementation principle of wxdump, please pay attention to the Official Accounts: `逍遥之芯`, reply: `原理` to get the principle analysis.
* [:sparkling\_heart: Support Me]( https://github.com/xaoyaoo/xaoyaoo/blob/main/donate.md)

## 5. Star History

&lt;details&gt;
&lt;summary&gt;click to expand&lt;/summary&gt;

[![Star History Chart](https://api.star-history.com/svg?repos=xaoyaoo/pywxdump&amp;type=Date)](https://star-history.com/#xaoyaoo/pywxdump&amp;Date)

&lt;/details&gt;

# Ⅱ. Instructions For Use

* Detailed instructions, see: [UserGuide.md](https://github.com/xaoyaoo/PyWxDump/tree/master/doc/UserGuide.md)

* the minimalist version, see: [pywxdumpmini](https://github.com/xaoyaoo/pywxdumpmini)

* If you want to modify the UI, clone the [wx_dump_web](https://github.com/xaoyaoo/wxdump_web) and modify it as needed (the UI is developed using VUE+ElementUI)

【note】:

* For obtaining the base address using cheat engine, refer to [CE obtaining base address.md](https://github.com/xaoyaoo/PyWxDump/tree/master/doc/CE获取基址.md)
  (This method can be replaced by the `wxdump bias` command, and is only used for learning principles.)
* For database parsing, refer to [wx database brief.md](https://github.com/xaoyaoo/PyWxDump/tree/master/doc/wx数据库简述.md)

# Ⅲ. Disclaimer (VERY VERY VERY IMPORTANT ! ! ! ! ! !)

### 1. Purpose of use

* This project is only for learning and communication purposes, **please do not use it for illegal purposes**, **please do not use it for illegal purposes**, **please do not use it for illegal purposes**, otherwise the consequences will be borne by yourself.
* Users understand and agree that any violation of laws and regulations, infringement of the legitimate rights and interests of others, is unrelated to this project and its developers, and the consequences are borne by the user themselves.

### 2. Usage Period

* You should delete the source code and (compiled) program of this project within 24 hours of downloading, saving, compiling, and using it; any use beyond this period is not related to this project or its developer.

### 3. Operation specifications

* This project only allows backup and viewing of the database under authorization. It is strictly prohibited for illegal purposes, otherwise all related responsibilities will be borne by the user. Any legal liability incurred by the user due to violation of this regulation will be borne by the user, and is unrelated to this project and its developer.
* It is strictly prohibited to use it to steal others&#039; privacy. Otherwise, all relevant responsibilities shall be borne by yourself.
* It is strictly prohibited to conduct secondary development, otherwise all related responsibilities shall be borne by yourself.

### 4. Acceptance of Disclaimer

* Downloading, saving, further browsing the source code, or downloading, installing, compiling, and using this program indicates that you agree with this warning and promise to abide by it;

### 5. Forbidden for illegal testing or penetration

* It is prohibited to use the relevant technologies of this project to engage in illegal testing or penetration, and it is prohibited to use the relevant codes or related technologies of this project to engage in any illegal work. Any adverse consequences arising therefrom are not related to this project and its developers.
* Any resulting adverse consequences, including but not limited to data leakage, system failure, and privacy infringement, are not related to this project or its developers and are the responsibility of the user.

### 6. Modification of disclaimer

* This disclaimer may be modified and adjusted based on the project&#039;s operating conditions and changes in laws and regulations. Users should regularly check this page for the latest version of the disclaimer, and should comply with the latest version of the disclaimer when using this project.

### 7. Others

* In addition to the provisions of this disclaimer, users should comply with relevant laws, regulations, and ethical norms during the use of this project. The project and its developers will not be held responsible for any disputes or losses caused by users&#039; violation of relevant regulations.

* Users are requested to carefully read and understand all contents of this disclaimer, and ensure that they strictly comply with relevant regulations when using this project.

# Ⅳ. Acknowledgments

[![PyWxDump CONTRIBUTORS](https://contrib.rocks/image?repo=xaoyaoo/PyWxDump)](https://github.com/xaoyaoo/PyWxDump/graphs/contributors)  

UI CONTRIBUTORS:    

[![UI CONTRIBUTORS](https://contrib.rocks/image?repo=xaoyaoo/wxdump_web)](https://github.com/xaoyaoo/wxdump_web/graphs/contributors)

otherContributors:

[643104191](https://github.com/643104191) (add [ctypes_utils](https://github.com/xaoyaoo/PyWxDump/blob/9e3e4cb5aec2b9b445c8283d61c58863f4129c6e/pywxdump/wx_info/ctypes_utils.py), Accelerated the acquisition of wxinfo; [9e3e4cb](https://github.com/xaoyaoo/PyWxDump/commit/9e3e4cb5aec2b9b445c8283d61c58863f4129c6e))

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Zeyi-Lin/HivisionIDPhotos]]></title>
            <link>https://github.com/Zeyi-Lin/HivisionIDPhotos</link>
            <guid>https://github.com/Zeyi-Lin/HivisionIDPhotos</guid>
            <pubDate>Fri, 16 May 2025 00:04:17 GMT</pubDate>
            <description><![CDATA[⚡️HivisionIDPhotos: a lightweight and efficient AI ID photos tools. 一个轻量级的AI证件照制作算法。]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Zeyi-Lin/HivisionIDPhotos">Zeyi-Lin/HivisionIDPhotos</a></h1>
            <p>⚡️HivisionIDPhotos: a lightweight and efficient AI ID photos tools. 一个轻量级的AI证件照制作算法。</p>
            <p>Language: Python</p>
            <p>Stars: 16,110</p>
            <p>Forks: 1,739</p>
            <p>Stars today: 107 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;img alt=&quot;hivision_logo&quot; src=&quot;assets/hivision_logo.png&quot; width=120 height=120&gt;
&lt;h1&gt;HivisionIDPhoto&lt;/h1&gt;

[English](README_EN.md) / 中文 / [日本語](README_JP.md) / [한국어](README_KO.md)

[![][release-shield]][release-link]
[![][dockerhub-shield]][dockerhub-link]
[![][github-stars-shield]][github-stars-link]
[![][github-issues-shield]][github-issues-link]
[![][github-contributors-shield]][github-contributors-link]
[![][github-forks-shield]][github-forks-link]
[![][license-shield]][license-link]  
[![][wechat-shield]][wechat-link]
[![][spaces-shield]][spaces-link]
[![][swanhub-demo-shield]][swanhub-demo-link]
[![][modelscope-shield]][modelscope-link]
[![][modelers-shield]][modelers-link]
[![][compshare-shield]][compshare-link]

[![][trendshift-shield]][trendshift-link]
[![][hellogithub-shield]][hellogithub-link]

&lt;img src=&quot;assets/demoImage.jpg&quot; width=900&gt;

&lt;/div&gt;

&gt; **相关项目**：
&gt;
&gt; - [SwanLab](https://github.com/SwanHubX/SwanLab)：一个开源、现代化设计的深度学习训练跟踪与可视化工具，同时支持云端/离线使用，国内好用的Wandb平替；适配30+主流框架（PyTorch、HuggingFace Transformers、LLaMA Factory、Lightning等），欢迎使用！


&lt;br&gt;

# 目录

- [最近更新](#-最近更新)
- [项目简介](#-项目简介)
- [社区](#-社区)
- [准备工作](#-准备工作)
- [Demo启动](#-运行-gradio-demo)
- [Python推理](#-python-推理)
- [API服务部署](#️-部署-api-服务)
- [Docker部署](#-docker-部署)
- [联系我们](#-联系我们)
- [FAQ](#faq)
- [感谢支持](#-感谢支持)
- [License](#-lincese)
- [引用](#-引用)

&lt;br&gt;

# 🤩 最近更新

- 在线体验： [![SwanHub Demo](https://img.shields.io/static/v1?label=Demo&amp;message=SwanHub%20Demo&amp;color=blue)](https://swanhub.co/ZeYiLin/HivisionIDPhotos/demo)、[![Spaces](https://img.shields.io/badge/🤗-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/TheEeeeLin/HivisionIDPhotos)、[![][modelscope-shield]][modelscope-link]、[![][compshare-shield]][compshare-link]

- 2024.11.20: Gradio Demo增加**打印排版**选项卡，支持六寸、五寸、A4、3R、4R五种排版尺寸
- 2024.11.16: API接口增加美颜参数
- 2024.09.25: 增加**五寸相纸**和**JPEG下载**选项｜默认照片下载支持300DPI
- 2024.09.24: API接口增加base64图像传入选项 | Gradio Demo增加**排版照裁剪线**功能
- 2024.09.22: Gradio Demo增加**野兽模式**，可设置内存加载策略 | API接口增加**dpi、face_alignment**参数
- 2024.09.18: Gradio Demo增加**分享模版照**功能、增加**美式证件照**背景选项
- 2024.09.17: Gradio Demo增加**自定义底色-HEX输入**功能 | **（社区贡献）C++版本** - [HivisionIDPhotos-cpp](https://github.com/zjkhahah/HivisionIDPhotos-cpp) 贡献 by [zjkhahah](https://github.com/zjkhahah)
- 2024.09.16: Gradio Demo增加**人脸旋转对齐**功能，自定义尺寸输入支持**毫米**单位

&lt;br&gt;

# 项目简介

&gt; 🚀 谢谢你对我们的工作感兴趣。您可能还想查看我们在图像领域的其他成果，欢迎来信:zeyi.lin@swanhub.co.

HivisionIDPhoto 旨在开发一种实用、系统性的证件照智能制作算法。

它利用一套完善的AI模型工作流程，实现对多种用户拍照场景的识别、抠图与证件照生成。

**HivisionIDPhoto 可以做到：**

1. 轻量级抠图（纯离线，仅需 **CPU** 即可快速推理）
2. 根据不同尺寸规格生成不同的标准证件照、六寸排版照
3. 支持 纯离线 或 端云 推理
4. 美颜
5. 智能换正装（waiting）

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;assets/demo.png&quot; width=900&gt;
&lt;/div&gt;

---

如果 HivisionIDPhoto 对你有帮助，请 star 这个 repo 或推荐给你的朋友，解决证件照应急制作问题！

&lt;br&gt;

# 🏠 社区

我们分享了一些由社区构建的HivisionIDPhotos的有趣应用和扩展：

| [HivisionIDPhotos-ComfyUI][community-hivision-comfyui] | [HivisionIDPhotos-wechat-weapp][community-hivision-wechat] |
| :----------------------------------------------------------------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------------------------------------------: |
| &lt;a href=&quot;https://github.com/AIFSH/HivisionIDPhotos-ComfyUI&quot;&gt; &lt;img src=&quot;assets/comfyui.png&quot; width=&quot;900&quot; alt=&quot;ComfyUI workflow&quot;&gt; &lt;/a&gt;  | &lt;a href=&quot;https://github.com/no1xuan/HivisionIDPhotos-wechat-weapp&quot;&gt; &lt;img src=&quot;assets/community-wechat-miniprogram.png&quot; width=&quot;900&quot; alt=&quot;ComfyUI workflow&quot;&gt; &lt;/a&gt;  |
|ComfyUI证件照处理工作流 | 证件照微信小程序（JAVA后端+原生前端） |

| [HivisionIDPhotos-Uniapp][community-hivision-uniapp] | [HivisionIDPhotos-web](https://github.com/jkm199/HivisionIDPhotos-web)|
| :------------------------------------------------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------------------------------------------: |
| &lt;a href=&quot;https://github.com/soulerror/HivisionIDPhotos-Uniapp&quot;&gt; &lt;img src=&quot;assets/community-uniapp-wechat-miniprogram.png&quot; width=&quot;900&quot; alt=&quot;HivisionIDPhotos-uniapp&quot;&gt; &lt;/a&gt;  | &lt;a href=&quot;https://github.com/jkm199/HivisionIDPhotos-web&quot;&gt; &lt;img src=&quot;assets/community-web.png&quot; width=&quot;900&quot; alt=&quot;HivisionIDPhotos-uniapp&quot;&gt; &lt;/a&gt;  |
| 证件照微信小程序（uniapp）| 证件照应用网页版 |


- [HivisionIDPhotos-cpp](https://github.com/zjkhahah/HivisionIDPhotos-cpp): HivisionIDphotos C++版本，由 [zjkhahah](https://github.com/zjkhahah) 构建
- [ai-idphoto](https://github.com/wmlcjj/ai-idphoto): [HivisionIDPhotos-wechat-weapp](https://github.com/no1xuan/HivisionIDPhotos-wechat-weapp) 的uniapp多端兼容版，由 [wmlcjj](https://github.com/wmlcjj) 贡献
- [HivisionIDPhotos-uniapp-WeChat-gpto1](https://github.com/jkm199/HivisionIDPhotos-uniapp-WeChat-gpto1/): 由gpt-o1辅助完成开发的证件照微信小程序，由 [jkm199](https://github.com/jkm199) 贡献
- [HivisionIDPhotos-windows-GUI](https://github.com/zhaoyun0071/HivisionIDPhotos-windows-GUI)：Windows客户端应用，由 [zhaoyun0071](https://github.com/zhaoyun0071) 构建
- [HivisionIDPhotos-NAS](https://github.com/ONG-Leo/HivisionIDPhotos-NAS): 群晖NAS部署中文教程，由 [ONG-Leo](https://github.com/ONG-Leo) 贡献


&lt;br&gt;

# 🔧 准备工作

环境安装与依赖：
- Python &gt;= 3.7（项目主要测试在 python 3.10）
- OS: Linux, Windows, MacOS

## 1. 克隆项目

```bash
git clone https://github.com/Zeyi-Lin/HivisionIDPhotos.git
cd  HivisionIDPhotos
```

## 2. 安装依赖环境

&gt; 建议 conda 创建一个 python3.10 虚拟环境后，执行以下命令

```bash
pip install -r requirements.txt
pip install -r requirements-app.txt
```

## 3. 下载人像抠图模型权重文件

**方式一：脚本下载**

```bash
python scripts/download_model.py --models all
# 如需指定下载某个模型
# python scripts/download_model.py --models modnet_photographic_portrait_matting
```

**方式二：直接下载**

模型均存到项目的`hivision/creator/weights`目录下：

| 人像抠图模型 | 介绍 | 下载 |
| -- | -- | -- |
| MODNet | [MODNet](https://github.com/ZHKKKe/MODNet)官方权重 | [下载](https://github.com/Zeyi-Lin/HivisionIDPhotos/releases/download/pretrained-model/modnet_photographic_portrait_matting.onnx)(24.7MB)|
| hivision_modnet | 对纯色换底适配性更好的抠图模型 | [下载](https://github.com/Zeyi-Lin/HivisionIDPhotos/releases/download/pretrained-model/hivision_modnet.onnx)(24.7MB) |
| rmbg-1.4 | [BRIA AI](https://huggingface.co/briaai/RMBG-1.4) 开源的抠图模型 | [下载](https://huggingface.co/briaai/RMBG-1.4/resolve/main/onnx/model.onnx?download=true)(176.2MB)后重命名为`rmbg-1.4.onnx` |
| birefnet-v1-lite | [ZhengPeng7](https://github.com/ZhengPeng7/BiRefNet) 开源的抠图模型，拥有最好的分割精度 | [下载](https://github.com/ZhengPeng7/BiRefNet/releases/download/v1/BiRefNet-general-bb_swin_v1_tiny-epoch_232.onnx)(224MB)后重命名为`birefnet-v1-lite.onnx` |

&gt; 如果下载网速不顺利：前往[SwanHub](https://swanhub.co/ZeYiLin/HivisionIDPhotos_models/tree/main)下载。


## 4. 人脸检测模型配置（可选）

| 拓展人脸检测模型 | 介绍 | 使用文档 |
| -- | -- | -- |
| MTCNN | **离线**人脸检测模型，高性能CPU推理（毫秒级），为默认模型，检测精度较低 | Clone此项目后直接使用 |
| RetinaFace | **离线**人脸检测模型，CPU推理速度中等（秒级），精度较高| [下载](https://github.com/Zeyi-Lin/HivisionIDPhotos/releases/download/pretrained-model/retinaface-resnet50.onnx)后放到`hivision/creator/retinaface/weights`目录下 |
| Face++ | 旷视推出的在线人脸检测API，检测精度较高，[官方文档](https://console.faceplusplus.com.cn/documents/4888373) | [使用文档](docs/face++_CN.md)|

## 5. 性能参考

&gt; 测试环境为Mac M1 Max 64GB，非GPU加速，测试图片分辨率为 512x715(1) 与 764×1146(2)。

| 模型组合 | 内存占用 | 推理时长(1) | 推理时长(2) |
| -- | -- | -- | -- |
| MODNet + mtcnn | 410MB | 0.207s | 0.246s |
| MODNet + retinaface | 405MB | 0.571s | 0.971s |
| birefnet-v1-lite + retinaface | 6.20GB | 7.063s | 7.128s |

## 6. GPU推理加速（可选）

在当前版本，可被英伟达GPU加速的模型为`birefnet-v1-lite`，并请确保你有16GB左右的显存。

如需使用英伟达GPU加速推理，在确保你已经安装[CUDA](https://developer.nvidia.com/cuda-downloads)与[cuDNN](https://developer.nvidia.com/cudnn)后，根据[onnxruntime-gpu文档](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#cuda-12x)找到对应的`onnxruntime-gpu`版本安装，以及根据[pytorch官网](https://pytorch.org/get-started/locally/)找到对应的`torch`版本安装。

```bash
# 假如你的电脑安装的是CUDA 12.x, cuDNN 8
# 安装torch是可选的，如果你始终配置不好cuDNN，那么试试安装torch
pip install onnxruntime-gpu==1.18.0
pip install torch --index-url https://download.pytorch.org/whl/cu121
```

完成安装后，调用`birefnet-v1-lite`模型即可利用GPU加速推理。

&gt; TIPS: CUDA 支持向下兼容。比如你的 CUDA 版本为 12.6，`torch` 官方目前支持的最高版本为 12.4（&lt;12.6），`torch`仍可以正常使用CUDA。

&lt;br&gt;

# ⚡️ 运行 Gradio Demo

```bash
python app.py
```

运行程序将生成一个本地 Web 页面，在页面中可完成证件照的操作与交互。

&lt;img src=&quot;assets/harry.png&quot; width=900&gt;

&lt;br&gt;

# 🚀 Python 推理

核心参数：

- `-i`: 输入图像路径
- `-o`: 保存图像路径
- `-t`: 推理类型，有idphoto、human_matting、add_background、generate_layout_photos可选
- `--matting_model`: 人像抠图模型权重选择
- `--face_detect_model`: 人脸检测模型选择

更多参数可通过`python inference.py --help`查看

## 1. 证件照制作

输入 1 张照片，获得 1 张标准证件照和 1 张高清证件照的 4 通道透明 png

```python
python inference.py -i demo/images/test0.jpg -o ./idphoto.png --height 413 --width 295
```

## 2. 人像抠图

输入 1 张照片，获得 1张 4 通道透明 png

```python
python inference.py -t human_matting -i demo/images/test0.jpg -o ./idphoto_matting.png --matting_model hivision_modnet
```

## 3. 透明图增加底色

输入 1 张 4 通道透明 png，获得 1 张增加了底色的 3通道图像

```python
python inference.py -t add_background -i ./idphoto.png -o ./idphoto_ab.jpg  -c 4f83ce -k 30 -r 1
```

## 4. 得到六寸排版照

输入 1 张 3 通道照片，获得 1 张六寸排版照

```python
python inference.py -t generate_layout_photos -i ./idphoto_ab.jpg -o ./idphoto_layout.jpg  --height 413 --width 295 -k 200
```

## 5. 证件照裁剪

输入 1 张 4 通道照片（抠图好的图像），获得 1 张标准证件照和 1 张高清证件照的 4 通道透明 png

```python
python inference.py -t idphoto_crop -i ./idphoto_matting.png -o ./idphoto_crop.png --height 413 --width 295
```


&lt;br&gt;

# ⚡️ 部署 API 服务

## 启动后端

```
python deploy_api.py
```

## 请求 API 服务

详细请求方式请参考 [API 文档](docs/api_CN.md)，包含以下请求示例：
- [cURL](docs/api_CN.md#curl-请求示例)
- [Python](docs/api_CN.md#python-请求示例)

&lt;br&gt;

# 🐳 Docker 部署

## 1. 拉取或构建镜像

&gt; 以下方式三选一

**方式一：拉取最新镜像：**

```bash
docker pull linzeyi/hivision_idphotos
```

**方式二：Dockrfile 直接构建镜像：**

在确保将至少一个[抠图模型权重文件](#3-下载权重文件)放到`hivision/creator/weights`下后，在项目根目录执行：

```bash
docker build -t linzeyi/hivision_idphotos .
```

**方式三：Docker compose 构建：**

在确保将至少一个[抠图模型权重文件](#3-下载权重文件)放到`hivision/creator/weights`下后，在项目根目录下执行：

```bash
docker compose build
```

## 2. 运行服务

**启动 Gradio Demo 服务**

运行下面的命令，在你的本地访问 [http://127.0.0.1:7860](http://127.0.0.1:7860/) 即可使用。

```bash
docker run -d -p 7860:7860 linzeyi/hivision_idphotos
```

**启动 API 后端服务**

```bash
docker run -d -p 8080:8080 linzeyi/hivision_idphotos python3 deploy_api.py
```

**两个服务同时启动**

```bash
docker compose up -d
```

## 环境变量

本项目提供了一些额外的配置项，使用环境变量进行设置：

| 环境变量 | 类型	| 描述 | 示例 |
|--|--|--|--|
| FACE_PLUS_API_KEY	 | 可选	| 这是你在 Face++ 控制台申请的 API 密钥	 | `7-fZStDJ····` |
| FACE_PLUS_API_SECRET	 | 可选	| Face++ API密钥对应的Secret | `VTee824E····` |
| RUN_MODE | 可选 | 运行模式，可选值为`beast`(野兽模式)。野兽模式下人脸检测和抠图模型将不释放内存，从而获得更快的二次推理速度。建议内存16GB以上尝试。 | `beast` |
| DEFAULT_LANG | 可选 | Gradio Demo启动时的默认语言| `en` |

docker使用环境变量示例：
```bash
docker run  -d -p 7860:7860 \
    -e FACE_PLUS_API_KEY=7-fZStDJ···· \
    -e FACE_PLUS_API_SECRET=VTee824E···· \
    -e RUN_MODE=beast \
    -e DEFAULT_LANG=en \
    linzeyi/hivision_idphotos  
```

&lt;br&gt;

# FAQ

## 1. 如何修改预设尺寸和颜色？

- 尺寸：修改[size_list_CN.csv](demo/assets/size_list_CN.csv)后再次运行 `app.py` 即可，其中第一列为尺寸名，第二列为高度，第三列为宽度。
- 颜色：修改[color_list_CN.csv](demo/assets/color_list_CN.csv)后再次运行 `app.py` 即可，其中第一列为颜色名，第二列为Hex值。

## 2. 如何修改水印字体？

1. 将字体文件放到`hivision/plugin/font`文件夹下
2. 修改`hivision/plugin/watermark.py`的`font_file`参数值为字体文件名

## 3. 如何添加社交媒体模板照？

1. 将模板图片放到`hivision/plugin/template/assets`文件夹下。模板图片是一个4通道的透明png。
2. 在`hivision/plugin/template/assets/template_config.json`文件中添加最新的模板信息，其中`width`为模板图宽度(px)，`height`为模板图高度(px)，`anchor_points`为模板中透明区域的四个角的坐标(px)；`rotation`为透明区域相对于垂直方向的旋转角度，&gt;0为逆时针，&lt;0为顺时针。
3. 在`demo/processor.py`的`_generate_image_template`函数中的`TEMPLATE_NAME_LIST`变量添加最新的模板名

&lt;img src=&quot;assets/social_template.png&quot; width=&quot;500&quot;&gt;

## 4. 如何修改Gradio Demo的顶部导航栏？

- 修改`demo/assets/title.md`

## 5. 如何添加/修改「打印排版」中的尺寸？

- 修改`demo/locales.py`中的`print_switch`字典，添加/修改新的尺寸名称和尺寸参数，然后重新运行`python app.py`

&lt;br&gt;

# 📧 联系我们

如果您有任何问题，请发邮件至 zeyi.lin@swanhub.co

&lt;br&gt;

# 🙏 感谢支持

[![Stargazers repo roster for @Zeyi-Lin/HivisionIDPhotos](https://reporoster.com/stars/Zeyi-Lin/HivisionIDPhotos)](https://github.com/Zeyi-Lin/HivisionIDPhotos/stargazers)

[![Forkers repo roster for @Zeyi-Lin/HivisionIDPhotos](https://reporoster.com/forks/Zeyi-Lin/HivisionIDPhotos)](https://github.com/Zeyi-Lin/HivisionIDPhotos/network/members)

[![Star History Chart](https://api.star-history.com/svg?repos=Zeyi-Lin/HivisionIDPhotos&amp;type=Date)](https://star-history.com/#Zeyi-Lin/HivisionIDPhotos&amp;Date)

贡献者们：

&lt;a href=&quot;https://github.com/Zeyi-Lin/HivisionIDPhotos/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=Zeyi-Lin/HivisionIDPhotos&quot; /&gt;
&lt;/a&gt;

[Zeyi-Lin](https://github.com/Zeyi-Lin)、[SAKURA-CAT](https://github.com/SAKURA-CAT)、[Feudalman](https://github.com/Feudalman)、[swpfY](https://github.com/swpfY)、[Kaikaikaifang](https://github.com/Kaikaikaifang)、[ShaohonChen](https://github.com/ShaohonChen)、[KashiwaByte](https://github.com/KashiwaByte)

&lt;br&gt;

# 📜 Lincese

This repository is licensed under the [Apache-2.0 License](LICENSE).

&lt;br&gt;

# 📚 引用

如果您在研究或项目中使用了HivisionIDPhotos，请考虑引用我们的工作。您可以使用以下BibTeX条目：

```bibtex
@misc{hivisionidphotos,
      title={{HivisionIDPhotos: A Lightweight and Efficient AI ID Photos Tool}},
      author={Zeyi Lin and SwanLab Team},
      year={2024},
      publisher={GitHub},
      url = {\url{https://github.com/Zeyi-Lin/HivisionIDPhotos}},
}
```




[github-stars-shield]: https://img.shields.io/github/stars/zeyi-lin/hivisionidphotos?color=ffcb47&amp;labelColor=black&amp;style=flat-square
[github-stars-link]: https://github.com/zeyi-lin/hivisionidphotos/stargazers

[swanhub-demo-shield]: https://swanhub.co/git/repo/SwanHub%2FAuto-README/file/preview?ref=main&amp;path=swanhub.svg
[swanhub-demo-link]: https://swanhub.co/ZeYiLin/HivisionIDPhotos/demo

[spaces-shield]: https://img.shields.io/badge/🤗-Open%20in%20Spaces-blue
[spaces-link]: https://huggingface.co/spaces/TheEeeeLin/HivisionIDPhotos

&lt;!-- 微信群链接 --&gt;
[wechat-shield]: https://img.shields.io/badge/WeChat-微信-4cb55e
[wechat-link]: https://docs.qq.com/doc/DUkpBdk90eWZFS2JW

&lt;!-- Github Release --&gt;
[release-shield]: https://img.shields.io/github/v/release/zeyi-lin/hivisionidphotos?color=369eff&amp;labelColor=black&amp;logo=github&amp;style=flat-square
[release-link]: https://github.com/zeyi-lin/hivisionidphotos/releases

[license-shield]: https://img.shields.io/badge/license-apache%202.0-white?labelColor=black&amp;style=flat-square
[license-link]: https://github.com/Zeyi-Lin/HivisionIDPhotos/blob/master/LICENSE

[github-issues-shield]: https://img.shields.io/github/issues/zeyi-lin/hivisionidphotos?color=ff80eb&amp;labelColor=black&amp;style=flat-square
[github-issues-link]: https://github.com/zeyi-lin/hivisionidphotos/issues

[dockerhub-shield]: https://img.shields.io/docker/v/linzeyi/hivision_idphotos?color=369eff&amp;label=docker&amp;labelColor=black&amp;logoColor=white&amp;style=flat-square
[dockerhub-link]: https://hub.docker.com/r/linzeyi/hivision_idphotos/tags

[trendshift-shield]: https://trendshift.io/api/badge/repositories/11622
[trendshift-link]: https://trendshift.io/repositories/11622

[hellogithub-shield]: https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=8ea1457289fb4062ba661e5299e733d6&amp;claim_uid=Oh5UaGjfrblg0yZ
[hellogithub-link]: https://hellogithub.com/repository/8ea1457289fb4062ba661e5299e733d6

[github-contributors-shield]: https://img.shields.io/github/contributors/zeyi-lin/hivisionidphotos?color=c4f042&amp;labelColor=black&amp;style=flat-square
[github-contributors-link]: https://github.com/zeyi-lin/hivisionidphotos/graphs/contributors

[github-forks-shield]: https://img.shields.io/github/forks/zeyi-lin/hivisionidphotos?color=8ae8ff&amp;labelColor=black&amp;style=flat-square
[github-forks-link]: https://github.com/zeyi-lin/hivisionidphotos/network/members

[modelscope-shield]: https://img.shields.io/badge/Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&amp;labelColor=white
[modelscope-link]: https://modelscope.cn/studios/SwanLab/HivisionIDPhotos

[modelers-shield]: https://img.shields.io/badge/Demo_on_Modelers-c42a2a?logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxMjQiIGhlaWdodD0iNjQiIHZpZXdCb3g9IjAgMCAxMjQgNjQiIGZpbGw9Im5vbmUiPgo8cGF0aCBkPSJNNDIuNzc4MyAwSDI2LjU5NzdWMTUuNzc4N0g0Mi43NzgzVjBaIiBmaWxsPSIjREUwNDI5Ii8+CjxwYXRoIGQ9Ik0xNi41MDg4IDQuMTc5MkgwLjMyODEyNVYxOS45NTc5SDE2LjUwODhWNC4xNzkyWiIgZmlsbD0iIzI0NDk5QyIvPgo8cGF0aCBkPSJNMTIzLjk1MiA0LjE3OTJIMTA3Ljc3MVYxOS45NTc5SDEyMy45NTJWNC4xNzkyWiIgZmlsbD0iIzI0NDk5QyIvPgo8cGF0aCBkPSJNMTYuNTA4OCA0NS40NjE5SDAuMzI4MTI1VjYxLjI0MDZIMTYuNTA4OFY0NS40NjE5WiIgZmlsbD0iIzI0NDk5QyIvPgo8cGF0aCBkPSJNMTIzLjk1MiA0NS40NjE5SDEwNy43NzFWNjEuMjQwNkgxMjMuOTUyVjQ1LjQ2MTlaIiBmaWxsPSIjMjQ0OTlDIi8+CjxwYXRoIGQ9Ik0zMi43MDggMTUuNzc4OEgxNi41MjczVjMxLjU1NzVIMzIuNzA4VjE1Ljc3ODhaIiBmaWxsPSIjREUwNDI5Ii8+CjxwYXRoIGQ9Ik01Mi44NDg2IDE1Ljc3ODhIMzYuNjY4VjMxLjU1NzVINTIuODQ4NlYxNS43Nzg4WiIgZmlsbD0iI0RFMDQyOSIvPgo8cGF0aCBkPSJNOTcuNzIzNyAwSDgxLjU0M1YxNS43Nzg3SDk3LjcyMzdWMFoiIGZpbGw9IiNERTA0MjkiLz4KPHBhdGggZD0iTTg3LjY1MzQgMTUuNzc4OEg3MS40NzI3VjMxLjU1NzVIODcuNjUzNFYxNS43Nzg4WiIgZmlsbD0iI0RFMDQyOSIvPgo8cGF0aCBkPSJNMTA3Ljc5NCAxNS43Nzg4SDkxLjYxMzNWMzEuNTU3NUgxMDcuNzk0VjE1Ljc3ODhaIiBmaWxsPSIjREUwNDI5Ii8+CjxwYXRoIGQ9Ik0yNC42NzQ4IDMxLjU1NzZIOC40OTQxNFY0Ny4zMzYzSDI0LjY3NDhWMzEuNTU3NloiIGZpbGw9IiNERTA0MjkiLz4KPHBhdGggZD0iTTYwLjg3OTkgMzEuNTU3Nkg0NC42OTkyVjQ3LjMzNjNINjAuODc5OVYzMS41NTc2WiIgZmlsbD0iI0RFMDQyOSIvPgo8cGF0aCBkPSJNNzkuNjIwMSAzMS41NTc2SDYzLjQzOTVWNDcuMzM2M0g3OS42MjAxVjMxLjU1NzZaIiBmaWxsPSIjREUwNDI5Ii8+CjxwYXRoIGQ9Ik0xMTUuODI1IDMxLjU1NzZIOTkuNjQ0NVY0Ny4zMzYzSDExNS44MjVWMzEuNTU3NloiIGZpbGw9IiNERTA0MjkiLz4KPHBhdGggZD0iTTcwLjI1NDkgNDcuMzM1OUg1NC4wNzQyVjYzLjExNDdINzAuMjU0OVY0Ny4zMzU5WiIgZmlsbD0iI0RFMDQyOSIvPgo8L3N2Zz4=&amp;labelColor=white
[modelers-link]: https://modelers.cn/spaces/SwanLab/HivisionIDPhotos

[compshare-shield]: https://www-s.ucloud.cn/2025/02/dbef8b07ea3d316006d9c22765c3cd53_1740104342584.svg
[compshare-link]: https://www.compshare.cn/images-detail?ImageID=compshareImage-17jacgm4ju16&amp;ytag=HG_GPU_HivisionIDPhotos

&lt;!-- 社区项目链接 --&gt;
[community-hivision-comfyui]: https://github.com/AIFSH/HivisionIDPhotos-ComfyUI
[community-hivision-wechat]: https://github.com/no1xuan/HivisionIDPhotos-wechat-weapp
[community-hivision-uniapp]: https://github.com/soulerror/HivisionIDPhotos-Uniapp
[community-hivision-cpp]: https://github.com/zjkhahah/HivisionIDPhotos-cpp
[community-hivision-windows-gui]: https://github.com/zhaoyun0071/HivisionIDPhotos-windows-GUI
[community-hivision-nas]: https://github.com/ONG-Leo/HivisionIDPhotos-NAS</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Stability-AI/stable-audio-tools]]></title>
            <link>https://github.com/Stability-AI/stable-audio-tools</link>
            <guid>https://github.com/Stability-AI/stable-audio-tools</guid>
            <pubDate>Fri, 16 May 2025 00:04:16 GMT</pubDate>
            <description><![CDATA[Generative models for conditional audio generation]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Stability-AI/stable-audio-tools">Stability-AI/stable-audio-tools</a></h1>
            <p>Generative models for conditional audio generation</p>
            <p>Language: Python</p>
            <p>Stars: 3,123</p>
            <p>Forks: 317</p>
            <p>Stars today: 45 stars today</p>
            <h2>README</h2><pre># stable-audio-tools
Training and inference code for audio generation models

# Install

The library can be installed from PyPI with:
```bash
$ pip install stable-audio-tools
```

To run the training scripts or inference code, you&#039;ll want to clone this repository, navigate to the root, and run:
```bash
$ pip install .
```

# Requirements
Requires PyTorch 2.5 or later for Flash Attention and Flex Attention support

Development for the repo is done in Python 3.10

# Interface

A basic Gradio interface is provided to test out trained models. 

For example, to create an interface for the [`stable-audio-open-1.0`](https://huggingface.co/stabilityai/stable-audio-open-1.0) model, once you&#039;ve accepted the terms for the model on Hugging Face, you can run:
```bash
$ python3 ./run_gradio.py --pretrained-name stabilityai/stable-audio-open-1.0
```

The `run_gradio.py` script accepts the following command line arguments:

- `--pretrained-name`
  - Hugging Face repository name for a Stable Audio Tools model
  - Will prioritize `model.safetensors` over `model.ckpt` in the repo
  - Optional, used in place of `model-config` and `ckpt-path` when using pre-trained model checkpoints on Hugging Face
- `--model-config`
  - Path to the model config file for a local model
- `--ckpt-path`
  - Path to unwrapped model checkpoint file for a local model
- `--pretransform-ckpt-path` 
  - Path to an unwrapped pretransform checkpoint, replaces the pretransform in the model, useful for testing out fine-tuned decoders
  - Optional
- `--share`
  - If true, a publicly shareable link will be created for the Gradio demo
  - Optional
- `--username` and `--password`
  - Used together to set a login for the Gradio demo
  - Optional
- `--model-half`
  - If true, the model weights to half-precision
  - Optional

# Training

## Prerequisites
Before starting your training run, you&#039;ll need a model config file, as well as a dataset config file. For more information about those, refer to the Configurations section below

The training code also requires a Weights &amp; Biases account to log the training outputs and demos. Create an account and log in with:
```bash
$ wandb login
```

## Start training
To start a training run, run the `train.py` script in the repo root with:
```bash
$ python3 ./train.py --dataset-config /path/to/dataset/config --model-config /path/to/model/config --name harmonai_train
```

The `--name` parameter will set the project name for your Weights and Biases run.

## Training wrappers and model unwrapping
`stable-audio-tools` uses PyTorch Lightning to facilitate multi-GPU and multi-node training. 

When a model is being trained, it is wrapped in a &quot;training wrapper&quot;, which is a `pl.LightningModule` that contains all of the relevant objects needed only for training. That includes things like discriminators for autoencoders, EMA copies of models, and all of the optimizer states.

The checkpoint files created during training include this training wrapper, which greatly increases the size of the checkpoint file.

`unwrap_model.py` in the repo root will take in a wrapped model checkpoint and save a new checkpoint file including only the model itself.

That can be run with from the repo root with:
```bash
$ python3 ./unwrap_model.py --model-config /path/to/model/config --ckpt-path /path/to/wrapped/ckpt --name model_unwrap
```

Unwrapped model checkpoints are required for:
  - Inference scripts
  - Using a model as a pretransform for another model (e.g. using an autoencoder model for latent diffusion)
  - Fine-tuning a pre-trained model with a modified configuration (i.e. partial initialization)

## Fine-tuning
Fine-tuning a model involves continuning a training run from a pre-trained checkpoint. 

To continue a training run from a wrapped model checkpoint, you can pass in the checkpoint path to `train.py` with the `--ckpt-path` flag.

To start a fresh training run using a pre-trained unwrapped model, you can pass in the unwrapped checkpoint to `train.py` with the `--pretrained-ckpt-path` flag.

## Additional training flags

Additional optional flags for `train.py` include:
- `--config-file`
  - The path to the defaults.ini file in the repo root, required if running `train.py` from a directory other than the repo root
- `--pretransform-ckpt-path`
  - Used in various model types such as latent diffusion models to load a pre-trained autoencoder. Requires an unwrapped model checkpoint.
- `--save-dir`
  - The directory in which to save the model checkpoints
- `--checkpoint-every`
  - The number of steps between saved checkpoints.
  - *Default*: 10000
- `--batch-size`
  - Number of samples per-GPU during training. Should be set as large as your GPU VRAM will allow.
  - *Default*: 8
- `--num-gpus`
  - Number of GPUs per-node to use for training
  - *Default*: 1
- `--num-nodes`
  - Number of GPU nodes being used for training
  - *Default*: 1
- `--accum-batches`
  - Enables and sets the number of batches for gradient batch accumulation. Useful for increasing effective batch size when training on smaller GPUs.
- `--strategy`
  - Multi-GPU strategy for distributed training. Setting to `deepspeed` will enable DeepSpeed ZeRO Stage 2.
  - *Default*: `ddp` if `--num_gpus` &gt; 1, else None
- `--precision`
  - floating-point precision to use during training
  - *Default*: 16
- `--num-workers`
  - Number of CPU workers used by the data loader
- `--seed`
  - RNG seed for PyTorch, helps with deterministic training

# Configurations
Training and inference code for `stable-audio-tools` is based around JSON configuration files that define model hyperparameters, training settings, and information about your training dataset.

## Model config
The model config file defines all of the information needed to load a model for training or inference. It also contains the training configuration needed to fine-tune a model or train from scratch.

The following properties are defined in the top level of the model configuration:

- `model_type`
  - The type of model being defined, currently limited to one of `&quot;autoencoder&quot;, &quot;diffusion_uncond&quot;, &quot;diffusion_cond&quot;, &quot;diffusion_cond_inpaint&quot;, &quot;diffusion_autoencoder&quot;, &quot;lm&quot;`.
- `sample_size`
  - The length of the audio provided to the model during training, in samples. For diffusion models, this is also the raw audio sample length used for inference.
- `sample_rate`
  - The sample rate of the audio provided to the model during training, and generated during inference, in Hz.
- `audio_channels`
  - The number of channels of audio provided to the model during training, and generated during inference. Defaults to 2. Set to 1 for mono.
- `model`
  - The specific configuration for the model being defined, varies based on `model_type`
- `training`
  - The training configuration for the model, varies based on `model_type`. Provides parameters for training as well as demos.

## Dataset config
`stable-audio-tools` currently supports two kinds of data sources: local directories of audio files, and WebDataset datasets stored in Amazon S3. More information can be found in [the dataset config documentation](docs/datasets.md)

# Todo
- [ ] Add troubleshooting section
- [ ] Add contribution guidelines 
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sinaptik-ai/pandas-ai]]></title>
            <link>https://github.com/sinaptik-ai/pandas-ai</link>
            <guid>https://github.com/sinaptik-ai/pandas-ai</guid>
            <pubDate>Fri, 16 May 2025 00:04:15 GMT</pubDate>
            <description><![CDATA[Chat with your database or your datalake (SQL, CSV, parquet). PandasAI makes data analysis conversational using LLMs and RAG.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sinaptik-ai/pandas-ai">sinaptik-ai/pandas-ai</a></h1>
            <p>Chat with your database or your datalake (SQL, CSV, parquet). PandasAI makes data analysis conversational using LLMs and RAG.</p>
            <p>Language: Python</p>
            <p>Stars: 20,135</p>
            <p>Forks: 1,910</p>
            <p>Stars today: 35 stars today</p>
            <h2>README</h2><pre># ![PandaAI](assets/logo.png)

[![Release](https://img.shields.io/pypi/v/pandasai?label=Release&amp;style=flat-square)](https://pypi.org/project/pandasai/)
[![CI](https://github.com/sinaptik-ai/pandas-ai/actions/workflows/ci-core.yml/badge.svg)](https://github.com/sinaptik-ai/pandas-ai/actions/workflows/ci-core.yml/badge.svg)
[![CD](https://github.com/sinaptik-ai/pandas-ai/actions/workflows/cd.yml/badge.svg)](https://github.com/sinaptik-ai/pandas-ai/actions/workflows/cd.yml/badge.svg)
[![Coverage](https://codecov.io/gh/sinaptik-ai/pandas-ai/branch/main/graph/badge.svg)](https://codecov.io/gh/sinaptik-ai/pandas-ai)
[![Discord](https://dcbadge.vercel.app/api/server/kF7FqH2FwS?style=flat&amp;compact=true)](https://discord.gg/KYKj9F2FRH)
[![Downloads](https://static.pepy.tech/badge/pandasai)](https://pepy.tech/project/pandasai) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1ZnO-njhL7TBOYPZaqvMvGtsjckZKrv2E?usp=sharing)

PandaAI is a Python platform that makes it easy to ask questions to your data in natural language. It helps non-technical users to interact with their data in a more natural way, and it helps technical users to save time, and effort when working with data.

# 🔧 Getting started

You can find the full documentation for PandaAI [here](https://pandas-ai.readthedocs.io/en/latest/).

You can either decide to use PandaAI in your Jupyter notebooks, Streamlit apps, or use the client and server architecture from the repo.

## ☁️ Using the platform

The library can be used alongside our powerful data platform, making end-to-end conversational data analytics possible with as little as a few lines of code.

Load your data, save them as a dataframe, and push them to the platform

```python
import pandasai as pai

pai.api_key.set(&quot;your-pai-api-key&quot;)

file = pai.read_csv(&quot;./filepath.csv&quot;)

dataset = pai.create(path=&quot;your-organization/dataset-name&quot;,
    df=file,
    name=&quot;dataset-name&quot;,
    description=&quot;dataset-description&quot;)

dataset.push()
```

Your team can now access and query this data using natural language through the platform.

![PandaAI](assets/demo.gif)

## 📚 Using the library

### Python Requirements

Python version `3.8+ &lt;3.12`

### 📦 Installation

You can install the PandaAI library using pip or poetry.

With pip:

```bash
pip install &quot;pandasai&gt;=3.0.0b2&quot;
```

With poetry:

```bash
poetry add &quot;pandasai&gt;=3.0.0b2&quot;
```

### 💻 Usage

#### Ask questions

```python
import pandasai as pai

# Sample DataFrame
df = pai.DataFrame({
    &quot;country&quot;: [&quot;United States&quot;, &quot;United Kingdom&quot;, &quot;France&quot;, &quot;Germany&quot;, &quot;Italy&quot;, &quot;Spain&quot;, &quot;Canada&quot;, &quot;Australia&quot;, &quot;Japan&quot;, &quot;China&quot;],
    &quot;revenue&quot;: [5000, 3200, 2900, 4100, 2300, 2100, 2500, 2600, 4500, 7000]
})

# By default, unless you choose a different LLM, it will use BambooLLM.
# You can get your free API key signing up at https://app.pandabi.ai (you can also configure it in your .env file)
pai.api_key.set(&quot;your-pai-api-key&quot;)

df.chat(&#039;Which are the top 5 countries by sales?&#039;)
```

```
China, United States, Japan, Germany, Australia
```

---

Or you can ask more complex questions:

```python
df.chat(
    &quot;What is the total sales for the top 3 countries by sales?&quot;
)
```

```
The total sales for the top 3 countries by sales is 16500.
```

#### Visualize charts

You can also ask PandaAI to generate charts for you:

```python
df.chat(
    &quot;Plot the histogram of countries showing for each one the gd. Use different colors for each bar&quot;,
)
```

![Chart](assets/histogram-chart.png?raw=true)

#### Multiple DataFrames

You can also pass in multiple dataframes to PandaAI and ask questions relating them.

```python
import pandasai as pai

employees_data = {
    &#039;EmployeeID&#039;: [1, 2, 3, 4, 5],
    &#039;Name&#039;: [&#039;John&#039;, &#039;Emma&#039;, &#039;Liam&#039;, &#039;Olivia&#039;, &#039;William&#039;],
    &#039;Department&#039;: [&#039;HR&#039;, &#039;Sales&#039;, &#039;IT&#039;, &#039;Marketing&#039;, &#039;Finance&#039;]
}

salaries_data = {
    &#039;EmployeeID&#039;: [1, 2, 3, 4, 5],
    &#039;Salary&#039;: [5000, 6000, 4500, 7000, 5500]
}

employees_df = pai.DataFrame(employees_data)
salaries_df = pai.DataFrame(salaries_data)

# By default, unless you choose a different LLM, it will use BambooLLM.
# You can get your free API key signing up at https://app.pandabi.ai (you can also configure it in your .env file)
pai.api_key.set(&quot;your-pai-api-key&quot;)

pai.chat(&quot;Who gets paid the most?&quot;, employees_df, salaries_df)
```

```
Olivia gets paid the most.
```

#### Docker Sandbox

You can run PandaAI in a Docker sandbox, providing a secure, isolated environment to execute code safely and mitigate the risk of malicious attacks.

##### Python Requirements

```bash
pip install &quot;pandasai-docker&quot;
```

##### Usage

```python
import pandasai as pai
from pandasai_docker import DockerSandbox

# Initialize the sandbox
sandbox = DockerSandbox()
sandbox.start()

employees_data = {
    &#039;EmployeeID&#039;: [1, 2, 3, 4, 5],
    &#039;Name&#039;: [&#039;John&#039;, &#039;Emma&#039;, &#039;Liam&#039;, &#039;Olivia&#039;, &#039;William&#039;],
    &#039;Department&#039;: [&#039;HR&#039;, &#039;Sales&#039;, &#039;IT&#039;, &#039;Marketing&#039;, &#039;Finance&#039;]
}

salaries_data = {
    &#039;EmployeeID&#039;: [1, 2, 3, 4, 5],
    &#039;Salary&#039;: [5000, 6000, 4500, 7000, 5500]
}

employees_df = pai.DataFrame(employees_data)
salaries_df = pai.DataFrame(salaries_data)

# By default, unless you choose a different LLM, it will use BambooLLM.
# You can get your free API key signing up at https://app.pandabi.ai (you can also configure it in your .env file)
pai.api_key.set(&quot;your-pai-api-key&quot;)

pai.chat(&quot;Who gets paid the most?&quot;, employees_df, salaries_df, sandbox=sandbox)

# Don&#039;t forget to stop the sandbox when done
sandbox.stop()
```

```
Olivia gets paid the most.
```

You can find more examples in the [examples](examples) directory.

## 📜 License

PandaAI is available under the MIT expat license, except for the `pandasai/ee` directory of this repository, which has its [license here](https://github.com/sinaptik-ai/pandas-ai/blob/main/ee/LICENSE).

If you are interested in managed PandaAI Cloud or self-hosted Enterprise Offering, [contact us](https://getpanda.ai/pricing).

## Resources

&gt; **Beta Notice**  
&gt; Release v3 is currently in beta. The following documentation and examples reflect the features and functionality in progress and may change before the final release.

- [Docs](https://pandas-ai.readthedocs.io/en/latest/) for comprehensive documentation
- [Examples](examples) for example notebooks
- [Discord](https://discord.gg/KYKj9F2FRH) for discussion with the community and PandaAI team

## 🤝 Contributing

Contributions are welcome! Please check the outstanding issues and feel free to open a pull request.
For more information, please check out the [contributing guidelines](CONTRIBUTING.md).

### Thank you!

[![Contributors](https://contrib.rocks/image?repo=sinaptik-ai/pandas-ai)](https://github.com/sinaptik-ai/pandas-ai/graphs/contributors)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[emcie-co/parlant]]></title>
            <link>https://github.com/emcie-co/parlant</link>
            <guid>https://github.com/emcie-co/parlant</guid>
            <pubDate>Fri, 16 May 2025 00:04:14 GMT</pubDate>
            <description><![CDATA[Parlant is the open-source engine for controlled, compliant, and purposeful generative AI conversations. It gives you the power of LLMs without the unpredictability.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/emcie-co/parlant">emcie-co/parlant</a></h1>
            <p>Parlant is the open-source engine for controlled, compliant, and purposeful generative AI conversations. It gives you the power of LLMs without the unpredictability.</p>
            <p>Language: Python</p>
            <p>Stars: 2,857</p>
            <p>Forks: 286</p>
            <p>Stars today: 82 stars today</p>
            <h2>README</h2><pre>
&lt;div align=&quot;center&quot;&gt;
&lt;!--&lt;img alt=&quot;Parlant Banner&quot; src=&quot;https://github.com/emcie-co/parlant/blob/develop/banner.png?raw=true&quot; /&gt;--&gt;


&lt;h1&gt;Parlant&lt;/h1&gt;
  &lt;h3&gt;The Conversation Modeling Engine&lt;/h3&gt;

Parlant is the open-source framework for safe, compliant, and custom generative AI conversations. It gives you the power of LLMs without the unpredictability.

  &lt;a href=&quot;https://trendshift.io/repositories/12768&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12768&quot; alt=&quot;emcie-co%2Fparlant | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;


  &lt;p&gt;
    &lt;a href=&quot;https://www.parlant.io/&quot; target=&quot;_blank&quot;&gt;Website&lt;/a&gt; —
    &lt;a href=&quot;https://www.parlant.io/docs/quickstart/introduction&quot; target=&quot;_blank&quot;&gt;Introduction&lt;/a&gt; —
    &lt;a href=&quot;https://www.parlant.io/docs/tutorial/getting-started&quot; target=&quot;_blank&quot;&gt;Tutorial&lt;/a&gt; —
    &lt;a href=&quot;https://www.parlant.io/docs/about&quot; target=&quot;_blank&quot;&gt;About&lt;/a&gt;
  &lt;/p&gt;


  
  &lt;p&gt;
    &lt;a href=&quot;https://pypi.org/project/parlant/&quot; alt=&quot;Parlant on PyPi&quot;&gt;&lt;img alt=&quot;PyPI - Version&quot; src=&quot;https://img.shields.io/pypi/v/parlant&quot;&gt;&lt;/a&gt;
    &lt;img alt=&quot;PyPI - Python Version&quot; src=&quot;https://img.shields.io/pypi/pyversions/parlant&quot;&gt;
    &lt;a href=&quot;https://opensource.org/licenses/Apache-2.0&quot;&gt;&lt;img alt=&quot;Apache 2 License&quot; src=&quot;https://img.shields.io/badge/license-Apache%202.0-blue.svg&quot; /&gt;&lt;/a&gt;
    &lt;img alt=&quot;GitHub commit activity&quot; src=&quot;https://img.shields.io/github/commit-activity/w/emcie-co/parlant?label=commits&quot;&gt;
    &lt;img alt=&quot;PyPI - Downloads&quot; src=&quot;https://img.shields.io/pypi/dm/parlant&quot;&gt;
    &lt;a href=&quot;https://discord.gg/duxWqxKk6J&quot;&gt;&lt;img alt=&quot;Discord&quot; src=&quot;https://img.shields.io/discord/1312378700993663007?style=flat&amp;logo=discord&amp;logoColor=white&amp;label=discord&quot;&gt;
&lt;/a&gt;
  &lt;/p&gt;

&lt;/div&gt;

## YouTube Video Intro
[![Parlant Introduction](https://github.com/emcie-co/parlant/blob/develop/yt-preview.png?raw=true)](https://www.youtube.com/watch?v=_39ERIb0100)

1. Install
```bash
pip install parlant
```

2. Start the server and start interact with the default agent
```bash
parlant-server run
# Now visit http://localhost:8800
```

3. Add behavioral guidelines and let Parlant do the rest
```bash
parlant guideline create \
    --condition &quot;the user greets you&quot; \
    --action &quot;thank them for checking out Parlant&quot;
# Now start a new conversation and greet the agent
```

## Quick Demo
&lt;img alt=&quot;Parlant Banner&quot; src=&quot;https://github.com/emcie-co/parlant/blob/develop/ParlantGIF.gif?raw=true&quot; /&gt;


## What is Conversation Modeling?
You&#039;ve built an AI agent—that&#039;s great! However, when you actually test it, you see it&#039;s not handling many customer interactions properly, and your business experts are displeased with it. What do you do?

Enter Conversation Modeling (CM): a new powerful and reliable approach to controlling how your agents interact with your users.

A conversation model is a structured, domain-specific set of principles, actions, objectives, and terms that an agent applies to a given conversation.

### Why Conversation Modeling?

The problem of getting your AI agent to say what _you_ want it to say is a hard one, experienced by virtually anyone building customer-facing agents. Here&#039;s how Conversation Modeling compares to other approaches to solving this problem.

- **Flow engines** _force_ the user to interact according to predefined flows. In contrast, a **CM engine** dynamically _adapts_ to a user&#039;s natural interaction patterns while conforming to your rules.

- **Free-form prompt engineering** leads to _inconsistency_, frequently failing to uphold requirements. Conversely, a **CM engine** leverages structure to _enforce_ conformance to a Conversation Model.


## Who uses Parlant?
Parlant is used to deliver complex conversational agents that reliably follow your business protocols in use cases such as:
- 🏦 Regulated financial services
- 🏥 Healthcare communications
- 📜 Legal assistance
- 🛡️ Compliance-focused use cases
- 🎯 Brand-sensitive customer service
- 🤝 Personal advocacy and representation

## How is Parlant used?
Developers and data-scientists are using Parlant to:

- 🤖 Create custom-tailored conversational agents quickly and easily
- 👣 Define behavioral guidelines for agents to follow (Parlant ensures they are followed reliably)
- 🛠️ Attach tools with specific guidance on how to properly use them in different contexts
- 📖 Manage their agents’ glossary to ensure strict interpretation of terms in a conversational context
- 👤 Add customer-specific information to deliver personalized interactions

#### How does Parlant work?
```mermaid
graph TD
    API(Parlant REST API) --&gt;|React to Session Trigger| Engine[AI Response Engine]
    Engine --&gt;|Load Domain Terminology| GlossaryStore
    Engine --&gt;|Match Guidelines| GuidelineMatcher
    Engine --&gt;|Infer &amp; Call Tools| ToolCaller
    Engine --&gt;|Tailor Guided Message| MessageComposer
```

When an agent needs to respond to a customer, Parlant&#039;s engine evaluates the situation, checks relevant guidelines, gathers necessary information through your tools, and continuously re-evaluates its approach based on your guidelines as new information emerges. When it&#039;s time to generate a message, Parlant implements self-critique mechanisms to ensure that the agent&#039;s responses precisely align with your intended behavior as given by the contextually-matched guidelines.

***📚 More technical docs on the architecture and API are available under [docs/](./docs)***.

## 📦 Quickstart
Parlant comes pre-built with responsive session (conversation) management, a detection mechanism for incoherence and contradictions in guidelines, content-filtering, jailbreak protection, an integrated sandbox UI for behavioral testing, native API clients in Python and TypeScript, and other goodies.

```bash
$ pip install parlant
$ parlant-server run
$ # Open the sandbox UI at http://localhost:8800 and play
```

## 🙋‍♂️🙋‍♀️ Who Is Parlant For?
Parlant is the right tool for the job if you&#039;re building an LLM-based chat agent, and:

1. 🎯 Your use case places a **high importance on behavioral precision and consistency**, particularly in customer-facing scenarios
1. 🔄 Your agent is expected to undergo **continuous behavioral refinements and changes**, and you need a way to implement those changes efficiently and confidently
1. 📈 You&#039;re expected to maintain a **growing set of behavioral guidelines**, and you need to maintain them coherently and with version-tracking
1. 💬 Conversational UX and user-engagmeent is an important concern for your use case, and you want to easily **control the flow and tone of conversations**

## ⭐ Star Us: Your Support Goes a Long Way!
[![Star History Chart](https://api.star-history.com/svg?repos=emcie-co/parlant&amp;type=Date)](https://star-history.com/#emcie-co/parlant&amp;Date)

## 🤔 What Makes Parlant Different?

In a word: **_Guidance._** 🧭🚦🤝

Parlant&#039;s engine revolves around solving one key problem: How can we _reliably guide_ customer-facing agents to behave in alignment with our needs and intentions.

Hence Parlant&#039;s fundamentally different approach to agent building: [Managed Guidelines](https://www.parlant.io/docs/concepts/customization/guidelines):

```bash
parlant guideline create \
  --condition &quot;the customer wants to return an item&quot; \
  --action &quot;get the order number and item name and then help them return it&quot;
```

By giving structure to behavioral guidelines, and _granularizing_ guidelines (i.e. making each behavioral guideline a first-class entity in the engine), Parlant&#039;s engine is able to offer unprecedented control, quality, and efficiency in building LLM-based agents:

1. 🛡️ **Reliability:** Running focused self-critique in real-time, per guideline, to ensure it is actually followed
1. 💡 **Explainability:** Providing feedback around its interpretation of guidelines in each real-life context, which helps in troubleshooting and improvement
1. 🔧 **Maintainability:** Helping you maintain a coherent set of guidelines by detecting and alerting you to possible contradictions (gross or subtle) in your instructions

## 🤖 Works with all major LLM providers
- [OpenAI](https://platform.openai.com/docs/overview) (also via [Azure](https://learn.microsoft.com/en-us/azure/ai-services/openai/))
- [Gemini](https://ai.google.dev/)
- [Meta Llama 3](https://www.llama.com/) (via [Together AI](https://www.together.ai/) or [Cerebras](https://cerebras.ai/))
- [Anthropic](https://www.anthropic.com/api) (also via [AWS Bedrock](https://aws.amazon.com/bedrock/))
- And more are added regularly

## 📚 Learning Parlant

To start learning and building with Parlant, visit our [documentation portal](https://parlant.io/docs/quickstart/introduction).

Need help? Ask us anything on [Discord](https://discord.gg/duxWqxKk6J). We&#039;re happy to answer questions and help you get up and running!

## 💻 Usage Example
Adding a guideline for an agent—for example, to ask a counter-question to get more info when a customer asks a question:
```bash
parlant guideline create \
    --condition &quot;a free-tier customer is asking how to use our product&quot; \
    --action &quot;first seek to understand what they&#039;re trying to achieve&quot;
```

## 👋 Contributing
We use the Linux-standard Developer Certificate of Origin ([DCO.md](DCO.md)), so that, by contributing, you confirm that you have the rights to submit your contribution under the Apache 2.0 license (i.e., that the code you&#039;re contributing is truly yours to share with the project).

Please consult [CONTRIBUTING.md](CONTRIBUTING.md) for more details.

Can&#039;t wait to get involved? Join us on [Discord](https://discord.gg/duxWqxKk6J) and let&#039;s discuss how you can help shape Parlant. We&#039;re excited to work with contributors directly while we set up our formal processes!

Otherwise, feel free to start a discussion or open an issue here on GitHub—freestyle 😎.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[LibreTranslate/LibreTranslate]]></title>
            <link>https://github.com/LibreTranslate/LibreTranslate</link>
            <guid>https://github.com/LibreTranslate/LibreTranslate</guid>
            <pubDate>Fri, 16 May 2025 00:04:13 GMT</pubDate>
            <description><![CDATA[Free and Open Source Machine Translation API. Self-hosted, offline capable and easy to setup.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/LibreTranslate/LibreTranslate">LibreTranslate/LibreTranslate</a></h1>
            <p>Free and Open Source Machine Translation API. Self-hosted, offline capable and easy to setup.</p>
            <p>Language: Python</p>
            <p>Stars: 11,421</p>
            <p>Forks: 1,110</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre># LibreTranslate

[Try it online!](https://libretranslate.com) | [API Docs](https://libretranslate.com/docs) | [Community Forum](https://community.libretranslate.com/) | [Bluesky](https://bsky.app/profile/libretranslate.com)

[![Python versions](https://img.shields.io/pypi/pyversions/libretranslate)](https://pypi.org/project/libretranslate) [![Run tests](https://github.com/LibreTranslate/LibreTranslate/workflows/Run%20tests/badge.svg)](https://github.com/LibreTranslate/LibreTranslate/actions?query=workflow%3A%22Run+tests%22) [![Build and Publish Docker Image](https://github.com/LibreTranslate/LibreTranslate/actions/workflows/publish-docker.yml/badge.svg)](https://github.com/LibreTranslate/LibreTranslate/actions/workflows/publish-docker.yml) [![Publish package](https://github.com/LibreTranslate/LibreTranslate/actions/workflows/publish-package.yml/badge.svg)](https://github.com/LibreTranslate/LibreTranslate/actions/workflows/publish-package.yml) [![Awesome Humane Tech](https://raw.githubusercontent.com/humanetech-community/awesome-humane-tech/main/humane-tech-badge.svg?sanitize=true)](https://codeberg.org/teaserbot-labs/delightful-humane-design)

Free and Open Source Machine Translation API, entirely self-hosted. Unlike other APIs, it doesn&#039;t rely on proprietary providers such as Google or Azure to perform translations. Instead, its translation engine is powered by the open source [Argos Translate](https://github.com/argosopentech/argos-translate) library.

![Translation](https://github.com/user-attachments/assets/457696b5-dbff-40ab-a18e-7bfb152c5121)

## API Examples

### Simple

Request:

```javascript
const res = await fetch(&quot;https://libretranslate.com/translate&quot;, {
  method: &quot;POST&quot;,
  body: JSON.stringify({
    q: &quot;Hello!&quot;,
    source: &quot;en&quot;,
    target: &quot;es&quot;,
  }),
  headers: { &quot;Content-Type&quot;: &quot;application/json&quot; },
});

console.log(await res.json());
```

Response:

```javascript
{
    &quot;translatedText&quot;: &quot;¡Hola!&quot;
}
```

List of language codes: https://libretranslate.com/languages

### Auto Detect Language

Request:

```javascript
const res = await fetch(&quot;https://libretranslate.com/translate&quot;, {
  method: &quot;POST&quot;,
  body: JSON.stringify({
    q: &quot;Ciao!&quot;,
    source: &quot;auto&quot;,
    target: &quot;en&quot;,
  }),
  headers: { &quot;Content-Type&quot;: &quot;application/json&quot; },
});

console.log(await res.json());
```

Response:

```javascript
{
    &quot;detectedLanguage&quot;: {
        &quot;confidence&quot;: 83,
        &quot;language&quot;: &quot;it&quot;
    },
    &quot;translatedText&quot;: &quot;Bye!&quot;
}
```

### HTML

Request:

```javascript
const res = await fetch(&quot;https://libretranslate.com/translate&quot;, {
  method: &quot;POST&quot;,
  body: JSON.stringify({
    q: &#039;&lt;p class=&quot;green&quot;&gt;Hello!&lt;/p&gt;&#039;,
    source: &quot;en&quot;,
    target: &quot;es&quot;,
    format: &quot;html&quot;,
  }),
  headers: { &quot;Content-Type&quot;: &quot;application/json&quot; },
});

console.log(await res.json());
```

Response:

```javascript
{
    &quot;translatedText&quot;: &quot;&lt;p class=\&quot;green\&quot;&gt;¡Hola!&lt;/p&gt;&quot;
}
```

### Alternative Translations

Request:

```javascript
const res = await fetch(&quot;https://libretranslate.com/translate&quot;, {
  method: &quot;POST&quot;,
  body: JSON.stringify({
    q: &quot;Hello&quot;,
    source: &quot;en&quot;,
    target: &quot;it&quot;,
    format: &quot;text&quot;,
    alternatives: 3,
  }),
  headers: { &quot;Content-Type&quot;: &quot;application/json&quot; },
});

console.log(await res.json());
```

Response:

```javascript
{
    &quot;alternatives&quot;: [
        &quot;Salve&quot;,
        &quot;Pronto&quot;
    ],
    &quot;translatedText&quot;: &quot;Ciao&quot;
}
```

## Install and Run

You can run your own API server with just a few lines of setup!

Make sure you have Python installed (3.8 or higher is recommended), then simply run:

```bash
pip install libretranslate
libretranslate [args]
```

Then open a web browser to &lt;http://localhost:5000&gt;

By default LibreTranslate will install support for all available languages. To only load certain languages and reduce startup time, you can use the **--load-only** argument:

```bash
libretranslate --load-only en,es,fr
```

Check also all other [arguments](#settings--flags) below.

On Ubuntu 20.04 you can also use the install script available at &lt;https://github.com/argosopentech/LibreTranslate-init&gt;

## Run with Docker

You can also run the application with [docker](https://docker.com):

### Linux/macOS

```bash
./run.sh [args]
```

### Windows

```bash
run.bat [args]
```

## Build and Run

See [CONTRIBUTING.md](./CONTRIBUTING.md) for information on how to build and run the project yourself.

### CUDA

You can use hardware acceleration to speed up translations on a GPU machine with CUDA 12.4.1 and [nvidia-docker](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html) installed.

Run this version with:

```bash
docker compose -f docker-compose.cuda.yml up -d --build
```

## Arguments

Arguments passed to the process or set via environment variables are split into two kinds.

- Settings or runtime flags used to toggle specific runmodes or disable parts of the application. These act as toggle when added or removed.

- Configuration parameters to set various limits and configure the application. These require a parameter to be passed to function, if removed the default parameters are used.

### Settings / Flags

| Argument                      | Description                                                                                                 | Default Setting                    | Env. name                      |
| ----------------------------- | ----------------------------------------------------------------------------------------------------------- | ---------------------------------- | ------------------------------ |
| --debug                       | Enable debug environment                                                                                    | `Disabled`                         | LT_DEBUG                       |
| --ssl                         | Whether to enable SSL                                                                                       | `Disabled`                         | LT_SSL                         |
| --api-keys                    | Enable API keys database for per-client rate limits when --req-limit is reached                             | `Don&#039;t use API keys`               | LT_API_KEYS                    |
| --require-api-key-origin      | Require use of an API key for programmatic access to the API, unless the request origin matches this domain | `No restrictions on domain origin` | LT_REQUIRE_API_KEY_ORIGIN      |
| --require-api-key-secret      | Require use of an API key for programmatic access to the API, unless the client also sends a secret match   | `No secrets required`              | LT_REQUIRE_API_KEY_SECRET      |
| --require-api-key-fingerprint | Require use of an API key for programmatic access to the API, unless the client also matches a fingerprint  | `No fingerprinting required`       | LT_REQUIRE_API_KEY_FINGERPRINT |
| --under-attack                | Enable under attack mode. When enabled, requests must be made with an API key                               | `Disabled`                         | LT_UNDER_ATTACK                |
| --suggestions                 | Allow user suggestions                                                                                      | `Disabled`                         | LT_SUGGESTIONS                 |
| --disable-files-translation   | Disable files translation                                                                                   | `File translation allowed`         | LT_DISABLE_FILES_TRANSLATION   |
| --disable-web-ui              | Disable web ui                                                                                              | `Web Ui enabled`                   | LT_DISABLE_WEB_UI              |
| --update-models               | Update language models at startup                                                                           | `Only on if no models found`       | LT_UPDATE_MODELS               |
| --metrics                     | Enable the /metrics endpoint for exporting [Prometheus](https://prometheus.io/) usage metrics               | `Disabled`                         | LT_METRICS                     |

### Configuration Parameters

| Argument                   | Description                                                                                                                                                                                                 | Default Parameter                     | Env. name                   |
| -------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------- | --------------------------- |
| --host                     | Set host to bind the server to                                                                                                                                                                              | `127.0.0.1`                           | LT_HOST                     |
| --port                     | Set port to bind the server to                                                                                                                                                                              | `5000`                                | LT_PORT                     |
| --char-limit               | Set character limit                                                                                                                                                                                         | `No limit`                            | LT_CHAR_LIMIT               |
| --req-limit                | Set maximum number of requests per minute per client (outside of limits set by api keys)                                                                                                                    | `No limit`                            | LT_REQ_LIMIT                |
| --req-limit-storage        | Storage URI to use for request limit data storage. See [Flask Limiter](https://flask-limiter.readthedocs.io/en/stable/configuration.html)                                                                   | `memory://`                           | LT_REQ_LIMIT_STORAGE        |
| --req-time-cost            | Considers a time cost (in seconds) for request limiting purposes. If a request takes 10 seconds and this value is set to 5, the request cost is either 2 or the actual request cost (whichever is greater). | `No time cost`                        | LT_REQ_TIME_COST            |
| --batch-limit              | Set maximum number of texts to translate in a batch request                                                                                                                                                 | `No limit`                            | LT_BATCH_LIMIT              |
| --ga-id                    | Enable Google Analytics on the API client page by providing an ID                                                                                                                                           | `Empty (no tracking)`                 | LT_GA_ID                    |
| --frontend-language-source | Set frontend default language - source                                                                                                                                                                      | `auto`                                | LT_FRONTEND_LANGUAGE_SOURCE |
| --frontend-language-target | Set frontend default language - target                                                                                                                                                                      | `locale` (match site&#039;s locale)        | LT_FRONTEND_LANGUAGE_TARGET |
| --frontend-timeout         | Set frontend translation timeout                                                                                                                                                                            | `500`                                 | LT_FRONTEND_TIMEOUT         |
| --api-keys-db-path         | Use a specific path inside the container for the local database. Can be absolute or relative                                                                                                                | `db/api_keys.db`                      | LT_API_KEYS_DB_PATH         |
| --api-keys-remote          | Use this remote endpoint to query for valid API keys instead of using the local database                                                                                                                    | `Empty (use local db instead)`        | LT_API_KEYS_REMOTE          |
| --get-api-key-link         | Show a link in the UI where to direct users to get an API key                                                                                                                                               | `Empty (no link shown on web ui)`     | LT_GET_API_KEY_LINK         |
| --shared-storage           | Shared storage URI to use for multi-process data sharing (e.g. when using gunicorn)                                                                                                                         | `memory://`                           | LT_SHARED_STORAGE           |
| --secondary                | Mark this instance as a secondary instance to avoid conflicts with the primary node in multi-node setups                                                                                                    | `Primary node`                        | LT_SECONDARY                |
| --load-only                | Set available languages                                                                                                                                                                                     | `Empty (use all from argostranslate)` | LT_LOAD_ONLY                |
| --threads                  | Set number of threads                                                                                                                                                                                       | `4`                                   | LT_THREADS                  |
| --metrics-auth-token       | Protect the /metrics endpoint by allowing only clients that have a valid Authorization Bearer token                                                                                                         | `Empty (no auth required)`            | LT_METRICS_AUTH_TOKEN       |
| --url-prefix               | Add prefix to URL: example.com:5000/url-prefix/                                                                                                                                                             | `/`                                   | LT_URL_PREFIX               |

### Notes:

- Each argument has an equivalent environment variable that can be used instead. The env. variables overwrite the default values but have lower priority than the command arguments and are particularly useful if used with Docker. The environment variable names are the upper-snake-case of the equivalent command argument&#039;s name with a `LT` prefix.

- To configure requirement for api key to use, set `--req-limit` to `0` and add the `--api-keys` flag. Requests made without a proper api key will be rejected.

- Setting `--update-models` will update models regardless of whether updates are available or not.

## Update

### Software

If you installed with pip:

`pip install -U libretranslate`

If you&#039;re using docker:

`docker pull libretranslate/libretranslate`

### Language Models

Start the program with the `--update-models` argument. For example: `libretranslate --update-models` or `./run.sh --update-models`.

Alternatively you can also run the `scripts/install_models.py` script.

## Run with WSGI and Gunicorn

```bash
pip install gunicorn
gunicorn --bind 0.0.0.0:5000 &#039;wsgi:app&#039;
```

You can pass application arguments directly to Gunicorn via:

```bash
gunicorn --bind 0.0.0.0:5000 &#039;wsgi:app(api_keys=True)&#039;
```

## Kubernetes Deployment

See [Medium article by JM Robles](https://jmrobles.medium.com/libretranslate-your-own-translation-service-on-kubernetes-b46c3e1af630) and the improved [k8s.yaml](https://github.com/LibreTranslate/LibreTranslate/blob/main/k8s.yaml) by @rasos.

### Helm Chart

Based on @rasos work you can now install LibreTranslate on Kubernetes using Helm.

A Helm chart is now available in the [helm-chart](https://github.com/LibreTranslate/helm-chart/) repository where you can find more details.

You can quickly install LibreTranslate on Kubernetes using Helm with the following command:

```bash
helm repo add libretranslate https://libretranslate.github.io/helm-chart/
helm repo update
helm search repo libretranslate

helm install libretranslate libretranslate/libretranslate --namespace libretranslate --create-namespace
```

## Manage API Keys

LibreTranslate supports per-user limit quotas, e.g. you can issue API keys to users so that they can enjoy higher requests limits per minute (if you also set `--req-limit`). By default all users are rate-limited based on `--req-limit`, but passing an optional `api_key` parameter to the REST endpoints allows a user to enjoy higher request limits. You can also specify different character limits that bypass the default `--char-limit` value on a per-key basis.

To use API keys simply start LibreTranslate with the `--api-keys` option. If you modified the API keys database path with the option `--api-keys-db-path`, you must specify the path with the same argument flag when using the `ltmanage keys` command.

### Add New Keys

To issue a new API key with 120 requests per minute limits:

```bash
ltmanage keys add 120
```

To issue a new API key with 120 requests per minute and a maximum of 5,000 characters per request:

```bash
ltmanage keys add 120 --char-limit 5000
```

If you changed the API keys database path:

```bash
ltmanage keys --api-keys-db-path path/to/db/dbName.db add 120
```

### Remove Keys

```bash
ltmanage keys remove &lt;api-key&gt;
```

### View Keys

```bash
ltmanage keys
```

## Prometheus Metrics

LibreTranslate has Prometheus [exporter](https://prometheus.io/docs/instrumenting/exporters/) capabilities when you pass the `--metrics` argument at startup (disabled by default). When metrics are enabled, a `/metrics` endpoint is mounted on the instance:

&lt;http://localhost:5000/metrics&gt;

```promql
# HELP libretranslate_http_requests_in_flight Multiprocess metric
# TYPE libretranslate_http_requests_in_flight gauge
libretranslate_http_requests_in_flight{api_key=&quot;&quot;,endpoint=&quot;/translate&quot;,request_ip=&quot;127.0.0.1&quot;} 0.0
# HELP libretranslate_http_request_duration_seconds Multiprocess metric
# TYPE libretranslate_http_request_duration_seconds summary
libretranslate_http_request_duration_seconds_count{api_key=&quot;&quot;,endpoint=&quot;/translate&quot;,request_ip=&quot;127.0.0.1&quot;,status=&quot;200&quot;} 0.0
libretranslate_http_request_duration_seconds_sum{api_key=&quot;&quot;,endpoint=&quot;/translate&quot;,request_ip=&quot;127.0.0.1&quot;,status=&quot;200&quot;} 0.0
```

You can then configure `prometheus.yml` to read the metrics:

```yaml
scrape_configs:
  - job_name: &quot;libretranslate&quot;

    # Needed only if you use --metrics-auth-token
    #authorization:
    #credentials: &quot;mytoken&quot;

    static_configs:
      - targets: [&quot;localhost:5000&quot;]
```

To secure the `/metrics` endpoint you can also use `--metrics-auth-token mytoken`.

If you use Gunicorn, make sure to create a directory for storing multiprocess data metrics and set `PROMETHEUS_MULTIPROC_DIR`:

```bash
mkdir -p /tmp/prometheus_data
rm /tmp/prometheus_data/*
export PROMETHEUS_MULTIPROC_DIR=/tmp/prometheus_data
gunicorn -c scripts/gunicorn_conf.py --bind 0.0.0.0:5000 &#039;wsgi:app(metrics=True)&#039;
```

## Language Bindings

You can use the LibreTranslate API using the following bindings:

- Rust: &lt;https://github.com/DefunctLizard/libretranslate-rs&gt;
- Node.js: &lt;https://

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mlflow/mlflow]]></title>
            <link>https://github.com/mlflow/mlflow</link>
            <guid>https://github.com/mlflow/mlflow</guid>
            <pubDate>Fri, 16 May 2025 00:04:12 GMT</pubDate>
            <description><![CDATA[Open source platform for the machine learning lifecycle]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mlflow/mlflow">mlflow/mlflow</a></h1>
            <p>Open source platform for the machine learning lifecycle</p>
            <p>Language: Python</p>
            <p>Stars: 20,524</p>
            <p>Forks: 4,523</p>
            <p>Stars today: 20 stars today</p>
            <h2>README</h2><pre># MLflow: A Machine Learning Lifecycle Platform

[![Latest Docs](https://img.shields.io/badge/docs-latest-success.svg?style=for-the-badge)](https://mlflow.org/docs/latest/index.html)
[![Apache 2 License](https://img.shields.io/badge/license-Apache%202-brightgreen.svg?style=for-the-badge&amp;logo=apache)](https://github.com/mlflow/mlflow/blob/master/LICENSE.txt)
[![Total Downloads](https://img.shields.io/pypi/dw/mlflow?style=for-the-badge&amp;logo=pypi&amp;logoColor=white)](https://pepy.tech/project/mlflow)
[![Slack](https://img.shields.io/badge/slack-@mlflow--users-CF0E5B.svg?logo=slack&amp;logoColor=white&amp;labelColor=3F0E40&amp;style=for-the-badge)](https://mlflow.org/community/#slack)
[![Twitter](https://img.shields.io/twitter/follow/MLflow?style=for-the-badge&amp;labelColor=00ACEE&amp;logo=twitter&amp;logoColor=white)](https://twitter.com/MLflow)

MLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in handling the complexities of the machine learning process. MLflow focuses on the full lifecycle for machine learning projects, ensuring that each phase is manageable, traceable, and reproducible

---

The core components of MLflow are:

- [Experiment Tracking](https://mlflow.org/docs/latest/tracking.html) 📝: A set of APIs to log models, params, and results in ML experiments and compare them using an interactive UI.
- [Model Packaging](https://mlflow.org/docs/latest/models.html) 📦: A standard format for packaging a model and its metadata, such as dependency versions, ensuring reliable deployment and strong reproducibility.
- [Model Registry](https://mlflow.org/docs/latest/model-registry.html) 💾: A centralized model store, set of APIs, and UI, to collaboratively manage the full lifecycle of MLflow Models.
- [Serving](https://mlflow.org/docs/latest/deployment/index.html) 🚀: Tools for seamless model deployment to batch and real-time scoring on platforms like Docker, Kubernetes, Azure ML, and AWS SageMaker.
- [Evaluation](https://mlflow.org/docs/latest/model-evaluation/index.html) 📊: A suite of automated model evaluation tools, seamlessly integrated with experiment tracking to record model performance and visually compare results across multiple models.
- [Observability](https://mlflow.org/docs/latest/llms/tracing/index.html) 🔍: Tracing integrations with various GenAI libraries and a Python SDK for manual instrumentation, offering smoother debugging experience and supporting online monitoring.

&lt;img src=&quot;https://mlflow.org/img/hero.png&quot; alt=&quot;MLflow Hero&quot; width=100%&gt;

## Installation

To install the MLflow Python package, run the following command:

```
pip install mlflow
```

Alternatively, you can install MLflow from on different package hosting platforms:

|               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| ------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| PyPI          | [![PyPI - mlflow](https://img.shields.io/pypi/v/mlflow.svg?style=for-the-badge&amp;logo=pypi&amp;logoColor=white&amp;label=mlflow)](https://pypi.org/project/mlflow/) [![PyPI - mlflow-skinny](https://img.shields.io/pypi/v/mlflow-skinny.svg?style=for-the-badge&amp;logo=pypi&amp;logoColor=white&amp;label=mlflow-skinny)](https://pypi.org/project/mlflow-skinny/)                                                                                                                                                                                                                                                                                                                                          |
| conda-forge   | [![Conda - mlflow](https://img.shields.io/conda/vn/conda-forge/mlflow.svg?style=for-the-badge&amp;logo=anaconda&amp;label=mlflow)](https://anaconda.org/conda-forge/mlflow) [![Conda - mlflow-skinny](https://img.shields.io/conda/vn/conda-forge/mlflow.svg?style=for-the-badge&amp;logo=anaconda&amp;label=mlflow-skinny)](https://anaconda.org/conda-forge/mlflow-skinny)                                                                                                                                                                                                                                                                                                                             |
| CRAN          | [![CRAN - mlflow](https://img.shields.io/cran/v/mlflow.svg?style=for-the-badge&amp;logo=r&amp;label=mlflow)](https://cran.r-project.org/package=mlflow)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| Maven Central | [![Maven Central - mlflow-client](https://img.shields.io/maven-central/v/org.mlflow/mlflow-client.svg?style=for-the-badge&amp;logo=apache-maven&amp;label=mlflow-client)](https://mvnrepository.com/artifact/org.mlflow/mlflow-client) [![Maven Central - mlflow-parent](https://img.shields.io/maven-central/v/org.mlflow/mlflow-parent.svg?style=for-the-badge&amp;logo=apache-maven&amp;label=mlflow-parent)](https://mvnrepository.com/artifact/org.mlflow/mlflow-parent) [![Maven Central - mlflow-spark](https://img.shields.io/maven-central/v/org.mlflow/mlflow-spark.svg?style=for-the-badge&amp;logo=apache-maven&amp;label=mlflow-spark)](https://mvnrepository.com/artifact/org.mlflow/mlflow-spark) |

## Documentation 📘

Official documentation for MLflow can be found at [here](https://mlflow.org/docs/latest/index.html).

## Running Anywhere 🌐

You can run MLflow on many different environments, including local development, Amazon SageMaker, AzureML, and Databricks. Please refer to [this guidance](https://mlflow.org/docs/latest/index.html#running-mlflow-anywhere) for how to setup MLflow on your environment.

## Usage

### Experiment Tracking ([Doc](https://mlflow.org/docs/latest/tracking.html))

The following examples trains a simple regression model with scikit-learn, while enabling MLflow&#039;s [autologging](https://mlflow.org/docs/latest/tracking/autolog.html) feature for experiment tracking.

```python
import mlflow

from sklearn.model_selection import train_test_split
from sklearn.datasets import load_diabetes
from sklearn.ensemble import RandomForestRegressor

# Enable MLflow&#039;s automatic experiment tracking for scikit-learn
mlflow.sklearn.autolog()

# Load the training dataset
db = load_diabetes()
X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)

rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)
# MLflow triggers logging automatically upon model fitting
rf.fit(X_train, y_train)
```

Once the above code finishes, run the following command in a separate terminal and access the MLflow UI via the printed URL. An MLflow **Run** should be automatically created, which tracks the training dataset, hyper parameters, performance metrics, the trained model, dependencies, and even more.

```
mlflow ui
```

### Serving Models ([Doc](https://mlflow.org/docs/latest/deployment/index.html))

You can deploy the logged model to a local inference server by a one-line command using the MLflow CLI. Visit the documentation for how to deploy models to other hosting platforms.

```bash
mlflow models serve --model-uri runs:/&lt;run-id&gt;/model
```

### Evaluating Models ([Doc](https://mlflow.org/docs/latest/model-evaluation/index.html))

The following example runs automatic evaluation for question-answering tasks with several built-in metrics.

```python
import mlflow
import pandas as pd

# Evaluation set contains (1) input question (2) model outputs (3) ground truth
df = pd.DataFrame(
    {
        &quot;inputs&quot;: [&quot;What is MLflow?&quot;, &quot;What is Spark?&quot;],
        &quot;outputs&quot;: [
            &quot;MLflow is an innovative fully self-driving airship powered by AI.&quot;,
            &quot;Sparks is an American pop and rock duo formed in Los Angeles.&quot;,
        ],
        &quot;ground_truth&quot;: [
            &quot;MLflow is an open-source platform for managing the end-to-end machine learning (ML) &quot;
            &quot;lifecycle.&quot;,
            &quot;Apache Spark is an open-source, distributed computing system designed for big data &quot;
            &quot;processing and analytics.&quot;,
        ],
    }
)
eval_dataset = mlflow.data.from_pandas(
    df, predictions=&quot;outputs&quot;, targets=&quot;ground_truth&quot;
)

# Start an MLflow Run to record the evaluation results to
with mlflow.start_run(run_name=&quot;evaluate_qa&quot;):
    # Run automatic evaluation with a set of built-in metrics for question-answering models
    results = mlflow.evaluate(
        data=eval_dataset,
        model_type=&quot;question-answering&quot;,
    )

print(results.tables[&quot;eval_results_table&quot;])
```

### Observability ([Doc](https://mlflow.org/docs/latest/llms/tracing/index.html))

MLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call `mlflow.xyz.autolog()` before running your models. Refer to the documentation for customization and manual instrumentation.

```python
import mlflow
from openai import OpenAI

# Enable tracing for OpenAI
mlflow.openai.autolog()

# Query OpenAI LLM normally
response = OpenAI().chat.completions.create(
    model=&quot;gpt-4o-mini&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hi!&quot;}],
    temperature=0.1,
)
```

Then navigate to the &quot;Traces&quot; tab in the MLflow UI to find the trace records OpenAI query.

## Community

- For help or questions about MLflow usage (e.g. &quot;how do I do X?&quot;) visit the [docs](https://mlflow.org/docs/latest/index.html)
  or [Stack Overflow](https://stackoverflow.com/questions/tagged/mlflow).
- Alternatively, you can ask the question to our AI-powered chat bot. Visit the doc website and click on the **&quot;Ask AI&quot;** button at the right bottom to start chatting with the bot.
- To report a bug, file a documentation issue, or submit a feature request, please [open a GitHub issue](https://github.com/mlflow/mlflow/issues/new/choose).
- For release announcements and other discussions, please subscribe to our mailing list (mlflow-users@googlegroups.com)
  or join us on [Slack](https://mlflow.org/slack).

## Contributing

We happily welcome contributions to MLflow! We are also seeking contributions to items on the
[MLflow Roadmap](https://github.com/mlflow/mlflow/milestone/3). Please see our
[contribution guide](CONTRIBUTING.md) to learn more about contributing to MLflow.

## Core Members

MLflow is currently maintained by the following core members with significant contributions from hundreds of exceptionally talented community members.

- [Ben Wilson](https://github.com/BenWilson2)
- [Corey Zumar](https://github.com/dbczumar)
- [Daniel Lok](https://github.com/daniellok-db)
- [Gabriel Fu](https://github.com/gabrielfu)
- [Harutaka Kawamura](https://github.com/harupy)
- [Serena Ruan](https://github.com/serena-ruan)
- [Weichen Xu](https://github.com/WeichenXu123)
- [Yuki Watanabe](https://github.com/B-Step62)
- [Tomu Hirata](https://github.com/TomeHirata)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Azure/Azure-Sentinel]]></title>
            <link>https://github.com/Azure/Azure-Sentinel</link>
            <guid>https://github.com/Azure/Azure-Sentinel</guid>
            <pubDate>Fri, 16 May 2025 00:04:11 GMT</pubDate>
            <description><![CDATA[Cloud-native SIEM for intelligent security analytics for your entire enterprise.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Azure/Azure-Sentinel">Azure/Azure-Sentinel</a></h1>
            <p>Cloud-native SIEM for intelligent security analytics for your entire enterprise.</p>
            <p>Language: Python</p>
            <p>Stars: 5,043</p>
            <p>Forks: 3,185</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre>
# Microsoft Sentinel and Microsoft 365 Defender 
Welcome to the unified Microsoft Sentinel and Microsoft 365 Defender repository! This repository contains out of the box detections, exploration queries, hunting queries, workbooks, playbooks and much more to help you get ramped up with Microsoft Sentinel and provide you security content to secure your environment and hunt for threats. The hunting queries also include Microsoft 365 Defender hunting queries for advanced hunting scenarios in both Microsoft 365 Defender and Microsoft Sentinel. You can also submit to [issues](https://github.com/Azure/Azure-Sentinel/issues) for any samples or resources you would like to see here as you onboard to Microsoft Sentinel. This repository welcomes contributions and refer to this repository&#039;s [wiki](https://aka.ms/threathunters) to get started. For questions and feedback, please contact [AzureSentinel@microsoft.com](AzureSentinel@microsoft.com) 

# Resources
* [Microsoft Sentinel documentation](https://go.microsoft.com/fwlink/?linkid=2073774&amp;clcid=0x409)
* [Microsoft 365 Defender documentation](https://docs.microsoft.com/microsoft-365/security/defender/microsoft-365-defender?view=o365-worldwide)
* [Security Community Webinars](https://aka.ms/securitywebinars)
* [Getting started with GitHub](https://help.github.com/en#dotcom)

We value your feedback. Here are some channels to help surface your questions or feedback:
1. General product specific Q&amp;A for SIEM and SOAR - Join in the [Microsoft Sentinel Tech Community conversations](https://techcommunity.microsoft.com/t5/microsoft-sentinel/bd-p/MicrosoftSentinel)
2. General product specific Q&amp;A for XDR - Join in the [Microsoft 365 Defender Tech Community conversations](https://techcommunity.microsoft.com/t5/microsoft-365-defender/bd-p/MicrosoftThreatProtection)
3. Product specific feature requests - Upvote or post new on [Microsoft Sentinel feedback forums](https://feedback.azure.com/d365community/forum/37638d17-0625-ec11-b6e6-000d3a4f07b8)
4. Report product or contribution bugs - File a GitHub Issue using [Bug template](https://github.com/Azure/Azure-Sentinel/issues/new?assignees=&amp;labels=&amp;template=bug_report.md&amp;title=)
5. General feedback on community and contribution process - File a GitHub Issue using [Feature Request template](https://github.com/Azure/Azure-Sentinel/issues/new?assignees=&amp;labels=&amp;template=feature_request.md&amp;title=)


# Contribution guidelines

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

## Add in your new or updated contributions to GitHub
Note: If you are a first time contributor to this repository, [General GitHub Fork the repo guidance](https://docs.github.com/github/getting-started-with-github/fork-a-repo) before cloning or [Specific steps for the Sentinel repo](https://github.com/Azure/Azure-Sentinel/blob/master/GettingStarted.md). 

## General Steps
Brand new or update to a contribution via these methods:
* Submit for review directly on GitHub website 
    * Browse to the folder you want to upload your file to
    * Choose Upload Files and browse to your file. 
    * You will be required to create your own branch and then submit the Pull Request for review.
* Use [GitHub Desktop](https://docs.github.com/en/desktop/overview/getting-started-with-github-desktop) or [Visual Studio](https://visualstudio.microsoft.com/vs/) or [VSCode](https://code.visualstudio.com/?wt.mc_id=DX_841432)
    * [Fork the repo](https://docs.github.com/github/getting-started-with-github/fork-a-repo)  
    * [Clone the repo](https://help.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository)
    * [Create your own branch](https://help.github.com/en/desktop/contributing-to-projects/creating-a-branch-for-your-work)
    * Do your additions/updates in GitHub Desktop
    * Be sure to merge master back to your branch before you push. 
    * [Push your changes to GitHub](https://help.github.com/en/github/using-git/pushing-commits-to-a-remote-repository)

## Pull Request
* After you push your changes, you will need to submit the [Pull Request (PR)](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests)
* Details about the Proposed Changes are required, be sure to include a minimal level of detail so a review can clearly understand the reason for the change and what he change is related to in the code.
* After submission, check the [Pull Request](https://github.com/Azure/Azure-Sentinel/pulls) for comments
* Make changes as suggested and update your branch or explain why no change is needed. Resolve the comment when done.

### Pull Request Detection Template Structure Validation Check
As part of the PR checks we run a structure validation to make sure all required parts of the YAML structure are included.  For Detections, there is a new section that must be included.  See the [contribution guidelines](https://github.com/Azure/Azure-Sentinel/wiki/Contribute-to-Sentinel-GitHub-Community-of-Queries#now-onto-the-how) for more information.  If this section or any other required section is not included, then a validation error will occur similar to the below.
The example is specifically if the YAML is missing the entityMappings section:

```
A total of 1 test files matched the specified pattern.
[xUnit.net 00:00:00.95]     Kqlvalidations.Tests.DetectionTemplateStructureValidationTests.Validate_DetectionTemplates_HaveValidTemplateStructure(detectionsYamlFileName: &quot;ExcessiveBlockedTrafficGeneratedbyUser.yaml&quot;) [FAIL]
  X Kqlvalidations.Tests.DetectionTemplateStructureValidationTests.Validate_DetectionTemplates_HaveValidTemplateStructure(detectionsYamlFileName: &quot;ExcessiveBlockedTrafficGeneratedbyUser.yaml&quot;) [104ms]
  Error Message:
   Expected object to be &lt;null&gt;, but found System.ComponentModel.DataAnnotations.ValidationException with message &quot;An old mapping for entity &#039;AccountCustomEntity&#039; does not have a matching new mapping entry.&quot;
```

### Pull Request KQL Validation Check
As part of the PR checks we run a syntax validation of the KQL queries defined in the template. If this check fails go to Azure Pipeline (by pressing on the errors link on the checks tab in your PR)
![Azurepipeline](.github/Media/Azurepipeline.png)
In the pipeline you can see which test failed and what is the cause:
![Pipeline Tests Tab](.github/Media/PipelineTestsTab.png)

Example error message:
```
A total of 1 test files matched the specified pattern.
[xUnit.net 00:00:01.81]     Kqlvalidations.Tests.KqlValidationTests.Validate_DetectionQueries_HaveValidKql(detectionsYamlFileName: &quot;ExcessiveBlockedTrafficGeneratedbyUser.yaml&quot;) [FAIL]
  X Kqlvalidations.Tests.KqlValidationTests.Validate_DetectionQueries_HaveValidKql(detectionsYamlFileName: &quot;ExcessiveBlockedTrafficGeneratedbyUser.yaml&quot;) [21ms]
  Error Message:
   Template Id:fa0ab69c-7124-4f62-acdd-61017cf6ce89 is not valid Errors:The name &#039;SymantecEndpointProtection&#039; does not refer to any known table, tabular variable or function., Code: &#039;KS204&#039;, Severity: &#039;Error&#039;, Location: &#039;67..93&#039;,The name &#039;SymantecEndpointProtection&#039; does not refer to any known table, tabular variable or function., Code: &#039;KS204&#039;, Severity: &#039;Error&#039;, Location: &#039;289..315&#039;
```
If you are using custom logs table (a table which is not defined on all workspaces by default) you should verify
your table schema is defined in json file in the folder *Azure-Sentinel\\.script\tests\KqlvalidationsTests\CustomTables*

**Example for table tablexyz.json**
```json
{
  &quot;Name&quot;: &quot;tablexyz&quot;,
  &quot;Properties&quot;: [
    {
      &quot;Name&quot;: &quot;SomeDateTimeColumn&quot;,
      &quot;Type&quot;: &quot;DateTime&quot;
    },
    {
      &quot;Name&quot;: &quot;SomeStringColumn&quot;,
      &quot;Type&quot;: &quot;String&quot;
    },
    {
      &quot;Name&quot;: &quot;SomeDynamicColumn&quot;,
      &quot;Type&quot;: &quot;Dynamic&quot;
    }
  ]
}
```
### Run KQL Validation Locally
In order to run the KQL validation before submitting Pull Request in you local machine:
* You need to have **.Net Core 3.1 SDK** installed [How to download .Net](https://dotnet.microsoft.com/download) (Supports all platforms)
* Open Shell and navigate to  `Azure-Sentinel\\.script\tests\KqlvalidationsTests\`
* Execute `dotnet test`

Example of output (in Ubuntu):
```
Welcome to .NET Core 3.1!
---------------------
SDK Version: 3.1.403

Telemetry
---------
The .NET Core tools collect usage data in order to help us improve your experience. The data is anonymous. It is collected by Microsoft and shared with the community. You can opt-out of telemetry by setting the DOTNET_CLI_TELEMETRY_OPTOUT environment variable to &#039;1&#039; or &#039;true&#039; using your favorite shell.

Read more about .NET Core CLI Tools telemetry: https://aka.ms/dotnet-cli-telemetry

----------------
Explore documentation: https://aka.ms/dotnet-docs
Report issues and find source on GitHub: https://github.com/dotnet/core
Find out what&#039;s new: https://aka.ms/dotnet-whats-new
Learn about the installed HTTPS developer cert: https://aka.ms/aspnet-core-https
Use &#039;dotnet --help&#039; to see available commands or visit: https://aka.ms/dotnet-cli-docs
Write your first app: https://aka.ms/first-net-core-app
--------------------------------------------------------------------------------------
Test run for /mnt/c/git/Azure-Sentinel/.script/tests/KqlvalidationsTests/bin/Debug/netcoreapp3.1/Kqlvalidations.Tests.dll(.NETCoreApp,Version=v3.1)
Microsoft (R) Test Execution Command Line Tool Version 16.7.0
Copyright (c) Microsoft Corporation.  All rights reserved.

Starting test execution, please wait...

A total of 1 test files matched the specified pattern.

Test Run Successful.
Total tests: 171
     Passed: 171
 Total time: 25.7973 Seconds
```

### Detection schema validation tests
Similarly to KQL Validation, there is an automatic validation of the schema of a detection.
The schema validation includes the detection&#039;s frequency and period, the detection&#039;s trigger type and threshold, validity of connectors Ids ([valid connectors Ids list](https://github.com/Azure/Azure-Sentinel/blob/master/.script/tests/detectionTemplateSchemaValidation/ValidConnectorIds.json)), etc.
A wrong format or missing attributes will result with an informative check failure, which should guide you through the resolution of the issue, but make sure to look into the format of already approved detection.

### Run Detection Schema Validation Locally
In order to run the KQL validation before submitting Pull Request in you local machine:
* You need to have **.Net Core 3.1 SDK** installed [How to download .Net](https://dotnet.microsoft.com/download) (Supports all platforms)
* Open Shell and navigate to  `Azure-Sentinel\\.script\tests\DetectionTemplateSchemaValidation\`
* Execute `dotnet test`


When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

For information on what you can contribute and further details, refer to the [&quot;get started&quot;](https://github.com/Azure/Azure-Sentinel/wiki#get-started) section on the project&#039;s [wiki](https://aka.ms/threathunters).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mikumifa/cppTickerBuy]]></title>
            <link>https://github.com/mikumifa/cppTickerBuy</link>
            <guid>https://github.com/mikumifa/cppTickerBuy</guid>
            <pubDate>Fri, 16 May 2025 00:04:10 GMT</pubDate>
            <description><![CDATA[cpp cp30 漫展 活动 抢票 无差别 同人展]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mikumifa/cppTickerBuy">mikumifa/cppTickerBuy</a></h1>
            <p>cpp cp30 漫展 活动 抢票 无差别 同人展</p>
            <p>Language: Python</p>
            <p>Stars: 518</p>
            <p>Forks: 54</p>
            <p>Stars today: 49 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/mikumifa/cppTickerBuy&quot; target=&quot;_blank&quot;&gt;
    &lt;img width=&quot;160&quot; src=&quot;icon.ico&quot; alt=&quot;logo&quot;&gt;
  &lt;/a&gt;
  &lt;h1 id=&quot;koishi&quot;&gt;cppTickerBuy&lt;/h1&gt;

![GitHub all releases](https://img.shields.io/github/downloads/mikumifa/cppTickerBuy/total)
![GitHub release (with filter)](https://img.shields.io/github/v/release/mikumifa/cppTickerBuy)
![GitHub issues](https://img.shields.io/github/issues/mikumifa/cppTickerBuy)
![GitHub Repo stars](https://img.shields.io/github/stars/mikumifa/cppTickerBuy)

&lt;/div&gt;

开源免费，简单易用，图形界面, 速度极快的CPP抢票辅助工具


## 快速安装

Windows 下载最新的release文件 (cppTickerBuy.zip) [下载链接](https://github.com/mikumifa/cppTickerBuy/releases)
&gt; **NOTE**
&gt;
&gt; 如果你对Github一点也不了解, 不知道在哪下载
&gt;
&gt; 这里有一份小白指南 [点我前往小白指南](https://github.com/mikumifa/biliTickerBuy/wiki/%E5%B0%8F%E7%99%BD%E4%B8%8B%E8%BD%BD%E6%8C%87%E5%8D%97)

## 使用说明书
重构了UI，启动终端第一行会显示

```
Running on local URL:  http://127.0.0.1:xxx
```

访问对应的网址即可

使用源码手动启动 `main.py` 时可带有如下参数：

- `--share` 选择是否创建sharelink，需传入布尔值 `True/False` ，默认为 `False`

说明书暂时没时间写，用以前的项目代替一下
[点我前往更加详细的使用说明书](https://github.com/mikumifa/biliTickerBuy/wiki/%E6%8A%A2%E7%A5%A8%E8%AF%B4%E6%98%8E)


## 项目问题

程序使用问题： [点此链接前往discussions](https://github.com/mikumifa/cppTickerBuy/discussions)

反馈程序BUG或者提新功能建议： [点此链接向项目提出反馈BUG](https://github.com/mikumifa/cppTickerBuy/issues/new/choose)

## 其他可用脚本

| 链接                                                      | 主要特色               |
| --------------------------------------------------------- | ---------------------- |
| https://github.com/Koileo/ticket_for_allcpp | 能同时开多张票               |



## 项目贡献者

&lt;!-- readme: collaborators,contributors -start --&gt;
&lt;table&gt;
	&lt;tbody&gt;
		&lt;tr&gt;
            &lt;td align=&quot;center&quot;&gt;
                &lt;a href=&quot;https://github.com/mikumifa&quot;&gt;
                    &lt;img src=&quot;https://avatars.githubusercontent.com/u/99951454?v=4&quot; width=&quot;100;&quot; alt=&quot;mikumifa&quot;/&gt;
                    &lt;br /&gt;
                    &lt;sub&gt;&lt;b&gt;mikumifa&lt;/b&gt;&lt;/sub&gt;
                &lt;/a&gt;
            &lt;/td&gt;
            &lt;td align=&quot;center&quot;&gt;
                &lt;a href=&quot;https://github.com/WittF&quot;&gt;
                    &lt;img src=&quot;https://avatars.githubusercontent.com/u/108567138?v=4&quot; width=&quot;100;&quot; alt=&quot;WittF&quot;/&gt;
                    &lt;br /&gt;
                    &lt;sub&gt;&lt;b&gt;W1ttF&lt;/b&gt;&lt;/sub&gt;
                &lt;/a&gt;
            &lt;/td&gt;
		&lt;/tr&gt;
	&lt;tbody&gt;
&lt;/table&gt;
&lt;!-- readme: collaborators,contributors -end --&gt;


## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=mikumifa/cppTickerBuy&amp;type=Date)](https://star-history.com/#mikumifa/cppTickerBuy&amp;Date)

## 免责声明

详见[MIT License](./LICENSE)，切勿进行盈利，所造成的后果与本人无关。

## 捐赠

如果你想支持这个项目的话 [爱发电](https://afdian.com/a/mikumifa)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>