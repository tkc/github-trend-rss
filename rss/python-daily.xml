<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sun, 14 Dec 2025 00:05:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[mindsdb/mindsdb]]></title>
            <link>https://github.com/mindsdb/mindsdb</link>
            <guid>https://github.com/mindsdb/mindsdb</guid>
            <pubDate>Sun, 14 Dec 2025 00:05:03 GMT</pubDate>
            <description><![CDATA[Federated query engine for AI - The only MCP Server you'll ever need]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mindsdb/mindsdb">mindsdb/mindsdb</a></h1>
            <p>Federated query engine for AI - The only MCP Server you'll ever need</p>
            <p>Language: Python</p>
            <p>Stars: 37,816</p>
            <p>Forks: 6,054</p>
            <p>Stars today: 23 stars today</p>
            <h2>README</h2><pre>

&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
	&lt;a href=&quot;https://pypi.org/project/MindsDB/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/MindsDB.svg&quot; alt=&quot;MindsDB Release&quot;&gt;&lt;/a&gt;
	&lt;a href=&quot;https://www.python.org/downloads/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/python-3.10.x%7C%203.11.x%7C%203.12.x%7C%203.13.x-brightgreen.svg&quot; alt=&quot;Python supported&quot;&gt;&lt;/a&gt;
	&lt;a href=&quot;https://hub.docker.com/u/mindsdb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/mindsdb/mindsdb&quot; alt=&quot;Docker pulls&quot;&gt;&lt;/a&gt;

  &lt;br /&gt;
  &lt;br /&gt;

  &lt;a href=&quot;https://trendshift.io/repositories/3068&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/3068&quot; alt=&quot;mindsdb%2Fmindsdb | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

  &lt;a href=&quot;https://github.com/mindsdb/mindsdb&quot;&gt;
    &lt;img src=&quot;/docs/assets/mindsdb_logo.png&quot; alt=&quot;MindsDB&quot; width=&quot;300&quot;&gt;
  &lt;/a&gt;

  &lt;p align=&quot;center&quot;&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://www.mindsdb.com?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Website&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://docs.mindsdb.com?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Docs&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://mindsdb.com/contact&quot;&gt;Contact us for a Demo&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://mindsdb.com/joincommunity?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Community Slack&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

----------------------------------------


MindsDB enables humans, AI, agents, and applications to get highly accurate answers across large scale data sources.

&lt;a href=&quot;https://www.youtube.com/watch?v=MX3OKpnsoLM&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/119e7b82-f901-4214-a26f-ff7c5ad86064&quot; alt=&quot;MindsDB Demo&quot;&gt;
	
&lt;/a&gt;


## Install MindsDB Server 

MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart&#039;s content.

  * [Using Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop). This is the fastest and recommended way to get started and have it all running.
  * [Using Docker](https://docs.mindsdb.com/setup/self-hosted/docker). This is also simple, but gives you more flexibility on how to further customize your server.

[MindsDB has an MCP server built in](https://docs.mindsdb.com/mcp/overview) that enables your MCP applications to connect, unify and respond to questions over large-scale federated dataâ€”spanning databases, data warehouses, and SaaS applications.
 
----------------------------------------

# Core Philosophy: Connect, Unify, Respond

MindsDB&#039;s architecture is built around three fundamental capabilities:

## [Connect](https://docs.mindsdb.com/integrations/data-overview) Your Data

You can connect to hundreds of enterprise [data sources (learn more)](https://docs.mindsdb.com/integrations/data-overview). These integrations allow MindsDB to access data wherever it resides, forming the foundation for all other capabilities.

## [Unify](https://docs.mindsdb.com/mindsdb_sql/overview) Your Data


In many situations, itâ€™s important to be able to prepare and unify data before generating responses from it. MindsDB SQL offers knowledge bases and views that allow indexing and organizing structured and unstructured data as if it were unified in a single system.

* [**KNOWLEDGE BASES**](https://docs.mindsdb.com/mindsdb_sql/knowledge-bases) â€“ Index and organize unstructured data for efficient Q&amp;A.
* [**VIEWS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/view) â€“ Simplify data access by creating unified views across different sources (no-ETL).


Unification of data can be automated using JOBs

* [**JOBS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/jobs) â€“ Schedule synchronization and transformation tasks for real-time processing.


## [Respond](https://docs.mindsdb.com/mindsdb_sql/agents/agent) From Your Data

Chat with Your Data

* [**AGENTS**](https://docs.mindsdb.com/mindsdb_sql/agents/agent) â€“ Configure built-in agents specialized in answering questions over your connected and unified data.
* [**MCP**](https://docs.mindsdb.com/mcp/overview) â€“ Connect to MindsDB through the MCP (Model Context Protocol) for seamless interaction.

----------------------------------------

## ğŸ¤ Contribute

Interested in contributing to MindsDB? Follow our [installation guide for development](https://docs.mindsdb.com/contribute/install?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

You can find our [contribution guide here](https://docs.mindsdb.com/contribute/contribute?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

We welcome suggestions! Feel free to open new issues with your ideas, and weâ€™ll guide you.

This project adheres to a [Contributor Code of Conduct](https://github.com/mindsdb/mindsdb/blob/main/CODE_OF_CONDUCT.md). By participating, you agree to follow its terms.

Also, check out our [community rewards and programs](https://mindsdb.com/community?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

## ğŸ¤ Support

If you find a bug, please submit an [issue on GitHub](https://github.com/mindsdb/mindsdb/issues/new/choose).

Hereâ€™s how you can get community support:

* Ask a question in our [Slack Community](https://mindsdb.com/joincommunity?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).
* Join our [GitHub Discussions](https://github.com/mindsdb/mindsdb/discussions).
* Post on [Stack Overflow](https://stackoverflow.com/questions/tagged/mindsdb) with the MindsDB tag.

For commercial support, please [contact the MindsDB team](https://mindsdb.com/contact?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

## ğŸ’š Current Contributors

&lt;a href=&quot;https://github.com/mindsdb/mindsdb/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contributors-img.web.app/image?repo=mindsdb/mindsdb&quot; /&gt;
&lt;/a&gt;

Generated with [contributors-img](https://contributors-img.web.app).

## ğŸ”” Subscribe for Updates

Join our [Slack community](https://mindsdb.com/joincommunity)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[spipm/Depixelization_poc]]></title>
            <link>https://github.com/spipm/Depixelization_poc</link>
            <guid>https://github.com/spipm/Depixelization_poc</guid>
            <pubDate>Sun, 14 Dec 2025 00:05:02 GMT</pubDate>
            <description><![CDATA[Depix is a PoC for a technique to recover plaintext from pixelized screenshots.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/spipm/Depixelization_poc">spipm/Depixelization_poc</a></h1>
            <p>Depix is a PoC for a technique to recover plaintext from pixelized screenshots.</p>
            <p>Language: Python</p>
            <p>Stars: 3,617</p>
            <p>Forks: 273</p>
            <p>Stars today: 197 stars today</p>
            <h2>README</h2><pre># Depix

Depix is a PoC for a technique to recover plaintext from pixelized screenshots.

This implementation works on pixelized images that were created with a linear box filter.
In [this article](https://www.spipm.nl/2030.html) I cover background information on pixelization and similar research.

## Example

![image](docs/img/Recovering_prototype_latest.png)

## Updates

* 24 dec &#039;24: Made repo private, changed the name and made it public again. It just had a ridiculous amount of stars because of the media hype, which didn&#039;t feel right. I made this as a quick PoC for a company back in the day, because someone pixelated part of a password for an account with Domain Admin rights. The hype got running by the catchy image and eventually this repo had 26152 stars. If I ever get this much stars again, I want it to be for a project that I&#039;m that hyped about as well.
![image](images/stars.png)
* 27 nov &#039;23: Refactored and removed all this pip stuff. I like scripts I can just run. If a package can&#039;t be found, just install it. Also added `tool_show_boxes.py` to show how bad the box detector is (you have to really cut out the pixels exactly). Made a TODO to create a version that just cuts out boxes of static size.

## Installation

* Install the dependencies
* Run Depix:

```sh
python3 depix.py \
    -p /path/to/your/input/image.png \
    -s images/searchimages/debruinseq_notepad_Windows10_closeAndSpaced.png \
    -o /path/to/your/output.png
```

## Example usage

* Depixelize example image created with Notepad and pixelized with Greenshot. Greenshot averages by averaging the gamma-encoded 0-255 values, which is Depix&#039;s default mode.

```sh
python3 depix.py \
    -p images/testimages/testimage3_pixels.png \
    -s images/searchimages/debruinseq_notepad_Windows10_closeAndSpaced.png
```

Result: ![image](docs/img/example_output_multiword.png)

* Depixelize example image created with Sublime and pixelized with Gimp, where averaging is done in linear sRGB. The backgroundcolor option filters out the background color of the editor.

```sh
python3 depix.py \
    -p images/testimages/sublime_screenshot_pixels_gimp.png \
    -s images/searchimages/debruin_sublime_Linux_small.png \
    --backgroundcolor 40,41,35 \
    --averagetype linear
```

Result: ![image](docs/img/output_depixelizedExample_linear.png)

* (Optional) You can view if the box detector thingie finds your pixels with `tool_show_boxes.py`. Consider a smaller batch of pixels if this looks all mangled. Example of good looking boxes:

```sh
python3 tool_show_boxes.py \ 
    -p images/testimages/testimage3_pixels.png \
    -s images/searchimages/debruinseq_notepad_Windows10_closeAndSpaced.png
```

* (Optional) You can create pixelized image by using `tool_gen_pixelated.py`.

```sh
python3 tool_gen_pixelated.py -i /path/to/image.png -o pixed_output.png
```

* For a detailed explanation, please try to run `$ python3 depix.py -h` and `tool_gen_pixelated.py`.

## About

### Making a Search Image

* Cut out the pixelated blocks from the screenshot as a single rectangle.
* Paste a [De Bruijn sequence](https://en.wikipedia.org/wiki/De_Bruijn_sequence) with expected characters in an editor with the same font settings as your input image (Same text size, similar font, same colors).
* Make a screenshot of the sequence.
* Move that screenshot into a folder like `images/searchimages/`.
* Run Depix with the `-s` flag set to the location of this screenshot.

### Making a Pixelized Image

* Cut out the pixelized blocks exactly. See the `testimages` for examples.
* It tries to detect blocks but it doesn&#039;t do an amazing job. Play with the `tool_show_boxes.py` script and different cutouts if your blocks aren&#039;t properly detected.

### Algorithm

The algorithm uses the fact that the linear box filter processes every block separately. For every block it pixelizes all blocks in the search image to check for direct matches.

For some pixelized images Depix manages to find single-match results. It assumes these are correct. The matches of surrounding multi-match blocks are then compared to be geometrically at the same distance as in the pixelized image. Matches are also treated as correct. This process is repeated a couple of times.

After correct blocks have no more geometrical matches, it will output all correct blocks directly. For multi-match blocks, it outputs the average of all matches.

### Known limitations

* The algorithm matches by integer block-boundaries. As a result, it has the underlying assumption that for all characters rendered (both in the de Brujin sequence and the pixelated image), the text positioning is done at pixel level. However, some modern text rasterizers position text [at sub-pixel accuracies](http://agg.sourceforge.net/antigrain.com/research/font_rasterization/).
* You need to know the font specifications and in some cases the screen settings with which the screenshot was taken. However, if there is enough plaintext in the original image you might be able to use the original as a search image.
* This approach doesn&#039;t work if additional image compression is performed, because it messes up the colors of a block.

### Future development

* Implement more filter functions

Create more averaging filters that work like some popular editors do.

* Create a new tool that utilizes HMMs

Still, anyone who is passionate about this type of depixelization is encouraged to implement their own HMM-based version and share it.

### Other sources and tools

After creating this program, someone pointed me to a [research document](https://www.researchgate.net/publication/305423573_On_the_Ineffectiveness_of_Mosaicing_and_Blurring_as_Tools_for_Document_Redaction) from 2016 where a group of researchers managed to create a similar tool. Their tool has better precision and works across many different fonts. While their original source code is not public, an open-source implementation exists at [DepixHMM](https://github.com/JonasSchatz/DepixHMM).

Edit 16 Feb &#039;22: [Dan Petro](https://bishopfox.com/authors/dan-petro) created the tool UnRedacter ([write-up](https://bishopfox.com/blog/unredacter-tool-never-pixelation), [source](https://github.com/BishopFox/unredacter)) to crack a [challenge](https://labs.jumpsec.com/can-depix-deobfuscate-your-data/) that was created as a response to Depix!

Edit 16 Apr &#039;25: Jeff Geerling created a [challenge](https://www.jeffgeerling.com/blog/2025/its-easier-ever-de-censor-videos) for depixelating pixelated folder content in a moving image. Three people were able to do it. [Here](https://github.com/KoKuToru/de-pixelate_gaV-O6NPWrI) is a repo from KoKuToru showing how to do this with TensorFlow! Amazing!
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[datawhalechina/hello-agents]]></title>
            <link>https://github.com/datawhalechina/hello-agents</link>
            <guid>https://github.com/datawhalechina/hello-agents</guid>
            <pubDate>Sun, 14 Dec 2025 00:05:01 GMT</pubDate>
            <description><![CDATA[ğŸ“š ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹â€”â€”ä»é›¶å¼€å§‹çš„æ™ºèƒ½ä½“åŸç†ä¸å®è·µæ•™ç¨‹]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/datawhalechina/hello-agents">datawhalechina/hello-agents</a></h1>
            <p>ğŸ“š ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹â€”â€”ä»é›¶å¼€å§‹çš„æ™ºèƒ½ä½“åŸç†ä¸å®è·µæ•™ç¨‹</p>
            <p>Language: Python</p>
            <p>Stars: 8,697</p>
            <p>Forks: 930</p>
            <p>Stars today: 423 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;right&quot;&gt;
  &lt;a href=&quot;./README_EN.md&quot;&gt;English&lt;/a&gt; | ä¸­æ–‡
&lt;/div&gt;

&lt;div align=&#039;center&#039;&gt;
  &lt;img src=&quot;./docs/images/hello-agents.png&quot; alt=&quot;alt text&quot; width=&quot;100%&quot;&gt;
  &lt;h1&gt;Hello-Agents&lt;/h1&gt;
  &lt;h3&gt;ğŸ¤– ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹&lt;/h3&gt;
  &lt;p&gt;&lt;em&gt;ä»åŸºç¡€ç†è®ºåˆ°å®é™…åº”ç”¨ï¼Œå…¨é¢æŒæ¡æ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡ä¸å®ç°&lt;/em&gt;&lt;/p&gt;
  &lt;img src=&quot;https://img.shields.io/github/stars/datawhalechina/Hello-Agents?style=flat&amp;logo=github&quot; alt=&quot;GitHub stars&quot;/&gt;
  &lt;img src=&quot;https://img.shields.io/github/forks/datawhalechina/Hello-Agents?style=flat&amp;logo=github&quot; alt=&quot;GitHub forks&quot;/&gt;
  &lt;img src=&quot;https://img.shields.io/badge/language-Chinese-brightgreen?style=flat&quot; alt=&quot;Language&quot;/&gt;
  &lt;a href=&quot;https://github.com/datawhalechina/Hello-Agents&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-Project-blue?style=flat&amp;logo=github&quot; alt=&quot;GitHub Project&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://datawhalechina.github.io/hello-agents/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/åœ¨çº¿é˜…è¯»-Online%20Reading-green?style=flat&amp;logo=gitbook&quot; alt=&quot;Online Reading&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

---

## ğŸ¯ é¡¹ç›®ä»‹ç»

&amp;emsp;&amp;emsp;å¦‚æœè¯´ 2024 å¹´æ˜¯&quot;ç™¾æ¨¡å¤§æˆ˜&quot;çš„å…ƒå¹´ï¼Œé‚£ä¹ˆ 2025 å¹´æ— ç–‘å¼€å¯äº†&quot;Agent å…ƒå¹´&quot;ã€‚æŠ€æœ¯çš„ç„¦ç‚¹æ­£ä»è®­ç»ƒæ›´å¤§çš„åŸºç¡€æ¨¡å‹ï¼Œè½¬å‘æ„å»ºæ›´èªæ˜çš„æ™ºèƒ½ä½“åº”ç”¨ã€‚ç„¶è€Œï¼Œå½“å‰ç³»ç»Ÿæ€§ã€é‡å®è·µçš„æ•™ç¨‹å´æåº¦åŒ®ä¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å‘èµ·äº† Hello-Agents é¡¹ç›®ï¼Œå¸Œæœ›èƒ½ä¸ºç¤¾åŒºæä¾›ä¸€æœ¬ä»é›¶å¼€å§‹ã€ç†è®ºä¸å®æˆ˜å¹¶é‡çš„æ™ºèƒ½ä½“ç³»ç»Ÿæ„å»ºæŒ‡å—ã€‚

&amp;emsp;&amp;emsp;Hello-Agents æ˜¯ Datawhale ç¤¾åŒºçš„&lt;strong&gt;ç³»ç»Ÿæ€§æ™ºèƒ½ä½“å­¦ä¹ æ•™ç¨‹&lt;/strong&gt;ã€‚å¦‚ä»Š Agent æ„å»ºä¸»è¦åˆ†ä¸ºä¸¤æ´¾ï¼Œä¸€æ´¾æ˜¯ Difyï¼ŒCozeï¼Œn8n è¿™ç±»è½¯ä»¶å·¥ç¨‹ç±» Agentï¼Œå…¶æœ¬è´¨æ˜¯æµç¨‹é©±åŠ¨çš„è½¯ä»¶å¼€å‘ï¼ŒLLM ä½œä¸ºæ•°æ®å¤„ç†çš„åç«¯ï¼›å¦ä¸€æ´¾åˆ™æ˜¯ AI åŸç”Ÿçš„ Agentï¼Œå³çœŸæ­£ä»¥ AI é©±åŠ¨çš„ Agentã€‚æœ¬æ•™ç¨‹æ—¨åœ¨å¸¦é¢†å¤§å®¶æ·±å…¥ç†è§£å¹¶æ„å»ºåè€…â€”â€”çœŸæ­£çš„ AI Native Agentã€‚æ•™ç¨‹å°†å¸¦é¢†ä½ ç©¿é€æ¡†æ¶è¡¨è±¡ï¼Œä»æ™ºèƒ½ä½“çš„æ ¸å¿ƒåŸç†å‡ºå‘ï¼Œæ·±å…¥å…¶æ ¸å¿ƒæ¶æ„ï¼Œç†è§£å…¶ç»å…¸èŒƒå¼ï¼Œå¹¶æœ€ç»ˆäº²æ‰‹æ„å»ºèµ·å±äºè‡ªå·±çš„å¤šæ™ºèƒ½ä½“åº”ç”¨ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæœ€å¥½çš„å­¦ä¹ æ–¹å¼å°±æ˜¯åŠ¨æ‰‹å®è·µã€‚å¸Œæœ›è¿™æœ¬æ•™ç¨‹èƒ½æˆä¸ºä½ æ¢ç´¢æ™ºèƒ½ä½“ä¸–ç•Œçš„èµ·ç‚¹ï¼Œèƒ½å¤Ÿä»ä¸€åå¤§è¯­è¨€æ¨¡å‹çš„&quot;ä½¿ç”¨è€…&quot;ï¼Œèœ•å˜ä¸ºä¸€åæ™ºèƒ½ä½“ç³»ç»Ÿçš„&quot;æ„å»ºè€…&quot;ã€‚

## ğŸ“š å¿«é€Ÿå¼€å§‹

### åœ¨çº¿é˜…è¯»
**[ğŸŒ ç‚¹å‡»è¿™é‡Œå¼€å§‹åœ¨çº¿é˜…è¯»](https://datawhalechina.github.io/hello-agents/)** - æ— éœ€ä¸‹è½½ï¼Œéšæ—¶éšåœ°å­¦ä¹ 

**[ğŸ“– Cookbook(æµ‹è¯•ç‰ˆ)](https://book.heterocat.com.cn/)**

### æœ¬åœ°é˜…è¯»
å¦‚æœæ‚¨å¸Œæœ›åœ¨æœ¬åœ°é˜…è¯»æˆ–è´¡çŒ®å†…å®¹ï¼Œè¯·å‚è€ƒä¸‹æ–¹çš„å­¦ä¹ æŒ‡å—ã€‚

### âœ¨ ä½ å°†æ”¶è·ä»€ä¹ˆï¼Ÿ

- ğŸ“– &lt;strong&gt;Datawhale å¼€æºå…è´¹&lt;/strong&gt; å®Œå…¨å…è´¹å­¦ä¹ æœ¬é¡¹ç›®æ‰€æœ‰å†…å®¹ï¼Œä¸ç¤¾åŒºå…±åŒæˆé•¿
- ğŸ” &lt;strong&gt;ç†è§£æ ¸å¿ƒåŸç†&lt;/strong&gt; æ·±å…¥ç†è§£æ™ºèƒ½ä½“çš„æ¦‚å¿µã€å†å²ä¸ç»å…¸èŒƒå¼
- ğŸ—ï¸ &lt;strong&gt;äº²æ‰‹å®ç°&lt;/strong&gt; æŒæ¡çƒ­é—¨ä½ä»£ç å¹³å°å’Œæ™ºèƒ½ä½“ä»£ç æ¡†æ¶çš„ä½¿ç”¨
- ğŸ› ï¸ &lt;strong&gt;è‡ªç ”æ¡†æ¶[HelloAgents](https://github.com/jjyaoao/helloagents)&lt;/strong&gt; åŸºäº Openai åŸç”Ÿ API ä»é›¶æ„å»ºä¸€ä¸ªè‡ªå·±çš„æ™ºèƒ½ä½“æ¡†æ¶
- âš™ï¸ &lt;strong&gt;æŒæ¡é«˜çº§æŠ€èƒ½&lt;/strong&gt; ä¸€æ­¥æ­¥å®ç°ä¸Šä¸‹æ–‡å·¥ç¨‹ã€Memoryã€åè®®ã€è¯„ä¼°ç­‰ç³»ç»Ÿæ€§æŠ€æœ¯
- ğŸ¤ &lt;strong&gt;æ¨¡å‹è®­ç»ƒ&lt;/strong&gt; æŒæ¡ Agentic RLï¼Œä» SFT åˆ° GRPO çš„å…¨æµç¨‹å®æˆ˜è®­ç»ƒ LLM
- ğŸš€ &lt;strong&gt;é©±åŠ¨çœŸå®æ¡ˆä¾‹&lt;/strong&gt; å®æˆ˜å¼€å‘æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹ã€èµ›åšå°é•‡ç­‰ç»¼åˆé¡¹ç›®
- ğŸ“– &lt;strong&gt;æ±‚èŒé¢è¯•&lt;/strong&gt; å­¦ä¹ æ™ºèƒ½ä½“æ±‚èŒç›¸å…³é¢è¯•é—®é¢˜

## ğŸ“– å†…å®¹å¯¼èˆª

| ç« èŠ‚                                                                                        | å…³é”®å†…å®¹                                      | çŠ¶æ€ |
| ------------------------------------------------------------------------------------------- | --------------------------------------------- | ---- |
| [å‰è¨€](./docs/å‰è¨€.md)                                                                      | é¡¹ç›®çš„ç¼˜èµ·ã€èƒŒæ™¯åŠè¯»è€…å»ºè®®                    | âœ…    |
| &lt;strong&gt;ç¬¬ä¸€éƒ¨åˆ†ï¼šæ™ºèƒ½ä½“ä¸è¯­è¨€æ¨¡å‹åŸºç¡€&lt;/strong&gt;                                             |                                               |      |
| [ç¬¬ä¸€ç«  åˆè¯†æ™ºèƒ½ä½“](./docs/chapter1/ç¬¬ä¸€ç« %20åˆè¯†æ™ºèƒ½ä½“.md)                                 | æ™ºèƒ½ä½“å®šä¹‰ã€ç±»å‹ã€èŒƒå¼ä¸åº”ç”¨                  | âœ…    |
| [ç¬¬äºŒç«  æ™ºèƒ½ä½“å‘å±•å²](./docs/chapter2/ç¬¬äºŒç« %20æ™ºèƒ½ä½“å‘å±•å².md)                             | ä»ç¬¦å·ä¸»ä¹‰åˆ° LLM é©±åŠ¨çš„æ™ºèƒ½ä½“æ¼”è¿›             | âœ…    |
| [ç¬¬ä¸‰ç«  å¤§è¯­è¨€æ¨¡å‹åŸºç¡€](./docs/chapter3/ç¬¬ä¸‰ç« %20å¤§è¯­è¨€æ¨¡å‹åŸºç¡€.md)                         | Transformerã€æç¤ºã€ä¸»æµ LLM åŠå…¶å±€é™          | âœ…    |
| &lt;strong&gt;ç¬¬äºŒéƒ¨åˆ†ï¼šæ„å»ºä½ çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“&lt;/strong&gt;                                         |                                               |      |
| [ç¬¬å››ç«  æ™ºèƒ½ä½“ç»å…¸èŒƒå¼æ„å»º](./docs/chapter4/ç¬¬å››ç« %20æ™ºèƒ½ä½“ç»å…¸èŒƒå¼æ„å»º.md)                 | æ‰‹æŠŠæ‰‹å®ç° ReActã€Plan-and-Solveã€Reflection  | âœ…    |
| [ç¬¬äº”ç«  åŸºäºä½ä»£ç å¹³å°çš„æ™ºèƒ½ä½“æ­å»º](./docs/chapter5/ç¬¬äº”ç« %20åŸºäºä½ä»£ç å¹³å°çš„æ™ºèƒ½ä½“æ­å»º.md) | äº†è§£ Cozeã€Difyã€n8n ç­‰ä½ä»£ç æ™ºèƒ½ä½“å¹³å°ä½¿ç”¨   | âœ…    |
| [ç¬¬å…­ç«  æ¡†æ¶å¼€å‘å®è·µ](./docs/chapter6/ç¬¬å…­ç« %20æ¡†æ¶å¼€å‘å®è·µ.md)                             | AutoGenã€AgentScopeã€LangGraph ç­‰ä¸»æµæ¡†æ¶åº”ç”¨ | âœ…    |
| [ç¬¬ä¸ƒç«  æ„å»ºä½ çš„Agentæ¡†æ¶](./docs/chapter7/ç¬¬ä¸ƒç« %20æ„å»ºä½ çš„Agentæ¡†æ¶.md)                   | ä» 0 å¼€å§‹æ„å»ºæ™ºèƒ½ä½“æ¡†æ¶                       | âœ…    |
| &lt;strong&gt;ç¬¬ä¸‰éƒ¨åˆ†ï¼šé«˜çº§çŸ¥è¯†æ‰©å±•&lt;/strong&gt;                                                     |                                               |      |
| [ç¬¬å…«ç«  è®°å¿†ä¸æ£€ç´¢](./docs/chapter8/ç¬¬å…«ç« %20è®°å¿†ä¸æ£€ç´¢.md)                                 | è®°å¿†ç³»ç»Ÿï¼ŒRAGï¼Œå­˜å‚¨                           | âœ…    |
| [ç¬¬ä¹ç«  ä¸Šä¸‹æ–‡å·¥ç¨‹](./docs/chapter9/ç¬¬ä¹ç« %20ä¸Šä¸‹æ–‡å·¥ç¨‹.md)                                 | æŒç»­äº¤äº’çš„&quot;æƒ…å¢ƒç†è§£&quot;                          | âœ…    |
| [ç¬¬åç«  æ™ºèƒ½ä½“é€šä¿¡åè®®](./docs/chapter10/ç¬¬åç« %20æ™ºèƒ½ä½“é€šä¿¡åè®®.md)                        | MCPã€A2Aã€ANP ç­‰åè®®è§£æ                      | âœ…    |
| [ç¬¬åä¸€ç«  Agentic-RL](./docs/chapter11/ç¬¬åä¸€ç« %20Agentic-RL.md)                            | ä» SFT åˆ° GRPO çš„ LLM è®­ç»ƒå®æˆ˜                | âœ…    |
| [ç¬¬åäºŒç«  æ™ºèƒ½ä½“æ€§èƒ½è¯„ä¼°](./docs/chapter12/ç¬¬åäºŒç« %20æ™ºèƒ½ä½“æ€§èƒ½è¯„ä¼°.md)                    | æ ¸å¿ƒæŒ‡æ ‡ã€åŸºå‡†æµ‹è¯•ä¸è¯„ä¼°æ¡†æ¶                  | âœ…    |
| &lt;strong&gt;ç¬¬å››éƒ¨åˆ†ï¼šç»¼åˆæ¡ˆä¾‹è¿›é˜¶&lt;/strong&gt;                                                     |                                               |      |
| [ç¬¬åä¸‰ç«  æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹](./docs/chapter13/ç¬¬åä¸‰ç« %20æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹.md)                        | MCP ä¸å¤šæ™ºèƒ½ä½“åä½œçš„çœŸå®ä¸–ç•Œåº”ç”¨              | âœ…    |
| [ç¬¬åå››ç«  è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“](./docs/chapter14/ç¬¬åå››ç« %20è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“.md)        | DeepResearch Agent å¤ç°ä¸è§£æ                 | âœ…    |
| [ç¬¬åäº”ç«  æ„å»ºèµ›åšå°é•‡](./docs/chapter15/ç¬¬åäº”ç« %20æ„å»ºèµ›åšå°é•‡.md)                        | Agent ä¸æ¸¸æˆçš„ç»“åˆï¼Œæ¨¡æ‹Ÿç¤¾ä¼šåŠ¨æ€              | âœ…    |
| &lt;strong&gt;ç¬¬äº”éƒ¨åˆ†ï¼šæ¯•ä¸šè®¾è®¡åŠæœªæ¥å±•æœ›&lt;/strong&gt;                                               |                                               |      |
| [ç¬¬åå…­ç«  æ¯•ä¸šè®¾è®¡](./docs/chapter16/ç¬¬åå…­ç« %20æ¯•ä¸šè®¾è®¡.md)                                | æ„å»ºå±äºä½ çš„å®Œæ•´å¤šæ™ºèƒ½ä½“åº”ç”¨                  | âœ…    |

### ç¤¾åŒºè´¡çŒ®ç²¾é€‰ (Community Blog)

&amp;emsp;&amp;emsp;æ¬¢è¿å¤§å®¶å°†åœ¨å­¦ä¹  Hello-Agents æˆ– Agent ç›¸å…³æŠ€æœ¯ä¸­çš„ç‹¬åˆ°è§è§£ã€å®è·µæ€»ç»“ï¼Œä»¥ PR çš„å½¢å¼è´¡çŒ®åˆ°ç¤¾åŒºç²¾é€‰ã€‚å¦‚æœæ˜¯ç‹¬ç«‹äºæ­£æ–‡çš„å†…å®¹ï¼Œä¹Ÿå¯ä»¥æŠ•ç¨¿è‡³ Extra-Chapterï¼&lt;strong&gt;æœŸå¾…ä½ çš„ç¬¬ä¸€æ¬¡è´¡çŒ®ï¼&lt;/strong&gt;

| ç¤¾åŒºç²¾é€‰                                                                                                                                      | å†…å®¹æ€»ç»“                 |
| --------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------ |
| [01-Agenté¢è¯•é¢˜æ€»ç»“](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra01-é¢è¯•é—®é¢˜æ€»ç»“.md)                          | Agent å²—ä½ç›¸å…³é¢è¯•é—®é¢˜   |
| [01-Agenté¢è¯•é¢˜ç­”æ¡ˆ](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra01-å‚è€ƒç­”æ¡ˆ.md)                              | ç›¸å…³é¢è¯•é—®é¢˜ç­”æ¡ˆ         |
| [02-ä¸Šä¸‹æ–‡å·¥ç¨‹å†…å®¹è¡¥å……](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra02-ä¸Šä¸‹æ–‡å·¥ç¨‹è¡¥å……çŸ¥è¯†.md)                 | ä¸Šä¸‹æ–‡å·¥ç¨‹å†…å®¹æ‰©å±•       |
| [03-Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ•™ç¨‹](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra03-Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ“ä½œæµç¨‹.md) | Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ•™ç¨‹ |
| [04-Hello-agentsè¯¾ç¨‹å¸¸è§é—®é¢˜](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra04-DatawhaleFAQ.md)                 | Datawhaleè¯¾ç¨‹å¸¸è§é—®é¢˜    |

### PDF ç‰ˆæœ¬ä¸‹è½½

&amp;emsp;&amp;emsp;*&lt;strong&gt;æœ¬ Hello-Agents PDF æ•™ç¨‹å®Œå…¨å¼€æºå…è´¹ã€‚ä¸ºé˜²æ­¢å„ç±»è¥é”€å·åŠ æ°´å°åè´©å–ç»™å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåˆå­¦è€…ï¼Œæˆ‘ä»¬ç‰¹åœ°åœ¨ PDF æ–‡ä»¶ä¸­é¢„å…ˆæ·»åŠ äº†ä¸å½±å“é˜…è¯»çš„ Datawhale å¼€æºæ ‡å¿—æ°´å°ï¼Œæ•¬è¯·è°…è§£ï½&lt;/strong&gt;*

&gt; *Hello-Agents PDF : https://github.com/datawhalechina/hello-agents/releases/tag/V1.0.0*  
&gt; *Hello-Agents PDF å›½å†…ä¸‹è½½åœ°å€ : https://www.datawhale.cn/learn/summary/239* 

## ğŸ’¡ å¦‚ä½•å­¦ä¹ 

&amp;emsp;&amp;emsp;æ¬¢è¿ä½ ï¼Œæœªæ¥çš„æ™ºèƒ½ç³»ç»Ÿæ„å»ºè€…ï¼åœ¨å¼€å¯è¿™æ®µæ¿€åŠ¨äººå¿ƒçš„æ—…ç¨‹ä¹‹å‰ï¼Œè¯·å…è®¸æˆ‘ä»¬ç»™ä½ ä¸€äº›æ¸…æ™°çš„æŒ‡å¼•ã€‚

&amp;emsp;&amp;emsp;æœ¬é¡¹ç›®å†…å®¹å…¼é¡¾ç†è®ºä¸å®æˆ˜ï¼Œæ—¨åœ¨å¸®åŠ©ä½ ç³»ç»Ÿæ€§åœ°æŒæ¡ä»å•ä¸ªæ™ºèƒ½ä½“åˆ°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡ä¸å¼€å‘å…¨æµç¨‹ã€‚å› æ­¤ï¼Œå°¤å…¶é€‚åˆæœ‰ä¸€å®šç¼–ç¨‹åŸºç¡€çš„ &lt;strong&gt;AI å¼€å‘è€…ã€è½¯ä»¶å·¥ç¨‹å¸ˆã€åœ¨æ ¡å­¦ç”Ÿ&lt;/strong&gt; ä»¥åŠå¯¹å‰æ²¿ AI æŠ€æœ¯æŠ±æœ‰æµ“åšå…´è¶£çš„ &lt;strong&gt;è‡ªå­¦è€…&lt;/strong&gt;ã€‚åœ¨å­¦ä¹ æœ¬é¡¹ç›®ä¹‹å‰ï¼Œæˆ‘ä»¬å¸Œæœ›ä½ å…·å¤‡åŸºç¡€çš„ Python ç¼–ç¨‹èƒ½åŠ›ï¼Œå¹¶å¯¹å¤§è¯­è¨€æ¨¡å‹æœ‰åŸºæœ¬çš„æ¦‚å¿µæ€§äº†è§£ï¼ˆä¾‹å¦‚ï¼ŒçŸ¥é“å¦‚ä½•é€šè¿‡ API è°ƒç”¨ä¸€ä¸ª LLMï¼‰ã€‚é¡¹ç›®çš„é‡ç‚¹æ˜¯åº”ç”¨ä¸æ„å»ºï¼Œå› æ­¤ä½ æ— éœ€å…·å¤‡æ·±åšçš„ç®—æ³•æˆ–æ¨¡å‹è®­ç»ƒèƒŒæ™¯ã€‚

&amp;emsp;&amp;emsp;é¡¹ç›®åˆ†ä¸ºäº”å¤§éƒ¨åˆ†ï¼Œæ¯ä¸€éƒ¨åˆ†éƒ½æ˜¯é€šå¾€ä¸‹ä¸€é˜¶æ®µçš„åšå®é˜¶æ¢¯ï¼š

- &lt;strong&gt;ç¬¬ä¸€éƒ¨åˆ†ï¼šæ™ºèƒ½ä½“ä¸è¯­è¨€æ¨¡å‹åŸºç¡€&lt;/strong&gt;ï¼ˆç¬¬ä¸€ç« ï½ç¬¬ä¸‰ç« ï¼‰ï¼Œæˆ‘ä»¬å°†ä»æ™ºèƒ½ä½“çš„å®šä¹‰ã€ç±»å‹ä¸å‘å±•å†å²è®²èµ·ï¼Œä¸ºä½ æ¢³ç†&quot;æ™ºèƒ½ä½“&quot;è¿™ä¸€æ¦‚å¿µçš„æ¥é¾™å»è„‰ã€‚éšåï¼Œæˆ‘ä»¬ä¼šå¿«é€Ÿå·©å›ºå¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œä¸ºä½ çš„å®è·µä¹‹æ—…æ‰“ä¸‹åšå®çš„ç†è®ºåœ°åŸºã€‚

- &lt;strong&gt;ç¬¬äºŒéƒ¨åˆ†ï¼šæ„å»ºä½ çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“&lt;/strong&gt;ï¼ˆç¬¬å››ç« ï½ç¬¬ä¸ƒç« ï¼‰ï¼Œè¿™æ˜¯ä½ åŠ¨æ‰‹å®è·µçš„èµ·ç‚¹ã€‚ä½ å°†äº²æ‰‹å®ç° ReAct ç­‰ç»å…¸èŒƒå¼ï¼Œä½“éªŒ Coze ç­‰ä½ä»£ç å¹³å°çš„ä¾¿æ·ï¼Œå¹¶æŒæ¡ Langgraph ç­‰ä¸»æµæ¡†æ¶çš„åº”ç”¨ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è¿˜ä¼šå¸¦ä½ ä»é›¶å¼€å§‹æ„å»ºä¸€ä¸ªå±äºè‡ªå·±çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œè®©ä½ å…¼å…·â€œç”¨è½®å­â€ä¸â€œé€ è½®å­â€çš„èƒ½åŠ›ã€‚

- &lt;strong&gt;ç¬¬ä¸‰éƒ¨åˆ†ï¼šé«˜çº§çŸ¥è¯†æ‰©å±•&lt;/strong&gt;ï¼ˆç¬¬å…«ç« ï½ç¬¬åäºŒç« ï¼‰ï¼Œåœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œä½ çš„æ™ºèƒ½ä½“å°†â€œå­¦ä¼šâ€æ€è€ƒä¸åä½œã€‚æˆ‘ä»¬å°†ä½¿ç”¨ç¬¬äºŒéƒ¨åˆ†çš„è‡ªç ”æ¡†æ¶ï¼Œæ·±å…¥æ¢ç´¢è®°å¿†ä¸æ£€ç´¢ã€ä¸Šä¸‹æ–‡å·¥ç¨‹ã€Agent è®­ç»ƒç­‰æ ¸å¿ƒæŠ€æœ¯ï¼Œå¹¶å­¦ä¹ å¤šæ™ºèƒ½ä½“é—´çš„é€šä¿¡åè®®ã€‚æœ€ç»ˆï¼Œä½ å°†æŒæ¡è¯„ä¼°æ™ºèƒ½ä½“ç³»ç»Ÿæ€§èƒ½çš„ä¸“ä¸šæ–¹æ³•ã€‚

- &lt;strong&gt;ç¬¬å››éƒ¨åˆ†ï¼šç»¼åˆæ¡ˆä¾‹è¿›é˜¶&lt;/strong&gt;ï¼ˆç¬¬åä¸‰ç« ï½ç¬¬åäº”ç« ï¼‰ï¼Œè¿™é‡Œæ˜¯ç†è®ºä¸å®è·µçš„äº¤æ±‡ç‚¹ã€‚ä½ å°†æŠŠæ‰€å­¦èä¼šè´¯é€šï¼Œäº²æ‰‹æ‰“é€ æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹ã€è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“ï¼Œä¹ƒè‡³ä¸€ä¸ªæ¨¡æ‹Ÿç¤¾ä¼šåŠ¨æ€çš„èµ›åšå°é•‡ï¼Œåœ¨çœŸå®æœ‰è¶£çš„é¡¹ç›®ä¸­æ·¬ç‚¼ä½ çš„æ„å»ºèƒ½åŠ›ã€‚

- &lt;strong&gt;ç¬¬äº”éƒ¨åˆ†ï¼šæ¯•ä¸šè®¾è®¡åŠæœªæ¥å±•æœ›&lt;/strong&gt;ï¼ˆç¬¬åå…­ç« ï¼‰ï¼Œåœ¨æ—…ç¨‹çš„ç»ˆç‚¹ï¼Œä½ å°†è¿æ¥ä¸€ä¸ªæ¯•ä¸šè®¾è®¡ï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„ã€å±äºä½ è‡ªå·±çš„å¤šæ™ºèƒ½ä½“åº”ç”¨ï¼Œå…¨é¢æ£€éªŒä½ çš„å­¦ä¹ æˆæœã€‚æˆ‘ä»¬è¿˜å°†ä¸ä½ ä¸€åŒå±•æœ›æ™ºèƒ½ä½“çš„æœªæ¥ï¼Œæ¢ç´¢æ¿€åŠ¨äººå¿ƒçš„å‰æ²¿æ–¹å‘ã€‚


&amp;emsp;&amp;emsp;æ™ºèƒ½ä½“æ˜¯ä¸€ä¸ªé£é€Ÿå‘å±•ä¸”æåº¦ä¾èµ–å®è·µçš„é¢†åŸŸã€‚ä¸ºäº†è·å¾—æœ€ä½³çš„å­¦ä¹ æ•ˆæœï¼Œæˆ‘ä»¬åœ¨é¡¹ç›®çš„`code`æ–‡ä»¶å¤¹å†…æä¾›äº†é…å¥—çš„å…¨éƒ¨ä»£ç ï¼Œå¼ºçƒˆå»ºè®®ä½ &lt;strong&gt;å°†ç†è®ºä¸å®è·µç›¸ç»“åˆ&lt;/strong&gt;ã€‚è¯·åŠ¡å¿…äº²æ‰‹è¿è¡Œã€è°ƒè¯•ç”šè‡³ä¿®æ”¹é¡¹ç›®é‡Œæä¾›çš„æ¯ä¸€ä»½ä»£ç ã€‚æ¬¢è¿ä½ éšæ—¶å…³æ³¨ Datawhale ä»¥åŠå…¶ä»– Agent ç›¸å…³ç¤¾åŒºï¼Œå½“é‡åˆ°é—®é¢˜æ—¶ï¼Œä½ å¯ä»¥éšæ—¶åœ¨æœ¬é¡¹ç›®çš„ issue åŒºæé—®ã€‚

&amp;emsp;&amp;emsp;ç°åœ¨ï¼Œå‡†å¤‡å¥½è¿›å…¥æ™ºèƒ½ä½“çš„å¥‡å¦™ä¸–ç•Œäº†å—ï¼Ÿè®©æˆ‘ä»¬å³åˆ»å¯ç¨‹ï¼

## ä¸‹ä¸€æ­¥è§„åˆ’
- []è‹±æ–‡ç‰ˆæ•™ç¨‹
- []åŒè¯­è§†é¢‘è¯¾ç¨‹[è‹±æ–‡+ä¸­æ–‡]ï¼ˆå°†ä¼šæ›´åŠ ç»†è‡´ï¼Œå®è·µè¯¾å¸¦é¢†å¤§å®¶ä»è®¾è®¡æ€è·¯åˆ°å®æ–½ï¼Œæˆäººä»¥é±¼ä¹Ÿæˆäººä»¥æ¸”ï¼‰
- []å…±åˆ›ç¬¬16ç« ï¼ˆæ‰“é€ å„ç±»Agentåº”ç”¨,æ›´æ‰“é€ Agentç”Ÿæ€ï¼‰
  
## ğŸ¤ å¦‚ä½•è´¡çŒ®

æˆ‘ä»¬æ˜¯ä¸€ä¸ªå¼€æ”¾çš„å¼€æºç¤¾åŒºï¼Œæ¬¢è¿ä»»ä½•å½¢å¼çš„è´¡çŒ®ï¼

- ğŸ› &lt;strong&gt;æŠ¥å‘Š Bug&lt;/strong&gt; - å‘ç°å†…å®¹æˆ–ä»£ç é—®é¢˜ï¼Œè¯·æäº¤ Issue
- ğŸ’¡ &lt;strong&gt;æå‡ºå»ºè®®&lt;/strong&gt; - å¯¹é¡¹ç›®æœ‰å¥½æƒ³æ³•ï¼Œæ¬¢è¿å‘èµ·è®¨è®º
- ğŸ“ &lt;strong&gt;å®Œå–„å†…å®¹&lt;/strong&gt; - å¸®åŠ©æ”¹è¿›æ•™ç¨‹ï¼Œæäº¤ä½ çš„ Pull Request
- âœï¸ &lt;strong&gt;åˆ†äº«å®è·µ&lt;/strong&gt; - åœ¨&quot;ç¤¾åŒºè´¡çŒ®ç²¾é€‰&quot;ä¸­åˆ†äº«ä½ çš„å­¦ä¹ ç¬”è®°å’Œé¡¹ç›®

## ğŸ™ è‡´è°¢

### æ ¸å¿ƒè´¡çŒ®è€…
- [é™ˆæ€å·-é¡¹ç›®è´Ÿè´£äºº](https://github.com/jjyaoao) (Datawhale æˆå‘˜, å…¨æ–‡å†™ä½œå’Œæ ¡å¯¹)
- [å­™éŸ¬-é¡¹ç›®è´Ÿè´£äºº](https://github.com/fengju0213) (Datawhale æˆå‘˜, ç¬¬ä¹ç« å†…å®¹å’Œæ ¡å¯¹)  
- [å§œèˆ’å‡¡-é¡¹ç›®è´Ÿè´£äºº](https://github.com/Tsumugii24)ï¼ˆDatawhale æˆå‘˜, ç« èŠ‚ä¹ é¢˜è®¾è®¡å’Œæ ¡å¯¹ï¼‰
- [é»„ä½©æ—-Datawhaleæ„å‘æˆå‘˜](https://github.com/HeteroCat) (Agent å¼€å‘å·¥ç¨‹å¸ˆ, ç¬¬äº”ç« å†…å®¹è´¡çŒ®è€…)
- [æ›¾é‘«æ°‘-Agentå·¥ç¨‹å¸ˆ](https://github.com/fancyboi999) (ç‰›å®¢ç§‘æŠ€, ç¬¬åå››ç« æ¡ˆä¾‹å¼€å‘)
- [æœ±ä¿¡å¿ -æŒ‡å¯¼ä¸“å®¶](https://xinzhongzhu.github.io/) (Datawhaleé¦–å¸­ç§‘å­¦å®¶-æµ™æ±Ÿå¸ˆèŒƒå¤§å­¦æ­å·äººå·¥æ™ºèƒ½ç ”ç©¶é™¢æ•™æˆ)
### Extra-Chapter è´¡çŒ®è€…
- [WH](https://github.com/WHQAQ11) (å†…å®¹è´¡çŒ®è€…)
- [å‘¨å¥¥æ°-DWè´¡çŒ®è€…å›¢é˜Ÿ](https://github.com/thunderbolt-fire) (è¥¿å®‰äº¤é€šå¤§å­¦, Extra02 å†…å®¹è´¡çŒ®)
- [å¼ å®¸æ—­-ä¸ªäººå¼€å‘è€…](https://github.com/Tasselszcx)(å¸å›½ç†å·¥å­¦é™¢, Extra03 å†…å®¹è´¡çŒ®)
- [é»„å®æ™—-DWè´¡çŒ®è€…å›¢é˜Ÿ](https://github.com/XiaoMa-PM) (æ·±åœ³å¤§å­¦, Extra04 å†…å®¹è´¡çŒ®)

### ç‰¹åˆ«æ„Ÿè°¢
- æ„Ÿè°¢ [@Sm1les](https://github.com/Sm1les) å¯¹æœ¬é¡¹ç›®çš„å¸®åŠ©ä¸æ”¯æŒ
- æ„Ÿè°¢æ‰€æœ‰ä¸ºæœ¬é¡¹ç›®åšå‡ºè´¡çŒ®çš„å¼€å‘è€…ä»¬ â¤ï¸

&lt;div align=center style=&quot;margin-top: 30px;&quot;&gt;
  &lt;a href=&quot;https://github.com/datawhalechina/Hello-Agents/graphs/contributors&quot;&gt;
    &lt;img src=&quot;https://contrib.rocks/image?repo=datawhalechina/Hello-Agents&quot; /&gt;
  &lt;/a&gt;
&lt;/div&gt;

## Star History

&lt;div align=&#039;center&#039;&gt;
    &lt;img src=&quot;./docs/images/star-history-20251212.png&quot; alt=&quot;Datawhale&quot; width=&quot;90%&quot;&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼&lt;/p&gt;
&lt;/div&gt;

## å…³äº Datawhale

&lt;div align=&#039;center&#039;&gt;
    &lt;img src=&quot;./docs/images/datawhale.png&quot; alt=&quot;Datawhale&quot; width=&quot;30%&quot;&gt;
    &lt;p&gt;æ‰«æäºŒç»´ç å…³æ³¨ Datawhale å…¬ä¼—å·ï¼Œè·å–æ›´å¤šä¼˜è´¨å¼€æºå†…å®¹&lt;/p&gt;
&lt;/div&gt;

---

## ğŸ“œ å¼€æºåè®®

æœ¬ä½œå“é‡‡ç”¨[çŸ¥è¯†å…±äº«ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç›¸åŒæ–¹å¼å…±äº« 4.0 å›½é™…è®¸å¯åè®®](http://creativecommons.org/licenses/by-nc-sa/4.0/)è¿›è¡Œè®¸å¯ã€‚
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[karpathy/nanoGPT]]></title>
            <link>https://github.com/karpathy/nanoGPT</link>
            <guid>https://github.com/karpathy/nanoGPT</guid>
            <pubDate>Sun, 14 Dec 2025 00:05:00 GMT</pubDate>
            <description><![CDATA[The simplest, fastest repository for training/finetuning medium-sized GPTs.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/karpathy/nanoGPT">karpathy/nanoGPT</a></h1>
            <p>The simplest, fastest repository for training/finetuning medium-sized GPTs.</p>
            <p>Language: Python</p>
            <p>Stars: 51,012</p>
            <p>Forks: 8,546</p>
            <p>Stars today: 78 stars today</p>
            <h2>README</h2><pre>
# nanoGPT

![nanoGPT](assets/nanogpt.jpg)


---

**Update Nov 2025** nanoGPT has a new and improved cousin called [nanochat](https://github.com/karpathy/nanochat). It is very likely you meant to use/find nanochat instead. nanoGPT (this repo) is now very old and deprecated but I will leave it up for posterity.

---

The simplest, fastest repository for training/finetuning medium-sized GPTs. It is a rewrite of [minGPT](https://github.com/karpathy/minGPT) that prioritizes teeth over education. Still under active development, but currently the file `train.py` reproduces GPT-2 (124M) on OpenWebText, running on a single 8XA100 40GB node in about 4 days of training. The code itself is plain and readable: `train.py` is a ~300-line boilerplate training loop and `model.py` a ~300-line GPT model definition, which can optionally load the GPT-2 weights from OpenAI. That&#039;s it.

![repro124m](assets/gpt2_124M_loss.png)

Because the code is so simple, it is very easy to hack to your needs, train new models from scratch, or finetune pretrained checkpoints (e.g. biggest one currently available as a starting point would be the GPT-2 1.3B model from OpenAI).

## install

```
pip install torch numpy transformers datasets tiktoken wandb tqdm
```

Dependencies:

- [pytorch](https://pytorch.org) &lt;3
- [numpy](https://numpy.org/install/) &lt;3
-  `transformers` for huggingface transformers &lt;3 (to load GPT-2 checkpoints)
-  `datasets` for huggingface datasets &lt;3 (if you want to download + preprocess OpenWebText)
-  `tiktoken` for OpenAI&#039;s fast BPE code &lt;3
-  `wandb` for optional logging &lt;3
-  `tqdm` for progress bars &lt;3

## quick start

If you are not a deep learning professional and you just want to feel the magic and get your feet wet, the fastest way to get started is to train a character-level GPT on the works of Shakespeare. First, we download it as a single (1MB) file and turn it from raw text into one large stream of integers:

```sh
python data/shakespeare_char/prepare.py
```

This creates a `train.bin` and `val.bin` in that data directory. Now it is time to train your GPT. The size of it very much depends on the computational resources of your system:

**I have a GPU**. Great, we can quickly train a baby GPT with the settings provided in the [config/train_shakespeare_char.py](config/train_shakespeare_char.py) config file:

```sh
python train.py config/train_shakespeare_char.py
```

If you peek inside it, you&#039;ll see that we&#039;re training a GPT with a context size of up to 256 characters, 384 feature channels, and it is a 6-layer Transformer with 6 heads in each layer. On one A100 GPU this training run takes about 3 minutes and the best validation loss is 1.4697. Based on the configuration, the model checkpoints are being written into the `--out_dir` directory `out-shakespeare-char`. So once the training finishes we can sample from the best model by pointing the sampling script at this directory:

```sh
python sample.py --out_dir=out-shakespeare-char
```

This generates a few samples, for example:

```
ANGELO:
And cowards it be strawn to my bed,
And thrust the gates of my threats,
Because he that ale away, and hang&#039;d
An one with him.

DUKE VINCENTIO:
I thank your eyes against it.

DUKE VINCENTIO:
Then will answer him to save the malm:
And what have you tyrannous shall do this?

DUKE VINCENTIO:
If you have done evils of all disposition
To end his power, the day of thrust for a common men
That I leave, to fight with over-liking
Hasting in a roseman.
```

lol  `Â¯\_(ãƒ„)_/Â¯`. Not bad for a character-level model after 3 minutes of training on a GPU. Better results are quite likely obtainable by instead finetuning a pretrained GPT-2 model on this dataset (see finetuning section later).

**I only have a macbook** (or other cheap computer). No worries, we can still train a GPT but we want to dial things down a notch. I recommend getting the bleeding edge PyTorch nightly ([select it here](https://pytorch.org/get-started/locally/) when installing) as it is currently quite likely to make your code more efficient. But even without it, a simple train run could look as follows:

```sh
python train.py config/train_shakespeare_char.py --device=cpu --compile=False --eval_iters=20 --log_interval=1 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=2000 --dropout=0.0
```

Here, since we are running on CPU instead of GPU we must set both `--device=cpu` and also turn off PyTorch 2.0 compile with `--compile=False`. Then when we evaluate we get a bit more noisy but faster estimate (`--eval_iters=20`, down from 200), our context size is only 64 characters instead of 256, and the batch size only 12 examples per iteration, not 64. We&#039;ll also use a much smaller Transformer (4 layers, 4 heads, 128 embedding size), and decrease the number of iterations to 2000 (and correspondingly usually decay the learning rate to around max_iters with `--lr_decay_iters`). Because our network is so small we also ease down on regularization (`--dropout=0.0`). This still runs in about ~3 minutes, but gets us a loss of only 1.88 and therefore also worse samples, but it&#039;s still good fun:

```sh
python sample.py --out_dir=out-shakespeare-char --device=cpu
```
Generates samples like this:

```
GLEORKEN VINGHARD III:
Whell&#039;s the couse, the came light gacks,
And the for mought you in Aut fries the not high shee
bot thou the sought bechive in that to doth groan you,
No relving thee post mose the wear
```

Not bad for ~3 minutes on a CPU, for a hint of the right character gestalt. If you&#039;re willing to wait longer, feel free to tune the hyperparameters, increase the size of the network, the context length (`--block_size`), the length of training, etc.

Finally, on Apple Silicon Macbooks and with a recent PyTorch version make sure to add `--device=mps` (short for &quot;Metal Performance Shaders&quot;); PyTorch then uses the on-chip GPU that can *significantly* accelerate training (2-3X) and allow you to use larger networks. See [Issue 28](https://github.com/karpathy/nanoGPT/issues/28) for more.

## reproducing GPT-2

A more serious deep learning professional may be more interested in reproducing GPT-2 results. So here we go - we first tokenize the dataset, in this case the [OpenWebText](https://openwebtext2.readthedocs.io/en/latest/), an open reproduction of OpenAI&#039;s (private) WebText:

```sh
python data/openwebtext/prepare.py
```

This downloads and tokenizes the [OpenWebText](https://huggingface.co/datasets/openwebtext) dataset. It will create a `train.bin` and `val.bin` which holds the GPT2 BPE token ids in one sequence, stored as raw uint16 bytes. Then we&#039;re ready to kick off training. To reproduce GPT-2 (124M) you&#039;ll want at least an 8X A100 40GB node and run:

```sh
torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py
```

This will run for about 4 days using PyTorch Distributed Data Parallel (DDP) and go down to loss of ~2.85. Now, a GPT-2 model just evaluated on OWT gets a val loss of about 3.11, but if you finetune it it will come down to ~2.85 territory (due to an apparent domain gap), making the two models ~match.

If you&#039;re in a cluster environment and you are blessed with multiple GPU nodes you can make GPU go brrrr e.g. across 2 nodes like:

```sh
# Run on the first (master) node with example IP 123.456.123.456:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py
# Run on the worker node:
torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py
```

It is a good idea to benchmark your interconnect (e.g. iperf3). In particular, if you don&#039;t have Infiniband then also prepend `NCCL_IB_DISABLE=1` to the above launches. Your multinode training will work, but most likely _crawl_. By default checkpoints are periodically written to the `--out_dir`. We can sample from the model by simply `python sample.py`.

Finally, to train on a single GPU simply run the `python train.py` script. Have a look at all of its args, the script tries to be very readable, hackable and transparent. You&#039;ll most likely want to tune a number of those variables depending on your needs.

## baselines

OpenAI GPT-2 checkpoints allow us to get some baselines in place for openwebtext. We can get the numbers as follows:

```sh
$ python train.py config/eval_gpt2.py
$ python train.py config/eval_gpt2_medium.py
$ python train.py config/eval_gpt2_large.py
$ python train.py config/eval_gpt2_xl.py
```

and observe the following losses on train and val:

| model | params | train loss | val loss |
| ------| ------ | ---------- | -------- |
| gpt2 | 124M         | 3.11  | 3.12     |
| gpt2-medium | 350M  | 2.85  | 2.84     |
| gpt2-large | 774M   | 2.66  | 2.67     |
| gpt2-xl | 1558M     | 2.56  | 2.54     |

However, we have to note that GPT-2 was trained on (closed, never released) WebText, while OpenWebText is just a best-effort open reproduction of this dataset. This means there is a dataset domain gap. Indeed, taking the GPT-2 (124M) checkpoint and finetuning on OWT directly for a while reaches loss down to ~2.85. This then becomes the more appropriate baseline w.r.t. reproduction.

## finetuning

Finetuning is no different than training, we just make sure to initialize from a pretrained model and train with a smaller learning rate. For an example of how to finetune a GPT on new text go to `data/shakespeare` and run `prepare.py` to download the tiny shakespeare dataset and render it into a `train.bin` and `val.bin`, using the OpenAI BPE tokenizer from GPT-2. Unlike OpenWebText this will run in seconds. Finetuning can take very little time, e.g. on a single GPU just a few minutes. Run an example finetuning like:

```sh
python train.py config/finetune_shakespeare.py
```

This will load the config parameter overrides in `config/finetune_shakespeare.py` (I didn&#039;t tune them much though). Basically, we initialize from a GPT2 checkpoint with `init_from` and train as normal, except shorter and with a small learning rate. If you&#039;re running out of memory try decreasing the model size (they are `{&#039;gpt2&#039;, &#039;gpt2-medium&#039;, &#039;gpt2-large&#039;, &#039;gpt2-xl&#039;}`) or possibly decreasing the `block_size` (context length). The best checkpoint (lowest validation loss) will be in the `out_dir` directory, e.g. in `out-shakespeare` by default, per the config file. You can then run the code in `sample.py --out_dir=out-shakespeare`:

```
THEODORE:
Thou shalt sell me to the highest bidder: if I die,
I sell thee to the first; if I go mad,
I sell thee to the second; if I
lie, I sell thee to the third; if I slay,
I sell thee to the fourth: so buy or sell,
I tell thee again, thou shalt not sell my
possession.

JULIET:
And if thou steal, thou shalt not sell thyself.

THEODORE:
I do not steal; I sell the stolen goods.

THEODORE:
Thou know&#039;st not what thou sell&#039;st; thou, a woman,
Thou art ever a victim, a thing of no worth:
Thou hast no right, no right, but to be sold.
```

Whoa there, GPT, entering some dark place over there. I didn&#039;t really tune the hyperparameters in the config too much, feel free to try!

## sampling / inference

Use the script `sample.py` to sample either from pre-trained GPT-2 models released by OpenAI, or from a model you trained yourself. For example, here is a way to sample from the largest available `gpt2-xl` model:

```sh
python sample.py \
    --init_from=gpt2-xl \
    --start=&quot;What is the answer to life, the universe, and everything?&quot; \
    --num_samples=5 --max_new_tokens=100
```

If you&#039;d like to sample from a model you trained, use the `--out_dir` to point the code appropriately. You can also prompt the model with some text from a file, e.g. ```python sample.py --start=FILE:prompt.txt```.

## efficiency notes

For simple model benchmarking and profiling, `bench.py` might be useful. It&#039;s identical to what happens in the meat of the training loop of `train.py`, but omits much of the other complexities.

Note that the code by default uses [PyTorch 2.0](https://pytorch.org/get-started/pytorch-2.0/). At the time of writing (Dec 29, 2022) this makes `torch.compile()` available in the nightly release. The improvement from the one line of code is noticeable, e.g. cutting down iteration time from ~250ms / iter to 135ms / iter. Nice work PyTorch team!

## todos

- Investigate and add FSDP instead of DDP
- Eval zero-shot perplexities on standard evals (e.g. LAMBADA? HELM? etc.)
- Finetune the finetuning script, I think the hyperparams are not great
- Schedule for linear batch size increase during training
- Incorporate other embeddings (rotary, alibi)
- Separate out the optim buffers from model params in checkpoints I think
- Additional logging around network health (e.g. gradient clip events, magnitudes)
- Few more investigations around better init etc.

## troubleshooting

Note that by default this repo uses PyTorch 2.0 (i.e. `torch.compile`). This is fairly new and experimental, and not yet available on all platforms (e.g. Windows). If you&#039;re running into related error messages try to disable this by adding `--compile=False` flag. This will slow down the code but at least it will run.

For some context on this repository, GPT, and language modeling it might be helpful to watch my [Zero To Hero series](https://karpathy.ai/zero-to-hero.html). Specifically, the [GPT video](https://www.youtube.com/watch?v=kCc8FmEb1nY) is popular if you have some prior language modeling context.

For more questions/discussions feel free to stop by **#nanoGPT** on Discord:

[![](https://dcbadge.vercel.app/api/server/3zy8kqD9Cp?compact=true&amp;style=flat)](https://discord.gg/3zy8kqD9Cp)

## acknowledgements

All nanoGPT experiments are powered by GPUs on [Lambda labs](https://lambdalabs.com), my favorite Cloud GPU provider. Thank you Lambda labs for sponsoring nanoGPT!
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[GoogleCloudPlatform/agent-starter-pack]]></title>
            <link>https://github.com/GoogleCloudPlatform/agent-starter-pack</link>
            <guid>https://github.com/GoogleCloudPlatform/agent-starter-pack</guid>
            <pubDate>Sun, 14 Dec 2025 00:04:59 GMT</pubDate>
            <description><![CDATA[Ship AI Agents to Google Cloud in minutes, not months. Production-ready templates with built-in CI/CD, evaluation, and observability.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/GoogleCloudPlatform/agent-starter-pack">GoogleCloudPlatform/agent-starter-pack</a></h1>
            <p>Ship AI Agents to Google Cloud in minutes, not months. Production-ready templates with built-in CI/CD, evaluation, and observability.</p>
            <p>Language: Python</p>
            <p>Stars: 4,544</p>
            <p>Forks: 1,128</p>
            <p>Stars today: 227 stars today</p>
            <h2>README</h2><pre># ğŸš€ Agent Starter Pack

![Version](https://img.shields.io/pypi/v/agent-starter-pack?color=blue) [![1-Minute Video Overview](https://img.shields.io/badge/1--Minute%20Overview-gray)](https://youtu.be/jHt-ZVD660g) [![Docs](https://img.shields.io/badge/Documentation-gray)](https://googlecloudplatform.github.io/agent-starter-pack/) &lt;a href=&quot;https://studio.firebase.google.com/new?template=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fagent-starter-pack%2Ftree%2Fmain%2Fagent_starter_pack%2Fresources%2Fidx&quot;&gt;
  &lt;picture&gt;
    &lt;source
      media=&quot;(prefers-color-scheme: dark)&quot;
      srcset=&quot;https://cdn.firebasestudio.dev/btn/try_light_20.svg&quot;&gt;
    &lt;source
      media=&quot;(prefers-color-scheme: light)&quot;
      srcset=&quot;https://cdn.firebasestudio.dev/btn/try_dark_20.svg&quot;&gt;
    &lt;img
      height=&quot;20&quot;
      alt=&quot;Try in Firebase Studio&quot;
      src=&quot;https://cdn.firebasestudio.dev/btn/try_blue_20.svg&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt; [![Launch in Cloud Shell](https://img.shields.io/badge/Launch-in_Cloud_Shell-white)](https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Feliasecchig%2Fasp-open-in-cloud-shell&amp;cloudshell_print=open-in-cs) ![Stars](https://img.shields.io/github/stars/GoogleCloudPlatform/agent-starter-pack?color=yellow)

A Python package that provides **production-ready templates** for GenAI agents on Google Cloud.

Focus on your agent logicâ€”the starter pack provides everything else: infrastructure, CI/CD, observability, and security.

| âš¡ï¸ Launch | ğŸ§ª Experiment  | âœ… Deploy | ğŸ› ï¸ Customize |
|---|---|---|---|
| [Pre-built agent templates](./agent_starter_pack/agents/) (ReAct, RAG, multi-agent, Live API). | [Vertex AI evaluation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview) and an interactive playground. | Production-ready infra with [monitoring, observability](https://googlecloudplatform.github.io/agent-starter-pack/guide/observability), and [CI/CD](https://googlecloudplatform.github.io/agent-starter-pack/guide/deployment) on [Cloud Run](https://cloud.google.com/run) or [Agent Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview). | Extend and customize templates according to your needs. ğŸ†• Now integrating with [Gemini CLI](https://github.com/google-gemini/gemini-cli) |

---

## âš¡ Get Started in 1 Minute

**From zero to production-ready agent in 60 seconds using [`uv`](https://docs.astral.sh/uv/getting-started/installation/):**

```bash
uvx agent-starter-pack create
```

&lt;details&gt;
&lt;summary&gt; âœ¨ Alternative: Using pip&lt;/summary&gt;

If you don&#039;t have [`uv`](https://github.com/astral-sh/uv) installed, you can use pip:
```bash
# Create and activate a Python virtual environment
python -m venv .venv &amp;&amp; source .venv/bin/activate

# Install the agent starter pack
pip install --upgrade agent-starter-pack

# Create a new agent project
agent-starter-pack create
```
&lt;/details&gt;

**That&#039;s it!** You now have a fully functional agent projectâ€”complete with backend, frontend, and deployment infrastructureâ€”ready for you to explore and customize.

### ğŸ”§ Enhance Existing Agents

Already have an agent? Add production-ready deployment and infrastructure by running this command in your project&#039;s root folder:

```bash
uvx agent-starter-pack enhance
```

See [Installation Guide](https://googlecloudplatform.github.io/agent-starter-pack/guide/installation) for more options, or try with zero setup in [Firebase Studio](https://studio.firebase.google.com/new?template=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fagent-starter-pack%2Ftree%2Fmain%2Fsrc%2Fresources%2Fidx) or [Cloud Shell](https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Feliasecchig%2Fasp-open-in-cloud-shell&amp;cloudshell_print=open-in-cs).

---

## ğŸ¤– Agents

| Agent Name                  | Description                                                                                                                       |
|-----------------------------|-----------------------------------------------------------------------------------------------------------------------------------|
| `adk_base`      | A base ReAct agent implemented using Google&#039;s [Agent Development Kit](https://github.com/google/adk-python) |
| `adk_a2a_base`  | An ADK agent with [Agent2Agent (A2A) Protocol](https://a2a-protocol.org/) support for distributed agent communication and interoperability |
| `agentic_rag` | A RAG agent for document retrieval and Q&amp;A. Supporting [Vertex AI Search](https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction) and [Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview).       |
| `langgraph_base`      | A base ReAct agent implemented using LangChain&#039;s [LangGraph](https://github.com/langchain-ai/langgraph) |
| `adk_live`       | A real-time multimodal RAG agent powered by Gemini, supporting audio/video/text chat     |

**More agents are on the way!** We are continuously expanding our [agent library](https://googlecloudplatform.github.io/agent-starter-pack/agents/overview). Have a specific agent type in mind? [Raise an issue as a feature request!](https://github.com/GoogleCloudPlatform/agent-starter-pack/issues/new?labels=enhancement)

**ğŸ” ADK Samples**

Looking to explore more ADK examples? Check out the [ADK Samples Repository](https://github.com/google/adk-samples) for additional examples and use cases demonstrating ADK&#039;s capabilities.

---

## ğŸŒŸ Community Showcase

Explore amazing projects built with the Agent Starter Pack! 

**[View Community Showcase â†’](https://googlecloudplatform.github.io/agent-starter-pack/guide/community-showcase)**

## Key Features

The `agent-starter-pack` offers key features to accelerate and simplify the development of your agent:
- **ğŸ”„ [CI/CD Automation](https://googlecloudplatform.github.io/agent-starter-pack/cli/setup_cicd)** - A single command to set up a complete CI/CD pipeline for all environments, supporting both **Google Cloud Build** and **GitHub Actions**.
- **ğŸ“¥ [Data Pipeline for RAG with Terraform/CI-CD](https://googlecloudplatform.github.io/agent-starter-pack/guide/data-ingestion)** - Seamlessly integrate a data pipeline to process embeddings for RAG into your agent system. Supporting [Vertex AI Search](https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction) and [Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview).
- **[Remote Templates](docs/guide/remote-templating.md)**: Create and share your own agent starter packs templates from any Git repository.
- **ğŸ¤– Gemini CLI Integration** - Use the [Gemini CLI](https://github.com/google-gemini/gemini-cli) and the included `GEMINI.md` context file to ask questions about your template, agent architecture, and the path to production. Get instant guidance and code examples directly in your terminal.

## High-Level Architecture

This starter pack covers all aspects of Agent development, from prototyping and evaluation to deployment and monitoring.

![High Level Architecture](docs/images/ags_high_level_architecture.png &quot;Architecture&quot;)

---

## ğŸ”§ Requirements

- Python 3.10+
- [Google Cloud SDK](https://cloud.google.com/sdk/docs/install)
- [Terraform](https://developer.hashicorp.com/terraform/downloads) (for deployment)
- [Make](https://www.gnu.org/software/make/) (for development tasks)


## ğŸ“š Documentation

Visit our [documentation site](https://googlecloudplatform.github.io/agent-starter-pack/) for comprehensive guides and references!

ğŸ” **New to the codebase?** Explore the [CodeWiki](https://codewiki.google/github.com/googlecloudplatform/agent-starter-pack) for AI-powered code understanding and navigation.

- [Getting Started Guide](https://googlecloudplatform.github.io/agent-starter-pack/guide/getting-started) - First steps with agent-starter-pack
- [Installation Guide](https://googlecloudplatform.github.io/agent-starter-pack/guide/installation) - Setting up your environment
- [Deployment Guide](https://googlecloudplatform.github.io/agent-starter-pack/guide/deployment) - Taking your agent to production
- [Agent Templates Overview](https://googlecloudplatform.github.io/agent-starter-pack/agents/overview) - Explore available agent patterns
- [CLI Reference](https://googlecloudplatform.github.io/agent-starter-pack/cli/) - Command-line tool documentation


### Video Walkthrough:

- **[Exploring the Agent Starter Pack](https://www.youtube.com/watch?v=9zqwym-N3lg)**: A comprehensive tutorial demonstrating how to rapidly deploy AI Agents using the Agent Starter Pack, covering architecture, templates, and step-by-step deployment.

- **[6-minute introduction](https://www.youtube.com/live/eZ-8UQ_t4YM?feature=shared&amp;t=2791)** (April 2024): Explaining the Agent Starter Pack and demonstrating its key features. Part of the Kaggle GenAI intensive course.

Looking for more examples and resources for Generative AI on Google Cloud? Check out the [GoogleCloudPlatform/generative-ai](https://github.com/GoogleCloudPlatform/generative-ai) repository for notebooks, code samples, and more!

## Contributing

Contributions are welcome! See the [Contributing Guide](CONTRIBUTING.md).

## Feedback

We value your input! Your feedback helps us improve this starter pack and make it more useful for the community.

### Getting Help

If you encounter any issues or have specific suggestions, please first consider [raising an issue](https://github.com/GoogleCloudPlatform/generative-ai/issues) on our GitHub repository.

### Share Your Experience

For other types of feedback, or if you&#039;d like to share a positive experience or success story using this starter pack, we&#039;d love to hear from you! You can reach out to us at &lt;a href=&quot;mailto:agent-starter-pack@google.com&quot;&gt;agent-starter-pack@google.com&lt;/a&gt;.

Thank you for your contributions!

## Disclaimer

This repository is for demonstrative purposes only and is not an officially supported Google product.

## Terms of Service

The agent-starter-pack templating CLI and the templates in this starter pack leverage Google Cloud APIs. When you use this starter pack, you&#039;ll be deploying resources in your own Google Cloud project and will be responsible for those resources. Please review the [Google Cloud Service Terms](https://cloud.google.com/terms/service-terms) for details on the terms of service associated with these APIs.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[thinking-machines-lab/tinker-cookbook]]></title>
            <link>https://github.com/thinking-machines-lab/tinker-cookbook</link>
            <guid>https://github.com/thinking-machines-lab/tinker-cookbook</guid>
            <pubDate>Sun, 14 Dec 2025 00:04:58 GMT</pubDate>
            <description><![CDATA[Post-training with Tinker]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/thinking-machines-lab/tinker-cookbook">thinking-machines-lab/tinker-cookbook</a></h1>
            <p>Post-training with Tinker</p>
            <p>Language: Python</p>
            <p>Stars: 2,416</p>
            <p>Forks: 225</p>
            <p>Stars today: 43 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;Tinker Cookbook&lt;/h1&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/tinker-cover.png&quot; width=&quot;60%&quot; /&gt;
&lt;/div&gt;

We provide two libraries for the broader community to customize their language models: `tinker` and `tinker-cookbook`.

- `tinker` is a training SDK for researchers and developers to fine-tune language models. You send API requests to us and we handle the complexities of distributed training.
- `tinker-cookbook` includes realistic examples of fine-tuning language models. It builds on the Tinker API and provides common abstractions to fine-tune language models.

## Installation

1. Sign up for Tinker through the [waitlist](https://thinkingmachines.ai/tinker).
2. Once you have access, create an API key from the [console](https://tinker-console.thinkingmachines.ai) and export it as environment variable `TINKER_API_KEY`.
3. Install tinker python client via `pip install tinker`
4. We recommend installing `tinker-cookbook` in a virtual env either with `conda` or `uv`. For running most examples, you can install via `pip install -e .`.

## Tinker

Refer to the [docs](https://tinker-docs.thinkingmachines.ai/training-sampling) to start from basics.
Here we introduce a few Tinker primitives - the basic components to fine-tune LLMs:

```python
import tinker
service_client = tinker.ServiceClient()
training_client = service_client.create_lora_training_client(
  base_model=&quot;meta-llama/Llama-3.2-1B&quot;, rank=32,
)
training_client.forward_backward(...)
training_client.optim_step(...)
training_client.save_state(...)
training_client.load_state(...)

sampling_client = training_client.save_weights_and_get_sampling_client(name=&quot;my_model&quot;)
sampling_client.sample(...)
```

See [tinker_cookbook/recipes/sl_loop.py](tinker_cookbook/recipes/sl_loop.py) and [tinker_cookbook/recipes/rl_loop.py](tinker_cookbook/recipes/rl_loop.py) for minimal examples of using these primitives to fine-tune LLMs.

To download the weights of any model:
```python
rest_client = service_client.create_rest_client()
future = rest_client.get_checkpoint_archive_url_from_tinker_path(sampling_client.model_path)
with open(f&quot;model-checkpoint.tar.gz&quot;, &quot;wb&quot;) as f:
    f.write(future.result())
```

### Tinker Cookbook

Besides these primitives, we also offer **Tinker Cookbook** (a.k.a. this repo), a library of a wide range of abstractions to help you customize training environments.
[`tinker_cookbook/recipes/sl_basic.py`](tinker_cookbook/recipes/sl_basic.py) and [`tinker_cookbook/recipes/rl_basic.py`](tinker_cookbook/recipes/rl_basic.py) contain minimal examples to configure supervised learning and reinforcement learning.

We also include a wide range of more sophisticated examples in the [`tinker_cookbook/recipes/`](tinker_cookbook/recipes/) folder:
1. **[Chat supervised learning](tinker_cookbook/recipes/chat_sl/)**: supervised fine-tuning on conversational datasets like Tulu3.
2. **[Math reasoning](tinker_cookbook/recipes/math_rl/)**: improve LLM reasoning capability by rewarding it for answering math questions correctly.
3. **[Preference learning](tinker_cookbook/recipes/preference/)**: showcase a three-stage RLHF pipeline: 1) supervised fine-tuning, 2) learning a reward model, 3) RL against the reward model.
4. **[Tool use](tinker_cookbook/recipes/tool_use/)**: train LLMs to better use retrieval tools to answer questions more accurately.
5. **[Prompt distillation](tinker_cookbook/recipes/prompt_distillation/)**: internalize long and complex instructions into LLMs.
6. **[Multi-Agent](tinker_cookbook/recipes/multiplayer_rl/)**: optimize LLMs to play against another LLM or themselves.

These examples are located in each subfolder, and their `README.md` files will walk you through the key implementation details, the commands to run them, and the expected performance.

### Import our utilities

Tinker cookbook includes several utilities. Here&#039;s a quick overview:
- [`renderers`](tinker_cookbook/renderers.py) converts tokens from/to structured chat message objects
- [`hyperparam_utils`](tinker_cookbook/hyperparam_utils.py) helps calculate hyperparameters suitable for LoRAs
- [`evaluation`](tinker_cookbook/eval/evaluators.py) provides abstractions for evaluating Tinker models and [`inspect_evaluation`](tinker_cookbook/eval/inspect_evaluators.py) shows how to integrate with InspectAI to make evaluating on standard benchmarks easy.

## Contributing

This project is built in the spirit of open science and collaborative development. We believe that the best tools emerge through community involvement and shared learning.

We welcome PR contributions after our private beta is over. If you have any feedback, please email us at tinker@thinkingmachines.ai.

## Citation
If you use Tinker for your research, please cite it as:
```
Thinking Machines Lab, 2025. Tinker. https://thinkingmachines.ai/tinker/.
```

Or use this BibTeX citation:
```
@misc{tml2025tinker,
  author = {Thinking Machines Lab},
  title = {Tinker},
  year = {2025},
  url = {https://thinkingmachines.ai/tinker/},
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[TEN-framework/ten-framework]]></title>
            <link>https://github.com/TEN-framework/ten-framework</link>
            <guid>https://github.com/TEN-framework/ten-framework</guid>
            <pubDate>Sun, 14 Dec 2025 00:04:57 GMT</pubDate>
            <description><![CDATA[Open-source framework for conversational voice AI agents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/TEN-framework/ten-framework">TEN-framework/ten-framework</a></h1>
            <p>Open-source framework for conversational voice AI agents</p>
            <p>Language: Python</p>
            <p>Stars: 9,158</p>
            <p>Forks: 1,065</p>
            <p>Stars today: 33 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; id=&quot;readme-top&quot;&gt;

![Image][ten-framework-banner]

[![TEN Releases][ten-releases-badge]][ten-releases]
[![Coverage Status][coverage-badge]][coverage]
[![Release Date][release-date-badge]][ten-releases]
[![Commits][commits-badge]][commit-activity]
[![Issues closed][issues-closed-badge]][issues-closed]
[![Contributors][contributors-badge]][contributors]
[![GitHub license][license-badge]][license]
[![Ask DeepWiki][deepwiki-badge]][deepwiki]
[![ReadmeX][readmex-badge]][readmex]

[![README in English][lang-en-badge]][lang-en-readme]
[![ç®€ä½“ä¸­æ–‡æ“ä½œæŒ‡å—][lang-zh-badge]][lang-zh-readme]
[![æ—¥æœ¬èªã®README][lang-jp-badge]][lang-jp-readme]
[![README in í•œêµ­ì–´][lang-kr-badge]][lang-kr-readme]
[![README en EspaÃ±ol][lang-es-badge]][lang-es-readme]
[![README en FranÃ§ais][lang-fr-badge]][lang-fr-readme]
[![README in Italiano][lang-it-badge]][lang-it-readme]

[![TEN-framework%2Ften_framework | Trendshift][trendshift-badge]][trendshift]

[Official Site][official-site] â€¢
[Documentation][documentation] â€¢
[Blog][blog]

&lt;/div&gt;

&lt;br&gt;

&lt;details open&gt;
  &lt;summary&gt;&lt;kbd&gt;Table of Contents&lt;/kbd&gt;&lt;/summary&gt;

  &lt;br&gt;

- [Welcome to TEN][welcome-to-ten]
- [Agent Examples][agent-examples-section]
- [Quick Start with Agent Examples][quick-start]
  - [Localhost][localhost-section]
  - [Codespaces][codespaces-section]
- [Agent Examples Self-Hosting][agent-examples-self-hosting]
  - [Deploying with Docker][deploying-with-docker]
  - [Deploying with other cloud services][deploying-with-other-cloud-services]
- [Stay Tuned][stay-tuned]
- [TEN Ecosystem][ten-ecosystem-anchor]
- [Questions][questions]
- [Contributing][contributing]
  - [Code Contributors][code-contributors]
  - [Contribution Guidelines][contribution-guidelines]
  - [License][license-section]

&lt;br/&gt;

&lt;/details&gt;

## Welcome to TEN

TEN is an open-source framework for real-time multimodal conversational AI.

[TEN Ecosystem][ten-ecosystem-anchor] includes [TEN Framework][ten-framework], [Agent Examples][agent-examples-repo], [VAD][ten-vad], [Turn Detection][ten-turn-detection] and [Portal][ten-portal].

&lt;br&gt;

| Community Channel | Purpose |
| ---------------- | ------- |
| [![Follow on X][follow-on-x-badge]][follow-on-x] | Follow TEN Framework on X for updates and announcements |
| [![Discord TEN Community][discord-badge]][discord-invite] | Join our Discord community to connect with developers |
| [![Follow on LinkedIn][linkedin-badge]][linkedin] | Follow TEN Framework on LinkedIn for updates and announcements |
| [![Hugging Face Space][hugging-face-badge]][hugging-face] | Join our Hugging Face community to explore our spaces and models |
| [![WeChat][wechat-badge]][wechat-discussion] | Join our WeChat group for Chinese community discussions |

&lt;br&gt;

## Agent Examples

&lt;br&gt;

![Image][voice-assistant-image]

&lt;strong&gt;Multi-Purpose Voice Assistant&lt;/strong&gt; â€” This low-latency, high-quality real-time assistant supports both RTC and [WebSocket][websocket-example] connections, and you can extend it with [Memory][memory-example], [VAD][voice-assistant-vad-example], [Turn Detection][voice-assistant-turn-detection-example], and other extensions.

See the [Example code][voice-assistant-example] for more details.

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

&lt;br&gt;

![Image][lip-sync-image]


&lt;strong&gt;Lip Sync Avatars&lt;/strong&gt; â€” Works with multiple avatar vendors, the main character features Kei, an anime character with MotionSync-powered lip sync, and also supports realistic avatars from Trulience, HeyGen, and Tavus.

See the [Example code][voice-assistant-live2d-example] for different Live2D characters.

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

&lt;br&gt;

![Image][speech-diarization-image]

&lt;strong&gt;Speech Diarization&lt;/strong&gt; â€” Real-time diarization that detects and labels speakers, the Who Likes What game shows an interactive use case.

[Example code][speechmatics-diarization-example]

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

&lt;br&gt;

![Image][sip-call-image]

&lt;strong&gt;SIP Call&lt;/strong&gt; â€” SIP extension that enables phone calls powered by TEN.

[Example code][voice-assistant-sip-example]

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

&lt;br&gt;

![Image][transcription-image]

&lt;strong&gt;Transcription&lt;/strong&gt; â€” A transcription tool that transcribes audio to text.

[Example code][transcription-example]

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

&lt;br&gt;

![Image][esp32-image]

&lt;strong&gt;ESP32-S3 Korvo V3&lt;/strong&gt; â€” Runs TEN agent example on the Espressif ESP32-S3 Korvo V3 development board to integrate LLM-powered communication with hardware.

See the [integration guide][esp32-guide] for more details.

&lt;br&gt;
&lt;div align=&quot;right&quot;&gt;

[![][back-to-top]][readme-top]

&lt;/div&gt;

## Quick Start with Agent Examples

### Localhost

#### Step â“µ - Prerequisites

| Category | Requirements |
| --- | --- |
| **Keys** | â€¢ Agora [App ID][agora-app-id] and [App Certificate][agora-app-certificate]&lt;br&gt;â€¢ [OpenAI][openai-api] API key&lt;br&gt;â€¢ [Deepgram][deepgram] ASR &lt;br&gt;â€¢ [ElevenLabs][elevenlabs] TTS  |
| **Installation** | â€¢ [Docker][docker] / [Docker Compose][docker-compose]&lt;br&gt;â€¢ [Node.js (LTS) v18][nodejs] |
| **Minimum System Requirements** | â€¢ CPU &gt;= 2 cores&lt;br&gt;â€¢ RAM &gt;= 4 GB |

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

&lt;!-- &gt; [!NOTE]
&gt; **macOS: Docker setting on Apple Silicon**
&gt;
&gt; Uncheck &quot;Use Rosetta for x86/amd64 emulation&quot; in Docker settings, it may result in slower build times on ARM, but performance will be normal when deployed to x64 servers. --&gt;

#### Step â“¶ - Build agent examples in VM

##### 1. Clone the repo, `cd` into `ai_agents`, and create a `.env` file from `.env.example`

```bash
cd ai_agents
cp ./.env.example ./.env
```

##### 2. Set up the Agora App ID and App Certificate in `.env`

```bash
AGORA_APP_ID=
AGORA_APP_CERTIFICATE=

# Deepgram (required for speech-to-text)
DEEPGRAM_API_KEY=

# OpenAI (required for language model)
OPENAI_API_KEY=

# ElevenLabs (required for text-to-speech)
ELEVENLABS_TTS_KEY=
```

##### 3. Start agent development containers

```bash
docker compose up -d
```

##### 4. Enter the container

```bash
docker exec -it ten_agent_dev bash
```

##### 5. Build the agent with the default example (~5-8 min)

Check the `agents/examples` folder for additional samples.
Start with one of these defaults:

```bash
# use the chained voice assistant
cd agents/examples/voice-assistant

# or use the speech-to-speech voice assistant in real time
cd agents/examples/voice-assistant-realtime
```

##### 6. Start the web server

Run `task build` if you changed any local source code. This step is required for compiled languages (for example, TypeScript or Go) and not needed for Python.

```bash
task install
task run
```

##### 7. Access the agent

Once the agent example is running, you can access the following interfaces:

| **localhost:49483** | **localhost:3000** |
| :-----------------: | :----------------: |
| ![Screenshot 1][localhost-49483-image] | ![Screenshot 2][localhost-3000-image] |

- TMAN Designer: [localhost:49483][localhost-49483]
- Agent Examples UI: [localhost:3000][localhost-3000]

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

#### Step â“· - Customize your agent example

1. Open [localhost:49483][localhost-49483].
2. Right-click the STT, LLM, and TTS extensions.
3. Open their properties and enter the corresponding API keys.
4. Submit your changes, now you can see the updated Agent Example in [localhost:3000][localhost-3000].

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

&lt;br&gt;

#### Run a transcriber app from TEN Manager without Docker (Beta)

TEN also provides a transcriber app that you can run from TEN Manager without using Docker.

Check the [quick start guide][quick-start-guide-ten-manager] for more details.

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

&lt;br&gt;

### Codespaces

GitHub offers free Codespaces for each repository. You can run Agent Examples in Codespaces without using Docker. Codespaces typically start faster than local Docker environments.

[![][codespaces-shield]][codespaces-new]

Check out [this guide][codespaces-guide] for more details.

&lt;div align=&quot;right&quot;&gt;

[![][back-to-top]][readme-top]

&lt;/div&gt;

&lt;br&gt;

## Agent Examples Self-Hosting

### Deploying with Docker

Once you have customized your agent (either by using the TMAN Designer or editing `property.json` directly), you can deploy it by creating a release Docker image for your service.

##### Release as Docker image

**Note**: The following commands need to be executed outside of any Docker container.

###### Build image

```bash
cd ai_agents
docker build -f agents/examples/&lt;example-name&gt;/Dockerfile -t example-app .
```

###### Run

```bash
docker run --rm -it --env-file .env -p 3000:3000 example-app
```

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

### Deploying with other cloud services

You can split the deployment into two pieces when you want to host TEN on providers such as [Vercel][vercel] or [Netlify][netlify].

1. Run the TEN backend on any container-friendly platform (a VM with Docker, Fly.io, Render, ECS, Cloud Run, or similar). Use the example Docker image without modifying it and expose port `8080` from that service.

2. Deploy only the frontend to Vercel or Netlify. Point the project root to `ai_agents/agents/examples/&lt;example&gt;/frontend`, run `pnpm install` (or `bun install`) followed by `pnpm build` (or `bun run build`), and keep the default `.next` output directory.

3. Configure environment variables in your hosting dashboard so that `AGENT_SERVER_URL` points to the backend URL, and add any `NEXT_PUBLIC_*` keys the UI needs (for example, Agora credentials you surface to the browser).

4. Ensure your backend accepts requests from the frontend origin â€” via open CORS or by using the built-in proxy middleware.

With this setup, the backend handles long-running worker processes, while the hosted frontend simply forwards API traffic to it.

&lt;div align=&quot;right&quot;&gt;

[![][back-to-top]][readme-top]

&lt;/div&gt;

&lt;br&gt;

## Stay Tuned

Get instant notifications for new releases and updates. Your support helps us grow and improve TEN!

&lt;br&gt;

![Image][stay-tuned-image]

&lt;br&gt;
&lt;div align=&quot;right&quot;&gt;

[![][back-to-top]][readme-top]

&lt;/div&gt;

&lt;br&gt;

## TEN Ecosystem

&lt;br&gt;

| Project | Preview |
| ------- | ------- |
| [**ï¸TEN Framework**][ten-framework-link]&lt;br&gt;Open-source framework for conversational AI Agents.&lt;br&gt;&lt;br&gt;![][ten-framework-shield] | ![][ten-framework-banner] |
| [**TEN VAD**][ten-vad-link]&lt;br&gt;Low-latency, lightweight and high-performance streaming voice activity detector (VAD).&lt;br&gt;&lt;br&gt;![][ten-vad-shield] | ![][ten-vad-banner] |
| [**ï¸ TEN Turn Detection**][ten-turn-detection-link]&lt;br&gt;TEN Turn Detection enables full-duplex dialogue communication.&lt;br&gt;&lt;br&gt;![][ten-turn-detection-shield] | ![][ten-turn-detection-banner] |
| [**TEN Agent Examples**][ten-agent-example-link]&lt;br&gt;Usecases powered by TEN.&lt;br&gt;&lt;br&gt; | ![][ten-agent-example-banner] |
| [**TEN Portal**][ten-portal-link]&lt;br&gt;The official site of the TEN Framework with documentation and a blog.&lt;br&gt;&lt;br&gt;![][ten-portal-shield] | ![][ten-portal-banner] |

&lt;br&gt;
&lt;div align=&quot;right&quot;&gt;

[![][back-to-top]][readme-top]

&lt;/div&gt;

&lt;br&gt;

## Questions

TEN Framework is available on these AI-powered Q&amp;A platforms. They can help you find answers quickly and accurately in multiple languages, covering everything from basic setup to advanced implementation details.

| Service | Link |
| ------- | ---- |
| DeepWiki | [![Ask DeepWiki][deepwiki-badge]][deepwiki] |
| ReadmeX | [![ReadmeX][readmex-badge]][readmex] |

&lt;br&gt;
&lt;div align=&quot;right&quot;&gt;

[![][back-to-top]][readme-top]

&lt;/div&gt;

## Contributing

We welcome all forms of open-source collaboration! Whether you&#039;re fixing bugs, adding features, improving documentation, or sharing ideas, your contributions help advance personalized AI tools. Check out our GitHub Issues and Projects to find ways to contribute and show your skills. Together, we can build something amazing!

&lt;br&gt;

&gt; [!TIP]
&gt;
&gt; **Welcome all kinds of contributions** ğŸ™
&gt;
&gt; Join us in building TEN better! Every contribution makes a difference, from code to documentation. Share your TEN Agent projects on social media to inspire others!
&gt;
&gt; Connect with one of the TEN maintainers [@elliotchen200][elliotchen200-x] on ğ• or [@cyfyifanchen][cyfyifanchen-github] on GitHub for project updates, discussions, and collaboration opportunities.

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

### Code Contributors

[![TEN][contributors-image]][contributors]

### Contribution Guidelines

Contributions are welcome! Please read the [contribution guidelines][contribution-guidelines-doc] first.

&lt;br&gt;

![divider][divider-light]
![divider][divider-dark]

### License

1. The entire TEN framework (except for the folders explicitly listed below) is released under the Apache License, Version 2.0, with additional restrictions. For details, please refer to the [LICENSE][license-file] file located in the root directory of the TEN framework.

2. The components within the `packages` directory are released under the Apache License, Version 2.0. For details, please refer to the `LICENSE` file located in each package&#039;s root directory.

3. The third-party libraries used by the TEN framework are listed and described in detail. For more information, please refer to the [third_party][third-party-folder] folder.

&lt;div align=&quot;right&quot;&gt;

[![][back-to-top]][readme-top]

&lt;/div&gt;

[back-to-top]: https://img.shields.io/badge/-Back_to_top-gray?style=flat-square
[readme-top]: #readme-top

&lt;!-- Navigation --&gt;
[welcome-to-ten]: #welcome-to-ten
[agent-examples-section]: #agent-examples
[quick-start]: #quick-start-with-agent-examples
[localhost-section]: #localhost
[codespaces-section]: #codespaces
[agent-examples-self-hosting]: #agent-examples-self-hosting
[deploying-with-docker]: #deploying-with-docker
[deploying-with-other-cloud-services]: #deploying-with-other-cloud-services
[stay-tuned]: #stay-tuned
[ten-ecosystem-anchor]: #ten-ecosystem
[questions]: #questions
[contributing]: #contributing
[code-contributors]: #code-contributors
[contribution-guidelines]: #contribution-guidelines
[license-section]: #license

&lt;!-- Header badges --&gt;
[ten-releases-badge]: https://img.shields.io/github/v/release/ten-framework/ten-framework?color=369eff&amp;labelColor=gray&amp;logo=github&amp;style=flat-square
[ten-releases]: https://github.com/TEN-framework/ten-framework/releases
[coverage-badge]: https://coveralls.io/repos/github/TEN-framework/ten-framework/badge.svg?branch=main
[coverage]: https://coveralls.io/github/TEN-framework/ten-framework?branch=main
[release-date-badge]: https://img.shields.io/github/release-date/ten-framework/ten-framework?labelColor=gray&amp;style=flat-square
[commits-badge]: https://img.shields.io/github/commit-activity/m/TEN-framework/ten-framework?labelColor=gray&amp;color=pink
[commit-activity]: https://github.com/TEN-framework/ten-framework/graphs/commit-activity
[issues-closed-badge]: https://img.shields.io/github/issues-search?query=repo%3ATEN-framework%2Ften-framework%20is%3Aclosed&amp;label=issues%20closed&amp;labelColor=gray&amp;color=green
[issues-closed]: https://github.com/TEN-framework/ten-framework/issues
[contributors-badge]: https://img.shields.io/github/contributors/ten-framework/ten-framework?color=c4f042&amp;labelColor=gray&amp;style=flat-square
[contributors]: https://github.com/TEN-framework/ten-framework/graphs/contributors
[license-badge]: https://img.shields.io/badge/License-Apache_2.0_with_certain_conditions-blue.svg?labelColor=%20%23155EEF&amp;color=%20%23528bff
[license]: https://github.com/TEN-framework/ten-framework/blob/main/LICENSE
[deepwiki-badge]: https://deepwiki.com/badge.svg
[deepwiki]: https://deepwiki.com/TEN-framework/TEN-framework
[readmex-badge]: https://raw.githubusercontent.com/CodePhiliaX/resource-trusteeship/main/readmex.svg
[readmex]: https://readmex.com/TEN-framework/ten-framework
[trendshift-badge]: https://trendshift.io/api/badge/repositories/11978
[trendshift]: https://trendshift.io/repositories/11978

&lt;!-- Localized READMEs --&gt;
[lang-en-badge]: https://img.shields.io/badge/English-lightgrey
[lang-en-readme]: https://github.com/TEN-framework/ten-framework/blob/main/README.md
[lang-zh-badge]: https://img.shields.io/badge/ç®€ä½“ä¸­æ–‡-lightgrey
[lang-zh-readme]: https://github.com/TEN-framework/ten-framework/blob/main/docs/README-CN.md
[lang-jp-badge]: https://img.shields.io/badge/æ—¥æœ¬èª-lightgrey
[lang-jp-readme]: https://github.com/TEN-framework/ten-framework/blob/main/docs/README-JP.md
[lang-kr-badge]: https://img.shields.io/badge/í•œêµ­ì–´-lightgrey
[lang-kr-readme]: https://github.com/TEN-framework/ten-framework/blob/main/docs/README-KR.md
[lang-es-badge]: https://img.shields.io/badge/EspaÃ±ol-lightgrey
[lang-es-readme]: https://github.com/TEN-framework/ten-framework/blob/main/docs/README-ES.md
[lang-fr-badge]: https://img.shields.io/badge/FranÃ§ais-lightgrey
[lang-fr-readme]: https://github.com/TEN-framework/ten-framework/blob/main/docs/README-FR.md
[lang-it-badge]: https://img.shields.io/badge/Italiano-lightgrey
[lang-it-readme]: https://github.com/TEN-framework/ten-framework/blob/main/docs/README-IT.md

&lt;!-- Primary sites --&gt;
[official-site]: https://theten.ai
[documentation]: https://theten.ai/docs
[blog]: https://theten.ai/blog

&lt;!-- Welcome --&gt;
[ten-framework]: https://github.com/ten-framework/ten-framework
[agent-examples-repo]: https://github.com/TEN-framework/ten-framework/tree/main/ai_agents/agents/examples
[ten-vad]: https://github.com/ten-framework/ten-vad
[ten-turn-detection]: https://github.com/ten-framework/ten-turn-detection
[ten-portal]: https://github.com/ten-framework/portal

&lt;!-- Community --&gt;
[follow-on-x-badge]: https://img.shields.io/twitter/follow/TenFramework?logo=X&amp;color=%20%23f5f5f5
[follow-on-x]: https://twitter.com/intent/follow?screen_name=TenFramework
[discord-badge]: https://img.shields.io/badge/Discord-Join%20TEN%20Community-5865F2?style=flat&amp;logo=discord&amp;logoColor=white
[discord-invite]: https://discord.gg/VnPftUzAMJ
[linkedin-badge]: https://custom-icon-badges.demolab.com/badge/LinkedIn-TEN_Framework-0A66C2?logo=linkedin-white&amp;logoColor=fff
[linkedin]: https://www.linkedin.com/company/ten-framework
[hugging-face-badge]: https://img.shields.io/badge/Hugging%20Face-TEN%20Framework-yellow?style=flat&amp;logo=huggingface
[hugging-face]: https://huggingface.co/TEN-framework
[wechat-badge]: https://img.shields.io/badge/TEN_Framework-WeChat_Group-%2307C160?logo=wechat&amp;labelColor=darkgreen&amp;color=gray
[wechat-discussion]: https://github.com/TEN-framework/ten-agent/discussions/170

&lt;!-- Agent examples --&gt;
[voice-assistant-image]: https://github.com/user-attachments/assets/dce3db80-fb48-4e2a-8ac7-33f50bcffa32
[websocket-example]: ai_agents/agents/examples/websocket-example
[memory-example]: ai_agents/agents/examples/voice-assistant-with-memU
[voice-assistant-vad-example]: ai_agents/agents/examples/voice-assistant-with-ten-vad
[voice-assistant-turn-detection-example]: ai_agents/agents/examples/voice-assistant-with-turn-detection
[voice-assistant-example]: ai_agents/agents/examples/voice-assistant
[divider-light]: https://github.com/user-attachments/assets/aec54c94-ced9-4683-ae58-0a5a7ed803bd#gh-light-mode-only
[divider-dark]: https://github.com/user-attachments/assets/d57fad08-4f49-4a1c-bdfc-f659a5d86150#gh-dark-mode-only
[lip-sync-image]: https://github.com/user-attachments/assets/51ab1504-b67c-49d4-8a7a-5582d9b254da
[voice-assistant-live2d-example]: ai_agents/agents/examples/voice-assistant-live2d
[speech-diarization-image]: https://github.com/user-attachments/assets/f94b21b8-9dda-4efc-9274-b028cc01296a
[speechmatics-diarization-example]: ai_agents/agents/examples/speechmatics-diarization
[sip-call-image]: https://github.com/user-attachments/assets/6ed5b04d-945a-4a30-a1cc-f8014b602b38
[voice-assistant-sip-example]: ai_agents/agents/examples/voice-assistant-sip-twilio
[transcription-image]: https://github.com/user-attachments/assets/d793bc6c-c8de-4996-bd85-9ce88c69dd8d
[transcription-example]: ai_age

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[DLR-RM/stable-baselines3]]></title>
            <link>https://github.com/DLR-RM/stable-baselines3</link>
            <guid>https://github.com/DLR-RM/stable-baselines3</guid>
            <pubDate>Sun, 14 Dec 2025 00:04:56 GMT</pubDate>
            <description><![CDATA[PyTorch version of Stable Baselines, reliable implementations of reinforcement learning algorithms.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/DLR-RM/stable-baselines3">DLR-RM/stable-baselines3</a></h1>
            <p>PyTorch version of Stable Baselines, reliable implementations of reinforcement learning algorithms.</p>
            <p>Language: Python</p>
            <p>Stars: 12,301</p>
            <p>Forks: 2,017</p>
            <p>Stars today: 8 stars today</p>
            <h2>README</h2><pre>&lt;!-- [![pipeline status](https://gitlab.com/araffin/stable-baselines3/badges/master/pipeline.svg)](https://gitlab.com/araffin/stable-baselines3/-/commits/master) --&gt;
[![CI](https://github.com/DLR-RM/stable-baselines3/workflows/CI/badge.svg)](https://github.com/DLR-RM/stable-baselines3/actions/workflows/ci.yml)
[![Documentation Status](https://readthedocs.org/projects/stable-baselines/badge/?version=master)](https://stable-baselines3.readthedocs.io/en/master/?badge=master) [![coverage report](https://gitlab.com/araffin/stable-baselines3/badges/master/coverage.svg)](https://github.com/DLR-RM/stable-baselines3/actions/workflows/ci.yml)
[![codestyle](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)


# Stable Baselines3

&lt;img src=&quot;docs/\_static/img/logo.png&quot; align=&quot;right&quot; width=&quot;40%&quot;/&gt;

Stable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It is the next major version of [Stable Baselines](https://github.com/hill-a/stable-baselines).

You can read a detailed presentation of Stable Baselines3 in the [v1.0 blog post](https://araffin.github.io/post/sb3/) or our [JMLR paper](https://jmlr.org/papers/volume22/20-1364/20-1364.pdf).


These algorithms will make it easier for the research community and industry to replicate, refine, and identify new ideas, and will create good baselines to build projects on top of. We expect these tools will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. We also hope that the simplicity of these tools will allow beginners to experiment with a more advanced toolset, without being buried in implementation details.

**Note: Despite its simplicity of use, Stable Baselines3 (SB3) assumes you have some knowledge about Reinforcement Learning (RL).** You should not utilize this library without some practice. To that extent, we provide good resources in the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/rl.html) to get started with RL.

## Main Features

**The performance of each algorithm was tested** (see *Results* section in their respective page),
you can take a look at the issues [#48](https://github.com/DLR-RM/stable-baselines3/issues/48) and [#49](https://github.com/DLR-RM/stable-baselines3/issues/49) for more details.

We also provide detailed logs and reports on the [OpenRL Benchmark](https://wandb.ai/openrlbenchmark/sb3) platform.


| **Features**                | **Stable-Baselines3** |
| --------------------------- | ----------------------|
| State of the art RL methods | :heavy_check_mark: |
| Documentation               | :heavy_check_mark: |
| Custom environments         | :heavy_check_mark: |
| Custom policies             | :heavy_check_mark: |
| Common interface            | :heavy_check_mark: |
| `Dict` observation space support  | :heavy_check_mark: |
| Ipython / Notebook friendly | :heavy_check_mark: |
| Tensorboard support         | :heavy_check_mark: |
| PEP8 code style             | :heavy_check_mark: |
| Custom callback             | :heavy_check_mark: |
| High code coverage          | :heavy_check_mark: |
| Type hints                  | :heavy_check_mark: |


### Planned features

Since most of the features from the [original roadmap](https://github.com/DLR-RM/stable-baselines3/issues/1) have been implemented, there are no major changes planned for SB3, it is now *stable*.
If you want to contribute, you can search in the issues for the ones where [help is welcomed](https://github.com/DLR-RM/stable-baselines3/labels/help%20wanted) and the other [proposed enhancements](https://github.com/DLR-RM/stable-baselines3/labels/enhancement).

While SB3 development is now focused on bug fixes and maintenance (doc update, user experience, ...), there is more active development going on in the associated repositories:
- newer algorithms are regularly added to the [SB3 Contrib](https://github.com/Stable-Baselines-Team/stable-baselines3-contrib) repository
- faster variants are developed in the [SBX (SB3 + Jax)](https://github.com/araffin/sbx) repository
- the training framework for SB3, the RL Zoo, has an active [roadmap](https://github.com/DLR-RM/rl-baselines3-zoo/issues/299)

## Migration guide: from Stable-Baselines (SB2) to Stable-Baselines3 (SB3)

A migration guide from SB2 to SB3 can be found in the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html).

## Documentation

Documentation is available online: [https://stable-baselines3.readthedocs.io/](https://stable-baselines3.readthedocs.io/)

## Integrations

Stable-Baselines3 has some integration with other libraries/services like Weights &amp; Biases for experiment tracking or Hugging Face for storing/sharing trained models. You can find out more in the [dedicated section](https://stable-baselines3.readthedocs.io/en/master/guide/integrations.html) of the documentation.


## RL Baselines3 Zoo: A Training Framework for Stable Baselines3 Reinforcement Learning Agents

[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a training framework for Reinforcement Learning (RL).

It provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.

In addition, it includes a collection of tuned hyperparameters for common environments and RL algorithms, and agents trained with those settings.

Goals of this repository:

1. Provide a simple interface to train and enjoy RL agents
2. Benchmark the different Reinforcement Learning algorithms
3. Provide tuned hyperparameters for each environment and RL algorithm
4. Have fun with the trained agents!

Github repo: https://github.com/DLR-RM/rl-baselines3-zoo

Documentation: https://rl-baselines3-zoo.readthedocs.io/en/master/

## SB3-Contrib: Experimental RL Features

We implement experimental features in a separate contrib repository: [SB3-Contrib](https://github.com/Stable-Baselines-Team/stable-baselines3-contrib)

This allows SB3 to maintain a stable and compact core, while still providing the latest features, like Recurrent PPO (PPO LSTM), CrossQ, Truncated Quantile Critics (TQC), Quantile Regression DQN (QR-DQN) or PPO with invalid action masking (Maskable PPO).

Documentation is available online: [https://sb3-contrib.readthedocs.io/](https://sb3-contrib.readthedocs.io/)

## Stable-Baselines Jax (SBX)

[Stable Baselines Jax (SBX)](https://github.com/araffin/sbx) is a proof of concept version of Stable-Baselines3 in Jax, with recent algorithms like DroQ or CrossQ.

It provides a minimal number of features compared to SB3 but can be much faster (up to 20x times!): https://twitter.com/araffin2/status/1590714558628253698


## Installation

**Note:** Stable-Baselines3 supports PyTorch &gt;= 2.3

### Prerequisites
Stable Baselines3 requires Python 3.10+.

#### Windows

To install stable-baselines on Windows, please look at the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/install.html#prerequisites).


### Install using pip
Install the Stable Baselines3 package:
```sh
pip install &#039;stable-baselines3[extra]&#039;
```

This includes optional dependencies like Tensorboard, OpenCV or `ale-py` to train on atari games. If you do not need those, you can use:
```sh
pip install stable-baselines3
```

Please read the [documentation](https://stable-baselines3.readthedocs.io/) for more details and alternatives (from source, using docker).


## Example

Most of the code in the library tries to follow a sklearn-like syntax for the Reinforcement Learning algorithms.

Here is a quick example of how to train and run PPO on a cartpole environment:
```python
import gymnasium as gym

from stable_baselines3 import PPO

env = gym.make(&quot;CartPole-v1&quot;, render_mode=&quot;human&quot;)

model = PPO(&quot;MlpPolicy&quot;, env, verbose=1)
model.learn(total_timesteps=10_000)

vec_env = model.get_env()
obs = vec_env.reset()
for i in range(1000):
    action, _states = model.predict(obs, deterministic=True)
    obs, reward, done, info = vec_env.step(action)
    vec_env.render()
    # VecEnv resets automatically
    # if done:
    #   obs = env.reset()

env.close()
```

Or just train a model with a one liner if [the environment is registered in Gymnasium](https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/#registering-envs) and if [the policy is registered](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html):

```python
from stable_baselines3 import PPO

model = PPO(&quot;MlpPolicy&quot;, &quot;CartPole-v1&quot;).learn(10_000)
```

Please read the [documentation](https://stable-baselines3.readthedocs.io/) for more examples.


## Try it online with Colab Notebooks !

All the following examples can be executed online using Google Colab notebooks:

- [Full Tutorial](https://github.com/araffin/rl-tutorial-jnrr19)
- [All Notebooks](https://github.com/Stable-Baselines-Team/rl-colab-notebooks/tree/sb3)
- [Getting Started](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/stable_baselines_getting_started.ipynb)
- [Training, Saving, Loading](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/saving_loading_dqn.ipynb)
- [Multiprocessing](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/multiprocessing_rl.ipynb)
- [Monitor Training and Plotting](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/monitor_training.ipynb)
- [Atari Games](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/atari_games.ipynb)
- [RL Baselines Zoo](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/rl-baselines-zoo.ipynb)
- [PyBullet](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/pybullet.ipynb)


## Implemented Algorithms

| **Name**         | **Recurrent**      | `Box`          | `Discrete`     | `MultiDiscrete` | `MultiBinary`  | **Multi Processing**              |
| ------------------- | ------------------ | ------------------ | ------------------ | ------------------- | ------------------ | --------------------------------- |
| ARS&lt;sup&gt;[1](#f1)&lt;/sup&gt;   | :x: | :heavy_check_mark: | :heavy_check_mark: | :x: | :x: | :heavy_check_mark: |
| A2C   | :x: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |
| CrossQ&lt;sup&gt;[1](#f1)&lt;/sup&gt;   | :x: | :heavy_check_mark: | :x:                | :x:                 | :x:                | :heavy_check_mark: |
| DDPG  | :x: | :heavy_check_mark: | :x:                | :x:                 | :x:                | :heavy_check_mark: |
| DQN   | :x: | :x: | :heavy_check_mark: | :x:                 | :x:                | :heavy_check_mark: |
| HER   | :x: | :heavy_check_mark: | :heavy_check_mark: | :x: | :x: | :heavy_check_mark: |
| PPO   | :x: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark:  | :heavy_check_mark: | :heavy_check_mark: |
| QR-DQN&lt;sup&gt;[1](#f1)&lt;/sup&gt;  | :x: | :x: | :heavy_check_mark: | :x:                 | :x:                | :heavy_check_mark: |
| RecurrentPPO&lt;sup&gt;[1](#f1)&lt;/sup&gt;   | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark:  | :heavy_check_mark: | :heavy_check_mark: |
| SAC   | :x: | :heavy_check_mark: | :x:                | :x:                 | :x:                | :heavy_check_mark: |
| TD3   | :x: | :heavy_check_mark: | :x:                | :x:                 | :x:                | :heavy_check_mark: |
| TQC&lt;sup&gt;[1](#f1)&lt;/sup&gt;   | :x: | :heavy_check_mark: | :x:                | :x:                 | :x: | :heavy_check_mark: |
| TRPO&lt;sup&gt;[1](#f1)&lt;/sup&gt;  | :x: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark:  | :heavy_check_mark: | :heavy_check_mark: |
| Maskable PPO&lt;sup&gt;[1](#f1)&lt;/sup&gt;   | :x: | :x: | :heavy_check_mark: | :heavy_check_mark:  | :heavy_check_mark: | :heavy_check_mark:  |

&lt;b id=&quot;f1&quot;&gt;1&lt;/b&gt;: Implemented in [SB3 Contrib](https://github.com/Stable-Baselines-Team/stable-baselines3-contrib) GitHub repository.

Actions `gymnasium.spaces`:
 * `Box`: A N-dimensional box that contains every point in the action space.
 * `Discrete`: A list of possible actions, where each timestep only one of the actions can be used.
 * `MultiDiscrete`: A list of possible actions, where each timestep only one action of each discrete set can be used.
 * `MultiBinary`: A list of possible actions, where each timestep any of the actions can be used in any combination.



## Testing the installation
### Install dependencies
```sh
pip install -e &#039;.[docs,tests,extra]&#039;
```
### Run tests
All unit tests in stable baselines3 can be run using `pytest` runner:
```sh
make pytest
```
To run a single test file:
```sh
python3 -m pytest -v tests/test_env_checker.py
```
To run a single test:
```sh
python3 -m pytest -v -k &#039;test_check_env_dict_action&#039;
```

You can also do a static type check using `mypy`:
```sh
pip install mypy
make type
```

Codestyle check with `ruff`:
```sh
pip install ruff
make lint
```

## Projects Using Stable-Baselines3

We try to maintain a list of projects using stable-baselines3 in the [documentation](https://stable-baselines3.readthedocs.io/en/master/misc/projects.html),
please tell us if you want your project to appear on this page ;)

## Citing the Project

To cite this repository in publications:

```bibtex
@article{stable-baselines3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}
```

Note: If you need to refer to a specific version of SB3, you can also use the [Zenodo DOI](https://doi.org/10.5281/zenodo.8123988).

## Maintainers

Stable-Baselines3 is currently maintained by [Ashley Hill](https://github.com/hill-a) (aka @hill-a), [Antonin Raffin](https://araffin.github.io/) (aka [@araffin](https://github.com/araffin)), [Maximilian Ernestus](https://github.com/ernestum) (aka @ernestum), [Adam Gleave](https://github.com/adamgleave) (@AdamGleave), [Anssi Kanervisto](https://github.com/Miffyli) (@Miffyli) and [Quentin GallouÃ©dec](https://gallouedec.com/) (@qgallouedec).

**Important Note: We do not provide technical support, or consulting** and do not answer personal questions via email.
Please post your question on the [RL Discord](https://discord.com/invite/xhfNqQv), [Reddit](https://www.reddit.com/r/reinforcementlearning/), or [Stack Overflow](https://stackoverflow.com/) in that case.


## How To Contribute

To any interested in making the baselines better, there is still some documentation that needs to be done.
If you want to contribute, please read [**CONTRIBUTING.md**](./CONTRIBUTING.md) guide first.

## Acknowledgments

The initial work to develop Stable Baselines3 was partially funded by the project *Reduced Complexity Models* from the *Helmholtz-Gemeinschaft Deutscher Forschungszentren*, and by the EU&#039;s Horizon 2020 Research and Innovation Programme under grant number 951992 ([VeriDream](https://www.veridream.eu/)).

The original version, Stable Baselines, was created in the [robotics lab U2IS](http://u2is.ensta-paristech.fr/index.php?lang=en) ([INRIA Flowers](https://flowers.inria.fr/) team) at [ENSTA ParisTech](http://www.ensta-paristech.fr/en).


Logo credits: [L.M. Tenkes](https://www.instagram.com/lucillehue/)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[opengeos/geoai]]></title>
            <link>https://github.com/opengeos/geoai</link>
            <guid>https://github.com/opengeos/geoai</guid>
            <pubDate>Sun, 14 Dec 2025 00:04:55 GMT</pubDate>
            <description><![CDATA[GeoAI: Artificial Intelligence for Geospatial Data]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/opengeos/geoai">opengeos/geoai</a></h1>
            <p>GeoAI: Artificial Intelligence for Geospatial Data</p>
            <p>Language: Python</p>
            <p>Stars: 2,106</p>
            <p>Forks: 295</p>
            <p>Stars today: 114 stars today</p>
            <h2>README</h2><pre># GeoAI: Artificial Intelligence for Geospatial Data

[![image](https://img.shields.io/pypi/v/geoai-py.svg)](https://pypi.python.org/pypi/geoai-py)
[![image](https://static.pepy.tech/badge/geoai-py)](https://pepy.tech/project/geoai-py)
[![image](https://img.shields.io/conda/vn/conda-forge/geoai.svg)](https://anaconda.org/conda-forge/geoai)
[![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/geoai.svg)](https://anaconda.org/conda-forge/geoai)
[![Conda Recipe](https://img.shields.io/badge/recipe-geoai-green.svg)](https://github.com/conda-forge/geoai-py-feedstock)
[![image](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![image](https://img.shields.io/badge/YouTube-Tutorials-red)](https://tinyurl.com/GeoAI-Tutorials)
[![QGIS](https://img.shields.io/badge/QGIS-plugin-orange.svg)](https://opengeoai.org/qgis_plugin)

[![logo](https://raw.githubusercontent.com/opengeos/geoai/master/docs/assets/logo_rect.png)](https://github.com/opengeos/geoai/blob/master/docs/assets/logo.png)

**A powerful Python package for integrating artificial intelligence with geospatial data analysis and visualization**

## ğŸ“– Introduction

[GeoAI](https://opengeoai.org) is a comprehensive Python package designed to bridge artificial intelligence (AI) and geospatial data analysis, providing researchers and practitioners with intuitive tools for applying machine learning techniques to geographic data. The package offers a unified framework for processing satellite imagery, aerial photographs, and vector data using state-of-the-art deep learning models. GeoAI integrates popular AI frameworks including [PyTorch](https://pytorch.org), [Transformers](https://github.com/huggingface/transformers), [PyTorch Segmentation Models](https://github.com/qubvel-org/segmentation_models.pytorch), and specialized geospatial libraries like [torchange](https://github.com/Z-Zheng/pytorch-change-models), enabling users to perform complex geospatial analyses with minimal code.

The package provides five core capabilities:

1. Interactive and programmatic search and download of remote sensing imagery and geospatial data.
2. Automated dataset preparation with image chips and label generation.
3. Model training for tasks such as classification, detection, and segmentation.
4. Inference pipelines for applying models to new geospatial datasets.
5. Interactive visualization through integration with [Leafmap](https://github.com/opengeos/leafmap/) and [MapLibre](https://github.com/eoda-dev/py-maplibregl).

GeoAI addresses the growing demand for accessible AI tools in geospatial research by providing high-level APIs that abstract complex machine learning workflows while maintaining flexibility for advanced users. The package supports multiple data formats (GeoTIFF, JPEG2000,GeoJSON, Shapefile, GeoPackage) and includes automatic device management for GPU acceleration when available. With over 10 modules and extensive notebook examples, GeoAI serves as both a research tool and educational resource for the geospatial AI community.

## ğŸ“ Statement of Need

The integration of artificial intelligence with geospatial data analysis has become increasingly critical across numerous scientific disciplines, from environmental monitoring and urban planning to disaster response and climate research. However, applying AI techniques to geospatial data presents unique challenges including data preprocessing complexities, specialized model architectures, and the need for domain-specific knowledge in both machine learning and geographic information systems.

Existing solutions often require researchers to navigate fragmented ecosystems of tools, combining general-purpose machine learning libraries with specialized geospatial packages, leading to steep learning curves and reproducibility challenges. While packages like TorchGeo and TerraTorch provide excellent foundational tools for geospatial deep learning, there remains a gap for comprehensive, high-level interfaces that can democratize access to advanced AI techniques for the broader geospatial community.

GeoAI addresses this need by providing a unified, user-friendly interface that abstracts the complexity of integrating multiple AI frameworks with geospatial data processing workflows. It lowers barriers for: (1) geospatial researchers who need accessible AI workflows without deep ML expertise; (2) AI practitioners who want streamlined geospatial preprocessing and domain-specific datasets; and (3) educators seeking reproducible examples and teaching-ready workflows.

The package&#039;s design philosophy emphasizes simplicity without sacrificing functionality, enabling users to perform sophisticated analyses such as building footprint extraction from satellite imagery, land cover classification, and change detection with just a few lines of code. By integrating cutting-edge AI models and providing seamless access to major geospatial data sources, GeoAI significantly lowers the barrier to entry for geospatial AI applications while maintaining the flexibility needed for advanced research applications.

## Citations

If you find GeoAI useful in your research, please consider citing the following paper to support my work. Thank you for your support.

-   Wu, Q. (2025). GeoAI: A Python package for integrating artificial intelligence with geospatial data analysis and visualization. _Journal of Open Source Software_, 9025. [https://doi.org/10.21105/joss.09025](https://github.com/openjournals/joss-papers/blob/joss.09605/joss.09605/10.21105.joss.09605.pdf) (Under Review)

## ğŸš€ Key Features

### ğŸ“Š Advanced Geospatial Data Visualization

-   Interactive multi-layer visualization of vector and raster data stored locally or in cloud storage
-   Customizable styling and symbology
-   Time-series data visualization capabilities

### ğŸ› ï¸ Data Preparation &amp; Processing

-   Streamlined access to satellite and aerial imagery from providers like Sentinel, Landsat, NAIP, and other open datasets
-   Tools for downloading, mosaicking, and preprocessing remote sensing data
-   Automated generation of training datasets with image chips and corresponding labels
-   Vector-to-raster and raster-to-vector conversion utilities optimized for AI workflows
-   Data augmentation techniques specific to geospatial data
-   Support for integrating Overture Maps data and other open datasets for training and validation

### ğŸ–¼ï¸ Image Segmentation

-   Integration with [PyTorch Segmentation Models](https://github.com/qubvel-org/segmentation_models.pytorch) for automatic feature extraction
-   Specialized segmentation algorithms optimized for satellite and aerial imagery
-   Streamlined workflows for segmenting buildings, water bodies, wetlands,solar panels, etc.
-   Export capabilities to standard geospatial formats (GeoJSON, Shapefile, GeoPackage, GeoParquet)

### ğŸ” Image Classification

-   Pre-trained models for land cover and land use classification
-   Transfer learning utilities for fine-tuning models with your own data
-   Multi-temporal classification support for change detection
-   Accuracy assessment and validation tools

### ğŸŒ Additional Capabilities

-   Change detection with AI-enhanced feature extraction
-   Object detection in aerial and satellite imagery
-   Georeferencing utilities for AI model outputs

## ğŸ“¦ Installation

### Using pip

```bash
pip install geoai-py
```

### Using conda

```bash
conda install -c conda-forge geoai
```

### Using mamba

```bash
mamba install -c conda-forge geoai
```

## âš™ï¸ QGIS Plugin

Check out the [QGIS Plugin](https://opengeoai.org/qgis_plugin/) page if you are interested in using GeoAI with QGIS.

[![demo](https://github.com/user-attachments/assets/5aabc3d3-efd1-4011-ab31-2b3f11aab3ed)](https://youtu.be/8-OhlqeoyiY)

## ğŸ“‹ Documentation

Comprehensive documentation is available at [https://opengeoai.org](https://opengeoai.org), including:

-   Detailed API reference
-   Tutorials and example notebooks
-   Contributing guide

## ğŸ“ºÂ Video Tutorials

### GeoAI Made Easy: Learn the Python Package Step-by-Step (Beginner Friendly)

[![intro](https://github.com/user-attachments/assets/7e60ce05-573d-4d0d-9876-5289b87e5136)](https://youtu.be/VIl29Rca6zE&amp;list=PLAxJ4-o7ZoPcvENqwaPa_QwbbkZ5sctZE)

### GeoAI Workshop: Unlocking the Power of GeoAI with Python

[![cover](https://github.com/user-attachments/assets/1c14e651-65b9-41ae-b42d-3ad028b3eeb8)](https://youtu.be/jdK-cleFUkc&amp;list=PLAxJ4-o7ZoPcvENqwaPa_QwbbkZ5sctZE)

### GeoAI Tutorials Playlist

[![cover](https://github.com/user-attachments/assets/3cde9547-ab62-4d70-b23a-3e5ed27c7407)](https://www.youtube.com/playlist?list=PLAxJ4-o7ZoPcvENqwaPa_QwbbkZ5sctZE)

## ğŸ¤ Contributing

We welcome contributions of all kinds! See our [contributing guide](https://opengeoai.org/contributing) for ways to get started.

## ğŸ“„ License

GeoAI is free and open source software, licensed under the MIT License.

## Acknowledgments

We gratefully acknowledge the support of the following organizations:

-   [NASA](https://www.nasa.gov): This research is partially supported by the National Aeronautics and Space Administration (NASA) through Grant No. 80NSSC22K1742, awarded under the [Open Source Tools, Frameworks, and Libraries Program](https://bit.ly/3RVBRcQ).
-   [AmericaView](https://americaview.org): This work is also partially supported by the U.S. Geological Survey through Grant/Cooperative Agreement No. G23AP00683 (GY23-GY27) in collaboration with AmericaView.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/presidio]]></title>
            <link>https://github.com/microsoft/presidio</link>
            <guid>https://github.com/microsoft/presidio</guid>
            <pubDate>Sun, 14 Dec 2025 00:04:54 GMT</pubDate>
            <description><![CDATA[An open-source framework for detecting, redacting, masking, and anonymizing sensitive data (PII) across text, images, and structured data. Supports NLP, pattern matching, and customizable pipelines.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/presidio">microsoft/presidio</a></h1>
            <p>An open-source framework for detecting, redacting, masking, and anonymizing sensitive data (PII) across text, images, and structured data. Supports NLP, pattern matching, and customizable pipelines.</p>
            <p>Language: Python</p>
            <p>Stars: 6,383</p>
            <p>Forks: 872</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mother-of-all-self-hosting/mash-playbook]]></title>
            <link>https://github.com/mother-of-all-self-hosting/mash-playbook</link>
            <guid>https://github.com/mother-of-all-self-hosting/mash-playbook</guid>
            <pubDate>Sun, 14 Dec 2025 00:04:53 GMT</pubDate>
            <description><![CDATA[ğŸ‹ Ansible playbook which helps you host various FOSS services as Docker containers on your own server]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mother-of-all-self-hosting/mash-playbook">mother-of-all-self-hosting/mash-playbook</a></h1>
            <p>ğŸ‹ Ansible playbook which helps you host various FOSS services as Docker containers on your own server</p>
            <p>Language: Python</p>
            <p>Stars: 902</p>
            <p>Forks: 112</p>
            <p>Stars today: 45 stars today</p>
            <h2>README</h2><pre>[![Support room on Matrix](https://img.shields.io/matrix/mash-playbook:devture.com.svg?label=%23mash-playbook%3Adevture.com&amp;logo=matrix&amp;style=for-the-badge&amp;server_fqdn=matrix.devture.com&amp;fetchMode=summary)](https://matrixrooms.info/room/mash-playbook:devture.com) [![donate](https://liberapay.com/assets/widgets/donate.svg)](https://liberapay.com/mother-of-all-self-hosting/donate)

# Mother-of-All-Self-Hosting Ansible playbook

**MASH** (**M**other-of-**A**ll-**S**elf-**H**osting) is an [Ansible](https://www.ansible.com/) playbook that helps you self-host services as [Docker](https://www.docker.com/) containers on your own server.

By running services in containers, we can have a predictable and up-to-date setup, across multiple supported distros and CPU architectures.

This project allows self-hosting of a [large number of services](docs/supported-services.md) and will continue to grow by adding support for [FOSS](https://en.wikipedia.org/wiki/Free_and_open-source_software).

[Installation](docs/README.md) (upgrades) and some maintenance tasks are automated using [Ansible](https://www.ansible.com/) (see [our Ansible guide](docs/ansible.md)).


## Supported services

See the [full list of supported services here](docs/supported-services.md).


## Installation

To configure and install services on your own server, follow the [README in the docs/ directory](docs/README.md).


## Changes

This playbook evolves over time, sometimes with backward-incompatible changes.

When updating the playbook, refer to [the changelog](CHANGELOG.md) to catch up with what&#039;s new.


## Support

- Matrix room: [#mash-playbook:devture.com](https://matrixrooms.info/room/mash-playbook:devture.com). To join Matrix, [use a public server](https://app.element.io) like `matrix.org` or self-host Matrix yourself using the related [matrix-docker-ansible-deploy](https://github.com/spantaleev/matrix-docker-ansible-deploy) Ansible playbook

- GitHub issues: [mother-of-all-self-hosting/mash-playbook/issues](https://github.com/mother-of-all-self-hosting/mash-playbook/issues)


## Why create such a mega playbook?

We used to maintain separate playbooks for various services ([Matrix](https://github.com/spantaleev/matrix-docker-ansible-deploy), [Nextcloud](https://github.com/spantaleev/nextcloud-docker-ansible-deploy), [Gitea](https://github.com/spantaleev/gitea-docker-ansible-deploy), [Gitlab](https://github.com/spantaleev/gitlab-docker-ansible-deploy), [Vaultwarden](https://github.com/spantaleev/vaultwarden-docker-ansible-deploy), [PeerTube](https://github.com/spantaleev/peertube-docker-ansible-deploy), ..). They re-used Ansible roles (for [Postgres](https://github.com/devture/com.devture.ansible.role.postgres), [Traefik](https://github.com/devture/com.devture.ansible.role.traefik), etc.), but were still hard to maintain due to the large duplication of effort.

Most of these playbooks hosted services which require a Postgres database, a Traefik reverse-proxy, a backup solution, etc. All of them needed to come with documentation, etc.
All these things need to be created and kept up-to-date in each and every playbook.

Having to use a dedicated Ansible playbook for each and every piece of software means that you have to juggle many playbooks and make sure they don&#039;t conflict with one another when installing services on the same server. All these related playbooks interoperated nicely, but still required at least a bit of manual configuration to achieve this interoperability.

Using specialized Ansible playbooks also means that trying out new software is difficult. Despite the playbooks being similar (which eases the learning curve), each one is still a new git repository you need to clone and maintain, etc.

Furthermore, not all pieces of software are large enough to justify having their own dedicated Ansible playbook. They have no home, so no one uses them.

We&#039;re finding the need for a playbook which combines all of this into one, so that:

- you don&#039;t need to juggle multiple Ansible playbooks
- you can try out various services easily - a few lines of extra configuration and you&#039;re ready to go
- small pieces of software (like [Miniflux](https://miniflux.app/), powered by the [miniflux](https://github.com/mother-of-all-self-hosting/ansible-role-miniflux) Ansible role) which don&#039;t have their own playbook can finally find a home
- you can use a single playbook with the quality you know and trust
- shared services (like Postgres) are maintained in one single place
- backups are made easy, because everything lives together (same base data path, same Postgres instance)

Having one large playbook with all services does not necessarily mean you need to host everything on the same server though. Feel free to use as many servers as you see fit. While containers provide some level of isolation, it&#039;s still better to not put all your eggs in one basket and create a single point of failure.

All of the aforementioned playbooks have been absorbed into this one. See the [full list of supported services here](docs/supported-services.md).
The [Matrix playbook](https://github.com/spantaleev/matrix-docker-ansible-deploy) will remain separate, because it contains a huge number of components and will likely grow even more. It deserves to stand on its own.


## What&#039;s with the name?

Our goal is to create a large Ansible playbook which can be your all-in-one-toolkit for self-hosting services in a clean and reliable way.

We like the MASH acronym, and [mashing](https://en.wikipedia.org/wiki/Mashing) is popular in the alcohol brewing industry. The result of all that mash is an enjoyable (at least by some) product.

Then, there&#039;s mixing and mashing stuff, which is also what this Ansible playbook is all about - you can mix and mash various pieces of software to create the self-hosted stack of your dreams!
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Mebus/cupp]]></title>
            <link>https://github.com/Mebus/cupp</link>
            <guid>https://github.com/Mebus/cupp</guid>
            <pubDate>Sun, 14 Dec 2025 00:04:52 GMT</pubDate>
            <description><![CDATA[Common User Passwords Profiler (CUPP)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Mebus/cupp">Mebus/cupp</a></h1>
            <p>Common User Passwords Profiler (CUPP)</p>
            <p>Language: Python</p>
            <p>Stars: 5,120</p>
            <p>Forks: 1,314</p>
            <p>Stars today: 21 stars today</p>
            <h2>README</h2><pre># CUPP - Common User Passwords Profiler

[![Build Status](https://travis-ci.org/Mebus/cupp.svg?branch=master)](https://travis-ci.org/Mebus/cupp)
[![Coverage Status](https://coveralls.io/repos/github/Mebus/cupp/badge.svg)](https://coveralls.io/github/Mebus/cupp)
[![Codacy Badge](https://api.codacy.com/project/badge/Grade/a578dde078ef481e97a0e7eac0c8d312)](https://app.codacy.com/app/Mebus/cupp?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=Mebus/cupp&amp;utm_campaign=Badge_Grade_Dashboard)
[![Rawsec&#039;s CyberSecurity Inventory](https://inventory.raw.pm/img/badges/Rawsec-inventoried-FF5050_plastic.svg)](https://inventory.raw.pm/)

 
## About

  The most common form of authentication is the combination of a username
  and a password or passphrase. If both match values stored within a locally
  stored table, the user is authenticated for a connection. Password strength is
  a measure of the difficulty involved in guessing or breaking the password
  through cryptographic techniques or library-based automated testing of
  alternate values.

  A weak password might be very short or only use alphanumberic characters,
  making decryption simple. A weak password can also be one that is easily
  guessed by someone profiling the user, such as a birthday, nickname, address,
  name of a pet or relative, or a common word such as God, love, money or password.

  That is why CUPP was born, and it can be used in situations like legal
  penetration tests or forensic crime investigations.


Requirements
------------

You need Python 3 to run CUPP.

Quick start
-----------

    $ python3 cupp.py -h

## Options

  Usage: cupp.py [OPTIONS]

        -h      this menu

        -i      Interactive questions for user password profiling

        -w      Use this option to profile existing dictionary,
                or WyD.pl output to make some pwnsauce :)

        -l      Download huge wordlists from repository

        -a      Parse default usernames and passwords directly from Alecto DB.
                Project Alecto uses purified databases of Phenoelit and CIRT which where merged and enhanced.

        -v      Version of the program



## Configuration

   CUPP has configuration file cupp.cfg with instructions.

## Example (Fast forwarded)

![cupp-example](screenshots/cupp-example.gif)

## License

  This program is free software; you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation; either version 3 of the License, or
  any later version.

  This program is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with this program; if not, write to the Free Software
  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA

  See &#039;./LICENSE&#039; for more information.

## Github import

This project was imported into https://github.com/Mebus/cupp by Mebus from:  
http://www.remote-exploit.org/content/cupp-3.0.tar.gz  
http://www.remote-exploit.org/articles/misc_research__amp_code/index.html  
to encourage further development of the tool.

## Original author

  Muris Kurgas aka j0rgan  
  j0rgan@remote-exploit.org  
  http://www.remote-exploit.org  
  http://www.azuzi.me  


## Contributors

  * Bosko Petrovic aka bolexxx  
  bole_loser@hotmail.com  
  http://www.offensive-security.com  
  http://www.bolexxx.net  

  * Mebus  
    https://github.com/Mebus/  

  * Abhro  
    https://github.com/Abhro/  

  * Andrea Giacomo  
    https://github.com/codepr

  * quantumcore  
    https://github.com/quantumcore
    

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[zhaochenyang20/Awesome-ML-SYS-Tutorial]]></title>
            <link>https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial</link>
            <guid>https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial</guid>
            <pubDate>Sun, 14 Dec 2025 00:04:51 GMT</pubDate>
            <description><![CDATA[My learning notes/codes for ML SYS.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial">zhaochenyang20/Awesome-ML-SYS-Tutorial</a></h1>
            <p>My learning notes/codes for ML SYS.</p>
            <p>Language: Python</p>
            <p>Stars: 4,465</p>
            <p>Forks: 280</p>
            <p>Stars today: 8 stars today</p>
            <h2>README</h2><pre># Awesome-ML-SYS-Tutorial

## [English README](./README-eng.md) | [ç®€ä½“ä¸­æ–‡](./README.md)

My learning notes/codes for ML SYS.

ä¸€ç›´ä»¥æ¥å¯¹ ML + SYS å¾ˆæ„Ÿå…´è¶£ï¼Œè‹¦äºæœ¬ç§‘æ²¡æœ‰å­¦å¥½ MLï¼Œæ›´æ²¡å­¦å¥½ SYSï¼Œä½†æ˜¯è¯»åšäº†è§‰å¾—è‡ªå·±åº”è¯¥å¯ä»¥åœ¨è¿™æ–¹é¢è¯•ä¸€è¯•ã€‚

æœ‰å¦‚æ­¤æ‰“ç®—ï¼Œä¸€æ¥æ˜¯æˆ‘å‘è§‰ç»„é‡Œå¾ˆå¤šèƒ½åŠ›å‡ºä¼—çš„é«˜å¹´çº§å­¦é•¿ä»¬åšçš„æ˜¯ ML Theory + Applicationã€‚ä¸è¿‡ï¼ŒçœŸçš„æŠŠä¸€ä¸ª Theory è½åˆ°ä¸€ä¸ªè‰¯å¥½çš„ Application ä¸Šï¼Œå³ä¾¿æ˜¯ä»–ä»¬è¿™æ ·è®©æˆ‘æ•¬ä½©çš„ theory researcherï¼Œä¹Ÿæœ‰ç€ä¸€å®šæŒ‘æˆ˜ã€‚åœ¨æˆ‘å…¥å­¦å‰ï¼Œç»„é‡Œæœ‰ä¸¤ç¯‡è®©æˆ‘çœ¼å‰ä¸€äº®çš„å·¥ä½œ [SPIN](https://github.com/uclaml/SPIN) å’Œ [SPPO](https://github.com/uclaml/SPPO)ã€‚å·¥ä½œæœ¬èº«éƒ½æœ‰éå¸¸æ£’çš„ä»·å€¼ï¼Œä½†æ˜¯å¦‚æœåœ¨å·¥ç¨‹/ç³»ç»Ÿä¸Šä¼˜åŒ–å¥½ï¼Œæƒ³æ¥å¯ä»¥æœ‰æ›´å¥½çš„å½±å“åŠ›ã€‚

æ­¤å¤–ï¼Œåšå£«å…¥å­¦å‰çš„æš‘å‡ï¼Œæˆ‘å’Œç»„é‡ŒåŒå­¦åšäº†ä¸€ä¸ª In-context Learning for Agent çš„å·¥ä½œ [COPS](https://github.com/uclaml/COPS)ï¼Œæ¯”è¾ƒç¬¦åˆæˆ‘çš„å®¡ç¾ã€‚æˆ‘ä»¬å°±ä¸¤ä¸ªäººä¸»åŠ›å¹²æ´»ï¼Œä¸€ä¸ªå¤§å“¥æ¨ç†è®ºï¼Œè€Œæˆ‘è´Ÿè´£åœ¨å·¥ç¨‹/ç³»ç»Ÿä¸Šå®ç°ã€‚è¿™ç§å·¥ä½œæ¨¡å¼è®©æˆ‘çš„ä½“æ„Ÿéå¸¸èˆ’é€‚ï¼ŒåŸºäºæ­¤ï¼Œæˆ‘ç”šè‡³å¾—å‡ºä¸€ä¸ªç²—ç³™çš„ç»“è®ºï¼š

$$
\dfrac{\text{Theory}+\text{System}}{2}=\text{Application}
$$

è¿™å°±æ˜¯æˆ‘æƒ³åš ML + SYS çš„åˆè¡·äº†ã€‚æ‰€ä»¥ä» 2024 å¹´çš„å¤å­£å¼€å§‹ï¼Œæˆ‘å¼€å§‹æ…¢æ…¢ä¸Šæ‰‹ ML + SYS è¿™ä¸ªå°šä¸”æ–¹å…´æœªè‰¾çš„é¢†åŸŸã€‚éœ€è¦å­¦ä¹ çš„å®åœ¨å¤ªå¤šäº†ï¼Œæœ‰çš„åœ¨ä¸€äº›å¹³å°ï¼ˆè­¬å¦‚çŸ¥ä¹å’Œ HuggingFace Blogï¼‰ä¸Šå·²ç»æœ‰äº†å¾ˆå¥½çš„èµ„æ–™ï¼Œä½†æ˜¯å…¶ä»–éƒ¨åˆ†ä»æœ‰æ‰€æ¬ ç¼ºã€‚æ‰€ä»¥ï¼Œè¿™ä¸ª repo ä¸»è¦è®°è½½äº†æˆ‘è‡ªå·±çš„ä¸€äº›å­¦ä¹ ç¬”è®°/è¯»åæ„Ÿ/æ€ç´¢/å‚è€ƒè¿‡çš„èµ„æ–™ etcï¼Œæˆ‘å§‘ä¸”æŒ‰ç…§è‡ªå·±çš„å¤§ç‰ˆå›¾è¿›è¡Œåˆ†ç±»ï¼Œä¹Ÿæ¬¢è¿å¤§å®¶ PRã€‚æ¯ä¸€ä¸ªå¤§çš„æ¿å—ï¼Œå€’å™é˜…è¯»å°±æ˜¯æˆ‘çš„å­¦ä¹ è¿‡ç¨‹ï¼Œæ¬¢è¿å¤§å®¶å‚è€ƒæ­¤è·¯å¾„ä¸Šæ‰‹ã€‚

## æœªå…¬å¼€éƒ¨åˆ†

ä¹‹å‰çš„ç¬”è®°å¤§å¤šå†™äº 2024 å¹´å¹´åº•ï¼Œç»è¿‡äº†åŠå¹´æ—¶é—´ï¼Œæˆ‘çš„ä»“åº“å·²ç•¥å¹´ä¹…å¤±ä¿®ã€‚ä¸€æ–¹é¢æˆ‘è‡ªå·±æ›´å¤šåœ¨é¡¹ç›®ä¸­è´Ÿè´£æ¨åŠ¨ + deliveryï¼Œåè€Œè‡ªå·±å¾ˆå°‘å†™ä»£ç ï¼›å¦ä¸€æ–¹é¢ï¼Œå¤šå¤šå°‘å°‘ä¸å°‘æœ‹å‹å‘æˆ‘ä»¬çš„ä»“åº“è´¡çŒ®äº†ç¬”è®°ï¼Œä½†æˆ‘å®Œå…¨æ²¡æœ‰æ¥å¾—åŠæ•´ç†ã€‚è¿™æ®µæ—¶é—´ä¼šä¸æ–­å®Œæˆæ•´ç†å¹¶å‘å¸ƒã€‚ä¸‹æ–¹æ–‡ç« åˆ—è¡¨ä¸­æ ‡è®°ä¸º [Pending Review] çš„æ–‡ç« å°šæœª reviewï¼Œæ¬¢è¿å¤§å®¶ä¸€èµ· reviewï¼

## RLHF System å¼€å‘ç¬”è®°

### slime æ¡†æ¶

- [è®©é€Ÿåº¦ä¸ç²¾åº¦åŒåœ¨ï¼šå…¨é¢è§£å†³ RL ä¸­çš„è®­æ¨ä¸ä¸€è‡´é—®é¢˜](./rlhf/mismatch/blog-cn.md)ï¼šä»‹ç» slime æ¡†æ¶å¯¹è®­æ¨ä¸ä¸€è‡´é—®é¢˜æä¾›çš„ä¸¤ç§è§£å†³æ–¹æ¡ˆï¼šé€šè¿‡ kernel å±‚é¢å¯¹é½å®ç°å®Œç¾çš„ True On-Policy è®­ç»ƒï¼Œä»¥åŠåŸºäº TIS/MIS ç­‰ç®—æ³•æ¥ç¼“è§£è®­æ¨ä¸ä¸€è‡´çš„å½±å“ã€‚åŒæ ·åˆŠè½½[è‹±æ–‡ç‰ˆæœ¬](./rlhf/mismatch/blog-en.md)ã€‚
- [Support FSDP2 as A Training Backend for slime](./rlhf/slime/fsdp/readme.md)ï¼šåœ¨ slime ä¸­æ–°å¢äº† FSDP ä½œä¸ºè®­ç»ƒåç«¯ï¼Œå¹¶ä¸ Megatron å®Œæˆå¯¹é½ã€‚FSDP èƒ½å¤Ÿæ›´åŠ çµæ´»æ”¯æŒè¯¸å¦‚ Qwen3-Next/gpt-oss ç­‰æ¶æ„åˆ›æ–°çš„æ¨¡å‹ï¼Œå¹¶ä¸”æœ‰åŠ©äºæˆ‘ä»¬è¿›ä¸€æ­¥æ”¯æŒ VLM RLã€‚åŒæ ·åˆŠè½½[è‹±æ–‡ç‰ˆæœ¬](./rlhf/slime/fsdp/readme_en.md)å’Œ[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1979141713449742500)ã€‚
- [Unified FP8: Moving Beyond Mixed Precision for Stable and Accelerated MoE RL](./rlhf/slime/fp8/readme.md)ï¼šåœ¨ RL ä¸­å®Œå…¨ä½¿ç”¨ FP8 è¿›è¡Œé‡‡æ ·ï¼ˆRolloutï¼‰å’Œè®­ç»ƒï¼ˆTrainingï¼‰ï¼ŒåŒæ ·åˆŠè½½[è‹±æ–‡ç‰ˆæœ¬](./rlhf/slime/fp8/readme_en.md)å’Œ[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1974681194017865986)ã€‚
- [Power Up Speculative Decoding In Reinforcement Learning](./rlhf/slime/spec/readme.md)ï¼šå°† speculative decoding å¼•å…¥åˆ°äº† RL çš„é‡‡æ ·æµç¨‹ä¸­ï¼Œåœ¨ batch size åˆé€‚çš„æƒ…å†µä¸‹ï¼Œé‡‡æ ·é€Ÿåº¦å¾—åˆ°äº†æ˜¾è‘—æå‡ï¼›å¹¶ä¸”ï¼Œdraft model ä¹Ÿä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ›´æ–°ã€‚ç›¸è¾ƒäºå†»ç»“ draft model çš„åšæ³•ï¼Œaccepted length æŒç»­ç»´æŒåœ¨è¾ƒé«˜æ°´å¹³ï¼Œäº§ç”Ÿé•¿æœŸç¨³å®šçš„æ­£æ”¶ç›Šã€‚åŒæ ·åˆŠè½½[è‹±æ–‡ç‰ˆæœ¬](./rlhf/slime/spec/readme-en.md)ã€‚
- [æ·±å…¥æµ…å‡º slime RL æ¡†æ¶çš„ä¼˜é›…è®¾è®¡ä¸æºç ](./rlhf/slime/code-walk-through/readme.md)ï¼šslime æºç èµæï¼ŒåŒæ ·åˆŠè½½äº[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1946402397409740613)å’Œ[è‹±æ–‡ç‰ˆæœ¬](./rlhf/slime/code-walk-through/readme_en.md)ã€‚
- [Pending Review] [slime FSDP Setup Guide](./rlhf/slime/fsdp/release_log/setup_fsdp.md)ï¼šè®°å½•å¦‚ä½•åœ¨ slime ä¸Šæµ‹è¯• FSDPï¼ŒåŒ…æ‹¬ H å¡å’Œ B å¡ï¼Œä»¥åŠ Colocate å’Œ Disaggregated ä¸¤ç§ placement æ–¹å¼ã€‚
- [Pending Review] [PPO ä¸­ GAE çš„åˆ† chunk å¹¶è¡Œè®¡ç®—ï¼ˆåŸºäº slime çš„å®ç°ï¼‰](./rlhf/slime/batch-GAE/ppo-gae-chunk.md)ï¼šå°†æ ‡å‡† GAE çš„åå‘é€’æ¨æ”¹å†™ä¸ºåŸºäº chunk çš„å¹¶è¡Œå‰ç¼€æ‰«æï¼Œåœ¨é•¿åºåˆ—åœºæ™¯ä¸‹å¤§å¹…ç¼“è§£ GAE è®¡ç®—ç“¶é¢ˆï¼Œåœ¨ slime ä¸­å®ç°çº¦ 100Ã—â€“300Ã— åŠ é€Ÿã€‚åŒæ ·åˆŠè½½äº[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1975237289425798560)ã€‚

### AReal æ¡†æ¶
- [AReal Code Walk Through](./rlhf/areal/code-walk-through_CN.md) AReal æºç èµæï¼ŒåŒæ ·åˆŠè½½äº[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1983417813080236770)å’Œ[è‹±æ–‡ç‰ˆæœ¬](./rlhf/areal/code-walk-through_EN.md)ã€‚


### verl æ¡†æ¶

- [é€šè¿‡ Torch Memory Snapshot åˆ†æ VLM RL è®­ç»ƒä¸­çš„æ˜¾å­˜æ³„éœ²é—®é¢˜](./torch/mem-snapshot/readme.md)ï¼šåˆ†æ SGLang çš„æ˜¾å­˜æ³„éœ²é—®é¢˜ï¼Œä»¥åŠè§£å†³æ–¹æ¡ˆï¼ŒåŒæ ·åˆŠè½½äº[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1943202817247519535)å’Œ[è‹±æ–‡ç‰ˆæœ¬](./torch/mem-snapshot/readme-en.md)ã€‚
- [Latency optimization for weight updates](./sglang/latency-accelerate-for-weight-updates/readme.md)ï¼šä¸€æ¬¡å¯¹æ•ˆç‡çš„ debug è¿‡ç¨‹ï¼ŒåŒæ ·åˆŠè½½äº[è®°ä¸€æ¬¡å¯¹ SGLang weight update latency çš„ä¼˜åŒ–](https://zhuanlan.zhihu.com/p/9908228168)ã€‚
- [æ·±å…¥æµ…å‡ºç†è§£ verl æºç ï¼ˆåˆå§‹åŒ–ï¼‰](./rlhf/verl/multi-turn/code-walk-through/readme.md)ï¼šåŒæ ·åˆŠè½½äº[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1920751852749849692)ï¼Œè¿˜æœ‰[è‹±æ–‡ç‰ˆæœ¬](./rlhf/verl/multi-turn/code-walk-through/readme_EN.md)ã€‚
- [æ·±å…¥æµ…å‡ºç†è§£ verl æºç ï¼ˆRolloutï¼‰](./rlhf/verl/multi-turn/code-walk-through/readme-2.md)ï¼šåŒæ ·åˆŠè½½äº[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1923349757566388159)ï¼Œè¿˜æœ‰[è‹±æ–‡ç‰ˆæœ¬](./rlhf/verl/multi-turn/code-walk-through/readme-2-EN.md)ã€‚
- [Pending Review] [æ·±å…¥æµ…å‡ºç†è§£ verl æºç ï¼ˆMake Experienceï¼‰](./rlhf/verl/multi-turn/code-walk-through/readme-3.md)ï¼šåˆ†æ verl ä¸­ make experience éƒ¨åˆ†çš„é€»è¾‘ã€‚
- [AgentLoop æºç æµ…æ](./rlhf/verl/multi-turn/code-walk-through/readme-6.md): åˆ†æ verl ä¸­åŸºäº AgentLoop çš„ multi-turn RL çš„å®ç°ã€‚
- [verl å‚æ•°é€Ÿè§ˆ](./rlhf/verl/multi-turn/code-walk-through/readme-5.md)ï¼šverl å‚æ•°é€Ÿè§ˆï¼ŒåŒæ ·åˆŠè½½äº[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1925041836998783250)ï¼Œè¿˜æœ‰[è‹±æ–‡ç‰ˆæœ¬](./rlhf/verl/multi-turn/code-walk-through/readme-5-EN.md)ã€‚
- [ä» tokenizer è§†è§’æ¥åˆ†æ Agentic å¤šè½®è®­ç»ƒçš„å¤æ‚æ€§](./rlhf/verl/multi-turn/fast_tokenization/multiturn_tokenization_and_masking_ZH.md)ï¼šåŒæ ·åˆŠè½½äº[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1917126584806139373)å’Œ[è‹±æ–‡ç‰ˆæœ¬](./rlhf/verl/multi-turn/fast_tokenization/multiturn_tokenization_and_masking.md)ã€‚
- [Pending Review] [DAPO Dynamic Filtering å®ç°ä¸ Batch Size è§£æ](./rlhf/verl/multi-turn/code-walk-through/dapo.md)ï¼šæ¢ç´¢é€šè¿‡å°† prompt è¡¥é½åˆ°æ›´å°çš„ batch size å®ç°æ›´é«˜çš„å¹¶è¡Œåº¦ã€‚
- [ç³»ç»Ÿæ€§åˆ†æ verl multi-turn training çš„æ—¶é—´æ¶ˆè€—](./rlhf/verl/multi-turn/tool_examples/profile.md)ï¼šverl å¤šè½®äº¤äº’ä¸å·¥å…·è°ƒç”¨ profile åˆ†æï¼Œè¿˜æœ‰[è‹±æ–‡ç‰ˆæœ¬](./rlhf/verl/multi-turn/tool_examples/profile_en.md)å’Œ[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1929748460212552414)ã€‚
- [SGLang, verl, OpenBMB ä¸æ¸…åå¤§å­¦å›¢é˜Ÿè”åˆå¼€æºï¼šåœ¨ä¸»æµ RLHF æ¡†æ¶ä¸Šé¦–æ¬¡æ”¯æŒå¤šè½®äº¤äº’ä¸å·¥å…·è°ƒç”¨](./rlhf/verl/multi-turn/release_log/verl-multiturn-rollout-Release_ZH.md)ï¼šåœ¨ä¸»æµ RLHF æ¡†æ¶ä¸Šé¦–æ¬¡æ”¯æŒå¤šè½®äº¤äº’ä¸å·¥å…·è°ƒç”¨ï¼ŒåŒæ ·åˆŠè½½äº[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1906007821889283171)ã€‚
- [Search-R1 &amp; veRL-SGLang: Train LLMs with Multi-Turn RL to Reason and Call a Search Engine](./rlhf/verl/multi-turn/tool_examples/verl-multiturn-searchR1-like_ZH.md)ï¼šæ•´åˆ Search-R1 framework åˆ° verl-sglang ç”Ÿæ€ï¼ŒåŒæ ·åˆŠè½½äº[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1912156329751081620)ã€‚
- [SGLang-veRL Serverï¼šä» Engine åˆ° Serverï¼Œæˆ‘ä»¬éœ€è¦æ›´çµæ´»çš„ RLHF rollout æ¥å£](./rlhf/verl/server-based/veRL-server-based-rollout.md)ï¼šä¸ºäº†å®ç°æ›´å¤æ‚çš„ RLHF ç³»ç»Ÿï¼Œæˆ‘ä»¬é€æ­¥å°† veRL å½“ä¸­çš„ rollout engine æ›¿ä»£ä¸º rollout serverï¼ŒåŒæ ·åˆŠè½½äº[çŸ¥ä¹ï¼šSGLang-veRL Server](https://zhuanlan.zhihu.com/p/1890631652486665464)ã€‚
- [HybridFlow veRL åŸæ–‡æµ…æ](./rlhf/verl/readme.md)ï¼šSGLang çš„ hybrid engine çš„åŸç†ä¸å®ç°ï¼ŒåŒæ ·åˆŠè½½äº[çŸ¥ä¹ï¼šHybridFlow veRL åŸæ–‡æµ…æ](https://zhuanlan.zhihu.com/p/24682036412)ã€‚

### OpenRLHF æ¡†æ¶

- [å›¾è§£å¤§æ¨¡å‹RLHFç³»åˆ—ä¹‹ï¼šäººäººéƒ½èƒ½çœ‹æ‡‚çš„PPOåŸç†ä¸æºç è§£è¯»](https://zhuanlan.zhihu.com/p/677607581)ä»¥åŠ[å›¾è§£OpenRLHFä¸­åŸºäºRayçš„åˆ†å¸ƒå¼è®­ç»ƒæµç¨‹](https://zhuanlan.zhihu.com/p/12871616401)ï¼šçŒ›çŒ¿å°å§å§çš„éå¸¸å¥½çš„ RLHF å…¥é—¨èµ„æ–™ï¼Œçœ‹äº†ä¹‹åä¼šå¯¹ RLHF çš„è®¡ç®—æµä»¥åŠ OpenRLHF PPO çš„æ¡†æ¶æœ‰å¾ˆå¥½çš„ç†è§£ï¼Œæˆ‘è‡ªå·±ä¹Ÿè¡¥å……äº†å†™è‡ªå·±çš„ç†è§£åœ¨ [RLHF çš„è®¡ç®—æµ](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/tree/main/rlhf/OpenRLHF#rlhf-%E7%9A%84%E8%AE%A1%E7%AE%97%E6%B5%81)ã€‚
- [æµ…æä»¥ OpenRLHF ä¸ºä»£è¡¨çš„ post-training ç³»ç»Ÿçš„è®¡ç®—æµç¨‹](./rlhf/OpenRLHF/readme.md)ï¼šåŸºäºçŒ›çŒ¿å°å§å§çš„æ–‡ç« å†åšè¡¥å……ï¼ŒGithub native æ¸²æŸ“çš„å·¨çƒ‚ï¼Œç”šè‡³çœ‹[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/16370000391)å¥½äº†ã€‚


### ç³»ç»Ÿè®¾è®¡ä¸ä¼˜åŒ–

- [RL ç³»ç»Ÿæ·±æ€ï¼šæ·±å…¥ç†è§£æƒé‡æ›´æ–°æœºåˆ¶](./rlhf/sys-design/readme-1.md)ï¼šåŠå¹´å·¥ä½œçš„æ€»ç»“ï¼Œæ·±å…¥ç†è§£æƒé‡æ›´æ–°æœºåˆ¶ï¼ŒåŒæ ·åˆŠè½½äº[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1925210722704531547)å’Œ[è‹±æ–‡ç‰ˆæœ¬](./rlhf/sys-design/readme-1-EN.md)ã€‚
- [RL ç³»ç»Ÿæ·±æ€ï¼šFSDP è®­ç»ƒåç«¯](./rlhf/sys-design/readme-2.md)ï¼šè®¨è®º FSDP çš„åŸç†å’Œå®ç°ï¼Œä»¥åŠåˆ†æ verl çš„ FSDP ä½¿ç”¨ã€‚åŒæ ·åˆŠè½½äº[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1929115059113693341)å’Œ[è‹±æ–‡ç‰ˆæœ¬](./rlhf/sys-design/readme-2-en.md)ã€‚
- [Pending Review] [RL ç³»ç»Ÿæ·±æ€ï¼šMegatron](./rlhf/sys-design/readme-3.md)ï¼šMegatron çš„åŸºæœ¬ç‰¹æ€§æµ…æï¼Œé‡ç‚¹åˆ†æ Megatron åœ¨ RL æ¡†æ¶ä¸­çš„ä½¿ç”¨ã€‚
- [æ‰©å±• OpenRLHF çš„æ¨ç†å¼•æ“](./rlhf/OpenRLHF/develop-log.md)ï¼šå°† SGLang æ¥å…¥åˆ° OpenRLHF çš„å¼€å‘ç¬”è®°ï¼Œæ•´ä¸ªè¿‡ç¨‹éå¸¸ç—›è‹¦ï¼Œè€Œä¸”ç›®å‰è¿˜æœ‰ nccl hang errorï¼Œå·²ç»ç›´æ¥è”ç³»äº† deepspeed core contributor åœ¨ä¿®å¤äº†ã€‚
- [Pending Review] [SGLang as rollout engine of GRPO trainer](./rlhf/GRPO/SGLang_GRPO.md)ï¼šä»‹ç»å¦‚ä½•å°† SGLang ä½œä¸º TRL ä¸­ GRPO Trainer çš„æ¨ç†åç«¯ï¼ŒGRPO æ˜¯ PPO çš„å˜ä½“ï¼Œåœ¨ä¼˜åŒ–æ•°å­¦æ¨ç†èƒ½åŠ›çš„åŒæ—¶ä¼˜åŒ– PPO çš„å†…å­˜ä½¿ç”¨ã€‚

### ç®—æ³•ä¸ç†è®º

- [Pending Review] [Learning to Reason under Off-Policy Guidance](./rlhf/partial-rollout/Learning_to_Reason_under_Off-Policy_Guidance.md)ï¼šä½¿ç”¨ç¦»çº¿ç­–ç•¥è¾…åŠ©åœ¨çº¿å­¦ä¹ çš„ LUFFY æ¡†æ¶ï¼Œé€šè¿‡å°† off-policy æ¨ç†è½¨è¿¹ä¸ on-policy rollout ç»“åˆï¼ŒåŠ¨æ€å¹³è¡¡æ¨¡ä»¿ä¸æ¢ç´¢ã€‚
- [Kimi K1.5: Long Context RL çš„æˆåŠŸå®è·µ](./rlhf/partial-rollout/readme.md)ï¼šLong Context RLHF çš„å·¥ä¸šçº§å®ç°ï¼Œä¸€ç›´å¾ˆå–œæ¬¢ kimi å›¢é˜Ÿçš„æŠ€æœ¯æŠ¥å‘Šï¼ŒåŒæ ·åˆŠè½½äº [Kimi K1.5: Long Context RL çš„æˆåŠŸå®è·µ](https://zhuanlan.zhihu.com/p/1894282607325344277)ã€‚
- [Rule-based Reward](https://zhuanlan.zhihu.com/p/13211508979)ï¼šè¿™ç¯‡åªæœ‰çŸ¥ä¹ï¼Œæµ…æµ…å†™äº†å†™ï¼Œè€å®è¯´åŸæ–‡å†™çš„æˆ‘å¹¶ä¸å¤ªå–œæ¬¢ï¼Œä½†æ˜¯ determined reward ç¡®å® charmingã€‚
- [SWE-Benchï¼šå¦‚ä½•æ„é€  LLM æ—¶ä»£çš„ä¼˜ç§€ Benchmark](https://zhuanlan.zhihu.com/p/16292266518)ï¼ŒåŸºäº SWE-Bench çš„è®ºæ–‡é˜…è¯»ç¬”è®°ï¼Œå¦‚ä½•æ„é€ å¥½çš„ benchmark ä»¥ä¸º post-training æä¾›ç»†ç²’åº¦ rewardï¼Œæ˜¯æ°¸æ’ä¸”ç¾å¦™çš„è¯é¢˜ã€‚
- [æµ…æä¸»æµ Alignment ç®—æ³•ä¸ NeMo-Aligner æ¡†æ¶](https://zhuanlan.zhihu.com/p/5220718268)


## SGLang å­¦ä¹ ç¬”è®°

### SGLang Diffusion å­¦ä¹ ç¬”è®°

- [SGLang Diffusion Code Walk Through](./sglang/code-walk-through/sgl_diffusion.md)ï¼šdiffusion model çš„åŸºç¡€åŸç†ï¼Œä»¥åŠä¸€ä¸ªè¯·æ±‚è¢« SGLang-Diffusion å¤„ç†çš„å…¨è¿‡ç¨‹ï¼ŒåŒæ ·åˆŠè½½äº[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1982441236066480797)å’Œ[è‹±æ–‡ç‰ˆæœ¬](./sglang/code-walk-through/sgl_diffusion_en.md)ã€‚

### æ ¸å¿ƒæ¶æ„ä¸ä¼˜åŒ–

- [SGLang Code Walk Through](./sglang/code-walk-through/readme.md)ï¼šä¸€ä¸ªè¯·æ±‚è¢« SGLang Engine å¤„ç†çš„å…¨è¿‡ç¨‹ï¼Œè¿˜æœ‰ä¸€äº› part æ²¡æœ‰å®Œæˆï¼Œä½†æ˜¯å¤§å¤šåœ°æ–¹å·²ç» okayï¼Œä¹Ÿè®©å¾ˆå¤š SGLang begginer å°±æ­¤å¼€å§‹ã€‚è¿™é‡Œè¿˜æœ‰[ä¸­æ–‡ç‰ˆæœ¬](./sglang/code-walk-through/readme-CN.md)ã€‚
- [Walk Through SGLang / VLLM Worker](./sglang/sglang-worker/readme.md)ï¼šSGLang çš„ä»£ç ä¸å®Œå…¨è§£æï¼ŒåŒæ ·åˆŠè½½äº [Walk Through SGLang / VLLM Worker](https://zhuanlan.zhihu.com/p/6363614076)ï¼Œè¿™æ¬¡æˆ‘ä»¬è¿˜è´´å¿ƒæä¾›äº†[è‹±æ–‡ç‰ˆæœ¬](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/sglang/sglang-worker/readme.md)ã€‚æ›´è¯¦ç»†çš„è§£æåº”è¯¥å‚è€ƒ [SGLang Code Walk Through](./sglang/code-walk-through/readme.md)ï¼Œè¿™ä¸ªåªæ˜¯è¾…åŠ©çœ‹çœ‹ã€‚
- [Walk Through SGLang Scheduler](./sglang/sglang-scheduler/readme-CN.md)
- [Pending Review] [SGLang Scheduler Evolution](./sglang/scheduler-evolution/SGLang%20Scheduler%20æŠ€æœ¯å˜è¿.md)ï¼šè¯¦ç»†ä»‹ç»äº† SGLang Scheduler ä»ä¸²è¡Œåˆ° CPU / GPU overlap çš„æŠ€æœ¯æ¼”è¿›åŠç›¸å…³ç»„ä»¶ï¼Œå¯¹æ¯”å‰ä»£ overlap Scheduler å’Œå½“å‰å¼•å…¥å¤š CUDA stream ä¸ FutureMap çš„ overlap Schedulerã€‚å¯åˆ°çŸ¥ä¹æŸ¥çœ‹[æ–‡ç« ](https://zhuanlan.zhihu.com/p/1969077475129688722)
- [Pending Review] [KV Cache Code Walkthrough](./sglang/kvcache-code-walk-through/readme.md)ï¼šKV cache ç®¡ç†å®ç°çš„æ¦‚è§ˆï¼Œä» Scheduler ç»„ä»¶å¼€å§‹ï¼Œè¯¦ç»†è¯´æ˜ prefill å’Œ decode é˜¶æ®µä¸­ KV cache å’Œå†…å­˜æ± çš„æ›´æ–°è¿‡ç¨‹ã€‚
- [Pending Review] [SGLang å¤šæ¨¡æ€è¯·æ±‚ç”Ÿå‘½å‘¨æœŸï¼šä»¥ Qwen2.5-VL ä¸ºä¾‹çš„æ¶æ„çº§æ·±åº¦è§£æ](./sglang/code-walk-through/multimodal_request_lifecycle.md)ï¼šä»¥ Qwen2.5-VL ä¸ºå‚è€ƒæ¨¡å‹ï¼Œæä¾›å¯¹ SGLang æ¡†æ¶å†…å¤šæ¨¡æ€è¯·æ±‚å¤„ç†æµç¨‹çš„è¯¦ç»†å‰–æã€‚
- [Pending Review] [How A Model is Loaded in Hugging Face and SGLang](./sglang/how-model-is-loaded/readme.md)ï¼šè®°å½•æ¨¡å‹åœ¨ Hugging Face å’Œ SGLang ä¸­çš„åŠ è½½è¿‡ç¨‹ï¼Œå¸®åŠ©ç†è§£æƒé‡åŠ è½½æœºåˆ¶ã€‚
- [Pending Review] [Speculative Decoding](./sglang/speculative-decoding/speculative-decoding.md)ï¼šä»‹ç» speculative decoding ä¼˜åŒ–æŠ€æœ¯ï¼Œåˆ©ç”¨è¾ƒå°çš„ draft model é¢„æµ‹ä¸‹ä¸€ä¸ª K ä¸ª tokenï¼Œå®ç°æœ€é«˜ K å€çš„åŠ é€Ÿã€‚
- [Pending Review] [Zero-Overhead Batch Scheduler](./sglang/zero-overhead-scheduler/zero-overhead-batch-scheduler.md)ï¼šä»‹ç»é›¶å¼€é”€æ‰¹å¤„ç†è°ƒåº¦å™¨ï¼Œè§£å†³ä¼ ç»Ÿæ¨ç†ç³»ç»Ÿä¸­ CPU è°ƒåº¦å’Œ GPU è®¡ç®—ä¸²è¡Œæ‰§è¡Œå¯¼è‡´çš„ GPU Bubble é—®é¢˜ã€‚
- [Pending Review] [Data Parallelism Attention](./sglang/dp-attention/readme.md)ï¼šè¯¦ç»†ä»‹ç» DP Attention çš„åŸç†ä¸å®ç°ï¼Œé’ˆå¯¹ DeepSeek ç­‰ä½¿ç”¨ MLA ä¸”åªæœ‰ä¸€ä¸ª KV head çš„æ¨¡å‹ï¼Œé¿å… tensor parallelism å¯¼è‡´çš„ KV cache é‡å¤ã€‚
- [æµ…æ SGLang æ¡†æ¶çš„é‡åŒ–è®¾è®¡ä¸æ€è·¯](./sglang/quantization/quantization_architecture.md)ï¼šåŒæ ·åˆŠè½½äº[çŸ¥ä¹ï¼šæµ…æ SGLang æ¡†æ¶çš„é‡åŒ–è®¾è®¡ä¸æ€è·¯](https://zhuanlan.zhihu.com/p/1971183020338832111)è¿˜æœ‰[è‹±æ–‡ç‰ˆæœ¬](./sglang/quantization/quantization_architecture_en.md)ã€‚
- [Constraint Decoding çš„æ¦‚å¿µã€æ–¹æ³•ä¸ä¼˜åŒ–](./sglang/constraint-decoding/readme.md)ï¼šåŒæ ·åˆŠè½½äº[çŸ¥ä¹ï¼šä¸€æ–‡ç†è§£ Constraint Decoding çš„æ¦‚å¿µã€æ–¹æ³•ä¸ä¼˜åŒ–](https://zhuanlan.zhihu.com/p/18336995950)ã€‚
- [Pending Review] [Online Update Weights](./sglang/online-update-weights/readme.md)ï¼šä»‹ç» SGLang ä¸­ `online_update_weights` æ¥å£çš„å®ç°ï¼ŒåŒºåˆ«äºä»ç£ç›˜è¯»å–æƒé‡çš„ `update_weights`ï¼Œè¯¥æ¥å£ä»è®­ç»ƒ engine ä¸­ç›´æ¥é€šè¿‡ nccl å¹¿æ’­æ–°çš„æƒé‡ã€‚
- [Pending Review] [SGLang Verl Engine ä¼˜åŒ–è§£æ](./sglang/sglang-verl-engine/readme.md)ï¼šè§£æ SGLang ä¸­ verl engine çš„ä¼˜åŒ–ï¼ŒåŒ…æ‹¬ `update_weights_from_tensor` ç­‰æ¥å£çš„å®ç°ã€‚
- [Latency Accelerate For Weight Updates](./sglang/latency-accelerate-for-weight-updates/readme-CN.md)
- **[ğŸ”¥ç›¸å…³è°ƒè¯•] [é€šè¿‡ Torch Memory Snapshot åˆ†æ VLM RL è®­ç»ƒä¸­çš„æ˜¾å­˜æ³„éœ²é—®é¢˜](./torch/mem-snapshot/readme.md)**ï¼šåˆ†æ SGLang çš„æ˜¾å­˜æ³„éœ²é—®é¢˜ï¼Œä»¥åŠè§£å†³æ–¹æ¡ˆï¼ŒåŒæ ·åˆŠè½½äº[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1943202817247519535)å’Œ[è‹±æ–‡ç‰ˆæœ¬](./torch/mem-snapshot/readme-en.md)ã€‚

### ä½¿ç”¨ä¸å®è·µ

- [Pending Review] [Qwen3-Coder Usage](./sglang/qwen/coder.md)ï¼šä»‹ç»å¦‚ä½•åœ¨ SGLang ä¸­ä½¿ç”¨ Qwen3-coderï¼ŒåŒ…æ‹¬ tool-parser çš„ä½¿ç”¨ã€‚
- [Pending Review] [NVIDIA Dynamo](./sglang/nvidia-dynamo/dynamo.md)ï¼šä»‹ç» NVIDIA Dynamoï¼Œä¸€ä¸ªä¸ºå¤šèŠ‚ç‚¹åˆ†å¸ƒå¼ç¯å¢ƒä¸­çš„ç”Ÿæˆå¼ AI å’Œæ¨ç†æ¨¡å‹æœåŠ¡è®¾è®¡çš„é«˜ååé‡ä½å»¶è¿Ÿæ¨ç†æ¡†æ¶ã€‚
- [æŸ¥çœ‹ HuggingFace æ¨¡å‹ç»“æ„](https://zhuanlan.zhihu.com/p/9912733791)
- [SGLang åç«¯åŸæ–‡è§£æ](https://zhuanlan.zhihu.com/p/716543182)
- [Reward / Embed Model Sever Engine ç°çŠ¶æµ…æ](https://zhuanlan.zhihu.com/p/4148050391)
- [å°ç™½è§†è§’ï¼švllm è¿ç§»åˆ° SGLang çš„ä½“éªŒä¸æ”¶è·](https://zhuanlan.zhihu.com/p/714833359)
- [å°ç™½è§†è§’ï¼šåˆ©ç”¨ SGL æ¥ Serve Embedding Model](https://zhuanlan.zhihu.com/p/715805386)
- [å°ç™½è§†è§’ï¼šåˆ©ç”¨ vllm serve æ–°çš„ Embedding Model](https://zhuanlan.zhihu.com/p/715857723)

## Scheduling and Routing

- [Mooncakeï¼šå°† P / D åˆ†ç¦»è¿›è¡Œåˆ°åº•](https://zhuanlan.zhihu.com/p/1711346141)
- [prefill å’Œ decode è¯¥åˆ†ç¦»åˆ°ä¸åŒçš„å¡ä¸Šä¹ˆï¼Ÿ](https://zhuanlan.zhihu.com/p/1280567902)
- [åŸºäº chunked prefill ç†è§£ prefill å’Œ decode çš„è®¡ç®—ç‰¹æ€§](https://zhuanlan.zhihu.com/p/718715866)
- [ModelServerï¼šåŸºäº SGLang çš„å‰ç«¯åˆ†å‘ç³»ç»Ÿ](https://zhuanlan.zhihu.com/p/718015016)


## ML System åŸºæœ¬åŠŸ

### Transformers &amp; Model Architecture

- [Pending Review] [Transformerä¸­çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶](./transformers/attention/cross_attention.md)ï¼šä»‹ç» Transformer ä¸­çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼Œå…è®¸è§£ç å™¨è®¿é—®å’Œä½¿ç”¨ç¼–ç å™¨çš„ç›¸å…³ä¿¡æ¯ï¼ŒåŒæ ·æœ‰[è‹±æ–‡ç‰ˆæœ¬](./transformers/attention/cross_attention_en.md)ã€‚
- [ä¸€æ–‡ç†è§£ special tokens å’Œ chat template](./transformers/special_tokens/special_tokens.md)ï¼šåŒæ ·è®°å½•äºçŸ¥ä¹ [ä¸€æ–‡ç†è§£ special tokens å’Œ chat template](https://zhuanlan.zhihu.com/p/17052593700)ã€‚

### CUDA &amp; GPU

- [åŸºäº torch-memory-savor æµ…æ CUDA Graph](./torch/cuda-graph/readme.md)ï¼šåŒæ ·åˆŠè½½äº[çŸ¥ä¹ï¼šåŸºäº torch-memory-savor æµ…æ CUDA Graph](https://zhuanlan.zhihu.com/p/1921726788574360686)å’Œ[è‹±æ–‡ç‰ˆ](./torch/cuda-graph/readme_en.md)ã€‚

### Distributed Training &amp; Communication

- [Pending Review] [æ‰‹æ“ Tensor Parallelism](./torch/tensor-parallelism/readme.md)ï¼šå…³äº Tensor Parallelism çš„å®ç°ä¸å®è·µã€‚
- [NCCL ä¸ NVIDIA TOPO](./torch/nccl/readme.md)ï¼šNCCL çš„å…¥é—¨ä¸ NVIDIA æ˜¾å¡çš„æ£€æµ‹ï¼ŒåŒæ ·åˆŠè½½äº[NCCL ä¸ NVIDIA TOPO](https://zhuanlan.zhihu.com/p/6160835906)ã€‚
- [NCCL and SGLang](./torch/nccl/readme_en.md)ï¼šNCCL åœ¨ SGLang ä¸­çš„åº”ç”¨ï¼Œå…¶å®å’Œä¸­æ–‡å†…å®¹éå¸¸æ¥è¿‘ï¼Œä½†æ˜¯é¢å¤–åˆŠè½½äº†ä¸€äº›å¹¶è¡Œç­–ç•¥çš„å†…å®¹ã€‚æˆ‘åº”è¯¥ä¸ä¼šä¿®ç¼®å®Œæˆè¿™ä¸ªç¬”è®°ï¼Œè€Œæ˜¯å•ç‹¬å†™ç¬”è®°æ¥è®°å½•å¹¶è¡Œç­–ç•¥ã€‚
- [PyTorch Distributed](./torch/torch-distributed/readme.md)ï¼š`torch.distributed` çš„é€šè®¯å®è·µï¼Œ GIL å’Œ `all_reduce` çš„ç»†èŠ‚ã€‚è¿™ä¸€éƒ¨åˆ†åŒæ ·åˆŠè½½åœ¨ [çŸ¥ä¹ï¼šPyTorch é€šè®¯å®è·µ](https://zhuanlan.zhihu.com/p/5853094319)ã€‚
- [[åŸåˆ›][æ·±åº¦][PyTorch] DDPç³»åˆ—ç¬¬ä¸€ç¯‡ï¼šå…¥é—¨æ•™ç¨‹](https://zhuanlan.zhihu.com/p/178402798)ï¼šè™½ç„¶æˆ‘æ²¡å­¦æ˜ç™½ DDP çš„å†…å®¹ï¼Œæˆ‘åªæ˜¯å€Ÿæ­¤å­¦ä¹ äº†ä¸‹ GIL å’Œ ring all reduceï¼Œè¿™ä¸€æ­¥åˆŠè½½äº [torch-distributed çš„åè®°](./torch/torch-distributed/readme.md#gil)ã€‚
- [nvidia-smiå‘½ä»¤è¯¦è§£å’Œä¸€äº›é«˜é˜¶æŠ€å·§ä»‹ç»](https://www.yourmetaverse.cn/deep_learning/199/)ï¼šä¸»è¦æ˜¯ä¸€äº›ç½‘ç»œæ‹“æ‰‘ï¼Œåœ¨æˆ‘æœ¬æœºçš„ç»“æœè®°å½•åœ¨ [nccl éƒ¨åˆ†](./torch/nccl/readme.md#nvlink-æŸ¥è¯¢)ã€‚

### Quantization

- [Give me BF16 or Give Me Deathï¼Œå½“ä¸‹é‡åŒ–æ–¹æ³•çš„å…¨é¢è¯„æµ‹](https://zhuanlan.zhihu.com/p/5485556270)
- [AWQï¼šæ¨¡å‹é‡åŒ–åº”å½“å…³æ³¨æ¿€æ´»å€¼](https://zhuanlan.zhihu.com/p/942485319)


## å¼€å‘æŒ‡å—

- [How to use docker](./engineer/how-to-use-docker/readme.md)ï¼šå¦‚ä½•ä½¿ç”¨ docker æ¥ç®¡ç†å¼€å‘ç¯å¢ƒã€‚è¯·æ³¨æ„ï¼Œä¸ºäº†å…±åŒå¡‘é€ è‰¯å¥½çš„ç§‘ç ”ç¯å¢ƒï¼Œé¿å…æœ‰äººç”¨ baseline &quot;åœ¨æˆ‘çš„æœºå™¨ä¸Šèƒ½è·‘&quot;æ¥æ¶å¿ƒåˆ«äººï¼Œå­¦ä¹  docker å¯¹ä»»ä½•äººéƒ½æ˜¯å¿…ä¸å¯å°‘çš„ã€‚åŒæ ·æˆ‘ä»¬ä¹Ÿæœ‰[è‹±æ–‡ç‰ˆæœ¬](./engineer/how-to-use-docker/readme_en.md)å’Œ[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/1916764175230801287)ã€‚
- [é…ç½®æ¸…çˆ½çš„å¼€å‘ç¯å¢ƒ](./engineer/uv/readme.md)ï¼šé…ç½®æ¸…çˆ½çš„å¼€å‘ç¯å¢ƒï¼ŒåŒæ ·åˆŠè½½äº[çŸ¥ä¹ï¼šé…ç½®æ¸…çˆ½çš„å¼€å‘ç¯å¢ƒ](https://zhuanlan.zhihu.com/p/23440683394)ã€‚
- [åœ¨ CI ä¸Šç¼–è¯‘ jupyter notebook å¹¶éƒ¨ç½²ä¸ºæ–‡æ¡£](https://zhuanlan.zhihu.com/p/2382351079)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pytorch/torchtitan]]></title>
            <link>https://github.com/pytorch/torchtitan</link>
            <guid>https://github.com/pytorch/torchtitan</guid>
            <pubDate>Sun, 14 Dec 2025 00:04:50 GMT</pubDate>
            <description><![CDATA[A PyTorch native platform for training generative AI models]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pytorch/torchtitan">pytorch/torchtitan</a></h1>
            <p>A PyTorch native platform for training generative AI models</p>
            <p>Language: Python</p>
            <p>Stars: 4,835</p>
            <p>Forks: 635</p>
            <p>Stars today: 5 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# torchtitan

#### A PyTorch native platform for training generative AI models

[![8 GPU Feature Tests](https://github.com/pytorch/torchtitan/actions/workflows/integration_test_8gpu_features.yaml/badge.svg?branch=main)](https://github.com/pytorch/torchtitan/actions/workflows/integration_test_8gpu_features.yaml?query=branch%3Amain)
[![8 GPU Model Tests](https://github.com/pytorch/torchtitan/actions/workflows/integration_test_8gpu_models.yaml/badge.svg?branch=main)](https://github.com/pytorch/torchtitan/actions/workflows/integration_test_8gpu_models.yaml?query=branch%3Amain)
[![arXiv](https://img.shields.io/badge/arXiv-2410.06511-b31b1b.svg)](https://arxiv.org/abs/2410.06511)
[![ICLR](https://img.shields.io/badge/ICLR-2025-violet.svg)](https://iclr.cc/virtual/2025/poster/29620)
[![forum](https://img.shields.io/badge/pytorch-forum-DE3412.svg)](https://discuss.pytorch.org/c/distributed/torchtitan/44)
[![license](https://img.shields.io/badge/license-BSD_3--Clause-lightgrey.svg)](./LICENSE)
[![pip](https://img.shields.io/pypi/v/torchtitan?color=blue)](https://pypi.org/project/torchtitan/)
[![conda](https://img.shields.io/conda/vn/conda-forge/torchtitan?color=green)](https://anaconda.org/conda-forge/torchtitan)


&lt;/div&gt;

`torchtitan` is under extensive development. To use the latest features of `torchtitan`, we recommend using the most recent PyTorch nightly.


## Latest News
- [2025/11] AMD released an [optimized fork](https://github.com/AMD-AGI/torchtitan-amd/tree/main) of `torchtitan` for AMD GPUs.
- [2025/10] We released `torchtitan` [v0.2.0](https://github.com/pytorch/torchtitan/releases).
- [2025/10] SkyPilot now supports `torchtitan`! See the tutorial [here](https://docs.skypilot.co/en/latest/examples/training/torchtitan.html).
- [2025/07] We published [instructions](/torchtitan/models/README.md) on how to add a model to `torchtitan`.
- [2025/04] Our paper was accepted by [ICLR 2025](https://iclr.cc/virtual/2025/poster/29620).
- [2024/12] GPU MODE [lecture](https://www.youtube.com/watch?v=VYWRjcUqW6w) on torchtitan.
- [2024/07] [Presentation](https://pytorch2024.sched.com/event/1fHn3) at PyTorch Conference 2024.


## Overview

`torchtitan` is a PyTorch native platform designed for **rapid experimentation and large-scale training** of generative AI models. As a minimal clean-room implementation of PyTorch native scaling techniques, `torchtitan` provides a flexible foundation for developers to build upon. With `torchtitan` [extension points](docs/extension.md), one can easily create custom extensions tailored to specific needs.

Our mission is to accelerate innovation in the field of generative AI by empowering researchers and developers to explore new modeling architectures and infrastructure techniques.

The Guiding Principles when building `torchtitan`
* Designed to be easy to understand, use and extend for different training purposes.
* Minimal changes to the model code when applying multi-dimensional parallelism.
* Bias towards a clean, minimal codebase while providing basic reusable / swappable components.

`torchtitan` has been showcasing PyTorch&#039;s latest distributed training features, via support for pretraining Llama 3.1 LLMs of various sizes.

## Contributing

We look forward to your contributions!

* To accelerate contributions to and innovations around torchtitan, we host an [`experiments`](torchtitan/experiments) folder. New ideas should start there. To contribute, follow the [`experiments guidelines`](torchtitan/experiments/README.md).
* For fixes and contributions to core, follow these [`guidelines`](CONTRIBUTING.md).

## Llama 3.1 training

### Key features available

1. Multi-dimensional composable parallelisms
   - [FSDP2](docs/fsdp.md) with per-parameter sharding
   - [Tensor Parallel](https://pytorch.org/docs/stable/distributed.tensor.parallel.html) (including [async TP](https://discuss.pytorch.org/t/distributed-w-torchtitan-introducing-async-tensor-parallelism-in-pytorch/209487))
   - [Pipeline Parallel](https://discuss.pytorch.org/t/distributed-w-torchtitan-training-with-zero-bubble-pipeline-parallelism/214420)
   - [Context Parallel](https://discuss.pytorch.org/t/distributed-w-torchtitan-breaking-barriers-training-long-context-llms-with-1m-sequence-length-in-pytorch-using-context-parallel/215082)
2. [Meta device](https://pytorch.org/docs/stable/meta.html) initialization
3. Selective (layer or operator) and full activation checkpointing
4. [Distributed checkpointing](https://discuss.pytorch.org/t/distributed-w-torchtitan-optimizing-checkpointing-efficiency-with-pytorch-dcp/211250) (including async checkpointing)
   - [Interoperable checkpoints](docs/checkpoint.md) which can be loaded directly into [`torchtune`](https://github.com/pytorch/torchtune) for fine-tuning
5. `torch.compile` support
6. [Float8](https://discuss.pytorch.org/t/distributed-w-torchtitan-enabling-float8-all-gather-in-fsdp2/209323) support ([how-to](docs/float8.md))
7. [MXFP8 training for dense and MoE models](docs/mxfp8.md) on Blackwell GPUs.
7. DDP and HSDP
8. [TorchFT](https://github.com/pytorch/torchft) integration
9. Checkpointable data-loading, with the C4 dataset pre-configured (144M entries) and support for [custom datasets](docs/datasets.md)
10. Gradient accumulation, enabled by giving an additional `--training.global_batch_size` argument in configuration
11. Flexible learning rate scheduler (warmup-stable-decay)
12. Loss, GPU memory, throughput (tokens/sec), TFLOPs, and MFU displayed and logged via [Tensorboard or Weights &amp; Biases](/docs/metrics.md)
13. [Debugging tools](docs/debugging.md) including CPU/GPU profiling, memory profiling, Flight Recorder, etc.
14. All options easily configured via [toml files](torchtitan/models/llama3/train_configs/)
15. [Helper scripts](scripts/) to
    - download tokenizers from Hugging Face
    - convert original Llama 3 checkpoints into the expected DCP format
    - estimate FSDP/HSDP memory usage without materializing the model
    - run distributed inference with Tensor Parallel

We report [performance](benchmarks/llama3_h100_202412_torchtitan.md) on up to 512 GPUs, and verify [loss converging](docs/converging.md) correctness of various techniques.

### Dive into the code

You may want to see how the model is defined or how parallelism techniques are applied. For a guided tour, see these files first:
* [torchtitan/train.py](torchtitan/train.py) - the main training loop and high-level setup code
* [torchtitan/models/llama3/model/model.py](torchtitan/models/llama3/model/model.py) - the Llama 3.1 model definition
* [torchtitan/models/llama3/infra/parallelize.py](torchtitan/models/llama3/infra/parallelize.py) - helpers for applying Data Parallel, Tensor Parallel, activation checkpointing, and `torch.compile` to the model
* [torchtitan/models/llama3/infra/pipeline.py](torchtitan/models/llama3/infra/pipeline.py) - helpers for applying Pipeline Parallel to the model
* [torchtitan/components/checkpoint.py](torchtitan/components/checkpoint.py) - utils for saving/loading distributed checkpoints
* [torchtitan/components/quantization/float8.py](torchtitan/components/quantization/float8.py) - utils for applying Float8 techniques


## Installation

One can directly run the source code, or install `torchtitan` from a nightly build, or a stable release.

### From source

This method requires the nightly build of PyTorch, or the latest PyTorch built [from source](https://github.com/pytorch/pytorch?tab=readme-ov-file#from-source).

```bash
git clone https://github.com/pytorch/torchtitan
cd torchtitan
pip install -r requirements.txt
```

### Nightly builds

This method requires the nightly build of PyTorch. You can replace `cu126` with another version of cuda (e.g. `cu128`) or an AMD GPU (e.g. `rocm6.3`).

```sh
pip3 install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu126 --force-reinstall
pip install --pre torchtitan --index-url https://download.pytorch.org/whl/nightly/cu126
```

### Stable releases
One can install the latest [stable release](https://github.com/pytorch/torchtitan/releases) of `torchtitan` via `pip` or `conda`.
```sh
pip install torchtitan
```
```sh
conda install conda-forge::torchtitan
```
Note that each stable release pins the nightly versions of `torch` and `torchao`. Please see [release.md](docs/release.md) for more details.

### Downloading a tokenizer

`torchtitan` currently supports training Llama 3.1 (8B, 70B, 405B) out of the box. To get started training these models, we need to download the tokenizer. Follow the instructions on the official [meta-llama](https://huggingface.co/meta-llama/Llama-3.1-8B) repository to ensure you have access to the Llama model weights.

Once you have confirmed access, you can run the following command to download the Llama 3.1 tokenizer to your local machine.

```bash
# Get your HF token from https://huggingface.co/settings/tokens

# Llama 3.1 tokenizer
python scripts/download_hf_assets.py --repo_id meta-llama/Llama-3.1-8B --assets tokenizer --hf_token=...
```

### Start a training run
Llama 3 8B model locally on 8 GPUs

```bash
CONFIG_FILE=&quot;./torchtitan/models/llama3/train_configs/llama3_8b.toml&quot; ./run_train.sh
```

### Multi-Node Training
For training on ParallelCluster/Slurm type configurations, you can use the `multinode_trainer.slurm` file to submit your sbatch job.

To get started adjust the number of nodes and GPUs
```
#SBATCH --ntasks=2
#SBATCH --nodes=2
```

Then start a run where `nnodes` is your total node count, matching the sbatch node count above.

```
srun torchrun --nnodes 2
```

If your gpu count per node is not 8, adjust `--nproc_per_node` in the torchrun command and `#SBATCH --gpus-per-task` in the SBATCH command section.


## Citation

We provide a detailed look into the parallelisms and optimizations available in `torchtitan`, along with summary advice on when to use various techniques.

[TorchTitan: One-stop PyTorch native solution for production ready LLM pre-training](https://openreview.net/forum?id=SFN6Wm7YBI)
```
@inproceedings{
   liang2025torchtitan,
   title={TorchTitan: One-stop PyTorch native solution for production ready {LLM} pretraining},
   author={Wanchao Liang and Tianyu Liu and Less Wright and Will Constable and Andrew Gu and Chien-Chin Huang and Iris Zhang and Wei Feng and Howard Huang and Junjie Wang and Sanket Purandare and Gokul Nadathur and Stratos Idreos},
   booktitle={The Thirteenth International Conference on Learning Representations},
   year={2025},
   url={https://openreview.net/forum?id=SFN6Wm7YBI}
}
```


## License

Source code is made available under a [BSD 3 license](./LICENSE), however you may have other legal obligations that govern your use of other content linked in this repository, such as the license or terms of service for third-party data and models.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[TagStudioDev/TagStudio]]></title>
            <link>https://github.com/TagStudioDev/TagStudio</link>
            <guid>https://github.com/TagStudioDev/TagStudio</guid>
            <pubDate>Sun, 14 Dec 2025 00:04:49 GMT</pubDate>
            <description><![CDATA[A User-Focused Photo & File Management System]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/TagStudioDev/TagStudio">TagStudioDev/TagStudio</a></h1>
            <p>A User-Focused Photo & File Management System</p>
            <p>Language: Python</p>
            <p>Stars: 6,451</p>
            <p>Forks: 435</p>
            <p>Stars today: 47 stars today</p>
            <h2>README</h2><pre># TagStudio: A User-Focused Photo &amp; File Management System

[![Downloads](https://img.shields.io/github/downloads/TagStudioDev/TagStudio/total.svg?maxAge=2592001)](https://github.com/TagStudioDev/TagStudio/releases)
[![Translations](https://hosted.weblate.org/widget/tagstudio/strings/svg-badge.svg)](https://hosted.weblate.org/projects/tagstudio/strings/)
[![PyTest](https://github.com/TagStudioDev/TagStudio/actions/workflows/pytest.yaml/badge.svg)](https://github.com/TagStudioDev/TagStudio/actions/workflows/pytest.yaml)
[![MyPy](https://github.com/TagStudioDev/TagStudio/actions/workflows/mypy.yaml/badge.svg)](https://github.com/TagStudioDev/TagStudio/actions/workflows/mypy.yaml)
[![Ruff](https://github.com/TagStudioDev/TagStudio/actions/workflows/ruff.yaml/badge.svg)](https://github.com/TagStudioDev/TagStudio/actions/workflows/ruff.yaml)

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;60%&quot; src=&quot;docs/assets/ts-9-3_logo_text.png&quot;&gt;
&lt;/p&gt;

TagStudio is a photo &amp; file organization application with an underlying tag-based system that focuses on giving freedom and flexibility to the user. No proprietary programs or formats, no sea of sidecar files, and no complete upheaval of your filesystem structure. **Read the documentation and more at [docs.tagstud.io](https://docs.tagstud.io)!**

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;80%&quot; src=&quot;docs/assets/screenshot.png&quot; alt=&quot;TagStudio Screenshot&quot;&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;i&gt;TagStudio Alpha v9.5.5 running on macOS Sequoia.&lt;/i&gt;
&lt;/p&gt;

## Contents

-   [Feature Highlights](#feature-highlights)
-   [Basic Usage](#basic-usage)
-   [Installation](#installation)
-   [Goals &amp; Priorities](#goals--priorities)
-   [FAQ](#faq)

Translation hosting generously provided by [Weblate](https://weblate.org/en/). Check out our [project page](https://hosted.weblate.org/projects/tagstudio/) to help translate TagStudio!

## Feature Highlights

### Libraries

A TagStudio library contains all of your tags, fields for a set of files based on one of your system directories. Similar to how Obsidian [vaults](https://help.obsidian.md/vault) function, TagStudio libraries act as a layer on top of your existing folders and file structure, and don&#039;t require your to move or duplicate files.

TagStudio places a `.TagStudio` folder in the folder you open as a library. Files included in your library are referred to as &quot;entries&quot;, and are kept track of inside of a SQLite database inside the `.TagStudio` folder along with tags and other library data.

### File Entries

All file types are supported in TagStudio libraries, just not all have dedicated preview support. For a full list of filetypes with supported previews, see the &quot;[Supported Previews](https://docs.tagstud.io/preview-support)&quot; page on the documentation site. There&#039;s also playback support for videos, audio files, and supported animated image formats.

For a generalized list of what&#039;s currently supported:

-   **Images**
    -   Raster Images (JPEG, PNG, etc.)
    -   Vector (SVG)
    -   Animated (GIF, WEBP, APNG)
    -   RAW Formats
-   **Videos**
-   **Plaintext Files**
-   **Documents** _(If supported)_
-   **eBooks** _(If supported)_
-   **Photoshop PSDs**, **Blender Projects**, **Krita Projects**, and more!

### [Tags](https://docs.tagstud.io/tags) and [Fields](https://docs.tagstud.io/fields)

Tags represent an object or attribute - this could be a person, place, object, concept, and more. Unlike most tagging systems, TagStudio tags are not solely represented by a line of text or a hashtag. Tags in TagStudio consist of several properties and relationships that give extra customization, searching power, and ease of tagging that cannot be achieved by string-based tags alone. TagStudio tags are designed to be as simple or as complex as you&#039;d like, giving options to users of all skill levels and use cases.

Tags currently consist of the following attributes:

-   **Name**: The full name for your tag. **_This does NOT have to be unique!_**
-   **Shorthand Name**: The shortest alternate name for your tag, used for abbreviations.
-   **Aliases**: Alternate names your tag goes by.
-   **Color**: The display color of your tag.
-   **Parent Tags**: Other tags in which this tag inherits from. In practice, this means that this tag can be substituted in searches for any listed parent tags.
    -   Parent tags checked with the &quot;disambiguation&quot; checkbox next to them will be used to help disambiguate tag names that may not be unique.
    -   For example: If you had a tag for &quot;Freddy Fazbear&quot;, you might add &quot;Five Nights at Freddy&#039;s&quot; as one of the parent tags. If the disambiguation box is checked next to &quot;Five Nights at Freddy&#039;s&quot; parent tag, then the tag &quot;Freddy Fazbear&quot; will display as &quot;Freddy Fazbear (Five Nights at Freddy&#039;s)&quot;. Furthermore, if the &quot;Five Nights at Freddy&#039;s&quot; tag has a shorthand like &quot;FNAF&quot;, then the &quot;Freddy Fazbear&quot; tag will display as &quot;Freddy Fazbear (FNAF)&quot;.
-   **Is Category**: A property that when checked, treats this tag as a category in the preview panel.

Fields, like tags, are additional pieces of custom metadata that you can add to your file entries. Fields currently have several hardcoded names (e.g. &quot;Title&quot;, &quot;Author&quot;, &quot;Series&quot;) but custom field names are planned for an upcoming update.

Field types currently include:

-   **Text Lines**: Single lines of text.
-   **Text Boxes**: Multi-line pieces of text.
-   **Datetimes**: Dates and times.

### [Search](https://docs.tagstud.io/search)

-   Search for file entries based on tags, file path (`path:`), file types (`filetype:`), and even media types! (`mediatype:`). Path searches currently use [glob](&lt;https://en.wikipedia.org/wiki/Glob_(programming)&gt;) syntax, so you may need to wrap your filename or filepath in asterisks while searching. This will not be strictly necessary in future versions of the program.
-   Use and combine boolean operators (`AND`, `OR`, `NOT`) along with parentheses groups, quotation escaping, and underscore substitution to create detailed search queries
-   Use special search conditions (`special:untagged` and `special:empty`) to find file entries without tags or fields, respectively

## Basic Usage

&gt; [!TIP]
&gt; For more usage instructions, see the [documentation site](https://docs.tagstud.io/libraries)!

### Creating/Opening a Library

With TagStudio opened, start by creating a new library or opening an existing one using File -&gt; Open/Create Library from the menu bar. TagStudio will automatically create a new library from the chosen directory if one does not already exist. Upon creating a new library, TagStudio will automatically scan your folders for files and add those to your library (no files are moved during this process!).

### Refreshing the Library

Libraries under 10,000 files automatically scan for new or modified files when opened. In order to refresh the library manually, select &quot;Refresh Directories&quot; under the File menu or by pressing &lt;kbd&gt;Ctrl&lt;/kbd&gt;&lt;/kbd&gt;+&lt;kbd&gt;R&lt;/kbd&gt; (macOS: &lt;kbd&gt;âŒ˜ Command&lt;/kbd&gt;+&lt;kbd&gt;R&lt;/kbd&gt;).

### Creating Tags

Create a new tag by accessing the &quot;New Tag&quot; option from the Edit menu or by pressing &lt;kbd&gt;Ctrl&lt;/kbd&gt;+&lt;kbd&gt;T&lt;/kbd&gt; (macOS: &lt;kbd&gt;âŒ˜ Command&lt;/kbd&gt;+&lt;kbd&gt;T&lt;/kbd&gt;). In the tag creation panel, enter a tag name, optional shorthand name, optional tag aliases, optional parent tags, and an optional color.

#### Tag Manager

You can manage your library of tags from opening the &quot;Tag Manager&quot; panel from Edit -&gt; &quot;Tag Manager&quot; or by pressing &lt;kbd&gt;Ctrl&lt;/kbd&gt;+&lt;kbd&gt;M&lt;/kbd&gt; (macOS: &lt;kbd&gt;âŒ˜ Command&lt;/kbd&gt;+&lt;kbd&gt;M&lt;/kbd&gt;). From here you can create, search for, edit, and permanently delete any tags you&#039;ve created in your library.

### Editing Tags

To edit a tag, click on it inside the preview panel or right-click the tag and select &quot;Edit Tag&quot; from the context menu.

### Adding Tags to File Entries

Access the &quot;Add Tag&quot; search box by either clicking on the &quot;Add Tag&quot; button at the bottom of the right sidebar, accessing the &quot;Add Tags to Selected&quot; option from the File menu, or by pressing &lt;kbd&gt;Ctrl&lt;/kbd&gt;+&lt;kbd&gt;Shift&lt;/kbd&gt;+&lt;kbd&gt;T&lt;/kbd&gt; (macOS: &lt;kbd&gt;âŒ˜ Command&lt;/kbd&gt;+&lt;kbd&gt;Shift&lt;/kbd&gt;+&lt;kbd&gt;T&lt;/kbd&gt;).

From here you can search for existing tags or create a new one if the one you&#039;re looking for doesn&#039;t exist. Click the &quot;+&quot; button next to any tags you want to the currently selected file entries. To quickly add the top result, press the &lt;kbd&gt;Enter&lt;/kbd&gt;/&lt;kbd&gt;Return&lt;/kbd&gt; key to add the top-most tag and reset the tag search. Press &lt;kbd&gt;Enter&lt;/kbd&gt;/&lt;kbd&gt;Return&lt;/kbd&gt; once more to close the dialog box. By using this method, you can quickly add various tags in quick succession just by using the keyboard!

To remove a tag from a file entry, hover over the tag in the preview panel and click on the &quot;-&quot; icon that appears.

### Adding Fields to File Entries

To add a metadata field to a file entry, start by clicking the &quot;Add Field&quot; button at the bottom of the preview panel. From the dropdown menu, select the type of metadata field youâ€™d like to add to the entry

### Editing Fields

Hover over the field and click the pencil icon. From there, add or edit text in the dialog box popup.

### Relinking Moved Files

Inevitably some of the files inside your library will be renamed, moved, or deleted. If a file has been renamed or moved, TagStudio will display the thumbnail as a red broken chain link. To relink moved files or delete these entries, select the &quot;Manage Unlinked Entries&quot; option under the Tools menu. Click the &quot;Refresh&quot; button to scan your library for unlinked entries. Once complete, you can attempt to &quot;Search &amp; Relink&quot; any unlinked file entries to their respective files, or &quot;Delete Unlinked Entries&quot; in the event the original files have been deleted and you no longer wish to keep their entries inside your library.

&gt; [!WARNING]
&gt; There is currently no method to relink entries to files that have been renamed - only moved or deleted. This is a high priority for future releases.

&gt; [!WARNING]
&gt; If multiple matches for a moved file are found (matches are currently defined as files with a matching filename as the original), TagStudio will currently ignore the match groups. Adding a GUI for manual selection, as well as smarter automated relinking, are high priorities for future versions.

See instructions in the &quot;[Creating Development Environment](/CONTRIBUTING.md/#creating-a-development-environment)&quot; section from the [contributing](https://docs.tagstud.io/contributing) page.

## Installation

To download executable builds of TagStudio, visit the [Releases](https://github.com/TagStudioDev/TagStudio/releases) page of the GitHub repository and download the latest release for your system under the &quot;Assets&quot; section at the bottom of the release.

TagStudio has builds for **Windows**, **macOS** _(Apple Silicon &amp; Intel)_, and **Linux**. We also offer portable releases for Windows and Linux which are self-contained and easier to move around.

For detailed instructions, installation help, and instructions for developing for TagStudio, please see the &quot;[Installation](https://docs.tagstud.io/install)&quot; page on our documentation website.

&gt; [!IMPORTANT]
&gt; If you&#039;re interested in contributing to TagStudio, please take a look at the [contribution guidelines](https://docs.tagstud.io/contributing) for how to get started!

### Third-Party Dependencies

For video thumbnails and playback, you&#039;ll also need [FFmpeg](https://ffmpeg.org/download.html) installed on your system. If you encounter any issues with this, please reference our [FFmpeg Help](/docs/help/ffmpeg.md) guide. For faster library scanning and refreshing, it&#039;s also recommended you install [ripgrep](https://github.com/BurntSushi/ripgrep).

&lt;!-- prettier-ignore --&gt;
&gt; [!CAUTION]
&gt; **We do not currently publish TagStudio to any package managers. Any TagStudio distributions outside of the GitHub [Releases](https://github.com/TagStudioDev/TagStudio/releases) page are _unofficial_ and not maintained by us.**
&gt;
&gt; Installation support will not be given to users installing from unofficial sources. Use these versions at your own risk!

## Goals &amp; Priorities

TagStudio aims to create an **open** and **robust** format for file tagging that isn&#039;t burdened by the limitations of traditional tagging and file metadata systems. **TagStudio** is the first proof-of-concept implementation of this system.

See the [**Roadmap**](docs/roadmap.md) on the documentation site for a complete list of planned features and estimated timeline.

### Overall Goals

-   To achieve a portable, private, extensible, open-format, and feature-rich system of organizing and rediscovering files.
-   To provide powerful methods for organization, notably the concept of tag inheritance, or &quot;taggable tags&quot; _(and in the near future, the combination of composition-based tags)._
-   To create an implementation of such a system that is resilient against a userâ€™s actions outside the program (modifying, moving, or renaming files) while also not burdening the user with mandatory sidecar files or requiring them to change their existing file structures and workflows.
-   To support a wide range of users spanning across different platforms, multi-user setups, and those with large (several terabyte) libraries.
-   To make the dang thing look nice, too. Itâ€™s 2025, not 1995.

### Project Priorities

1. **The concept.** Even if TagStudio as an application fails, Iâ€™d hope that the idea lives on in a superior project. The goals outlined above donâ€™t reference TagStudio once - _TagStudio_ is what references the _goals._
2. **The system.** Frontends and implementations can vary, as they should. The core underlying metadata management system is what should be interoperable between different frontends, programs, and operating systems. A standard implementation for this should settle as development continues. This opens up the doors for improved and varied clients, integration with third-party applications, and more.
3. **The application.** If nothing else, TagStudio the application serves as the first (and so far only) implementation for this system of metadata management. This has the responsibility of doing the idea justice and showing just whatâ€™s possible when it comes to user file management.

## FAQ

### Will TagStudio move, modify, or mess with my files?

**No**, outside of _explicit_ functionality such as &quot;Move File(s) to Trash&quot;.

### Will TagStudio require me to recreate my tags or library in future updates?

**No.** It&#039;s our highest priority to ensure that your data safely and smoothly transfers over to newer versions.

### What state is the project currently in?

As of writing (Alpha v9.5.5) the project is very usable, however there&#039;s still some quirks and missing QoL features. Several additional features and changes are still planned (see: [roadmap](https://docs.tagstud.io/roadmap)) that add even more power and flexibility to the tagging and field systems while making it easier to tag in bulk and perform automated operations. Bugfixes and polishes are constantly trickling in along with the larger feature releases.

### What features are you planning on adding?

See the [roadmap](https://docs.tagstud.io/roadmap) page for the core features being planned and implemented for TagStudio. For a more up to date look on what&#039;s currently being added for upcoming releases, see our GitHub [milestones](https://github.com/TagStudioDev/TagStudio/milestones) for versioned releases.

The most important remaining features before I consider the program to be &quot;feature complete&quot; are:

-   Custom names for Fields
-   List views for files
-   Multiple root directory support for libraries
-   Improved file entry relinking
-   File entry groups
-   Sorting by file date modified and created
-   Macros
-   Improved search bar with visualized tags and improved autocomplete
-   Side panel for easier tagging (pinned tags, recent tags, tag search, tag palette)
-   Improved tag management interface
-   Improved and finalized Tag Categories
-   Fixed and improved mixed entry data displays (see: [#337](https://github.com/TagStudioDev/TagStudio/issues/337))
-   Sharable tag data
-   Separate core library + API

### What features will NOT be added?

-   Native Cloud Integration
    -   There are plenty of services already (native or third-party) that allow you to mount your cloud drives as virtual drives on your system. Hosting a TagStudio library on one of these mounts should function similarly to what native integration would look like.
    -   Supporting native cloud integrations such as these would be an unnecessary &quot;reinventing the wheel&quot; burden for us that is outside the scope of this project.
-   Native ChatGPT/Claude/Gemini/_Non-Local_ LLM Integration
    -   This could mean different things depending on your intentions. Whether it&#039;s trying to use an LLM to replace the native search, or to trying to use a model for image recognition, I&#039;m not interested in hooking people&#039;s TagStudio libraries into non-local LLMs such as ChatGPT and/or turn the program into a &quot;chatbot&quot; interface (see: [Overall Goals/Privacy](#overall-goals)).
    -   With that being said, the future TagStudio API should be well-suited to connect to any sort of service you&#039;d like, including machine learning models if so you choose. I just won&#039;t _personally_ add any native integrations with online services.

### Is a Rust port coming?

Not from us, or at least _not quite_. There are plans to break off the core TagStudio library into its own MIT-licensed module that can be used in other applications and plugins, and ideally this would be written in Rust. While I understand there&#039;s a lot of vocal support and volunteers willing to help with this, it&#039;s something that&#039;s better off coming at my/our own pace in order to ensure it&#039;s done correctly to align with the project&#039;s intentions and to remain maintainable in the future.

### Windows Defender thinks TagStudio is a virus or a trojan, why?

Unfortunately, executable Python applications &quot;compiled&quot; with something like PyInstaller are notorious for raising false positives in anti-virus software, most commonly Windows Defender (see: [#276](https://github.com/TagStudioDev/TagStudio/issues/276) and related issues). There&#039;s really not much we can do about this on our end, as the malware matches frequently change and sample submissions to Microsoft are slow and often ineffective. If you&#039;re effected by this, you may need to allow TagStudio to bypass your anti-virus software.

### Why is TagStudio already on version 9.x?

Over the first few years of private development the project went through several major iterations and rewrites. These major version bumps came quickly, and by the time TagStudio was opened-sourced the version number had already reached v9.0. Instead of resetting to &quot;v0.0&quot; or &quot;v1.0&quot; for this public release I decided to keep my v9.x numbering scheme and reserve v10.0 for when all the core features on the [roadmap](https://docs.tagstud.io/roadmap/) are implemented. Iâ€™ve also labeled this version as an &quot;Alpha&quot; and will drop this once either all of the core features are implemented or the project feels stable and feature-rich enough to be considered &quot;Beta&quot; and beyond.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[WEIFENG2333/VideoCaptioner]]></title>
            <link>https://github.com/WEIFENG2333/VideoCaptioner</link>
            <guid>https://github.com/WEIFENG2333/VideoCaptioner</guid>
            <pubDate>Sun, 14 Dec 2025 00:04:48 GMT</pubDate>
            <description><![CDATA[ğŸ¬ å¡å¡å­—å¹•åŠ©æ‰‹ | VideoCaptioner - åŸºäº LLM çš„æ™ºèƒ½å­—å¹•åŠ©æ‰‹ - è§†é¢‘å­—å¹•ç”Ÿæˆã€æ–­å¥ã€æ ¡æ­£ã€å­—å¹•ç¿»è¯‘å…¨æµç¨‹å¤„ç†ï¼- A powered tool for easy and efficient video subtitling.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/WEIFENG2333/VideoCaptioner">WEIFENG2333/VideoCaptioner</a></h1>
            <p>ğŸ¬ å¡å¡å­—å¹•åŠ©æ‰‹ | VideoCaptioner - åŸºäº LLM çš„æ™ºèƒ½å­—å¹•åŠ©æ‰‹ - è§†é¢‘å­—å¹•ç”Ÿæˆã€æ–­å¥ã€æ ¡æ­£ã€å­—å¹•ç¿»è¯‘å…¨æµç¨‹å¤„ç†ï¼- A powered tool for easy and efficient video subtitling.</p>
            <p>Language: Python</p>
            <p>Stars: 12,209</p>
            <p>Forks: 958</p>
            <p>Stars today: 30 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./legacy-docs/images/logo.png&quot;alt=&quot;VideoCaptioner Logo&quot; width=&quot;100&quot;&gt;
  &lt;p&gt;å¡å¡å­—å¹•åŠ©æ‰‹&lt;/p&gt;
  &lt;h1&gt;VideoCaptioner&lt;/h1&gt;
  &lt;p&gt;ä¸€æ¬¾åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„è§†é¢‘å­—å¹•å¤„ç†åŠ©æ‰‹ï¼Œæ”¯æŒè¯­éŸ³è¯†åˆ«ã€å­—å¹•æ–­å¥ã€ä¼˜åŒ–ã€ç¿»è¯‘å…¨æµç¨‹å¤„ç†&lt;/p&gt;

ç®€ä½“ä¸­æ–‡ / [æ­£é«”ä¸­æ–‡](./legacy-docs/README_TW.md) / [English](./legacy-docs/README_EN.md) / [æ—¥æœ¬èª](./legacy-docs/README_JA.md)

ğŸ“š **[åœ¨çº¿æ–‡æ¡£](https://weifeng2333.github.io/VideoCaptioner/)** | ğŸš€ **[å¿«é€Ÿå¼€å§‹](https://weifeng2333.github.io/VideoCaptioner/guide/getting-started)** | âš™ï¸ **[é…ç½®æŒ‡å—](https://weifeng2333.github.io/VideoCaptioner/config/llm)**

&lt;/div&gt;

## é¡¹ç›®ä»‹ç»

æ–°ç‰ˆæœ¬ï¼š

- ä¼˜åŒ– Whisper API çš„å¤„ç†
- æ”¯æŒåˆ†æ®µè¿›è¡Œå¹¶å‘è½¬å½•ï¼Œ ä»¥åŠç®—æ³•è‡ªèƒ½åˆå¹¶è½¬å½•å—
- ä¼˜åŒ–å­—å¹•ç¿»è¯‘ã€ä¼˜åŒ–ã€æ–­å¥çš„å¤„ç†æ–¹å¼ï¼Œ åˆ©ç”¨ LLM åé¦ˆå¾ªç¯ æå¤§é™ä½å‡ºé”™ç‡

å¡å¡å­—å¹•åŠ©æ‰‹ï¼ˆVideoCaptionerï¼‰æ“ä½œç®€å•ä¸”æ— éœ€é«˜é…ç½®ï¼Œæ”¯æŒç½‘ç»œè°ƒç”¨å’Œæœ¬åœ°ç¦»çº¿ï¼ˆæ”¯æŒè°ƒç”¨GPUï¼‰ä¸¤ç§æ–¹å¼è¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå­—å¹•æ™ºèƒ½æ–­å¥ã€æ ¡æ­£ã€ç¿»è¯‘ï¼Œå­—å¹•è§†é¢‘å…¨æµç¨‹ä¸€é”®å¤„ç†ã€‚ä¸ºè§†é¢‘é…ä¸Šæ•ˆæœæƒŠè‰³çš„å­—å¹•ã€‚

æœ€æ–°ç‰ˆæœ¬å·²ç»æ”¯æŒ VAD ã€äººå£°åˆ†ç¦»ã€å­—çº§æ—¶é—´æˆ³ã€æ‰¹é‡å­—å¹•ç­‰å®ç”¨åŠŸèƒ½

- æ— éœ€GPUå³å¯ä½¿ç”¨å¼ºå¤§çš„è¯­éŸ³è¯†åˆ«å¼•æ“ï¼Œç”Ÿæˆç²¾å‡†å­—å¹•
- åŸºäº LLM çš„æ™ºèƒ½åˆ†å‰²ä¸æ–­å¥ï¼Œå­—å¹•é˜…è¯»æ›´è‡ªç„¶æµç•…
- AIå­—å¹•å¤šçº¿ç¨‹ä¼˜åŒ–ä¸ç¿»è¯‘ï¼Œè°ƒæ•´å­—å¹•æ ¼å¼ã€è¡¨è¾¾æ›´åœ°é“ä¸“ä¸š
- æ”¯æŒæ‰¹é‡è§†é¢‘å­—å¹•åˆæˆï¼Œæå‡å¤„ç†æ•ˆç‡
- ç›´è§‚çš„å­—å¹•ç¼–è¾‘æŸ¥çœ‹ç•Œé¢ï¼Œæ”¯æŒå®æ—¶é¢„è§ˆå’Œå¿«æ·ç¼–è¾‘
- æ¶ˆè€—æ¨¡å‹ Token å°‘ï¼Œä¸”å†…ç½®åŸºç¡€ LLM æ¨¡å‹ï¼Œä¿è¯å¼€ç®±å³ç”¨

## ç•Œé¢é¢„è§ˆ

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://h1.appinn.me/file/1731487405884_main.png&quot; alt=&quot;è½¯ä»¶ç•Œé¢é¢„è§ˆ&quot; width=&quot;90%&quot; style=&quot;border-radius: 5px;&quot;&gt;
&lt;/div&gt;

![é¡µé¢é¢„è§ˆ](https://h1.appinn.me/file/1731487410170_preview1.png)
![é¡µé¢é¢„è§ˆ](https://h1.appinn.me/file/1731487410832_preview2.png)

## æµ‹è¯•

å…¨æµç¨‹å¤„ç†ä¸€ä¸ª14åˆ†é’Ÿ1080Pçš„ [Bç«™è‹±æ–‡ TED è§†é¢‘](https://www.bilibili.com/video/BV1jT411X7Dz)ï¼Œè°ƒç”¨æœ¬åœ° Whisper æ¨¡å‹è¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼Œä½¿ç”¨ `gpt-5-mini` æ¨¡å‹ä¼˜åŒ–å’Œç¿»è¯‘ä¸ºä¸­æ–‡ï¼Œæ€»å…±æ¶ˆè€—æ—¶é—´çº¦ **4 åˆ†é’Ÿ**ã€‚

è¿‘åå°è®¡ç®—ï¼Œæ¨¡å‹ä¼˜åŒ–å’Œç¿»è¯‘æ¶ˆè€—è´¹ç”¨ä¸è¶³ ï¿¥0.01ï¼ˆä»¥OpenAIå®˜æ–¹ä»·æ ¼ä¸ºè®¡ç®—ï¼‰

å…·ä½“å­—å¹•å’Œè§†é¢‘åˆæˆçš„æ•ˆæœçš„æµ‹è¯•ç»“æœå›¾ç‰‡ï¼Œè¯·å‚è€ƒ [TEDè§†é¢‘æµ‹è¯•](./docs/test.md)

## å¿«é€Ÿå¼€å§‹

### Windows ç”¨æˆ·

#### æ–¹å¼ä¸€ï¼šä½¿ç”¨æ‰“åŒ…ç¨‹åºï¼ˆæ¨èï¼‰

è½¯ä»¶è¾ƒä¸ºè½»é‡ï¼Œæ‰“åŒ…å¤§å°ä¸è¶³ 60M,å·²é›†æˆæ‰€æœ‰å¿…è¦ç¯å¢ƒï¼Œä¸‹è½½åå¯ç›´æ¥è¿è¡Œã€‚

1. ä» [Release](https://github.com/WEIFENG2333/VideoCaptioner/releases) é¡µé¢ä¸‹è½½æœ€æ–°ç‰ˆæœ¬çš„å¯æ‰§è¡Œç¨‹åºã€‚æˆ–è€…ï¼š[è“å¥ç›˜ä¸‹è½½](https://wwwm.lanzoue.com/ii14G2pdsbej)

2. æ‰“å¼€å®‰è£…åŒ…è¿›è¡Œå®‰è£…

3. LLM API é…ç½®ï¼Œï¼ˆç”¨äºå­—å¹•æ–­å¥ã€æ ¡æ­£ï¼‰ï¼Œå¯ä½¿ç”¨[æœ¬é¡¹ç›®çš„ä¸­è½¬ç«™](https://api.videocaptioner.cn)

4. ç¿»è¯‘é…ç½®ï¼Œé€‰æ‹©æ˜¯å¦å¯ç”¨ç¿»è¯‘ï¼Œç¿»è¯‘æœåŠ¡ï¼ˆé»˜è®¤ä½¿ç”¨å¾®è½¯ç¿»è¯‘ï¼Œè´¨é‡ä¸€èˆ¬ï¼Œæ¨èä½¿ç”¨å¤§æ¨¡å‹ç¿»è¯‘ï¼‰

5. è¯­éŸ³è¯†åˆ«é…ç½®ï¼ˆé»˜è®¤ä½¿ç”¨Bæ¥å£ï¼Œä¸­è‹±ä»¥å¤–çš„è¯­è¨€è¯·ä½¿ç”¨æœ¬åœ°è½¬å½•ï¼‰

6. æ‹–æ‹½è§†é¢‘æ–‡ä»¶åˆ°è½¯ä»¶çª—å£ï¼Œå³å¯å…¨è‡ªåŠ¨å¤„ç†

æç¤ºï¼šæ¯ä¸€ä¸ªæ­¥éª¤å‡æ”¯æŒå•ç‹¬å¤„ç†ï¼Œå‡æ”¯æŒæ–‡ä»¶æ‹–æ‹½ã€‚è½¯ä»¶å…·ä½“æ¨¡å‹é€‰æ‹©å’Œå‚æ•°é…ç½®è¯´æ˜ï¼Œè¯·æŸ¥çœ‹ä¸‹æ–‡ã€‚

### macOS / Linux ç”¨æˆ·

1. å…‹éš†é¡¹ç›®å¹¶è¿›å…¥ç›®å½•

```bash
git clone https://github.com/WEIFENG2333/VideoCaptioner.git
cd VideoCaptioner
```

2. è¿è¡Œå¯åŠ¨è„šæœ¬

```bash
chmod +x run.sh
./run.sh
```

è„šæœ¬ä¼šè‡ªåŠ¨ï¼š

- æ£€æµ‹Pythonç¯å¢ƒ
- åˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…Pythonä¾èµ–
- æ£€æµ‹ç³»ç»Ÿå·¥å…·ï¼ˆffmpegã€aria2ï¼‰
- å¯åŠ¨åº”ç”¨ç¨‹åº

**æ³¨æ„**ï¼šmacOSç”¨æˆ·éœ€è¦å…ˆå®‰è£…Homebrewã€‚

&lt;details&gt;
&lt;summary&gt;æ‰‹åŠ¨å®‰è£…æ­¥éª¤&lt;/summary&gt;


1. å®‰è£… ffmpeg å’Œ Aria2 ä¸‹è½½å·¥å…·

```bash
brew install ffmpeg
brew install aria2
brew install python@3.**
```

2. å…‹éš†é¡¹ç›®

```bash
git clone https://github.com/WEIFENG2333/VideoCaptioner.git
cd VideoCaptioner
```

3. å®‰è£…ä¾èµ–

```bash
python3.** -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

4. è¿è¡Œç¨‹åº

```bash
python main.py
```

&lt;/details&gt;

## åŸºæœ¬é…ç½®

### 1. LLM API é…ç½®è¯´æ˜

LLM å¤§æ¨¡å‹æ˜¯ç”¨æ¥å­—å¹•æ®µå¥ã€å­—å¹•ä¼˜åŒ–ã€ä»¥åŠå­—å¹•ç¿»è¯‘ï¼ˆå¦‚æœé€‰æ‹©äº†LLM å¤§æ¨¡å‹ç¿»è¯‘ï¼‰ã€‚

| é…ç½®é¡¹         | è¯´æ˜                                                                                                                                              |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------- |
| SiliconCloud   | [SiliconCloud å®˜ç½‘](https://cloud.siliconflow.cn/i/onCHcaDx)é…ç½®æ–¹æ³•è¯·å‚è€ƒ[é…ç½®æ–‡æ¡£](./docs/llm_config.md)&lt;br&gt;è¯¥å¹¶å‘è¾ƒä½ï¼Œå»ºè®®æŠŠçº¿ç¨‹è®¾ç½®ä¸º5ä»¥ä¸‹ã€‚ |
| DeepSeek       | [DeepSeek å®˜ç½‘](https://platform.deepseek.com)ï¼Œå»ºè®®ä½¿ç”¨ `deepseek-v3` æ¨¡å‹ï¼Œ&lt;br&gt;å®˜æ–¹ç½‘ç«™æœ€è¿‘æœåŠ¡å¥½åƒå¹¶ä¸å¤ªç¨³å®šã€‚                                 |
| OpenAIå…¼å®¹æ¥å£ | å¦‚æœæœ‰å…¶ä»–æœåŠ¡å•†çš„APIï¼Œå¯ç›´æ¥åœ¨è½¯ä»¶ä¸­å¡«å†™ã€‚base_url å’Œapi_key [VideoCaptioner API](https://api.videocaptioner.cn)                                 |

æ³¨ï¼šå¦‚æœç”¨çš„ API æœåŠ¡å•†ä¸æ”¯æŒé«˜å¹¶å‘ï¼Œè¯·åœ¨è½¯ä»¶è®¾ç½®ä¸­å°†â€œçº¿ç¨‹æ•°â€è°ƒä½ï¼Œé¿å…è¯·æ±‚é”™è¯¯ã€‚

---

å¦‚æœå¸Œæœ›é«˜å¹¶å‘ï¼Œæˆ–è€…å¸Œæœ›åœ¨åœ¨è½¯ä»¶å†…ä½¿ç”¨ä½¿ç”¨ OpenAI æˆ–è€… Claude ç­‰ä¼˜è´¨å¤§æ¨¡å‹è¿›è¡Œå­—å¹•æ ¡æ­£å’Œç¿»è¯‘ã€‚

å¯ä½¿ç”¨æœ¬é¡¹ç›®çš„âœ¨LLM APIä¸­è½¬ç«™âœ¨ï¼š [https://api.videocaptioner.cn](https://api.videocaptioner.cn)

å…¶æ”¯æŒé«˜å¹¶å‘ï¼Œæ€§ä»·æ¯”æé«˜ï¼Œä¸”æœ‰å›½å†…å¤–å¤§é‡æ¨¡å‹å¯æŒ‘é€‰ã€‚

æ³¨å†Œè·å–keyä¹‹åï¼Œè®¾ç½®ä¸­æŒ‰ç…§ä¸‹é¢é…ç½®ï¼š

BaseURL: `https://api.videocaptioner.cn/v1`

API-key: `ä¸ªäººä¸­å¿ƒ-API ä»¤ç‰Œé¡µé¢è‡ªè¡Œè·å–ã€‚`

ğŸ’¡ æ¨¡å‹é€‰æ‹©å»ºè®® (æœ¬äººåœ¨å„è´¨é‡å±‚çº§ä¸­ç²¾é€‰å‡ºçš„é«˜æ€§ä»·æ¯”æ¨¡å‹)ï¼š

- é«˜è´¨é‡ä¹‹é€‰ï¼š `gemini-2.5-pro`ã€`claude-sonnet-4-5-20250929` (è€—è´¹æ¯”ä¾‹ï¼š3)

- è¾ƒé«˜è´¨é‡ä¹‹é€‰ï¼š `gpt-5-2025-08-07`ã€ `claude-haiku-4-5-20251001` (è€—è´¹æ¯”ä¾‹ï¼š1.2)

- ä¸­è´¨é‡ä¹‹é€‰ï¼š `gpt-5-mini`ã€`gemini-2.5-flash` (è€—è´¹æ¯”ä¾‹ï¼š0.3)

æœ¬ç«™æ”¯æŒè¶…é«˜å¹¶å‘ï¼Œè½¯ä»¶ä¸­çº¿ç¨‹æ•°ç›´æ¥æ‹‰æ»¡å³å¯~ å¤„ç†é€Ÿåº¦éå¸¸å¿«~

æ›´è¯¦ç»†çš„APIé…ç½®æ•™ç¨‹ï¼š[ä¸­è½¬ç«™é…ç½®é…ç½®](./docs/llm_config.md#ä¸­è½¬ç«™é…ç½®)

---

## 2. ç¿»è¯‘é…ç½®

| é…ç½®é¡¹         | è¯´æ˜                                                                                                                          |
| -------------- | ----------------------------------------------------------------------------------------------------------------------------- |
| LLM å¤§æ¨¡å‹ç¿»è¯‘ | ğŸŒŸ ç¿»è¯‘è´¨é‡æœ€å¥½çš„é€‰æ‹©ã€‚ä½¿ç”¨ AI å¤§æ¨¡å‹è¿›è¡Œç¿»è¯‘,èƒ½æ›´å¥½ç†è§£ä¸Šä¸‹æ–‡,ç¿»è¯‘æ›´è‡ªç„¶ã€‚éœ€è¦åœ¨è®¾ç½®ä¸­é…ç½® LLM API(æ¯”å¦‚ OpenAIã€DeepSeek ç­‰) |
| å¾®è½¯ç¿»è¯‘       | ä½¿ç”¨å¾®è½¯çš„ç¿»è¯‘æœåŠ¡, é€Ÿåº¦éå¸¸å¿«                                                                                                |
| è°·æ­Œç¿»è¯‘       | è°·æ­Œçš„ç¿»è¯‘æœåŠ¡,é€Ÿåº¦å¿«,ä½†éœ€è¦èƒ½è®¿é—®è°·æ­Œçš„ç½‘ç»œç¯å¢ƒ                                                                              |

æ¨èä½¿ç”¨ `LLM å¤§æ¨¡å‹ç¿»è¯‘` ï¼Œç¿»è¯‘è´¨é‡æœ€å¥½ã€‚

### 3. è¯­éŸ³è¯†åˆ«æ¥å£è¯´æ˜

| æ¥å£åç§°         | æ”¯æŒè¯­è¨€                                           | è¿è¡Œæ–¹å¼ | è¯´æ˜                                                                                                              |
| ---------------- | -------------------------------------------------- | -------- | ----------------------------------------------------------------------------------------------------------------- |
| Bæ¥å£            | ä»…æ”¯æŒä¸­æ–‡ã€è‹±æ–‡                                   | åœ¨çº¿     | å…è´¹ã€é€Ÿåº¦è¾ƒå¿«                                                                                                    |
| Jæ¥å£            | ä»…æ”¯æŒä¸­æ–‡ã€è‹±æ–‡                                   | åœ¨çº¿     | å…è´¹ã€é€Ÿåº¦è¾ƒå¿«                                                                                                    |
| WhisperCpp       | ä¸­æ–‡ã€æ—¥è¯­ã€éŸ©è¯­ã€è‹±æ–‡ç­‰ 99 ç§è¯­è¨€ï¼Œå¤–è¯­æ•ˆæœè¾ƒå¥½   | æœ¬åœ°     | ï¼ˆå®é™…ä½¿ç”¨ä¸ç¨³å®šï¼‰éœ€è¦ä¸‹è½½è½¬å½•æ¨¡å‹&lt;br&gt;ä¸­æ–‡å»ºè®®mediumä»¥ä¸Šæ¨¡å‹&lt;br&gt;è‹±æ–‡ç­‰ä½¿ç”¨è¾ƒå°æ¨¡å‹å³å¯è¾¾åˆ°ä¸é”™æ•ˆæœã€‚              |
| fasterWhisper ğŸ‘ | ä¸­æ–‡ã€è‹±æ–‡ç­‰å¤š99ç§è¯­è¨€ï¼Œå¤–è¯­æ•ˆæœä¼˜ç§€ï¼Œæ—¶é—´è½´æ›´å‡†ç¡® | æœ¬åœ°     | ï¼ˆğŸŒŸæåŠ›æ¨èğŸŒŸï¼‰éœ€è¦ä¸‹è½½ç¨‹åºå’Œè½¬å½•æ¨¡å‹&lt;br&gt;æ”¯æŒCUDA,é€Ÿåº¦æ›´å¿«ï¼Œè½¬å½•å‡†ç¡®ã€‚&lt;br&gt;è¶…çº§å‡†ç¡®çš„æ—¶é—´æˆ³å­—å¹•ã€‚&lt;br&gt;å»ºè®®ä¼˜å…ˆä½¿ç”¨ |

### 4. æœ¬åœ° Whisper è¯­éŸ³è¯†åˆ«æ¨¡å‹

Whisper ç‰ˆæœ¬æœ‰ WhisperCpp å’Œ fasterWhisperï¼ˆæ¨èï¼‰ ä¸¤ç§ï¼Œåè€…æ•ˆæœæ›´å¥½ï¼Œéƒ½éœ€è¦è‡ªè¡Œåœ¨è½¯ä»¶å†…ä¸‹è½½æ¨¡å‹ã€‚

| æ¨¡å‹        | ç£ç›˜ç©ºé—´ | å†…å­˜å ç”¨ | è¯´æ˜                                |
| ----------- | -------- | -------- | ----------------------------------- |
| Tiny        | 75 MiB   | ~273 MB  | è½¬å½•å¾ˆä¸€èˆ¬ï¼Œä»…ç”¨äºæµ‹è¯•              |
| Small       | 466 MiB  | ~852 MB  | è‹±æ–‡è¯†åˆ«æ•ˆæœå·²ç»ä¸é”™                |
| Medium      | 1.5 GiB  | ~2.1 GB  | ä¸­æ–‡è¯†åˆ«å»ºè®®è‡³å°‘ä½¿ç”¨æ­¤ç‰ˆæœ¬          |
| Large-v2 ğŸ‘ | 2.9 GiB  | ~3.9 GB  | æ•ˆæœå¥½ï¼Œé…ç½®å…è®¸æƒ…å†µæ¨èä½¿ç”¨        |
| Large-v3    | 2.9 GiB  | ~3.9 GB  | ç¤¾åŒºåé¦ˆå¯èƒ½ä¼šå‡ºç°å¹»è§‰/å­—å¹•é‡å¤é—®é¢˜ |

æ¨èæ¨¡å‹: `Large-v2` ç¨³å®šä¸”è´¨é‡è¾ƒå¥½ã€‚

æ³¨ï¼šä»¥ä¸Šæ¨¡å‹å›½å†…ç½‘ç»œå¯ç›´æ¥åœ¨è½¯ä»¶å†…ä¸‹è½½ã€‚

### 5. æ–‡ç¨¿åŒ¹é…

- åœ¨&quot;å­—å¹•ä¼˜åŒ–ä¸ç¿»è¯‘&quot;é¡µé¢ï¼ŒåŒ…å«&quot;æ–‡ç¨¿åŒ¹é…&quot;é€‰é¡¹ï¼Œæ”¯æŒä»¥ä¸‹**ä¸€ç§æˆ–è€…å¤šç§**å†…å®¹ï¼Œè¾…åŠ©æ ¡æ­£å­—å¹•å’Œç¿»è¯‘:

| ç±»å‹       | è¯´æ˜                                 | å¡«å†™ç¤ºä¾‹                                                                                                                                                |
| ---------- | ------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------- |
| æœ¯è¯­è¡¨     | ä¸“ä¸šæœ¯è¯­ã€äººåã€ç‰¹å®šè¯è¯­çš„ä¿®æ­£å¯¹ç…§è¡¨ | æœºå™¨å­¦ä¹ -&gt;Machine Learning&lt;br&gt;é©¬æ–¯å…‹-&gt;Elon Musk&lt;br&gt;æ‰“call -&gt; åº”æ´&lt;br&gt;å›¾çµæ–‘å›¾&lt;br&gt;å…¬äº¤è½¦æ‚–è®º                                                             |
| åŸå­—å¹•æ–‡ç¨¿ | è§†é¢‘çš„åŸæœ‰æ–‡ç¨¿æˆ–ç›¸å…³å†…å®¹             | å®Œæ•´çš„æ¼”è®²ç¨¿ã€è¯¾ç¨‹è®²ä¹‰ç­‰                                                                                                                                |
| ä¿®æ­£è¦æ±‚   | å†…å®¹ç›¸å…³çš„å…·ä½“ä¿®æ­£è¦æ±‚               | ç»Ÿä¸€äººç§°ä»£è¯ã€è§„èŒƒä¸“ä¸šæœ¯è¯­ç­‰&lt;br&gt;å¡«å†™**å†…å®¹ç›¸å…³**çš„è¦æ±‚å³å¯ï¼Œ[ç¤ºä¾‹å‚è€ƒ](https://github.com/WEIFENG2333/VideoCaptioner/issues/59#issuecomment-2495849752) |

- å¦‚æœéœ€è¦æ–‡ç¨¿è¿›è¡Œå­—å¹•ä¼˜åŒ–è¾…åŠ©ï¼Œå…¨æµç¨‹å¤„ç†æ—¶ï¼Œå…ˆå¡«å†™æ–‡ç¨¿ä¿¡æ¯ï¼Œå†è¿›è¡Œå¼€å§‹ä»»åŠ¡å¤„ç†
- æ³¨æ„: ä½¿ç”¨ä¸Šä¸‹æ–‡å‚æ•°é‡ä¸é«˜çš„å°å‹LLMæ¨¡å‹æ—¶ï¼Œå»ºè®®æ§åˆ¶æ–‡ç¨¿å†…å®¹åœ¨1åƒå­—å†…ï¼Œå¦‚æœä½¿ç”¨ä¸Šä¸‹æ–‡è¾ƒå¤§çš„æ¨¡å‹ï¼Œåˆ™å¯ä»¥é€‚å½“å¢åŠ æ–‡ç¨¿å†…å®¹ã€‚

æ— ç‰¹æ®Šéœ€æ±‚ï¼Œå¯ä¸å¡«å†™ã€‚

### 6. Cookie é…ç½®è¯´æ˜

å¦‚æœä½¿ç”¨URLä¸‹è½½åŠŸèƒ½æ—¶ï¼Œå¦‚æœé‡åˆ°ä»¥ä¸‹æƒ…å†µ:

1. ä¸‹è½½è§†é¢‘ç½‘ç«™éœ€è¦ç™»å½•ä¿¡æ¯æ‰å¯ä»¥ä¸‹è½½ï¼›
2. åªèƒ½ä¸‹è½½è¾ƒä½åˆ†è¾¨ç‡çš„è§†é¢‘ï¼›
3. ç½‘ç»œæ¡ä»¶è¾ƒå·®æ—¶éœ€è¦éªŒè¯ï¼›

- è¯·å‚è€ƒ [Cookie é…ç½®è¯´æ˜](./docs/get_cookies.md) è·å–Cookieä¿¡æ¯ï¼Œå¹¶å°†cookies.txtæ–‡ä»¶æ”¾ç½®åˆ°è½¯ä»¶å®‰è£…ç›®å½•çš„ `AppData` ç›®å½•ä¸‹ï¼Œå³å¯æ­£å¸¸ä¸‹è½½é«˜è´¨é‡è§†é¢‘ã€‚

## è½¯ä»¶æµç¨‹ä»‹ç»

ç¨‹åºç®€å•çš„å¤„ç†æµç¨‹å¦‚ä¸‹:

```
è¯­éŸ³è¯†åˆ«è½¬å½• -&gt; å­—å¹•æ–­å¥(å¯é€‰) -&gt; å­—å¹•ä¼˜åŒ–ç¿»è¯‘(å¯é€‰) -&gt; å­—å¹•è§†é¢‘åˆæˆ
```

## è½¯ä»¶ä¸»è¦åŠŸèƒ½

è½¯ä»¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLM)åœ¨ç†è§£ä¸Šä¸‹æ–‡æ–¹é¢çš„ä¼˜åŠ¿ï¼Œå¯¹è¯­éŸ³è¯†åˆ«ç”Ÿæˆçš„å­—å¹•è¿›ä¸€æ­¥å¤„ç†ã€‚æœ‰æ•ˆä¿®æ­£é”™åˆ«å­—ã€ç»Ÿä¸€ä¸“ä¸šæœ¯è¯­ï¼Œè®©å­—å¹•å†…å®¹æ›´åŠ å‡†ç¡®è¿è´¯ï¼Œä¸ºç”¨æˆ·å¸¦æ¥å‡ºè‰²çš„è§‚çœ‹ä½“éªŒï¼

#### 1. å¤šå¹³å°è§†é¢‘ä¸‹è½½ä¸å¤„ç†

- æ”¯æŒå›½å†…å¤–ä¸»æµè§†é¢‘å¹³å°ï¼ˆBç«™ã€Youtubeã€å°çº¢ä¹¦ã€TikTokã€Xã€è¥¿ç“œè§†é¢‘ã€æŠ–éŸ³ç­‰ï¼‰
- è‡ªåŠ¨æå–è§†é¢‘åŸæœ‰å­—å¹•å¤„ç†

#### 2. ä¸“ä¸šçš„è¯­éŸ³è¯†åˆ«å¼•æ“

- æä¾›å¤šç§æ¥å£åœ¨çº¿è¯†åˆ«ï¼Œæ•ˆæœåª²ç¾å‰ªæ˜ ï¼ˆå…è´¹ã€é«˜é€Ÿï¼‰
- æ”¯æŒæœ¬åœ°Whisperæ¨¡å‹ï¼ˆä¿æŠ¤éšç§ã€å¯ç¦»çº¿ï¼‰

#### 3. å­—å¹•æ™ºèƒ½çº é”™

- è‡ªåŠ¨ä¼˜åŒ–ä¸“ä¸šæœ¯è¯­ã€ä»£ç ç‰‡æ®µå’Œæ•°å­¦å…¬å¼æ ¼å¼
- ä¸Šä¸‹æ–‡è¿›è¡Œæ–­å¥ä¼˜åŒ–ï¼Œæå‡é˜…è¯»ä½“éªŒ
- æ”¯æŒæ–‡ç¨¿æç¤ºï¼Œä½¿ç”¨åŸæœ‰æ–‡ç¨¿æˆ–è€…ç›¸å…³æç¤ºä¼˜åŒ–å­—å¹•æ–­å¥

#### 4. é«˜è´¨é‡å­—å¹•ç¿»è¯‘

- ç»“åˆä¸Šä¸‹æ–‡çš„æ™ºèƒ½ç¿»è¯‘ï¼Œç¡®ä¿è¯‘æ–‡å…¼é¡¾å…¨æ–‡
- é€šè¿‡PromptæŒ‡å¯¼å¤§æ¨¡å‹åæ€ç¿»è¯‘ï¼Œæå‡ç¿»è¯‘è´¨é‡
- ä½¿ç”¨åºåˆ—æ¨¡ç³ŠåŒ¹é…ç®—æ³•ã€ä¿è¯æ—¶é—´è½´å®Œå…¨ä¸€è‡´

#### 5. å­—å¹•æ ·å¼è°ƒæ•´

- ä¸°å¯Œçš„å­—å¹•æ ·å¼æ¨¡æ¿ï¼ˆç§‘æ™®é£ã€æ–°é—»é£ã€ç•ªå‰§é£ç­‰ç­‰ï¼‰
- å¤šç§æ ¼å¼å­—å¹•è§†é¢‘ï¼ˆSRTã€ASSã€VTTã€TXTï¼‰

é’ˆå¯¹å°ç™½ç”¨æˆ·ï¼Œå¯¹ä¸€äº›è½¯ä»¶å†…çš„é€‰é¡¹è¯´æ˜ï¼š

#### 1. è¯­éŸ³è½¬å½•é¡µé¢

- `VADè¿‡æ»¤`ï¼šå¼€å¯åï¼ŒVADï¼ˆè¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼‰å°†è¿‡æ»¤æ— äººå£°çš„è¯­éŸ³ç‰‡æ®µï¼Œä»è€Œå‡å°‘å¹»è§‰ç°è±¡ã€‚å»ºè®®ä¿æŒé»˜è®¤å¼€å¯çŠ¶æ€ã€‚å¦‚æœä¸æ‡‚ï¼Œå…¶ä»–VADé€‰é¡¹å»ºè®®ç›´æ¥ä¿æŒé»˜è®¤å³å¯ã€‚

- `éŸ³é¢‘åˆ†ç¦»`ï¼šå¼€å¯åï¼Œä½¿ç”¨MDX-Netè¿›è¡Œé™å™ªå¤„ç†ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ†ç¦»äººå£°å’ŒèƒŒæ™¯éŸ³ä¹ï¼Œä»è€Œæå‡éŸ³é¢‘è´¨é‡ã€‚å»ºè®®åªåœ¨å˜ˆæ‚çš„è§†é¢‘ä¸­å¼€å¯ã€‚

#### 2. å­—å¹•ä¼˜åŒ–ä¸ç¿»è¯‘é¡µé¢

- `æ™ºèƒ½æ–­å¥`ï¼šå¼€å¯åï¼Œå…¨æµç¨‹å¤„ç†æ—¶ç”Ÿæˆå­—çº§æ—¶é—´æˆ³ï¼Œç„¶åé€šè¿‡LLMå¤§æ¨¡å‹è¿›è¡Œæ–­å¥ï¼Œä»è€Œåœ¨è§†é¢‘æœ‰æ›´å®Œç¾çš„è§‚çœ‹ä½“éªŒã€‚æœ‰æŒ‰ç…§å¥å­æ–­å¥å’ŒæŒ‰ç…§è¯­ä¹‰æ–­å¥ä¸¤ç§æ¨¡å¼ã€‚å¯æ ¹æ®è‡ªå·±çš„éœ€æ±‚é…ç½®ã€‚

- `å­—å¹•æ ¡æ­£`ï¼šå¼€å¯åï¼Œä¼šé€šè¿‡LLMå¤§æ¨¡å‹å¯¹å­—å¹•å†…å®¹è¿›è¡Œæ ¡æ­£(å¦‚ï¼šè‹±æ–‡å•è¯å¤§å°å†™ã€æ ‡ç‚¹ç¬¦å·ã€é”™åˆ«å­—ã€æ•°å­¦å…¬å¼å’Œä»£ç çš„æ ¼å¼ç­‰)ï¼Œæå‡å­—å¹•çš„è´¨é‡ã€‚

- `åæ€ç¿»è¯‘`ï¼šå¼€å¯åï¼Œä¼šé€šè¿‡LLMå¤§æ¨¡å‹è¿›è¡Œåæ€ç¿»è¯‘ï¼Œæå‡ç¿»è¯‘çš„è´¨é‡ã€‚ç›¸åº”çš„ä¼šå¢åŠ è¯·æ±‚çš„æ—¶é—´å’Œæ¶ˆè€—çš„Tokenã€‚(é€‰é¡¹åœ¨ è®¾ç½®é¡µ-LLMå¤§æ¨¡å‹ç¿»è¯‘-åæ€ç¿»è¯‘ ä¸­å¼€å¯ã€‚)

- `æ–‡ç¨¿æç¤º`ï¼šå¡«å†™åï¼Œè¿™éƒ¨åˆ†ä¹Ÿå°†ä½œä¸ºæç¤ºè¯å‘é€ç»™å¤§æ¨¡å‹ï¼Œè¾…åŠ©å­—å¹•ä¼˜åŒ–å’Œç¿»è¯‘ã€‚

#### 3. å­—å¹•è§†é¢‘åˆæˆé¡µé¢

- `è§†é¢‘åˆæˆ`ï¼šå¼€å¯åï¼Œä¼šæ ¹æ®åˆæˆå­—å¹•è§†é¢‘ï¼›å…³é—­å°†è·³è¿‡è§†é¢‘åˆæˆçš„æµç¨‹ã€‚

- `è½¯å­—å¹•`ï¼šå¼€å¯åï¼Œå­—å¹•ä¸ä¼šçƒ§å½•åˆ°è§†é¢‘ä¸­ï¼Œå¤„ç†é€Ÿåº¦æå¿«ã€‚ä½†æ˜¯è½¯å­—å¹•éœ€è¦ä¸€äº›æ’­æ”¾å™¨ï¼ˆå¦‚PotPlayerï¼‰æ”¯æŒæ‰å¯ä»¥è¿›è¡Œæ˜¾ç¤ºæ’­æ”¾ã€‚è€Œä¸”è½¯å­—å¹•çš„æ ·å¼ä¸æ˜¯è½¯ä»¶å†…è°ƒæ•´çš„å­—å¹•æ ·å¼ï¼Œè€Œæ˜¯æ’­æ”¾å™¨é»˜è®¤çš„ç™½è‰²æ ·å¼ã€‚

é¡¹ç›®ä¸»è¦ç›®å½•ç»“æ„è¯´æ˜å¦‚ä¸‹ï¼š

```
VideoCaptioner/
â”œâ”€â”€ runtime/                    # è¿è¡Œç¯å¢ƒç›®å½•
â”œâ”€â”€ resources/                  # è½¯ä»¶èµ„æºæ–‡ä»¶ç›®å½•ï¼ˆäºŒè¿›åˆ¶ç¨‹åºã€å›¾æ ‡ç­‰,ä»¥åŠä¸‹è½½çš„faster-whisperç¨‹åºï¼‰
â”œâ”€â”€ work-dir/                   # å·¥ä½œç›®å½•ï¼Œå¤„ç†å®Œæˆçš„è§†é¢‘å’Œå­—å¹•æ–‡ä»¶ä¿å­˜åœ¨è¿™é‡Œ
â”œâ”€â”€ AppData/                    # åº”ç”¨æ•°æ®ç›®å½•
    â”œâ”€â”€ cache/                  # ç¼“å­˜ç›®å½•ï¼Œç¼“å­˜è½¬å½•ã€å¤§æ¨¡å‹è¯·æ±‚çš„æ•°æ®ã€‚
    â”œâ”€â”€ models/                 # å­˜æ”¾ Whisper æ¨¡å‹æ–‡ä»¶
    â”œâ”€â”€ logs/                   # æ—¥å¿—ç›®å½•ï¼Œè®°å½•è½¯ä»¶è¿è¡ŒçŠ¶æ€
    â”œâ”€â”€ settings.json           # å­˜å‚¨ç”¨æˆ·è®¾ç½®
    â””â”€â”€  cookies.txt            # è§†é¢‘å¹³å°çš„ cookie ä¿¡æ¯ï¼ˆä¸‹è½½é«˜æ¸…è§†é¢‘æ—¶éœ€è¦ï¼‰
â””â”€â”€ VideoCaptioner.exe          # ä¸»ç¨‹åºæ‰§è¡Œæ–‡ä»¶
```

## ğŸ“ è¯´æ˜

1. å­—å¹•æ–­å¥çš„è´¨é‡å¯¹è§‚çœ‹ä½“éªŒè‡³å…³é‡è¦ã€‚è½¯ä»¶èƒ½å°†é€å­—å­—å¹•æ™ºèƒ½é‡ç»„ä¸ºç¬¦åˆè‡ªç„¶è¯­è¨€ä¹ æƒ¯çš„æ®µè½ï¼Œå¹¶ä¸è§†é¢‘ç”»é¢å®Œç¾åŒæ­¥ã€‚

2. åœ¨å¤„ç†è¿‡ç¨‹ä¸­ï¼Œä»…å‘å¤§è¯­è¨€æ¨¡å‹å‘é€æ–‡æœ¬å†…å®¹ï¼Œä¸åŒ…å«æ—¶é—´è½´ä¿¡æ¯ï¼Œè¿™å¤§å¤§é™ä½äº†å¤„ç†å¼€é”€ã€‚

3. åœ¨ç¿»è¯‘ç¯èŠ‚ï¼Œæˆ‘ä»¬é‡‡ç”¨å´æ©è¾¾æå‡ºçš„&quot;ç¿»è¯‘-åæ€-ç¿»è¯‘&quot;æ–¹æ³•è®ºã€‚è¿™ç§è¿­ä»£ä¼˜åŒ–çš„æ–¹å¼ç¡®ä¿äº†ç¿»è¯‘çš„å‡†ç¡®æ€§ã€‚

4. å¡«å…¥ YouTube é“¾æ¥æ—¶è¿›è¡Œå¤„ç†æ—¶ï¼Œä¼šè‡ªåŠ¨ä¸‹è½½è§†é¢‘çš„å­—å¹•ï¼Œä»è€Œçœå»è½¬å½•æ­¥éª¤ï¼Œæå¤§åœ°èŠ‚çœæ“ä½œæ—¶é—´ã€‚

## ğŸ¤ è´¡çŒ®æŒ‡å—

é¡¹ç›®åœ¨ä¸æ–­å®Œå–„ä¸­ï¼Œå¦‚æœåœ¨ä½¿ç”¨è¿‡ç¨‹é‡åˆ°çš„Bugï¼Œæ¬¢è¿æäº¤ [Issue](https://github.com/WEIFENG2333/VideoCaptioner/issues) å’Œ Pull Request å¸®åŠ©æ”¹è¿›é¡¹ç›®ã€‚

## ğŸ“ æ›´æ–°æ—¥å¿—

æŸ¥çœ‹å®Œæ•´çš„æ›´æ–°å†å²ï¼Œè¯·è®¿é—® [CHANGELOG.md](./CHANGELOG.md)

## ğŸ’– æ”¯æŒä½œè€…

å¦‚æœè§‰å¾—é¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œå¯ä»¥ç»™é¡¹ç›®ç‚¹ä¸ªStarï¼

&lt;details&gt;
&lt;summary&gt;æåŠ©æ”¯æŒ&lt;/summary&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./docs/images/alipay.jpg&quot; alt=&quot;æ”¯ä»˜å®äºŒç»´ç &quot; width=&quot;30%&quot;&gt;
  &lt;img src=&quot;./docs/images/wechat.jpg&quot; alt=&quot;å¾®ä¿¡äºŒç»´ç &quot; width=&quot;30%&quot;&gt;
&lt;/div&gt;
&lt;/details&gt;

## â­ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=WEIFENG2333/VideoCaptioner&amp;type=Date)](https://star-history.com/#WEIFENG2333/VideoCaptioner&amp;Date)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>