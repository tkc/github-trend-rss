<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Mon, 31 Mar 2025 00:04:40 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[Shubhamsaboo/awesome-llm-apps]]></title>
            <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
            <guid>https://github.com/Shubhamsaboo/awesome-llm-apps</guid>
            <pubDate>Mon, 31 Mar 2025 00:04:40 GMT</pubDate>
            <description><![CDATA[Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Shubhamsaboo/awesome-llm-apps">Shubhamsaboo/awesome-llm-apps</a></h1>
            <p>Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p>
            <p>Language: Python</p>
            <p>Stars: 25,661</p>
            <p>Forks: 2,898</p>
            <p>Stars today: 1,141 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;http://www.theunwindai.com&quot;&gt;
    &lt;img src=&quot;docs/banner/unwind_black.png&quot; width=&quot;900px&quot; alt=&quot;Unwind AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.linkedin.com/in/shubhamsaboo/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;style=flat-square&quot; alt=&quot;LinkedIn&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/Saboo_Shubham_&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/Shubham_Saboo&quot; alt=&quot;Twitter&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;hr/&gt;

# üåü Awesome LLM Apps

A curated collection of awesome LLM apps built with RAG and AI agents. This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/9876&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/9876&quot; alt=&quot;Shubhamsaboo%2Fawesome-llm-apps | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## ü§î Why Awesome LLM Apps?

- üí° Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.
- üî• Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with RAG and AI Agents.
- üéì Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.

## üö® Open Source AI Agent Hackathon! üö®

We&#039;re launching a Global AI Agent Hackathon in collaboration with AI Agent ecosystem partners ‚Äî open to all developers, builders, and startups working on agents, RAG, tool use, or multi-agent systems.

### üí∞ Win up to $20,000 in cash by building Agents

- üèÖ 10 winners: $300 each
- ü•â 10 winners: $500 each
- ü•à 5 winners: $1,000 each
- ü•á 1 winner: $2,000
- üèÜ GRAND PRIZE: $5,000 üèÜ

### üéÅ Bonus
- Top 5 projects will be featured in the top trending [Awesome LLM Apps](https://github.com/Shubhamsaboo/awesome-llm-apps) repo.

### ü§ù Partners

[Unwind AI](https://www.theunwindai.com), [Agno](https://www.agno.com) and more Agent ecosystem companies joining soon.

### üìÖ Here&#039;s the timeline:

- April 3rd - Final dates revealed
- April 10th - Prize and success criteria announced
- April 15th (tentative) - Hackathon starts
- May 30th (tentative) - Hackathon ends

Join us for a month of building Agents!

&gt; Prizes will be distributed on an ongoing basis and continue till all prizes are awarded.

‚≠ê Star this repo and follow along to stay updated.

### ü§ù Want to join us as a partner or judge?

If you&#039;re a company in the AI agent ecosystem or would like to judge the hackathon, reach out to [Shubham Saboo](https://x.com/Saboo_Shubham_) or [Ashpreet Bedi](https://x.com/ashpreetbedi) on X to partner. Let‚Äôs make this the biggest open source AI Agent hackathon.

## üìÇ Featured AI Projects

### AI Agents
- [üíº AI Customer Support Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_customer_support_agent)
- [üìà AI Investment Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_investment_agent)
- [üë®‚Äç‚öñÔ∏è AI Legal Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_legal_agent_team)
- [üíº AI Recruitment Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_recruitment_agent_team)
- [üë®‚Äçüíº AI Services Agency](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_services_agency)
- [üß≤ AI Competitor Intelligence Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_competitor_intelligence_agent_team)
- [üèãÔ∏è‚Äç‚ôÇÔ∏è AI Health &amp; Fitness Planner Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_health_fitness_agent)
- [üìà AI Startup Trend Analysis Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_startup_trend_analysis_agent)
- [üóûÔ∏è AI Journalist Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_journalist_agent)
- [üí≤ AI Finance Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_finance_agent_team)
- [üéØ AI Lead Generation Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_lead_generation_agent)
- [üí∞ AI Personal Finance Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_personal_finance_agent)
- [ü©ª AI Medical Scan Diagnosis Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_medical_imaging_agent)
- [üë®‚Äçüè´ AI Teaching Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_teaching_agent_team)
- [üõ´ AI Travel Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_travel_agent)
- [üé¨ AI Movie Production Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_movie_production_agent)
- [üì∞ Multi-Agent AI Researcher](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/multi_agent_researcher)
- [üíª Multimodal AI Coding Agent Team with o3-mini and Gemini](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_coding_agent_o3-mini)
- [üìë AI Meeting Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_meeting_agent)
- [‚ôú AI Chess Agent Game](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_chess_agent)
- [üè† AI Real Estate Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_real_estate_agent)
- [üåê Local News Agent OpenAI Swarm](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/local_news_agent_openai_swarm)
- [üìä AI Finance Agent with xAI Grok](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/xai_finance_agent)
- [üéÆ AI 3D PyGame Visualizer with DeepSeek R1](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_3dpygame_r1)
- [üß† AI Reasoning Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_reasoning_agent)
- [üß¨ Multimodal AI Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/multimodal_ai_agent)

### RAG (Retrieval Augmented Generation)
- [üîç Autonomous RAG](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/autonomous_rag)
- [üîó Agentic RAG](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/agentic_rag)
- [ü§î Agentic RAG with Gemini Flash Thinking](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/gemini_agentic_rag)
- [üêã Deepseek Local RAG Reasoning Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/deepseek_local_rag_agent)
- [üîÑ Llama3.1 Local RAG](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/llama3.1_local_rag)
- [üß© RAG-as-a-Service](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/rag-as-a-service)
- [ü¶ô Local RAG Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/local_rag_agent)
- [üëÄ RAG App with Hybrid Search](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/hybrid_search_rag)
- [üñ•Ô∏è Local RAG App with Hybrid Search](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/local_hybrid_search_rag)
- [üì† RAG Agent with Database Routing](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/rag_database_routing)
- [üîÑ Corrective RAG Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/corrective_rag)

### MCP AI Agents
- [üêô MCP GitHub Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/mcp_ai_agents/github_mcp_agent)

### LLM Apps with Memory
- [üíæ AI Arxiv Agent with Memory](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory)
- [üìù LLM App with Personalized Memory](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/llm_app_personalized_memory)
- [üõ©Ô∏è AI Travel Agent with Memory](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/ai_travel_agent_memory)
- [üóÑÔ∏è Local ChatGPT with Memory](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/local_chatgpt_with_memory)

### Chat with X
- [üí¨ Chat with GitHub Repo](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_github)
- [üì® Chat with Gmail](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_gmail)
- [üìÑ Chat with PDF](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_pdf)
- [üìö Chat with Research Papers](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_research_papers)
- [üìù Chat with Substack Newsletter](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_substack)
- [üìΩÔ∏è Chat with YouTube Videos](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_youtube_videos)

### LLM Finetuning
- [üåê Llama3.2 Finetuning](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_finetuning_tutorials/llama3.2_finetuning)

### Advanced Tools and Frameworks
- [üß™ Gemini Multimodal Chatbot](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/gemini_multimodal_chatbot)
- [üîÑ Mixture of Agents](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/mixture_of_agents)
- [üåê MultiLLM Chat Playground](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/multillm_chat_playground)
- [üîó LLM Router App](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/llm_router_app)
- [üí¨ Local ChatGPT Clone](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/local_chatgpt_clone)
- [üåç Web Scraping AI Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/web_scrapping_ai_agent)
- [üîç Web Search AI Assistant](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/web_search_ai_assistant)
- [üß™ Cursor AI Experiments](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/cursor_ai_experiments)

## üöÄ Getting Started

1. **Clone the repository** 

    ```bash 
    git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
    ```

2. **Navigate to the desired project directory**

    ```bash 
    cd awesome-llm-apps/chat_with_X_tutorials/chat_with_gmail
    ```

3. **Install the required dependencies**

    ```bash
    pip install -r requirements.txt
    ```

4. **Follow the project-specific instructions** in each project&#039;s `README.md` file to set up and run the app.

## ü§ù Contributing to Open Source

Contributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new [GitHub Issue](https://github.com/Shubhamsaboo/awesome-llm-apps/issues) or submit a pull request. Make sure to follow the existing project structure and include a detailed `README.md` for each new app.

### Thank You, Community, for the Support! üôè

[![Star History Chart](https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;type=Date)](https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;Date)

üåü **Don‚Äôt miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[causify-ai/helpers]]></title>
            <link>https://github.com/causify-ai/helpers</link>
            <guid>https://github.com/causify-ai/helpers</guid>
            <pubDate>Mon, 31 Mar 2025 00:04:39 GMT</pubDate>
            <description><![CDATA[Causify development system]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/causify-ai/helpers">causify-ai/helpers</a></h1>
            <p>Causify development system</p>
            <p>Language: Python</p>
            <p>Stars: 95</p>
            <p>Forks: 104</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre># helpers
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[AUTOMATIC1111/stable-diffusion-webui]]></title>
            <link>https://github.com/AUTOMATIC1111/stable-diffusion-webui</link>
            <guid>https://github.com/AUTOMATIC1111/stable-diffusion-webui</guid>
            <pubDate>Mon, 31 Mar 2025 00:04:38 GMT</pubDate>
            <description><![CDATA[Stable Diffusion web UI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">AUTOMATIC1111/stable-diffusion-webui</a></h1>
            <p>Stable Diffusion web UI</p>
            <p>Language: Python</p>
            <p>Stars: 150,299</p>
            <p>Forks: 28,000</p>
            <p>Stars today: 58 stars today</p>
            <h2>README</h2><pre># Stable Diffusion web UI
A web interface for Stable Diffusion, implemented using Gradio library.

![](screenshot.png)

## Features
[Detailed feature showcase with images](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features):
- Original txt2img and img2img modes
- One click install and run script (but you still must install python and git)
- Outpainting
- Inpainting
- Color Sketch
- Prompt Matrix
- Stable Diffusion Upscale
- Attention, specify parts of text that the model should pay more attention to
    - a man in a `((tuxedo))` - will pay more attention to tuxedo
    - a man in a `(tuxedo:1.21)` - alternative syntax
    - select text and press `Ctrl+Up` or `Ctrl+Down` (or `Command+Up` or `Command+Down` if you&#039;re on a MacOS) to automatically adjust attention to selected text (code contributed by anonymous user)
- Loopback, run img2img processing multiple times
- X/Y/Z plot, a way to draw a 3 dimensional plot of images with different parameters
- Textual Inversion
    - have as many embeddings as you want and use any names you like for them
    - use multiple embeddings with different numbers of vectors per token
    - works with half precision floating point numbers
    - train embeddings on 8GB (also reports of 6GB working)
- Extras tab with:
    - GFPGAN, neural network that fixes faces
    - CodeFormer, face restoration tool as an alternative to GFPGAN
    - RealESRGAN, neural network upscaler
    - ESRGAN, neural network upscaler with a lot of third party models
    - SwinIR and Swin2SR ([see here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/2092)), neural network upscalers
    - LDSR, Latent diffusion super resolution upscaling
- Resizing aspect ratio options
- Sampling method selection
    - Adjust sampler eta values (noise multiplier)
    - More advanced noise setting options
- Interrupt processing at any time
- 4GB video card support (also reports of 2GB working)
- Correct seeds for batches
- Live prompt token length validation
- Generation parameters
     - parameters you used to generate images are saved with that image
     - in PNG chunks for PNG, in EXIF for JPEG
     - can drag the image to PNG info tab to restore generation parameters and automatically copy them into UI
     - can be disabled in settings
     - drag and drop an image/text-parameters to promptbox
- Read Generation Parameters Button, loads parameters in promptbox to UI
- Settings page
- Running arbitrary python code from UI (must run with `--allow-code` to enable)
- Mouseover hints for most UI elements
- Possible to change defaults/mix/max/step values for UI elements via text config
- Tiling support, a checkbox to create images that can be tiled like textures
- Progress bar and live image generation preview
    - Can use a separate neural network to produce previews with almost none VRAM or compute requirement
- Negative prompt, an extra text field that allows you to list what you don&#039;t want to see in generated image
- Styles, a way to save part of prompt and easily apply them via dropdown later
- Variations, a way to generate same image but with tiny differences
- Seed resizing, a way to generate same image but at slightly different resolution
- CLIP interrogator, a button that tries to guess prompt from an image
- Prompt Editing, a way to change prompt mid-generation, say to start making a watermelon and switch to anime girl midway
- Batch Processing, process a group of files using img2img
- Img2img Alternative, reverse Euler method of cross attention control
- Highres Fix, a convenience option to produce high resolution pictures in one click without usual distortions
- Reloading checkpoints on the fly
- Checkpoint Merger, a tab that allows you to merge up to 3 checkpoints into one
- [Custom scripts](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Custom-Scripts) with many extensions from community
- [Composable-Diffusion](https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/), a way to use multiple prompts at once
     - separate prompts using uppercase `AND`
     - also supports weights for prompts: `a cat :1.2 AND a dog AND a penguin :2.2`
- No token limit for prompts (original stable diffusion lets you use up to 75 tokens)
- DeepDanbooru integration, creates danbooru style tags for anime prompts
- [xformers](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers), major speed increase for select cards: (add `--xformers` to commandline args)
- via extension: [History tab](https://github.com/yfszzx/stable-diffusion-webui-images-browser): view, direct and delete images conveniently within the UI
- Generate forever option
- Training tab
     - hypernetworks and embeddings options
     - Preprocessing images: cropping, mirroring, autotagging using BLIP or deepdanbooru (for anime)
- Clip skip
- Hypernetworks
- Loras (same as Hypernetworks but more pretty)
- A separate UI where you can choose, with preview, which embeddings, hypernetworks or Loras to add to your prompt
- Can select to load a different VAE from settings screen
- Estimated completion time in progress bar
- API
- Support for dedicated [inpainting model](https://github.com/runwayml/stable-diffusion#inpainting-with-stable-diffusion) by RunwayML
- via extension: [Aesthetic Gradients](https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients), a way to generate images with a specific aesthetic by using clip images embeds (implementation of [https://github.com/vicgalle/stable-diffusion-aesthetic-gradients](https://github.com/vicgalle/stable-diffusion-aesthetic-gradients))
- [Stable Diffusion 2.0](https://github.com/Stability-AI/stablediffusion) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#stable-diffusion-20) for instructions
- [Alt-Diffusion](https://arxiv.org/abs/2211.06679) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#alt-diffusion) for instructions
- Now without any bad letters!
- Load checkpoints in safetensors format
- Eased resolution restriction: generated image&#039;s dimensions must be a multiple of 8 rather than 64
- Now with a license!
- Reorder elements in the UI from settings screen
- [Segmind Stable Diffusion](https://huggingface.co/segmind/SSD-1B) support

## Installation and Running
Make sure the required [dependencies](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Dependencies) are met and follow the instructions available for:
- [NVidia](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs) (recommended)
- [AMD](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-AMD-GPUs) GPUs.
- [Intel CPUs, Intel GPUs (both integrated and discrete)](https://github.com/openvinotoolkit/stable-diffusion-webui/wiki/Installation-on-Intel-Silicon) (external wiki page)
- [Ascend NPUs](https://github.com/wangshuai09/stable-diffusion-webui/wiki/Install-and-run-on-Ascend-NPUs) (external wiki page)

Alternatively, use online services (like Google Colab):

- [List of Online Services](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services)

### Installation on Windows 10/11 with NVidia-GPUs using release package
1. Download `sd.webui.zip` from [v1.0.0-pre](https://github.com/AUTOMATIC1111/stable-diffusion-webui/releases/tag/v1.0.0-pre) and extract its contents.
2. Run `update.bat`.
3. Run `run.bat`.
&gt; For more details see [Install-and-Run-on-NVidia-GPUs](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs)

### Automatic Installation on Windows
1. Install [Python 3.10.6](https://www.python.org/downloads/release/python-3106/) (Newer version of Python does not support torch), checking &quot;Add Python to PATH&quot;.
2. Install [git](https://git-scm.com/download/win).
3. Download the stable-diffusion-webui repository, for example by running `git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git`.
4. Run `webui-user.bat` from Windows Explorer as normal, non-administrator, user.

### Automatic Installation on Linux
1. Install the dependencies:
```bash
# Debian-based:
sudo apt install wget git python3 python3-venv libgl1 libglib2.0-0
# Red Hat-based:
sudo dnf install wget git python3 gperftools-libs libglvnd-glx
# openSUSE-based:
sudo zypper install wget git python3 libtcmalloc4 libglvnd
# Arch-based:
sudo pacman -S wget git python3
```
If your system is very new, you need to install python3.11 or python3.10:
```bash
# Ubuntu 24.04
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt update
sudo apt install python3.11

# Manjaro/Arch
sudo pacman -S yay
yay -S python311 # do not confuse with python3.11 package

# Only for 3.11
# Then set up env variable in launch script
export python_cmd=&quot;python3.11&quot;
# or in webui-user.sh
python_cmd=&quot;python3.11&quot;
```
2. Navigate to the directory you would like the webui to be installed and execute the following command:
```bash
wget -q https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh
```
Or just clone the repo wherever you want:
```bash
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui
```

3. Run `webui.sh`.
4. Check `webui-user.sh` for options.
### Installation on Apple Silicon

Find the instructions [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Installation-on-Apple-Silicon).

## Contributing
Here&#039;s how to add code to this repo: [Contributing](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)

## Documentation

The documentation was moved from this README over to the project&#039;s [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki).

For the purposes of getting Google and other search engines to crawl the wiki, here&#039;s a link to the (not for humans) [crawlable wiki](https://github-wiki-see.page/m/AUTOMATIC1111/stable-diffusion-webui/wiki).

## Credits
Licenses for borrowed code can be found in `Settings -&gt; Licenses` screen, and also in `html/licenses.html` file.

- Stable Diffusion - https://github.com/Stability-AI/stablediffusion, https://github.com/CompVis/taming-transformers, https://github.com/mcmonkey4eva/sd3-ref
- k-diffusion - https://github.com/crowsonkb/k-diffusion.git
- Spandrel - https://github.com/chaiNNer-org/spandrel implementing
  - GFPGAN - https://github.com/TencentARC/GFPGAN.git
  - CodeFormer - https://github.com/sczhou/CodeFormer
  - ESRGAN - https://github.com/xinntao/ESRGAN
  - SwinIR - https://github.com/JingyunLiang/SwinIR
  - Swin2SR - https://github.com/mv-lab/swin2sr
- LDSR - https://github.com/Hafiidz/latent-diffusion
- MiDaS - https://github.com/isl-org/MiDaS
- Ideas for optimizations - https://github.com/basujindal/stable-diffusion
- Cross Attention layer optimization - Doggettx - https://github.com/Doggettx/stable-diffusion, original idea for prompt editing.
- Cross Attention layer optimization - InvokeAI, lstein - https://github.com/invoke-ai/InvokeAI (originally http://github.com/lstein/stable-diffusion)
- Sub-quadratic Cross Attention layer optimization - Alex Birch (https://github.com/Birch-san/diffusers/pull/1), Amin Rezaei (https://github.com/AminRezaei0x443/memory-efficient-attention)
- Textual Inversion - Rinon Gal - https://github.com/rinongal/textual_inversion (we&#039;re not using his code, but we are using his ideas).
- Idea for SD upscale - https://github.com/jquesnelle/txt2imghd
- Noise generation for outpainting mk2 - https://github.com/parlance-zz/g-diffuser-bot
- CLIP interrogator idea and borrowing some code - https://github.com/pharmapsychotic/clip-interrogator
- Idea for Composable Diffusion - https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch
- xformers - https://github.com/facebookresearch/xformers
- DeepDanbooru - interrogator for anime diffusers https://github.com/KichangKim/DeepDanbooru
- Sampling in float32 precision from a float16 UNet - marunine for the idea, Birch-san for the example Diffusers implementation (https://github.com/Birch-san/diffusers-play/tree/92feee6)
- Instruct pix2pix - Tim Brooks (star), Aleksander Holynski (star), Alexei A. Efros (no star) - https://github.com/timothybrooks/instruct-pix2pix
- Security advice - RyotaK
- UniPC sampler - Wenliang Zhao - https://github.com/wl-zhao/UniPC
- TAESD - Ollin Boer Bohan - https://github.com/madebyollin/taesd
- LyCORIS - KohakuBlueleaf
- Restart sampling - lambertae - https://github.com/Newbeeer/diffusion_restart_sampling
- Hypertile - tfernd - https://github.com/tfernd/HyperTile
- Initial Gradio script - posted on 4chan by an Anonymous user. Thank you Anonymous user.
- (You)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jlowin/fastmcp]]></title>
            <link>https://github.com/jlowin/fastmcp</link>
            <guid>https://github.com/jlowin/fastmcp</guid>
            <pubDate>Mon, 31 Mar 2025 00:04:37 GMT</pubDate>
            <description><![CDATA[The fast, Pythonic way to build Model Context Protocol servers üöÄ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jlowin/fastmcp">jlowin/fastmcp</a></h1>
            <p>The fast, Pythonic way to build Model Context Protocol servers üöÄ</p>
            <p>Language: Python</p>
            <p>Stars: 2,497</p>
            <p>Forks: 126</p>
            <p>Stars today: 318 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

### üéâ FastMCP has been added to the official MCP SDK! üéâ

You can now find FastMCP as part of the official Model Context Protocol Python SDK:

üëâ [github.com/modelcontextprotocol/python-sdk](https://github.com/modelcontextprotocol/python-sdk)

*Please note: this repository is no longer maintained.*

---


&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

&lt;!-- omit in toc --&gt;
# FastMCP üöÄ
&lt;strong&gt;The fast, Pythonic way to build MCP servers.&lt;/strong&gt;

[![PyPI - Version](https://img.shields.io/pypi/v/fastmcp.svg)](https://pypi.org/project/fastmcp)
[![Tests](https://github.com/jlowin/fastmcp/actions/workflows/run-tests.yml/badge.svg)](https://github.com/jlowin/fastmcp/actions/workflows/run-tests.yml)
[![License](https://img.shields.io/github/license/jlowin/fastmcp.svg)](https://github.com/jlowin/fastmcp/blob/main/LICENSE)


&lt;/div&gt;

[Model Context Protocol (MCP)](https://modelcontextprotocol.io) servers are a new, standardized way to provide context and tools to your LLMs, and FastMCP makes building MCP servers simple and intuitive. Create tools, expose resources, and define prompts with clean, Pythonic code:

```python
# demo.py

from fastmcp import FastMCP


mcp = FastMCP(&quot;Demo üöÄ&quot;)


@mcp.tool()
def add(a: int, b: int) -&gt; int:
    &quot;&quot;&quot;Add two numbers&quot;&quot;&quot;
    return a + b
```

That&#039;s it! Give Claude access to the server by running:

```bash
fastmcp install demo.py
```

FastMCP handles all the complex protocol details and server management, so you can focus on building great tools. It&#039;s designed to be high-level and Pythonic - in most cases, decorating a function is all you need.


### Key features:
* **Fast**: High-level interface means less code and faster development
* **Simple**: Build MCP servers with minimal boilerplate
* **Pythonic**: Feels natural to Python developers
* **Complete***: FastMCP aims to provide a full implementation of the core MCP specification

(\*emphasis on *aims*)

üö® üöß üèóÔ∏è *FastMCP is under active development, as is the MCP specification itself. Core features are working but some advanced capabilities are still in progress.* 


&lt;!-- omit in toc --&gt;
## Table of Contents

- [Installation](#installation)
- [Quickstart](#quickstart)
- [What is MCP?](#what-is-mcp)
- [Core Concepts](#core-concepts)
  - [Server](#server)
  - [Resources](#resources)
  - [Tools](#tools)
  - [Prompts](#prompts)
  - [Images](#images)
  - [Context](#context)
- [Running Your Server](#running-your-server)
  - [Development Mode (Recommended for Building \&amp; Testing)](#development-mode-recommended-for-building--testing)
  - [Claude Desktop Integration (For Regular Use)](#claude-desktop-integration-for-regular-use)
  - [Direct Execution (For Advanced Use Cases)](#direct-execution-for-advanced-use-cases)
  - [Server Object Names](#server-object-names)
- [Examples](#examples)
  - [Echo Server](#echo-server)
  - [SQLite Explorer](#sqlite-explorer)
- [Contributing](#contributing)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation-1)
  - [Testing](#testing)
  - [Formatting](#formatting)
  - [Opening a Pull Request](#opening-a-pull-request)

## Installation

We strongly recommend installing FastMCP with [uv](https://docs.astral.sh/uv/), as it is required for deploying servers:

```bash
uv pip install fastmcp
```

Note: on macOS, uv may need to be installed with Homebrew (`brew install uv`) in order to make it available to the Claude Desktop app.

Alternatively, to use the SDK without deploying, you may use pip:

```bash
pip install fastmcp
```

## Quickstart

Let&#039;s create a simple MCP server that exposes a calculator tool and some data:

```python
# server.py

from fastmcp import FastMCP


# Create an MCP server
mcp = FastMCP(&quot;Demo&quot;)


# Add an addition tool
@mcp.tool()
def add(a: int, b: int) -&gt; int:
    &quot;&quot;&quot;Add two numbers&quot;&quot;&quot;
    return a + b


# Add a dynamic greeting resource
@mcp.resource(&quot;greeting://{name}&quot;)
def get_greeting(name: str) -&gt; str:
    &quot;&quot;&quot;Get a personalized greeting&quot;&quot;&quot;
    return f&quot;Hello, {name}!&quot;
```

You can install this server in [Claude Desktop](https://claude.ai/download) and interact with it right away by running:
```bash
fastmcp install server.py
```

Alternatively, you can test it with the MCP Inspector:
```bash
fastmcp dev server.py
```

![MCP Inspector](/docs/assets/demo-inspector.png)

## What is MCP?

The [Model Context Protocol (MCP)](https://modelcontextprotocol.io) lets you build servers that expose data and functionality to LLM applications in a secure, standardized way. Think of it like a web API, but specifically designed for LLM interactions. MCP servers can:

- Expose data through **Resources** (think of these sort of like GET endpoints; they are used to load information into the LLM&#039;s context)
- Provide functionality through **Tools** (sort of like POST endpoints; they are used to execute code or otherwise produce a side effect)
- Define interaction patterns through **Prompts** (reusable templates for LLM interactions)
- And more!

There is a low-level [Python SDK](https://github.com/modelcontextprotocol/python-sdk) available for implementing the protocol directly, but FastMCP aims to make that easier by providing a high-level, Pythonic interface.

## Core Concepts


### Server

The FastMCP server is your core interface to the MCP protocol. It handles connection management, protocol compliance, and message routing:

```python
from fastmcp import FastMCP

# Create a named server
mcp = FastMCP(&quot;My App&quot;)

# Specify dependencies for deployment and development
mcp = FastMCP(&quot;My App&quot;, dependencies=[&quot;pandas&quot;, &quot;numpy&quot;])
```

### Resources

Resources are how you expose data to LLMs. They&#039;re similar to GET endpoints in a REST API - they provide data but shouldn&#039;t perform significant computation or have side effects. Some examples:

- File contents
- Database schemas
- API responses
- System information

Resources can be static:
```python
@mcp.resource(&quot;config://app&quot;)
def get_config() -&gt; str:
    &quot;&quot;&quot;Static configuration data&quot;&quot;&quot;
    return &quot;App configuration here&quot;
```

Or dynamic with parameters (FastMCP automatically handles these as MCP templates):
```python
@mcp.resource(&quot;users://{user_id}/profile&quot;)
def get_user_profile(user_id: str) -&gt; str:
    &quot;&quot;&quot;Dynamic user data&quot;&quot;&quot;
    return f&quot;Profile data for user {user_id}&quot;
```

### Tools

Tools let LLMs take actions through your server. Unlike resources, tools are expected to perform computation and have side effects. They&#039;re similar to POST endpoints in a REST API.

Simple calculation example:
```python
@mcp.tool()
def calculate_bmi(weight_kg: float, height_m: float) -&gt; float:
    &quot;&quot;&quot;Calculate BMI given weight in kg and height in meters&quot;&quot;&quot;
    return weight_kg / (height_m ** 2)
```

HTTP request example:
```python
import httpx

@mcp.tool()
async def fetch_weather(city: str) -&gt; str:
    &quot;&quot;&quot;Fetch current weather for a city&quot;&quot;&quot;
    async with httpx.AsyncClient() as client:
        response = await client.get(
            f&quot;https://api.weather.com/{city}&quot;
        )
        return response.text
```

Complex input handling example:
```python
from pydantic import BaseModel, Field
from typing import Annotated

class ShrimpTank(BaseModel):
    class Shrimp(BaseModel):
        name: Annotated[str, Field(max_length=10)]

    shrimp: list[Shrimp]

@mcp.tool()
def name_shrimp(
    tank: ShrimpTank,
    # You can use pydantic Field in function signatures for validation.
    extra_names: Annotated[list[str], Field(max_length=10)],
) -&gt; list[str]:
    &quot;&quot;&quot;List all shrimp names in the tank&quot;&quot;&quot;
    return [shrimp.name for shrimp in tank.shrimp] + extra_names
```

### Prompts

Prompts are reusable templates that help LLMs interact with your server effectively. They&#039;re like &quot;best practices&quot; encoded into your server. A prompt can be as simple as a string:

```python
@mcp.prompt()
def review_code(code: str) -&gt; str:
    return f&quot;Please review this code:\n\n{code}&quot;
```

Or a more structured sequence of messages:
```python
from fastmcp.prompts.base import UserMessage, AssistantMessage

@mcp.prompt()
def debug_error(error: str) -&gt; list[Message]:
    return [
        UserMessage(&quot;I&#039;m seeing this error:&quot;),
        UserMessage(error),
        AssistantMessage(&quot;I&#039;ll help debug that. What have you tried so far?&quot;)
    ]
```


### Images

FastMCP provides an `Image` class that automatically handles image data in your server:

```python
from fastmcp import FastMCP, Image
from PIL import Image as PILImage

@mcp.tool()
def create_thumbnail(image_path: str) -&gt; Image:
    &quot;&quot;&quot;Create a thumbnail from an image&quot;&quot;&quot;
    img = PILImage.open(image_path)
    img.thumbnail((100, 100))
    
    # FastMCP automatically handles conversion and MIME types
    return Image(data=img.tobytes(), format=&quot;png&quot;)

@mcp.tool()
def load_image(path: str) -&gt; Image:
    &quot;&quot;&quot;Load an image from disk&quot;&quot;&quot;
    # FastMCP handles reading and format detection
    return Image(path=path)
```

Images can be used as the result of both tools and resources.

### Context

The Context object gives your tools and resources access to MCP capabilities. To use it, add a parameter annotated with `fastmcp.Context`:

```python
from fastmcp import FastMCP, Context

@mcp.tool()
async def long_task(files: list[str], ctx: Context) -&gt; str:
    &quot;&quot;&quot;Process multiple files with progress tracking&quot;&quot;&quot;
    for i, file in enumerate(files):
        ctx.info(f&quot;Processing {file}&quot;)
        await ctx.report_progress(i, len(files))
        
        # Read another resource if needed
        data = await ctx.read_resource(f&quot;file://{file}&quot;)
        
    return &quot;Processing complete&quot;
```

The Context object provides:
- Progress reporting through `report_progress()`
- Logging via `debug()`, `info()`, `warning()`, and `error()`
- Resource access through `read_resource()`
- Request metadata via `request_id` and `client_id`

## Running Your Server

There are three main ways to use your FastMCP server, each suited for different stages of development:

### Development Mode (Recommended for Building &amp; Testing)

The fastest way to test and debug your server is with the MCP Inspector:

```bash
fastmcp dev server.py
```

This launches a web interface where you can:
- Test your tools and resources interactively
- See detailed logs and error messages
- Monitor server performance
- Set environment variables for testing

During development, you can:
- Add dependencies with `--with`: 
  ```bash
  fastmcp dev server.py --with pandas --with numpy
  ```
- Mount your local code for live updates:
  ```bash
  fastmcp dev server.py --with-editable .
  ```

### Claude Desktop Integration (For Regular Use)

Once your server is ready, install it in Claude Desktop to use it with Claude:

```bash
fastmcp install server.py
```

Your server will run in an isolated environment with:
- Automatic installation of dependencies specified in your FastMCP instance:
  ```python
  mcp = FastMCP(&quot;My App&quot;, dependencies=[&quot;pandas&quot;, &quot;numpy&quot;])
  ```
- Custom naming via `--name`:
  ```bash
  fastmcp install server.py --name &quot;My Analytics Server&quot;
  ```
- Environment variable management:
  ```bash
  # Set variables individually
  fastmcp install server.py -e API_KEY=abc123 -e DB_URL=postgres://...
  
  # Or load from a .env file
  fastmcp install server.py -f .env
  ```

### Direct Execution (For Advanced Use Cases)

For advanced scenarios like custom deployments or running without Claude, you can execute your server directly:

```python
from fastmcp import FastMCP

mcp = FastMCP(&quot;My App&quot;)

if __name__ == &quot;__main__&quot;:
    mcp.run()
```

Run it with:
```bash
# Using the FastMCP CLI
fastmcp run server.py

# Or with Python/uv directly
python server.py
uv run python server.py
```


Note: When running directly, you are responsible for ensuring all dependencies are available in your environment. Any dependencies specified on the FastMCP instance are ignored.

Choose this method when you need:
- Custom deployment configurations
- Integration with other services
- Direct control over the server lifecycle

### Server Object Names

All FastMCP commands will look for a server object called `mcp`, `app`, or `server` in your file. If you have a different object name or multiple servers in one file, use the syntax `server.py:my_server`:

```bash
# Using a standard name
fastmcp run server.py

# Using a custom name
fastmcp run server.py:my_custom_server
```

## Examples

Here are a few examples of FastMCP servers. For more, see the `examples/` directory.

### Echo Server
A simple server demonstrating resources, tools, and prompts:

```python
from fastmcp import FastMCP

mcp = FastMCP(&quot;Echo&quot;)

@mcp.resource(&quot;echo://{message}&quot;)
def echo_resource(message: str) -&gt; str:
    &quot;&quot;&quot;Echo a message as a resource&quot;&quot;&quot;
    return f&quot;Resource echo: {message}&quot;

@mcp.tool()
def echo_tool(message: str) -&gt; str:
    &quot;&quot;&quot;Echo a message as a tool&quot;&quot;&quot;
    return f&quot;Tool echo: {message}&quot;

@mcp.prompt()
def echo_prompt(message: str) -&gt; str:
    &quot;&quot;&quot;Create an echo prompt&quot;&quot;&quot;
    return f&quot;Please process this message: {message}&quot;
```

### SQLite Explorer
A more complex example showing database integration:

```python
from fastmcp import FastMCP
import sqlite3

mcp = FastMCP(&quot;SQLite Explorer&quot;)

@mcp.resource(&quot;schema://main&quot;)
def get_schema() -&gt; str:
    &quot;&quot;&quot;Provide the database schema as a resource&quot;&quot;&quot;
    conn = sqlite3.connect(&quot;database.db&quot;)
    schema = conn.execute(
        &quot;SELECT sql FROM sqlite_master WHERE type=&#039;table&#039;&quot;
    ).fetchall()
    return &quot;\n&quot;.join(sql[0] for sql in schema if sql[0])

@mcp.tool()
def query_data(sql: str) -&gt; str:
    &quot;&quot;&quot;Execute SQL queries safely&quot;&quot;&quot;
    conn = sqlite3.connect(&quot;database.db&quot;)
    try:
        result = conn.execute(sql).fetchall()
        return &quot;\n&quot;.join(str(row) for row in result)
    except Exception as e:
        return f&quot;Error: {str(e)}&quot;

@mcp.prompt()
def analyze_table(table: str) -&gt; str:
    &quot;&quot;&quot;Create a prompt template for analyzing tables&quot;&quot;&quot;
    return f&quot;&quot;&quot;Please analyze this database table:
Table: {table}
Schema: 
{get_schema()}

What insights can you provide about the structure and relationships?&quot;&quot;&quot;
```

## Contributing

&lt;details&gt;

&lt;summary&gt;&lt;h3&gt;Open Developer Guide&lt;/h3&gt;&lt;/summary&gt;

### Prerequisites

FastMCP requires Python 3.10+ and [uv](https://docs.astral.sh/uv/).

### Installation

For development, we recommend installing FastMCP with development dependencies, which includes various utilities the maintainers find useful.

```bash
git clone https://github.com/jlowin/fastmcp.git
cd fastmcp
uv sync --frozen --extra dev
```

For running tests only (e.g., in CI), you only need the testing dependencies:

```bash
uv sync --frozen --extra tests
```

### Testing

Please make sure to test any new functionality. Your tests should be simple and atomic and anticipate change rather than cement complex patterns.

Run tests from the root directory:


```bash
pytest -vv
```

### Formatting

FastMCP enforces a variety of required formats, which you can automatically enforce with pre-commit. 

Install the pre-commit hooks:

```bash
pre-commit install
```

The hooks will now run on every commit (as well as on every PR). To run them manually:

```bash
pre-commit run --all-files
```

### Opening a Pull Request

Fork the repository and create a new branch:

```bash
git checkout -b my-branch
```

Make your changes and commit them:


```bash
git add . &amp;&amp; git commit -m &quot;My changes&quot;
```

Push your changes to your fork:


```bash
git push origin my-branch
```

Feel free to reach out in a GitHub issue or discussion if you have any questions!

&lt;/details&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[blazickjp/arxiv-mcp-server]]></title>
            <link>https://github.com/blazickjp/arxiv-mcp-server</link>
            <guid>https://github.com/blazickjp/arxiv-mcp-server</guid>
            <pubDate>Mon, 31 Mar 2025 00:04:36 GMT</pubDate>
            <description><![CDATA[A Model Context Protocol server for searching and analyzing arXiv papers]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/blazickjp/arxiv-mcp-server">blazickjp/arxiv-mcp-server</a></h1>
            <p>A Model Context Protocol server for searching and analyzing arXiv papers</p>
            <p>Language: Python</p>
            <p>Stars: 335</p>
            <p>Forks: 32</p>
            <p>Stars today: 85 stars today</p>
            <h2>README</h2><pre>[![Twitter Follow](https://img.shields.io/twitter/follow/JoeBlazick?style=social)](https://twitter.com/JoeBlazick)
[![smithery badge](https://smithery.ai/badge/arxiv-mcp-server)](https://smithery.ai/server/arxiv-mcp-server)
[![Python Version](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PyPI Downloads](https://img.shields.io/pypi/dm/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)
[![PyPI Version](https://img.shields.io/pypi/v/arxiv-mcp-server.svg)](https://pypi.org/project/arxiv-mcp-server/)

# ArXiv MCP Server

&gt; üîç Enable AI assistants to search and access arXiv papers through a simple MCP interface.

The ArXiv MCP Server provides a bridge between AI assistants and arXiv&#039;s research repository through the Message Control Protocol (MCP). It allows AI models to search for papers and access their content in a programmatic way.

&lt;div align=&quot;center&quot;&gt;
  
ü§ù **[Contribute](https://github.com/blazickjp/arxiv-mcp-server/blob/main/CONTRIBUTING.md)** ‚Ä¢ 
üìù **[Report Bug](https://github.com/blazickjp/arxiv-mcp-server/issues)**

&lt;/div&gt;

## ‚ú® Core Features

- üîé **Paper Search**: Query arXiv papers with filters for date ranges and categories
- üìÑ **Paper Access**: Download and read paper content
- üìã **Paper Listing**: View all downloaded papers
- üóÉÔ∏è **Local Storage**: Papers are saved locally for faster access
- üìù **Prompts**: A Set of Research Prompts

## üöÄ Quick Start

### Installing via Smithery

To install ArXiv Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/arxiv-mcp-server):

```bash
npx -y @smithery/cli install arxiv-mcp-server --client claude
```

### Installing Manually
Install using uv:

```bash
uv tool install arxiv-mcp-server
```

For development:

```bash
# Clone and set up development environment
git clone https://github.com/blazickjp/arxiv-mcp-server.git
cd arxiv-mcp-server

# Create and activate virtual environment
uv venv
source .venv/bin/activate

# Install with test dependencies
uv pip install -e &quot;.[test]&quot;
```

### üîå MCP Integration

Add this configuration to your MCP client config file:

```json
{
    &quot;mcpServers&quot;: {
        &quot;arxiv-mcp-server&quot;: {
            &quot;command&quot;: &quot;uv&quot;,
            &quot;args&quot;: [
                &quot;tool&quot;,
                &quot;run&quot;,
                &quot;arxiv-mcp-server&quot;,
                &quot;--storage-path&quot;, &quot;/path/to/paper/storage&quot;
            ]
        }
    }
}
```

For Development:

```json
{
    &quot;mcpServers&quot;: {
        &quot;arxiv-mcp-server&quot;: {
            &quot;command&quot;: &quot;uv&quot;,
            &quot;args&quot;: [
                &quot;--directory&quot;,
                &quot;path/to/cloned/arxiv-mcp-server&quot;,
                &quot;run&quot;,
                &quot;arxiv-mcp-server&quot;,
                &quot;--storage-path&quot;, &quot;/path/to/paper/storage&quot;
            ]
        }
    }
}
```

## üí° Available Tools

The server provides four main tools:

### 1. Paper Search
Search for papers with optional filters:

```python
result = await call_tool(&quot;search_papers&quot;, {
    &quot;query&quot;: &quot;transformer architecture&quot;,
    &quot;max_results&quot;: 10,
    &quot;date_from&quot;: &quot;2023-01-01&quot;,
    &quot;categories&quot;: [&quot;cs.AI&quot;, &quot;cs.LG&quot;]
})
```

### 2. Paper Download
Download a paper by its arXiv ID:

```python
result = await call_tool(&quot;download_paper&quot;, {
    &quot;paper_id&quot;: &quot;2401.12345&quot;
})
```

### 3. List Papers
View all downloaded papers:

```python
result = await call_tool(&quot;list_papers&quot;, {})
```

### 4. Read Paper
Access the content of a downloaded paper:

```python
result = await call_tool(&quot;read_paper&quot;, {
    &quot;paper_id&quot;: &quot;2401.12345&quot;
})
```

## üìù Research Prompts

The server offers specialized prompts to help analyze academic papers:

### Paper Analysis Prompt
A comprehensive workflow for analyzing academic papers that only requires a paper ID:

```python
result = await call_prompt(&quot;deep-paper-analysis&quot;, {
    &quot;paper_id&quot;: &quot;2401.12345&quot;
})
```

This prompt includes:
- Detailed instructions for using available tools (list_papers, download_paper, read_paper, search_papers)
- A systematic workflow for paper analysis
- Comprehensive analysis structure covering:
  - Executive summary
  - Research context
  - Methodology analysis
  - Results evaluation
  - Practical and theoretical implications
  - Future research directions
  - Broader impacts

## ‚öôÔ∏è Configuration

Configure through environment variables:

| Variable | Purpose | Default |
|----------|---------|---------|
| `ARXIV_STORAGE_PATH` | Paper storage location | ~/.arxiv-mcp-server/papers |

## üß™ Testing

Run the test suite:

```bash
python -m pytest
```

## üìÑ License

Released under the MIT License. See the LICENSE file for details.

---

&lt;div align=&quot;center&quot;&gt;

Made with ‚ù§Ô∏è by the Pearl Labs Team

&lt;a href=&quot;https://glama.ai/mcp/servers/04dtxi5i5n&quot;&gt;&lt;img width=&quot;380&quot; height=&quot;200&quot; src=&quot;https://glama.ai/mcp/servers/04dtxi5i5n/badge&quot; alt=&quot;ArXiv Server MCP server&quot; /&gt;&lt;/a&gt;
&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[basicmachines-co/basic-memory]]></title>
            <link>https://github.com/basicmachines-co/basic-memory</link>
            <guid>https://github.com/basicmachines-co/basic-memory</guid>
            <pubDate>Mon, 31 Mar 2025 00:04:35 GMT</pubDate>
            <description><![CDATA[Basic Memory is a knowledge management system that allows you to build a persistent semantic graph from conversations with AI assistants. All knowledge is stored in standard Markdown files on your computer, giving you full control and ownership of your data. Integrates directly with Obsidan.md]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/basicmachines-co/basic-memory">basicmachines-co/basic-memory</a></h1>
            <p>Basic Memory is a knowledge management system that allows you to build a persistent semantic graph from conversations with AI assistants. All knowledge is stored in standard Markdown files on your computer, giving you full control and ownership of your data. Integrates directly with Obsidan.md</p>
            <p>Language: Python</p>
            <p>Stars: 280</p>
            <p>Forks: 22</p>
            <p>Stars today: 32 stars today</p>
            <h2>README</h2><pre>[![License: AGPL v3](https://img.shields.io/badge/License-AGPL_v3-blue.svg)](https://www.gnu.org/licenses/agpl-3.0)
[![PyPI version](https://badge.fury.io/py/basic-memory.svg)](https://badge.fury.io/py/basic-memory)
[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![Tests](https://github.com/basicmachines-co/basic-memory/workflows/Tests/badge.svg)](https://github.com/basicmachines-co/basic-memory/actions)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
![](https://badge.mcpx.dev?type=server &#039;MCP Server&#039;)
![](https://badge.mcpx.dev?type=dev &#039;MCP Dev&#039;)
[![smithery badge](https://smithery.ai/badge/@basicmachines-co/basic-memory)](https://smithery.ai/server/@basicmachines-co/basic-memory)

# Basic Memory

Basic Memory lets you build persistent knowledge through natural conversations with Large Language Models (LLMs) like
Claude, while keeping everything in simple Markdown files on your computer. It uses the Model Context Protocol (MCP) to
enable any compatible LLM to read and write to your local knowledge base.

- Website: http://basicmachines.co
- Documentation: http://memory.basicmachines.co

## Pick up your conversation right where you left off

- AI assistants can load context from local files in a new conversation
- Notes are saved locally as Markdown files in real time
- No project knowledge or special prompting required

https://github.com/user-attachments/assets/a55d8238-8dd0-454a-be4c-8860dbbd0ddc

## Quick Start

```bash
# Install with uv (recommended)
uv tool install basic-memory

# Configure Claude Desktop (edit ~/Library/Application Support/Claude/claude_desktop_config.json)
# Add this to your config:
{
  &quot;mcpServers&quot;: {
    &quot;basic-memory&quot;: {
      &quot;command&quot;: &quot;uvx&quot;,
      &quot;args&quot;: [
        &quot;basic-memory&quot;,
        &quot;mcp&quot;
      ]
    }
  }
}
# Now in Claude Desktop, you can:
# - Write notes with &quot;Create a note about coffee brewing methods&quot;
# - Read notes with &quot;What do I know about pour over coffee?&quot;
# - Search with &quot;Find information about Ethiopian beans&quot;

```

You can view shared context via files in `~/basic-memory` (default directory location).

### Alternative Installation via Smithery

You can use [Smithery](https://smithery.ai/server/@basicmachines-co/basic-memory) to automatically configure Basic
Memory for Claude Desktop:

```bash
npx -y @smithery/cli install @basicmachines-co/basic-memory --client claude
```

This installs and configures Basic Memory without requiring manual edits to the Claude Desktop configuration file. The
Smithery server hosts the MCP server component, while your data remains stored locally as Markdown files.

### Glama.ai

&lt;a href=&quot;https://glama.ai/mcp/servers/o90kttu9ym&quot;&gt;
  &lt;img width=&quot;380&quot; height=&quot;200&quot; src=&quot;https://glama.ai/mcp/servers/o90kttu9ym/badge&quot; alt=&quot;basic-memory MCP server&quot; /&gt;
&lt;/a&gt;

## Why Basic Memory?

Most LLM interactions are ephemeral - you ask a question, get an answer, and everything is forgotten. Each conversation
starts fresh, without the context or knowledge from previous ones. Current workarounds have limitations:

- Chat histories capture conversations but aren&#039;t structured knowledge
- RAG systems can query documents but don&#039;t let LLMs write back
- Vector databases require complex setups and often live in the cloud
- Knowledge graphs typically need specialized tools to maintain

Basic Memory addresses these problems with a simple approach: structured Markdown files that both humans and LLMs can
read
and write to. The key advantages:

- **Local-first:** All knowledge stays in files you control
- **Bi-directional:** Both you and the LLM read and write to the same files
- **Structured yet simple:** Uses familiar Markdown with semantic patterns
- **Traversable knowledge graph:** LLMs can follow links between topics
- **Standard formats:** Works with existing editors like Obsidian
- **Lightweight infrastructure:** Just local files indexed in a local SQLite database

With Basic Memory, you can:

- Have conversations that build on previous knowledge
- Create structured notes during natural conversations
- Have conversations with LLMs that remember what you&#039;ve discussed before
- Navigate your knowledge graph semantically
- Keep everything local and under your control
- Use familiar tools like Obsidian to view and edit notes
- Build a personal knowledge base that grows over time

## How It Works in Practice

Let&#039;s say you&#039;re exploring coffee brewing methods and want to capture your knowledge. Here&#039;s how it works:

1. Start by chatting normally:

```
I&#039;ve been experimenting with different coffee brewing methods. Key things I&#039;ve learned:

- Pour over gives more clarity in flavor than French press
- Water temperature is critical - around 205¬∞F seems best
- Freshly ground beans make a huge difference
```

... continue conversation.

2. Ask the LLM to help structure this knowledge:

```
&quot;Let&#039;s write a note about coffee brewing methods.&quot;
```

LLM creates a new Markdown file on your system (which you can see instantly in Obsidian or your editor):

```markdown
---
title: Coffee Brewing Methods
permalink: coffee-brewing-methods
tags:
- coffee
- brewing
---

# Coffee Brewing Methods

## Observations

- [method] Pour over provides more clarity and highlights subtle flavors
- [technique] Water temperature at 205¬∞F (96¬∞C) extracts optimal compounds
- [principle] Freshly ground beans preserve aromatics and flavor

## Relations

- relates_to [[Coffee Bean Origins]]
- requires [[Proper Grinding Technique]]
- affects [[Flavor Extraction]]
```

The note embeds semantic content and links to other topics via simple Markdown formatting.

3. You see this file on your computer in real time in the current project directory (default `~/$HOME/basic-memory`).

- Realtime sync can be enabled via running `basic-memory sync --watch`

4. In a chat with the LLM, you can reference a topic:

```
Look at `coffee-brewing-methods` for context about pour over coffee
```

The LLM can now build rich context from the knowledge graph. For example:

```
Following relation &#039;relates_to [[Coffee Bean Origins]]&#039;:
- Found information about Ethiopian Yirgacheffe
- Notes on Colombian beans&#039; nutty profile
- Altitude effects on bean characteristics

Following relation &#039;requires [[Proper Grinding Technique]]&#039;:
- Burr vs. blade grinder comparisons
- Grind size recommendations for different methods
- Impact of consistent particle size on extraction
```

Each related document can lead to more context, building a rich semantic understanding of your knowledge base.

This creates a two-way flow where:

- Humans write and edit Markdown files
- LLMs read and write through the MCP protocol
- Sync keeps everything consistent
- All knowledge stays in local files.

## Technical Implementation

Under the hood, Basic Memory:

1. Stores everything in Markdown files
2. Uses a SQLite database for searching and indexing
3. Extracts semantic meaning from simple Markdown patterns
    - Files become `Entity` objects
    - Each `Entity` can have `Observations`, or facts associated with it
    - `Relations` connect entities together to form the knowledge graph
4. Maintains the local knowledge graph derived from the files
5. Provides bidirectional synchronization between files and the knowledge graph
6. Implements the Model Context Protocol (MCP) for AI integration
7. Exposes tools that let AI assistants traverse and manipulate the knowledge graph
8. Uses memory:// URLs to reference entities across tools and conversations

The file format is just Markdown with some simple markup:

Each Markdown file has:

### Frontmatter

```markdown
title: &lt;Entity title&gt;
type: &lt;The type of Entity&gt; (e.g. note)
permalink: &lt;a uri slug&gt;

- &lt;optional metadata&gt; (such as tags) 
```

### Observations

Observations are facts about a topic.
They can be added by creating a Markdown list with a special format that can reference a `category`, `tags` using a
&quot;#&quot; character, and an optional `context`.

Observation Markdown format:

```markdown
- [category] content #tag (optional context)
```

Examples of observations:

```markdown
- [method] Pour over extracts more floral notes than French press
- [tip] Grind size should be medium-fine for pour over #brewing
- [preference] Ethiopian beans have bright, fruity flavors (especially from Yirgacheffe)
- [fact] Lighter roasts generally contain more caffeine than dark roasts
- [experiment] Tried 1:15 coffee-to-water ratio with good results
- [resource] James Hoffman&#039;s V60 technique on YouTube is excellent
- [question] Does water temperature affect extraction of different compounds differently?
- [note] My favorite local shop uses a 30-second bloom time
```

### Relations

Relations are links to other topics. They define how entities connect in the knowledge graph.

Markdown format:

```markdown
- relation_type [[WikiLink]] (optional context)
```

Examples of relations:

```markdown
- pairs_well_with [[Chocolate Desserts]]
- grown_in [[Ethiopia]]
- contrasts_with [[Tea Brewing Methods]]
- requires [[Burr Grinder]]
- improves_with [[Fresh Beans]]
- relates_to [[Morning Routine]]
- inspired_by [[Japanese Coffee Culture]]
- documented_in [[Coffee Journal]]
```

## Using with Claude Desktop

Basic Memory is built using the MCP (Model Context Protocol) and works with the Claude desktop app (https://claude.ai/):

1. Configure Claude Desktop to use Basic Memory:

Edit your MCP configuration file (usually located at `~/Library/Application Support/Claude/claude_desktop_config.json`
for OS X):

```json
{
  &quot;mcpServers&quot;: {
    &quot;basic-memory&quot;: {
      &quot;command&quot;: &quot;uvx&quot;,
      &quot;args&quot;: [
        &quot;basic-memory&quot;,
        &quot;mcp&quot;
      ]
    }
  }
}
```

If you want to use a specific project (see [Multiple Projects](docs/User%20Guide.md#multiple-projects)), update your
Claude Desktop
config:

```json
{
  &quot;mcpServers&quot;: {
    &quot;basic-memory&quot;: {
      &quot;command&quot;: &quot;uvx&quot;,
      &quot;args&quot;: [
        &quot;basic-memory&quot;,
        &quot;mcp&quot;,
        &quot;--project&quot;,
        &quot;your-project-name&quot;
      ]
    }
  }
}
```

2. Sync your knowledge:

```bash
# One-time sync of local knowledge updates
basic-memory sync

# Run realtime sync process (recommended)
basic-memory sync --watch
```

3. In Claude Desktop, the LLM can now use these tools:

```
write_note(title, content, folder, tags) - Create or update notes
read_note(identifier, page, page_size) - Read notes by title or permalink
build_context(url, depth, timeframe) - Navigate knowledge graph via memory:// URLs
search_notes(query, page, page_size) - Search across your knowledge base
recent_activity(type, depth, timeframe) - Find recently updated information
canvas(nodes, edges, title, folder) - Generate knowledge visualizations
```

5. Example prompts to try:

```
&quot;Create a note about our project architecture decisions&quot;
&quot;Find information about JWT authentication in my notes&quot;
&quot;Create a canvas visualization of my project components&quot;
&quot;Read my notes on the authentication system&quot;
&quot;What have I been working on in the past week?&quot;
```

## Futher info

See the [Documentation](https://memory.basicmachines.co/) for more info, including:

- [Complete User Guide](https://memory.basicmachines.co/docs/user-guide)
- [CLI tools](https://memory.basicmachines.co/docs/cli-reference)
- [Managing multiple Projects](https://memory.basicmachines.co/docs/cli-reference#project)
- [Importing data from OpenAI/Claude Projects](https://memory.basicmachines.co/docs/cli-reference#import)

## License

AGPL-3.0

Contributions are welcome. See the [Contributing](CONTRIBUTING.md) guide for info about setting up the project locally
and submitting PRs.

## Star History

&lt;a href=&quot;https://www.star-history.com/#basicmachines-co/basic-memory&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=basicmachines-co/basic-memory&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=basicmachines-co/basic-memory&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=basicmachines-co/basic-memory&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;

Built with ‚ô•Ô∏è by Basic Machines
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[getzep/graphiti]]></title>
            <link>https://github.com/getzep/graphiti</link>
            <guid>https://github.com/getzep/graphiti</guid>
            <pubDate>Mon, 31 Mar 2025 00:04:34 GMT</pubDate>
            <description><![CDATA[Build Real-Time Knowledge Graphs for AI Agents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/getzep/graphiti">getzep/graphiti</a></h1>
            <p>Build Real-Time Knowledge Graphs for AI Agents</p>
            <p>Language: Python</p>
            <p>Stars: 3,042</p>
            <p>Forks: 236</p>
            <p>Stars today: 70 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.getzep.com/&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73&quot; width=&quot;150&quot; alt=&quot;Zep Logo&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt;
Graphiti
&lt;/h1&gt;
&lt;h2 align=&quot;center&quot;&gt; Build Real-Time Knowledge Graphs for AI Agents&lt;/h2&gt;
&lt;br /&gt;

[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)
[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)
[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)
[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)
[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/getzep/Graphiti)

:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_
&lt;br /&gt;

Graphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.

Use Graphiti to:

- Integrate and maintain dynamic user interactions and business data.
- Facilitate state-based reasoning and task automation for agents.
- Query complex, evolving data with semantic, keyword, and graph-based search methods.

&lt;br /&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;images/graphiti-graph-intro.gif&quot; alt=&quot;Graphiti temporal walkthrough&quot; width=&quot;700px&quot;&gt;   
&lt;/p&gt;

&lt;br /&gt;

A knowledge graph is a network of interconnected facts, such as _‚ÄúKendra loves Adidas shoes.‚Äù_ Each fact is a ‚Äútriplet‚Äù represented by two entities, or
nodes (_‚ÄùKendra‚Äù_, _‚ÄúAdidas shoes‚Äù_), and their relationship, or edge (_‚Äùloves‚Äù_). Knowledge Graphs have been explored
extensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph
while handling changing relationships and maintaining historical context.

## Graphiti and Zep Memory

Graphiti powers the core of [Zep&#039;s memory layer](https://www.getzep.com) for AI Agents.

Using Graphiti, we&#039;ve demonstrated Zep is
the [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).

Read our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).

We&#039;re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2501.13956&quot;&gt;&lt;img src=&quot;images/arxiv-screenshot.png&quot; alt=&quot;Zep: A Temporal Knowledge Graph Architecture for Agent Memory&quot; width=&quot;700px&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

## Why Graphiti?

Traditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:

- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.
- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.
- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.
- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.
- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/images/graphiti-intro-slides-stock-2.gif&quot; alt=&quot;Graphiti structured + unstructured demo&quot; width=&quot;700px&quot;&gt;   
&lt;/p&gt;

## Graphiti vs. GraphRAG

| Aspect                     | GraphRAG                              | Graphiti                                         |
| -------------------------- | ------------------------------------- | ------------------------------------------------ |
| **Primary Use**            | Static document summarization         | Dynamic data management                          |
| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |
| **Knowledge Structure**    | Entity clusters &amp; community summaries | Episodic data, semantic entities, communities    |
| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |
| **Adaptability**           | Low                                   | High                                             |
| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |
| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |
| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |
| **Custom Entity Types**    | No                                    | Yes, customizable                                |
| **Scalability**            | Moderate                              | High, optimized for large datasets               |

Graphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.

## Installation

Requirements:

- Python 3.10 or higher
- Neo4j 5.26 or higher (serves as the embeddings storage backend)
- OpenAI API key (for LLM inference and embedding)

Optional:

- Anthropic or Groq API key (for alternative LLM providers)

&gt; [!TIP]
&gt; The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly
&gt; interface to manage Neo4j instances and databases.

```bash
pip install graphiti-core
```

or

```bash
poetry add graphiti-core
```

## Quick Start

&gt; [!IMPORTANT]
&gt; Graphiti uses OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your environment.
&gt; Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI
&gt; compatible APIs.

```python
from graphiti_core import Graphiti
from graphiti_core.nodes import EpisodeType
from datetime import datetime, timezone

# Initialize Graphiti as Your Memory Layer
graphiti = Graphiti(&quot;bolt://localhost:7687&quot;, &quot;neo4j&quot;, &quot;password&quot;)

# Initialize the graph database with Graphiti&#039;s indices. This only needs to be done once.
graphiti.build_indices_and_constraints()

# Add episodes
episodes = [
    &quot;Kamala Harris is the Attorney General of California. She was previously &quot;
    &quot;the district attorney for San Francisco.&quot;,
    &quot;As AG, Harris was in office from January 3, 2011 ‚Äì January 3, 2017&quot;,
]
for i, episode in enumerate(episodes):
    await graphiti.add_episode(
        name=f&quot;Freakonomics Radio {i}&quot;,
        episode_body=episode,
        source=EpisodeType.text,
        source_description=&quot;podcast&quot;,
        reference_time=datetime.now(timezone.utc)
    )

# Search the graph for semantic memory retrieval
# Execute a hybrid search combining semantic similarity and BM25 retrieval
# Results are combined and reranked using Reciprocal Rank Fusion
results = await graphiti.search(&#039;Who was the California Attorney General?&#039;)
[
    EntityEdge(
‚îÇ   uuid = &#039;3133258f738e487383f07b04e15d4ac0&#039;,
‚îÇ   source_node_uuid = &#039;2a85789b318d4e418050506879906e62&#039;,
‚îÇ   target_node_uuid = &#039;baf7781f445945989d6e4f927f881556&#039;,
‚îÇ   created_at = datetime.datetime(2024, 8, 26, 13, 13, 24, 861097),
‚îÇ   name = &#039;HELD_POSITION&#039;,
# the fact reflects the updated state that Harris is
# no longer the AG of California
‚îÇ   fact = &#039;Kamala Harris was the Attorney General of California&#039;,
‚îÇ   fact_embedding = [
‚îÇ   ‚îÇ   -0.009955154731869698,
‚îÇ       ...
‚îÇ   ‚îÇ   0.00784289836883545
‚îÇ],
‚îÇ   episodes = [&#039;b43e98ad0a904088a76c67985caecc22&#039;],
‚îÇ   expired_at = datetime.datetime(2024, 8, 26, 20, 18, 1, 53812),
# These dates represent the date this edge was true.
‚îÇ   valid_at = datetime.datetime(2011, 1, 3, 0, 0, tzinfo= &lt; UTC &gt;),
‚îÇ   invalid_at = datetime.datetime(2017, 1, 3, 0, 0, tzinfo= &lt; UTC &gt;)
)
]

# Rerank search results based on graph distance
# Provide a node UUID to prioritize results closer to that node in the graph.
# Results are weighted by their proximity, with distant edges receiving lower scores.
await graphiti.search(&#039;Who was the California Attorney General?&#039;, center_node_uuid)

# Close the connection when chat state management is complete
graphiti.close()
```

## Graph Service

The `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.

Please see the [server README](./server/README.md) for more information.

## MCP Server

The `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti&#039;s knowledge graph capabilities through the MCP protocol.

Key features of the MCP server include:

- Episode management (add, retrieve, delete)
- Entity management and relationship handling
- Semantic and hybrid search capabilities
- Group management for organizing related data
- Graph maintenance operations

The MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.

For detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md).

## Optional Environment Variables

In addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.
If you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables
must be set.

`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish
to enable Neo4j&#039;s parallel runtime feature for several of our search queries.
Note that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,
as such this feature is off by default.

## Using Graphiti with Azure OpenAI

Graphiti supports Azure OpenAI for both LLM inference and embeddings. To use Azure OpenAI, you&#039;ll need to configure both the LLM client and embedder with your Azure OpenAI credentials.

```python
from openai import AsyncAzureOpenAI
from graphiti_core import Graphiti
from graphiti_core.llm_client import OpenAIClient
from graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig
from graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient

# Azure OpenAI configuration
api_key = &quot;&lt;your-api-key&gt;&quot;
api_version = &quot;&lt;your-api-version&gt;&quot;
azure_endpoint = &quot;&lt;your-azure-endpoint&gt;&quot;

# Create Azure OpenAI client for LLM
azure_openai_client = AsyncAzureOpenAI(
    api_key=api_key,
    api_version=api_version,
    azure_endpoint=azure_endpoint
)

# Initialize Graphiti with Azure OpenAI clients
graphiti = Graphiti(
    &quot;bolt://localhost:7687&quot;,
    &quot;neo4j&quot;,
    &quot;password&quot;,
    llm_client=OpenAIClient(
        client=azure_openai_client
    ),
    embedder=OpenAIEmbedder(
        config=OpenAIEmbedderConfig(
            embedding_model=&quot;text-embedding-3-small&quot;  # Use your Azure deployed embedding model name
        ),
        client=azure_openai_client
    ),
    # Optional: Configure the OpenAI cross encoder with Azure OpenAI
    cross_encoder=OpenAIRerankerClient(
        client=azure_openai_client
    )
)

# Now you can use Graphiti with Azure OpenAI
```

Make sure to replace the placeholder values with your actual Azure OpenAI credentials and specify the correct embedding model name that&#039;s deployed in your Azure OpenAI service.

## Documentation

- [Guides and API documentation](https://help.getzep.com/graphiti).
- [Quick Start](https://help.getzep.com/graphiti/graphiti/quick-start)
- [Building an agent with LangChain&#039;s LangGraph and Graphiti](https://help.getzep.com/graphiti/graphiti/lang-graph-agent)

## Status and Roadmap

Graphiti is under active development. We aim to maintain API stability while working on:

- [x] Supporting custom graph schemas:
  - Allow developers to provide their own defined node and edge classes when ingesting episodes
  - Enable more flexible knowledge representation tailored to specific use cases
- [x] Enhancing retrieval capabilities with more robust and configurable options
- [x] Graphiti MCP Server
- [ ] Expanding test coverage to ensure reliability and catch edge cases

## Contributing

We encourage and appreciate all forms of contributions, whether it&#039;s code, documentation, addressing GitHub Issues, or
answering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer
to [CONTRIBUTING](CONTRIBUTING.md).

## Support

Join the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[lllyasviel/Fooocus]]></title>
            <link>https://github.com/lllyasviel/Fooocus</link>
            <guid>https://github.com/lllyasviel/Fooocus</guid>
            <pubDate>Mon, 31 Mar 2025 00:04:33 GMT</pubDate>
            <description><![CDATA[Focus on prompting and generating]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lllyasviel/Fooocus">lllyasviel/Fooocus</a></h1>
            <p>Focus on prompting and generating</p>
            <p>Language: Python</p>
            <p>Stars: 44,066</p>
            <p>Forks: 6,713</p>
            <p>Stars today: 23 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[commaai/openpilot]]></title>
            <link>https://github.com/commaai/openpilot</link>
            <guid>https://github.com/commaai/openpilot</guid>
            <pubDate>Mon, 31 Mar 2025 00:04:32 GMT</pubDate>
            <description><![CDATA[openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/commaai/openpilot">commaai/openpilot</a></h1>
            <p>openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.</p>
            <p>Language: Python</p>
            <p>Stars: 52,984</p>
            <p>Forks: 9,582</p>
            <p>Stars today: 8 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; style=&quot;text-align: center;&quot;&gt;

&lt;h1&gt;openpilot&lt;/h1&gt;

&lt;p&gt;
  &lt;b&gt;openpilot is an operating system for robotics.&lt;/b&gt;
  &lt;br&gt;
  Currently, it upgrades the driver assistance system in 275+ supported cars.
&lt;/p&gt;

&lt;h3&gt;
  &lt;a href=&quot;https://docs.comma.ai&quot;&gt;Docs&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
  &lt;a href=&quot;https://docs.comma.ai/contributing/roadmap/&quot;&gt;Roadmap&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
  &lt;a href=&quot;https://github.com/commaai/openpilot/blob/master/docs/CONTRIBUTING.md&quot;&gt;Contribute&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
  &lt;a href=&quot;https://discord.comma.ai&quot;&gt;Community&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
  &lt;a href=&quot;https://comma.ai/shop&quot;&gt;Try it on a comma 3X&lt;/a&gt;
&lt;/h3&gt;

Quick start: `bash &lt;(curl -fsSL openpilot.comma.ai)`

![openpilot tests](https://github.com/commaai/openpilot/actions/workflows/selfdrive_tests.yaml/badge.svg)
[![codecov](https://codecov.io/gh/commaai/openpilot/branch/master/graph/badge.svg)](https://codecov.io/gh/commaai/openpilot)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![X Follow](https://img.shields.io/twitter/follow/comma_ai)](https://x.com/comma_ai)
[![Discord](https://img.shields.io/discord/469524606043160576)](https://discord.comma.ai)

&lt;/div&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/NmBfgOanCyk&quot; title=&quot;Video By Greer Viau&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/2f7112ae-f748-4f39-b617-fabd689c3772&quot;&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/VHKyqZ7t8Gw&quot; title=&quot;Video By Logan LeGrand&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/92351544-2833-40d7-9e0b-7ef7ae37ec4c&quot;&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/SUIZYzxtMQs&quot; title=&quot;A drive to Taco Bell&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/05ceefc5-2628-439c-a9b2-89ce77dc6f63&quot;&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;


Using openpilot in a car
------

To use openpilot in a car, you need four things:
1. **Supported Device:** a comma 3/3X, available at [comma.ai/shop](https://comma.ai/shop/comma-3x).
2. **Software:** The setup procedure for the comma 3/3X allows users to enter a URL for custom software. Use the URL `openpilot.comma.ai` to install the release version.
3. **Supported Car:** Ensure that you have one of [the 275+ supported cars](docs/CARS.md).
4. **Car Harness:** You will also need a [car harness](https://comma.ai/shop/car-harness) to connect your comma 3/3X to your car.

We have detailed instructions for [how to install the harness and device in a car](https://comma.ai/setup). Note that it&#039;s possible to run openpilot on [other hardware](https://blog.comma.ai/self-driving-car-for-free/), although it&#039;s not plug-and-play.

### Branches
| branch           | URL                                    | description                                                                         |
|------------------|----------------------------------------|-------------------------------------------------------------------------------------|
| `release3`         | openpilot.comma.ai                      | This is openpilot&#039;s release branch.                                                 |
| `release3-staging` | openpilot-test.comma.ai                | This is the staging branch for releases. Use it to get new releases slightly early. |
| `nightly`          | openpilot-nightly.comma.ai             | This is the bleeding edge development branch. Do not expect this to be stable.      |
| `nightly-dev`      | installer.comma.ai/commaai/nightly-dev | Same as nightly, but includes experimental development features for some cars.      |

To start developing openpilot
------

openpilot is developed by [comma](https://comma.ai/) and by users like you. We welcome both pull requests and issues on [GitHub](http://github.com/commaai/openpilot).

* Join the [community Discord](https://discord.comma.ai)
* Check out [the contributing docs](docs/CONTRIBUTING.md)
* Check out the [openpilot tools](tools/)
* Read about the [development workflow](docs/WORKFLOW.md)
* Code documentation lives at https://docs.comma.ai
* Information about running openpilot lives on the [community wiki](https://github.com/commaai/openpilot/wiki)

Want to get paid to work on openpilot? [comma is hiring](https://comma.ai/jobs#open-positions) and offers lots of [bounties](https://comma.ai/bounties) for external contributors.

Safety and Testing
----

* openpilot observes [ISO26262](https://en.wikipedia.org/wiki/ISO_26262) guidelines, see [SAFETY.md](docs/SAFETY.md) for more details.
* openpilot has software-in-the-loop [tests](.github/workflows/selfdrive_tests.yaml) that run on every commit.
* The code enforcing the safety model lives in panda and is written in C, see [code rigor](https://github.com/commaai/panda#code-rigor) for more details.
* panda has software-in-the-loop [safety tests](https://github.com/commaai/panda/tree/master/tests/safety).
* Internally, we have a hardware-in-the-loop Jenkins test suite that builds and unit tests the various processes.
* panda has additional hardware-in-the-loop [tests](https://github.com/commaai/panda/blob/master/Jenkinsfile).
* We run the latest openpilot in a testing closet containing 10 comma devices continuously replaying routes.

Licensing
------

openpilot is released under the MIT license. Some parts of the software are released under other licenses as specified.

Any user of this software shall indemnify and hold harmless Comma.ai, Inc. and its directors, officers, employees, agents, stockholders, affiliates, subcontractors and customers from and against all allegations, claims, actions, suits, demands, damages, liabilities, obligations, losses, settlements, judgments, costs and expenses (including without limitation attorneys‚Äô fees and costs) which arise out of, relate to or result from any use of this software by user.

**THIS IS ALPHA QUALITY SOFTWARE FOR RESEARCH PURPOSES ONLY. THIS IS NOT A PRODUCT.
YOU ARE RESPONSIBLE FOR COMPLYING WITH LOCAL LAWS AND REGULATIONS.
NO WARRANTY EXPRESSED OR IMPLIED.**

User Data and comma Account
------

By default, openpilot uploads the driving data to our servers. You can also access your data through [comma connect](https://connect.comma.ai/). We use your data to train better models and improve openpilot for everyone.

openpilot is open source software: the user is free to disable data collection if they wish to do so.

openpilot logs the road-facing cameras, CAN, GPS, IMU, magnetometer, thermal sensors, crashes, and operating system logs.
The driver-facing camera is only logged if you explicitly opt-in in settings. The microphone is not recorded.

By using openpilot, you agree to [our Privacy Policy](https://comma.ai/privacy). You understand that use of this software or its related services will generate certain types of user data, which may be logged and stored at the sole discretion of comma. By accepting this agreement, you grant an irrevocable, perpetual, worldwide right to comma for the use of this data.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[leminlimez/Nugget]]></title>
            <link>https://github.com/leminlimez/Nugget</link>
            <guid>https://github.com/leminlimez/Nugget</guid>
            <pubDate>Mon, 31 Mar 2025 00:04:31 GMT</pubDate>
            <description><![CDATA[Unlock the fullest potential of your device]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/leminlimez/Nugget">leminlimez/Nugget</a></h1>
            <p>Unlock the fullest potential of your device</p>
            <p>Language: Python</p>
            <p>Stars: 2,363</p>
            <p>Forks: 152</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre># Nugget
Unlock your device&#039;s full potential!

Sparserestore works on all versions iOS 17.0-18.2 developer beta 2. There is partial support for iOS 18.2 developer beta 3 and newer.

**Mobilegestalt and AI Enabler tweaks are not supported on iOS 18.2+.** It will never be supported, do not make issues asking for when it is supported.

Make sure you have installed the [requirements](#requirements) if you are on Windows or Linux.

This uses the sparserestore exploit to write to files outside of the intended restore location, like mobilegestalt. Read the [Getting the File](#getting-the-file) section to learn how to get your mobilegestalt file.

Note: I am not responsible if your device bootloops. Please back up your data before using!

## Features
### iOS 17.0+
- Enable Dynamic Island on any device
- Enable iPhone X gestures on iPhone SEs
- Change Device Model Name (ie what shows in the Settings app)
- Enable Boot Chime
- Enable Charge Limit
- Enable Tap to Wake on unsupported devices (ie iPhone SEs)
- Enable Collision SOS
- Enable Stage Manager
- Disable the Wallpaper Parallax
- Disable Region Restrictions (ie. Shutter Sound)
  - Note: This does not include enabling EU sideloading outside the EU. That will come later.
- Show the Apple Pencil options in Settings app
- Show the Action Button options in Settings app
- Show Internal Storage info (Might cause problems on some devices, use at your own risk)
- EU Enabler (iOS 17.6-)
- Springboard Options (from [Cowabunga Lite](https://github.com/leminlimez/CowabungaLite))
  - Set Lock Screen Footnote
  - Disable Lock After Respring
  - Disable Screen Dimming While Charging
  - Disable Low Battery Alerts
- Internal Options (from [Cowabunga Lite](https://github.com/leminlimez/CowabungaLite))
  - Build Version in Status Bar
  - Force Right to Left
  - Force Metal HUD Debug
  - iMessage Diagnostics
  - IDS Diagnostics
  - VC Diagnostics
  - App Store Debug Gesture
  - Notes App Debug Mode
- Disable Daemons:
  - OTAd
  - UsageTrackingAgent
  - Game Center
  - Screen Time Agent
  - Logs, Dumps, and Crash Reports
  - ATWAKEUP
  - Tipsd
  - VPN
  - Chinese WLAN service
  - HealthKit
  - AirPrint
  - Assistive Touch
  - iCloud
  - Internet Tethering (aka Personal Hotspot)
  - PassBook
  - Spotlight
  - Voice Control
- PosterBoard: Animated wallpapers and descriptors.
  - Community wallpapers can be found [here](https://cowabun.ga/wallpapers)
  - See documentation on the structure of tendies files in `documentation.md`
- Risky (Hidden) Options:
  - Disable thermalmonitord
  - OTA Killer
  - Custom Resolution
### iOS 18.0+
- Enable iPhone 16 camera button page in the Settings app
- Enable AOD &amp; AOD Vibrancy on any device
- Feature Flags (iOS 18.1b4-):
  - Enabling lock screen clock animation, lock screen page duplication button, and more!
  - Disabling the new iOS 18 Photos UI (iOS 18.0 betas only, unknown which patched it)
### iOS 18.1+
- AI Enabler + Device Spoofing (fixed in iOS 18.2db3)

## Requirements:
- **Windows:**
  - Either [Apple Devices (from Microsoft Store)](https://apps.microsoft.com/detail/9np83lwlpz9k%3Fhl%3Den-US%26gl%3DUS&amp;ved=2ahUKEwjE-svo7qyJAxWTlYkEHQpbH3oQFnoECBoQAQ&amp;usg=AOvVaw0rZTXCFmRaHAifkEEu9tMI) app or [iTunes (from Apple website)](https://support.apple.com/en-us/106372)
- **Linux:**
  - [usbmuxd](https://github.com/libimobiledevice/usbmuxd) and [libimobiledevice](https://github.com/libimobiledevice/libimobiledevice)

- **For Running Python:**
  - pymobiledevice3
  - PySide6
  - Python 3.8 or newer

## Running the Python Program
Note: It is highly recommended to use a virtual environment:
```
python3 -m venv .env # only needed once
# macOS/Linux:  source .env/bin/activate
# Windows:      .env/Scripts/activate.bat
pip3 install -r requirements.txt # only needed once
python3 main_app.py
```
Note: It may be either `python`/`pip` or `python3`/`pip3` depending on your path.

The CLI version can be ran with `python3 cli_app.py`.

## Getting the File
You need to get the mobilegestalt file that is specific to your device. To do that, follow these steps:
1. Install the `Shortcuts` app from the iOS app store.
2. Download this shortcut: https://www.icloud.com/shortcuts/d6f0a136ddda4714a80750512911c53b
3. Save the file and share it to your computer.
4. Place it in the same folder as the python file (or specify the path in the program)

## Building
To compile `mainwindow.ui` for Python, run the following command:
`pyside6-uic qt/mainwindow.ui -o qt/ui_mainwindow.py`

To compile the resources file for Python, run the following command:
`pyside6-rcc qt/resources.qrc -o resources_rc.py`

The application itself can be compiled by running `compile.py`.

## Read More
If you would like to read more about the inner workings of the exploit and iOS restore system, I made a write up which you can read [here](https://gist.github.com/leminlimez/c602c067349140fe979410ef69d39c28).

## Credits
- [JJTech](https://github.com/JJTech0130) for Sparserestore/[TrollRestore](https://github.com/JJTech0130/TrollRestore)
- [PosterRestore](https://discord.gg/gWtzTVhMvh) for their help with PosterBoard
  - Special thanks to dootskyre, [Middo](https://twitter.com/MWRevamped), [dulark](https://github.com/dularkian), forcequitOS, and pingubow for their work on this. It would not have been possible without them!
- [disfordottie](https://x.com/disfordottie) for some global flag features
- [Mikasa-san](https://github.com/Mikasa-san) for [Quiet Daemon](https://github.com/Mikasa-san/QuietDaemon)
- [sneakyf1shy](https://github.com/f1shy-dev) for [AI Eligibility](https://gist.github.com/f1shy-dev/23b4a78dc283edd30ae2b2e6429129b5) (iOS 18.1 beta 4 and below)
- [lrdsnow](https://github.com/Lrdsnow) for [EU Enabler](https://github.com/Lrdsnow/EUEnabler)
- [pymobiledevice3](https://github.com/doronz88/pymobiledevice3) for restoring and device algorithms.
- [PySide6](https://doc.qt.io/qtforpython-6/) for the GUI library.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[StructuredLabs/preswald]]></title>
            <link>https://github.com/StructuredLabs/preswald</link>
            <guid>https://github.com/StructuredLabs/preswald</guid>
            <pubDate>Mon, 31 Mar 2025 00:04:30 GMT</pubDate>
            <description><![CDATA[Preswald is a framework for building and deploying interactive data apps, internal tools, and dashboards with Python. With one command, you can launch, share, and deploy locally or in the cloud, turning Python scripts into powerful shareable apps.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/StructuredLabs/preswald">StructuredLabs/preswald</a></h1>
            <p>Preswald is a framework for building and deploying interactive data apps, internal tools, and dashboards with Python. With one command, you can launch, share, and deploy locally or in the cloud, turning Python scripts into powerful shareable apps.</p>
            <p>Language: Python</p>
            <p>Stars: 2,403</p>
            <p>Forks: 581</p>
            <p>Stars today: 129 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/banner.png&quot; alt=&quot;Banner&quot;&gt;
&lt;/p&gt;


&lt;p align=&quot;center&quot;&gt;
    &lt;em&gt;Turn Python scripts into interactive data apps and deploy them anywhere in one command.&lt;/em&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;LICENSE&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/license-Apache%202.0-blue.svg&quot; alt=&quot;Apache 2.0 License&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://www.python.org/downloads/&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/python-3.7%2B-blue.svg&quot; alt=&quot;Python Version&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://join.slack.com/t/structuredlabs-users/shared_invite/zt-31vvfitfm-_vG1HR9hYysR_56u_PfI8Q&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/Slack-Join%20Community-orange&quot; alt=&quot;Slack Community&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://pypi.org/project/preswald/&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/pypi/v/preswald&quot; alt=&quot;PyPI Version&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://preswald.com&quot; target=&quot;_blank&quot;&gt;
&lt;img src=&quot;https://img.shields.io/badge/Landing%20Page-Visit-blue?style=for-the-badge&quot; alt=&quot;Website&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://docs.preswald.com&quot; target=&quot;_blank&quot;&gt;
&lt;img src=&quot;https://img.shields.io/badge/Documentation-Read-green?style=for-the-badge&quot; alt=&quot;Documentation&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://app.preswald.com&quot; target=&quot;_blank&quot;&gt;
&lt;img src=&quot;https://img.shields.io/badge/Cloud-Get Started-orange?style=for-the-badge&quot; alt=&quot;Studio&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://cal.com/amruthagujjar&quot; target=&quot;_blank&quot;&gt;
&lt;img src=&quot;https://img.shields.io/badge/Book%20a%20Demo-Schedule-red?style=for-the-badge&quot; alt=&quot;Book a Demo&quot;&gt;
&lt;/a&gt;
&lt;/p&gt;

## **What is Preswald?**

Preswald is an open-source framework for building **data apps, dashboards, and internal tools** with just Python. It gives you **pre-built UI components** like tables, charts, and forms, so you don‚Äôt have to write frontend code. Users can interact with your app, changing inputs, running queries, and updating visualizations, without you needing to manage the UI manually.

Preswald tracks state and dependencies, so computations update only when needed instead of re-running everything from scratch. It uses a **workflow DAG** to manage execution order, making apps more predictable and performant. Preswald lets you **turn Python scripts into shareable, production-ready applications** easily. 

## **Key Features**

- Add UI components to python scripts ‚Äì Drop in buttons, text inputs, tables, and charts that users can interact with.
- Stateful execution ‚Äì Automatically tracks dependencies and updates results when inputs change.
- Structured computation ‚Äì Uses a DAG-based execution model to prevent out-of-order runs.
- Deploy with one command ‚Äì Run preswald deploy and instantly share your app online.
- Query and display data ‚Äì Fetch live data from databases, or local files and display it in a UI.
- Build interactive reports ‚Äì Create dashboards where users can change filters and see results update.
- Run locally or in the cloud ‚Äì Start your app on your laptop or host it in Preswald Cloud for easy access.
- Share with a link ‚Äì No need to send scripts or install dependencies‚Äîjust share a URL.
- High-performance GPU charts ‚Äì Render real-time, interactive charts using fastplotlib, with offscreen GPU acceleration and WebSocket-based streaming to the browser.

&lt;br&gt;

&lt;br&gt;

# **üöÄ Getting Started**

## **Installation**

First, install Preswald using pip. https://pypi.org/project/preswald/

```bash
pip install preswald
```

![Demo GIF](assets/demo1.gif)

## **üë©‚Äçüíª Quick Start**

### **1. Initialize a New Project**

Start your journey with Preswald by initializing a new project:

```bash
preswald init my_project
cd my_project
```

This will create a folder called `my_project` with all the basics you need:

- `hello.py`: Your first Preswald app.
- `preswald.toml`: Customize your app‚Äôs settings and style.
- `secrets.toml`: Keep your API keys and sensitive information safe.
- `.gitignore`: Preconfigured to keep `secrets.toml` out of your Git repository.

### **2. Write Your First App**

Time to make something magical! Open up `hello.py` and write:

```python
from preswald import text, plotly, connect, get_df, table
import pandas as pd
import plotly.express as px

text(&quot;# Welcome to Preswald!&quot;)
text(&quot;This is your first app. üéâ&quot;)

# Load the CSV
connect() # load in all sources, which by default is the sample_csv
df = get_df(&#039;sample_csv&#039;)

# Create a scatter plot
fig = px.scatter(df, x=&#039;quantity&#039;, y=&#039;value&#039;, text=&#039;item&#039;,
                 title=&#039;Quantity vs. Value&#039;,
                 labels={&#039;quantity&#039;: &#039;Quantity&#039;, &#039;value&#039;: &#039;Value&#039;})

# Add labels for each point
fig.update_traces(textposition=&#039;top center&#039;, marker=dict(size=12, color=&#039;lightblue&#039;))

# Style the plot
fig.update_layout(template=&#039;plotly_white&#039;)

# Show the plot
plotly(fig)

# Show the data
table(df)
```
### **3. Run Your App**

Now the fun part‚Äîsee it in action! Run your app locally with:

```bash
preswald run
```

This command launches a development server, and Preswald will let you know where your app is hosted. Typically, it‚Äôs here:

```
üåê App running at: http://localhost:8501
```

Open your browser, and voil√†‚Äîyour first Preswald app is live!

### **4. Deploy Your App to the Cloud**

Preswald provides its own cloud platform for hosting and sharing your applications. You can authenticate with GitHub, create an organization, and generate an API key at [app.preswald.com](https://app.preswald.com). Once set up, deploying is as simple as running:  

```bash
preswald deploy --target structured
```

The first time you deploy, you&#039;ll be prompted to enter your **GitHub username** and **Preswald API key**. After that, your app will be built, deployed, and accessible online.  

```
üåê App deployed at: https://your-app-name-abc123.preswald.app
```

Now your app is live, shareable, and scalable‚Äîwithout any extra setup.


## **üîß Configuration**

Preswald uses `preswald.toml` for project settings and theming. It‚Äôs straightforward, and it makes your app look polished.

### **Sample `preswald.toml`:**

```
[project]
title = &quot;Preswald Project&quot;
version = &quot;0.1.0&quot;
port = 8501
slug = &quot;preswald-project&quot;
entrypoint = &quot;hello.py&quot;

[branding]
name = &quot;Preswald Project&quot;
logo = &quot;images/logo.png&quot;
favicon = &quot;images/favicon.ico&quot;
primaryColor = &quot;#F89613&quot;

[data.sample_csv]
type = &quot;csv&quot;
path = &quot;data/sample.csv&quot;

[logging]
level = &quot;INFO&quot;  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
format = &quot;%(asctime)s - %(name)s - %(levelname)s - %(message)s&quot;
```

&lt;br&gt;

## **üìö Documentation**

We‚Äôre here to help! Check out our full documentation at [Preswald Docs](https://docs.preswald.com/).

&lt;br&gt;

## **ü§ù Contributing**

Check out [CONTRIBUTING.md](CONTRIBUTING.md).

&lt;br&gt;

## **üéâ Join the Community**

- **GitHub Issues**: Found a bug? Let us know [here](https://github.com/StructuredLabs/preswald/issues).
- **Community Forum**: Reach out [here](https://join.slack.com/t/structuredlabs-users/shared_invite/zt-31vvfitfm-_vG1HR9hYysR_56u_PfI8Q)
- **Discussions**: Share your ideas and ask questions in our [discussion forum](https://github.com/StructuredLabs/preswald/discussions).
- **Contributors**: Meet the awesome people who make Preswald better [here](https://github.com/StructuredLabs/preswald/graphs/contributors).

&lt;br&gt;

## **üì¢ Stay Connected**

&lt;p&gt;
    &lt;a href=&quot;https://www.linkedin.com/company/structuredlabs/&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/Follow%20Us-LinkedIn-blue?style=for-the-badge&amp;logo=linkedin&quot; alt=&quot;Follow us on LinkedIn&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://x.com/StructuredLabs&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/Follow%20Us-Twitter-1DA1F2?style=for-the-badge&amp;logo=twitter&quot; alt=&quot;Follow us on Twitter&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

## **üìÑ License**

Preswald is licensed under the [Apache 2.0 License](LICENSE).

## ‚ú® Contributors

Thanks to everyone who has contributed to Preswald üíú

[![](https://contrib.rocks/image?repo=StructuredLabs/preswald)](https://github.com/StructuredLabs/preswald/graphs/contributors)

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[GreyDGL/PentestGPT]]></title>
            <link>https://github.com/GreyDGL/PentestGPT</link>
            <guid>https://github.com/GreyDGL/PentestGPT</guid>
            <pubDate>Mon, 31 Mar 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[A GPT-empowered penetration testing tool]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/GreyDGL/PentestGPT">GreyDGL/PentestGPT</a></h1>
            <p>A GPT-empowered penetration testing tool</p>
            <p>Language: Python</p>
            <p>Stars: 8,013</p>
            <p>Forks: 1,000</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre>&lt;!-- Improved compatibility of back to top link: See: https://github.com/othneildrew/Best-README-Template/pull/73 --&gt;
&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;!-- PROJECT SHIELDS --&gt;
&lt;!--
*** I&#039;m using markdown &quot;reference style&quot; links for readability.
*** Reference links are enclosed in brackets [ ] instead of parentheses ( ).
*** See the bottom of this document for the declaration of the reference variables
*** for contributors-url, forks-url, etc. This is an optional, concise syntax you may use.
*** https://www.markdownguide.org/basic-syntax/#reference-style-links
--&gt;
[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]
[![MIT License][license-shield]][license-url]
[![Discord][discord-shield]][discord-url]



&lt;!-- PROJECT LOGO --&gt;
&lt;br /&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/GreyDGL/PentestGPT&quot;&gt;
  &lt;/a&gt;

&lt;h3 align=&quot;center&quot;&gt;PentestGPT&lt;/h3&gt;

  &lt;p align=&quot;center&quot;&gt;
    A GPT-empowered penetration testing tool. 
    &lt;br /&gt;
    &lt;a href=&quot;https://github.com/GreyDGL/PentestGPT&quot;&gt;&lt;strong&gt;Explore the docs ¬ª&lt;/strong&gt;&lt;/a&gt;
    &lt;br /&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://github.com/GreyDGL/PentestGPT/blob/main/PentestGPT_design.md&quot;&gt;Design Details&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://www.youtube.com/watch?v=lAjLIj1JT3c&quot;&gt;View Demo&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://github.com/GreyDGL/PentestGPT/issues&quot;&gt;Report Bug or Request Feature&lt;/a&gt;
    &lt;/p&gt;
&lt;/div&gt;





&lt;!-- ABOUT THE PROJECT --&gt;
&lt;a href=&quot;https://trendshift.io/repositories/3770&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/3770&quot; alt=&quot;GreyDGL%2FPentestGPT | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
## General Updates
- [Update on 25/10/2024] We&#039;re completing the refactoring of PentestGPT and will release v1.0 soon!
- [Update on 12/08/2024] The research paper on PentestGPT is published at [USENIX Security 2024](https://www.usenix.org/conference/usenixsecurity24/presentation/deng)
- [Update on 25/03/2024] We&#039;re working on the next version of PentestGPT, with online searching, RAGs and more powerful prompting. Stay tuned!
- [Update on 17/11/2023] GPTs for PentestGPT is out! Check this: https://chat.openai.com/g/g-4MHbTepWO-pentestgpt
- [Update on 07/11/2023] GPT-4-turbo is out! Update the default API usage to GPT-4-turbo. 
- Available videos:
  - The latest installation video is [here](https://youtu.be/tGC5z14dE24).
  - **PentestGPT for OSCP-like machine: [HTB-Jarvis](https://youtu.be/lAjLIj1JT3c)**. This is the first part only, and I&#039;ll complete the rest when I have time.
  - **PentestGPT on [HTB-Lame](https://youtu.be/Vs9DFtAkODM)**. This is an easy machine, but it shows you how PentestGPT skipped the rabbit hole and worked on other potential vulnerabilities.
- **We&#039;re testing PentestGPT on HackTheBox**. You may follow [this link](https://www.hackthebox.com/home/users/profile/1489431). More details will be released soon.
- Feel free to join the [Discord Channel](https://discord.gg/eC34CEfEkK) for more updates and share your ideas!


&lt;!-- Quick Start --&gt;
## Quick Start
1. Create a virtual environment if necessary. (`virtualenv -p python3 venv`, `source venv/bin/activate`)
2. Install the project with `pip3 install git+https://github.com/GreyDGL/PentestGPT`
3. **Ensure that you have link a payment method to your OpenAI account.** Export your API key with `export OPENAI_API_KEY=&#039;&lt;your key here&gt;&#039;`,export API base with `export OPENAI_BASEURL=&#039;https://api.xxxx.xxx/v1&#039;`if you need.
4. Test the connection with `pentestgpt-connection`
5. For Kali Users: use `tmux` as terminal environment. You can do so by simply run `tmux` in the native terminal.
6. To start: `pentestgpt --logging`


&lt;!-- GETTING STARTED --&gt;
## Getting Started
- **PentestGPT** is a penetration testing tool empowered by **ChatGPT**. 
- It is designed to automate the penetration testing process. It is built on top of ChatGPT and operate in an interactive mode to guide penetration testers in both overall progress and specific operations.
- **PentestGPT** is able to solve easy to medium HackTheBox machines, and other CTF challenges. You can check [this](./resources/README.md) example in `resources` where we use it to solve HackTheBox challenge **TEMPLATED** (web challenge). 
- A sample testing process of **PentestGPT** on a target VulnHub machine (Hackable II) is available at [here](./resources/PentestGPT_Hackable2.pdf).
- A sample usage video is below: (or available here: [Demo](https://youtu.be/h0k6kWWaCEU))

&lt;!-- Common Questions --&gt;
## Common Questions
- **Q**: What is PentestGPT?
  - **A**: PentestGPT is a penetration testing tool empowered by Large Language Models (LLMs). It is designed to automate the penetration testing process. It is built on top of ChatGPT API and operate in an interactive mode to guide penetration testers in both overall progress and specific operations.
- **Q**: Do I need to pay to use PentestGPT?
  - **A**: Yes in order to achieve the best performance. In general, you can use any LLMs you want, but you&#039;re recommended to use GPT-4 API, for which you have to [link a payment method to OpenAI](https://help.openai.com/en/collections/3943089-billing?q=API). 
- **Q**: Why GPT-4?
  - **A**: After empirical evaluation, we find that GPT-4 performs better than GPT-3.5 and other LLMs in terms of penetration testing reasoning. In fact, GPT-3.5 leads to failed test in simple tasks.
- **Q**: Why not just use GPT-4 directly?
  - **A**: We found that GPT-4 suffers from losses of context as test goes deeper. It is essential to maintain a &quot;test status awareness&quot; in this process. You may check the [PentestGPT Arxiv Paper](https://arxiv.org/abs/2308.06782) for details.
- **Q**: Can I use local GPT models?
  - **A**: Yes. We support local LLMs with custom parser. Look at examples [here](./pentestgpt/utils/APIs/gpt4all_api.py).


## Installation
PentestGPT is tested under `Python 3.10`. Other Python3 versions should work but are not tested.
### Install with pip
**PentestGPT** relies on **OpenAI API** to achieve high-quality reasoning. You may refer to the installation video [here](https://youtu.be/tGC5z14dE24).
1. Install the latest version with `pip3 install git+https://github.com/GreyDGL/PentestGPT`
   - You may also clone the project to local environment and install for better customization and development
     - `git clone https://github.com/GreyDGL/PentestGPT`
     - `cd PentestGPT`
     - `pip3 install -e .`
2. To use OpenAI API
   - **Ensure that you have link a payment method to your OpenAI account.**
   - export your API key with `export OPENAI_API_KEY=&#039;&lt;your key here&gt;&#039;`
   - export API base with `export OPENAI_BASEURL=&#039;https://api.xxxx.xxx/v1&#039;`if you need.
   - Test the connection with `pentestgpt-connection`
3. To verify that the connection is configured properly, you may run `pentestgpt-connection`. After a while, you should see some sample conversation with ChatGPT.
   - A sample output is below
   ```
   You&#039;re testing the connection for PentestGPT v 0.11.0
   #### Test connection for OpenAI api (GPT-4)
   1. You&#039;re connected with OpenAI API. You have GPT-4 access. To start PentestGPT, please use &lt;pentestgpt --reasoning_model=gpt-4&gt;
   
   #### Test connection for OpenAI api (GPT-3.5)
   2. You&#039;re connected with OpenAI API. You have GPT-3.5 access. To start PentestGPT, please use &lt;pentestgpt --reasoning_model=gpt-3.5-turbo-16k&gt;
   ```
   - notice: if you have not linked a payment method to your OpenAI account, you will see error messages.
4. The ChatGPT cookie solution is deprecated and not recommended. You may still use it by running `pentestgpt --reasoning_model=gpt-4 --useAPI=False`. 


### Build from Source
1. Clone the repository to your local environment.
2. Ensure that `poetry` is installed. If not, please refer to the [poetry installation guide](https://python-poetry.org/docs/).
3. 

&lt;!-- USAGE EXAMPLES --&gt;

## Usage
1. **You are recommended to run**:
   - (recommended) - `pentestgpt --reasoning_model=gpt-4-turbo` to use the latest GPT-4-turbo API.
   - `pentestgpt --reasoning_model=gpt-4` if you have access to GPT-4 API.
   - `pentestgpt --reasoning_model=gpt-3.5-turbo-16k` if you only have access to GPT-3.5 API.
   
2. To start, run `pentestgpt --args`.
    - `--help` show the help message
    - `--reasoning_model` is the reasoning model you want to use. 
    - `--parsing_model` is the parsing model you want to use. 
    - `--useAPI` is whether you want to use OpenAI API. By default it is set to `True`.
    - `--log_dir` is the customized log output directory. The location is a relative directory.
    - `--logging` defines if you would like to share the logs with us. By default it is set to `False`.
3. The tool works similar to *msfconsole*. Follow the guidance to perform penetration testing. 
4. In general, PentestGPT intakes commands similar to chatGPT. There are several basic commands.
   1. The commands are: 
      - `help`: show the help message.
      - `next`: key in the test execution result and get the next step.
      - `more`: let **PentestGPT** to explain more details of the current step. Also, a new sub-task solver will be created to guide the tester.
      - `todo`: show the todo list.
      - `discuss`: discuss with the **PentestGPT**.
      - `google`: search on Google. This function is still under development.
      - `quit`: exit the tool and save the output as log file (see the **reporting** section below).
   2. You can use &lt;SHIFT + right arrow&gt; to end your input (and &lt;ENTER&gt; is for next line).
   3. You may always use `TAB` to autocomplete the commands.
   4. When you&#039;re given a drop-down selection list, you can use cursor or arrow key to navigate the list. Press `ENTER` to select the item. Similarly, use &lt;SHIFT + right arrow&gt; to confirm selection.\
      The user can submit info about:
        * **tool**: output of the security test tool used
        * **web**: relevant content of a web page
        * **default**: whatever you want, the tool will handle it
        * **user-comments**: user comments about PentestGPT operations
5. In the sub-task handler initiated by `more`, users can execute more commands to investigate into a specific problem:
   1. The commands are:
        - `help`: show the help message.
        - `brainstorm`: let PentestGPT brainstorm on the local task for all the possible solutions.
        - `discuss`: discuss with PentestGPT about this local task.
        - `google`: search on Google. This function is still under development.
        - `continue`: exit the subtask and continue the main testing session.

### Report and Logging
1. [Update] If you would like us to collect the logs to improve the tool, please run `pentestgpt --logging`. We will only collect the LLM usage, without any information related to your OpenAI key.
2. After finishing the penetration testing, a report will be automatically generated in `logs` folder (if you quit with `quit` command).
3. The report can be printed in a human-readable format by running `python3 utils/report_generator.py &lt;log file&gt;`. A sample report `sample_pentestGPT_log.txt` is also uploaded.

## Custom Model Endpoints and Local LLMs
PentestGPT now support local LLMs, but the prompts are only optimized for GPT-4.
- To use local GPT4ALL model, you may run `pentestgpt --reasoning_model=gpt4all --parsing_model=gpt4all`.
- To select the particular model you want to use with GPT4ALL, you may update the `module_mapping` class in `pentestgpt/utils/APIs/module_import.py`.
- You can also follow the examples of `module_import.py`, `gpt4all.py` and `chatgpt_api.py` to create API support for your own model.

## Citation
Please cite our paper at:
```
@inproceedings {299699,
author = {Gelei Deng and Yi Liu and V{\&#039;\i}ctor Mayoral-Vilches and Peng Liu and Yuekang Li and Yuan Xu and Tianwei Zhang and Yang Liu and Martin Pinzger and Stefan Rass},
title = {{PentestGPT}: Evaluating and Harnessing Large Language Models for Automated Penetration Testing},
booktitle = {33rd USENIX Security Symposium (USENIX Security 24)},
year = {2024},
isbn = {978-1-939133-44-1},
address = {Philadelphia, PA},
pages = {847--864},
url = {https://www.usenix.org/conference/usenixsecurity24/presentation/deng},
publisher = {USENIX Association},
month = aug
}
```

&lt;!-- LICENSE --&gt;
## License

Distributed under the MIT License. See `LICENSE.txt` for more information.
The tool is for educational purpose only and the author does not condone any illegal use. Use as your own risk.



&lt;!-- CONTACT --&gt;
## Contact the Contributors!

- Gelei Deng - [![LinkedIn][linkedin-shield]][linkedin-url] - gelei.deng@ntu.edu.sg
- V√≠ctor Mayoral Vilches - [![LinkedIn][linkedin-shield]][linkedin-url2] - v.mayoralv@gmail.com
- Yi Liu - yi009@e.ntu.edu.sg
- Peng Liu - liu_peng@i2r.a-star.edu.sg
- Yuekang Li - yuekang.li@unsw.edu.au


&lt;p align=&quot;right&quot;&gt;(&lt;a href=&quot;#readme-top&quot;&gt;back to top&lt;/a&gt;)&lt;/p&gt;





&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt;
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;
[contributors-shield]: https://img.shields.io/github/contributors/GreyDGL/PentestGPT.svg?style=for-the-badge
[contributors-url]: https://github.com/GreyDGL/PentestGPT/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/GreyDGL/PentestGPT.svg?style=for-the-badge
[forks-url]: https://github.com/GreyDGL/PentestGPT/network/members
[stars-shield]: https://img.shields.io/github/stars/GreyDGL/PentestGPT.svg?style=for-the-badge
[stars-url]: https://github.com/GreyDGL/PentestGPT/stargazers
[issues-shield]: https://img.shields.io/github/issues/GreyDGL/PentestGPT.svg?style=for-the-badge
[issues-url]: https://github.com/GreyDGL/PentestGPT/issues
[license-shield]: https://img.shields.io/github/license/GreyDGL/PentestGPT.svg?style=for-the-badge
[license-url]: https://github.com/GreyDGL/PentestGPT/blob/master/LICENSE.txt
[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&amp;logo=linkedin&amp;colorB=555
[linkedin-url]: https://www.linkedin.com/in/gelei-deng-225a10112/
[linkedin-url2]: https://www.linkedin.com/in/vmayoral/
[discord-shield]: https://dcbadge.vercel.app/api/server/eC34CEfEkK
[discord-url]: https://discord.gg/eC34CEfEkK
[product-screenshot]: images/screenshot.png
[Next.js]: https://img.shields.io/badge/next.js-000000?style=for-the-badge&amp;logo=nextdotjs&amp;logoColor=white
[Next-url]: https://nextjs.org/
[React.js]: https://img.shields.io/badge/React-20232A?style=for-the-badge&amp;logo=react&amp;logoColor=61DAFB
[React-url]: https://reactjs.org/
[Vue.js]: https://img.shields.io/badge/Vue.js-35495E?style=for-the-badge&amp;logo=vuedotjs&amp;logoColor=4FC08D
[Vue-url]: https://vuejs.org/
[Angular.io]: https://img.shields.io/badge/Angular-DD0031?style=for-the-badge&amp;logo=angular&amp;logoColor=white
[Angular-url]: https://angular.io/
[Svelte.dev]: https://img.shields.io/badge/Svelte-4A4A55?style=for-the-badge&amp;logo=svelte&amp;logoColor=FF3E00
[Svelte-url]: https://svelte.dev/
[Laravel.com]: https://img.shields.io/badge/Laravel-FF2D20?style=for-the-badge&amp;logo=laravel&amp;logoColor=white
[Laravel-url]: https://laravel.com
[Bootstrap.com]: https://img.shields.io/badge/Bootstrap-563D7C?style=for-the-badge&amp;logo=bootstrap&amp;logoColor=white
[Bootstrap-url]: https://getbootstrap.com
[JQuery.com]: https://img.shields.io/badge/jQuery-0769AD?style=for-the-badge&amp;logo=jquery&amp;logoColor=white
[JQuery-url]: https://jquery.com
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ok-oldking/ok-wuthering-waves]]></title>
            <link>https://github.com/ok-oldking/ok-wuthering-waves</link>
            <guid>https://github.com/ok-oldking/ok-wuthering-waves</guid>
            <pubDate>Mon, 31 Mar 2025 00:04:28 GMT</pubDate>
            <description><![CDATA[È∏£ÊΩÆ ÂêéÂè∞Ëá™Âä®ÊàòÊñó Ëá™Âä®Âà∑Â£∞È™∏ ‰∏ÄÈîÆÊó•Â∏∏ Automation for Wuthering Waves]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ok-oldking/ok-wuthering-waves">ok-oldking/ok-wuthering-waves</a></h1>
            <p>È∏£ÊΩÆ ÂêéÂè∞Ëá™Âä®ÊàòÊñó Ëá™Âä®Âà∑Â£∞È™∏ ‰∏ÄÈîÆÊó•Â∏∏ Automation for Wuthering Waves</p>
            <p>Language: Python</p>
            <p>Stars: 2,102</p>
            <p>Forks: 148</p>
            <p>Stars today: 21 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;h1 align=&quot;center&quot;&gt;
    &lt;img src=&quot;icon.png&quot; width=&quot;200&quot;/&gt;
    &lt;br/&gt;
      ok-ww
  &lt;/h1&gt; 
&lt;h3&gt;&lt;i&gt;Automation for Wuthering Waves using computer vision and win32api&lt;/i&gt;&lt;/h3&gt;
&lt;/div&gt;

![Static Badge](https://img.shields.io/badge/platfrom-Windows-blue?color=blue)
[![GitHub release (with filter)](https://img.shields.io/github/v/release/ok-oldking/ok-wuthering-waves)](https://github.com/ok-oldking/ok-wuthering-waves/releases)
[![GitHub all releases](https://img.shields.io/github/downloads/ok-oldking/ok-wuthering-waves/total)](https://github.com/ok-oldking/ok-wuthering-waves/releases)
[![Discord](https://img.shields.io/discord/296598043787132928?color=5865f2&amp;label=%20Discord)](https://discord.gg/vVyCatEBgA)

### English Readme | [‰∏≠ÊñáËØ¥Êòé](README_cn.md)

![img.png](readme/img.png)
![img_1.png](readme/img_1.png)

## Key Features

* Works while the Game is in the Background
* Farm Boss Echo (Dreamless, Jue, and World Bosses)
* One Press Clear All Daily Tasks and Tacet Field
* Auto Combat in Abyss, Game World, Tacet Field, etc.
* Auto Skip Dialogs in Quests
* Auto Pick-up (Echos, Flowers, Chests)
* Supports All Game Languages (Most Features)

### Usage (Run from Compiled .exe)

* Download `ok-ww.7z` from the latest releases
* Extract and double-click the `ok-ww.exe`

### Usage (Run from Python Source Code)

Use Python 3.12, other versions might work but are not tested.

```
git clone https://github.com/ok-oldking/ok-wuthering-waves
pip install -r requirements.txt --upgrade #install python dependencies, you might need do run this again after updating the code
python main.py # run the release version
python main_debug.py # run the debug version
```

### Command Line Arguments

```
ok-ww.exe -t 1 -e
```

- `-t` or `--task` represents the task number to execute automatically upon startup. `1` means the first task, a
  one-click execution task.
- `-e` or `--exit` when added, indicates that the program should automatically exit after the task is completed.

### Must Set Game Settings

![image](https://github.com/user-attachments/assets/7d5f27b4-7b28-4471-bf7b-096dccd4ec4d)
![image](https://github.com/user-attachments/assets/66deba93-d0e7-41c0-985c-248deee9b8ff)

### FAQ

## Frequently Asked Questions (FAQ)

1. **Extraction Issues:** Extract the archive to a directory with only English characters.
2. **Antivirus Interference:** Add the download and extraction directories to your antivirus/Windows Defender whitelist.
3. **Display Settings:** Disable Windows HDR, eye protection modes, and automatic color management. Use default game
   brightness and disable external overlays (FPS, GPU info).
4. **Custom Keybinding:** If you are not using default keybindings, Set yours in the app settings, keys not in the
   settings are not supported.
5. **Outdated Version:** Ensure you are using the latest version of OK-GI.
6. **Performance:** Maintain a stable 60 FPS in the game, reduce resolution if needed.
7. **Disconnection** If you often got disconnected, try open the game first, and start playing 5 mins later, or when
   disconnected, don&#039;t close the game, and re-login.
8. **Further Assistance:** Submit a bug report if issues persist.

# Disclaimer

This software is an external tool designed to automate the gameplay of ‚ÄúWuthering Waves.‚Äù It interacts with the game
solely through the existing user interface and complies with relevant laws and regulations. The package aims to simplify
user interaction with the game without disrupting game balance or providing any unfair advantages. It does not modify
any game files or code.

This software is open-source and free, intended solely for personal learning and communication purposes, and is limited
to personal game accounts. It is not allowed for any commercial or profit-making purposes. The development team reserves
the final interpretation rights of this project. Any issues arising from the use of this software are unrelated to the
project and the development team. If you find merchants using this software for paid boosting services, it is their
personal behavior, and this software is not authorized for boosting services. Any issues and consequences arising from
such use are unrelated to this software. This software is not authorized for sale, and any sold versions may contain
malicious code, leading to the theft of game accounts or computer data, which is unrelated to this software.

### Related Projects

* [ok-genshin-impact](https://github.com/ok-oldking/ok-genshin-impact) Genshin Impact Automation
* [ok-gf2](https://github.com/ok-oldking/ok-gf2) Girls Frontline 2 Automation(Simplified-Chinese Only)

### Credits

[https://github.com/lazydog28/mc_auto_boss](https://github.com/lazydog28/mc_auto_boss) 
  
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[comfyanonymous/ComfyUI]]></title>
            <link>https://github.com/comfyanonymous/ComfyUI</link>
            <guid>https://github.com/comfyanonymous/ComfyUI</guid>
            <pubDate>Mon, 31 Mar 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[The most powerful and modular diffusion model GUI, api and backend with a graph/nodes interface.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/comfyanonymous/ComfyUI">comfyanonymous/ComfyUI</a></h1>
            <p>The most powerful and modular diffusion model GUI, api and backend with a graph/nodes interface.</p>
            <p>Language: Python</p>
            <p>Stars: 72,831</p>
            <p>Forks: 7,890</p>
            <p>Stars today: 92 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# ComfyUI
**The most powerful and modular visual AI engine and application.**


[![Website][website-shield]][website-url]
[![Dynamic JSON Badge][discord-shield]][discord-url]
[![Matrix][matrix-shield]][matrix-url]
&lt;br&gt;
[![][github-release-shield]][github-release-link]
[![][github-release-date-shield]][github-release-link]
[![][github-downloads-shield]][github-downloads-link]
[![][github-downloads-latest-shield]][github-downloads-link]

[matrix-shield]: https://img.shields.io/badge/Matrix-000000?style=flat&amp;logo=matrix&amp;logoColor=white
[matrix-url]: https://app.element.io/#/room/%23comfyui_space%3Amatrix.org
[website-shield]: https://img.shields.io/badge/ComfyOrg-4285F4?style=flat
[website-url]: https://www.comfy.org/
&lt;!-- Workaround to display total user from https://github.com/badges/shields/issues/4500#issuecomment-2060079995 --&gt;
[discord-shield]: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2Fcomfyorg%3Fwith_counts%3Dtrue&amp;query=%24.approximate_member_count&amp;logo=discord&amp;logoColor=white&amp;label=Discord&amp;color=green&amp;suffix=%20total
[discord-url]: https://www.comfy.org/discord

[github-release-shield]: https://img.shields.io/github/v/release/comfyanonymous/ComfyUI?style=flat&amp;sort=semver
[github-release-link]: https://github.com/comfyanonymous/ComfyUI/releases
[github-release-date-shield]: https://img.shields.io/github/release-date/comfyanonymous/ComfyUI?style=flat
[github-downloads-shield]: https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/total?style=flat
[github-downloads-latest-shield]: https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/latest/total?style=flat&amp;label=downloads%40latest
[github-downloads-link]: https://github.com/comfyanonymous/ComfyUI/releases

![ComfyUI Screenshot](https://github.com/user-attachments/assets/7ccaf2c1-9b72-41ae-9a89-5688c94b7abe)
&lt;/div&gt;

ComfyUI lets you design and execute advanced stable diffusion pipelines using a graph/nodes/flowchart based interface. Available on Windows, Linux, and macOS.

## Get Started

#### [Desktop Application](https://www.comfy.org/download)
- The easiest way to get started. 
- Available on Windows &amp; macOS.

#### [Windows Portable Package](#installing)
- Get the latest commits and completely portable.
- Available on Windows.

#### [Manual Install](#manual-install-windows-linux)
Supports all operating systems and GPU types (NVIDIA, AMD, Intel, Apple Silicon, Ascend).

## [Examples](https://comfyanonymous.github.io/ComfyUI_examples/)
See what ComfyUI can do with the [example workflows](https://comfyanonymous.github.io/ComfyUI_examples/).


## Features
- Nodes/graph/flowchart interface to experiment and create complex Stable Diffusion workflows without needing to code anything.
- Image Models
   - SD1.x, SD2.x,
   - [SDXL](https://comfyanonymous.github.io/ComfyUI_examples/sdxl/), [SDXL Turbo](https://comfyanonymous.github.io/ComfyUI_examples/sdturbo/)
   - [Stable Cascade](https://comfyanonymous.github.io/ComfyUI_examples/stable_cascade/)
   - [SD3 and SD3.5](https://comfyanonymous.github.io/ComfyUI_examples/sd3/)
   - Pixart Alpha and Sigma
   - [AuraFlow](https://comfyanonymous.github.io/ComfyUI_examples/aura_flow/)
   - [HunyuanDiT](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_dit/)
   - [Flux](https://comfyanonymous.github.io/ComfyUI_examples/flux/)
   - [Lumina Image 2.0](https://comfyanonymous.github.io/ComfyUI_examples/lumina2/)
- Video Models
   - [Stable Video Diffusion](https://comfyanonymous.github.io/ComfyUI_examples/video/)
   - [Mochi](https://comfyanonymous.github.io/ComfyUI_examples/mochi/)
   - [LTX-Video](https://comfyanonymous.github.io/ComfyUI_examples/ltxv/)
   - [Hunyuan Video](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/)
   - [Nvidia Cosmos](https://comfyanonymous.github.io/ComfyUI_examples/cosmos/)
   - [Wan 2.1](https://comfyanonymous.github.io/ComfyUI_examples/wan/)
- 3D Models
   - [Hunyuan3D 2.0](https://docs.comfy.org/tutorials/3d/hunyuan3D-2)
- [Stable Audio](https://comfyanonymous.github.io/ComfyUI_examples/audio/)
- Asynchronous Queue system
- Many optimizations: Only re-executes the parts of the workflow that changes between executions.
- Smart memory management: can automatically run models on GPUs with as low as 1GB vram.
- Works even if you don&#039;t have a GPU with: ```--cpu``` (slow)
- Can load ckpt, safetensors and diffusers models/checkpoints. Standalone VAEs and CLIP models.
- Embeddings/Textual inversion
- [Loras (regular, locon and loha)](https://comfyanonymous.github.io/ComfyUI_examples/lora/)
- [Hypernetworks](https://comfyanonymous.github.io/ComfyUI_examples/hypernetworks/)
- Loading full workflows (with seeds) from generated PNG, WebP and FLAC files.
- Saving/Loading workflows as Json files.
- Nodes interface can be used to create complex workflows like one for [Hires fix](https://comfyanonymous.github.io/ComfyUI_examples/2_pass_txt2img/) or much more advanced ones.
- [Area Composition](https://comfyanonymous.github.io/ComfyUI_examples/area_composition/)
- [Inpainting](https://comfyanonymous.github.io/ComfyUI_examples/inpaint/) with both regular and inpainting models.
- [ControlNet and T2I-Adapter](https://comfyanonymous.github.io/ComfyUI_examples/controlnet/)
- [Upscale Models (ESRGAN, ESRGAN variants, SwinIR, Swin2SR, etc...)](https://comfyanonymous.github.io/ComfyUI_examples/upscale_models/)
- [unCLIP Models](https://comfyanonymous.github.io/ComfyUI_examples/unclip/)
- [GLIGEN](https://comfyanonymous.github.io/ComfyUI_examples/gligen/)
- [Model Merging](https://comfyanonymous.github.io/ComfyUI_examples/model_merging/)
- [LCM models and Loras](https://comfyanonymous.github.io/ComfyUI_examples/lcm/)
- Latent previews with [TAESD](#how-to-show-high-quality-previews)
- Starts up very fast.
- Works fully offline: will never download anything.
- [Config file](extra_model_paths.yaml.example) to set the search paths for models.

Workflow examples can be found on the [Examples page](https://comfyanonymous.github.io/ComfyUI_examples/)

## Shortcuts

| Keybind                            | Explanation                                                                                                        |
|------------------------------------|--------------------------------------------------------------------------------------------------------------------|
| `Ctrl` + `Enter`                      | Queue up current graph for generation                                                                              |
| `Ctrl` + `Shift` + `Enter`              | Queue up current graph as first for generation                                                                     |
| `Ctrl` + `Alt` + `Enter`                | Cancel current generation                                                                                          |
| `Ctrl` + `Z`/`Ctrl` + `Y`                 | Undo/Redo                                                                                                          |
| `Ctrl` + `S`                          | Save workflow                                                                                                      |
| `Ctrl` + `O`                          | Load workflow                                                                                                      |
| `Ctrl` + `A`                          | Select all nodes                                                                                                   |
| `Alt `+ `C`                           | Collapse/uncollapse selected nodes                                                                                 |
| `Ctrl` + `M`                          | Mute/unmute selected nodes                                                                                         |
| `Ctrl` + `B`                           | Bypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)            |
| `Delete`/`Backspace`                   | Delete selected nodes                                                                                              |
| `Ctrl` + `Backspace`                   | Delete the current graph                                                                                           |
| `Space`                              | Move the canvas around when held and moving the cursor                                                             |
| `Ctrl`/`Shift` + `Click`                 | Add clicked node to selection                                                                                      |
| `Ctrl` + `C`/`Ctrl` + `V`                  | Copy and paste selected nodes (without maintaining connections to outputs of unselected nodes)                     |
| `Ctrl` + `C`/`Ctrl` + `Shift` + `V`          | Copy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes) |
| `Shift` + `Drag`                       | Move multiple selected nodes at the same time                                                                      |
| `Ctrl` + `D`                           | Load default graph                                                                                                 |
| `Alt` + `+`                          | Canvas Zoom in                                                                                                     |
| `Alt` + `-`                          | Canvas Zoom out                                                                                                    |
| `Ctrl` + `Shift` + LMB + Vertical drag | Canvas Zoom in/out                                                                                                 |
| `P`                                  | Pin/Unpin selected nodes                                                                                           |
| `Ctrl` + `G`                           | Group selected nodes                                                                                               |
| `Q`                                 | Toggle visibility of the queue                                                                                     |
| `H`                                  | Toggle visibility of history                                                                                       |
| `R`                                  | Refresh graph                                                                                                      |
| `F`                                  | Show/Hide menu                                                                                                      |
| `.`                                  | Fit view to selection (Whole graph when nothing is selected)                                                        |
| Double-Click LMB                   | Open node quick search palette                                                                                     |
| `Shift` + Drag                       | Move multiple wires at once                                                                                        |
| `Ctrl` + `Alt` + LMB                   | Disconnect all wires from clicked slot                                                                             |

`Ctrl` can also be replaced with `Cmd` instead for macOS users

# Installing

## Windows Portable

There is a portable standalone build for Windows that should work for running on Nvidia GPUs or for running on your CPU only on the [releases page](https://github.com/comfyanonymous/ComfyUI/releases).

### [Direct link to download](https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z)

Simply download, extract with [7-Zip](https://7-zip.org) and run. Make sure you put your Stable Diffusion checkpoints/models (the huge ckpt/safetensors files) in: ComfyUI\models\checkpoints

If you have trouble extracting it, right click the file -&gt; properties -&gt; unblock

If you have a 50 series Blackwell card like a 5090 or 5080 see [this discussion thread](https://github.com/comfyanonymous/ComfyUI/discussions/6643)

#### How do I share models between another UI and ComfyUI?

See the [Config file](extra_model_paths.yaml.example) to set the search paths for models. In the standalone windows build you can find this file in the ComfyUI directory. Rename this file to extra_model_paths.yaml and edit it with your favorite text editor.

## Jupyter Notebook

To run it on services like paperspace, kaggle or colab you can use my [Jupyter Notebook](notebooks/comfyui_colab.ipynb)


## [comfy-cli](https://docs.comfy.org/comfy-cli/getting-started)

You can install and start ComfyUI using comfy-cli:
```bash
pip install comfy-cli
comfy install
```

## Manual Install (Windows, Linux)

python 3.13 is supported but using 3.12 is recommended because some custom nodes and their dependencies might not support it yet.

Git clone this repo.

Put your SD checkpoints (the huge ckpt/safetensors files) in: models/checkpoints

Put your VAE in: models/vae


### AMD GPUs (Linux only)
AMD users can install rocm and pytorch with pip if you don&#039;t have it already installed, this is the command to install the stable version:

```pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.2.4```

This is the command to install the nightly with ROCm 6.3 which might have some performance improvements:

```pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.3```

### Intel GPUs (Windows and Linux)

(Option 1) Intel Arc GPU users can install native PyTorch with torch.xpu support using pip (currently available in PyTorch nightly builds). More information can be found [here](https://pytorch.org/docs/main/notes/get_start_xpu.html)
  
1. To install PyTorch nightly, use the following command:

```pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/xpu```

2. Launch ComfyUI by running `python main.py`


(Option 2) Alternatively, Intel GPUs supported by Intel Extension for PyTorch (IPEX) can leverage IPEX for improved performance.

1. For Intel¬Æ Arc‚Ñ¢ A-Series Graphics utilizing IPEX, create a conda environment and use the commands below:

```
conda install libuv
pip install torch==2.3.1.post0+cxx11.abi torchvision==0.18.1.post0+cxx11.abi torchaudio==2.3.1.post0+cxx11.abi intel-extension-for-pytorch==2.3.110.post0+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/ --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/cn/
```

For other supported Intel GPUs with IPEX, visit [Installation](https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=gpu) for more information.

Additional discussion and help can be found [here](https://github.com/comfyanonymous/ComfyUI/discussions/476).

### NVIDIA

Nvidia users should install stable pytorch using this command:

```pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu126```

This is the command to install pytorch nightly instead which supports the new blackwell 50xx series GPUs and might have performance improvements.

```pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128```

#### Troubleshooting

If you get the &quot;Torch not compiled with CUDA enabled&quot; error, uninstall torch with:

```pip uninstall torch```

And install it again with the command above.

### Dependencies

Install the dependencies by opening your terminal inside the ComfyUI folder and:

```pip install -r requirements.txt```

After this you should have everything installed and can proceed to running ComfyUI.

### Others:

#### Apple Mac silicon

You can install ComfyUI in Apple Mac silicon (M1 or M2) with any recent macOS version.

1. Install pytorch nightly. For instructions, read the [Accelerated PyTorch training on Mac](https://developer.apple.com/metal/pytorch/) Apple Developer guide (make sure to install the latest pytorch nightly).
1. Follow the [ComfyUI manual installation](#manual-install-windows-linux) instructions for Windows and Linux.
1. Install the ComfyUI [dependencies](#dependencies). If you have another Stable Diffusion UI [you might be able to reuse the dependencies](#i-already-have-another-ui-for-stable-diffusion-installed-do-i-really-have-to-install-all-of-these-dependencies).
1. Launch ComfyUI by running `python main.py`

&gt; **Note**: Remember to add your models, VAE, LoRAs etc. to the corresponding Comfy folders, as discussed in [ComfyUI manual installation](#manual-install-windows-linux).

#### DirectML (AMD Cards on Windows)

```pip install torch-directml``` Then you can launch ComfyUI with: ```python main.py --directml```

#### Ascend NPUs

For models compatible with Ascend Extension for PyTorch (torch_npu). To get started, ensure your environment meets the prerequisites outlined on the [installation](https://ascend.github.io/docs/sources/ascend/quick_install.html) page. Here&#039;s a step-by-step guide tailored to your platform and installation method:

1. Begin by installing the recommended or newer kernel version for Linux as specified in the Installation page of torch-npu, if necessary.
2. Proceed with the installation of Ascend Basekit, which includes the driver, firmware, and CANN, following the instructions provided for your specific platform.
3. Next, install the necessary packages for torch-npu by adhering to the platform-specific instructions on the [Installation](https://ascend.github.io/docs/sources/pytorch/install.html#pytorch) page.
4. Finally, adhere to the [ComfyUI manual installation](#manual-install-windows-linux) guide for Linux. Once all components are installed, you can run ComfyUI as described earlier.

#### Cambricon MLUs

For models compatible with Cambricon Extension for PyTorch (torch_mlu). Here&#039;s a step-by-step guide tailored to your platform and installation method:

1. Install the Cambricon CNToolkit by adhering to the platform-specific instructions on the [Installation](https://www.cambricon.com/docs/sdk_1.15.0/cntoolkit_3.7.2/cntoolkit_install_3.7.2/index.html)
2. Next, install the PyTorch(torch_mlu) following the instructions on the [Installation](https://www.cambricon.com/docs/sdk_1.15.0/cambricon_pytorch_1.17.0/user_guide_1.9/index.html)
3. Launch ComfyUI by running `python main.py`

# Running

```python main.py```

### For AMD cards not officially supported by ROCm

Try running it with this command if you have issues:

For 6700, 6600 and maybe other RDNA2 or older: ```HSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py```

For AMD 7600 and maybe other RDNA3 cards: ```HSA_OVERRIDE_GFX_VERSION=11.0.0 python main.py```

### AMD ROCm Tips

You can enable experimental memory efficient attention on pytorch 2.5 in ComfyUI on RDNA3 and potentially other AMD GPUs using this command:

```TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1 python main.py --use-pytorch-cross-attention```

You can also try setting this env variable `PYTORCH_TUNABLEOP_ENABLED=1` which might speed things up at the cost of a very slow initial run.

# Notes

Only parts of the graph that have an output with all the correct inputs will be executed.

Only parts of the graph that change from each execution to the next will be executed, if you submit the same graph twice only the first will be executed. If you change the last part of the graph only the part you changed and the part that depends on it will be executed.

Dragging a generated png on the webpage or loading one will give you the full workflow including seeds that were used to create it.

You can use () to change emphasis of a word or phrase like: (good code:1.2) or (bad code:0.8). The default emphasis for () is 1.1. To use () characters in your actual prompt escape them like \\( or \\).

You can use {day|night}, for wildcard/dynamic prompts. With 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hinthornw/trustcall]]></title>
            <link>https://github.com/hinthornw/trustcall</link>
            <guid>https://github.com/hinthornw/trustcall</guid>
            <pubDate>Mon, 31 Mar 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[Tenacious tool calling built on LangGraph]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hinthornw/trustcall">hinthornw/trustcall</a></h1>
            <p>Tenacious tool calling built on LangGraph</p>
            <p>Language: Python</p>
            <p>Stars: 570</p>
            <p>Forks: 52</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre># ü§ùtrustcall

[![CI](https://github.com/hinthornw/trustcall/actions/workflows/test.yml/badge.svg)](https://github.com/hinthornw/trustcall/actions/workflows/test.yml)

![](_static/cover.png)

LLMs struggle when asked to generate or modify large JSON blobs. `trustcall` solves this by asking the LLM to generate [JSON patch](https://datatracker.ietf.org/doc/html/rfc6902) operations. This is a simpler task that can be done iteratively. This enables:

- ‚ö° Faster &amp; cheaper generation of structured output.
- üê∫Resilient retrying of validation errors, even for complex, nested schemas (defined as pydantic, schema dictionaries, or regular python functions)
- üß©Acccurate updates to existing schemas, avoiding undesired deletions.

Works flexibly across a number of common LLM workflows like:

- ‚úÇÔ∏è Extraction
- üß≠ LLM routing
- ü§ñ Multi-step agent tool use

## Installation

`pip install trustcall`

## Usage

- [Extracting complex schemas](#complex-schema)
- [Updating schemas](#updating-schemas)
- [Simultanous updates &amp; insertions](#simultanous-updates--insertions)

## Why trustcall?

[Tool calling](https://python.langchain.com/docs/how_to/tool_calling/) makes it easier to compose LLM calls within reliable software systems, but LLM&#039;s today can be error prone and inefficient in two common scenarios:

1. Populating complex, nested schemas
2. Updating existing schemas without information loss

These problems are both exaggerated when you want to handle multiple tool calls.

Trustcall increases structured extraction reliability without restricting you to a subset of the JSON schema.

Let&#039;s see a couple examples to see what we mean.

### Complex schema

Take the following example:

&lt;details&gt;
    &lt;summary&gt;Schema definition&lt;/summary&gt;

    from typing import List, Optional

    from pydantic import BaseModel


    class OutputFormat(BaseModel):
        preference: str
        sentence_preference_revealed: str


    class TelegramPreferences(BaseModel):
        preferred_encoding: Optional[List[OutputFormat]] = None
        favorite_telegram_operators: Optional[List[OutputFormat]] = None
        preferred_telegram_paper: Optional[List[OutputFormat]] = None


    class MorseCode(BaseModel):
        preferred_key_type: Optional[List[OutputFormat]] = None
        favorite_morse_abbreviations: Optional[List[OutputFormat]] = None


    class Semaphore(BaseModel):
        preferred_flag_color: Optional[List[OutputFormat]] = None
        semaphore_skill_level: Optional[List[OutputFormat]] = None


    class TrustFallPreferences(BaseModel):
        preferred_fall_height: Optional[List[OutputFormat]] = None
        trust_level: Optional[List[OutputFormat]] = None
        preferred_catching_technique: Optional[List[OutputFormat]] = None


    class CommunicationPreferences(BaseModel):
        telegram: TelegramPreferences
        morse_code: MorseCode
        semaphore: Semaphore


    class UserPreferences(BaseModel):
        communication_preferences: CommunicationPreferences
        trust_fall_preferences: TrustFallPreferences


    class TelegramAndTrustFallPreferences(BaseModel):
        pertinent_user_preferences: UserPreferences

&lt;/details&gt;
    If you naively extract these values using `gpt-4o`, it&#039;s prone to failure:

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model=&quot;gpt-4o&quot;)
bound = llm.with_structured_output(TelegramAndTrustFallPreferences)

conversation = &quot;&quot;&quot;Operator: How may I assist with your telegram, sir?
Customer: I need to send a message about our trust fall exercise.
Operator: Certainly. Morse code or standard encoding?
Customer: Morse, please. I love using a straight key.
Operator: Excellent. What&#039;s your message?
Customer: Tell him I&#039;m ready for a higher fall, and I prefer the diamond formation for catching.
Operator: Done. Shall I use our &quot;Daredevil&quot; paper for this daring message?
Customer: Perfect! Send it by your fastest carrier pigeon.
Operator: It&#039;ll be there within the hour, sir.&quot;&quot;&quot;

bound.invoke(f&quot;&quot;&quot;Extract the preferences from the following conversation:
&lt;convo&gt;
{conversation}
&lt;/convo&gt;&quot;&quot;&quot;)
```

```
ValidationError: 1 validation error for TelegramAndTrustFallPreferences
pertinent_user_preferences.communication_preferences.semaphore
  Input should be a valid dictionary or instance of Semaphore [type=model_type, input_value=None, input_type=NoneType]
    For further information visit https://errors.pydantic.dev/2.8/v/model_type
```

If you try to use **strict** mode or OpenAI&#039;s `json_schema`, it will give you an error as well, since their parser doesn&#039;t support the complex JSON schemas:

```python
bound = llm.bind_tools([TelegramAndTrustFallPreferences], strict=True, response_format=TelegramAndTrustFallPreferences)

bound.invoke(f&quot;&quot;&quot;Extract the preferences from the following conversation:
&lt;convo&gt;
{conversation}
&lt;/convo&gt;&quot;&quot;&quot;)
```

```text
BadRequestError: Error code: 400 - {&#039;error&#039;: {&#039;message&#039;: &quot;Invalid schema for function &#039;TelegramAndTrustFallPreferences&#039;: &quot;}}
```

With `trustcall`, this extraction task is easy.

```python
from trustcall import create_extractor

bound = create_extractor(
    llm,
    tools=[TelegramAndTrustFallPreferences],
    tool_choice=&quot;TelegramAndTrustFallPreferences&quot;,
)

result = bound.invoke(
    f&quot;&quot;&quot;Extract the preferences from the following conversation:
&lt;convo&gt;
{conversation}
&lt;/convo&gt;&quot;&quot;&quot;
)
result[&quot;responses&quot;][0]
```

```python
{
    &quot;pertinent_user_preferences&quot;: {
        &quot;communication_preferences&quot;: {
            &quot;telegram&quot;: {
                &quot;preferred_encoding&quot;: [
                    {
                        &quot;preference&quot;: &quot;morse&quot;,
                        &quot;sentence_preference_revealed&quot;: &quot;Morse, please.&quot;,
                    }
                ],
                &quot;favorite_telegram_operators&quot;: None,
                &quot;preferred_telegram_paper&quot;: [
                    {
                        &quot;preference&quot;: &quot;Daredevil&quot;,
                        &quot;sentence_preference_revealed&quot;: &#039;Shall I use our &quot;Daredevil&quot; paper for this daring message?&#039;,
                    }
                ],
            },
            &quot;morse_code&quot;: {
                &quot;preferred_key_type&quot;: [
                    {
                        &quot;preference&quot;: &quot;straight key&quot;,
                        &quot;sentence_preference_revealed&quot;: &quot;I love using a straight key.&quot;,
                    }
                ],
                &quot;favorite_morse_abbreviations&quot;: None,
            },
            &quot;semaphore&quot;: {&quot;preferred_flag_color&quot;: None, &quot;semaphore_skill_level&quot;: None},
        },
        &quot;trust_fall_preferences&quot;: {
            &quot;preferred_fall_height&quot;: [
                {
                    &quot;preference&quot;: &quot;higher&quot;,
                    &quot;sentence_preference_revealed&quot;: &quot;I&#039;m ready for a higher fall.&quot;,
                }
            ],
            &quot;trust_level&quot;: None,
            &quot;preferred_catching_technique&quot;: [
                {
                    &quot;preference&quot;: &quot;diamond formation&quot;,
                    &quot;sentence_preference_revealed&quot;: &quot;I prefer the diamond formation for catching.&quot;,
                }
            ],
        },
    }
}
```

What&#039;s different? `trustcall` handles prompt retries with a twist: rather than naively re-generating the full output, it prompts the LLM to generate a concise patch to fix the error in question. This is both **more reliable** than naive reprompting and **cheaper** since you only regenerate a subset of the full schema.

The &quot;patch-don&#039;t-post&quot; mantra affords us better performance in other ways too! Let&#039;s see how it helps **updates**.

### Updating schemas

Many tasks expect an LLM to correct or modify an existing object based on new information.

Take memory management as an example. Suppose you structure memories as JSON objects. When new information is provided, the LLM must reconcile this information with the existing document. Let&#039;s try this using naive regeneration of the document. We&#039;ll model memory as a single user profile:

```python
from typing import Dict, List, Optional

from pydantic import BaseModel


class Address(BaseModel):
    street: str
    city: str
    country: str
    postal_code: str


class Pet(BaseModel):
    kind: str
    name: Optional[str]
    age: Optional[int]


class Hobby(BaseModel):
    name: str
    skill_level: str
    frequency: str


class FavoriteMedia(BaseModel):
    shows: List[str]
    movies: List[str]
    books: List[str]


class User(BaseModel):
    preferred_name: str
    favorite_media: FavoriteMedia
    favorite_foods: List[str]
    hobbies: List[Hobby]
    age: int
    occupation: str
    address: Address
    favorite_color: Optional[str] = None
    pets: Optional[List[Pet]] = None
    languages: Dict[str, str] = {}
```

And set a starting profile state:

&lt;details&gt;
&lt;summary&gt;Starting profile&lt;/summary&gt;

    initial_user = User(
        preferred_name=&quot;Alex&quot;,
        favorite_media=FavoriteMedia(
            shows=[
                &quot;Friends&quot;,
                &quot;Game of Thrones&quot;,
                &quot;Breaking Bad&quot;,
                &quot;The Office&quot;,
                &quot;Stranger Things&quot;,
            ],
            movies=[&quot;The Shawshank Redemption&quot;, &quot;Inception&quot;, &quot;The Dark Knight&quot;],
            books=[&quot;1984&quot;, &quot;To Kill a Mockingbird&quot;, &quot;The Great Gatsby&quot;],
        ),
        favorite_foods=[&quot;sushi&quot;, &quot;pizza&quot;, &quot;tacos&quot;, &quot;ice cream&quot;, &quot;pasta&quot;, &quot;curry&quot;],
        hobbies=[
            Hobby(name=&quot;reading&quot;, skill_level=&quot;expert&quot;, frequency=&quot;daily&quot;),
            Hobby(name=&quot;hiking&quot;, skill_level=&quot;intermediate&quot;, frequency=&quot;weekly&quot;),
            Hobby(name=&quot;photography&quot;, skill_level=&quot;beginner&quot;, frequency=&quot;monthly&quot;),
            Hobby(name=&quot;biking&quot;, skill_level=&quot;intermediate&quot;, frequency=&quot;weekly&quot;),
            Hobby(name=&quot;swimming&quot;, skill_level=&quot;expert&quot;, frequency=&quot;weekly&quot;),
            Hobby(name=&quot;canoeing&quot;, skill_level=&quot;beginner&quot;, frequency=&quot;monthly&quot;),
            Hobby(name=&quot;sailing&quot;, skill_level=&quot;intermediate&quot;, frequency=&quot;monthly&quot;),
            Hobby(name=&quot;weaving&quot;, skill_level=&quot;beginner&quot;, frequency=&quot;weekly&quot;),
            Hobby(name=&quot;painting&quot;, skill_level=&quot;intermediate&quot;, frequency=&quot;weekly&quot;),
            Hobby(name=&quot;cooking&quot;, skill_level=&quot;expert&quot;, frequency=&quot;daily&quot;),
        ],
        age=28,
        occupation=&quot;Software Engineer&quot;,
        address=Address(
            street=&quot;123 Tech Lane&quot;, city=&quot;San Francisco&quot;, country=&quot;USA&quot;, postal_code=&quot;94105&quot;
        ),
        favorite_color=&quot;blue&quot;,
        pets=[Pet(kind=&quot;cat&quot;, name=&quot;Luna&quot;, age=3)],
        languages={&quot;English&quot;: &quot;native&quot;, &quot;Spanish&quot;: &quot;intermediate&quot;, &quot;Python&quot;: &quot;expert&quot;},
    )

&lt;/details&gt;

Giving the following conversation, we&#039;d expect the memory to be **expanded** to include video gaming but not drop any other information:

```python

conversation = &quot;&quot;&quot;Friend: Hey Alex, how&#039;s the new job going? I heard you switched careers recently.
Alex: It&#039;s going great! I&#039;m loving my new role as a Data Scientist. The work is challenging but exciting. I&#039;ve moved to a new apartment in New York to be closer to the office.
Friend: That&#039;s a big change! Are you still finding time for your hobbies?
Alex: Well, I&#039;ve had to cut back on some. I&#039;m not doing much sailing or canoeing these days. But I&#039;ve gotten really into machine learning projects in my free time. I&#039;d say I&#039;m getting pretty good at it - probably an intermediate level now.
Friend: Sounds like you&#039;re keeping busy! How&#039;s Luna doing?
Alex: Oh, Luna&#039;s great. She just turned 4 last week. She&#039;s actually made friends with my new pet, Max the dog. He&#039;s a playful 2-year-old golden retriever.
Friend: Two pets now! That&#039;s exciting. Hey, want to catch the new season of Stranger Things this weekend?
Alex: Actually, I&#039;ve kind of lost interest in that show. But I&#039;m really into this new series called &quot;The Mandalorian&quot;. We could watch that instead! Oh, and I recently watched &quot;Parasite&quot; - it&#039;s become one of my favorite movies.
Friend: Sure, that sounds fun. Should I bring some food? I remember you love sushi.
Alex: Sushi would be perfect! Or maybe some Thai food - I&#039;ve been really into that lately. By the way, I&#039;ve been practicing my French. I&#039;d say I&#039;m at a beginner level now.
Friend: That&#039;s great! You&#039;re always learning something new. How&#039;s the cooking going?
Alex: It&#039;s going well! I&#039;ve been cooking almost every day now. I&#039;d say I&#039;ve become quite proficient at it.&quot;&quot;&quot;


# Naive approach
bound = llm.with_structured_output(User)
naive_result = bound.invoke(
    f&quot;&quot;&quot;Update the memory (JSON doc) to incorporate new information from the following conversation:
&lt;user_info&gt;
{initial_user.model_dump()}
&lt;/user_info&gt;
&lt;convo&gt;
{conversation}
&lt;/convo&gt;&quot;&quot;&quot;
)
print(&quot;Naive approach result:&quot;)
naive_output = naive_result.model_dump()
print(naive_output)
```

&lt;details&gt;
    &lt;summary&gt;Naive output&lt;/summary&gt;
    {
        &quot;preferred_name&quot;: &quot;Alex&quot;,
        &quot;favorite_media&quot;: {
            &quot;shows&quot;: [&quot;Friends&quot;, &quot;Game of Thrones&quot;, &quot;Breaking Bad&quot;, &quot;The Office&quot;],
            &quot;movies&quot;: [
                &quot;The Shawshank Redemption&quot;,
                &quot;Inception&quot;,
                &quot;The Dark Knight&quot;,
                &quot;Parasite&quot;,
            ],
            &quot;books&quot;: [&quot;1984&quot;, &quot;To Kill a Mockingbird&quot;, &quot;The Great Gatsby&quot;],
        },
        &quot;favorite_foods&quot;: [
            &quot;sushi&quot;,
            &quot;pizza&quot;,
            &quot;tacos&quot;,
            &quot;ice cream&quot;,
            &quot;pasta&quot;,
            &quot;curry&quot;,
            &quot;Thai food&quot;,
        ],
        &quot;hobbies&quot;: [
            {&quot;name&quot;: &quot;reading&quot;, &quot;skill_level&quot;: &quot;expert&quot;, &quot;frequency&quot;: &quot;daily&quot;},
            {&quot;name&quot;: &quot;hiking&quot;, &quot;skill_level&quot;: &quot;intermediate&quot;, &quot;frequency&quot;: &quot;weekly&quot;},
            {&quot;name&quot;: &quot;photography&quot;, &quot;skill_level&quot;: &quot;beginner&quot;, &quot;frequency&quot;: &quot;monthly&quot;},
            {&quot;name&quot;: &quot;biking&quot;, &quot;skill_level&quot;: &quot;intermediate&quot;, &quot;frequency&quot;: &quot;weekly&quot;},
            {&quot;name&quot;: &quot;swimming&quot;, &quot;skill_level&quot;: &quot;expert&quot;, &quot;frequency&quot;: &quot;weekly&quot;},
            {&quot;name&quot;: &quot;weaving&quot;, &quot;skill_level&quot;: &quot;beginner&quot;, &quot;frequency&quot;: &quot;weekly&quot;},
            {&quot;name&quot;: &quot;painting&quot;, &quot;skill_level&quot;: &quot;intermediate&quot;, &quot;frequency&quot;: &quot;weekly&quot;},
            {&quot;name&quot;: &quot;cooking&quot;, &quot;skill_level&quot;: &quot;expert&quot;, &quot;frequency&quot;: &quot;daily&quot;},
            {
                &quot;name&quot;: &quot;machine learning projects&quot;,
                &quot;skill_level&quot;: &quot;intermediate&quot;,
                &quot;frequency&quot;: &quot;free time&quot;,
            },
        ],
        &quot;age&quot;: 28,
        &quot;occupation&quot;: &quot;Data Scientist&quot;,
        &quot;address&quot;: {
            &quot;street&quot;: &quot;New Apartment&quot;,
            &quot;city&quot;: &quot;New York&quot;,
            &quot;country&quot;: &quot;USA&quot;,
            &quot;postal_code&quot;: &quot;unknown&quot;,
        },
        &quot;favorite_color&quot;: &quot;blue&quot;,
        &quot;pets&quot;: [
            {&quot;kind&quot;: &quot;cat&quot;, &quot;name&quot;: &quot;Luna&quot;, &quot;age&quot;: 4},
            {&quot;kind&quot;: &quot;dog&quot;, &quot;name&quot;: &quot;Max&quot;, &quot;age&quot;: 2},
        ],
        &quot;languages&quot;: {},
    }

&lt;/details&gt;

You&#039;ll notice that all the &quot;languages&quot; section was dropped here, and &quot;The Mandalorian&quot; was omitted. Alex may be injured, but he didn&#039;t forget how to speak!

When you run this code, it&#039;s _possible_ it will get it right: LLMs are stochastic after all (which is a good thing). And you could definitely prompt engineer it to be more reliable, but **that&#039;s not good enough.**

For memory management, you will be updating objects **constantly**, and it&#039;s still **too easy** for LLMs to &quot;accidentally&quot; omit information when generating updates, or to miss content in the conversation.

`trustcall` lets the LLM **focus on what has changed**.

```python
# Trustcall approach
from trustcall import create_extractor

bound = create_extractor(llm, tools=[User])

trustcall_result = bound.invoke(
    {
        &quot;messages&quot;: [
            {
                &quot;role&quot;: &quot;user&quot;,
                &quot;content&quot;: f&quot;&quot;&quot;Update the memory (JSON doc) to incorporate new information from the following conversation:
&lt;convo&gt;
{conversation}
&lt;/convo&gt;&quot;&quot;&quot;,
            }
        ],
        &quot;existing&quot;: {&quot;User&quot;: initial_user.model_dump()},
    }
)
print(&quot;\nTrustcall approach result:&quot;)
trustcall_output = trustcall_result[&quot;responses&quot;][0].model_dump()
print(trustcall_output)
```

Output:

&lt;details&gt;
    &lt;summary&gt;`trustcall` output&lt;/summary&gt;

    {
    &quot;preferred_name&quot;: &quot;Alex&quot;,
    &quot;favorite_media&quot;: {
        &quot;shows&quot;: [
            &quot;Friends&quot;,
            &quot;Game of Thrones&quot;,
            &quot;Breaking Bad&quot;,
            &quot;The Office&quot;,
            &quot;The Mandalorian&quot;,
        ],
        &quot;movies&quot;: [
            &quot;The Shawshank Redemption&quot;,
            &quot;Inception&quot;,
            &quot;The Dark Knight&quot;,
            &quot;Parasite&quot;,
        ],
        &quot;books&quot;: [&quot;1984&quot;, &quot;To Kill a Mockingbird&quot;, &quot;The Great Gatsby&quot;],
    },
    &quot;favorite_foods&quot;: [
        &quot;sushi&quot;,
        &quot;pizza&quot;,
        &quot;tacos&quot;,
        &quot;ice cream&quot;,
        &quot;pasta&quot;,
        &quot;curry&quot;,
        &quot;Thai food&quot;,
    ],
    &quot;hobbies&quot;: [
        {&quot;name&quot;: &quot;reading&quot;, &quot;skill_level&quot;: &quot;expert&quot;, &quot;frequency&quot;: &quot;daily&quot;},
        {&quot;name&quot;: &quot;hiking&quot;, &quot;skill_level&quot;: &quot;intermediate&quot;, &quot;frequency&quot;: &quot;weekly&quot;},
        {&quot;name&quot;: &quot;photography&quot;, &quot;skill_level&quot;: &quot;beginner&quot;, &quot;frequency&quot;: &quot;monthly&quot;},
        {&quot;name&quot;: &quot;biking&quot;, &quot;skill_level&quot;: &quot;intermediate&quot;, &quot;frequency&quot;: &quot;weekly&quot;},
        {&quot;name&quot;: &quot;swimming&quot;, &quot;skill_level&quot;: &quot;expert&quot;, &quot;frequency&quot;: &quot;weekly&quot;},
        {&quot;name&quot;: &quot;weaving&quot;, &quot;skill_level&quot;: &quot;beginner&quot;, &quot;frequency&quot;: &quot;weekly&quot;},
        {&quot;name&quot;: &quot;painting&quot;, &quot;skill_level&quot;: &quot;intermediate&quot;, &quot;frequency&quot;: &quot;weekly&quot;},
        {&quot;name&quot;: &quot;cooking&quot;, &quot;skill_level&quot;: &quot;expert&quot;, &quot;frequency&quot;: &quot;daily&quot;},
        {
            &quot;name&quot;: &quot;machine learning projects&quot;,
            &quot;skill_level&quot;: &quot;intermediate&quot;,
            &quot;frequency&quot;: &quot;daily&quot;,
        },
    ],
    &quot;age&quot;: 28,
    &quot;occupation&quot;: &quot;Data Scientist&quot;,
    &quot;address&quot;: {
        &quot;street&quot;: &quot;New Apartment&quot;,
        &quot;city&quot;: &quot;New York&quot;,
        &quot;country&quot;: &quot;USA&quot;,
        &quot;postal_code&quot;: &quot;10001&quot;,
    },
    &quot;favorite_color&quot;: &quot;blue&quot;,
    &quot;pets&quot;: [
        {&quot;kind&quot;: &quot;cat&quot;, &quot;name&quot;: &quot;Luna&quot;, &quot;age&quot;: 4},
        {&quot;kind&quot;: &quot;dog&quot;, &quot;name&quot;: &quot;Max&quot;, &quot;age&quot;: 2},
    ],
    &quot;languages&quot;: {
        &quot;English&quot;: &quot;native&quot;,
        &quot;Spanish&quot;: &quot;intermediate&quot;,
        &quot;Python&quot;: &quot;expert&quot;,
        &quot;French&quot;: &quot;beginner&quot;,
    },
    }

&lt;/details&gt;

No fields omitted, and the important new information is seamlessly integrated.

### Simultanous updates &amp; insertions

Both problems above (difficulty with type-safe generation of complex schemas &amp; difficulty with generating the correct edits to existing schemas) are compounded when you have to be prompting the LLM to handle **both** updates **and** inserts, as is often the case when extracting multiple memory &quot;events&quot; from conversations.

Let&#039;s see an example below. Suppose you are managing a list of &quot;relationships&quot;:

```python
import uuid
from typing import List, Optional

from pydantic import BaseModel, Field


class Person(BaseModel):
    &quot;&quot;&quot;Someone the user knows or interacts with.&quot;&quot;&quot;

    name: str
    relationship: str = Field(description=&quot;How they relate to the user.&quot;)

    notes: List[str] = Field(
        description=&quot;Memories and other observations about the person&quot;
    )


# Initial data
initial_people = [
    Person(
        name=&quot;Emma Thompson&quot;,
        relationship=&quot;College friend&quot;,
        notes=[&quot;Loves hiking&quot;, &quot;Works in marketing&quot;, &quot;Has a dog named Max&quot;],
    ),
    Person(
        name=&quot;Michael Chen&quot;,
        relationship=&quot;Coworker&quot;,
        notes=[&quot;Great at problem-solving&quot;, &quot;Vegetarian&quot;, &quot;Plays guitar&quot;],
    ),
    Person(
        name=&quot;Sarah Johnson&quot;,
        relationship=&quot;Neighbor&quot;,
        notes=[&quot;Has two kids&quot;, &quot;Loves gardening&quot;, &quot;Makes amazing cookies&quot;],
    ),
]

# Convert to the format expected by the extractor
existing_data = [
    (str(i), &quot;Person&quot;, person.model_dump()) for i, person in enumerate(initial_people)
]
```

```python
conversation = &quot;&quot;&quot;
Me: I ran into Emma Thompson at the park yesterday. She was walking her new puppy, a golden retriever named Sunny. She mentioned she got promoted to Senior Marketing Manager last month.
Friend: That&#039;s great news for Emma! How&#039;s she enjoying the new role?
Me: She seems to be thriving. Oh, and did you know she&#039;s taken up rock climbing? She invited me to join her at the climbing gym sometime.
Friend: Wow, rock climbing? That&#039;s quite a change from hiking. Speaking of friends, have you heard from Michael Chen recently?
Me: Actually, yes. We had a video call last week. He&#039;s switched jobs and is

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[yt-dlp/yt-dlp]]></title>
            <link>https://github.com/yt-dlp/yt-dlp</link>
            <guid>https://github.com/yt-dlp/yt-dlp</guid>
            <pubDate>Mon, 31 Mar 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[A feature-rich command-line audio/video downloader]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/yt-dlp/yt-dlp">yt-dlp/yt-dlp</a></h1>
            <p>A feature-rich command-line audio/video downloader</p>
            <p>Language: Python</p>
            <p>Stars: 106,062</p>
            <p>Forks: 8,323</p>
            <p>Stars today: 132 stars today</p>
            <h2>README</h2><pre>&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
&lt;div align=&quot;center&quot;&gt;

[![YT-DLP](https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/.github/banner.svg)](#readme)

[![Release version](https://img.shields.io/github/v/release/yt-dlp/yt-dlp?color=brightgreen&amp;label=Download&amp;style=for-the-badge)](#installation &quot;Installation&quot;)
[![PyPI](https://img.shields.io/badge/-PyPI-blue.svg?logo=pypi&amp;labelColor=555555&amp;style=for-the-badge)](https://pypi.org/project/yt-dlp &quot;PyPI&quot;)
[![Donate](https://img.shields.io/badge/_-Donate-red.svg?logo=githubsponsors&amp;labelColor=555555&amp;style=for-the-badge)](Collaborators.md#collaborators &quot;Donate&quot;)
[![Discord](https://img.shields.io/discord/807245652072857610?color=blue&amp;labelColor=555555&amp;label=&amp;logo=discord&amp;style=for-the-badge)](https://discord.gg/H5MNcFW63r &quot;Discord&quot;)
[![Supported Sites](https://img.shields.io/badge/-Supported_Sites-brightgreen.svg?style=for-the-badge)](supportedsites.md &quot;Supported Sites&quot;)
[![License: Unlicense](https://img.shields.io/badge/-Unlicense-blue.svg?style=for-the-badge)](LICENSE &quot;License&quot;)
[![CI Status](https://img.shields.io/github/actions/workflow/status/yt-dlp/yt-dlp/core.yml?branch=master&amp;label=Tests&amp;style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/actions &quot;CI Status&quot;)
[![Commits](https://img.shields.io/github/commit-activity/m/yt-dlp/yt-dlp?label=commits&amp;style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/commits &quot;Commit History&quot;)
[![Last Commit](https://img.shields.io/github/last-commit/yt-dlp/yt-dlp/master?label=&amp;style=for-the-badge&amp;display_timestamp=committer)](https://github.com/yt-dlp/yt-dlp/pulse/monthly &quot;Last activity&quot;)

&lt;/div&gt;
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

yt-dlp is a feature-rich command-line audio/video downloader with support for [thousands of sites](supportedsites.md). The project is a fork of [youtube-dl](https://github.com/ytdl-org/youtube-dl) based on the now inactive [youtube-dlc](https://github.com/blackjack4494/yt-dlc).

&lt;!-- MANPAGE: MOVE &quot;USAGE AND OPTIONS&quot; SECTION HERE --&gt;

&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
* [INSTALLATION](#installation)
    * [Detailed instructions](https://github.com/yt-dlp/yt-dlp/wiki/Installation)
    * [Release Files](#release-files)
    * [Update](#update)
    * [Dependencies](#dependencies)
    * [Compile](#compile)
* [USAGE AND OPTIONS](#usage-and-options)
    * [General Options](#general-options)
    * [Network Options](#network-options)
    * [Geo-restriction](#geo-restriction)
    * [Video Selection](#video-selection)
    * [Download Options](#download-options)
    * [Filesystem Options](#filesystem-options)
    * [Thumbnail Options](#thumbnail-options)
    * [Internet Shortcut Options](#internet-shortcut-options)
    * [Verbosity and Simulation Options](#verbosity-and-simulation-options)
    * [Workarounds](#workarounds)
    * [Video Format Options](#video-format-options)
    * [Subtitle Options](#subtitle-options)
    * [Authentication Options](#authentication-options)
    * [Post-processing Options](#post-processing-options)
    * [SponsorBlock Options](#sponsorblock-options)
    * [Extractor Options](#extractor-options)
* [CONFIGURATION](#configuration)
    * [Configuration file encoding](#configuration-file-encoding)
    * [Authentication with netrc](#authentication-with-netrc)
    * [Notes about environment variables](#notes-about-environment-variables)
* [OUTPUT TEMPLATE](#output-template)
    * [Output template examples](#output-template-examples)
* [FORMAT SELECTION](#format-selection)
    * [Filtering Formats](#filtering-formats)
    * [Sorting Formats](#sorting-formats)
    * [Format Selection examples](#format-selection-examples)
* [MODIFYING METADATA](#modifying-metadata)
    * [Modifying metadata examples](#modifying-metadata-examples)
* [EXTRACTOR ARGUMENTS](#extractor-arguments)
* [PLUGINS](#plugins)
    * [Installing Plugins](#installing-plugins)
    * [Developing Plugins](#developing-plugins)
* [EMBEDDING YT-DLP](#embedding-yt-dlp)
    * [Embedding examples](#embedding-examples)
* [CHANGES FROM YOUTUBE-DL](#changes-from-youtube-dl)
    * [New features](#new-features)
    * [Differences in default behavior](#differences-in-default-behavior)
    * [Deprecated options](#deprecated-options)
* [CONTRIBUTING](CONTRIBUTING.md#contributing-to-yt-dlp)
    * [Opening an Issue](CONTRIBUTING.md#opening-an-issue)
    * [Developer Instructions](CONTRIBUTING.md#developer-instructions)
* [WIKI](https://github.com/yt-dlp/yt-dlp/wiki)
    * [FAQ](https://github.com/yt-dlp/yt-dlp/wiki/FAQ)
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;


# INSTALLATION

&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
[![Windows](https://img.shields.io/badge/-Windows_x64-blue.svg?style=for-the-badge&amp;logo=windows)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe)
[![Unix](https://img.shields.io/badge/-Linux/BSD-red.svg?style=for-the-badge&amp;logo=linux)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp)
[![MacOS](https://img.shields.io/badge/-MacOS-lightblue.svg?style=for-the-badge&amp;logo=apple)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos)
[![PyPI](https://img.shields.io/badge/-PyPI-blue.svg?logo=pypi&amp;labelColor=555555&amp;style=for-the-badge)](https://pypi.org/project/yt-dlp)
[![Source Tarball](https://img.shields.io/badge/-Source_tar-green.svg?style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)
[![Other variants](https://img.shields.io/badge/-Other-grey.svg?style=for-the-badge)](#release-files)
[![All versions](https://img.shields.io/badge/-All_Versions-lightgrey.svg?style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/releases)
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

You can install yt-dlp using [the binaries](#release-files), [pip](https://pypi.org/project/yt-dlp) or one using a third-party package manager. See [the wiki](https://github.com/yt-dlp/yt-dlp/wiki/Installation) for detailed instructions


&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
## RELEASE FILES

#### Recommended

File|Description
:---|:---
[yt-dlp](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp)|Platform-independent [zipimport](https://docs.python.org/3/library/zipimport.html) binary. Needs Python (recommended for **Linux/BSD**)
[yt-dlp.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe)|Windows (Win8+) standalone x64 binary (recommended for **Windows**)
[yt-dlp_macos](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos)|Universal MacOS (10.15+) standalone executable (recommended for **MacOS**)

#### Alternatives

File|Description
:---|:---
[yt-dlp_x86.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_x86.exe)|Windows (Win8+) standalone x86 (32-bit) binary
[yt-dlp_linux](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux)|Linux standalone x64 binary
[yt-dlp_linux_armv7l](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_armv7l)|Linux standalone armv7l (32-bit) binary
[yt-dlp_linux_aarch64](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_aarch64)|Linux standalone aarch64 (64-bit) binary
[yt-dlp_win.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win.zip)|Unpackaged Windows executable (no auto-update)
[yt-dlp_macos.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos.zip)|Unpackaged MacOS (10.15+) executable (no auto-update)
[yt-dlp_macos_legacy](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos_legacy)|MacOS (10.9+) standalone x64 executable

#### Misc

File|Description
:---|:---
[yt-dlp.tar.gz](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)|Source tarball
[SHA2-512SUMS](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS)|GNU-style SHA512 sums
[SHA2-512SUMS.sig](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS.sig)|GPG signature file for SHA512 sums
[SHA2-256SUMS](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS)|GNU-style SHA256 sums
[SHA2-256SUMS.sig](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS.sig)|GPG signature file for SHA256 sums

The public key that can be used to verify the GPG signatures is [available here](https://github.com/yt-dlp/yt-dlp/blob/master/public.key)
Example usage:
```
curl -L https://github.com/yt-dlp/yt-dlp/raw/master/public.key | gpg --import
gpg --verify SHA2-256SUMS.sig SHA2-256SUMS
gpg --verify SHA2-512SUMS.sig SHA2-512SUMS
```
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

**Note**: The manpages, shell completion (autocomplete) files etc. are available inside the [source tarball](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)


## UPDATE
You can use `yt-dlp -U` to update if you are using the [release binaries](#release-files)

If you [installed with pip](https://github.com/yt-dlp/yt-dlp/wiki/Installation#with-pip), simply re-run the same command that was used to install the program

For other third-party package managers, see [the wiki](https://github.com/yt-dlp/yt-dlp/wiki/Installation#third-party-package-managers) or refer to their documentation

&lt;a id=&quot;update-channels&quot;&gt;&lt;/a&gt;

There are currently three release channels for binaries: `stable`, `nightly` and `master`.

* `stable` is the default channel, and many of its changes have been tested by users of the `nightly` and `master` channels.
* The `nightly` channel has releases scheduled to build every day around midnight UTC, for a snapshot of the project&#039;s new patches and changes. This is the **recommended channel for regular users** of yt-dlp. The `nightly` releases are available from [yt-dlp/yt-dlp-nightly-builds](https://github.com/yt-dlp/yt-dlp-nightly-builds/releases) or as development releases of the `yt-dlp` PyPI package (which can be installed with pip&#039;s `--pre` flag).
* The `master` channel features releases that are built after each push to the master branch, and these will have the very latest fixes and additions, but may also be more prone to regressions. They are available from [yt-dlp/yt-dlp-master-builds](https://github.com/yt-dlp/yt-dlp-master-builds/releases).

When using `--update`/`-U`, a release binary will only update to its current channel.
`--update-to CHANNEL` can be used to switch to a different channel when a newer version is available. `--update-to [CHANNEL@]TAG` can also be used to upgrade or downgrade to specific tags from a channel.

You may also use `--update-to &lt;repository&gt;` (`&lt;owner&gt;/&lt;repository&gt;`) to update to a channel on a completely different repository. Be careful with what repository you are updating to though, there is no verification done for binaries from different repositories.

Example usage:

* `yt-dlp --update-to master` switch to the `master` channel and update to its latest release
* `yt-dlp --update-to stable@2023.07.06` upgrade/downgrade to release to `stable` channel tag `2023.07.06`
* `yt-dlp --update-to 2023.10.07` upgrade/downgrade to tag `2023.10.07` if it exists on the current channel
* `yt-dlp --update-to example/yt-dlp@2023.09.24` upgrade/downgrade to the release from the `example/yt-dlp` repository, tag `2023.09.24`

**Important**: Any user experiencing an issue with the `stable` release should install or update to the `nightly` release before submitting a bug report:
```
# To update to nightly from stable executable/binary:
yt-dlp --update-to nightly

# To install nightly with pip:
python3 -m pip install -U --pre &quot;yt-dlp[default]&quot;
```

## DEPENDENCIES
Python versions 3.9+ (CPython) and 3.10+ (PyPy) are supported. Other versions and implementations may or may not work correctly.

&lt;!-- Python 3.5+ uses VC++14 and it is already embedded in the binary created
&lt;!x-- https://www.microsoft.com/en-us/download/details.aspx?id=26999 --x&gt;
On Windows, [Microsoft Visual C++ 2010 SP1 Redistributable Package (x86)](https://download.microsoft.com/download/1/6/5/165255E7-1014-4D0A-B094-B6A430A6BFFC/vcredist_x86.exe) is also necessary to run yt-dlp. You probably already have this, but if the executable throws an error due to missing `MSVCR100.dll` you need to install it manually.
--&gt;

While all the other dependencies are optional, `ffmpeg` and `ffprobe` are highly recommended

### Strongly recommended

* [**ffmpeg** and **ffprobe**](https://www.ffmpeg.org) - Required for [merging separate video and audio files](#format-selection), as well as for various [post-processing](#post-processing-options) tasks. License [depends on the build](https://www.ffmpeg.org/legal.html)

    There are bugs in ffmpeg that cause various issues when used alongside yt-dlp. Since ffmpeg is such an important dependency, we provide [custom builds](https://github.com/yt-dlp/FFmpeg-Builds#ffmpeg-static-auto-builds) with patches for some of these issues at [yt-dlp/FFmpeg-Builds](https://github.com/yt-dlp/FFmpeg-Builds). See [the readme](https://github.com/yt-dlp/FFmpeg-Builds#patches-applied) for details on the specific issues solved by these builds

    **Important**: What you need is ffmpeg *binary*, **NOT** [the Python package of the same name](https://pypi.org/project/ffmpeg)

### Networking
* [**certifi**](https://github.com/certifi/python-certifi)\* - Provides Mozilla&#039;s root certificate bundle. Licensed under [MPLv2](https://github.com/certifi/python-certifi/blob/master/LICENSE)
* [**brotli**](https://github.com/google/brotli)\* or [**brotlicffi**](https://github.com/python-hyper/brotlicffi) - [Brotli](https://en.wikipedia.org/wiki/Brotli) content encoding support. Both licensed under MIT &lt;sup&gt;[1](https://github.com/google/brotli/blob/master/LICENSE) [2](https://github.com/python-hyper/brotlicffi/blob/master/LICENSE) &lt;/sup&gt;
* [**websockets**](https://github.com/aaugustin/websockets)\* - For downloading over websocket. Licensed under [BSD-3-Clause](https://github.com/aaugustin/websockets/blob/main/LICENSE)
* [**requests**](https://github.com/psf/requests)\* - HTTP library. For HTTPS proxy and persistent connections support. Licensed under [Apache-2.0](https://github.com/psf/requests/blob/main/LICENSE)

#### Impersonation

The following provide support for impersonating browser requests. This may be required for some sites that employ TLS fingerprinting.

* [**curl_cffi**](https://github.com/lexiforest/curl_cffi) (recommended) - Python binding for [curl-impersonate](https://github.com/lexiforest/curl-impersonate). Provides impersonation targets for Chrome, Edge and Safari. Licensed under [MIT](https://github.com/lexiforest/curl_cffi/blob/main/LICENSE)
  * Can be installed with the `curl-cffi` group, e.g. `pip install &quot;yt-dlp[default,curl-cffi]&quot;`
  * Currently included in `yt-dlp.exe`, `yt-dlp_linux` and `yt-dlp_macos` builds


### Metadata

* [**mutagen**](https://github.com/quodlibet/mutagen)\* - For `--embed-thumbnail` in certain formats. Licensed under [GPLv2+](https://github.com/quodlibet/mutagen/blob/master/COPYING)
* [**AtomicParsley**](https://github.com/wez/atomicparsley) - For `--embed-thumbnail` in `mp4`/`m4a` files when `mutagen`/`ffmpeg` cannot. Licensed under [GPLv2+](https://github.com/wez/atomicparsley/blob/master/COPYING)
* [**xattr**](https://github.com/xattr/xattr), [**pyxattr**](https://github.com/iustin/pyxattr) or [**setfattr**](http://savannah.nongnu.org/projects/attr) - For writing xattr metadata (`--xattr`) on **Mac** and **BSD**. Licensed under [MIT](https://github.com/xattr/xattr/blob/master/LICENSE.txt), [LGPL2.1](https://github.com/iustin/pyxattr/blob/master/COPYING) and [GPLv2+](http://git.savannah.nongnu.org/cgit/attr.git/tree/doc/COPYING) respectively

### Misc

* [**pycryptodomex**](https://github.com/Legrandin/pycryptodome)\* - For decrypting AES-128 HLS streams and various other data. Licensed under [BSD-2-Clause](https://github.com/Legrandin/pycryptodome/blob/master/LICENSE.rst)
* [**phantomjs**](https://github.com/ariya/phantomjs) - Used in extractors where javascript needs to be run. Licensed under [BSD-3-Clause](https://github.com/ariya/phantomjs/blob/master/LICENSE.BSD)
* [**secretstorage**](https://github.com/mitya57/secretstorage)\* - For `--cookies-from-browser` to access the **Gnome** keyring while decrypting cookies of **Chromium**-based browsers on **Linux**. Licensed under [BSD-3-Clause](https://github.com/mitya57/secretstorage/blob/master/LICENSE)
* Any external downloader that you want to use with `--downloader`

### Deprecated

* [**avconv** and **avprobe**](https://www.libav.org) - Now **deprecated** alternative to ffmpeg. License [depends on the build](https://libav.org/legal)
* [**sponskrub**](https://github.com/faissaloo/SponSkrub) - For using the now **deprecated** [sponskrub options](#sponskrub-options). Licensed under [GPLv3+](https://github.com/faissaloo/SponSkrub/blob/master/LICENCE.md)
* [**rtmpdump**](http://rtmpdump.mplayerhq.hu) - For downloading `rtmp` streams. ffmpeg can be used instead with `--downloader ffmpeg`. Licensed under [GPLv2+](http://rtmpdump.mplayerhq.hu)
* [**mplayer**](http://mplayerhq.hu/design7/info.html) or [**mpv**](https://mpv.io) - For downloading `rstp`/`mms` streams. ffmpeg can be used instead with `--downloader ffmpeg`. Licensed under [GPLv2+](https://github.com/mpv-player/mpv/blob/master/Copyright)

To use or redistribute the dependencies, you must agree to their respective licensing terms.

The standalone release binaries are built with the Python interpreter and the packages marked with **\*** included.

If you do not have the necessary dependencies for a task you are attempting, yt-dlp will warn you. All the currently available dependencies are visible at the top of the `--verbose` output


## COMPILE

### Standalone PyInstaller Builds
To build the standalone executable, you must have Python and `pyinstaller` (plus any of yt-dlp&#039;s [optional dependencies](#dependencies) if needed). The executable will be built for the same CPU architecture as the Python used.

You can run the following commands:

```
python3 devscripts/install_deps.py --include pyinstaller
python3 devscripts/make_lazy_extractors.py
python3 -m bundle.pyinstaller
```

On some systems, you may need to use `py` or `python` instead of `python3`.

`python -m bundle.pyinstaller` accepts any arguments that can be passed to `pyinstaller`, such as `--onefile/-F` or `--onedir/-D`, which is further [documented here](https://pyinstaller.org/en/stable/usage.html#what-to-generate).

**Note**: Pyinstaller versions below 4.4 [do not support](https://github.com/pyinstaller/pyinstaller#requirements-and-tested-platforms) Python installed from the Windows store without using a virtual environment.

**Important**: Running `pyinstaller` directly **instead of** using `python -m bundle.pyinstaller` is **not** officially supported. This may or may not work correctly.

### Platform-independent Binary (UNIX)
You will need the build tools `python` (3.9+), `zip`, `make` (GNU), `pandoc`\* and `pytest`\*.

After installing these, simply run `make`.

You can also run `make yt-dlp` instead to compile only the binary without updating any of the additional files. (The build tools marked with **\*** are not needed for this)

### Related scripts

* **`devscripts/install_deps.py`** - Install dependencies for yt-dlp.
* **`devscripts/update-version.py`** - Update the version number based on the current date.
* **`devscripts/set-variant.py`** - Set the build variant of the executable.
* **`devscripts/make_changelog.py`** - Create a markdown changelog using short commit messages and update `CONTRIBUTORS` file.
* **`devscripts/make_lazy_extractors.py`** - Create lazy extractors. Running this before building the binaries (any variant) will improve their startup performance. Set the environment variable `YTDLP_NO_LAZY_EXTRACTORS` to something nonempty to forcefully disable lazy extractor loading.

Note: See their `--help` for more info.

### Forking the project
If you fork the project on GitHub, you can run your fork&#039;s [build workflow](.github/workflows/build.yml) to automatically build the selected version(s) as ar

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[aigc-apps/VideoX-Fun]]></title>
            <link>https://github.com/aigc-apps/VideoX-Fun</link>
            <guid>https://github.com/aigc-apps/VideoX-Fun</guid>
            <pubDate>Mon, 31 Mar 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[üìπ A more flexible framework that can generate videos at any resolution and creates videos from images.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/aigc-apps/VideoX-Fun">aigc-apps/VideoX-Fun</a></h1>
            <p>üìπ A more flexible framework that can generate videos at any resolution and creates videos from images.</p>
            <p>Language: Python</p>
            <p>Stars: 764</p>
            <p>Forks: 55</p>
            <p>Stars today: 23 stars today</p>
            <h2>README</h2><pre># VideoX-Fun

üòä Welcome!

CogVideoX-Fun:
[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-yellow)](https://huggingface.co/spaces/alibaba-pai/CogVideoX-Fun-5b)

Wan-Fun:
[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-yellow)](https://huggingface.co/spaces/alibaba-pai/Wan2.1-Fun-1.3B-InP)

English | [ÁÆÄ‰Ωì‰∏≠Êñá](./README_zh-CN.md) | [Êó•Êú¨Ë™û](./README_ja-JP.md)

# Table of Contents
- [Table of Contents](#table-of-contents)
- [Introduction](#introduction)
- [Quick Start](#quick-start)
- [Video Result](#video-result)
- [How to use](#how-to-use)
- [Model zoo](#model-zoo)
- [Reference](#reference)
- [License](#license)

# Introduction
VideoX-Fun is a video generation pipeline that can be used to generate AI images and videos, as well as to train baseline and Lora models for Diffusion Transformer. We support direct prediction from pre-trained baseline models to generate videos with different resolutions, durations, and FPS. Additionally, we also support users in training their own baseline and Lora models to perform specific style transformations.

We will support quick pull-ups from different platforms, refer to [Quick Start](#quick-start).

What&#039;s New:
- Update Wan2.1-Fun-V1.0: Support I2V and Control models for 14B and 1.3B models, with support for start and end frame prediction. [2025.03.26]
- Update CogVideoX-Fun-V1.5: Upload I2V model and related training/prediction code. [2024.12.16]
- Reward Lora Support: Train Lora using reward backpropagation techniques to optimize generated videos, making them better aligned with human preferences. [More Information](scripts/README_TRAIN_REWARD.md). New version of the control model supports various control conditions such as Canny, Depth, Pose, MLSD, etc. [2024.11.21]
- Diffusers Support: CogVideoX-Fun Control is now supported in diffusers. Thanks to [a-r-r-o-w](https://github.com/a-r-r-o-w) for contributing support in this [PR](https://github.com/huggingface/diffusers/pull/9671). Check out the [documentation](https://huggingface.co/docs/diffusers/main/en/api/pipelines/cogvideox) for more details. [2024.10.16]
- Update CogVideoX-Fun-V1.1: Retrain i2v model, add Noise to increase the motion amplitude of the video. Upload control model training code and Control model. [2024.09.29]
- Update CogVideoX-Fun-V1.0: Initial code release! Now supports Windows and Linux. Supports video generation at arbitrary resolutions from 256x256x49 to 1024x1024x49 for 2B and 5B models. [2024.09.18]

FunctionÔºö
- [Data Preprocessing](#data-preprocess)
- [Train DiT](#dit-train)
- [Video Generation](#video-gen)

Our UI interface is as follows:
![ui](https://pai-aigc-photog.oss-cn-hangzhou.aliyuncs.com/cogvideox_fun/asset/v1/ui.jpg)

# Quick Start
### 1. Cloud usage: AliyunDSW/Docker
#### a. From AliyunDSW
DSW has free GPU time, which can be applied once by a user and is valid for 3 months after applying.

Aliyun provide free GPU time in [Freetier](https://free.aliyun.com/?product=9602825&amp;crowd=enterprise&amp;spm=5176.28055625.J_5831864660.1.e939154aRgha4e&amp;scm=20140722.M_9974135.P_110.MO_1806-ID_9974135-MID_9974135-CID_30683-ST_8512-V_1), get it and use in Aliyun PAI-DSW to start CogVideoX-Fun within 5min!

[![DSW Notebook](https://pai-aigc-photog.oss-cn-hangzhou.aliyuncs.com/easyanimate/asset/dsw.png)](https://gallery.pai-ml.com/#/preview/deepLearning/cv/cogvideox_fun)

#### b. From ComfyUI
Our ComfyUI is as follows, please refer to [ComfyUI README](comfyui/README.md) for details.
![workflow graph](https://pai-aigc-photog.oss-cn-hangzhou.aliyuncs.com/cogvideox_fun/asset/v1/cogvideoxfunv1_workflow_i2v.jpg)

#### c. From docker
If you are using docker, please make sure that the graphics card driver and CUDA environment have been installed correctly in your machine.

Then execute the following commands in this way:

```
# pull image
docker pull mybigpai-public-registry.cn-beijing.cr.aliyuncs.com/easycv/torch_cuda:cogvideox_fun

# enter image
docker run -it -p 7860:7860 --network host --gpus all --security-opt seccomp:unconfined --shm-size 200g mybigpai-public-registry.cn-beijing.cr.aliyuncs.com/easycv/torch_cuda:cogvideox_fun

# clone code
git clone https://github.com/aigc-apps/CogVideoX-Fun.git

# enter CogVideoX-Fun&#039;s dir
cd CogVideoX-Fun

# download weights
mkdir models/Diffusion_Transformer
mkdir models/Personalized_Model

# Please use the hugginface link or modelscope link to download the model.
# CogVideoX-Fun
# https://huggingface.co/alibaba-pai/CogVideoX-Fun-V1.1-5b-InP
# https://modelscope.cn/models/PAI/CogVideoX-Fun-V1.1-5b-InP

# Wan
# https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-InP
# https://modelscope.cn/models/PAI/Wan2.1-Fun-14B-InP
```

### 2. Local install: Environment Check/Downloading/Installation
#### a. Environment Check
We have verified this repo execution on the following environment:

The detailed of Windows:
- OS: Windows 10
- python: python3.10 &amp; python3.11
- pytorch: torch2.2.0
- CUDA: 11.8 &amp; 12.1
- CUDNN: 8+
- GPUÔºö Nvidia-3060 12G &amp; Nvidia-3090 24G

The detailed of Linux:
- OS: Ubuntu 20.04, CentOS
- python: python3.10 &amp; python3.11
- pytorch: torch2.2.0
- CUDA: 11.8 &amp; 12.1
- CUDNN: 8+
- GPUÔºöNvidia-V100 16G &amp; Nvidia-A10 24G &amp; Nvidia-A100 40G &amp; Nvidia-A100 80G

We need about 60GB available on disk (for saving weights), please check!

#### b. Weights
We&#039;d better place the [weights](#model-zoo) along the specified path:

```
üì¶ models/
‚îú‚îÄ‚îÄ üìÇ Diffusion_Transformer/
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ CogVideoX-Fun-V1.1-2b-InP/
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ CogVideoX-Fun-V1.1-5b-InP/
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ Wan2.1-Fun-14B-InP
‚îÇ   ‚îî‚îÄ‚îÄ üìÇ Wan2.1-Fun-1.3B-InP/
‚îú‚îÄ‚îÄ üìÇ Personalized_Model/
‚îÇ   ‚îî‚îÄ‚îÄ your trained trainformer model / your trained lora model (for UI load)
```

# Video Result

### Wan2.1-Fun-14B-InP &amp;&amp; Wan2.1-Fun-1.3B-InP

&lt;table border=&quot;0&quot; style=&quot;width: 100%; text-align: left; margin-top: 20px;&quot;&gt;
  &lt;tr&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/bd72a276-e60e-4b5d-86c1-d0f67e7425b9&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
       &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/cb7aef09-52c2-4973-80b4-b2fb63425044&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
     &lt;/td&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/4e10d491-f1cf-4b08-a7c5-1e01e5418140&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/f7e363a9-be09-4b72-bccf-cce9c9ebeb9b&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
     &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table border=&quot;0&quot; style=&quot;width: 100%; text-align: left; margin-top: 20px;&quot;&gt;
  &lt;tr&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/28f3e720-8acc-4f22-a5d0-ec1c571e9466&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/fb6e4cb9-270d-47cd-8501-caf8f3e91b5c&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
       &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/989a4644-e33b-4f0c-b68e-2ff6ba37ac7e&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
     &lt;/td&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/9c604fa7-8657-49d1-8066-b5bb198b28b6&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
     &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

### Wan2.1-Fun-14B-Control &amp;&amp; Wan2.1-Fun-1.3B-Control

&lt;table border=&quot;0&quot; style=&quot;width: 100%; text-align: left; margin-top: 20px;&quot;&gt;
  &lt;tr&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/f35602c4-9f0a-4105-9762-1e3a88abbac6&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/8b0f0e87-f1be-4915-bb35-2d53c852333e&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
       &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/972012c1-772b-427a-bce6-ba8b39edcfad&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
     &lt;/td&gt;
  &lt;tr&gt;
&lt;/table&gt;

&lt;table border=&quot;0&quot; style=&quot;width: 100%; text-align: left; margin-top: 20px;&quot;&gt;
  &lt;tr&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/53002ce2-dd18-4d4f-8135-b6f68364cabd&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/fce43c0b-81fa-4ab2-9ca7-78d786f520e6&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
       &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/b208b92c-5add-4ece-a200-3dbbe47b93c3&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
     &lt;/td&gt;
  &lt;tr&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/c6c5d557-9772-483e-ae47-863d8a26db4a&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/af617971-597c-4be4-beb5-f9e8aaca2d14&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
       &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/8411151e-f491-4264-8368-7fc3c5a6992b&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
     &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

### CogVideoX-Fun-V1.1-5B

Resolution-1024

&lt;table border=&quot;0&quot; style=&quot;width: 100%; text-align: left; margin-top: 20px;&quot;&gt;
  &lt;tr&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/34e7ec8f-293e-4655-bb14-5e1ee476f788&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/7809c64f-eb8c-48a9-8bdc-ca9261fd5434&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
       &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/8e76aaa4-c602-44ac-bcb4-8b24b72c386c&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
     &lt;/td&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/19dba894-7c35-4f25-b15c-384167ab3b03&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
     &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;


Resolution-768

&lt;table border=&quot;0&quot; style=&quot;width: 100%; text-align: left; margin-top: 20px;&quot;&gt;
  &lt;tr&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/0bc339b9-455b-44fd-8917-80272d702737&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/70a043b9-6721-4bd9-be47-78b7ec5c27e9&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
       &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/d5dd6c09-14f3-40f8-8b6d-91e26519b8ac&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
     &lt;/td&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/9327e8bc-4f17-46b0-b50d-38c250a9483a&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
     &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

Resolution-512

&lt;table border=&quot;0&quot; style=&quot;width: 100%; text-align: left; margin-top: 20px;&quot;&gt;
  &lt;tr&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/ef407030-8062-454d-aba3-131c21e6b58c&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/7610f49e-38b6-4214-aa48-723ae4d1b07e&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
       &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/1fff0567-1e15-415c-941e-53ee8ae2c841&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
     &lt;/td&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/bcec48da-b91b-43a0-9d50-cf026e00fa4f&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
     &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

### CogVideoX-Fun-V1.1-5B-Control

&lt;table border=&quot;0&quot; style=&quot;width: 100%; text-align: left; margin-top: 20px;&quot;&gt;
  &lt;tr&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/53002ce2-dd18-4d4f-8135-b6f68364cabd&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/fce43c0b-81fa-4ab2-9ca7-78d786f520e6&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
       &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/b208b92c-5add-4ece-a200-3dbbe47b93c3&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
     &lt;/td&gt;
  &lt;tr&gt;
      &lt;td&gt;
          A young woman with beautiful clear eyes and blonde hair, wearing white clothes and twisting her body, with the camera focused on her face. High quality, masterpiece, best quality, high resolution, ultra-fine, dreamlike.
      &lt;/td&gt;
      &lt;td&gt;
          A young woman with beautiful clear eyes and blonde hair, wearing white clothes and twisting her body, with the camera focused on her face. High quality, masterpiece, best quality, high resolution, ultra-fine, dreamlike.
      &lt;/td&gt;
       &lt;td&gt;
          A young bear.
     &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/ea908454-684b-4d60-b562-3db229a250a9&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/ffb7c6fc-8b69-453b-8aad-70dfae3899b9&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
       &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/d3f757a3-3551-4dcb-9372-7a61469813f5&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
     &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

# How to Use

&lt;h3 id=&quot;video-gen&quot;&gt;1. Generation&lt;/h3&gt;

#### a. GPU Memory Optimization
Since Wan2.1 has a very large number of parameters, we need to consider memory optimization strategies to adapt to consumer-grade GPUs. We provide `GPU_memory_mode` for each prediction file, allowing you to choose between `model_cpu_offload`, `model_cpu_offload_and_qfloat8`, and `sequential_cpu_offload`. This solution is also applicable to CogVideoX-Fun generation.

- `model_cpu_offload`: The entire model is moved to the CPU after use, saving some GPU memory.
- `model_cpu_offload_and_qfloat8`: The entire model is moved to the CPU after use, and the transformer model is quantized to float8, saving more GPU memory.
- `sequential_cpu_offload`: Each layer of the model is moved to the CPU after use. It is slower but saves a significant amount of GPU memory.

`qfloat8` may slightly reduce model performance but saves more GPU memory. If you have sufficient GPU memory, it is recommended to use `model_cpu_offload`.

#### b. Using ComfyUI
For details, refer to [ComfyUI README](comfyui/README.md).

#### c. Running Python Files
- **Step 1**: Download the corresponding [weights](#model-zoo) and place them in the `models` folder.
- **Step 2**: Use different files for prediction based on the weights and prediction goals. This library currently supports CogVideoX-Fun, Wan2.1, and Wan2.1-Fun. Different models are distinguished by folder names under the `examples` folder, and their supported features vary. Use them accordingly. Below is an example using CogVideoX-Fun:
  - **Text-to-Video**:
    - Modify `prompt`, `neg_prompt`, `guidance_scale`, and `seed` in the file `examples/cogvideox_fun/predict_t2v.py`.
    - Run the file `examples/cogvideox_fun/predict_t2v.py` and wait for the results. The generated videos will be saved in the folder `samples/cogvideox-fun-videos`.
  - **Image-to-Video**:
    - Modify `validation_image_start`, `validation_image_end`, `prompt`, `neg_prompt`, `guidance_scale`, and `seed` in the file `examples/cogvideox_fun/predict_i2v.py`.
    - `validation_image_start` is the starting image of the video, and `validation_image_end` is the ending image of the video.
    - Run the file `examples/cogvideox_fun/predict_i2v.py` and wait for the results. The generated videos will be saved in the folder `samples/cogvideox-fun-videos_i2v`.
  - **Video-to-Video**:
    - Modify `validation_video`, `validation_image_end`, `prompt`, `neg_prompt`, `guidance_scale`, and `seed` in the file `examples/cogvideox_fun/predict_v2v.py`.
    - `validation_video` is the reference video for video-to-video generation. You can use the following demo video: [Demo Video](https://pai-aigc-photog.oss-cn-hangzhou.aliyuncs.com/cogvideox_fun/asset/v1/play_guitar.mp4).
    - Run the file `examples/cogvideox_fun/predict_v2v.py` and wait for the results. The generated videos will be saved in the folder `samples/cogvideox-fun-videos_v2v`.
  - **Controlled Video Generation (Canny, Pose, Depth, etc.)**:
    - Modify `control_video`, `validation_image_end`, `prompt`, `neg_prompt`, `guidance_scale`, and `seed` in the file `examples/cogvideox_fun/predict_v2v_control.py`.
    - `control_video` is the control video extracted using operators such as Canny, Pose, or Depth. You can use the following demo video: [Demo Video](https://pai-aigc-photog.oss-cn-hangzhou.aliyuncs.com/cogvideox_fun/asset/v1.1/pose.mp4).
    - Run the file `examples/cogvideox_fun/predict_v2v_control.py` and wait for the results. The generated videos will be saved in the folder `samples/cogvideox-fun-videos_v2v_control`.
- **Step 3**: If you want to integrate other backbones or Loras trained by yourself, modify `lora_path` and relevant paths in `examples/{model_name}/predict_t2v.py` or `examples/{model_name}/predict_i2v.py` as needed.

#### d. Using the Web UI
The web UI supports text-to-video, image-to-video, video-to-video, and controlled video generation (Canny, Pose, Depth, etc.). This library currently supports CogVideoX-Fun, Wan2.1, and Wan2.1-Fun. Different models are distinguished by folder names under the `examples` folder, and their supported features vary. Use them accordingly. Below is an example using CogVideoX-Fun:

- **Step 1**: Download the corresponding [weights](#model-zoo) and place them in the `models` folder.
- **Step 2**: Run the file `examples/cogvideox_fun/app.py` to access the Gradio interface.
- **Step 3**: Select the generation model on the page, fill in `prompt`, `neg_prompt`, `guidance_scale`, and `seed`, click &quot;Generate,&quot; and wait for the results. The generated videos will be saved in the `sample` folder.

### 2. Model Training
A complete model training pipeline should include data preprocessing and Video DiT training. The training process for different models is similar, and the data formats are also similar:

&lt;h4 id=&quot;data-preprocess&quot;&gt;a. data preprocessing&lt;/h4&gt;

We have provided a simple demo of training the Lora model through image data, which can be found in the [wiki](https://github.com/aigc-apps/CogVideoX-Fun/wiki/Training-Lora) for details.

A complete data preprocessing link for long video segmentation, cleaning, and description can refer to [README](cogvideox/video_caption/README.md) in the video captions section. 

If you want to train a text to image and video generation model. You need to arrange the dataset in this format.

```
üì¶ project/
‚îú‚îÄ‚îÄ üìÇ datasets/
‚îÇ   ‚îú‚îÄ‚îÄ üìÇ internal_datasets/
‚îÇ       ‚îú‚îÄ‚îÄ üìÇ train/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 00000001.mp4
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 00000002.jpg
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ .....
‚îÇ       ‚îî‚îÄ‚îÄ üìÑ json_of_internal_datasets.json
```

The json_of_internal_datasets.json is a standard JSON file. The file_path in the json can to be set as relative path, as shown in below:
```json
[
    {
      &quot;file_path&quot;: &quot;train/00000001.mp4&quot;,
      &quot;text&quot;: &quot;A group of young men in suits and sunglasses are walking down a city street.&quot;,
      &quot;type&quot;: &quot;video&quot;
    },
    {
      &quot;file_path&quot;: &quot;train/00000002.jpg&quot;,
      &quot;text&quot;: &quot;A group of young men in suits and sunglasses are walking down a city street.&quot;,
      &quot;type&quot;: &quot;image&quot;
    },
    .....
]
```

You can also set the path as absolute path as follow:
```json
[
    {
      &quot;file_path&quot;: &quot;/mnt/data/videos/00000001.mp4&quot;,
      &quot;text&quot;: &quot;A group of young men in suits and sunglasses are walking down a city street.&quot;,
      &quot;type&quot;: &quot;video&quot;
    },
    {
      &quot;file_path&quot;: &quot;/mnt/data/train/00000001.jpg&quot;,
      &quot;text&quot;: &quot;A group of young men in suits and sunglasses are walking down a city street.&quot;,
      &quot;type&quot;: &quot;image&quot;
    },
    .....
]
```

&lt;h4 id=&quot;dit-train&quot;&gt;b. Video DiT training &lt;/h4&gt;
 
If the data format is relative path during data preprocessing, please set ```scri

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sympy/sympy]]></title>
            <link>https://github.com/sympy/sympy</link>
            <guid>https://github.com/sympy/sympy</guid>
            <pubDate>Mon, 31 Mar 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[A computer algebra system written in pure Python]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sympy/sympy">sympy/sympy</a></h1>
            <p>A computer algebra system written in pure Python</p>
            <p>Language: Python</p>
            <p>Stars: 13,481</p>
            <p>Forks: 4,644</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre># SymPy

[![pypi version](https://img.shields.io/pypi/v/sympy.svg)](https://pypi.python.org/pypi/sympy)
[![Join the chat at https://gitter.im/sympy/sympy](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/sympy/sympy?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge)
[![Zenodo Badge](https://zenodo.org/badge/18918/sympy/sympy.svg)](https://zenodo.org/badge/latestdoi/18918/sympy/sympy)
[![Downloads](https://pepy.tech/badge/sympy/month)](https://pepy.tech/project/sympy)
[![GitHub Issues](https://img.shields.io/badge/issue_tracking-github-blue.svg)](https://github.com/sympy/sympy/issues)
[![Git Tutorial](https://img.shields.io/badge/PR-Welcome-%23FF8300.svg?)](https://git-scm.com/book/en/v2/GitHub-Contributing-to-a-Project)
[![Powered by NumFocus](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&amp;colorA=E1523D&amp;colorB=007D8A)](https://numfocus.org)
[![Commits since last release](https://img.shields.io/github/commits-since/sympy/sympy/latest.svg?longCache=true&amp;style=flat-square&amp;logo=git&amp;logoColor=fff)](https://github.com/sympy/sympy/releases)

[![SymPy Banner](https://github.com/sympy/sympy/raw/master/banner.svg)](https://sympy.org/)


See the [AUTHORS](AUTHORS) file for the list of authors.

And many more people helped on the SymPy mailing list, reported bugs,
helped organize SymPy&#039;s participation in the Google Summer of Code, the
Google Highly Open Participation Contest, Google Code-In, wrote and
blogged about SymPy...

License: New BSD License (see the [LICENSE](LICENSE) file for details) covers all
files in the sympy repository unless stated otherwise.

Our mailing list is at
&lt;https://groups.google.com/forum/?fromgroups#!forum/sympy&gt;.

We have a community chat at [Gitter](https://gitter.im/sympy/sympy). Feel
free to ask us anything there. We have a very welcoming and helpful
community.

## Download

The recommended installation method is through Anaconda,
&lt;https://www.anaconda.com/products/distribution&gt;

You can also get the latest version of SymPy from
&lt;https://pypi.python.org/pypi/sympy/&gt;

To get the git version do

    $ git clone https://github.com/sympy/sympy.git

For other options (tarballs, debs, etc.), see
&lt;https://docs.sympy.org/dev/install.html&gt;.

## Documentation and Usage

For in-depth instructions on installation and building the
documentation, see the [SymPy Documentation Style Guide](https://docs.sympy.org/dev/documentation-style-guide.html).

Everything is at:

&lt;https://docs.sympy.org/&gt;

You can generate everything at the above site in your local copy of
SymPy by:

    $ cd doc
    $ make html

Then the docs will be in &lt;span class=&quot;title-ref&quot;&gt;\_build/html&lt;/span&gt;. If
you don&#039;t want to read that, here is a short usage:

From this directory, start Python and:

``` python
&gt;&gt;&gt; from sympy import Symbol, cos
&gt;&gt;&gt; x = Symbol(&#039;x&#039;)
&gt;&gt;&gt; e = 1/cos(x)
&gt;&gt;&gt; print(e.series(x, 0, 10))
1 + x**2/2 + 5*x**4/24 + 61*x**6/720 + 277*x**8/8064 + O(x**10)
```

SymPy also comes with a console that is a simple wrapper around the
classic python console (or IPython when available) that loads the SymPy
namespace and executes some common commands for you.

To start it, issue:

    $ bin/isympy

from this directory, if SymPy is not installed or simply:

    $ isympy

if SymPy is installed.

## Installation

To install SymPy using PyPI, run the following command:

    $ pip install sympy

To install SymPy using Anaconda, run the following command:

    $ conda install -c anaconda sympy

To install SymPy from GitHub source, first clone SymPy using `git`:

    $ git clone https://github.com/sympy/sympy.git

Then, in the `sympy` repository that you cloned, simply run:

    $ pip install .

See &lt;https://docs.sympy.org/dev/install.html&gt; for more information.

## Contributing

We welcome contributions from anyone, even if you are new to open
source. Please read our [Introduction to Contributing](https://docs.sympy.org/dev/contributing/introduction-to-contributing.html)
page and the [SymPy Documentation Style Guide](https://docs.sympy.org/dev/documentation-style-guide.html). If you
are new and looking for some way to contribute, a good place to start is
to look at the issues tagged [Easy to Fix](https://github.com/sympy/sympy/issues?q=is%3Aopen+is%3Aissue+label%3A%22Easy+to+Fix%22).

Please note that all participants in this project are expected to follow
our Code of Conduct. By participating in this project you agree to abide
by its terms. See [CODE\_OF\_CONDUCT.md](CODE_OF_CONDUCT.md).

## Tests

To execute all tests, run:

    $./setup.py test

in the current directory.

For the more fine-grained running of tests or doctests, use `bin/test`
or respectively `bin/doctest`. The master branch is automatically tested
by GitHub Actions.

To test pull requests, use
[sympy-bot](https://github.com/sympy/sympy-bot).

## Regenerate Experimental &lt;span class=&quot;title-ref&quot;&gt;LaTeX&lt;/span&gt; Parser/Lexer

The parser and lexer were generated with the [ANTLR4](http://antlr4.org)
toolchain in `sympy/parsing/latex/_antlr` and checked into the repo.
Presently, most users should not need to regenerate these files, but
if you plan to work on this feature, you will need the `antlr4`
command-line tool (and you must ensure that it is in your `PATH`).
One way to get it is:

    $ conda install -c conda-forge antlr=4.11.1

Alternatively, follow the instructions on the ANTLR website and download
the `antlr-4.11.1-complete.jar`. Then export the `CLASSPATH` as instructed
and instead of creating `antlr4` as an alias, make it an executable file
with the following contents:
``` bash
#!/bin/bash
java -jar /usr/local/lib/antlr-4.11.1-complete.jar &quot;$@&quot;
```

After making changes to `sympy/parsing/latex/LaTeX.g4`, run:

    $ ./setup.py antlr

## Clean

To clean everything (thus getting the same tree as in the repository):

    $ git clean -Xdf

which will clear everything ignored by `.gitignore`, and:

    $ git clean -df

to clear all untracked files. You can revert the most recent changes in
git with:

    $ git reset --hard

WARNING: The above commands will all clear changes you may have made,
and you will lose them forever. Be sure to check things with `git
status`, `git diff`, `git clean -Xn`, and `git clean -n` before doing any
of those.

## Bugs

Our issue tracker is at &lt;https://github.com/sympy/sympy/issues&gt;. Please
report any bugs that you find. Or, even better, fork the repository on
GitHub and create a pull request. We welcome all changes, big or small,
and we will help you make the pull request if you are new to git (just
ask on our mailing list or Gitter Channel). If you further have any queries, you can find answers
on Stack Overflow using the [sympy](https://stackoverflow.com/questions/tagged/sympy) tag.

## Brief History

SymPy was started by Ond≈ôej ƒåert√≠k in 2005, he wrote some code during
the summer, then he wrote some more code during summer 2006. In February
2007, Fabian Pedregosa joined the project and helped fix many things,
contributed documentation, and made it alive again. 5 students (Mateusz
Paprocki, Brian Jorgensen, Jason Gedge, Robert Schwarz, and Chris Wu)
improved SymPy incredibly during summer 2007 as part of the Google
Summer of Code. Pearu Peterson joined the development during the summer
2007 and he has made SymPy much more competitive by rewriting the core
from scratch, which has made it from 10x to 100x faster. Jurjen N.E. Bos
has contributed pretty-printing and other patches. Fredrik Johansson has
written mpmath and contributed a lot of patches.

SymPy has participated in every Google Summer of Code since 2007. You
can see &lt;https://github.com/sympy/sympy/wiki#google-summer-of-code&gt; for
full details. Each year has improved SymPy by bounds. Most of SymPy&#039;s
development has come from Google Summer of Code students.

In 2011, Ond≈ôej ƒåert√≠k stepped down as lead developer, with Aaron
Meurer, who also started as a Google Summer of Code student, taking his
place. Ond≈ôej ƒåert√≠k is still active in the community but is too busy
with work and family to play a lead development role.

Since then, a lot more people have joined the development and some
people have also left. You can see the full list in doc/src/aboutus.rst,
or online at:

&lt;https://docs.sympy.org/dev/aboutus.html#sympy-development-team&gt;

The git history goes back to 2007 when development moved from svn to hg.
To see the history before that point, look at
&lt;https://github.com/sympy/sympy-old&gt;.

You can use git to see the biggest developers. The command:

    $ git shortlog -ns

will show each developer, sorted by commits to the project. The command:

    $ git shortlog -ns --since=&quot;1 year&quot;

will show the top developers from the last year.

## Citation

To cite SymPy in publications use

&gt; Meurer A, Smith CP, Paprocki M, ƒåert√≠k O, Kirpichev SB, Rocklin M,
&gt; Kumar A, Ivanov S, Moore JK, Singh S, Rathnayake T, Vig S, Granger BE,
&gt; Muller RP, Bonazzi F, Gupta H, Vats S, Johansson F, Pedregosa F, Curry
&gt; MJ, Terrel AR, Rouƒçka ≈†, Saboo A, Fernando I, Kulal S, Cimrman R,
&gt; Scopatz A. (2017) SymPy: symbolic computing in Python. *PeerJ Computer
&gt; Science* 3:e103 &lt;https://doi.org/10.7717/peerj-cs.103&gt;

A BibTeX entry for LaTeX users is

``` bibtex
@article{10.7717/peerj-cs.103,
 title = {SymPy: symbolic computing in Python},
 author = {Meurer, Aaron and Smith, Christopher P. and Paprocki, Mateusz and \v{C}ert\&#039;{i}k, Ond\v{r}ej and Kirpichev, Sergey B. and Rocklin, Matthew and Kumar, Amit and Ivanov, Sergiu and Moore, Jason K. and Singh, Sartaj and Rathnayake, Thilina and Vig, Sean and Granger, Brian E. and Muller, Richard P. and Bonazzi, Francesco and Gupta, Harsh and Vats, Shivam and Johansson, Fredrik and Pedregosa, Fabian and Curry, Matthew J. and Terrel, Andy R. and Rou\v{c}ka, \v{S}t\v{e}p\&#039;{a}n and Saboo, Ashutosh and Fernando, Isuru and Kulal, Sumith and Cimrman, Robert and Scopatz, Anthony},
 year = 2017,
 month = Jan,
 keywords = {Python, Computer algebra system, Symbolics},
 abstract = {
            SymPy is an open-source computer algebra system written in pure Python. It is built with a focus on extensibility and ease of use, through both interactive and programmatic applications. These characteristics have led SymPy to become a popular symbolic library for the scientific Python ecosystem. This paper presents the architecture of SymPy, a description of its features, and a discussion of select submodules. The supplementary material provides additional examples and further outlines details of the architecture and features of SymPy.
         },
 volume = 3,
 pages = {e103},
 journal = {PeerJ Computer Science},
 issn = {2376-5992},
 url = {https://doi.org/10.7717/peerj-cs.103},
 doi = {10.7717/peerj-cs.103}
}
```

SymPy is BSD licensed, so you are free to use it whatever you like, be
it academic, commercial, creating forks or derivatives, as long as you
copy the BSD statement if you redistribute it (see the LICENSE file for
details). That said, although not required by the SymPy license, if it
is convenient for you, please cite SymPy when using it in your work and
also consider contributing all your changes back, so that we can
incorporate it and all of us will benefit in the end.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>