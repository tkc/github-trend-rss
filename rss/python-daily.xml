<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Tue, 23 Dec 2025 00:04:41 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[exo-explore/exo]]></title>
            <link>https://github.com/exo-explore/exo</link>
            <guid>https://github.com/exo-explore/exo</guid>
            <pubDate>Tue, 23 Dec 2025 00:04:41 GMT</pubDate>
            <description><![CDATA[Run your own AI cluster at home with everyday devices üì±üíª üñ•Ô∏è‚åö]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/exo-explore/exo">exo-explore/exo</a></h1>
            <p>Run your own AI cluster at home with everyday devices üì±üíª üñ•Ô∏è‚åö</p>
            <p>Language: Python</p>
            <p>Stars: 36,993</p>
            <p>Forks: 2,478</p>
            <p>Stars today: 1,320 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;/docs/imgs/exo-logo-black-bg.jpg&quot;&gt;
  &lt;img alt=&quot;exo logo&quot; src=&quot;/docs/imgs/exo-logo-transparent.png&quot; width=&quot;50%&quot; height=&quot;50%&quot;&gt;
&lt;/picture&gt;

exo: Run your own AI cluster at home with everyday devices. Maintained by [exo labs](https://x.com/exolabs).

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://discord.gg/72NsF6ux&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20Server-5865F2?logo=discord&amp;logoColor=white&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://x.com/exolabs&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/exolabs?style=social&quot; alt=&quot;X&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-Apache2.0-blue.svg&quot; alt=&quot;License: Apache-2.0&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;/div&gt;

---

exo connects all your devices into an AI cluster. Not only does exo enable running models larger than would fit on a single device, but with [day-0 support for RDMA over Thunderbolt](https://x.com/exolabs/status/2001817749744476256?s=20), makes models run faster as you add more devices.

## Features

- **Automatic Device Discovery**: Devices running exo automatically discover each other - no manual configuration.
- **RDMA over Thunderbolt**: exo ships with [day-0 support for RDMA over Thunderbolt 5](https://x.com/exolabs/status/2001817749744476256?s=20), enabling 99% reduction in latency between devices.
- **Topology-Aware Auto Parallel**: exo figures out the best way to split your model across all available devices based on a realtime view of your device topology. It takes into account device resources and network latency/bandwidth between each link.
- **Tensor Parallelism**: exo supports sharding models, for up to 1.8x speedup on 2 devices and 3.2x speedup on 4 devices.
- **MLX Support**: exo uses [MLX](https://github.com/ml-explore/mlx) as an inference backend and [MLX distributed](https://ml-explore.github.io/mlx/build/html/usage/distributed.html) for distributed communication.

## Benchmarks

&lt;details&gt;
  &lt;summary&gt;Qwen3-235B (8-bit) on 4 √ó M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt;
  &lt;img src=&quot;docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-1-qwen3-235b.jpeg&quot; alt=&quot;Benchmark - Qwen3-235B (8-bit) on 4 √ó M3 Ultra Mac Studio with Tensor Parallel RDMA&quot; width=&quot;80%&quot; /&gt;
  &lt;p&gt;
    &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href=&quot;https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5&quot;&gt;Jeff Geerling: 15 TB VRAM on Mac Studio ‚Äì RDMA over Thunderbolt‚ÄØ5&lt;/a&gt;
  &lt;/p&gt;
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;DeepSeek v3.1 671B (8-bit) on 4 √ó M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt;
  &lt;img src=&quot;docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-2-deepseek-3.1-671b.jpeg&quot; alt=&quot;Benchmark - DeepSeek v3.1 671B (8-bit) on 4 √ó M3 Ultra Mac Studio with Tensor Parallel RDMA&quot; width=&quot;80%&quot; /&gt;
  &lt;p&gt;
    &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href=&quot;https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5&quot;&gt;Jeff Geerling: 15 TB VRAM on Mac Studio ‚Äì RDMA over Thunderbolt‚ÄØ5&lt;/a&gt;
  &lt;/p&gt;
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;Kimi K2 Thinking (native 4-bit) on 4 √ó M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt;
  &lt;img src=&quot;docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-3-kimi-k2-thinking.jpeg&quot; alt=&quot;Benchmark - Kimi K2 Thinking (native 4-bit) on 4 √ó M3 Ultra Mac Studio with Tensor Parallel RDMA&quot; width=&quot;80%&quot; /&gt;
  &lt;p&gt;
    &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href=&quot;https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5&quot;&gt;Jeff Geerling: 15 TB VRAM on Mac Studio ‚Äì RDMA over Thunderbolt‚ÄØ5&lt;/a&gt;
  &lt;/p&gt;
&lt;/details&gt;

---

## Quick Start

Devices running exo automatically discover each other, without needing any manual configuration. Each device provides an API and a dashboard for interacting with your cluster (runs at `http://localhost:52415`).

There are two ways to run exo:

### Run from Source (Mac &amp; Linux)

**Prerequisites:**
- [brew](https://github.com/Homebrew/brew) (for simple package management on MacOS)
  
  ```bash
  /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot;
  ```
- [uv](https://github.com/astral-sh/uv) (for Python dependency management)
- [macmon](https://github.com/vladkens/macmon) (for hardware monitoring on Apple Silicon)
- [node](https://github.com/nodejs/node) (for building the dashboard)
  
  ```bash
  brew install uv macmon node
  ```
- [rust](https://github.com/rust-lang/rustup) (to build Rust bindings, nightly for now)

  ```bash
  curl --proto &#039;=https&#039; --tlsv1.2 -sSf https://sh.rustup.rs | sh
  rustup toolchain install nightly
  ```

Clone the repo, build the dashboard, and run exo:

```bash
# Clone exo
git clone https://github.com/exo-explore/exo

# Build dashboard
cd exo/dashboard &amp;&amp; npm install &amp;&amp; npm run build &amp;&amp; cd ..

# Run exo
uv run exo
```

This starts the exo dashboard and API at http://localhost:52415/

### macOS App

exo ships a macOS app that runs in the background on your Mac.

&lt;img src=&quot;docs/imgs/macos-app-one-macbook.png&quot; alt=&quot;exo macOS App - running on a MacBook&quot; width=&quot;35%&quot; /&gt;

The macOS app requires macOS Tahoe 26.2 or later.

Download the latest build here: [EXO-latest.dmg](https://assets.exolabs.net/EXO-latest.dmg).

The app will ask for permission to modify system settings and install a new Network profile. Improvements to this are being worked on.

---

### Using the API

If you prefer to interact with exo via the API, here is an example creating an instance of a small model (`mlx-community/Llama-3.2-1B-Instruct-4bit`), sending a chat completions request and deleting the instance.

---

**1. Preview instance placements**

The `/instance/previews` endpoint will preview all valid placements for your model.

```bash
curl &quot;http://localhost:52415/instance/previews?model_id=llama-3.2-1b&quot;
```

Sample response:

```json
{
  &quot;previews&quot;: [
    {
      &quot;model_id&quot;: &quot;mlx-community/Llama-3.2-1B-Instruct-4bit&quot;,
      &quot;sharding&quot;: &quot;Pipeline&quot;,
      &quot;instance_meta&quot;: &quot;MlxRing&quot;,
      &quot;instance&quot;: {...},
      &quot;memory_delta_by_node&quot;: {&quot;local&quot;: 729808896},
      &quot;error&quot;: null
    }
    // ...possibly more placements...
  ]
}
```

This will return all valid placements for this model. Pick a placement that you like.
To pick the first one, pipe into `jq`:

```bash
curl &quot;http://localhost:52415/instance/previews?model_id=llama-3.2-1b&quot; | jq -c &#039;.previews[] | select(.error == null) | .instance&#039; | head -n1
```

---

**2. Create a model instance**

Send a POST to `/instance` with your desired placement in the `instance` field (the full payload must match types as in `CreateInstanceParams`), which you can copy from step 1:

```bash
curl -X POST http://localhost:52415/instance \
  -H &#039;Content-Type: application/json&#039; \
  -d &#039;{
    &quot;instance&quot;: {...}
  }&#039;
```


Sample response:

```json
{
  &quot;message&quot;: &quot;Command received.&quot;,
  &quot;command_id&quot;: &quot;e9d1a8ab-....&quot;
}
```

---

**3. Send a chat completion**

Now, make a POST to `/v1/chat/completions` (the same format as OpenAI&#039;s API):

```bash
curl -N -X POST http://localhost:52415/v1/chat/completions \
  -H &#039;Content-Type: application/json&#039; \
  -d &#039;{
    &quot;model&quot;: &quot;mlx-community/Llama-3.2-1B-Instruct-4bit&quot;,
    &quot;messages&quot;: [
      {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is Llama 3.2 1B?&quot;}
    ],
    &quot;stream&quot;: true
  }&#039;
```

---

**4. Delete the instance**

When you&#039;re done, delete the instance by its ID (find it via `/state` or `/instance` endpoints):

```bash
curl -X DELETE http://localhost:52415/instance/YOUR_INSTANCE_ID
```

**Other useful API endpoints*:**

- List all models: `curl http://localhost:52415/models`
- Inspect instance IDs and deployment state: `curl http://localhost:52415/state`

For further details, see API types and endpoints in [src/exo/master/api.py](src/exo/master/api.py).

---

## Hardware Accelerator Support

On macOS, exo uses the GPU. On Linux, exo currently runs on CPU. We are working on extending hardware accelerator support. If you&#039;d like support for a new hardware platform, please [search for an existing feature request](https://github.com/exo-explore/exo/issues) and add a thumbs up so we know what hardware is important to the community.

---

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines on how to contribute to exo.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[swisskyrepo/PayloadsAllTheThings]]></title>
            <link>https://github.com/swisskyrepo/PayloadsAllTheThings</link>
            <guid>https://github.com/swisskyrepo/PayloadsAllTheThings</guid>
            <pubDate>Tue, 23 Dec 2025 00:04:40 GMT</pubDate>
            <description><![CDATA[A list of useful payloads and bypass for Web Application Security and Pentest/CTF]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/swisskyrepo/PayloadsAllTheThings">swisskyrepo/PayloadsAllTheThings</a></h1>
            <p>A list of useful payloads and bypass for Web Application Security and Pentest/CTF</p>
            <p>Language: Python</p>
            <p>Stars: 73,363</p>
            <p>Forks: 16,395</p>
            <p>Stars today: 335 stars today</p>
            <h2>README</h2><pre># Payloads All The Things

A list of useful payloads and bypasses for Web Application Security.
Feel free to improve with your payloads and techniques!

You can also contribute with a :beers: IRL, or using the sponsor button.

[![Sponsor](https://img.shields.io/static/v1?label=Sponsor&amp;message=%E2%9D%A4&amp;logo=GitHub&amp;link=https://github.com/sponsors/swisskyrepo)](https://github.com/sponsors/swisskyrepo)
[![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Payloads%20All%20The%20Things,%20a%20list%20of%20useful%20payloads%20and%20bypasses%20for%20Web%20Application%20Security%20-%20by%20@pentest_swissky&amp;url=https://github.com/swisskyrepo/PayloadsAllTheThings/)

An alternative display version is available at [PayloadsAllTheThingsWeb](https://swisskyrepo.github.io/PayloadsAllTheThings/).

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/.github/banner.png&quot; alt=&quot;banner&quot;&gt;
&lt;/p&gt;

## :book: Documentation

Every section contains the following files, you can use the `_template_vuln` folder to create a new chapter:

- README.md - vulnerability description and how to exploit it, including several payloads
- Intruder - a set of files to give to Burp Intruder
- Images - pictures for the README.md
- Files - some files referenced in the README.md

You might also like the other projects from the AllTheThings family :

- [InternalAllTheThings](https://swisskyrepo.github.io/InternalAllTheThings/) - Active Directory and Internal Pentest Cheatsheets
- [HardwareAllTheThings](https://swisskyrepo.github.io/HardwareAllTheThings/) - Hardware/IOT Pentesting Wiki

You want more? Check the [Books](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/_LEARNING_AND_SOCIALS/BOOKS.md) and [YouTube channel](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/_LEARNING_AND_SOCIALS/YOUTUBE.md) selections.

## :technologist: Contributions

Be sure to read [CONTRIBUTING.md](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/CONTRIBUTING.md)

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/swisskyrepo/PayloadsAllTheThings/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=swisskyrepo/PayloadsAllTheThings&amp;max=36&quot; alt=&quot;sponsors-list&quot; &gt;
&lt;/a&gt;
&lt;/p&gt;

Thanks again for your contribution! :heart:

## :beers: Sponsors

This project is proudly sponsored by these companies.

| Logo | Description |
| --- | --- |
| [&lt;img src=&quot;https://avatars.githubusercontent.com/u/34724717?s=40&amp;v=4&quot; alt=&quot;sponsor-serpapi&quot;&gt;](https://serpapi.com) | **SerpApi** is a real time API to access Google search results. It solves the issues of having to rent proxies, solving captchas, and JSON parsing. |
| [&lt;img src=&quot;https://avatars.githubusercontent.com/u/50994705?s=40&amp;v=4&quot; alt=&quot;sponsor-projectdiscovery&quot;&gt;](https://projectdiscovery.io/) | **ProjectDiscovery** - Detect real, exploitable vulnerabilities. Harness the power of Nuclei for fast and accurate findings without false positives. |
| [&lt;img src=&quot;https://avatars.githubusercontent.com/u/48131541?s=40&amp;v=4&quot; alt=&quot;sponsor-vaadata&quot;&gt;](https://www.vaadata.com/) | **VAADATA** - Ethical Hacking Services |
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[GreyDGL/PentestGPT]]></title>
            <link>https://github.com/GreyDGL/PentestGPT</link>
            <guid>https://github.com/GreyDGL/PentestGPT</guid>
            <pubDate>Tue, 23 Dec 2025 00:04:39 GMT</pubDate>
            <description><![CDATA[A GPT-empowered penetration testing tool]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/GreyDGL/PentestGPT">GreyDGL/PentestGPT</a></h1>
            <p>A GPT-empowered penetration testing tool</p>
            <p>Language: Python</p>
            <p>Stars: 10,326</p>
            <p>Forks: 1,542</p>
            <p>Stars today: 327 stars today</p>
            <h2>README</h2><pre>&lt;!-- Improved compatibility of back to top link: See: https://github.com/othneildrew/Best-README-Template/pull/73 --&gt;
&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;!-- PROJECT SHIELDS --&gt;
[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]
[![MIT License][license-shield]][license-url]
[![Discord][discord-shield]][discord-url]

&lt;!-- PROJECT LOGO --&gt;
&lt;br /&gt;
&lt;div align=&quot;center&quot;&gt;

&lt;h3 align=&quot;center&quot;&gt;PentestGPT&lt;/h3&gt;

  &lt;p align=&quot;center&quot;&gt;
    AI-Powered Autonomous Penetration Testing Agent
    &lt;br /&gt;
    &lt;strong&gt;Published at USENIX Security 2024&lt;/strong&gt;
    &lt;br /&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://www.usenix.org/conference/usenixsecurity24/presentation/deng&quot;&gt;Research Paper&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://github.com/GreyDGL/PentestGPT/issues&quot;&gt;Report Bug&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://github.com/GreyDGL/PentestGPT/issues&quot;&gt;Request Feature&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

&lt;!-- ABOUT THE PROJECT --&gt;
&lt;a href=&quot;https://trendshift.io/repositories/3770&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/3770&quot; alt=&quot;GreyDGL%2FPentestGPT | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&gt; [!WARNING]
&gt; **PentestGPT is a research prototype only**
&gt;
&gt; PentestGPT is a research prototype that pioneered the use of GenAI in cybersecurity. Please be aware of third-party services claiming to offer paid PentestGPT products - the original project is free and open-source.

---

## Demo

### Installation
[![Installation Demo](https://asciinema.org/a/761661.svg)](https://asciinema.org/a/761661)

[Watch on YouTube](https://www.youtube.com/watch?v=RUNmoXqBwVg)

### PentestGPT in Action
[![PentestGPT Demo](https://asciinema.org/a/761663.svg)](https://asciinema.org/a/761663)

[Watch on YouTube](https://www.youtube.com/watch?v=cWi3Yb7RmZA)

---

## What&#039;s New in v1.0 (Agentic Upgrade)

- **Autonomous Agent** - Agentic pipeline for intelligent, autonomous penetration testing
- **Session Persistence** - Save and resume penetration testing sessions
- **Docker-First** - Isolated, reproducible environment with security tools pre-installed

&gt; **In Progress**: Multi-model support for OpenAI, Gemini, and other LLM providers

---

## Features

- **AI-Powered Challenge Solver** - Leverages LLM advanced reasoning to perform penetration testing and CTFs
- **Live Walkthrough** - Tracks steps in real-time as the agent works through challenges
- **Multi-Category Support** - Web, Crypto, Reversing, Forensics, PWN, Privilege Escalation
- **Real-Time Feedback** - Watch the AI work with live activity updates
- **Extensible Architecture** - Clean, modular design ready for future enhancements

---

## Quick Start

### Prerequisites

- **Docker** (required) - [Install Docker](https://docs.docker.com/get-docker/)
- **LLM Provider** (choose one):
  - Anthropic API Key from [console.anthropic.com](https://console.anthropic.com/)
  - Claude OAuth Login (requires Claude subscription)
  - OpenRouter for alternative models at [openrouter.ai](https://openrouter.ai/keys)
  - [Tutorial: Using Local Models with Claude Code](https://docs.google.com/document/d/1ixK7x-wlr5t5TYZJdfm75UME5KnPCpS46boLkUXKg1w/edit?usp=sharing)


### Installation

```bash
# Clone and build
git clone --recurse-submodules https://github.com/GreyDGL/PentestGPT.git
cd PentestGPT
make install

# Configure authentication (first time only)
make config

# Connect to container
make connect
```

&gt; **Note**: The `--recurse-submodules` flag downloads the benchmark suite. If you already cloned without it, run: `git submodule update --init --recursive`

### Try a Benchmark

```bash
uv run pentestgpt-benchmark start XBEN-037-24 
```

Then connect into the container and run:

```bash
pentestgpt --target http://host.docker.internal:8000
```

### Commands Reference

| Command | Description |
|---------|-------------|
| `make install` | Build the Docker image |
| `make config` | Configure API key (first-time setup) |
| `make connect` | Connect to container (main entry point) |
| `make stop` | Stop container (config persists) |
| `make clean-docker` | Remove everything including config |


---

## Usage

```bash
# Interactive TUI mode (default)
pentestgpt --target 10.10.11.234

# Non-interactive mode
pentestgpt --target 10.10.11.100 --non-interactive

# With challenge context
pentestgpt --target 10.10.11.50 --instruction &quot;WordPress site, focus on plugin vulnerabilities&quot;
```

**Keyboard Shortcuts:** `F1` Help | `Ctrl+P` Pause/Resume | `Ctrl+Q` Quit

---

## Using Local LLMs

PentestGPT supports routing requests to local LLM servers (LM Studio, Ollama, text-generation-webui, etc.) running on your host machine.

### Prerequisites

- Local LLM server with an OpenAI-compatible API endpoint
  - **LM Studio**: Enable server mode (default port 1234)
  - **Ollama**: Run `ollama serve` (default port 11434)

### Setup

```bash
# Configure PentestGPT for local LLM
make config
# Select option 4: Local LLM

# Start your local LLM server on the host machine
# Then connect to the container
make connect
```

### Customizing Models

Edit `scripts/ccr-config-template.json` to customize:

- **`localLLM.api_base_url`**: Your LLM server URL (default: `host.docker.internal:1234`)
- **`localLLM.models`**: Available model names on your server
- **Router section**: Which models handle which operations

| Route | Purpose | Default Model |
|-------|---------|---------------|
| `default` | General tasks | openai/gpt-oss-20b |
| `background` | Background operations | openai/gpt-oss-20b |
| `think` | Reasoning-heavy tasks | qwen/qwen3-coder-30b |
| `longContext` | Large context handling | qwen/qwen3-coder-30b |
| `webSearch` | Web search operations | openai/gpt-oss-20b |

### Troubleshooting

- **Connection refused**: Ensure your LLM server is running and listening on the configured port
- **Docker networking**: Use `host.docker.internal` (not `localhost`) to access host services from Docker
- **Check CCR logs**: Inside the container, run `cat /tmp/ccr.log`

---

## Telemetry

PentestGPT collects anonymous usage data to help improve the tool. This data is sent to our [Langfuse](https://langfuse.com) project and includes:
- Session metadata (target type, duration, completion status)
- Tool execution patterns (which tools are used, not the actual commands)
- Flag detection events (that a flag was found, not the flag content)

**No sensitive data is collected** - command outputs, credentials, or actual flag values are never transmitted.

### Opting Out

```bash
# Via command line flag
pentestgpt --target 10.10.11.234 --no-telemetry

# Via environment variable
export LANGFUSE_ENABLED=false
```

---

## Benchmarks

PentestGPT includes 100+ vulnerability challenges for testing and development.

```bash
pentestgpt-benchmark list                    # List all benchmarks
pentestgpt-benchmark list --levels 1         # Filter by difficulty
pentestgpt-benchmark list --tags sqli        # Filter by vulnerability type
pentestgpt-benchmark start XBEN-037-24       # Start a benchmark
pentestgpt-benchmark status                  # Check running benchmarks
pentestgpt-benchmark stop XBEN-037-24        # Stop a benchmark
```

**Available Tags:** `sqli`, `xss`, `idor`, `ssti`, `ssrf`, `lfi`, `rce`

---

## Development

### Prerequisites

- **uv** (required) - Python package manager: `curl -LsSf https://astral.sh/uv/install.sh | sh`
- **Claude Code CLI** - Configure with `claude login` or `export ANTHROPIC_API_KEY=&#039;your-key&#039;`
  - [Tutorial: Using Local Models with Claude Code](https://docs.google.com/document/d/1ixK7x-wlr5t5TYZJdfm75UME5KnPCpS46boLkUXKg1w/edit?usp=sharing)

### Local Development

```bash
uv sync                                      # Install dependencies
uv run pentestgpt --target 10.10.11.234      # Run locally
```

### Project Commands

```bash
make test          # Run pytest
make lint          # Run ruff linter
make typecheck     # Run mypy
make ci            # Run full CI simulation (lint, format, typecheck, test, build)
make ci-quick      # Quick CI without build step
```

---

## Legacy Version

The previous multi-LLM version (v0.15) supporting OpenAI, Gemini, Deepseek, and Ollama is archived in [`legacy/`](legacy/):

```bash
cd legacy &amp;&amp; pip install -e . &amp;&amp; pentestgpt --reasoning gpt-4o
```

---

## Citation

If you use PentestGPT in your research, please cite our paper:

```bibtex
@inproceedings{299699,
  author = {Gelei Deng and Yi Liu and V√≠ctor Mayoral-Vilches and Peng Liu and Yuekang Li and Yuan Xu and Tianwei Zhang and Yang Liu and Martin Pinzger and Stefan Rass},
  title = {{PentestGPT}: Evaluating and Harnessing Large Language Models for Automated Penetration Testing},
  booktitle = {33rd USENIX Security Symposium (USENIX Security 24)},
  year = {2024},
  isbn = {978-1-939133-44-1},
  address = {Philadelphia, PA},
  pages = {847--864},
  url = {https://www.usenix.org/conference/usenixsecurity24/presentation/deng},
  publisher = {USENIX Association},
  month = aug
}
```

---

## License

Distributed under the MIT License. See `LICENSE.md` for more information.

**Disclaimer**: This tool is for educational purposes and authorized security testing only. The authors do not condone any illegal use. Use at your own risk.

---

## Contact

- **Gelei Deng** - [![LinkedIn][linkedin-shield]][linkedin-url] - gelei.deng@ntu.edu.sg
- **Yi Liu** - yi009@e.ntu.edu.sg
- **Yuekang Li** - yuekang.li@unsw.edu.au
- **V√≠ctor Mayoral Vilches** - [![LinkedIn][linkedin-shield]][linkedin-url2] - v.mayoralv@gmail.com
- **Peng Liu** - liu_peng@i2r.a-star.edu.sg

---

## Acknowledgments

- Research supported by [Quantstamp](https://www.quantstamp.com/) and [NTU Singapore](https://www.ntu.edu.sg/)

&lt;p align=&quot;right&quot;&gt;(&lt;a href=&quot;#readme-top&quot;&gt;back to top&lt;/a&gt;)&lt;/p&gt;

&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt;
[contributors-shield]: https://img.shields.io/github/contributors/GreyDGL/PentestGPT.svg?style=for-the-badge
[contributors-url]: https://github.com/GreyDGL/PentestGPT/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/GreyDGL/PentestGPT.svg?style=for-the-badge
[forks-url]: https://github.com/GreyDGL/PentestGPT/network/members
[stars-shield]: https://img.shields.io/github/stars/GreyDGL/PentestGPT.svg?style=for-the-badge
[stars-url]: https://github.com/GreyDGL/PentestGPT/stargazers
[issues-shield]: https://img.shields.io/github/issues/GreyDGL/PentestGPT.svg?style=for-the-badge
[issues-url]: https://github.com/GreyDGL/PentestGPT/issues
[license-shield]: https://img.shields.io/github/license/GreyDGL/PentestGPT.svg?style=for-the-badge
[license-url]: https://github.com/GreyDGL/PentestGPT/blob/master/LICENSE.md
[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&amp;logo=linkedin&amp;colorB=555
[linkedin-url]: https://www.linkedin.com/in/gelei-deng-225a10112/
[linkedin-url2]: https://www.linkedin.com/in/vmayoral/
[discord-shield]: https://dcbadge.vercel.app/api/server/eC34CEfEkK
[discord-url]: https://discord.gg/eC34CEfEkK
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[anthropics/skills]]></title>
            <link>https://github.com/anthropics/skills</link>
            <guid>https://github.com/anthropics/skills</guid>
            <pubDate>Tue, 23 Dec 2025 00:04:38 GMT</pubDate>
            <description><![CDATA[Public repository for Agent Skills]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/anthropics/skills">anthropics/skills</a></h1>
            <p>Public repository for Agent Skills</p>
            <p>Language: Python</p>
            <p>Stars: 24,956</p>
            <p>Forks: 2,335</p>
            <p>Stars today: 928 stars today</p>
            <h2>README</h2><pre>&gt; **Note:** This repository contains Anthropic&#039;s implementation of skills for Claude. For information about the Agent Skills standard, see [agentskills.io](http://agentskills.io).

# Skills
Skills are folders of instructions, scripts, and resources that Claude loads dynamically to improve performance on specialized tasks. Skills teach Claude how to complete specific tasks in a repeatable way, whether that&#039;s creating documents with your company&#039;s brand guidelines, analyzing data using your organization&#039;s specific workflows, or automating personal tasks.

For more information, check out:
- [What are skills?](https://support.claude.com/en/articles/12512176-what-are-skills)
- [Using skills in Claude](https://support.claude.com/en/articles/12512180-using-skills-in-claude)
- [How to create custom skills](https://support.claude.com/en/articles/12512198-creating-custom-skills)
- [Equipping agents for the real world with Agent Skills](https://anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills)

# About This Repository

This repository contains skills that demonstrate what&#039;s possible with Claude&#039;s skills system. These skills range from creative applications (art, music, design) to technical tasks (testing web apps, MCP server generation) to enterprise workflows (communications, branding, etc.).

Each skill is self-contained in its own folder with a `SKILL.md` file containing the instructions and metadata that Claude uses. Browse through these skills to get inspiration for your own skills or to understand different patterns and approaches.

Many skills in this repo are open source (Apache 2.0). We&#039;ve also included the document creation &amp; editing skills that power [Claude&#039;s document capabilities](https://www.anthropic.com/news/create-files) under the hood in the [`skills/docx`](./skills/docx), [`skills/pdf`](./skills/pdf), [`skills/pptx`](./skills/pptx), and [`skills/xlsx`](./skills/xlsx) subfolders. These are source-available, not open source, but we wanted to share these with developers as a reference for more complex skills that are actively used in a production AI application.

## Disclaimer

**These skills are provided for demonstration and educational purposes only.** While some of these capabilities may be available in Claude, the implementations and behaviors you receive from Claude may differ from what is shown in these skills. These skills are meant to illustrate patterns and possibilities. Always test skills thoroughly in your own environment before relying on them for critical tasks.

# Skill Sets
- [./skills](./skills): Skill examples for Creative &amp; Design, Development &amp; Technical, Enterprise &amp; Communication, and Document Skills
- [./spec](./spec): The Agent Skills specification
- [./template](./template): Skill template

# Try in Claude Code, Claude.ai, and the API

## Claude Code
You can register this repository as a Claude Code Plugin marketplace by running the following command in Claude Code:
```
/plugin marketplace add anthropics/skills
```

Then, to install a specific set of skills:
1. Select `Browse and install plugins`
2. Select `anthropic-agent-skills`
3. Select `document-skills` or `example-skills`
4. Select `Install now`

Alternatively, directly install either Plugin via:
```
/plugin install document-skills@anthropic-agent-skills
/plugin install example-skills@anthropic-agent-skills
```

After installing the plugin, you can use the skill by just mentioning it. For instance, if you install the `document-skills` plugin from the marketplace, you can ask Claude Code to do something like: &quot;Use the PDF skill to extract the form fields from `path/to/some-file.pdf`&quot;

## Claude.ai

These example skills are all already available to paid plans in Claude.ai. 

To use any skill from this repository or upload custom skills, follow the instructions in [Using skills in Claude](https://support.claude.com/en/articles/12512180-using-skills-in-claude#h_a4222fa77b).

## Claude API

You can use Anthropic&#039;s pre-built skills, and upload custom skills, via the Claude API. See the [Skills API Quickstart](https://docs.claude.com/en/api/skills-guide#creating-a-skill) for more.

# Creating a Basic Skill

Skills are simple to create - just a folder with a `SKILL.md` file containing YAML frontmatter and instructions. You can use the **template-skill** in this repository as a starting point:

```markdown
---
name: my-skill-name
description: A clear description of what this skill does and when to use it
---

# My Skill Name

[Add your instructions here that Claude will follow when this skill is active]

## Examples
- Example usage 1
- Example usage 2

## Guidelines
- Guideline 1
- Guideline 2
```

The frontmatter requires only two fields:
- `name` - A unique identifier for your skill (lowercase, hyphens for spaces)
- `description` - A complete description of what the skill does and when to use it

The markdown content below contains the instructions, examples, and guidelines that Claude will follow. For more details, see [How to create custom skills](https://support.claude.com/en/articles/12512198-creating-custom-skills).

# Partner Skills

Skills are a great way to teach Claude how to get better at using specific pieces of software. As we see awesome example skills from partners, we may highlight some of them here:

- **Notion** - [Notion Skills for Claude](https://www.notion.so/notiondevs/Notion-Skills-for-Claude-28da4445d27180c7af1df7d8615723d0)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[rendercv/rendercv]]></title>
            <link>https://github.com/rendercv/rendercv</link>
            <guid>https://github.com/rendercv/rendercv</guid>
            <pubDate>Tue, 23 Dec 2025 00:04:37 GMT</pubDate>
            <description><![CDATA[Typst-based CV/resume generator for academics and engineers]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/rendercv/rendercv">rendercv/rendercv</a></h1>
            <p>Typst-based CV/resume generator for academics and engineers</p>
            <p>Language: Python</p>
            <p>Stars: 5,046</p>
            <p>Forks: 416</p>
            <p>Stars today: 368 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;h1&gt;RenderCV&lt;/h1&gt;

_CV/resume generator for academics and engineers_

[![test](https://github.com/rendercv/rendercv/actions/workflows/test.yaml/badge.svg?branch=main)](https://github.com/rendercv/rendercv/actions/workflows/test.yaml)
[![coverage](https://coverage-badge.samuelcolvin.workers.dev/rendercv/rendercv.svg)](https://coverage-badge.samuelcolvin.workers.dev/redirect/rendercv/rendercv)
[![docs](&lt;https://img.shields.io/badge/docs-mkdocs-rgb(0%2C79%2C144)&gt;)](https://docs.rendercv.com)
[![pypi-version](&lt;https://img.shields.io/pypi/v/rendercv?label=PyPI%20version&amp;color=rgb(0%2C79%2C144)&gt;)](https://pypi.python.org/pypi/rendercv)
[![pypi-downloads](&lt;https://img.shields.io/pepy/dt/rendercv?label=PyPI%20downloads&amp;color=rgb(0%2C%2079%2C%20144)&gt;)](https://pypistats.org/packages/rendercv)

&lt;/div&gt;

Write your CV or resume as YAML, then run RenderCV,

```bash
rendercv render John_Doe_CV.yaml
```

and get a PDF with perfect typography. No template wrestling. No broken layouts. Consistent spacing, every time.

With RenderCV, you can:

- Version-control your CV ‚Äî it&#039;s just text.
- Focus on content ‚Äî don&#039;t wory about the formatting.
- Get perfect typography ‚Äî pixel-perfect alignment and spacing, handled for you.

A YAML file like this:

```yaml
cv:
  name: John Doe
  location: San Francisco, CA
  email: john.doe@email.com
  website: https://rendercv.com/
  social_networks:
    - network: LinkedIn
      username: rendercv
    - network: GitHub
      username: rendercv
  sections:
    Welcome to RenderCV:
      - RenderCV reads a CV written in a YAML file, and generates a PDF with professional typography.
      - See the [documentation](https://docs.rendercv.com) for more details.
    education:
      - institution: Princeton University
        area: Computer Science
        degree: PhD
        date:
        start_date: 2018-09
        end_date: 2023-05
        location: Princeton, NJ
        summary:
        highlights:
          - &quot;Thesis: Efficient Neural Architecture Search for Resource-Constrained Deployment&quot;
          - &quot;Advisor: Prof. Sanjeev Arora&quot;
          - NSF Graduate Research Fellowship, Siebel Scholar (Class of 2022)
    ...
```

becomes one of these PDFs. Click on the images to preview.

| [![Classic Theme Example of RenderCV](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/classic.png)](https://github.com/rendercv/rendercv/blob/main/examples/John_Doe_ClassicTheme_CV.pdf)    | [![Engineeringresumes Theme Example of RenderCV](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/engineeringresumes.png)](https://github.com/rendercv/rendercv/blob/main/examples/John_Doe_EngineeringresumesTheme_CV.pdf) | [![Sb2nov Theme Example of RenderCV](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/sb2nov.png)](https://github.com/rendercv/rendercv/blob/main/examples/John_Doe_Sb2novTheme_CV.pdf) |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [![Moderncv Theme Example of RenderCV](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/moderncv.png)](https://github.com/rendercv/rendercv/blob/main/examples/John_Doe_ModerncvTheme_CV.pdf) | [![Engineeringclassic Theme Example of RenderCV](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/engineeringclassic.png)](https://github.com/rendercv/rendercv/blob/main/examples/John_Doe_EngineeringclassicTheme_CV.pdf) | ![Custom themes can be added.](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/customtheme.png)                                                                                        |


## JSON Schema

RenderCV&#039;s JSON Schema lets you fill out the YAML interactively, with autocompletion and inline documentation.

![JSON Schema of RenderCV](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/json_schema.gif)


## Extensive Design Options

You have full control over every detail.

```yaml
design:
  theme: classic
  page:
    size: us-letter
    top_margin: 0.7in
    bottom_margin: 0.7in
    left_margin: 0.7in
    right_margin: 0.7in
    show_footer: true
    show_top_note: true
  colors:
    body: rgb(0, 0, 0)
    name: rgb(0, 79, 144)
    headline: rgb(0, 79, 144)
    connections: rgb(0, 79, 144)
    section_titles: rgb(0, 79, 144)
    links: rgb(0, 79, 144)
    footer: rgb(128, 128, 128)
    top_note: rgb(128, 128, 128)
  typography:
    line_spacing: 0.6em
    alignment: justified
    date_and_location_column_alignment: right
    font_family: Source Sans 3
  # ...and much more
```

![Design Options of RenderCV](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/design_options.gif)

&gt; [!TIP]
&gt; Want to set up a live preview environment like the one shown above? See [how to set up VS Code for RenderCV](https://docs.rendercv.com/user_guide/how_to/set_up_vs_code_for_rendercv).

## Strict Validation

No surprises. If something&#039;s wrong, you&#039;ll know exactly what and where. If it&#039;s valid, you get a perfect PDF.

![Strict Validation Feature of RenderCV](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/validation.gif)


## Any Language

Fill out the locale field for your language.

```yaml
locale:
  language: english
  last_updated: Last updated in
  month: month
  months: months
  year: year
  years: years
  present: present
  month_abbreviations:
    - Jan
    - Feb
    - Mar
  ...
```

## Get Started

Install RenderCV (Requires Python 3.12+):

```
pip install &quot;rendercv[full]&quot;
```

Create a new CV yaml file:

```
rendercv new &quot;John Doe&quot;
```

Edit the YAML, then render:

```
rendercv render &quot;John_Doe_CV.yaml&quot;
```

For more details, see the [user guide](https://docs.rendercv.com/user_guide/).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[home-assistant/core]]></title>
            <link>https://github.com/home-assistant/core</link>
            <guid>https://github.com/home-assistant/core</guid>
            <pubDate>Tue, 23 Dec 2025 00:04:36 GMT</pubDate>
            <description><![CDATA[üè° Open source home automation that puts local control and privacy first.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/home-assistant/core">home-assistant/core</a></h1>
            <p>üè° Open source home automation that puts local control and privacy first.</p>
            <p>Language: Python</p>
            <p>Stars: 83,461</p>
            <p>Forks: 36,250</p>
            <p>Stars today: 40 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[google/langextract]]></title>
            <link>https://github.com/google/langextract</link>
            <guid>https://github.com/google/langextract</guid>
            <pubDate>Tue, 23 Dec 2025 00:04:35 GMT</pubDate>
            <description><![CDATA[A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google/langextract">google/langextract</a></h1>
            <p>A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.</p>
            <p>Language: Python</p>
            <p>Stars: 18,210</p>
            <p>Forks: 1,301</p>
            <p>Stars today: 299 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/google/langextract&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/google/langextract/main/docs/_static/logo.svg&quot; alt=&quot;LangExtract Logo&quot; width=&quot;128&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

# LangExtract

[![PyPI version](https://img.shields.io/pypi/v/langextract.svg)](https://pypi.org/project/langextract/)
[![GitHub stars](https://img.shields.io/github/stars/google/langextract.svg?style=social&amp;label=Star)](https://github.com/google/langextract)
![Tests](https://github.com/google/langextract/actions/workflows/ci.yaml/badge.svg)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.17015089.svg)](https://doi.org/10.5281/zenodo.17015089)

## Table of Contents

- [Introduction](#introduction)
- [Why LangExtract?](#why-langextract)
- [Quick Start](#quick-start)
- [Installation](#installation)
- [API Key Setup for Cloud Models](#api-key-setup-for-cloud-models)
- [Adding Custom Model Providers](#adding-custom-model-providers)
- [Using OpenAI Models](#using-openai-models)
- [Using Local LLMs with Ollama](#using-local-llms-with-ollama)
- [More Examples](#more-examples)
  - [*Romeo and Juliet* Full Text Extraction](#romeo-and-juliet-full-text-extraction)
  - [Medication Extraction](#medication-extraction)
  - [Radiology Report Structuring: RadExtract](#radiology-report-structuring-radextract)
- [Community Providers](#community-providers)
- [Contributing](#contributing)
- [Testing](#testing)
- [Disclaimer](#disclaimer)

## Introduction

LangExtract is a Python library that uses LLMs to extract structured information from unstructured text documents based on user-defined instructions. It processes materials such as clinical notes or reports, identifying and organizing key details while ensuring the extracted data corresponds to the source text.

## Why LangExtract?

1.  **Precise Source Grounding:** Maps every extraction to its exact location in the source text, enabling visual highlighting for easy traceability and verification.
2.  **Reliable Structured Outputs:** Enforces a consistent output schema based on your few-shot examples, leveraging controlled generation in supported models like Gemini to guarantee robust, structured results.
3.  **Optimized for Long Documents:** Overcomes the &quot;needle-in-a-haystack&quot; challenge of large document extraction by using an optimized strategy of text chunking, parallel processing, and multiple passes for higher recall.
4.  **Interactive Visualization:** Instantly generates a self-contained, interactive HTML file to visualize and review thousands of extracted entities in their original context.
5.  **Flexible LLM Support:** Supports your preferred models, from cloud-based LLMs like the Google Gemini family to local open-source models via the built-in Ollama interface.
6.  **Adaptable to Any Domain:** Define extraction tasks for any domain using just a few examples. LangExtract adapts to your needs without requiring any model fine-tuning.
7.  **Leverages LLM World Knowledge:** Utilize precise prompt wording and few-shot examples to influence how the extraction task may utilize LLM knowledge. The accuracy of any inferred information and its adherence to the task specification are contingent upon the selected LLM, the complexity of the task, the clarity of the prompt instructions, and the nature of the prompt examples.

## Quick Start

&gt; **Note:** Using cloud-hosted models like Gemini requires an API key. See the [API Key Setup](#api-key-setup-for-cloud-models) section for instructions on how to get and configure your key.

Extract structured information with just a few lines of code.

### 1. Define Your Extraction Task

First, create a prompt that clearly describes what you want to extract. Then, provide a high-quality example to guide the model.

```python
import langextract as lx
import textwrap

# 1. Define the prompt and extraction rules
prompt = textwrap.dedent(&quot;&quot;&quot;\
    Extract characters, emotions, and relationships in order of appearance.
    Use exact text for extractions. Do not paraphrase or overlap entities.
    Provide meaningful attributes for each entity to add context.&quot;&quot;&quot;)

# 2. Provide a high-quality example to guide the model
examples = [
    lx.data.ExampleData(
        text=&quot;ROMEO. But soft! What light through yonder window breaks? It is the east, and Juliet is the sun.&quot;,
        extractions=[
            lx.data.Extraction(
                extraction_class=&quot;character&quot;,
                extraction_text=&quot;ROMEO&quot;,
                attributes={&quot;emotional_state&quot;: &quot;wonder&quot;}
            ),
            lx.data.Extraction(
                extraction_class=&quot;emotion&quot;,
                extraction_text=&quot;But soft!&quot;,
                attributes={&quot;feeling&quot;: &quot;gentle awe&quot;}
            ),
            lx.data.Extraction(
                extraction_class=&quot;relationship&quot;,
                extraction_text=&quot;Juliet is the sun&quot;,
                attributes={&quot;type&quot;: &quot;metaphor&quot;}
            ),
        ]
    )
]
```

### 2. Run the Extraction

Provide your input text and the prompt materials to the `lx.extract` function.

```python
# The input text to be processed
input_text = &quot;Lady Juliet gazed longingly at the stars, her heart aching for Romeo&quot;

# Run the extraction
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gemini-2.5-flash&quot;,
)
```

&gt; **Model Selection**: `gemini-2.5-flash` is the recommended default, offering an excellent balance of speed, cost, and quality. For highly complex tasks requiring deeper reasoning, `gemini-2.5-pro` may provide superior results. For large-scale or production use, a Tier 2 Gemini quota is suggested to increase throughput and avoid rate limits. See the [rate-limit documentation](https://ai.google.dev/gemini-api/docs/rate-limits#tier-2) for details.
&gt;
&gt; **Model Lifecycle**: Note that Gemini models have a lifecycle with defined retirement dates. Users should consult the [official model version documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions) to stay informed about the latest stable and legacy versions.

### 3. Visualize the Results

The extractions can be saved to a `.jsonl` file, a popular format for working with language model data. LangExtract can then generate an interactive HTML visualization from this file to review the entities in context.

```python
# Save the results to a JSONL file
lx.io.save_annotated_documents([result], output_name=&quot;extraction_results.jsonl&quot;, output_dir=&quot;.&quot;)

# Generate the visualization from the file
html_content = lx.visualize(&quot;extraction_results.jsonl&quot;)
with open(&quot;visualization.html&quot;, &quot;w&quot;) as f:
    if hasattr(html_content, &#039;data&#039;):
        f.write(html_content.data)  # For Jupyter/Colab
    else:
        f.write(html_content)
```

This creates an animated and interactive HTML file:

![Romeo and Juliet Basic Visualization ](https://raw.githubusercontent.com/google/langextract/main/docs/_static/romeo_juliet_basic.gif)

&gt; **Note on LLM Knowledge Utilization:** This example demonstrates extractions that stay close to the text evidence - extracting &quot;longing&quot; for Lady Juliet&#039;s emotional state and identifying &quot;yearning&quot; from &quot;gazed longingly at the stars.&quot; The task could be modified to generate attributes that draw more heavily from the LLM&#039;s world knowledge (e.g., adding `&quot;identity&quot;: &quot;Capulet family daughter&quot;` or `&quot;literary_context&quot;: &quot;tragic heroine&quot;`). The balance between text-evidence and knowledge-inference is controlled by your prompt instructions and example attributes.

### Scaling to Longer Documents

For larger texts, you can process entire documents directly from URLs with parallel processing and enhanced sensitivity:

```python
# Process Romeo &amp; Juliet directly from Project Gutenberg
result = lx.extract(
    text_or_documents=&quot;https://www.gutenberg.org/files/1513/1513-0.txt&quot;,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gemini-2.5-flash&quot;,
    extraction_passes=3,    # Improves recall through multiple passes
    max_workers=20,         # Parallel processing for speed
    max_char_buffer=1000    # Smaller contexts for better accuracy
)
```

This approach can extract hundreds of entities from full novels while maintaining high accuracy. The interactive visualization seamlessly handles large result sets, making it easy to explore hundreds of entities from the output JSONL file. **[See the full *Romeo and Juliet* extraction example ‚Üí](https://github.com/google/langextract/blob/main/docs/examples/longer_text_example.md)** for detailed results and performance insights.

### Vertex AI Batch Processing

Save costs on large-scale tasks by enabling Vertex AI Batch API: `language_model_params={&quot;vertexai&quot;: True, &quot;batch&quot;: {&quot;enabled&quot;: True}}`.

See an example of the Vertex AI Batch API usage in [this example](docs/examples/batch_api_example.md).

## Installation

### From PyPI

```bash
pip install langextract
```

*Recommended for most users. For isolated environments, consider using a virtual environment:*

```bash
python -m venv langextract_env
source langextract_env/bin/activate  # On Windows: langextract_env\Scripts\activate
pip install langextract
```

### From Source

LangExtract uses modern Python packaging with `pyproject.toml` for dependency management:

*Installing with `-e` puts the package in development mode, allowing you to modify the code without reinstalling.*


```bash
git clone https://github.com/google/langextract.git
cd langextract

# For basic installation:
pip install -e .

# For development (includes linting tools):
pip install -e &quot;.[dev]&quot;

# For testing (includes pytest):
pip install -e &quot;.[test]&quot;
```

### Docker

```bash
docker build -t langextract .
docker run --rm -e LANGEXTRACT_API_KEY=&quot;your-api-key&quot; langextract python your_script.py
```

## API Key Setup for Cloud Models

When using LangExtract with cloud-hosted models (like Gemini or OpenAI), you&#039;ll need to
set up an API key. On-device models don&#039;t require an API key. For developers
using local LLMs, LangExtract offers built-in support for Ollama and can be
extended to other third-party APIs by updating the inference endpoints.

### API Key Sources

Get API keys from:

*   [AI Studio](https://aistudio.google.com/app/apikey) for Gemini models
*   [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/sdks/overview) for enterprise use
*   [OpenAI Platform](https://platform.openai.com/api-keys) for OpenAI models

### Setting up API key in your environment

**Option 1: Environment Variable**

```bash
export LANGEXTRACT_API_KEY=&quot;your-api-key-here&quot;
```

**Option 2: .env File (Recommended)**

Add your API key to a `.env` file:

```bash
# Add API key to .env file
cat &gt;&gt; .env &lt;&lt; &#039;EOF&#039;
LANGEXTRACT_API_KEY=your-api-key-here
EOF

# Keep your API key secure
echo &#039;.env&#039; &gt;&gt; .gitignore
```

In your Python code:
```python
import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=&quot;Extract information...&quot;,
    examples=[...],
    model_id=&quot;gemini-2.5-flash&quot;
)
```

**Option 3: Direct API Key (Not Recommended for Production)**

You can also provide the API key directly in your code, though this is not recommended for production use:

```python
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=&quot;Extract information...&quot;,
    examples=[...],
    model_id=&quot;gemini-2.5-flash&quot;,
    api_key=&quot;your-api-key-here&quot;  # Only use this for testing/development
)
```

**Option 4: Vertex AI (Service Accounts)**

Use [Vertex AI](https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform) for authentication with service accounts:

```python
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=&quot;Extract information...&quot;,
    examples=[...],
    model_id=&quot;gemini-2.5-flash&quot;,
    language_model_params={
        &quot;vertexai&quot;: True,
        &quot;project&quot;: &quot;your-project-id&quot;,
        &quot;location&quot;: &quot;global&quot;  # or regional endpoint
    }
)
```

## Adding Custom Model Providers

LangExtract supports custom LLM providers via a lightweight plugin system. You can add support for new models without changing core code.

- Add new model support independently of the core library
- Distribute your provider as a separate Python package
- Keep custom dependencies isolated
- Override or extend built-in providers via priority-based resolution

See the detailed guide in [Provider System Documentation](langextract/providers/README.md) to learn how to:

- Register a provider with `@registry.register(...)`
- Publish an entry point for discovery
- Optionally provide a schema with `get_schema_class()` for structured output
- Integrate with the factory via `create_model(...)`

## Using OpenAI Models

LangExtract supports OpenAI models (requires optional dependency: `pip install langextract[openai]`):

```python
import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gpt-4o&quot;,  # Automatically selects OpenAI provider
    api_key=os.environ.get(&#039;OPENAI_API_KEY&#039;),
    fence_output=True,
    use_schema_constraints=False
)
```

Note: OpenAI models require `fence_output=True` and `use_schema_constraints=False` because LangExtract doesn&#039;t implement schema constraints for OpenAI yet.

## Using Local LLMs with Ollama
LangExtract supports local inference using Ollama, allowing you to run models without API keys:

```python
import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gemma2:2b&quot;,  # Automatically selects Ollama provider
    model_url=&quot;http://localhost:11434&quot;,
    fence_output=False,
    use_schema_constraints=False
)
```

**Quick setup:** Install Ollama from [ollama.com](https://ollama.com/), run `ollama pull gemma2:2b`, then `ollama serve`.

For detailed installation, Docker setup, and examples, see [`examples/ollama/`](examples/ollama/).

## More Examples

Additional examples of LangExtract in action:

### *Romeo and Juliet* Full Text Extraction

LangExtract can process complete documents directly from URLs. This example demonstrates extraction from the full text of *Romeo and Juliet* from Project Gutenberg (147,843 characters), showing parallel processing, sequential extraction passes, and performance optimization for long document processing.

**[View *Romeo and Juliet* Full Text Example ‚Üí](https://github.com/google/langextract/blob/main/docs/examples/longer_text_example.md)**

### Medication Extraction

&gt; **Disclaimer:** This demonstration is for illustrative purposes of LangExtract&#039;s baseline capability only. It does not represent a finished or approved product, is not intended to diagnose or suggest treatment of any disease or condition, and should not be used for medical advice.

LangExtract excels at extracting structured medical information from clinical text. These examples demonstrate both basic entity recognition (medication names, dosages, routes) and relationship extraction (connecting medications to their attributes), showing LangExtract&#039;s effectiveness for healthcare applications.

**[View Medication Examples ‚Üí](https://github.com/google/langextract/blob/main/docs/examples/medication_examples.md)**

### Radiology Report Structuring: RadExtract

Explore RadExtract, a live interactive demo on HuggingFace Spaces that shows how LangExtract can automatically structure radiology reports. Try it directly in your browser with no setup required.

**[View RadExtract Demo ‚Üí](https://huggingface.co/spaces/google/radextract)**

## Community Providers

Extend LangExtract with custom model providers! Check out our [Community Provider Plugins](COMMUNITY_PROVIDERS.md) registry to discover providers created by the community or add your own.

For detailed instructions on creating a provider plugin, see the [Custom Provider Plugin Example](examples/custom_provider_plugin/).

## Contributing

Contributions are welcome! See [CONTRIBUTING.md](https://github.com/google/langextract/blob/main/CONTRIBUTING.md) to get started
with development, testing, and pull requests. You must sign a
[Contributor License Agreement](https://cla.developers.google.com/about)
before submitting patches.



## Testing

To run tests locally from the source:

```bash
# Clone the repository
git clone https://github.com/google/langextract.git
cd langextract

# Install with test dependencies
pip install -e &quot;.[test]&quot;

# Run all tests
pytest tests
```

Or reproduce the full CI matrix locally with tox:

```bash
tox  # runs pylint + pytest on Python 3.10 and 3.11
```

### Ollama Integration Testing

If you have Ollama installed locally, you can run integration tests:

```bash
# Test Ollama integration (requires Ollama running with gemma2:2b model)
tox -e ollama-integration
```

This test will automatically detect if Ollama is available and run real inference tests.

## Development

### Code Formatting

This project uses automated formatting tools to maintain consistent code style:

```bash
# Auto-format all code
./autoformat.sh

# Or run formatters separately
isort langextract tests --profile google --line-length 80
pyink langextract tests --config pyproject.toml
```

### Pre-commit Hooks

For automatic formatting checks:
```bash
pre-commit install  # One-time setup
pre-commit run --all-files  # Manual run
```

### Linting

Run linting before submitting PRs:

```bash
pylint --rcfile=.pylintrc langextract tests
```

See [CONTRIBUTING.md](CONTRIBUTING.md) for full development guidelines.

## Disclaimer

This is not an officially supported Google product. If you use
LangExtract in production or publications, please cite accordingly and
acknowledge usage. Use is subject to the [Apache 2.0 License](https://github.com/google/langextract/blob/main/LICENSE).
For health-related applications, use of LangExtract is also subject to the
[Health AI Developer Foundations Terms of Use](https://developers.google.com/health-ai-developer-foundations/terms).

---

**Happy Extracting!**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[lintsinghua/DeepAudit]]></title>
            <link>https://github.com/lintsinghua/DeepAudit</link>
            <guid>https://github.com/lintsinghua/DeepAudit</guid>
            <pubDate>Tue, 23 Dec 2025 00:04:34 GMT</pubDate>
            <description><![CDATA[DeepAuditÔºö‰∫∫‰∫∫Êã•ÊúâÁöÑ AI ÈªëÂÆ¢ÊàòÈòüÔºåËÆ©ÊºèÊ¥ûÊåñÊéòËß¶ÊâãÂèØÂèä„ÄÇÂõΩÂÜÖÈ¶ñ‰∏™ÂºÄÊ∫êÁöÑ‰ª£Á†ÅÊºèÊ¥ûÊåñÊéòÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªü„ÄÇÂ∞èÁôΩ‰∏ÄÈîÆÈÉ®ÁΩ≤ËøêË°åÔºåËá™‰∏ªÂçè‰ΩúÂÆ°ËÆ° + Ëá™Âä®ÂåñÊ≤ôÁÆ± PoC È™åËØÅ„ÄÇÊîØÊåÅ Ollama ÁßÅÊúâÈÉ®ÁΩ≤ Ôºå‰∏ÄÈîÆÁîüÊàêÊä•Âëä„ÄÇÊîØÊåÅ‰∏≠ËΩ¨Á´ô„ÄÇ‚ÄãËÆ©ÂÆâÂÖ®‰∏çÂÜçÊòÇË¥µÔºåËÆ©ÂÆ°ËÆ°‰∏çÂÜçÂ§çÊùÇ„ÄÇ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lintsinghua/DeepAudit">lintsinghua/DeepAudit</a></h1>
            <p>DeepAuditÔºö‰∫∫‰∫∫Êã•ÊúâÁöÑ AI ÈªëÂÆ¢ÊàòÈòüÔºåËÆ©ÊºèÊ¥ûÊåñÊéòËß¶ÊâãÂèØÂèä„ÄÇÂõΩÂÜÖÈ¶ñ‰∏™ÂºÄÊ∫êÁöÑ‰ª£Á†ÅÊºèÊ¥ûÊåñÊéòÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªü„ÄÇÂ∞èÁôΩ‰∏ÄÈîÆÈÉ®ÁΩ≤ËøêË°åÔºåËá™‰∏ªÂçè‰ΩúÂÆ°ËÆ° + Ëá™Âä®ÂåñÊ≤ôÁÆ± PoC È™åËØÅ„ÄÇÊîØÊåÅ Ollama ÁßÅÊúâÈÉ®ÁΩ≤ Ôºå‰∏ÄÈîÆÁîüÊàêÊä•Âëä„ÄÇÊîØÊåÅ‰∏≠ËΩ¨Á´ô„ÄÇ‚ÄãËÆ©ÂÆâÂÖ®‰∏çÂÜçÊòÇË¥µÔºåËÆ©ÂÆ°ËÆ°‰∏çÂÜçÂ§çÊùÇ„ÄÇ</p>
            <p>Language: Python</p>
            <p>Stars: 2,390</p>
            <p>Forks: 245</p>
            <p>Stars today: 140 stars today</p>
            <h2>README</h2><pre># XCodeReviewer - ÊÇ®ÁöÑÊô∫ËÉΩ‰ª£Á†ÅÂÆ°ËÆ°‰ºô‰º¥ üöÄ

&lt;div style=&quot;width: 100%; max-width: 600px; margin: 0 auto;&quot;&gt;
  &lt;img src=&quot;public/images/logo.png&quot; alt=&quot;XCodeReviewer Logo&quot; style=&quot;width: 100%; height: auto; display: block; margin: 0 auto;&quot;&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
    &lt;a href=&quot;README.md&quot;&gt;‰∏≠Êñá&lt;/a&gt; ‚Ä¢
    &lt;a href=&quot;README_EN.md&quot;&gt;English&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

[![Version](https://img.shields.io/badge/version-1.2.0-blue.svg)](https://github.com/lintsinghua/XCodeReviewer/releases)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![React](https://img.shields.io/badge/React-18-61dafb.svg)](https://reactjs.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.7-3178c6.svg)](https://www.typescriptlang.org/)
[![Vite](https://img.shields.io/badge/Vite-5.1-646cff.svg)](https://vitejs.dev/)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/lintsinghua/XCodeReviewer)

[![Stars](https://img.shields.io/github/stars/lintsinghua/XCodeReviewer?style=social)](https://github.com/lintsinghua/XCodeReviewer/stargazers)
[![Forks](https://img.shields.io/github/forks/lintsinghua/XCodeReviewer?style=social)](https://github.com/lintsinghua/XCodeReviewer/network/members)

[![Sponsor](https://img.shields.io/badge/Sponsor-ËµûÂä©-blueviolet)](https://github.com/lintsinghua/lintsinghua.github.io/issues/1)
&lt;/div&gt;

&lt;div style=&quot;width: 100%; max-width: 600px; margin: 0 auto;&quot;&gt;
  &lt;a href=&quot;https://github.com/lintsinghua/XCodeReviewer&quot;&gt;
    &lt;img src=&quot;public/star-me-cn.svg&quot; alt=&quot;Star this project&quot; style=&quot;width: 100%; height: auto; display: block; margin: 0 auto;&quot; /&gt;
  &lt;/a&gt;
&lt;/div&gt;

**XCodeReviewer** ÊòØ‰∏Ä‰∏™Áî±Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÈ©±Âä®ÁöÑÁé∞‰ª£Âåñ‰ª£Á†ÅÂÆ°ËÆ°Âπ≥Âè∞ÔºåÊó®Âú®‰∏∫ÂºÄÂèëËÄÖÊèê‰æõÊô∫ËÉΩ„ÄÅÂÖ®Èù¢‰∏îÊûÅÂÖ∑Ê∑±Â∫¶ÁöÑ‰ª£Á†ÅË¥®ÈáèÂàÜÊûêÂíåÂÆ°Êü•ÊúçÂä°„ÄÇ

#### üåê Âú®Á∫øÊºîÁ§∫

Êó†ÈúÄÈÉ®ÁΩ≤ÔºåÁõ¥Êé•ËÆøÈóÆÂú®Á∫øÊºîÁ§∫ÔºàÊï∞ÊçÆÂ≠òÂÇ®Âú®ÊµèËßàÂô®Êú¨Âú∞ÔºåÊîØÊåÅÊâÄÊúâÊ†∏ÂøÉÂäüËÉΩÔºâÔºö

**[https://xcodereviewer-preview.vercel.app](https://xcodereviewer-preview.vercel.app)**

## üåü ‰∏∫‰ªÄ‰πàÈÄâÊã© XCodeReviewerÔºü

Âú®Âø´ËäÇÂ•èÁöÑËΩØ‰ª∂ÂºÄÂèë‰∏≠Ôºå‰øùËØÅ‰ª£Á†ÅË¥®ÈáèËá≥ÂÖ≥ÈáçË¶Å„ÄÇ‰º†Áªü‰ª£Á†ÅÂÆ°ËÆ°Â∑•ÂÖ∑ËßÑÂàôÊ≠ªÊùø„ÄÅÊïàÁéá‰Ωé‰∏ãÔºåËÄå‰∫∫Â∑•ÂÆ°ËÆ°ÂàôËÄóÊó∂ËÄóÂäõ„ÄÇXCodeReviewer ÂÄüÂä© LLM ÁöÑÂº∫Â§ßËÉΩÂäõÔºåÂΩªÂ∫ïÊîπÂèò‰∫Ü‰ª£Á†ÅÂÆ°Êü•ÁöÑÊñπÂºèÔºö

![Á≥ªÁªüÊû∂ÊûÑÂõæ](public/diagram.svg)

&lt;div div align=&quot;center&quot;&gt;
  &lt;em&gt;
    XCodeReviewerÁ≥ªÁªüÊû∂ÊûÑÂõæ
  &lt;/em&gt;
&lt;/div&gt;

---

- **AI È©±Âä®ÁöÑÊ∑±Â∫¶ÂàÜÊûê**ÔºöË∂ÖË∂ä‰º†ÁªüÈùôÊÄÅÂàÜÊûêÔºåÁêÜËß£‰ª£Á†ÅÊÑèÂõæÔºåÂèëÁé∞Ê∑±Â±ÇÈÄªËæëÈóÆÈ¢ò„ÄÇ
- **Â§öÁª¥Â∫¶„ÄÅÂÖ®Êñπ‰ΩçËØÑ‰º∞**Ôºö‰ªé**ÂÆâÂÖ®ÊÄß**„ÄÅ**ÊÄßËÉΩ**„ÄÅ**ÂèØÁª¥Êä§ÊÄß**Âà∞**‰ª£Á†ÅÈ£éÊ†º**ÔºåÊèê‰æõ 360 Â∫¶Êó†Ê≠ªËßíÁöÑË¥®ÈáèËØÑ‰º∞„ÄÇ
- **Ê∏ÖÊô∞„ÄÅÂèØË°åÁöÑ‰øÆÂ§çÂª∫ËÆÆ**ÔºöÁã¨Âàõ **What-Why-How** Ê®°ÂºèÔºå‰∏ç‰ªÖÂëäËØâÊÇ®&quot;ÊòØ‰ªÄ‰πà&quot;ÈóÆÈ¢òÔºåËøòËß£Èáä&quot;‰∏∫‰ªÄ‰πà&quot;ÔºåÂπ∂Êèê‰æõ&quot;Â¶Ç‰Ωï‰øÆÂ§ç&quot;ÁöÑÂÖ∑‰Ωì‰ª£Á†ÅÁ§∫‰æã„ÄÇ
- **Â§öÂπ≥Âè∞LLM/Êú¨Âú∞LLMÊîØÊåÅ**: Â∑≤ÂÆûÁé∞ 10+ ‰∏ªÊµÅÂπ≥Âè∞APIË∞ÉÁî®ÂäüËÉΩÔºàGemini„ÄÅOpenAI„ÄÅClaude„ÄÅÈÄö‰πâÂçÉÈóÆ„ÄÅDeepSeek„ÄÅÊô∫Ë∞±AI„ÄÅKimi„ÄÅÊñáÂøÉ‰∏ÄË®Ä„ÄÅMiniMax„ÄÅË±ÜÂåÖ„ÄÅOllamaÊú¨Âú∞Â§ßÊ®°ÂûãÔºâÔºåÊîØÊåÅÁî®Êà∑Ëá™Áî±ÈÖçÁΩÆÂíåÂàáÊç¢„ÄÇ
- **ÂèØËßÜÂåñËøêË°åÊó∂ÈÖçÁΩÆ**ÔºöÊó†ÈúÄÈáçÊñ∞ÊûÑÂª∫ÈïúÂÉèÔºåÁõ¥Êé•Âú®ÊµèËßàÂô®‰∏≠ÈÖçÁΩÆÊâÄÊúâ LLM ÂèÇÊï∞Âíå API KeysÔºåÊîØÊåÅ API ‰∏≠ËΩ¨Á´ôÔºåÈÖçÁΩÆ‰øùÂ≠òÂú®Êú¨Âú∞ÊµèËßàÂô®ÔºåÂÆâÂÖ®‰æøÊç∑„ÄÇ
- **Áé∞‰ª£Âåñ„ÄÅÈ´òÈ¢úÂÄºÁöÑÁî®Êà∑ÁïåÈù¢**ÔºöÂü∫‰∫é React + TypeScript ÊûÑÂª∫ÔºåÊèê‰æõÊµÅÁïÖ„ÄÅÁõ¥ËßÇÁöÑÊìç‰Ωú‰ΩìÈ™å„ÄÇ

## üé¨ È°πÁõÆÊºîÁ§∫

### ‰∏ªË¶ÅÂäüËÉΩÁïåÈù¢

#### Êô∫ËÉΩ‰ª™Ë°®Áõò
![Êô∫ËÉΩ‰ª™Ë°®Áõò](public/images/example1.png)
*ÂÆûÊó∂Â±ïÁ§∫È°πÁõÆÁªüËÆ°„ÄÅË¥®ÈáèË∂ãÂäøÂíåÁ≥ªÁªüÊÄßËÉΩÔºåÊèê‰æõÂÖ®Èù¢ÁöÑ‰ª£Á†ÅÂÆ°ËÆ°Ê¶ÇËßà*

#### Âç≥Êó∂ÂàÜÊûê
![Âç≥Êó∂ÂàÜÊûê](public/images/example2.png)
*ÊîØÊåÅ‰ª£Á†ÅÁâáÊÆµÂø´ÈÄüÂàÜÊûêÔºåÊèê‰æõËØ¶ÁªÜÁöÑ What-Why-How Ëß£ÈáäÂíå‰øÆÂ§çÂª∫ËÆÆ*

#### È°πÁõÆÁÆ°ÁêÜ
![È°πÁõÆÁÆ°ÁêÜ](public/images/example3.png)
*ÈõÜÊàê GitHub/GitLab ‰ªìÂ∫ìÔºåÊîØÊåÅÂ§öËØ≠Ë®ÄÈ°πÁõÆÂÆ°ËÆ°ÂíåÊâπÈáè‰ª£Á†ÅÂàÜÊûê*

## üöÄ Âø´ÈÄüÂºÄÂßã

### ‚òÅÔ∏è Vercel ‰∏ÄÈîÆÈÉ®ÁΩ≤

ÈÄÇÂêàÂø´ÈÄüÈÉ®ÁΩ≤Âíå‰ΩìÈ™åÔºåÊó†ÈúÄÊúçÂä°Âô®ÔºåÂÖ®ÁêÉ CDN Âä†ÈÄü„ÄÇ

#### ÊñπÂºè‰∏ÄÔºö‰∏ÄÈîÆÈÉ®ÁΩ≤ÊåâÈíÆÔºàÊé®ËçêÔºâ‚≠ê

ÁÇπÂáª‰∏ãÊñπÊåâÈíÆÁõ¥Êé•ÈÉ®ÁΩ≤Âà∞ VercelÔºö

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/lintsinghua/XCodeReviewer)

#### ÊñπÂºè‰∫åÔºöÈÄöËøá Vercel CLI ÈÉ®ÁΩ≤

```bash
# 1. ÂÆâË£Ö Vercel CLI
npm i -g vercel

# 2. ÁôªÂΩï Vercel
vercel login

# 3. ÈÉ®ÁΩ≤È°πÁõÆ
vercel

# 4. ÈÉ®ÁΩ≤Âà∞Áîü‰∫ßÁéØÂ¢É
vercel --prod
```

#### ÊñπÂºè‰∏âÔºöÈÄöËøá Vercel Dashboard ÈÉ®ÁΩ≤

1. ËÆøÈóÆ [Vercel Dashboard](https://vercel.com/dashboard)
2. ÁÇπÂáª &quot;Add New...&quot; ‚Üí &quot;Project&quot;
3. ÂØºÂÖ•‰Ω†ÁöÑ GitHub ‰ªìÂ∫ì
4. Vercel ‰ºöËá™Âä®Ê£ÄÊµã Vite È°πÁõÆÈÖçÁΩÆ
5. ÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáèÔºàËá≥Â∞ëÈúÄË¶ÅÔºâÔºö
   ```
   VITE_LLM_PROVIDER=your_llm_provider
   VITE_LLM_API_KEY=your_api_key_here
   VITE_USE_LOCAL_DB=true
   ```
6. ÁÇπÂáª &quot;Deploy&quot;

**‚ú® Vercel ÈÉ®ÁΩ≤‰ºòÂäø**Ôºö
- ‚úÖ ÂÖ®ÁêÉ CDN Âä†ÈÄüÔºåËÆøÈóÆÈÄüÂ∫¶Âø´
- ‚úÖ Ëá™Âä® HTTPS ÂíåÂüüÂêçÈÖçÁΩÆ
- ‚úÖ Èõ∂ÈÖçÁΩÆÔºåÂºÄÁÆ±Âç≥Áî®
- ‚úÖ ÊîØÊåÅËá™ÂÆö‰πâÂüüÂêç
- ‚úÖ Ëá™Âä®ÈÉ®ÁΩ≤ÔºàGit Êé®ÈÄÅÂêéËá™Âä®Êõ¥Êñ∞Ôºâ

**‚ú® Êï∞ÊçÆÂ∫ìÊ®°Âºè**Ôºö
- ÈªòËÆ§Ëá™Âä®‰ΩøÁî®**Êú¨Âú∞Êï∞ÊçÆÂ∫ìÊ®°Âºè**ÔºàIndexedDBÔºâÔºåÊï∞ÊçÆÂ≠òÂÇ®Âú®ÊµèËßàÂô®‰∏≠
- Êó†ÈúÄÈÖçÁΩÆ‰ªª‰ΩïÊï∞ÊçÆÂ∫ìÔºåÂºÄÁÆ±Âç≥Áî®
- Â¶ÇÈúÄ‰ΩøÁî® Supabase ‰∫ëÁ´ØÊï∞ÊçÆÂ∫ìÔºåÂèØÂú®ÁéØÂ¢ÉÂèòÈáè‰∏≠ÈÖçÁΩÆ

**‚ö†Ô∏è Ê≥®ÊÑè‰∫ãÈ°π**Ôºö
- Vercel ‰∏ªË¶ÅÁî®‰∫éÂâçÁ´ØÈÉ®ÁΩ≤ÔºåÂêéÁ´Ø API ÈúÄÂçïÁã¨ÈÉ®ÁΩ≤
- ÈÉ®ÁΩ≤ÂêéÂèØÂú® `/admin` È°µÈù¢ËøõË°åËøêË°åÊó∂ÈÖçÁΩÆ

---

### üê≥ Docker ÈÉ®ÁΩ≤ÔºàÊé®ËçêÁîü‰∫ßÁéØÂ¢ÉÔºâ

#### ÊñπÂºè‰∏ÄÔºö‰ΩøÁî®ÂèëÂ∏ÉÁöÑÈïúÂÉèÔºàÊúÄÁÆÄÂçïÔºâ‚≠ê

Áõ¥Êé•‰ΩøÁî®ÊúÄÊñ∞ÂèëÂ∏ÉÁöÑ Docker ÈïúÂÉèÔºåÊîØÊåÅ x86„ÄÅARM64ÔºàMac MÁ≥ªÂàóÔºâ„ÄÅARMv7 Êû∂ÊûÑÔºö

```bash
# 1. ÊãâÂèñÊúÄÊñ∞ÁâàÊú¨ÈïúÂÉè
docker pull ghcr.io/lintsinghua/xcodereviewer:latest

# 2. ËøêË°åÂÆπÂô®
docker run -d \
  -p 8888:80 \
  --name xcodereviewer \
  --restart unless-stopped \
  ghcr.io/lintsinghua/xcodereviewer:latest

# 3. ËÆøÈóÆÂ∫îÁî®
# ÊµèËßàÂô®ÊâìÂºÄ http://localhost:8888
```

**‰ΩøÁî®ÁâπÂÆöÁâàÊú¨**Ôºö
```bash
# ÊãâÂèñÊåáÂÆöÁâàÊú¨ÔºàÂ¶Ç v1.1.0Ôºâ
docker pull ghcr.io/lintsinghua/xcodereviewer:v1.1.0

# ËøêË°å
docker run -d -p 8888:80 --name xcodereviewer ghcr.io/lintsinghua/xcodereviewer:v1.1.0
```

#### ÊñπÂºè‰∫åÔºöÊú¨Âú∞ÊûÑÂª∫ÔºàÂèØÈÄâÔºâ

Â¶ÇÊûúÈúÄË¶ÅËá™ÂÆö‰πâÊûÑÂª∫Ôºö

```bash
# 1. ÂÖãÈöÜÈ°πÁõÆ
git clone https://github.com/lintsinghua/XCodeReviewer.git
cd XCodeReviewer

# 2. ‰ΩøÁî® Docker Compose ÊûÑÂª∫Âπ∂ÂêØÂä®
docker-compose up -d

# 3. ËÆøÈóÆÂ∫îÁî®
# ÊµèËßàÂô®ÊâìÂºÄ http://localhost:8888
```

**‚ú® ËøêË°åÊó∂ÈÖçÁΩÆÔºàÊé®ËçêÔºâ**

Docker ÈÉ®ÁΩ≤ÂêéÔºåÊÇ®ÂèØ‰ª•Áõ¥Êé•Âú®ÊµèËßàÂô®‰∏≠ÈÖçÁΩÆÊâÄÊúâËÆæÁΩÆÔºåÊó†ÈúÄÈáçÊñ∞ÊûÑÂª∫ÈïúÂÉèÔºö

1. ËÆøÈóÆ `http://localhost:8888/admin`ÔºàÁ≥ªÁªüÁÆ°ÁêÜÈ°µÈù¢Ôºâ
2. Âú®&quot;Á≥ªÁªüÈÖçÁΩÆ&quot;Ê†áÁ≠æÈ°µ‰∏≠ÈÖçÁΩÆ LLM API Keys ÂíåÂÖ∂‰ªñÂèÇÊï∞
3. ÁÇπÂáª‰øùÂ≠òÂπ∂Âà∑Êñ∞È°µÈù¢Âç≥ÂèØ‰ΩøÁî®

&gt; üìñ **ËØ¶ÁªÜÈÖçÁΩÆËØ¥ÊòéËØ∑ÂèÇËÄÉ**Ôºö[Á≥ªÁªüÈÖçÁΩÆ‰ΩøÁî®ÊåáÂçó](#Á≥ªÁªüÈÖçÁΩÆÈ¶ñÊ¨°‰ΩøÁî®ÂøÖÁúã)

### üíª Êú¨Âú∞ÂºÄÂèëÈÉ®ÁΩ≤

ÈÄÇÂêàÈúÄË¶ÅÂºÄÂèëÊàñËá™ÂÆö‰πâ‰øÆÊîπÁöÑÂú∫ÊôØ„ÄÇ

#### ÁéØÂ¢ÉË¶ÅÊ±Ç
- Node.js 18+
- pnpm 8+ (Êé®Ëçê) Êàñ npm/yarn

#### Âø´ÈÄüÂêØÂä®

```bash
# 1. ÂÖãÈöÜÈ°πÁõÆ
git clone https://github.com/lintsinghua/XCodeReviewer.git
cd XCodeReviewer

# 2. ÂÆâË£Ö‰æùËµñ
pnpm install  # Êàñ npm install / yarn install

# 3. ÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáè
cp .env.example .env
# ÁºñËæë .env Êñá‰ª∂ÔºåÈÖçÁΩÆÂøÖË¶ÅÂèÇÊï∞ÔºàËßÅ‰∏ãÊñπÈÖçÁΩÆËØ¥ÊòéÔºâ

# 4. ÂêØÂä®ÂºÄÂèëÊúçÂä°Âô®
pnpm dev

# 5. ËÆøÈóÆÂ∫îÁî®
# ÊµèËßàÂô®ÊâìÂºÄ http://localhost:5173
```

#### Ê†∏ÂøÉÈÖçÁΩÆËØ¥Êòé

ÁºñËæë `.env` Êñá‰ª∂ÔºåÈÖçÁΩÆ‰ª•‰∏ãÂøÖÈúÄÂèÇÊï∞Ôºö

```env
# ========== ÂøÖÈúÄÈÖçÁΩÆ ==========
# LLM Êèê‰æõÂïÜÈÄâÊã© (gemini|openai|claude|qwen|deepseek|zhipu|moonshot|baidu|minimax|doubao|ollama)
VITE_LLM_PROVIDER=gemini
# ÂØπÂ∫îÁöÑ API Key
VITE_LLM_API_KEY=your_api_key_here

# ========== Êï∞ÊçÆÂ∫ìÈÖçÁΩÆÔºà‰∏âÈÄâ‰∏ÄÔºâ==========
# ÊñπÂºè1ÔºöÊú¨Âú∞Êï∞ÊçÆÂ∫ìÔºàÊé®ËçêÔºåÂºÄÁÆ±Âç≥Áî®Ôºâ
VITE_USE_LOCAL_DB=true

# ÊñπÂºè2ÔºöSupabase ‰∫ëÁ´ØÊï∞ÊçÆÂ∫ìÔºàÊîØÊåÅÂ§öËÆæÂ§áÂêåÊ≠•Ôºâ
# VITE_SUPABASE_URL=https://your-project.supabase.co
# VITE_SUPABASE_ANON_KEY=your_anon_key

# ÊñπÂºè3ÔºöÊºîÁ§∫Ê®°ÂºèÔºà‰∏çÈÖçÁΩÆ‰ªª‰ΩïÊï∞ÊçÆÂ∫ìÔºåÊï∞ÊçÆ‰∏çÊåÅ‰πÖÂåñÔºâ

# ========== ÂèØÈÄâÈÖçÁΩÆ ==========
# GitHub ÈõÜÊàêÔºàÁî®‰∫é‰ªìÂ∫ìÂàÜÊûêÔºâ
# VITE_GITHUB_TOKEN=your_github_token

# ËæìÂá∫ËØ≠Ë®ÄÔºàzh-CN: ‰∏≠Êñá | en-US: Ëã±ÊñáÔºâ
VITE_OUTPUT_LANGUAGE=zh-CN

# ÂàÜÊûêÂèÇÊï∞Ë∞É‰ºò
VITE_MAX_ANALYZE_FILES=40    # ÂçïÊ¨°ÊúÄÂ§ßÂàÜÊûêÊñá‰ª∂Êï∞
VITE_LLM_CONCURRENCY=2       # Âπ∂ÂèëËØ∑Ê±ÇÊï∞
VITE_LLM_GAP_MS=500          # ËØ∑Ê±ÇÈó¥Èöî(ms)
```

#### È´òÁ∫ßÈÖçÁΩÆ

ÈÅáÂà∞Ë∂ÖÊó∂ÊàñËøûÊé•ÈóÆÈ¢òÊó∂ÔºåÂèØË∞ÉÊï¥‰ª•‰∏ãÂèÇÊï∞Ôºö

```env
VITE_LLM_TIMEOUT=300000                      # Â¢ûÂä†Ë∂ÖÊó∂Êó∂Èó¥
VITE_LLM_BASE_URL=https://your-proxy.com/v1 # ‰ΩøÁî®‰ª£ÁêÜÊàñ‰∏≠ËΩ¨ÊúçÂä°
VITE_LLM_CONCURRENCY=1                       # Èôç‰ΩéÂπ∂ÂèëÊï∞
VITE_LLM_GAP_MS=1000                         # Â¢ûÂä†ËØ∑Ê±ÇÈó¥Èöî
```

**Ëá™ÂÆö‰πâËØ∑Ê±ÇÂ§¥Á§∫‰æã**ÔºàÈíàÂØπÁâπÊÆä‰∏≠ËΩ¨Á´ôÔºâÔºö

```env
# JSON Ê†ºÂºèÂ≠óÁ¨¶‰∏≤
VITE_LLM_CUSTOM_HEADERS=&#039;{&quot;X-API-Version&quot;:&quot;v1&quot;,&quot;X-Custom-Auth&quot;:&quot;token123&quot;}&#039;
```

### Â∏∏ËßÅÈóÆÈ¢ò

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Â¶Ç‰ΩïÂø´ÈÄüÂàáÊç¢ LLM Âπ≥Âè∞Ôºü&lt;/b&gt;&lt;/summary&gt;

**ÊñπÂºè‰∏ÄÔºöÊµèËßàÂô®ÈÖçÁΩÆÔºàÊé®ËçêÔºâ**

1. ËÆøÈóÆ `http://localhost:8888/admin` Á≥ªÁªüÁÆ°ÁêÜÈ°µÈù¢
2. Âú®&quot;Á≥ªÁªüÈÖçÁΩÆ&quot;Ê†áÁ≠æÈ°µÈÄâÊã©‰∏çÂêåÁöÑ LLM Êèê‰æõÂïÜ
3. Â°´ÂÖ•ÂØπÂ∫îÁöÑ API Key
4. ‰øùÂ≠òÂπ∂Âà∑Êñ∞È°µÈù¢

**ÊñπÂºè‰∫åÔºöÁéØÂ¢ÉÂèòÈáèÈÖçÁΩÆ**

‰øÆÊîπ `.env` ‰∏≠ÁöÑÈÖçÁΩÆÔºö

```env
# ÂàáÊç¢Âà∞ OpenAI
VITE_LLM_PROVIDER=openai
VITE_OPENAI_API_KEY=your_key

# ÂàáÊç¢Âà∞ÈÄö‰πâÂçÉÈóÆ
VITE_LLM_PROVIDER=qwen
VITE_QWEN_API_KEY=your_key
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;ÈÅáÂà∞ËØ∑Ê±ÇË∂ÖÊó∂ÊÄé‰πàÂäûÔºü&lt;/b&gt;&lt;/summary&gt;

1. Â¢ûÂä†Ë∂ÖÊó∂Êó∂Èó¥Ôºö`VITE_LLM_TIMEOUT=300000`
2. ‰ΩøÁî®‰ª£ÁêÜÔºöÈÖçÁΩÆ `VITE_LLM_BASE_URL`
3. ÂàáÊç¢Âà∞ÂõΩÂÜÖÂπ≥Âè∞ÔºöÈÄö‰πâÂçÉÈóÆ„ÄÅDeepSeek„ÄÅÊô∫Ë∞±AI Á≠â
4. Èôç‰ΩéÂπ∂ÂèëÔºö`VITE_LLM_CONCURRENCY=1`
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Êï∞ÊçÆÂ∫ìÊ®°ÂºèÂ¶Ç‰ΩïÈÄâÊã©Ôºü&lt;/b&gt;&lt;/summary&gt;

**Êú¨Âú∞Ê®°ÂºèÔºàÊé®ËçêÔºâ**ÔºöÊï∞ÊçÆÂ≠òÂÇ®Âú®ÊµèËßàÂô® IndexedDBÔºåÂºÄÁÆ±Âç≥Áî®ÔºåÈöêÁßÅÂÆâÂÖ®
```env
VITE_USE_LOCAL_DB=true
```

**‰∫ëÁ´ØÊ®°Âºè**ÔºöÊï∞ÊçÆÂ≠òÂÇ®Âú® SupabaseÔºåÊîØÊåÅÂ§öËÆæÂ§áÂêåÊ≠•
```env
VITE_SUPABASE_URL=https://your-project.supabase.co
VITE_SUPABASE_ANON_KEY=your_key
```

**ÊºîÁ§∫Ê®°Âºè**Ôºö‰∏çÈÖçÁΩÆ‰ªª‰ΩïÊï∞ÊçÆÂ∫ìÔºåÊï∞ÊçÆ‰∏çÊåÅ‰πÖÂåñ
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Â¶Ç‰Ωï‰ΩøÁî® Ollama Êú¨Âú∞Â§ßÊ®°ÂûãÔºü&lt;/b&gt;&lt;/summary&gt;

```bash
# 1. ÂÆâË£Ö Ollama
curl -fsSL https://ollama.com/install.sh | sh  # macOS/Linux
# Windows: ËÆøÈóÆ https://ollama.com/download

# 2. ÊãâÂèñÊ®°Âûã
ollama pull llama3  # Êàñ codellama„ÄÅqwen2.5„ÄÅdeepseek-coder

# 3. ÈÖçÁΩÆ XCodeReviewer
# Âú® .env ‰∏≠ËÆæÁΩÆÔºö
VITE_LLM_PROVIDER=ollama
VITE_LLM_MODEL=llama3
VITE_LLM_BASE_URL=http://localhost:11434/v1
```

Êé®ËçêÊ®°ÂûãÔºö`llama3`ÔºàÁªºÂêàÔºâ„ÄÅ`codellama`Ôºà‰ª£Á†Å‰∏ìÁî®Ôºâ„ÄÅ`qwen2.5`Ôºà‰∏≠ÊñáÔºâ
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;ÁôæÂ∫¶ÊñáÂøÉ‰∏ÄË®ÄÁöÑ API Key Ê†ºÂºèÔºü&lt;/b&gt;&lt;/summary&gt;

ÁôæÂ∫¶ÈúÄË¶ÅÂêåÊó∂Êèê‰æõ API Key Âíå Secret KeyÔºåÁî®ÂÜíÂè∑ÂàÜÈöîÔºö
```env
VITE_LLM_PROVIDER=baidu
VITE_BAIDU_API_KEY=your_api_key:your_secret_key
```
Ëé∑ÂèñÂú∞ÂùÄÔºöhttps://console.bce.baidu.com/qianfan/
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Â¶Ç‰Ωï‰ΩøÁî® API ‰∏≠ËΩ¨Á´ôÔºü&lt;/b&gt;&lt;/summary&gt;

ËÆ∏Â§öÁî®Êà∑‰ΩøÁî® API ‰∏≠ËΩ¨ÊúçÂä°Êù•ËÆøÈóÆ LLMÔºàÊõ¥Á®≥ÂÆö„ÄÅÊõ¥‰æøÂÆúÔºâ„ÄÇÈÖçÁΩÆÊñπÊ≥ïÔºö

1. ËÆøÈóÆÁ≥ªÁªüÁÆ°ÁêÜÈ°µÈù¢Ôºà`/admin`Ôºâ
2. Âú®&quot;Á≥ªÁªüÈÖçÁΩÆ&quot;Ê†áÁ≠æÈ°µ‰∏≠Ôºö
   - ÈÄâÊã© LLM Êèê‰æõÂïÜÔºàÂ¶Ç OpenAIÔºâ
   - **API Âü∫Á°Ä URL**: Â°´ÂÖ•‰∏≠ËΩ¨Á´ôÂú∞ÂùÄÔºàÂ¶Ç `https://your-proxy.com/v1`Ôºâ
   - **API Key**: Â°´ÂÖ•‰∏≠ËΩ¨Á´ôÊèê‰æõÁöÑÂØÜÈí•ÔºàËÄåÈùûÂÆòÊñπÂØÜÈí•Ôºâ
3. ‰øùÂ≠òÂπ∂Âà∑Êñ∞È°µÈù¢

**Ê≥®ÊÑè**Ôºö
- ‰∏≠ËΩ¨Á´ô URL ÈÄöÂ∏∏‰ª• `/v1` ÁªìÂ∞æÔºàOpenAI ÂÖºÂÆπÊ†ºÂºèÔºâ
- ‰ΩøÁî®‰∏≠ËΩ¨Á´ôÁöÑ API KeyÔºå‰∏çÊòØÂÆòÊñπÁöÑ
- Á°ÆËÆ§‰∏≠ËΩ¨Á´ôÊîØÊåÅ‰Ω†ÈÄâÊã©ÁöÑ AI Ê®°Âûã
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Â¶Ç‰ΩïÂ§á‰ªΩÊú¨Âú∞Êï∞ÊçÆÂ∫ìÔºü&lt;/b&gt;&lt;/summary&gt;

Êú¨Âú∞Êï∞ÊçÆÂ≠òÂÇ®Âú®ÊµèËßàÂô® IndexedDB ‰∏≠Ôºö
- Âú®Â∫îÁî®ÁöÑ&quot;Á≥ªÁªüÁÆ°ÁêÜ&quot;È°µÈù¢ÂØºÂá∫‰∏∫ JSON Êñá‰ª∂
- ÈÄöËøáÂØºÂÖ• JSON Êñá‰ª∂ÊÅ¢Â§çÊï∞ÊçÆ
- Ê≥®ÊÑèÔºöÊ∏ÖÈô§ÊµèËßàÂô®Êï∞ÊçÆ‰ºöÂà†Èô§ÊâÄÊúâÊú¨Âú∞Êï∞ÊçÆ
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Â¶Ç‰ΩïËÆæÁΩÆËæìÂá∫ËØ≠Ë®ÄÔºü&lt;/b&gt;&lt;/summary&gt;

```env
VITE_OUTPUT_LANGUAGE=zh-CN  # ‰∏≠ÊñáÔºàÈªòËÆ§Ôºâ
VITE_OUTPUT_LANGUAGE=en-US  # Ëã±Êñá
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Â¶Ç‰ΩïÈÖçÁΩÆÂ§ö‰∏™Âπ≥Âè∞Âπ∂Âø´ÈÄüÂàáÊç¢Ôºü&lt;/b&gt;&lt;/summary&gt;

Âú® `.env` ‰∏≠È¢ÑÈÖçÁΩÆÊâÄÊúâÂπ≥Âè∞ÁöÑ KeyÔºåÂàáÊç¢Êó∂Âè™ÈúÄ‰øÆÊîπ `VITE_LLM_PROVIDER`Ôºö
```env
VITE_LLM_PROVIDER=gemini  # ÂΩìÂâç‰ΩøÁî®ÁöÑÂπ≥Âè∞

# È¢ÑÈÖçÁΩÆÊâÄÊúâÂπ≥Âè∞
VITE_GEMINI_API_KEY=key1
VITE_OPENAI_API_KEY=key2
VITE_QWEN_API_KEY=key3
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Â¶Ç‰ΩïÊü•ÁúãÁ≥ªÁªüÊó•ÂøóÂíåË∞ÉËØï‰ø°ÊÅØÔºü&lt;/b&gt;&lt;/summary&gt;

XCodeReviewer ÂÜÖÁΩÆ‰∫ÜÊó•ÂøóÁ≥ªÁªüÔºåËÆ∞ÂΩïÊ†∏ÂøÉÊìç‰ΩúÂíåÈîôËØØÔºö

**Êü•ÁúãÊó•Âøó**Ôºö
- ÂØºËà™Ê†è -&gt; Á≥ªÁªüÊó•Âøó
- ÊàñËÆøÈóÆÔºö`http://localhost:5173/logs` (ÂºÄÂèë) / `http://localhost:8888/logs` (Áîü‰∫ß)

**ËÆ∞ÂΩïÂÜÖÂÆπ**Ôºö
- ‚úÖ Áî®Êà∑Ê†∏ÂøÉÊìç‰ΩúÔºàÂàõÂª∫È°πÁõÆ„ÄÅÂÆ°ËÆ°‰ªªÂä°„ÄÅ‰øÆÊîπÈÖçÁΩÆÁ≠âÔºâ
- ‚úÖ API ËØ∑Ê±ÇÂ§±Ë¥•ÂíåÈîôËØØ
- ‚úÖ ÊéßÂà∂Âè∞ÈîôËØØÔºàËá™Âä®ÊçïËé∑Ôºâ
- ‚úÖ Êú™Â§ÑÁêÜÁöÑÂºÇÂ∏∏

**ÂäüËÉΩÁâπÊÄß**Ôºö
- Êó•ÂøóÁ≠õÈÄâ„ÄÅÊêúÁ¥¢
- ÂØºÂá∫Êó•ÂøóÔºàJSON/CSVÔºâ
- ÈîôËØØËØ¶ÊÉÖÊü•Áúã

**ÊâãÂä®ËÆ∞ÂΩïÁî®Êà∑Êìç‰Ωú**Ôºö
```typescript
import { logger, LogCategory } from &#039;@/shared/utils/logger&#039;;

// ËÆ∞ÂΩïÁî®Êà∑Êìç‰Ωú
logger.logUserAction(&#039;ÂàõÂª∫È°πÁõÆ&#039;, { projectName, projectType });
logger.logUserAction(&#039;ÂºÄÂßãÂÆ°ËÆ°&#039;, { taskId, fileCount });
```

&lt;/details&gt;

### üîë Ëé∑Âèñ API Key

#### ÊîØÊåÅÁöÑ LLM Âπ≥Âè∞

XCodeReviewer ÊîØÊåÅ 10+ ‰∏ªÊµÅ LLM Âπ≥Âè∞ÔºåÂèØÊ†πÊçÆÈúÄÊ±ÇËá™Áî±ÈÄâÊã©Ôºö

| Âπ≥Âè∞Á±ªÂûã | Âπ≥Âè∞ÂêçÁß∞ | ÁâπÁÇπ | Ëé∑ÂèñÂú∞ÂùÄ |
|---------|---------|------|---------|
| **ÂõΩÈôÖÂπ≥Âè∞** | Google Gemini | ÂÖçË¥πÈÖçÈ¢ùÂÖÖË∂≥ÔºåÊé®Ëçê | [Ëé∑Âèñ](https://makersuite.google.com/app/apikey) |
| | OpenAI GPT | Á®≥ÂÆöÂèØÈù†ÔºåÊÄßËÉΩÊúÄ‰Ω≥ | [Ëé∑Âèñ](https://platform.openai.com/api-keys) |
| | Anthropic Claude | ‰ª£Á†ÅÁêÜËß£ËÉΩÂäõÂº∫ | [Ëé∑Âèñ](https://console.anthropic.com/) |
| | DeepSeek | ÊÄß‰ª∑ÊØîÈ´ò | [Ëé∑Âèñ](https://platform.deepseek.com/) |
| **ÂõΩÂÜÖÂπ≥Âè∞** | ÈòøÈáå‰∫ëÈÄö‰πâÂçÉÈóÆ | ÂõΩÂÜÖËÆøÈóÆÂø´ | [Ëé∑Âèñ](https://dashscope.console.aliyun.com/) |
| | Êô∫Ë∞±AI (GLM) | ‰∏≠ÊñáÊîØÊåÅÂ•Ω | [Ëé∑Âèñ](https://open.bigmodel.cn/) |
| | Êúà‰πãÊöóÈù¢ Kimi | ÈïøÊñáÊú¨Â§ÑÁêÜ | [Ëé∑Âèñ](https://platform.moonshot.cn/) |
| | ÁôæÂ∫¶ÊñáÂøÉ‰∏ÄË®Ä | ‰ºÅ‰∏öÁ∫ßÊúçÂä° | [Ëé∑Âèñ](https://console.bce.baidu.com/qianfan/) |
| | MiniMax | Â§öÊ®°ÊÄÅËÉΩÂäõ | [Ëé∑Âèñ](https://www.minimaxi.com/) |
| | Â≠óËäÇË±ÜÂåÖ | È´òÊÄß‰ª∑ÊØî | [Ëé∑Âèñ](https://console.volcengine.com/ark) |
| **Êú¨Âú∞ÈÉ®ÁΩ≤** | Ollama | ÂÆåÂÖ®Êú¨Âú∞ÂåñÔºåÈöêÁßÅÂÆâÂÖ® | [ÂÆâË£Ö](https://ollama.com/) |

#### ÈÖçÁΩÆÁ§∫‰æã

```env
# ÈÄöÁî®ÈÖçÁΩÆÔºàÊé®ËçêÔºâ
VITE_LLM_PROVIDER=gemini
VITE_LLM_API_KEY=your_api_key_here

# Êàñ‰ΩøÁî®Âπ≥Âè∞‰∏ìÁî®ÈÖçÁΩÆ
VITE_GEMINI_API_KEY=your_gemini_key
VITE_OPENAI_API_KEY=your_openai_key
# ... Êõ¥Â§öÂπ≥Âè∞ÈÖçÁΩÆËßÅ .env.example
```

#### Supabase ÈÖçÁΩÆÔºàÂèØÈÄâÔºâ

Â¶ÇÈúÄ‰∫ëÁ´ØÊï∞ÊçÆÂêåÊ≠•Ôºö
1. ËÆøÈóÆ [Supabase](https://supabase.com/) ÂàõÂª∫È°πÁõÆ
2. Ëé∑Âèñ URL ÂíåÂåøÂêçÂØÜÈí•
3. Âú® Supabase SQL ÁºñËæëÂô®ÊâßË°å `supabase/migrations/full_schema.sql`
4. Âú® `.env` ‰∏≠ÈÖçÁΩÆÁõ∏ÂÖ≥ÂèÇÊï∞

## ‚ú® Ê†∏ÂøÉÂäüËÉΩ

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;üöÄ È°πÁõÆÁÆ°ÁêÜ&lt;/b&gt;&lt;/summary&gt;

- **‰∏ÄÈîÆÈõÜÊàê‰ª£Á†Å‰ªìÂ∫ì**ÔºöÊó†ÁºùÂØπÊé• GitHub„ÄÅGitLab Á≠â‰∏ªÊµÅÂπ≥Âè∞„ÄÇ
- **Â§öËØ≠Ë®Ä‚ÄúÂÖ®ÂÆ∂Ê°∂‚ÄùÊîØÊåÅ**ÔºöË¶ÜÁõñ JavaScript, TypeScript, Python, Java, Go, Rust Á≠âÁÉ≠Èó®ËØ≠Ë®Ä„ÄÇ
- **ÁÅµÊ¥ªÁöÑÂàÜÊîØÂÆ°ËÆ°**ÔºöÊîØÊåÅÂØπÊåáÂÆö‰ª£Á†ÅÂàÜÊîØËøõË°åÁ≤æÁ°ÆÂàÜÊûê„ÄÇ
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;‚ö° Âç≥Êó∂ÂàÜÊûê&lt;/b&gt;&lt;/summary&gt;

- **‰ª£Á†ÅÁâáÊÆµ‚ÄúÈöèÊâãË¥¥‚Äù**ÔºöÁõ¥Êé•Âú® Web ÁïåÈù¢Á≤òË¥¥‰ª£Á†ÅÔºåÁ´ãÂç≥Ëé∑ÂæóÂàÜÊûêÁªìÊûú„ÄÇ
- **10+ ÁßçËØ≠Ë®ÄÂç≥Êó∂ÊîØÊåÅ**ÔºöÊª°Ë∂≥ÊÇ®Â§öÊ†∑ÂåñÁöÑ‰ª£Á†ÅÂàÜÊûêÈúÄÊ±Ç„ÄÇ
- **ÊØ´ÁßíÁ∫ßÂìçÂ∫î**ÔºöÂø´ÈÄüËé∑Âèñ‰ª£Á†ÅË¥®ÈáèËØÑÂàÜÂíå‰ºòÂåñÂª∫ËÆÆ„ÄÇ
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;üß† Êô∫ËÉΩÂÆ°ËÆ°&lt;/b&gt;&lt;/summary&gt;

- **AI Ê∑±Â∫¶‰ª£Á†ÅÁêÜËß£**ÔºöÊîØÊåÅÂ§ö‰∏™‰∏ªÊµÅ LLM Âπ≥Âè∞ÔºàGemini„ÄÅOpenAI„ÄÅClaude„ÄÅÈÄö‰πâÂçÉÈóÆ„ÄÅDeepSeek Á≠âÔºâÔºåÊèê‰æõË∂ÖË∂äÂÖ≥ÈîÆËØçÂåπÈÖçÁöÑÊô∫ËÉΩÂàÜÊûê„ÄÇ
- **‰∫îÂ§ßÊ†∏ÂøÉÁª¥Â∫¶Ê£ÄÊµã**Ôºö
  - üêõ **ÊΩúÂú® Bug**ÔºöÁ≤æÂáÜÊçïÊçâÈÄªËæëÈîôËØØ„ÄÅËæπÁïåÊù°‰ª∂ÂíåÁ©∫ÊåáÈíàÁ≠âÈóÆÈ¢ò„ÄÇ
  - üîí **ÂÆâÂÖ®ÊºèÊ¥û**ÔºöËØÜÂà´ SQL Ê≥®ÂÖ•„ÄÅXSS„ÄÅÊïèÊÑü‰ø°ÊÅØÊ≥ÑÈú≤Á≠âÂÆâÂÖ®È£éÈô©„ÄÇ
  - ‚ö° **ÊÄßËÉΩÁì∂È¢à**ÔºöÂèëÁé∞‰ΩéÊïàÁÆóÊ≥ï„ÄÅÂÜÖÂ≠òÊ≥ÑÊºèÂíå‰∏çÂêàÁêÜÁöÑÂºÇÊ≠•Êìç‰Ωú„ÄÇ
  - üé® **‰ª£Á†ÅÈ£éÊ†º**ÔºöÁ°Æ‰øù‰ª£Á†ÅÈÅµÂæ™Ë°å‰∏öÊúÄ‰Ω≥ÂÆûË∑µÂíåÁªü‰∏ÄËßÑËåÉ„ÄÇ
  - üîß **ÂèØÁª¥Êä§ÊÄß**ÔºöËØÑ‰º∞‰ª£Á†ÅÁöÑÂèØËØªÊÄß„ÄÅÂ§çÊùÇÂ∫¶ÂíåÊ®°ÂùóÂåñÁ®ãÂ∫¶„ÄÇ
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;üí° ÂèØËß£ÈáäÊÄßÂàÜÊûê (What-Why-How)&lt;/b&gt;&lt;/summary&gt;

- **What (ÊòØ‰ªÄ‰πà)**ÔºöÊ∏ÖÊô∞Âú∞ÊåáÂá∫‰ª£Á†Å‰∏≠Â≠òÂú®ÁöÑÈóÆÈ¢ò„ÄÇ
- **Why (‰∏∫‰ªÄ‰πà)**ÔºöËØ¶ÁªÜËß£ÈáäËØ•ÈóÆÈ¢òÂèØËÉΩÂ∏¶Êù•ÁöÑÊΩúÂú®È£éÈô©ÂíåÂΩ±Âìç„ÄÇ
- **How (Â¶Ç‰Ωï‰øÆÂ§ç)**ÔºöÊèê‰æõÂÖ∑‰ΩìÁöÑ„ÄÅÂèØÁõ¥Êé•‰ΩøÁî®ÁöÑ‰ª£Á†Å‰øÆÂ§çÁ§∫‰æã„ÄÇ
- **Á≤æÂáÜ‰ª£Á†ÅÂÆö‰Ωç**ÔºöÂø´ÈÄüË∑≥ËΩ¨Âà∞ÈóÆÈ¢òÊâÄÂú®ÁöÑË°åÂíåÂàó„ÄÇ
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;üìä ÂèØËßÜÂåñÊä•Âëä&lt;/b&gt;&lt;/summary&gt;

- **‰ª£Á†ÅË¥®Èáè‰ª™Ë°®Áõò**ÔºöÊèê‰æõ 0-100 ÂàÜÁöÑÁªºÂêàË¥®ÈáèËØÑ‰º∞ÔºåËÆ©‰ª£Á†ÅÂÅ•Â∫∑Áä∂ÂÜµ‰∏ÄÁõÆ‰∫ÜÁÑ∂„ÄÇ
- **Â§öÁª¥Â∫¶ÈóÆÈ¢òÁªüËÆ°**ÔºöÊåâÁ±ªÂûãÂíå‰∏•ÈáçÁ®ãÂ∫¶ÂØπÈóÆÈ¢òËøõË°åÂàÜÁ±ªÁªüËÆ°„ÄÇ
- **Ë¥®ÈáèË∂ãÂäøÂàÜÊûê**ÔºöÈÄöËøáÂõæË°®Â±ïÁ§∫‰ª£Á†ÅË¥®ÈáèÈöèÊó∂Èó¥ÁöÑÂèòÂåñË∂ãÂäø„ÄÇ
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;‚öôÔ∏è Á≥ªÁªüÁÆ°ÁêÜ&lt;/b&gt;&lt;/summary&gt;

ËÆøÈóÆ `/admin` È°µÈù¢ÔºåÊèê‰æõÂÆåÊï¥ÁöÑÁ≥ªÁªüÈÖçÁΩÆÂíåÊï∞ÊçÆÁÆ°ÁêÜÂäüËÉΩÔºö

- **üîß ÂèØËßÜÂåñÈÖçÁΩÆÁÆ°ÁêÜ**ÔºàËøêË°åÊó∂ÈÖçÁΩÆÔºâÔºö
  - üéØ **LLM ÈÖçÁΩÆ**ÔºöÂú®ÊµèËßàÂô®‰∏≠Áõ¥Êé•ÈÖçÁΩÆ API Keys„ÄÅÊ®°Âûã„ÄÅË∂ÖÊó∂Á≠âÂèÇÊï∞
  - üîë **Âπ≥Âè∞ÂØÜÈí•**ÔºöÁÆ°ÁêÜ 10+ LLM Âπ≥Âè∞ÁöÑ API KeysÔºåÊîØÊåÅÂø´ÈÄüÂàáÊç¢
  - ‚ö° **ÂàÜÊûêÂèÇÊï∞**ÔºöË∞ÉÊï¥Âπ∂ÂèëÊï∞„ÄÅÈó¥ÈöîÊó∂Èó¥„ÄÅÊúÄÂ§ßÊñá‰ª∂Êï∞Á≠â
  - üåê **API ‰∏≠ËΩ¨Á´ôÊîØÊåÅ**ÔºöËΩªÊùæÈÖçÁΩÆÁ¨¨‰∏âÊñπ API ‰ª£ÁêÜÊúçÂä°
  - üíæ **ÈÖçÁΩÆ‰ºòÂÖàÁ∫ß**ÔºöËøêË°åÊó∂ÈÖçÁΩÆ &gt; ÊûÑÂª∫Êó∂ÈÖçÁΩÆÔºåÊó†ÈúÄÈáçÊñ∞ÊûÑÂª∫ÈïúÂÉè
  
- **üíæ Êï∞ÊçÆÂ∫ìÁÆ°ÁêÜ**Ôºö
  - üè† **‰∏âÁßçÊ®°Âºè**ÔºöÊú¨Âú∞ IndexedDB / Supabase ‰∫ëÁ´Ø / ÊºîÁ§∫Ê®°Âºè
  - üì§ **ÂØºÂá∫Â§á‰ªΩ**ÔºöÂ∞ÜÊï∞ÊçÆÂØºÂá∫‰∏∫ JSON Êñá‰ª∂
  - üì• **ÂØºÂÖ•ÊÅ¢Â§ç**Ôºö‰ªéÂ§á‰ªΩÊñá‰ª∂ÊÅ¢Â§çÊï∞ÊçÆ
  - üóëÔ∏è **Ê∏ÖÁ©∫Êï∞ÊçÆ**Ôºö‰∏ÄÈîÆÊ∏ÖÁêÜÊâÄÊúâÊú¨Âú∞Êï∞ÊçÆ
  - üìä **Â≠òÂÇ®ÁõëÊéß**ÔºöÂÆûÊó∂Êü•ÁúãÂ≠òÂÇ®Á©∫Èó¥‰ΩøÁî®ÊÉÖÂÜµ
  
- **üìà Êï∞ÊçÆÊ¶ÇËßà**Ôºö
  - È°πÁõÆ„ÄÅ‰ªªÂä°„ÄÅÈóÆÈ¢òÁöÑÂÆåÊï¥ÁªüËÆ°
  - ÂèØËßÜÂåñÂõæË°®Â±ïÁ§∫Ë¥®ÈáèË∂ãÂäø
  - Â≠òÂÇ®‰ΩøÁî®ÊÉÖÂÜµÂàÜÊûê
&lt;/details&gt;

## üõ†Ô∏è ÊäÄÊúØÊ†à

| ÂàÜÁ±ª | ÊäÄÊúØ | ËØ¥Êòé |
| :--- | :--- | :--- |
| **ÂâçÁ´ØÊ°ÜÊû∂** | `React 18` `TypeScript` `Vite` | Áé∞‰ª£ÂåñÂâçÁ´ØÂºÄÂèëÊ†àÔºåÊîØÊåÅÁÉ≠ÈáçËΩΩÂíåÁ±ªÂûãÂÆâÂÖ® |
| **UI ÁªÑ‰ª∂** | `Tailwind CSS` `Radix UI` `Lucide React` | ÂìçÂ∫îÂºèËÆæËÆ°ÔºåÊó†ÈöúÁ¢çËÆøÈóÆÔºå‰∏∞ÂØåÁöÑÂõæÊ†áÂ∫ì |
| **Êï∞ÊçÆÂèØËßÜÂåñ** | `Recharts` | ‰∏ì‰∏öÁöÑÂõæË°®Â∫ìÔºåÊîØÊåÅÂ§öÁßçÂõæË°®Á±ªÂûã |
| **Ë∑ØÁî±ÁÆ°ÁêÜ** | `React Router v6` | ÂçïÈ°µÂ∫îÁî®Ë∑ØÁî±Ëß£ÂÜ≥ÊñπÊ°à |
| **Áä∂ÊÄÅÁÆ°ÁêÜ** | `React Hooks` `Sonner` | ËΩªÈáèÁ∫ßÁä∂ÊÄÅÁÆ°ÁêÜÂíåÈÄöÁü•Á≥ªÁªü |
| **AI ÂºïÊìé** | `Â§öÂπ≥Âè∞ LLM` | ÊîØÊåÅ Gemini„ÄÅOpenAI„ÄÅClaude„ÄÅÈÄö‰πâÂçÉÈóÆ„ÄÅDeepSeek Á≠â 10+ ‰∏ªÊµÅÂπ≥Âè∞ |
| **Êï∞ÊçÆÂ≠òÂÇ®** | `IndexedDB` `Supabase` `PostgreSQL` | Êú¨Âú∞Êï∞ÊçÆÂ∫ì + ‰∫ëÁ´ØÊï∞ÊçÆÂ∫ìÂèåÊ®°ÂºèÊîØÊåÅ |
| **HTTP ÂÆ¢Êà∑Á´Ø** | `Axios` `Ky` | Áé∞‰ª£ÂåñÁöÑ HTTP ËØ∑Ê±ÇÂ∫ì |
| **‰ª£Á†ÅË¥®Èáè** | `Biome` `Ast-grep` `TypeScript` | ‰ª£Á†ÅÊ†ºÂºèÂåñ„ÄÅÈùôÊÄÅÂàÜÊûêÂíåÁ±ªÂûãÊ£ÄÊü• |
| **ÊûÑÂª∫Â∑•ÂÖ∑** | `Vite` `PostCSS` `Autoprefixer` | Âø´ÈÄüÁöÑÊûÑÂª∫Â∑•ÂÖ∑Âíå CSS Â§ÑÁêÜ |

## üìÅ È°πÁõÆÁªìÊûÑ

```
XCodeReviewer/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ app/                # Â∫îÁî®ÈÖçÁΩÆ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.tsx         # ‰∏ªÂ∫îÁî®ÁªÑ‰ª∂
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.tsx        # Â∫îÁî®ÂÖ•Âè£ÁÇπ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes.tsx      # Ë∑ØÁî±ÈÖçÁΩÆ
‚îÇ   ‚îú‚îÄ‚îÄ components/         # React ÁªÑ‰ª∂
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ layout/         # Â∏ÉÂ±ÄÁªÑ‰ª∂ (Header, Footer, PageMeta)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ui/             # UI ÁªÑ‰ª∂Â∫ì (Âü∫‰∫é Radix UI)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ system/         # Á≥ªÁªüÈÖçÁΩÆÁªÑ‰ª∂
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database/       # Êï∞ÊçÆÂ∫ìÁÆ°ÁêÜÁªÑ‰ª∂
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ debug/          # Ë∞ÉËØïÁªÑ‰ª∂
‚îÇ   ‚îú‚îÄ‚îÄ pages/              # È°µÈù¢ÁªÑ‰ª∂
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dashboard.tsx   # ‰ª™Ë°®Áõò
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Projects.tsx    # È°πÁõÆÁÆ°ÁêÜ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ InstantAnalysis.tsx # Âç≥Êó∂ÂàÜÊûê
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AuditTasks.tsx  # ÂÆ°ËÆ°‰ªªÂä°
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ AdminDashboard.tsx # Á≥ªÁªüÁÆ°ÁêÜ
‚îÇ   ‚îú‚îÄ‚îÄ features/           # ÂäüËÉΩÊ®°Âùó
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analysis/       # ÂàÜÊûêÁõ∏ÂÖ≥ÊúçÂä°
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ services/   # AI ‰ª£Á†ÅÂàÜÊûêÂºïÊìé
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ projects/       # È°πÁõÆÁõ∏ÂÖ≥ÊúçÂä°
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ services/   # ‰ªìÂ∫ìÊâ´Êèè„ÄÅZIP Êñá‰ª∂Êâ´Êèè
‚îÇ   ‚îú‚îÄ‚îÄ shared/             # ÂÖ±‰∫´Â∑•ÂÖ∑
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config/         # ÈÖçÁΩÆÊñá‰ª∂
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database.ts      # Êï∞ÊçÆÂ∫ìÁªü‰∏ÄÊé•Âè£
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ localDatabase.ts # IndexedDB ÂÆûÁé∞
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ env.ts           # ÁéØÂ¢ÉÂèòÈáèÈÖçÁΩÆ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ types/          # TypeScript Á±ªÂûãÂÆö‰πâ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hooks/          # Ëá™ÂÆö‰πâ React Hooks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ utils/          # Â∑•ÂÖ∑ÂáΩÊï∞
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ initLocalDB.ts   # Êú¨Âú∞Êï∞ÊçÆÂ∫ìÂàùÂßãÂåñ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ constants/      # Â∏∏ÈáèÂÆö‰πâ
‚îÇ   ‚îî‚îÄ‚îÄ assets/             # ÈùôÊÄÅËµÑÊ∫ê
‚îÇ       ‚îî‚îÄ‚îÄ styles/         # Ê†∑ÂºèÊñá‰ª∂
‚îú‚îÄ‚îÄ supabase/
‚îÇ   ‚îî‚îÄ‚îÄ migrations/         # Êï∞ÊçÆÂ∫ìËøÅÁßªÊñá‰ª∂
‚îú‚îÄ‚îÄ public/
‚îÇ   ‚îî‚îÄ‚îÄ images/             # ÂõæÁâáËµÑÊ∫ê
‚îú‚îÄ‚îÄ scripts/                # ÊûÑÂª∫ÂíåËÆæÁΩÆËÑöÊú¨
‚îî‚îÄ‚îÄ rules/                  # ‰ª£Á†ÅËßÑÂàôÈÖçÁΩÆ
```

## üéØ ‰ΩøÁî®ÊåáÂçó

### Á≥ªÁªüÈÖçÁΩÆÔºàÈ¶ñÊ¨°‰ΩøÁî®ÂøÖÁúãÔºâ

ËÆøÈóÆ `/admin` Á≥ªÁªüÁÆ°ÁêÜÈ°µÈù¢ÔºåÂú®&quot;Á≥ªÁªüÈÖçÁΩÆ&quot;Ê†áÁ≠æÈ°µ‰∏≠ÈÖçÁΩÆÔºö

#### 1. **ÈÖçÁΩÆ LLM Êèê‰æõÂïÜ**
- ÈÄâÊã©ÊÇ®Ë¶Å‰ΩøÁî®ÁöÑ LLM Âπ≥Âè∞ÔºàGemini„ÄÅOpenAI„ÄÅClaude Á≠âÔºâ
- Â°´ÂÖ• API KeyÔºàÊîØÊåÅÈÄöÁî® Key ÊàñÂπ≥Âè∞‰∏ìÁî® KeyÔºâ
- ÂèØÈÄâÔºöÈÖçÁΩÆÊ®°ÂûãÂêçÁß∞„ÄÅAPI Âü∫Á°Ä URLÔºàÁî®‰∫é‰∏≠ËΩ¨Á´ôÔºâ

#### 2. **ÈÖçÁΩÆ API ‰∏≠ËΩ¨Á´ô**ÔºàÂ¶ÇÊûú‰ΩøÁî®Ôºâ
- Âú®&quot;API Âü∫Á°Ä URL&quot;‰∏≠Â°´ÂÖ•‰∏≠ËΩ¨Á´ôÂú∞ÂùÄÔºàÂ¶Ç `https://your-proxy.com/v1`Ôºâ
- Â°´ÂÖ•‰∏≠ËΩ¨Á´ôÊèê‰æõÁöÑ API Key
- ‰øùÂ≠òÈÖçÁΩÆ

#### 3. **Ë∞ÉÊï¥ÂàÜÊûêÂèÇÊï∞**ÔºàÂèØÈÄâÔºâ
- ÊúÄÂ§ßÂàÜÊûêÊñá‰ª∂Êï∞„ÄÅÂπ∂ÂèëËØ∑Ê±ÇÊï∞„ÄÅËØ∑Ê±ÇÈó¥Èöî
- ËæìÂá∫ËØ≠Ë®ÄÔºà‰∏≠Êñá/Ëã±ÊñáÔºâ

**ÈÖçÁΩÆÂÆåÊàêÂêéÁÇπÂáª&quot;‰øùÂ≠òÊâÄÊúâÊõ¥Êîπ&quot;Âπ∂Âà∑Êñ∞È°µÈù¢Âç≥ÂèØ‰ΩøÁî®„ÄÇ**

### Âç≥Êó∂‰ª£Á†ÅÂàÜÊûê
1. ËÆøÈóÆ `/instant-analysis` È°µÈù¢
2. ÈÄâÊã©ÁºñÁ®ãËØ≠Ë®ÄÔºàÊîØÊåÅ 10+ ÁßçËØ≠Ë®ÄÔºâ
3. Á≤òË¥¥‰ª£Á†ÅÊàñ‰∏ä‰º†Êñá‰ª∂
4. ÁÇπÂáª&quot;ÂºÄÂßãÂàÜÊûê&quot;Ëé∑Âæó AI ÂàÜÊûêÁªìÊûú
5. Êü•ÁúãËØ¶ÁªÜÁöÑÈóÆÈ¢òÊä•ÂëäÂíå‰øÆÂ§çÂª∫ËÆÆ

### È°πÁõÆÁÆ°ÁêÜ
1. ËÆøÈóÆ `/projects` È°µÈù¢
2. ÁÇπÂáª&quot;Êñ∞Âª∫È°πÁõÆ&quot;ÂàõÂª∫È°πÁõÆ
3. ÈÖçÁΩÆ‰ªìÂ∫ì URL ÂíåÊâ´ÊèèÂèÇÊï∞
4. ÂêØÂä®‰ª£Á†ÅÂÆ°ËÆ°‰ªªÂä°
5. Êü•ÁúãÂÆ°ËÆ°ÁªìÊûúÂíåÈóÆÈ¢òÁªüËÆ°

### ÂÆ°ËÆ°‰ªªÂä°
1. Âú®È°πÁõÆËØ¶ÊÉÖÈ°µÂàõÂª∫ÂÆ°ËÆ°‰ªªÂä°
2. ÈÄâÊã©Êâ´ÊèèÂàÜÊîØÂíåÊéíÈô§Ê®°Âºè
3. ÈÖçÁΩÆÂàÜÊûêÊ∑±Â∫¶ÂíåËåÉÂõ¥
4. ÁõëÊéß‰ªªÂä°ÊâßË°åÁä∂ÊÄÅ
5. Êü•ÁúãËØ¶ÁªÜÁöÑÈóÆÈ¢òÊä•Âëä

### ÂÆ°ËÆ°Êä•ÂëäÂØºÂá∫
1. Âú®‰ªªÂä°ËØ¶ÊÉÖÈ°µÁÇπÂáª&quot;ÂØºÂá∫Êä•Âëä&quot;ÊåâÈíÆ
2. ÈÄâÊã©ÂØºÂá∫Ê†ºÂºèÔºö
   - **JSON Ê†ºÂºè**ÔºöÁªìÊûÑÂåñÊï∞ÊçÆÔºåÈÄÇÂêàÁ®ãÂ∫èÂ§ÑÁêÜÂíåÈõÜÊàê
   - **PDF Ê†ºÂºè**Ôºö‰∏ì‰∏öÊä•ÂëäÔºåÈÄÇÂêàÊâìÂç∞ÂíåÂàÜ‰∫´ÔºàÈÄöËøáÊµèËßàÂô®ÊâìÂç∞ÂäüËÉΩÔºâ
3. JSON Êä•ÂëäÂåÖÂê´ÂÆåÊï¥ÁöÑ‰ªªÂä°‰ø°ÊÅØ„ÄÅÈóÆÈ¢òËØ¶ÊÉÖÂíåÁªüËÆ°Êï∞ÊçÆ
4. PDF Êä•ÂëäÊèê‰æõÁæéËßÇÁöÑÂèØËßÜÂåñÂ±ïÁ§∫ÔºåÊîØÊåÅ‰∏≠ÊñáÊòæÁ§∫
5. Êä•ÂëäÂÜÖÂÆπÂåÖÊã¨ÔºöÈ°πÁõÆ‰ø°ÊÅØ„ÄÅÂÆ°ËÆ°ÁªüËÆ°„ÄÅÈóÆÈ¢òËØ¶ÊÉÖÔºàÊåâ‰∏•ÈáçÁ®ãÂ∫¶ÂàÜÁ±ªÔºâ„ÄÅ‰øÆÂ§çÂª∫ËÆÆÁ≠â

**PDF ÂØºÂá∫ÊèêÁ§∫Ôºö**
- ÁÇπÂáª&quot;ÂØºÂá∫ PDF&quot;Âêé‰ºöÂºπÂá∫ÊµèËßàÂô®ÊâìÂç∞ÂØπËØùÊ°Ü
- Âª∫ËÆÆÂú®ÊâìÂç∞ËÆæÁΩÆ‰∏≠**ÂèñÊ∂àÂãæÈÄâ&quot;È°µÁúâÂíåÈ°µËÑö&quot;ÈÄâÈ°π**Ôºå‰ª•Ëé∑ÂæóÊõ¥Âπ≤ÂáÄÁöÑÊä•ÂëäÔºàÈÅøÂÖçÊòæÁ§∫ URL Á≠â‰ø°ÊÅØÔºâ
- Âú®ÊâìÂç∞ÂØπËØùÊ°Ü‰∏≠ÈÄâÊã©&quot;Âè¶Â≠ò‰∏∫ PDF&quot;Âç≥ÂèØ‰øùÂ≠òÊä•ÂëäÊñá‰ª∂

### ÊûÑÂª∫ÂíåÈÉ®ÁΩ≤

```bash
# ÂºÄÂèëÊ®°Âºè
pnpm dev

# ÊûÑÂª∫Áîü‰∫ßÁâàÊú¨
pnpm build

# È¢ÑËßàÊûÑÂª∫ÁªìÊûú
pnpm preview

# ‰ª£Á†ÅÊ£ÄÊü•
pnpm lint
```

### ÁéØÂ¢ÉÂèòÈáèËØ¥Êòé

#### Ê†∏ÂøÉLLMÈÖçÁΩÆ
| ÂèòÈáèÂêç | ÂøÖÈúÄ | ÈªòËÆ§ÂÄº | ËØ¥Êòé |
|--------|------|--------|------|
| `VITE_LLM_PROVIDER` | ‚úÖ | `gemini` | LLMÊèê‰æõÂïÜÔºö`gemini`\|`openai`\|`claude`\|`qwen`\|`deepseek`\|`zhipu`\|`moonshot`\|`baidu`\|`minimax`\|`doubao`\|`ollama` |
| `VITE_LLM_API_KEY` | ‚úÖ | - | ÈÄöÁî®API KeyÔºà‰ºòÂÖàÁ∫ßÈ´ò‰∫éÂπ≥Âè∞‰∏ìÁî®ÈÖçÁΩÆÔºâ |
| `VITE_LLM_MODEL` | ‚ùå | Ëá™Âä® | Ê®°ÂûãÂêçÁß∞Ôºà‰∏çÊåáÂÆöÂàô‰ΩøÁî®ÂêÑÂπ≥Âè∞ÈªòËÆ§Ê®°ÂûãÔºâ |
| `VITE_LLM_BASE_URL` | ‚ùå | - | Ëá™ÂÆö‰πâAPIÁ´ØÁÇπÔºà**ÊîØÊåÅÊâÄÊúâÂπ≥Âè∞ÁöÑ‰∏≠ËΩ¨Á´ô**„ÄÅ‰ª£ÁêÜÊàñÁßÅÊúâÈÉ®ÁΩ≤Ôºâ |
| `VITE_LLM_TIMEOUT` | ‚ùå | `150000` | ËØ∑Ê±ÇË∂ÖÊó∂Êó∂Èó¥ÔºàÊØ´ÁßíÔºâ |
| `VITE_LLM_TEMPERATURE` | ‚ùå | `0.2` | Ê∏©Â∫¶ÂèÇÊï∞Ôºà0.0-2.0ÔºâÔºåÊéßÂà∂ËæìÂá∫ÈöèÊú∫ÊÄß |
| `VITE_LLM_MAX_TOKENS` | ‚ùå | `4096` | ÊúÄÂ§ßËæìÂá∫tokenÊï∞ |
| `VITE_LLM_CUSTOM_HEADERS` | ‚ùå | - | Ëá™ÂÆö‰πâHTTPËØ∑Ê±ÇÂ§¥ÔºàJSONÊ†ºÂºèÂ≠óÁ¨¶‰∏≤ÔºâÔºåÁî®‰∫éÁâπÊÆä‰∏≠ËΩ¨Á´ôÊàñËá™Âª∫ÊúçÂä° |

&gt; üí° **API Ê†ºÂºèÊîØÊåÅ**ÔºöXCodeReviewer ÊîØÊåÅ‰∏âÁßç‰∏ªÊµÅ API Ê†ºÂºèÔºö
&gt; - **OpenAI ÂÖºÂÆπÊ†ºÂºè**ÔºàÊúÄÂ∏∏ËßÅÔºâÔºöÈÄÇÁî®‰∫éÂ§ßÂ§öÊï∞‰∏≠ËΩ¨Á´ôÂíå OpenRouter
&gt; - **Gemini Ê†ºÂºè**ÔºöGoogle Gemini ÂÆòÊñπÂèäÂÖºÂÆπÊúçÂä°
&gt; - **Claude Ê†ºÂºè**ÔºöAnthropic Claude ÂÆòÊñπÂèäÂÖºÂÆπÊúçÂä°
&gt; 
&gt; ÈÖçÁΩÆÊó∂Âè™ÈúÄÈÄâÊã©ÂØπÂ∫îÁöÑ LLM Êèê‰æõÂïÜÔºåÂ°´ÂÖ•‰∏≠ËΩ¨Á´ôÂú∞ÂùÄÂíå Key Âç≥ÂèØ„ÄÇËá™ÂÆö‰πâËØ∑Ê±ÇÂ§¥ÂäüËÉΩÂèØÊª°Ë∂≥ÁâπÊÆä‰∏≠ËΩ¨Á´ôÁöÑÈ¢ùÂ§ñË¶ÅÊ±Ç„ÄÇ

#### Âπ≥Âè∞‰∏ìÁî®API KeyÈÖçÁΩÆÔºàÂèØÈÄâÔºâ
| ÂèòÈáèÂêç | ËØ¥Êòé | ÁâπÊÆäË¶ÅÊ±Ç |
|--------|------|---------|
| `VITE_GEMINI_API_KEY` | Google Gemini API Key | - |
| `VITE_GEMINI_MODEL` | GeminiÊ®°Âûã (ÈªòËÆ§: gemini-1.5-flash) | - |
| `VITE_OPENAI_API_KEY` | OpenAI API Key | - |
| `VITE_OPENAI_MODEL` | OpenAIÊ®°Âûã (ÈªòËÆ§: gpt-4o-mini) | - |
| `VITE_OPENAI_BASE_URL` | OpenAIËá™ÂÆö‰πâÁ´ØÁÇπ | Áî®‰∫é‰∏≠ËΩ¨ÊúçÂä° |
| `VITE_CLAUDE_API_KEY` | Anthropic Claude API Key | - |
| `VITE_CLAUDE_MODEL` | ClaudeÊ®°Âûã (ÈªòËÆ§: claude-3-5-sonnet-20241022) | - |
| `VITE_QWEN_API_KEY` | ÈòøÈáå‰∫ëÈÄö‰πâÂçÉÈóÆ API Key | - |
| `VITE_QWEN_MODEL` | ÈÄö‰πâÂçÉÈóÆÊ®°Âûã (ÈªòËÆ§: qwen-turbo) | - |
| `VITE_DEEPSEEK_API_KEY` | DeepSeek API Key | - |
| `VITE_DEEPSEEK_MODEL` | DeepSeekÊ®°Âûã (ÈªòËÆ§: deepseek-chat) | - |
| `VITE_ZHIPU_API_KEY` | Êô∫Ë∞±AI API Key | - |
| `VITE_ZHIPU_MODEL` | Êô∫Ë∞±Ê®°Âûã (ÈªòËÆ§: glm-4-flash) | - |
| `VITE_MOONSHOT_API_KEY` | Êúà‰πãÊöóÈù¢ Kimi API Key | - |
| `VITE_MOONSHOT_MODEL` | KimiÊ®°Âûã (ÈªòËÆ§: moonshot-v1-8k) | - |
| `VITE_BAIDU_API_KEY` | ÁôæÂ∫¶ÊñáÂøÉ‰∏ÄË®Ä API Key | ‚ö†Ô∏è Ê†ºÂºè: `API_KEY:SECRET_KEY` |
| `VITE_BAIDU_MODEL` | ÊñáÂøÉÊ®°Âûã (ÈªòËÆ§: ERNIE-3.5-8K) | - |
| `VITE_MINIMAX_API_KEY` | MiniMax API Key | - |
| `VITE_MINIMAX_MODEL` | MiniMaxÊ®°Âûã (ÈªòËÆ§: abab6.5-chat) | - |
| `VITE_DOUBAO_API_KEY` | Â≠óËäÇË±ÜÂåÖ API Key | - |
| `VITE_DOUBAO_MODEL` | Ë±ÜÂåÖÊ®°Âûã (ÈªòËÆ§: doubao-pro-32k) | - |

#### Êï∞ÊçÆÂ∫ìÈÖçÁΩÆÔºàÂèØÈÄâÔºâ
| ÂèòÈáèÂêç | ÂøÖÈúÄ | ËØ¥Êòé |
|--------|------|------|
| `VITE_SUPABASE_URL` | ‚ùå | SupabaseÈ°πÁõÆURLÔºàÁî®‰∫éÊï∞ÊçÆÊåÅ‰πÖÂåñÔºâ |
| `VITE_SUPABASE_ANON_KEY` | ‚ùå | SupabaseÂåøÂêçÂØÜÈí• |

&gt; üí° **ÊèêÁ§∫**Ôºö‰∏çÈÖçÁΩÆSupabaseÊó∂ÔºåÁ≥ªÁªü‰ª•ÊºîÁ§∫Ê®°ÂºèËøêË°åÔºåÊï∞ÊçÆ‰∏çÊåÅ‰πÖÂåñ

#### Git‰ªìÂ∫ìÈõÜÊàêÈÖçÁΩÆ
| ÂèòÈáèÂêç | ÂøÖÈúÄ | ËØ¥Êòé |
|--------|------|------|
| `VITE_GITHUB_TOKEN` | ‚úÖ | GitHub Personal Access Token |
| `VITE_GITLAB_TOKEN` | ‚úÖ | GitLab Personal Access Token Êàñ Project Access Token |

#### ÂàÜÊûêË°å‰∏∫ÈÖçÁΩÆ
| ÂèòÈáèÂêç | ÈªòËÆ§ÂÄº | ËØ¥Êòé |
|--------|--------|------|
| `VITE_MAX_ANALYZE_FILES` | `40` | ÂçïÊ¨°ÂàÜÊûêÁöÑÊúÄÂ§ßÊñá‰ª∂Êï∞ |
| `VITE_LLM_CONCURRENCY` | `2` | LLMÂπ∂ÂèëËØ∑Ê±ÇÊï∞ÔºàÈôç‰ΩéÂèØÈÅøÂÖçÈ¢ëÁéáÈôêÂà∂Ôºâ |
| `VITE_LLM_GAP_MS` | `500` | LLMËØ∑Ê±ÇÈó¥ÈöîÔºàÊØ´ÁßíÔºåÂ¢ûÂä†ÂèØÈÅøÂÖçÈ¢ëÁéáÈôêÂà∂Ôºâ |

#### Â∫îÁî®ÈÖçÁΩÆ
| ÂèòÈáèÂêç | ÈªòËÆ§ÂÄº | ËØ¥Êòé |
|--------|--------|------|
| `VITE_APP_ID` | `xcodereviewer` | Â∫îÁî®Ê†áËØÜÁ¨¶ |

## ü§ù Ë¥°ÁåÆÊåáÂçó

Êàë‰ª¨ÁÉ≠ÁÉàÊ¨¢ËøéÊâÄÊúâÂΩ¢ÂºèÁöÑË¥°ÁåÆÔºÅÊó†ËÆ∫ÊòØÊèê‰∫§ issue„ÄÅÂàõÂª∫ PRÔºåËøòÊòØÊîπËøõÊñáÊ°£ÔºåÊÇ®ÁöÑÊØè‰∏ÄÊ¨°Ë¥°ÁåÆÂØπÊàë‰ª¨ÈÉΩËá≥ÂÖ≥ÈáçË¶Å„ÄÇËØ∑ËÅîÁ≥ªÊàë‰ª¨‰∫ÜËß£ËØ¶ÁªÜ‰ø°ÊÅØ„ÄÇ

### ÂºÄÂèëÊµÅÁ®ã

1.  **Fork** Êú¨È°πÁõÆ
2.  ÂàõÂª∫ÊÇ®ÁöÑÂäüËÉΩÂàÜÊîØ (`git checkout -b feature/AmazingFeature`)
3.  Êèê‰∫§ÊÇ®ÁöÑÊõ¥Êîπ (`git commit -m &#039;Add some AmazingFeature&#039;`)
4.  Êé®ÈÄÅÂà∞ÂàÜÊîØ (`git push origin feature/AmazingFeature`)
5.  ÂàõÂª∫‰∏Ä‰∏™ **Pull Request**

## üôè Ëá¥Ë∞¢

### Ê†∏ÂøÉÊäÄÊúØÊîØÊåÅ
- **[React](https://reactjs.org/)** &amp; **[Vite](https://vitejs.dev/)**: Êèê‰æõÁé∞‰ª£ÂåñÁöÑÂâçÁ´ØÂºÄÂèë‰ΩìÈ™å
- **[TypeScript](https://www.typescriptlang.org/)**: Êèê‰æõÁ±ªÂûãÂÆâÂÖ®‰øùÈöú
- **[Tailwind CSS](https://tailwindcss.com/)**: Êèê‰æõÁé∞‰ª£ÂåñÁöÑ CSS Ê°ÜÊû∂
- **[Radix UI](https://www.radix-ui.com/)**: Êèê‰æõÊó†ÈöúÁ¢çÁöÑ UI ÁªÑ‰ª∂Â∫ì

### AI Âπ≥Âè∞ÊîØÊåÅ
- **[Google Gemini AI](https://ai.google.dev/)**: Êèê‰æõÂº∫Â§ßÁöÑ AI ÂàÜÊûêËÉΩÂäõ
- **[OpenAI](https://openai.com/)**: GPTÁ≥ªÂàóÊ®°ÂûãÊîØÊåÅ
- **[Anthropic Claude](https://www.anthropic.com/)**: ClaudeÊ®°ÂûãÊîØÊåÅ
- **[DeepSeek](https://www.deepseek.com/)**: ÂõΩ‰∫ßAIÂ§ßÊ®°ÂûãÊîØÊåÅ
- **[ÈòøÈáå‰∫ëÈÄö‰πâÂçÉÈóÆ](https://tongyi.aliyun.com/)**: ‰ºÅ‰∏öÁ∫ßAIÊúçÂä°
- **[Êô∫Ë∞±AI](https://www.zhipuai.cn/)**: GLMÁ≥ªÂàóÊ®°Âûã
- **[Moonshot AI](https://www.moonshot.cn/)**: KimiÊ®°ÂûãÊîØÊåÅ
- **[Ollama](https://ollama.com/)**: Êú¨Âú∞Ê®°ÂûãÈÉ®ÁΩ≤ÊñπÊ°à

### Êï∞ÊçÆÂ≠òÂÇ®
- **[Supabase](https://supabase.com/)**: Êèê‰æõ‰æøÊç∑ÁöÑÂêéÁ´ØÂç≥ÊúçÂä°ÊîØÊåÅ
- **[IndexedDB](https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API)**: ÊµèËßàÂô®Êú¨Âú∞Â≠òÂÇ®ÊñπÊ°à

### ÂäüËÉΩÁªÑ‰ª∂
- **[Recharts](https://recharts.org/)**: Êèê‰æõ‰∏ì‰∏öÁöÑÂõæË°®ÁªÑ‰ª∂
- **[Lucide Icons](https://lucide.dev/)**: Êèê‰æõÁ≤æÁæéÁöÑÂõæÊ†áÂ∫ì
- **[Sonner](https://sonner.emilkowal.ski/)**: Êèê‰æõ‰ºòÈõÖÁöÑÈÄöÁü•ÁªÑ‰ª∂
- **[fflate](https://github.com/101arrowz/fflate)**: ZIPÊñá‰ª∂Â§ÑÁêÜ

### ÁâπÂà´ÊÑüË∞¢
- ÊÑüË∞¢ÊâÄÊúâ‰∏∫Êú¨È°πÁõÆÊèê‰∫§ Issue Âíå Pull Request ÁöÑË¥°ÁåÆËÄÖ
- ÊÑüË∞¢ÊâÄÊúâ Star Êú¨È°πÁõÆÁöÑÂºÄÂèëËÄÖ
- ÊÑüË∞¢ÂºÄÊ∫êÁ§æÂå∫ÁöÑÊó†ÁßÅÂàÜ‰∫´Á≤æÁ•û
- ‰ª•ÂèäÊâÄÊúâÊú¨È°πÁõÆÊâÄ‰ΩøÁî®ÁöÑÂºÄÊ∫êËΩØ‰ª∂ÁöÑ‰ΩúËÄÖ‰ª¨ÔºÅ

## üë• Ë¥°ÁåÆËÄÖ

ÊÑüË∞¢‰ª•‰∏ã‰ºòÁßÄÁöÑË¥°ÁåÆËÄÖ‰ª¨Ôºå‰ªñ‰ª¨ËÆ© XCodeReviewer Êõ¥Âº∫Â§ßÔºÅ

[![Contributors](https://contrib.rocks/image?re

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[andrewyng/aisuite]]></title>
            <link>https://github.com/andrewyng/aisuite</link>
            <guid>https://github.com/andrewyng/aisuite</guid>
            <pubDate>Tue, 23 Dec 2025 00:04:33 GMT</pubDate>
            <description><![CDATA[Simple, unified interface to multiple Generative AI providers]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/andrewyng/aisuite">andrewyng/aisuite</a></h1>
            <p>Simple, unified interface to multiple Generative AI providers</p>
            <p>Language: Python</p>
            <p>Stars: 13,233</p>
            <p>Forks: 1,354</p>
            <p>Stars today: 74 stars today</p>
            <h2>README</h2><pre>#  aisuite

[![PyPI](https://img.shields.io/pypi/v/aisuite)](https://pypi.org/project/aisuite/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

`aisuite` is a lightweight Python library that provides a **unified API for working with multiple Generative AI providers**.  
It offers a consistent interface for models from *OpenAI, Anthropic, Google, Hugging Face, AWS, Cohere, Mistral, Ollama*, and others‚Äîabstracting away SDK differences, authentication details, and parameter variations.  
Its design is modeled after OpenAI‚Äôs API style, making it instantly familiar and easy to adopt.

`aisuite` lets developers build and **run LLM-based or agentic applications across providers** with minimal setup.  
While it‚Äôs not a full-blown agents framework, it includes simple abstractions for creating standalone, lightweight agents.  
It‚Äôs designed for low learning curve ‚Äî so you can focus on building AI systems, not integrating APIs.

---

## Key Features

`aisuite` is designed to eliminate the complexity of working with multiple LLM providers while keeping your code simple and portable. Whether you&#039;re building a chatbot, an agentic application, or experimenting with different models, `aisuite` provides the abstractions you need without getting in your way.

* **Unified API for multiple model providers** ‚Äì Write your code once and run it with any supported provider. Switch between OpenAI, Anthropic, Google, and others with a single parameter change.
* **Easy agentic app or agent creation** ‚Äì Build multi-turn agentic applications using a single parameter `max_turns`. No need to manually manage tool execution loops.
* **Pass Tool calls easily** ‚Äì Pass real Python functions instead of JSON specs; aisuite handles schema generation and execution automatically.
* **MCP tools** ‚Äì Connect to MCP-based tools without writing boilerplate; aisuite handles connection, schema and execution seamlessly.
* **Modular and extensible provider architecture** ‚Äì Add support for new providers with minimal code. The plugin-style architecture makes extensions straightforward.

---

## Installation

You can install just the base `aisuite` package, or install a provider&#039;s package along with `aisuite`.

Install just the base package without any provider SDKs:

```shell
pip install aisuite
```

Install aisuite with a specific provider (e.g., Anthropic):

```shell
pip install &#039;aisuite[anthropic]&#039;
```

Install aisuite with all provider libraries:

```shell
pip install &#039;aisuite[all]&#039;
```

## Setup

To get started, you will need API Keys for the providers you intend to use. You&#039;ll need to
install the provider-specific library either separately or when installing aisuite.

The API Keys can be set as environment variables, or can be passed as config to the aisuite Client constructor.
You can use tools like [`python-dotenv`](https://pypi.org/project/python-dotenv/) or [`direnv`](https://direnv.net/) to set the environment variables manually. Please take a look at the `examples` folder to see usage.

Here is a short example of using `aisuite` to generate chat completion responses from gpt-4o and claude-3-5-sonnet.

Set the API keys.

```shell
export OPENAI_API_KEY=&quot;your-openai-api-key&quot;
export ANTHROPIC_API_KEY=&quot;your-anthropic-api-key&quot;
```

Use the python client.

```python
import aisuite as ai
client = ai.Client()

models = [&quot;openai:gpt-4o&quot;, &quot;anthropic:claude-3-5-sonnet-20240620&quot;]

messages = [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Respond in Pirate English.&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Tell me a joke.&quot;},
]

for model in models:
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.75
    )
    print(response.choices[0].message.content)

```

Note that the model name in the create() call uses the format - `&lt;provider&gt;:&lt;model-name&gt;`.
`aisuite` will call the appropriate provider with the right parameters based on the provider value.
For a list of provider values, you can look at the directory - `aisuite/providers/`. The list of supported providers are of the format - `&lt;provider&gt;_provider.py` in that directory. We welcome providers to add support to this library by adding an implementation file in this directory. Please see section below for how to contribute.

For more examples, check out the `examples` directory where you will find several notebooks that you can run to experiment with the interface.

---

## Chat Completions

The chat API provides a high-level abstraction for model interactions. It supports all core parameters (`temperature`, `max_tokens`, `tools`, etc.) in a provider-agnostic way.

```python
response = client.chat.completions.create(
    model=&quot;google:gemini-pro&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Summarize this paragraph.&quot;}],
)
print(response.choices[0].message.content)
```

`aisuite` standardizes request and response structures so you can focus on logic rather than SDK differences.

---

## Tool Calling &amp; Agentic apps

`aisuite` provides a simple abstraction for tool/function calling that works across supported providers. This is in addition to the regular abstraction of passing JSON spec of the tool to the `tools` parameter. The tool calling abstraction makes it easy to use tools with different LLMs without changing your code.

There are two ways to use tools with `aisuite`:

### 1. Manual Tool Handling

This is the default behavior when `max_turns` is not specified. In this mode, you have full control over the tool execution flow. You pass tools using the standard OpenAI JSON schema format, and `aisuite` returns the LLM&#039;s tool call requests in the response. You&#039;re then responsible for executing the tools, processing results, and sending them back to the model in subsequent requests.

This approach is useful when you need:
- Fine-grained control over tool execution logic
- Custom error handling or validation before executing tools
- The ability to selectively execute or skip certain tool calls
- Integration with existing tool execution pipelines

You can pass tools in the OpenAI tool format:

```python
def will_it_rain(location: str, time_of_day: str):
    &quot;&quot;&quot;Check if it will rain in a location at a given time today.
    
    Args:
        location (str): Name of the city
        time_of_day (str): Time of the day in HH:MM format.
    &quot;&quot;&quot;
    return &quot;YES&quot;

tools = [{
    &quot;type&quot;: &quot;function&quot;,
    &quot;function&quot;: {
        &quot;name&quot;: &quot;will_it_rain&quot;,
        &quot;description&quot;: &quot;Check if it will rain in a location at a given time today&quot;,
        &quot;parameters&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {
                &quot;location&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;Name of the city&quot;
                },
                &quot;time_of_day&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;Time of the day in HH:MM format.&quot;
                }
            },
            &quot;required&quot;: [&quot;location&quot;, &quot;time_of_day&quot;]
        }
    }
}]

response = client.chat.completions.create(
    model=&quot;openai:gpt-4o&quot;,
    messages=messages,
    tools=tools
)
```

### 2. Automatic Tool Execution

When `max_turns` is specified, you can pass a list of callable Python functions as the `tools` parameter. `aisuite` will automatically handle the tool calling flow:

```python
def will_it_rain(location: str, time_of_day: str):
    &quot;&quot;&quot;Check if it will rain in a location at a given time today.
    
    Args:
        location (str): Name of the city
        time_of_day (str): Time of the day in HH:MM format.
    &quot;&quot;&quot;
    return &quot;YES&quot;

client = ai.Client()
messages = [{
    &quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: &quot;I live in San Francisco. Can you check for weather &quot;
               &quot;and plan an outdoor picnic for me at 2pm?&quot;
}]

# Automatic tool execution with max_turns
response = client.chat.completions.create(
    model=&quot;openai:gpt-4o&quot;,
    messages=messages,
    tools=[will_it_rain],
    max_turns=2  # Maximum number of back-and-forth tool calls
)
print(response.choices[0].message.content)
```

When `max_turns` is specified, `aisuite` will:
1. Send your message to the LLM
2. Execute any tool calls the LLM requests
3. Send the tool results back to the LLM
4. Repeat until the conversation is complete or max_turns is reached

In addition to `response.choices[0].message`, there is an additional field `response.choices[0].intermediate_messages` which contains the list of all messages including tool interactions used. This can be used to continue the conversation with the model.
For more detailed examples of tool calling, check out the `examples/tool_calling_abstraction.ipynb` notebook.

### Model Context Protocol (MCP) Integration

`aisuite` natively supports **MCP**, a standard protocol that allows LLMs to securely call external tools and access data. You can connect to MCP servers‚Äîsuch as a filesystem or database‚Äîand expose their tools directly to your model.
Read more about MCP here - https://modelcontextprotocol.io/docs/getting-started/intro

Install aisuite with MCP support:

```shell
pip install &#039;aisuite[mcp]&#039;
```

You&#039;ll also need an MCP server. For example, to use the filesystem server:

```shell
npm install -g @modelcontextprotocol/server-filesystem
```

There are two ways to use MCP tools with aisuite:

#### Option 1: Config Dict Format (Recommended for Simple Use Cases)

```python
import aisuite as ai

client = ai.Client()
response = client.chat.completions.create(
    model=&quot;openai:gpt-4o&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;List the files in the current directory&quot;}],
    tools=[{
        &quot;type&quot;: &quot;mcp&quot;,
        &quot;name&quot;: &quot;filesystem&quot;,
        &quot;command&quot;: &quot;npx&quot;,
        &quot;args&quot;: [&quot;-y&quot;, &quot;@modelcontextprotocol/server-filesystem&quot;, &quot;/path/to/directory&quot;]
    }],
    max_turns=3
)

print(response.choices[0].message.content)
```

#### Option 2: Explicit MCPClient (Recommended for Advanced Use Cases)

```python
import aisuite as ai
from aisuite.mcp import MCPClient

# Create MCP client once, reuse across requests
mcp = MCPClient(
    command=&quot;npx&quot;,
    args=[&quot;-y&quot;, &quot;@modelcontextprotocol/server-filesystem&quot;, &quot;/path/to/directory&quot;]
)

# Use with aisuite
client = ai.Client()
response = client.chat.completions.create(
    model=&quot;openai:gpt-4o&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;List the files&quot;}],
    tools=mcp.get_callable_tools(),
    max_turns=3
)

print(response.choices[0].message.content)
mcp.close()  # Clean up
```

For detailed usage (security filters, tool prefixing, and `MCPClient` management), see [docs/mcp-tools.md](docs/mcp-tools.md).
For detailed examples, see `examples/mcp_tools_example.ipynb`.

---

## Extending aisuite: Adding a Provider

New providers can be added by implementing a lightweight adapter. The system uses a naming convention for discovery:

| Element         | Convention                         |
| --------------- | ---------------------------------- |
| **Module file** | `&lt;provider&gt;_provider.py`           |
| **Class name**  | `&lt;Provider&gt;Provider` (capitalized) |

Example:

```python
# providers/openai_provider.py
class OpenaiProvider(BaseProvider):
    ...
```

This convention ensures consistency and enables automatic loading of new integrations.

---

## Contributing

Contributions are welcome. Please review the [Contributing Guide](https://github.com/andrewyng/aisuite/blob/main/CONTRIBUTING.md) and join our [Discord](https://discord.gg/T6Nvn8ExSb) for discussions.

---

## License

Released under the **MIT License** ‚Äî free for commercial and non-commercial use.

---
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[AUTOMATIC1111/stable-diffusion-webui]]></title>
            <link>https://github.com/AUTOMATIC1111/stable-diffusion-webui</link>
            <guid>https://github.com/AUTOMATIC1111/stable-diffusion-webui</guid>
            <pubDate>Tue, 23 Dec 2025 00:04:32 GMT</pubDate>
            <description><![CDATA[Stable Diffusion web UI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">AUTOMATIC1111/stable-diffusion-webui</a></h1>
            <p>Stable Diffusion web UI</p>
            <p>Language: Python</p>
            <p>Stars: 159,174</p>
            <p>Forks: 29,591</p>
            <p>Stars today: 35 stars today</p>
            <h2>README</h2><pre># Stable Diffusion web UI
A web interface for Stable Diffusion, implemented using Gradio library.

![](screenshot.png)

## Features
[Detailed feature showcase with images](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features):
- Original txt2img and img2img modes
- One click install and run script (but you still must install python and git)
- Outpainting
- Inpainting
- Color Sketch
- Prompt Matrix
- Stable Diffusion Upscale
- Attention, specify parts of text that the model should pay more attention to
    - a man in a `((tuxedo))` - will pay more attention to tuxedo
    - a man in a `(tuxedo:1.21)` - alternative syntax
    - select text and press `Ctrl+Up` or `Ctrl+Down` (or `Command+Up` or `Command+Down` if you&#039;re on a MacOS) to automatically adjust attention to selected text (code contributed by anonymous user)
- Loopback, run img2img processing multiple times
- X/Y/Z plot, a way to draw a 3 dimensional plot of images with different parameters
- Textual Inversion
    - have as many embeddings as you want and use any names you like for them
    - use multiple embeddings with different numbers of vectors per token
    - works with half precision floating point numbers
    - train embeddings on 8GB (also reports of 6GB working)
- Extras tab with:
    - GFPGAN, neural network that fixes faces
    - CodeFormer, face restoration tool as an alternative to GFPGAN
    - RealESRGAN, neural network upscaler
    - ESRGAN, neural network upscaler with a lot of third party models
    - SwinIR and Swin2SR ([see here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/2092)), neural network upscalers
    - LDSR, Latent diffusion super resolution upscaling
- Resizing aspect ratio options
- Sampling method selection
    - Adjust sampler eta values (noise multiplier)
    - More advanced noise setting options
- Interrupt processing at any time
- 4GB video card support (also reports of 2GB working)
- Correct seeds for batches
- Live prompt token length validation
- Generation parameters
     - parameters you used to generate images are saved with that image
     - in PNG chunks for PNG, in EXIF for JPEG
     - can drag the image to PNG info tab to restore generation parameters and automatically copy them into UI
     - can be disabled in settings
     - drag and drop an image/text-parameters to promptbox
- Read Generation Parameters Button, loads parameters in promptbox to UI
- Settings page
- Running arbitrary python code from UI (must run with `--allow-code` to enable)
- Mouseover hints for most UI elements
- Possible to change defaults/mix/max/step values for UI elements via text config
- Tiling support, a checkbox to create images that can be tiled like textures
- Progress bar and live image generation preview
    - Can use a separate neural network to produce previews with almost none VRAM or compute requirement
- Negative prompt, an extra text field that allows you to list what you don&#039;t want to see in generated image
- Styles, a way to save part of prompt and easily apply them via dropdown later
- Variations, a way to generate same image but with tiny differences
- Seed resizing, a way to generate same image but at slightly different resolution
- CLIP interrogator, a button that tries to guess prompt from an image
- Prompt Editing, a way to change prompt mid-generation, say to start making a watermelon and switch to anime girl midway
- Batch Processing, process a group of files using img2img
- Img2img Alternative, reverse Euler method of cross attention control
- Highres Fix, a convenience option to produce high resolution pictures in one click without usual distortions
- Reloading checkpoints on the fly
- Checkpoint Merger, a tab that allows you to merge up to 3 checkpoints into one
- [Custom scripts](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Custom-Scripts) with many extensions from community
- [Composable-Diffusion](https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/), a way to use multiple prompts at once
     - separate prompts using uppercase `AND`
     - also supports weights for prompts: `a cat :1.2 AND a dog AND a penguin :2.2`
- No token limit for prompts (original stable diffusion lets you use up to 75 tokens)
- DeepDanbooru integration, creates danbooru style tags for anime prompts
- [xformers](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers), major speed increase for select cards: (add `--xformers` to commandline args)
- via extension: [History tab](https://github.com/yfszzx/stable-diffusion-webui-images-browser): view, direct and delete images conveniently within the UI
- Generate forever option
- Training tab
     - hypernetworks and embeddings options
     - Preprocessing images: cropping, mirroring, autotagging using BLIP or deepdanbooru (for anime)
- Clip skip
- Hypernetworks
- Loras (same as Hypernetworks but more pretty)
- A separate UI where you can choose, with preview, which embeddings, hypernetworks or Loras to add to your prompt
- Can select to load a different VAE from settings screen
- Estimated completion time in progress bar
- API
- Support for dedicated [inpainting model](https://github.com/runwayml/stable-diffusion#inpainting-with-stable-diffusion) by RunwayML
- via extension: [Aesthetic Gradients](https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients), a way to generate images with a specific aesthetic by using clip images embeds (implementation of [https://github.com/vicgalle/stable-diffusion-aesthetic-gradients](https://github.com/vicgalle/stable-diffusion-aesthetic-gradients))
- [Stable Diffusion 2.0](https://github.com/Stability-AI/stablediffusion) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#stable-diffusion-20) for instructions
- [Alt-Diffusion](https://arxiv.org/abs/2211.06679) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#alt-diffusion) for instructions
- Now without any bad letters!
- Load checkpoints in safetensors format
- Eased resolution restriction: generated image&#039;s dimensions must be a multiple of 8 rather than 64
- Now with a license!
- Reorder elements in the UI from settings screen
- [Segmind Stable Diffusion](https://huggingface.co/segmind/SSD-1B) support

## Installation and Running
Make sure the required [dependencies](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Dependencies) are met and follow the instructions available for:
- [NVidia](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs) (recommended)
- [AMD](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-AMD-GPUs) GPUs.
- [Intel CPUs, Intel GPUs (both integrated and discrete)](https://github.com/openvinotoolkit/stable-diffusion-webui/wiki/Installation-on-Intel-Silicon) (external wiki page)
- [Ascend NPUs](https://github.com/wangshuai09/stable-diffusion-webui/wiki/Install-and-run-on-Ascend-NPUs) (external wiki page)

Alternatively, use online services (like Google Colab):

- [List of Online Services](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services)

### Installation on Windows 10/11 with NVidia-GPUs using release package
1. Download `sd.webui.zip` from [v1.0.0-pre](https://github.com/AUTOMATIC1111/stable-diffusion-webui/releases/tag/v1.0.0-pre) and extract its contents.
2. Run `update.bat`.
3. Run `run.bat`.
&gt; For more details see [Install-and-Run-on-NVidia-GPUs](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs)

### Automatic Installation on Windows
1. Install [Python 3.10.6](https://www.python.org/downloads/release/python-3106/) (Newer version of Python does not support torch), checking &quot;Add Python to PATH&quot;.
2. Install [git](https://git-scm.com/download/win).
3. Download the stable-diffusion-webui repository, for example by running `git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git`.
4. Run `webui-user.bat` from Windows Explorer as normal, non-administrator, user.

### Automatic Installation on Linux
1. Install the dependencies:
```bash
# Debian-based:
sudo apt install wget git python3 python3-venv libgl1 libglib2.0-0
# Red Hat-based:
sudo dnf install wget git python3 gperftools-libs libglvnd-glx
# openSUSE-based:
sudo zypper install wget git python3 libtcmalloc4 libglvnd
# Arch-based:
sudo pacman -S wget git python3
```
If your system is very new, you need to install python3.11 or python3.10:
```bash
# Ubuntu 24.04
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt update
sudo apt install python3.11

# Manjaro/Arch
sudo pacman -S yay
yay -S python311 # do not confuse with python3.11 package

# Only for 3.11
# Then set up env variable in launch script
export python_cmd=&quot;python3.11&quot;
# or in webui-user.sh
python_cmd=&quot;python3.11&quot;
```
2. Navigate to the directory you would like the webui to be installed and execute the following command:
```bash
wget -q https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh
```
Or just clone the repo wherever you want:
```bash
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui
```

3. Run `webui.sh`.
4. Check `webui-user.sh` for options.
### Installation on Apple Silicon

Find the instructions [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Installation-on-Apple-Silicon).

## Contributing
Here&#039;s how to add code to this repo: [Contributing](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)

## Documentation

The documentation was moved from this README over to the project&#039;s [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki).

For the purposes of getting Google and other search engines to crawl the wiki, here&#039;s a link to the (not for humans) [crawlable wiki](https://github-wiki-see.page/m/AUTOMATIC1111/stable-diffusion-webui/wiki).

## Credits
Licenses for borrowed code can be found in `Settings -&gt; Licenses` screen, and also in `html/licenses.html` file.

- Stable Diffusion - https://github.com/Stability-AI/stablediffusion, https://github.com/CompVis/taming-transformers, https://github.com/mcmonkey4eva/sd3-ref
- k-diffusion - https://github.com/crowsonkb/k-diffusion.git
- Spandrel - https://github.com/chaiNNer-org/spandrel implementing
  - GFPGAN - https://github.com/TencentARC/GFPGAN.git
  - CodeFormer - https://github.com/sczhou/CodeFormer
  - ESRGAN - https://github.com/xinntao/ESRGAN
  - SwinIR - https://github.com/JingyunLiang/SwinIR
  - Swin2SR - https://github.com/mv-lab/swin2sr
- LDSR - https://github.com/Hafiidz/latent-diffusion
- MiDaS - https://github.com/isl-org/MiDaS
- Ideas for optimizations - https://github.com/basujindal/stable-diffusion
- Cross Attention layer optimization - Doggettx - https://github.com/Doggettx/stable-diffusion, original idea for prompt editing.
- Cross Attention layer optimization - InvokeAI, lstein - https://github.com/invoke-ai/InvokeAI (originally http://github.com/lstein/stable-diffusion)
- Sub-quadratic Cross Attention layer optimization - Alex Birch (https://github.com/Birch-san/diffusers/pull/1), Amin Rezaei (https://github.com/AminRezaei0x443/memory-efficient-attention)
- Textual Inversion - Rinon Gal - https://github.com/rinongal/textual_inversion (we&#039;re not using his code, but we are using his ideas).
- Idea for SD upscale - https://github.com/jquesnelle/txt2imghd
- Noise generation for outpainting mk2 - https://github.com/parlance-zz/g-diffuser-bot
- CLIP interrogator idea and borrowing some code - https://github.com/pharmapsychotic/clip-interrogator
- Idea for Composable Diffusion - https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch
- xformers - https://github.com/facebookresearch/xformers
- DeepDanbooru - interrogator for anime diffusers https://github.com/KichangKim/DeepDanbooru
- Sampling in float32 precision from a float16 UNet - marunine for the idea, Birch-san for the example Diffusers implementation (https://github.com/Birch-san/diffusers-play/tree/92feee6)
- Instruct pix2pix - Tim Brooks (star), Aleksander Holynski (star), Alexei A. Efros (no star) - https://github.com/timothybrooks/instruct-pix2pix
- Security advice - RyotaK
- UniPC sampler - Wenliang Zhao - https://github.com/wl-zhao/UniPC
- TAESD - Ollin Boer Bohan - https://github.com/madebyollin/taesd
- LyCORIS - KohakuBlueleaf
- Restart sampling - lambertae - https://github.com/Newbeeer/diffusion_restart_sampling
- Hypertile - tfernd - https://github.com/tfernd/HyperTile
- Initial Gradio script - posted on 4chan by an Anonymous user. Thank you Anonymous user.
- (You)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[dortania/OpenCore-Legacy-Patcher]]></title>
            <link>https://github.com/dortania/OpenCore-Legacy-Patcher</link>
            <guid>https://github.com/dortania/OpenCore-Legacy-Patcher</guid>
            <pubDate>Tue, 23 Dec 2025 00:04:31 GMT</pubDate>
            <description><![CDATA[Experience macOS just like before]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/dortania/OpenCore-Legacy-Patcher">dortania/OpenCore-Legacy-Patcher</a></h1>
            <p>Experience macOS just like before</p>
            <p>Language: Python</p>
            <p>Stars: 16,238</p>
            <p>Forks: 1,678</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
             &lt;img src=&quot;docs/images/OC-Patcher.png&quot; alt=&quot;OpenCore Patcher Logo&quot; width=&quot;256&quot; /&gt;
             &lt;h1&gt;OpenCore Legacy Patcher&lt;/h1&gt;
&lt;/div&gt;

A Python-based project revolving around [Acidanthera&#039;s OpenCorePkg](https://github.com/acidanthera/OpenCorePkg) and [Lilu](https://github.com/acidanthera/Lilu) for both running and unlocking features in macOS on supported and unsupported Macs.

Our project&#039;s main goal is to breathe new life into Macs no longer supported by Apple, allowing for the installation and usage of macOS Big Sur and newer on machines as old as 2007.

----------

![GitHub all releases](https://img.shields.io/github/downloads/dortania/OpenCore-Legacy-Patcher/total?color=white&amp;style=plastic) ![GitHub top language](https://img.shields.io/github/languages/top/dortania/OpenCore-Legacy-Patcher?color=4B8BBE&amp;style=plastic) ![Discord](https://img.shields.io/discord/417165963327176704?color=7289da&amp;label=discord&amp;style=plastic)

----------

Noteworthy features of OpenCore Legacy Patcher:

* Support for macOS Big Sur, Monterey, Ventura, Sonoma and Sequoia
* Native Over the Air (OTA) System Updates
* Supports Penryn and newer Macs
* Full support for WPA Wi-Fi and Personal Hotspot on BCM943224 and newer wireless chipsets
* System Integrity Protection, FileVault 2, .im4m Secure Boot and Vaulting
* Recovery OS, Safe Mode and Single-user Mode booting on non-native OSes
* Unlocks features such as Sidecar and AirPlay to Mac even on native Macs
* Enables enhanced SATA and NVMe power management on non-Apple storage devices
* Zero firmware patching required (ie. APFS ROM patching)
* Graphics acceleration for both Metal and non-Metal GPUs

----------

Note: Only clean-installs and upgrades are supported. macOS Big Sur installs already patched with other patchers, such as [Patched Sur](https://github.com/BenSova/Patched-Sur) or [bigmac](https://github.com/StarPlayrX/bigmac), cannot be used due to broken file integrity with APFS snapshots and SIP.

* You can, however, reinstall macOS with this patcher and retain your original data

Note 2: Currently, OpenCore Legacy Patcher officially supports patching to run macOS Big Sur through Sonoma installs. For older OSes, OpenCore may function; however, support is currently not provided from Dortania.

* For macOS Mojave and Catalina support, we recommend the use of [dosdude1&#039;s patchers](http://dosdude1.com)

## Getting Started

To start using the project, please see our in-depth guide:

* [OpenCore Legacy Patcher Guide](https://dortania.github.io/OpenCore-Legacy-Patcher/)

## Support

This project is offered on an AS-IS basis, we do not guarantee support for any issues that may arise. However, there is a community server with other passionate users and developers that can aid you:

* [OpenCore Patcher Paradise Discord Server](https://discord.gg/rqdPgH8xSN)
  * Keep in mind that the Discord server is maintained by the community, so we ask everyone to be respectful.
  * Please review our docs on [how to debug with OpenCore](https://dortania.github.io/OpenCore-Legacy-Patcher/DEBUG.html) to gather important information to help others with troubleshooting.

## Running from source

To run the project from source, see here: [Build and run from source](./SOURCE.md)

## Credits

* [Acidanthera](https://github.com/Acidanthera)
  * OpenCorePkg, as well as many of the core kexts and tools
* [DhinakG](https://github.com/DhinakG)
  * Main co-author
* [Khronokernel](https://github.com/Khronokernel)
  * Main co-author
* [Ausdauersportler](https://github.com/Ausdauersportler)
  * iMacs Metal GPUs Upgrade Patch set and documentation
  * Great amounts of help with debugging, and code suggestions
* [vit9696](https://github.com/vit9696)
  * Endless amount of help troubleshooting, determining fixes and writing patches
* [EduCovas](https://github.com/covasedu)
  * [non-Metal patch set](https://github.com/moraea/non-metal-frameworks) for nVidia Tesla/Fermi/Maxwell/Pascal, AMD TeraScale 1/2, and Intel Core 1st/2nd Generation GPUs
  * [3802 Metal patch set](https://github.com/moraea/misc-patches/tree/main/3802-Metal-15) and [MetallibSupportPkg](https://github.com/dortania/MetallibSupportPkg) for nVidia Kepler and Intel Core 3rd/4th Generation GPUs
  * Metal bundle patches and shims for [nVidia Kepler](https://github.com/moraea/misc-patches/tree/main/Kepler%2013%2B), [AMD GCN 1 - 4](https://github.com/moraea/misc-patches/tree/main/GCN%2013%2B), and [AMD GCN 5 (Vega)](https://github.com/moraea/misc-patches/tree/main/vega%2013%2B)
  * [IOSurface offset patches](https://github.com/moraea/misc-patches/tree/main/Sonoma%2014.4%20IOSurface) for nVidia Kepler, AMD GCN 1 - 5, and Intel Core 3rd - 6th Generation GPUs
  * [legacy Wi-Fi patch set](https://github.com/moraea/unsupported-wifi-patches) restores functionality for Wi-Fi cards in all 2007 - 2017 models
  * [T1 patch set](https://github.com/moraea/misc-patches/tree/main/T1-Patch) restores Touch ID, Apple Pay, and other secure functionality in 2016 - 2017 models
  * AppleGVA downgrade for accelerated video decoding on 2012 - 2016 models
  * OpenCL and OpenGL downgrade for AMD GCN
  * [USB 1 patch](https://github.com/moraea/misc-patches/tree/main/IOUSBHostFamily-14.4)
* [ASentientHedgehog](https://github.com/moosethegoose2213)
  * [non-Metal patch set](https://github.com/moraea/non-metal-frameworks) for nVidia Tesla/Fermi/Maxwell/Pascal, AMD TeraScale 1/2, and Intel Core 1st/2nd Generation GPUs
* [ASentientBot](https://github.com/ASentientBot)
  * [non-Metal patch set](https://github.com/moraea/non-metal-frameworks) for nVidia Tesla/Fermi/Maxwell/Pascal, AMD TeraScale 1/2, and Intel Core 1st/2nd Generation GPUs
  * [Metal bundle interposer](https://github.com/moraea/misc-patches/tree/main/sequoia%2031001%20interposer) for AMD GCN 1 - 5 and Intel Core 5th/6th Generation GPUs
  * [dsce](https://github.com/moraea/dsce) and [shared code](https://github.com/moraea/moraea-common) used by some other patches
* [cdf](https://github.com/cdf)
  * Mac Pro on OpenCore Patch set and documentation
  * [Innie](https://github.com/cdf/Innie) and [NightShiftEnabler](https://github.com/cdf/NightShiftEnabler)
* [Syncretic](https://forums.macrumors.com/members/syncretic.1173816/)
  * [AAAMouSSE](https://forums.macrumors.com/threads/mp3-1-others-sse-4-2-emulation-to-enable-amd-metal-driver.2206682/), [telemetrap](https://forums.macrumors.com/threads/mp3-1-others-sse-4-2-emulation-to-enable-amd-metal-driver.2206682/post-28447707) and [SurPlus](https://github.com/reenigneorcim/SurPlus)
* [dosdude1](https://github.com/dosdude1)
  * Main author of the [original GUI](https://github.com/dortania/OCLP-GUI)
  * Development of previous patchers, laying out much of what needs to be patched
* [parrotgeek1](https://github.com/parrotgeek1)
  * [VMM Patch Set](https://github.com/dortania/OpenCore-Legacy-Patcher/blob/4a8f61a01da72b38a4b2250386cc4b497a31a839/payloads/Config/config.plist#L1222-L1281)
* [BarryKN](https://github.com/BarryKN)
  * Development of previous patchers, laying out much of what needs to be patched
* [mario_bros_tech](https://github.com/mariobrostech) and the rest of the Unsupported Mac Discord
  * Catalyst that started OpenCore Legacy Patcher
* [arter97](https://github.com/arter97/)
  * [SimpleMSR](https://github.com/arter97/SimpleMSR/) to disable firmware throttling in Nehalem+ MacBooks without batteries
* [Mr.Macintosh](https://mrmacintosh.com)
  * Endless hours helping architect and troubleshoot many portions of the project
* [flagers](https://github.com/flagersgit)
  * Aid with Nvidia Web Driver research and development
  * [non-Metal patch set](https://github.com/moraea/non-metal-frameworks) for nVidia Tesla/Fermi/Maxwell/Pascal, AMD TeraScale 1/2, and Intel Core 1st/2nd Generation GPUs
  * [Metal bundle interposer](https://github.com/moraea/misc-patches/tree/main/sequoia%2031001%20interposer) for AMD GCN 1 - 5 and Intel Core 5th/6th Generation GPUs
  * LegacyRVPL, SnapshotIsKill, etc. to aid in rapid testing and development
* [joevt](https://github.com/joevt)
  * [FixPCIeLinkrate](https://github.com/joevt/joevtApps)
* [Jazzzny](https://github.com/Jazzzny)
  * Research and various contributions to the project
  * UEFI Legacy XHCI research and development
  * NVIDIA OpenCL research and development
  * `MacBook5,2` research and development
    * LegacyKeyboardInjector
  * Pre-Ivy Bridge Aquantia Ethernet Patch
  * Non-Metal Photo Booth Patch for Monterey+
  * GUI and Backend Development
    * Updater UI
    * macOS Downloader UI
    * Downloader UI
    * USB Top Case probing
    * Developer root patching
  * Vaulting implementation
  * macOS 15 3802 Helios Research
  * UEFI bootx64.efi research
  * universal2 build research
  * Various documentation contributions
* Amazing users who&#039;ve graciously donate hardware:
  * [JohnD](https://forums.macrumors.com/members/johnd.53633/) - 2013 Mac Pro
  * [SpiGAndromeda](https://github.com/SpiGAndromeda) - AMD Vega 64
  * [turbomacs](https://github.com/turbomacs) - 2014 5k iMac
  * [vinaypundith](https://forums.macrumors.com/members/vinaypundith.1212357/) - MacBook7,1
   * [ThatStella7922](https://github.com/ThatStella7922) - 2017 13&quot; MacBook Pro (A1708)
  * zephar - 2008 Mac Pro
  * jazo97 - 2011 15&quot; MacBook Pro
  * And others (reach out if we forgot you!)
* MacRumors and Unsupported Mac Communities
  * Endless testing and reporting issues
* Apple
  * for macOS and many of the kexts, frameworks and other binaries we reimplemented into newer OSes
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[davila7/claude-code-templates]]></title>
            <link>https://github.com/davila7/claude-code-templates</link>
            <guid>https://github.com/davila7/claude-code-templates</guid>
            <pubDate>Tue, 23 Dec 2025 00:04:30 GMT</pubDate>
            <description><![CDATA[CLI tool for configuring and monitoring Claude Code]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/davila7/claude-code-templates">davila7/claude-code-templates</a></h1>
            <p>CLI tool for configuring and monitoring Claude Code</p>
            <p>Language: Python</p>
            <p>Stars: 13,094</p>
            <p>Forks: 1,154</p>
            <p>Stars today: 112 stars today</p>
            <h2>README</h2><pre>[![npm version](https://img.shields.io/npm/v/claude-code-templates.svg)](https://www.npmjs.com/package/claude-code-templates)
[![npm downloads](https://img.shields.io/npm/dt/claude-code-templates.svg)](https://www.npmjs.com/package/claude-code-templates)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![Sponsored by Z.AI](https://img.shields.io/badge/Sponsored%20by-Z.AI-2563eb?style=flat&amp;logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTEyIDJMMiAyMkgyMkwxMiAyWiIgZmlsbD0id2hpdGUiLz4KPC9zdmc+)](https://z.ai/subscribe?ic=8JVLJQFSKB&amp;utm_source=github&amp;utm_medium=badge&amp;utm_campaign=readme)
[![Buy Me A Coffee](https://img.shields.io/badge/Buy%20Me%20A%20Coffee-support-yellow?style=flat&amp;logo=buy-me-a-coffee)](https://buymeacoffee.com/daniavila)
[![GitHub stars](https://img.shields.io/github/stars/davila7/claude-code-templates.svg?style=social&amp;label=Star)](https://github.com/davila7/claude-code-templates)

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/15113&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/15113&quot; alt=&quot;davila7%2Fclaude-code-templates | Trendshift&quot; style=&quot;width: 200px; height: 40px;&quot; width=&quot;125&quot; height=&quot;40&quot;/&gt;
  &lt;/a&gt;
  &lt;br /&gt;
  &lt;br /&gt;
  &lt;a href=&quot;https://vercel.com/oss&quot;&gt;
  &lt;img alt=&quot;Vercel OSS Program&quot; src=&quot;https://vercel.com/oss/program-badge.svg&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

---

&lt;div align=&quot;center&quot;&gt;
  &lt;h3&gt;ü§ù Partnership&lt;/h3&gt;
  &lt;p&gt;
    &lt;strong&gt;This project is sponsored by &lt;a href=&quot;https://z.ai&quot; target=&quot;_blank&quot;&gt;Z.AI&lt;/a&gt;&lt;/strong&gt;&lt;br/&gt;
    Supporting Claude Code Templates with the &lt;strong&gt;GLM CODING PLAN&lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
    &lt;a href=&quot;https://z.ai/subscribe?ic=8JVLJQFSKB&amp;utm_source=github&amp;utm_medium=readme&amp;utm_campaign=partnership&quot; target=&quot;_blank&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/badge/Get%2010%25%20OFF-GLM%20Coding%20Plan-2563eb?style=for-the-badge&quot; alt=&quot;GLM Coding Plan&quot; /&gt;
    &lt;/a&gt;
  &lt;/p&gt;
  &lt;p&gt;
    &lt;em&gt;Top-tier coding performance powered by GLM-4.6 ‚Ä¢ Starting at $3/month&lt;/em&gt;&lt;br/&gt;
    &lt;em&gt;Seamlessly integrates with Claude Code, Cursor, Cline &amp; 10+ AI coding tools&lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
    &lt;code&gt;npx claude-code-templates@latest --setting partnerships/glm-coding-plan --yes&lt;/code&gt;
  &lt;/p&gt;
&lt;/div&gt;

---

# Claude Code Templates ([aitmpl.com](https://aitmpl.com))

**Ready-to-use configurations for Anthropic&#039;s Claude Code.** A comprehensive collection of AI agents, custom commands, settings, hooks, external integrations (MCPs), and project templates to enhance your development workflow.

## Browse &amp; Install Components and Templates

**[Browse All Templates](https://aitmpl.com)** - Interactive web interface to explore and install 100+ agents, commands, settings, hooks, and MCPs.

&lt;img width=&quot;1049&quot; height=&quot;855&quot; alt=&quot;Screenshot 2025-08-19 at 08 09 24&quot; src=&quot;https://github.com/user-attachments/assets/e3617410-9b1c-4731-87b7-a3858800b737&quot; /&gt;

## üöÄ Quick Installation

```bash
# Install a complete development stack
npx claude-code-templates@latest --agent development-team/frontend-developer --command testing/generate-tests --mcp development/github-integration --yes

# Browse and install interactively
npx claude-code-templates@latest

# Install specific components
npx claude-code-templates@latest --agent development-tools/code-reviewer --yes
npx claude-code-templates@latest --command performance/optimize-bundle --yes
npx claude-code-templates@latest --setting performance/mcp-timeouts --yes
npx claude-code-templates@latest --hook git/pre-commit-validation --yes
npx claude-code-templates@latest --mcp database/postgresql-integration --yes
```

## What You Get

| Component | Description | Examples |
|-----------|-------------|----------|
| **ü§ñ Agents** | AI specialists for specific domains | Security auditor, React performance optimizer, database architect |
| **‚ö° Commands** | Custom slash commands | `/generate-tests`, `/optimize-bundle`, `/check-security` |
| **üîå MCPs** | External service integrations | GitHub, PostgreSQL, Stripe, AWS, OpenAI |
| **‚öôÔ∏è Settings** | Claude Code configurations | Timeouts, memory settings, output styles |
| **ü™ù Hooks** | Automation triggers | Pre-commit validation, post-completion actions |
| **üé® Skills** | Reusable capabilities with progressive disclosure | PDF processing, Excel automation, custom workflows |

## üõ†Ô∏è Additional Tools

Beyond the template catalog, Claude Code Templates includes powerful development tools:

### üìä Claude Code Analytics
Monitor your AI-powered development sessions in real-time with live state detection and performance metrics.

```bash
npx claude-code-templates@latest --analytics
```

### üí¨ Conversation Monitor  
Mobile-optimized interface to view Claude responses in real-time with secure remote access.

```bash
# Local access
npx claude-code-templates@latest --chats

# Secure remote access via Cloudflare Tunnel
npx claude-code-templates@latest --chats --tunnel
```

### üîç Health Check
Comprehensive diagnostics to ensure your Claude Code installation is optimized.

```bash
npx claude-code-templates@latest --health-check
```

### üîå Plugin Dashboard
View marketplaces, installed plugins, and manage permissions from a unified interface.

```bash
npx claude-code-templates@latest --plugins
```

## üìñ Documentation

**[üìö docs.aitmpl.com](https://docs.aitmpl.com/)** - Complete guides, examples, and API reference for all components and tools.

## Contributing

We welcome contributions! **[Browse existing templates](https://aitmpl.com)** to see what&#039;s available, then check our [contributing guidelines](CONTRIBUTING.md) to add your own agents, commands, MCPs, settings, or hooks.

**Please read our [Code of Conduct](CODE_OF_CONDUCT.md) before contributing.**

## Attribution

This collection includes components from multiple sources:

**Scientific Skills:**
- **[K-Dense-AI/claude-scientific-skills](https://github.com/K-Dense-AI/claude-scientific-skills)** by K-Dense Inc. - MIT License (139 scientific skills for biology, chemistry, medicine, and computational research)

**Official Anthropic:**
- **[anthropics/skills](https://github.com/anthropics/skills)** - Official Anthropic skills (21 skills)
- **[anthropics/claude-code](https://github.com/anthropics/claude-code)** - Development guides and examples (10 skills)

**Community Skills &amp; Agents:**
- **[obra/superpowers](https://github.com/obra/superpowers)** by Jesse Obra - MIT License (14 workflow skills)
- **[alirezarezvani/claude-skills](https://github.com/alirezarezvani/claude-skills)** by Alireza Rezvani - MIT License (36 professional role skills)
- **[wshobson/agents](https://github.com/wshobson/agents)** by wshobson - MIT License (48 agents)
- **NerdyChefsAI Skills** - Community contribution - MIT License (specialized enterprise skills)

**Commands &amp; Tools:**
- **[awesome-claude-code](https://github.com/hesreallyhim/awesome-claude-code)** by hesreallyhim - CC0 1.0 Universal (21 commands)
- **[awesome-claude-skills](https://github.com/mehdi-lamrani/awesome-claude-skills)** - Apache 2.0 (community skills)
- **move-code-quality-skill** - MIT License
- **cocoindex-claude** - Apache 2.0

Each of these resources retains its **original license and attribution**, as defined by their respective authors.
We respect and credit all original creators for their work and contributions to the Claude ecosystem.

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üîó Links

- **üåê Browse Templates**: [aitmpl.com](https://aitmpl.com)
- **üìö Documentation**: [docs.aitmpl.com](https://docs.aitmpl.com)
- **üí¨ Community**: [GitHub Discussions](https://github.com/davila7/claude-code-templates/discussions)
- **üêõ Issues**: [GitHub Issues](https://github.com/davila7/claude-code-templates/issues)

## Stargazers over time
[![Stargazers over time](https://starchart.cc/davila7/claude-code-templates.svg?variant=adaptive)](https://starchart.cc/davila7/claude-code-templates)

---

**‚≠ê Found this useful? Give us a star to support the project!**

[![Buy Me A Coffee](https://img.buymeacoffee.com/button-api/?text=Buy%20me%20a%20coffee&amp;slug=daniavila&amp;button_colour=FFDD00&amp;font_colour=000000&amp;font_family=Cookie&amp;outline_colour=000000&amp;coffee_colour=ffffff)](https://buymeacoffee.com/daniavila)</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sgl-project/mini-sglang]]></title>
            <link>https://github.com/sgl-project/mini-sglang</link>
            <guid>https://github.com/sgl-project/mini-sglang</guid>
            <pubDate>Tue, 23 Dec 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[A compact implementation of SGLang, designed to demystify the complexities of modern LLM serving systems.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sgl-project/mini-sglang">sgl-project/mini-sglang</a></h1>
            <p>A compact implementation of SGLang, designed to demystify the complexities of modern LLM serving systems.</p>
            <p>Language: Python</p>
            <p>Stars: 2,145</p>
            <p>Forks: 180</p>
            <p>Stars today: 289 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;400&quot; src=&quot;/assets/logo.png&quot;&gt;
&lt;/p&gt;

# Mini-SGLang

A **lightweight yet high-performance** inference framework for Large Language Models.

---

Mini-SGLang is a compact implementation of [SGLang](https://github.com/sgl-project/sglang), designed to demystify the complexities of modern LLM serving systems. With a compact codebase of **~5,000 lines of Python**, it serves as both a capable inference engine and a transparent reference for researchers and developers.

## ‚ú® Key Features

- **High Performance**: Achieves state-of-the-art throughput and latency with advanced optimizations.
- **Lightweight &amp; Readable**: A clean, modular, and fully type-annotated codebase that is easy to understand and modify.
- **Advanced Optimizations**:
  - **Radix Cache**: Reuses KV cache for shared prefixes across requests.
  - **Chunked Prefill**: Reduces peak memory usage for long-context serving.
  - **Overlap Scheduling**: Hides CPU scheduling overhead with GPU computation.
  - **Tensor Parallelism**: Scales inference across multiple GPUs.
  - **Optimized Kernels**: Integrates **FlashAttention** and **FlashInfer** for maximum efficiency.
  - ...

## üöÄ Quick Start

&gt; **‚ö†Ô∏è Platform Support**: Mini-SGLang currently supports **Linux only** (x86_64 and aarch64). Windows and macOS are not supported due to dependencies on Linux-specific CUDA kernels (`sgl-kernel`, `flashinfer`). We recommend using [WSL2](https://learn.microsoft.com/en-us/windows/wsl/install) on Windows or Docker for cross-platform compatibility.

### 1. Environment Setup

We recommend using `uv` for a fast and reliable installation (note that `uv` does not conflict with `conda`).

```bash
# Create a virtual environment (Python 3.10+ recommended)
uv venv --python=3.12
source .venv/bin/activate
```

**Prerequisites**: Mini-SGLang relies on CUDA kernels that are JIT-compiled. Ensure you have the **NVIDIA CUDA Toolkit** installed and that its version matches your driver&#039;s version. You can check your driver&#039;s CUDA capability with `nvidia-smi`.

### 2. Installation

Install Mini-SGLang directly from the source:

```bash
git clone https://github.com/sgl-project/mini-sglang.git
cd mini-sglang &amp;&amp; uv venv --python=3.12 &amp;&amp; source .venv/bin/activate
uv pip install -e .
```

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;üí° Installing on Windows (WSL2)&lt;/b&gt;&lt;/summary&gt;

Since Mini-SGLang requires Linux-specific dependencies, Windows users should use WSL2:

1. **Install WSL2** (if not already installed):
   ```powershell
   # In PowerShell (as Administrator)
   wsl --install
   ```

2. **Install CUDA on WSL2**:
   - Follow [NVIDIA&#039;s WSL2 CUDA guide](https://docs.nvidia.com/cuda/wsl-user-guide/index.html)
   - Ensure your Windows GPU drivers support WSL2

3. **Install Mini-SGLang in WSL2**:
   ```bash
   # Inside WSL2 terminal
   git clone https://github.com/sgl-project/mini-sglang.git
   cd mini-sglang &amp;&amp; uv venv --python=3.12 &amp;&amp; source .venv/bin/activate
   uv pip install -e .
   ```

4. **Access from Windows**: The server will be accessible at `http://localhost:8000` from Windows browsers and applications.

&lt;/details&gt;

### 3. Online Serving

Launch an OpenAI-compatible API server with a single command.

```bash
# Deploy Qwen/Qwen3-0.6B on a single GPU
python -m minisgl --model &quot;Qwen/Qwen3-0.6B&quot;

# Deploy meta-llama/Llama-3.1-70B-Instruct on 4 GPUs with Tensor Parallelism, on port 30000
python -m minisgl --model &quot;meta-llama/Llama-3.1-70B-Instruct&quot; --tp 4 --port 30000
```

Once the server is running, you can send requests using standard tools like `curl` or any OpenAI-compatible client.

### 4. Interactive Shell

Chat with your model directly in the terminal by adding the `--shell` flag.

```bash
python -m minisgl --model &quot;Qwen/Qwen3-0.6B&quot; --shell
```

![shell-example](https://lmsys.org/images/blog/minisgl/shell.png)

You can also use `/reset` to clear the chat history.

## Benchmark

### Offline inference

See [bench.py](./benchmark/offline/bench.py) for more details. Set `MINISGL_DISABLE_OVERLAP_SCHEDULING=1` for ablation study on overlap scheduling.

Test Configuration:

- Hardware: 1xH200 GPU.
- Model: Qwen3-0.6B, Qwen3-14B
- Total Requests: 256 sequences
- Input Length: Randomly sampled between 100-1024 tokens
- Output Length: Randomly sampled between 100-1024 tokens

![offline](https://lmsys.org/images/blog/minisgl/offline.png)

### Online inference

See [benchmark_qwen.py](./benchmark/online/bench_qwen.py) for more details.

Test Configuration:

- Hardware: 4xH200 GPU, connected by NVLink.
- Model: Qwen3-32B
- Dataset: [Qwen trace](https://github.com/alibaba-edu/qwen-bailian-usagetraces-anon/blob/main/qwen_traceA_blksz_16.jsonl), replaying first 1000 requests.

Launch command:

```bash
# Mini-SGLang
python -m minisgl --model &quot;Qwen/Qwen3-32B&quot; --tp 4 --cache naive

# SGLang
python3 -m sglang.launch_server --model &quot;Qwen/Qwen3-32B&quot; --tp 4 \
    --disable-radix --port 1919 --decode-attention flashinfer
```

![online](https://lmsys.org/images/blog/minisgl/online.png)

## üìö Learn More

- **[Detailed Features](./docs/features.md)**: Explore all available features and command-line arguments.
- **[System Architecture](./docs/structures.md)**: Dive deep into the design and data flow of Mini-SGLang.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pollen-robotics/reachy_mini]]></title>
            <link>https://github.com/pollen-robotics/reachy_mini</link>
            <guid>https://github.com/pollen-robotics/reachy_mini</guid>
            <pubDate>Tue, 23 Dec 2025 00:04:28 GMT</pubDate>
            <description><![CDATA[Reachy Mini's SDK]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pollen-robotics/reachy_mini">pollen-robotics/reachy_mini</a></h1>
            <p>Reachy Mini's SDK</p>
            <p>Language: Python</p>
            <p>Stars: 554</p>
            <p>Forks: 76</p>
            <p>Stars today: 106 stars today</p>
            <h2>README</h2><pre># Reachy Mini ü§ñ

[![Ask on HuggingChat](https://img.shields.io/badge/Ask_on-HuggingChat-yellow?logo=huggingface&amp;logoColor=yellow&amp;style=for-the-badge)](https://huggingface.co/chat/?attachments=https%3A%2F%2Fgist.githubusercontent.com%2FFabienDanieau%2F919e1d7468fb16e70dbe984bdc277bba%2Fraw%2Fdoc_reachy_mini_full.md&amp;prompt=Read%20this%20documentation%20about%20Reachy%20Mini%20so%20I%20can%20ask%20questions%20about%20it.)
[![Discord](https://img.shields.io/badge/Discord-Join_the_Community-7289DA?logo=discord&amp;logoColor=white)](https://discord.gg/Y7FgMqHsub)

**Reachy Mini is an open-source, expressive robot made for hackers and AI builders.**

üõí [**Buy Reachy Mini**](https://www.hf.co/reachy-mini/)

[![Reachy Mini Hello](/docs/assets/reachy_mini_hello.gif)](https://www.pollen-robotics.com/reachy-mini/)

## ‚ö°Ô∏è Build and start your own robot

**Choose your platform to access the specific guide:**

| **ü§ñ Reachy Mini (Wireless)** | **üîå Reachy Mini Lite** | **üíª Simulation** |
| :---: | :---: | :---: |
| The full autonomous experience.&lt;br&gt;Raspberry Pi 4 + Battery + WiFi. | The developer version.&lt;br&gt;USB connection to your computer. | No hardware required.&lt;br&gt;Prototype in MuJoCo. |
| üëâ [**Go to Wireless Guide**](docs/platforms/reachy_mini/get_started.md) | üëâ [**Go to Lite Guide**](docs/platforms/reachy_mini_lite/get_started.md) | üëâ [**Go to Simulation**](docs/platforms/simulation/get_started.md) |



&gt; ‚ö° **Pro tip:** Install [uv](https://docs.astral.sh/uv/getting-started/installation/) for 10-100x faster app installations (auto-detected, falls back to `pip`).

&lt;br&gt;

## üì± Apps &amp; Ecosystem

Reachy Mini comes with an app store powered by Hugging Face Spaces. You can install these apps directly from your robot&#039;s dashboard with one click!

* **üó£Ô∏è [Conversation App](https://huggingface.co/spaces/pollen-robotics/reachy_mini_conversation_app):** Talk naturally with Reachy Mini (powered by LLMs).
* **üìª [Radio](https://huggingface.co/spaces/pollen-robotics/reachy_mini_radio):** Listen to the radio with Reachy Mini !
* **üëã [Hand Tracker](https://huggingface.co/spaces/pollen-robotics/hand_tracker_v2):** The robot follows your hand movements in real-time.

üëâ [**Browse all apps on Hugging Face**](https://hf.co/reachy-mini/#/apps)

&lt;br&gt;

## üöÄ Getting Started with Reachy Mini SDK

### Quick Look
Control your robot in just **a few lines of code**:

```python
from reachy_mini import ReachyMini
from reachy_mini.utils import create_head_pose

with ReachyMini() as mini:
    # Look up and tilt head
    mini.goto_target(
        head=create_head_pose(z=10, roll=15, degrees=True, mm=True),
        duration=1.0
    )
```

### User guides
* **[Installation](docs/SDK/installation.md)**: 5 minutes to set up your computer
* **[Quickstart Guide](docs/SDK/quickstart.md)**: Run your first behavior on Reachy Mini
* **[Python SDK](docs/SDK/python-sdk.md)**: Learn to move, see, speak, and hear.
* **[AI Integrations](docs/SDK/integration.md)**: Connect LLMs, build Apps, and publish to Hugging Face.
* **[Core Concepts](docs/SDK/core-concept.md)**: Architecture, coordinate systems, and safety limits.
* ü§ó[**Share your app with the community**](https://huggingface.co/blog/pollen-robotics/make-and-publish-your-reachy-mini-apps)
* üìÇ [**Browse the Examples Folder**](examples)


&lt;br&gt;

## üõ† Hardware Overview

Reachy Mini robots are sold as kits and generally take **2 to 3 hours** to assemble. Detailed step-by-step guides are available in the platform-specific folders linked above.

* **Reachy Mini (Wireless):** Runs onboard (RPi 4), autonomous, includes IMU. [See specs](docs/platforms/reachy_mini/hardware.md).
* **Reachy Mini Lite:** Runs on your PC, powered via wall outlet. [See specs](docs/platforms/reachy_mini_lite/hardware.md).

&lt;br&gt;

## ‚ùì Troubleshooting

Encountering an issue? üëâ **[Check the Troubleshooting &amp; FAQ Guide](/docs/troubleshooting.md)**

&lt;br&gt;

## ü§ù Community &amp; Contributing

* **Join the Community:** Join [Discord](https://discord.gg/2bAhWfXme9) to share your moments with Reachy, build apps together, and get help.
* **Found a bug?** Open an issue on this repository.


## License

This project is licensed under the Apache 2.0 License. See the [LICENSE](LICENSE) file for details.
Hardware design files are licensed under Creative Commons BY-SA-NC.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[yt-dlp/yt-dlp]]></title>
            <link>https://github.com/yt-dlp/yt-dlp</link>
            <guid>https://github.com/yt-dlp/yt-dlp</guid>
            <pubDate>Tue, 23 Dec 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[A feature-rich command-line audio/video downloader]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/yt-dlp/yt-dlp">yt-dlp/yt-dlp</a></h1>
            <p>A feature-rich command-line audio/video downloader</p>
            <p>Language: Python</p>
            <p>Stars: 139,141</p>
            <p>Forks: 11,239</p>
            <p>Stars today: 134 stars today</p>
            <h2>README</h2><pre>&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
&lt;div align=&quot;center&quot;&gt;

[![YT-DLP](https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/.github/banner.svg)](#readme)

[![Release version](https://img.shields.io/github/v/release/yt-dlp/yt-dlp?color=brightgreen&amp;label=Download&amp;style=for-the-badge)](#installation &quot;Installation&quot;)
[![PyPI](https://img.shields.io/badge/-PyPI-blue.svg?logo=pypi&amp;labelColor=555555&amp;style=for-the-badge)](https://pypi.org/project/yt-dlp &quot;PyPI&quot;)
[![Donate](https://img.shields.io/badge/_-Donate-red.svg?logo=githubsponsors&amp;labelColor=555555&amp;style=for-the-badge)](Maintainers.md#maintainers &quot;Donate&quot;)
[![Discord](https://img.shields.io/discord/807245652072857610?color=blue&amp;labelColor=555555&amp;label=&amp;logo=discord&amp;style=for-the-badge)](https://discord.gg/H5MNcFW63r &quot;Discord&quot;)
[![Supported Sites](https://img.shields.io/badge/-Supported_Sites-brightgreen.svg?style=for-the-badge)](supportedsites.md &quot;Supported Sites&quot;)
[![License: Unlicense](https://img.shields.io/badge/-Unlicense-blue.svg?style=for-the-badge)](LICENSE &quot;License&quot;)
[![CI Status](https://img.shields.io/github/actions/workflow/status/yt-dlp/yt-dlp/core.yml?branch=master&amp;label=Tests&amp;style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/actions &quot;CI Status&quot;)
[![Commits](https://img.shields.io/github/commit-activity/m/yt-dlp/yt-dlp?label=commits&amp;style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/commits &quot;Commit History&quot;)
[![Last Commit](https://img.shields.io/github/last-commit/yt-dlp/yt-dlp/master?label=&amp;style=for-the-badge&amp;display_timestamp=committer)](https://github.com/yt-dlp/yt-dlp/pulse/monthly &quot;Last activity&quot;)

&lt;/div&gt;
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

yt-dlp is a feature-rich command-line audio/video downloader with support for [thousands of sites](supportedsites.md). The project is a fork of [youtube-dl](https://github.com/ytdl-org/youtube-dl) based on the now inactive [youtube-dlc](https://github.com/blackjack4494/yt-dlc).

&lt;!-- MANPAGE: MOVE &quot;USAGE AND OPTIONS&quot; SECTION HERE --&gt;

&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
* [INSTALLATION](#installation)
    * [Detailed instructions](https://github.com/yt-dlp/yt-dlp/wiki/Installation)
    * [Release Files](#release-files)
    * [Update](#update)
    * [Dependencies](#dependencies)
    * [Compile](#compile)
* [USAGE AND OPTIONS](#usage-and-options)
    * [General Options](#general-options)
    * [Network Options](#network-options)
    * [Geo-restriction](#geo-restriction)
    * [Video Selection](#video-selection)
    * [Download Options](#download-options)
    * [Filesystem Options](#filesystem-options)
    * [Thumbnail Options](#thumbnail-options)
    * [Internet Shortcut Options](#internet-shortcut-options)
    * [Verbosity and Simulation Options](#verbosity-and-simulation-options)
    * [Workarounds](#workarounds)
    * [Video Format Options](#video-format-options)
    * [Subtitle Options](#subtitle-options)
    * [Authentication Options](#authentication-options)
    * [Post-processing Options](#post-processing-options)
    * [SponsorBlock Options](#sponsorblock-options)
    * [Extractor Options](#extractor-options)
    * [Preset Aliases](#preset-aliases)
* [CONFIGURATION](#configuration)
    * [Configuration file encoding](#configuration-file-encoding)
    * [Authentication with netrc](#authentication-with-netrc)
    * [Notes about environment variables](#notes-about-environment-variables)
* [OUTPUT TEMPLATE](#output-template)
    * [Output template examples](#output-template-examples)
* [FORMAT SELECTION](#format-selection)
    * [Filtering Formats](#filtering-formats)
    * [Sorting Formats](#sorting-formats)
    * [Format Selection examples](#format-selection-examples)
* [MODIFYING METADATA](#modifying-metadata)
    * [Modifying metadata examples](#modifying-metadata-examples)
* [EXTRACTOR ARGUMENTS](#extractor-arguments)
* [PLUGINS](#plugins)
    * [Installing Plugins](#installing-plugins)
    * [Developing Plugins](#developing-plugins)
* [EMBEDDING YT-DLP](#embedding-yt-dlp)
    * [Embedding examples](#embedding-examples)
* [CHANGES FROM YOUTUBE-DL](#changes-from-youtube-dl)
    * [New features](#new-features)
    * [Differences in default behavior](#differences-in-default-behavior)
    * [Deprecated options](#deprecated-options)
* [CONTRIBUTING](CONTRIBUTING.md#contributing-to-yt-dlp)
    * [Opening an Issue](CONTRIBUTING.md#opening-an-issue)
    * [Developer Instructions](CONTRIBUTING.md#developer-instructions)
* [WIKI](https://github.com/yt-dlp/yt-dlp/wiki)
    * [FAQ](https://github.com/yt-dlp/yt-dlp/wiki/FAQ)
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;


# INSTALLATION

&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
[![Windows](https://img.shields.io/badge/-Windows_x64-blue.svg?style=for-the-badge&amp;logo=windows)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe)
[![Unix](https://img.shields.io/badge/-Linux/BSD-red.svg?style=for-the-badge&amp;logo=linux)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp)
[![MacOS](https://img.shields.io/badge/-MacOS-lightblue.svg?style=for-the-badge&amp;logo=apple)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos)
[![PyPI](https://img.shields.io/badge/-PyPI-blue.svg?logo=pypi&amp;labelColor=555555&amp;style=for-the-badge)](https://pypi.org/project/yt-dlp)
[![Source Tarball](https://img.shields.io/badge/-Source_tar-green.svg?style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)
[![Other variants](https://img.shields.io/badge/-Other-grey.svg?style=for-the-badge)](#release-files)
[![All versions](https://img.shields.io/badge/-All_Versions-lightgrey.svg?style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/releases)
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

You can install yt-dlp using [the binaries](#release-files), [pip](https://pypi.org/project/yt-dlp) or one using a third-party package manager. See [the wiki](https://github.com/yt-dlp/yt-dlp/wiki/Installation) for detailed instructions


&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
## RELEASE FILES

#### Recommended

File|Description
:---|:---
[yt-dlp](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp)|Platform-independent [zipimport](https://docs.python.org/3/library/zipimport.html) binary. Needs Python (recommended for **Linux/BSD**)
[yt-dlp.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe)|Windows (Win8+) standalone x64 binary (recommended for **Windows**)
[yt-dlp_macos](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos)|Universal MacOS (10.15+) standalone executable (recommended for **MacOS**)

#### Alternatives

File|Description
:---|:---
[yt-dlp_linux](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux)|Linux (glibc 2.17+) standalone x86_64 binary
[yt-dlp_linux.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux.zip)|Unpackaged Linux (glibc 2.17+) x86_64 executable (no auto-update)
[yt-dlp_linux_aarch64](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_aarch64)|Linux (glibc 2.17+) standalone aarch64 binary
[yt-dlp_linux_aarch64.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_aarch64.zip)|Unpackaged Linux (glibc 2.17+) aarch64 executable (no auto-update)
[yt-dlp_linux_armv7l.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_armv7l.zip)|Unpackaged Linux (glibc 2.31+) armv7l executable (no auto-update)
[yt-dlp_musllinux](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux)|Linux (musl 1.2+) standalone x86_64 binary
[yt-dlp_musllinux.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux.zip)|Unpackaged Linux (musl 1.2+) x86_64 executable (no auto-update)
[yt-dlp_musllinux_aarch64](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux_aarch64)|Linux (musl 1.2+) standalone aarch64 binary
[yt-dlp_musllinux_aarch64.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux_aarch64.zip)|Unpackaged Linux (musl 1.2+) aarch64 executable (no auto-update)
[yt-dlp_x86.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_x86.exe)|Windows (Win8+) standalone x86 (32-bit) binary
[yt-dlp_win_x86.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win_x86.zip)|Unpackaged Windows (Win8+) x86 (32-bit) executable (no auto-update)
[yt-dlp_arm64.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_arm64.exe)|Windows (Win10+) standalone ARM64 binary
[yt-dlp_win_arm64.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win_arm64.zip)|Unpackaged Windows (Win10+) ARM64 executable (no auto-update)
[yt-dlp_win.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win.zip)|Unpackaged Windows (Win8+) x64 executable (no auto-update)
[yt-dlp_macos.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos.zip)|Unpackaged MacOS (10.15+) executable (no auto-update)

#### Misc

File|Description
:---|:---
[yt-dlp.tar.gz](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)|Source tarball
[SHA2-512SUMS](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS)|GNU-style SHA512 sums
[SHA2-512SUMS.sig](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS.sig)|GPG signature file for SHA512 sums
[SHA2-256SUMS](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS)|GNU-style SHA256 sums
[SHA2-256SUMS.sig](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS.sig)|GPG signature file for SHA256 sums

The public key that can be used to verify the GPG signatures is [available here](https://github.com/yt-dlp/yt-dlp/blob/master/public.key)
Example usage:
```
curl -L https://github.com/yt-dlp/yt-dlp/raw/master/public.key | gpg --import
gpg --verify SHA2-256SUMS.sig SHA2-256SUMS
gpg --verify SHA2-512SUMS.sig SHA2-512SUMS
```

#### Licensing

While yt-dlp is licensed under the [Unlicense](LICENSE), many of the release files contain code from other projects with different licenses.

Most notably, the PyInstaller-bundled executables include GPLv3+ licensed code, and as such the combined work is licensed under [GPLv3+](https://www.gnu.org/licenses/gpl-3.0.html).

The zipimport Unix executable (`yt-dlp`) contains [ISC](https://github.com/meriyah/meriyah/blob/main/LICENSE.md) licensed code from [`meriyah`](https://github.com/meriyah/meriyah) and [MIT](https://github.com/davidbonnet/astring/blob/main/LICENSE) licensed code from [`astring`](https://github.com/davidbonnet/astring).

See [THIRD_PARTY_LICENSES.txt](THIRD_PARTY_LICENSES.txt) for more details.

The git repository, the source tarball (`yt-dlp.tar.gz`), the PyPI source distribution and the PyPI built distribution (wheel) only contain code licensed under the [Unlicense](LICENSE).

&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

**Note**: The manpages, shell completion (autocomplete) files etc. are available inside the [source tarball](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)


## UPDATE
You can use `yt-dlp -U` to update if you are using the [release binaries](#release-files)

If you [installed with pip](https://github.com/yt-dlp/yt-dlp/wiki/Installation#with-pip), simply re-run the same command that was used to install the program

For other third-party package managers, see [the wiki](https://github.com/yt-dlp/yt-dlp/wiki/Installation#third-party-package-managers) or refer to their documentation

&lt;a id=&quot;update-channels&quot;&gt;&lt;/a&gt;

There are currently three release channels for binaries: `stable`, `nightly` and `master`.

* `stable` is the default channel, and many of its changes have been tested by users of the `nightly` and `master` channels.
* The `nightly` channel has releases scheduled to build every day around midnight UTC, for a snapshot of the project&#039;s new patches and changes. This is the **recommended channel for regular users** of yt-dlp. The `nightly` releases are available from [yt-dlp/yt-dlp-nightly-builds](https://github.com/yt-dlp/yt-dlp-nightly-builds/releases) or as development releases of the `yt-dlp` PyPI package (which can be installed with pip&#039;s `--pre` flag).
* The `master` channel features releases that are built after each push to the master branch, and these will have the very latest fixes and additions, but may also be more prone to regressions. They are available from [yt-dlp/yt-dlp-master-builds](https://github.com/yt-dlp/yt-dlp-master-builds/releases).

When using `--update`/`-U`, a release binary will only update to its current channel.
`--update-to CHANNEL` can be used to switch to a different channel when a newer version is available. `--update-to [CHANNEL@]TAG` can also be used to upgrade or downgrade to specific tags from a channel.

You may also use `--update-to &lt;repository&gt;` (`&lt;owner&gt;/&lt;repository&gt;`) to update to a channel on a completely different repository. Be careful with what repository you are updating to though, there is no verification done for binaries from different repositories.

Example usage:

* `yt-dlp --update-to master` switch to the `master` channel and update to its latest release
* `yt-dlp --update-to stable@2023.07.06` upgrade/downgrade to release to `stable` channel tag `2023.07.06`
* `yt-dlp --update-to 2023.10.07` upgrade/downgrade to tag `2023.10.07` if it exists on the current channel
* `yt-dlp --update-to example/yt-dlp@2023.09.24` upgrade/downgrade to the release from the `example/yt-dlp` repository, tag `2023.09.24`

**Important**: Any user experiencing an issue with the `stable` release should install or update to the `nightly` release before submitting a bug report:
```
# To update to nightly from stable executable/binary:
yt-dlp --update-to nightly

# To install nightly with pip:
python -m pip install -U --pre &quot;yt-dlp[default]&quot;
```

When running a yt-dlp version that is older than 90 days, you will see a warning message suggesting to update to the latest version.
You can suppress this warning by adding `--no-update` to your command or configuration file.

## DEPENDENCIES
Python versions 3.10+ (CPython) and 3.11+ (PyPy) are supported. Other versions and implementations may or may not work correctly.

&lt;!-- Python 3.5+ uses VC++14 and it is already embedded in the binary created
&lt;!x-- https://www.microsoft.com/en-us/download/details.aspx?id=26999 --x&gt;
On Windows, [Microsoft Visual C++ 2010 SP1 Redistributable Package (x86)](https://download.microsoft.com/download/1/6/5/165255E7-1014-4D0A-B094-B6A430A6BFFC/vcredist_x86.exe) is also necessary to run yt-dlp. You probably already have this, but if the executable throws an error due to missing `MSVCR100.dll` you need to install it manually.
--&gt;

While all the other dependencies are optional, `ffmpeg`, `ffprobe`, `yt-dlp-ejs` and a supported JavaScript runtime/engine are highly recommended

### Strongly recommended

* [**ffmpeg** and **ffprobe**](https://www.ffmpeg.org) - Required for [merging separate video and audio files](#format-selection), as well as for various [post-processing](#post-processing-options) tasks. License [depends on the build](https://www.ffmpeg.org/legal.html)

    There are bugs in ffmpeg that cause various issues when used alongside yt-dlp. Since ffmpeg is such an important dependency, we provide [custom builds](https://github.com/yt-dlp/FFmpeg-Builds#ffmpeg-static-auto-builds) with patches for some of these issues at [yt-dlp/FFmpeg-Builds](https://github.com/yt-dlp/FFmpeg-Builds). See [the readme](https://github.com/yt-dlp/FFmpeg-Builds#patches-applied) for details on the specific issues solved by these builds

    **Important**: What you need is ffmpeg *binary*, **NOT** [the Python package of the same name](https://pypi.org/project/ffmpeg)

* [**yt-dlp-ejs**](https://github.com/yt-dlp/ejs) - Required for deciphering YouTube n/sig values. Licensed under [Unlicense](https://github.com/yt-dlp/ejs/blob/main/LICENSE), bundles [MIT](https://github.com/davidbonnet/astring/blob/main/LICENSE) and [ISC](https://github.com/meriyah/meriyah/blob/main/LICENSE.md) components.

    A JavaScript runtime/engine like [**deno**](https://deno.land) (recommended), [**node.js**](https://nodejs.org), [**bun**](https://bun.sh), or [**QuickJS**](https://bellard.org/quickjs/) is also required to run yt-dlp-ejs. See [the wiki](https://github.com/yt-dlp/yt-dlp/wiki/EJS).

### Networking
* [**certifi**](https://github.com/certifi/python-certifi)\* - Provides Mozilla&#039;s root certificate bundle. Licensed under [MPLv2](https://github.com/certifi/python-certifi/blob/master/LICENSE)
* [**brotli**](https://github.com/google/brotli)\* or [**brotlicffi**](https://github.com/python-hyper/brotlicffi) - [Brotli](https://en.wikipedia.org/wiki/Brotli) content encoding support. Both licensed under MIT &lt;sup&gt;[1](https://github.com/google/brotli/blob/master/LICENSE) [2](https://github.com/python-hyper/brotlicffi/blob/master/LICENSE) &lt;/sup&gt;
* [**websockets**](https://github.com/aaugustin/websockets)\* - For downloading over websocket. Licensed under [BSD-3-Clause](https://github.com/aaugustin/websockets/blob/main/LICENSE)
* [**requests**](https://github.com/psf/requests)\* - HTTP library. For HTTPS proxy and persistent connections support. Licensed under [Apache-2.0](https://github.com/psf/requests/blob/main/LICENSE)

#### Impersonation

The following provide support for impersonating browser requests. This may be required for some sites that employ TLS fingerprinting.

* [**curl_cffi**](https://github.com/lexiforest/curl_cffi) (recommended) - Python binding for [curl-impersonate](https://github.com/lexiforest/curl-impersonate). Provides impersonation targets for Chrome, Edge and Safari. Licensed under [MIT](https://github.com/lexiforest/curl_cffi/blob/main/LICENSE)
  * Can be installed with the `curl-cffi` extra, e.g. `pip install &quot;yt-dlp[default,curl-cffi]&quot;`
  * Currently included in most builds *except* `yt-dlp` (Unix zipimport binary), `yt-dlp_x86` (Windows 32-bit) and `yt-dlp_musllinux_aarch64`


### Metadata

* [**mutagen**](https://github.com/quodlibet/mutagen)\* - For `--embed-thumbnail` in certain formats. Licensed under [GPLv2+](https://github.com/quodlibet/mutagen/blob/master/COPYING)
* [**AtomicParsley**](https://github.com/wez/atomicparsley) - For `--embed-thumbnail` in `mp4`/`m4a` files when `mutagen`/`ffmpeg` cannot. Licensed under [GPLv2+](https://github.com/wez/atomicparsley/blob/master/COPYING)
* [**xattr**](https://github.com/xattr/xattr), [**pyxattr**](https://github.com/iustin/pyxattr) or [**setfattr**](http://savannah.nongnu.org/projects/attr) - For writing xattr metadata (`--xattrs`) on **Mac** and **BSD**. Licensed under [MIT](https://github.com/xattr/xattr/blob/master/LICENSE.txt), [LGPL2.1](https://github.com/iustin/pyxattr/blob/master/COPYING) and [GPLv2+](http://git.savannah.nongnu.org/cgit/attr.git/tree/doc/COPYING) respectively

### Misc

* [**pycryptodomex**](https://github.com/Legrandin/pycryptodome)\* - For decrypting AES-128 HLS streams and various other data. Licensed under [BSD-2-Clause](https://github.com/Legrandin/pycryptodome/blob/master/LICENSE.rst)
* [**phantomjs**](https://github.com/ariya/phantomjs) - Used in some extractors where JavaScript needs to be run. No longer used for YouTube. To be deprecated in the near future. Licensed under [BSD-3-Clause](https://github.com/ariya/phantomjs/blob/master/LICENSE.BSD)
* [**secretstorage**](https://github.com/mitya57/secretstorage)\* - For `--cookies-from-browser` to access the **Gnome** keyring while decrypting cookies of **Chromium**-based browsers on **Linux**. Licensed under [BSD-3-Clause](https://github.com/mitya57/secretstorage/blob/master/LICENSE)
* Any external downloader that you want to use with `--downloader`

### Deprecated

* [**rtmpdump**](http://rtmpdump.mplayerhq.hu) - For downloading `rtmp` streams. ffmpeg can be used instead with `--downloader ffmpeg`. Licensed under [GPLv2+](http://rtmpdump.mplayerhq.hu)
* [**mplayer**](http://mplayerhq.hu/design7/info.html) or [**mpv

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[vllm-project/vllm-omni]]></title>
            <link>https://github.com/vllm-project/vllm-omni</link>
            <guid>https://github.com/vllm-project/vllm-omni</guid>
            <pubDate>Tue, 23 Dec 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[A framework for efficient model inference with omni-modality models]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/vllm-project/vllm-omni">vllm-project/vllm-omni</a></h1>
            <p>A framework for efficient model inference with omni-modality models</p>
            <p>Language: Python</p>
            <p>Stars: 1,265</p>
            <p>Forks: 170</p>
            <p>Stars today: 146 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; src=&quot;https://raw.githubusercontent.com/vllm-project/vllm-omni/refs/heads/main/docs/source/logos/vllm-omni-logo.png&quot;&gt;
    &lt;img alt=&quot;vllm-omni&quot; src=&quot;https://raw.githubusercontent.com/vllm-project/vllm-omni/refs/heads/main/docs/source/logos/vllm-omni-logo.png&quot; width=55%&gt;
  &lt;/picture&gt;
&lt;/p&gt;
&lt;h3 align=&quot;center&quot;&gt;
Easy, fast, and cheap omni-modality model serving for everyone
&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
| &lt;a href=&quot;https://vllm-omni.readthedocs.io/en/latest/&quot;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://discuss.vllm.ai&quot;&gt;&lt;b&gt;User Forum&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://slack.vllm.ai&quot;&gt;&lt;b&gt;Developer Slack&lt;/b&gt;&lt;/a&gt; |
&lt;/p&gt;

---

*Latest News* üî•

- [2025/11] vLLM community officially released [vllm-project/vllm-omni](https://github.com/vllm-project/vllm-omni) in order to support omni-modality models serving.

---

## About

[vLLM](https://github.com/vllm-project/vllm) was originally designed to support large language models for text-based autoregressive generation tasks. vLLM-Omni is a framework that extends its support for omni-modality model inference and serving:

- **Omni-modality**: Text, image, video, and audio data processing
- **Non-autoregressive Architectures**: extend the AR support of vLLM to Diffusion Transformers (DiT) and other parallel generation models
- **Heterogeneous outputs**: from traditional text generation to multimodal outputs

&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;img alt=&quot;vllm-omni&quot; src=&quot;https://raw.githubusercontent.com/vllm-project/vllm-omni/refs/heads/main/docs/source/architecture/omni-modality-model-architecture.png&quot; width=55%&gt;
  &lt;/picture&gt;
&lt;/p&gt;

vLLM-Omni is fast with:

- State-of-the-art AR support by leveraging efficient KV cache management from vLLM
- Pipelined stage execution overlapping for high throughput performance
- Fully disaggregation based on OmniConnector and dynamic resource allocation across stages

vLLM-Omni is flexible and easy to use with:

- Heterogeneous pipeline abstraction to manage complex model workflows
- Seamless integration with popular Hugging Face models
- Tensor, pipeline, data and expert parallelism support for distributed inference
- Streaming outputs
- OpenAI-compatible API server

vLLM-Omni seamlessly supports most popular open-source models on HuggingFace, including:

- Omni-modality models (e.g. Qwen-Omni)
- Multi-modality generation models (e.g. Qwen-Image)

## Getting Started

Visit our [documentation](https://vllm-omni.readthedocs.io/en/latest/) to learn more.

- [Installation](https://vllm-omni.readthedocs.io/en/latest/getting_started/installation/)
- [Quickstart](https://vllm-omni.readthedocs.io/en/latest/getting_started/quickstart/)
- [List of Supported Models](https://vllm-omni.readthedocs.io/en/latest/models/supported_models/)

## Contributing

We welcome and value any contributions and collaborations.
Please check out [Contributing to vLLM-Omni](https://vllm-omni.readthedocs.io/en/latest/contributing/) for how to get involved.

## Join the Community
Feel free to ask questions, provide feedbacks and discuss with fellow users of vLLM-Omni in `#sig-omni` slack channel at [slack.vllm.ai](https://slack.vllm.ai) or vLLM user forum at [discuss.vllm.ai](https://discuss.vllm.ai).

## License

Apache License 2.0, as found in the [LICENSE](./LICENSE) file.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Comfy-Org/ComfyUI-Manager]]></title>
            <link>https://github.com/Comfy-Org/ComfyUI-Manager</link>
            <guid>https://github.com/Comfy-Org/ComfyUI-Manager</guid>
            <pubDate>Tue, 23 Dec 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[ComfyUI-Manager is an extension designed to enhance the usability of ComfyUI. It offers management functions to install, remove, disable, and enable various custom nodes of ComfyUI. Furthermore, this extension provides a hub feature and convenience functions to access a wide range of information within ComfyUI.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Comfy-Org/ComfyUI-Manager">Comfy-Org/ComfyUI-Manager</a></h1>
            <p>ComfyUI-Manager is an extension designed to enhance the usability of ComfyUI. It offers management functions to install, remove, disable, and enable various custom nodes of ComfyUI. Furthermore, this extension provides a hub feature and convenience functions to access a wide range of information within ComfyUI.</p>
            <p>Language: Python</p>
            <p>Stars: 12,900</p>
            <p>Forks: 1,788</p>
            <p>Stars today: 21 stars today</p>
            <h2>README</h2><pre># ComfyUI Manager

**ComfyUI-Manager** is an extension designed to enhance the usability of [ComfyUI](https://github.com/comfyanonymous/ComfyUI). It offers management functions to **install, remove, disable, and enable** various custom nodes of ComfyUI. Furthermore, this extension provides a hub feature and convenience functions to access a wide range of information within ComfyUI.

![menu](https://raw.githubusercontent.com/ltdrdata/ComfyUI-extension-tutorials/refs/heads/Main/ComfyUI-Manager/images/dialog.jpg)

## NOTICE
* V3.38: **Security patch** - Manager data migrated to protected path. See [Migration Guide](docs/en/v3.38-userdata-security-migration.md).
* V3.16: Support for `uv` has been added. Set `use_uv` in `config.ini`.
* V3.10: `double-click feature` is removed
  * This feature has been moved to https://github.com/ltdrdata/comfyui-connection-helper
* V3.3.2: Overhauled. Officially supports [https://registry.comfy.org/](https://registry.comfy.org/).
* You can see whole nodes info on [ComfyUI Nodes Info](https://ltdrdata.github.io/) page.

## Installation

### Installation[method1] (General installation method: ComfyUI-Manager only)

To install ComfyUI-Manager in addition to an existing installation of ComfyUI, you can follow the following steps:

1. Go to `ComfyUI/custom_nodes` dir in terminal (cmd)
2. `git clone https://github.com/ltdrdata/ComfyUI-Manager comfyui-manager`
3. Restart ComfyUI


### Installation[method2] (Installation for portable ComfyUI version: ComfyUI-Manager only)
1. install git 
- https://git-scm.com/download/win
- standalone version  
- select option: use windows default console window
2. Download [scripts/install-manager-for-portable-version.bat](https://github.com/ltdrdata/ComfyUI-Manager/raw/main/scripts/install-manager-for-portable-version.bat) into installed `&quot;ComfyUI_windows_portable&quot;` directory
- Don&#039;t click. Right-click the link and choose &#039;Save As...&#039;
3. Double-click `install-manager-for-portable-version.bat` batch file

![portable-install](https://raw.githubusercontent.com/ltdrdata/ComfyUI-extension-tutorials/Main/ComfyUI-Manager/images/portable-install.jpg)


### Installation[method3] (Installation through comfy-cli: install ComfyUI and ComfyUI-Manager at once.)  
&gt; RECOMMENDED: comfy-cli provides various features to manage ComfyUI from the CLI.

* **prerequisite: python 3, git**

Windows:
```commandline
python -m venv venv
venv\Scripts\activate
pip install comfy-cli
comfy install
```

Linux/macOS:
```commandline
python -m venv venv
. venv/bin/activate
pip install comfy-cli
comfy install
```
* See also: https://github.com/Comfy-Org/comfy-cli


### Installation[method4] (Installation for Linux+venv: ComfyUI + ComfyUI-Manager)

To install ComfyUI with ComfyUI-Manager on Linux using a venv environment, you can follow these steps:
* **prerequisite: python-is-python3, python3-venv, git**

1. Download [scripts/install-comfyui-venv-linux.sh](https://github.com/ltdrdata/ComfyUI-Manager/raw/main/scripts/install-comfyui-venv-linux.sh) into empty install directory
- Don&#039;t click. Right-click the link and choose &#039;Save As...&#039;
- ComfyUI will be installed in the subdirectory of the specified directory, and the directory will contain the generated executable script.
2. `chmod +x install-comfyui-venv-linux.sh`
3. `./install-comfyui-venv-linux.sh`

### Installation Precautions
* **DO**: `ComfyUI-Manager` files must be accurately located in the path `ComfyUI/custom_nodes/comfyui-manager`
  * Installing in a compressed file format is not recommended.
* **DON&#039;T**: Decompress directly into the `ComfyUI/custom_nodes` location, resulting in the Manager contents like `__init__.py` being placed directly in that directory.
  * You have to remove all ComfyUI-Manager files from `ComfyUI/custom_nodes`
* **DON&#039;T**: In a form where decompression occurs in a path such as `ComfyUI/custom_nodes/ComfyUI-Manager/ComfyUI-Manager`.
* **DON&#039;T**: In a form where decompression occurs in a path such as `ComfyUI/custom_nodes/ComfyUI-Manager-main`.
  * In such cases, `ComfyUI-Manager` may operate, but it won&#039;t be recognized within `ComfyUI-Manager`, and updates cannot be performed. It also poses the risk of duplicate installations. Remove it and install properly via `git clone` method.


You can execute ComfyUI by running either `./run_gpu.sh` or `./run_cpu.sh` depending on your system configuration.

## Colab Notebook
This repository provides Colab notebooks that allow you to install and use ComfyUI, including ComfyUI-Manager. To use ComfyUI, [click on this link](https://colab.research.google.com/github/ltdrdata/ComfyUI-Manager/blob/main/notebooks/comfyui_colab_with_manager.ipynb).
* Support for installing ComfyUI
* Support for basic installation of ComfyUI-Manager
* Support for automatically installing dependencies of custom nodes upon restarting Colab notebooks.


## How To Use

1. Click &quot;Manager&quot; button on main menu

    ![mainmenu](https://raw.githubusercontent.com/ltdrdata/ComfyUI-extension-tutorials/Main/ComfyUI-Manager/images/topbar.jpg)


2. If you click on &#039;Install Custom Nodes&#039; or &#039;Install Models&#039;, an installer dialog will open.

    ![menu](https://raw.githubusercontent.com/ltdrdata/ComfyUI-extension-tutorials/refs/heads/Main/ComfyUI-Manager/images/dialog.jpg)

    * There are three DB modes: `DB: Channel (1day cache)`, `DB: Local`, and `DB: Channel (remote)`. 
      * `Channel (1day cache)` utilizes Channel cache information with a validity period of one day to quickly display the list.
        * This information will be updated when there is no cache, when the cache expires, or when external information is retrieved through the Channel (remote).
        * Whenever you start ComfyUI anew, this mode is always set as the **default** mode.
      * `Local` uses information stored locally in ComfyUI-Manager.
        * This information will be updated only when you update ComfyUI-Manager.
        * For custom node developers, they should use this mode when registering their nodes in `custom-node-list.json` and testing them.
      * `Channel (remote)` retrieves information from the remote channel, always displaying the latest list.
      * In cases where retrieval is not possible due to network errors, it will forcibly use local information.

    * The ```Fetch Updates``` menu retrieves update data for custom nodes locally. Actual updates are applied by clicking the ```Update``` button in the ```Install Custom Nodes``` menu.

3. Click &#039;Install&#039; or &#039;Try Install&#039; button.

    ![node-install-dialog](https://raw.githubusercontent.com/ltdrdata/ComfyUI-extension-tutorials/Main/ComfyUI-Manager/images/custom-nodes.jpg)

    ![model-install-dialog](https://raw.githubusercontent.com/ltdrdata/ComfyUI-extension-tutorials/Main/ComfyUI-Manager/images/models.jpg)

    * Installed: This item is already installed.
    * Install: Clicking this button will install the item.
    * Try Install: This is a custom node of which installation information cannot be confirmed. Click the button to try installing it.

    * If a red background `Channel` indicator appears at the top, it means it is not the default channel. Since the amount of information held is different from the default channel, many custom nodes may not appear in this channel state.
      * Channel settings have a broad impact, affecting not only the node list but also all functions like &quot;Update all.&quot;
    * Conflicted Nodes with a yellow background show a list of nodes conflicting with other extensions in the respective extension. This issue needs to be addressed by the developer, and users should be aware that due to these conflicts, some nodes may not function correctly and may need to be installed accordingly.

4. Share
  ![menu](https://raw.githubusercontent.com/ltdrdata/ComfyUI-extension-tutorials/Main/ComfyUI-Manager/images/topbar.jpg) ![share](https://raw.githubusercontent.com/ltdrdata/ComfyUI-extension-tutorials/Main/ComfyUI-Manager/images/share.jpg) 

  * You can share the workflow by clicking the Share button at the bottom of the main menu or selecting Share Output from the Context Menu of the Image node.
  * Currently, it supports sharing via [https://comfyworkflows.com/](https://comfyworkflows.com/),
    [https://openart.ai](https://openart.ai/workflows/dev), [https://youml.com](https://youml.com) 
    as well as through the Matrix channel.

  ![menu](https://raw.githubusercontent.com/ltdrdata/ComfyUI-extension-tutorials/Main/ComfyUI-Manager/images/share-setting.jpg)
  
  * Through the Share settings in the Manager menu, you can configure the behavior of the Share button in the Main menu or Share Output button on Context Menu.
    * `None`: hide from Main menu
    * `All`: Show a dialog where the user can select a title for sharing.


## Paths
Starting from V3.38, Manager uses a protected system path for enhanced security.

* &lt;USER_DIRECTORY&gt;
  * If executed without any options, the path defaults to ComfyUI/user.
  * It can be set using --user-directory &lt;USER_DIRECTORY&gt;.

| ComfyUI Version | Manager Path |
|-----------------|--------------|
| v0.3.76+ (with System User API) | `&lt;USER_DIRECTORY&gt;/__manager/` |
| Older versions | `&lt;USER_DIRECTORY&gt;/default/ComfyUI-Manager/` |

* Basic config files: `config.ini`
* Configurable channel lists: `channels.list`
* Configurable pip overrides: `pip_overrides.json`
* Configurable pip blacklist: `pip_blacklist.list`
* Configurable pip auto fix: `pip_auto_fix.list`
* Saved snapshot files: `snapshots/`
* Startup script files: `startup-scripts/`
* Component files: `components/`

&gt; **Note**: See [Migration Guide](docs/en/v3.38-userdata-security-migration.md) for upgrade details.


## `extra_model_paths.yaml` Configuration
The following settings are applied based on the section marked as `is_default`.

* `custom_nodes`: Path for installing custom nodes
    * Importing does not need to adhere to the path set as `is_default`, but this is the path where custom nodes are installed by the `ComfyUI Nodes Manager`.
* `download_model_base`: Path for downloading models


## Snapshot-Manager
* When you press `Save snapshot` or use `Update All` on `Manager Menu`, the current installation status snapshot is saved.
  * Snapshot file dir: `&lt;USER_DIRECTORY&gt;/default/ComfyUI-Manager/snapshots`
  * You can rename snapshot file.
* Press the &quot;Restore&quot; button to revert to the installation status of the respective snapshot.
  * However, for custom nodes not managed by Git, snapshot support is incomplete.
* When you press `Restore`, it will take effect on the next ComfyUI startup.
  * The selected snapshot file is saved in `&lt;USER_DIRECTORY&gt;/default/ComfyUI-Manager/startup-scripts/restore-snapshot.json`, and upon restarting ComfyUI, the snapshot is applied and then deleted.

![model-install-dialog](https://raw.githubusercontent.com/ltdrdata/ComfyUI-extension-tutorials/Main/ComfyUI-Manager/images/snapshot.jpg)


## cm-cli: command line tools for power users
* A tool is provided that allows you to use the features of ComfyUI-Manager without running ComfyUI.
* For more details, please refer to the [cm-cli documentation](docs/en/cm-cli.md).


## How to register your custom node into ComfyUI-Manager

* Add an entry to `custom-node-list.json` located in the root of ComfyUI-Manager and submit a Pull Request.
* NOTE: Before submitting the PR after making changes, please check `Use local DB` and ensure that the extension list loads without any issues in the `Install custom nodes` dialog. Occasionally, missing or extra commas can lead to JSON syntax errors.
* The remaining JSON will be updated through scripts in the future, so you don&#039;t need to worry about it.


## Custom node support guide

* **NOTICE:**
    - You should no longer assume that the GitHub repository name will match the subdirectory name under `custom_nodes`. The name of the subdirectory under `custom_nodes` will now use the normalized name from the `name` field in `pyproject.toml`.
    - Avoid relying on directory names for imports whenever possible.

* https://docs.comfy.org/registry/overview
* https://github.com/Comfy-Org/rfcs

**Special purpose files** (optional)
  * `pyproject.toml` - Spec file for comfyregistry.
  * `node_list.json` - When your custom nodes pattern of NODE_CLASS_MAPPINGS is not conventional, it is used to manually provide a list of nodes for reference. ([example](https://github.com/melMass/comfy_mtb/raw/main/node_list.json))
  * `requirements.txt` - When installing, this pip requirements will be installed automatically 
  * `install.py` - When installing, it is automatically called
  * **All scripts are executed from the root path of the corresponding custom node.**


## Component Sharing
* **Copy &amp; Paste**
  * [Demo Page](https://ltdrdata.github.io/component-demo/)
  * When pasting a component from the clipboard, it supports text in the following JSON format. (text/plain)
    ```
    {
      &quot;kind&quot;: &quot;ComfyUI Components&quot;,
      &quot;timestamp&quot;: &lt;current timestamp&gt;,
      &quot;components&quot;: 
        {
          &lt;component name&gt;: &lt;component nodedata&gt;
        }
    }
    ```
  * `&lt;current timestamp&gt;` Ensure that the timestamp is always unique.
    * &quot;components&quot; should have the same structure as the content of the file stored in `&lt;USER_DIRECTORY&gt;/default/ComfyUI-Manager/components`.
      * `&lt;component name&gt;`: The name should be in the format `&lt;prefix&gt;::&lt;node name&gt;`.
        * `&lt;component node data&gt;`: In the node data of the group node.
          * `&lt;version&gt;`: Only two formats are allowed: `major.minor.patch` or `major.minor`. (e.g. `1.0`, `2.2.1`)
          * `&lt;datetime&gt;`: Saved time
          * `&lt;packname&gt;`: If the packname is not empty, the category becomes packname/workflow, and it is saved in the &lt;packname&gt;.pack file in `&lt;USER_DIRECTORY&gt;/default/ComfyUI-Manager/components`.
          * `&lt;category&gt;`: If there is neither a category nor a packname, it is saved in the components category.
          ```
              &quot;version&quot;:&quot;1.0&quot;,
              &quot;datetime&quot;: 1705390656516,
              &quot;packname&quot;: &quot;mypack&quot;,
              &quot;category&quot;: &quot;util/pipe&quot;,
          ```
* **Drag &amp; Drop**
  * Dragging and dropping a `.pack` or `.json` file will add the corresponding components.
  * Example pack: [Impact.pack](misc/Impact.pack)

* Dragging and dropping or pasting a single component will add a node. However, when adding multiple components, nodes will not be added.


## Support for installing missing nodes

![missing-menu](https://raw.githubusercontent.com/ltdrdata/ComfyUI-extension-tutorials/Main/ComfyUI-Manager/images/missing-menu.jpg)

* When you click on the ```Install Missing Custom Nodes``` button in the menu, it displays a list of extension nodes that contain nodes not currently present in the workflow.

![missing-list](https://raw.githubusercontent.com/ltdrdata/ComfyUI-extension-tutorials/Main/ComfyUI-Manager/images/missing-list.jpg)


# Config
* You can modify the `config.ini` file to apply the settings for ComfyUI-Manager.
    * The path to the `config.ini` used by ComfyUI-Manager is displayed in the startup log messages.
    * See also: [https://github.com/ltdrdata/ComfyUI-Manager#paths]
* Configuration options:
    ```
    [default]
    git_exe = &lt;Manually specify the path to the git executable. If left empty, the default git executable path will be used.&gt;
    use_uv = &lt;Use uv instead of pip for dependency installation.&gt;
    default_cache_as_channel_url = &lt;Determines whether to retrieve the DB designated as channel_url at startup&gt;
    bypass_ssl = &lt;Set to True if SSL errors occur to disable SSL.&gt;
    file_logging = &lt;Configure whether to create a log file used by ComfyUI-Manager.&gt;
    windows_selector_event_loop_policy = &lt;If an event loop error occurs on Windows, set this to True.&gt;
    model_download_by_agent = &lt;When downloading models, use an agent instead of torchvision_download_url.&gt;
    downgrade_blacklist = &lt;Set a list of packages to prevent downgrades. List them separated by commas.&gt;
    security_level = &lt;Set the security level =&gt; strong|normal|normal-|weak&gt;
    always_lazy_install = &lt;Whether to perform dependency installation on restart even in environments other than Windows.&gt;
    network_mode = &lt;Set the network mode =&gt; public|private|offline&gt;
    ```

    * network_mode:
      - public: An environment that uses a typical public network.
      - private: An environment that uses a closed network, where a private node DB is configured via `channel_url`. (Uses cache if available)
      - offline: An environment that does not use any external connections when using an offline network. (Uses cache if available)


## Additional Feature
* Logging to file feature
  * This feature is enabled by default and can be disabled by setting `file_logging = False` in the `config.ini`.

* Fix node (recreate): When right-clicking on a node and selecting `Fix node (recreate)`, you can recreate the node. The widget&#039;s values are reset, while the connections maintain those with the same names.
  * It is used to correct errors in nodes of old workflows created before, which are incompatible with the version changes of custom nodes.

* Double-Click Node Title: You can set the double-click behavior of nodes in the ComfyUI-Manager menu.
  * `Copy All Connections`, `Copy Input Connections`: Double-clicking a node copies the connections of the nearest node.
    * This action targets the nearest node within a straight-line distance of 1000 pixels from the center of the node.
    * In the case of `Copy All Connections`, it duplicates existing outputs, but since it does not allow duplicate connections, the existing output connections of the original node are disconnected.
    * This feature copies only the input and output that match the names.
  
  * `Possible Input Connections`: It connects all outputs that match the closest type within the specified range.
    * This connection links to the closest outputs among the nodes located on the left side of the target node.
    
  * `Possible(left) + Copy(right)`: When you Double-Click on the left half of the title, it operates as `Possible Input Connections`, and when you Double-Click on the right half, it operates as `Copy All Connections`.

* Prevent downgrade of specific packages
  * List the package names in the `downgrade_blacklist` section of the `config.ini` file, separating them with commas.
    * e.g
    ```
      downgrade_blacklist = diffusers, kornia
    ```

* Custom pip mapping
  * When you create the `pip_overrides.json` file, it changes the installation of specific pip packages to installations defined by the user.
    * Please refer to the `pip_overrides.json.template` file.

* Prevent the installation of specific pip packages
  * List the package names one per line in the `pip_blacklist.list` file.

* Automatically Restoring pip Installation
 * If you list pip spec requirements in `pip_auto_fix.list`, similar to `requirements.txt`, it will automatically restore the specified versions when starting ComfyUI or when versions get mismatched during various custom node installations.
 * `--index-url` can be used.

* Use `aria2` as downloader
  * [howto](docs/en/use_aria2.md)


## Environment Variables

The following features can be configured using environment variables:

* **COMFYUI_PATH**: The installation path of ComfyUI
* **GITHUB_ENDPOINT**: Reverse proxy configuration for environments with limited access to GitHub
* **HF_ENDPOINT**: Reverse proxy configuration for environments with limited access to Hugging Face


### Example 1:
Redirecting `https://github.com/ltdrdata/ComfyUI-Impact-Pack` to `https://mirror.ghproxy.com/https://github.com/ltdrdata/ComfyUI-Impact-Pack`

```
GITHUB_ENDPOINT=https://mirror.ghproxy.com/https://github.com
```

#### Example 2:
Changing `https://huggingface.co/path/to/somewhere` to `https://some-hf-mirror.com/path/to/somewhere`

```
HF_ENDPOINT=https://some-hf-mirror.com 
```

## Scanner
When you run the `scan.sh` script:

* It updates the `extension-node-map.json`.
  * To do this, it pulls or clones the custom nodes lis

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>