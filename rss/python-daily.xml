<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Fri, 11 Apr 2025 00:04:37 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[NVIDIA/cuda-python]]></title>
            <link>https://github.com/NVIDIA/cuda-python</link>
            <guid>https://github.com/NVIDIA/cuda-python</guid>
            <pubDate>Fri, 11 Apr 2025 00:04:37 GMT</pubDate>
            <description><![CDATA[CUDA Python: Performance meets Productivity]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NVIDIA/cuda-python">NVIDIA/cuda-python</a></h1>
            <p>CUDA Python: Performance meets Productivity</p>
            <p>Language: Python</p>
            <p>Stars: 1,912</p>
            <p>Forks: 128</p>
            <p>Stars today: 44 stars today</p>
            <h2>README</h2><pre># cuda-python

CUDA Python is the home for accessing NVIDIA‚Äôs CUDA platform from Python. It consists of multiple components:

* [cuda.core](https://nvidia.github.io/cuda-python/cuda-core/latest): Pythonic access to CUDA Runtime and other core functionalities
* [cuda.bindings](https://nvidia.github.io/cuda-python/cuda-bindings/latest): Low-level Python bindings to CUDA C APIs
* [cuda.cooperative](https://nvidia.github.io/cccl/cuda_cooperative/): A Python package providing CCCL&#039;s reusable block-wide and warp-wide *device* primitives for use within Numba CUDA kernels
* [cuda.parallel](https://nvidia.github.io/cccl/cuda_parallel/): A Python package for easy access to CCCL&#039;s highly efficient and customizable parallel algorithms, like `sort`, `scan`, `reduce`, `transform`, etc, that are callable on the *host*
* [numba.cuda](https://nvidia.github.io/numba-cuda/): Numba&#039;s target for CUDA GPU programming by directly compiling a restricted subset of Python code into CUDA kernels and device functions following the CUDA execution model.

For access to NVIDIA CPU &amp; GPU Math Libraries, please refer to [nvmath-python](https://docs.nvidia.com/cuda/nvmath-python/latest).

CUDA Python is currently undergoing an overhaul to improve existing and bring up new components. All of the previously available functionalities from the `cuda-python` package will continue to be available, please refer to the [cuda.bindings](https://nvidia.github.io/cuda-python/cuda-bindings/latest) documentation for installation guide and further detail.

## cuda-python as a metapackage

`cuda-python` is being re-structured to become a metapackage that contains a collection of subpackages. Each subpackage is versioned independently, allowing installation of each component as needed.

### Subpackage: `cuda.core`

The `cuda.core` package offers idiomatic, pythonic access to CUDA Runtime and other functionalities.

The goals are to

1. Provide **idiomatic (&quot;pythonic&quot;)** access to CUDA Driver, Runtime, and JIT compiler toolchain
2. Focus on **developer productivity** by ensuring end-to-end CUDA development can be performed quickly and entirely in Python
3. **Avoid homegrown** Python abstractions for CUDA for new Python GPU libraries starting from scratch
4. **Ease** developer **burden of maintaining** and catching up with latest CUDA features
5. **Flatten the learning curve** for current and future generations of CUDA developers

### Subpackage: `cuda.bindings`

The `cuda.bindings` package is a standard set of low-level interfaces, providing full coverage of and access to the CUDA host APIs from Python.

The list of available interfaces are:

* CUDA Driver
* CUDA Runtime
* NVRTC
* nvJitLink
* NVVM
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/markitdown]]></title>
            <link>https://github.com/microsoft/markitdown</link>
            <guid>https://github.com/microsoft/markitdown</guid>
            <pubDate>Fri, 11 Apr 2025 00:04:36 GMT</pubDate>
            <description><![CDATA[Python tool for converting files and office documents to Markdown.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/markitdown">microsoft/markitdown</a></h1>
            <p>Python tool for converting files and office documents to Markdown.</p>
            <p>Language: Python</p>
            <p>Stars: 47,752</p>
            <p>Forks: 2,249</p>
            <p>Stars today: 642 stars today</p>
            <h2>README</h2><pre># MarkItDown

[![PyPI](https://img.shields.io/pypi/v/markitdown.svg)](https://pypi.org/project/markitdown/)
![PyPI - Downloads](https://img.shields.io/pypi/dd/markitdown)
[![Built by AutoGen Team](https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue)](https://github.com/microsoft/autogen)

&gt; [!TIP]
&gt; MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See [markitdown-mcp](https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp) for more information.

&gt; [!IMPORTANT]
&gt; Breaking changes between 0.0.1 to 0.1.0:
&gt; * Dependencies are now organized into optional feature-groups (further details below). Use `pip install &#039;markitdown[all]&#039;` to have backward-compatible behavior. 
&gt; * convert\_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.
&gt; * The DocumentConverter class interface has changed to read from file-like streams rather than file paths. *No temporary files are created anymore*. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.

MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to [textract](https://github.com/deanmalmgren/textract), but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.

At present, MarkItDown supports:

- PDF
- PowerPoint
- Word
- Excel
- Images (EXIF metadata and OCR)
- Audio (EXIF metadata and speech transcription)
- HTML
- Text-based formats (CSV, JSON, XML)
- ZIP files (iterates over contents)
- Youtube URLs
- EPubs
- ... and more!

## Why Markdown?

Markdown is extremely close to plain text, with minimal markup or formatting, but still
provides a way to represent important document structure. Mainstream LLMs, such as
OpenAI&#039;s GPT-4o, natively &quot;_speak_&quot; Markdown, and often incorporate Markdown into their
responses unprompted. This suggests that they have been trained on vast amounts of
Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions
are also highly token-efficient.

## Installation

To install MarkItDown, use pip: `pip install &#039;markitdown[all]&#039;`. Alternatively, you can install it from the source:

```bash
git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e packages/markitdown[all]
```

## Usage

### Command-Line

```bash
markitdown path-to-file.pdf &gt; document.md
```

Or use `-o` to specify the output file:

```bash
markitdown path-to-file.pdf -o document.md
```

You can also pipe content:

```bash
cat path-to-file.pdf | markitdown
```

### Optional Dependencies
MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the `[all]` option. However, you can also install them individually for more control. For example:

```bash
pip install markitdown[pdf, docx, pptx]
```

will install only the dependencies for PDF, DOCX, and PPTX files.

At the moment, the following optional dependencies are available:

* `[all]` Installs all optional dependencies
* `[pptx]` Installs dependencies for PowerPoint files
* `[docx]` Installs dependencies for Word files
* `[xlsx]` Installs dependencies for Excel files
* `[xls]` Installs dependencies for older Excel files
* `[pdf]` Installs dependencies for PDF files
* `[outlook]` Installs dependencies for Outlook messages
* `[az-doc-intel]` Installs dependencies for Azure Document Intelligence
* `[audio-transcription]` Installs dependencies for audio transcription of wav and mp3 files
* `[youtube-transcription]` Installs dependencies for fetching YouTube video transcription

### Plugins

MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:

```bash
markitdown --list-plugins
```

To enable plugins use:

```bash
markitdown --use-plugins path-to-file.pdf
```

To find available plugins, search GitHub for the hashtag `#markitdown-plugin`. To develop a plugin, see `packages/markitdown-sample-plugin`.

### Azure Document Intelligence

To use Microsoft Document Intelligence for conversion:

```bash
markitdown path-to-file.pdf -o document.md -d -e &quot;&lt;document_intelligence_endpoint&gt;&quot;
```

More information about how to set up an Azure Document Intelligence Resource can be found [here](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0)

### Python API

Basic usage in Python:

```python
from markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert(&quot;test.xlsx&quot;)
print(result.text_content)
```

Document Intelligence conversion in Python:

```python
from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint=&quot;&lt;document_intelligence_endpoint&gt;&quot;)
result = md.convert(&quot;test.pdf&quot;)
print(result.text_content)
```

To use Large Language Models for image descriptions, provide `llm_client` and `llm_model`:

```python
from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model=&quot;gpt-4o&quot;)
result = md.convert(&quot;example.jpg&quot;)
print(result.text_content)
```

### Docker

```sh
docker build -t markitdown:latest .
docker run --rm -i markitdown:latest &lt; ~/your-file.pdf &gt; output.md
```

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

### How to Contribute

You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as &#039;open for contribution&#039; and &#039;open for reviewing&#039; to help facilitate community contributions. These are ofcourse just suggestions and you are welcome to contribute in any way you like.

&lt;div align=&quot;center&quot;&gt;

|            | All                                                          | Especially Needs Help from Community                                                                                                      |
| ---------- | ------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------- |
| **Issues** | [All Issues](https://github.com/microsoft/markitdown/issues) | [Issues open for contribution](https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22) |
| **PRs**    | [All PRs](https://github.com/microsoft/markitdown/pulls)     | [PRs open for reviewing](https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22)              |

&lt;/div&gt;

### Running Tests and Checks

- Navigate to the MarkItDown package:

  ```sh
  cd packages/markitdown
  ```

- Install `hatch` in your environment and run tests:

  ```sh
  pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
  hatch shell
  hatch test
  ```

  (Alternative) Use the Devcontainer which has all the dependencies installed:

  ```sh
  # Reopen the project in Devcontainer and run:
  hatch test
  ```

- Run pre-commit checks before submitting a PR: `pre-commit run --all-files`

### Contributing 3rd-party Plugins

You can also contribute by creating and sharing 3rd party plugins. See `packages/markitdown-sample-plugin` for more details.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
[Microsoft&#039;s Trademark &amp; Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party&#039;s policies.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[HKUDS/LightRAG]]></title>
            <link>https://github.com/HKUDS/LightRAG</link>
            <guid>https://github.com/HKUDS/LightRAG</guid>
            <pubDate>Fri, 11 Apr 2025 00:04:35 GMT</pubDate>
            <description><![CDATA["LightRAG: Simple and Fast Retrieval-Augmented Generation"]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/HKUDS/LightRAG">HKUDS/LightRAG</a></h1>
            <p>"LightRAG: Simple and Fast Retrieval-Augmented Generation"</p>
            <p>Language: Python</p>
            <p>Stars: 14,708</p>
            <p>Forks: 2,025</p>
            <p>Stars today: 295 stars today</p>
            <h2>README</h2><pre>&lt;center&gt;&lt;h2&gt;üöÄ LightRAG: Simple and Fast Retrieval-Augmented Generation&lt;/h2&gt;&lt;/center&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;table border=&quot;0&quot; width=&quot;100%&quot;&gt;
&lt;tr&gt;
&lt;td width=&quot;100&quot; align=&quot;center&quot;&gt;
&lt;img src=&quot;./assets/logo.png&quot; width=&quot;80&quot; height=&quot;80&quot; alt=&quot;lightrag&quot;&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;div&gt;
    &lt;p&gt;
        &lt;a href=&#039;https://lightrag.github.io&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/Project-Page-Green&#039;&gt;&lt;/a&gt;
        &lt;a href=&#039;https://youtu.be/oageL-1I0GE&#039;&gt;&lt;img src=&#039;https://badges.aleen42.com/src/youtube.svg&#039;&gt;&lt;/a&gt;
        &lt;a href=&#039;https://arxiv.org/abs/2410.05779&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/arXiv-2410.05779-b31b1b&#039;&gt;&lt;/a&gt;
        &lt;a href=&#039;https://learnopencv.com/lightrag&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/LearnOpenCV-blue&#039;&gt;&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
        &lt;img src=&#039;https://img.shields.io/github/stars/hkuds/lightrag?color=green&amp;style=social&#039; /&gt;
        &lt;img src=&quot;https://img.shields.io/badge/python-3.10-blue&quot;&gt;
        &lt;a href=&quot;https://pypi.org/project/lightrag-hku/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/lightrag-hku.svg&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://pepy.tech/project/lightrag-hku&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/lightrag-hku/month&quot;&gt;&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
        &lt;a href=&#039;https://discord.gg/yF2MmDJyGJ&#039;&gt;&lt;img src=&#039;https://discordapp.com/api/guilds/1296348098003734629/widget.png?style=shield&#039;&gt;&lt;/a&gt;
        &lt;a href=&#039;https://github.com/HKUDS/LightRAG/issues/285&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/Áæ§ËÅä-wechat-green&#039;&gt;&lt;/a&gt;
    &lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;img src=&quot;./README.assets/b2aaf634151b4706892693ffb43d9093.png&quot; width=&quot;800&quot; alt=&quot;LightRAG Diagram&quot;&gt;

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://trendshift.io/repositories/13043&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13043&quot; alt=&quot;HKUDS%2FLightRAG | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

## üéâ News

- [X] [2025.03.18]üéØüì¢LightRAG now supports citation functionality, enabling proper source attribution.
- [X] [2025.02.05]üéØüì¢Our team has released [VideoRAG](https://github.com/HKUDS/VideoRAG) understanding extremely long-context videos.
- [X] [2025.01.13]üéØüì¢Our team has released [MiniRAG](https://github.com/HKUDS/MiniRAG) making RAG simpler with small models.
- [X] [2025.01.06]üéØüì¢You can now [use PostgreSQL for Storage](#using-postgresql-for-storage).
- [X] [2024.12.31]üéØüì¢LightRAG now supports [deletion by document ID](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#delete).
- [X] [2024.11.25]üéØüì¢LightRAG now supports seamless integration of [custom knowledge graphs](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#insert-custom-kg), empowering users to enhance the system with their own domain expertise.
- [X] [2024.11.19]üéØüì¢A comprehensive guide to LightRAG is now available on [LearnOpenCV](https://learnopencv.com/lightrag). Many thanks to the blog author.
- [X] [2024.11.11]üéØüì¢LightRAG now supports [deleting entities by their names](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#delete).
- [X] [2024.11.09]üéØüì¢Introducing the [LightRAG Gui](https://lightrag-gui.streamlit.app), which allows you to insert, query, visualize, and download LightRAG knowledge.
- [X] [2024.11.04]üéØüì¢You can now [use Neo4J for Storage](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#using-neo4j-for-storage).
- [X] [2024.10.29]üéØüì¢LightRAG now supports multiple file types, including PDF, DOC, PPT, and CSV via `textract`.
- [X] [2024.10.20]üéØüì¢We&#039;ve added a new feature to LightRAG: Graph Visualization.
- [X] [2024.10.18]üéØüì¢We&#039;ve added a link to a [LightRAG Introduction Video](https://youtu.be/oageL-1I0GE). Thanks to the author!
- [X] [2024.10.17]üéØüì¢We have created a [Discord channel](https://discord.gg/yF2MmDJyGJ)! Welcome to join for sharing and discussions! üéâüéâ
- [X] [2024.10.16]üéØüì¢LightRAG now supports [Ollama models](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#quick-start)!
- [X] [2024.10.15]üéØüì¢LightRAG now supports [Hugging Face models](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#quick-start)!

&lt;details&gt;
  &lt;summary style=&quot;font-size: 1.4em; font-weight: bold; cursor: pointer; display: list-item;&quot;&gt;
    Algorithm Flowchart
  &lt;/summary&gt;

![LightRAG Indexing Flowchart](https://learnopencv.com/wp-content/uploads/2024/11/LightRAG-VectorDB-Json-KV-Store-Indexing-Flowchart-scaled.jpg)
*Figure 1: LightRAG Indexing Flowchart - Img Caption : [Source](https://learnopencv.com/lightrag/)*
![LightRAG Retrieval and Querying Flowchart](https://learnopencv.com/wp-content/uploads/2024/11/LightRAG-Querying-Flowchart-Dual-Level-Retrieval-Generation-Knowledge-Graphs-scaled.jpg)
*Figure 2: LightRAG Retrieval and Querying Flowchart - Img Caption : [Source](https://learnopencv.com/lightrag/)*

&lt;/details&gt;

## Installation

### Install  LightRAG Core

* Install from source (Recommend)

```bash
cd LightRAG
pip install -e .
```

* Install from PyPI

```bash
pip install lightrag-hku
```

### Install LightRAG Server

The LightRAG Server is designed to provide Web UI and API support. The Web UI facilitates document indexing, knowledge graph exploration, and a simple RAG query interface. LightRAG Server also provide an Ollama compatible interfaces, aiming to emulate LightRAG as an Ollama chat model. This allows AI chat bot, such as Open WebUI, to access LightRAG easily.

* Install from PyPI

```bash
pip install &quot;lightrag-hku[api]&quot;
```

* Installation from Source

```bash
# create a Python virtual enviroment if neccesary
# Install in editable mode with API support
pip install -e &quot;.[api]&quot;
```

**For more information about LightRAG Server, please refer to [LightRAG Server](./lightrag/api/README.md).**

## Quick Start

* [Video demo](https://www.youtube.com/watch?v=g21royNJ4fw) of running LightRAG locally.
* All the code can be found in the `examples`.
* Set OpenAI API key in environment if using OpenAI models: `export OPENAI_API_KEY=&quot;sk-...&quot;.`
* Download the demo text &quot;A Christmas Carol by Charles Dickens&quot;:

```bash
curl https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/tests/mock_data.txt &gt; ./book.txt
```

## Query

Use the below Python snippet (in a script) to initialize LightRAG and perform queries:

```python
import os
import asyncio
from lightrag import LightRAG, QueryParam
from lightrag.llm.openai import gpt_4o_mini_complete, gpt_4o_complete, openai_embed
from lightrag.kg.shared_storage import initialize_pipeline_status
from lightrag.utils import setup_logger

setup_logger(&quot;lightrag&quot;, level=&quot;INFO&quot;)

async def initialize_rag():
    rag = LightRAG(
        working_dir=&quot;your/path&quot;,
        embedding_func=openai_embed,
        llm_model_func=gpt_4o_mini_complete
    )

    await rag.initialize_storages()
    await initialize_pipeline_status()

    return rag

def main():
    # Initialize RAG instance
    rag = asyncio.run(initialize_rag())
    # Insert text
    rag.insert(&quot;Your text&quot;)

    # Perform naive search
    mode=&quot;naive&quot;
    # Perform local search
    mode=&quot;local&quot;
    # Perform global search
    mode=&quot;global&quot;
    # Perform hybrid search
    mode=&quot;hybrid&quot;
    # Mix mode Integrates knowledge graph and vector retrieval.
    mode=&quot;mix&quot;

    rag.query(
        &quot;What are the top themes in this story?&quot;,
        param=QueryParam(mode=mode)
    )

if __name__ == &quot;__main__&quot;:
    main()
```

### Query Param

```python
class QueryParam:
    mode: Literal[&quot;local&quot;, &quot;global&quot;, &quot;hybrid&quot;, &quot;naive&quot;, &quot;mix&quot;] = &quot;global&quot;
    &quot;&quot;&quot;Specifies the retrieval mode:
    - &quot;local&quot;: Focuses on context-dependent information.
    - &quot;global&quot;: Utilizes global knowledge.
    - &quot;hybrid&quot;: Combines local and global retrieval methods.
    - &quot;naive&quot;: Performs a basic search without advanced techniques.
    - &quot;mix&quot;: Integrates knowledge graph and vector retrieval. Mix mode combines knowledge graph and vector search:
        - Uses both structured (KG) and unstructured (vector) information
        - Provides comprehensive answers by analyzing relationships and context
        - Supports image content through HTML img tags
        - Allows control over retrieval depth via top_k parameter
    &quot;&quot;&quot;
    only_need_context: bool = False
    &quot;&quot;&quot;If True, only returns the retrieved context without generating a response.&quot;&quot;&quot;
    response_type: str = &quot;Multiple Paragraphs&quot;
    &quot;&quot;&quot;Defines the response format. Examples: &#039;Multiple Paragraphs&#039;, &#039;Single Paragraph&#039;, &#039;Bullet Points&#039;.&quot;&quot;&quot;
    top_k: int = 60
    &quot;&quot;&quot;Number of top items to retrieve. Represents entities in &#039;local&#039; mode and relationships in &#039;global&#039; mode.&quot;&quot;&quot;
    max_token_for_text_unit: int = 4000
    &quot;&quot;&quot;Maximum number of tokens allowed for each retrieved text chunk.&quot;&quot;&quot;
    max_token_for_global_context: int = 4000
    &quot;&quot;&quot;Maximum number of tokens allocated for relationship descriptions in global retrieval.&quot;&quot;&quot;
    max_token_for_local_context: int = 4000
    &quot;&quot;&quot;Maximum number of tokens allocated for entity descriptions in local retrieval.&quot;&quot;&quot;
    ids: list[str] | None = None # ONLY SUPPORTED FOR PG VECTOR DBs
    &quot;&quot;&quot;List of ids to filter the RAG.&quot;&quot;&quot;
    model_func: Callable[..., object] | None = None
    &quot;&quot;&quot;Optional override for the LLM model function to use for this specific query.
    If provided, this will be used instead of the global model function.
    This allows using different models for different query modes.
    &quot;&quot;&quot;
    ...
```

&gt; default value of Top_k can be change by environment  variables  TOP_K.

### LLM and Embedding Injection

LightRAG requires the utilization of LLM and Embedding models to accomplish document indexing and querying tasks. During the initialization phase, it is necessary to inject the invocation methods of the relevant models into LightRAGÔºö

&lt;details&gt;
&lt;summary&gt; &lt;b&gt;Using Open AI-like APIs&lt;/b&gt; &lt;/summary&gt;

* LightRAG also supports Open AI-like chat/embeddings APIs:

```python
async def llm_model_func(
    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
) -&gt; str:
    return await openai_complete_if_cache(
        &quot;solar-mini&quot;,
        prompt,
        system_prompt=system_prompt,
        history_messages=history_messages,
        api_key=os.getenv(&quot;UPSTAGE_API_KEY&quot;),
        base_url=&quot;https://api.upstage.ai/v1/solar&quot;,
        **kwargs
    )

async def embedding_func(texts: list[str]) -&gt; np.ndarray:
    return await openai_embed(
        texts,
        model=&quot;solar-embedding-1-large-query&quot;,
        api_key=os.getenv(&quot;UPSTAGE_API_KEY&quot;),
        base_url=&quot;https://api.upstage.ai/v1/solar&quot;
    )

async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=llm_model_func,
        embedding_func=EmbeddingFunc(
            embedding_dim=4096,
            max_token_size=8192,
            func=embedding_func
        )
    )

    await rag.initialize_storages()
    await initialize_pipeline_status()

    return rag
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt; &lt;b&gt;Using Hugging Face Models&lt;/b&gt; &lt;/summary&gt;

* If you want to use Hugging Face models, you only need to set LightRAG as follows:

See `lightrag_hf_demo.py`

```python
# Initialize LightRAG with Hugging Face model
rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=hf_model_complete,  # Use Hugging Face model for text generation
    llm_model_name=&#039;meta-llama/Llama-3.1-8B-Instruct&#039;,  # Model name from Hugging Face
    # Use Hugging Face embedding function
    embedding_func=EmbeddingFunc(
        embedding_dim=384,
        max_token_size=5000,
        func=lambda texts: hf_embed(
            texts,
            tokenizer=AutoTokenizer.from_pretrained(&quot;sentence-transformers/all-MiniLM-L6-v2&quot;),
            embed_model=AutoModel.from_pretrained(&quot;sentence-transformers/all-MiniLM-L6-v2&quot;)
        )
    ),
)
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt; &lt;b&gt;Using Ollama Models&lt;/b&gt; &lt;/summary&gt;
**Overview**

If you want to use Ollama models, you need to pull model you plan to use and embedding model, for example `nomic-embed-text`.

Then you only need to set LightRAG as follows:

```python
# Initialize LightRAG with Ollama model
rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=ollama_model_complete,  # Use Ollama model for text generation
    llm_model_name=&#039;your_model_name&#039;, # Your model name
    # Use Ollama embedding function
    embedding_func=EmbeddingFunc(
        embedding_dim=768,
        max_token_size=8192,
        func=lambda texts: ollama_embed(
            texts,
            embed_model=&quot;nomic-embed-text&quot;
        )
    ),
)
```

* **Increasing context size**

In order for LightRAG to work context should be at least 32k tokens. By default Ollama models have context size of 8k. You can achieve this using one of two ways:

* **Increasing the `num_ctx` parameter in Modelfile**

1. Pull the model:

```bash
ollama pull qwen2
```

2. Display the model file:

```bash
ollama show --modelfile qwen2 &gt; Modelfile
```

3. Edit the Modelfile by adding the following line:

```bash
PARAMETER num_ctx 32768
```

4. Create the modified model:

```bash
ollama create -f Modelfile qwen2m
```

* **Setup `num_ctx` via Ollama API**

Tiy can use `llm_model_kwargs` param to configure ollama:

```python
rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=ollama_model_complete,  # Use Ollama model for text generation
    llm_model_name=&#039;your_model_name&#039;, # Your model name
    llm_model_kwargs={&quot;options&quot;: {&quot;num_ctx&quot;: 32768}},
    # Use Ollama embedding function
    embedding_func=EmbeddingFunc(
        embedding_dim=768,
        max_token_size=8192,
        func=lambda texts: ollama_embedding(
            texts,
            embed_model=&quot;nomic-embed-text&quot;
        )
    ),
)
```

* **Low RAM GPUs**

In order to run this experiment on low RAM GPU you should select small model and tune context window (increasing context increase memory consumption). For example, running this ollama example on repurposed mining GPU with 6Gb of RAM required to set context size to 26k while using `gemma2:2b`. It was able to find 197 entities and 19 relations on `book.txt`.

&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt; &lt;b&gt;LlamaIndex&lt;/b&gt; &lt;/summary&gt;

LightRAG supports integration with LlamaIndex (`llm/llama_index_impl.py`):

- Integrates with OpenAI and other providers through LlamaIndex
- See [LlamaIndex Documentation](lightrag/llm/Readme.md) for detailed setup and examples

**Example Usage**

```python
# Using LlamaIndex with direct OpenAI access
import asyncio
from lightrag import LightRAG
from lightrag.llm.llama_index_impl import llama_index_complete_if_cache, llama_index_embed
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from lightrag.kg.shared_storage import initialize_pipeline_status
from lightrag.utils import setup_logger

# Setup log handler for LightRAG
setup_logger(&quot;lightrag&quot;, level=&quot;INFO&quot;)

async def initialize_rag():
    rag = LightRAG(
        working_dir=&quot;your/path&quot;,
        llm_model_func=llama_index_complete_if_cache,  # LlamaIndex-compatible completion function
        embedding_func=EmbeddingFunc(    # LlamaIndex-compatible embedding function
            embedding_dim=1536,
            max_token_size=8192,
            func=lambda texts: llama_index_embed(texts, embed_model=embed_model)
        ),
    )

    await rag.initialize_storages()
    await initialize_pipeline_status()

    return rag

def main():
    # Initialize RAG instance
    rag = asyncio.run(initialize_rag())

    with open(&quot;./book.txt&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
        rag.insert(f.read())

    # Perform naive search
    print(
        rag.query(&quot;What are the top themes in this story?&quot;, param=QueryParam(mode=&quot;naive&quot;))
    )

    # Perform local search
    print(
        rag.query(&quot;What are the top themes in this story?&quot;, param=QueryParam(mode=&quot;local&quot;))
    )

    # Perform global search
    print(
        rag.query(&quot;What are the top themes in this story?&quot;, param=QueryParam(mode=&quot;global&quot;))
    )

    # Perform hybrid search
    print(
        rag.query(&quot;What are the top themes in this story?&quot;, param=QueryParam(mode=&quot;hybrid&quot;))
    )

if __name__ == &quot;__main__&quot;:
    main()
```

**For detailed documentation and examples, see:**

- [LlamaIndex Documentation](lightrag/llm/Readme.md)
- [Direct OpenAI Example](examples/lightrag_llamaindex_direct_demo.py)
- [LiteLLM Proxy Example](examples/lightrag_llamaindex_litellm_demo.py)

&lt;/details&gt;

### Token Usage Tracking

&lt;details&gt;
&lt;summary&gt; &lt;b&gt;Overview and Usage&lt;/b&gt; &lt;/summary&gt;

LightRAG provides a TokenTracker tool to monitor and manage token consumption by large language models. This feature is particularly useful for controlling API costs and optimizing performance.

#### Usage

```python
from lightrag.utils import TokenTracker

# Create TokenTracker instance
token_tracker = TokenTracker()

# Method 1: Using context manager (Recommended)
# Suitable for scenarios requiring automatic token usage tracking
with token_tracker:
    result1 = await llm_model_func(&quot;your question 1&quot;)
    result2 = await llm_model_func(&quot;your question 2&quot;)

# Method 2: Manually adding token usage records
# Suitable for scenarios requiring more granular control over token statistics
token_tracker.reset()

rag.insert()

rag.query(&quot;your question 1&quot;, param=QueryParam(mode=&quot;naive&quot;))
rag.query(&quot;your question 2&quot;, param=QueryParam(mode=&quot;mix&quot;))

# Display total token usage (including insert and query operations)
print(&quot;Token usage:&quot;, token_tracker.get_usage())
```

#### Usage Tips
- Use context managers for long sessions or batch operations to automatically track all token consumption
- For scenarios requiring segmented statistics, use manual mode and call reset() when appropriate
- Regular checking of token usage helps detect abnormal consumption early
- Actively use this feature during development and testing to optimize production costs

#### Practical Examples
You can refer to these examples for implementing token tracking:
- `examples/lightrag_gemini_track_token_demo.py`: Token tracking example using Google Gemini model
- `examples/lightrag_siliconcloud_track_token_demo.py`: Token tracking example using SiliconCloud model

These examples demonstrate how to effectively use the TokenTracker feature with different models and scenarios.

&lt;/details&gt;

### Conversation History Support


LightRAG now supports multi-turn dialogue through the conversation history feature. Here&#039;s how to use it:

&lt;details&gt;
  &lt;summary&gt; &lt;b&gt; Usage Example &lt;/b&gt;&lt;/summary&gt;

```python
# Create conversation history
conversation_history = [
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is the main character&#039;s attitude towards Christmas?&quot;},
    {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;At the beginning of the story, Ebenezer Scrooge has a very negative attitude towards Christmas...&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;How does his attitude change?&quot;}
]

# Create query parameters with conversation history
query_param = QueryParam(
    mode=&quot;mix&quot;,  # or any other mode: &quot;local&quot;, &quot;global&quot;, &quot;hybrid&quot;
    conversation_history=conversation_history,  # Add the conversation history
    history_turns=3  # Number of recent conversation turns to consider
)

# Make a query that takes into account the conversation history
response = rag.query(
    &quot;What causes this change in his character?&quot;,
    param=query_param
)
```

&lt;/details&gt;

### Custom Prompt Support

LightRAG now supports custom prompts for fine-tuned control over the system&#039;s behavior. Here&#039;s how to use it:

&lt;details&gt;
  &lt;summary&gt; &lt;b&gt; Usage Example &lt;/b&gt;&lt;/summary&gt;

```python
# Create query parameters
query_param = QueryParam(
    mode=&quot;hybrid&quot;,  # or other mode: &quot;local&quot;, &quot;global&quot;, &quot;hybrid&quot;, &quot;mix&quot; and &quot;naive&quot;
)

# Example 1: Using the default system prompt
response_default = rag.query(
    &quot;What are the primary benefits of renewable energy?&quot;,
    param=query_param
)
print(response_default)

# Example 2: Using a custom prompt
custom_prompt = &quot;&quot;&quot;
You are an expert assistant in environmental science. Provide detailed and structured answers with examples.
---Conversation History---
{history}

---Knowledge Base---
{context_data}

---Response Rules---

- Target format and length: {response_type}
&quot;&quot;&quot;
response_custom = rag.query(
    &quot;What are the primary benefits of ren

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[openedx/edx-platform]]></title>
            <link>https://github.com/openedx/edx-platform</link>
            <guid>https://github.com/openedx/edx-platform</guid>
            <pubDate>Fri, 11 Apr 2025 00:04:34 GMT</pubDate>
            <description><![CDATA[The Open edX LMS & Studio, powering education sites around the world!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/openedx/edx-platform">openedx/edx-platform</a></h1>
            <p>The Open edX LMS & Studio, powering education sites around the world!</p>
            <p>Language: Python</p>
            <p>Stars: 7,672</p>
            <p>Forks: 4,004</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[volcengine/verl]]></title>
            <link>https://github.com/volcengine/verl</link>
            <guid>https://github.com/volcengine/verl</guid>
            <pubDate>Fri, 11 Apr 2025 00:04:33 GMT</pubDate>
            <description><![CDATA[verl: Volcano Engine Reinforcement Learning for LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/volcengine/verl">volcengine/verl</a></h1>
            <p>verl: Volcano Engine Reinforcement Learning for LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 6,432</p>
            <p>Forks: 682</p>
            <p>Stars today: 70 stars today</p>
            <h2>README</h2><pre>&lt;h1 style=&quot;text-align: center;&quot;&gt;verl: Volcano Engine Reinforcement Learning for LLMs&lt;/h1&gt;

[![GitHub Repo stars](https://img.shields.io/github/stars/volcengine/verl)](https://github.com/volcengine/verl/stargazers)
![GitHub forks](https://img.shields.io/github/forks/volcengine/verl)
[![Twitter](https://img.shields.io/twitter/follow/verl_project)](https://twitter.com/verl_project)
&lt;a href=&quot;https://join.slack.com/t/verlgroup/shared_invite/zt-2w5p9o4c3-yy0x2Q56s_VlGLsJ93A6vA&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Slack-verl-blueviolet?logo=slack&amp;amp&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://arxiv.org/pdf/2409.19256&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=EuroSys&amp;message=Paper&amp;color=red&quot;&gt;&lt;/a&gt;
![GitHub contributors](https://img.shields.io/github/contributors/volcengine/verl)
[![Documentation](https://img.shields.io/badge/documentation-blue)](https://verl.readthedocs.io/en/latest/)
&lt;a href=&quot;https://raw.githubusercontent.com/eric-haibin-lin/verl-community/refs/heads/main/WeChat.JPG&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ÂæÆ‰ø°-green?logo=wechat&amp;amp&quot;&gt;&lt;/a&gt;


verl is a flexible, efficient and production-ready RL training library for large language models (LLMs).

verl is the open-source version of **[HybridFlow: A Flexible and Efficient RLHF Framework](https://arxiv.org/abs/2409.19256v2)** paper.

verl is flexible and easy to use with:

- **Easy extension of diverse RL algorithms**: The hybrid-controller programming model enables flexible representation and efficient execution of complex Post-Training dataflows. Build RL dataflows such as GRPO, PPO in a few lines of code.

- **Seamless integration of existing LLM infra with modular APIs**: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as FSDP, Megatron-LM, vLLM, SGLang, etc

- **Flexible device mapping**: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.

- Ready integration with popular HuggingFace models


verl is fast with:

- **State-of-the-art throughput**: SOTA LLM training and inference engine integrations and SOTA RL throughput.

- **Efficient actor model resharding with 3D-HybridEngine**: Eliminates memory redundancy and significantly reduces communication overhead during transitions between training and generation phases.

&lt;/p&gt;

## News
- [2025/05] verl will be presented at [GOSIM x PyTorch Day 2025](https://paris2025.gosim.org/). See you in Paris!
- [2025/04] We will give a tutorial about latest post-training techniques and programming guide for verl at [ICLR 2025](https://iclr.cc/virtual/2025/calendar?filter_events=Expo+Talk+Panel&amp;filter_rooms=). See you in Singapore!
- [2025/03] verl v0.3.0.post1 is released! See [release note](https://github.com/volcengine/verl/releases/) for details.
- [2025/03] [DAPO](https://dapo-sia.github.io/) is the open-sourced SOTA RL algorithm that achieves 50 points on AIME 2024 based on the Qwen2.5-32B pre-trained model, surpassing the previous SOTA achieved by DeepSeek&#039;s GRPO (DeepSeek-R1-Zero-Qwen-32B). DAPO&#039;s training is fully powered by verl and the reproduction code is [publicly available](https://github.com/volcengine/verl/tree/gm-tyx/puffin/main/recipe/dapo) now.
- [2025/03] We introduced the programming model of verl at the [vLLM Beijing Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg) and [verl intro and updates](https://github.com/eric-haibin-lin/verl-community/blob/main/slides/verl-lmsys-meetup.pdf) at the [LMSys Meetup](https://lu.ma/ntjrr7ig) in Sunnyvale mid March.
&lt;details&gt;&lt;summary&gt; more... &lt;/summary&gt;
&lt;ul&gt;
  &lt;li&gt;[2025/02] verl v0.2.0.post2 is released!&lt;/li&gt;
  &lt;li&gt;[2025/01] [Doubao-1.5-pro](https://team.doubao.com/zh/special/doubao_1_5_pro) is released with SOTA-level performance on LLM &amp; VLM. The RL scaling preview model is trained using verl, reaching OpenAI O1-level performance on math benchmarks (70.0 pass@1 on AIME).&lt;/li&gt;
  &lt;li&gt;[2025/03] We will present verl(HybridFlow) at EuroSys 2025. See you in Rotterdam!&lt;/li&gt;
  &lt;li&gt;[2025/02] We presented verl in the &lt;a href=&quot;https://lu.ma/ji7atxux&quot;&gt;Bytedance/NVIDIA/Anyscale Ray Meetup&lt;/a&gt;. See you in San Jose!&lt;/li&gt;
  &lt;li&gt;[2024/12] verl is presented at Ray Forward 2024. Slides available &lt;a href=&quot;https://github.com/eric-haibin-lin/verl-community/blob/main/slides/Ray_Forward_2024_%E5%B7%AB%E9%94%A1%E6%96%8C.pdf&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2024/10] verl is presented at Ray Summit. &lt;a href=&quot;https://www.youtube.com/watch?v=MrhMcXkXvJU&amp;list=PLzTswPQNepXntmT8jr9WaNfqQ60QwW7-U&amp;index=37&quot;&gt;Youtube video&lt;/a&gt; available.&lt;/li&gt;
  &lt;li&gt;[2024/12] The team presented &lt;a href=&quot;https://neurips.cc/Expo/Conferences/2024/workshop/100677&quot;&gt;Post-training LLMs: From Algorithms to Infrastructure&lt;/a&gt; at NeurIPS 2024. &lt;a href=&quot;https://github.com/eric-haibin-lin/verl-data/tree/neurips&quot;&gt;Slides&lt;/a&gt; and &lt;a href=&quot;https://neurips.cc/Expo/Conferences/2024/workshop/100677&quot;&gt;video&lt;/a&gt; available.&lt;/li&gt;
  &lt;li&gt;[2024/08] HybridFlow (verl) is accepted to EuroSys 2025.&lt;/li&gt;
&lt;/ul&gt;   
&lt;/details&gt;

## Key Features

- **FSDP** and **Megatron-LM** for training.
- **vLLM**, **SGLang**(experimental) and **HF Transformers** for rollout generation.
- Compatible with Hugging Face Transformers and Modelscope Hub: Qwen-2.5, Llama3.1, Gemma2, DeepSeek-LLM, etc
- Supervised fine-tuning.
- Reinforcement learning with [PPO](examples/ppo_trainer/), [GRPO](examples/grpo_trainer/), [ReMax](examples/remax_trainer/), [REINFORCE++](https://verl.readthedocs.io/en/latest/examples/config.html#algorithm), [RLOO](examples/rloo_trainer/), [PRIME](recipe/prime/), etc.
  - Support model-based reward and function-based reward (verifiable reward)
  - Support vision-language models (VLMs) and [multi-modal RL](examples/grpo_trainer/run_qwen2_5_vl-7b.sh)
- Flash attention 2, [sequence packing](examples/ppo_trainer/run_qwen2-7b_seq_balance.sh), [sequence parallelism](examples/ppo_trainer/run_deepseek7b_llm_sp2.sh) support via DeepSpeed Ulysses, [LoRA](examples/sft/gsm8k/run_qwen_05_peft.sh), [Liger-kernel](examples/sft/gsm8k/run_qwen_05_sp2_liger.sh).
- Scales up to 70B models and hundreds of GPUs.
- Experiment tracking with wandb, swanlab, mlflow and tensorboard.

## Upcoming Features
- Roadmap https://github.com/volcengine/verl/issues/710 
- DeepSeek 671b optimizations with Megatron v0.11 https://github.com/volcengine/verl/issues/708 
- Multi-turn rollout optimizations
- Environment interactions

## Getting Started

&lt;a href=&quot;https://verl.readthedocs.io/en/latest/index.html&quot;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt;

**Quickstart:**
- [Installation](https://verl.readthedocs.io/en/latest/start/install.html)
- [Quickstart](https://verl.readthedocs.io/en/latest/start/quickstart.html)
- [Programming Guide](https://verl.readthedocs.io/en/latest/hybrid_flow.html)

**Running a PPO example step-by-step:**
- Data and Reward Preparation
  - [Prepare Data for Post-Training](https://verl.readthedocs.io/en/latest/preparation/prepare_data.html)
  - [Implement Reward Function for Dataset](https://verl.readthedocs.io/en/latest/preparation/reward_function.html)
- Understanding the PPO Example
  - [PPO Example Architecture](https://verl.readthedocs.io/en/latest/examples/ppo_code_architecture.html)
  - [Config Explanation](https://verl.readthedocs.io/en/latest/examples/config.html)
  - [Run GSM8K Example](https://verl.readthedocs.io/en/latest/examples/gsm8k_example.html)

**Reproducible algorithm baselines:**
- [PPO, GRPO, ReMax](https://verl.readthedocs.io/en/latest/experiment/ppo.html)

**For code explanation and advance usage (extension):**
- PPO Trainer and Workers
  - [PPO Ray Trainer](https://verl.readthedocs.io/en/latest/workers/ray_trainer.html)
  - [PyTorch FSDP Backend](https://verl.readthedocs.io/en/latest/workers/fsdp_workers.html)
  - [Megatron-LM Backend](https://verl.readthedocs.io/en/latest/index.html)
- Advance Usage and Extension
  - [Ray API design tutorial](https://verl.readthedocs.io/en/latest/advance/placement.html)
  - [Extend to Other RL(HF) algorithms](https://verl.readthedocs.io/en/latest/advance/dpo_extension.html)
  - [Add Models with the FSDP Backend](https://verl.readthedocs.io/en/latest/advance/fsdp_extension.html)
  - [Add Models with the Megatron-LM Backend](https://verl.readthedocs.io/en/latest/advance/megatron_extension.html)
  - [Deployment using Separate GPU Resources](https://github.com/volcengine/verl/tree/main/examples/split_placement)

**Blogs from the community**
- [‰ΩøÁî®verlËøõË°åGRPOÂàÜÂ∏ÉÂºèÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÊúÄ‰Ω≥ÂÆûË∑µ](https://www.volcengine.com/docs/6459/1463942)
- [HybridFlow veRL ÂéüÊñáÊµÖÊûê](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/readme.md)
- [ÊúÄÈ´òÊèêÂçá20ÂÄçÂêûÂêêÈáèÔºÅË±ÜÂåÖÂ§ßÊ®°ÂûãÂõ¢ÈòüÂèëÂ∏ÉÂÖ®Êñ∞ RLHF Ê°ÜÊû∂ÔºåÁé∞Â∑≤ÂºÄÊ∫êÔºÅ](https://team.doubao.com/en/blog/%E6%9C%80%E9%AB%98%E6%8F%90%E5%8D%8720%E5%80%8D%E5%90%9E%E5%90%90%E9%87%8F-%E8%B1%86%E5%8C%85%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%A2%E9%98%9F%E5%8F%91%E5%B8%83%E5%85%A8%E6%96%B0-rlhf-%E6%A1%86%E6%9E%B6-%E7%8E%B0%E5%B7%B2%E5%BC%80%E6%BA%90)


## Performance Tuning Guide
The performance is essential for on-policy RL algorithm. We have written a detailed [performance tuning guide](https://verl.readthedocs.io/en/latest/perf/perf_tuning.html) to help you optimize performance.

## Use vLLM v0.8.2
veRL now supports vLLM&gt;=0.8.2 when using FSDP as the training backend. Please refer to [this document](https://github.com/volcengine/verl/blob/main/docs/README_vllm0.8.md) for installation guide and more information. Please avoid vllm 0.7.x which contains bugs that may lead to OOMs and unexpected errors.

## Citation and acknowledgement

If you find the project helpful, please cite:
- [HybridFlow: A Flexible and Efficient RLHF Framework](https://arxiv.org/abs/2409.19256v2)
- [A Framework for Training Large Language Models for Code Generation via Proximal Policy Optimization](https://i.cs.hku.hk/~cwu/papers/gmsheng-NL2Code24.pdf)

```bibtex
@article{sheng2024hybridflow,
  title   = {HybridFlow: A Flexible and Efficient RLHF Framework},
  author  = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2409.19256}
}
```

verl is inspired by the design of Nemo-Aligner, Deepspeed-chat and OpenRLHF. The project is adopted and contributed by Bytedance, Anyscale, LMSys.org, [Alibaba Qwen team](https://github.com/QwenLM/), Shanghai AI Lab, Tsinghua University, UC Berkeley, UCLA, UIUC, University of Hong Kong, ke.com, [All Hands AI](https://www.all-hands.dev/), [ModelBest](http://modelbest.cn/), [OpenPipe](https://openpipe.ai/), JD AI Lab, Microsoft Research, [StepFun](https://www.stepfun.com/), Amazon, Linkedin, Meituan, [Camel-AI](https://www.camel-ai.org/), [OpenManus](https://github.com/OpenManus), [Prime Intellect](https://www.primeintellect.ai/), NVIDIA research, [Baichuan](https://www.baichuan-ai.com/home), and many more.

## Awesome work using verl
- [TinyZero](https://github.com/Jiayi-Pan/TinyZero): a reproduction of **DeepSeek R1 Zero** recipe for reasoning tasks ![GitHub Repo stars](https://img.shields.io/github/stars/Jiayi-Pan/TinyZero)
- [DAPO](https://dapo-sia.github.io/): the fully open source SOTA RL algorithm that beats DeepSeek-R1-zero-32B ![GitHub Repo stars](https://img.shields.io/github/stars/volcengine/verl)
- [SkyThought](https://github.com/NovaSky-AI/SkyThought): RL training for Sky-T1-7B by NovaSky AI team. ![GitHub Repo stars](https://img.shields.io/github/stars/NovaSky-AI/SkyThought)
- [simpleRL-reason](https://github.com/hkust-nlp/simpleRL-reason): SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild ![GitHub Repo stars](https://img.shields.io/github/stars/hkust-nlp/simpleRL-reason)
- [Easy-R1](https://github.com/hiyouga/EasyR1): **Multi-modal** RL training framework ![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/EasyR1)
- [OpenManus-RL](https://github.com/OpenManus/OpenManus-RL): LLM Agents RL tunning framework for multiple agent environments. ![GitHub Repo stars](https://img.shields.io/github/stars/OpenManus/OpenManus-RL)
- [deepscaler](https://github.com/agentica-project/deepscaler): iterative context scaling with GRPO ![GitHub Repo stars](https://img.shields.io/github/stars/agentica-project/deepscaler)
- [PRIME](https://github.com/PRIME-RL/PRIME): Process reinforcement through implicit rewards ![GitHub Repo stars](https://img.shields.io/github/stars/PRIME-RL/PRIME)
- [RAGEN](https://github.com/ZihanWang314/ragen): a general-purpose reasoning **agent** training framework ![GitHub Repo stars](https://img.shields.io/github/stars/ZihanWang314/ragen)
- [Logic-RL](https://github.com/Unakar/Logic-RL): a reproduction of DeepSeek R1 Zero on 2K Tiny Logic Puzzle Dataset. ![GitHub Repo stars](https://img.shields.io/github/stars/Unakar/Logic-RL)
- [Search-R1](https://github.com/PeterGriffinJin/Search-R1): RL with reasoning and **searching (tool-call)** interleaved LLMs ![GitHub Repo stars](https://img.shields.io/github/stars/PeterGriffinJin/Search-R1)
- [ReSearch](https://github.com/Agent-RL/ReSearch): Learning to **Re**ason with **Search** for LLMs via Reinforcement Learning ![GitHub Repo stars](https://img.shields.io/github/stars/Agent-RL/ReSearch)
- [DeepRetrieval](https://github.com/pat-jj/DeepRetrieval): Hacking **Real Search Engines** and **retrievers** with LLMs via RL for **information retrieval** ![GitHub Repo stars](https://img.shields.io/github/stars/pat-jj/DeepRetrieval)
- [Code-R1](https://github.com/ganler/code-r1): Reproducing R1 for **Code** with Reliable Rewards ![GitHub Repo stars](https://img.shields.io/github/stars/ganler/code-r1)
- [cognitive-behaviors](https://github.com/kanishkg/cognitive-behaviors): Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs ![GitHub Repo stars](https://img.shields.io/github/stars/kanishkg/cognitive-behaviors)
- [PURE](https://github.com/CJReinforce/PURE): **Credit assignment** is the key to successful reinforcement fine-tuning using **process reward model** ![GitHub Repo stars](https://img.shields.io/github/stars/CJReinforce/PURE)
- [MetaSpatial](https://github.com/PzySeere/MetaSpatial): Reinforcing **3D Spatial Reasoning** in **VLMs** for the **Metaverse** ![GitHub Repo stars](https://img.shields.io/github/stars/PzySeere/MetaSpatial)
- [DeepEnlighten](https://github.com/DolbyUUU/DeepEnlighten): Reproduce R1 with **social reasoning** tasks and analyze key findings ![GitHub Repo stars](https://img.shields.io/github/stars/DolbyUUU/DeepEnlighten)
- [DeepResearcher](https://github.com/GAIR-NLP/DeepResearcher): Scaling deep research via reinforcement learning in real-world environments ![GitHub Repo stars](https://img.shields.io/github/stars/GAIR-NLP/DeepResearcher)
- [self-rewarding-reasoning-LLM](https://arxiv.org/pdf/2502.19613): self-rewarding and correction with **generative reward models** ![GitHub Repo stars](https://img.shields.io/github/stars/RLHFlow/Self-rewarding-reasoning-LLM)
- [critic-rl](https://github.com/HKUNLP/critic-rl): LLM critics for code generation ![GitHub Repo stars](https://img.shields.io/github/stars/HKUNLP/critic-rl)
- [VAGEN](https://github.com/RAGEN-AI/VAGEN): Training VLM agents with multi-turn reinforcement learning ![GitHub Repo stars](https://img.shields.io/github/stars/RAGEN-AI/VAGEN)
- [DQO](https://arxiv.org/abs/2410.09302): Enhancing multi-Step reasoning abilities of language models through direct Q-function optimization
- [FIRE](https://arxiv.org/abs/2410.21236): Flaming-hot initiation with regular execution sampling for large language models
- [Rec-R1](https://arxiv.org/pdf/2503.24289): Bridging Generative Large Language Models and Recommendation Systems via Reinforcement Learning
- [all-hands/openhands-lm-32b-v0.1](https://www.all-hands.dev/blog/introducing-openhands-lm-32b----a-strong-open-coding-agent-model): A strong, open coding agent model, trained with [multi-turn fine-tuning](https://github.com/volcengine/verl/pull/195)

## Contribution Guide
Contributions from the community are welcome! Please check out our [project roadmap](https://github.com/volcengine/verl/issues/710) and [good first issues](https://github.com/volcengine/verl/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22good%20first%20issue%22) to see where you can contribute.

### Code formatting
We use yapf (Google style) to enforce strict code formatting when reviewing PRs. To reformat your code locally, make sure you have installed the **latest** version of `yapf`
```bash
pip3 install yapf --upgrade
```
Then, make sure you are at top level of verl repo and run
```bash
bash scripts/format.sh
```
We are HIRING! Send us an [email](mailto:haibin.lin@bytedance.com) if you are interested in internship/FTE opportunities in MLSys/LLM reasoning/multimodal alignment.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[zhayujie/chatgpt-on-wechat]]></title>
            <link>https://github.com/zhayujie/chatgpt-on-wechat</link>
            <guid>https://github.com/zhayujie/chatgpt-on-wechat</guid>
            <pubDate>Fri, 11 Apr 2025 00:04:32 GMT</pubDate>
            <description><![CDATA[Âü∫‰∫éÂ§ßÊ®°ÂûãÊê≠Âª∫ÁöÑËÅäÂ§©Êú∫Âô®‰∫∫ÔºåÂêåÊó∂ÊîØÊåÅ ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®„ÄÅÈ£û‰π¶„ÄÅÈíâÈíâ Á≠âÊé•ÂÖ•ÔºåÂèØÈÄâÊã©GPT3.5/GPT-4o/GPT-o1/ DeepSeek/Claude/ÊñáÂøÉ‰∏ÄË®Ä/ËÆØÈ£ûÊòüÁÅ´/ÈÄö‰πâÂçÉÈóÆ/ Gemini/GLM-4/Claude/Kimi/LinkAIÔºåËÉΩÂ§ÑÁêÜÊñáÊú¨„ÄÅËØ≠Èü≥ÂíåÂõæÁâáÔºåËÆøÈóÆÊìç‰ΩúÁ≥ªÁªüÂíå‰∫íËÅîÁΩëÔºåÊîØÊåÅÂü∫‰∫éËá™ÊúâÁü•ËØÜÂ∫ìËøõË°åÂÆöÂà∂‰ºÅ‰∏öÊô∫ËÉΩÂÆ¢Êúç„ÄÇ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/zhayujie/chatgpt-on-wechat">zhayujie/chatgpt-on-wechat</a></h1>
            <p>Âü∫‰∫éÂ§ßÊ®°ÂûãÊê≠Âª∫ÁöÑËÅäÂ§©Êú∫Âô®‰∫∫ÔºåÂêåÊó∂ÊîØÊåÅ ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®„ÄÅÈ£û‰π¶„ÄÅÈíâÈíâ Á≠âÊé•ÂÖ•ÔºåÂèØÈÄâÊã©GPT3.5/GPT-4o/GPT-o1/ DeepSeek/Claude/ÊñáÂøÉ‰∏ÄË®Ä/ËÆØÈ£ûÊòüÁÅ´/ÈÄö‰πâÂçÉÈóÆ/ Gemini/GLM-4/Claude/Kimi/LinkAIÔºåËÉΩÂ§ÑÁêÜÊñáÊú¨„ÄÅËØ≠Èü≥ÂíåÂõæÁâáÔºåËÆøÈóÆÊìç‰ΩúÁ≥ªÁªüÂíå‰∫íËÅîÁΩëÔºåÊîØÊåÅÂü∫‰∫éËá™ÊúâÁü•ËØÜÂ∫ìËøõË°åÂÆöÂà∂‰ºÅ‰∏öÊô∫ËÉΩÂÆ¢Êúç„ÄÇ</p>
            <p>Language: Python</p>
            <p>Stars: 36,201</p>
            <p>Forks: 9,071</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;img src= &quot;https://github.com/user-attachments/assets/31fb4eab-3be4-477d-aa76-82cf62bfd12c&quot; alt=&quot;Chatgpt-on-Wechat&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;a href=&quot;https://github.com/zhayujie/chatgpt-on-wechat/releases/latest&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/zhayujie/chatgpt-on-wechat&quot; alt=&quot;Latest release&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/zhayujie/chatgpt-on-wechat/blob/master/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/zhayujie/chatgpt-on-wechat&quot; alt=&quot;License: MIT&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/zhayujie/chatgpt-on-wechat&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/zhayujie/chatgpt-on-wechat?style=flat-square&quot; alt=&quot;Stars&quot;&gt;&lt;/a&gt; &lt;br/&gt;
&lt;/p&gt;

chatgpt-on-wechatÔºàÁÆÄÁß∞CoWÔºâÈ°πÁõÆÊòØÂü∫‰∫éÂ§ßÊ®°ÂûãÁöÑÊô∫ËÉΩÂØπËØùÊú∫Âô®‰∫∫ÔºåÊîØÊåÅÂæÆ‰ø°ÂÖ¨‰ºóÂè∑„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®„ÄÅÈ£û‰π¶„ÄÅÈíâÈíâÊé•ÂÖ•ÔºåÂèØÈÄâÊã©GPT3.5/GPT4.0/Claude/Gemini/LinkAI/ChatGLM/KIMI/ÊñáÂøÉ‰∏ÄË®Ä/ËÆØÈ£ûÊòüÁÅ´/ÈÄö‰πâÂçÉÈóÆ/LinkAI/ModelScopeÔºåËÉΩÂ§ÑÁêÜÊñáÊú¨„ÄÅËØ≠Èü≥ÂíåÂõæÁâáÔºåÈÄöËøáÊèí‰ª∂ËÆøÈóÆÊìç‰ΩúÁ≥ªÁªüÂíå‰∫íËÅîÁΩëÁ≠âÂ§ñÈÉ®ËµÑÊ∫êÔºåÊîØÊåÅÂü∫‰∫éËá™ÊúâÁü•ËØÜÂ∫ìÂÆöÂà∂‰ºÅ‰∏öAIÂ∫îÁî®„ÄÇ

# ÁÆÄ‰ªã

ÊúÄÊñ∞ÁâàÊú¨ÊîØÊåÅÁöÑÂäüËÉΩÂ¶Ç‰∏ãÔºö

-  ‚úÖ   **Â§öÁ´ØÈÉ®ÁΩ≤Ôºö** ÊúâÂ§öÁßçÈÉ®ÁΩ≤ÊñπÂºèÂèØÈÄâÊã©‰∏îÂäüËÉΩÂÆåÂ§áÔºåÁõÆÂâçÂ∑≤ÊîØÊåÅÂæÆ‰ø°ÂÖ¨‰ºóÂè∑„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®„ÄÅÈ£û‰π¶„ÄÅÈíâÈíâÁ≠âÈÉ®ÁΩ≤ÊñπÂºè
-  ‚úÖ   **Âü∫Á°ÄÂØπËØùÔºö** ÁßÅËÅäÂèäÁæ§ËÅäÁöÑÊ∂àÊÅØÊô∫ËÉΩÂõûÂ§çÔºåÊîØÊåÅÂ§öËΩÆ‰ºöËØù‰∏ä‰∏ãÊñáËÆ∞ÂøÜÔºåÊîØÊåÅ GPT-3.5, GPT-4o-mini, GPT-4o,  GPT-4, Claude-3.5, Gemini, ÊñáÂøÉ‰∏ÄË®Ä, ËÆØÈ£ûÊòüÁÅ´, ÈÄö‰πâÂçÉÈóÆÔºåChatGLM-4ÔºåKimi(Êúà‰πãÊöóÈù¢), MiniMax, GiteeAI, ModelScope(È≠îÊê≠Á§æÂå∫)
-  ‚úÖ   **ËØ≠Èü≥ËÉΩÂäõÔºö** ÂèØËØÜÂà´ËØ≠Èü≥Ê∂àÊÅØÔºåÈÄöËøáÊñáÂ≠óÊàñËØ≠Èü≥ÂõûÂ§çÔºåÊîØÊåÅ azure, baidu, google, openai(whisper/tts) Á≠âÂ§öÁßçËØ≠Èü≥Ê®°Âûã
-  ‚úÖ   **ÂõæÂÉèËÉΩÂäõÔºö** ÊîØÊåÅÂõæÁâáÁîüÊàê„ÄÅÂõæÁâáËØÜÂà´„ÄÅÂõæÁîüÂõæÔºàÂ¶ÇÁÖßÁâá‰øÆÂ§çÔºâÔºåÂèØÈÄâÊã© Dall-E-3, stable diffusion, replicate, midjourney, CogView-3, visionÊ®°Âûã
-  ‚úÖ   **‰∏∞ÂØåÊèí‰ª∂Ôºö** ÊîØÊåÅ‰∏™ÊÄßÂåñÊèí‰ª∂Êâ©Â±ïÔºåÂ∑≤ÂÆûÁé∞Â§öËßíËâ≤ÂàáÊç¢„ÄÅÊñáÂ≠óÂÜíÈô©„ÄÅÊïèÊÑüËØçËøáÊª§„ÄÅËÅäÂ§©ËÆ∞ÂΩïÊÄªÁªì„ÄÅÊñáÊ°£ÊÄªÁªìÂíåÂØπËØù„ÄÅËÅîÁΩëÊêúÁ¥¢Á≠âÊèí‰ª∂
-  ‚úÖ   **Áü•ËØÜÂ∫ìÔºö** ÈÄöËøá‰∏ä‰º†Áü•ËØÜÂ∫ìÊñá‰ª∂Ëá™ÂÆö‰πâ‰∏ìÂ±ûÊú∫Âô®‰∫∫ÔºåÂèØ‰Ωú‰∏∫Êï∞Â≠óÂàÜË∫´„ÄÅÊô∫ËÉΩÂÆ¢Êúç„ÄÅÁßÅÂüüÂä©Êâã‰ΩøÁî®ÔºåÂü∫‰∫é [LinkAI](https://link-ai.tech) ÂÆûÁé∞

## Â£∞Êòé

1. Êú¨È°πÁõÆÈÅµÂæ™ [MITÂºÄÊ∫êÂçèËÆÆ](/LICENSE)Ôºå‰ªÖÁî®‰∫éÊäÄÊúØÁ†îÁ©∂ÂíåÂ≠¶‰π†Ôºå‰ΩøÁî®Êú¨È°πÁõÆÊó∂ÈúÄÈÅµÂÆàÊâÄÂú®Âú∞Ê≥ïÂæãÊ≥ïËßÑ„ÄÅÁõ∏ÂÖ≥ÊîøÁ≠ñ‰ª•Âèä‰ºÅ‰∏öÁ´†Á®ãÔºåÁ¶ÅÊ≠¢Áî®‰∫é‰ªª‰ΩïËøùÊ≥ïÊàñ‰æµÁäØ‰ªñ‰∫∫ÊùÉÁõäÁöÑË°å‰∏∫
2. Â¢ÉÂÜÖ‰ΩøÁî®ËØ•È°πÁõÆÊó∂ÔºåËØ∑‰ΩøÁî®ÂõΩÂÜÖÂéÇÂïÜÁöÑÂ§ßÊ®°ÂûãÊúçÂä°ÔºåÂπ∂ËøõË°åÂøÖË¶ÅÁöÑÂÜÖÂÆπÂÆâÂÖ®ÂÆ°Ê†∏ÂèäËøáÊª§
3. Êú¨È°πÁõÆ‰∏ªË¶ÅÊé•ÂÖ•ÂçèÂêåÂäûÂÖ¨Âπ≥Âè∞ÔºåÊé®Ëçê‰ΩøÁî®ÂÖ¨‰ºóÂè∑„ÄÅ‰ºÅÂæÆËá™Âª∫Â∫îÁî®„ÄÅÈíâÈíâ„ÄÅÈ£û‰π¶Á≠âÊé•ÂÖ•ÈÄöÈÅìÔºåÂÖ∂‰ªñÈÄöÈÅì‰∏∫ÂéÜÂè≤‰∫ßÁâ©Â∑≤‰∏çÁª¥Êä§
4. ‰ªª‰Ωï‰∏™‰∫∫„ÄÅÂõ¢ÈòüÂíå‰ºÅ‰∏öÔºåÊó†ËÆ∫‰ª•‰ΩïÁßçÊñπÂºè‰ΩøÁî®ËØ•È°πÁõÆ„ÄÅÂØπ‰ΩïÂØπË±°Êèê‰æõÊúçÂä°ÔºåÊâÄ‰∫ßÁîüÁöÑ‰∏ÄÂàáÂêéÊûúÔºåÊú¨È°πÁõÆÂùá‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªª

## ÊºîÁ§∫

DEMOËßÜÈ¢ëÔºöhttps://cdn.link-ai.tech/doc/cow_demo.mp4

## Á§æÂå∫

Ê∑ªÂä†Â∞èÂä©ÊâãÂæÆ‰ø°Âä†ÂÖ•ÂºÄÊ∫êÈ°πÁõÆ‰∫§ÊµÅÁæ§Ôºö

&lt;img width=&quot;160&quot; src=&quot;https://img-1317903499.cos.ap-guangzhou.myqcloud.com/docs/open-community.png&quot;&gt;

&lt;br&gt;

# ‰ºÅ‰∏öÊúçÂä°

&lt;a href=&quot;https://link-ai.tech&quot; target=&quot;_blank&quot;&gt;&lt;img width=&quot;800&quot; src=&quot;https://cdn.link-ai.tech/image/link-ai-intro.jpg&quot;&gt;&lt;/a&gt;

&gt; [LinkAI](https://link-ai.tech/) ÊòØÈù¢Âêë‰ºÅ‰∏öÂíåÂºÄÂèëËÄÖÁöÑ‰∏ÄÁ´ôÂºèAIÂ∫îÁî®Âπ≥Âè∞ÔºåËÅöÂêàÂ§öÊ®°ÊÄÅÂ§ßÊ®°Âûã„ÄÅÁü•ËØÜÂ∫ì„ÄÅAgent Êèí‰ª∂„ÄÅÂ∑•‰ΩúÊµÅÁ≠âËÉΩÂäõÔºåÊîØÊåÅ‰∏ÄÈîÆÊé•ÂÖ•‰∏ªÊµÅÂπ≥Âè∞Âπ∂ËøõË°åÁÆ°ÁêÜÔºåÊîØÊåÅSaaS„ÄÅÁßÅÊúâÂåñÈÉ®ÁΩ≤Â§öÁßçÊ®°Âºè„ÄÇ
&gt;
&gt; LinkAI ÁõÆÂâç Â∑≤Âú®ÁßÅÂüüËøêËê•„ÄÅÊô∫ËÉΩÂÆ¢Êúç„ÄÅ‰ºÅ‰∏öÊïàÁéáÂä©ÊâãÁ≠âÂú∫ÊôØÁßØÁ¥Ø‰∫Ü‰∏∞ÂØåÁöÑ AI Ëß£ÂÜ≥ÊñπÊ°àÔºå Âú®ÁîµÂïÜ„ÄÅÊñáÊïô„ÄÅÂÅ•Â∫∑„ÄÅÊñ∞Ê∂àË¥π„ÄÅÁßëÊäÄÂà∂ÈÄ†Á≠âÂêÑË°å‰∏öÊ≤âÊ∑Ä‰∫ÜÂ§ßÊ®°ÂûãËêΩÂú∞Â∫îÁî®ÁöÑÊúÄ‰Ω≥ÂÆûË∑µÔºåËá¥Âäõ‰∫éÂ∏ÆÂä©Êõ¥Â§ö‰ºÅ‰∏öÂíåÂºÄÂèëËÄÖÊã•Êä± AI Áîü‰∫ßÂäõ„ÄÇ

**‰ºÅ‰∏öÊúçÂä°Âíå‰∫ßÂìÅÂí®ËØ¢** ÂèØËÅîÁ≥ª‰∫ßÂìÅÈ°æÈóÆÔºö

&lt;img width=&quot;160&quot; src=&quot;https://cdn.link-ai.tech/consultant-s.jpg&quot;&gt;

&lt;br&gt;

# üè∑ Êõ¥Êñ∞Êó•Âøó
&gt;**2024.10.31Ôºö** [1.7.3ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.3) Á®ãÂ∫èÁ®≥ÂÆöÊÄßÊèêÂçá„ÄÅÊï∞ÊçÆÂ∫ìÂäüËÉΩ„ÄÅClaudeÊ®°Âûã‰ºòÂåñ„ÄÅlinkaiÊèí‰ª∂‰ºòÂåñ„ÄÅÁ¶ªÁ∫øÈÄöÁü•

&gt;**2024.09.26Ôºö** [1.7.2ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.2)  Âíå [1.7.1ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.1) ÊñáÂøÉÔºåËÆØÈ£ûÁ≠âÊ®°Âûã‰ºòÂåñ„ÄÅo1 Ê®°Âûã„ÄÅÂø´ÈÄüÂÆâË£ÖÂíåÁÆ°ÁêÜËÑöÊú¨

&gt;**2024.08.02Ôºö** [1.7.0ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.0) Êñ∞Â¢û ËÆØÈ£û4.0 Ê®°Âûã„ÄÅÁü•ËØÜÂ∫ìÂºïÁî®Êù•Ê∫êÂ±ïÁ§∫„ÄÅÁõ∏ÂÖ≥Êèí‰ª∂‰ºòÂåñ

&gt;**2024.07.19Ôºö** [1.6.9ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.6.9) Êñ∞Â¢û gpt-4o-mini Ê®°Âûã„ÄÅÈòøÈáåËØ≠Èü≥ËØÜÂà´„ÄÅ‰ºÅÂæÆÂ∫îÁî®Ê∏†ÈÅìË∑ØÁî±‰ºòÂåñ

&gt;**2024.07.05Ôºö** [1.6.8ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.6.8) Âíå [1.6.7ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.6.7)ÔºåClaude3.5, Gemini 1.5 Pro, MiniMaxÊ®°Âûã„ÄÅÂ∑•‰ΩúÊµÅÂõæÁâáËæìÂÖ•„ÄÅÊ®°ÂûãÂàóË°®ÂÆåÂñÑ

&gt;**2024.06.04Ôºö** [1.6.6ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.6.6) Âíå [1.6.5ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.6.5)Ôºågpt-4oÊ®°Âûã„ÄÅÈíâÈíâÊµÅÂºèÂç°Áâá„ÄÅËÆØÈ£ûËØ≠Èü≥ËØÜÂà´/ÂêàÊàê

&gt;**2024.04.26Ôºö** [1.6.0ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.6.0)ÔºåÊñ∞Â¢û Kimi Êé•ÂÖ•„ÄÅgpt-4-turboÁâàÊú¨ÂçáÁ∫ß„ÄÅÊñá‰ª∂ÊÄªÁªìÂíåËØ≠Èü≥ËØÜÂà´ÈóÆÈ¢ò‰øÆÂ§ç

&gt;**2024.03.26Ôºö** [1.5.8ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.8) Âíå [1.5.7ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.7)ÔºåÊñ∞Â¢û GLM-4„ÄÅClaude-3 Ê®°ÂûãÔºåedge-tts ËØ≠Èü≥ÊîØÊåÅ

&gt;**2024.01.26Ôºö** [1.5.6ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.6) Âíå [1.5.5ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.5)ÔºåÈíâÈíâÊé•ÂÖ•ÔºåtoolÊèí‰ª∂ÂçáÁ∫ßÔºå4-turboÊ®°ÂûãÊõ¥Êñ∞

&gt;**2023.11.11Ôºö** [1.5.3ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.3) Âíå [1.5.4ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.4)ÔºåÊñ∞Â¢ûÈÄö‰πâÂçÉÈóÆÊ®°Âûã„ÄÅGoogle Gemini

&gt;**2023.11.10Ôºö** [1.5.2ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.2)ÔºåÊñ∞Â¢ûÈ£û‰π¶ÈÄöÈÅì„ÄÅÂõæÂÉèËØÜÂà´ÂØπËØù„ÄÅÈªëÂêçÂçïÈÖçÁΩÆ

&gt;**2023.11.10Ôºö** [1.5.0ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.0)ÔºåÊñ∞Â¢û `gpt-4-turbo`, `dall-e-3`, `tts` Ê®°ÂûãÊé•ÂÖ•ÔºåÂÆåÂñÑÂõæÂÉèÁêÜËß£&amp;ÁîüÊàê„ÄÅËØ≠Èü≥ËØÜÂà´&amp;ÁîüÊàêÁöÑÂ§öÊ®°ÊÄÅËÉΩÂäõ

&gt;**2023.10.16Ôºö** ÊîØÊåÅÈÄöËøáÊÑèÂõæËØÜÂà´‰ΩøÁî®LinkAIËÅîÁΩëÊêúÁ¥¢„ÄÅÊï∞Â≠¶ËÆ°ÁÆó„ÄÅÁΩëÈ°µËÆøÈóÆÁ≠âÊèí‰ª∂ÔºåÂèÇËÄÉ[Êèí‰ª∂ÊñáÊ°£](https://docs.link-ai.tech/platform/plugins)

&gt;**2023.09.26Ôºö** Êèí‰ª∂Â¢ûÂä† Êñá‰ª∂/ÊñáÁ´†ÈìæÊé• ‰∏ÄÈîÆÊÄªÁªìÂíåÂØπËØùÁöÑÂäüËÉΩÔºå‰ΩøÁî®ÂèÇËÄÉÔºö[Êèí‰ª∂ËØ¥Êòé](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins/linkai#3%E6%96%87%E6%A1%A3%E6%80%BB%E7%BB%93%E5%AF%B9%E8%AF%9D%E5%8A%9F%E8%83%BD)

&gt;**2023.08.08Ôºö** Êé•ÂÖ•ÁôæÂ∫¶ÊñáÂøÉ‰∏ÄË®ÄÊ®°ÂûãÔºåÈÄöËøá [Êèí‰ª∂](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins/linkai) ÊîØÊåÅ Midjourney ÁªòÂõæ

&gt;**2023.06.12Ôºö** Êé•ÂÖ• [LinkAI](https://link-ai.tech/console) Âπ≥Âè∞ÔºåÂèØÂú®Á∫øÂàõÂª∫È¢ÜÂüüÁü•ËØÜÂ∫ìÔºåÊâìÈÄ†‰∏ìÂ±ûÂÆ¢ÊúçÊú∫Âô®‰∫∫„ÄÇ‰ΩøÁî®ÂèÇËÄÉ [Êé•ÂÖ•ÊñáÊ°£](https://link-ai.tech/platform/link-app/wechat)„ÄÇ

Êõ¥Êó©Êõ¥Êñ∞Êó•ÂøóÊü•Áúã: [ÂΩíÊ°£Êó•Âøó](/docs/version/old-version.md)

&lt;br&gt;

# üöÄ Âø´ÈÄüÂºÄÂßã

- Âø´ÈÄüÂºÄÂßãËØ¶ÁªÜÊñáÊ°£Ôºö[È°πÁõÆÊê≠Âª∫ÊñáÊ°£](https://docs.link-ai.tech/cow/quick-start)

- Âø´ÈÄüÂÆâË£ÖËÑöÊú¨ÔºåËØ¶ÁªÜ‰ΩøÁî®ÊåáÂØºÔºö[‰∏ÄÈîÆÂÆâË£ÖÂêØÂä®ËÑöÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/wiki/%E4%B8%80%E9%94%AE%E5%AE%89%E8%A3%85%E5%90%AF%E5%8A%A8%E8%84%9A%E6%9C%AC)
```bash
bash &lt;(curl -sS https://cdn.link-ai.tech/code/cow/install.sh)
```
- È°πÁõÆÁÆ°ÁêÜËÑöÊú¨ÔºåËØ¶ÁªÜ‰ΩøÁî®ÊåáÂØºÔºö[È°πÁõÆÁÆ°ÁêÜËÑöÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/wiki/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E8%84%9A%E6%9C%AC)
## ‰∏Ä„ÄÅÂáÜÂ§á

### 1. Ë¥¶Âè∑Ê≥®ÂÜå

È°πÁõÆÈªòËÆ§‰ΩøÁî®OpenAIÊé•Âè£ÔºåÈúÄÂâçÂæÄ [OpenAIÊ≥®ÂÜåÈ°µÈù¢](https://beta.openai.com/signup) ÂàõÂª∫Ë¥¶Âè∑ÔºåÂàõÂª∫ÂÆåË¥¶Âè∑ÂàôÂâçÂæÄ [APIÁÆ°ÁêÜÈ°µÈù¢](https://beta.openai.com/account/api-keys) ÂàõÂª∫‰∏Ä‰∏™ API Key Âπ∂‰øùÂ≠ò‰∏ãÊù•ÔºåÂêéÈù¢ÈúÄË¶ÅÂú®È°πÁõÆ‰∏≠ÈÖçÁΩÆËøô‰∏™key„ÄÇÊé•Âè£ÈúÄË¶ÅÊµ∑Â§ñÁΩëÁªúËÆøÈóÆÂèäÁªëÂÆö‰ø°Áî®Âç°ÊîØ‰ªò„ÄÇ

&gt; ÈªòËÆ§ÂØπËØùÊ®°ÂûãÊòØ openai ÁöÑ gpt-3.5-turboÔºåËÆ°Ë¥πÊñπÂºèÊòØÁ∫¶ÊØè 1000tokens (Á∫¶750‰∏™Ëã±ÊñáÂçïËØç Êàñ 500Ê±âÂ≠óÔºåÂåÖÂê´ËØ∑Ê±ÇÂíåÂõûÂ§ç) Ê∂àËÄó $0.002ÔºåÂõæÁâáÁîüÊàêÊòØDell EÊ®°ÂûãÔºåÊØèÂº†Ê∂àËÄó $0.016„ÄÇ

È°πÁõÆÂêåÊó∂‰πüÊîØÊåÅ‰ΩøÁî® LinkAI Êé•Âè£ÔºåÊó†ÈúÄ‰ª£ÁêÜÔºåÂèØ‰ΩøÁî® Kimi„ÄÅÊñáÂøÉ„ÄÅËÆØÈ£û„ÄÅGPT-3.5„ÄÅGPT-4o Á≠âÊ®°ÂûãÔºåÊîØÊåÅ ÂÆöÂà∂ÂåñÁü•ËØÜÂ∫ì„ÄÅËÅîÁΩëÊêúÁ¥¢„ÄÅMJÁªòÂõæ„ÄÅÊñáÊ°£ÊÄªÁªì„ÄÅÂ∑•‰ΩúÊµÅÁ≠âËÉΩÂäõ„ÄÇ‰øÆÊîπÈÖçÁΩÆÂç≥ÂèØ‰∏ÄÈîÆ‰ΩøÁî®ÔºåÂèÇËÄÉ [Êé•ÂÖ•ÊñáÊ°£](https://link-ai.tech/platform/link-app/wechat)„ÄÇ

### 2.ËøêË°åÁéØÂ¢É

ÊîØÊåÅ Linux„ÄÅMacOS„ÄÅWindows Á≥ªÁªüÔºàÂèØÂú®LinuxÊúçÂä°Âô®‰∏äÈïøÊúüËøêË°å)ÔºåÂêåÊó∂ÈúÄÂÆâË£Ö `Python`„ÄÇ
&gt; Âª∫ËÆÆPythonÁâàÊú¨Âú® 3.7.1~3.9.X ‰πãÈó¥ÔºåÊé®Ëçê3.8ÁâàÊú¨Ôºå3.10Âèä‰ª•‰∏äÁâàÊú¨Âú® MacOS ÂèØÁî®ÔºåÂÖ∂‰ªñÁ≥ªÁªü‰∏ä‰∏çÁ°ÆÂÆöËÉΩÂê¶Ê≠£Â∏∏ËøêË°å„ÄÇ

&gt; Ê≥®ÊÑèÔºöDocker Êàñ Railway ÈÉ®ÁΩ≤Êó†ÈúÄÂÆâË£ÖpythonÁéØÂ¢ÉÂíå‰∏ãËΩΩÊ∫êÁ†ÅÔºåÂèØÁõ¥Êé•Âø´ËøõÂà∞‰∏ã‰∏ÄËäÇ„ÄÇ

**(1) ÂÖãÈöÜÈ°πÁõÆ‰ª£Á†ÅÔºö**

```bash
git clone https://github.com/zhayujie/chatgpt-on-wechat
cd chatgpt-on-wechat/
```

Ê≥®: Â¶ÇÈÅáÂà∞ÁΩëÁªúÈóÆÈ¢òÂèØÈÄâÊã©ÂõΩÂÜÖÈïúÂÉè https://gitee.com/zhayujie/chatgpt-on-wechat

**(2) ÂÆâË£ÖÊ†∏ÂøÉ‰æùËµñ (ÂøÖÈÄâ)Ôºö**
&gt; ËÉΩÂ§ü‰ΩøÁî®`itchat`ÂàõÂª∫Êú∫Âô®‰∫∫ÔºåÂπ∂ÂÖ∑ÊúâÊñáÂ≠ó‰∫§ÊµÅÂäüËÉΩÊâÄÈúÄÁöÑÊúÄÂ∞è‰æùËµñÈõÜÂêà„ÄÇ
```bash
pip3 install -r requirements.txt
```

**(3) ÊãìÂ±ï‰æùËµñ (ÂèØÈÄâÔºåÂª∫ËÆÆÂÆâË£Ö)Ôºö**

```bash
pip3 install -r requirements-optional.txt
```
&gt; Â¶ÇÊûúÊüêÈ°π‰æùËµñÂÆâË£ÖÂ§±Ë¥•ÂèØÊ≥®ÈáäÊéâÂØπÂ∫îÁöÑË°åÂÜçÁªßÁª≠

## ‰∫å„ÄÅÈÖçÁΩÆ

ÈÖçÁΩÆÊñá‰ª∂ÁöÑÊ®°ÊùøÂú®Ê†πÁõÆÂΩïÁöÑ`config-template.json`‰∏≠ÔºåÈúÄÂ§çÂà∂ËØ•Ê®°ÊùøÂàõÂª∫ÊúÄÁªàÁîüÊïàÁöÑ `config.json` Êñá‰ª∂Ôºö

```bash
  cp config-template.json config.json
```

ÁÑ∂ÂêéÂú®`config.json`‰∏≠Â°´ÂÖ•ÈÖçÁΩÆÔºå‰ª•‰∏ãÊòØÂØπÈªòËÆ§ÈÖçÁΩÆÁöÑËØ¥ÊòéÔºåÂèØÊ†πÊçÆÈúÄË¶ÅËøõË°åËá™ÂÆö‰πâ‰øÆÊîπÔºàÊ≥®ÊÑèÂÆûÈôÖ‰ΩøÁî®Êó∂ËØ∑ÂéªÊéâÊ≥®ÈáäÔºå‰øùËØÅJSONÊ†ºÂºèÁöÑÂÆåÊï¥ÔºâÔºö

```bash
# config.jsonÊñá‰ª∂ÂÜÖÂÆπÁ§∫‰æã
{
  &quot;model&quot;: &quot;gpt-3.5-turbo&quot;,                                   # Ê®°ÂûãÂêçÁß∞, ÊîØÊåÅ gpt-3.5-turbo, gpt-4, gpt-4-turbo, wenxin, xunfei, glm-4, claude-3-haiku, moonshot
  &quot;open_ai_api_key&quot;: &quot;YOUR API KEY&quot;,                          # Â¶ÇÊûú‰ΩøÁî®openAIÊ®°ÂûãÂàôÂ°´ÂÖ•‰∏äÈù¢ÂàõÂª∫ÁöÑ OpenAI API KEY
  &quot;open_ai_api_base&quot;: &quot;https://api.openai.com/v1&quot;,            # OpenAIÊé•Âè£‰ª£ÁêÜÂú∞ÂùÄ
  &quot;proxy&quot;: &quot;&quot;,                                                # ‰ª£ÁêÜÂÆ¢Êà∑Á´ØÁöÑipÂíåÁ´ØÂè£ÔºåÂõΩÂÜÖÁéØÂ¢ÉÂºÄÂêØ‰ª£ÁêÜÁöÑÈúÄË¶ÅÂ°´ÂÜôËØ•È°πÔºåÂ¶Ç &quot;127.0.0.1:7890&quot;
  &quot;single_chat_prefix&quot;: [&quot;bot&quot;, &quot;@bot&quot;],                      # ÁßÅËÅäÊó∂ÊñáÊú¨ÈúÄË¶ÅÂåÖÂê´ËØ•ÂâçÁºÄÊâçËÉΩËß¶ÂèëÊú∫Âô®‰∫∫ÂõûÂ§ç
  &quot;single_chat_reply_prefix&quot;: &quot;[bot] &quot;,                       # ÁßÅËÅäÊó∂Ëá™Âä®ÂõûÂ§çÁöÑÂâçÁºÄÔºåÁî®‰∫éÂå∫ÂàÜÁúü‰∫∫
  &quot;group_chat_prefix&quot;: [&quot;@bot&quot;],                              # Áæ§ËÅäÊó∂ÂåÖÂê´ËØ•ÂâçÁºÄÂàô‰ºöËß¶ÂèëÊú∫Âô®‰∫∫ÂõûÂ§ç
  &quot;group_name_white_list&quot;: [&quot;ChatGPTÊµãËØïÁæ§&quot;, &quot;ChatGPTÊµãËØïÁæ§2&quot;], # ÂºÄÂêØËá™Âä®ÂõûÂ§çÁöÑÁæ§ÂêçÁß∞ÂàóË°®
  &quot;group_chat_in_one_session&quot;: [&quot;ChatGPTÊµãËØïÁæ§&quot;],              # ÊîØÊåÅ‰ºöËØù‰∏ä‰∏ãÊñáÂÖ±‰∫´ÁöÑÁæ§ÂêçÁß∞  
  &quot;image_create_prefix&quot;: [&quot;Áîª&quot;, &quot;Áúã&quot;, &quot;Êâæ&quot;],                   # ÂºÄÂêØÂõæÁâáÂõûÂ§çÁöÑÂâçÁºÄ
  &quot;conversation_max_tokens&quot;: 1000,                            # ÊîØÊåÅ‰∏ä‰∏ãÊñáËÆ∞ÂøÜÁöÑÊúÄÂ§öÂ≠óÁ¨¶Êï∞
  &quot;speech_recognition&quot;: false,                                # ÊòØÂê¶ÂºÄÂêØËØ≠Èü≥ËØÜÂà´
  &quot;group_speech_recognition&quot;: false,                          # ÊòØÂê¶ÂºÄÂêØÁæ§ÁªÑËØ≠Èü≥ËØÜÂà´
  &quot;voice_reply_voice&quot;: false,                                 # ÊòØÂê¶‰ΩøÁî®ËØ≠Èü≥ÂõûÂ§çËØ≠Èü≥
  &quot;character_desc&quot;: &quot;‰Ω†ÊòØÂü∫‰∫éÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑAIÊô∫ËÉΩÂä©ÊâãÔºåÊó®Âú®ÂõûÁ≠îÂπ∂Ëß£ÂÜ≥‰∫∫‰ª¨ÁöÑ‰ªª‰ΩïÈóÆÈ¢òÔºåÂπ∂‰∏îÂèØ‰ª•‰ΩøÁî®Â§öÁßçËØ≠Ë®Ä‰∏é‰∫∫‰∫§ÊµÅ„ÄÇ&quot;,  # ‰∫∫Ê†ºÊèèËø∞
  # ËÆ¢ÈòÖÊ∂àÊÅØÔºåÂÖ¨‰ºóÂè∑Âíå‰ºÅ‰∏öÂæÆ‰ø°channel‰∏≠ËØ∑Â°´ÂÜôÔºåÂΩìË¢´ËÆ¢ÈòÖÊó∂‰ºöËá™Âä®ÂõûÂ§çÔºåÂèØ‰ΩøÁî®ÁâπÊÆäÂç†‰ΩçÁ¨¶„ÄÇÁõÆÂâçÊîØÊåÅÁöÑÂç†‰ΩçÁ¨¶Êúâ{trigger_prefix}ÔºåÂú®Á®ãÂ∫è‰∏≠ÂÆÉ‰ºöËá™Âä®ÊõøÊç¢ÊàêbotÁöÑËß¶ÂèëËØç„ÄÇ
  &quot;subscribe_msg&quot;: &quot;ÊÑüË∞¢ÊÇ®ÁöÑÂÖ≥Ê≥®ÔºÅ\nËøôÈáåÊòØChatGPTÔºåÂèØ‰ª•Ëá™Áî±ÂØπËØù„ÄÇ\nÊîØÊåÅËØ≠Èü≥ÂØπËØù„ÄÇ\nÊîØÊåÅÂõæÁâáËæìÂá∫ÔºåÁîªÂ≠óÂºÄÂ§¥ÁöÑÊ∂àÊÅØÂ∞ÜÊåâË¶ÅÊ±ÇÂàõ‰ΩúÂõæÁâá„ÄÇ\nÊîØÊåÅËßíËâ≤ÊâÆÊºîÂíåÊñáÂ≠óÂÜíÈô©Á≠â‰∏∞ÂØåÊèí‰ª∂„ÄÇ\nËæìÂÖ•{trigger_prefix}#help Êü•ÁúãËØ¶ÁªÜÊåá‰ª§„ÄÇ&quot;,
  &quot;use_linkai&quot;: false,                                        # ÊòØÂê¶‰ΩøÁî®LinkAIÊé•Âè£ÔºåÈªòËÆ§ÂÖ≥Èó≠ÔºåÂºÄÂêØÂêéÂèØÂõΩÂÜÖËÆøÈóÆÔºå‰ΩøÁî®Áü•ËØÜÂ∫ìÂíåMJ
  &quot;linkai_api_key&quot;: &quot;&quot;,                                       # LinkAI Api Key
  &quot;linkai_app_code&quot;: &quot;&quot;                                       # LinkAI Â∫îÁî®ÊàñÂ∑•‰ΩúÊµÅcode
}
```
**ÈÖçÁΩÆËØ¥ÊòéÔºö**

**1.‰∏™‰∫∫ËÅäÂ§©**

+ ‰∏™‰∫∫ËÅäÂ§©‰∏≠ÔºåÈúÄË¶Å‰ª• &quot;bot&quot;Êàñ&quot;@bot&quot; ‰∏∫ÂºÄÂ§¥ÁöÑÂÜÖÂÆπËß¶ÂèëÊú∫Âô®‰∫∫ÔºåÂØπÂ∫îÈÖçÁΩÆÈ°π `single_chat_prefix` (Â¶ÇÊûú‰∏çÈúÄË¶Å‰ª•ÂâçÁºÄËß¶ÂèëÂèØ‰ª•Â°´ÂÜô  `&quot;single_chat_prefix&quot;: [&quot;&quot;]`)
+ Êú∫Âô®‰∫∫ÂõûÂ§çÁöÑÂÜÖÂÆπ‰ºö‰ª• &quot;[bot] &quot; ‰Ωú‰∏∫ÂâçÁºÄÔºå ‰ª•Âå∫ÂàÜÁúü‰∫∫ÔºåÂØπÂ∫îÁöÑÈÖçÁΩÆÈ°π‰∏∫ `single_chat_reply_prefix` (Â¶ÇÊûú‰∏çÈúÄË¶ÅÂâçÁºÄÂèØ‰ª•Â°´ÂÜô `&quot;single_chat_reply_prefix&quot;: &quot;&quot;`)

**2.Áæ§ÁªÑËÅäÂ§©**

+ Áæ§ÁªÑËÅäÂ§©‰∏≠ÔºåÁæ§ÂêçÁß∞ÈúÄÈÖçÁΩÆÂú® `group_name_white_list ` ‰∏≠ÊâçËÉΩÂºÄÂêØÁæ§ËÅäËá™Âä®ÂõûÂ§ç„ÄÇÂ¶ÇÊûúÊÉ≥ÂØπÊâÄÊúâÁæ§ËÅäÁîüÊïàÔºåÂèØ‰ª•Áõ¥Êé•Â°´ÂÜô `&quot;group_name_white_list&quot;: [&quot;ALL_GROUP&quot;]`
+ ÈªòËÆ§Âè™Ë¶ÅË¢´‰∫∫ @ Â∞±‰ºöËß¶ÂèëÊú∫Âô®‰∫∫Ëá™Âä®ÂõûÂ§çÔºõÂè¶Â§ñÁæ§ËÅäÂ§©‰∏≠Âè™Ë¶ÅÊ£ÄÊµãÂà∞‰ª• &quot;@bot&quot; ÂºÄÂ§¥ÁöÑÂÜÖÂÆπÔºåÂêåÊ†∑‰ºöËá™Âä®ÂõûÂ§çÔºàÊñπ‰æøËá™Â∑±Ëß¶ÂèëÔºâÔºåËøôÂØπÂ∫îÈÖçÁΩÆÈ°π `group_chat_prefix`
+ ÂèØÈÄâÈÖçÁΩÆ: `group_name_keyword_white_list`ÈÖçÁΩÆÈ°πÊîØÊåÅÊ®°Á≥äÂåπÈÖçÁæ§ÂêçÁß∞Ôºå`group_chat_keyword`ÈÖçÁΩÆÈ°πÂàôÊîØÊåÅÊ®°Á≥äÂåπÈÖçÁæ§Ê∂àÊÅØÂÜÖÂÆπÔºåÁî®Ê≥ï‰∏é‰∏äËø∞‰∏§‰∏™ÈÖçÁΩÆÈ°πÁõ∏Âêå„ÄÇÔºàContributed by [evolay](https://github.com/evolay))
+ `group_chat_in_one_session`Ôºö‰ΩøÁæ§ËÅäÂÖ±‰∫´‰∏Ä‰∏™‰ºöËØù‰∏ä‰∏ãÊñáÔºåÈÖçÁΩÆ `[&quot;ALL_GROUP&quot;]` Âàô‰ΩúÁî®‰∫éÊâÄÊúâÁæ§ËÅä

**3.ËØ≠Èü≥ËØÜÂà´**

+ Ê∑ªÂä† `&quot;speech_recognition&quot;: true` Â∞ÜÂºÄÂêØËØ≠Èü≥ËØÜÂà´ÔºåÈªòËÆ§‰ΩøÁî®openaiÁöÑwhisperÊ®°ÂûãËØÜÂà´‰∏∫ÊñáÂ≠óÔºåÂêåÊó∂‰ª•ÊñáÂ≠óÂõûÂ§çÔºåËØ•ÂèÇÊï∞‰ªÖÊîØÊåÅÁßÅËÅä (Ê≥®ÊÑèÁî±‰∫éËØ≠Èü≥Ê∂àÊÅØÊó†Ê≥ïÂåπÈÖçÂâçÁºÄÔºå‰∏ÄÊó¶ÂºÄÂêØÂ∞ÜÂØπÊâÄÊúâËØ≠Èü≥Ëá™Âä®ÂõûÂ§çÔºåÊîØÊåÅËØ≠Èü≥Ëß¶ÂèëÁîªÂõæ)Ôºõ
+ Ê∑ªÂä† `&quot;group_speech_recognition&quot;: true` Â∞ÜÂºÄÂêØÁæ§ÁªÑËØ≠Èü≥ËØÜÂà´ÔºåÈªòËÆ§‰ΩøÁî®openaiÁöÑwhisperÊ®°ÂûãËØÜÂà´‰∏∫ÊñáÂ≠óÔºåÂêåÊó∂‰ª•ÊñáÂ≠óÂõûÂ§çÔºåÂèÇÊï∞‰ªÖÊîØÊåÅÁæ§ËÅä (‰ºöÂåπÈÖçgroup_chat_prefixÂíågroup_chat_keyword, ÊîØÊåÅËØ≠Èü≥Ëß¶ÂèëÁîªÂõæ)Ôºõ
+ Ê∑ªÂä† `&quot;voice_reply_voice&quot;: true` Â∞ÜÂºÄÂêØËØ≠Èü≥ÂõûÂ§çËØ≠Èü≥ÔºàÂêåÊó∂‰ΩúÁî®‰∫éÁßÅËÅäÂíåÁæ§ËÅäÔºâ

**4.ÂÖ∂‰ªñÈÖçÁΩÆ**

+ `model`: Ê®°ÂûãÂêçÁß∞ÔºåÁõÆÂâçÊîØÊåÅ `gpt-3.5-turbo`, `gpt-4o-mini`, `gpt-4o`, `gpt-4`, `wenxin` , `claude` , `gemini`, `glm-4`,  `xunfei`, `moonshot`Á≠âÔºåÂÖ®ÈÉ®Ê®°ÂûãÂêçÁß∞ÂèÇËÄÉ[common/const.py](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/common/const.py)Êñá‰ª∂
+ `temperature`,`frequency_penalty`,`presence_penalty`: Chat APIÊé•Âè£ÂèÇÊï∞ÔºåËØ¶ÊÉÖÂèÇËÄÉ[OpenAIÂÆòÊñπÊñáÊ°£„ÄÇ](https://platform.openai.com/docs/api-reference/chat)
+ `proxy`ÔºöÁî±‰∫éÁõÆÂâç `openai` Êé•Âè£ÂõΩÂÜÖÊó†Ê≥ïËÆøÈóÆÔºåÈúÄÈÖçÁΩÆ‰ª£ÁêÜÂÆ¢Êà∑Á´ØÁöÑÂú∞ÂùÄÔºåËØ¶ÊÉÖÂèÇËÄÉ  [#351](https://github.com/zhayujie/chatgpt-on-wechat/issues/351)
+ ÂØπ‰∫éÂõæÂÉèÁîüÊàêÔºåÂú®Êª°Ë∂≥‰∏™‰∫∫ÊàñÁæ§ÁªÑËß¶ÂèëÊù°‰ª∂Â§ñÔºåËøòÈúÄË¶ÅÈ¢ùÂ§ñÁöÑÂÖ≥ÈîÆËØçÂâçÁºÄÊù•Ëß¶ÂèëÔºåÂØπÂ∫îÈÖçÁΩÆ `image_create_prefix `
+ ÂÖ≥‰∫éOpenAIÂØπËØùÂèäÂõæÁâáÊé•Âè£ÁöÑÂèÇÊï∞ÈÖçÁΩÆÔºàÂÜÖÂÆπËá™Áî±Â∫¶„ÄÅÂõûÂ§çÂ≠óÊï∞ÈôêÂà∂„ÄÅÂõæÁâáÂ§ßÂ∞èÁ≠âÔºâÔºåÂèØ‰ª•ÂèÇËÄÉ [ÂØπËØùÊé•Âè£](https://beta.openai.com/docs/api-reference/completions) Âíå [ÂõæÂÉèÊé•Âè£](https://beta.openai.com/docs/api-reference/completions)  ÊñáÊ°£ÔºåÂú®[`config.py`](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/config.py)‰∏≠Ê£ÄÊü•Âì™‰∫õÂèÇÊï∞Âú®Êú¨È°πÁõÆ‰∏≠ÊòØÂèØÈÖçÁΩÆÁöÑ„ÄÇ
+ `conversation_max_tokens`ÔºöË°®Á§∫ËÉΩÂ§üËÆ∞ÂøÜÁöÑ‰∏ä‰∏ãÊñáÊúÄÂ§ßÂ≠óÊï∞Ôºà‰∏ÄÈóÆ‰∏ÄÁ≠î‰∏∫‰∏ÄÁªÑÂØπËØùÔºåÂ¶ÇÊûúÁ¥ØÁßØÁöÑÂØπËØùÂ≠óÊï∞Ë∂ÖÂá∫ÈôêÂà∂ÔºåÂ∞±‰ºö‰ºòÂÖàÁßªÈô§ÊúÄÊó©ÁöÑ‰∏ÄÁªÑÂØπËØùÔºâ
+ `rate_limit_chatgpt`Ôºå`rate_limit_dalle`ÔºöÊØèÂàÜÈíüÊúÄÈ´òÈóÆÁ≠îÈÄüÁéá„ÄÅÁîªÂõæÈÄüÁéáÔºåË∂ÖÈÄüÂêéÊéíÈòüÊåâÂ∫èÂ§ÑÁêÜ„ÄÇ
+ `clear_memory_commands`: ÂØπËØùÂÜÖÊåá‰ª§Ôºå‰∏ªÂä®Ê∏ÖÁ©∫ÂâçÊñáËÆ∞ÂøÜÔºåÂ≠óÁ¨¶‰∏≤Êï∞ÁªÑÂèØËá™ÂÆö‰πâÊåá‰ª§Âà´Âêç„ÄÇ
+ `hot_reload`: Á®ãÂ∫èÈÄÄÂá∫ÂêéÔºåÊöÇÂ≠òÁ≠â‰∫éÁä∂ÊÄÅÔºåÈªòËÆ§ÂÖ≥Èó≠„ÄÇ
+ `character_desc` ÈÖçÁΩÆ‰∏≠‰øùÂ≠òÁùÄ‰Ω†ÂØπÊú∫Âô®‰∫∫ËØ¥ÁöÑ‰∏ÄÊÆµËØùÔºå‰ªñ‰ºöËÆ∞‰ΩèËøôÊÆµËØùÂπ∂‰Ωú‰∏∫‰ªñÁöÑËÆæÂÆöÔºå‰Ω†ÂèØ‰ª•‰∏∫‰ªñÂÆöÂà∂‰ªª‰Ωï‰∫∫Ê†º      (ÂÖ≥‰∫é‰ºöËØù‰∏ä‰∏ãÊñáÁöÑÊõ¥Â§öÂÜÖÂÆπÂèÇËÄÉËØ• [issue](https://github.com/zhayujie/chatgpt-on-wechat/issues/43))
+ `subscribe_msg`ÔºöËÆ¢ÈòÖÊ∂àÊÅØÔºåÂÖ¨‰ºóÂè∑Âíå‰ºÅ‰∏öÂæÆ‰ø°channel‰∏≠ËØ∑Â°´ÂÜôÔºåÂΩìË¢´ËÆ¢ÈòÖÊó∂‰ºöËá™Âä®ÂõûÂ§çÔºå ÂèØ‰ΩøÁî®ÁâπÊÆäÂç†‰ΩçÁ¨¶„ÄÇÁõÆÂâçÊîØÊåÅÁöÑÂç†‰ΩçÁ¨¶Êúâ{trigger_prefix}ÔºåÂú®Á®ãÂ∫è‰∏≠ÂÆÉ‰ºöËá™Âä®ÊõøÊç¢ÊàêbotÁöÑËß¶ÂèëËØç„ÄÇ

**5.LinkAIÈÖçÁΩÆ (ÂèØÈÄâ)**

+ `use_linkai`: ÊòØÂê¶‰ΩøÁî®LinkAIÊé•Âè£ÔºåÂºÄÂêØÂêéÂèØÂõΩÂÜÖËÆøÈóÆÔºå‰ΩøÁî®Áü•ËØÜÂ∫ìÂíå `Midjourney` ÁªòÁîª, ÂèÇËÄÉ [ÊñáÊ°£](https://link-ai.tech/platform/link-app/wechat)
+ `linkai_api_key`: LinkAI Api KeyÔºåÂèØÂú® [ÊéßÂà∂Âè∞](https://link-ai.tech/console/interface) ÂàõÂª∫
+ `linkai_app_code`: LinkAI Â∫îÁî®ÊàñÂ∑•‰ΩúÊµÅÁöÑcodeÔºåÈÄâÂ°´

**Êú¨ËØ¥ÊòéÊñáÊ°£ÂèØËÉΩ‰ºöÊú™ÂèäÊó∂Êõ¥Êñ∞ÔºåÂΩìÂâçÊâÄÊúâÂèØÈÄâÁöÑÈÖçÁΩÆÈ°πÂùáÂú®ËØ•[`config.py`](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/config.py)‰∏≠ÂàóÂá∫„ÄÇ**

## ‰∏â„ÄÅËøêË°å

### 1.Êú¨Âú∞ËøêË°å

Â¶ÇÊûúÊòØÂºÄÂèëÊú∫ **Êú¨Âú∞ËøêË°å**ÔºåÁõ¥Êé•Âú®È°πÁõÆÊ†πÁõÆÂΩï‰∏ãÊâßË°åÔºö

```bash
python3 app.py                                    # windowsÁéØÂ¢É‰∏ãËØ•ÂëΩ‰ª§ÈÄöÂ∏∏‰∏∫ python app.py
```

ÁªàÁ´ØËæìÂá∫‰∫åÁª¥Á†ÅÂêéÔºåËøõË°åÊâ´Á†ÅÁôªÂΩïÔºåÂΩìËæìÂá∫ &quot;Start auto replying&quot; Êó∂Ë°®Á§∫Ëá™Âä®ÂõûÂ§çÁ®ãÂ∫èÂ∑≤ÁªèÊàêÂäüËøêË°å‰∫ÜÔºàÊ≥®ÊÑèÔºöÁî®‰∫éÁôªÂΩïÁöÑË¥¶Âè∑ÈúÄË¶ÅÂú®ÊîØ‰ªòÂ§ÑÂ∑≤ÂÆåÊàêÂÆûÂêçËÆ§ËØÅÔºâ„ÄÇÊâ´Á†ÅÁôªÂΩïÂêé‰Ω†ÁöÑË¥¶Âè∑Â∞±Êàê‰∏∫Êú∫Âô®‰∫∫‰∫ÜÔºåÂèØ‰ª•Âú®ÊâãÊú∫Á´ØÈÄöËøáÈÖçÁΩÆÁöÑÂÖ≥ÈîÆËØçËß¶ÂèëËá™Âä®ÂõûÂ§ç (‰ªªÊÑèÂ•ΩÂèãÂèëÈÄÅÊ∂àÊÅØÁªô‰Ω†ÔºåÊàñÊòØËá™Â∑±ÂèëÊ∂àÊÅØÁªôÂ•ΩÂèã)ÔºåÂèÇËÄÉ[#142](https://github.com/zhayujie/chatgpt-on-wechat/issues/142)„ÄÇ

### 2.ÊúçÂä°Âô®ÈÉ®ÁΩ≤

‰ΩøÁî®nohupÂëΩ‰ª§Âú®ÂêéÂè∞ËøêË°åÁ®ãÂ∫èÔºö

```bash
nohup python3 app.py &amp; tail -f nohup.out          # Âú®ÂêéÂè∞ËøêË°åÁ®ãÂ∫èÂπ∂ÈÄöËøáÊó•ÂøóËæìÂá∫‰∫åÁª¥Á†Å
```
Êâ´Á†ÅÁôªÂΩïÂêéÁ®ãÂ∫èÂç≥ÂèØËøêË°å‰∫éÊúçÂä°Âô®ÂêéÂè∞ÔºåÊ≠§Êó∂ÂèØÈÄöËøá `ctrl+c` ÂÖ≥Èó≠Êó•ÂøóÔºå‰∏ç‰ºöÂΩ±ÂìçÂêéÂè∞Á®ãÂ∫èÁöÑËøêË°å„ÄÇ‰ΩøÁî® `ps -ef | grep app.py | grep -v grep` ÂëΩ‰ª§ÂèØÊü•ÁúãËøêË°å‰∫éÂêéÂè∞ÁöÑËøõÁ®ãÔºåÂ¶ÇÊûúÊÉ≥Ë¶ÅÈáçÊñ∞ÂêØÂä®Á®ãÂ∫èÂèØ‰ª•ÂÖà `kill` ÊéâÂØπÂ∫îÁöÑËøõÁ®ã„ÄÇÊó•ÂøóÂÖ≥Èó≠ÂêéÂ¶ÇÊûúÊÉ≥Ë¶ÅÂÜçÊ¨°ÊâìÂºÄÂè™ÈúÄËæìÂÖ•¬†`tail -f nohup.out`„ÄÇÊ≠§Â§ñÔºå`scripts` ÁõÆÂΩï‰∏ãÊúâ‰∏ÄÈîÆËøêË°å„ÄÅÂÖ≥Èó≠Á®ãÂ∫èÁöÑËÑöÊú¨‰æõ‰ΩøÁî®„ÄÇ

&gt; **Â§öË¥¶Âè∑ÊîØÊåÅÔºö** Â∞ÜÈ°πÁõÆÂ§çÂà∂Â§ö‰ªΩÔºåÂàÜÂà´ÂêØÂä®Á®ãÂ∫èÔºåÁî®‰∏çÂêåË¥¶Âè∑Êâ´Á†ÅÁôªÂΩïÂç≥ÂèØÂÆûÁé∞ÂêåÊó∂ËøêË°å„ÄÇ

&gt; **ÁâπÊÆäÊåá‰ª§Ôºö** Áî®Êà∑ÂêëÊú∫Âô®‰∫∫ÂèëÈÄÅ **#reset** Âç≥ÂèØÊ∏ÖÁ©∫ËØ•Áî®Êà∑ÁöÑ‰∏ä‰∏ãÊñáËÆ∞ÂøÜ„ÄÇ


### 3.DockerÈÉ®ÁΩ≤

&gt; ‰ΩøÁî®dockerÈÉ®ÁΩ≤Êó†ÈúÄ‰∏ãËΩΩÊ∫êÁ†ÅÂíåÂÆâË£Ö‰æùËµñÔºåÂè™ÈúÄË¶ÅËé∑Âèñ docker-compose.yml ÈÖçÁΩÆÊñá‰ª∂Âπ∂ÂêØÂä®ÂÆπÂô®Âç≥ÂèØ„ÄÇ

&gt; ÂâçÊèêÊòØÈúÄË¶ÅÂÆâË£ÖÂ•Ω `docker` Âèä `docker-compose`ÔºåÂÆâË£ÖÊàêÂäüÁöÑË°®Áé∞ÊòØÊâßË°å `docker -v` Âíå `docker-compose version` (Êàñ docker compose version) ÂèØ‰ª•Êü•ÁúãÂà∞ÁâàÊú¨Âè∑ÔºåÂèØÂâçÂæÄ [dockerÂÆòÁΩë](https://docs.docker.com/engine/install/) ËøõË°å‰∏ãËΩΩ„ÄÇ

**(1) ‰∏ãËΩΩ docker-compose.yml Êñá‰ª∂**

```bash
wget https://open-1317903499.cos.ap-guangzhou.myqcloud.com/docker-compose.yml
```

‰∏ãËΩΩÂÆåÊàêÂêéÊâìÂºÄ `docker-compose.yml` ‰øÆÊîπÊâÄÈúÄÈÖçÁΩÆÔºåÂ¶Ç `OPEN_AI_API_KEY` Âíå `GROUP_NAME_WHITE_LIST` Á≠â„ÄÇ

**(2) ÂêØÂä®ÂÆπÂô®**

Âú® `docker-compose.yml` ÊâÄÂú®ÁõÆÂΩï‰∏ãÊâßË°å‰ª•‰∏ãÂëΩ‰ª§ÂêØÂä®ÂÆπÂô®Ôºö

```bash
sudo docker compose up -d
```

ËøêË°å `sudo docker ps` ËÉΩÊü•ÁúãÂà∞ NAMES ‰∏∫ chatgpt-on-wechat ÁöÑÂÆπÂô®Âç≥Ë°®Á§∫ËøêË°åÊàêÂäü„ÄÇ

Ê≥®ÊÑèÔºö

 - Â¶ÇÊûú `docker-compose` ÊòØ 1.X ÁâàÊú¨ ÂàôÈúÄË¶ÅÊâßË°å `sudo  docker-compose up -d` Êù•ÂêØÂä®ÂÆπÂô®
 - ËØ•ÂëΩ‰ª§‰ºöËá™Âä®Âéª [docker hub](https://hub.docker.com/r/zhayujie/chatgpt-on-wechat) ÊãâÂèñ latest ÁâàÊú¨ÁöÑÈïúÂÉèÔºålatest ÈïúÂÉè‰ºöÂú®ÊØèÊ¨°È°πÁõÆ release Êñ∞ÁöÑÁâàÊú¨Êó∂ÁîüÊàê

ÊúÄÂêéËøêË°å‰ª•‰∏ãÂëΩ‰ª§ÂèØÊü•ÁúãÂÆπÂô®ËøêË°åÊó•ÂøóÔºåÊâ´ÊèèÊó•Âøó‰∏≠ÁöÑ‰∫åÁª¥Á†ÅÂç≥ÂèØÂÆåÊàêÁôªÂΩïÔºö

```bash
sudo docker logs -f chatgpt-on-wechat
```

**(3) Êèí‰ª∂‰ΩøÁî®**

Â¶ÇÊûúÈúÄË¶ÅÂú®dockerÂÆπÂô®‰∏≠‰øÆÊîπÊèí‰ª∂ÈÖçÁΩÆÔºåÂèØÈÄöËøáÊåÇËΩΩÁöÑÊñπÂºèÂÆåÊàêÔºåÂ∞Ü [Êèí‰ª∂ÈÖçÁΩÆÊñá‰ª∂](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/plugins/config.json.template)
ÈáçÂëΩÂêç‰∏∫ `config.json`ÔºåÊîæÁΩÆ‰∫é `docker-compose.yml` Áõ∏ÂêåÁõÆÂΩï‰∏ãÔºåÂπ∂Âú® `docker-compose.yml` ‰∏≠ÁöÑ `chatgpt-on-wechat` ÈÉ®ÂàÜ‰∏ãÊ∑ªÂä† `volumes` Êò†Â∞Ñ:

```
volumes:
  - ./config.json:/app/plugins/config.json
```
**Ê≥®**ÔºöÈááÁî®dockerÊñπÂºèÈÉ®ÁΩ≤ÁöÑËØ¶ÁªÜÊïôÁ®ãÂèØ‰ª•ÂèÇËÄÉÔºö[dockerÈÉ®ÁΩ≤CoWÈ°πÁõÆ](https://www.wangpc.cc/ai/docker-deploy-cow/)
### 4. RailwayÈÉ®ÁΩ≤

&gt; Railway ÊØèÊúàÊèê‰æõ5ÂàÄÂíåÊúÄÂ§ö500Â∞èÊó∂ÁöÑÂÖçË¥πÈ¢ùÂ∫¶„ÄÇ (07.11Êõ¥Êñ∞: ÁõÆÂâçÂ§ßÈÉ®ÂàÜË¥¶Âè∑Â∑≤Êó†Ê≥ïÂÖçË¥πÈÉ®ÁΩ≤)

1. ËøõÂÖ• [Railway](https://railway.app/template/qApznZ?referralCode=RC3znh)
2. ÁÇπÂáª `Deploy Now` ÊåâÈíÆ„ÄÇ
3. ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáèÊù•ÈáçËΩΩÁ®ãÂ∫èËøêË°åÁöÑÂèÇÊï∞Ôºå‰æãÂ¶Ç`open_ai_api_key`, `character_desc`„ÄÇ

**‰∏ÄÈîÆÈÉ®ÁΩ≤:**
  
  [![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template/qApznZ?referralCode=RC3znh)

&lt;br&gt;

# üîé Â∏∏ËßÅÈóÆÈ¢ò

FAQsÔºö &lt;https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs&gt;

ÊàñÁõ¥Êé•Âú®Á∫øÂí®ËØ¢ [È°πÁõÆÂ∞èÂä©Êâã](https://link-ai.tech/app/Kv2fXJcH)  (ËØ≠ÊñôÊåÅÁª≠ÂÆåÂñÑ‰∏≠ÔºåÂõûÂ§ç‰ªÖ‰æõÂèÇËÄÉ)

# üõ†Ô∏è ÂºÄÂèë

Ê¨¢ËøéÊé•ÂÖ•Êõ¥Â§öÂ∫îÁî®ÔºåÂèÇËÄÉ [Terminal‰ª£Á†Å](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/channel/terminal/terminal_channel.py) ÂÆûÁé∞Êé•Êî∂ÂíåÂèëÈÄÅÊ∂àÊÅØÈÄªËæëÂç≥ÂèØÊé•ÂÖ•„ÄÇ ÂêåÊó∂Ê¨¢ËøéÂ¢ûÂä†Êñ∞ÁöÑÊèí‰ª∂ÔºåÂèÇËÄÉ [Êèí‰ª∂ËØ¥ÊòéÊñáÊ°£](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins)„ÄÇ

# ‚úâ ËÅîÁ≥ª

Ê¨¢ËøéÊèê‰∫§PR„ÄÅIssuesÔºå‰ª•ÂèäStarÊîØÊåÅ‰∏Ä‰∏ã„ÄÇÁ®ãÂ∫èËøêË°åÈÅáÂà∞ÈóÆÈ¢òÂèØ‰ª•Êü•Áúã [Â∏∏ËßÅÈóÆÈ¢òÂàóË°®](https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs) ÔºåÂÖ∂Ê¨°ÂâçÂæÄ [Issues](https://github.com/zhayujie/chatgpt-on-wechat/issues) ‰∏≠ÊêúÁ¥¢„ÄÇ‰∏™‰∫∫ÂºÄÂèëËÄÖÂèØÂä†ÂÖ•ÂºÄÊ∫ê‰∫§ÊµÅÁæ§ÂèÇ‰∏éÊõ¥Â§öËÆ®ËÆ∫Ôºå‰ºÅ‰∏öÁî®Êà∑ÂèØËÅîÁ≥ª[‰∫ßÂìÅÈ°æÈóÆ](https://img-1317903499.cos.ap-guangzhou.myqcloud.com/docs/product-manager-qrcode.jpg)Âí®ËØ¢„ÄÇ

# üåü Ë¥°ÁåÆËÄÖ

![cow contributors](https://contrib.rocks/image?repo=zhayujie/chatgpt-on-wechat&amp;max=1000)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[openai/whisper]]></title>
            <link>https://github.com/openai/whisper</link>
            <guid>https://github.com/openai/whisper</guid>
            <pubDate>Fri, 11 Apr 2025 00:04:31 GMT</pubDate>
            <description><![CDATA[Robust Speech Recognition via Large-Scale Weak Supervision]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/openai/whisper">openai/whisper</a></h1>
            <p>Robust Speech Recognition via Large-Scale Weak Supervision</p>
            <p>Language: Python</p>
            <p>Stars: 79,742</p>
            <p>Forks: 9,583</p>
            <p>Stars today: 65 stars today</p>
            <h2>README</h2><pre># Whisper

[[Blog]](https://openai.com/blog/whisper)
[[Paper]](https://arxiv.org/abs/2212.04356)
[[Model card]](https://github.com/openai/whisper/blob/main/model-card.md)
[[Colab example]](https://colab.research.google.com/github/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb)

Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.


## Approach

![Approach](https://raw.githubusercontent.com/openai/whisper/main/approach.png)

A Transformer sequence-to-sequence model is trained on various speech processing tasks, including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. These tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing a single model to replace many stages of a traditional speech-processing pipeline. The multitask training format uses a set of special tokens that serve as task specifiers or classification targets.


## Setup

We used Python 3.9.9 and [PyTorch](https://pytorch.org/) 1.10.1 to train and test our models, but the codebase is expected to be compatible with Python 3.8-3.11 and recent PyTorch versions. The codebase also depends on a few Python packages, most notably [OpenAI&#039;s tiktoken](https://github.com/openai/tiktoken) for their fast tokenizer implementation. You can download and install (or update to) the latest release of Whisper with the following command:

    pip install -U openai-whisper

Alternatively, the following command will pull and install the latest commit from this repository, along with its Python dependencies:

    pip install git+https://github.com/openai/whisper.git 

To update the package to the latest version of this repository, please run:

    pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git

It also requires the command-line tool [`ffmpeg`](https://ffmpeg.org/) to be installed on your system, which is available from most package managers:

```bash
# on Ubuntu or Debian
sudo apt update &amp;&amp; sudo apt install ffmpeg

# on Arch Linux
sudo pacman -S ffmpeg

# on MacOS using Homebrew (https://brew.sh/)
brew install ffmpeg

# on Windows using Chocolatey (https://chocolatey.org/)
choco install ffmpeg

# on Windows using Scoop (https://scoop.sh/)
scoop install ffmpeg
```

You may need [`rust`](http://rust-lang.org) installed as well, in case [tiktoken](https://github.com/openai/tiktoken) does not provide a pre-built wheel for your platform. If you see installation errors during the `pip install` command above, please follow the [Getting started page](https://www.rust-lang.org/learn/get-started) to install Rust development environment. Additionally, you may need to configure the `PATH` environment variable, e.g. `export PATH=&quot;$HOME/.cargo/bin:$PATH&quot;`. If the installation fails with `No module named &#039;setuptools_rust&#039;`, you need to install `setuptools_rust`, e.g. by running:

```bash
pip install setuptools-rust
```


## Available models and languages

There are six model sizes, four with English-only versions, offering speed and accuracy tradeoffs.
Below are the names of the available models and their approximate memory requirements and inference speed relative to the large model.
The relative speeds below are measured by transcribing English speech on a A100, and the real-world speed may vary significantly depending on many factors including the language, the speaking speed, and the available hardware.

|  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |
|:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|
|  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~10x      |
|  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~7x       |
| small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~4x       |
| medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |
| large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |
| turbo  |   809 M    |        N/A         |      `turbo`       |     ~6 GB     |      ~8x       |

The `.en` models for English-only applications tend to perform better, especially for the `tiny.en` and `base.en` models. We observed that the difference becomes less significant for the `small.en` and `medium.en` models.
Additionally, the `turbo` model is an optimized version of `large-v3` that offers faster transcription speed with a minimal degradation in accuracy.

Whisper&#039;s performance varies widely depending on the language. The figure below shows a performance breakdown of `large-v3` and `large-v2` models by language, using WERs (word error rates) or CER (character error rates, shown in *Italic*) evaluated on the Common Voice 15 and Fleurs datasets. Additional WER/CER metrics corresponding to the other models and datasets can be found in Appendix D.1, D.2, and D.4 of [the paper](https://arxiv.org/abs/2212.04356), as well as the BLEU (Bilingual Evaluation Understudy) scores for translation in Appendix D.3.

![WER breakdown by language](https://github.com/openai/whisper/assets/266841/f4619d66-1058-4005-8f67-a9d811b77c62)



## Command-line usage

The following command will transcribe speech in audio files, using the `turbo` model:

    whisper audio.flac audio.mp3 audio.wav --model turbo

The default setting (which selects the `turbo` model) works well for transcribing English. To transcribe an audio file containing non-English speech, you can specify the language using the `--language` option:

    whisper japanese.wav --language Japanese

Adding `--task translate` will translate the speech into English:

    whisper japanese.wav --language Japanese --task translate

Run the following to view all available options:

    whisper --help

See [tokenizer.py](https://github.com/openai/whisper/blob/main/whisper/tokenizer.py) for the list of all available languages.


## Python usage

Transcription can also be performed within Python: 

```python
import whisper

model = whisper.load_model(&quot;turbo&quot;)
result = model.transcribe(&quot;audio.mp3&quot;)
print(result[&quot;text&quot;])
```

Internally, the `transcribe()` method reads the entire file and processes the audio with a sliding 30-second window, performing autoregressive sequence-to-sequence predictions on each window.

Below is an example usage of `whisper.detect_language()` and `whisper.decode()` which provide lower-level access to the model.

```python
import whisper

model = whisper.load_model(&quot;turbo&quot;)

# load audio and pad/trim it to fit 30 seconds
audio = whisper.load_audio(&quot;audio.mp3&quot;)
audio = whisper.pad_or_trim(audio)

# make log-Mel spectrogram and move to the same device as the model
mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)

# detect the spoken language
_, probs = model.detect_language(mel)
print(f&quot;Detected language: {max(probs, key=probs.get)}&quot;)

# decode the audio
options = whisper.DecodingOptions()
result = whisper.decode(model, mel, options)

# print the recognized text
print(result.text)
```

## More examples

Please use the [üôå Show and tell](https://github.com/openai/whisper/discussions/categories/show-and-tell) category in Discussions for sharing more example usages of Whisper and third-party extensions such as web demos, integrations with other tools, ports for different platforms, etc.


## License

Whisper&#039;s code and model weights are released under the MIT License. See [LICENSE](https://github.com/openai/whisper/blob/main/LICENSE) for further details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[THUDM/CogVideo]]></title>
            <link>https://github.com/THUDM/CogVideo</link>
            <guid>https://github.com/THUDM/CogVideo</guid>
            <pubDate>Fri, 11 Apr 2025 00:04:30 GMT</pubDate>
            <description><![CDATA[text and image to video generation: CogVideoX (2024) and CogVideo (ICLR 2023)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/THUDM/CogVideo">THUDM/CogVideo</a></h1>
            <p>text and image to video generation: CogVideoX (2024) and CogVideo (ICLR 2023)</p>
            <p>Language: Python</p>
            <p>Stars: 11,166</p>
            <p>Forks: 1,066</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre># CogVideo &amp; CogVideoX

[‰∏≠ÊñáÈòÖËØª](./README_zh.md)

[Êó•Êú¨Ë™û„ÅßË™≠„ÇÄ](./README_ja.md)

&lt;div align=&quot;center&quot;&gt;
&lt;img src=resources/logo.svg width=&quot;50%&quot;/&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
Experience the CogVideoX-5B model online at &lt;a href=&quot;https://huggingface.co/spaces/THUDM/CogVideoX-5B&quot; target=&quot;_blank&quot;&gt; ü§ó Huggingface Space&lt;/a&gt; or &lt;a href=&quot;https://modelscope.cn/studios/ZhipuAI/CogVideoX-5b-demo&quot; target=&quot;_blank&quot;&gt; ü§ñ ModelScope Space&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
üìö View the &lt;a href=&quot;https://arxiv.org/abs/2408.06072&quot; target=&quot;_blank&quot;&gt;paper&lt;/a&gt; and &lt;a href=&quot;https://zhipu-ai.feishu.cn/wiki/DHCjw1TrJiTyeukfc9RceoSRnCh&quot; target=&quot;_blank&quot;&gt;user guide&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    üëã Join our &lt;a href=&quot;resources/WECHAT.md&quot; target=&quot;_blank&quot;&gt;WeChat&lt;/a&gt; and &lt;a href=&quot;https://discord.gg/dCGfUsagrD&quot; target=&quot;_blank&quot;&gt;Discord&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
üìç Visit &lt;a href=&quot;https://chatglm.cn/video?lang=en?fr=osm_cogvideo&quot;&gt;QingYing&lt;/a&gt; and &lt;a href=&quot;https://open.bigmodel.cn/?utm_campaign=open&amp;_channel_track_key=OWTVNma9&quot;&gt;API Platform&lt;/a&gt; to experience larger-scale commercial video generation models.
&lt;/p&gt;

## Project Updates

- üî•üî• **News**: ```2025/03/24```: We have launched [CogKit](https://github.com/THUDM/CogKit), a fine-tuning and inference framework for the **CogView4** and **CogVideoX** series. This toolkit allows you to fully explore and utilize our multimodal generation models.
- üî• **News**: ```2025/02/28```: DDIM Inverse is now supported in `CogVideoX-5B` and `CogVideoX1.5-5B`. Check [here](inference/ddim_inversion.py).
- üî• **News**: ```2025/01/08```: We have updated the code for `Lora` fine-tuning based on the `diffusers` version model, which uses less GPU memory. For more details, please see [here](finetune/README.md).
- üî• **News**: ```2024/11/15```: We released the `CogVideoX1.5` model in the diffusers version. Only minor parameter adjustments are needed to continue using previous code.
- üî• **News**: ```2024/11/08```: We have released the CogVideoX1.5 model. CogVideoX1.5 is an upgraded version of the open-source model CogVideoX.
The CogVideoX1.5-5B series supports 10-second videos with higher resolution, and CogVideoX1.5-5B-I2V supports video generation at any resolution.
The SAT code has already been updated, while the diffusers version is still under adaptation. Download the SAT version code [here](https://huggingface.co/THUDM/CogVideoX1.5-5B-SAT).
- üî• **News**: ```2024/10/13```: A more cost-effective fine-tuning framework for `CogVideoX-5B` that works with a single
  4090 GPU, [cogvideox-factory](https://github.com/a-r-r-o-w/cogvideox-factory), has been released. It supports
  fine-tuning with multiple resolutions. Feel free to use it!
- üî• **News**: ```2024/10/10```: We have updated our technical report. Please
  click [here](https://arxiv.org/pdf/2408.06072) to view it. More training details and a demo have been added. To see
  the demo, click [here](https://yzy-thu.github.io/CogVideoX-demo/).- üî• **News**: ```2024/10/09```: We have publicly
  released the [technical documentation](https://zhipu-ai.feishu.cn/wiki/DHCjw1TrJiTyeukfc9RceoSRnCh) for CogVideoX
  fine-tuning on Feishu, further increasing distribution flexibility. All examples in the public documentation can be
  fully reproduced.
- üî• **News**: ```2024/9/19```: We have open-sourced the CogVideoX series image-to-video model **CogVideoX-5B-I2V**.
  This model can take an image as a background input and generate a video combined with prompt words, offering greater
  controllability. With this, the CogVideoX series models now support three tasks: text-to-video generation, video
  continuation, and image-to-video generation. Welcome to try it online
  at [Experience](https://huggingface.co/spaces/THUDM/CogVideoX-5B-Space).
- üî• ```2024/9/19```: The Caption
  model [CogVLM2-Caption](https://huggingface.co/THUDM/cogvlm2-llama3-caption), used in the training process of
  CogVideoX to convert video data into text descriptions, has been open-sourced. Welcome to download and use it.
- üî• ```2024/8/27```: We have open-sourced a larger model in the CogVideoX series, **CogVideoX-5B**. We have
  significantly optimized the model&#039;s inference performance, greatly lowering the inference threshold.
  You can run **CogVideoX-2B** on older GPUs like `GTX 1080TI`, and **CogVideoX-5B** on desktop GPUs like `RTX 3060`. Please strictly
  follow the [requirements](requirements.txt) to update and install dependencies, and refer
  to [cli_demo](inference/cli_demo.py) for inference code. Additionally, the open-source license for
  the **CogVideoX-2B** model has been changed to the **Apache 2.0 License**.
- üî• ```2024/8/6```: We have open-sourced **3D Causal VAE**, used for **CogVideoX-2B**, which can reconstruct videos with
  almost no loss.
- üî• ```2024/8/6```: We have open-sourced the first model of the CogVideoX series video generation models, **CogVideoX-2B
  **.
- üå± **Source**: ```2022/5/19```: We have open-sourced the CogVideo video generation model (now you can see it in
  the `CogVideo` branch). This is the first open-source large Transformer-based text-to-video generation model. You can
  access the [ICLR&#039;23 paper](https://arxiv.org/abs/2205.15868) for technical details.

## Table of Contents

Jump to a specific section:

- [Quick Start](#quick-start)
  - [Prompt Optimization](#prompt-optimization)
  - [SAT](#sat)
  - [Diffusers](#diffusers)
- [Gallery](#gallery)
  - [CogVideoX-5B](#cogvideox-5b)
  - [CogVideoX-2B](#cogvideox-2b)
- [Model Introduction](#model-introduction)
- [Friendly Links](#friendly-links)
- [Project Structure](#project-structure)
  - [Quick Start with Colab](#quick-start-with-colab)
  - [Inference](#inference)
  - [finetune](#finetune)
  - [sat](#sat-1)
  - [Tools](#tools)
- [CogVideo(ICLR&#039;23)](#cogvideoiclr23)
- [Citation](#citation)
- [Model-License](#model-license)

## Quick Start

### Prompt Optimization

Before running the model, please refer to [this guide](inference/convert_demo.py) to see how we use large models like
GLM-4 (or other comparable products, such as GPT-4) to optimize the model. This is crucial because the model is trained
with long prompts, and a good prompt directly impacts the quality of the video generation.

### SAT

**Please make sure your Python version is between 3.10 and 3.12, inclusive of both 3.10 and 3.12.**

Follow instructions in [sat_demo](sat/README.md): Contains the inference code and fine-tuning code of SAT weights. It is
recommended to improve based on the CogVideoX model structure. Innovative researchers use this code to better perform
rapid stacking and development.

### Diffusers

**Please make sure your Python version is between 3.10 and 3.12, inclusive of both 3.10 and 3.12.**

```
pip install -r requirements.txt
```

Then follow [diffusers_demo](inference/cli_demo.py): A more detailed explanation of the inference code, mentioning the
significance of common parameters.

For more details on quantized inference, please refer
to [diffusers-torchao](https://github.com/sayakpaul/diffusers-torchao/). With Diffusers and TorchAO, quantized inference
is also possible leading to memory-efficient inference as well as speedup in some cases when compiled. A full list of
memory and time benchmarks with various settings on A100 and H100 has been published
at [diffusers-torchao](https://github.com/sayakpaul/diffusers-torchao).

## Gallery

### CogVideoX-5B

&lt;table border=&quot;0&quot; style=&quot;width: 100%; text-align: left; margin-top: 20px;&quot;&gt;
  &lt;tr&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/cf5953ea-96d3-48fd-9907-c4708752c714&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/fe0a78e6-b669-4800-8cf0-b5f9b5145b52&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
       &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/c182f606-8f8c-421d-b414-8487070fcfcb&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
     &lt;/td&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/7db2bbce-194d-434d-a605-350254b6c298&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
     &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/62b01046-8cab-44cc-bd45-4d965bb615ec&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/d78e552a-4b3f-4b81-ac3f-3898079554f6&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
       &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/30894f12-c741-44a2-9e6e-ddcacc231e5b&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
     &lt;/td&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/926575ca-7150-435b-a0ff-4900a963297b&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
     &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

### CogVideoX-2B

&lt;table border=&quot;0&quot; style=&quot;width: 100%; text-align: left; margin-top: 20px;&quot;&gt;
  &lt;tr&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/ea3af39a-3160-4999-90ec-2f7863c5b0e9&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/9de41efd-d4d1-4095-aeda-246dd834e91d&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
      &lt;/td&gt;
       &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/941d6661-6a8d-4a1b-b912-59606f0b2841&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
     &lt;/td&gt;
      &lt;td&gt;
          &lt;video src=&quot;https://github.com/user-attachments/assets/938529c4-91ae-4f60-b96b-3c3947fa63cb&quot; width=&quot;100%&quot; controls autoplay loop&gt;&lt;/video&gt;
     &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

To view the corresponding prompt words for the gallery, please click [here](resources/galary_prompt.md)

## Model Introduction

CogVideoX is an open-source version of the video generation model originating
from [QingYing](https://chatglm.cn/video?lang=en?fr=osm_cogvideo). The table below displays the list of video generation
models we currently offer, along with their foundational information.

&lt;table style=&quot;border-collapse: collapse; width: 100%;&quot;&gt;
  &lt;tr&gt;
    &lt;th style=&quot;text-align: center;&quot;&gt;Model Name&lt;/th&gt;
    &lt;th style=&quot;text-align: center;&quot;&gt;CogVideoX1.5-5B (Latest)&lt;/th&gt;
    &lt;th style=&quot;text-align: center;&quot;&gt;CogVideoX1.5-5B-I2V (Latest)&lt;/th&gt;
    &lt;th style=&quot;text-align: center;&quot;&gt;CogVideoX-2B&lt;/th&gt;
    &lt;th style=&quot;text-align: center;&quot;&gt;CogVideoX-5B&lt;/th&gt;
    &lt;th style=&quot;text-align: center;&quot;&gt;CogVideoX-5B-I2V&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;Release Date&lt;/td&gt;
    &lt;th style=&quot;text-align: center;&quot;&gt;November 8, 2024&lt;/th&gt;
    &lt;th style=&quot;text-align: center;&quot;&gt;November 8, 2024&lt;/th&gt;
    &lt;th style=&quot;text-align: center;&quot;&gt;August 6, 2024&lt;/th&gt;
    &lt;th style=&quot;text-align: center;&quot;&gt;August 27, 2024&lt;/th&gt;
    &lt;th style=&quot;text-align: center;&quot;&gt;September 19, 2024&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;Video Resolution&lt;/td&gt;
    &lt;td colspan=&quot;1&quot; style=&quot;text-align: center;&quot;&gt;1360 * 768&lt;/td&gt;
    &lt;td colspan=&quot;1&quot; style=&quot;text-align: center;&quot;&gt; Min(W, H) = 768 &lt;br&gt; 768 ‚â§ Max(W, H) ‚â§ 1360 &lt;br&gt; Max(W, H) % 16 = 0 &lt;/td&gt;
    &lt;td colspan=&quot;3&quot; style=&quot;text-align: center;&quot;&gt;720 * 480&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;Number of Frames&lt;/td&gt;
    &lt;td colspan=&quot;2&quot; style=&quot;text-align: center;&quot;&gt;Should be &lt;b&gt;16N + 1&lt;/b&gt; where N &lt;= 10 (default 81)&lt;/td&gt;
    &lt;td colspan=&quot;3&quot; style=&quot;text-align: center;&quot;&gt;Should be &lt;b&gt;8N + 1&lt;/b&gt; where N &lt;= 6 (default 49)&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;Inference Precision&lt;/td&gt;
    &lt;td colspan=&quot;2&quot; style=&quot;text-align: center;&quot;&gt;&lt;b&gt;BF16 (Recommended)&lt;/b&gt;, FP16, FP32, FP8*, INT8, Not supported: INT4&lt;/td&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;&lt;b&gt;FP16*(Recommended)&lt;/b&gt;, BF16, FP32, FP8*, INT8, Not supported: INT4&lt;/td&gt;
    &lt;td colspan=&quot;2&quot; style=&quot;text-align: center;&quot;&gt;&lt;b&gt;BF16 (Recommended)&lt;/b&gt;, FP16, FP32, FP8*, INT8, Not supported: INT4&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;Single GPU Memory Usage&lt;br&gt;&lt;/td&gt;
    &lt;td colspan=&quot;2&quot; style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://github.com/THUDM/SwissArmyTransformer&quot;&gt;SAT&lt;/a&gt; BF16: 76GB &lt;br&gt;&lt;b&gt;diffusers BF16: from 10GB*&lt;/b&gt;&lt;br&gt;&lt;b&gt;diffusers INT8(torchao): from 7GB*&lt;/b&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://github.com/THUDM/SwissArmyTransformer&quot;&gt;SAT&lt;/a&gt; FP16: 18GB &lt;br&gt;&lt;b&gt;diffusers FP16: 4GB minimum* &lt;/b&gt;&lt;br&gt;&lt;b&gt;diffusers INT8 (torchao): 3.6GB minimum*&lt;/b&gt;&lt;/td&gt;
    &lt;td colspan=&quot;2&quot; style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://github.com/THUDM/SwissArmyTransformer&quot;&gt;SAT&lt;/a&gt; BF16: 26GB &lt;br&gt;&lt;b&gt;diffusers BF16 : 5GB minimum* &lt;/b&gt;&lt;br&gt;&lt;b&gt;diffusers INT8 (torchao): 4.4GB minimum* &lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;Multi-GPU Memory Usage&lt;/td&gt;
    &lt;td colspan=&quot;2&quot; style=&quot;text-align: center;&quot;&gt;&lt;b&gt;BF16: 24GB* using diffusers&lt;/b&gt;&lt;br&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;&lt;b&gt;FP16: 10GB* using diffusers&lt;/b&gt;&lt;br&gt;&lt;/td&gt;
    &lt;td colspan=&quot;2&quot; style=&quot;text-align: center;&quot;&gt;&lt;b&gt;BF16: 15GB* using diffusers&lt;/b&gt;&lt;br&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;Inference Speed&lt;br&gt;(Step = 50, FP/BF16)&lt;/td&gt;
    &lt;td colspan=&quot;2&quot; style=&quot;text-align: center;&quot;&gt;Single A100: ~1000 seconds (5-second video)&lt;br&gt;Single H100: ~550 seconds (5-second video)&lt;/td&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;Single A100: ~90 seconds&lt;br&gt;Single H100: ~45 seconds&lt;/td&gt;
    &lt;td colspan=&quot;2&quot; style=&quot;text-align: center;&quot;&gt;Single A100: ~180 seconds&lt;br&gt;Single H100: ~90 seconds&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;Prompt Language&lt;/td&gt;
    &lt;td colspan=&quot;5&quot; style=&quot;text-align: center;&quot;&gt;English*&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;Prompt Token Limit&lt;/td&gt;
    &lt;td colspan=&quot;2&quot; style=&quot;text-align: center;&quot;&gt;224 Tokens&lt;/td&gt;
    &lt;td colspan=&quot;3&quot; style=&quot;text-align: center;&quot;&gt;226 Tokens&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;Video Length&lt;/td&gt;
    &lt;td colspan=&quot;2&quot; style=&quot;text-align: center;&quot;&gt;5 seconds or 10 seconds&lt;/td&gt;
    &lt;td colspan=&quot;3&quot; style=&quot;text-align: center;&quot;&gt;6 seconds&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;Frame Rate&lt;/td&gt;
    &lt;td colspan=&quot;2&quot; style=&quot;text-align: center;&quot;&gt;16 frames / second &lt;/td&gt;
    &lt;td colspan=&quot;3&quot; style=&quot;text-align: center;&quot;&gt;8 frames / second &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;Position Encoding&lt;/td&gt;
    &lt;td colspan=&quot;2&quot; style=&quot;text-align: center;&quot;&gt;3d_rope_pos_embed&lt;/td&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;3d_sincos_pos_embed&lt;/td&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;3d_rope_pos_embed&lt;/td&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;3d_rope_pos_embed + learnable_pos_embed&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;Download Link (Diffusers)&lt;/td&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://huggingface.co/THUDM/CogVideoX1.5-5B&quot;&gt;ü§ó HuggingFace&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://modelscope.cn/models/ZhipuAI/CogVideoX1.5-5B&quot;&gt;ü§ñ ModelScope&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://wisemodel.cn/models/ZhipuAI/CogVideoX1.5-5B&quot;&gt;üü£ WiseModel&lt;/a&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://huggingface.co/THUDM/CogVideoX1.5-5B-I2V&quot;&gt;ü§ó HuggingFace&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://modelscope.cn/models/ZhipuAI/CogVideoX1.5-5B-I2V&quot;&gt;ü§ñ ModelScope&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://wisemodel.cn/models/ZhipuAI/CogVideoX1.5-5B-I2V&quot;&gt;üü£ WiseModel&lt;/a&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://huggingface.co/THUDM/CogVideoX-2b&quot;&gt;ü§ó HuggingFace&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://modelscope.cn/models/ZhipuAI/CogVideoX-2b&quot;&gt;ü§ñ ModelScope&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://wisemodel.cn/models/ZhipuAI/CogVideoX-2b&quot;&gt;üü£ WiseModel&lt;/a&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://huggingface.co/THUDM/CogVideoX-5b&quot;&gt;ü§ó HuggingFace&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://modelscope.cn/models/ZhipuAI/CogVideoX-5b&quot;&gt;ü§ñ ModelScope&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://wisemodel.cn/models/ZhipuAI/CogVideoX-5b&quot;&gt;üü£ WiseModel&lt;/a&gt;&lt;/td&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://huggingface.co/THUDM/CogVideoX-5b-I2V&quot;&gt;ü§ó HuggingFace&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://modelscope.cn/models/ZhipuAI/CogVideoX-5b-I2V&quot;&gt;ü§ñ ModelScope&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://wisemodel.cn/models/ZhipuAI/CogVideoX-5b-I2V&quot;&gt;üü£ WiseModel&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;text-align: center;&quot;&gt;Download Link (SAT)&lt;/td&gt;
    &lt;td colspan=&quot;2&quot; style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://huggingface.co/THUDM/CogVideoX1.5-5b-SAT&quot;&gt;ü§ó HuggingFace&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://modelscope.cn/models/ZhipuAI/CogVideoX1.5-5b-SAT&quot;&gt;ü§ñ ModelScope&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://wisemodel.cn/models/ZhipuAI/CogVideoX1.5-5b-SAT&quot;&gt;üü£ WiseModel&lt;/a&gt;&lt;/td&gt;
    &lt;td colspan=&quot;3&quot; style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;./sat/README_zh.md&quot;&gt;SAT&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

**Data Explanation**

+ While testing using the diffusers library, all optimizations included in the diffusers library were enabled. This
  scheme has not been tested for actual memory usage on devices outside of **NVIDIA A100 / H100** architectures.
  Generally, this scheme can be adapted to all **NVIDIA Ampere architecture** and above devices. If optimizations are
  disabled, memory consumption will multiply, with peak memory usage being about 3 times the value in the table.
  However, speed will increase by about 3-4 times. You can selectively disable some optimizations, including:

```
pipe.enable_sequential_cpu_offload()
pipe.vae.enable_slicing()
pipe.vae.enable_tiling()
```

+ For multi-GPU inference, the `enable_sequential_cpu_offload()` optimization needs to be disabled.
+ Using INT8 models will slow down inference, which is done to accommodate lower-memory GPUs while maintaining minimal
  video quality loss, though inference speed will significantly decrease.
+ The CogVideoX-2B model was trained in `FP16` precision, and all CogVideoX-5B models were trained in `BF16` precision.
  We recommend using the precision in which the model was trained for inference.
+ [PytorchAO](https://github.com/pytorch/ao) and [Optimum-quanto](https://github.com/huggingface/optimum-quanto/) can be
  used to quantize the text encoder, transformer, and VAE modules to reduce the memory requirements of CogVideoX. This
  allows the model to run on free T4 Colabs or GPUs with smaller memory! Also, note that TorchAO quantization is fully
  compatible with `torch.compile`, which can significantly improve inference speed. FP8 precision must be used on
  devices with NVIDIA H100 and above, requiring source installation of `torch`, `torchao` Python packages. CUDA 12.4 is recommended.
+ The inference speed tests also used the above memory optimization scheme. Without memory optimization, inference speed
  increases by about 10%. Only the `diffusers` version of the model supports quantization.
+ The model only supports English input; other languages can be translated into English for use via large model
  refinement.


## Friendly Links

We highly welcome contributions from the community and actively contribute to the open-source community. The following
works have already been adapted for CogVideoX, and we invite everyone to use them:

+ [RIFLEx-CogVideoX](https://github.com/thu-ml/RIFLEx)Ôºö
  RIFLEx extends the video with just one line of code: `freq[k-1]=(2np.pi)/(Ls)`. The framework not only supports training-free inference, but also offers models fine-tuned based on CogVideoX. By fine-tuning the model for just 1,000 steps on original-length videos, RIFLEx significantly enhances its length extrapolation capability.
+ [CogVideoX-Fun](https://github.com/aigc-apps/CogVideoX-Fun): CogVideoX-Fun is a modified pipeline based on the
  CogVideoX architecture, supporting flexible resolutions and multiple launch methods.
+ [CogStudio](https://github.com/pinokiofactory/cogstudio): A separate repository for CogVideo&#039;s Gradio Web UI, which
  supports more functional Web UIs.
+ [Xorbits Inference](https://github.com/xorbitsai/inference): A powerful and comprehensive distributed inference
  framework, allowing you to easily deploy your own models or the latest cutting-edge open-source models with just one
  click.
+ [ComfyUI-CogVideoXWrapper](https://github.com/kijai/ComfyUI-CogVideoXWrapper) Use the ComfyUI framework to integrate
  CogVideoX into your workflow.
+ [VideoSys](https://github.com/NUS-HPC-AI-Lab/VideoSys): VideoSys pr

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[instructlab/instructlab]]></title>
            <link>https://github.com/instructlab/instructlab</link>
            <guid>https://github.com/instructlab/instructlab</guid>
            <pubDate>Fri, 11 Apr 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[InstructLab Core package. Use this to chat with a model and execute the InstructLab workflow to train a model using custom taxonomy data.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/instructlab/instructlab">instructlab/instructlab</a></h1>
            <p>InstructLab Core package. Use this to chat with a model and execute the InstructLab workflow to train a model using custom taxonomy data.</p>
            <p>Language: Python</p>
            <p>Stars: 1,229</p>
            <p>Forks: 416</p>
            <p>Stars today: 5 stars today</p>
            <h2>README</h2><pre># InstructLab üê∂ (`ilab`)

![Lint](https://github.com/instructlab/instructlab/actions/workflows/lint.yml/badge.svg?branch=main)
![Tests](https://github.com/instructlab/instructlab/actions/workflows/test.yml/badge.svg?branch=main)
![Build](https://github.com/instructlab/instructlab/actions/workflows/pypi.yaml/badge.svg?branch=main)
![Release](https://img.shields.io/github/v/release/instructlab/instructlab)
![License](https://img.shields.io/github/license/instructlab/instructlab)

![`e2e-nvidia-t4-x1.yaml` on `main`](https://github.com/instructlab/instructlab/actions/workflows/e2e-nvidia-t4-x1.yml/badge.svg?branch=main)
![`e2e-nvidia-l4-x1.yaml` on `main`](https://github.com/instructlab/instructlab/actions/workflows/e2e-nvidia-l4-x1.yml/badge.svg?branch=main)
![`e2e-nvidia-l40s-x4.yaml` on `main`](https://github.com/instructlab/instructlab/actions/workflows/e2e-nvidia-l40s-x4.yml/badge.svg?branch=main)
![`e2e-nvidia-l40s-x8.yaml` on `main`](https://github.com/instructlab/instructlab/actions/workflows/e2e-nvidia-l40s-x8.yml/badge.svg?branch=main)

## üìñ Contents

- [Welcome to the InstructLab Core](#welcome-to-the-instructlab-core)
- [‚ùì What is InstructLab Core](#-what-is-instructlab-core)
- [üìã Requirements](#-requirements)
- [‚úÖ Getting started](#-getting-started)
  - [üß∞ Installing InstructLab Core](#-installing-instructlab-core)
    - [Install with Apple Metal on M1/M2/M3 Macs](#install-with-apple-metal-on-m1m2m3-macs)
    - [Install with no GPU acceleration and PyTorch without CUDA bindings](#install-using-pytorch-without-cuda-bindings-and-no-gpu-acceleration)
    - [Install with AMD ROCm](#install-with-amd-rocm)
    - [Install with Nvidia CUDA](#install-with-nvidia-cuda)
  - [üèóÔ∏è Initialize `ilab`](#Ô∏è-initialize-ilab)
  - [üì• Download the model](#-download-the-model)
  - [üç¥ Serving the model](#-serving-the-model)
  - [üì£ Chat with the model (Optional)](#-chat-with-the-model-optional)
  - [üìá Configure retrieval-augmented generation (developer preview)](#-configure-retrieval-augmented-generation-developer-preview)
  - [üöÄ Upgrade InstructLab to latest version](#-upgrade-instructlab-to-latest-version)
- [üíª Creating new knowledge or skills and training the model](#-creating-new-knowledge-or-skills-and-training-the-model)
  - [üéÅ Contribute knowledge or compositional skills](#-contribute-knowledge-or-compositional-skills)
  - [üìú List and validate your new data](#-list-and-validate-your-new-data)
  - [üöÄ Generate a synthetic dataset](#-generate-a-synthetic-dataset)
  - [üë©‚Äçüè´ Training the model](#-training-the-model)
    - [‚úã Before you begin training](#-before-you-begin-training)
    - [InstructLab training pipelines](#instructlab-model-training-pipelines)
    - [Train the model locally](#train-the-model-locally)
    - [Train the model locally on an M-Series Mac or on Linux using the full pipeline](#train-the-model-locally-on-an-m-series-mac-or-on-linux-using-the-full-pipeline)
    - [Train the model locally on an M-Series Mac or on Linux using the simple pipeline](#train-the-model-locally-on-an-m-series-mac-or-on-linux-using-the-simple-pipeline)
    - [Train the model locally with GPU acceleration](#train-the-model-locally-with-gpu-acceleration)
    - [Train the model in the cloud](#train-the-model-in-the-cloud)
  - [üìú Test the newly trained model](#-test-the-newly-trained-model)
  - [üß™ Evaluate the newly trained model](#-evaluate-the-newly-trained-model)
  - [üç¥ Serve the newly trained model](#-serve-the-newly-trained-model)
- [üì£ Chat with the new model (not optional this time)](#-chat-with-the-new-model-not-optional-this-time)
- [‚òÅÔ∏è Upload the new model](#Ô∏è-upload-the-new-model)
- [üéÅ Submit your new knowledge or skills](#-submit-your-new-knowledge-or-skills)
- [üì¨ Contributing](#-contributing)

## Welcome to the InstructLab Core

InstructLab üê∂ uses a novel synthetic data-based alignment tuning method for
Large Language Models (LLMs.) The &quot;**lab**&quot; in Instruct**Lab** üê∂ stands for
[**L**arge-Scale **A**lignment for Chat**B**ots](https://arxiv.org/abs/2403.01081) [1].

[1] Shivchander Sudalairaj*, Abhishek Bhandwaldar*, Aldo Pareja*, Kai Xu, David D. Cox, Akash Srivastava*. &quot;LAB: Large-Scale Alignment for ChatBots&quot;, arXiv preprint arXiv: 2403.01081, 2024. (* denotes equal contributions)

## ‚ùì What is InstructLab Core

`instructlab` is the Core package for the InstructLab project that contains the `ilab` Command-Line Interface (CLI) tool and allows you to perform the following actions:

1. Download a pre-trained Large Language Model (LLM).
1. Chat with the LLM.

To add new knowledge and skills to the pre-trained LLM, add information to the companion [taxonomy](https://github.com/instructlab/taxonomy.git) repository.

After you have added knowledge and skills to the taxonomy, you can perform the following actions:

1. Use `ilab` to generate new synthetic training data based on the changes in your local `taxonomy` repository.
1. Re-train the LLM with the new training data.
1. Chat with the re-trained LLM to see the results.

```mermaid
graph TD;
  download--&gt;chat
  chat[Chat with the LLM]--&gt;add
  add[Add new knowledge&lt;br/&gt;or skill to taxonomy]--&gt;generate[generate new&lt;br/&gt;synthetic training data]
  generate--&gt;train
  train[Re-train]--&gt;|Chat with&lt;br/&gt;the re-trained LLM&lt;br/&gt;to see the results|chat
```

For an overview of the full workflow, see the [workflow diagram](./docs/workflow.png).

&gt; [!IMPORTANT]
&gt; We have optimized InstructLab so that community members with commodity hardware can perform these steps. However, running InstructLab on a laptop will provide a low-fidelity approximation of synthetic data generation
&gt; (using the `ilab data generate` command) and model instruction tuning (using the `ilab model train` command, which uses QLoRA). To achieve higher quality, use more sophisticated hardware and configure InstructLab to use a
&gt; larger teacher model [such as Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral).

## üìã Requirements

- **üçé Apple M1/M2/M3 Mac or üêß Linux system** (tested on Fedora). We anticipate support for more operating systems in the future.

   üìã  When installing InstructLab Core on macOS, you may have to run the `xcode-select --install` command, installing the required packages listed.

- C++ compiler
- Python 3.10 or Python 3.11

   ‚ö†Ô∏è Python 3.12+ is currently not supported. Some InstructLab dependencies don&#039;t work on Python 3.12, yet. It is recommended to use a specific version of Python in the below commands, e.g. `python3.11` instead of simply `python3`.

- Minimum 250GB disk space. Approximately 500GB disk space is recommended for the entire InstructLab end-to-end process.

## ‚úÖ Getting started

- When installing on Fedora Linux, install C++, Python 3.10 or 3.11, and other necessary tools by running the following command:

   ```shell
   sudo dnf install gcc gcc-c++ make git-core python3.11 python3.11-devel
   ```

   Some Python version management tools that build Python (instead of using a pre-built binary) may not by default build libraries implemented in C, and CPython when installing a Python version. This can result in the following error when running the `ilab data generate` command: `ModuleNotFoundError: No module named &#039;_lzma&#039;`. This can be resolved by building CPython during the Python installation with the `--enable-framework`. For example for `pyenv` on MacOS: `PYTHON_CONFIGURE_OPTS=&quot;--enable-framework&quot; pyenv install 3.x`. You may need to recreate your virtual environment after reinstalling Python.

&gt; [!NOTE]
&gt; The following steps in this document use [Python venv](https://docs.python.org/3/library/venv.html) for virtual environments. However, if you use another tool such as [pyenv](https://github.com/pyenv/pyenv) or [Conda Miniforge](https://github.com/conda-forge/miniforge) for managing Python environments on your machine continue to use that tool instead. Otherwise, you may have issues with packages that are installed but not found in your virtual environment.

### üß∞ Installing InstructLab Core

1. There are a few ways you can locally install the InstructLab Core package. Select your preferred installation method from the following instructions. You can then install `ilab` and activate your `venv` environment.

   ‚ö†Ô∏è The `python3` binary shown in the installation guides are the Python version that you installed in the above step. The command can also be `python3.11` or `python3.10` instead of `python3`. You can check Python&#039;s version by `python3 -V`.

   ‚è≥ `pip install` may take some time, depending on your internet connection. In case the installation fails with error ``unsupported instruction `vpdpbusd&#039;``, append `-C cmake.args=&quot;-DGGML_NATIVE=off&quot;` to `pip install` command.

   See [the GPU acceleration documentation](./docs/accelerators/gpu-acceleration.md) for how to to enable hardware acceleration for interaction and training on AMD ROCm, Apple Metal Performance Shaders (MPS), and Nvidia CUDA.

#### Install with Apple Metal on M1/M2/M3 Macs

- Install on Apple metal with:

   ```shell
   python&lt;version&gt; -m venv --upgrade-deps venv
   source venv/bin/activate
   pip cache remove llama_cpp_python
   pip install instructlab
   ```

   üìã Make sure your system Python build is `Mach-O 64-bit executable arm64` by using `file -b $(command -v python)`, or if your system is setup with [pyenv](https://github.com/pyenv/pyenv) by using the `file -b $(pyenv which python)` command.

   You can also quickly install using the [Bash script](https://github.com/instructlab/instructlab/blob/main/scripts/ilab-macos-installer.sh).

#### Install using PyTorch without CUDA bindings and no GPU acceleration

- Install on a standard Linux machine with:

   ```shell
   python&lt;version&gt; -m venv --upgrade-deps venv
   source venv/bin/activate
   pip install instructlab
   ```

   *Additional Build Argument for Intel Macs*
   If you have a Mac with an Intel CPU, you must add a prefix of
   `CMAKE_ARGS=&quot;-DGGML_METAL=off&quot;` to the `pip install` command to ensure
   that the build is done without Apple M-series GPU support.
   `(venv) $ CMAKE_ARGS=&quot;-DGGML_METAL=off&quot; pip install ...`

#### Install with AMD ROCm

- Install on AMD ROCm with:

   ```shell
   python&lt;version&gt; -m venv --upgrade-deps venv
   source venv/bin/activate
   pip cache remove llama_cpp_python
   CMAKE_ARGS=&quot;-DGGML_HIPBLAS=on \
      -DAMDGPU_TARGETS=all \
      -DCMAKE_C_COMPILER=/opt/rocm/llvm/bin/clang \
      -DCMAKE_CXX_COMPILER=/opt/rocm/llvm/bin/clang++ \
      -DCMAKE_PREFIX_PATH=/opt/rocm \
      -DGGML_NATIVE=off&quot; \
      pip install &#039;instructlab[rocm]&#039; \
      --extra-index-url https://download.pytorch.org/whl/rocm6.0
   ```

   On Fedora 40+, use `-DCMAKE_C_COMPILER=clang-17` and `-DCMAKE_CXX_COMPILER=clang++-17`.

#### Install with Nvidia CUDA

- For the best CUDA experience, installing vLLM is necessary to serve Safetensors format models.

   ```shell
   python&lt;version&gt; -m venv --upgrade-deps venv
   source venv/bin/activate
   pip install torch psutil
   pip cache remove llama_cpp_python
   CMAKE_ARGS=&quot;-DGGML_CUDA=on -DGGML_NATIVE=off&quot; pip install &#039;instructlab[cuda]&#039;
   pip install -r requirements-vllm-cuda.txt
   ```

4. From your `venv` environment, verify `ilab` is installed correctly, by running the `ilab` command.

   ```shell
   ilab
   ```

   *Example output of the `ilab` command*

   ```shell
   (venv) $ ilab
   Usage: ilab [OPTIONS] COMMAND [ARGS]...

     CLI for interacting with InstructLab.

     If this is your first time running InstructLab, it&#039;s best to start with `ilab config init` to create the environment.

   Options:
     --config PATH  Path to a configuration file.  [default:
                    /home/user/.config/instructlab/config.yaml]
     -v, --verbose  Enable debug logging (repeat for even more verbosity)
     --version      Show the version and exit.
     --help         Show this message and exit.

   Commands:
     config    Manage InstructLab configuration.
     data      Generate synthetic data.
     model     Manage GenAI (LLM) models.
     process   Manage running processes.
     rag       Retrieval-Augmented Generation (RAG).
     system    Execute system commands.
     taxonomy  Manage taxonomy datasets.

   Aliases:
     chat      model chat
     generate  data generate
     serve     model serve
     train     model train
   ```

&gt; [!IMPORTANT]
&gt; Every `ilab` command needs to be run from within your Python virtual environment. You can enter the Python environment by running the `source venv/bin/activate` command. Or you can simply create a symbolic link to a directory that is included in your system‚Äôs `$PATH`, for example in Linux: `mkdir -p ~/bin/ &amp;&amp; ln -s /path/venv/bin/ilab ~/bin/ilab`, or use an alias: `alias ilab=&#039;/path/venv/bin/ilab&#039;` (add it to `~/.bashrc` or `~/.zshrc` to persist).

5. *Optional:* You can enable tab completion for the `ilab` command.

   #### Bash (version 4.4 or newer)

   Enable tab completion in `bash` with the following command:

   ```sh
   eval &quot;$(_ILAB_COMPLETE=bash_source ilab)&quot;
   ```

   To have this enabled automatically every time you open a new shell,
   you can save the completion script and source it from `~/.bashrc`:

   ```sh
   _ILAB_COMPLETE=bash_source ilab &gt; ~/.ilab-complete.bash
   echo &quot;. ~/.ilab-complete.bash&quot; &gt;&gt; ~/.bashrc
   ```

   üìã To use Bash version 4.4 or higher on macOS (default is 3.2.57), ensure your login shell is set to the updated version. You can verify this with `echo $SHELL`. If you encounter the error `bash: complete: nosort: invalid option name`, check your terminal or configuration files (e.g., ~/.bash_profile, ~/.bashrc, ~/.profile) to see whether they are referencing the old version for login.

   #### Zsh

   Enable tab completion in `zsh` with the following command:

   ```sh
   eval &quot;$(_ILAB_COMPLETE=zsh_source ilab)&quot;
   ```

   To have this enabled automatically every time you open a new shell,
   you can save the completion script and source it from `~/.zshrc`:

   ```sh
   _ILAB_COMPLETE=zsh_source ilab &gt; ~/.ilab-complete.zsh
   echo &quot;. ~/.ilab-complete.zsh&quot; &gt;&gt; ~/.zshrc
   ```

   #### Fish

   Enable tab completion in `fish` with the following command:

   ```sh
   _ILAB_COMPLETE=fish_source ilab | source
   ```

   To have this enabled automatically every time you open a new shell,
   you can save the completion script and source it from `~/.bashrc`:

   ```sh
   _ILAB_COMPLETE=fish_source ilab &gt; ~/.config/fish/completions/ilab.fish
   ```

### üèóÔ∏è Initialize `ilab`

1. Initialize `ilab` by running the following command:

   ```shell
   ilab config init
   ```

2. When prompted, clone the `https://github.com/instructlab/taxonomy.git` repository into the current directory by typing **enter**

   **Optional**: If you want to point to an existing local clone of the `taxonomy` repository, you can pass the path interactively or alternatively with the `--taxonomy-path` flag.

   `ilab` will use the default configuration file unless otherwise specified. You can override this behavior with the `--config` parameter for any `ilab` command.

4. When prompted, provide the path to your default model. Otherwise, the default of a quantized [Granite](https://huggingface.co/instructlab/granite-7b-lab-GGUF) model is used.

   *Example output of steps 1 - 3*

   ```shell
   Path to your model [/home/user/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf]: &lt;ENTER&gt;
   ```

   You can download this model with `ilab model download` command as well.

5. When prompted, please choose a train profile. Train profiles are GPU specific profiles that enable accelerated training behavior. If you are on MacOS or a Linux machine without a dedicated GPU, please choose `No Profile (CPU, Apple Metal, AMD ROCm)` by hitting Enter. There are various flags you can utilize with individual `ilab` commands that allow you to utilize your GPU if applicable.

   *Example output of selecting a training profile*

   ```shell
   ----------------------------------------------------
            Welcome to the InstructLab CLI
   This guide will help you to setup your environment
   ----------------------------------------------------

   Please provide the following values to initiate the environment [press Enter for defaults]:
   Path to taxonomy repo [/home/user/.local/share/instructlab/taxonomy]:
   Path to your model [/home/user/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf]:

   You can download this model with `ilab model download` command as well.

4. The InstructLab Core package auto-detects your hardware and select the exact system profile that matches your machine. System profiles populate the `config.yaml` file with the proper parameter values based on your detected CPU/GPU types. This system is only applicable to Apple M-Series Chips, Nvidia GPUs, and Intel/AMD CPUs.

   *Example output of profile auto-detection*

   ```shell
   Generating config file and profiles:
       /home/user/.config/instructlab/config.yaml
       /home/user/.local/share/instructlab/internal/train_configuration/profiles

   We have detected the AMD CPU profile as an exact match for your system.

    --------------------------------------------
      Initialization completed successfully!
   You&#039;re ready to start using `ilab`. Enjoy!
   --------------------------------------------
   ```

5. If there is not an exact match for your system, you can manually select a system profile when prompted. There are various flags you can utilize with individual `ilab` commands that allow you to utilize your GPU if applicable.

   *Example output of selecting a system profile*

   ```shell
   Please choose a system profile to use.
   System profiles apply to all parts of the config file and set hardware specific defaults for each command.
   First, please select the hardware vendor your system falls into
   [1] APPLE
   [2] INTEL
   [3] AMD
   [4] NVIDIA
   Enter the number of your choice [0]: 1
   You selected: APPLE
   Next, please select the specific hardware configuration that most closely matches your system.
   [0] No system profile
   [1] APPLE M1 ULTRA
   [2] APPLE M1 MAX
   [3] APPLE M2 MAX
   [4] APPLE M2 ULTRA
   [5] APPLE M2 PRO
   [6] APPLE M2
   [7] APPLE M3 MAX
   [8] APPLE M3 PRO
   [9] APPLE M3
   Enter the number of your choice [hit enter for hardware defaults] [0]: 8
   You selected: /home/&lt;user&gt;/.local/share/instructlab/internal/system_profiles/apple/m3/m3_pro.yaml

   --------------------------------------------
   Initialization completed successfully!
   You&#039;re ready to start using `ilab`. Enjoy!
   --------------------------------------------
   ```

   The GPU profiles are listed by GPU type and number of GPUs present. If you happen to have a GPU configuration with a similar amount of vRAM as any of the above profiles, feel free to try them out!

### `ilab` directory layout after initializing your system

After running `ilab config init` your directories will look like the following on a Linux system:

| **Directory**                              | **Description**                                                                 |
|--------------------------------------------|---------------------------------------------------------------------------------|
| `~/.cache/instructlab/models/`             | Contains all downloaded large language models, including the saved output of ones you generate with ilab.|
| `~/.local/share/instructlab/datasets/`     | Contains data output from the SDG phase, built on modifications to the taxonomy repository.   |
| `~/.local/share/instructlab/taxonomy/`     | Contains the skill and knowledge data.                                              |
| `~/.local/share/instructlab/checkpoints/`  | Contains the output of the training process.                                |
| `~/.config/instructlab/config.yaml`        | Contains the `config.yaml` file |

You can view your `config.yaml` file with the following command (use `-k` to show a specific config section and/or `-wc` to show without comments

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[crewAIInc/crewAI]]></title>
            <link>https://github.com/crewAIInc/crewAI</link>
            <guid>https://github.com/crewAIInc/crewAI</guid>
            <pubDate>Fri, 11 Apr 2025 00:04:28 GMT</pubDate>
            <description><![CDATA[Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/crewAIInc/crewAI">crewAIInc/crewAI</a></h1>
            <p>Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.</p>
            <p>Language: Python</p>
            <p>Stars: 29,847</p>
            <p>Forks: 4,032</p>
            <p>Stars today: 87 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

![Logo of CrewAI](./docs/crewai_logo.png)


&lt;/div&gt;

### Fast and Flexible Multi-Agent Automation Framework

CrewAI is a lean, lightning-fast Python framework built entirely from
scratch‚Äîcompletely **independent of LangChain or other agent frameworks**.
It empowers developers with both high-level simplicity and precise low-level
control, ideal for creating autonomous AI agents tailored to any scenario.

- **CrewAI Crews**: Optimize for autonomy and collaborative intelligence.
- **CrewAI Flows**: Enable granular, event-driven control, single LLM calls for precise task orchestration and supports Crews natively

With over 100,000 developers certified through our community courses at
[learn.crewai.com](https://learn.crewai.com), CrewAI is rapidly becoming the
standard for enterprise-ready AI automation.

# CrewAI Enterprise Suite

CrewAI Enterprise Suite is a comprehensive bundle tailored for organizations
that require secure, scalable, and easy-to-manage agent-driven automation.

You can try one part of the suite the [Crew Control Plane for free](https://app.crewai.com)

## Crew Control Plane Key Features:
- **Tracing &amp; Observability**: Monitor and track your AI agents and workflows in real-time, including metrics, logs, and traces.
- **Unified Control Plane**: A centralized platform for managing, monitoring, and scaling your AI agents and workflows.
- **Seamless Integrations**: Easily connect with existing enterprise systems, data sources, and cloud infrastructure.
- **Advanced Security**: Built-in robust security and compliance measures ensuring safe deployment and management.
- **Actionable Insights**: Real-time analytics and reporting to optimize performance and decision-making.
- **24/7 Support**: Dedicated enterprise support to ensure uninterrupted operation and quick resolution of issues.
- **On-premise and Cloud Deployment Options**: Deploy CrewAI Enterprise on-premise or in the cloud, depending on your security and compliance requirements.

CrewAI Enterprise is designed for enterprises seeking a powerful,
reliable solution to transform complex business processes into efficient,
intelligent automations.

&lt;h3&gt;

[Homepage](https://www.crewai.com/) | [Documentation](https://docs.crewai.com/) | [Chat with Docs](https://chatg.pt/DWjSBZn) | [Discourse](https://community.crewai.com)

&lt;/h3&gt;

[![GitHub Repo stars](https://img.shields.io/github/stars/joaomdmoura/crewAI)](https://github.com/crewAIInc/crewAI)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)

&lt;/div&gt;

## Table of contents

- [Why CrewAI?](#why-crewai)
- [Getting Started](#getting-started)
- [Key Features](#key-features)
- [Understanding Flows and Crews](#understanding-flows-and-crews)
- [CrewAI vs LangGraph](#how-crewai-compares)
- [Examples](#examples)
  - [Quick Tutorial](#quick-tutorial)
  - [Write Job Descriptions](#write-job-descriptions)
  - [Trip Planner](#trip-planner)
  - [Stock Analysis](#stock-analysis)
  - [Using Crews and Flows Together](#using-crews-and-flows-together)
- [Connecting Your Crew to a Model](#connecting-your-crew-to-a-model)
- [How CrewAI Compares](#how-crewai-compares)
- [Frequently Asked Questions (FAQ)](#frequently-asked-questions-faq)
- [Contribution](#contribution)
- [Telemetry](#telemetry)
- [License](#license)

## Why CrewAI?

&lt;div align=&quot;center&quot; style=&quot;margin-bottom: 30px;&quot;&gt;
  &lt;img src=&quot;docs/asset.png&quot; alt=&quot;CrewAI Logo&quot; width=&quot;100%&quot;&gt;
&lt;/div&gt;

CrewAI unlocks the true potential of multi-agent automation, delivering the best-in-class combination of speed, flexibility, and control with either Crews of AI Agents or Flows of Events:

- **Standalone Framework**: Built from scratch, independent of LangChain or any other agent framework.
- **High Performance**: Optimized for speed and minimal resource usage, enabling faster execution.
- **Flexible Low Level Customization**: Complete freedom to customize at both high and low levels - from overall workflows and system architecture to granular agent behaviors, internal prompts, and execution logic.
- **Ideal for Every Use Case**: Proven effective for both simple tasks and highly complex, real-world, enterprise-grade scenarios.
- **Robust Community**: Backed by a rapidly growing community of over **100,000 certified** developers offering comprehensive support and resources.

CrewAI empowers developers and enterprises to confidently build intelligent automations, bridging the gap between simplicity, flexibility, and performance.

## Getting Started

### Learning Resources

Learn CrewAI through our comprehensive courses:
- [Multi AI Agent Systems with CrewAI](https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/) - Master the fundamentals of multi-agent systems
- [Practical Multi AI Agents and Advanced Use Cases](https://www.deeplearning.ai/short-courses/practical-multi-ai-agents-and-advanced-use-cases-with-crewai/) - Deep dive into advanced implementations

### Understanding Flows and Crews

CrewAI offers two powerful, complementary approaches that work seamlessly together to build sophisticated AI applications:

1. **Crews**: Teams of AI agents with true autonomy and agency, working together to accomplish complex tasks through role-based collaboration. Crews enable:
   - Natural, autonomous decision-making between agents
   - Dynamic task delegation and collaboration
   - Specialized roles with defined goals and expertise
   - Flexible problem-solving approaches

2. **Flows**: Production-ready, event-driven workflows that deliver precise control over complex automations. Flows provide:
   - Fine-grained control over execution paths for real-world scenarios
   - Secure, consistent state management between tasks
   - Clean integration of AI agents with production Python code
   - Conditional branching for complex business logic

The true power of CrewAI emerges when combining Crews and Flows. This synergy allows you to:
- Build complex, production-grade applications
- Balance autonomy with precise control
- Handle sophisticated real-world scenarios
- Maintain clean, maintainable code structure

### Getting Started with Installation

To get started with CrewAI, follow these simple steps:

### 1. Installation

Ensure you have Python &gt;=3.10 &lt;3.13 installed on your system. CrewAI uses [UV](https://docs.astral.sh/uv/) for dependency management and package handling, offering a seamless setup and execution experience.

First, install CrewAI:

```shell
pip install crewai
```
If you want to install the &#039;crewai&#039; package along with its optional features that include additional tools for agents, you can do so by using the following command:

```shell
pip install &#039;crewai[tools]&#039;
```
The command above installs the basic package and also adds extra components which require more dependencies to function.

### Troubleshooting Dependencies

If you encounter issues during installation or usage, here are some common solutions:

#### Common Issues

1. **ModuleNotFoundError: No module named &#039;tiktoken&#039;**
   - Install tiktoken explicitly: `pip install &#039;crewai[embeddings]&#039;`
   - If using embedchain or other tools: `pip install &#039;crewai[tools]&#039;`

2. **Failed building wheel for tiktoken**
   - Ensure Rust compiler is installed (see installation steps above)
   - For Windows: Verify Visual C++ Build Tools are installed
   - Try upgrading pip: `pip install --upgrade pip`
   - If issues persist, use a pre-built wheel: `pip install tiktoken --prefer-binary`

### 2. Setting Up Your Crew with the YAML Configuration

To create a new CrewAI project, run the following CLI (Command Line Interface) command:

```shell
crewai create crew &lt;project_name&gt;
```

This command creates a new project folder with the following structure:

```
my_project/
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ .env
‚îî‚îÄ‚îÄ src/
    ‚îî‚îÄ‚îÄ my_project/
        ‚îú‚îÄ‚îÄ __init__.py
        ‚îú‚îÄ‚îÄ main.py
        ‚îú‚îÄ‚îÄ crew.py
        ‚îú‚îÄ‚îÄ tools/
        ‚îÇ   ‚îú‚îÄ‚îÄ custom_tool.py
        ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
        ‚îî‚îÄ‚îÄ config/
            ‚îú‚îÄ‚îÄ agents.yaml
            ‚îî‚îÄ‚îÄ tasks.yaml
```

You can now start developing your crew by editing the files in the `src/my_project` folder. The `main.py` file is the entry point of the project, the `crew.py` file is where you define your crew, the `agents.yaml` file is where you define your agents, and the `tasks.yaml` file is where you define your tasks.

#### To customize your project, you can:

- Modify `src/my_project/config/agents.yaml` to define your agents.
- Modify `src/my_project/config/tasks.yaml` to define your tasks.
- Modify `src/my_project/crew.py` to add your own logic, tools, and specific arguments.
- Modify `src/my_project/main.py` to add custom inputs for your agents and tasks.
- Add your environment variables into the `.env` file.

#### Example of a simple crew with a sequential process:

Instantiate your crew:

```shell
crewai create crew latest-ai-development
```

Modify the files as needed to fit your use case:

**agents.yaml**

```yaml
# src/my_project/config/agents.yaml
researcher:
  role: &gt;
    {topic} Senior Data Researcher
  goal: &gt;
    Uncover cutting-edge developments in {topic}
  backstory: &gt;
    You&#039;re a seasoned researcher with a knack for uncovering the latest
    developments in {topic}. Known for your ability to find the most relevant
    information and present it in a clear and concise manner.

reporting_analyst:
  role: &gt;
    {topic} Reporting Analyst
  goal: &gt;
    Create detailed reports based on {topic} data analysis and research findings
  backstory: &gt;
    You&#039;re a meticulous analyst with a keen eye for detail. You&#039;re known for
    your ability to turn complex data into clear and concise reports, making
    it easy for others to understand and act on the information you provide.
```

**tasks.yaml**

```yaml
# src/my_project/config/tasks.yaml
research_task:
  description: &gt;
    Conduct a thorough research about {topic}
    Make sure you find any interesting and relevant information given
    the current year is 2025.
  expected_output: &gt;
    A list with 10 bullet points of the most relevant information about {topic}
  agent: researcher

reporting_task:
  description: &gt;
    Review the context you got and expand each topic into a full section for a report.
    Make sure the report is detailed and contains any and all relevant information.
  expected_output: &gt;
    A fully fledge reports with the mains topics, each with a full section of information.
    Formatted as markdown without &#039;```&#039;
  agent: reporting_analyst
  output_file: report.md
```

**crew.py**

```python
# src/my_project/crew.py
from crewai import Agent, Crew, Process, Task
from crewai.project import CrewBase, agent, crew, task
from crewai_tools import SerperDevTool

@CrewBase
class LatestAiDevelopmentCrew():
	&quot;&quot;&quot;LatestAiDevelopment crew&quot;&quot;&quot;

	@agent
	def researcher(self) -&gt; Agent:
		return Agent(
			config=self.agents_config[&#039;researcher&#039;],
			verbose=True,
			tools=[SerperDevTool()]
		)

	@agent
	def reporting_analyst(self) -&gt; Agent:
		return Agent(
			config=self.agents_config[&#039;reporting_analyst&#039;],
			verbose=True
		)

	@task
	def research_task(self) -&gt; Task:
		return Task(
			config=self.tasks_config[&#039;research_task&#039;],
		)

	@task
	def reporting_task(self) -&gt; Task:
		return Task(
			config=self.tasks_config[&#039;reporting_task&#039;],
			output_file=&#039;report.md&#039;
		)

	@crew
	def crew(self) -&gt; Crew:
		&quot;&quot;&quot;Creates the LatestAiDevelopment crew&quot;&quot;&quot;
		return Crew(
			agents=self.agents, # Automatically created by the @agent decorator
			tasks=self.tasks, # Automatically created by the @task decorator
			process=Process.sequential,
			verbose=True,
		)
```

**main.py**

```python
#!/usr/bin/env python
# src/my_project/main.py
import sys
from latest_ai_development.crew import LatestAiDevelopmentCrew

def run():
    &quot;&quot;&quot;
    Run the crew.
    &quot;&quot;&quot;
    inputs = {
        &#039;topic&#039;: &#039;AI Agents&#039;
    }
    LatestAiDevelopmentCrew().crew().kickoff(inputs=inputs)
```

### 3. Running Your Crew

Before running your crew, make sure you have the following keys set as environment variables in your `.env` file:

- An [OpenAI API key](https://platform.openai.com/account/api-keys) (or other LLM API key): `OPENAI_API_KEY=sk-...`
- A [Serper.dev](https://serper.dev/) API key: `SERPER_API_KEY=YOUR_KEY_HERE`

Lock the dependencies and install them by using the CLI command but first, navigate to your project directory:

```shell
cd my_project
crewai install (Optional)
```

To run your crew, execute the following command in the root of your project:

```bash
crewai run
```

or

```bash
python src/my_project/main.py
```

If an error happens due to the usage of poetry, please run the following command to update your crewai package:

```bash
crewai update
```

You should see the output in the console and the `report.md` file should be created in the root of your project with the full final report.

In addition to the sequential process, you can use the hierarchical process, which automatically assigns a manager to the defined crew to properly coordinate the planning and execution of tasks through delegation and validation of results. [See more about the processes here](https://docs.crewai.com/core-concepts/Processes/).

## Key Features

CrewAI stands apart as a lean, standalone, high-performance framework delivering simplicity, flexibility, and precise control‚Äîfree from the complexity and limitations found in other agent frameworks.

- **Standalone &amp; Lean**: Completely independent from other frameworks like LangChain, offering faster execution and lighter resource demands.
- **Flexible &amp; Precise**: Easily orchestrate autonomous agents through intuitive [Crews](https://docs.crewai.com/concepts/crews) or precise [Flows](https://docs.crewai.com/concepts/flows), achieving perfect balance for your needs.
- **Seamless Integration**: Effortlessly combine Crews (autonomy) and Flows (precision) to create complex, real-world automations.
- **Deep Customization**: Tailor every aspect‚Äîfrom high-level workflows down to low-level internal prompts and agent behaviors.
- **Reliable Performance**: Consistent results across simple tasks and complex, enterprise-level automations.
- **Thriving Community**: Backed by robust documentation and over 100,000 certified developers, providing exceptional support and guidance.

Choose CrewAI to easily build powerful, adaptable, and production-ready AI automations.

## Examples

You can test different real life examples of AI crews in the [CrewAI-examples repo](https://github.com/crewAIInc/crewAI-examples?tab=readme-ov-file):

- [Landing Page Generator](https://github.com/crewAIInc/crewAI-examples/tree/main/landing_page_generator)
- [Having Human input on the execution](https://docs.crewai.com/how-to/Human-Input-on-Execution)
- [Trip Planner](https://github.com/crewAIInc/crewAI-examples/tree/main/trip_planner)
- [Stock Analysis](https://github.com/crewAIInc/crewAI-examples/tree/main/stock_analysis)

### Quick Tutorial

[![CrewAI Tutorial](https://img.youtube.com/vi/tnejrr-0a94/maxresdefault.jpg)](https://www.youtube.com/watch?v=tnejrr-0a94 &quot;CrewAI Tutorial&quot;)

### Write Job Descriptions

[Check out code for this example](https://github.com/crewAIInc/crewAI-examples/tree/main/job-posting) or watch a video below:

[![Jobs postings](https://img.youtube.com/vi/u98wEMz-9to/maxresdefault.jpg)](https://www.youtube.com/watch?v=u98wEMz-9to &quot;Jobs postings&quot;)

### Trip Planner

[Check out code for this example](https://github.com/crewAIInc/crewAI-examples/tree/main/trip_planner) or watch a video below:

[![Trip Planner](https://img.youtube.com/vi/xis7rWp-hjs/maxresdefault.jpg)](https://www.youtube.com/watch?v=xis7rWp-hjs &quot;Trip Planner&quot;)

### Stock Analysis

[Check out code for this example](https://github.com/crewAIInc/crewAI-examples/tree/main/stock_analysis) or watch a video below:

[![Stock Analysis](https://img.youtube.com/vi/e0Uj4yWdaAg/maxresdefault.jpg)](https://www.youtube.com/watch?v=e0Uj4yWdaAg &quot;Stock Analysis&quot;)

### Using Crews and Flows Together

CrewAI&#039;s power truly shines when combining Crews with Flows to create sophisticated automation pipelines.
CrewAI flows support logical operators like `or_` and `and_` to combine multiple conditions. This can be used with `@start`, `@listen`, or `@router` decorators to create complex triggering conditions.
- `or_`: Triggers when any of the specified conditions are met. 
- `and_`Triggers when all of the specified conditions are met.

Here&#039;s how you can orchestrate multiple Crews within a Flow:

```python
from crewai.flow.flow import Flow, listen, start, router, or_
from crewai import Crew, Agent, Task, Process
from pydantic import BaseModel

# Define structured state for precise control
class MarketState(BaseModel):
    sentiment: str = &quot;neutral&quot;
    confidence: float = 0.0
    recommendations: list = []

class AdvancedAnalysisFlow(Flow[MarketState]):
    @start()
    def fetch_market_data(self):
        # Demonstrate low-level control with structured state
        self.state.sentiment = &quot;analyzing&quot;
        return {&quot;sector&quot;: &quot;tech&quot;, &quot;timeframe&quot;: &quot;1W&quot;}  # These parameters match the task description template

    @listen(fetch_market_data)
    def analyze_with_crew(self, market_data):
        # Show crew agency through specialized roles
        analyst = Agent(
            role=&quot;Senior Market Analyst&quot;,
            goal=&quot;Conduct deep market analysis with expert insight&quot;,
            backstory=&quot;You&#039;re a veteran analyst known for identifying subtle market patterns&quot;
        )
        researcher = Agent(
            role=&quot;Data Researcher&quot;,
            goal=&quot;Gather and validate supporting market data&quot;,
            backstory=&quot;You excel at finding and correlating multiple data sources&quot;
        )

        analysis_task = Task(
            description=&quot;Analyze {sector} sector data for the past {timeframe}&quot;,
            expected_output=&quot;Detailed market analysis with confidence score&quot;,
            agent=analyst
        )
        research_task = Task(
            description=&quot;Find supporting data to validate the analysis&quot;,
            expected_output=&quot;Corroborating evidence and potential contradictions&quot;,
            agent=researcher
        )

        # Demonstrate crew autonomy
        analysis_crew = Crew(
            agents=[analyst, researcher],
            tasks=[analysis_task, research_task],
            process=Process.sequential,
            verbose=True
        )
        return analysis_crew.kickoff(inputs=market_data)  # Pass market_data as named inputs

    @router(analyze_with_crew)
    def determine_next_steps(self):
        # Show flow control with conditional routing
        if self.state.confidence &gt; 0.8:
            return &quot;high_confidence&quot;
        elif self.state.confidence &gt; 0.5:
            return &quot;medium_confidence&quot;
        return &quot;low_confidence&quot;

    @listen(&quot;high_confidence&quot;)
    def execute_strategy(self):
        # Demonstrate complex decision making
        strategy_crew = Crew(
            agents=[
                Agent(role=&quot;Strategy Expert&quot;,
                      goal=&quot;Develop optimal market strategy&quot;)
            ],
            tasks=[
                Task(description=&quot;Create detailed strategy based on analysis&quot;,
                     expected_output=&quot;Step-by-step action plan&quot;)
            ]
        )
        return strategy_crew.kickoff()

    @listen(or_(&quot;medium_confidence&quot;, &quot;low_confidence&quot;))
    def request_additional_analysis(self):
        self.state.recommendations.append(&quot;Gather more data&quot;)
        return &quot;Additional analysis required&quot;
```

This example demonstrates how to:
1. Use Python code for basic data operations
2. Create and execute Crews as steps in your workflow
3. Use Flow decorators to manage the sequence of operations
4. Implement conditional branching based on Crew results

## Connecting Your Crew to a Model

CrewAI supports using various LLMs through a variety of connection options. By default your agents will use the OpenAI API when querying the model. However, there are several 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[modelscope/FunASR]]></title>
            <link>https://github.com/modelscope/FunASR</link>
            <guid>https://github.com/modelscope/FunASR</guid>
            <pubDate>Fri, 11 Apr 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[A Fundamental End-to-End Speech Recognition Toolkit and Open Source SOTA Pretrained Models, Supporting Speech Recognition, Voice Activity Detection, Text Post-processing etc.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/modelscope/FunASR">modelscope/FunASR</a></h1>
            <p>A Fundamental End-to-End Speech Recognition Toolkit and Open Source SOTA Pretrained Models, Supporting Speech Recognition, Voice Activity Detection, Text Post-processing etc.</p>
            <p>Language: Python</p>
            <p>Stars: 9,670</p>
            <p>Forks: 973</p>
            <p>Stars today: 49 stars today</p>
            <h2>README</h2><pre>[//]: # (&lt;div align=&quot;left&quot;&gt;&lt;img src=&quot;docs/images/funasr_logo.jpg&quot; width=&quot;400&quot;/&gt;&lt;/div&gt;)

([ÁÆÄ‰Ωì‰∏≠Êñá](./README_zh.md)|English)

[//]: # (# FunASR: A Fundamental End-to-End Speech Recognition Toolkit)

[![SVG Banners](https://svg-banners.vercel.app/api?type=origin&amp;text1=FunASRü§†&amp;text2=üíñ%20A%20Fundamental%20End-to-End%20Speech%20Recognition%20Toolkit&amp;width=800&amp;height=210)](https://github.com/Akshay090/svg-banners)

[![PyPI](https://img.shields.io/pypi/v/funasr)](https://pypi.org/project/funasr/)

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/3839&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/3839&quot; alt=&quot;alibaba-damo-academy%2FFunASR | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;strong&gt;FunASR&lt;/strong&gt; hopes to build a bridge between academic research and industrial applications on speech recognition. By supporting the training &amp; finetuning of the industrial-grade speech recognition model, researchers and developers can conduct research and production of speech recognition models more conveniently, and promote the development of speech recognition ecology. ASR for FunÔºÅ

[**Highlights**](#highlights)
| [**News**](https://github.com/alibaba-damo-academy/FunASR#whats-new) 
| [**Installation**](#installation)
| [**Quick Start**](#quick-start)
| [**Tutorial**](https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/tutorial/README.md)
| [**Runtime**](./runtime/readme.md)
| [**Model Zoo**](#model-zoo)
| [**Contact**](#contact)




&lt;a name=&quot;highlights&quot;&gt;&lt;/a&gt;
## Highlights
- FunASR is a fundamental speech recognition toolkit that offers a variety of features, including speech recognition (ASR), Voice Activity Detection (VAD), Punctuation Restoration, Language Models, Speaker Verification, Speaker Diarization and multi-talker ASR. FunASR provides convenient scripts and tutorials, supporting inference and fine-tuning of pre-trained models.
- We have released a vast collection of academic and industrial pretrained models on the [ModelScope](https://www.modelscope.cn/models?page=1&amp;tasks=auto-speech-recognition) and [huggingface](https://huggingface.co/FunASR), which can be accessed through our [Model Zoo](https://github.com/alibaba-damo-academy/FunASR/blob/main/docs/model_zoo/modelscope_models.md). The representative [Paraformer-large](https://www.modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary), a non-autoregressive end-to-end speech recognition model, has the advantages of high accuracy, high efficiency, and convenient deployment, supporting the rapid construction of speech recognition services. For more details on service deployment, please refer to the [service deployment document](runtime/readme_cn.md). 


&lt;a name=&quot;whats-new&quot;&gt;&lt;/a&gt;
## What&#039;s new:
- 2024/10/29: Real-time Transcription Service 1.12 released, The 2pass-offline mode supports the SensevoiceSmal modelÔºõ([docs](runtime/readme.md));
- 2024/10/10ÔºöAdded support for the Whisper-large-v3-turbo model, a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. It can be downloaded from the [modelscope](examples/industrial_data_pretraining/whisper/demo.py), and [openai](examples/industrial_data_pretraining/whisper/demo_from_openai.py).
- 2024/09/26: Offline File Transcription Service 4.6, Offline File Transcription Service of English 1.7, Real-time Transcription Service 1.11 released, fix memory leak &amp; Support the SensevoiceSmall onnx modelÔºõFile Transcription Service 2.0 GPU released, Fix GPU memory leak; ([docs](runtime/readme.md));
- 2024/09/25Ôºökeyword spotting models are new supported. Supports fine-tuning and inference for four models: [fsmn_kws](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online), [fsmn_kws_mt](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online), [sanm_kws](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-offline), [sanm_kws_streaming](https://modelscope.cn/models/iic/speech_sanm_kws_phone-xiaoyun-commands-online).
- 2024/07/04Ôºö[SenseVoice](https://github.com/FunAudioLLM/SenseVoice) is a speech foundation model with multiple speech understanding capabilities, including ASR, LID, SER, and AED.
- 2024/07/01: Offline File Transcription Service GPU 1.1 released, optimize BladeDISC model compatibility issues; ref to ([docs](runtime/readme.md))
- 2024/06/27: Offline File Transcription Service GPU 1.0 released, supporting dynamic batch processing and multi-threading concurrency. In the long audio test set, the single-thread RTF is 0.0076, and multi-threads&#039; speedup is 1200+ (compared to 330+ on CPU); ref to ([docs](runtime/readme.md))
- 2024/05/15Ôºöemotion recognition models are new supported. [emotion2vec+large](https://modelscope.cn/models/iic/emotion2vec_plus_large/summary)Ôºå[emotion2vec+base](https://modelscope.cn/models/iic/emotion2vec_plus_base/summary)Ôºå[emotion2vec+seed](https://modelscope.cn/models/iic/emotion2vec_plus_seed/summary). currently supports the following categories: 0: angry 1: happy 2: neutral 3: sad 4: unknown.
- 2024/05/15: Offline File Transcription Service 4.5, Offline File Transcription Service of English 1.6, Real-time Transcription Service 1.10 released, adapting to FunASR 1.0 model structureÔºõ([docs](runtime/readme.md))

&lt;details&gt;&lt;summary&gt;Full Changelog&lt;/summary&gt;

- 2024/03/05ÔºöAdded the Qwen-Audio and Qwen-Audio-Chat large-scale audio-text multimodal models, which have topped multiple audio domain leaderboards. These models support speech dialogue, [usage](examples/industrial_data_pretraining/qwen_audio).
- 2024/03/05ÔºöAdded support for the Whisper-large-v3 model, a multitasking model that can perform multilingual speech recognition, speech translation, and language identification. It can be downloaded from the[modelscope](examples/industrial_data_pretraining/whisper/demo.py), and [openai](examples/industrial_data_pretraining/whisper/demo_from_openai.py).
- 2024/03/05: Offline File Transcription Service 4.4, Offline File Transcription Service of English 1.5ÔºåReal-time Transcription Service 1.9 releasedÔºådocker image supports ARM64 platform, update modelscopeÔºõ([docs](runtime/readme.md))
- 2024/01/30Ôºöfunasr-1.0 has been released ([docs](https://github.com/alibaba-damo-academy/FunASR/discussions/1319))
- 2024/01/30Ôºöemotion recognition models are new supported. [model link](https://www.modelscope.cn/models/iic/emotion2vec_base_finetuned/summary), modified from [repo](https://github.com/ddlBoJack/emotion2vec).
- 2024/01/25: Offline File Transcription Service 4.2, Offline File Transcription Service of English 1.3 releasedÔºåoptimized the VAD (Voice Activity Detection) data processing method, significantly reducing peak memory usage, memory leak optimization; Real-time Transcription Service 1.7 releasedÔºåoptimizatized the client-sideÔºõ([docs](runtime/readme.md))
- 2024/01/09: The Funasr SDK for Windows version 2.0 has been released, featuring support for The offline file transcription service (CPU) of Mandarin 4.1, The offline file transcription service (CPU) of English 1.2, The real-time transcription service (CPU) of Mandarin 1.6. For more details, please refer to the official documentation or release notes([FunASR-Runtime-Windows](https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary))
- 2024/01/03: File Transcription Service 4.0 released, Added support for 8k models, optimized timestamp mismatch issues and added sentence-level timestamps, improved the effectiveness of English word FST hotwords, supported automated configuration of thread parameters, and fixed known crash issues as well as memory leak problems, refer to ([docs](runtime/readme.md#file-transcription-service-mandarin-cpu)).
- 2024/01/03: Real-time Transcription Service 1.6 releasedÔºåThe 2pass-offline mode supports Ngram language model decoding and WFST hotwords, while also addressing known crash issues and memory leak problems, ([docs](runtime/readme.md#the-real-time-transcription-service-mandarin-cpu))
- 2024/01/03: Fixed known crash issues as well as memory leak problems, ([docs](runtime/readme.md#file-transcription-service-english-cpu)).
- 2023/12/04: The Funasr SDK for Windows version 1.0 has been released, featuring support for The offline file transcription service (CPU) of Mandarin, The offline file transcription service (CPU) of English, The real-time transcription service (CPU) of Mandarin. For more details, please refer to the official documentation or release notes([FunASR-Runtime-Windows](https://www.modelscope.cn/models/damo/funasr-runtime-win-cpu-x64/summary))
- 2023/11/08: The offline file transcription service 3.0 (CPU) of Mandarin has been released, adding punctuation large model, Ngram language model, and wfst hot words. For detailed information, please refer to [docs](runtime#file-transcription-service-mandarin-cpu). 
- 2023/10/17: The offline file transcription service (CPU) of English has been released. For more details, please refer to ([docs](runtime#file-transcription-service-english-cpu)).
- 2023/10/13: [SlideSpeech](https://slidespeech.github.io/): A large scale multi-modal audio-visual corpus with a significant amount of real-time synchronized slides.
- 2023/10/10: The ASR-SpeakersDiarization combined pipeline [Paraformer-VAD-SPK](https://github.com/alibaba-damo-academy/FunASR/blob/main/egs_modelscope/asr_vad_spk/speech_paraformer-large-vad-punc-spk_asr_nat-zh-cn/demo.py) is now released. Experience the model to get recognition results with speaker information.
- 2023/10/07: [FunCodec](https://github.com/alibaba-damo-academy/FunCodec): A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec.
- 2023/09/01: The offline file transcription service 2.0 (CPU) of Mandarin has been released, with added support for ffmpeg, timestamp, and hotword models. For more details, please refer to ([docs](runtime#file-transcription-service-mandarin-cpu)).
- 2023/08/07: The real-time transcription service (CPU) of Mandarin has been released. For more details, please refer to ([docs](runtime#the-real-time-transcription-service-mandarin-cpu)).
- 2023/07/17: BAT is released, which is a low-latency and low-memory-consumption RNN-T model. For more details, please refer to ([BAT](egs/aishell/bat)).
- 2023/06/26: ASRU2023 Multi-Channel Multi-Party Meeting Transcription Challenge 2.0 completed the competition and announced the results. For more details, please refer to ([M2MeT2.0](https://alibaba-damo-academy.github.io/FunASR/m2met2/index.html)).

&lt;/details&gt;

&lt;a name=&quot;Installation&quot;&gt;&lt;/a&gt;
## Installation

- Requirements
```text
python&gt;=3.8
torch&gt;=1.13
torchaudio
```

- Install for pypi
```shell
pip3 install -U funasr
```
- Or install from source code
``` sh
git clone https://github.com/alibaba/FunASR.git &amp;&amp; cd FunASR
pip3 install -e ./
```
- Install modelscope or huggingface_hub for the pretrained models (Optional)

```shell
pip3 install -U modelscope huggingface_hub
```

## Model Zoo
FunASR has open-sourced a large number of pre-trained models on industrial data. You are free to use, copy, modify, and share FunASR models under the [Model License Agreement](./MODEL_LICENSE). Below are some representative models, for more models please refer to the [Model Zoo](./model_zoo).

(Note: ‚≠ê represents the ModelScope model zoo, ü§ó represents the Huggingface model zoo, üçÄ represents the OpenAI model zoo)


|                                                                                                         Model Name                                                                                                         |                                   Task Details                                   |          Training Data           | Parameters |
|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------------:|:--------------------------------:|:----------:|
|                                        SenseVoiceSmall &lt;br&gt; ([‚≠ê](https://www.modelscope.cn/models/iic/SenseVoiceSmall)  [ü§ó](https://huggingface.co/FunAudioLLM/SenseVoiceSmall) )                                         | multiple speech understanding capabilities, including ASR, ITN, LID, SER, and AED, support languages such as zh, yue, en, ja, ko   |           300000 hours           |    234M    |
|          paraformer-zh &lt;br&gt; ([‚≠ê](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary)  [ü§ó](https://huggingface.co/funasr/paraformer-zh) )           |                speech recognition, with timestamps, non-streaming                |      60000 hours, Mandarin       |    220M    |
| &lt;nobr&gt;paraformer-zh-streaming &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online/summary) [ü§ó](https://huggingface.co/funasr/paraformer-zh-streaming) )&lt;/nobr&gt; |                          speech recognition, streaming                           |      60000 hours, Mandarin       |    220M    |
|               paraformer-en &lt;br&gt; ( [‚≠ê](https://www.modelscope.cn/models/damo/speech_paraformer-large-vad-punc_asr_nat-en-16k-common-vocab10020/summary) [ü§ó](https://huggingface.co/funasr/paraformer-en) )                |              speech recognition, without timestamps, non-streaming               |       50000 hours, English       |    220M    |
|                            conformer-en &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/damo/speech_conformer_asr-en-16k-vocab4199-pytorch/summary) [ü§ó](https://huggingface.co/funasr/conformer-en) )                             |                        speech recognition, non-streaming                         |       50000 hours, English       |    220M    |
|                               ct-punc &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/damo/punc_ct-transformer_cn-en-common-vocab471067-large/summary) [ü§ó](https://huggingface.co/funasr/ct-punc) )                               |                             punctuation restoration                              |    100M, Mandarin and English    |    290M    | 
|                                   fsmn-vad &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/summary) [ü§ó](https://huggingface.co/funasr/fsmn-vad) )                                   |                             voice activity detection                             | 5000 hours, Mandarin and English |    0.4M    | 
|                                                              fsmn-kws &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/iic/speech_charctc_kws_phone-xiaoyun/summary) )                                                              |     keyword spottingÔºåstreaming      |  5000 hours, Mandarin  |    0.7M    | 
|                                     fa-zh &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/damo/speech_timestamp_prediction-v1-16k-offline/summary) [ü§ó](https://huggingface.co/funasr/fa-zh) )                                     |                               timestamp prediction                               |       5000 hours, Mandarin       |    38M     | 
|                                       cam++ &lt;br&gt; ( [‚≠ê](https://modelscope.cn/models/iic/speech_campplus_sv_zh-cn_16k-common/summary) [ü§ó](https://huggingface.co/funasr/campplus) )                                        |                         speaker verification/diarization                         |            5000 hours            |    7.2M    | 
|                                            Whisper-large-v3 &lt;br&gt; ([‚≠ê](https://www.modelscope.cn/models/iic/Whisper-large-v3/summary)  [üçÄ](https://github.com/openai/whisper) )                                            |                speech recognition, with timestamps, non-streaming                |           multilingual           |   1550 M   |
|                                      Whisper-large-v3-turbo &lt;br&gt; ([‚≠ê](https://www.modelscope.cn/models/iic/Whisper-large-v3-turbo/summary)  [üçÄ](https://github.com/openai/whisper) )                                      |                speech recognition, with timestamps, non-streaming                |           multilingual           |   809 M    |
|                                               Qwen-Audio &lt;br&gt; ([‚≠ê](examples/industrial_data_pretraining/qwen_audio/demo.py)  [ü§ó](https://huggingface.co/Qwen/Qwen-Audio) )                                                |                    audio-text multimodal models (pretraining)                    |           multilingual           |     8B     |
|                                        Qwen-Audio-Chat &lt;br&gt; ([‚≠ê](examples/industrial_data_pretraining/qwen_audio/demo_chat.py)  [ü§ó](https://huggingface.co/Qwen/Qwen-Audio-Chat) )                                        |                       audio-text multimodal models (chat)                        |           multilingual           |     8B     |
|                              emotion2vec+large &lt;br&gt; ([‚≠ê](https://modelscope.cn/models/iic/emotion2vec_plus_large/summary)  [ü§ó](https://huggingface.co/emotion2vec/emotion2vec_plus_large) )                               |                           speech emotion recongintion                            |           40000 hours            |    300M    |




[//]: # ()
[//]: # (FunASR supports pre-trained or further fine-tuned models for deployment as a service. The CPU version of the Chinese offline file conversion service has been released, details can be found in [docs]&amp;#40;funasr/runtime/docs/SDK_tutorial.md&amp;#41;. More detailed information about service deployment can be found in the [deployment roadmap]&amp;#40;funasr/runtime/readme_cn.md&amp;#41;.)


&lt;a name=&quot;quick-start&quot;&gt;&lt;/a&gt;
## Quick Start

Below is a quick start tutorial. Test audio files ([Mandarin](https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/vad_example.wav), [English](https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/test_audio/asr_example_en.wav)).

### Command-line usage

```shell
funasr ++model=paraformer-zh ++vad_model=&quot;fsmn-vad&quot; ++punc_model=&quot;ct-punc&quot; ++input=asr_example_zh.wav
```

Notes: Support recognition of single audio file, as well as file list in Kaldi-style wav.scp format: `wav_id wav_pat`

### Speech Recognition (Non-streaming)
#### SenseVoice
```python
from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess

model_dir = &quot;iic/SenseVoiceSmall&quot;

model = AutoModel(
    model=model_dir,
    vad_model=&quot;fsmn-vad&quot;,
    vad_kwargs={&quot;max_single_segment_time&quot;: 30000},
    device=&quot;cuda:0&quot;,
)

# en
res = model.generate(
    input=f&quot;{model.model_path}/example/en.mp3&quot;,
    cache={},
    language=&quot;auto&quot;,  # &quot;zn&quot;, &quot;en&quot;, &quot;yue&quot;, &quot;ja&quot;, &quot;ko&quot;, &quot;nospeech&quot;
    use_itn=True,
    batch_size_s=60,
    merge_vad=True,  #
    merge_length_s=15,
)
text = rich_transcription_postprocess(res[0][&quot;text&quot;])
print(text)
```
Parameter Description:
- `model_dir`: The name of the model, or the path to the model on the local disk.
- `vad_model`: This indicates the activation of VAD (Voice Activity Detection). The purpose of VAD is to split long audio into shorter clips. In this case, the inference time includes both VAD and SenseVoice total consumption, and represents the end-to-end latency. If you wish to test the SenseVoice model&#039;s inference time separately, the VAD model can be disabled.
- `vad_kwargs`: Specifies the configurations for the VAD model. `max_single_segment_time`: denotes the maximum duration for audio segmentation by the `vad_model`, with the unit being milliseconds (ms).
- `use_itn`: Whether the output result includes punctuation and inverse text normalization.
- `batch_size_s`: Indicates the use of dynamic batching, where the total duration of audio in the batch is measured in seconds (s).
- `mer

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[modelscope/DiffSynth-Studio]]></title>
            <link>https://github.com/modelscope/DiffSynth-Studio</link>
            <guid>https://github.com/modelscope/DiffSynth-Studio</guid>
            <pubDate>Fri, 11 Apr 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[Enjoy the magic of Diffusion models!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/modelscope/DiffSynth-Studio">modelscope/DiffSynth-Studio</a></h1>
            <p>Enjoy the magic of Diffusion models!</p>
            <p>Language: Python</p>
            <p>Stars: 8,280</p>
            <p>Forks: 741</p>
            <p>Stars today: 12 stars today</p>
            <h2>README</h2><pre># DiffSynth Studio
[![PyPI](https://img.shields.io/pypi/v/DiffSynth)](https://pypi.org/project/DiffSynth/)
[![license](https://img.shields.io/github/license/modelscope/DiffSynth-Studio.svg)](https://github.com/modelscope/DiffSynth-Studio/blob/master/LICENSE)
[![open issues](https://isitmaintained.com/badge/open/modelscope/DiffSynth-Studio.svg)](https://github.com/modelscope/DiffSynth-Studio/issues)
[![GitHub pull-requests](https://img.shields.io/github/issues-pr/modelscope/DiffSynth-Studio.svg)](https://GitHub.com/modelscope/DiffSynth-Studio/pull/)
[![GitHub latest commit](https://badgen.net/github/last-commit/modelscope/DiffSynth-Studio)](https://GitHub.com/modelscope/DiffSynth-Studio/commit/)

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/10946&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/10946&quot; alt=&quot;modelscope%2FDiffSynth-Studio | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

Document: https://diffsynth-studio.readthedocs.io/zh-cn/latest/index.html

## Introduction

Welcome to the magic world of Diffusion models!

DiffSynth consists of two open-source projects:
* [DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio): Focused on aggressive technological exploration. Targeted at academia. Provides more cutting-edge technical support and novel inference capabilities.
* [DiffSynth-Engine](https://github.com/modelscope/DiffSynth-Engine): Focused on stable model deployment. Geared towards industry. Offers better engineering support, higher computational performance, and more stable functionality.

DiffSynth-Studio is an open-source project aimed at exploring innovations in AIGC technology. We have integrated numerous open-source Diffusion models, including FLUX and Wan, among others. Through this open-source project, we hope to connect models within the open-source community and explore new technologies based on diffusion models.

Until now, DiffSynth-Studio has supported the following models:

* [Wan-Video](https://github.com/Wan-Video/Wan2.1)
* [StepVideo](https://github.com/stepfun-ai/Step-Video-T2V)
* [HunyuanVideo](https://github.com/Tencent/HunyuanVideo), [HunyuanVideo-I2V]()
* [CogVideoX](https://huggingface.co/THUDM/CogVideoX-5b)
* [FLUX](https://huggingface.co/black-forest-labs/FLUX.1-dev)
* [ExVideo](https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1)
* [Kolors](https://huggingface.co/Kwai-Kolors/Kolors)
* [Stable Diffusion 3](https://huggingface.co/stabilityai/stable-diffusion-3-medium)
* [Stable Video Diffusion](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt)
* [Hunyuan-DiT](https://github.com/Tencent/HunyuanDiT)
* [RIFE](https://github.com/hzwer/ECCV2022-RIFE)
* [ESRGAN](https://github.com/xinntao/ESRGAN)
* [Ip-Adapter](https://github.com/tencent-ailab/IP-Adapter)
* [AnimateDiff](https://github.com/guoyww/animatediff/)
* [ControlNet](https://github.com/lllyasviel/ControlNet)
* [Stable Diffusion XL](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)
* [Stable Diffusion](https://huggingface.co/runwayml/stable-diffusion-v1-5)

## News
- **March 31, 2025** We support InfiniteYou, an identity preserving method for FLUX. Please refer to [./examples/InfiniteYou/](./examples/InfiniteYou/) for more details.

- **March 25, 2025** üî•üî•üî• Our new open-source project, [DiffSynth-Engine](https://github.com/modelscope/DiffSynth-Engine), is now open-sourced! Focused on stable model deployment. Geared towards industry. Offers better engineering support, higher computational performance, and more stable functionality.

- **March 13, 2025** We support HunyuanVideo-I2V, the image-to-video generation version of HunyuanVideo open-sourced by Tencent. Please refer to [./examples/HunyuanVideo/](./examples/HunyuanVideo/) for more details.

- **February 25, 2025** We support Wan-Video, a collection of SOTA video synthesis models open-sourced by Alibaba. See [./examples/wanvideo/](./examples/wanvideo/).

- **February 17, 2025** We support [StepVideo](https://modelscope.cn/models/stepfun-ai/stepvideo-t2v/summary)! State-of-the-art video synthesis model! See [./examples/stepvideo](./examples/stepvideo/).

- **December 31, 2024** We propose EliGen, a novel framework for precise entity-level controlled text-to-image generation, complemented by an inpainting fusion pipeline to extend its capabilities to image inpainting tasks. EliGen seamlessly integrates with existing community models, such as IP-Adapter and In-Context LoRA, enhancing its versatility. For more details, see [./examples/EntityControl](./examples/EntityControl/).
  - Paper: [EliGen: Entity-Level Controlled Image Generation with Regional Attention](https://arxiv.org/abs/2501.01097)
  - Model: [ModelScope](https://www.modelscope.cn/models/DiffSynth-Studio/Eligen), [HuggingFace](https://huggingface.co/modelscope/EliGen)
  - Online Demo: [ModelScope EliGen Studio](https://www.modelscope.cn/studios/DiffSynth-Studio/EliGen)
  - Training Dataset: [EliGen Train Set](https://www.modelscope.cn/datasets/DiffSynth-Studio/EliGenTrainSet)

- **December 19, 2024** We implement advanced VRAM management for HunyuanVideo, making it possible to generate videos at a resolution of 129x720x1280 using 24GB of VRAM, or at 129x512x384 resolution with just 6GB of VRAM. Please refer to [./examples/HunyuanVideo/](./examples/HunyuanVideo/) for more details.

- **December 18, 2024** We propose ArtAug, an approach designed to improve text-to-image synthesis models through synthesis-understanding interactions. We have trained an ArtAug enhancement module for FLUX.1-dev in the format of LoRA. This model integrates the aesthetic understanding of Qwen2-VL-72B into FLUX.1-dev, leading to an improvement in the quality of generated images.
  - Paper: https://arxiv.org/abs/2412.12888
  - Examples: https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/ArtAug
  - Model: [ModelScope](https://www.modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1), [HuggingFace](https://huggingface.co/ECNU-CILab/ArtAug-lora-FLUX.1dev-v1)
  - Demo: [ModelScope](https://modelscope.cn/aigc/imageGeneration?tab=advanced&amp;versionId=7228&amp;modelType=LoRA&amp;sdVersion=FLUX_1&amp;modelUrl=modelscope%3A%2F%2FDiffSynth-Studio%2FArtAug-lora-FLUX.1dev-v1%3Frevision%3Dv1.0), HuggingFace (Coming soon)

- **October 25, 2024** We provide extensive FLUX ControlNet support. This project supports many different ControlNet models that can be freely combined, even if their structures differ. Additionally, ControlNet models are compatible with high-resolution refinement and partition control techniques, enabling very powerful controllable image generation. See [`./examples/ControlNet/`](./examples/ControlNet/).

- **October 8, 2024.** We release the extended LoRA based on CogVideoX-5B and ExVideo. You can download this model from [ModelScope](https://modelscope.cn/models/ECNU-CILab/ExVideo-CogVideoX-LoRA-129f-v1) or [HuggingFace](https://huggingface.co/ECNU-CILab/ExVideo-CogVideoX-LoRA-129f-v1).

- **August 22, 2024.** CogVideoX-5B is supported in this project. See [here](/examples/video_synthesis/). We provide several interesting features for this text-to-video model, including
  - Text to video
  - Video editing
  - Self-upscaling
  - Video interpolation

- **August 22, 2024.** We have implemented an interesting painter that supports all text-to-image models. Now you can create stunning images using the painter, with assistance from AI!
  - Use it in our [WebUI](#usage-in-webui).

- **August 21, 2024.** FLUX is supported in DiffSynth-Studio.
  - Enable CFG and highres-fix to improve visual quality. See [here](/examples/image_synthesis/README.md)
  - LoRA, ControlNet, and additional models will be available soon.

- **June 21, 2024.** We propose ExVideo, a post-tuning technique aimed at enhancing the capability of video generation models. We have extended Stable Video Diffusion to achieve the generation of long videos up to 128 frames.
  - [Project Page](https://ecnu-cilab.github.io/ExVideoProjectPage/)
  - Source code is released in this repo. See [`examples/ExVideo`](./examples/ExVideo/).
  - Models are released on [HuggingFace](https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1) and [ModelScope](https://modelscope.cn/models/ECNU-CILab/ExVideo-SVD-128f-v1).
  - Technical report is released on [arXiv](https://arxiv.org/abs/2406.14130).
  - You can try ExVideo in this [Demo](https://huggingface.co/spaces/modelscope/ExVideo-SVD-128f-v1)!

- **June 13, 2024.** DiffSynth Studio is transferred to ModelScope. The developers have transitioned from &quot;I&quot; to &quot;we&quot;. Of course, I will still participate in development and maintenance.

- **Jan 29, 2024.** We propose Diffutoon, a fantastic solution for toon shading.
  - [Project Page](https://ecnu-cilab.github.io/DiffutoonProjectPage/)
  - The source codes are released in this project.
  - The technical report (IJCAI 2024) is released on [arXiv](https://arxiv.org/abs/2401.16224).

- **Dec 8, 2023.** We decide to develop a new Project, aiming to release the potential of diffusion models, especially in video synthesis. The development of this project is started.

- **Nov 15, 2023.** We propose FastBlend, a powerful video deflickering algorithm.
  - The sd-webui extension is released on [GitHub](https://github.com/Artiprocher/sd-webui-fastblend).
  - Demo videos are shown on Bilibili, including three tasks.
    - [Video deflickering](https://www.bilibili.com/video/BV1d94y1W7PE)
    - [Video interpolation](https://www.bilibili.com/video/BV1Lw411m71p)
    - [Image-driven video rendering](https://www.bilibili.com/video/BV1RB4y1Z7LF)
  - The technical report is released on [arXiv](https://arxiv.org/abs/2311.09265).
  - An unofficial ComfyUI extension developed by other users is released on [GitHub](https://github.com/AInseven/ComfyUI-fastblend).

- **Oct 1, 2023.** We release an early version of this project, namely FastSDXL. A try for building a diffusion engine.
  - The source codes are released on [GitHub](https://github.com/Artiprocher/FastSDXL).
  - FastSDXL includes a trainable OLSS scheduler for efficiency improvement.
    - The original repo of OLSS is [here](https://github.com/alibaba/EasyNLP/tree/master/diffusion/olss_scheduler).
    - The technical report (CIKM 2023) is released on [arXiv](https://arxiv.org/abs/2305.14677).
    - A demo video is shown on [Bilibili](https://www.bilibili.com/video/BV1w8411y7uj).
    - Since OLSS requires additional training, we don&#039;t implement it in this project.

- **Aug 29, 2023.** We propose DiffSynth, a video synthesis framework.
  - [Project Page](https://ecnu-cilab.github.io/DiffSynth.github.io/).
  - The source codes are released in [EasyNLP](https://github.com/alibaba/EasyNLP/tree/master/diffusion/DiffSynth).
  - The technical report (ECML PKDD 2024) is released on [arXiv](https://arxiv.org/abs/2308.03463).


## Installation

Install from source code (recommended):

```
git clone https://github.com/modelscope/DiffSynth-Studio.git
cd DiffSynth-Studio
pip install -e .
```

Or install from pypi (There is a delay in the update. If you want to experience the latest features, please do not use this installation method.):

```
pip install diffsynth
```

If you encounter issues during installation, it may be caused by the packages we depend on. Please refer to the documentation of the package that caused the problem.

* [torch](https://pytorch.org/get-started/locally/)
* [sentencepiece](https://github.com/google/sentencepiece)
* [cmake](https://cmake.org)
* [cupy](https://docs.cupy.dev/en/stable/install.html)

## Usage (in Python code)

The Python examples are in [`examples`](./examples/). We provide an overview here.

### Download Models

Download the pre-set models. Model IDs can be found in [config file](/diffsynth/configs/model_config.py).

```python
from diffsynth import download_models

download_models([&quot;FLUX.1-dev&quot;, &quot;Kolors&quot;])
```

Download your own models.

```python
from diffsynth.models.downloader import download_from_huggingface, download_from_modelscope

# From Modelscope (recommended)
download_from_modelscope(&quot;Kwai-Kolors/Kolors&quot;, &quot;vae/diffusion_pytorch_model.fp16.bin&quot;, &quot;models/kolors/Kolors/vae&quot;)
# From Huggingface
download_from_huggingface(&quot;Kwai-Kolors/Kolors&quot;, &quot;vae/diffusion_pytorch_model.fp16.safetensors&quot;, &quot;models/kolors/Kolors/vae&quot;)
```

### Video Synthesis

#### Text-to-video using CogVideoX-5B

CogVideoX-5B is released by ZhiPu. We provide an improved pipeline, supporting text-to-video, video editing, self-upscaling and video interpolation. [`examples/video_synthesis`](./examples/video_synthesis/)

The video on the left is generated using the original text-to-video pipeline, while the video on the right is the result after editing and frame interpolation.

https://github.com/user-attachments/assets/26b044c1-4a60-44a4-842f-627ff289d006

#### Long Video Synthesis

We trained extended video synthesis models, which can generate 128 frames. [`examples/ExVideo`](./examples/ExVideo/)

https://github.com/modelscope/DiffSynth-Studio/assets/35051019/d97f6aa9-8064-4b5b-9d49-ed6001bb9acc

https://github.com/user-attachments/assets/321ee04b-8c17-479e-8a95-8cbcf21f8d7e

#### Toon Shading

Render realistic videos in a flatten style and enable video editing features. [`examples/Diffutoon`](./examples/Diffutoon/)

https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/b54c05c5-d747-4709-be5e-b39af82404dd

https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/20528af5-5100-474a-8cdc-440b9efdd86c

#### Video Stylization

Video stylization without video models. [`examples/diffsynth`](./examples/diffsynth/)

https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/59fb2f7b-8de0-4481-b79f-0c3a7361a1ea

### Image Synthesis

Generate high-resolution images, by breaking the limitation of diffusion models! [`examples/image_synthesis`](./examples/image_synthesis/).

LoRA fine-tuning is supported in [`examples/train`](./examples/train/).

|FLUX|Stable Diffusion 3|
|-|-|
|![image_1024_cfg](https://github.com/user-attachments/assets/984561e9-553d-4952-9443-79ce144f379f)|![image_1024](https://github.com/modelscope/DiffSynth-Studio/assets/35051019/4df346db-6f91-420a-b4c1-26e205376098)|

|Kolors|Hunyuan-DiT|
|-|-|
|![image_1024](https://github.com/modelscope/DiffSynth-Studio/assets/35051019/53ef6f41-da11-4701-8665-9f64392607bf)|![image_1024](https://github.com/modelscope/DiffSynth-Studio/assets/35051019/60b022c8-df3f-4541-95ab-bf39f2fa8bb5)|

|Stable Diffusion|Stable Diffusion XL|
|-|-|
|![1024](https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/6fc84611-8da6-4a1f-8fee-9a34eba3b4a5)|![1024](https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/67687748-e738-438c-aee5-96096f09ac90)|

## Usage (in WebUI)

Create stunning images using the painter, with assistance from AI!

https://github.com/user-attachments/assets/95265d21-cdd6-4125-a7cb-9fbcf6ceb7b0

**This video is not rendered in real-time.**

Before launching the WebUI, please download models to the folder `./models`. See [here](#download-models).

* `Gradio` version

```
pip install gradio
```

```
python apps/gradio/DiffSynth_Studio.py
```

![20240822102002](https://github.com/user-attachments/assets/59613157-de51-4109-99b3-97cbffd88076)

* `Streamlit` version

```
pip install streamlit streamlit-drawable-canvas
```

```
python -m streamlit run apps/streamlit/DiffSynth_Studio.py
```

https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/93085557-73f3-4eee-a205-9829591ef954
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>