<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Fri, 05 Sep 2025 00:04:10 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[crewAIInc/crewAI]]></title>
            <link>https://github.com/crewAIInc/crewAI</link>
            <guid>https://github.com/crewAIInc/crewAI</guid>
            <pubDate>Fri, 05 Sep 2025 00:04:10 GMT</pubDate>
            <description><![CDATA[Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/crewAIInc/crewAI">crewAIInc/crewAI</a></h1>
            <p>Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.</p>
            <p>Language: Python</p>
            <p>Stars: 37,558</p>
            <p>Forks: 4,953</p>
            <p>Stars today: 90 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI&quot;&gt;
    &lt;img src=&quot;docs/images/crewai_logo.png&quot; width=&quot;600px&quot; alt=&quot;Open source Multi-AI Agent orchestration framework&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot; style=&quot;display: flex; justify-content: center; gap: 20px; align-items: center;&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/11239&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/11239&quot; alt=&quot;crewAIInc%2FcrewAI | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://crewai.com&quot;&gt;Homepage&lt;/a&gt;
  ·
  &lt;a href=&quot;https://docs.crewai.com&quot;&gt;Docs&lt;/a&gt;
  ·
  &lt;a href=&quot;https://app.crewai.com&quot;&gt;Start Cloud Trial&lt;/a&gt;
  ·
  &lt;a href=&quot;https://blog.crewai.com&quot;&gt;Blog&lt;/a&gt;
  ·
  &lt;a href=&quot;https://community.crewai.com&quot;&gt;Forum&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/crewAIInc/crewAI&quot; alt=&quot;GitHub Repo stars&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI/network/members&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/forks/crewAIInc/crewAI&quot; alt=&quot;GitHub forks&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI/issues&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/issues/crewAIInc/crewAI&quot; alt=&quot;GitHub issues&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI/pulls&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/issues-pr/crewAIInc/crewAI&quot; alt=&quot;GitHub pull requests&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/License-MIT-green.svg&quot; alt=&quot;License: MIT&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pypi.org/project/crewai/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/crewai&quot; alt=&quot;PyPI version&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/crewai/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/dm/crewai&quot; alt=&quot;PyPI downloads&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/crewAIInc&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/crewAIInc?style=social&quot; alt=&quot;Twitter Follow&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

### Fast and Flexible Multi-Agent Automation Framework

&gt; CrewAI is a lean, lightning-fast Python framework built entirely from scratch—completely **independent of LangChain or other agent frameworks**.
&gt; It empowers developers with both high-level simplicity and precise low-level control, ideal for creating autonomous AI agents tailored to any scenario.

- **CrewAI Crews**: Optimize for autonomy and collaborative intelligence.
- **CrewAI Flows**: Enable granular, event-driven control, single LLM calls for precise task orchestration and supports Crews natively

With over 100,000 developers certified through our community courses at [learn.crewai.com](https://learn.crewai.com), CrewAI is rapidly becoming the
standard for enterprise-ready AI automation.

# CrewAI Enterprise Suite

CrewAI Enterprise Suite is a comprehensive bundle tailored for organizations that require secure, scalable, and easy-to-manage agent-driven automation.

You can try one part of the suite the [Crew Control Plane for free](https://app.crewai.com)

## Crew Control Plane Key Features:

- **Tracing &amp; Observability**: Monitor and track your AI agents and workflows in real-time, including metrics, logs, and traces.
- **Unified Control Plane**: A centralized platform for managing, monitoring, and scaling your AI agents and workflows.
- **Seamless Integrations**: Easily connect with existing enterprise systems, data sources, and cloud infrastructure.
- **Advanced Security**: Built-in robust security and compliance measures ensuring safe deployment and management.
- **Actionable Insights**: Real-time analytics and reporting to optimize performance and decision-making.
- **24/7 Support**: Dedicated enterprise support to ensure uninterrupted operation and quick resolution of issues.
- **On-premise and Cloud Deployment Options**: Deploy CrewAI Enterprise on-premise or in the cloud, depending on your security and compliance requirements.

CrewAI Enterprise is designed for enterprises seeking a powerful, reliable solution to transform complex business processes into efficient,
intelligent automations.

## Table of contents

- [Why CrewAI?](#why-crewai)
- [Getting Started](#getting-started)
- [Key Features](#key-features)
- [Understanding Flows and Crews](#understanding-flows-and-crews)
- [CrewAI vs LangGraph](#how-crewai-compares)
- [Examples](#examples)
  - [Quick Tutorial](#quick-tutorial)
  - [Write Job Descriptions](#write-job-descriptions)
  - [Trip Planner](#trip-planner)
  - [Stock Analysis](#stock-analysis)
  - [Using Crews and Flows Together](#using-crews-and-flows-together)
- [Connecting Your Crew to a Model](#connecting-your-crew-to-a-model)
- [How CrewAI Compares](#how-crewai-compares)
- [Frequently Asked Questions (FAQ)](#frequently-asked-questions-faq)
- [Contribution](#contribution)
- [Telemetry](#telemetry)
- [License](#license)

## Why CrewAI?

&lt;div align=&quot;center&quot; style=&quot;margin-bottom: 30px;&quot;&gt;
  &lt;img src=&quot;docs/images/asset.png&quot; alt=&quot;CrewAI Logo&quot; width=&quot;100%&quot;&gt;
&lt;/div&gt;

CrewAI unlocks the true potential of multi-agent automation, delivering the best-in-class combination of speed, flexibility, and control with either Crews of AI Agents or Flows of Events:

- **Standalone Framework**: Built from scratch, independent of LangChain or any other agent framework.
- **High Performance**: Optimized for speed and minimal resource usage, enabling faster execution.
- **Flexible Low Level Customization**: Complete freedom to customize at both high and low levels - from overall workflows and system architecture to granular agent behaviors, internal prompts, and execution logic.
- **Ideal for Every Use Case**: Proven effective for both simple tasks and highly complex, real-world, enterprise-grade scenarios.
- **Robust Community**: Backed by a rapidly growing community of over **100,000 certified** developers offering comprehensive support and resources.

CrewAI empowers developers and enterprises to confidently build intelligent automations, bridging the gap between simplicity, flexibility, and performance.

## Getting Started

Setup and run your first CrewAI agents by following this tutorial.

[![CrewAI Getting Started Tutorial](https://img.youtube.com/vi/-kSOTtYzgEw/hqdefault.jpg)](https://www.youtube.com/watch?v=-kSOTtYzgEw &quot;CrewAI Getting Started Tutorial&quot;)

###
 Learning Resources

Learn CrewAI through our comprehensive courses:

- [Multi AI Agent Systems with CrewAI](https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/) - Master the fundamentals of multi-agent systems
- [Practical Multi AI Agents and Advanced Use Cases](https://www.deeplearning.ai/short-courses/practical-multi-ai-agents-and-advanced-use-cases-with-crewai/) - Deep dive into advanced implementations

### Understanding Flows and Crews

CrewAI offers two powerful, complementary approaches that work seamlessly together to build sophisticated AI applications:

1. **Crews**: Teams of AI agents with true autonomy and agency, working together to accomplish complex tasks through role-based collaboration. Crews enable:

   - Natural, autonomous decision-making between agents
   - Dynamic task delegation and collaboration
   - Specialized roles with defined goals and expertise
   - Flexible problem-solving approaches
2. **Flows**: Production-ready, event-driven workflows that deliver precise control over complex automations. Flows provide:

   - Fine-grained control over execution paths for real-world scenarios
   - Secure, consistent state management between tasks
   - Clean integration of AI agents with production Python code
   - Conditional branching for complex business logic

The true power of CrewAI emerges when combining Crews and Flows. This synergy allows you to:

- Build complex, production-grade applications
- Balance autonomy with precise control
- Handle sophisticated real-world scenarios
- Maintain clean, maintainable code structure

### Getting Started with Installation

To get started with CrewAI, follow these simple steps:

### 1. Installation

Ensure you have Python &gt;=3.10 &lt;3.14 installed on your system. CrewAI uses [UV](https://docs.astral.sh/uv/) for dependency management and package handling, offering a seamless setup and execution experience.

First, install CrewAI:

```shell
pip install crewai
```

If you want to install the &#039;crewai&#039; package along with its optional features that include additional tools for agents, you can do so by using the following command:

```shell
pip install &#039;crewai[tools]&#039;
```

The command above installs the basic package and also adds extra components which require more dependencies to function.

### Troubleshooting Dependencies

If you encounter issues during installation or usage, here are some common solutions:

#### Common Issues

1. **ModuleNotFoundError: No module named &#039;tiktoken&#039;**

   - Install tiktoken explicitly: `pip install &#039;crewai[embeddings]&#039;`
   - If using embedchain or other tools: `pip install &#039;crewai[tools]&#039;`
2. **Failed building wheel for tiktoken**

   - Ensure Rust compiler is installed (see installation steps above)
   - For Windows: Verify Visual C++ Build Tools are installed
   - Try upgrading pip: `pip install --upgrade pip`
   - If issues persist, use a pre-built wheel: `pip install tiktoken --prefer-binary`

### 2. Setting Up Your Crew with the YAML Configuration

To create a new CrewAI project, run the following CLI (Command Line Interface) command:

```shell
crewai create crew &lt;project_name&gt;
```

This command creates a new project folder with the following structure:

```
my_project/
├── .gitignore
├── pyproject.toml
├── README.md
├── .env
└── src/
    └── my_project/
        ├── __init__.py
        ├── main.py
        ├── crew.py
        ├── tools/
        │   ├── custom_tool.py
        │   └── __init__.py
        └── config/
            ├── agents.yaml
            └── tasks.yaml
```

You can now start developing your crew by editing the files in the `src/my_project` folder. The `main.py` file is the entry point of the project, the `crew.py` file is where you define your crew, the `agents.yaml` file is where you define your agents, and the `tasks.yaml` file is where you define your tasks.

#### To customize your project, you can:

- Modify `src/my_project/config/agents.yaml` to define your agents.
- Modify `src/my_project/config/tasks.yaml` to define your tasks.
- Modify `src/my_project/crew.py` to add your own logic, tools, and specific arguments.
- Modify `src/my_project/main.py` to add custom inputs for your agents and tasks.
- Add your environment variables into the `.env` file.

#### Example of a simple crew with a sequential process:

Instantiate your crew:

```shell
crewai create crew latest-ai-development
```

Modify the files as needed to fit your use case:

**agents.yaml**

```yaml
# src/my_project/config/agents.yaml
researcher:
  role: &gt;
    {topic} Senior Data Researcher
  goal: &gt;
    Uncover cutting-edge developments in {topic}
  backstory: &gt;
    You&#039;re a seasoned researcher with a knack for uncovering the latest
    developments in {topic}. Known for your ability to find the most relevant
    information and present it in a clear and concise manner.

reporting_analyst:
  role: &gt;
    {topic} Reporting Analyst
  goal: &gt;
    Create detailed reports based on {topic} data analysis and research findings
  backstory: &gt;
    You&#039;re a meticulous analyst with a keen eye for detail. You&#039;re known for
    your ability to turn complex data into clear and concise reports, making
    it easy for others to understand and act on the information you provide.
```

**tasks.yaml**

```yaml
# src/my_project/config/tasks.yaml
research_task:
  description: &gt;
    Conduct a thorough research about {topic}
    Make sure you find any interesting and relevant information given
    the current year is 2025.
  expected_output: &gt;
    A list with 10 bullet points of the most relevant information about {topic}
  agent: researcher

reporting_task:
  description: &gt;
    Review the context you got and expand each topic into a full section for a report.
    Make sure the report is detailed and contains any and all relevant information.
  expected_output: &gt;
    A fully fledge reports with the mains topics, each with a full section of information.
    Formatted as markdown without &#039;```&#039;
  agent: reporting_analyst
  output_file: report.md
```

**crew.py**

```python
# src/my_project/crew.py
from crewai import Agent, Crew, Process, Task
from crewai.project import CrewBase, agent, crew, task
from crewai_tools import SerperDevTool
from crewai.agents.agent_builder.base_agent import BaseAgent
from typing import List

@CrewBase
class LatestAiDevelopmentCrew():
	&quot;&quot;&quot;LatestAiDevelopment crew&quot;&quot;&quot;
	agents: List[BaseAgent]
	tasks: List[Task]

	@agent
	def researcher(self) -&gt; Agent:
		return Agent(
			config=self.agents_config[&#039;researcher&#039;],
			verbose=True,
			tools=[SerperDevTool()]
		)

	@agent
	def reporting_analyst(self) -&gt; Agent:
		return Agent(
			config=self.agents_config[&#039;reporting_analyst&#039;],
			verbose=True
		)

	@task
	def research_task(self) -&gt; Task:
		return Task(
			config=self.tasks_config[&#039;research_task&#039;],
		)

	@task
	def reporting_task(self) -&gt; Task:
		return Task(
			config=self.tasks_config[&#039;reporting_task&#039;],
			output_file=&#039;report.md&#039;
		)

	@crew
	def crew(self) -&gt; Crew:
		&quot;&quot;&quot;Creates the LatestAiDevelopment crew&quot;&quot;&quot;
		return Crew(
			agents=self.agents, # Automatically created by the @agent decorator
			tasks=self.tasks, # Automatically created by the @task decorator
			process=Process.sequential,
			verbose=True,
		)
```

**main.py**

```python
#!/usr/bin/env python
# src/my_project/main.py
import sys
from latest_ai_development.crew import LatestAiDevelopmentCrew

def run():
    &quot;&quot;&quot;
    Run the crew.
    &quot;&quot;&quot;
    inputs = {
        &#039;topic&#039;: &#039;AI Agents&#039;
    }
    LatestAiDevelopmentCrew().crew().kickoff(inputs=inputs)
```

### 3. Running Your Crew

Before running your crew, make sure you have the following keys set as environment variables in your `.env` file:

- An [OpenAI API key](https://platform.openai.com/account/api-keys) (or other LLM API key): `OPENAI_API_KEY=sk-...`
- A [Serper.dev](https://serper.dev/) API key: `SERPER_API_KEY=YOUR_KEY_HERE`

Lock the dependencies and install them by using the CLI command but first, navigate to your project directory:

```shell
cd my_project
crewai install (Optional)
```

To run your crew, execute the following command in the root of your project:

```bash
crewai run
```

or

```bash
python src/my_project/main.py
```

If an error happens due to the usage of poetry, please run the following command to update your crewai package:

```bash
crewai update
```

You should see the output in the console and the `report.md` file should be created in the root of your project with the full final report.

In addition to the sequential process, you can use the hierarchical process, which automatically assigns a manager to the defined crew to properly coordinate the planning and execution of tasks through delegation and validation of results. [See more about the processes here](https://docs.crewai.com/core-concepts/Processes/).

## Key Features

CrewAI stands apart as a lean, standalone, high-performance multi-AI Agent framework delivering simplicity, flexibility, and precise control—free from the complexity and limitations found in other agent frameworks.

- **Standalone &amp; Lean**: Completely independent from other frameworks like LangChain, offering faster execution and lighter resource demands.
- **Flexible &amp; Precise**: Easily orchestrate autonomous agents through intuitive [Crews](https://docs.crewai.com/concepts/crews) or precise [Flows](https://docs.crewai.com/concepts/flows), achieving perfect balance for your needs.
- **Seamless Integration**: Effortlessly combine Crews (autonomy) and Flows (precision) to create complex, real-world automations.
- **Deep Customization**: Tailor every aspect—from high-level workflows down to low-level internal prompts and agent behaviors.
- **Reliable Performance**: Consistent results across simple tasks and complex, enterprise-level automations.
- **Thriving Community**: Backed by robust documentation and over 100,000 certified developers, providing exceptional support and guidance.

Choose CrewAI to easily build powerful, adaptable, and production-ready AI automations.

## Examples

You can test different real life examples of AI crews in the [CrewAI-examples repo](https://github.com/crewAIInc/crewAI-examples?tab=readme-ov-file):

- [Landing Page Generator](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/landing_page_generator)
- [Having Human input on the execution](https://docs.crewai.com/how-to/Human-Input-on-Execution)
- [Trip Planner](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/trip_planner)
- [Stock Analysis](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/stock_analysis)

### Quick Tutorial

[![CrewAI Tutorial](https://img.youtube.com/vi/tnejrr-0a94/maxresdefault.jpg)](https://www.youtube.com/watch?v=tnejrr-0a94 &quot;CrewAI Tutorial&quot;)

### Write Job Descriptions

[Check out code for this example](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/job-posting) or watch a video below:

[![Jobs postings](https://img.youtube.com/vi/u98wEMz-9to/maxresdefault.jpg)](https://www.youtube.com/watch?v=u98wEMz-9to &quot;Jobs postings&quot;)

### Trip Planner

[Check out code for this example](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/trip_planner) or watch a video below:

[![Trip Planner](https://img.youtube.com/vi/xis7rWp-hjs/maxresdefault.jpg)](https://www.youtube.com/watch?v=xis7rWp-hjs &quot;Trip Planner&quot;)

### Stock Analysis

[Check out code for this example](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/stock_analysis) or watch a video below:

[![Stock Analysis](https://img.youtube.com/vi/e0Uj4yWdaAg/maxresdefault.jpg)](https://www.youtube.com/watch?v=e0Uj4yWdaAg &quot;Stock Analysis&quot;)

### Using Crews and Flows Together

CrewAI&#039;s power truly shines when combining Crews with Flows to create sophisticated automation pipelines.
CrewAI flows support logical operators like `or_` and `and_` to combine multiple conditions. This can be used with `@start`, `@listen`, or `@router` decorators to create complex triggering conditions.

- `or_`: Triggers when any of the specified conditions are met.
- `and_`Triggers when all of the specified conditions are met.

Here&#039;s how you can orchestrate multiple Crews within a Flow:

```python
from crewai.flow.flow import Flow, listen, start, router, or_
from crewai import Crew, Agent, Task, Process
from pydantic import BaseModel

# Define structured state for precise control
class MarketState(BaseModel):
    sentiment: str = &quot;neutral&quot;
    confidence: float = 0.0
    recommendations: list = []

class AdvancedAnalysisFlow(Flow[MarketState]):
    @start()
    def fetch_market_data(self):
        # Demonstrate low-level control with structured state
        self.state.sentiment = &quot;analyzing&quot;
        return {&quot;sector&quot;: &quot;tech&quot;, &quot;timeframe&quot;: &quot;1W&quot;}  # These parameters match the task description template

    @listen(fetch_market_data)
    def analyze_with_crew(self, market_data):
        # Show crew agency through specialized roles
        analyst = Agent(
            role=&quot;Senior Market Analyst&quot;,
            goal=&quot;Conduct deep market analysis with expert insight&quot;,
            backstory=&quot;You&#039;re a veteran analyst known for identifying subtle market patterns&quot;
        )
        researcher = Agent(
            role=&quot;Data Researcher&quot;,
            goal=&quot;Gather and validate supporting market data&quot;,
            backstory=&quot;You excel at finding and correlating multiple data sources&quot;
        )

        analysis_task = Task(
            description=&quot;Analyze {sector} sector data for the past {timeframe}&quot;,
            expected_output=&quot;Detailed market analysis with confidence score&quot;,
            agent=analyst
        )
        research_task = Task(
            description=&quot;Fi

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[eriklindernoren/ML-From-Scratch]]></title>
            <link>https://github.com/eriklindernoren/ML-From-Scratch</link>
            <guid>https://github.com/eriklindernoren/ML-From-Scratch</guid>
            <pubDate>Fri, 05 Sep 2025 00:04:09 GMT</pubDate>
            <description><![CDATA[Machine Learning From Scratch. Bare bones NumPy implementations of machine learning models and algorithms with a focus on accessibility. Aims to cover everything from linear regression to deep learning.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/eriklindernoren/ML-From-Scratch">eriklindernoren/ML-From-Scratch</a></h1>
            <p>Machine Learning From Scratch. Bare bones NumPy implementations of machine learning models and algorithms with a focus on accessibility. Aims to cover everything from linear regression to deep learning.</p>
            <p>Language: Python</p>
            <p>Stars: 27,940</p>
            <p>Forks: 4,860</p>
            <p>Stars today: 239 stars today</p>
            <h2>README</h2><pre># Machine Learning From Scratch

## About
Python implementations of some of the fundamental Machine Learning models and algorithms from scratch.

The purpose of this project is not to produce as optimized and computationally efficient algorithms as possible
but rather to present the inner workings of them in a transparent and accessible way.

## Table of Contents
- [Machine Learning From Scratch](#machine-learning-from-scratch)
  * [About](#about)
  * [Table of Contents](#table-of-contents)
  * [Installation](#installation)
  * [Examples](#examples)
    + [Polynomial Regression](#polynomial-regression)
    + [Classification With CNN](#classification-with-cnn)
    + [Density-Based Clustering](#density-based-clustering)
    + [Generating Handwritten Digits](#generating-handwritten-digits)
    + [Deep Reinforcement Learning](#deep-reinforcement-learning)
    + [Image Reconstruction With RBM](#image-reconstruction-with-rbm)
    + [Evolutionary Evolved Neural Network](#evolutionary-evolved-neural-network)
    + [Genetic Algorithm](#genetic-algorithm)
    + [Association Analysis](#association-analysis)
  * [Implementations](#implementations)
    + [Supervised Learning](#supervised-learning)
    + [Unsupervised Learning](#unsupervised-learning)
    + [Reinforcement Learning](#reinforcement-learning)
    + [Deep Learning](#deep-learning)
  * [Contact](#contact)

## Installation
    $ git clone https://github.com/eriklindernoren/ML-From-Scratch
    $ cd ML-From-Scratch
    $ python setup.py install

## Examples
### Polynomial Regression
    $ python mlfromscratch/examples/polynomial_regression.py

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;http://eriklindernoren.se/images/p_reg.gif&quot; width=&quot;640&quot;\&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    Figure: Training progress of a regularized polynomial regression model fitting &lt;br&gt;
    temperature data measured in Linköping, Sweden 2016.
&lt;/p&gt;

### Classification With CNN
    $ python mlfromscratch/examples/convolutional_neural_network.py

    +---------+
    | ConvNet |
    +---------+
    Input Shape: (1, 8, 8)
    +----------------------+------------+--------------+
    | Layer Type           | Parameters | Output Shape |
    +----------------------+------------+--------------+
    | Conv2D               | 160        | (16, 8, 8)   |
    | Activation (ReLU)    | 0          | (16, 8, 8)   |
    | Dropout              | 0          | (16, 8, 8)   |
    | BatchNormalization   | 2048       | (16, 8, 8)   |
    | Conv2D               | 4640       | (32, 8, 8)   |
    | Activation (ReLU)    | 0          | (32, 8, 8)   |
    | Dropout              | 0          | (32, 8, 8)   |
    | BatchNormalization   | 4096       | (32, 8, 8)   |
    | Flatten              | 0          | (2048,)      |
    | Dense                | 524544     | (256,)       |
    | Activation (ReLU)    | 0          | (256,)       |
    | Dropout              | 0          | (256,)       |
    | BatchNormalization   | 512        | (256,)       |
    | Dense                | 2570       | (10,)        |
    | Activation (Softmax) | 0          | (10,)        |
    +----------------------+------------+--------------+
    Total Parameters: 538570

    Training: 100% [------------------------------------------------------------------------] Time: 0:01:55
    Accuracy: 0.987465181058

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;http://eriklindernoren.se/images/mlfs_cnn1.png&quot; width=&quot;640&quot;&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    Figure: Classification of the digit dataset using CNN.
&lt;/p&gt;

### Density-Based Clustering
    $ python mlfromscratch/examples/dbscan.py

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;http://eriklindernoren.se/images/mlfs_dbscan.png&quot; width=&quot;640&quot;&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    Figure: Clustering of the moons dataset using DBSCAN.
&lt;/p&gt;

### Generating Handwritten Digits
    $ python mlfromscratch/unsupervised_learning/generative_adversarial_network.py

    +-----------+
    | Generator |
    +-----------+
    Input Shape: (100,)
    +------------------------+------------+--------------+
    | Layer Type             | Parameters | Output Shape |
    +------------------------+------------+--------------+
    | Dense                  | 25856      | (256,)       |
    | Activation (LeakyReLU) | 0          | (256,)       |
    | BatchNormalization     | 512        | (256,)       |
    | Dense                  | 131584     | (512,)       |
    | Activation (LeakyReLU) | 0          | (512,)       |
    | BatchNormalization     | 1024       | (512,)       |
    | Dense                  | 525312     | (1024,)      |
    | Activation (LeakyReLU) | 0          | (1024,)      |
    | BatchNormalization     | 2048       | (1024,)      |
    | Dense                  | 803600     | (784,)       |
    | Activation (TanH)      | 0          | (784,)       |
    +------------------------+------------+--------------+
    Total Parameters: 1489936

    +---------------+
    | Discriminator |
    +---------------+
    Input Shape: (784,)
    +------------------------+------------+--------------+
    | Layer Type             | Parameters | Output Shape |
    +------------------------+------------+--------------+
    | Dense                  | 401920     | (512,)       |
    | Activation (LeakyReLU) | 0          | (512,)       |
    | Dropout                | 0          | (512,)       |
    | Dense                  | 131328     | (256,)       |
    | Activation (LeakyReLU) | 0          | (256,)       |
    | Dropout                | 0          | (256,)       |
    | Dense                  | 514        | (2,)         |
    | Activation (Softmax)   | 0          | (2,)         |
    +------------------------+------------+--------------+
    Total Parameters: 533762


&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;http://eriklindernoren.se/images/gan_mnist5.gif&quot; width=&quot;640&quot;&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    Figure: Training progress of a Generative Adversarial Network generating &lt;br&gt;
    handwritten digits.
&lt;/p&gt;

### Deep Reinforcement Learning
    $ python mlfromscratch/examples/deep_q_network.py

    +----------------+
    | Deep Q-Network |
    +----------------+
    Input Shape: (4,)
    +-------------------+------------+--------------+
    | Layer Type        | Parameters | Output Shape |
    +-------------------+------------+--------------+
    | Dense             | 320        | (64,)        |
    | Activation (ReLU) | 0          | (64,)        |
    | Dense             | 130        | (2,)         |
    +-------------------+------------+--------------+
    Total Parameters: 450

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;http://eriklindernoren.se/images/mlfs_dql1.gif&quot; width=&quot;640&quot;&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    Figure: Deep Q-Network solution to the CartPole-v1 environment in OpenAI gym.
&lt;/p&gt;

### Image Reconstruction With RBM
    $ python mlfromscratch/examples/restricted_boltzmann_machine.py

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;http://eriklindernoren.se/images/rbm_digits1.gif&quot; width=&quot;640&quot;&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    Figure: Shows how the network gets better during training at reconstructing &lt;br&gt;
    the digit 2 in the MNIST dataset.
&lt;/p&gt;

### Evolutionary Evolved Neural Network
    $ python mlfromscratch/examples/neuroevolution.py

    +---------------+
    | Model Summary |
    +---------------+
    Input Shape: (64,)
    +----------------------+------------+--------------+
    | Layer Type           | Parameters | Output Shape |
    +----------------------+------------+--------------+
    | Dense                | 1040       | (16,)        |
    | Activation (ReLU)    | 0          | (16,)        |
    | Dense                | 170        | (10,)        |
    | Activation (Softmax) | 0          | (10,)        |
    +----------------------+------------+--------------+
    Total Parameters: 1210

    Population Size: 100
    Generations: 3000
    Mutation Rate: 0.01

    [0 Best Individual - Fitness: 3.08301, Accuracy: 10.5%]
    [1 Best Individual - Fitness: 3.08746, Accuracy: 12.0%]
    ...
    [2999 Best Individual - Fitness: 94.08513, Accuracy: 98.5%]
    Test set accuracy: 96.7%

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;http://eriklindernoren.se/images/evo_nn4.png&quot; width=&quot;640&quot;&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    Figure: Classification of the digit dataset by a neural network which has&lt;br&gt;
    been evolutionary evolved.
&lt;/p&gt;

### Genetic Algorithm
    $ python mlfromscratch/examples/genetic_algorithm.py

    +--------+
    |   GA   |
    +--------+
    Description: Implementation of a Genetic Algorithm which aims to produce
    the user specified target string. This implementation calculates each
    candidate&#039;s fitness based on the alphabetical distance between the candidate
    and the target. A candidate is selected as a parent with probabilities proportional
    to the candidate&#039;s fitness. Reproduction is implemented as a single-point
    crossover between pairs of parents. Mutation is done by randomly assigning
    new characters with uniform probability.

    Parameters
    ----------
    Target String: &#039;Genetic Algorithm&#039;
    Population Size: 100
    Mutation Rate: 0.05

    [0 Closest Candidate: &#039;CJqlJguPlqzvpoJmb&#039;, Fitness: 0.00]
    [1 Closest Candidate: &#039;MCxZxdr nlfiwwGEk&#039;, Fitness: 0.01]
    [2 Closest Candidate: &#039;MCxZxdm nlfiwwGcx&#039;, Fitness: 0.01]
    [3 Closest Candidate: &#039;SmdsAklMHn kBIwKn&#039;, Fitness: 0.01]
    [4 Closest Candidate: &#039;  lotneaJOasWfu Z&#039;, Fitness: 0.01]
    ...
    [292 Closest Candidate: &#039;GeneticaAlgorithm&#039;, Fitness: 1.00]
    [293 Closest Candidate: &#039;GeneticaAlgorithm&#039;, Fitness: 1.00]
    [294 Answer: &#039;Genetic Algorithm&#039;]

### Association Analysis
    $ python mlfromscratch/examples/apriori.py
    +-------------+
    |   Apriori   |
    +-------------+
    Minimum Support: 0.25
    Minimum Confidence: 0.8
    Transactions:
        [1, 2, 3, 4]
        [1, 2, 4]
        [1, 2]
        [2, 3, 4]
        [2, 3]
        [3, 4]
        [2, 4]
    Frequent Itemsets:
        [1, 2, 3, 4, [1, 2], [1, 4], [2, 3], [2, 4], [3, 4], [1, 2, 4], [2, 3, 4]]
    Rules:
        1 -&gt; 2 (support: 0.43, confidence: 1.0)
        4 -&gt; 2 (support: 0.57, confidence: 0.8)
        [1, 4] -&gt; 2 (support: 0.29, confidence: 1.0)


## Implementations
### Supervised Learning
- [Adaboost](mlfromscratch/supervised_learning/adaboost.py)
- [Bayesian Regression](mlfromscratch/supervised_learning/bayesian_regression.py)
- [Decision Tree](mlfromscratch/supervised_learning/decision_tree.py)
- [Elastic Net](mlfromscratch/supervised_learning/regression.py)
- [Gradient Boosting](mlfromscratch/supervised_learning/gradient_boosting.py)
- [K Nearest Neighbors](mlfromscratch/supervised_learning/k_nearest_neighbors.py)
- [Lasso Regression](mlfromscratch/supervised_learning/regression.py)
- [Linear Discriminant Analysis](mlfromscratch/supervised_learning/linear_discriminant_analysis.py)
- [Linear Regression](mlfromscratch/supervised_learning/regression.py)
- [Logistic Regression](mlfromscratch/supervised_learning/logistic_regression.py)
- [Multi-class Linear Discriminant Analysis](mlfromscratch/supervised_learning/multi_class_lda.py)
- [Multilayer Perceptron](mlfromscratch/supervised_learning/multilayer_perceptron.py)
- [Naive Bayes](mlfromscratch/supervised_learning/naive_bayes.py)
- [Neuroevolution](mlfromscratch/supervised_learning/neuroevolution.py)
- [Particle Swarm Optimization of Neural Network](mlfromscratch/supervised_learning/particle_swarm_optimization.py)
- [Perceptron](mlfromscratch/supervised_learning/perceptron.py)
- [Polynomial Regression](mlfromscratch/supervised_learning/regression.py)
- [Random Forest](mlfromscratch/supervised_learning/random_forest.py)
- [Ridge Regression](mlfromscratch/supervised_learning/regression.py)
- [Support Vector Machine](mlfromscratch/supervised_learning/support_vector_machine.py)
- [XGBoost](mlfromscratch/supervised_learning/xgboost.py)

### Unsupervised Learning
- [Apriori](mlfromscratch/unsupervised_learning/apriori.py)
- [Autoencoder](mlfromscratch/unsupervised_learning/autoencoder.py)
- [DBSCAN](mlfromscratch/unsupervised_learning/dbscan.py)
- [FP-Growth](mlfromscratch/unsupervised_learning/fp_growth.py)
- [Gaussian Mixture Model](mlfromscratch/unsupervised_learning/gaussian_mixture_model.py)
- [Generative Adversarial Network](mlfromscratch/unsupervised_learning/generative_adversarial_network.py)
- [Genetic Algorithm](mlfromscratch/unsupervised_learning/genetic_algorithm.py)
- [K-Means](mlfromscratch/unsupervised_learning/k_means.py)
- [Partitioning Around Medoids](mlfromscratch/unsupervised_learning/partitioning_around_medoids.py)
- [Principal Component Analysis](mlfromscratch/unsupervised_learning/principal_component_analysis.py)
- [Restricted Boltzmann Machine](mlfromscratch/unsupervised_learning/restricted_boltzmann_machine.py)

### Reinforcement Learning
- [Deep Q-Network](mlfromscratch/reinforcement_learning/deep_q_network.py)

### Deep Learning
  + [Neural Network](mlfromscratch/deep_learning/neural_network.py)
  + [Layers](mlfromscratch/deep_learning/layers.py)
    * Activation Layer
    * Average Pooling Layer
    * Batch Normalization Layer
    * Constant Padding Layer
    * Convolutional Layer
    * Dropout Layer
    * Flatten Layer
    * Fully-Connected (Dense) Layer
    * Fully-Connected RNN Layer
    * Max Pooling Layer
    * Reshape Layer
    * Up Sampling Layer
    * Zero Padding Layer
  + Model Types
    * [Convolutional Neural Network](mlfromscratch/examples/convolutional_neural_network.py)
    * [Multilayer Perceptron](mlfromscratch/examples/multilayer_perceptron.py)
    * [Recurrent Neural Network](mlfromscratch/examples/recurrent_neural_network.py)

## Contact
If there&#039;s some implementation you would like to see here or if you&#039;re just feeling social,
feel free to [email](mailto:eriklindernoren@gmail.com) me or connect with me on [LinkedIn](https://www.linkedin.com/in/eriklindernoren/).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[weaviate/elysia]]></title>
            <link>https://github.com/weaviate/elysia</link>
            <guid>https://github.com/weaviate/elysia</guid>
            <pubDate>Fri, 05 Sep 2025 00:04:08 GMT</pubDate>
            <description><![CDATA[Python package and backend for the Elysia platform app.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/weaviate/elysia">weaviate/elysia</a></h1>
            <p>Python package and backend for the Elysia platform app.</p>
            <p>Language: Python</p>
            <p>Stars: 1,327</p>
            <p>Forks: 168</p>
            <p>Stars today: 129 stars today</p>
            <h2>README</h2><pre># Elysia: Agentic Framework Powered by Decision Trees

&gt; **⚠️ Elysia is in beta!**
&gt;
&gt; If you encounter any issues, please [open an issue on GitHub](https://github.com/weaviate/elysia/issues).

[![PyPI Downloads](https://static.pepy.tech/badge/elysia-ai)](https://pepy.tech/projects/elysia-ai) [![Demo](https://img.shields.io/badge/Check%20out%20the%20demo!-yellow?&amp;style=flat-square&amp;logo=react&amp;logoColor=white)](https://elysia.weaviate.io/)

Elysia is an agentic platform designed to use tools in a decision tree. A decision agent decides which tools to use dynamically based on its environment and context. You can use custom tools or use the pre-built tools designed to retrieve your data in a Weaviate cluster.

[Read the docs!](https://weaviate.github.io/elysia/)

Installation is as simple as:
```bash
pip install elysia-ai
```

## Get started (App)

Run the app via

```bash
elysia start
```
Then navigate to the settings page, add your required API keys, Weaviate cloud cluster details and specify your models.

Don&#039;t forget to check out [the Github Repository for the Frontend](https://github.com/weaviate/elysia-frontend)!

Alternatively, we have created a demo version of Elysia (rate-limited, fixed datasets) to experiment with. Find it at: https://elysia.weaviate.io/

## Get Started (Python)

To use Elysia, you need to either set up your models and API keys in your `.env` file, or specify them in the config. [See the setup page to get started.](https://weaviate.github.io/elysia/setting_up/)

Elysia can be used very simply:
```python
from elysia import tool, Tree

tree = Tree()

@tool(tree=tree)
async def add(x: int, y: int) -&gt; int:
    return x + y

tree(&quot;What is the sum of 9009 and 6006?&quot;)
```

Elysia is pre-configured to be capable of connecting to and interacting with your [Weaviate](https://weaviate.io/deployment/serverless) clusters!
```python
import elysia
tree = elysia.Tree()
response, objects = tree(
    &quot;What are the 10 most expensive items in the Ecommerce collection?&quot;,
    collection_names = [&quot;Ecommerce&quot;]
)
```
This will use the built-in open source _query_ tool or _aggregate_ tool to interact with your Weaviate collections. To get started connecting to Weaviate, [see the setting up page in the docs](https://weaviate.github.io/elysia/setting_up/).

## Installation (bash) (Linux/MacOS)

### PyPi (Recommended)

Elysia requires Python 3.12:
- [Installation via brew (macOS)](https://formulae.brew.sh/formula/python@3.12)
- [Installation via installer (Windows)](https://www.python.org/downloads/release/python-3120/)
- [Installation (Ubuntu)](https://ubuntuhandbook.org/index.php/2023/05/install-python-3-12-ubuntu/)

Optionally create a virtual environment via
```bash
python3.12 -m venv .venv
source .venv/bin/activate
```

Then simply run 
```bash
pip install elysia-ai
```
to install straight away!

### GitHub

To get the latest development version, you can clone the github repo by running
```bash
git clone https://github.com/weaviate/elysia
```
move to the working directory via
```bash
cd elysia
```
Create a virtual environment with Python (version 3.10 - 3.12)
```bash
python3.12 -m venv .venv
source .venv/bin/activate
```
and then install Elysia via pip
```bash
pip install -e .
```
Done! You can now use the Elysia python package


### Configuring Settings

To use Elysia with Weaviate, i.e. for agentic searching and retrieval, you need a Weaviate cluster api key and URL. This can be specific in the app directly, or by creating a `.env` file with
```
WCD_URL=...
WCD_API_KEY=...
```
Elysia will automatically detect these when running locally, and this will be the default Weaviate cluster for all users logging into the Elysia app. But these can be configured on a user-by-user basis through the config.

Whichever vectoriser you use for your Weaviate collection you will need to specify your corresponding API key, e.g.
```
OPENAI_API_KEY=...
```
These will automatically be added to the headers for the Weaviate client.

Same for whichever model you choose for the LLM in Elysia, so if you are using GPT-4o, for example, specify an `OPENAI_API_KEY`.

Elysia&#039;s recommended config is to use [OpenRouter](https://openrouter.ai/) to give easy access to a variety of models. So this requires
```
OPENROUTER_API_KEY=...
```

## FAQ

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;How do I use Elysia with my own data?&lt;/b&gt;&lt;/summary&gt;

You can connect to your own Weaviate cloud cluster, which will automatically identify any collections that exist in the cluster.

Collections require being _preprocessed_ for Elysia. In the app, you just click the &#039;analyze&#039; button in the Data tab. In Python you can do:

```python
from elysia.preprocessing.collection import preprocess

preprocess(collection_names=[&quot;YourCollectionName&quot;])
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Can I use a locally running version of Weaviate such as with Docker?&lt;/b&gt;&lt;/summary&gt;
Locally running versions of Weaviate are currently not implemented in the current version of the app but this is planned for a future release. Stay tuned!
&lt;/details&gt;


&lt;details&gt;
&lt;summary&gt;&lt;b&gt;How do I clear all my Elysia data?&lt;/b&gt;&lt;/summary&gt;

Everything Elysia doesn&#039;t store locally will be a collection in your Weaviate cluster. You can delete any collections that start with `ELYSIA_` to reset all your Elysia data.

For example, in Python:
```python
from elysia.util.client import ClientManager()
with ClientManager().connect_to_client() as client:
    for collection_name in client.collections.list_all():
        if collection_name.startswith(&quot;ELYSIA_&quot;):
            client.collections.delete(collection_name)
```
&lt;/details&gt;


&lt;details&gt;

&lt;summary&gt;&lt;b&gt;Can I contribute to Elysia?&lt;/b&gt;&lt;/summary&gt;

Elysia is **fully open source**, so yes of course you can! Clone and create a new branch of Elysia via
```bash
git clone https://github.com/weaviate/elysia
git checkout -b &lt;branch_name&gt;
```
Make your changes, push them to your branch, go to GitHub and submit a pull request.

&lt;/details&gt;


&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Where is the best place I can start contributing?&lt;/b&gt;&lt;/summary&gt;

There are no &#039;huge&#039; new features we are planning for Elysia (for the moment). You could start with creating a new tool, or multiple new tools to create a custom workflow for something specific. Look for pain points you experience from your user journey and find what exactly is causing these. Then try to fix them or create an alternative way of doing things!

&lt;/details&gt;

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[socfortress/Wazuh-Rules]]></title>
            <link>https://github.com/socfortress/Wazuh-Rules</link>
            <guid>https://github.com/socfortress/Wazuh-Rules</guid>
            <pubDate>Fri, 05 Sep 2025 00:04:07 GMT</pubDate>
            <description><![CDATA[Advanced Wazuh Rules for more accurate threat detection. Feel free to implement within your own Wazuh environment, contribute, or fork!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/socfortress/Wazuh-Rules">socfortress/Wazuh-Rules</a></h1>
            <p>Advanced Wazuh Rules for more accurate threat detection. Feel free to implement within your own Wazuh environment, contribute, or fork!</p>
            <p>Language: Python</p>
            <p>Stars: 979</p>
            <p>Forks: 243</p>
            <p>Stars today: 29 stars today</p>
            <h2>README</h2><pre>[&lt;img src=&quot;images/logo_orange.svg&quot; align=&quot;right&quot; width=&quot;100&quot; height=&quot;100&quot; /&gt;](https://www.socfortress.co/)

# Advanced Wazuh Detection Rules [![Awesome](https://img.shields.io/badge/SOCFortress-Worlds%20First%20Free%20Cloud%20SOC-orange)](https://www.socfortress.co/trial.html)
&gt; The SOCFortress Team has committed to contributing to the Open Source community. We hope you find these rulesets helpful and robust as you work to keep your networks secure.


[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![MIT License][license-shield]][license-url]
[![LinkedIn][linkedin-shield]][linkedin-url]
[![your-own-soc-free-for-life-tier](https://img.shields.io/badge/Get%20Started-FREE%20FOR%20LIFE%20TIER-orange)](https://www.socfortress.co/trial.html)

&lt;!-- PROJECT LOGO --&gt;
&lt;br /&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/socfortress/Wazuh-Rules&quot;&gt;
    &lt;img src=&quot;images/logo_orange.svg&quot; alt=&quot;Logo&quot; width=&quot;100&quot; height=&quot;100&quot;&gt;
    &lt;img src=&quot;images/wazuh_logo.png&quot; alt=&quot;Logo&quot;&gt;
  &lt;/a&gt;

  &lt;h3 align=&quot;center&quot;&gt;Advanced Wazuh Detection Rules&lt;/h3&gt;

  &lt;p align=&quot;center&quot;&gt;
    Have Wazuh deployed and ingesting your logs but looking for some better detection rules? Look no further. The objective for this repo is to provide the Wazuh community with rulesets that are more accurate, descriptive, and enriched from various sources and integrations.
    &lt;br /&gt;
    &lt;a href=&quot;https://www.socfortress.co/index.html&quot;&gt;&lt;strong&gt;Worlds First Open Source Cloud SOC »&lt;/strong&gt;&lt;/a&gt;
    &lt;br /&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://documentation.wazuh.com/current/index.html&quot;&gt;Wazuh Docs&lt;/a&gt;
    ·
    &lt;a href=&quot;https://www.socfortress.co/trial.html&quot;&gt;FREE FOR LIFE TIER&lt;/a&gt;
    ·
    &lt;a href=&quot;https://socfortress.medium.com/&quot;&gt;Our Blog&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;


&lt;!-- TABLE OF CONTENTS --&gt;
&lt;details&gt;
  &lt;summary&gt;Table of Contents&lt;/summary&gt;
  &lt;ol&gt;
    &lt;li&gt;
      &lt;a href=&quot;#about-this-repo&quot;&gt;About This Repo&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&quot;#supported-rules-and-integrations&quot;&gt;Supported Rules and Integrations&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;
      &lt;a href=&quot;#getting-started&quot;&gt;Getting Started&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&quot;#prerequisites&quot;&gt;Prerequisites&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#installation&quot;&gt;Installation&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#contributing&quot;&gt;Contributing&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#contact&quot;&gt;Contact&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#acknowledgments&quot;&gt;Acknowledgments&lt;/a&gt;&lt;/li&gt;
  &lt;/ol&gt;
&lt;/details&gt;



&lt;!-- ABOUT THE PROJECT --&gt;
## About This Repo

The objective for this repo is to provide the Wazuh community with rulesets that are more accurate, descriptive, and enriched from various sources and integrations.

Here&#039;s why:
* Detection rules can be a tricky business and we believe everyone should have access to a strong and growing ruleset.
* Wazuh serves as a great EDR agent, however the default rulesets are rather laxed (in our opinion). We wanted to start building a strong repo of Wazuh rules for the community to implement themselves and expand upon as new threats arise.
* Cybersecurity is hard enough, let&#039;s work together :smile:


&lt;p align=&quot;right&quot;&gt;(&lt;a href=&quot;#readme-top&quot;&gt;back to top&lt;/a&gt;)&lt;/p&gt;


### Supported Rules and Integrations

Below are the current rules and integrations currently contained within this repo. Integrations, such as Office365, Trend Micro, etc. will have scripts provided within their respective folders for use. Feel free to build upon these scripts and contribute back :smile:

* [Sysmon for Windows](https://github.com/socfortress/Wazuh-Rules/tree/main/Windows_Sysmon)
* [Sysmon for Linux](https://github.com/socfortress/Wazuh-Rules/tree/main/Sysmon%20Linux)
* [Office365](https://github.com/socfortress/Wazuh-Rules/tree/main/Office%20365)
* [Microsoft Defender](https://github.com/socfortress/Wazuh-Rules/tree/main/Office%20Defender)
* [Sophos](https://github.com/socfortress/Wazuh-Rules/tree/main/Sophos)
* [MISP](https://github.com/socfortress/Wazuh-Rules/tree/main/MISP)
* [Osquery](https://github.com/socfortress/Wazuh-Rules/tree/main/Osquery)
* [Yara](https://github.com/socfortress/Wazuh-Rules/tree/main/Yara)
* [Suricata](https://github.com/socfortress/Wazuh-Rules/tree/main/Suricata)
* [Packetbeat](https://github.com/socfortress/Wazuh-Rules/tree/main/Packetbeat)
* [Falco](https://github.com/socfortress/Wazuh-Rules/tree/main/Falco)
* [Modsecurity](https://github.com/socfortress/Wazuh-Rules/tree/main/Modsecurity)
* [F-Secure](https://github.com/socfortress/Wazuh-Rules/tree/main/F-Secure)
* [Domain Stats](https://github.com/socfortress/Wazuh-Rules/tree/main/Domain%20Stats)
* [Snyk](https://github.com/socfortress/Wazuh-Rules/tree/main/Snyk)
* [Autoruns](https://github.com/socfortress/Wazuh-Rules/tree/main/Windows%20Autoruns)
* [Sigcheck](https://github.com/socfortress/Wazuh-Rules/tree/main/Windows%20Sysinternals%20Sigcheck)
* [Powershell](https://github.com/socfortress/Wazuh-Rules/tree/main/Windows%20Powershell)
* [Crowdstrike](https://github.com/socfortress/Wazuh-Rules/tree/main/Crowdstrike)
* [Alienvault](https://github.com/socfortress/Wazuh-Rules/tree/main/Domain%20Stats)
* Tessian - WIP

### Roadmap

Have an Integration already configured that you&#039;d like to share? Or have an idea for an Integration that you would like help on? Feel free to add it to the Roadmap.
- [ ] Feel free to bring ideas :smile:

&lt;p align=&quot;right&quot;&gt;(&lt;a href=&quot;#readme-top&quot;&gt;back to top&lt;/a&gt;)&lt;/p&gt;

&lt;!-- GETTING STARTED --&gt;
## Getting Started

Feel free to implement all of the rules that are contained within this repo, or pick and choose as you see fit. See our Installation section below for a bash script that can be ran on your Wazuh Manager to quickly put these rules to work!

### Prerequisites

Wazuh-Manager Version 4.x Required.

[Wazuh Install Docs](https://documentation.wazuh.com/current/index.html)

[Need Assitance? - Hire SOCFortress](https://www.socfortress.co/contact_form.html)

### Installation

_You can either manually download the .xml rule files onto your Wazuh Manager or make use of our wazuh_socfortress_rules.sh script_

&gt; :warning: **USE AT OWN RISK**: If you already have custom rules built out, there is a good chance duplicate Rule IDs will exists. This will casue the Wazuh-Manager service to fail! Ensure there are no conflicting Rule IDs and your custom rules are backed up prior to running the wazuh_socfortress_rules.sh script!


1. Become Root User
2. Run the Script
   ```sh
   curl -so ~/wazuh_socfortress_rules.sh https://raw.githubusercontent.com/socfortress/Wazuh-Rules/main/wazuh_socfortress_rules.sh &amp;&amp; bash ~/wazuh_socfortress_rules.sh
   ```

![Alt Text](https://github.com/socfortress/Wazuh-Rules/blob/main/images/run%20install.gif)

&lt;p align=&quot;right&quot;&gt;(&lt;a href=&quot;#readme-top&quot;&gt;back to top&lt;/a&gt;)&lt;/p&gt;

&lt;!-- CONTRIBUTING --&gt;
## Contributing

Contributions are what make the open source community such an amazing place to learn, inspire, and create. Any contributions you make are **greatly appreciated**.

If you have a suggestion that would make this better, please fork the repo and create a pull request. You can also simply open an issue with the tag &quot;enhancement&quot;.
Don&#039;t forget to give the project a star! Thanks again!

1. Fork the Project
2. Create your Feature Branch (`git checkout -b ruleCategory/DetectionRule`)
3. Commit your Changes (`git commit -m &#039;Add some DetectionRules&#039;`)
4. Push to the Branch (`git push origin ruleCategory/DetectionRule`)
5. Open a Pull Request

&lt;p align=&quot;right&quot;&gt;(&lt;a href=&quot;#readme-top&quot;&gt;back to top&lt;/a&gt;)&lt;/p&gt;

&lt;!-- CONTACT --&gt;
## Contact

SOCFortress - [![LinkedIn][linkedin-shield]][linkedin-url] - info@socfortress.co

&lt;div align=&quot;center&quot;&gt;
  &lt;h2 align=&quot;center&quot;&gt;Let SOCFortress Take Your Open Source SIEM to the Next Level&lt;/h3&gt;
  &lt;a href=&quot;https://www.socfortress.co/contact_form.html&quot;&gt;
    &lt;img src=&quot;images/Email%20Banner.png&quot; alt=&quot;Banner&quot;&gt;
  &lt;/a&gt;


&lt;/div&gt;

&lt;p align=&quot;right&quot;&gt;(&lt;a href=&quot;#readme-top&quot;&gt;back to top&lt;/a&gt;)&lt;/p&gt;

&lt;!-- ACKNOWLEDGMENTS --&gt;
## Acknowledgments

Security is best when we work together! Huge thank you to those supporting and those future supporters!

* [Wazuh Team](https://documentation.wazuh.com/current/index.html)
* [Taylor Walton](https://www.youtube.com/channel/UC4EUQtTxeC8wGrKRafI6pZg)
* [Juan Romero](https://github.com/juaromu)

&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt;
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;
[contributors-shield]: https://img.shields.io/github/contributors/socfortress/Wazuh-Rules
[contributors-url]: https://github.com/socfortress/Wazuh-Rules/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/socfortress/Wazuh-Rules
[forks-url]: https://github.com/socfortress/Wazuh-Rules/network/members
[stars-shield]: https://img.shields.io/github/stars/socfortress/Wazuh-Rules
[stars-url]: https://github.com/socfortress/Wazuh-Rules/stargazers
[issues-shield]: https://img.shields.io/github/issues/othneildrew/Best-README-Template.svg?style=for-the-badge
[issues-url]: https://github.com/othneildrew/Best-README-Template/issues
[license-shield]: https://img.shields.io/badge/Help%20Desk-Help%20Desk-blue
[license-url]: https://socfortress.supportbench.net
[linkedin-shield]: https://img.shields.io/badge/Visit%20Us-www.socfortress.co-orange
[linkedin-url]: https://www.socfortress.co/
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ansible/ansible]]></title>
            <link>https://github.com/ansible/ansible</link>
            <guid>https://github.com/ansible/ansible</guid>
            <pubDate>Fri, 05 Sep 2025 00:04:06 GMT</pubDate>
            <description><![CDATA[Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain. Automate everything from code deployment to network configuration to cloud management, in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ansible/ansible">ansible/ansible</a></h1>
            <p>Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain. Automate everything from code deployment to network configuration to cloud management, in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com.</p>
            <p>Language: Python</p>
            <p>Stars: 66,247</p>
            <p>Forks: 24,073</p>
            <p>Stars today: 48 stars today</p>
            <h2>README</h2><pre>[![PyPI version](https://img.shields.io/pypi/v/ansible-core.svg)](https://pypi.org/project/ansible-core)
[![Docs badge](https://img.shields.io/badge/docs-latest-brightgreen.svg)](https://docs.ansible.com/ansible/latest/)
[![Chat badge](https://img.shields.io/badge/chat-IRC-brightgreen.svg)](https://docs.ansible.com/ansible/devel/community/communication.html)
[![Build Status](https://dev.azure.com/ansible/ansible/_apis/build/status/CI?branchName=devel)](https://dev.azure.com/ansible/ansible/_build/latest?definitionId=20&amp;branchName=devel)
[![Ansible Code of Conduct](https://img.shields.io/badge/code%20of%20conduct-Ansible-silver.svg)](https://docs.ansible.com/ansible/devel/community/code_of_conduct.html)
[![Ansible mailing lists](https://img.shields.io/badge/mailing%20lists-Ansible-orange.svg)](https://docs.ansible.com/ansible/devel/community/communication.html#mailing-list-information)
[![Repository License](https://img.shields.io/badge/license-GPL%20v3.0-brightgreen.svg)](COPYING)
[![Ansible CII Best Practices certification](https://bestpractices.coreinfrastructure.org/projects/2372/badge)](https://bestpractices.coreinfrastructure.org/projects/2372)

# Ansible

Ansible is a radically simple IT automation system. It handles
configuration management, application deployment, cloud provisioning,
ad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complex
changes like zero-downtime rolling updates with load balancers easy. More information on the Ansible [website](https://ansible.com/).

## Design Principles

* Have an extremely simple setup process with a minimal learning curve.
* Manage machines quickly and in parallel.
* Avoid custom-agents and additional open ports, be agentless by
  leveraging the existing SSH daemon.
* Describe infrastructure in a language that is both machine and human
  friendly.
* Focus on security and easy auditability/review/rewriting of content.
* Manage new remote machines instantly, without bootstrapping any
  software.
* Allow module development in any dynamic language, not just Python.
* Be usable as non-root.
* Be the easiest IT automation system to use, ever.

## Use Ansible

You can install a released version of Ansible with `pip` or a package manager. See our
[installation guide](https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html) for details on installing Ansible
on a variety of platforms.

Power users and developers can run the `devel` branch, which has the latest
features and fixes, directly. Although it is reasonably stable, you are more likely to encounter
breaking changes when running the `devel` branch. We recommend getting involved
in the Ansible community if you want to run the `devel` branch.

## Communication

Join the Ansible forum to ask questions, get help, and interact with the
community.

* [Get Help](https://forum.ansible.com/c/help/6): Find help or share your Ansible knowledge to help others.
  Use tags to filter and subscribe to posts, such as the following:
  * Posts tagged with [ansible](https://forum.ansible.com/tag/ansible)
  * Posts tagged with [ansible-core](https://forum.ansible.com/tag/ansible-core)
  * Posts tagged with [playbook](https://forum.ansible.com/tag/playbook)
* [Social Spaces](https://forum.ansible.com/c/chat/4): Meet and interact with fellow enthusiasts.
* [News &amp; Announcements](https://forum.ansible.com/c/news/5): Track project-wide announcements including social events.
* [Bullhorn newsletter](https://docs.ansible.com/ansible/devel/community/communication.html#the-bullhorn): Get release announcements and important changes.

For more ways to get in touch, see [Communicating with the Ansible community](https://docs.ansible.com/ansible/devel/community/communication.html).

## Contribute to Ansible

* Check out the [Contributor&#039;s Guide](./.github/CONTRIBUTING.md).
* Read [Community Information](https://docs.ansible.com/ansible/devel/community) for all
  kinds of ways to contribute to and interact with the project,
  including how to submit bug reports and code to Ansible.
* Submit a proposed code update through a pull request to the `devel` branch.
* Talk to us before making larger changes
  to avoid duplicate efforts. This not only helps everyone
  know what is going on, but it also helps save time and effort if we decide
  some changes are needed.

## Coding Guidelines

We document our Coding Guidelines in the [Developer Guide](https://docs.ansible.com/ansible/devel/dev_guide/). We particularly suggest you review:

* [Contributing your module to Ansible](https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_checklist.html)
* [Conventions, tips, and pitfalls](https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_best_practices.html)

## Branch Info

* The `devel` branch corresponds to the release actively under development.
* The `stable-2.X` branches correspond to stable releases.
* Create a branch based on `devel` and set up a [dev environment](https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_general.html#common-environment-setup) if you want to open a PR.
* See the [Ansible release and maintenance](https://docs.ansible.com/ansible/devel/reference_appendices/release_and_maintenance.html) page for information about active branches.

## Roadmap

Based on team and community feedback, an initial roadmap will be published for a major or minor version (ex: 2.7, 2.8).
The [Ansible Roadmap page](https://docs.ansible.com/ansible/devel/roadmap/) details what is planned and how to influence the roadmap.

## Authors

Ansible was created by [Michael DeHaan](https://github.com/mpdehaan)
and has contributions from over 5000 users (and growing). Thanks everyone!

[Ansible](https://www.ansible.com) is sponsored by [Red Hat, Inc.](https://www.redhat.com)

## License

GNU General Public License v3.0 or later

See [COPYING](COPYING) to see the full text.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[oraios/serena]]></title>
            <link>https://github.com/oraios/serena</link>
            <guid>https://github.com/oraios/serena</guid>
            <pubDate>Fri, 05 Sep 2025 00:04:05 GMT</pubDate>
            <description><![CDATA[A powerful coding agent toolkit providing semantic retrieval and editing capabilities (MCP server & other integrations)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/oraios/serena">oraios/serena</a></h1>
            <p>A powerful coding agent toolkit providing semantic retrieval and editing capabilities (MCP server & other integrations)</p>
            <p>Language: Python</p>
            <p>Stars: 11,420</p>
            <p>Forks: 800</p>
            <p>Stars today: 295 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot; style=&quot;text-align:center&quot;&gt;
  &lt;img src=&quot;resources/serena-logo.svg#gh-light-mode-only&quot; style=&quot;width:500px&quot;&gt;
  &lt;img src=&quot;resources/serena-logo-dark-mode.svg#gh-dark-mode-only&quot; style=&quot;width:500px&quot;&gt;
&lt;/p&gt;

* :rocket: Serena is a powerful **coding agent toolkit** capable of turning an LLM into a fully-featured agent that works **directly on your codebase**.
  Unlike most other tools, it is not tied to an LLM, framework or an interface, making it easy to use it in a variety of ways.
* :wrench: Serena provides essential **semantic code retrieval and editing tools** that are akin to an IDE&#039;s capabilities, extracting code entities at the symbol level and exploiting relational structure. When combined with an existing coding agent, these tools greatly enhance (token) efficiency.
* :free: Serena is **free &amp; open-source**, enhancing the capabilities of LLMs you already have access to free of charge.

You can think of Serena as providing IDE-like tools to your LLM/coding agent. With it, the agent no longer needs to read entire
files, perform grep-like searches or string replacements to find and edit the right code. Instead, it can use code centered tools like `find_symbol`, `find_referencing_symbols` and `insert_after_symbol`.

&lt;p align=&quot;center&quot;&gt;
  &lt;em&gt;Serena is under active development! See the latest updates, upcoming features, and lessons learned to stay up to date.&lt;/em&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;CHANGELOG.md&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Updates-1e293b?style=flat&amp;logo=rss&amp;logoColor=white&amp;labelColor=1e293b&quot; alt=&quot;Changelog&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;roadmap.md&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Roadmap-14532d?style=flat&amp;logo=target&amp;logoColor=white&amp;labelColor=14532d&quot; alt=&quot;Roadmap&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;lessons_learned.md&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Lessons-Learned-7c4700?style=flat&amp;logo=readthedocs&amp;logoColor=white&amp;labelColor=7c4700&quot; alt=&quot;Lessons Learned&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

### LLM Integration

Serena provides the necessary [tools](#list-of-tools) for coding workflows, but an LLM is required to do the actual work,
orchestrating tool use.

For example, **supercharge the performance of Claude Code** with a [one-line shell command](#claude-code).

In general, Serena can be integrated with an LLM in several ways:

* by using the **model context protocol (MCP)**.
  Serena provides an MCP server which integrates with
    * Claude Code and Claude Desktop,
    * Terminal-based clients like Codex, Gemini-CLI, Qwen3-Coder, rovodev, OpenHands CLI and others,
    * IDEs like VSCode, Cursor or IntelliJ,
    * Extensions like Cline or Roo Code
    * Local clients like [OpenWebUI](https://docs.openwebui.com/openapi-servers/mcp), [Jan](https://jan.ai/docs/mcp-examples/browser/browserbase#enable-mcp), [Agno](https://docs.agno.com/introduction/playground) and others
* by using [mcpo to connect it to ChatGPT](docs/serena_on_chatgpt.md) or other clients that don&#039;t support MCP but do support tool calling via OpenAPI.
* by incorporating Serena&#039;s tools into an agent framework of your choice, as illustrated [here](docs/custom_agent.md).
  Serena&#039;s tool implementation is decoupled from the framework-specific code and can thus easily be adapted to any agent framework.

### Serena in Action

#### Demonstration 1: Efficient Operation in Claude Code

A demonstration of Serena efficiently retrieving and editing code within Claude Code, thereby saving tokens and time. Efficient operations are not only useful for saving costs, but also for generally improving the generated code&#039;s quality. This effect may be less pronounced in very small projects, but often becomes of crucial importance in larger ones.

https://github.com/user-attachments/assets/ab78ebe0-f77d-43cc-879a-cc399efefd87

#### Demonstration 2: Serena in Claude Desktop

A demonstration of Serena implementing a small feature for itself (a better log GUI) with Claude Desktop.
Note how Serena&#039;s tools enable Claude to find and edit the right symbols.

https://github.com/user-attachments/assets/6eaa9aa1-610d-4723-a2d6-bf1e487ba753

### Programming Language Support &amp; Semantic Analysis Capabilities

Serena&#039;s semantic code analysis capabilities build on **language servers** using the widely implemented
language server protocol (LSP). The LSP provides a set of versatile code querying
and editing functionalities based on symbolic understanding of the code.
Equipped with these capabilities, Serena discovers and edits code just like a seasoned developer
making use of an IDE&#039;s capabilities would.
Serena can efficiently find the right context and do the right thing even in very large and
complex projects! So not only is it free and open-source, it frequently achieves better results
than existing solutions that charge a premium.

Language servers provide support for a wide range of programming languages.
With Serena, we provide direct, out-of-the-box support for:

  * Python
  * TypeScript/Javascript
  * PHP (uses Intelephense LSP; set `INTELEPHENSE_LICENSE_KEY` environment variable for premium features)
  * Go (requires installation of gopls)
  * R (requires installation of the `languageserver` R package)
  * Rust (requires [rustup](https://rustup.rs/) - uses rust-analyzer from your toolchain)
  * C/C++ (you may experience issues with finding references, we are working on it)
  * Zig (requires installation of ZLS - Zig Language Server)
  * C#
  * Ruby (by default, uses [ruby-lsp](https://github.com/Shopify/ruby-lsp), specify ruby_solargraph as your language to use the previous solargraph based implementation)
  * Swift
  * Kotlin (uses the pre-alpha [official kotlin LS](https://github.com/Kotlin/kotlin-lsp), some issues may appear)
  * Java (_Note_: startup is slow, initial startup especially so. There may be issues with java on macos and linux, we are working on it.)
  * Clojure
  * Dart
  * Bash
  * Lua (automatically downloads lua-language-server if not installed)
  * Nix (requires nixd installation)
  * Elixir (requires installation of NextLS and Elixir; **Windows not supported**)
  * Erlang (requires installation of beam and [erlang_ls](https://github.com/erlang-ls/erlang_ls), experimental, might be slow or hang)
  * AL

Support for further languages can easily be added by providing a shallow adapter for a new language server implementation,
see Serena&#039;s [memory on that](.serena/memories/adding_new_language_support_guide.md).

### Community Feedback

Most users report that Serena has strong positive effects on the results of their coding agents, even when used within
very capable agents like Claude Code. Serena is often described to be a [game changer](https://www.reddit.com/r/ClaudeAI/comments/1lfsdll/try_out_serena_mcp_thank_me_later/), providing an enormous [productivity boost](https://www.reddit.com/r/ClaudeCode/comments/1mguoia/absolutely_insane_improvement_of_claude_code).

Serena excels at navigating and manipulating complex codebases, providing tools that support precise code retrieval and editing in the presence of large, strongly structured codebases.
However, when dealing with tasks that involve only very few/small files, you may not benefit from including Serena on top of your existing coding agent. 
In particular, when writing code from scratch, Serena will not provide much value initially, as the more complex structures that Serena handles more gracefully than simplistic, file-based approaches are yet to be created.

Several videos and blog posts have talked about Serena:

* YouTube:
    * [AI Labs](https://www.youtube.com/watch?v=wYWyJNs1HVk&amp;t=1s)
    * [Yo Van Eyck](https://www.youtube.com/watch?v=UqfxuQKuMo8&amp;t=45s)
    * [JeredBlu](https://www.youtube.com/watch?v=fzPnM3ySmjE&amp;t=32s)

* Blog posts:
    * [Serena&#039;s Design Principles](https://medium.com/@souradip1000/deconstructing-serenas-mcp-powered-semantic-code-understanding-architecture-75802515d116)
    * [Serena with Claude Code (in Japanese)](https://blog.lai.so/serena/)
    * [Turning Claude Code into a Development Powerhouse](https://robertmarshall.dev/blog/turning-claude-code-into-a-development-powerhouse/)

## Table of Contents

&lt;!-- Created with markdown-toc -i README.md --&gt;
&lt;!-- Install it with npm install -g markdown-toc --&gt;

&lt;!-- toc --&gt;

- [Quick Start](#quick-start)
  * [Running the Serena MCP Server](#running-the-serena-mcp-server)
    + [Usage](#usage)
      - [Using uvx](#using-uvx)
        * [Local Installation](#local-installation)
      - [Using Docker (Experimental)](#using-docker-experimental)
    + [SSE Mode](#sse-mode)
    + [Command-Line Arguments](#command-line-arguments)
  * [Configuration](#configuration)
  * [Project Activation &amp; Indexing](#project-activation--indexing)
  * [Claude Code](#claude-code)
  * [Codex](#codex)
  * [Other Terminal-Based Clients](#other-terminal-based-clients)
  * [Claude Desktop](#claude-desktop)
  * [MCP Coding Clients (Cline, Roo-Code, Cursor, Windsurf, etc.)](#mcp-coding-clients-cline-roo-code-cursor-windsurf-etc)
  * [Local GUIs and Frameworks](#local-guis-and-frameworks)
- [Detailed Usage and Recommendations](#detailed-usage-and-recommendations)
  * [Tool Execution](#tool-execution)
    + [Shell Execution and Editing Tools](#shell-execution-and-editing-tools)
  * [Modes and Contexts](#modes-and-contexts)
    + [Contexts](#contexts)
    + [Modes](#modes)
    + [Customization](#customization)
  * [Onboarding and Memories](#onboarding-and-memories)
  * [Prepare Your Project](#prepare-your-project)
    + [Structure Your Codebase](#structure-your-codebase)
    + [Start from a Clean State](#start-from-a-clean-state)
    + [Logging, Linting, and Automated Tests](#logging-linting-and-automated-tests)
  * [Prompting Strategies](#prompting-strategies)
  * [Potential Issues in Code Editing](#potential-issues-in-code-editing)
  * [Running Out of Context](#running-out-of-context)
  * [Combining Serena with Other MCP Servers](#combining-serena-with-other-mcp-servers)
  * [Serena&#039;s Logs: The Dashboard and GUI Tool](#serenas-logs-the-dashboard-and-gui-tool)
  * [Troubleshooting](#troubleshooting)
- [Comparison with Other Coding Agents](#comparison-with-other-coding-agents)
  * [Subscription-Based Coding Agents](#subscription-based-coding-agents)
  * [API-Based Coding Agents](#api-based-coding-agents)
  * [Other MCP-Based Coding Agents](#other-mcp-based-coding-agents)
- [Acknowledgements](#acknowledgements)
- [Customizing and Extending Serena](#customizing-and-extending-serena)
- [List of Tools](#list-of-tools)

&lt;!-- tocstop --&gt;

## Quick Start

Serena can be used in various ways, below you will find instructions for selected integrations.

* For coding with Claude, we recommend using Serena through [Claude Code](#claude-code) or [Claude Desktop](#claude-desktop). You can also use Serena in most other [terminal-based clients](#other-terminal-based-clients).
* If you want a GUI experience outside an IDE, you can use one of the many [local GUIs](#local-guis-and-frameworks) that support MCP servers.
  You can also connect Serena to many web clients (including ChatGPT) using [mcpo](docs/serena_on_chatgpt.md).
* If you want to use Serena integrated in your IDE, see the section on [other MCP clients](#other-mcp-clients---cline-roo-code-cursor-windsurf-etc).
* You can use Serena as a library for building your own applications. We try to keep the public API stable, but you should still
  expect breaking changes and pin Serena to a fixed version if you use it as a dependency.

Serena is managed by `uv`, so you will need to [install it](https://docs.astral.sh/uv/getting-started/installation/)).

### Running the Serena MCP Server

You have several options for running the MCP server, which are explained in the subsections below.

#### Usage

The typical usage involves the client (Claude Code, Claude Desktop, etc.) running
the MCP server as a subprocess (using stdio communication),
so the client needs to be provided with the command to run the MCP server.
(Alternatively, you can run the MCP server in SSE mode and tell your client
how to connect to it.)

Note that no matter how you run the MCP server, Serena will, by default, start a small web-based dashboard on localhost that will display logs and allow shutting down the
MCP server (since many clients fail to clean up processes correctly).
This and other settings can be adjusted in the [configuration](#configuration) and/or by providing [command-line arguments](#command-line-arguments).

##### Using uvx

`uvx` can be used to run the latest version of Serena directly from the repository, without an explicit local installation.

```shell
uvx --from git+https://github.com/oraios/serena serena start-mcp-server
```

Explore the CLI to see some of the customization options that serena provides (more info on them below).

###### Local Installation

1. Clone the repository and change into it.

   ```shell
   git clone https://github.com/oraios/serena
   cd serena
   ```

2. Optionally edit the configuration file in your home directory with

   ```shell
   uv run serena config edit
   ```

   If you just want the default config, you can skip this part, and a config file will be created when you first run Serena.
3. Run the server with `uv`:

   ```shell
   uv run serena start-mcp-server
   ```

   When running from outside the serena installation directory, be sure to pass it, i.e., use

   ```shell
    uv run --directory /abs/path/to/serena serena start-mcp-server
   ```

##### Using Docker (Experimental)

⚠️ Docker support is currently experimental with several limitations. Please read the [Docker documentation](DOCKER.md) for important caveats before using it.

You can run the Serena MCP server directly via docker as follows,
assuming that the projects you want to work on are all located in `/path/to/your/projects`:

```shell
docker run --rm -i --network host -v /path/to/your/projects:/workspaces/projects ghcr.io/oraios/serena:latest serena start-mcp-server --transport stdio
```

Replace `/path/to/your/projects` with the absolute path to your projects directory. The Docker approach provides:

* Better security isolation for shell command execution
* No need to install language servers and dependencies locally
* Consistent environment across different systems

Alternatively, use docker compose with the `compose.yml` file provided in the repository.

See the [Docker documentation](DOCKER.md) for detailed setup instructions, configuration options, and known limitations.

##### Using Nix

If you are using Nix and [have enabled the `nix-command` and `flakes` features](https://nixos.wiki/wiki/flakes), you can run Serena using the following command:

```bash
nix run github:oraios/serena -- start-mcp-server --transport stdio
```

You can also install Serena by referencing this repo (`github:oraios/serena`) and using it in your Nix flake. The package is exported as `serena`.

#### SSE Mode

ℹ️ Note that MCP servers which use stdio as a protocol are somewhat unusual as far as client/server architectures go, as the server
necessarily has to be started by the client in order for communication to take place via the server&#039;s standard input/output stream.
In other words, you do not need to start the server yourself. The client application (e.g. Claude Desktop) takes care of this and
therefore needs to be configured with a launch command.

When using instead the SSE mode, which uses HTTP-based communication, you control the server lifecycle yourself,
i.e. you start the server and provide the client with the URL to connect to it.

Simply provide `start-mcp-server` with the `--transport sse` option and optionally provide the port.
For example, to run the Serena MCP server in SSE mode on port 9121 using a local installation,
you would run this command from the Serena directory,

```shell
uv run serena start-mcp-server --transport sse --port 9121
```

and then configure your client to connect to `http://localhost:9121/sse`.

#### Command-Line Arguments

The Serena MCP server supports a wide range of additional command-line options, including the option to run in SSE mode
and to adapt Serena to various [contexts and modes of operation](#modes-and-contexts).

Run with parameter `--help` to get a list of available options.

### Configuration

Serena is very flexible in terms of configuration. While for most users, the default configurations will work,
you can fully adjust it to your needs by editing a few yaml files. You can disable tools, change Serena&#039;s instructions
(what we denote as the `system_prompt`), adjust the output of tools that just provide a prompt, and even adjust tool descriptions.

Serena is configured in four places:

1. The `serena_config.yml` for general settings that apply to all clients and projects.
   It is located in your user directory under `.serena/serena_config.yml`.
   If you do not explicitly create the file, it will be auto-generated when you first run Serena.
   You can edit it directly or use

   ```shell
   uvx --from git+https://github.com/oraios/serena serena config edit
   ```

   (or use the `--directory` command version).
2. In the arguments passed to the `start-mcp-server` in your client&#039;s config (see below),
   which will apply to all sessions started by the respective client. In particular, the [context](#contexts) parameter
   should be set appropriately for Serena to be best adjusted to existing tools and capabilities of your client.
   See for a detailed explanation. You can override all entries from the `serena_config.yml` through command line arguments.
3. In the `.serena/project.yml` file within your project. This will hold project-level configuration that is used whenever
   that project is activated. This file will be autogenerated when you first use Serena on that project, but you can also
   generate it explicitly with

   ```shell
   uvx --from git+https://github.com/oraios/serena serena project generate-yml
   ```

   (or use the `--directory` command version).
4. Through the context and modes. Explore the [modes and contexts](#modes-and-contexts) section for more details.

After the initial setup, continue with one of the sections below, depending on how you
want to use Serena.

### Project Activation &amp; Indexing

If you are mostly working with the same project, you can configure to always activate it at startup
by passing `--project &lt;path_or_name&gt;` to the `start-mcp-server` command in your client&#039;s MCP config.
This is especially useful for clients which configure MCP servers on a per-project basis, like Claude Code.

Otherwise, the recommended way is to just ask the LLM to activate a project by providing it an absolute path to, or,
in case the project was activated in the past, by its name. The default project name is the directory name.

* &quot;Activate the project /path/to/my_project&quot;
* &quot;Activate the project my_project&quot;

All projects that have been activated will be automatically added to your `serena_config.yml`, and for each
project, the file `.serena/project.yml` will be generated. You can adjust the latter, e.g., by changing the name
(which you refer to during the activation) or other options. Make sure to not have two different projects with the
same name.

ℹ️ For larger projects, we recommend that you index your project to accelerate Serena&#039;s tools; otherwise the first
tool application may be very slow.
To do so, run this from the project directory (or pass the path to the project as an argument):

```shell
uvx --from git+https://github.com/oraios/serena serena project index
```

(or use the `--directory` command version).

### Claude Code

Serena is a great way to make Claude Code both cheaper and more powerful!

From your project directory, add serena with a command like this,

```shell
claude mcp add serena -- &lt;serena-mcp-server&gt; --context ide-assistant --project $(pwd)
```

where `&lt;serena-mcp-server&gt;` is your way of [running the Serena MCP server](#running-the-serena-mcp-server).
For example, when using `uvx`, you would run

```shell
claude mcp add serena -- uvx --from

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NVIDIA-NeMo/NeMo]]></title>
            <link>https://github.com/NVIDIA-NeMo/NeMo</link>
            <guid>https://github.com/NVIDIA-NeMo/NeMo</guid>
            <pubDate>Fri, 05 Sep 2025 00:04:04 GMT</pubDate>
            <description><![CDATA[A scalable generative AI framework built for researchers and developers working on Large Language Models, Multimodal, and Speech AI (Automatic Speech Recognition and Text-to-Speech)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NVIDIA-NeMo/NeMo">NVIDIA-NeMo/NeMo</a></h1>
            <p>A scalable generative AI framework built for researchers and developers working on Large Language Models, Multimodal, and Speech AI (Automatic Speech Recognition and Text-to-Speech)</p>
            <p>Language: Python</p>
            <p>Stars: 15,610</p>
            <p>Forks: 3,084</p>
            <p>Stars today: 12 stars today</p>
            <h2>README</h2><pre>[![Project Status: Active -- The project has reached a stable, usable state and is being actively developed.](http://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active)
[![Documentation](https://readthedocs.com/projects/nvidia-nemo/badge/?version=main)](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/)
[![CodeQL](https://github.com/nvidia/nemo/actions/workflows/codeql.yml/badge.svg?branch=main&amp;event=push)](https://github.com/nvidia/nemo/actions/workflows/codeql.yml)
[![NeMo core license and license for collections in this repo](https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg)](https://github.com/NVIDIA/NeMo/blob/master/LICENSE)
[![Release version](https://badge.fury.io/py/nemo-toolkit.svg)](https://badge.fury.io/py/nemo-toolkit)
[![Python version](https://img.shields.io/pypi/pyversions/nemo-toolkit.svg)](https://badge.fury.io/py/nemo-toolkit)
[![PyPi total downloads](https://static.pepy.tech/personalized-badge/nemo-toolkit?period=total&amp;units=international_system&amp;left_color=grey&amp;right_color=brightgreen&amp;left_text=downloads)](https://pepy.tech/project/nemo-toolkit)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

# **NVIDIA NeMo Framework**

## Latest News

&lt;!-- markdownlint-disable --&gt;
&lt;details open&gt;
  &lt;summary&gt;&lt;b&gt;Pretrain and finetune :hugs:Hugging Face models via AutoModel&lt;/b&gt;&lt;/summary&gt;
      Nemo Framework&#039;s latest feature AutoModel enables broad support for :hugs:Hugging Face models, with 25.04 focusing on

  
- &lt;a href=https://huggingface.co/transformers/v3.5.1/model_doc/auto.html#automodelforcausallm&gt;AutoModelForCausalLM&lt;a&gt; in the &lt;a href=&quot;https://huggingface.co/models?pipeline_tag=text-generation&amp;sort=trending&quot;&gt;Text Generation&lt;a&gt; category
- &lt;a href=https://huggingface.co/docs/transformers/main/model_doc/auto#transformers.AutoModelForImageTextToText&gt;AutoModelForImageTextToText&lt;a&gt; in the &lt;a href=&quot;https://huggingface.co/models?pipeline_tag=image-text-to-text&amp;sort=trending&quot;&gt;Image-Text-to-Text&lt;a&gt; category

More Details in Blog: &lt;a href=https://developer.nvidia.com/blog/run-hugging-face-models-instantly-with-day-0-support-from-nvidia-nemo-framework&gt;Run Hugging Face Models Instantly with Day-0 Support from NVIDIA NeMo Framework&lt;a&gt;. Future releases will enable support for more model families such as Video Generation models.(2025-05-19)
&lt;/details&gt;

&lt;details open&gt;
  &lt;summary&gt;&lt;b&gt;Training on Blackwell using Nemo&lt;/b&gt;&lt;/summary&gt;
      NeMo Framework has added Blackwell support, with &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance_summary.html&gt;performance benchmarks on GB200 &amp; B200&lt;a&gt;. More optimizations to come in the upcoming releases.(2025-05-19)
&lt;/details&gt;

&lt;details open&gt;
  &lt;summary&gt;&lt;b&gt;Training Performance on GPU Tuning Guide&lt;/b&gt;&lt;/summary&gt;
      NeMo Framework has published &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance-guide.html&gt;a comprehensive guide for performance tuning to achieve optimal throughput&lt;a&gt;! (2025-05-19)
&lt;/details&gt;

&lt;details open&gt;
  &lt;summary&gt;&lt;b&gt;New Models Support&lt;/b&gt;&lt;/summary&gt;
      NeMo Framework has added support for latest community models - &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/vlms/llama4.html&gt;Llama 4&lt;a&gt;, &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/vision/diffusionmodels/flux.html&gt;Flux&lt;a&gt;, &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/llama_nemotron.html&gt;Llama Nemotron&lt;a&gt;, &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/hyena.html#&gt;Hyena &amp; Evo2&lt;a&gt;, &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/vlms/qwen2vl.html&gt;Qwen2-VL&lt;a&gt;, &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/qwen2.html&gt;Qwen2.5&lt;a&gt;, Gemma3, Qwen3-30B&amp;32B.(2025-05-19)
&lt;/details&gt;


&lt;details open&gt;
  &lt;summary&gt;&lt;b&gt;NeMo Framework 2.0&lt;/b&gt;&lt;/summary&gt;
      We&#039;ve released NeMo 2.0, an update on the NeMo Framework which prioritizes modularity and ease-of-use. Please refer to the &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/index.html&gt;NeMo Framework User Guide&lt;/a&gt; to get started.
&lt;/details&gt;
&lt;details open&gt;
  &lt;summary&gt;&lt;b&gt;New Cosmos World Foundation Models Support&lt;/b&gt;&lt;/summary&gt;
    &lt;details&gt; 
      &lt;summary&gt; &lt;a href=&quot;https://developer.nvidia.com/blog/advancing-physical-ai-with-nvidia-cosmos-world-foundation-model-platform&quot;&gt;Advancing Physical AI with NVIDIA Cosmos World Foundation Model Platform &lt;/a&gt; (2025-01-09) 
      &lt;/summary&gt; 
        The end-to-end NVIDIA Cosmos platform accelerates world model development for physical AI systems. Built on CUDA, Cosmos combines state-of-the-art world foundation models, video tokenizers, and AI-accelerated data processing pipelines. Developers can accelerate world model development by fine-tuning Cosmos world foundation models or building new ones from the ground up. These models create realistic synthetic videos of environments and interactions, providing a scalable foundation for training complex systems, from simulating humanoid robots performing advanced actions to developing end-to-end autonomous driving models. 
        &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/accelerate-custom-video-foundation-model-pipelines-with-new-nvidia-nemo-framework-capabilities/&quot;&gt;
          Accelerate Custom Video Foundation Model Pipelines with New NVIDIA NeMo Framework Capabilities
        &lt;/a&gt; (2025-01-07)
      &lt;/summary&gt;
        The NeMo Framework now supports training and customizing the &lt;a href=&quot;https://github.com/NVIDIA/Cosmos&quot;&gt;NVIDIA Cosmos&lt;/a&gt; collection of world foundation models. Cosmos leverages advanced text-to-world generation techniques to create fluid, coherent video content from natural language prompts.
        &lt;br&gt;&lt;br&gt;
        You can also now accelerate your video processing step using the &lt;a href=&quot;https://developer.nvidia.com/nemo-curator-video-processing-early-access&quot;&gt;NeMo Curator&lt;/a&gt; library, which provides optimized video processing and captioning features that can deliver up to 89x faster video processing when compared to an unoptimized CPU pipeline.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
&lt;/details&gt;
&lt;details open&gt;
  &lt;summary&gt;&lt;b&gt;Large Language Models and Multimodal Models&lt;/b&gt;&lt;/summary&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/state-of-the-art-multimodal-generative-ai-model-development-with-nvidia-nemo/&quot;&gt;
          State-of-the-Art Multimodal Generative AI Model Development with NVIDIA NeMo
        &lt;/a&gt; (2024-11-06)
      &lt;/summary&gt;
        NVIDIA recently announced significant enhancements to the NeMo platform, focusing on multimodal generative AI models. The update includes NeMo Curator and the Cosmos tokenizer, which streamline the data curation process and enhance the quality of visual data. These tools are designed to handle large-scale data efficiently, making it easier to develop high-quality AI models for various applications, including robotics and autonomous driving. The Cosmos tokenizers, in particular, efficiently map visual data into compact, semantic tokens, which is crucial for training large-scale generative models. The tokenizer is available now on the &lt;a href=http://github.com/NVIDIA/cosmos-tokenizer/NVIDIA/cosmos-tokenizer&gt;NVIDIA/cosmos-tokenizer&lt;/a&gt; GitHub repo and on &lt;a href=https://huggingface.co/nvidia/Cosmos-Tokenizer-CV8x8x8&gt;Hugging Face&lt;/a&gt;.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/llama/index.html#new-llama-3-1-support for more information/&quot;&gt;
        New Llama 3.1 Support
        &lt;/a&gt; (2024-07-23)
      &lt;/summary&gt;
        The NeMo Framework now supports training and customizing the Llama 3.1 collection of LLMs from Meta.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://aws.amazon.com/blogs/machine-learning/accelerate-your-generative-ai-distributed-training-workloads-with-the-nvidia-nemo-framework-on-amazon-eks/&quot;&gt;
          Accelerate your Generative AI Distributed Training Workloads with the NVIDIA NeMo Framework on Amazon EKS
        &lt;/a&gt; (2024-07-16)
      &lt;/summary&gt;
     NVIDIA NeMo Framework now runs distributed training workloads on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. For step-by-step instructions on creating an EKS cluster and running distributed training workloads with NeMo, see the GitHub repository &lt;a href=&quot;https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/2.nemo-launcher/EKS/&quot;&gt; here.&lt;/a&gt;
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-nemo-accelerates-llm-innovation-with-hybrid-state-space-model-support/&quot;&gt;
          NVIDIA NeMo Accelerates LLM Innovation with Hybrid State Space Model Support
        &lt;/a&gt; (2024/06/17)
      &lt;/summary&gt;
     NVIDIA NeMo and Megatron Core now support pre-training and fine-tuning of state space models (SSMs). NeMo also supports training models based on the Griffin architecture as described by Google DeepMind. 
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
      &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://huggingface.co/models?sort=trending&amp;search=nvidia%2Fnemotron-4-340B&quot;&gt;
          NVIDIA releases 340B base, instruct, and reward models pretrained on a total of 9T tokens.
        &lt;/a&gt; (2024-06-18)
      &lt;/summary&gt;
      See documentation and tutorials for SFT, PEFT, and PTQ with 
      &lt;a href=&quot;https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/nemotron/index.html&quot;&gt;
        Nemotron 340B 
      &lt;/a&gt;
      in the NeMo Framework User Guide.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-sets-new-generative-ai-performance-and-scale-records-in-mlperf-training-v4-0/&quot;&gt;
          NVIDIA sets new generative AI performance and scale records in MLPerf Training v4.0
        &lt;/a&gt; (2024/06/12)
      &lt;/summary&gt;
      Using NVIDIA NeMo Framework and NVIDIA Hopper GPUs NVIDIA was able to scale to 11,616 H100 GPUs and achieve near-linear performance scaling on LLM pretraining. 
      NVIDIA also achieved the highest LLM fine-tuning performance and raised the bar for text-to-image training.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
        &lt;summary&gt;
          &lt;a href=&quot;https://cloud.google.com/blog/products/compute/gke-and-nvidia-nemo-framework-to-train-generative-ai-models&quot;&gt;
            Accelerate your generative AI journey with NVIDIA NeMo Framework on GKE
          &lt;/a&gt; (2024/03/16)
        &lt;/summary&gt;
        An end-to-end walkthrough to train generative AI models on the Google Kubernetes Engine (GKE) using the NVIDIA NeMo Framework is available at https://github.com/GoogleCloudPlatform/nvidia-nemo-on-gke. 
        The walkthrough includes detailed instructions on how to set up a Google Cloud Project and pre-train a GPT model using the NeMo Framework.
        &lt;br&gt;&lt;br&gt;
      &lt;/details&gt;
&lt;/details&gt;
&lt;details open&gt;
  &lt;summary&gt;&lt;b&gt;Speech Recognition&lt;/b&gt;&lt;/summary&gt;
  &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/accelerating-leaderboard-topping-asr-models-10x-with-nvidia-nemo/&quot;&gt;
          Accelerating Leaderboard-Topping ASR Models 10x with NVIDIA NeMo
        &lt;/a&gt; (2024/09/24)
      &lt;/summary&gt;
      NVIDIA NeMo team released a number of inference optimizations for CTC, RNN-T, and TDT models that resulted in up to 10x inference speed-up. 
      These models now exceed an inverse real-time factor (RTFx) of 2,000, with some reaching RTFx of even 6,000.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/new-standard-for-speech-recognition-and-translation-from-the-nvidia-nemo-canary-model/&quot;&gt;
          New Standard for Speech Recognition and Translation from the NVIDIA NeMo Canary Model
        &lt;/a&gt; (2024/04/18)
      &lt;/summary&gt;
      The NeMo team just released Canary, a multilingual model that transcribes speech in English, Spanish, German, and French with punctuation and capitalization. 
      Canary also provides bi-directional translation, between English and the three other supported languages.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/pushing-the-boundaries-of-speech-recognition-with-nemo-parakeet-asr-models/&quot;&gt;
          Pushing the Boundaries of Speech Recognition with NVIDIA NeMo Parakeet ASR Models
        &lt;/a&gt; (2024/04/18)
      &lt;/summary&gt;
      NVIDIA NeMo, an end-to-end platform for the development of multimodal generative AI models at scale anywhere—on any cloud and on-premises—released the Parakeet family of automatic speech recognition (ASR) models. 
      These state-of-the-art ASR models, developed in collaboration with Suno.ai, transcribe spoken English with exceptional accuracy.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
  &lt;details&gt;
    &lt;summary&gt;
      &lt;a href=&quot;https://developer.nvidia.com/blog/turbocharge-asr-accuracy-and-speed-with-nvidia-nemo-parakeet-tdt/&quot;&gt;
        Turbocharge ASR Accuracy and Speed with NVIDIA NeMo Parakeet-TDT
      &lt;/a&gt; (2024/04/18)
    &lt;/summary&gt;
    NVIDIA NeMo, an end-to-end platform for developing multimodal generative AI models at scale anywhere—on any cloud and on-premises—recently released Parakeet-TDT. 
    This new addition to the  NeMo ASR Parakeet model family boasts better accuracy and 64% greater speed over the previously best model, Parakeet-RNNT-1.1B.
    &lt;br&gt;&lt;br&gt;
  &lt;/details&gt;
&lt;/details&gt;
&lt;!-- markdownlint-enable --&gt;

## Introduction

NVIDIA NeMo Framework is a scalable and cloud-native generative AI
framework built for researchers and PyTorch developers working on Large
Language Models (LLMs), Multimodal Models (MMs), Automatic Speech
Recognition (ASR), Text to Speech (TTS), and Computer Vision (CV)
domains. It is designed to help you efficiently create, customize, and
deploy new generative AI models by leveraging existing code and
pre-trained model checkpoints.

For technical documentation, please see the [NeMo Framework User
Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html).

## What&#039;s New in NeMo 2.0

NVIDIA NeMo 2.0 introduces several significant improvements over its predecessor, NeMo 1.0, enhancing flexibility, performance, and scalability.

- **Python-Based Configuration** - NeMo 2.0 transitions from YAML files to a Python-based configuration, providing more flexibility and control. This shift makes it easier to extend and customize configurations programmatically.

- **Modular Abstractions** - By adopting PyTorch Lightning’s modular abstractions, NeMo 2.0 simplifies adaptation and experimentation. This modular approach allows developers to more easily modify and experiment with different components of their models.

- **Scalability** - NeMo 2.0 seamlessly scaling large-scale experiments across thousands of GPUs using [NeMo-Run](https://github.com/NVIDIA/NeMo-Run), a powerful tool designed to streamline the configuration, execution, and management of machine learning experiments across computing environments.

Overall, these enhancements make NeMo 2.0 a powerful, scalable, and user-friendly framework for AI model development.

&gt; [!IMPORTANT]  
&gt; NeMo 2.0 is currently supported by the LLM (large language model) and VLM (vision language model) collections.

### Get Started with NeMo 2.0

- Refer to the [Quickstart](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/quickstart.html) for examples of using NeMo-Run to launch NeMo 2.0 experiments locally and on a slurm cluster.
- For more information about NeMo 2.0, see the [NeMo Framework User Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/index.html).
- [NeMo 2.0 Recipes](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/llm/recipes) contains additional examples of launching large-scale runs using NeMo 2.0 and NeMo-Run.
- For an in-depth exploration of the main features of NeMo 2.0, see the [Feature Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/features/index.html#feature-guide).
- To transition from NeMo 1.0 to 2.0, see the [Migration Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/migration/index.html#migration-guide) for step-by-step instructions.

### Get Started with Cosmos

NeMo Curator and NeMo Framework support video curation and post-training of the Cosmos World Foundation Models, which are open and available on [NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/cosmos/collections/cosmos) and [Hugging Face](https://huggingface.co/collections/nvidia/cosmos-6751e884dc10e013a0a0d8e6). For more information on video datasets, refer to [NeMo Curator](https://developer.nvidia.com/nemo-curator). To post-train World Foundation Models using the NeMo Framework for your custom physical AI tasks, see the [Cosmos Diffusion models](https://github.com/NVIDIA/Cosmos/blob/main/cosmos1/models/diffusion/nemo/post_training/README.md) and the [Cosmos Autoregressive models](https://github.com/NVIDIA/Cosmos/blob/main/cosmos1/models/autoregressive/nemo/post_training/README.md).

## LLMs and MMs Training, Alignment, and Customization

All NeMo models are trained with
[Lightning](https://github.com/Lightning-AI/lightning). Training is
automatically scalable to 1000s of GPUs. You can check the performance benchmarks using the
latest NeMo Framework container [here](https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance_summary.html).

When applicable, NeMo models leverage cutting-edge distributed training
techniques, incorporating [parallelism
strategies](https://docs.nvidia.com/nemo-framework/user-guide/latest/modeloverview.html)
to enable efficient training of very large models. These techniques
include Tensor Parallelism (TP), Pipeline Parallelism (PP), Fully
Sharded Data Parallelism (FSDP), Mixture-of-Experts (MoE), and Mixed
Precision Training with BFloat16 and FP8, as well as others.

NeMo Transformer-based LLMs and MMs utilize [NVIDIA Transformer
Engine](https://github.com/NVIDIA/TransformerEngine) for FP8 training on
NVIDIA Hopper GPUs, while leveraging [NVIDIA Megatron
Core](https://github.com/NVIDIA/Megatron-LM/tree/main/megatron/core) for
scaling Transformer model training.

NeMo LLMs can be aligned with state-of-the-art methods such as SteerLM,
Direct Preference Optimization (DPO), and Reinforcement Learning from
Human Feedback (RLHF). See [NVIDIA NeMo
Aligner](https://github.com/NVIDIA/NeMo-Aligner) for more information.

In addition to supervised fine-tuning (SFT), NeMo also supports the
latest parameter efficient fine-tuning (PEFT) techniques such as LoRA,
P-Tuning, Adapters, and IA3. Refer to the [NeMo Framework User
Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/sft_peft/index.html)
for the full list of supported models and techniques.

## LLMs and MMs Deployment and Optimization

NeMo LLMs and MMs can be deployed and optimized with [NVIDIA NeMo
Microservices](https://developer.nvidia.com/nemo-microservices-early-access).

## Speech AI

NeMo ASR and TTS models can be optimized for inference and deployed for
production use cases with [NVIDIA Riva](https://developer.nvidia.com/riva).

## NeMo Framework Launcher

&gt; [!IMPORTANT]  
&gt; NeMo Framework Launcher is compatible with NeMo version 1.0 only. [NeMo-Run](https://github.com/NVIDIA/NeMo-Run) is recommended for launching experiments using NeMo 2.0.

[NeMo Framework
Launcher](https://github.com/NVIDIA/NeMo-Megatron-Launcher) is a
cloud-native tool that streamlines the NeMo Framework experience. It is
used for launching end-to-end NeMo Framework training jobs on CSPs and
Slurm clusters.

The NeMo Framework Launcher includes extensive recipes, scripts,
utilities, and documentation for training NeMo LLMs. It also includes
the NeMo Framework [Autoconfigurator](https://github.com/NVIDIA/NeMo-Megatron-Launcher#53

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[getsentry/sentry]]></title>
            <link>https://github.com/getsentry/sentry</link>
            <guid>https://github.com/getsentry/sentry</guid>
            <pubDate>Fri, 05 Sep 2025 00:04:03 GMT</pubDate>
            <description><![CDATA[Developer-first error tracking and performance monitoring]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/getsentry/sentry">getsentry/sentry</a></h1>
            <p>Developer-first error tracking and performance monitoring</p>
            <p>Language: Python</p>
            <p>Stars: 41,900</p>
            <p>Forks: 4,421</p>
            <p>Stars today: 8 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://sentry.io/?utm_source=github&amp;utm_medium=logo&quot; target=&quot;_blank&quot;&gt;
      &lt;img src=&quot;https://sentry-brand.storage.googleapis.com/sentry-wordmark-dark-280x84.png&quot; alt=&quot;Sentry&quot; width=&quot;280&quot; height=&quot;84&quot; /&gt;
    &lt;/a&gt;
  &lt;/p&gt;
  &lt;p align=&quot;center&quot;&gt;
    Users and logs provide clues. Sentry provides answers.
  &lt;/p&gt;
&lt;/p&gt;

# What&#039;s Sentry?

Sentry is the debugging platform that helps every developer detect, trace, and fix issues. Code breaks, fix it faster.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/issue-details.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/seer.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/insights.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/traces.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/trace-explorer.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/replays.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/insights.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/logs.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/uptime.png&quot; width=&quot;270&quot; /&gt;
&lt;/p&gt;

## Official Sentry SDKs

- [JavaScript](https://github.com/getsentry/sentry-javascript)
- [Electron](https://github.com/getsentry/sentry-electron/)
- [React-Native](https://github.com/getsentry/sentry-react-native)
- [Python](https://github.com/getsentry/sentry-python)
- [Ruby](https://github.com/getsentry/sentry-ruby)
- [PHP](https://github.com/getsentry/sentry-php)
- [Laravel](https://github.com/getsentry/sentry-laravel)
- [Go](https://github.com/getsentry/sentry-go)
- [Rust](https://github.com/getsentry/sentry-rust)
- [Java/Kotlin](https://github.com/getsentry/sentry-java)
- [Objective-C/Swift](https://github.com/getsentry/sentry-cocoa)
- [C\#/F\#](https://github.com/getsentry/sentry-dotnet)
- [C/C++](https://github.com/getsentry/sentry-native)
- [Dart/Flutter](https://github.com/getsentry/sentry-dart)
- [Perl](https://github.com/getsentry/perl-raven)
- [Clojure](https://github.com/getsentry/sentry-clj/)
- [Elixir](https://github.com/getsentry/sentry-elixir)
- [Unity](https://github.com/getsentry/sentry-unity)
- [Unreal Engine](https://github.com/getsentry/sentry-unreal)
- [Godot Engine](https://github.com/getsentry/sentry-godot)
- [PowerShell](https://github.com/getsentry/sentry-powershell)

# Resources

- [Documentation](https://docs.sentry.io/)
- [Discussions](https://github.com/getsentry/sentry/discussions) (Bugs, feature requests,
  general questions)
- [Discord](https://discord.gg/PXa5Apfe7K)
- [Contributing](https://docs.sentry.io/internal/contributing/)
- [Bug Tracker](https://github.com/getsentry/sentry/issues)
- [Code](https://github.com/getsentry/sentry)
- [Transifex](https://explore.transifex.com/getsentry/sentry/) (Translate
  Sentry\!)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[slavakurilyak/awesome-ai-agents]]></title>
            <link>https://github.com/slavakurilyak/awesome-ai-agents</link>
            <guid>https://github.com/slavakurilyak/awesome-ai-agents</guid>
            <pubDate>Fri, 05 Sep 2025 00:04:02 GMT</pubDate>
            <description><![CDATA[Awesome list of 300+ agentic AI resources]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/slavakurilyak/awesome-ai-agents">slavakurilyak/awesome-ai-agents</a></h1>
            <p>Awesome list of 300+ agentic AI resources</p>
            <p>Language: Python</p>
            <p>Stars: 1,026</p>
            <p>Forks: 142</p>
            <p>Stars today: 21 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/slavakurilyak/awesome-ai-agents/assets/6625584/f37cd4ef-84e7-424d-93d7-40a3086e3c95&quot; height=&quot;300&quot; alt=&quot;Awesome List of AI Agents&quot; /&gt;
&lt;/p&gt;
&lt;h1 align=&quot;center&quot;&gt;Awesome List of AI Agents&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;
    🔗 Follow Slava for more agentic AI resources
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://twitter.com/slavakurilyak&quot;&gt;🐦 Twitter&lt;/a&gt; | &lt;a href=&quot;https://linkedin.com/in/slavakurilyak&quot;&gt;💼 LinkedIn&lt;/a&gt; | &lt;a href=&quot;https://github.com/slavakurilyak&quot;&gt;🐙 Github&lt;/a&gt;
&lt;/p&gt;

This project tracks the latest agentic AI projects and provides a list of 200+ resources, curated by Slava Kurilyak

**Wait But Why**

AI agents outperform traditional LLMs

According to [Andrew Ng (@andrewyng)](https://github.com/andrewyng):

&gt; &quot;GPT-3.5 with an agentic workflow actually outperforms GPT-4&quot;

## 🏆 Top 10 Projects

&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/Significant-Gravitas/AutoGPT&quot;&gt;&lt;strong&gt;AutoGPT&lt;/strong&gt;&lt;/a&gt; - 177,350 stars (Updated: 2025-07-30)&lt;br&gt;AutoGPT provides accessible AI tools for building and using AI agents, offering a comprehensive framework including Forge for agent creation, agbenchmark for performance evaluation, a leaderboard for competition, a user-friendly UI, and CLI for seamless integration and management&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/ollama/ollama&quot;&gt;&lt;strong&gt;Ollama&lt;/strong&gt;&lt;/a&gt; - 147,994 stars (Updated: 2025-07-30)&lt;br&gt;Ollama is a tool for running large language models locally, offering easy setup for macOS, Windows, Linux, and Docker, along with a library of models and quickstart guides for customization and integration&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/langchain-ai/langchain&quot;&gt;&lt;strong&gt;LangChain&lt;/strong&gt;&lt;/a&gt; - 112,549 stars (Updated: 2025-07-30)&lt;br&gt;LangChain is a framework enabling context-aware reasoning applications with integrated libraries, templates, and developer tools&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/langchain-ai/langchain/&quot;&gt;&lt;strong&gt;LangChain Tools&lt;/strong&gt;&lt;/a&gt; - 112,549 stars (Updated: 2025-07-30)&lt;br&gt;Langchain integrates various providers like Anthropic, AWS, and OpenAI, and offers tools for components such as LLMs, chat models, and data analysis, supporting functionalities from Alpha Vantage to YouTube&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/lobehub/lobe-chat&quot;&gt;&lt;strong&gt;Lobe Chat&lt;/strong&gt;&lt;/a&gt; - 63,923 stars (Updated: 2025-07-30)&lt;br&gt;Lobe Chat is an open-source UI framework for building ChatGPT/LLM-based chat applications, featuring modern design, speech synthesis, multi-modal support, extensible plugins, and free one-click deployment for various AI agents&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/OpenDevin/OpenDevin&quot;&gt;&lt;strong&gt;OpenDevin&lt;/strong&gt;&lt;/a&gt; - 61,429 stars (Updated: 2025-07-30)&lt;br&gt;OpenDevin is an open-source initiative aimed at replicating and enhancing the autonomous AI software engineer Devin, focusing on collaboration and complex task execution in software development, emphasizing its relevance to advancing agentic AI technologies&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/OpenInterpreter/open-interpreter&quot;&gt;&lt;strong&gt;Open Interpreter&lt;/strong&gt;&lt;/a&gt; - 60,095 stars (Updated: 2025-07-30)&lt;br&gt;Open Interpreter is a coding agent enabling language models to execute code locally, facilitating natural-language interaction with your computer&#039;s capabilities, overcoming limitations of hosted solutions like internet access and package restrictions. It features interactive and programmatic chats, system message customization, and can control your computer&#039;s keyboard and mouse, allowing for enhanced control and flexibility in development environments&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/OpenInterpreter/open-interpreter&quot;&gt;&lt;strong&gt;Open Interpreter&lt;/strong&gt;&lt;/a&gt; - 60,095 stars (Updated: 2025-07-30)&lt;br&gt;Open Interpreter is a coding agent enabling language models to execute code locally, facilitating natural-language interaction with your computer&#039;s capabilities, overcoming limitations of hosted solutions like internet access and package restrictions. It features interactive and programmatic chats, system message customization, and can control your computer&#039;s keyboard and mouse, allowing for enhanced control and flexibility in development environments&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/geekan/MetaGPT&quot;&gt;&lt;strong&gt;MetaGPT&lt;/strong&gt;&lt;/a&gt; - 57,568 stars (Updated: 2025-07-30)&lt;br&gt;MetaGPT is a multi-agent framework enabling GPT to collaborate within a software company, facilitating complex tasks by assigning different roles to GPTs&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/zylon-ai/private-gpt/&quot;&gt;&lt;strong&gt;PrivateGPT&lt;/strong&gt;&lt;/a&gt; - 56,373 stars (Updated: 2025-07-30)&lt;br&gt;PrivateGPT is a secure, offline-capable AI tool for querying documents with Large Language Models, offering high-level and low-level APIs for privacy-conscious, context-aware application development&lt;/li&gt;
&lt;/ol&gt;

## 🚀 Rising 10 Projects (Recently Active, Excluding Top 10)

&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/gpt-engineer-org/gpt-engineer&quot;&gt;&lt;strong&gt;GPT Engineer&lt;/strong&gt;&lt;/a&gt; - 54,614 stars (Updated: 2025-07-30)&lt;br&gt;GPT-Engineer is an AI-powered tool allowing users to specify software in natural language, automatically generating and executing code, with options for improvement suggestions, and fostering collaboration within the open-source community&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/autogen&quot;&gt;&lt;strong&gt;AutoGen by Microsoft&lt;/strong&gt;&lt;/a&gt; - 48,077 stars (Updated: 2025-07-30)&lt;br&gt;AutoGen is a multi-agent conversation framework facilitating the development of next-gen LLM applications, highlighted by various accomplishments and offering enhanced LLM inferences, customizable agents, and comprehensive documentation&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/autogen&quot;&gt;&lt;strong&gt;AutoGen Studio by Microsoft&lt;/strong&gt;&lt;/a&gt; - 48,077 stars (Updated: 2025-07-30)&lt;br&gt;AutoGen Studio 2.0 is Microsoft&#039;s advanced AI development tool, offering a user-friendly interface, powerful Python API, and comprehensive features for creating and controlling AI agents and workflows&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/run-llama/llama_index&quot;&gt;&lt;strong&gt;LlamaIndex Tools&lt;/strong&gt;&lt;/a&gt; - 43,398 stars (Updated: 2025-07-30)&lt;br&gt;LlamaIndex offers a variety of tools for building data agents, with top downloads including IonicShoppingToolSpec, OpenAPIToolSpec, WikipediaToolSpec, GmailToolSpec, and GoogleCalendarToolSpec, enabling seamless integration with user-defined functions, query engines, and third-party services&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/FlowiseAI/Flowise&quot;&gt;&lt;strong&gt;Flowise&lt;/strong&gt;&lt;/a&gt; - 42,100 stars (Updated: 2025-07-30)&lt;br&gt;Flowise simplifies the creation of applications leveraging large language models (LLMs) by providing a drag-and-drop interface for customizing AI workflows, offering easy installation, Docker support, development tools, and documentation for integrating various functionalities such as authentication, streaming, and custom tools to enhance AI agents&#039; capabilities&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/lm-sys/FastChat&quot;&gt;&lt;strong&gt;FastChat&lt;/strong&gt;&lt;/a&gt; - 38,916 stars (Updated: 2025-07-30)&lt;br&gt;FastChat is a platform for training, serving, and evaluating large language model chatbots, featuring an open-source distributed multi-model system, API compatibility, and a dataset for LLM conversations&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/mem0ai/mem0&quot;&gt;&lt;strong&gt;mem0&lt;/strong&gt;&lt;/a&gt; - 37,574 stars (Updated: 2025-07-30)&lt;br&gt;Mem0 is an intelligent memory layer for Large Language Models that enhances personalized AI experiences by retaining and utilizing contextual information across various applications.&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/calcom/cal.com/tree/main/apps/ai&quot;&gt;&lt;strong&gt;Cal.ai&lt;/strong&gt;&lt;/a&gt; - 37,384 stars (Updated: 2025-07-30)&lt;br&gt;Cal.ai is an open-source AI scheduling assistant that manages email communications for booking, rearranging, and inquiring about meetings, leveraging a LangChain Agent Executor and MailParser for efficient scheduling without API key exposure&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/paul-gauthier/aider&quot;&gt;&lt;strong&gt;Aider&lt;/strong&gt;&lt;/a&gt; - 36,185 stars (Updated: 2025-07-30)&lt;br&gt;Aider is a command-line tool for AI-assisted pair programming, allowing code editing in local git repositories with GPT-3.5/GPT-4, featuring direct file edits, automatic git commits, and support for most popular programming languages&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/janhq/jan&quot;&gt;&lt;strong&gt;Jan&lt;/strong&gt;&lt;/a&gt; - 35,212 stars (Updated: 2025-07-30)&lt;br&gt;Jan is an open-source, development-stage ChatGPT alternative that operates fully offline on diverse hardware platforms, supporting universal architectures from PCs to multi-GPU clusters&lt;/li&gt;
&lt;/ol&gt;

## ✍️ All Projects

Here&#039;s an awesome list of AI agents:

### 01
&lt;div&gt;&lt;a href=&quot;https://github.com/OpenInterpreter/01&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Open%20Source-Yes-green&quot; alt=&quot;Open Source&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/OpenInterpreter/01&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/OpenInterpreter/01?style=social&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;⭐ 5,080 stars (Updated: 2025-07-30)&lt;/p&gt;
&lt;p&gt;⌚ Hardware (Wearables)&lt;/p&gt;

&lt;p&gt;The &#039;01 Project&#039; by Open Interpreter is an open-source initiative focused on creating an ecosystem for AI devices, aiming to become the GNU/Linux in this domain, with details on its experimental status, software, hardware, and a speech-to-speech interface based on a code-interpreting language model for dynamic interactions&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://changes.openinterpreter.com/log/introducing-the-01-developer-preview&quot;&gt;announcement&lt;/a&gt; | &lt;a href=&quot;https://twitter.com/OpenInterpreter/status/1770821439458840846&quot;&gt;demo&lt;/a&gt; | &lt;a href=&quot;https://github.com/OpenInterpreter/01&quot;&gt;github&lt;/a&gt; | &lt;a href=&quot;http://openinterpreter.com/01&quot;&gt;website&lt;/a&gt; | &lt;a href=&quot;https://01.openinterpreter.com/&quot;&gt;docs&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

### Adala
&lt;div&gt;&lt;a href=&quot;https://github.com/HumanSignal/Adala&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Open%20Source-Yes-green&quot; alt=&quot;Open Source&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/HumanSignal/Adala&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/HumanSignal/Adala?style=social&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;⭐ 1,228 stars (Updated: 2025-07-30)&lt;/p&gt;
&lt;p&gt;⚙️ Development Frameworks&lt;/p&gt;

&lt;p&gt;Adala is a framework for autonomous data labeling agents, supporting Python 3.8 to 3.11, with features for customizable, intelligent data processing and integration into Python Notebooks&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/HumanSignal/Adala&quot;&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

### Agency Swarm by VRSEN
&lt;div&gt;&lt;a href=&quot;https://github.com/VRSEN/agency-swarm&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Open%20Source-Yes-green&quot; alt=&quot;Open Source&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/VRSEN/agency-swarm&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/VRSEN/agency-swarm?style=social&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;⭐ 3,729 stars (Updated: 2025-07-30)&lt;/p&gt;
&lt;p&gt;⚙️ Development Frameworks&lt;/p&gt;

&lt;p&gt;Agency Swarm is a framework designed to automate AI agencies by creating a swarm of collaborative agents with customizable roles and functionalities, aiming to simplify the agent creation process and make automation more intuitive&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/VRSEN/agency-swarm&quot;&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

### Agent by Stately AI
&lt;div&gt;&lt;a href=&quot;https://github.com/statelyai/agent/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Open%20Source-Yes-green&quot; alt=&quot;Open Source&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/statelyai/agent/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/statelyai/agent?style=social&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;⭐ 299 stars (Updated: 2025-07-30)&lt;/p&gt;
&lt;p&gt;🤖 AI Agents&lt;/p&gt;

&lt;p&gt;Stately Agent is a software for building intelligent agents that interact via chat and events, with examples including joke generation, tic-tac-toe, and weather querying, requiring installation and an OpenAI API key&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/statelyai/agent/&quot;&gt;github&lt;/a&gt; | &lt;a href=&quot;https://stately.ai/agent&quot;&gt;website&lt;/a&gt; | &lt;a href=&quot;https://twitter.com/statelyai&quot;&gt;twitter&lt;/a&gt; | &lt;a href=&quot;https://discord.gg/xstate&quot;&gt;discord&lt;/a&gt; | &lt;a href=&quot;https://youtube.com/c/statelyai&quot;&gt;youtube&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

### Agent Protocol
&lt;div&gt;&lt;a href=&quot;https://github.com/AI-Engineer-Foundation/agent-protocol&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Open%20Source-No-red&quot; alt=&quot;Open Source&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/AI-Engineer-Foundation/agent-protocol&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/AI-Engineer-Foundation/agent-protocol?style=social&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;⭐ 1,427 stars (Updated: 2025-07-30)&lt;/p&gt;
&lt;p&gt;📏 Standardization&lt;/p&gt;

&lt;p&gt;The Agent Protocol establishes a unified API standard for seamless interaction and integration across diverse AI agents, promoting ecosystem growth and simplification of agent development and benchmarking&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.aie.foundation/&quot;&gt;website&lt;/a&gt; | &lt;a href=&quot;https://www.aie.foundation/&quot;&gt;website&lt;/a&gt; | &lt;a href=&quot;https://github.com/AI-Engineer-Foundation/agent-protocol&quot;&gt;github&lt;/a&gt; | &lt;a href=&quot;https://github.com/AI-Engineer-Foundation&quot;&gt;github profile&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

### Agent Tools
&lt;div&gt;&lt;a href=&quot;https://github.com/aibtcdev/agent-tools-ts&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Open%20Source-Yes-green&quot; alt=&quot;Open Source&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/aibtcdev/agent-tools-ts&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/aibtcdev/agent-tools-ts?style=social&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;⭐ 16 stars (Updated: 2025-07-30)&lt;/p&gt;
&lt;p&gt;₿ Bitcoin&lt;/p&gt;

&lt;p&gt;Typescript tools for Bitcoin/Stacks blockchain interaction, utilizing Bun.js and Stacks.js, with a focus on AI integration&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/aibtcdev/agent-tools-ts&quot;&gt;github&lt;/a&gt; | &lt;a href=&quot;https://github.com/aibtcdev&quot;&gt;github profile&lt;/a&gt; | &lt;a href=&quot;https://aibtc.dev/&quot;&gt;website&lt;/a&gt; | &lt;a href=&quot;https://discord.gg/5DJaBrf&quot;&gt;discord&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

### AgentBench
&lt;div&gt;&lt;a href=&quot;https://github.com/THUDM/AgentBench&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Open%20Source-Yes-green&quot; alt=&quot;Open Source&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/THUDM/AgentBench&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/THUDM/AgentBench?style=social&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;⭐ 2,711 stars (Updated: 2025-07-30)&lt;/p&gt;
&lt;p&gt;📊 Evaluation Frameworks&lt;/p&gt;

&lt;p&gt;AgentBench v0.2 is a benchmark designed to evaluate Large Language Models as agents across a diverse set of environments, enhancing framework usability and extending model evaluations&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/THUDM/AgentBench&quot;&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

### AgentGPT by Reworkd
&lt;div&gt;&lt;a href=&quot;https://github.com/reworkd/AgentGPT&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Open%20Source-Yes-green&quot; alt=&quot;Open Source&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/reworkd/AgentGPT&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/reworkd/AgentGPT?style=social&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;⭐ 34,613 stars (Updated: 2025-07-30)&lt;/p&gt;
&lt;p&gt;⚙️ Development Frameworks&lt;/p&gt;

&lt;p&gt;AgentGPT allows users to configure and deploy autonomous AI agents, enabling them to name their own custom AI and guide it towards any desired goal through task execution and learning&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/reworkd/AgentGPT&quot;&gt;github&lt;/a&gt; | &lt;a href=&quot;https://github.com/reworkd&quot;&gt;github profile&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

### Agentive
&lt;div&gt;&lt;a href=&quot;https://agentivehub.com/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Open%20Source-No-red&quot; alt=&quot;Open Source&quot;&gt;&lt;/a&gt;&lt;/div&gt;

&lt;p&gt;⚙️ Development Frameworks&lt;/p&gt;

&lt;p&gt;Agentive is a platform for AI Automation Agency owners, offering tools for creating, managing, and deploying custom AI solutions, with features like model selection, tool integration, prompt crafting, versioning, and training with own data, designed to simplify AI agent delivery&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://agentivehub.com/&quot;&gt;website&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

### AgentLabs
&lt;div&gt;&lt;a href=&quot;https://github.com/agentlabs-inc/agentlabs&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Open%20Source-Yes-green&quot; alt=&quot;Open Source&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/agentlabs-inc/agentlabs&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/agentlabs-inc/agentlabs?style=social&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;⭐ 498 stars (Updated: 2025-07-30)&lt;/p&gt;
&lt;p&gt;⚙️ Development Frameworks&lt;/p&gt;

&lt;p&gt;AgentLabs is an open-source, universal frontend solution for AI agents, offering an authentication portal, chat interface, analytics, and payment features to streamline the deployment of AI agents to public users&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/agentlabs-inc/agentlabs&quot;&gt;github&lt;/a&gt; | &lt;a href=&quot;https://www.agentlabs.dev/&quot;&gt;website&lt;/a&gt; | &lt;a href=&quot;https://docs.agentlabs.dev/&quot;&gt;docs&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

### AgentOps
&lt;div&gt;&lt;a href=&quot;https://github.com/AgentOps-AI/agentops&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Open%20Source-Yes-green&quot; alt=&quot;Open Source&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/AgentOps-AI/agentops&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/AgentOps-AI/agentops?style=social&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;⭐ 4,716 stars (Updated: 2025-07-30)&lt;/p&gt;
&lt;p&gt;📊 Evaluation Frameworks&lt;/p&gt;

&lt;p&gt;AgentOps aims to improve AI agent development with tools for observability, evaluations, and replay analytics, offering a streamlined process for testing and debugging compliant AI agents through a user-friendly interface and comprehensive documentation&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/AgentOps-AI/agentops&quot;&gt;github&lt;/a&gt; | &lt;a href=&quot;https://www.agentops.ai/&quot;&gt;website&lt;/a&gt; | &lt;a href=&quot;https://docs.agentops.ai&quot;&gt;docs&lt;/a&gt; | &lt;a href=&quot;https://discord.gg/mKW3ZhN9p2&quot;&gt;discord&lt;/a&gt; | &lt;a href=&quot;https://x.com/AlexReibman/status/1772771418780176674&quot;&gt;demo&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

### AgentOS
&lt;div&gt;&lt;a href=&quot;https://github.com/smartcomputer-ai/agent-os&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Open%20Source-Yes-green&quot; alt=&quot;Open Source&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/smartcomputer-ai/agent-os&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/smartcomputer-ai/agent-os?style=social&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;⭐ 125 stars (Updated: 2025-07-30)&lt;/p&gt;
&lt;p&gt;⚙️ Development Frameworks&lt;/p&gt;

&lt;p&gt;The Agent OS is an experimental platform for creating self-evolving, autonomous AI agents capable of writing and executing their own code, designed to be a long-term environment for such agents and supports various programming languages&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/smartcomputer-ai/agent-os&quot;&gt;github&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

### Agents by AI Waves
&lt;div&gt;&lt;a href=&quot;https://github.com/aiwaves-cn/agents&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Open%20Source-Yes-green&quot; alt=&quot;Open Source&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/aiwaves-cn/agents&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/aiwaves-cn/agents?style=social&quot; alt=&quot;GitHub stars&quot;&gt;&lt;/a&gt;&lt;/div&gt;
&lt;p&gt;⭐ 5,666 stars (Updated: 2025-07-30)&lt;/p&gt;
&lt;p&gt;⚙️ Development Frameworks&lt;/p&gt;

&lt;p&gt;Agents is an open-source framework for building autonomous language agents with features including long-short term memory, tool usage, web navigation, multi-agent communication, human-agent interaction, and symbolic control, allowing customization through natural language config files and deployment in various interfaces&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/aiwaves-cn/agents&quot;&gt;github&lt;/a&gt; | &lt;a href=&quot;https://github.com/aiwaves-cn&quot;&gt;github profile&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

### Agents by Hugging Face
&lt;div&gt;&lt;a href=&quot;https://huggingface.co/docs/transformers/main_classes/agent&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Open%20Source-No-red&quot; alt=&quot;Open Source&quot;&gt;&lt;/a&gt;&lt;/div&gt;

&lt;p&gt;⚙️ Development Frameworks&lt;/p&gt;

&lt;p&gt;Hugging Face&#039;s Transformers Agents provide three main types: HfAgent for inference with open-source models, LocalAgent for using local models and tokenizers, and OpenAiAgent for access to OpenAI&#039;s closed models, enabling code generation and other AI tasks with varying levels of customization and local or remote execution&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://huggingface.co/docs/transformers/main_classes/agent&quot;&gt;website&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

### Agentsy
&lt;div&gt;&lt;a href=&quot;https://x.com/AnnieLiao_2000/status/1792175318595453046&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Open%20Source-No-red&quot; alt=&quot;Open Source&quot;&gt;&lt;/a&gt;&lt;/div&gt;

&lt;p&gt;🛠️ Build Club&lt;/p&gt;

&lt;p&gt;Agentsy is an AI-driven platform designed to double team capacity by enhancing efficiency and creativity, starting with operations use cases like real estate&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://x.com/AnnieLiao_2000/status/1792175318595453046&quot;&gt;demo&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;

### AgentVerse
&lt;div&gt;&lt;a href=&quot;https://github.com/OpenBMB/AgentVer

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[SigmaHQ/sigma]]></title>
            <link>https://github.com/SigmaHQ/sigma</link>
            <guid>https://github.com/SigmaHQ/sigma</guid>
            <pubDate>Fri, 05 Sep 2025 00:04:01 GMT</pubDate>
            <description><![CDATA[Main Sigma Rule Repository]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/SigmaHQ/sigma">SigmaHQ/sigma</a></h1>
            <p>Main Sigma Rule Repository</p>
            <p>Language: Python</p>
            <p>Stars: 9,584</p>
            <p>Forks: 2,409</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre># Sigma - Generic Signature Format for SIEM Systems

&lt;a href=&quot;https://sigmahq.io/&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;br /&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;./images/sigma_logo_dark.png&quot;&gt;
  &lt;img width=&quot;454&quot; alt=&quot;Sigma Logo&quot; src=&quot;./images/sigma_logo_light.png&quot;&gt;
&lt;/picture&gt;
&lt;/p&gt;
&lt;/a&gt;
&lt;br /&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/SigmaHQ/sigma/actions?query=branch%3Amaster&quot;&gt;&lt;img src=&quot;https://github.com/SigmaHQ/sigma/actions/workflows/sigma-test.yml/badge.svg?branch=master&quot; alt=&quot;Sigma Build Status&quot;&gt;&lt;/a&gt; &lt;a href=&quot;https://sigmahq.io/&quot;&gt;&lt;img src=&quot;https://cdn.jsdelivr.net/gh/SigmaHQ/sigmahq.github.io@master/images/Sigma%20Official%20Badge.svg&quot; alt=&quot;Sigma Official Badge&quot;&gt;&lt;/a&gt; &lt;img alt=&quot;GitHub Repo stars&quot; src=&quot;https://img.shields.io/github/stars/SigmaHQ/sigma&quot;&gt;
&lt;img alt=&quot;GitHub all releases&quot; src=&quot;https://img.shields.io/github/downloads/SigmaHq/Sigma/total&quot;&gt;
&lt;br /&gt;
&lt;a href=&quot;https://opensourcesecurityindex.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
&lt;img style=&quot;width: 170px;&quot; src=&quot;https://opensourcesecurityindex.io/badge.svg&quot; alt=&quot;Open Source Security Index - Fastest Growing Open Source Security Projects&quot; width=&quot;170&quot; /&gt;
&lt;/a&gt;
&lt;/p&gt;

Welcome to the Sigma main rule repository. The place where detection engineers, threat hunters and all defensive security practitioners collaborate on detection rules. The repository offers more than 3000 detection rules of different type and aims to make reliable detections accessible to all at no cost.

Currently the repository offers three types of rules:

* [Generic Detection Rules](./rules/) - Are threat agnostic, their aim is to detect a behavior or an implementation of a technique or procedure that was, can or will be used by a potential threat actor.
* [Threat Hunting Rules](./rules-threat-hunting/) - Are broader in scope and are meant to give the analyst a starting point to hunt for potential suspicious or malicious activity
* [Emerging Threat Rules](./rules-emerging-threats/) - Are rules that cover specific threats, that are timely and relevant for certain periods of time. These threats include specific APT campaigns, exploitation of Zero-Day vulnerabilities, specific malware used during an attack,...etc.

## Explore Sigma

To start exploring the Sigma ecosystem, please visit the official website [sigmahq.io](https://sigmahq.io)

### What is Sigma

Sigma is a generic and open signature format that allows you to describe relevant log events in a straightforward manner. The rule format is very flexible, easy to write and applicable to any type of log file.

The main purpose of this project is to provide a structured form in which researchers or analysts can describe their once developed detection methods and make them shareable with others.

Sigma is for log files what [Snort](https://www.snort.org/) is for network traffic and [YARA](https://github.com/VirusTotal/yara) is for files.

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;./images/Sigma_description_dark.png&quot;&gt;
  &lt;img alt=&quot;Sigma Description - A diagram showing Yaml Files (Sigma Rules) moving through a Sigma Convertor, and coming out as many SIEM logos, showing how Sigma rules can be converted to many different available SIEM query languages&quot; src=&quot;./images/Sigma_description_light.png&quot;&gt;
&lt;/picture&gt;

### Why Sigma

Today, everyone collects log data for analysis. People start working on their own, processing numerous white papers, blog posts and log analysis guidelines, extracting the necessary information and build their own searches and dashboard. Some of their searches and correlations are great and very useful but they lack a standardized format in which they can share their work with others.

Others provide excellent analyses, include IOCs and YARA rules to detect the malicious files and network connections, but have no way to describe a specific or generic detection method in log events. Sigma is meant to be an open standard in which such detection mechanisms can be defined, shared and collected in order to improve the detection capabilities for everyone.

### 🌟 Key Features

* A continuously growing list of detection and hunting rules, peer reviewed by a community of professional Detection Engineers.
* Vendor agnostic detection rules.
* Easily shareable across communities and reports

## 🏗️ Rule Creation

To start writing Sigma rules please check the following guides:

* [Rule Creation Guide](https://github.com/SigmaHQ/sigma/wiki/Rule-Creation-Guide)
* [How to Write Sigma Rules - Nextron Systems](https://www.nextron-systems.com/2018/02/10/write-sigma-rules/)

## 🔎 Contributing &amp; Making PRs

Please refer to the [CONTRIBUTING](./CONTRIBUTING.md) guide for detailed instructions on how you can start contributing new rules.

## 📦 Rule Packages

You can download the latest rule packages from the [release page](https://github.com/SigmaHQ/sigma/releases/latest) and start leveraging Sigma rules today.

## 🧬 Rule Usage and Conversion

* You can start converting Sigma rules today using [Sigma CLI](https://github.com/SigmaHQ/sigma-cli) or [sigconverter.io](https://sigconverter.io) the GUI interface

* To integrate Sigma rules in your own toolchain or products use [pySigma](https://github.com/SigmaHQ/pySigma).

## 🚨 Reporting False Positives or New Rule Ideas

If you find a false positive or would like to propose a new detection rule idea but do not have the time to create one, please create a new issue on the [GitHub repository](https://github.com/SigmaHQ/sigma/issues/new/choose) by selecting one of the available templates.

## 📚 Resources &amp; Further Reading

* [Hack.lu 2017 Sigma - Generic Signatures for Log Events by Thomas Patzke](https://www.youtube.com/watch?v=OheVuE9Ifhs)
* [MITRE ATT&amp;CK® and Sigma Alerting SANS Webcast Recording](https://www.sans.org/webcasts/mitre-att-ck-sigma-alerting-110010 &quot;MITRE ATT&amp;CK® and Sigma Alerting&quot;)
* [Sigma - Generic Signatures for SIEM Systems by Florian Roth](https://www.slideshare.net/secret/gvgxeXoKblXRcA)

## Projects or Products that use or integrate Sigma rules
* [AlphaSOC](https://docs.alphasoc.com/detections_and_findings/sigma_community/) - Leverages Sigma rules to increase coverage across all supported log sources
* [alterix](https://github.com/mtnmunuklu/alterix) - Converts Sigma rules to the query language of CRYPTTECH&#039;s SIEM
* [AttackIQ](https://www.attackiq.com/2024/01/10/sigmaiq-attackiqs-latest-innovation-for-actionable-detections/) - Sigma Rules integrated in AttackIQ&#039;s platform, and [SigmAIQ](https://github.com/AttackIQ/SigmAIQ) for Sigma rule conversion and LLM apps
* [Atomic Threat Coverage](https://github.com/atc-project/atomic-threat-coverage) (Since December 2018)
* [AttackRuleMap - Mapping of Atomic Red Team tests and Sigma Rules](https://attackrulemap.com/)
* [Confluent Sigma](https://github.com/confluentinc/confluent-sigma) - Kafka Streams supported Sigma rules
* [Detection Studio](https://detection.studio/?ref=sigmahq_readme) - Convert Sigma rules to any supported SIEM.
* [IBM QRadar](https://community.ibm.com/community/user/security/blogs/gladys-koskas1/2023/08/02/qradar-natively-supports-sigma-for-rules-creation)
* [Impede Detection Platform](https://impede.ai/)
* [Joe Sandbox](https://www.joesecurity.org/blog/8225577975210857708)
* [LimaCharlie](https://limacharlie.io/)
* [MISP](http://www.misp-project.org/2017/03/26/MISP.2.4.70.released.html) (Since Version 2.4.70, March 2017)
* [Nextron&#039;s Aurora Agent](https://www.nextron-systems.com/aurora/)
* [Nextron&#039;s THOR Scanner](https://www.nextron-systems.com/thor/) - Scan with Sigma rules on endpoints
* [RANK VASA](https://globenewswire.com/news-release/2019/03/04/1745907/0/en/RANK-Software-to-Help-MSSPs-Scale-Cybersecurity-Offerings.html)
* [Security Onion](https://docs.securityonion.net/en/latest/sigma.html)
* [Sekoia.io XDR](https://www.sekoia.io) - XDR supporting Sigma and Sigma Correlation rules languages
* [sigma2stix](https://github.com/muchdogesec/sigma2stix) - Converts the entire SigmaHQ Ruleset into STIX 2.1 Objects.
  * A versioned archive of sigma2stix STIX 2.1 data is also available to [download here](https://github.com/muchdogesec/cti_knowledge_base_store/tree/main/sigma-rules).
* [SIΣGMA](https://github.com/3CORESec/SIEGMA) - SIEM consumable generator that utilizes Sigma for query conversion
* [SOC Prime](https://tdm.socprime.com/sigma/)
* [TA-Sigma-Searches](https://github.com/dstaulcu/TA-Sigma-Searches) (Splunk App)
* [TimeSketch](https://github.com/google/timesketch/commit/0c6c4b65a6c0f2051d074e87bbb2da2424fa6c35)
* [ypsilon](https://github.com/P4T12ICK/ypsilon) - Automated Use Case Testing

## 📜 Maintainers

* [Nasreddine Bencherchali (@nas_bench)](https://twitter.com/nas_bench)
* [Florian Roth (@cyb3rops)](https://twitter.com/cyb3rops)
* [Christian Burkard (@phantinuss)](https://twitter.com/phantinuss)
* [François Hubaut (@frack113)](https://twitter.com/frack113)
* [Thomas Patzke (@blubbfiction)](https://twitter.com/blubbfiction)

## Credits

This project would&#039;ve never reached this height without the help of the hundreds of contributors. Thanks to all past and present contributors for their help.

## Licenses

The content of this repository is released under the [Detection Rule License (DRL) 1.1](https://github.com/SigmaHQ/Detection-Rule-License).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[numpy/numpy]]></title>
            <link>https://github.com/numpy/numpy</link>
            <guid>https://github.com/numpy/numpy</guid>
            <pubDate>Fri, 05 Sep 2025 00:04:00 GMT</pubDate>
            <description><![CDATA[The fundamental package for scientific computing with Python.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/numpy/numpy">numpy/numpy</a></h1>
            <p>The fundamental package for scientific computing with Python.</p>
            <p>Language: Python</p>
            <p>Stars: 30,317</p>
            <p>Forks: 11,301</p>
            <p>Stars today: 12 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/numpy/numpy/main/branding/logo/primary/numpylogo.svg&quot; width=&quot;300&quot;&gt;
&lt;/h1&gt;&lt;br&gt;


[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&amp;colorA=E1523D&amp;colorB=007D8A)](
https://numfocus.org)
[![PyPI Downloads](https://img.shields.io/pypi/dm/numpy.svg?label=PyPI%20downloads)](
https://pypi.org/project/numpy/)
[![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/numpy.svg?label=Conda%20downloads)](
https://anaconda.org/conda-forge/numpy)
[![Stack Overflow](https://img.shields.io/badge/stackoverflow-Ask%20questions-blue.svg)](
https://stackoverflow.com/questions/tagged/numpy)
[![Nature Paper](https://img.shields.io/badge/DOI-10.1038%2Fs41586--020--2649--2-blue)](
https://doi.org/10.1038/s41586-020-2649-2)
[![LFX Health Score](https://insights.linuxfoundation.org/api/badge/health-score?project=numpy)](https://insights.linuxfoundation.org/project/numpy)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/numpy/numpy/badge)](https://securityscorecards.dev/viewer/?uri=github.com/numpy/numpy)
[![Typing](https://img.shields.io/pypi/types/numpy)](https://pypi.org/project/numpy/)


NumPy is the fundamental package for scientific computing with Python.

- **Website:** https://numpy.org
- **Documentation:** https://numpy.org/doc
- **Mailing list:** https://mail.python.org/mailman/listinfo/numpy-discussion
- **Source code:** https://github.com/numpy/numpy
- **Contributing:** https://numpy.org/devdocs/dev/index.html
- **Bug reports:** https://github.com/numpy/numpy/issues
- **Report a security vulnerability:** https://tidelift.com/docs/security

It provides:

- a powerful N-dimensional array object
- sophisticated (broadcasting) functions
- tools for integrating C/C++ and Fortran code
- useful linear algebra, Fourier transform, and random number capabilities

Testing:

NumPy requires `pytest` and `hypothesis`.  Tests can then be run after installation with:

    python -c &quot;import numpy, sys; sys.exit(numpy.test() is False)&quot;

Code of Conduct
----------------------

NumPy is a community-driven open source project developed by a diverse group of
[contributors](https://numpy.org/teams/). The NumPy leadership has made a strong
commitment to creating an open, inclusive, and positive community. Please read the
[NumPy Code of Conduct](https://numpy.org/code-of-conduct/) for guidance on how to interact
with others in a way that makes our community thrive.

Call for Contributions
----------------------

The NumPy project welcomes your expertise and enthusiasm!

Small improvements or fixes are always appreciated. If you are considering larger contributions
to the source code, please contact us through the [mailing
list](https://mail.python.org/mailman/listinfo/numpy-discussion) first.

Writing code isn’t the only way to contribute to NumPy. You can also:
- review pull requests
- help us stay on top of new and old issues
- develop tutorials, presentations, and other educational materials
- maintain and improve [our website](https://github.com/numpy/numpy.org)
- develop graphic design for our brand assets and promotional materials
- translate website content
- help with outreach and onboard new contributors
- write grant proposals and help with other fundraising efforts

For more information about the ways you can contribute to NumPy, visit [our website](https://numpy.org/contribute/). 
If you’re unsure where to start or how your skills fit in, reach out! You can
ask on the mailing list or here, on GitHub, by opening a new issue or leaving a
comment on a relevant issue that is already open.

Our preferred channels of communication are all public, but if you’d like to
speak to us in private first, contact our community coordinators at
numpy-team@googlegroups.com or on Slack (write numpy-team@googlegroups.com for
an invitation).

We also have a biweekly community call, details of which are announced on the
mailing list. You are very welcome to join.

If you are new to contributing to open source, [this
guide](https://opensource.guide/how-to-contribute/) helps explain why, what,
and how to successfully get involved.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[BerriAI/litellm]]></title>
            <link>https://github.com/BerriAI/litellm</link>
            <guid>https://github.com/BerriAI/litellm</guid>
            <pubDate>Fri, 05 Sep 2025 00:03:59 GMT</pubDate>
            <description><![CDATA[Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq]]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/BerriAI/litellm">BerriAI/litellm</a></h1>
            <p>Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq]</p>
            <p>Language: Python</p>
            <p>Stars: 28,368</p>
            <p>Forks: 4,018</p>
            <p>Stars today: 49 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
        🚅 LiteLLM
    &lt;/h1&gt;
    &lt;p align=&quot;center&quot;&gt;
        &lt;p align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://render.com/deploy?repo=https://github.com/BerriAI/litellm&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;https://render.com/images/deploy-to-render-button.svg&quot; alt=&quot;Deploy to Render&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://railway.app/template/HLP0Ub?referralCode=jch2ME&quot;&gt;
          &lt;img src=&quot;https://railway.app/button.svg&quot; alt=&quot;Deploy on Railway&quot;&gt;
        &lt;/a&gt;
        &lt;/p&gt;
        &lt;p align=&quot;center&quot;&gt;Call all LLM APIs using the OpenAI format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.]
        &lt;br&gt;
    &lt;/p&gt;
&lt;h4 align=&quot;center&quot;&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/simple_proxy&quot; target=&quot;_blank&quot;&gt;LiteLLM Proxy Server (LLM Gateway)&lt;/a&gt; | &lt;a href=&quot;https://docs.litellm.ai/docs/hosted&quot; target=&quot;_blank&quot;&gt; Hosted Proxy (Preview)&lt;/a&gt; | &lt;a href=&quot;https://docs.litellm.ai/docs/enterprise&quot;target=&quot;_blank&quot;&gt;Enterprise Tier&lt;/a&gt;&lt;/h4&gt;
&lt;h4 align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://pypi.org/project/litellm/&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/pypi/v/litellm.svg&quot; alt=&quot;PyPI Version&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://www.ycombinator.com/companies/berriai&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/Y%20Combinator-W23-orange?style=flat-square&quot; alt=&quot;Y Combinator W23&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://wa.link/huol9n&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/static/v1?label=Chat%20on&amp;message=WhatsApp&amp;color=success&amp;logo=WhatsApp&amp;style=flat-square&quot; alt=&quot;Whatsapp&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://discord.gg/wuPM9dRgDw&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/static/v1?label=Chat%20on&amp;message=Discord&amp;color=blue&amp;logo=Discord&amp;style=flat-square&quot; alt=&quot;Discord&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://join.slack.com/share/enQtOTE0ODczMzk2Nzk4NC01YjUxNjY2YjBlYTFmNDRiZTM3NDFiYTM3MzVkODFiMDVjOGRjMmNmZTZkZTMzOWQzZGQyZWIwYjQ0MWExYmE3&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/static/v1?label=Chat%20on&amp;message=Slack&amp;color=black&amp;logo=Slack&amp;style=flat-square&quot; alt=&quot;Slack&quot;&gt;
    &lt;/a&gt;
&lt;/h4&gt;

LiteLLM manages:

- Translate inputs to provider&#039;s `completion`, `embedding`, and `image_generation` endpoints
- [Consistent output](https://docs.litellm.ai/docs/completion/output), text responses will always be available at `[&#039;choices&#039;][0][&#039;message&#039;][&#039;content&#039;]`
- Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - [Router](https://docs.litellm.ai/docs/routing)
- Set Budgets &amp; Rate limits per project, api key, model [LiteLLM Proxy Server (LLM Gateway)](https://docs.litellm.ai/docs/simple_proxy)

[**Jump to LiteLLM Proxy (LLM Gateway) Docs**](https://github.com/BerriAI/litellm?tab=readme-ov-file#openai-proxy---docs) &lt;br&gt;
[**Jump to Supported LLM Providers**](https://github.com/BerriAI/litellm?tab=readme-ov-file#supported-providers-docs)

🚨 **Stable Release:** Use docker images with the `-stable` tag. These have undergone 12 hour load tests, before being published. [More information about the release cycle here](https://docs.litellm.ai/docs/proxy/release_cycle)

Support for more providers. Missing a provider or LLM Platform, raise a [feature request](https://github.com/BerriAI/litellm/issues/new?assignees=&amp;labels=enhancement&amp;projects=&amp;template=feature_request.yml&amp;title=%5BFeature%5D%3A+).

# Usage ([**Docs**](https://docs.litellm.ai/docs/))

&gt; [!IMPORTANT]
&gt; LiteLLM v1.0.0 now requires `openai&gt;=1.0.0`. Migration guide [here](https://docs.litellm.ai/docs/migration)
&gt; LiteLLM v1.40.14+ now requires `pydantic&gt;=2.0.0`. No changes required.

&lt;a target=&quot;_blank&quot; href=&quot;https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/liteLLM_Getting_Started.ipynb&quot;&gt;
  &lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot;/&gt;
&lt;/a&gt;

```shell
pip install litellm
```

```python
from litellm import completion
import os

## set ENV variables
os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;your-openai-key&quot;
os.environ[&quot;ANTHROPIC_API_KEY&quot;] = &quot;your-anthropic-key&quot;

messages = [{ &quot;content&quot;: &quot;Hello, how are you?&quot;,&quot;role&quot;: &quot;user&quot;}]

# openai call
response = completion(model=&quot;openai/gpt-4o&quot;, messages=messages)

# anthropic call
response = completion(model=&quot;anthropic/claude-sonnet-4-20250514&quot;, messages=messages)
print(response)
```

### Response (OpenAI Format)

```json
{
    &quot;id&quot;: &quot;chatcmpl-1214900a-6cdd-4148-b663-b5e2f642b4de&quot;,
    &quot;created&quot;: 1751494488,
    &quot;model&quot;: &quot;claude-sonnet-4-20250514&quot;,
    &quot;object&quot;: &quot;chat.completion&quot;,
    &quot;system_fingerprint&quot;: null,
    &quot;choices&quot;: [
        {
            &quot;finish_reason&quot;: &quot;stop&quot;,
            &quot;index&quot;: 0,
            &quot;message&quot;: {
                &quot;content&quot;: &quot;Hello! I&#039;m doing well, thank you for asking. I&#039;m here and ready to help with whatever you&#039;d like to discuss or work on. How are you doing today?&quot;,
                &quot;role&quot;: &quot;assistant&quot;,
                &quot;tool_calls&quot;: null,
                &quot;function_call&quot;: null
            }
        }
    ],
    &quot;usage&quot;: {
        &quot;completion_tokens&quot;: 39,
        &quot;prompt_tokens&quot;: 13,
        &quot;total_tokens&quot;: 52,
        &quot;completion_tokens_details&quot;: null,
        &quot;prompt_tokens_details&quot;: {
            &quot;audio_tokens&quot;: null,
            &quot;cached_tokens&quot;: 0
        },
        &quot;cache_creation_input_tokens&quot;: 0,
        &quot;cache_read_input_tokens&quot;: 0
    }
}
```

Call any model supported by a provider, with `model=&lt;provider_name&gt;/&lt;model_name&gt;`. There might be provider-specific details here, so refer to [provider docs for more information](https://docs.litellm.ai/docs/providers)

## Async ([Docs](https://docs.litellm.ai/docs/completion/stream#async-completion))

```python
from litellm import acompletion
import asyncio

async def test_get_response():
    user_message = &quot;Hello, how are you?&quot;
    messages = [{&quot;content&quot;: user_message, &quot;role&quot;: &quot;user&quot;}]
    response = await acompletion(model=&quot;openai/gpt-4o&quot;, messages=messages)
    return response

response = asyncio.run(test_get_response())
print(response)
```

## Streaming ([Docs](https://docs.litellm.ai/docs/completion/stream))

liteLLM supports streaming the model response back, pass `stream=True` to get a streaming iterator in response.
Streaming is supported for all models (Bedrock, Huggingface, TogetherAI, Azure, OpenAI, etc.)

```python
from litellm import completion
response = completion(model=&quot;openai/gpt-4o&quot;, messages=messages, stream=True)
for part in response:
    print(part.choices[0].delta.content or &quot;&quot;)

# claude sonnet 4
response = completion(&#039;anthropic/claude-sonnet-4-20250514&#039;, messages, stream=True)
for part in response:
    print(part)
```

### Response chunk (OpenAI Format)

```json
{
    &quot;id&quot;: &quot;chatcmpl-fe575c37-5004-4926-ae5e-bfbc31f356ca&quot;,
    &quot;created&quot;: 1751494808,
    &quot;model&quot;: &quot;claude-sonnet-4-20250514&quot;,
    &quot;object&quot;: &quot;chat.completion.chunk&quot;,
    &quot;system_fingerprint&quot;: null,
    &quot;choices&quot;: [
        {
            &quot;finish_reason&quot;: null,
            &quot;index&quot;: 0,
            &quot;delta&quot;: {
                &quot;provider_specific_fields&quot;: null,
                &quot;content&quot;: &quot;Hello&quot;,
                &quot;role&quot;: &quot;assistant&quot;,
                &quot;function_call&quot;: null,
                &quot;tool_calls&quot;: null,
                &quot;audio&quot;: null
            },
            &quot;logprobs&quot;: null
        }
    ],
    &quot;provider_specific_fields&quot;: null,
    &quot;stream_options&quot;: null,
    &quot;citations&quot;: null
}
```

## Logging Observability ([Docs](https://docs.litellm.ai/docs/observability/callbacks))

LiteLLM exposes pre defined callbacks to send data to Lunary, MLflow, Langfuse, DynamoDB, s3 Buckets, Helicone, Promptlayer, Traceloop, Athina, Slack

```python
from litellm import completion

## set env variables for logging tools (when using MLflow, no API key set up is required)
os.environ[&quot;LUNARY_PUBLIC_KEY&quot;] = &quot;your-lunary-public-key&quot;
os.environ[&quot;HELICONE_API_KEY&quot;] = &quot;your-helicone-auth-key&quot;
os.environ[&quot;LANGFUSE_PUBLIC_KEY&quot;] = &quot;&quot;
os.environ[&quot;LANGFUSE_SECRET_KEY&quot;] = &quot;&quot;
os.environ[&quot;ATHINA_API_KEY&quot;] = &quot;your-athina-api-key&quot;

os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;your-openai-key&quot;

# set callbacks
litellm.success_callback = [&quot;lunary&quot;, &quot;mlflow&quot;, &quot;langfuse&quot;, &quot;athina&quot;, &quot;helicone&quot;] # log input/output to lunary, langfuse, supabase, athina, helicone etc

#openai call
response = completion(model=&quot;openai/gpt-4o&quot;, messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hi 👋 - i&#039;m openai&quot;}])
```

# LiteLLM Proxy Server (LLM Gateway) - ([Docs](https://docs.litellm.ai/docs/simple_proxy))

Track spend + Load Balance across multiple projects

[Hosted Proxy (Preview)](https://docs.litellm.ai/docs/hosted)

The proxy provides:

1. [Hooks for auth](https://docs.litellm.ai/docs/proxy/virtual_keys#custom-auth)
2. [Hooks for logging](https://docs.litellm.ai/docs/proxy/logging#step-1---create-your-custom-litellm-callback-class)
3. [Cost tracking](https://docs.litellm.ai/docs/proxy/virtual_keys#tracking-spend)
4. [Rate Limiting](https://docs.litellm.ai/docs/proxy/users#set-rate-limits)

## 📖 Proxy Endpoints - [Swagger Docs](https://litellm-api.up.railway.app/)


## Quick Start Proxy - CLI

```shell
pip install &#039;litellm[proxy]&#039;
```

### Step 1: Start litellm proxy

```shell
$ litellm --model huggingface/bigcode/starcoder

#INFO: Proxy running on http://0.0.0.0:4000
```

### Step 2: Make ChatCompletions Request to Proxy


&gt; [!IMPORTANT]
&gt; 💡 [Use LiteLLM Proxy with Langchain (Python, JS), OpenAI SDK (Python, JS) Anthropic SDK, Mistral SDK, LlamaIndex, Instructor, Curl](https://docs.litellm.ai/docs/proxy/user_keys)

```python
import openai # openai v1.0.0+
client = openai.OpenAI(api_key=&quot;anything&quot;,base_url=&quot;http://0.0.0.0:4000&quot;) # set proxy to base_url
# request sent to model set on litellm proxy, `litellm --model`
response = client.chat.completions.create(model=&quot;gpt-3.5-turbo&quot;, messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;this is a test request, write a short poem&quot;
    }
])

print(response)
```

## Proxy Key Management ([Docs](https://docs.litellm.ai/docs/proxy/virtual_keys))

Connect the proxy with a Postgres DB to create proxy keys

```bash
# Get the code
git clone https://github.com/BerriAI/litellm

# Go to folder
cd litellm

# Add the master key - you can change this after setup
echo &#039;LITELLM_MASTER_KEY=&quot;sk-1234&quot;&#039; &gt; .env

# Add the litellm salt key - you cannot change this after adding a model
# It is used to encrypt / decrypt your LLM API Key credentials
# We recommend - https://1password.com/password-generator/
# password generator to get a random hash for litellm salt key
echo &#039;LITELLM_SALT_KEY=&quot;sk-1234&quot;&#039; &gt;&gt; .env

source .env

# Start
docker-compose up
```


UI on `/ui` on your proxy server
![ui_3](https://github.com/BerriAI/litellm/assets/29436595/47c97d5e-b9be-4839-b28c-43d7f4f10033)

Set budgets and rate limits across multiple projects
`POST /key/generate`

### Request

```shell
curl &#039;http://0.0.0.0:4000/key/generate&#039; \
--header &#039;Authorization: Bearer sk-1234&#039; \
--header &#039;Content-Type: application/json&#039; \
--data-raw &#039;{&quot;models&quot;: [&quot;gpt-3.5-turbo&quot;, &quot;gpt-4&quot;, &quot;claude-2&quot;], &quot;duration&quot;: &quot;20m&quot;,&quot;metadata&quot;: {&quot;user&quot;: &quot;ishaan@berri.ai&quot;, &quot;team&quot;: &quot;core-infra&quot;}}&#039;
```

### Expected Response

```shell
{
    &quot;key&quot;: &quot;sk-kdEXbIqZRwEeEiHwdg7sFA&quot;, # Bearer token
    &quot;expires&quot;: &quot;2023-11-19T01:38:25.838000+00:00&quot; # datetime object
}
```

## Supported Providers ([Docs](https://docs.litellm.ai/docs/providers))

| Provider                                                                            | [Completion](https://docs.litellm.ai/docs/#basic-usage) | [Streaming](https://docs.litellm.ai/docs/completion/stream#streaming-responses) | [Async Completion](https://docs.litellm.ai/docs/completion/stream#async-completion) | [Async Streaming](https://docs.litellm.ai/docs/completion/stream#async-streaming) | [Async Embedding](https://docs.litellm.ai/docs/embedding/supported_embedding) | [Async Image Generation](https://docs.litellm.ai/docs/image_generation) |
|-------------------------------------------------------------------------------------|---------------------------------------------------------|---------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------|
| [openai](https://docs.litellm.ai/docs/providers/openai)                             | ✅                                                       | ✅                                                                               | ✅                                                                                   | ✅                                                                                 | ✅                                                                             | ✅                                                                       |
| [Meta - Llama API](https://docs.litellm.ai/docs/providers/meta_llama)                               | ✅                                                       | ✅                                                                               | ✅                                                                                   | ✅                                                                                 |                                                                              |                                                                        |
| [azure](https://docs.litellm.ai/docs/providers/azure)                               | ✅                                                       | ✅                                                                               | ✅                                                                                   | ✅                                                                                 | ✅                                                                             | ✅                                                                       |
| [AI/ML API](https://docs.litellm.ai/docs/providers/aiml)                               | ✅                                                       | ✅                                                                               | ✅                                                                                   | ✅                                                                                 | ✅                                                                             | ✅                                                                       |
| [aws - sagemaker](https://docs.litellm.ai/docs/providers/aws_sagemaker)             | ✅                                                       | ✅                                                                               | ✅                                                                                   | ✅                                                                                 | ✅                                                                             |                                                                         |
| [aws - bedrock](https://docs.litellm.ai/docs/providers/bedrock)                     | ✅                                                       | ✅                                                                               | ✅                                                                                   | ✅                                                                                 | ✅                                                                             |                                                                         |
| [google - vertex_ai](https://docs.litellm.ai/docs/providers/vertex)                 | ✅                                                       | ✅                                                                               | ✅                                                                                   | ✅                                                                                 | ✅                                                                             | ✅                                                                       |
| [google - palm](https://docs.litellm.ai/docs/providers/palm)                        | ✅                                                       | ✅                                                                               | ✅                                                                                   | ✅                                                                                 |                                                                               |                                                                         |
| [google AI Studio - gemini](https://docs.litellm.ai/docs/providers/gemini)          | ✅                                                       | ✅                                                                               | ✅                                                                                   | ✅                                                                                 |                                                                               |                                                                         |
| [mistral ai api](https://docs.litellm.ai/docs/providers/mistral)                    | ✅                                                       | ✅                                                                               | ✅                                                                                   | ✅                                                                                 | ✅                                                                             |                                                                         |
| [cloudflare AI Workers](https://docs.litellm.ai/docs/providers/cloudflare_workers)  | ✅                                                       | ✅                                                                               | ✅                                                                                   | ✅                                                                                 |                                                                               |                                                                         |
| [cohere](https://docs.litellm.ai/docs/providers/cohere)                             | ✅                                                       | ✅                                                                               | ✅                                                                                   | ✅                                                                                 | ✅                                                                             |                                                                         |
| [anthropic](https://docs.litellm.ai/docs/providers/anthropic)                       | ✅                                                       | ✅                                                                               | ✅                                                                                   | ✅                                                                                 |                                                                               |                                                                         |
| [empower](https://docs.litellm.ai/docs/providers/empower)                    | ✅                                                      | ✅                                                                              | ✅                                                                                  | ✅                                                                                |
| [huggingface](https://docs.litellm.ai/docs/providers/huggingface)                   | ✅                                                       | ✅                            

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>