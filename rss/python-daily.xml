<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sun, 18 May 2025 00:04:35 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[public-apis/public-apis]]></title>
            <link>https://github.com/public-apis/public-apis</link>
            <guid>https://github.com/public-apis/public-apis</guid>
            <pubDate>Sun, 18 May 2025 00:04:35 GMT</pubDate>
            <description><![CDATA[A collective list of free APIs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/public-apis/public-apis">public-apis/public-apis</a></h1>
            <p>A collective list of free APIs</p>
            <p>Language: Python</p>
            <p>Stars: 341,601</p>
            <p>Forks: 36,001</p>
            <p>Stars today: 1,165 stars today</p>
            <h2>README</h2><pre># Try Public APIs for free
The Public APIs repository is manually curated by community members like you and folks working at [APILayer](https://apilayer.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo). It includes an extensive list of public APIs from many domains that you can use for your own products. Consider it a treasure trove of APIs well-managed by the community over the years.

&lt;br &gt;

&lt;p&gt;
    &lt;a href=&quot;https://apilayer.com&quot;&gt;
        &lt;div&gt;
            &lt;img src=&quot;.github/cs1586-APILayerLogoUpdate2022-LJ_v2-HighRes.png&quot; width=&quot;100%&quot; alt=&quot;APILayer Logo&quot; /&gt;
        &lt;/div&gt;
    &lt;/a&gt;
  &lt;/p&gt;

[APILayer](https://apilayer.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo) is the fastest way to integrate APIs into any product. There are a lot of APIs available at [APILayer Marketplace](https://apilayer.com/#bestSellers&amp;utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo).

&lt;br &gt;

## APILayer APIs
| API | Description | Call this API |
|:---|:---|:---|
| [IP Stack](https://ipstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Locate and Identify Website Visitors by IP Address | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-55145132-244c-448c-8e6f-8780866e4862?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-55145132-244c-448c-8e6f-8780866e4862%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Marketstack](https://marketstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Free, easy-to-use REST API interface delivering worldwide stock market data in JSON format | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Weatherstack](https://weatherstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Retrieve instant, accurate weather information for any location in the world in lightweight JSON format | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Numverify](https://numverify.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers ) | Global Phone Number Validation &amp; Lookup JSON API |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Fixer](https://fixer.io/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Fixer is a simple and lightweight API for current and historical foreign exchange (forex) rates. |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|

&lt;br &gt;

## Learn more about Public APIs

&lt;strong&gt;Get Involved&lt;/strong&gt;

* [Contributing Guide](CONTRIBUTING.md)
* [API for this project](https://github.com/davemachado/public-api)
* [Issues](https://github.com/public-apis/public-apis/issues)
* [Pull Requests](https://github.com/public-apis/public-apis/pulls)
* [LICENSE](LICENSE) 

&lt;br /&gt;

## Index

* [Animals](#animals)
* [Anime](#anime)
* [Anti-Malware](#anti-malware)
* [Art &amp; Design](#art--design)
* [Authentication &amp; Authorization](#authentication--authorization)
* [Blockchain](#blockchain)
* [Books](#books)
* [Business](#business)
* [Calendar](#calendar)
* [Cloud Storage &amp; File Sharing](#cloud-storage--file-sharing)
* [Continuous Integration](#continuous-integration)
* [Cryptocurrency](#cryptocurrency)
* [Currency Exchange](#currency-exchange)
* [Data Validation](#data-validation)
* [Development](#development)
* [Dictionaries](#dictionaries)
* [Documents &amp; Productivity](#documents--productivity)
* [Email](#email)
* [Entertainment](#entertainment)
* [Environment](#environment)
* [Events](#events)
* [Finance](#finance)
* [Food &amp; Drink](#food--drink)
* [Games &amp; Comics](#games--comics)
* [Geocoding](#geocoding)
* [Government](#government)
* [Health](#health)
* [Jobs](#jobs)
* [Machine Learning](#machine-learning)
* [Music](#music)
* [News](#news)
* [Open Data](#open-data)
* [Open Source Projects](#open-source-projects)
* [Patent](#patent)
* [Personality](#personality)
* [Phone](#phone)
* [Photography](#photography)
* [Programming](#programming)
* [Science &amp; Math](#science--math)
* [Security](#security)
* [Shopping](#shopping)
* [Social](#social)
* [Sports &amp; Fitness](#sports--fitness)
* [Test Data](#test-data)
* [Text Analysis](#text-analysis)
* [Tracking](#tracking)
* [Transportation](#transportation)
* [URL Shorteners](#url-shorteners)
* [Vehicle](#vehicle)
* [Video](#video)
* [Weather](#weather)
&lt;br &gt;

### Animals
API | Description | Auth | HTTPS | CORS 
|:---|:---|:---|:---|:---|
| [AdoptAPet](https://www.adoptapet.com/public/apis/pet_list.html) | Resource to help get pets adopted | `apiKey` | Yes | Yes |
| [Axolotl](https://theaxolotlapi.netlify.app/) | Collection of axolotl pictures and facts | No | Yes | No |
| [Cat Facts](https://alexwohlbruck.github.io/cat-facts/) | Daily cat facts | No | Yes | No | |
| [Cataas](https://cataas.com/) | Cat as a service (cats pictures and gifs) | No | Yes | No |
| [Cats](https://docs.thecatapi.com/) | Pictures of cats from Tumblr | `apiKey` | Yes | No |
| [Dog Facts](https://dukengn.github.io/Dog-facts-API/) | Random dog facts | No | Yes | Yes |
| [Dog Facts](https://kinduff.github.io/dog-api/) | Random facts of Dogs | No | Yes | Yes |
| [Dogs](https://dog.ceo/dog-api/) | Based on the Stanford Dogs Dataset | No | Yes | Yes |
| [eBird](https://documenter.getpostman.com/view/664302/S1ENwy59) | Retrieve recent or notable birding observations within a region | `apiKey` | Yes | No |
| [FishWatch](https://www.fishwatch.gov/developers) | Information and pictures about individual fish species | No | Yes | Yes |
| [HTTP Cat](https://http.cat/) | Cat for every HTTP Status | No | Yes | Yes |
| [HTTP Dog](https://http.dog/) | Dogs for every HTTP response status code | No | Yes | Yes |
| [IUCN](http://apiv3.iucnredlist.org/api/v3/docs) | IUCN Red List of Threatened Species | `apiKey` | No | No |
| [MeowFacts](https://github.com/wh-iterabb-it/meowfacts) | Get random cat facts | No | Yes | No |
| [Movebank](https://github.com/movebank/movebank-api-doc) | Movement and Migration data of animals | No | Yes | Yes |
| [Petfinder](https://www.petfinder.com/developers/) | Petfinder is dedicated to helping pets find homes, another resource to get pets adopted | `apiKey` | Yes | Yes |
| [PlaceBear](https://placebear.com/) | Placeholder bear pictures | No | Yes | Yes |
| [PlaceDog](https://place.dog) | Placeholder Dog pictures | No | Yes | Yes |
| [PlaceKitten](https://placekitten.com/) | Placeholder Kitten pictures | No | Yes | Yes |
| [RandomDog](https://random.dog/woof.json) | Random pictures of dogs | No | Yes | Yes |
| [RandomDuck](https://random-d.uk/api) | Random pictures of ducks | No | Yes | No |
| [RandomFox](https://randomfox.ca/floof/) | Random pictures of foxes | No | Yes | No |
| [RescueGroups](https://userguide.rescuegroups.org/display/APIDG/API+Developers+Guide+Home) | Adoption | No | Yes | Unknown |
| [Shibe.Online](http://shibe.online/) | Random pictures of Shiba Inu, cats or birds | No | Yes | Yes |
| [The Dog](https://thedogapi.com/) | A public service all about Dogs, free to use when making your fancy new App, Website or Service | `apiKey` | Yes | No |
| [xeno-canto](https://xeno-canto.org/explore/api) | Bird recordings | No | Yes | Unknown |
| [Zoo Animals](https://zoo-animal-api.herokuapp.com/) | Facts and pictures of zoo animals | No | Yes | Yes |

**[â¬† Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;

### Anime
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AniAPI](https://aniapi.com/docs/) | Anime discovery, streaming &amp; syncing with trackers | `OAuth` | Yes | Yes |
| [AniDB](https://wiki.anidb.net/HTTP_API_Definition) | Anime Database | `apiKey` | No | Unknown |
| [AniList](https://github.com/AniList/ApiV2-GraphQL-Docs) | Anime discovery &amp; tracking | `OAuth` | Yes | Unknown |
| [AnimeChan](https://github.com/RocktimSaikia/anime-chan) | Anime quotes (over 10k+) | No | Yes | No |
| [AnimeFacts](https://chandan-02.github.io/anime-facts-rest-api/) | Anime Facts (over 100+) | No | Yes | Yes |
| [AnimeNewsNetwork](https://www.animenewsnetwork.com/encyclopedia/api.php) | Anime industry news | No | Yes | Yes |
| [Catboy](https://catboys.com/api) | Neko images, funny GIFs &amp; more | No | Yes | Yes |
| [Danbooru Anime](https://danbooru.donmai.us/wiki_pages/help:api) | Thousands of anime artist database to find good anime art | `apiKey` | Yes | Yes |
| [Jikan](https://jikan.moe) | Unofficial MyAnimeList API | No | Yes | Yes |
| [Kitsu](https://kitsu.docs.apiary.io/) | Anime discovery platform | `OAuth` | Yes | Yes |
| [MangaDex](https://api.mangadex.org/docs.html) | Manga Database and Community | `apiKey` | Yes | Unknown |
| [Mangapi](https://rapidapi.com/pierre.carcellermeunier/api/mangapi3/) | Translate manga pages from one language to another | `apiKey` | Yes | Unknown |
| [MyAnimeList](https://myanimelist.net/clubs.php?cid=13727) | Anime and Manga Database and Community | `OAuth` | Yes | Unknown |
| [NekosBest](https://docs.nekos.best) | Neko Images &amp; Anime roleplaying GIFs | No | Yes | Yes |
| [Shikimori](https://shikimori.one/api/doc) | Anime discovery, tracking, forum, rates | `OAuth` | Yes | Unknown |
| [Studio Ghibli](https://ghibliapi.herokuapp.com) | Resources from Studio Ghibli films | No | Yes | Yes |
| [Trace Moe](https://soruly.github.io/trace.moe-api/#/) | A useful tool to get the exact scene of an anime from a screenshot | No | Yes | No |
| [Waifu.im](https://waifu.im/docs) | Get waifu pictures from an archive of over 4000 images and multiple tags | No | Yes | Yes |
| [Waifu.pics](https://waifu.pics/docs) | Image sharing platform for anime images | No | Yes | No |

**[â¬† Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;

### Anti-Malware
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AbuseIPDB](https://docs.abuseipdb.com/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [AlienVault Open Threat Exchange (OTX)](https://otx.alienvault.com/api) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [CAPEsandbox](https://capev2.readthedocs.io/en/latest/usage/api.html) | Malware execution and analysis | `apiKey` | Yes | Unknown |
| [Google Safe Browsing](https://developers.google.com/safe-browsing/) | Google Link/Domain Flagging | `apiKey` | Yes | Unknown |
| [MalDatabase](https://maldatabase.com/api-doc.html) | Provide malware datasets and threat intelligence feeds | `apiKey` | Yes | Unknown |
| [MalShare](https://malshare.com/doc.php) | Malware Archive / file sourcing | `apiKey` | Yes | No |
| [MalwareBazaar](https://bazaar.abuse.ch/api/) | Collect and share malware samples | `apiKey` | Yes | Unknown |
| [Metacert](https://metacert.com/) | Metacert Link Flagging | `apiKey` | Yes | Unknown |
| [NoPhishy](https://rapidapi.com/Amiichu/api/exerra-phishing-check/) | Check links to see if they&#039;re known phishing attempts | `apiKey` | Yes | Yes |
| [Phisherman](https://phisherman.gg/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [Scanii](https://docs.scanii.com/) | Simple REST API that can scan submitted documents/files for the presence of threats | `apiKey` | Yes | Yes |
| [URLhaus](https://urlhaus-api.abuse.ch/) | Bulk queries and Download Malware Samples | No | Yes | Yes |
| [URLScan.io](https://urlscan.io/about-api/) | Scan and Analyse URLs | `apiKey` | Yes | Unknown |
| [VirusTotal](https://www.virustotal.com/en/documentation/public-api/) | VirusTotal File/URL Analysis | `apiKey` | Yes | Unknown |
| [Web of Trust](https://support.mywot.com/hc/en-us/sections/360004477734-API-) | IP/domain/URL reputation | `apiKey` | Yes | Unknown | 

**[â¬† Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Art &amp; Design
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AmÃ©thyste](https://api.amethyste.moe/) | Generate images for Discord users | `apiKey` | Yes | Unknown |
| [Art Institute of Chicago](https://api.artic.edu/docs/) | Art | No | Yes | Yes |
| [Colormind](http://colormind.io/api-access/) | Color scheme generator | No | No | Unknown |
| [ColourLovers](http://www.colourlovers.com/api) | Get various patterns, palettes and images | No | No | Unknown |
| [Cooper Hewitt](https://collection.cooperhewitt.org/api) | Smithsonian Design Museum | `apiKey` | Yes | Unknown |
| [Dribbble](https://developer.dribbble.com) | Discover the worldâ€™s top designers &amp; creatives | `OAuth` | Yes | Unknown |
| [EmojiHub](https://github.com/cheatsnake/emojihub) | Get emojis by categories and groups | No | Yes | Yes |
| [Europeana](https://pro.europeana.eu/resources/apis/search) | European Museum and Galleries content | `apiKey` | Yes | Unknown |
| [Harvard Art Museums](https://github.com/harvardartmuseums/api-docs) | Art | `apiKey` | No | Unknown |
| [Icon Horse](https://icon.horse) | Favicons for any website, with fallbacks | No | Yes | Yes |
| [Iconfinder](https://developer.iconfinder.com) | Icons | `apiKey` | Yes | Unknown |
| [Icons8](https://img.icons8.com/) | Icons (find &quot;search icon&quot; hyperlink in page) | No | Yes | Unknown |
| [Lordicon](https://lordicon.com/) | Icons with predone Animations | No | Yes | Yes |
| [Metropolitan Museum of Art](https://metmuseum.github.io/) | Met Museum of Art | No | Yes | No |
| [Noun Project](http://api.thenounproject.com/index.html) | Icons | `OAuth` | No | Unknown |
| [PHP-Noise](https://php-noise.com/) | Noise Background Image Generator | No | Yes | Yes |
| [Pixel Encounter](https://pixelencounter.com/api) | SVG Icon Generator | No | Yes | No |
| [Rijksmuseum](https://data.rijksmuseum.nl/object-metadata/api/) | RijksMuseum Data | `apiKey` | Yes | Unknown |
| [Word Cloud](https://wordcloudapi.com/) | Easily create word clouds | `apiKey` | Yes | Unknown |
| [xColors](https://x-colors.herokuapp.com/) | Generate &amp; convert colors | No | Yes | Yes |

**[â¬† Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Authentication &amp; Authorization
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [Auth0](https://auth0.com) | Easy to implement, adaptable authentication and authorization platform | `apiKey` | Yes | Yes |
| [GetOTP](https://otp.dev/en/docs/) | Implement OTP flow quickly | `apiKey` | Yes | No |
| [Micro User Service](https://m3o.com/user) | User management and authentication | `apiKey` | Yes | No |
| [MojoAuth](https://mojoauth.com) | Secure and modern passwordless authentication platform | `apiKey` | Yes | Yes |
| [SAWO Labs](https://sawolabs.com) | Simplify login and improve user experience by integrating passwordless authentication in your app | `apiKey` | Yes | Yes |
| [Stytch](https://stytch.com/) | User infrastructure for modern applications | `apiKey` | Yes | No |
| [Warrant](https://warrant.dev/) | APIs for authorization and access control | `apiKey` | Yes | Yes |

**[â¬† Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Blockchain
| API | Description | Auth | HTTPS | CORS |
|---|:---|:---|:---|:---|
| [Bitquery](https://graphql.bitquery.io/ide) | Onchain GraphQL APIs &amp; DEX APIs | `apiKey` | Yes | Yes |
| [Chainlink](https://chain.link/developer-resources) | Build hybrid smart contracts with Chainlink | No | Yes | Unknown |
| [Chainpoint](https://tierion.com/chainpoint/) | Chainpoint is a global network for anchoring data to the Bitcoin blockchain | No | Yes | Unknown |
| [Covalent](https://www.covalenthq.com/docs/api/) | Multi-blockchain data aggregator platform | `apiKey` | Yes | Unknown |
| [Etherscan](https://etherscan.io/apis) | Ethereum explorer API | `apiKey` | Yes | Yes |
| [Helium](https://docs.helium.com/api/blockchain/introduction/) | Helium is a global, distributed network of Hotspots that create public, long-range wireless coverage | No | Yes | Unknown |
| [Nownodes](https://nownodes.io/) | Blockchain-as-a-service solution that provides high-quality connection via API | `apiKey` | Yes | Unknown |
| [Steem](https://developers.steem.io/) | Blockchain-based blogging and social media website | No | No | No |
| [The Graph](https://thegraph.com) | Indexing protocol for querying networks like Ethereum with GraphQL | `apiKey` | Yes | Unknown |
| [Walltime](https://walltime.info/api.html) | To retrieve Walltime&#039;s market info | No | Yes | Unknown |
| [Watchdata](https://docs.watchdata.io) | Provide simple and reliable API access to Ethereum blockchain | `apiKey` | Yes | Unknown |

**[â¬† Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Books
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [A BÃ­blia Digital](https://www.abibliadigital.com.br/en) | Do not worry about managing the multiple versions of the Bible | `apiKey` | Yes | No |
| [Bhagavad Gita](https://docs.bhagavadgitaapi.in) | Open Source Shrimad Bhagavad Gita API including 21+ authors translation in Sanskrit/English/Hindi | `apiKey` | Yes | Yes |
| [Bhagavad Gita](https://bhagavadgita.io/api) | Bhagavad Gita text | `OAuth` | Yes | Yes |
| [Bhagavad Gita telugu](https://gita-api.vercel.app) | Bhagavad Gita API in telugu and odia languages | No | Yes | Yes |
| [Bible-api](https://bible-api.com/) | Free Bible API with multiple languages | No | Yes | Yes |
| [British National Bibliography](http://bnb.data.bl.uk/) | Books | No | No | Unknown |
| [Crossref Metadata Search](https://github.com/CrossRef/rest-api-doc) | Books &amp; Articles Metadata | No | Yes | Unknown |
| [Ganjoor](https://api.ganjoor.net) | Classic Persian poetry works including access to related manuscripts, recitations and music tracks | `OAuth` | Yes | Yes |
| [Google Books](https://developers.google.com/books/) | Books | `OAuth` | Yes | Unknown |
| [GurbaniNow](https://github.com/GurbaniNow/api) | Fast and Accurate Gurbani RESTful API | No | Yes | Unknown |
| [Gutendex](https://gutendex.com/) | Web-API for fetching data from Project Gutenberg Books Library | No | Yes | Unknown |
| [Open Library](https://openlibrary.org/developers/api) | Books, book covers and related data | No | Yes | No |
| [Penguin Publishing](http://www.penguinrandomhouse.biz/webservices/rest/) | Books, book covers and related data | No | Yes | Yes |
| [PoetryDB](https://github.com/thundercomb/poetrydb#readme) | Enables you to get instant data from our vast poetry collection | No | Yes | Yes |
| [Quran](https://quran.api-docs.io/) | RESTful Quran API with multiple languages | No | Yes | Yes |
| [Quran Cloud](https://alquran.cloud/api) | A RESTful Quran API to retrieve an Ayah, Surah, Juz or the entire Holy Quran | No | Yes | Yes |
| [Quran-api](https://github.com/fawazahmed0/quran-api#readme) | Free Quran API Service with 90+ different languages and 400+ translations | No | Yes | Yes |
| [Rig Veda](https://aninditabasu.github.io/indica/html/rv.html) | Gods and 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[virattt/ai-hedge-fund]]></title>
            <link>https://github.com/virattt/ai-hedge-fund</link>
            <guid>https://github.com/virattt/ai-hedge-fund</guid>
            <pubDate>Sun, 18 May 2025 00:04:34 GMT</pubDate>
            <description><![CDATA[An AI Hedge Fund Team]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/virattt/ai-hedge-fund">virattt/ai-hedge-fund</a></h1>
            <p>An AI Hedge Fund Team</p>
            <p>Language: Python</p>
            <p>Stars: 27,911</p>
            <p>Forks: 4,837</p>
            <p>Stars today: 203 stars today</p>
            <h2>README</h2><pre># AI Hedge Fund

This is a proof of concept for an AI-powered hedge fund.  The goal of this project is to explore the use of AI to make trading decisions.  This project is for **educational** purposes only and is not intended for real trading or investment.

This system employs several agents working together:

1. Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation
2. Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety
3. Bill Ackman Agent - An activist investor, takes bold positions and pushes for change
4. Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption
5. Charlie Munger Agent - Warren Buffett&#039;s partner, only buys wonderful businesses at fair prices
6. Michael Burry Agent - The Big Short contrarian who hunts for deep value
7. Peter Lynch Agent - Practical investor who seeks &quot;ten-baggers&quot; in everyday businesses
8. Phil Fisher Agent - Meticulous growth investor who uses deep &quot;scuttlebutt&quot; research 
9. Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential
10. Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price
11. Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals
12. Sentiment Agent - Analyzes market sentiment and generates trading signals
13. Fundamentals Agent - Analyzes fundamental data and generates trading signals
14. Technicals Agent - Analyzes technical indicators and generates trading signals
15. Risk Manager - Calculates risk metrics and sets position limits
16. Portfolio Manager - Makes final trading decisions and generates orders
    
&lt;img width=&quot;1042&quot; alt=&quot;Screenshot 2025-03-22 at 6 19 07 PM&quot; src=&quot;https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4&quot; /&gt;


**Note**: the system simulates trading decisions, it does not actually trade.

[![Twitter Follow](https://img.shields.io/twitter/follow/virattt?style=social)](https://twitter.com/virattt)

## Disclaimer

This project is for **educational and research purposes only**.

- Not intended for real trading or investment
- No warranties or guarantees provided
- Past performance does not indicate future results
- Creator assumes no liability for financial losses
- Consult a financial advisor for investment decisions

By using this software, you agree to use it solely for learning purposes.

## Table of Contents
- [Setup](#setup)
  - [Using Poetry](#using-poetry)
  - [Using Docker](#using-docker)
- [Usage](#usage)
  - [Running the Hedge Fund](#running-the-hedge-fund)
  - [Running the Backtester](#running-the-backtester)
- [Project Structure](#project-structure)
- [Contributing](#contributing)
- [Feature Requests](#feature-requests)
- [License](#license)

## Setup

### Using Poetry

Clone the repository:
```bash
git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
```

1. Install Poetry (if not already installed):
```bash
curl -sSL https://install.python-poetry.org | python3 -
```

2. Install dependencies:
```bash
poetry install
```

3. Set up your environment variables:
```bash
# Create .env file for your API keys
cp .env.example .env
```

4. Set your API keys:
```bash
# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
# Get your OpenAI API key from https://platform.openai.com/
OPENAI_API_KEY=your-openai-api-key

# For running LLMs hosted by groq (deepseek, llama3, etc.)
# Get your Groq API key from https://groq.com/
GROQ_API_KEY=your-groq-api-key

# For getting financial data to power the hedge fund
# Get your Financial Datasets API key from https://financialdatasets.ai/
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
```

### Using Docker

1. Make sure you have Docker installed on your system. If not, you can download it from [Docker&#039;s official website](https://www.docker.com/get-started).

2. Clone the repository:
```bash
git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
```

3. Set up your environment variables:
```bash
# Create .env file for your API keys
cp .env.example .env
```

4. Edit the .env file to add your API keys as described above.

5. Build the Docker image:
```bash
# On Linux/Mac:
./run.sh build

# On Windows:
run.bat build
```

**Important**: You must set `OPENAI_API_KEY`, `GROQ_API_KEY`, `ANTHROPIC_API_KEY`, or `DEEPSEEK_API_KEY` for the hedge fund to work.  If you want to use LLMs from all providers, you will need to set all API keys.

Financial data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key.

For any other ticker, you will need to set the `FINANCIAL_DATASETS_API_KEY` in the .env file.

## Usage

### Running the Hedge Fund

#### With Poetry
```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA
```

#### With Docker
```bash
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA main

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA main
```

**Example Output:**
&lt;img width=&quot;992&quot; alt=&quot;Screenshot 2025-01-06 at 5 50 17 PM&quot; src=&quot;https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b&quot; /&gt;

You can also specify a `--ollama` flag to run the AI hedge fund using local LLMs.

```bash
# With Poetry:
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama

# With Docker (on Linux/Mac):
./run.sh --ticker AAPL,MSFT,NVDA --ollama main

# With Docker (on Windows):
run.bat --ticker AAPL,MSFT,NVDA --ollama main
```

You can also specify a `--show-reasoning` flag to print the reasoning of each agent to the console.

```bash
# With Poetry:
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --show-reasoning

# With Docker (on Linux/Mac):
./run.sh --ticker AAPL,MSFT,NVDA --show-reasoning main

# With Docker (on Windows):
run.bat --ticker AAPL,MSFT,NVDA --show-reasoning main
```

You can optionally specify the start and end dates to make decisions for a specific time period.

```bash
# With Poetry:
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 

# With Docker (on Linux/Mac):
./run.sh --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 main

# With Docker (on Windows):
run.bat --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 main
```

### Running the Backtester

#### With Poetry
```bash
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
```

#### With Docker
```bash
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA backtest

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA backtest
```

**Example Output:**
&lt;img width=&quot;941&quot; alt=&quot;Screenshot 2025-01-06 at 5 47 52 PM&quot; src=&quot;https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47&quot; /&gt;


You can optionally specify the start and end dates to backtest over a specific time period.

```bash
# With Poetry:
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01

# With Docker (on Linux/Mac):
./run.sh --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 backtest

# With Docker (on Windows):
run.bat --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 backtest
```

You can also specify a `--ollama` flag to run the backtester using local LLMs.
```bash
# With Poetry:
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --ollama

# With Docker (on Linux/Mac):
./run.sh --ticker AAPL,MSFT,NVDA --ollama backtest

# With Docker (on Windows):
run.bat --ticker AAPL,MSFT,NVDA --ollama backtest
```


## Project Structure 
```
ai-hedge-fund/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/                   # Agent definitions and workflow
â”‚   â”‚   â”œâ”€â”€ bill_ackman.py        # Bill Ackman agent
â”‚   â”‚   â”œâ”€â”€ fundamentals.py       # Fundamental analysis agent
â”‚   â”‚   â”œâ”€â”€ portfolio_manager.py  # Portfolio management agent
â”‚   â”‚   â”œâ”€â”€ risk_manager.py       # Risk management agent
â”‚   â”‚   â”œâ”€â”€ sentiment.py          # Sentiment analysis agent
â”‚   â”‚   â”œâ”€â”€ technicals.py         # Technical analysis agent
â”‚   â”‚   â”œâ”€â”€ valuation.py          # Valuation analysis agent
â”‚   â”‚   â”œâ”€â”€ ...                   # Other agents
â”‚   â”‚   â”œâ”€â”€ warren_buffett.py     # Warren Buffett agent
â”‚   â”‚   â”œâ”€â”€ aswath_damodaran.py   # Aswath Damodaran agent
â”‚   â”‚   â”œâ”€â”€ ...                   # Other agents
â”‚   â”‚   â”œâ”€â”€ ...                   # Other agents
â”‚   â”œâ”€â”€ tools/                    # Agent tools
â”‚   â”‚   â”œâ”€â”€ api.py                # API tools
â”‚   â”œâ”€â”€ backtester.py             # Backtesting tools
â”‚   â”œâ”€â”€ main.py # Main entry point
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ ...
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

**Important**: Please keep your pull requests small and focused.  This will make it easier to review and merge.

## Feature Requests

If you have a feature request, please open an [issue](https://github.com/virattt/ai-hedge-fund/issues) and make sure it is tagged with `enhancement`.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mem0ai/mem0]]></title>
            <link>https://github.com/mem0ai/mem0</link>
            <guid>https://github.com/mem0ai/mem0</guid>
            <pubDate>Sun, 18 May 2025 00:04:33 GMT</pubDate>
            <description><![CDATA[Memory for AI Agents; SOTA in AI Agent Memory; Announcing OpenMemory MCP - local and secure memory management.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mem0ai/mem0">mem0ai/mem0</a></h1>
            <p>Memory for AI Agents; SOTA in AI Agent Memory; Announcing OpenMemory MCP - local and secure memory management.</p>
            <p>Language: Python</p>
            <p>Stars: 31,152</p>
            <p>Forks: 3,013</p>
            <p>Stars today: 277 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/mem0ai/mem0&quot;&gt;
    &lt;img src=&quot;docs/images/banner-sm.png&quot; width=&quot;800px&quot; alt=&quot;Mem0 - The Memory Layer for Personalized AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot; style=&quot;display: flex; justify-content: center; gap: 20px; align-items: center;&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/11194&quot; target=&quot;blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/11194&quot; alt=&quot;mem0ai%2Fmem0 | Trendshift&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://mem0.ai&quot;&gt;Learn more&lt;/a&gt;
  Â·
  &lt;a href=&quot;https://mem0.dev/DiG&quot;&gt;Join Discord&lt;/a&gt;
  Â·
  &lt;a href=&quot;https://mem0.dev/demo&quot;&gt;Demo&lt;/a&gt;
  Â·
  &lt;a href=&quot;https://mem0.dev/openmemory&quot;&gt;OpenMemory&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://mem0.dev/DiG&quot;&gt;
    &lt;img src=&quot;https://dcbadge.vercel.app/api/server/6PzXDgEjG5?style=flat&quot; alt=&quot;Mem0 Discord&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://pepy.tech/project/mem0ai&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/dm/mem0ai&quot; alt=&quot;Mem0 PyPI - Downloads&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/mem0ai/mem0&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/commit-activity/m/mem0ai/mem0?style=flat-square&quot; alt=&quot;GitHub commit activity&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/mem0ai&quot; target=&quot;blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/mem0ai?color=%2334D058&amp;label=pypi%20package&quot; alt=&quot;Package version&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.npmjs.com/package/mem0ai&quot; target=&quot;blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/npm/v/mem0ai&quot; alt=&quot;Npm package&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.ycombinator.com/companies/mem0&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Y%20Combinator-S24-orange?style=flat-square&quot; alt=&quot;Y Combinator S24&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://mem0.ai/research&quot;&gt;&lt;strong&gt;ğŸ“„ Building Production-Ready AI Agents with Scalable Long-Term Memory â†’&lt;/strong&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;âš¡ +26% Accuracy vs. OpenAI Memory â€¢ ğŸš€ 91% Faster â€¢ ğŸ’° 90% Fewer Tokens&lt;/strong&gt;
&lt;/p&gt;

##  ğŸ”¥ Research Highlights
- **+26% Accuracy** over OpenAI Memory on the LOCOMO benchmark
- **91% Faster Responses** than full-context, ensuring low-latency at scale
- **90% Lower Token Usage** than full-context, cutting costs without compromise
- [Read the full paper](https://mem0.ai/research)

# Introduction

[Mem0](https://mem0.ai) (&quot;mem-zero&quot;) enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions. It remembers user preferences, adapts to individual needs, and continuously learns over timeâ€”ideal for customer support chatbots, AI assistants, and autonomous systems.

### Key Features &amp; Use Cases

**Core Capabilities:**
- **Multi-Level Memory**: Seamlessly retains User, Session, and Agent state with adaptive personalization
- **Developer-Friendly**: Intuitive API, cross-platform SDKs, and a fully managed service option

**Applications:**
- **AI Assistants**: Consistent, context-rich conversations
- **Customer Support**: Recall past tickets and user history for tailored help
- **Healthcare**: Track patient preferences and history for personalized care
- **Productivity &amp; Gaming**: Adaptive workflows and environments based on user behavior

## ğŸš€ Quickstart Guide &lt;a name=&quot;quickstart&quot;&gt;&lt;/a&gt;

Choose between our hosted platform or self-hosted package:

### Hosted Platform

Get up and running in minutes with automatic updates, analytics, and enterprise security.

1. Sign up on [Mem0 Platform](https://app.mem0.ai)
2. Embed the memory layer via SDK or API keys

### Self-Hosted (Open Source)

Install the sdk via pip:

```bash
pip install mem0ai
```

Install sdk via npm:
```bash
npm install mem0ai
```

### Basic Usage

Mem0 requires an LLM to function, with `gpt-4o-mini` from OpenAI as the default. However, it supports a variety of LLMs; for details, refer to our [Supported LLMs documentation](https://docs.mem0.ai/components/llms/overview).

First step is to instantiate the memory:

```python
from openai import OpenAI
from mem0 import Memory

openai_client = OpenAI()
memory = Memory()

def chat_with_memories(message: str, user_id: str = &quot;default_user&quot;) -&gt; str:
    # Retrieve relevant memories
    relevant_memories = memory.search(query=message, user_id=user_id, limit=3)
    memories_str = &quot;\n&quot;.join(f&quot;- {entry[&#039;memory&#039;]}&quot; for entry in relevant_memories[&quot;results&quot;])

    # Generate Assistant response
    system_prompt = f&quot;You are a helpful AI. Answer the question based on query and memories.\nUser Memories:\n{memories_str}&quot;
    messages = [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_prompt}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: message}]
    response = openai_client.chat.completions.create(model=&quot;gpt-4o-mini&quot;, messages=messages)
    assistant_response = response.choices[0].message.content

    # Create new memories from the conversation
    messages.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: assistant_response})
    memory.add(messages, user_id=user_id)

    return assistant_response

def main():
    print(&quot;Chat with AI (type &#039;exit&#039; to quit)&quot;)
    while True:
        user_input = input(&quot;You: &quot;).strip()
        if user_input.lower() == &#039;exit&#039;:
            print(&quot;Goodbye!&quot;)
            break
        print(f&quot;AI: {chat_with_memories(user_input)}&quot;)

if __name__ == &quot;__main__&quot;:
    main()
```

For detailed integration steps, see the [Quickstart](https://docs.mem0.ai/quickstart) and [API Reference](https://docs.mem0.ai/api-reference).

## ğŸ”— Integrations &amp; Demos

- **ChatGPT with Memory**: Personalized chat powered by Mem0 ([Live Demo](https://mem0.dev/demo))
- **Browser Extension**: Store memories across ChatGPT, Perplexity, and Claude ([Chrome Extension](https://chromewebstore.google.com/detail/onihkkbipkfeijkadecaafbgagkhglop?utm_source=item-share-cb))
- **Langgraph Support**: Build a customer bot with Langgraph + Mem0 ([Guide](https://docs.mem0.ai/integrations/langgraph))
- **CrewAI Integration**: Tailor CrewAI outputs with Mem0 ([Example](https://docs.mem0.ai/integrations/crewai))

## ğŸ“š Documentation &amp; Support

- Full docs: https://docs.mem0.ai
- Community: [Discord](https://mem0.dev/DiG) Â· [Twitter](https://x.com/mem0ai)
- Contact: founders@mem0.ai

## Citation

We now have a paper you can cite:

```bibtex
@article{mem0,
  title={Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory},
  author={Chhikara, Prateek and Khant, Dev and Aryan, Saket and Singh, Taranjeet and Yadav, Deshraj},
  journal={arXiv preprint arXiv:2504.19413},
  year={2025}
}
```

## âš–ï¸ License

Apache 2.0 â€” see the [LICENSE](LICENSE) file for details.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[happycola233/tchMaterial-parser]]></title>
            <link>https://github.com/happycola233/tchMaterial-parser</link>
            <guid>https://github.com/happycola233/tchMaterial-parser</guid>
            <pubDate>Sun, 18 May 2025 00:04:32 GMT</pubDate>
            <description><![CDATA[å›½å®¶ä¸­å°å­¦æ™ºæ…§æ•™è‚²å¹³å° ç”µå­è¯¾æœ¬ä¸‹è½½å·¥å…·ï¼Œå¸®åŠ©æ‚¨ä»æ™ºæ…§æ•™è‚²å¹³å°ä¸­è·å–ç”µå­è¯¾æœ¬çš„ PDF æ–‡ä»¶ç½‘å€å¹¶è¿›è¡Œä¸‹è½½ï¼Œè®©æ‚¨æ›´æ–¹ä¾¿åœ°è·å–è¯¾æœ¬å†…å®¹ã€‚]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/happycola233/tchMaterial-parser">happycola233/tchMaterial-parser</a></h1>
            <p>å›½å®¶ä¸­å°å­¦æ™ºæ…§æ•™è‚²å¹³å° ç”µå­è¯¾æœ¬ä¸‹è½½å·¥å…·ï¼Œå¸®åŠ©æ‚¨ä»æ™ºæ…§æ•™è‚²å¹³å°ä¸­è·å–ç”µå­è¯¾æœ¬çš„ PDF æ–‡ä»¶ç½‘å€å¹¶è¿›è¡Œä¸‹è½½ï¼Œè®©æ‚¨æ›´æ–¹ä¾¿åœ°è·å–è¯¾æœ¬å†…å®¹ã€‚</p>
            <p>Language: Python</p>
            <p>Stars: 1,699</p>
            <p>Forks: 173</p>
            <p>Stars today: 243 stars today</p>
            <h2>README</h2><pre># [å›½å®¶ä¸­å°å­¦æ™ºæ…§æ•™è‚²å¹³å°](https://basic.smartedu.cn/tchMaterial/) ç”µå­è¯¾æœ¬ä¸‹è½½å·¥å…·

![Python Version](https://img.shields.io/badge/Python-3.x-blue.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Made With Loveâ¤ï¸](https://img.shields.io/badge/Made_With-%E2%9D%A4-red.svg)

&gt; [!TIP]
&gt; ğŸš€æœ€æ–°ç‰ˆæœ¬ v3.0 ç°å·²å‘å¸ƒï¼Œæ¬¢è¿ä½“éªŒï¼

æœ¬å·¥å…·å¯ä»¥å¸®åŠ©æ‚¨ä»[**å›½å®¶ä¸­å°å­¦æ™ºæ…§æ•™è‚²å¹³å°**](https://basic.smartedu.cn/tchMaterial/)è·å–ç”µå­è¯¾æœ¬çš„ PDF æ–‡ä»¶ç½‘å€å¹¶è¿›è¡Œä¸‹è½½ï¼Œè®©æ‚¨æ›´æ–¹ä¾¿åœ°è·å–è¯¾æœ¬å†…å®¹ã€‚

&gt; [!NOTE]
&gt;
&gt; è‡ª**2025 å¹´ 2 æœˆ**èµ·ï¼Œå›½å®¶ä¸­å°å­¦æ™ºæ…§æ•™è‚²å¹³å°**éœ€è¦ç™»å½•**æ‰èƒ½è®¿é—®ç”µå­è¯¾æœ¬èµ„æºï¼Œç”¨æˆ·éœ€æä¾› **Access Token**ï¼ˆå³ç™»å½•å‡­æ®ï¼‰æ‰å¯æ­£å¸¸ä½¿ç”¨æœ¬å·¥å…·çš„ä¸‹è½½åŠŸèƒ½ã€‚
&gt;
&gt; **ğŸ‘‰è¯·å…ˆæŒ‰ç…§[ä¸‹æ–¹æŒ‡å—](#2-è®¾ç½®-access-token)è®¾ç½® Access Tokenï¼Œå¦åˆ™ç¨‹åºå°†æ— æ³•è§£æèµ„æºï¼**

## âœ¨å·¥å…·ç‰¹ç‚¹

- **æ”¯æŒ Access Token ç™»å½•**ğŸ”‘ï¼šæ”¯æŒç”¨æˆ·æ‰‹åŠ¨è¾“å…¥ Access Tokenï¼Œåœ¨ Windows æ“ä½œç³»ç»Ÿä¸‹ä¼šå­˜å…¥æ³¨å†Œè¡¨ï¼Œä¸‹æ¬¡å¯åŠ¨å¯è‡ªåŠ¨åŠ è½½ã€‚
- **æ”¯æŒæ‰¹é‡ä¸‹è½½**ğŸ“šï¼šä¸€æ¬¡è¾“å…¥å¤šä¸ªç”µå­è¯¾æœ¬é¢„è§ˆé¡µé¢ç½‘å€ï¼Œå³å¯æ‰¹é‡ä¸‹è½½ PDF è¯¾æœ¬æ–‡ä»¶ã€‚
- **è‡ªåŠ¨æ–‡ä»¶å‘½å**ğŸ“‚ï¼šç¨‹åºä¼šè‡ªåŠ¨ä½¿ç”¨æ•™æåç§°ä½œä¸ºæ–‡ä»¶åï¼Œæ–¹ä¾¿ç®¡ç†ä¸‹è½½çš„è¯¾æœ¬æ–‡ä»¶ã€‚
- **é«˜ DPI é€‚é…**ğŸ–¥ï¸ï¼šä¼˜åŒ– UI ä»¥é€‚é…é«˜åˆ†è¾¨ç‡å±å¹•ï¼Œé¿å…ç•Œé¢æ¨¡ç³Šé—®é¢˜ã€‚
- **ä¸‹è½½è¿›åº¦å¯è§†åŒ–**ğŸ“Šï¼šå®æ—¶æ˜¾ç¤ºä¸‹è½½è¿›åº¦ï¼Œæ”¯æŒæš‚åœ/æ¢å¤æ“ä½œã€‚
- **è·¨å¹³å°æ”¯æŒ**ğŸ’»ï¼šæ”¯æŒ Windowsã€Linuxã€macOS ç­‰æ“ä½œç³»ç»Ÿï¼ˆéœ€è¦å›¾å½¢ç•Œé¢ï¼‰ã€‚

![ç¨‹åºæˆªå›¾](./res/PixPin_2025-03-14_23-44-26.png)

## â¬‡ï¸ä¸‹è½½ä¸å®‰è£…æ–¹æ³•

### GitHub Releases é¡µé¢

ç”±äºæˆ‘ä»¬çš„ç²¾åŠ›æœ‰é™ï¼Œæœ¬é¡¹ç›®çš„ [GitHub Releases é¡µé¢](https://github.com/happycola233/tchMaterial-parser/releases)**ä»…ä¼šå‘å¸ƒé€‚ç”¨äº Windows ä¸ Linux æ“ä½œç³»ç»Ÿçš„ x64 æ¶æ„**çš„ç¨‹åºã€‚

åœ¨ä¸‹è½½å®Œæˆä¹‹åï¼Œå³å¯è¿è¡Œæœ¬ç¨‹åºï¼Œä¸éœ€è¦é¢å¤–çš„å®‰è£…æ­¥éª¤ã€‚

### Arch ç”¨æˆ·è½¯ä»¶ä»“åº“ï¼ˆAURï¼‰

å¯¹äº **Arch Linux** æ“ä½œç³»ç»Ÿï¼Œæœ¬ç¨‹åºå·²å‘å¸ƒè‡³[Arch ç”¨æˆ·è½¯ä»¶ä»“åº“](https://aur.archlinux.org/packages/tchmaterial-parser)ï¼Œå› æ­¤æ‚¨è¿˜å¯ä»¥é€šè¿‡åœ¨ç»ˆç«¯ä¸­è¾“å…¥ä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š

```sh
yay -S tchmaterial-parser
```

æ„Ÿè°¢ [@iamzhz](https://github.com/iamzhz) åˆ¶ä½œäº†æœ¬å·¥å…·çš„å‘è¡ŒåŒ…ï¼ˆ[#26](../../issues/26)ï¼‰ï¼

## ğŸ› ï¸ä½¿ç”¨æ–¹æ³•

### 1. è¾“å…¥æ•™æé“¾æ¥ğŸ“¥

å°†ç”µå­è¯¾æœ¬çš„**é¢„è§ˆé¡µé¢ç½‘å€**ç²˜è´´åˆ°ç¨‹åºæ–‡æœ¬æ¡†ä¸­ï¼Œæ”¯æŒå¤šä¸ª URLï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰ã€‚

**ç¤ºä¾‹ç½‘å€**ï¼š

```text
https://basic.smartedu.cn/tchMaterial/detail?contentType=assets_document&amp;contentId=XXXXXX&amp;catalogType=tchMaterial&amp;subCatalog=tchMaterial
```

### 2. è®¾ç½® Access TokenğŸ”‘

è‹¥æ‚¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æœ¬ç¨‹åºï¼Œéœ€ç‚¹å‡» â€œ**è®¾ç½® Token**â€ æŒ‰é’®ï¼Œç²˜è´´ Access Token å¹¶ä¿å­˜ã€‚

1. **æ‰“å¼€æµè§ˆå™¨**ï¼Œè®¿é—®[å›½å®¶ä¸­å°å­¦æ™ºæ…§æ•™è‚²å¹³å°](https://auth.smartedu.cn/uias/login)å¹¶**ç™»å½•è´¦å·**ã€‚
2. æŒ‰ä¸‹ **F12** æˆ– **Ctrl+Shift+I**ï¼Œæˆ–å³é”®â€”â€”æ£€æŸ¥ï¼ˆå®¡æŸ¥å…ƒç´ ï¼‰æ‰“å¼€**å¼€å‘è€…å·¥å…·**ï¼Œé€‰æ‹©**æ§åˆ¶å°ï¼ˆConsoleï¼‰**ã€‚
3. åœ¨æ§åˆ¶å°ç²˜è´´ä»¥ä¸‹ä»£ç åå›è½¦ï¼ˆEnterï¼‰ï¼š

   ```js
   (function() {
     const authKey = Object.keys(localStorage).find(key =&gt; key.startsWith(&quot;ND_UC_AUTH&quot;));
     if (!authKey) {
       console.error(&quot;æœªæ‰¾åˆ° Access Tokenï¼Œè¯·ç¡®ä¿å·²ç™»å½•ï¼&quot;);
       return;
     }
     const tokenData = JSON.parse(localStorage.getItem(authKey));
     const accessToken = JSON.parse(tokenData.value).access_token;
     console.log(&quot;%cAccess Token: &quot;, &quot;color: green; font-weight: bold&quot;, accessToken);
   })();
   ```
  
4. å¤åˆ¶æ§åˆ¶å°è¾“å‡ºçš„ **Access Token**ï¼Œç„¶ååœ¨æœ¬ç¨‹åºä¸­ç‚¹å‡» â€œ**è®¾ç½® Token**â€ æŒ‰é’®ï¼Œç²˜è´´å¹¶ä¿å­˜ Tokenã€‚

&gt; [!NOTE]
&gt; Access Token å¯èƒ½ä¼šè¿‡æœŸï¼Œè‹¥ä¸‹è½½å¤±è´¥æç¤º **401 Unauthorized**ï¼Œè¯·é‡æ–°è·å–å¹¶è®¾ç½®æ–°çš„ Tokenã€‚

### 3. å¼€å§‹ä¸‹è½½ğŸš€

ç‚¹å‡» â€œ**ä¸‹è½½**â€ æŒ‰é’®ï¼Œç¨‹åºå°†è‡ªåŠ¨è§£æå¹¶ä¸‹è½½ PDF è¯¾æœ¬ã€‚

æœ¬å·¥å…·æ”¯æŒ**æ‰¹é‡ä¸‹è½½**ï¼Œæ‰€æœ‰ PDF æ–‡ä»¶ä¼šè‡ªåŠ¨æŒ‰è¯¾æœ¬åç§°å‘½åå¹¶ä¿å­˜åœ¨é€‰å®šç›®å½•ä¸­ã€‚

## â“å¸¸è§é—®é¢˜

### 1. ä¸ºä»€ä¹ˆä¸‹è½½å¤±è´¥ï¼Ÿâš ï¸

- æ£€æŸ¥æ˜¯å¦å·²[**æ­£ç¡®è®¾ç½® Access Token**](#2-è®¾ç½®-access-token)ğŸ”‘ï¼Œä¸”æ²¡æœ‰è¿‡æœŸã€‚
- **ç¡®è®¤ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸**ğŸŒï¼Œæœ‰æ—¶ç½‘ç»œä¸ç¨³å®šå¯èƒ½å¯¼è‡´ä¸‹è½½å¤±è´¥ã€‚
- **ç¡®ä¿è¾“å…¥çš„ç½‘å€æœ‰æ•ˆ**ğŸ”—ï¼Œéƒ¨åˆ†æ—§èµ„æºå¯èƒ½å·²è¢«ç§»é™¤ã€‚

### 2. Access Token ä¿å­˜åœ¨å“ªé‡Œï¼ŸğŸ’¾

- **Windows æ“ä½œç³»ç»Ÿ**ï¼šToken ä¼šå­˜å‚¨åœ¨**æ³¨å†Œè¡¨** `HKEY_CURRENT_USER\Software\tchMaterial-parser` é¡¹ä¸­çš„ `AccessToken` å€¼ã€‚
- **Linux æ“ä½œç³»ç»Ÿ**: Token ä¼šå­˜å‚¨åœ¨ `~/.config/tchMaterial-parser/data.json` çš„æ–‡ä»¶ä¸­ã€‚
- **macOS ç­‰æ“ä½œç³»ç»Ÿ**ï¼šToken ä»…åœ¨è¿è¡Œæ—¶ä¸´æ—¶å­˜å‚¨äºå†…å­˜ï¼Œä¸ä¼šè‡ªåŠ¨ä¿å­˜ï¼Œç¨‹åºé‡å¯åéœ€é‡æ–°è¾“å…¥ï¼Œç›®å‰æˆ‘ä»¬æ­£åœ¨åŠªåŠ›æ”¹è¿›è¯¥åŠŸèƒ½ã€‚

### 3. Token ä¼šä¸ä¼šæ³„éœ²ï¼ŸğŸ”

- æœ¬ç¨‹åº**ä¸ä¼šä¸Šä¼ ** Tokenï¼Œä¹Ÿä¸ä¼šå­˜å‚¨åœ¨äº‘ç«¯ï¼Œä»…ç”¨äºæœ¬åœ°è¯·æ±‚æˆæƒã€‚
- **è¯·å‹¿åœ¨å…¬å¼€åœºåˆåˆ†äº« Token**ï¼Œä»¥å…æ‚¨çš„è´¦å·è¢«ä»–äººä½¿ç”¨ï¼Œé€ æˆä¸¥é‡åæœã€‚

## â­Star History

[![Star History Chart](https://api.star-history.com/svg?repos=happycola233/tchMaterial-parser&amp;type=Date)](https://star-history.com/#happycola233/tchMaterial-parser&amp;Date)

## ğŸ¤è´¡çŒ®æŒ‡å—

å¦‚æœæ‚¨å‘ç° Bug æˆ–æœ‰æ”¹è¿›å»ºè®®ï¼Œæ¬¢è¿æäº¤ **Issue** æˆ– **Pull Request**ï¼Œè®©æˆ‘ä»¬ä¸€èµ·å®Œå–„æœ¬å·¥å…·ï¼

## ğŸ“œè®¸å¯è¯

æœ¬é¡¹ç›®åŸºäº [MIT è®¸å¯è¯](LICENSE)ï¼Œæ¬¢è¿è‡ªç”±ä½¿ç”¨å’ŒäºŒæ¬¡å¼€å‘ã€‚

## ğŸ’Œå‹æƒ…é“¾æ¥

- ğŸ“šæ‚¨ä¹Ÿå¯ä»¥åœ¨ [ChinaTextbook](https://github.com/TapXWorld/ChinaTextbook) é¡¹ç›®ä¸­ä¸‹è½½å½’æ¡£çš„æ•™æ PDFã€‚
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[trycua/cua]]></title>
            <link>https://github.com/trycua/cua</link>
            <guid>https://github.com/trycua/cua</guid>
            <pubDate>Sun, 18 May 2025 00:04:31 GMT</pubDate>
            <description><![CDATA[c/ua is the Docker Container for Computer-Use AI Agents.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/trycua/cua">trycua/cua</a></h1>
            <p>c/ua is the Docker Container for Computer-Use AI Agents.</p>
            <p>Language: Python</p>
            <p>Stars: 7,325</p>
            <p>Forks: 292</p>
            <p>Stars today: 223 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; alt=&quot;Cua logo&quot; height=&quot;150&quot; srcset=&quot;img/logo_white.png&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; alt=&quot;Cua logo&quot; height=&quot;150&quot; srcset=&quot;img/logo_black.png&quot;&gt;
    &lt;img alt=&quot;Cua logo&quot; height=&quot;150&quot; src=&quot;img/logo_black.png&quot;&gt;
  &lt;/picture&gt;

  [![Python](https://img.shields.io/badge/Python-333333?logo=python&amp;logoColor=white&amp;labelColor=333333)](#)
  [![Swift](https://img.shields.io/badge/Swift-F05138?logo=swift&amp;logoColor=white)](#)
  [![macOS](https://img.shields.io/badge/macOS-000000?logo=apple&amp;logoColor=F0F0F0)](#)
  [![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?&amp;logo=discord&amp;logoColor=white)](https://discord.com/invite/mVnXXpdE85)
  &lt;br&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/13685&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13685&quot; alt=&quot;trycua%2Fcua | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

**c/ua** (pronounced &quot;koo-ah&quot;) enables AI agents to control full operating systems in high-performance virtual containers with near-native speed on Apple Silicon.

&lt;div align=&quot;center&quot;&gt;
  &lt;video src=&quot;https://github.com/user-attachments/assets/c619b4ea-bb8e-4382-860e-f3757e36af20&quot; width=&quot;800&quot; controls&gt;&lt;/video&gt;
&lt;/div&gt;

# ğŸš€ Quick Start

Get started with a Computer-Use Agent UI and a VM with a single command:


```bash
/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/trycua/cua/main/scripts/playground.sh)&quot;
```


This script will:
- Install Lume CLI for VM management (if needed)
- Pull the latest macOS CUA image (if needed)
- Set up Python environment and install/update required packages
- Launch the Computer-Use Agent UI

#### Supported [Agent Loops](https://github.com/trycua/cua/blob/main/libs/agent/README.md#agent-loops)
- [UITARS-1.5](https://github.com/trycua/cua/blob/main/libs/agent/README.md#agent-loops) - Run locally on Apple Silicon with MLX, or use cloud providers
- [OpenAI CUA](https://github.com/trycua/cua/blob/main/libs/agent/README.md#agent-loops) - Use OpenAI&#039;s Computer-Use Preview model
- [Anthropic CUA](https://github.com/trycua/cua/blob/main/libs/agent/README.md#agent-loops) - Use Anthropic&#039;s Computer-Use capabilities
- [OmniParser-v2.0](https://github.com/trycua/cua/blob/main/libs/agent/README.md#agent-loops) - Control UI with [Set-of-Marks prompting](https://som-gpt4v.github.io/) using any vision model

### System Requirements

- Mac with Apple Silicon (M1/M2/M3/M4 series)
- macOS 15 (Sequoia) or newer
- Disk space for VM images (30GB+ recommended)


# ğŸ’» For Developers

### Step 1: Install Lume CLI

```bash
/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/trycua/cua/main/libs/lume/scripts/install.sh)&quot;
```

Lume CLI manages high-performance macOS/Linux VMs with near-native speed on Apple Silicon.

### Step 2: Pull the macOS CUA Image

```bash
lume pull macos-sequoia-cua:latest
```

The macOS CUA image contains the default Mac apps and the Computer Server for easy automation.

### Step 3: Install Python SDK

```bash
pip install &quot;cua-computer[all]&quot; &quot;cua-agent[all]&quot;
```

Alternatively, see the [Developer Guide](./docs/Developer-Guide.md) for building from source.

### Step 4: Use in Your Code

```python
from computer import Computer
from agent import ComputerAgent, LLM

async def main():
    # Start a local macOS VM with a 1024x768 display
    async with Computer(os_type=&quot;macos&quot;, display=&quot;1024x768&quot;) as computer:

        # Example: Direct control of a macOS VM with Computer
        await computer.interface.left_click(100, 200)
        await computer.interface.type_text(&quot;Hello, world!&quot;)
        screenshot_bytes = await computer.interface.screenshot()
        
        # Example: Create and run an agent locally using mlx-community/UI-TARS-1.5-7B-6bit
        agent = ComputerAgent(
          computer=computer,
          loop=&quot;UITARS&quot;,
          model=LLM(provider=&quot;MLXVLM&quot;, name=&quot;mlx-community/UI-TARS-1.5-7B-6bit&quot;)
        )
        await agent.run(&quot;Find the trycua/cua repository on GitHub and follow the quick start guide&quot;)

main()
```

For ready-to-use examples, check out our [Notebooks](./notebooks/) collection.

### Lume CLI Reference

```bash
# Install Lume CLI and background service
curl -fsSL https://raw.githubusercontent.com/trycua/cua/main/libs/lume/scripts/install.sh | bash

# List all VMs
lume ls

# Pull a VM image
lume pull macos-sequoia-cua:latest

# Create a new VM
lume create my-vm --os macos --cpu 4 --memory 8GB --disk-size 50GB

# Run a VM (creates and starts if it doesn&#039;t exist)
lume run macos-sequoia-cua:latest

# Stop a VM
lume stop macos-sequoia-cua_latest

# Delete a VM
lume delete macos-sequoia-cua_latest
```

### Lumier CLI Reference

For advanced container-like virtualization, check out [Lumier](./libs/lumier/README.md) - a Docker interface for macOS and Linux VMs.

```bash
# Install Lume CLI and background service
curl -fsSL https://raw.githubusercontent.com/trycua/cua/main/libs/lume/scripts/install.sh | bash

# Run macOS in a Docker container
docker run -it --rm \
    --name lumier-vm \
    -p 8006:8006 \
    -v $(pwd)/storage:/storage \
    -v $(pwd)/shared:/shared \
    -e VM_NAME=lumier-vm \
    -e VERSION=ghcr.io/trycua/macos-sequoia-cua:latest \
    -e CPU_CORES=4 \
    -e RAM_SIZE=8192 \
    -e HOST_STORAGE_PATH=$(pwd)/storage \
    -e HOST_SHARED_PATH=$(pwd)/shared \
    trycua/lumier:latest
```

## Resources

- [How to use the MCP Server with Claude Desktop or other MCP clients](./libs/mcp-server/README.md) - One of the easiest ways to get started with C/ua
- [How to use OpenAI Computer-Use, Anthropic, OmniParser, or UI-TARS for your Computer-Use Agent](./libs/agent/README.md)
- [How to use Lume CLI for managing desktops](./libs/lume/README.md)
- [Training Computer-Use Models: Collecting Human Trajectories with C/ua (Part 1)](https://www.trycua.com/blog/training-computer-use-models-trajectories-1)
- [Build Your Own Operator on macOS (Part 1)](https://www.trycua.com/blog/build-your-own-operator-on-macos-1)

## Modules

| Module | Description | Installation |
|--------|-------------|---------------|
| [**Lume**](./libs/lume/README.md) | VM management for macOS/Linux using Apple&#039;s Virtualization.Framework | `curl -fsSL https://raw.githubusercontent.com/trycua/cua/main/libs/lume/scripts/install.sh \| bash` |
| [**Lumier**](./libs/lumier/README.md) | Docker interface for macOS and Linux VMs | `docker pull trycua/lumier:latest` |
| [**Computer**](./libs/computer/README.md) | Interface for controlling virtual machines | `pip install &quot;cua-computer[all]&quot;` |
| [**Agent**](./libs/agent/README.md) | AI agent framework for automating tasks | `pip install &quot;cua-agent[all]&quot;` |
| [**MCP Server**](./libs/mcp-server/README.md) | MCP server for using CUA with Claude Desktop | `pip install cua-mcp-server` |
| [**SOM**](./libs/som/README.md) | Self-of-Mark library for Agent | `pip install cua-som` |
| [**PyLume**](./libs/pylume/README.md) | Python bindings for Lume | `pip install pylume` |
| [**Computer Server**](./libs/computer-server/README.md) | Server component for Computer | `pip install cua-computer-server` |
| [**Core**](./libs/core/README.md) | Core utilities | `pip install cua-core` |

## Computer Interface Reference

For complete examples, see [computer_examples.py](./examples/computer_examples.py) or [computer_nb.ipynb](./notebooks/computer_nb.ipynb)

```python
# Mouse Actions
await computer.interface.left_click(x, y)       # Left click at coordinates
await computer.interface.right_click(x, y)      # Right click at coordinates
await computer.interface.double_click(x, y)     # Double click at coordinates
await computer.interface.move_cursor(x, y)      # Move cursor to coordinates
await computer.interface.drag_to(x, y, duration)  # Drag to coordinates
await computer.interface.get_cursor_position()  # Get current cursor position

# Keyboard Actions
await computer.interface.type_text(&quot;Hello&quot;)     # Type text
await computer.interface.press_key(&quot;enter&quot;)     # Press a single key
await computer.interface.hotkey(&quot;command&quot;, &quot;c&quot;) # Press key combination

# Screen Actions
await computer.interface.screenshot()           # Take a screenshot
await computer.interface.get_screen_size()      # Get screen dimensions

# Clipboard Actions
await computer.interface.set_clipboard(text)    # Set clipboard content
await computer.interface.copy_to_clipboard()    # Get clipboard content

# File System Operations
await computer.interface.file_exists(path)      # Check if file exists
await computer.interface.directory_exists(path) # Check if directory exists
await computer.interface.run_command(cmd)       # Run shell command

# Accessibility
await computer.interface.get_accessibility_tree() # Get accessibility tree
```

## ComputerAgent Reference

For complete examples, see [agent_examples.py](./examples/agent_examples.py) or [agent_nb.ipynb](./notebooks/agent_nb.ipynb)

```python
# Import necessary components
from agent import ComputerAgent, LLM, AgentLoop, LLMProvider

# UI-TARS-1.5 agent for local execution with MLX
ComputerAgent(loop=AgentLoop.UITARS, model=LLM(provider=LLMProvider.MLXVLM, name=&quot;mlx-community/UI-TARS-1.5-7B-6bit&quot;))   
# OpenAI Computer-Use agent using OPENAI_API_KEY  
ComputerAgent(loop=AgentLoop.OPENAI, model=LLM(provider=LLMProvider.OPENAI, name=&quot;computer-use-preview&quot;))
# Anthropic Claude agent using ANTHROPIC_API_KEY
ComputerAgent(loop=AgentLoop.ANTHROPIC, model=LLM(provider=LLMProvider.ANTHROPIC))

# OmniParser loop for UI control using Set-of-Marks (SOM) prompting and any vision LLM
ComputerAgent(loop=AgentLoop.OMNI, model=LLM(provider=LLMProvider.OLLAMA, name=&quot;gemma3:12b-it-q4_K_M&quot;))      
# OpenRouter example using OAICOMPAT provider
ComputerAgent(
    loop=AgentLoop.OMNI,
    model=LLM(
        provider=LLMProvider.OAICOMPAT, 
        name=&quot;openai/gpt-4o-mini&quot;,
        provider_base_url=&quot;https://openrouter.ai/api/v1&quot;
    ),
    api_key=&quot;your-openrouter-api-key&quot;
)
```

## Demos

Check out these demos of the Computer-Use Agent in action:

&lt;details open&gt;
&lt;summary&gt;&lt;b&gt;MCP Server: Work with Claude Desktop and Tableau&lt;/b&gt;&lt;/summary&gt;
&lt;br&gt;
&lt;div align=&quot;center&quot;&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/9f573547-5149-493e-9a72-396f3cff29df&quot; width=&quot;800&quot; controls&gt;&lt;/video&gt;
&lt;/div&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;AI-Gradio: Multi-app workflow with browser, VS Code and terminal&lt;/b&gt;&lt;/summary&gt;
&lt;br&gt;
&lt;div align=&quot;center&quot;&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/723a115d-1a07-4c8e-b517-88fbdf53ed0f&quot; width=&quot;800&quot; controls&gt;&lt;/video&gt;
&lt;/div&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Notebook: Fix GitHub issue in Cursor&lt;/b&gt;&lt;/summary&gt;
&lt;br&gt;
&lt;div align=&quot;center&quot;&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/f67f0107-a1e1-46dc-aa9f-0146eb077077&quot; width=&quot;800&quot; controls&gt;&lt;/video&gt;
&lt;/div&gt;
&lt;/details&gt;

## Community

Join our [Discord community](https://discord.com/invite/mVnXXpdE85) to discuss ideas, get assistance, or share your demos!

## License

Cua is open-sourced under the MIT License - see the [LICENSE](LICENSE) file for details.

Microsoft&#039;s OmniParser, which is used in this project, is licensed under the Creative Commons Attribution 4.0 International License (CC-BY-4.0) - see the [OmniParser LICENSE](https://github.com/microsoft/OmniParser/blob/master/LICENSE) file for details.

## Contributing

We welcome contributions to CUA! Please refer to our [Contributing Guidelines](CONTRIBUTING.md) for details.

## Trademarks

Apple, macOS, and Apple Silicon are trademarks of Apple Inc. Ubuntu and Canonical are registered trademarks of Canonical Ltd. Microsoft is a registered trademark of Microsoft Corporation. This project is not affiliated with, endorsed by, or sponsored by Apple Inc., Canonical Ltd., or Microsoft Corporation.

## Stargazers

Thank you to all our supporters!

[![Stargazers over time](https://starchart.cc/trycua/cua.svg?variant=adaptive)](https://starchart.cc/trycua/cua)

## Contributors

&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt;
&lt;!-- prettier-ignore-start --&gt;
&lt;!-- markdownlint-disable --&gt;
&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/f-trycua&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/195596869?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;f-trycua&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;f-trycua&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-f-trycua&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;http://pepicrft.me&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/663605?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Pedro PiÃ±era BuendÃ­a&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Pedro PiÃ±era BuendÃ­a&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-pepicrft&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://iamit.in&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/5647941?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Amit Kumar&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Amit Kumar&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-aktech&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://productsway.com/&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/870029?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Dung Duc Huynh (Kaka)&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Dung Duc Huynh (Kaka)&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-jellydn&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;http://zaydkrunz.com&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/70227235?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Zayd Krunz&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Zayd Krunz&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-ShrootBuck&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/PrashantRaj18198&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/23168997?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Prashant Raj&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Prashant Raj&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-PrashantRaj18198&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://www.mobile.dev&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/847683?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Leland Takamine&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Leland Takamine&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-Leland-Takamine&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/ddupont808&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/3820588?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;ddupont&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;ddupont&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-ddupont808&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/Lizzard1123&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/46036335?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Ethan Gutierrez&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ethan Gutierrez&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-Lizzard1123&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://ricterz.me&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/5282759?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Ricter Zheng&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ricter Zheng&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-RicterZ&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://www.trytruffle.ai/&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/50844303?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Rahul Karajgikar&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Rahul Karajgikar&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-rahulkarajgikar&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/trospix&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/81363696?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;trospix&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;trospix&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-trospix&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://wavee.world/invitation/b96d00e6-b802-4a1b-8a66-2e3854a01ffd&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/22633385?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Ikko Eltociear Ashimine&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ikko Eltociear Ashimine&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-eltociear&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/dp221125&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/10572119?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;í•œì„í˜¸(MilKyo)&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;í•œì„í˜¸(MilKyo)&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-dp221125&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://www.encona.com/&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/891558?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Rahim Nathwani&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Rahim Nathwani&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-rahimnathwani&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://mjspeck.github.io/&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/20689127?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Matt Speck&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Matt Speck&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-mjspeck&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/FinnBorge&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/9272726?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;FinnBorge&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;FinnBorge&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-FinnBorge&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/jklapacz&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/5343758?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Jakub Klapacz&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Jakub Klapacz&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#code-jklapacz&quot; title=&quot;Code&quot;&gt;ğŸ’»&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;!-- markdownlint-restore --&gt;
&lt;!-- prettier-ignore-end --&gt;

&lt;!-- ALL-CONTRIBUTORS-LIST:END --&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[xming521/WeClone]]></title>
            <link>https://github.com/xming521/WeClone</link>
            <guid>https://github.com/xming521/WeClone</guid>
            <pubDate>Sun, 18 May 2025 00:04:30 GMT</pubDate>
            <description><![CDATA[ğŸš€ä»èŠå¤©è®°å½•åˆ›é€ æ•°å­—åˆ†èº«çš„ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆğŸ’¡ ä½¿ç”¨èŠå¤©è®°å½•å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ï¼Œè®©å¤§æ¨¡å‹æœ‰â€œé‚£å‘³å„¿â€ï¼Œå¹¶ç»‘å®šåˆ°èŠå¤©æœºå™¨äººï¼Œå®ç°è‡ªå·±çš„æ•°å­—åˆ†èº«ã€‚ æ•°å­—å…‹éš†/æ•°å­—åˆ†èº«/æ•°å­—æ°¸ç”Ÿ/LLM/èŠå¤©æœºå™¨äºº/LoRA]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/xming521/WeClone">xming521/WeClone</a></h1>
            <p>ğŸš€ä»èŠå¤©è®°å½•åˆ›é€ æ•°å­—åˆ†èº«çš„ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆğŸ’¡ ä½¿ç”¨èŠå¤©è®°å½•å¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ï¼Œè®©å¤§æ¨¡å‹æœ‰â€œé‚£å‘³å„¿â€ï¼Œå¹¶ç»‘å®šåˆ°èŠå¤©æœºå™¨äººï¼Œå®ç°è‡ªå·±çš„æ•°å­—åˆ†èº«ã€‚ æ•°å­—å…‹éš†/æ•°å­—åˆ†èº«/æ•°å­—æ°¸ç”Ÿ/LLM/èŠå¤©æœºå™¨äºº/LoRA</p>
            <p>Language: Python</p>
            <p>Stars: 10,088</p>
            <p>Forks: 775</p>
            <p>Stars today: 634 stars today</p>
            <h2>README</h2><pre>![download](https://github.com/user-attachments/assets/5842e84e-004f-4afd-9373-af64e9575b78)
&lt;h3 align=&quot;center&quot;&gt;ğŸš€ä»èŠå¤©è®°å½•åˆ›é€ æ•°å­—åˆ†èº«çš„ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆğŸ’¡&lt;/h3&gt;  

&lt;div align=&quot;center&quot;&gt;

[![GitHub stars](https://img.shields.io/github/stars/xming521/WeClone?style=for-the-badge&amp;logo=github&amp;label=Stars&amp;logoColor=white&amp;color=ffda65)](https://github.com/xming521/WeClone/stargazers)
[![GitHub release](https://img.shields.io/github/v/release/xming521/WeClone?style=for-the-badge&amp;logo=github&amp;label=Release&amp;logoColor=white&amp;color=06d094)](https://github.com/xming521/WeClone/releases)
&lt;a href=&quot;https://qm.qq.com/cgi-bin/qm/qr?k=wNdgbOVT6oFOJ2wlMLsolUXErW9ESLpk&amp;jump_from=webapi&amp;authKey=z/reOp6YLyvR4Tl2k2nYMsLoMC3w9/99ucgKMX0oRGlxDV/WbYnvq2QxODoIkfxn&quot; target=&quot;_blank&quot; style=&quot;text-decoration: none;&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/QQç¾¤-708067078-12B7F5?style=for-the-badge&amp;logo=qq&amp;logoColor=white&quot; alt=&quot;WeCloneâ‘ &quot; title=&quot;WeCloneâ‘ &quot;&gt;
&lt;/a&gt;
[![Telegram](https://img.shields.io/badge/Telegram-2CA5E0?style=for-the-badge&amp;logo=telegram&amp;logoColor=white)](https://t.me/+JEdak4m0XEQ3NGNl)

&lt;a href=&quot;https://hellogithub.com/repository/12ab209b56cb4cfd885c8cfd4cfdd53e&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=12ab209b56cb4cfd885c8cfd4cfdd53e&amp;claim_uid=RThlPDoGrFvdMY5&quot; alt=&quot;Featuredï½œHelloGitHub&quot; style=&quot;width: 150px; height: 28px;&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://trendshift.io/repositories/13759&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13759&quot; alt=&quot;xming521%2FWeClone | Trendshift&quot; style=&quot;width: 220px; height: 50px;&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://deepwiki.com/xming521/WeClone&quot;&gt;&lt;img src=&quot;https://deepwiki.com/badge.svg&quot; alt=&quot;Ask DeepWiki&quot;  style=&quot;width: 134px; height: 23px;margin-bottom: 3px;&quot;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://blog.051088.xyz/2025/05/14/WeClone-%E7%94%A8%E5%BE%AE%E4%BF%A1%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84AI%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB/&quot; target=&quot;_blank&quot;&gt;
    Windowséƒ¨ç½²æŒ‡å—
  &lt;/a&gt;
&lt;/p&gt;

## âœ¨æ ¸å¿ƒåŠŸèƒ½
- ğŸ’« æ¶µç›–æ‰“é€ æ•°å­—åˆ†èº«çš„å…¨é“¾è·¯æ–¹æ¡ˆï¼ŒåŒ…æ‹¬èŠå¤©æ•°æ®å¯¼å‡ºã€é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€éƒ¨ç½²
- ğŸ’¬ ä½¿ç”¨å¾®ä¿¡èŠå¤©è®°å½•å¾®è°ƒLLMï¼Œè®©å¤§æ¨¡å‹æœ‰&quot;é‚£å‘³å„¿&quot;
- ğŸ”— ç»‘å®šåˆ°å¾®ä¿¡ã€QQã€Telegramã€ä¼å¾®ã€é£ä¹¦æœºå™¨äººï¼Œå®ç°è‡ªå·±çš„æ•°å­—åˆ†èº«
- ğŸ›¡ï¸ éšç§ä¿¡æ¯è¿‡æ»¤ï¼Œæœ¬åœ°åŒ–å¾®è°ƒéƒ¨ç½²ï¼Œæ•°æ®å®‰å…¨å¯æ§

## ğŸ“‹ç‰¹æ€§ä¸è¯´æ˜

&gt; [!IMPORTANT]
&gt; ### 0.2.1ç‰ˆæœ¬æ”¯æŒäº†å‘½ä»¤è¡Œå·¥å…·ï¼Œä½¿ç”¨å‰éœ€è¦é‡æ–°æ‰§è¡Œ `uv pip install -e .` 

&gt; [!IMPORTANT]
&gt; 0.2.0ç‰ˆæœ¬è¿›è¡Œäº†å…¨é¢é‡æ„ï¼Œæ•°æ®é›†ç›®å½•å’Œè„šæœ¬è·¯å¾„å…¨éƒ¨è¿›è¡Œäº†ä¿®æ”¹ï¼Œæ‹‰å–æ–°ä»£ç åï¼Œ`csv`æ–‡ä»¶å¤¹æ”¾åœ¨`dataset`ä¸‹ï¼Œå¹¶ä¸”éœ€è¦é‡æ–°å®‰è£…ä¾èµ–ã€‚

&gt; [!IMPORTANT]
&gt; - WeCloneä»åœ¨å¿«é€Ÿè¿­ä»£æœŸï¼Œå½“å‰æ•ˆæœä¸ä»£è¡¨æœ€ç»ˆæ•ˆæœã€‚  
&gt; - å¾®è°ƒLLMæ•ˆæœå¾ˆå¤§ç¨‹åº¦å–å†³äºæ¨¡å‹å¤§å°ã€èŠå¤©æ•°æ®çš„æ•°é‡å’Œè´¨é‡ï¼Œç†è®ºä¸Šæ¨¡å‹è¶Šå¤§ï¼Œæ•°æ®è¶Šå¤šï¼Œæ•ˆæœè¶Šå¥½ã€‚   
&gt; - Windowsç¯å¢ƒæœªè¿›è¡Œä¸¥æ ¼æµ‹è¯•ï¼Œå¯ä»¥ä½¿ç”¨WSLä½œä¸ºè¿è¡Œç¯å¢ƒã€‚è¯¦ç»†æ•™ç¨‹å¯ç‚¹å‡»[Windowséƒ¨ç½²æŒ‡å—](https://blog.051088.xyz/2025/05/14/WeClone-%E7%94%A8%E5%BE%AE%E4%BF%A1%E8%81%8A%E5%A4%A9%E8%AE%B0%E5%BD%95%E6%89%93%E9%80%A0%E8%87%AA%E5%B7%B1%E7%9A%84AI%E6%95%B0%E5%AD%97%E5%88%86%E8%BA%AB/)æŸ¥çœ‹ã€‚

### ç¡¬ä»¶è¦æ±‚

é¡¹ç›®é»˜è®¤ä½¿ç”¨Qwen2.5-7B-Instructæ¨¡å‹ï¼ŒLoRAæ–¹æ³•å¯¹sfté˜¶æ®µå¾®è°ƒï¼Œå¤§çº¦éœ€è¦16GBæ˜¾å­˜ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨[LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md#%E6%A8%A1%E5%9E%8B)æ”¯æŒçš„å…¶ä»–æ¨¡å‹å’Œæ–¹æ³•ã€‚

éœ€è¦æ˜¾å­˜çš„ä¼°ç®—å€¼ï¼š
| æ–¹æ³•                             | ç²¾åº¦ |   7B  |  14B  |  30B  |   70B  |   `x`B  |
| ------------------------------- | ---- | ----- | ----- | ----- | ------ | ------- |
| Full (`bf16` or `fp16`)         |  32  | 120GB | 240GB | 600GB | 1200GB | `18x`GB |
| Full (`pure_bf16`)              |  16  |  60GB | 120GB | 300GB |  600GB |  `8x`GB |
| Freeze/LoRA/GaLore/APOLLO/BAdam |  16  |  16GB |  32GB |  64GB |  160GB |  `2x`GB |
| QLoRA                           |   8  |  10GB |  20GB |  40GB |   80GB |   `x`GB |
| QLoRA                           |   4  |   6GB |  12GB |  24GB |   48GB | `x/2`GB |
| QLoRA                           |   2  |   4GB |   8GB |  16GB |   24GB | `x/4`GB |


## ç¯å¢ƒæ­å»º
1.cudaå®‰è£…(å·²å®‰è£…å¯è·³è¿‡ï¼Œ**è¦æ±‚ç‰ˆæœ¬12.4åŠä»¥ä¸Š**)ï¼š[LLaMA Factory](https://llamafactory.readthedocs.io/zh-cn/latest/getting_started/installation.html#cuda) 

2.å»ºè®®ä½¿ç”¨ [uv](https://docs.astral.sh/uv/)å®‰è£…ä¾èµ–ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸å¿«é€Ÿçš„ Python ç¯å¢ƒç®¡ç†å™¨ã€‚å®‰è£…uvåï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆ›å»ºä¸€ä¸ªæ–°çš„Pythonç¯å¢ƒå¹¶å®‰è£…ä¾èµ–é¡¹ï¼Œæ³¨æ„è¿™ä¸åŒ…å«éŸ³é¢‘å…‹éš†åŠŸèƒ½çš„ä¾èµ–ï¼š
```bash
git clone https://github.com/xming521/WeClone.git
cd WeClone
uv venv .venv --python=3.10
source .venv/bin/activate # windowsä¸‹æ‰§è¡Œ .venv\Scripts\activate
uv pip install --group main -e . 
```
&gt; [!TIP]
&gt; å¦‚æœè¦ä½¿ç”¨æœ€æ–°çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œéœ€è¦æ‰‹åŠ¨å®‰è£…æœ€æ–°ç‰ˆLLaMA Factoryï¼š`uv pip install --upgrade git+https://github.com/hiyouga/LLaMA-Factory.git`,åŒæ—¶å…¶ä»–ä¾èµ–ç‰ˆæœ¬ä¹Ÿå¯èƒ½éœ€è¦ä¿®æ”¹ï¼Œä¾‹å¦‚vllm pytorch transforms

3.å°†é…ç½®æ–‡ä»¶æ¨¡æ¿å¤åˆ¶ä¸€ä»½å¹¶é‡å‘½åä¸º`settings.jsonc`ï¼Œåç»­é…ç½®ä¿®æ”¹åœ¨æ­¤æ–‡ä»¶è¿›è¡Œï¼š
```bash
cp settings.template.jsonc settings.jsonc
```
&gt; [!NOTE]
&gt; è®­ç»ƒä»¥åŠæ¨ç†ç›¸å…³é…ç½®ç»Ÿä¸€åœ¨æ–‡ä»¶`settings.jsonc`

4.ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æµ‹è¯•CUDAç¯å¢ƒæ˜¯å¦æ­£ç¡®é…ç½®å¹¶å¯è¢«PyTorchè¯†åˆ«ï¼ŒMacä¸éœ€è¦ï¼š
```bash
python -c &quot;import torch; print(&#039;CUDAæ˜¯å¦å¯ç”¨:&#039;, torch.cuda.is_available());&quot;
```

5.ï¼ˆå¯é€‰ï¼‰å®‰è£…FlashAttentionï¼ŒåŠ é€Ÿè®­ç»ƒå’Œæ¨ç†ï¼š`uv pip install flash-attn --no-build-isolation`

## æ¨¡å‹ä¸‹è½½
```bash
git lfs install
git clone https://www.modelscope.cn/Qwen/Qwen2.5-7B-Instruct.git
```
ä¸‹è½½æœ‰é—®é¢˜ä½¿ç”¨å…¶ä»–æ–¹å¼ä¸‹è½½ï¼š[æ¨¡å‹çš„ä¸‹è½½](https://www.modelscope.cn/docs/models/download)


## æ•°æ®å‡†å¤‡

è¯·ä½¿ç”¨[PyWxDump](https://github.com/xaoyaoo/PyWxDump)æå–å¾®ä¿¡èŠå¤©è®°å½•ï¼ˆä¸æ”¯æŒ4.0ç‰ˆæœ¬å¾®ä¿¡ï¼‰ã€‚å¯ä»¥å…ˆå°†æ‰‹æœºçš„èŠå¤©è®°å½•è¿ç§»ï¼ˆå¤‡ä»½ï¼‰åˆ°ç”µè„‘ï¼Œæ•°æ®é‡æ›´å¤šä¸€äº›ã€‚ä¸‹è½½è½¯ä»¶å¹¶è§£å¯†æ•°æ®åº“åï¼Œç‚¹å‡»èŠå¤©å¤‡ä»½ï¼Œå¯¼å‡ºç±»å‹ä¸ºCSVï¼Œå¯ä»¥å¯¼å‡ºå¤šä¸ªè”ç³»äººï¼ˆä¸å»ºè®®ä½¿ç”¨ç¾¤èŠè®°å½•ï¼‰ï¼Œç„¶åå°†å¯¼å‡ºçš„ä½äº`wxdump_tmp/export` çš„ `csv` æ–‡ä»¶å¤¹æ”¾åœ¨`./dataset`ç›®å½•å³å¯ï¼Œä¹Ÿå°±æ˜¯ä¸åŒäººèŠå¤©è®°å½•çš„æ–‡ä»¶å¤¹ä¸€èµ·æ”¾åœ¨ `./dataset/csv`ã€‚   

## æ•°æ®é¢„å¤„ç†

- é¡¹ç›®é»˜è®¤å»é™¤äº†æ•°æ®ä¸­çš„æ‰‹æœºå·ã€èº«ä»½è¯å·ã€é‚®ç®±ã€ç½‘å€ã€‚è¿˜åœ¨`settings.jsonc`ä¸­æä¾›äº†ä¸€ä¸ªç¦ç”¨è¯è¯åº“`blocked_words`ï¼Œå¯ä»¥è‡ªè¡Œæ·»åŠ éœ€è¦è¿‡æ»¤çš„è¯å¥ï¼ˆä¼šé»˜è®¤å»æ‰åŒ…æ‹¬ç¦ç”¨è¯çš„æ•´å¥ï¼‰ã€‚
&gt; [!IMPORTANT]
&gt; ğŸš¨ è¯·ä¸€å®šæ³¨æ„ä¿æŠ¤ä¸ªäººéšç§ï¼Œä¸è¦æ³„éœ²ä¸ªäººä¿¡æ¯ï¼

- æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å¯¹æ•°æ®è¿›è¡Œå¤„ç†ï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„èŠå¤©é£æ ¼ä¿®æ”¹settings.jsoncçš„`make_dataset_args`ã€‚
```bash
weclone-cli make-dataset
```
- ç›®å‰ä»…æ”¯æŒæ—¶é—´çª—å£ç­–ç•¥ï¼Œæ ¹æ®`single_combine_time_window`å°†å•äººè¿ç»­æ¶ˆæ¯é€šè¿‡é€—å·è¿æ¥åˆå¹¶ä¸ºä¸€å¥ï¼Œæ ¹æ®`qa_match_time_window`åŒ¹é…é—®ç­”å¯¹ã€‚
- å¯ä»¥å¯ç”¨`clean_dataset`ä¸­çš„`enable_clean`é€‰é¡¹ï¼Œå¯¹æ•°æ®è¿›è¡Œæ¸…æ´—ï¼Œä»¥è¾¾åˆ°æ›´å¥½æ•ˆæœã€‚å½“å‰ä½¿ç”¨llm judgeå¯¹èŠå¤©è®°å½•è¿›è¡Œæ‰“åˆ†ï¼Œä½¿ç”¨vllmè¿›è¡Œç¦»çº¿æ¨ç†ã€‚åœ¨å¾—åˆ°`llmæ‰“åˆ†åˆ†æ•°åˆ†å¸ƒæƒ…å†µ`åï¼Œè°ƒæ•´`accept_score`é€‰æ‹©å¯ä»¥æ¥å—çš„åˆ†æ•°ï¼Œå†é€‚å½“é™ä½`train_sft_args`çš„`lora_dropout`å‚æ•°æå‡æ‹Ÿåˆæ•ˆæœã€‚

## é…ç½®å‚æ•°å¹¶å¾®è°ƒæ¨¡å‹

- (å¯é€‰)ä¿®æ”¹ `settings.jsonc` çš„ `model_name_or_path` å’Œ `template` é€‰æ‹©æœ¬åœ°ä¸‹è½½å¥½çš„å…¶ä»–æ¨¡å‹ã€‚  
- ä¿®æ”¹`per_device_train_batch_size`ä»¥åŠ`gradient_accumulation_steps`æ¥è°ƒæ•´æ˜¾å­˜å ç”¨ã€‚  
- å¯ä»¥æ ¹æ®è‡ªå·±æ•°æ®é›†çš„æ•°é‡å’Œè´¨é‡ä¿®æ”¹`train_sft_args`çš„`num_train_epochs`ã€`lora_rank`ã€`lora_dropout`ç­‰å‚æ•°ã€‚

### å•å¡è®­ç»ƒ
```bash
weclone-cli train-sft
```
å¤šå¡ç¯å¢ƒå•å¡è®­ç»ƒï¼Œéœ€è¦å…ˆæ‰§è¡Œ `export CUDA_VISIBLE_DEVICES=0`

### å¤šå¡è®­ç»ƒ
å–æ¶ˆ`settings.jsonc`ä¸­`deepspeed`è¡Œä»£ç æ³¨é‡Šï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¤šå¡è®­ç»ƒï¼š
```bash
uv pip install deepspeed
deepspeed --num_gpus=ä½¿ç”¨æ˜¾å¡æ•°é‡ weclone/train/train_sft.py
```

### ä½¿ç”¨æµè§ˆå™¨demoç®€å•æ¨ç†
å¯ä»¥åœ¨è¿™ä¸€æ­¥æµ‹è¯•å‡ºåˆé€‚çš„temperatureã€top_på€¼ï¼Œä¿®æ”¹settings.jsoncçš„`infer_args`åï¼Œä¾›åç»­æ¨ç†æ—¶ä½¿ç”¨ã€‚
```bash
weclone-cli webchat-demo
```

### ä½¿ç”¨æ¥å£è¿›è¡Œæ¨ç†

```bash
weclone-cli server
```

### ä½¿ç”¨å¸¸è§èŠå¤©é—®é¢˜æµ‹è¯•
ä¸åŒ…å«è¯¢é—®ä¸ªäººä¿¡æ¯çš„é—®é¢˜ï¼Œä»…æœ‰æ—¥å¸¸èŠå¤©ã€‚æµ‹è¯•ç»“æœåœ¨test_result-my.txtã€‚
```bash
weclone-cli server
weclone-cli test-model
```

## ğŸ–¼ï¸ å¾®è°ƒæ•ˆæœ
ä½¿ç”¨Qwen2.5-14B-Instructæ¨¡å‹ï¼Œå¤§æ¦‚3ä¸‡æ¡å¤„ç†åçš„æœ‰æ•ˆæ•°æ®ï¼Œlossé™åˆ°äº†3.5å·¦å³çš„æ•ˆæœã€‚
&lt;details&gt;
&lt;summary&gt;æˆªå›¾&lt;/summary&gt;
&lt;div style=&quot;display: flex; flex-wrap: wrap; gap: 10px;&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/0775ec52-452b-485f-9785-c6eb7b277132&quot; alt=&quot;alt text&quot; style=&quot;width: 48%; min-width: 150px;&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/8c7628b5-da70-4c37-9e51-fdfb0eadd2df&quot; alt=&quot;alt text&quot; style=&quot;width: 48%; min-width: 150px;&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/523aa742-2aa3-40e9-bd67-b98b336e83a8&quot; alt=&quot;alt text&quot; style=&quot;width: 48%; min-width: 150px;&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/dabf0603-dcc4-4a47-b5c3-2bbc036820d9&quot; alt=&quot;alt text&quot; style=&quot;width: 48%; min-width: 150px;&quot;&gt;
&lt;/div&gt;
&lt;/details&gt;


## ğŸ¤– éƒ¨ç½²åˆ°èŠå¤©æœºå™¨äºº

### AstrBot

[AstrBot](https://github.com/AstrBotDevs/AstrBot) æ˜¯æ˜“ä¸Šæ‰‹çš„å¤šå¹³å° LLM èŠå¤©æœºå™¨äººåŠå¼€å‘æ¡†æ¶ âœ¨ å¹³å°æ”¯æŒ QQã€QQé¢‘é“ã€Telegramã€å¾®ä¿¡ã€ä¼å¾®ã€é£ä¹¦ã€‚      

ä½¿ç”¨æ­¥éª¤ï¼š
1. éƒ¨ç½² AstrBot
2. åœ¨ AstrBot ä¸­éƒ¨ç½²æ¶ˆæ¯å¹³å°
3. æ‰§è¡Œ `weclone-cli server` å¯åŠ¨apiæœåŠ¡
4. åœ¨ AstrBot ä¸­æ–°å¢æœåŠ¡æä¾›å•†ï¼Œç±»å‹é€‰æ‹©OpenAIï¼ŒAPI Base URL æ ¹æ®AstrBotéƒ¨ç½²æ–¹å¼å¡«å†™ï¼ˆä¾‹å¦‚dockeréƒ¨ç½²å¯èƒ½ä¸ºhttp://172.17.0.1:8005/v1ï¼‰ ï¼Œæ¨¡å‹å¡«å†™gpt-3.5-turbo,API Keyéšæ„å¡«å†™ä¸€ä¸ª
5. å¾®è°ƒåä¸æ”¯æŒå·¥å…·è°ƒç”¨ï¼Œè¯·å…ˆå…³æ‰é»˜è®¤çš„å·¥å…·ï¼Œæ¶ˆæ¯å¹³å°å‘é€æŒ‡ä»¤ï¼š `/tool off all`ï¼Œå¦åˆ™ä¼šæ²¡æœ‰å¾®è°ƒåçš„æ•ˆæœã€‚ 
6. æ ¹æ®å¾®è°ƒæ—¶ä½¿ç”¨çš„default_systemï¼Œåœ¨ AstrBot ä¸­è®¾ç½®ç³»ç»Ÿæç¤ºè¯ã€‚
![5](https://github.com/user-attachments/assets/19de7072-076a-4cdf-8ae6-46b9b89f536a)
&gt; [!IMPORTANT]
&gt; æ£€æŸ¥api_serviceçš„æ—¥å¿—ï¼Œå°½é‡ä¿è¯å¤§æ¨¡å‹æœåŠ¡è¯·æ±‚çš„å‚æ•°å’Œå¾®è°ƒæ—¶ä¸€è‡´ï¼Œtoolæ’ä»¶èƒ½åŠ›éƒ½å…³æ‰ã€‚
7. è°ƒæ•´é‡‡æ ·å‚æ•°ï¼Œä¾‹å¦‚temperatureã€top_pã€top_kç­‰
[é…ç½®è‡ªå®šä¹‰çš„æ¨¡å‹å‚æ•°](https://astrbot.app/config/model-config.html#%E9%85%8D%E7%BD%AE%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0)

### LangBot

[LangBot](https://github.com/RockChinQ/LangBot) æ˜¯ä¸€ä¸ªå¼€æºçš„æ¥å…¥å…¨çƒå¤šç§å³æ—¶é€šä¿¡å¹³å°çš„ LLM æœºå™¨äººå¹³å°ï¼Œé€‚åˆå„ç§åœºæ™¯ä½¿ç”¨ã€‚

1. [éƒ¨ç½² LangBot](https://github.com/RockChinQ/LangBot#-%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8)
2. åœ¨ LangBot ä¸­æ·»åŠ ä¸€ä¸ªæœºå™¨äºº
4. åœ¨æ¨¡å‹é¡µæ·»åŠ æ–°æ¨¡å‹ï¼Œåç§°`gpt-3.5-turbo`ï¼Œä¾›åº”å•†é€‰æ‹© OpenAIï¼Œå¡«å†™ è¯·æ±‚ URL ä¸º WeClone çš„åœ°å€ï¼Œè¯¦ç»†è¿æ¥æ–¹å¼å¯ä»¥å‚è€ƒ[æ–‡æ¡£](https://docs.langbot.app/zh/workshop/network-details.html)ï¼ŒAPI Key ä»»æ„å¡«å†™ã€‚

&lt;img width=&quot;400px&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/fc167dea-7c93-4d94-9c5f-db709d0320ba&quot; /&gt;

6. åœ¨æµæ°´çº¿é…ç½®ä¸­é€‰æ‹©åˆšæ‰æ·»åŠ çš„æ¨¡å‹ï¼Œæˆ–ä¿®æ”¹æç¤ºè¯é…ç½®

&lt;img width=&quot;400px&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/dbb0fd0a-f760-42db-acd0-bb99c859b52e&quot; /&gt;

## ğŸ“Œ è·¯çº¿å›¾
- [ ] æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ï¼šåŒ…æ‹¬ä¸Šä¸‹æ–‡å¯¹è¯ã€èŠå¤©å¯¹è±¡ä¿¡æ¯ã€æ—¶é—´ç­‰ + æ€è€ƒ
- [ ] Memory æ”¯æŒ
- [ ] æ”¯æŒå¤šæ¨¡æ€
- [ ] æ•°æ®å¢å¼º
- [ ] æ”¯æŒGUI

## é—®é¢˜è§£å†³
- å¾®è°ƒé—®é¢˜ï¼š[LLaMA-Factory| FAQs | å¸¸è§é—®é¢˜](https://github.com/hiyouga/LLaMA-Factory/issues/4614) æˆ–è€…æ›´æ–¹ä¾¿çš„ [![æ›´æ–¹ä¾¿çš„Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/hiyouga/LLaMA-Factory)

## â¤ï¸ è´¡çŒ®ä»£ç 

æ¬¢è¿ä»»ä½• Issues/Pull Requestsï¼

ä½ å¯ä»¥é€šè¿‡æŸ¥çœ‹Issuesæˆ–å¸®åŠ©å®¡æ ¸ PRï¼ˆæ‹‰å–è¯·æ±‚ï¼‰æ¥è´¡çŒ®ã€‚å¯¹äºæ–°åŠŸèƒ½çš„æ·»åŠ ï¼Œè¯·å…ˆé€šè¿‡ Issue è®¨è®ºã€‚   
è¿è¡Œ`uv pip install --group dev -e .`å®‰è£…å¼€å‘ä¾èµ–ã€‚   
é¡¹ç›®ä½¿ç”¨`pytest`æµ‹è¯•(æµ‹è¯•è„šæœ¬å¾…å®Œå–„)ï¼Œ`pyright`æ£€æŸ¥ç±»å‹ï¼Œ`ruff`æ£€æŸ¥ä»£ç æ ¼å¼ã€‚


## âš ï¸ å…è´£å£°æ˜
&gt; [!CAUTION]
&gt; è¯·å‹¿ç”¨äºéæ³•ç”¨é€”ï¼Œå¦åˆ™åæœè‡ªè´Ÿã€‚
&lt;details&gt;
&lt;summary&gt;1. ä½¿ç”¨ç›®çš„&lt;/summary&gt;

* æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨ï¼Œ**è¯·å‹¿ç”¨äºéæ³•ç”¨é€”**ï¼Œ**è¯·å‹¿ç”¨äºéæ³•ç”¨é€”**ï¼Œ**è¯·å‹¿ç”¨äºéæ³•ç”¨é€”**ï¼Œå¦åˆ™åæœè‡ªè´Ÿã€‚
* ç”¨æˆ·ç†è§£å¹¶åŒæ„ï¼Œä»»ä½•è¿åæ³•å¾‹æ³•è§„ã€ä¾µçŠ¯ä»–äººåˆæ³•æƒç›Šçš„è¡Œä¸ºï¼Œå‡ä¸æœ¬é¡¹ç›®åŠå…¶å¼€å‘è€…æ— å…³ï¼Œåæœç”±ç”¨æˆ·è‡ªè¡Œæ‰¿æ‹…ã€‚

2. ä½¿ç”¨æœŸé™

* æ‚¨åº”è¯¥åœ¨ä¸‹è½½ä¿å­˜ä½¿ç”¨æœ¬é¡¹ç›®çš„24å°æ—¶å†…ï¼Œåˆ é™¤æœ¬é¡¹ç›®çš„æºä»£ç å’Œç¨‹åºï¼›è¶…å‡ºæ­¤æœŸé™çš„ä»»ä½•ä½¿ç”¨è¡Œä¸ºï¼Œä¸€æ¦‚ä¸æœ¬é¡¹ç›®åŠå…¶å¼€å‘è€…æ— å…³ã€‚

3. æ“ä½œè§„èŒƒ

* æœ¬é¡¹ç›®ä»…å…è®¸åœ¨æˆæƒæƒ…å†µä¸‹ä½¿ç”¨æ•°æ®è®­ç»ƒï¼Œä¸¥ç¦ç”¨äºéæ³•ç›®çš„ï¼Œå¦åˆ™è‡ªè¡Œæ‰¿æ‹…æ‰€æœ‰ç›¸å…³è´£ä»»ï¼›ç”¨æˆ·å¦‚å› è¿åæ­¤è§„å®šè€Œå¼•å‘çš„ä»»ä½•æ³•å¾‹è´£ä»»ï¼Œå°†ç”±ç”¨æˆ·è‡ªè¡Œæ‰¿æ‹…ï¼Œä¸æœ¬é¡¹ç›®åŠå…¶å¼€å‘è€…æ— å…³ã€‚
* ä¸¥ç¦ç”¨äºçªƒå–ä»–äººéšç§ï¼Œä¸¥ç¦ç”¨äºçªƒå–ä»–äººéšç§ï¼Œä¸¥ç¦ç”¨äºçªƒå–ä»–äººéšç§ï¼Œå¦åˆ™è‡ªè¡Œæ‰¿æ‹…æ‰€æœ‰ç›¸å…³è´£ä»»ã€‚

4. å…è´£å£°æ˜æ¥å—

* ä¸‹è½½ã€ä¿å­˜ã€è¿›ä¸€æ­¥æµè§ˆæºä»£ç æˆ–è€…ä¸‹è½½å®‰è£…ã€ç¼–è¯‘ä½¿ç”¨æœ¬ç¨‹åºï¼Œè¡¨ç¤ºä½ åŒæ„æœ¬è­¦å‘Šï¼Œå¹¶æ‰¿è¯ºéµå®ˆå®ƒ;

5. ç¦æ­¢ç”¨äºéæ³•æµ‹è¯•æˆ–æ¸—é€

* ç¦æ­¢åˆ©ç”¨æœ¬é¡¹ç›®çš„ç›¸å…³æŠ€æœ¯ä»äº‹éæ³•æµ‹è¯•æˆ–æ¸—é€ï¼Œç¦æ­¢åˆ©ç”¨æœ¬é¡¹ç›®çš„ç›¸å…³ä»£ç æˆ–ç›¸å…³æŠ€æœ¯ä»äº‹ä»»ä½•éæ³•å·¥ä½œï¼Œå¦‚å› æ­¤äº§ç”Ÿçš„ä¸€åˆ‡ä¸è‰¯åæœä¸æœ¬é¡¹ç›®åŠå…¶å¼€å‘è€…æ— å…³ã€‚
* ä»»ä½•å› æ­¤äº§ç”Ÿçš„ä¸è‰¯åæœï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®æ³„éœ²ã€ç³»ç»Ÿç˜«ç—ªã€ä¾µçŠ¯éšç§ç­‰ï¼Œå‡ä¸æœ¬é¡¹ç›®åŠå…¶å¼€å‘è€…æ— å…³ï¼Œè´£ä»»ç”±ç”¨æˆ·è‡ªè¡Œæ‰¿æ‹…ã€‚

6. å…è´£å£°æ˜ä¿®æ”¹

* æœ¬å…è´£å£°æ˜å¯èƒ½æ ¹æ®é¡¹ç›®è¿è¡Œæƒ…å†µå’Œæ³•å¾‹æ³•è§„çš„å˜åŒ–è¿›è¡Œä¿®æ”¹å’Œè°ƒæ•´ã€‚ç”¨æˆ·åº”å®šæœŸæŸ¥é˜…æœ¬é¡µé¢ä»¥è·å–æœ€æ–°ç‰ˆæœ¬çš„å…è´£å£°æ˜ï¼Œä½¿ç”¨æœ¬é¡¹ç›®æ—¶åº”éµå®ˆæœ€æ–°ç‰ˆæœ¬çš„å…è´£å£°æ˜ã€‚

7. å…¶ä»–

* é™¤æœ¬å…è´£å£°æ˜è§„å®šå¤–ï¼Œç”¨æˆ·åœ¨ä½¿ç”¨æœ¬é¡¹ç›®è¿‡ç¨‹ä¸­åº”éµå®ˆç›¸å…³çš„æ³•å¾‹æ³•è§„å’Œé“å¾·è§„èŒƒã€‚å¯¹äºå› ç”¨æˆ·è¿åç›¸å…³è§„å®šè€Œå¼•å‘çš„ä»»ä½•çº çº·æˆ–æŸå¤±ï¼Œæœ¬é¡¹ç›®åŠå…¶å¼€å‘è€…ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚

* è¯·ç”¨æˆ·æ…é‡é˜…è¯»å¹¶ç†è§£æœ¬å…è´£å£°æ˜çš„æ‰€æœ‰å†…å®¹ï¼Œç¡®ä¿åœ¨ä½¿ç”¨æœ¬é¡¹ç›®æ—¶ä¸¥æ ¼éµå®ˆç›¸å…³è§„å®šã€‚

&lt;/details&gt;
è¯·ç”¨æˆ·æ…é‡é˜…è¯»å¹¶ç†è§£æœ¬å…è´£å£°æ˜çš„æ‰€æœ‰å†…å®¹ï¼Œç¡®ä¿åœ¨ä½¿ç”¨æœ¬é¡¹ç›®æ—¶ä¸¥æ ¼éµå®ˆç›¸å…³è§„å®šã€‚

&lt;br&gt;  
&lt;br&gt;  
&lt;br&gt;  

## â­ Star History
&gt; [!TIP] 
&gt; å¦‚æœæœ¬é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œæˆ–è€…æ‚¨å…³æ³¨æœ¬é¡¹ç›®çš„æœªæ¥å‘å±•ï¼Œè¯·ç»™é¡¹ç›® Starï¼Œè°¢è°¢ 

&lt;div align=&quot;center&quot;&gt;

[![Star History Chart](https://api.star-history.com/svg?repos=xming521/WeClone&amp;type=Date)](https://www.star-history.com/#xming521/WeClone&amp;Date)

&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt; å…‹éš†æˆ‘ä»¬ï¼Œä¿ç•™çµé­‚çš„èŠ¬èŠ³ &lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Capsize-Games/airunner]]></title>
            <link>https://github.com/Capsize-Games/airunner</link>
            <guid>https://github.com/Capsize-Games/airunner</guid>
            <pubDate>Sun, 18 May 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[Offline inference engine for art, real-time voice conversations, LLM powered chatbots and automated workflows]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Capsize-Games/airunner">Capsize-Games/airunner</a></h1>
            <p>Offline inference engine for art, real-time voice conversations, LLM powered chatbots and automated workflows</p>
            <p>Language: Python</p>
            <p>Stars: 977</p>
            <p>Forks: 77</p>
            <p>Stars today: 70 stars today</p>
            <h2>README</h2><pre>[![AI Runner Logo](images/banner.png)](https://github.com/Capsize-Games/airunner)

[![Discord](https://img.shields.io/discord/839511291466219541?color=5865F2&amp;logo=discord&amp;logoColor=white)](https://discord.gg/PUVDDCJ7gz)
![GitHub](https://img.shields.io/github/license/Capsize-Games/airunner)
[![PyPi](https://github.com/Capsize-Games/airunner/actions/workflows/pypi-dispatch.yml/badge.svg)](https://github.com/Capsize-Games/airunner/actions/workflows/pypi-dispatch.yml)
![GitHub last commit](https://img.shields.io/github/last-commit/Capsize-Games/airunner)

---

# AI Runner: Offline AI Inference Engine for Hackers, Makers, and Builders.

![image](https://github.com/user-attachments/assets/392375c8-a7f6-4e6e-8662-511cffc608aa)

Here are some of the things you can do with AI Runner:

- âœ… **Voice-based chatbots** for real-time conversations (three text-to-speech engines each with multiple voices to choose from)
- âœ… **Customizable agents with names, moods and personalities** for more engaging conversations
- âœ… **Retrieval-Augmented Generation** (RAG) for documents and websites
- âœ… **Text-to-Image** generation with **Stable Diffusion** and **ControlNet**
- âœ… **Image manipulation** with **inpainting**, **outpainting**, **ControlNet** and **Image filters**

For extra security, performance, and compatibility, AI Runner is built with **Wayland support**, **Python 3.13**, and the latest stable torch libraries.

---

## System Requirements

| Specification       | Minimum                              | Recommended                          |
|---------------------|--------------------------------------------|--------------------------------------------|
| **OS**             | Ubuntu 22.04, Windows 10                               | Ubuntu 22.04 (Wayland)                              |
| **CPU**            | Ryzen 2700K or Intel Core i7-8700K         | Ryzen 5800X or Intel Core i7-11700K        |
| **Memory**         | 16 GB RAM                                  | 32 GB RAM                                  |
| **GPU**            | NVIDIA RTX 3060 or better                  | NVIDIA RTX 4090 or better                  |
| **Network**        | Broadband (used to download models)        | Broadband (used to download models)        |
| **Storage**        | 22 GB                                      | 50 GB                                      |
---

## ğŸ’¾ Installation Quick Start

### ğŸ”§ Installation Steps

1. **Install system requirements**
   ```bash
   sudo apt update &amp;&amp; sudo apt upgrade -y
   sudo apt install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python3-openssl git nvidia-cuda-toolkit pipewire libportaudio2 libxcb-cursor0 gnupg gpg-agent pinentry-curses espeak xclip cmake qt6-qpa-plugins qt6-wayland qt6-gtk-platformtheme mecab libmecab-dev mecab-ipadic-utf8
   sudo apt install espeak
   sudo apt install espeak-ng-espeak
   ```
2. **Create `airunner` directory**
   ```bash
   sudo mkdir ~/.local/share/airunner
   sudo chown $USER:$USER ~/.local/share/airunner
   ```
3. **Install AI Runner** - **Python 3.13+ required** `pyenv` and `venv` are recommended ([see wiki](https://github.com/Capsize-Games/airunner/wiki/Installation-instructions) for more info)
   ```bash
   pip install &quot;typing-extensions==4.13.2&quot;
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
   pip install airunner[all_dev]
   pip install -U timm
   ```
4. **Run AI Runner**
   ```bash
   airunner
   ```

For more options, including Docker, see the [Installation Wiki](https://github.com/Capsize-Games/airunner/wiki/Installation-instructions).

**Note: *AI Runner does not distribute AI art models. You are responsible for obtaining and your own.***

---

## ğŸ› ï¸ Usage

### Basic Usage

- **Run AI Runner**: `airunner`
- **Build templates**: `airunner-build-ui`

---

## â­ Features

Below is a high-level list of capabilities in AI Runner:

| Feature                                  | Description                                                                                  |
|------------------------------------------|----------------------------------------------------------------------------------------------|
| **LLMs &amp; Communication**                 |                                                                                              |
| Voice-based chatbot conversations        | Have real-time voice-chat sessions with an LLM (speech-to-text + text-to-speech)            |
| Text-to-speech (TTS)                     | Convert text to spoken audio using **OpenVoice**, **SpeechT5**, and **Espeak**                                       |
| Speech-to-text (STT)                     | Convert spoken audio to text with **Whisper**                                                   |
| Customizable chatbots                    | Create AI personalities and moods for more engaging conversations                            |
| Retrieval-Augmented Generation           | Use local doc or website data to enrich chat responses                                      |
| **Image Generation**                     |                                                                                              |
| Stable Diffusion (1.5, SDXL, Turbo)      | Generate images from textual prompts, sketches, or existing images                           |
| Drawing tools &amp; ControlNet              | Fine-tune image outputs with extra input or guides                                          |
| LoRA &amp; Embeddings                        | Load LoRA models or textual embeddings for specialized image generation                     |
| **Image Manipulation**                   |                                                                                              |
| Inpaint &amp; Outpaint                       | Modify portions of generated images while keeping context                                   |
| Image filters                            | Blur, film grain, pixel art, etc.                                                            |
| **Utility**                              |                                                                                              |
| **Offline**                              | Everything runs locally, no external API required                                           |
| Fast generation                          | E.g., ~2 seconds on an RTX 2080s for stable diffusion                                        |
| Docker-based approach                    | Simplifies setup &amp; ensures GPU acceleration works out of the box                            |
| Dark mode                                | Built-in theming (Light / Dark / System)                                                    |
| NSFW toggles                             | Enable or disable NSFW detection for images                                                 |
| Ethical guardrails                       | Basic guardrails for safe LLM usage (optional)                                              |
| **Extensions**                           | Build your own feature add-ons via the extension API                                        |
| **Python Library**                       | `pip install airunner` and embed it in your own projects                                    |
| **API Support**                          | Optionally use OpenRouter or other external LLMs                                            |

---

## âš™ï¸ System Requirements

### System Requirements

| Specification       | Minimum                              | Recommended                          |
|---------------------|--------------------------------------------|--------------------------------------------|
| **OS**             | Ubuntu 22.04, Windows 10                               | Ubuntu 22.04 (Wayland)                              |
| **CPU**            | Ryzen 2700K or Intel Core i7-8700K         | Ryzen 5800X or Intel Core i7-11700K        |
| **Memory**         | 16 GB RAM                                  | 32 GB RAM                                  |
| **GPU**            | NVIDIA RTX 3060 or better                  | NVIDIA RTX 4090 or better                  |
| **Network**        | Broadband (used to download models)        | Broadband (used to download models)        |
| **Storage**        | 22 GB (with models), 6 GB (without models) | 100 GB or higher                           |
---

### Models

These are the sizes of the various models that power AI Runner.

| Model                | Size     |
|-------------------------|----------|
| Controlnet (SD 1.5)             | 10.6 GB  |
| Controlnet (SDXL)             | 320.2 MB  |
| Safety Checker + Feature Extractor               | 3.2 GB   |
| SD 1.5                | 1.6 MB   |
| SDXL 1.0                | 6.45 MB   |
| LLM                     | 5.8 GB   |
| e5 large (embedding model) | 1.3 GB   |
| Whisper Tiny            | 155.4 MB |
| Speech T5 (Voice)       | 654.4 MB |
| OpenVoice (Voice)       | 4.0 GB |

---

## AI Models

By default, AI Runner installs essential TTS/STT and minimal LLM components.  
You **must supply** additional Stable Diffusion models (e.g., from [Hugging Face](https://huggingface.co/) or [Civitai](https://civitai.com/)).

Organize them under your local AI Runner data directory:
```plaintext
~/.local/share/airunner
â”œâ”€â”€ art
â”‚   â””â”€â”€ models
â”‚       â”œâ”€â”€ SD 1.5
â”‚       â”‚   â”œâ”€â”€ lora
â”‚       â”‚   â””â”€â”€ embeddings
â”‚       â”œâ”€â”€ Flux
â”‚       â”œâ”€â”€ SDXL 1.0
â”‚       â”‚   â”œâ”€â”€ lora
â”‚       â”‚   â””â”€â”€ embeddings
â”‚       â””â”€â”€ SDXL Turbo
â”‚           â”œâ”€â”€ lora
â”‚           â””â”€â”€ embeddings
```

---

## Unit Tests

To run all tests:

```bash
python -m unittest discover -s src/airunner/tests
```

Or a single test:

```bash
python -m unittest src/airunner/tests/test_prompt_weight_convert.py
```

---

## Database

AI Runner supports a simple database system. See the [Wiki](https://github.com/Capsize-Games/airunner/wiki/Database) for how to:
- Switch engines (SQLite, etc.)
- Make schema changes
- Run migrations

---

## Advanced Features

- **Memory Optimization**: TF32 Mode, VAE/Attention Slicing, Torch 2.0, sequential CPU offload, ToMe token merging.  
- **Experimental Integrations**: Weather-based chatbot prompts, advanced command-line arguments (`--perform-llm-analysis`, `--disable-setup-wizard`, etc.).  
- **Safety &amp; Guardrails**: Optional NSFW content detection and adjustable guardrails for LLMs.  

---

## Contributing

We welcome pull requests for new features, bug fixes, or documentation improvements. You can also build and share **extensions** to expand AI Runnerâ€™s functionality. For details, see the [Extensions Wiki](https://github.com/Capsize-Games/airunner/wiki/Extensions).

Take a look at the [Contributing document](https://github.com/Capsize-Games/airunner/CONTRIBUTING.md) and the [Development wiki page](https://github.com/Capsize-Games/airunner/wiki/Development) for detailed instructions.

---

## Thank You!

Thanks for checking out AI Runner.  
Get started with local AI inference in minutesâ€”no more endless environment setup.  
Questions or ideas? Join our [Discord](https://discord.gg/PUVDDCJ7gz) or open a [GitHub Issue](https://github.com/Capsize-Games/airunner/issues).  

**Happy building!**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[facebookresearch/fairchem]]></title>
            <link>https://github.com/facebookresearch/fairchem</link>
            <guid>https://github.com/facebookresearch/fairchem</guid>
            <pubDate>Sun, 18 May 2025 00:04:28 GMT</pubDate>
            <description><![CDATA[FAIR Chemistry's library of machine learning methods for chemistry]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/facebookresearch/fairchem">facebookresearch/fairchem</a></h1>
            <p>FAIR Chemistry's library of machine learning methods for chemistry</p>
            <p>Language: Python</p>
            <p>Stars: 1,326</p>
            <p>Forks: 312</p>
            <p>Stars today: 27 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt; &lt;code&gt;fairchem&lt;/code&gt; by FAIR Chemistry &lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;559&quot; height=&quot;200&quot; src=&quot;https://github.com/FAIR-Chem/fairchem/assets/45150244/5872c21c-8f39-41af-b703-af9817f0affe&quot;?
&lt;/p&gt;


&lt;h4 align=&quot;center&quot;&gt;

![tests](https://github.com/FAIR-Chem/fairchem/actions/workflows/test.yml/badge.svg?branch=main)
![PyPI - Version](https://img.shields.io/pypi/v/fairchem-core)
![Static Badge](https://img.shields.io/badge/python-3.10%2B-blue)

[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://github.com/codespaces/new/FAIR-Chem/fairchem?quickstart=1)

`fairchem` is the [FAIR](https://ai.meta.com/research/) Chemistry&#039;s centralized repository of all its data, models,
demos, and application efforts for materials science and quantum chemistry.

&gt; :warning: **FAIRChem version 2 is a breaking change from version 1 and is not compatible with our previous pretrained models and code.**
&gt; If you want to use an older model or code from version 1 you will need to install [version 1](https://pypi.org/project/fairchem-core/1.10.0/),
&gt; as detailed [here](#looking-for-fairchem-v1-models-and-code).

### Read our latest release post!
Read about the [UMA model and dataset](https://ai.meta.com/blog/meta-fair-science-new-open-source-releases/) release.

[![Meta FAIR Science Release](https://github.com/user-attachments/assets/acddd09b-ed6f-4d05-9a4b-9ba5e2301150)](https://ai.meta.com/blog/meta-fair-science-new-open-source-releases/?ref=shareable)

### Try the demo!
If you want to explore model capabilities check out our
[educational demo](https://facebook-fairchem-uma-demo.hf.space/)

[![Educational Demo](https://github.com/user-attachments/assets/7005d1bb-4459-403d-b299-d41fdd8c48ec)](https://facebook-fairchem-uma-demo.hf.space/)


### Installation
Install fairchem-core using pip,
```bash
pip install git+https://github.com/facebookresearch/fairchem.git@fairchem_core-2.0.0#subdirectory=packages/fairchem-core
```
**The PyPI install (pip install fairchem-core) is not available right now as we are waiting for a few dependencies to release their PyPI packages, will update this soon when it&#039;s available!**

### Quick Start
The easiest way to use pretrained models is via the [ASE](https://wiki.fysik.dtu.dk/ase/) `FAIRChemCalculator`.
A single uma model can be used for a wide range of applications in chemistry and materials science by picking the
appropriate task name for domain specific prediction.

#### Instantiate a calculator from a pretrained model
Make sure you have a Hugging Face account, have already applied for model access to the 
[UMA model repository](https://huggingface.co/facebook/UMA), and have logged in to Hugging Face using an access token.

#### Set the task for your application and calculate

- **oc20:** use this for catalysis
- **omat:** use this for inorganic materials
- **omol:** use this for molecules
- **odac:** use this for MOFs
- **omc:** use this for molecular crystals

Relax adsorbate on a catalytic surface,
```python
from ase.build import fcc100, add_adsorbate, molecule
from ase.optimize import LBFGS
from fairchem.core import FAIRChemCalculator

calc = FAIRChemCalculator(hf_hub_filename=&quot;uma_sm.pt&quot;, device=&quot;cuda&quot;, task_name=&quot;oc20&quot;)

# Set up your system as an ASE atoms object
slab = fcc100(&quot;Cu&quot;, (3, 3, 3), vacuum=8, periodic=True)
adsorbate = molecule(&quot;CO&quot;)
add_adsorbate(slab, adsorbate, 2.0, &quot;bridge&quot;)

slab.calc = calc

# Set up LBFGS dynamics object
opt = LBFGS(slab)
opt.run(0.05, 100)
```

Or relax an inorganic crystal,
```python
from ase.build import bulk
from ase.optimize import FIRE
from ase.filters import FrechetCellFilter
from fairchem.core import FAIRChemCalculator

calc = FAIRChemCalculator(hf_hub_filename=&quot;uma_sm.pt&quot;, device=&quot;cuda&quot;, task_name=&quot;omat&quot;)

atoms = bulk(&quot;Fe&quot;)
atoms.calc = calc

opt = LBFGS(FrechetCellFilter(atoms))
opt.run(0.05, 100)
```

Run molecular MD,
```python
from ase import units
from ase.io import Trajectory
from ase.md.langevin import Langevin
from ase.build import molecule
from fairchem.core import FAIRChemCalculator

calc = FAIRChemCalculator(hf_hub_filename=&quot;uma_sm.pt&quot;, device=&quot;cuda&quot;, task_name=&quot;omol&quot;)

atoms = molecule(&quot;H2O&quot;)
atoms.calc = calc

dyn = Langevin(
    atoms,
    timestep=0.1 * units.fs,
    temperature_K=400,
    friction=0.001 / units.fs,
)
trajectory = Trajectory(&quot;my_md.traj&quot;, &quot;w&quot;, atoms)
dyn.attach(trajectory.write, interval=1)
dyn.run(steps=1000)
```


### Looking for Fairchem V1, models and code?
Fairchem V2 is a major upgrade and we completely rewrote the trainer, fine-tuning, models and calculators. 

We plan to bring back the following models compatible with Fairchem V2 soon:
* Gemnet-OC
* EquiformersV2
* ESEN

We will also be releasing more detailed documentation on how to use Fairchem V2, stay tuned! 

The old OCPCalculator, trainer code will NOT be revived. We apologize for the inconvenience and please raise Issues if you need help!
In the meantime, you can still use models from fairchem version 1, by installing version 1,

```bash
pip install fairchem-core==1.10
```

And using the `OCPCalculator`
```python
from fairchem.core import OCPCalculator

calc = OCPCalculator(
    model_name=&quot;EquiformerV2-31M-S2EF-OC20-All+MD&quot;,
    local_cache=&quot;pretrained_models&quot;,
    cpu=False,
)
```

### LICENSE
`fairchem` is available under a [MIT License](LICENSE.md).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[localstack/localstack]]></title>
            <link>https://github.com/localstack/localstack</link>
            <guid>https://github.com/localstack/localstack</guid>
            <pubDate>Sun, 18 May 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[ğŸ’» A fully functional local AWS cloud stack. Develop and test your cloud & Serverless apps offline]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/localstack/localstack">localstack/localstack</a></h1>
            <p>ğŸ’» A fully functional local AWS cloud stack. Develop and test your cloud & Serverless apps offline</p>
            <p>Language: Python</p>
            <p>Stars: 58,961</p>
            <p>Forks: 4,147</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
:zap: We are thrilled to announce the release of &lt;a href=&quot;https://blog.localstack.cloud/localstack-release-v-4-4-0/&quot;&gt;LocalStack 4.4&lt;/a&gt; :zap:
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/localstack/localstack/master/docs/localstack-readme-banner.svg&quot; alt=&quot;LocalStack - A fully functional local cloud stack&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://circleci.com/gh/localstack/localstack&quot;&gt;&lt;img alt=&quot;CircleCI&quot; src=&quot;https://img.shields.io/circleci/build/gh/localstack/localstack/master?logo=circleci&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://coveralls.io/github/localstack/localstack?branch=master&quot;&gt;&lt;img alt=&quot;Coverage Status&quot; src=&quot;https://coveralls.io/repos/github/localstack/localstack/badge.svg?branch=master&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/localstack/&quot;&gt;&lt;img alt=&quot;PyPI Version&quot; src=&quot;https://img.shields.io/pypi/v/localstack?color=blue&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://hub.docker.com/r/localstack/localstack&quot;&gt;&lt;img alt=&quot;Docker Pulls&quot; src=&quot;https://img.shields.io/docker/pulls/localstack/localstack&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/localstack&quot;&gt;&lt;img alt=&quot;PyPi downloads&quot; src=&quot;https://static.pepy.tech/badge/localstack&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;#backers&quot;&gt;&lt;img alt=&quot;Backers on Open Collective&quot; src=&quot;https://opencollective.com/localstack/backers/badge.svg&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;#sponsors&quot;&gt;&lt;img alt=&quot;Sponsors on Open Collective&quot; src=&quot;https://opencollective.com/localstack/sponsors/badge.svg&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://img.shields.io/pypi/l/localstack.svg&quot;&gt;&lt;img alt=&quot;PyPI License&quot; src=&quot;https://img.shields.io/pypi/l/localstack.svg&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/psf/black&quot;&gt;&lt;img alt=&quot;Code style: black&quot; src=&quot;https://img.shields.io/badge/code%20style-black-000000.svg&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/astral-sh/ruff&quot;&gt;&lt;img alt=&quot;Ruff&quot; src=&quot;https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/localstack&quot;&gt;&lt;img alt=&quot;Twitter&quot; src=&quot;https://img.shields.io/twitter/url/http/shields.io.svg?style=social&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  LocalStack is a cloud software development framework to develop and test your AWS applications locally.
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;#overview&quot;&gt;Overview&lt;/a&gt; â€¢
  &lt;a href=&quot;#install&quot;&gt;Install&lt;/a&gt; â€¢
  &lt;a href=&quot;#quickstart&quot;&gt;Quickstart&lt;/a&gt; â€¢
  &lt;a href=&quot;#running&quot;&gt;Run&lt;/a&gt; â€¢
  &lt;a href=&quot;#usage&quot;&gt;Usage&lt;/a&gt; â€¢
  &lt;a href=&quot;#releases&quot;&gt;Releases&lt;/a&gt; â€¢
  &lt;a href=&quot;#contributing&quot;&gt;Contributing&lt;/a&gt;
  &lt;br/&gt;
  &lt;a href=&quot;https://docs.localstack.cloud&quot; target=&quot;_blank&quot;&gt;ğŸ“– Docs&lt;/a&gt; â€¢
  &lt;a href=&quot;https://app.localstack.cloud&quot; target=&quot;_blank&quot;&gt;ğŸ’» Pro version&lt;/a&gt; â€¢
  &lt;a href=&quot;https://docs.localstack.cloud/references/coverage/&quot; target=&quot;_blank&quot;&gt;â˜‘ï¸ LocalStack coverage&lt;/a&gt;
&lt;/p&gt;

---

# Overview

[LocalStack](https://localstack.cloud) is a cloud service emulator that runs in a single container on your laptop or in your CI environment. With LocalStack, you can run your AWS applications or Lambdas entirely on your local machine without connecting to a remote cloud provider! Whether you are testing complex CDK applications or Terraform configurations, or just beginning to learn about AWS services, LocalStack helps speed up and simplify your testing and development workflow.

LocalStack supports a growing number of AWS services, like AWS Lambda, S3, DynamoDB, Kinesis, SQS, SNS, and many more! The [Pro version of LocalStack](https://localstack.cloud/pricing) supports additional APIs and advanced features. You can find a comprehensive list of supported APIs on our [â˜‘ï¸ Feature Coverage](https://docs.localstack.cloud/user-guide/aws/feature-coverage/) page.

LocalStack also provides additional features to make your life as a cloud developer easier! Check out LocalStack&#039;s [User Guides](https://docs.localstack.cloud/user-guide/) for more information.

## Install

The quickest way to get started with LocalStack is by using the LocalStack CLI. It enables you to start and manage the LocalStack Docker container directly through your command line. Ensure that your machine has a functional [`docker` environment](https://docs.docker.com/get-docker/) installed before proceeding.

### Brew (macOS or Linux with Homebrew)

Install the LocalStack CLI through our [official LocalStack Brew Tap](https://github.com/localstack/homebrew-tap):

```bash
brew install localstack/tap/localstack-cli
```

### Binary download (macOS, Linux, Windows)

If Brew is not installed on your machine, you can download the pre-built LocalStack CLI binary directly:

- Visit [localstack/localstack-cli](https://github.com/localstack/localstack-cli/releases/latest) and download the latest release for your platform.
- Extract the downloaded archive to a directory included in your `PATH` variable:
    -   For macOS/Linux, use the command: `sudo tar xvzf ~/Downloads/localstack-cli-*-darwin-*-onefile.tar.gz -C /usr/local/bin`

### PyPI (macOS, Linux, Windows)

LocalStack is developed using Python. To install the LocalStack CLI using `pip`, run the following command:

```bash
python3 -m pip install localstack
```

The `localstack-cli` installation enables you to run the Docker image containing the LocalStack runtime. To interact with the local AWS services, you need to install the `awslocal` CLI separately. For installation guidelines, refer to the [`awslocal` documentation](https://docs.localstack.cloud/user-guide/integrations/aws-cli/#localstack-aws-cli-awslocal).

&gt; **Important**: Do not use `sudo` or run as `root` user. LocalStack must be installed and started entirely under a local non-root user. If you have problems with permissions in macOS High Sierra, install with `pip install --user localstack`

## Quickstart

Start LocalStack inside a Docker container by running:

```bash
 % localstack start -d

     __                     _______ __             __
    / /   ____  _________ _/ / ___// /_____ ______/ /__
   / /   / __ \/ ___/ __ `/ /\__ \/ __/ __ `/ ___/ //_/
  / /___/ /_/ / /__/ /_/ / /___/ / /_/ /_/ / /__/ ,&lt;
 /_____/\____/\___/\__,_/_//____/\__/\__,_/\___/_/|_|

- LocalStack CLI: 4.4.0
- Profile: default
- App: https://app.localstack.cloud

[17:00:15] starting LocalStack in Docker mode ğŸ³               localstack.py:512
           preparing environment                               bootstrap.py:1322
           configuring container                               bootstrap.py:1330
           starting container                                  bootstrap.py:1340
[17:00:16] detaching                                           bootstrap.py:1344
```

You can query the status of respective services on LocalStack by running:

```bash
% localstack status services
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Service                  â”ƒ Status      â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ acm                      â”‚ âœ” available â”‚
â”‚ apigateway               â”‚ âœ” available â”‚
â”‚ cloudformation           â”‚ âœ” available â”‚
â”‚ cloudwatch               â”‚ âœ” available â”‚
â”‚ config                   â”‚ âœ” available â”‚
â”‚ dynamodb                 â”‚ âœ” available â”‚
...
```

To use SQS, a fully managed distributed message queuing service, on LocalStack, run:

```shell
% awslocal sqs create-queue --queue-name sample-queue
{
    &quot;QueueUrl&quot;: &quot;http://sqs.us-east-1.localhost.localstack.cloud:4566/000000000000/sample-queue&quot;
}
```

Learn more about [LocalStack AWS services](https://docs.localstack.cloud/references/coverage/) and using them with LocalStack&#039;s `awslocal` CLI.

## Running

You can run LocalStack through the following options:

- [LocalStack CLI](https://docs.localstack.cloud/getting-started/installation/#localstack-cli)
- [Docker](https://docs.localstack.cloud/getting-started/installation/#docker)
- [Docker Compose](https://docs.localstack.cloud/getting-started/installation/#docker-compose)
- [Helm](https://docs.localstack.cloud/getting-started/installation/#helm)

## Usage

To start using LocalStack, check out our [documentation](https://docs.localstack.cloud).

- [LocalStack Configuration](https://docs.localstack.cloud/references/configuration/)
- [LocalStack in CI](https://docs.localstack.cloud/user-guide/ci/)
- [LocalStack Integrations](https://docs.localstack.cloud/user-guide/integrations/)
- [LocalStack Tools](https://docs.localstack.cloud/user-guide/tools/)
- [Understanding LocalStack](https://docs.localstack.cloud/references/)
- [Frequently Asked Questions](https://docs.localstack.cloud/getting-started/faq/)

To use LocalStack with a graphical user interface, you can use the following UI clients:

* [LocalStack Web Application](https://app.localstack.cloud)
* [LocalStack Desktop](https://docs.localstack.cloud/user-guide/tools/localstack-desktop/)
* [LocalStack Docker Extension](https://docs.localstack.cloud/user-guide/tools/localstack-docker-extension/)

## Releases

Please refer to [GitHub releases](https://github.com/localstack/localstack/releases) to see the complete list of changes for each release. For extended release notes, please refer to the [changelog](https://docs.localstack.cloud/references/changelog/).

## Contributing

If you are interested in contributing to LocalStack:

- Start by reading our [contributing guide](docs/CONTRIBUTING.md).
- Check out our [development environment setup guide](docs/development-environment-setup/README.md).
- Navigate our codebase and [open issues](https://github.com/localstack/localstack/issues).

We are thankful for all the contributions and feedback we receive.

## Get in touch

Get in touch with the LocalStack Team to
report ğŸ [issues](https://github.com/localstack/localstack/issues/new/choose),
upvote ğŸ‘ [feature requests](https://github.com/localstack/localstack/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc+),
ğŸ™‹ğŸ½ ask [support questions](https://docs.localstack.cloud/getting-started/help-and-support/),
or ğŸ—£ï¸ discuss local cloud development:

- [LocalStack Slack Community](https://localstack.cloud/contact/)
- [LocalStack GitHub Issue tracker](https://github.com/localstack/localstack/issues)

### Contributors

We are thankful to all the people who have contributed to this project.

&lt;a href=&quot;https://github.com/localstack/localstack/graphs/contributors&quot;&gt;&lt;img src=&quot;https://opencollective.com/localstack/contributors.svg?width=890&quot; /&gt;&lt;/a&gt;

### Backers

We are also grateful to all our backers who have donated to the project. You can become a backer on [Open Collective](https://opencollective.com/localstack#backer).

&lt;a href=&quot;https://opencollective.com/localstack#backers&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://opencollective.com/localstack/backers.svg?width=890&quot;&gt;&lt;/a&gt;

### Sponsors

You can also support this project by becoming a sponsor on [Open Collective](https://opencollective.com/localstack#sponsor). Your logo will show up here along with a link to your website.

&lt;a href=&quot;https://opencollective.com/localstack/sponsor/0/website&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://opencollective.com/localstack/sponsor/0/avatar.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://opencollective.com/localstack/sponsor/1/website&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://opencollective.com/localstack/sponsor/1/avatar.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://opencollective.com/localstack/sponsor/2/website&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://opencollective.com/localstack/sponsor/2/avatar.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://opencollective.com/localstack/sponsor/3/website&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://opencollective.com/localstack/sponsor/3/avatar.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://opencollective.com/localstack/sponsor/4/website&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://opencollective.com/localstack/sponsor/4/avatar.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://opencollective.com/localstack/sponsor/5/website&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://opencollective.com/localstack/sponsor/5/avatar.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://opencollective.com/localstack/sponsor/6/website&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://opencollective.com/localstack/sponsor/6/avatar.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://opencollective.com/localstack/sponsor/7/website&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://opencollective.com/localstack/sponsor/7/avatar.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://opencollective.com/localstack/sponsor/8/website&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://opencollective.com/localstack/sponsor/8/avatar.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://opencollective.com/localstack/sponsor/9/website&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://opencollective.com/localstack/sponsor/9/avatar.svg&quot;&gt;&lt;/a&gt;

## License

Copyright (c) 2017-2025 LocalStack maintainers and contributors.

Copyright (c) 2016 Atlassian and others.

This version of LocalStack is released under the Apache License, Version 2.0 (see [LICENSE](LICENSE.txt)). By downloading and using this software you agree to the [End-User License Agreement (EULA)](docs/end_user_license_agreement).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[frappe/hrms]]></title>
            <link>https://github.com/frappe/hrms</link>
            <guid>https://github.com/frappe/hrms</guid>
            <pubDate>Sun, 18 May 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[Open Source HR and Payroll Software]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/frappe/hrms">frappe/hrms</a></h1>
            <p>Open Source HR and Payroll Software</p>
            <p>Language: Python</p>
            <p>Stars: 2,145</p>
            <p>Forks: 1,057</p>
            <p>Stars today: 23 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
	&lt;a href=&quot;https://frappe.io/hr&quot;&gt;
		&lt;img src=&quot;.github/frappe-hr-logo.png&quot; height=&quot;80px&quot; width=&quot;80px&quot; alt=&quot;Frappe HR Logo&quot;&gt;
	&lt;/a&gt;
	&lt;h2&gt;Frappe HR&lt;/h2&gt;
	&lt;p align=&quot;center&quot;&gt;
		&lt;p&gt;Open Source, modern, and easy-to-use HR and Payroll Software&lt;/p&gt;
	&lt;/p&gt;

[![CI](https://github.com/frappe/hrms/actions/workflows/ci.yml/badge.svg?branch=develop)](https://github.com/frappe/hrms/actions/workflows/ci.yml)
[![codecov](https://codecov.io/gh/frappe/hrms/branch/develop/graph/badge.svg?token=0TwvyUg3I5)](https://codecov.io/gh/frappe/hrms)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
	&lt;img src=&quot;.github/hrms-hero.png&quot;/&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
	&lt;a href=&quot;https://frappe.io/hr&quot;&gt;Website&lt;/a&gt;
	-
	&lt;a href=&quot;https://docs.frappe.io/hr/introduction&quot;&gt;Documentation&lt;/a&gt;
&lt;/div&gt;

## Frappe HR

Frappe HR has everything you need to drive excellence within the company. It&#039;s a complete HRMS solution with over 13 different modules right from Employee Management, Onboarding, Leaves, to Payroll, Taxation, and more!

## Motivation
When Frappe team started growing in terms of size, we needed an open-source HR and Payroll software. We didn&#039;t find any &quot;true&quot; open-source HR software out there and so decided to build one ourselves.
Initially, it was a set of modules within ERPNext but version 14 onwards, as the modules became more mature, Frappe HR was created as a separate product.

## Key Features

- **Employee Lifecycle**: From onboarding employees, managing promotions and transfers, all the way to documenting feedback with exit interviews, make life easier for employees throughout their life cycle.
- **Leave and Attendance**: Configure leave policies, pull regional holidays with a click, check-in and check-out with geolocation capturing, track leave balances and attendance with reports.
- **Expense Claims and Advances**: Manage employee advances, claim expenses, configure multi-level approval workflows, all this with seamless integration with ERPNext accounting.
- **Performance Management**: Track goals, align goals with key result areas (KRAs), enable employees to evaluate themselves, make managing appraisal cycles easy.
- **Payroll &amp; Taxation**: Create salary structures, configure income tax slabs, run standard payroll, accomodate additional salaries and off cycle payments, view income breakup on salary slips and so much more.
- **Frappe HR Mobile App**: Apply for and approve leaves on the go, check-in and check-out, access employee profile right from the mobile app.

&lt;details open&gt;

&lt;summary&gt;View Screenshots&lt;/summary&gt;
	&lt;img src=&quot;.github/hrms-appraisal.png&quot;/&gt;
	&lt;img src=&quot;.github/hrms-requisition.png&quot;/&gt;
	&lt;img src=&quot;.github/hrms-attendance.png&quot;/&gt;
	&lt;img src=&quot;.github/hrms-salary.png&quot;/&gt;
	&lt;img src=&quot;.github/hrms-pwa.png&quot;/&gt;
&lt;/details&gt;

### Under the Hood

- [**Frappe Framework**](https://github.com/frappe/frappe): A full-stack web application framework written in Python and Javascript. The framework provides a robust foundation for building web applications, including a database abstraction layer, user authentication, and a REST API.

- [**Frappe UI**](https://github.com/frappe/frappe-ui): A Vue-based UI library, to provide a modern user interface. The Frappe UI library provides a variety of components that can be used to build single-page applications on top of the Frappe Framework.

## Production Setup

### Managed Hosting

You can try [Frappe Cloud](https://frappecloud.com), a simple, user-friendly and sophisticated [open-source](https://github.com/frappe/press) platform to host Frappe applications with peace of mind.

It takes care of installation, setup, upgrades, monitoring, maintenance and support of your Frappe deployments. It is a fully featured developer platform with an ability to manage and control multiple Frappe deployments.

&lt;div&gt;
	&lt;a href=&quot;https://frappecloud.com/hrms/signup&quot; target=&quot;_blank&quot;&gt;
		&lt;picture&gt;
			&lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://frappe.io/files/try-on-fc-white.png&quot;&gt;
			&lt;img src=&quot;https://frappe.io/files/try-on-fc-black.png&quot; alt=&quot;Try on Frappe Cloud&quot; height=&quot;28&quot; /&gt;
		&lt;/picture&gt;
	&lt;/a&gt;
&lt;/div&gt;


## Development setup
### Docker
You need Docker, docker-compose and git setup on your machine. Refer [Docker documentation](https://docs.docker.com/). After that, run the following commands:
```
git clone https://github.com/frappe/hrms
cd hrms/docker
docker-compose up
```

Wait for some time until the setup script creates a site. After that you can access `http://localhost:8000` in your browser and the login screen for HR should show up.

Use the following credentials to log in:

- Username: `Administrator`
- Password: `admin`

### Local

1. Set up bench by following the [Installation Steps](https://frappeframework.com/docs/user/en/installation) and start the server and keep it running
	```sh
	$ bench start
	```
2. In a separate terminal window, run the following commands
	```sh
	$ bench new-site hrms.local
	$ bench get-app erpnext
	$ bench get-app hrms
	$ bench --site hrms.local install-app hrms
	$ bench --site hrms.local add-to-hosts
	```
3. You can access the site at `http://hrms.local:8080`

## Learning and Community

1. [Frappe School](https://frappe.school) - Learn Frappe Framework and ERPNext from the various courses by the maintainers or from the community.
2. [Documentation](https://docs.frappe.io/hr) - Extensive documentation for Frappe HR.
3. [User Forum](https://discuss.erpnext.com/) - Engage with the community of ERPNext users and service providers.
4. [Telegram Group](https://t.me/frappehr) - Get instant help from the community of users.


## Contributing

1. [Issue Guidelines](https://github.com/frappe/erpnext/wiki/Issue-Guidelines)
1. [Report Security Vulnerabilities](https://erpnext.com/security)
1. [Pull Request Requirements](https://github.com/frappe/erpnext/wiki/Contribution-Guidelines)


## Logo and Trademark Policy

Please read our [Logo and Trademark Policy](TRADEMARK_POLICY.md).

&lt;br /&gt;
&lt;br /&gt;
&lt;div align=&quot;center&quot; style=&quot;padding-top: 0.75rem;&quot;&gt;
	&lt;a href=&quot;https://frappe.io&quot; target=&quot;_blank&quot;&gt;
		&lt;picture&gt;
			&lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://frappe.io/files/Frappe-white.png&quot;&gt;
			&lt;img src=&quot;https://frappe.io/files/Frappe-black.png&quot; alt=&quot;Frappe Technologies&quot; height=&quot;28&quot;/&gt;
		&lt;/picture&gt;
	&lt;/a&gt;
&lt;/div&gt;

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[prs-eth/Marigold]]></title>
            <link>https://github.com/prs-eth/Marigold</link>
            <guid>https://github.com/prs-eth/Marigold</guid>
            <pubDate>Sun, 18 May 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[[CVPR 2024 - Oral, Best Paper Award Candidate] Marigold: Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/prs-eth/Marigold">prs-eth/Marigold</a></h1>
            <p>[CVPR 2024 - Oral, Best Paper Award Candidate] Marigold: Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation</p>
            <p>Language: Python</p>
            <p>Stars: 2,713</p>
            <p>Forks: 170</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre># Marigold Computer Vision

This project implements Marigold, a Computer Vision method for estimating image characteristics. Initially proposed for
extracting high-resolution depth maps in our CVPR 2024 paper **&quot;Repurposing Diffusion-Based Image Generators for Monocular 
Depth Estimation&quot;**, we extended the method to other modalities as described in our follow-up paper **&quot;Marigold: Affordable 
Adaptation of Diffusion-Based Image Generators for Image Analysis&quot;**. 

## Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis

[![Website](doc/badges/badge-website.svg)](https://marigoldcomputervision.github.io)
[![Paper](doc/badges/badge-pdf.svg)](https://arxiv.org/abs/2505.09358)
[![Depth Demo](https://img.shields.io/badge/ğŸ¤—%20Depth-Demo-yellow)](https://huggingface.co/spaces/prs-eth/marigold)
[![Normals Demo](https://img.shields.io/badge/ğŸ¤—%20Normals-Demo-yellow)](https://huggingface.co/spaces/prs-eth/marigold-normals)
[![Intrinsics Demo](https://img.shields.io/badge/ğŸ¤—%20Image%20Intrinsics-Demo-yellow)](https://huggingface.co/spaces/prs-eth/marigold-iid)
[![Depth Model](https://img.shields.io/badge/ğŸ¤—%20Depth-Model-green)](https://huggingface.co/prs-eth/marigold-depth-v1-1)
[![Normals Model](https://img.shields.io/badge/ğŸ¤—%20Normals-Model-green)](https://huggingface.co/prs-eth/marigold-normals-v1-1)
[![Intrinsics Appearance Model](https://img.shields.io/badge/ğŸ¤—%20Image%20Intrinsics%20Appearance-Model-green)](https://huggingface.co/prs-eth/marigold-iid-appearance-v1-1)
[![Intrinsics Lighting Model](https://img.shields.io/badge/ğŸ¤—%20Image%20Intrinsics%20Lighting-Model-green)](https://huggingface.co/prs-eth/marigold-iid-lighting-v1-1)
[![Diffusers Tutorial](doc/badges/badge-hfdiffusers.svg)](https://huggingface.co/docs/diffusers/using-diffusers/marigold_usage)

Team:
[Bingxin Ke](http://www.kebingxin.com/),
[Kevin Qu](https://www.linkedin.com/in/kevin-qu-b3417621b/),
[Tianfu Wang](https://tianfwang.github.io/)
[Nando Metzger](https://nandometzger.github.io/),
[Shengyu Huang](https://shengyuh.github.io/),
[Bo Li](https://www.linkedin.com/in/bobboli0202/),
[Anton Obukhov](https://www.obukhov.ai/),
[Konrad Schindler](https://scholar.google.com/citations?user=FZuNgqIAAAAJ)

We present Marigold, a family of conditional generative models and a fine-tuning protocol that extracts the knowledge 
from pretrained latent diffusion models like Stable Diffusion and adapts them for dense image analysis tasks, including 
monocular depth estimation, surface normal prediction, and intrinsic decomposition. Marigold requires minimal 
modification of the pre-trained latent diffusion model&#039;s architecture, trains with small synthetic datasets on a single 
GPU over a few days, and demonstrates state-of-the-art zero-shot generalization.

![teaser_all](doc/teaser_marigold_all.jpg)

## Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation

[![Website](doc/badges/badge-website.svg)](https://marigoldmonodepth.github.io)
[![Paper](doc/badges/badge-pdf.svg)](https://arxiv.org/abs/2312.02145)
[![Hugging Face Space](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Space-yellow)](https://huggingface.co/spaces/prs-eth/marigold)
[![Hugging Face Model](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face%20-Model-green)](https://huggingface.co/prs-eth/marigold-depth-v1-1)
[![Open In Colab](doc/badges/badge-colab.svg)](https://colab.research.google.com/drive/12G8reD13DdpMie5ZQlaFNo2WCGeNUH-u?usp=sharing)

In **CVPR 2024 (Oral, Best Paper Award Candidate)**&lt;br&gt; 
Team:
[Bingxin Ke](http://www.kebingxin.com/),
[Anton Obukhov](https://www.obukhov.ai/),
[Shengyu Huang](https://shengyuh.github.io/),
[Nando Metzger](https://nandometzger.github.io/),
[Rodrigo Caye Daudt](https://rcdaudt.github.io/),
[Konrad Schindler](https://scholar.google.com/citations?user=FZuNgqIAAAAJ)

We present Marigold, a diffusion model, and an associated fine-tuning protocol for monocular depth estimation. Its core 
principle is to leverage the rich visual knowledge stored in modern generative image models. Our model, derived from 
Stable Diffusion and fine-tuned with synthetic data, can zero-shot transfer to unseen data, offering state-of-the-art 
monocular depth estimation results.

![teaser_depth](doc/teaser_marigold_depth.png)

## ğŸ“¢ News
2025-05-15: Released code and a [checkpoint](https://huggingface.co/prs-eth/marigold-iid-lighting-v1-1) of Marigold Intrinsic Image Decomposition predicting Albedo, diffuse Shading, and non-diffuse Residual (Marigold-IID-Lighting v1.1).&lt;br&gt;
2025-05-15: Released code and a [checkpoint](https://huggingface.co/prs-eth/marigold-iid-appearance-v1-1) of Marigold Intrinsic Image Decomposition predicting Albedo, Roughness, and Metallicity (Marigold-IID-Appearance v1.1).&lt;br&gt;
2025-05-15: Released code and a [checkpoint](https://huggingface.co/prs-eth/marigold-normals-v1-1) of Marigold Surface Normals Estimation (v1.1).&lt;br&gt;
2025-05-15: Released an updated [checkpoint](https://huggingface.co/prs-eth/marigold-depth-v1-1) of Marigold Depth (v1.1), trained with updated noise scheduler settings (zero-SNR and trailing timestamps), and augmentations.&lt;br&gt;
2024-05-28: Training code is released.&lt;br&gt;
2024-05-27: Marigold pipelines are merged into the `diffusers` core starting v0.28.0 [release](https://github.com/huggingface/diffusers/releases/tag/v0.28.0)!&lt;br&gt;
2024-03-23: Added a Latent Consistency Model (LCM) [checkpoint](https://huggingface.co/prs-eth/marigold-depth-lcm-v1-0).&lt;br&gt;
2024-03-04: The paper is accepted at CVPR 2024.&lt;br&gt;
2023-12-22: Contributed to Diffusers [community pipeline](https://github.com/huggingface/diffusers/tree/main/examples/community#marigold-depth-estimation).&lt;br&gt;
2023-12-19: Updated [license](LICENSE.txt) to Apache License, Version 2.0.&lt;br&gt;
2023-12-08: Added the first interactive [Hugging Face Space Demo](https://huggingface.co/spaces/prs-eth/marigold) of depth estimation.&lt;br&gt;
2023-12-05: Added a [Google Colab](https://colab.research.google.com/drive/12G8reD13DdpMie5ZQlaFNo2WCGeNUH-u?usp=sharing)&lt;br&gt;
2023-12-04: Added an [arXiv paper](https://arxiv.org/abs/2312.02145) and inference code (this repository).

## ğŸš€ Usage

**We offer several ways to interact with Marigold**:

1. A family of free online interactive demos: 
&lt;a href=&quot;https://huggingface.co/spaces/prs-eth/marigold&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ğŸ¤—%20Depth-Demo-yellow&quot; height=&quot;16&quot;&gt;&lt;/a&gt; 
&lt;a href=&quot;https://huggingface.co/spaces/prs-eth/marigold-normals&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ğŸ¤—%20Normals-Demo-yellow&quot; height=&quot;16&quot;&gt;&lt;/a&gt; 
&lt;a href=&quot;https://huggingface.co/spaces/prs-eth/marigold-iid&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ğŸ¤—%20Image%20Intrinsics-Demo-yellow&quot; height=&quot;16&quot;&gt;&lt;/a&gt; 
(kudos to the HF team for the GPU grants)

1. Marigold pipelines are part of
&lt;a href=&quot;https://huggingface.co/docs/diffusers/using-diffusers/marigold_usage&quot;&gt;&lt;img src=&quot;doc/badges/badge-hfdiffusers.svg&quot; height=&quot;16&quot;&gt;&lt;/a&gt; - a one-stop shop for diffusion ğŸ§¨!

1. Run the demo locally (requires a GPU and an `nvidia-docker2`, see [Installation Guide](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)):
`docker run -it -p 7860:7860 --platform=linux/amd64 --gpus all registry.hf.space/prs-eth-marigold:latest python app.py`

1. Extended demo on a Google Colab: &lt;a href=&quot;https://colab.research.google.com/drive/12G8reD13DdpMie5ZQlaFNo2WCGeNUH-u?usp=sharing&quot;&gt;&lt;img src=&quot;doc/badges/badge-colab.svg&quot; height=&quot;16&quot;&gt;&lt;/a&gt;

1. If you just want to see the examples, visit our gallery: &lt;a href=&quot;https://marigoldcomputervision.github.io&quot;&gt;&lt;img src=&quot;doc/badges/badge-website.svg&quot; height=&quot;16&quot;&gt;&lt;/a&gt;

1. Finally, local development instructions with this codebase are given below.

## ğŸ› ï¸ Setup

The inference code was tested on:

- Ubuntu 22.04 LTS, Python 3.10.12,  CUDA 11.7, GeForce RTX 3090 (pip)

### ğŸª§ A Note for Windows users

We recommend running the code in WSL2:

1. Install WSL following [installation guide](https://learn.microsoft.com/en-us/windows/wsl/install#install-wsl-command).
1. Install CUDA support for WSL following [installation guide](https://docs.nvidia.com/cuda/wsl-user-guide/index.html#cuda-support-for-wsl-2).
1. Find your drives in `/mnt/&lt;drive letter&gt;/`; check [WSL FAQ](https://learn.microsoft.com/en-us/windows/wsl/faq#how-do-i-access-my-c--drive-) for more details. Navigate to the working directory of choice. 

### ğŸ“¦ Repository

Clone the repository (requires git):

```bash
git clone https://github.com/prs-eth/Marigold.git
cd Marigold
```

### ğŸ’» Dependencies

Install the dependencies:

```bash
python -m venv venv/marigold
source venv/marigold/bin/activate
pip install -r requirements.txt
```

Keep the environment activated before running the inference script. 
Activate the environment again after restarting the terminal session.

## ğŸƒ Testing on your images

### ğŸ“· Prepare images

Use selected images from our paper:

```bash
bash script/download_sample_data.sh
```

Or place your images in a directory, for example, under `input/in-the-wild_example`, and run the following inference command.

### ğŸš€ Run inference (for practical usage)

```bash
# Depth
python script/depth/run.py \
    --checkpoint prs-eth/marigold-depth-v1-1 \
    --input_rgb_dir input/in-the-wild_example \
    --output_dir output/in-the-wild_example \
    --fp16
```

```bash
# Normals
python script/normals/run.py \
    --checkpoint prs-eth/marigold-normals-v1-1 \
    --input_rgb_dir input/in-the-wild_example \
    --output_dir output/in-the-wild_example \
    --fp16
```

```bash
# IID (appearance model)
python script/iid/run.py \
    --checkpoint prs-eth/marigold-iid-appearance-v1-1 \
    --input_rgb_dir input/in-the-wild_example \
    --output_dir output/in-the-wild_example \
    --fp16

# IID (lighting model)
python script/iid/run.py \
    --checkpoint prs-eth/marigold-iid-lighting-v1-1 \
    --input_rgb_dir input/in-the-wild_example \
    --output_dir output/in-the-wild_example \
    --fp16
```

### âš™ï¸ Inference settings

The default settings are optimized for the best results. However, the behavior of the code can be customized:

- `--half_precision` or `--fp16`: Run with half-precision (16-bit float) to have faster speed and reduced VRAM usage, but might lead to suboptimal results.

- `--ensemble_size`: Number of inference passes in the ensemble. Larger values tend to give better results in evaluations at the cost of slower inference; for most cases 1 is enough. Default: 1.

- `--denoise_steps`: Number of denoising diffusion steps. Default settings are defined in the model checkpoints and are sufficient for most cases.

- By default, the inference script resizes input images to the *processing resolution*, and then resizes the prediction back to the original resolution. This gives the best quality, as Stable Diffusion, from which Marigold is derived, performs best at 768x768 resolution.  
  
  - `--processing_res`: the processing resolution; set as 0 to process the input resolution directly. When unassigned (`None`), will read default setting from model config. Default: `None`.
  - `--output_processing_res`: produce output at the processing resolution instead of upsampling it to the input resolution. Default: False.
  - `--resample_method`: the resampling method used to resize images and depth predictions. This can be one of `bilinear`, `bicubic`, or `nearest`. Default: `bilinear`.

- `--seed`: Random seed can be set to ensure additional reproducibility. Default: None (unseeded). Note: forcing `--batch_size 1` helps to increase reproducibility. To ensure full reproducibility, [deterministic mode](https://pytorch.org/docs/stable/notes/randomness.html#avoiding-nondeterministic-algorithms) needs to be used.
- `--batch_size`: Batch size of repeated inference. Default: 0 (best value determined automatically).
- `--color_map`: [Colormap](https://matplotlib.org/stable/users/explain/colors/colormaps.html) used to colorize the depth prediction. Default: Spectral. Set to `None` to skip colored depth map generation.
- `--apple_silicon`: Use Apple Silicon MPS acceleration.


### ğŸ® Run inference (for academic comparisons)

These settings correspond to our paper. For academic comparison, please run with the settings below (if you only want to do fast inference on your own images, you can set `--ensemble_size 1`).

```bash
# Depth
python script/depth/run.py \
    --checkpoint prs-eth/marigold-depth-v1-1 \
    --denoise_steps 1 \
    --ensemble_size 10 \
    --input_rgb_dir input/in-the-wild_example \
    --output_dir output/in-the-wild_example
```

```bash
# Normals
python script/normals/run.py \
    --checkpoint prs-eth/marigold-normals-v1-1 \
    --denoise_steps 4 \
    --ensemble_size 10 \
    --input_rgb_dir input/in-the-wild_example \
    --output_dir output/in-the-wild_example
```

```bash
# IID (appearance model)
python script/iid/run.py \
    --checkpoint prs-eth/marigold-iid-appearance-v1-1 \
    --denoise_steps 4 \
    --ensemble_size 1 \
    --input_rgb_dir input/in-the-wild_example \
    --output_dir output/in-the-wild_example

# IID (lighting model)
python script/iid/run.py \
    --checkpoint prs-eth/marigold-iid-lighting-v1-1 \
    --denoise_steps 4 \
    --ensemble_size 1 \
    --input_rgb_dir input/in-the-wild_example \
    --output_dir output/in-the-wild_example
```

```bash
# Depth (the original CVPR version)
python script/depth/run.py \
    --checkpoint prs-eth/marigold-depth-v1-0 \
    --denoise_steps 50 \
    --ensemble_size 10 \
    --input_rgb_dir input/in-the-wild_example \
    --output_dir output/in-the-wild_example
```

You can find all results in the `output` directory. Enjoy!


### â¬‡ Checkpoint cache

By default, the checkpoint ([depth](https://huggingface.co/prs-eth/marigold-depth-v1-1), [normals](https://huggingface.co/prs-eth/marigold-normals-v1-1), [iid](https://huggingface.co/prs-eth/marigold-iid-appearance-v1-1))  is stored in the Hugging Face cache.
The `HF_HOME` environment variable defines its location and can be overridden, e.g.:

```bash
export HF_HOME=$(pwd)/cache
```

Alternatively, use the following script to download the checkpoint weights locally:

```bash
bash script/download_weights.sh marigold-depth-v1-1           # depth checkpoint
bash script/download_weights.sh marigold-normals-v1-1         # normals checkpoint
bash script/download_weights.sh marigold-iid-appearance-v1-1  # iid appearance checkpoint
bash script/download_weights.sh marigold-iid-lighting-v1-1    # iid lighting checkpoint
# bash script/download_weights.sh marigold-depth-v1-0         # CVPR depth checkpoint
```

At inference, specify the checkpoint path:

```bash
# Depth
python script/depth/run.py \
    --checkpoint checkpoint/marigold-depth-v1-1 \
    --denoise_steps 4 \
    --ensemble_size 1 \
    --input_rgb_dir input/in-the-wild_example \
    --output_dir output/in-the-wild_example
```

```bash
# Normals
python script/normals/run.py \
    --checkpoint checkpoint/marigold-normals-v1-1 \
    --denoise_steps 4 \
    --ensemble_size 1 \
    --input_rgb_dir input/in-the-wild_example \
    --output_dir output/in-the-wild_example
```

```bash
# IID (appearance model)
python script/iid/run.py \
    --checkpoint checkpoint/marigold-iid-appearance-v1-1 \
    --denoise_steps 4 \
    --ensemble_size 1 \
    --input_rgb_dir input/in-the-wild_example \
    --output_dir output/in-the-wild_example

# IID (lighting model)
python script/iid/run.py \
    --checkpoint checkpoint/marigold-iid-lighting-v1-1 \
    --denoise_steps 4 \
    --ensemble_size 1 \
    --input_rgb_dir input/in-the-wild_example \
    --output_dir output/in-the-wild_example
```

## ğŸ¦¿ Evaluation on test datasets &lt;a name=&quot;evaluation&quot;&gt;&lt;/a&gt;
Install additional dependencies:

```bash
pip install -r requirements+.txt -r requirements.txt
``` 

Set data directory variable (also needed in evaluation scripts) and download the evaluation datasets ([depth](https://share.phys.ethz.ch/~pf/bingkedata/marigold/evaluation_dataset), [normals](https://share.phys.ethz.ch/~pf/bingkedata/marigold/marigold_normals/evaluation_dataset)) into the corresponding subfolders:

```bash
export BASE_DATA_DIR=&lt;YOUR_DATA_DIR&gt;  # Set target data directory

# Depth
wget -r -np -nH --cut-dirs=4 -R &quot;index.html*&quot; -P ${BASE_DATA_DIR} https://share.phys.ethz.ch/~pf/bingkedata/marigold/evaluation_dataset/

# Normals
wget -r -np -nH --cut-dirs=4 -R &quot;index.html*&quot; -P ${BASE_DATA_DIR} https://share.phys.ethz.ch/~pf/bingkedata/marigold/marigold_normals/evaluation_dataset.zip
unzip ${BASE_DATA_DIR}/evaluation_dataset.zip -d ${BASE_DATA_DIR}/
rm -f ${BASE_DATA_DIR}/evaluation_dataset.zip
```
For download instructions of the intrinsic image decomposition test data, please refer to [iid-appearance instructions](script/iid/dataset_preprocess/interiorverse_appearance/README.md) and [iid-lighting instructions](script/iid/dataset_preprocess/hypersim_lighting/README.md). 

Run inference and evaluation scripts, for example:

```bash
# Depth
bash script/depth/eval/11_infer_nyu.sh  # Run inference
bash script/depth/eval/12_eval_nyu.sh   # Evaluate predictions
```

```bash
# Normals
bash script/normals/eval/11_infer_scannet.sh  # Run inference
bash script/normals/eval/12_eval_scannet.sh   # Evaluate predictions
```

```bash
# IID
bash script/iid/eval/11_infer_appearance_interiorverse.sh  # Run inference
bash script/iid/eval/12_eval_appearance_interiorverse.sh   # Evaluate predictions

bash script/iid/eval/21_infer_lighting_hypersim.sh  # Run inference
bash script/iid/eval/22_eval_lighting_hypersim.sh   # Evaluate predictions
```

```bash
# Depth (the original CVPR version)
bash script/depth/eval_old/11_infer_nyu.sh  # Run inference
bash script/depth/eval_old/12_eval_nyu.sh   # Evaluate predictions
```

Note: although the seed has been set, the results might still be slightly different on different hardware.

## ğŸ‹ï¸ Training

Based on the previously created environment, install extended requirements:

```bash
pip install -r requirements++.txt -r requirements+.txt -r requirements.txt
```

Set environment parameters for the data directory:

```bash
export BASE_DATA_DIR=YOUR_DATA_DIR        # directory of training data
export BASE_CKPT_DIR=YOUR_CHECKPOINT_DIR  # directory of pretrained checkpoint
```

Download Stable Diffusion v2 [checkpoint](https://huggingface.co/stabilityai/stable-diffusion-2) into `${BASE_CKPT_DIR}`

### Prepare for training data
**Depth**

Prepare for [Hypersim](https://github.com/apple/ml-hypersim) and [Virtual KITTI 2](https://europe.naverlabs.com/research/computer-vision/proxy-virtual-worlds-vkitti-2/) datasets and save into `${BASE_DATA_DIR}`. Please refer to [this README](script/depth/dataset_preprocess/hypersim/README.md) for Hypersim preprocessing.

**Normals**

Prepare for [Hypersim](https://github.com/apple/ml-hypersim), [Interiorverse](https://interiorverse.github.io/) and [Sintel](http://sintel.is.tue.mpg.de/) datasets and save into `${BASE_DATA_DIR}`. Please refer to [this README](script/normals/dataset_preprocess/hypersim/README.md) for Hypersim preprocessing, [this README](script/normals/dataset_preprocess/interiorverse/README.md) for Interiorverse and [this README](script/normals/dataset_preprocess/sintel/README.md) for Sintel.

**Intrinsic Image Decomposition**

*Appearance model*: Prepare for [Interiorverse](https://interiorverse.github.io/) dataset and save into `${BASE_DATA_DIR}`. Please refer to [this README](script/iid/dataset_preprocess/interiorverse_appearance/README.md) for Interiorverse preprocessing.

*Lighting model*: Prepare for [Hypersim](https://github.com/apple/ml-hypersim) dataset and save into `${BASE_DATA_DIR}`. Please refer to [this README](script/iid/dataset_preprocess/hypersim_lighting/README.md) for Hypersim preprocessing.


### Run training script

```bash
# Depth
python script/depth/train.py --config config/train_marigold_depth.yaml
```

```bash
# Normals
python script/normals/train.py --config config/train_marigold_normals.yaml
```

```bash
# IID (appearance model)
python script/iid/train.py --config

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[BerriAI/litellm]]></title>
            <link>https://github.com/BerriAI/litellm</link>
            <guid>https://github.com/BerriAI/litellm</guid>
            <pubDate>Sun, 18 May 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq]]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/BerriAI/litellm">BerriAI/litellm</a></h1>
            <p>Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq]</p>
            <p>Language: Python</p>
            <p>Stars: 22,747</p>
            <p>Forks: 2,924</p>
            <p>Stars today: 71 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
        ğŸš… LiteLLM
    &lt;/h1&gt;
    &lt;p align=&quot;center&quot;&gt;
        &lt;p align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://render.com/deploy?repo=https://github.com/BerriAI/litellm&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;https://render.com/images/deploy-to-render-button.svg&quot; alt=&quot;Deploy to Render&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://railway.app/template/HLP0Ub?referralCode=jch2ME&quot;&gt;
          &lt;img src=&quot;https://railway.app/button.svg&quot; alt=&quot;Deploy on Railway&quot;&gt;
        &lt;/a&gt;
        &lt;/p&gt;
        &lt;p align=&quot;center&quot;&gt;Call all LLM APIs using the OpenAI format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.]
        &lt;br&gt;
    &lt;/p&gt;
&lt;h4 align=&quot;center&quot;&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/simple_proxy&quot; target=&quot;_blank&quot;&gt;LiteLLM Proxy Server (LLM Gateway)&lt;/a&gt; | &lt;a href=&quot;https://docs.litellm.ai/docs/hosted&quot; target=&quot;_blank&quot;&gt; Hosted Proxy (Preview)&lt;/a&gt; | &lt;a href=&quot;https://docs.litellm.ai/docs/enterprise&quot;target=&quot;_blank&quot;&gt;Enterprise Tier&lt;/a&gt;&lt;/h4&gt;
&lt;h4 align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://pypi.org/project/litellm/&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/pypi/v/litellm.svg&quot; alt=&quot;PyPI Version&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://www.ycombinator.com/companies/berriai&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/Y%20Combinator-W23-orange?style=flat-square&quot; alt=&quot;Y Combinator W23&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://wa.link/huol9n&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/static/v1?label=Chat%20on&amp;message=WhatsApp&amp;color=success&amp;logo=WhatsApp&amp;style=flat-square&quot; alt=&quot;Whatsapp&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://discord.gg/wuPM9dRgDw&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/static/v1?label=Chat%20on&amp;message=Discord&amp;color=blue&amp;logo=Discord&amp;style=flat-square&quot; alt=&quot;Discord&quot;&gt;
    &lt;/a&gt;
&lt;/h4&gt;

LiteLLM manages:

- Translate inputs to provider&#039;s `completion`, `embedding`, and `image_generation` endpoints
- [Consistent output](https://docs.litellm.ai/docs/completion/output), text responses will always be available at `[&#039;choices&#039;][0][&#039;message&#039;][&#039;content&#039;]`
- Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - [Router](https://docs.litellm.ai/docs/routing)
- Set Budgets &amp; Rate limits per project, api key, model [LiteLLM Proxy Server (LLM Gateway)](https://docs.litellm.ai/docs/simple_proxy)

[**Jump to LiteLLM Proxy (LLM Gateway) Docs**](https://github.com/BerriAI/litellm?tab=readme-ov-file#openai-proxy---docs) &lt;br&gt;
[**Jump to Supported LLM Providers**](https://github.com/BerriAI/litellm?tab=readme-ov-file#supported-providers-docs)

ğŸš¨ **Stable Release:** Use docker images with the `-stable` tag. These have undergone 12 hour load tests, before being published. [More information about the release cycle here](https://docs.litellm.ai/docs/proxy/release_cycle)

Support for more providers. Missing a provider or LLM Platform, raise a [feature request](https://github.com/BerriAI/litellm/issues/new?assignees=&amp;labels=enhancement&amp;projects=&amp;template=feature_request.yml&amp;title=%5BFeature%5D%3A+).

# Usage ([**Docs**](https://docs.litellm.ai/docs/))

&gt; [!IMPORTANT]
&gt; LiteLLM v1.0.0 now requires `openai&gt;=1.0.0`. Migration guide [here](https://docs.litellm.ai/docs/migration)  
&gt; LiteLLM v1.40.14+ now requires `pydantic&gt;=2.0.0`. No changes required.

&lt;a target=&quot;_blank&quot; href=&quot;https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/liteLLM_Getting_Started.ipynb&quot;&gt;
  &lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot;/&gt;
&lt;/a&gt;

```shell
pip install litellm
```

```python
from litellm import completion
import os

## set ENV variables
os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;your-openai-key&quot;
os.environ[&quot;ANTHROPIC_API_KEY&quot;] = &quot;your-anthropic-key&quot;

messages = [{ &quot;content&quot;: &quot;Hello, how are you?&quot;,&quot;role&quot;: &quot;user&quot;}]

# openai call
response = completion(model=&quot;openai/gpt-4o&quot;, messages=messages)

# anthropic call
response = completion(model=&quot;anthropic/claude-3-sonnet-20240229&quot;, messages=messages)
print(response)
```

### Response (OpenAI Format)

```json
{
    &quot;id&quot;: &quot;chatcmpl-565d891b-a42e-4c39-8d14-82a1f5208885&quot;,
    &quot;created&quot;: 1734366691,
    &quot;model&quot;: &quot;claude-3-sonnet-20240229&quot;,
    &quot;object&quot;: &quot;chat.completion&quot;,
    &quot;system_fingerprint&quot;: null,
    &quot;choices&quot;: [
        {
            &quot;finish_reason&quot;: &quot;stop&quot;,
            &quot;index&quot;: 0,
            &quot;message&quot;: {
                &quot;content&quot;: &quot;Hello! As an AI language model, I don&#039;t have feelings, but I&#039;m operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?&quot;,
                &quot;role&quot;: &quot;assistant&quot;,
                &quot;tool_calls&quot;: null,
                &quot;function_call&quot;: null
            }
        }
    ],
    &quot;usage&quot;: {
        &quot;completion_tokens&quot;: 43,
        &quot;prompt_tokens&quot;: 13,
        &quot;total_tokens&quot;: 56,
        &quot;completion_tokens_details&quot;: null,
        &quot;prompt_tokens_details&quot;: {
            &quot;audio_tokens&quot;: null,
            &quot;cached_tokens&quot;: 0
        },
        &quot;cache_creation_input_tokens&quot;: 0,
        &quot;cache_read_input_tokens&quot;: 0
    }
}
```

Call any model supported by a provider, with `model=&lt;provider_name&gt;/&lt;model_name&gt;`. There might be provider-specific details here, so refer to [provider docs for more information](https://docs.litellm.ai/docs/providers)

## Async ([Docs](https://docs.litellm.ai/docs/completion/stream#async-completion))

```python
from litellm import acompletion
import asyncio

async def test_get_response():
    user_message = &quot;Hello, how are you?&quot;
    messages = [{&quot;content&quot;: user_message, &quot;role&quot;: &quot;user&quot;}]
    response = await acompletion(model=&quot;openai/gpt-4o&quot;, messages=messages)
    return response

response = asyncio.run(test_get_response())
print(response)
```

## Streaming ([Docs](https://docs.litellm.ai/docs/completion/stream))

liteLLM supports streaming the model response back, pass `stream=True` to get a streaming iterator in response.  
Streaming is supported for all models (Bedrock, Huggingface, TogetherAI, Azure, OpenAI, etc.)

```python
from litellm import completion
response = completion(model=&quot;openai/gpt-4o&quot;, messages=messages, stream=True)
for part in response:
    print(part.choices[0].delta.content or &quot;&quot;)

# claude 2
response = completion(&#039;anthropic/claude-3-sonnet-20240229&#039;, messages, stream=True)
for part in response:
    print(part)
```

### Response chunk (OpenAI Format)

```json
{
    &quot;id&quot;: &quot;chatcmpl-2be06597-eb60-4c70-9ec5-8cd2ab1b4697&quot;,
    &quot;created&quot;: 1734366925,
    &quot;model&quot;: &quot;claude-3-sonnet-20240229&quot;,
    &quot;object&quot;: &quot;chat.completion.chunk&quot;,
    &quot;system_fingerprint&quot;: null,
    &quot;choices&quot;: [
        {
            &quot;finish_reason&quot;: null,
            &quot;index&quot;: 0,
            &quot;delta&quot;: {
                &quot;content&quot;: &quot;Hello&quot;,
                &quot;role&quot;: &quot;assistant&quot;,
                &quot;function_call&quot;: null,
                &quot;tool_calls&quot;: null,
                &quot;audio&quot;: null
            },
            &quot;logprobs&quot;: null
        }
    ]
}
```

## Logging Observability ([Docs](https://docs.litellm.ai/docs/observability/callbacks))

LiteLLM exposes pre defined callbacks to send data to Lunary, MLflow, Langfuse, DynamoDB, s3 Buckets, Helicone, Promptlayer, Traceloop, Athina, Slack

```python
from litellm import completion

## set env variables for logging tools (when using MLflow, no API key set up is required)
os.environ[&quot;LUNARY_PUBLIC_KEY&quot;] = &quot;your-lunary-public-key&quot;
os.environ[&quot;HELICONE_API_KEY&quot;] = &quot;your-helicone-auth-key&quot;
os.environ[&quot;LANGFUSE_PUBLIC_KEY&quot;] = &quot;&quot;
os.environ[&quot;LANGFUSE_SECRET_KEY&quot;] = &quot;&quot;
os.environ[&quot;ATHINA_API_KEY&quot;] = &quot;your-athina-api-key&quot;

os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;your-openai-key&quot;

# set callbacks
litellm.success_callback = [&quot;lunary&quot;, &quot;mlflow&quot;, &quot;langfuse&quot;, &quot;athina&quot;, &quot;helicone&quot;] # log input/output to lunary, langfuse, supabase, athina, helicone etc

#openai call
response = completion(model=&quot;openai/gpt-4o&quot;, messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hi ğŸ‘‹ - i&#039;m openai&quot;}])
```

# LiteLLM Proxy Server (LLM Gateway) - ([Docs](https://docs.litellm.ai/docs/simple_proxy))

Track spend + Load Balance across multiple projects

[Hosted Proxy (Preview)](https://docs.litellm.ai/docs/hosted)

The proxy provides:

1. [Hooks for auth](https://docs.litellm.ai/docs/proxy/virtual_keys#custom-auth)
2. [Hooks for logging](https://docs.litellm.ai/docs/proxy/logging#step-1---create-your-custom-litellm-callback-class)
3. [Cost tracking](https://docs.litellm.ai/docs/proxy/virtual_keys#tracking-spend)
4. [Rate Limiting](https://docs.litellm.ai/docs/proxy/users#set-rate-limits)

## ğŸ“– Proxy Endpoints - [Swagger Docs](https://litellm-api.up.railway.app/)


## Quick Start Proxy - CLI

```shell
pip install &#039;litellm[proxy]&#039;
```

### Step 1: Start litellm proxy

```shell
$ litellm --model huggingface/bigcode/starcoder

#INFO: Proxy running on http://0.0.0.0:4000
```

### Step 2: Make ChatCompletions Request to Proxy


&gt; [!IMPORTANT]
&gt; ğŸ’¡ [Use LiteLLM Proxy with Langchain (Python, JS), OpenAI SDK (Python, JS) Anthropic SDK, Mistral SDK, LlamaIndex, Instructor, Curl](https://docs.litellm.ai/docs/proxy/user_keys)  

```python
import openai # openai v1.0.0+
client = openai.OpenAI(api_key=&quot;anything&quot;,base_url=&quot;http://0.0.0.0:4000&quot;) # set proxy to base_url
# request sent to model set on litellm proxy, `litellm --model`
response = client.chat.completions.create(model=&quot;gpt-3.5-turbo&quot;, messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;this is a test request, write a short poem&quot;
    }
])

print(response)
```

## Proxy Key Management ([Docs](https://docs.litellm.ai/docs/proxy/virtual_keys))

Connect the proxy with a Postgres DB to create proxy keys

```bash
# Get the code
git clone https://github.com/BerriAI/litellm

# Go to folder
cd litellm

# Add the master key - you can change this after setup
echo &#039;LITELLM_MASTER_KEY=&quot;sk-1234&quot;&#039; &gt; .env

# Add the litellm salt key - you cannot change this after adding a model
# It is used to encrypt / decrypt your LLM API Key credentials
# We recommend - https://1password.com/password-generator/ 
# password generator to get a random hash for litellm salt key
echo &#039;LITELLM_SALT_KEY=&quot;sk-1234&quot;&#039; &gt; .env

source .env

# Start
docker-compose up
```


UI on `/ui` on your proxy server
![ui_3](https://github.com/BerriAI/litellm/assets/29436595/47c97d5e-b9be-4839-b28c-43d7f4f10033)

Set budgets and rate limits across multiple projects
`POST /key/generate`

### Request

```shell
curl &#039;http://0.0.0.0:4000/key/generate&#039; \
--header &#039;Authorization: Bearer sk-1234&#039; \
--header &#039;Content-Type: application/json&#039; \
--data-raw &#039;{&quot;models&quot;: [&quot;gpt-3.5-turbo&quot;, &quot;gpt-4&quot;, &quot;claude-2&quot;], &quot;duration&quot;: &quot;20m&quot;,&quot;metadata&quot;: {&quot;user&quot;: &quot;ishaan@berri.ai&quot;, &quot;team&quot;: &quot;core-infra&quot;}}&#039;
```

### Expected Response

```shell
{
    &quot;key&quot;: &quot;sk-kdEXbIqZRwEeEiHwdg7sFA&quot;, # Bearer token
    &quot;expires&quot;: &quot;2023-11-19T01:38:25.838000+00:00&quot; # datetime object
}
```

## Supported Providers ([Docs](https://docs.litellm.ai/docs/providers))

| Provider                                                                            | [Completion](https://docs.litellm.ai/docs/#basic-usage) | [Streaming](https://docs.litellm.ai/docs/completion/stream#streaming-responses) | [Async Completion](https://docs.litellm.ai/docs/completion/stream#async-completion) | [Async Streaming](https://docs.litellm.ai/docs/completion/stream#async-streaming) | [Async Embedding](https://docs.litellm.ai/docs/embedding/supported_embedding) | [Async Image Generation](https://docs.litellm.ai/docs/image_generation) |
|-------------------------------------------------------------------------------------|---------------------------------------------------------|---------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------|
| [openai](https://docs.litellm.ai/docs/providers/openai)                             | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 | âœ…                                                                             | âœ…                                                                       |
| [Meta - Llama API](https://docs.litellm.ai/docs/providers/meta_llama)                               | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 |                                                                              |                                                                        |
| [azure](https://docs.litellm.ai/docs/providers/azure)                               | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 | âœ…                                                                             | âœ…                                                                       |
| [AI/ML API](https://docs.litellm.ai/docs/providers/aiml)                               | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 | âœ…                                                                             | âœ…                                                                       |
| [aws - sagemaker](https://docs.litellm.ai/docs/providers/aws_sagemaker)             | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 | âœ…                                                                             |                                                                         |
| [aws - bedrock](https://docs.litellm.ai/docs/providers/bedrock)                     | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 | âœ…                                                                             |                                                                         |
| [google - vertex_ai](https://docs.litellm.ai/docs/providers/vertex)                 | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 | âœ…                                                                             | âœ…                                                                       |
| [google - palm](https://docs.litellm.ai/docs/providers/palm)                        | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 |                                                                               |                                                                         |
| [google AI Studio - gemini](https://docs.litellm.ai/docs/providers/gemini)          | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 |                                                                               |                                                                         |
| [mistral ai api](https://docs.litellm.ai/docs/providers/mistral)                    | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 | âœ…                                                                             |                                                                         |
| [cloudflare AI Workers](https://docs.litellm.ai/docs/providers/cloudflare_workers)  | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 |                                                                               |                                                                         |
| [cohere](https://docs.litellm.ai/docs/providers/cohere)                             | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 | âœ…                                                                             |                                                                         |
| [anthropic](https://docs.litellm.ai/docs/providers/anthropic)                       | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 |                                                                               |                                                                         |
| [empower](https://docs.litellm.ai/docs/providers/empower)                    | âœ…                                                      | âœ…                                                                              | âœ…                                                                                  | âœ…                                                                                |
| [huggingface](https://docs.litellm.ai/docs/providers/huggingface)                   | âœ…                                                       | âœ…                                                                               | âœ…                                                                                   | âœ…                                                                                 | âœ…                                                                             |                                                                         |
| [replicate](https://docs.litel

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>