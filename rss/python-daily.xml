<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Wed, 30 Apr 2025 00:04:31 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[hacksider/Deep-Live-Cam]]></title>
            <link>https://github.com/hacksider/Deep-Live-Cam</link>
            <guid>https://github.com/hacksider/Deep-Live-Cam</guid>
            <pubDate>Wed, 30 Apr 2025 00:04:31 GMT</pubDate>
            <description><![CDATA[real time face swap and one-click video deepfake with only a single image]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hacksider/Deep-Live-Cam">hacksider/Deep-Live-Cam</a></h1>
            <p>real time face swap and one-click video deepfake with only a single image</p>
            <p>Language: Python</p>
            <p>Stars: 53,307</p>
            <p>Forks: 7,770</p>
            <p>Stars today: 1,498 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;Deep-Live-Cam&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  Real-time face swap and video deepfake with a single click and only a single image.
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/11395&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11395&quot; alt=&quot;hacksider%2FDeep-Live-Cam | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/demo.gif&quot; alt=&quot;Demo GIF&quot; width=&quot;800&quot;&gt;
&lt;/p&gt;

##  Disclaimer

This deepfake software is designed to be a productive tool for the AI-generated media industry. It can assist artists in animating custom characters, creating engaging content, and even using models for clothing design.

We are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to the law and ethics. We may shut down the project or add watermarks if legally required.

- Ethical Use: Users are expected to use this software responsibly and legally. If using a real person&#039;s face, obtain their consent and clearly label any output as a deepfake when sharing online.

- Content Restrictions: The software includes built-in checks to prevent processing inappropriate media, such as nudity, graphic content, or sensitive material.

- Legal Compliance: We adhere to all relevant laws and ethical guidelines. If legally required, we may shut down the project or add watermarks to the output.

- User Responsibility: We are not responsible for end-user actions. Users must ensure their use of the software aligns with ethical standards and legal requirements.

By using this software, you agree to these terms and commit to using it in a manner that respects the rights and dignity of others.

Users are expected to use this software responsibly and legally. If using a real person&#039;s face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user actions.

## Exclusive v2.0 Quick Start - Pre-built (Windows / Nvidia)

  &lt;a href=&quot;https://deeplivecam.net/index.php/quickstart&quot;&gt; &lt;img src=&quot;https://github.com/user-attachments/assets/7d993b32-e3e8-4cd3-bbfb-a549152ebdd5&quot; width=&quot;285&quot; height=&quot;77&quot; /&gt;

##### This is the fastest build you can get if you have a discrete NVIDIA GPU.
 
###### These Pre-builts are perfect for non-technical users or those who don&#039;t have time to, or can&#039;t manually install all the requirements. Just a heads-up: this is an open-source project, so you can also install it manually. This will be 60 days ahead on the open source version.

## TLDR; Live Deepfake in just 3 Clicks
![easysteps](https://github.com/user-attachments/assets/af825228-852c-411b-b787-ffd9aac72fc6)
1. Select a face
2. Select which camera to use
3. Press live!

## Features &amp; Uses - Everything is in real-time

### Mouth Mask

**Retain your original mouth for accurate movement using Mouth Mask**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/ludwig.gif&quot; alt=&quot;resizable-gif&quot;&gt;
&lt;/p&gt;

### Face Mapping

**Use different faces on multiple subjects simultaneously**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/streamers.gif&quot; alt=&quot;face_mapping_source&quot;&gt;
&lt;/p&gt;

### Your Movie, Your Face

**Watch movies with any face in real-time**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/movie.gif&quot; alt=&quot;movie&quot;&gt;
&lt;/p&gt;

### Live Show

**Run Live shows and performances**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/live_show.gif&quot; alt=&quot;show&quot;&gt;
&lt;/p&gt;

### Memes

**Create Your Most Viral Meme Yet**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/meme.gif&quot; alt=&quot;show&quot; width=&quot;450&quot;&gt; 
  &lt;br&gt;
  &lt;sub&gt;Created using Many Faces feature in Deep-Live-Cam&lt;/sub&gt;
&lt;/p&gt;

### Omegle

**Surprise people on Omegle**

&lt;p align=&quot;center&quot;&gt;
  &lt;video src=&quot;https://github.com/user-attachments/assets/2e9b9b82-fa04-4b70-9f56-b1f68e7672d0&quot; width=&quot;450&quot; controls&gt;&lt;/video&gt;
&lt;/p&gt;

## Installation (Manual)

**Please be aware that the installation requires technical skills and is not for beginners. Consider downloading the prebuilt version.**

&lt;details&gt;
&lt;summary&gt;Click to see the process&lt;/summary&gt;

### Installation

This is more likely to work on your computer but will be slower as it utilizes the CPU.

**1. Set up Your Platform**

-   Python (3.10 recommended)
-   pip
-   git
-   [ffmpeg](https://www.youtube.com/watch?v=OlNWCpFdVMA) - ```iex (irm ffmpeg.tc.ht)```
-   [Visual Studio 2022 Runtimes (Windows)](https://visualstudio.microsoft.com/visual-cpp-build-tools/)

**2. Clone the Repository**

```bash
git clone https://github.com/hacksider/Deep-Live-Cam.git
cd Deep-Live-Cam
```

**3. Download the Models**

1. [GFPGANv1.4](https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth)
2. [inswapper\_128\_fp16.onnx](https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx)

Place these files in the &quot;**models**&quot; folder.

**4. Install Dependencies**

We highly recommend using a `venv` to avoid issues.

For Windows:
```bash
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
```

**For macOS:**

Apple Silicon (M1/M2/M3) requires specific setup:

```bash
# Install Python 3.10 (specific version is important)
brew install python@3.10

# Install tkinter package (required for the GUI)
brew install python-tk@3.10

# Create and activate virtual environment with Python 3.10
python3.10 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

** In case something goes wrong and you need to reinstall the virtual environment **

```bash
# Deactivate the virtual environment
rm -rf venv

# Reinstall the virtual environment
python -m venv venv
source venv/bin/activate

# install the dependencies again
pip install -r requirements.txt
```

**Run:** If you don&#039;t have a GPU, you can run Deep-Live-Cam using `python run.py`. Note that initial execution will download models (~300MB).

### GPU Acceleration

**CUDA Execution Provider (Nvidia)**

1. Install [CUDA Toolkit 11.8.0](https://developer.nvidia.com/cuda-11-8-0-download-archive)
2. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu==1.16.3
```

3. Usage:

```bash
python run.py --execution-provider cuda
```

**CoreML Execution Provider (Apple Silicon)**

Apple Silicon (M1/M2/M3) specific installation:

1. Make sure you&#039;ve completed the macOS setup above using Python 3.10.
2. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-silicon
pip install onnxruntime-silicon==1.13.1
```

3. Usage (important: specify Python 3.10):

```bash
python3.10 run.py --execution-provider coreml
```

**Important Notes for macOS:**
- You **must** use Python 3.10, not newer versions like 3.11 or 3.13
- Always run with `python3.10` command not just `python` if you have multiple Python versions installed
- If you get error about `_tkinter` missing, reinstall the tkinter package: `brew reinstall python-tk@3.10`
- If you get model loading errors, check that your models are in the correct folder
- If you encounter conflicts with other Python versions, consider uninstalling them:
  ```bash
  # List all installed Python versions
  brew list | grep python
  
  # Uninstall conflicting versions if needed
  brew uninstall --ignore-dependencies python@3.11 python@3.13
  
  # Keep only Python 3.10
  brew cleanup
  ```

**CoreML Execution Provider (Apple Legacy)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-coreml
pip install onnxruntime-coreml==1.13.1
```

2. Usage:

```bash
python run.py --execution-provider coreml
```

**DirectML Execution Provider (Windows)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-directml
pip install onnxruntime-directml==1.15.1
```

2. Usage:

```bash
python run.py --execution-provider directml
```

**OpenVINO‚Ñ¢ Execution Provider (Intel)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-openvino
pip install onnxruntime-openvino==1.15.0
```

2. Usage:

```bash
python run.py --execution-provider openvino
```
&lt;/details&gt;

## Usage

**1. Image/Video Mode**

-   Execute `python run.py`.
-   Choose a source face image and a target image/video.
-   Click &quot;Start&quot;.
-   The output will be saved in a directory named after the target video.

**2. Webcam Mode**

-   Execute `python run.py`.
-   Select a source face image.
-   Click &quot;Live&quot;.
-   Wait for the preview to appear (10-30 seconds).
-   Use a screen capture tool like OBS to stream.
-   To change the face, select a new source image.

## Tips and Tricks

Check out these helpful guides to get the most out of Deep-Live-Cam:

- [Unlocking the Secrets to the Perfect Deepfake Image](https://deeplivecam.net/index.php/blog/tips-and-tricks/unlocking-the-secrets-to-the-perfect-deepfake-image) - Learn how to create the best deepfake with full head coverage
- [Video Call with DeepLiveCam](https://deeplivecam.net/index.php/blog/tips-and-tricks/video-call-with-deeplivecam) - Make your meetings livelier by using DeepLiveCam with OBS and meeting software
- [Have a Special Guest!](https://deeplivecam.net/index.php/blog/tips-and-tricks/have-a-special-guest) - Tutorial on how to use face mapping to add special guests to your stream
- [Watch Deepfake Movies in Realtime](https://deeplivecam.net/index.php/blog/tips-and-tricks/watch-deepfake-movies-in-realtime) - See yourself star in any video without processing the video
- [Better Quality without Sacrificing Speed](https://deeplivecam.net/index.php/blog/tips-and-tricks/better-quality-without-sacrificing-speed) - Tips for achieving better results without impacting performance
- [Instant Vtuber!](https://deeplivecam.net/index.php/blog/tips-and-tricks/instant-vtuber) - Create a new persona/vtuber easily using Metahuman Creator

Visit our [official blog](https://deeplivecam.net/index.php/blog/tips-and-tricks) for more tips and tutorials.

## Command Line Arguments (Unmaintained)

```
options:
  -h, --help                                               show this help message and exit
  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image
  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video
  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory
  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)
  --keep-fps                                               keep original fps
  --keep-audio                                             keep original audio
  --keep-frames                                            keep temporary frames
  --many-faces                                             process every face
  --map-faces                                              map source target faces
  --mouth-mask                                             mask the mouth region
  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder
  --video-quality [0-51]                                   adjust output video quality
  --live-mirror                                            the live camera display as you see it in the front-facing camera frame
  --live-resizable                                         the live camera frame is resizable
  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB
  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)
  --execution-threads EXECUTION_THREADS                    number of execution threads
  -v, --version                                            show program&#039;s version number and exit
```

Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.

## Press

**We are always open to criticism and are ready to improve, that&#039;s why we didn&#039;t cherry-pick anything.**

 - [*&quot;Deep-Live-Cam goes viral, allowing anyone to become a digital doppelganger&quot;*](https://arstechnica.com/information-technology/2024/08/new-ai-tool-enables-real-time-face-swapping-on-webcams-raising-fraud-concerns/) - Ars Technica
 - [*&quot;Thanks Deep Live Cam, shapeshifters are among us now&quot;*](https://dataconomy.com/2024/08/15/what-is-deep-live-cam-github-deepfake/) - Dataconomy
 - [*&quot;This free AI tool lets you become anyone during video-calls&quot;*](https://www.newsbytesapp.com/news/science/deep-live-cam-ai-impersonation-tool-goes-viral/story) - NewsBytes
 - [*&quot;OK, this viral AI live stream software is truly terrifying&quot;*](https://www.creativebloq.com/ai/ok-this-viral-ai-live-stream-software-is-truly-terrifying) - Creative Bloq
 - [*&quot;Deepfake AI Tool Lets You Become Anyone in a Video Call With Single Photo&quot;*](https://petapixel.com/2024/08/14/deep-live-cam-deepfake-ai-tool-lets-you-become-anyone-in-a-video-call-with-single-photo-mark-zuckerberg-jd-vance-elon-musk/) - PetaPixel
 - [*&quot;Deep-Live-Cam Uses AI to Transform Your Face in Real-Time, Celebrities Included&quot;*](https://www.techeblog.com/deep-live-cam-ai-transform-face/) - TechEBlog
 - [*&quot;An AI tool that &quot;makes you look like anyone&quot; during a video call is going viral online&quot;*](https://telegrafi.com/en/a-tool-that-makes-you-look-like-anyone-during-a-video-call-is-going-viral-on-the-Internet/) - Telegrafi
 - [*&quot;This Deepfake Tool Turning Images Into Livestreams is Topping the GitHub Charts&quot;*](https://decrypt.co/244565/this-deepfake-tool-turning-images-into-livestreams-is-topping-the-github-charts) - Emerge
 - [*&quot;New Real-Time Face-Swapping AI Allows Anyone to Mimic Famous Faces&quot;*](https://www.digitalmusicnews.com/2024/08/15/face-swapping-ai-real-time-mimic/) - Digital Music News
 - [*&quot;This real-time webcam deepfake tool raises alarms about the future of identity theft&quot;*](https://www.diyphotography.net/this-real-time-webcam-deepfake-tool-raises-alarms-about-the-future-of-identity-theft/) - DIYPhotography
 - [*&quot;That&#039;s Crazy, Oh God. That&#039;s Fucking Freaky Dude... That&#039;s So Wild Dude&quot;*](https://www.youtube.com/watch?time_continue=1074&amp;v=py4Tc-Y8BcY) - SomeOrdinaryGamers
 - [*&quot;Alright look look look, now look chat, we can do any face we want to look like chat&quot;*](https://www.youtube.com/live/mFsCe7AIxq8?feature=shared&amp;t=2686) - IShowSpeed

## Credits

-   [ffmpeg](https://ffmpeg.org/): for making video-related operations easy
-   [deepinsight](https://github.com/deepinsight): for their [insightface](https://github.com/deepinsight/insightface) project which provided a well-made library and models. Please be reminded that the [use of the model is for non-commercial research purposes only](https://github.com/deepinsight/insightface?tab=readme-ov-file#license).
-   [havok2-htwo](https://github.com/havok2-htwo): for sharing the code for webcam
-   [GosuDRM](https://github.com/GosuDRM): for the open version of roop
-   [pereiraroland26](https://github.com/pereiraroland26): Multiple faces support
-   [vic4key](https://github.com/vic4key): For supporting/contributing to this project
-   [kier007](https://github.com/kier007): for improving the user experience
-   [qitianai](https://github.com/qitianai): for multi-lingual support
-   and [all developers](https://github.com/hacksider/Deep-Live-Cam/graphs/contributors) behind libraries used in this project.
-   Footnote: Please be informed that the base author of the code is [s0md3v](https://github.com/s0md3v/roop)
-   All the wonderful users who helped make this project go viral by starring the repo ‚ù§Ô∏è

[![Stargazers](https://reporoster.com/stars/hacksider/Deep-Live-Cam)](https://github.com/hacksider/Deep-Live-Cam/stargazers)

## Contributions

![Alt](https://repobeats.axiom.co/api/embed/fec8e29c45dfdb9c5916f3a7830e1249308d20e1.svg &quot;Repobeats analytics image&quot;)

## Stars to the Moon üöÄ

&lt;a href=&quot;https://star-history.com/#hacksider/deep-live-cam&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[simular-ai/Agent-S]]></title>
            <link>https://github.com/simular-ai/Agent-S</link>
            <guid>https://github.com/simular-ai/Agent-S</guid>
            <pubDate>Wed, 30 Apr 2025 00:04:30 GMT</pubDate>
            <description><![CDATA[Agent S: an open agentic framework that uses computers like a human]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/simular-ai/Agent-S">simular-ai/Agent-S</a></h1>
            <p>Agent S: an open agentic framework that uses computers like a human</p>
            <p>Language: Python</p>
            <p>Stars: 3,260</p>
            <p>Forks: 338</p>
            <p>Stars today: 97 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/agent_s.png&quot; alt=&quot;Logo&quot; style=&quot;vertical-align:middle&quot; width=&quot;60&quot;&gt; Agent S2:
  &lt;small&gt;A Compositional Generalist-Specialist Framework for Computer Use Agents&lt;/small&gt;
&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;&amp;nbsp;
  üåê &lt;a href=&quot;https://www.simular.ai/articles/agent-s2-technical-review&quot;&gt;[S2 blog]&lt;/a&gt;&amp;nbsp;
  üìÑ &lt;a href=&quot;https://arxiv.org/abs/2504.00906&quot;&gt;[S2 Paper]&lt;/a&gt;&amp;nbsp;
  üé• &lt;a href=&quot;https://www.youtube.com/watch?v=wUGVQl7c0eg&quot;&gt;[S2 Video]&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&amp;nbsp;
  üåê &lt;a href=&quot;https://www.simular.ai/agent-s&quot;&gt;[S1 blog]&lt;/a&gt;&amp;nbsp;
  üìÑ &lt;a href=&quot;https://arxiv.org/abs/2410.08164&quot;&gt;[S1 Paper (ICLR 2025)]&lt;/a&gt;&amp;nbsp;
  üé• &lt;a href=&quot;https://www.youtube.com/watch?v=OBDE3Knte0g&quot;&gt;[S1 Video]&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&amp;nbsp;
&lt;a href=&quot;https://trendshift.io/repositories/13151&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13151&quot; alt=&quot;simular-ai%2FAgent-S | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://discord.gg/E2XfsK9fPV&quot;&gt;
    &lt;img src=&quot;https://dcbadge.limes.pink/api/server/https://discord.gg/E2XfsK9fPV?style=flat&quot; alt=&quot;Discord&quot;&gt;
  &lt;/a&gt;
  &amp;nbsp;&amp;nbsp;
  &lt;a href=&quot;https://pepy.tech/projects/gui-agents&quot;&gt;
    &lt;img src=&quot;https://static.pepy.tech/badge/gui-agents&quot; alt=&quot;PyPI Downloads&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

## ü•≥ Updates
- [x] **2025/04/01**: Released &lt;a href=&quot;https://arxiv.org/abs/2504.00906&quot;&gt;Agent S2 paper&lt;/a&gt; with new SOTA results on OSWorld, WindowsAgentArena, and AndroidWorld!
- [x] **2025/03/12**: Released Agent S2 along with v0.2.0 of [gui-agents](https://github.com/simular-ai/Agent-S), the new state-of-the-art for computer use agents (CUA), outperforming OpenAI&#039;s CUA/Operator and Anthropic&#039;s Claude 3.7 Sonnet Computer-Use!
- [x] **2025/01/22**: The [Agent S paper](https://arxiv.org/abs/2410.08164) is accepted to ICLR 2025!
- [x] **2025/01/21**: Released v0.1.2 of [gui-agents](https://github.com/simular-ai/Agent-S) library, with support for Linux and Windows!
- [x] **2024/12/05**: Released v0.1.0 of [gui-agents](https://github.com/simular-ai/Agent-S) library, allowing you to use Agent-S for Mac, OSWorld, and WindowsAgentArena with ease!
- [x] **2024/10/10**: Released the [Agent S paper](https://arxiv.org/abs/2410.08164) and codebase!

## Table of Contents

1. [üí° Introduction](#-introduction)
2. [üéØ Current Results](#-current-results)
3. [üõ†Ô∏è Installation &amp; Setup](#%EF%B8%8F-installation--setup) 
4. [üöÄ Usage](#-usage)
5. [ü§ù Acknowledgements](#-acknowledgements)
6. [üí¨ Citation](#-citation)

## üí° Introduction

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;./images/agent_s2_teaser.png&quot; width=&quot;800&quot;&gt;
&lt;/p&gt;

Welcome to **Agent S**, an open-source framework designed to enable autonomous interaction with computers through Agent-Computer Interface. Our mission is to build intelligent GUI agents that can learn from past experiences and perform complex tasks autonomously on your computer. 

Whether you&#039;re interested in AI, automation, or contributing to cutting-edge agent-based systems, we&#039;re excited to have you here!

## üéØ Current Results

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;./images/agent_s2_osworld_result.png&quot; width=&quot;600&quot;&gt;
    &lt;br&gt;
    Results of Agent S2&#039;s Successful Rate (%) on the OSWorld full test set using Screenshot input only.
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;table border=&quot;0&quot; cellspacing=&quot;0&quot; cellpadding=&quot;5&quot;&gt;
    &lt;tr&gt;
      &lt;th&gt;Benchmark&lt;/th&gt;
      &lt;th&gt;Agent S2&lt;/th&gt;
      &lt;th&gt;Previous SOTA&lt;/th&gt;
      &lt;th&gt;Œî improve&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;OSWorld (15 step)&lt;/td&gt;
      &lt;td&gt;27.0%&lt;/td&gt;
      &lt;td&gt;22.7% (UI-TARS)&lt;/td&gt;
      &lt;td&gt;+4.3%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;OSWorld (50 step)&lt;/td&gt;
      &lt;td&gt;34.5%&lt;/td&gt;
      &lt;td&gt;32.6% (OpenAI CUA)&lt;/td&gt;
      &lt;td&gt;+1.9%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;WindowsAgentArena&lt;/td&gt;
      &lt;td&gt;29.8%&lt;/td&gt;
      &lt;td&gt;19.5% (NAVI)&lt;/td&gt;
      &lt;td&gt;+10.3%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;AndroidWorld&lt;/td&gt;
      &lt;td&gt;54.3%&lt;/td&gt;
      &lt;td&gt;46.8% (UI-TARS)&lt;/td&gt;
      &lt;td&gt;+7.5%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;


## üõ†Ô∏è Installation &amp; Setup

&gt; ‚ùó**Warning**‚ùó: If you are on a Linux machine, creating a `conda` environment will interfere with `pyatspi`. As of now, there&#039;s no clean solution for this issue. Proceed through the installation without using `conda` or any virtual environment.

&gt; ‚ö†Ô∏è**Disclaimer**‚ö†Ô∏è: To leverage the full potential of Agent S2, we utilize [UI-TARS](https://github.com/bytedance/UI-TARS) as a grounding model (7B-DPO or 72B-DPO for better performance). They can be hosted locally, or on Hugging Face Inference Endpoints. Our code supports Hugging Face Inference Endpoints. Check out [Hugging Face Inference Endpoints](https://huggingface.co/learn/cookbook/en/enterprise_dedicated_endpoints) for more information on how to set up and query this endpoint. However, running Agent S2 does not require this model, and you can use alternative API based models for visual grounding, such as Claude.

Install the package:
```
pip install gui-agents
```

Set your LLM API Keys and other environment variables. You can do this by adding the following line to your .bashrc (Linux), or .zshrc (MacOS) file. 

```
export OPENAI_API_KEY=&lt;YOUR_API_KEY&gt;
export ANTHROPIC_API_KEY=&lt;YOUR_ANTHROPIC_API_KEY&gt;
export HF_TOKEN=&lt;YOUR_HF_TOKEN&gt;
```

Alternatively, you can set the environment variable in your Python script:

```
import os
os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;&lt;YOUR_API_KEY&gt;&quot;
```

We also support Azure OpenAI, Anthropic, Gemini, Open Router, and vLLM inference. For more information refer to [models.md](models.md).

### Setup Retrieval from Web using Perplexica
Agent S works best with web-knowledge retrieval. To enable this feature, you need to setup Perplexica: 

1. Ensure Docker Desktop is installed and running on your system.

2. Navigate to the directory containing the project files.

   ```bash
    cd Perplexica
    git submodule update --init
   ```

3. Rename the `sample.config.toml` file to `config.toml`. For Docker setups, you need only fill in the following fields:

   - `OPENAI`: Your OpenAI API key. **You only need to fill this if you wish to use OpenAI&#039;s models**.
   - `OLLAMA`: Your Ollama API URL. You should enter it as `http://host.docker.internal:PORT_NUMBER`. If you installed Ollama on port 11434, use `http://host.docker.internal:11434`. For other ports, adjust accordingly. **You need to fill this if you wish to use Ollama&#039;s models instead of OpenAI&#039;s**.
   - `GROQ`: Your Groq API key. **You only need to fill this if you wish to use Groq&#039;s hosted models**.
   - `ANTHROPIC`: Your Anthropic API key. **You only need to fill this if you wish to use Anthropic models**.

     **Note**: You can change these after starting Perplexica from the settings dialog.

   - `SIMILARITY_MEASURE`: The similarity measure to use (This is filled by default; you can leave it as is if you are unsure about it.)

4. Ensure you are in the directory containing the `docker-compose.yaml` file and execute:

   ```bash
   docker compose up -d
   ```
5. Next, export your Perplexica URL. This URL is used to interact with the Perplexica API backend. The port is given by the `config.toml` in your Perplexica directory.

   ```bash
   export PERPLEXICA_URL=http://localhost:{port}/api/search
   ```
6. Our implementation of Agent S incorporates the Perplexica API to integrate a search engine capability, which allows for a more convenient and responsive user experience. If you want to tailor the API to your settings and specific requirements, you may modify the URL and the message of request parameters in  `agent_s/query_perplexica.py`. For a comprehensive guide on configuring the Perplexica API, please refer to [Perplexica Search API Documentation](https://github.com/ItzCrazyKns/Perplexica/blob/master/docs/API/SEARCH.md).
For a more detailed setup and usage guide, please refer to the [Perplexica Repository](https://github.com/ItzCrazyKns/Perplexica.git).

&gt; ‚ùó**Warning**‚ùó: The agent will directly run python code to control your computer. Please use with care.

## üöÄ Usage


&gt; **Note**: Our best configuration uses Claude 3.7 with extended thinking and UI-TARS-72B-DPO. If you are unable to run UI-TARS-72B-DPO due to resource constraints, UI-TARS-7B-DPO can be used as a lighter alternative with minimal performance degradation.

### CLI

Run Agent S2 with a specific model (default is `gpt-4o`):

```sh
agent_s2 \
  --provider &quot;anthropic&quot; \
  --model &quot;claude-3-7-sonnet-20250219&quot; \
  --grounding_model_provider &quot;anthropic&quot; \
  --grounding_model &quot;claude-3-7-sonnet-20250219&quot; \
```

Or use a custom endpoint:

```bash
agent_s2 \
  --provider &quot;anthropic&quot; \
  --model &quot;claude-3-7-sonnet-20250219&quot; \
  --endpoint_provider &quot;huggingface&quot; \
  --endpoint_url &quot;&lt;endpoint_url&gt;/v1/&quot;
```

#### Main Model Settings
- **`--provider`**, **`--model`** 
  - Purpose: Specifies the main generation model
  - Supports: all model providers in [models.md](models.md)
  - Default: `--provider &quot;anthropic&quot; --model &quot;claude-3-7-sonnet-20250219&quot;`

#### Grounding Configuration Options

You can use either Configuration 1 or Configuration 2:

##### **(Default) Configuration 1: API-Based Models**
- **`--grounding_model_provider`**, **`--grounding_model`**
  - Purpose: Specifies the model for visual grounding (coordinate prediction)
  - Supports: all model providers in [models.md](models.md)
  - Default: `--grounding_model_provider &quot;anthropic&quot; --grounding_model &quot;claude-3-7-sonnet-20250219&quot;`
- ‚ùó**Important**‚ùó **`--grounding_model_resize_width`**
  - Purpose:  Some API providers automatically rescale images. Therefore, the generated (x, y) will be relative to the rescaled image dimensions, instead of the original image dimensions.
  - Supports: [Anthropic rescaling](https://docs.anthropic.com/en/docs/build-with-claude/vision#)
  - Tips: If your grounding is inaccurate even for very simple queries, double check your rescaling width is correct for your machine&#039;s resolution.
  - Default: `--grounding_model_resize_width 1366` (Anthropic)

##### **Configuration 2: Custom Endpoint**
- **`--endpoint_provider`**
  - Purpose: Specifies the endpoint provider
  - Supports: HuggingFace TGI, vLLM, Open Router
  - Default: None

- **`--endpoint_url`**
  - Purpose: The URL for your custom endpoint
  - Default: None

&gt; **Note**: Configuration 2 takes precedence over Configuration 1.

This will show a user query prompt where you can enter your query and interact with Agent S2. You can use any model from the list of supported models in [models.md](models.md).

### `gui_agents` SDK

First, we import the necessary modules. `AgentS2` is the main agent class for Agent S2. `OSWorldACI` is our grounding agent that translates agent actions into executable python code.
```
import pyautogui
import io
from gui_agents.s2.agents.agent_s import AgentS2
from gui_agents.s2.agents.grounding import OSWorldACI

# Load in your API keys.
from dotenv import load_dotenv
load_dotenv()

current_platform = &quot;linux&quot;  # &quot;darwin&quot;, &quot;windows&quot;
```

Next, we define our engine parameters. `engine_params` is used for the main agent, and `engine_params_for_grounding` is for grounding. For `engine_params_for_grounding`, we support the Claude, GPT series, and Hugging Face Inference Endpoints.

```
engine_type_for_grounding = &quot;huggingface&quot;

engine_params = {
    &quot;engine_type&quot;: &quot;openai&quot;,
    &quot;model&quot;: &quot;gpt-4o&quot;,
}

if engine_type_for_grounding == &quot;huggingface&quot;:
  engine_params_for_grounding = {
      &quot;engine_type&quot;: &quot;huggingface&quot;,
      &quot;endpoint_url&quot;: &quot;&lt;endpoint_url&gt;/v1/&quot;,
  }
elif engine_type_for_grounding == &quot;claude&quot;:
  engine_params_for_grounding = {
      &quot;engine_type&quot;: &quot;claude&quot;,
      &quot;model&quot;: &quot;claude-3-7-sonnet-20250219&quot;,
  }
elif engine_type_for_grounding == &quot;gpt&quot;:
  engine_params_for_grounding = {
    &quot;engine_type&quot;: &quot;gpt&quot;,
    &quot;model&quot;: &quot;gpt-4o&quot;,
  }
else:
  raise ValueError(&quot;Invalid engine type for grounding&quot;)
```

Then, we define our grounding agent and Agent S2.

```
grounding_agent = OSWorldACI(
    platform=current_platform,
    engine_params_for_generation=engine_params,
    engine_params_for_grounding=engine_params_for_grounding
)

agent = AgentS2(
  engine_params,
  grounding_agent,
  platform=current_platform,
  action_space=&quot;pyautogui&quot;,
  observation_type=&quot;mixed&quot;,
  search_engine=&quot;Perplexica&quot;  # Assuming you have set up Perplexica.
)
```

Finally, let&#039;s query the agent!

```
# Get screenshot.
screenshot = pyautogui.screenshot()
buffered = io.BytesIO() 
screenshot.save(buffered, format=&quot;PNG&quot;)
screenshot_bytes = buffered.getvalue()

obs = {
  &quot;screenshot&quot;: screenshot_bytes,
}

instruction = &quot;Close VS Code&quot;
info, action = agent.predict(instruction=instruction, observation=obs)

exec(action[0])
```

Refer to `gui_agents/s2/cli_app.py` for more details on how the inference loop works.

#### Downloading the Knowledge Base

Agent S2 uses a knowledge base that continually updates with new knowledge during inference. The knowledge base is initially downloaded when initializing `AgentS2`. The knowledge base is stored as assets under our [GitHub Releases](https://github.com/simular-ai/Agent-S/releases). The `AgentS2` initialization will only download the knowledge base for your specified platform and agent version (e.g s1, s2). If you&#039;d like to download the knowledge base programmatically, you can use the following code:

```
download_kb_data(
    version=&quot;s2&quot;,
    release_tag=&quot;v0.2.2&quot;,
    download_dir=&quot;kb_data&quot;,
    platform=&quot;linux&quot;  # &quot;darwin&quot;, &quot;windows&quot;
)
```

This will download Agent S2&#039;s knowledge base for Linux from release tag `v0.2.2` to the `kb_data` directory. Refer to our [GitHub Releases](https://github.com/simular-ai/Agent-S/releases) or release tags that include the knowledge bases.

### OSWorld

To deploy Agent S2 in OSWorld, follow the [OSWorld Deployment instructions](OSWorld.md).

## ü§ù Acknowledgements

We extend our sincere thanks to Tianbao Xie for developing OSWorld and discussing computer use challenges. We also appreciate the engaging discussions with Yujia Qin and Shihao Liang regarding UI-TARS.

## üí¨ Citations

If you find this codebase useful, please cite 

```
@misc{Agent-S2,
      title={Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents}, 
      author={Saaket Agashe and Kyle Wong and Vincent Tu and Jiachen Yang and Ang Li and Xin Eric Wang},
      year={2025},
      eprint={2504.00906},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2504.00906}, 
}
```

```
@inproceedings{Agent-S,
    title={{Agent S: An Open Agentic Framework that Uses Computers Like a Human}},
    author={Saaket Agashe and Jiuzhou Han and Shuyu Gan and Jiachen Yang and Ang Li and Xin Eric Wang},
    booktitle={International Conference on Learning Representations (ICLR)},
    year={2025},
    url={https://arxiv.org/abs/2410.08164}
}
```

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[carykh/jes]]></title>
            <link>https://github.com/carykh/jes</link>
            <guid>https://github.com/carykh/jes</guid>
            <pubDate>Wed, 30 Apr 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[Jelly Evolution Simulator]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/carykh/jes">carykh/jes</a></h1>
            <p>Jelly Evolution Simulator</p>
            <p>Language: Python</p>
            <p>Stars: 79</p>
            <p>Forks: 40</p>
            <p>Stars today: 12 stars today</p>
            <h2>README</h2><pre># Jelly Evolution Simulator

To run this program, run this command:

```
cmd python jes.py
```

NOTE: This project, like all my projects, are not meant to be consumer products with perfect QA. Rather, it&#039;s just me, as one person, coding a casual experiment to the point that it works well enough on my computer to make a video from it! No more, no less. (I used to not put my code online, just like when you create a Minecraft world with your friends, you don&#039;t have to share the world with everyone. I just started posting code here because I wanted to make it easier for eager devs to make mods.) Long story short, I won&#039;t be doing bug-fixing or tech support on this project.

# Key-controls

ESC: Close the program

X: Toggle whether or not X&#039;s show up over killed jellies

S: Store the species you&#039;re highlighting in memory. (Press S a 2nd time to unstore.) Why do this? Well, say you notice there&#039;s a creature who got #1 in a certain generation, but you can&#039;t find any trace of it elsewhere. Now, you can highlight the creature, press S, and their species bubble will show up in the upper-left. Then, roll your mouse 
over the species bubble, and there&#039;s all the species info!

C: Change the color of the species you&#039;re highlighting. Do this when 2 species are annoyingly close in color, and you want a better way to tell them apart.

Q: Open/close the creature mosaic (can also be done by clicking &quot;Show creatures&quot; button)

LEFT/RIGHT: Scroll through forward/backward through the timeline (can also be done by scrolling the scroll bar)

# Updates (2025-01-11)

-Mutation-finding-bug fixed (I think)

There used to be a bug where, late in the simulation, big mutations would take forever to find. That has been resolved. (It was the 0.5+ rigidity-forcing)

-Added all those key controls

-Allowed the user to change the number of creatures in the simulation (first user input)

-Fixed but where, when you click &quot;Watch sample&quot;, it ONLY shows you a sampling of 8 creatures from the recent-est generation. Now, it always shows you a sampling of the generation your scroll bar is currently at.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Alvin9999/new-pac]]></title>
            <link>https://github.com/Alvin9999/new-pac</link>
            <guid>https://github.com/Alvin9999/new-pac</guid>
            <pubDate>Wed, 30 Apr 2025 00:04:28 GMT</pubDate>
            <description><![CDATA[ÁøªÂ¢ô-ÁßëÂ≠¶‰∏äÁΩë„ÄÅËá™Áî±‰∏äÁΩë„ÄÅÂÖçË¥πÁßëÂ≠¶‰∏äÁΩë„ÄÅÂÖçË¥πÁøªÂ¢ô„ÄÅfanqiang„ÄÅÊ≤πÁÆ°youtube/ËßÜÈ¢ë‰∏ãËΩΩ„ÄÅËΩØ‰ª∂„ÄÅVPN„ÄÅ‰∏ÄÈîÆÁøªÂ¢ôÊµèËßàÂô®Ôºåvps‰∏ÄÈîÆÊê≠Âª∫ÁøªÂ¢ôÊúçÂä°Âô®ËÑöÊú¨/ÊïôÁ®ãÔºåÂÖçË¥πshadowsocks/ss/ssr/v2ray/goflywayË¥¶Âè∑/ËäÇÁÇπÔºåÁøªÂ¢ôÊ¢ØÂ≠êÔºåÁîµËÑë„ÄÅÊâãÊú∫„ÄÅiOS„ÄÅÂÆâÂçì„ÄÅwindows„ÄÅMac„ÄÅLinux„ÄÅË∑ØÁî±Âô®ÁøªÂ¢ô„ÄÅÁßëÂ≠¶‰∏äÁΩë„ÄÅyoutubeËßÜÈ¢ë‰∏ãËΩΩ„ÄÅyoutubeÊ≤πÁÆ°ÈïúÂÉè/ÂÖçÁøªÂ¢ôÁΩëÁ´ô„ÄÅÁæéÂå∫apple idÂÖ±‰∫´Ë¥¶Âè∑„ÄÅÁøªÂ¢ô-ÁßëÂ≠¶‰∏äÁΩë-Ê¢ØÂ≠ê]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Alvin9999/new-pac">Alvin9999/new-pac</a></h1>
            <p>ÁøªÂ¢ô-ÁßëÂ≠¶‰∏äÁΩë„ÄÅËá™Áî±‰∏äÁΩë„ÄÅÂÖçË¥πÁßëÂ≠¶‰∏äÁΩë„ÄÅÂÖçË¥πÁøªÂ¢ô„ÄÅfanqiang„ÄÅÊ≤πÁÆ°youtube/ËßÜÈ¢ë‰∏ãËΩΩ„ÄÅËΩØ‰ª∂„ÄÅVPN„ÄÅ‰∏ÄÈîÆÁøªÂ¢ôÊµèËßàÂô®Ôºåvps‰∏ÄÈîÆÊê≠Âª∫ÁøªÂ¢ôÊúçÂä°Âô®ËÑöÊú¨/ÊïôÁ®ãÔºåÂÖçË¥πshadowsocks/ss/ssr/v2ray/goflywayË¥¶Âè∑/ËäÇÁÇπÔºåÁøªÂ¢ôÊ¢ØÂ≠êÔºåÁîµËÑë„ÄÅÊâãÊú∫„ÄÅiOS„ÄÅÂÆâÂçì„ÄÅwindows„ÄÅMac„ÄÅLinux„ÄÅË∑ØÁî±Âô®ÁøªÂ¢ô„ÄÅÁßëÂ≠¶‰∏äÁΩë„ÄÅyoutubeËßÜÈ¢ë‰∏ãËΩΩ„ÄÅyoutubeÊ≤πÁÆ°ÈïúÂÉè/ÂÖçÁøªÂ¢ôÁΩëÁ´ô„ÄÅÁæéÂå∫apple idÂÖ±‰∫´Ë¥¶Âè∑„ÄÅÁøªÂ¢ô-ÁßëÂ≠¶‰∏äÁΩë-Ê¢ØÂ≠ê</p>
            <p>Language: Python</p>
            <p>Stars: 60,860</p>
            <p>Forks: 9,906</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre>ÁßëÂ≠¶‰∏äÁΩë-ÁøªÂ¢ô„ÄÅÂÖçË¥πÁøªÂ¢ô„ÄÅÂÖçË¥πÁßëÂ≠¶‰∏äÁΩë„ÄÅËΩØ‰ª∂„ÄÅVPN„ÄÅ‰∏ÄÈîÆÁøªÂ¢ôÊµèËßàÂô®Ôºåvps‰∏ÄÈîÆÊê≠Âª∫ÁøªÂ¢ôÊúçÂä°Âô®ËÑöÊú¨/ÊïôÁ®ãÔºåÂÖçË¥πshadowsocks/ss/ssr/v2ray/goflywayË¥¶Âè∑/ËäÇÁÇπÔºåÂÖçË¥πËá™Áî±‰∏äÁΩë„ÄÅfanqiang„ÄÅÁøªÂ¢ôÊ¢ØÂ≠êÔºåÁîµËÑë„ÄÅÊâãÊú∫„ÄÅiOS„ÄÅÂÆâÂçì„ÄÅwindows„ÄÅMac„ÄÅLinux„ÄÅË∑ØÁî±Âô®ÁøªÂ¢ô„ÄÅyoutubeËßÜÈ¢ë‰∏ãËΩΩ„ÄÅyoutubeÊ≤πÁÆ°ÈïúÂÉè/ÂÖçÁøªÂ¢ôÁΩëÁ´ô„ÄÅÁæéÂå∫apple idÂÖ±‰∫´Ë¥¶Âè∑

**https://github.com/Alvin9999/new-pac/wiki**

Âåó‰∫¨Êó∂Èó¥2025Âπ¥04Êúà30Êó•07ÁÇπ51ÂàÜÊõ¥Êñ∞„ÄÇ
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sherlock-project/sherlock]]></title>
            <link>https://github.com/sherlock-project/sherlock</link>
            <guid>https://github.com/sherlock-project/sherlock</guid>
            <pubDate>Wed, 30 Apr 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[Hunt down social media accounts by username across social networks]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sherlock-project/sherlock">sherlock-project/sherlock</a></h1>
            <p>Hunt down social media accounts by username across social networks</p>
            <p>Language: Python</p>
            <p>Stars: 64,037</p>
            <p>Forks: 7,424</p>
            <p>Stars today: 61 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[alexta69/metube]]></title>
            <link>https://github.com/alexta69/metube</link>
            <guid>https://github.com/alexta69/metube</guid>
            <pubDate>Wed, 30 Apr 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[Self-hosted YouTube downloader (web UI for youtube-dl / yt-dlp)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/alexta69/metube">alexta69/metube</a></h1>
            <p>Self-hosted YouTube downloader (web UI for youtube-dl / yt-dlp)</p>
            <p>Language: Python</p>
            <p>Stars: 8,706</p>
            <p>Forks: 575</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre># MeTube

&gt; **_NOTE:_**  32-bit ARM builds have been retired (a full year after [other major players](https://www.linuxserver.io/blog/a-farewell-to-arm-hf)), as new Node versions don&#039;t support them, and continued security updates and dependencies require new Node versions. Please migrate to a 64-bit OS to continue receiving MeTube upgrades.

![Build Status](https://github.com/alexta69/metube/actions/workflows/main.yml/badge.svg)
![Docker Pulls](https://img.shields.io/docker/pulls/alexta69/metube.svg)

Web GUI for youtube-dl (using the [yt-dlp](https://github.com/yt-dlp/yt-dlp) fork) with playlist support. Allows you to download videos from YouTube and [dozens of other sites](https://github.com/yt-dlp/yt-dlp/blob/master/supportedsites.md).

![screenshot1](https://github.com/alexta69/metube/raw/master/screenshot.gif)

## Run using Docker

```bash
docker run -d -p 8081:8081 -v /path/to/downloads:/downloads ghcr.io/alexta69/metube
```

## Run using docker-compose

```yaml
services:
  metube:
    image: ghcr.io/alexta69/metube
    container_name: metube
    restart: unless-stopped
    ports:
      - &quot;8081:8081&quot;
    volumes:
      - /path/to/downloads:/downloads
```

## Configuration via environment variables

Certain values can be set via environment variables, using the `-e` parameter on the docker command line, or the `environment:` section in docker-compose.

* __UID__: user under which MeTube will run. Defaults to `1000`.
* __GID__: group under which MeTube will run. Defaults to `1000`.
* __UMASK__: umask value used by MeTube. Defaults to `022`.
* __DEFAULT_THEME__: default theme to use for the ui, can be set to `light`, `dark` or `auto`. Defaults to `auto`.
* __DOWNLOAD_DIR__: path to where the downloads will be saved. Defaults to `/downloads` in the docker image, and `.` otherwise.
* __AUDIO_DOWNLOAD_DIR__: path to where audio-only downloads will be saved, if you wish to separate them from the video downloads. Defaults to the value of `DOWNLOAD_DIR`.
* __DOWNLOAD_DIRS_INDEXABLE__: if `true`, the download dirs (__DOWNLOAD_DIR__ and __AUDIO_DOWNLOAD_DIR__) are indexable on the webserver. Defaults to `false`.
* __CUSTOM_DIRS__: whether to enable downloading videos into custom directories within the __DOWNLOAD_DIR__ (or __AUDIO_DOWNLOAD_DIR__). When enabled, a drop-down appears next to the Add button to specify the download directory. Defaults to `true`.
* __CREATE_CUSTOM_DIRS__: whether to support automatically creating directories within the __DOWNLOAD_DIR__ (or __AUDIO_DOWNLOAD_DIR__) if they do not exist. When enabled, the download directory selector becomes supports free-text input, and the specified directory will be created recursively. Defaults to `true`.
* __STATE_DIR__: path to where the queue persistence files will be saved. Defaults to `/downloads/.metube` in the docker image, and `.` otherwise.
* __TEMP_DIR__: path where intermediary download files will be saved. Defaults to `/downloads` in the docker image, and `.` otherwise.
  * Set this to an SSD or RAM filesystem (e.g., `tmpfs`) for better performance
  * __Note__: Using a RAM filesystem may prevent downloads from being resumed
* __DELETE_FILE_ON_TRASHCAN__: if `true`, downloaded files are deleted on the server, when they are trashed from the &quot;Completed&quot; section of the UI. Defaults to `false`.
* __URL_PREFIX__: base path for the web server (for use when hosting behind a reverse proxy). Defaults to `/`.
* __PUBLIC_HOST_URL__: base URL for the download links shown in the UI for completed files. By default MeTube serves them under its own URL. If your download directory is accessible on another URL and you want the download links to be based there, use this variable to set it.
* __HTTPS__: use `https` instead of `http`(__CERTFILE__ and __KEYFILE__ required). Defaults to `false`.
* __CERTFILE__: HTTPS certificate file path.
* __KEYFILE__: HTTPS key file path.
* __PUBLIC_HOST_AUDIO_URL__: same as PUBLIC_HOST_URL but for audio downloads.
* __OUTPUT_TEMPLATE__: the template for the filenames of the downloaded videos, formatted according to [this spec](https://github.com/yt-dlp/yt-dlp/blob/master/README.md#output-template). Defaults to `%(title)s.%(ext)s`.
* __OUTPUT_TEMPLATE_CHAPTER__: the template for the filenames of the downloaded videos, when split into chapters via postprocessors. Defaults to `%(title)s - %(section_number)s %(section_title)s.%(ext)s`.
* __OUTPUT_TEMPLATE_PLAYLIST__: the template for the filenames of the downloaded videos, when downloaded as a playlist. Defaults to `%(playlist_title)s/%(title)s.%(ext)s`. When empty then `OUTPUT_TEMPLATE` is used.
* __DEFAULT_OPTION_PLAYLIST_STRICT_MODE__: if `true`, the &quot;Strict Playlist mode&quot; switch will be enabled by default. In this mode the playlists will be downloaded only if the url strictly points to a playlist. Urls to videos inside a playlist will be treated same as direct video url. Defaults to `false` .
* __DEFAULT_OPTION_PLAYLIST_ITEM_LIMIT__: Maximum number of playlist items that can be downloaded. Defaults to `0` (no limit).
* __YTDL_OPTIONS__: Additional options to pass to yt-dlp, in JSON format. [See available options here](https://github.com/yt-dlp/yt-dlp/blob/master/yt_dlp/YoutubeDL.py#L220). They roughly correspond to command-line options, though some do not have exact equivalents here, for example `--recode-video` has to be specified via `postprocessors`. Also note that dashes are replaced with underscores. You may find [this script](https://github.com/yt-dlp/yt-dlp/blob/master/devscripts/cli_to_api.py) helpful for converting from command line options to `YTDL_OPTIONS`.
* __YTDL_OPTIONS_FILE__: A path to a JSON file that will be loaded and used for populating `YTDL_OPTIONS` above. Please note that if both `YTDL_OPTIONS_FILE` and `YTDL_OPTIONS` are specified, the options in `YTDL_OPTIONS` take precedence.
* __ROBOTS_TXT__: A path to a `robots.txt` file mounted in the container
* __DOWNLOAD_MODE__ :This flag controls how downloads are scheduled and executed. Options are `sequential`, `concurrent`, and `limited`.  Defaults to `limited`:
    *   `sequential`: Downloads are processed one at a time. A new download won‚Äôt start until the previous one has finished. This mode is useful for conserving system resources or ensuring downloads occur in a strict order.
    *   `concurrent`: Downloads are started immediately as they are added, with no built-in limit on how many run simultaneously. This mode may overwhelm your system if too many downloads start at once.
    *   `limited`: Downloads are started concurrently but are capped by a concurrency limit. In this mode, a semaphore is used so that at most a fixed number of downloads run at any given time.
*   **MAX\_CONCURRENT\_DOWNLOADS**  This flag is used only when **DOWNLOAD\_MODE** is set to **limited**.  
    It specifies the maximum number of simultaneous downloads allowed. For example, if set to `5`, then at most five downloads will run concurrently, and any additional downloads will wait until one of the active downloads completes. Defaults to `3`. 

The project&#039;s Wiki contains examples of useful configurations contributed by users of MeTube:
* [YTDL_OPTIONS Cookbook](https://github.com/alexta69/metube/wiki/YTDL_OPTIONS-Cookbook)
* [OUTPUT_TEMPLATE Cookbook](https://github.com/alexta69/metube/wiki/OUTPUT_TEMPLATE-Cookbook)

## Using browser cookies

In case you need to use your browser&#039;s cookies with MeTube, for example to download restricted or private videos:

* Add the following to your docker-compose.yml:

```yaml
    volumes:
      - /path/to/cookies:/cookies
    environment:
      - YTDL_OPTIONS={&quot;cookiefile&quot;:&quot;/cookies/cookies.txt&quot;}
```

* Install in your browser an extension to extract cookies:
  * [Firefox](https://addons.mozilla.org/en-US/firefox/addon/export-cookies-txt/)
  * [Chrome](https://chrome.google.com/webstore/detail/get-cookiestxt-locally/cclelndahbckbenkjhflpdbgdldlbecc)
* Extract the cookies you need with the extension and rename the file `cookies.txt`
* Drop the file in the folder you configured in the docker-compose.yml above
* Restart the container

## Browser extensions

Browser extensions allow right-clicking videos and sending them directly to MeTube. Please note that if you&#039;re on an HTTPS page, your MeTube instance must be behind an HTTPS reverse proxy (see below) for the extensions to work.

__Chrome:__ contributed by [Rpsl](https://github.com/rpsl). You can install it from [Google Chrome Webstore](https://chrome.google.com/webstore/detail/metube-downloader/fbmkmdnlhacefjljljlbhkodfmfkijdh) or use developer mode and install [from sources](https://github.com/Rpsl/metube-browser-extension).

__Firefox:__ contributed by [nanocortex](https://github.com/nanocortex). You can install it from [Firefox Addons](https://addons.mozilla.org/en-US/firefox/addon/metube-downloader) or get sources from [here](https://github.com/nanocortex/metube-firefox-addon).

## iOS Shortcut

[rithask](https://github.com/rithask) created an iOS shortcut to send URLs to MeTube from Safari. Enter the MeTube instance address when prompted which will be saved for later use. You can run the shortcut from Safari‚Äôs share menu. The shortcut can be downloaded from [this iCloud link](https://www.icloud.com/shortcuts/66627a9f334c467baabdb2769763a1a6).

## iOS Compatibility

iOS has strict requirements for video files, requiring h264 or h265 video codec and aac audio codec in MP4 container. This can sometimes be a lower quality than the best quality available. To accommodate iOS requirements, when downloading a MP4 format you can choose &quot;Best (iOS)&quot; to get the best quality formats as compatible as possible with iOS requirements.

To force all downloads to be converted to an iOS compatible codec insert this as an environment variable 

```yaml
  environment:
    - &#039;YTDL_OPTIONS={&quot;format&quot;: &quot;best&quot;, &quot;exec&quot;: &quot;ffmpeg -i %(filepath)q -c:v libx264 -c:a aac %(filepath)q.h264.mp4&quot;}&#039;
```

## Bookmarklet

[kushfest](https://github.com/kushfest) has created a Chrome bookmarklet for sending the currently open webpage to MeTube. Please note that if you&#039;re on an HTTPS page, your MeTube instance must be configured with `HTTPS` as `true` in the environment, or be behind an HTTPS reverse proxy (see below) for the bookmarklet to work.

GitHub doesn&#039;t allow embedding JavaScript as a link, so the bookmarklet has to be created manually by copying the following code to a new bookmark you create on your bookmarks bar. Change the hostname in the URL below to point to your MeTube instance.

```javascript
javascript:!function(){xhr=new XMLHttpRequest();xhr.open(&quot;POST&quot;,&quot;https://metube.domain.com/add&quot;);xhr.withCredentials=true;xhr.send(JSON.stringify({&quot;url&quot;:document.location.href,&quot;quality&quot;:&quot;best&quot;}));xhr.onload=function(){if(xhr.status==200){alert(&quot;Sent to metube!&quot;)}else{alert(&quot;Send to metube failed. Check the javascript console for clues.&quot;)}}}();
```

[shoonya75](https://github.com/shoonya75) has contributed a Firefox version:

```javascript
javascript:(function(){xhr=new XMLHttpRequest();xhr.open(&quot;POST&quot;,&quot;https://metube.domain.com/add&quot;);xhr.send(JSON.stringify({&quot;url&quot;:document.location.href,&quot;quality&quot;:&quot;best&quot;}));xhr.onload=function(){if(xhr.status==200){alert(&quot;Sent to metube!&quot;)}else{alert(&quot;Send to metube failed. Check the javascript console for clues.&quot;)}}})();
```

The above bookmarklets use `alert()` as a success/failure notification. The following will show a toast message instead:

Chrome:

```javascript
javascript:!function(){function notify(msg) {var sc = document.scrollingElement.scrollTop; var text = document.createElement(&#039;span&#039;);text.innerHTML=msg;var ts = text.style;ts.all = &#039;revert&#039;;ts.color = &#039;#000&#039;;ts.fontFamily = &#039;Verdana, sans-serif&#039;;ts.fontSize = &#039;15px&#039;;ts.backgroundColor = &#039;white&#039;;ts.padding = &#039;15px&#039;;ts.border = &#039;1px solid gainsboro&#039;;ts.boxShadow = &#039;3px 3px 10px&#039;;ts.zIndex = &#039;100&#039;;document.body.appendChild(text);ts.position = &#039;absolute&#039;; ts.top = 50 + sc + &#039;px&#039;; ts.left = (window.innerWidth / 2)-(text.offsetWidth / 2) + &#039;px&#039;; setTimeout(function () { text.style.visibility = &quot;hidden&quot;; }, 1500);}xhr=new XMLHttpRequest();xhr.open(&quot;POST&quot;,&quot;https://metube.domain.com/add&quot;);xhr.send(JSON.stringify({&quot;url&quot;:document.location.href,&quot;quality&quot;:&quot;best&quot;}));xhr.onload=function() { if(xhr.status==200){notify(&quot;Sent to metube!&quot;)}else {notify(&quot;Send to metube failed. Check the javascript console for clues.&quot;)}}}();
```

Firefox:

```javascript
javascript:(function(){function notify(msg) {var sc = document.scrollingElement.scrollTop; var text = document.createElement(&#039;span&#039;);text.innerHTML=msg;var ts = text.style;ts.all = &#039;revert&#039;;ts.color = &#039;#000&#039;;ts.fontFamily = &#039;Verdana, sans-serif&#039;;ts.fontSize = &#039;15px&#039;;ts.backgroundColor = &#039;white&#039;;ts.padding = &#039;15px&#039;;ts.border = &#039;1px solid gainsboro&#039;;ts.boxShadow = &#039;3px 3px 10px&#039;;ts.zIndex = &#039;100&#039;;document.body.appendChild(text);ts.position = &#039;absolute&#039;; ts.top = 50 + sc + &#039;px&#039;; ts.left = (window.innerWidth / 2)-(text.offsetWidth / 2) + &#039;px&#039;; setTimeout(function () { text.style.visibility = &quot;hidden&quot;; }, 1500);}xhr=new XMLHttpRequest();xhr.open(&quot;POST&quot;,&quot;https://metube.domain.com/add&quot;);xhr.send(JSON.stringify({&quot;url&quot;:document.location.href,&quot;quality&quot;:&quot;best&quot;}));xhr.onload=function() { if(xhr.status==200){notify(&quot;Sent to metube!&quot;)}else {notify(&quot;Send to metube failed. Check the javascript console for clues.&quot;)}}})();
```

## Raycast extension

[dotvhs](https://github.com/dotvhs) has created an [extension for Raycast](https://www.raycast.com/dot/metube) that allows adding videos to MeTube directly from Raycast.

## HTTPS support, and running behind a reverse proxy

It&#039;s possible to configure MeTube to listen in HTTPS mode. `docker-compose` example:

```yaml
services:
  metube:
    image: ghcr.io/alexta69/metube
    container_name: metube
    restart: unless-stopped
    ports:
      - &quot;8081:8081&quot;
    volumes:
      - /path/to/downloads:/downloads
      - /path/to/ssl/crt:/ssl/crt.pem
      - /path/to/ssl/key:/ssl/key.pem
    environment:
      - HTTPS=true
      - CERTFILE=/ssl/crt.pem
      - KEYFILE=/ssl/key.pem
```

It&#039;s also possible to run MeTube behind a reverse proxy, in order to support authentication. HTTPS support can also be added in this way.

When running behind a reverse proxy which remaps the URL (i.e. serves MeTube under a subdirectory and not under root), don&#039;t forget to set the URL_PREFIX environment variable to the correct value.

If you&#039;re using the [linuxserver/swag](https://docs.linuxserver.io/general/swag) image for your reverse proxying needs (which I can heartily recommend), it already includes ready snippets for proxying MeTube both in [subfolder](https://github.com/linuxserver/reverse-proxy-confs/blob/master/metube.subfolder.conf.sample) and [subdomain](https://github.com/linuxserver/reverse-proxy-confs/blob/master/metube.subdomain.conf.sample) modes under the `nginx/proxy-confs` directory in the configuration volume. It also includes Authelia which can be used for authentication.

### NGINX

```nginx
location /metube/ {
        proxy_pass http://metube:8081;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection &quot;upgrade&quot;;
        proxy_set_header Host $host;
}
```

Note: the extra `proxy_set_header` directives are there to make WebSocket work.

### Apache

Contributed by [PIE-yt](https://github.com/PIE-yt). Source [here](https://gist.github.com/PIE-yt/29e7116588379032427f5bd446b2cac4).

```apache
# For putting in your Apache sites site.conf
# Serves MeTube under a /metube/ subdir (http://yourdomain.com/metube/)
&lt;Location /metube/&gt;
    ProxyPass http://localhost:8081/ retry=0 timeout=30
    ProxyPassReverse http://localhost:8081/
&lt;/Location&gt;

&lt;Location /metube/socket.io&gt;
    RewriteEngine On
    RewriteCond %{QUERY_STRING} transport=websocket    [NC]
    RewriteRule /(.*) ws://localhost:8081/socket.io/$1 [P,L]
    ProxyPass http://localhost:8081/socket.io retry=0 timeout=30
    ProxyPassReverse http://localhost:8081/socket.io
&lt;/Location&gt;
```

### Caddy

The following example Caddyfile gets a reverse proxy going behind [caddy](https://caddyserver.com).

```caddyfile
example.com {
  route /metube/* {
    uri strip_prefix metube
    reverse_proxy metube:8081
  }
}
```

## Updating yt-dlp

The engine which powers the actual video downloads in MeTube is [yt-dlp](https://github.com/yt-dlp/yt-dlp). Since video sites regularly change their layouts, frequent updates of yt-dlp are required to keep up.

There&#039;s an automatic nightly build of MeTube which looks for a new version of yt-dlp, and if one exists, the build pulls it and publishes an updated docker image. Therefore, in order to keep up with the changes, it&#039;s recommended that you update your MeTube container regularly with the latest image.

I recommend installing and setting up [watchtower](https://github.com/containrrr/watchtower) for this purpose.

## Troubleshooting and submitting issues

Before asking a question or submitting an issue for MeTube, please remember that MeTube is only a UI for [yt-dlp](https://github.com/yt-dlp/yt-dlp). Any issues you might be experiencing with authentication to video websites, postprocessing, permissions, other `YTDL_OPTIONS` configurations which seem not to work, or anything else that concerns the workings of the underlying yt-dlp library, need not be opened on the MeTube project. In order to debug and troubleshoot them, it&#039;s advised to try using the yt-dlp binary directly first, bypassing the UI, and once that is working, importing the options that worked for you into `YTDL_OPTIONS`.

In order to test with the yt-dlp command directly, you can either download it and run it locally, or for a better simulation of its actual conditions, you can run it within the MeTube container itself. Assuming your MeTube container is called `metube`, run the following on your Docker host to get a shell inside the container:

```bash
docker exec -ti metube sh
cd /downloads
```

Once there, you can use the yt-dlp command freely.

## Submitting feature requests

MeTube development relies on code contributions by the community. The program as it currently stands fits my own use cases, and is therefore feature-complete as far as I&#039;m concerned. If your use cases are different and require additional features, please feel free to submit PRs that implement those features. It&#039;s advisable to create an issue first to discuss the planned implementation, because in an effort to reduce bloat, some PRs may not be accepted. However, note that opening a feature request when you don&#039;t intend to implement the feature will rarely result in the request being fulfilled.

## Building and running locally

Make sure you have node.js and Python 3.11 installed.

```bash
cd metube/ui
# install Angular and build the UI
npm install
node_modules/.bin/ng build
# install python dependencies
cd ..
pip3 install pipenv
pipenv install
# run
pipenv run python3 app/main.py
```

A Docker image can be built locally (it will build the UI too):

```bash
docker build -t metube .
```

## Development notes

* The above works on Windows and macOS as well as Linux.
* If you&#039;re running the server in VSCode, your downloads will go to your user&#039;s Downloads folder (this is configured via the environment in .vscode/launch.json).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[stanfordnlp/dspy]]></title>
            <link>https://github.com/stanfordnlp/dspy</link>
            <guid>https://github.com/stanfordnlp/dspy</guid>
            <pubDate>Wed, 30 Apr 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[DSPy: The framework for programming‚Äînot prompting‚Äîlanguage models]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/stanfordnlp/dspy">stanfordnlp/dspy</a></h1>
            <p>DSPy: The framework for programming‚Äînot prompting‚Äîlanguage models</p>
            <p>Language: Python</p>
            <p>Stars: 23,915</p>
            <p>Forks: 1,839</p>
            <p>Stars today: 120 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img align=&quot;center&quot; src=&quot;docs/docs/static/img/dspy_logo.png&quot; width=&quot;460px&quot; /&gt;
&lt;/p&gt;
&lt;p align=&quot;left&quot;&gt;


## DSPy: _Programming_‚Äînot prompting‚ÄîFoundation Models

**Documentation:** [DSPy Docs](https://dspy.ai/)

[![PyPI Downloads](https://static.pepy.tech/badge/dspy/month)](https://pepy.tech/projects/dspy)


----

DSPy is the framework for _programming‚Äîrather than prompting‚Äîlanguage models_. It allows you to iterate fast on **building modular AI systems** and offers algorithms for **optimizing their prompts and weights**, whether you&#039;re building simple classifiers, sophisticated RAG pipelines, or Agent loops.

DSPy stands for Declarative Self-improving Python. Instead of brittle prompts, you write compositional _Python code_ and use DSPy to **teach your LM to deliver high-quality outputs**. Learn more via our [official documentation site](https://dspy.ai/) or meet the community, seek help, or start contributing via this GitHub repo and our [Discord server](https://discord.gg/XCGy2WDCQB).


## Documentation: [dspy.ai](https://dspy.ai)


**Please go to the [DSPy Docs at dspy.ai](https://dspy.ai)**


## Installation


```bash
pip install dspy
```

To install the very latest from `main`:

```bash
pip install git+https://github.com/stanfordnlp/dspy.git
````




## üìú Citation &amp; Reading More

If you&#039;re looking to understand the framework, please go to the [DSPy Docs at dspy.ai](https://dspy.ai).

If you&#039;re looking to understand the underlying research, this is a set of our papers:

**[Jun&#039;24] [Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs](https://arxiv.org/abs/2406.11695)**       
**[Oct&#039;23] [DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines](https://arxiv.org/abs/2310.03714)**     
[Jul&#039;24] [Fine-Tuning and Prompt Optimization: Two Great Steps that Work Better Together](https://arxiv.org/abs/2407.10930)     
[Jun&#039;24] [Prompts as Auto-Optimized Training Hyperparameters](https://arxiv.org/abs/2406.11706)    
[Feb&#039;24] [Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models](https://arxiv.org/abs/2402.14207)         
[Jan&#039;24] [In-Context Learning for Extreme Multi-Label Classification](https://arxiv.org/abs/2401.12178)       
[Dec&#039;23] [DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines](https://arxiv.org/abs/2312.13382)   
[Dec&#039;22] [Demonstrate-Search-Predict: Composing Retrieval &amp; Language Models for Knowledge-Intensive NLP](https://arxiv.org/abs/2212.14024.pdf)

To stay up to date or learn more, follow [@lateinteraction](https://twitter.com/lateinteraction) on Twitter.

The **DSPy** logo is designed by **Chuyi Zhang**.

If you use DSPy or DSP in a research paper, please cite our work as follows:

```
@inproceedings{khattab2024dspy,
  title={DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines},
  author={Khattab, Omar and Singhvi, Arnav and Maheshwari, Paridhi and Zhang, Zhiyuan and Santhanam, Keshav and Vardhamanan, Sri and Haq, Saiful and Sharma, Ashutosh and Joshi, Thomas T. and Moazam, Hanna and Miller, Heather and Zaharia, Matei and Potts, Christopher},
  journal={The Twelfth International Conference on Learning Representations},
  year={2024}
}
@article{khattab2022demonstrate,
  title={Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive {NLP}},
  author={Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
  journal={arXiv preprint arXiv:2212.14024},
  year={2022}
}
```

&lt;!-- You can also read more about the evolution of the framework from Demonstrate-Search-Predict to DSPy:

* [**DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines**](https://arxiv.org/abs/2312.13382)   (Academic Paper, Dec 2023) 
* [**DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines**](https://arxiv.org/abs/2310.03714) (Academic Paper, Oct 2023) 
* [**Releasing DSPy, the latest iteration of the framework**](https://twitter.com/lateinteraction/status/1694748401374490946) (Twitter Thread, Aug 2023)
* [**Releasing the DSP Compiler (v0.1)**](https://twitter.com/lateinteraction/status/1625231662849073160)  (Twitter Thread, Feb 2023)
* [**Introducing DSP**](https://twitter.com/lateinteraction/status/1617953413576425472)  (Twitter Thread, Jan 2023)
* [**Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP**](https://arxiv.org/abs/2212.14024.pdf) (Academic Paper, Dec 2022) --&gt;

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[unclecode/crawl4ai]]></title>
            <link>https://github.com/unclecode/crawl4ai</link>
            <guid>https://github.com/unclecode/crawl4ai</guid>
            <pubDate>Wed, 30 Apr 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[üöÄü§ñ Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper. Don't be shy, join here: https://discord.gg/jP8KfhDhyN]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/unclecode/crawl4ai">unclecode/crawl4ai</a></h1>
            <p>üöÄü§ñ Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper. Don't be shy, join here: https://discord.gg/jP8KfhDhyN</p>
            <p>Language: Python</p>
            <p>Stars: 41,554</p>
            <p>Forks: 3,765</p>
            <p>Stars today: 350 stars today</p>
            <h2>README</h2><pre># üöÄü§ñ Crawl4AI: Open-source LLM Friendly Web Crawler &amp; Scraper.

&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://trendshift.io/repositories/11716&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11716&quot; alt=&quot;unclecode%2Fcrawl4ai | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

[![GitHub Stars](https://img.shields.io/github/stars/unclecode/crawl4ai?style=social)](https://github.com/unclecode/crawl4ai/stargazers)
[![GitHub Forks](https://img.shields.io/github/forks/unclecode/crawl4ai?style=social)](https://github.com/unclecode/crawl4ai/network/members)

[![PyPI version](https://badge.fury.io/py/crawl4ai.svg)](https://badge.fury.io/py/crawl4ai)
[![Python Version](https://img.shields.io/pypi/pyversions/crawl4ai)](https://pypi.org/project/crawl4ai/)
[![Downloads](https://static.pepy.tech/badge/crawl4ai/month)](https://pepy.tech/project/crawl4ai)

&lt;!-- [![Documentation Status](https://readthedocs.org/projects/crawl4ai/badge/?version=latest)](https://crawl4ai.readthedocs.io/) --&gt;
[![License](https://img.shields.io/github/license/unclecode/crawl4ai)](https://github.com/unclecode/crawl4ai/blob/main/LICENSE)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Security: bandit](https://img.shields.io/badge/security-bandit-yellow.svg)](https://github.com/PyCQA/bandit)
[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](code_of_conduct.md)

&lt;/div&gt;

Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for LLMs, AI agents, and data pipelines. Open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease.  

[‚ú® Check out latest update v0.6.0](#-recent-updates)

üéâ **Version 0.6.0 is now available!** This release candidate introduces World-aware Crawling with geolocation and locale settings, Table-to-DataFrame extraction, Browser pooling with pre-warming, Network and console traffic capture, MCP integration for AI tools, and a completely revamped Docker deployment! [Read the release notes ‚Üí](https://docs.crawl4ai.com/blog)

&lt;details&gt;
&lt;summary&gt;ü§ì &lt;strong&gt;My Personal Story&lt;/strong&gt;&lt;/summary&gt;

My journey with computers started in childhood when my dad, a computer scientist, introduced me to an Amstrad computer. Those early days sparked a fascination with technology, leading me to pursue computer science and specialize in NLP during my postgraduate studies. It was during this time that I first delved into web crawling, building tools to help researchers organize papers and extract information from publications a challenging yet rewarding experience that honed my skills in data extraction.

Fast forward to 2023, I was working on a tool for a project and needed a crawler to convert a webpage into markdown. While exploring solutions, I found one that claimed to be open-source but required creating an account and generating an API token. Worse, it turned out to be a SaaS model charging $16, and its quality didn‚Äôt meet my standards. Frustrated, I realized this was a deeper problem. That frustration turned into turbo anger mode, and I decided to build my own solution. In just a few days, I created Crawl4AI. To my surprise, it went viral, earning thousands of GitHub stars and resonating with a global community.

I made Crawl4AI open-source for two reasons. First, it‚Äôs my way of giving back to the open-source community that has supported me throughout my career. Second, I believe data should be accessible to everyone, not locked behind paywalls or monopolized by a few. Open access to data lays the foundation for the democratization of AI, a vision where individuals can train their own models and take ownership of their information. This library is the first step in a larger journey to create the best open-source data extraction and generation tool the world has ever seen, built collaboratively by a passionate community.

Thank you to everyone who has supported this project, used it, and shared feedback. Your encouragement motivates me to dream even bigger. Join us, file issues, submit PRs, or spread the word. Together, we can build a tool that truly empowers people to access their own data and reshape the future of AI.
&lt;/details&gt;

## üßê Why Crawl4AI?

1. **Built for LLMs**: Creates smart, concise Markdown optimized for RAG and fine-tuning applications.  
2. **Lightning Fast**: Delivers results 6x faster with real-time, cost-efficient performance.  
3. **Flexible Browser Control**: Offers session management, proxies, and custom hooks for seamless data access.  
4. **Heuristic Intelligence**: Uses advanced algorithms for efficient extraction, reducing reliance on costly models.  
5. **Open Source &amp; Deployable**: Fully open-source with no API keys‚Äîready for Docker and cloud integration.  
6. **Thriving Community**: Actively maintained by a vibrant community and the #1 trending GitHub repository.

## üöÄ Quick Start 

1. Install Crawl4AI:
```bash
# Install the package
pip install -U crawl4ai

# For pre release versions
pip install crawl4ai --pre

# Run post-installation setup
crawl4ai-setup

# Verify your installation
crawl4ai-doctor
```

If you encounter any browser-related issues, you can install them manually:
```bash
python -m playwright install --with-deps chromium
```

2. Run a simple web crawl with Python:
```python
import asyncio
from crawl4ai import *

async def main():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url=&quot;https://www.nbcnews.com/business&quot;,
        )
        print(result.markdown)

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

3. Or use the new command-line interface:
```bash
# Basic crawl with markdown output
crwl https://www.nbcnews.com/business -o markdown

# Deep crawl with BFS strategy, max 10 pages
crwl https://docs.crawl4ai.com --deep-crawl bfs --max-pages 10

# Use LLM extraction with a specific question
crwl https://www.example.com/products -q &quot;Extract all product prices&quot;
```

## ‚ú® Features 

&lt;details&gt;
&lt;summary&gt;üìù &lt;strong&gt;Markdown Generation&lt;/strong&gt;&lt;/summary&gt;

- üßπ **Clean Markdown**: Generates clean, structured Markdown with accurate formatting.
- üéØ **Fit Markdown**: Heuristic-based filtering to remove noise and irrelevant parts for AI-friendly processing.
- üîó **Citations and References**: Converts page links into a numbered reference list with clean citations.
- üõ†Ô∏è **Custom Strategies**: Users can create their own Markdown generation strategies tailored to specific needs.
- üìö **BM25 Algorithm**: Employs BM25-based filtering for extracting core information and removing irrelevant content. 
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üìä &lt;strong&gt;Structured Data Extraction&lt;/strong&gt;&lt;/summary&gt;

- ü§ñ **LLM-Driven Extraction**: Supports all LLMs (open-source and proprietary) for structured data extraction.
- üß± **Chunking Strategies**: Implements chunking (topic-based, regex, sentence-level) for targeted content processing.
- üåå **Cosine Similarity**: Find relevant content chunks based on user queries for semantic extraction.
- üîé **CSS-Based Extraction**: Fast schema-based data extraction using XPath and CSS selectors.
- üîß **Schema Definition**: Define custom schemas for extracting structured JSON from repetitive patterns.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üåê &lt;strong&gt;Browser Integration&lt;/strong&gt;&lt;/summary&gt;

- üñ•Ô∏è **Managed Browser**: Use user-owned browsers with full control, avoiding bot detection.
- üîÑ **Remote Browser Control**: Connect to Chrome Developer Tools Protocol for remote, large-scale data extraction.
- üë§ **Browser Profiler**: Create and manage persistent profiles with saved authentication states, cookies, and settings.
- üîí **Session Management**: Preserve browser states and reuse them for multi-step crawling.
- üß© **Proxy Support**: Seamlessly connect to proxies with authentication for secure access.
- ‚öôÔ∏è **Full Browser Control**: Modify headers, cookies, user agents, and more for tailored crawling setups.
- üåç **Multi-Browser Support**: Compatible with Chromium, Firefox, and WebKit.
- üìê **Dynamic Viewport Adjustment**: Automatically adjusts the browser viewport to match page content, ensuring complete rendering and capturing of all elements.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üîé &lt;strong&gt;Crawling &amp; Scraping&lt;/strong&gt;&lt;/summary&gt;

- üñºÔ∏è **Media Support**: Extract images, audio, videos, and responsive image formats like `srcset` and `picture`.
- üöÄ **Dynamic Crawling**: Execute JS and wait for async or sync for dynamic content extraction.
- üì∏ **Screenshots**: Capture page screenshots during crawling for debugging or analysis.
- üìÇ **Raw Data Crawling**: Directly process raw HTML (`raw:`) or local files (`file://`).
- üîó **Comprehensive Link Extraction**: Extracts internal, external links, and embedded iframe content.
- üõ†Ô∏è **Customizable Hooks**: Define hooks at every step to customize crawling behavior.
- üíæ **Caching**: Cache data for improved speed and to avoid redundant fetches.
- üìÑ **Metadata Extraction**: Retrieve structured metadata from web pages.
- üì° **IFrame Content Extraction**: Seamless extraction from embedded iframe content.
- üïµÔ∏è **Lazy Load Handling**: Waits for images to fully load, ensuring no content is missed due to lazy loading.
- üîÑ **Full-Page Scanning**: Simulates scrolling to load and capture all dynamic content, perfect for infinite scroll pages.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üöÄ &lt;strong&gt;Deployment&lt;/strong&gt;&lt;/summary&gt;

- üê≥ **Dockerized Setup**: Optimized Docker image with FastAPI server for easy deployment.
- üîë **Secure Authentication**: Built-in JWT token authentication for API security.
- üîÑ **API Gateway**: One-click deployment with secure token authentication for API-based workflows.
- üåê **Scalable Architecture**: Designed for mass-scale production and optimized server performance.
- ‚òÅÔ∏è **Cloud Deployment**: Ready-to-deploy configurations for major cloud platforms.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üéØ &lt;strong&gt;Additional Features&lt;/strong&gt;&lt;/summary&gt;

- üï∂Ô∏è **Stealth Mode**: Avoid bot detection by mimicking real users.
- üè∑Ô∏è **Tag-Based Content Extraction**: Refine crawling based on custom tags, headers, or metadata.
- üîó **Link Analysis**: Extract and analyze all links for detailed data exploration.
- üõ°Ô∏è **Error Handling**: Robust error management for seamless execution.
- üîê **CORS &amp; Static Serving**: Supports filesystem-based caching and cross-origin requests.
- üìñ **Clear Documentation**: Simplified and updated guides for onboarding and advanced usage.
- üôå **Community Recognition**: Acknowledges contributors and pull requests for transparency.

&lt;/details&gt;

## Try it Now!

‚ú® Play around with this [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1SgRPrByQLzjRfwoRNq1wSGE9nYY_EE8C?usp=sharing)

‚ú® Visit our [Documentation Website](https://docs.crawl4ai.com/)

## Installation üõ†Ô∏è

Crawl4AI offers flexible installation options to suit various use cases. You can install it as a Python package or use Docker.

&lt;details&gt;
&lt;summary&gt;üêç &lt;strong&gt;Using pip&lt;/strong&gt;&lt;/summary&gt;

Choose the installation option that best fits your needs:

### Basic Installation

For basic web crawling and scraping tasks:

```bash
pip install crawl4ai
crawl4ai-setup # Setup the browser
```

By default, this will install the asynchronous version of Crawl4AI, using Playwright for web crawling.

üëâ **Note**: When you install Crawl4AI, the `crawl4ai-setup` should automatically install and set up Playwright. However, if you encounter any Playwright-related errors, you can manually install it using one of these methods:

1. Through the command line:

   ```bash
   playwright install
   ```

2. If the above doesn&#039;t work, try this more specific command:

   ```bash
   python -m playwright install chromium
   ```

This second method has proven to be more reliable in some cases.

---

### Installation with Synchronous Version

The sync version is deprecated and will be removed in future versions. If you need the synchronous version using Selenium:

```bash
pip install crawl4ai[sync]
```

---

### Development Installation

For contributors who plan to modify the source code:

```bash
git clone https://github.com/unclecode/crawl4ai.git
cd crawl4ai
pip install -e .                    # Basic installation in editable mode
```

Install optional features:

```bash
pip install -e &quot;.[torch]&quot;           # With PyTorch features
pip install -e &quot;.[transformer]&quot;     # With Transformer features
pip install -e &quot;.[cosine]&quot;          # With cosine similarity features
pip install -e &quot;.[sync]&quot;            # With synchronous crawling (Selenium)
pip install -e &quot;.[all]&quot;             # Install all optional features
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üê≥ &lt;strong&gt;Docker Deployment&lt;/strong&gt;&lt;/summary&gt;

&gt; üöÄ **Now Available!** Our completely redesigned Docker implementation is here! This new solution makes deployment more efficient and seamless than ever.

### New Docker Features

The new Docker implementation includes:
- **Browser pooling** with page pre-warming for faster response times
- **Interactive playground** to test and generate request code
- **MCP integration** for direct connection to AI tools like Claude Code
- **Comprehensive API endpoints** including HTML extraction, screenshots, PDF generation, and JavaScript execution
- **Multi-architecture support** with automatic detection (AMD64/ARM64)
- **Optimized resources** with improved memory management

### Getting Started

```bash
# Pull and run the latest release candidate
docker pull unclecode/crawl4ai:0.6.0-rN # Use your favorite revision number
docker run -d -p 11235:11235 --name crawl4ai --shm-size=1g unclecode/crawl4ai:0.6.0-rN # Use your favorite revision number

# Visit the playground at http://localhost:11235/playground
```

For complete documentation, see our [Docker Deployment Guide](https://docs.crawl4ai.com/core/docker-deployment/).

&lt;/details&gt;

---

### Quick Test

Run a quick test (works for both Docker options):

```python
import requests

# Submit a crawl job
response = requests.post(
    &quot;http://localhost:11235/crawl&quot;,
    json={&quot;urls&quot;: &quot;https://example.com&quot;, &quot;priority&quot;: 10}
)
task_id = response.json()[&quot;task_id&quot;]

# Continue polling until the task is complete (status=&quot;completed&quot;)
result = requests.get(f&quot;http://localhost:11235/task/{task_id}&quot;)
```

For more examples, see our [Docker Examples](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/docker_example.py). For advanced configuration, environment variables, and usage examples, see our [Docker Deployment Guide](https://docs.crawl4ai.com/basic/docker-deployment/).

&lt;/details&gt;


## üî¨ Advanced Usage Examples üî¨

You can check the project structure in the directory [https://github.com/unclecode/crawl4ai/docs/examples](docs/examples). Over there, you can find a variety of examples; here, some popular examples are shared.

&lt;details&gt;
&lt;summary&gt;üìù &lt;strong&gt;Heuristic Markdown Generation with Clean and Fit Markdown&lt;/strong&gt;&lt;/summary&gt;

```python
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.content_filter_strategy import PruningContentFilter, BM25ContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

async def main():
    browser_config = BrowserConfig(
        headless=True,  
        verbose=True,
    )
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.ENABLED,
        markdown_generator=DefaultMarkdownGenerator(
            content_filter=PruningContentFilter(threshold=0.48, threshold_type=&quot;fixed&quot;, min_word_threshold=0)
        ),
        # markdown_generator=DefaultMarkdownGenerator(
        #     content_filter=BM25ContentFilter(user_query=&quot;WHEN_WE_FOCUS_BASED_ON_A_USER_QUERY&quot;, bm25_threshold=1.0)
        # ),
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url=&quot;https://docs.micronaut.io/4.7.6/guide/&quot;,
            config=run_config
        )
        print(len(result.markdown.raw_markdown))
        print(len(result.markdown.fit_markdown))

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üñ•Ô∏è &lt;strong&gt;Executing JavaScript &amp; Extract Structured Data without LLMs&lt;/strong&gt;&lt;/summary&gt;

```python
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
import json

async def main():
    schema = {
    &quot;name&quot;: &quot;KidoCode Courses&quot;,
    &quot;baseSelector&quot;: &quot;section.charge-methodology .w-tab-content &gt; div&quot;,
    &quot;fields&quot;: [
        {
            &quot;name&quot;: &quot;section_title&quot;,
            &quot;selector&quot;: &quot;h3.heading-50&quot;,
            &quot;type&quot;: &quot;text&quot;,
        },
        {
            &quot;name&quot;: &quot;section_description&quot;,
            &quot;selector&quot;: &quot;.charge-content&quot;,
            &quot;type&quot;: &quot;text&quot;,
        },
        {
            &quot;name&quot;: &quot;course_name&quot;,
            &quot;selector&quot;: &quot;.text-block-93&quot;,
            &quot;type&quot;: &quot;text&quot;,
        },
        {
            &quot;name&quot;: &quot;course_description&quot;,
            &quot;selector&quot;: &quot;.course-content-text&quot;,
            &quot;type&quot;: &quot;text&quot;,
        },
        {
            &quot;name&quot;: &quot;course_icon&quot;,
            &quot;selector&quot;: &quot;.image-92&quot;,
            &quot;type&quot;: &quot;attribute&quot;,
            &quot;attribute&quot;: &quot;src&quot;
        }
    }
}

    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)

    browser_config = BrowserConfig(
        headless=False,
        verbose=True
    )
    run_config = CrawlerRunConfig(
        extraction_strategy=extraction_strategy,
        js_code=[&quot;&quot;&quot;(async () =&gt; {const tabs = document.querySelectorAll(&quot;section.charge-methodology .tabs-menu-3 &gt; div&quot;);for(let tab of tabs) {tab.scrollIntoView();tab.click();await new Promise(r =&gt; setTimeout(r, 500));}})();&quot;&quot;&quot;],
        cache_mode=CacheMode.BYPASS
    )
        
    async with AsyncWebCrawler(config=browser_config) as crawler:
        
        result = await crawler.arun(
            url=&quot;https://www.kidocode.com/degrees/technology&quot;,
            config=run_config
        )

        companies = json.loads(result.extracted_content)
        print(f&quot;Successfully extracted {len(companies)} companies&quot;)
        print(json.dumps(companies[0], indent=2))


if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üìö &lt;strong&gt;Extracting Structured Data with LLMs&lt;/strong&gt;&lt;/summary&gt;

```python
import os
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel, Field

class OpenAIModelFee(BaseModel):
    model_name: str = Field(..., description=&quot;Name of the OpenAI model.&quot;)
    input_fee: str = Field(..., description=&quot;Fee for input token for the OpenAI model.&quot;)
    output_fee: str = Field(..., description=&quot;Fee for output token for the OpenAI model.&quot;)

async def main():
    browser_config = BrowserConfig(verbose=True)
    run_config = CrawlerRunConfig(
        word_count_threshold=1,
        extraction_strategy=LLMExtractionStrategy(
            # Here you can use any provider that Litellm library supports, for instance: ollama/qwen2
            # provider=&quot;ollama/qwen2&quot;, api_token=&quot;no-token&quot;, 
            llm_config = LLMConfig(provider=&quot;openai/gpt-4o&quot;, api_token=os.getenv(&#039;OPENAI_API_KEY&#039;)), 
            schema=OpenAIModelFee.schema(),
            extraction_type=&quot;schema&quot;,
            instruction=&quot;&quot;&quot;From the crawled content, extract all mentioned model names along with their fees for input and output tokens. 
            Do not miss any models in the entire content. One extracted model JSON format should look like this: 
            {&quot;model_name&quot;: &quot;GPT-4&quot;, &quot;input_fee&quot;: &quot;US$10.00 / 1M tokens&quot;, &quot;output_fee&quot;: &quot;US$30.00 / 1M tokens&quot;}.&quot;&quot;&quot;
        ),            
     

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[volcengine/verl]]></title>
            <link>https://github.com/volcengine/verl</link>
            <guid>https://github.com/volcengine/verl</guid>
            <pubDate>Wed, 30 Apr 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[verl: Volcano Engine Reinforcement Learning for LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/volcengine/verl">volcengine/verl</a></h1>
            <p>verl: Volcano Engine Reinforcement Learning for LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 7,423</p>
            <p>Forks: 816</p>
            <p>Stars today: 90 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
 üëã Hi, everyone! 
    &lt;br&gt;
    verl is a RL training library initiated by &lt;b&gt;ByteDance Seed team&lt;/b&gt; and maintained by the verl community.
&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt;
  You can get to know Bytedance Seed better through the following channelsüëá
  &lt;br&gt;
  &lt;a href=&quot;https://team.doubao.com/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Website-%231e37ff?style=for-the-badge&amp;logo=bytedance&amp;logoColor=white&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/user-attachments/assets/469535a8-42f2-4797-acdf-4f7a1d4a0c3e&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&amp;logo=wechat&amp;logoColor=white&quot;&gt;&lt;/a&gt;
 &lt;a href=&quot;https://www.xiaohongshu.com/user/profile/668e7e15000000000303157d?xsec_token=ABl2-aqekpytY6A8TuxjrwnZskU-6BsMRE_ufQQaSAvjc%3D&amp;xsec_source=pc_search&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Xiaohongshu-%23FF2442?style=for-the-badge&amp;logo=xiaohongshu&amp;logoColor=white&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.zhihu.com/org/dou-bao-da-mo-xing-tuan-dui/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/zhihu-%230084FF?style=for-the-badge&amp;logo=zhihu&amp;logoColor=white&quot;&gt;&lt;/a&gt;

&lt;/div&gt;

![seed logo](https://github.com/user-attachments/assets/c42e675e-497c-4508-8bb9-093ad4d1f216)

&lt;h1 style=&quot;text-align: center;&quot;&gt;verl: Volcano Engine Reinforcement Learning for LLMs&lt;/h1&gt;

&lt;div align=&quot;center&quot;&gt;

[&lt;img src=&quot;https://devin.ai/assets/deepwiki-badge.png&quot; alt=&quot;Ask DeepWiki.com&quot; height=&quot;20&quot;/&gt;](https://deepwiki.com/volcengine/verl)
[![GitHub Repo stars](https://img.shields.io/github/stars/volcengine/verl)](https://github.com/volcengine/verl/stargazers)
![GitHub forks](https://img.shields.io/github/forks/volcengine/verl)
[![Twitter](https://img.shields.io/twitter/follow/verl_project)](https://twitter.com/verl_project)
&lt;a href=&quot;https://join.slack.com/t/verlgroup/shared_invite/zt-2w5p9o4c3-yy0x2Q56s_VlGLsJ93A6vA&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Slack-verl-blueviolet?logo=slack&amp;amp&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://arxiv.org/pdf/2409.19256&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=EuroSys&amp;message=Paper&amp;color=red&quot;&gt;&lt;/a&gt;
![GitHub contributors](https://img.shields.io/github/contributors/volcengine/verl)
[![Documentation](https://img.shields.io/badge/documentation-blue)](https://verl.readthedocs.io/en/latest/)
&lt;a href=&quot;https://raw.githubusercontent.com/eric-haibin-lin/verl-community/refs/heads/main/WeChat.JPG&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ÂæÆ‰ø°-green?logo=wechat&amp;amp&quot;&gt;&lt;/a&gt;

&lt;/div&gt;

verl is a flexible, efficient and production-ready RL training library for large language models (LLMs).

verl is the open-source version of **[HybridFlow: A Flexible and Efficient RLHF Framework](https://arxiv.org/abs/2409.19256v2)** paper.

verl is flexible and easy to use with:

- **Easy extension of diverse RL algorithms**: The hybrid-controller programming model enables flexible representation and efficient execution of complex Post-Training dataflows. Build RL dataflows such as GRPO, PPO in a few lines of code.

- **Seamless integration of existing LLM infra with modular APIs**: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as FSDP, Megatron-LM, vLLM, SGLang, etc

- **Flexible device mapping**: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.

- Ready integration with popular HuggingFace models

verl is fast with:

- **State-of-the-art throughput**: SOTA LLM training and inference engine integrations and SOTA RL throughput.

- **Efficient actor model resharding with 3D-HybridEngine**: Eliminates memory redundancy and significantly reduces communication overhead during transitions between training and generation phases.

&lt;/p&gt;

## News

- [2025/05] verl will be presented at [GOSIM x PyTorch Day 2025](https://paris2025.gosim.org/). See you in Paris!
- [2025/04] We will give a tutorial about latest post-training techniques and programming guide for verl at [ICLR 2025 Expo](https://iclr.cc/virtual/2025/calendar?filter_events=Expo+Talk+Panel&amp;filter_rooms=), [SCI-FM workshop](https://open-foundation-model.github.io/) and [LMSys afterparty](https://lu.ma/d23nyynm). Talk materials available [here](https://github.com/eric-haibin-lin/verl-community/tree/main/iclr25).
- [2025/04] [Seed-Thinking-v1.5](https://github.com/ByteDance-Seed/Seed-Thinking-v1.5/blob/main/seed-thinking-v1.5.pdf) tech report is released! Trained with verl, Seed-Thinking-v1.5 achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on GPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond reasoning tasks, the method demonstrates notable generalization across diverse domains.
- [2025/04] We are working on open source recipe for [VAPO](https://arxiv.org/pdf/2504.05118) (value-based augmented PPO), our latest RL method for reasoning models. Trained from Qwen-32B-base model, VAPO achieves 60.4 on AIME 2024, outperforming DeepSeek-zero-32B and DAPO-32B.
- [2025/03] verl v0.3.0.post1 is released! See [release note](https://github.com/volcengine/verl/releases/) for details.
- [2025/03] [DAPO](https://dapo-sia.github.io/) is the open-sourced SOTA RL algorithm that achieves 50 points on AIME 2024 based on the Qwen2.5-32B pre-trained model, surpassing the previous SOTA achieved by DeepSeek&#039;s GRPO (DeepSeek-R1-Zero-Qwen-32B). DAPO&#039;s training is fully powered by verl and the reproduction code is available in `recipe/dapo` now.
&lt;details&gt;&lt;summary&gt; more... &lt;/summary&gt;
&lt;ul&gt;
  &lt;li&gt;[2025/03] We introduced the programming model of verl at the [vLLM Beijing Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg) and [verl intro and updates](https://github.com/eric-haibin-lin/verl-community/blob/main/slides/verl-lmsys-meetup.pdf) at the [SGLang-LMSYS Org Meetup](https://lu.ma/ntjrr7ig) in Sunnyvale mid-March.&lt;/li&gt;
  &lt;li&gt;[2025/02] verl v0.2.0.post2 is released!&lt;/li&gt;
  &lt;li&gt;[2025/01] [Doubao-1.5-pro](https://team.doubao.com/zh/special/doubao_1_5_pro) is released with SOTA-level performance on LLM &amp; VLM. The RL scaling preview model is trained using verl, reaching OpenAI O1-level performance on math benchmarks (70.0 pass@1 on AIME).&lt;/li&gt;
  &lt;li&gt;[2025/03] We will present verl(HybridFlow) at EuroSys 2025. See you in Rotterdam!&lt;/li&gt;
  &lt;li&gt;[2025/02] We presented verl in the &lt;a href=&quot;https://lu.ma/ji7atxux&quot;&gt;Bytedance/NVIDIA/Anyscale Ray Meetup&lt;/a&gt;. See you in San Jose!&lt;/li&gt;
  &lt;li&gt;[2024/12] verl is presented at Ray Forward 2024. Slides available &lt;a href=&quot;https://github.com/eric-haibin-lin/verl-community/blob/main/slides/Ray_Forward_2024_%E5%B7%AB%E9%94%A1%E6%96%8C.pdf&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2024/10] verl is presented at Ray Summit. &lt;a href=&quot;https://www.youtube.com/watch?v=MrhMcXkXvJU&amp;list=PLzTswPQNepXntmT8jr9WaNfqQ60QwW7-U&amp;index=37&quot;&gt;Youtube video&lt;/a&gt; available.&lt;/li&gt;
  &lt;li&gt;[2024/12] The team presented &lt;a href=&quot;https://neurips.cc/Expo/Conferences/2024/workshop/100677&quot;&gt;Post-training LLMs: From Algorithms to Infrastructure&lt;/a&gt; at NeurIPS 2024. &lt;a href=&quot;https://github.com/eric-haibin-lin/verl-data/tree/neurips&quot;&gt;Slides&lt;/a&gt; and &lt;a href=&quot;https://neurips.cc/Expo/Conferences/2024/workshop/100677&quot;&gt;video&lt;/a&gt; available.&lt;/li&gt;
  &lt;li&gt;[2024/08] HybridFlow (verl) is accepted to EuroSys 2025.&lt;/li&gt;
&lt;/ul&gt;   
&lt;/details&gt;

## Key Features

- **FSDP** and **Megatron-LM** for training.
- **vLLM**, **SGLang** and **HF Transformers** for rollout generation.
- Compatible with Hugging Face Transformers and Modelscope Hub: Qwen-2.5, Llama3.1, Gemma2, DeepSeek-LLM, etc
- Supervised fine-tuning.
- Reinforcement learning with [PPO](examples/ppo_trainer/), [GRPO](examples/grpo_trainer/), [ReMax](examples/remax_trainer/), [REINFORCE++](https://verl.readthedocs.io/en/latest/examples/config.html#algorithm), [RLOO](examples/rloo_trainer/), [PRIME](recipe/prime/), [DAPO](recipe/dapo/), [DrGRPO](recipe/drgrpo), etc.
  - Support model-based reward and function-based reward (verifiable reward)
  - Support vision-language models (VLMs) and [multi-modal RL](examples/grpo_trainer/run_qwen2_5_vl-7b.sh)
- Flash attention 2, [sequence packing](examples/ppo_trainer/run_qwen2-7b_seq_balance.sh), [sequence parallelism](examples/ppo_trainer/run_deepseek7b_llm_sp2.sh) support via DeepSpeed Ulysses, [LoRA](examples/sft/gsm8k/run_qwen_05_peft.sh), [Liger-kernel](examples/sft/gsm8k/run_qwen_05_sp2_liger.sh).
- Scales up to 70B models and hundreds of GPUs.
- Experiment tracking with wandb, swanlab, mlflow and tensorboard.

## Upcoming Features and Changes

- Roadmap https://github.com/volcengine/verl/issues/710
- DeepSeek 671b optimizations with Megatron v0.11 https://github.com/volcengine/verl/issues/708
- Multi-turn rollout optimizations https://github.com/volcengine/verl/pull/1037 https://github.com/volcengine/verl/pull/1138
- Environment interactions https://github.com/volcengine/verl/issues/1172
- List of breaking changes since v0.3 https://github.com/volcengine/verl/discussions/943

## Getting Started

&lt;a href=&quot;https://verl.readthedocs.io/en/latest/index.html&quot;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt;

**Quickstart:**

- [Installation](https://verl.readthedocs.io/en/latest/start/install.html)
- [Quickstart](https://verl.readthedocs.io/en/latest/start/quickstart.html)
- [Programming Guide](https://verl.readthedocs.io/en/latest/hybrid_flow.html)

**Running a PPO example step-by-step:**

- Data and Reward Preparation
  - [Prepare Data for Post-Training](https://verl.readthedocs.io/en/latest/preparation/prepare_data.html)
  - [Implement Reward Function for Dataset](https://verl.readthedocs.io/en/latest/preparation/reward_function.html)
- Understanding the PPO Example
  - [PPO Example Architecture](https://verl.readthedocs.io/en/latest/examples/ppo_code_architecture.html)
  - [Config Explanation](https://verl.readthedocs.io/en/latest/examples/config.html)
  - [Run GSM8K Example](https://verl.readthedocs.io/en/latest/examples/gsm8k_example.html)

**Reproducible algorithm baselines:**

- [PPO, GRPO, ReMax](https://verl.readthedocs.io/en/latest/experiment/ppo.html)

**For code explanation and advance usage (extension):**

- PPO Trainer and Workers
  - [PPO Ray Trainer](https://verl.readthedocs.io/en/latest/workers/ray_trainer.html)
  - [PyTorch FSDP Backend](https://verl.readthedocs.io/en/latest/workers/fsdp_workers.html)
  - [Megatron-LM Backend](https://verl.readthedocs.io/en/latest/index.html)
- Advance Usage and Extension
  - [Ray API design tutorial](https://verl.readthedocs.io/en/latest/advance/placement.html)
  - [Extend to Other RL(HF) algorithms](https://verl.readthedocs.io/en/latest/advance/dpo_extension.html)
  - [Add Models with the FSDP Backend](https://verl.readthedocs.io/en/latest/advance/fsdp_extension.html)
  - [Add Models with the Megatron-LM Backend](https://verl.readthedocs.io/en/latest/advance/megatron_extension.html)
  - [Deployment using Separate GPU Resources](https://github.com/volcengine/verl/tree/main/examples/split_placement)

**Blogs from the community**

- [‰ΩøÁî® verl ËøõË°å GRPO ÂàÜÂ∏ÉÂºèÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÊúÄ‰Ω≥ÂÆûË∑µ](https://www.volcengine.com/docs/6459/1463942)
- [HybridFlow veRL ÂéüÊñáÊµÖÊûê](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/readme.md)
- [ÊúÄÈ´òÊèêÂçá 20 ÂÄçÂêûÂêêÈáèÔºÅË±ÜÂåÖÂ§ßÊ®°ÂûãÂõ¢ÈòüÂèëÂ∏ÉÂÖ®Êñ∞ RLHF Ê°ÜÊû∂ÔºåÁé∞Â∑≤ÂºÄÊ∫êÔºÅ](https://team.doubao.com/en/blog/%E6%9C%80%E9%AB%98%E6%8F%90%E5%8D%8720%E5%80%8D%E5%90%9E%E5%90%90%E9%87%8F-%E8%B1%86%E5%8C%85%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%A2%E9%98%9F%E5%8F%91%E5%B8%83%E5%85%A8%E6%96%B0-rlhf-%E6%A1%86%E6%9E%B6-%E7%8E%B0%E5%B7%B2%E5%BC%80%E6%BA%90)

## Performance Tuning Guide

The performance is essential for on-policy RL algorithm. We have written a detailed [performance tuning guide](https://verl.readthedocs.io/en/latest/perf/perf_tuning.html) to help you optimize performance.

## Upgrade to vLLM &gt;= v0.8.2

verl now supports vLLM&gt;=0.8.2 when using FSDP as the training backend. Please refer to [this document](https://github.com/volcengine/verl/blob/main/docs/README_vllm0.8.md) for the installation guide and more information. Please avoid vllm 0.7.x, which contains bugs that may lead to OOMs and unexpected errors.

## Use Latest SGLang

SGLang is fully supported with verl, and SGLang RL Group is working extensively on building unique features, including multi-turn agentic RL, VLM RLHF, server-based RL, and partial rollout. Please refer to [this document](https://verl.readthedocs.io/en/latest/workers/sglang_worker.html) for the installation guide and more information.

## [Hardware] Support AMD (ROCm Kernel)

verl now supports FSDP as the training engine (Megatron support coming soon) and both integrates with vLLM and SGLang as inference engines. Please refer to [this document](https://github.com/volcengine/verl/blob/main/docs/amd_tutorial/amd_build_dockerfile_page.rst) for the installation guide and more information, and [this document] (https://github.com/volcengine/verl/blob/main/docs/amd_tutorial/amd_vllm_page.rst) for the vLLM performance tuning for ROCm.

## Citation and acknowledgement

If you find the project helpful, please cite:

- [HybridFlow: A Flexible and Efficient RLHF Framework](https://arxiv.org/abs/2409.19256v2)
- [A Framework for Training Large Language Models for Code Generation via Proximal Policy Optimization](https://i.cs.hku.hk/~cwu/papers/gmsheng-NL2Code24.pdf)

```bibtex
@article{sheng2024hybridflow,
  title   = {HybridFlow: A Flexible and Efficient RLHF Framework},
  author  = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2409.19256}
}
```

verl is inspired by the design of Nemo-Aligner, Deepspeed-chat and OpenRLHF. The project is adopted and contributed by Bytedance, Anyscale, LMSys.org, [Alibaba Qwen team](https://github.com/QwenLM/), Shanghai AI Lab, Tsinghua University, UC Berkeley, UCLA, UIUC, University of Hong Kong, ke.com, [All Hands AI](https://www.all-hands.dev/), [ModelBest](http://modelbest.cn/), [OpenPipe](https://openpipe.ai/), JD AI Lab, Microsoft Research, [StepFun](https://www.stepfun.com/), Amazon, Linkedin, Meituan, [Camel-AI](https://www.camel-ai.org/), [OpenManus](https://github.com/OpenManus), Prime Intellect, NVIDIA research, [Baichuan](https://www.baichuan-ai.com/home), and many more.

## Awesome work using verl

- [TinyZero](https://github.com/Jiayi-Pan/TinyZero): a reproduction of **DeepSeek R1 Zero** recipe for reasoning tasks ![GitHub Repo stars](https://img.shields.io/github/stars/Jiayi-Pan/TinyZero)
- [DAPO](https://dapo-sia.github.io/): the fully open source SOTA RL algorithm that beats DeepSeek-R1-zero-32B ![GitHub Repo stars](https://img.shields.io/github/stars/volcengine/verl)
- [SkyThought](https://github.com/NovaSky-AI/SkyThought): RL training for Sky-T1-7B by NovaSky AI team. ![GitHub Repo stars](https://img.shields.io/github/stars/NovaSky-AI/SkyThought)
- [simpleRL-reason](https://github.com/hkust-nlp/simpleRL-reason): SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild ![GitHub Repo stars](https://img.shields.io/github/stars/hkust-nlp/simpleRL-reason)
- [Easy-R1](https://github.com/hiyouga/EasyR1): **Multi-modal** RL training framework ![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/EasyR1)
- [OpenManus-RL](https://github.com/OpenManus/OpenManus-RL): LLM Agents RL tunning framework for multiple agent environments. ![GitHub Repo stars](https://img.shields.io/github/stars/OpenManus/OpenManus-RL)
- [deepscaler](https://github.com/agentica-project/rllm/tree/deepscaler): iterative context scaling with GRPO ![GitHub Repo stars](https://img.shields.io/github/stars/agentica-project/deepscaler)
- [rllm](https://github.com/agentica-project/rllm): async RL training with [verl-pipeline](https://github.com/agentica-project/verl-pipeline) ![GitHub Repo stars](https://img.shields.io/github/stars/agentica-project/rllm)
- [PRIME](https://github.com/PRIME-RL/PRIME): Process reinforcement through implicit rewards ![GitHub Repo stars](https://img.shields.io/github/stars/PRIME-RL/PRIME)
- [RAGEN](https://github.com/ZihanWang314/ragen): a general-purpose reasoning **agent** training framework ![GitHub Repo stars](https://img.shields.io/github/stars/ZihanWang314/ragen)
- [Logic-RL](https://github.com/Unakar/Logic-RL): a reproduction of DeepSeek R1 Zero on 2K Tiny Logic Puzzle Dataset. ![GitHub Repo stars](https://img.shields.io/github/stars/Unakar/Logic-RL)
- [Search-R1](https://github.com/PeterGriffinJin/Search-R1): RL with reasoning and **searching (tool-call)** interleaved LLMs ![GitHub Repo stars](https://img.shields.io/github/stars/PeterGriffinJin/Search-R1)
- [ReSearch](https://github.com/Agent-RL/ReSearch): Learning to **Re**ason with **Search** for LLMs via Reinforcement Learning ![GitHub Repo stars](https://img.shields.io/github/stars/Agent-RL/ReSearch)
- [DeepRetrieval](https://github.com/pat-jj/DeepRetrieval): Hacking **Real Search Engines** and **retrievers** with LLMs via RL for **information retrieval** ![GitHub Repo stars](https://img.shields.io/github/stars/pat-jj/DeepRetrieval)
- [Code-R1](https://github.com/ganler/code-r1): Reproducing R1 for **Code** with Reliable Rewards ![GitHub Repo stars](https://img.shields.io/github/stars/ganler/code-r1)
- [Skywork-OR1](https://github.com/SkyworkAI/Skywork-OR1): Skywork open reaonser series ![GitHub Repo stars](https://img.shields.io/github/stars/SkyworkAI/Skywork-OR1)
- [ToRL](https://github.com/GAIR-NLP/ToRL): Scaling tool-integrated RL ![GitHub Repo stars](https://img.shields.io/github/stars/GAIR-NLP/ToRL)
- [cognitive-behaviors](https://github.com/kanishkg/cognitive-behaviors): Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs ![GitHub Repo stars](https://img.shields.io/github/stars/kanishkg/cognitive-behaviors)
- [PURE](https://github.com/CJReinforce/PURE): **Credit assignment** is the key to successful reinforcement fine-tuning using **process reward model** ![GitHub Repo stars](https://img.shields.io/github/stars/CJReinforce/PURE)
- [MetaSpatial](https://github.com/PzySeere/MetaSpatial): Reinforcing **3D Spatial Reasoning** in **VLMs** for the **Metaverse** ![GitHub Repo stars](https://img.shields.io/github/stars/PzySeere/MetaSpatial)
- [GUI-R1](https://github.com/ritzz-ai/GUI-R1): **GUI-R1**: A Generalist R1-style Vision-Language Action Model For **GUI Agents** ![GitHub Repo stars](https://img.shields.io/github/stars/ritzz-ai/GUI-R1)
- [DeepEnlighten](https://github.com/DolbyUUU/DeepEnlighten): Reproduce R1 with **social reasoning** tasks and analyze key findings ![GitHub Repo stars](https://img.shields.io/github/stars/DolbyUUU/DeepEnlighten)
- [DeepResearcher](https://github.com/GAIR-NLP/DeepResearcher): Scaling deep research via reinforcement learning in real-world environments ![GitHub Repo stars](https://img.shields.io/github/stars/GAIR-NLP/DeepResearcher)
- [self-rewarding-reasoning-LLM](https://arxiv.org/pdf/2502.19613): self-rewarding and correction with **generative reward models** ![GitHub Repo stars](https://img.shields.io/github/stars/RLHFlow/Self-rewarding-reasoning-LLM)
- [critic-rl](https://github.com/HKUNLP/critic-rl): LLM critics for code generation ![GitHub Repo stars](https://img.shields.io/github/stars/HKUNLP/critic-rl)
- [VAGEN](https://github.com/RAGEN-AI/VAGEN): Training VLM agents with multi-turn reinforcement learning ![GitHub Repo stars](https://img.shields.io/github/stars/RAGEN-AI/VAGEN)
- [AdaRFT](https://github.com/uscnlp-lime/verl): Efficient Reinforcement Finetuning via **Adaptive Curriculum Learning** ![GitHub Repo stars](https://img.shields.io/github/stars/uscnlp-lime/verl)
- [Trust Region Preference Approximation](https://github.com/XueruiSu/Trust-Region-Preference-Approximation): A simple and stable **reinforcement learning algorithm** for LLM reasoning. ![GitHub Repo stars](https://img.shields.io/github/stars/XueruiSu/Trust-Region-Preference-Approximation)
- [cognition-engineering](https://github.com/gair-nlp/cognition-engineering): Test time scaling drives cognition engineer

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[HumanSignal/labelImg]]></title>
            <link>https://github.com/HumanSignal/labelImg</link>
            <guid>https://github.com/HumanSignal/labelImg</guid>
            <pubDate>Wed, 30 Apr 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[LabelImg is now part of the Label Studio community. The popular image annotation tool created by Tzutalin is no longer actively being developed, but you can check out Label Studio, the open source data labeling tool for images, text, hypertext, audio, video and time-series data.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/HumanSignal/labelImg">HumanSignal/labelImg</a></h1>
            <p>LabelImg is now part of the Label Studio community. The popular image annotation tool created by Tzutalin is no longer actively being developed, but you can check out Label Studio, the open source data labeling tool for images, text, hypertext, audio, video and time-series data.</p>
            <p>Language: Python</p>
            <p>Stars: 23,622</p>
            <p>Forks: 6,433</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[getzep/graphiti]]></title>
            <link>https://github.com/getzep/graphiti</link>
            <guid>https://github.com/getzep/graphiti</guid>
            <pubDate>Wed, 30 Apr 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[Build Real-Time Knowledge Graphs for AI Agents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/getzep/graphiti">getzep/graphiti</a></h1>
            <p>Build Real-Time Knowledge Graphs for AI Agents</p>
            <p>Language: Python</p>
            <p>Stars: 7,400</p>
            <p>Forks: 495</p>
            <p>Stars today: 351 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.getzep.com/&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73&quot; width=&quot;150&quot; alt=&quot;Zep Logo&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt;
Graphiti
&lt;/h1&gt;
&lt;h2 align=&quot;center&quot;&gt; Build Real-Time Knowledge Graphs for AI Agents&lt;/h2&gt;
&lt;div align=&quot;center&quot;&gt;

[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)
[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)
[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)

![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)
[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)
[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)
[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&amp;label=Release&amp;color=limegreen)](https://github.com/getzep/graphiti/releases)

&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://trendshift.io/repositories/12986&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12986&quot; alt=&quot;getzep%2Fgraphiti | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;

:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_

&lt;br /&gt;

&gt; [!TIP]
&gt; Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.

Graphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.

Use Graphiti to:

- Integrate and maintain dynamic user interactions and business data.
- Facilitate state-based reasoning and task automation for agents.
- Query complex, evolving data with semantic, keyword, and graph-based search methods.

&lt;br /&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;images/graphiti-graph-intro.gif&quot; alt=&quot;Graphiti temporal walkthrough&quot; width=&quot;700px&quot;&gt;   
&lt;/p&gt;

&lt;br /&gt;

A knowledge graph is a network of interconnected facts, such as _&quot;Kendra loves Adidas shoes.&quot;_ Each fact is a &quot;triplet&quot; represented by two entities, or
nodes (&quot;Kendra&quot;, &quot;Adidas shoes&quot;), and their relationship, or edge (&quot;loves&quot;). Knowledge Graphs have been explored
extensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph
while handling changing relationships and maintaining historical context.

## Graphiti and Zep Memory

Graphiti powers the core of [Zep&#039;s memory layer](https://www.getzep.com) for AI Agents.

Using Graphiti, we&#039;ve demonstrated Zep is
the [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).

Read our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).

We&#039;re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2501.13956&quot;&gt;&lt;img src=&quot;images/arxiv-screenshot.png&quot; alt=&quot;Zep: A Temporal Knowledge Graph Architecture for Agent Memory&quot; width=&quot;700px&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

## Why Graphiti?

Traditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:

- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.
- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.
- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.
- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.
- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/images/graphiti-intro-slides-stock-2.gif&quot; alt=&quot;Graphiti structured + unstructured demo&quot; width=&quot;700px&quot;&gt;   
&lt;/p&gt;

## Graphiti vs. GraphRAG

| Aspect                     | GraphRAG                              | Graphiti                                         |
| -------------------------- | ------------------------------------- | ------------------------------------------------ |
| **Primary Use**            | Static document summarization         | Dynamic data management                          |
| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |
| **Knowledge Structure**    | Entity clusters &amp; community summaries | Episodic data, semantic entities, communities    |
| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |
| **Adaptability**           | Low                                   | High                                             |
| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |
| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |
| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |
| **Custom Entity Types**    | No                                    | Yes, customizable                                |
| **Scalability**            | Moderate                              | High, optimized for large datasets               |

Graphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.

## Installation

Requirements:

- Python 3.10 or higher
- Neo4j 5.26 or higher (serves as the embeddings storage backend)
- OpenAI API key (for LLM inference and embedding)

&gt; [!IMPORTANT]
&gt; Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).
&gt; Using other services may result in incorrect output schemas and ingestion failures. This is particularly
&gt; problematic when using smaller models.

Optional:

- Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)

&gt; [!TIP]
&gt; The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly
&gt; interface to manage Neo4j instances and databases.

```bash
pip install graphiti-core
```

or

```bash
poetry add graphiti-core
```

You can also install optional LLM providers as extras:

```bash
# Install with Anthropic support
pip install graphiti-core[anthropic]

# Install with Groq support
pip install graphiti-core[groq]

# Install with Google Gemini support
pip install graphiti-core[google-genai]

# Install with multiple providers
pip install graphiti-core[anthropic,groq,google-genai]
```

## Quick Start

&gt; [!IMPORTANT]
&gt; Graphiti uses OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your environment.
&gt; Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI
&gt; compatible APIs.

For a complete working example, see the [Quickstart Example](./examples/quickstart/README.md) in the examples directory. The quickstart demonstrates:

1. Connecting to a Neo4j database
2. Initializing Graphiti indices and constraints
3. Adding episodes to the graph (both text and structured JSON)
4. Searching for relationships (edges) using hybrid search
5. Reranking search results using graph distance
6. Searching for nodes using predefined search recipes

The example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.

## MCP Server

The `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti&#039;s knowledge graph capabilities through the MCP protocol.

Key features of the MCP server include:

- Episode management (add, retrieve, delete)
- Entity management and relationship handling
- Semantic and hybrid search capabilities
- Group management for organizing related data
- Graph maintenance operations

The MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.

For detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md).

## REST Service

The `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.

Please see the [server README](./server/README.md) for more information.

## Optional Environment Variables

In addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.
If you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables
must be set.

`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish
to enable Neo4j&#039;s parallel runtime feature for several of our search queries.
Note that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,
as such this feature is off by default.

## Using Graphiti with Azure OpenAI

Graphiti supports Azure OpenAI for both LLM inference and embeddings. To use Azure OpenAI, you&#039;ll need to configure both the LLM client and embedder with your Azure OpenAI credentials.

```python
from openai import AsyncAzureOpenAI
from graphiti_core import Graphiti
from graphiti_core.llm_client import OpenAIClient
from graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig
from graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient

# Azure OpenAI configuration
api_key = &quot;&lt;your-api-key&gt;&quot;
api_version = &quot;&lt;your-api-version&gt;&quot;
azure_endpoint = &quot;&lt;your-azure-endpoint&gt;&quot;

# Create Azure OpenAI client for LLM
azure_openai_client = AsyncAzureOpenAI(
    api_key=api_key,
    api_version=api_version,
    azure_endpoint=azure_endpoint
)

# Initialize Graphiti with Azure OpenAI clients
graphiti = Graphiti(
    &quot;bolt://localhost:7687&quot;,
    &quot;neo4j&quot;,
    &quot;password&quot;,
    llm_client=OpenAIClient(
        client=azure_openai_client
    ),
    embedder=OpenAIEmbedder(
        config=OpenAIEmbedderConfig(
            embedding_model=&quot;text-embedding-3-small&quot;  # Use your Azure deployed embedding model name
        ),
        client=azure_openai_client
    ),
    # Optional: Configure the OpenAI cross encoder with Azure OpenAI
    cross_encoder=OpenAIRerankerClient(
        client=azure_openai_client
    )
)

# Now you can use Graphiti with Azure OpenAI
```

Make sure to replace the placeholder values with your actual Azure OpenAI credentials and specify the correct embedding model name that&#039;s deployed in your Azure OpenAI service.

## Using Graphiti with Google Gemini

Graphiti supports Google&#039;s Gemini models for both LLM inference and embeddings. To use Gemini, you&#039;ll need to configure both the LLM client and embedder with your Google API key.

Install Graphiti:

```bash
poetry add &quot;graphiti-core[google-genai]&quot;

# or

uv add &quot;graphiti-core[google-genai]&quot;
```

```python
from graphiti_core import Graphiti
from graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig
from graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig

# Google API key configuration
api_key = &quot;&lt;your-google-api-key&gt;&quot;

# Initialize Graphiti with Gemini clients
graphiti = Graphiti(
    &quot;bolt://localhost:7687&quot;,
    &quot;neo4j&quot;,
    &quot;password&quot;,
    llm_client=GeminiClient(
        config=LLMConfig(
            api_key=api_key,
            model=&quot;gemini-2.0-flash&quot;
        )
    ),
    embedder=GeminiEmbedder(
        config=GeminiEmbedderConfig(
            api_key=api_key,
            embedding_model=&quot;embedding-001&quot;
        )
    )
)

# Now you can use Graphiti with Google Gemini
```

## Documentation

- [Guides and API documentation](https://help.getzep.com/graphiti).
- [Quick Start](https://help.getzep.com/graphiti/graphiti/quick-start)
- [Building an agent with LangChain&#039;s LangGraph and Graphiti](https://help.getzep.com/graphiti/graphiti/lang-graph-agent)

## Status and Roadmap

Graphiti is under active development. We aim to maintain API stability while working on:

- [x] Supporting custom graph schemas:
  - Allow developers to provide their own defined node and edge classes when ingesting episodes
  - Enable more flexible knowledge representation tailored to specific use cases
- [x] Enhancing retrieval capabilities with more robust and configurable options
- [x] Graphiti MCP Server
- [ ] Expanding test coverage to ensure reliability and catch edge cases

## Contributing

We encourage and appreciate all forms of contributions, whether it&#039;s code, documentation, addressing GitHub Issues, or
answering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer
to [CONTRIBUTING](CONTRIBUTING.md).

## Support

Join the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[myhhub/stock]]></title>
            <link>https://github.com/myhhub/stock</link>
            <guid>https://github.com/myhhub/stock</guid>
            <pubDate>Wed, 30 Apr 2025 00:04:20 GMT</pubDate>
            <description><![CDATA[stockËÇ°Á•®.Ëé∑ÂèñËÇ°Á•®Êï∞ÊçÆ,ËÆ°ÁÆóËÇ°Á•®ÊåáÊ†á,Á≠πÁ†ÅÂàÜÂ∏É,ËØÜÂà´ËÇ°Á•®ÂΩ¢ÊÄÅ,ÁªºÂêàÈÄâËÇ°,ÈÄâËÇ°Á≠ñÁï•,ËÇ°Á•®È™åËØÅÂõûÊµã,ËÇ°Á•®Ëá™Âä®‰∫§Êòì,ÊîØÊåÅPCÂèäÁßªÂä®ËÆæÂ§á„ÄÇ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/myhhub/stock">myhhub/stock</a></h1>
            <p>stockËÇ°Á•®.Ëé∑ÂèñËÇ°Á•®Êï∞ÊçÆ,ËÆ°ÁÆóËÇ°Á•®ÊåáÊ†á,Á≠πÁ†ÅÂàÜÂ∏É,ËØÜÂà´ËÇ°Á•®ÂΩ¢ÊÄÅ,ÁªºÂêàÈÄâËÇ°,ÈÄâËÇ°Á≠ñÁï•,ËÇ°Á•®È™åËØÅÂõûÊµã,ËÇ°Á•®Ëá™Âä®‰∫§Êòì,ÊîØÊåÅPCÂèäÁßªÂä®ËÆæÂ§á„ÄÇ</p>
            <p>Language: Python</p>
            <p>Stars: 8,397</p>
            <p>Forks: 1,671</p>
            <p>Stars today: 49 stars today</p>
            <h2>README</h2><pre>**InStockËÇ°Á•®Á≥ªÁªü**

InStockËÇ°Á•®Á≥ªÁªüÔºåÊäìÂèñÊØèÊó•ËÇ°Á•®„ÄÅETFÂÖ≥ÈîÆÊï∞ÊçÆÔºåËÆ°ÁÆóËÇ°Á•®ÊäÄÊúØÊåáÊ†á„ÄÅÁ≠πÁ†ÅÂàÜÂ∏ÉÔºåËØÜÂà´KÁ∫øÂêÑÁßçÂΩ¢ÊÄÅÔºåÁªºÂêàÈÄâËÇ°ÔºåÂÜÖÁΩÆÂ§öÁßçÈÄâËÇ°Á≠ñÁï•ÔºåÊîØÊåÅÈÄâËÇ°È™åËØÅÂõûÊµãÔºåÊîØÊåÅËá™Âä®‰∫§ÊòìÔºåÊîØÊåÅÊâπÈáèÊó∂Èó¥ÔºåËøêË°åÈ´òÊïàÔºåÊîØÊåÅPC„ÄÅÂπ≥Êùø„ÄÅÊâãÊú∫ÁßªÂä®ËÆæÂ§áÊòæÁ§∫ÔºåÂêåÊó∂Êèê‰æõDockerÈïúÂÉèÊñπ‰æøÂÆâË£ÖÔºåÊòØÈáèÂåñÊäïËµÑÁöÑÂ•ΩÂ∏ÆÊâã„ÄÇ

The stock system,Capture key data on daily stocks and ETFs, calculate stock technical indicators, chip distribution, Position Cost Distribution(CYQ), identify various K-line forms, comprehensive stock selection, built-in multiple stock selection strategies, support stock selection verification and backtesting, support automatic trading, and support batch time , runs efficiently, supports display on PCs, tablets, and mobile phones, and provides Docker images for easy installation, making it a good helper for quantitative investment.

DockerÈïúÂÉèÔºöhttps://hub.docker.com/r/mayanghua/instock **ÈïúÂÉè‰ºòÂåñÊûÑÂª∫‰ªÖ170M**„ÄÇ

# ÂäüËÉΩ‰ªãÁªç

##  ‰∏ÄÔºöÁªºÂêàÈÄâËÇ°
ÁªºÂêàÈÄâËÇ°ÊîØÊåÅËÇ°Á•®ËåÉÂõ¥„ÄÅÂü∫Êú¨Èù¢„ÄÅÊäÄÊúØÈù¢„ÄÅÊ∂àÊÅØÈù¢„ÄÅ‰∫∫Ê∞îÊåáÊ†á„ÄÅË°åÊÉÖÊï∞ÊçÆÁ≠âÊñπÈù¢ÂÖ±200Â§ö‰∏™‰ø°ÊÅØÊ†èÁõÆËøõË°åËá™Áî±ÁªÑÂêàÈÄâËÇ°„ÄÇÈÄâËÇ°Êù°‰ª∂ÂàÜ‰∏∫‰ª•‰∏ãÂ§ßÁ±ªÔºö
```
1.ËÇ°Á•®ËåÉÂõ¥
Â∏ÇÂú∫„ÄÅ Ë°å‰∏ö„ÄÅÂú∞Âå∫„ÄÅ Ê¶ÇÂøµ„ÄÅ È£éÊ†º„ÄÅÊåáÊï∞Êàê‰ªΩ„ÄÅ ‰∏äÂ∏ÇÊó∂Èó¥„ÄÇ
2.Âü∫Êú¨Èù¢
‰º∞ÂÄºÊåáÊ†á„ÄÅÊØèËÇ°ÊåáÊ†á„ÄÅÁõàÂà©ËÉΩÂäõ„ÄÅÊàêÈïøËÉΩÂäõ„ÄÅËµÑÊú¨ÁªìÊûÑ‰∏éÂÅøÂÄ∫ËÉΩÂäõ„ÄÅËÇ°Êú¨ËÇ°‰∏ú„ÄÇ
3.ÊäÄÊúØÈù¢
MACDÈáëÂèâ„ÄÅKDJÈáëÂèâ„ÄÅÊîæÈáèÁ™ÅÁ†¥„ÄÅ‰Ωé‰ΩçËµÑÈáëÂáÄÊµÅÂÖ•„ÄÅÈ´ò‰ΩçËµÑÈáëÂáÄÊµÅÂá∫„ÄÅÂêë‰∏äÁ™ÅÁ†¥ÂùáÁ∫ø„ÄÅÂùáÁ∫øÂ§öÂ§¥ÊéíÂàó„ÄÅÂùáÁ∫øÁ©∫Â§¥ÊéíÂàó„ÄÅËøûÊ∂®ÊîæÈáè„ÄÅ‰∏ãË∑åÊó†Èáè„ÄÅ‰∏ÄÊ†πÂ§ßÈò≥Á∫ø„ÄÅ‰∏§Ê†πÂ§ßÈò≥Á∫ø„ÄÅÊó≠Êó•‰∏úÂçá„ÄÅÂº∫ÂäøÂ§öÊñπ„ÄÅÁÇÆÊã®‰∫ëËßÅÊó•„ÄÅ‰∏É‰ªôÂ•≥‰∏ãÂá°(‰∏ÉËøûÈò¥)„ÄÅÂÖ´‰ªôËøáÊµ∑(ÂÖ´ËøûÈò≥)„ÄÅ‰πùÈò≥Á•ûÂäü(‰πùËøûÈò≥)„ÄÅÂõõ‰∏≤Èò≥„ÄÅÂ§©ÈáèÊ≥ïÂàô„ÄÅÊîæÈáè‰∏äÊîª„ÄÅÁ©øÂ§¥Á†¥ËÑö„ÄÅÂÄíËΩ¨Èî§Â§¥„ÄÅÂ∞ÑÂáª‰πãÊòü„ÄÅÈªÑÊòè‰πãÊòü„ÄÅÊõôÂÖâÂàùÁé∞„ÄÅË∫´ÊÄÄÂÖ≠Áî≤„ÄÅ‰πå‰∫ëÁõñÈ°∂„ÄÅÊó©Êô®‰πãÊòü„ÄÅÁ™ÑÂπÖÊï¥ÁêÜ„ÄÇ
4.Ê∂àÊÅØÈù¢
ÂÖ¨ÂëäÂ§ß‰∫ã„ÄÅÊú∫ÊûÑÂÖ≥Ê≥®ÊÉÖÂÜµ„ÄÅÊú∫ÊûÑÊåÅËÇ°ÂÆ∂Êï∞„ÄÅÊú∫ÊûÑÊåÅËÇ°ÊØî‰æã„ÄÇ
5.‰∫∫Ê∞îÊåáÊ†á
ËÇ°Âêß‰∫∫Ê∞îÊéíÂêç„ÄÅ‰∫∫Ê∞îÊéíÂêçÂèòÂåñ„ÄÅ‰∫∫Ê∞îÊéíÂêçËøûÊ∂®„ÄÅ‰∫∫Ê∞îÊéíÂêçËøûË∑å„ÄÅ‰∫∫Ê∞îÊéíÂêçÂàõÊñ∞È´ò„ÄÅ‰∫∫Ê∞îÊéíÂêçÂàõÊñ∞‰Ωé„ÄÅÊñ∞ÊôãÁ≤â‰∏ùÂç†ÊØî„ÄÅÈìÅÊùÜÁ≤â‰∏ùÂç†ÊØî„ÄÅ7Êó•ÂÖ≥Ê≥®ÊéíÂêç„ÄÅ‰ªäÊó•ÊµèËßàÊéíÂêç„ÄÇ
6.Ë°åÊÉÖÊï∞ÊçÆ
ËÇ°‰ª∑Ë°®Áé∞„ÄÅÊàê‰∫§ÊÉÖÂÜµ„ÄÅËµÑÈáëÊµÅÂêë„ÄÅË°åÊÉÖÁªüËÆ°„ÄÅÊ≤™Ê∑±ËÇ°ÈÄö„ÄÇ
```
![](img/a3.jpg)
![](img/a1.jpg)

##  ‰∫åÔºöËÇ°Á•®ÊØèÊó•Êï∞ÊçÆ

ÂåÖÊã¨ÊØèÊó•ËÇ°Á•®Êï∞ÊçÆ„ÄÅËÇ°Á•®ËµÑÈáëÊµÅÂêë„ÄÅËÇ°Á•®ÂàÜÁ∫¢ÈÖçÈÄÅ„ÄÅËÇ°Á•®ÈæôËôéÊ¶ú„ÄÅËÇ°Á•®Â§ßÂÆó‰∫§Êòì„ÄÅËÇ°Á•®Âü∫Êú¨Èù¢Êï∞ÊçÆ„ÄÅË°å‰∏öËµÑÈáëÊµÅÂêë„ÄÅÊ¶ÇÂøµËµÑÈáëÊµÅÂêë„ÄÅÊØèÊó•ETFÊï∞ÊçÆ„ÄÇ

ÊäìÂèñAËÇ°Á•®ÊØèÊó•Êï∞ÊçÆÔºå‰∏ªË¶Å‰∏∫‰∏Ä‰∫õÂÖ≥ÈîÆÊï∞ÊçÆÔºåÂêåÊó∂Â∞ÅË£ÖÊäìÂèñÊñπÊ≥ïÔºåÊñπ‰æøÊâ©Â±ïÁ≥ªÁªüËé∑Âèñ‰∏™‰∫∫ÂÖ≥Ê≥®ÁöÑÊï∞ÊçÆ„ÄÇ

![](img/00.jpg)
![](img/12.jpg)
## ‰∏âÔºöËÇ°Á•®ÊåáÊ†áËÆ°ÁÆó
Âü∫‰∫étalib„ÄÅpandas ËÆ°ÁÆóÊåáÊ†áÔºåËÆ°ÁÆóÈ´òÊïàÂáÜÁ°Æ„ÄÇË∞ÉÊï¥‰∏™Âà´ÊåáÊ†áÂÖ¨ÂºèÔºåÁ°Æ‰øùÁªìÊûúÂíåÂêåËä±È°∫„ÄÅÈÄö‰ø°ËææÁªìÊûú‰∏ÄËá¥„ÄÇ
ÊåáÊ†áÔºö

```
1„ÄÅMACD 2„ÄÅKDJ 3„ÄÅBOLL 4„ÄÅTRIXÔºåTRMA 5„ÄÅCR 6„ÄÅSMA 7„ÄÅRSI 
8„ÄÅVRÔºåMAVR 9„ÄÅROC 10„ÄÅDMIÔºå+DIÔºå-DIÔºåDXÔºåADXÔºåADXR 11„ÄÅW&amp;R 
12„ÄÅCCI 13„ÄÅTR„ÄÅATR 14„ÄÅDMA„ÄÅAMA 15„ÄÅOBV 16„ÄÅSAR 17„ÄÅPSY 
18„ÄÅBRAR 19„ÄÅEMV 20„ÄÅBIAS 21„ÄÅTEMA  22„ÄÅMFI 23„ÄÅVWMA
24„ÄÅPPO 25„ÄÅWT 26„ÄÅSupertrend  27„ÄÅDPO  28„ÄÅVHF  29„ÄÅRVI
30„ÄÅFI 31„ÄÅENE 32„ÄÅSTOCHRSI
```

![](img/01.jpg)
![](img/06.jpg)

## ÂõõÔºöÂà§Êñ≠‰π∞ÂÖ•ÂçñÂá∫ÁöÑËÇ°Á•®

Ê†πÊçÆÊåáÊ†áÂà§ÂÆöÂèØËÉΩ‰π∞ÂÖ•ÂçñÂá∫ÁöÑËÇ°Á•®ÔºåÂÖ∑‰ΩìÁ≠õÈÄâÊù°‰ª∂Â¶Ç‰∏ãÔºö


```
KDJ:
1„ÄÅË∂Ö‰π∞Âå∫ÔºöKÂÄºÂú®80‰ª•‰∏äÔºåDÂÄºÂú®70‰ª•‰∏äÔºåJÂÄºÂ§ß‰∫é90Êó∂‰∏∫Ë∂Ö‰π∞„ÄÇ‰∏ÄËà¨ÊÉÖÂÜµ‰∏ãÔºåËÇ°‰ª∑ÊúâÂèØËÉΩ‰∏ãË∑å„ÄÇÊäïËµÑËÄÖÂ∫îË∞®ÊÖéË°å‰∫ãÔºåÂ±ÄÂ§ñ‰∫∫‰∏çÂ∫îÂÜçËøΩÊ∂®ÔºåÂ±ÄÂÜÖ‰∫∫Â∫îÈÄÇÊó∂ÂçñÂá∫„ÄÇ
2„ÄÅË∂ÖÂçñÂå∫ÔºöKÂÄºÂú®20‰ª•‰∏ãÔºåDÂÄºÂú®30‰ª•‰∏ã‰∏∫Ë∂ÖÂçñÂå∫„ÄÇ‰∏ÄËà¨ÊÉÖÂÜµ‰∏ãÔºåËÇ°‰ª∑ÊúâÂèØËÉΩ‰∏äÊ∂®ÔºåÂèçÂºπÁöÑÂèØËÉΩÊÄßÂ¢ûÂ§ß„ÄÇÂ±ÄÂÜÖ‰∫∫‰∏çÂ∫îËΩªÊòìÊäõÂá∫ËÇ°Á•®ÔºåÂ±ÄÂ§ñ‰∫∫ÂèØÂØªÊú∫ÂÖ•Âú∫„ÄÇ
RSI:
1„ÄÅÂΩìÂÖ≠Êó•ÊåáÊ†á‰∏äÂçáÂà∞Ëææ80Êó∂ÔºåË°®Á§∫ËÇ°Â∏ÇÂ∑≤ÊúâË∂Ö‰π∞Áé∞Ë±°ÔºåÂ¶ÇÊûú‰∏ÄÊó¶ÁªßÁª≠‰∏äÂçáÔºåË∂ÖËøá90‰ª•‰∏äÊó∂ÔºåÂàôË°®Á§∫Â∑≤Âà∞‰∏•ÈáçË∂Ö‰π∞ÁöÑË≠¶ÊàíÂå∫ÔºåËÇ°‰ª∑Â∑≤ÂΩ¢ÊàêÂ§¥ÈÉ®ÔºåÊûÅÂèØËÉΩÂú®Áü≠ÊúüÂÜÖÂèçËΩ¨ÂõûËΩ¨„ÄÇ
2„ÄÅÂΩìÂÖ≠Êó•Âº∫Âº±ÊåáÊ†á‰∏ãÈôçËá≥20Êó∂ÔºåË°®Á§∫ËÇ°Â∏ÇÊúâË∂ÖÂçñÁé∞Ë±°ÔºåÂ¶ÇÊûú‰∏ÄÊó¶ÁªßÁª≠‰∏ãÈôçËá≥10‰ª•‰∏ãÊó∂ÂàôË°®Á§∫Â∑≤Âà∞‰∏•ÈáçË∂ÖÂçñÂå∫ÂüüÔºåËÇ°‰ª∑ÊûÅÂèØËÉΩÊúâÊ≠¢Ë∑åÂõûÂçáÁöÑÊú∫‰ºö„ÄÇ
CCI:
1„ÄÅÂΩìCCIÔºûÔπ¢100Êó∂ÔºåË°®ÊòéËÇ°‰ª∑Â∑≤ÁªèËøõÂÖ•ÈùûÂ∏∏ÊÄÅÂå∫Èó¥‚Äî‚ÄîË∂Ö‰π∞Âå∫Èó¥ÔºåËÇ°‰ª∑ÁöÑÂºÇÂä®Áé∞Ë±°Â∫îÂ§öÂä†ÂÖ≥Ê≥®„ÄÇ
2„ÄÅÂΩìCCIÔºúÔπ£100Êó∂ÔºåË°®ÊòéËÇ°‰ª∑Â∑≤ÁªèËøõÂÖ•Âè¶‰∏Ä‰∏™ÈùûÂ∏∏ÊÄÅÂå∫Èó¥‚Äî‚ÄîË∂ÖÂçñÂå∫Èó¥ÔºåÊäïËµÑËÄÖÂèØ‰ª•ÈÄ¢‰ΩéÂê∏Á∫≥ËÇ°Á•®„ÄÇ
CR:
1„ÄÅË∑åÁ©øa„ÄÅb„ÄÅc„ÄÅdÂõõÊù°Á∫øÔºåÂÜçÁî±‰ΩéÁÇπÂêë‰∏äÁà¨Âçá160Êó∂Ôºå‰∏∫Áü≠Á∫øËé∑Âà©ÁöÑ‰∏Ä‰∏™ËâØÊú∫ÔºåÂ∫îÈÄÇÂΩìÂçñÂá∫ËÇ°Á•®„ÄÇ
2„ÄÅCRË∑åËá≥40‰ª•‰∏ãÊó∂ÔºåÊòØÂª∫‰ªìËâØÊú∫„ÄÇ
WR:
1„ÄÅÂΩìÔºÖRÁ∫øËææÂà∞20Êó∂ÔºåÂ∏ÇÂú∫Â§Ñ‰∫éË∂Ö‰π∞Áä∂ÂÜµÔºåËµ∞ÂäøÂèØËÉΩÂç≥Â∞ÜËßÅÈ°∂„ÄÇ
2„ÄÅÂΩìÔºÖRÁ∫øËææÂà∞80Êó∂ÔºåÂ∏ÇÂú∫Â§Ñ‰∫éË∂ÖÂçñÁä∂ÂÜµÔºåËÇ°‰ª∑Ëµ∞ÂäøÈöèÊó∂ÂèØËÉΩËßÅÂ∫ï„ÄÇ
VR:
1„ÄÅËé∑Âà©Âå∫Âüü160Ôºç450Ê†πÊçÆÊÉÖÂÜµËé∑Âà©‰∫ÜÁªì„ÄÇ
2„ÄÅ‰Ωé‰ª∑Âå∫Âüü40Ôºç70ÂèØ‰ª•‰π∞Ëøõ„ÄÇ
```

![](img/05.jpg)

## ‰∫îÔºöKÁ∫øÂΩ¢ÊÄÅËØÜÂà´

Á≤æÂáÜËØÜÂà´61ÁßçKÁ∫øÂΩ¢ÊÄÅÔºåÊîØÊåÅÁî®Êà∑Ëá™ÈÄâÂΩ¢ÊÄÅËØÜÂà´„ÄÇ

ËØÜÂà´ÂΩ¢ÊÄÅ:

```
1„ÄÅ‰∏§Âè™‰πåÈ∏¶2„ÄÅ‰∏âÂè™‰πåÈ∏¶3„ÄÅ‰∏âÂÜÖÈÉ®‰∏äÊ∂®Âíå‰∏ãË∑å4„ÄÅ‰∏âÁ∫øÊâìÂáª5„ÄÅ‰∏âÂ§ñÈÉ®‰∏äÊ∂®Âíå‰∏ãË∑å6„ÄÅÂçóÊñπ‰∏âÊòü7„ÄÅ‰∏â‰∏™ÁôΩÂÖµ8„ÄÅÂºÉÂ©¥
9„ÄÅÂ§ßÊïåÂΩìÂâç10„ÄÅÊçâËÖ∞Â∏¶Á∫ø11„ÄÅËÑ±Á¶ª12„ÄÅÊî∂ÁõòÁº∫ÂΩ±Á∫ø13„ÄÅËóèÂ©¥ÂêûÊ≤°14„ÄÅÂèçÂáªÁ∫ø15„ÄÅ‰πå‰∫ëÂéãÈ°∂16„ÄÅÂçÅÂ≠ó17„ÄÅÂçÅÂ≠óÊòü
18„ÄÅËúªËúìÂçÅÂ≠ó/TÂΩ¢ÂçÅÂ≠ó19„ÄÅÂêûÂô¨Ê®°Âºè20„ÄÅÂçÅÂ≠óÊöÆÊòü  21„ÄÅÊöÆÊòü22„ÄÅÂêë‰∏ä/‰∏ãË∑≥Á©∫Âπ∂ÂàóÈò≥Á∫ø23„ÄÅÂ¢ìÁ¢ëÂçÅÂ≠ó/ÂÄíTÂçÅÂ≠ó
24„ÄÅÈî§Â§¥25„ÄÅ‰∏äÂêäÁ∫ø26„ÄÅÊØçÂ≠êÁ∫ø27„ÄÅÂçÅÂ≠óÂ≠ïÁ∫ø28„ÄÅÈ£éÈ´òÊµ™Â§ßÁ∫ø29„ÄÅÈô∑Èò±30„ÄÅ‰øÆÊ≠£Èô∑Èò±31„ÄÅÂÆ∂È∏Ω32„ÄÅ‰∏âËÉûËÉé‰πåÈ∏¶
33„ÄÅÈ¢àÂÜÖÁ∫ø34„ÄÅÂÄíÈî§Â§¥35„ÄÅÂèçÂÜ≤ÂΩ¢ÊÄÅ36„ÄÅÁî±ËæÉÈïøÁº∫ÂΩ±Á∫øÂÜ≥ÂÆöÁöÑÂèçÂÜ≤ÂΩ¢ÊÄÅ37„ÄÅÊ¢ØÂ∫ï38„ÄÅÈïøËÑöÂçÅÂ≠ó39„ÄÅÈïøËú°ÁÉõ
40„ÄÅÂÖâÂ§¥ÂÖâËÑö/Áº∫ÂΩ±Á∫ø 41„ÄÅÁõ∏Âêå‰Ωé‰ª∑42„ÄÅÈì∫Âû´43„ÄÅÂçÅÂ≠óÊô®Êòü44„ÄÅÊô®Êòü45„ÄÅÈ¢à‰∏äÁ∫ø46„ÄÅÂà∫ÈÄèÂΩ¢ÊÄÅ47„ÄÅÈªÑÂåÖËΩ¶Â§´
48„ÄÅ‰∏äÂçá/‰∏ãÈôç‰∏âÊ≥ï49„ÄÅÂàÜÁ¶ªÁ∫ø50„ÄÅÂ∞ÑÂáª‰πãÊòü51„ÄÅÁü≠Ëú°ÁÉõ52„ÄÅÁ∫∫Èî§53„ÄÅÂÅúÈ°øÂΩ¢ÊÄÅ54„ÄÅÊù°ÂΩ¢‰∏âÊòéÊ≤ª55„ÄÅÊé¢Ê∞¥Á´ø
56„ÄÅË∑≥Á©∫Âπ∂ÂàóÈò¥Èò≥Á∫ø57„ÄÅÊèíÂÖ•58„ÄÅ‰∏âÊòü59„ÄÅÂ•áÁâπ‰∏âÊ≤≥Â∫ä60„ÄÅÂêë‰∏äË∑≥Á©∫ÁöÑ‰∏§Âè™‰πåÈ∏¶61„ÄÅ‰∏äÂçá/‰∏ãÈôçË∑≥Á©∫‰∏âÊ≥ï 
```
ÂΩ¢ÊÄÅËØÜÂà´ÁªìÊûúÔºö
```
Ë¥üÔºöÂá∫Áé∞ÂçñÂá∫‰ø°Âè∑
0ÔºöÊ≤°ÊúâÂá∫Áé∞ËØ•ÂΩ¢ÊÄÅ
Ê≠£ÔºöÂá∫Áé∞‰π∞ÂÖ•‰ø°Âè∑
```
![](img/09.jpg)
![](img/13.jpg)

## ÂÖ≠ÔºöÁ≠πÁ†ÅÂàÜÂ∏É

Á≠πÁ†ÅÂàÜÂ∏ÉÈÄöËøáËÆ°ÁÆó‰∏ÄÂÆöÊó∂Èó¥ËåÉÂõ¥ÂÜÖËÇ°Á•®ÁöÑ:ÊúÄÈ´ò‰ª∑„ÄÅÊúÄ‰Ωé‰ª∑„ÄÅÊàê‰∫§Êï∞ÔºåËæìÂá∫ÂØπÂ∫î‰ª∑Ê†ºÊàê‰∫§Êï∞Âç†Êï¥‰∏™ÊµÅÈÄöÁõòÊØîÂÄºÁöÑÂàÜÂ∏ÉÂõæÂΩ¢„ÄÇËÆ°ÁÆóÈ´òÊïàÂáÜÁ°ÆÔºåÁªìÊûú‰∏é‰∏úÊñπË¥¢ÂØåÁ≠â‰∏ì‰∏öËΩØ‰ª∂ÁöÑ‰∏ÄËá¥ÔºåÁº∫ÁúÅËÆ°ÁÆó210‰∏™‰∫§ÊòìÊó•ÁöÑÊàêÊú¨ÔºåÂèØ‰ª•Ëá™Ë°åËÆæÂÆöÊó∂Èó¥ËåÉÂõ¥„ÄÇ
![](img/06.jpg)

## ‰∏ÉÔºöÁ≠ñÁï•ÈÄâËÇ°

ÂÜÖÁΩÆÊîæÈáè‰∏äÊ∂®„ÄÅÂÅúÊú∫Âù™„ÄÅÂõûË∏©Âπ¥Á∫ø„ÄÅÁ™ÅÁ†¥Âπ≥Âè∞„ÄÅÊîæÈáèË∑åÂÅúÁ≠âÂ§öÁßçÈÄâËÇ°Á≠ñÁï•ÔºåÂêåÊó∂Â∞ÅË£Ö‰∫ÜÁ≠ñÁï•Ê®°ÊùøÔºåÊñπ‰æøÊâ©Â±ïÂÆûÁé∞Ëá™Â∑±ÁöÑÁ≠ñÁï•„ÄÇ


```
1„ÄÅÊîæÈáè‰∏äÊ∂®
    1ÔºâÂΩìÊó•ÊØîÂâç‰∏ÄÂ§©‰∏äÊ∂®Â∞è‰∫é2%ÊàñÊî∂Áõò‰ª∑Â∞è‰∫éÂºÄÁõò‰ª∑„ÄÇ
    2ÔºâÂΩìÊó•Êàê‰∫§È¢ù‰∏ç‰Ωé‰∫é2‰∫ø„ÄÇ
    3ÔºâÂΩìÊó•Êàê‰∫§Èáè/5Êó•Âπ≥ÂùáÊàê‰∫§Èáè&gt;=2„ÄÇ
2„ÄÅÂùáÁ∫øÂ§öÂ§¥
    MA30Âêë‰∏ä
    1Ôºâ30Êó•ÂâçÁöÑ30Êó•ÂùáÁ∫ø&lt;20Êó•ÂâçÁöÑ30Êó•ÂùáÁ∫ø&lt;10Êó•ÂâçÁöÑ30Êó•ÂùáÁ∫ø&lt;ÂΩìÊó•ÁöÑ30Êó•ÂùáÁ∫ø„ÄÇ
    2Ôºâ(ÂΩìÊó•ÁöÑ30Êó•ÂùáÁ∫ø/30Êó•ÂâçÁöÑ30Êó•ÂùáÁ∫ø)&gt;1.2„ÄÇ
3„ÄÅÂÅúÊú∫Âù™
    1ÔºâÊúÄËøë15Êó•ÊúâÊ∂®ÂπÖÂ§ß‰∫é9.5%Ôºå‰∏îÂøÖÈ°ªÊòØÊîæÈáè‰∏äÊ∂®„ÄÇ
    2ÔºâÁ¥ßÊé•ÁöÑ‰∏ã‰∏™‰∫§ÊòìÊó•ÂøÖÈ°ªÈ´òÂºÄÔºåÊî∂Áõò‰ª∑ÂøÖÈ°ª‰∏äÊ∂®Ôºå‰∏î‰∏éÂºÄÁõò‰ª∑‰∏çËÉΩÂ§ß‰∫éÁ≠â‰∫éÁõ∏Â∑Æ3%„ÄÇ
    3ÔºâÊé•‰∏ã2„ÄÅ3‰∏™‰∫§ÊòìÊó•ÂøÖÈ°ªÈ´òÂºÄÔºåÊî∂Áõò‰ª∑ÂøÖÈ°ª‰∏äÊ∂®Ôºå‰∏î‰∏éÂºÄÁõò‰ª∑‰∏çËÉΩÂ§ß‰∫éÁ≠â‰∫éÁõ∏Â∑Æ3%Ôºå‰∏îÊØèÂ§©Ê∂®Ë∑åÂπÖÂú®5%Èó¥„ÄÇ
4„ÄÅÂõûË∏©Âπ¥Á∫ø
    1ÔºâÂàÜ2‰∏™Êó∂Èó¥ÊÆµÔºöÂâçÊÆµ=ÊúÄËøë60‰∫§ÊòìÊó•ÊúÄÈ´òÊî∂Áõò‰ª∑‰πãÂâç‰∫§ÊòìÊó•(ÈïøÂ∫¶&gt;0)ÔºåÂêéÊÆµ=ÊúÄÈ´ò‰ª∑ÂΩìÊó•ÂèäÂêéÈù¢ÁöÑ‰∫§ÊòìÊó•„ÄÇ
    2ÔºâÂâçÊÆµÁî±Âπ¥Á∫ø(250Êó•)‰ª•‰∏ãÂêë‰∏äÁ™ÅÁ†¥„ÄÇ
    3ÔºâÂêéÊÆµÂøÖÈ°ªÂú®Âπ¥Á∫ø‰ª•‰∏äËøêË°åÔºå‰∏îÂêéÊÆµÊúÄ‰Ωé‰ª∑Êó•‰∏éÊúÄÈ´ò‰ª∑Êó•Áõ∏Â∑ÆÂøÖÈ°ªÂú®10-50Êó•Èó¥„ÄÇ
    4ÔºâÂõûË∏©‰º¥ÈöèÁº©ÈáèÔºöÊúÄÈ´ò‰ª∑Êó•‰∫§ÊòìÈáè/ÂêéÊÆµÊúÄ‰Ωé‰ª∑Êó•‰∫§ÊòìÈáè&gt;2,ÂêéÊÆµÊúÄ‰Ωé‰ª∑/ÊúÄÈ´ò‰ª∑&lt;0.8„ÄÇ
5„ÄÅÁ™ÅÁ†¥Âπ≥Âè∞
    1Ôºâ60Êó•ÂÜÖÊüêÊó•Êî∂Áõò‰ª∑&gt;=60Êó•ÂùáÁ∫ø&gt;ÂºÄÁõò‰ª∑„ÄÇ
    2Ôºâ‰∏î„Äê1„ÄëÊîæÈáè‰∏äÊ∂®„ÄÇ
    3Ôºâ‰∏î„Äê1„ÄëÈó¥‰πãÂâçÊó∂Èó¥Ôºå‰ªªÊÑè‰∏ÄÂ§©Êî∂Áõò‰ª∑‰∏é60Êó•ÂùáÁ∫øÂÅèÁ¶ªÂú®-5%~20%‰πãÈó¥„ÄÇ
6„ÄÅÊó†Â§ßÂπÖÂõûÊí§
    1ÔºâÂΩìÊó•Êî∂Áõò‰ª∑ÊØî60Êó•ÂâçÁöÑÊî∂Áõò‰ª∑ÁöÑÊ∂®ÂπÖÂ∞è‰∫é0.6„ÄÇ
    2ÔºâÊúÄËøë60Êó•Ôºå‰∏çËÉΩÊúâÂçïÊó•Ë∑åÂπÖË∂Ö7%„ÄÅÈ´òÂºÄ‰ΩéËµ∞7%„ÄÅ‰∏§Êó•Á¥ØËÆ°Ë∑åÂπÖ10%„ÄÅ‰∏§Êó•È´òÂºÄ‰ΩéËµ∞Á¥ØËÆ°10%„ÄÇ
7„ÄÅÊµ∑Èæü‰∫§ÊòìÊ≥ïÂàô
    ÊúÄÂêé‰∏Ä‰∏™‰∫§ÊòìÊó•Êî∂Â∏Ç‰ª∑‰∏∫ÊåáÂÆöÂå∫Èó¥ÂÜÖÊúÄÈ´ò‰ª∑„ÄÇ
    1ÔºâÂΩìÊó•Êî∂Áõò‰ª∑&gt;=ÊúÄËøë60Êó•ÊúÄÈ´òÊî∂Áõò‰ª∑„ÄÇ
8„ÄÅÈ´òËÄåÁ™ÑÁöÑÊóóÂΩ¢
    1ÔºâÂøÖÈ°ªËá≥Â∞ë‰∏äÂ∏Ç‰∫§Êòì60Êó•„ÄÇ
    2ÔºâÂΩìÊó•Êî∂Áõò‰ª∑/‰πãÂâç24~10Êó•ÁöÑÊúÄ‰Ωé‰ª∑&gt;=1.9„ÄÇ
    3Ôºâ‰πãÂâç24~10Êó•ÂøÖÈ°ªËøûÁª≠‰∏§Â§©Ê∂®ÂπÖÂ§ß‰∫éÁ≠â‰∫é9.5%„ÄÇ
9„ÄÅÊîæÈáèË∑åÂÅú„ÄÇ
    1ÔºâË∑å&gt;9.5%„ÄÇ
    2ÔºâÊàê‰∫§È¢ù‰∏ç‰Ωé‰∫é2‰∫ø„ÄÇ
    3ÔºâÊàê‰∫§ÈáèËá≥Â∞ëÊòØ5Êó•Âπ≥ÂùáÊàê‰∫§ÈáèÁöÑ4ÂÄç„ÄÇ
10„ÄÅ‰ΩéATRÊàêÈïø
    1ÔºâÂøÖÈ°ªËá≥Â∞ë‰∏äÂ∏Ç‰∫§Êòì250Êó•„ÄÇ
    2ÔºâÊúÄËøë10‰∏™‰∫§ÊòìÊó•ÁöÑÊúÄÈ´òÊî∂Áõò‰ª∑ÂøÖÈ°ªÊØîÊúÄËøë10‰∏™‰∫§ÊòìÊó•ÁöÑÊúÄ‰ΩéÊî∂Áõò‰ª∑È´ò1.1ÂÄç„ÄÇ
11„ÄÅËÇ°Á•®Âü∫Êú¨Èù¢ÈÄâËÇ°
    1ÔºâÂ∏ÇÁõàÁéáÂ∞è‰∫éÁ≠â‰∫é20Ôºå‰∏îÂ§ß‰∫é0„ÄÇ
    2ÔºâÂ∏ÇÂáÄÁéáÂ∞è‰∫éÁ≠â‰∫é10„ÄÇ
    3ÔºâÂáÄËµÑ‰∫ßÊî∂ÁõäÁéáÂ§ß‰∫éÁ≠â‰∫é15„ÄÇ
```

![](img/04.jpg)

## ÂÖ´ÔºöÈÄâËÇ°È™åËØÅ


ÂØπÊåáÊ†á„ÄÅÁ≠ñÁï•Á≠âÈÄâÂá∫ÁöÑËÇ°Á•®ËøõË°åÂõûÊµãÔºåÈ™åËØÅÁ≠ñÁï•ÁöÑÊàêÂäüÁéáÔºåÊòØÂê¶ÂèØÁî®„ÄÇ


![](img/05.jpg)

## ‰πùÔºöËá™Âä®‰∫§Êòì

ÊîØÊåÅËá™Âä®‰∫§ÊòìÔºåÂÜÖÁΩÆËá™Âä®ÊâìÊñ∞ËÇ°ÁöÑÁ≠ñÁï•ÂèäÁ§∫‰æãÁ≠ñÁï•ÔºåÁî±‰∫é**Ê∂âÂèäÈáëÈí±**ÔºåËßÑÈÅøÂèØËÉΩÂ≠òÂú®È£éÈô©ÔºåÊ≤°ÊúâÊèê‰æõÂÖ∂‰ªñ‰∫§ÊòìÁ≠ñÁï•„ÄÇ

ÂÖ∑Êúâ‰∫§ÊòìÊó•ÂøóÔºå‰ª•ÂèäÊîØÊåÅ‰∏∫ÊØè‰∏™‰∫§ÊòìÁ≠ñÁï•ÈÖçÁΩÆ‰∫§ÊòìÊó•Âøó„ÄÇ

**ÁâπÂà´ÊèêÈÜí**Ôºö‰∫§ÊòìÊó•10:00ÁÇπ‰ºöËß¶ÂèëÊâìÊñ∞Ôºå‰∏çÊÉ≥ÊâìÊñ∞ÁöÑÂà†Èô§stagging.pyÊàñ‰∏çË¶ÅÂêØÂä®‚Äú‰∫§ÊòìÊúçÂä°‚Äù„ÄÇ

![](img/11.jpg)

## ÂçÅÔºöÂÖ≥Ê≥®ÂäüËÉΩ

ÊîØÊåÅËÇ°Á•®ÂÖ≥Ê≥®ÔºåÂÖ≥Ê≥®ËÇ°Á•®Âú®ÂêÑ‰∏™Ê®°Âùó(Âê´ÊúâÁöÑ)ÁΩÆÈ°∂„ÄÅÊ†áÁ∫¢ÊòæÁ§∫„ÄÇ

## ÂçÅ‰∏ÄÔºöÊîØÊåÅÊâπÈáè


ÂèØ‰ª•ÈÄöËøáÊó∂Èó¥ÊÆµ„ÄÅÊûö‰∏æÊó∂Èó¥„ÄÅÂΩìÂâçÊó∂Èó¥ËøõË°åÊåáÊ†áËÆ°ÁÆó„ÄÅÁ≠ñÁï•ÈÄâËÇ°ÂèäÂõûÊµãÁ≠â„ÄÇÂêåÊó∂ÊîØÊåÅÊô∫ËÉΩËØÜÂà´‰∫§ÊòìÊó•ÔºåÂèØ‰ª•ËæìÂÖ•‰ªªÊÑèÊó•Êúü„ÄÇ

ÂÖ∑‰ΩìÊâßË°åËÆæÁΩÆÂ¶Ç‰∏ãÔºö
```
------Êï¥‰Ωì‰Ωú‰∏öÔºåÊîØÊåÅÊâπÈáè‰Ωú‰∏ö------
ÂΩìÂâçÊó∂Èó¥‰Ωú‰∏ö python execute_daily_job.py
Âçï‰∏™Êó∂Èó¥‰Ωú‰∏ö python execute_daily_job.py 2022-03-01
Êûö‰∏æÊó∂Èó¥‰Ωú‰∏ö python execute_daily_job.py 2022-01-01,2021-02-08,2022-03-12
Âå∫Èó¥Êó∂Èó¥‰Ωú‰∏ö python execute_daily_job.py 2022-01-01 2022-03-01

------ÂçïÂäüËÉΩ‰Ωú‰∏öÔºåÊîØÊåÅÊâπÈáè‰Ωú‰∏öÔºåÂõûÊµãÊï∞ÊçÆËá™Âä®Â°´Ë°•Âà∞ÂΩìÂâç
Âü∫Á°ÄÊï∞ÊçÆÂÆûÊó∂‰Ωú‰∏ö python basic_data_daily_job.py
Âü∫Á°ÄÊï∞ÊçÆÈùûÂÆûÊó∂‰Ωú‰∏ö python basic_data_other_daily_job.py
ÊåáÊ†áÊï∞ÊçÆ‰Ωú‰∏ö python indicators_data_daily_job.py
KÁ∫øÂΩ¢ÊÄÅ‰Ωú‰∏ö klinepattern_data_daily_job.py
Á≠ñÁï•Êï∞ÊçÆ‰Ωú‰∏ö python strategy_data_daily_job.py
ÂõûÊµãÊï∞ÊçÆ python backtest_data_daily_job.py
```

## ÂçÅ‰∫åÔºöÂ≠òÂÇ®ÈááÁî®Êï∞ÊçÆÂ∫ìËÆæËÆ°

Êï∞ÊçÆÂ≠òÂÇ®ÈááÁî®Êï∞ÊçÆÂ∫ìËÆæËÆ°ÔºåËÉΩ‰øùÂ≠òÂéÜÂè≤Êï∞ÊçÆÔºå‰ª•ÂèäÂØπÊï∞ÊçÆËøõË°åÊâ©Â±ïÂàÜÊûê„ÄÅÁªüËÆ°„ÄÅÊåñÊéò„ÄÇÁ≥ªÁªüÂÆûÁé∞Ëá™Âä®ÂàõÂª∫Êï∞ÊçÆÂ∫ì„ÄÅÊï∞ÊçÆË°®ÔºåÂ∞ÅË£Ö‰∫ÜÊâπÈáèÊõ¥Êñ∞„ÄÅÊèíÂÖ•Êï∞ÊçÆÔºåÊñπ‰æø‰∏öÂä°Êâ©Â±ï„ÄÇ

![](img/07.jpg)

## ÂçÅ‰∏âÔºöÂ±ïÁ§∫ÈááÁî®webËÆæËÆ°

ÈááÁî®webËÆæËÆ°ÔºåÂèØËßÜÂåñÂ±ïÁ§∫ÁªìÊûú„ÄÇÂØπÂ±ïÁ§∫ËøõË°åÂ∞ÅË£ÖÔºåÊ∑ªÂä†Êñ∞ÁöÑ‰∏öÂä°Ë°®ÂçïÔºåÂè™ÈúÄË¶ÅÈÖçÁΩÆËßÜÂõæÂ≠óÂÖ∏Â∞±ÂèØËá™Âä®Âá∫Áé∞‰∏öÂä°ÂèØËßÜÂåñÁïåÈù¢ÔºåÊñπ‰æø‰∏öÂä°ÂäüËÉΩÊâ©Â±ï„ÄÇ

## ÂçÅÂõõÔºöËøêË°åÈ´òÊïà


ÈááÁî®Â§öÁ∫øÁ®ã„ÄÅÂçï‰æãÂÖ±‰∫´ËµÑÊ∫êÊúâÊïàÊèêÈ´òËøêÁÆóÊïàÁéá„ÄÇ1Â§©Êï∞ÊçÆÁöÑÊäìÂèñ„ÄÅËÆ°ÁÆóÊåáÊ†á„ÄÅÂΩ¢ÊÄÅËØÜÂà´„ÄÅÁ≠ñÁï•ÈÄâËÇ°„ÄÅÂõûÊµãÁ≠âÂÖ®ÈÉ®‰ªªÂä°ËøêË°åÊó∂Èó¥Â§ßÊ¶Ç4ÂàÜÈíüÔºàÊôÆÈÄöÁ¨îËÆ∞Êú¨ÔºâÔºåËÆ°ÁÆóÂ§©Êï∞Ë∂äÂ§öÊïàÁéáË∂äÈ´ò„ÄÇ


## ÂçÅ‰∫îÔºöÊñπ‰æøË∞ÉËØï

Á≥ªÁªüËøêË°åÁöÑÈáçË¶ÅÊó•ÂøóËÆ∞ÂΩïÂú®stock_execute_job.log(Êï∞ÊçÆÊäìÂèñ„ÄÅÂ§ÑÁêÜ„ÄÅÂàÜÊûê)„ÄÅstock_web.log(webÊúçÂä°)„ÄÅstock_trade.log(‰∫§ÊòìÊúçÂä°)ÔºåÊñπ‰æøË∞ÉËØïÂèëÁé∞ÈóÆÈ¢ò„ÄÇ

![](img/08.jpg)


# ÂÆâË£ÖËØ¥Êòé

Êú¨Á≥ªÁªüÊîØÊåÅWindows„ÄÅLinux„ÄÅMacOSÔºåÂêåÊó∂Êú¨Á≥ªÁªüÂàõÂª∫‰∫ÜDockerÈïúÂÉèÔºåÊåâËá™Â∑±ÈúÄË¶ÅÈÄâÊã©ÂÆâË£ÖÊñπÂºè„ÄÇ

‰∏ãÈù¢ÊåâÂàÜÂ∏∏ËßÑÂÆâË£ÖÊñπÂºè„ÄÅdockerÈïúÂÉèÂÆâË£ÖÊñπÂºèËøõË°å‰∏Ä‰∏ÄËØ¥Êòé„ÄÇ

## ‰∏ÄÔºöÂ∏∏ËßÑÂÆâË£ÖÊñπÂºè

Âª∫ËÆÆwindows‰∏ãÂÆâË£ÖÔºåÊñπ‰æøÊìç‰ΩúÂèä‰ΩøÁî®Á≥ªÁªüÔºåÂêåÊó∂ÂÆâË£Ö‰πüÈùûÂ∏∏ÁÆÄÂçï„ÄÇ

‰ª•‰∏ãÂÆâË£ÖÂèäËøêË°å‰ª•windows‰∏∫‰æãËøõË°å‰ªãÁªç„ÄÇ

### 1.ÂÆâË£Öpython

È°πÁõÆÂºÄÂèë‰ΩøÁî®python 3.11ÔºåÂª∫ËÆÆÊúÄÊñ∞Áâà„ÄÇ

```
Ôºà1ÔºâÂú®ÂÆòÁΩë https://www.python.org/downloads/ ‰∏ãËΩΩÂÆâË£ÖÂåÖÔºå‰∏ÄÈîÆÂÆâË£ÖÂç≥ÂèØÔºåÂÆâË£ÖÂàáËÆ∞ÂãæÈÄâËá™Âä®ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáè„ÄÇ
Ôºà2ÔºâÈÖçÁΩÆÊ∞∏‰πÖÂÖ®Â±ÄÂõΩÂÜÖÈïúÂÉèÂ∫ìÔºàÂõ†‰∏∫ÊúâÂ¢ôÔºåÊó†Ê≥ïÊ≠£Â∏∏ÂÆâË£ÖÂ∫ìÊñá‰ª∂ÔºâÔºåÊâßË°åÂ¶Ç‰∏ãdosÂëΩ‰ª§Ôºö
python pip config --global set  global.index-url https://mirrors.aliyun.com/pypi/simple/
# Â¶ÇÊûú‰Ω†Âè™ÊÉ≥‰∏∫ÂΩìÂâçÁî®Êà∑ËÆæÁΩÆÔºå‰Ω†‰πüÂèØ‰ª•ÂéªÊéâ‰∏ãÈù¢ÁöÑ&quot;--global&quot;ÈÄâÈ°π
```
### 2.ÂÆâË£Ömysql

Âª∫ËÆÆÊúÄÊñ∞Áâà„ÄÇ

```
Âú®ÂÆòÁΩë https://dev.mysql.com/downloads/mysql/ ‰∏ãËΩΩÂÆâË£ÖÂåÖÔºå‰∏ÄÈîÆÂÆâË£ÖÂç≥ÂèØ„ÄÇ
```
### 3.ÂÆâË£Ö TA-Lib ÂÖ±‰∫´ÈùôÊÄÅÂ∫ìÂíåÂ§¥Êñá‰ª∂

ÂÆâË£Ö TA-Lib C/C++ ÂÖ±‰∫´ÈùôÊÄÅÂ∫ìÂíåÂ§¥Êñá‰ª∂

```
https://ta-lib.org/install/ ‰∏ãËΩΩÊúÄÊñ∞ ta-lib ÂÖ±‰∫´ÈùôÊÄÅÂ∫ìÂíåÂ§¥Êñá‰ª∂ÔºåÊåâÁÖßËØ¥ÊòéËøõË°åÂÆâË£Ö„ÄÇ
ÂÆâË£ÖÊñπÂºèÊåâÂÆòÊñπÂª∫ËÆÆÔºå‰ºöÊõ¥ÁÆÄÂçïÔºö
Windows Executable Installer
macOS Homebrew
Linux Debian packages
```

### 4.ÂÆâË£Ö‰æùËµñÂ∫ì

‰æùËµñÂ∫ìÈÉΩÊòØÁõÆÂâçÊúÄÊñ∞ÁâàÊú¨„ÄÇ

a.ÂÆâË£Ö‰æùËµñÂ∫ìÔºö

```
#dosÂàáÊç¢Âà∞Êú¨Á≥ªÁªüÁöÑÊ†πÁõÆÂΩïÔºåÊâßË°å‰∏ãÈù¢ÂëΩ‰ª§Ôºö
python pip install -r requirements.txt
```
b.Ëã•ÊÉ≥ÂçáÁ∫ßÈ°πÁõÆ‰æùËµñÂ∫ìËá≥ÊúÄÊñ∞ÁâàÔºåÂèØ‰ª•ÈÄöËøá‰∏ãÈù¢ÊñπÊ≥ïÔºö

ÂÖàÊâìÂºÄrequirements.txtÔºåÁÑ∂Âêé‰øÆÊîπÊñá‰ª∂‰∏≠ÁöÑ‚Äú==‚Äù‰∏∫‚Äú&gt;=‚ÄùÔºåÊé•ÁùÄÊâßË°å‰∏ãÈù¢ÂëΩ‰ª§Ôºö

```
python pip install -r requirements.txt --upgrade
```

c.Ëã•Êâ©Â±ï‰∫ÜÊú¨È°πÁõÆÔºåÂèØ‰ª•ÈÄöËøá‰∏ãÈù¢ÊñπÊ≥ïÁîüÊàêÈ°πÁõÆ‰æùËµñÔºö

```
#‰ΩøÁî®pipreqsÁîüÊàêÈ°πÁõÆÁõ∏ÂÖ≥‰æùËµñÁöÑrequirements.txt

python pip install pipreqs
# ÂÆâË£ÖpipreqsÔºåËã•ÊúâÂÆâË£ÖÂèØË∑≥Ëøá

python  pipreqs --encoding utf-8 --force ./ 
# Êú¨È°πÁõÆÊòØutf-8ÁºñÁ†Å
```


### 5.ÂÆâË£Ö NavicatÔºàÂèØÈÄâÔºâ

NavicatÂèØ‰ª•Êñπ‰æøÁÆ°ÁêÜÊï∞ÊçÆÂ∫ìÔºå‰ª•ÂèäÂèØ‰ª•ÊâãÂ∑•ÂØπÊï∞ÊçÆËøõË°åÊü•Áúã„ÄÅÂ§ÑÁêÜ„ÄÅÂàÜÊûê„ÄÅÊåñÊéò„ÄÇ

NavicatÊòØ‰∏ÄÂ•óÂèØÂàõÂª∫Â§ö‰∏™ËøûÊé•ÁöÑÊï∞ÊçÆÂ∫ìÁÆ°ÁêÜÂ∑•ÂÖ∑ÔºåÁî®‰ª•Êñπ‰æøÁÆ°ÁêÜ MySQL„ÄÅOracle„ÄÅPostgreSQL„ÄÅSQLite„ÄÅSQL Server„ÄÅMariaDB Âíå MongoDB Á≠â‰∏çÂêåÁ±ªÂûãÁöÑÊï∞ÊçÆÂ∫ì

```
Ôºà1ÔºâÂú®ÂÆòÁΩë https://www.navicat.com.cn/download/navicat-premium ‰∏ãËΩΩÂÆâË£ÖÂåÖÔºå‰∏ÄÈîÆÂÆâË£ÖÂç≥ÂèØ„ÄÇ

Ôºà2ÔºâÁÑ∂Âêé‰∏ãËΩΩÁ†¥Ëß£Ë°•‰∏Å: https://pan.baidu.com/s/18XpTHrm9OiLEl3u6z_uxnw ÊèêÂèñÁ†Å: 8888 ÔºåÁ†¥Ëß£Âç≥ÂèØ„ÄÇ
```
### 6.ÈÖçÁΩÆÊï∞ÊçÆÂ∫ì

‰∏ÄËà¨ÂèØËÉΩ‰ºö‰øÆÊîπÁöÑ‰ø°ÊÅØÊòØ‚ÄùÊï∞ÊçÆÂ∫ìËÆøÈóÆÂØÜÁ†Å‚Äú„ÄÇ

‰øÆÊîπdatabase.pyÁõ∏ÂÖ≥‰ø°ÊÅØ:

```
db_host = &quot;localhost&quot;  # Êï∞ÊçÆÂ∫ìÊúçÂä°‰∏ªÊú∫
db_user = &quot;root&quot;  # Êï∞ÊçÆÂ∫ìËÆøÈóÆÁî®Êà∑
db_password = &quot;root&quot;  # Êï∞ÊçÆÂ∫ìËÆøÈóÆÂØÜÁ†Å
db_port = 3306  # Êï∞ÊçÆÂ∫ìÊúçÂä°Á´ØÂè£
db_charset = &quot;utf8mb4&quot;  # Êï∞ÊçÆÂ∫ìÂ≠óÁ¨¶ÈõÜ
```

### 7.ÂÆâË£ÖËá™Âä®‰∫§ÊòìÔºàÂèØÈÄâÔºâ

```
1.ÂÆâË£Ö‰∫§ÊòìËΩØ‰ª∂
    1.1 ÈÄöÁî®ÂêåËä±È°∫ÂÆ¢Êà∑Á´ØÂà∏ÂïÜÁöÑÂÆ¢Êà∑
        ÈÄöÁî®ÂêåËä±È°∫ÂÆ¢Êà∑Á´Ø:
        https://activity.ths123.com/acmake/cache/1361.html
    1.2 ‰∏ìÁî®ÂêåËä±È°∫ÂÆ¢Êà∑Á´ØÂà∏ÂïÜÁöÑÂÆ¢Êà∑
        Ëá™Ë°åÂéªÂà∏ÂïÜÂÆòÁΩëÊâæÂêåËä±È°∫‰∏ìÁî®Áâà
        ‰æãÂ¶ÇÔºöÂπøÂèëÁöÑ‰∏ãËΩΩÊ†∏Êñ∞Áã¨Á´ãÂßîÊâòÁ´Ø(ÂêåËä±È°∫Áâà):
        http://www.gf.com.cn/softdownload/index?tab=1
2.ÂÆâË£Ötesseract(Ëá™Âä®ËØÜÂà´È™åËØÅÁ†Å)
    Á¨¨‰∏ÄÁßçÊñπÊ≥ï.‰∏ãËΩΩÁºñËØëÂ•ΩÁöÑ
        Âú®‰∏ãÈù¢ÈìæÊé•È°µÔºåÊ†πÊçÆÊìç‰ΩúÁ≥ªÁªüÈÄâÊã©Áõ∏Â∫îÁâàÊú¨
        https://digi.bib.uni-mannheim.de/tesseract/
    Á¨¨‰∫åÁßçÊñπÊ≥ï.Áî®Ê∫êÁ†ÅÁºñËØë
        ‰∏ãËΩΩÊ∫êÁ†ÅÔºöhttps://github.com/tesseract-ocr/tesseract
    Ê≥®ÊÑèÔºö
        ÂÆâË£ÖÂÆåË¶ÅÂ∞ÜÂÆâË£ÖË∑ØÂæÑËÆæÁΩÆÂà∞PATHÁéØÂ¢ÉÂèòÈáèÈáå„ÄÇ
        ‰∏ãÈù¢Êèê‰æõdosÂëΩ‰ª§ËÆæÁΩÆÔºå‰ª•ÁÆ°ÁêÜÂëòË∫´‰ªΩËøêË°åcmdÔºåËæìÂÖ•:
        setx /m PATH &quot;%PATH%;C:\Program Files\Tesseract-OCR&quot;
3.ËÆæÁΩÆ‰∫§ÊòìÈÖçÁΩÆ   
    3.1.‰øÆÊîπtrade_client.json
        &quot;user&quot;: &quot;888888888888&quot;,               #‰∫§ÊòìË¥¶Âè∑
        &quot;password&quot;: &quot;888888&quot;,                 #‰∫§ÊòìÂØÜÁ†Å
        &quot;exe_path&quot;: &quot;C:/gfzqrzrq/xiadan.exe&quot;  #‰∫§ÊòìËΩØ‰ª∂Ë∑ØÂæÑ
    3.2.‰øÆÊîπtrade_service.py
        broker = &#039;gf_client&#039; #ËøôÊòØÂπøÂèë
        ËØ¶ÊÉÖÂèÇÈòÖusage.mdÔºåÈÖçÁΩÆÂØπÂ∫îÂà∏ÂïÜ
```

### 8.ËøêË°åËØ¥Êòé

#### 8.1.ÊâßË°åÊï∞ÊçÆÊäìÂèñ„ÄÅÂ§ÑÁêÜ„ÄÅÂàÜÊûê„ÄÅËØÜÂà´

ÊîØÊåÅÊâπÈáè‰Ωú‰∏öÔºåÂÖ∑‰ΩìÂèÇËßÅrun_job.bat‰∏≠ÁöÑÊ≥®ÈáäËØ¥Êòé„ÄÇ

Âª∫ËÆÆÂ∞ÜÂÖ∂Âä†ÂÖ•Âà∞‰ªªÂä°ËÆ°Âàí‰∏≠ÔºåÂ∑•‰ΩúÊó•ÁöÑÊØèÂ§©17Ôºö00ÊâßË°å„ÄÇ

**Êï∞ÊçÆÊäìÂèñ„ÄÅÂ§ÑÁêÜÂéüÂàôÔºö**

1).ÂºÄÁõòÂç≥Êúâ‰∏îÊó†ÂéÜÂè≤Êï∞ÊçÆÁöÑÔºöÁªºÂêàÈÄâËÇ°„ÄÅÊØèÊó•ËÇ°Á•®Êï∞ÊçÆ„ÄÅËÇ°Á•®ËµÑÈáëÊµÅÂêë„ÄÅËÇ°Á•®ÂàÜÁ∫¢ÈÖçÈÄÅ„ÄÅÈæôËôéÊ¶ú„ÄÅÊØèÊó•ETFÊï∞ÊçÆÔºõ

2).Êî∂ÁõòÂç≥Êúâ‰∏îÊúâÂéÜÂè≤Êï∞ÊçÆÁöÑÔºöËÇ°Á•®ÊåáÊ†áÊï∞ÊçÆ„ÄÅËÇ°Á•®KÁ∫øÂΩ¢ÊÄÅ„ÄÅËÇ°Á•®Á≠ñÁï•Êï∞ÊçÆÔºõ

3).Êî∂ÁõòÂêé1~2Â∞èÊó∂ÊâçÊúâ‰∏îÊúâÂéÜÂè≤Êï∞ÊçÆÁöÑÔºöÂ§ßÂÆó‰∫§Êòì„ÄÇ

ËøêË°årun_job.batÔºå‰ºö‰æùÊçÆ‰∏äÈù¢ÂéüÂàôËé∑ÂèñÂêÑÊ®°ÂùóÂΩìÂâçÊàñÂâç‰∏™‰∫§ÊòìÊó•ÁöÑÊï∞ÊçÆ„ÄÇ

```

ËøêË°å run_job.bat
```
Ëã•ÊÉ≥ÁúãÂºÄÁõòÂêéÁöÑÂΩìÂâçÂÆûÊó∂Êï∞ÊçÆÔºåÂèØ‰ª•ËøêË°å‰∏ãÈù¢ÔºåÂæàÂø´Â§ßÊ¶Ç1ÁßíÔºö

```
#Âü∫Á°ÄÊï∞ÊçÆ‰Ωú‰∏ö 
python basic_data_daily_job.py
```
#### 8.2.ÂêØÂä®webÊúçÂä°

```
ËøêË°å run_web.bat
```
ÂêØÂä®ÊúçÂä°ÂêéÔºåÊâìÂºÄÊµèËßàÂô®ÔºåËæìÂÖ•Ôºöhttp://localhost:9988/ ÔºåÂç≥ÂèØ‰ΩøÁî®Êú¨Á≥ªÁªüÁöÑÂèØËßÜÂåñÂäüËÉΩ„ÄÇ

#### 8.3.ÂêØÂä®‰∫§ÊòìÊúçÂä°

```
ËøêË°å run_trade.bat
```

## ‰∫åÔºödockerÈïúÂÉèÂÆâË£ÖÊñπÂºè

Ê≤°ÊúâdockerÁéØÂ¢ÉÔºåÂèØ‰ª•ÂèÇËÄÉÔºö[VirtualBoxËôöÊãüÊú∫ÂÆâË£ÖUbuntu](https://www.ljjyy.com/archives/2019/10/100590.html)ÔºåÈáåÈù¢‰πü‰ªãÁªç‰∫Üpython„ÄÅdockerÁ≠âÂ∏∏Áî®ËΩØ‰ª∂ÁöÑÂÆâË£ÖÔºåËã•ÊÉ≥Âú®Windows‰∏ãÂÆâË£ÖdockerËá™Ë°åÁôæÂ∫¶„ÄÇ

### 1.ÂÆâË£ÖÊï∞ÊçÆÂ∫ìÈïúÂÉè

Â¶ÇÊûúÂ∑≤ÁªèÊúâMysql„ÄÅmariadbÊï∞ÊçÆÂ∫ìÂèØ‰ª•Ë∑≥ËøáÊú¨Ê≠•„ÄÇ

ËøêË°å‰∏ãÈù¢ÂëΩ‰ª§Ôºö

**ÁâπÂà´ÊèêÈÜíÔºöÊâßË°åÂëΩ‰ª§ÁöÑÁî®Êà∑Ë¶ÅÊúârootÊùÉÈôêÔºåÂÖ∂‰ªñÂëΩ‰ª§‰πüÂ¶ÇÊ≠§„ÄÇ‰æãÂ¶ÇÔºöubuntuÁ≥ªÁªüÂú®ÂëΩ‰ª§ÂâçÂä†‰∏äsudo** Ôºåsudo docker......

```
docker run -d --name InStockDbService \
    -v /data/mariadb/data:/var/lib/instockdb \
    -e MYSQL_ROOT_PASSWORD=root \
    library/mariadb:latest
```

### 2.ÂÆâË£ÖÊú¨Á≥ªÁªüÈïúÂÉè

a.Ëã•Êåâ‰∏äÈù¢„Äê1.ÂÆâË£ÖÊï∞ÊçÆÂ∫ìÈïúÂÉè„ÄëË£ÖÁöÑÊï∞ÊçÆÂ∫ìÔºåËøêË°å‰∏ãÈù¢ÂëΩ‰ª§Ôºö

```
docker run -dit --name InStock --link=InStockDbService \
    -p 9988:9988 \
    -e db_host=InStockDbService \
    mayanghua/instock:latest
```

b.Â∑≤ÁªèÊúâMysql„ÄÅmariadbÊï∞ÊçÆÂ∫ìÔºåËøêË°å‰∏ãÈù¢ÂëΩ‰ª§Ôºö

```
docker run -dit --name InStock \
    -p 9988:9988 \
    -e db_host=localhost \
    -e db_user=root \
    -e db_password=root \
    -e db_database=instockdb \
    -e db_port=3306 \
    mayanghua/instock:latest
```

docker -e ÂèÇÊï∞ËØ¥ÊòéÔºö
```
db_host       # Êï∞ÊçÆÂ∫ìÊúçÂä°‰∏ªÊú∫
db_user       # Êï∞ÊçÆÂ∫ìËÆøÈóÆÁî®Êà∑
db_password   # Êï∞ÊçÆÂ∫ìËÆøÈóÆÂØÜÁ†Å
db_database   # Êï∞ÊçÆÂ∫ìÂêçÁß∞
db_port       # Êï∞ÊçÆÂ∫ìÊúçÂä°Á´ØÂè£
```
ÊåâËá™Â∑±Êï∞ÊçÆÂ∫ìÂÆûÈôÖÊÉÖÂÜµÈÖçÁΩÆÂèÇÊï∞„ÄÇ

### 3. Á≥ªÁªüËøêË°å

ÂêØÂä®ÂÆπÂô®ÂêéÔºå‰ºöËá™Âä®ËøêË°åÔºåÈ¶ñÂÖà‰ºöÂàùÂßãÂåñÊï∞ÊçÆ„ÄÅÂêØÂä®webÊúçÂä°„ÄÇÁÑ∂ÂêéÊØèÂ∞èÊó∂ÊâßË°å‚ÄúÂü∫Á°ÄÊï∞ÊçÆÊäìÂèñ‚ÄùÔºåÊØèÂ§©17:30ÊâßË°åÊâÄÊúâÁöÑÊï∞ÊçÆÊäìÂèñ„ÄÅÂ§ÑÁêÜ„ÄÅÂàÜÊûê„ÄÅËØÜÂà´„ÄÅÂõûÊµã„ÄÇ

ÊâìÂºÄÊµèËßàÂô®ÔºåËæìÂÖ•Ôºöhttp://localhost:9988/ ÔºåÂç≥ÂèØ‰ΩøÁî®Êú¨Á≥ªÁªüÁöÑÂèØËßÜÂåñÂäüËÉΩ„ÄÇ

### 4.ÂéÜÂè≤Êï∞ÊçÆ

ÂéÜÂè≤Êï∞ÊçÆÊäìÂèñ„ÄÅÂ§ÑÁêÜ„ÄÅÂàÜÊûê„ÄÅËØÜÂà´„ÄÅÂõûÊµãÔºåËøêË°å‰∏ãÈù¢ÂëΩ‰ª§Ôºö

```
docker exec -it InStock bash 
cat InStock/instock/bin/run_job.sh
#Êü•Áúãrun_job.shÊ≥®Èáä,Ëá™Â∑±ÈÄâÊã©‰Ωú‰∏ö
------Êï¥‰Ωì‰Ωú‰∏öÔºåÊîØÊåÅÊâπÈáè‰Ωú‰∏ö------
ÂΩìÂâçÊó∂Èó¥‰Ωú‰∏ö python execute_daily_job.py
Âçï‰∏™Êó∂Èó¥‰Ωú‰∏ö python execute_daily_job.py 2022-03-01
Êûö‰∏æÊó∂Èó¥‰Ωú‰∏ö python execute_daily_job.py 2022-01-01,2021-02-08,2022-03-12
Âå∫Èó¥Êó∂Èó¥‰Ωú‰∏ö python execute_daily_job.py 2022-01-01 2022-03-01
------ÂçïÂäüËÉΩ‰Ωú‰∏öÔºåÊîØÊåÅÊâπÈáè‰Ωú‰∏öÔºåÂõûÊµãÊï∞ÊçÆËá™Âä®Â°´Ë°•Âà∞ÂΩìÂâç
ÁªºÂêàÈÄâËÇ°‰Ωú‰∏ö python selection_data_daily_job.py
Âü∫Á°ÄÊï∞ÊçÆÂÆûÊó∂‰Ωú‰∏ö python basic_data_daily_job.py
Âü∫Á°ÄÊï∞ÊçÆÊî∂Áõò2Â∞èÊó∂Âêé‰Ωú‰∏ö python backtest_data_daily_job.py
Âü∫Á°ÄÊï∞ÊçÆÈùûÂÆûÊó∂‰Ωú‰∏ö python basic_data_other_daily_job.py
ÊåáÊ†áÊï∞ÊçÆ‰Ωú‰∏ö python indicators_data_daily_job.py
KÁ∫øÂΩ¢ÊÄÅ‰Ωú‰∏ö klinepattern_data_daily_job.py
Á≠ñÁï•Êï∞ÊçÆ‰Ωú‰∏ö python strategy_data_daily_job.py
ÂõûÊµãÊï∞ÊçÆ python backtest_data_daily_job.py
Á¨¨‰∏ÄÁßçÊñπÊ≥ïÔºö
python execute_daily_job.py 2023-03-01,2023-03-02
Á¨¨‰∫åÁßçÊñπÊ≥ïÔºö
‰øÆÊîπrun_job.shÔºåÁÑ∂ÂêéËøêË°å bash InStock/instock/bin/run_job.sh
```

### 5.Êü•ÁúãÊó•Âøó

ËøêË°å‰∏ãÈù¢ÂëΩ‰ª§Ôºö

```
docker exec -it InStock bash 
cat InStock/instock/log/stock_execute_job.log
cat InStock/instock/log/stock_web.log
```

### 6.dockerÂ∏∏Áî®ÂëΩ‰ª§

```
docker container stop InStock InStockDbService
#ÂÅúÊ≠¢ÂÆπÂô®
docker container prune
#ÂõûÊî∂ÂÆπÂô®
docker rmi mayanghua/instock:latest library/mariadb:latest
#Âà†Èô§ÈïúÂÉè
```

ÂÖ∑‰ΩìÂèÇËßÅÔºö[DockerÂü∫Á°Ä‰πã ‰∫å.ÈïúÂÉèÂèäÂÆπÂô®ÁöÑÂü∫Êú¨Êìç‰Ωú](https://www.ljjyy.com/archives/2018/06/100208.html)

### 7.Ëá™Âä®‰∫§Êòì

ÁõÆÂâçÂè™ÊîØÊåÅwindows„ÄÇÂèÇËÄÉÂ∏∏ËßÑÂÆâË£ÖÊñπÂºè,Âè™ÈúÄÂÆâË£Öpython„ÄÅ‰æùËµñÂ∫ìÔºå**‰∏çÈúÄÂÆâË£Ömysql„ÄÅtalibÁ≠â**„ÄÇ

# ÁâπÂà´Â£∞Êòé

ËÇ°Â∏ÇÊúâÈ£éÈô©ÊäïËµÑÈúÄË∞®ÊÖéÔºåÊú¨Á≥ªÁªüÂè™ËÉΩÁî®‰∫éÂ≠¶‰π†„ÄÅËÇ°Á•®ÂàÜÊûêÔºåÊäïËµÑÁõà‰∫èÊ¶Ç‰∏çË¥üË¥£„ÄÇ

Êú¨Á≥ªÁªü‰∏≠ÁöÑË°®Ê†º‰∏∫Á¨¨‰∏âÊñπÂïÜ‰∏öÊéß‰ª∂Ôºå‰ªÖ‰ΩøÁî®‰∫ÜËØÑ‰º∞ÁâàËøõË°åÂ≠¶‰π†ÂèäÊµãËØï„ÄÇ
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Tencent/Hunyuan3D-2]]></title>
            <link>https://github.com/Tencent/Hunyuan3D-2</link>
            <guid>https://github.com/Tencent/Hunyuan3D-2</guid>
            <pubDate>Wed, 30 Apr 2025 00:04:19 GMT</pubDate>
            <description><![CDATA[High-Resolution 3D Assets Generation with Large Scale Hunyuan3D Diffusion Models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Tencent/Hunyuan3D-2">Tencent/Hunyuan3D-2</a></h1>
            <p>High-Resolution 3D Assets Generation with Large Scale Hunyuan3D Diffusion Models.</p>
            <p>Language: Python</p>
            <p>Stars: 9,168</p>
            <p>Forks: 770</p>
            <p>Stars today: 252 stars today</p>
            <h2>README</h2><pre>[‰∏≠ÊñáÈòÖËØª](README_zh_cn.md)
[Êó•Êú¨Ë™û„ÅßË™≠„ÇÄ](README_ja_jp.md)

&lt;p align=&quot;center&quot;&gt; 
  &lt;img src=&quot;https://github.com/user-attachments/assets/efb402a1-0b09-41e0-a6cb-259d442e76aa&quot;&gt;

&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=https://3d.hunyuan.tencent.com target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage height=22px&gt;&lt;/a&gt;
  &lt;a href=https://huggingface.co/spaces/tencent/Hunyuan3D-2  target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/%F0%9F%A4%97%20Demo-276cb4.svg height=22px&gt;&lt;/a&gt;
  &lt;a href=https://huggingface.co/tencent/Hunyuan3D-2 target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px&gt;&lt;/a&gt;
  &lt;a href=https://3d-models.hunyuan.tencent.com/ target=&quot;_blank&quot;&gt;&lt;img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px&gt;&lt;/a&gt;
  &lt;a href=https://discord.gg/dNBrdrGGMa target=&quot;_blank&quot;&gt;&lt;img src= https://img.shields.io/badge/Discord-white.svg?logo=discord height=22px&gt;&lt;/a&gt;
  &lt;a href=https://arxiv.org/abs/2501.12202 target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px&gt;&lt;/a&gt;
  &lt;a href=https://x.com/txhunyuan target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px&gt;&lt;/a&gt;
 &lt;a href=&quot;#community-resources&quot; target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/Community-lavender.svg?logo=homeassistantcommunitystore height=22px&gt;&lt;/a&gt;
&lt;/div&gt;

[//]: # (  &lt;a href=# target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px&gt;&lt;/a&gt;)

[//]: # (  &lt;a href=# target=&quot;_blank&quot;&gt;&lt;img src= https://img.shields.io/badge/Colab-8f2628.svg?logo=googlecolab height=22px&gt;&lt;/a&gt;)

[//]: # (  &lt;a href=&quot;#&quot;&gt;&lt;img alt=&quot;PyPI - Downloads&quot; src=&quot;https://img.shields.io/pypi/v/mulankit?logo=pypi&quot;  height=22px&gt;&lt;/a&gt;)

&lt;br&gt;

&lt;p align=&quot;center&quot;&gt;
‚Äú Living out everyone‚Äôs imagination on creating and manipulating 3D assets.‚Äù
&lt;/p&gt;

https://github.com/user-attachments/assets/a2cbc5b8-be22-49d7-b1c3-7aa2b20ba460




## üî• News

- Apr 1, 2025: ü§ó Release turbo paint model [Hunyuan3D-Paint-v2-0-Turbo](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-paint-v2-0-turbo), and multiview texture generation pipeline, try it [here](examples/fast_texture_gen_multiview.py)! Stay tuned for our new texture generation model [RomanTex](https://github.com/oakshy/RomanTex) and PBR material generation [MaterialMVP](https://github.com/ZebinHe/MaterialMVP/)! 
- Mar 19, 2025: ü§ó Release turbo model [Hunyuan3D-2-Turbo](https://huggingface.co/tencent/Hunyuan3D-2/), [Hunyuan3D-2mini-Turbo](https://huggingface.co/tencent/Hunyuan3D-2mini/) and [FlashVDM](https://github.com/Tencent/FlashVDM).
- Mar 18, 2025: ü§ó Release multiview shape model [Hunyuan3D-2mv](https://huggingface.co/tencent/Hunyuan3D-2mv) and 0.6B
  shape model [Hunyuan3D-2mini](https://huggingface.co/tencent/Hunyuan3D-2mini).
- Feb 14, 2025: üõ†Ô∏è Release texture enhancement module, please obtain high-definition textures
  via [here](minimal_demo.py)!
- Feb 3, 2025: üêé
  Release [Hunyuan3D-DiT-v2-0-Fast](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0-fast), our
  guidance distillation model that could half the dit inference time, see [here](minimal_demo.py) for usage.
- Jan 27, 2025: üõ†Ô∏è Release Blender addon for Hunyuan3D 2.0, Check it out [here](#blender-addon).
- Jan 23, 2025: üí¨ We thank community members for
  creating [Windows installation tool](https://github.com/YanWenKun/Hunyuan3D-2-WinPortable), ComfyUI support
  with [ComfyUI-Hunyuan3DWrapper](https://github.com/kijai/ComfyUI-Hunyuan3DWrapper)
  and [ComfyUI-3D-Pack](https://github.com/MrForExample/ComfyUI-3D-Pack) and other
  awesome [extensions](#community-resources).
- Jan 21, 2025: üí¨ Enjoy exciting 3D generation on our website [Hunyuan3D Studio](https://3d.hunyuan.tencent.com)!
- Jan 21, 2025: ü§ó Release inference code and pretrained models
  of [Hunyuan3D 2.0](https://huggingface.co/tencent/Hunyuan3D-2). Please give it a try
  via [huggingface space](https://huggingface.co/spaces/tencent/Hunyuan3D-2) and
  our [official site](https://3d.hunyuan.tencent.com)!

&gt; Join our **[Wechat](#)** and **[Discord](https://discord.gg/dNBrdrGGMa)** group to discuss and find help from us.

| Wechat Group                                     | Xiaohongshu                                           | X                                           | Discord                                           |
|--------------------------------------------------|-------------------------------------------------------|---------------------------------------------|---------------------------------------------------|
| &lt;img src=&quot;assets/qrcode/wechat.png&quot;  height=140&gt; | &lt;img src=&quot;assets/qrcode/xiaohongshu.png&quot;  height=140&gt; | &lt;img src=&quot;assets/qrcode/x.png&quot;  height=140&gt; | &lt;img src=&quot;assets/qrcode/discord.png&quot;  height=140&gt; |        




## **Abstract**

We present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for generating high-resolution textured 3D assets.
This system includes two foundation components: a large-scale shape generation model - Hunyuan3D-DiT, and a large-scale
texture synthesis model - Hunyuan3D-Paint.
The shape generative model, built on a scalable flow-based diffusion transformer, aims to create geometry that properly
aligns with a given condition image, laying a solid foundation for downstream applications.
The texture synthesis model, benefiting from strong geometric and diffusion priors, produces high-resolution and vibrant
texture maps for either generated or hand-crafted meshes.
Furthermore, we build Hunyuan3D-Studio - a versatile, user-friendly production platform that simplifies the re-creation
process of 3D assets. It allows both professional and amateur users to manipulate or even animate their meshes
efficiently.
We systematically evaluate our models, showing that Hunyuan3D 2.0 outperforms previous state-of-the-art models,
including the open-source models and closed-source models in geometry details, condition alignment, texture quality, and
e.t.c.



&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/images/system.jpg&quot;&gt;
&lt;/p&gt;

## ‚òØÔ∏è **Hunyuan3D 2.0**

### Architecture

Hunyuan3D 2.0 features a two-stage generation pipeline, starting with the creation of a bare mesh, followed by the
synthesis of a texture map for that mesh. This strategy is effective for decoupling the difficulties of shape and
texture generation and also provides flexibility for texturing either generated or handcrafted meshes.

&lt;p align=&quot;left&quot;&gt;
  &lt;img src=&quot;assets/images/arch.jpg&quot;&gt;
&lt;/p&gt;

### Performance

We have evaluated Hunyuan3D 2.0 with other open-source as well as close-source 3d-generation methods.
The numerical results indicate that Hunyuan3D 2.0 surpasses all baselines in the quality of generated textured 3D assets
and the condition following ability.

| Model                   | CMMD(‚¨á)   | FID_CLIP(‚¨á) | FID(‚¨á)      | CLIP-score(‚¨Ü) |
|-------------------------|-----------|-------------|-------------|---------------|
| Top Open-source Model1  | 3.591     | 54.639      | 289.287     | 0.787         |
| Top Close-source Model1 | 3.600     | 55.866      | 305.922     | 0.779         |
| Top Close-source Model2 | 3.368     | 49.744      | 294.628     | 0.806         |
| Top Close-source Model3 | 3.218     | 51.574      | 295.691     | 0.799         |
| Hunyuan3D 2.0           | **3.193** | **49.165**  | **282.429** | **0.809**     |

Generation results of Hunyuan3D 2.0:
&lt;p align=&quot;left&quot;&gt;
  &lt;img src=&quot;assets/images/e2e-1.gif&quot;  height=250&gt;
  &lt;img src=&quot;assets/images/e2e-2.gif&quot;  height=250&gt;
&lt;/p&gt;

## üéÅ Models Zoo

It takes 6 GB VRAM for shape generation and 16 GB for shape and texture generation in total.

Hunyuan3D-2mini Series

| Model                       | Description                   | Date       | Size | Huggingface                                                                                      |
|-----------------------------|-------------------------------|------------|------|--------------------------------------------------------------------------------------------------|
| Hunyuan3D-DiT-v2-mini-Turbo | Step Distillation Version     | 2025-03-19 | 0.6B | [Download](https://huggingface.co/tencent/Hunyuan3D-2mini/tree/main/hunyuan3d-dit-v2-mini-turbo) |
| Hunyuan3D-DiT-v2-mini-Fast  | Guidance Distillation Version | 2025-03-18 | 0.6B | [Download](https://huggingface.co/tencent/Hunyuan3D-2mini/tree/main/hunyuan3d-dit-v2-mini-fast)  |
| Hunyuan3D-DiT-v2-mini       | Mini Image to Shape Model     | 2025-03-18 | 0.6B | [Download](https://huggingface.co/tencent/Hunyuan3D-2mini/tree/main/hunyuan3d-dit-v2-mini)       |


Hunyuan3D-2mv Series

| Model                     | Description                    | Date       | Size | Huggingface                                                                                  |
|---------------------------|--------------------------------|------------|------|----------------------------------------------------------------------------------------------| 
| Hunyuan3D-DiT-v2-mv-Turbo | Step Distillation Version      | 2025-03-19 | 1.1B | [Download](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv-turbo) |
| Hunyuan3D-DiT-v2-mv-Fast  | Guidance Distillation Version  | 2025-03-18 | 1.1B | [Download](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv-fast)  |
| Hunyuan3D-DiT-v2-mv       | Multiview Image to Shape Model | 2025-03-18 | 1.1B | [Download](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv)       |

Hunyuan3D-2 Series

| Model                      | Description                 | Date       | Size | Huggingface                                                                               |
|----------------------------|-----------------------------|------------|------|-------------------------------------------------------------------------------------------| 
| Hunyuan3D-DiT-v2-0-Turbo   | Step Distillation Model     | 2025-03-19 | 1.1B | [Download](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0-turbo)   |
| Hunyuan3D-DiT-v2-0-Fast    | Guidance Distillation Model | 2025-02-03 | 1.1B | [Download](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0-fast)    |
| Hunyuan3D-DiT-v2-0         | Image to Shape Model        | 2025-01-21 | 1.1B | [Download](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0)         |
| Hunyuan3D-Paint-v2-0       | Texture Generation Model    | 2025-01-21 | 1.3B | [Download](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-paint-v2-0)       |
| Hunyuan3D-Paint-v2-0-Turbo | Distillation Texure Model   | 2025-04-01 | 1.3B | [Download](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-paint-v2-0-turbo) |
| Hunyuan3D-Delight-v2-0     | Image Delight Model         | 2025-01-21 | 1.3B | [Download](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-delight-v2-0)     | 

## ü§ó Get Started with Hunyuan3D 2.0

Hunyuan3D 2.0 supports Macos, Windows, Linux. You may follow the next steps to use Hunyuan3D 2.0 via:

- [Code](#code-usage)
- [Gradio App](#gradio-app)
- [API Server](#api-server)
- [Blender Addon](#blender-addon)
- [Official Site](#official-site)

### Install Requirements

Please install Pytorch via the [official](https://pytorch.org/) site. Then install the other requirements via

```bash
pip install -r requirements.txt
pip install -e .
# for texture
cd hy3dgen/texgen/custom_rasterizer
python3 setup.py install
cd ../../..
cd hy3dgen/texgen/differentiable_renderer
python3 setup.py install
```

### Code Usage

We designed a diffusers-like API to use our shape generation model - Hunyuan3D-DiT and texture synthesis model -
Hunyuan3D-Paint.

You could assess **Hunyuan3D-DiT** via:

```python
from hy3dgen.shapegen import Hunyuan3DDiTFlowMatchingPipeline

pipeline = Hunyuan3DDiTFlowMatchingPipeline.from_pretrained(&#039;tencent/Hunyuan3D-2&#039;)
mesh = pipeline(image=&#039;assets/demo.png&#039;)[0]
```

The output mesh is a [trimesh object](https://trimesh.org/trimesh.html), which you could save to glb/obj (or other
format) file.

For **Hunyuan3D-Paint**, do the following:

```python
from hy3dgen.texgen import Hunyuan3DPaintPipeline
from hy3dgen.shapegen import Hunyuan3DDiTFlowMatchingPipeline

# let&#039;s generate a mesh first
pipeline = Hunyuan3DDiTFlowMatchingPipeline.from_pretrained(&#039;tencent/Hunyuan3D-2&#039;)
mesh = pipeline(image=&#039;assets/demo.png&#039;)[0]

pipeline = Hunyuan3DPaintPipeline.from_pretrained(&#039;tencent/Hunyuan3D-2&#039;)
mesh = pipeline(mesh, image=&#039;assets/demo.png&#039;)
```

Please visit [examples](examples) folder for more advanced usage, such as **multiview image to 3D generation** and *
*texture generation
for handcrafted mesh**.

### Gradio App

You could also host a [Gradio](https://www.gradio.app/) App in your own computer via:

Standard Version

```bash
# Hunyuan3D-2mini
python3 gradio_app.py --model_path tencent/Hunyuan3D-2mini --subfolder hunyuan3d-dit-v2-mini --texgen_model_path tencent/Hunyuan3D-2 --low_vram_mode
# Hunyuan3D-2mv
python3 gradio_app.py --model_path tencent/Hunyuan3D-2mv --subfolder hunyuan3d-dit-v2-mv --texgen_model_path tencent/Hunyuan3D-2 --low_vram_mode
# Hunyuan3D-2
python3 gradio_app.py --model_path tencent/Hunyuan3D-2 --subfolder hunyuan3d-dit-v2-0 --texgen_model_path tencent/Hunyuan3D-2 --low_vram_mode
```

Turbo Version

```bash
# Hunyuan3D-2mini
python3 gradio_app.py --model_path tencent/Hunyuan3D-2mini --subfolder hunyuan3d-dit-v2-mini-turbo --texgen_model_path tencent/Hunyuan3D-2 --low_vram_mode --enable_flashvdm
# Hunyuan3D-2mv
python3 gradio_app.py --model_path tencent/Hunyuan3D-2mv --subfolder hunyuan3d-dit-v2-mv-turbo --texgen_model_path tencent/Hunyuan3D-2 --low_vram_mode --enable_flashvdm
# Hunyuan3D-2
python3 gradio_app.py --model_path tencent/Hunyuan3D-2 --subfolder hunyuan3d-dit-v2-0-turbo --texgen_model_path tencent/Hunyuan3D-2 --low_vram_mode --enable_flashvdm
```

### API Server

You could launch an API server locally, which you could post web request for Image/Text to 3D, Texturing existing mesh,
and e.t.c.

```bash
python api_server.py --host 0.0.0.0 --port 8080
```

A demo post request for image to 3D without texture.

```bash
img_b64_str=$(base64 -i assets/demo.png)
curl -X POST &quot;http://localhost:8080/generate&quot; \
     -H &quot;Content-Type: application/json&quot; \
     -d &#039;{
           &quot;image&quot;: &quot;&#039;&quot;$img_b64_str&quot;&#039;&quot;,
         }&#039; \
     -o test2.glb
```

### Blender Addon

With an API server launched, you could also directly use Hunyuan3D 2.0 in your blender with
our [Blender Addon](blender_addon.py). Please follow our tutorial to install and use.

https://github.com/user-attachments/assets/8230bfb5-32b1-4e48-91f4-a977c54a4f3e

### Official Site

Don&#039;t forget to visit [Hunyuan3D](https://3d.hunyuan.tencent.com) for quick use, if you don&#039;t want to host yourself.

## üìë Open-Source Plan

- [x] Inference Code
- [x] Model Checkpoints
- [x] Technical Report
- [x] ComfyUI
- [ ] TensorRT Version
- [ ] Finetuning

## üîó BibTeX

If you found this repository helpful, please cite our reports:

```bibtex
@misc{hunyuan3d22025tencent,
    title={Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation},
    author={Tencent Hunyuan3D Team},
    year={2025},
    eprint={2501.12202},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{yang2024hunyuan3d,
    title={Hunyuan3D 1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation},
    author={Tencent Hunyuan3D Team},
    year={2024},
    eprint={2411.02293},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
```

## Community Resources

Thanks for the contributions of community members, here we have these great extensions of Hunyuan3D 2.0:

- [ComfyUI-3D-Pack](https://github.com/MrForExample/ComfyUI-3D-Pack)
- [ComfyUI-Hunyuan3DWrapper](https://github.com/kijai/ComfyUI-Hunyuan3DWrapper)
- [Hunyuan3D-2-for-windows](https://github.com/sdbds/Hunyuan3D-2-for-windows)
- [üì¶ A bundle for running on Windows | Êï¥ÂêàÂåÖ](https://github.com/YanWenKun/Hunyuan3D-2-WinPortable)
- [Hunyuan3D-2GP](https://github.com/deepbeepmeep/Hunyuan3D-2GP)
- [Kaggle Notebook](https://github.com/darkon12/Hunyuan3D-2GP_Kaggle)

## Acknowledgements

We would like to thank the contributors to
the [Trellis](https://github.com/microsoft/TRELLIS),  [DINOv2](https://github.com/facebookresearch/dinov2), [Stable Diffusion](https://github.com/Stability-AI/stablediffusion), [FLUX](https://github.com/black-forest-labs/flux), [diffusers](https://github.com/huggingface/diffusers), [HuggingFace](https://huggingface.co), [CraftsMan3D](https://github.com/wyysf-98/CraftsMan3D),
and [Michelangelo](https://github.com/NeuralCarver/Michelangelo/tree/main) repositories, for their open research and
exploration.

## Star History

&lt;a href=&quot;https://star-history.com/#Tencent/Hunyuan3D-2&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Tencent/Hunyuan3D-2&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Tencent/Hunyuan3D-2&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=Tencent/Hunyuan3D-2&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/UFO]]></title>
            <link>https://github.com/microsoft/UFO</link>
            <guid>https://github.com/microsoft/UFO</guid>
            <pubDate>Wed, 30 Apr 2025 00:04:18 GMT</pubDate>
            <description><![CDATA[The Desktop AgentOS.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/UFO">microsoft/UFO</a></h1>
            <p>The Desktop AgentOS.</p>
            <p>Language: Python</p>
            <p>Stars: 6,920</p>
            <p>Forks: 869</p>
            <p>Stars today: 35 stars today</p>
            <h2>README</h2><pre>&lt;!-- markdownlint-disable MD033 MD041 --&gt;
&lt;h1 align=&quot;center&quot;&gt;
  &lt;b&gt;UFO¬≤&lt;/b&gt; &lt;img src=&quot;assets/ufo_blue.png&quot; alt=&quot;UFO logo&quot; width=&quot;40&quot;&gt; :&amp;nbsp;The&amp;nbsp;Desktop&amp;nbsp;AgentOS
&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;em&gt;Turn natural‚Äëlanguage requests into automatic, reliable, multi‚Äëapplication workflows on Windows, beyond UI-Focused.&lt;/em&gt;
&lt;/p&gt;


&lt;div align=&quot;center&quot;&gt;

[![arxiv](https://img.shields.io/badge/Paper-arXiv:2504.14603-b31b1b.svg)](https://arxiv.org/abs/2504.14603)&amp;ensp;
![Python Version](https://img.shields.io/badge/Python-3776AB?&amp;logo=python&amp;logoColor=white-blue&amp;label=3.10%20%7C%203.11)&amp;ensp;
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)&amp;ensp;
[![Documentation](https://img.shields.io/badge/Documentation-%230ABAB5?style=flat&amp;logo=readthedocs&amp;logoColor=black)](https://microsoft.github.io/UFO/)&amp;ensp;
[![YouTube](https://img.shields.io/badge/YouTube-white?logo=youtube&amp;logoColor=%23FF0000)](https://www.youtube.com/watch?v=QT_OhygMVXU)&amp;ensp;
&lt;!-- [![X (formerly Twitter) Follow](https://img.shields.io/twitter/follow/UFO_Agent)](https://twitter.com/intent/follow?screen_name=UFO_Agent) --&gt;
&lt;!-- ![Welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)&amp;ensp; --&gt;

&lt;/div&gt;

&lt;!-- **UFO** is a **UI-Focused** multi-agent framework to fulfill user requests on **Windows OS** by seamlessly navigating and operating within individual or spanning multiple applications. --&gt;

&lt;h1 align=&quot;center&quot;&gt;
    &lt;img src=&quot;./assets/comparison.png&quot; width=&quot;60%&quot;/&gt; 
&lt;/h1&gt;

---

## ‚ú®¬†Key Capabilities
&lt;div align=&quot;center&quot;&gt;

| [Deep¬†OS¬†Integration](https://microsoft.github.io/UFO)  | Picture‚Äëin‚ÄëPicture Desktop *(coming soon)* | [Hybrid¬†GUI¬†+¬†API¬†Actions](https://microsoft.github.io/UFO/automator/overview) |
|---------------------|-------------------------------------------|---------------------------|
| Combines Windows¬†UIA, Win32 and WinCOM for first‚Äëclass control detection and native commands. | Automation runs in a sandboxed virtual desktop so you can keep using your main screen. | Chooses native APIs when available, falls back to clicks/keystrokes when not‚Äîfast *and* robust. |

| [Speculative¬†Multi‚ÄëAction](https://microsoft.github.io/UFO/advanced_usage/multi_action) | [Continuous¬†Knowledge¬†Substrate](https://microsoft.github.io/UFO/advanced_usage/reinforce_appagent/overview/) | [UIA¬†+¬†Visual¬†Control¬†Detection](https://microsoft.github.io/UFO/advanced_usage/control_detection/hybrid_detection) |
|--------------------------|--------------------------------|--------------------------------|
| Bundles several predicted steps into one LLM call, validated live‚Äîup to **51¬†% fewer** queries. | Mixes docs, Bing search, user demos and execution traces via RAG for agents that learn over time. | Detects standard *and* custom controls with a hybrid UIA¬†+¬†vision pipeline. |

&lt;/div&gt;

*See the [documentation](https://microsoft.github.io/UFO/) for full details.*

---

## üì¢ News
- üìÖ 2025-04-19: Version **v2.0.0** Released! We‚Äôre excited to announce the release the **UFO¬≤**! UFO¬≤ is a major upgrade to the original UFO, featuring with enhanced capabilities. It introduces the **AgentOS** concept, enabling seamless integration of multiple agents for complex tasks. Please check our [new technical report](https://arxiv.org/pdf/2504.14603) for more details.
- üìÖ ...
- üìÖ 2024-02-14: Our [technical report](https://arxiv.org/abs/2402.07939) for UFO is online!
- üìÖ 2024-02-10: The first version of UFO is released on GitHubüéà. Happy Chinese New yearüêâ!

---

## üèóÔ∏è¬†Architecture overview
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/framework2.png&quot;  width=&quot;80%&quot; alt=&quot;UFO¬≤ architecture&quot;/&gt;
&lt;/p&gt;


UFO¬≤ operates as a **Desktop AgentOS**, encompassing a multi-agent framework that includes:

1. **HostAgent** ‚Äì Parses the natural‚Äëlanguage goal, launches the necessary applications, spins up¬†/¬†coordinates AppAgents, and steers a global finite‚Äëstate machine (FSM).  
2. **AppAgents** ‚Äì One per application; each runs a ReAct loop with multimodal perception, hybrid control detection, retrieval‚Äëaugmented knowledge, and the **Puppeteer** executor that chooses between GUI actions and native APIs.  
3. **Knowledge¬†Substrate** ‚Äì Blends offline documentation, online search, demonstrations, and execution traces into a vector store that is retrieved on‚Äëthe‚Äëfly at inference.  
4. **Speculative¬†Executor** ‚Äì Slashes LLM latency by predicting batches of likely actions and validating them against live UIA state in a single shot.  
5. **Picture‚Äëin‚ÄëPicture¬†Desktop** *(coming soon)* ‚Äì Runs the agent in an isolated virtual desktop so your main workspace and input devices remain untouched.

For a deep dive see our [technical report](https://arxiv.org/pdf/2504.14603) or the [docs site](https://microsoft.github.io/UFO).

---

## üåê Media Coverage 

UFO sightings have garnered attention from various media outlets, including:
- [Microsoft&#039;s UFO abducts traditional user interfaces for a smarter Windows experience](https://the-decoder.com/microsofts-ufo-abducts-traditional-user-interfaces-for-a-smarter-windows-experience/)
- [üöÄ UFO &amp; GPT-4-V: Sit back and relax, mientras GPT lo hace todoüåå](https://www.linkedin.com/posts/gutierrezfrancois_ai-ufo-microsoft-activity-7176819900399652865-pLoo?utm_source=share&amp;utm_medium=member_desktop)
- [The AI PC - The Future of Computers? - Microsoft UFO](https://www.youtube.com/watch?v=1k4LcffCq3E)
- [‰∏ã‰∏Ä‰ª£WindowsÁ≥ªÁªüÊõùÂÖâÔºöÂü∫‰∫éGPT-4VÔºåAgentË∑®Â∫îÁî®Ë∞ÉÂ∫¶Ôºå‰ª£Âè∑UFO](https://baijiahao.baidu.com/s?id=1790938358152188625&amp;wfr=spider&amp;for=pc)
- [‰∏ã‰∏Ä‰ª£Êô∫ËÉΩÁâà Windows Ë¶ÅÊù•‰∫ÜÔºüÂæÆËΩØÊé®Âá∫È¶ñ‰∏™ Windows AgentÔºåÂëΩÂêç‰∏∫ UFOÔºÅ](https://blog.csdn.net/csdnnews/article/details/136161570)
- [MicrosoftÁô∫„ÅÆ„Ç™„Éº„Éó„É≥„ÇΩ„Éº„ÇπÁâà„ÄåUFO„ÄçÁôªÂ†¥ÔºÅ„ÄÄWindows„ÇíËá™ÂãïÊìçÁ∏¶„Åô„ÇãAI„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÇíË©¶„Åô](https://internet.watch.impress.co.jp/docs/column/shimizu/1570581.html)
- ...

These sources provide insights into the evolving landscape of technology and the implications of UFO phenomena on various platforms.

---

## üöÄ¬†Three‚Äëminute Quickstart


### üõ†Ô∏è Step 1: Installation
UFO requires **Python &gt;= 3.10** running on **Windows OS &gt;= 10**. It can be installed by running the following command:
```powershell
# [optional to create conda environment]
# conda create -n ufo python=3.10
# conda activate ufo

# clone the repository
git clone https://github.com/microsoft/UFO.git
cd UFO
# install the requirements
pip install -r requirements.txt
# If you want to use the Qwen as your LLMs, uncomment the related libs.
```

### ‚öôÔ∏è Step 2: Configure the LLMs
Before running UFO, you need to provide your LLM configurations **individually for HostAgent and AppAgent**. You can create your own config file `ufo/config/config.yaml`, by copying the `ufo/config/config.yaml.template` and editing config for **HOST_AGENT** and **APP_AGENT** as follows: 

```powershell
copy ufo\config\config.yaml.template ufo\config\config.yaml
notepad ufo\config\config.yaml   # paste your key &amp; endpoint
```

#### OpenAI
```yaml
VISUAL_MODE: True, # Whether to use the visual mode
API_TYPE: &quot;openai&quot; , # The API type, &quot;openai&quot; for the OpenAI API.  
API_BASE: &quot;https://api.openai.com/v1/chat/completions&quot;, # The the OpenAI API endpoint.
API_KEY: &quot;sk-&quot;,  # The OpenAI API key, begin with sk-
API_VERSION: &quot;2024-02-15-preview&quot;, # &quot;2024-02-15-preview&quot; by default
API_MODEL: &quot;gpt-4o&quot;,  # The only OpenAI model
```

#### Azure OpenAI (AOAI)
```yaml
VISUAL_MODE: True, # Whether to use the visual mode
API_TYPE: &quot;aoai&quot; , # The API type, &quot;aoai&quot; for the Azure OpenAI.  
API_BASE: &quot;YOUR_ENDPOINT&quot;, #  The AOAI API address. Format: https://{your-resource-name}.openai.azure.com
API_KEY: &quot;YOUR_KEY&quot;,  # The aoai API key
API_VERSION: &quot;2024-02-15-preview&quot;, # &quot;2024-02-15-preview&quot; by default
API_MODEL: &quot;gpt-4o&quot;,  # The only OpenAI model
API_DEPLOYMENT_ID: &quot;YOUR_AOAI_DEPLOYMENT&quot;, # The deployment id for the AOAI API
```

&gt; Need Qwen, Gemini, non‚Äëvisual GPT‚Äë4, or even **OpenAI CUA Operator** as a AppAgent? See the [model guide](https://microsoft.github.io/UFO/supported_models/overview/).

### üìî Step 3: Additional Setting for RAG (optional).
If you want to enhance UFO&#039;s ability with external knowledge, you can optionally configure it with an external database for retrieval augmented generation (RAG) in the `ufo/config/config.yaml` file. 

We provide the following options for RAG to enhance UFO&#039;s capabilities:
- [Offline Help Document](https://microsoft.github.io/UFO/advanced_usage/reinforce_appagent/learning_from_help_document/) Enable UFO to retrieve information from offline help documents.
- [Online Bing Search Engine](https://microsoft.github.io/UFO/advanced_usage/reinforce_appagent/learning_from_bing_search/): Enhance UFO&#039;s capabilities by utilizing the most up-to-date online search results.
- [Self-Experience](https://microsoft.github.io/UFO/advanced_usage/reinforce_appagent/experience_learning/): Save task completion trajectories into UFO&#039;s memory for future reference.
- [User-Demonstration](https://microsoft.github.io/UFO/advanced_usage/reinforce_appagent/learning_from_demonstration/): Boost UFO&#039;s capabilities through user demonstration.

Consult their respective documentation for more information on how to configure these settings.


### üéâ Step 4: Start UFO

#### ‚å®Ô∏è You can execute the following on your Windows command Line (CLI):

```powershell
# assume you are in the cloned UFO folder
python -m ufo --task &lt;your_task_name&gt;
```

This will start the UFO process and you can interact with it through the command line interface. 
If everything goes well, you will see the following message:

```powershell
Welcome to use UFOüõ∏, A UI-focused Agent for Windows OS Interaction. 
 _   _  _____   ___
| | | ||  ___| / _ \
| | | || |_   | | | |
| |_| ||  _|  | |_| |
 \___/ |_|     \___/
Please enter your request to be completedüõ∏:
```

Alternatively, you can also directly invoke UFO with a specific task and request by using the following command:

```powershell
python -m ufo --task &lt;your_task_name&gt; -r &quot;&lt;your_request&gt;&quot;
```


###  Step 5 üé•: Execution Logs 

You can find the screenshots taken and request &amp; response logs in the following folder:
```
./ufo/logs/&lt;your_task_name&gt;/
```
You may use them to debug, replay, or analyze the agent output.


## ‚ùìGet help 
* Please first check our our documentation [here](https://microsoft.github.io/UFO/).
* ‚ùîGitHub Issues (prefered)
* For other communications, please contact [ufo-agent@microsoft.com](mailto:ufo-agent@microsoft.com).
---


## üìä Evaluation

UFO¬≤ is rigorously benchmarked on two publicly‚Äëavailable live‚Äëtask suites:

| Benchmark | Scope | Documents |
|-----------|-------|-------|
| [**Windows¬†Agent¬†Arena¬†(WAA)**](https://github.com/nice-mee/WindowsAgentArena) | 154 real Windows tasks across 15 applications (Office, Edge, File¬†Explorer, VS¬†Code, ‚Ä¶) | &lt;https://microsoft.github.io/UFO/benchmark/windows_agent_arena/&gt; |
| [**OSWorld (Windows)**](https://github.com/nice-mee/WindowsAgentArena/tree/2020-qqtcg/osworld) | 49 cross‚Äëapplication tasks that mix Office¬†365, browser and system utilities | &lt;https://microsoft.github.io/UFO/benchmark/osworld&gt; |

The integration of these benchmarks into UFO¬≤ is in separate repositories. Please follow the above documents for more details.

---


## üìö¬†Citation

If you build on this work, please cite our the AgentOS framework:

**UFO¬≤¬†‚Äì¬†The¬†Desktop¬†AgentOS (2025)**  
&lt;https://arxiv.org/abs/2504.14603&gt;
```bibtex
@article{zhang2025ufo2,
  title   = {{UFO2: The Desktop AgentOS}},
  author  = {Zhang, Chaoyun and Huang, He and Ni, Chiming and Mu, Jian and Qin, Si and He, Shilin and Wang, Lu and Yang, Fangkai and Zhao, Pu and Du, Chao and Li, Liqun and Kang, Yu and Jiang, Zhao and Zheng, Suzhen and Wang, Rujia and Qian, Jiaxu and Ma, Minghua and Lou, Jian-Guang and Lin, Qingwei and Rajmohan, Saravan and Zhang, Dongmei},
  journal = {arXiv preprint arXiv:2504.14603},
  year    = {2025}
}
```

**UFO¬†‚Äì¬†A¬†UI‚ÄëFocused Agent for Windows¬†OS Interaction (2024)**  
&lt;https://arxiv.org/abs/2402.07939&gt;
```bibtex
@article{zhang2024ufo,
  title   = {{UFO: A UI-Focused Agent for Windows OS Interaction}},
  author  = {Zhang, Chaoyun and Li, Liqun and He, Shilin and Zhang, Xu and Qiao, Bo and Qin, Si and Ma, Minghua and Kang, Yu and Lin, Qingwei and Rajmohan, Saravan and Zhang, Dongmei and Zhang, Qi},
  journal = {arXiv preprint arXiv:2402.07939},
  year    = {2024}
}
```



---

## üìù¬†Roadmap

The UFO¬≤ team is actively working on the following features and improvements:

- [ ] **Picture‚Äëin‚ÄëPicture Mode** ‚Äì Completed and will be available in the next release  
- [ ] **AgentOS‚Äëas‚Äëa‚ÄëService** ‚Äì Completed and will be available in the next release  
- [ ] **Auto‚ÄëDebugging Toolkit** ‚Äì Completed and will be available in the next release  
- [ ] **Integration with MCP and Agent2Agent Communication** ‚Äì Planned; under implementation  


---

## üé®¬†Related Projects
- **TaskWeaver**¬†‚Äî¬†a code‚Äëfirst LLM agent for data analytics: &lt;https://github.com/microsoft/TaskWeaver&gt;  
- **LLM‚ÄëBrained¬†GUI¬†Agents:¬†A¬†Survey**: &lt;https://arxiv.org/abs/2411.18279&gt; ‚Ä¢ [GitHub](https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey) ‚Ä¢ [Interactive site](https://vyokky.github.io/LLM-Brained-GUI-Agents-Survey/)

---


## ‚ö†Ô∏è Disclaimer
By choosing to run the provided code, you acknowledge and agree to the following terms and conditions regarding the functionality and data handling practices in [DISCLAIMER.md](./DISCLAIMER.md)


## &lt;img src=&quot;./assets/ufo_blue.png&quot; alt=&quot;logo&quot; width=&quot;30&quot;&gt; Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft 
trademarks or logos is subject to and must follow 
[Microsoft&#039;s Trademark &amp; Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party&#039;s policies.


---

## ‚öñÔ∏è¬†License
This repository is released under the [MIT¬†License](LICENSE) (SPDX‚ÄëIdentifier:¬†MIT).  
See [DISCLAIMER.md](DISCLAIMER.md) for privacy &amp; safety notices.

---

&lt;p align=&quot;center&quot;&gt;&lt;sub&gt;¬©¬†Microsoft¬†2025 ‚Ä¢ UFO¬≤ is an open‚Äësource project, not an official Windows feature.&lt;/sub&gt;&lt;/p&gt;

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>