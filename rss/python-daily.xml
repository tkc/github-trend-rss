<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sun, 20 Jul 2025 00:05:11 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[langchain-ai/open_deep_research]]></title>
            <link>https://github.com/langchain-ai/open_deep_research</link>
            <guid>https://github.com/langchain-ai/open_deep_research</guid>
            <pubDate>Sun, 20 Jul 2025 00:05:11 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/langchain-ai/open_deep_research">langchain-ai/open_deep_research</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 5,273</p>
            <p>Forks: 744</p>
            <p>Stars today: 249 stars today</p>
            <h2>README</h2><pre># Open Deep Research

&lt;img width=&quot;1388&quot; height=&quot;298&quot; alt=&quot;full_diagram&quot; src=&quot;https://github.com/user-attachments/assets/12a2371b-8be2-4219-9b48-90503eb43c69&quot; /&gt;

Deep research has broken out as one of the most popular agent applications. This is a simple, configurable, fully open source deep research agent that works across many model providers, search tools, and MCP servers. 

* Read more in our [blog](https://blog.langchain.com/open-deep-research/) 
* See our [video](https://www.youtube.com/watch?v=agGiWUpxkhg) for a quick overview

### üöÄ Quickstart

1. Clone the repository and activate a virtual environment:
```bash
git clone https://github.com/langchain-ai/open_deep_research.git
cd open_deep_research
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

2. Install dependencies:
```bash
uv pip install -r pyproject.toml
```

3. Set up your `.env` file to customize the environment variables (for model selection, search tools, and other configuration settings):
```bash
cp .env.example .env
```

4. Launch the assistant with the LangGraph server locally to open LangGraph Studio in your browser:

```bash
# Install dependencies and start the LangGraph server
uvx --refresh --from &quot;langgraph-cli[inmem]&quot; --with-editable . --python 3.11 langgraph dev --allow-blocking
```

Use this to open the Studio UI:
```
- üöÄ API: http://127.0.0.1:2024
- üé® Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024
- üìö API Docs: http://127.0.0.1:2024/docs
```
&lt;img width=&quot;817&quot; height=&quot;666&quot; alt=&quot;Screenshot 2025-07-13 at 11 21 12‚ÄØPM&quot; src=&quot;https://github.com/user-attachments/assets/052f2ed3-c664-4a4f-8ec2-074349dcaa3f&quot; /&gt;

Ask a question in the `messages` input field and click `Submit`.

### Configurations

Open Deep Research offers extensive configuration options to customize the research process and model behavior. All configurations can be set via the web UI, environment variables, or by modifying the configuration directly.

#### General Settings

- **Max Structured Output Retries** (default: 3): Maximum number of retries for structured output calls from models when parsing fails
- **Allow Clarification** (default: true): Whether to allow the researcher to ask clarifying questions before starting research
- **Max Concurrent Research Units** (default: 5): Maximum number of research units to run concurrently using sub-agents. Higher values enable faster research but may hit rate limits

#### Research Configuration

- **Search API** (default: Tavily): Choose from Tavily (works with all models), OpenAI Native Web Search, Anthropic Native Web Search, or None
- **Max Researcher Iterations** (default: 3): Number of times the Research Supervisor will reflect on research and ask follow-up questions
- **Max React Tool Calls** (default: 5): Maximum number of tool calling iterations in a single researcher step

#### Models

Open Deep Research uses multiple specialized models for different research tasks:

- **Summarization Model** (default: `openai:gpt-4.1-nano`): Summarizes research results from search APIs
- **Research Model** (default: `openai:gpt-4.1`): Conducts research and analysis 
- **Compression Model** (default: `openai:gpt-4.1-mini`): Compresses research findings from sub-agents
- **Final Report Model** (default: `openai:gpt-4.1`): Writes the final comprehensive report

All models are configured using [init_chat_model() API](https://python.langchain.com/docs/how_to/chat_models_universal_init/) which supports providers like OpenAI, Anthropic, Google Vertex AI, and others.

**Important Model Requirements:**

1. **Structured Outputs**: All models must support structured outputs. Check support [here](https://python.langchain.com/docs/integrations/chat/).

2. **Search API Compatibility**: Research and Compression models must support your selected search API:
   - Anthropic search requires Anthropic models with web search capability
   - OpenAI search requires OpenAI models with web search capability  
   - Tavily works with all models

3. **Tool Calling**: All models must support tool calling functionality

4. **Special Configurations**:
   - For OpenRouter: Follow [this guide](https://github.com/langchain-ai/open_deep_research/issues/75#issuecomment-2811472408)
   - For local models via Ollama: See [setup instructions](https://github.com/langchain-ai/open_deep_research/issues/65#issuecomment-2743586318)

#### Example MCP (Model Context Protocol) Servers

Open Deep Research supports MCP servers to extend research capabilities. 

#### Local MCP Servers

**Filesystem MCP Server** provides secure file system operations with robust access control:
- Read, write, and manage files and directories
- Perform operations like reading file contents, creating directories, moving files, and searching
- Restrict operations to predefined directories for security
- Support for both command-line configuration and dynamic MCP roots

Example usage:
```bash
mcp-server-filesystem /path/to/allowed/dir1 /path/to/allowed/dir2
```

#### Remote MCP Servers  

**Remote MCP servers** enable distributed agent coordination and support streamable HTTP requests. Unlike local servers, they can be multi-tenant and require more complex authentication.

**Arcade MCP Server Example**:
```json
{
  &quot;url&quot;: &quot;https://api.arcade.dev/v1/mcps/ms_0ujssxh0cECutqzMgbtXSGnjorm&quot;,
  &quot;tools&quot;: [&quot;Search_SearchHotels&quot;, &quot;Search_SearchOneWayFlights&quot;, &quot;Search_SearchRoundtripFlights&quot;]
}
```

Remote servers can be configured as authenticated or unauthenticated and support JWT-based authentication through OAuth endpoints.

### Evaluation

A comprehensive batch evaluation system designed for detailed analysis and comparative studies.

#### **Features:**
- **Multi-dimensional Scoring**: Specialized evaluators with 0-1 scale ratings
- **Dataset-driven Evaluation**: Batch processing across multiple test cases

#### **Usage:**
```bash
# Run comprehensive evaluation on LangSmith datasets
python tests/run_evaluate.py
```
#### **Key Files:**
- `tests/run_evaluate.py`: Main evaluation script
- `tests/evaluators.py`: Specialized evaluator functions
- `tests/prompts.py`: Evaluation prompts for each dimension

### Deployments and Usages

#### LangGraph Studio

Follow the [quickstart](#-quickstart) to start LangGraph server locally and test the agent out on LangGraph Studio.

#### Hosted deployment
 
You can easily deploy to [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/#deployment-options). 

#### Open Agent Platform

Open Agent Platform (OAP) is a UI from which non-technical users can build and configure their own agents. OAP is great for allowing users to configure the Deep Researcher with different MCP tools and search APIs that are best suited to their needs and the problems that they want to solve.

We&#039;ve deployed Open Deep Research to our public demo instance of OAP. All you need to do is add your API Keys, and you can test out the Deep Researcher for yourself! Try it out [here](https://oap.langchain.com)

You can also deploy your own instance of OAP, and make your own custom agents (like Deep Researcher) available on it to your users.
1. [Deploy Open Agent Platform](https://docs.oap.langchain.com/quickstart)
2. [Add Deep Researcher to OAP](https://docs.oap.langchain.com/setup/agents)

### Updates üî•

### Legacy Implementations üèõÔ∏è

The `src/legacy/` folder contains two earlier implementations that provide alternative approaches to automated research:

#### 1. Workflow Implementation (`legacy/graph.py`)
- **Plan-and-Execute**: Structured workflow with human-in-the-loop planning
- **Sequential Processing**: Creates sections one by one with reflection
- **Interactive Control**: Allows feedback and approval of report plans
- **Quality Focused**: Emphasizes accuracy through iterative refinement

#### 2. Multi-Agent Implementation (`legacy/multi_agent.py`)  
- **Supervisor-Researcher Architecture**: Coordinated multi-agent system
- **Parallel Processing**: Multiple researchers work simultaneously
- **Speed Optimized**: Faster report generation through concurrency
- **MCP Support**: Extensive Model Context Protocol integration

See `src/legacy/legacy.md` for detailed documentation, configuration options, and usage examples for both legacy implementations.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[soxoj/maigret]]></title>
            <link>https://github.com/soxoj/maigret</link>
            <guid>https://github.com/soxoj/maigret</guid>
            <pubDate>Sun, 20 Jul 2025 00:05:10 GMT</pubDate>
            <description><![CDATA[üïµÔ∏è‚Äç‚ôÇÔ∏è Collect a dossier on a person by username from thousands of sites]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/soxoj/maigret">soxoj/maigret</a></h1>
            <p>üïµÔ∏è‚Äç‚ôÇÔ∏è Collect a dossier on a person by username from thousands of sites</p>
            <p>Language: Python</p>
            <p>Stars: 16,773</p>
            <p>Forks: 1,134</p>
            <p>Stars today: 321 stars today</p>
            <h2>README</h2><pre># Maigret

&lt;p align=&quot;center&quot;&gt;
  &lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://pypi.org/project/maigret/&quot;&gt;
        &lt;img alt=&quot;PyPI version badge for Maigret&quot; src=&quot;https://img.shields.io/pypi/v/maigret?style=flat-square&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://pypi.org/project/maigret/&quot;&gt;  
        &lt;img alt=&quot;PyPI download count for Maigret&quot; src=&quot;https://img.shields.io/pypi/dw/maigret?style=flat-square&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/soxoj/maigret&quot;&gt;
        &lt;img alt=&quot;Minimum Python version required: 3.10+&quot; src=&quot;https://img.shields.io/badge/Python-3.10%2B-brightgreen?style=flat-square&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/soxoj/maigret/blob/main/LICENSE&quot;&gt;
        &lt;img alt=&quot;License badge for Maigret&quot; src=&quot;https://img.shields.io/github/license/soxoj/maigret?style=flat-square&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/soxoj/maigret&quot;&gt;
        &lt;img alt=&quot;View count for Maigret project&quot; src=&quot;https://komarev.com/ghpvc/?username=maigret&amp;color=brightgreen&amp;label=views&amp;style=flat-square&quot; /&gt;
    &lt;/a&gt;
  &lt;/p&gt;
  &lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/soxoj/maigret/main/static/maigret.png&quot; height=&quot;300&quot;/&gt;
  &lt;/p&gt;
&lt;/p&gt;

&lt;i&gt;The Commissioner Jules Maigret is a fictional French police detective, created by Georges Simenon. His investigation method is based on understanding the personality of different people and their interactions.&lt;/i&gt;

&lt;b&gt;üëâüëâüëâ [Online Telegram bot](https://t.me/osint_maigret_bot)&lt;/b&gt;

## About

**Maigret** collects a dossier on a person **by username only**, checking for accounts on a huge number of sites and gathering all the available information from web pages. No API keys are required. Maigret is an easy-to-use and powerful fork of [Sherlock](https://github.com/sherlock-project/sherlock).

Currently supports more than 3000 sites ([full list](https://github.com/soxoj/maigret/blob/main/sites.md)), search is launched against 500 popular sites in descending order of popularity by default. Also supported checking Tor sites, I2P sites, and domains (via DNS resolving).

## Powered By Maigret

These are professional tools for social media content analysis and OSINT investigations that use Maigret (banners are clickable).

&lt;a href=&quot;https://github.com/SocialLinks-IO/sociallinks-api&quot;&gt;&lt;img height=&quot;60&quot; alt=&quot;Social Links API&quot; src=&quot;https://github.com/user-attachments/assets/789747b2-d7a0-4d4e-8868-ffc4427df660&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://sociallinks.io/products/sl-crimewall&quot;&gt;&lt;img height=&quot;60&quot; alt=&quot;Social Links Crimewall&quot; src=&quot;https://github.com/user-attachments/assets/0b18f06c-2f38-477b-b946-1be1a632a9d1&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://usersearch.ai/&quot;&gt;&lt;img height=&quot;60&quot; alt=&quot;UserSearch&quot; src=&quot;https://github.com/user-attachments/assets/66daa213-cf7d-40cf-9267-42f97cf77580&quot;&gt;&lt;/a&gt;

## Main features

* Profile page parsing, [extraction](https://github.com/soxoj/socid_extractor) of personal info, links to other profiles, etc.
* Recursive search by new usernames and other IDs found
* Search by tags (site categories, countries)
* Censorship and captcha detection
* Requests retries

See the full description of Maigret features [in the documentation](https://maigret.readthedocs.io/en/latest/features.html).

## Installation

‚ÄºÔ∏è Maigret is available online via [official Telegram bot](https://t.me/osint_maigret_bot). Consider using it if you don&#039;t want to install anything.

### Windows

Standalone EXE-binaries for Windows are located in [Releases section](https://github.com/soxoj/maigret/releases) of GitHub repository.

Video guide on how to run it: https://youtu.be/qIgwTZOmMmM.

### Installation in Cloud Shells

You can launch Maigret using cloud shells and Jupyter notebooks. Press one of the buttons below and follow the instructions to launch it in your browser.

[![Open in Cloud Shell](https://user-images.githubusercontent.com/27065646/92304704-8d146d80-ef80-11ea-8c29-0deaabb1c702.png)](https://console.cloud.google.com/cloudshell/open?git_repo=https://github.com/soxoj/maigret&amp;tutorial=README.md)
&lt;a href=&quot;https://repl.it/github/soxoj/maigret&quot;&gt;&lt;img src=&quot;https://replit.com/badge/github/soxoj/maigret&quot; alt=&quot;Run on Replit&quot; height=&quot;50&quot;&gt;&lt;/a&gt;

&lt;a href=&quot;https://colab.research.google.com/gist/soxoj/879b51bc3b2f8b695abb054090645000/maigret-collab.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot; height=&quot;45&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://mybinder.org/v2/gist/soxoj/9d65c2f4d3bec5dd25949197ea73cf3a/HEAD&quot;&gt;&lt;img src=&quot;https://mybinder.org/badge_logo.svg&quot; alt=&quot;Open In Binder&quot; height=&quot;45&quot;&gt;&lt;/a&gt;

### Local installation

Maigret can be installed using pip, Docker, or simply can be launched from the cloned repo.


**NOTE**: Python 3.10 or higher and pip is required, **Python 3.11 is recommended.**

```bash
# install from pypi
pip3 install maigret

# usage
maigret username
```

### Cloning a repository

```bash
# or clone and install manually
git clone https://github.com/soxoj/maigret &amp;&amp; cd maigret

# build and install
pip3 install .

# usage
maigret username
```

### Docker

```bash
# official image
docker pull soxoj/maigret

# usage
docker run -v /mydir:/app/reports soxoj/maigret:latest username --html

# manual build
docker build -t maigret .
```

## Usage examples

```bash
# make HTML, PDF, and Xmind8 reports
maigret user --html
maigret user --pdf
maigret user --xmind #Output not compatible with xmind 2022+

# search on sites marked with tags photo &amp; dating
maigret user --tags photo,dating

# search on sites marked with tag us
maigret user --tags us

# search for three usernames on all available sites
maigret user1 user2 user3 -a
```

Use `maigret --help` to get full options description. Also options [are documented](https://maigret.readthedocs.io/en/latest/command-line-options.html).

### Web interface

You can run Maigret with a web interface, where you can view the graph with results and download reports of all formats on a single page.

&lt;details&gt;
&lt;summary&gt;Web Interface Screenshots&lt;/summary&gt;

![Web interface: how to start](https://raw.githubusercontent.com/soxoj/maigret/main/static/web_interface_screenshot_start.png)

![Web interface: results](https://raw.githubusercontent.com/soxoj/maigret/main/static/web_interface_screenshot.png)

&lt;/details&gt;

Instructions:

1. Run Maigret with the ``--web`` flag and specify the port number.

```console
maigret --web 5000
```
2. Open http://127.0.0.1:5000 in your browser and enter one or more usernames to make a search.

3. Wait a bit for the search to complete and view the graph with results, the table with all accounts found, and download reports of all formats.

## Contributing

Maigret has open-source code, so you may contribute your own sites by adding them to `data.json` file, or bring changes to it&#039;s code!

For more information about development and contribution, please read the [development documentation](https://maigret.readthedocs.io/en/latest/development.html).

## Demo with page parsing and recursive username search

### Video (asciinema)

&lt;a href=&quot;https://asciinema.org/a/Ao0y7N0TTxpS0pisoprQJdylZ&quot;&gt;
  &lt;img src=&quot;https://asciinema.org/a/Ao0y7N0TTxpS0pisoprQJdylZ.svg&quot; alt=&quot;asciicast&quot; width=&quot;600&quot;&gt;
&lt;/a&gt;

### Reports

[PDF report](https://raw.githubusercontent.com/soxoj/maigret/main/static/report_alexaimephotographycars.pdf), [HTML report](https://htmlpreview.github.io/?https://raw.githubusercontent.com/soxoj/maigret/main/static/report_alexaimephotographycars.html)

![HTML report screenshot](https://raw.githubusercontent.com/soxoj/maigret/main/static/report_alexaimephotography_html_screenshot.png)

![XMind 8 report screenshot](https://raw.githubusercontent.com/soxoj/maigret/main/static/report_alexaimephotography_xmind_screenshot.png)

[Full console output](https://raw.githubusercontent.com/soxoj/maigret/main/static/recursive_search.md)

## Disclaimer

**This tool is intended for educational and lawful purposes only.** The developers do not endorse or encourage any illegal activities or misuse of this tool. Regulations regarding the collection and use of personal data vary by country and region, including but not limited to GDPR in the EU, CCPA in the USA, and similar laws worldwide.

It is your sole responsibility to ensure that your use of this tool complies with all applicable laws and regulations in your jurisdiction. Any illegal use of this tool is strictly prohibited, and you are fully accountable for your actions.

The authors and developers of this tool bear no responsibility for any misuse or unlawful activities conducted by its users.

## Feedback

If you have any questions, suggestions, or feedback, please feel free to [open an issue](https://github.com/soxoj/maigret/issues), create a [GitHub discussion](https://github.com/soxoj/maigret/discussions), or contact the author directly via [Telegram](https://t.me/soxoj).

## SOWEL classification

This tool uses the following OSINT techniques:
- [SOTL-2.2. Search For Accounts On Other Platforms](https://sowel.soxoj.com/other-platform-accounts)
- [SOTL-6.1. Check Logins Reuse To Find Another Account](https://sowel.soxoj.com/logins-reuse)
- [SOTL-6.2. Check Nicknames Reuse To Find Another Account](https://sowel.soxoj.com/nicknames-reuse) 

## License

MIT ¬© [Maigret](https://github.com/soxoj/maigret)&lt;br/&gt;
MIT ¬© [Sherlock Project](https://github.com/sherlock-project/)&lt;br/&gt;
Original Creator of Sherlock Project - [Siddharth Dushantha](https://github.com/sdushantha)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Lightricks/LTX-Video]]></title>
            <link>https://github.com/Lightricks/LTX-Video</link>
            <guid>https://github.com/Lightricks/LTX-Video</guid>
            <pubDate>Sun, 20 Jul 2025 00:05:09 GMT</pubDate>
            <description><![CDATA[Official repository for LTX-Video]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Lightricks/LTX-Video">Lightricks/LTX-Video</a></h1>
            <p>Official repository for LTX-Video</p>
            <p>Language: Python</p>
            <p>Stars: 7,378</p>
            <p>Forks: 636</p>
            <p>Stars today: 144 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# LTX-Video

[![Website](https://img.shields.io/badge/Website-LTXV-181717?logo=google-chrome)](https://www.lightricks.com/ltxv)
[![Model](https://img.shields.io/badge/HuggingFace-Model-orange?logo=huggingface)](https://huggingface.co/Lightricks/LTX-Video)
[![Demo](https://img.shields.io/badge/Demo-Try%20Now-brightgreen?logo=vercel)](https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b)
[![Paper](https://img.shields.io/badge/Paper-arXiv-B31B1B?logo=arxiv)](https://arxiv.org/abs/2501.00103)
[![Trainer](https://img.shields.io/badge/LTXV-Trainer-9146FF?logo=github)](https://github.com/Lightricks/LTX-Video-Trainer)
[![Discord](https://img.shields.io/badge/Join-Discord-5865F2?logo=discord)](https://discord.gg/Mn8BRgUKKy)

This is the official repository for LTX-Video.

&lt;/div&gt;

## Table of Contents

- [Introduction](#introduction)
- [What&#039;s new](#news)
- [Models](#models)
- [Quick Start Guide](#quick-start-guide)
  - [Online demo](#online-inference)
  - [Run locally](#run-locally)
    - [Installation](#installation)
    - [Inference](#inference)
  - [ComfyUI Integration](#comfyui-integration)
  - [Diffusers Integration](#diffusers-integration)
- [Model User Guide](#model-user-guide)
- [Community Contribution](#community-contribution)
- [Training](#training)
- [Control Models](#control-models)
- [Join Us!](#join-us-)
- [Acknowledgement](#acknowledgement)

# Introduction

LTX-Video is the first DiT-based video generation model that can generate high-quality videos in *real-time*.
It can generate 30 FPS videos at 1216√ó704 resolution, faster than it takes to watch them.
The model is trained on a large-scale dataset of diverse videos and can generate high-resolution videos
with realistic and diverse content.

The model supports image-to-video, keyframe-based animation, video extension (both forward and backward), video-to-video transformations, and any combination of these features.

### Image-to-video examples
| | | |
|:---:|:---:|:---:|
| ![example1](./docs/_static/ltx-video_i2v_example_00001.gif) | ![example2](./docs/_static/ltx-video_i2v_example_00002.gif) | ![example3](./docs/_static/ltx-video_i2v_example_00003.gif) |
| ![example4](./docs/_static/ltx-video_i2v_example_00004.gif) | ![example5](./docs/_static/ltx-video_i2v_example_00005.gif) |  ![example6](./docs/_static/ltx-video_i2v_example_00006.gif) |
| ![example7](./docs/_static/ltx-video_i2v_example_00007.gif) |  ![example8](./docs/_static/ltx-video_i2v_example_00008.gif) | ![example9](./docs/_static/ltx-video_i2v_example_00009.gif) |

### Controlled video examples
| | | |
|:---:|:---:|:---:|
| ![control0](./docs/_static/ltx-video_ic_2v_example_00000.gif) | ![control1](./docs/_static/ltx-video_ic_2v_example_00001.gif) | ![control2](./docs/_static/ltx-video_ic_2v_example_00002.gif) |

| | |
|:---:|:---:|
| ![control3](./docs/_static/ltx-video_ic_2v_example_00003.gif) | ![control4](./docs/_static/ltx-video_ic_2v_example_00004.gif) |

# News

## July, 16th, 2025: New Distilled models v0.9.8 with up to 60 seconds of video:
- Long shot generation in LTXV-13B!
  * LTX-Video now supports up to 60 seconds of video.
  * Compatible also with the official IC-LoRAs.
  * Try now in [ComfyUI](https://github.com/Lightricks/ComfyUI-LTXVideo/tree/master/example_workflows/ltxv-13b-i2v-long-multi-prompt.json).
- Release a new distilled models:
  * 13B distilled model [ltxv-13b-0.9.8-distilled](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-distilled.yaml)
  * 2B distilled model [ltxv-2b-0.9.8-distilled](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.8-distilled.yaml)
  * Both models are distilled from the same base model [ltxv-13b-0.9.8-dev](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-dev.yaml) and are compatible for use together in the same multiscale pipeline.
  * Improved prompt understanding and detail generation
  * Includes corresponding FP8 weights and workflows.
- Release a new detailer model [LTX-Video-ICLoRA-detailer-13B-0.9.8](https://huggingface.co/Lightricks/LTX-Video-ICLoRA-detailer-13b-0.9.8)
  * Available in [ComfyUI](https://github.com/Lightricks/ComfyUI-LTXVideo/tree/master/example_workflows/ltxv-13b-upscale.json).

## July, 8th, 2025: New Control Models Released!
- Released three new control models for LTX-Video on HuggingFace:
    * **Depth Control**: [LTX-Video-ICLoRA-depth-13b-0.9.7](https://huggingface.co/Lightricks/LTX-Video-ICLoRA-depth-13b-0.9.7)
    * **Pose Control**: [LTX-Video-ICLoRA-pose-13b-0.9.7](https://huggingface.co/Lightricks/LTX-Video-ICLoRA-pose-13b-0.9.7)
    * **Canny Control**: [LTX-Video-ICLoRA-canny-13b-0.9.7](https://huggingface.co/Lightricks/LTX-Video-ICLoRA-canny-13b-0.9.7)


## May, 14th, 2025: New distilled model 13B v0.9.7:
- Release a new 13B distilled model [ltxv-13b-0.9.7-distilled](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled.safetensors)
    * Amazing for iterative work - generates HD videos in 10 seconds, with low-res preview after just 3 seconds (on H100)!
    * Does not require classifier-free guidance and spatio-temporal guidance.
    * Supports sampling with 8 (recommended), or less diffusion steps.
    * Also released a LoRA version of the distilled model, [ltxv-13b-0.9.7-distilled-lora128](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-lora128.safetensors)
        * Requires only 1GB of VRAM
        * Can be used with the full 13B model for fast inference
- Release a new quantized distilled model [ltxv-13b-0.9.7-distilled-fp8](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-fp8.safetensors) for *real-time* generation (on H100) with even less VRAM

## May, 5th, 2025: New model 13B v0.9.7:
- Release a new 13B model [ltxv-13b-0.9.7-dev](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev.safetensors)
- Release a new quantized model [ltxv-13b-0.9.7-dev-fp8](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev-fp8.safetensors) for faster inference with less VRam
- Release a new upscalers
  * [ltxv-temporal-upscaler-0.9.7](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-temporal-upscaler-0.9.7.safetensors)
  * [ltxv-spatial-upscaler-0.9.7](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-spatial-upscaler-0.9.7.safetensors)
- Breakthrough prompt adherence and physical understanding.
- New Pipeline for multi-scale video rendering for fast and high quality results


## April, 15th, 2025: New checkpoints v0.9.6:
- Release a new checkpoint [ltxv-2b-0.9.6-dev-04-25](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-dev-04-25.safetensors) with improved quality
- Release a new distilled model [ltxv-2b-0.9.6-distilled-04-25](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-distilled-04-25.safetensors)
    * 15x faster inference than non-distilled model.
    * Does not require classifier-free guidance and spatio-temporal guidance.
    * Supports sampling with 8 (recommended), or less diffusion steps.
- Improved prompt adherence, motion quality and fine details.
- New default resolution and FPS: 1216 √ó 704 pixels at 30 FPS
    * Still real time on H100 with the distilled model.
    * Other resolutions and FPS are still supported.
- Support stochastic inference (can improve visual quality when using the distilled model)

## March, 5th, 2025: New checkpoint v0.9.5
- New license for commercial use ([OpenRail-M](https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.5.license.txt))
- Release a new checkpoint v0.9.5 with improved quality
- Support keyframes and video extension
- Support higher resolutions
- Improved prompt understanding
- Improved VAE
- New online web app in [LTX-Studio](https://app.ltx.studio/ltx-video)
- Automatic prompt enhancement

## February, 20th, 2025: More inference options
- Improve STG (Spatiotemporal Guidance) for LTX-Video
- Support MPS on macOS with PyTorch 2.3.0
- Add support for 8-bit model, LTX-VideoQ8
- Add TeaCache for LTX-Video
- Add [ComfyUI-LTXTricks](#comfyui-integration)
- Add Diffusion-Pipe

## December 31st, 2024: Research paper
- Release the [research paper](https://arxiv.org/abs/2501.00103)

## December 20th, 2024: New checkpoint v0.9.1
- Release a new checkpoint v0.9.1 with improved quality
- Support for STG / PAG
- Support loading checkpoints of LTX-Video in Diffusers format (conversion is done on-the-fly)
- Support offloading unused parts to CPU
- Support the new timestep-conditioned VAE decoder
- Reference contributions from the community in the readme file
- Relax transformers dependency

## November 21th, 2024: Initial release v0.9.0
- Initial release of LTX-Video
- Support text-to-video and image-to-video generation


# Models &amp; Workflows

| Name                    | Notes                                                                                      | inference.py config                                                                                                                                      | ComfyUI workflow (Recommended) |
|-------------------------|--------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------|
| ltxv-13b-0.9.8-dev                   | Highest quality, requires more VRAM                                                        | [ltxv-13b-0.9.8-dev.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-dev.yaml)                                             | [ltxv-13b-i2v-base.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/ltxv-13b-i2v-base.json)             |
| [ltxv-13b-0.9.8-mix](https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b)            | Mix ltxv-13b-dev and ltxv-13b-distilled in the same multi-scale rendering workflow for balanced speed-quality | N/A                                             | [ltxv-13b-i2v-mixed-multiscale.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/ltxv-13b-i2v-mixed-multiscale.json)             |
 [ltxv-13b-0.9.8-distilled](https://app.ltx.studio/motion-workspace?videoModel=ltxv)        | Faster, less VRAM usage, slight quality reduction compared to 13b. Ideal for rapid iterations | [ltxv-13b-0.9.8-distilled.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-distilled.yaml)                                    | [ltxv-13b-dist-i2v-base.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base.json) |
ltxv-2b-0.9.8-distilled        | Smaller model, slight quality reduction compared to 13b distilled. Ideal for fast generation with light VRAM usage | [ltxv-2b-0.9.8-distilled.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.8-distilled.yaml)                                    | N/A |
| ltxv-13b-0.9.8-dev-fp8               | Quantized version of ltxv-13b | [ltxv-13b-0.9.8-dev-fp8.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-dev-fp8.yaml) | [ltxv-13b-i2v-base-fp8.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/ltxv-13b-i2v-base-fp8.json) |
| ltxv-13b-0.9.8-distilled-fp8     | Quantized version of ltxv-13b-distilled | [ltxv-13b-0.9.8-distilled-fp8.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-distilled-fp8.yaml) | [ltxv-13b-dist-i2v-base-fp8.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base-fp8.json) |
| ltxv-2b-0.9.8-distilled-fp8     | Quantized version of ltxv-2b-distilled | [ltxv-2b-0.9.8-distilled-fp8.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.8-distilled-fp8.yaml) | N/A |
| ltxv-2b-0.9.6                     | Good quality, lower VRAM requirement than ltxv-13b                                         | [ltxv-2b-0.9.6-dev.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.6-dev.yaml)                                                 | [ltxvideo-i2v.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/low_level/ltxvideo-i2v.json)             |
| ltxv-2b-0.9.6-distilled         | 15√ó faster, real-time capable, fewer steps needed, no STG/CFG required                     | [ltxv-2b-0.9.6-distilled.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.6-distilled.yaml)                                     | [ltxvideo-i2v-distilled.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/low_level/ltxvideo-i2v-distilled.json)             |


# Quick Start Guide

## Online inference
The model is accessible right away via the following links:
- [LTX-Studio image-to-video (13B-mix)](https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b)
- [LTX-Studio image-to-video (13B distilled)](https://app.ltx.studio/motion-workspace?videoModel=ltxv)
- [Fal.ai image-to-video (13B full)](https://fal.ai/models/fal-ai/ltx-video-13b-dev/image-to-video)
- [Fal.ai image-to-video (13B distilled)](https://fal.ai/models/fal-ai/ltx-video-13b-distilled/image-to-video)
- [Replicate image-to-video](https://replicate.com/lightricks/ltx-video)

## Run locally

### Installation
The codebase was tested with Python 3.10.5, CUDA version 12.2, and supports PyTorch &gt;= 2.1.2.
On macOS, MPS was tested with PyTorch 2.3.0, and should support PyTorch == 2.3 or &gt;= 2.6.

```bash
git clone https://github.com/Lightricks/LTX-Video.git
cd LTX-Video

# create env
python -m venv env
source env/bin/activate
python -m pip install -e .\[inference\]
```

#### FP8 Kernels (optional)

[FP8 kernels](https://github.com/Lightricks/LTXVideo-Q8-Kernels) developed for LTX-Video provide performance boost on supported graphics cards (Ada architecture and later). To install FP8 kernels, follow the instructions in that repository.

### Inference

üìù **Note:** For best results, we recommend using our [ComfyUI](#comfyui-integration) workflow. We&#039;re working on updating the inference.py script to match the high quality and output fidelity of ComfyUI.

To use our model, please follow the inference code in [inference.py](./inference.py):

#### For image-to-video generation:

```bash
python inference.py --prompt &quot;PROMPT&quot; --conditioning_media_paths IMAGE_PATH --conditioning_start_frames 0 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
```

#### Extending a video:

üìù **Note:** Input video segments must contain a multiple of 8 frames plus 1 (e.g., 9, 17, 25, etc.), and the target frame number should be a multiple of 8.


```bash
python inference.py --prompt &quot;PROMPT&quot; --conditioning_media_paths VIDEO_PATH --conditioning_start_frames START_FRAME --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
```

#### For video generation with multiple conditions:

You can now generate a video conditioned on a set of images and/or short video segments.
Simply provide a list of paths to the images or video segments you want to condition on, along with their target frame numbers in the generated video. You can also specify the conditioning strength for each item (default: 1.0).

```bash
python inference.py --prompt &quot;PROMPT&quot; --conditioning_media_paths IMAGE_OR_VIDEO_PATH_1 IMAGE_OR_VIDEO_PATH_2 --conditioning_start_frames TARGET_FRAME_1 TARGET_FRAME_2 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
```

### Using as a library

```python
from ltx_video.inference import infer, InferenceConfig

infer(
    InferenceConfig(
        pipeline_config=&quot;configs/ltxv-13b-0.9.8-distilled.yaml&quot;,
        prompt=PROMPT,
        height=HEIGHT,
        width=WIDTH,
        num_frames=NUM_FRAMES,
        output_path=&quot;output.mp4&quot;,
    )
)
```

## ComfyUI Integration
To use our model with ComfyUI, please follow the instructions at [https://github.com/Lightricks/ComfyUI-LTXVideo/](https://github.com/Lightricks/ComfyUI-LTXVideo/).

## Diffusers Integration
To use our model with the Diffusers Python library, check out the [official documentation](https://huggingface.co/docs/diffusers/main/en/api/pipelines/ltx_video).

Diffusers also support an 8-bit version of LTX-Video, [see details below](#ltx-videoq8)

# Model User Guide

## üìù Prompt Engineering

When writing prompts, focus on detailed, chronological descriptions of actions and scenes. Include specific movements, appearances, camera angles, and environmental details - all in a single flowing paragraph. Start directly with the action, and keep descriptions literal and precise. Think like a cinematographer describing a shot list. Keep within 200 words. For best results, build your prompts using this structure:

* Start with main action in a single sentence
* Add specific details about movements and gestures
* Describe character/object appearances precisely
* Include background and environment details
* Specify camera angles and movements
* Describe lighting and colors
* Note any changes or sudden events
* See [examples](#introduction) for more inspiration.

### Automatic Prompt Enhancement

When using `LTXVideoPipeline` directly, you can enable prompt enhancement by setting `enhance_prompt=True`.

## üéÆ Parameter Guide

* Resolution Preset: Higher resolutions for detailed scenes, lower for faster generation and simpler scenes. The model works on resolutions that are divisible by 32 and number of frames that are divisible by 8 + 1 (e.g. 257). In case the resolution or number of frames are not divisible by 32 or 8 + 1, the input will be padded with -1 and then cropped to the desired resolution and number of frames. The model works best on resolutions under 720 x 1280 and number of frames below 257
* Seed: Save seed values to recreate specific styles or compositions you like
* Guidance Scale: 3-3.5 are the recommended values
* Inference Steps: More steps (40+) for quality, fewer steps (20-30) for speed

üìù For advanced parameters usage, please see `python inference.py --help`

## Community Contribution

### ComfyUI-LTXTricks üõ†Ô∏è

A community project providing additional nodes for enhanced control over the LTX Video model. It includes implementations of advanced techniques like RF-Inversion, RF-Edit, FlowEdit, and more. These nodes enable workflows such as Image and Video to Video (I+V2V), enhanced sampling via Spatiotemporal Skip Guidance (STG), and interpolation with precise frame settings.

- **Repository:** [ComfyUI-LTXTricks](https://github.com/logtd/ComfyUI-LTXTricks)
- **Features:**
  - üîÑ **RF-Inversion:** Implements [RF-Inversion](https://rf-inversion.github.io/) with an [example workflow here](https://github.com/logtd/ComfyUI-LTXTricks/blob/main/example_workflows/example_ltx_inversion.json).
  - ‚úÇÔ∏è **RF-Edit:** Implements [RF-Solver-Edit](https://github.com/wangjiangshan0725/RF-Solver-Edit) with an [example workflow here](https://github.com/logtd/ComfyUI-LTXTricks/blob/main/example_workflows/example_ltx_rf_edit.json).
  - üåä **FlowEdit:** Implements [FlowEdit](https://github.com/fallenshock/FlowEdit) with an [example workflow here](https://github.com/logtd/ComfyUI-LTXTricks/blob/main/example_workflows/example_ltx_flow_edit.json).
  - üé• **I+V2V:** Enables Video to Video with a reference image. [Example workflow](https://github.com/logtd/ComfyUI-LTXTricks/blob/main/example_workflows/example_ltx_iv2v.json).
  - ‚ú® **Enhance:** Partial implementation of [STGuidance](https://junhahyung.github.io/STGuidance/). [Example workflow](https://github.com/logtd/ComfyUI-LTXTricks/blob/main/example_workflows/example_ltxv_stg.json).
  - üñºÔ∏è **Interpolation

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/markitdown]]></title>
            <link>https://github.com/microsoft/markitdown</link>
            <guid>https://github.com/microsoft/markitdown</guid>
            <pubDate>Sun, 20 Jul 2025 00:05:08 GMT</pubDate>
            <description><![CDATA[Python tool for converting files and office documents to Markdown.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/markitdown">microsoft/markitdown</a></h1>
            <p>Python tool for converting files and office documents to Markdown.</p>
            <p>Language: Python</p>
            <p>Stars: 68,373</p>
            <p>Forks: 3,632</p>
            <p>Stars today: 815 stars today</p>
            <h2>README</h2><pre># MarkItDown

[![PyPI](https://img.shields.io/pypi/v/markitdown.svg)](https://pypi.org/project/markitdown/)
![PyPI - Downloads](https://img.shields.io/pypi/dd/markitdown)
[![Built by AutoGen Team](https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue)](https://github.com/microsoft/autogen)

&gt; [!TIP]
&gt; MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See [markitdown-mcp](https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp) for more information.

&gt; [!IMPORTANT]
&gt; Breaking changes between 0.0.1 to 0.1.0:
&gt; * Dependencies are now organized into optional feature-groups (further details below). Use `pip install &#039;markitdown[all]&#039;` to have backward-compatible behavior. 
&gt; * convert\_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.
&gt; * The DocumentConverter class interface has changed to read from file-like streams rather than file paths. *No temporary files are created anymore*. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.

MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to [textract](https://github.com/deanmalmgren/textract), but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.

MarkItDown currently supports the conversion from:

- PDF
- PowerPoint
- Word
- Excel
- Images (EXIF metadata and OCR)
- Audio (EXIF metadata and speech transcription)
- HTML
- Text-based formats (CSV, JSON, XML)
- ZIP files (iterates over contents)
- Youtube URLs
- EPubs
- ... and more!

## Why Markdown?

Markdown is extremely close to plain text, with minimal markup or formatting, but still
provides a way to represent important document structure. Mainstream LLMs, such as
OpenAI&#039;s GPT-4o, natively &quot;_speak_&quot; Markdown, and often incorporate Markdown into their
responses unprompted. This suggests that they have been trained on vast amounts of
Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions
are also highly token-efficient.

## Prerequisites
MarkItDown requires Python 3.10 or higher. It is recommended to use a virtual environment to avoid dependency conflicts.

With the standard Python installation, you can create and activate a virtual environment using the following commands:

```bash
python -m venv .venv
source .venv/bin/activate
```

If using `uv`, you can create a virtual environment with:

```bash
uv venv --python=3.12 .venv
source .venv/bin/activate
# NOTE: Be sure to use &#039;uv pip install&#039; rather than just &#039;pip install&#039; to install packages in this virtual environment
```

If you are using Anaconda, you can create a virtual environment with:

```bash
conda create -n markitdown python=3.12
conda activate markitdown
```

## Installation

To install MarkItDown, use pip: `pip install &#039;markitdown[all]&#039;`. Alternatively, you can install it from the source:

```bash
git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e &#039;packages/markitdown[all]&#039;
```

## Usage

### Command-Line

```bash
markitdown path-to-file.pdf &gt; document.md
```

Or use `-o` to specify the output file:

```bash
markitdown path-to-file.pdf -o document.md
```

You can also pipe content:

```bash
cat path-to-file.pdf | markitdown
```

### Optional Dependencies
MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the `[all]` option. However, you can also install them individually for more control. For example:

```bash
pip install &#039;markitdown[pdf, docx, pptx]&#039;
```

will install only the dependencies for PDF, DOCX, and PPTX files.

At the moment, the following optional dependencies are available:

* `[all]` Installs all optional dependencies
* `[pptx]` Installs dependencies for PowerPoint files
* `[docx]` Installs dependencies for Word files
* `[xlsx]` Installs dependencies for Excel files
* `[xls]` Installs dependencies for older Excel files
* `[pdf]` Installs dependencies for PDF files
* `[outlook]` Installs dependencies for Outlook messages
* `[az-doc-intel]` Installs dependencies for Azure Document Intelligence
* `[audio-transcription]` Installs dependencies for audio transcription of wav and mp3 files
* `[youtube-transcription]` Installs dependencies for fetching YouTube video transcription

### Plugins

MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:

```bash
markitdown --list-plugins
```

To enable plugins use:

```bash
markitdown --use-plugins path-to-file.pdf
```

To find available plugins, search GitHub for the hashtag `#markitdown-plugin`. To develop a plugin, see `packages/markitdown-sample-plugin`.

### Azure Document Intelligence

To use Microsoft Document Intelligence for conversion:

```bash
markitdown path-to-file.pdf -o document.md -d -e &quot;&lt;document_intelligence_endpoint&gt;&quot;
```

More information about how to set up an Azure Document Intelligence Resource can be found [here](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0)

### Python API

Basic usage in Python:

```python
from markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert(&quot;test.xlsx&quot;)
print(result.text_content)
```

Document Intelligence conversion in Python:

```python
from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint=&quot;&lt;document_intelligence_endpoint&gt;&quot;)
result = md.convert(&quot;test.pdf&quot;)
print(result.text_content)
```

To use Large Language Models for image descriptions, provide `llm_client` and `llm_model`:

```python
from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model=&quot;gpt-4o&quot;)
result = md.convert(&quot;example.jpg&quot;)
print(result.text_content)
```

### Docker

```sh
docker build -t markitdown:latest .
docker run --rm -i markitdown:latest &lt; ~/your-file.pdf &gt; output.md
```

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

### How to Contribute

You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as &#039;open for contribution&#039; and &#039;open for reviewing&#039; to help facilitate community contributions. These are ofcourse just suggestions and you are welcome to contribute in any way you like.

&lt;div align=&quot;center&quot;&gt;

|            | All                                                          | Especially Needs Help from Community                                                                                                      |
| ---------- | ------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------- |
| **Issues** | [All Issues](https://github.com/microsoft/markitdown/issues) | [Issues open for contribution](https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22) |
| **PRs**    | [All PRs](https://github.com/microsoft/markitdown/pulls)     | [PRs open for reviewing](https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22)              |

&lt;/div&gt;

### Running Tests and Checks

- Navigate to the MarkItDown package:

  ```sh
  cd packages/markitdown
  ```

- Install `hatch` in your environment and run tests:

  ```sh
  pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
  hatch shell
  hatch test
  ```

  (Alternative) Use the Devcontainer which has all the dependencies installed:

  ```sh
  # Reopen the project in Devcontainer and run:
  hatch test
  ```

- Run pre-commit checks before submitting a PR: `pre-commit run --all-files`

### Contributing 3rd-party Plugins

You can also contribute by creating and sharing 3rd party plugins. See `packages/markitdown-sample-plugin` for more details.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
[Microsoft&#039;s Trademark &amp; Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party&#039;s policies.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[maxdorninger/MediaManager]]></title>
            <link>https://github.com/maxdorninger/MediaManager</link>
            <guid>https://github.com/maxdorninger/MediaManager</guid>
            <pubDate>Sun, 20 Jul 2025 00:05:07 GMT</pubDate>
            <description><![CDATA[A modern selfhosted media management system for your media library]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/maxdorninger/MediaManager">maxdorninger/MediaManager</a></h1>
            <p>A modern selfhosted media management system for your media library</p>
            <p>Language: Python</p>
            <p>Stars: 886</p>
            <p>Forks: 14</p>
            <p>Stars today: 72 stars today</p>
            <h2>README</h2><pre>&lt;br /&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://maxdorninger.github.io/MediaManager&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/maxdorninger/MediaManager/refs/heads/master/Writerside/images/logo.svg&quot; alt=&quot;Logo&quot; width=&quot;260&quot; height=&quot;260&quot;&gt;
  &lt;/a&gt;

&lt;h3 align=&quot;center&quot;&gt;MediaManager&lt;/h3&gt;

  &lt;p align=&quot;center&quot;&gt;
    Modern management system for your media library
    &lt;br /&gt;
    &lt;a href=&quot;https://maxdorninger.github.io/MediaManager/introduction.html&quot;&gt;&lt;strong&gt;Explore the docs ¬ª&lt;/strong&gt;&lt;/a&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://github.com/maxdorninger/MediaManager/issues/new?labels=bug&amp;template=bug_report.md&quot;&gt;Report Bug&lt;/a&gt;
    &amp;middot;
    &lt;a href=&quot;https://github.com/maxdorninger/MediaManager/issues/new?template=feature_request.md&quot;&gt;Request Feature&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;


MediaManager is modern software to manage your TV and movie library. It is designed to be a replacement for Sonarr,
Radarr, Overseer, and Jellyseer.
It supports TVDB and TMDB for metadata, supports OIDC and OAuth 2.0 for authentication and supports Prowlarr and
Jackett.
MediaManager is built first and foremost for deployment with Docker, making it easy to set up.

It also provides an API to interact with the software programmatically, allowing for automation and integration with
other services.

## Quick Start

```sh
wget -O docker-compose.yaml https://raw.githubusercontent.com/maxdorninger/MediaManager/refs/heads/master/docker-compose.yaml   
mkdir config
wget -O ./config/config.toml https://raw.githubusercontent.com/maxdorninger/MediaManager/refs/heads/master/config.example.toml   
# you probably need to edit the config.toml file in the ./config directory, for more help see the documentation
docker compose up -d
```

### [View the docs for installation instructions and more](https://maxdorninger.github.io/MediaManager/configuration-overview.html#configuration-overview)

## Support MediaManager

&lt;a href=&quot;https://github.com/sponsors/maxdorninger&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Sponsor-Maximilian Dorninger-orange&quot; alt=&quot;Sponsor @maxdorninger&quot; /&gt;
&lt;/a&gt;

&lt;a href=&quot;https://buymeacoffee.com/maxdorninger&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png&quot; alt=&quot;Buy Me A Coffee&quot; style=&quot;height: 41px !important;width: 174px !important;box-shadow: 0px 3px 2px 0px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 3px 2px 0px rgba(190, 190, 190, 0.5) !important;&quot; &gt;
&lt;/a&gt;

## Check out the awesome sponsors of MediaManager ‚ù§Ô∏è

&lt;a href=&quot;https://fosstodon.org/@aljazmerzen&quot;&gt;&lt;img src=&quot;https://github.com/aljazerzen.png&quot; width=&quot;80px&quot; alt=&quot;Alja≈æ Mur Er≈æen&quot; /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
&lt;a href=&quot;https://github.com/ldrrp&quot;&gt;&lt;img src=&quot;https://github.com/ldrrp.png&quot; width=&quot;80px&quot; alt=&quot;Luis Rodriguez&quot; /&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;


## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=maxdorninger/MediaManager&amp;type=Date)](https://www.star-history.com/#maxdorninger/MediaManager&amp;Date)


## Roadmap

- [x] support for more torrent indexers
- [x] fully automatic downloads
- [x] add tests
- [x] add more logs/errors
- [x] make API return proper error codes
- [x] optimize images for web in the backend
- [x] responsive ui
- [x] automatically update metadata of shows
- [x] automatically download new seasons/episodes of shows
- [x] add fallback to just copy files if hardlinks don&#039;t work
- [x] add check at startup if hardlinks work
- [x] create separate metadata relay service, so that api keys for TMDB and TVDB are not strictly needed
- [x] support for movies
- [x] expand README with more information and a quickstart guide
- [x] improve reliability of scheduled tasks
- [x] add notification system
- [x] add sequence diagrams to the documentation
- [x] add usenet support
- [x] provide example configuration files
- [x] make media sorting algorithm configurable
- [x] add support for transmission
- [x] add support for configuration via toml/yaml config file
- [ ] add in-depth documentation on the architecture of the codebase
- [ ] make indexer module multithreaded
- [ ] add delete button for movies/TV shows
- [ ] rework prowlarr module (select which indexers to use, etc.)
- [ ] _maybe_ rework the logo

See the [open issues](hhttps://maxdorninger.github.io/MediaManager/issues) for a full list of proposed features (and known issues).

## Screenshots

![Screenshot 2025-07-02 174732](https://github.com/user-attachments/assets/49fc18aa-b471-4be8-983e-c0ab240dfb73)
![Screenshot 2025-07-02 174342](https://github.com/user-attachments/assets/3a38953d-d0fa-4a7e-83d0-dd6e6427681c)
![Screenshot 2025-07-02 174616](https://github.com/user-attachments/assets/c3af4be8-b873-448c-8a4d-0d5db863aec7)
![Screenshot 2025-07-02 174416](https://github.com/user-attachments/assets/0d50f53b-64da-4243-8408-1d6fc85fe81b)
![Screenshot 2025-06-28 222908](https://github.com/user-attachments/assets/193e1afd-dabb-42a2-ab28-59f2784371c7)

## Developer Quick Start

For the developer guide see the [Developer Guide](https://maxdorninger.github.io/MediaManager/developer-guide.html).

&lt;!-- LICENSE --&gt;

## License

Distributed under the AGPL 3.0. See `LICENSE.txt` for more information.


&lt;!-- ACKNOWLEDGMENTS --&gt;

## Acknowledgments

* [Thanks to Pawel Czerwinski for the image on the login screen](https://unsplash.com/@pawel_czerwinski)

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[All-Hands-AI/OpenHands]]></title>
            <link>https://github.com/All-Hands-AI/OpenHands</link>
            <guid>https://github.com/All-Hands-AI/OpenHands</guid>
            <pubDate>Sun, 20 Jul 2025 00:05:06 GMT</pubDate>
            <description><![CDATA[üôå OpenHands: Code Less, Make More]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/All-Hands-AI/OpenHands">All-Hands-AI/OpenHands</a></h1>
            <p>üôå OpenHands: Code Less, Make More</p>
            <p>Language: Python</p>
            <p>Stars: 60,776</p>
            <p>Forks: 7,162</p>
            <p>Stars today: 50 stars today</p>
            <h2>README</h2><pre>&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./docs/static/img/logo.png&quot; alt=&quot;Logo&quot; width=&quot;200&quot;&gt;
  &lt;h1 align=&quot;center&quot;&gt;OpenHands: Code Less, Make More&lt;/h1&gt;
&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/All-Hands-AI/OpenHands/graphs/contributors&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/All-Hands-AI/OpenHands?style=for-the-badge&amp;color=blue&quot; alt=&quot;Contributors&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/All-Hands-AI/OpenHands/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/All-Hands-AI/OpenHands?style=for-the-badge&amp;color=blue&quot; alt=&quot;Stargazers&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/All-Hands-AI/OpenHands/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/All-Hands-AI/OpenHands?style=for-the-badge&amp;color=blue&quot; alt=&quot;MIT License&quot;&gt;&lt;/a&gt;
  &lt;br/&gt;
  &lt;a href=&quot;https://join.slack.com/t/openhands-ai/shared_invite/zt-3847of6xi-xuYJIPa6YIPg4ElbDWbtSA&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Slack-Join%20Us-red?logo=slack&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Join our Slack community&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/ESHStjSjD4&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Join our Discord community&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/All-Hands-AI/OpenHands/blob/main/CREDITS.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Project-Credits-blue?style=for-the-badge&amp;color=FFE165&amp;logo=github&amp;logoColor=white&quot; alt=&quot;Credits&quot;&gt;&lt;/a&gt;
  &lt;br/&gt;
  &lt;a href=&quot;https://docs.all-hands.dev/usage/getting-started&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;logoColor=FFE165&amp;style=for-the-badge&quot; alt=&quot;Check out the documentation&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://arxiv.org/abs/2407.16741&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Paper%20on%20Arxiv-000?logoColor=FFE165&amp;logo=arxiv&amp;style=for-the-badge&quot; alt=&quot;Paper on Arxiv&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1wOUdFCMyY6Nt0AIqF705KN4JKOWgeI4wUGUP60krXXs/edit?gid=0#gid=0&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Benchmark%20score-000?logoColor=FFE165&amp;logo=huggingface&amp;style=for-the-badge&quot; alt=&quot;Evaluation Benchmark Score&quot;&gt;&lt;/a&gt;

  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=de&quot;&gt;Deutsch&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=es&quot;&gt;Espa√±ol&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=fr&quot;&gt;fran√ßais&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=ja&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=ko&quot;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=pt&quot;&gt;Portugu√™s&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=ru&quot;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=zh&quot;&gt;‰∏≠Êñá&lt;/a&gt;

  &lt;hr&gt;
&lt;/div&gt;

Welcome to OpenHands (formerly OpenDevin), a platform for software development agents powered by AI.

OpenHands agents can do anything a human developer can: modify code, run commands, browse the web,
call APIs, and yes‚Äîeven copy code snippets from StackOverflow.

Learn more at [docs.all-hands.dev](https://docs.all-hands.dev), or [sign up for OpenHands Cloud](https://app.all-hands.dev) to get started.

&gt; [!IMPORTANT]
&gt; Using OpenHands for work? We&#039;d love to chat! Fill out
&gt; [this short form](https://docs.google.com/forms/d/e/1FAIpQLSet3VbGaz8z32gW9Wm-Grl4jpt5WgMXPgJ4EDPVmCETCBpJtQ/viewform)
&gt; to join our Design Partner program, where you&#039;ll get early access to commercial features and the opportunity to provide input on our product roadmap.

![App screenshot](./docs/static/img/screenshot.png)

## ‚òÅÔ∏è OpenHands Cloud
The easiest way to get started with OpenHands is on [OpenHands Cloud](https://app.all-hands.dev),
which comes with $20 in free credits for new users.

## üíª Running OpenHands Locally

OpenHands can also run on your local system using Docker.
See the [Running OpenHands](https://docs.all-hands.dev/usage/installation) guide for
system requirements and more information.

&gt; [!WARNING]
&gt; On a public network? See our [Hardened Docker Installation Guide](https://docs.all-hands.dev/usage/runtimes/docker#hardened-docker-installation)
&gt; to secure your deployment by restricting network binding and implementing additional security measures.


```bash
docker pull docker.all-hands.dev/all-hands-ai/runtime:0.49-nikolaik

docker run -it --rm --pull=always \
    -e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.49-nikolaik \
    -e LOG_ALL_EVENTS=true \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -v ~/.openhands:/.openhands \
    -p 3000:3000 \
    --add-host host.docker.internal:host-gateway \
    --name openhands-app \
    docker.all-hands.dev/all-hands-ai/openhands:0.49
```

&gt; **Note**: If you used OpenHands before version 0.44, you may want to run `mv ~/.openhands-state ~/.openhands` to migrate your conversation history to the new location.

You&#039;ll find OpenHands running at [http://localhost:3000](http://localhost:3000)!

When you open the application, you&#039;ll be asked to choose an LLM provider and add an API key.
[Anthropic&#039;s Claude Sonnet 4](https://www.anthropic.com/api) (`anthropic/claude-sonnet-4-20250514`)
works best, but you have [many options](https://docs.all-hands.dev/usage/llms).

## üí° Other ways to run OpenHands

&gt; [!WARNING]
&gt; OpenHands is meant to be run by a single user on their local workstation.
&gt; It is not appropriate for multi-tenant deployments where multiple users share the same instance. There is no built-in authentication, isolation, or scalability.
&gt;
&gt; If you&#039;re interested in running OpenHands in a multi-tenant environment, check out the source-available, commercially-licensed
&gt; [OpenHands Cloud Helm Chart](https://github.com/all-Hands-AI/OpenHands-cloud)

You can [connect OpenHands to your local filesystem](https://docs.all-hands.dev/usage/runtimes/docker#connecting-to-your-filesystem),
run OpenHands in a scriptable [headless mode](https://docs.all-hands.dev/usage/how-to/headless-mode),
interact with it via a [friendly CLI](https://docs.all-hands.dev/usage/how-to/cli-mode),
or run it on tagged issues with [a github action](https://docs.all-hands.dev/usage/how-to/github-action).

Visit [Running OpenHands](https://docs.all-hands.dev/usage/installation) for more information and setup instructions.

If you want to modify the OpenHands source code, check out [Development.md](https://github.com/All-Hands-AI/OpenHands/blob/main/Development.md).

Having issues? The [Troubleshooting Guide](https://docs.all-hands.dev/usage/troubleshooting) can help.

## üìñ Documentation
  &lt;a href=&quot;https://deepwiki.com/All-Hands-AI/OpenHands&quot;&gt;&lt;img src=&quot;https://deepwiki.com/badge.svg&quot; alt=&quot;Ask DeepWiki&quot; title=&quot;Autogenerated Documentation by DeepWiki&quot;&gt;&lt;/a&gt;

To learn more about the project, and for tips on using OpenHands,
check out our [documentation](https://docs.all-hands.dev/usage/getting-started).

There you&#039;ll find resources on how to use different LLM providers,
troubleshooting resources, and advanced configuration options.

## ü§ù How to Join the Community

OpenHands is a community-driven project, and we welcome contributions from everyone. We do most of our communication
through Slack, so this is the best place to start, but we also are happy to have you contact us on Discord or Github:

- [Join our Slack workspace](https://join.slack.com/t/openhands-ai/shared_invite/zt-3847of6xi-xuYJIPa6YIPg4ElbDWbtSA) - Here we talk about research, architecture, and future development.
- [Join our Discord server](https://discord.gg/ESHStjSjD4) - This is a community-run server for general discussion, questions, and feedback.
- [Read or post Github Issues](https://github.com/All-Hands-AI/OpenHands/issues) - Check out the issues we&#039;re working on, or add your own ideas.

See more about the community in [COMMUNITY.md](./COMMUNITY.md) or find details on contributing in [CONTRIBUTING.md](./CONTRIBUTING.md).

## üìà Progress

See the monthly OpenHands roadmap [here](https://github.com/orgs/All-Hands-AI/projects/1) (updated at the maintainer&#039;s meeting at the end of each month).

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://star-history.com/#All-Hands-AI/OpenHands&amp;Date&quot;&gt;
    &lt;img src=&quot;https://api.star-history.com/svg?repos=All-Hands-AI/OpenHands&amp;type=Date&quot; width=&quot;500&quot; alt=&quot;Star History Chart&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

## üìú License

Distributed under the MIT License. See [`LICENSE`](./LICENSE) for more information.

## üôè Acknowledgements

OpenHands is built by a large number of contributors, and every contribution is greatly appreciated! We also build upon other open source projects, and we are deeply thankful for their work.

For a list of open source projects and licenses used in OpenHands, please see our [CREDITS.md](./CREDITS.md) file.

## üìö Cite

```
@inproceedings{
  wang2025openhands,
  title={OpenHands: An Open Platform for {AI} Software Developers as Generalist Agents},
  author={Xingyao Wang and Boxuan Li and Yufan Song and Frank F. Xu and Xiangru Tang and Mingchen Zhuge and Jiayi Pan and Yueqi Song and Bowen Li and Jaskirat Singh and Hoang H. Tran and Fuqiang Li and Ren Ma and Mingzhang Zheng and Bill Qian and Yanjun Shao and Niklas Muennighoff and Yizhe Zhang and Binyuan Hui and Junyang Lin and Robert Brennan and Hao Peng and Heng Ji and Graham Neubig},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=OJd3ayDDoF}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[subframe7536/maple-font]]></title>
            <link>https://github.com/subframe7536/maple-font</link>
            <guid>https://github.com/subframe7536/maple-font</guid>
            <pubDate>Sun, 20 Jul 2025 00:05:05 GMT</pubDate>
            <description><![CDATA[Maple Mono: Open source monospace font with round corner, ligatures and Nerd-Font icons for IDE and terminal, fine-grained customization options. Â∏¶ËøûÂ≠óÂíåÊéßÂà∂Âè∞ÂõæÊ†áÁöÑÂúÜËßíÁ≠âÂÆΩÂ≠ó‰ΩìÔºå‰∏≠Ëã±ÊñáÂÆΩÂ∫¶ÂÆåÁæé2:1ÔºåÁªÜÁ≤íÂ∫¶ÁöÑËá™ÂÆö‰πâÈÄâÈ°π]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/subframe7536/maple-font">subframe7536/maple-font</a></h1>
            <p>Maple Mono: Open source monospace font with round corner, ligatures and Nerd-Font icons for IDE and terminal, fine-grained customization options. Â∏¶ËøûÂ≠óÂíåÊéßÂà∂Âè∞ÂõæÊ†áÁöÑÂúÜËßíÁ≠âÂÆΩÂ≠ó‰ΩìÔºå‰∏≠Ëã±ÊñáÂÆΩÂ∫¶ÂÆåÁæé2:1ÔºåÁªÜÁ≤íÂ∫¶ÁöÑËá™ÂÆö‰πâÈÄâÈ°π</p>
            <p>Language: Python</p>
            <p>Stars: 18,231</p>
            <p>Forks: 614</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre>&lt;br&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./img/head.svg&quot; height=&quot;230&quot; alt=&quot;logo&quot;&gt;
&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt; Maple Font &lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
Open source monospace &amp; nerd font with round corners and ligatures.
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/subframe7536/Maple-font/releases&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/v/release/subframe7536/Maple-font?display_name=tag&quot; alt=&quot;release version&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;#install&quot;&gt;install&lt;/a&gt; |
  &lt;a href=&quot;https://github.com/users/subframe7536/projects/1&quot;&gt;what&#039;s next&lt;/a&gt; |
  English |
  &lt;a href=&quot;./README_CN.md&quot;&gt;‰∏≠Êñá&lt;/a&gt;
&lt;/p&gt;

## Preparing for [V7](https://github.com/subframe7536/maple-font/tree/variable), try the new variable font at [latest release](https://github.com/subframe7536/maple-font/releases)

## Features

Inspired by [Source Code Pro](https://github.com/adobe-fonts/source-code-pro), [Fira Code Retina](https://github.com/tonsky/FiraCode), [Sarasa Mono SC Nerd](https://github.com/laishulu/Sarasa-Mono-SC-Nerd) and so on, but:

- üé® **New shape** - such as `@ # $ % &amp;` and new shape of italic style
- ü§ôüèª **More ligatures** - such as `.., ..., /*, /**`
- üì¶ **Small size** - leave only contains Latin, standard set of accents, table control characters and few symbols
- ü¶æ **Better rendering effect** - redesigned it according to Fira Code Retina&#039;s spacing and glyph

  |                           v4                           |                           v5                            |
  | :----------------------------------------------------: | :-----------------------------------------------------: |
  | &lt;img src=&quot;./img/sizechange.gif&quot; height=&quot;200&quot; alt=&quot;v4&quot;&gt; | &lt;img src=&quot;./img/sizechange1.gif&quot; height=&quot;200&quot; alt=&quot;v5&quot;&gt; |
  |     `+` and `=` are not centered at some font-size     |             `+` and `=` are always centered             |

- üóí **More readable** - cursive style, better glyph shape, lower the height of capital letters and numbers, reduce or modify kerning and center operators `+ - * = ^ ~ &lt; &gt;`
- üõ†Ô∏è **More configurable** - enable or disable font features as you want, just make your own font
- ‚ú® See it in [screenshots](#screenshots)



## Install

### V6

| Platform   | Command                                                                          |
| :--------- | :------------------------------------------------------------------------------- |
| macOS      | `brew install --cask font-maple`                                                 |
| Arch Linux | `paru -S ttf-maple`                                                              |
| Others     | Download in [releases](https://github.com/subframe7536/Maple-font/releases/v6.4) |

### V7 Beta

| Platform   | Command                  |
| :--------- | :----------------------- |
| Arch Linux | `paru -S ttf-maple-beta` |


## Notice


Because I don&#039;t have a Mac OS machine, this is the greatest adaption I can do with Mac OS currently, but I can&#039;t test whether it works.

My ability is not enough to solve other problems on Mac OS. I will record the problem and try to solve it, and **PR welcome!**

`Maple Mono NF` now maybe can&#039;t be recognized as Mono, and I try my best but it doesn&#039;t work orz


## Overview

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;./img/base.png&quot; /&gt;&lt;br&gt;
&lt;img src=&quot;./img/ligature.png&quot; /&gt;&lt;br&gt;
&lt;img src=&quot;./img/ligature.gif&quot;/&gt;&lt;br&gt;
multiple ways to get TODO tag&lt;br&gt;
ps: in JetBrains&#039; product, [todo) can&#039;t be properly rendered, so please use todo))&lt;br&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;./img/option.png&quot;/&gt;&lt;br&gt;
&lt;h3 align=&quot;center&quot;&gt;font features are different in V7, see in &lt;a href=&quot;https://github.com/subframe7536/maple-font/tree/variable?tab=readme-ov-file#features&quot;&gt;docs&lt;/h3&gt;&lt;br/&gt;
Compatibility &amp; usage: in &lt;a href=&quot;https://github.com/tonsky/FiraCode#editor-compatibility-list&quot; target=&quot;_blank&quot;&gt;FiraCode README&lt;/a&gt;
&lt;/p&gt;

## Screenshots

Code theme: [vscode-theme-maple](https://github.com/subframe7536/vscode-theme-maple)

~~generate by: [VSCodeSnap](https://github.com/luisllamasbinaburo/VSCodeSnap)~~ Seems deprecated, so I made a new one: [CodeImg](https://github.com/subframe7536/vscode-codeimg)

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Cli (click to expand!)&lt;/b&gt;&lt;/summary&gt;

![](img/code_sample/cli.webp)

&lt;/details&gt;



&lt;details&gt;
&lt;summary&gt;&lt;b&gt;React&lt;/b&gt;&lt;/summary&gt;

![](img/code_sample/react.webp)

&lt;/details&gt;



&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Vue&lt;/b&gt;&lt;/summary&gt;

![](img/code_sample/vue.webp)

&lt;/details&gt;


&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Java&lt;/b&gt;&lt;/summary&gt;

![](img/code_sample/java.webp)

&lt;/details&gt;


&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Go&lt;/b&gt;&lt;/summary&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;img/code_sample/go.webp&quot; width=&quot;540px&quot;/&gt;
&lt;/p&gt;

&lt;/details&gt;


&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Python&lt;/b&gt;&lt;/summary&gt;

![](img/code_sample/python.webp)

&lt;/details&gt;


&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Rust&lt;/b&gt;&lt;/summary&gt;

![](img/code_sample/rust.webp)


&lt;/details&gt;


## Build your own font

See [doc](./source/README.md)

## Donate

If this was helpful to you, please feel free to buy me a coffee

&lt;a href=&quot;https://www.buymeacoffee.com/subframe753&quot;&gt;&lt;img src=&quot;https://img.buymeacoffee.com/button-api/?text=Buy me a coffee&amp;emoji=&amp;slug=subframe753&amp;button_colour=5F7FFF&amp;font_colour=ffffff&amp;font_family=Lato&amp;outline_colour=000000&amp;coffee_colour=FFDD00&quot; /&gt;&lt;/a&gt;

![](img/donate.webp)

## License

SIL Open Font License 1.1
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[QuivrHQ/MegaParse]]></title>
            <link>https://github.com/QuivrHQ/MegaParse</link>
            <guid>https://github.com/QuivrHQ/MegaParse</guid>
            <pubDate>Sun, 20 Jul 2025 00:05:04 GMT</pubDate>
            <description><![CDATA[File Parser optimised for LLM Ingestion with no loss üß† Parse PDFs, Docx, PPTx in a format that is ideal for LLMs.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/QuivrHQ/MegaParse">QuivrHQ/MegaParse</a></h1>
            <p>File Parser optimised for LLM Ingestion with no loss üß† Parse PDFs, Docx, PPTx in a format that is ideal for LLMs.</p>
            <p>Language: Python</p>
            <p>Stars: 6,876</p>
            <p>Forks: 369</p>
            <p>Stars today: 97 stars today</p>
            <h2>README</h2><pre># MegaParse - Your Parser for every type of documents

&lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/QuivrHQ/MegaParse/main/logo.png&quot; alt=&quot;Quivr-logo&quot; width=&quot;30%&quot;  style=&quot;border-radius: 50%; padding-bottom: 20px&quot;/&gt;
&lt;/div&gt;

MegaParse is a powerful and versatile parser that can handle various types of documents with ease. Whether you&#039;re dealing with text, PDFs, Powerpoint presentations, Word documents MegaParse has got you covered. Focus on having no information loss during parsing.

## Key Features üéØ

- **Versatile Parser**: MegaParse is a powerful and versatile parser that can handle various types of documents with ease.
- **No Information Loss**: Focus on having no information loss during parsing.
- **Fast and Efficient**: Designed with speed and efficiency at its core.
- **Wide File Compatibility**: Supports Text, PDF, Powerpoint presentations, Excel, CSV, Word documents.
- **Open Source**: Freedom is beautiful, and so is MegaParse. Open source and free to use.

## Support

- Files: ‚úÖ PDF ‚úÖ Powerpoint ‚úÖ Word
- Content: ‚úÖ Tables ‚úÖ TOC ‚úÖ Headers ‚úÖ Footers ‚úÖ Images

### Example

https://github.com/QuivrHQ/MegaParse/assets/19614572/1b4cdb73-8dc2-44ef-b8b4-a7509bc8d4f3

## Installation

required python version &gt;= 3.11

```bash
pip install megaparse
```

## Usage

1. Add your OpenAI or Anthropic API key to the .env file

2. Install poppler on your computer (images and PDFs)

3. Install tesseract on your computer (images and PDFs)

4. If you have a mac, you also need to install libmagic ```brew install libmagic```

Use MegaParse as it is : 
```python
from megaparse import MegaParse
from langchain_openai import ChatOpenAI

megaparse = MegaParse()
response = megaparse.load(&quot;./test.pdf&quot;)
print(response)
```

### Use MegaParse Vision

```python
from megaparse.parser.megaparse_vision import MegaParseVision

model = ChatOpenAI(model=&quot;gpt-4o&quot;, api_key=os.getenv(&quot;OPENAI_API_KEY&quot;))  # type: ignore
parser = MegaParseVision(model=model)
response = parser.convert(&quot;./test.pdf&quot;)
print(response)

```
**Note**: The model supported by MegaParse Vision are the multimodal ones such as claude 3.5, claude 4, gpt-4o and gpt-4.

## Use as an API
There is a MakeFile for you, simply use :
```make dev```
at the root of the project and you are good to go.

See localhost:8000/docs for more info on the different endpoints !

## BenchMark

&lt;!---BENCHMARK--&gt;
| Parser                        | similarity_ratio |
| ----------------------------- | ---------------- |
| megaparse_vision              | 0.87             |
| unstructured_with_check_table | 0.77             |
| unstructured                  | 0.59             |
| llama_parser                  | 0.33             |
&lt;!---END_BENCHMARK--&gt;

_Higher the better_

Note: Want to evaluate and compare your Megaparse module with ours ? Please add your config in ```evaluations/script.py``` and then run ```python evaluations/script.py```. If it is better, do a PR, I mean, let&#039;s go higher together .

## In Construction üöß
- Improve table checker
- Create Checkers to add **modular postprocessing** ‚öôÔ∏è
- Add Structured output, **let&#039;s get computer talking** ü§ñ



## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=QuivrHQ/MegaParse&amp;type=Date)](https://star-history.com/#QuivrHQ/MegaParse&amp;Date)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[1Panel-dev/MaxKB]]></title>
            <link>https://github.com/1Panel-dev/MaxKB</link>
            <guid>https://github.com/1Panel-dev/MaxKB</guid>
            <pubDate>Sun, 20 Jul 2025 00:05:03 GMT</pubDate>
            <description><![CDATA[üî• MaxKB is an open-source platform for building enterprise-grade agents. MaxKB ÊòØÂº∫Â§ßÊòìÁî®ÁöÑÂºÄÊ∫ê‰ºÅ‰∏öÁ∫ßÊô∫ËÉΩ‰ΩìÂπ≥Âè∞„ÄÇ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/1Panel-dev/MaxKB">1Panel-dev/MaxKB</a></h1>
            <p>üî• MaxKB is an open-source platform for building enterprise-grade agents. MaxKB ÊòØÂº∫Â§ßÊòìÁî®ÁöÑÂºÄÊ∫ê‰ºÅ‰∏öÁ∫ßÊô∫ËÉΩ‰ΩìÂπ≥Âè∞„ÄÇ</p>
            <p>Language: Python</p>
            <p>Stars: 17,214</p>
            <p>Forks: 2,242</p>
            <p>Stars today: 14 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;img src= &quot;https://github.com/1Panel-dev/maxkb/assets/52996290/c0694996-0eed-40d8-b369-322bf2a380bf&quot; alt=&quot;MaxKB&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;
&lt;h3 align=&quot;center&quot;&gt;Open-source platform for building enterprise-grade agents&lt;/h3&gt;
&lt;h3 align=&quot;center&quot;&gt;Âº∫Â§ßÊòìÁî®ÁöÑ‰ºÅ‰∏öÁ∫ßÊô∫ËÉΩ‰ΩìÂπ≥Âè∞&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://trendshift.io/repositories/9113&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/9113&quot; alt=&quot;1Panel-dev%2FMaxKB | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.gnu.org/licenses/gpl-3.0.html#license-text&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/1Panel-dev/maxkb?color=%231890FF&quot; alt=&quot;License: GPL v3&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/1Panel-dev/maxkb/releases/latest&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/1Panel-dev/maxkb&quot; alt=&quot;Latest release&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/1Panel-dev/maxkb&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/1Panel-dev/maxkb?color=%231890FF&amp;style=flat-square&quot; alt=&quot;Stars&quot;&gt;&lt;/a&gt;    
  &lt;a href=&quot;https://hub.docker.com/r/1panel/maxkb&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/1panel/maxkb?label=downloads&quot; alt=&quot;Download&quot;&gt;&lt;/a&gt;&lt;br/&gt;
 [&lt;a href=&quot;/README_CN.md&quot;&gt;‰∏≠Êñá(ÁÆÄ‰Ωì)&lt;/a&gt;] | [&lt;a href=&quot;/README.md&quot;&gt;English&lt;/a&gt;] 
&lt;/p&gt;
&lt;hr/&gt;

MaxKB = Max Knowledge Brain, it is an open-source platform for building enterprise-grade agents. MaxKB integrates Retrieval-Augmented Generation (RAG) pipelines, supports robust workflows, and provides advanced MCP tool-use capabilities. MaxKB is widely applied in scenarios such as intelligent customer service, corporate internal knowledge bases, academic research, and education.

- **RAG Pipeline**: Supports direct uploading of documents / automatic crawling of online documents, with features for automatic text splitting, vectorization. This effectively reduces hallucinations in large models, providing a superior smart Q&amp;A interaction experience.
- **Agentic Workflow**: Equipped with a powerful workflow engine, function library and MCP tool-use, enabling the orchestration of AI processes to meet the needs of complex business scenarios. 
- **Seamless Integration**: Facilitates zero-coding rapid integration into third-party business systems, quickly equipping existing systems with intelligent Q&amp;A capabilities to enhance user satisfaction.
- **Model-Agnostic**: Supports various large models, including private models (such as DeepSeek, Llama, Qwen, etc.) and public models (like OpenAI, Claude, Gemini, etc.).
- **Multi Modal**: Native support for input and output text, image, audio and video.

## Quick start

Execute the script below to start a MaxKB container using Docker:

```bash
docker run -d --name=maxkb --restart=always -p 8080:8080 -v ~/.maxkb:/var/lib/postgresql/data -v ~/.python-packages:/opt/maxkb/app/sandbox/python-packages 1panel/maxkb:v1
```

Access MaxKB web interface at `http://your_server_ip:8080` with default admin credentials:

- username: admin
- password: MaxKB@123..

‰∏≠ÂõΩÁî®Êà∑Â¶ÇÈÅáÂà∞ Docker ÈïúÂÉè Pull Â§±Ë¥•ÈóÆÈ¢òÔºåËØ∑ÂèÇÁÖßËØ• [Á¶ªÁ∫øÂÆâË£ÖÊñáÊ°£](https://maxkb.cn/docs/v1/installation/offline_installtion/) ËøõË°åÂÆâË£Ö„ÄÇ

## Screenshots

&lt;table style=&quot;border-collapse: collapse; border: 1px solid black;&quot;&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://maxkb.hk/images/overview.png&quot; alt=&quot;MaxKB Demo1&quot;   /&gt;&lt;/td&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://maxkb.hk/images/screenshot-models.png&quot; alt=&quot;MaxKB Demo2&quot;   /&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://maxkb.hk/images/screenshot-knowledge.png&quot; alt=&quot;MaxKB Demo3&quot;   /&gt;&lt;/td&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://maxkb.hk/images/screenshot-function.png&quot; alt=&quot;MaxKB Demo4&quot;   /&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## Technical stack

- FrontendÔºö[Vue.js](https://vuejs.org/)
- BackendÔºö[Python / Django](https://www.djangoproject.com/)
- LLM FrameworkÔºö[LangChain](https://www.langchain.com/)
- DatabaseÔºö[PostgreSQL + pgvector](https://www.postgresql.org/)

## Feature Comparison

&lt;table style=&quot;width: 100%;&quot;&gt;
  &lt;tr&gt;
    &lt;th align=&quot;center&quot;&gt;Feature&lt;/th&gt;
    &lt;th align=&quot;center&quot;&gt;LangChain&lt;/th&gt;
    &lt;th align=&quot;center&quot;&gt;Dify.AI&lt;/th&gt;
    &lt;th align=&quot;center&quot;&gt;Flowise&lt;/th&gt;
    &lt;th align=&quot;center&quot;&gt;MaxKB &lt;br&gt;ÔºàBuilt upon LangChainÔºâ&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;Supported LLMs&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;Rich Variety&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;Rich Variety&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;Rich Variety&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;Rich Variety&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;RAG Engine&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;Agent&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚ùå&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;Workflow&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚ùå&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;Observability&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚ùå&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;SSO/Access control&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚ùå&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚ùå&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ (Pro)&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;On-premise Deployment&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=1Panel-dev/MaxKB&amp;type=Date)](https://star-history.com/#1Panel-dev/MaxKB&amp;Date)

## License

Licensed under The GNU General Public License version 3 (GPLv3)  (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at

&lt;https://www.gnu.org/licenses/gpl-3.0.html&gt;

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[PromtEngineer/localGPT]]></title>
            <link>https://github.com/PromtEngineer/localGPT</link>
            <guid>https://github.com/PromtEngineer/localGPT</guid>
            <pubDate>Sun, 20 Jul 2025 00:05:02 GMT</pubDate>
            <description><![CDATA[Chat with your documents on your local device using GPT models. No data leaves your device and 100% private.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/PromtEngineer/localGPT">PromtEngineer/localGPT</a></h1>
            <p>Chat with your documents on your local device using GPT models. No data leaves your device and 100% private.</p>
            <p>Language: Python</p>
            <p>Stars: 21,640</p>
            <p>Forks: 2,380</p>
            <p>Stars today: 115 stars today</p>
            <h2>README</h2><pre># LocalGPT: Secure, Local Conversations with Your Documents üåê

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/2947&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/2947&quot; alt=&quot;PromtEngineer%2FlocalGPT | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

[![GitHub Stars](https://img.shields.io/github/stars/PromtEngineer/localGPT?style=social)](https://github.com/PromtEngineer/localGPT/stargazers)
[![GitHub Forks](https://img.shields.io/github/forks/PromtEngineer/localGPT?style=social)](https://github.com/PromtEngineer/localGPT/network/members)
[![GitHub Issues](https://img.shields.io/github/issues/PromtEngineer/localGPT)](https://github.com/PromtEngineer/localGPT/issues)
[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/PromtEngineer/localGPT)](https://github.com/PromtEngineer/localGPT/pulls)
[![License](https://img.shields.io/github/license/PromtEngineer/localGPT)](https://github.com/PromtEngineer/localGPT/blob/main/LICENSE)

üö®üö® NEW VERSION OF LOCALGPT IS OUT ON THE [LOCALGPT-V2](https://github.com/PromtEngineer/localGPT/tree/localgpt-v2) BRANCH. 

You can run localGPT on a pre-configured [Virtual Machine](https://bit.ly/localGPT). Make sure to use the code: PromptEngineering to get 50% off. I will get a small commision!

**LocalGPT** is an open-source initiative that allows you to converse with your documents without compromising your privacy. With everything running locally, you can be assured that no data ever leaves your computer. Dive into the world of secure, local document interactions with LocalGPT.

## Features üåü
- **Utmost Privacy**: Your data remains on your computer, ensuring 100% security.
- **Versatile Model Support**: Seamlessly integrate a variety of open-source models, including HF, GPTQ, GGML, and GGUF.
- **Diverse Embeddings**: Choose from a range of open-source embeddings.
- **Reuse Your LLM**: Once downloaded, reuse your LLM without the need for repeated downloads.
- **Chat History**: Remembers your previous conversations (in a session).
- **API**: LocalGPT has an API that you can use for building RAG Applications.
- **Graphical Interface**: LocalGPT comes with two GUIs, one uses the API and the other is standalone (based on streamlit).
- **GPU, CPU, HPU &amp; MPS Support**: Supports multiple platforms out of the box, Chat with your data using `CUDA`, `CPU`, `HPU (Intel¬Æ Gaudi¬Æ)` or `MPS` and more!

## Dive Deeper with Our Videos üé•
- [Detailed code-walkthrough](https://youtu.be/MlyoObdIHyo)
- [Llama-2 with LocalGPT](https://youtu.be/lbFmceo4D5E)
- [Adding Chat History](https://youtu.be/d7otIM_MCZs)
- [LocalGPT - Updated (09/17/2023)](https://youtu.be/G_prHSKX9d4)

## Technical Details üõ†Ô∏è
By selecting the right local models and the power of `LangChain` you can run the entire RAG pipeline locally, without any data leaving your environment, and with reasonable performance.

- `ingest.py` uses `LangChain` tools to parse the document and create embeddings locally using `InstructorEmbeddings`. It then stores the result in a local vector database using `Chroma` vector store.
- `run_localGPT.py` uses a local LLM to understand questions and create answers. The context for the answers is extracted from the local vector store using a similarity search to locate the right piece of context from the docs.
- You can replace this local LLM with any other LLM from the HuggingFace. Make sure whatever LLM you select is in the HF format.

This project was inspired by the original [privateGPT](https://github.com/imartinez/privateGPT).

## Built Using üß©
- [LangChain](https://github.com/hwchase17/langchain)
- [HuggingFace LLMs](https://huggingface.co/models)
- [InstructorEmbeddings](https://instructor-embedding.github.io/)
- [LLAMACPP](https://github.com/abetlen/llama-cpp-python)
- [ChromaDB](https://www.trychroma.com/)
- [Streamlit](https://streamlit.io/)

# Environment Setup üåç

1. üì• Clone the repo using git:

```shell
git clone https://github.com/PromtEngineer/localGPT.git
```

2. üêç Install [conda](https://www.anaconda.com/download) for virtual environment management. Create and activate a new virtual environment.

```shell
conda create -n localGPT python=3.10.0
conda activate localGPT
```

3. üõ†Ô∏è Install the dependencies using pip

To set up your environment to run the code, first install all requirements:

```shell
pip install -r requirements.txt
```

***Installing LLAMA-CPP :***

LocalGPT uses [LlamaCpp-Python](https://github.com/abetlen/llama-cpp-python) for GGML (you will need llama-cpp-python &lt;=0.1.76) and GGUF (llama-cpp-python &gt;=0.1.83) models.

To run the quantized Llama3 model, ensure you have llama-cpp-python version 0.2.62 or higher installed.

If you want to use BLAS or Metal with [llama-cpp](https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal) you can set appropriate flags:

For `NVIDIA` GPUs support, use `cuBLAS`

```shell
# Example: cuBLAS
CMAKE_ARGS=&quot;-DLLAMA_CUBLAS=on&quot; FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir
```

For Apple Metal (`M1/M2`) support, use

```shell
# Example: METAL
CMAKE_ARGS=&quot;-DLLAMA_METAL=on&quot;  FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir
```
For more details, please refer to [llama-cpp](https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal)

## Docker üê≥

Installing the required packages for GPU inference on NVIDIA GPUs, like gcc 11 and CUDA 11, may cause conflicts with other packages in your system.
As an alternative to Conda, you can use Docker with the provided Dockerfile.
It includes CUDA, your system just needs Docker, BuildKit, your NVIDIA GPU driver and the NVIDIA container toolkit.
Build as `docker build -t localgpt .`, requires BuildKit.
Docker BuildKit does not support GPU during *docker build* time right now, only during *docker run*.
Run as `docker run -it --mount src=&quot;$HOME/.cache&quot;,target=/root/.cache,type=bind --gpus=all localgpt`.
For running the code on Intel¬Æ Gaudi¬Æ HPU, use the following Dockerfile - `Dockerfile_hpu`.

## Test dataset

For testing, this repository comes with [Constitution of USA](https://constitutioncenter.org/media/files/constitution.pdf) as an example file to use.

## Ingesting your OWN Data.
Put your files in the `SOURCE_DOCUMENTS` folder. You can put multiple folders within the `SOURCE_DOCUMENTS` folder and the code will recursively read your files.

### Support file formats:
LocalGPT currently supports the following file formats. LocalGPT uses `LangChain` for loading these file formats. The code in `constants.py` uses a `DOCUMENT_MAP` dictionary to map a file format to the corresponding loader. In order to add support for another file format, simply add this dictionary with the file format and the corresponding loader from [LangChain](https://python.langchain.com/docs/modules/data_connection/document_loaders/).

```shell
DOCUMENT_MAP = {
    &quot;.txt&quot;: TextLoader,
    &quot;.md&quot;: TextLoader,
    &quot;.py&quot;: TextLoader,
    &quot;.pdf&quot;: PDFMinerLoader,
    &quot;.csv&quot;: CSVLoader,
    &quot;.xls&quot;: UnstructuredExcelLoader,
    &quot;.xlsx&quot;: UnstructuredExcelLoader,
    &quot;.docx&quot;: Docx2txtLoader,
    &quot;.doc&quot;: Docx2txtLoader,
}
```

### Ingest

Run the following command to ingest all the data.

If you have `cuda` setup on your system.

```shell
python ingest.py
```
You will see an output like this:
&lt;img width=&quot;1110&quot; alt=&quot;Screenshot 2023-09-14 at 3 36 27 PM&quot; src=&quot;https://github.com/PromtEngineer/localGPT/assets/134474669/c9274e9a-842c-49b9-8d95-606c3d80011f&quot;&gt;


Use the device type argument to specify a given device.
To run on `cpu`

```sh
python ingest.py --device_type cpu
```

To run on `M1/M2`

```sh
python ingest.py --device_type mps
```

Use help for a full list of supported devices.

```sh
python ingest.py --help
```

This will create a new folder called `DB` and use it for the newly created vector store. You can ingest as many documents as you want, and all will be accumulated in the local embeddings database.
If you want to start from an empty database, delete the `DB` and reingest your documents.

Note: When you run this for the first time, it will need internet access to download the embedding model (default: `Instructor Embedding`). In the subsequent runs, no data will leave your local environment and you can ingest data without internet connection.

## Ask questions to your documents, locally!

In order to chat with your documents, run the following command (by default, it will run on `cuda`).

```shell
python run_localGPT.py
```
You can also specify the device type just like `ingest.py`

```shell
python run_localGPT.py --device_type mps # to run on Apple silicon
```

```shell
# To run on Intel¬Æ Gaudi¬Æ hpu
MODEL_ID = &quot;mistralai/Mistral-7B-Instruct-v0.2&quot; # in constants.py
python run_localGPT.py --device_type hpu
```

This will load the ingested vector store and embedding model. You will be presented with a prompt:

```shell
&gt; Enter a query:
```

After typing your question, hit enter. LocalGPT will take some time based on your hardware. You will get a response like this below.
&lt;img width=&quot;1312&quot; alt=&quot;Screenshot 2023-09-14 at 3 33 19 PM&quot; src=&quot;https://github.com/PromtEngineer/localGPT/assets/134474669/a7268de9-ade0-420b-a00b-ed12207dbe41&quot;&gt;

Once the answer is generated, you can then ask another question without re-running the script, just wait for the prompt again.


***Note:*** When you run this for the first time, it will need internet connection to download the LLM (default: `TheBloke/Llama-2-7b-Chat-GGUF`). After that you can turn off your internet connection, and the script inference would still work. No data gets out of your local environment.

Type `exit` to finish the script.

### Extra Options with run_localGPT.py

You can use the `--show_sources` flag with `run_localGPT.py` to show which chunks were retrieved by the embedding model. By default, it will show 4 different sources/chunks. You can change the number of sources/chunks

```shell
python run_localGPT.py --show_sources
```

Another option is to enable chat history. ***Note***: This is disabled by default and can be enabled by using the  `--use_history` flag. The context window is limited so keep in mind enabling history will use it and might overflow.

```shell
python run_localGPT.py --use_history
```

You can store user questions and model responses with flag `--save_qa` into a csv file `/local_chat_history/qa_log.csv`. Every interaction will be stored. 

```shell
python run_localGPT.py --save_qa
```

# Run the Graphical User Interface

1. Open `constants.py` in an editor of your choice and depending on choice add the LLM you want to use. By default, the following model will be used:

   ```shell
   MODEL_ID = &quot;TheBloke/Llama-2-7b-Chat-GGUF&quot;
   MODEL_BASENAME = &quot;llama-2-7b-chat.Q4_K_M.gguf&quot;
   ```

3. Open up a terminal and activate your python environment that contains the dependencies installed from requirements.txt.

4. Navigate to the `/LOCALGPT` directory.

5. Run the following command `python run_localGPT_API.py`. The API should being to run.

6. Wait until everything has loaded in. You should see something like `INFO:werkzeug:Press CTRL+C to quit`.

7. Open up a second terminal and activate the same python environment.

8. Navigate to the `/LOCALGPT/localGPTUI` directory.

9. Run the command `python localGPTUI.py`.

10. Open up a web browser and go the address `http://localhost:5111/`.


# How to select different LLM models?

To change the models you will need to set both `MODEL_ID` and `MODEL_BASENAME`.

1. Open up `constants.py` in the editor of your choice.
2. Change the `MODEL_ID` and `MODEL_BASENAME`. If you are using a quantized model (`GGML`, `GPTQ`, `GGUF`), you will need to provide `MODEL_BASENAME`. For unquantized models, set `MODEL_BASENAME` to `NONE`
5. There are a number of example models from HuggingFace that have already been tested to be run with the original trained model (ending with HF or have a .bin in its &quot;Files and versions&quot;), and quantized models (ending with GPTQ or have a .no-act-order or .safetensors in its &quot;Files and versions&quot;).
6. For models that end with HF or have a .bin inside its &quot;Files and versions&quot; on its HuggingFace page.

   - Make sure you have a `MODEL_ID` selected. For example -&gt; `MODEL_ID = &quot;TheBloke/guanaco-7B-HF&quot;`
   - Go to the [HuggingFace Repo](https://huggingface.co/TheBloke/guanaco-7B-HF)

7. For models that contain GPTQ in its name and or have a .no-act-order or .safetensors extension inside its &quot;Files and versions on its HuggingFace page.

   - Make sure you have a `MODEL_ID` selected. For example -&gt; model_id = `&quot;TheBloke/wizardLM-7B-GPTQ&quot;`
   - Got to the corresponding [HuggingFace Repo](https://huggingface.co/TheBloke/wizardLM-7B-GPTQ) and select &quot;Files and versions&quot;.
   - Pick one of the model names and set it as  `MODEL_BASENAME`. For example -&gt; `MODEL_BASENAME = &quot;wizardLM-7B-GPTQ-4bit.compat.no-act-order.safetensors&quot;`

8. Follow the same steps for `GGUF` and `GGML` models.

# GPU and VRAM Requirements

Below is the VRAM requirement for different models depending on their size (Billions of parameters). The estimates in the table does not include VRAM used by the Embedding models - which use an additional 2GB-7GB of VRAM depending on the model.

| Mode Size (B) | float32   | float16   | GPTQ 8bit      | GPTQ 4bit          |
| ------- | --------- | --------- | -------------- | ------------------ |
| 7B      | 28 GB     | 14 GB     | 7 GB - 9 GB    | 3.5 GB - 5 GB      |
| 13B     | 52 GB     | 26 GB     | 13 GB - 15 GB  | 6.5 GB - 8 GB      |
| 32B     | 130 GB    | 65 GB     | 32.5 GB - 35 GB| 16.25 GB - 19 GB   |
| 65B     | 260.8 GB  | 130.4 GB  | 65.2 GB - 67 GB| 32.6 GB - 35 GB    |


# System Requirements

## Python Version

To use this software, you must have Python 3.10 or later installed. Earlier versions of Python will not compile.

## C++ Compiler

If you encounter an error while building a wheel during the `pip install` process, you may need to install a C++ compiler on your computer.

### For Windows 10/11

To install a C++ compiler on Windows 10/11, follow these steps:

1. Install Visual Studio 2022.
2. Make sure the following components are selected:
   - Universal Windows Platform development
   - C++ CMake tools for Windows
3. Download the MinGW installer from the [MinGW website](https://sourceforge.net/projects/mingw/).
4. Run the installer and select the &quot;gcc&quot; component.

### NVIDIA Driver&#039;s Issues:

Follow this [page](https://linuxconfig.org/how-to-install-the-nvidia-drivers-on-ubuntu-22-04) to install NVIDIA Drivers.

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=PromtEngineer/localGPT&amp;type=Date)](https://star-history.com/#PromtEngineer/localGPT&amp;Date)

# Disclaimer

This is a test project to validate the feasibility of a fully local solution for question answering using LLMs and Vector embeddings. It is not production ready, and it is not meant to be used in production. Vicuna-7B is based on the Llama model so that has the original Llama license.

# Common Errors

 - [Torch not compatible with CUDA enabled](https://github.com/pytorch/pytorch/issues/30664)

   -  Get CUDA version
      ```shell
      nvcc --version
      ```
      ```shell
      nvidia-smi
      ```
   - Try installing PyTorch depending on your CUDA version
      ```shell
         conda install -c pytorch torchvision cudatoolkit=10.1 pytorch
      ```
   - If it doesn&#039;t work, try reinstalling
      ```shell
         pip uninstall torch
         pip cache purge
         pip install torch -f https://download.pytorch.org/whl/torch_stable.html
      ```

- [ERROR: pip&#039;s dependency resolver does not currently take into account all the packages that are installed](https://stackoverflow.com/questions/72672196/error-pips-dependency-resolver-does-not-currently-take-into-account-all-the-pa/76604141#76604141)
  ```shell
     pip install h5py
     pip install typing-extensions
     pip install wheel
  ```
- [Failed to import transformers](https://github.com/huggingface/transformers/issues/11262)
  - Try re-install
    ```shell
       conda uninstall tokenizers, transformers
       pip install transformers
    ```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[apache/airflow]]></title>
            <link>https://github.com/apache/airflow</link>
            <guid>https://github.com/apache/airflow</guid>
            <pubDate>Sun, 20 Jul 2025 00:05:01 GMT</pubDate>
            <description><![CDATA[Apache Airflow - A platform to programmatically author, schedule, and monitor workflows]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/apache/airflow">apache/airflow</a></h1>
            <p>Apache Airflow - A platform to programmatically author, schedule, and monitor workflows</p>
            <p>Language: Python</p>
            <p>Stars: 41,116</p>
            <p>Forks: 15,337</p>
            <p>Stars today: 31 stars today</p>
            <h2>README</h2><pre>&lt;!--
 Licensed to the Apache Software Foundation (ASF) under one
 or more contributor license agreements.  See the NOTICE file
 distributed with this work for additional information
 regarding copyright ownership.  The ASF licenses this file
 to you under the Apache License, Version 2.0 (the
 &quot;License&quot;); you may not use this file except in compliance
 with the License.  You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

 Unless required by applicable law or agreed to in writing,
 software distributed under the License is distributed on an
 &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 KIND, either express or implied.  See the License for the
 specific language governing permissions and limitations
 under the License.
--&gt;

&lt;!-- START Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt;
# Apache Airflow

| Badges     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| License    | [![License](https://img.shields.io/:license-Apache%202-blue.svg)](https://www.apache.org/licenses/LICENSE-2.0.txt)                                                                                                                                                                                                                                                                                                                                               |
| PyPI       | [![PyPI version](https://badge.fury.io/py/apache-airflow.svg)](https://badge.fury.io/py/apache-airflow) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/apache-airflow.svg)](https://pypi.org/project/apache-airflow/) [![PyPI - Downloads](https://img.shields.io/pypi/dm/apache-airflow)](https://pypi.org/project/apache-airflow/)                                                                                                           |
| Containers | [![Docker Pulls](https://img.shields.io/docker/pulls/apache/airflow.svg)](https://hub.docker.com/r/apache/airflow) [![Docker Stars](https://img.shields.io/docker/stars/apache/airflow.svg)](https://hub.docker.com/r/apache/airflow) [![Artifact HUB](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/apache-airflow)](https://artifacthub.io/packages/search?repo=apache-airflow)                                                  |
| Community  | [![Contributors](https://img.shields.io/github/contributors/apache/airflow)](https://github.com/apache/airflow/graphs/contributors) [![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&amp;style=social)](https://s.apache.org/airflow-slack) ![Commit Activity](https://img.shields.io/github/commit-activity/m/apache/airflow) [![OSSRank](https://shields.io/endpoint?url=https://ossrank.com/shield/6)](https://ossrank.com/p/6) |



| Version | Build Status                                                                                                                                                                                                                                                                                                            |
|---------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Main    | [![GitHub Build main](https://github.com/apache/airflow/actions/workflows/ci-amd.yml/badge.svg)](https://github.com/apache/airflow/actions) [![GitHub Build main](https://github.com/apache/airflow/actions/workflows/ci-arm.yml/badge.svg)](https://github.com/apache/airflow/actions)                                 |
| 3.x     | [![GitHub Build 3.0](https://github.com/apache/airflow/actions/workflows/ci-amd.yml/badge.svg?branch=v3-0-test)](https://github.com/apache/airflow/actions) [![GitHub Build 3.0](https://github.com/apache/airflow/actions/workflows/ci-arm.yml/badge.svg?branch=v3-0-test)](https://github.com/apache/airflow/actions) |
| 2.x     | [![GitHub Build 2.11](https://github.com/apache/airflow/actions/workflows/ci.yml/badge.svg?branch=v2-11-test)](https://github.com/apache/airflow/actions)                                                                                                                                                               |



&lt;picture width=&quot;500&quot;&gt;
  &lt;img
    src=&quot;https://github.com/apache/airflow/blob/19ebcac2395ef9a6b6ded3a2faa29dc960c1e635/docs/apache-airflow/img/logos/wordmark_1.png?raw=true&quot;
    alt=&quot;Apache Airflow logo&quot;
  /&gt;
&lt;/picture&gt;

[Apache Airflow](https://airflow.apache.org/docs/apache-airflow/stable/) (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows.

When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.

Use Airflow to author workflows (Dags) that orchestrate tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.

&lt;!-- END Apache Airflow, please keep comment here to allow auto update of PyPI readme.md --&gt;
&lt;!-- START doctoc generated TOC please keep comment here to allow auto update --&gt;
&lt;!-- DON&#039;T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --&gt;
**Table of contents**

- [Project Focus](#project-focus)
- [Principles](#principles)
- [Requirements](#requirements)
- [Getting started](#getting-started)
- [Installing from PyPI](#installing-from-pypi)
- [Installation](#installation)
- [Official source code](#official-source-code)
- [Convenience packages](#convenience-packages)
- [User Interface](#user-interface)
- [Semantic versioning](#semantic-versioning)
- [Version Life Cycle](#version-life-cycle)
- [Support for Python and Kubernetes versions](#support-for-python-and-kubernetes-versions)
- [Base OS support for reference Airflow images](#base-os-support-for-reference-airflow-images)
- [Approach to dependencies of Airflow](#approach-to-dependencies-of-airflow)
- [Contributing](#contributing)
- [Voting Policy](#voting-policy)
- [Who uses Apache Airflow?](#who-uses-apache-airflow)
- [Who maintains Apache Airflow?](#who-maintains-apache-airflow)
- [What goes into the next release?](#what-goes-into-the-next-release)
- [Can I use the Apache Airflow logo in my presentation?](#can-i-use-the-apache-airflow-logo-in-my-presentation)
- [Links](#links)
- [Sponsors](#sponsors)

&lt;!-- END doctoc generated TOC please keep comment here to allow auto update --&gt;

## Project Focus

Airflow works best with workflows that are mostly static and slowly changing. When the DAG structure is similar from one run to the next, it clarifies the unit of work and continuity. Other similar projects include [Luigi](https://github.com/spotify/luigi), [Oozie](https://oozie.apache.org/) and [Azkaban](https://azkaban.github.io/).

Airflow is commonly used to process data, but has the opinion that tasks should ideally be idempotent (i.e., results of the task will be the same, and will not create duplicated data in a destination system), and should not pass large quantities of data from one task to the next (though tasks can pass metadata using Airflow&#039;s [XCom feature](https://airflow.apache.org/docs/apache-airflow/stable/concepts/xcoms.html)). For high-volume, data-intensive tasks, a best practice is to delegate to external services specializing in that type of work.

Airflow is not a streaming solution, but it is often used to process real-time data, pulling data off streams in batches.

## Principles

- **Dynamic**: Pipelines are defined in code, enabling dynamic dag generation and parameterization.
- **Extensible**: The Airflow framework includes a wide range of built-in operators and can be extended to fit your needs.
- **Flexible**: Airflow leverages the [**Jinja**](https://jinja.palletsprojects.com) templating engine, allowing rich customizations.

&lt;!-- START Requirements, please keep comment here to allow auto update of PyPI readme.md --&gt;
## Requirements

Apache Airflow is tested with:

|            | Main version (dev)     | Stable version (3.0.3) |
|------------|------------------------|------------------------|
| Python     | 3.10, 3.11, 3.12, 3.13 | 3.9, 3.10, 3.11, 3.12  |
| Platform   | AMD64/ARM64(\*)        | AMD64/ARM64(\*)        |
| Kubernetes | 1.30, 1.31, 1.32, 1.33 | 1.30, 1.31, 1.32, 1.33 |
| PostgreSQL | 13, 14, 15, 16, 17     | 13, 14, 15, 16, 17     |
| MySQL      | 8.0, 8.4, Innovation   | 8.0, 8.4, Innovation   |
| SQLite     | 3.15.0+                | 3.15.0+                |

\* Experimental

**Note**: MariaDB is not tested/recommended.

**Note**: SQLite is used in Airflow tests. Do not use it in production. We recommend
using the latest stable version of SQLite for local development.

**Note**: Airflow currently can be run on POSIX-compliant Operating Systems. For development, it is regularly
tested on fairly modern Linux Distros and recent versions of macOS.
On Windows you can run it via WSL2 (Windows Subsystem for Linux 2) or via Linux Containers.
The work to add Windows support is tracked via [#10388](https://github.com/apache/airflow/issues/10388), but
it is not a high priority. You should only use Linux-based distros as &quot;Production&quot; execution environment
as this is the only environment that is supported. The only distro that is used in our CI tests and that
is used in the [Community managed DockerHub image](https://hub.docker.com/p/apache/airflow) is
`Debian Bookworm`.

&lt;!-- END Requirements, please keep comment here to allow auto update of PyPI readme.md --&gt;
&lt;!-- START Getting started, please keep comment here to allow auto update of PyPI readme.md --&gt;
## Getting started

Visit the official Airflow website documentation (latest **stable** release) for help with
[installing Airflow](https://airflow.apache.org/docs/apache-airflow/stable/installation/),
[getting started](https://airflow.apache.org/docs/apache-airflow/stable/start.html), or walking
through a more complete [tutorial](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/).

&gt; Note: If you&#039;re looking for documentation for the main branch (latest development branch): you can find it on [s.apache.org/airflow-docs](https://s.apache.org/airflow-docs/).

For more information on Airflow Improvement Proposals (AIPs), visit
the [Airflow Wiki](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals).

Documentation for dependent projects like provider distributions, Docker image, Helm Chart, you&#039;ll find it in [the documentation index](https://airflow.apache.org/docs/).

&lt;!-- END Getting started, please keep comment here to allow auto update of PyPI readme.md --&gt;
&lt;!-- START Installing from PyPI, please keep comment here to allow auto update of PyPI readme.md --&gt;

## Installing from PyPI

We publish Apache Airflow as `apache-airflow` package in PyPI. Installing it however might be sometimes tricky
because Airflow is a bit of both a library and application. Libraries usually keep their dependencies open, and
applications usually pin them, but we should do neither and both simultaneously. We decided to keep
our dependencies as open as possible (in `pyproject.toml`) so users can install different versions of libraries
if needed. This means that `pip install apache-airflow` will not work from time to time or will
produce unusable Airflow installation.

To have repeatable installation, however, we keep a set of &quot;known-to-be-working&quot; constraint
files in the orphan `constraints-main` and `constraints-2-0` branches. We keep those &quot;known-to-be-working&quot;
constraints files separately per major/minor Python version.
You can use them as constraint files when installing Airflow from PyPI. Note that you have to specify
correct Airflow tag/version/branch and Python versions in the URL.

1. Installing just Airflow:

&gt; Note: Only `pip` installation is currently officially supported.

While it is possible to install Airflow with tools like [Poetry](https://python-poetry.org) or
[pip-tools](https://pypi.org/project/pip-tools), they do not share the same workflow as
`pip` - especially when it comes to constraint vs. requirements management.
Installing via `Poetry` or `pip-tools` is not currently supported.

There are known issues with ``bazel`` that might lead to circular dependencies when using it to install
Airflow. Please switch to ``pip`` if you encounter such problems. ``Bazel`` community works on fixing
the problem in `this PR &lt;https://github.com/bazelbuild/rules_python/pull/1166&gt;`_ so it might be that
newer versions of ``bazel`` will handle it.

If you wish to install Airflow using those tools, you should use the constraint files and convert
them to the appropriate format and workflow that your tool requires.


```bash
pip install &#039;apache-airflow==3.0.3&#039; \
 --constraint &quot;https://raw.githubusercontent.com/apache/airflow/constraints-3.0.3/constraints-3.10.txt&quot;
```

2. Installing with extras (i.e., postgres, google)

```bash
pip install &#039;apache-airflow[postgres,google]==3.0.3&#039; \
 --constraint &quot;https://raw.githubusercontent.com/apache/airflow/constraints-3.0.3/constraints-3.10.txt&quot;
```

For information on installing provider distributions, check
[providers](http://airflow.apache.org/docs/apache-airflow-providers/index.html).

&lt;!-- END Installing from PyPI, please keep comment here to allow auto update of PyPI readme.md --&gt;

## Installation

For comprehensive instructions on setting up your local development environment and installing Apache Airflow, please refer to the [INSTALLING.md](INSTALLING.md) file.

&lt;!-- START Official source code, please keep comment here to allow auto update of PyPI readme.md --&gt;
## Official source code

Apache Airflow is an [Apache Software Foundation](https://www.apache.org) (ASF) project,
and our official source code releases:

- Follow the [ASF Release Policy](https://www.apache.org/legal/release-policy.html)
- Can be downloaded from [the ASF Distribution Directory](https://downloads.apache.org/airflow)
- Are cryptographically signed by the release manager
- Are officially voted on by the PMC members during the
  [Release Approval Process](https://www.apache.org/legal/release-policy.html#release-approval)

Following the ASF rules, the source packages released must be sufficient for a user to build and test the
release provided they have access to the appropriate platform and tools.

&lt;!-- END Official source code, please keep comment here to allow auto update of PyPI readme.md --&gt;
## Convenience packages

There are other ways of installing and using Airflow. Those are &quot;convenience&quot; methods - they are
not &quot;official releases&quot; as stated by the `ASF Release Policy`, but they can be used by the users
who do not want to build the software themselves.

Those are - in the order of most common ways people install Airflow:

- [PyPI releases](https://pypi.org/project/apache-airflow/) to install Airflow using standard `pip` tool
- [Docker Images](https://hub.docker.com/r/apache/airflow) to install airflow via
  `docker` tool, use them in Kubernetes, Helm Charts, `docker-compose`, `docker swarm`, etc. You can
  read more about using, customizing, and extending the images in the
  [Latest docs](https://airflow.apache.org/docs/docker-stack/index.html), and
  learn details on the internals in the [images](https://airflow.apache.org/docs/docker-stack/index.html) document.
- [Tags in GitHub](https://github.com/apache/airflow/tags) to retrieve the git project sources that
  were used to generate official source packages via git

All those artifacts are not official releases, but they are prepared using officially released sources.
Some of those artifacts are &quot;development&quot; or &quot;pre-release&quot; ones, and they are clearly marked as such
following the ASF Policy.

## User Interface

- **DAGs**: Overview of all DAGs in your environment.

  ![DAGs](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/dags.png)

- **Assets**: Overview of Assets with dependencies.

  ![Asset Dependencies](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/assets_graph.png)

- **Grid**: Grid representation of a DAG that spans across time.

  ![Grid](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/grid.png)

- **Graph**: Visualization of a DAG&#039;s dependencies and their current status for a specific run.

  ![Graph](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/graph.png)

- **Home**: Summary statistics of your Airflow environment.

  ![Home](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/home.png)

- **Backfill**: Backfilling a DAG for a specific date range.

  ![Backfill](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/backfill.png)

- **Code**: Quick way to view source code of a DAG.

  ![Code](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/code.png)

## Semantic versioning

As of Airflow 2.0.0, we support a strict [SemVer](https://semver.org/) approach for all packages released.

There are few specific rules that we agreed to that define details of versioning of the different
packages:

* **Airflow**: SemVer rules apply to core airflow only (excludes any changes to providers).
  Changing limits for versions of Airflow dependencies is not a breaking change on its own.
* **Airflow Providers**: SemVer rules apply to changes in the particular provider&#039;s code only.
  SemVer MAJOR and MINOR versions for the packages are independent of the Airflow version.
  For example, `google 4.1.0` and `amazon 3.0.3` providers can happily be installed
  with `Airflow 2.1.2`. If there are limits of cross-dependencies between providers and Airflow packages,
  they are present in providers as `install_requires` limitations. We aim to keep backwards
  compatibility of providers with all previously released Airflow 2 versions but
  there will sometimes be breaking changes that might make some, or all
  providers, have minimum Airflow version specified.
* **Airflow Helm Chart**: SemVer rules apply to changes in the chart only. SemVer MAJOR and MINOR
  versions for the chart are independent of the Airflow version. We aim to keep backwards
  compatibility of the Helm Chart with all released Airflow 2 versions, but some new features might
  only work starting from specific Airflow releases. We might however limit the Helm
  Chart to depend on minimal Airflow version.
* **Airflow API clients**: Their versioning is independent from Airflow versions. They follow their own
  SemVer rules for breaking changes and new features - which for example allows to change the way we generate
  the clients.

## Version Life Cycle

Apache Airflow version life cycle:

&lt;!-- This table is autom

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[philippemerle/KubeDiagrams]]></title>
            <link>https://github.com/philippemerle/KubeDiagrams</link>
            <guid>https://github.com/philippemerle/KubeDiagrams</guid>
            <pubDate>Sun, 20 Jul 2025 00:05:00 GMT</pubDate>
            <description><![CDATA[Generate Kubernetes architecture diagrams from Kubernetes manifest files, kustomization files, Helm charts, helmfiles, and actual cluster state]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/philippemerle/KubeDiagrams">philippemerle/KubeDiagrams</a></h1>
            <p>Generate Kubernetes architecture diagrams from Kubernetes manifest files, kustomization files, Helm charts, helmfiles, and actual cluster state</p>
            <p>Language: Python</p>
            <p>Stars: 1,366</p>
            <p>Forks: 78</p>
            <p>Stars today: 19 stars today</p>
            <h2>README</h2><pre># KubeDiagrams

[![license](https://img.shields.io/github/license/philippemerle/KubeDiagrams)](https://github.com/philippemerle/KubeDiagrams/blob/main/LICENSE)
![python version](https://img.shields.io/badge/python-%3E%3D%203.9-blue?logo=python)
[![Socket Badge](https://socket.dev/api/badge/pypi/package/KubeDiagrams/0.4.0?artifact_id=tar-gz)](https://socket.dev/pypi/package/KubeDiagrams/overview/0.4.0/tar-gz)
[![pypi version](https://badge.fury.io/py/KubeDiagrams.svg)](https://badge.fury.io/py/KubeDiagrams)
[![PyPI Downloads](https://static.pepy.tech/badge/kubediagrams)](https://pepy.tech/projects/kubediagrams)
[![Docker Stars](https://img.shields.io/docker/stars/philippemerle/kubediagrams)](https://hub.docker.com/r/philippemerle/kubediagrams)
[![Docker Image Version](https://img.shields.io/docker/v/philippemerle/kubediagrams)](https://hub.docker.com/r/philippemerle/kubediagrams)
[![Docker Pulls](https://img.shields.io/docker/pulls/philippemerle/kubediagrams)](https://hub.docker.com/r/philippemerle/kubediagrams)
![contributors](https://img.shields.io/github/contributors/philippemerle/KubeDiagrams)

![KubeDiagrams Logo](https://raw.githubusercontent.com/philippemerle/KubeDiagrams/refs/heads/main/images/KubeDiagrams.png)

Generate Kubernetes architecture diagrams from Kubernetes manifest files, kustomization files, Helm charts, helmfile descriptors, and actual cluster state.

There are several tools to generate Kubernetes architecture diagrams (see **[here](https://github.com/philippemerle/Awesome-Kubernetes-Architecture-Diagrams)**).
The main originality of **KubeDiagrams** is its **[configurability](https://github.com/philippemerle/KubeDiagrams/blob/main/bin/kube-diagrams.yaml)** allowing for instance to deal with custom Kubernetes resources.

## Examples

Architecture diagram for **[official Kubernetes WordPress tutorial](https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/)** manifests:
![WordPress Manifests](https://raw.githubusercontent.com/philippemerle/KubeDiagrams/refs/heads/main/examples/wordpress/wordpress.png)

Architecture diagram for **[official Kubernetes ZooKeeper tutorial](https://kubernetes.io/docs/tutorials/stateful-application/zookeeper/)** manifests:
![ZooKeeper Manifest](https://raw.githubusercontent.com/philippemerle/KubeDiagrams/refs/heads/main/examples/zookeeper/zookeeper.png)

Architecture diagram of a deployed **[Cassandra](https://kubernetes.io/docs/tutorials/stateful-application/cassandra/)** instance:
![Deployed Cassandra Instance](https://raw.githubusercontent.com/philippemerle/KubeDiagrams/refs/heads/main/examples/cassandra/default.png)

Architecture diagram for **[Train TicketÔºöA Benchmark Microservice System](https://github.com/FudanSELab/train-ticket/)**:
![train-ticket.png](https://raw.githubusercontent.com/philippemerle/KubeDiagrams/refs/heads/main/examples/train-ticket/train-ticket.png)

Architecture diagram of the Minikube Ingress Addon:
![Minikube Ingress Addon](https://raw.githubusercontent.com/philippemerle/KubeDiagrams/refs/heads/main/examples/minikube/minikube-ingress-nginx.png)

Architecture diagram for the **[Kube Prometheus Stack](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack)** chart:
![kube-prometheus-stack.png](https://raw.githubusercontent.com/philippemerle/KubeDiagrams/refs/heads/main/examples/kube-prometheus-stack/kube-prometheus-stack.png)

Architecture diagram for **[free5gc-k8s](https://github.com/niloysh/free5gc-k8s)** manifests:
![free5gc-k8s-diagram.png](https://raw.githubusercontent.com/philippemerle/KubeDiagrams/refs/heads/main/examples/free5gc-k8s/free5gc-k8s-diagram.png)

Architecture diagram for **[open5gs-k8s](https://github.com/niloysh/open5gs-k8s)** manifests:
![open5gs-k8s-diagram.png](https://raw.githubusercontent.com/philippemerle/KubeDiagrams/refs/heads/main/examples/open5gs-k8s/open5gs-k8s-diagram.png)

Architecture diagram for the **[Towards5GS-helm](https://github.com/Orange-OpenSource/towards5gs-helm)** chart:
![towards5gs_free5gc.png](https://raw.githubusercontent.com/philippemerle/KubeDiagrams/refs/heads/main/examples/towards5gs-helm/towards5gs_free5gc.png)

Architecture diagram for a deployed **CronJob** instance:
![cronjob-deployed.png](https://raw.githubusercontent.com/philippemerle/KubeDiagrams/refs/heads/main/examples/miscellaneous/cronjob-deployed.png)

Architecture diagram for **NetworkPolicy** resources: ![network_policies.png](https://raw.githubusercontent.com/philippemerle/KubeDiagrams/refs/heads/main/examples/miscellaneous/network_policies.png)

Architecture diagram for an **Argo CD** example:
![argoproj-argocd-example-apps-apps.png](https://raw.githubusercontent.com/philippemerle/KubeDiagrams/refs/heads/main/examples/argo/diagrams/argoproj-argocd-example-apps-apps.png)

Architecture diagram for an **Argo Events** example:
![argoproj-argo-events-examples.png](https://raw.githubusercontent.com/philippemerle/KubeDiagrams/refs/heads/main/examples/argo/diagrams/argoproj-argo-events-examples.png)

Many other architecture diagrams are available into [examples/](https://github.com/philippemerle/KubeDiagrams/blob/main/examples/).

### Business Applications

1. [DeathStarBench](https://github.com/philippemerle/KubeDiagrams/blob/main/examples/deathstarbench/)
1. [Official Kubernetes WordPress tutorial](https://github.com/philippemerle/KubeDiagrams/blob/main/examples/wordpress/)
1. [Official Kubernetes ZooKeeper tutorial](https://github.com/philippemerle/KubeDiagrams/blob/main/examples/zookeeper/)
1. [Official Kubernetes Cassandra tutorial](https://github.com/philippemerle/KubeDiagrams/blob/main/examples/cassandra/)
1. [Online Boutique](https://github.com/philippemerle/KubeDiagrams/blob/main/examples/online-boutique/)
1. [TeaStore](https://github.com/philippemerle/KubeDiagrams/blob/main/examples/teastore/)
1. [Train Ticket](https://github.com/philippemerle/KubeDiagrams/blob/main/examples/train-ticket/)

### 5G Core Network Functions

1. [free5gc-k8s](https://github.com/philippemerle/KubeDiagrams/blob/main/examples/free5gc-k8s/)
1. [docker-open5gs](https://github.com/philippemerle/KubeDiagrams/blob/main/examples/docker-open5gs/)
1. [Gradiant 5G Charts](https://github.com/philippemerle/KubeDiagrams/blob/main/examples/gradiant-5g-charts)
1. [open5gs-k8s](https://github.com/philippemerle/KubeDiagrams/blob/main/examples/open5gs-k8s/)
1. [OpenAirInterface 5G Core Network](https://github.com/philippemerle/KubeDiagrams/blob/main/examples/oai-5g-cn/)
1. [Towards5GS-helm](https://github.com/philippemerle/KubeDiagrams/blob/main/examples/towards5gs-helm/)

### Kubernetes Operators

1. [Argo](https://github.com/philippemerle/KubeDiagrams/blob/main/examples/argo/)
1. [Istio](https://github.com/philippemerle/KubeDiagrams/blob/main/examples/istio/)
1. [Kube Prometheus Stack](https://github.com/philippemerle/KubeDiagrams/blob/main/examples/kube-prometheus-stack/)
1. [LeaderWorkerSet API](https://github.com/philippemerle/KubeDiagrams/blob/main/examples/lws/)

### Kubernetes Control Planes

1. [k0s architecture diagrams](https://github.com/philippemerle/KubeDiagrams/blob/main/examples/k0s/)
1. [minikube architecture diagrams](https://github.com/philippemerle/KubeDiagrams/blob/main/examples/minikube/)

### Other examples

1. [Some Helm charts](https://github.com/philippemerle/KubeDiagrams/blob/main/examples/helm-charts/)
1. [helmfile](https://github.com/philippemerle/KubeDiagrams/blob/main/examples/helmfile/)
1. [Miscellaneous examples](https://github.com/philippemerle/KubeDiagrams/blob/main/examples/miscellaneous/)

## Prerequisites

Following software must be installed:

- [Python](https://www.python.org) 3.9 or higher
- `dot` command ([Graphviz](https://www.graphviz.org/))

## Getting Started

### From PyPI

Following command installs **KubeDiagrams** and all its Python dependencies, i.e., [PyYAML](https://pyyaml.org) and [Diagrams](https://diagrams.mingrammer.com/).

```ssh
# using pip (pip3)
pip install KubeDiagrams
```

### From Nix

Alternatively, you can install via Nix:

```sh
nix shell github:philippemerle/KubeDiagrams
```

### From Docker Hub

**KubeDiagrams** container images are available in [Docker Hub](https://hub.docker.com/r/philippemerle/kubediagrams).
You can download the latest container image via:

```sh
docker pull philippemerle/kubediagrams
```

### From source

You can start directly from source:

```sh
# clone the KubeDiagrams repository
git clone https://github.com/philippemerle/KubeDiagrams.git

# install required Python packages
pip install PyYAML diagrams

# make KubeDiagrams commands available into $PATH
PATH=$(pwd)/KubeDiagrams/bin:$PATH
```

### From Windows

To use **KubeDiagrams** from Windows operating system, only the container image is supported currently.

&gt; [!NOTE]
&gt;
&gt; Any contribution would be welcome to translate KubeDiagrams&#039; Unix-based scripts to Windows-based scripts.

## Usage

**KubeDiagrams** provides two commands: `kube-diagrams` and `helm-diagrams`.

### `kube-diagrams`

`kube-diagrams` generates a Kubernetes architecture diagram from one or several Kubernetes manifest files.

```sh
kube-diagrams -h
usage: kube-diagrams [-h] [-o OUTPUT] [-f FORMAT] [-c CONFIG] [-v] [--without-namespace] filename [filename ...]

Generate Kubernetes architecture diagrams from Kubernetes manifest files

positional arguments:
  filename              the Kubernetes manifest filename to process

options:
  -h, --help            show this help message and exit
  -o, --output OUTPUT   output diagram filename
  -f, --format FORMAT   output format, allowed formats are dot, dot_json, gif, jp2, jpe, jpeg, jpg, pdf, png, svg, tif, tiff, set to png by default
  --embed-all-icons     embed all icons into svg or dot_json output diagrams
  -c, --config CONFIG   custom kube-diagrams configuration file
  -n, --namespace NAMESPACE
                        visualize only the resources inside a given namespace
  -v, --verbose         verbosity, set to false by default
  --without-namespace   disable namespace cluster generation
```

Examples:

```sh
# generate a diagram from a manifest
kube-diagrams -o cassandra.png examples/cassandra/cassandra.yml

# generate a diagram from a kustomize folder
kubectl kustomize path_to_a_kustomize_folder | kube-diagrams - -o diagram.png

# generate a diagram from a helmfile descriptor
helmfile template -f helmfile.yaml | kube-diagrams - -o diagram.png

# generate a diagram from the actual default namespace state
kubectl get all -o yaml | kube-diagrams -o default-namespace.png -

# generate a diagram of all workload and service resources from all namespaces
kubectl get all --all-namespaces -o yaml | kube-diagrams -o all-namespaces.png -
```

#### üß© `kubectl` Plugin Support

You can use KubeDiagrams as a `kubectl` plugin as well for a more integrated Kubernetes workflow. This allows you to run commands like:

```sh
kubectl diagrams all -o diagram.png
```

To enable this, simply symlink or copy the [`kubectl-diagrams`](./bin/kubectl-diagrams) script onto your `$PATH`:

```sh
ln -s $(which kube-diagrams) /usr/local/bin/kubectl-diagrams
```

&gt; [!NOTE]
&gt;
&gt; You will also already need `kube-diagrams` on your `$PATH` as well for this to work.

You can alternatively install it via Nix:

```sh
nix shell github:philippemerle/KubeDiagrams#kubectl-diagrams
```

### `helm-diagrams`

`helm-diagrams` generates a Kubernetes architecture diagram from an Helm chart.

```sh
Usage: helm-diagrams &lt;helm-chart-url&gt; [OPTIONS] [FLAGS]

A script to generate a diagram of an Helm chart using kube-diagrams.

Options:
  -o, --output &lt;file&gt;          Specify the output file for the diagram
  -f, --format &lt;format&gt;        Specify the output format (e.g., png, svg)
  --embed-all-icons            Embed all icons into svg or dot_json output diagrams
  -c, --config &lt;file&gt;          Specify the custom kube-diagrams configuration file
  -h, --help                   Display this help message

Any flag supported by helm template, e.g.:
  -g, --generate-name          Generate the name (and omit the NAME parameter)
  --include-crds               Include CRDs in the templated output
  -l, --labels stringToString  Labels that would be added to release metadata. Should be divided by comma. (default [])
  --name-template string       Specify template used to name the release
  --set stringArray            Set values on the command line (can specify multiple or separate values with commas: key1=val1,key2=val2)
  --set-file stringArray       Set values from respective files specified via the command line (can specify multiple or separate values with commas: key1=path1,key2=path2)
  --set-json stringArray       Set JSON values on the command line (can specify multiple or separate values with commas: key1=jsonval1,key2=jsonval2)
  --set-literal stringArray    Set a literal STRING value on the command line
  --set-string stringArray     Set STRING values on the command line (can specify multiple or separate values with commas: key1=val1,key2=val2)
  -f, --values strings         Specify values in a YAML file or a URL (can specify multiple)
  --version string             Specify a version constraint for the chart version to use. This constraint can be a specific tag (e.g. 1.1.1) or it may reference a valid range (e.g. ^2.0.0). If this is not specified, the latest version is used

Examples:
  helm-diagrams https://charts.jetstack.io/cert-manager -o diagram.png
  helm-diagrams https://charts.jetstack.io/cert-manager --set crds.enabled=true -o cert-manager.png
  helm-diagrams oci://ghcr.io/argoproj/argo-helm/argo-cd -f svg
  helm-diagrams --help
```

&gt; [!NOTE]
&gt;
&gt; `helm-diagrams` requires that the `helm` command was installed.

Examples:

```ssh
# generate a diagram for the Helm chart &#039;cert-manager&#039; available in HTTP repository &#039;charts.jetstack.io&#039;
helm-diagrams https://charts.jetstack.io/cert-manager

# generate a diagram for the Helm chart &#039;argo-cd&#039; available in OCI repository &#039;ghcr.io&#039;
helm-diagrams oci://ghcr.io/argoproj/argo-helm/argo-cd

# generate a diagram for the Helm chart &#039;some-chart&#039; available locally
helm-diagrams some-path/some-chart
```

### With Docker/Podman

**KubeDiagrams** images are available in [Docker Hub](https://hub.docker.com/r/philippemerle/kubediagrams).

```ssh
# For usage with Podman, replace &#039;docker&#039; by &#039;podman&#039; in the following lines.

# generate a diagram from a manifest
docker run -v &quot;$(pwd)&quot;:/work philippemerle/kubediagrams kube-diagrams -o cassandra.png examples/cassandra/cassandra.yml

# generate a diagram from a kustomize folder
kubectl kustomize path_to_a_kustomize_folder | docker run -v &quot;$(pwd)&quot;:/work -i philippemerle/kubediagrams kube-diagrams - -o diagram.png

# generate a diagram from a helmfile descriptor
helmfile template -f helmfile.yaml | docker run -v &quot;$(pwd)&quot;:/work -i philippemerle/kubediagrams kube-diagrams - -o diagram.png

# generate a diagram from the actual default namespace state
kubectl get all -o yaml | docker run -v &quot;$(pwd)&quot;:/work -i philippemerle/kubediagrams kube-diagrams -o default-namespace.png -

# generate a diagram of all workload and service resources from all namespaces
kubectl get all --all-namespaces -o yaml | docker run -v &quot;$(pwd)&quot;:/work -i philippemerle/kubediagrams kube-diagrams -o all-namespaces.png -

# generate a diagram for the Helm chart &#039;cert-manager&#039; available in HTTP repository &#039;charts.jetstack.io&#039;
docker run -v &quot;$(pwd)&quot;:/work philippemerle/kubediagrams helm-diagrams https://charts.jetstack.io/cert-manager

# generate a diagram for the Helm chart &#039;argo-cd&#039; available in OCI repository &#039;ghcr.io&#039;
docker run -v &quot;$(pwd)&quot;:/work philippemerle/kubediagrams helm-diagrams oci://ghcr.io/argoproj/argo-helm/argo-cd
```

### GitHub Action

You can use **KubeDiagrams** (and Helm Diagrams) in your GitHub Action workflows.

```yaml
name: &quot;Your GitHub Action Name&quot;
on:
  workflow_dispatch: # add your specific triggers (https://docs.github.com/en/actions/writing-workflows/choosing-when-your-workflow-runs/events-that-trigger-workflows)
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: &quot;Generate diagram from Kubernetes manifest&quot;
        uses: philippemerle/KubeDiagrams@main
        with:
          type: &quot;kubernetes&quot;
          args: &quot;-o examples/cassandra/cassandra.png examples/cassandra/cassandra.yml&quot;

      - name: &quot;Generate diagram from Helm chart&quot;
        uses: philippemerle/KubeDiagrams@main
        with:
          type: &quot;helm&quot;
          args: &quot;https://charts.jetstack.io/cert-manager&quot;
```

Action `philippemerle/KubeDiagrams@main` is available [here](https://raw.githubusercontent.com/philippemerle/KubeDiagrams/refs/heads/main/action.yml).

## Features

### Kubernetes resources

**KubeDiagrams** supported the following 47 Kubernetes resource types:

|               Kind               |            ApiGroup            |           Versions            |                                                                          Icon                                                                          |
| :------------------------------: | :----------------------------: | :---------------------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------: |
|           `APIService`           |    `apiregistration.k8s.io`    |        `v1beta1` `v1`         |                  ![APIService](https://raw.githubusercontent.com/philippemerle/KubeDiagrams/refs/heads/main/bin/icons/apiservice.png)                  |
|          `ClusterRole`           |  `rbac.authorization.k8s.io`   |        `v1beta1` `v1`         |          ![ClusterRole](https://raw.githubusercontent.com/kubernetes/community/refs/heads/master/icons/png/resources/labeled/c-role-128.png)           |
|       `ClusterRoleBinding`       |  `rbac.authorization.k8s.io`   |        `v1beta1` `v1`         |        ![ClusterRoleBinding](https://raw.githubusercontent.com/kubernetes/community/refs/heads/master/icons/png/resources/labeled/crb-128.png)         |
|           `ConfigMap`            |                                |             `v1`              |             ![ConfigMap](https://raw.githubusercontent.com/kubernetes/community/refs/heads/master/icons/png/resources/labeled/cm-128.png)              |
|            `CronJob`             |            `batch`             |        `v1beta1` `v1`         |            ![CronJob](https://raw.githubusercontent.com/kubernetes/community/refs/heads/master/icons/png/resources/labeled/cronjob-128.png)            |
|           `CSIDriver`            |        `storage.k8s.io`        |        `v1beta1` `v1`         |                   ![CSIDriver](https://raw.githubusercontent.com/philippemerle/KubeDiagrams/refs/heads/main/bin/icons/csidriver.png)                   |
|            `CSINode`             |        `storage.k8s.io`        |             `v1`              |                     ![CSINode](https://raw.githubusercontent.com/philippemerle/KubeDiagrams/refs/heads/main/bin/icons/csinode.png)                     |
|       `CSIStorageCapacity`       |        `storage.k8s.io`        |             `v1`              |                ![CSIStorageCapacity](https://raw.githubusercontent.com/philippemerle/KubeDiagrams/refs/heads/main/bin/icons/csisc.png)                 |
|    `CustomResourceDefinition`    |     `apiextensions.k8s.io`     |        `v1beta1` `v1`         |     ![CustomResourceDefinition](https://raw.githubusercontent.com/kubernetes/community/refs/heads/master/icons/png/resources/labeled/crd-128.png)      |
|           `DaemonSet`            |    `apps` `extensions`         |   `v1beta1` `v1beta2` `v1`    |             ![DaemonSet](https://raw.githubusercontent.com/kubernetes/community/refs/heads/master/icons/png/resources/labeled/ds-128.png)              |
|           `Deployment`           |         `apps` `extensions`    |   `v1beta1` `v1beta2` `v1`    |           ![Deployment](https://raw.githubusercontent.com/kubernetes/co

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pydantic/pydantic-ai]]></title>
            <link>https://github.com/pydantic/pydantic-ai</link>
            <guid>https://github.com/pydantic/pydantic-ai</guid>
            <pubDate>Sun, 20 Jul 2025 00:04:59 GMT</pubDate>
            <description><![CDATA[Agent Framework / shim to use Pydantic with LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pydantic/pydantic-ai">pydantic/pydantic-ai</a></h1>
            <p>Agent Framework / shim to use Pydantic with LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 11,110</p>
            <p>Forks: 1,049</p>
            <p>Stars today: 157 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://ai.pydantic.dev/&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://ai.pydantic.dev/img/pydantic-ai-dark.svg&quot;&gt;
      &lt;img src=&quot;https://ai.pydantic.dev/img/pydantic-ai-light.svg&quot; alt=&quot;Pydantic AI&quot;&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;em&gt;Agent Framework / shim to use Pydantic with LLMs&lt;/em&gt;
&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml?query=branch%3Amain&quot;&gt;&lt;img src=&quot;https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml/badge.svg?event=push&quot; alt=&quot;CI&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://coverage-badge.samuelcolvin.workers.dev/redirect/pydantic/pydantic-ai&quot;&gt;&lt;img src=&quot;https://coverage-badge.samuelcolvin.workers.dev/pydantic/pydantic-ai.svg&quot; alt=&quot;Coverage&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pypi.python.org/pypi/pydantic-ai&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/pydantic-ai.svg&quot; alt=&quot;PyPI&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/pydantic/pydantic-ai&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/pyversions/pydantic-ai.svg&quot; alt=&quot;versions&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/pydantic/pydantic-ai/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/pydantic/pydantic-ai.svg?v&quot; alt=&quot;license&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://logfire.pydantic.dev/docs/join-slack/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Slack-Join%20Slack-4A154B?logo=slack&quot; alt=&quot;Join Slack&quot; /&gt;&lt;/a&gt;
&lt;/div&gt;

---

**Documentation**: [ai.pydantic.dev](https://ai.pydantic.dev/)

---

Pydantic AI is a Python agent framework designed to make it less painful to build production grade applications with Generative AI.

FastAPI revolutionized web development by offering an innovative and ergonomic design, built on the foundation of [Pydantic Validation](https://docs.pydantic.dev).

Similarly, virtually every agent framework and LLM library in Python uses Pydantic Validation, yet when we began to use LLMs in [Pydantic Logfire](https://pydantic.dev/logfire), we couldn&#039;t find anything that gave us the same feeling.

We built Pydantic AI with one simple aim: to bring that FastAPI feeling to GenAI app development.

## Why use Pydantic AI

- **Built by the Pydantic Team**
  Built by the team behind [Pydantic Validation](https://docs.pydantic.dev/latest/) (the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more).

- **Model-agnostic**
  Supports OpenAI, Anthropic, Gemini, Deepseek, Ollama, Groq, Cohere, and Mistral, and there is a simple interface to implement support for [other models](https://ai.pydantic.dev/models/).

- **Pydantic Logfire Integration**
  Seamlessly [integrates](https://ai.pydantic.dev/logfire/) with [Pydantic Logfire](https://pydantic.dev/logfire) for real-time debugging, performance monitoring, and behavior tracking of your LLM-powered applications.

- **Type-safe**
  Designed to make [type checking](https://ai.pydantic.dev/agents/#static-type-checking) as powerful and informative as possible for you.

- **Python-centric Design**
  Leverages Python&#039;s familiar control flow and agent composition to build your AI-driven projects, making it easy to apply standard Python best practices you&#039;d use in any other (non-AI) project.

- **Structured Responses**
  Harnesses the power of [Pydantic Validation](https://docs.pydantic.dev/latest/) to [validate and structure](https://ai.pydantic.dev/output/#structured-output) model outputs, ensuring responses are consistent across runs.

- **Dependency Injection System**
  Offers an optional [dependency injection](https://ai.pydantic.dev/dependencies/) system to provide data and services to your agent&#039;s [system prompts](https://ai.pydantic.dev/agents/#system-prompts), [tools](https://ai.pydantic.dev/tools/) and [output validators](https://ai.pydantic.dev/output/#output-validator-functions).
  This is useful for testing and eval-driven iterative development.

- **Streamed Responses**
  Provides the ability to [stream](https://ai.pydantic.dev/output/#streamed-results) LLM outputs continuously, with immediate validation, ensuring rapid and accurate outputs.

- **Graph Support**
  [Pydantic Graph](https://ai.pydantic.dev/graph) provides a powerful way to define graphs using typing hints, this is useful in complex applications where standard control flow can degrade to spaghetti code.

## Hello World Example

Here&#039;s a minimal example of Pydantic AI:

```python
from pydantic_ai import Agent

# Define a very simple agent including the model to use, you can also set the model when running the agent.
agent = Agent(
    &#039;google-gla:gemini-1.5-flash&#039;,
    # Register a static system prompt using a keyword argument to the agent.
    # For more complex dynamically-generated system prompts, see the example below.
    system_prompt=&#039;Be concise, reply with one sentence.&#039;,
)

# Run the agent synchronously, conducting a conversation with the LLM.
# Here the exchange should be very short: Pydantic AI will send the system prompt and the user query to the LLM,
# the model will return a text response. See below for a more complex run.
result = agent.run_sync(&#039;Where does &quot;hello world&quot; come from?&#039;)
print(result.output)
&quot;&quot;&quot;
The first known use of &quot;hello, world&quot; was in a 1974 textbook about the C programming language.
&quot;&quot;&quot;
```

_(This example is complete, it can be run &quot;as is&quot;)_

Not very interesting yet, but we can easily add &quot;tools&quot;, dynamic system prompts, and structured responses to build more powerful agents.

## Tools &amp; Dependency Injection Example

Here is a concise example using Pydantic AI to build a support agent for a bank:

**(Better documented example [in the docs](https://ai.pydantic.dev/#tools-dependency-injection-example))**

```python
from dataclasses import dataclass

from pydantic import BaseModel, Field
from pydantic_ai import Agent, RunContext

from bank_database import DatabaseConn


# SupportDependencies is used to pass data, connections, and logic into the model that will be needed when running
# system prompt and tool functions. Dependency injection provides a type-safe way to customise the behavior of your agents.
@dataclass
class SupportDependencies:
    customer_id: int
    db: DatabaseConn


# This pydantic model defines the structure of the output returned by the agent.
class SupportOutput(BaseModel):
    support_advice: str = Field(description=&#039;Advice returned to the customer&#039;)
    block_card: bool = Field(description=&quot;Whether to block the customer&#039;s card&quot;)
    risk: int = Field(description=&#039;Risk level of query&#039;, ge=0, le=10)


# This agent will act as first-tier support in a bank.
# Agents are generic in the type of dependencies they accept and the type of output they return.
# In this case, the support agent has type `Agent[SupportDependencies, SupportOutput]`.
support_agent = Agent(
    &#039;openai:gpt-4o&#039;,
    deps_type=SupportDependencies,
    # The response from the agent will, be guaranteed to be a SupportOutput,
    # if validation fails the agent is prompted to try again.
    output_type=SupportOutput,
    system_prompt=(
        &#039;You are a support agent in our bank, give the &#039;
        &#039;customer support and judge the risk level of their query.&#039;
    ),
)


# Dynamic system prompts can make use of dependency injection.
# Dependencies are carried via the `RunContext` argument, which is parameterized with the `deps_type` from above.
# If the type annotation here is wrong, static type checkers will catch it.
@support_agent.system_prompt
async def add_customer_name(ctx: RunContext[SupportDependencies]) -&gt; str:
    customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)
    return f&quot;The customer&#039;s name is {customer_name!r}&quot;


# `tool` let you register functions which the LLM may call while responding to a user.
# Again, dependencies are carried via `RunContext`, any other arguments become the tool schema passed to the LLM.
# Pydantic is used to validate these arguments, and errors are passed back to the LLM so it can retry.
@support_agent.tool
async def customer_balance(
        ctx: RunContext[SupportDependencies], include_pending: bool
) -&gt; float:
    &quot;&quot;&quot;Returns the customer&#039;s current account balance.&quot;&quot;&quot;
    # The docstring of a tool is also passed to the LLM as the description of the tool.
    # Parameter descriptions are extracted from the docstring and added to the parameter schema sent to the LLM.
    balance = await ctx.deps.db.customer_balance(
        id=ctx.deps.customer_id,
        include_pending=include_pending,
    )
    return balance


...  # In a real use case, you&#039;d add more tools and a longer system prompt


async def main():
    deps = SupportDependencies(customer_id=123, db=DatabaseConn())
    # Run the agent asynchronously, conducting a conversation with the LLM until a final response is reached.
    # Even in this fairly simple case, the agent will exchange multiple messages with the LLM as tools are called to retrieve an output.
    result = await support_agent.run(&#039;What is my balance?&#039;, deps=deps)
    # The `result.output` will be validated with Pydantic to guarantee it is a `SupportOutput`. Since the agent is generic,
    # it&#039;ll also be typed as a `SupportOutput` to aid with static type checking.
    print(result.output)
    &quot;&quot;&quot;
    support_advice=&#039;Hello John, your current account balance, including pending transactions, is $123.45.&#039; block_card=False risk=1
    &quot;&quot;&quot;

    result = await support_agent.run(&#039;I just lost my card!&#039;, deps=deps)
    print(result.output)
    &quot;&quot;&quot;
    support_advice=&quot;I&#039;m sorry to hear that, John. We are temporarily blocking your card to prevent unauthorized transactions.&quot; block_card=True risk=8
    &quot;&quot;&quot;
```

## Next Steps

To try Pydantic AI yourself, follow the instructions [in the examples](https://ai.pydantic.dev/examples/).

Read the [docs](https://ai.pydantic.dev/agents/) to learn more about building applications with Pydantic AI.

Read the [API Reference](https://ai.pydantic.dev/api/agent/) to understand Pydantic AI&#039;s interface.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[psf/black]]></title>
            <link>https://github.com/psf/black</link>
            <guid>https://github.com/psf/black</guid>
            <pubDate>Sun, 20 Jul 2025 00:04:58 GMT</pubDate>
            <description><![CDATA[The uncompromising Python code formatter]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/psf/black">psf/black</a></h1>
            <p>The uncompromising Python code formatter</p>
            <p>Language: Python</p>
            <p>Stars: 40,686</p>
            <p>Forks: 2,614</p>
            <p>Stars today: 21 stars today</p>
            <h2>README</h2><pre>[![Black Logo](https://raw.githubusercontent.com/psf/black/main/docs/_static/logo2-readme.png)](https://black.readthedocs.io/en/stable/)

&lt;h2 align=&quot;center&quot;&gt;The Uncompromising Code Formatter&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/psf/black/actions&quot;&gt;&lt;img alt=&quot;Actions Status&quot; src=&quot;https://github.com/psf/black/workflows/Test/badge.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://black.readthedocs.io/en/stable/?badge=stable&quot;&gt;&lt;img alt=&quot;Documentation Status&quot; src=&quot;https://readthedocs.org/projects/black/badge/?version=stable&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://coveralls.io/github/psf/black?branch=main&quot;&gt;&lt;img alt=&quot;Coverage Status&quot; src=&quot;https://coveralls.io/repos/github/psf/black/badge.svg?branch=main&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/psf/black/blob/main/LICENSE&quot;&gt;&lt;img alt=&quot;License: MIT&quot; src=&quot;https://black.readthedocs.io/en/stable/_static/license.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://pypi.org/project/black/&quot;&gt;&lt;img alt=&quot;PyPI&quot; src=&quot;https://img.shields.io/pypi/v/black&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://pepy.tech/project/black&quot;&gt;&lt;img alt=&quot;Downloads&quot; src=&quot;https://static.pepy.tech/badge/black&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://anaconda.org/conda-forge/black/&quot;&gt;&lt;img alt=&quot;conda-forge&quot; src=&quot;https://img.shields.io/conda/dn/conda-forge/black.svg?label=conda-forge&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/psf/black&quot;&gt;&lt;img alt=&quot;Code style: black&quot; src=&quot;https://img.shields.io/badge/code%20style-black-000000.svg&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&gt; ‚ÄúAny color you like.‚Äù

_Black_ is the uncompromising Python code formatter. By using it, you agree to cede
control over minutiae of hand-formatting. In return, _Black_ gives you speed,
determinism, and freedom from `pycodestyle` nagging about formatting. You will save time
and mental energy for more important matters.

Blackened code looks the same regardless of the project you&#039;re reading. Formatting
becomes transparent after a while and you can focus on the content instead.

_Black_ makes code review faster by producing the smallest diffs possible.

Try it out now using the [Black Playground](https://black.vercel.app). Watch the
[PyCon 2019 talk](https://youtu.be/esZLCuWs_2Y) to learn more.

---

**[Read the documentation on ReadTheDocs!](https://black.readthedocs.io/en/stable)**

---

## Installation and usage

### Installation

_Black_ can be installed by running `pip install black`. It requires Python 3.9+ to run.
If you want to format Jupyter Notebooks, install with `pip install &quot;black[jupyter]&quot;`.

If you can&#039;t wait for the latest _hotness_ and want to install from GitHub, use:

`pip install git+https://github.com/psf/black`

### Usage

To get started right away with sensible defaults:

```sh
black {source_file_or_directory}
```

You can run _Black_ as a package if running it as a script doesn&#039;t work:

```sh
python -m black {source_file_or_directory}
```

Further information can be found in our docs:

- [Usage and Configuration](https://black.readthedocs.io/en/stable/usage_and_configuration/index.html)

_Black_ is already [successfully used](https://github.com/psf/black#used-by) by many
projects, small and big. _Black_ has a comprehensive test suite, with efficient parallel
tests, and our own auto formatting and parallel Continuous Integration runner. Now that
we have become stable, you should not expect large formatting changes in the future.
Stylistic changes will mostly be responses to bug reports and support for new Python
syntax. For more information please refer to
[The Black Code Style](https://black.readthedocs.io/en/stable/the_black_code_style/index.html).

Also, as a safety measure which slows down processing, _Black_ will check that the
reformatted code still produces a valid AST that is effectively equivalent to the
original (see the
[Pragmatism](https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html#ast-before-and-after-formatting)
section for details). If you&#039;re feeling confident, use `--fast`.

## The _Black_ code style

_Black_ is a PEP 8 compliant opinionated formatter. _Black_ reformats entire files in
place. Style configuration options are deliberately limited and rarely added. It doesn&#039;t
take previous formatting into account (see
[Pragmatism](https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html#pragmatism)
for exceptions).

Our documentation covers the current _Black_ code style, but planned changes to it are
also documented. They&#039;re both worth taking a look at:

- [The _Black_ Code Style: Current style](https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html)
- [The _Black_ Code Style: Future style](https://black.readthedocs.io/en/stable/the_black_code_style/future_style.html)

Changes to the _Black_ code style are bound by the Stability Policy:

- [The _Black_ Code Style: Stability Policy](https://black.readthedocs.io/en/stable/the_black_code_style/index.html#stability-policy)

Please refer to this document before submitting an issue. What seems like a bug might be
intended behaviour.

### Pragmatism

Early versions of _Black_ used to be absolutist in some respects. They took after its
initial author. This was fine at the time as it made the implementation simpler and
there were not many users anyway. Not many edge cases were reported. As a mature tool,
_Black_ does make some exceptions to rules it otherwise holds.

- [The _Black_ code style: Pragmatism](https://black.readthedocs.io/en/stable/the_black_code_style/current_style.html#pragmatism)

Please refer to this document before submitting an issue just like with the document
above. What seems like a bug might be intended behaviour.

## Configuration

_Black_ is able to read project-specific default values for its command line options
from a `pyproject.toml` file. This is especially useful for specifying custom
`--include` and `--exclude`/`--force-exclude`/`--extend-exclude` patterns for your
project.

You can find more details in our documentation:

- [The basics: Configuration via a file](https://black.readthedocs.io/en/stable/usage_and_configuration/the_basics.html#configuration-via-a-file)

And if you&#039;re looking for more general configuration documentation:

- [Usage and Configuration](https://black.readthedocs.io/en/stable/usage_and_configuration/index.html)

**Pro-tip**: If you&#039;re asking yourself &quot;Do I need to configure anything?&quot; the answer is
&quot;No&quot;. _Black_ is all about sensible defaults. Applying those defaults will have your
code in compliance with many other _Black_ formatted projects.

## Used by

The following notable open-source projects trust _Black_ with enforcing a consistent
code style: pytest, tox, Pyramid, Django, Django Channels, Hypothesis, attrs,
SQLAlchemy, Poetry, PyPA applications (Warehouse, Bandersnatch, Pipenv, virtualenv),
pandas, Pillow, Twisted, LocalStack, every Datadog Agent Integration, Home Assistant,
Zulip, Kedro, OpenOA, FLORIS, ORBIT, WOMBAT, and many more.

The following organizations use _Black_: Dropbox, KeepTruckin, Lyft, Mozilla, Quora,
Duolingo, QuantumBlack, Tesla, Archer Aviation.

Are we missing anyone? Let us know.

## Testimonials

**Mike Bayer**, [author of `SQLAlchemy`](https://www.sqlalchemy.org/):

&gt; I can&#039;t think of any single tool in my entire programming career that has given me a
&gt; bigger productivity increase by its introduction. I can now do refactorings in about
&gt; 1% of the keystrokes that it would have taken me previously when we had no way for
&gt; code to format itself.

**Dusty Phillips**,
[writer](https://smile.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&amp;field-keywords=dusty+phillips):

&gt; _Black_ is opinionated so you don&#039;t have to be.

**Hynek Schlawack**, [creator of `attrs`](https://www.attrs.org/), core developer of
Twisted and CPython:

&gt; An auto-formatter that doesn&#039;t suck is all I want for Xmas!

**Carl Meyer**, [Django](https://www.djangoproject.com/) core developer:

&gt; At least the name is good.

**Kenneth Reitz**, creator of [`requests`](https://requests.readthedocs.io/en/latest/)
and [`pipenv`](https://readthedocs.org/projects/pipenv/):

&gt; This vastly improves the formatting of our code. Thanks a ton!

## Show your style

Use the badge in your project&#039;s README.md:

```md
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
```

Using the badge in README.rst:

```
.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/psf/black
```

Looks like this:
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

## License

MIT

## Contributing

Welcome! Happy to see you willing to make the project better. You can get started by
reading this:

- [Contributing: The basics](https://black.readthedocs.io/en/latest/contributing/the_basics.html)

You can also take a look at the rest of the contributing docs or talk with the
developers:

- [Contributing documentation](https://black.readthedocs.io/en/latest/contributing/index.html)
- [Chat on Discord](https://discord.gg/RtVdv86PrH)

## Change log

The log has become rather long. It moved to its own file.

See [CHANGES](https://black.readthedocs.io/en/latest/change_log.html).

## Authors

The author list is quite long nowadays, so it lives in its own file.

See [AUTHORS.md](./AUTHORS.md)

## Code of Conduct

Everyone participating in the _Black_ project, and in particular in the issue tracker,
pull requests, and social media activity, is expected to treat other people with respect
and more generally to follow the guidelines articulated in the
[Python Community Code of Conduct](https://www.python.org/psf/codeofconduct/).

At the same time, humor is encouraged. In fact, basic familiarity with Monty Python&#039;s
Flying Circus is expected. We are not savages.

And if you _really_ need to slap somebody, do it with a fish while dancing.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>