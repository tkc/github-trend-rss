<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Thu, 05 Feb 2026 00:07:29 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[openai/skills]]></title>
            <link>https://github.com/openai/skills</link>
            <guid>https://github.com/openai/skills</guid>
            <pubDate>Thu, 05 Feb 2026 00:07:29 GMT</pubDate>
            <description><![CDATA[Skills Catalog for Codex]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/openai/skills">openai/skills</a></h1>
            <p>Skills Catalog for Codex</p>
            <p>Language: Python</p>
            <p>Stars: 3,649</p>
            <p>Forks: 209</p>
            <p>Stars today: 746 stars today</p>
            <h2>README</h2><pre># Agent Skills

Agent Skills are folders of instructions, scripts, and resources that AI agents can discover and use to perform at specific tasks. Write once, use everywhere.

Codex uses skills to help package capabilities that teams and individuals can use to complete specific tasks in a repeatable way. This repository catalogs skills for use and distribution with Codex.

Learn more:
- [Using skills in Codex](https://developers.openai.com/codex/skills)
- [Create custom skills in Codex](https://developers.openai.com/codex/skills/create-skill)
- [Agent Skills open standard](https://agentskills.io)

## Installing a skill

Skills in [`.system`](skills/.system/) are automatically installed in the latest version of Codex.

To install [curated](skills/.curated/) or [experimental](skills/.experimental/) skills, you can use the `$skill-installer` inside Codex.

Curated skills can be installed by name (defaults to `skills/.curated`):

```
$skill-installer gh-address-comments
```

For experimental skills, specify the skill folder. For example:

```
$skill-installer install the create-plan skill from the .experimental folder
```

Or provide the GitHub directory URL:

```
$skill-installer install https://github.com/openai/skills/tree/main/skills/.experimental/create-plan
```

After installing a skill, restart Codex to pick up new skills.

## License

The license of an individual skill can be found directly inside the skill&#039;s directory inside the `LICENSE.txt` file.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[disler/claude-code-hooks-mastery]]></title>
            <link>https://github.com/disler/claude-code-hooks-mastery</link>
            <guid>https://github.com/disler/claude-code-hooks-mastery</guid>
            <pubDate>Thu, 05 Feb 2026 00:07:28 GMT</pubDate>
            <description><![CDATA[Master Claude Code Hooks]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/disler/claude-code-hooks-mastery">disler/claude-code-hooks-mastery</a></h1>
            <p>Master Claude Code Hooks</p>
            <p>Language: Python</p>
            <p>Stars: 2,354</p>
            <p>Forks: 504</p>
            <p>Stars today: 47 stars today</p>
            <h2>README</h2><pre># Claude Code Hooks Mastery

[Claude Code Hooks](https://docs.anthropic.com/en/docs/claude-code/hooks) - Quickly master how to use Claude Code hooks to add deterministic (or non-deterministic) control over Claude Code&#039;s behavior. Plus learn about [Claude Code Sub-Agents](#claude-code-sub-agents), the powerful [Meta-Agent](#the-meta-agent), and [Team-Based Validation](#team-based-validation-system) with agent orchestration.

&lt;img src=&quot;images/hooked.png&quot; alt=&quot;Claude Code Hooks&quot; style=&quot;max-width: 800px; width: 100%;&quot; /&gt;

## Table of Contents

- [Prerequisites](#prerequisites)
- [Hook Lifecycle &amp; Payloads](#hook-lifecycle--payloads)
- [What This Shows](#what-this-shows)
- [UV Single-File Scripts Architecture](#uv-single-file-scripts-architecture)
- [Key Files](#key-files)
- [Features Demonstrated](#features-demonstrated)
- [Hook Error Codes &amp; Flow Control](#hook-error-codes--flow-control)
- [UserPromptSubmit Hook Deep Dive](#userpromptsubmit-hook-deep-dive)
- [Claude Code Sub-Agents](#claude-code-sub-agents)
- [Team-Based Validation System](#team-based-validation-system)
- [Output Styles Collection](#output-styles-collection)
- [Custom Status Lines](#custom-status-lines)

## Prerequisites

This requires:
- **[Astral UV](https://docs.astral.sh/uv/getting-started/installation/)** - Fast Python package installer and resolver
- **[Claude Code](https://docs.anthropic.com/en/docs/claude-code)** - Anthropic&#039;s CLI for Claude AI

### Optional Setup:

Optional:
- **[ElevenLabs](https://elevenlabs.io/)** - Text-to-speech provider (with MCP server integration)
- **[ElevenLabs MCP Server](https://github.com/elevenlabs/elevenlabs-mcp)** - MCP server for ElevenLabs
- **[Firecrawl MCP Server](https://www.firecrawl.dev/mcp)** - Web scraping and crawling MCP server (my favorite scraper)
- **[OpenAI](https://openai.com/)** - Language model provider + Text-to-speech provider
- **[Anthropic](https://www.anthropic.com/)** - Language model provider
- **[Ollama](https://ollama.com/)** - Local language model provider

## Hook Lifecycle &amp; Payloads

This demo captures all 13 Claude Code hook lifecycle events with their JSON payloads:

### Hook Lifecycle Overview

```mermaid
flowchart TB
    subgraph SESSION[&quot;üü¢ Session Lifecycle&quot;]
        direction TB
        SETUP[[&quot;üîß Setup&lt;br/&gt;(init/maintenance)&quot;]]
        START[[&quot;‚ñ∂Ô∏è SessionStart&lt;br/&gt;(startup/resume/clear)&quot;]]
        END[[&quot;‚èπÔ∏è SessionEnd&lt;br/&gt;(exit/sigint/error)&quot;]]
    end

    subgraph MAIN[&quot;üîÑ Main Conversation Loop&quot;]
        direction TB
        PROMPT[[&quot;üìù UserPromptSubmit&quot;]]
        CLAUDE[&quot;Claude Processes&quot;]

        subgraph TOOLS[&quot;üõ†Ô∏è Tool Execution&quot;]
            direction TB
            PRE[[&quot;üîí PreToolUse&quot;]]
            PERM[[&quot;‚ùì PermissionRequest&quot;]]
            EXEC[&quot;Tool Executes&quot;]
            POST[[&quot;‚úÖ PostToolUse&quot;]]
            FAIL[[&quot;‚ùå PostToolUseFailure&quot;]]
        end

        subgraph SUBAGENT[&quot;ü§ñ Subagent Lifecycle&quot;]
            direction TB
            SSTART[[&quot;üöÄ SubagentStart&quot;]]
            SWORK[&quot;Subagent Works&quot;]
            SSTOP[[&quot;üèÅ SubagentStop&quot;]]
        end

        NOTIFY[[&quot;üîî Notification&lt;br/&gt;(Async)&quot;]]
        STOP[[&quot;üõë Stop&quot;]]
    end

    subgraph COMPACT[&quot;üóúÔ∏è Maintenance&quot;]
        PRECOMPACT[[&quot;üì¶ PreCompact&quot;]]
    end

    SETUP --&gt; START
    START --&gt; PROMPT
    PROMPT --&gt; CLAUDE
    CLAUDE --&gt; PRE
    PRE --&gt; PERM
    PERM --&gt; EXEC
    EXEC --&gt; POST
    EXEC -.-&gt; FAIL
    CLAUDE -.-&gt; SSTART
    SSTART --&gt; SWORK
    SWORK --&gt; SSTOP
    POST --&gt; CLAUDE
    CLAUDE --&gt; STOP
    CLAUDE -.-&gt; NOTIFY
    STOP --&gt; PROMPT
    STOP -.-&gt; END
    PROMPT -.-&gt; PRECOMPACT
    PRECOMPACT -.-&gt; PROMPT
```

### 1. UserPromptSubmit Hook
**Fires:** Immediately when user submits a prompt (before Claude processes it)  
**Payload:** `prompt` text, `session_id`, timestamp  
**Enhanced:** Prompt validation, logging, context injection, security filtering

### 2. PreToolUse Hook
**Fires:** Before any tool execution  
**Payload:** `tool_name`, `tool_input` parameters  
**Enhanced:** Blocks dangerous commands (`rm -rf`, `.env` access)

### 3. PostToolUse Hook  
**Fires:** After successful tool completion  
**Payload:** `tool_name`, `tool_input`, `tool_response` with results

### 4. Notification Hook
**Fires:** When Claude Code sends notifications (waiting for input, etc.)  
**Payload:** `message` content  
**Enhanced:** TTS alerts - &quot;Your agent needs your input&quot; (30% chance includes name)

### 5. Stop Hook
**Fires:** When Claude Code finishes responding  
**Payload:** `stop_hook_active` boolean flag  
**Enhanced:** AI-generated completion messages with TTS playback (LLM priority: OpenAI &gt; Anthropic &gt; Ollama &gt; random)

### 6. SubagentStop Hook
**Fires:** When Claude Code subagents (Task tools) finish responding  
**Payload:** `stop_hook_active` boolean flag  
**Enhanced:** TTS playback - &quot;Subagent Complete&quot;

### 7. PreCompact Hook
**Fires:** Before Claude Code performs a compaction operation  
**Payload:** `trigger` (&quot;manual&quot; or &quot;auto&quot;), `custom_instructions` (for manual), session info  
**Enhanced:** Transcript backup, verbose feedback for manual compaction

### 8. SessionStart Hook
**Fires:** When Claude Code starts a new session or resumes an existing one
**Payload:** `source` (&quot;startup&quot;, &quot;resume&quot;, or &quot;clear&quot;), session info
**Enhanced:** Development context loading (git status, recent issues, context files)

### 9. SessionEnd Hook
**Fires:** When Claude Code session ends (exit, sigint, or error)
**Payload:** `session_id`, `transcript_path`, `cwd`, `permission_mode`, `reason`
**Enhanced:** Session logging with optional cleanup tasks (removes temp files, stale logs)

### 10. PermissionRequest Hook
**Fires:** When user is shown a permission dialog
**Payload:** `tool_name`, `tool_input`, `tool_use_id`, session info
**Enhanced:** Permission auditing, auto-allow for read-only ops (Read, Glob, Grep, safe Bash)

### 11. PostToolUseFailure Hook
**Fires:** When a tool execution fails
**Payload:** `tool_name`, `tool_input`, `tool_use_id`, `error` object
**Enhanced:** Structured error logging with timestamps and full context

### 12. SubagentStart Hook
**Fires:** When a subagent (Task tool) spawns
**Payload:** `agent_id`, `agent_type`, session info
**Enhanced:** Subagent spawn logging with optional TTS announcement

### 13. Setup Hook
**Fires:** When Claude enters a repository (init) or periodically (maintenance)
**Payload:** `trigger` (&quot;init&quot; or &quot;maintenance&quot;), session info
**Enhanced:** Environment persistence via `CLAUDE_ENV_FILE`, context injection via `additionalContext`


## What This Shows

- **Complete hook lifecycle coverage** - All 13 hook events implemented and logging (11/13 validated via automated testing)
- **Prompt-level control** - UserPromptSubmit validates and enhances prompts before Claude sees them
- **Intelligent TTS system** - AI-generated audio feedback with voice priority (ElevenLabs &gt; OpenAI &gt; pyttsx3)
- **Security enhancements** - Blocks dangerous commands and sensitive file access at multiple levels
- **Personalized experience** - Uses engineer name from environment variables
- **Automatic logging** - All hook events are logged as JSON to `logs/` directory
- **Chat transcript extraction** - PostToolUse hook converts JSONL transcripts to readable JSON format
- **Team-based validation** - Builder/Validator agent pattern with code quality hooks

&gt; **Warning:** The `chat.json` file contains only the most recent Claude Code conversation. It does not preserve conversations from previous sessions - each new conversation is fully copied and overwrites the previous one. This is unlike the other logs which are appended to from every claude code session.

## UV Single-File Scripts Architecture

This project leverages **[UV single-file scripts](https://docs.astral.sh/uv/guides/scripts/)** to keep hook logic cleanly separated from your main codebase. All hooks live in `.claude/hooks/` as standalone Python scripts with embedded dependency declarations.

**Benefits:**
- **Isolation** - Hook logic stays separate from your project dependencies
- **Portability** - Each hook script declares its own dependencies inline
- **No Virtual Environment Management** - UV handles dependencies automatically
- **Fast Execution** - UV&#039;s dependency resolution is lightning-fast
- **Self-Contained** - Each hook can be understood and modified independently

This approach ensures your hooks remain functional across different environments without polluting your main project&#039;s dependency tree.

## Key Files

- `.claude/settings.json` - Hook configuration with permissions
- `.claude/hooks/` - Python scripts using uv for each hook type
  - `user_prompt_submit.py` - Prompt validation, logging, and context injection
  - `pre_tool_use.py` - Security blocking and logging
  - `post_tool_use.py` - Logging and transcript conversion
  - `post_tool_use_failure.py` - Error logging with structured details
  - `notification.py` - Logging with optional TTS (--notify flag)
  - `stop.py` - AI-generated completion messages with TTS
  - `subagent_stop.py` - Simple &quot;Subagent Complete&quot; TTS
  - `subagent_start.py` - Subagent spawn logging with optional TTS
  - `pre_compact.py` - Transcript backup and compaction logging
  - `session_start.py` - Development context loading and session logging
  - `session_end.py` - Session cleanup and logging
  - `permission_request.py` - Permission auditing and auto-allow
  - `setup.py` - Repository initialization and maintenance
  - `validators/` - Code quality validation hooks
    - `ruff_validator.py` - Python linting via Ruff (PostToolUse)
    - `ty_validator.py` - Python type checking (PostToolUse)
  - `utils/` - Intelligent TTS and LLM utility scripts
    - `tts/` - Text-to-speech providers (ElevenLabs, OpenAI, pyttsx3)
      - `tts_queue.py` - Queue-based TTS management (prevents overlapping audio)
    - `llm/` - Language model integrations (OpenAI, Anthropic, Ollama)
      - `task_summarizer.py` - LLM-powered task completion summaries
- `.claude/status_lines/` - Real-time terminal status displays
  - `status_line.py` - Basic MVP with git info
  - `status_line_v2.py` - Smart prompts with color coding
  - `status_line_v3.py` - Agent sessions with history
  - `status_line_v4.py` - Extended metadata support
  - `status_line_v5.py` - Cost tracking with line changes
  - `status_line_v6.py` - Context window usage bar
  - `status_line_v7.py` - Session duration timer
  - `status_line_v8.py` - Token usage with cache stats
  - `status_line_v9.py` - Minimal powerline style
- `.claude/output-styles/` - Response formatting configurations
  - `genui.md` - Generates beautiful HTML with embedded styling
  - `table-based.md` - Organizes information in markdown tables
  - `yaml-structured.md` - YAML configuration format
  - `bullet-points.md` - Clean nested lists
  - `ultra-concise.md` - Minimal words, maximum speed
  - `html-structured.md` - Semantic HTML5
  - `markdown-focused.md` - Rich markdown features
  - `tts-summary.md` - Audio feedback via TTS
- `.claude/commands/` - Custom slash commands
  - `prime.md` - Project analysis and understanding
  - `plan_w_team.md` - Team-based build/validate workflow
  - `crypto_research.md` - Cryptocurrency research workflows
  - `cook.md` - Advanced task execution
  - `update_status_line.md` - Dynamic status updates
- `.claude/agents/` - Sub-agent configurations
  - `crypto/` - Cryptocurrency analysis agents
  - `team/` - Team-based workflow agents
    - `builder.md` - Implementation agent (all tools)
    - `validator.md` - Read-only validation agent
  - `hello-world-agent.md` - Simple greeting example
  - `llm-ai-agents-and-eng-research.md` - AI research specialist
  - `meta-agent.md` - Agent that creates other agents
  - `work-completion-summary.md` - Audio summary generator
- `logs/` - JSON logs of all hook executions
  - `user_prompt_submit.json` - User prompt submissions with validation
  - `pre_tool_use.json` - Tool use events with security blocking
  - `post_tool_use.json` - Tool completion events
  - `post_tool_use_failure.json` - Tool failure events with error details
  - `notification.json` - Notification events
  - `stop.json` - Stop events with completion messages
  - `subagent_stop.json` - Subagent completion events
  - `subagent_start.json` - Subagent spawn events
  - `pre_compact.json` - Pre-compaction events with trigger type
  - `session_start.json` - Session start events with source type
  - `session_end.json` - Session end events with reason
  - `permission_request.json` - Permission request audit log
  - `setup.json` - Setup events with trigger type
  - `chat.json` - Readable conversation transcript (generated by --chat flag)
- `ai_docs/` - Documentation resources
  - `cc_hooks_docs.md` - Complete hooks documentation from Anthropic
  - `claude_code_status_lines_docs.md` - Status line input schema and configuration
  - `user_prompt_submit_hook.md` - Comprehensive UserPromptSubmit hook documentation
  - `uv-single-file-scripts.md` - UV script architecture documentation
  - `anthropic_custom_slash_commands.md` - Slash commands documentation
  - `anthropic_docs_subagents.md` - Sub-agents documentation
- `ruff.toml` - Ruff linter configuration for Python code quality
- `ty.toml` - Type checker configuration for Python type validation

Hooks provide deterministic control over Claude Code behavior without relying on LLM decisions.

## Features Demonstrated

- Prompt validation and security filtering
- Context injection for enhanced AI responses
- Command logging and auditing
- Automatic transcript conversion  
- Permission-based tool access control
- Error handling in hook execution

Run any Claude Code command to see hooks in action via the `logs/` files.

## Hook Error Codes &amp; Flow Control

Claude Code hooks provide powerful mechanisms to control execution flow and provide feedback through exit codes and structured JSON output.

### Exit Code Behavior

Hooks communicate status and control flow through exit codes:

| Exit Code | Behavior           | Description                                                                                  |
| --------- | ------------------ | -------------------------------------------------------------------------------------------- |
| **0**     | Success            | Hook executed successfully. `stdout` shown to user in transcript mode (Ctrl-R)               |
| **2**     | Blocking Error     | **Critical**: `stderr` is fed back to Claude automatically. See hook-specific behavior below |
| **Other** | Non-blocking Error | `stderr` shown to user, execution continues normally                                         |

### Hook-Specific Flow Control

Each hook type has different capabilities for blocking and controlling Claude Code&#039;s behavior:

#### UserPromptSubmit Hook - **CAN BLOCK PROMPTS &amp; ADD CONTEXT**
- **Primary Control Point**: Intercepts user prompts before Claude processes them
- **Exit Code 2 Behavior**: Blocks the prompt entirely, shows error message to user
- **Use Cases**: Prompt validation, security filtering, context injection, audit logging
- **Example**: Our `user_prompt_submit.py` logs all prompts and can validate them

#### PreToolUse Hook - **CAN BLOCK TOOL EXECUTION**
- **Primary Control Point**: Intercepts tool calls before they execute
- **Exit Code 2 Behavior**: Blocks the tool call entirely, shows error message to Claude
- **Use Cases**: Security validation, parameter checking, dangerous command prevention
- **Example**: Our `pre_tool_use.py` blocks `rm -rf` commands with exit code 2

```python
# Block dangerous commands
if is_dangerous_rm_command(command):
    print(&quot;BLOCKED: Dangerous rm command detected&quot;, file=sys.stderr)
    sys.exit(2)  # Blocks tool call, shows error to Claude
```

#### PostToolUse Hook - **CANNOT BLOCK (Tool Already Executed)**
- **Primary Control Point**: Provides feedback after tool completion
- **Exit Code 2 Behavior**: Shows error to Claude (tool already ran, cannot be undone)
- **Use Cases**: Validation of results, formatting, cleanup, logging
- **Limitation**: Cannot prevent tool execution since it fires after completion

#### Notification Hook - **CANNOT BLOCK**
- **Primary Control Point**: Handles Claude Code notifications
- **Exit Code 2 Behavior**: N/A - shows stderr to user only, no blocking capability
- **Use Cases**: Custom notifications, logging, user alerts
- **Limitation**: Cannot control Claude Code behavior, purely informational

#### Stop Hook - **CAN BLOCK STOPPING**
- **Primary Control Point**: Intercepts when Claude Code tries to finish responding
- **Exit Code 2 Behavior**: Blocks stoppage, shows error to Claude (forces continuation)
- **Use Cases**: Ensuring tasks complete, validation of final state use this to FORCE CONTINUATION
- **Caution**: Can cause infinite loops if not properly controlled

#### SubagentStop Hook - **CAN BLOCK SUBAGENT STOPPING**
- **Primary Control Point**: Intercepts when Claude Code subagents try to finish
- **Exit Code 2 Behavior**: Blocks subagent stoppage, shows error to subagent
- **Use Cases**: Ensuring subagent tasks complete properly
- **Example**: Our `subagent_stop.py` logs events and announces completion

#### PreCompact Hook - **CANNOT BLOCK**
- **Primary Control Point**: Fires before compaction operations
- **Exit Code 2 Behavior**: N/A - shows stderr to user only, no blocking capability
- **Use Cases**: Transcript backup, context preservation, pre-compaction logging
- **Example**: Our `pre_compact.py` creates transcript backups before compaction

#### SessionStart Hook - **CANNOT BLOCK**
- **Primary Control Point**: Fires when new sessions start or resume
- **Exit Code 2 Behavior**: N/A - shows stderr to user only, no blocking capability
- **Use Cases**: Loading development context, session initialization, environment setup
- **Example**: Our `session_start.py` loads git status, recent issues, and context files

### Advanced JSON Output Control

Beyond simple exit codes, hooks can return structured JSON for sophisticated control:

#### Common JSON Fields (All Hook Types)
```json
{
  &quot;continue&quot;: true,           // Whether Claude should continue (default: true)
  &quot;stopReason&quot;: &quot;string&quot;,     // Message when continue=false (shown to user)
  &quot;suppressOutput&quot;: true      // Hide stdout from transcript (default: false)
}
```

#### PreToolUse Decision Control
```json
{
  &quot;decision&quot;: &quot;approve&quot; | &quot;block&quot; | undefined,
  &quot;reason&quot;: &quot;Explanation for decision&quot;
}
```

- **&quot;approve&quot;**: Bypasses permission system, `reason` shown to user
- **&quot;block&quot;**: Prevents tool execution, `reason` shown to Claude
- **undefined**: Normal permission flow, `reason` ignored

#### PostToolUse Decision Control
```json
{
  &quot;decision&quot;: &quot;block&quot; | undefined,
  &quot;reason&quot;: &quot;Explanation for decision&quot;
}
```

- **&quot;block&quot;**: Automatically prompts Claude with `reason`
- **undefined**: No action, `reason` ignored

#### Stop Decision Control
```json
{
  &quot;decision&quot;: &quot;block&quot; | undefined,
  &quot;reason&quot;: &quot;Must be provided when blocking Claude from stopping&quot;
}
```

- **&quot;block&quot;**: Prevents Claude from stopping, `reason` tells Claude how to proceed
- **undefined**: Allows normal stopping, `reason` ignored

### Flow Control Priority

When multiple control mechanisms are used, they follow this priority:

1. **`&quot;continue&quot;: false`** - Takes precedence over all other controls
2. **`&quot;decision&quot;: &quot;block&quot;`** - Hook-specific blocking behavior
3. **Exit Code 2** - Simple blocking via stderr
4. **Other Exit Codes** - Non-blocking errors

### Security Implementation Examples

#### 1. Command Validation (PreToolUse)
```python
# Block dangerous patterns
dangerous_patterns = [
    r&#039;rm\s+.*-[rf]&#039;,           # rm -rf variants
    r&#039;sudo\s+rm&#039;,              # sudo rm commands
    r&#039;chmod\s+777&#039;,            # Dangerous permissions
    r&#039;&gt;\s*/etc/&#039;,              # Writing to system directories
]

for pattern in dangerous_patterns:
    if re.search(pattern, command, re.IGNORECASE):
        print(f&quot;BLOCKED: {pattern} detected&quot;, file=sys.stderr)
        sys.exit(2)
```

#### 2. Result Validatio

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[OpenBMB/ChatDev]]></title>
            <link>https://github.com/OpenBMB/ChatDev</link>
            <guid>https://github.com/OpenBMB/ChatDev</guid>
            <pubDate>Thu, 05 Feb 2026 00:07:27 GMT</pubDate>
            <description><![CDATA[ChatDev 2.0: Dev All through LLM-powered Multi-Agent Collaboration]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenBMB/ChatDev">OpenBMB/ChatDev</a></h1>
            <p>ChatDev 2.0: Dev All through LLM-powered Multi-Agent Collaboration</p>
            <p>Language: Python</p>
            <p>Stars: 29,961</p>
            <p>Forks: 3,703</p>
            <p>Stars today: 227 stars today</p>
            <h2>README</h2><pre># ChatDev 2.0 - DevAll

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;frontend/public/media/logo.png&quot; alt=&quot;DevAll Logo&quot; width=&quot;500&quot;/&gt;
&lt;/p&gt;


&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;A Zero-Code Multi-Agent Platform for Developing Everything&lt;/strong&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  „Äê&lt;a href=&quot;./README.md&quot;&gt;English&lt;/a&gt; | &lt;a href=&quot;./README-zh.md&quot;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;„Äë
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    „Äêüìö &lt;a href=&quot;#developers&quot;&gt;Developers&lt;/a&gt; | üë• &lt;a href=&quot;#primary-contributors&quot;&gt;Contributors&lt;/a&gt;ÔΩú‚≠êÔ∏è &lt;a href=&quot;https://github.com/OpenBMB/ChatDev/tree/chatdev1.0&quot;&gt;ChatDev 1.0 (Legacy)&lt;/a&gt;„Äë
&lt;/p&gt;

## üìñ Overview
ChatDev has evolved from a specialized software development multi-agent system into a comprehensive multi-agent orchestration platform.

- &lt;a href=&quot;https://github.com/OpenBMB/ChatDev/tree/main&quot;&gt;**ChatDev 2.0 (DevAll)**&lt;/a&gt; is a **Zero-Code Multi-Agent Platform** for &quot;Developing Everything&quot;. It empowers users to rapidly build and execute customized multi-agent systems through simple configuration. No coding is required‚Äîusers can define agents, workflows, and tasks to orchestrate complex scenarios such as data visualization, 3D generation, and deep research.
- &lt;a href=&quot;https://github.com/OpenBMB/ChatDev/tree/chatdev1.0&quot;&gt;**ChatDev 1.0 (Legacy)**&lt;/a&gt; operates as a **Virtual Software Company**. It utilizes various intelligent agents (e.g., CEO, CTO, Programmer) participating in specialized functional seminars to automate the entire software development life cycle‚Äîincluding designing, coding, testing, and documenting. It serves as the foundational paradigm for communicative agent collaboration.

## üéâ News
‚Ä¢ **Jan 07, 2026: üöÄ We are excited to announce the official release of ChatDev 2.0 (DevAll)!** This version introduces a zero-code multi-agent orchestration platform. The classic ChatDev (v1.x) has been moved to the [`chatdev1.0`](https://github.com/OpenBMB/ChatDev/tree/chatdev1.0) branch for maintenance. More details about ChatDev 2.0 can be found on [our official post](https://x.com/OpenBMB/status/2008916790399701335).

&lt;details&gt;
&lt;summary&gt;Old News&lt;/summary&gt;

‚Ä¢Sep 24, 2025: üéâ Our paper [Multi-Agent Collaboration via Evolving Orchestration](https://arxiv.org/abs/2505.19591) has been accepted to NeurIPS 2025. The implementation is available in the `puppeteer` branch of this repository.

‚Ä¢May 26, 2025: üéâ We propose a novel puppeteer-style paradigm for multi-agent collaboration among large language model based agents. By leveraging a learnable central orchestrator optimized with reinforcement learning, our method dynamically activates and sequences agents to construct efficient, context-aware reasoning paths. This approach not only improves reasoning quality but also reduces computational costs, enabling scalable and adaptable multi-agent cooperation in complex tasks.
See our paper in [Multi-Agent Collaboration via Evolving Orchestration](https://arxiv.org/abs/2505.19591).
  &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&#039;./assets/puppeteer.png&#039; width=800&gt;
  &lt;/p&gt;

‚Ä¢June 25, 2024: üéâTo foster development in LLM-powered multi-agent collaborationü§ñü§ñ and related fields, the ChatDev team has curated a collection of seminal papersüìÑ presented in a [open-source](https://github.com/OpenBMB/ChatDev/tree/main/MultiAgentEbook) interactive e-booküìö format. Now you can explore the latest advancements on the [Ebook Website](https://thinkwee.top/multiagent_ebook) and download the [paper list](https://github.com/OpenBMB/ChatDev/blob/main/MultiAgentEbook/papers.csv).
  &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&#039;./assets/ebook.png&#039; width=800&gt;
  &lt;/p&gt;
  
‚Ä¢June 12, 2024: We introduced Multi-Agent Collaboration Networks (MacNet) üéâ, which utilize directed acyclic graphs to facilitate effective task-oriented collaboration among agents through linguistic interactions ü§ñü§ñ. MacNet supports co-operation across various topologies and among more than a thousand agents without exceeding context limits. More versatile and scalable, MacNet can be considered as a more advanced version of ChatDev&#039;s chain-shaped topology. Our preprint paper is available at [https://arxiv.org/abs/2406.07155](https://arxiv.org/abs/2406.07155). This technique has been incorporated into the [macnet](https://github.com/OpenBMB/ChatDev/tree/macnet) branch, enhancing support for diverse organizational structures and offering richer solutions beyond software development (e.g., logical reasoning, data analysis, story generation, and more).
  &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&#039;./assets/macnet.png&#039; width=500&gt;
  &lt;/p&gt;

‚Ä¢ May 07, 2024, we introduced &quot;Iterative Experience Refinement&quot; (IER), a novel method where instructor and assistant agents enhance shortcut-oriented experiences to efficiently adapt to new tasks. This approach encompasses experience acquisition, utilization, propagation and elimination across a series of tasks and making the pricess shorter and efficient. Our preprint paper is available at https://arxiv.org/abs/2405.04219, and this technique will soon be incorporated into ChatDev.
  &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&#039;./assets/ier.png&#039; width=220&gt;
  &lt;/p&gt;

‚Ä¢ January 25, 2024: We have integrated Experiential Co-Learning Module into ChatDev. Please see the [Experiential Co-Learning Guide](wiki.md#co-tracking).

‚Ä¢ December 28, 2023: We present Experiential Co-Learning, an innovative approach where instructor and assistant agents accumulate shortcut-oriented experiences to effectively solve new tasks, reducing repetitive errors and enhancing efficiency.  Check out our preprint paper at https://arxiv.org/abs/2312.17025 and this technique will soon be integrated into ChatDev.
  &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&#039;./assets/ecl.png&#039; width=860&gt;
  &lt;/p&gt;
‚Ä¢ November 15, 2023: We launched ChatDev as a SaaS platform that enables software developers and innovative entrepreneurs to build software efficiently at a very low cost and remove the barrier to entry. Try it out at https://chatdev.modelbest.cn/.
  &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&#039;./assets/saas.png&#039; width=560&gt;
  &lt;/p&gt;

‚Ä¢ November 2, 2023: ChatDev is now supported with a new feature: incremental development, which allows agents to develop upon existing codes. Try ```--config &quot;incremental&quot; --path &quot;[source_code_directory_path]&quot;``` to start it.
  &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&#039;./assets/increment.png&#039; width=700&gt;
  &lt;/p&gt;

‚Ä¢ October 26, 2023: ChatDev is now supported with Docker for safe execution (thanks to contribution from [ManindraDeMel](https://github.com/ManindraDeMel)). Please see [Docker Start Guide](wiki.md#docker-start).
  &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&#039;./assets/docker.png&#039; width=400&gt;
  &lt;/p&gt;
  
‚Ä¢ September 25, 2023: The **Git** mode is now available, enabling the programmer &lt;img src=&#039;visualizer/static/figures/programmer.png&#039; height=20&gt; to utilize Git for version control. To enable this feature, simply set ``&quot;git_management&quot;`` to ``&quot;True&quot;`` in ``ChatChainConfig.json``. See [guide](wiki.md#git-mode).
  &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&#039;./assets/github.png&#039; width=600&gt;
  &lt;/p&gt;

‚Ä¢ September 20, 2023: The **Human-Agent-Interaction** mode is now available! You can get involved with the ChatDev team by playing the role of reviewer &lt;img src=&#039;visualizer/static/figures/reviewer.png&#039; height=20&gt; and making suggestions to the programmer &lt;img src=&#039;visualizer/static/figures/programmer.png&#039; height=20&gt;;
  try ``python3 run.py --task [description_of_your_idea] --config &quot;Human&quot;``. See [guide](wiki.md#human-agent-interaction) and [example](WareHouse/Gomoku_HumanAgentInteraction_20230920135038).
  &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&#039;./assets/Human_intro.png&#039; width=600&gt;
  &lt;/p&gt;

‚Ä¢ September 1, 2023: The **Art** mode is available now! You can activate the designer agent &lt;img src=&#039;visualizer/static/figures/designer.png&#039; height=20&gt; to generate images used in the software;
  try ``python3 run.py --task [description_of_your_idea] --config &quot;Art&quot;``. See [guide](wiki.md#art) and [example](WareHouse/gomokugameArtExample_THUNLP_20230831122822).
  
‚Ä¢ August 28, 2023: The system is publicly available.

‚Ä¢ August 17, 2023: The v1.0.0 version was ready for release.

‚Ä¢ July 30, 2023: Users can customize ChatChain, Phasea and Role settings. Additionally, both online Log mode and replay
  mode are now supported.

‚Ä¢ July 16, 2023: The [preprint paper](https://arxiv.org/abs/2307.07924) associated with this project was published.

‚Ä¢ June 30, 2023: The initial version of the ChatDev repository was released.
&lt;/details&gt;


## üöÄ Quick Start

### üìã Prerequisites

*   **OS**: macOS / Linux / WSL / Windows
*   **Python**: 3.12+
*   **Node.js**: 18+
*   **Package Manager**: [uv](https://docs.astral.sh/uv/)

### üì¶ Installation

1.  **Backend Dependencies** (Python managed by `uv`):
    ```bash
    uv sync
    ```

2.  **Frontend Dependencies** (Vite + Vue 3):
    ```bash
    cd frontend &amp;&amp; npm install
    ```

### ‚ö°Ô∏è Run the Application

1.  **Start Backend** :
    ```bash
    # Run from the project root
    uv run python server_main.py --port 6400 --reload
    ```
    &gt; Remove `--reload` if output files (e.g., GameDev) trigger restarts, which interrupts tasks and loses progress.

2.  **Start Frontend**:
    ```bash
    cd frontend
    VITE_API_BASE_URL=http://localhost:6400 npm run dev
    ```
    &gt; Then access the Web Console at **[http://localhost:5173](http://localhost:5173)**. 
    
    
    &gt; **üí° Tip**: If the frontend fails to connect to the backend, the default port `6400` may already be occupied.
    &gt; Please switch both services to an available port, for example:
    &gt;
    &gt; * **Backend**: start with `--port 6401`
    &gt; * **Frontend**: set `VITE_API_BASE_URL=http://localhost:6401`


### üîë Configuration

*   **Environment Variables**: Create a `.env` file in the project root.
*   **Model Keys**: Set `API_KEY` and `BASE_URL` in `.env` for your LLM provider.
*   **YAML placeholders**: Use `${VAR}`Ôºàe.g., `${API_KEY}`Ôºâin configuration files to reference these variables.

---

## üí° How to Use

### üñ•Ô∏è Web Console

The DevAll interface provides a seamless experience for both construction and execution

*   **Tutorial**: Comprehensive step-by-step guides and documentation integrated directly into the platform to help you get started quickly.
&lt;img src=&quot;assets/tutorial-en.png&quot;/&gt; 

*   **Workflow**: A visual canvas to design your multi-agent systems. Configure node parameters, define context flows, and orchestrate complex agent interactions with drag-and-drop ease.
&lt;img src=&quot;assets/workflow.gif&quot;/&gt;

*   **Launch**: Initiate workflows, monitor real-time logs, inspect intermediate artifacts, and provide human-in-the-loop feedback.
&lt;img src=&quot;assets/launch.gif&quot;/&gt;

### üß∞ Python SDK
For automation and batch processing, use our lightweight Python SDK to execute workflows programmatically and retrieve results directly.

```python
from runtime.sdk import run_workflow

# Execute a workflow and get the final node message
result = run_workflow(
    yaml_file=&quot;yaml_instance/demo.yaml&quot;,
    task_prompt=&quot;Summarize the attached document in one sentence.&quot;,
    attachments=[&quot;/path/to/document.pdf&quot;],
    variables={&quot;API_KEY&quot;: &quot;sk-xxxx&quot;} # Override .env variables if needed
)

if result.final_message:
    print(f&quot;Output: {result.final_message.text_content()}&quot;)
```

---

&lt;a id=&quot;developers&quot;&gt;&lt;/a&gt;
## ‚öôÔ∏è For Developers

**For secondary development and extensions, please proceed with this section.**

Extend DevAll with new nodes, providers, and tools.
The project is organized into a modular structure:
*   **Core Systems**: `server/` hosts the FastAPI backend, while `runtime/` manages agent abstraction and tool execution.
*   **Orchestration**: `workflow/` handles the multi-agent logic, driven by configurations in `entity/`.
*   **Frontend**: `frontend/` contains the Vue 3 Web Console.
*   **Extensibility**: `functions/` is the place for custom Python tools.

Relevant reference documentation:
*   **Getting Started**: [Start Guide](./docs/user_guide/en/index.md)
*   **Core Modules**: [Workflow Authoring](./docs/user_guide/en/workflow_authoring.md), [Memory](./docs/user_guide/en/modules/memory.md), and [Tooling](./docs/user_guide/en/modules/tooling/index.md)

---

## üåü Featured Workflows
We provide robust, out-of-the-box templates for common scenarios. All runnable workflow configs are located in `yaml_instance/`.
*   **Demos**: Files named `demo_*.yaml` showcase specific features or modules.
*   **Implementations**: Files named directly (e.g., `ChatDev_v1.yaml`) are full in-house or recreated workflows. As follows:

### üìã Workflow Collection

| Category | Workflow                                                                                                    | Case | 
| :--- |:------------------------------------------------------------------------------------------------------------| :--- | 
| **üìà Data Visualization** | `data_visualization_basic.yaml`&lt;br&gt;`data_visualization_enhanced.yaml`                                       | &lt;img src=&quot;assets/cases/data_analysis/data_analysis.gif&quot; width=&quot;100%&quot;&gt;&lt;br&gt;Prompt: *&quot;Create 4‚Äì6 high-quality PNG charts for my large real-estate transactions dataset.&quot;* |
| **üõ†Ô∏è 3D Generation**&lt;br&gt;*(Requires [Blender](https://www.blender.org/) &amp; [blender-mcp](https://github.com/ahujasid/blender-mcp))* | `blender_3d_builder_simple.yaml`&lt;br&gt;`blender_3d_builder_hub.yaml`&lt;br&gt;`blender_scientific_illustration.yaml` | &lt;img src=&quot;assets/cases/3d_generation/3d.gif&quot; width=&quot;100%&quot;&gt;&lt;br&gt;Prompt: *&quot;Please build a Christmas tree.&quot;* |
| **üéÆ Game Dev** | `GameDev_v1.yaml`&lt;br&gt;`ChatDev_v1.yaml`                                                                      | &lt;img src=&quot;assets/cases/game_development/game.gif&quot; width=&quot;100%&quot;&gt;&lt;br&gt;Prompt: *&quot;Please help me design and develop a Tank Battle game.&quot;* |
| **üìö Deep Research** | `deep_research_v1.yaml`                                                                                     | &lt;img src=&quot;assets/cases/deep_research/deep_research.gif&quot; width=&quot;85%&quot;&gt;&lt;br&gt;Prompt: *&quot;Research about recent advances in the field of LLM-based agent RL&quot;* |
| **üéì Teach Video** | `teach_video.yaml` (Please run command `uv add manim` before running this workflow)                         | &lt;img src=&quot;assets/cases/video_generation/video.gif&quot; width=&quot;140%&quot;&gt;&lt;br&gt;Prompt: *&quot;ËÆ≤‰∏Ä‰∏ã‰ªÄ‰πàÊòØÂá∏‰ºòÂåñ&quot;* |

---

### üí° Usage Guide
For those implementations, you can use the **Launch** tab to execute them.
1.  **Select**: Choose a workflow in the **Launch** tab.
2.  **Upload**: Upload necessary files (e.g., `.csv` for data analysis) if required.
3.  **Prompt**: Enter your request (e.g., *&quot;Visualize the sales trends&quot;* or *&quot;Design a snake game&quot;*).

---

## ü§ù Contributing

We welcome contributions from the community! Whether you&#039;re fixing bugs, adding new workflow templates, or sharing high-quality cases/artifacts produced by DevAll, your help is much appreciated. Feel free to contribute by submitting **Issues** or **Pull Requests**.

By contributing to DevAll, you&#039;ll be recognized in our **Contributors** list below. Check out our [Developer Guide](#developers) to get started!

### üë• Contributors

#### Primary Contributors

&lt;table&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/NA-Wen&quot;&gt;&lt;img src=&quot;https://github.com/NA-Wen.png?size=100&quot; width=&quot;64px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;NA-Wen&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/zxrys&quot;&gt;&lt;img src=&quot;https://github.com/zxrys.png?size=100&quot; width=&quot;64px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;zxrys&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/swugi&quot;&gt;&lt;img src=&quot;https://github.com/swugi.png?size=100&quot; width=&quot;64px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;swugi&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/huatl98&quot;&gt;&lt;img src=&quot;https://github.com/huatl98.png?size=100&quot; width=&quot;64px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;huatl98&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

#### Contributors
&lt;table&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/shiowen&quot;&gt;&lt;img src=&quot;https://github.com/shiowen.png?size=100&quot; width=&quot;64px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;shiowen&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/kilo2127&quot;&gt;&lt;img src=&quot;https://github.com/kilo2127.png?size=100&quot; width=&quot;64px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;kilo2127&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/AckerlyLau&quot;&gt;&lt;img src=&quot;https://github.com/AckerlyLau.png?size=100&quot; width=&quot;64px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;AckerlyLau&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/table&gt;

## ü§ù Acknowledgments

&lt;a href=&quot;http://nlp.csai.tsinghua.edu.cn/&quot;&gt;&lt;img src=&quot;assets/thunlp.png&quot; height=50pt&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
&lt;a href=&quot;https://modelbest.cn/&quot;&gt;&lt;img src=&quot;assets/modelbest.png&quot; height=50pt&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
&lt;a href=&quot;https://github.com/OpenBMB/AgentVerse/&quot;&gt;&lt;img src=&quot;assets/agentverse.png&quot; height=50pt&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
&lt;a href=&quot;https://github.com/OpenBMB/RepoAgent&quot;&gt;&lt;img src=&quot;assets/repoagent.png&quot;  height=50pt&gt;&lt;/a&gt;
&lt;a href=&quot;https://app.commanddash.io/agent?github=https://github.com/OpenBMB/ChatDev&quot;&gt;&lt;img src=&quot;assets/CommandDash.png&quot; height=50pt&gt;&lt;/a&gt;
&lt;a href=&quot;www.teachmaster.cn&quot;&gt;&lt;img src=&quot;assets/teachmaster.png&quot; height=50pt&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/OpenBMB/AppCopilot&quot;&gt;&lt;img src=&quot;assets/appcopilot.png&quot; height=50pt&gt;&lt;/a&gt;

## üîé Citation

```
@article{chatdev,
    title = {ChatDev: Communicative Agents for Software Development},
    author = {Chen Qian and Wei Liu and Hongzhang Liu and Nuo Chen and Yufan Dang and Jiahao Li and Cheng Yang and Weize Chen and Yusheng Su and Xin Cong and Juyuan Xu and Dahai Li and Zhiyuan Liu and Maosong Sun},
    journal = {arXiv preprint arXiv:2307.07924},
    url = {https://arxiv.org/abs/2307.07924},
    year = {2023}
}

@article{colearning,
    title = {Experiential Co-Learning of Software-Developing Agents},
    author = {Chen Qian and Yufan Dang and Jiahao Li and Wei Liu and Zihao Xie and Yifei Wang and Weize Chen and Cheng Yang and Xin Cong and Xiaoyin Che and Zhiyuan Liu and Maosong Sun},
    journal = {arXiv preprint arXiv:2312.17025},
    url = {https://arxiv.org/abs/2312.17025},
    year = {2023}
}

@article{macnet,
    title={Scaling Large-Language-Model-based Multi-Agent Collaboration},
    author={Chen Qian and Zihao Xie and Yifei Wang and Wei Liu and Yufan Dang and Zhuoyun Du and Weize Chen and Cheng Yang and Zhiyuan Liu and Maosong Sun}
    journal={arXiv preprint arXiv:2406.07155},
    url = {https://arxiv.org/abs/2406.07155},
    year={2024}
}

@article{iagents,
    title={Autonomous Agents for Collaborative Task under Information Asymmetry},
    author={Wei Liu and Chenxi Wang and Yifei Wang and Zihao Xie and Rennai Qiu and Yufan Dnag and Zhuoyun Du and Weize Chen and Cheng Yang and Chen Qian},
    journal={arXiv preprint arXiv:2406.14928},
    url = {https://arxiv.org/abs/2406.14928},
    year={2024}
}

@article{puppeteer,
      title={Multi-Agent Collaboration via Evolving Orchestration}, 
      author={Yufan Dang and Chen Qian and Xueheng Luo and Jingru Fan and Zihao Xie and Ruijie Shi and Weize Chen and Cheng Yang and Xiaoyin Che and Ye Tian and Xuantang Xiong and Lei Han and Zhiyuan Liu and Maosong Sun},
      journal={arXiv preprint arXiv:2505.19591},
      url={https://arxiv.org/abs/2505.19591},
      year={2025}
}
```

## üì¨ Contact

If you have any questions, feedback, or would like to get in touch, please feel free to reach out to us via email at [qianc62@gmail.com](mailto:qianc62@gmail.com)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/qlib]]></title>
            <link>https://github.com/microsoft/qlib</link>
            <guid>https://github.com/microsoft/qlib</guid>
            <pubDate>Thu, 05 Feb 2026 00:07:26 GMT</pubDate>
            <description><![CDATA[Qlib is an AI-oriented Quant investment platform that aims to use AI tech to empower Quant Research, from exploring ideas to implementing productions. Qlib supports diverse ML modeling paradigms, including supervised learning, market dynamics modeling, and RL, and is now equipped with https://github.com/microsoft/RD-Agent to automate R&D process.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/qlib">microsoft/qlib</a></h1>
            <p>Qlib is an AI-oriented Quant investment platform that aims to use AI tech to empower Quant Research, from exploring ideas to implementing productions. Qlib supports diverse ML modeling paradigms, including supervised learning, market dynamics modeling, and RL, and is now equipped with https://github.com/microsoft/RD-Agent to automate R&D process.</p>
            <p>Language: Python</p>
            <p>Stars: 36,522</p>
            <p>Forks: 5,686</p>
            <p>Stars today: 83 stars today</p>
            <h2>README</h2><pre>[![Python Versions](https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&amp;logoColor=white)](https://pypi.org/project/pyqlib/#files)
[![Platform](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey)](https://pypi.org/project/pyqlib/#files)
[![PypI Versions](https://img.shields.io/pypi/v/pyqlib)](https://pypi.org/project/pyqlib/#history)
[![Upload Python Package](https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/pyqlib/)
[![Github Actions Test Status](https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main)](https://github.com/microsoft/qlib/actions)
[![Documentation Status](https://readthedocs.org/projects/qlib/badge/?version=latest)](https://qlib.readthedocs.io/en/latest/?badge=latest)
[![License](https://img.shields.io/pypi/l/pyqlib)](LICENSE)
[![Join the chat at https://gitter.im/Microsoft/qlib](https://badges.gitter.im/Microsoft/qlib.svg)](https://gitter.im/Microsoft/qlib?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge)

## :newspaper: **What&#039;s NEW!** &amp;nbsp;   :sparkling_heart: 

Recent released features

### Introducing &lt;a href=&quot;https://github.com/microsoft/RD-Agent&quot;&gt;&lt;img src=&quot;docs/_static/img/rdagent_logo.png&quot; alt=&quot;RD_Agent&quot; style=&quot;height: 2em&quot;&gt;&lt;/a&gt;: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&amp;D

We are excited to announce the release of **RD-Agent**üì¢, a powerful tool that supports automated factor mining and model optimization in quant investment R&amp;D.

RD-Agent is now available on [GitHub](https://github.com/microsoft/RD-Agent), and we welcome your starüåü!

To learn more, please visit our [‚ôæÔ∏èDemo page](https://rdagent.azurewebsites.net/). Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.

We have prepared several demo videos for you:
| Scenario | Demo video (English) | Demo video (‰∏≠Êñá) |
| --                      | ------    | ------    |
| Quant Factor Mining | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=zh) |
| Quant Factor Mining from reports | [Link](https://rdagent.azurewebsites.net/report_factor?lang=en) | [Link](https://rdagent.azurewebsites.net/report_factor?lang=zh) |
| Quant Model Optimization | [Link](https://rdagent.azurewebsites.net/model_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/model_loop?lang=zh) |

- üìÉ**Paper**: [R&amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization](https://arxiv.org/abs/2505.15155)
- üëæ**Code**: https://github.com/microsoft/RD-Agent/
```BibTeX
@misc{li2025rdagentquant,
    title={R\&amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},
    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},
    year={2025},
    eprint={2505.15155},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
```
![image](https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d)

***

| Feature | Status |
| --                      | ------    |
| [R&amp;D-Agent-Quant](https://arxiv.org/abs/2505.15155) Published | Apply R&amp;D-Agent to Qlib for quant trading | 
| BPQP for End-to-end learning | üìàComing soon!([Under review](https://github.com/microsoft/qlib/pull/1863)) |
| üî•LLM-driven Auto Quant Factoryüî• | üöÄ Released in [‚ôæÔ∏èRD-Agent](https://github.com/microsoft/RD-Agent) on Aug 8, 2024 |
| KRNN and Sandwich models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1414/) on May 26, 2023 |
| Release Qlib v0.9.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.9.0) on Dec 9, 2022 |
| RL Learning Framework | :hammer: :chart_with_upwards_trend: Released on Nov 10, 2022. [#1332](https://github.com/microsoft/qlib/pull/1332), [#1322](https://github.com/microsoft/qlib/pull/1322), [#1316](https://github.com/microsoft/qlib/pull/1316),[#1299](https://github.com/microsoft/qlib/pull/1299),[#1263](https://github.com/microsoft/qlib/pull/1263), [#1244](https://github.com/microsoft/qlib/pull/1244), [#1169](https://github.com/microsoft/qlib/pull/1169), [#1125](https://github.com/microsoft/qlib/pull/1125), [#1076](https://github.com/microsoft/qlib/pull/1076)|
| HIST and IGMTF models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1040) on Apr 10, 2022 |
| Qlib [notebook tutorial](https://github.com/microsoft/qlib/tree/main/examples/tutorial) | üìñ [Released](https://github.com/microsoft/qlib/pull/1037) on Apr 7, 2022 | 
| Ibovespa index data | :rice: [Released](https://github.com/microsoft/qlib/pull/990) on Apr 6, 2022 |
| Point-in-Time database | :hammer: [Released](https://github.com/microsoft/qlib/pull/343) on Mar 10, 2022 |
| Arctic Provider Backend &amp; Orderbook data example | :hammer: [Released](https://github.com/microsoft/qlib/pull/744) on Jan 17, 2022 |
| Meta-Learning-based framework &amp; DDG-DA  | :chart_with_upwards_trend:  :hammer: [Released](https://github.com/microsoft/qlib/pull/743) on Jan 10, 2022 | 
| Planning-based portfolio optimization | :hammer: [Released](https://github.com/microsoft/qlib/pull/754) on Dec 28, 2021 | 
| Release Qlib v0.8.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.8.0) on Dec 8, 2021 |
| ADD model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/704) on Nov 22, 2021 |
| ADARNN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/689) on Nov 14, 2021 |
| TCN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/668) on Nov 4, 2021 |
| Nested Decision Framework | :hammer: [Released](https://github.com/microsoft/qlib/pull/438) on Oct 1, 2021. [Example](https://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/workflow.py) and [Doc](https://qlib.readthedocs.io/en/latest/component/highfreq.html) |
| Temporal Routing Adaptor (TRA) | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/531) on July 30, 2021 |
| Transformer &amp; Localformer | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/508) on July 22, 2021 |
| Release Qlib v0.7.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.7.0) on July 12, 2021 |
| TCTS Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/491) on July 1, 2021 |
| Online serving and automatic model rolling | :hammer:  [Released](https://github.com/microsoft/qlib/pull/290) on May 17, 2021 | 
| DoubleEnsemble Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/286) on Mar 2, 2021 | 
| High-frequency data processing example | :hammer: [Released](https://github.com/microsoft/qlib/pull/257) on Feb 5, 2021  |
| High-frequency trading example | :chart_with_upwards_trend: [Part of code released](https://github.com/microsoft/qlib/pull/227) on Jan 28, 2021  | 
| High-frequency data(1min) | :rice: [Released](https://github.com/microsoft/qlib/pull/221) on Jan 27, 2021 |
| Tabnet Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/205) on Jan 22, 2021 |

Features released before 2021 are not listed here.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/_static/img/logo/1.png&quot; /&gt;
&lt;/p&gt;

Qlib is an open-source, AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms, including supervised learning, market dynamics modeling, and reinforcement learning.

An increasing number of SOTA Quant research works/papers in diverse paradigms are being released in Qlib to collaboratively solve key challenges in quantitative investment. For example, 1) using supervised learning to mine the market&#039;s complex non-linear patterns from rich and heterogeneous financial data, 2) modeling the dynamic nature of the financial market using adaptive concept drift technology, and 3) using reinforcement learning to model continuous investment decisions and assist investors in optimizing their trading strategies.

It contains the full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution. 
For more details, please refer to our paper [&quot;Qlib: An AI-oriented Quantitative Investment Platform&quot;](https://arxiv.org/abs/2009.11189).


&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Frameworks, Tutorial, Data &amp; DevOps&lt;/th&gt;
      &lt;th&gt;Main Challenges &amp; Solutions in Quant Research&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;li&gt;&lt;a href=&quot;#plans&quot;&gt;&lt;strong&gt;Plans&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#framework-of-qlib&quot;&gt;Framework of Qlib&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#quick-start&quot;&gt;Quick Start&lt;/a&gt;&lt;/li&gt;
          &lt;ul dir=&quot;auto&quot;&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#installation&quot;&gt;Installation&lt;/a&gt; &lt;/li&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#data-preparation&quot;&gt;Data Preparation&lt;/a&gt;&lt;/li&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#auto-quant-research-workflow&quot;&gt;Auto Quant Research Workflow&lt;/a&gt;&lt;/li&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#building-customized-quant-research-workflow-by-code&quot;&gt;Building Customized Quant Research Workflow by Code&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
        &lt;li&gt;&lt;a href=&quot;#quant-dataset-zoo&quot;&gt;&lt;strong&gt;Quant Dataset Zoo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#learning-framework&quot;&gt;Learning Framework&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#more-about-qlib&quot;&gt;More About Qlib&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#offline-mode-and-online-mode&quot;&gt;Offline Mode and Online Mode&lt;/a&gt;
        &lt;ul&gt;
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#performance-of-qlib-data-server&quot;&gt;Performance of Qlib Data Server&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
        &lt;li&gt;&lt;a href=&quot;#related-reports&quot;&gt;Related Reports&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#contact-us&quot;&gt;Contact Us&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#contributing&quot;&gt;Contributing&lt;/a&gt;&lt;/li&gt;
      &lt;/td&gt;
      &lt;td valign=&quot;baseline&quot;&gt;
        &lt;li&gt;&lt;a href=&quot;#main-challenges--solutions-in-quant-research&quot;&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/a&gt;
          &lt;ul&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#forecasting-finding-valuable-signalspatterns&quot;&gt;Forecasting: Finding Valuable Signals/Patterns&lt;/a&gt;
              &lt;ul&gt;
                &lt;li type=&quot;disc&quot;&gt;&lt;a href=&quot;#quant-model-paper-zoo&quot;&gt;&lt;strong&gt;Quant Model (Paper) Zoo&lt;/strong&gt;&lt;/a&gt;
                  &lt;ul&gt;
                    &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#run-a-single-model&quot;&gt;Run a Single Model&lt;/a&gt;&lt;/li&gt;
                    &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#run-multiple-models&quot;&gt;Run Multiple Models&lt;/a&gt;&lt;/li&gt;
                  &lt;/ul&gt;
                &lt;/li&gt;
              &lt;/ul&gt;
            &lt;/li&gt;
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#adapting-to-market-dynamics&quot;&gt;Adapting to Market Dynamics&lt;/a&gt;&lt;/li&gt;
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#reinforcement-learning-modeling-continuous-decisions&quot;&gt;Reinforcement Learning: modeling continuous decisions&lt;/a&gt;&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

# Plans
New features under development(order by estimated release time).
Your feedbacks about the features are very important.
&lt;!-- | Feature                        | Status      | --&gt;
&lt;!-- | --                      | ------    | --&gt;

# Framework of Qlib

&lt;div style=&quot;align: center&quot;&gt;
&lt;img src=&quot;docs/_static/img/framework-abstract.jpg&quot; /&gt;
&lt;/div&gt;

The high-level framework of Qlib can be found above(users can find the [detailed framework](https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework) of Qlib&#039;s design when getting into nitty gritty).
The components are designed as loose-coupled modules, and each component could be used stand-alone.

Qlib provides a strong infrastructure to support Quant research. [Data](https://qlib.readthedocs.io/en/latest/component/data.html) is always an important part.
A strong learning framework is designed to support diverse learning paradigms (e.g. [reinforcement learning](https://qlib.readthedocs.io/en/latest/component/rl.html), [supervised learning](https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section)) and patterns at different levels(e.g. [market dynamic modeling](https://qlib.readthedocs.io/en/latest/component/meta.html)).
By modeling the market, [trading strategies](https://qlib.readthedocs.io/en/latest/component/strategy.html) will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be [nested to be optimized and run together](https://qlib.readthedocs.io/en/latest/component/highfreq.html).
At last, a comprehensive [analysis](https://qlib.readthedocs.io/en/latest/component/report.html) will be provided and the model can be [served online](https://qlib.readthedocs.io/en/latest/component/online.html) in a low cost.


# Quick Start

This quick start guide tries to demonstrate
1. It&#039;s very easy to build a complete Quant research workflow and try your ideas with _Qlib_.
2. Though with *public data* and *simple models*, machine learning technologies **work very well** in practical Quant investment.

Here is a quick **[demo](https://terminalizer.com/view/3f24561a4470)** shows how to install ``Qlib``, and run LightGBM with ``qrun``. **But**, please make sure you have already prepared the data following the [instruction](#data-preparation).


## Installation

This table demonstrates the supported Python version of `Qlib`:
|               | install with pip      | install from source  |        plot        |
| ------------- |:---------------------:|:--------------------:|:------------------:|
| Python 3.8    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.9    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.10   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.11   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.12   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |

**Note**: 
1. **Conda** is suggested for managing your Python environment. In some cases, using Python outside of a `conda` environment may result in missing header files, causing the installation failure of certain packages.
2. Please pay attention that installing cython in Python 3.6 will raise some error when installing ``Qlib`` from source. If users use Python 3.6 on their machines, it is recommended to *upgrade* Python to version 3.8 or higher, or use `conda`&#039;s Python to install ``Qlib`` from source.

### Install with pip
Users can easily install ``Qlib`` by pip according to the following command.

```bash
  pip install pyqlib
```

**Note**: pip will install the latest stable qlib. However, the main branch of qlib is in active development. If you want to test the latest scripts or functions in the main branch. Please install qlib with the methods below.

### Install from source
Also, users can install the latest dev version ``Qlib`` by the source code according to the following steps:

* Before installing ``Qlib`` from source, users need to install some dependencies:

  ```bash
  pip install numpy
  pip install --upgrade cython
  ```

* Clone the repository and install ``Qlib`` as follows.
    ```bash
    git clone https://github.com/microsoft/qlib.git &amp;&amp; cd qlib
    pip install .  # `pip install -e .[dev]` is recommended for development. check details in docs/developer/code_standard_and_dev_guide.rst
    ```

**Tips**: If you fail to install `Qlib` or run the examples in your environment,  comparing your steps and the [CI workflow](.github/workflows/test_qlib_from_source.yml) may help you find the problem.

**Tips for Mac**: If you are using Mac with M1, you might encounter issues in building the wheel for LightGBM, which is due to missing dependencies from OpenMP. To solve the problem, install openmp first with ``brew install libomp`` and then run ``pip install .`` to build it successfully. 

## Data Preparation
‚ùó Due to more restrict data security policy. The official dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.
Here is an example to download the latest data.
```bash
wget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz
mkdir -p ~/.qlib/qlib_data/cn_data
tar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1
rm -f qlib_bin.tar.gz
```

The official dataset below will resume in short future.


----

Load and prepare data by running the following code:

### Get with module
  ```bash
  # get 1d data
  python -m qlib.cli.data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

  # get 1min data
  python -m qlib.cli.data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

  ```

### Get from source

  ```bash
  # get 1d data
  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

  # get 1min data
  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

  ```

This dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in
the same repository.
Users could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)

*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.
We recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.

### Automatic update of daily frequency data (from yahoo finance)
  &gt; This step is *Optional* if users only want to try their models and strategies on history data.
  &gt; 
  &gt; It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.
  &gt;
  &gt; **NOTE**: Users can&#039;t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.
  &gt; 
  &gt; For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)

  * Automatic update of data to the &quot;qlib&quot; directory each trading day(Linux)
      * use *crontab*: `crontab -e`
      * set up timed tasks:

        ```
        * * * * 1-5 python &lt;script path&gt; update_data_to_bin --qlib_data_1d_dir &lt;user data dir&gt;
        ```
        * **script path**: *scripts/data_collector/yahoo/collector.py*

  * Manual update of data
      ```
      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir &lt;user data dir&gt; --trading_date &lt;start date&gt; --end_date &lt;end date&gt;
      ```
      * *trading_date*: start of trading day
      * *end_date*: end of trading day(not included)

### Checking the health of the data
  * We provide a script to check the health of the data, you can run the following commands to check whether the data is healthy or not.
    ```
    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data
    ```
  * Of course, you can also add some parameters to adjust the test results, such as this.
    ```
    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_dat

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[karpathy/nanochat]]></title>
            <link>https://github.com/karpathy/nanochat</link>
            <guid>https://github.com/karpathy/nanochat</guid>
            <pubDate>Thu, 05 Feb 2026 00:07:25 GMT</pubDate>
            <description><![CDATA[The best ChatGPT that $100 can buy.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/karpathy/nanochat">karpathy/nanochat</a></h1>
            <p>The best ChatGPT that $100 can buy.</p>
            <p>Language: Python</p>
            <p>Stars: 42,261</p>
            <p>Forks: 5,448</p>
            <p>Stars today: 313 stars today</p>
            <h2>README</h2><pre># nanochat

![nanochat logo](dev/nanochat.png)
![scaling laws](dev/scaling_laws_jan26.png)

nanochat is the simplest experimental harness for training LLMs. It is designed to run on a single GPU node, the code is minimal/hackable, and it covers all major LLM stages including tokenization, pretraining, finetuning, evaluation, inference, and a chat UI. For example, you can train your own GPT-2 capability LLM (which cost ~$50,000 to train in 2019) for only $73 (3 hours of 8XH100 GPU node) and then talk to it in a familiar ChatGPT-like web UI.

For questions about the repo, I recommend either using [DeepWiki](https://deepwiki.com/karpathy/nanochat) from Devin/Cognition to ask questions about the repo, or use the [Discussions tab](https://github.com/karpathy/nanochat/discussions), or come by the [#nanochat](https://discord.com/channels/1020383067459821711/1427295580895314031) channel on Discord.

## Updates

- (Jan 31 2026) Major revamp of all scripts/README ongoing, deleting midtraining stage, might be a bit messy briefly...
- (Jan 30 2026) With all the latest improvements we&#039;re able to train GPT-2 grade LLM in about $73. The [runs/speedrun.sh](runs/speedrun.sh) script will become the refernece way to train GPT-2 grade model and talk to it.

## Leaderboard

| # | time | val_bpb | CORE | Description | Date | Commit | Contributors |
|---|-------------|---------|------|-------------|------|--------|--------------|
| 0 | 168 hours | - | 0.2565 | Original OpenAI GPT-2 checkpoint | 2019 | - | OpenAI |
| 1 | 3.04 | 0.74833 | 0.2585 | d24 baseline, slightly overtrained | Jan 29 2026 | 348fbb3 | @karpathy |
| 2 | 2.91 | 0.74504 | 0.2578 | d26 slightly undertrained **+fp8** | Feb 2 2026 | 8309b83 | @karpathy |

The primary metric we care about is &quot;time to GPT-2&quot; - the wall clock time needed to outperform the GPT-2 (1.6B) CORE metric on an 8XH100 GPU node. The GPT-2 CORE score is 0.256525. In 2019, the training of GPT-2 cost approximately $50,000 so it is incredible that due to many advances over 7 years across the stack, we can now do so much faster and for well below $100 (e.g. at the current ~$3/GPU/hr, an 8XH100 node is ~$24/hr, so 3 hours is ~$72).

See [dev/LEADERBOARD.md](dev/LEADERBOARD.md) for more docs on how to interpret and contribute to the leaderboard.

## Getting started

### Reproduce and talk to GPT-2

The most fun you can have is to train your own GPT-2 and talk to it. The entire pipeline to do so is contained in the single file [runs/speedrun.sh](runs/speedrun.sh), which is designed to be run on an 8XH100 GPU node. Currently, at ~$24/hour for these nodes, pretraining GPT-2 grade model takes approximately 3 hours and will set you back about $75. Boot up a new 8XH100 GPU box from your favorite provider (e.g. I use and like [Lambda](https://lambda.ai/service/gpu-cloud)), and kick off the training script:

```bash
bash runs/speedrun.sh
```

You mish to do so in a screen session as this will take ~3 hours to run. Once it&#039;s done, you can talk to it via the ChatGPT-like web UI. Make sure again that your local uv virtual environment is active (run `source .venv/bin/activate`), and serve it:

```bash
python -m scripts.chat_web
```

And then visit the URL shown. Make sure to access it correctly, e.g. on Lambda use the public IP of the node you&#039;re on, followed by the port, so for example [http://209.20.xxx.xxx:8000/](http://209.20.xxx.xxx:8000/), etc. Then talk to your LLM as you&#039;d normally talk to ChatGPT! Get it to write stories or poems. Ask it to tell you who you are to see a hallucination. Ask it why the sky is blue. Or why it&#039;s green. The speedrun is a 4e19 FLOPs capability model so it&#039;s a bit like talking to a kindergartener :).

---

&lt;img width=&quot;2672&quot; height=&quot;1520&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/ed39ddf8-2370-437a-bedc-0f39781e76b5&quot; /&gt;

---

A few more notes:

- The code will run just fine on the Ampere 8XA100 GPU node as well, but a bit slower.
- All code will run just fine on even a single GPU by omitting `torchrun`, and will produce ~identical results (code will automatically switch to gradient accumulation), but you&#039;ll have to wait 8 times longer.
- If your GPU(s) have less than 80GB, you&#039;ll have to tune some of the hyperparameters or you will OOM / run out of VRAM. Look for `--device_batch_size` in the scripts and reduce it until things fit. E.g. from 32 (default) to 16, 8, 4, 2, or even 1. Less than that you&#039;ll have to know a bit more what you&#039;re doing and get more creative.
- Most of the code is fairly vanilla PyTorch so it should run on anything that supports that - xpu, mps, or etc, but I haven&#039;t personally exercised all of these code paths so there might be sharp edges.

## Research

If you are a researcher and wish to help improve nanochat, two scripts of interest are [runs/scaling_laws.sh](runs/scaling_laws.sh) and [runs/miniseries.sh](runs/miniseries.sh). See [Jan 7 miniseries v1](https://github.com/karpathy/nanochat/discussions/420) for related documentation. For quick experimentation (~5 min pretraining runs) my favorite scale is to train a 12-layer model (GPT-1 sized), e.g. like this:

```
OMP_NUM_THREADS=1 torchrun --standalone --nproc_per_node=8 -m scripts.base_train -- \
    --depth=12 \
    --run=&quot;d12&quot; \
    --model-tag=&quot;d12&quot; \
    --core-metric-every=999999 \
    --sample-every=-1 \
    --save-every=-1 \
```

This uses wandb (run name &quot;d12&quot;), only runs the CORE metric on last step, and it doesn&#039;t sample and save intermediate checkpoints. I like to change something in the code, re-run a d12 (or a d16 etc) and see if it helped, in an iteration loop.

The overall approach is to treat the depth of the model as the single dial of complexity. By sweeping out the depth, we get increasingly more powerful models. We determine the scaling laws, set the data budget to a compute optimal setting, train a whole miniseries of models of increasing sizes, and compare them to the GPT-2 and GPT-3 miniseries. Right now, beating GPT-2 specifically faster and faster is the most interesting target.

## Running on CPU / MPS

The script [runs/runcpu.sh](runs/runcpu.sh) shows a very simple example of running on CPU or Apple Silicon. It dramatically shrinks the LLM tha tis being trained to make things fit into a reasonable time interval of a few ten minutes of training. You will not get strong results in this way.

## Guides

I&#039;ve published a number of guides that might contain helpful information:

- [Oct 13 2025 original nanochat post](https://github.com/karpathy/nanochat/discussions/1) introducing nanochat, though now it contains some deprecated information and the model is a lot older (with worse results) than current master.
- [Jan 7 miniseries v1](https://github.com/karpathy/nanochat/discussions/420) documents the first nanochat miniseries of models.
- To customize your nanochat, see [Guide: infusing identity to your nanochat](https://github.com/karpathy/nanochat/discussions/139) in Discussions, which describes how you can tune your nanochat&#039;s personality through synthetic data generation and mixing that data into the SFT stage.
- To add new abilities to nanochat, see [Guide: counting r in strawberry (and how to add abilities generally)](https://github.com/karpathy/nanochat/discussions/164).

## File structure

```
.
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ dev
‚îÇ   ‚îú‚îÄ‚îÄ gen_synthetic_data.py       # Example synthetic data for identity
‚îÇ   ‚îú‚îÄ‚îÄ generate_logo.html
‚îÇ   ‚îú‚îÄ‚îÄ nanochat.png
‚îÇ   ‚îî‚îÄ‚îÄ repackage_data_reference.py # Pretraining data shard generation
‚îú‚îÄ‚îÄ nanochat
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                 # empty
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint_manager.py       # Save/Load model checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ common.py                   # Misc small utilities, quality of life
‚îÇ   ‚îú‚îÄ‚îÄ core_eval.py                # Evaluates base model CORE score (DCLM paper)
‚îÇ   ‚îú‚îÄ‚îÄ dataloader.py               # Tokenizing Distributed Data Loader
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py                  # Download/read utils for pretraining data
‚îÇ   ‚îú‚îÄ‚îÄ engine.py                   # Efficient model inference with KV Cache
‚îÇ   ‚îú‚îÄ‚îÄ execution.py                # Allows the LLM to execute Python code as tool
‚îÇ   ‚îú‚îÄ‚îÄ gpt.py                      # The GPT nn.Module Transformer
‚îÇ   ‚îú‚îÄ‚îÄ logo.svg
‚îÇ   ‚îú‚îÄ‚îÄ loss_eval.py                # Evaluate bits per byte (instead of loss)
‚îÇ   ‚îú‚îÄ‚îÄ optim.py                    # AdamW + Muon optimizer, 1GPU and distributed
‚îÇ   ‚îú‚îÄ‚îÄ report.py                   # Utilities for writing the nanochat Report
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.py                # BPE Tokenizer wrapper in style of GPT-4
‚îÇ   ‚îî‚îÄ‚îÄ ui.html                     # HTML/CSS/JS for nanochat frontend
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ runs
‚îÇ   ‚îú‚îÄ‚îÄ miniseries.sh               # Miniseries training script
‚îÇ   ‚îú‚îÄ‚îÄ runcpu.sh                   # Small example of how to run on CPU/MPS
‚îÇ   ‚îú‚îÄ‚îÄ scaling_laws.sh             # Scaling laws experiments
‚îÇ   ‚îî‚îÄ‚îÄ speedrun.sh                 # Train the ~$100 nanochat d20
‚îú‚îÄ‚îÄ scripts
‚îÇ   ‚îú‚îÄ‚îÄ base_eval.py                # Base model: CORE score, bits per byte, samples
‚îÇ   ‚îú‚îÄ‚îÄ base_train.py               # Base model: train
‚îÇ   ‚îú‚îÄ‚îÄ chat_cli.py                 # Chat model: talk to over CLI
‚îÇ   ‚îú‚îÄ‚îÄ chat_eval.py                # Chat model: eval tasks
‚îÇ   ‚îú‚îÄ‚îÄ chat_rl.py                  # Chat model: reinforcement learning
‚îÇ   ‚îú‚îÄ‚îÄ chat_sft.py                 # Chat model: train SFT
‚îÇ   ‚îú‚îÄ‚îÄ chat_web.py                 # Chat model: talk to over WebUI
‚îÇ   ‚îú‚îÄ‚îÄ tok_eval.py                 # Tokenizer: evaluate compression rate
‚îÇ   ‚îî‚îÄ‚îÄ tok_train.py                # Tokenizer: train it
‚îú‚îÄ‚îÄ tasks
‚îÇ   ‚îú‚îÄ‚îÄ arc.py                      # Multiple choice science questions
‚îÇ   ‚îú‚îÄ‚îÄ common.py                   # TaskMixture | TaskSequence
‚îÇ   ‚îú‚îÄ‚îÄ customjson.py               # Make Task from arbitrary jsonl convos
‚îÇ   ‚îú‚îÄ‚îÄ gsm8k.py                    # 8K Grade School Math questions
‚îÇ   ‚îú‚îÄ‚îÄ humaneval.py                # Misnomer; Simple Python coding task
‚îÇ   ‚îú‚îÄ‚îÄ mmlu.py                     # Multiple choice questions, broad topics
‚îÇ   ‚îú‚îÄ‚îÄ smoltalk.py                 # Conglomerate dataset of SmolTalk from HF
‚îÇ   ‚îî‚îÄ‚îÄ spellingbee.py              # Task teaching model to spell/count letters
‚îú‚îÄ‚îÄ tests
‚îÇ   ‚îî‚îÄ‚îÄ test_engine.py
‚îî‚îÄ‚îÄ uv.lock
```

## Contributing

The goal of nanochat is to improve the state of the art in micro models that are accessible to work with end to end on budgets of &lt; $1000 dollars. Accessibility is about overall cost but also about cognitive complexity - nanochat is not an exhaustively configurable LLM &quot;framework&quot;; there are no giant configuration objects, model factories, or if-then-else monsters in the code base. It is a single, cohesive, minimal, readable, hackable, maximally-forkable &quot;strong baseline&quot; codebase designed to run start to end and produce a ChatGPT model you can talk to. Currently, the most interesting part personally is speeding up the latency to GPT-2 (i.e. getting a CORE score above 0.256525). Currently this takes ~3 hours, but by improving the pretraining stage we can improve this further.

Current AI policy: disclosure. When submitting a PR, please declare any parts that had substantial LLM contribution and that you have not written or that you do not fully understand.

## Acknowledgements

- The name (nanochat) derives from my earlier project [nanoGPT](https://github.com/karpathy/nanoGPT), which only covered pretraining.
- nanochat is also inspired by [modded-nanoGPT](https://github.com/KellerJordan/modded-nanogpt), which gamified the nanoGPT repo with clear metrics and a leaderboard, and borrows a lot of its ideas and some implementation for pretraining.
- Thank you to [HuggingFace](https://huggingface.co/) for fineweb and smoltalk.
- Thank you [Lambda](https://lambda.ai/service/gpu-cloud) for the compute used in developing this project.
- Thank you to chief LLM whisperer üßô‚Äç‚ôÇÔ∏è Alec Radford for advice/guidance.
- Thank you to the repo czar Sofie [@svlandeg](https://github.com/svlandeg) for help with managing issues, pull requests and discussions of nanochat.

## Cite

If you find nanochat helpful in your research cite simply as:

```bibtex
@misc{nanochat,
  author = {Andrej Karpathy},
  title = {nanochat: The best ChatGPT that \$100 can buy},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/karpathy/nanochat}
}
```

## License

MIT
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[AtsushiSakai/PythonRobotics]]></title>
            <link>https://github.com/AtsushiSakai/PythonRobotics</link>
            <guid>https://github.com/AtsushiSakai/PythonRobotics</guid>
            <pubDate>Thu, 05 Feb 2026 00:07:24 GMT</pubDate>
            <description><![CDATA[Python sample codes and textbook for robotics algorithms.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/AtsushiSakai/PythonRobotics">AtsushiSakai/PythonRobotics</a></h1>
            <p>Python sample codes and textbook for robotics algorithms.</p>
            <p>Language: Python</p>
            <p>Stars: 28,547</p>
            <p>Forks: 7,210</p>
            <p>Stars today: 14 stars today</p>
            <h2>README</h2><pre>&lt;img src=&quot;https://github.com/AtsushiSakai/PythonRobotics/raw/master/icon.png?raw=true&quot; align=&quot;right&quot; width=&quot;300&quot; alt=&quot;header pic&quot;/&gt;

# PythonRobotics
![GitHub_Action_Linux_CI](https://github.com/AtsushiSakai/PythonRobotics/workflows/Linux_CI/badge.svg)
![GitHub_Action_MacOS_CI](https://github.com/AtsushiSakai/PythonRobotics/workflows/MacOS_CI/badge.svg)
![GitHub_Action_Windows_CI](https://github.com/AtsushiSakai/PythonRobotics/workflows/Windows_CI/badge.svg)
[![Build status](https://ci.appveyor.com/api/projects/status/sb279kxuv1be391g?svg=true)](https://ci.appveyor.com/project/AtsushiSakai/pythonrobotics)

Python codes and [textbook](https://atsushisakai.github.io/PythonRobotics/index.html) for robotics algorithm.


# Table of Contents
   * [What is this?](#what-is-this)
   * [Requirements](#requirements)
   * [Documentation](#documentation)
   * [How to use](#how-to-use)
   * [Localization](#localization)
      * [Extended Kalman Filter localization](#extended-kalman-filter-localization)
      * [Particle filter localization](#particle-filter-localization)
      * [Histogram filter localization](#histogram-filter-localization)
   * [Mapping](#mapping)
      * [Gaussian grid map](#gaussian-grid-map)
      * [Ray casting grid map](#ray-casting-grid-map)
      * [Lidar to grid map](#lidar-to-grid-map)
      * [k-means object clustering](#k-means-object-clustering)
      * [Rectangle fitting](#rectangle-fitting)
   * [SLAM](#slam)
      * [Iterative Closest Point (ICP) Matching](#iterative-closest-point-icp-matching)
      * [FastSLAM 1.0](#fastslam-10)
   * [Path Planning](#path-planning)
      * [Dynamic Window Approach](#dynamic-window-approach)
      * [Grid based search](#grid-based-search)
         * [Dijkstra algorithm](#dijkstra-algorithm)
         * [A* algorithm](#a-algorithm)
         * [D* algorithm](#d-algorithm)
         * [D* Lite algorithm](#d-lite-algorithm)
         * [Potential Field algorithm](#potential-field-algorithm)
         * [Grid based coverage path planning](#grid-based-coverage-path-planning)
         * [Particle Swarm Optimization (PSO)](#particle-swarm-optimization-pso)  
      * [State Lattice Planning](#state-lattice-planning)
         * [Biased polar sampling](#biased-polar-sampling)
         * [Lane sampling](#lane-sampling)
      * [Probabilistic Road-Map (PRM) planning](#probabilistic-road-map-prm-planning)
      * [Rapidly-Exploring Random Trees (RRT)](#rapidly-exploring-random-trees-rrt)
         * [RRT*](#rrt)
         * [RRT* with reeds-shepp path](#rrt-with-reeds-shepp-path)
         * [LQR-RRT*](#lqr-rrt)
      * [Quintic polynomials planning](#quintic-polynomials-planning)
      * [Reeds Shepp planning](#reeds-shepp-planning)
      * [LQR based path planning](#lqr-based-path-planning)
      * [Optimal Trajectory in a Frenet Frame](#optimal-trajectory-in-a-frenet-frame)
   * [Path Tracking](#path-tracking)
      * [move to a pose control](#move-to-a-pose-control)
      * [Stanley control](#stanley-control)
      * [Rear wheel feedback control](#rear-wheel-feedback-control)
      * [Linear‚Äìquadratic regulator (LQR) speed and steering control](#linearquadratic-regulator-lqr-speed-and-steering-control)
      * [Model predictive speed and steering control](#model-predictive-speed-and-steering-control)
      * [Nonlinear Model predictive control with C-GMRES](#nonlinear-model-predictive-control-with-c-gmres)
   * [Arm Navigation](#arm-navigation)
      * [N joint arm to point control](#n-joint-arm-to-point-control)
      * [Arm navigation with obstacle avoidance](#arm-navigation-with-obstacle-avoidance)
   * [Aerial Navigation](#aerial-navigation)
      * [drone 3d trajectory following](#drone-3d-trajectory-following)
      * [rocket powered landing](#rocket-powered-landing)
   * [Bipedal](#bipedal)
      * [bipedal planner with inverted pendulum](#bipedal-planner-with-inverted-pendulum)
   * [License](#license)
   * [Use-case](#use-case)
   * [Contribution](#contribution)
   * [Citing](#citing)
   * [Support](#support)
   * [Sponsors](#sponsors)
      * [JetBrains](#JetBrains)
      * [1Password](#1password)
   * [Authors](#authors)

# What is PythonRobotics?

PythonRobotics is a Python code collection and a [textbook](https://atsushisakai.github.io/PythonRobotics/index.html) of robotics algorithms.

Features:

1. Easy to read for understanding each algorithm&#039;s basic idea.

2. Widely used and practical algorithms are selected.

3. Minimum dependency.

See this documentation 

- [Getting Started ‚Äî PythonRobotics documentation](https://atsushisakai.github.io/PythonRobotics/modules/0_getting_started/1_what_is_python_robotics.html)

or this Youtube video:

- [PythonRobotics project audio overview](https://www.youtube.com/watch?v=uMeRnNoJAfU)

or this paper for more details:

- [\[1808\.10703\] PythonRobotics: a Python code collection of robotics algorithms](https://arxiv.org/abs/1808.10703) ([BibTeX](https://github.com/AtsushiSakai/PythonRoboticsPaper/blob/master/python_robotics.bib))


# Requirements to run the code

For running each sample code:

- [Python 3.13.x](https://www.python.org/)
 
- [NumPy](https://numpy.org/)
 
- [SciPy](https://scipy.org/)
 
- [Matplotlib](https://matplotlib.org/)
 
- [cvxpy](https://www.cvxpy.org/) 

For development:
  
- [pytest](https://pytest.org/) (for unit tests)
  
- [pytest-xdist](https://pypi.org/project/pytest-xdist/) (for parallel unit tests)
  
- [mypy](https://mypy-lang.org/) (for type check)
  
- [sphinx](https://www.sphinx-doc.org/) (for document generation)
  
- [pycodestyle](https://pypi.org/project/pycodestyle/) (for code style check)

# Documentation (Textbook)

This README only shows some examples of this project. 

If you are interested in other examples or mathematical backgrounds of each algorithm, 

You can check the full documentation (textbook) online: [Welcome to PythonRobotics‚Äôs documentation\! ‚Äî PythonRobotics documentation](https://atsushisakai.github.io/PythonRobotics/index.html)

All animation gifs are stored here: [AtsushiSakai/PythonRoboticsGifs: Animation gifs of PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs)

# How to use

1. Clone this repo.

   ```terminal
   git clone https://github.com/AtsushiSakai/PythonRobotics.git
   ```


2. Install the required libraries.

- using conda :

  ```terminal
  conda env create -f requirements/environment.yml
  ```
 
- using pip :

  ```terminal
  pip install -r requirements/requirements.txt
  ```


3. Execute python script in each directory.

4. Add star to this repo if you like it :smiley:. 

# Localization

## Extended Kalman Filter localization

&lt;img src=&quot;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/extended_kalman_filter/animation.gif&quot; width=&quot;640&quot; alt=&quot;EKF pic&quot;&gt;

Reference

- [documentation](https://atsushisakai.github.io/PythonRobotics/modules/2_localization/extended_kalman_filter_localization_files/extended_kalman_filter_localization.html)

## Particle filter localization

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/particle_filter/animation.gif)

This is a sensor fusion localization with Particle Filter(PF).

The blue line is true trajectory, the black line is dead reckoning trajectory,

and the red line is an estimated trajectory with PF.

It is assumed that the robot can measure a distance from landmarks (RFID).

These measurements are used for PF localization.

Reference

- [PROBABILISTIC ROBOTICS](http://www.probabilistic-robotics.org/)


## Histogram filter localization

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/histogram_filter/animation.gif)

This is a 2D localization example with Histogram filter.

The red cross is true position, black points are RFID positions.

The blue grid shows a position probability of histogram filter.  

In this simulation, x,y are unknown, yaw is known.

The filter integrates speed input and range observations from RFID for localization.

Initial position is not needed.

Reference

- [PROBABILISTIC ROBOTICS](http://www.probabilistic-robotics.org/)

# Mapping

## Gaussian grid map

This is a 2D Gaussian grid mapping example.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/gaussian_grid_map/animation.gif)

## Ray casting grid map

This is a 2D ray casting grid mapping example.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/raycasting_grid_map/animation.gif)

## Lidar to grid map

This example shows how to convert a 2D range measurement to a grid map.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/lidar_to_grid_map/animation.gif)

## k-means object clustering

This is a 2D object clustering with k-means algorithm.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/kmeans_clustering/animation.gif)

## Rectangle fitting

This is a 2D rectangle fitting for vehicle detection.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/rectangle_fitting/animation.gif)


# SLAM

Simultaneous Localization and Mapping(SLAM) examples

## Iterative Closest Point (ICP) Matching

This is a 2D ICP matching example with singular value decomposition.

It can calculate a rotation matrix, and a translation vector between points and points.

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/iterative_closest_point/animation.gif)

Reference

- [Introduction to Mobile Robotics: Iterative Closest Point Algorithm](https://cs.gmu.edu/~kosecka/cs685/cs685-icp.pdf)


## FastSLAM 1.0

This is a feature based SLAM example using FastSLAM 1.0.

The blue line is ground truth, the black line is dead reckoning, the red line is the estimated trajectory with FastSLAM.

The red points are particles of FastSLAM.

Black points are landmarks, blue crosses are estimated landmark positions by FastSLAM.


![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/FastSLAM1/animation.gif)


Reference

- [PROBABILISTIC ROBOTICS](http://www.probabilistic-robotics.org/)

- [SLAM simulations by Tim Bailey](http://www-personal.acfr.usyd.edu.au/tbailey/software/slam_simulations.htm)


# Path Planning

## Dynamic Window Approach

This is a 2D navigation sample code with Dynamic Window Approach.

- [The Dynamic Window Approach to Collision Avoidance](https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf)

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DynamicWindowApproach/animation.gif)


## Grid based search

### Dijkstra algorithm

This is a 2D grid based the shortest path planning with Dijkstra&#039;s algorithm.

![PythonRobotics/figure_1.png at master ¬∑ AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/Dijkstra/animation.gif)

In the animation, cyan points are searched nodes.

### A\* algorithm

This is a 2D grid based the shortest path planning with A star algorithm.

![PythonRobotics/figure_1.png at master ¬∑ AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/AStar/animation.gif)

In the animation, cyan points are searched nodes.

Its heuristic is 2D Euclid distance.

### D\* algorithm

This is a 2D grid based the shortest path planning with D star algorithm.

![figure at master ¬∑ nirnayroy/intelligentrobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DStar/animation.gif)

The animation shows a robot finding its path avoiding an obstacle using the D* search algorithm.

Reference

- [D* Algorithm Wikipedia](https://en.wikipedia.org/wiki/D*)

### D\* Lite algorithm

This algorithm finds the shortest path between two points while rerouting when obstacles are discovered. It has been implemented here for a 2D grid.

![D* Lite](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DStarLite/animation.gif)

The animation shows a robot finding its path and rerouting to avoid obstacles as they are discovered using the D* Lite search algorithm.

Refs:

- [D* Lite](http://idm-lab.org/bib/abstracts/papers/aaai02b.pdf)
- [Improved Fast Replanning for Robot Navigation in Unknown Terrain](http://www.cs.cmu.edu/~maxim/files/dlite_icra02.pdf)

### Potential Field algorithm

This is a 2D grid based path planning with Potential Field algorithm.

![PotentialField](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/PotentialFieldPlanning/animation.gif)

In the animation, the blue heat map shows potential value on each grid.

Reference

- [Robotic Motion Planning:Potential Functions](https://www.cs.cmu.edu/~motionplanning/lecture/Chap4-Potential-Field_howie.pdf)

### Grid based coverage path planning

This is a 2D grid based coverage path planning simulation.

![PotentialField](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/GridBasedSweepCPP/animation.gif)

### Particle Swarm Optimization (PSO)

This is a 2D path planning simulation using the Particle Swarm Optimization algorithm.

![PSO](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ParticleSwarmOptimization/animation.gif)

PSO is a metaheuristic optimization algorithm inspired by bird flocking behavior. In path planning, particles explore the search space to find collision-free paths while avoiding obstacles.

The animation shows particles (blue dots) converging towards the optimal path (yellow line) from start (green area) to goal (red star).

References

- [Particle swarm optimization - Wikipedia](https://en.wikipedia.org/wiki/Particle_swarm_optimization)

- [Kennedy, J.; Eberhart, R. (1995). &quot;Particle Swarm Optimization&quot;](https://ieeexplore.ieee.org/document/488968)



## State Lattice Planning

This script is a path planning code with state lattice planning.

This code uses the model predictive trajectory generator to solve boundary problem.

Reference 

- [Optimal rough terrain trajectory generation for wheeled mobile robots](https://journals.sagepub.com/doi/pdf/10.1177/0278364906075328)

- [State Space Sampling of Feasible Motions for High-Performance Mobile Robot Navigation in Complex Environments](https://www.cs.cmu.edu/~alonzo/pubs/papers/JFR_08_SS_Sampling.pdf)


### Biased polar sampling

![PythonRobotics/figure_1.png at master ¬∑ AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/BiasedPolarSampling.gif)


### Lane sampling

![PythonRobotics/figure_1.png at master ¬∑ AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/LaneSampling.gif)

## Probabilistic Road-Map (PRM) planning 

![PRM](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ProbabilisticRoadMap/animation.gif)

This PRM planner uses Dijkstra method for graph search.

In the animation, blue points are sampled points,

Cyan crosses means searched points with Dijkstra method,

The red line is the final path of PRM.

Reference

- [Probabilistic roadmap \- Wikipedia](https://en.wikipedia.org/wiki/Probabilistic_roadmap)

„ÄÄ„ÄÄ

## Rapidly-Exploring Random Trees (RRT)

### RRT\*

![PythonRobotics/figure_1.png at master ¬∑ AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTstar/animation.gif)

This is a path planning code with RRT\*

Black circles are obstacles, green line is a searched tree, red crosses are start and goal positions.

Reference

- [Incremental Sampling-based Algorithms for Optimal Motion Planning](https://arxiv.org/abs/1005.0416)

- [Sampling-based Algorithms for Optimal Motion Planning](https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=bddbc99f97173430aa49a0ada53ab5bade5902fa)

### RRT\* with reeds-shepp path

![Robotics/animation.gif at master ¬∑ AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTStarReedsShepp/animation.gif)

Path planning for a car robot with RRT\* and reeds shepp path planner.

### LQR-RRT\*

This is a path planning simulation with LQR-RRT\*.

A double integrator motion model is used for LQR local planner.

![LQR_RRT](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRRRTStar/animation.gif)

Reference

- [LQR\-RRT\*: Optimal Sampling\-Based Motion Planning with Automatically Derived Extension Heuristics](https://lis.csail.mit.edu/pubs/perez-icra12.pdf)

- [MahanFathi/LQR\-RRTstar: LQR\-RRT\* method is used for random motion planning of a simple pendulum in its phase plot](https://github.com/MahanFathi/LQR-RRTstar)


## Quintic polynomials planning

Motion planning with quintic polynomials.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/QuinticPolynomialsPlanner/animation.gif)

It can calculate a 2D path, velocity, and acceleration profile based on quintic polynomials.

Reference

- [Local Path Planning And Motion Control For Agv In Positioning](https://ieeexplore.ieee.org/document/637936/)

## Reeds Shepp planning

A sample code with Reeds Shepp path planning.

![RSPlanning](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ReedsSheppPath/animation.gif?raw=true)

Reference

- [15.3.2 Reeds\-Shepp Curves](http://planning.cs.uiuc.edu/node822.html) 

- [optimal paths for a car that goes both forwards and backwards](https://pdfs.semanticscholar.org/932e/c495b1d0018fd59dee12a0bf74434fac7af4.pdf)

- [ghliu/pyReedsShepp: Implementation of Reeds Shepp curve\.](https://github.com/ghliu/pyReedsShepp)


## LQR based path planning

A sample code using LQR based path planning for double integrator model.

![RSPlanning](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRPlanner/animation.gif?raw=true)


## Optimal Trajectory in a Frenet Frame 

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/FrenetOptimalTrajectory/animation.gif)

This is optimal trajectory generation in a Frenet Frame.

The cyan line is the target course and black crosses are obstacles.

The red line is the predicted path.

Reference

- [Optimal Trajectory Generation for Dynamic Street Scenarios in a Frenet Frame](https://www.researchgate.net/profile/Moritz_Werling/publication/224156269_Optimal_Trajectory_Generation_for_Dynamic_Street_Scenarios_in_a_Frenet_Frame/links/54f749df0cf210398e9277af.pdf)

- [Optimal trajectory generation for dynamic street scenarios in a Frenet Frame](https://www.youtube.com/watch?v=Cj6tAQe7UCY)


# Path Tracking

## move to a pose control

This is a simulation of moving to a pose control

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Control/move_to_pose/animation.gif)

Reference

- [P. I. Corke, &quot;Robotics, Vision and Control&quot; \| SpringerLink p102](https://link.springer.com/book/10.1007/978-3-642-20144-8)


## Stanley control

Path tracking simulation with Stanley steering control and PID speed control.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/stanley_controller/animation.gif)

Reference

- [Stanley: The robot that won the DARPA grand challenge](http://robots.stanford.edu/papers/thrun.stanley05.pdf)

- [Automatic Steering Methods for Autonomous Automobile Path Tracking](https://www.ri.cmu.edu/pub_files/2009/2/Automatic_Steering_Methods_for_Autonomous_Automobile_Path_Tracking.pdf)



## Rear wheel feedback control

Path tracking simulation with rear wheel feedback steering control and PID speed control.

![PythonRobotics/figure_1.png at master ¬∑ AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/rear_wheel_feedback/animation.gif)

Reference

- [A Survey of Motion Planning and Control Techniques for Self-driving Urban Vehicles](https://arxiv.org/abs/1604.07446)


## Linear‚Äìquadratic regulator (LQR) speed and steering control

Path tracking simulation with LQR speed

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[zhayujie/chatgpt-on-wechat]]></title>
            <link>https://github.com/zhayujie/chatgpt-on-wechat</link>
            <guid>https://github.com/zhayujie/chatgpt-on-wechat</guid>
            <pubDate>Thu, 05 Feb 2026 00:07:23 GMT</pubDate>
            <description><![CDATA[CowAgentÊòØÂü∫‰∫éÂ§ßÊ®°ÂûãÁöÑË∂ÖÁ∫ßAIÂä©ÁêÜÔºåËÉΩ‰∏ªÂä®ÊÄùËÄÉÂíå‰ªªÂä°ËßÑÂàí„ÄÅËÆøÈóÆÊìç‰ΩúÁ≥ªÁªüÂíåÂ§ñÈÉ®ËµÑÊ∫ê„ÄÅÂàõÈÄ†ÂíåÊâßË°åSkills„ÄÅÊã•ÊúâÈïøÊúüËÆ∞ÂøÜÂπ∂‰∏çÊñ≠ÊàêÈïø„ÄÇÂêåÊó∂ÊîØÊåÅÈ£û‰π¶„ÄÅÈíâÈíâ„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®„ÄÅÂæÆ‰ø°ÂÖ¨‰ºóÂè∑„ÄÅÁΩëÈ°µÁ≠âÊé•ÂÖ•ÔºåÂèØÈÄâÊã©OpenAI/Claude/Gemini/DeepSeek/ Qwen/GLM/Kimi/LinkAIÔºåËÉΩÂ§ÑÁêÜÊñáÊú¨„ÄÅËØ≠Èü≥„ÄÅÂõæÁâáÂíåÊñá‰ª∂ÔºåÂèØÂø´ÈÄüÊê≠Âª∫‰∏™‰∫∫AIÂä©ÊâãÂíå‰ºÅ‰∏öÊï∞Â≠óÂëòÂ∑•„ÄÇ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/zhayujie/chatgpt-on-wechat">zhayujie/chatgpt-on-wechat</a></h1>
            <p>CowAgentÊòØÂü∫‰∫éÂ§ßÊ®°ÂûãÁöÑË∂ÖÁ∫ßAIÂä©ÁêÜÔºåËÉΩ‰∏ªÂä®ÊÄùËÄÉÂíå‰ªªÂä°ËßÑÂàí„ÄÅËÆøÈóÆÊìç‰ΩúÁ≥ªÁªüÂíåÂ§ñÈÉ®ËµÑÊ∫ê„ÄÅÂàõÈÄ†ÂíåÊâßË°åSkills„ÄÅÊã•ÊúâÈïøÊúüËÆ∞ÂøÜÂπ∂‰∏çÊñ≠ÊàêÈïø„ÄÇÂêåÊó∂ÊîØÊåÅÈ£û‰π¶„ÄÅÈíâÈíâ„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®„ÄÅÂæÆ‰ø°ÂÖ¨‰ºóÂè∑„ÄÅÁΩëÈ°µÁ≠âÊé•ÂÖ•ÔºåÂèØÈÄâÊã©OpenAI/Claude/Gemini/DeepSeek/ Qwen/GLM/Kimi/LinkAIÔºåËÉΩÂ§ÑÁêÜÊñáÊú¨„ÄÅËØ≠Èü≥„ÄÅÂõæÁâáÂíåÊñá‰ª∂ÔºåÂèØÂø´ÈÄüÊê≠Âª∫‰∏™‰∫∫AIÂä©ÊâãÂíå‰ºÅ‰∏öÊï∞Â≠óÂëòÂ∑•„ÄÇ</p>
            <p>Language: Python</p>
            <p>Stars: 41,013</p>
            <p>Forks: 9,706</p>
            <p>Stars today: 32 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;img src= &quot;https://github.com/user-attachments/assets/eca9a9ec-8534-4615-9e0f-96c5ac1d10a3&quot; alt=&quot;Chatgpt-on-Wechat&quot; width=&quot;550&quot; /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;a href=&quot;https://github.com/zhayujie/chatgpt-on-wechat/releases/latest&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/zhayujie/chatgpt-on-wechat&quot; alt=&quot;Latest release&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/zhayujie/chatgpt-on-wechat/blob/master/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/zhayujie/chatgpt-on-wechat&quot; alt=&quot;License: MIT&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/zhayujie/chatgpt-on-wechat&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/zhayujie/chatgpt-on-wechat?style=flat-square&quot; alt=&quot;Stars&quot;&gt;&lt;/a&gt; &lt;br/&gt;
&lt;/p&gt;

**CowAgent** ÊòØÂü∫‰∫éÂ§ßÊ®°ÂûãÁöÑË∂ÖÁ∫ßAIÂä©ÁêÜÔºåËÉΩÂ§ü‰∏ªÂä®ÊÄùËÄÉÂíå‰ªªÂä°ËßÑÂàí„ÄÅÊìç‰ΩúËÆ°ÁÆóÊú∫ÂíåÂ§ñÈÉ®ËµÑÊ∫ê„ÄÅÂàõÈÄ†ÂíåÊâßË°åSkills„ÄÅÊã•ÊúâÈïøÊúüËÆ∞ÂøÜÂπ∂‰∏çÊñ≠ÊàêÈïø„ÄÇCowAgent ÊîØÊåÅÁÅµÊ¥ªÂàáÊç¢Â§öÁßçÊ®°ÂûãÔºåËÉΩÂ§ÑÁêÜÊñáÊú¨„ÄÅËØ≠Èü≥„ÄÅÂõæÁâá„ÄÅÊñá‰ª∂Á≠âÂ§öÊ®°ÊÄÅÊ∂àÊÅØÔºåÂèØÊé•ÂÖ•ÁΩëÈ°µ„ÄÅÈ£û‰π¶„ÄÅÈíâÈíâ„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®„ÄÅÂæÆ‰ø°ÂÖ¨‰ºóÂè∑‰∏≠‰ΩøÁî®Ôºå7*24Â∞èÊó∂ËøêË°å‰∫é‰Ω†ÁöÑ‰∏™‰∫∫ÁîµËÑëÊàñÊúçÂä°Âô®‰∏≠„ÄÇ

üìñËÉΩÂäõ‰ªãÁªçÔºö[CowAgent 2.0](/docs/agent.md)

# ÁÆÄ‰ªã

&gt; ËØ•È°πÁõÆÊó¢ÊòØ‰∏Ä‰∏™ÂèØ‰ª•ÂºÄÁÆ±Âç≥Áî®ÁöÑË∂ÖÁ∫ßAIÂä©ÁêÜÔºå‰πüÊòØ‰∏Ä‰∏™ÊîØÊåÅÈ´òÊâ©Â±ïÁöÑAgentÊ°ÜÊû∂ÔºåÂèØ‰ª•ÈÄöËøá‰∏∫È°πÁõÆÊâ©Â±ïÂ§ßÊ®°ÂûãÊé•Âè£„ÄÅÊé•ÂÖ•Ê∏†ÈÅì„ÄÅÂÜÖÁΩÆÂ∑•ÂÖ∑„ÄÅSkillsÁ≥ªÁªüÊù•ÁÅµÊ¥ªÂÆûÁé∞ÂêÑÁßçÂÆöÂà∂ÈúÄÊ±Ç„ÄÇÊ†∏ÂøÉËÉΩÂäõÂ¶Ç‰∏ãÔºö

-  ‚úÖ  **Â§çÊùÇ‰ªªÂä°ËßÑÂàí**ÔºöËÉΩÂ§üÁêÜËß£Â§çÊùÇ‰ªªÂä°Âπ∂Ëá™‰∏ªËßÑÂàíÊâßË°åÔºåÊåÅÁª≠ÊÄùËÄÉÂíåË∞ÉÁî®Â∑•ÂÖ∑Áõ¥Âà∞ÂÆåÊàêÁõÆÊ†áÔºåÊîØÊåÅÈÄöËøáÂ∑•ÂÖ∑Êìç‰ΩúËÆøÈóÆÊñá‰ª∂„ÄÅÁªàÁ´Ø„ÄÅÊµèËßàÂô®„ÄÅÂÆöÊó∂‰ªªÂä°Á≠âÁ≥ªÁªüËµÑÊ∫ê
-  ‚úÖ  **ÈïøÊúüËÆ∞ÂøÜÔºö** Ëá™Âä®Â∞ÜÂØπËØùËÆ∞ÂøÜÊåÅ‰πÖÂåñËá≥Êú¨Âú∞Êñá‰ª∂ÂíåÊï∞ÊçÆÂ∫ì‰∏≠ÔºåÂåÖÊã¨ÂÖ®Â±ÄËÆ∞ÂøÜÂíåÂ§©Á∫ßËÆ∞ÂøÜÔºåÊîØÊåÅÂÖ≥ÈîÆËØçÂèäÂêëÈáèÊ£ÄÁ¥¢
-  ‚úÖ  **ÊäÄËÉΩÁ≥ªÁªüÔºö** ÂÆûÁé∞‰∫ÜSkillsÂàõÂª∫ÂíåËøêË°åÁöÑÂºïÊìéÔºåÂÜÖÁΩÆÂ§öÁßçÊäÄËÉΩÔºåÂπ∂ÊîØÊåÅÈÄöËøáËá™ÁÑ∂ËØ≠Ë®ÄÂØπËØùÂÆåÊàêËá™ÂÆö‰πâSkillsÂºÄÂèë
-  ‚úÖ  **Â§öÊ®°ÊÄÅÊ∂àÊÅØÔºö** ÊîØÊåÅÂØπÊñáÊú¨„ÄÅÂõæÁâá„ÄÅËØ≠Èü≥„ÄÅÊñá‰ª∂Á≠âÂ§öÁ±ªÂûãÊ∂àÊÅØËøõË°åËß£Êûê„ÄÅÂ§ÑÁêÜ„ÄÅÁîüÊàê„ÄÅÂèëÈÄÅÁ≠âÊìç‰Ωú
-  ‚úÖ  **Â§öÊ®°ÂûãÊé•ÂÖ•Ôºö** ÊîØÊåÅOpenAI, Claude, Gemini, DeepSeek, MiniMax„ÄÅGLM„ÄÅQwen„ÄÅKimiÁ≠âÂõΩÂÜÖÂ§ñ‰∏ªÊµÅÊ®°ÂûãÂéÇÂïÜ
-  ‚úÖ  **Â§öÁ´ØÈÉ®ÁΩ≤Ôºö** ÊîØÊåÅËøêË°åÂú®Êú¨Âú∞ËÆ°ÁÆóÊú∫ÊàñÊúçÂä°Âô®ÔºåÂèØÈõÜÊàêÂà∞ÁΩëÈ°µ„ÄÅÈ£û‰π¶„ÄÅÈíâÈíâ„ÄÅÂæÆ‰ø°ÂÖ¨‰ºóÂè∑„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®‰∏≠‰ΩøÁî®
-  ‚úÖ  **Áü•ËØÜÂ∫ìÔºö** ÈõÜÊàê‰ºÅ‰∏öÁü•ËØÜÂ∫ìËÉΩÂäõÔºåËÆ©AgentÊàê‰∏∫‰∏ìÂ±ûÊï∞Â≠óÂëòÂ∑•ÔºåÂü∫‰∫é[LinkAI](https://link-ai.tech)Âπ≥Âè∞ÂÆûÁé∞

## Â£∞Êòé

1. Êú¨È°πÁõÆÈÅµÂæ™ [MITÂºÄÊ∫êÂçèËÆÆ](/LICENSE)Ôºå‰∏ªË¶ÅÁî®‰∫éÊäÄÊúØÁ†îÁ©∂ÂíåÂ≠¶‰π†Ôºå‰ΩøÁî®Êú¨È°πÁõÆÊó∂ÈúÄÈÅµÂÆàÊâÄÂú®Âú∞Ê≥ïÂæãÊ≥ïËßÑ„ÄÅÁõ∏ÂÖ≥ÊîøÁ≠ñ‰ª•Âèä‰ºÅ‰∏öÁ´†Á®ãÔºåÁ¶ÅÊ≠¢Áî®‰∫é‰ªª‰ΩïËøùÊ≥ïÊàñ‰æµÁäØ‰ªñ‰∫∫ÊùÉÁõäÁöÑË°å‰∏∫„ÄÇ‰ªª‰Ωï‰∏™‰∫∫„ÄÅÂõ¢ÈòüÂíå‰ºÅ‰∏öÔºåÊó†ËÆ∫‰ª•‰ΩïÁßçÊñπÂºè‰ΩøÁî®ËØ•È°πÁõÆ„ÄÅÂØπ‰ΩïÂØπË±°Êèê‰æõÊúçÂä°ÔºåÊâÄ‰∫ßÁîüÁöÑ‰∏ÄÂàáÂêéÊûúÔºåÊú¨È°πÁõÆÂùá‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªª
2. ÊàêÊú¨‰∏éÂÆâÂÖ®ÔºöAgentÊ®°Âºè‰∏ãToken‰ΩøÁî®ÈáèÈ´ò‰∫éÊôÆÈÄöÂØπËØùÊ®°ÂºèÔºåËØ∑Ê†πÊçÆÊïàÊûúÂèäÊàêÊú¨ÁªºÂêàÈÄâÊã©Ê®°Âûã„ÄÇAgentÂÖ∑ÊúâËÆøÈóÆÊâÄÂú®Êìç‰ΩúÁ≥ªÁªüÁöÑËÉΩÂäõÔºåËØ∑Ë∞®ÊÖéÈÄâÊã©È°πÁõÆÈÉ®ÁΩ≤ÁéØÂ¢É„ÄÇÂêåÊó∂È°πÁõÆ‰πü‰ºöÊåÅÁª≠ÂçáÁ∫ßÂÆâÂÖ®Êú∫Âà∂„ÄÅÂπ∂Èôç‰ΩéÊ®°ÂûãÊ∂àËÄóÊàêÊú¨

## ÊºîÁ§∫

‰ΩøÁî®ËØ¥Êòé(AgentÊ®°Âºè)Ôºö[CowAgent‰ªãÁªç](/docs/agent.md)

DEMOËßÜÈ¢ë(ÂØπËØùÊ®°Âºè)Ôºöhttps://cdn.link-ai.tech/doc/cow_demo.mp4

## Á§æÂå∫

Ê∑ªÂä†Â∞èÂä©ÊâãÂæÆ‰ø°Âä†ÂÖ•ÂºÄÊ∫êÈ°πÁõÆ‰∫§ÊµÅÁæ§Ôºö

&lt;img width=&quot;140&quot; src=&quot;https://img-1317903499.cos.ap-guangzhou.myqcloud.com/docs/open-community.png&quot;&gt;

&lt;br/&gt;

# ‰ºÅ‰∏öÊúçÂä°

&lt;a href=&quot;https://link-ai.tech&quot; target=&quot;_blank&quot;&gt;&lt;img width=&quot;720&quot; src=&quot;https://cdn.link-ai.tech/image/link-ai-intro.jpg&quot;&gt;&lt;/a&gt;

&gt; [LinkAI](https://link-ai.tech/) ÊòØÈù¢Âêë‰ºÅ‰∏öÂíåÂºÄÂèëËÄÖÁöÑ‰∏ÄÁ´ôÂºèAIÊô∫ËÉΩ‰ΩìÂπ≥Âè∞ÔºåËÅöÂêàÂ§öÊ®°ÊÄÅÂ§ßÊ®°Âûã„ÄÅÁü•ËØÜÂ∫ì„ÄÅAgent Êèí‰ª∂„ÄÅÂ∑•‰ΩúÊµÅÁ≠âËÉΩÂäõÔºåÊîØÊåÅ‰∏ÄÈîÆÊé•ÂÖ•‰∏ªÊµÅÂπ≥Âè∞Âπ∂ËøõË°åÁÆ°ÁêÜÔºåÊîØÊåÅSaaS„ÄÅÁßÅÊúâÂåñÈÉ®ÁΩ≤Á≠âÂ§öÁßçÊ®°Âºè„ÄÇ
&gt;
&gt; LinkAI ÁõÆÂâçÂ∑≤Âú®Êô∫ËÉΩÂÆ¢Êúç„ÄÅÁßÅÂüüËøêËê•„ÄÅ‰ºÅ‰∏öÊïàÁéáÂä©ÊâãÁ≠âÂú∫ÊôØÁßØÁ¥Ø‰∫Ü‰∏∞ÂØåÁöÑAIËß£ÂÜ≥ÊñπÊ°àÔºåÂú®Ê∂àË¥π„ÄÅÂÅ•Â∫∑„ÄÅÊñáÊïô„ÄÅÁßëÊäÄÂà∂ÈÄ†Á≠âÂêÑË°å‰∏öÊ≤âÊ∑Ä‰∫ÜÂ§ßÊ®°ÂûãËêΩÂú∞Â∫îÁî®ÁöÑÊúÄ‰Ω≥ÂÆûË∑µÔºåËá¥Âäõ‰∫éÂ∏ÆÂä©Êõ¥Â§ö‰ºÅ‰∏öÂíåÂºÄÂèëËÄÖÊã•Êä± AI Áîü‰∫ßÂäõ„ÄÇ

**‰∫ßÂìÅÂí®ËØ¢Âíå‰ºÅ‰∏öÊúçÂä°** ÂèØËÅîÁ≥ª‰∫ßÂìÅÂÆ¢ÊúçÔºö

&lt;img width=&quot;150&quot; src=&quot;https://cdn.link-ai.tech/portal/linkai-customer-service.png&quot;&gt;

&lt;br/&gt;

# üè∑ Êõ¥Êñ∞Êó•Âøó

&gt;**2026.02.03Ôºö** [2.0.0ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/2.0.0)ÔºåÊ≠£ÂºèÂçáÁ∫ß‰∏∫Ë∂ÖÁ∫ßAgentÂä©ÁêÜÔºåÊîØÊåÅÂ§öËΩÆ‰ªªÂä°ÂÜ≥Á≠ñ„ÄÅÂÖ∑Â§áÈïøÊúüËÆ∞ÂøÜ„ÄÅÂÆûÁé∞Â§öÁßçÁ≥ªÁªüÂ∑•ÂÖ∑„ÄÅÊîØÊåÅSkillsÊ°ÜÊû∂ÔºåÊñ∞Â¢ûÂ§öÁßçÊ®°ÂûãÂπ∂‰ºòÂåñ‰∫ÜÊé•ÂÖ•Ê∏†ÈÅì„ÄÇ

&gt;**2025.05.23Ôºö** [1.7.6ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.6) ‰ºòÂåñwebÁΩëÈ°µchannel„ÄÅÊñ∞Â¢û [AgentMesh](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/plugins/agent/README.md)Â§öÊô∫ËÉΩ‰ΩìÊèí‰ª∂„ÄÅÁôæÂ∫¶ËØ≠Èü≥ÂêàÊàê‰ºòÂåñ„ÄÅ‰ºÅÂæÆÂ∫îÁî®`access_token`Ëé∑Âèñ‰ºòÂåñ„ÄÅÊîØÊåÅ`claude-4-sonnet`Âíå`claude-4-opus`Ê®°Âûã

&gt;**2025.04.11Ôºö** [1.7.5ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.5) Êñ∞Â¢ûÊîØÊåÅ [wechatferry](https://github.com/zhayujie/chatgpt-on-wechat/pull/2562) ÂçèËÆÆ„ÄÅÊñ∞Â¢û deepseek Ê®°Âûã„ÄÅÊñ∞Â¢ûÊîØÊåÅËÖæËÆØ‰∫ëËØ≠Èü≥ËÉΩÂäõ„ÄÅÊñ∞Â¢ûÊîØÊåÅ ModelScope Âíå Gitee-AI APIÊé•Âè£

&gt;**2024.12.13Ôºö** [1.7.4ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.4) Êñ∞Â¢û Gemini 2.0 Ê®°Âûã„ÄÅÊñ∞Â¢ûweb channel„ÄÅËß£ÂÜ≥ÂÜÖÂ≠òÊ≥ÑÊºèÈóÆÈ¢ò„ÄÅËß£ÂÜ≥ `#reloadp` ÂëΩ‰ª§ÈáçËΩΩ‰∏çÁîüÊïàÈóÆÈ¢ò

&gt;**2024.10.31Ôºö** [1.7.3ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.3) Á®ãÂ∫èÁ®≥ÂÆöÊÄßÊèêÂçá„ÄÅÊï∞ÊçÆÂ∫ìÂäüËÉΩ„ÄÅClaudeÊ®°Âûã‰ºòÂåñ„ÄÅlinkaiÊèí‰ª∂‰ºòÂåñ„ÄÅÁ¶ªÁ∫øÈÄöÁü•

Êõ¥Â§öÊõ¥Êñ∞ÂéÜÂè≤ËØ∑Êü•Áúã: [Êõ¥Êñ∞Êó•Âøó](/docs/release/history.md)

&lt;br/&gt;

# üöÄ Âø´ÈÄüÂºÄÂßã

È°πÁõÆÊèê‰æõ‰∫Ü‰∏ÄÈîÆÂÆâË£Ö„ÄÅÈÖçÁΩÆ„ÄÅÂêØÂä®„ÄÅÁÆ°ÁêÜÁ®ãÂ∫èÁöÑËÑöÊú¨ÔºåÊé®Ëçê‰ΩøÁî®ËÑöÊú¨Âø´ÈÄüËøêË°åÔºå‰πüÂèØ‰ª•Ê†πÊçÆ‰∏ãÊñá‰∏≠ÁöÑËØ¶ÁªÜÊåáÂºï‰∏ÄÊ≠•Ê≠•ÂÆâË£ÖËøêË°å„ÄÇ

Âú®ÁªàÁ´ØÊâßË°å‰ª•‰∏ãÂëΩ‰ª§Ôºö

```bash
bash &lt;(curl -sS https://cdn.link-ai.tech/code/cow/run.sh)
```

ËÑöÊú¨‰ΩøÁî®ËØ¥ÊòéÔºö[‰∏ÄÈîÆËøêË°åËÑöÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/wiki/CowAgentQuickStart)


## ‰∏Ä„ÄÅÂáÜÂ§á

### 1. Ê®°ÂûãAPI

È°πÁõÆÊîØÊåÅÂõΩÂÜÖÂ§ñ‰∏ªÊµÅÂéÇÂïÜÁöÑÊ®°ÂûãÊé•Âè£ÔºåÂèØÈÄâÊ®°ÂûãÂèäÈÖçÁΩÆËØ¥ÊòéÂèÇËÄÉÔºö[Ê®°ÂûãËØ¥Êòé](#Ê®°ÂûãËØ¥Êòé)„ÄÇ

&gt; Ê≥®ÔºöAgentÊ®°Âºè‰∏ãÊé®Ëçê‰ΩøÁî®‰ª•‰∏ãÊ®°ÂûãÔºåÂèØÊ†πÊçÆÊïàÊûúÂèäÊàêÊú¨ÁªºÂêàÈÄâÊã©ÔºöMiniMAx(MiniMax-M2.1)„ÄÅGLM(glm-4.7)„ÄÅQwen(qwen3-max)„ÄÅClaude(claude-sonnet-4-5„ÄÅclaude-sonnet-4-0)„ÄÅGemini(gemini-3-flash-preview„ÄÅgemini-3-pro-preview)

ÂêåÊó∂ÊîØÊåÅ‰ΩøÁî® **LinkAIÂπ≥Âè∞** Êé•Âè£ÔºåÂèØÁÅµÊ¥ªÂàáÊç¢ OpenAI„ÄÅClaude„ÄÅGemini„ÄÅDeepSeek„ÄÅQwen„ÄÅKimi Á≠âÂ§öÁßçÂ∏∏Áî®Ê®°ÂûãÔºåÂπ∂ÊîØÊåÅÁü•ËØÜÂ∫ì„ÄÅÂ∑•‰ΩúÊµÅ„ÄÅÊèí‰ª∂Á≠âAgentËÉΩÂäõÔºåÂèÇËÄÉ [Êé•Âè£ÊñáÊ°£](https://docs.link-ai.tech/platform/api)„ÄÇ

### 2.ÁéØÂ¢ÉÂÆâË£Ö

ÊîØÊåÅ Linux„ÄÅMacOS„ÄÅWindows Êìç‰ΩúÁ≥ªÁªüÔºåÂèØÂú®‰∏™‰∫∫ËÆ°ÁÆóÊú∫ÂèäÊúçÂä°Âô®‰∏äËøêË°åÔºåÈúÄÂÆâË£Ö `Python`ÔºåPythonÁâàÊú¨ÈúÄÂú®3.7 ~ 3.12 ‰πãÈó¥ÔºåÊé®Ëçê‰ΩøÁî®3.9ÁâàÊú¨„ÄÇ

&gt; Ê≥®ÊÑèÔºöAgentÊ®°ÂºèÊé®Ëçê‰ΩøÁî®Ê∫êÁ†ÅËøêË°åÔºåËã•ÈÄâÊã©DockerÈÉ®ÁΩ≤ÂàôÊó†ÈúÄÂÆâË£ÖpythonÁéØÂ¢ÉÂíå‰∏ãËΩΩÊ∫êÁ†ÅÔºåÂèØÁõ¥Êé•Âø´ËøõÂà∞‰∏ã‰∏ÄËäÇ„ÄÇ

**(1) ÂÖãÈöÜÈ°πÁõÆ‰ª£Á†ÅÔºö**

```bash
git clone https://github.com/zhayujie/chatgpt-on-wechat
cd chatgpt-on-wechat/
```

Ëã•ÈÅáÂà∞ÁΩëÁªúÈóÆÈ¢òÂèØ‰ΩøÁî®ÂõΩÂÜÖ‰ªìÂ∫ìÂú∞ÂùÄÔºöhttps://gitee.com/zhayujie/chatgpt-on-wechat

**(2) ÂÆâË£ÖÊ†∏ÂøÉ‰æùËµñ (ÂøÖÈÄâ)Ôºö**

```bash
pip3 install -r requirements.txt
```

**(3) ÊãìÂ±ï‰æùËµñ (ÂèØÈÄâÔºåÂª∫ËÆÆÂÆâË£Ö)Ôºö**

```bash
pip3 install -r requirements-optional.txt
```
Â¶ÇÊûúÊüêÈ°π‰æùËµñÂÆâË£ÖÂ§±Ë¥•ÂèØÊ≥®ÈáäÊéâÂØπÂ∫îÁöÑË°åÂêéÈáçËØï„ÄÇ

## ‰∫å„ÄÅÈÖçÁΩÆ

ÈÖçÁΩÆÊñá‰ª∂ÁöÑÊ®°ÊùøÂú®Ê†πÁõÆÂΩïÁöÑ`config-template.json`‰∏≠ÔºåÈúÄÂ§çÂà∂ËØ•Ê®°ÊùøÂàõÂª∫ÊúÄÁªàÁîüÊïàÁöÑ `config.json` Êñá‰ª∂Ôºö

```bash
  cp config-template.json config.json
```

ÁÑ∂ÂêéÂú®`config.json`‰∏≠Â°´ÂÖ•ÈÖçÁΩÆÔºå‰ª•‰∏ãÊòØÂØπÈªòËÆ§ÈÖçÁΩÆÁöÑËØ¥ÊòéÔºåÂèØÊ†πÊçÆÈúÄË¶ÅËøõË°åËá™ÂÆö‰πâ‰øÆÊîπÔºàÊ≥®ÊÑèÂÆûÈôÖ‰ΩøÁî®Êó∂ËØ∑ÂéªÊéâÊ≥®ÈáäÔºå‰øùËØÅJSONÊ†ºÂºèÁöÑËßÑËåÉÔºâÔºö

```bash
# config.json Êñá‰ª∂ÂÜÖÂÆπÁ§∫‰æã
{
  &quot;channel_type&quot;: &quot;web&quot;,                                      # Êé•ÂÖ•Ê∏†ÈÅìÁ±ªÂûãÔºåÈªòËÆ§‰∏∫webÔºåÊîØÊåÅ‰øÆÊîπ‰∏∫:feishu,dingtalk,wechatcom_app,terminal,wechatmp,wechatmp_service
  &quot;model&quot;: &quot;MiniMax-M2.1&quot;,                                    # Ê®°ÂûãÂêçÁß∞
  &quot;minimax_api_key&quot;: &quot;&quot;,                                      # MiniMax API Key
  &quot;zhipu_ai_api_key&quot;: &quot;&quot;,                                     # Êô∫Ë∞±GLM API Key
  &quot;dashscope_api_key&quot;: &quot;&quot;,                                    # ÁôæÁÇº(ÈÄö‰πâÂçÉÈóÆ)API Key
  &quot;claude_api_key&quot;: &quot;&quot;,                                       # Claude API Key
  &quot;claude_api_base&quot;: &quot;https://api.anthropic.com/v1&quot;,          # Claude API Âú∞ÂùÄÔºå‰øÆÊîπÂèØÊé•ÂÖ•‰∏âÊñπ‰ª£ÁêÜÂπ≥Âè∞
  &quot;gemini_api_key&quot;: &quot;&quot;,                                       # Gemini API Key
  &quot;gemini_api_base&quot;: &quot;https://generativelanguage.googleapis.com&quot;, # Gemini APIÂú∞ÂùÄ
  &quot;open_ai_api_key&quot;: &quot;&quot;,                                      # OpenAI API Key
  &quot;open_ai_api_base&quot;: &quot;https://api.openai.com/v1&quot;,            # OpenAI API Âú∞ÂùÄ
  &quot;linkai_api_key&quot;: &quot;&quot;,                                       # LinkAI API Key
  &quot;proxy&quot;: &quot;&quot;,                                                # ‰ª£ÁêÜÂÆ¢Êà∑Á´ØÁöÑipÂíåÁ´ØÂè£ÔºåÂõΩÂÜÖÁéØÂ¢ÉÈúÄË¶ÅÂºÄÂêØ‰ª£ÁêÜÁöÑÂèØÂ°´ÂÜôËØ•È°πÔºåÂ¶Ç &quot;127.0.0.1:7890&quot;
  &quot;speech_recognition&quot;: false,                                # ÊòØÂê¶ÂºÄÂêØËØ≠Èü≥ËØÜÂà´
  &quot;group_speech_recognition&quot;: false,                          # ÊòØÂê¶ÂºÄÂêØÁæ§ÁªÑËØ≠Èü≥ËØÜÂà´
  &quot;voice_reply_voice&quot;: false,                                 # ÊòØÂê¶‰ΩøÁî®ËØ≠Èü≥ÂõûÂ§çËØ≠Èü≥
  &quot;use_linkai&quot;: false,                                        # ÊòØÂê¶‰ΩøÁî®LinkAIÊé•Âè£ÔºåÈªòËÆ§ÂÖ≥Èó≠ÔºåËÆæÁΩÆ‰∏∫trueÂêéÂèØÂØπÊé•LinkAIÂπ≥Âè∞Êé•Âè£
  &quot;agent&quot;: true,                                              # ÊòØÂê¶ÂêØÁî®AgentÊ®°ÂºèÔºåÂêØÁî®ÂêéÊã•ÊúâÂ§öËΩÆÂ∑•ÂÖ∑ÂÜ≥Á≠ñ„ÄÅÈïøÊúüËÆ∞ÂøÜ„ÄÅSkillsËÉΩÂäõÁ≠â
  &quot;agent_workspace&quot;: &quot;~/cow&quot;,                                 # AgentÁöÑÂ∑•‰ΩúÁ©∫Èó¥Ë∑ØÂæÑÔºåÁî®‰∫éÂ≠òÂÇ®memory„ÄÅskills„ÄÅÁ≥ªÁªüËÆæÂÆöÁ≠â
  &quot;agent_max_context_tokens&quot;: 40000,                          # AgentÊ®°Âºè‰∏ãÊúÄÂ§ß‰∏ä‰∏ãÊñátokensÔºåË∂ÖÂá∫Â∞ÜËá™Âä®‰∏¢ÂºÉÊúÄÊó©ÁöÑ‰∏ä‰∏ãÊñá
  &quot;agent_max_context_turns&quot;: 30,                              # AgentÊ®°Âºè‰∏ãÊúÄÂ§ß‰∏ä‰∏ãÊñáËÆ∞ÂøÜËΩÆÊ¨°ÔºåÊØèËΩÆÂåÖÊã¨‰∏ÄÊ¨°Áî®Êà∑ÊèêÈóÆÂíåAIÂõûÂ§ç
  &quot;agent_max_steps&quot;: 15                                       # AgentÊ®°Âºè‰∏ãÂçïÊ¨°‰ªªÂä°ÁöÑÊúÄÂ§ßÂÜ≥Á≠ñÊ≠•Êï∞ÔºåË∂ÖÂá∫ÂêéÂ∞ÜÂÅúÊ≠¢ÁªßÁª≠Ë∞ÉÁî®Â∑•ÂÖ∑
}
```

**ÈÖçÁΩÆË°•ÂÖÖËØ¥Êòé:** 

&lt;details&gt;
&lt;summary&gt;1. ËØ≠Èü≥ÈÖçÁΩÆ&lt;/summary&gt;

+ Ê∑ªÂä† `&quot;speech_recognition&quot;: true` Â∞ÜÂºÄÂêØËØ≠Èü≥ËØÜÂà´ÔºåÈªòËÆ§‰ΩøÁî®openaiÁöÑwhisperÊ®°ÂûãËØÜÂà´‰∏∫ÊñáÂ≠óÔºåÂêåÊó∂‰ª•ÊñáÂ≠óÂõûÂ§çÔºåËØ•ÂèÇÊï∞‰ªÖÊîØÊåÅÁßÅËÅä (Ê≥®ÊÑèÁî±‰∫éËØ≠Èü≥Ê∂àÊÅØÊó†Ê≥ïÂåπÈÖçÂâçÁºÄÔºå‰∏ÄÊó¶ÂºÄÂêØÂ∞ÜÂØπÊâÄÊúâËØ≠Èü≥Ëá™Âä®ÂõûÂ§çÔºåÊîØÊåÅËØ≠Èü≥Ëß¶ÂèëÁîªÂõæ)Ôºõ
+ Ê∑ªÂä† `&quot;group_speech_recognition&quot;: true` Â∞ÜÂºÄÂêØÁæ§ÁªÑËØ≠Èü≥ËØÜÂà´ÔºåÈªòËÆ§‰ΩøÁî®openaiÁöÑwhisperÊ®°ÂûãËØÜÂà´‰∏∫ÊñáÂ≠óÔºåÂêåÊó∂‰ª•ÊñáÂ≠óÂõûÂ§çÔºåÂèÇÊï∞‰ªÖÊîØÊåÅÁæ§ËÅä (‰ºöÂåπÈÖçgroup_chat_prefixÂíågroup_chat_keyword, ÊîØÊåÅËØ≠Èü≥Ëß¶ÂèëÁîªÂõæ)Ôºõ
+ Ê∑ªÂä† `&quot;voice_reply_voice&quot;: true` Â∞ÜÂºÄÂêØËØ≠Èü≥ÂõûÂ§çËØ≠Èü≥ÔºàÂêåÊó∂‰ΩúÁî®‰∫éÁßÅËÅäÂíåÁæ§ËÅäÔºâ
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;2. ÂÖ∂‰ªñÈÖçÁΩÆ&lt;/summary&gt;

+ `model`: Ê®°ÂûãÂêçÁß∞ÔºåAgentÊ®°Âºè‰∏ãÊé®Ëçê‰ΩøÁî® `MiniMax-M2.1`„ÄÅ`glm-4.7`„ÄÅ`qwen3-max`„ÄÅ`claude-sonnet-4-5`„ÄÅ`claude-sonnet-4-0`„ÄÅ`gemini-3-flash-preview`„ÄÅ`gemini-3-pro-preview`ÔºåÂÖ®ÈÉ®Ê®°ÂûãÂêçÁß∞ÂèÇËÄÉ[common/const.py](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/common/const.py)Êñá‰ª∂
+ `character_desc`ÔºöÊôÆÈÄöÂØπËØùÊ®°Âºè‰∏ãÁöÑÊú∫Âô®‰∫∫Á≥ªÁªüÊèêÁ§∫ËØç„ÄÇÂú®AgentÊ®°Âºè‰∏ãËØ•ÈÖçÁΩÆ‰∏çÁîüÊïàÔºåÁî±Â∑•‰ΩúÁ©∫Èó¥‰∏≠ÁöÑÊñá‰ª∂ÂÜÖÂÆπÊûÑÊàê„ÄÇ
+ `subscribe_msg`ÔºöËÆ¢ÈòÖÊ∂àÊÅØÔºåÂÖ¨‰ºóÂè∑Âíå‰ºÅ‰∏öÂæÆ‰ø°channel‰∏≠ËØ∑Â°´ÂÜôÔºåÂΩìË¢´ËÆ¢ÈòÖÊó∂‰ºöËá™Âä®ÂõûÂ§çÔºå ÂèØ‰ΩøÁî®ÁâπÊÆäÂç†‰ΩçÁ¨¶„ÄÇÁõÆÂâçÊîØÊåÅÁöÑÂç†‰ΩçÁ¨¶Êúâ{trigger_prefix}ÔºåÂú®Á®ãÂ∫è‰∏≠ÂÆÉ‰ºöËá™Âä®ÊõøÊç¢ÊàêbotÁöÑËß¶ÂèëËØç„ÄÇ
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;5. LinkAIÈÖçÁΩÆ&lt;/summary&gt;

+ `use_linkai`: ÊòØÂê¶‰ΩøÁî®LinkAIÊé•Âè£ÔºåÈªòËÆ§ÂÖ≥Èó≠ÔºåËÆæÁΩÆ‰∏∫trueÂêéÂèØÂØπÊé•LinkAIÂπ≥Âè∞Ôºå‰ΩøÁî®Áü•ËØÜÂ∫ì„ÄÅÂ∑•‰ΩúÊµÅ„ÄÅÊèí‰ª∂Á≠âËÉΩÂäõ, ÂèÇËÄÉ[Êé•Âè£ÊñáÊ°£](https://docs.link-ai.tech/platform/api/chat)
+ `linkai_api_key`: LinkAI Api KeyÔºåÂèØÂú® [ÊéßÂà∂Âè∞](https://link-ai.tech/console/interface) ÂàõÂª∫
+ `linkai_app_code`: LinkAI Â∫îÁî®ÊàñÂ∑•‰ΩúÊµÅÁöÑcodeÔºåÈÄâÂ°´ÔºåÊôÆÈÄöÂØπËØùÊ®°Âºè‰∏≠‰ΩøÁî®„ÄÇ
&lt;/details&gt;

Ê≥®ÔºöÂÖ®ÈÉ®ÈÖçÁΩÆÈ°πËØ¥ÊòéÂèØÂú® [`config.py`](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/config.py) Êñá‰ª∂‰∏≠Êü•Áúã„ÄÇ

## ‰∏â„ÄÅËøêË°å

### 1.Êú¨Âú∞ËøêË°å

Â¶ÇÊûúÊòØ‰∏™‰∫∫ËÆ°ÁÆóÊú∫ **Êú¨Âú∞ËøêË°å**ÔºåÁõ¥Êé•Âú®È°πÁõÆÊ†πÁõÆÂΩï‰∏ãÊâßË°åÔºö

```bash
python3 app.py         # windowsÁéØÂ¢É‰∏ãËØ•ÂëΩ‰ª§ÈÄöÂ∏∏‰∏∫ python app.py
```

ËøêË°åÂêéÈªòËÆ§‰ºöÂêØÂä®webÊúçÂä°ÔºåÂèØÈÄöËøáËÆøÈóÆ `http://localhost:9899/chat` Âú®ÁΩëÈ°µÁ´ØÂØπËØù„ÄÇ

Â¶ÇÊûúÈúÄË¶ÅÊé•ÂÖ•ÂÖ∂‰ªñÂ∫îÁî®ÈÄöÈÅìÂè™ÈúÄ‰øÆÊîπ `config.json` ÈÖçÁΩÆÊñá‰ª∂‰∏≠ÁöÑ `channel_type` ÂèÇÊï∞ÔºåËØ¶ÊÉÖÂèÇËÄÉÔºö[ÈÄöÈÅìËØ¥Êòé](#ÈÄöÈÅìËØ¥Êòé)„ÄÇ


### 2.ÊúçÂä°Âô®ÈÉ®ÁΩ≤

Âú®ÊúçÂä°Âô®‰∏≠ÂèØ‰ΩøÁî® `nohup` ÂëΩ‰ª§Âú®ÂêéÂè∞ËøêË°åÁ®ãÂ∫èÔºö

```bash
nohup python3 app.py &amp; tail -f nohup.out
```

ÊâßË°åÂêéÁ®ãÂ∫èËøêË°å‰∫éÊúçÂä°Âô®ÂêéÂè∞ÔºåÂèØÈÄöËøá `ctrl+c` ÂÖ≥Èó≠Êó•ÂøóÔºå‰∏ç‰ºöÂΩ±ÂìçÂêéÂè∞Á®ãÂ∫èÁöÑËøêË°å„ÄÇ‰ΩøÁî® `ps -ef | grep app.py | grep -v grep` ÂëΩ‰ª§ÂèØÊü•ÁúãËøêË°å‰∫éÂêéÂè∞ÁöÑËøõÁ®ãÔºåÂ¶ÇÊûúÊÉ≥Ë¶ÅÈáçÊñ∞ÂêØÂä®Á®ãÂ∫èÂèØ‰ª•ÂÖà `kill` ÊéâÂØπÂ∫îÁöÑËøõÁ®ã„ÄÇ Êó•ÂøóÂÖ≥Èó≠ÂêéÂ¶ÇÊûúÊÉ≥Ë¶ÅÂÜçÊ¨°ÊâìÂºÄÂè™ÈúÄËæìÂÖ• `tail -f nohup.out`„ÄÇ 

Ê≠§Â§ñÔºåÈ°πÁõÆÁöÑ `scripts` ÁõÆÂΩï‰∏ãÊúâ‰∏ÄÈîÆËøêË°å„ÄÅÂÖ≥Èó≠Á®ãÂ∫èÁöÑËÑöÊú¨‰æõ‰ΩøÁî®„ÄÇ ËøêË°åÂêéÈªòËÆ§channel‰∏∫webÔºåÈÄöËøáÂèØ‰ª•ÈÄöËøá‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂ËøõË°åÂàáÊç¢„ÄÇ


### 3.DockerÈÉ®ÁΩ≤

‰ΩøÁî®dockerÈÉ®ÁΩ≤Êó†ÈúÄ‰∏ãËΩΩÊ∫êÁ†ÅÂíåÂÆâË£Ö‰æùËµñÔºåÂè™ÈúÄË¶ÅËé∑Âèñ `docker-compose.yml` ÈÖçÁΩÆÊñá‰ª∂Âπ∂ÂêØÂä®ÂÆπÂô®Âç≥ÂèØ„ÄÇAgentÊ®°Âºè‰∏ãÊõ¥Êé®Ëçê‰ΩøÁî®Ê∫êÁ†ÅËøõË°åÈÉ®ÁΩ≤Ôºå‰ª•Ëé∑ÂæóÊõ¥Â§öÁ≥ªÁªüËÆøÈóÆËÉΩÂäõ„ÄÇ

&gt; ÂâçÊèêÊòØÈúÄË¶ÅÂÆâË£ÖÂ•Ω `docker` Âèä `docker-compose`ÔºåÂÆâË£ÖÊàêÂäüÂêéÊâßË°å `docker -v` Âíå `docker-compose version` (Êàñ `docker compose version`) ÂèØÊü•ÁúãÂà∞ÁâàÊú¨Âè∑„ÄÇÂÆâË£ÖÂú∞ÂùÄ‰∏∫ [dockerÂÆòÁΩë](https://docs.docker.com/engine/install/) „ÄÇ

**(1) ‰∏ãËΩΩ docker-compose.yml Êñá‰ª∂**

```bash
wget https://cdn.link-ai.tech/code/cow/docker-compose.yml
```

‰∏ãËΩΩÂÆåÊàêÂêéÊâìÂºÄ `docker-compose.yml` Â°´ÂÜôÊâÄÈúÄÈÖçÁΩÆÔºå‰æãÂ¶Ç `CHANNEL_TYPE`„ÄÅ`OPEN_AI_API_KEY` ÂíåÁ≠âÈÖçÁΩÆ„ÄÇ

**(2) ÂêØÂä®ÂÆπÂô®**

Âú® `docker-compose.yml` ÊâÄÂú®ÁõÆÂΩï‰∏ãÊâßË°å‰ª•‰∏ãÂëΩ‰ª§ÂêØÂä®ÂÆπÂô®Ôºö

```bash
sudo docker compose up -d         # Ëã•docker-compose‰∏∫ 1.X ÁâàÊú¨ÔºåÂàôÊâßË°å `sudo  docker-compose up -d`
```

ËøêË°åÂëΩ‰ª§ÂêéÔºå‰ºöËá™Âä®Âèñ [docker hub](https://hub.docker.com/r/zhayujie/chatgpt-on-wechat) ÊãâÂèñÊúÄÊñ∞releaseÁâàÊú¨ÁöÑÈïúÂÉè„ÄÇÂΩìÊâßË°å `sudo docker ps` ËÉΩÊü•ÁúãÂà∞ NAMES ‰∏∫ chatgpt-on-wechat ÁöÑÂÆπÂô®Âç≥Ë°®Á§∫ËøêË°åÊàêÂäü„ÄÇÊúÄÂêéÊâßË°å‰ª•‰∏ãÂëΩ‰ª§ÂèØÊü•ÁúãÂÆπÂô®ÁöÑËøêË°åÊó•ÂøóÔºö

```bash
sudo docker logs -f chatgpt-on-wechat
```

**(3) Êèí‰ª∂‰ΩøÁî®**

Â¶ÇÊûúÈúÄË¶ÅÂú®dockerÂÆπÂô®‰∏≠‰øÆÊîπÊèí‰ª∂ÈÖçÁΩÆÔºåÂèØÈÄöËøáÊåÇËΩΩÁöÑÊñπÂºèÂÆåÊàêÔºåÂ∞Ü [Êèí‰ª∂ÈÖçÁΩÆÊñá‰ª∂](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/plugins/config.json.template)
ÈáçÂëΩÂêç‰∏∫ `config.json`ÔºåÊîæÁΩÆ‰∫é `docker-compose.yml` Áõ∏ÂêåÁõÆÂΩï‰∏ãÔºåÂπ∂Âú® `docker-compose.yml` ‰∏≠ÁöÑ `chatgpt-on-wechat` ÈÉ®ÂàÜ‰∏ãÊ∑ªÂä† `volumes` Êò†Â∞Ñ:

```
volumes:
  - ./config.json:/app/plugins/config.json
```
**Ê≥®**Ôºö‰ΩøÁî®dockerÊñπÂºèÈÉ®ÁΩ≤ÁöÑËØ¶ÁªÜÊïôÁ®ãÂèØ‰ª•ÂèÇËÄÉÔºö[dockerÈÉ®ÁΩ≤CoWÈ°πÁõÆ](https://www.wangpc.cc/ai/docker-deploy-cow/)


## Ê®°ÂûãËØ¥Êòé

‰ª•‰∏ãÂØπÊâÄÊúâÂèØÊîØÊåÅÁöÑÊ®°ÂûãÁöÑÈÖçÁΩÆÂíå‰ΩøÁî®ÊñπÊ≥ïËøõË°åËØ¥ÊòéÔºåÊ®°ÂûãÊé•Âè£ÂÆûÁé∞Âú®È°πÁõÆÁöÑ `models/` ÁõÆÂΩï‰∏ã„ÄÇ

&lt;details&gt;
&lt;summary&gt;OpenAI&lt;/summary&gt;

1. API KeyÂàõÂª∫ÔºöÂú® [OpenAIÂπ≥Âè∞](https://platform.openai.com/api-keys) ÂàõÂª∫API Key

2. Â°´ÂÜôÈÖçÁΩÆ

```json
{
    &quot;model&quot;: &quot;gpt-4.1-mini&quot;,
    &quot;open_ai_api_key&quot;: &quot;YOUR_API_KEY&quot;,
    &quot;open_ai_api_base&quot;: &quot;https://api.openai.com/v1&quot;,
    &quot;bot_type&quot;: &quot;chatGPT&quot;
}
```

 - `model`: ‰∏éOpenAIÊé•Âè£ÁöÑ [modelÂèÇÊï∞](https://platform.openai.com/docs/models) ‰∏ÄËá¥ÔºåÊîØÊåÅÂåÖÊã¨ oÁ≥ªÂàó„ÄÅgpt-5.2„ÄÅgpt-5.1„ÄÅgpt-4.1Á≠âÁ≥ªÂàóÊ®°Âûã
 - `open_ai_api_base`: Â¶ÇÊûúÈúÄË¶ÅÊé•ÂÖ•Á¨¨‰∏âÊñπ‰ª£ÁêÜÊé•Âè£ÔºåÂèØÈÄöËøá‰øÆÊîπËØ•ÂèÇÊï∞ËøõË°åÊé•ÂÖ•
 - `bot_type`: ‰ΩøÁî®OpenAIÁõ∏ÂÖ≥Ê®°ÂûãÊó∂Êó†ÈúÄÂ°´ÂÜô„ÄÇÂΩì‰ΩøÁî®Á¨¨‰∏âÊñπ‰ª£ÁêÜÊé•Âè£Êé•ÂÖ•ClaudeÁ≠âÈùûOpenAIÂÆòÊñπÊ®°ÂûãÊó∂ÔºåËØ•ÂèÇÊï∞ËÆæ‰∏∫ `chatGPT`
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;LinkAI&lt;/summary&gt;

1. API KeyÂàõÂª∫ÔºöÂú® [LinkAIÂπ≥Âè∞](https://link-ai.tech/console/interface) ÂàõÂª∫API Key 

2. Â°´ÂÜôÈÖçÁΩÆ

```json
{
    &quot;use_linkai&quot;: true,
    &quot;linkai_api_key&quot;: &quot;YOUR API KEY&quot;,
    &quot;linkai_app_code&quot;: &quot;YOUR APP CODE&quot;
}
```

+ `use_linkai`: ÊòØÂê¶‰ΩøÁî®LinkAIÊé•Âè£ÔºåÈªòËÆ§ÂÖ≥Èó≠ÔºåËÆæÁΩÆ‰∏∫trueÂêéÂèØÂØπÊé•LinkAIÂπ≥Âè∞ÁöÑÊô∫ËÉΩ‰ΩìÔºå‰ΩøÁî®Áü•ËØÜÂ∫ì„ÄÅÂ∑•‰ΩúÊµÅ„ÄÅÊï∞ÊçÆÂ∫ì„ÄÅMCPÊèí‰ª∂Á≠â‰∏∞ÂØåÁöÑAgentËÉΩÂäõ
+ `linkai_api_key`: LinkAIÂπ≥Âè∞ÁöÑAPI KeyÔºåÂèØÂú® [ÊéßÂà∂Âè∞](https://link-ai.tech/console/interface) ‰∏≠ÂàõÂª∫
+ `linkai_app_code`: LinkAIÊô∫ËÉΩ‰Ωì (Â∫îÁî®ÊàñÂ∑•‰ΩúÊµÅ) ÁöÑcodeÔºåÈÄâÂ°´ÔºåÊôÆÈÄöÂØπËØùÊ®°ÂºèÂèØÁî®„ÄÇÊô∫ËÉΩ‰ΩìÂàõÂª∫ÂèØÂèÇËÄÉ [ËØ¥ÊòéÊñáÊ°£](https://docs.link-ai.tech/platform/quick-start)
+ `model`: modelÂ≠óÊÆµÂ°´ÂÜôÁ©∫ÂàôÁõ¥Êé•‰ΩøÁî®Êô∫ËÉΩ‰ΩìÁöÑÊ®°ÂûãÔºåÂèØÂú®Âπ≥Âè∞‰∏≠ÁÅµÊ¥ªÂàáÊç¢Ôºå[Ê®°ÂûãÂàóË°®](https://link-ai.tech/console/models)‰∏≠ÁöÑÂÖ®ÈÉ®Ê®°ÂûãÂùáÂèØ‰ΩøÁî®
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;MiniMax&lt;/summary&gt;

ÊñπÂºè‰∏ÄÔºöÂÆòÊñπÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ã(Êé®Ëçê)Ôºö

```json
{
    &quot;model&quot;: &quot;MiniMax-M2.1&quot;,
    &quot;minimax_api_key&quot;: &quot;&quot;
}
```
 - `model`: ÂèØÂ°´ÂÜô `MiniMax-M2.1„ÄÅMiniMax-M2.1-lightning„ÄÅMiniMax-M2„ÄÅabab6.5-chat` Á≠â
 - `minimax_api_key`ÔºöMiniMaxÂπ≥Âè∞ÁöÑAPI-KEYÔºåÂú® [ÊéßÂà∂Âè∞](https://platform.minimaxi.com/user-center/basic-information/interface-key) ÂàõÂª∫

ÊñπÂºè‰∫åÔºöOpenAIÂÖºÂÆπÊñπÂºèÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö
```json
{
  &quot;bot_type&quot;: &quot;chatGPT&quot;,
  &quot;model&quot;: &quot;MiniMax-M2.1&quot;,
  &quot;open_ai_api_base&quot;: &quot;https://api.minimaxi.com/v1&quot;,
  &quot;open_ai_api_key&quot;: &quot;&quot;
}
```
- `bot_type`: OpenAIÂÖºÂÆπÊñπÂºè
- `model`: ÂèØÂ°´ `MiniMax-M2.1„ÄÅMiniMax-M2.1-lightning„ÄÅMiniMax-M2`ÔºåÂèÇËÄÉ[APIÊñáÊ°£](https://platform.minimaxi.com/document/%E5%AF%B9%E8%AF%9D?key=66701d281d57f38758d581d0#QklxsNSbaf6kM4j6wjO5eEek)
- `open_ai_api_base`: MiniMaxÂπ≥Âè∞APIÁöÑ BASE URL
- `open_ai_api_key`: MiniMaxÂπ≥Âè∞ÁöÑAPI-KEY
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Êô∫Ë∞±AI (GLM)&lt;/summary&gt;

ÊñπÂºè‰∏ÄÔºöÂÆòÊñπÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ã(Êé®Ëçê)Ôºö

```json
{
  &quot;model&quot;: &quot;glm-4.7&quot;,
  &quot;zhipu_ai_api_key&quot;: &quot;&quot;
}
```
 - `model`: ÂèØÂ°´ `glm-4.7„ÄÅglm-4-plus„ÄÅglm-4-flash„ÄÅglm-4-air„ÄÅglm-4-airx„ÄÅglm-4-long` Á≠â, ÂèÇËÄÉ [glm-4Á≥ªÂàóÊ®°ÂûãÁºñÁ†Å](https://bigmodel.cn/dev/api/normal-model/glm-4)
 - `zhipu_ai_api_key`: Êô∫Ë∞±AIÂπ≥Âè∞ÁöÑ API KEYÔºåÂú® [ÊéßÂà∂Âè∞](https://www.bigmodel.cn/usercenter/proj-mgmt/apikeys) ÂàõÂª∫

ÊñπÂºè‰∫åÔºöOpenAIÂÖºÂÆπÊñπÂºèÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö
```json
{
  &quot;bot_type&quot;: &quot;chatGPT&quot;,
  &quot;model&quot;: &quot;glm-4.7&quot;,
  &quot;open_ai_api_base&quot;: &quot;https://open.bigmodel.cn/api/paas/v4&quot;,
  &quot;open_ai_api_key&quot;: &quot;&quot;
}
```
- `bot_type`: OpenAIÂÖºÂÆπÊñπÂºè
- `model`: ÂèØÂ°´ `glm-4.7„ÄÅglm-4.6„ÄÅglm-4-plus„ÄÅglm-4-flash„ÄÅglm-4-air„ÄÅglm-4-airx„ÄÅglm-4-long` Á≠â
- `open_ai_api_base`: Êô∫Ë∞±AIÂπ≥Âè∞ÁöÑ BASE URL
- `open_ai_api_key`: Êô∫Ë∞±AIÂπ≥Âè∞ÁöÑ API KEY
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;ÈÄö‰πâÂçÉÈóÆ (Qwen)&lt;/summary&gt;

ÊñπÂºè‰∏ÄÔºöÂÆòÊñπSDKÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ã(Êé®Ëçê)Ôºö

```json
{
    &quot;model&quot;: &quot;qwen3-max&quot;,
    &quot;dashscope_api_key&quot;: &quot;sk-qVxxxxG&quot;
}
```
 - `model`: ÂèØÂ°´ÂÜô `qwen3-max„ÄÅqwen-max„ÄÅqwen-plus„ÄÅqwen-turbo„ÄÅqwen-long„ÄÅqwq-plus` Á≠â
 - `dashscope_api_key`: ÈÄö‰πâÂçÉÈóÆÁöÑ API-KEYÔºåÂèÇËÄÉ [ÂÆòÊñπÊñáÊ°£](https://bailian.console.aliyun.com/?tab=api#/api) ÔºåÂú® [ÊéßÂà∂Âè∞](https://bailian.console.aliyun.com/?tab=model#/api-key) ÂàõÂª∫

ÊñπÂºè‰∫åÔºöOpenAIÂÖºÂÆπÊñπÂºèÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö
```json
{
  &quot;bot_type&quot;: &quot;chatGPT&quot;,
  &quot;model&quot;: &quot;qwen3-max&quot;,
  &quot;open_ai_api_base&quot;: &quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,
  &quot;open_ai_api_key&quot;: &quot;sk-qVxxxxG&quot;
}
```
- `bot_type`: OpenAIÂÖºÂÆπÊñπÂºè
- `model`: ÊîØÊåÅÂÆòÊñπÊâÄÊúâÊ®°ÂûãÔºåÂèÇËÄÉ[Ê®°ÂûãÂàóË°®](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.0.78d84823Kth5on#9f8890ce29g5u)
- `open_ai_api_base`: ÈÄö‰πâÂçÉÈóÆAPIÁöÑ BASE URL
- `open_ai_api_key`: ÈÄö‰πâÂçÉÈóÆÁöÑ API-KEY
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Claude&lt;/summary&gt;

1. API KeyÂàõÂª∫ÔºöÂú® [ClaudeÊéßÂà∂Âè∞](https://console.anthropic.com/settings/keys) ÂàõÂª∫API Key

2. Â°´ÂÜôÈÖçÁΩÆ

```json
{
    &quot;model&quot;: &quot;claude-sonnet-4-5&quot;,
    &quot;claude_api_key&quot;: &quot;YOUR_API_KEY&quot;
}
```
 - `model`: ÂèÇËÄÉ [ÂÆòÊñπÊ®°ÂûãID](https://docs.anthropic.com/en/docs/about-claude/models/overview#model-aliases) ÔºåÊîØÊåÅ `claude-sonnet-4-5„ÄÅclaude-sonnet-4-0„ÄÅclaude-opus-4-0„ÄÅclaude-3-5-sonnet-latest` Á≠â
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Gemini&lt;/summary&gt;

API KeyÂàõÂª∫ÔºöÂú® [ÊéßÂà∂Âè∞](https://aistudio.google.com/app/apikey?hl=zh-cn) ÂàõÂª∫API Key ÔºåÈÖçÁΩÆÂ¶Ç‰∏ã
```json
{
    &quot;model&quot;: &quot;gemini-3-flash-preview&quot;,
    &quot;gemini_api_key&quot;: &quot;&quot;
}
```
 - `model`: ÂèÇËÄÉ[ÂÆòÊñπÊñáÊ°£-Ê®°ÂûãÂàóË°®](https://ai.google.dev/gemini-api/docs/models?hl=zh-cn)ÔºåÊîØÊåÅ `gemini-3-flash-preview„ÄÅgemini-3-pro-preview„ÄÅgemini-2.5-pro„ÄÅgemini-2.0-flash` Á≠â
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;DeepSeek&lt;/summary&gt;

1. API KeyÂàõÂª∫ÔºöÂú® [DeepSeekÂπ≥Âè∞](https://platform.deepseek.com/api_keys) ÂàõÂª∫API Key 

2. Â°´ÂÜôÈÖçÁΩÆ

```json
{
    &quot;model&quot;: &quot;deepseek-chat&quot;,
    &quot;open_ai_api_key&quot;: &quot;sk-xxxxxxxxxxx&quot;,
    &quot;open_ai_api_base&quot;: &quot;https://api.deepseek.com/v1&quot;, 
    &quot;bot_type&quot;: &quot;chatGPT&quot;

}
```

 - `bot_type`: OpenAIÂÖºÂÆπÊñπÂºè
 - `model`: ÂèØÂ°´ `deepseek-chat„ÄÅdeepseek-reasoner`ÔºåÂàÜÂà´ÂØπÂ∫îÁöÑÊòØ DeepSeek-V3 Âíå DeepSeek-R1 Ê®°Âûã
 - `open_ai_api_key`: DeepSeekÂπ≥Âè∞ÁöÑ API Key
 - `open_ai_api_base`: DeepSeekÂπ≥Âè∞ BASE URL
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Kimi (Moonshot)&lt;/summary&gt;

ÊñπÂºè‰∏ÄÔºöÂÆòÊñπÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö

```json
{
    &quot;model&quot;: &quot;moonshot-v1-128k&quot;,
    &quot;moonshot_api_key&quot;: &quot;&quot;
}
```
 - `model`: ÂèØÂ°´ÂÜô `moonshot-v1-8k„ÄÅmoonshot-v1-32k„ÄÅmoonshot-v1-128k`
 - `moonshot_api_key`: MoonshotÁöÑAPI-KEYÔºåÂú® [ÊéßÂà∂Âè∞](https://platform.moonshot.cn/console/api-keys) ÂàõÂª∫
 
ÊñπÂºè‰∫åÔºöOpenAIÂÖºÂÆπÊñπÂºèÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö
```json
{
  &quot;bot_type&quot;: &quot;chatGPT&quot;,
  &quot;model&quot;: &quot;moonshot-v1-128k&quot;,
  &quot;open_ai_api_base&quot;: &quot;https://api.moonshot.cn/v1&quot;,
  &quot;open_ai_api_key&quot;: &quot;&quot;
}
```
- `bot_type`: OpenAIÂÖºÂÆπÊñπÂºè
- `model`: ÂèØÂ°´ÂÜô `moonshot-v1-8k„ÄÅmoonshot-v1-32k„ÄÅmoonshot-v1-128k`
- `open_ai_api_base`: MoonshotÁöÑ BASE URL
- `open_ai_api_key`: MoonshotÁöÑ API-KEY
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Azure&lt;/summary&gt;

1. API KeyÂàõÂª∫ÔºöÂú® [AzureÂπ≥Âè∞](https://oai.azure.com/) ÂàõÂª∫API Key 

2. Â°´ÂÜôÈÖçÁΩÆ

```json
{
  &quot;model&quot;: &quot;&quot;,
  &quot;use_azure_chatgpt&quot;: true,
  &quot;open_ai_api_key&quot;: &quot;&quot;,
  &quot;open_ai_api_base&quot;: &quot;&quot;,
  &quot;azure_deployment_id&quot;: &quot;&quot;,
  &quot;azure_api_version&quot;: &quot;2025-01-01-preview&quot;
}
```

 - `model`: ÁïôÁ©∫Âç≥ÂèØ
 - `use_azure_chatgpt`: ËÆæ‰∏∫ true 
 - `open_ai_api_key`: AzureÂπ≥Âè∞ÁöÑÂØÜÈí•
 - `open_ai_api_base`: AzureÂπ≥Âè∞ÁöÑ BASE URL
 - `azure_deployment_id`: AzureÂπ≥Âè∞ÈÉ®ÁΩ≤ÁöÑÊ®°ÂûãÂêçÁß∞
 - `azure_api_version`: apiÁâàÊú¨‰ª•Âèä‰ª•‰∏äÂèÇÊï∞ÂèØ‰ª•Âú®ÈÉ®ÁΩ≤ÁöÑ [Ê®°ÂûãÈÖçÁΩÆ](https://oai.azure.com/resource/deployments) ÁïåÈù¢Êü•Áúã
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;ÁôæÂ∫¶ÊñáÂøÉ&lt;/summary&gt;
ÊñπÂºè‰∏ÄÔºöÂÆòÊñπSDKÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö

```json
{
    &quot;model&quot;: &quot;wenxin-4&quot;, 
    &quot;baidu_wenxin_api_key&quot;: &quot;IajztZ0bDxgnP9bEykU7lBer&quot;,
    &quot;baidu_wenxin_secret_key&quot;: &quot;EDPZn6L24uAS9d8RWFfotK47dPvkjD6G&quot;
}
```
 - `model`: ÂèØÂ°´ `wenxin`Âíå`wenxin-4`ÔºåÂØπÂ∫îÊ®°Âûã‰∏∫ ÊñáÂøÉ-3.5 Âíå ÊñáÂøÉ-4.0
 - `baidu_wenxin_api_key`ÔºöÂèÇËÄÉ [ÂçÉÂ∏ÜÂπ≥Âè∞-access_tokenÈâ¥ÊùÉ](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/dlv4pct3s) ÊñáÊ°£Ëé∑Âèñ API Key
 - `baidu_wenxin_secret_key`ÔºöÂèÇËÄÉ [ÂçÉÂ∏ÜÂπ≥Âè∞-access_tokenÈâ¥ÊùÉ](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/dlv4pct3s) ÊñáÊ°£Ëé∑Âèñ Secret Key

ÊñπÂºè‰∫åÔºöOpenAIÂÖºÂÆπÊñπÂºèÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö
```json
{
  &quot;bot_type&quot;: &quot;chatGPT&quot;,
  &quot;model&quot;: &quot;ERNIE-4.0-Turbo-8K&quot;,
  &quot;open_ai_api_base&quot;: &quot;https://qianfan.baidubce.com/v2&quot;,
  &quot;open_ai_api_key&quot;: &quot;bce-v3/ALTxxxxxxd2b&quot;
}
```
- `bot_type`: OpenAIÂÖºÂÆπÊñπÂºè
- `model`: ÊîØÊåÅÂÆòÊñπÊâÄÊúâÊ®°ÂûãÔºåÂèÇËÄÉ[Ê®°ÂûãÂàóË°®](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Wm9cvy6rl)
- `open_ai_api_base`: ÁôæÂ∫¶ÊñáÂøÉAPIÁöÑ BASE URL
- `open_ai_api_key`: ÁôæÂ∫¶ÊñáÂøÉÁöÑ API-KEYÔºåÂèÇËÄÉ [ÂÆòÊñπÊñáÊ°£](https://cloud.baidu.com/doc/qianfan-api/s/ym9chdsy5) ÔºåÂú® [ÊéßÂà∂Âè∞](https://console.bce.baidu.com/iam/#/iam/apikey/list) ÂàõÂª∫API Key

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;ËÆØÈ£ûÊòüÁÅ´&lt;/summary&gt;

ÊñπÂºè‰∏ÄÔºöÂÆòÊñπÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö
ÂèÇËÄÉ [ÂÆòÊñπÊñáÊ°£-Âø´ÈÄüÊåáÂºï](https://www.xfyun.cn/doc/platform/quickguide.html#%E7%AC%AC%E4%BA%8C%E6%AD%A5-%E5%88%9B%E5%BB%BA%E6%82%A8%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%BA%94%E7%94%A8-%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8%E6%9C%8D%E5%8A%A1) Ëé∑Âèñ `APPID„ÄÅ APISecret„ÄÅ APIKey` ‰∏â‰∏™ÂèÇÊï∞

```json
{
  &quot;model&quot;: &quot;xunfei&quot;,
  &quot;xunfei_app_id&quot;: &quot;&quot;,
  &quot;xunfei_api_key&quot;: &quot;&quot;,
  &quot;xunfei_api_secret&quot;: &quot;&quot;,
  &quot;xunfei_domain&quot;: &quot;4.0Ultra&quot;,
  &quot;xunfei_spark_url&quot;: &quot;wss://spark-api.xf-yun.com/v4.0/chat&quot;
}
```
 - `model`: Â°´ `xunfei`
 - `xunfei_domain`: ÂèØÂ°´ÂÜô `4.0Ultra„ÄÅgeneralv3.5„ÄÅmax-32k„ÄÅgeneralv3„ÄÅpro-128k„ÄÅlite`
 - `xunfei_spark_url`: Â°´ÂÜôÂèÇËÄÉ [ÂÆòÊñπÊñáÊ°£-ËØ∑Ê±ÇÂú∞ÂùÄ](https://www.xfyun.cn/doc/spark/Web.html#_1-1-%E8%AF%B7%E6%B1%82%E5%9C%B0%E5%9D%80) ÁöÑËØ¥Êòé
 
ÊñπÂºè‰∫åÔºöOpenAIÂÖºÂÆπÊñπÂºèÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö
```json
{
  &quot;bot_type&quot;: &quot;chatGPT&quot;,
  &quot;model&quot;: &quot;4.0Ultra&quot;,
  &quot;open_ai_api_base&quot;: &quot;https://spark-api-open.xf-yun.com/v1&quot;,
  &quot;open_ai_api_key&quot;: &quot;&quot;
}
```
- `bot_type`: OpenAIÂÖºÂÆπÊñπÂºè
- `model`: ÂèØÂ°´ÂÜô `4.0Ultra„ÄÅgeneralv3.5„ÄÅmax-32k„ÄÅgeneralv3„ÄÅpro-128k„ÄÅlite`
- `open_ai_api_base`: ËÆØÈ£ûÊòüÁÅ´Âπ≥Âè∞ÁöÑ BASE URL
- `open_ai_api_key`: ËÆØÈ£ûÊòüÁÅ´Âπ≥Âè∞ÁöÑ[APIPassword](https://console.xfyun.cn/services/bm3) ÔºåÂõ†Ê®°ÂûãËÄåÂ∑≤
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;ModelScope&lt;/summary&gt;

```json
{
  &quot;bot_type&quot;: &quot;modelscope&quot;,
  &quot;model&quot;: &quot;Qwen/QwQ-32B&quot;,
  &quot;modelscope_api_key&quot;: &quot;your_api_key&quot;,
  &quot;modelscope_base_url&quot;: &quot;https://api-inference.modelscope.cn/v1/chat/completions&quot;,
  &quot;text_to_image&quot;: &quot;MusePublic/489_ckpt_FLUX_1&quot;
}
```

- `bot_type`: modelscopeÊé•Âè£Ê†ºÂºè
- `model`: ÂèÇËÄÉ[Ê®°ÂûãÂàóË°®](https://www.modelscope.cn/models?filter=inference_type&amp;page=1)
- `modelscope_api_key`: ÂèÇËÄÉ [ÂÆòÊñπÊñáÊ°£-ËÆøÈóÆ‰ª§Áâå](https://modelscope.cn/docs/accounts/token) ÔºåÂú® [ÊéßÂà∂Âè∞](https://modelscope.cn/my/myaccesstoken) 
- `modelscope_base_url`: modelscopeÂπ≥Âè∞ÁöÑ BASE URL
- `text_to_image`: ÂõæÂÉèÁîüÊàêÊ®°ÂûãÔºåÂèÇËÄÉ[Ê®°ÂûãÂàóË°®](https://www.modelscope.cn/models?filter=inference_type&amp;page=1)
&lt;/details&gt;


## ÈÄöÈÅìËØ¥Êòé

‰ª•‰∏ãÂØπÂèØÊé•ÂÖ•ÈÄöÈÅìÁöÑÈÖçÁΩÆÊñπÂºèËøõË°åËØ¥ÊòéÔºåÂ∫îÁî®ÈÄöÈÅì‰ª£Á†ÅÂú®È°πÁõÆÁöÑ `channel/` ÁõÆÂΩï‰∏ã„ÄÇ

&lt;details&gt;
&lt;summary&gt;1. Web&lt;/summary&gt;

È°πÁõÆÂêØÂä®ÂêéÈªòËÆ§ËøêË°åWebÈÄöÈÅìÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö

```json
{
    &quot;channel_type&quot;: &quot;web&quot;,
    &quot;web_port&quot;: 9899
}
```

- `web_port`: ÈªòËÆ§‰∏∫ 9899ÔºåÂèØÊåâÈúÄÊõ¥ÊîπÔºåÈúÄË¶ÅÊúçÂä°Âô®Èò≤ÁÅ´Â¢ôÂíåÂÆâÂÖ®ÁªÑÊîæË°åËØ•Á´ØÂè£
- Â¶ÇÊú¨Âú∞ËøêË°åÔºåÂêØÂä®ÂêéËØ∑ËÆøÈóÆ `http://localhost:9899/chat` ÔºõÂ¶ÇÊúçÂä°Âô®ËøêË°åÔºåËØ∑ËÆøÈóÆ `http://ip:9899/chat` 
&gt; Ê≥®ÔºöËØ∑Â∞Ü‰∏äËø∞ url ‰∏≠ÁöÑ ip ÊàñËÄÖ port ÊõøÊç¢‰∏∫ÂÆûÈôÖÁöÑÂÄº
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;2. Feishu - È£û‰π¶&lt;/summary&gt;

È£û‰π¶ÊîØÊåÅ‰∏§Áßç‰∫ã‰ª∂Êé•Êî∂Ê®°ÂºèÔºöWebSocket ÈïøËøûÊé•ÔºàÊé®ËçêÔºâÂíå Webhook„ÄÇ

**ÊñπÂºè‰∏ÄÔºöWebSocket Ê®°ÂºèÔºàÊé®ËçêÔºåÊó†ÈúÄÂÖ¨ÁΩë IPÔºâ**

```json
{
    &quot;channel_type&quot;: &quot;feishu&quot;,
    &quot;feishu_app_id&quot;: &quot;APP_ID&quot;,
    &quot;feishu_app_secret&quot;: &quot;APP_SECRET&quot;,
    &quot;feishu_event_mode&quot;: &quot;websocket&quot;
}
```

**ÊñπÂºè‰∫åÔºöWebhook Ê®°ÂºèÔºàÈúÄË¶ÅÂÖ¨ÁΩë IPÔºâ**

```json
{
    &quot;channel_type&quot;: &quot;feishu&quot;,
    &quot;feishu_app_id&quot;: &quot;APP_ID&quot;,
    &quot;feishu_app_secret&quot;: &quot;APP_SECRET&quot;,
    &quot;feishu_token&quot;: &quot;VERIFICATION_TOKEN&quot;,
    &quot;feishu_event_mode&quot;: &quot;webhook&quot;,
    &quot;feishu_port&quot;: 9891
}
```

- `feishu_event_mode`: ‰∫ã‰ª∂Êé•Êî∂Ê®°ÂºèÔºå`

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[masoncl/review-prompts]]></title>
            <link>https://github.com/masoncl/review-prompts</link>
            <guid>https://github.com/masoncl/review-prompts</guid>
            <pubDate>Thu, 05 Feb 2026 00:07:22 GMT</pubDate>
            <description><![CDATA[AI review prompts]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/masoncl/review-prompts">masoncl/review-prompts</a></h1>
            <p>AI review prompts</p>
            <p>Language: Python</p>
            <p>Stars: 553</p>
            <p>Forks: 43</p>
            <p>Stars today: 289 stars today</p>
            <h2>README</h2><pre># Review Prompts for AI-Assisted Code Review

AI-assisted code review prompts for Linux kernel and systemd development.
Works with Claude Code and other AI tools.

## Quick Start

### Install Kernel Prompts Only

```bash
cd kernel/scripts
./claude-setup.sh
```

### Install systemd Prompts Only

```bash
cd systemd/scripts
./claude-setup.sh
```

### Install Both

```bash
cd kernel/scripts &amp;&amp; ./claude-setup.sh
cd ../systemd/scripts &amp;&amp; ./claude-setup.sh
```

## Available Commands

| Project | Review | Debug | Verify |
|---------|--------|-------|--------|
| Kernel  | `/kreview` | `/kdebug` | `/kverify` |
| systemd | `/systemd-review` | `/systemd-debug` | `/systemd-verify` |

## Project Documentation

- [Kernel Review Prompts](kernel/README.md) - Linux kernel specific patterns and protocols
- [systemd Review Prompts](systemd/README.md) - systemd specific patterns and protocols

## How It Works

Each project has:
- **Skill file** - Automatically loads context when working in the project tree
- **Slash commands** - Quick access to review, debug, and verify workflows
- **Subsystem files** - Domain-specific knowledge loaded on demand

The skills detect your working directory and load appropriate context:
- In a kernel tree: kernel skill loads automatically
- In a systemd tree: systemd skill loads automatically

## Structure

```
review-prompts/
‚îú‚îÄ‚îÄ kernel/                    # Linux kernel prompts
‚îÇ   ‚îú‚îÄ‚îÄ skills/               # Skill template
‚îÇ   ‚îú‚îÄ‚îÄ slash-commands/       # /kreview, /kdebug, /kverify
‚îÇ   ‚îú‚îÄ‚îÄ scripts/              # Setup script and utilities
‚îÇ   ‚îú‚îÄ‚îÄ patterns/             # Bug pattern documentation
‚îÇ   ‚îî‚îÄ‚îÄ *.md                  # Subsystem and protocol files
‚îÇ
‚îú‚îÄ‚îÄ systemd/                   # systemd prompts
‚îÇ   ‚îú‚îÄ‚îÄ skills/               # Skill template
‚îÇ   ‚îú‚îÄ‚îÄ slash-commands/       # /systemd-review, /systemd-debug, /systemd-verify
‚îÇ   ‚îú‚îÄ‚îÄ scripts/              # Setup script
‚îÇ   ‚îú‚îÄ‚îÄ patterns/             # Bug pattern documentation
‚îÇ   ‚îî‚îÄ‚îÄ *.md                  # Subsystem and protocol files
‚îÇ
‚îî‚îÄ‚îÄ README.md                  # This file
```

## Semcode Integration

These prompts work best with [semcode](https://github.com/facebookexperimental/semcode)
for fast code navigation and semantic search.

## License

See [kernel/LICENSE](kernel/LICENSE) for license information.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pipecat-ai/pipecat]]></title>
            <link>https://github.com/pipecat-ai/pipecat</link>
            <guid>https://github.com/pipecat-ai/pipecat</guid>
            <pubDate>Thu, 05 Feb 2026 00:07:21 GMT</pubDate>
            <description><![CDATA[Open Source framework for voice and multimodal conversational AI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pipecat-ai/pipecat">pipecat-ai/pipecat</a></h1>
            <p>Open Source framework for voice and multimodal conversational AI</p>
            <p>Language: Python</p>
            <p>Stars: 10,176</p>
            <p>Forks: 1,692</p>
            <p>Stars today: 26 stars today</p>
            <h2>README</h2><pre>&lt;h1&gt;&lt;div align=&quot;center&quot;&gt;
 &lt;img alt=&quot;pipecat&quot; width=&quot;300px&quot; height=&quot;auto&quot; src=&quot;https://raw.githubusercontent.com/pipecat-ai/pipecat/main/pipecat.png&quot;&gt;
&lt;/div&gt;&lt;/h1&gt;

[![PyPI](https://img.shields.io/pypi/v/pipecat-ai)](https://pypi.org/project/pipecat-ai) ![Tests](https://github.com/pipecat-ai/pipecat/actions/workflows/tests.yaml/badge.svg) [![codecov](https://codecov.io/gh/pipecat-ai/pipecat/graph/badge.svg?token=LNVUIVO4Y9)](https://codecov.io/gh/pipecat-ai/pipecat) [![Docs](https://img.shields.io/badge/Documentation-blue)](https://docs.pipecat.ai) [![Discord](https://img.shields.io/discord/1239284677165056021)](https://discord.gg/pipecat) [![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/pipecat-ai/pipecat)

# üéôÔ∏è Pipecat: Real-Time Voice &amp; Multimodal AI Agents

**Pipecat** is an open-source Python framework for building real-time voice and multimodal conversational agents. Orchestrate audio and video, AI services, different transports, and conversation pipelines effortlessly‚Äîso you can focus on what makes your agent unique.

&gt; Want to dive right in? Try the [quickstart](https://docs.pipecat.ai/getting-started/quickstart).

## üöÄ What You Can Build

- **Voice Assistants** ‚Äì natural, streaming conversations with AI
- **AI Companions** ‚Äì coaches, meeting assistants, characters
- **Multimodal Interfaces** ‚Äì voice, video, images, and more
- **Interactive Storytelling** ‚Äì creative tools with generative media
- **Business Agents** ‚Äì customer intake, support bots, guided flows
- **Complex Dialog Systems** ‚Äì design logic with structured conversations

## üß† Why Pipecat?

- **Voice-first**: Integrates speech recognition, text-to-speech, and conversation handling
- **Pluggable**: Supports many AI services and tools
- **Composable Pipelines**: Build complex behavior from modular components
- **Real-Time**: Ultra-low latency interaction with different transports (e.g. WebSockets or WebRTC)

## üåê Pipecat Ecosystem

### üì± Client SDKs

Building client applications? You can connect to Pipecat from any platform using our official SDKs:

&lt;a href=&quot;https://docs.pipecat.ai/client/js/introduction&quot;&gt;JavaScript&lt;/a&gt; | &lt;a href=&quot;https://docs.pipecat.ai/client/react/introduction&quot;&gt;React&lt;/a&gt; | &lt;a href=&quot;https://docs.pipecat.ai/client/react-native/introduction&quot;&gt;React Native&lt;/a&gt; |
&lt;a href=&quot;https://docs.pipecat.ai/client/ios/introduction&quot;&gt;Swift&lt;/a&gt; | &lt;a href=&quot;https://docs.pipecat.ai/client/android/introduction&quot;&gt;Kotlin&lt;/a&gt; | &lt;a href=&quot;https://docs.pipecat.ai/client/c++/introduction&quot;&gt;C++&lt;/a&gt; | &lt;a href=&quot;https://github.com/pipecat-ai/pipecat-esp32&quot;&gt;ESP32&lt;/a&gt;

### üß≠ Structured conversations

Looking to build structured conversations? Check out [Pipecat Flows](https://github.com/pipecat-ai/pipecat-flows) for managing complex conversational states and transitions.

### ü™Ñ Beautiful UIs

Want to build beautiful and engaging experiences? Checkout the [Voice UI Kit](https://github.com/pipecat-ai/voice-ui-kit), a collection of components, hooks and templates for building voice AI applications quickly.

### üõ†Ô∏è Create and deploy projects

Create a new project in under a minute with the [Pipecat CLI](https://github.com/pipecat-ai/pipecat-cli). Then use the CLI to monitor and deploy your agent to production.

### üîç Debugging

Looking for help debugging your pipeline and processors? Check out [Whisker](https://github.com/pipecat-ai/whisker), a real-time Pipecat debugger.

### üñ•Ô∏è Terminal

Love terminal applications? Check out [Tail](https://github.com/pipecat-ai/tail), a terminal dashboard for Pipecat.

### üì∫Ô∏è Pipecat TV Channel

Catch new features, interviews, and how-tos on our [Pipecat TV](https://www.youtube.com/playlist?list=PLzU2zoMTQIHjqC3v4q2XVSR3hGSzwKFwH) channel.

## üé¨ See it in action

&lt;p float=&quot;left&quot;&gt;
    &lt;a href=&quot;https://github.com/pipecat-ai/pipecat-examples/tree/main/simple-chatbot&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/pipecat-ai/pipecat-examples/main/simple-chatbot/image.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;nbsp;
    &lt;a href=&quot;https://github.com/pipecat-ai/pipecat-examples/tree/main/storytelling-chatbot&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/pipecat-ai/pipecat-examples/main/storytelling-chatbot/image.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;
    &lt;br/&gt;
    &lt;a href=&quot;https://github.com/pipecat-ai/pipecat-examples/tree/main/translation-chatbot&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/pipecat-ai/pipecat-examples/main/translation-chatbot/image.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;&amp;nbsp;
    &lt;a href=&quot;https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/12-describe-video.py&quot;&gt;&lt;img src=&quot;https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/assets/moondream.png&quot; width=&quot;400&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;

## üß© Available services

| Category            | Services                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| ------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Speech-to-Text      | [AssemblyAI](https://docs.pipecat.ai/server/services/stt/assemblyai), [AWS](https://docs.pipecat.ai/server/services/stt/aws), [Azure](https://docs.pipecat.ai/server/services/stt/azure), [Cartesia](https://docs.pipecat.ai/server/services/stt/cartesia), [Deepgram](https://docs.pipecat.ai/server/services/stt/deepgram), [ElevenLabs](https://docs.pipecat.ai/server/services/stt/elevenlabs), [Fal Wizper](https://docs.pipecat.ai/server/services/stt/fal), [Gladia](https://docs.pipecat.ai/server/services/stt/gladia), [Google](https://docs.pipecat.ai/server/services/stt/google), [Gradium](https://docs.pipecat.ai/server/services/stt/gradium), [Groq (Whisper)](https://docs.pipecat.ai/server/services/stt/groq), [Hathora](https://docs.pipecat.ai/server/services/stt/hathora), [NVIDIA Riva](https://docs.pipecat.ai/server/services/stt/riva), [OpenAI (Whisper)](https://docs.pipecat.ai/server/services/stt/openai), [SambaNova (Whisper)](https://docs.pipecat.ai/server/services/stt/sambanova), [Sarvam](https://docs.pipecat.ai/server/services/stt/sarvam), [Soniox](https://docs.pipecat.ai/server/services/stt/soniox), [Speechmatics](https://docs.pipecat.ai/server/services/stt/speechmatics), [Whisper](https://docs.pipecat.ai/server/services/stt/whisper)                                                                                                                                                                                                                                                            |
| LLMs                | [Anthropic](https://docs.pipecat.ai/server/services/llm/anthropic), [AWS](https://docs.pipecat.ai/server/services/llm/aws), [Azure](https://docs.pipecat.ai/server/services/llm/azure), [Cerebras](https://docs.pipecat.ai/server/services/llm/cerebras), [DeepSeek](https://docs.pipecat.ai/server/services/llm/deepseek), [Fireworks AI](https://docs.pipecat.ai/server/services/llm/fireworks), [Gemini](https://docs.pipecat.ai/server/services/llm/gemini), [Grok](https://docs.pipecat.ai/server/services/llm/grok), [Groq](https://docs.pipecat.ai/server/services/llm/groq), [Mistral](https://docs.pipecat.ai/server/services/llm/mistral), [NVIDIA NIM](https://docs.pipecat.ai/server/services/llm/nim), [Ollama](https://docs.pipecat.ai/server/services/llm/ollama), [OpenAI](https://docs.pipecat.ai/server/services/llm/openai), [OpenRouter](https://docs.pipecat.ai/server/services/llm/openrouter), [Perplexity](https://docs.pipecat.ai/server/services/llm/perplexity), [Qwen](https://docs.pipecat.ai/server/services/llm/qwen), [SambaNova](https://docs.pipecat.ai/server/services/llm/sambanova) [Together AI](https://docs.pipecat.ai/server/services/llm/together)                                                                                                                                                                                                                                                                                              |
| Text-to-Speech      | [Async](https://docs.pipecat.ai/server/services/tts/asyncai), [AWS](https://docs.pipecat.ai/server/services/tts/aws), [Azure](https://docs.pipecat.ai/server/services/tts/azure), [Camb AI](https://docs.pipecat.ai/server/services/tts/camb), [Cartesia](https://docs.pipecat.ai/server/services/tts/cartesia), [Deepgram](https://docs.pipecat.ai/server/services/tts/deepgram), [ElevenLabs](https://docs.pipecat.ai/server/services/tts/elevenlabs), [Fish](https://docs.pipecat.ai/server/services/tts/fish), [Google](https://docs.pipecat.ai/server/services/tts/google), [Gradium](https://docs.pipecat.ai/server/services/tts/gradium), [Groq](https://docs.pipecat.ai/server/services/tts/groq), [Hathora](https://docs.pipecat.ai/server/services/tts/hathora), [Hume](https://docs.pipecat.ai/server/services/tts/hume), [Inworld](https://docs.pipecat.ai/server/services/tts/inworld), [LMNT](https://docs.pipecat.ai/server/services/tts/lmnt), [MiniMax](https://docs.pipecat.ai/server/services/tts/minimax), [Neuphonic](https://docs.pipecat.ai/server/services/tts/neuphonic), [NVIDIA Riva](https://docs.pipecat.ai/server/services/tts/riva), [OpenAI](https://docs.pipecat.ai/server/services/tts/openai), [Piper](https://docs.pipecat.ai/server/services/tts/piper), [PlayHT](https://docs.pipecat.ai/server/services/tts/playht), [Resemble](https://docs.pipecat.ai/server/services/tts/resemble), [Rime](https://docs.pipecat.ai/server/services/tts/rime), [Sarvam](https://docs.pipecat.ai/server/services/tts/sarvam), [Speechmatics](https://docs.pipecat.ai/server/services/tts/speechmatics), [XTTS](https://docs.pipecat.ai/server/services/tts/xtts) |
| Speech-to-Speech    | [AWS Nova Sonic](https://docs.pipecat.ai/server/services/s2s/aws), [Gemini Multimodal Live](https://docs.pipecat.ai/server/services/s2s/gemini), [Grok Voice Agent](https://docs.pipecat.ai/server/services/s2s/grok), [OpenAI Realtime](https://docs.pipecat.ai/server/services/s2s/openai), [Ultravox](https://docs.pipecat.ai/server/services/s2s/ultravox),                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| Transport           | [Daily (WebRTC)](https://docs.pipecat.ai/server/services/transport/daily), [FastAPI Websocket](https://docs.pipecat.ai/server/services/transport/fastapi-websocket), [SmallWebRTCTransport](https://docs.pipecat.ai/server/services/transport/small-webrtc), [WebSocket Server](https://docs.pipecat.ai/server/services/transport/websocket-server), Local                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Serializers         | [Exotel](https://docs.pipecat.ai/server/utilities/serializers/exotel), [Plivo](https://docs.pipecat.ai/server/utilities/serializers/plivo), [Twilio](https://docs.pipecat.ai/server/utilities/serializers/twilio), [Telnyx](https://docs.pipecat.ai/server/utilities/serializers/telnyx), [Vonage](https://docs.pipecat.ai/server/utilities/serializers/vonage)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| Video               | [HeyGen](https://docs.pipecat.ai/server/services/video/heygen), [Tavus](https://docs.pipecat.ai/server/services/video/tavus), [Simli](https://docs.pipecat.ai/server/services/video/simli)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| Memory              | [mem0](https://docs.pipecat.ai/server/services/memory/mem0)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| Vision &amp; Image      | [fal](https://docs.pipecat.ai/server/services/image-generation/fal), [Google Imagen](https://docs.pipecat.ai/server/services/image-generation/google-imagen), [Moondream](https://docs.pipecat.ai/server/services/vision/moondream)                                                                                                                                                                                                                

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[dagster-io/dagster]]></title>
            <link>https://github.com/dagster-io/dagster</link>
            <guid>https://github.com/dagster-io/dagster</guid>
            <pubDate>Thu, 05 Feb 2026 00:07:20 GMT</pubDate>
            <description><![CDATA[An orchestration platform for the development, production, and observation of data assets.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/dagster-io/dagster">dagster-io/dagster</a></h1>
            <p>An orchestration platform for the development, production, and observation of data assets.</p>
            <p>Language: Python</p>
            <p>Stars: 14,894</p>
            <p>Forks: 1,967</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre>python_modules/dagster/README.md</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[QwenLM/Qwen3-Coder]]></title>
            <link>https://github.com/QwenLM/Qwen3-Coder</link>
            <guid>https://github.com/QwenLM/Qwen3-Coder</guid>
            <pubDate>Thu, 05 Feb 2026 00:07:19 GMT</pubDate>
            <description><![CDATA[Qwen3-Coder is the code version of Qwen3, the large language model series developed by Qwen team.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/QwenLM/Qwen3-Coder">QwenLM/Qwen3-Coder</a></h1>
            <p>Qwen3-Coder is the code version of Qwen3, the large language model series developed by Qwen team.</p>
            <p>Language: Python</p>
            <p>Stars: 15,258</p>
            <p>Forks: 1,060</p>
            <p>Stars today: 123 stars today</p>
            <h2>README</h2><pre>&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/qwen3_coder.png&quot; width=&quot;400&quot;/&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen3-Coder-Next/swebench_pro.png&quot; width=&quot;800&quot;/&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
        üíú &lt;a href=&quot;https://chat.qwen.ai/&quot;&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbspü§ó &lt;a href=&quot;https://huggingface.co/collections/Qwen/qwen3-coder-687fc861e53c939e52d52d10&quot;&gt;Hugging Face&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbspü§ñ &lt;a href=&quot;https://modelscope.cn/organization/qwen&quot;&gt;ModelScope&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbsp üìë &lt;a href=&quot;https://qwenlm.github.io/blog/qwen3-coder-next/&quot;&gt;Blog&lt;/a&gt; &amp;nbsp&amp;nbsp ÔΩú &amp;nbsp&amp;nbspüìñ &lt;a href=&quot;https://qwen.readthedocs.io/&quot;&gt;Documentation&lt;/a&gt;
&lt;br&gt;
üåç &lt;a href=&quot;https://huggingface.co/spaces/Qwen/Qwen3-Coder-WebDev&quot;&gt;WebDev&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbspüí¨ &lt;a href=&quot;https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png&quot;&gt;WeChat (ÂæÆ‰ø°)&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbspü´® &lt;a href=&quot;https://discord.gg/CV4E9rpNSD&quot;&gt; Discord&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbsp üìÑ &lt;a href=&quot;https://github.com/QwenLM/Qwen3-Coder/blob/main/qwen3_coder_next_tech_report.pdf&quot;&gt;Arxiv&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbsp üëΩ &lt;a href=&quot;https://github.com/QwenLM/qwen-code&quot;&gt;Qwen Code&lt;/a&gt;
&lt;/p&gt;

Visit our Hugging Face or ModelScope organization (click links above), search checkpoints with names starting with `Qwen3-Coder-`, and you will find all you need! Enjoy!

---

## Table of Contents
  - [Introduction](#introduction)
    - [Key Features](#key-features)
  - [Basic Information](#basic-information)
  - [Quick Start](#quick-start)
    - [üëâüèª Chat with Qwen3-Coder](#-chat-with-qwen3-coder)
      - [Fill in the middle with Qwen3-Coder](#fill-in-the-middle-with-qwen3-coder)
  - [Use Cases](#use-cases)
    - [Example: Releasing a Website](#example-releasing-a-website)
    - [Example: Desktop Tidy](#example-desktop-tidy)
    - [Example: Zombies vs. Plants](#example-zombies-vs-plants)
    - [Example: Sound ASCII Art](#example-sound-ascii-art)
    - [Example: Vibe Checking](#example-vibe-checking)
    - [Example: Parkour Game](#example-parkour-game)
  - [Star History](#star-history)
  - [Citation](#citation)
  - [Contact Us](#contact-us)

---

# Qwen3-Coder-Next: Pushing Small Hybrid Models on Agentic Coding

## Introduction

We are announcing Qwen3-Coder, our most agentic code model to date. **Qwen3-Coder** is available in multiple sizes, **Qwen3-Coder-480B-A35B-Instruct**, **Qwen3-Coder-30B-A3B-Instruct**, **Qwen3-Coder-Next**, offering exceptional performance in both coding and agentic tasks. 

**Qwen3-Coder-Next**, an open-weight language model designed specifically for coding agents and local development. Built on top of **Qwen3-Next-80B-A3B-Base**, which adopts a novel architecture with hybrid attention and MoE, Qwen3-Coder-Next has been agentically trained at scale on large-scale executable task synthesis, environment interaction, and reinforcement learning, obtaining strong coding and agentic capabilities with significantly lower inference costs.

### Key Features

üíª **Efficiency-Performance Tradeoff**: among open models on **Agentic Coding**, **Agentic Browser-Use**, and other foundational coding tasks, achieving results comparable to Claude Sonnet.

üõ† **Scaling Agentic Coding**: supporting most platforms such as **Qwen Code**, **CLINE**, **Claude Code**, featuring a specially designed function call format;

üìö **Long-context Capabilities**: with native support for **256K** tokens, extendable up to **1M** tokens using Yarn, optimized for repository-scale understanding.

---


## Basic Information

1. ‚ú® Supporting long context understanding and generation with the context length of 256K tokens;
2. ‚ú® Supporting 358 coding languages;

&lt;details&gt;
&lt;summary&gt;Click to view all supported languages&lt;/summary&gt;
```
[&#039;ABAP&#039;, &#039;ActionScript&#039;, &#039;Ada&#039;, &#039;Agda&#039;, &#039;Alloy&#039;, &#039;ApacheConf&#039;, &#039;AppleScript&#039;, &#039;Arc&#039;, &#039;Arduino&#039;, &#039;AsciiDoc&#039;, &#039;AspectJ&#039;, &#039;Assembly&#039;, &#039;Augeas&#039;, &#039;AutoHotkey&#039;, &#039;AutoIt&#039;, &#039;Awk&#039;, &#039;Batchfile&#039;, &#039;Befunge&#039;, &#039;Bison&#039;, &#039;BitBake&#039;, &#039;BlitzBasic&#039;, &#039;BlitzMax&#039;, &#039;Bluespec&#039;, &#039;Boo&#039;, &#039;Brainfuck&#039;, &#039;Brightscript&#039;, &#039;Bro&#039;, &#039;C&#039;, &#039;C#&#039;, &#039;C++&#039;, &#039;C2hs Haskell&#039;, &#039;CLIPS&#039;, &#039;CMake&#039;, &#039;COBOL&#039;, &#039;CSS&#039;, &#039;CSV&#039;, &quot;Cap&#039;n Proto&quot;, &#039;CartoCSS&#039;, &#039;Ceylon&#039;, &#039;Chapel&#039;, &#039;ChucK&#039;, &#039;Cirru&#039;, &#039;Clarion&#039;, &#039;Clean&#039;, &#039;Click&#039;, &#039;Clojure&#039;, &#039;CoffeeScript&#039;, &#039;ColdFusion&#039;, &#039;ColdFusion CFC&#039;, &#039;Common Lisp&#039;, &#039;Component Pascal&#039;, &#039;Coq&#039;, &#039;Creole&#039;, &#039;Crystal&#039;, &#039;Csound&#039;, &#039;Cucumber&#039;, &#039;Cuda&#039;, &#039;Cycript&#039;, &#039;Cython&#039;, &#039;D&#039;, &#039;DIGITAL Command Language&#039;, &#039;DM&#039;, &#039;DNS Zone&#039;, &#039;Darcs Patch&#039;, &#039;Dart&#039;, &#039;Diff&#039;, &#039;Dockerfile&#039;, &#039;Dogescript&#039;, &#039;Dylan&#039;, &#039;E&#039;, &#039;ECL&#039;, &#039;Eagle&#039;, &#039;Ecere Projects&#039;, &#039;Eiffel&#039;, &#039;Elixir&#039;, &#039;Elm&#039;, &#039;Emacs Lisp&#039;, &#039;EmberScript&#039;, &#039;Erlang&#039;, &#039;F#&#039;, &#039;FLUX&#039;, &#039;FORTRAN&#039;, &#039;Factor&#039;, &#039;Fancy&#039;, &#039;Fantom&#039;, &#039;Forth&#039;, &#039;FreeMarker&#039;, &#039;G-code&#039;, &#039;GAMS&#039;, &#039;GAP&#039;, &#039;GAS&#039;, &#039;GDScript&#039;, &#039;GLSL&#039;, &#039;Genshi&#039;, &#039;Gentoo Ebuild&#039;, &#039;Gentoo Eclass&#039;, &#039;Gettext Catalog&#039;, &#039;Glyph&#039;, &#039;Gnuplot&#039;, &#039;Go&#039;, &#039;Golo&#039;, &#039;Gosu&#039;, &#039;Grace&#039;, &#039;Gradle&#039;, &#039;Grammatical Framework&#039;, &#039;GraphQL&#039;, &#039;Graphviz (DOT)&#039;, &#039;Groff&#039;, &#039;Groovy&#039;, &#039;Groovy Server Pages&#039;, &#039;HCL&#039;, &#039;HLSL&#039;, &#039;HTML&#039;, &#039;HTML+Django&#039;, &#039;HTML+EEX&#039;, &#039;HTML+ERB&#039;, &#039;HTML+PHP&#039;, &#039;HTTP&#039;, &#039;Haml&#039;, &#039;Handlebars&#039;, &#039;Harbour&#039;, &#039;Haskell&#039;, &#039;Haxe&#039;, &#039;Hy&#039;, &#039;IDL&#039;, &#039;IGOR Pro&#039;, &#039;INI&#039;, &#039;IRC log&#039;, &#039;Idris&#039;, &#039;Inform 7&#039;, &#039;Inno Setup&#039;, &#039;Io&#039;, &#039;Ioke&#039;, &#039;Isabelle&#039;, &#039;J&#039;, &#039;JFlex&#039;, &#039;JSON&#039;, &#039;JSON5&#039;, &#039;JSONLD&#039;, &#039;JSONiq&#039;, &#039;JSX&#039;, &#039;Jade&#039;, &#039;Jasmin&#039;, &#039;Java&#039;, &#039;Java Server Pages&#039;, &#039;JavaScript&#039;, &#039;Julia&#039;, &#039;Jupyter Notebook&#039;, &#039;KRL&#039;, &#039;KiCad&#039;, &#039;Kit&#039;, &#039;Kotlin&#039;, &#039;LFE&#039;, &#039;LLVM&#039;, &#039;LOLCODE&#039;, &#039;LSL&#039;, &#039;LabVIEW&#039;, &#039;Lasso&#039;, &#039;Latte&#039;, &#039;Lean&#039;, &#039;Less&#039;, &#039;Lex&#039;, &#039;LilyPond&#039;, &#039;Linker Script&#039;, &#039;Liquid&#039;, &#039;Literate Agda&#039;, &#039;Literate CoffeeScript&#039;, &#039;Literate Haskell&#039;, &#039;LiveScript&#039;, &#039;Logos&#039;, &#039;Logtalk&#039;, &#039;LookML&#039;, &#039;Lua&#039;, &#039;M&#039;, &#039;M4&#039;, &#039;MAXScript&#039;, &#039;MTML&#039;, &#039;MUF&#039;, &#039;Makefile&#039;, &#039;Mako&#039;, &#039;Maple&#039;, &#039;Markdown&#039;, &#039;Mask&#039;, &#039;Mathematica&#039;, &#039;Matlab&#039;, &#039;Max&#039;, &#039;MediaWiki&#039;, &#039;Metal&#039;, &#039;MiniD&#039;, &#039;Mirah&#039;, &#039;Modelica&#039;, &#039;Module Management System&#039;, &#039;Monkey&#039;, &#039;MoonScript&#039;, &#039;Myghty&#039;, &#039;NSIS&#039;, &#039;NetLinx&#039;, &#039;NetLogo&#039;, &#039;Nginx&#039;, &#039;Nimrod&#039;, &#039;Ninja&#039;, &#039;Nit&#039;, &#039;Nix&#039;, &#039;Nu&#039;, &#039;NumPy&#039;, &#039;OCaml&#039;, &#039;ObjDump&#039;, &#039;Objective-C++&#039;, &#039;Objective-J&#039;, &#039;Octave&#039;, &#039;Omgrofl&#039;, &#039;Opa&#039;, &#039;Opal&#039;, &#039;OpenCL&#039;, &#039;OpenEdge ABL&#039;, &#039;OpenSCAD&#039;, &#039;Org&#039;, &#039;Ox&#039;, &#039;Oxygene&#039;, &#039;Oz&#039;, &#039;PAWN&#039;, &#039;PHP&#039;, &#039;POV-Ray SDL&#039;, &#039;Pan&#039;, &#039;Papyrus&#039;, &#039;Parrot&#039;, &#039;Parrot Assembly&#039;, &#039;Parrot Internal Representation&#039;, &#039;Pascal&#039;, &#039;Perl&#039;, &#039;Perl6&#039;, &#039;Pickle&#039;, &#039;PigLatin&#039;, &#039;Pike&#039;, &#039;Pod&#039;, &#039;PogoScript&#039;, &#039;Pony&#039;, &#039;PostScript&#039;, &#039;PowerShell&#039;, &#039;Processing&#039;, &#039;Prolog&#039;, &#039;Propeller Spin&#039;, &#039;Protocol Buffer&#039;, &#039;Public Key&#039;, &#039;Pure Data&#039;, &#039;PureBasic&#039;, &#039;PureScript&#039;, &#039;Python&#039;, &#039;Python traceback&#039;, &#039;QML&#039;, &#039;QMake&#039;, &#039;R&#039;, &#039;RAML&#039;, &#039;RDoc&#039;, &#039;REALbasic&#039;, &#039;RHTML&#039;, &#039;RMarkdown&#039;, &#039;Racket&#039;, &#039;Ragel in Ruby Host&#039;, &#039;Raw token data&#039;, &#039;Rebol&#039;, &#039;Red&#039;, &#039;Redcode&#039;, &quot;Ren&#039;Py&quot;, &#039;RenderScript&#039;, &#039;RobotFramework&#039;, &#039;Rouge&#039;, &#039;Ruby&#039;, &#039;Rust&#039;, &#039;SAS&#039;, &#039;SCSS&#039;, &#039;SMT&#039;, &#039;SPARQL&#039;, &#039;SQF&#039;, &#039;SQL&#039;, &#039;STON&#039;, &#039;SVG&#039;, &#039;Sage&#039;, &#039;SaltStack&#039;, &#039;Sass&#039;, &#039;Scala&#039;, &#039;Scaml&#039;, &#039;Scheme&#039;, &#039;Scilab&#039;, &#039;Self&#039;, &#039;Shell&#039;, &#039;ShellSession&#039;, &#039;Shen&#039;, &#039;Slash&#039;, &#039;Slim&#039;, &#039;Smali&#039;, &#039;Smalltalk&#039;, &#039;Smarty&#039;, &#039;Solidity&#039;, &#039;SourcePawn&#039;, &#039;Squirrel&#039;, &#039;Stan&#039;, &#039;Standard ML&#039;, &#039;Stata&#039;, &#039;Stylus&#039;, &#039;SuperCollider&#039;, &#039;Swift&#039;, &#039;SystemVerilog&#039;, &#039;TOML&#039;, &#039;TXL&#039;, &#039;Tcl&#039;, &#039;Tcsh&#039;, &#039;TeX&#039;, &#039;Tea&#039;, &#039;Text&#039;, &#039;Textile&#039;, &#039;Thrift&#039;, &#039;Turing&#039;, &#039;Turtle&#039;, &#039;Twig&#039;, &#039;TypeScript&#039;, &#039;Unified Parallel C&#039;, &#039;Unity3D Asset&#039;, &#039;Uno&#039;, &#039;UnrealScript&#039;, &#039;UrWeb&#039;, &#039;VCL&#039;, &#039;VHDL&#039;, &#039;Vala&#039;, &#039;Verilog&#039;, &#039;VimL&#039;, &#039;Visual Basic&#039;, &#039;Volt&#039;, &#039;Vue&#039;, &#039;Web Ontology Language&#039;, &#039;WebAssembly&#039;, &#039;WebIDL&#039;, &#039;X10&#039;, &#039;XC&#039;, &#039;XML&#039;, &#039;XPages&#039;, &#039;XProc&#039;, &#039;XQuery&#039;, &#039;XS&#039;, &#039;XSLT&#039;, &#039;Xojo&#039;, &#039;Xtend&#039;, &#039;YAML&#039;, &#039;YANG&#039;, &#039;Yacc&#039;, &#039;Zephir&#039;, &#039;Zig&#039;, &#039;Zimpl&#039;, &#039;desktop&#039;, &#039;eC&#039;, &#039;edn&#039;, &#039;fish&#039;, &#039;mupad&#039;, &#039;nesC&#039;, &#039;ooc&#039;, &#039;reStructuredText&#039;, &#039;wisp&#039;, &#039;xBase&#039;]
```
&lt;/details&gt;

3. ‚ú® Retain strengths in math and general capabilities from base model.

&gt; [!Important]
&gt; 
&gt; Qwen3-Coder function calling relies on our new tool parser in both **SGLang** and **vLLM** &lt;a href=&quot;https://huggingface.co/Qwen/Qwen3-Coder-Next/blob/main/&quot;&gt;here&lt;/a&gt;.
&gt;
&gt; We updated both the special tokens and their corresponding token ids, in order to maintain consistency with Qwen3. Please make sure to use the new tokenizer.


| model name                  | type     | length | Download                                                                                                                                                                        |
|-----------------------------|----------|--------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Qwen3-Coder-Next         | instruct     | 256k    | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen3-Coder-Next  ) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen3-Coder-Next)                                       |
| Qwen3-Coder-Next-Base         | base     | 256k    | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen3-Coder-Next-Base) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen3-Coder-Next-Base)     |
| Qwen3-Coder-480B-A35B-Instruct         | instruct     | 256k    | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct  ) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen3-Coder-480B-A35B-Instruct)                                       |
| Qwen3-Coder-30B-A3B-Instruct         | instruct     | 256k    | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen3-Coder-30B-A3B-Instruct)                                       |
| Qwen3-Coder-Next-FP8         | instruct     | 256k    | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen3-Coder-Next-FP8  ) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen3-Coder-Next-FP8)
| Qwen3-Coder-Next-GGUF         | instruct     | 256k    | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen3-Coder-Next-GGUF  ) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen3-Coder-Next-GGUF)                                       |
| Qwen3-Coder-480B-A35B-Instruct-FP8         | instruct     | 256k    | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8)                                       |
| Qwen3-Coder-30B-A3B-Instruct-FP8         | instruct     | 256k    | ü§ó [Hugging Face](https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8) ‚Ä¢ ü§ñ [ModelScope](https://modelscope.cn/models/Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8)                                       |


Detailed performance and introduction are shown in this &lt;a href=&quot;https://qwenlm.github.io/blog/qwen3-coder-next/&quot;&gt;üìë blog&lt;/a&gt;.

---

## Quick Start

&gt; [!Important]
&gt; **Qwen3-Coder** are instruct models for chatting;
&gt;
&gt; This model supports only non-thinking mode and does not generate `&lt;think&gt;&lt;/think&gt;` blocks in its output. Meanwhile, specifying `enable_thinking=False` is no longer required.
&gt;
### üëâüèª Chat with Qwen3-Coder
You can write several lines of code with `transformers` to chat with Qwen3-Coder-Next. Essentially, we build the tokenizer and the model with the `from_pretrained` method, and we use the generate method to perform chatting with the help of the chat template provided by the tokenizer. Below is an example of how to chat with **Qwen3-Coder-Next**:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = &quot;Qwen/Qwen3-Coder-Next&quot;

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=&quot;auto&quot;,
    device_map=&quot;auto&quot;
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = &quot;write a quick sort algorithm.&quot;
messages = [
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors=&quot;pt&quot;).to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=65536
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
```
The `apply_chat_template()` function is used to convert the messages into a format that the model can understand.
The `add_generation_prompt` argument is used to add a generation prompt, which refers to `&lt;|im_start|&gt;assistant\n` to the input. Notably, we apply the ChatML template for chat models following our previous practice.
The `max_new_tokens` argument is used to set the maximum length of the response. The `tokenizer.batch_decode()` function is used to decode the response. In terms of the input, the above messages are an example to show how to format your dialog history and system prompt.
You can use the other sizes of instruct models in the same way.


#### Fill in the middle with Qwen3-Coder

The code insertion task, also referred to as the &quot;fill-in-the-middle&quot; challenge, requires the insertion of code segments in a manner that bridges the gaps within a given code context. For an approach aligned with best practices, we recommend adhering to the formatting guidelines outlined in the paper &quot;Efficient Training of Language Models to Fill in the Middle&quot; [[arxiv](https://arxiv.org/abs/2207.14255)]. 

&gt; [!Important]
&gt; It should be noted that FIM is supported in every version of Qwen3-Coder. Qwen3-Coder-Next is shown here as an example.
&gt;

The prompt should be structured as follows:
```python
prompt = &#039;&lt;|fim_prefix|&gt;&#039; + prefix_code + &#039;&lt;|fim_suffix|&gt;&#039; + suffix_code + &#039;&lt;|fim_middle|&gt;&#039;
```
Following the approach mentioned, an example would be structured in this manner:

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
# load model
device = &quot;cuda&quot; # the device to load the model onto

TOKENIZER = AutoTokenizer.from_pretrained(&quot;Qwen/Qwen3-Coder-Next&quot;)
MODEL = AutoModelForCausalLM.from_pretrained(&quot;Qwen/Qwen3-Coder-Next&quot;, device_map=&quot;auto&quot;).eval()


input_text = &quot;&quot;&quot;&lt;|fim_prefix|&gt;def quicksort(arr):
    if len(arr) &lt;= 1:
        return arr
    pivot = arr[len(arr) // 2]
    &lt;|fim_suffix|&gt;
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x &gt; pivot]
    return quicksort(left) + middle + quicksort(right)&lt;|fim_middle|&gt;&quot;&quot;&quot;
            
messages = [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a code completion assistant.&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: input_text}
]


text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = TOKENIZER([text], return_tensors=&quot;pt&quot;).to(model.device)

# Use `max_new_tokens` to control the maximum output length.
eos_token_ids = [151659, 151661, 151662, 151663, 151664, 151643, 151645]
generated_ids = MODEL.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=False, eos_token_id=eos_token_ids)[0]
# The generated_ids include prompt_ids, we only need to decode the tokens after prompt_ids.
output_text = TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens=True)

print(f&quot;Prompt: {input_text}\n\nGenerated text: {output_text}&quot;)
```

## Use Cases

### Example: Releasing a Website

&lt;details&gt;
&lt;summary&gt;Prompt with OpenClaw &lt;/summary&gt;

```
next week we will release new coder model, can you collect the history of qwen coder and write a web page, the release the website with the nginx, you can seach how to do this in alibaba cloud linux first
```

&lt;/details&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen3-Coder-Next/openclaw/claw_mix.mp4&quot;&gt;
    &lt;img src=&quot;assets/qwen3-coder-next-demo/openclaw.png&quot; width=&quot;400&quot; /&gt;
    &lt;/a&gt;
&lt;/p&gt;


### Example: Desktop Tidy

&lt;details&gt;
&lt;summary&gt;Prompt with Qwen Code &lt;/summary&gt;

```
Please tidy up my desk.
```
&lt;/details&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen3-Coder-Next/qwencode/exp-tidy-desktop.mp4&quot;&gt;
    &lt;img src=&quot;assets/qwen3-coder-next-demo/tidy_desktop.png&quot; width=&quot;400&quot; /&gt;
    &lt;/a&gt;
&lt;/p&gt;

### Example: Zombies vs. Plants

&lt;details&gt;
&lt;summary&gt;Prompt with Claude Code &lt;/summary&gt;

```
Â∏ÆÊàëÂÆûÁé∞„ÄäÂÉµÂ∞∏Â§ßÊàòÊ§çÁâ©„ÄãÁΩëÈ°µÊ∏∏Êàè

„ÄêÊ†∏ÂøÉÊú∫Âà∂„Äë
- ÂèçÂêëÂ°îÈò≤ÔºöÁé©ÂÆ∂ÊâÆÊºîÂÉµÂ∞∏ÊñπÔºå‰ªéÂú∞ÂõæÂè≥‰æßÔºàÈÉ®ÁΩ≤Âå∫ÔºâÂè¨Âî§ÂÉµÂ∞∏ÂêëÂ∑¶ËøõÊîª
- ËµÑÊ∫êÂæ™ÁéØÔºöÂàùÂßã300ËÑëÂ≠êÁÇπÊï∞ÔºåÂÉµÂ∞∏ÂêÉÊéâÊ§çÁâ©ËøîËøò100ÁÇπÔºåÂΩ¢ÊàêÁªèÊµéÂæ™ÁéØ
- ÂÄíËÆ°Êó∂Âà∂Ôºö120ÁßíÂÜÖÊ∏ÖÈô§ÊâÄÊúâÊ§çÁâ©Ëé∑ËÉúÔºåË∂ÖÊó∂Â§±Ë¥•

„ÄêÂú∞ÂõæËßÑÊ†º„Äë
- 5Ë°å9ÂàóÁΩëÊ†ºÔºåÂè≥‰æß3Âàó‰∏∫ÂèØÈÉ®ÁΩ≤Âå∫ÂüüÔºàÁ∫¢Ëâ≤È´ò‰∫ÆÊ†áËØÜÔºâ
- ÊØèÊ†º100x100ÂÉèÁ¥†ÔºåËçâÂú∞Á∫πÁêÜ‰∫§ÊõøÊ∏≤Êüì
- ÂùêÊ†áÁ≥ªÔºöÂ∑¶‰æß‰∏∫Ê§çÁâ©Èò≤Á∫øÔºåÂè≥‰æß‰∏∫ÂÉµÂ∞∏Âá∫ÁîüÁÇπ

„ÄêÂçï‰ΩçÁ≥ªÁªü„Äë
ÂÉµÂ∞∏ÊñπÔºàÂè≥‰æßË¥≠‰π∞ÔºâÔºö
- ÊôÆÈÄöÂÉµÂ∞∏Ôºö50ËÑëÔºå100HPÔºå0.5ÈÄüÔºåÊ†áÂáÜÂçï‰Ωç
- Ë∑ØÈöúÂÉµÂ∞∏Ôºö100ËÑëÔºå200HPÔºå0.5ÈÄüÔºå‰∏≠ÊúüËÇâÁõæ  
- ÈìÅÊ°∂ÂÉµÂ∞∏Ôºö150ËÑëÔºå400HPÔºå0.3ÈÄüÔºåÈáçÂûãÂù¶ÂÖã
- ÂÜ≤Âà∫ÂÉµÂ∞∏Ôºö80ËÑëÔºå80HPÔºå1.2ÈÄüÔºåÂø´ÈÄüÁ™ÅËøõ

Ê§çÁâ©ÊñπÔºàÂ∑¶‰æßÈöèÊú∫ÂàùÂßãÈÉ®ÁΩ≤12‰∏™ÔºâÔºö
- Ë±åË±ÜÂ∞ÑÊâãÔºö100HPÔºå20‰º§/ÂèëÔºå2ÁßíÈó¥ÈöîÔºåÁõ¥Á∫øÂ∞ÑÂáª
- ÂèåÂèëÂ∞ÑÊâãÔºö120HPÔºå20‰º§/ÂèëÔºå1ÁßíÈó¥ÈöîÔºåÁÅ´ÂäõÂéãÂà∂
- ÂùöÊûúÂ¢ôÔºö300HPÔºå0‰º§ÔºåÁ∫ØËÇâÁõæÈòªÊå°
- ÂêëÊó•ËëµÔºö80HPÔºå0‰º§ÔºåÁªèÊµéÂçï‰ΩçÔºàÁ∫ØÂπ≤Êâ∞Ôºâ

„ÄêÊàòÊñóÈÄªËæë„Äë
- Á¢∞ÊíûÊ£ÄÊµãÔºöÂÉµÂ∞∏Âà∞ËææÊ§çÁâ©50pxÂÜÖËß¶ÂèëÂïÉÈ£üÁä∂ÊÄÅÔºåÂÅúÊ≠¢ÁßªÂä®
- ‰º§ÂÆ≥ÁªìÁÆóÔºöÂÉµÂ∞∏30Â∏ß/Ê¨°Âí¨ÂáªÔºà0.5ÁßíÔºâÔºåÊ§çÁâ©Â∞ÑÂá∫ÂºπÈÅìÁâ©ÁêÜ
- ÂáªÊØÅÂèçÈ¶àÔºöÊ§çÁâ©Ê≠ª‰∫°Êó∂ÁîüÊàê&quot;+100&quot;È£òÂ≠óÁâπÊïà‰∏éÁ≤íÂ≠êÁàÜÁÇ∏
- Ë∑ØÂæÑAIÔºöÂêåÊ†ºÂÉµÂ∞∏ÈòüÂàó‰∏çÈáçÂè†ÔºåÊ§çÁâ©‰ºòÂÖàÊîªÂáªÊ®™ÂêëÊúÄËøëÁõÆÊ†á

„Äê‰∫§‰∫íËÆæËÆ°„Äë
- Âè≥‰æßÂç°ÁâáÂºèUIÔºöÊòæÁ§∫ÂÉµÂ∞∏ÂõæÊ†á„ÄÅÂêçÁß∞„ÄÅËÑëÂ≠êÊ∂àËÄó
- ËµÑÊ∫ê‰∏çË∂≥Êó∂Âç°ÁâáÁΩÆÁÅ∞Âπ∂Ëá™Âä®ÂàáÊç¢ÂèØÈÄâÁ±ªÂûã
- Èº†Ê†áÊÇ¨ÂÅúÈÉ®ÁΩ≤Âå∫ÊòæÁ§∫ÂçäÈÄèÊòéÈ¢ÑËßàÂúà
- ÂÆûÊó∂Ë°ÄÊù°ÔºöÂÆû‰ΩìÂ§¥È°∂ÊòæÁ§∫Áªø/ÈªÑ/Á∫¢‰∏âËâ≤Ë°ÄÊßΩ

„ÄêËÉúÂà©Êù°‰ª∂„Äë
- ËÉúÂà©Ôºöplants.length === 0 &amp;&amp; timeLeft &gt; 0
- Â§±Ë¥•ÔºötimeLeft === 0 || (ÂèØÈÄâ)ÂÉµÂ∞∏ÂÖ®ÁÅ≠‰∏îËÑëÂ≠ê‰∏∫0
```

&lt;/details&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen3-Coder-Next/claudecode/cc_zombine_vs_plants.mp4&quot;&gt;
    &lt;img src=&quot;assets/qwen3-coder-next-demo/zombiesvsplants.png&quot; width=&quot;400&quot; /&gt;
    &lt;/a&gt;
&lt;/p&gt;

### Example: Sound ASCII Art

&lt;details&gt;
&lt;summary&gt;Prompt with Cline &lt;/summary&gt;

```
Build an interactive ASCII art drawing tool with sound feedback. The application should:
 
1. Create a canvas where users can draw by clicking and dragging
2. Place different ASCII characters or symbols when the user draws
3. Play corresponding musical notes when each character is placed
4. Include multiple pattern sets with different characters and
corresponding note scales
5. Add a pattern switcher button to cycle through different
character/sound themes
6. Include a clear button to reset the canvas
7. Support both mouse and touch input for mobile compatibility
 
The application should be creative and fun to use, creating an audio-visual experience where patterns of characters create both visual art and musical patterns. Ensure the musical notes are harmonious when played in sequence.
```

&lt;/details&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen3-Coder-Next/cline/sound_art.mp4&quot;&gt;
    &lt;img src=&quot;assets/qwen3-coder-next-demo/sound_art.png&quot; width=&quot;400&quot; /&gt;
    &lt;/a&gt;
&lt;/p&gt;

### Example: Vibe Checking


&lt;details&gt;
&lt;summary&gt; Prompt with Browser Use Agent &lt;/summary&gt;

```
Vibe test this website. Click around, try things, report what&#039;s broken.
```
&lt;/details&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen3-Coder-Next/bua/vibe.mp4&quot;&gt;
    &lt;img src=&quot;assets/qwen3-coder-next-demo/vibing_check.png&quot; width=&quot;400&quot; /&gt;
    &lt;/a&gt;
&lt;/p&gt;

### Example: Parkour Game


&lt;details&gt;
&lt;summary&gt; Prompt with Qwen Chat Web Dev &lt;/summary&gt;

```
Create an interactive real-time particle system using HTML5 Canvas:

Core Features:
- Render 800-1200 animated particles with physics-based movement
- Mouse cursor exerts attractive/repulsive force on nearby particles
- Click to toggle between attraction and repulsion modes
- Particles respond with smooth acceleration and velocity calculations

Technical Requirements:
- Use requestAnimationFrame for optimal performance
- Implement force calculation based on distance from cursor
- Add visual feedback: particle glow, color variation, and fade effects
- Include performance monitoring (FPS counter)

Deliverables:
- Single HTML file with embedded CSS and JavaScript
- Clean, commented code following best practices
- Responsive design compatible with modern browsers
```
&lt;/details&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen3-Coder-Next/WebDev/chico_paredao.mp4&quot;&gt;
    &lt;img src=&quot;assets/qwen3-coder-next-demo/parkourgame.png&quot; width=&quot;400&quot; /&gt;
    &lt;/a&gt;
&lt;/p&gt;


---

## Star History
[![Star History Chart](https://api.star-history.com/svg?repos=QwenLM/Qwen3-Coder&amp;type=Date)](https://star-history.com/#QwenLM/Qwen3-Coder&amp;Date)

---

## Citation

If you find our work helpful, feel free to give us a cite.

```bibtex
@techreport{qwen_qwen3_coder_next_tech_report,
  title

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[confident-ai/deepeval]]></title>
            <link>https://github.com/confident-ai/deepeval</link>
            <guid>https://github.com/confident-ai/deepeval</guid>
            <pubDate>Thu, 05 Feb 2026 00:07:18 GMT</pubDate>
            <description><![CDATA[The LLM Evaluation Framework]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/confident-ai/deepeval">confident-ai/deepeval</a></h1>
            <p>The LLM Evaluation Framework</p>
            <p>Language: Python</p>
            <p>Stars: 13,423</p>
            <p>Forks: 1,206</p>
            <p>Stars today: 109 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/confident-ai/deepeval/blob/main/docs/static/img/deepeval.png&quot; alt=&quot;DeepEval Logo&quot; width=&quot;100%&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;h1 align=&quot;center&quot;&gt;The LLM Evaluation Framework&lt;/h1&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/5917&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/5917&quot; alt=&quot;confident-ai%2Fdeepeval | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://discord.gg/3SEyvpgu2f&quot;&gt;
        &lt;img alt=&quot;discord-invite&quot; src=&quot;https://dcbadge.vercel.app/api/server/3SEyvpgu2f?style=flat&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;h4 align=&quot;center&quot;&gt;
    &lt;p&gt;
        &lt;a href=&quot;https://deepeval.com/docs/getting-started?utm_source=GitHub&quot;&gt;Documentation&lt;/a&gt; |
        &lt;a href=&quot;#-metrics-and-features&quot;&gt;Metrics and Features&lt;/a&gt; |
        &lt;a href=&quot;#-quickstart&quot;&gt;Getting Started&lt;/a&gt; |
        &lt;a href=&quot;#-integrations&quot;&gt;Integrations&lt;/a&gt; |
        &lt;a href=&quot;https://confident-ai.com?utm_source=GitHub&quot;&gt;DeepEval Platform&lt;/a&gt;
    &lt;p&gt;
&lt;/h4&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://github.com/confident-ai/deepeval/releases&quot;&gt;
        &lt;img alt=&quot;GitHub release&quot; src=&quot;https://img.shields.io/github/release/confident-ai/deepeval.svg?color=violet&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://colab.research.google.com/drive/1PPxYEBa6eu__LquGoFFJZkhYgWVYE6kh?usp=sharing&quot;&gt;
        &lt;img alt=&quot;Try Quickstart in Colab&quot; src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/confident-ai/deepeval/blob/master/LICENSE.md&quot;&gt;
        &lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/github/license/confident-ai/deepeval.svg?color=yellow&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://x.com/deepeval&quot;&gt;
        &lt;img alt=&quot;Twitter Follow&quot; src=&quot;https://img.shields.io/twitter/follow/deepeval?style=social&amp;logo=x&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=de&quot;&gt;Deutsch&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=es&quot;&gt;Espa√±ol&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=fr&quot;&gt;fran√ßais&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=ja&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=ko&quot;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=pt&quot;&gt;Portugu√™s&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=ru&quot;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=zh&quot;&gt;‰∏≠Êñá&lt;/a&gt;
&lt;/p&gt;

**DeepEval** is a simple-to-use, open-source LLM evaluation framework, for evaluating and testing large-language model systems. It is similar to Pytest but specialized for unit testing LLM outputs. DeepEval incorporates the latest research to evaluate LLM outputs based on metrics such as G-Eval, task completion, answer relevancy, hallucination, etc., which uses LLM-as-a-judge and other NLP models that run **locally on your machine** for evaluation.

Whether your LLM applications are AI agents, RAG pipelines, or chatbots, implemented via LangChain or OpenAI, DeepEval has you covered. With it, you can easily determine the optimal models, prompts, and architecture to improve your RAG pipeline, agentic workflows, prevent prompt drifting, or even transition from OpenAI to hosting your own Deepseek R1 with confidence.

&gt; [!IMPORTANT]
&gt; Need a place for your DeepEval testing data to live üè°‚ù§Ô∏è? [Sign up to the DeepEval platform](https://confident-ai.com?utm_source=GitHub) to compare iterations of your LLM app, generate &amp; share testing reports, and more.
&gt;
&gt; ![Demo GIF](assets/demo.gif)

&gt; Want to talk LLM evaluation, need help picking metrics, or just to say hi? [Come join our discord.](https://discord.com/invite/3SEyvpgu2f)

&lt;br /&gt;

# üî• Metrics and Features

&gt; ü•≥ You can now share DeepEval&#039;s test results on the cloud directly on [Confident AI](https://confident-ai.com?utm_source=GitHub)

- Supports both end-to-end and component-level LLM evaluation.
- Large variety of ready-to-use LLM evaluation metrics (all with explanations) powered by **ANY** LLM of your choice, statistical methods, or NLP models that run **locally on your machine**:
  - G-Eval
  - DAG ([deep acyclic graph](https://deepeval.com/docs/metrics-dag))
  - **RAG metrics:**
    - Answer Relevancy
    - Faithfulness
    - Contextual Recall
    - Contextual Precision
    - Contextual Relevancy
    - RAGAS
  - **Agentic metrics:**
    - Task Completion
    - Tool Correctness
  - **Others:**
    - Hallucination
    - Summarization
    - Bias
    - Toxicity
  - **Conversational metrics:**
    - Knowledge Retention
    - Conversation Completeness
    - Conversation Relevancy
    - Role Adherence
  - etc.
- Build your own custom metrics that are automatically integrated with DeepEval&#039;s ecosystem.
- Generate synthetic datasets for evaluation.
- Integrates seamlessly with **ANY** CI/CD environment.
- [Red team your LLM application](https://deepeval.com/docs/red-teaming-introduction) for 40+ safety vulnerabilities in a few lines of code, including:
  - Toxicity
  - Bias
  - SQL Injection
  - etc., using advanced 10+ attack enhancement strategies such as prompt injections.
- Easily benchmark **ANY** LLM on popular LLM benchmarks in [under 10 lines of code.](https://deepeval.com/docs/benchmarks-introduction?utm_source=GitHub), which includes:
  - MMLU
  - HellaSwag
  - DROP
  - BIG-Bench Hard
  - TruthfulQA
  - HumanEval
  - GSM8K
- [100% integrated with Confident AI](https://confident-ai.com?utm_source=GitHub) for the full evaluation &amp; observability lifecycle:
  - Curate/annotate evaluation datasets on the cloud
  - Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best
  - Fine-tune metrics for custom results
  - Debug evaluation results via LLM traces
  - Monitor &amp; evaluate LLM responses in product to improve datasets with real-world data
  - Repeat until perfection

&gt; [!NOTE]
&gt; DeepEval is available on Confident AI, an LLM evals platform for AI observability and quality. Create an account [here.](https://app.confident-ai.com?utm_source=GitHub)

&lt;br /&gt;

# üîå Integrations

- ü¶Ñ LlamaIndex, to [**unit test RAG applications in CI/CD**](https://www.deepeval.com/integrations/frameworks/llamaindex?utm_source=GitHub)
- ü§ó Hugging Face, to [**enable real-time evaluations during LLM fine-tuning**](https://www.deepeval.com/integrations/frameworks/huggingface?utm_source=GitHub)

&lt;br /&gt;

# üöÄ QuickStart

Let&#039;s pretend your LLM application is a RAG based customer support chatbot; here&#039;s how DeepEval can help test what you&#039;ve built.

## Installation

Deepeval works with **Python&gt;=3.9+**.

```
pip install -U deepeval
```

## Create an account (highly recommended)

Using the `deepeval` platform will allow you to generate sharable testing reports on the cloud. It is free, takes no additional code to setup, and we highly recommend giving it a try.

To login, run:

```
deepeval login
```

Follow the instructions in the CLI to create an account, copy your API key, and paste it into the CLI. All test cases will automatically be logged (find more information on data privacy [here](https://deepeval.com/docs/data-privacy?utm_source=GitHub)).

## Writing your first test case

Create a test file:

```bash
touch test_chatbot.py
```

Open `test_chatbot.py` and write your first test case to run an **end-to-end** evaluation using DeepEval, which treats your LLM app as a black-box:

```python
import pytest
from deepeval import assert_test
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams

def test_case():
    correctness_metric = GEval(
        name=&quot;Correctness&quot;,
        criteria=&quot;Determine if the &#039;actual output&#039; is correct based on the &#039;expected output&#039;.&quot;,
        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],
        threshold=0.5
    )
    test_case = LLMTestCase(
        input=&quot;What if these shoes don&#039;t fit?&quot;,
        # Replace this with the actual output from your LLM application
        actual_output=&quot;You have 30 days to get a full refund at no extra cost.&quot;,
        expected_output=&quot;We offer a 30-day full refund at no extra costs.&quot;,
        retrieval_context=[&quot;All customers are eligible for a 30 day full refund at no extra costs.&quot;]
    )
    assert_test(test_case, [correctness_metric])
```

Set your `OPENAI_API_KEY` as an environment variable (you can also evaluate using your own custom model, for more details visit [this part of our docs](https://deepeval.com/docs/metrics-introduction#using-a-custom-llm?utm_source=GitHub)):

```
export OPENAI_API_KEY=&quot;...&quot;
```

And finally, run `test_chatbot.py` in the CLI:

```
deepeval test run test_chatbot.py
```

**Congratulations! Your test case should have passed ‚úÖ** Let&#039;s breakdown what happened.

- The variable `input` mimics a user input, and `actual_output` is a placeholder for what your application&#039;s supposed to output based on this input.
- The variable `expected_output` represents the ideal answer for a given `input`, and [`GEval`](https://deepeval.com/docs/metrics-llm-evals) is a research-backed metric provided by `deepeval` for you to evaluate your LLM output&#039;s on any custom with human-like accuracy.
- In this example, the metric `criteria` is correctness of the `actual_output` based on the provided `expected_output`.
- All metric scores range from 0 - 1, which the `threshold=0.5` threshold ultimately determines if your test have passed or not.

[Read our documentation](https://deepeval.com/docs/getting-started?utm_source=GitHub) for more information on more options to run end-to-end evaluation, how to use additional metrics, create your own custom metrics, and tutorials on how to integrate with other tools like LangChain and LlamaIndex.

&lt;br /&gt;

## Evaluating Nested Components

If you wish to evaluate individual components within your LLM app, you need to run **component-level** evals - a powerful way to evaluate any component within an LLM system.

Simply trace &quot;components&quot; such as LLM calls, retrievers, tool calls, and agents within your LLM application using the `@observe` decorator to apply metrics on a component-level. Tracing with `deepeval` is non-instrusive (learn more [here](https://deepeval.com/docs/evaluation-llm-tracing#dont-be-worried-about-tracing)) and helps you avoid rewriting your codebase just for evals:

```python
from deepeval.tracing import observe, update_current_span
from deepeval.test_case import LLMTestCase
from deepeval.dataset import Golden
from deepeval.metrics import GEval
from deepeval import evaluate

correctness = GEval(name=&quot;Correctness&quot;, criteria=&quot;Determine if the &#039;actual output&#039; is correct based on the &#039;expected output&#039;.&quot;, evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT])

@observe(metrics=[correctness])
def inner_component():
    # Component can be anything from an LLM call, retrieval, agent, tool use, etc.
    update_current_span(test_case=LLMTestCase(input=&quot;...&quot;, actual_output=&quot;...&quot;))
    return

@observe
def llm_app(input: str):
    inner_component()
    return

evaluate(observed_callback=llm_app, goldens=[Golden(input=&quot;Hi!&quot;)])
```

You can learn everything about component-level evaluations [here.](https://www.deepeval.com/docs/evaluation-component-level-llm-evals)

&lt;br /&gt;

## Evaluating Without Pytest Integration

Alternatively, you can evaluate without Pytest, which is more suited for a notebook environment.

```python
from deepeval import evaluate
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)
test_case = LLMTestCase(
    input=&quot;What if these shoes don&#039;t fit?&quot;,
    # Replace this with the actual output from your LLM application
    actual_output=&quot;We offer a 30-day full refund at no extra costs.&quot;,
    retrieval_context=[&quot;All customers are eligible for a 30 day full refund at no extra costs.&quot;]
)
evaluate([test_case], [answer_relevancy_metric])
```

## Using Standalone Metrics

DeepEval is extremely modular, making it easy for anyone to use any of our metrics. Continuing from the previous example:

```python
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)
test_case = LLMTestCase(
    input=&quot;What if these shoes don&#039;t fit?&quot;,
    # Replace this with the actual output from your LLM application
    actual_output=&quot;We offer a 30-day full refund at no extra costs.&quot;,
    retrieval_context=[&quot;All customers are eligible for a 30 day full refund at no extra costs.&quot;]
)

answer_relevancy_metric.measure(test_case)
print(answer_relevancy_metric.score)
# All metrics also offer an explanation
print(answer_relevancy_metric.reason)
```

Note that some metrics are for RAG pipelines, while others are for fine-tuning. Make sure to use our docs to pick the right one for your use case.

## Evaluating a Dataset / Test Cases in Bulk

In DeepEval, a dataset is simply a collection of test cases. Here is how you can evaluate these in bulk:

```python
import pytest
from deepeval import assert_test
from deepeval.dataset import EvaluationDataset, Golden
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

dataset = EvaluationDataset(goldens=[Golden(input=&quot;What&#039;s the weather like today?&quot;)])

for golden in dataset.goldens:
    test_case = LLMTestCase(
        input=golden.input,
        actual_output=your_llm_app(golden.input)
    )
    dataset.add_test_case(test_case)

@pytest.mark.parametrize(
    &quot;test_case&quot;,
    dataset.test_cases,
)
def test_customer_chatbot(test_case: LLMTestCase):
    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)
    assert_test(test_case, [answer_relevancy_metric])
```

```bash
# Run this in the CLI, you can also add an optional -n flag to run tests in parallel
deepeval test run test_&lt;filename&gt;.py -n 4
```

&lt;br/&gt;

Alternatively, although we recommend using `deepeval test run`, you can evaluate a dataset/test cases without using our Pytest integration:

```python
from deepeval import evaluate
...

evaluate(dataset, [answer_relevancy_metric])
# or
dataset.evaluate([answer_relevancy_metric])
```

## A Note on Env Variables (.env / .env.local)

DeepEval auto-loads `.env.local` then `.env` from the current working directory **at import time**.
**Precedence:** process env -&gt; `.env.local` -&gt; `.env`.
Opt out with `DEEPEVAL_DISABLE_DOTENV=1`.

```bash
cp .env.example .env.local
# then edit .env.local (ignored by git)
```

# DeepEval With Confident AI

DeepEval is available on [Confident AI](https://confident-ai.com?utm_source=Github), an evals &amp; observability platform that allows you to:

1. Curate/annotate evaluation datasets on the cloud
2. Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best
3. Fine-tune metrics for custom results
4. Debug evaluation results via LLM traces
5. Monitor &amp; evaluate LLM responses in product to improve datasets with real-world data
6. Repeat until perfection

Everything on Confident AI, including how to use Confident is available [here](https://www.confident-ai.com/docs?utm_source=GitHub).

To begin, login from the CLI:

```bash
deepeval login
```

Follow the instructions to log in, create your account, and paste your API key into the CLI.

Now, run your test file again:

```bash
deepeval test run test_chatbot.py
```

You should see a link displayed in the CLI once the test has finished running. Paste it into your browser to view the results!

![Demo GIF](assets/demo.gif)

&lt;br /&gt;

## Configuration

### Environment variables via .env files

Using `.env.local` or `.env` is optional. If they are missing, DeepEval uses your existing environment variables. When present, dotenv environment variables are auto-loaded at import time (unless you set `DEEPEVAL_DISABLE_DOTENV=1`).

**Precedence:** process env -&gt; `.env.local` -&gt; `.env`

```bash
cp .env.example .env.local
# then edit .env.local (ignored by git)
```

&lt;br /&gt;

# Contributing

Please read [CONTRIBUTING.md](https://github.com/confident-ai/deepeval/blob/main/CONTRIBUTING.md) for details on our code of conduct, and the process for submitting pull requests to us.

&lt;br /&gt;

# Roadmap

Features:

- [x] Integration with Confident AI
- [x] Implement G-Eval
- [x] Implement RAG metrics
- [x] Implement Conversational metrics
- [x] Evaluation Dataset Creation
- [x] Red-Teaming
- [ ] DAG custom metrics
- [ ] Guardrails

&lt;br /&gt;

# Authors

Built by the founders of Confident AI. Contact jeffreyip@confident-ai.com for all enquiries.

&lt;br /&gt;

# License

DeepEval is licensed under Apache 2.0 - see the [LICENSE.md](https://github.com/confident-ai/deepeval/blob/main/LICENSE.md) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/agent-lightning]]></title>
            <link>https://github.com/microsoft/agent-lightning</link>
            <guid>https://github.com/microsoft/agent-lightning</guid>
            <pubDate>Thu, 05 Feb 2026 00:07:17 GMT</pubDate>
            <description><![CDATA[The absolute trainer to light up AI agents.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/agent-lightning">microsoft/agent-lightning</a></h1>
            <p>The absolute trainer to light up AI agents.</p>
            <p>Language: Python</p>
            <p>Stars: 13,961</p>
            <p>Forks: 1,171</p>
            <p>Stars today: 315 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/assets/readme-banner.svg&quot; alt=&quot;Agent-lightning-banner&quot; style=&quot;width:600px&quot;/&gt;
&lt;/p&gt;

# Agent Lightning‚ö°

[![Unit Tests](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml)
[![Documentation](https://img.shields.io/badge/GitHub%20Pages-Documentation-blue)](https://microsoft.github.io/agent-lightning/)
[![PyPI version](https://badge.fury.io/py/agentlightning.svg)](https://badge.fury.io/py/agentlightning)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/microsoft/agent-lightning)
[![Discord](https://img.shields.io/badge/Discord-Join-5865F2?logo=discord&amp;logoColor=white)](https://discord.gg/RYk7CdvDR7)

**The absolute trainer to light up AI agents.**

Join our [Discord community](https://discord.gg/RYk7CdvDR7) to connect with other users and contributors.

## ‚ö° Core Features

- Turn your agent into an optimizable beast with **ZERO CODE CHANGE** (almost)! üí§
- Build with **ANY** agent framework (LangChain, OpenAI Agent SDK, AutoGen, CrewAI, Microsoft Agent Framework...); or even WITHOUT agent framework (Python OpenAI). You name it! ü§ñ
- **Selectively** optimize one or more agents in a multi-agent system. üéØ
- Embraces **Algorithms** like Reinforcement Learning, Automatic Prompt Optimization, Supervised Fine-tuning and more. ü§ó

Read more on our [documentation website](https://microsoft.github.io/agent-lightning/).

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/assets/readme-diff.svg&quot; alt=&quot;Agent-Lightning Core Quickstart&quot; style=&quot;width:100%&quot;/&gt;
&lt;/p&gt;

## ‚ö° Installation

```bash
pip install agentlightning
```

For the latest nightly build (cutting-edge features), you can install from Test PyPI:

```bash
pip install --upgrade --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ --pre agentlightning
```

Please refer to our [installation guide](https://microsoft.github.io/agent-lightning/stable/tutorials/installation/) for more details.

To start using Agent-lightning, check out our [documentation](https://microsoft.github.io/agent-lightning/) and [examples](./examples).

## ‚ö° Articles

- 12/17/2025 [Adopting the Trajectory Level Aggregation for Faster Training](https://agent-lightning.github.io/posts/trajectory_level_aggregation/) Agent-lightning blog.
- 11/4/2025 [Tuning ANY AI agent with Tinker ‚úï Agent-lightning](https://medium.com/@yugez/tuning-any-ai-agent-with-tinker-agent-lightning-part-1-1d8c9a397f0e) Medium. See also [Part 2](https://medium.com/@yugez/tuning-any-ai-agent-with-tinker-agent-lightning-part-2-332c5437f0dc).
- 10/22/2025 [No More Retokenization Drift: Returning Token IDs via the OpenAI Compatible API Matters in Agent RL](https://blog.vllm.ai/2025/10/22/agent-lightning.html) vLLM blog. See also [Zhihu writeup](https://zhuanlan.zhihu.com/p/1965067274642785725).
- 8/11/2025 [Training AI Agents to Write and Self-correct SQL with Reinforcement Learning](https://medium.com/@yugez/training-ai-agents-to-write-and-self-correct-sql-with-reinforcement-learning-571ed31281ad) Medium.
- 8/5/2025 [Agent Lightning: Train ANY AI Agents with Reinforcement Learning](https://arxiv.org/abs/2508.03680) arXiv paper.
- 7/26/2025 [We discovered an approach to train any AI agent with RL, with (almost) zero code changes.](https://www.reddit.com/r/LocalLLaMA/comments/1m9m670/we_discovered_an_approach_to_train_any_ai_agent/) Reddit.
- 6/6/2025 [Agent Lightning - Microsoft Research](https://www.microsoft.com/en-us/research/project/agent-lightning/) Project page.

## ‚ö° Community Projects

- [DeepWerewolf](https://github.com/af-74413592/DeepWerewolf) ‚Äî A case study of agent RL training for the Chinese Werewolf game built with AgentScope and Agent Lightning.
- [AgentFlow](https://agentflow.stanford.edu/) ‚Äî A modular multi-agent framework that combines planner, executor, verifier, and generator agents with the Flow-GRPO algorithm to tackle long-horizon, sparse-reward tasks.
- [Youtu-Agent](https://github.com/TencentCloudADP/Youtu-agent) ‚Äî Youtu-Agent lets you build and train your agent with ease. Built with [a modified branch](https://github.com/microsoft/agent-lightning/tree/contrib/youtu-agent-lightning) of Agent Lightning, Youtu-Agent has verified up to 128 GPUs RL training on maths/code and search capabilities with steady convergence. Also check [the recipe](https://github.com/TencentCloudADP/youtu-agent/tree/rl/agl) and their blog [*Stop Wrestling with Your Agent RL: How Youtu-Agent Achieved Stable, 128-GPU Scaling Without Breaking a Sweat*](https://spotted-coconut-df8.notion.site/Stop-Wrestling-with-Your-Agent-RL-How-Youtu-Agent-Achieved-Stable-128-GPU-Scaling-Without-Breaking-2ca5e8f089ba80539a98c582b65e0233).

## ‚ö° Architecture

Agent Lightning keeps the moving parts to a minimum so you can focus on your idea, not the plumbing. Your agent continues to run as usual; you can still use any agent framework you like; you drop in the lightweight `agl.emit_xxx()` helper, or let the tracer collect every prompt, tool call, and reward. Those events become structured spans that flow into the LightningStore, a central hub that keeps tasks, resources, and traces in sync.

On the other side of the store sits the algorithm you choose, or write yourself. The algorithm reads spans, learns from them, and posts updated resources such as refined prompt templates or new policy weights. The Trainer ties it all together: it streams datasets to runners, ferries resources between the store and the algorithm, and updates the inference engine when improvements land. You can either stop there, or simply let the same loop keep turning.

No rewrites, no lock-in, just a clear path from first rollout to steady improvement.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/assets/readme-architecture.svg&quot; alt=&quot;Agent-lightning Architecture&quot; style=&quot;width:100%&quot;/&gt;
&lt;/p&gt;

## ‚ö° CI Status

| Workflow | Status |
|----------|--------|
| CPU Tests | [![tests workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml) |
| Full Tests | [![tests summary workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml) |
| UI Tests | [![UI Tests](https://github.com/microsoft/agent-lightning/actions/workflows/dashboard.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/dashboard.yml) |
| Examples Integration | [![examples summary workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml) |
| Latest Dependency Compatibility | [![latest summary workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml) |
| Legacy Examples Compatibility | [![compat summary workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-compat.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/badge-compat.yml) |

## ‚ö° Citation

If you find Agent Lightning useful in your research or projects, please cite our paper:

```bibtex
@misc{luo2025agentlightningtrainai,
      title={Agent Lightning: Train ANY AI Agents with Reinforcement Learning},
      author={Xufang Luo and Yuge Zhang and Zhiyuan He and Zilong Wang and Siyun Zhao and Dongsheng Li and Luna K. Qiu and Yuqing Yang},
      year={2025},
      eprint={2508.03680},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.03680},
}
```

## ‚ö° Contributing

This project welcomes contributions and suggestions. Start by reading the [Contributing Guide](docs/community/contributing.md) for recommended contribution points, environment setup, branching conventions, and pull request expectations. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## ‚ö° Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft&#039;s Trademark &amp; Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#039;s policies.

## ‚ö° Responsible AI

This project has been evaluated and certified to comply with the Microsoft Responsible AI Standard. The team will continue to monitor and maintain the repository, addressing any severe issues, including potential harms, if they arise.

## ‚ö° License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[TauricResearch/TradingAgents]]></title>
            <link>https://github.com/TauricResearch/TradingAgents</link>
            <guid>https://github.com/TauricResearch/TradingAgents</guid>
            <pubDate>Thu, 05 Feb 2026 00:07:16 GMT</pubDate>
            <description><![CDATA[TradingAgents: Multi-Agents LLM Financial Trading Framework]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/TauricResearch/TradingAgents">TauricResearch/TradingAgents</a></h1>
            <p>TradingAgents: Multi-Agents LLM Financial Trading Framework</p>
            <p>Language: Python</p>
            <p>Stars: 29,279</p>
            <p>Forks: 5,606</p>
            <p>Stars today: 135 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/TauricResearch.png&quot; style=&quot;width: 60%; height: auto;&quot;&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot; style=&quot;line-height: 1;&quot;&gt;
  &lt;a href=&quot;https://arxiv.org/abs/2412.20138&quot; target=&quot;_blank&quot;&gt;&lt;img alt=&quot;arXiv&quot; src=&quot;https://img.shields.io/badge/arXiv-2412.20138-B31B1B?logo=arxiv&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.com/invite/hk9PGKShPK&quot; target=&quot;_blank&quot;&gt;&lt;img alt=&quot;Discord&quot; src=&quot;https://img.shields.io/badge/Discord-TradingResearch-7289da?logo=discord&amp;logoColor=white&amp;color=7289da&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;./assets/wechat.png&quot; target=&quot;_blank&quot;&gt;&lt;img alt=&quot;WeChat&quot; src=&quot;https://img.shields.io/badge/WeChat-TauricResearch-brightgreen?logo=wechat&amp;logoColor=white&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://x.com/TauricResearch&quot; target=&quot;_blank&quot;&gt;&lt;img alt=&quot;X Follow&quot; src=&quot;https://img.shields.io/badge/X-TauricResearch-white?logo=x&amp;logoColor=white&quot;/&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://github.com/TauricResearch/&quot; target=&quot;_blank&quot;&gt;&lt;img alt=&quot;Community&quot; src=&quot;https://img.shields.io/badge/Join_GitHub_Community-TauricResearch-14C290?logo=discourse&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=de&quot;&gt;Deutsch&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=es&quot;&gt;Espa√±ol&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=fr&quot;&gt;fran√ßais&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=ja&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=ko&quot;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=pt&quot;&gt;Portugu√™s&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=ru&quot;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/TauricResearch/TradingAgents?lang=zh&quot;&gt;‰∏≠Êñá&lt;/a&gt;
&lt;/div&gt;

---

# TradingAgents: Multi-Agents LLM Financial Trading Framework

## News
- [2026-02] **TradingAgents v0.2.0** released with multi-provider LLM support (GPT-5.x, Gemini 3.x, Claude 4.x, Grok 4.x) and improved system architecture.
- [2026-01] **Trading-R1** [Technical Report](https://arxiv.org/abs/2509.11420) released, with [Terminal](https://github.com/TauricResearch/Trading-R1) expected to land soon.

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://www.star-history.com/#TauricResearch/TradingAgents&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=TauricResearch/TradingAgents&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=TauricResearch/TradingAgents&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;TradingAgents Star History&quot; src=&quot;https://api.star-history.com/svg?repos=TauricResearch/TradingAgents&amp;type=Date&quot; style=&quot;width: 80%; height: auto;&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;
&lt;/div&gt;

&gt; üéâ **TradingAgents** officially released! We have received numerous inquiries about the work, and we would like to express our thanks for the enthusiasm in our community.
&gt;
&gt; So we decided to fully open-source the framework. Looking forward to building impactful projects with you!

&lt;div align=&quot;center&quot;&gt;

üöÄ [TradingAgents](#tradingagents-framework) | ‚ö° [Installation &amp; CLI](#installation-and-cli) | üé¨ [Demo](https://www.youtube.com/watch?v=90gr5lwjIho) | üì¶ [Package Usage](#tradingagents-package) | ü§ù [Contributing](#contributing) | üìÑ [Citation](#citation)

&lt;/div&gt;

## TradingAgents Framework

TradingAgents is a multi-agent trading framework that mirrors the dynamics of real-world trading firms. By deploying specialized LLM-powered agents: from fundamental analysts, sentiment experts, and technical analysts, to trader, risk management team, the platform collaboratively evaluates market conditions and informs trading decisions. Moreover, these agents engage in dynamic discussions to pinpoint the optimal strategy.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/schema.png&quot; style=&quot;width: 100%; height: auto;&quot;&gt;
&lt;/p&gt;

&gt; TradingAgents framework is designed for research purposes. Trading performance may vary based on many factors, including the chosen backbone language models, model temperature, trading periods, the quality of data, and other non-deterministic factors. [It is not intended as financial, investment, or trading advice.](https://tauric.ai/disclaimer/)

Our framework decomposes complex trading tasks into specialized roles. This ensures the system achieves a robust, scalable approach to market analysis and decision-making.

### Analyst Team
- Fundamentals Analyst: Evaluates company financials and performance metrics, identifying intrinsic values and potential red flags.
- Sentiment Analyst: Analyzes social media and public sentiment using sentiment scoring algorithms to gauge short-term market mood.
- News Analyst: Monitors global news and macroeconomic indicators, interpreting the impact of events on market conditions.
- Technical Analyst: Utilizes technical indicators (like MACD and RSI) to detect trading patterns and forecast price movements.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/analyst.png&quot; width=&quot;100%&quot; style=&quot;display: inline-block; margin: 0 2%;&quot;&gt;
&lt;/p&gt;

### Researcher Team
- Comprises both bullish and bearish researchers who critically assess the insights provided by the Analyst Team. Through structured debates, they balance potential gains against inherent risks.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/researcher.png&quot; width=&quot;70%&quot; style=&quot;display: inline-block; margin: 0 2%;&quot;&gt;
&lt;/p&gt;

### Trader Agent
- Composes reports from the analysts and researchers to make informed trading decisions. It determines the timing and magnitude of trades based on comprehensive market insights.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/trader.png&quot; width=&quot;70%&quot; style=&quot;display: inline-block; margin: 0 2%;&quot;&gt;
&lt;/p&gt;

### Risk Management and Portfolio Manager
- Continuously evaluates portfolio risk by assessing market volatility, liquidity, and other risk factors. The risk management team evaluates and adjusts trading strategies, providing assessment reports to the Portfolio Manager for final decision.
- The Portfolio Manager approves/rejects the transaction proposal. If approved, the order will be sent to the simulated exchange and executed.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/risk.png&quot; width=&quot;70%&quot; style=&quot;display: inline-block; margin: 0 2%;&quot;&gt;
&lt;/p&gt;

## Installation and CLI

### Installation

Clone TradingAgents:
```bash
git clone https://github.com/TauricResearch/TradingAgents.git
cd TradingAgents
```

Create a virtual environment in any of your favorite environment managers:
```bash
conda create -n tradingagents python=3.13
conda activate tradingagents
```

Install dependencies:
```bash
pip install -r requirements.txt
```

### Required APIs

TradingAgents supports multiple LLM providers. Set the API key for your chosen provider:

```bash
export OPENAI_API_KEY=...          # OpenAI (GPT)
export GOOGLE_API_KEY=...          # Google (Gemini)
export ANTHROPIC_API_KEY=...       # Anthropic (Claude)
export XAI_API_KEY=...             # xAI (Grok)
export OPENROUTER_API_KEY=...      # OpenRouter
export ALPHA_VANTAGE_API_KEY=...   # Alpha Vantage
```

For local models, configure Ollama with `llm_provider: &quot;ollama&quot;` in your config.

Alternatively, copy `.env.example` to `.env` and fill in your keys:
```bash
cp .env.example .env
```

### CLI Usage

You can also try out the CLI directly by running:
```bash
python -m cli.main
```
You will see a screen where you can select your desired tickers, date, LLMs, research depth, etc.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/cli/cli_init.png&quot; width=&quot;100%&quot; style=&quot;display: inline-block; margin: 0 2%;&quot;&gt;
&lt;/p&gt;

An interface will appear showing results as they load, letting you track the agent&#039;s progress as it runs.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/cli/cli_news.png&quot; width=&quot;100%&quot; style=&quot;display: inline-block; margin: 0 2%;&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/cli/cli_transaction.png&quot; width=&quot;100%&quot; style=&quot;display: inline-block; margin: 0 2%;&quot;&gt;
&lt;/p&gt;

## TradingAgents Package

### Implementation Details

We built TradingAgents with LangGraph to ensure flexibility and modularity. The framework supports multiple LLM providers: OpenAI, Google, Anthropic, xAI, OpenRouter, and Ollama.

### Python Usage

To use TradingAgents inside your code, you can import the `tradingagents` module and initialize a `TradingAgentsGraph()` object. The `.propagate()` function will return a decision. You can run `main.py`, here&#039;s also a quick example:

```python
from tradingagents.graph.trading_graph import TradingAgentsGraph
from tradingagents.default_config import DEFAULT_CONFIG

ta = TradingAgentsGraph(debug=True, config=DEFAULT_CONFIG.copy())

# forward propagate
_, decision = ta.propagate(&quot;NVDA&quot;, &quot;2026-01-15&quot;)
print(decision)
```

You can also adjust the default configuration to set your own choice of LLMs, debate rounds, etc.

```python
from tradingagents.graph.trading_graph import TradingAgentsGraph
from tradingagents.default_config import DEFAULT_CONFIG

config = DEFAULT_CONFIG.copy()
config[&quot;llm_provider&quot;] = &quot;openai&quot;        # openai, google, anthropic, xai, openrouter, ollama
config[&quot;deep_think_llm&quot;] = &quot;gpt-5.2&quot;     # Model for complex reasoning
config[&quot;quick_think_llm&quot;] = &quot;gpt-5-mini&quot; # Model for quick tasks
config[&quot;max_debate_rounds&quot;] = 2

ta = TradingAgentsGraph(debug=True, config=config)
_, decision = ta.propagate(&quot;NVDA&quot;, &quot;2026-01-15&quot;)
print(decision)
```

See `tradingagents/default_config.py` for all configuration options.

## Contributing

We welcome contributions from the community! Whether it&#039;s fixing a bug, improving documentation, or suggesting a new feature, your input helps make this project better. If you are interested in this line of research, please consider joining our open-source financial AI research community [Tauric Research](https://tauric.ai/).

## Citation

Please reference our work if you find *TradingAgents* provides you with some help :)

```
@misc{xiao2025tradingagentsmultiagentsllmfinancial,
      title={TradingAgents: Multi-Agents LLM Financial Trading Framework}, 
      author={Yijia Xiao and Edward Sun and Di Luo and Wei Wang},
      year={2025},
      eprint={2412.20138},
      archivePrefix={arXiv},
      primaryClass={q-fin.TR},
      url={https://arxiv.org/abs/2412.20138}, 
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>