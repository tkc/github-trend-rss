<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Thu, 10 Apr 2025 00:04:29 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[microsoft/markitdown]]></title>
            <link>https://github.com/microsoft/markitdown</link>
            <guid>https://github.com/microsoft/markitdown</guid>
            <pubDate>Thu, 10 Apr 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[Python tool for converting files and office documents to Markdown.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/markitdown">microsoft/markitdown</a></h1>
            <p>Python tool for converting files and office documents to Markdown.</p>
            <p>Language: Python</p>
            <p>Stars: 47,184</p>
            <p>Forks: 2,224</p>
            <p>Stars today: 1,575 stars today</p>
            <h2>README</h2><pre># MarkItDown

[![PyPI](https://img.shields.io/pypi/v/markitdown.svg)](https://pypi.org/project/markitdown/)
![PyPI - Downloads](https://img.shields.io/pypi/dd/markitdown)
[![Built by AutoGen Team](https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue)](https://github.com/microsoft/autogen)

&gt; [!TIP]
&gt; MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See [markitdown-mcp](https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp) for more information.

&gt; [!IMPORTANT]
&gt; Breaking changes between 0.0.1 to 0.1.0:
&gt; * Dependencies are now organized into optional feature-groups (further details below). Use `pip install &#039;markitdown[all]&#039;` to have backward-compatible behavior. 
&gt; * convert\_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.
&gt; * The DocumentConverter class interface has changed to read from file-like streams rather than file paths. *No temporary files are created anymore*. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.

MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to [textract](https://github.com/deanmalmgren/textract), but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.

At present, MarkItDown supports:

- PDF
- PowerPoint
- Word
- Excel
- Images (EXIF metadata and OCR)
- Audio (EXIF metadata and speech transcription)
- HTML
- Text-based formats (CSV, JSON, XML)
- ZIP files (iterates over contents)
- Youtube URLs
- EPubs
- ... and more!

## Why Markdown?

Markdown is extremely close to plain text, with minimal markup or formatting, but still
provides a way to represent important document structure. Mainstream LLMs, such as
OpenAI&#039;s GPT-4o, natively &quot;_speak_&quot; Markdown, and often incorporate Markdown into their
responses unprompted. This suggests that they have been trained on vast amounts of
Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions
are also highly token-efficient.

## Installation

To install MarkItDown, use pip: `pip install &#039;markitdown[all]&#039;`. Alternatively, you can install it from the source:

```bash
git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e packages/markitdown[all]
```

## Usage

### Command-Line

```bash
markitdown path-to-file.pdf &gt; document.md
```

Or use `-o` to specify the output file:

```bash
markitdown path-to-file.pdf -o document.md
```

You can also pipe content:

```bash
cat path-to-file.pdf | markitdown
```

### Optional Dependencies
MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the `[all]` option. However, you can also install them individually for more control. For example:

```bash
pip install markitdown[pdf, docx, pptx]
```

will install only the dependencies for PDF, DOCX, and PPTX files.

At the moment, the following optional dependencies are available:

* `[all]` Installs all optional dependencies
* `[pptx]` Installs dependencies for PowerPoint files
* `[docx]` Installs dependencies for Word files
* `[xlsx]` Installs dependencies for Excel files
* `[xls]` Installs dependencies for older Excel files
* `[pdf]` Installs dependencies for PDF files
* `[outlook]` Installs dependencies for Outlook messages
* `[az-doc-intel]` Installs dependencies for Azure Document Intelligence
* `[audio-transcription]` Installs dependencies for audio transcription of wav and mp3 files
* `[youtube-transcription]` Installs dependencies for fetching YouTube video transcription

### Plugins

MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:

```bash
markitdown --list-plugins
```

To enable plugins use:

```bash
markitdown --use-plugins path-to-file.pdf
```

To find available plugins, search GitHub for the hashtag `#markitdown-plugin`. To develop a plugin, see `packages/markitdown-sample-plugin`.

### Azure Document Intelligence

To use Microsoft Document Intelligence for conversion:

```bash
markitdown path-to-file.pdf -o document.md -d -e &quot;&lt;document_intelligence_endpoint&gt;&quot;
```

More information about how to set up an Azure Document Intelligence Resource can be found [here](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0)

### Python API

Basic usage in Python:

```python
from markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert(&quot;test.xlsx&quot;)
print(result.text_content)
```

Document Intelligence conversion in Python:

```python
from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint=&quot;&lt;document_intelligence_endpoint&gt;&quot;)
result = md.convert(&quot;test.pdf&quot;)
print(result.text_content)
```

To use Large Language Models for image descriptions, provide `llm_client` and `llm_model`:

```python
from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model=&quot;gpt-4o&quot;)
result = md.convert(&quot;example.jpg&quot;)
print(result.text_content)
```

### Docker

```sh
docker build -t markitdown:latest .
docker run --rm -i markitdown:latest &lt; ~/your-file.pdf &gt; output.md
```

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

### How to Contribute

You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as &#039;open for contribution&#039; and &#039;open for reviewing&#039; to help facilitate community contributions. These are ofcourse just suggestions and you are welcome to contribute in any way you like.

&lt;div align=&quot;center&quot;&gt;

|            | All                                                          | Especially Needs Help from Community                                                                                                      |
| ---------- | ------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------- |
| **Issues** | [All Issues](https://github.com/microsoft/markitdown/issues) | [Issues open for contribution](https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22) |
| **PRs**    | [All PRs](https://github.com/microsoft/markitdown/pulls)     | [PRs open for reviewing](https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22)              |

&lt;/div&gt;

### Running Tests and Checks

- Navigate to the MarkItDown package:

  ```sh
  cd packages/markitdown
  ```

- Install `hatch` in your environment and run tests:

  ```sh
  pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
  hatch shell
  hatch test
  ```

  (Alternative) Use the Devcontainer which has all the dependencies installed:

  ```sh
  # Reopen the project in Devcontainer and run:
  hatch test
  ```

- Run pre-commit checks before submitting a PR: `pre-commit run --all-files`

### Contributing 3rd-party Plugins

You can also contribute by creating and sharing 3rd party plugins. See `packages/markitdown-sample-plugin` for more details.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
[Microsoft&#039;s Trademark &amp; Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party&#039;s policies.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[HKUDS/LightRAG]]></title>
            <link>https://github.com/HKUDS/LightRAG</link>
            <guid>https://github.com/HKUDS/LightRAG</guid>
            <pubDate>Thu, 10 Apr 2025 00:04:28 GMT</pubDate>
            <description><![CDATA["LightRAG: Simple and Fast Retrieval-Augmented Generation"]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/HKUDS/LightRAG">HKUDS/LightRAG</a></h1>
            <p>"LightRAG: Simple and Fast Retrieval-Augmented Generation"</p>
            <p>Language: Python</p>
            <p>Stars: 14,457</p>
            <p>Forks: 2,008</p>
            <p>Stars today: 488 stars today</p>
            <h2>README</h2><pre>&lt;center&gt;&lt;h2&gt;ğŸš€ LightRAG: Simple and Fast Retrieval-Augmented Generation&lt;/h2&gt;&lt;/center&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;table border=&quot;0&quot; width=&quot;100%&quot;&gt;
&lt;tr&gt;
&lt;td width=&quot;100&quot; align=&quot;center&quot;&gt;
&lt;img src=&quot;./assets/logo.png&quot; width=&quot;80&quot; height=&quot;80&quot; alt=&quot;lightrag&quot;&gt;
&lt;/td&gt;
&lt;td&gt;

&lt;div&gt;
    &lt;p&gt;
        &lt;a href=&#039;https://lightrag.github.io&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/Project-Page-Green&#039;&gt;&lt;/a&gt;
        &lt;a href=&#039;https://youtu.be/oageL-1I0GE&#039;&gt;&lt;img src=&#039;https://badges.aleen42.com/src/youtube.svg&#039;&gt;&lt;/a&gt;
        &lt;a href=&#039;https://arxiv.org/abs/2410.05779&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/arXiv-2410.05779-b31b1b&#039;&gt;&lt;/a&gt;
        &lt;a href=&#039;https://learnopencv.com/lightrag&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/LearnOpenCV-blue&#039;&gt;&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
        &lt;img src=&#039;https://img.shields.io/github/stars/hkuds/lightrag?color=green&amp;style=social&#039; /&gt;
        &lt;img src=&quot;https://img.shields.io/badge/python-3.10-blue&quot;&gt;
        &lt;a href=&quot;https://pypi.org/project/lightrag-hku/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/lightrag-hku.svg&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://pepy.tech/project/lightrag-hku&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/lightrag-hku/month&quot;&gt;&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
        &lt;a href=&#039;https://discord.gg/yF2MmDJyGJ&#039;&gt;&lt;img src=&#039;https://discordapp.com/api/guilds/1296348098003734629/widget.png?style=shield&#039;&gt;&lt;/a&gt;
        &lt;a href=&#039;https://github.com/HKUDS/LightRAG/issues/285&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/ç¾¤èŠ-wechat-green&#039;&gt;&lt;/a&gt;
    &lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;img src=&quot;./README.assets/b2aaf634151b4706892693ffb43d9093.png&quot; width=&quot;800&quot; alt=&quot;LightRAG Diagram&quot;&gt;

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://trendshift.io/repositories/13043&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13043&quot; alt=&quot;HKUDS%2FLightRAG | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

## ğŸ‰ News

- [X] [2025.03.18]ğŸ¯ğŸ“¢LightRAG now supports citation functionality, enabling proper source attribution.
- [X] [2025.02.05]ğŸ¯ğŸ“¢Our team has released [VideoRAG](https://github.com/HKUDS/VideoRAG) understanding extremely long-context videos.
- [X] [2025.01.13]ğŸ¯ğŸ“¢Our team has released [MiniRAG](https://github.com/HKUDS/MiniRAG) making RAG simpler with small models.
- [X] [2025.01.06]ğŸ¯ğŸ“¢You can now [use PostgreSQL for Storage](#using-postgresql-for-storage).
- [X] [2024.12.31]ğŸ¯ğŸ“¢LightRAG now supports [deletion by document ID](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#delete).
- [X] [2024.11.25]ğŸ¯ğŸ“¢LightRAG now supports seamless integration of [custom knowledge graphs](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#insert-custom-kg), empowering users to enhance the system with their own domain expertise.
- [X] [2024.11.19]ğŸ¯ğŸ“¢A comprehensive guide to LightRAG is now available on [LearnOpenCV](https://learnopencv.com/lightrag). Many thanks to the blog author.
- [X] [2024.11.11]ğŸ¯ğŸ“¢LightRAG now supports [deleting entities by their names](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#delete).
- [X] [2024.11.09]ğŸ¯ğŸ“¢Introducing the [LightRAG Gui](https://lightrag-gui.streamlit.app), which allows you to insert, query, visualize, and download LightRAG knowledge.
- [X] [2024.11.04]ğŸ¯ğŸ“¢You can now [use Neo4J for Storage](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#using-neo4j-for-storage).
- [X] [2024.10.29]ğŸ¯ğŸ“¢LightRAG now supports multiple file types, including PDF, DOC, PPT, and CSV via `textract`.
- [X] [2024.10.20]ğŸ¯ğŸ“¢We&#039;ve added a new feature to LightRAG: Graph Visualization.
- [X] [2024.10.18]ğŸ¯ğŸ“¢We&#039;ve added a link to a [LightRAG Introduction Video](https://youtu.be/oageL-1I0GE). Thanks to the author!
- [X] [2024.10.17]ğŸ¯ğŸ“¢We have created a [Discord channel](https://discord.gg/yF2MmDJyGJ)! Welcome to join for sharing and discussions! ğŸ‰ğŸ‰
- [X] [2024.10.16]ğŸ¯ğŸ“¢LightRAG now supports [Ollama models](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#quick-start)!
- [X] [2024.10.15]ğŸ¯ğŸ“¢LightRAG now supports [Hugging Face models](https://github.com/HKUDS/LightRAG?tab=readme-ov-file#quick-start)!

&lt;details&gt;
  &lt;summary style=&quot;font-size: 1.4em; font-weight: bold; cursor: pointer; display: list-item;&quot;&gt;
    Algorithm Flowchart
  &lt;/summary&gt;

![LightRAG Indexing Flowchart](https://learnopencv.com/wp-content/uploads/2024/11/LightRAG-VectorDB-Json-KV-Store-Indexing-Flowchart-scaled.jpg)
*Figure 1: LightRAG Indexing Flowchart - Img Caption : [Source](https://learnopencv.com/lightrag/)*
![LightRAG Retrieval and Querying Flowchart](https://learnopencv.com/wp-content/uploads/2024/11/LightRAG-Querying-Flowchart-Dual-Level-Retrieval-Generation-Knowledge-Graphs-scaled.jpg)
*Figure 2: LightRAG Retrieval and Querying Flowchart - Img Caption : [Source](https://learnopencv.com/lightrag/)*

&lt;/details&gt;

## Installation

### Install  LightRAG Core

* Install from source (Recommend)

```bash
cd LightRAG
pip install -e .
```

* Install from PyPI

```bash
pip install lightrag-hku
```

### Install LightRAG Server

The LightRAG Server is designed to provide Web UI and API support. The Web UI facilitates document indexing, knowledge graph exploration, and a simple RAG query interface. LightRAG Server also provide an Ollama compatible interfaces, aiming to emulate LightRAG as an Ollama chat model. This allows AI chat bot, such as Open WebUI, to access LightRAG easily.

* Install from PyPI

```bash
pip install &quot;lightrag-hku[api]&quot;
```

* Installation from Source

```bash
# create a Python virtual enviroment if neccesary
# Install in editable mode with API support
pip install -e &quot;.[api]&quot;
```

**For more information about LightRAG Server, please refer to [LightRAG Server](./lightrag/api/README.md).**

## Quick Start

* [Video demo](https://www.youtube.com/watch?v=g21royNJ4fw) of running LightRAG locally.
* All the code can be found in the `examples`.
* Set OpenAI API key in environment if using OpenAI models: `export OPENAI_API_KEY=&quot;sk-...&quot;.`
* Download the demo text &quot;A Christmas Carol by Charles Dickens&quot;:

```bash
curl https://raw.githubusercontent.com/gusye1234/nano-graphrag/main/tests/mock_data.txt &gt; ./book.txt
```

## Query

Use the below Python snippet (in a script) to initialize LightRAG and perform queries:

```python
import os
import asyncio
from lightrag import LightRAG, QueryParam
from lightrag.llm.openai import gpt_4o_mini_complete, gpt_4o_complete, openai_embed
from lightrag.kg.shared_storage import initialize_pipeline_status
from lightrag.utils import setup_logger

setup_logger(&quot;lightrag&quot;, level=&quot;INFO&quot;)

async def initialize_rag():
    rag = LightRAG(
        working_dir=&quot;your/path&quot;,
        embedding_func=openai_embed,
        llm_model_func=gpt_4o_mini_complete
    )

    await rag.initialize_storages()
    await initialize_pipeline_status()

    return rag

def main():
    # Initialize RAG instance
    rag = asyncio.run(initialize_rag())
    # Insert text
    rag.insert(&quot;Your text&quot;)

    # Perform naive search
    mode=&quot;naive&quot;
    # Perform local search
    mode=&quot;local&quot;
    # Perform global search
    mode=&quot;global&quot;
    # Perform hybrid search
    mode=&quot;hybrid&quot;
    # Mix mode Integrates knowledge graph and vector retrieval.
    mode=&quot;mix&quot;

    rag.query(
        &quot;What are the top themes in this story?&quot;,
        param=QueryParam(mode=mode)
    )

if __name__ == &quot;__main__&quot;:
    main()
```

### Query Param

```python
class QueryParam:
    mode: Literal[&quot;local&quot;, &quot;global&quot;, &quot;hybrid&quot;, &quot;naive&quot;, &quot;mix&quot;] = &quot;global&quot;
    &quot;&quot;&quot;Specifies the retrieval mode:
    - &quot;local&quot;: Focuses on context-dependent information.
    - &quot;global&quot;: Utilizes global knowledge.
    - &quot;hybrid&quot;: Combines local and global retrieval methods.
    - &quot;naive&quot;: Performs a basic search without advanced techniques.
    - &quot;mix&quot;: Integrates knowledge graph and vector retrieval. Mix mode combines knowledge graph and vector search:
        - Uses both structured (KG) and unstructured (vector) information
        - Provides comprehensive answers by analyzing relationships and context
        - Supports image content through HTML img tags
        - Allows control over retrieval depth via top_k parameter
    &quot;&quot;&quot;
    only_need_context: bool = False
    &quot;&quot;&quot;If True, only returns the retrieved context without generating a response.&quot;&quot;&quot;
    response_type: str = &quot;Multiple Paragraphs&quot;
    &quot;&quot;&quot;Defines the response format. Examples: &#039;Multiple Paragraphs&#039;, &#039;Single Paragraph&#039;, &#039;Bullet Points&#039;.&quot;&quot;&quot;
    top_k: int = 60
    &quot;&quot;&quot;Number of top items to retrieve. Represents entities in &#039;local&#039; mode and relationships in &#039;global&#039; mode.&quot;&quot;&quot;
    max_token_for_text_unit: int = 4000
    &quot;&quot;&quot;Maximum number of tokens allowed for each retrieved text chunk.&quot;&quot;&quot;
    max_token_for_global_context: int = 4000
    &quot;&quot;&quot;Maximum number of tokens allocated for relationship descriptions in global retrieval.&quot;&quot;&quot;
    max_token_for_local_context: int = 4000
    &quot;&quot;&quot;Maximum number of tokens allocated for entity descriptions in local retrieval.&quot;&quot;&quot;
    ids: list[str] | None = None # ONLY SUPPORTED FOR PG VECTOR DBs
    &quot;&quot;&quot;List of ids to filter the RAG.&quot;&quot;&quot;
    model_func: Callable[..., object] | None = None
    &quot;&quot;&quot;Optional override for the LLM model function to use for this specific query.
    If provided, this will be used instead of the global model function.
    This allows using different models for different query modes.
    &quot;&quot;&quot;
    ...
```

&gt; default value of Top_k can be change by environment  variables  TOP_K.

### LLM and Embedding Injection

LightRAG requires the utilization of LLM and Embedding models to accomplish document indexing and querying tasks. During the initialization phase, it is necessary to inject the invocation methods of the relevant models into LightRAGï¼š

&lt;details&gt;
&lt;summary&gt; &lt;b&gt;Using Open AI-like APIs&lt;/b&gt; &lt;/summary&gt;

* LightRAG also supports Open AI-like chat/embeddings APIs:

```python
async def llm_model_func(
    prompt, system_prompt=None, history_messages=[], keyword_extraction=False, **kwargs
) -&gt; str:
    return await openai_complete_if_cache(
        &quot;solar-mini&quot;,
        prompt,
        system_prompt=system_prompt,
        history_messages=history_messages,
        api_key=os.getenv(&quot;UPSTAGE_API_KEY&quot;),
        base_url=&quot;https://api.upstage.ai/v1/solar&quot;,
        **kwargs
    )

async def embedding_func(texts: list[str]) -&gt; np.ndarray:
    return await openai_embed(
        texts,
        model=&quot;solar-embedding-1-large-query&quot;,
        api_key=os.getenv(&quot;UPSTAGE_API_KEY&quot;),
        base_url=&quot;https://api.upstage.ai/v1/solar&quot;
    )

async def initialize_rag():
    rag = LightRAG(
        working_dir=WORKING_DIR,
        llm_model_func=llm_model_func,
        embedding_func=EmbeddingFunc(
            embedding_dim=4096,
            max_token_size=8192,
            func=embedding_func
        )
    )

    await rag.initialize_storages()
    await initialize_pipeline_status()

    return rag
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt; &lt;b&gt;Using Hugging Face Models&lt;/b&gt; &lt;/summary&gt;

* If you want to use Hugging Face models, you only need to set LightRAG as follows:

See `lightrag_hf_demo.py`

```python
# Initialize LightRAG with Hugging Face model
rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=hf_model_complete,  # Use Hugging Face model for text generation
    llm_model_name=&#039;meta-llama/Llama-3.1-8B-Instruct&#039;,  # Model name from Hugging Face
    # Use Hugging Face embedding function
    embedding_func=EmbeddingFunc(
        embedding_dim=384,
        max_token_size=5000,
        func=lambda texts: hf_embed(
            texts,
            tokenizer=AutoTokenizer.from_pretrained(&quot;sentence-transformers/all-MiniLM-L6-v2&quot;),
            embed_model=AutoModel.from_pretrained(&quot;sentence-transformers/all-MiniLM-L6-v2&quot;)
        )
    ),
)
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt; &lt;b&gt;Using Ollama Models&lt;/b&gt; &lt;/summary&gt;
**Overview**

If you want to use Ollama models, you need to pull model you plan to use and embedding model, for example `nomic-embed-text`.

Then you only need to set LightRAG as follows:

```python
# Initialize LightRAG with Ollama model
rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=ollama_model_complete,  # Use Ollama model for text generation
    llm_model_name=&#039;your_model_name&#039;, # Your model name
    # Use Ollama embedding function
    embedding_func=EmbeddingFunc(
        embedding_dim=768,
        max_token_size=8192,
        func=lambda texts: ollama_embed(
            texts,
            embed_model=&quot;nomic-embed-text&quot;
        )
    ),
)
```

* **Increasing context size**

In order for LightRAG to work context should be at least 32k tokens. By default Ollama models have context size of 8k. You can achieve this using one of two ways:

* **Increasing the `num_ctx` parameter in Modelfile**

1. Pull the model:

```bash
ollama pull qwen2
```

2. Display the model file:

```bash
ollama show --modelfile qwen2 &gt; Modelfile
```

3. Edit the Modelfile by adding the following line:

```bash
PARAMETER num_ctx 32768
```

4. Create the modified model:

```bash
ollama create -f Modelfile qwen2m
```

* **Setup `num_ctx` via Ollama API**

Tiy can use `llm_model_kwargs` param to configure ollama:

```python
rag = LightRAG(
    working_dir=WORKING_DIR,
    llm_model_func=ollama_model_complete,  # Use Ollama model for text generation
    llm_model_name=&#039;your_model_name&#039;, # Your model name
    llm_model_kwargs={&quot;options&quot;: {&quot;num_ctx&quot;: 32768}},
    # Use Ollama embedding function
    embedding_func=EmbeddingFunc(
        embedding_dim=768,
        max_token_size=8192,
        func=lambda texts: ollama_embedding(
            texts,
            embed_model=&quot;nomic-embed-text&quot;
        )
    ),
)
```

* **Low RAM GPUs**

In order to run this experiment on low RAM GPU you should select small model and tune context window (increasing context increase memory consumption). For example, running this ollama example on repurposed mining GPU with 6Gb of RAM required to set context size to 26k while using `gemma2:2b`. It was able to find 197 entities and 19 relations on `book.txt`.

&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt; &lt;b&gt;LlamaIndex&lt;/b&gt; &lt;/summary&gt;

LightRAG supports integration with LlamaIndex (`llm/llama_index_impl.py`):

- Integrates with OpenAI and other providers through LlamaIndex
- See [LlamaIndex Documentation](lightrag/llm/Readme.md) for detailed setup and examples

**Example Usage**

```python
# Using LlamaIndex with direct OpenAI access
import asyncio
from lightrag import LightRAG
from lightrag.llm.llama_index_impl import llama_index_complete_if_cache, llama_index_embed
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
from lightrag.kg.shared_storage import initialize_pipeline_status
from lightrag.utils import setup_logger

# Setup log handler for LightRAG
setup_logger(&quot;lightrag&quot;, level=&quot;INFO&quot;)

async def initialize_rag():
    rag = LightRAG(
        working_dir=&quot;your/path&quot;,
        llm_model_func=llama_index_complete_if_cache,  # LlamaIndex-compatible completion function
        embedding_func=EmbeddingFunc(    # LlamaIndex-compatible embedding function
            embedding_dim=1536,
            max_token_size=8192,
            func=lambda texts: llama_index_embed(texts, embed_model=embed_model)
        ),
    )

    await rag.initialize_storages()
    await initialize_pipeline_status()

    return rag

def main():
    # Initialize RAG instance
    rag = asyncio.run(initialize_rag())

    with open(&quot;./book.txt&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
        rag.insert(f.read())

    # Perform naive search
    print(
        rag.query(&quot;What are the top themes in this story?&quot;, param=QueryParam(mode=&quot;naive&quot;))
    )

    # Perform local search
    print(
        rag.query(&quot;What are the top themes in this story?&quot;, param=QueryParam(mode=&quot;local&quot;))
    )

    # Perform global search
    print(
        rag.query(&quot;What are the top themes in this story?&quot;, param=QueryParam(mode=&quot;global&quot;))
    )

    # Perform hybrid search
    print(
        rag.query(&quot;What are the top themes in this story?&quot;, param=QueryParam(mode=&quot;hybrid&quot;))
    )

if __name__ == &quot;__main__&quot;:
    main()
```

**For detailed documentation and examples, see:**

- [LlamaIndex Documentation](lightrag/llm/Readme.md)
- [Direct OpenAI Example](examples/lightrag_llamaindex_direct_demo.py)
- [LiteLLM Proxy Example](examples/lightrag_llamaindex_litellm_demo.py)

&lt;/details&gt;

### Token Usage Tracking

&lt;details&gt;
&lt;summary&gt; &lt;b&gt;Overview and Usage&lt;/b&gt; &lt;/summary&gt;

LightRAG provides a TokenTracker tool to monitor and manage token consumption by large language models. This feature is particularly useful for controlling API costs and optimizing performance.

#### Usage

```python
from lightrag.utils import TokenTracker

# Create TokenTracker instance
token_tracker = TokenTracker()

# Method 1: Using context manager (Recommended)
# Suitable for scenarios requiring automatic token usage tracking
with token_tracker:
    result1 = await llm_model_func(&quot;your question 1&quot;)
    result2 = await llm_model_func(&quot;your question 2&quot;)

# Method 2: Manually adding token usage records
# Suitable for scenarios requiring more granular control over token statistics
token_tracker.reset()

rag.insert()

rag.query(&quot;your question 1&quot;, param=QueryParam(mode=&quot;naive&quot;))
rag.query(&quot;your question 2&quot;, param=QueryParam(mode=&quot;mix&quot;))

# Display total token usage (including insert and query operations)
print(&quot;Token usage:&quot;, token_tracker.get_usage())
```

#### Usage Tips
- Use context managers for long sessions or batch operations to automatically track all token consumption
- For scenarios requiring segmented statistics, use manual mode and call reset() when appropriate
- Regular checking of token usage helps detect abnormal consumption early
- Actively use this feature during development and testing to optimize production costs

#### Practical Examples
You can refer to these examples for implementing token tracking:
- `examples/lightrag_gemini_track_token_demo.py`: Token tracking example using Google Gemini model
- `examples/lightrag_siliconcloud_track_token_demo.py`: Token tracking example using SiliconCloud model

These examples demonstrate how to effectively use the TokenTracker feature with different models and scenarios.

&lt;/details&gt;

### Conversation History Support


LightRAG now supports multi-turn dialogue through the conversation history feature. Here&#039;s how to use it:

&lt;details&gt;
  &lt;summary&gt; &lt;b&gt; Usage Example &lt;/b&gt;&lt;/summary&gt;

```python
# Create conversation history
conversation_history = [
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is the main character&#039;s attitude towards Christmas?&quot;},
    {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;At the beginning of the story, Ebenezer Scrooge has a very negative attitude towards Christmas...&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;How does his attitude change?&quot;}
]

# Create query parameters with conversation history
query_param = QueryParam(
    mode=&quot;mix&quot;,  # or any other mode: &quot;local&quot;, &quot;global&quot;, &quot;hybrid&quot;
    conversation_history=conversation_history,  # Add the conversation history
    history_turns=3  # Number of recent conversation turns to consider
)

# Make a query that takes into account the conversation history
response = rag.query(
    &quot;What causes this change in his character?&quot;,
    param=query_param
)
```

&lt;/details&gt;

### Custom Prompt Support

LightRAG now supports custom prompts for fine-tuned control over the system&#039;s behavior. Here&#039;s how to use it:

&lt;details&gt;
  &lt;summary&gt; &lt;b&gt; Usage Example &lt;/b&gt;&lt;/summary&gt;

```python
# Create query parameters
query_param = QueryParam(
    mode=&quot;hybrid&quot;,  # or other mode: &quot;local&quot;, &quot;global&quot;, &quot;hybrid&quot;, &quot;mix&quot; and &quot;naive&quot;
)

# Example 1: Using the default system prompt
response_default = rag.query(
    &quot;What are the primary benefits of renewable energy?&quot;,
    param=query_param
)
print(response_default)

# Example 2: Using a custom prompt
custom_prompt = &quot;&quot;&quot;
You are an expert assistant in environmental science. Provide detailed and structured answers with examples.
---Conversation History---
{history}

---Knowledge Base---
{context_data}

---Response Rules---

- Target format and length: {response_type}
&quot;&quot;&quot;
response_custom = rag.query(
    &quot;What are the primary benefits of ren

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jiji262/douyin-downloader]]></title>
            <link>https://github.com/jiji262/douyin-downloader</link>
            <guid>https://github.com/jiji262/douyin-downloader</guid>
            <pubDate>Thu, 10 Apr 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[æŠ–éŸ³æ‰¹é‡ä¸‹è½½å·¥å…·ï¼Œå»æ°´å°ï¼Œæ”¯æŒè§†é¢‘ã€å›¾é›†ã€åˆé›†ã€éŸ³ä¹(åŸå£°)ã€‚å…è´¹ï¼å…è´¹ï¼å…è´¹ï¼]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jiji262/douyin-downloader">jiji262/douyin-downloader</a></h1>
            <p>æŠ–éŸ³æ‰¹é‡ä¸‹è½½å·¥å…·ï¼Œå»æ°´å°ï¼Œæ”¯æŒè§†é¢‘ã€å›¾é›†ã€åˆé›†ã€éŸ³ä¹(åŸå£°)ã€‚å…è´¹ï¼å…è´¹ï¼å…è´¹ï¼</p>
            <p>Language: Python</p>
            <p>Stars: 2,925</p>
            <p>Forks: 432</p>
            <p>Stars today: 57 stars today</p>
            <h2>README</h2><pre># DouYin Downloader

DouYin Downloader æ˜¯ä¸€ä¸ªç”¨äºæ‰¹é‡ä¸‹è½½æŠ–éŸ³å†…å®¹çš„å·¥å…·ã€‚åŸºäºæŠ–éŸ³ API å®ç°ï¼Œæ”¯æŒå‘½ä»¤è¡Œå‚æ•°æˆ– YAML é…ç½®æ–‡ä»¶æ–¹å¼è¿è¡Œï¼Œå¯æ»¡è¶³å¤§éƒ¨åˆ†æŠ–éŸ³å†…å®¹çš„ä¸‹è½½éœ€æ±‚ã€‚

## âœ¨ ç‰¹æ€§

- **å¤šç§å†…å®¹æ”¯æŒ**
  - è§†é¢‘ã€å›¾é›†ã€éŸ³ä¹ã€ç›´æ’­ä¿¡æ¯ä¸‹è½½
  - æ”¯æŒä¸ªäººä¸»é¡µã€ä½œå“åˆ†äº«ã€ç›´æ’­ã€åˆé›†ã€éŸ³ä¹é›†åˆç­‰å¤šç§é“¾æ¥
  - æ”¯æŒå»æ°´å°ä¸‹è½½
  
- **æ‰¹é‡ä¸‹è½½èƒ½åŠ›**
  - å¤šçº¿ç¨‹å¹¶å‘ä¸‹è½½
  - æ”¯æŒå¤šé“¾æ¥æ‰¹é‡ä¸‹è½½
  - è‡ªåŠ¨è·³è¿‡å·²ä¸‹è½½å†…å®¹
  
- **çµæ´»é…ç½®**
  - æ”¯æŒå‘½ä»¤è¡Œå‚æ•°å’Œé…ç½®æ–‡ä»¶ä¸¤ç§æ–¹å¼
  - å¯è‡ªå®šä¹‰ä¸‹è½½è·¯å¾„ã€çº¿ç¨‹æ•°ç­‰
  - æ”¯æŒä¸‹è½½æ•°é‡é™åˆ¶
  
- **å¢é‡æ›´æ–°**
  - æ”¯æŒä¸»é¡µä½œå“å¢é‡æ›´æ–°
  - æ”¯æŒæ•°æ®æŒä¹…åŒ–åˆ°æ•°æ®åº“
  - å¯æ ¹æ®æ—¶é—´èŒƒå›´è¿‡æ»¤

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

1. å®‰è£… Python ä¾èµ–ï¼š
```bash
pip install -r requirements.txt
```

2. å¤åˆ¶é…ç½®æ–‡ä»¶ï¼š
```bash
cp config.example.yml config.yml
```

### é…ç½®

ç¼–è¾‘ `config.yml` æ–‡ä»¶ï¼Œè®¾ç½®ï¼š
- ä¸‹è½½é“¾æ¥
- ä¿å­˜è·¯å¾„
- Cookie ä¿¡æ¯ï¼ˆä»æµè§ˆå™¨å¼€å‘è€…å·¥å…·è·å–ï¼‰
- å…¶ä»–ä¸‹è½½é€‰é¡¹

### è¿è¡Œ

**æ–¹å¼ä¸€ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶ï¼ˆæ¨èï¼‰**
```bash
python DouYinCommand.py
```

**æ–¹å¼äºŒï¼šä½¿ç”¨å‘½ä»¤è¡Œ**
```bash
python DouYinCommand.py -C True -l &quot;æŠ–éŸ³åˆ†äº«é“¾æ¥&quot; -p &quot;ä¸‹è½½è·¯å¾„&quot;
```

## ä½¿ç”¨äº¤æµç¾¤

![fuye](img/fuye.png)

## ä½¿ç”¨æˆªå›¾

![DouYinCommand1](img/DouYinCommand1.png)
![DouYinCommand2](img/DouYinCommand2.png)
![DouYinCommand download](img/DouYinCommanddownload.jpg)
![DouYinCommand download detail](img/DouYinCommanddownloaddetail.jpg)

## ğŸ“ æ”¯æŒçš„é“¾æ¥ç±»å‹

- ä½œå“åˆ†äº«é“¾æ¥ï¼š`https://v.douyin.com/xxx/`
- ä¸ªäººä¸»é¡µï¼š`https://www.douyin.com/user/xxx`
- å•ä¸ªè§†é¢‘ï¼š`https://www.douyin.com/video/xxx`
- å›¾é›†ï¼š`https://www.douyin.com/note/xxx`
- åˆé›†ï¼š`https://www.douyin.com/collection/xxx`
- éŸ³ä¹åŸå£°ï¼š`https://www.douyin.com/music/xxx`
- ç›´æ’­ï¼š`https://live.douyin.com/xxx`

## ğŸ› ï¸ é«˜çº§ç”¨æ³•

### å‘½ä»¤è¡Œå‚æ•°

åŸºç¡€å‚æ•°ï¼š
```
-C, --cmd            ä½¿ç”¨å‘½ä»¤è¡Œæ¨¡å¼
-l, --link          ä¸‹è½½é“¾æ¥
-p, --path          ä¿å­˜è·¯å¾„
-t, --thread        çº¿ç¨‹æ•°ï¼ˆé»˜è®¤5ï¼‰
```

ä¸‹è½½é€‰é¡¹ï¼š
```
-m, --music         ä¸‹è½½éŸ³ä¹ï¼ˆé»˜è®¤Trueï¼‰
-c, --cover         ä¸‹è½½å°é¢ï¼ˆé»˜è®¤Trueï¼‰
-a, --avatar        ä¸‹è½½å¤´åƒï¼ˆé»˜è®¤Trueï¼‰
-j, --json          ä¿å­˜JSONæ•°æ®ï¼ˆé»˜è®¤Trueï¼‰
```

æ›´å¤šå‚æ•°è¯´æ˜è¯·ä½¿ç”¨ `-h` æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯ã€‚

### ç¤ºä¾‹å‘½ä»¤

1. ä¸‹è½½å•ä¸ªè§†é¢‘ï¼š
```bash
python DouYinCommand.py -C True -l &quot;https://v.douyin.com/xxx/&quot;
```

2. ä¸‹è½½ä¸»é¡µä½œå“ï¼š
```bash
python DouYinCommand.py -C True -l &quot;https://v.douyin.com/xxx/&quot; -M post
```

3. æ‰¹é‡ä¸‹è½½ï¼š
```bash
python DouYinCommand.py -C True -l &quot;é“¾æ¥1&quot; -l &quot;é“¾æ¥2&quot; -p &quot;./downloads&quot;
```

æ›´å¤šç¤ºä¾‹è¯·å‚è€ƒ[ä½¿ç”¨ç¤ºä¾‹æ–‡æ¡£](docs/examples.md)ã€‚

## ğŸ“‹ æ³¨æ„äº‹é¡¹

1. æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨
2. ä½¿ç”¨å‰è¯·ç¡®ä¿å·²å®‰è£…æ‰€éœ€ä¾èµ–
3. Cookie ä¿¡æ¯éœ€è¦è‡ªè¡Œè·å–
4. å»ºè®®é€‚å½“è°ƒæ•´çº¿ç¨‹æ•°ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestã€‚

## ğŸ“œ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT](LICENSE) è®¸å¯è¯ã€‚

## ğŸ™ é¸£è°¢

- [TikTokDownload](https://github.com/Johnserf-Seed/TikTokDownload)
- æœ¬é¡¹ç›®ä½¿ç”¨äº† ChatGPT è¾…åŠ©å¼€å‘ï¼Œå¦‚æœ‰é—®é¢˜è¯·æ Issue

## ğŸ“Š Star History

[![Star History Chart](https://api.star-history.com/svg?repos=jiji262/douyin-downloader&amp;type=Date)](https://star-history.com/#jiji262/douyin-downloader&amp;Date)




# License

[MIT](https://opensource.org/licenses/MIT) 

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[LC044/WeChatMsg]]></title>
            <link>https://github.com/LC044/WeChatMsg</link>
            <guid>https://github.com/LC044/WeChatMsg</guid>
            <pubDate>Thu, 10 Apr 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[æå–å¾®ä¿¡èŠå¤©è®°å½•ï¼Œå°†å…¶å¯¼å‡ºæˆHTMLã€Wordã€Excelæ–‡æ¡£æ°¸ä¹…ä¿å­˜ï¼Œå¯¹èŠå¤©è®°å½•è¿›è¡Œåˆ†æç”Ÿæˆå¹´åº¦èŠå¤©æŠ¥å‘Šï¼Œç”¨èŠå¤©æ•°æ®è®­ç»ƒä¸“å±äºä¸ªäººçš„AIèŠå¤©åŠ©æ‰‹]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/LC044/WeChatMsg">LC044/WeChatMsg</a></h1>
            <p>æå–å¾®ä¿¡èŠå¤©è®°å½•ï¼Œå°†å…¶å¯¼å‡ºæˆHTMLã€Wordã€Excelæ–‡æ¡£æ°¸ä¹…ä¿å­˜ï¼Œå¯¹èŠå¤©è®°å½•è¿›è¡Œåˆ†æç”Ÿæˆå¹´åº¦èŠå¤©æŠ¥å‘Šï¼Œç”¨èŠå¤©æ•°æ®è®­ç»ƒä¸“å±äºä¸ªäººçš„AIèŠå¤©åŠ©æ‰‹</p>
            <p>Language: Python</p>
            <p>Stars: 38,914</p>
            <p>Forks: 4,009</p>
            <p>Stars today: 98 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[DataDog/integrations-core]]></title>
            <link>https://github.com/DataDog/integrations-core</link>
            <guid>https://github.com/DataDog/integrations-core</guid>
            <pubDate>Thu, 10 Apr 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[Core integrations of the Datadog Agent]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/DataDog/integrations-core">DataDog/integrations-core</a></h1>
            <p>Core integrations of the Datadog Agent</p>
            <p>Language: Python</p>
            <p>Stars: 1,000</p>
            <p>Forks: 1,453</p>
            <p>Stars today: 1 star today</p>
            <h2>README</h2><pre># Datadog Integrations - Core

| | |
| --- | --- |
| CI/CD | [![CI - Test][1]][2] [![CI - Coverage][17]][18] |
| Docs | [![Docs - Release][19]][20] |
| Meta | [![Hatch project][26]][27] [![Linting - Ruff][24]][25] [![Code style - black][21]][22] [![Typing - Mypy][28]][29] [![License - BSD-3-Clause][30]][31] |

This repository contains open source integrations that Datadog officially develops and supports.
To add a new integration, please see the [Integrations Extras][5] repository and the
[accompanying documentation][6].

The [Datadog Agent][7] packages are equipped with all the Agent integrations from this
repository, so to get started using them, you can simply [install the Agent][8]
for your operating system. The [AGENT_CHANGELOG](AGENT_CHANGELOG.md) file shows
which Integrations have been updated in each Agent version.

## Contributing

Working with integrations is easy, the main page of the [development docs][6]
contains all the info you need to get your dev environment up and running in minutes
to run, test and build a Check. More advanced documentation can be found [here][3].

## Reporting Issues

For more information on integrations, please reference our [documentation][11] and
[knowledge base][12]. You can also visit our [help page][13] to connect with us.


[1]: https://raw.githubusercontent.com/DataDog/integrations-core/badges/test-results.svg
[2]: https://github.com/DataDog/integrations-core/actions/workflows/master.yml
[3]: https://datadoghq.dev/integrations-core/
[5]: https://github.com/DataDog/integrations-extras
[6]: https://docs.datadoghq.com/developers/integrations/
[7]: https://github.com/DataDog/datadog-agent
[8]: https://app.datadoghq.com/account/settings/agent/latest
[9]: https://docs.pytest.org/en/latest/
[10]: https://packaging.python.org/tutorials/distributing-packages/
[11]: https://docs.datadoghq.com
[12]: https://help.datadoghq.com/hc/en-us
[13]: https://docs.datadoghq.com/help/
[15]: https://github.com/DataDog/integrations-core/blob/6.2.1/requirements-integration-core.txt
[16]: https://github.com/DataDog/integrations-core/blob/ea2dfbf1e8859333af4c8db50553eb72a3b466f9/requirements-agent-release.txt
[17]: https://codecov.io/github/DataDog/integrations-core/coverage.svg?branch=master
[18]: https://codecov.io/github/DataDog/integrations-core?branch=master
[19]: https://github.com/DataDog/integrations-core/workflows/docs/badge.svg
[20]: https://github.com/DataDog/integrations-core/actions?workflow=docs
[21]: https://img.shields.io/badge/code%20style-black-000000.svg
[22]: https://github.com/ambv/black
[24]: https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/charliermarsh/ruff/main/assets/badge/v0.json
[25]: https://github.com/charliermarsh/ruff
[26]: https://img.shields.io/badge/%F0%9F%A5%9A-Hatch-4051b5.svg
[27]: https://github.com/pypa/hatch
[28]: https://img.shields.io/badge/typing-Mypy-blue.svg
[29]: https://github.com/python/mypy
[30]: https://img.shields.io/badge/license-BSD--3--Clause-9400d3.svg
[31]: https://spdx.org/licenses/BSD-3-Clause.html</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[RockChinQ/LangBot]]></title>
            <link>https://github.com/RockChinQ/LangBot</link>
            <guid>https://github.com/RockChinQ/LangBot</guid>
            <pubDate>Thu, 10 Apr 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[ğŸ˜ç®€å•æ˜“ç”¨ã€ğŸ§©ä¸°å¯Œç”Ÿæ€ - å¤§æ¨¡å‹åŸç”Ÿå³æ—¶é€šä¿¡æœºå™¨äººå¹³å° | é€‚é… QQ / å¾®ä¿¡ï¼ˆä¼ä¸šå¾®ä¿¡ã€ä¸ªäººå¾®ä¿¡ï¼‰/ é£ä¹¦ / é’‰é’‰ / Discord / Telegram / Slack ç­‰å¹³å° | æ”¯æŒ ChatGPTã€DeepSeekã€Difyã€Claudeã€Geminiã€xAI Grokã€Ollamaã€LM Studioã€é˜¿é‡Œäº‘ç™¾ç‚¼ã€ç«å±±æ–¹èˆŸã€SiliconFlowã€Qwenã€Moonshotã€ChatGLMã€SillyTravenã€MCP ç­‰ LLM çš„æœºå™¨äºº / Agent | LLM-based instant messaging bots platform, supports Discord, Telegram, WeChat, Lark, DingTalk, QQ, Slack]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/RockChinQ/LangBot">RockChinQ/LangBot</a></h1>
            <p>ğŸ˜ç®€å•æ˜“ç”¨ã€ğŸ§©ä¸°å¯Œç”Ÿæ€ - å¤§æ¨¡å‹åŸç”Ÿå³æ—¶é€šä¿¡æœºå™¨äººå¹³å° | é€‚é… QQ / å¾®ä¿¡ï¼ˆä¼ä¸šå¾®ä¿¡ã€ä¸ªäººå¾®ä¿¡ï¼‰/ é£ä¹¦ / é’‰é’‰ / Discord / Telegram / Slack ç­‰å¹³å° | æ”¯æŒ ChatGPTã€DeepSeekã€Difyã€Claudeã€Geminiã€xAI Grokã€Ollamaã€LM Studioã€é˜¿é‡Œäº‘ç™¾ç‚¼ã€ç«å±±æ–¹èˆŸã€SiliconFlowã€Qwenã€Moonshotã€ChatGLMã€SillyTravenã€MCP ç­‰ LLM çš„æœºå™¨äºº / Agent | LLM-based instant messaging bots platform, supports Discord, Telegram, WeChat, Lark, DingTalk, QQ, Slack</p>
            <p>Language: Python</p>
            <p>Stars: 10,375</p>
            <p>Forks: 759</p>
            <p>Stars today: 35 stars today</p>
            <h2>README</h2><pre>
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://langbot.app&quot;&gt;
&lt;img src=&quot;https://docs.langbot.app/social.png&quot; alt=&quot;LangBot&quot;/&gt;
&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://trendshift.io/repositories/12901&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12901&quot; alt=&quot;RockChinQ%2FLangBot | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;a href=&quot;https://docs.langbot.app&quot;&gt;é¡¹ç›®ä¸»é¡µ&lt;/a&gt; ï½œ
&lt;a href=&quot;https://docs.langbot.app/insight/intro.html&quot;&gt;åŠŸèƒ½ä»‹ç»&lt;/a&gt; ï½œ
&lt;a href=&quot;https://docs.langbot.app/insight/guide.html&quot;&gt;éƒ¨ç½²æ–‡æ¡£&lt;/a&gt; ï½œ
&lt;a href=&quot;https://docs.langbot.app/usage/faq.html&quot;&gt;å¸¸è§é—®é¢˜&lt;/a&gt; ï½œ
&lt;a href=&quot;https://docs.langbot.app/plugin/plugin-intro.html&quot;&gt;æ’ä»¶ä»‹ç»&lt;/a&gt; ï½œ
&lt;a href=&quot;https://github.com/RockChinQ/LangBot/issues/new?assignees=&amp;labels=%E7%8B%AC%E7%AB%8B%E6%8F%92%E4%BB%B6&amp;projects=&amp;template=submit-plugin.yml&amp;title=%5BPlugin%5D%3A+%E8%AF%B7%E6%B1%82%E7%99%BB%E8%AE%B0%E6%96%B0%E6%8F%92%E4%BB%B6&quot;&gt;æäº¤æ’ä»¶&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
ğŸ˜é«˜ç¨³å®šã€ğŸ§©æ”¯æŒæ‰©å±•ã€ğŸ¦„å¤šæ¨¡æ€ - å¤§æ¨¡å‹åŸç”Ÿå³æ—¶é€šä¿¡æœºå™¨äººå¹³å°ğŸ¤–  
&lt;/div&gt;

&lt;br/&gt;


[![Discord](https://img.shields.io/discord/1335141740050649118?logo=discord&amp;labelColor=%20%235462eb&amp;logoColor=%20%23f5f5f5&amp;color=%20%235462eb)](https://discord.gg/wdNEHETs87)
[![QQ Group](https://img.shields.io/badge/%E7%A4%BE%E5%8C%BAQQ%E7%BE%A4-966235608-blue)](https://qm.qq.com/q/JLi38whHum)
[![GitHub release (latest by date)](https://img.shields.io/github/v/release/RockChinQ/LangBot)](https://github.com/RockChinQ/LangBot/releases/latest)
 ![Dynamic JSON Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.qchatgpt.rockchin.top%2Fapi%2Fv2%2Fview%2Frealtime%2Fcount_query%3Fminute%3D10080&amp;query=%24.data.count&amp;label=%E4%BD%BF%E7%94%A8%E9%87%8F%EF%BC%887%E6%97%A5%EF%BC%89)
&lt;img src=&quot;https://img.shields.io/badge/python-3.10 ~ 3.13 -blue.svg&quot; alt=&quot;python&quot;&gt;
[![star](https://gitcode.com/RockChinQ/LangBot/star/badge.svg)](https://gitcode.com/RockChinQ/LangBot)

[ç®€ä½“ä¸­æ–‡](README.md) / [English](README_EN.md) / [æ—¥æœ¬èª](README_JP.md)

&lt;/div&gt;

&lt;/p&gt;

## âœ¨ ç‰¹æ€§

- ğŸ’¬ å¤§æ¨¡å‹å¯¹è¯ã€Agentï¼šæ”¯æŒå¤šç§å¤§æ¨¡å‹ï¼Œé€‚é…ç¾¤èŠå’Œç§èŠï¼›å…·æœ‰å¤šè½®å¯¹è¯ã€å·¥å…·è°ƒç”¨ã€å¤šæ¨¡æ€èƒ½åŠ›ï¼Œå¹¶æ·±åº¦é€‚é… [Dify](https://dify.ai)ã€‚ç›®å‰æ”¯æŒ QQã€QQé¢‘é“ã€ä¼ä¸šå¾®ä¿¡ã€ä¸ªäººå¾®ä¿¡ã€é£ä¹¦ã€Discordã€Telegram ç­‰å¹³å°ã€‚
- ğŸ› ï¸ é«˜ç¨³å®šæ€§ã€åŠŸèƒ½å®Œå¤‡ï¼šåŸç”Ÿæ”¯æŒè®¿é—®æ§åˆ¶ã€é™é€Ÿã€æ•æ„Ÿè¯è¿‡æ»¤ç­‰æœºåˆ¶ï¼›é…ç½®ç®€å•ï¼Œæ”¯æŒå¤šç§éƒ¨ç½²æ–¹å¼ã€‚
- ğŸ§© æ’ä»¶æ‰©å±•ã€æ´»è·ƒç¤¾åŒºï¼šæ”¯æŒäº‹ä»¶é©±åŠ¨ã€ç»„ä»¶æ‰©å±•ç­‰æ’ä»¶æœºåˆ¶ï¼›é€‚é… Anthropic [MCP åè®®](https://modelcontextprotocol.io/)ï¼›ç›®å‰å·²æœ‰æ•°åä¸ª[æ’ä»¶](https://docs.langbot.app/plugin/plugin-intro.html)
- ğŸ˜» [New] Web ç®¡ç†é¢æ¿ï¼šæ”¯æŒé€šè¿‡æµè§ˆå™¨ç®¡ç† LangBot å®ä¾‹ï¼Œå…·ä½“æ”¯æŒåŠŸèƒ½ï¼ŒæŸ¥çœ‹[æ–‡æ¡£](https://docs.langbot.app/webui/intro.html)

## ğŸ“¦ å¼€å§‹ä½¿ç”¨

&gt; [!IMPORTANT]
&gt;
&gt; åœ¨æ‚¨å¼€å§‹ä»»ä½•æ–¹å¼éƒ¨ç½²ä¹‹å‰ï¼Œè¯·åŠ¡å¿…é˜…è¯»[æ–°æ‰‹æŒ‡å¼•](https://docs.langbot.app/insight/guide.html)ã€‚

#### Docker Compose éƒ¨ç½²

é€‚åˆç†Ÿæ‚‰ Docker çš„ç”¨æˆ·ï¼ŒæŸ¥çœ‹æ–‡æ¡£[Docker éƒ¨ç½²](https://docs.langbot.app/deploy/langbot/docker.html)ã€‚

#### å®å¡”é¢æ¿éƒ¨ç½²

å·²ä¸Šæ¶å®å¡”é¢æ¿ï¼Œè‹¥æ‚¨å·²å®‰è£…å®å¡”é¢æ¿ï¼Œå¯ä»¥æ ¹æ®[æ–‡æ¡£](https://docs.langbot.app/deploy/langbot/one-click/bt.html)ä½¿ç”¨ã€‚

#### Zeabur äº‘éƒ¨ç½²

ç¤¾åŒºè´¡çŒ®çš„ Zeabur æ¨¡æ¿ã€‚

[![Deploy on Zeabur](https://zeabur.com/button.svg)](https://zeabur.com/zh-CN/templates/ZKTBDH)

#### Railway äº‘éƒ¨ç½²

[![Deploy on Railway](https://railway.com/button.svg)](https://railway.app/template/yRrAyL?referralCode=vogKPF)

#### æ‰‹åŠ¨éƒ¨ç½²

ç›´æ¥ä½¿ç”¨å‘è¡Œç‰ˆè¿è¡Œï¼ŒæŸ¥çœ‹æ–‡æ¡£[æ‰‹åŠ¨éƒ¨ç½²](https://docs.langbot.app/deploy/langbot/manual.html)ã€‚

## ğŸ“¸ æ•ˆæœå±•ç¤º

&lt;img alt=&quot;å›å¤æ•ˆæœï¼ˆå¸¦æœ‰è”ç½‘æ’ä»¶ï¼‰&quot; src=&quot;https://docs.langbot.app/QChatGPT-0516.png&quot; width=&quot;500px&quot;/&gt;

- WebUI Demo: https://demo.langbot.dev/
    - ç™»å½•ä¿¡æ¯ï¼šé‚®ç®±ï¼š`demo@langbot.app` å¯†ç ï¼š`langbot123456`
    - æ³¨æ„ï¼šä»…å±•ç¤ºwebuiæ•ˆæœï¼Œå…¬å¼€ç¯å¢ƒï¼Œè¯·ä¸è¦åœ¨å…¶ä¸­å¡«å…¥æ‚¨çš„ä»»ä½•æ•æ„Ÿä¿¡æ¯ã€‚

## ğŸ”Œ ç»„ä»¶å…¼å®¹æ€§

### æ¶ˆæ¯å¹³å°

| å¹³å° | çŠ¶æ€ | å¤‡æ³¨ |
| --- | --- | --- |
| QQ ä¸ªäººå· | âœ… | QQ ä¸ªäººå·ç§èŠã€ç¾¤èŠ |
| QQ å®˜æ–¹æœºå™¨äºº | âœ… | QQ å®˜æ–¹æœºå™¨äººï¼Œæ”¯æŒé¢‘é“ã€ç§èŠã€ç¾¤èŠ |
| ä¼ä¸šå¾®ä¿¡ | âœ… |  |
| ä¸ªäººå¾®ä¿¡ | âœ… | ä½¿ç”¨ [Gewechat](https://github.com/Devo919/Gewechat) æ¥å…¥ |
| å¾®ä¿¡å…¬ä¼—å· | âœ… |  |
| é£ä¹¦ | âœ… |  |
| é’‰é’‰ | âœ… |  |
| Discord | âœ… |  |
| Telegram | âœ… |  |
| Slack | âœ… |  |
| LINE | ğŸš§ |  |
| WhatsApp | ğŸš§ |  |

ğŸš§: æ­£åœ¨å¼€å‘ä¸­

### å¤§æ¨¡å‹èƒ½åŠ›

| æ¨¡å‹ | çŠ¶æ€ | å¤‡æ³¨ |
| --- | --- | --- |
| [OpenAI](https://platform.openai.com/) | âœ… | å¯æ¥å…¥ä»»ä½• OpenAI æ¥å£æ ¼å¼æ¨¡å‹ |
| [DeepSeek](https://www.deepseek.com/) | âœ… |  |
| [Moonshot](https://www.moonshot.cn/) | âœ… |  |
| [Anthropic](https://www.anthropic.com/) | âœ… |  |
| [xAI](https://x.ai/) | âœ… |  |
| [æ™ºè°±AI](https://open.bigmodel.cn/) | âœ… |  |
| [Dify](https://dify.ai) | âœ… | LLMOps å¹³å° |
| [Ollama](https://ollama.com/) | âœ… | æœ¬åœ°å¤§æ¨¡å‹è¿è¡Œå¹³å° |
| [LMStudio](https://lmstudio.ai/) | âœ… | æœ¬åœ°å¤§æ¨¡å‹è¿è¡Œå¹³å° |
| [GiteeAI](https://ai.gitee.com/) | âœ… | å¤§æ¨¡å‹æ¥å£èšåˆå¹³å° |
| [SiliconFlow](https://siliconflow.cn/) | âœ… | å¤§æ¨¡å‹èšåˆå¹³å° |
| [é˜¿é‡Œäº‘ç™¾ç‚¼](https://bailian.console.aliyun.com/) | âœ… | å¤§æ¨¡å‹èšåˆå¹³å°, LLMOps å¹³å° |
| [ç«å±±æ–¹èˆŸ](https://console.volcengine.com/ark/region:ark+cn-beijing/model?vendor=Bytedance&amp;view=LIST_VIEW) | âœ… | å¤§æ¨¡å‹èšåˆå¹³å°, LLMOps å¹³å° |
| [ModelScope](https://modelscope.cn/docs/model-service/API-Inference/intro) | âœ… | å¤§æ¨¡å‹èšåˆå¹³å° |
| [MCP](https://modelcontextprotocol.io/) | âœ… | æ”¯æŒé€šè¿‡ MCP åè®®è·å–å·¥å…· |

### TTS

| å¹³å°/æ¨¡å‹ | å¤‡æ³¨ |
| --- | --- |
| [FishAudio](https://fish.audio/zh-CN/discovery/) | [æ’ä»¶](https://github.com/the-lazy-me/NewChatVoice) |
| [æµ·è±š AI](https://www.ttson.cn/?source=thelazy) | [æ’ä»¶](https://github.com/the-lazy-me/NewChatVoice) |
| [AzureTTS](https://portal.azure.com/) | [æ’ä»¶](https://github.com/Ingnaryk/LangBot_AzureTTS) |

### æ–‡ç”Ÿå›¾

| å¹³å°/æ¨¡å‹ | å¤‡æ³¨ |
| --- | --- |
| é˜¿é‡Œäº‘ç™¾ç‚¼ | [æ’ä»¶](https://github.com/Thetail001/LangBot_BailianTextToImagePlugin)

## ğŸ˜˜ ç¤¾åŒºè´¡çŒ®

æ„Ÿè°¢ä»¥ä¸‹[ä»£ç è´¡çŒ®è€…](https://github.com/RockChinQ/LangBot/graphs/contributors)å’Œç¤¾åŒºé‡Œå…¶ä»–æˆå‘˜å¯¹ LangBot çš„è´¡çŒ®ï¼š

&lt;a href=&quot;https://github.com/RockChinQ/LangBot/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=RockChinQ/LangBot&quot; /&gt;
&lt;/a&gt;

ä»¥åŠ LangBot æ ¸å¿ƒå›¢é˜Ÿæˆå‘˜ï¼š

- [RockChinQ](https://github.com/RockChinQ)
- [the-lazy-me](https://github.com/the-lazy-me)
- [wangcham](https://github.com/wangcham)
- [KaedeSAMA](https://github.com/KaedeSAMA)</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[agno-agi/agno]]></title>
            <link>https://github.com/agno-agi/agno</link>
            <guid>https://github.com/agno-agi/agno</guid>
            <pubDate>Thu, 10 Apr 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[Build Reasoning Agents with memory, knowledge, tools and native multi-modal support.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/agno-agi/agno">agno-agi/agno</a></h1>
            <p>Build Reasoning Agents with memory, knowledge, tools and native multi-modal support.</p>
            <p>Language: Python</p>
            <p>Stars: 24,445</p>
            <p>Forks: 3,110</p>
            <p>Stars today: 138 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; id=&quot;top&quot;&gt;
  &lt;a href=&quot;https://docs.agno.com&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-dark.svg&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-light.svg&quot;&gt;
      &lt;img src=&quot;https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-light.svg&quot; alt=&quot;Agno&quot;&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://docs.agno.com&quot;&gt;ğŸ“š Documentation&lt;/a&gt; &amp;nbsp;|&amp;nbsp;
  &lt;a href=&quot;https://docs.agno.com/examples/introduction&quot;&gt;ğŸ’¡ Examples&lt;/a&gt; &amp;nbsp;|&amp;nbsp;
  &lt;a href=&quot;https://github.com/agno-agi/agno/stargazers&quot;&gt;ğŸŒŸ Star Us&lt;/a&gt;
&lt;/div&gt;

## Introduction

[Agno](https://docs.agno.com) is a lightweight library for building Reasoning Agents with memory, knowledge, tools and native multi-modal support. Use Agno to build Reasoning Agents, Multi-Modal Agents, Teams of Agents and Agentic Workflows.

Here&#039;s an Agent that writes a financial report by reasoning through each step:

```python reasoning_finance_agent.py
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.reasoning import ReasoningTools
from agno.tools.yfinance import YFinanceTools

agent = Agent(
    model=Claude(id=&quot;claude-3-7-sonnet-latest&quot;),
    tools=[
        ReasoningTools(add_instructions=True),
        YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True, company_news=True),
    ],
    instructions=[
        &quot;Use tables to display data&quot;,
        &quot;Only output the report, no other text&quot;,
    ],
    markdown=True,
)
agent.print_response(&quot;Write a report on NVDA&quot;, stream=True, show_full_reasoning=True, stream_intermediate_steps=True)
```
https://github.com/user-attachments/assets/bbb99955-9848-49a9-9732-3e19d77b2ff8

## Key features

Agno is simple, fast and model-agnostic. Here are the key features:

- **Lightning Fast**: Agents instantiate 10,000x faster than LangGraph and use 50x less memory (see [performance](#performance)).
- **Model Agnostic**: Use any model, any provider, no lock-in.
- **Reasoning Agents**: Build best in class reasoning agents using Reasoning Models, Reasoning Tools or our custom `CoT+Tool-use` approach.
- **Natively Multi Modal**: Built in support for text, image, audio and video.
- **Multi Agent Teams**: Industry leading multi-agent architecture with 3 different modes: `route`, `collaborate` and `coordinate`.
- **Long-term Memory**: Built in support for long-term memory with our `Storage` and `Memory` classes.
- **Domain Knowledge**: Add domain knowledge to your Agents with our `Knowledge` classes. Fully async and highly performant.
- **Structured Outputs**: First class support for structured outputs using native structured outputs or `json_mode`.
- **Monitoring**: Track agent sessions and performance in real-time on [agno.com](https://app.agno.com).

## Getting Started with Agno

- Start by [building your first Agent](https://docs.agno.com/introduction/agents)
- Check out the [examples](https://docs.agno.com/examples/introduction)
- Read the [documentation](https://docs.agno.com)

## Installation

```shell
pip install -U agno
```

## What are Agents?

**Agents** are intelligent programs that solve problems autonomously.

Agents have memory, domain knowledge and the ability to use tools (like searching the web, querying a database, making API calls). Unlike traditional programs that follow a predefined execution path, Agents dynamically adapt their approach based on the context and tool results.

Instead of a rigid binary definition, let&#039;s think of Agents in terms of agency and autonomy.
- **Level 0**: Agents with no tools (basic inference tasks).
- **Level 1**: Agents with tools for autonomous task execution.
- **Level 2**: Agents with knowledge, combining memory and reasoning.
- **Level 3**: Teams of specialized agents collaborating on complex workflows.

## Example - Basic Agent

The simplest Agent is just an inference task, no tools, no memory, no knowledge.

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    description=&quot;You are an enthusiastic news reporter with a flair for storytelling!&quot;,
    markdown=True
)
agent.print_response(&quot;Tell me about a breaking news story from New York.&quot;, stream=True)
```

To run the agent, install dependencies and export your `OPENAI_API_KEY`.

```shell
pip install agno openai

export OPENAI_API_KEY=sk-xxxx

python basic_agent.py
```

[View this example in the cookbook](./cookbook/getting_started/01_basic_agent.py)

## Example - Agent with tools

This basic agent will obviously make up a story, lets give it a tool to search the web.

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    description=&quot;You are an enthusiastic news reporter with a flair for storytelling!&quot;,
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True
)
agent.print_response(&quot;Tell me about a breaking news story from New York.&quot;, stream=True)
```

Install dependencies and run the Agent:

```shell
pip install duckduckgo-search

python agent_with_tools.py
```

Now you should see a much more relevant result.

[View this example in the cookbook](./cookbook/getting_started/02_agent_with_tools.py)

## Example - Agent with knowledge

Agents can store knowledge in a vector database and use it for RAG or dynamic few-shot learning.

**Agno agents use Agentic RAG** by default, which means they will search their knowledge base for the specific information they need to achieve their task.

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.embedder.openai import OpenAIEmbedder
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.lancedb import LanceDb, SearchType

agent = Agent(
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    description=&quot;You are a Thai cuisine expert!&quot;,
    instructions=[
        &quot;Search your knowledge base for Thai recipes.&quot;,
        &quot;If the question is better suited for the web, search the web to fill in gaps.&quot;,
        &quot;Prefer the information in your knowledge base over the web results.&quot;
    ],
    knowledge=PDFUrlKnowledgeBase(
        urls=[&quot;https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf&quot;],
        vector_db=LanceDb(
            uri=&quot;tmp/lancedb&quot;,
            table_name=&quot;recipes&quot;,
            search_type=SearchType.hybrid,
            embedder=OpenAIEmbedder(id=&quot;text-embedding-3-small&quot;),
        ),
    ),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True
)

# Comment out after the knowledge base is loaded
if agent.knowledge is not None:
    agent.knowledge.load()

agent.print_response(&quot;How do I make chicken and galangal in coconut milk soup&quot;, stream=True)
agent.print_response(&quot;What is the history of Thai curry?&quot;, stream=True)
```

Install dependencies and run the Agent:

```shell
pip install lancedb tantivy pypdf duckduckgo-search

python agent_with_knowledge.py
```

[View this example in the cookbook](./cookbook/getting_started/03_agent_with_knowledge.py)

## Example - Multi Agent Teams

Agents work best when they have a singular purpose, a narrow scope and a small number of tools. When the number of tools grows beyond what the language model can handle or the tools belong to different categories, use a team of agents to spread the load.

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools
from agno.team import Team

web_agent = Agent(
    name=&quot;Web Agent&quot;,
    role=&quot;Search the web for information&quot;,
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    tools=[DuckDuckGoTools()],
    instructions=&quot;Always include sources&quot;,
    show_tool_calls=True,
    markdown=True,
)

finance_agent = Agent(
    name=&quot;Finance Agent&quot;,
    role=&quot;Get financial data&quot;,
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    tools=[YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)],
    instructions=&quot;Use tables to display data&quot;,
    show_tool_calls=True,
    markdown=True,
)

agent_team = Team(
    mode=&quot;coordinate&quot;,
    members=[web_agent, finance_agent],
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    success_criteria=&quot;A comprehensive financial news report with clear sections and data-driven insights.&quot;,
    instructions=[&quot;Always include sources&quot;, &quot;Use tables to display data&quot;],
    show_tool_calls=True,
    markdown=True,
)

agent_team.print_response(&quot;What&#039;s the market outlook and financial performance of AI semiconductor companies?&quot;, stream=True)
```

Install dependencies and run the Agent team:

```shell
pip install duckduckgo-search yfinance

python agent_team.py
```

[View this example in the cookbook](./cookbook/getting_started/05_agent_team.py)

## ğŸš¨ Global Agent Hackathon! ğŸš¨

We&#039;re thrilled to announce a month long, open source AI Agent Hackathon â€” open to all builders and dreamers working on agents, RAG, tool use, and multi-agent systems.

### ğŸ’° Build something extordinary, win up to $20,000 in cash

We&#039;re giving away $20,000 in prizes for the most ambitious Agent projects

- ğŸ… 10 winners: $300 each
- ğŸ¥‰ 10 winners: $500 each
- ğŸ¥ˆ 5 winners: $1,000 each
- ğŸ¥‡ 1 winner: $2,000
- ğŸ† GRAND PRIZE: $5,000 ğŸ†

&gt; Follow this [post](https://www.agno.com/blog/agent-hackathon-april-2025) for more details and updates

### ğŸ¤ Want to partner or judge?

If you&#039;re building in the AI Agent space, or want to help shape the next generation of Agent builders - we&#039;d love to work with you.

Reach out to support@agno.com to get involved.

## Performance

At Agno, we&#039;re obsessed with performance. Why? because even simple AI workflows can spawn thousands of Agents to achieve their goals. Scale that to a modest number of users and performance becomes a bottleneck. Agno is designed to power high performance agentic systems:

- Agent instantiation: ~2Î¼s on average (~10,000x faster than LangGraph).
- Memory footprint: ~3.75Kib on average (~50x less memory than LangGraph).

&gt; Tested on an Apple M4 Mackbook Pro.

While an Agent&#039;s run-time is bottlenecked by inference, we must do everything possible to minimize execution time, reduce memory usage, and parallelize tool calls. These numbers may seem trivial at first, but our experience shows that they add up even at a reasonably small scale.

### Instantiation time

Let&#039;s measure the time it takes for an Agent with 1 tool to start up. We&#039;ll run the evaluation 1000 times to get a baseline measurement.

You should run the evaluation yourself on your own machine, please, do not take these results at face value.

```shell
# Setup virtual environment
./scripts/perf_setup.sh
source .venvs/perfenv/bin/activate
# OR Install dependencies manually
# pip install openai agno langgraph langchain_openai

# Agno
python evals/performance/instantiation_with_tool.py

# LangGraph
python evals/performance/other/langgraph_instantiation.py
```

&gt; The following evaluation is run on an Apple M4 Mackbook Pro. It also runs as a Github action on this repo.

LangGraph is on the right, **let&#039;s start it first and give it a head start**.

Agno is on the left, notice how it finishes before LangGraph gets 1/2 way through the runtime measurement, and hasn&#039;t even started the memory measurement. That&#039;s how fast Agno is.

https://github.com/user-attachments/assets/ba466d45-75dd-45ac-917b-0a56c5742e23

Dividing the average time of a Langgraph Agent by the average time of an Agno Agent:

```
0.020526s / 0.000002s ~ 10,263
```

In this particular run, **Agno Agents startup is roughly 10,000 times faster than Langgraph Agents**. The numbers continue to favor Agno as the number of tools grow, and we add memory and knowledge stores.

### Memory usage

To measure memory usage, we use the `tracemalloc` library. We first calculate a baseline memory usage by running an empty function, then run the Agent 1000x times and calculate the difference. This gives a (reasonably) isolated measurement of the memory usage of the Agent.

We recommend running the evaluation yourself on your own machine, and digging into the code to see how it works. If we&#039;ve made a mistake, please let us know.

Dividing the average memory usage of a Langgraph Agent by the average memory usage of an Agno Agent:

```
0.137273/0.002528 ~ 54.3
```

**Langgraph Agents use ~50x more memory than Agno Agents**. In our opinion, memory usage is a much more important metric than instantiation time. As we start running thousands of Agents in production, these numbers directly start affecting the cost of running the Agents.

### Conclusion

Agno agents are designed for performance and while we do share some benchmarks against other frameworks, we should be mindful that accuracy and reliability are more important than speed.

We&#039;ll be publishing accuracy and reliability benchmarks running on Github actions in the coming weeks. Given that each framework is different and we won&#039;t be able to tune their performance like we do with Agno, for future benchmarks we&#039;ll only be comparing against ourselves.

## Cursor Setup

When building Agno agents, using Agno documentation as a source in Cursor is a great way to speed up your development.

1. In Cursor, go to the settings or preferences section.
2. Find the section to manage documentation sources.
3. Add `https://docs.agno.com` to the list of documentation URLs.
4. Save the changes.

Now, Cursor will have access to the Agno documentation.

## Documentation, Community &amp; More examples

- Docs: &lt;a href=&quot;https://docs.agno.com&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;docs.agno.com&lt;/a&gt;
- Getting Started Examples: &lt;a href=&quot;https://github.com/agno-agi/agno/tree/main/cookbook/getting_started&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Getting Started Cookbook&lt;/a&gt;
- All Examples: &lt;a href=&quot;https://github.com/agno-agi/agno/tree/main/cookbook&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Cookbook&lt;/a&gt;
- Community forum: &lt;a href=&quot;https://community.agno.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;community.agno.com&lt;/a&gt;
- Chat: &lt;a href=&quot;https://discord.gg/4MtYHHrgA8&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;discord&lt;/a&gt;

## Contributions

We welcome contributions, read our [contributing guide](https://github.com/agno-agi/agno/blob/main/CONTRIBUTING.md) to get started.

## Telemetry

Agno logs which model an agent used so we can prioritize updates to the most popular providers. You can disable this by setting `AGNO_TELEMETRY=false` in your environment.

&lt;p align=&quot;left&quot;&gt;
  &lt;a href=&quot;#top&quot;&gt;â¬†ï¸ Back to Top&lt;/a&gt;
&lt;/p&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[squidfunk/mkdocs-material]]></title>
            <link>https://github.com/squidfunk/mkdocs-material</link>
            <guid>https://github.com/squidfunk/mkdocs-material</guid>
            <pubDate>Thu, 10 Apr 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[Documentation that simply works]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/squidfunk/mkdocs-material">squidfunk/mkdocs-material</a></h1>
            <p>Documentation that simply works</p>
            <p>Language: Python</p>
            <p>Stars: 22,789</p>
            <p>Forks: 3,712</p>
            <p>Stars today: 22 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://squidfunk.github.io/mkdocs-material/&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/logo.svg&quot; width=&quot;320&quot; alt=&quot;Material for MkDocs&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;
    A powerful documentation framework on top of
    &lt;a href=&quot;https://www.mkdocs.org/&quot;&gt;MkDocs&lt;/a&gt;
  &lt;/strong&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/squidfunk/mkdocs-material/actions&quot;&gt;&lt;img
    src=&quot;https://github.com/squidfunk/mkdocs-material/workflows/build/badge.svg&quot;
    alt=&quot;Build&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pypistats.org/packages/mkdocs-material&quot;&gt;&lt;img
    src=&quot;https://img.shields.io/pypi/dm/mkdocs-material.svg&quot;
    alt=&quot;Downloads&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/mkdocs-material&quot;&gt;&lt;img
    src=&quot;https://img.shields.io/pypi/v/mkdocs-material.svg&quot;
    alt=&quot;Python Package Index&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://hub.docker.com/r/squidfunk/mkdocs-material/&quot;&gt;&lt;img
    src=&quot;https://img.shields.io/docker/pulls/squidfunk/mkdocs-material&quot;
    alt=&quot;Docker Pulls&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/sponsors/squidfunk&quot;&gt;&lt;img
    src=&quot;https://img.shields.io/github/sponsors/squidfunk&quot;
    alt=&quot;Sponsors&quot;
  /&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  Write your documentation in Markdown and create a professional static site for
  your Open Source or commercial project in minutes â€“ searchable, customizable,
  more than 60 languages, for all devices.
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://squidfunk.github.io/mkdocs-material/getting-started/&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/screenshot.png&quot; width=&quot;700&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;em&gt;
    Check out the demo â€“
    &lt;a
      href=&quot;https://squidfunk.github.io/mkdocs-material/&quot;
    &gt;squidfunk.github.io/mkdocs-material&lt;/a&gt;.
  &lt;/em&gt;
&lt;/p&gt;

&lt;h2&gt;&lt;/h2&gt;
&lt;p id=&quot;premium-sponsors&quot;&gt;&amp;nbsp;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;Silver sponsors&lt;/strong&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://fastapi.tiangolo.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-fastapi.png&quot; height=&quot;120&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.trendpop.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-trendpop.png&quot; height=&quot;120&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://documentation.sailpoint.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-sailpoint.png&quot; height=&quot;120&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://futureplc.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-future.svg&quot; height=&quot;120&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://opensource.siemens.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-siemens.png&quot; height=&quot;120&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pydantic.dev/logfire/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-logfire.png&quot; height=&quot;120&quot;
  /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;Bronze sponsors&lt;/strong&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://cirrus-ci.org/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-cirrus-ci.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://docs.baslerweb.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-basler.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://kx.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-kx.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://orion-docs.prefect.io/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-prefect.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.zenoss.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-zenoss.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://docs.posit.co&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-posit.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://n8n.io&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-n8n.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.dogado.de&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-dogado.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://wwt.com&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-wwt.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://coda.io&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-coda.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://elastic.co&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-elastic.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://ipfabric.io/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-ip-fabric.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.apex.ai/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-apex-ai.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://jitterbit.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-jitterbit.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://sparkfun.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-sparkfun.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://eccenca.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-eccenca.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://neptune.ai/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-neptune-ai.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://rackn.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-rackn.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://civicactions.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-civic-actions.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://bitcrowd.net/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-bitcrowd.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://getscreen.me/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-getscreenme.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://botcity.dev/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-botcity.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://kolena.io/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-kolena.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.evergiving.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-evergiving.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://astral.sh/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-astral.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://oikolab.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-oikolab.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.buhlergroup.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-buhler.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://3dr.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-3dr.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://spotware.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-spotware.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://milfordasset.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-milford.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.lechler.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-lechler.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://invers.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-invers.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://maxar.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-maxar.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.equipmentshare.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-equipmentshare.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://hummingbot.org/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-hummingbot.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://octoperf.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-octoperf.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://intercomestibles.ch/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-intercomestibles.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.centara.com/&quot; target=_blank&gt;&lt;img
    src=&quot;https://raw.githubusercontent.com/squidfunk/mkdocs-material/master/.github/assets/sponsors/sponsor-centara.png&quot; height=&quot;58&quot;
  /&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;

## Everything you would expect

### It&#039;s just Markdown

Focus on the content of your documentation and create a professional static site
in minutes. No need to know HTML, CSS or JavaScript â€“ let Material for MkDocs do
the heavy lifting for you.

### Works on all devices

Serve your documentation with confidence â€“ Material for MkDocs automatically
adapts to perfectly fit the available screen estate, no matter the type or size
of the viewing device. Desktop. Tablet. Mobile. All great.

### Made to measure

Make it yours â€“ change the colors, fonts, language, icons, logo, and more with
a few lines of configuration. Material for MkDocs can be easily extended and
provides many options to alter appearance and behavior.

### Fast and lightweight

Don&#039;t let your users wait â€“ get incredible value with a small footprint by using
one of the fastest themes available with excellent performance, yielding optimal
search engine rankings and happy users that return.

### Maintain ownership

Own your documentation&#039;s complete sources and outputs, guaranteeing both
integrity and security â€“ no need to entrust the backbone of your product
knowledge to third-party platforms. Retain full control.

### Open Source

You&#039;re in good company â€“ choose a mature and actively maintained solution built
with state-of-the-art Open Source technologies, trusted by more than 50,000
individuals and organizations. Licensed under MIT.

## Quick start

Material for MkDocs can be installed with `pip`:

``` sh
pip install mkdocs-material
```

Add the following lines to `mkdocs.yml`:

``` yaml
theme:
  name: material
```

For detailed installation instructions, configuration options, and a demo, visit
[squidfunk.github.io/mkdocs-material][Material for MkDocs]

  [Material for MkDocs]: https://squidfunk.github.io/mkdocs-material/

## Trusted by ...

### ... industry leaders

[ArXiv](https://info.arxiv.org),
[Atlassian](https://atlassian.github.io/data-center-helm-charts/),
[AWS](https://aws.github.io/copilot-cli/),
[Bloomberg](https://bloomberg.github.io/selekt/),
[CERN](http://abpcomputing.web.cern.ch/),
[Datadog](https://datadoghq.dev/integrations-core/),
[Google](https://google.github.io/accompanist/),
[Harvard](https://informatics.fas.harvard.edu/),
[Hewlett Packard](https://hewlettpackard.github.io/squest/),
[HSBC](https://hsbc.github.io/pyratings/),
[ING](https://ing-bank.github.io/baker/),
[Intel](https://open-amt-cloud-toolkit.github.io/docs/),
[JetBrains](https://jetbrains.github.io/projector-client/mkdocs/),
[LinkedIn](https://linkedin.github.io/school-of-sre/),
[Microsoft](https://microsoft.github.io/code-with-engineering-playbook/),
[Mozilla](https://mozillafoundation.github.io/engineering-handbook/),
[Netflix](https://netflix.github.io/titus/),
[OpenAI](https://openai.github.io/openai-agents-python/),
[Red Hat](https://ansible.readthedocs.io/projects/lint/),
[Roboflow](https://inference.roboflow.com/),
[Salesforce](https://policy-sentry.readthedocs.io/),
[SIEMENS](https://opensource.siemens.com/),
[Slack](https://slackhq.github.io/circuit/),
[Square](https://square.github.io/okhttp/),
[Uber](https://uber-go.github.io/fx/),
[Zalando](https://opensource.zalando.com/skipper/)

### ... and successful Open Source projects

[Amp](https://amp.rs/docs/),
[Arduino](https://arduino.github.io/arduino-cli/),
[Auto-GPT](https://docs.agpt.co/),
[AutoKeras](https://autokeras.com/),
[BFE](https://www.bfe-networks.net/),
[CentOS](https://docs.infra.centos.org/),
[Crystal](https://crystal-lang.org/reference/),
[eBPF](https://ebpf-go.dev/),
[ejabberd](https://docs.ejabberd.im/),
[Electron](https://www.electron.build/),
[FastAPI](https://fastapi.tiangolo.com/),
[FlatBuffers](https://flatbuffers.dev/),
[Freqtrade](https://www.freqtrade.io/en/stable/),
[GoReleaser](https://goreleaser.com/),
[GraphRAG](https://microsoft.github.io/graphrag/),
[HedgeDoc](https://docs.hedgedoc.org/),
[Hummingbot](https://hummingbot.org/),
[Knative](https://knative.dev/docs/),
[Kubernetes](https://kops.sigs.k8s.io/),
[kSQL](https://docs.ksqldb.io/),
[LeakCanary](https://square.github.io/leakcanary/),
[LlamaIndex](https://docs.llamaindex.ai/),
[NetBox](https://netboxlabs.com/docs/netbox/en/stable/),
[Nokogiri](https://nokogiri.org/),
[OpenAI](https://openai.github.io/openai-agents-python/),
[OpenFaaS](https://docs.openfaas.com/),
[OpenSSL](https://docs.openssl.org/),
[Orchard Core](https://docs.orchardcore.net/en/latest/),
[Percona](https://docs.percona.com/percona-monitoring-and-management/),
[Pi-Hole](https://docs.pi-hole.net/),
[Polars](https://docs.pola.rs/),
[Pydantic](https://pydantic-docs.helpmanual.io/),
[PyPI](https://docs.pypi.org/),
[Quivr](https://core.quivr.com/),
[Renovate](https://docs.renovatebot.com/),
[RetroPie](https://retropie.org.uk/docs/),
[Ruff](https://docs.astral.sh/ruff/),
[Supervision](https://supervision.roboflow.com/latest/),
[Textual](https://textual.textualize.io/),
[Traefik](https://docs.traefik.io/),
[Trivy](https://aquasecurity.github.io/trivy/),
[Typer](https://typer.tiangolo.com/),
[tinygrad](https://docs.tinygrad.org/),
[Ultralytics](https://docs.ultralytics.com/),
[UV](https://docs.astral.sh/uv/),
[Vapor](https://docs.vapor.codes/),
[WebKit](https://docs.webkit.org/),
[WTF](https://wtfutil.com/),
[ZeroNet](https://zeronet.io/docs/)

## License

**MIT License**

Copyright (c) 2016-2025 Martin Donath

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the &quot;Software&quot;), to
deal in the Software without restriction, including without limitation the
rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
sell copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
IN THE SOFTWARE.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[harry0703/MoneyPrinterTurbo]]></title>
            <link>https://github.com/harry0703/MoneyPrinterTurbo</link>
            <guid>https://github.com/harry0703/MoneyPrinterTurbo</guid>
            <pubDate>Thu, 10 Apr 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[åˆ©ç”¨AIå¤§æ¨¡å‹ï¼Œä¸€é”®ç”Ÿæˆé«˜æ¸…çŸ­è§†é¢‘ Generate short videos with one click using AI LLM.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/harry0703/MoneyPrinterTurbo">harry0703/MoneyPrinterTurbo</a></h1>
            <p>åˆ©ç”¨AIå¤§æ¨¡å‹ï¼Œä¸€é”®ç”Ÿæˆé«˜æ¸…çŸ­è§†é¢‘ Generate short videos with one click using AI LLM.</p>
            <p>Language: Python</p>
            <p>Stars: 26,027</p>
            <p>Forks: 3,820</p>
            <p>Stars today: 37 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;h1 align=&quot;center&quot;&gt;MoneyPrinterTurbo ğŸ’¸&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&quot; alt=&quot;Stargazers&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&quot; alt=&quot;Issues&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/network/members&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&quot; alt=&quot;Forks&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&quot; alt=&quot;License&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;ç®€ä½“ä¸­æ–‡ | &lt;a href=&quot;README-en.md&quot;&gt;English&lt;/a&gt;&lt;/h3&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/8731&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/8731&quot; alt=&quot;harry0703%2FMoneyPrinterTurbo | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;br&gt;
åªéœ€æä¾›ä¸€ä¸ªè§†é¢‘ &lt;b&gt;ä¸»é¢˜&lt;/b&gt; æˆ– &lt;b&gt;å…³é”®è¯&lt;/b&gt; ï¼Œå°±å¯ä»¥å…¨è‡ªåŠ¨ç”Ÿæˆè§†é¢‘æ–‡æ¡ˆã€è§†é¢‘ç´ æã€è§†é¢‘å­—å¹•ã€è§†é¢‘èƒŒæ™¯éŸ³ä¹ï¼Œç„¶ååˆæˆä¸€ä¸ªé«˜æ¸…çš„çŸ­è§†é¢‘ã€‚
&lt;br&gt;

&lt;h4&gt;Webç•Œé¢&lt;/h4&gt;

![](docs/webui.jpg)

&lt;h4&gt;APIç•Œé¢&lt;/h4&gt;

![](docs/api.jpg)

&lt;/div&gt;

## ç‰¹åˆ«æ„Ÿè°¢ ğŸ™

ç”±äºè¯¥é¡¹ç›®çš„ **éƒ¨ç½²** å’Œ **ä½¿ç”¨**ï¼Œå¯¹äºä¸€äº›å°ç™½ç”¨æˆ·æ¥è¯´ï¼Œè¿˜æ˜¯ **æœ‰ä¸€å®šçš„é—¨æ§›**ï¼Œåœ¨æ­¤ç‰¹åˆ«æ„Ÿè°¢
**å½•å’–ï¼ˆAIæ™ºèƒ½ å¤šåª’ä½“æœåŠ¡å¹³å°ï¼‰** ç½‘ç«™åŸºäºè¯¥é¡¹ç›®ï¼Œæä¾›çš„å…è´¹`AIè§†é¢‘ç”Ÿæˆå™¨`æœåŠ¡ï¼Œå¯ä»¥ä¸ç”¨éƒ¨ç½²ï¼Œç›´æ¥åœ¨çº¿ä½¿ç”¨ï¼Œéå¸¸æ–¹ä¾¿ã€‚

- ä¸­æ–‡ç‰ˆï¼šhttps://reccloud.cn
- è‹±æ–‡ç‰ˆï¼šhttps://reccloud.com

![](docs/reccloud.cn.jpg)

## æ„Ÿè°¢èµåŠ© ğŸ™

æ„Ÿè°¢ä½ç³– https://picwish.cn å¯¹è¯¥é¡¹ç›®çš„æ”¯æŒå’ŒèµåŠ©ï¼Œä½¿å¾—è¯¥é¡¹ç›®èƒ½å¤ŸæŒç»­çš„æ›´æ–°å’Œç»´æŠ¤ã€‚

ä½ç³–ä¸“æ³¨äº**å›¾åƒå¤„ç†é¢†åŸŸ**ï¼Œæä¾›ä¸°å¯Œçš„**å›¾åƒå¤„ç†å·¥å…·**ï¼Œå°†å¤æ‚æ“ä½œæè‡´ç®€åŒ–ï¼ŒçœŸæ­£å®ç°è®©å›¾åƒå¤„ç†æ›´ç®€å•ã€‚

![picwish.jpg](docs/picwish.jpg)

## åŠŸèƒ½ç‰¹æ€§ ğŸ¯

- [x] å®Œæ•´çš„ **MVCæ¶æ„**ï¼Œä»£ç  **ç»“æ„æ¸…æ™°**ï¼Œæ˜“äºç»´æŠ¤ï¼Œæ”¯æŒ `API` å’Œ `Webç•Œé¢`
- [x] æ”¯æŒè§†é¢‘æ–‡æ¡ˆ **AIè‡ªåŠ¨ç”Ÿæˆ**ï¼Œä¹Ÿå¯ä»¥**è‡ªå®šä¹‰æ–‡æ¡ˆ**
- [x] æ”¯æŒå¤šç§ **é«˜æ¸…è§†é¢‘** å°ºå¯¸
    - [x] ç«–å± 9:16ï¼Œ`1080x1920`
    - [x] æ¨ªå± 16:9ï¼Œ`1920x1080`
- [x] æ”¯æŒ **æ‰¹é‡è§†é¢‘ç”Ÿæˆ**ï¼Œå¯ä»¥ä¸€æ¬¡ç”Ÿæˆå¤šä¸ªè§†é¢‘ï¼Œç„¶åé€‰æ‹©ä¸€ä¸ªæœ€æ»¡æ„çš„
- [x] æ”¯æŒ **è§†é¢‘ç‰‡æ®µæ—¶é•¿** è®¾ç½®ï¼Œæ–¹ä¾¿è°ƒèŠ‚ç´ æåˆ‡æ¢é¢‘ç‡
- [x] æ”¯æŒ **ä¸­æ–‡** å’Œ **è‹±æ–‡** è§†é¢‘æ–‡æ¡ˆ
- [x] æ”¯æŒ **å¤šç§è¯­éŸ³** åˆæˆï¼Œå¯ **å®æ—¶è¯•å¬** æ•ˆæœ
- [x] æ”¯æŒ **å­—å¹•ç”Ÿæˆ**ï¼Œå¯ä»¥è°ƒæ•´ `å­—ä½“`ã€`ä½ç½®`ã€`é¢œè‰²`ã€`å¤§å°`ï¼ŒåŒæ—¶æ”¯æŒ`å­—å¹•æè¾¹`è®¾ç½®
- [x] æ”¯æŒ **èƒŒæ™¯éŸ³ä¹**ï¼Œéšæœºæˆ–è€…æŒ‡å®šéŸ³ä¹æ–‡ä»¶ï¼Œå¯è®¾ç½®`èƒŒæ™¯éŸ³ä¹éŸ³é‡`
- [x] è§†é¢‘ç´ ææ¥æº **é«˜æ¸…**ï¼Œè€Œä¸” **æ— ç‰ˆæƒ**ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨è‡ªå·±çš„ **æœ¬åœ°ç´ æ**
- [x] æ”¯æŒ **OpenAI**ã€**Moonshot**ã€**Azure**ã€**gpt4free**ã€**one-api**ã€**é€šä¹‰åƒé—®**ã€**Google Gemini**ã€**Ollama**ã€
  **DeepSeek**ã€ **æ–‡å¿ƒä¸€è¨€** ç­‰å¤šç§æ¨¡å‹æ¥å…¥
    - ä¸­å›½ç”¨æˆ·å»ºè®®ä½¿ç”¨ **DeepSeek** æˆ– **Moonshot** ä½œä¸ºå¤§æ¨¡å‹æä¾›å•†ï¼ˆå›½å†…å¯ç›´æ¥è®¿é—®ï¼Œä¸éœ€è¦VPNã€‚æ³¨å†Œå°±é€é¢åº¦ï¼ŒåŸºæœ¬å¤Ÿç”¨ï¼‰

### åæœŸè®¡åˆ’ ğŸ“…

- [ ] GPT-SoVITS é…éŸ³æ”¯æŒ
- [ ] ä¼˜åŒ–è¯­éŸ³åˆæˆï¼Œåˆ©ç”¨å¤§æ¨¡å‹ï¼Œä½¿å…¶åˆæˆçš„å£°éŸ³ï¼Œæ›´åŠ è‡ªç„¶ï¼Œæƒ…ç»ªæ›´åŠ ä¸°å¯Œ
- [ ] å¢åŠ è§†é¢‘è½¬åœºæ•ˆæœï¼Œä½¿å…¶çœ‹èµ·æ¥æ›´åŠ çš„æµç•…
- [ ] å¢åŠ æ›´å¤šè§†é¢‘ç´ ææ¥æºï¼Œä¼˜åŒ–è§†é¢‘ç´ æå’Œæ–‡æ¡ˆçš„åŒ¹é…åº¦
- [ ] å¢åŠ è§†é¢‘é•¿åº¦é€‰é¡¹ï¼šçŸ­ã€ä¸­ã€é•¿
- [ ] æ”¯æŒæ›´å¤šçš„è¯­éŸ³åˆæˆæœåŠ¡å•†ï¼Œæ¯”å¦‚ OpenAI TTS
- [ ] è‡ªåŠ¨ä¸Šä¼ åˆ°YouTubeå¹³å°

## è§†é¢‘æ¼”ç¤º ğŸ“º

### ç«–å± 9:16

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;â–¶ï¸&lt;/g-emoji&gt; ã€Šå¦‚ä½•å¢åŠ ç”Ÿæ´»çš„ä¹è¶£ã€‹&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;â–¶ï¸&lt;/g-emoji&gt; ã€Šé‡‘é’±çš„ä½œç”¨ã€‹&lt;br&gt;æ›´çœŸå®çš„åˆæˆå£°éŸ³&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;â–¶ï¸&lt;/g-emoji&gt; ã€Šç”Ÿå‘½çš„æ„ä¹‰æ˜¯ä»€ä¹ˆã€‹&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/a84d33d5-27a2-4aba-8fd0-9fb2bd91c6a6&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/af2f3b0b-002e-49fe-b161-18ba91c055e8&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/112c9564-d52b-4472-99ad-970b75f66476&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

### æ¨ªå± 16:9

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;â–¶ï¸&lt;/g-emoji&gt;ã€Šç”Ÿå‘½çš„æ„ä¹‰æ˜¯ä»€ä¹ˆã€‹&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;â–¶ï¸&lt;/g-emoji&gt;ã€Šä¸ºä»€ä¹ˆè¦è¿åŠ¨ã€‹&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/346ebb15-c55f-47a9-a653-114f08bb8073&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/271f2fae-8283-44a0-8aa0-0ed8f9a6fa87&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

## é…ç½®è¦æ±‚ ğŸ“¦

- å»ºè®®æœ€ä½ CPU 4æ ¸æˆ–ä»¥ä¸Šï¼Œå†…å­˜ 8G æˆ–ä»¥ä¸Šï¼Œæ˜¾å¡éå¿…é¡»
- Windows 10 æˆ– MacOS 11.0 ä»¥ä¸Šç³»ç»Ÿ

## å¿«é€Ÿå¼€å§‹ ğŸš€

ä¸‹è½½ä¸€é”®å¯åŠ¨åŒ…ï¼Œè§£å‹ç›´æ¥ä½¿ç”¨ï¼ˆè·¯å¾„ä¸è¦æœ‰ **ä¸­æ–‡**ã€**ç‰¹æ®Šå­—ç¬¦**ã€**ç©ºæ ¼**ï¼‰

### Windows
- ç™¾åº¦ç½‘ç›˜ï¼ˆ1.2.1 æœ€æ–°ç‰ˆæœ¬ï¼‰: https://pan.baidu.com/s/1pSNjxTYiVENulTLm6zieMQ?pwd=g36q æå–ç : g36q

ä¸‹è½½åï¼Œå»ºè®®å…ˆ**åŒå‡»æ‰§è¡Œ** `update.bat` æ›´æ–°åˆ°**æœ€æ–°ä»£ç **ï¼Œç„¶ååŒå‡» `start.bat` å¯åŠ¨

å¯åŠ¨åï¼Œä¼šè‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼ˆå¦‚æœæ‰“å¼€æ˜¯ç©ºç™½ï¼Œå»ºè®®æ¢æˆ **Chrome** æˆ–è€… **Edge** æ‰“å¼€ï¼‰

### å…¶ä»–ç³»ç»Ÿ

è¿˜æ²¡æœ‰åˆ¶ä½œä¸€é”®å¯åŠ¨åŒ…ï¼Œçœ‹ä¸‹é¢çš„ **å®‰è£…éƒ¨ç½²** éƒ¨åˆ†ï¼Œå»ºè®®ä½¿ç”¨ **docker** éƒ¨ç½²ï¼Œæ›´åŠ æ–¹ä¾¿ã€‚

## å®‰è£…éƒ¨ç½² ğŸ“¥

### å‰ææ¡ä»¶

- å°½é‡ä¸è¦ä½¿ç”¨ **ä¸­æ–‡è·¯å¾„**ï¼Œé¿å…å‡ºç°ä¸€äº›æ— æ³•é¢„æ–™çš„é—®é¢˜
- è¯·ç¡®ä¿ä½ çš„ **ç½‘ç»œ** æ˜¯æ­£å¸¸çš„ï¼ŒVPNéœ€è¦æ‰“å¼€`å…¨å±€æµé‡`æ¨¡å¼

#### â‘  å…‹éš†ä»£ç 

```shell
git clone https://github.com/harry0703/MoneyPrinterTurbo.git
```

#### â‘¡ ä¿®æ”¹é…ç½®æ–‡ä»¶

- å°† `config.example.toml` æ–‡ä»¶å¤åˆ¶ä¸€ä»½ï¼Œå‘½åä¸º `config.toml`
- æŒ‰ç…§ `config.toml` æ–‡ä»¶ä¸­çš„è¯´æ˜ï¼Œé…ç½®å¥½ `pexels_api_keys` å’Œ `llm_provider`ï¼Œå¹¶æ ¹æ® llm_provider å¯¹åº”çš„æœåŠ¡å•†ï¼Œé…ç½®ç›¸å…³çš„
  API Key

### Dockeréƒ¨ç½² ğŸ³

#### â‘  å¯åŠ¨Docker

å¦‚æœæœªå®‰è£… Dockerï¼Œè¯·å…ˆå®‰è£… https://www.docker.com/products/docker-desktop/

å¦‚æœæ˜¯Windowsç³»ç»Ÿï¼Œè¯·å‚è€ƒå¾®è½¯çš„æ–‡æ¡£ï¼š

1. https://learn.microsoft.com/zh-cn/windows/wsl/install
2. https://learn.microsoft.com/zh-cn/windows/wsl/tutorials/wsl-containers

```shell
cd MoneyPrinterTurbo
docker-compose up
```

&gt; æ³¨æ„ï¼šæœ€æ–°ç‰ˆçš„dockerå®‰è£…æ—¶ä¼šè‡ªåŠ¨ä»¥æ’ä»¶çš„å½¢å¼å®‰è£…docker composeï¼Œå¯åŠ¨å‘½ä»¤è°ƒæ•´ä¸ºdocker compose up

#### â‘¡ è®¿é—®Webç•Œé¢

æ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—® http://0.0.0.0:8501

#### â‘¢ è®¿é—®APIæ–‡æ¡£

æ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—® http://0.0.0.0:8080/docs æˆ–è€… http://0.0.0.0:8080/redoc

### æ‰‹åŠ¨éƒ¨ç½² ğŸ“¦

&gt; è§†é¢‘æ•™ç¨‹

- å®Œæ•´çš„ä½¿ç”¨æ¼”ç¤ºï¼šhttps://v.douyin.com/iFhnwsKY/
- å¦‚ä½•åœ¨Windowsä¸Šéƒ¨ç½²ï¼šhttps://v.douyin.com/iFyjoW3M

#### â‘  åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

å»ºè®®ä½¿ç”¨ [conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html) åˆ›å»º python è™šæ‹Ÿç¯å¢ƒ

```shell
git clone https://github.com/harry0703/MoneyPrinterTurbo.git
cd MoneyPrinterTurbo
conda create -n MoneyPrinterTurbo python=3.11
conda activate MoneyPrinterTurbo
pip install -r requirements.txt
```

#### â‘¡ å®‰è£…å¥½ ImageMagick

- Windows:
    - ä¸‹è½½ https://imagemagick.org/script/download.php é€‰æ‹©Windowsç‰ˆæœ¬ï¼Œåˆ‡è®°ä¸€å®šè¦é€‰æ‹© **é™æ€åº“** ç‰ˆæœ¬ï¼Œæ¯”å¦‚
      ImageMagick-7.1.1-32-Q16-x64-**static**.exe
    - å®‰è£…ä¸‹è½½å¥½çš„ ImageMagickï¼Œ**æ³¨æ„ä¸è¦ä¿®æ”¹å®‰è£…è·¯å¾„**
    - ä¿®æ”¹ `é…ç½®æ–‡ä»¶ config.toml` ä¸­çš„ `imagemagick_path` ä¸ºä½ çš„ **å®é™…å®‰è£…è·¯å¾„**

- MacOS:
  ```shell
  brew install imagemagick
  ````
- Ubuntu
  ```shell
  sudo apt-get install imagemagick
  ```
- CentOS
  ```shell
  sudo yum install ImageMagick
  ```

#### â‘¢ å¯åŠ¨Webç•Œé¢ ğŸŒ

æ³¨æ„éœ€è¦åˆ° MoneyPrinterTurbo é¡¹ç›® `æ ¹ç›®å½•` ä¸‹æ‰§è¡Œä»¥ä¸‹å‘½ä»¤

###### Windows

```bat
conda activate MoneyPrinterTurbo
webui.bat
```

###### MacOS or Linux

```shell
conda activate MoneyPrinterTurbo
sh webui.sh
```

å¯åŠ¨åï¼Œä¼šè‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼ˆå¦‚æœæ‰“å¼€æ˜¯ç©ºç™½ï¼Œå»ºè®®æ¢æˆ **Chrome** æˆ–è€… **Edge** æ‰“å¼€ï¼‰

#### â‘£ å¯åŠ¨APIæœåŠ¡ ğŸš€

```shell
python main.py
```

å¯åŠ¨åï¼Œå¯ä»¥æŸ¥çœ‹ `APIæ–‡æ¡£` http://127.0.0.1:8080/docs æˆ–è€… http://127.0.0.1:8080/redoc ç›´æ¥åœ¨çº¿è°ƒè¯•æ¥å£ï¼Œå¿«é€Ÿä½“éªŒã€‚

## è¯­éŸ³åˆæˆ ğŸ—£

æ‰€æœ‰æ”¯æŒçš„å£°éŸ³åˆ—è¡¨ï¼Œå¯ä»¥æŸ¥çœ‹ï¼š[å£°éŸ³åˆ—è¡¨](./docs/voice-list.txt)

2024-04-16 v1.1.2 æ–°å¢äº†9ç§Azureçš„è¯­éŸ³åˆæˆå£°éŸ³ï¼Œéœ€è¦é…ç½®API KEYï¼Œè¯¥å£°éŸ³åˆæˆçš„æ›´åŠ çœŸå®ã€‚

## å­—å¹•ç”Ÿæˆ ğŸ“œ

å½“å‰æ”¯æŒ2ç§å­—å¹•ç”Ÿæˆæ–¹å¼ï¼š

- **edge**: ç”Ÿæˆ`é€Ÿåº¦å¿«`ï¼Œæ€§èƒ½æ›´å¥½ï¼Œå¯¹ç”µè„‘é…ç½®æ²¡æœ‰è¦æ±‚ï¼Œä½†æ˜¯è´¨é‡å¯èƒ½ä¸ç¨³å®š
- **whisper**: ç”Ÿæˆ`é€Ÿåº¦æ…¢`ï¼Œæ€§èƒ½è¾ƒå·®ï¼Œå¯¹ç”µè„‘é…ç½®æœ‰ä¸€å®šè¦æ±‚ï¼Œä½†æ˜¯`è´¨é‡æ›´å¯é `ã€‚

å¯ä»¥ä¿®æ”¹ `config.toml` é…ç½®æ–‡ä»¶ä¸­çš„ `subtitle_provider` è¿›è¡Œåˆ‡æ¢

å»ºè®®ä½¿ç”¨ `edge` æ¨¡å¼ï¼Œå¦‚æœç”Ÿæˆçš„å­—å¹•è´¨é‡ä¸å¥½ï¼Œå†åˆ‡æ¢åˆ° `whisper` æ¨¡å¼

&gt; æ³¨æ„ï¼š

1. whisper æ¨¡å¼ä¸‹éœ€è¦åˆ° HuggingFace ä¸‹è½½ä¸€ä¸ªæ¨¡å‹æ–‡ä»¶ï¼Œå¤§çº¦ 3GB å·¦å³ï¼Œè¯·ç¡®ä¿ç½‘ç»œé€šç•…
2. å¦‚æœç•™ç©ºï¼Œè¡¨ç¤ºä¸ç”Ÿæˆå­—å¹•ã€‚

&gt; ç”±äºå›½å†…æ— æ³•è®¿é—® HuggingFaceï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ä¸‹è½½ `whisper-large-v3` çš„æ¨¡å‹æ–‡ä»¶

ä¸‹è½½åœ°å€ï¼š

- ç™¾åº¦ç½‘ç›˜: https://pan.baidu.com/s/11h3Q6tsDtjQKTjUu3sc5cA?pwd=xjs9
- å¤¸å…‹ç½‘ç›˜ï¼šhttps://pan.quark.cn/s/3ee3d991d64b

æ¨¡å‹ä¸‹è½½åè§£å‹ï¼Œæ•´ä¸ªç›®å½•æ”¾åˆ° `.\MoneyPrinterTurbo\models` é‡Œé¢ï¼Œ
æœ€ç»ˆçš„æ–‡ä»¶è·¯å¾„åº”è¯¥æ˜¯è¿™æ ·: `.\MoneyPrinterTurbo\models\whisper-large-v3`

```
MoneyPrinterTurbo  
  â”œâ”€models
  â”‚   â””â”€whisper-large-v3
  â”‚          config.json
  â”‚          model.bin
  â”‚          preprocessor_config.json
  â”‚          tokenizer.json
  â”‚          vocabulary.json
```

## èƒŒæ™¯éŸ³ä¹ ğŸµ

ç”¨äºè§†é¢‘çš„èƒŒæ™¯éŸ³ä¹ï¼Œä½äºé¡¹ç›®çš„ `resource/songs` ç›®å½•ä¸‹ã€‚
&gt; å½“å‰é¡¹ç›®é‡Œé¢æ”¾äº†ä¸€äº›é»˜è®¤çš„éŸ³ä¹ï¼Œæ¥è‡ªäº YouTube è§†é¢‘ï¼Œå¦‚æœ‰ä¾µæƒï¼Œè¯·åˆ é™¤ã€‚

## å­—å¹•å­—ä½“ ğŸ…°

ç”¨äºè§†é¢‘å­—å¹•çš„æ¸²æŸ“ï¼Œä½äºé¡¹ç›®çš„ `resource/fonts` ç›®å½•ä¸‹ï¼Œä½ ä¹Ÿå¯ä»¥æ”¾è¿›å»è‡ªå·±çš„å­—ä½“ã€‚

## å¸¸è§é—®é¢˜ ğŸ¤”

### â“å¦‚ä½•ä½¿ç”¨å…è´¹çš„OpenAI GPT-3.5æ¨¡å‹?

[OpenAIå®£å¸ƒChatGPTé‡Œé¢3.5å·²ç»å…è´¹äº†](https://openai.com/blog/start-using-chatgpt-instantly)ï¼Œæœ‰å¼€å‘è€…å°†å…¶å°è£…æˆäº†APIï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨

**ç¡®ä¿ä½ å®‰è£…å’Œå¯åŠ¨äº†dockeræœåŠ¡**ï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨dockeræœåŠ¡

```shell
docker run -p 3040:3040 missuo/freegpt35
```

å¯åŠ¨æˆåŠŸåï¼Œä¿®æ”¹ `config.toml` ä¸­çš„é…ç½®

- `llm_provider` è®¾ç½®ä¸º `openai`
- `openai_api_key` éšä¾¿å¡«å†™ä¸€ä¸ªå³å¯ï¼Œæ¯”å¦‚ &#039;123456&#039;
- `openai_base_url` æ”¹ä¸º `http://localhost:3040/v1/`
- `openai_model_name` æ”¹ä¸º `gpt-3.5-turbo`

&gt; æ³¨æ„ï¼šè¯¥æ–¹å¼ç¨³å®šæ€§è¾ƒå·®

### â“AttributeError: &#039;str&#039; object has no attribute &#039;choices&#039;`

è¿™ä¸ªé—®é¢˜æ˜¯ç”±äºå¤§æ¨¡å‹æ²¡æœ‰è¿”å›æ­£ç¡®çš„å›å¤å¯¼è‡´çš„ã€‚

å¤§æ¦‚ç‡æ˜¯ç½‘ç»œåŸå› ï¼Œ ä½¿ç”¨ **VPN**ï¼Œæˆ–è€…è®¾ç½® `openai_base_url` ä¸ºä½ çš„ä»£ç† ï¼Œåº”è¯¥å°±å¯ä»¥è§£å†³äº†ã€‚

åŒæ—¶å»ºè®®ä½¿ç”¨ **Moonshot** æˆ– **DeepSeek** ä½œä¸ºå¤§æ¨¡å‹æä¾›å•†ï¼Œè¿™ä¸¤ä¸ªæœåŠ¡å•†åœ¨å›½å†…è®¿é—®é€Ÿåº¦æ›´å¿«ï¼Œæ›´åŠ ç¨³å®šã€‚

### â“RuntimeError: No ffmpeg exe could be found

é€šå¸¸æƒ…å†µä¸‹ï¼Œffmpeg ä¼šè¢«è‡ªåŠ¨ä¸‹è½½ï¼Œå¹¶ä¸”ä¼šè¢«è‡ªåŠ¨æ£€æµ‹åˆ°ã€‚
ä½†æ˜¯å¦‚æœä½ çš„ç¯å¢ƒæœ‰é—®é¢˜ï¼Œæ— æ³•è‡ªåŠ¨ä¸‹è½½ï¼Œå¯èƒ½ä¼šé‡åˆ°å¦‚ä¸‹é”™è¯¯ï¼š

```
RuntimeError: No ffmpeg exe could be found.
Install ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable.
```

æ­¤æ—¶ä½ å¯ä»¥ä» https://www.gyan.dev/ffmpeg/builds/ ä¸‹è½½ffmpegï¼Œè§£å‹åï¼Œè®¾ç½® `ffmpeg_path` ä¸ºä½ çš„å®é™…å®‰è£…è·¯å¾„å³å¯ã€‚

```toml
[app]
# è¯·æ ¹æ®ä½ çš„å®é™…è·¯å¾„è®¾ç½®ï¼Œæ³¨æ„ Windows è·¯å¾„åˆ†éš”ç¬¦ä¸º \\
ffmpeg_path = &quot;C:\\Users\\harry\\Downloads\\ffmpeg.exe&quot;
```

### â“ImageMagickçš„å®‰å…¨ç­–ç•¥é˜»æ­¢äº†ä¸ä¸´æ—¶æ–‡ä»¶@/tmp/tmpur5hyyto.txtç›¸å…³çš„æ“ä½œ

å¯ä»¥åœ¨ImageMagickçš„é…ç½®æ–‡ä»¶policy.xmlä¸­æ‰¾åˆ°è¿™äº›ç­–ç•¥ã€‚
è¿™ä¸ªæ–‡ä»¶é€šå¸¸ä½äº /etc/ImageMagick-`X`/ æˆ– ImageMagick å®‰è£…ç›®å½•çš„ç±»ä¼¼ä½ç½®ã€‚
ä¿®æ”¹åŒ…å«`pattern=&quot;@&quot;`çš„æ¡ç›®ï¼Œå°†`rights=&quot;none&quot;`æ›´æ”¹ä¸º`rights=&quot;read|write&quot;`ä»¥å…è®¸å¯¹æ–‡ä»¶çš„è¯»å†™æ“ä½œã€‚

### â“OSError: [Errno 24] Too many open files

è¿™ä¸ªé—®é¢˜æ˜¯ç”±äºç³»ç»Ÿæ‰“å¼€æ–‡ä»¶æ•°é™åˆ¶å¯¼è‡´çš„ï¼Œå¯ä»¥é€šè¿‡ä¿®æ”¹ç³»ç»Ÿçš„æ–‡ä»¶æ‰“å¼€æ•°é™åˆ¶æ¥è§£å†³ã€‚

æŸ¥çœ‹å½“å‰é™åˆ¶

```shell
ulimit -n
```

å¦‚æœè¿‡ä½ï¼Œå¯ä»¥è°ƒé«˜ä¸€äº›ï¼Œæ¯”å¦‚

```shell
ulimit -n 10240
```

### â“Whisper æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œå‡ºç°å¦‚ä¸‹é”™è¯¯

LocalEntryNotfoundEror: Cannot find an appropriate cached snapshotfolderfor the specified revision on the local disk and
outgoing trafic has been disabled.
To enablerepo look-ups and downloads online, pass &#039;local files only=False&#039; as input.

æˆ–è€…

An error occured while synchronizing the model Systran/faster-whisper-large-v3 from the Hugging Face Hub:
An error happened while trying to locate the files on the Hub and we cannot find the appropriate snapshot folder for the
specified revision on the local disk. Please check your internet connection and try again.
Trying to load the model directly from the local cache, if it exists.

è§£å†³æ–¹æ³•ï¼š[ç‚¹å‡»æŸ¥çœ‹å¦‚ä½•ä»ç½‘ç›˜æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹](#%E5%AD%97%E5%B9%95%E7%94%9F%E6%88%90-)

## åé¦ˆå»ºè®® ğŸ“¢

- å¯ä»¥æäº¤ [issue](https://github.com/harry0703/MoneyPrinterTurbo/issues)
  æˆ–è€… [pull request](https://github.com/harry0703/MoneyPrinterTurbo/pulls)ã€‚

## å‚è€ƒé¡¹ç›® ğŸ“š

è¯¥é¡¹ç›®åŸºäº https://github.com/FujiwaraChoki/MoneyPrinter é‡æ„è€Œæ¥ï¼Œåšäº†å¤§é‡çš„ä¼˜åŒ–ï¼Œå¢åŠ äº†æ›´å¤šçš„åŠŸèƒ½ã€‚
æ„Ÿè°¢åŸä½œè€…çš„å¼€æºç²¾ç¥ã€‚

## è®¸å¯è¯ ğŸ“

ç‚¹å‡»æŸ¥çœ‹ [`LICENSE`](LICENSE) æ–‡ä»¶

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=harry0703/MoneyPrinterTurbo&amp;type=Date)](https://star-history.com/#harry0703/MoneyPrinterTurbo&amp;Date)</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pytorch/pytorch]]></title>
            <link>https://github.com/pytorch/pytorch</link>
            <guid>https://github.com/pytorch/pytorch</guid>
            <pubDate>Thu, 10 Apr 2025 00:04:20 GMT</pubDate>
            <description><![CDATA[Tensors and Dynamic neural networks in Python with strong GPU acceleration]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pytorch/pytorch">pytorch/pytorch</a></h1>
            <p>Tensors and Dynamic neural networks in Python with strong GPU acceleration</p>
            <p>Language: Python</p>
            <p>Stars: 88,803</p>
            <p>Forks: 23,806</p>
            <p>Stars today: 38 stars today</p>
            <h2>README</h2><pre>![PyTorch Logo](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png)

--------------------------------------------------------------------------------

PyTorch is a Python package that provides two high-level features:
- Tensor computation (like NumPy) with strong GPU acceleration
- Deep neural networks built on a tape-based autograd system

You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.

Our trunk health (Continuous Integration signals) can be found at [hud.pytorch.org](https://hud.pytorch.org/ci/pytorch/pytorch/main).

&lt;!-- toc --&gt;

- [More About PyTorch](#more-about-pytorch)
  - [A GPU-Ready Tensor Library](#a-gpu-ready-tensor-library)
  - [Dynamic Neural Networks: Tape-Based Autograd](#dynamic-neural-networks-tape-based-autograd)
  - [Python First](#python-first)
  - [Imperative Experiences](#imperative-experiences)
  - [Fast and Lean](#fast-and-lean)
  - [Extensions Without Pain](#extensions-without-pain)
- [Installation](#installation)
  - [Binaries](#binaries)
    - [NVIDIA Jetson Platforms](#nvidia-jetson-platforms)
  - [From Source](#from-source)
    - [Prerequisites](#prerequisites)
      - [NVIDIA CUDA Support](#nvidia-cuda-support)
      - [AMD ROCm Support](#amd-rocm-support)
      - [Intel GPU Support](#intel-gpu-support)
    - [Get the PyTorch Source](#get-the-pytorch-source)
    - [Install Dependencies](#install-dependencies)
    - [Install PyTorch](#install-pytorch)
      - [Adjust Build Options (Optional)](#adjust-build-options-optional)
  - [Docker Image](#docker-image)
    - [Using pre-built images](#using-pre-built-images)
    - [Building the image yourself](#building-the-image-yourself)
  - [Building the Documentation](#building-the-documentation)
  - [Previous Versions](#previous-versions)
- [Getting Started](#getting-started)
- [Resources](#resources)
- [Communication](#communication)
- [Releases and Contributing](#releases-and-contributing)
- [The Team](#the-team)
- [License](#license)

&lt;!-- tocstop --&gt;

## More About PyTorch

[Learn the basics of PyTorch](https://pytorch.org/tutorials/beginner/basics/intro.html)

At a granular level, PyTorch is a library that consists of the following components:

| Component | Description |
| ---- | --- |
| [**torch**](https://pytorch.org/docs/stable/torch.html) | A Tensor library like NumPy, with strong GPU support |
| [**torch.autograd**](https://pytorch.org/docs/stable/autograd.html) | A tape-based automatic differentiation library that supports all differentiable Tensor operations in torch |
| [**torch.jit**](https://pytorch.org/docs/stable/jit.html) | A compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code  |
| [**torch.nn**](https://pytorch.org/docs/stable/nn.html) | A neural networks library deeply integrated with autograd designed for maximum flexibility |
| [**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html) | Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training |
| [**torch.utils**](https://pytorch.org/docs/stable/data.html) | DataLoader and other utility functions for convenience |

Usually, PyTorch is used either as:

- A replacement for NumPy to use the power of GPUs.
- A deep learning research platform that provides maximum flexibility and speed.

Elaborating Further:

### A GPU-Ready Tensor Library

If you use NumPy, then you have used Tensors (a.k.a. ndarray).

![Tensor illustration](./docs/source/_static/img/tensor_illustration.png)

PyTorch provides Tensors that can live either on the CPU or the GPU and accelerates the
computation by a huge amount.

We provide a wide variety of tensor routines to accelerate and fit your scientific computation needs
such as slicing, indexing, mathematical operations, linear algebra, reductions.
And they are fast!

### Dynamic Neural Networks: Tape-Based Autograd

PyTorch has a unique way of building neural networks: using and replaying a tape recorder.

Most frameworks such as TensorFlow, Theano, Caffe, and CNTK have a static view of the world.
One has to build a neural network and reuse the same structure again and again.
Changing the way the network behaves means that one has to start from scratch.

With PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to
change the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes
from several research papers on this topic, as well as current and past work such as
[torch-autograd](https://github.com/twitter/torch-autograd),
[autograd](https://github.com/HIPS/autograd),
[Chainer](https://chainer.org), etc.

While this technique is not unique to PyTorch, it&#039;s one of the fastest implementations of it to date.
You get the best of speed and flexibility for your crazy research.

![Dynamic graph](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/dynamic_graph.gif)

### Python First

PyTorch is not a Python binding into a monolithic C++ framework.
It is built to be deeply integrated into Python.
You can use it naturally like you would use [NumPy](https://www.numpy.org/) / [SciPy](https://www.scipy.org/) / [scikit-learn](https://scikit-learn.org) etc.
You can write your new neural network layers in Python itself, using your favorite libraries
and use packages such as [Cython](https://cython.org/) and [Numba](http://numba.pydata.org/).
Our goal is to not reinvent the wheel where appropriate.

### Imperative Experiences

PyTorch is designed to be intuitive, linear in thought, and easy to use.
When you execute a line of code, it gets executed. There isn&#039;t an asynchronous view of the world.
When you drop into a debugger or receive error messages and stack traces, understanding them is straightforward.
The stack trace points to exactly where your code was defined.
We hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.

### Fast and Lean

PyTorch has minimal framework overhead. We integrate acceleration libraries
such as [Intel MKL](https://software.intel.com/mkl) and NVIDIA ([cuDNN](https://developer.nvidia.com/cudnn), [NCCL](https://developer.nvidia.com/nccl)) to maximize speed.
At the core, its CPU and GPU Tensor and neural network backends
are mature and have been tested for years.

Hence, PyTorch is quite fast â€” whether you run small or large neural networks.

The memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives.
We&#039;ve written custom memory allocators for the GPU to make sure that
your deep learning models are maximally memory efficient.
This enables you to train bigger deep learning models than before.

### Extensions Without Pain

Writing new neural network modules, or interfacing with PyTorch&#039;s Tensor API was designed to be straightforward
and with minimal abstractions.

You can write new neural network layers in Python using the torch API
[or your favorite NumPy-based libraries such as SciPy](https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html).

If you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate.
No wrapper code needs to be written. You can see [a tutorial here](https://pytorch.org/tutorials/advanced/cpp_extension.html) and [an example here](https://github.com/pytorch/extension-cpp).


## Installation

### Binaries
Commands to install binaries via Conda or pip wheels are on our website: [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)


#### NVIDIA Jetson Platforms

Python wheels for NVIDIA&#039;s Jetson Nano, Jetson TX1/TX2, Jetson Xavier NX/AGX, and Jetson AGX Orin are provided [here](https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048) and the L4T container is published [here](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch)

They require JetPack 4.2 and above, and [@dusty-nv](https://github.com/dusty-nv) and [@ptrblck](https://github.com/ptrblck) are maintaining them.


### From Source

#### Prerequisites
If you are installing from source, you will need:
- Python 3.9 or later
- A compiler that fully supports C++17, such as clang or gcc (gcc 9.4.0 or newer is required, on Linux)
- Visual Studio or Visual Studio Build Tool (Windows only)

\* PyTorch CI uses Visual C++ BuildTools, which come with Visual Studio Enterprise,
Professional, or Community Editions. You can also install the build tools from
https://visualstudio.microsoft.com/visual-cpp-build-tools/. The build tools *do not*
come with Visual Studio Code by default.

An example of environment setup is shown below:

* Linux:

```bash
$ source &lt;CONDA_INSTALL_DIR&gt;/bin/activate
$ conda create -y -n &lt;CONDA_NAME&gt;
$ conda activate &lt;CONDA_NAME&gt;
```

* Windows:

```bash
$ source &lt;CONDA_INSTALL_DIR&gt;\Scripts\activate.bat
$ conda create -y -n &lt;CONDA_NAME&gt;
$ conda activate &lt;CONDA_NAME&gt;
$ call &quot;C:\Program Files\Microsoft Visual Studio\&lt;VERSION&gt;\Community\VC\Auxiliary\Build\vcvarsall.bat&quot; x64
```

##### NVIDIA CUDA Support
If you want to compile with CUDA support, [select a supported version of CUDA from our support matrix](https://pytorch.org/get-started/locally/), then install the following:
- [NVIDIA CUDA](https://developer.nvidia.com/cuda-downloads)
- [NVIDIA cuDNN](https://developer.nvidia.com/cudnn) v8.5 or above
- [Compiler](https://gist.github.com/ax3l/9489132) compatible with CUDA

Note: You could refer to the [cuDNN Support Matrix](https://docs.nvidia.com/deeplearning/cudnn/reference/support-matrix.html) for cuDNN versions with the various supported CUDA, CUDA driver and NVIDIA hardware

If you want to disable CUDA support, export the environment variable `USE_CUDA=0`.
Other potentially useful environment variables may be found in `setup.py`.

If you are building for NVIDIA&#039;s Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to install PyTorch for Jetson Nano are [available here](https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/)

##### AMD ROCm Support
If you want to compile with ROCm support, install
- [AMD ROCm](https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html) 4.0 and above installation
- ROCm is currently supported only for Linux systems.

By default the build system expects ROCm to be installed in `/opt/rocm`. If ROCm is installed in a different directory, the `ROCM_PATH` environment variable must be set to the ROCm installation directory. The build system automatically detects the AMD GPU architecture. Optionally, the AMD GPU architecture can be explicitly set with the `PYTORCH_ROCM_ARCH` environment variable [AMD GPU architecture](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#supported-gpus)

If you want to disable ROCm support, export the environment variable `USE_ROCM=0`.
Other potentially useful environment variables may be found in `setup.py`.

##### Intel GPU Support
If you want to compile with Intel GPU support, follow these
- [PyTorch Prerequisites for Intel GPUs](https://www.intel.com/content/www/us/en/developer/articles/tool/pytorch-prerequisites-for-intel-gpus.html) instructions.
- Intel GPU is supported for Linux and Windows.

If you want to disable Intel GPU support, export the environment variable `USE_XPU=0`.
Other potentially useful environment variables may be found in `setup.py`.

#### Get the PyTorch Source
```bash
git clone https://github.com/pytorch/pytorch
cd pytorch
# if you are updating an existing checkout
git submodule sync
git submodule update --init --recursive
```

#### Install Dependencies

**Common**

```bash
conda install cmake ninja
# Run this command from the PyTorch directory after cloning the source code using the â€œGet the PyTorch Sourceâ€œ section below
pip install -r requirements.txt
```

**On Linux**

```bash
pip install mkl-static mkl-include
# CUDA only: Add LAPACK support for the GPU if needed
conda install -c pytorch magma-cuda121  # or the magma-cuda* that matches your CUDA version from https://anaconda.org/pytorch/repo

# (optional) If using torch.compile with inductor/triton, install the matching version of triton
# Run from the pytorch directory after cloning
# For Intel GPU support, please explicitly `export USE_XPU=1` before running command.
make triton
```

**On MacOS**

```bash
# Add this package on intel x86 processor machines only
pip install mkl-static mkl-include
# Add these packages if torch.distributed is needed
conda install pkg-config libuv
```

**On Windows**

```bash
pip install mkl-static mkl-include
# Add these packages if torch.distributed is needed.
# Distributed package support on Windows is a prototype feature and is subject to changes.
conda install -c conda-forge libuv=1.39
```

#### Install PyTorch
**On Linux**

If you&#039;re compiling for AMD ROCm then first run this command:
```bash
# Only run this if you&#039;re compiling for ROCm
python tools/amd_build/build_amd.py
```

Install PyTorch
```bash
export CMAKE_PREFIX_PATH=&quot;${CONDA_PREFIX:-&#039;$(dirname $(which conda))/../&#039;}:${CMAKE_PREFIX_PATH}&quot;
python setup.py develop
```

**On macOS**

```bash
python3 setup.py develop
```

**On Windows**

If you want to build legacy python code, please refer to [Building on legacy code and CUDA](https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#building-on-legacy-code-and-cuda)

**CPU-only builds**

In this mode PyTorch computations will run on your CPU, not your GPU.

```cmd
python setup.py develop
```

Note on OpenMP: The desired OpenMP implementation is Intel OpenMP (iomp). In order to link against iomp, you&#039;ll need to manually download the library and set up the building environment by tweaking `CMAKE_INCLUDE_PATH` and `LIB`. The instruction [here](https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#building-from-source) is an example for setting up both MKL and Intel OpenMP. Without these configurations for CMake, Microsoft Visual C OpenMP runtime (vcomp) will be used.

**CUDA based build**

In this mode PyTorch computations will leverage your GPU via CUDA for faster number crunching

[NVTX](https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm) is needed to build Pytorch with CUDA.
NVTX is a part of CUDA distributive, where it is called &quot;Nsight Compute&quot;. To install it onto an already installed CUDA run CUDA installation once again and check the corresponding checkbox.
Make sure that CUDA with Nsight Compute is installed after Visual Studio.

Currently, VS 2017 / 2019, and Ninja are supported as the generator of CMake. If `ninja.exe` is detected in `PATH`, then Ninja will be used as the default generator, otherwise, it will use VS 2017 / 2019.
&lt;br/&gt; If Ninja is selected as the generator, the latest MSVC will get selected as the underlying toolchain.

Additional libraries such as
[Magma](https://developer.nvidia.com/magma), [oneDNN, a.k.a. MKLDNN or DNNL](https://github.com/oneapi-src/oneDNN), and [Sccache](https://github.com/mozilla/sccache) are often needed. Please refer to the [installation-helper](https://github.com/pytorch/pytorch/tree/main/.ci/pytorch/win-test-helpers/installation-helpers) to install them.

You can refer to the [build_pytorch.bat](https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/win-test-helpers/build_pytorch.bat) script for some other environment variables configurations


```cmd
cmd

:: Set the environment variables after you have downloaded and unzipped the mkl package,
:: else CMake would throw an error as `Could NOT find OpenMP`.
set CMAKE_INCLUDE_PATH={Your directory}\mkl\include
set LIB={Your directory}\mkl\lib;%LIB%

:: Read the content in the previous section carefully before you proceed.
:: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.
:: &quot;Visual Studio 2019 Developer Command Prompt&quot; will be run automatically.
:: Make sure you have CMake &gt;= 3.12 before you do this when you use the Visual Studio generator.
set CMAKE_GENERATOR_TOOLSET_VERSION=14.27
set DISTUTILS_USE_SDK=1
for /f &quot;usebackq tokens=*&quot; %i in (`&quot;%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe&quot; -version [15^,17^) -products * -latest -property installationPath`) do call &quot;%i\VC\Auxiliary\Build\vcvarsall.bat&quot; x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%

:: [Optional] If you want to override the CUDA host compiler
set CUDAHOSTCXX=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64\cl.exe

python setup.py develop

```

**Intel GPU builds**

In this mode PyTorch with Intel GPU support will be built.

Please make sure [the common prerequisites](#prerequisites) as well as [the prerequisites for Intel GPU](#intel-gpu-support) are properly installed and the environment variables are configured prior to starting the build. For build tool support, `Visual Studio 2022` is required.

Then PyTorch can be built with the command:

```cmd
:: CMD Commands:
:: Set the CMAKE_PREFIX_PATH to help find corresponding packages
:: %CONDA_PREFIX% only works after `conda activate custom_env`

if defined CMAKE_PREFIX_PATH (
    set &quot;CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library;%CMAKE_PREFIX_PATH%&quot;
) else (
    set &quot;CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library&quot;
)

python setup.py develop
```

##### Adjust Build Options (Optional)

You can adjust the configuration of cmake variables optionally (without building first), by doing
the following. For example, adjusting the pre-detected directories for CuDNN or BLAS can be done
with such a step.

On Linux
```bash
export CMAKE_PREFIX_PATH=&quot;${CONDA_PREFIX:-&#039;$(dirname $(which conda))/../&#039;}:${CMAKE_PREFIX_PATH}&quot;
python setup.py build --cmake-only
ccmake build  # or cmake-gui build
```

On macOS
```bash
export CMAKE_PREFIX_PATH=&quot;${CONDA_PREFIX:-&#039;$(dirname $(which conda))/../&#039;}:${CMAKE_PREFIX_PATH}&quot;
MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build --cmake-only
ccmake build  # or cmake-gui build
```

### Docker Image

#### Using pre-built images

You can also pull a pre-built docker image from Docker Hub and run with docker v19.03+

```bash
docker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest
```

Please note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g.
for multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you
should increase shared memory size either with `--ipc=host` or `--shm-size` command line options to `nvidia-docker run`.

#### Building the image yourself

**NOTE:** Must be built with a docker version &gt; 18.06

The `Dockerfile` is supplied to build images with CUDA 11.1 support and cuDNN v8.
You can pass `PYTHON_VERSION=x.y` make variable to specify which Python version is to be used by Miniconda, or leave it
unset to use the default.

```bash
make -f docker.Makefile
# images are tagged as docker.io/${your_docker_username}/pytorch
```

You can also pass the `CMAKE_VARS=&quot;...&quot;` environment variable to specify additional CMake variables to be passed to CMake during the build.
See [setup.py](./setup.py) for the list of available variables.

```bash
make -f docker.Makefile
```

### Building the Documentation

To build documentation in various formats, you will need [Sphinx](http://www.sphinx-doc.org) and the
readthedocs theme.

```bash
cd docs/
pip install -r requirements.txt
make html
make serve
```

Run `make` to get a list of all available output formats.

If you get a katex error run `npm install katex`.  If it persists, try
`npm install -g katex`

&gt; Note: if you installed `nodejs` with a different package manager (e.g.,
`conda`) then `npm` will probably install a version of `katex` that is not
compatible with yo

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sgl-project/sglang]]></title>
            <link>https://github.com/sgl-project/sglang</link>
            <guid>https://github.com/sgl-project/sglang</guid>
            <pubDate>Thu, 10 Apr 2025 00:04:19 GMT</pubDate>
            <description><![CDATA[SGLang is a fast serving framework for large language models and vision language models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sgl-project/sglang">sgl-project/sglang</a></h1>
            <p>SGLang is a fast serving framework for large language models and vision language models.</p>
            <p>Language: Python</p>
            <p>Stars: 13,020</p>
            <p>Forks: 1,471</p>
            <p>Stars today: 43 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; id=&quot;sglangtop&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/sgl-project/sglang/main/assets/logo.png&quot; alt=&quot;logo&quot; width=&quot;400&quot; margin=&quot;10px&quot;&gt;&lt;/img&gt;

[![PyPI](https://img.shields.io/pypi/v/sglang)](https://pypi.org/project/sglang)
![PyPI - Downloads](https://img.shields.io/pypi/dm/sglang)
[![license](https://img.shields.io/github/license/sgl-project/sglang.svg)](https://github.com/sgl-project/sglang/tree/main/LICENSE)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/sgl-project/sglang)](https://github.com/sgl-project/sglang/issues)
[![open issues](https://img.shields.io/github/issues-raw/sgl-project/sglang)](https://github.com/sgl-project/sglang/issues)
[![](https://img.shields.io/badge/Gurubase-(experimental)-006BFF)](https://gurubase.io/g/sglang)

&lt;/div&gt;

--------------------------------------------------------------------------------

| [**Blog**](https://lmsys.org/blog/2024-07-25-sglang-llama3/)
| [**Documentation**](https://docs.sglang.ai/)
| [**Join Slack**](https://slack.sglang.ai/)
| [**Join Bi-Weekly Development Meeting**](https://meeting.sglang.ai/)
| [**Roadmap**](https://github.com/sgl-project/sglang/issues/4042)
| [**Slides**](https://github.com/sgl-project/sgl-learning-materials?tab=readme-ov-file#slides) |

## News
- [2025/03] Supercharge DeepSeek-R1 Inference on AMD Instinct MI300X ([AMD blog](https://rocm.blogs.amd.com/artificial-intelligence/DeepSeekR1-Part2/README.html))
- [2025/03] SGLang Joins PyTorch Ecosystem: Efficient LLM Serving Engine ([PyTorch blog](https://pytorch.org/blog/sglang-joins-pytorch/))
- [2025/02] Unlock DeepSeek-R1 Inference Performance on AMD Instinctâ„¢ MI300X GPU ([AMD blog](https://rocm.blogs.amd.com/artificial-intelligence/DeepSeekR1_Perf/README.html))
- [2025/01] ğŸ”¥ SGLang provides day one support for DeepSeek V3/R1 models on NVIDIA and AMD GPUs with DeepSeek-specific optimizations. ([instructions](https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3), [AMD blog](https://www.amd.com/en/developer/resources/technical-articles/amd-instinct-gpus-power-deepseek-v3-revolutionizing-ai-development-with-sglang.html), [10+ other companies](https://x.com/lmsysorg/status/1887262321636221412))
- [2024/12] ğŸ”¥ v0.4 Release: Zero-Overhead Batch Scheduler, Cache-Aware Load Balancer, Faster Structured Outputs ([blog](https://lmsys.org/blog/2024-12-04-sglang-v0-4/)).
- [2024/09] v0.3 Release: 7x Faster DeepSeek MLA, 1.5x Faster torch.compile, Multi-Image/Video LLaVA-OneVision ([blog](https://lmsys.org/blog/2024-09-04-sglang-v0-3/)).
- [2024/07] v0.2 Release: Faster Llama3 Serving with SGLang Runtime (vs. TensorRT-LLM, vLLM) ([blog](https://lmsys.org/blog/2024-07-25-sglang-llama3/)).

&lt;details&gt;
&lt;summary&gt;More&lt;/summary&gt;

- [2024/10] The First SGLang Online Meetup ([slides](https://github.com/sgl-project/sgl-learning-materials?tab=readme-ov-file#the-first-sglang-online-meetup)).
- [2024/02] SGLang enables **3x faster JSON decoding** with compressed finite state machine ([blog](https://lmsys.org/blog/2024-02-05-compressed-fsm/)).
- [2024/01] SGLang provides up to **5x faster inference** with RadixAttention ([blog](https://lmsys.org/blog/2024-01-17-sglang/)).
- [2024/01] SGLang powers the serving of the official **LLaVA v1.6** release demo ([usage](https://github.com/haotian-liu/LLaVA?tab=readme-ov-file#demo)).

&lt;/details&gt;

## About
SGLang is a fast serving framework for large language models and vision language models.
It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.
The core features include:

- **Fast Backend Runtime**: Provides efficient serving with RadixAttention for prefix caching, zero-overhead CPU scheduler, continuous batching, token attention (paged attention), speculative decoding, tensor parallelism, chunked prefill, structured outputs, and quantization (FP8/INT4/AWQ/GPTQ).
- **Flexible Frontend Language**: Offers an intuitive interface for programming LLM applications, including chained generation calls, advanced prompting, control flow, multi-modal inputs, parallelism, and external interactions.
- **Extensive Model Support**: Supports a wide range of generative models (Llama, Gemma, Mistral, QWen, DeepSeek, LLaVA, etc.), embedding models (e5-mistral, gte, mcdse) and reward models (Skywork), with easy extensibility for integrating new models.
- **Active Community**: SGLang is open-source and backed by an active community with industry adoption.

## Getting Started
- [Install SGLang](https://docs.sglang.ai/start/install.html)
- [Quick Start](https://docs.sglang.ai/backend/send_request.html)
- [Backend Tutorial](https://docs.sglang.ai/backend/openai_api_completions.html)
- [Frontend Tutorial](https://docs.sglang.ai/frontend/frontend.html)
- [Contribution Guide](https://docs.sglang.ai/references/contribution_guide.html)

## Benchmark and Performance
Learn more in the release blogs: [v0.2 blog](https://lmsys.org/blog/2024-07-25-sglang-llama3/), [v0.3 blog](https://lmsys.org/blog/2024-09-04-sglang-v0-3/), [v0.4 blog](https://lmsys.org/blog/2024-12-04-sglang-v0-4/)

## Roadmap
[Development Roadmap (2025 H1)](https://github.com/sgl-project/sglang/issues/4042)

## Adoption and Sponsorship
The project has been deployed to large-scale production, generating trillions of tokens every day.
It is supported by the following institutions: AMD, Atlas Cloud, Baseten, Cursor, DataCrunch, Etched, Hyperbolic, Iflytek, Jam &amp; Tea Studios, LinkedIn, LMSYS, Meituan, Nebius, Novita AI, NVIDIA, RunPod, Stanford, UC Berkeley, UCLA, xAI, and 01.AI.

&lt;img src=&quot;https://raw.githubusercontent.com/sgl-project/sgl-learning-materials/main/slides/adoption.png&quot; alt=&quot;logo&quot; width=&quot;800&quot; margin=&quot;10px&quot;&gt;&lt;/img&gt;

## Contact Us

For enterprises interested in adopting or deploying SGLang at scale, including technical consulting, sponsorship opportunities, or partnership inquiries, please contact us at contact@sglang.ai.

## Acknowledgment and Citation
We learned the design and reused code from the following projects: [Guidance](https://github.com/guidance-ai/guidance), [vLLM](https://github.com/vllm-project/vllm), [LightLLM](https://github.com/ModelTC/lightllm), [FlashInfer](https://github.com/flashinfer-ai/flashinfer), [Outlines](https://github.com/outlines-dev/outlines), and [LMQL](https://github.com/eth-sri/lmql). Please cite the paper, [SGLang: Efficient Execution of Structured Language Model Programs](https://arxiv.org/abs/2312.07104), if you find the project useful.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[marketcalls/openalgo]]></title>
            <link>https://github.com/marketcalls/openalgo</link>
            <guid>https://github.com/marketcalls/openalgo</guid>
            <pubDate>Thu, 10 Apr 2025 00:04:18 GMT</pubDate>
            <description><![CDATA[Open Source Algo Trading Platform for Everyone]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/marketcalls/openalgo">marketcalls/openalgo</a></h1>
            <p>Open Source Algo Trading Platform for Everyone</p>
            <p>Language: Python</p>
            <p>Stars: 419</p>
            <p>Forks: 169</p>
            <p>Stars today: 24 stars today</p>
            <h2>README</h2><pre># OpenAlgo - Take Control of Your Algo Platform

&lt;div align=&quot;center&quot;&gt;

[![PyPI Downloads](https://static.pepy.tech/badge/openalgo)](https://pepy.tech/projects/openalgo)
[![PyPI Downloads](https://static.pepy.tech/badge/openalgo/month)](https://pepy.tech/projects/openalgo)
[![X (formerly Twitter) Follow](https://img.shields.io/twitter/follow/openalgoHQ)](https://twitter.com/openalgoHQ)
[![YouTube Channel Subscribers](https://img.shields.io/youtube/channel/subscribers/UCw7eVneIEyiTApy4RtxrJsQ)](https://youtube.com/@openalgoHQ)
[![Discord](https://img.shields.io/discord/1219847221055455263)](https://discord.com/invite/UPh7QPsNhP)

&lt;/div&gt;

OpenAlgo is an open-source, Flask-based Python application designed to bridge the gap between traders and major trading platforms such as Amibroker, Tradingview, Python, Chartink, MetaTrader, Excel, and Google Spreadsheets. With a focus on simplifying algotrading, OpenAlgo facilitates easy integration, automation, and execution of trading strategies, providing a user-friendly interface to enhance trading performance.

## Installation Guide
[![OpenAlgo Windows Installation Tutorial](https://img.youtube.com/vi/PCPAeDKTh50/0.jpg)](https://www.youtube.com/watch?v=PCPAeDKTh50 &quot;Watch the Installation Tutorial&quot;)

For detailed installation instructions, please refer to [INSTALL.md](INSTALL.md)

## What is OpenAlgo?
[![What is OpenAlgo](https://img.youtube.com/vi/kAS3jTb3OkI/0.jpg)](https://www.youtube.com/watch?v=kAS3jTb3OkI &quot;Watch the OpenAlgo Tutorial Video&quot;)

## Supported Brokers

- **5paisa**
- **5paisa (XTS)**
- **AliceBlue**
- **AngelOne**
- **Dhan**
- **Firstock**
- **Flattrade**
- **Fyers**
- **ICICI Direct**
- **Jainam (Retail)**
- **Jainam (Dealer)**
- **Kotak** 
- **Paytm**
- **Shoonya**
- **Upstox**
- **Wisdom Capital**
- **Zebu**
- **Zerodha**

## Features

- **ChartInk Platform Integration**: 
  - Direct integration with ChartInk for strategy execution
  - Automated scanning and trading based on ChartInk signals
  - Real-time strategy monitoring and management
  - Custom strategy configuration and deployment
  - Seamless execution of ChartInk strategies through your broker

- **Advanced Monitoring Tools**:
  - **Latency Monitor**: Track and analyze order execution performance
    - Real-time latency tracking across different brokers
    - Detailed breakdown of execution times
    - Performance comparison between brokers
    - Order execution success rates and patterns
  - **Traffic Monitor**: Monitor system performance and API usage
    - Real-time API request tracking
    - Endpoint-specific analytics
    - Error rate monitoring
    - System performance metrics
  For detailed information about monitoring tools, see [traffic.md](docs/traffic.md)

- **Modern UI with DaisyUI**: 
  - Sleek and responsive interface built with DaisyUI components
  - Three distinct themes:
    - Light theme for normal mode
    - Dark theme for reduced eye strain
    - Garden theme for analyzer mode
  - Instant theme switching with state preservation
  - Theme-aware syntax highlighting for code and JSON
  - Mobile-friendly layout with drawer navigation

- **Real-Time Trading Updates**:
  - Instant order book updates via WebSocket
  - Live trade book monitoring with automatic refresh
  - Real-time position tracking
  - Dynamic log updates for trade activities
  - Contextual notifications with sound alerts

- **API Analyzer**:
  - Real-time request validation and testing
  - Strategy testing without live execution
  - Detailed request/response analysis
  - Comprehensive error detection
  - Dedicated garden theme for better focus
  - See [Analyzer.md](docs/Analyzer.md) for detailed documentation

- **Comprehensive Integration**: Seamlessly connect with Amibroker, Tradingview, Excel, and Google Spreadsheets for smooth data and strategy transition.

- **User-Friendly Interface**: A straightforward Flask-based application interface accessible to traders of all levels of expertise.

- **Real-Time Execution**: Implement your trading strategies in real time, ensuring immediate action to capitalize on market opportunities.

- **Customizable Strategies**: Easily adapt and tailor your trading strategies to meet your specific needs, with extensive options for customization and automation.

- **Secure and Reliable**: With a focus on security and reliability, OpenAlgo provides a dependable platform for your algotrading activities, safeguarding your data and trades.

## Documentation

For detailed documentation on OpenAlgo, including setup guides, API references, and usage examples, refer to [https://docs.openalgo.in](https://docs.openalgo.in)

### Minimum Hardware Requirements

To run OpenAlgo we recommend:
- 2GB RAM
- 1GB disk space
- 2vCPU

## Contributing

We welcome contributions to OpenAlgo! If you&#039;re interested in improving the application or adding new features, please feel free to fork the repository, make your changes, and submit a pull request.

## License

OpenAlgo is released under the AGPL V3.0 License. See the `LICENSE` file for more details.

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=marketcalls/openalgo&amp;type=Date)](https://star-history.com/#marketcalls/openalgo&amp;Date)

## Disclaimer

This software is for educational purposes only. Do not risk money which
you are afraid to lose. USE THE SOFTWARE AT YOUR OWN RISK. THE AUTHORS
AND ALL AFFILIATES ASSUME NO RESPONSIBILITY FOR YOUR TRADING RESULTS.

## Support

For any questions not covered by the documentation or for further information about OpenAlgo, join our [Discord server](https://discord.com/invite/UPh7QPsNhP).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[meta-llama/llama-models]]></title>
            <link>https://github.com/meta-llama/llama-models</link>
            <guid>https://github.com/meta-llama/llama-models</guid>
            <pubDate>Thu, 10 Apr 2025 00:04:17 GMT</pubDate>
            <description><![CDATA[Utilities intended for use with Llama models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/meta-llama/llama-models">meta-llama/llama-models</a></h1>
            <p>Utilities intended for use with Llama models.</p>
            <p>Language: Python</p>
            <p>Stars: 6,700</p>
            <p>Forks: 1,092</p>
            <p>Stars today: 31 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/Llama_Repo.jpeg&quot; width=&quot;400&quot;/&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
        ğŸ¤— &lt;a href=&quot;https://huggingface.co/meta-Llama&quot;&gt; Models on Hugging Face&lt;/a&gt;&amp;nbsp | &lt;a href=&quot;https://ai.meta.com/blog/&quot;&gt; Blog&lt;/a&gt;&amp;nbsp |  &lt;a href=&quot;https://llama.meta.com/&quot;&gt;Website&lt;/a&gt;&amp;nbsp | &lt;a href=&quot;https://llama.meta.com/get-started/&quot;&gt;Get Started&lt;/a&gt;&amp;nbsp | &lt;a href=&quot;https://github.com/meta-llama/llama-cookbook&quot;&gt;Llama Cookbook&lt;/a&gt;&amp;nbsp
&lt;br&gt;

---

# Llama Models

Llama is an accessible, open large language model (LLM) designed for developers, researchers, and businesses to build, experiment, and responsibly scale their generative AI ideas. Part of a foundational system, it serves as a bedrock for innovation in the global community. A few key aspects:
1. **Open access**: Easy accessibility to cutting-edge large language models, fostering collaboration and advancements among developers, researchers, and organizations
2. **Broad ecosystem**: Llama models have been downloaded hundreds of millions of times, there are thousands of community projects built on Llama and platform support is broad from cloud providers to startups - the world is building with Llama!
3. **Trust &amp; safety**: Llama models are part of a comprehensive approach to trust and safety, releasing models and tools that are designed to enable community collaboration and encourage the standardization of the development and usage of trust and safety tools for generative AI

Our mission is to empower individuals and industry through this opportunity while fostering an environment of discovery and ethical AI advancements. The model weights are licensed for researchers and commercial entities, upholding the principles of openness.

## Llama Models

[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-models)](https://pypi.org/project/llama-models/)
[![Discord](https://img.shields.io/discord/1257833999603335178)](https://discord.gg/TZAAYNVtrU)

|  **Model** | **Launch date** | **Model sizes** | **Context Length** | **Tokenizer** | **Acceptable use policy**  |  **License** | **Model Card** |
| :----: | :----: | :----: | :----:|:----:|:----:|:----:|:----:|
| Llama 2 | 7/18/2023 | 7B, 13B, 70B | 4K | Sentencepiece | [Use Policy](models/llama2/USE_POLICY.md) | [License](models/llama2/LICENSE) | [Model Card](models/llama2/MODEL_CARD.md) |
| Llama 3 | 4/18/2024 | 8B, 70B | 8K | TikToken-based | [Use Policy](models/llama3/USE_POLICY.md) | [License](models/llama3/LICENSE) | [Model Card](models/llama3/MODEL_CARD.md) |
| Llama 3.1 | 7/23/2024 | 8B, 70B, 405B | 128K | TikToken-based | [Use Policy](models/llama3_1/USE_POLICY.md) | [License](models/llama3_1/LICENSE) | [Model Card](models/llama3_1/MODEL_CARD.md) |
| Llama 3.2 | 9/25/2024 | 1B, 3B | 128K | TikToken-based | [Use Policy](models/llama3_2/USE_POLICY.md) | [License](models/llama3_2/LICENSE) | [Model Card](models/llama3_2/MODEL_CARD.md) |
| Llama 3.2-Vision | 9/25/2024 | 11B, 90B | 128K | TikToken-based | [Use Policy](models/llama3_2/USE_POLICY.md) | [License](models/llama3_2/LICENSE) | [Model Card](models/llama3_2/MODEL_CARD_VISION.md) |
| Llama 3.3 | 12/04/2024 | 11B, 90B | 128K | TikToken-based | [Use Policy](models/llama3_3/USE_POLICY.md) | [License](models/llama3_3/LICENSE) | [Model Card](models/llama3_3/MODEL_CARD.md) |
| Llama 4 | 4/5/2025 | Scout-17B-16E, Maverick-17B-128E | 10M, 1M | TikToken-based | [Use Policy](models/llama4/USE_POLICY.md) | [License](models/llama4/LICENSE) | [Model Card](models/llama4/MODEL_CARD.md) |

## Download

To download the model weights and tokenizer:

1. Visit the [Meta Llama website](https://llama.meta.com/llama-downloads/).
2. Read and accept the license.
3. Once your request is approved you will receive a signed URL via email.
4. Install the [Llama CLI](https://github.com/meta-llama/llama-stack): `pip install llama-stack`. (**&lt;-- Start Here if you have received an email already.**)
5. Run `llama model list` to show the latest available models and determine the model ID you wish to download. **NOTE**:
If you want older versions of models, run `llama model list --show-all` to show all the available Llama models.

6. Run: `llama download --source meta --model-id CHOSEN_MODEL_ID`
7. Pass the URL provided when prompted to start the download.

Remember that the links expire after 24 hours and a certain amount of downloads. You can always re-request a link if you start seeing errors such as `403: Forbidden`.

## Running the models

In order to run the models, you will need to install dependencies after checking out the repository.

```bash
# Run this within a suitable Python environment (uv, conda, or virtualenv)
pip install .[torch]
```

Example scripts are available in `models/{ llama3, llama4 }/scripts/` sub-directory. Note that the Llama4 series of models require at least 4 GPUs to run inference at full (bf16) precision.

```bash
#!/bin/bash

NGPUS=4
CHECKPOINT_DIR=~/.llama/checkpoints/Llama-4-Scout-17B-16E-Instruct
PYTHONPATH=$(git rev-parse --show-toplevel) \
  torchrun --nproc_per_node=$NGPUS \
  -m models.llama4.scripts.chat_completion $CHECKPOINT_DIR \
  --world_size $NGPUS
```

The above script should be used with an Instruct (Chat) model. For a Base model, update the `CHECKPOINT_DIR` path and use the script `models.llama4.scripts.completion`.


## Running inference with FP8 and Int4 Quantization

You can reduce the memory footprint of the models at the cost of minimal loss in accuracy by running inference with FP8 or Int4 quantization. Use the `--quantization-mode` flag to specify the quantization mode. There are two modes:
- `fp8_mixed`: Mixed precision inference with FP8 for some weights and bfloat16 for activations.
- `int4_mixed`: Mixed precision inference with Int4 for some weights and bfloat16 for activations.

Using FP8, running Llama-4-Scout-17B-16E-Instruct requires 2 GPUs with 80GB of memory. Using Int4, you need a single GPU with 80GB of memory.

```bash
MODE=fp8_mixed  # or int4_mixed
if [ $MODE == &quot;fp8_mixed&quot; ]; then
  NGPUS=2
else
  NGPUS=1
fi
CHECKPOINT_DIR=~/.llama/checkpoints/Llama-4-Scout-17B-16E-Instruct
PYTHONPATH=$(git rev-parse --show-toplevel) \
  torchrun --nproc_per_node=$NGPUS \
  -m models.llama4.scripts.chat_completion $CHECKPOINT_DIR \
  --world_size $NGPUS \
  --quantization-mode $MODE
```


For more flexibility in running inference (including using other providers), please see the [`Llama Stack`](https://github.com/meta-llama/llama-stack) toolset.


## Access to Hugging Face

We also provide downloads on [Hugging Face](https://huggingface.co/meta-llama), in both transformers and native `llama4` formats. To download the weights from Hugging Face, please follow these steps:

- Visit one of the repos, for example [meta-llama/Llama-4-Scout-17B-16E](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E).
- Read and accept the license. Once your request is approved, you&#039;ll be granted access to all Llama 3.1 models as well as previous versions. Note that requests used to take up to one hour to get processed.
- To download the original native weights to use with this repo, click on the &quot;Files and versions&quot; tab and download the contents of the `original` folder. You can also download them from the command line if you `pip install huggingface-hub`:

```bash
huggingface-cli download meta-llama/Llama-4-Scout-17B-16E-Instruct-Original --local-dir meta-llama/Llama-4-Scout-17B-16E-Instruct-Original
```

- To use with transformers, the following snippet will download and cache the weights:

  ```python
  # inference.py
  from transformers import AutoTokenizer, Llama4ForConditionalGeneration
  import torch

  model_id = &quot;meta-llama/Llama-4-Scout-17B-16E-Instruct&quot;

  tokenizer = AutoTokenizer.from_pretrained(model_id)

  messages = [
      {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who are you?&quot;},
  ]
  inputs = tokenizer.apply_chat_template(
      messages, add_generation_prompt=True, return_tensors=&quot;pt&quot;, return_dict=True
  )

  model = Llama4ForConditionalGeneration.from_pretrained(
      model_id, device_map=&quot;auto&quot;, torch_dtype=torch.bfloat16
  )

  outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)
  outputs = tokenizer.batch_decode(outputs[:, inputs[&quot;input_ids&quot;].shape[-1] :])
  print(outputs[0])
  ```
  ```bash
   torchrun --nnodes=1 --nproc_per_node=8 inference.py
   ```

## Installations

You can install this repository as a [package](https://pypi.org/project/llama-models/) by just doing `pip install llama-models`

## Responsible Use

Llama models are a new technology that carries potential risks with use. Testing conducted to date has not â€” and could not â€” cover all scenarios.
To help developers address these risks, we have created the [Responsible Use Guide](https://ai.meta.com/static-resource/responsible-use-guide/).

## Issues

Please report any software â€œbugâ€ or other problems with the models through one of the following means:
- Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues)
- Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)
- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)


## Questions

For common questions, the FAQ can be found [here](https://llama.meta.com/faq), which will be updated over time as new questions arise.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>