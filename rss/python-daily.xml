<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Tue, 09 Sep 2025 00:04:14 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[emcie-co/parlant]]></title>
            <link>https://github.com/emcie-co/parlant</link>
            <guid>https://github.com/emcie-co/parlant</guid>
            <pubDate>Tue, 09 Sep 2025 00:04:14 GMT</pubDate>
            <description><![CDATA[LLM agents built for control. Designed for real-world use. Deployed in minutes.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/emcie-co/parlant">emcie-co/parlant</a></h1>
            <p>LLM agents built for control. Designed for real-world use. Deployed in minutes.</p>
            <p>Language: Python</p>
            <p>Stars: 10,728</p>
            <p>Forks: 863</p>
            <p>Stars today: 1,178 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/emcie-co/parlant/blob/develop/docs/LogoTransparentLight.png?raw=true&quot;&gt;
  &lt;img alt=&quot;Parlant - AI Agent Framework&quot; src=&quot;https://github.com/emcie-co/parlant/blob/develop/docs/LogoTransparentDark.png?raw=true&quot; width=400 /&gt;
&lt;/picture&gt;

&lt;h3&gt;Finally, LLM agents that actually follow instructions&lt;/h3&gt;

&lt;p&gt;
  &lt;a href=&quot;https://www.parlant.io/&quot; target=&quot;_blank&quot;&gt;üåê Website&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://www.parlant.io/docs/quickstart/installation&quot; target=&quot;_blank&quot;&gt;‚ö° Quick Start&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://discord.gg/duxWqxKk6J&quot; target=&quot;_blank&quot;&gt;üí¨ Discord&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://www.parlant.io/docs/quickstart/examples&quot; target=&quot;_blank&quot;&gt;üìñ Examples&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://zdoc.app/de/emcie-co/parlant&quot;&gt;Deutsch&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/es/emcie-co/parlant&quot;&gt;Espa√±ol&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/fr/emcie-co/parlant&quot;&gt;fran√ßais&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/ja/emcie-co/parlant&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/ko/emcie-co/parlant&quot;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/pt/emcie-co/parlant&quot;&gt;Portugu√™s&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/ru/emcie-co/parlant&quot;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; |
  &lt;a href=&quot;https://zdoc.app/zh/emcie-co/parlant&quot;&gt;‰∏≠Êñá&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;
  &lt;a href=&quot;https://pypi.org/project/parlant/&quot;&gt;&lt;img alt=&quot;PyPI&quot; src=&quot;https://img.shields.io/pypi/v/parlant?color=blue&quot;&gt;&lt;/a&gt;
  &lt;img alt=&quot;Python 3.10+&quot; src=&quot;https://img.shields.io/badge/python-3.10+-blue&quot;&gt;
  &lt;a href=&quot;https://opensource.org/licenses/Apache-2.0&quot;&gt;&lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/badge/license-Apache%202.0-green&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/duxWqxKk6J&quot;&gt;&lt;img alt=&quot;Discord&quot; src=&quot;https://img.shields.io/discord/1312378700993663007?color=7289da&amp;logo=discord&amp;logoColor=white&quot;&gt;&lt;/a&gt;
  &lt;img alt=&quot;GitHub Repo stars&quot; src=&quot;https://img.shields.io/github/stars/emcie-co/parlant?style=social&quot;&gt;
&lt;/p&gt;

&lt;a href=&quot;https://trendshift.io/repositories/12768&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://trendshift.io/api/badge/repositories/12768&quot; alt=&quot;Trending on TrendShift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
&lt;/a&gt;

&lt;/div&gt;

## üéØ The Problem Every AI Developer Faces

You build an AI agent. It works great in testing. Then real users start talking to it and...

- ‚ùå It ignores your carefully crafted system prompts
- ‚ùå It hallucinates responses in critical moments
- ‚ùå It can&#039;t handle edge cases consistently
- ‚ùå Each conversation feels like a roll of the dice

**Sound familiar?** You&#039;re not alone. This is the #1 pain point for developers building production AI agents.

## ‚ö° The Solution: Stop Fighting Prompts, Teach Principles

Parlant flips the script on AI agent development. Instead of hoping your LLM will follow instructions, **Parlant ensures it**.

```python
# Traditional approach: Cross your fingers ü§û
system_prompt = &quot;You are a helpful assistant. Please follow these 47 rules...&quot;

# Parlant approach: Ensured compliance ‚úÖ
await agent.create_guideline(
    condition=&quot;Customer asks about refunds&quot;,
    action=&quot;Check order status first to see if eligible&quot;,
    tools=[check_order_status],
)
```

#### Parlant gives you all the structure you need to build customer-facing agents that behave exactly as your business requires:

- **[Journeys](https://parlant.io/docs/concepts/customization/journeys)**:
  Define clear customer journeys and how your agent should respond at each step.

- **[Behavioral Guidelines](https://parlant.io/docs/concepts/customization/guidelines)**:
  Easily craft agent behavior; Parlant will match the relevant elements contextually.

- **[Tool Use](https://parlant.io/docs/concepts/customization/tools)**:
  Attach external APIs, data fetchers, or backend services to specific interaction events.

- **[Domain Adaptation](https://parlant.io/docs/concepts/customization/glossary)**:
  Teach your agent domain-specific terminology and craft personalized responses.

- **[Canned Responses](https://parlant.io/docs/concepts/customization/canned-responses)**:
  Use response templates to eliminate hallucinations and guarantee style consistency.

- **[Explainability](https://parlant.io/docs/advanced/explainability)**:
  Understand why and when each guideline was matched and followed.

&lt;div align=&quot;center&quot;&gt;

## üöÄ Get Your Agent Running in 60 Seconds

&lt;/div&gt;

```bash
pip install parlant
```

```python
import parlant.sdk as p

@p.tool
async def get_weather(context: p.ToolContext, city: str) -&gt; p.ToolResult:
    # Your weather API logic here
    return p.ToolResult(f&quot;Sunny, 72¬∞F in {city}&quot;)

@p.tool
async def get_datetime(context: p.ToolContext) -&gt; p.ToolResult:
    from datetime import datetime
    return p.ToolResult(datetime.now())

async def main():
    async with p.Server() as server:
        agent = await server.create_agent(
            name=&quot;WeatherBot&quot;,
            description=&quot;Helpful weather assistant&quot;
        )

        # Have the agent&#039;s context be updated on every response (though
        # update interval is customizable) using a context variable.
        await agent.create_variable(name=&quot;current-datetime&quot;, tool=get_datetime)

        # Control and guide agent behavior with natural language
        await agent.create_guideline(
            condition=&quot;User asks about weather&quot;,
            action=&quot;Get current weather and provide a friendly response with suggestions&quot;,
            tools=[get_weather]
        )

        # Add other (reliably enforced) behavioral modeling elements
        # ...

        # üéâ Test playground ready at http://localhost:8800
        # Integrate the official React widget into your app,
        # or follow the tutorial to build your own frontend!

if __name__ == &quot;__main__&quot;:
    import asyncio
    asyncio.run(main())
```

**That&#039;s it!** Your agent is running with ensured rule-following behavior.

## üé¨ See It In Action

&lt;img alt=&quot;Parlant Demo&quot; src=&quot;https://github.com/emcie-co/parlant/blob/develop/docs/demo.gif?raw=true&quot; width=&quot;100%&quot; /&gt;

## üî• Why Developers Are Switching to Parlant

&lt;table width=&quot;100%&quot;&gt;
&lt;tr&gt;
  &lt;td width=&quot;50%&quot;&gt;

### üèóÔ∏è **Traditional AI Frameworks**

  &lt;/td&gt;
  &lt;td width=&quot;50%&quot;&gt;

### ‚ö° **Parlant**

  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td width=&quot;50%&quot;&gt;

- Write complex system prompts
- Hope the LLM follows them
- Debug unpredictable behaviors
- Scale by prompt engineering
- Cross fingers for reliability

&lt;/td&gt;
&lt;td width=&quot;50%&quot;&gt;

- Define rules in natural language
- **Ensured** rule compliance
- Predictable, consistent behavior
- Scale by adding guidelines
- Production-ready from day one

&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

## üéØ Perfect For Your Use Case

&lt;div align=&quot;center&quot;&gt;

|  **Financial Services**  |     **Healthcare**      |       **E-commerce**        |       **Legal Tech**       |
| :----------------------: | :---------------------: | :-------------------------: | :------------------------: |
| Compliance-first design  |   HIPAA-ready agents    |  Customer service at scale  |   Precise legal guidance   |
| Built-in risk management | Patient data protection | Order processing automation | Document review assistance |

&lt;/div&gt;

## üõ†Ô∏è Enterprise-Grade Features

- **üß≠ Conversational Journeys** - Lead the customer step-by-step to a goal
- **üéØ Dynamic Guideline Matching** - Context-aware rule application
- **üîß Reliable Tool Integration** - APIs, databases, external services
- **üìä Conversation Analytics** - Deep insights into agent behavior
- **üîÑ Iterative Refinement** - Continuously improve agent responses
- **üõ°Ô∏è Built-in Guardrails** - Prevent hallucination and off-topic responses
- **üì± React Widget** - [Drop-in chat UI for any web app](https://github.com/emcie-co/parlant-chat-react)
- **üîç Full Explainability** - Understand every decision your agent makes

## üìà Join 8,000+ Developers Building Better AI

&lt;div align=&quot;center&quot;&gt;

**Companies using Parlant:**

_Financial institutions ‚Ä¢ Healthcare providers ‚Ä¢ Legal firms ‚Ä¢ E-commerce platforms_

[![Star History Chart](https://api.star-history.com/svg?repos=emcie-co/parlant&amp;type=Date)](https://star-history.com/#emcie-co/parlant&amp;Date)

&lt;/div&gt;

## üåü What Developers Are Saying

&gt; _&quot;By far the most elegant conversational AI framework that I&#039;ve come across! Developing with Parlant is pure joy.&quot;_ **‚Äî Vishal Ahuja, Senior Lead, Customer-Facing Conversational AI @ JPMorgan Chase**

## üèÉ‚Äç‚ôÇÔ∏è Quick Start Paths

&lt;table border=&quot;0&quot;&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;üéØ I want to test it myself&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.parlant.io/docs/quickstart/installation&quot;&gt;‚Üí 5-minute quickstart&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;üõ†Ô∏è I want to see an example&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.parlant.io/docs/quickstart/examples&quot;&gt;‚Üí Healthcare agent example&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;üöÄ I want to get involved&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://discord.gg/duxWqxKk6J&quot;&gt;‚Üí Join our Discord community&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

## ü§ù Community &amp; Support

- üí¨ **[Discord Community](https://discord.gg/duxWqxKk6J)** - Get help from the team and community
- üìñ **[Documentation](https://parlant.io/docs/quickstart/installation)** - Comprehensive guides and examples
- üêõ **[GitHub Issues](https://github.com/emcie-co/parlant/issues)** - Bug reports and feature requests
- üìß **[Direct Support](https://parlant.io/contact)** - Direct line to our engineering team

## üìÑ License

Apache 2.0 - Use it anywhere, including commercial projects.

---

&lt;div align=&quot;center&quot;&gt;

**Ready to build AI agents that actually work?**

‚≠ê **Star this repo** ‚Ä¢ üöÄ **[Try Parlant now](https://parlant.io/)** ‚Ä¢ üí¨ **[Join Discord](https://discord.gg/duxWqxKk6J)**

_Built with ‚ù§Ô∏è by the team at [Emcie](https://emcie.co)_

&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Cinnamon/kotaemon]]></title>
            <link>https://github.com/Cinnamon/kotaemon</link>
            <guid>https://github.com/Cinnamon/kotaemon</guid>
            <pubDate>Tue, 09 Sep 2025 00:04:13 GMT</pubDate>
            <description><![CDATA[An open-source RAG-based tool for chatting with your documents.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Cinnamon/kotaemon">Cinnamon/kotaemon</a></h1>
            <p>An open-source RAG-based tool for chatting with your documents.</p>
            <p>Language: Python</p>
            <p>Stars: 23,386</p>
            <p>Forks: 1,913</p>
            <p>Stars today: 116 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# kotaemon

An open-source clean &amp; customizable RAG UI for chatting with your documents. Built with both end users and
developers in mind.

![Preview](https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/preview-graph.png)

&lt;a href=&quot;https://trendshift.io/repositories/11607&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11607&quot; alt=&quot;Cinnamon%2Fkotaemon | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

[Live Demo #1](https://huggingface.co/spaces/cin-model/kotaemon) |
[Live Demo #2](https://huggingface.co/spaces/cin-model/kotaemon-demo) |
[Online Install](https://cinnamon.github.io/kotaemon/online_install/) |
[Colab Notebook (Local RAG)](https://colab.research.google.com/drive/1eTfieec_UOowNizTJA1NjawBJH9y_1nn)

[User Guide](https://cinnamon.github.io/kotaemon/) |
[Developer Guide](https://cinnamon.github.io/kotaemon/development/) |
[Feedback](https://github.com/Cinnamon/kotaemon/issues) |
[Contact](mailto:kotaemon.support@cinnamon.is)

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-31013/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
&lt;a href=&quot;https://github.com/Cinnamon/kotaemon/pkgs/container/kotaemon&quot; target=&quot;_blank&quot;&gt;
&lt;img src=&quot;https://img.shields.io/badge/docker_pull-kotaemon:latest-brightgreen&quot; alt=&quot;docker pull ghcr.io/cinnamon/kotaemon:latest&quot;&gt;&lt;/a&gt;
![download](https://img.shields.io/github/downloads/Cinnamon/kotaemon/total.svg?label=downloads&amp;color=blue)
&lt;a href=&#039;https://huggingface.co/spaces/cin-model/kotaemon-demo&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#039;&gt;&lt;/a&gt;
&lt;a href=&quot;https://hellogithub.com/en/repository/d3141471a0244d5798bc654982b263eb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=d3141471a0244d5798bc654982b263eb&amp;claim_uid=RLiD9UZ1rEHNaMf&amp;theme=small&quot; alt=&quot;FeaturedÔΩúHelloGitHub&quot; /&gt;&lt;/a&gt;

&lt;/div&gt;

&lt;!-- start-intro --&gt;

## Introduction

This project serves as a functional RAG UI for both end users who want to do QA on their
documents and developers who want to build their own RAG pipeline.
&lt;br&gt;

```yml
+----------------------------------------------------------------------------+
| End users: Those who use apps built with `kotaemon`.                       |
| (You use an app like the one in the demo above)                            |
|     +----------------------------------------------------------------+     |
|     | Developers: Those who built with `kotaemon`.                   |     |
|     | (You have `import kotaemon` somewhere in your project)         |     |
|     |     +----------------------------------------------------+     |     |
|     |     | Contributors: Those who make `kotaemon` better.    |     |     |
|     |     | (You make PR to this repo)                         |     |     |
|     |     +----------------------------------------------------+     |     |
|     +----------------------------------------------------------------+     |
+----------------------------------------------------------------------------+
```

### For end users

- **Clean &amp; Minimalistic UI**: A user-friendly interface for RAG-based QA.
- **Support for Various LLMs**: Compatible with LLM API providers (OpenAI, AzureOpenAI, Cohere, etc.) and local LLMs (via `ollama` and `llama-cpp-python`).
- **Easy Installation**: Simple scripts to get you started quickly.

### For developers

- **Framework for RAG Pipelines**: Tools to build your own RAG-based document QA pipeline.
- **Customizable UI**: See your RAG pipeline in action with the provided UI, built with &lt;a href=&#039;https://github.com/gradio-app/gradio&#039;&gt;Gradio &lt;img src=&#039;https://img.shields.io/github/stars/gradio-app/gradio&#039;&gt;&lt;/a&gt;.
- **Gradio Theme**: If you use Gradio for development, check out our theme here: [kotaemon-gradio-theme](https://github.com/lone17/kotaemon-gradio-theme).

## Key Features

- **Host your own document QA (RAG) web-UI**: Support multi-user login, organize your files in private/public collections, collaborate and share your favorite chat with others.

- **Organize your LLM &amp; Embedding models**: Support both local LLMs &amp; popular API providers (OpenAI, Azure, Ollama, Groq).

- **Hybrid RAG pipeline**: Sane default RAG pipeline with hybrid (full-text &amp; vector) retriever and re-ranking to ensure best retrieval quality.

- **Multi-modal QA support**: Perform Question Answering on multiple documents with figures and tables support. Support multi-modal document parsing (selectable options on UI).

- **Advanced citations with document preview**: By default the system will provide detailed citations to ensure the correctness of LLM answers. View your citations (incl. relevant score) directly in the _in-browser PDF viewer_ with highlights. Warning when retrieval pipeline return low relevant articles.

- **Support complex reasoning methods**: Use question decomposition to answer your complex/multi-hop question. Support agent-based reasoning with `ReAct`, `ReWOO` and other agents.

- **Configurable settings UI**: You can adjust most important aspects of retrieval &amp; generation process on the UI (incl. prompts).

- **Extensible**: Being built on Gradio, you are free to customize or add any UI elements as you like. Also, we aim to support multiple strategies for document indexing &amp; retrieval. `GraphRAG` indexing pipeline is provided as an example.

![Preview](https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/preview.png)

## Installation

&gt; If you are not a developer and just want to use the app, please check out our easy-to-follow [User Guide](https://cinnamon.github.io/kotaemon/). Download the `.zip` file from the [latest release](https://github.com/Cinnamon/kotaemon/releases/latest) to get all the newest features and bug fixes.

### System requirements

1. [Python](https://www.python.org/downloads/) &gt;= 3.10
2. [Docker](https://www.docker.com/): optional, if you [install with Docker](#with-docker-recommended)
3. [Unstructured](https://docs.unstructured.io/open-source/installation/full-installation#full-installation) if you want to process files other than `.pdf`, `.html`, `.mhtml`, and `.xlsx` documents. Installation steps differ depending on your operating system. Please visit the link and follow the specific instructions provided there.

### With Docker (recommended)

1. We support both `lite` &amp; `full` version of Docker images. With `full` version, the extra packages of `unstructured` will be installed, which can support additional file types (`.doc`, `.docx`, ...) but the cost is larger docker image size. For most users, the `lite` image should work well in most cases.

   - To use the `full` version.

     ```bash
     docker run \
     -e GRADIO_SERVER_NAME=0.0.0.0 \
     -e GRADIO_SERVER_PORT=7860 \
     -v ./ktem_app_data:/app/ktem_app_data \
     -p 7860:7860 -it --rm \
     ghcr.io/cinnamon/kotaemon:main-full
     ```

   - To use the `full` version with bundled **Ollama** for _local / private RAG_.

     ```bash
     # change image name to
     docker run &lt;...&gt; ghcr.io/cinnamon/kotaemon:main-ollama
     ```

   - To use the `lite` version.

   ```bash
    # change image name to
    docker run &lt;...&gt; ghcr.io/cinnamon/kotaemon:main-lite
   ```

2. We currently support and test two platforms: `linux/amd64` and `linux/arm64` (for newer Mac). You can specify the platform by passing `--platform` in the `docker run` command. For example:

   ```bash
   # To run docker with platform linux/arm64
   docker run \
   -e GRADIO_SERVER_NAME=0.0.0.0 \
   -e GRADIO_SERVER_PORT=7860 \
   -v ./ktem_app_data:/app/ktem_app_data \
   -p 7860:7860 -it --rm \
   --platform linux/arm64 \
   ghcr.io/cinnamon/kotaemon:main-lite
   ```

3. Once everything is set up correctly, you can go to `http://localhost:7860/` to access the WebUI.

4. We use [GHCR](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry) to store docker images, all images can be found [here.](https://github.com/Cinnamon/kotaemon/pkgs/container/kotaemon)

### Without Docker

1. Clone and install required packages on a fresh python environment.

   ```shell
   # optional (setup env)
   conda create -n kotaemon python=3.10
   conda activate kotaemon

   # clone this repo
   git clone https://github.com/Cinnamon/kotaemon
   cd kotaemon

   pip install -e &quot;libs/kotaemon[all]&quot;
   pip install -e &quot;libs/ktem&quot;
   ```

2. Create a `.env` file in the root of this project. Use `.env.example` as a template

   The `.env` file is there to serve use cases where users want to pre-config the models before starting up the app (e.g. deploy the app on HF hub). The file will only be used to populate the db once upon the first run, it will no longer be used in consequent runs.

3. (Optional) To enable in-browser `PDF_JS` viewer, download [PDF_JS_DIST](https://github.com/mozilla/pdf.js/releases/download/v4.0.379/pdfjs-4.0.379-dist.zip) then extract it to `libs/ktem/ktem/assets/prebuilt`

&lt;img src=&quot;https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/pdf-viewer-setup.png&quot; alt=&quot;pdf-setup&quot; width=&quot;300&quot;&gt;

4. Start the web server:

   ```shell
   python app.py
   ```

   - The app will be automatically launched in your browser.
   - Default username and password are both `admin`. You can set up additional users directly through the UI.

   ![Chat tab](https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/chat-tab.png)

5. Check the `Resources` tab and `LLMs and Embeddings` and ensure that your `api_key` value is set correctly from your `.env` file. If it is not set, you can set it there.

### Setup GraphRAG

&gt; [!NOTE]
&gt; Official MS GraphRAG indexing only works with OpenAI or Ollama API.
&gt; We recommend most users to use NanoGraphRAG implementation for straightforward integration with Kotaemon.

&lt;details&gt;

&lt;summary&gt;Setup Nano GRAPHRAG&lt;/summary&gt;

- Install nano-GraphRAG: `pip install nano-graphrag`
- `nano-graphrag` install might introduce version conflicts, see [this issue](https://github.com/Cinnamon/kotaemon/issues/440)
  - To quickly fix: `pip uninstall hnswlib chroma-hnswlib &amp;&amp; pip install chroma-hnswlib`
- Launch Kotaemon with `USE_NANO_GRAPHRAG=true` environment variable.
- Set your default LLM &amp; Embedding models in Resources setting and it will be recognized automatically from NanoGraphRAG.

&lt;/details&gt;

&lt;details&gt;

&lt;summary&gt;Setup LIGHTRAG&lt;/summary&gt;

- Install LightRAG: `pip install git+https://github.com/HKUDS/LightRAG.git`
- `LightRAG` install might introduce version conflicts, see [this issue](https://github.com/Cinnamon/kotaemon/issues/440)
  - To quickly fix: `pip uninstall hnswlib chroma-hnswlib &amp;&amp; pip install chroma-hnswlib`
- Launch Kotaemon with `USE_LIGHTRAG=true` environment variable.
- Set your default LLM &amp; Embedding models in Resources setting and it will be recognized automatically from LightRAG.

&lt;/details&gt;

&lt;details&gt;

&lt;summary&gt;Setup MS GRAPHRAG&lt;/summary&gt;

- **Non-Docker Installation**: If you are not using Docker, install GraphRAG with the following command:

  ```shell
  pip install &quot;graphrag&lt;=0.3.6&quot; future
  ```

- **Setting Up API KEY**: To use the GraphRAG retriever feature, ensure you set the `GRAPHRAG_API_KEY` environment variable. You can do this directly in your environment or by adding it to a `.env` file.
- **Using Local Models and Custom Settings**: If you want to use GraphRAG with local models (like `Ollama`) or customize the default LLM and other configurations, set the `USE_CUSTOMIZED_GRAPHRAG_SETTING` environment variable to true. Then, adjust your settings in the `settings.yaml.example` file.

&lt;/details&gt;

### Setup Local Models (for local/private RAG)

See [Local model setup](docs/local_model.md).

### Setup multimodal document parsing (OCR, table parsing, figure extraction)

These options are available:

- [Azure Document Intelligence (API)](https://azure.microsoft.com/en-us/products/ai-services/ai-document-intelligence)
- [Adobe PDF Extract (API)](https://developer.adobe.com/document-services/docs/overview/pdf-extract-api/)
- [Docling (local, open-source)](https://github.com/DS4SD/docling)
  - To use Docling, first install required dependencies: `pip install docling`

Select corresponding loaders in `Settings -&gt; Retrieval Settings -&gt; File loader`

### Customize your application

- By default, all application data is stored in the `./ktem_app_data` folder. You can back up or copy this folder to transfer your installation to a new machine.

- For advanced users or specific use cases, you can customize these files:

  - `flowsettings.py`
  - `.env`

#### `flowsettings.py`

This file contains the configuration of your application. You can use the example
[here](flowsettings.py) as the starting point.

&lt;details&gt;

&lt;summary&gt;Notable settings&lt;/summary&gt;

```python
# setup your preferred document store (with full-text search capabilities)
KH_DOCSTORE=(Elasticsearch | LanceDB | SimpleFileDocumentStore)

# setup your preferred vectorstore (for vector-based search)
KH_VECTORSTORE=(ChromaDB | LanceDB | InMemory | Milvus | Qdrant)

# Enable / disable multimodal QA
KH_REASONINGS_USE_MULTIMODAL=True

# Setup your new reasoning pipeline or modify existing one.
KH_REASONINGS = [
    &quot;ktem.reasoning.simple.FullQAPipeline&quot;,
    &quot;ktem.reasoning.simple.FullDecomposeQAPipeline&quot;,
    &quot;ktem.reasoning.react.ReactAgentPipeline&quot;,
    &quot;ktem.reasoning.rewoo.RewooAgentPipeline&quot;,
]
```

&lt;/details&gt;

#### `.env`

This file provides another way to configure your models and credentials.

&lt;details&gt;

&lt;summary&gt;Configure model via the .env file&lt;/summary&gt;

- Alternatively, you can configure the models via the `.env` file with the information needed to connect to the LLMs. This file is located in the folder of the application. If you don&#039;t see it, you can create one.

- Currently, the following providers are supported:

  - **OpenAI**

    In the `.env` file, set the `OPENAI_API_KEY` variable with your OpenAI API key in order
    to enable access to OpenAI&#039;s models. There are other variables that can be modified,
    please feel free to edit them to fit your case. Otherwise, the default parameter should
    work for most people.

    ```shell
    OPENAI_API_BASE=https://api.openai.com/v1
    OPENAI_API_KEY=&lt;your OpenAI API key here&gt;
    OPENAI_CHAT_MODEL=gpt-3.5-turbo
    OPENAI_EMBEDDINGS_MODEL=text-embedding-ada-002
    ```

  - **Azure OpenAI**

    For OpenAI models via Azure platform, you need to provide your Azure endpoint and API
    key. Your might also need to provide your developments&#039; name for the chat model and the
    embedding model depending on how you set up Azure development.

    ```shell
    AZURE_OPENAI_ENDPOINT=
    AZURE_OPENAI_API_KEY=
    OPENAI_API_VERSION=2024-02-15-preview
    AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-35-turbo
    AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT=text-embedding-ada-002
    ```

  - **Local Models**

    - Using `ollama` OpenAI compatible server:

      - Install [ollama](https://github.com/ollama/ollama) and start the application.

      - Pull your model, for example:

        ```shell
        ollama pull llama3.1:8b
        ollama pull nomic-embed-text
        ```

      - Set the model names on web UI and make it as default:

        ![Models](https://raw.githubusercontent.com/Cinnamon/kotaemon/main/docs/images/models.png)

    - Using `GGUF` with `llama-cpp-python`

      You can search and download a LLM to be ran locally from the [Hugging Face Hub](https://huggingface.co/models). Currently, these model formats are supported:

      - GGUF

        You should choose a model whose size is less than your device&#039;s memory and should leave
        about 2 GB. For example, if you have 16 GB of RAM in total, of which 12 GB is available,
        then you should choose a model that takes up at most 10 GB of RAM. Bigger models tend to
        give better generation but also take more processing time.

        Here are some recommendations and their size in memory:

      - [Qwen1.5-1.8B-Chat-GGUF](https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat-GGUF/resolve/main/qwen1_5-1_8b-chat-q8_0.gguf?download=true): around 2 GB

        Add a new LlamaCpp model with the provided model name on the web UI.

  &lt;/details&gt;

### Adding your own RAG pipeline

#### Custom Reasoning Pipeline

1. Check the default pipeline implementation in [here](libs/ktem/ktem/reasoning/simple.py). You can make quick adjustment to how the default QA pipeline work.
2. Add new `.py` implementation in `libs/ktem/ktem/reasoning/` and later include it in `flowssettings` to enable it on the UI.

#### Custom Indexing Pipeline

- Check sample implementation in `libs/ktem/ktem/index/file/graph`

&gt; (more instruction WIP).

&lt;!-- end-intro --&gt;

## Citation

Please cite this project as

```BibTeX
@misc{kotaemon2024,
    title = {Kotaemon - An open-source RAG-based tool for chatting with any content.},
    author = {The Kotaemon Team},
    year = {2024},
    howpublished = {\url{https://github.com/Cinnamon/kotaemon}},
}
```

## Star History

&lt;a href=&quot;https://star-history.com/#Cinnamon/kotaemon&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Cinnamon/kotaemon&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Cinnamon/kotaemon&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=Cinnamon/kotaemon&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;

## Contribution

Since our project is actively being developed, we greatly value your feedback and contributions. Please see our [Contributing Guide](https://github.com/Cinnamon/kotaemon/blob/main/CONTRIBUTING.md) to get started. Thank you to all our contributors!

&lt;a href=&quot;https://github.com/Cinnamon/kotaemon/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=Cinnamon/kotaemon&quot; /&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Vector-Wangel/XLeRobot]]></title>
            <link>https://github.com/Vector-Wangel/XLeRobot</link>
            <guid>https://github.com/Vector-Wangel/XLeRobot</guid>
            <pubDate>Tue, 09 Sep 2025 00:04:12 GMT</pubDate>
            <description><![CDATA[XLeRobot: Practical Dual-Arm Mobile Home Robot for $660]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Vector-Wangel/XLeRobot">Vector-Wangel/XLeRobot</a></h1>
            <p>XLeRobot: Practical Dual-Arm Mobile Home Robot for $660</p>
            <p>Language: Python</p>
            <p>Stars: 2,414</p>
            <p>Forks: 230</p>
            <p>Stars today: 522 stars today</p>
            <h2>README</h2><pre># [XLeRobot ü§ñ](https://xlerobot.readthedocs.io/en/latest/index.html)

[![en](https://img.shields.io/badge/lang-en-blue.svg)](README.md)
[![‰∏≠Êñá](https://img.shields.io/badge/lang-‰∏≠Êñá-brown.svg)](README_CN.md)

&lt;a href=&quot;https://xlerobot.readthedocs.io/en/latest/index.html&quot;&gt;
  &lt;img width=&quot;1725&quot; height=&quot;1140&quot; alt=&quot;front&quot; src=&quot;https://github.com/user-attachments/assets/f9c454ee-2c46-42b4-a5d7-88834a1c95ab&quot; /&gt;
&lt;/a&gt;

[![Apache License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Twitter/X](https://img.shields.io/twitter/follow/VectorWang?style=social)](https://twitter.com/VectorWang2)
[![Docs status](https://img.shields.io/badge/docs-passing-brightgreen.svg)](https://xlerobot.readthedocs.io/en/latest/)
[![Discord](https://img.shields.io/badge/Discord-XLeRobot-7289da?style=flat&amp;logo=discord&amp;logoColor=white)](https://discord.gg/bjZveEUh6F)
---


**üöÄ Bringing Embodied AI to Everyone - Cheaper Than an iPhone! üì±**  
**üíµ Starts from $660 cost and ‚è∞ &lt;4hrs total assembly time!!**

*Built upon the giants: [LeRobot](https://github.com/huggingface/lerobot), [SO-100/SO-101](https://github.com/TheRobotStudio/SO-ARM100), [Lekiwi](https://github.com/SIGRobotics-UIUC/LeKiwi), [Bambot](https://github.com/timqian/bambot)*

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/17e31979-bd5e-4790-be70-566ea8bb181e&quot; width=&quot;250&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/96ff4a3e-3402-47a2-bc6b-b45137ee3fdd&quot; width=&quot;250&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/f6d52acc-bc8d-46f6-b3cd-8821f0306a7f&quot; width=&quot;250&quot;/&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/59086300-3e6f-4a3c-b5e0-db893eeabc0c&quot; width=&quot;250&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/4ddbc0ff-ca42-4ad0-94c6-4e0f4047fd01&quot; width=&quot;250&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/7abc890e-9c9c-4983-8b25-122573028de5&quot; width=&quot;250&quot;/&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/e74a602b-0146-49c4-953d-3fa3b038a7f7&quot; width=&quot;250&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/d8090b15-97f3-4abc-98c8-208ae79894d5&quot; width=&quot;250&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/8b54adc3-d61b-42a0-8985-ea28f2e8f64c&quot; width=&quot;250&quot;/&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## üíµ Total Cost üíµ

&gt; [!NOTE] 
&gt; Cost excludes 3D printing, tools, shipping, and taxes.

| Price (Buy all the parts yourself) | US | EU | CN |
| --- | --- | --- | --- |
| **Basic** (use your laptop, single RGB head cam) | **~$660** | **~‚Ç¨680** | **~¬•3999** |
| ‚Üë Stereo dual-eye RGB head cam | +$30 | +‚Ç¨30 | +¬•199 |
| + RasberryPi | +$79 | +‚Ç¨79 | +¬•399 |
| ‚Üë RealSense RGBD head cam | +$220 | +‚Ç¨230 | +¬•1499 |

---

# üì∞ News 

- 2025-08-30: XLeRobot 0.3.0 Release with final outfit touch up and household chores showcase demos. Assembly kit ready for purchase soon, stay tuned!

- 2025-07-30: [Control XLeRobot in real life](https://xlerobot.readthedocs.io/en/latest/software/index.html)  with **keyboard/Xbox controller/Switch joycon** in the wild anywhere. All bluetooth, no wifi needed and zero latency.
- ![rea](https://github.com/user-attachments/assets/de8f50ad-a370-406c-97fb-fc01638d5624)


- 2025-07-08: [**Simulation**](https://xlerobot.readthedocs.io/en/latest/simulation/index.html) with updated urdfs, control scripts (support Quest3 VR, keyboard, Xbox controller, switch joycon), support for new hardware and cameras, RL environment. Get started in 15 min.
-  ![vr](https://github.com/user-attachments/assets/68b77bea-fdcf-4f42-9cf0-efcf1b188358)

- 2025-07-01: [**Documentation** website](https://xlerobot.readthedocs.io/en/latest/index.html) out for more orgainized tutorials, demos and resources.

- 2025-06-13: [**XLeRobot 0.2.0**](https://xlerobot.readthedocs.io) hardware setup, the 1st version fully capable for autonomous household tasks, starts from 660$. 


---
## üöÄ Get Started üöÄ

&gt; [!NOTE] 
&gt; If you are totally new to programming, please spend at least a day to get yourself familiar with basic Python, Ubuntu and Github (with the help of Google and AI). At least you should know how to setup ubuntu system, git clone, pip install, use intepreters (VS Code, Cursor, Pycharm, etc.) and directly run commands in the terminals.

1. üíµ **Buy your parts**: [Bill of Materials](https://xlerobot.readthedocs.io/en/latest/hardware/getting_started/material.html)
2. üñ®Ô∏è **Print your stuff**: [3D printing](https://xlerobot.readthedocs.io/en/latest/hardware/getting_started/3d.html)
3. üî® ~~Avengers~~: [**Assemble**!](https://xlerobot.readthedocs.io/en/latest/hardware/getting_started/assemble.html)
4. üíª **Software**: [Get your robot moving!](https://xlerobot.readthedocs.io/en/latest/software/index.html)

---

## Contribute


**üëã Want to contribute to XLeRobot?**
Please refer to [CONTRIBUTING.md](CONTRIBUTING.md) for guidance on how to get involved!

**Main Contributors**

- [Gaotian/Vector Wang](https://vector-wangel.github.io/)
- [Zhuoyi Lu](https://lzhuoyi.github.io/Zhuoyi_Lu.github.io/): RL sim2real deploy, teleop on real robot (Xbox, VR, Joycon)
- Nicole Yue: Documentation website setup
- Yuesong Wang: Mujoco simulation


This is just a small brick in the pyramid, made possible by¬†[LeRobot](https://github.com/huggingface/lerobot),¬†[SO-100](https://github.com/TheRobotStudio/SO-ARM100),¬†[Lekiwi](https://github.com/SIGRobotics-UIUC/LeKiwi), and¬†[Bambot](https://github.com/timqian/bambot). Thanks to all the talented contributors behind these detailed and professional projects.

Looking forward to collaborating with anyone interested in contributing to this project!

## About me

[Gaotian/Vector Wang](https://vector-wangel.github.io/)

I am a CS graduate student at Rice University [RobotPi Lab](https://robotpilab.github.io/), focusing on robust object manipulation, where we propse virtual cages and funnels and physics-aware world models to close the Sim2real gap and achieve robust manipulation under uncertainties. One of my papers, Caging in Time, has recently been accepted by International Journal of Robotics Research (IJRR).

I built XLeRobot as a personal hobby to instantiate my research theory, also to provide a low-cost platform for people who are interested in robotics and embodied AI to work with. 

[![Star History Chart](https://api.star-history.com/svg?repos=Vector-Wangel/XLeRobot&amp;type=Timeline)](https://star-history.com/#Vector-Wangel/XLeRobot&amp;Timeline)
---

## Citation

If you want, you can cite this work with:

```bibtex
@misc{wang2025xlerobot,
    author = {Wang, Gaotian and Lu, Zhuoyi},
    title = {XLeRobot: A Practical Low-cost Household Dual-Arm Mobile Robot Design for General Manipulation},
    howpublished = &quot;\url{https://github.com/Vector-Wangel/XLeRobot}&quot;,
    year = {2025}
}
```
---![Generated Image August 27, 2025 - 4_58PM](https://github.com/user-attachments/assets/682ef049-bb42-4b50-bf98-74d6311e774d)


## ü™ß Disclaimer ü™ß

&gt; [!NOTE]
&gt; If you build, buy, or develop a XLeRobot based on this repo, you will be fully responsible for all the physical and mental damages it does to you or others.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[megadose/toutatis]]></title>
            <link>https://github.com/megadose/toutatis</link>
            <guid>https://github.com/megadose/toutatis</guid>
            <pubDate>Tue, 09 Sep 2025 00:04:11 GMT</pubDate>
            <description><![CDATA[Toutatis is a tool that allows you to extract information from instagrams accounts such as e-mails, phone numbers and more]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/megadose/toutatis">megadose/toutatis</a></h1>
            <p>Toutatis is a tool that allows you to extract information from instagrams accounts such as e-mails, phone numbers and more</p>
            <p>Language: Python</p>
            <p>Stars: 3,157</p>
            <p>Forks: 657</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre># Toutatis
üëã Hi there! For any professional inquiries or collaborations, please reach out to me at:
megadose@protonmail.com

üìß Preferably, use your professional email for correspondence. Let&#039;s keep it short and sweet, and all in English!

Toutatis is a tool that allows you to extract information from instagrams accounts such as e-mails, phone numbers and more &lt;/br&gt;
For BTC Donations : 1FHDM49QfZX6pJmhjLE5tB2K6CaTLMZpXZ
## üí° Prerequisite
[Python 3](https://www.python.org/downloads/release/python-370/)

## üõ†Ô∏è Installation
### With PyPI

```pip install toutatis```

### With Github

```bash
git clone https://github.com/megadose/toutatis.git
cd toutatis/
python3 setup.py install
```

## üìö Usage:

### Find information from a username

```
toutatis -u username -s instagramsessionid
```

### Find information from an Instagram ID

```
toutatis -i instagramID -s instagramsessionid
```

## üìà Example

```
Informations about     : xxxusernamexxx
Full Name              : xxxusernamesxx | userID : 123456789
Verified               : False | Is buisness Account : False
Is private Account     : False
Follower               : xxx | Following : xxx
Number of posts        : x
Number of tag in posts : x
External url           : http://example.com
IGTV posts             : x
Biography              : example biography
Public Email           : public@example.com
Public Phone           : +00 0 00 00 00 00
Obfuscated email       : me********s@examplemail.com
Obfuscated phone       : +00 0xx xxx xx 00
------------------------
Profile Picture        : https://scontent-X-X.cdninstagram.com/
```

## üìö To retrieve the sessionID
![](https://files.catbox.moe/1rfi6j.png)

## Thank you to :

- [EyupErgin](https://github.com/eyupergin)
- [yazeed44](https://github.com/yazeed44)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pathwaycom/pathway]]></title>
            <link>https://github.com/pathwaycom/pathway</link>
            <guid>https://github.com/pathwaycom/pathway</guid>
            <pubDate>Tue, 09 Sep 2025 00:04:10 GMT</pubDate>
            <description><![CDATA[Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pathwaycom/pathway">pathwaycom/pathway</a></h1>
            <p>Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.</p>
            <p>Language: Python</p>
            <p>Stars: 39,834</p>
            <p>Forks: 1,205</p>
            <p>Stars today: 1,513 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pathway.com/&quot;&gt;
    &lt;img src=&quot;https://pathway.com/logo-light.svg&quot;/&gt;
  &lt;/a&gt;
  &lt;br /&gt;&lt;br /&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/10388&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/10388&quot; alt=&quot;pathwaycom%2Fpathway | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
  &lt;br /&gt;&lt;br /&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml&quot;&gt;
        &lt;img src=&quot;https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml/badge.svg&quot; alt=&quot;ubuntu&quot;/&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/actions/workflows/release.yml&quot;&gt;
        &lt;img src=&quot;https://github.com/pathwaycom/pathway/actions/workflows/release.yml/badge.svg&quot; alt=&quot;Last release&quot;/&gt;&lt;/a&gt;
        &lt;a href=&quot;https://badge.fury.io/py/pathway&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/pathway.svg&quot; alt=&quot;PyPI version&quot; height=&quot;18&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://badge.fury.io/py/pathway&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/pathway&quot; alt=&quot;PyPI downloads&quot; height=&quot;18&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/blob/main/LICENSE.txt&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/license-BSL-green&quot; alt=&quot;License: BSL&quot;/&gt;&lt;/a&gt;
      &lt;br&gt;
        &lt;a href=&quot;https://discord.gg/pathway&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/discord/1042405378304004156?logo=discord&quot;
            alt=&quot;chat on Discord&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://twitter.com/intent/follow?screen_name=pathway_com&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/twitter/follow/pathwaycom&quot;
            alt=&quot;follow on Twitter&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://linkedin.com/company/pathway&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/pathway-0077B5?style=social&amp;logo=linkedin&quot; alt=&quot;follow on LinkedIn&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://github.com/dylanhogg/awesome-python/blob/main/README.md&quot;&gt;
      &lt;img src=&quot;https://awesome.re/badge.svg&quot; alt=&quot;Awesome Python&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://gurubase.io/g/pathway&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/badge/Gurubase-Ask%20Pathway%20Guru-006BFF&quot; alt=&quot;Pathway Guru&quot;&gt;&lt;/a&gt;
    &lt;br&gt;
    &lt;a href=&quot;#getting-started&quot;&gt;Getting Started&lt;/a&gt; |
    &lt;a href=&quot;#deployment&quot;&gt;Deployment&lt;/a&gt; |
    &lt;a href=&quot;#resources&quot;&gt;Documentation and Support&lt;/a&gt; |
    &lt;a href=&quot;https://pathway.com/blog/&quot;&gt;Blog&lt;/a&gt; |
    &lt;a href=&quot;#license&quot;&gt;License&lt;/a&gt;

  
&lt;/p&gt;

# Pathway&lt;a id=&quot;pathway&quot;&gt; Live Data Framework&lt;/a&gt;

[Pathway](https://pathway.com) is a Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.

Pathway comes with an **easy-to-use Python API**, allowing you to seamlessly integrate your favorite Python ML libraries.
Pathway code is versatile and robust: **you can use it in both development and production environments, handling both batch and streaming data effectively**.
The same code can be used for local development, CI/CD tests, running batch jobs, handling stream replays, and processing data streams.

Pathway is powered by a **scalable Rust engine** based on Differential Dataflow and performs incremental computation.
Your Pathway code, despite being written in Python, is run by the Rust engine, enabling multithreading, multiprocessing, and distributed computations.
All the pipeline is kept in memory and can be easily deployed with **Docker and Kubernetes**.

You can install Pathway with pip:
```
pip install -U pathway
```

For any questions, you will find the community and team behind the project [on Discord](https://discord.com/invite/pathway).

## Use-cases and templates

Ready to see what Pathway can do?

[Try one of our easy-to-run examples](https://pathway.com/developers/templates)!

Available in both notebook and docker formats, these ready-to-launch examples can be launched in just a few clicks. Pick one and start your hands-on experience with Pathway today!

### Event processing and real-time analytics pipelines
With its unified engine for batch and streaming and its full Python compatibility, Pathway makes data processing as easy as possible. It&#039;s the ideal solution for a wide range of data processing pipelines, including:

- [Showcase: Real-time ETL.](https://pathway.com/developers/templates/kafka-etl)
- [Showcase: Event-driven pipelines with alerting.](https://pathway.com/developers/templates/realtime-log-monitoring)
- [Showcase: Realtime analytics.](https://pathway.com/developers/templates/linear_regression_with_kafka/)
- [Docs: Switch from batch to streaming.](https://pathway.com/developers/user-guide/connecting-to-data/switch-from-batch-to-streaming)



### AI Pipelines

Pathway provides dedicated LLM tooling to build live LLM and RAG pipelines. Wrappers for most common LLM services and utilities are included, making working with LLMs and RAGs pipelines incredibly easy. Check out our [LLM xpack documentation](https://pathway.com/developers/user-guide/llm-xpack/overview).

Don&#039;t hesitate to try one of our runnable examples featuring LLM tooling.
You can find such examples [here](https://pathway.com/developers/user-guide/llm-xpack/llm-examples).

  - [Template: Unstructured data to SQL on-the-fly.](https://pathway.com/developers/templates/unstructured-to-structured/)
  - [Template: Private RAG with Ollama and Mistral AI](https://pathway.com/developers/templates/private-rag-ollama-mistral)
  - [Template: Adaptive RAG](https://pathway.com/developers/templates/adaptive-rag)
  - [Template: Multimodal RAG with gpt-4o](https://pathway.com/developers/templates/multimodal-rag)

## Features

- **A wide range of connectors**: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector.
- **Stateless and stateful transformations**: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data.
- **Persistence**: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!
- **Consistency**: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the &quot;at least once&quot; consistency while the enterprise version provides the &quot;exactly once&quot; consistency.
- **Scalable Rust engine**: with Pathway Rust engine, you are free from the usual limits imposed by Python. You can easily do multithreading, multiprocessing, and distributed computations.
- **LLM helpers**: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents.


## Getting started&lt;a id=&quot;getting-started&quot;&gt;&lt;/a&gt;

### Installation&lt;a id=&quot;installation&quot;&gt;&lt;/a&gt;

Pathway requires Python 3.10 or above.

You can install the current release of Pathway using `pip`:

```
$ pip install -U pathway
```

‚ö†Ô∏è Pathway is available on MacOS and Linux. Users of other systems should run Pathway on a Virtual Machine.


### Example: computing the sum of positive values in real time.&lt;a id=&quot;example&quot;&gt;&lt;/a&gt;

```python
import pathway as pw

# Define the schema of your data (Optional)
class InputSchema(pw.Schema):
  value: int

# Connect to your data using connectors
input_table = pw.io.csv.read(
  &quot;./input/&quot;,
  schema=InputSchema
)

#Define your operations on the data
filtered_table = input_table.filter(input_table.value&gt;=0)
result_table = filtered_table.reduce(
  sum_value = pw.reducers.sum(filtered_table.value)
)

# Load your results to external systems
pw.io.jsonlines.write(result_table, &quot;output.jsonl&quot;)

# Run the computation
pw.run()
```

Run Pathway [in Google Colab](https://colab.research.google.com/drive/1aBIJ2HCng-YEUOMrr0qtj0NeZMEyRz55?usp=sharing).

You can find more examples [here](https://github.com/pathwaycom/pathway/tree/main/examples).


## Deployment&lt;a id=&quot;deployment&quot;&gt;&lt;/a&gt;

### Locally&lt;a id=&quot;running-pathway-locally&quot;&gt;&lt;/a&gt;

To use Pathway, you only need to import it:

```python
import pathway as pw
```

Now, you can easily create your processing pipeline, and let Pathway handle the updates. Once your pipeline is created, you can launch the computation on streaming data with a one-line command:

```python
pw.run()
```

You can then run your Pathway project (say, `main.py`) just like a normal Python script: `$ python main.py`.
Pathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages. 

&lt;img src=&quot;https://d14l3brkh44201.cloudfront.net/pathway-dashboard.png&quot; width=&quot;1326&quot; alt=&quot;Pathway dashboard&quot;/&gt;

Alternatively, you can use the pathway&#039;ish version:

```
$ pathway spawn python main.py
```

Pathway natively supports multithreading.
To launch your application with 3 threads, you can do as follows:
```
$ pathway spawn --threads 3 python main.py
```

To jumpstart a Pathway project, you can use our [cookiecutter template](https://github.com/pathwaycom/cookiecutter-pathway).


### Docker&lt;a id=&quot;docker&quot;&gt;&lt;/a&gt;

You can easily run Pathway using docker.

#### Pathway image

You can use the [Pathway docker image](https://hub.docker.com/r/pathwaycom/pathway), using a Dockerfile:

```dockerfile
FROM pathwaycom/pathway:latest

WORKDIR /app

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD [ &quot;python&quot;, &quot;./your-script.py&quot; ]
```

You can then build and run the Docker image:

```console
docker build -t my-pathway-app .
docker run -it --rm --name my-pathway-app my-pathway-app
```

#### Run a single Python script

When dealing with single-file projects, creating a full-fledged `Dockerfile`
might seem unnecessary. In such scenarios, you can execute a
Python script directly using the Pathway Docker image. For example:

```console
docker run -it --rm --name my-pathway-app -v &quot;$PWD&quot;:/app pathwaycom/pathway:latest python my-pathway-app.py
```

#### Python docker image

You can also use a standard Python image and install Pathway using pip with a Dockerfile:

```dockerfile
FROM --platform=linux/x86_64 python:3.10

RUN pip install -U pathway
COPY ./pathway-script.py pathway-script.py

CMD [&quot;python&quot;, &quot;-u&quot;, &quot;pathway-script.py&quot;]
```

### Kubernetes and cloud&lt;a id=&quot;k8s&quot;&gt;&lt;/a&gt;

Docker containers are ideally suited for deployment on the cloud with Kubernetes.
If you want to scale your Pathway application, you may be interested in our Pathway for Enterprise.
Pathway for Enterprise is specially tailored towards end-to-end data processing and real time intelligent analytics.
It scales using distributed computing on the cloud and supports distributed Kubernetes deployment, with external persistence setup.

You can easily deploy Pathway using services like Render: see [how to deploy Pathway in a few clicks](https://pathway.com/developers/user-guide/deployment/render-deploy/).

If you are interested, don&#039;t hesitate to [contact us](mailto:contact@pathway.com) to learn more.

## Performance&lt;a id=&quot;performance&quot;&gt;&lt;/a&gt;

Pathway is made to outperform state-of-the-art technologies designed for streaming and batch data processing tasks, including: Flink, Spark, and Kafka Streaming. It also makes it possible to implement a lot of algorithms/UDF&#039;s in streaming mode which are not readily supported by other streaming frameworks (especially: temporal joins, iterative graph algorithms, machine learning routines).

If you are curious, here are [some benchmarks to play with](https://github.com/pathwaycom/pathway-benchmarks).

&lt;img src=&quot;https://github.com/pathwaycom/pathway-benchmarks/raw/main/images/bm-wordcount-lineplot.png&quot; width=&quot;1326&quot; alt=&quot;WordCount Graph&quot;/&gt;

## Documentation and Support&lt;a id=&quot;resources&quot;&gt;&lt;/a&gt;

The entire documentation of Pathway is available at [pathway.com/developers/](https://pathway.com/developers/user-guide/introduction/welcome), including the [API Docs](https://pathway.com/developers/api-docs/pathway).

If you have any question, don&#039;t hesitate to [open an issue on GitHub](https://github.com/pathwaycom/pathway/issues), join us on [Discord](https://discord.com/invite/pathway), or send us an email at [contact@pathway.com](mailto:contact@pathway.com).

## License&lt;a id=&quot;license&quot;&gt;&lt;/a&gt;

Pathway is distributed on a [BSL 1.1 License](https://github.com/pathwaycom/pathway/blob/main/LICENSE.txt) which allows for unlimited non-commercial use, as well as use of the Pathway package [for most commercial purposes](https://pathway.com/license/), free of charge. Code in this repository automatically converts to Open Source (Apache 2.0 License) after 4 years. Some [public repos](https://github.com/pathwaycom) which are complementary to this one (examples, libraries, connectors, etc.) are licensed as Open Source, under the MIT license.


## Contribution guidelines&lt;a id=&quot;contribution-guidelines&quot;&gt;&lt;/a&gt;

If you develop a library or connector which you would like to integrate with this repo, we suggest releasing it first as a separate repo on a MIT/Apache 2.0 license. 

For all concerns regarding core Pathway functionalities, Issues are encouraged. For further information, don&#039;t hesitate to engage with Pathway&#039;s [Discord community](https://discord.gg/pathway).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[coderamp-labs/gitingest]]></title>
            <link>https://github.com/coderamp-labs/gitingest</link>
            <guid>https://github.com/coderamp-labs/gitingest</guid>
            <pubDate>Tue, 09 Sep 2025 00:04:09 GMT</pubDate>
            <description><![CDATA[Replace 'hub' with 'ingest' in any GitHub URL to get a prompt-friendly extract of a codebase]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/coderamp-labs/gitingest">coderamp-labs/gitingest</a></h1>
            <p>Replace 'hub' with 'ingest' in any GitHub URL to get a prompt-friendly extract of a codebase</p>
            <p>Language: Python</p>
            <p>Stars: 12,294</p>
            <p>Forks: 908</p>
            <p>Stars today: 141 stars today</p>
            <h2>README</h2><pre># Gitingest

[![Screenshot of Gitingest front page](https://raw.githubusercontent.com/coderamp-labs/gitingest/refs/heads/main/docs/frontpage.png)](https://gitingest.com)

&lt;!-- Badges --&gt;
&lt;!-- markdownlint-disable MD033 --&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;!-- row 1 ‚Äî install &amp; compat --&gt;
  &lt;a href=&quot;https://pypi.org/project/gitingest&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/gitingest.svg&quot; alt=&quot;PyPI&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/gitingest&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/pyversions/gitingest.svg&quot; alt=&quot;Python Versions&quot;&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;!-- row 2 ‚Äî quality &amp; community --&gt;
  &lt;a href=&quot;https://github.com/coderamp-labs/gitingest/actions/workflows/ci.yml?query=branch%3Amain&quot;&gt;&lt;img src=&quot;https://github.com/coderamp-labs/gitingest/actions/workflows/ci.yml/badge.svg?branch=main&quot; alt=&quot;CI&quot;&gt;&lt;/a&gt;

  &lt;a href=&quot;https://github.com/astral-sh/ruff&quot;&gt;&lt;img src=&quot;https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json&quot; alt=&quot;Ruff&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://scorecard.dev/viewer/?uri=github.com/coderamp-labs/gitingest&quot;&gt;&lt;img src=&quot;https://api.scorecard.dev/projects/github.com/coderamp-labs/gitingest/badge&quot; alt=&quot;OpenSSF Scorecard&quot;&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://github.com/coderamp-labs/gitingest/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/coderamp-labs/gitingest.svg&quot; alt=&quot;License&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pepy.tech/project/gitingest&quot;&gt;&lt;img src=&quot;https://pepy.tech/badge/gitingest&quot; alt=&quot;Downloads&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/coderamp-labs/gitingest&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/coderamp-labs/gitingest&quot; alt=&quot;GitHub Stars&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.com/invite/zerRaGK9EC&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join_chat-5865F2?logo=discord&amp;logoColor=white&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/13519&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13519&quot; alt=&quot;Trendshift&quot; height=&quot;50&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;!-- markdownlint-enable MD033 --&gt;

Turn any Git repository into a prompt-friendly text ingest for LLMs.

You can also replace `hub` with `ingest` in any GitHub URL to access the corresponding digest.

&lt;!-- Extensions --&gt;
[gitingest.com](https://gitingest.com) ¬∑ [Chrome Extension](https://chromewebstore.google.com/detail/adfjahbijlkjfoicpjkhjicpjpjfaood) ¬∑ [Firefox Add-on](https://addons.mozilla.org/firefox/addon/gitingest)

&lt;!-- Languages --&gt;
[Deutsch](https://www.readme-i18n.com/coderamp-labs/gitingest?lang=de) |
[Espa√±ol](https://www.readme-i18n.com/coderamp-labs/gitingest?lang=es) |
[Fran√ßais](https://www.readme-i18n.com/coderamp-labs/gitingest?lang=fr) |
[Êó•Êú¨Ë™û](https://www.readme-i18n.com/coderamp-labs/gitingest?lang=ja) |
[ÌïúÍµ≠Ïñ¥](https://www.readme-i18n.com/coderamp-labs/gitingest?lang=ko) |
[Portugu√™s](https://www.readme-i18n.com/coderamp-labs/gitingest?lang=pt) |
[–†—É—Å—Å–∫–∏–π](https://www.readme-i18n.com/coderamp-labs/gitingest?lang=ru) |
[‰∏≠Êñá](https://www.readme-i18n.com/coderamp-labs/gitingest?lang=zh)

## üöÄ Features

- **Easy code context**: Get a text digest from a Git repository URL or a directory
- **Smart Formatting**: Optimized output format for LLM prompts
- **Statistics about**:
  - File and directory structure
  - Size of the extract
  - Token count
- **CLI tool**: Run it as a shell command
- **Python package**: Import it in your code

## üìö Requirements

- Python 3.8+
- For private repositories: A GitHub Personal Access Token (PAT). [Generate your token **here**!](https://github.com/settings/tokens/new?description=gitingest&amp;scopes=repo)

### üì¶ Installation

Gitingest is available on [PyPI](https://pypi.org/project/gitingest/).
You can install it using `pip`:

```bash
pip install gitingest
```

or

```bash
pip install gitingest[server]
```

to include server dependencies for self-hosting.

However, it might be a good idea to use `pipx` to install it.
You can install `pipx` using your preferred package manager.

```bash
brew install pipx
apt install pipx
scoop install pipx
...
```

If you are using pipx for the first time, run:

```bash
pipx ensurepath
```

```bash
# install gitingest
pipx install gitingest
```

## üß© Browser Extension Usage

&lt;!-- markdownlint-disable MD033 --&gt;
&lt;a href=&quot;https://chromewebstore.google.com/detail/adfjahbijlkjfoicpjkhjicpjpjfaood&quot; target=&quot;_blank&quot; title=&quot;Get Gitingest Extension from Chrome Web Store&quot;&gt;&lt;img height=&quot;48&quot; src=&quot;https://github.com/user-attachments/assets/20a6e44b-fd46-4e6c-8ea6-aad436035753&quot; alt=&quot;Available in the Chrome Web Store&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://addons.mozilla.org/firefox/addon/gitingest&quot; target=&quot;_blank&quot; title=&quot;Get Gitingest Extension from Firefox Add-ons&quot;&gt;&lt;img height=&quot;48&quot; src=&quot;https://github.com/user-attachments/assets/c0e99e6b-97cf-4af2-9737-099db7d3538b&quot; alt=&quot;Get The Add-on for Firefox&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://microsoftedge.microsoft.com/addons/detail/nfobhllgcekbmpifkjlopfdfdmljmipf&quot; target=&quot;_blank&quot; title=&quot;Get Gitingest Extension from Microsoft Edge Add-ons&quot;&gt;&lt;img height=&quot;48&quot; src=&quot;https://github.com/user-attachments/assets/204157eb-4cae-4c0e-b2cb-db514419fd9e&quot; alt=&quot;Get from the Edge Add-ons&quot; /&gt;&lt;/a&gt;
&lt;!-- markdownlint-enable MD033 --&gt;

The extension is open source at [lcandy2/gitingest-extension](https://github.com/lcandy2/gitingest-extension).

Issues and feature requests are welcome to the repo.

## üí° Command line usage

The `gitingest` command line tool allows you to analyze codebases and create a text dump of their contents.

```bash
# Basic usage (writes to digest.txt by default)
gitingest /path/to/directory

# From URL
gitingest https://github.com/coderamp-labs/gitingest

# or from specific subdirectory
gitingest https://github.com/coderamp-labs/gitingest/tree/main/src/gitingest/utils
```

For private repositories, use the `--token/-t` option.

```bash
# Get your token from https://github.com/settings/personal-access-tokens
gitingest https://github.com/username/private-repo --token github_pat_...

# Or set it as an environment variable
export GITHUB_TOKEN=github_pat_...
gitingest https://github.com/username/private-repo

# Include repository submodules
gitingest https://github.com/username/repo-with-submodules --include-submodules
```

By default, files listed in `.gitignore` are skipped. Use `--include-gitignored` if you
need those files in the digest.

By default, the digest is written to a text file (`digest.txt`) in your current working directory. You can customize the output in two ways:

- Use `--output/-o &lt;filename&gt;` to write to a specific file.
- Use `--output/-o -` to output directly to `STDOUT` (useful for piping to other tools).

See more options and usage details with:

```bash
gitingest --help
```

## üêç Python package usage

```python
# Synchronous usage
from gitingest import ingest

summary, tree, content = ingest(&quot;path/to/directory&quot;)

# or from URL
summary, tree, content = ingest(&quot;https://github.com/coderamp-labs/gitingest&quot;)

# or from a specific subdirectory
summary, tree, content = ingest(&quot;https://github.com/coderamp-labs/gitingest/tree/main/src/gitingest/utils&quot;)
```

For private repositories, you can pass a token:

```python
# Using token parameter
summary, tree, content = ingest(&quot;https://github.com/username/private-repo&quot;, token=&quot;github_pat_...&quot;)

# Or set it as an environment variable
import os
os.environ[&quot;GITHUB_TOKEN&quot;] = &quot;github_pat_...&quot;
summary, tree, content = ingest(&quot;https://github.com/username/private-repo&quot;)

# Include repository submodules
summary, tree, content = ingest(&quot;https://github.com/username/repo-with-submodules&quot;, include_submodules=True)
```

By default, this won&#039;t write a file but can be enabled with the `output` argument.

```python
# Asynchronous usage
from gitingest import ingest_async
import asyncio

result = asyncio.run(ingest_async(&quot;path/to/directory&quot;))
```

### Jupyter notebook usage

```python
from gitingest import ingest_async

# Use await directly in Jupyter
summary, tree, content = await ingest_async(&quot;path/to/directory&quot;)

```

This is because Jupyter notebooks are asynchronous by default.

## üê≥ Self-host

### Using Docker

1. Build the image:

   ``` bash
   docker build -t gitingest .
   ```

2. Run the container:

   ``` bash
   docker run -d --name gitingest -p 8000:8000 gitingest
   ```

The application will be available at `http://localhost:8000`.

If you are hosting it on a domain, you can specify the allowed hostnames via env variable `ALLOWED_HOSTS`.

   ```bash
   # Default: &quot;gitingest.com, *.gitingest.com, localhost, 127.0.0.1&quot;.
   ALLOWED_HOSTS=&quot;example.com, localhost, 127.0.0.1&quot;
   ```

### Environment Variables

The application can be configured using the following environment variables:

- **ALLOWED_HOSTS**: Comma-separated list of allowed hostnames (default: &quot;gitingest.com, *.gitingest.com, localhost, 127.0.0.1&quot;)
- **GITINGEST_METRICS_ENABLED**: Enable Prometheus metrics server (set to any value to enable)
- **GITINGEST_METRICS_HOST**: Host for the metrics server (default: &quot;127.0.0.1&quot;)
- **GITINGEST_METRICS_PORT**: Port for the metrics server (default: &quot;9090&quot;)
- **GITINGEST_SENTRY_ENABLED**: Enable Sentry error tracking (set to any value to enable)
- **GITINGEST_SENTRY_DSN**: Sentry DSN (required if Sentry is enabled)
- **GITINGEST_SENTRY_TRACES_SAMPLE_RATE**: Sampling rate for performance data (default: &quot;1.0&quot;, range: 0.0-1.0)
- **GITINGEST_SENTRY_PROFILE_SESSION_SAMPLE_RATE**: Sampling rate for profile sessions (default: &quot;1.0&quot;, range: 0.0-1.0)
- **GITINGEST_SENTRY_PROFILE_LIFECYCLE**: Profile lifecycle mode (default: &quot;trace&quot;)
- **GITINGEST_SENTRY_SEND_DEFAULT_PII**: Send default personally identifiable information (default: &quot;true&quot;)
- **S3_ALIAS_HOST**: Public URL/CDN for accessing S3 resources (default: &quot;127.0.0.1:9000/gitingest-bucket&quot;)
- **S3_DIRECTORY_PREFIX**: Optional prefix for S3 file paths (if set, prefixes all S3 paths with this value)

### Using Docker Compose

The project includes a `compose.yml` file that allows you to easily run the application in both development and production environments.

#### Compose File Structure

The `compose.yml` file uses YAML anchoring with `&amp;app-base` and `&lt;&lt;: *app-base` to define common configuration that is shared between services:

```yaml
# Common base configuration for all services
x-app-base: &amp;app-base
  build:
    context: .
    dockerfile: Dockerfile
  ports:
    - &quot;${APP_WEB_BIND:-8000}:8000&quot;  # Main application port
    - &quot;${GITINGEST_METRICS_HOST:-127.0.0.1}:${GITINGEST_METRICS_PORT:-9090}:9090&quot;  # Metrics port
  # ... other common configurations
```

#### Services

The file defines three services:

1. **app**: Production service configuration
   - Uses the `prod` profile
   - Sets the Sentry environment to &quot;production&quot;
   - Configured for stable operation with `restart: unless-stopped`

2. **app-dev**: Development service configuration
   - Uses the `dev` profile
   - Enables debug mode
   - Mounts the source code for live development
   - Uses hot reloading for faster development

3. **minio**: S3-compatible object storage for development
   - Uses the `dev` profile (only available in development mode)
   - Provides S3-compatible storage for local development
   - Accessible via:
     - API: Port 9000 ([localhost:9000](http://localhost:9000))
     - Web Console: Port 9001 ([localhost:9001](http://localhost:9001))
   - Default admin credentials:
     - Username: `minioadmin`
     - Password: `minioadmin`
   - Configurable via environment variables:
     - `MINIO_ROOT_USER`: Custom admin username (default: minioadmin)
     - `MINIO_ROOT_PASSWORD`: Custom admin password (default: minioadmin)
   - Includes persistent storage via Docker volume
   - Auto-creates a bucket and application-specific credentials:
     - Bucket name: `gitingest-bucket` (configurable via `S3_BUCKET_NAME`)
     - Access key: `gitingest` (configurable via `S3_ACCESS_KEY`)
     - Secret key: `gitingest123` (configurable via `S3_SECRET_KEY`)
   - These credentials are automatically passed to the app-dev service via environment variables:
     - `S3_ENDPOINT`: URL of the MinIO server
     - `S3_ACCESS_KEY`: Access key for the S3 bucket
     - `S3_SECRET_KEY`: Secret key for the S3 bucket
     - `S3_BUCKET_NAME`: Name of the S3 bucket
     - `S3_REGION`: Region for the S3 bucket (default: us-east-1)
     - `S3_ALIAS_HOST`: Public URL/CDN for accessing S3 resources (default: &quot;127.0.0.1:9000/gitingest-bucket&quot;)

#### Usage Examples

To run the application in development mode:

```bash
docker compose --profile dev up
```

To run the application in production mode:

```bash
docker compose --profile prod up -d
```

To build and run the application:

```bash
docker compose --profile prod build
docker compose --profile prod up -d
```

## ü§ù Contributing

### Non-technical ways to contribute

- **Create an Issue**: If you find a bug or have an idea for a new feature, please [create an issue](https://github.com/coderamp-labs/gitingest/issues/new) on GitHub. This will help us track and prioritize your request.
- **Spread the Word**: If you like Gitingest, please share it with your friends, colleagues, and on social media. This will help us grow the community and make Gitingest even better.
- **Use Gitingest**: The best feedback comes from real-world usage! If you encounter any issues or have ideas for improvement, please let us know by [creating an issue](https://github.com/coderamp-labs/gitingest/issues/new) on GitHub or by reaching out to us on [Discord](https://discord.com/invite/zerRaGK9EC).

### Technical ways to contribute

Gitingest aims to be friendly for first time contributors, with a simple Python and HTML codebase. If you need any help while working with the code, reach out to us on [Discord](https://discord.com/invite/zerRaGK9EC). For detailed instructions on how to make a pull request, see [CONTRIBUTING.md](./CONTRIBUTING.md).

## üõ†Ô∏è Stack

- [Tailwind CSS](https://tailwindcss.com) - Frontend
- [FastAPI](https://github.com/fastapi/fastapi) - Backend framework
- [Jinja2](https://jinja.palletsprojects.com) - HTML templating
- [tiktoken](https://github.com/openai/tiktoken) - Token estimation
- [posthog](https://github.com/PostHog/posthog) - Amazing analytics
- [Sentry](https://sentry.io) - Error tracking and performance monitoring

### Looking for a JavaScript/FileSystemNode package?

Check out the NPM alternative üì¶ Repomix: &lt;https://github.com/yamadashy/repomix&gt;

## üöÄ Project Growth

[![Star History Chart](https://api.star-history.com/svg?repos=coderamp-labs/gitingest&amp;type=Date)](https://star-history.com/#coderamp-labs/gitingest&amp;Date)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mealie-recipes/mealie]]></title>
            <link>https://github.com/mealie-recipes/mealie</link>
            <guid>https://github.com/mealie-recipes/mealie</guid>
            <pubDate>Tue, 09 Sep 2025 00:04:08 GMT</pubDate>
            <description><![CDATA[Mealie is a self hosted recipe manager and meal planner with a RestAPI backend and a reactive frontend application built in Vue for a pleasant user experience for the whole family. Easily add recipes into your database by providing the url and mealie will automatically import the relevant data or add a family recipe with the UI editor]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mealie-recipes/mealie">mealie-recipes/mealie</a></h1>
            <p>Mealie is a self hosted recipe manager and meal planner with a RestAPI backend and a reactive frontend application built in Vue for a pleasant user experience for the whole family. Easily add recipes into your database by providing the url and mealie will automatically import the relevant data or add a family recipe with the UI editor</p>
            <p>Language: Python</p>
            <p>Stars: 10,010</p>
            <p>Forks: 973</p>
            <p>Stars today: 17 stars today</p>
            <h2>README</h2><pre>[![Latest Release][latest-release-shield]][latest-release-url]
[![Contributors][contributors-shield]][contributors-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]
[![AGPL License][license-shield]][license-url]
[![Docker Pulls][docker-pull]][docker-url]
[![GHCR Pulls][ghcr-pulls]][ghcr-url]

&lt;!-- PROJECT LOGO --&gt;
&lt;br /&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/mealie-recipes/mealie&quot;&gt;
&lt;svg style=&quot;width:100px;height:100px&quot; viewBox=&quot;0 0 24 24&quot;&gt;
    &lt;path fill=&quot;currentColor&quot; d=&quot;M8.1,13.34L3.91,9.16C2.35,7.59 2.35,5.06 3.91,3.5L10.93,10.5L8.1,13.34M13.41,13L20.29,19.88L18.88,21.29L12,14.41L5.12,21.29L3.71,19.88L13.36,10.22L13.16,10C12.38,9.23 12.38,7.97 13.16,7.19L17.5,2.82L18.43,3.74L15.19,7L16.15,7.94L19.39,4.69L20.31,5.61L17.06,8.85L18,9.81L21.26,6.56L22.18,7.5L17.81,11.84C17.03,12.62 15.77,12.62 15,11.84L14.78,11.64L13.41,13Z&quot; /&gt;
&lt;/svg&gt;
  &lt;/a&gt;

  &lt;h3 align=&quot;center&quot;&gt;Mealie&lt;/h3&gt;

  &lt;p align=&quot;center&quot;&gt;
    A Place For All Your Recipes
    &lt;br /&gt;
    &lt;a href=&quot;https://docs.mealie.io/&quot;&gt;&lt;strong&gt;Explore the docs ¬ª&lt;/strong&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/mealie-recipes/mealie&quot;&gt;
  &lt;/a&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://demo.mealie.io/&quot;&gt;View Demo&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://github.com/mealie-recipes/mealie/issues&quot;&gt;Report Bug&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://github.com/mealie-recipes/mealie/pkgs/container/mealie&quot;&gt;GitHub Container Registry&lt;/a&gt;
&lt;/p&gt;




[![Product Name Screen Shot][product-screenshot]](https://docs.mealie.io)

# About The Project

Mealie is a self hosted recipe manager, meal planner and shopping list with a RestAPI backend and a reactive frontend built in Vue for a pleasant user experience for the whole family. Easily add recipes into your database by providing the URL and Mealie will automatically import the relevant data, or add a family recipe with the UI editor. Mealie also provides an API for interactions from 3rd party applications.

- [Remember to join the Discord](https://discord.gg/QuStdQGSGK)!
- [Documentation](https://docs.mealie.io/)


## Key Features
- Recipe imports: Create recipes, by **importing from a URL** or entering data manually
- Meal Planner: Use the **Meal Planner** to plan your what you&#039;ll cook for the next week
- Shopping List: Put the necessary ingredients on your **Shopping List**, organised into sections of your local supermarket
- Cookbooks: Group recipes into **Cookbooks** based on your own criteria
- Docker: Easy **Docker** deployment
- Localisation: **Translations** for 35+ languages

&lt;!-- CONTRIBUTING --&gt;
## Contributing

Contributions are what make the open source community such an amazing place to learn, inspire, and create. Any contributions you make are **greatly appreciated**. If you&#039;re going to be working on the code-base, you&#039;ll want to use the nightly documentation to ensure you get the latest information.

- See the [Contributors Guide](https://nightly.mealie.io/contributors/developers-guide/code-contributions/) for help getting started.
- We use [VSCode Dev Containers](https://code.visualstudio.com/docs/remote/containers) to make it easy for contributors to get started!

If you are not a coder, you can still contribute financially. Financial contributions help me prioritize working on this project over others and helps me know that there is a real demand for project development.

&lt;a href=&quot;https://www.buymeacoffee.com/haykot&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://cdn.buymeacoffee.com/buttons/v2/default-green.png&quot; alt=&quot;Buy Me A Coffee&quot; style=&quot;height: 30px !important;width: 107px !important;&quot; &gt;&lt;/a&gt;

### Translations

Translations can be a great way for **non-coders** to contribute to the project. We use [Crowdin](https://crowdin.com/project/mealie) to allow several contributors to work on translating Mealie. You can simply help by voting for your preferred translations, or even by completely translating Mealie into a new language.

For more information, check out the translation page on the [contributor&#039;s guide](https://nightly.mealie.io/contributors/translating/).

&lt;!-- LICENSE --&gt;
## License
Distributed under the AGPL License. See `LICENSE` for more information.


## Sponsors

Huge thanks to all the sponsors of this project on [Github Sponsors](https://github.com/sponsors/hay-kot) and Buy Me a Coffee. Without you, this project would surely not be possible.

Thanks to Depot for providing build instances for our Docker image builds.

[![Built with Depot](https://depot.dev/badges/built-with-depot.svg)](https://depot.dev?utm_source=Mealie)



&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt;
&lt;!-- https://www.markdownguide.org/basic-syntax/#reference-style-links --&gt;
[contributors-shield]: https://img.shields.io/github/contributors/mealie-recipes/mealie.svg?style=flat-square
[docker-pull]: https://img.shields.io/docker/pulls/hkotel/mealie?style=flat-square
[docker-url]: https://hub.docker.com/r/hkotel/mealie
[ghcr-pulls]: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fipitio.github.io%2Fbackage%2Fmealie-recipes%2Fmealie%2Fmealie.json&amp;query=%24.downloads&amp;style=flat-square&amp;label=ghcr%20pulls
[ghcr-url]: https://github.com/mealie-recipes/mealie/pkgs/container/mealie
[contributors-url]: https://github.com/mealie-recipes/mealie/graphs/contributors
[stars-shield]: https://img.shields.io/github/stars/mealie-recipes/mealie.svg?style=flat-square
[stars-url]: https://github.com/mealie-recipes/mealie/stargazers
[issues-shield]: https://img.shields.io/github/issues/mealie-recipes/mealie.svg?style=flat-square
[issues-url]: https://github.com/mealie-recipes/mealie/issues
[latest-release-shield]: https://img.shields.io/github/v/release/mealie-recipes/mealie?style=flat-square&amp;label=latest%20release
[latest-release-url]: https://github.com/mealie-recipes/mealie/releases
[license-shield]: https://img.shields.io/github/license/mealie-recipes/mealie.svg?style=flat-square
[license-url]: https://github.com/mealie-recipes/mealie/blob/mealie-next/LICENSE
[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=flat-square&amp;logo=linkedin&amp;colorB=555
[linkedin-url]: https://linkedin.com/in/hay-kot
[product-screenshot]: docs/docs/assets/img/home_screenshot.png
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ungoogled-software/ungoogled-chromium]]></title>
            <link>https://github.com/ungoogled-software/ungoogled-chromium</link>
            <guid>https://github.com/ungoogled-software/ungoogled-chromium</guid>
            <pubDate>Tue, 09 Sep 2025 00:04:07 GMT</pubDate>
            <description><![CDATA[Google Chromium, sans integration with Google]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ungoogled-software/ungoogled-chromium">ungoogled-software/ungoogled-chromium</a></h1>
            <p>Google Chromium, sans integration with Google</p>
            <p>Language: Python</p>
            <p>Stars: 23,903</p>
            <p>Forks: 984</p>
            <p>Stars today: 29 stars today</p>
            <h2>README</h2><pre># ungoogled-chromium

*A lightweight approach to removing Google web service dependency*

**Help is welcome!** See the [docs/contributing.md](docs/contributing.md) document for more information.

## Objectives

In descending order of significance (i.e. most important objective first):

1. **ungoogled-chromium is Google Chromium, sans dependency on Google web services**.
2. **ungoogled-chromium retains the default Chromium experience as closely as possible**. Unlike other Chromium forks that have their own visions of a web browser, ungoogled-chromium is essentially a drop-in replacement for Chromium.
3. **ungoogled-chromium features tweaks to enhance privacy, control, and transparency**. However, almost all of these features must be manually activated or enabled. For more details, see [Feature Overview](#feature-overview).

In scenarios where the objectives conflict, the objective of higher significance should take precedence.

## Content Overview

* [Objectives](#objectives)
* [Motivation and Philosophy](#motivation-and-philosophy)
* [Feature Overview](#feature-overview)
* [**Downloads**](#downloads)
* [Source Code](#source-code)
* [**FAQ**](#faq)
* [Building Instructions](#building-instructions)
* [Design Documentation](#design-documentation)
* [**Contributing, Reporting, Contacting**](#contributing-reporting-contacting)
* [Credits](#credits)
* [Related Projects](#related-projects)
* [License](#license)

## Motivation and Philosophy

Without signing in to a Google Account, Chromium does pretty well in terms of security and privacy. However, Chromium still has some dependency on Google web services and binaries. In addition, Google designed Chromium to be easy and intuitive for users, which means they compromise on transparency and control of internal operations.

ungoogled-chromium addresses these issues in the following ways:

1. Remove all remaining background requests to any web services while building and running the browser
2. Remove all code specific to Google web services
3. Remove all uses of pre-made binaries from the source code, and replace them with user-provided alternatives when possible.
4. Disable features that inhibit control and transparency, and add or modify features that promote them (these changes will almost always require manual activation or enabling).

These features are implemented as configuration flags, patches, and custom scripts. For more details, consult the [Design Documentation](docs/design.md).

## Feature Overview

*This section overviews the features of ungoogled-chromium. For more detailed information, it is best to consult the source code.*

Contents of this section:

* [Key Features](#key-features)
* [Enhancing Features](#enhancing-features)
* [Borrowed Features](#borrowed-features)
* [Supported Platforms and Distributions](#supported-platforms-and-distributions)

### Key Features

*These are the core features introduced by ungoogled-chromium.*

* Disable functionality specific to Google domains (e.g. Google Host Detector, Google URL Tracker, Google Cloud Messaging, Google Hotwording, etc.)
    * This includes disabling [Safe Browsing](https://en.wikipedia.org/wiki/Google_Safe_Browsing). Consult [the FAQ for the rationale](https://ungoogled-software.github.io/ungoogled-chromium-wiki/faq#why-is-safe-browsing-disabled).
* Block internal requests to Google at runtime. This feature is a fail-safe measure for the above, in case Google changes or introduces new components that our patches do not disable. This feature is implemented by replacing many Google web domains in the source code with non-existent alternatives ending in `qjz9zk` (known as domain substitution; [see docs/design.md](docs/design.md#source-file-processors) for details), then [modifying Chromium to block its own requests with such domains](patches/core/ungoogled-chromium/block-trk-and-subdomains.patch). In other words, no connections are attempted to the `qjz9zk` domain.
* Strip binaries from the source code (known as binary pruning; [see docs/design.md](docs/design.md#source-file-processors) for details)

### Enhancing Features

*These are the non-essential features introduced by ungoogled-chromium.*

* Add many new command-line switches and `chrome://flags` entries to configure new features (which are disabled by default). See [docs/flags.md](docs/flags.md) for the exhaustive list.
* Add *Suggestions URL* text field in the search engine editor (`chrome://settings/searchEngines`) for customizing search engine suggestions.
* Add more URL schemes allowed to save page schemes.
* Add Omnibox search provider &quot;No Search&quot; to allow disabling of searching
* Add a custom cross-platform build configuration and packaging wrapper for Chromium. It currently supports many Linux distributions, macOS, and Windows. (See [docs/design.md](docs/design.md) for details on the system.)
* Force all pop-ups into tabs
* Disable automatic formatting of URLs in Omnibox (e.g. stripping `http://`, hiding certain parameters)
* Disable intranet redirect detector (extraneous DNS requests)
    * This breaks captive portal detection, but captive portals still work.
* (Iridium Browser feature change) Prevent URLs with the `trk:` scheme from connecting to the Internet
    * Also prevents any URLs with the top-level domain `qjz9zk` (as used in domain substitution) from attempting a connection.
* (Windows-specific) Do not set the Zone Identifier on downloaded files

### Borrowed Features

In addition to the features introduced by ungoogled-chromium, ungoogled-chromium selectively borrows many features from the following projects (in approximate order of significance):

* [Inox patchset](https://github.com/gcarq/inox-patchset)
* [Bromite](https://github.com/bromite/bromite)
* [Debian](https://tracker.debian.org/pkg/chromium)
* [Iridium Browser](https://iridiumbrowser.de/)

### Supported Platforms and Distributions

[See docs/platforms.md for a list of supported platforms](docs/platforms.md).

Other platforms are discussed and tracked in this repository&#039;s Issue Tracker. Learn more about using the Issue Tracker under the section [Contributing, Reporting, Contacting](#contributing-reporting-contacting).

## Downloads

### Automated or maintained builds

ungoogled-chromium is available in the following **software repositories**:

* Arch: Available in the AUR, [see instructions in ungoogled-chromium-archlinux](https://github.com/ungoogled-software/ungoogled-chromium-archlinux)
* Debian &amp; Ubuntu: Available in OBS, find your [distribution specific instructions](https://github.com/ungoogled-software/ungoogled-chromium-debian) in the Installing section
* Fedora: Available in [COPR](https://copr.fedorainfracloud.org/coprs/) as [`wojnilowicz/ungoogled-chromium`](https://copr.fedorainfracloud.org/coprs/wojnilowicz/ungoogled-chromium/). Also available in [RPM Fusion](https://rpmfusion.org/Configuration) as `chromium-browser-privacy` (outdated).
* Gentoo: Available in [`::pf4public`](https://github.com/PF4Public/gentoo-overlay) overlay as [`ungoogled-chromium`](https://github.com/PF4Public/gentoo-overlay/tree/master/www-client/ungoogled-chromium) and [`ungoogled-chromium-bin`](https://github.com/PF4Public/gentoo-overlay/tree/master/www-client/ungoogled-chromium-bin) ebuilds
* [OpenMandriva](https://openmandriva.org/) includes ungoogled-chromium as its main browser. The `chromium` package includes all ungoogling patches.
* macOS: Available in [Homebrew](https://brew.sh/) as [`ungoogled-chromium`](https://formulae.brew.sh/cask/ungoogled-chromium). Just run `brew install --cask ungoogled-chromium`. Chromium will appear in your `/Applications` directory.
* FreeBSD: Available in pkg as [`www/ungoogled-chromium`](https://www.freshports.org/www/ungoogled-chromium/).

If your GNU/Linux distribution is not listed, there are distro-independent builds available via the following **package managers**:

* Flatpak: Available [in the Flathub repo](https://flathub.org/apps/details/io.github.ungoogled_software.ungoogled_chromium) as `io.github.ungoogled_software.ungoogled_chromium`
* GNU Guix: Available as `ungoogled-chromium`
* NixOS/nixpkgs: Available as `ungoogled-chromium`

### Third-party binaries

If your operating system is not listed above, you can also try to [**Download binaries from here**](https://ungoogled-software.github.io/ungoogled-chromium-binaries/)

*NOTE: These binaries are provided by anyone who are willing to build and submit them. Because these binaries are not necessarily [reproducible](https://reproducible-builds.org/), authenticity cannot be guaranteed; In other words, there is always a non-zero probability that these binaries may have been tampered with. In the unlikely event that this has happened to you, please [report it in a new issue](#contributing-reporting-contacting).*

These binaries are known as **contributor binaries**.

## Source Code

This repository only contains the common code for all platforms; it does not contain all the configuration and scripts necessary to build ungoogled-chromium. Most users will want to use platform-specific repos, where all the remaining configuration and scripts are provided for specific platforms:

[**Find the repo for a specific platform here**](docs/platforms.md).

If you wish to include ungoogled-chromium code in your own build process, consider using [the tags in this repo](https://github.com/ungoogled-software/ungoogled-chromium/tags). These tags follow the format `{chromium_version}-{revision}` where

* `chromium_version` is the version of Chromium used in `x.x.x.x` format, and
* `revision` is a number indicating the version of ungoogled-chromium for the corresponding Chromium version.

Additionally, most platform-specific repos extend their tag scheme upon this one.

**Building the source code**: [See docs/building.md](docs/building.md)

### Mirrors

List of mirrors:

* [Codeberg](https://codeberg.org): [main repo](https://codeberg.org/ungoogled-software/ungoogled-chromium) and [ungoogled-software](https://codeberg.org/ungoogled-software)

## FAQ

[See the frequently-asked questions (FAQ) on the Wiki](https://ungoogled-software.github.io/ungoogled-chromium-wiki/faq)

## Building Instructions

[See docs/building.md](docs/building.md)

## Design Documentation

[See docs/design.md](docs/design.md)

## Contributing, Reporting, Contacting

* For reporting and contacting, see [SUPPORT.md](SUPPORT.md)
* If you&#039;re willing to help, check out the [Issue Tracker](https://github.com/ungoogled-software/ungoogled-chromium/issues) and especially issues, which [need help](https://github.com/ungoogled-software/ungoogled-chromium/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22)
* For contributing (e.g. how to help, submitting changes, criteria for new features), see [docs/contributing.md](docs/contributing.md)
* If you have some small contributions that don&#039;t fit our criteria, consider adding them to [ungoogled-software/contrib](https://github.com/ungoogled-software/contrib) or [our Wiki](https://github.com/ungoogled-software/ungoogled-chromium-wiki) instead.

## Credits

* [The Chromium Project](https://www.chromium.org/)
* [Inox patchset](https://github.com/gcarq/inox-patchset)
* [Debian](https://tracker.debian.org/pkg/chromium-browser)
* [Bromite](https://github.com/bromite/bromite)
* [Iridium Browser](https://iridiumbrowser.de/)
* The users for testing and debugging, [contributing code](https://github.com/ungoogled-software/ungoogled-chromium/graphs/contributors), providing feedback, or simply using ungoogled-chromium in some capacity.

## Related Projects

List of known projects that fork or use changes from ungoogled-chromium:

* [Bromite](https://github.com/bromite/bromite) (Borrows some patches. Features builds for Android)
* [ppc64le fork](https://github.com/leo-lb/ungoogled-chromium) (Fork with changes to build for ppc64le CPUs)

## License

BSD-3-clause. See [LICENSE](LICENSE)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/BitNet]]></title>
            <link>https://github.com/microsoft/BitNet</link>
            <guid>https://github.com/microsoft/BitNet</guid>
            <pubDate>Tue, 09 Sep 2025 00:04:06 GMT</pubDate>
            <description><![CDATA[Official inference framework for 1-bit LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/BitNet">microsoft/BitNet</a></h1>
            <p>Official inference framework for 1-bit LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 21,871</p>
            <p>Forks: 1,675</p>
            <p>Stars today: 191 stars today</p>
            <h2>README</h2><pre># bitnet.cpp
[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)
![version](https://img.shields.io/badge/version-1.0-blue)

[&lt;img src=&quot;./assets/header_model_release.png&quot; alt=&quot;BitNet Model on Hugging Face&quot; width=&quot;800&quot;/&gt;](https://huggingface.co/microsoft/BitNet-b1.58-2B-4T)

Try it out via this [demo](https://bitnet-demo.azurewebsites.net/), or build and run it on your own [CPU](https://github.com/microsoft/BitNet?tab=readme-ov-file#build-from-source) or [GPU](https://github.com/microsoft/BitNet/blob/main/gpu/README.md).

bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support **fast** and **lossless** inference of 1.58-bit models on CPU and GPU (NPU support will coming next).

The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of **1.37x** to **5.07x** on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by **55.4%** to **70.0%**, further boosting overall efficiency. On x86 CPUs, speedups range from **2.37x** to **6.17x** with energy reductions between **71.9%** to **82.2%**. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. Please refer to the [technical report](https://arxiv.org/abs/2410.16144) for more details.

&lt;img src=&quot;./assets/m2_performance.jpg&quot; alt=&quot;m2_performance&quot; width=&quot;800&quot;/&gt;
&lt;img src=&quot;./assets/intel_performance.jpg&quot; alt=&quot;m2_performance&quot; width=&quot;800&quot;/&gt;

&gt;The tested models are dummy setups used in a research context to demonstrate the inference performance of bitnet.cpp.

## Demo

A demo of bitnet.cpp running a BitNet b1.58 3B model on Apple M2:

https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1

## What&#039;s New:
- 05/20/2025 [BitNet Official GPU inference kernel](https://github.com/microsoft/BitNet/blob/main/gpu/README.md) ![NEW](https://img.shields.io/badge/NEW-red)
- 04/14/2025 [BitNet Official 2B Parameter Model on Hugging Face](https://huggingface.co/microsoft/BitNet-b1.58-2B-4T)
- 02/18/2025 [Bitnet.cpp: Efficient Edge Inference for Ternary LLMs](https://arxiv.org/abs/2502.11880)
- 11/08/2024 [BitNet a4.8: 4-bit Activations for 1-bit LLMs](https://arxiv.org/abs/2411.04965)
- 10/21/2024 [1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs](https://arxiv.org/abs/2410.16144)
- 10/17/2024 bitnet.cpp 1.0 released.
- 03/21/2024 [The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ](https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf)
- 02/27/2024 [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764)
- 10/17/2023 [BitNet: Scaling 1-bit Transformers for Large Language Models](https://arxiv.org/abs/2310.11453)

## Acknowledgements

This project is based on the [llama.cpp](https://github.com/ggerganov/llama.cpp) framework. We would like to thank all the authors for their contributions to the open-source community. Also, bitnet.cpp&#039;s kernels are built on top of the Lookup Table methodologies pioneered in [T-MAC](https://github.com/microsoft/T-MAC/). For inference of general low-bit LLMs beyond ternary models, we recommend using T-MAC.
## Official Models
&lt;table&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th rowspan=&quot;2&quot;&gt;Model&lt;/th&gt;
        &lt;th rowspan=&quot;2&quot;&gt;Parameters&lt;/th&gt;
        &lt;th rowspan=&quot;2&quot;&gt;CPU&lt;/th&gt;
        &lt;th colspan=&quot;3&quot;&gt;Kernel&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;I2_S&lt;/th&gt;
        &lt;th&gt;TL1&lt;/th&gt;
        &lt;th&gt;TL2&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/microsoft/BitNet-b1.58-2B-4T&quot;&gt;BitNet-b1.58-2B-4T&lt;/a&gt;&lt;/td&gt;
        &lt;td rowspan=&quot;2&quot;&gt;2.4B&lt;/td&gt;
        &lt;td&gt;x86&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ARM&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

## Supported Models
‚ùóÔ∏è**We use existing 1-bit LLMs available on [Hugging Face](https://huggingface.co/) to demonstrate the inference capabilities of bitnet.cpp. We hope the release of bitnet.cpp will inspire the development of 1-bit LLMs in large-scale settings in terms of model size and training tokens.**

&lt;table&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th rowspan=&quot;2&quot;&gt;Model&lt;/th&gt;
        &lt;th rowspan=&quot;2&quot;&gt;Parameters&lt;/th&gt;
        &lt;th rowspan=&quot;2&quot;&gt;CPU&lt;/th&gt;
        &lt;th colspan=&quot;3&quot;&gt;Kernel&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;I2_S&lt;/th&gt;
        &lt;th&gt;TL1&lt;/th&gt;
        &lt;th&gt;TL2&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/1bitLLM/bitnet_b1_58-large&quot;&gt;bitnet_b1_58-large&lt;/a&gt;&lt;/td&gt;
        &lt;td rowspan=&quot;2&quot;&gt;0.7B&lt;/td&gt;
        &lt;td&gt;x86&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ARM&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/1bitLLM/bitnet_b1_58-3B&quot;&gt;bitnet_b1_58-3B&lt;/a&gt;&lt;/td&gt;
        &lt;td rowspan=&quot;2&quot;&gt;3.3B&lt;/td&gt;
        &lt;td&gt;x86&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ARM&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens&quot;&gt;Llama3-8B-1.58-100B-tokens&lt;/a&gt;&lt;/td&gt;
        &lt;td rowspan=&quot;2&quot;&gt;8.0B&lt;/td&gt;
        &lt;td&gt;x86&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ARM&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026&quot;&gt;Falcon3 Family&lt;/a&gt;&lt;/td&gt;
        &lt;td rowspan=&quot;2&quot;&gt;1B-10B&lt;/td&gt;
        &lt;td&gt;x86&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ARM&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/collections/tiiuae/falcon-edge-series-6804fd13344d6d8a8fa71130&quot;&gt;Falcon-E Family&lt;/a&gt;&lt;/td&gt;
        &lt;td rowspan=&quot;2&quot;&gt;1B-3B&lt;/td&gt;
        &lt;td&gt;x86&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ARM&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;



## Installation

### Requirements
- python&gt;=3.9
- cmake&gt;=3.22
- clang&gt;=18
    - For Windows users, install [Visual Studio 2022](https://visualstudio.microsoft.com/downloads/). In the installer, toggle on at least the following options(this also automatically installs the required additional tools like CMake):
        -  Desktop-development with C++
        -  C++-CMake Tools for Windows
        -  Git for Windows
        -  C++-Clang Compiler for Windows
        -  MS-Build Support for LLVM-Toolset (clang)
    - For Debian/Ubuntu users, you can download with [Automatic installation script](https://apt.llvm.org/)

        `bash -c &quot;$(wget -O - https://apt.llvm.org/llvm.sh)&quot;`
- conda (highly recommend)

### Build from source

&gt; [!IMPORTANT]
&gt; If you are using Windows, please remember to always use a Developer Command Prompt / PowerShell for VS2022 for the following commands. Please refer to the FAQs below if you see any issues.

1. Clone the repo
```bash
git clone --recursive https://github.com/microsoft/BitNet.git
cd BitNet
```
2. Install the dependencies
```bash
# (Recommended) Create a new conda environment
conda create -n bitnet-cpp python=3.9
conda activate bitnet-cpp

pip install -r requirements.txt
```
3. Build the project
```bash
# Manually download the model and run with local path
huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf --local-dir models/BitNet-b1.58-2B-4T
python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s

```
&lt;pre&gt;
usage: setup_env.py [-h] [--hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}] [--model-dir MODEL_DIR] [--log-dir LOG_DIR] [--quant-type {i2_s,tl1}] [--quant-embd]
                    [--use-pretuned]

Setup the environment for running inference

optional arguments:
  -h, --help            show this help message and exit
  --hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}, -hr {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}
                        Model used for inference
  --model-dir MODEL_DIR, -md MODEL_DIR
                        Directory to save/load the model
  --log-dir LOG_DIR, -ld LOG_DIR
                        Directory to save the logging info
  --quant-type {i2_s,tl1}, -q {i2_s,tl1}
                        Quantization type
  --quant-embd          Quantize the embeddings to f16
  --use-pretuned, -p    Use the pretuned kernel parameters
&lt;/pre&gt;
## Usage
### Basic usage
```bash
# Run inference with the quantized model
python run_inference.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -p &quot;You are a helpful assistant&quot; -cnv
```
&lt;pre&gt;
usage: run_inference.py [-h] [-m MODEL] [-n N_PREDICT] -p PROMPT [-t THREADS] [-c CTX_SIZE] [-temp TEMPERATURE] [-cnv]

Run inference

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Path to model file
  -n N_PREDICT, --n-predict N_PREDICT
                        Number of tokens to predict when generating text
  -p PROMPT, --prompt PROMPT
                        Prompt to generate text from
  -t THREADS, --threads THREADS
                        Number of threads to use
  -c CTX_SIZE, --ctx-size CTX_SIZE
                        Size of the prompt context
  -temp TEMPERATURE, --temperature TEMPERATURE
                        Temperature, a hyperparameter that controls the randomness of the generated text
  -cnv, --conversation  Whether to enable chat mode or not (for instruct models.)
                        (When this option is turned on, the prompt specified by -p will be used as the system prompt.)
&lt;/pre&gt;

### Benchmark
We provide scripts to run the inference benchmark providing a model.

```  
usage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  
   
Setup the environment for running the inference  
   
required arguments:  
  -m MODEL, --model MODEL  
                        Path to the model file. 
   
optional arguments:  
  -h, --help  
                        Show this help message and exit. 
  -n N_TOKEN, --n-token N_TOKEN  
                        Number of generated tokens. 
  -p N_PROMPT, --n-prompt N_PROMPT  
                        Prompt to generate text from. 
  -t THREADS, --threads THREADS  
                        Number of threads to use. 
```  
   
Here&#039;s a brief explanation of each argument:  
   
- `-m`, `--model`: The path to the model file. This is a required argument that must be provided when running the script.  
- `-n`, `--n-token`: The number of tokens to generate during the inference. It is an optional argument with a default value of 128.  
- `-p`, `--n-prompt`: The number of prompt tokens to use for generating text. This is an optional argument with a default value of 512.  
- `-t`, `--threads`: The number of threads to use for running the inference. It is an optional argument with a default value of 2.  
- `-h`, `--help`: Show the help message and exit. Use this argument to display usage information.  
   
For example:  
   
```sh  
python utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  
```  
   
This command would run the inference benchmark using the model located at `/path/to/model`, generating 200 tokens from a 256 token prompt, utilizing 4 threads.  

For the model layout that do not supported by any public model, we provide scripts to generate a dummy model with the given model layout, and run the benchmark on your machine:

```bash
python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M

# Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate
python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128
```

### Convert from `.safetensors` Checkpoints

```sh
# Prepare the .safetensors model file
huggingface-cli download microsoft/bitnet-b1.58-2B-4T-bf16 --local-dir ./models/bitnet-b1.58-2B-4T-bf16

# Convert to gguf model
python ./utils/convert-helper-bitnet.py ./models/bitnet-b1.58-2B-4T-bf16
```

### FAQ (Frequently Asked Questions)üìå 

#### Q1: The build dies with errors building llama.cpp due to issues with std::chrono in log.cpp?

**A:**
This is an issue introduced in recent version of llama.cpp. Please refer to this [commit](https://github.com/tinglou/llama.cpp/commit/4e3db1e3d78cc1bcd22bcb3af54bd2a4628dd323) in the [discussion](https://github.com/abetlen/llama-cpp-python/issues/1942) to fix this issue.

#### Q2: How to build with clang in conda environment on windows?

**A:** 
Before building the project, verify your clang installation and access to Visual Studio tools by running:
```
clang -v
```

This command checks that you are using the correct version of clang and that the Visual Studio tools are available. If you see an error message such as:
```
&#039;clang&#039; is not recognized as an internal or external command, operable program or batch file.
```

It indicates that your command line window is not properly initialized for Visual Studio tools.

‚Ä¢ If you are using Command Prompt, run:
```
&quot;C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\VsDevCmd.bat&quot; -startdir=none -arch=x64 -host_arch=x64
```

‚Ä¢ If you are using Windows PowerShell, run the following commands:
```
Import-Module &quot;C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\Microsoft.VisualStudio.DevShell.dll&quot; Enter-VsDevShell 3f0e31ad -SkipAutomaticLocation -DevCmdArguments &quot;-arch=x64 -host_arch=x64&quot;
```

These steps will initialize your environment and allow you to use the correct Visual Studio tools.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[scipy/scipy]]></title>
            <link>https://github.com/scipy/scipy</link>
            <guid>https://github.com/scipy/scipy</guid>
            <pubDate>Tue, 09 Sep 2025 00:04:05 GMT</pubDate>
            <description><![CDATA[SciPy library main repository]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/scipy/scipy">scipy/scipy</a></h1>
            <p>SciPy library main repository</p>
            <p>Language: Python</p>
            <p>Stars: 13,994</p>
            <p>Forks: 5,469</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[coleam00/ottomator-agents]]></title>
            <link>https://github.com/coleam00/ottomator-agents</link>
            <guid>https://github.com/coleam00/ottomator-agents</guid>
            <pubDate>Tue, 09 Sep 2025 00:04:04 GMT</pubDate>
            <description><![CDATA[All the open source AI Agents hosted on the oTTomator Live Agent Studio platform!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/coleam00/ottomator-agents">coleam00/ottomator-agents</a></h1>
            <p>All the open source AI Agents hosted on the oTTomator Live Agent Studio platform!</p>
            <p>Language: Python</p>
            <p>Stars: 4,101</p>
            <p>Forks: 1,450</p>
            <p>Stars today: 117 stars today</p>
            <h2>README</h2><pre># What is the Live Agent Studio?

The [Live Agent Studio](https://studio.ottomator.ai) is a community-driven platform developed by [oTTomator](https://ottomator.ai) for you to explore cutting-edge AI agents and learn how to implement them for yourself or your business! All agents on this platform are open source and, over time, will cover a very large variety of use cases.

The goal with the studio is to build an educational platform for you to learn how to do incredible things with AI, while still providing practical value so that you‚Äôll want to use the agents just for the sake of what they can do for you!

This platform is still in beta ‚Äì expect longer response times under load, a rapidly growing agent library over the coming months, and a lot more content on this platform soon on Cole Medin‚Äôs YouTube channel!

# What is this Repository for?

This repository contains the source code/workflow JSON for all the agents on the Live Agent Studio! Every agent being added to the platform is currently be open sourced here so we can not only create a curated collection of cutting-edge agents together as a community, but also learn from one another!

## Tokens

Most agents on the Live Agent Studio cost tokens to use, which are purchasable on the platform. However, when you first sign in you are given some tokens to start so you can use the agents free of charge! The biggest reason agents cost tokens is that we pay for the LLM usage since we host all the agents developed by you and the rest of the community!

[Purchase Tokens](https://studio.ottomator.ai/pricing)

## Future Plans

As the Live Agent Studio develops, it will become the go-to place to stay on top of what is possible with AI agents! Anytime there is a new AI technology, groundbreaking agent research, or a new tool/library to build agents with, it‚Äôll be featured through agents on the platform. It‚Äôs a tall order, but we have big plans for the oTTomator community, and we‚Äôre confident we can grow to accomplish this!

## FAQ

### I want to build an agent to showcase in the Live Agent Studio! How do I do that?

Head on over here to learn how to build an agent for the platform:

[Developer Guide](https://studio.ottomator.ai/guide)

Also check out [the sample n8n agent](~sample-n8n-agent~) for a starting point of building an n8n agent for the Live Agent Studio, and [the sample Python agent](~sample-python-agent~) for Python.

### How many tokens does it cost to use an agent?

Each agent will charge tokens per prompt. The number of tokens depends on the agent, as some agents use larger LLMs, some call LLMs multiple times, and some use paid APIs.

### Where can I go to talk about all these agents and get help implementing them myself?

Head on over to our Think Tank community and feel free to make a post!

[Think Tank Community](https://thinktank.ottomator.ai)

---

&amp;copy; 2024 Live Agent Studio. All rights reserved.  
Created by oTTomator
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NVIDIA/TensorRT-Model-Optimizer]]></title>
            <link>https://github.com/NVIDIA/TensorRT-Model-Optimizer</link>
            <guid>https://github.com/NVIDIA/TensorRT-Model-Optimizer</guid>
            <pubDate>Tue, 09 Sep 2025 00:04:03 GMT</pubDate>
            <description><![CDATA[A unified library of state-of-the-art model optimization techniques like quantization, pruning, distillation, speculative decoding, etc. It compresses deep learning models for downstream deployment frameworks like TensorRT-LLM or TensorRT to optimize inference speed.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NVIDIA/TensorRT-Model-Optimizer">NVIDIA/TensorRT-Model-Optimizer</a></h1>
            <p>A unified library of state-of-the-art model optimization techniques like quantization, pruning, distillation, speculative decoding, etc. It compresses deep learning models for downstream deployment frameworks like TensorRT-LLM or TensorRT to optimize inference speed.</p>
            <p>Language: Python</p>
            <p>Stars: 1,308</p>
            <p>Forks: 145</p>
            <p>Stars today: 52 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

![Banner image](docs/source/assets/model-optimizer-banner.png)

# NVIDIA TensorRT Model Optimizer

[![Documentation](https://img.shields.io/badge/Documentation-latest-brightgreen.svg?style=flat)](https://nvidia.github.io/TensorRT-Model-Optimizer)
[![version](https://img.shields.io/pypi/v/nvidia-modelopt?label=Release)](https://pypi.org/project/nvidia-modelopt/)
[![license](https://img.shields.io/badge/License-Apache%202.0-blue)](./LICENSE)

[Documentation](https://nvidia.github.io/TensorRT-Model-Optimizer) |
[Roadmap](https://github.com/NVIDIA/TensorRT-Model-Optimizer/issues/146)

&lt;/div&gt;

______________________________________________________________________

The **NVIDIA TensorRT Model Optimizer** (referred to as **Model Optimizer**, or **ModelOpt**) is a library comprising state-of-the-art model optimization [techniques](#techniques) including quantization, distillation, pruning, speculative decoding and sparsity to accelerate models.

**[Input]** Model Optimizer currently supports inputs of a [Hugging Face](https://huggingface.co/), [PyTorch](https://github.com/pytorch/pytorch) or [ONNX](https://github.com/onnx/onnx) model.

**[Optimize]** Model Optimizer provides Python APIs for users to easily compose the above model optimization techniques and export an optimized quantized checkpoint.
Model Optimizer is also integrated with [NVIDIA NeMo](https://github.com/NVIDIA-NeMo/NeMo), [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) and [Hugging Face Accelerate](https://github.com/huggingface/accelerate) for training required inference optimization techniques.

**[Export for deployment]** Seamlessly integrated within the NVIDIA AI software ecosystem, the quantized checkpoint generated from Model Optimizer is ready for deployment in downstream inference frameworks like [SGLang](https://github.com/sgl-project/sglang), [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/quantization), [TensorRT](https://github.com/NVIDIA/TensorRT), or [vLLM](https://github.com/vllm-project/vllm).

## Latest News

- [2025/08/29] [Fine-Tuning gpt-oss for Accuracy and Performance with Quantization Aware Training](https://developer.nvidia.com/blog/fine-tuning-gpt-oss-for-accuracy-and-performance-with-quantization-aware-training/)
- [2025/08/01] [Optimizing LLMs for Performance and Accuracy with Post-Training Quantization](https://developer.nvidia.com/blog/optimizing-llms-for-performance-and-accuracy-with-post-training-quantization/)
- [2025/06/24] [Introducing NVFP4 for Efficient and Accurate Low-Precision Inference](https://developer.nvidia.com/blog/introducing-nvfp4-for-efficient-and-accurate-low-precision-inference/)
- [2025/05/14] [NVIDIA TensorRT Unlocks FP4 Image Generation for NVIDIA Blackwell GeForce RTX 50 Series GPUs](https://developer.nvidia.com/blog/nvidia-tensorrt-unlocks-fp4-image-generation-for-nvidia-blackwell-geforce-rtx-50-series-gpus/)
- [2025/04/21] [Adobe optimized deployment using TensorRT-Model-Optimizer + TensorRT leading to a 60% reduction in diffusion latency, a 40% reduction in total cost of ownership](https://developer.nvidia.com/blog/optimizing-transformer-based-diffusion-models-for-video-generation-with-nvidia-tensorrt/)
- [2025/04/05] [NVIDIA Accelerates Inference on Meta Llama 4 Scout and Maverick](https://developer.nvidia.com/blog/nvidia-accelerates-inference-on-meta-llama-4-scout-and-maverick/). Check out how to quantize Llama4 for deployment acceleration [here](./examples/llm_ptq/README.md#llama-4)
- [2025/03/18] [World&#039;s Fastest DeepSeek-R1 Inference with Blackwell FP4 &amp; Increasing Image Generation Efficiency on Blackwell](https://developer.nvidia.com/blog/nvidia-blackwell-delivers-world-record-deepseek-r1-inference-performance/)
- [2025/02/25] Model Optimizer quantized NVFP4 models available on Hugging Face for download: [DeepSeek-R1-FP4](https://huggingface.co/nvidia/DeepSeek-R1-FP4), [Llama-3.3-70B-Instruct-FP4](https://huggingface.co/nvidia/Llama-3.3-70B-Instruct-FP4), [Llama-3.1-405B-Instruct-FP4](https://huggingface.co/nvidia/Llama-3.1-405B-Instruct-FP4)
- [2025/01/28] Model Optimizer has added support for NVFP4. Check out an example of NVFP4 PTQ [here](./examples/llm_ptq/README.md#model-quantization-and-trt-llm-conversion).
- [2025/01/28] Model Optimizer is now open source!
- [2024/10/23] Model Optimizer quantized FP8 Llama-3.1 Instruct models available on Hugging Face for download: [8B](https://huggingface.co/nvidia/Llama-3.1-8B-Instruct-FP8), [70B](https://huggingface.co/nvidia/Llama-3.1-70B-Instruct-FP8), [405B](https://huggingface.co/nvidia/Llama-3.1-405B-Instruct-FP8).
- [2024/09/10] [Post-Training Quantization of LLMs with NVIDIA NeMo and TensorRT Model Optimizer](https://developer.nvidia.com/blog/post-training-quantization-of-llms-with-nvidia-nemo-and-nvidia-tensorrt-model-optimizer/).

&lt;details close&gt;
&lt;summary&gt;Previous News&lt;/summary&gt;

- [2024/08/28] [Boosting Llama 3.1 405B Performance up to 44% with TensorRT Model Optimizer on NVIDIA H200 GPUs](https://developer.nvidia.com/blog/boosting-llama-3-1-405b-performance-by-up-to-44-with-nvidia-tensorrt-model-optimizer-on-nvidia-h200-gpus/)
- [2024/08/28] [Up to 1.9X Higher Llama 3.1 Performance with Medusa](https://developer.nvidia.com/blog/low-latency-inference-chapter-1-up-to-1-9x-higher-llama-3-1-performance-with-medusa-on-nvidia-hgx-h200-with-nvlink-switch/)
- [2024/08/15] New features in recent releases: [Cache Diffusion](https://github.com/NVIDIA/TensorRT-Model-Optimizer/tree/main/examples/diffusers/cache_diffusion), [QLoRA workflow with NVIDIA NeMo](https://docs.nvidia.com/nemo-framework/user-guide/24.09/sft_peft/qlora.html), and more. Check out [our blog](https://developer.nvidia.com/blog/nvidia-tensorrt-model-optimizer-v0-15-boosts-inference-performance-and-expands-model-support/) for details.
- [2024/06/03] Model Optimizer now has an experimental feature to deploy to vLLM as part of our effort to support popular deployment frameworks. Check out the workflow [here](./examples/llm_ptq/README.md#deploy-fp8-quantized-model-using-vllm)
- [2024/05/08] [Announcement: Model Optimizer Now Formally Available to Further Accelerate GenAI Inference Performance](https://developer.nvidia.com/blog/accelerate-generative-ai-inference-performance-with-nvidia-tensorrt-model-optimizer-now-publicly-available/)
- [2024/03/27] [Model Optimizer supercharges TensorRT-LLM to set MLPerf LLM inference records](https://developer.nvidia.com/blog/nvidia-h200-tensor-core-gpus-and-nvidia-tensorrt-llm-set-mlperf-llm-inference-records/)
- [2024/03/18] [GTC Session: Optimize Generative AI Inference with Quantization in TensorRT-LLM and TensorRT](https://www.nvidia.com/en-us/on-demand/session/gtc24-s63213/)
- [2024/03/07] [Model Optimizer&#039;s 8-bit Post-Training Quantization enables TensorRT to accelerate Stable Diffusion to nearly 2x faster](https://developer.nvidia.com/blog/tensorrt-accelerates-stable-diffusion-nearly-2x-faster-with-8-bit-post-training-quantization/)
- [2024/02/01] [Speed up inference with Model Optimizer quantization techniques in TRT-LLM](https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/blogs/quantization-in-TRT-LLM.md)

&lt;/details&gt;

## Install

To install stable release packages for Model Optimizer with `pip` from [PyPI](https://pypi.org/project/nvidia-modelopt/):

```bash
pip install nvidia-modelopt[all]
```

To install from source in editable mode with all development dependencies or to test the latest changes, run:

```bash
# Clone the Model Optimizer repository
git clone https://github.com/NVIDIA/TensorRT-Model-Optimizer.git
cd TensorRT-Model-Optimizer

pip install -e .[dev]
```

Visit our [installation guide](https://nvidia.github.io/TensorRT-Model-Optimizer/getting_started/2_installation.html) for more fine-grained control on installed dependencies or view our pre-made [dockerfiles](docker/README.md) for more information.

## Techniques

&lt;div align=&quot;center&quot;&gt;

| **Technique** | **Description** | **Examples** | **Docs** |
| :------------: | :------------: | :------------: | :------------: |
| Post Training Quantization | Compress model size by 2x-4x, speeding up inference while preserving model quality! | \[[LLMs](./examples/llm_ptq/)\] \[[diffusers](./examples/diffusers/)\] \[[VLMs](./examples/vlm_ptq/)\] \[[onnx](./examples/onnx_ptq/)\] \[[windows](./examples/windows/)\] | \[[docs](https://nvidia.github.io/TensorRT-Model-Optimizer/guides/1_quantization.html)\] |
| Quantization Aware Training | Refine accuracy even further with a few training steps! | \[[NeMo](./examples/llm_qat#nemo-qatqad-simplified-flow-example)\] \[[Hugging Face](./examples/llm_qat/)\] | \[[docs](https://nvidia.github.io/TensorRT-Model-Optimizer/guides/1_quantization.html)\] |
| Pruning | Reduce your model size and accelerate inference by removing unnecessary weights! | \[[PyTorch](./examples/pruning/)\] | \[[docs](https://nvidia.github.io/TensorRT-Model-Optimizer/guides/3_pruning.html)\] |
| Distillation | Reduce deployment model size by teaching small models to behave like larger models! | \[[NeMo](./examples/llm_distill#knowledge-distillation-kd-for-nvidia-nemo-models)\] \[[Hugging Face](./examples/llm_distill/)\] | \[[docs](https://nvidia.github.io/TensorRT-Model-Optimizer/guides/4_distillation.html)\] |
| Speculative Decoding | Train draft modules to predict extra tokens during inference! | \[[Megatron](./examples/speculative_decoding#mlm-example)\] \[[Hugging Face](./examples/speculative_decoding/)\] | \[[docs](https://nvidia.github.io/TensorRT-Model-Optimizer/guides/5_speculative_decoding.html)\] |
| Sparsity | Efficiently compress your model by storing only its non-zero parameter values and their locations | \[[PyTorch](./examples/llm_sparsity/)\] | \[[docs](https://nvidia.github.io/TensorRT-Model-Optimizer/guides/6_sparsity.html)\] |

&lt;/div&gt;

## Pre-Quantized Checkpoints

- Ready-to-deploy checkpoints \[[ü§ó Hugging Face - Nvidia TensorRT Model Optimizer Collection](https://huggingface.co/collections/nvidia/model-optimizer-66aa84f7966b3150262481a4)\]
- Deployable on [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM), [vLLM](https://github.com/vllm-project/vllm) and [SGLang](https://github.com/sgl-project/sglang)
- More models coming soon!

## Resources

- üìÖ [Roadmap](https://github.com/NVIDIA/TensorRT-Model-Optimizer/issues/146)
- üìñ [Documentation](https://nvidia.github.io/TensorRT-Model-Optimizer)
- üéØ [Benchmarks](./examples/benchmark.md)
- üí° [Release Notes](https://nvidia.github.io/TensorRT-Model-Optimizer/reference/0_changelog.html)
- üêõ [File a bug](https://github.com/NVIDIA/TensorRT-Model-Optimizer/issues/new?template=1_bug_report.md)
- ‚ú® [File a Feature Request](https://github.com/NVIDIA/TensorRT-Model-Optimizer/issues/new?template=2_feature_request.md)

## Model Support Matrix

| Model Type | Support Matrix |
|------------|----------------|
| LLM Quantization | [View Support Matrix](./examples/llm_ptq/README.md#support-matrix) |
| Diffusers Quantization | [View Support Matrix](./examples/diffusers/README.md#support-matrix) |
| VLM Quantization | [View Support Matrix](./examples/vlm_ptq/README.md#support-matrix) |
| ONNX Quantization | [View Support Matrix](./examples/onnx_ptq/README.md#onnx-export-supported-llm-models) |
| Windows Quantization | [View Support Matrix](./examples/windows/README.md#support-matrix) |
| Quantization Aware Training | [View Support Matrix](./examples/llm_qat/README.md#support-matrix) |
| Pruning | [View Support Matrix](./examples/pruning/README.md#support-matrix) |
| Distillation | [View Support Matrix](./examples/llm_distill/README.md#support-matrix) |
| Speculative Decoding | [View Support Matrix](./examples/speculative_decoding/README.md#support-matrix) |

## Contributing

Model Optimizer is now open source! We welcome any feedback, feature requests and PRs.
Please read our [Contributing](./CONTRIBUTING.md) guidelines for details on how to contribute to this project.

### Top Contributors

[![Contributors](https://contrib.rocks/image?repo=NVIDIA/TensorRT-Model-Optimizer)](https://github.com/NVIDIA/TensorRT-Model-Optimizer/graphs/contributors)

Happy optimizing!
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hacksider/Deep-Live-Cam]]></title>
            <link>https://github.com/hacksider/Deep-Live-Cam</link>
            <guid>https://github.com/hacksider/Deep-Live-Cam</guid>
            <pubDate>Tue, 09 Sep 2025 00:04:02 GMT</pubDate>
            <description><![CDATA[real time face swap and one-click video deepfake with only a single image]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hacksider/Deep-Live-Cam">hacksider/Deep-Live-Cam</a></h1>
            <p>real time face swap and one-click video deepfake with only a single image</p>
            <p>Language: Python</p>
            <p>Stars: 73,040</p>
            <p>Forks: 10,569</p>
            <p>Stars today: 40 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;Deep-Live-Cam&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  Real-time face swap and video deepfake with a single click and only a single image.
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/11395&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11395&quot; alt=&quot;hacksider%2FDeep-Live-Cam | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/demo.gif&quot; alt=&quot;Demo GIF&quot; width=&quot;800&quot;&gt;
&lt;/p&gt;

##  Disclaimer

This deepfake software is designed to be a productive tool for the AI-generated media industry. It can assist artists in animating custom characters, creating engaging content, and even using models for clothing design.

We are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to the law and ethics. We may shut down the project or add watermarks if legally required.

- Ethical Use: Users are expected to use this software responsibly and legally. If using a real person&#039;s face, obtain their consent and clearly label any output as a deepfake when sharing online.

- Content Restrictions: The software includes built-in checks to prevent processing inappropriate media, such as nudity, graphic content, or sensitive material.

- Legal Compliance: We adhere to all relevant laws and ethical guidelines. If legally required, we may shut down the project or add watermarks to the output.

- User Responsibility: We are not responsible for end-user actions. Users must ensure their use of the software aligns with ethical standards and legal requirements.

By using this software, you agree to these terms and commit to using it in a manner that respects the rights and dignity of others.

Users are expected to use this software responsibly and legally. If using a real person&#039;s face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user actions.

## Exclusive v2.2 Quick Start - Pre-built (Windows/Mac Silicon)

  &lt;a href=&quot;https://deeplivecam.net/index.php/quickstart&quot;&gt; &lt;img src=&quot;media/Download.png&quot; width=&quot;285&quot; height=&quot;77&quot; /&gt;

##### This is the fastest build you can get if you have a discrete NVIDIA or AMD GPU or Mac Silicon, And you&#039;ll receive special priority support.
 
###### These Pre-builts are perfect for non-technical users or those who don&#039;t have time to, or can&#039;t manually install all the requirements. Just a heads-up: this is an open-source project, so you can also install it manually. 

## TLDR; Live Deepfake in just 3 Clicks
![easysteps](https://github.com/user-attachments/assets/af825228-852c-411b-b787-ffd9aac72fc6)
1. Select a face
2. Select which camera to use
3. Press live!

## Features &amp; Uses - Everything is in real-time

### Mouth Mask

**Retain your original mouth for accurate movement using Mouth Mask**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/ludwig.gif&quot; alt=&quot;resizable-gif&quot;&gt;
&lt;/p&gt;

### Face Mapping

**Use different faces on multiple subjects simultaneously**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/streamers.gif&quot; alt=&quot;face_mapping_source&quot;&gt;
&lt;/p&gt;

### Your Movie, Your Face

**Watch movies with any face in real-time**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/movie.gif&quot; alt=&quot;movie&quot;&gt;
&lt;/p&gt;

### Live Show

**Run Live shows and performances**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/live_show.gif&quot; alt=&quot;show&quot;&gt;
&lt;/p&gt;

### Memes

**Create Your Most Viral Meme Yet**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/meme.gif&quot; alt=&quot;show&quot; width=&quot;450&quot;&gt; 
  &lt;br&gt;
  &lt;sub&gt;Created using Many Faces feature in Deep-Live-Cam&lt;/sub&gt;
&lt;/p&gt;

### Omegle

**Surprise people on Omegle**

&lt;p align=&quot;center&quot;&gt;
  &lt;video src=&quot;https://github.com/user-attachments/assets/2e9b9b82-fa04-4b70-9f56-b1f68e7672d0&quot; width=&quot;450&quot; controls&gt;&lt;/video&gt;
&lt;/p&gt;

## Installation (Manual)

**Please be aware that the installation requires technical skills and is not for beginners. Consider downloading the quickstart version.**

&lt;details&gt;
&lt;summary&gt;Click to see the process&lt;/summary&gt;

### Installation

This is more likely to work on your computer but will be slower as it utilizes the CPU.

**1. Set up Your Platform**

-   Python (3.11 recommended)
-   pip
-   git
-   [ffmpeg](https://www.youtube.com/watch?v=OlNWCpFdVMA) - ```iex (irm ffmpeg.tc.ht)```
-   [Visual Studio 2022 Runtimes (Windows)](https://visualstudio.microsoft.com/visual-cpp-build-tools/)

**2. Clone the Repository**

```bash
git clone https://github.com/hacksider/Deep-Live-Cam.git
cd Deep-Live-Cam
```

**3. Download the Models**

1. [GFPGANv1.4](https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth)
2. [inswapper\_128\_fp16.onnx](https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx)

Place these files in the &quot;**models**&quot; folder.

**4. Install Dependencies**

We highly recommend using a `venv` to avoid issues.


For Windows:
```bash
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
```
For Linux:
```bash
# Ensure you use the installed Python 3.10
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

**For macOS:**

Apple Silicon (M1/M2/M3) requires specific setup:

```bash
# Install Python 3.11 (specific version is important)
brew install python@3.11

# Install tkinter package (required for the GUI)
brew install python-tk@3.10

# Create and activate virtual environment with Python 3.11
python3.11 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

** In case something goes wrong and you need to reinstall the virtual environment **

```bash
# Deactivate the virtual environment
rm -rf venv

# Reinstall the virtual environment
python -m venv venv
source venv/bin/activate

# install the dependencies again
pip install -r requirements.txt

# gfpgan and basicsrs issue fix
pip install git+https://github.com/xinntao/BasicSR.git@master
pip uninstall gfpgan -y
pip install git+https://github.com/TencentARC/GFPGAN.git@master
```

**Run:** If you don&#039;t have a GPU, you can run Deep-Live-Cam using `python run.py`. Note that initial execution will download models (~300MB).

### GPU Acceleration

**CUDA Execution Provider (Nvidia)**

1. Install [CUDA Toolkit 12.8.0](https://developer.nvidia.com/cuda-12-8-0-download-archive)
2. Install [cuDNN v8.9.7 for CUDA 12.x](https://developer.nvidia.com/rdp/cudnn-archive) (required for onnxruntime-gpu):
   - Download cuDNN v8.9.7 for CUDA 12.x
   - Make sure the cuDNN bin directory is in your system PATH
3. Install dependencies:

```bash
pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu==1.21.0
```

3. Usage:

```bash
python run.py --execution-provider cuda
```

**CoreML Execution Provider (Apple Silicon)**

Apple Silicon (M1/M2/M3) specific installation:

1. Make sure you&#039;ve completed the macOS setup above using Python 3.10.
2. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-silicon
pip install onnxruntime-silicon==1.13.1
```

3. Usage (important: specify Python 3.10):

```bash
python3.10 run.py --execution-provider coreml
```

**Important Notes for macOS:**
- You **must** use Python 3.10, not newer versions like 3.11 or 3.13
- Always run with `python3.10` command not just `python` if you have multiple Python versions installed
- If you get error about `_tkinter` missing, reinstall the tkinter package: `brew reinstall python-tk@3.10`
- If you get model loading errors, check that your models are in the correct folder
- If you encounter conflicts with other Python versions, consider uninstalling them:
  ```bash
  # List all installed Python versions
  brew list | grep python
  
  # Uninstall conflicting versions if needed
  brew uninstall --ignore-dependencies python@3.11 python@3.13
  
  # Keep only Python 3.11
  brew cleanup
  ```

**CoreML Execution Provider (Apple Legacy)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-coreml
pip install onnxruntime-coreml==1.21.0
```

2. Usage:

```bash
python run.py --execution-provider coreml
```

**DirectML Execution Provider (Windows)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-directml
pip install onnxruntime-directml==1.21.0
```

2. Usage:

```bash
python run.py --execution-provider directml
```

**OpenVINO‚Ñ¢ Execution Provider (Intel)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-openvino
pip install onnxruntime-openvino==1.21.0
```

2. Usage:

```bash
python run.py --execution-provider openvino
```
&lt;/details&gt;

## Usage

**1. Image/Video Mode**

-   Execute `python run.py`.
-   Choose a source face image and a target image/video.
-   Click &quot;Start&quot;.
-   The output will be saved in a directory named after the target video.

**2. Webcam Mode**

-   Execute `python run.py`.
-   Select a source face image.
-   Click &quot;Live&quot;.
-   Wait for the preview to appear (10-30 seconds).
-   Use a screen capture tool like OBS to stream.
-   To change the face, select a new source image.

## Command Line Arguments (Unmaintained)

```
options:
  -h, --help                                               show this help message and exit
  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image
  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video
  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory
  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)
  --keep-fps                                               keep original fps
  --keep-audio                                             keep original audio
  --keep-frames                                            keep temporary frames
  --many-faces                                             process every face
  --map-faces                                              map source target faces
  --mouth-mask                                             mask the mouth region
  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder
  --video-quality [0-51]                                   adjust output video quality
  --live-mirror                                            the live camera display as you see it in the front-facing camera frame
  --live-resizable                                         the live camera frame is resizable
  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB
  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)
  --execution-threads EXECUTION_THREADS                    number of execution threads
  -v, --version                                            show program&#039;s version number and exit
```

Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.

## Press

**We are always open to criticism and are ready to improve, that&#039;s why we didn&#039;t cherry-pick anything.**

 - [*&quot;Deep-Live-Cam goes viral, allowing anyone to become a digital doppelganger&quot;*](https://arstechnica.com/information-technology/2024/08/new-ai-tool-enables-real-time-face-swapping-on-webcams-raising-fraud-concerns/) - Ars Technica
 - [*&quot;Thanks Deep Live Cam, shapeshifters are among us now&quot;*](https://dataconomy.com/2024/08/15/what-is-deep-live-cam-github-deepfake/) - Dataconomy
 - [*&quot;This free AI tool lets you become anyone during video-calls&quot;*](https://www.newsbytesapp.com/news/science/deep-live-cam-ai-impersonation-tool-goes-viral/story) - NewsBytes
 - [*&quot;OK, this viral AI live stream software is truly terrifying&quot;*](https://www.creativebloq.com/ai/ok-this-viral-ai-live-stream-software-is-truly-terrifying) - Creative Bloq
 - [*&quot;Deepfake AI Tool Lets You Become Anyone in a Video Call With Single Photo&quot;*](https://petapixel.com/2024/08/14/deep-live-cam-deepfake-ai-tool-lets-you-become-anyone-in-a-video-call-with-single-photo-mark-zuckerberg-jd-vance-elon-musk/) - PetaPixel
 - [*&quot;Deep-Live-Cam Uses AI to Transform Your Face in Real-Time, Celebrities Included&quot;*](https://www.techeblog.com/deep-live-cam-ai-transform-face/) - TechEBlog
 - [*&quot;An AI tool that &quot;makes you look like anyone&quot; during a video call is going viral online&quot;*](https://telegrafi.com/en/a-tool-that-makes-you-look-like-anyone-during-a-video-call-is-going-viral-on-the-Internet/) - Telegrafi
 - [*&quot;This Deepfake Tool Turning Images Into Livestreams is Topping the GitHub Charts&quot;*](https://decrypt.co/244565/this-deepfake-tool-turning-images-into-livestreams-is-topping-the-github-charts) - Emerge
 - [*&quot;New Real-Time Face-Swapping AI Allows Anyone to Mimic Famous Faces&quot;*](https://www.digitalmusicnews.com/2024/08/15/face-swapping-ai-real-time-mimic/) - Digital Music News
 - [*&quot;This real-time webcam deepfake tool raises alarms about the future of identity theft&quot;*](https://www.diyphotography.net/this-real-time-webcam-deepfake-tool-raises-alarms-about-the-future-of-identity-theft/) - DIYPhotography
 - [*&quot;That&#039;s Crazy, Oh God. That&#039;s Fucking Freaky Dude... That&#039;s So Wild Dude&quot;*](https://www.youtube.com/watch?time_continue=1074&amp;v=py4Tc-Y8BcY) - SomeOrdinaryGamers
 - [*&quot;Alright look look look, now look chat, we can do any face we want to look like chat&quot;*](https://www.youtube.com/live/mFsCe7AIxq8?feature=shared&amp;t=2686) - IShowSpeed
 - [*&quot;They do a pretty good job matching poses, expression and even the lighting&quot;*](https://www.youtube.com/watch?v=wnCghLjqv3s&amp;t=551s) - TechLinked (LTT)
 - [*&quot;Als Sean Connery an der Redaktionskonferenz teilnahm&quot;*](https://www.golem.de/news/deepfakes-als-sean-connery-an-der-redaktionskonferenz-teilnahm-2408-188172.html) - Golem.de (German)


## Credits

-   [ffmpeg](https://ffmpeg.org/): for making video-related operations easy
-   [deepinsight](https://github.com/deepinsight): for their [insightface](https://github.com/deepinsight/insightface) project which provided a well-made library and models. Please be reminded that the [use of the model is for non-commercial research purposes only](https://github.com/deepinsight/insightface?tab=readme-ov-file#license).
-   [havok2-htwo](https://github.com/havok2-htwo): for sharing the code for webcam
-   [GosuDRM](https://github.com/GosuDRM): for the open version of roop
-   [pereiraroland26](https://github.com/pereiraroland26): Multiple faces support
-   [vic4key](https://github.com/vic4key): For supporting/contributing to this project
-   [kier007](https://github.com/kier007): for improving the user experience
-   [qitianai](https://github.com/qitianai): for multi-lingual support
-   and [all developers](https://github.com/hacksider/Deep-Live-Cam/graphs/contributors) behind libraries used in this project.
-   Footnote: Please be informed that the base author of the code is [s0md3v](https://github.com/s0md3v/roop)
-   All the wonderful users who helped make this project go viral by starring the repo ‚ù§Ô∏è

[![Stargazers](https://reporoster.com/stars/hacksider/Deep-Live-Cam)](https://github.com/hacksider/Deep-Live-Cam/stargazers)

## Contributions

![Alt](https://repobeats.axiom.co/api/embed/fec8e29c45dfdb9c5916f3a7830e1249308d20e1.svg &quot;Repobeats analytics image&quot;)

## Stars to the Moon üöÄ

&lt;a href=&quot;https://star-history.com/#hacksider/deep-live-cam&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[virattt/ai-hedge-fund]]></title>
            <link>https://github.com/virattt/ai-hedge-fund</link>
            <guid>https://github.com/virattt/ai-hedge-fund</guid>
            <pubDate>Tue, 09 Sep 2025 00:04:01 GMT</pubDate>
            <description><![CDATA[An AI Hedge Fund Team]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/virattt/ai-hedge-fund">virattt/ai-hedge-fund</a></h1>
            <p>An AI Hedge Fund Team</p>
            <p>Language: Python</p>
            <p>Stars: 39,906</p>
            <p>Forks: 7,045</p>
            <p>Stars today: 78 stars today</p>
            <h2>README</h2><pre># AI Hedge Fund

This is a proof of concept for an AI-powered hedge fund.  The goal of this project is to explore the use of AI to make trading decisions.  This project is for **educational** purposes only and is not intended for real trading or investment.

This system employs several agents working together:

1. Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation
2. Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety
3. Bill Ackman Agent - An activist investor, takes bold positions and pushes for change
4. Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption
5. Charlie Munger Agent - Warren Buffett&#039;s partner, only buys wonderful businesses at fair prices
6. Michael Burry Agent - The Big Short contrarian who hunts for deep value
7. Mohnish Pabrai Agent - The Dhandho investor, who looks for doubles at low risk
8. Peter Lynch Agent - Practical investor who seeks &quot;ten-baggers&quot; in everyday businesses
9. Phil Fisher Agent - Meticulous growth investor who uses deep &quot;scuttlebutt&quot; research 
10. Rakesh Jhunjhunwala Agent - The Big Bull of India
11. Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential
12. Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price
13. Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals
14. Sentiment Agent - Analyzes market sentiment and generates trading signals
15. Fundamentals Agent - Analyzes fundamental data and generates trading signals
16. Technicals Agent - Analyzes technical indicators and generates trading signals
17. Risk Manager - Calculates risk metrics and sets position limits
18. Portfolio Manager - Makes final trading decisions and generates orders

&lt;img width=&quot;1042&quot; alt=&quot;Screenshot 2025-03-22 at 6 19 07 PM&quot; src=&quot;https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4&quot; /&gt;

Note: the system does not actually make any trades.

[![Twitter Follow](https://img.shields.io/twitter/follow/virattt?style=social)](https://twitter.com/virattt)

## Disclaimer

This project is for **educational and research purposes only**.

- Not intended for real trading or investment
- No investment advice or guarantees provided
- Creator assumes no liability for financial losses
- Consult a financial advisor for investment decisions
- Past performance does not indicate future results

By using this software, you agree to use it solely for learning purposes.

## Table of Contents
- [How to Install](#how-to-install)
- [How to Run](#how-to-run)
  - [‚å®Ô∏è Command Line Interface](#Ô∏è-command-line-interface)
  - [üñ•Ô∏è Web Application](#Ô∏è-web-application)
- [How to Contribute](#how-to-contribute)
- [Feature Requests](#feature-requests)
- [License](#license)

## How to Install

Before you can run the AI Hedge Fund, you&#039;ll need to install it and set up your API keys. These steps are common to both the full-stack web application and command line interface.

### 1. Clone the Repository

```bash
git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
```

### 2. Set up API keys

Create a `.env` file for your API keys:
```bash
# Create .env file for your API keys (in the root directory)
cp .env.example .env
```

Open and edit the `.env` file to add your API keys:
```bash
# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
OPENAI_API_KEY=your-openai-api-key

# For getting financial data to power the hedge fund
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
```

**Important**: You must set at least one LLM API key (e.g. `OPENAI_API_KEY`, `GROQ_API_KEY`, `ANTHROPIC_API_KEY`, or `DEEPSEEK_API_KEY`) for the hedge fund to work. 

**Financial Data**: Data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key. For any other ticker, you will need to set the `FINANCIAL_DATASETS_API_KEY` in the .env file.

## How to Run

### ‚å®Ô∏è Command Line Interface

You can run the AI Hedge Fund directly via terminal. This approach offers more granular control and is useful for automation, scripting, and integration purposes.

&lt;img width=&quot;992&quot; alt=&quot;Screenshot 2025-01-06 at 5 50 17 PM&quot; src=&quot;https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b&quot; /&gt;

#### Quick Start

1. Install Poetry (if not already installed):
```bash
curl -sSL https://install.python-poetry.org | python3 -
```

2. Install dependencies:
```bash
poetry install
```

#### Run the AI Hedge Fund
```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA
```

You can also specify a `--ollama` flag to run the AI hedge fund using local LLMs.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama
```

You can optionally specify the start and end dates to make decisions over a specific time period.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
```

#### Run the Backtester
```bash
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
```

**Example Output:**
&lt;img width=&quot;941&quot; alt=&quot;Screenshot 2025-01-06 at 5 47 52 PM&quot; src=&quot;https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47&quot; /&gt;


Note: The `--ollama`, `--start-date`, and `--end-date` flags work for the backtester, as well!

### üñ•Ô∏è Web Application

The new way to run the AI Hedge Fund is through our web application that provides a user-friendly interface. This is recommended for users who prefer visual interfaces over command line tools.

Please see detailed instructions on how to install and run the web application [here](https://github.com/virattt/ai-hedge-fund/tree/main/app).

&lt;img width=&quot;1721&quot; alt=&quot;Screenshot 2025-06-28 at 6 41 03‚ÄØPM&quot; src=&quot;https://github.com/user-attachments/assets/b95ab696-c9f4-416c-9ad1-51feb1f5374b&quot; /&gt;


## How to Contribute

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

**Important**: Please keep your pull requests small and focused.  This will make it easier to review and merge.

## Feature Requests

If you have a feature request, please open an [issue](https://github.com/virattt/ai-hedge-fund/issues) and make sure it is tagged with `enhancement`.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>