<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Fri, 27 Feb 2026 00:08:12 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[muratcankoylan/Agent-Skills-for-Context-Engineering]]></title>
            <link>https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering</link>
            <guid>https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering</guid>
            <pubDate>Fri, 27 Feb 2026 00:08:12 GMT</pubDate>
            <description><![CDATA[A comprehensive collection of Agent Skills for context engineering, multi-agent architectures, and production agent systems. Use when building, optimizing, or debugging agent systems that require effective context management.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering">muratcankoylan/Agent-Skills-for-Context-Engineering</a></h1>
            <p>A comprehensive collection of Agent Skills for context engineering, multi-agent architectures, and production agent systems. Use when building, optimizing, or debugging agent systems that require effective context management.</p>
            <p>Language: Python</p>
            <p>Stars: 11,693</p>
            <p>Forks: 908</p>
            <p>Stars today: 922 stars today</p>
            <h2>README</h2><pre># Agent Skills for Context Engineering

A comprehensive, open collection of Agent Skills focused on context engineering principles for building production-grade AI agent systems. These skills teach the art and science of curating context to maximize agent effectiveness across any agent platform.

## What is Context Engineering?

Context engineering is the discipline of managing the language model&#039;s context window. Unlike prompt engineering, which focuses on crafting effective instructions, context engineering addresses the holistic curation of all information that enters the model&#039;s limited attention budget: system prompts, tool definitions, retrieved documents, message history, and tool outputs.

The fundamental challenge is that context windows are constrained not by raw token capacity but by attention mechanics. As context length increases, models exhibit predictable degradation patterns: the &quot;lost-in-the-middle&quot; phenomenon, U-shaped attention curves, and attention scarcity. Effective context engineering means finding the smallest possible set of high-signal tokens that maximize the likelihood of desired outcomes.

## Recognition

This repository is cited in academic research as foundational work on static skill architecture:

&gt; &quot;While static skills are well-recognized [Anthropic, 2025b; Muratcan Koylan, 2025], MCE is among the first to dynamically evolve them, bridging manual skill engineering and autonomous self-improvement.&quot;

‚Äî [Meta Context Engineering via Agentic Skill Evolution](https://arxiv.org/pdf/2601.21557), Peking University State Key Laboratory of General Artificial Intelligence (2026)

## Skills Overview

### Foundational Skills

These skills establish the foundational understanding required for all subsequent context engineering work.

| Skill | Description |
|-------|-------------|
| [context-fundamentals](skills/context-fundamentals/) | Understand what context is, why it matters, and the anatomy of context in agent systems |
| [context-degradation](skills/context-degradation/) | Recognize patterns of context failure: lost-in-middle, poisoning, distraction, and clash |
| [context-compression](skills/context-compression/) | Design and evaluate compression strategies for long-running sessions |

### Architectural Skills

These skills cover the patterns and structures for building effective agent systems.

| Skill | Description |
|-------|-------------|
| [multi-agent-patterns](skills/multi-agent-patterns/) | Master orchestrator, peer-to-peer, and hierarchical multi-agent architectures |
| [memory-systems](skills/memory-systems/) | Design short-term, long-term, and graph-based memory architectures |
| [tool-design](skills/tool-design/) | Build tools that agents can use effectively |
| [filesystem-context](skills/filesystem-context/) | Use filesystems for dynamic context discovery, tool output offloading, and plan persistence |
| [hosted-agents](skills/hosted-agents/) | **NEW** Build background coding agents with sandboxed VMs, pre-built images, multiplayer support, and multi-client interfaces |

### Operational Skills

These skills address the ongoing operation and optimization of agent systems.

| Skill | Description |
|-------|-------------|
| [context-optimization](skills/context-optimization/) | Apply compaction, masking, and caching strategies |
| [evaluation](skills/evaluation/) | Build evaluation frameworks for agent systems |
| [advanced-evaluation](skills/advanced-evaluation/) | Master LLM-as-a-Judge techniques: direct scoring, pairwise comparison, rubric generation, and bias mitigation |

### Development Methodology

These skills cover the meta-level practices for building LLM-powered projects.

| Skill | Description |
|-------|-------------|
| [project-development](skills/project-development/) | Design and build LLM projects from ideation through deployment, including task-model fit analysis, pipeline architecture, and structured output design |

### Cognitive Architecture Skills

These skills cover formal cognitive modeling for rational agent systems.

| Skill | Description |
|-------|-------------|
| [bdi-mental-states](skills/bdi-mental-states/) | **NEW** Transform external RDF context into agent mental states (beliefs, desires, intentions) using formal BDI ontology patterns for deliberative reasoning and explainability |

## Design Philosophy

### Progressive Disclosure

Each skill is structured for efficient context use. At startup, agents load only skill names and descriptions. Full content loads only when a skill is activated for relevant tasks.

### Platform Agnosticism

These skills focus on transferable principles rather than vendor-specific implementations. The patterns work across Claude Code, Cursor, and any agent platform that supports skills or allows custom instructions.

### Conceptual Foundation with Practical Examples

Scripts and examples demonstrate concepts using Python pseudocode that works across environments without requiring specific dependency installations.

## Usage

### Usage with Claude Code

This repository is a **Claude Code Plugin Marketplace** containing context engineering skills that Claude automatically discovers and activates based on your task context.

### Installation

**Step 1: Add the Marketplace**

Run this command in Claude Code to register this repository as a plugin source:

```
/plugin marketplace add muratcankoylan/Agent-Skills-for-Context-Engineering
```

**Step 2: Browse and Install**

Option A - Browse available plugins:
1. Select `Browse and install plugins`
2. Select `context-engineering-marketplace`
3. Choose a plugin (e.g., `context-engineering-fundamentals`, `agent-architecture`)
4. Select `Install now`

Option B - Direct install via command:

```
/plugin install context-engineering-fundamentals@context-engineering-marketplace
/plugin install agent-architecture@context-engineering-marketplace
/plugin install agent-evaluation@context-engineering-marketplace
/plugin install agent-development@context-engineering-marketplace
/plugin install cognitive-architecture@context-engineering-marketplace
```

### Available Plugins

| Plugin | Skills Included |
|--------|-----------------|
| `context-engineering-fundamentals` | context-fundamentals, context-degradation, context-compression, context-optimization |
| `agent-architecture` | multi-agent-patterns, memory-systems, tool-design, filesystem-context, hosted-agents |
| `agent-evaluation` | evaluation, advanced-evaluation |
| `agent-development` | project-development |
| `cognitive-architecture` | bdi-mental-states |

### Skill Triggers

| Skill | Triggers On |
|-------|-------------|
| `context-fundamentals` | &quot;understand context&quot;, &quot;explain context windows&quot;, &quot;design agent architecture&quot; |
| `context-degradation` | &quot;diagnose context problems&quot;, &quot;fix lost-in-middle&quot;, &quot;debug agent failures&quot; |
| `context-compression` | &quot;compress context&quot;, &quot;summarize conversation&quot;, &quot;reduce token usage&quot; |
| `context-optimization` | &quot;optimize context&quot;, &quot;reduce token costs&quot;, &quot;implement KV-cache&quot; |
| `multi-agent-patterns` | &quot;design multi-agent system&quot;, &quot;implement supervisor pattern&quot; |
| `memory-systems` | &quot;implement agent memory&quot;, &quot;build knowledge graph&quot;, &quot;track entities&quot; |
| `tool-design` | &quot;design agent tools&quot;, &quot;reduce tool complexity&quot;, &quot;implement MCP tools&quot; |
| `filesystem-context` | &quot;offload context to files&quot;, &quot;dynamic context discovery&quot;, &quot;agent scratch pad&quot;, &quot;file-based context&quot; |
| `hosted-agents` | &quot;build background agent&quot;, &quot;create hosted coding agent&quot;, &quot;sandboxed execution&quot;, &quot;multiplayer agent&quot;, &quot;Modal sandboxes&quot; |
| `evaluation` | &quot;evaluate agent performance&quot;, &quot;build test framework&quot;, &quot;measure quality&quot; |
| `advanced-evaluation` | &quot;implement LLM-as-judge&quot;, &quot;compare model outputs&quot;, &quot;mitigate bias&quot; |
| `project-development` | &quot;start LLM project&quot;, &quot;design batch pipeline&quot;, &quot;evaluate task-model fit&quot; |
| `bdi-mental-states` | &quot;model agent mental states&quot;, &quot;implement BDI architecture&quot;, &quot;transform RDF to beliefs&quot;, &quot;build cognitive agent&quot; |

&lt;img width=&quot;1014&quot; height=&quot;894&quot; alt=&quot;Screenshot 2025-12-26 at 12 34 47‚ÄØPM&quot; src=&quot;https://github.com/user-attachments/assets/f79aaf03-fd2d-4c71-a630-7027adeb9bfe&quot; /&gt;

### For Cursor &amp; Codex &amp; IDE

Copy skill content into `.rules` or create project-specific Skills folders. The skills provide the context and guidelines that agent needs for effective context engineering and agent design.

### For Custom Implementations

Extract the principles and patterns from any skill and implement them in your agent framework. The skills are deliberately platform-agnostic.

## Examples

The [examples](examples/) folder contains complete system designs that demonstrate how multiple skills work together in practice.

| Example | Description | Skills Applied |
|---------|-------------|----------------|
| [digital-brain-skill](examples/digital-brain-skill/) | **NEW** Personal operating system for founders and creators. Complete Claude Code skill with 6 modules, 4 automation scripts | context-fundamentals, context-optimization, memory-systems, tool-design, multi-agent-patterns, evaluation, project-development |
| [x-to-book-system](examples/x-to-book-system/) | Multi-agent system that monitors X accounts and generates daily synthesized books | multi-agent-patterns, memory-systems, context-optimization, tool-design, evaluation |
| [llm-as-judge-skills](examples/llm-as-judge-skills/) | Production-ready LLM evaluation tools with TypeScript implementation, 19 passing tests | advanced-evaluation, tool-design, context-fundamentals, evaluation |
| [book-sft-pipeline](examples/book-sft-pipeline/) | Train models to write in any author&#039;s style. Includes Gertrude Stein case study with 70% human score on Pangram, $2 total cost | project-development, context-compression, multi-agent-patterns, evaluation |

Each example includes:
- Complete PRD with architecture decisions
- Skills mapping showing which concepts informed each decision
- Implementation guidance

### Digital Brain Skill Example

The [digital-brain-skill](examples/digital-brain-skill/) example is a complete personal operating system demonstrating comprehensive skills application:

- **Progressive Disclosure**: 3-level loading (SKILL.md ‚Üí MODULE.md ‚Üí data files)
- **Module Isolation**: 6 independent modules (identity, content, knowledge, network, operations, agents)
- **Append-Only Memory**: JSONL files with schema-first lines for agent-friendly parsing
- **Automation Scripts**: 4 consolidated tools (weekly_review, content_ideas, stale_contacts, idea_to_draft)

Includes detailed traceability in [HOW-SKILLS-BUILT-THIS.md](examples/digital-brain-skill/HOW-SKILLS-BUILT-THIS.md) mapping every architectural decision to specific skill principles.

### LLM-as-Judge Skills Example

The [llm-as-judge-skills](examples/llm-as-judge-skills/) example is a complete TypeScript implementation demonstrating:

- **Direct Scoring**: Evaluate responses against weighted criteria with rubric support
- **Pairwise Comparison**: Compare responses with position bias mitigation
- **Rubric Generation**: Create domain-specific evaluation standards
- **EvaluatorAgent**: High-level agent combining all evaluation capabilities

### Book SFT Pipeline Example

The [book-sft-pipeline](examples/book-sft-pipeline/) example demonstrates training small models (8B) to write in any author&#039;s style:

- **Intelligent Segmentation**: Two-tier chunking with overlap for maximum training examples
- **Prompt Diversity**: 15+ templates to prevent memorization and force style learning
- **Tinker Integration**: Complete LoRA training workflow with $2 total cost
- **Validation Methodology**: Modern scenario testing proves style transfer vs content memorization

Integrates with context engineering skills: project-development, context-compression, multi-agent-patterns, evaluation.

## Star History
&lt;img width=&quot;3664&quot; height=&quot;2648&quot; alt=&quot;star-history-2026224&quot; src=&quot;https://github.com/user-attachments/assets/b3bdbf23-4b6a-4774-ae85-42ef4d9b2d79&quot; /&gt;

## Structure

Each skill follows the Agent Skills specification:

```
skill-name/
‚îú‚îÄ‚îÄ SKILL.md              # Required: instructions + metadata
‚îú‚îÄ‚îÄ scripts/              # Optional: executable code demonstrating concepts
‚îî‚îÄ‚îÄ references/           # Optional: additional documentation and resources
```

See the [template](template/) folder for the canonical skill structure.

## Contributing

This repository follows the Agent Skills open development model. Contributions are welcome from the broader ecosystem. When contributing:

1. Follow the skill template structure
2. Provide clear, actionable instructions
3. Include working examples where appropriate
4. Document trade-offs and potential issues
5. Keep SKILL.md under 500 lines for optimal performance

Feel free to contact [Muratcan Koylan](https://x.com/koylanai) for collaboration opportunities or any inquiries.

## License

MIT License - see LICENSE file for details.

## References

The principles in these skills are derived from research and production experience at leading AI labs and framework developers. Each skill includes references to the underlying research and case studies that inform its recommendations.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[huggingface/skills]]></title>
            <link>https://github.com/huggingface/skills</link>
            <guid>https://github.com/huggingface/skills</guid>
            <pubDate>Fri, 27 Feb 2026 00:08:11 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/huggingface/skills">huggingface/skills</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 6,959</p>
            <p>Forks: 406</p>
            <p>Stars today: 715 stars today</p>
            <h2>README</h2><pre># Hugging Face Skills

Hugging Face Skills are definitions for AI/ML tasks like dataset creation, model training, and evaluation. They are interoperable with all major coding agent tools like OpenAI Codex, Anthropic&#039;s Claude Code, Google DeepMind&#039;s Gemini CLI, and Cursor.

The Skills in this repository follow the standardized format [Agent Skill](https://agentskills.io/home) format.

## How do Skills work?

In practice, skills are self-contained folders that package instructions, scripts, and resources together for an AI agent to use on a specific use case. Each folder includes a `SKILL.md` file with YAML frontmatter (name and description) followed by the guidance your coding agent follows while the skill is active. 

&gt; [!NOTE]
&gt; &#039;Skills&#039; is actually an Anthropic term used within Claude AI and Claude Code and not adopted by other agent tools, but we love it! OpenAI Codex uses the open [Agent Skills](https://agentskills.io/specification) format, where each skill is a directory with a `SKILL.md` file that Codex discovers from standard `.agents/skills` locations documented in the [Codex Skills guide](https://developers.openai.com/codex/skills/). Codex can also work with an `AGENTS.md` file. Google Gemini uses &#039;extensions&#039; to define the instructions for your coding agent in a `gemini-extension.json` file. **This repo is compatible with all of them, and more!**

&gt; [!TIP]
&gt; If your agent doesn&#039;t support skills, you can use [`agents/AGENTS.md`](agents/AGENTS.md) directly as a fallback.

## Installation

Hugging Face skills are compatible with Claude Code, Codex, Gemini CLI, and Cursor.

### Claude Code

1. Register the repository as a plugin marketplace:  
   
```
/plugin marketplace add huggingface/skills
```

2. To install a skill, run:  
   
```
/plugin install &lt;skill-name&gt;@huggingface/skills
```

For example:  

```
/plugin install hugging-face-cli@huggingface/skills
```

### Codex

1. Copy or symlink any skills you want to use from this repository&#039;s `skills/` directory into one of Codex&#039;s standard `.agents/skills` locations (for example, `$REPO_ROOT/.agents/skills` or `$HOME/.agents/skills`) as described in the [Codex Skills guide](https://developers.openai.com/codex/skills/).

2. Once a skill is available in one of those locations, Codex will discover it using the Agent Skills standard and load the `SKILL.md` instructions when it decides to use that skill or when you explicitly invoke it.

3. If your Codex setup still relies on `AGENTS.md`, you can use the generated [`agents/AGENTS.md`](agents/AGENTS.md) file in this repo as a fallback bundle of instructions.

### Gemini CLI

1. This repo includes `gemini-extension.json` to integrate with the Gemini CLI.

2. Install locally:  

```
gemini extensions install . --consent
```

or use the GitHub URL:

```
gemini extensions install https://github.com/huggingface/skills.git --consent
```

4. See [Gemini CLI extensions docs](https://geminicli.com/docs/extensions/#installing-an-extension) for more help.

### Cursor

This repository includes Cursor plugin manifests:

- `.cursor-plugin/plugin.json`
- `.mcp.json` (configured with the Hugging Face MCP server URL)

Install from repository URL (or local checkout) via the Cursor plugin flow.

For contributors, regenerate manifests with:

```bash
./scripts/publish.sh
```

## Skills

This repository contains a few skills to get you started. You can also contribute your own skills to the repository.

### Available skills

&lt;!-- This table is auto-generated by scripts/generate_agents.py. Do not edit manually. --&gt;
&lt;!-- BEGIN_SKILLS_TABLE --&gt;
| Name | Description | Documentation |
|------|-------------|---------------|
| `gradio` | Build Gradio web UIs and demos in Python. Use when creating or editing Gradio apps, components, event listeners, layouts, or chatbots. | [SKILL.md](skills/huggingface-gradio/SKILL.md) |
| `hugging-face-cli` | Execute Hugging Face Hub operations using the hf CLI. Download models/datasets, upload files, manage repos, and run cloud compute jobs. | [SKILL.md](skills/hugging-face-cli/SKILL.md) |
| `hugging-face-datasets` | Create and manage datasets on Hugging Face Hub. Supports initializing repos, defining configs/system prompts, streaming row updates, and SQL-based dataset querying/transformation. | [SKILL.md](skills/hugging-face-datasets/SKILL.md) |
| `hugging-face-evaluation` | Add and manage evaluation results in Hugging Face model cards. Supports extracting eval tables from README content, importing scores from Artificial Analysis API, and running custom evaluations with vLLM/lighteval. | [SKILL.md](skills/hugging-face-evaluation/SKILL.md) |
| `hugging-face-jobs` | Run compute jobs on Hugging Face infrastructure. Execute Python scripts, manage scheduled jobs, and monitor job status. | [SKILL.md](skills/hugging-face-jobs/SKILL.md) |
| `hugging-face-model-trainer` | Train or fine-tune language models using TRL on Hugging Face Jobs infrastructure. Covers SFT, DPO, GRPO and reward modeling training methods, plus GGUF conversion for local deployment. Includes hardware selection, cost estimation, Trackio monitoring, and Hub persistence. | [SKILL.md](skills/hugging-face-model-trainer/SKILL.md) |
| `hugging-face-paper-publisher` | Publish and manage research papers on Hugging Face Hub. Supports creating paper pages, linking papers to models/datasets, claiming authorship, and generating professional markdown-based research articles. | [SKILL.md](skills/hugging-face-paper-publisher/SKILL.md) |
| `hugging-face-tool-builder` | Build reusable scripts for Hugging Face API operations. Useful for chaining API calls or automating repeated tasks. | [SKILL.md](skills/hugging-face-tool-builder/SKILL.md) |
| `hugging-face-trackio` | Track and visualize ML training experiments with Trackio. Log metrics via Python API and retrieve them via CLI. Supports real-time dashboards synced to HF Spaces. | [SKILL.md](skills/hugging-face-trackio/SKILL.md) |
&lt;!-- END_SKILLS_TABLE --&gt;

### Using skills in your coding agent

Once a skill is installed, mention it directly while giving your coding agent instructions:

- &quot;Use the HF LLM trainer skill to estimate the GPU memory needed for a 70B model run.&quot;
- &quot;Use the HF model evaluation skill to launch `run_eval_job.py` on the latest checkpoint.&quot;
- &quot;Use the HF dataset creator skill to draft new few-shot classification templates.&quot;
- &quot;Use the HF paper publisher skill to index my arXiv paper and link it to my model.&quot;

Your coding agent automatically loads the corresponding `SKILL.md` instructions and helper scripts while it completes the task.

### Contribute or customize a skill

1. Copy one of the existing skill folders (for example, `hf-datasets/`) and rename it.
2. Update the new folder&#039;s `SKILL.md` frontmatter:
   ```markdown
   ---
   name: my-skill-name
   description: Describe what the skill does and when to use it
   ---

   # Skill Title
   Guidance + examples + guardrails
   ```
3. Add or edit supporting scripts, templates, and documents referenced by your instructions.
4. Add an entry to `.claude-plugin/marketplace.json` with a concise, human-readable description.
5. Run:
   ```bash
   ./scripts/publish.sh
   ```
   to regenerate and validate all generated metadata.
6. Reinstall or reload the skill bundle in your coding agent so the updated folder is available.

### Marketplace

The `.claude-plugin/marketplace.json` file lists skills with human-readable descriptions for the plugin marketplace. The CI validates that skill names and paths match between `SKILL.md` files and `marketplace.json`, but descriptions are maintained separately: `SKILL.md` descriptions guide when Claude activates the skill, while marketplace descriptions are written for humans browsing available skills.

### Additional references
- Browse the latest instructions, scripts, and templates directly at [huggingface/skills](https://github.com/huggingface/skills).
- Review Hugging Face documentation for the specific libraries or workflows you reference inside each skill.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[GVCLab/PersonaLive]]></title>
            <link>https://github.com/GVCLab/PersonaLive</link>
            <guid>https://github.com/GVCLab/PersonaLive</guid>
            <pubDate>Fri, 27 Feb 2026 00:08:10 GMT</pubDate>
            <description><![CDATA[[CVPR 2026] PersonaLive! : Expressive Portrait Image Animation for Live Streaming]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/GVCLab/PersonaLive">GVCLab/PersonaLive</a></h1>
            <p>[CVPR 2026] PersonaLive! : Expressive Portrait Image Animation for Live Streaming</p>
            <p>Language: Python</p>
            <p>Stars: 2,297</p>
            <p>Forks: 309</p>
            <p>Stars today: 67 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;assets/header.svg&quot; alt=&quot;PersonaLive&quot; width=&quot;100%&quot;&gt;

&lt;h2&gt;Expressive Portrait Image Animation for Live Streaming&lt;/h2&gt;

#### [Zhiyuan Li&lt;sup&gt;1,2,3&lt;/sup&gt;](https://huai-chang.github.io/) ¬∑ [Chi-Man Pun&lt;sup&gt;1,üì™&lt;/sup&gt;](https://cmpun.github.io/) ¬∑ [Chen Fang&lt;sup&gt;2&lt;/sup&gt;](http://fangchen.org/) ¬∑ [Jue Wang&lt;sup&gt;2&lt;/sup&gt;](https://scholar.google.com/citations?user=Bt4uDWMAAAAJ&amp;hl=en) ¬∑ [Xiaodong Cun&lt;sup&gt;3,üì™&lt;/sup&gt;](https://vinthony.github.io/academic/) 
&lt;sup&gt;1&lt;/sup&gt; University of Macau  &amp;nbsp;&amp;nbsp; &lt;sup&gt;2&lt;/sup&gt; [Dzine.ai](https://www.dzine.ai/)  &amp;nbsp;&amp;nbsp; &lt;sup&gt;3&lt;/sup&gt; [GVC Lab, Great Bay University](https://gvclab.github.io/)

&lt;a href=&#039;https://arxiv.org/abs/2512.11253&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/ArXiv-2512.11253-red&#039;&gt;&lt;/a&gt; &lt;a href=&#039;https://huggingface.co/huaichang/PersonaLive&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-ffc107&#039;&gt;&lt;/a&gt; &lt;a href=&#039;https://modelscope.cn/models/huaichang/PersonaLive&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/ModelScope-Model-624AFF&#039;&gt;&lt;/a&gt; [![GitHub](https://img.shields.io/github/stars/GVCLab/PersonaLive?style=social)](https://github.com/GVCLab/PersonaLive)

&lt;img src=&quot;assets/highlight.svg&quot; alt=&quot;highlight&quot; width=&quot;95%&quot;&gt;

&lt;img src=&quot;assets/demo_3.gif&quot; width=&quot;46%&quot;&gt; &amp;nbsp;&amp;nbsp; &lt;img src=&quot;assets/demo_2.gif&quot; width=&quot;40.5%&quot;&gt;
&lt;/div&gt;

## üìã TODO
- [ ] If you find PersonaLive useful or interesting, please give us a Starüåü! Your support drives us to keep improving.
- [ ] Fix bugs (If you encounter any issues, please feel free to open an issue or contact me! üôè)
- [x] **[2026.02.21]** ü•≥ PersonaLive is accepted by CVPR2026 üéâ.
- [x] **[2025.12.29]** üî• Enhance WebUI (Support reference image replacement).
- [x] **[2025.12.22]** üî• Supported streaming strategy in offline inference to generate long videos on 12GB VRAM!
- [x] **[2025.12.17]** üî• [ComfyUI-PersonaLive](https://github.com/okdalto/ComfyUI-PersonaLive) is now supported! (Thanks to [@okdalto](https://github.com/okdalto))
- [x] **[2025.12.15]** üî• Release `paper`!
- [x] **[2025.12.12]** üî• Release `inference code`, `config`, and `pretrained weights`!
  
## ‚öñÔ∏è Disclaimer

- [x] This project is released for **academic research only**.
- [x] Users must not use this repository to generate harmful, defamatory, or illegal content.
- [x] The authors bear no responsibility for any misuse or legal consequences arising from the use of this tool.
- [x] By using this code, you agree that you are solely responsible for any content generated.

## ‚öôÔ∏è Framework
&lt;img src=&quot;assets/overview.png&quot; alt=&quot;Image 1&quot; width=&quot;100%&quot;&gt;


We present PersonaLive, a `real-time` and `streamable` diffusion framework capable of generating `infinite-length` portrait animations.


## üöÄ Getting Started
### üõ† Installation
```
# clone this repo
git clone https://github.com/GVCLab/PersonaLive
cd PersonaLive

# Create conda environment
conda create -n personalive python=3.10
conda activate personalive

# Install packages with pip
pip install -r requirements_base.txt
```

### ‚è¨ Download weights
Option 1: Download pre-trained weights of base models and other components ([sd-image-variations-diffusers](https://huggingface.co/lambdalabs/sd-image-variations-diffusers) and [sd-vae-ft-mse](https://huggingface.co/stabilityai/sd-vae-ft-mse)). You can run the following command to download weights automatically:
    
```bash
python tools/download_weights.py
```

Option 2: Download pre-trained weights into the `./pretrained_weights` folder from one of the below URLs:
    
&lt;a href=&#039;https://drive.google.com/drive/folders/1GOhDBKIeowkMpBnKhGB8jgEhJt_--vbT?usp=drive_link&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/Google%20Drive-5B8DEF?style=for-the-badge&amp;logo=googledrive&amp;logoColor=white&#039;&gt;&lt;/a&gt; &lt;a href=&#039;https://pan.baidu.com/s/1DCv4NvUy_z7Gj2xCGqRMkQ?pwd=gj64&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/Baidu%20Netdisk-3E4A89?style=for-the-badge&amp;logo=baidu&amp;logoColor=white&#039;&gt;&lt;/a&gt; &lt;a href=&#039;https://modelscope.cn/models/huaichang/PersonaLive&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/ModelScope-624AFF?style=for-the-badge&amp;logo=alibabacloud&amp;logoColor=white&#039;&gt;&lt;/a&gt; &lt;a href=&#039;https://huggingface.co/huaichang/PersonaLive&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/HuggingFace-E67E22?style=for-the-badge&amp;logo=huggingface&amp;logoColor=white&#039;&gt;&lt;/a&gt;

Finally, these weights should be organized as follows:
```
pretrained_weights
‚îú‚îÄ‚îÄ onnx
‚îÇ   ‚îú‚îÄ‚îÄ unet_opt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ unet_opt.onnx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ unet_opt.onnx.data
‚îÇ   ‚îî‚îÄ‚îÄ unet
‚îú‚îÄ‚îÄ personalive
‚îÇ   ‚îú‚îÄ‚îÄ denoising_unet.pth
‚îÇ   ‚îú‚îÄ‚îÄ motion_encoder.pth
‚îÇ   ‚îú‚îÄ‚îÄ motion_extractor.pth
‚îÇ   ‚îú‚îÄ‚îÄ pose_guider.pth
‚îÇ   ‚îú‚îÄ‚îÄ reference_unet.pth
‚îÇ   ‚îî‚îÄ‚îÄ temporal_module.pth
‚îú‚îÄ‚îÄ sd-vae-ft-mse
‚îÇ   ‚îú‚îÄ‚îÄ diffusion_pytorch_model.bin
‚îÇ   ‚îî‚îÄ‚îÄ config.json
‚îú‚îÄ‚îÄ sd-image-variations-diffusers
‚îÇ   ‚îú‚îÄ‚îÄ image_encoder
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pytorch_model.bin
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.json
‚îÇ   ‚îú‚îÄ‚îÄ unet
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ diffusion_pytorch_model.bin
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.json
‚îÇ   ‚îî‚îÄ‚îÄ model_index.json
‚îî‚îÄ‚îÄ tensorrt
    ‚îî‚îÄ‚îÄ unet_work.engine
```

### üéûÔ∏è Offline Inference
Run offline inference with the default configuration:

```
python inference_offline.py
```

* `-L`: Max number of frames to generate. (Default: 100)
* `--use_xformers`: Enable xFormers memory efficient attention. (Default: True)
* `--stream_gen`: Enable streaming generation strategy. (Default: True)
* `--reference_image`: Path to a specific reference image. Overrides settings in config.
* `--driving_video`: Path to a specific driving video. Overrides settings in config.

‚ö†Ô∏è Note for RTX 50-Series (Blackwell) Users: xformers is not yet fully compatible with the new architecture. To avoid crashes, please disable it by running:

```
python inference_offline.py --use_xformers False
```

### üì∏ Online Inference
#### üì¶ Setup Web UI
```
# install Node.js 18+
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash
nvm install 18

source web_start.sh
```

#### üèéÔ∏è Acceleration (Optional)
Converting the model to TensorRT can significantly speed up inference (~ 2x ‚ö°Ô∏è). Building the engine may take about `20 minutes` depending on your device. Note that TensorRT optimizations may lead to slight variations or a small drop in output quality.
```
# Install packages with pip
pip install -r requirements_trt.txt

# Converting the model to TensorRT
python torch2trt.py
```
üí° **PyCUDA Installation Issues**: If you encounter a &quot;Failed to build wheel for pycuda&quot; error during the installation above, please follow these steps:
```
# Install PyCUDA manually using Conda (avoids compilation issues):
conda install -c conda-forge pycuda &quot;numpy&lt;2.0&quot;

# Open requirements_trt.txt and comment out or remove the line &quot;pycuda==2024.1.2&quot;

# Install other packages with pip
pip install -r requirements_trt.txt

# Converting the model to TensorRT
python torch2trt.py
```
‚ö†Ô∏è The provided TensorRT model is from an `H100`. We recommend `ALL users` (including H100 users) re-run `python torch2trt.py` locally to ensure best compatibility.

#### ‚ñ∂Ô∏è Start Streaming
```
python inference_online.py --acceleration none (for RTX 50-Series) or xformers or tensorrt
```
Then open `http://0.0.0.0:7860` in your browser. (*If `http://0.0.0.0:7860` does not work well, try `http://localhost:7860`)

**How to use**: Upload Image ‚û°Ô∏è Fuse Reference ‚û°Ô∏è Start Animation ‚û°Ô∏è Enjoy! üéâ
&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/guide.png&quot; alt=&quot;PersonaLive&quot; width=&quot;60%&quot;&gt;
&lt;/div&gt;

**Regarding Latency**: Latency varies depending on your device&#039;s computing power. You can try the following methods to optimize it:

1. Lower the &quot;Driving FPS&quot; setting in the WebUI to reduce the computational workload.
2. You can increase the multiplier (e.g., set to `num_frames_needed * 4` or higher) to better match your device&#039;s inference speed. https://github.com/GVCLab/PersonaLive/blob/6953d1a8b409f360a3ee1d7325093622b29f1e22/webcam/util.py#L73

## üìö Community Contribution

Special thanks to the community for providing helpful setups! ü•Ç

* **Windows + RTX 50-Series Guide**: Thanks to [@dknos](https://github.com/dknos) for providing a [detailed guide](https://github.com/GVCLab/PersonaLive/issues/10#issuecomment-3662785532) on running this project on Windows with Blackwell GPUs.

* **TensorRT on Windows**: If you are trying to convert TensorRT models on Windows, [this discussion](https://github.com/GVCLab/PersonaLive/issues/8) might be helpful. Special thanks to [@MaraScott](https://github.com/MaraScott) and [@Jeremy8776](https://github.com/Jeremy8776) for their insights.
  
* **ComfyUI**: Thanks to [@okdalto](https://github.com/okdalto) for helping implement the [ComfyUI-PersonaLive](https://github.com/okdalto/ComfyUI-PersonaLive) support.

* **Useful Scripts**: Thanks to [@suruoxi](https://github.com/suruoxi) for implementing `download_weights.py`, and to [@andchir](https://github.com/andchir) for adding audio merging functionality.

## üé¨ More Results
#### üëÄ Visualization results

&lt;table width=&quot;100%&quot;&gt;
  &lt;tr&gt;
    &lt;td width=&quot;50%&quot;&gt;
      &lt;video src=&quot;https://github.com/user-attachments/assets/cdc885ef-5e1c-4139-987a-2fa50fefd6a4&quot; controls=&quot;controls&quot; style=&quot;max-width: 100%; display: block;&quot;&gt;&lt;/video&gt;
    &lt;/td&gt;
    &lt;td width=&quot;50%&quot;&gt;
      &lt;video src=&quot;https://github.com/user-attachments/assets/014f7bae-74ce-4f56-8621-24bc76f3c123&quot; controls=&quot;controls&quot; style=&quot;max-width: 100%; display: block;&quot;&gt;&lt;/video&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;table width=&quot;100%&quot;&gt;
  &lt;tr&gt;
    &lt;td width=&quot;25%&quot;&gt;
      &lt;video src=&quot;https://github.com/user-attachments/assets/1e6a0809-15d2-4cab-ae8f-8cf1728c6281&quot; controls=&quot;controls&quot; style=&quot;max-width: 100%; display: block;&quot;&gt;&lt;/video&gt;
    &lt;/td&gt;
    &lt;td width=&quot;25%&quot;&gt;
      &lt;video src=&quot;https://github.com/user-attachments/assets/d9cf265d-9db0-4f83-81da-be967bbd5f26&quot; controls=&quot;controls&quot; style=&quot;max-width: 100%; display: block;&quot;&gt;&lt;/video&gt;
    &lt;/td&gt;
    &lt;td width=&quot;25%&quot;&gt;
      &lt;video src=&quot;https://github.com/user-attachments/assets/86235139-b63e-4f26-b09c-d218466e8e24&quot; controls=&quot;controls&quot; style=&quot;max-width: 100%; display: block;&quot;&gt;&lt;/video&gt;
    &lt;/td&gt;
    &lt;td width=&quot;25%&quot;&gt;
      &lt;video src=&quot;https://github.com/user-attachments/assets/238785de-3b4c-484e-9ad0-9d90e7962fee&quot; controls=&quot;controls&quot; style=&quot;max-width: 100%; display: block;&quot;&gt;&lt;/video&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;25%&quot;&gt;
      &lt;video src=&quot;https://github.com/user-attachments/assets/c71c4717-d528-4a98-b132-2b0ec8cec22d&quot; controls=&quot;controls&quot; style=&quot;max-width: 100%; display: block;&quot;&gt;&lt;/video&gt;
    &lt;/td&gt;
    &lt;td width=&quot;25%&quot;&gt;
      &lt;video src=&quot;https://github.com/user-attachments/assets/7e11fe71-fd16-4011-a6b2-2dbaf7e343fb&quot; controls=&quot;controls&quot; style=&quot;max-width: 100%; display: block;&quot;&gt;&lt;/video&gt;
    &lt;/td&gt;
    &lt;td width=&quot;25%&quot;&gt;
      &lt;video src=&quot;https://github.com/user-attachments/assets/f62e2162-d239-4575-9514-34575c16301c&quot; controls=&quot;controls&quot; style=&quot;max-width: 100%; display: block;&quot;&gt;&lt;/video&gt;
    &lt;/td&gt;
    &lt;td width=&quot;25%&quot;&gt;
      &lt;video src=&quot;https://github.com/user-attachments/assets/813e7fbd-37e9-47d7-a270-59887fafeca5&quot; controls=&quot;controls&quot; style=&quot;max-width: 100%; display: block;&quot;&gt;&lt;/video&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

#### ü§∫ Comparisons

&lt;table width=&quot;100%&quot;&gt;
  &lt;tr&gt;
    &lt;td width=&quot;100%&quot;&gt;
      &lt;video src=&quot;https://github.com/user-attachments/assets/36407cf9-bf82-43ff-9508-a794d223d3f7&quot; controls=&quot;controls&quot; style=&quot;max-width: 100%; display: block;&quot;&gt;&lt;/video&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;100%&quot;&gt;
      &lt;video src=&quot;https://github.com/user-attachments/assets/3be99b91-c6a1-4ca4-89e9-8fad42bb9583&quot; controls=&quot;controls&quot; style=&quot;max-width: 100%; display: block;&quot;&gt;&lt;/video&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=&quot;100%&quot;&gt;
      &lt;video src=&quot;https://github.com/user-attachments/assets/5bd21fe4-96ae-4be6-bf06-a7c476b04ec9&quot; controls=&quot;controls&quot; style=&quot;max-width: 100%; display: block;&quot;&gt;&lt;/video&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;


## ‚≠ê Citation
If you find PersonaLive useful for your research, welcome to cite our work using the following BibTeX:
```bibtex
@article{li2025personalive,
  title={PersonaLive! Expressive Portrait Image Animation for Live Streaming},
  author={Li, Zhiyuan and Pun, Chi-Man and Fang, Chen and Wang, Jue and Cun, Xiaodong},
  journal={arXiv preprint arXiv:2512.11253},
  year={2025}
}
```

## ‚ù§Ô∏è Acknowledgement
This code is mainly built upon [Moore-AnimateAnyone](https://github.com/MooreThreads/Moore-AnimateAnyone), [X-NeMo](https://byteaigc.github.io/X-Portrait2/), [StreamDiffusion](https://github.com/cumulo-autumn/StreamDiffusion), [RAIN](https://pscgylotti.github.io/pages/RAIN/) and [LivePortrait](https://github.com/KlingTeam/LivePortrait), thanks to their invaluable contributions.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[deepseek-ai/DeepSeek-V3]]></title>
            <link>https://github.com/deepseek-ai/DeepSeek-V3</link>
            <guid>https://github.com/deepseek-ai/DeepSeek-V3</guid>
            <pubDate>Fri, 27 Feb 2026 00:08:09 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/deepseek-ai/DeepSeek-V3">deepseek-ai/DeepSeek-V3</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 101,754</p>
            <p>Forks: 16,537</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre>&lt;!-- markdownlint-disable first-line-h1 --&gt;
&lt;!-- markdownlint-disable html --&gt;
&lt;!-- markdownlint-disable no-duplicate-header --&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true&quot; width=&quot;60%&quot; alt=&quot;DeepSeek-V3&quot; /&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div align=&quot;center&quot; style=&quot;line-height: 1;&quot;&gt;
  &lt;a href=&quot;https://www.deepseek.com/&quot;&gt;&lt;img alt=&quot;Homepage&quot;
    src=&quot;https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://chat.deepseek.com/&quot;&gt;&lt;img alt=&quot;Chat&quot;
    src=&quot;https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20V3-536af5?color=536af5&amp;logoColor=white&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://huggingface.co/deepseek-ai&quot;&gt;&lt;img alt=&quot;Hugging Face&quot;
    src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;logoColor=white&quot;/&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://discord.gg/Tc7c45Zzu5&quot;&gt;&lt;img alt=&quot;Discord&quot;
    src=&quot;https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;logoColor=white&amp;color=7289da&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true&quot;&gt;&lt;img alt=&quot;Wechat&quot;
    src=&quot;https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&amp;logoColor=white&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/deepseek_ai&quot;&gt;&lt;img alt=&quot;Twitter Follow&quot;
    src=&quot;https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;logoColor=white&quot;/&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE&quot;&gt;&lt;img alt=&quot;Code License&quot;
    src=&quot;https://img.shields.io/badge/Code_License-MIT-f5de53?&amp;color=f5de53&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL&quot;&gt;&lt;img alt=&quot;Model License&quot;
    src=&quot;https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&amp;color=f5de53&quot;/&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://arxiv.org/pdf/2412.19437&quot;&gt;&lt;b&gt;Paper Link&lt;/b&gt;üëÅÔ∏è&lt;/a&gt;
&lt;/div&gt;

## Table of Contents

1. [Introduction](#1-introduction)
2. [Model Summary](#2-model-summary)
3. [Model Downloads](#3-model-downloads)
4. [Evaluation Results](#4-evaluation-results)
5. [Chat Website &amp; API Platform](#5-chat-website--api-platform)
6. [How to Run Locally](#6-how-to-run-locally)
7. [License](#7-license)
8. [Citation](#8-citation)
9. [Contact](#9-contact)


## 1. Introduction

We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. 
To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. 
Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. 
We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. 
Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.
Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.
In addition, its training process is remarkably stable. 
Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. 
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;80%&quot; src=&quot;figures/benchmark.png&quot;&gt;
&lt;/p&gt;

## 2. Model Summary

---

**Architecture: Innovative Load Balancing Strategy and Training Objective**

- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.
-  We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. 
    It can also be used for speculative decoding for inference acceleration. 

---

**Pre-Training: Towards Ultimate Training Efficiency**

- We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.  
- Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.  
  This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.  
- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.

---

**Post-Training: Knowledge Distillation from DeepSeek-R1**

-   We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.

---


## 3. Model Downloads

&lt;div align=&quot;center&quot;&gt;

| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |
| :------------: | :------------: | :------------: | :------------: | :------------: |
| DeepSeek-V3-Base | 671B | 37B | 128K   | [ü§ó Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base)   |
| DeepSeek-V3   | 671B | 37B |  128K   | [ü§ó Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V3)   |

&lt;/div&gt;

&gt; [!NOTE]
&gt; The total size of DeepSeek-V3 models on Hugging Face is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.

To ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: [How_to Run_Locally](#6-how-to-run-locally).

For developers looking to dive deeper, we recommend exploring [README_WEIGHTS.md](./README_WEIGHTS.md) for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.

## 4. Evaluation Results
### Base Model
#### Standard Benchmarks

&lt;div align=&quot;center&quot;&gt;


|  | Benchmark (Metric) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |
|---|-------------------|----------|--------|-------------|---------------|---------|
| | Architecture | - | MoE | Dense | Dense | MoE |
| | # Activated Params | - | 21B | 72B | 405B | 37B |
| | # Total Params | - | 236B | 72B | 405B | 671B |
| English | Pile-test (BPB) | - | 0.606 | 0.638 | **0.542** | 0.548 |
| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | **87.5** |
| | MMLU (Acc.) | 5-shot | 78.4 | 85.0 | 84.4 | **87.1** |
| | MMLU-Redux (Acc.) | 5-shot | 75.6 | 83.2 | 81.3 | **86.2** |
| | MMLU-Pro (Acc.) | 5-shot | 51.4 | 58.3 | 52.8 | **64.4** |
| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | **89.0** |
| | ARC-Easy (Acc.) | 25-shot | 97.6 | 98.4 | 98.4 | **98.9** |
| | ARC-Challenge (Acc.) | 25-shot | 92.2 | 94.5 | **95.3** | **95.3** |
| | HellaSwag (Acc.) | 10-shot | 87.1 | 84.8 | **89.2** | 88.9 |
| | PIQA (Acc.) | 0-shot | 83.9 | 82.6 | **85.9** | 84.7 |
| | WinoGrande (Acc.) | 5-shot | **86.3** | 82.3 | 85.2 | 84.9 |
| | RACE-Middle (Acc.) | 5-shot | 73.1 | 68.1 | **74.2** | 67.1 |
| | RACE-High (Acc.) | 5-shot | 52.6 | 50.3 | **56.8** | 51.3 |
| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | 82.7 | **82.9** |
| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | **41.5** | 40.0 |
| | AGIEval (Acc.) | 0-shot | 57.5 | 75.8 | 60.6 | **79.6** |
| Code | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | **65.2** |
| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | **75.4** |
| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | **19.4** |
| | CRUXEval-I (Acc.) | 2-shot | 52.5 | 59.1 | 58.5 | **67.3** |
| | CRUXEval-O (Acc.) | 2-shot | 49.8 | 59.9 | 59.9 | **69.8** |
| Math | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | **89.3** |
| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | **61.6** |
| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | **79.8** |
| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | **90.7** |
| Chinese | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | **83.0** | 82.7 |
| | C-Eval (Acc.) | 5-shot | 81.4 | 89.2 | 72.5 | **90.1** |
| | CMMLU (Acc.) | 5-shot | 84.0 | **89.5** | 73.7 | 88.8 |
| | CMRC (EM) | 1-shot | **77.4** | 75.8 | 76.0 | 76.3 |
| | C3 (Acc.) | 0-shot | 77.4 | 76.7 | **79.7** | 78.6 |
| | CCPM (Acc.) | 0-shot | **93.0** | 88.5 | 78.6 | 92.0 |
| Multilingual | MMMLU-non-English (Acc.) | 5-shot | 64.0 | 74.8 | 73.8 | **79.4** |

&lt;/div&gt;

&gt; [!NOTE]
&gt; Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.
&gt; For more evaluation details, please check our paper. 

#### Context Window
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;80%&quot; src=&quot;figures/niah.png&quot;&gt;
&lt;/p&gt;

Evaluation results on the ``Needle In A Haystack`` (NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to **128K**. 

### Chat Model
#### Standard Benchmarks (Models larger than 67B)
&lt;div align=&quot;center&quot;&gt;

| | **Benchmark (Metric)** | **DeepSeek V2-0506** | **DeepSeek V2.5-0905** | **Qwen2.5 72B-Inst.** | **Llama3.1 405B-Inst.** | **Claude-3.5-Sonnet-1022** | **GPT-4o 0513** | **DeepSeek V3** |
|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|
| | Architecture | MoE | MoE | Dense | Dense | - | - | MoE |
| | # Activated Params | 21B | 21B | 72B | 405B | - | - | 37B |
| | # Total Params | 236B | 236B | 72B | 405B | - | - | 671B |
| English | MMLU (EM) | 78.2 | 80.6 | 85.3 | **88.6** | **88.3** | 87.2 | **88.5** |
| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | **88.9** | 88.0 | **89.1** |
| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | **78.0** | 72.6 | 75.9 |
| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | **91.6** |
| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | **86.5** | 84.3 | 86.1 |
| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | **65.0** | 49.9 | 59.1 |
| | SimpleQA (Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | **38.2** | 24.9 |
| | FRAMES (Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | **80.5** | 73.3 |
| | LongBench v2 (Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | **48.7** |
| Code | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | **82.6** |
| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | **40.5** |
| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | **37.6** |
| | Codeforces (Percentile) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | **51.6** |
| | SWE Verified (Resolved) | - | 22.6 | 23.8 | 24.5 | **50.8** | 38.8 | 42.0 |
| | Aider-Edit (Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | **84.2** | 72.9 | 79.7 |
| | Aider-Polyglot (Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | **49.6** |
| Math | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | **39.2** |
| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | **90.2** |
| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | **43.2** |
| Chinese | CLUEWSC (EM) | 89.9 | 90.4 | **91.4** | 84.7 | 85.4 | 87.9 | 90.9 |
| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | **86.5** |
| | C-SimpleQA (Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | **64.8** |

&lt;/div&gt;

&gt; [!NOTE]
&gt; All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.


####  Open Ended Generation Evaluation

&lt;div align=&quot;center&quot;&gt;



| Model | Arena-Hard | AlpacaEval 2.0 |
|-------|------------|----------------|
| DeepSeek-V2.5-0905 | 76.2 | 50.5 |
| Qwen2.5-72B-Instruct | 81.2 | 49.1 |
| LLaMA-3.1 405B | 69.3 | 40.5 |
| GPT-4o-0513 | 80.4 | 51.1 |
| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |
| DeepSeek-V3 | **85.5** | **70.0** |

&lt;/div&gt;

&gt; [!NOTE]
&gt; English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.


## 5. Chat Website &amp; API Platform
You can chat with DeepSeek-V3 on DeepSeek&#039;s official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in)

We also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)

## 6. How to Run Locally

DeepSeek-V3 can be deployed locally using the following hardware and open-source community software:

1. **DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference.
2. **SGLang**: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes, with Multi-Token Prediction [coming soon](https://github.com/sgl-project/sglang/issues/2591).
3. **LMDeploy**: Enables efficient FP8 and BF16 inference for local and cloud deployment.
4. **TensorRT-LLM**: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.
5. **vLLM**: Support DeepSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.
6. **LightLLM**: Supports efficient single-node or multi-node deployment for FP8 and BF16.
7. **AMD GPU**: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.
8. **Huawei Ascend NPU**: Supports running DeepSeek-V3 on Huawei Ascend devices in both INT8 and BF16.

Since FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.

Here is an example of converting FP8 weights to BF16:

```shell
cd inference
python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights
```

&gt; [!NOTE]
&gt; Hugging Face&#039;s Transformers has not been directly supported yet.

### 6.1 Inference with DeepSeek-Infer Demo (example only)

#### System Requirements

&gt; [!NOTE] 
&gt; Linux with Python 3.10 only. Mac and Windows are not supported.

Dependencies:
```pip-requirements
torch==2.4.1
triton==3.0.0
transformers==4.46.3
safetensors==0.4.5
```
#### Model Weights &amp; Demo Code Preparation

First, clone our DeepSeek-V3 GitHub repository:

```shell
git clone https://github.com/deepseek-ai/DeepSeek-V3.git
```

Navigate to the `inference` folder and install dependencies listed in `requirements.txt`. Easiest way is to use a package manager like `conda` or `uv` to create a new virtual environment and install the dependencies.

```shell
cd DeepSeek-V3/inference
pip install -r requirements.txt
```

Download the model weights from Hugging Face, and put them into `/path/to/DeepSeek-V3` folder.

#### Model Weights Conversion

Convert Hugging Face model weights to a specific format:

```shell
python convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16
```

#### Run

Then you can chat with DeepSeek-V3:

```shell
torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200
```

Or batch inference on a given file:

```shell
torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE
```

### 6.2 Inference with SGLang (recommended)

[SGLang](https://github.com/sgl-project/sglang) currently supports [MLA optimizations](https://lmsys.org/blog/2024-09-04-sglang-v0-3/#deepseek-multi-head-latent-attention-mla-throughput-optimizations), [DP Attention](https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models), FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.

Notably, [SGLang v0.4.1](https://github.com/sgl-project/sglang/releases/tag/v0.4.1) fully supports running DeepSeek-V3 on both **NVIDIA and AMD GPUs**, making it a highly versatile and robust solution.

SGLang also supports [multi-node tensor parallelism](https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#example-serving-with-2-h208), enabling you to run this model on multiple network-connected machines.

Multi-Token Prediction (MTP) is in development, and progress can be tracked in the [optimization plan](https://github.com/sgl-project/sglang/issues/2591).

Here are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3

### 6.3 Inference with LMDeploy (recommended)
[LMDeploy](https://github.com/InternLM/lmdeploy), a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.

For comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960


### 6.4 Inference with TRT-LLM (recommended)

[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/deepseek_v3. 


### 6.5 Inference with vLLM (recommended)

[vLLM](https://github.com/vllm-project/vllm) v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers _pipeline parallelism_ allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the [vLLM instructions](https://docs.vllm.ai/en/latest/serving/distributed_serving.html). Please feel free to follow [the enhancement plan](https://github.com/vllm-project/vllm/issues/11539) as well.

### 6.6 Inference with LightLLM (recommended)

[LightLLM](https://github.com/ModelTC/lightllm/tree/main) v1.0.1 supports single-machine and multi-machine tensor parallel deployment for DeepSeek-R1 (FP8/BF16) and provides mixed-precision deployment, with more quantization modes continuously integrated. For more details, please refer to [LightLLM instructions](https://lightllm-en.readthedocs.io/en/latest/getting_started/quickstart.html). Additionally, LightLLM offers PD-disaggregation deployment for DeepSeek-V2, and the implementation of PD-disaggregation for DeepSeek-V3 is in development.

### 6.7 Recommended Inference Functionality with AMD GPUs

In collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the [SGLang instructions](#63-inference-with-lmdeploy-recommended).

### 6.8 Recommended Inference Functionality with Huawei Ascend NPUs
The [MindIE](https://www.hiascend.com/en/software/mindie) framework from the Huawei Ascend community has successfull

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[alibaba/OpenSandbox]]></title>
            <link>https://github.com/alibaba/OpenSandbox</link>
            <guid>https://github.com/alibaba/OpenSandbox</guid>
            <pubDate>Fri, 27 Feb 2026 00:08:08 GMT</pubDate>
            <description><![CDATA[OpenSandbox is a general-purpose sandbox platform for AI applications, offering multi-language SDKs, unified sandbox APIs, and Docker/Kubernetes runtimes for scenarios like Coding Agents, GUI Agents, Agent Evaluation, AI Code Execution, and RL Training.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/alibaba/OpenSandbox">alibaba/OpenSandbox</a></h1>
            <p>OpenSandbox is a general-purpose sandbox platform for AI applications, offering multi-language SDKs, unified sandbox APIs, and Docker/Kubernetes runtimes for scenarios like Coding Agents, GUI Agents, Agent Evaluation, AI Code Execution, and RL Training.</p>
            <p>Language: Python</p>
            <p>Stars: 1,269</p>
            <p>Forks: 101</p>
            <p>Stars today: 76 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/assets/logo.svg&quot; alt=&quot;OpenSandbox logo&quot; width=&quot;150&quot; /&gt;

  &lt;h1&gt;OpenSandbox&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/alibaba/OpenSandbox&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/alibaba/OpenSandbox.svg?style=social&quot; alt=&quot;GitHub stars&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://deepwiki.com/alibaba/OpenSandbox&quot;&gt;
    &lt;img src=&quot;https://deepwiki.com/badge.svg&quot; alt=&quot;Ask DeepWiki&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/license/alibaba/OpenSandbox.svg&quot; alt=&quot;license&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://badge.fury.io/py/opensandbox&quot;&gt;
    &lt;img src=&quot;https://badge.fury.io/py/opensandbox.svg&quot; alt=&quot;PyPI version&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://badge.fury.io/js/@alibaba-group%2Fopensandbox&quot;&gt;
    &lt;img src=&quot;https://badge.fury.io/js/@alibaba-group%2Fopensandbox.svg&quot; alt=&quot;npm version&quot; /&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/alibaba/OpenSandbox/actions&quot;&gt;
    &lt;img src=&quot;https://github.com/alibaba/OpenSandbox/actions/workflows/real-e2e.yml/badge.svg?branch=main&quot; alt=&quot;E2E Status&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

  &lt;hr /&gt;
&lt;/div&gt;

English | [‰∏≠Êñá](docs/README_zh.md)

OpenSandbox is a **general-purpose sandbox platform** for AI applications, offering multi-language SDKs, unified sandbox APIs, and Docker/Kubernetes runtimes for scenarios like Coding Agents, GUI Agents, Agent Evaluation, AI Code Execution, and RL Training.

## Features

- **Multi-language SDKs**: Client SDKs for Python, Java/Kotlin, and JavaScript/TypeScript.
- **Sandbox Protocol**: Defines sandbox lifecycle management APIs and sandbox execution APIs so you can extend custom sandbox runtimes.
- **Sandbox Runtime**: Built-in lifecycle management supporting Docker and [high-performance Kubernetes runtime](./kubernetes), enabling both local runs and large-scale distributed scheduling.
- **Sandbox Environments**: Built-in Command, Filesystem, and Code Interpreter implementations. Examples cover Coding Agents (e.g., Claude Code), browser automation (Chrome, Playwright), and desktop environments (VNC, VS Code).
- **Network Policy**: Unified [Ingress Gateway](components/ingress) with multiple routing strategies plus per-sandbox [egress controls](components/egress).

## Examples

### Basic Sandbox Operations

Requirements:

- Docker (required for local execution)
- Python 3.10+ (recommended for examples and local runtime)

#### 1. Install and Configure the Sandbox Server

```bash
uv pip install opensandbox-server
opensandbox-server init-config ~/.sandbox.toml --example docker
```

&gt; If you prefer working from source, you can still clone the repo for development, but server startup no longer requires it.
&gt;
&gt; ```bash
&gt; git clone https://github.com/alibaba/OpenSandbox.git
&gt; cd OpenSandbox/server
&gt; uv sync
&gt; cp example.config.toml ~/.sandbox.toml # Copy configuration file
&gt; uv run python -m src.main # Start the service
&gt; ```

#### 2. Start the Sandbox Server

```bash
opensandbox-server

# Show help
opensandbox-server -h
```

#### 3. Create a Code Interpreter and Execute Commands

Install the Code Interpreter SDK

```bash
uv pip install opensandbox-code-interpreter
```

Create a sandbox and execute commands

```python
import asyncio
from datetime import timedelta

from code_interpreter import CodeInterpreter, SupportedLanguage
from opensandbox import Sandbox
from opensandbox.models import WriteEntry

async def main() -&gt; None:
    # 1. Create a sandbox
    sandbox = await Sandbox.create(
        &quot;opensandbox/code-interpreter:v1.0.1&quot;,
        entrypoint=[&quot;/opt/opensandbox/code-interpreter.sh&quot;],
        env={&quot;PYTHON_VERSION&quot;: &quot;3.11&quot;},
        timeout=timedelta(minutes=10),
    )

    async with sandbox:

        # 2. Execute a shell command
        execution = await sandbox.commands.run(&quot;echo &#039;Hello OpenSandbox!&#039;&quot;)
        print(execution.logs.stdout[0].text)

        # 3. Write a file
        await sandbox.files.write_files([
            WriteEntry(path=&quot;/tmp/hello.txt&quot;, data=&quot;Hello World&quot;, mode=644)
        ])

        # 4. Read a file
        content = await sandbox.files.read_file(&quot;/tmp/hello.txt&quot;)
        print(f&quot;Content: {content}&quot;) # Content: Hello World

        # 5. Create a code interpreter
        interpreter = await CodeInterpreter.create(sandbox)

        # 6. Execute Python code (single-run, pass language directly)
        result = await interpreter.codes.run(
              &quot;&quot;&quot;
                  import sys
                  print(sys.version)
                  result = 2 + 2
                  result
              &quot;&quot;&quot;,
              language=SupportedLanguage.PYTHON,
        )

        print(result.result[0].text) # 4
        print(result.logs.stdout[0].text) # 3.11.14

    # 7. Cleanup the sandbox
    await sandbox.kill()

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

### More Examples

OpenSandbox provides rich examples demonstrating sandbox usage in different scenarios. All example code is located in the `examples/` directory.

#### üéØ Basic Examples

- **[code-interpreter](examples/code-interpreter/README.md)** - End-to-end Code Interpreter SDK workflow in a sandbox.
- **[aio-sandbox](examples/aio-sandbox/README.md)** - All-in-One sandbox setup using the OpenSandbox SDK.
- **[agent-sandbox](examples/agent-sandbox/README.md)** - Run OpenSandbox on Kubernetes via [kubernetes-sigs/agent-sandbox](https://github.com/kubernetes-sigs/agent-sandbox).

#### ü§ñ Coding Agent Integrations

- **[claude-code](examples/claude-code/README.md)** - Run Claude Code inside OpenSandbox.
- **[gemini-cli](examples/gemini-cli/README.md)** - Run Google Gemini CLI inside OpenSandbox.
- **[codex-cli](examples/codex-cli/README.md)** - Run OpenAI Codex CLI inside OpenSandbox.
- **[iflow-cli](examples/iflow-cli/README.md)** - Run iFLow CLI inside OpenSandbox.
- **[langgraph](examples/langgraph/README.md)** - LangGraph state-machine workflow that creates/runs a sandbox job with fallback retry.
- **[google-adk](examples/google-adk/README.md)** - Google ADK agent using OpenSandbox tools to write/read files and run commands.
- **[openclaw](examples/openclaw/README.md)** - Launch an OpenClaw Gateway inside a sandbox.

#### üåê Browser and Desktop Environments

- **[chrome](examples/chrome/README.md)** - Headless Chromium with VNC and DevTools access for automation/debugging.
- **[playwright](examples/playwright/README.md)** - Playwright + Chromium headless scraping and testing example.
- **[desktop](examples/desktop/README.md)** - Full desktop environment in a sandbox with VNC access.
- **[vscode](examples/vscode/README.md)** - code-server (VS Code Web) running inside a sandbox for remote dev.

#### üß† ML and Training

- **[rl-training](examples/rl-training/README.md)** - DQN CartPole training in a sandbox with checkpoints and summary output.

For more details, please refer to [examples](examples/README.md) and the README files in each example directory.

## Project Structure

| Directory | Description                                                      |
|-----------|------------------------------------------------------------------|
| [`sdks/`](sdks/) | Multi-language SDKs (Python, Java/Kotlin, TypeScript/JavaScript) |
| [`specs/`](specs/README.md) | OpenAPI specs and lifecycle specifications                      |
| [`server/`](server/README.md) | Python FastAPI sandbox lifecycle server                          |
| [`kubernetes/`](kubernetes/README.md) | Kubernetes deployment and examples                               |
| [`components/execd/`](components/execd/README.md) | Sandbox execution daemon (commands and file operations)          |
| [`components/ingress/`](components/ingress/README.md) | Sandbox traffic ingress proxy                                    |
| [`components/egress/`](components/egress/README.md) | Sandbox network egress control                                   |
| [`sandboxes/`](sandboxes/) | Runtime sandbox implementations                                   |
| [`examples/`](examples/README.md) | Integration examples and use cases                               |
| [`oseps/`](oseps/README.md) | OpenSandbox Enhancement Proposals                                |
| [`docs/`](docs/) | Architecture and design documentation                            |
| [`tests/`](tests/) | Cross-component E2E tests                                        |
| [`scripts/`](scripts/) | Development and maintenance scripts                              |

For detailed architecture, see [docs/architecture.md](docs/architecture.md).

## Documentation

- [docs/architecture.md](docs/architecture.md) ‚Äì Overall architecture &amp; design philosophy
- SDK
  - Sandbox base SDK ([Java\Kotlin SDK](sdks/sandbox/kotlin/README.md), [Python SDK](sdks/sandbox/python/README.md), [JavaScript/TypeScript SDK](sdks/sandbox/javascript/README.md)) - includes sandbox lifecycle, command execution, file operations
  - Code Interpreter SDK ([Java\Kotlin SDK](sdks/code-interpreter/kotlin/README.md), [Python SDK](sdks/code-interpreter/python/README.md), [JavaScript/TypeScript SDK](sdks/code-interpreter/javascript/README.md)) - code interpreter
- [specs/README.md](specs/README.md) - OpenAPI definitions for sandbox lifecycle API and sandbox execution API
- [server/README.md](server/README.md) - Sandbox server startup and configuration; supports Docker and Kubernetes runtimes

## License

This project is open source under the [Apache 2.0 License](LICENSE).

## Roadmap

### SDK

- [ ] **Go SDK** - Go client SDK for sandbox lifecycle management, command execution, and file operations.

### Sandbox Runtime

- [ ] **Persistent storage** - Mountable persistent storage for sandboxes (see [Proposal 0003](oseps/0003-volume-and-volumebinding-support.md)).
- [ ] **Ingress multi-network strategies** - Multi-Kubernetes provisioning and multi-network modes for the Ingress Gateway.
- [ ] **Local lightweight sandbox** - Lightweight sandbox for AI tools running directly on PCs.

### Deployment

- [ ] **Kubernetes Helm** - Helm charts to deploy all components.

## Contact and Discussion

- Issues: Submit bugs, feature requests, or design discussions through GitHub Issues
## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=alibaba/OpenSandbox&amp;type=date&amp;legend=top-left)](https://www.star-history.com/#alibaba/OpenSandbox&amp;type=date&amp;legend=top-left)

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[yichuan-w/LEANN]]></title>
            <link>https://github.com/yichuan-w/LEANN</link>
            <guid>https://github.com/yichuan-w/LEANN</guid>
            <pubDate>Fri, 27 Feb 2026 00:08:07 GMT</pubDate>
            <description><![CDATA[[MLsys2026]: RAG on Everything with LEANN. Enjoy 97% storage savings while running a fast, accurate, and 100% private RAG application on your personal device.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/yichuan-w/LEANN">yichuan-w/LEANN</a></h1>
            <p>[MLsys2026]: RAG on Everything with LEANN. Enjoy 97% storage savings while running a fast, accurate, and 100% private RAG application on your personal device.</p>
            <p>Language: Python</p>
            <p>Stars: 10,107</p>
            <p>Forks: 873</p>
            <p>Stars today: 23 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/logo-text.png&quot; alt=&quot;LEANN Logo&quot; width=&quot;400&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/15049&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/15049&quot; alt=&quot;yichuan-w/LEANN | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Python-3.9%20%7C%203.10%20%7C%203.11%20%7C%203.12%20%7C%203.13-blue.svg&quot; alt=&quot;Python Versions&quot;&gt;
  &lt;img src=&quot;https://github.com/yichuan-w/LEANN/actions/workflows/build-and-publish.yml/badge.svg&quot; alt=&quot;CI Status&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Platform-Ubuntu%20%26%20Arch%20%26%20WSL%20%7C%20macOS%20(ARM64%2FIntel)-lightgrey&quot; alt=&quot;Platform&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/License-MIT-green.svg&quot; alt=&quot;MIT License&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/MCP-Native%20Integration-blue&quot; alt=&quot;MCP Integration&quot;&gt;
  &lt;a href=&quot;https://join.slack.com/t/leann-e2u9779/shared_invite/zt-3ol2ww9ic-Eg_kB8omwe6xmYVd0epr4Q&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Slack-Join-4A154B?logo=slack&amp;logoColor=white&quot; alt=&quot;Join Slack&quot;&gt;
  &lt;/a&gt;

&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://forms.gle/rDbZf864gMNxhpTq8&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/üì£_Community_Survey-Help_Shape_v0.4-007ec6?style=for-the-badge&amp;logo=google-forms&amp;logoColor=white&quot; alt=&quot;Take Survey&quot;&gt;
  &lt;/a&gt;
  &lt;p&gt;
    We track &lt;b&gt;zero telemetry&lt;/b&gt;. This survey is the ONLY way to tell us if you want &lt;br&gt;
    &lt;b&gt;GPU Acceleration&lt;/b&gt; or &lt;b&gt;More Integrations&lt;/b&gt; next.&lt;br&gt;
    üëâ &lt;a href=&quot;https://forms.gle/rDbZf864gMNxhpTq8&quot;&gt;&lt;b&gt;Click here to cast your vote (2 mins)&lt;/b&gt;&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;h3&gt;üí¨ Join our Slack community!&lt;/h3&gt;
  &lt;p&gt;
    We&#039;d love for you to be part of the LEANN community!&lt;br&gt;
    üëâ &lt;a href=&quot;https://join.slack.com/t/leann-e2u9779/shared_invite/zt-3ol2ww9ic-Eg_kB8omwe6xmYVd0epr4Q&quot;&gt;&lt;b&gt;Join LEANN Slack&lt;/b&gt;&lt;/a&gt;&lt;br&gt;
    If the invite link has expired or you have trouble joining, please &lt;a href=&quot;https://github.com/yichuan-w/LEANN/issues&quot;&gt;open an issue&lt;/a&gt; and we&#039;ll help you get in!
  &lt;/p&gt;
&lt;/div&gt;

&lt;h2 align=&quot;center&quot; tabindex=&quot;-1&quot; class=&quot;heading-element&quot; dir=&quot;auto&quot;&gt;
    The smallest vector index in the world. RAG Everything with LEANN!
&lt;/h2&gt;

LEANN is an innovative vector database that democratizes personal AI. Transform your laptop into a powerful RAG system that can index and search through millions of documents while using **97% less storage** than traditional solutions **without accuracy loss**.


LEANN achieves this through *graph-based selective recomputation* with *high-degree preserving pruning*, computing embeddings on-demand instead of storing them all. [Illustration Fig ‚Üí](#Ô∏è-architecture--how-it-works) | [Paper ‚Üí](https://arxiv.org/abs/2506.08276)

**Ready to RAG Everything?** Transform your laptop into a personal AI assistant that can semantic search your **[file system](#-personal-data-manager-process-any-documents-pdf-txt-md)**, **[emails](#-your-personal-email-secretary-rag-on-apple-mail)**, **[browser history](#-time-machine-for-the-web-rag-your-entire-browser-history)**, **[chat history](#-wechat-detective-unlock-your-golden-memories)** ([WeChat](#-wechat-detective-unlock-your-golden-memories), [iMessage](#-imessage-history-your-personal-conversation-archive)), **[agent memory](#-chatgpt-chat-history-your-personal-ai-conversation-archive)** ([ChatGPT](#-chatgpt-chat-history-your-personal-ai-conversation-archive), [Claude](#-claude-chat-history-your-personal-ai-conversation-archive)), **[live data](#mcp-integration-rag-on-live-data-from-any-platform)** ([Slack](#slack-messages-search-your-team-conversations), [Twitter](#-twitter-bookmarks-your-personal-tweet-library)), **[codebase](#-claude-code-integration-transform-your-development-workflow)**\* , or external knowledge bases (i.e., 60M documents) - all on your laptop, with zero cloud costs and complete privacy.


\* Claude Code only supports basic `grep`-style keyword search. **LEANN** is a drop-in **semantic search MCP service fully compatible with Claude Code**, unlocking intelligent retrieval without changing your workflow. üî• Check out [the easy setup ‚Üí](packages/leann-mcp/README.md)



## Why LEANN?

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/effects.png&quot; alt=&quot;LEANN vs Traditional Vector DB Storage Comparison&quot; width=&quot;70%&quot;&gt;
&lt;/p&gt;

&gt; **The numbers speak for themselves:** Index 60 million text chunks in just 6GB instead of 201GB. From emails to browser history, everything fits on your laptop. [See detailed benchmarks for different applications below ‚Üì](#-storage-comparison)


üîí **Privacy:** Your data never leaves your laptop. No OpenAI, no cloud, no &quot;terms of service&quot;.

ü™∂ **Lightweight:** Graph-based recomputation eliminates heavy embedding storage, while smart graph pruning and CSR format minimize graph storage overhead. Always less storage, less memory usage!

üì¶ **Portable:** Transfer your entire knowledge base between devices (even with others) with minimal cost - your personal AI memory travels with you.

üìà **Scalability:** Handle messy personal data that would crash traditional vector DBs, easily managing your growing personalized data and agent generated memory!

‚ú® **No Accuracy Loss:** Maintain the same search quality as heavyweight solutions while using 97% less storage.

## Installation

### üì¶ Prerequisites: Install uv

[Install uv](https://docs.astral.sh/uv/getting-started/installation/#installation-methods) first if you don&#039;t have it. Typically, you can install it with:

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### üöÄ Quick Install

Clone the repository to access all examples and try amazing applications,

```bash
git clone https://github.com/yichuan-w/LEANN.git leann
cd leann
```

and install LEANN from [PyPI](https://pypi.org/project/leann/) to run them immediately:

```bash
uv venv
source .venv/bin/activate
uv pip install leann

# CPU-only (Linux): use the `cpu` extra (e.g. `leann[cpu]`)
```

&lt;!--
&gt; Low-resource? See &quot;Low-resource setups&quot; in the [Configuration Guide](docs/configuration-guide.md#low-resource-setups). --&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;strong&gt;üîß Build from Source (Recommended for development)&lt;/strong&gt;
&lt;/summary&gt;



```bash
git clone https://github.com/yichuan-w/LEANN.git leann
cd leann
git submodule update --init --recursive
```

**macOS:**

Note: DiskANN requires MacOS 13.3 or later.

```bash
brew install libomp boost protobuf zeromq pkgconf
uv sync --extra diskann
```

**Linux (Ubuntu/Debian):**

Note: On Ubuntu 20.04, you may need to build a newer Abseil and pin Protobuf (e.g., v3.20.x) for building DiskANN. See [Issue #30](https://github.com/yichuan-w/LEANN/issues/30) for a step-by-step note.

You can manually install [Intel oneAPI MKL](https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html) instead of `libmkl-full-dev` for DiskANN. You can also use `libopenblas-dev` for building HNSW only, by removing `--extra diskann` in the command below.

```bash
sudo apt-get update &amp;&amp; sudo apt-get install -y \
  libomp-dev libboost-all-dev protobuf-compiler libzmq3-dev \
  pkg-config libabsl-dev libaio-dev libprotobuf-dev \
  libmkl-full-dev

uv sync --extra diskann
```

**Linux (Arch Linux):**

```bash
sudo pacman -Syu &amp;&amp; sudo pacman -S --needed base-devel cmake pkgconf git gcc \
  boost boost-libs protobuf abseil-cpp libaio zeromq

# For MKL in DiskANN
sudo pacman -S --needed base-devel git
git clone https://aur.archlinux.org/paru-bin.git
cd paru-bin &amp;&amp; makepkg -si
paru -S intel-oneapi-mkl intel-oneapi-compiler
source /opt/intel/oneapi/setvars.sh

uv sync --extra diskann
```

**Linux (RHEL / CentOS Stream / Oracle / Rocky / AlmaLinux):**

See [Issue #50](https://github.com/yichuan-w/LEANN/issues/50) for more details.

```bash
sudo dnf groupinstall -y &quot;Development Tools&quot;
sudo dnf install -y libomp-devel boost-devel protobuf-compiler protobuf-devel \
  abseil-cpp-devel libaio-devel zeromq-devel pkgconf-pkg-config

# For MKL in DiskANN
sudo dnf install -y intel-oneapi-mkl intel-oneapi-mkl-devel \
  intel-oneapi-openmp || sudo dnf install -y intel-oneapi-compiler
source /opt/intel/oneapi/setvars.sh

uv sync --extra diskann
```

&lt;/details&gt;


## Quick Start

Our declarative API makes RAG as easy as writing a config file.

Check out [demo.ipynb](demo.ipynb) or [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yichuan-w/LEANN/blob/main/demo.ipynb)

```python
from leann import LeannBuilder, LeannSearcher, LeannChat
from pathlib import Path
INDEX_PATH = str(Path(&quot;./&quot;).resolve() / &quot;demo.leann&quot;)

# Build an index
builder = LeannBuilder(backend_name=&quot;hnsw&quot;)
builder.add_text(&quot;LEANN saves 97% storage compared to traditional vector databases.&quot;)
builder.add_text(&quot;Tung Tung Tung Sahur called‚Äîthey need their banana‚Äëcrocodile hybrid back&quot;)
builder.build_index(INDEX_PATH)

# Search
searcher = LeannSearcher(INDEX_PATH)
results = searcher.search(&quot;fantastical AI-generated creatures&quot;, top_k=1)

# Chat with your data
chat = LeannChat(INDEX_PATH, llm_config={&quot;type&quot;: &quot;hf&quot;, &quot;model&quot;: &quot;Qwen/Qwen3-0.6B&quot;})
response = chat.ask(&quot;How much storage does LEANN save?&quot;, top_k=1)
```

## RAG on Everything!

LEANN supports RAG on various data sources including documents (`.pdf`, `.txt`, `.md`), Apple Mail, Google Search History, WeChat, ChatGPT conversations, Claude conversations, iMessage conversations, and **live data from any platform through MCP (Model Context Protocol) servers** - including Slack, Twitter, and more.



### Generation Model Setup

#### LLM Backend

LEANN supports many LLM providers for text generation (HuggingFace, Ollama, Anthropic, and Any OpenAI compatible API).


&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üîë OpenAI API Setup (Default)&lt;/strong&gt;&lt;/summary&gt;

Set your OpenAI API key as an environment variable:

```bash
export OPENAI_API_KEY=&quot;your-api-key-here&quot;
```

Make sure to use `--llm openai` flag when using the CLI.
You can also specify the model name with `--llm-model &lt;model-name&gt;` flag.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üõ†Ô∏è Supported LLM &amp; Embedding Providers (via OpenAI Compatibility)&lt;/strong&gt;&lt;/summary&gt;

Thanks to the widespread adoption of the OpenAI API format, LEANN is compatible out-of-the-box with a vast array of LLM and embedding providers. Simply set the `OPENAI_BASE_URL` and `OPENAI_API_KEY` environment variables to connect to your preferred service.

```sh
export OPENAI_API_KEY=&quot;xxx&quot;
export OPENAI_BASE_URL=&quot;http://localhost:1234/v1&quot; # base url of the provider
```

To use OpenAI compatible endpoint with the CLI interface:

If you are using it for text generation, make sure to use `--llm openai` flag and specify the model name with `--llm-model &lt;model-name&gt;` flag.

If you are using it for embedding, set the `--embedding-mode openai` flag and specify the model name with `--embedding-model &lt;MODEL&gt;`.

-----


Below is a list of base URLs for common providers to get you started.


### üñ•Ô∏è Local Inference Engines (Recommended for full privacy)

| Provider         | Sample Base URL             |
| ---------------- | --------------------------- |
| **Ollama** | `http://localhost:11434/v1` |
| **LM Studio** | `http://localhost:1234/v1`  |
| **vLLM** | `http://localhost:8000/v1`  |
| **llama.cpp** | `http://localhost:8080/v1`  |
| **SGLang** | `http://localhost:30000/v1` |
| **LiteLLM** | `http://localhost:4000`     |

-----

### ‚òÅÔ∏è Cloud Providers

&gt; **üö® A Note on Privacy:** Before choosing a cloud provider, carefully review their privacy and data retention policies. Depending on their terms, your data may be used for their own purposes, including but not limited to human reviews and model training, which can lead to serious consequences if not handled properly.


| Provider         | Base URL                                                   |
| ---------------- | ---------------------------------------------------------- |
| **OpenAI** | `https://api.openai.com/v1`                                |
| **OpenRouter** | `https://openrouter.ai/api/v1`                             |
| **Gemini** | `https://generativelanguage.googleapis.com/v1beta/openai/` |
| **x.AI (Grok)** | `https://api.x.ai/v1`                                      |
| **Groq AI** | `https://api.groq.com/openai/v1`                           |
| **DeepSeek** | `https://api.deepseek.com/v1`                              |
| **SiliconFlow** | `https://api.siliconflow.cn/v1`                            |
| **Zhipu (BigModel)** | `https://open.bigmodel.cn/api/paas/v4/`                |
| **Mistral AI** | `https://api.mistral.ai/v1`                                |
| **Anthropic** | `https://api.anthropic.com/v1`                             |
| **Jina AI** (Embeddings) | `https://api.jina.ai/v1`                         |

&gt; **üí° Tip: Separate Embedding Provider**
&gt;
&gt; To use a different provider for embeddings (e.g., Jina AI) while using another for LLM, use `--embedding-api-base` and `--embedding-api-key`:
&gt; ```bash
&gt; leann build my-index --docs ./docs \
&gt;   --embedding-mode openai \
&gt;   --embedding-model jina-embeddings-v3 \
&gt;   --embedding-api-base https://api.jina.ai/v1 \
&gt;   --embedding-api-key $JINA_API_KEY
&gt; ```

If your provider isn&#039;t on this list, don&#039;t worry! Check their documentation for an OpenAI-compatible endpoint‚Äîchances are, it&#039;s OpenAI Compatible too!

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üîß Ollama Setup (Recommended for full privacy)&lt;/strong&gt;&lt;/summary&gt;

**macOS:**

First, [download Ollama for macOS](https://ollama.com/download/mac).

```bash
# Pull a lightweight model (recommended for consumer hardware)
ollama pull llama3.2:1b
```

**Linux:**

```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama service manually
ollama serve &amp;

# Pull a lightweight model (recommended for consumer hardware)
ollama pull llama3.2:1b
```

&lt;/details&gt;


## ‚≠ê Flexible Configuration

LEANN provides flexible parameters for embedding models, search strategies, and data processing to fit your specific needs.

üìö **Need configuration best practices?** Check our [Configuration Guide](docs/configuration-guide.md) for detailed optimization tips, model selection advice, and solutions to common issues like slow embeddings or poor search quality.

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üìã Click to expand: Common Parameters (Available in All Examples)&lt;/strong&gt;&lt;/summary&gt;

All RAG examples share these common parameters. **Interactive mode** is available in all examples - simply run without `--query` to start a continuous Q&amp;A session where you can ask multiple questions. Type &#039;quit&#039; to exit.

```bash
# Environment Variables (GPU Device Selection)
LEANN_EMBEDDING_DEVICE       # GPU for embedding model (e.g., cuda:0, cuda:1, cpu)
LEANN_LLM_DEVICE             # GPU for HFChat LLM (e.g., cuda:1, or &quot;cuda&quot; for multi-GPU auto)

# Core Parameters (General preprocessing for all examples)
--index-dir DIR              # Directory to store the index (default: current directory)
--query &quot;YOUR QUESTION&quot;      # Single query mode. Omit for interactive chat (type &#039;quit&#039; to exit), and now you can play with your index interactively
--max-items N                # Limit data preprocessing (default: -1, process all data)
--force-rebuild              # Force rebuild index even if it exists

# Embedding Parameters
--embedding-model MODEL      # e.g., facebook/contriever, text-embedding-3-small, mlx-community/Qwen3-Embedding-0.6B-8bit or nomic-embed-text
--embedding-mode MODE        # sentence-transformers, openai, mlx, or ollama

# LLM Parameters (Text generation models)
--llm TYPE                   # LLM backend: openai, ollama, hf, or anthropic (default: openai)
--llm-model MODEL            # Model name (default: gpt-4o) e.g., gpt-4o-mini, llama3.2:1b, Qwen/Qwen2.5-1.5B-Instruct
--thinking-budget LEVEL      # Thinking budget for reasoning models: low/medium/high (supported by o3, o3-mini, GPT-Oss:20b, and other reasoning models)

# Search Parameters
--top-k N                    # Number of results to retrieve (default: 20)
--search-complexity N        # Search complexity for graph traversal (default: 32)

# Chunking Parameters
--chunk-size N               # Size of text chunks (default varies by source: 256 for most, 192 for WeChat)
--chunk-overlap N            # Overlap between chunks (default varies: 25-128 depending on source)

# Index Building Parameters
--backend-name NAME          # Backend to use: hnsw or diskann (default: hnsw)
--graph-degree N             # Graph degree for index construction (default: 32)
--build-complexity N         # Build complexity for index construction (default: 64)
--compact / --no-compact     # Use compact storage (default: true). Must be `no-compact` for `no-recompute` build.
--recompute / --no-recompute # Enable/disable embedding recomputation (default: enabled). Should not do a `no-recompute` search in a `recompute` build.
```

&lt;/details&gt;

### üìÑ Personal Data Manager: Process Any Documents (`.pdf`, `.txt`, `.md`)!

Ask questions directly about your personal PDFs, documents, and any directory containing your files!

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;videos/paper_clear.gif&quot; alt=&quot;LEANN Document Search Demo&quot; width=&quot;600&quot;&gt;
&lt;/p&gt;

The example below asks a question about summarizing our paper (uses default data in `data/`, which is a directory with diverse data sources: two papers, Pride and Prejudice, and a Technical report about LLM in Huawei in Chinese), and this is the **easiest example** to run here:

```bash
source .venv/bin/activate # Don&#039;t forget to activate the virtual environment
python -m apps.document_rag --query &quot;What are the main techniques LEANN explores?&quot;
```

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üìã Click to expand: Document-Specific Arguments&lt;/strong&gt;&lt;/summary&gt;

#### Parameters
```bash
--data-dir DIR           # Directory containing documents to process (default: data)
--file-types .ext .ext   # Filter by specific file types (optional - all LlamaIndex supported types if omitted)
```

#### Example Commands
```bash
# Process all documents with larger chunks for academic papers
python -m apps.document_rag --data-dir &quot;~/Documents/Papers&quot; --chunk-size 1024

# Filter only markdown and Python files with smaller chunks
python -m apps.document_rag --data-dir &quot;./docs&quot; --chunk-size 256 --file-types .md .py

# Enable AST-aware chunking for code files
python -m apps.document_rag --enable-code-chunking --data-dir &quot;./my_project&quot;

# Or use the specialized code RAG for better code understanding
python -m apps.code_rag --repo-dir &quot;./my_codebase&quot; --query &quot;How does authentication work?&quot;
```

&lt;/details&gt;

### üé® ColQwen: Multimodal PDF Retrieval with Vision-Language Models

Search through PDFs using both text and visual understanding with ColQwen2/ColPali models. Perfect for research papers, technical documents, and any PDFs with complex layouts, figures, or diagrams.

&gt; **üçé Mac Users**: ColQwen is optimized for Apple Silicon with MPS acceleration for faster inference!

```bash
# Build index from PDFs
python -m apps.colqwen_rag build --pdfs ./my_papers/ --index research_papers

# Search with text queries
python -m apps.colqwen_rag search research_papers &quot;How does attention mechanism work?&quot;

# Interactive Q&amp;A
python -m apps.colqwen_rag ask research_papers --interactive
```

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üìã Click to expand: ColQwen Setup &amp; Usage&lt;/strong&gt;&lt;/summary&gt;

#### Prerequisites
```bash
# Install dependencies
uv pip install colpali_engine pdf2image pillow matplotlib qwen_vl_utils einops seaborn
brew install poppler  # macOS only, for PDF processing
```

#### Build Index
```bash
python -m apps.colqwen_rag build \
  --pdfs ./pdf_directory/ \
  --index my_index \
  --model colqwen2  # or colpali
```

#### Search
```bash
python -m apps.colqwen_rag search my_index &quot;your question here&quot; --top-k 5
```

#### Models
- **ColQwen2** (`colqwen2`): Latest vision-language model with improved performance
- **ColPali** (`colpali`): Proven multimodal retriever

For detailed usage, see the [ColQwen Guide](docs/COLQWEN_

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[anthropics/skills]]></title>
            <link>https://github.com/anthropics/skills</link>
            <guid>https://github.com/anthropics/skills</guid>
            <pubDate>Fri, 27 Feb 2026 00:08:06 GMT</pubDate>
            <description><![CDATA[Public repository for Agent Skills]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/anthropics/skills">anthropics/skills</a></h1>
            <p>Public repository for Agent Skills</p>
            <p>Language: Python</p>
            <p>Stars: 77,275</p>
            <p>Forks: 8,040</p>
            <p>Stars today: 1,208 stars today</p>
            <h2>README</h2><pre>&gt; **Note:** This repository contains Anthropic&#039;s implementation of skills for Claude. For information about the Agent Skills standard, see [agentskills.io](http://agentskills.io).

# Skills
Skills are folders of instructions, scripts, and resources that Claude loads dynamically to improve performance on specialized tasks. Skills teach Claude how to complete specific tasks in a repeatable way, whether that&#039;s creating documents with your company&#039;s brand guidelines, analyzing data using your organization&#039;s specific workflows, or automating personal tasks.

For more information, check out:
- [What are skills?](https://support.claude.com/en/articles/12512176-what-are-skills)
- [Using skills in Claude](https://support.claude.com/en/articles/12512180-using-skills-in-claude)
- [How to create custom skills](https://support.claude.com/en/articles/12512198-creating-custom-skills)
- [Equipping agents for the real world with Agent Skills](https://anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills)

# About This Repository

This repository contains skills that demonstrate what&#039;s possible with Claude&#039;s skills system. These skills range from creative applications (art, music, design) to technical tasks (testing web apps, MCP server generation) to enterprise workflows (communications, branding, etc.).

Each skill is self-contained in its own folder with a `SKILL.md` file containing the instructions and metadata that Claude uses. Browse through these skills to get inspiration for your own skills or to understand different patterns and approaches.

Many skills in this repo are open source (Apache 2.0). We&#039;ve also included the document creation &amp; editing skills that power [Claude&#039;s document capabilities](https://www.anthropic.com/news/create-files) under the hood in the [`skills/docx`](./skills/docx), [`skills/pdf`](./skills/pdf), [`skills/pptx`](./skills/pptx), and [`skills/xlsx`](./skills/xlsx) subfolders. These are source-available, not open source, but we wanted to share these with developers as a reference for more complex skills that are actively used in a production AI application.

## Disclaimer

**These skills are provided for demonstration and educational purposes only.** While some of these capabilities may be available in Claude, the implementations and behaviors you receive from Claude may differ from what is shown in these skills. These skills are meant to illustrate patterns and possibilities. Always test skills thoroughly in your own environment before relying on them for critical tasks.

# Skill Sets
- [./skills](./skills): Skill examples for Creative &amp; Design, Development &amp; Technical, Enterprise &amp; Communication, and Document Skills
- [./spec](./spec): The Agent Skills specification
- [./template](./template): Skill template

# Try in Claude Code, Claude.ai, and the API

## Claude Code
You can register this repository as a Claude Code Plugin marketplace by running the following command in Claude Code:
```
/plugin marketplace add anthropics/skills
```

Then, to install a specific set of skills:
1. Select `Browse and install plugins`
2. Select `anthropic-agent-skills`
3. Select `document-skills` or `example-skills`
4. Select `Install now`

Alternatively, directly install either Plugin via:
```
/plugin install document-skills@anthropic-agent-skills
/plugin install example-skills@anthropic-agent-skills
```

After installing the plugin, you can use the skill by just mentioning it. For instance, if you install the `document-skills` plugin from the marketplace, you can ask Claude Code to do something like: &quot;Use the PDF skill to extract the form fields from `path/to/some-file.pdf`&quot;

## Claude.ai

These example skills are all already available to paid plans in Claude.ai. 

To use any skill from this repository or upload custom skills, follow the instructions in [Using skills in Claude](https://support.claude.com/en/articles/12512180-using-skills-in-claude#h_a4222fa77b).

## Claude API

You can use Anthropic&#039;s pre-built skills, and upload custom skills, via the Claude API. See the [Skills API Quickstart](https://docs.claude.com/en/api/skills-guide#creating-a-skill) for more.

# Creating a Basic Skill

Skills are simple to create - just a folder with a `SKILL.md` file containing YAML frontmatter and instructions. You can use the **template-skill** in this repository as a starting point:

```markdown
---
name: my-skill-name
description: A clear description of what this skill does and when to use it
---

# My Skill Name

[Add your instructions here that Claude will follow when this skill is active]

## Examples
- Example usage 1
- Example usage 2

## Guidelines
- Guideline 1
- Guideline 2
```

The frontmatter requires only two fields:
- `name` - A unique identifier for your skill (lowercase, hyphens for spaces)
- `description` - A complete description of what the skill does and when to use it

The markdown content below contains the instructions, examples, and guidelines that Claude will follow. For more details, see [How to create custom skills](https://support.claude.com/en/articles/12512198-creating-custom-skills).

# Partner Skills

Skills are a great way to teach Claude how to get better at using specific pieces of software. As we see awesome example skills from partners, we may highlight some of them here:

- **Notion** - [Notion Skills for Claude](https://www.notion.so/notiondevs/Notion-Skills-for-Claude-28da4445d27180c7af1df7d8615723d0)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[lintsinghua/DeepAudit]]></title>
            <link>https://github.com/lintsinghua/DeepAudit</link>
            <guid>https://github.com/lintsinghua/DeepAudit</guid>
            <pubDate>Fri, 27 Feb 2026 00:08:05 GMT</pubDate>
            <description><![CDATA[DeepAuditÔºö‰∫∫‰∫∫Êã•ÊúâÁöÑ AI ÈªëÂÆ¢ÊàòÈòüÔºåËÆ©ÊºèÊ¥ûÊåñÊéòËß¶ÊâãÂèØÂèä„ÄÇÂõΩÂÜÖÈ¶ñ‰∏™ÂºÄÊ∫êÁöÑ‰ª£Á†ÅÊºèÊ¥ûÊåñÊéòÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªü„ÄÇÂ∞èÁôΩ‰∏ÄÈîÆÈÉ®ÁΩ≤ËøêË°åÔºåËá™‰∏ªÂçè‰ΩúÂÆ°ËÆ° + Ëá™Âä®ÂåñÊ≤ôÁÆ± PoC È™åËØÅ„ÄÇÊîØÊåÅ Ollama ÁßÅÊúâÈÉ®ÁΩ≤ Ôºå‰∏ÄÈîÆÁîüÊàêÊä•Âëä„ÄÇÊîØÊåÅ‰∏≠ËΩ¨Á´ô„ÄÇ‚ÄãËÆ©ÂÆâÂÖ®‰∏çÂÜçÊòÇË¥µÔºåËÆ©ÂÆ°ËÆ°‰∏çÂÜçÂ§çÊùÇ„ÄÇ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lintsinghua/DeepAudit">lintsinghua/DeepAudit</a></h1>
            <p>DeepAuditÔºö‰∫∫‰∫∫Êã•ÊúâÁöÑ AI ÈªëÂÆ¢ÊàòÈòüÔºåËÆ©ÊºèÊ¥ûÊåñÊéòËß¶ÊâãÂèØÂèä„ÄÇÂõΩÂÜÖÈ¶ñ‰∏™ÂºÄÊ∫êÁöÑ‰ª£Á†ÅÊºèÊ¥ûÊåñÊéòÂ§öÊô∫ËÉΩ‰ΩìÁ≥ªÁªü„ÄÇÂ∞èÁôΩ‰∏ÄÈîÆÈÉ®ÁΩ≤ËøêË°åÔºåËá™‰∏ªÂçè‰ΩúÂÆ°ËÆ° + Ëá™Âä®ÂåñÊ≤ôÁÆ± PoC È™åËØÅ„ÄÇÊîØÊåÅ Ollama ÁßÅÊúâÈÉ®ÁΩ≤ Ôºå‰∏ÄÈîÆÁîüÊàêÊä•Âëä„ÄÇÊîØÊåÅ‰∏≠ËΩ¨Á´ô„ÄÇ‚ÄãËÆ©ÂÆâÂÖ®‰∏çÂÜçÊòÇË¥µÔºåËÆ©ÂÆ°ËÆ°‰∏çÂÜçÂ§çÊùÇ„ÄÇ</p>
            <p>Language: Python</p>
            <p>Stars: 4,802</p>
            <p>Forks: 570</p>
            <p>Stars today: 35 stars today</p>
            <h2>README</h2><pre># XCodeReviewer - ÊÇ®ÁöÑÊô∫ËÉΩ‰ª£Á†ÅÂÆ°ËÆ°‰ºô‰º¥ üöÄ

&lt;div style=&quot;width: 100%; max-width: 600px; margin: 0 auto;&quot;&gt;
  &lt;img src=&quot;public/images/logo.png&quot; alt=&quot;XCodeReviewer Logo&quot; style=&quot;width: 100%; height: auto; display: block; margin: 0 auto;&quot;&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
    &lt;a href=&quot;README.md&quot;&gt;‰∏≠Êñá&lt;/a&gt; ‚Ä¢
    &lt;a href=&quot;README_EN.md&quot;&gt;English&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

[![Version](https://img.shields.io/badge/version-1.2.0-blue.svg)](https://github.com/lintsinghua/XCodeReviewer/releases)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![React](https://img.shields.io/badge/React-18-61dafb.svg)](https://reactjs.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.7-3178c6.svg)](https://www.typescriptlang.org/)
[![Vite](https://img.shields.io/badge/Vite-5.1-646cff.svg)](https://vitejs.dev/)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/lintsinghua/XCodeReviewer)

[![Stars](https://img.shields.io/github/stars/lintsinghua/XCodeReviewer?style=social)](https://github.com/lintsinghua/XCodeReviewer/stargazers)
[![Forks](https://img.shields.io/github/forks/lintsinghua/XCodeReviewer?style=social)](https://github.com/lintsinghua/XCodeReviewer/network/members)

[![Sponsor](https://img.shields.io/badge/Sponsor-ËµûÂä©-blueviolet)](https://github.com/lintsinghua/lintsinghua.github.io/issues/1)
&lt;/div&gt;

&lt;div style=&quot;width: 100%; max-width: 600px; margin: 0 auto;&quot;&gt;
  &lt;a href=&quot;https://github.com/lintsinghua/XCodeReviewer&quot;&gt;
    &lt;img src=&quot;public/star-me-cn.svg&quot; alt=&quot;Star this project&quot; style=&quot;width: 100%; height: auto; display: block; margin: 0 auto;&quot; /&gt;
  &lt;/a&gt;
&lt;/div&gt;

**XCodeReviewer** ÊòØ‰∏Ä‰∏™Áî±Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMÔºâÈ©±Âä®ÁöÑÁé∞‰ª£Âåñ‰ª£Á†ÅÂÆ°ËÆ°Âπ≥Âè∞ÔºåÊó®Âú®‰∏∫ÂºÄÂèëËÄÖÊèê‰æõÊô∫ËÉΩ„ÄÅÂÖ®Èù¢‰∏îÊûÅÂÖ∑Ê∑±Â∫¶ÁöÑ‰ª£Á†ÅË¥®ÈáèÂàÜÊûêÂíåÂÆ°Êü•ÊúçÂä°„ÄÇ

#### üåê Âú®Á∫øÊºîÁ§∫

Êó†ÈúÄÈÉ®ÁΩ≤ÔºåÁõ¥Êé•ËÆøÈóÆÂú®Á∫øÊºîÁ§∫ÔºàÊï∞ÊçÆÂ≠òÂÇ®Âú®ÊµèËßàÂô®Êú¨Âú∞ÔºåÊîØÊåÅÊâÄÊúâÊ†∏ÂøÉÂäüËÉΩÔºâÔºö

**[https://xcodereviewer-preview.vercel.app](https://xcodereviewer-preview.vercel.app)**

## üåü ‰∏∫‰ªÄ‰πàÈÄâÊã© XCodeReviewerÔºü

Âú®Âø´ËäÇÂ•èÁöÑËΩØ‰ª∂ÂºÄÂèë‰∏≠Ôºå‰øùËØÅ‰ª£Á†ÅË¥®ÈáèËá≥ÂÖ≥ÈáçË¶Å„ÄÇ‰º†Áªü‰ª£Á†ÅÂÆ°ËÆ°Â∑•ÂÖ∑ËßÑÂàôÊ≠ªÊùø„ÄÅÊïàÁéá‰Ωé‰∏ãÔºåËÄå‰∫∫Â∑•ÂÆ°ËÆ°ÂàôËÄóÊó∂ËÄóÂäõ„ÄÇXCodeReviewer ÂÄüÂä© LLM ÁöÑÂº∫Â§ßËÉΩÂäõÔºåÂΩªÂ∫ïÊîπÂèò‰∫Ü‰ª£Á†ÅÂÆ°Êü•ÁöÑÊñπÂºèÔºö

![Á≥ªÁªüÊû∂ÊûÑÂõæ](public/diagram.svg)

&lt;div div align=&quot;center&quot;&gt;
  &lt;em&gt;
    XCodeReviewerÁ≥ªÁªüÊû∂ÊûÑÂõæ
  &lt;/em&gt;
&lt;/div&gt;

---

- **AI È©±Âä®ÁöÑÊ∑±Â∫¶ÂàÜÊûê**ÔºöË∂ÖË∂ä‰º†ÁªüÈùôÊÄÅÂàÜÊûêÔºåÁêÜËß£‰ª£Á†ÅÊÑèÂõæÔºåÂèëÁé∞Ê∑±Â±ÇÈÄªËæëÈóÆÈ¢ò„ÄÇ
- **Â§öÁª¥Â∫¶„ÄÅÂÖ®Êñπ‰ΩçËØÑ‰º∞**Ôºö‰ªé**ÂÆâÂÖ®ÊÄß**„ÄÅ**ÊÄßËÉΩ**„ÄÅ**ÂèØÁª¥Êä§ÊÄß**Âà∞**‰ª£Á†ÅÈ£éÊ†º**ÔºåÊèê‰æõ 360 Â∫¶Êó†Ê≠ªËßíÁöÑË¥®ÈáèËØÑ‰º∞„ÄÇ
- **Ê∏ÖÊô∞„ÄÅÂèØË°åÁöÑ‰øÆÂ§çÂª∫ËÆÆ**ÔºöÁã¨Âàõ **What-Why-How** Ê®°ÂºèÔºå‰∏ç‰ªÖÂëäËØâÊÇ®&quot;ÊòØ‰ªÄ‰πà&quot;ÈóÆÈ¢òÔºåËøòËß£Èáä&quot;‰∏∫‰ªÄ‰πà&quot;ÔºåÂπ∂Êèê‰æõ&quot;Â¶Ç‰Ωï‰øÆÂ§ç&quot;ÁöÑÂÖ∑‰Ωì‰ª£Á†ÅÁ§∫‰æã„ÄÇ
- **Â§öÂπ≥Âè∞LLM/Êú¨Âú∞LLMÊîØÊåÅ**: Â∑≤ÂÆûÁé∞ 10+ ‰∏ªÊµÅÂπ≥Âè∞APIË∞ÉÁî®ÂäüËÉΩÔºàGemini„ÄÅOpenAI„ÄÅClaude„ÄÅÈÄö‰πâÂçÉÈóÆ„ÄÅDeepSeek„ÄÅÊô∫Ë∞±AI„ÄÅKimi„ÄÅÊñáÂøÉ‰∏ÄË®Ä„ÄÅMiniMax„ÄÅË±ÜÂåÖ„ÄÅOllamaÊú¨Âú∞Â§ßÊ®°ÂûãÔºâÔºåÊîØÊåÅÁî®Êà∑Ëá™Áî±ÈÖçÁΩÆÂíåÂàáÊç¢„ÄÇ
- **ÂèØËßÜÂåñËøêË°åÊó∂ÈÖçÁΩÆ**ÔºöÊó†ÈúÄÈáçÊñ∞ÊûÑÂª∫ÈïúÂÉèÔºåÁõ¥Êé•Âú®ÊµèËßàÂô®‰∏≠ÈÖçÁΩÆÊâÄÊúâ LLM ÂèÇÊï∞Âíå API KeysÔºåÊîØÊåÅ API ‰∏≠ËΩ¨Á´ôÔºåÈÖçÁΩÆ‰øùÂ≠òÂú®Êú¨Âú∞ÊµèËßàÂô®ÔºåÂÆâÂÖ®‰æøÊç∑„ÄÇ
- **Áé∞‰ª£Âåñ„ÄÅÈ´òÈ¢úÂÄºÁöÑÁî®Êà∑ÁïåÈù¢**ÔºöÂü∫‰∫é React + TypeScript ÊûÑÂª∫ÔºåÊèê‰æõÊµÅÁïÖ„ÄÅÁõ¥ËßÇÁöÑÊìç‰Ωú‰ΩìÈ™å„ÄÇ

## üé¨ È°πÁõÆÊºîÁ§∫

### ‰∏ªË¶ÅÂäüËÉΩÁïåÈù¢

#### Êô∫ËÉΩ‰ª™Ë°®Áõò
![Êô∫ËÉΩ‰ª™Ë°®Áõò](public/images/example1.png)
*ÂÆûÊó∂Â±ïÁ§∫È°πÁõÆÁªüËÆ°„ÄÅË¥®ÈáèË∂ãÂäøÂíåÁ≥ªÁªüÊÄßËÉΩÔºåÊèê‰æõÂÖ®Èù¢ÁöÑ‰ª£Á†ÅÂÆ°ËÆ°Ê¶ÇËßà*

#### Âç≥Êó∂ÂàÜÊûê
![Âç≥Êó∂ÂàÜÊûê](public/images/example2.png)
*ÊîØÊåÅ‰ª£Á†ÅÁâáÊÆµÂø´ÈÄüÂàÜÊûêÔºåÊèê‰æõËØ¶ÁªÜÁöÑ What-Why-How Ëß£ÈáäÂíå‰øÆÂ§çÂª∫ËÆÆ*

#### È°πÁõÆÁÆ°ÁêÜ
![È°πÁõÆÁÆ°ÁêÜ](public/images/example3.png)
*ÈõÜÊàê GitHub/GitLab ‰ªìÂ∫ìÔºåÊîØÊåÅÂ§öËØ≠Ë®ÄÈ°πÁõÆÂÆ°ËÆ°ÂíåÊâπÈáè‰ª£Á†ÅÂàÜÊûê*

## üöÄ Âø´ÈÄüÂºÄÂßã

### ‚òÅÔ∏è Vercel ‰∏ÄÈîÆÈÉ®ÁΩ≤

ÈÄÇÂêàÂø´ÈÄüÈÉ®ÁΩ≤Âíå‰ΩìÈ™åÔºåÊó†ÈúÄÊúçÂä°Âô®ÔºåÂÖ®ÁêÉ CDN Âä†ÈÄü„ÄÇ

#### ÊñπÂºè‰∏ÄÔºö‰∏ÄÈîÆÈÉ®ÁΩ≤ÊåâÈíÆÔºàÊé®ËçêÔºâ‚≠ê

ÁÇπÂáª‰∏ãÊñπÊåâÈíÆÁõ¥Êé•ÈÉ®ÁΩ≤Âà∞ VercelÔºö

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/lintsinghua/XCodeReviewer)

#### ÊñπÂºè‰∫åÔºöÈÄöËøá Vercel CLI ÈÉ®ÁΩ≤

```bash
# 1. ÂÆâË£Ö Vercel CLI
npm i -g vercel

# 2. ÁôªÂΩï Vercel
vercel login

# 3. ÈÉ®ÁΩ≤È°πÁõÆ
vercel

# 4. ÈÉ®ÁΩ≤Âà∞Áîü‰∫ßÁéØÂ¢É
vercel --prod
```

#### ÊñπÂºè‰∏âÔºöÈÄöËøá Vercel Dashboard ÈÉ®ÁΩ≤

1. ËÆøÈóÆ [Vercel Dashboard](https://vercel.com/dashboard)
2. ÁÇπÂáª &quot;Add New...&quot; ‚Üí &quot;Project&quot;
3. ÂØºÂÖ•‰Ω†ÁöÑ GitHub ‰ªìÂ∫ì
4. Vercel ‰ºöËá™Âä®Ê£ÄÊµã Vite È°πÁõÆÈÖçÁΩÆ
5. ÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáèÔºàËá≥Â∞ëÈúÄË¶ÅÔºâÔºö
   ```
   VITE_LLM_PROVIDER=your_llm_provider
   VITE_LLM_API_KEY=your_api_key_here
   VITE_USE_LOCAL_DB=true
   ```
6. ÁÇπÂáª &quot;Deploy&quot;

**‚ú® Vercel ÈÉ®ÁΩ≤‰ºòÂäø**Ôºö
- ‚úÖ ÂÖ®ÁêÉ CDN Âä†ÈÄüÔºåËÆøÈóÆÈÄüÂ∫¶Âø´
- ‚úÖ Ëá™Âä® HTTPS ÂíåÂüüÂêçÈÖçÁΩÆ
- ‚úÖ Èõ∂ÈÖçÁΩÆÔºåÂºÄÁÆ±Âç≥Áî®
- ‚úÖ ÊîØÊåÅËá™ÂÆö‰πâÂüüÂêç
- ‚úÖ Ëá™Âä®ÈÉ®ÁΩ≤ÔºàGit Êé®ÈÄÅÂêéËá™Âä®Êõ¥Êñ∞Ôºâ

**‚ú® Êï∞ÊçÆÂ∫ìÊ®°Âºè**Ôºö
- ÈªòËÆ§Ëá™Âä®‰ΩøÁî®**Êú¨Âú∞Êï∞ÊçÆÂ∫ìÊ®°Âºè**ÔºàIndexedDBÔºâÔºåÊï∞ÊçÆÂ≠òÂÇ®Âú®ÊµèËßàÂô®‰∏≠
- Êó†ÈúÄÈÖçÁΩÆ‰ªª‰ΩïÊï∞ÊçÆÂ∫ìÔºåÂºÄÁÆ±Âç≥Áî®
- Â¶ÇÈúÄ‰ΩøÁî® Supabase ‰∫ëÁ´ØÊï∞ÊçÆÂ∫ìÔºåÂèØÂú®ÁéØÂ¢ÉÂèòÈáè‰∏≠ÈÖçÁΩÆ

**‚ö†Ô∏è Ê≥®ÊÑè‰∫ãÈ°π**Ôºö
- Vercel ‰∏ªË¶ÅÁî®‰∫éÂâçÁ´ØÈÉ®ÁΩ≤ÔºåÂêéÁ´Ø API ÈúÄÂçïÁã¨ÈÉ®ÁΩ≤
- ÈÉ®ÁΩ≤ÂêéÂèØÂú® `/admin` È°µÈù¢ËøõË°åËøêË°åÊó∂ÈÖçÁΩÆ

---

### üê≥ Docker ÈÉ®ÁΩ≤ÔºàÊé®ËçêÁîü‰∫ßÁéØÂ¢ÉÔºâ

#### ÊñπÂºè‰∏ÄÔºö‰ΩøÁî®ÂèëÂ∏ÉÁöÑÈïúÂÉèÔºàÊúÄÁÆÄÂçïÔºâ‚≠ê

Áõ¥Êé•‰ΩøÁî®ÊúÄÊñ∞ÂèëÂ∏ÉÁöÑ Docker ÈïúÂÉèÔºåÊîØÊåÅ x86„ÄÅARM64ÔºàMac MÁ≥ªÂàóÔºâ„ÄÅARMv7 Êû∂ÊûÑÔºö

```bash
# 1. ÊãâÂèñÊúÄÊñ∞ÁâàÊú¨ÈïúÂÉè
docker pull ghcr.io/lintsinghua/xcodereviewer:latest

# 2. ËøêË°åÂÆπÂô®
docker run -d \
  -p 8888:80 \
  --name xcodereviewer \
  --restart unless-stopped \
  ghcr.io/lintsinghua/xcodereviewer:latest

# 3. ËÆøÈóÆÂ∫îÁî®
# ÊµèËßàÂô®ÊâìÂºÄ http://localhost:8888
```

**‰ΩøÁî®ÁâπÂÆöÁâàÊú¨**Ôºö
```bash
# ÊãâÂèñÊåáÂÆöÁâàÊú¨ÔºàÂ¶Ç v1.1.0Ôºâ
docker pull ghcr.io/lintsinghua/xcodereviewer:v1.1.0

# ËøêË°å
docker run -d -p 8888:80 --name xcodereviewer ghcr.io/lintsinghua/xcodereviewer:v1.1.0
```

#### ÊñπÂºè‰∫åÔºöÊú¨Âú∞ÊûÑÂª∫ÔºàÂèØÈÄâÔºâ

Â¶ÇÊûúÈúÄË¶ÅËá™ÂÆö‰πâÊûÑÂª∫Ôºö

```bash
# 1. ÂÖãÈöÜÈ°πÁõÆ
git clone https://github.com/lintsinghua/XCodeReviewer.git
cd XCodeReviewer

# 2. ‰ΩøÁî® Docker Compose ÊûÑÂª∫Âπ∂ÂêØÂä®
docker-compose up -d

# 3. ËÆøÈóÆÂ∫îÁî®
# ÊµèËßàÂô®ÊâìÂºÄ http://localhost:8888
```

**‚ú® ËøêË°åÊó∂ÈÖçÁΩÆÔºàÊé®ËçêÔºâ**

Docker ÈÉ®ÁΩ≤ÂêéÔºåÊÇ®ÂèØ‰ª•Áõ¥Êé•Âú®ÊµèËßàÂô®‰∏≠ÈÖçÁΩÆÊâÄÊúâËÆæÁΩÆÔºåÊó†ÈúÄÈáçÊñ∞ÊûÑÂª∫ÈïúÂÉèÔºö

1. ËÆøÈóÆ `http://localhost:8888/admin`ÔºàÁ≥ªÁªüÁÆ°ÁêÜÈ°µÈù¢Ôºâ
2. Âú®&quot;Á≥ªÁªüÈÖçÁΩÆ&quot;Ê†áÁ≠æÈ°µ‰∏≠ÈÖçÁΩÆ LLM API Keys ÂíåÂÖ∂‰ªñÂèÇÊï∞
3. ÁÇπÂáª‰øùÂ≠òÂπ∂Âà∑Êñ∞È°µÈù¢Âç≥ÂèØ‰ΩøÁî®

&gt; üìñ **ËØ¶ÁªÜÈÖçÁΩÆËØ¥ÊòéËØ∑ÂèÇËÄÉ**Ôºö[Á≥ªÁªüÈÖçÁΩÆ‰ΩøÁî®ÊåáÂçó](#Á≥ªÁªüÈÖçÁΩÆÈ¶ñÊ¨°‰ΩøÁî®ÂøÖÁúã)

### üíª Êú¨Âú∞ÂºÄÂèëÈÉ®ÁΩ≤

ÈÄÇÂêàÈúÄË¶ÅÂºÄÂèëÊàñËá™ÂÆö‰πâ‰øÆÊîπÁöÑÂú∫ÊôØ„ÄÇ

#### ÁéØÂ¢ÉË¶ÅÊ±Ç
- Node.js 18+
- pnpm 8+ (Êé®Ëçê) Êàñ npm/yarn

#### Âø´ÈÄüÂêØÂä®

```bash
# 1. ÂÖãÈöÜÈ°πÁõÆ
git clone https://github.com/lintsinghua/XCodeReviewer.git
cd XCodeReviewer

# 2. ÂÆâË£Ö‰æùËµñ
pnpm install  # Êàñ npm install / yarn install

# 3. ÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáè
cp .env.example .env
# ÁºñËæë .env Êñá‰ª∂ÔºåÈÖçÁΩÆÂøÖË¶ÅÂèÇÊï∞ÔºàËßÅ‰∏ãÊñπÈÖçÁΩÆËØ¥ÊòéÔºâ

# 4. ÂêØÂä®ÂºÄÂèëÊúçÂä°Âô®
pnpm dev

# 5. ËÆøÈóÆÂ∫îÁî®
# ÊµèËßàÂô®ÊâìÂºÄ http://localhost:5173
```

#### Ê†∏ÂøÉÈÖçÁΩÆËØ¥Êòé

ÁºñËæë `.env` Êñá‰ª∂ÔºåÈÖçÁΩÆ‰ª•‰∏ãÂøÖÈúÄÂèÇÊï∞Ôºö

```env
# ========== ÂøÖÈúÄÈÖçÁΩÆ ==========
# LLM Êèê‰æõÂïÜÈÄâÊã© (gemini|openai|claude|qwen|deepseek|zhipu|moonshot|baidu|minimax|doubao|ollama)
VITE_LLM_PROVIDER=gemini
# ÂØπÂ∫îÁöÑ API Key
VITE_LLM_API_KEY=your_api_key_here

# ========== Êï∞ÊçÆÂ∫ìÈÖçÁΩÆÔºà‰∏âÈÄâ‰∏ÄÔºâ==========
# ÊñπÂºè1ÔºöÊú¨Âú∞Êï∞ÊçÆÂ∫ìÔºàÊé®ËçêÔºåÂºÄÁÆ±Âç≥Áî®Ôºâ
VITE_USE_LOCAL_DB=true

# ÊñπÂºè2ÔºöSupabase ‰∫ëÁ´ØÊï∞ÊçÆÂ∫ìÔºàÊîØÊåÅÂ§öËÆæÂ§áÂêåÊ≠•Ôºâ
# VITE_SUPABASE_URL=https://your-project.supabase.co
# VITE_SUPABASE_ANON_KEY=your_anon_key

# ÊñπÂºè3ÔºöÊºîÁ§∫Ê®°ÂºèÔºà‰∏çÈÖçÁΩÆ‰ªª‰ΩïÊï∞ÊçÆÂ∫ìÔºåÊï∞ÊçÆ‰∏çÊåÅ‰πÖÂåñÔºâ

# ========== ÂèØÈÄâÈÖçÁΩÆ ==========
# GitHub ÈõÜÊàêÔºàÁî®‰∫é‰ªìÂ∫ìÂàÜÊûêÔºâ
# VITE_GITHUB_TOKEN=your_github_token

# ËæìÂá∫ËØ≠Ë®ÄÔºàzh-CN: ‰∏≠Êñá | en-US: Ëã±ÊñáÔºâ
VITE_OUTPUT_LANGUAGE=zh-CN

# ÂàÜÊûêÂèÇÊï∞Ë∞É‰ºò
VITE_MAX_ANALYZE_FILES=40    # ÂçïÊ¨°ÊúÄÂ§ßÂàÜÊûêÊñá‰ª∂Êï∞
VITE_LLM_CONCURRENCY=2       # Âπ∂ÂèëËØ∑Ê±ÇÊï∞
VITE_LLM_GAP_MS=500          # ËØ∑Ê±ÇÈó¥Èöî(ms)
```

#### È´òÁ∫ßÈÖçÁΩÆ

ÈÅáÂà∞Ë∂ÖÊó∂ÊàñËøûÊé•ÈóÆÈ¢òÊó∂ÔºåÂèØË∞ÉÊï¥‰ª•‰∏ãÂèÇÊï∞Ôºö

```env
VITE_LLM_TIMEOUT=300000                      # Â¢ûÂä†Ë∂ÖÊó∂Êó∂Èó¥
VITE_LLM_BASE_URL=https://your-proxy.com/v1 # ‰ΩøÁî®‰ª£ÁêÜÊàñ‰∏≠ËΩ¨ÊúçÂä°
VITE_LLM_CONCURRENCY=1                       # Èôç‰ΩéÂπ∂ÂèëÊï∞
VITE_LLM_GAP_MS=1000                         # Â¢ûÂä†ËØ∑Ê±ÇÈó¥Èöî
```

**Ëá™ÂÆö‰πâËØ∑Ê±ÇÂ§¥Á§∫‰æã**ÔºàÈíàÂØπÁâπÊÆä‰∏≠ËΩ¨Á´ôÔºâÔºö

```env
# JSON Ê†ºÂºèÂ≠óÁ¨¶‰∏≤
VITE_LLM_CUSTOM_HEADERS=&#039;{&quot;X-API-Version&quot;:&quot;v1&quot;,&quot;X-Custom-Auth&quot;:&quot;token123&quot;}&#039;
```

### Â∏∏ËßÅÈóÆÈ¢ò

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Â¶Ç‰ΩïÂø´ÈÄüÂàáÊç¢ LLM Âπ≥Âè∞Ôºü&lt;/b&gt;&lt;/summary&gt;

**ÊñπÂºè‰∏ÄÔºöÊµèËßàÂô®ÈÖçÁΩÆÔºàÊé®ËçêÔºâ**

1. ËÆøÈóÆ `http://localhost:8888/admin` Á≥ªÁªüÁÆ°ÁêÜÈ°µÈù¢
2. Âú®&quot;Á≥ªÁªüÈÖçÁΩÆ&quot;Ê†áÁ≠æÈ°µÈÄâÊã©‰∏çÂêåÁöÑ LLM Êèê‰æõÂïÜ
3. Â°´ÂÖ•ÂØπÂ∫îÁöÑ API Key
4. ‰øùÂ≠òÂπ∂Âà∑Êñ∞È°µÈù¢

**ÊñπÂºè‰∫åÔºöÁéØÂ¢ÉÂèòÈáèÈÖçÁΩÆ**

‰øÆÊîπ `.env` ‰∏≠ÁöÑÈÖçÁΩÆÔºö

```env
# ÂàáÊç¢Âà∞ OpenAI
VITE_LLM_PROVIDER=openai
VITE_OPENAI_API_KEY=your_key

# ÂàáÊç¢Âà∞ÈÄö‰πâÂçÉÈóÆ
VITE_LLM_PROVIDER=qwen
VITE_QWEN_API_KEY=your_key
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;ÈÅáÂà∞ËØ∑Ê±ÇË∂ÖÊó∂ÊÄé‰πàÂäûÔºü&lt;/b&gt;&lt;/summary&gt;

1. Â¢ûÂä†Ë∂ÖÊó∂Êó∂Èó¥Ôºö`VITE_LLM_TIMEOUT=300000`
2. ‰ΩøÁî®‰ª£ÁêÜÔºöÈÖçÁΩÆ `VITE_LLM_BASE_URL`
3. ÂàáÊç¢Âà∞ÂõΩÂÜÖÂπ≥Âè∞ÔºöÈÄö‰πâÂçÉÈóÆ„ÄÅDeepSeek„ÄÅÊô∫Ë∞±AI Á≠â
4. Èôç‰ΩéÂπ∂ÂèëÔºö`VITE_LLM_CONCURRENCY=1`
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Êï∞ÊçÆÂ∫ìÊ®°ÂºèÂ¶Ç‰ΩïÈÄâÊã©Ôºü&lt;/b&gt;&lt;/summary&gt;

**Êú¨Âú∞Ê®°ÂºèÔºàÊé®ËçêÔºâ**ÔºöÊï∞ÊçÆÂ≠òÂÇ®Âú®ÊµèËßàÂô® IndexedDBÔºåÂºÄÁÆ±Âç≥Áî®ÔºåÈöêÁßÅÂÆâÂÖ®
```env
VITE_USE_LOCAL_DB=true
```

**‰∫ëÁ´ØÊ®°Âºè**ÔºöÊï∞ÊçÆÂ≠òÂÇ®Âú® SupabaseÔºåÊîØÊåÅÂ§öËÆæÂ§áÂêåÊ≠•
```env
VITE_SUPABASE_URL=https://your-project.supabase.co
VITE_SUPABASE_ANON_KEY=your_key
```

**ÊºîÁ§∫Ê®°Âºè**Ôºö‰∏çÈÖçÁΩÆ‰ªª‰ΩïÊï∞ÊçÆÂ∫ìÔºåÊï∞ÊçÆ‰∏çÊåÅ‰πÖÂåñ
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Â¶Ç‰Ωï‰ΩøÁî® Ollama Êú¨Âú∞Â§ßÊ®°ÂûãÔºü&lt;/b&gt;&lt;/summary&gt;

```bash
# 1. ÂÆâË£Ö Ollama
curl -fsSL https://ollama.com/install.sh | sh  # macOS/Linux
# Windows: ËÆøÈóÆ https://ollama.com/download

# 2. ÊãâÂèñÊ®°Âûã
ollama pull llama3  # Êàñ codellama„ÄÅqwen2.5„ÄÅdeepseek-coder

# 3. ÈÖçÁΩÆ XCodeReviewer
# Âú® .env ‰∏≠ËÆæÁΩÆÔºö
VITE_LLM_PROVIDER=ollama
VITE_LLM_MODEL=llama3
VITE_LLM_BASE_URL=http://localhost:11434/v1
```

Êé®ËçêÊ®°ÂûãÔºö`llama3`ÔºàÁªºÂêàÔºâ„ÄÅ`codellama`Ôºà‰ª£Á†Å‰∏ìÁî®Ôºâ„ÄÅ`qwen2.5`Ôºà‰∏≠ÊñáÔºâ
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;ÁôæÂ∫¶ÊñáÂøÉ‰∏ÄË®ÄÁöÑ API Key Ê†ºÂºèÔºü&lt;/b&gt;&lt;/summary&gt;

ÁôæÂ∫¶ÈúÄË¶ÅÂêåÊó∂Êèê‰æõ API Key Âíå Secret KeyÔºåÁî®ÂÜíÂè∑ÂàÜÈöîÔºö
```env
VITE_LLM_PROVIDER=baidu
VITE_BAIDU_API_KEY=your_api_key:your_secret_key
```
Ëé∑ÂèñÂú∞ÂùÄÔºöhttps://console.bce.baidu.com/qianfan/
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Â¶Ç‰Ωï‰ΩøÁî® API ‰∏≠ËΩ¨Á´ôÔºü&lt;/b&gt;&lt;/summary&gt;

ËÆ∏Â§öÁî®Êà∑‰ΩøÁî® API ‰∏≠ËΩ¨ÊúçÂä°Êù•ËÆøÈóÆ LLMÔºàÊõ¥Á®≥ÂÆö„ÄÅÊõ¥‰æøÂÆúÔºâ„ÄÇÈÖçÁΩÆÊñπÊ≥ïÔºö

1. ËÆøÈóÆÁ≥ªÁªüÁÆ°ÁêÜÈ°µÈù¢Ôºà`/admin`Ôºâ
2. Âú®&quot;Á≥ªÁªüÈÖçÁΩÆ&quot;Ê†áÁ≠æÈ°µ‰∏≠Ôºö
   - ÈÄâÊã© LLM Êèê‰æõÂïÜÔºàÂ¶Ç OpenAIÔºâ
   - **API Âü∫Á°Ä URL**: Â°´ÂÖ•‰∏≠ËΩ¨Á´ôÂú∞ÂùÄÔºàÂ¶Ç `https://your-proxy.com/v1`Ôºâ
   - **API Key**: Â°´ÂÖ•‰∏≠ËΩ¨Á´ôÊèê‰æõÁöÑÂØÜÈí•ÔºàËÄåÈùûÂÆòÊñπÂØÜÈí•Ôºâ
3. ‰øùÂ≠òÂπ∂Âà∑Êñ∞È°µÈù¢

**Ê≥®ÊÑè**Ôºö
- ‰∏≠ËΩ¨Á´ô URL ÈÄöÂ∏∏‰ª• `/v1` ÁªìÂ∞æÔºàOpenAI ÂÖºÂÆπÊ†ºÂºèÔºâ
- ‰ΩøÁî®‰∏≠ËΩ¨Á´ôÁöÑ API KeyÔºå‰∏çÊòØÂÆòÊñπÁöÑ
- Á°ÆËÆ§‰∏≠ËΩ¨Á´ôÊîØÊåÅ‰Ω†ÈÄâÊã©ÁöÑ AI Ê®°Âûã
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Â¶Ç‰ΩïÂ§á‰ªΩÊú¨Âú∞Êï∞ÊçÆÂ∫ìÔºü&lt;/b&gt;&lt;/summary&gt;

Êú¨Âú∞Êï∞ÊçÆÂ≠òÂÇ®Âú®ÊµèËßàÂô® IndexedDB ‰∏≠Ôºö
- Âú®Â∫îÁî®ÁöÑ&quot;Á≥ªÁªüÁÆ°ÁêÜ&quot;È°µÈù¢ÂØºÂá∫‰∏∫ JSON Êñá‰ª∂
- ÈÄöËøáÂØºÂÖ• JSON Êñá‰ª∂ÊÅ¢Â§çÊï∞ÊçÆ
- Ê≥®ÊÑèÔºöÊ∏ÖÈô§ÊµèËßàÂô®Êï∞ÊçÆ‰ºöÂà†Èô§ÊâÄÊúâÊú¨Âú∞Êï∞ÊçÆ
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Â¶Ç‰ΩïËÆæÁΩÆËæìÂá∫ËØ≠Ë®ÄÔºü&lt;/b&gt;&lt;/summary&gt;

```env
VITE_OUTPUT_LANGUAGE=zh-CN  # ‰∏≠ÊñáÔºàÈªòËÆ§Ôºâ
VITE_OUTPUT_LANGUAGE=en-US  # Ëã±Êñá
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Â¶Ç‰ΩïÈÖçÁΩÆÂ§ö‰∏™Âπ≥Âè∞Âπ∂Âø´ÈÄüÂàáÊç¢Ôºü&lt;/b&gt;&lt;/summary&gt;

Âú® `.env` ‰∏≠È¢ÑÈÖçÁΩÆÊâÄÊúâÂπ≥Âè∞ÁöÑ KeyÔºåÂàáÊç¢Êó∂Âè™ÈúÄ‰øÆÊîπ `VITE_LLM_PROVIDER`Ôºö
```env
VITE_LLM_PROVIDER=gemini  # ÂΩìÂâç‰ΩøÁî®ÁöÑÂπ≥Âè∞

# È¢ÑÈÖçÁΩÆÊâÄÊúâÂπ≥Âè∞
VITE_GEMINI_API_KEY=key1
VITE_OPENAI_API_KEY=key2
VITE_QWEN_API_KEY=key3
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Â¶Ç‰ΩïÊü•ÁúãÁ≥ªÁªüÊó•ÂøóÂíåË∞ÉËØï‰ø°ÊÅØÔºü&lt;/b&gt;&lt;/summary&gt;

XCodeReviewer ÂÜÖÁΩÆ‰∫ÜÊó•ÂøóÁ≥ªÁªüÔºåËÆ∞ÂΩïÊ†∏ÂøÉÊìç‰ΩúÂíåÈîôËØØÔºö

**Êü•ÁúãÊó•Âøó**Ôºö
- ÂØºËà™Ê†è -&gt; Á≥ªÁªüÊó•Âøó
- ÊàñËÆøÈóÆÔºö`http://localhost:5173/logs` (ÂºÄÂèë) / `http://localhost:8888/logs` (Áîü‰∫ß)

**ËÆ∞ÂΩïÂÜÖÂÆπ**Ôºö
- ‚úÖ Áî®Êà∑Ê†∏ÂøÉÊìç‰ΩúÔºàÂàõÂª∫È°πÁõÆ„ÄÅÂÆ°ËÆ°‰ªªÂä°„ÄÅ‰øÆÊîπÈÖçÁΩÆÁ≠âÔºâ
- ‚úÖ API ËØ∑Ê±ÇÂ§±Ë¥•ÂíåÈîôËØØ
- ‚úÖ ÊéßÂà∂Âè∞ÈîôËØØÔºàËá™Âä®ÊçïËé∑Ôºâ
- ‚úÖ Êú™Â§ÑÁêÜÁöÑÂºÇÂ∏∏

**ÂäüËÉΩÁâπÊÄß**Ôºö
- Êó•ÂøóÁ≠õÈÄâ„ÄÅÊêúÁ¥¢
- ÂØºÂá∫Êó•ÂøóÔºàJSON/CSVÔºâ
- ÈîôËØØËØ¶ÊÉÖÊü•Áúã

**ÊâãÂä®ËÆ∞ÂΩïÁî®Êà∑Êìç‰Ωú**Ôºö
```typescript
import { logger, LogCategory } from &#039;@/shared/utils/logger&#039;;

// ËÆ∞ÂΩïÁî®Êà∑Êìç‰Ωú
logger.logUserAction(&#039;ÂàõÂª∫È°πÁõÆ&#039;, { projectName, projectType });
logger.logUserAction(&#039;ÂºÄÂßãÂÆ°ËÆ°&#039;, { taskId, fileCount });
```

&lt;/details&gt;

### üîë Ëé∑Âèñ API Key

#### ÊîØÊåÅÁöÑ LLM Âπ≥Âè∞

XCodeReviewer ÊîØÊåÅ 10+ ‰∏ªÊµÅ LLM Âπ≥Âè∞ÔºåÂèØÊ†πÊçÆÈúÄÊ±ÇËá™Áî±ÈÄâÊã©Ôºö

| Âπ≥Âè∞Á±ªÂûã | Âπ≥Âè∞ÂêçÁß∞ | ÁâπÁÇπ | Ëé∑ÂèñÂú∞ÂùÄ |
|---------|---------|------|---------|
| **ÂõΩÈôÖÂπ≥Âè∞** | Google Gemini | ÂÖçË¥πÈÖçÈ¢ùÂÖÖË∂≥ÔºåÊé®Ëçê | [Ëé∑Âèñ](https://makersuite.google.com/app/apikey) |
| | OpenAI GPT | Á®≥ÂÆöÂèØÈù†ÔºåÊÄßËÉΩÊúÄ‰Ω≥ | [Ëé∑Âèñ](https://platform.openai.com/api-keys) |
| | Anthropic Claude | ‰ª£Á†ÅÁêÜËß£ËÉΩÂäõÂº∫ | [Ëé∑Âèñ](https://console.anthropic.com/) |
| | DeepSeek | ÊÄß‰ª∑ÊØîÈ´ò | [Ëé∑Âèñ](https://platform.deepseek.com/) |
| **ÂõΩÂÜÖÂπ≥Âè∞** | ÈòøÈáå‰∫ëÈÄö‰πâÂçÉÈóÆ | ÂõΩÂÜÖËÆøÈóÆÂø´ | [Ëé∑Âèñ](https://dashscope.console.aliyun.com/) |
| | Êô∫Ë∞±AI (GLM) | ‰∏≠ÊñáÊîØÊåÅÂ•Ω | [Ëé∑Âèñ](https://open.bigmodel.cn/) |
| | Êúà‰πãÊöóÈù¢ Kimi | ÈïøÊñáÊú¨Â§ÑÁêÜ | [Ëé∑Âèñ](https://platform.moonshot.cn/) |
| | ÁôæÂ∫¶ÊñáÂøÉ‰∏ÄË®Ä | ‰ºÅ‰∏öÁ∫ßÊúçÂä° | [Ëé∑Âèñ](https://console.bce.baidu.com/qianfan/) |
| | MiniMax | Â§öÊ®°ÊÄÅËÉΩÂäõ | [Ëé∑Âèñ](https://www.minimaxi.com/) |
| | Â≠óËäÇË±ÜÂåÖ | È´òÊÄß‰ª∑ÊØî | [Ëé∑Âèñ](https://console.volcengine.com/ark) |
| **Êú¨Âú∞ÈÉ®ÁΩ≤** | Ollama | ÂÆåÂÖ®Êú¨Âú∞ÂåñÔºåÈöêÁßÅÂÆâÂÖ® | [ÂÆâË£Ö](https://ollama.com/) |

#### ÈÖçÁΩÆÁ§∫‰æã

```env
# ÈÄöÁî®ÈÖçÁΩÆÔºàÊé®ËçêÔºâ
VITE_LLM_PROVIDER=gemini
VITE_LLM_API_KEY=your_api_key_here

# Êàñ‰ΩøÁî®Âπ≥Âè∞‰∏ìÁî®ÈÖçÁΩÆ
VITE_GEMINI_API_KEY=your_gemini_key
VITE_OPENAI_API_KEY=your_openai_key
# ... Êõ¥Â§öÂπ≥Âè∞ÈÖçÁΩÆËßÅ .env.example
```

#### Supabase ÈÖçÁΩÆÔºàÂèØÈÄâÔºâ

Â¶ÇÈúÄ‰∫ëÁ´ØÊï∞ÊçÆÂêåÊ≠•Ôºö
1. ËÆøÈóÆ [Supabase](https://supabase.com/) ÂàõÂª∫È°πÁõÆ
2. Ëé∑Âèñ URL ÂíåÂåøÂêçÂØÜÈí•
3. Âú® Supabase SQL ÁºñËæëÂô®ÊâßË°å `supabase/migrations/full_schema.sql`
4. Âú® `.env` ‰∏≠ÈÖçÁΩÆÁõ∏ÂÖ≥ÂèÇÊï∞

## ‚ú® Ê†∏ÂøÉÂäüËÉΩ

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;üöÄ È°πÁõÆÁÆ°ÁêÜ&lt;/b&gt;&lt;/summary&gt;

- **‰∏ÄÈîÆÈõÜÊàê‰ª£Á†Å‰ªìÂ∫ì**ÔºöÊó†ÁºùÂØπÊé• GitHub„ÄÅGitLab Á≠â‰∏ªÊµÅÂπ≥Âè∞„ÄÇ
- **Â§öËØ≠Ë®Ä‚ÄúÂÖ®ÂÆ∂Ê°∂‚ÄùÊîØÊåÅ**ÔºöË¶ÜÁõñ JavaScript, TypeScript, Python, Java, Go, Rust Á≠âÁÉ≠Èó®ËØ≠Ë®Ä„ÄÇ
- **ÁÅµÊ¥ªÁöÑÂàÜÊîØÂÆ°ËÆ°**ÔºöÊîØÊåÅÂØπÊåáÂÆö‰ª£Á†ÅÂàÜÊîØËøõË°åÁ≤æÁ°ÆÂàÜÊûê„ÄÇ
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;‚ö° Âç≥Êó∂ÂàÜÊûê&lt;/b&gt;&lt;/summary&gt;

- **‰ª£Á†ÅÁâáÊÆµ‚ÄúÈöèÊâãË¥¥‚Äù**ÔºöÁõ¥Êé•Âú® Web ÁïåÈù¢Á≤òË¥¥‰ª£Á†ÅÔºåÁ´ãÂç≥Ëé∑ÂæóÂàÜÊûêÁªìÊûú„ÄÇ
- **10+ ÁßçËØ≠Ë®ÄÂç≥Êó∂ÊîØÊåÅ**ÔºöÊª°Ë∂≥ÊÇ®Â§öÊ†∑ÂåñÁöÑ‰ª£Á†ÅÂàÜÊûêÈúÄÊ±Ç„ÄÇ
- **ÊØ´ÁßíÁ∫ßÂìçÂ∫î**ÔºöÂø´ÈÄüËé∑Âèñ‰ª£Á†ÅË¥®ÈáèËØÑÂàÜÂíå‰ºòÂåñÂª∫ËÆÆ„ÄÇ
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;üß† Êô∫ËÉΩÂÆ°ËÆ°&lt;/b&gt;&lt;/summary&gt;

- **AI Ê∑±Â∫¶‰ª£Á†ÅÁêÜËß£**ÔºöÊîØÊåÅÂ§ö‰∏™‰∏ªÊµÅ LLM Âπ≥Âè∞ÔºàGemini„ÄÅOpenAI„ÄÅClaude„ÄÅÈÄö‰πâÂçÉÈóÆ„ÄÅDeepSeek Á≠âÔºâÔºåÊèê‰æõË∂ÖË∂äÂÖ≥ÈîÆËØçÂåπÈÖçÁöÑÊô∫ËÉΩÂàÜÊûê„ÄÇ
- **‰∫îÂ§ßÊ†∏ÂøÉÁª¥Â∫¶Ê£ÄÊµã**Ôºö
  - üêõ **ÊΩúÂú® Bug**ÔºöÁ≤æÂáÜÊçïÊçâÈÄªËæëÈîôËØØ„ÄÅËæπÁïåÊù°‰ª∂ÂíåÁ©∫ÊåáÈíàÁ≠âÈóÆÈ¢ò„ÄÇ
  - üîí **ÂÆâÂÖ®ÊºèÊ¥û**ÔºöËØÜÂà´ SQL Ê≥®ÂÖ•„ÄÅXSS„ÄÅÊïèÊÑü‰ø°ÊÅØÊ≥ÑÈú≤Á≠âÂÆâÂÖ®È£éÈô©„ÄÇ
  - ‚ö° **ÊÄßËÉΩÁì∂È¢à**ÔºöÂèëÁé∞‰ΩéÊïàÁÆóÊ≥ï„ÄÅÂÜÖÂ≠òÊ≥ÑÊºèÂíå‰∏çÂêàÁêÜÁöÑÂºÇÊ≠•Êìç‰Ωú„ÄÇ
  - üé® **‰ª£Á†ÅÈ£éÊ†º**ÔºöÁ°Æ‰øù‰ª£Á†ÅÈÅµÂæ™Ë°å‰∏öÊúÄ‰Ω≥ÂÆûË∑µÂíåÁªü‰∏ÄËßÑËåÉ„ÄÇ
  - üîß **ÂèØÁª¥Êä§ÊÄß**ÔºöËØÑ‰º∞‰ª£Á†ÅÁöÑÂèØËØªÊÄß„ÄÅÂ§çÊùÇÂ∫¶ÂíåÊ®°ÂùóÂåñÁ®ãÂ∫¶„ÄÇ
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;üí° ÂèØËß£ÈáäÊÄßÂàÜÊûê (What-Why-How)&lt;/b&gt;&lt;/summary&gt;

- **What (ÊòØ‰ªÄ‰πà)**ÔºöÊ∏ÖÊô∞Âú∞ÊåáÂá∫‰ª£Á†Å‰∏≠Â≠òÂú®ÁöÑÈóÆÈ¢ò„ÄÇ
- **Why (‰∏∫‰ªÄ‰πà)**ÔºöËØ¶ÁªÜËß£ÈáäËØ•ÈóÆÈ¢òÂèØËÉΩÂ∏¶Êù•ÁöÑÊΩúÂú®È£éÈô©ÂíåÂΩ±Âìç„ÄÇ
- **How (Â¶Ç‰Ωï‰øÆÂ§ç)**ÔºöÊèê‰æõÂÖ∑‰ΩìÁöÑ„ÄÅÂèØÁõ¥Êé•‰ΩøÁî®ÁöÑ‰ª£Á†Å‰øÆÂ§çÁ§∫‰æã„ÄÇ
- **Á≤æÂáÜ‰ª£Á†ÅÂÆö‰Ωç**ÔºöÂø´ÈÄüË∑≥ËΩ¨Âà∞ÈóÆÈ¢òÊâÄÂú®ÁöÑË°åÂíåÂàó„ÄÇ
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;üìä ÂèØËßÜÂåñÊä•Âëä&lt;/b&gt;&lt;/summary&gt;

- **‰ª£Á†ÅË¥®Èáè‰ª™Ë°®Áõò**ÔºöÊèê‰æõ 0-100 ÂàÜÁöÑÁªºÂêàË¥®ÈáèËØÑ‰º∞ÔºåËÆ©‰ª£Á†ÅÂÅ•Â∫∑Áä∂ÂÜµ‰∏ÄÁõÆ‰∫ÜÁÑ∂„ÄÇ
- **Â§öÁª¥Â∫¶ÈóÆÈ¢òÁªüËÆ°**ÔºöÊåâÁ±ªÂûãÂíå‰∏•ÈáçÁ®ãÂ∫¶ÂØπÈóÆÈ¢òËøõË°åÂàÜÁ±ªÁªüËÆ°„ÄÇ
- **Ë¥®ÈáèË∂ãÂäøÂàÜÊûê**ÔºöÈÄöËøáÂõæË°®Â±ïÁ§∫‰ª£Á†ÅË¥®ÈáèÈöèÊó∂Èó¥ÁöÑÂèòÂåñË∂ãÂäø„ÄÇ
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;‚öôÔ∏è Á≥ªÁªüÁÆ°ÁêÜ&lt;/b&gt;&lt;/summary&gt;

ËÆøÈóÆ `/admin` È°µÈù¢ÔºåÊèê‰æõÂÆåÊï¥ÁöÑÁ≥ªÁªüÈÖçÁΩÆÂíåÊï∞ÊçÆÁÆ°ÁêÜÂäüËÉΩÔºö

- **üîß ÂèØËßÜÂåñÈÖçÁΩÆÁÆ°ÁêÜ**ÔºàËøêË°åÊó∂ÈÖçÁΩÆÔºâÔºö
  - üéØ **LLM ÈÖçÁΩÆ**ÔºöÂú®ÊµèËßàÂô®‰∏≠Áõ¥Êé•ÈÖçÁΩÆ API Keys„ÄÅÊ®°Âûã„ÄÅË∂ÖÊó∂Á≠âÂèÇÊï∞
  - üîë **Âπ≥Âè∞ÂØÜÈí•**ÔºöÁÆ°ÁêÜ 10+ LLM Âπ≥Âè∞ÁöÑ API KeysÔºåÊîØÊåÅÂø´ÈÄüÂàáÊç¢
  - ‚ö° **ÂàÜÊûêÂèÇÊï∞**ÔºöË∞ÉÊï¥Âπ∂ÂèëÊï∞„ÄÅÈó¥ÈöîÊó∂Èó¥„ÄÅÊúÄÂ§ßÊñá‰ª∂Êï∞Á≠â
  - üåê **API ‰∏≠ËΩ¨Á´ôÊîØÊåÅ**ÔºöËΩªÊùæÈÖçÁΩÆÁ¨¨‰∏âÊñπ API ‰ª£ÁêÜÊúçÂä°
  - üíæ **ÈÖçÁΩÆ‰ºòÂÖàÁ∫ß**ÔºöËøêË°åÊó∂ÈÖçÁΩÆ &gt; ÊûÑÂª∫Êó∂ÈÖçÁΩÆÔºåÊó†ÈúÄÈáçÊñ∞ÊûÑÂª∫ÈïúÂÉè
  
- **üíæ Êï∞ÊçÆÂ∫ìÁÆ°ÁêÜ**Ôºö
  - üè† **‰∏âÁßçÊ®°Âºè**ÔºöÊú¨Âú∞ IndexedDB / Supabase ‰∫ëÁ´Ø / ÊºîÁ§∫Ê®°Âºè
  - üì§ **ÂØºÂá∫Â§á‰ªΩ**ÔºöÂ∞ÜÊï∞ÊçÆÂØºÂá∫‰∏∫ JSON Êñá‰ª∂
  - üì• **ÂØºÂÖ•ÊÅ¢Â§ç**Ôºö‰ªéÂ§á‰ªΩÊñá‰ª∂ÊÅ¢Â§çÊï∞ÊçÆ
  - üóëÔ∏è **Ê∏ÖÁ©∫Êï∞ÊçÆ**Ôºö‰∏ÄÈîÆÊ∏ÖÁêÜÊâÄÊúâÊú¨Âú∞Êï∞ÊçÆ
  - üìä **Â≠òÂÇ®ÁõëÊéß**ÔºöÂÆûÊó∂Êü•ÁúãÂ≠òÂÇ®Á©∫Èó¥‰ΩøÁî®ÊÉÖÂÜµ
  
- **üìà Êï∞ÊçÆÊ¶ÇËßà**Ôºö
  - È°πÁõÆ„ÄÅ‰ªªÂä°„ÄÅÈóÆÈ¢òÁöÑÂÆåÊï¥ÁªüËÆ°
  - ÂèØËßÜÂåñÂõæË°®Â±ïÁ§∫Ë¥®ÈáèË∂ãÂäø
  - Â≠òÂÇ®‰ΩøÁî®ÊÉÖÂÜµÂàÜÊûê
&lt;/details&gt;

## üõ†Ô∏è ÊäÄÊúØÊ†à

| ÂàÜÁ±ª | ÊäÄÊúØ | ËØ¥Êòé |
| :--- | :--- | :--- |
| **ÂâçÁ´ØÊ°ÜÊû∂** | `React 18` `TypeScript` `Vite` | Áé∞‰ª£ÂåñÂâçÁ´ØÂºÄÂèëÊ†àÔºåÊîØÊåÅÁÉ≠ÈáçËΩΩÂíåÁ±ªÂûãÂÆâÂÖ® |
| **UI ÁªÑ‰ª∂** | `Tailwind CSS` `Radix UI` `Lucide React` | ÂìçÂ∫îÂºèËÆæËÆ°ÔºåÊó†ÈöúÁ¢çËÆøÈóÆÔºå‰∏∞ÂØåÁöÑÂõæÊ†áÂ∫ì |
| **Êï∞ÊçÆÂèØËßÜÂåñ** | `Recharts` | ‰∏ì‰∏öÁöÑÂõæË°®Â∫ìÔºåÊîØÊåÅÂ§öÁßçÂõæË°®Á±ªÂûã |
| **Ë∑ØÁî±ÁÆ°ÁêÜ** | `React Router v6` | ÂçïÈ°µÂ∫îÁî®Ë∑ØÁî±Ëß£ÂÜ≥ÊñπÊ°à |
| **Áä∂ÊÄÅÁÆ°ÁêÜ** | `React Hooks` `Sonner` | ËΩªÈáèÁ∫ßÁä∂ÊÄÅÁÆ°ÁêÜÂíåÈÄöÁü•Á≥ªÁªü |
| **AI ÂºïÊìé** | `Â§öÂπ≥Âè∞ LLM` | ÊîØÊåÅ Gemini„ÄÅOpenAI„ÄÅClaude„ÄÅÈÄö‰πâÂçÉÈóÆ„ÄÅDeepSeek Á≠â 10+ ‰∏ªÊµÅÂπ≥Âè∞ |
| **Êï∞ÊçÆÂ≠òÂÇ®** | `IndexedDB` `Supabase` `PostgreSQL` | Êú¨Âú∞Êï∞ÊçÆÂ∫ì + ‰∫ëÁ´ØÊï∞ÊçÆÂ∫ìÂèåÊ®°ÂºèÊîØÊåÅ |
| **HTTP ÂÆ¢Êà∑Á´Ø** | `Axios` `Ky` | Áé∞‰ª£ÂåñÁöÑ HTTP ËØ∑Ê±ÇÂ∫ì |
| **‰ª£Á†ÅË¥®Èáè** | `Biome` `Ast-grep` `TypeScript` | ‰ª£Á†ÅÊ†ºÂºèÂåñ„ÄÅÈùôÊÄÅÂàÜÊûêÂíåÁ±ªÂûãÊ£ÄÊü• |
| **ÊûÑÂª∫Â∑•ÂÖ∑** | `Vite` `PostCSS` `Autoprefixer` | Âø´ÈÄüÁöÑÊûÑÂª∫Â∑•ÂÖ∑Âíå CSS Â§ÑÁêÜ |

## üìÅ È°πÁõÆÁªìÊûÑ

```
XCodeReviewer/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ app/                # Â∫îÁî®ÈÖçÁΩÆ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.tsx         # ‰∏ªÂ∫îÁî®ÁªÑ‰ª∂
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.tsx        # Â∫îÁî®ÂÖ•Âè£ÁÇπ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes.tsx      # Ë∑ØÁî±ÈÖçÁΩÆ
‚îÇ   ‚îú‚îÄ‚îÄ components/         # React ÁªÑ‰ª∂
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ layout/         # Â∏ÉÂ±ÄÁªÑ‰ª∂ (Header, Footer, PageMeta)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ui/             # UI ÁªÑ‰ª∂Â∫ì (Âü∫‰∫é Radix UI)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ system/         # Á≥ªÁªüÈÖçÁΩÆÁªÑ‰ª∂
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database/       # Êï∞ÊçÆÂ∫ìÁÆ°ÁêÜÁªÑ‰ª∂
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ debug/          # Ë∞ÉËØïÁªÑ‰ª∂
‚îÇ   ‚îú‚îÄ‚îÄ pages/              # È°µÈù¢ÁªÑ‰ª∂
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dashboard.tsx   # ‰ª™Ë°®Áõò
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Projects.tsx    # È°πÁõÆÁÆ°ÁêÜ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ InstantAnalysis.tsx # Âç≥Êó∂ÂàÜÊûê
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AuditTasks.tsx  # ÂÆ°ËÆ°‰ªªÂä°
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ AdminDashboard.tsx # Á≥ªÁªüÁÆ°ÁêÜ
‚îÇ   ‚îú‚îÄ‚îÄ features/           # ÂäüËÉΩÊ®°Âùó
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analysis/       # ÂàÜÊûêÁõ∏ÂÖ≥ÊúçÂä°
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ services/   # AI ‰ª£Á†ÅÂàÜÊûêÂºïÊìé
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ projects/       # È°πÁõÆÁõ∏ÂÖ≥ÊúçÂä°
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ services/   # ‰ªìÂ∫ìÊâ´Êèè„ÄÅZIP Êñá‰ª∂Êâ´Êèè
‚îÇ   ‚îú‚îÄ‚îÄ shared/             # ÂÖ±‰∫´Â∑•ÂÖ∑
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config/         # ÈÖçÁΩÆÊñá‰ª∂
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database.ts      # Êï∞ÊçÆÂ∫ìÁªü‰∏ÄÊé•Âè£
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ localDatabase.ts # IndexedDB ÂÆûÁé∞
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ env.ts           # ÁéØÂ¢ÉÂèòÈáèÈÖçÁΩÆ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ types/          # TypeScript Á±ªÂûãÂÆö‰πâ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hooks/          # Ëá™ÂÆö‰πâ React Hooks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ utils/          # Â∑•ÂÖ∑ÂáΩÊï∞
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ initLocalDB.ts   # Êú¨Âú∞Êï∞ÊçÆÂ∫ìÂàùÂßãÂåñ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ constants/      # Â∏∏ÈáèÂÆö‰πâ
‚îÇ   ‚îî‚îÄ‚îÄ assets/             # ÈùôÊÄÅËµÑÊ∫ê
‚îÇ       ‚îî‚îÄ‚îÄ styles/         # Ê†∑ÂºèÊñá‰ª∂
‚îú‚îÄ‚îÄ supabase/
‚îÇ   ‚îî‚îÄ‚îÄ migrations/         # Êï∞ÊçÆÂ∫ìËøÅÁßªÊñá‰ª∂
‚îú‚îÄ‚îÄ public/
‚îÇ   ‚îî‚îÄ‚îÄ images/             # ÂõæÁâáËµÑÊ∫ê
‚îú‚îÄ‚îÄ scripts/                # ÊûÑÂª∫ÂíåËÆæÁΩÆËÑöÊú¨
‚îî‚îÄ‚îÄ rules/                  # ‰ª£Á†ÅËßÑÂàôÈÖçÁΩÆ
```

## üéØ ‰ΩøÁî®ÊåáÂçó

### Á≥ªÁªüÈÖçÁΩÆÔºàÈ¶ñÊ¨°‰ΩøÁî®ÂøÖÁúãÔºâ

ËÆøÈóÆ `/admin` Á≥ªÁªüÁÆ°ÁêÜÈ°µÈù¢ÔºåÂú®&quot;Á≥ªÁªüÈÖçÁΩÆ&quot;Ê†áÁ≠æÈ°µ‰∏≠ÈÖçÁΩÆÔºö

#### 1. **ÈÖçÁΩÆ LLM Êèê‰æõÂïÜ**
- ÈÄâÊã©ÊÇ®Ë¶Å‰ΩøÁî®ÁöÑ LLM Âπ≥Âè∞ÔºàGemini„ÄÅOpenAI„ÄÅClaude Á≠âÔºâ
- Â°´ÂÖ• API KeyÔºàÊîØÊåÅÈÄöÁî® Key ÊàñÂπ≥Âè∞‰∏ìÁî® KeyÔºâ
- ÂèØÈÄâÔºöÈÖçÁΩÆÊ®°ÂûãÂêçÁß∞„ÄÅAPI Âü∫Á°Ä URLÔºàÁî®‰∫é‰∏≠ËΩ¨Á´ôÔºâ

#### 2. **ÈÖçÁΩÆ API ‰∏≠ËΩ¨Á´ô**ÔºàÂ¶ÇÊûú‰ΩøÁî®Ôºâ
- Âú®&quot;API Âü∫Á°Ä URL&quot;‰∏≠Â°´ÂÖ•‰∏≠ËΩ¨Á´ôÂú∞ÂùÄÔºàÂ¶Ç `https://your-proxy.com/v1`Ôºâ
- Â°´ÂÖ•‰∏≠ËΩ¨Á´ôÊèê‰æõÁöÑ API Key
- ‰øùÂ≠òÈÖçÁΩÆ

#### 3. **Ë∞ÉÊï¥ÂàÜÊûêÂèÇÊï∞**ÔºàÂèØÈÄâÔºâ
- ÊúÄÂ§ßÂàÜÊûêÊñá‰ª∂Êï∞„ÄÅÂπ∂ÂèëËØ∑Ê±ÇÊï∞„ÄÅËØ∑Ê±ÇÈó¥Èöî
- ËæìÂá∫ËØ≠Ë®ÄÔºà‰∏≠Êñá/Ëã±ÊñáÔºâ

**ÈÖçÁΩÆÂÆåÊàêÂêéÁÇπÂáª&quot;‰øùÂ≠òÊâÄÊúâÊõ¥Êîπ&quot;Âπ∂Âà∑Êñ∞È°µÈù¢Âç≥ÂèØ‰ΩøÁî®„ÄÇ**

### Âç≥Êó∂‰ª£Á†ÅÂàÜÊûê
1. ËÆøÈóÆ `/instant-analysis` È°µÈù¢
2. ÈÄâÊã©ÁºñÁ®ãËØ≠Ë®ÄÔºàÊîØÊåÅ 10+ ÁßçËØ≠Ë®ÄÔºâ
3. Á≤òË¥¥‰ª£Á†ÅÊàñ‰∏ä‰º†Êñá‰ª∂
4. ÁÇπÂáª&quot;ÂºÄÂßãÂàÜÊûê&quot;Ëé∑Âæó AI ÂàÜÊûêÁªìÊûú
5. Êü•ÁúãËØ¶ÁªÜÁöÑÈóÆÈ¢òÊä•ÂëäÂíå‰øÆÂ§çÂª∫ËÆÆ

### È°πÁõÆÁÆ°ÁêÜ
1. ËÆøÈóÆ `/projects` È°µÈù¢
2. ÁÇπÂáª&quot;Êñ∞Âª∫È°πÁõÆ&quot;ÂàõÂª∫È°πÁõÆ
3. ÈÖçÁΩÆ‰ªìÂ∫ì URL ÂíåÊâ´ÊèèÂèÇÊï∞
4. ÂêØÂä®‰ª£Á†ÅÂÆ°ËÆ°‰ªªÂä°
5. Êü•ÁúãÂÆ°ËÆ°ÁªìÊûúÂíåÈóÆÈ¢òÁªüËÆ°

### ÂÆ°ËÆ°‰ªªÂä°
1. Âú®È°πÁõÆËØ¶ÊÉÖÈ°µÂàõÂª∫ÂÆ°ËÆ°‰ªªÂä°
2. ÈÄâÊã©Êâ´ÊèèÂàÜÊîØÂíåÊéíÈô§Ê®°Âºè
3. ÈÖçÁΩÆÂàÜÊûêÊ∑±Â∫¶ÂíåËåÉÂõ¥
4. ÁõëÊéß‰ªªÂä°ÊâßË°åÁä∂ÊÄÅ
5. Êü•ÁúãËØ¶ÁªÜÁöÑÈóÆÈ¢òÊä•Âëä

### ÂÆ°ËÆ°Êä•ÂëäÂØºÂá∫
1. Âú®‰ªªÂä°ËØ¶ÊÉÖÈ°µÁÇπÂáª&quot;ÂØºÂá∫Êä•Âëä&quot;ÊåâÈíÆ
2. ÈÄâÊã©ÂØºÂá∫Ê†ºÂºèÔºö
   - **JSON Ê†ºÂºè**ÔºöÁªìÊûÑÂåñÊï∞ÊçÆÔºåÈÄÇÂêàÁ®ãÂ∫èÂ§ÑÁêÜÂíåÈõÜÊàê
   - **PDF Ê†ºÂºè**Ôºö‰∏ì‰∏öÊä•ÂëäÔºåÈÄÇÂêàÊâìÂç∞ÂíåÂàÜ‰∫´ÔºàÈÄöËøáÊµèËßàÂô®ÊâìÂç∞ÂäüËÉΩÔºâ
3. JSON Êä•ÂëäÂåÖÂê´ÂÆåÊï¥ÁöÑ‰ªªÂä°‰ø°ÊÅØ„ÄÅÈóÆÈ¢òËØ¶ÊÉÖÂíåÁªüËÆ°Êï∞ÊçÆ
4. PDF Êä•ÂëäÊèê‰æõÁæéËßÇÁöÑÂèØËßÜÂåñÂ±ïÁ§∫ÔºåÊîØÊåÅ‰∏≠ÊñáÊòæÁ§∫
5. Êä•ÂëäÂÜÖÂÆπÂåÖÊã¨ÔºöÈ°πÁõÆ‰ø°ÊÅØ„ÄÅÂÆ°ËÆ°ÁªüËÆ°„ÄÅÈóÆÈ¢òËØ¶ÊÉÖÔºàÊåâ‰∏•ÈáçÁ®ãÂ∫¶ÂàÜÁ±ªÔºâ„ÄÅ‰øÆÂ§çÂª∫ËÆÆÁ≠â

**PDF ÂØºÂá∫ÊèêÁ§∫Ôºö**
- ÁÇπÂáª&quot;ÂØºÂá∫ PDF&quot;Âêé‰ºöÂºπÂá∫ÊµèËßàÂô®ÊâìÂç∞ÂØπËØùÊ°Ü
- Âª∫ËÆÆÂú®ÊâìÂç∞ËÆæÁΩÆ‰∏≠**ÂèñÊ∂àÂãæÈÄâ&quot;È°µÁúâÂíåÈ°µËÑö&quot;ÈÄâÈ°π**Ôºå‰ª•Ëé∑ÂæóÊõ¥Âπ≤ÂáÄÁöÑÊä•ÂëäÔºàÈÅøÂÖçÊòæÁ§∫ URL Á≠â‰ø°ÊÅØÔºâ
- Âú®ÊâìÂç∞ÂØπËØùÊ°Ü‰∏≠ÈÄâÊã©&quot;Âè¶Â≠ò‰∏∫ PDF&quot;Âç≥ÂèØ‰øùÂ≠òÊä•ÂëäÊñá‰ª∂

### ÊûÑÂª∫ÂíåÈÉ®ÁΩ≤

```bash
# ÂºÄÂèëÊ®°Âºè
pnpm dev

# ÊûÑÂª∫Áîü‰∫ßÁâàÊú¨
pnpm build

# È¢ÑËßàÊûÑÂª∫ÁªìÊûú
pnpm preview

# ‰ª£Á†ÅÊ£ÄÊü•
pnpm lint
```

### ÁéØÂ¢ÉÂèòÈáèËØ¥Êòé

#### Ê†∏ÂøÉLLMÈÖçÁΩÆ
| ÂèòÈáèÂêç | ÂøÖÈúÄ | ÈªòËÆ§ÂÄº | ËØ¥Êòé |
|--------|------|--------|------|
| `VITE_LLM_PROVIDER` | ‚úÖ | `gemini` | LLMÊèê‰æõÂïÜÔºö`gemini`\|`openai`\|`claude`\|`qwen`\|`deepseek`\|`zhipu`\|`moonshot`\|`baidu`\|`minimax`\|`doubao`\|`ollama` |
| `VITE_LLM_API_KEY` | ‚úÖ | - | ÈÄöÁî®API KeyÔºà‰ºòÂÖàÁ∫ßÈ´ò‰∫éÂπ≥Âè∞‰∏ìÁî®ÈÖçÁΩÆÔºâ |
| `VITE_LLM_MODEL` | ‚ùå | Ëá™Âä® | Ê®°ÂûãÂêçÁß∞Ôºà‰∏çÊåáÂÆöÂàô‰ΩøÁî®ÂêÑÂπ≥Âè∞ÈªòËÆ§Ê®°ÂûãÔºâ |
| `VITE_LLM_BASE_URL` | ‚ùå | - | Ëá™ÂÆö‰πâAPIÁ´ØÁÇπÔºà**ÊîØÊåÅÊâÄÊúâÂπ≥Âè∞ÁöÑ‰∏≠ËΩ¨Á´ô**„ÄÅ‰ª£ÁêÜÊàñÁßÅÊúâÈÉ®ÁΩ≤Ôºâ |
| `VITE_LLM_TIMEOUT` | ‚ùå | `150000` | ËØ∑Ê±ÇË∂ÖÊó∂Êó∂Èó¥ÔºàÊØ´ÁßíÔºâ |
| `VITE_LLM_TEMPERATURE` | ‚ùå | `0.2` | Ê∏©Â∫¶ÂèÇÊï∞Ôºà0.0-2.0ÔºâÔºåÊéßÂà∂ËæìÂá∫ÈöèÊú∫ÊÄß |
| `VITE_LLM_MAX_TOKENS` | ‚ùå | `4096` | ÊúÄÂ§ßËæìÂá∫tokenÊï∞ |
| `VITE_LLM_CUSTOM_HEADERS` | ‚ùå | - | Ëá™ÂÆö‰πâHTTPËØ∑Ê±ÇÂ§¥ÔºàJSONÊ†ºÂºèÂ≠óÁ¨¶‰∏≤ÔºâÔºåÁî®‰∫éÁâπÊÆä‰∏≠ËΩ¨Á´ôÊàñËá™Âª∫ÊúçÂä° |

&gt; üí° **API Ê†ºÂºèÊîØÊåÅ**ÔºöXCodeReviewer ÊîØÊåÅ‰∏âÁßç‰∏ªÊµÅ API Ê†ºÂºèÔºö
&gt; - **OpenAI ÂÖºÂÆπÊ†ºÂºè**ÔºàÊúÄÂ∏∏ËßÅÔºâÔºöÈÄÇÁî®‰∫éÂ§ßÂ§öÊï∞‰∏≠ËΩ¨Á´ôÂíå OpenRouter
&gt; - **Gemini Ê†ºÂºè**ÔºöGoogle Gemini ÂÆòÊñπÂèäÂÖºÂÆπÊúçÂä°
&gt; - **Claude Ê†ºÂºè**ÔºöAnthropic Claude ÂÆòÊñπÂèäÂÖºÂÆπÊúçÂä°
&gt; 
&gt; ÈÖçÁΩÆÊó∂Âè™ÈúÄÈÄâÊã©ÂØπÂ∫îÁöÑ LLM Êèê‰æõÂïÜÔºåÂ°´ÂÖ•‰∏≠ËΩ¨Á´ôÂú∞ÂùÄÂíå Key Âç≥ÂèØ„ÄÇËá™ÂÆö‰πâËØ∑Ê±ÇÂ§¥ÂäüËÉΩÂèØÊª°Ë∂≥ÁâπÊÆä‰∏≠ËΩ¨Á´ôÁöÑÈ¢ùÂ§ñË¶ÅÊ±Ç„ÄÇ

#### Âπ≥Âè∞‰∏ìÁî®API KeyÈÖçÁΩÆÔºàÂèØÈÄâÔºâ
| ÂèòÈáèÂêç | ËØ¥Êòé | ÁâπÊÆäË¶ÅÊ±Ç |
|--------|------|---------|
| `VITE_GEMINI_API_KEY` | Google Gemini API Key | - |
| `VITE_GEMINI_MODEL` | GeminiÊ®°Âûã (ÈªòËÆ§: gemini-1.5-flash) | - |
| `VITE_OPENAI_API_KEY` | OpenAI API Key | - |
| `VITE_OPENAI_MODEL` | OpenAIÊ®°Âûã (ÈªòËÆ§: gpt-4o-mini) | - |
| `VITE_OPENAI_BASE_URL` | OpenAIËá™ÂÆö‰πâÁ´ØÁÇπ | Áî®‰∫é‰∏≠ËΩ¨ÊúçÂä° |
| `VITE_CLAUDE_API_KEY` | Anthropic Claude API Key | - |
| `VITE_CLAUDE_MODEL` | ClaudeÊ®°Âûã (ÈªòËÆ§: claude-3-5-sonnet-20241022) | - |
| `VITE_QWEN_API_KEY` | ÈòøÈáå‰∫ëÈÄö‰πâÂçÉÈóÆ API Key | - |
| `VITE_QWEN_MODEL` | ÈÄö‰πâÂçÉÈóÆÊ®°Âûã (ÈªòËÆ§: qwen-turbo) | - |
| `VITE_DEEPSEEK_API_KEY` | DeepSeek API Key | - |
| `VITE_DEEPSEEK_MODEL` | DeepSeekÊ®°Âûã (ÈªòËÆ§: deepseek-chat) | - |
| `VITE_ZHIPU_API_KEY` | Êô∫Ë∞±AI API Key | - |
| `VITE_ZHIPU_MODEL` | Êô∫Ë∞±Ê®°Âûã (ÈªòËÆ§: glm-4-flash) | - |
| `VITE_MOONSHOT_API_KEY` | Êúà‰πãÊöóÈù¢ Kimi API Key | - |
| `VITE_MOONSHOT_MODEL` | KimiÊ®°Âûã (ÈªòËÆ§: moonshot-v1-8k) | - |
| `VITE_BAIDU_API_KEY` | ÁôæÂ∫¶ÊñáÂøÉ‰∏ÄË®Ä API Key | ‚ö†Ô∏è Ê†ºÂºè: `API_KEY:SECRET_KEY` |
| `VITE_BAIDU_MODEL` | ÊñáÂøÉÊ®°Âûã (ÈªòËÆ§: ERNIE-3.5-8K) | - |
| `VITE_MINIMAX_API_KEY` | MiniMax API Key | - |
| `VITE_MINIMAX_MODEL` | MiniMaxÊ®°Âûã (ÈªòËÆ§: abab6.5-chat) | - |
| `VITE_DOUBAO_API_KEY` | Â≠óËäÇË±ÜÂåÖ API Key | - |
| `VITE_DOUBAO_MODEL` | Ë±ÜÂåÖÊ®°Âûã (ÈªòËÆ§: doubao-pro-32k) | - |

#### Êï∞ÊçÆÂ∫ìÈÖçÁΩÆÔºàÂèØÈÄâÔºâ
| ÂèòÈáèÂêç | ÂøÖÈúÄ | ËØ¥Êòé |
|--------|------|------|
| `VITE_SUPABASE_URL` | ‚ùå | SupabaseÈ°πÁõÆURLÔºàÁî®‰∫éÊï∞ÊçÆÊåÅ‰πÖÂåñÔºâ |
| `VITE_SUPABASE_ANON_KEY` | ‚ùå | SupabaseÂåøÂêçÂØÜÈí• |

&gt; üí° **ÊèêÁ§∫**Ôºö‰∏çÈÖçÁΩÆSupabaseÊó∂ÔºåÁ≥ªÁªü‰ª•ÊºîÁ§∫Ê®°ÂºèËøêË°åÔºåÊï∞ÊçÆ‰∏çÊåÅ‰πÖÂåñ

#### Git‰ªìÂ∫ìÈõÜÊàêÈÖçÁΩÆ
| ÂèòÈáèÂêç | ÂøÖÈúÄ | ËØ¥Êòé |
|--------|------|------|
| `VITE_GITHUB_TOKEN` | ‚úÖ | GitHub Personal Access Token |
| `VITE_GITLAB_TOKEN` | ‚úÖ | GitLab Personal Access Token Êàñ Project Access Token |

#### ÂàÜÊûêË°å‰∏∫ÈÖçÁΩÆ
| ÂèòÈáèÂêç | ÈªòËÆ§ÂÄº | ËØ¥Êòé |
|--------|--------|------|
| `VITE_MAX_ANALYZE_FILES` | `40` | ÂçïÊ¨°ÂàÜÊûêÁöÑÊúÄÂ§ßÊñá‰ª∂Êï∞ |
| `VITE_LLM_CONCURRENCY` | `2` | LLMÂπ∂ÂèëËØ∑Ê±ÇÊï∞ÔºàÈôç‰ΩéÂèØÈÅøÂÖçÈ¢ëÁéáÈôêÂà∂Ôºâ |
| `VITE_LLM_GAP_MS` | `500` | LLMËØ∑Ê±ÇÈó¥ÈöîÔºàÊØ´ÁßíÔºåÂ¢ûÂä†ÂèØÈÅøÂÖçÈ¢ëÁéáÈôêÂà∂Ôºâ |

#### Â∫îÁî®ÈÖçÁΩÆ
| ÂèòÈáèÂêç | ÈªòËÆ§ÂÄº | ËØ¥Êòé |
|--------|--------|------|
| `VITE_APP_ID` | `xcodereviewer` | Â∫îÁî®Ê†áËØÜÁ¨¶ |

## ü§ù Ë¥°ÁåÆÊåáÂçó

Êàë‰ª¨ÁÉ≠ÁÉàÊ¨¢ËøéÊâÄÊúâÂΩ¢ÂºèÁöÑË¥°ÁåÆÔºÅÊó†ËÆ∫ÊòØÊèê‰∫§ issue„ÄÅÂàõÂª∫ PRÔºåËøòÊòØÊîπËøõÊñáÊ°£ÔºåÊÇ®ÁöÑÊØè‰∏ÄÊ¨°Ë¥°ÁåÆÂØπÊàë‰ª¨ÈÉΩËá≥ÂÖ≥ÈáçË¶Å„ÄÇËØ∑ËÅîÁ≥ªÊàë‰ª¨‰∫ÜËß£ËØ¶ÁªÜ‰ø°ÊÅØ„ÄÇ

### ÂºÄÂèëÊµÅÁ®ã

1.  **Fork** Êú¨È°πÁõÆ
2.  ÂàõÂª∫ÊÇ®ÁöÑÂäüËÉΩÂàÜÊîØ (`git checkout -b feature/AmazingFeature`)
3.  Êèê‰∫§ÊÇ®ÁöÑÊõ¥Êîπ (`git commit -m &#039;Add some AmazingFeature&#039;`)
4.  Êé®ÈÄÅÂà∞ÂàÜÊîØ (`git push origin feature/AmazingFeature`)
5.  ÂàõÂª∫‰∏Ä‰∏™ **Pull Request**

## üôè Ëá¥Ë∞¢

### Ê†∏ÂøÉÊäÄÊúØÊîØÊåÅ
- **[React](https://reactjs.org/)** &amp; **[Vite](https://vitejs.dev/)**: Êèê‰æõÁé∞‰ª£ÂåñÁöÑÂâçÁ´ØÂºÄÂèë‰ΩìÈ™å
- **[TypeScript](https://www.typescriptlang.org/)**: Êèê‰æõÁ±ªÂûãÂÆâÂÖ®‰øùÈöú
- **[Tailwind CSS](https://tailwindcss.com/)**: Êèê‰æõÁé∞‰ª£ÂåñÁöÑ CSS Ê°ÜÊû∂
- **[Radix UI](https://www.radix-ui.com/)**: Êèê‰æõÊó†ÈöúÁ¢çÁöÑ UI ÁªÑ‰ª∂Â∫ì

### AI Âπ≥Âè∞ÊîØÊåÅ
- **[Google Gemini AI](https://ai.google.dev/)**: Êèê‰æõÂº∫Â§ßÁöÑ AI ÂàÜÊûêËÉΩÂäõ
- **[OpenAI](https://openai.com/)**: GPTÁ≥ªÂàóÊ®°ÂûãÊîØÊåÅ
- **[Anthropic Claude](https://www.anthropic.com/)**: ClaudeÊ®°ÂûãÊîØÊåÅ
- **[DeepSeek](https://www.deepseek.com/)**: ÂõΩ‰∫ßAIÂ§ßÊ®°ÂûãÊîØÊåÅ
- **[ÈòøÈáå‰∫ëÈÄö‰πâÂçÉÈóÆ](https://tongyi.aliyun.com/)**: ‰ºÅ‰∏öÁ∫ßAIÊúçÂä°
- **[Êô∫Ë∞±AI](https://www.zhipuai.cn/)**: GLMÁ≥ªÂàóÊ®°Âûã
- **[Moonshot AI](https://www.moonshot.cn/)**: KimiÊ®°ÂûãÊîØÊåÅ
- **[Ollama](https://ollama.com/)**: Êú¨Âú∞Ê®°ÂûãÈÉ®ÁΩ≤ÊñπÊ°à

### Êï∞ÊçÆÂ≠òÂÇ®
- **[Supabase](https://supabase.com/)**: Êèê‰æõ‰æøÊç∑ÁöÑÂêéÁ´ØÂç≥ÊúçÂä°ÊîØÊåÅ
- **[IndexedDB](https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API)**: ÊµèËßàÂô®Êú¨Âú∞Â≠òÂÇ®ÊñπÊ°à

### ÂäüËÉΩÁªÑ‰ª∂
- **[Recharts](https://recharts.org/)**: Êèê‰æõ‰∏ì‰∏öÁöÑÂõæË°®ÁªÑ‰ª∂
- **[Lucide Icons](https://lucide.dev/)**: Êèê‰æõÁ≤æÁæéÁöÑÂõæÊ†áÂ∫ì
- **[Sonner](https://sonner.emilkowal.ski/)**: Êèê‰æõ‰ºòÈõÖÁöÑÈÄöÁü•ÁªÑ‰ª∂
- **[fflate](https://github.com/101arrowz/fflate)**: ZIPÊñá‰ª∂Â§ÑÁêÜ

### ÁâπÂà´ÊÑüË∞¢
- ÊÑüË∞¢ÊâÄÊúâ‰∏∫Êú¨È°πÁõÆÊèê‰∫§ Issue Âíå Pull Request ÁöÑË¥°ÁåÆËÄÖ
- ÊÑüË∞¢ÊâÄÊúâ Star Êú¨È°πÁõÆÁöÑÂºÄÂèëËÄÖ
- ÊÑüË∞¢ÂºÄÊ∫êÁ§æÂå∫ÁöÑÊó†ÁßÅÂàÜ‰∫´Á≤æÁ•û
- ‰ª•ÂèäÊâÄÊúâÊú¨È°πÁõÆÊâÄ‰ΩøÁî®ÁöÑÂºÄÊ∫êËΩØ‰ª∂ÁöÑ‰ΩúËÄÖ‰ª¨ÔºÅ

## üë• Ë¥°ÁåÆËÄÖ

ÊÑüË∞¢‰ª•‰∏ã‰ºòÁßÄÁöÑË¥°ÁåÆËÄÖ‰ª¨Ôºå‰ªñ‰ª¨ËÆ© XCodeReviewer Êõ¥Âº∫Â§ßÔºÅ

[![Contributors](https://contrib.rocks/image?re

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[TheAlgorithms/Python]]></title>
            <link>https://github.com/TheAlgorithms/Python</link>
            <guid>https://github.com/TheAlgorithms/Python</guid>
            <pubDate>Fri, 27 Feb 2026 00:08:04 GMT</pubDate>
            <description><![CDATA[All Algorithms implemented in Python]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/TheAlgorithms/Python">TheAlgorithms/Python</a></h1>
            <p>All Algorithms implemented in Python</p>
            <p>Language: Python</p>
            <p>Stars: 218,175</p>
            <p>Forks: 50,100</p>
            <p>Stars today: 107 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;!-- Title: --&gt;
  &lt;a href=&quot;https://github.com/TheAlgorithms/&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/TheAlgorithms/website/1cd824df116b27029f17c2d1b42d81731f28a920/public/logo.svg&quot; height=&quot;100&quot;&gt;
  &lt;/a&gt;
  &lt;h1&gt;&lt;a href=&quot;https://github.com/TheAlgorithms/&quot;&gt;The Algorithms&lt;/a&gt; - Python&lt;/h1&gt;

&lt;!-- Labels: --&gt;
  &lt;!-- First row: --&gt;
  &lt;a href=&quot;https://gitpod.io/#https://github.com/TheAlgorithms/Python&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;Gitpod Ready-to-Code&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/TheAlgorithms/Python/blob/master/CONTRIBUTING.md&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/static/v1.svg?label=Contributions&amp;message=Welcome&amp;color=0059b3&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;Contributions Welcome&quot;&gt;
  &lt;/a&gt;
  &lt;img src=&quot;https://img.shields.io/github/repo-size/TheAlgorithms/Python.svg?label=Repo%20size&amp;style=flat-square&quot; height=&quot;20&quot;&gt;
  &lt;a href=&quot;https://the-algorithms.com/discord&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/discord/808045925556682782.svg?logo=discord&amp;colorB=7289DA&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;Discord chat&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://gitter.im/TheAlgorithms/community&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Chat-Gitter-ff69b4.svg?label=Chat&amp;logo=gitter&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;Gitter chat&quot;&gt;
  &lt;/a&gt;

  &lt;!-- Second row: --&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://github.com/TheAlgorithms/Python/actions&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/TheAlgorithms/Python/build.yml?branch=master&amp;label=CI&amp;logo=github&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;GitHub Workflow Status&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/pre-commit/pre-commit&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;logoColor=white&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;pre-commit&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://docs.astral.sh/ruff/formatter/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/static/v1?label=code%20style&amp;message=ruff&amp;color=black&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;code style: black&quot;&gt;
  &lt;/a&gt;

&lt;!-- Short description: --&gt;
  &lt;h3&gt;All algorithms implemented in Python - for education üìö&lt;/h3&gt;
&lt;/div&gt;

Implementations are for learning purposes only. They may be less efficient than the implementations in the Python standard library. Use them at your discretion.

## üöÄ Getting Started

üìã Read through our [Contribution Guidelines](CONTRIBUTING.md) before you contribute.

## üåê Community Channels

We are on [Discord](https://the-algorithms.com/discord) and [Gitter](https://gitter.im/TheAlgorithms/community)! Community channels are a great way for you to ask questions and get help. Please join us!

## üìú List of Algorithms

See our [directory](DIRECTORY.md) for easier navigation and a better overview of the project.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[qodo-ai/pr-agent]]></title>
            <link>https://github.com/qodo-ai/pr-agent</link>
            <guid>https://github.com/qodo-ai/pr-agent</guid>
            <pubDate>Fri, 27 Feb 2026 00:08:03 GMT</pubDate>
            <description><![CDATA[üöÄ PR Agent - The Original Open-Source PR Reviewer, This repo is not the Qodo free tier! Try the free version on our website.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/qodo-ai/pr-agent">qodo-ai/pr-agent</a></h1>
            <p>üöÄ PR Agent - The Original Open-Source PR Reviewer, This repo is not the Qodo free tier! Try the free version on our website.</p>
            <p>Language: Python</p>
            <p>Stars: 10,304</p>
            <p>Forks: 1,309</p>
            <p>Stars today: 20 stars today</p>
            <h2>README</h2><pre>&lt;a href=&quot;https://github.com/Codium-ai/pr-agent/commits/main&quot;&gt;
&lt;img alt=&quot;GitHub&quot; src=&quot;https://img.shields.io/github/last-commit/Codium-ai/pr-agent/main?style=for-the-badge&quot; height=&quot;20&quot;&gt;
&lt;/a&gt;

&lt;br /&gt;

# üöÄ PR Agent - The Original Open-Source PR Reviewer.

 This repository contains the open-source PR Agent Project. 
 It is not the Qodo free tier.
 
Try the free version on our website.

üëâ[Get Started Now](www.qodo.ai/get-started/)

PR-Agent is an open-source, AI-powered code review agent and a community-maintained legacy project of Qodo. It is distinct from Qodo‚Äôs primary AI code review offering, which provides a feature-rich, context-aware experience. Qodo now offers a free tier that integrates seamlessly with GitHub, GitLab, Bitbucket, and Azure DevOps for high-quality automated reviews.

## Table of Contents

- [Getting Started](#getting-started)
- [Why Use PR-Agent?](#why-use-pr-agent)
- [Features](#features)
- [See It in Action](#see-it-in-action)
- [Try It Now](#try-it-now)
- [How It Works](#how-it-works)
- [Data Privacy](#data-privacy)
- [Contributing](#contributing)

## Getting Started

### üöÄ Quick Start for PR-Agent

#### 1. Try it Instantly (No Setup)
Test PR-Agent on any public GitHub repository by commenting `@CodiumAI-Agent /improve`

#### 2. GitHub Action (Recommended)
Add automated PR reviews to your repository with a simple workflow file:
```yaml
# .github/workflows/pr-agent.yml
name: PR Agent
on:
  pull_request:
    types: [opened, synchronize]
jobs:
  pr_agent_job:
    runs-on: ubuntu-latest
    steps:
    - name: PR Agent action step
      uses: Codium-ai/pr-agent@main
      env:
        OPENAI_KEY: ${{ secrets.OPENAI_KEY }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
```
[Full GitHub Action setup guide](https://qodo-merge-docs.qodo.ai/installation/github/#run-as-a-github-action)

#### 3. CLI Usage (Local Development)
Run PR-Agent locally on your repository:
```bash
pip install pr-agent
export OPENAI_KEY=your_key_here
pr-agent --pr_url https://github.com/owner/repo/pull/123 review
```
[Complete CLI setup guide](https://qodo-merge-docs.qodo.ai/usage-guide/automations_and_usage/#local-repo-cli)

#### 4. Other Platforms
- [GitLab webhook setup](https://qodo-merge-docs.qodo.ai/installation/gitlab/)
- [BitBucket app installation](https://qodo-merge-docs.qodo.ai/installation/bitbucket/)
- [Azure DevOps setup](https://qodo-merge-docs.qodo.ai/installation/azure/)

[//]: # (## News and Updates)

[//]: # ()
[//]: # (## Aug 8, 2025)

[//]: # ()
[//]: # (Added full support for GPT-5 models. View the [benchmark results]&amp;#40;https://qodo-merge-docs.qodo.ai/pr_benchmark/#pr-benchmark-results&amp;#41; for details on the performance of GPT-5 models in PR-Agent.)

[//]: # ()
[//]: # ()
[//]: # (## Jul 17, 2025)

[//]: # ()
[//]: # (Introducing `/compliance`, a new Qodo Merge üíé tool that runs comprehensive checks for security, ticket requirements, codebase duplication, and custom organizational rules. )

[//]: # ()
[//]: # (&lt;img width=&quot;384&quot; alt=&quot;compliance-image&quot; src=&quot;https://codium.ai/images/pr_agent/compliance_partial.png&quot;/&gt;)

[//]: # ()
[//]: # (Read more about it [here]&amp;#40;https://qodo-merge-docs.qodo.ai/tools/compliance/&amp;#41;)

[//]: # ()
[//]: # ()
[//]: # (## Jul 1, 2025)

[//]: # (You can now receive automatic feedback from Qodo Merge in your local IDE after each commit. Read more about it [here]&amp;#40;https://github.com/qodo-ai/agents/tree/main/agents/qodo-merge-post-commit&amp;#41;.)

[//]: # ()
[//]: # ()
[//]: # (## Jun 21, 2025)

[//]: # ()
[//]: # (v0.30 was [released]&amp;#40;https://github.com/qodo-ai/pr-agent/releases&amp;#41;)

[//]: # ()
[//]: # ()
[//]: # (## Jun 3, 2025)

[//]: # ()
[//]: # (Qodo Merge now offers a simplified free tier üíé.)

[//]: # (Organizations can use Qodo Merge at no cost, with a [monthly limit]&amp;#40;https://qodo-merge-docs.qodo.ai/installation/qodo_merge/#cloud-users&amp;#41; of 75 PR reviews per organization.)

[//]: # ()
[//]: # ()
[//]: # (## Apr 30, 2025)

[//]: # ()
[//]: # (A new feature is now available in the `/improve` tool for Qodo Merge üíé - Chat on code suggestions.)

[//]: # ()
[//]: # (&lt;img width=&quot;512&quot; alt=&quot;image&quot; src=&quot;https://codium.ai/images/pr_agent/improve_chat_on_code_suggestions_ask.png&quot; /&gt;)

[//]: # ()
[//]: # (Read more about it [here]&amp;#40;https://qodo-merge-docs.qodo.ai/tools/improve/#chat-on-code-suggestions&amp;#41;.)

[//]: # ()
[//]: # ()
[//]: # (## Apr 16, 2025)

[//]: # ()
[//]: # (New tool for Qodo Merge üíé - `/scan_repo_discussions`.)

[//]: # ()
[//]: # (&lt;img width=&quot;635&quot; alt=&quot;image&quot; src=&quot;https://codium.ai/images/pr_agent/scan_repo_discussions_2.png&quot; /&gt;)

[//]: # ()
[//]: # (Read more about it [here]&amp;#40;https://qodo-merge-docs.qodo.ai/tools/scan_repo_discussions/&amp;#41;.)

## Why Use PR-Agent?

### üéØ Built for Real Development Teams

**Fast &amp; Affordable**: Each tool (`/review`, `/improve`, `/ask`) uses a single LLM call (~30 seconds, low cost)

**Handles Any PR Size**: Our [PR Compression strategy](https://qodo-merge-docs.qodo.ai/core-abilities/#pr-compression-strategy) effectively processes both small and large PRs

**Highly Customizable**: JSON-based prompting allows easy customization of review categories and behavior via [configuration files](pr_agent/settings/configuration.toml)

**Platform Agnostic**: 
- **Git Providers**: GitHub, GitLab, BitBucket, Azure DevOps, Gitea
- **Deployment**: CLI, GitHub Actions, Docker, self-hosted, webhooks
- **AI Models**: OpenAI GPT, Claude, Deepseek, and more

**Open Source Benefits**:
- Full control over your data and infrastructure
- Customize prompts and behavior for your team&#039;s needs
- No vendor lock-in
- Community-driven development

## Features

&lt;div style=&quot;text-align:left;&quot;&gt;

PR-Agent offers comprehensive pull request functionalities integrated with various git providers:

|                                                         |                                                                                        | GitHub | GitLab | Bitbucket | Azure DevOps | Gitea |
|---------------------------------------------------------|----------------------------------------------------------------------------------------|:------:|:------:|:---------:|:------------:|:-----:|
| [TOOLS](https://qodo-merge-docs.qodo.ai/tools/)         | [Describe](https://qodo-merge-docs.qodo.ai/tools/describe/)                            |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |  ‚úÖ   |
|                                                         | [Review](https://qodo-merge-docs.qodo.ai/tools/review/)                                |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |  ‚úÖ   |
|                                                         | [Improve](https://qodo-merge-docs.qodo.ai/tools/improve/)                              |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |  ‚úÖ   |
|                                                         | [Ask](https://qodo-merge-docs.qodo.ai/tools/ask/)                                      |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |       |
|                                                         | ‚Æë [Ask on code lines](https://qodo-merge-docs.qodo.ai/tools/ask/#ask-lines)            |   ‚úÖ   |   ‚úÖ   |           |              |       |
|                                                         | [Help Docs](https://qodo-merge-docs.qodo.ai/tools/help_docs/?h=auto#auto-approval)     |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |              |       |
|                                                         | [Update CHANGELOG](https://qodo-merge-docs.qodo.ai/tools/update_changelog/)            |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |       |
|                                                         |                                                                                                                     |        |        |           |              |       |
| [USAGE](https://qodo-merge-docs.qodo.ai/usage-guide/)   | [CLI](https://qodo-merge-docs.qodo.ai/usage-guide/automations_and_usage/#local-repo-cli)                            |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |  ‚úÖ   |
|                                                         | [App / webhook](https://qodo-merge-docs.qodo.ai/usage-guide/automations_and_usage/#github-app)                      |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |  ‚úÖ   |
|                                                         | [Tagging bot](https://github.com/Codium-ai/pr-agent#try-it-now)                                                     |   ‚úÖ   |        |           |              |       |
|                                                         | [Actions](https://qodo-merge-docs.qodo.ai/installation/github/#run-as-a-github-action)                              |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |       |
|                                                         |                                                                                                                     |        |        |           |              |       |
| [CORE](https://qodo-merge-docs.qodo.ai/core-abilities/) | [Adaptive and token-aware file patch fitting](https://qodo-merge-docs.qodo.ai/core-abilities/compression_strategy/) |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |       |
|                                                         | [Chat on code suggestions](https://qodo-merge-docs.qodo.ai/core-abilities/chat_on_code_suggestions/)                |   ‚úÖ   |  ‚úÖ   |           |              |       |
|                                                         | [Dynamic context](https://qodo-merge-docs.qodo.ai/core-abilities/dynamic_context/)                                  |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |       |
|                                                         | [Fetching ticket context](https://qodo-merge-docs.qodo.ai/core-abilities/fetching_ticket_context/)                  |   ‚úÖ    |  ‚úÖ    |     ‚úÖ     |              |       |
|                                                         | [Incremental Update](https://qodo-merge-docs.qodo.ai/core-abilities/incremental_update/)                            |   ‚úÖ    |       |           |              |       |
|                                                         | [Interactivity](https://qodo-merge-docs.qodo.ai/core-abilities/interactivity/)                                      |   ‚úÖ   |  ‚úÖ   |           |              |       |
|                                                         | [Local and global metadata](https://qodo-merge-docs.qodo.ai/core-abilities/metadata/)                               |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |       |
|                                                         | [Multiple models support](https://qodo-merge-docs.qodo.ai/usage-guide/changing_a_model/)                            |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |       |
|                                                         | [PR compression](https://qodo-merge-docs.qodo.ai/core-abilities/compression_strategy/)                              |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |       |
|                                                         | [RAG context enrichment](https://qodo-merge-docs.qodo.ai/core-abilities/rag_context_enrichment/)                    |   ‚úÖ    |       |    ‚úÖ     |              |       |
|                                                         | [Self reflection](https://qodo-merge-docs.qodo.ai/core-abilities/self_reflection/)                                  |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |       |

[//]: # (- Support for additional git providers is described in [here]&amp;#40;./docs/Full_environments.md&amp;#41;)
___

## See It in Action

&lt;/div&gt;
&lt;h4&gt;&lt;a href=&quot;https://github.com/Codium-ai/pr-agent/pull/530&quot;&gt;/describe&lt;/a&gt;&lt;/h4&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;p float=&quot;center&quot;&gt;
&lt;img src=&quot;https://www.codium.ai/images/pr_agent/describe_new_short_main.png&quot; width=&quot;512&quot;&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;

&lt;h4&gt;&lt;a href=&quot;https://github.com/Codium-ai/pr-agent/pull/732#issuecomment-1975099151&quot;&gt;/review&lt;/a&gt;&lt;/h4&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;p float=&quot;center&quot;&gt;
&lt;kbd&gt;
&lt;img src=&quot;https://www.codium.ai/images/pr_agent/review_new_short_main.png&quot; width=&quot;512&quot;&gt;
&lt;/kbd&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;hr&gt;

&lt;h4&gt;&lt;a href=&quot;https://github.com/Codium-ai/pr-agent/pull/732#issuecomment-1975099159&quot;&gt;/improve&lt;/a&gt;&lt;/h4&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;p float=&quot;center&quot;&gt;
&lt;kbd&gt;
&lt;img src=&quot;https://www.codium.ai/images/pr_agent/improve_new_short_main.png&quot; width=&quot;512&quot;&gt;
&lt;/kbd&gt;
&lt;/p&gt;
&lt;/div&gt;

&lt;div align=&quot;left&quot;&gt;

&lt;/div&gt;
&lt;hr&gt;

## Try It Now

Try the GPT-5 powered PR-Agent instantly on _your public GitHub repository_. Just mention `@CodiumAI-Agent` and add the desired command in any PR comment. The agent will generate a response based on your command.
For example, add a comment to any pull request with the following text:

```
@CodiumAI-Agent /review
```

and the agent will respond with a review of your PR.

Note that this is a promotional bot, suitable only for initial experimentation.
It does not have &#039;edit&#039; access to your repo, for example, so it cannot update the PR description or add labels (`@CodiumAI-Agent /describe` will publish PR description as a comment). In addition, the bot cannot be used on private repositories, as it does not have access to the files there.


## How It Works

The following diagram illustrates PR-Agent tools and their flow:

![PR-Agent Tools](https://www.qodo.ai/images/pr_agent/diagram-v0.9.png)

## Data Privacy

### Self-hosted PR-Agent

- If you host PR-Agent with your OpenAI API key, it is between you and OpenAI. You can read their API data privacy policy here:
https://openai.com/enterprise-privacy

## Contributing

To contribute to the project, get started by reading our [Contributing Guide](https://github.com/qodo-ai/pr-agent/blob/b09eec265ef7d36c232063f76553efb6b53979ff/CONTRIBUTING.md).


## ‚ù§Ô∏è Community

This open-source release remains here as a community contribution from Qodo ‚Äî the origin of modern AI-powered code collaboration. We‚Äôre proud to share it and inspire developers worldwide.

The project now has its first external maintainer, Naor ([@naorpeled](https://github.com/naorpeled)), and is currently in the process of being donated to an open-source foundation.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[zhayujie/chatgpt-on-wechat]]></title>
            <link>https://github.com/zhayujie/chatgpt-on-wechat</link>
            <guid>https://github.com/zhayujie/chatgpt-on-wechat</guid>
            <pubDate>Fri, 27 Feb 2026 00:08:02 GMT</pubDate>
            <description><![CDATA[CowAgentÊòØÂü∫‰∫éÂ§ßÊ®°ÂûãÁöÑË∂ÖÁ∫ßAIÂä©ÁêÜÔºåËÉΩ‰∏ªÂä®ÊÄùËÄÉÂíå‰ªªÂä°ËßÑÂàí„ÄÅËÆøÈóÆÊìç‰ΩúÁ≥ªÁªüÂíåÂ§ñÈÉ®ËµÑÊ∫ê„ÄÅÂàõÈÄ†ÂíåÊâßË°åSkills„ÄÅÊã•ÊúâÈïøÊúüËÆ∞ÂøÜÂπ∂‰∏çÊñ≠ÊàêÈïø„ÄÇÂêåÊó∂ÊîØÊåÅÈ£û‰π¶„ÄÅÈíâÈíâ„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®„ÄÅÂæÆ‰ø°ÂÖ¨‰ºóÂè∑„ÄÅÁΩëÈ°µÁ≠âÊé•ÂÖ•ÔºåÂèØÈÄâÊã©OpenAI/Claude/Gemini/DeepSeek/ Qwen/GLM/Kimi/LinkAIÔºåËÉΩÂ§ÑÁêÜÊñáÊú¨„ÄÅËØ≠Èü≥„ÄÅÂõæÁâáÂíåÊñá‰ª∂ÔºåÂèØÂø´ÈÄüÊê≠Âª∫‰∏™‰∫∫AIÂä©ÊâãÂíå‰ºÅ‰∏öÊï∞Â≠óÂëòÂ∑•„ÄÇ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/zhayujie/chatgpt-on-wechat">zhayujie/chatgpt-on-wechat</a></h1>
            <p>CowAgentÊòØÂü∫‰∫éÂ§ßÊ®°ÂûãÁöÑË∂ÖÁ∫ßAIÂä©ÁêÜÔºåËÉΩ‰∏ªÂä®ÊÄùËÄÉÂíå‰ªªÂä°ËßÑÂàí„ÄÅËÆøÈóÆÊìç‰ΩúÁ≥ªÁªüÂíåÂ§ñÈÉ®ËµÑÊ∫ê„ÄÅÂàõÈÄ†ÂíåÊâßË°åSkills„ÄÅÊã•ÊúâÈïøÊúüËÆ∞ÂøÜÂπ∂‰∏çÊñ≠ÊàêÈïø„ÄÇÂêåÊó∂ÊîØÊåÅÈ£û‰π¶„ÄÅÈíâÈíâ„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®„ÄÅÂæÆ‰ø°ÂÖ¨‰ºóÂè∑„ÄÅÁΩëÈ°µÁ≠âÊé•ÂÖ•ÔºåÂèØÈÄâÊã©OpenAI/Claude/Gemini/DeepSeek/ Qwen/GLM/Kimi/LinkAIÔºåËÉΩÂ§ÑÁêÜÊñáÊú¨„ÄÅËØ≠Èü≥„ÄÅÂõæÁâáÂíåÊñá‰ª∂ÔºåÂèØÂø´ÈÄüÊê≠Âª∫‰∏™‰∫∫AIÂä©ÊâãÂíå‰ºÅ‰∏öÊï∞Â≠óÂëòÂ∑•„ÄÇ</p>
            <p>Language: Python</p>
            <p>Stars: 41,535</p>
            <p>Forks: 9,765</p>
            <p>Stars today: 59 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;img src= &quot;https://github.com/user-attachments/assets/eca9a9ec-8534-4615-9e0f-96c5ac1d10a3&quot; alt=&quot;Chatgpt-on-Wechat&quot; width=&quot;550&quot; /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;a href=&quot;https://github.com/zhayujie/chatgpt-on-wechat/releases/latest&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/zhayujie/chatgpt-on-wechat&quot; alt=&quot;Latest release&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/zhayujie/chatgpt-on-wechat/blob/master/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/zhayujie/chatgpt-on-wechat&quot; alt=&quot;License: MIT&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/zhayujie/chatgpt-on-wechat&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/zhayujie/chatgpt-on-wechat?style=flat-square&quot; alt=&quot;Stars&quot;&gt;&lt;/a&gt; &lt;br/&gt;
&lt;/p&gt;

**CowAgent** ÊòØÂü∫‰∫éÂ§ßÊ®°ÂûãÁöÑË∂ÖÁ∫ßAIÂä©ÁêÜÔºåËÉΩÂ§ü‰∏ªÂä®ÊÄùËÄÉÂíå‰ªªÂä°ËßÑÂàí„ÄÅÊìç‰ΩúËÆ°ÁÆóÊú∫ÂíåÂ§ñÈÉ®ËµÑÊ∫ê„ÄÅÂàõÈÄ†ÂíåÊâßË°åSkills„ÄÅÊã•ÊúâÈïøÊúüËÆ∞ÂøÜÂπ∂‰∏çÊñ≠ÊàêÈïø„ÄÇCowAgent ÊîØÊåÅÁÅµÊ¥ªÂàáÊç¢Â§öÁßçÊ®°ÂûãÔºåËÉΩÂ§ÑÁêÜÊñáÊú¨„ÄÅËØ≠Èü≥„ÄÅÂõæÁâá„ÄÅÊñá‰ª∂Á≠âÂ§öÊ®°ÊÄÅÊ∂àÊÅØÔºåÂèØÊé•ÂÖ•ÁΩëÈ°µ„ÄÅÈ£û‰π¶„ÄÅÈíâÈíâ„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®„ÄÅÂæÆ‰ø°ÂÖ¨‰ºóÂè∑‰∏≠‰ΩøÁî®Ôºå7*24Â∞èÊó∂ËøêË°å‰∫é‰Ω†ÁöÑ‰∏™‰∫∫ÁîµËÑëÊàñÊúçÂä°Âô®‰∏≠„ÄÇ

üìñËÉΩÂäõ‰ªãÁªçÔºö[CowAgent 2.0](/docs/agent.md)

# ÁÆÄ‰ªã

&gt; ËØ•È°πÁõÆÊó¢ÊòØ‰∏Ä‰∏™ÂèØ‰ª•ÂºÄÁÆ±Âç≥Áî®ÁöÑË∂ÖÁ∫ßAIÂä©ÁêÜÔºå‰πüÊòØ‰∏Ä‰∏™ÊîØÊåÅÈ´òÊâ©Â±ïÁöÑAgentÊ°ÜÊû∂ÔºåÂèØ‰ª•ÈÄöËøá‰∏∫È°πÁõÆÊâ©Â±ïÂ§ßÊ®°ÂûãÊé•Âè£„ÄÅÊé•ÂÖ•Ê∏†ÈÅì„ÄÅÂÜÖÁΩÆÂ∑•ÂÖ∑„ÄÅSkillsÁ≥ªÁªüÊù•ÁÅµÊ¥ªÂÆûÁé∞ÂêÑÁßçÂÆöÂà∂ÈúÄÊ±Ç„ÄÇÊ†∏ÂøÉËÉΩÂäõÂ¶Ç‰∏ãÔºö

-  ‚úÖ  **Â§çÊùÇ‰ªªÂä°ËßÑÂàí**ÔºöËÉΩÂ§üÁêÜËß£Â§çÊùÇ‰ªªÂä°Âπ∂Ëá™‰∏ªËßÑÂàíÊâßË°åÔºåÊåÅÁª≠ÊÄùËÄÉÂíåË∞ÉÁî®Â∑•ÂÖ∑Áõ¥Âà∞ÂÆåÊàêÁõÆÊ†áÔºåÊîØÊåÅÈÄöËøáÂ∑•ÂÖ∑Êìç‰ΩúËÆøÈóÆÊñá‰ª∂„ÄÅÁªàÁ´Ø„ÄÅÊµèËßàÂô®„ÄÅÂÆöÊó∂‰ªªÂä°Á≠âÁ≥ªÁªüËµÑÊ∫ê
-  ‚úÖ  **ÈïøÊúüËÆ∞ÂøÜÔºö** Ëá™Âä®Â∞ÜÂØπËØùËÆ∞ÂøÜÊåÅ‰πÖÂåñËá≥Êú¨Âú∞Êñá‰ª∂ÂíåÊï∞ÊçÆÂ∫ì‰∏≠ÔºåÂåÖÊã¨ÂÖ®Â±ÄËÆ∞ÂøÜÂíåÂ§©Á∫ßËÆ∞ÂøÜÔºåÊîØÊåÅÂÖ≥ÈîÆËØçÂèäÂêëÈáèÊ£ÄÁ¥¢
-  ‚úÖ  **ÊäÄËÉΩÁ≥ªÁªüÔºö** ÂÆûÁé∞‰∫ÜSkillsÂàõÂª∫ÂíåËøêË°åÁöÑÂºïÊìéÔºåÂÜÖÁΩÆÂ§öÁßçÊäÄËÉΩÔºåÂπ∂ÊîØÊåÅÈÄöËøáËá™ÁÑ∂ËØ≠Ë®ÄÂØπËØùÂÆåÊàêËá™ÂÆö‰πâSkillsÂºÄÂèë
-  ‚úÖ  **Â§öÊ®°ÊÄÅÊ∂àÊÅØÔºö** ÊîØÊåÅÂØπÊñáÊú¨„ÄÅÂõæÁâá„ÄÅËØ≠Èü≥„ÄÅÊñá‰ª∂Á≠âÂ§öÁ±ªÂûãÊ∂àÊÅØËøõË°åËß£Êûê„ÄÅÂ§ÑÁêÜ„ÄÅÁîüÊàê„ÄÅÂèëÈÄÅÁ≠âÊìç‰Ωú
-  ‚úÖ  **Â§öÊ®°ÂûãÊé•ÂÖ•Ôºö** ÊîØÊåÅOpenAI, Claude, Gemini, DeepSeek, MiniMax„ÄÅGLM„ÄÅQwen„ÄÅKimi„ÄÅDoubaoÁ≠âÂõΩÂÜÖÂ§ñ‰∏ªÊµÅÊ®°ÂûãÂéÇÂïÜ
-  ‚úÖ  **Â§öÁ´ØÈÉ®ÁΩ≤Ôºö** ÊîØÊåÅËøêË°åÂú®Êú¨Âú∞ËÆ°ÁÆóÊú∫ÊàñÊúçÂä°Âô®ÔºåÂèØÈõÜÊàêÂà∞ÁΩëÈ°µ„ÄÅÈ£û‰π¶„ÄÅÈíâÈíâ„ÄÅÂæÆ‰ø°ÂÖ¨‰ºóÂè∑„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®‰∏≠‰ΩøÁî®
-  ‚úÖ  **Áü•ËØÜÂ∫ìÔºö** ÈõÜÊàê‰ºÅ‰∏öÁü•ËØÜÂ∫ìËÉΩÂäõÔºåËÆ©AgentÊàê‰∏∫‰∏ìÂ±ûÊï∞Â≠óÂëòÂ∑•ÔºåÂü∫‰∫é[LinkAI](https://link-ai.tech)Âπ≥Âè∞ÂÆûÁé∞

## Â£∞Êòé

1. Êú¨È°πÁõÆÈÅµÂæ™ [MITÂºÄÊ∫êÂçèËÆÆ](/LICENSE)Ôºå‰∏ªË¶ÅÁî®‰∫éÊäÄÊúØÁ†îÁ©∂ÂíåÂ≠¶‰π†Ôºå‰ΩøÁî®Êú¨È°πÁõÆÊó∂ÈúÄÈÅµÂÆàÊâÄÂú®Âú∞Ê≥ïÂæãÊ≥ïËßÑ„ÄÅÁõ∏ÂÖ≥ÊîøÁ≠ñ‰ª•Âèä‰ºÅ‰∏öÁ´†Á®ãÔºåÁ¶ÅÊ≠¢Áî®‰∫é‰ªª‰ΩïËøùÊ≥ïÊàñ‰æµÁäØ‰ªñ‰∫∫ÊùÉÁõäÁöÑË°å‰∏∫„ÄÇ‰ªª‰Ωï‰∏™‰∫∫„ÄÅÂõ¢ÈòüÂíå‰ºÅ‰∏öÔºåÊó†ËÆ∫‰ª•‰ΩïÁßçÊñπÂºè‰ΩøÁî®ËØ•È°πÁõÆ„ÄÅÂØπ‰ΩïÂØπË±°Êèê‰æõÊúçÂä°ÔºåÊâÄ‰∫ßÁîüÁöÑ‰∏ÄÂàáÂêéÊûúÔºåÊú¨È°πÁõÆÂùá‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªª„ÄÇ
2. ÊàêÊú¨‰∏éÂÆâÂÖ®ÔºöAgentÊ®°Âºè‰∏ãToken‰ΩøÁî®ÈáèÈ´ò‰∫éÊôÆÈÄöÂØπËØùÊ®°ÂºèÔºåËØ∑Ê†πÊçÆÊïàÊûúÂèäÊàêÊú¨ÁªºÂêàÈÄâÊã©Ê®°Âûã„ÄÇAgentÂÖ∑ÊúâËÆøÈóÆÊâÄÂú®Êìç‰ΩúÁ≥ªÁªüÁöÑËÉΩÂäõÔºåËØ∑Ë∞®ÊÖéÈÄâÊã©È°πÁõÆÈÉ®ÁΩ≤ÁéØÂ¢É„ÄÇÂêåÊó∂È°πÁõÆ‰πü‰ºöÊåÅÁª≠ÂçáÁ∫ßÂÆâÂÖ®Êú∫Âà∂„ÄÅÂπ∂Èôç‰ΩéÊ®°ÂûãÊ∂àËÄóÊàêÊú¨„ÄÇ
3. CowAgentÈ°πÁõÆ‰∏ìÊ≥®‰∫éÂºÄÊ∫êÊäÄÊúØÂºÄÂèëÔºå‰∏ç‰ºöÂèÇ‰∏é„ÄÅÊéàÊùÉÊàñÂèëË°å‰ªª‰ΩïÂä†ÂØÜË¥ßÂ∏Å„ÄÇ

## ÊºîÁ§∫

‰ΩøÁî®ËØ¥Êòé(AgentÊ®°Âºè)Ôºö[CowAgent‰ªãÁªç](/docs/agent.md)

DEMOËßÜÈ¢ë(ÂØπËØùÊ®°Âºè)Ôºöhttps://cdn.link-ai.tech/doc/cow_demo.mp4

## Á§æÂå∫

Ê∑ªÂä†Â∞èÂä©ÊâãÂæÆ‰ø°Âä†ÂÖ•ÂºÄÊ∫êÈ°πÁõÆ‰∫§ÊµÅÁæ§Ôºö

&lt;img width=&quot;140&quot; src=&quot;https://img-1317903499.cos.ap-guangzhou.myqcloud.com/docs/open-community.png&quot;&gt;

&lt;br/&gt;

# ‰ºÅ‰∏öÊúçÂä°

&lt;a href=&quot;https://link-ai.tech&quot; target=&quot;_blank&quot;&gt;&lt;img width=&quot;720&quot; src=&quot;https://cdn.link-ai.tech/image/link-ai-intro.jpg&quot;&gt;&lt;/a&gt;

&gt; [LinkAI](https://link-ai.tech/) ÊòØÈù¢Âêë‰ºÅ‰∏öÂíåÂºÄÂèëËÄÖÁöÑ‰∏ÄÁ´ôÂºèAIÊô∫ËÉΩ‰ΩìÂπ≥Âè∞ÔºåËÅöÂêàÂ§öÊ®°ÊÄÅÂ§ßÊ®°Âûã„ÄÅÁü•ËØÜÂ∫ì„ÄÅAgent Êèí‰ª∂„ÄÅÂ∑•‰ΩúÊµÅÁ≠âËÉΩÂäõÔºåÊîØÊåÅ‰∏ÄÈîÆÊé•ÂÖ•‰∏ªÊµÅÂπ≥Âè∞Âπ∂ËøõË°åÁÆ°ÁêÜÔºåÊîØÊåÅSaaS„ÄÅÁßÅÊúâÂåñÈÉ®ÁΩ≤Á≠âÂ§öÁßçÊ®°Âºè„ÄÇ
&gt;
&gt; LinkAI ÁõÆÂâçÂ∑≤Âú®Êô∫ËÉΩÂÆ¢Êúç„ÄÅÁßÅÂüüËøêËê•„ÄÅ‰ºÅ‰∏öÊïàÁéáÂä©ÊâãÁ≠âÂú∫ÊôØÁßØÁ¥Ø‰∫Ü‰∏∞ÂØåÁöÑAIËß£ÂÜ≥ÊñπÊ°àÔºåÂú®Ê∂àË¥π„ÄÅÂÅ•Â∫∑„ÄÅÊñáÊïô„ÄÅÁßëÊäÄÂà∂ÈÄ†Á≠âÂêÑË°å‰∏öÊ≤âÊ∑Ä‰∫ÜÂ§ßÊ®°ÂûãËêΩÂú∞Â∫îÁî®ÁöÑÊúÄ‰Ω≥ÂÆûË∑µÔºåËá¥Âäõ‰∫éÂ∏ÆÂä©Êõ¥Â§ö‰ºÅ‰∏öÂíåÂºÄÂèëËÄÖÊã•Êä± AI Áîü‰∫ßÂäõ„ÄÇ

**‰∫ßÂìÅÂí®ËØ¢Âíå‰ºÅ‰∏öÊúçÂä°** ÂèØËÅîÁ≥ª‰∫ßÂìÅÂÆ¢ÊúçÔºö

&lt;img width=&quot;150&quot; src=&quot;https://cdn.link-ai.tech/portal/linkai-customer-service.png&quot;&gt;

&lt;br/&gt;

# üè∑ Êõ¥Êñ∞Êó•Âøó

&gt;**2026.02.03Ôºö** [2.0.0ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/2.0.0)ÔºåÊ≠£ÂºèÂçáÁ∫ß‰∏∫Ë∂ÖÁ∫ßAgentÂä©ÁêÜÔºåÊîØÊåÅÂ§öËΩÆ‰ªªÂä°ÂÜ≥Á≠ñ„ÄÅÂÖ∑Â§áÈïøÊúüËÆ∞ÂøÜ„ÄÅÂÆûÁé∞Â§öÁßçÁ≥ªÁªüÂ∑•ÂÖ∑„ÄÅÊîØÊåÅSkillsÊ°ÜÊû∂ÔºåÊñ∞Â¢ûÂ§öÁßçÊ®°ÂûãÂπ∂‰ºòÂåñ‰∫ÜÊé•ÂÖ•Ê∏†ÈÅì„ÄÇ

&gt;**2025.05.23Ôºö** [1.7.6ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.6) ‰ºòÂåñwebÁΩëÈ°µchannel„ÄÅÊñ∞Â¢û [AgentMesh](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/plugins/agent/README.md)Â§öÊô∫ËÉΩ‰ΩìÊèí‰ª∂„ÄÅÁôæÂ∫¶ËØ≠Èü≥ÂêàÊàê‰ºòÂåñ„ÄÅ‰ºÅÂæÆÂ∫îÁî®`access_token`Ëé∑Âèñ‰ºòÂåñ„ÄÅÊîØÊåÅ`claude-4-sonnet`Âíå`claude-4-opus`Ê®°Âûã

&gt;**2025.04.11Ôºö** [1.7.5ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.5) Êñ∞Â¢ûÊîØÊåÅ [wechatferry](https://github.com/zhayujie/chatgpt-on-wechat/pull/2562) ÂçèËÆÆ„ÄÅÊñ∞Â¢û deepseek Ê®°Âûã„ÄÅÊñ∞Â¢ûÊîØÊåÅËÖæËÆØ‰∫ëËØ≠Èü≥ËÉΩÂäõ„ÄÅÊñ∞Â¢ûÊîØÊåÅ ModelScope Âíå Gitee-AI APIÊé•Âè£

&gt;**2024.12.13Ôºö** [1.7.4ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.4) Êñ∞Â¢û Gemini 2.0 Ê®°Âûã„ÄÅÊñ∞Â¢ûweb channel„ÄÅËß£ÂÜ≥ÂÜÖÂ≠òÊ≥ÑÊºèÈóÆÈ¢ò„ÄÅËß£ÂÜ≥ `#reloadp` ÂëΩ‰ª§ÈáçËΩΩ‰∏çÁîüÊïàÈóÆÈ¢ò

&gt;**2024.10.31Ôºö** [1.7.3ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.3) Á®ãÂ∫èÁ®≥ÂÆöÊÄßÊèêÂçá„ÄÅÊï∞ÊçÆÂ∫ìÂäüËÉΩ„ÄÅClaudeÊ®°Âûã‰ºòÂåñ„ÄÅlinkaiÊèí‰ª∂‰ºòÂåñ„ÄÅÁ¶ªÁ∫øÈÄöÁü•

Êõ¥Â§öÊõ¥Êñ∞ÂéÜÂè≤ËØ∑Êü•Áúã: [Êõ¥Êñ∞Êó•Âøó](/docs/release/history.md)

&lt;br/&gt;

# üöÄ Âø´ÈÄüÂºÄÂßã

È°πÁõÆÊèê‰æõ‰∫Ü‰∏ÄÈîÆÂÆâË£Ö„ÄÅÈÖçÁΩÆ„ÄÅÂêØÂä®„ÄÅÁÆ°ÁêÜÁ®ãÂ∫èÁöÑËÑöÊú¨ÔºåÊé®Ëçê‰ΩøÁî®ËÑöÊú¨Âø´ÈÄüËøêË°åÔºå‰πüÂèØ‰ª•Ê†πÊçÆ‰∏ãÊñá‰∏≠ÁöÑËØ¶ÁªÜÊåáÂºï‰∏ÄÊ≠•Ê≠•ÂÆâË£ÖËøêË°å„ÄÇ

Âú®ÁªàÁ´ØÊâßË°å‰ª•‰∏ãÂëΩ‰ª§Ôºö

```bash
bash &lt;(curl -sS https://cdn.link-ai.tech/code/cow/run.sh)
```

ËÑöÊú¨‰ΩøÁî®ËØ¥ÊòéÔºö[‰∏ÄÈîÆËøêË°åËÑöÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/wiki/CowAgentQuickStart)


## ‰∏Ä„ÄÅÂáÜÂ§á

### 1. Ê®°ÂûãAPI

È°πÁõÆÊîØÊåÅÂõΩÂÜÖÂ§ñ‰∏ªÊµÅÂéÇÂïÜÁöÑÊ®°ÂûãÊé•Âè£ÔºåÂèØÈÄâÊ®°ÂûãÂèäÈÖçÁΩÆËØ¥ÊòéÂèÇËÄÉÔºö[Ê®°ÂûãËØ¥Êòé](#Ê®°ÂûãËØ¥Êòé)„ÄÇ

&gt; Ê≥®ÔºöAgentÊ®°Âºè‰∏ãÊé®Ëçê‰ΩøÁî®‰ª•‰∏ãÊ®°ÂûãÔºåÂèØÊ†πÊçÆÊïàÊûúÂèäÊàêÊú¨ÁªºÂêàÈÄâÊã©ÔºöMiniMax-M2.5„ÄÅglm-5„ÄÅkimi-k2.5„ÄÅqwen3.5-plus„ÄÅclaude-sonnet-4-6„ÄÅgemini-3.1-pro-preview

ÂêåÊó∂ÊîØÊåÅ‰ΩøÁî® **LinkAIÂπ≥Âè∞** Êé•Âè£ÔºåÂèØÁÅµÊ¥ªÂàáÊç¢ OpenAI„ÄÅClaude„ÄÅGemini„ÄÅDeepSeek„ÄÅQwen„ÄÅKimi Á≠âÂ§öÁßçÂ∏∏Áî®Ê®°ÂûãÔºåÂπ∂ÊîØÊåÅÁü•ËØÜÂ∫ì„ÄÅÂ∑•‰ΩúÊµÅ„ÄÅÊèí‰ª∂Á≠âAgentËÉΩÂäõÔºåÂèÇËÄÉ [Êé•Âè£ÊñáÊ°£](https://docs.link-ai.tech/platform/api)„ÄÇ

### 2.ÁéØÂ¢ÉÂÆâË£Ö

ÊîØÊåÅ Linux„ÄÅMacOS„ÄÅWindows Êìç‰ΩúÁ≥ªÁªüÔºåÂèØÂú®‰∏™‰∫∫ËÆ°ÁÆóÊú∫ÂèäÊúçÂä°Âô®‰∏äËøêË°åÔºåÈúÄÂÆâË£Ö `Python`ÔºåPythonÁâàÊú¨ÈúÄÂú®3.7 ~ 3.12 ‰πãÈó¥ÔºåÊé®Ëçê‰ΩøÁî®3.9ÁâàÊú¨„ÄÇ

&gt; Ê≥®ÊÑèÔºöAgentÊ®°ÂºèÊé®Ëçê‰ΩøÁî®Ê∫êÁ†ÅËøêË°åÔºåËã•ÈÄâÊã©DockerÈÉ®ÁΩ≤ÂàôÊó†ÈúÄÂÆâË£ÖpythonÁéØÂ¢ÉÂíå‰∏ãËΩΩÊ∫êÁ†ÅÔºåÂèØÁõ¥Êé•Âø´ËøõÂà∞‰∏ã‰∏ÄËäÇ„ÄÇ

**(1) ÂÖãÈöÜÈ°πÁõÆ‰ª£Á†ÅÔºö**

```bash
git clone https://github.com/zhayujie/chatgpt-on-wechat
cd chatgpt-on-wechat/
```

Ëã•ÈÅáÂà∞ÁΩëÁªúÈóÆÈ¢òÂèØ‰ΩøÁî®ÂõΩÂÜÖ‰ªìÂ∫ìÂú∞ÂùÄÔºöhttps://gitee.com/zhayujie/chatgpt-on-wechat

**(2) ÂÆâË£ÖÊ†∏ÂøÉ‰æùËµñ (ÂøÖÈÄâ)Ôºö**

```bash
pip3 install -r requirements.txt
```

**(3) ÊãìÂ±ï‰æùËµñ (ÂèØÈÄâÔºåÂª∫ËÆÆÂÆâË£Ö)Ôºö**

```bash
pip3 install -r requirements-optional.txt
```
Â¶ÇÊûúÊüêÈ°π‰æùËµñÂÆâË£ÖÂ§±Ë¥•ÂèØÊ≥®ÈáäÊéâÂØπÂ∫îÁöÑË°åÂêéÈáçËØï„ÄÇ

## ‰∫å„ÄÅÈÖçÁΩÆ

ÈÖçÁΩÆÊñá‰ª∂ÁöÑÊ®°ÊùøÂú®Ê†πÁõÆÂΩïÁöÑ`config-template.json`‰∏≠ÔºåÈúÄÂ§çÂà∂ËØ•Ê®°ÊùøÂàõÂª∫ÊúÄÁªàÁîüÊïàÁöÑ `config.json` Êñá‰ª∂Ôºö

```bash
  cp config-template.json config.json
```

ÁÑ∂ÂêéÂú®`config.json`‰∏≠Â°´ÂÖ•ÈÖçÁΩÆÔºå‰ª•‰∏ãÊòØÂØπÈªòËÆ§ÈÖçÁΩÆÁöÑËØ¥ÊòéÔºåÂèØÊ†πÊçÆÈúÄË¶ÅËøõË°åËá™ÂÆö‰πâ‰øÆÊîπÔºàÊ≥®ÊÑèÂÆûÈôÖ‰ΩøÁî®Êó∂ËØ∑ÂéªÊéâÊ≥®ÈáäÔºå‰øùËØÅJSONÊ†ºÂºèÁöÑËßÑËåÉÔºâÔºö

```bash
# config.json Êñá‰ª∂ÂÜÖÂÆπÁ§∫‰æã
{
  &quot;channel_type&quot;: &quot;web&quot;,                                      # Êé•ÂÖ•Ê∏†ÈÅìÁ±ªÂûãÔºåÈªòËÆ§‰∏∫webÔºåÊîØÊåÅ‰øÆÊîπ‰∏∫:feishu,dingtalk,wechatcom_app,terminal,wechatmp,wechatmp_service
  &quot;model&quot;: &quot;MiniMax-M2.5&quot;,                                    # Ê®°ÂûãÂêçÁß∞
  &quot;minimax_api_key&quot;: &quot;&quot;,                                      # MiniMax API Key
  &quot;zhipu_ai_api_key&quot;: &quot;&quot;,                                     # Êô∫Ë∞±GLM API Key
  &quot;moonshot_api_key&quot;: &quot;&quot;,                                     # Kimi/Moonshot API Key
  &quot;ark_api_key&quot;: &quot;&quot;,                                          # Ë±ÜÂåÖ(ÁÅ´Â±±ÊñπËàü) API Key
  &quot;dashscope_api_key&quot;: &quot;&quot;,                                    # ÁôæÁÇº(ÈÄö‰πâÂçÉÈóÆ)API Key
  &quot;claude_api_key&quot;: &quot;&quot;,                                       # Claude API Key
  &quot;claude_api_base&quot;: &quot;https://api.anthropic.com/v1&quot;,          # Claude API Âú∞ÂùÄÔºå‰øÆÊîπÂèØÊé•ÂÖ•‰∏âÊñπ‰ª£ÁêÜÂπ≥Âè∞
  &quot;gemini_api_key&quot;: &quot;&quot;,                                       # Gemini API Key
  &quot;gemini_api_base&quot;: &quot;https://generativelanguage.googleapis.com&quot;, # Gemini APIÂú∞ÂùÄ
  &quot;open_ai_api_key&quot;: &quot;&quot;,                                      # OpenAI API Key
  &quot;open_ai_api_base&quot;: &quot;https://api.openai.com/v1&quot;,            # OpenAI API Âú∞ÂùÄ
  &quot;linkai_api_key&quot;: &quot;&quot;,                                       # LinkAI API Key
  &quot;proxy&quot;: &quot;&quot;,                                                # ‰ª£ÁêÜÂÆ¢Êà∑Á´ØÁöÑipÂíåÁ´ØÂè£ÔºåÂõΩÂÜÖÁéØÂ¢ÉÈúÄË¶ÅÂºÄÂêØ‰ª£ÁêÜÁöÑÂèØÂ°´ÂÜôËØ•È°πÔºåÂ¶Ç &quot;127.0.0.1:7890&quot;
  &quot;speech_recognition&quot;: false,                                # ÊòØÂê¶ÂºÄÂêØËØ≠Èü≥ËØÜÂà´
  &quot;group_speech_recognition&quot;: false,                          # ÊòØÂê¶ÂºÄÂêØÁæ§ÁªÑËØ≠Èü≥ËØÜÂà´
  &quot;voice_reply_voice&quot;: false,                                 # ÊòØÂê¶‰ΩøÁî®ËØ≠Èü≥ÂõûÂ§çËØ≠Èü≥
  &quot;use_linkai&quot;: false,                                        # ÊòØÂê¶‰ΩøÁî®LinkAIÊé•Âè£ÔºåÈªòËÆ§ÂÖ≥Èó≠ÔºåËÆæÁΩÆ‰∏∫trueÂêéÂèØÂØπÊé•LinkAIÂπ≥Âè∞Êé•Âè£
  &quot;agent&quot;: true,                                              # ÊòØÂê¶ÂêØÁî®AgentÊ®°ÂºèÔºåÂêØÁî®ÂêéÊã•ÊúâÂ§öËΩÆÂ∑•ÂÖ∑ÂÜ≥Á≠ñ„ÄÅÈïøÊúüËÆ∞ÂøÜ„ÄÅSkillsËÉΩÂäõÁ≠â
  &quot;agent_workspace&quot;: &quot;~/cow&quot;,                                 # AgentÁöÑÂ∑•‰ΩúÁ©∫Èó¥Ë∑ØÂæÑÔºåÁî®‰∫éÂ≠òÂÇ®memory„ÄÅskills„ÄÅÁ≥ªÁªüËÆæÂÆöÁ≠â
  &quot;agent_max_context_tokens&quot;: 40000,                          # AgentÊ®°Âºè‰∏ãÊúÄÂ§ß‰∏ä‰∏ãÊñátokensÔºåË∂ÖÂá∫Â∞ÜËá™Âä®‰∏¢ÂºÉÊúÄÊó©ÁöÑ‰∏ä‰∏ãÊñá
  &quot;agent_max_context_turns&quot;: 30,                              # AgentÊ®°Âºè‰∏ãÊúÄÂ§ß‰∏ä‰∏ãÊñáËÆ∞ÂøÜËΩÆÊ¨°ÔºåÊØèËΩÆÂåÖÊã¨‰∏ÄÊ¨°Áî®Êà∑ÊèêÈóÆÂíåAIÂõûÂ§ç
  &quot;agent_max_steps&quot;: 15                                       # AgentÊ®°Âºè‰∏ãÂçïÊ¨°‰ªªÂä°ÁöÑÊúÄÂ§ßÂÜ≥Á≠ñÊ≠•Êï∞ÔºåË∂ÖÂá∫ÂêéÂ∞ÜÂÅúÊ≠¢ÁªßÁª≠Ë∞ÉÁî®Â∑•ÂÖ∑
}
```

**ÈÖçÁΩÆË°•ÂÖÖËØ¥Êòé:** 

&lt;details&gt;
&lt;summary&gt;1. ËØ≠Èü≥ÈÖçÁΩÆ&lt;/summary&gt;

+ Ê∑ªÂä† `&quot;speech_recognition&quot;: true` Â∞ÜÂºÄÂêØËØ≠Èü≥ËØÜÂà´ÔºåÈªòËÆ§‰ΩøÁî®openaiÁöÑwhisperÊ®°ÂûãËØÜÂà´‰∏∫ÊñáÂ≠óÔºåÂêåÊó∂‰ª•ÊñáÂ≠óÂõûÂ§çÔºåËØ•ÂèÇÊï∞‰ªÖÊîØÊåÅÁßÅËÅä (Ê≥®ÊÑèÁî±‰∫éËØ≠Èü≥Ê∂àÊÅØÊó†Ê≥ïÂåπÈÖçÂâçÁºÄÔºå‰∏ÄÊó¶ÂºÄÂêØÂ∞ÜÂØπÊâÄÊúâËØ≠Èü≥Ëá™Âä®ÂõûÂ§çÔºåÊîØÊåÅËØ≠Èü≥Ëß¶ÂèëÁîªÂõæ)Ôºõ
+ Ê∑ªÂä† `&quot;group_speech_recognition&quot;: true` Â∞ÜÂºÄÂêØÁæ§ÁªÑËØ≠Èü≥ËØÜÂà´ÔºåÈªòËÆ§‰ΩøÁî®openaiÁöÑwhisperÊ®°ÂûãËØÜÂà´‰∏∫ÊñáÂ≠óÔºåÂêåÊó∂‰ª•ÊñáÂ≠óÂõûÂ§çÔºåÂèÇÊï∞‰ªÖÊîØÊåÅÁæ§ËÅä (‰ºöÂåπÈÖçgroup_chat_prefixÂíågroup_chat_keyword, ÊîØÊåÅËØ≠Èü≥Ëß¶ÂèëÁîªÂõæ)Ôºõ
+ Ê∑ªÂä† `&quot;voice_reply_voice&quot;: true` Â∞ÜÂºÄÂêØËØ≠Èü≥ÂõûÂ§çËØ≠Èü≥ÔºàÂêåÊó∂‰ΩúÁî®‰∫éÁßÅËÅäÂíåÁæ§ËÅäÔºâ
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;2. ÂÖ∂‰ªñÈÖçÁΩÆ&lt;/summary&gt;

+ `model`: Ê®°ÂûãÂêçÁß∞ÔºåAgentÊ®°Âºè‰∏ãÊé®Ëçê‰ΩøÁî® `MiniMax-M2.5`„ÄÅ`glm-5`„ÄÅ`kimi-k2.5`„ÄÅ`qwen3.5-plus`„ÄÅ`claude-sonnet-4-6`„ÄÅ`gemini-3.1-pro-preview`ÔºåÂÖ®ÈÉ®Ê®°ÂûãÂêçÁß∞ÂèÇËÄÉ[common/const.py](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/common/const.py)Êñá‰ª∂
+ `character_desc`ÔºöÊôÆÈÄöÂØπËØùÊ®°Âºè‰∏ãÁöÑÊú∫Âô®‰∫∫Á≥ªÁªüÊèêÁ§∫ËØç„ÄÇÂú®AgentÊ®°Âºè‰∏ãËØ•ÈÖçÁΩÆ‰∏çÁîüÊïàÔºåÁî±Â∑•‰ΩúÁ©∫Èó¥‰∏≠ÁöÑÊñá‰ª∂ÂÜÖÂÆπÊûÑÊàê„ÄÇ
+ `subscribe_msg`ÔºöËÆ¢ÈòÖÊ∂àÊÅØÔºåÂÖ¨‰ºóÂè∑Âíå‰ºÅ‰∏öÂæÆ‰ø°channel‰∏≠ËØ∑Â°´ÂÜôÔºåÂΩìË¢´ËÆ¢ÈòÖÊó∂‰ºöËá™Âä®ÂõûÂ§çÔºå ÂèØ‰ΩøÁî®ÁâπÊÆäÂç†‰ΩçÁ¨¶„ÄÇÁõÆÂâçÊîØÊåÅÁöÑÂç†‰ΩçÁ¨¶Êúâ{trigger_prefix}ÔºåÂú®Á®ãÂ∫è‰∏≠ÂÆÉ‰ºöËá™Âä®ÊõøÊç¢ÊàêbotÁöÑËß¶ÂèëËØç„ÄÇ
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;3. LinkAIÈÖçÁΩÆ&lt;/summary&gt;

+ `use_linkai`: ÊòØÂê¶‰ΩøÁî®LinkAIÊé•Âè£ÔºåÈªòËÆ§ÂÖ≥Èó≠ÔºåËÆæÁΩÆ‰∏∫trueÂêéÂèØÂØπÊé•LinkAIÂπ≥Âè∞Ôºå‰ΩøÁî®Áü•ËØÜÂ∫ì„ÄÅÂ∑•‰ΩúÊµÅ„ÄÅÊèí‰ª∂Á≠âËÉΩÂäõ, ÂèÇËÄÉ[Êé•Âè£ÊñáÊ°£](https://docs.link-ai.tech/platform/api/chat)
+ `linkai_api_key`: LinkAI Api KeyÔºåÂèØÂú® [ÊéßÂà∂Âè∞](https://link-ai.tech/console/interface) ÂàõÂª∫
+ `linkai_app_code`: LinkAI Â∫îÁî®ÊàñÂ∑•‰ΩúÊµÅÁöÑcodeÔºåÈÄâÂ°´ÔºåÊôÆÈÄöÂØπËØùÊ®°Âºè‰∏≠‰ΩøÁî®„ÄÇ
&lt;/details&gt;

Ê≥®ÔºöÂÖ®ÈÉ®ÈÖçÁΩÆÈ°πËØ¥ÊòéÂèØÂú® [`config.py`](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/config.py) Êñá‰ª∂‰∏≠Êü•Áúã„ÄÇ

## ‰∏â„ÄÅËøêË°å

### 1.Êú¨Âú∞ËøêË°å

Â¶ÇÊûúÊòØ‰∏™‰∫∫ËÆ°ÁÆóÊú∫ **Êú¨Âú∞ËøêË°å**ÔºåÁõ¥Êé•Âú®È°πÁõÆÊ†πÁõÆÂΩï‰∏ãÊâßË°åÔºö

```bash
python3 app.py         # windowsÁéØÂ¢É‰∏ãËØ•ÂëΩ‰ª§ÈÄöÂ∏∏‰∏∫ python app.py
```

ËøêË°åÂêéÈªòËÆ§‰ºöÂêØÂä®webÊúçÂä°ÔºåÂèØÈÄöËøáËÆøÈóÆ `http://localhost:9899/chat` Âú®ÁΩëÈ°µÁ´ØÂØπËØù„ÄÇ

Â¶ÇÊûúÈúÄË¶ÅÊé•ÂÖ•ÂÖ∂‰ªñÂ∫îÁî®ÈÄöÈÅìÂè™ÈúÄ‰øÆÊîπ `config.json` ÈÖçÁΩÆÊñá‰ª∂‰∏≠ÁöÑ `channel_type` ÂèÇÊï∞ÔºåËØ¶ÊÉÖÂèÇËÄÉÔºö[ÈÄöÈÅìËØ¥Êòé](#ÈÄöÈÅìËØ¥Êòé)„ÄÇ


### 2.ÊúçÂä°Âô®ÈÉ®ÁΩ≤

Âú®ÊúçÂä°Âô®‰∏≠ÂèØ‰ΩøÁî® `nohup` ÂëΩ‰ª§Âú®ÂêéÂè∞ËøêË°åÁ®ãÂ∫èÔºö

```bash
nohup python3 app.py &amp; tail -f nohup.out
```

ÊâßË°åÂêéÁ®ãÂ∫èËøêË°å‰∫éÊúçÂä°Âô®ÂêéÂè∞ÔºåÂèØÈÄöËøá `ctrl+c` ÂÖ≥Èó≠Êó•ÂøóÔºå‰∏ç‰ºöÂΩ±ÂìçÂêéÂè∞Á®ãÂ∫èÁöÑËøêË°å„ÄÇ‰ΩøÁî® `ps -ef | grep app.py | grep -v grep` ÂëΩ‰ª§ÂèØÊü•ÁúãËøêË°å‰∫éÂêéÂè∞ÁöÑËøõÁ®ãÔºåÂ¶ÇÊûúÊÉ≥Ë¶ÅÈáçÊñ∞ÂêØÂä®Á®ãÂ∫èÂèØ‰ª•ÂÖà `kill` ÊéâÂØπÂ∫îÁöÑËøõÁ®ã„ÄÇ Êó•ÂøóÂÖ≥Èó≠ÂêéÂ¶ÇÊûúÊÉ≥Ë¶ÅÂÜçÊ¨°ÊâìÂºÄÂè™ÈúÄËæìÂÖ• `tail -f nohup.out`„ÄÇ 

Ê≠§Â§ñÔºåÈ°πÁõÆÁöÑ `scripts` ÁõÆÂΩï‰∏ãÊúâ‰∏ÄÈîÆËøêË°å„ÄÅÂÖ≥Èó≠Á®ãÂ∫èÁöÑËÑöÊú¨‰æõ‰ΩøÁî®„ÄÇ ËøêË°åÂêéÈªòËÆ§channel‰∏∫webÔºåÈÄöËøáÂèØ‰ª•ÈÄöËøá‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂ËøõË°åÂàáÊç¢„ÄÇ


### 3.DockerÈÉ®ÁΩ≤

‰ΩøÁî®dockerÈÉ®ÁΩ≤Êó†ÈúÄ‰∏ãËΩΩÊ∫êÁ†ÅÂíåÂÆâË£Ö‰æùËµñÔºåÂè™ÈúÄË¶ÅËé∑Âèñ `docker-compose.yml` ÈÖçÁΩÆÊñá‰ª∂Âπ∂ÂêØÂä®ÂÆπÂô®Âç≥ÂèØ„ÄÇAgentÊ®°Âºè‰∏ãÊõ¥Êé®Ëçê‰ΩøÁî®Ê∫êÁ†ÅËøõË°åÈÉ®ÁΩ≤Ôºå‰ª•Ëé∑ÂæóÊõ¥Â§öÁ≥ªÁªüËÆøÈóÆËÉΩÂäõ„ÄÇ

&gt; ÂâçÊèêÊòØÈúÄË¶ÅÂÆâË£ÖÂ•Ω `docker` Âèä `docker-compose`ÔºåÂÆâË£ÖÊàêÂäüÂêéÊâßË°å `docker -v` Âíå `docker-compose version` (Êàñ `docker compose version`) ÂèØÊü•ÁúãÂà∞ÁâàÊú¨Âè∑„ÄÇÂÆâË£ÖÂú∞ÂùÄ‰∏∫ [dockerÂÆòÁΩë](https://docs.docker.com/engine/install/) „ÄÇ

**(1) ‰∏ãËΩΩ docker-compose.yml Êñá‰ª∂**

```bash
wget https://cdn.link-ai.tech/code/cow/docker-compose.yml
```

‰∏ãËΩΩÂÆåÊàêÂêéÊâìÂºÄ `docker-compose.yml` Â°´ÂÜôÊâÄÈúÄÈÖçÁΩÆÔºå‰æãÂ¶Ç `CHANNEL_TYPE`„ÄÅ`OPEN_AI_API_KEY` ÂíåÁ≠âÈÖçÁΩÆ„ÄÇ

**(2) ÂêØÂä®ÂÆπÂô®**

Âú® `docker-compose.yml` ÊâÄÂú®ÁõÆÂΩï‰∏ãÊâßË°å‰ª•‰∏ãÂëΩ‰ª§ÂêØÂä®ÂÆπÂô®Ôºö

```bash
sudo docker compose up -d         # Ëã•docker-compose‰∏∫ 1.X ÁâàÊú¨ÔºåÂàôÊâßË°å `sudo  docker-compose up -d`
```

ËøêË°åÂëΩ‰ª§ÂêéÔºå‰ºöËá™Âä®Âèñ [docker hub](https://hub.docker.com/r/zhayujie/chatgpt-on-wechat) ÊãâÂèñÊúÄÊñ∞releaseÁâàÊú¨ÁöÑÈïúÂÉè„ÄÇÂΩìÊâßË°å `sudo docker ps` ËÉΩÊü•ÁúãÂà∞ NAMES ‰∏∫ chatgpt-on-wechat ÁöÑÂÆπÂô®Âç≥Ë°®Á§∫ËøêË°åÊàêÂäü„ÄÇÊúÄÂêéÊâßË°å‰ª•‰∏ãÂëΩ‰ª§ÂèØÊü•ÁúãÂÆπÂô®ÁöÑËøêË°åÊó•ÂøóÔºö

```bash
sudo docker logs -f chatgpt-on-wechat
```

**(3) Êèí‰ª∂‰ΩøÁî®**

Â¶ÇÊûúÈúÄË¶ÅÂú®dockerÂÆπÂô®‰∏≠‰øÆÊîπÊèí‰ª∂ÈÖçÁΩÆÔºåÂèØÈÄöËøáÊåÇËΩΩÁöÑÊñπÂºèÂÆåÊàêÔºåÂ∞Ü [Êèí‰ª∂ÈÖçÁΩÆÊñá‰ª∂](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/plugins/config.json.template)
ÈáçÂëΩÂêç‰∏∫ `config.json`ÔºåÊîæÁΩÆ‰∫é `docker-compose.yml` Áõ∏ÂêåÁõÆÂΩï‰∏ãÔºåÂπ∂Âú® `docker-compose.yml` ‰∏≠ÁöÑ `chatgpt-on-wechat` ÈÉ®ÂàÜ‰∏ãÊ∑ªÂä† `volumes` Êò†Â∞Ñ:

```
volumes:
  - ./config.json:/app/plugins/config.json
```
**Ê≥®**Ôºö‰ΩøÁî®dockerÊñπÂºèÈÉ®ÁΩ≤ÁöÑËØ¶ÁªÜÊïôÁ®ãÂèØ‰ª•ÂèÇËÄÉÔºö[dockerÈÉ®ÁΩ≤CoWÈ°πÁõÆ](https://www.wangpc.cc/ai/docker-deploy-cow/)


## Ê®°ÂûãËØ¥Êòé

‰ª•‰∏ãÂØπÊâÄÊúâÂèØÊîØÊåÅÁöÑÊ®°ÂûãÁöÑÈÖçÁΩÆÂíå‰ΩøÁî®ÊñπÊ≥ïËøõË°åËØ¥ÊòéÔºåÊ®°ÂûãÊé•Âè£ÂÆûÁé∞Âú®È°πÁõÆÁöÑ `models/` ÁõÆÂΩï‰∏ã„ÄÇ

&lt;details&gt;
&lt;summary&gt;OpenAI&lt;/summary&gt;

1. API KeyÂàõÂª∫ÔºöÂú® [OpenAIÂπ≥Âè∞](https://platform.openai.com/api-keys) ÂàõÂª∫API Key

2. Â°´ÂÜôÈÖçÁΩÆ

```json
{
    &quot;model&quot;: &quot;gpt-4.1-mini&quot;,
    &quot;open_ai_api_key&quot;: &quot;YOUR_API_KEY&quot;,
    &quot;open_ai_api_base&quot;: &quot;https://api.openai.com/v1&quot;,
    &quot;bot_type&quot;: &quot;chatGPT&quot;
}
```

 - `model`: ‰∏éOpenAIÊé•Âè£ÁöÑ [modelÂèÇÊï∞](https://platform.openai.com/docs/models) ‰∏ÄËá¥ÔºåÊîØÊåÅÂåÖÊã¨ oÁ≥ªÂàó„ÄÅgpt-5.2„ÄÅgpt-5.1„ÄÅgpt-4.1Á≠âÁ≥ªÂàóÊ®°Âûã
 - `open_ai_api_base`: Â¶ÇÊûúÈúÄË¶ÅÊé•ÂÖ•Á¨¨‰∏âÊñπ‰ª£ÁêÜÊé•Âè£ÔºåÂèØÈÄöËøá‰øÆÊîπËØ•ÂèÇÊï∞ËøõË°åÊé•ÂÖ•
 - `bot_type`: ‰ΩøÁî®OpenAIÁõ∏ÂÖ≥Ê®°ÂûãÊó∂Êó†ÈúÄÂ°´ÂÜô„ÄÇÂΩì‰ΩøÁî®Á¨¨‰∏âÊñπ‰ª£ÁêÜÊé•Âè£Êé•ÂÖ•ClaudeÁ≠âÈùûOpenAIÂÆòÊñπÊ®°ÂûãÊó∂ÔºåËØ•ÂèÇÊï∞ËÆæ‰∏∫ `chatGPT`
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;LinkAI&lt;/summary&gt;

1. API KeyÂàõÂª∫ÔºöÂú® [LinkAIÂπ≥Âè∞](https://link-ai.tech/console/interface) ÂàõÂª∫API Key 

2. Â°´ÂÜôÈÖçÁΩÆ

```json
{
    &quot;use_linkai&quot;: true,
    &quot;linkai_api_key&quot;: &quot;YOUR API KEY&quot;,
    &quot;linkai_app_code&quot;: &quot;YOUR APP CODE&quot;
}
```

+ `use_linkai`: ÊòØÂê¶‰ΩøÁî®LinkAIÊé•Âè£ÔºåÈªòËÆ§ÂÖ≥Èó≠ÔºåËÆæÁΩÆ‰∏∫trueÂêéÂèØÂØπÊé•LinkAIÂπ≥Âè∞ÁöÑÊô∫ËÉΩ‰ΩìÔºå‰ΩøÁî®Áü•ËØÜÂ∫ì„ÄÅÂ∑•‰ΩúÊµÅ„ÄÅÊï∞ÊçÆÂ∫ì„ÄÅMCPÊèí‰ª∂Á≠â‰∏∞ÂØåÁöÑAgentËÉΩÂäõ
+ `linkai_api_key`: LinkAIÂπ≥Âè∞ÁöÑAPI KeyÔºåÂèØÂú® [ÊéßÂà∂Âè∞](https://link-ai.tech/console/interface) ‰∏≠ÂàõÂª∫
+ `linkai_app_code`: LinkAIÊô∫ËÉΩ‰Ωì (Â∫îÁî®ÊàñÂ∑•‰ΩúÊµÅ) ÁöÑcodeÔºåÈÄâÂ°´ÔºåÊôÆÈÄöÂØπËØùÊ®°ÂºèÂèØÁî®„ÄÇÊô∫ËÉΩ‰ΩìÂàõÂª∫ÂèØÂèÇËÄÉ [ËØ¥ÊòéÊñáÊ°£](https://docs.link-ai.tech/platform/quick-start)
+ `model`: modelÂ≠óÊÆµÂ°´ÂÜôÁ©∫ÂàôÁõ¥Êé•‰ΩøÁî®Êô∫ËÉΩ‰ΩìÁöÑÊ®°ÂûãÔºåÂèØÂú®Âπ≥Âè∞‰∏≠ÁÅµÊ¥ªÂàáÊç¢Ôºå[Ê®°ÂûãÂàóË°®](https://link-ai.tech/console/models)‰∏≠ÁöÑÂÖ®ÈÉ®Ê®°ÂûãÂùáÂèØ‰ΩøÁî®
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;MiniMax&lt;/summary&gt;

ÊñπÂºè‰∏ÄÔºöÂÆòÊñπÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ã(Êé®Ëçê)Ôºö

```json
{
    &quot;model&quot;: &quot;MiniMax-M2.5&quot;,
    &quot;minimax_api_key&quot;: &quot;&quot;
}
```
 - `model`: ÂèØÂ°´ÂÜô `MiniMax-M2.5„ÄÅMiniMax-M2.1„ÄÅMiniMax-M2.1-lightning„ÄÅMiniMax-M2„ÄÅabab6.5-chat` Á≠â
 - `minimax_api_key`ÔºöMiniMaxÂπ≥Âè∞ÁöÑAPI-KEYÔºåÂú® [ÊéßÂà∂Âè∞](https://platform.minimaxi.com/user-center/basic-information/interface-key) ÂàõÂª∫

ÊñπÂºè‰∫åÔºöOpenAIÂÖºÂÆπÊñπÂºèÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö
```json
{
  &quot;bot_type&quot;: &quot;chatGPT&quot;,
  &quot;model&quot;: &quot;MiniMax-M2.5&quot;,
  &quot;open_ai_api_base&quot;: &quot;https://api.minimaxi.com/v1&quot;,
  &quot;open_ai_api_key&quot;: &quot;&quot;
}
```
- `bot_type`: OpenAIÂÖºÂÆπÊñπÂºè
- `model`: ÂèØÂ°´ `MiniMax-M2.5„ÄÅMiniMax-M2.1„ÄÅMiniMax-M2.1-lightning„ÄÅMiniMax-M2`ÔºåÂèÇËÄÉ[APIÊñáÊ°£](https://platform.minimaxi.com/document/%E5%AF%B9%E8%AF%9D?key=66701d281d57f38758d581d0#QklxsNSbaf6kM4j6wjO5eEek)
- `open_ai_api_base`: MiniMaxÂπ≥Âè∞APIÁöÑ BASE URL
- `open_ai_api_key`: MiniMaxÂπ≥Âè∞ÁöÑAPI-KEY
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Êô∫Ë∞±AI (GLM)&lt;/summary&gt;

ÊñπÂºè‰∏ÄÔºöÂÆòÊñπÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ã(Êé®Ëçê)Ôºö

```json
{
  &quot;model&quot;: &quot;glm-5&quot;,
  &quot;zhipu_ai_api_key&quot;: &quot;&quot;
}
```
 - `model`: ÂèØÂ°´ `glm-5„ÄÅglm-4.7„ÄÅglm-4-plus„ÄÅglm-4-flash„ÄÅglm-4-air„ÄÅglm-4-airx„ÄÅglm-4-long` Á≠â, ÂèÇËÄÉ [glmÁ≥ªÂàóÊ®°ÂûãÁºñÁ†Å](https://bigmodel.cn/dev/api/normal-model/glm-4)
 - `zhipu_ai_api_key`: Êô∫Ë∞±AIÂπ≥Âè∞ÁöÑ API KEYÔºåÂú® [ÊéßÂà∂Âè∞](https://www.bigmodel.cn/usercenter/proj-mgmt/apikeys) ÂàõÂª∫

ÊñπÂºè‰∫åÔºöOpenAIÂÖºÂÆπÊñπÂºèÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö
```json
{
  &quot;bot_type&quot;: &quot;chatGPT&quot;,
  &quot;model&quot;: &quot;glm-5&quot;,
  &quot;open_ai_api_base&quot;: &quot;https://open.bigmodel.cn/api/paas/v4&quot;,
  &quot;open_ai_api_key&quot;: &quot;&quot;
}
```
- `bot_type`: OpenAIÂÖºÂÆπÊñπÂºè
- `model`: ÂèØÂ°´ `glm-5„ÄÅglm-4.7„ÄÅglm-4-plus„ÄÅglm-4-flash„ÄÅglm-4-air„ÄÅglm-4-airx„ÄÅglm-4-long` Á≠â
- `open_ai_api_base`: Êô∫Ë∞±AIÂπ≥Âè∞ÁöÑ BASE URL
- `open_ai_api_key`: Êô∫Ë∞±AIÂπ≥Âè∞ÁöÑ API KEY
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;ÈÄö‰πâÂçÉÈóÆ (Qwen)&lt;/summary&gt;

ÊñπÂºè‰∏ÄÔºöÂÆòÊñπSDKÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ã(Êé®Ëçê)Ôºö

```json
{
    &quot;model&quot;: &quot;qwen3.5-plus&quot;,
    &quot;dashscope_api_key&quot;: &quot;sk-qVxxxxG&quot;
}
```
 - `model`: ÂèØÂ°´ÂÜô `qwen3.5-plus„ÄÅqwen3-max„ÄÅqwen-max„ÄÅqwen-plus„ÄÅqwen-turbo„ÄÅqwen-long„ÄÅqwq-plus` Á≠â
 - `dashscope_api_key`: ÈÄö‰πâÂçÉÈóÆÁöÑ API-KEYÔºåÂèÇËÄÉ [ÂÆòÊñπÊñáÊ°£](https://bailian.console.aliyun.com/?tab=api#/api) ÔºåÂú® [ÊéßÂà∂Âè∞](https://bailian.console.aliyun.com/?tab=model#/api-key) ÂàõÂª∫

ÊñπÂºè‰∫åÔºöOpenAIÂÖºÂÆπÊñπÂºèÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö
```json
{
  &quot;bot_type&quot;: &quot;chatGPT&quot;,
  &quot;model&quot;: &quot;qwen3.5-plus&quot;,
  &quot;open_ai_api_base&quot;: &quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,
  &quot;open_ai_api_key&quot;: &quot;sk-qVxxxxG&quot;
}
```
- `bot_type`: OpenAIÂÖºÂÆπÊñπÂºè
- `model`: ÊîØÊåÅÂÆòÊñπÊâÄÊúâÊ®°ÂûãÔºåÂèÇËÄÉ[Ê®°ÂûãÂàóË°®](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.0.78d84823Kth5on#9f8890ce29g5u)
- `open_ai_api_base`: ÈÄö‰πâÂçÉÈóÆAPIÁöÑ BASE URL
- `open_ai_api_key`: ÈÄö‰πâÂçÉÈóÆÁöÑ API-KEY
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Kimi (Moonshot)&lt;/summary&gt;

ÊñπÂºè‰∏ÄÔºöÂÆòÊñπÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö

```json
{
    &quot;model&quot;: &quot;kimi-k2.5&quot;,
    &quot;moonshot_api_key&quot;: &quot;&quot;
}
```
 - `model`: ÂèØÂ°´ÂÜô `kimi-k2.5„ÄÅkimi-k2„ÄÅmoonshot-v1-8k„ÄÅmoonshot-v1-32k„ÄÅmoonshot-v1-128k`
 - `moonshot_api_key`: MoonshotÁöÑAPI-KEYÔºåÂú® [ÊéßÂà∂Âè∞](https://platform.moonshot.cn/console/api-keys) ÂàõÂª∫
 
ÊñπÂºè‰∫åÔºöOpenAIÂÖºÂÆπÊñπÂºèÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö
```json
{
  &quot;bot_type&quot;: &quot;chatGPT&quot;,
  &quot;model&quot;: &quot;kimi-k2.5&quot;,
  &quot;open_ai_api_base&quot;: &quot;https://api.moonshot.cn/v1&quot;,
  &quot;open_ai_api_key&quot;: &quot;&quot;
}
```
- `bot_type`: OpenAIÂÖºÂÆπÊñπÂºè
- `model`: ÂèØÂ°´ÂÜô `kimi-k2.5„ÄÅkimi-k2„ÄÅmoonshot-v1-8k„ÄÅmoonshot-v1-32k„ÄÅmoonshot-v1-128k`
- `open_ai_api_base`: MoonshotÁöÑ BASE URL
- `open_ai_api_key`: MoonshotÁöÑ API-KEY
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Ë±ÜÂåÖ (Doubao)&lt;/summary&gt;

1. API KeyÂàõÂª∫ÔºöÂú® [ÁÅ´Â±±ÊñπËàüÊéßÂà∂Âè∞](https://console.volcengine.com/ark/region:ark+cn-beijing/apikey) ÂàõÂª∫API Key

2. Â°´ÂÜôÈÖçÁΩÆ

```json
{
    &quot;model&quot;: &quot;doubao-seed-2-0-code-preview-260215&quot;,
    &quot;ark_api_key&quot;: &quot;YOUR_API_KEY&quot;
}
```
 - `model`: ÂèØÂ°´ÂÜô `doubao-seed-2-0-code-preview-260215„ÄÅdoubao-seed-2-0-pro-260215„ÄÅdoubao-seed-2-0-lite-260215„ÄÅdoubao-seed-2-0-mini-260215` Á≠â
 - `ark_api_key`: ÁÅ´Â±±ÊñπËàüÂπ≥Âè∞ÁöÑ API KeyÔºåÂú® [ÊéßÂà∂Âè∞](https://console.volcengine.com/ark/region:ark+cn-beijing/apikey) ÂàõÂª∫
 - `ark_base_url`: ÂèØÈÄâÔºåÈªòËÆ§‰∏∫ `https://ark.cn-beijing.volces.com/api/v3`
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Claude&lt;/summary&gt;

1. API KeyÂàõÂª∫ÔºöÂú® [ClaudeÊéßÂà∂Âè∞](https://console.anthropic.com/settings/keys) ÂàõÂª∫API Key

2. Â°´ÂÜôÈÖçÁΩÆ

```json
{
    &quot;model&quot;: &quot;claude-sonnet-4-6&quot;,
    &quot;claude_api_key&quot;: &quot;YOUR_API_KEY&quot;
}
```
 - `model`: ÂèÇËÄÉ [ÂÆòÊñπÊ®°ÂûãID](https://docs.anthropic.com/en/docs/about-claude/models/overview#model-aliases) ÔºåÊîØÊåÅ `claude-sonnet-4-6„ÄÅclaude-opus-4-6„ÄÅclaude-sonnet-4-5„ÄÅclaude-sonnet-4-0„ÄÅclaude-opus-4-0„ÄÅclaude-3-5-sonnet-latest` Á≠â
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Gemini&lt;/summary&gt;

API KeyÂàõÂª∫ÔºöÂú® [ÊéßÂà∂Âè∞](https://aistudio.google.com/app/apikey?hl=zh-cn) ÂàõÂª∫API Key ÔºåÈÖçÁΩÆÂ¶Ç‰∏ã
```json
{
    &quot;model&quot;: &quot;gemini-3.1-pro-preview&quot;,
    &quot;gemini_api_key&quot;: &quot;&quot;
}
```
 - `model`: ÂèÇËÄÉ[ÂÆòÊñπÊñáÊ°£-Ê®°ÂûãÂàóË°®](https://ai.google.dev/gemini-api/docs/models?hl=zh-cn)ÔºåÊîØÊåÅ `gemini-3.1-pro-preview„ÄÅgemini-3-flash-preview„ÄÅgemini-3-pro-preview„ÄÅgemini-2.5-pro„ÄÅgemini-2.0-flash` Á≠â
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;DeepSeek&lt;/summary&gt;

1. API KeyÂàõÂª∫ÔºöÂú® [DeepSeekÂπ≥Âè∞](https://platform.deepseek.com/api_keys) ÂàõÂª∫API Key 

2. Â°´ÂÜôÈÖçÁΩÆ

```json
{
    &quot;model&quot;: &quot;deepseek-chat&quot;,
    &quot;open_ai_api_key&quot;: &quot;sk-xxxxxxxxxxx&quot;,
    &quot;open_ai_api_base&quot;: &quot;https://api.deepseek.com/v1&quot;, 
    &quot;bot_type&quot;: &quot;chatGPT&quot;

}
```

 - `bot_type`: OpenAIÂÖºÂÆπÊñπÂºè
 - `model`: ÂèØÂ°´ `deepseek-chat„ÄÅdeepseek-reasoner`ÔºåÂàÜÂà´ÂØπÂ∫îÁöÑÊòØ DeepSeek-V3 Âíå DeepSeek-R1 Ê®°Âûã
 - `open_ai_api_key`: DeepSeekÂπ≥Âè∞ÁöÑ API Key
 - `open_ai_api_base`: DeepSeekÂπ≥Âè∞ BASE URL
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Azure&lt;/summary&gt;

1. API KeyÂàõÂª∫ÔºöÂú® [AzureÂπ≥Âè∞](https://oai.azure.com/) ÂàõÂª∫API Key 

2. Â°´ÂÜôÈÖçÁΩÆ

```json
{
  &quot;model&quot;: &quot;&quot;,
  &quot;use_azure_chatgpt&quot;: true,
  &quot;open_ai_api_key&quot;: &quot;&quot;,
  &quot;open_ai_api_base&quot;: &quot;&quot;,
  &quot;azure_deployment_id&quot;: &quot;&quot;,
  &quot;azure_api_version&quot;: &quot;2025-01-01-preview&quot;
}
```

 - `model`: ÁïôÁ©∫Âç≥ÂèØ
 - `use_azure_chatgpt`: ËÆæ‰∏∫ true 
 - `open_ai_api_key`: AzureÂπ≥Âè∞ÁöÑÂØÜÈí•
 - `open_ai_api_base`: AzureÂπ≥Âè∞ÁöÑ BASE URL
 - `azure_deployment_id`: AzureÂπ≥Âè∞ÈÉ®ÁΩ≤ÁöÑÊ®°ÂûãÂêçÁß∞
 - `azure_api_version`: apiÁâàÊú¨‰ª•Âèä‰ª•‰∏äÂèÇÊï∞ÂèØ‰ª•Âú®ÈÉ®ÁΩ≤ÁöÑ [Ê®°ÂûãÈÖçÁΩÆ](https://oai.azure.com/resource/deployments) ÁïåÈù¢Êü•Áúã
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;ÁôæÂ∫¶ÊñáÂøÉ&lt;/summary&gt;
ÊñπÂºè‰∏ÄÔºöÂÆòÊñπSDKÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö

```json
{
    &quot;model&quot;: &quot;wenxin-4&quot;, 
    &quot;baidu_wenxin_api_key&quot;: &quot;IajztZ0bDxgnP9bEykU7lBer&quot;,
    &quot;baidu_wenxin_secret_key&quot;: &quot;EDPZn6L24uAS9d8RWFfotK47dPvkjD6G&quot;
}
```
 - `model`: ÂèØÂ°´ `wenxin`Âíå`wenxin-4`ÔºåÂØπÂ∫îÊ®°Âûã‰∏∫ ÊñáÂøÉ-3.5 Âíå ÊñáÂøÉ-4.0
 - `baidu_wenxin_api_key`ÔºöÂèÇËÄÉ [ÂçÉÂ∏ÜÂπ≥Âè∞-access_tokenÈâ¥ÊùÉ](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/dlv4pct3s) ÊñáÊ°£Ëé∑Âèñ API Key
 - `baidu_wenxin_secret_key`ÔºöÂèÇËÄÉ [ÂçÉÂ∏ÜÂπ≥Âè∞-access_tokenÈâ¥ÊùÉ](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/dlv4pct3s) ÊñáÊ°£Ëé∑Âèñ Secret Key

ÊñπÂºè‰∫åÔºöOpenAIÂÖºÂÆπÊñπÂºèÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö
```json
{
  &quot;bot_type&quot;: &quot;chatGPT&quot;,
  &quot;model&quot;: &quot;ERNIE-4.0-Turbo-8K&quot;,
  &quot;open_ai_api_base&quot;: &quot;https://qianfan.baidubce.com/v2&quot;,
  &quot;open_ai_api_key&quot;: &quot;bce-v3/ALTxxxxxxd2b&quot;
}
```
- `bot_type`: OpenAIÂÖºÂÆπÊñπÂºè
- `model`: ÊîØÊåÅÂÆòÊñπÊâÄÊúâÊ®°ÂûãÔºåÂèÇËÄÉ[Ê®°ÂûãÂàóË°®](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Wm9cvy6rl)
- `open_ai_api_base`: ÁôæÂ∫¶ÊñáÂøÉAPIÁöÑ BASE URL
- `open_ai_api_key`: ÁôæÂ∫¶ÊñáÂøÉÁöÑ API-KEYÔºåÂèÇËÄÉ [ÂÆòÊñπÊñáÊ°£](https://cloud.baidu.com/doc/qianfan-api/s/ym9chdsy5) ÔºåÂú® [ÊéßÂà∂Âè∞](https://console.bce.baidu.com/iam/#/iam/apikey/list) ÂàõÂª∫API Key

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;ËÆØÈ£ûÊòüÁÅ´&lt;/summary&gt;

ÊñπÂºè‰∏ÄÔºöÂÆòÊñπÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö
ÂèÇËÄÉ [ÂÆòÊñπÊñáÊ°£-Âø´ÈÄüÊåáÂºï](https://www.xfyun.cn/doc/platform/quickguide.html#%E7%AC%AC%E4%BA%8C%E6%AD%A5-%E5%88%9B%E5%BB%BA%E6%82%A8%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%BA%94%E7%94%A8-%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8%E6%9C%8D%E5%8A%A1) Ëé∑Âèñ `APPID„ÄÅ APISecret„ÄÅ APIKey` ‰∏â‰∏™ÂèÇÊï∞

```json
{
  &quot;model&quot;: &quot;xunfei&quot;,
  &quot;xunfei_app_id&quot;: &quot;&quot;,
  &quot;xunfei_api_key&quot;: &quot;&quot;,
  &quot;xunfei_api_secret&quot;: &quot;&quot;,
  &quot;xunfei_domain&quot;: &quot;4.0Ultra&quot;,
  &quot;xunfei_spark_url&quot;: &quot;wss://spark-api.xf-yun.com/v4.0/chat&quot;
}
```
 - `model`: Â°´ `xunfei`
 - `xunfei_domain`: ÂèØÂ°´ÂÜô `4.0Ultra„ÄÅgeneralv3.5„ÄÅmax-32k„ÄÅgeneralv3„ÄÅpro-128k„ÄÅlite`
 - `xunfei_spark_url`: Â°´ÂÜôÂèÇËÄÉ [ÂÆòÊñπÊñáÊ°£-ËØ∑Ê±ÇÂú∞ÂùÄ](https://www.xfyun.cn/doc/spark/Web.html#_1-1-%E8%AF%B7%E6%B1%82%E5%9C%B0%E5%9D%80) ÁöÑËØ¥Êòé
 
ÊñπÂºè‰∫åÔºöOpenAIÂÖºÂÆπÊñπÂºèÊé•ÂÖ•ÔºåÈÖçÁΩÆÂ¶Ç‰∏ãÔºö
```json
{
  &quot;bot_type&quot;: &quot;chatGPT&quot;,
  &quot;model&quot;: &quot;4.0Ultra&quot;,
  &quot;open_ai_api_base&quot;: &quot;https://spark-api-open.xf-yun.com/v1&quot;,
  &quot;open_ai_api_key&quot;: &quot;&quot;
}
```
- `bot_type`: OpenAIÂÖºÂÆπÊñπÂºè
- `model`: ÂèØÂ°´ÂÜô `4.0Ultra„ÄÅgeneralv3.5„ÄÅmax-32k„ÄÅgeneralv3„ÄÅpro-128k„ÄÅlite`
- `open_ai_api_base`: ËÆØÈ£ûÊòüÁÅ´Âπ≥Âè∞ÁöÑ BASE URL
- `open_ai_api_key`: ËÆØÈ£ûÊòüÁÅ´Âπ≥Âè∞ÁöÑ[APIPassword](https://console.xfyun.cn/services/bm3) ÔºåÂõ†Ê®°ÂûãËÄåÂ∑≤
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;ModelScope&lt;/summary&gt;

```json
{
  &quot;bot_type&quot;: &quot;modelscope&quot;,
  &quot;model&quot;: &quot;Qwen/QwQ-32B&quot;,
  &quot;modelscope_api_key&quot;: &quot;your_api_key&quot;,
  &quot;modelscope_base_url&quot;: &quot;https://api-inference.modelscope.cn/v1/chat/completions&quot;,
  &quot;text_to_image&quot;: &quot;MusePublic/489_ckpt_FLUX_1&quot;
}
```

- `bot_type`: modelscopeÊé•Âè£Ê†ºÂºè
- `model`: ÂèÇËÄÉ[Ê®°ÂûãÂàóË°®](https://www.modelscope.cn/models?filter=inference_type&amp;page=1)
- `modelscope_api_key`: ÂèÇËÄÉ [ÂÆòÊñπÊñáÊ°£-ËÆøÈóÆ‰ª§Áâå](https://modelscope.cn/docs/accounts/token) ÔºåÂú® [ÊéßÂà∂Âè∞](https://modelscope.cn/my/myaccesstoken) 
- `modelscope_base_url`: modelscopeÂπ≥Âè∞ÁöÑ BASE URL
- `text_to_image`: ÂõæÂÉèÁîüÊàêÊ®°ÂûãÔºåÂèÇËÄÉ[Ê®°ÂûãÂàóË°®](https://www.modelscope.cn/models?filter=inference_type&amp;page=1)
&lt;/details&gt;


## ÈÄöÈÅìËØ¥Êòé

‰ª•‰∏ãÂØπÂèØÊé•ÂÖ•ÈÄöÈÅìÁöÑÈÖçÁΩÆÊñπÂºèËøõË°åËØ¥ÊòéÔºåÂ∫îÁî®ÈÄöÈÅì‰ª£Á†ÅÂú®È°πÁõÆÁöÑ `channel/` ÁõÆÂΩï‰∏ã„ÄÇ

ÊîØÊåÅÂêåÊó∂ÂèØÊé•ÂÖ•Â§ö‰∏™ÈÄöÈÅìÔºåÈÖçÁΩÆÊó∂ÂèØÈÄöËøáÈÄóÂè∑ËøõË°å

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[infiniflow/ragflow]]></title>
            <link>https://github.com/infiniflow/ragflow</link>
            <guid>https://github.com/infiniflow/ragflow</guid>
            <pubDate>Fri, 27 Feb 2026 00:08:01 GMT</pubDate>
            <description><![CDATA[RAGFlow is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/infiniflow/ragflow">infiniflow/ragflow</a></h1>
            <p>RAGFlow is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 73,788</p>
            <p>Forks: 8,202</p>
            <p>Stars today: 87 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://demo.ragflow.io/&quot;&gt;
&lt;img src=&quot;web/src/assets/logo-with-text.svg&quot; width=&quot;520&quot; alt=&quot;ragflow logo&quot;&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;./README.md&quot;&gt;&lt;img alt=&quot;README in English&quot; src=&quot;https://img.shields.io/badge/English-DBEDFA&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_zh.md&quot;&gt;&lt;img alt=&quot;ÁÆÄ‰Ωì‰∏≠ÊñáÁâàËá™Ëø∞Êñá‰ª∂&quot; src=&quot;https://img.shields.io/badge/ÁÆÄ‰Ωì‰∏≠Êñá-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_tzh.md&quot;&gt;&lt;img alt=&quot;ÁπÅÈ´îÁâà‰∏≠ÊñáËá™Ëø∞Êñá‰ª∂&quot; src=&quot;https://img.shields.io/badge/ÁπÅÈ´î‰∏≠Êñá-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_ja.md&quot;&gt;&lt;img alt=&quot;Êó•Êú¨Ë™û„ÅÆREADME&quot; src=&quot;https://img.shields.io/badge/Êó•Êú¨Ë™û-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_ko.md&quot;&gt;&lt;img alt=&quot;ÌïúÍµ≠Ïñ¥&quot; src=&quot;https://img.shields.io/badge/ÌïúÍµ≠Ïñ¥-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_id.md&quot;&gt;&lt;img alt=&quot;Bahasa Indonesia&quot; src=&quot;https://img.shields.io/badge/Bahasa Indonesia-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_pt_br.md&quot;&gt;&lt;img alt=&quot;Portugu√™s(Brasil)&quot; src=&quot;https://img.shields.io/badge/Portugu√™s(Brasil)-DFE0E5&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://x.com/intent/follow?screen_name=infiniflowai&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/twitter/follow/infiniflow?logo=X&amp;color=%20%23f5f5f5&quot; alt=&quot;follow on X(Twitter)&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://demo.ragflow.io&quot; target=&quot;_blank&quot;&gt;
        &lt;img alt=&quot;Static Badge&quot; src=&quot;https://img.shields.io/badge/Online-Demo-4e6b99&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://hub.docker.com/r/infiniflow/ragflow&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/docker/pulls/infiniflow/ragflow?label=Docker%20Pulls&amp;color=0db7ed&amp;logo=docker&amp;logoColor=white&amp;style=flat-square&quot; alt=&quot;docker pull infiniflow/ragflow:v0.24.0&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/infiniflow/ragflow/releases/latest&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/github/v/release/infiniflow/ragflow?color=blue&amp;label=Latest%20Release&quot; alt=&quot;Latest Release&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/infiniflow/ragflow/blob/main/LICENSE&quot;&gt;
        &lt;img height=&quot;21&quot; src=&quot;https://img.shields.io/badge/License-Apache--2.0-ffffff?labelColor=d4eaf7&amp;color=2e6cc4&quot; alt=&quot;license&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://deepwiki.com/infiniflow/ragflow&quot;&gt;
        &lt;img alt=&quot;Ask DeepWiki&quot; src=&quot;https://deepwiki.com/badge.svg&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;h4 align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://ragflow.io/docs/dev/&quot;&gt;Document&lt;/a&gt; |
  &lt;a href=&quot;https://github.com/infiniflow/ragflow/issues/12241&quot;&gt;Roadmap&lt;/a&gt; |
  &lt;a href=&quot;https://twitter.com/infiniflowai&quot;&gt;Twitter&lt;/a&gt; |
  &lt;a href=&quot;https://discord.gg/NjYzJD3GM3&quot;&gt;Discord&lt;/a&gt; |
  &lt;a href=&quot;https://demo.ragflow.io&quot;&gt;Demo&lt;/a&gt;
&lt;/h4&gt;

&lt;div align=&quot;center&quot; style=&quot;margin-top:20px;margin-bottom:20px;&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/ragflow-octoverse.png&quot; width=&quot;1200&quot;/&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/9064&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/9064&quot; alt=&quot;infiniflow%2Fragflow | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;details open&gt;
&lt;summary&gt;&lt;b&gt;üìï Table of Contents&lt;/b&gt;&lt;/summary&gt;

- üí° [What is RAGFlow?](#-what-is-ragflow)
- üéÆ [Demo](#-demo)
- üìå [Latest Updates](#-latest-updates)
- üåü [Key Features](#-key-features)
- üîé [System Architecture](#-system-architecture)
- üé¨ [Get Started](#-get-started)
- üîß [Configurations](#-configurations)
- üîß [Build a Docker image](#-build-a-docker-image)
- üî® [Launch service from source for development](#-launch-service-from-source-for-development)
- üìö [Documentation](#-documentation)
- üìú [Roadmap](#-roadmap)
- üèÑ [Community](#-community)
- üôå [Contributing](#-contributing)

&lt;/details&gt;

## üí° What is RAGFlow?

[RAGFlow](https://ragflow.io/) is a leading open-source Retrieval-Augmented Generation ([RAG](https://ragflow.io/basics/what-is-rag)) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs. It offers a streamlined RAG workflow adaptable to enterprises of any scale. Powered by a converged [context engine](https://ragflow.io/basics/what-is-agent-context-engine) and pre-built agent templates, RAGFlow enables developers to transform complex data into high-fidelity, production-ready AI systems with exceptional efficiency and precision.

## üéÆ Demo

Try our demo at [https://demo.ragflow.io](https://demo.ragflow.io).

&lt;div align=&quot;center&quot; style=&quot;margin-top:20px;margin-bottom:20px;&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/chunking.gif&quot; width=&quot;1200&quot;/&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/agentic-dark.gif&quot; width=&quot;1200&quot;/&gt;
&lt;/div&gt;

## üî• Latest Updates

- 2025-12-26 Supports &#039;Memory&#039; for AI agent.
- 2025-11-19 Supports Gemini 3 Pro.
- 2025-11-12 Supports data synchronization from Confluence, S3, Notion, Discord, Google Drive.
- 2025-10-23 Supports MinerU &amp; Docling as document parsing methods.
- 2025-10-15 Supports orchestrable ingestion pipeline.
- 2025-08-08 Supports OpenAI&#039;s latest GPT-5 series models.
- 2025-08-01 Supports agentic workflow and MCP.
- 2025-05-23 Adds a Python/JavaScript code executor component to Agent.
- 2025-05-05 Supports cross-language query.
- 2025-03-19 Supports using a multi-modal model to make sense of images within PDF or DOCX files.

## üéâ Stay Tuned

‚≠êÔ∏è Star our repository to stay up-to-date with exciting new features and improvements! Get instant notifications for new
releases! üåü

&lt;div align=&quot;center&quot; style=&quot;margin-top:20px;margin-bottom:20px;&quot;&gt;
&lt;img src=&quot;https://github.com/user-attachments/assets/18c9707e-b8aa-4caf-a154-037089c105ba&quot; width=&quot;1200&quot;/&gt;
&lt;/div&gt;

## üåü Key Features

### üç≠ **&quot;Quality in, quality out&quot;**

- [Deep document understanding](./deepdoc/README.md)-based knowledge extraction from unstructured data with complicated
  formats.
- Finds &quot;needle in a data haystack&quot; of literally unlimited tokens.

### üç± **Template-based chunking**

- Intelligent and explainable.
- Plenty of template options to choose from.

### üå± **Grounded citations with reduced hallucinations**

- Visualization of text chunking to allow human intervention.
- Quick view of the key references and traceable citations to support grounded answers.

### üçî **Compatibility with heterogeneous data sources**

- Supports Word, slides, excel, txt, images, scanned copies, structured data, web pages, and more.

### üõÄ **Automated and effortless RAG workflow**

- Streamlined RAG orchestration catered to both personal and large businesses.
- Configurable LLMs as well as embedding models.
- Multiple recall paired with fused re-ranking.
- Intuitive APIs for seamless integration with business.

## üîé System Architecture

&lt;div align=&quot;center&quot; style=&quot;margin-top:20px;margin-bottom:20px;&quot;&gt;
&lt;img src=&quot;https://github.com/user-attachments/assets/31b0dd6f-ca4f-445a-9457-70cb44a381b2&quot; width=&quot;1000&quot;/&gt;
&lt;/div&gt;

## üé¨ Get Started

### üìù Prerequisites

- CPU &gt;= 4 cores
- RAM &gt;= 16 GB
- Disk &gt;= 50 GB
- Docker &gt;= 24.0.0 &amp; Docker Compose &gt;= v2.26.1
- [gVisor](https://gvisor.dev/docs/user_guide/install/): Required only if you intend to use the code executor (sandbox) feature of RAGFlow.

&gt; [!TIP]
&gt; If you have not installed Docker on your local machine (Windows, Mac, or Linux), see [Install Docker Engine](https://docs.docker.com/engine/install/).

### üöÄ Start up the server

1. Ensure `vm.max_map_count` &gt;= 262144:

   &gt; To check the value of `vm.max_map_count`:
   &gt;
   &gt; ```bash
   &gt; $ sysctl vm.max_map_count
   &gt; ```
   &gt;
   &gt; Reset `vm.max_map_count` to a value at least 262144 if it is not.
   &gt;
   &gt; ```bash
   &gt; # In this case, we set it to 262144:
   &gt; $ sudo sysctl -w vm.max_map_count=262144
   &gt; ```
   &gt;
   &gt; This change will be reset after a system reboot. To ensure your change remains permanent, add or update the
   &gt; `vm.max_map_count` value in **/etc/sysctl.conf** accordingly:
   &gt;
   &gt; ```bash
   &gt; vm.max_map_count=262144
   &gt; ```
   &gt;
2. Clone the repo:

   ```bash
   $ git clone https://github.com/infiniflow/ragflow.git
   ```
3. Start up the server using the pre-built Docker images:

&gt; [!CAUTION]
&gt; All Docker images are built for x86 platforms. We don&#039;t currently offer Docker images for ARM64.
&gt; If you are on an ARM64 platform, follow [this guide](https://ragflow.io/docs/dev/build_docker_image) to build a Docker image compatible with your system.

&gt; The command below downloads the `v0.24.0` edition of the RAGFlow Docker image. See the following table for descriptions of different RAGFlow editions. To download a RAGFlow edition different from `v0.24.0`, update the `RAGFLOW_IMAGE` variable accordingly in **docker/.env** before using `docker compose` to start the server.

```bash
   $ cd ragflow/docker

   # git checkout v0.24.0
   # Optional: use a stable tag (see releases: https://github.com/infiniflow/ragflow/releases)
   # This step ensures the **entrypoint.sh** file in the code matches the Docker image version.

   # Use CPU for DeepDoc tasks:
   $ docker compose -f docker-compose.yml up -d

   # To use GPU to accelerate DeepDoc tasks:
   # sed -i &#039;1i DEVICE=gpu&#039; .env
   # docker compose -f docker-compose.yml up -d
```

&gt; Note: Prior to `v0.22.0`, we provided both images with embedding models and slim images without embedding models. Details as follows:

| RAGFlow image tag | Image size (GB) | Has embedding models? | Stable?        |
|-------------------|-----------------|-----------------------|----------------|
| v0.21.1           | &amp;approx;9       | ‚úîÔ∏è                    | Stable release |
| v0.21.1-slim      | &amp;approx;2       | ‚ùå                     | Stable release |

&gt; Starting with `v0.22.0`, we ship only the slim edition and no longer append the **-slim** suffix to the image tag.

4. Check the server status after having the server up and running:

   ```bash
   $ docker logs -f docker-ragflow-cpu-1
   ```

   _The following output confirms a successful launch of the system:_

   ```bash

         ____   ___    ______ ______ __
        / __ \ /   |  / ____// ____// /____  _      __
       / /_/ // /| | / / __ / /_   / // __ \| | /| / /
      / _, _// ___ |/ /_/ // __/  / // /_/ /| |/ |/ /
     /_/ |_|/_/  |_|\____//_/    /_/ \____/ |__/|__/

    * Running on all addresses (0.0.0.0)
   ```

   &gt; If you skip this confirmation step and directly log in to RAGFlow, your browser may prompt a `network abnormal`
   &gt; error because, at that moment, your RAGFlow may not be fully initialized.
   &gt;
5. In your web browser, enter the IP address of your server and log in to RAGFlow.

   &gt; With the default settings, you only need to enter `http://IP_OF_YOUR_MACHINE` (**sans** port number) as the default
   &gt; HTTP serving port `80` can be omitted when using the default configurations.
   &gt;
6. In [service_conf.yaml.template](./docker/service_conf.yaml.template), select the desired LLM factory in `user_default_llm` and update
   the `API_KEY` field with the corresponding API key.

   &gt; See [llm_api_key_setup](https://ragflow.io/docs/dev/llm_api_key_setup) for more information.
   &gt;

   _The show is on!_

## üîß Configurations

When it comes to system configurations, you will need to manage the following files:

- [.env](./docker/.env): Keeps the fundamental setups for the system, such as `SVR_HTTP_PORT`, `MYSQL_PASSWORD`, and
  `MINIO_PASSWORD`.
- [service_conf.yaml.template](./docker/service_conf.yaml.template): Configures the back-end services. The environment variables in this file will be automatically populated when the Docker container starts. Any environment variables set within the Docker container will be available for use, allowing you to customize service behavior based on the deployment environment.
- [docker-compose.yml](./docker/docker-compose.yml): The system relies on [docker-compose.yml](./docker/docker-compose.yml) to start up.

&gt; The [./docker/README](./docker/README.md) file provides a detailed description of the environment settings and service
&gt; configurations which can be used as `${ENV_VARS}` in the [service_conf.yaml.template](./docker/service_conf.yaml.template) file.

To update the default HTTP serving port (80), go to [docker-compose.yml](./docker/docker-compose.yml) and change `80:80`
to `&lt;YOUR_SERVING_PORT&gt;:80`.

Updates to the above configurations require a reboot of all containers to take effect:

&gt; ```bash
&gt; $ docker compose -f docker-compose.yml up -d
&gt; ```

### Switch doc engine from Elasticsearch to Infinity

RAGFlow uses Elasticsearch by default for storing full text and vectors. To switch to [Infinity](https://github.com/infiniflow/infinity/), follow these steps:

1. Stop all running containers:

   ```bash
   $ docker compose -f docker/docker-compose.yml down -v
   ```

&gt; [!WARNING]
&gt; `-v` will delete the docker container volumes, and the existing data will be cleared.

2. Set `DOC_ENGINE` in **docker/.env** to `infinity`.
3. Start the containers:

   ```bash
   $ docker compose -f docker-compose.yml up -d
   ```

&gt; [!WARNING]
&gt; Switching to Infinity on a Linux/arm64 machine is not yet officially supported.

## üîß Build a Docker image

This image is approximately 2 GB in size and relies on external LLM and embedding services.

```bash
git clone https://github.com/infiniflow/ragflow.git
cd ragflow/
docker build --platform linux/amd64 -f Dockerfile -t infiniflow/ragflow:nightly .
```

Or if you are behind a proxy, you can pass proxy arguments:

```bash
docker build --platform linux/amd64 \
  --build-arg http_proxy=http://YOUR_PROXY:PORT \
  --build-arg https_proxy=http://YOUR_PROXY:PORT \
  -f Dockerfile -t infiniflow/ragflow:nightly .
```

## üî® Launch service from source for development

1. Install `uv` and `pre-commit`, or skip this step if they are already installed:

   ```bash
   pipx install uv pre-commit
   ```
2. Clone the source code and install Python dependencies:

   ```bash
   git clone https://github.com/infiniflow/ragflow.git
   cd ragflow/
   uv sync --python 3.12 # install RAGFlow dependent python modules
   uv run download_deps.py
   pre-commit install
   ```
3. Launch the dependent services (MinIO, Elasticsearch, Redis, and MySQL) using Docker Compose:

   ```bash
   docker compose -f docker/docker-compose-base.yml up -d
   ```

   Add the following line to `/etc/hosts` to resolve all hosts specified in **docker/.env** to `127.0.0.1`:

   ```
   127.0.0.1       es01 infinity mysql minio redis sandbox-executor-manager
   ```
4. If you cannot access HuggingFace, set the `HF_ENDPOINT` environment variable to use a mirror site:

   ```bash
   export HF_ENDPOINT=https://hf-mirror.com
   ```
5. If your operating system does not have jemalloc, please install it as follows:

   ```bash
   # Ubuntu
   sudo apt-get install libjemalloc-dev
   # CentOS
   sudo yum install jemalloc
   # OpenSUSE
   sudo zypper install jemalloc
   # macOS
   sudo brew install jemalloc
   ```
6. Launch backend service:

   ```bash
   source .venv/bin/activate
   export PYTHONPATH=$(pwd)
   bash docker/launch_backend_service.sh
   ```
7. Install frontend dependencies:

   ```bash
   cd web
   npm install
   ```
8. Launch frontend service:

   ```bash
   npm run dev
   ```

   _The following output confirms a successful launch of the system:_

   ![](https://github.com/user-attachments/assets/0daf462c-a24d-4496-a66f-92533534e187)
9. Stop RAGFlow front-end and back-end service after development is complete:

   ```bash
   pkill -f &quot;ragflow_server.py|task_executor.py&quot;
   ```

## üìö Documentation

- [Quickstart](https://ragflow.io/docs/dev/)
- [Configuration](https://ragflow.io/docs/dev/configurations)
- [Release notes](https://ragflow.io/docs/dev/release_notes)
- [User guides](https://ragflow.io/docs/dev/category/guides)
- [Developer guides](https://ragflow.io/docs/dev/category/developers)
- [References](https://ragflow.io/docs/dev/category/references)
- [FAQs](https://ragflow.io/docs/dev/faq)

## üìú Roadmap

See the [RAGFlow Roadmap 2026](https://github.com/infiniflow/ragflow/issues/12241)

## üèÑ Community

- [Discord](https://discord.gg/NjYzJD3GM3)
- [Twitter](https://twitter.com/infiniflowai)
- [GitHub Discussions](https://github.com/orgs/infiniflow/discussions)

## üôå Contributing

RAGFlow flourishes via open-source collaboration. In this spirit, we embrace diverse contributions from the community.
If you would like to be a part, review our [Contribution Guidelines](https://ragflow.io/docs/dev/contributing) first.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[9001/copyparty]]></title>
            <link>https://github.com/9001/copyparty</link>
            <guid>https://github.com/9001/copyparty</guid>
            <pubDate>Fri, 27 Feb 2026 00:08:00 GMT</pubDate>
            <description><![CDATA[Portable file server with accelerated resumable uploads, dedup, WebDAV, SFTP, FTP, TFTP, zeroconf, media indexer, thumbnails++ all in one file]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/9001/copyparty">9001/copyparty</a></h1>
            <p>Portable file server with accelerated resumable uploads, dedup, WebDAV, SFTP, FTP, TFTP, zeroconf, media indexer, thumbnails++ all in one file</p>
            <p>Language: Python</p>
            <p>Stars: 42,725</p>
            <p>Forks: 1,752</p>
            <p>Stars today: 42 stars today</p>
            <h2>README</h2><pre>&lt;img src=&quot;https://github.com/9001/copyparty/raw/hovudstraum/docs/logo.svg&quot; width=&quot;250&quot; align=&quot;right&quot;/&gt;

### üíæüéâ copyparty

turn almost any device into a file server with resumable uploads/downloads using [*any*](#browser-support) web browser

* server only needs Python (2 or 3), all dependencies optional
* üîå protocols: [http(s)](#the-browser) // [webdav](#webdav-server) // [sftp](#sftp-server) // [ftp(s)](#ftp-server) // [tftp](#tftp-server) // [smb/cifs](#smb-server)
* üì± [android app](#android-app) // [iPhone shortcuts](#ios-shortcuts)

üëâ **[Get started](#quickstart)!** or visit the **[read-only demo server](https://a.ocv.me/pub/demo/)** üëÄ running on a nuc in my basement

üì∑ **screenshots:** [browser](#the-browser) // [upload](#uploading) // [unpost](#unpost) // [thumbnails](#thumbnails) // [search](#searching) // [fsearch](#file-search) // [zip-DL](#zip-downloads) // [md-viewer](#markdown-viewer)

üé¨ **videos:** [upload](https://a.ocv.me/pub/demo/pics-vids/up2k.webm) // [cli-upload](https://a.ocv.me/pub/demo/pics-vids/u2cli.webm) // [race-the-beam](https://a.ocv.me/pub/g/nerd-stuff/cpp/2024-0418-race-the-beam.webm) // üëâ **[feature-showcase](https://a.ocv.me/pub/demo/showcase-hq.webm)** ([youtube](https://www.youtube.com/watch?v=15_-hgsX2V0))

built in Norway üá≥üá¥ with contributions from [not-norway](https://github.com/9001/copyparty/graphs/contributors)


## readme toc

* top
    * [quickstart](#quickstart) - just run **[copyparty-sfx.py](https://github.com/9001/copyparty/releases/latest/download/copyparty-sfx.py)** -- that&#039;s it! üéâ
        * [mirrors](#mirrors) - other places to download copyparty from
        * [at home](#at-home) - make it accessible over the internet
        * [on servers](#on-servers) - you may also want these, especially on servers
    * [features](#features) - also see [comparison to similar software](./docs/versus.md)
    * [testimonials](#testimonials) - small collection of user feedback
* [motivations](#motivations) - project goals / philosophy
    * [notes](#notes) - general notes
* [bugs](#bugs) - roughly sorted by chance of encounter
    * [not my bugs](#not-my-bugs) - same order here too
* [breaking changes](#breaking-changes) - upgrade notes
* [FAQ](#FAQ) - &quot;frequently&quot; asked questions
* [accounts and volumes](#accounts-and-volumes) - per-folder, per-user permissions
    * [shadowing](#shadowing) - hiding specific subfolders
    * [dotfiles](#dotfiles) - unix-style hidden files/folders
* [the browser](#the-browser) - accessing a copyparty server using a web-browser
    * [tabs](#tabs) - the main tabs in the ui
    * [hotkeys](#hotkeys) - the browser has the following hotkeys
    * [navpane](#navpane) - switching between breadcrumbs or navpane
    * [thumbnails](#thumbnails) - press `g` or `Áî∞` to toggle grid-view instead of the file listing
    * [zip downloads](#zip-downloads) - download folders (or file selections) as `zip` or `tar` files
    * [uploading](#uploading) - drag files/folders into the web-browser to upload
        * [file-search](#file-search) - dropping files into the browser also lets you see if they exist on the server
        * [unpost](#unpost) - undo/delete accidental uploads
        * [self-destruct](#self-destruct) - uploads can be given a lifetime
        * [race the beam](#race-the-beam) - download files while they&#039;re still uploading ([demo video](http://a.ocv.me/pub/g/nerd-stuff/cpp/2024-0418-race-the-beam.webm))
        * [incoming files](#incoming-files) - the control-panel shows the ETA for all incoming files
    * [file manager](#file-manager) - cut/paste, rename, and delete files/folders (if you have permission)
    * [shares](#shares) - share a file or folder by creating a temporary link
    * [batch rename](#batch-rename) - select some files and press `F2` to bring up the rename UI
    * [rss feeds](#rss-feeds) - monitor a folder with your RSS reader
    * [opds feeds](#opds-feeds) - browse and download files from your e-book reader
    * [recent uploads](#recent-uploads) - list all recent uploads
    * [media player](#media-player) - plays almost every audio format there is
        * [playlists](#playlists) - create and play [m3u8](https://en.wikipedia.org/wiki/M3U) playlists
        * [creating a playlist](#creating-a-playlist) - with a standalone mediaplayer or copyparty
        * [audio equalizer](#audio-equalizer) - and [dynamic range compressor](https://en.wikipedia.org/wiki/Dynamic_range_compression)
        * [fix unreliable playback on android](#fix-unreliable-playback-on-android) - due to phone / app settings
    * [textfile viewer](#textfile-viewer) - with realtime streaming of logfiles and such ([demo](https://a.ocv.me/pub/demo/logtail/))
    * [markdown viewer](#markdown-viewer) - and there are *two* editors
        * [markdown vars](#markdown-vars) - dynamic docs with serverside variable expansion
    * [other tricks](#other-tricks)
    * [searching](#searching) - search by size, date, path/name, mp3-tags, ...
* [server config](#server-config) - using arguments or config files, or a mix of both
    * [zeroconf](#zeroconf) - announce enabled services on the LAN ([pic](https://user-images.githubusercontent.com/241032/215344737-0eae8d98-9496-4256-9aa8-cd2f6971810d.png))
        * [mdns](#mdns) - LAN domain-name and feature announcer
        * [ssdp](#ssdp) - windows-explorer announcer
    * [qr-code](#qr-code) - print a qr-code [(screenshot)](https://user-images.githubusercontent.com/241032/194728533-6f00849b-c6ac-43c6-9359-83e454d11e00.png) for quick access
    * [ftp server](#ftp-server) - an FTP server can be started using `--ftp 3921`
    * [sftp server](#sftp-server) - goes roughly 700 MiB/s (slower than webdav and ftp)
    * [webdav server](#webdav-server) - with read-write support
        * [connecting to webdav from windows](#connecting-to-webdav-from-windows) - using the GUI
    * [tftp server](#tftp-server) - a TFTP server (read/write) can be started using `--tftp 3969`
    * [smb server](#smb-server) - unsafe, slow, not recommended for wan
    * [browser ux](#browser-ux) - tweaking the ui
    * [opengraph](#opengraph) - discord and social-media embeds
    * [file deduplication](#file-deduplication) - enable symlink-based upload deduplication
    * [file indexing](#file-indexing) - enable music search, upload-undo, and better dedup
        * [exclude-patterns](#exclude-patterns) - to save some time
        * [filesystem guards](#filesystem-guards) - avoid traversing into other filesystems
        * [periodic rescan](#periodic-rescan) - filesystem monitoring
    * [upload rules](#upload-rules) - set upload rules using volflags
    * [compress uploads](#compress-uploads) - files can be autocompressed on upload
    * [chmod and chown](#chmod-and-chown) - per-volume filesystem-permissions and ownership
    * [other flags](#other-flags)
    * [database location](#database-location) - in-volume (`.hist/up2k.db`, default) or somewhere else
    * [metadata from audio files](#metadata-from-audio-files) - set `-e2t` to index tags on upload
        * [metadata from xattrs](#metadata-from-xattrs) - unix extended file attributes
    * [file parser plugins](#file-parser-plugins) - provide custom parsers to index additional tags
    * [event hooks](#event-hooks) - trigger a program on uploads, renames etc ([examples](./bin/hooks/))
        * [zeromq](#zeromq) - event-hooks can send zeromq messages
        * [upload events](#upload-events) - the older, more powerful approach ([examples](./bin/mtag/))
    * [handlers](#handlers) - redefine behavior with plugins ([examples](./bin/handlers/))
    * [ip auth](#ip-auth) - autologin based on IP range (CIDR)
        * [restrict to ip](#restrict-to-ip) - limit a user to certain IP ranges (CIDR)
    * [identity providers](#identity-providers) - replace copyparty passwords with oauth and such
        * [generic header auth](#generic-header-auth) - other ways to auth by header
    * [user-changeable passwords](#user-changeable-passwords) - if permitted, users can change their own passwords
    * [using the cloud as storage](#using-the-cloud-as-storage) - connecting to an aws s3 bucket and similar
    * [hiding from google](#hiding-from-google) - tell search engines you don&#039;t wanna be indexed
    * [themes](#themes)
    * [complete examples](#complete-examples)
    * [listen on port 80 and 443](#listen-on-port-80-and-443) - become a *real* webserver
    * [reverse-proxy](#reverse-proxy) - running copyparty next to other websites
        * [real-ip](#real-ip) - teaching copyparty how to see client IPs
        * [reverse-proxy performance](#reverse-proxy-performance)
    * [permanent cloudflare tunnel](#permanent-cloudflare-tunnel) - if you have a domain and want to get your copyparty online real quick
    * [prometheus](#prometheus) - metrics/stats can be enabled
    * [other extremely specific features](#other-extremely-specific-features) - you&#039;ll never find a use for these
        * [custom mimetypes](#custom-mimetypes) - change the association of a file extension
        * [GDPR compliance](#GDPR-compliance) - imagine using copyparty professionally...
        * [feature chickenbits](#feature-chickenbits) - buggy feature? rip it out
        * [feature beefybits](#feature-beefybits) - force-enable features with known issues on your OS/env
* [packages](#packages) - the party might be closer than you think
    * [arch package](#arch-package) - `pacman -S copyparty` (in [arch linux extra](https://archlinux.org/packages/extra/any/copyparty/))
    * [fedora package](#fedora-package) - does not exist yet
    * [homebrew formulae](#homebrew-formulae) - `brew install copyparty ffmpeg`
    * [nix package](#nix-package) - `nix profile install github:9001/copyparty`
    * [nixos module](#nixos-module)
* [browser support](#browser-support) - TLDR: yes
* [server hall of fame](#server-hall-of-fame) - unexpected things that run copyparty
* [client examples](#client-examples) - interact with copyparty using non-browser clients
    * [folder sync](#folder-sync) - sync folders to/from copyparty
    * [mount as drive](#mount-as-drive) - a remote copyparty server as a local filesystem
* [android app](#android-app) - upload to copyparty with one tap
* [iOS shortcuts](#iOS-shortcuts) - there is no iPhone app, but
* [performance](#performance) - defaults are usually fine - expect `8 GiB/s` download, `1 GiB/s` upload
    * [client-side](#client-side) - when uploading files
* [security](#security) - there is a [discord server](https://discord.gg/25J8CdTT6G) with announcements
    * [gotchas](#gotchas) - behavior that might be unexpected
    * [cors](#cors) - cross-site request config
    * [filekeys](#filekeys) - prevent filename bruteforcing
        * [dirkeys](#dirkeys) - share specific folders in a volume
    * [password hashing](#password-hashing) - you can hash passwords
    * [https](#https) - both HTTP and HTTPS are accepted
* [recovering from crashes](#recovering-from-crashes)
    * [client crashes](#client-crashes)
        * [firefox wsod](#firefox-wsod) - firefox 87 can crash during uploads
* [HTTP API](#HTTP-API) - see [devnotes](./docs/devnotes.md#http-api)
* [dependencies](#dependencies) - mandatory deps
    * [optional dependencies](#optional-dependencies) - enable bonus features
        * [dependency chickenbits](#dependency-chickenbits) - prevent loading an optional dependency
        * [dependency unvendoring](#dependency-unvendoring) - force use of system modules
    * [optional gpl stuff](#optional-gpl-stuff)
* [sfx](#sfx) - the self-contained &quot;binary&quot; (recommended!)
    * [copyparty.exe](#copypartyexe) - download [copyparty.exe](https://github.com/9001/copyparty/releases/latest/download/copyparty.exe) (win8+) or [copyparty32.exe](https://github.com/9001/copyparty/releases/latest/download/copyparty32.exe) (win7+)
    * [zipapp](#zipapp) - another emergency alternative, [copyparty.pyz](https://github.com/9001/copyparty/releases/latest/download/copyparty.pyz)
* [install on android](#install-on-android)
* [install on iOS](#install-on-iOS)
* [reporting bugs](#reporting-bugs) - ideas for context to include, and where to submit them
* [devnotes](#devnotes) - for build instructions etc, see [./docs/devnotes.md](./docs/devnotes.md)


## quickstart

just run **[copyparty-sfx.py](https://github.com/9001/copyparty/releases/latest/download/copyparty-sfx.py)** -- that&#039;s it! üéâ

&gt; ‚ÑπÔ∏è the sfx is a [self-extractor](https://github.com/9001/copyparty/issues/270) which unpacks an embedded `tar.gz` into `$TEMP` -- if this looks too scary, you can use the [zipapp](#zipapp) which has slightly worse performance

* or install through [pypi](https://pypi.org/project/copyparty/): `python3 -m pip install --user -U copyparty`
* or if you cannot install python, you can use [copyparty.exe](#copypartyexe) instead
* or install [on arch](#arch-package) / [homebrew](#homebrew-formulae) ‚ï± [on NixOS](#nixos-module) ‚ï± [through nix](#nix-package)
* or if you are on android, [install copyparty in termux](#install-on-android)
* or maybe an iPhone or iPad? [install in a-Shell on iOS](#install-on-iOS)
* or maybe you have a [synology nas / dsm](./docs/synology-dsm.md)
* or if you have [uv](https://docs.astral.sh/uv/) installed, run `uv tool run copyparty`
* or if your computer is messed up and nothing else works, [try the pyz](#zipapp)
* or if your OS is dead, give the [bootable flashdrive / cd-rom](https://a.ocv.me/pub/stuff/edcd001/enterprise-edition/) a spin
* or if you don&#039;t trust copyparty yet and want to isolate it a little, then...
  * ...maybe [prisonparty](./bin/prisonparty.sh) to create a tiny [chroot](https://wiki.archlinux.org/title/Chroot) (very portable),
  * ...or [bubbleparty](./bin/bubbleparty.sh) to wrap it in [bubblewrap](https://github.com/containers/bubblewrap) (much better)
* or if you prefer to [use docker](./scripts/docker/) üêã you can do that too
  * docker has all deps built-in, so skip this step:

enable thumbnails (images/audio/video), media indexing, and audio transcoding by installing some recommended deps:

* **Alpine:** `apk add py3-pillow ffmpeg`
* **Debian:** `apt install --no-install-recommends python3-pil ffmpeg`
* **Fedora:** rpmfusion + `dnf install python3-pillow ffmpeg --allowerasing`
* **FreeBSD:** `pkg install py39-sqlite3 py39-pillow ffmpeg`
* **MacOS:** `port install py-Pillow ffmpeg`
* **MacOS** (alternative): `brew install pillow ffmpeg`
* **Windows:** `python -m pip install --user -U Pillow`
  * install [python](https://www.python.org/downloads/windows/) and [ffmpeg](#optional-dependencies) manually; do not use `winget` or `Microsoft Store` (it breaks $PATH)
  * copyparty.exe comes with `Pillow` and only needs [ffmpeg](#optional-dependencies) for mediatags/videothumbs
* see [optional dependencies](#optional-dependencies) to enable even more features

running copyparty without arguments (for example doubleclicking it on Windows) will give everyone read/write access to the current folder; you may want [accounts and volumes](#accounts-and-volumes)

or see [some usage examples](#complete-examples) for inspiration, or the [complete windows example](./docs/examples/windows.md)

some recommended options:
* `-e2dsa` enables general [file indexing](#file-indexing)
* `-e2ts` enables audio metadata indexing (needs either FFprobe or Mutagen)
* `-v /mnt/music:/music:r:rw,foo -a foo:bar` shares `/mnt/music` as `/music`, `r`eadable by anyone, and read-write for user `foo`, password `bar`
  * replace `:r:rw,foo` with `:r,foo` to only make the folder readable by `foo` and nobody else
  * see [accounts and volumes](#accounts-and-volumes) (or [`--help-accounts`](https://copyparty.eu/cli/#accounts-help-page)) for the syntax and other permissions


### mirrors

other places to download copyparty from  (non-github links):

* https://copyparty.eu/ (hetzner, finland, official mirror):
  * https://copyparty.eu/py = https://copyparty.eu/copyparty-sfx.py = the sfx
  * https://copyparty.eu/en = https://copyparty.eu/copyparty-en.py = the english-only sfx
  * https://copyparty.eu/pyz = https://copyparty.eu/copyparty.pyz = the zipapp
  * https://copyparty.eu/enz = https://copyparty.eu/copyparty-en.pyz = the enterprise pyz
  * https://copyparty.eu/cli = online cli helptext


### at home

make it accessible over the internet  by starting a [cloudflare quicktunnel](https://developers.cloudflare.com/cloudflare-one/connections/connect-networks/do-more-with-tunnels/trycloudflare/) like so:

first download [cloudflared](https://developers.cloudflare.com/cloudflare-one/connections/connect-networks/downloads/) and then start the tunnel with `cloudflared tunnel --url http://127.0.0.1:3923`

as the tunnel starts, it will show a URL which you can share to let anyone browse your stash or upload files to you

but if you have a domain, then you probably want to skip the random autogenerated URL and instead make a [permanent cloudflare tunnel](#permanent-cloudflare-tunnel)

since people will be connecting through cloudflare, run copyparty with `--xff-hdr cf-connecting-ip` to detect client IPs correctly


### on servers

you may also want these, especially on servers:

* [contrib/systemd/copyparty.service](contrib/systemd/copyparty.service) to run copyparty as a systemd service (see guide inside)
* [contrib/systemd/prisonparty.service](contrib/systemd/prisonparty.service) to run it in a chroot (for extra security)
* [contrib/podman-systemd/](contrib/podman-systemd/) to run copyparty in a Podman container as a systemd service (see guide inside)
* [contrib/openrc/copyparty](contrib/openrc/copyparty) to run copyparty on Alpine / Gentoo
* [contrib/rc/copyparty](contrib/rc/copyparty) to run copyparty on FreeBSD
* [nixos module](#nixos-module) to run copyparty on NixOS hosts
* [contrib/nginx/copyparty.conf](contrib/nginx/copyparty.conf) to [reverse-proxy](#reverse-proxy) behind nginx (for better https)

and remember to open the ports you want; here&#039;s a complete example including every feature copyparty has to offer:
```
firewall-cmd --permanent --add-port={80,443,3921,3922,3923,3945,3990}/tcp  # --zone=libvirt
firewall-cmd --permanent --add-port=12000-12099/tcp  # --zone=libvirt
firewall-cmd --permanent --add-port={69,1900,3969,5353}/udp  # --zone=libvirt
firewall-cmd --reload
```
(69:tftp, 1900:ssdp, 3921:ftp, 3922:sftp, 3923:http/https, 3945:smb, 3969:tftp, 3990:ftps, 5353:mdns, 12000:passive-ftp)


## features

also see [comparison to similar software](./docs/versus.md)

* backend stuff
  * ‚òë IPv6 + unix-sockets
  * ‚òë [multiprocessing](#performance) (actual multithreading)
  * ‚òë volumes (mountpoints)
  * ‚òë [accounts](#accounts-and-volumes)
  * ‚òë [ftp server](#ftp-server)
  * ‚òë [tftp server](#tftp-server)
  * ‚òë [webdav server](#webdav-server)
  * ‚òë [smb/cifs server](#smb-server)
  * ‚òë [qr-code](#qr-code) for quick access
  * ‚òë [upnp / zeroconf / mdns / ssdp](#zeroconf)
  * ‚òë [event hooks](#event-hooks) / script runner
  * ‚òë [reverse-proxy support](https://github.com/9001/copyparty#reverse-proxy)
  * ‚òë cross-platform (Windows, Linux, Macos, Android, iOS, FreeBSD, arm32/arm64, ppc64le, s390x, risc-v/riscv64, SGI IRIX)
* upload
  * ‚òë basic: plain multipart, ie6 support
  * ‚òë [up2k](#uploading): js, resumable, multithreaded
    * **no filesize limit!** even on Cloudflare
  * ‚òë stash: simple PUT filedropper
  * ‚òë filename randomizer
  * ‚òë write-only folders
  * ‚òë [unpost](#unpost): undo/delete accidental uploads
  * ‚òë [self-destruct](#self-destruct) (specified server-side or client-side)
  * ‚òë [race the beam](#race-the-beam) (almost like peer-to-peer)
  * ‚òë symlink/discard duplicates (content-matching)
* download
  * ‚òë single files in browser
  * ‚òë [folders as zip / tar files](#zip-downloads)
  * ‚òë [FUSE client](https://github.com/9001/copyparty/tree/hovudstraum/bin#partyfusepy) (read-only)
* browser
  * ‚òë [navpane](#navpane) (directory tree sidebar)
  * ‚òë file manager (cut/paste, delete, [batch-rename](#batch-rename))
  * ‚òë audio player (with [OS media con

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[AUTOMATIC1111/stable-diffusion-webui]]></title>
            <link>https://github.com/AUTOMATIC1111/stable-diffusion-webui</link>
            <guid>https://github.com/AUTOMATIC1111/stable-diffusion-webui</guid>
            <pubDate>Fri, 27 Feb 2026 00:07:59 GMT</pubDate>
            <description><![CDATA[Stable Diffusion web UI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">AUTOMATIC1111/stable-diffusion-webui</a></h1>
            <p>Stable Diffusion web UI</p>
            <p>Language: Python</p>
            <p>Stars: 161,339</p>
            <p>Forks: 30,086</p>
            <p>Stars today: 156 stars today</p>
            <h2>README</h2><pre># Stable Diffusion web UI
A web interface for Stable Diffusion, implemented using Gradio library.

![](screenshot.png)

## Features
[Detailed feature showcase with images](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features):
- Original txt2img and img2img modes
- One click install and run script (but you still must install python and git)
- Outpainting
- Inpainting
- Color Sketch
- Prompt Matrix
- Stable Diffusion Upscale
- Attention, specify parts of text that the model should pay more attention to
    - a man in a `((tuxedo))` - will pay more attention to tuxedo
    - a man in a `(tuxedo:1.21)` - alternative syntax
    - select text and press `Ctrl+Up` or `Ctrl+Down` (or `Command+Up` or `Command+Down` if you&#039;re on a MacOS) to automatically adjust attention to selected text (code contributed by anonymous user)
- Loopback, run img2img processing multiple times
- X/Y/Z plot, a way to draw a 3 dimensional plot of images with different parameters
- Textual Inversion
    - have as many embeddings as you want and use any names you like for them
    - use multiple embeddings with different numbers of vectors per token
    - works with half precision floating point numbers
    - train embeddings on 8GB (also reports of 6GB working)
- Extras tab with:
    - GFPGAN, neural network that fixes faces
    - CodeFormer, face restoration tool as an alternative to GFPGAN
    - RealESRGAN, neural network upscaler
    - ESRGAN, neural network upscaler with a lot of third party models
    - SwinIR and Swin2SR ([see here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/2092)), neural network upscalers
    - LDSR, Latent diffusion super resolution upscaling
- Resizing aspect ratio options
- Sampling method selection
    - Adjust sampler eta values (noise multiplier)
    - More advanced noise setting options
- Interrupt processing at any time
- 4GB video card support (also reports of 2GB working)
- Correct seeds for batches
- Live prompt token length validation
- Generation parameters
     - parameters you used to generate images are saved with that image
     - in PNG chunks for PNG, in EXIF for JPEG
     - can drag the image to PNG info tab to restore generation parameters and automatically copy them into UI
     - can be disabled in settings
     - drag and drop an image/text-parameters to promptbox
- Read Generation Parameters Button, loads parameters in promptbox to UI
- Settings page
- Running arbitrary python code from UI (must run with `--allow-code` to enable)
- Mouseover hints for most UI elements
- Possible to change defaults/mix/max/step values for UI elements via text config
- Tiling support, a checkbox to create images that can be tiled like textures
- Progress bar and live image generation preview
    - Can use a separate neural network to produce previews with almost none VRAM or compute requirement
- Negative prompt, an extra text field that allows you to list what you don&#039;t want to see in generated image
- Styles, a way to save part of prompt and easily apply them via dropdown later
- Variations, a way to generate same image but with tiny differences
- Seed resizing, a way to generate same image but at slightly different resolution
- CLIP interrogator, a button that tries to guess prompt from an image
- Prompt Editing, a way to change prompt mid-generation, say to start making a watermelon and switch to anime girl midway
- Batch Processing, process a group of files using img2img
- Img2img Alternative, reverse Euler method of cross attention control
- Highres Fix, a convenience option to produce high resolution pictures in one click without usual distortions
- Reloading checkpoints on the fly
- Checkpoint Merger, a tab that allows you to merge up to 3 checkpoints into one
- [Custom scripts](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Custom-Scripts) with many extensions from community
- [Composable-Diffusion](https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/), a way to use multiple prompts at once
     - separate prompts using uppercase `AND`
     - also supports weights for prompts: `a cat :1.2 AND a dog AND a penguin :2.2`
- No token limit for prompts (original stable diffusion lets you use up to 75 tokens)
- DeepDanbooru integration, creates danbooru style tags for anime prompts
- [xformers](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers), major speed increase for select cards: (add `--xformers` to commandline args)
- via extension: [History tab](https://github.com/yfszzx/stable-diffusion-webui-images-browser): view, direct and delete images conveniently within the UI
- Generate forever option
- Training tab
     - hypernetworks and embeddings options
     - Preprocessing images: cropping, mirroring, autotagging using BLIP or deepdanbooru (for anime)
- Clip skip
- Hypernetworks
- Loras (same as Hypernetworks but more pretty)
- A separate UI where you can choose, with preview, which embeddings, hypernetworks or Loras to add to your prompt
- Can select to load a different VAE from settings screen
- Estimated completion time in progress bar
- API
- Support for dedicated [inpainting model](https://github.com/runwayml/stable-diffusion#inpainting-with-stable-diffusion) by RunwayML
- via extension: [Aesthetic Gradients](https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients), a way to generate images with a specific aesthetic by using clip images embeds (implementation of [https://github.com/vicgalle/stable-diffusion-aesthetic-gradients](https://github.com/vicgalle/stable-diffusion-aesthetic-gradients))
- [Stable Diffusion 2.0](https://github.com/Stability-AI/stablediffusion) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#stable-diffusion-20) for instructions
- [Alt-Diffusion](https://arxiv.org/abs/2211.06679) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#alt-diffusion) for instructions
- Now without any bad letters!
- Load checkpoints in safetensors format
- Eased resolution restriction: generated image&#039;s dimensions must be a multiple of 8 rather than 64
- Now with a license!
- Reorder elements in the UI from settings screen
- [Segmind Stable Diffusion](https://huggingface.co/segmind/SSD-1B) support

## Installation and Running
Make sure the required [dependencies](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Dependencies) are met and follow the instructions available for:
- [NVidia](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs) (recommended)
- [AMD](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-AMD-GPUs) GPUs.
- [Intel CPUs, Intel GPUs (both integrated and discrete)](https://github.com/openvinotoolkit/stable-diffusion-webui/wiki/Installation-on-Intel-Silicon) (external wiki page)
- [Ascend NPUs](https://github.com/wangshuai09/stable-diffusion-webui/wiki/Install-and-run-on-Ascend-NPUs) (external wiki page)

Alternatively, use online services (like Google Colab):

- [List of Online Services](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services)

### Installation on Windows 10/11 with NVidia-GPUs using release package
1. Download `sd.webui.zip` from [v1.0.0-pre](https://github.com/AUTOMATIC1111/stable-diffusion-webui/releases/tag/v1.0.0-pre) and extract its contents.
2. Run `update.bat`.
3. Run `run.bat`.
&gt; For more details see [Install-and-Run-on-NVidia-GPUs](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs)

### Automatic Installation on Windows
1. Install [Python 3.10.6](https://www.python.org/downloads/release/python-3106/) (Newer version of Python does not support torch), checking &quot;Add Python to PATH&quot;.
2. Install [git](https://git-scm.com/download/win).
3. Download the stable-diffusion-webui repository, for example by running `git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git`.
4. Run `webui-user.bat` from Windows Explorer as normal, non-administrator, user.

### Automatic Installation on Linux
1. Install the dependencies:
```bash
# Debian-based:
sudo apt install wget git python3 python3-venv libgl1 libglib2.0-0
# Red Hat-based:
sudo dnf install wget git python3 gperftools-libs libglvnd-glx
# openSUSE-based:
sudo zypper install wget git python3 libtcmalloc4 libglvnd
# Arch-based:
sudo pacman -S wget git python3
```
If your system is very new, you need to install python3.11 or python3.10:
```bash
# Ubuntu 24.04
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt update
sudo apt install python3.11

# Manjaro/Arch
sudo pacman -S yay
yay -S python311 # do not confuse with python3.11 package

# Only for 3.11
# Then set up env variable in launch script
export python_cmd=&quot;python3.11&quot;
# or in webui-user.sh
python_cmd=&quot;python3.11&quot;
```
2. Navigate to the directory you would like the webui to be installed and execute the following command:
```bash
wget -q https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh
```
Or just clone the repo wherever you want:
```bash
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui
```

3. Run `webui.sh`.
4. Check `webui-user.sh` for options.
### Installation on Apple Silicon

Find the instructions [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Installation-on-Apple-Silicon).

## Contributing
Here&#039;s how to add code to this repo: [Contributing](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)

## Documentation

The documentation was moved from this README over to the project&#039;s [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki).

For the purposes of getting Google and other search engines to crawl the wiki, here&#039;s a link to the (not for humans) [crawlable wiki](https://github-wiki-see.page/m/AUTOMATIC1111/stable-diffusion-webui/wiki).

## Credits
Licenses for borrowed code can be found in `Settings -&gt; Licenses` screen, and also in `html/licenses.html` file.

- Stable Diffusion - https://github.com/Stability-AI/stablediffusion, https://github.com/CompVis/taming-transformers, https://github.com/mcmonkey4eva/sd3-ref
- k-diffusion - https://github.com/crowsonkb/k-diffusion.git
- Spandrel - https://github.com/chaiNNer-org/spandrel implementing
  - GFPGAN - https://github.com/TencentARC/GFPGAN.git
  - CodeFormer - https://github.com/sczhou/CodeFormer
  - ESRGAN - https://github.com/xinntao/ESRGAN
  - SwinIR - https://github.com/JingyunLiang/SwinIR
  - Swin2SR - https://github.com/mv-lab/swin2sr
- LDSR - https://github.com/Hafiidz/latent-diffusion
- MiDaS - https://github.com/isl-org/MiDaS
- Ideas for optimizations - https://github.com/basujindal/stable-diffusion
- Cross Attention layer optimization - Doggettx - https://github.com/Doggettx/stable-diffusion, original idea for prompt editing.
- Cross Attention layer optimization - InvokeAI, lstein - https://github.com/invoke-ai/InvokeAI (originally http://github.com/lstein/stable-diffusion)
- Sub-quadratic Cross Attention layer optimization - Alex Birch (https://github.com/Birch-san/diffusers/pull/1), Amin Rezaei (https://github.com/AminRezaei0x443/memory-efficient-attention)
- Textual Inversion - Rinon Gal - https://github.com/rinongal/textual_inversion (we&#039;re not using his code, but we are using his ideas).
- Idea for SD upscale - https://github.com/jquesnelle/txt2imghd
- Noise generation for outpainting mk2 - https://github.com/parlance-zz/g-diffuser-bot
- CLIP interrogator idea and borrowing some code - https://github.com/pharmapsychotic/clip-interrogator
- Idea for Composable Diffusion - https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch
- xformers - https://github.com/facebookresearch/xformers
- DeepDanbooru - interrogator for anime diffusers https://github.com/KichangKim/DeepDanbooru
- Sampling in float32 precision from a float16 UNet - marunine for the idea, Birch-san for the example Diffusers implementation (https://github.com/Birch-san/diffusers-play/tree/92feee6)
- Instruct pix2pix - Tim Brooks (star), Aleksander Holynski (star), Alexei A. Efros (no star) - https://github.com/timothybrooks/instruct-pix2pix
- Security advice - RyotaK
- UniPC sampler - Wenliang Zhao - https://github.com/wl-zhao/UniPC
- TAESD - Ollin Boer Bohan - https://github.com/madebyollin/taesd
- LyCORIS - KohakuBlueleaf
- Restart sampling - lambertae - https://github.com/Newbeeer/diffusion_restart_sampling
- Hypertile - tfernd - https://github.com/tfernd/HyperTile
- Initial Gradio script - posted on 4chan by an Anonymous user. Thank you Anonymous user.
- (You)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NVIDIA/Megatron-LM]]></title>
            <link>https://github.com/NVIDIA/Megatron-LM</link>
            <guid>https://github.com/NVIDIA/Megatron-LM</guid>
            <pubDate>Fri, 27 Feb 2026 00:07:58 GMT</pubDate>
            <description><![CDATA[Ongoing research training transformer models at scale]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NVIDIA/Megatron-LM">NVIDIA/Megatron-LM</a></h1>
            <p>Ongoing research training transformer models at scale</p>
            <p>Language: Python</p>
            <p>Stars: 15,437</p>
            <p>Forks: 3,634</p>
            <p>Stars today: 150 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

Megatron-LM and Megatron Core
=============================

&lt;h4&gt;GPU-optimized library for training transformer models at scale&lt;/h4&gt;

[![Documentation](https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat)](https://docs.nvidia.com/megatron-core/developer-guide/latest/index.html)
[![version](https://img.shields.io/badge/release-0.15.0-green)](./CHANGELOG.md)
[![license](https://img.shields.io/badge/license-Apache-blue)](./LICENSE)

&lt;div align=&quot;left&quot;&gt;

## About

This repository contains two components: **Megatron-LM** and **Megatron Core**.

**Megatron-LM** is a reference example that includes Megatron Core plus pre-configured training scripts. Best for research teams, learning distributed training, and quick experimentation.

**Megatron Core** is a composable library with GPU-optimized building blocks for custom training frameworks. It provides transformer building blocks, advanced parallelism strategies (TP, PP, DP, EP, CP), mixed precision support (FP16, BF16, FP8, FP4), and model architectures. Best for framework developers and ML engineers building custom training pipelines.

**[Megatron Bridge](https://github.com/NVIDIA-NeMo/Megatron-Bridge)** provides bidirectional Hugging Face ‚Üî Megatron checkpoint conversion with production-ready recipes.


## Quick Start

Install Megatron Core with pip:

1. Install Megatron Core with required dependencies:

    ```bash
    pip install --no-build-isolation megatron-core[mlm,dev]
    ```

2. Clone repository for examples:

    ```bash
    git clone https://github.com/NVIDIA/Megatron-LM.git
    cd Megatron-LM
    pip install --no-build-isolation .[mlm,dev]
    ```


# Latest News

- **[2026/01]** **[Dynamic Context Parallelism](https://developer.nvidia.com/blog/speeding-up-variable-length-training-with-dynamic-context-parallelism-and-nvidia-megatron-core/)** - Up to 1.48x speedup for variable-length sequence training with adaptive CP sizing.
- **[2025/12]** **Megatron Core development has moved to GitHub!** All development and CI now happens in the open. We welcome community contributions.
- **[2025/10]** **[Megatron Dev Branch](https://github.com/NVIDIA/Megatron-LM/tree/dev)** - early access branch with experimental features.
- **[2025/10]** **[Megatron Bridge](https://github.com/NVIDIA-NeMo/Megatron-Bridge)** - Bidirectional converter for interoperability between Hugging Face and Megatron checkpoints, featuring production-ready recipes for popular models.
- **[2025/08]** **[MoE Q3-Q4 2025 Roadmap](https://github.com/NVIDIA/Megatron-LM/issues/1729)** - Comprehensive roadmap for MoE features including DeepSeek-V3, Qwen3, advanced parallelism strategies, FP8 optimizations, and Blackwell performance enhancements.
- **[2025/08]** **[GPT-OSS Model](https://github.com/NVIDIA/Megatron-LM/issues/1739)** - Advanced features including YaRN RoPE scaling, attention sinks, and custom activation functions are being integrated into Megatron Core.
- **[2025/06]** **[Megatron MoE Model Zoo](https://github.com/yanring/Megatron-MoE-ModelZoo)** - Best practices and optimized configurations for training DeepSeek-V3, Mixtral, and Qwen3 MoE models with performance benchmarking and checkpoint conversion tools.
- **[2025/05]** Megatron Core v0.11.0 brings new capabilities for multi-data center LLM training ([blog](https://developer.nvidia.com/blog/turbocharge-llm-training-across-long-haul-data-center-networks-with-nvidia-nemo-framework/)).

&lt;details&gt;
&lt;summary&gt;Previous News&lt;/summary&gt;

- **[2024/07]** Megatron Core v0.7 improves scalability and training resiliency and adds support for multimodal training ([blog](https://developer.nvidia.com/blog/train-generative-ai-models-more-efficiently-with-new-nvidia-Megatron-Core-functionalities/)).
- **[2024/06]** Megatron Core added supports for Mamba-based models. Check out our paper [An Empirical Study of Mamba-based Language Models](https://arxiv.org/pdf/2406.07887) and [code example](https://github.com/NVIDIA/Megatron-LM/tree/ssm/examples/mamba).
- **[2024/01 Announcement]** NVIDIA has released the core capabilities in **Megatron-LM** into [**Megatron Core**](https://github.com/NVIDIA/Megatron-LM/tree/main/megatron/core) in this repository. Megatron Core expands upon Megatron-LM&#039;s GPU-optimized techniques with more cutting-edge innovations on system-level optimizations, featuring composable and modular APIs.

&lt;/details&gt;


# Project Structure

```
Megatron-LM/
‚îú‚îÄ‚îÄ megatron/
‚îÇ   ‚îú‚îÄ‚îÄ core/                    # Megatron Core (kernels, parallelism, building blocks)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/              # Transformer models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transformer/         # Transformer building blocks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tensor_parallel/     # Tensor parallelism
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipeline_parallel/   # Pipeline parallelism
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ distributed/         # Distributed training (FSDP, DDP)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ optimizer/           # Optimizers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ datasets/            # Dataset loaders
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inference/           # Inference engines and server
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ export/              # Model export (e.g. TensorRT-LLM)
‚îÇ   ‚îú‚îÄ‚îÄ training/                # Training scripts
‚îÇ   ‚îú‚îÄ‚îÄ legacy/                  # Legacy components
‚îÇ   ‚îú‚îÄ‚îÄ post_training/           # Post-training (quantization, distillation, pruning, etc.)
‚îÇ   ‚îî‚îÄ‚îÄ rl/                      # Reinforcement learning (RLHF, etc.)
‚îú‚îÄ‚îÄ examples/                    # Ready-to-use training examples
‚îú‚îÄ‚îÄ tools/                       # Utility tools
‚îú‚îÄ‚îÄ tests/                       # Comprehensive test suite
‚îî‚îÄ‚îÄ docs/                        # Documentation
```


# Performance Benchmarking

For our latest performance benchmarking results, please refer to [NVIDIA Megatron Bridge Performance Summary](https://docs.nvidia.com/nemo/megatron-bridge/latest/performance-summary.html).

Our codebase efficiently trains models from 2B to 462B parameters across thousands of GPUs, achieving up to **47% Model FLOP Utilization (MFU)** on H100 clusters.

![Model table](images/model_table.png)

**Benchmark Configuration:**

- **Vocabulary size**: 131,072 tokens
- **Sequence length**: 4096 tokens
- **Model scaling**: Varied hidden size, attention heads, and layers to achieve target parameter counts
- **Communication optimizations**: Fine-grained overlapping with DP (`--overlap-grad-reduce`, `--overlap-param-gather`), TP (`--tp-comm-overlap`), and PP (enabled by default)

**Key Results:**

- **6144 H100 GPUs**: Successfully benchmarked 462B parameter model training
- **Superlinear scaling**: MFU increases from 41% to 47-48% with model size
- **End-to-end measurement**: Throughputs include all operations (data loading, optimizer steps, communication, logging)
- **Production ready**: Full training pipeline with checkpointing and fault tolerance
- *Note: Performance results measured without training to convergence*

## Weak Scaling Results

Our weak scaled results show superlinear scaling (MFU increases from 41% for the smallest model considered to 47-48% for the largest models); this is because larger GEMMs have higher arithmetic intensity and are consequently more efficient to execute.

![Weak scaling](images/weak_scaling.png)

## Strong Scaling Results

We also strong scaled the standard GPT-3 model (our version has slightly more than 175 billion parameters due to larger vocabulary size) from 96 H100 GPUs to 4608 GPUs, using the same batch size of 1152 sequences throughout. Communication becomes more exposed at larger scale, leading to a reduction in MFU from 47% to 42%.

![Strong scaling](images/strong_scaling.png)


# Roadmaps

- **[MoE Roadmap](https://github.com/NVIDIA/Megatron-LM/issues/1729)** - DeepSeek-V3, Qwen3, advanced parallelism, FP8 optimizations, and Blackwell enhancements


# Resources

## Getting Help

- üìñ **[Documentation](https://docs.nvidia.com/megatron-core/developer-guide/latest/index.html)** - Official documentation
- üêõ **[Issues](https://github.com/NVIDIA/Megatron-LM/issues)** - Bug reports and feature requests

## Contributing

We ‚ù§Ô∏è contributions! Ways to contribute:

- üêõ **Report bugs** - Help us improve reliability
- üí° **Suggest features** - Shape the future of Megatron Core
- üìù **Improve docs** - Make Megatron Core more accessible
- üîß **Submit PRs** - Contribute code improvements

**‚Üí [Contributing Guide](https://docs.nvidia.com/megatron-core/developer-guide/latest/developer/contribute.html)**

## Citation

If you use Megatron in your research or project, we appreciate that you use the following citations:

```bibtex
@article{megatron-lm,
  title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[MoonshotAI/kimi-cli]]></title>
            <link>https://github.com/MoonshotAI/kimi-cli</link>
            <guid>https://github.com/MoonshotAI/kimi-cli</guid>
            <pubDate>Fri, 27 Feb 2026 00:07:57 GMT</pubDate>
            <description><![CDATA[Kimi Code CLI is your next CLI agent.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/MoonshotAI/kimi-cli">MoonshotAI/kimi-cli</a></h1>
            <p>Kimi Code CLI is your next CLI agent.</p>
            <p>Language: Python</p>
            <p>Stars: 6,837</p>
            <p>Forks: 654</p>
            <p>Stars today: 51 stars today</p>
            <h2>README</h2><pre># Kimi Code CLI

[![Commit Activity](https://img.shields.io/github/commit-activity/w/MoonshotAI/kimi-cli)](https://github.com/MoonshotAI/kimi-cli/graphs/commit-activity)
[![Checks](https://img.shields.io/github/check-runs/MoonshotAI/kimi-cli/main)](https://github.com/MoonshotAI/kimi-cli/actions)
[![Version](https://img.shields.io/pypi/v/kimi-cli)](https://pypi.org/project/kimi-cli/)
[![Downloads](https://img.shields.io/pypi/dw/kimi-cli)](https://pypistats.org/packages/kimi-cli)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/MoonshotAI/kimi-cli)

[Kimi Code](https://www.kimi.com/code/) | [Documentation](https://moonshotai.github.io/kimi-cli/en/) | [ÊñáÊ°£](https://moonshotai.github.io/kimi-cli/zh/)

Kimi Code CLI is an AI agent that runs in the terminal, helping you complete software development tasks and terminal operations. It can read and edit code, execute shell commands, search and fetch web pages, and autonomously plan and adjust actions during execution.

## Getting Started

See [Getting Started](https://moonshotai.github.io/kimi-cli/en/guides/getting-started.html) for how to install and start using Kimi Code CLI.

## Key Features

### Shell command mode

Kimi Code CLI is not only a coding agent, but also a shell. You can switch the shell command mode by pressing `Ctrl-X`. In this mode, you can directly run shell commands without leaving Kimi Code CLI.

![](./docs/media/shell-mode.gif)

&gt; [!NOTE]
&gt; Built-in shell commands like `cd` are not supported yet.

### VS Code extension

Kimi Code CLI can be integrated with [Visual Studio Code](https://code.visualstudio.com/) via the [Kimi Code VS Code Extension](https://marketplace.visualstudio.com/items?itemName=moonshot-ai.kimi-code).

![VS Code Extension](./docs/media/vscode.png)

### IDE integration via ACP

Kimi Code CLI supports [Agent Client Protocol] out of the box. You can use it together with any ACP-compatible editor or IDE.

[Agent Client Protocol]: https://github.com/agentclientprotocol/agent-client-protocol

To use Kimi Code CLI with ACP clients, make sure to run Kimi Code CLI in the terminal and send `/login` to complete the login first. Then, you can configure your ACP client to start Kimi Code CLI as an ACP agent server with command `kimi acp`.

For example, to use Kimi Code CLI with [Zed](https://zed.dev/) or [JetBrains](https://blog.jetbrains.com/ai/2025/12/bring-your-own-ai-agent-to-jetbrains-ides/), add the following configuration to your `~/.config/zed/settings.json` or `~/.jetbrains/acp.json` file:

```json
{
  &quot;agent_servers&quot;: {
    &quot;Kimi Code CLI&quot;: {
      &quot;command&quot;: &quot;kimi&quot;,
      &quot;args&quot;: [&quot;acp&quot;],
      &quot;env&quot;: {}
    }
  }
}
```

Then you can create Kimi Code CLI threads in IDE&#039;s agent panel.

![](./docs/media/acp-integration.gif)

### Zsh integration

You can use Kimi Code CLI together with Zsh, to empower your shell experience with AI agent capabilities.

Install the [zsh-kimi-cli](https://github.com/MoonshotAI/zsh-kimi-cli) plugin via:

```sh
git clone https://github.com/MoonshotAI/zsh-kimi-cli.git \
  ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/kimi-cli
```

&gt; [!NOTE]
&gt; If you are using a plugin manager other than Oh My Zsh, you may need to refer to the plugin&#039;s README for installation instructions.

Then add `kimi-cli` to your Zsh plugin list in `~/.zshrc`:

```sh
plugins=(... kimi-cli)
```

After restarting Zsh, you can switch to agent mode by pressing `Ctrl-X`.

### MCP support

Kimi Code CLI supports MCP (Model Context Protocol) tools.

**`kimi mcp` sub-command group**

You can manage MCP servers with `kimi mcp` sub-command group. For example:

```sh
# Add streamable HTTP server:
kimi mcp add --transport http context7 https://mcp.context7.com/mcp --header &quot;CONTEXT7_API_KEY: ctx7sk-your-key&quot;

# Add streamable HTTP server with OAuth authorization:
kimi mcp add --transport http --auth oauth linear https://mcp.linear.app/mcp

# Add stdio server:
kimi mcp add --transport stdio chrome-devtools -- npx chrome-devtools-mcp@latest

# List added MCP servers:
kimi mcp list

# Remove an MCP server:
kimi mcp remove chrome-devtools

# Authorize an MCP server:
kimi mcp auth linear
```

**Ad-hoc MCP configuration**

Kimi Code CLI also supports ad-hoc MCP server configuration via CLI option.

Given an MCP config file in the well-known MCP config format like the following:

```json
{
  &quot;mcpServers&quot;: {
    &quot;context7&quot;: {
      &quot;url&quot;: &quot;https://mcp.context7.com/mcp&quot;,
      &quot;headers&quot;: {
        &quot;CONTEXT7_API_KEY&quot;: &quot;YOUR_API_KEY&quot;
      }
    },
    &quot;chrome-devtools&quot;: {
      &quot;command&quot;: &quot;npx&quot;,
      &quot;args&quot;: [&quot;-y&quot;, &quot;chrome-devtools-mcp@latest&quot;]
    }
  }
}
```

Run `kimi` with `--mcp-config-file` option to connect to the specified MCP servers:

```sh
kimi --mcp-config-file /path/to/mcp.json
```

### More

See more features in the [Documentation](https://moonshotai.github.io/kimi-cli/en/).

## Development

To develop Kimi Code CLI, run:

```sh
git clone https://github.com/MoonshotAI/kimi-cli.git
cd kimi-cli

make prepare  # prepare the development environment
```

Then you can start working on Kimi Code CLI.

Refer to the following commands after you make changes:

```sh
uv run kimi  # run Kimi Code CLI

make format  # format code
make check  # run linting and type checking
make test  # run tests
make test-kimi-cli  # run Kimi Code CLI tests only
make test-kosong  # run kosong tests only
make test-pykaos  # run pykaos tests only
make build-web  # build the web UI and sync it into the package (requires Node.js/npm)
make build  # build python packages
make build-bin  # build standalone binary
make help  # show all make targets
```

Note: `make build` and `make build-bin` automatically run `make build-web` to embed the web UI.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>