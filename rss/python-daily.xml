<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Wed, 13 Aug 2025 00:04:16 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[denizsafak/abogen]]></title>
            <link>https://github.com/denizsafak/abogen</link>
            <guid>https://github.com/denizsafak/abogen</guid>
            <pubDate>Wed, 13 Aug 2025 00:04:16 GMT</pubDate>
            <description><![CDATA[Generate audiobooks from EPUBs, PDFs and text with synchronized captions.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/denizsafak/abogen">denizsafak/abogen</a></h1>
            <p>Generate audiobooks from EPUBs, PDFs and text with synchronized captions.</p>
            <p>Language: Python</p>
            <p>Stars: 2,284</p>
            <p>Forks: 112</p>
            <p>Stars today: 153 stars today</p>
            <h2>README</h2><pre># abogen &lt;img width=&quot;40px&quot; title=&quot;abogen icon&quot; src=&quot;https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/abogen/assets/icon.ico&quot; align=&quot;right&quot; style=&quot;padding-left: 10px; padding-top:5px;&quot;&gt;

[![Build Status](https://github.com/denizsafak/abogen/actions/workflows/test_pip.yml/badge.svg)](https://github.com/denizsafak/abogen/actions)
[![GitHub Release](https://img.shields.io/github/v/release/denizsafak/abogen)](https://github.com/denizsafak/abogen/releases/latest)
[![Abogen PyPi Python Versions](https://img.shields.io/pypi/pyversions/abogen)](https://pypi.org/project/abogen/)
[![Operating Systems](https://img.shields.io/badge/os-windows%20%7C%20linux%20%7C%20macos%20-blue)](https://github.com/denizsafak/abogen/releases/latest)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![License: MIT](https://img.shields.io/badge/License-MIT-maroon.svg)](https://opensource.org/licenses/MIT)

Abogen is a powerful text-to-speech conversion tool that makes it easy to turn ePub, PDF, or text files into high-quality audio with matching subtitles in seconds. Use it for audiobooks, voiceovers for Instagram, YouTube, TikTok, or any project that needs natural-sounding text-to-speech, using [Kokoro-82M](https://huggingface.co/hexgrad/Kokoro-82M).

&lt;img title=&quot;Abogen Main&quot; src=&#039;https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen.png&#039; width=&quot;380&quot;&gt; &lt;img title=&quot;Abogen Processing&quot; src=&#039;https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen2.png&#039; width=&quot;380&quot;&gt;

## Demo
https://github.com/user-attachments/assets/cb66512d-0a52-48c3-bda4-f1e6a03fb8d6

&gt; This demo was generated in just 5¬†seconds, producing ‚àº1¬†minute of audio with perfectly synced subtitles. To create a similar video, see [the demo guide](https://github.com/denizsafak/abogen/tree/main/demo).

## `How to install?` &lt;a href=&quot;https://pypi.org/project/abogen/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/pyversions/abogen&quot; alt=&quot;Abogen Compatible PyPi Python Versions&quot; align=&quot;right&quot; style=&quot;margin-top:6px;&quot;&gt;&lt;/a&gt;

### Windows
Go to [espeak-ng latest release](https://github.com/espeak-ng/espeak-ng/releases/latest) download and run the *.msi file.

#### OPTION 1: Install using script
1. [Download](https://github.com/denizsafak/abogen/archive/refs/heads/main.zip) the repository
2. Extract the ZIP file
3. Run `WINDOWS_INSTALL.bat` by double-clicking it

This method handles everything automatically - installing all dependencies including CUDA in a self-contained environment without requiring a separate Python installation. (You still need to install [espeak-ng](https://github.com/espeak-ng/espeak-ng/releases/latest).)

&gt; [!NOTE]
&gt; You don&#039;t need to install Python separately. The script will install Python automatically.

#### OPTION 2: Install using pip
```bash
# Create a virtual environment (optional)
mkdir abogen &amp;&amp; cd abogen
python -m venv venv
venv\Scripts\activate

# For NVIDIA GPUs:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

# For AMD GPUs:
# Not supported yet, because ROCm is not available on Windows. Use Linux if you have AMD GPU.

# Install abogen
pip install abogen
```

### Mac
```bash
# Install espeak-ng
brew install espeak-ng

# Create a virtual environment (recommended)
mkdir abogen &amp;&amp; cd abogen
python3 -m venv venv
source venv/bin/activate

# Install abogen
pip3 install abogen
```
### Linux
```bash
# Install espeak-ng
sudo apt install espeak-ng # Ubuntu/Debian
sudo pacman -S espeak-ng # Arch Linux
sudo dnf install espeak-ng # Fedora

# Create a virtual environment (recommended)
mkdir abogen &amp;&amp; cd abogen
python3 -m venv venv
source venv/bin/activate

# Install abogen
pip3 install abogen

# For NVIDIA GPUs:
# Already supported, no need to install CUDA separately.

# For AMD GPUs:
# After installing abogen, we need to uninstall the existing torch package
pip3 uninstall torch 
pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.4
```
&gt; [!TIP]
&gt; If you get `WARNING: The script abogen-cli is installed in &#039;/home/username/.local/bin&#039; which is not on PATH.` error, run the following command to add it to your PATH:
&gt;```bash
&gt;echo &quot;export PATH=\&quot;/home/$USER/.local/bin:\$PATH\&quot;&quot; &gt;&gt; ~/.bashrc &amp;&amp; source ~/.bashrc
&gt;```

&gt; [!TIP]
&gt; If you get &quot;No matching distribution found&quot; error, try installing it on supported Python (3.10 to 3.12). You can use [pyenv](https://github.com/pyenv/pyenv) to manage multiple Python versions easily in Linux. Watch this [video](https://www.youtube.com/watch?v=MVyb-nI4KyI) by NetworkChuck for a quick guide.

&gt; Special thanks to [@hg000125](https://github.com/hg000125) for his contribution in [#23](https://github.com/denizsafak/abogen/issues/23). AMD GPU support is possible thanks to his work.

## `How to run?`
If you installed using pip, you can simply run the following command to start Abogen:

```bash
abogen
```
&gt; [!TIP]
&gt; If you installed using the Windows installer `(WINDOWS_INSTALL.bat)`, It should have created a shortcut in the same folder, or your desktop. You can run it from there. If you lost the shortcut, Abogen is located in `python_embedded/Scripts/abogen.exe`. You can run it from there directly.

## `How to use?`
1) Drag and drop any ePub, PDF, or text file (or use the built-in text editor)
2) Configure the settings:
    - Set speech speed
    - Select a voice (or create a custom voice using voice mixer)
    - Select subtitle generation style (by sentence, word, etc.)
    - Select output format
    - Select where to save the output
3) Hit Start

## `In action`
&lt;img title=&quot;Abogen in action&quot; src=&#039;https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/abogen.gif&#039;&gt; 

Here‚Äôs Abogen in action: in this demo, it processes ‚àº3,000 characters of text in just 11 seconds and turns it into 3 minutes and 28 seconds of audio, and I have a low-end **RTX¬†2060¬†Mobile laptop GPU**. Your results may vary depending on your hardware.

## `Configuration`

| Options | Description |
|---------|-------------|
| **Input Box** | Drag and drop `ePub`, `PDF`, or `.TXT` files (or use built-in text editor) |
| **Queue options** | Add multiple files to a queue and process them in batch, with individual settings for each file. See [Queue mode](#queue-mode) for more details. |
| **Speed** | Adjust speech rate from `0.1x` to `2.0x` |
| **Select Voice** | First letter of the language code (e.g., `a` for American English, `b` for British English, etc.), second letter is for `m` for male and `f` for female. |
| **Voice mixer** | Create custom voices by mixing different voice models with a profile system. See [Voice Mixer](#voice-mixer) for more details. |
| **Voice preview** | Listen to the selected voice before processing. |
| **Generate subtitles** | `Disabled`, `Sentence`, `Sentence + Comma`, `1 word`, `2 words`, `3 words`, etc. (Represents the number of words in each subtitle entry) |
| **Output voice format** | `.WAV`, `.FLAC`, `.MP3`, `.OPUS (best compression)` and `M4B (with chapters)` (Special thanks to [@jborza](https://github.com/jborza) for chapter support in PR [#10](https://github.com/denizsafak/abogen/pull/10)) |
| **Output subtitle format** | Configures the subtitle format as `SRT (standard)`, `ASS (wide)`, `ASS (narrow)`, `ASS (centered wide)`, or `ASS (centered narrow)`. |
| **Replace single newlines with spaces** | Replaces single newlines with spaces in the text. This is useful for texts that have imaginary line breaks. |
| **Save location** | `Save next to input file`, `Save to desktop`, or `Choose output folder` |

| Book handler options | Description |
|---------|-------------|
| **Chapter Control** | Select specific `chapters` from ePUBs or `chapters + pages` from PDFs. |
| **Save each chapter separately** | Save each chapter in e-books as a separate audio file. |
| **Create a merged version** | Create a single audio file that combines all chapters. (If `Save each chapter separately` is disabled, this option will be the default behavior.) |
| **Save in a project folder with metadata** | Save the converted items in a project folder with available metadata files. |

| Menu options | Description |
|---------|-------------|
| **Theme** | Change the application&#039;s theme using `System`, `Light`, or `Dark` options. |
| **Configure max words per subtitle** | Configures the maximum number of words per subtitle entry. |
| **Configure max lines in log window** | Configures the maximum number of lines to display in the log window. |
| **Separate chapters audio format** | Configures the audio format for separate chapters as `wav`, `flac`, `mp3`, or `opus`. |
| **Create desktop shortcut** | Creates a shortcut on your desktop for easy access. |
| **Open config directory** | Opens the directory where the configuration file is stored. |
| **Open cache directory** | Opens the cache directory where converted text files are stored. |
| **Clear cache files** | Deletes cache files created during the conversion or preview. |
| **Check for updates at startup** | Automatically checks for updates when the program starts. |
| **Disable Kokoro&#039;s internet access** | Prevents Kokoro from downloading models or voices from HuggingFace Hub, useful for offline use. |
| **Reset to default settings** | Resets all settings to their default values. |

## `Voice Mixer`
&lt;img title=&quot;Abogen Voice Mixer&quot; src=&#039;https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/voice_mixer.png&#039;&gt;

With voice mixer, you can create custom voices by mixing different voice models. You can adjust the weight of each voice and save your custom voice as a profile for future use. The voice mixer allows you to create unique and personalized voices. (Huge thanks to [@jborza](https://github.com/jborza) for making this possible through his contributions in [#5](https://github.com/denizsafak/abogen/pull/5))

## `Queue Mode`
&lt;img title=&quot;Abogen queue mode&quot; src=&#039;https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/queue.png&#039;&gt;

Abogen supports **queue mode**, allowing you to add multiple files to a processing queue. This is useful if you want to convert several files in one batch.

- You can add text files (`.txt`) directly using the **Add files** button in the Queue Manager. To add PDF or EPUB files, use the input box in the main window and click the **Add to Queue** button.
- Each file in the queue keeps the configuration settings that were active when it was added. Changing the main window configuration afterward does **not** affect files already in the queue.
- You can view each file&#039;s configuration by hovering over them.

Abogen will process each item in the queue automatically, saving outputs as configured.
&gt; Special thanks to [@jborza](https://github.com/jborza) for adding queue mode in PR [#35](https://github.com/denizsafak/abogen/pull/35)

## `About Chapter Markers`
When you process ePUB or PDF files, Abogen converts them into text files stored in your cache directory. When you click &quot;Edit,&quot; you&#039;re actually modifying these converted text files. In these text files, you&#039;ll notice tags that look like this:

```
&lt;&lt;CHAPTER_MARKER:Chapter Title&gt;&gt;
```
These are chapter markers. They are automatically added when you process ePUB or PDF files, based on the chapters you select. They serve an important purpose:
-  Allow you to split the text into separate audio files for each chapter
-  Save time by letting you reprocess only specific chapters if errors occur, rather than the entire file

You can manually add these markers to plain text files for the same benefits. Simply include them in your text like this:

```
&lt;&lt;CHAPTER_MARKER:Introduction&gt;&gt;
This is the beginning of my text...  

&lt;&lt;CHAPTER_MARKER:Main Content&gt;&gt; 
Here&#039;s another part...  
```
When you process the text file, Abogen will detect these markers automatically and ask if you want to save each chapter separately and create a merged version.

![Abogen Chapter Marker](https://raw.githubusercontent.com/denizsafak/abogen/refs/heads/main/demo/chapter_marker.png)

## `About Metadata Tags`
Similar to chapter markers, it is possible to add metadata tags for `M4B` files. This is useful for audiobook players that support metadata, allowing you to add information like title, author, year, etc. Abogen automatically adds these tags when you process ePUB or PDF files, but you can also add them manually to your text files. Add metadata tags **at the beginning of your text file** like this:
```
&lt;&lt;METADATA_TITLE:Title&gt;&gt;
&lt;&lt;METADATA_ARTIST:Author&gt;&gt;
&lt;&lt;METADATA_ALBUM:Album Title&gt;&gt;
&lt;&lt;METADATA_YEAR:Year&gt;&gt;
&lt;&lt;METADATA_ALBUM_ARTIST:Album Artist&gt;&gt;
&lt;&lt;METADATA_COMPOSER:Narrator&gt;&gt;
&lt;&lt;METADATA_GENRE:Audiobook&gt;&gt;
```

## `Supported Languages`
```
# üá∫üá∏ &#039;a&#039; =&gt; American English, üá¨üáß &#039;b&#039; =&gt; British English
# üá™üá∏ &#039;e&#039; =&gt; Spanish es
# üá´üá∑ &#039;f&#039; =&gt; French fr-fr
# üáÆüá≥ &#039;h&#039; =&gt; Hindi hi
# üáÆüáπ &#039;i&#039; =&gt; Italian it
# üáØüáµ &#039;j&#039; =&gt; Japanese: pip install misaki[ja]
# üáßüá∑ &#039;p&#039; =&gt; Brazilian Portuguese pt-br
# üá®üá≥ &#039;z&#039; =&gt; Mandarin Chinese: pip install misaki[zh]
```
For a complete list of supported languages and voices, refer to Kokoro&#039;s [VOICES.md](https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md). To listen to sample audio outputs, see [SAMPLES.md](https://huggingface.co/hexgrad/Kokoro-82M/blob/main/SAMPLES.md).

## `MPV Config`
I highly recommend using [MPV](https://mpv.io/installation/) to play your audio files, as it supports displaying subtitles even without a video track. Here&#039;s my `mpv.conf`:
```
# --- MPV Settings ---
save-position-on-quit
keep-open=yes
# --- Subtitle ---
sub-ass-override=no
sub-margin-y=50
sub-margin-x=50
# --- Audio Quality ---
audio-spdif=ac3,dts,eac3,truehd,dts-hd
audio-channels=auto
audio-samplerate=48000
volume-max=200
```

## `Docker Guide`
If you want to run Abogen in a Docker container:
1) [Download the repository](https://github.com/denizsafak/abogen/archive/refs/heads/main.zip) and extract, or clone it using git.
2) Go to `abogen` folder. You should see `Dockerfile` there.
3) Open your termminal in that directory and run the following commands:

```bash
# Build the Docker image:
docker build --progress plain -t abogen .

# Note that building the image may take a while.
# After building is complete, run the Docker container:

# Windows
docker run --name abogen -v %cd%:/shared -p 5800:5800 -p 5900:5900 --gpus all abogen

# Linux
docker run --name abogen -v $(pwd):/shared -p 5800:5800 -p 5900:5900 --gpus all abogen

# MacOS
docker run --name abogen -v $(pwd):/shared -p 5800:5800 -p 5900:5900 abogen

# We expose port 5800 for use by a web browser, 5900 if you want to connect with a VNC client.
```

Abogen launches automatically inside the container. 
- You can access it via a web browser at [http://localhost:5800](http://localhost:5800) or connect to it using a VNC client at `localhost:5900`.
- You can use `/shared` directory to share files between your host and the container.
- For later use, start it with `docker start abogen` and stop it with `docker stop abogen`.

Known issues:
- Audio preview is not working inside container (ALSA error).
- `Open cache directory` and `Open configuration directory` options in settings not working. (Tried pcmanfm, did not work with Abogen).

(Special thanks to [@geo38](https://www.reddit.com/user/geo38/) from Reddit, who provided the Dockerfile and instructions in [this comment](https://www.reddit.com/r/selfhosted/comments/1k8x1yo/comment/mpe0bz8/).)

## `Similar Projects`
Abogen is a standalone project, but it is inspired by and shares some similarities with other projects. Here are a few:
- [audiblez](https://github.com/santinic/audiblez): Generate audiobooks from e-books. **(Has CLI and GUI support)**
- [autiobooks](https://github.com/plusuncold/autiobooks): Automatically convert epubs to audiobooks
- [pdf-narrator](https://github.com/mateogon/pdf-narrator): Convert your PDFs and EPUBs into audiobooks effortlessly.
- [epub_to_audiobook](https://github.com/p0n1/epub_to_audiobook): EPUB to audiobook converter, optimized for Audiobookshelf
- [ebook2audiobook](https://github.com/DrewThomasson/ebook2audiobook): Convert ebooks to audiobooks with chapters and metadata using dynamic AI models and voice cloning

## `Roadmap`
- [ ] Add OCR scan feature for PDF files using docling/teserract.
- [x] Add chapter metadata for .m4a files. (Issue [#9](https://github.com/denizsafak/abogen/issues/9), PR [#10](https://github.com/denizsafak/abogen/pull/10))
- [ ] Add support for different languages in GUI.
- [x] Add voice formula feature that enables mixing different voice models. (Issue [#1](https://github.com/denizsafak/abogen/issues/1), PR [#5](https://github.com/denizsafak/abogen/pull/5))
- [ ] Add support for kokoro-onnx (If it&#039;s necessary).
- [x] Add dark mode.

## `Troubleshooting`
If you encounter any issues while running Abogen, try launching it from the command line with:
```
abogen-cli
```
This will start Abogen in command-line mode and display detailed error messages. Please open a new issue on the [Issues](https://github.com/denizsafak/abogen/issues) page with the error message and a description of your problem.

## `Contributing`
I welcome contributions! If you have ideas for new features, improvements, or bug fixes, please fork the repository and submit a pull request.
### For developers and contributors
If you&#039;d like to modify the code and contribute to development, you can [download the repository](https://github.com/denizsafak/abogen/archive/refs/heads/main.zip), extract it and run the following commands to build **or** install the package:
```bash
# Go to the directory where you extracted the repository and run:
pip install -e .      # Installs the package in editable mode
pip install build     # Install the build package
python -m build       # Builds the package in dist folder (optional)
abogen                # Opens the GUI
```
Feel free to explore the code and make any changes you like.

## `Credits`
- Abogen uses [Kokoro](https://github.com/hexgrad/kokoro) for its high-quality, natural-sounding text-to-speech synthesis. Huge thanks to the Kokoro team for making this possible.
- Thanks to [@wojiushixiaobai](https://github.com/wojiushixiaobai) for [Embedded Python](https://github.com/wojiushixiaobai/Python-Embed-Win64) packages. These modified packages include pip pre-installed, enabling Abogen to function as a standalone application without requiring users to separately install Python in Windows.
- Thanks to creators of [EbookLib](https://github.com/aerkalov/ebooklib), a Python library for reading and writing ePub files, which is used for extracting text from ePub files.
- Special thanks to the [PyQt](https://www.riverbankcomputing.com/software/pyqt/) team for providing the cross-platform GUI toolkit that powers Abogen&#039;s interface.
- Icons: [US](https://icons8.com/icon/aRiu1GGi6Aoe/usa), [Great Britain](https://icons8.com/icon/t3NE3BsOAQwq/great-britain), [Spain](https://icons8.com/icon/ly7tzANRt33n/spain), [France](https://icons8.com/icon/3muzEmi4dpD5/france), [India](https://icons8.com/icon/esGVrxg9VCJ1/india), [Italy](https://icons8.com/icon/PW8KZnP7qXzO/italy), [Japan](https://icons8.com/icon/McQbrq9qaQye/japan), [Brazil](https://icons8.com/icon/zHmH8HpOmM90/brazil), [China](https://icons8.com/icon/Ej50Oe3crXwF/china), [Female](https://icons8.com/icon/uI49hxbpxTkp/female), [Male](https://icons8.com/icon/12351/male), [Adjust](https://icons8.com/icon/21698/adjust) and [Voice Id](https://icons8.com/icon/GskSeVoroQ7u/voice-id) icons by [Icons8](https://icons8.com/).

## `License`
This project is available under the MIT License - see the [LICENSE](https://github.com/denizsafak/abogen/blob/main/LICENSE) file for details.
[Kokoro](https://github.com/hexgrad/kokoro) is licensed under [Apache-2.0](https://github.com/hexgrad/kokoro/blob/main/LICENSE) which allows commercial use, modification, distribution, and private use.

&gt; [!IMPORTANT]
&gt; Subtitle generation currentl

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[donnemartin/system-design-primer]]></title>
            <link>https://github.com/donnemartin/system-design-primer</link>
            <guid>https://github.com/donnemartin/system-design-primer</guid>
            <pubDate>Wed, 13 Aug 2025 00:04:15 GMT</pubDate>
            <description><![CDATA[Learn how to design large-scale systems. Prep for the system design interview. Includes Anki flashcards.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/donnemartin/system-design-primer">donnemartin/system-design-primer</a></h1>
            <p>Learn how to design large-scale systems. Prep for the system design interview. Includes Anki flashcards.</p>
            <p>Language: Python</p>
            <p>Stars: 315,643</p>
            <p>Forks: 51,757</p>
            <p>Stars today: 276 stars today</p>
            <h2>README</h2><pre>*[English](README.md) ‚àô [Êó•Êú¨Ë™û](README-ja.md) ‚àô [ÁÆÄ‰Ωì‰∏≠Êñá](README-zh-Hans.md) ‚àô [ÁπÅÈ´î‰∏≠Êñá](README-zh-TW.md) | [ÿßŸÑÿπŸéÿ±Ÿéÿ®ŸêŸäŸéŸëÿ©‚Äé](https://github.com/donnemartin/system-design-primer/issues/170) ‚àô [‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ](https://github.com/donnemartin/system-design-primer/issues/220) ‚àô [Portugu√™s do Brasil](https://github.com/donnemartin/system-design-primer/issues/40) ‚àô [Deutsch](https://github.com/donnemartin/system-design-primer/issues/186) ‚àô [ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨](https://github.com/donnemartin/system-design-primer/issues/130) ‚àô [◊¢◊ë◊®◊ô◊™](https://github.com/donnemartin/system-design-primer/issues/272) ‚àô [Italiano](https://github.com/donnemartin/system-design-primer/issues/104) ‚àô [ÌïúÍµ≠Ïñ¥](https://github.com/donnemartin/system-design-primer/issues/102) ‚àô [ŸÅÿßÿ±ÿ≥€å](https://github.com/donnemartin/system-design-primer/issues/110) ‚àô [Polski](https://github.com/donnemartin/system-design-primer/issues/68) ‚àô [—Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫](https://github.com/donnemartin/system-design-primer/issues/87) ‚àô [Espa√±ol](https://github.com/donnemartin/system-design-primer/issues/136) ‚àô [‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢](https://github.com/donnemartin/system-design-primer/issues/187) ‚àô [T√ºrk√ße](https://github.com/donnemartin/system-design-primer/issues/39) ‚àô [ti·∫øng Vi·ªát](https://github.com/donnemartin/system-design-primer/issues/127) ‚àô [Fran√ßais](https://github.com/donnemartin/system-design-primer/issues/250) | [Add Translation](https://github.com/donnemartin/system-design-primer/issues/28)*

**Help [translate](TRANSLATIONS.md) this guide!**

# The System Design Primer

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/jj3A5N8.png&quot;&gt;
  &lt;br/&gt;
&lt;/p&gt;

## Motivation

&gt; Learn how to design large-scale systems.
&gt;
&gt; Prep for the system design interview.

### Learn how to design large-scale systems

Learning how to design scalable systems will help you become a better engineer.

System design is a broad topic.  There is a **vast amount of resources scattered throughout the web** on system design principles.

This repo is an **organized collection** of resources to help you learn how to build systems at scale.

### Learn from the open source community

This is a continually updated, open source project.

[Contributions](#contributing) are welcome!

### Prep for the system design interview

In addition to coding interviews, system design is a **required component** of the **technical interview process** at many tech companies.

**Practice common system design interview questions** and **compare** your results with **sample solutions**: discussions, code, and diagrams.

Additional topics for interview prep:

* [Study guide](#study-guide)
* [How to approach a system design interview question](#how-to-approach-a-system-design-interview-question)
* [System design interview questions, **with solutions**](#system-design-interview-questions-with-solutions)
* [Object-oriented design interview questions, **with solutions**](#object-oriented-design-interview-questions-with-solutions)
* [Additional system design interview questions](#additional-system-design-interview-questions)

## Anki flashcards

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/zdCAkB3.png&quot;&gt;
  &lt;br/&gt;
&lt;/p&gt;

The provided [Anki flashcard decks](https://apps.ankiweb.net/) use spaced repetition to help you retain key system design concepts.

* [System design deck](https://github.com/donnemartin/system-design-primer/tree/master/resources/flash_cards/System%20Design.apkg)
* [System design exercises deck](https://github.com/donnemartin/system-design-primer/tree/master/resources/flash_cards/System%20Design%20Exercises.apkg)
* [Object oriented design exercises deck](https://github.com/donnemartin/system-design-primer/tree/master/resources/flash_cards/OO%20Design.apkg)

Great for use while on-the-go.

### Coding Resource: Interactive Coding Challenges

Looking for resources to help you prep for the [**Coding Interview**](https://github.com/donnemartin/interactive-coding-challenges)?

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/b4YtAEN.png&quot;&gt;
  &lt;br/&gt;
&lt;/p&gt;

Check out the sister repo [**Interactive Coding Challenges**](https://github.com/donnemartin/interactive-coding-challenges), which contains an additional Anki deck:

* [Coding deck](https://github.com/donnemartin/interactive-coding-challenges/tree/master/anki_cards/Coding.apkg)

## Contributing

&gt; Learn from the community.

Feel free to submit pull requests to help:

* Fix errors
* Improve sections
* Add new sections
* [Translate](https://github.com/donnemartin/system-design-primer/issues/28)

Content that needs some polishing is placed [under development](#under-development).

Review the [Contributing Guidelines](CONTRIBUTING.md).

## Index of system design topics

&gt; Summaries of various system design topics, including pros and cons.  **Everything is a trade-off**.
&gt;
&gt; Each section contains links to more in-depth resources.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/jrUBAF7.png&quot;&gt;
  &lt;br/&gt;
&lt;/p&gt;

* [System design topics: start here](#system-design-topics-start-here)
    * [Step 1: Review the scalability video lecture](#step-1-review-the-scalability-video-lecture)
    * [Step 2: Review the scalability article](#step-2-review-the-scalability-article)
    * [Next steps](#next-steps)
* [Performance vs scalability](#performance-vs-scalability)
* [Latency vs throughput](#latency-vs-throughput)
* [Availability vs consistency](#availability-vs-consistency)
    * [CAP theorem](#cap-theorem)
        * [CP - consistency and partition tolerance](#cp---consistency-and-partition-tolerance)
        * [AP - availability and partition tolerance](#ap---availability-and-partition-tolerance)
* [Consistency patterns](#consistency-patterns)
    * [Weak consistency](#weak-consistency)
    * [Eventual consistency](#eventual-consistency)
    * [Strong consistency](#strong-consistency)
* [Availability patterns](#availability-patterns)
    * [Fail-over](#fail-over)
    * [Replication](#replication)
    * [Availability in numbers](#availability-in-numbers)
* [Domain name system](#domain-name-system)
* [Content delivery network](#content-delivery-network)
    * [Push CDNs](#push-cdns)
    * [Pull CDNs](#pull-cdns)
* [Load balancer](#load-balancer)
    * [Active-passive](#active-passive)
    * [Active-active](#active-active)
    * [Layer 4 load balancing](#layer-4-load-balancing)
    * [Layer 7 load balancing](#layer-7-load-balancing)
    * [Horizontal scaling](#horizontal-scaling)
* [Reverse proxy (web server)](#reverse-proxy-web-server)
    * [Load balancer vs reverse proxy](#load-balancer-vs-reverse-proxy)
* [Application layer](#application-layer)
    * [Microservices](#microservices)
    * [Service discovery](#service-discovery)
* [Database](#database)
    * [Relational database management system (RDBMS)](#relational-database-management-system-rdbms)
        * [Master-slave replication](#master-slave-replication)
        * [Master-master replication](#master-master-replication)
        * [Federation](#federation)
        * [Sharding](#sharding)
        * [Denormalization](#denormalization)
        * [SQL tuning](#sql-tuning)
    * [NoSQL](#nosql)
        * [Key-value store](#key-value-store)
        * [Document store](#document-store)
        * [Wide column store](#wide-column-store)
        * [Graph Database](#graph-database)
    * [SQL or NoSQL](#sql-or-nosql)
* [Cache](#cache)
    * [Client caching](#client-caching)
    * [CDN caching](#cdn-caching)
    * [Web server caching](#web-server-caching)
    * [Database caching](#database-caching)
    * [Application caching](#application-caching)
    * [Caching at the database query level](#caching-at-the-database-query-level)
    * [Caching at the object level](#caching-at-the-object-level)
    * [When to update the cache](#when-to-update-the-cache)
        * [Cache-aside](#cache-aside)
        * [Write-through](#write-through)
        * [Write-behind (write-back)](#write-behind-write-back)
        * [Refresh-ahead](#refresh-ahead)
* [Asynchronism](#asynchronism)
    * [Message queues](#message-queues)
    * [Task queues](#task-queues)
    * [Back pressure](#back-pressure)
* [Communication](#communication)
    * [Transmission control protocol (TCP)](#transmission-control-protocol-tcp)
    * [User datagram protocol (UDP)](#user-datagram-protocol-udp)
    * [Remote procedure call (RPC)](#remote-procedure-call-rpc)
    * [Representational state transfer (REST)](#representational-state-transfer-rest)
* [Security](#security)
* [Appendix](#appendix)
    * [Powers of two table](#powers-of-two-table)
    * [Latency numbers every programmer should know](#latency-numbers-every-programmer-should-know)
    * [Additional system design interview questions](#additional-system-design-interview-questions)
    * [Real world architectures](#real-world-architectures)
    * [Company architectures](#company-architectures)
    * [Company engineering blogs](#company-engineering-blogs)
* [Under development](#under-development)
* [Credits](#credits)
* [Contact info](#contact-info)
* [License](#license)

## Study guide

&gt; Suggested topics to review based on your interview timeline (short, medium, long).

![Imgur](images/OfVllex.png)

**Q: For interviews, do I need to know everything here?**

**A: No, you don&#039;t need to know everything here to prepare for the interview**.

What you are asked in an interview depends on variables such as:

* How much experience you have
* What your technical background is
* What positions you are interviewing for
* Which companies you are interviewing with
* Luck

More experienced candidates are generally expected to know more about system design.  Architects or team leads might be expected to know more than individual contributors.  Top tech companies are likely to have one or more design interview rounds.

Start broad and go deeper in a few areas.  It helps to know a little about various key system design topics.  Adjust the following guide based on your timeline, experience, what positions you are interviewing for, and which companies you are interviewing with.

* **Short timeline** - Aim for **breadth** with system design topics.  Practice by solving **some** interview questions.
* **Medium timeline** - Aim for **breadth** and **some depth** with system design topics.  Practice by solving **many** interview questions.
* **Long timeline** - Aim for **breadth** and **more depth** with system design topics.  Practice by solving **most** interview questions.

| | Short | Medium | Long |
|---|---|---|---|
| Read through the [System design topics](#index-of-system-design-topics) to get a broad understanding of how systems work | :+1: | :+1: | :+1: |
| Read through a few articles in the [Company engineering blogs](#company-engineering-blogs) for the companies you are interviewing with | :+1: | :+1: | :+1: |
| Read through a few [Real world architectures](#real-world-architectures) | :+1: | :+1: | :+1: |
| Review [How to approach a system design interview question](#how-to-approach-a-system-design-interview-question) | :+1: | :+1: | :+1: |
| Work through [System design interview questions with solutions](#system-design-interview-questions-with-solutions) | Some | Many | Most |
| Work through [Object-oriented design interview questions with solutions](#object-oriented-design-interview-questions-with-solutions) | Some | Many | Most |
| Review [Additional system design interview questions](#additional-system-design-interview-questions) | Some | Many | Most |

## How to approach a system design interview question

&gt; How to tackle a system design interview question.

The system design interview is an **open-ended conversation**.  You are expected to lead it.

You can use the following steps to guide the discussion.  To help solidify this process, work through the [System design interview questions with solutions](#system-design-interview-questions-with-solutions) section using the following steps.

### Step 1: Outline use cases, constraints, and assumptions

Gather requirements and scope the problem.  Ask questions to clarify use cases and constraints.  Discuss assumptions.

* Who is going to use it?
* How are they going to use it?
* How many users are there?
* What does the system do?
* What are the inputs and outputs of the system?
* How much data do we expect to handle?
* How many requests per second do we expect?
* What is the expected read to write ratio?

### Step 2: Create a high level design

Outline a high level design with all important components.

* Sketch the main components and connections
* Justify your ideas

### Step 3: Design core components

Dive into details for each core component.  For example, if you were asked to [design a url shortening service](solutions/system_design/pastebin/README.md), discuss:

* Generating and storing a hash of the full url
    * [MD5](solutions/system_design/pastebin/README.md) and [Base62](solutions/system_design/pastebin/README.md)
    * Hash collisions
    * SQL or NoSQL
    * Database schema
* Translating a hashed url to the full url
    * Database lookup
* API and object-oriented design

### Step 4: Scale the design

Identify and address bottlenecks, given the constraints.  For example, do you need the following to address scalability issues?

* Load balancer
* Horizontal scaling
* Caching
* Database sharding

Discuss potential solutions and trade-offs.  Everything is a trade-off.  Address bottlenecks using [principles of scalable system design](#index-of-system-design-topics).

### Back-of-the-envelope calculations

You might be asked to do some estimates by hand.  Refer to the [Appendix](#appendix) for the following resources:

* [Use back of the envelope calculations](http://highscalability.com/blog/2011/1/26/google-pro-tip-use-back-of-the-envelope-calculations-to-choo.html)
* [Powers of two table](#powers-of-two-table)
* [Latency numbers every programmer should know](#latency-numbers-every-programmer-should-know)

### Source(s) and further reading

Check out the following links to get a better idea of what to expect:

* [How to ace a systems design interview](https://www.palantir.com/2011/10/how-to-rock-a-systems-design-interview/)
* [The system design interview](http://www.hiredintech.com/system-design)
* [Intro to Architecture and Systems Design Interviews](https://www.youtube.com/watch?v=ZgdS0EUmn70)
* [System design template](https://leetcode.com/discuss/career/229177/My-System-Design-Template)

## System design interview questions with solutions

&gt; Common system design interview questions with sample discussions, code, and diagrams.
&gt;
&gt; Solutions linked to content in the `solutions/` folder.

| Question | |
|---|---|
| Design Pastebin.com (or Bit.ly) | [Solution](solutions/system_design/pastebin/README.md) |
| Design the Twitter timeline and search (or Facebook feed and search) | [Solution](solutions/system_design/twitter/README.md) |
| Design a web crawler | [Solution](solutions/system_design/web_crawler/README.md) |
| Design Mint.com | [Solution](solutions/system_design/mint/README.md) |
| Design the data structures for a social network | [Solution](solutions/system_design/social_graph/README.md) |
| Design a key-value store for a search engine | [Solution](solutions/system_design/query_cache/README.md) |
| Design Amazon&#039;s sales ranking by category feature | [Solution](solutions/system_design/sales_rank/README.md) |
| Design a system that scales to millions of users on AWS | [Solution](solutions/system_design/scaling_aws/README.md) |
| Add a system design question | [Contribute](#contributing) |

### Design Pastebin.com (or Bit.ly)

[View exercise and solution](solutions/system_design/pastebin/README.md)

![Imgur](images/4edXG0T.png)

### Design the Twitter timeline and search (or Facebook feed and search)

[View exercise and solution](solutions/system_design/twitter/README.md)

![Imgur](images/jrUBAF7.png)

### Design a web crawler

[View exercise and solution](solutions/system_design/web_crawler/README.md)

![Imgur](images/bWxPtQA.png)

### Design Mint.com

[View exercise and solution](solutions/system_design/mint/README.md)

![Imgur](images/V5q57vU.png)

### Design the data structures for a social network

[View exercise and solution](solutions/system_design/social_graph/README.md)

![Imgur](images/cdCv5g7.png)

### Design a key-value store for a search engine

[View exercise and solution](solutions/system_design/query_cache/README.md)

![Imgur](images/4j99mhe.png)

### Design Amazon&#039;s sales ranking by category feature

[View exercise and solution](solutions/system_design/sales_rank/README.md)

![Imgur](images/MzExP06.png)

### Design a system that scales to millions of users on AWS

[View exercise and solution](solutions/system_design/scaling_aws/README.md)

![Imgur](images/jj3A5N8.png)

## Object-oriented design interview questions with solutions

&gt; Common object-oriented design interview questions with sample discussions, code, and diagrams.
&gt;
&gt; Solutions linked to content in the `solutions/` folder.

&gt;**Note: This section is under development**

| Question | |
|---|---|
| Design a hash map | [Solution](solutions/object_oriented_design/hash_table/hash_map.ipynb)  |
| Design a least recently used cache | [Solution](solutions/object_oriented_design/lru_cache/lru_cache.ipynb)  |
| Design a call center | [Solution](solutions/object_oriented_design/call_center/call_center.ipynb)  |
| Design a deck of cards | [Solution](solutions/object_oriented_design/deck_of_cards/deck_of_cards.ipynb)  |
| Design a parking lot | [Solution](solutions/object_oriented_design/parking_lot/parking_lot.ipynb)  |
| Design a chat server | [Solution](solutions/object_oriented_design/online_chat/online_chat.ipynb)  |
| Design a circular array | [Contribute](#contributing)  |
| Add an object-oriented design question | [Contribute](#contributing) |

## System design topics: start here

New to system design?

First, you&#039;ll need a basic understanding of common principles, learning about what they are, how they are used, and their pros and cons.

### Step 1: Review the scalability video lecture

[Scalability Lecture at Harvard](https://www.youtube.com/watch?v=-W9F__D3oY4)

* Topics covered:
    * Vertical scaling
    * Horizontal scaling
    * Caching
    * Load balancing
    * Database replication
    * Database partitioning

### Step 2: Review the scalability article

[Scalability](https://web.archive.org/web/20221030091841/http://www.lecloud.net/tagged/scalability/chrono)

* Topics covered:
    * [Clones](https://web.archive.org/web/20220530193911/https://www.lecloud.net/post/7295452622/scalability-for-dummies-part-1-clones)
    * [Databases](https://web.archive.org/web/20220602114024/https://www.lecloud.net/post/7994751381/scalability-for-dummies-part-2-database)
    * [Caches](https://web.archive.org/web/20230126233752/https://www.lecloud.net/post/9246290032/scalability-for-dummies-part-3-cache)
    * [Asynchronism](https://web.archive.org/web/20220926171507/https://www.lecloud.net/post/9699762917/scalability-for-dummies-part-4-asynchronism)

### Next steps

Next, we&#039;ll look at high-level trade-offs:

* **Performance** vs **scalability**
* **Latency** vs **throughput**
* **Availability** vs **consistency**

Keep in mind that **everything is a trade-off**.

Then we&#039;ll dive into more specific topics such as DNS, CDNs, and load balancers.

## Performance vs scalability

A service is **scalable** if it results in increased **performance** in a manner proportional to resources added. Generally, increasing performance means serving more units of work, but it can also be to handle larger units of work, such as when datasets grow.&lt;sup&gt;&lt;a href=http://www.allthingsdistributed.com/2006/03/a_word_on_scalability.html&gt;1&lt;/a&gt;&lt;/sup&gt;

Another way to look at performance vs scalability:

* If you have a **performance** problem, your system is slow for a single user.
* If you have a **scalability** problem, your system is fast for a single user but slow under heavy load.

### Source(s) and further reading

* [A word on scalability](http://www.allthingsdistributed.com/2006/03/a_word_on_scalability.html)
* [Scalability, availability, s

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[trailofbits/buttercup]]></title>
            <link>https://github.com/trailofbits/buttercup</link>
            <guid>https://github.com/trailofbits/buttercup</guid>
            <pubDate>Wed, 13 Aug 2025 00:04:14 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/trailofbits/buttercup">trailofbits/buttercup</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 703</p>
            <p>Forks: 72</p>
            <p>Stars today: 223 stars today</p>
            <h2>README</h2><pre># Buttercup Cyber Reasoning System (CRS)

[![Tests](https://github.com/trailofbits/buttercup/actions/workflows/tests.yml/badge.svg)](https://github.com/trailofbits/buttercup/actions/workflows/tests.yml)
[![Tests (Nightly)](https://github.com/trailofbits/buttercup/actions/workflows/tests.yml/badge.svg?event=schedule)](https://github.com/trailofbits/buttercup/actions/workflows/tests.yml)
[![Integration](https://github.com/trailofbits/buttercup/actions/workflows/integration.yml/badge.svg)](https://github.com/trailofbits/buttercup/actions/workflows/integration.yml)

**Buttercup** is a Cyber Reasoning System (CRS) developed by **Trail of Bits** for the **DARPA AIxCC (AI Cyber Challenge)**. Buttercup finds and patches software vulnerabilities in open-source code repositories like [example-libpng](https://github.com/tob-challenges/example-libpng). It starts by running an AI/ML-assisted fuzzing campaign (built on oss-fuzz) for the program. When vulnerabilities are found, Buttercup analyzes them and uses a multi-agent AI-driven patcher to repair the vulnerability. **Buttercup** system consists of several components:

- **Orchestrator**: Coordinates the overall task process and manages the workflow
- **Seed Generator**: Creates inputs for vulnerability discovery
- **Fuzzer**: Discovers vulnerabilities through intelligent fuzzing techniques
- **Program Model**: Analyzes code structure and semantics for better understanding
- **Patcher**: Generates and applies security patches to fix vulnerabilities

## System Requirements

### Minimum Requirements

- **CPU:** 8 cores
- **Memory:** 16 GB RAM
- **Storage:** 100 GB available disk space
- **Network:** Stable internet connection for downloading dependencies

**Note:** Buttercup uses third-party AI providers (LLMs from companies like OpenAI, Anthropic and Google), which cost money. Please ensure that you manage per-deployment costs by using the built-in LLM budget setting.

**Note:** Buttercup works best with access to models from OpenAI **and** Anthropic, but can be run with at least one API key from one third-party provider (support for Gemini coming soon).

### Supported Systems

- **Linux x86_64** (fully supported)
- **ARM64** (partial support for upstream Google OSS-Fuzz projects)

### Required System Packages

Before setup, ensure you have these packages installed:

```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install -y make curl git

# RHEL/CentOS/Fedora
sudo yum install -y make curl git
# or
sudo dnf install -y make curl git

# MacOS
brew install make curl git
```

### Supported Targets

Buttercup works with:

- **C source code repositories** that are OSS-Fuzz compatible
- **Java source code repositories** that are OSS-Fuzz compatible
- Projects that build successfully and have existing fuzzing harnesses

## Quick Start

1. Clone the repository with submodules:

```bash
git clone --recurse-submodules https://github.com/trailofbits/buttercup.git
cd buttercup
```

2. Run automated setup (Recommended)

```bash
make setup-local
```

This script will install all dependencies, configure the environment, and guide you through the setup process.

**Note:** If you prefer manual setup, see the [Manual Setup Guide](MANUAL_SETUP.md).

3. Start Buttercup locally

```bash
make deploy-local
```

4. Verify local deployment:

```bash
make status
```

When a deployment is successful, you should see all pods in &quot;Running&quot; or &quot;Completed&quot; status.

5. Send Buttercup a simple task

**Note:** When tasked, Buttercup will start consuming third-party AI resources.

This command will make Buttercup pull down an example repo [example-libpng](https://github.com/tob-challenges/example-libpng) with a known vulnerability. Buttercup will start fuzzing it to find and patch vulnerabilities.

```bash
make send-libpng-task
```

6. Access Buttercup&#039;s web-based GUI

Run:

```bash
make web-ui
```

Then navigate to `http://localhost:31323` in your web browser.

In the GUI you can monitor active tasks and see when Buttercup finds bugs and generates patches for them.

7. Stop Buttercup

**Note:** This is an important step to ensure Buttercup shuts down and stops consuming third-party AI resources.

```bash
make undeploy
```

## Accessing Logs

Buttercup includes local SigNoz deployment by default for comprehensive system observability. You can access logs, traces, and metrics through the SigNoz UI:

```bash
make signoz-ui
```

Then navigate to `http://localhost:33301` in your web browser to view:
- Distributed traces
- Application metrics 
- Error monitoring
- Performance insights

If you configured LangFuse during setup, you can also monitor LLM usage and costs there.

For additional log access methods, see the [Quick Reference Guide](QUICK_REFERENCE.md).

## Additional Resources

- [Quick Reference Guide](QUICK_REFERENCE.md) - Common commands and troubleshooting
- [Manual Setup Guide](MANUAL_SETUP.md) - Detailed manual installation steps
- [AKS Deployment Guide](AKS_DEPLOYMENT.md) - Production deployment on Azure
- [Contributing Guidelines](CONTRIBUTING.md) - Development workflow and standards
- [Deployment Documentation](deployment/README.md) - Advanced deployment configuration
- [Writing Custom Challenges](CUSTOM_CHALLENGES.md) - Custom project configuration and setup
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[cheahjs/free-llm-api-resources]]></title>
            <link>https://github.com/cheahjs/free-llm-api-resources</link>
            <guid>https://github.com/cheahjs/free-llm-api-resources</guid>
            <pubDate>Wed, 13 Aug 2025 00:04:13 GMT</pubDate>
            <description><![CDATA[A list of free LLM inference resources accessible via API.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/cheahjs/free-llm-api-resources">cheahjs/free-llm-api-resources</a></h1>
            <p>A list of free LLM inference resources accessible via API.</p>
            <p>Language: Python</p>
            <p>Stars: 4,646</p>
            <p>Forks: 399</p>
            <p>Stars today: 32 stars today</p>
            <h2>README</h2><pre>&lt;!---
WARNING: DO NOT EDIT THIS FILE DIRECTLY. IT IS GENERATED BY src/pull_available_models.py
---&gt;
# Free LLM API resources

This lists various services that provide free access or credits towards API-based LLM usage.

&gt; [!NOTE]  
&gt; Please don&#039;t abuse these services, else we might lose them.

&gt; [!WARNING]  
&gt; This list explicitly excludes any services that are not legitimate (eg reverse engineers an existing chatbot)

- [Free Providers](#free-providers)
  - [OpenRouter](#openrouter)
  - [Google AI Studio](#google-ai-studio)
  - [NVIDIA NIM](#nvidia-nim)
  - [Mistral (La Plateforme)](#mistral-la-plateforme)
  - [Mistral (Codestral)](#mistral-codestral)
  - [HuggingFace Inference Providers](#huggingface-inference-providers)
  - [Vercel AI Gateway](#vercel-ai-gateway)
  - [Cerebras](#cerebras)
  - [Groq](#groq)
  - [Together (Free)](#together-free)
  - [Cohere](#cohere)
  - [GitHub Models](#github-models)
  - [Cloudflare Workers AI](#cloudflare-workers-ai)
  - [Google Cloud Vertex AI](#google-cloud-vertex-ai)
- [Providers with trial credits](#providers-with-trial-credits)
  - [Together](#together)
  - [Fireworks](#fireworks)
  - [Baseten](#baseten)
  - [Nebius](#nebius)
  - [Novita](#novita)
  - [AI21](#ai21)
  - [Upstage](#upstage)
  - [NLP Cloud](#nlp-cloud)
  - [Alibaba Cloud (International) Model Studio](#alibaba-cloud-international-model-studio)
  - [Modal](#modal)
  - [Inference.net](#inferencenet)
  - [nCompass](#ncompass)
  - [Hyperbolic](#hyperbolic)
  - [SambaNova Cloud](#sambanova-cloud)
  - [Scaleway Generative APIs](#scaleway-generative-apis)

## Free Providers

### [OpenRouter](https://openrouter.ai)

**Limits:**

[20 requests/minute&lt;br&gt;50 requests/day&lt;br&gt;1000 requests/day with $10 lifetime topup](https://openrouter.ai/docs/api-reference/limits)

Models share a common quota.

- [DeepCoder 14B Preview](https://openrouter.ai/agentica-org/deepcoder-14b-preview:free)
- [DeepHermes 3 Llama 3 8B Preview](https://openrouter.ai/nousresearch/deephermes-3-llama-3-8b-preview:free)
- [DeepSeek R1](https://openrouter.ai/deepseek/deepseek-r1:free)
- [DeepSeek R1 Distill Llama 70B](https://openrouter.ai/deepseek/deepseek-r1-distill-llama-70b:free)
- [DeepSeek R1 Distill Qwen 14B](https://openrouter.ai/deepseek/deepseek-r1-distill-qwen-14b:free)
- [DeepSeek V3 0324](https://openrouter.ai/deepseek/deepseek-chat-v3-0324:free)
- [Dolphin 3.0 Mistral 24B](https://openrouter.ai/cognitivecomputations/dolphin3.0-mistral-24b:free)
- [Dolphin 3.0 R1 Mistral 24B](https://openrouter.ai/cognitivecomputations/dolphin3.0-r1-mistral-24b:free)
- [Featherless Qwerky 72B](https://openrouter.ai/featherless/qwerky-72b:free)
- [Gemma 2 9B Instruct](https://openrouter.ai/google/gemma-2-9b-it:free)
- [Gemma 3 12B Instruct](https://openrouter.ai/google/gemma-3-12b-it:free)
- [Gemma 3 27B Instruct](https://openrouter.ai/google/gemma-3-27b-it:free)
- [Gemma 3 4B Instruct](https://openrouter.ai/google/gemma-3-4b-it:free)
- [Kimi VL A3B Thinking](https://openrouter.ai/moonshotai/kimi-vl-a3b-thinking:free)
- [Llama 3.1 405B Instruct](https://openrouter.ai/meta-llama/llama-3.1-405b-instruct:free)
- [Llama 3.1 Nemotron Ultra 253B v1](https://openrouter.ai/nvidia/llama-3.1-nemotron-ultra-253b-v1:free)
- [Llama 3.2 11B Vision Instruct](https://openrouter.ai/meta-llama/llama-3.2-11b-vision-instruct:free)
- [Llama 3.2 3B Instruct](https://openrouter.ai/meta-llama/llama-3.2-3b-instruct:free)
- [Llama 3.3 70B Instruct](https://openrouter.ai/meta-llama/llama-3.3-70b-instruct:free)
- [Mistral 7B Instruct](https://openrouter.ai/mistralai/mistral-7b-instruct:free)
- [Mistral Nemo](https://openrouter.ai/mistralai/mistral-nemo:free)
- [Mistral Small 24B Instruct 2501](https://openrouter.ai/mistralai/mistral-small-24b-instruct-2501:free)
- [Mistral Small 3.1 24B Instruct](https://openrouter.ai/mistralai/mistral-small-3.1-24b-instruct:free)
- [QwQ 32B ArliAI RpR v1](https://openrouter.ai/arliai/qwq-32b-arliai-rpr-v1:free)
- [Qwen 2.5 72B Instruct](https://openrouter.ai/qwen/qwen-2.5-72b-instruct:free)
- [Qwen 2.5 VL 32B Instruct](https://openrouter.ai/qwen/qwen2.5-vl-32b-instruct:free)
- [Qwen QwQ 32B](https://openrouter.ai/qwen/qwq-32b:free)
- [Qwen2.5 Coder 32B Instruct](https://openrouter.ai/qwen/qwen-2.5-coder-32b-instruct:free)
- [Qwen2.5 VL 72B Instruct](https://openrouter.ai/qwen/qwen2.5-vl-72b-instruct:free)
- [Reka Flash 3](https://openrouter.ai/rekaai/reka-flash-3:free)
- [Shisa V2 Llama 3.3 70B](https://openrouter.ai/shisa-ai/shisa-v2-llama3.3-70b:free)
- [cognitivecomputations/dolphin-mistral-24b-venice-edition:free](https://openrouter.ai/cognitivecomputations/dolphin-mistral-24b-venice-edition:free)
- [deepseek/deepseek-r1-0528-qwen3-8b:free](https://openrouter.ai/deepseek/deepseek-r1-0528-qwen3-8b:free)
- [deepseek/deepseek-r1-0528:free](https://openrouter.ai/deepseek/deepseek-r1-0528:free)
- [google/gemma-3n-e2b-it:free](https://openrouter.ai/google/gemma-3n-e2b-it:free)
- [google/gemma-3n-e4b-it:free](https://openrouter.ai/google/gemma-3n-e4b-it:free)
- [microsoft/mai-ds-r1:free](https://openrouter.ai/microsoft/mai-ds-r1:free)
- [mistralai/devstral-small-2505:free](https://openrouter.ai/mistralai/devstral-small-2505:free)
- [mistralai/mistral-small-3.2-24b-instruct:free](https://openrouter.ai/mistralai/mistral-small-3.2-24b-instruct:free)
- [moonshotai/kimi-dev-72b:free](https://openrouter.ai/moonshotai/kimi-dev-72b:free)
- [moonshotai/kimi-k2:free](https://openrouter.ai/moonshotai/kimi-k2:free)
- [openai/gpt-oss-20b:free](https://openrouter.ai/openai/gpt-oss-20b:free)
- [qwen/qwen3-14b:free](https://openrouter.ai/qwen/qwen3-14b:free)
- [qwen/qwen3-235b-a22b:free](https://openrouter.ai/qwen/qwen3-235b-a22b:free)
- [qwen/qwen3-30b-a3b:free](https://openrouter.ai/qwen/qwen3-30b-a3b:free)
- [qwen/qwen3-4b:free](https://openrouter.ai/qwen/qwen3-4b:free)
- [qwen/qwen3-8b:free](https://openrouter.ai/qwen/qwen3-8b:free)
- [qwen/qwen3-coder:free](https://openrouter.ai/qwen/qwen3-coder:free)
- [sarvamai/sarvam-m:free](https://openrouter.ai/sarvamai/sarvam-m:free)
- [tencent/hunyuan-a13b-instruct:free](https://openrouter.ai/tencent/hunyuan-a13b-instruct:free)
- [thudm/glm-z1-32b:free](https://openrouter.ai/thudm/glm-z1-32b:free)
- [tngtech/deepseek-r1t-chimera:free](https://openrouter.ai/tngtech/deepseek-r1t-chimera:free)
- [tngtech/deepseek-r1t2-chimera:free](https://openrouter.ai/tngtech/deepseek-r1t2-chimera:free)
- [z-ai/glm-4.5-air:free](https://openrouter.ai/z-ai/glm-4.5-air:free)

### [Google AI Studio](https://aistudio.google.com)

Data is used for training when used outside of the UK/CH/EEA/EU.

&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Model Name&lt;/th&gt;&lt;th&gt;Model Limits&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;Gemini 2.5 Pro&lt;/td&gt;&lt;td&gt;6,000,000 tokens/day&lt;br&gt;250,000 tokens/minute&lt;br&gt;100 requests/day&lt;br&gt;5 requests/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Gemini 2.5 Flash&lt;/td&gt;&lt;td&gt;250,000 tokens/minute&lt;br&gt;250 requests/day&lt;br&gt;10 requests/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Gemini 2.0 Flash&lt;/td&gt;&lt;td&gt;1,000,000 tokens/minute&lt;br&gt;200 requests/day&lt;br&gt;15 requests/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Gemini 2.0 Flash-Lite&lt;/td&gt;&lt;td&gt;1,000,000 tokens/minute&lt;br&gt;200 requests/day&lt;br&gt;30 requests/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Gemini 2.0 Flash (Experimental)&lt;/td&gt;&lt;td&gt;250,000 tokens/minute&lt;br&gt;50 requests/day&lt;br&gt;10 requests/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Gemini 1.5 Flash&lt;/td&gt;&lt;td&gt;250,000 tokens/minute&lt;br&gt;50 requests/day&lt;br&gt;15 requests/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Gemini 1.5 Flash-8B&lt;/td&gt;&lt;td&gt;250,000 tokens/minute&lt;br&gt;50 requests/day&lt;br&gt;15 requests/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;LearnLM 2.0 Flash (Experimental)&lt;/td&gt;&lt;td&gt;1,500 requests/day&lt;br&gt;15 requests/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Gemma 3 27B Instruct&lt;/td&gt;&lt;td&gt;15,000 tokens/minute&lt;br&gt;14,400 requests/day&lt;br&gt;30 requests/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Gemma 3 12B Instruct&lt;/td&gt;&lt;td&gt;15,000 tokens/minute&lt;br&gt;14,400 requests/day&lt;br&gt;30 requests/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Gemma 3 4B Instruct&lt;/td&gt;&lt;td&gt;15,000 tokens/minute&lt;br&gt;14,400 requests/day&lt;br&gt;30 requests/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Gemma 3 1B Instruct&lt;/td&gt;&lt;td&gt;15,000 tokens/minute&lt;br&gt;14,400 requests/day&lt;br&gt;30 requests/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;text-embedding-004&lt;/td&gt;&lt;td rowspan=&quot;2&quot;&gt;10,000 batch requests/minute&lt;br&gt;10,000 requests/minute&lt;br&gt;100 content/batch&lt;br&gt;Shared Quota&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;embedding-001&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

### [NVIDIA NIM](https://build.nvidia.com/explore/discover)

Phone number verification required.
Models tend to be context window limited.

**Limits:** 40 requests/minute

- [Various open models](https://build.nvidia.com/models)

### [Mistral (La Plateforme)](https://console.mistral.ai/)

* Free tier (Experiment plan) requires opting into data training
* Requires phone number verification.

**Limits (per-model):** 1 request/second, 500,000 tokens/minute, 1,000,000,000 tokens/month

- [Open and Proprietary Mistral models](https://docs.mistral.ai/getting-started/models/models_overview/)

### [Mistral (Codestral)](https://codestral.mistral.ai/)

* Currently free to use
* Monthly subscription based
* Requires phone number verification

**Limits:** 30 requests/minute, 2,000 requests/day

- Codestral

### [HuggingFace Inference Providers](https://huggingface.co/docs/inference-providers/en/index)

HuggingFace Serverless Inference limited to models smaller than 10GB. Some popular models are supported even if they exceed 10GB.

**Limits:** [$0.10/month in credits](https://huggingface.co/docs/inference-providers/en/pricing)

- Various open models across supported providers

### [Vercel AI Gateway](https://vercel.com/docs/ai-gateway)

Routes to various supported providers.

**Limits:** [$5/month](https://vercel.com/docs/ai-gateway/pricing)


### [Cerebras](https://cloud.cerebras.ai/)

&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Model Name&lt;/th&gt;&lt;th&gt;Model Limits&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;Qwen 3 235B A22B Instruct&lt;/td&gt;&lt;td&gt;30 requests/minute&lt;br&gt;60,000 tokens/minute&lt;br&gt;900 requests/hour&lt;br&gt;1,000,000 tokens/hour&lt;br&gt;14,400 requests/day&lt;br&gt;1,000,000 tokens/day&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Qwen 3 235B A22B Thinking&lt;/td&gt;&lt;td&gt;30 requests/minute&lt;br&gt;60,000 tokens/minute&lt;br&gt;900 requests/hour&lt;br&gt;1,000,000 tokens/hour&lt;br&gt;14,400 requests/day&lt;br&gt;1,000,000 tokens/day&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Qwen 3 Coder 480B&lt;/td&gt;&lt;td&gt;10 requests/minute&lt;br&gt;150,000 tokens/minute&lt;br&gt;100 requests/hour&lt;br&gt;1,000,000 tokens/hour&lt;br&gt;100 requests/day&lt;br&gt;1,000,000 tokens/day&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Llama 3.3 70B&lt;/td&gt;&lt;td&gt;30 requests/minute&lt;br&gt;64,000 tokens/minute&lt;br&gt;900 requests/hour&lt;br&gt;1,000,000 tokens/hour&lt;br&gt;14,400 requests/day&lt;br&gt;1,000,000 tokens/day&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Qwen 3 32B&lt;/td&gt;&lt;td&gt;30 requests/minute&lt;br&gt;64,000 tokens/minute&lt;br&gt;900 requests/hour&lt;br&gt;1,000,000 tokens/hour&lt;br&gt;14,400 requests/day&lt;br&gt;1,000,000 tokens/day&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Llama 3.1 8B&lt;/td&gt;&lt;td&gt;30 requests/minute&lt;br&gt;60,000 tokens/minute&lt;br&gt;900 requests/hour&lt;br&gt;1,000,000 tokens/hour&lt;br&gt;14,400 requests/day&lt;br&gt;1,000,000 tokens/day&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Llama 4 Scout&lt;/td&gt;&lt;td&gt;30 requests/minute&lt;br&gt;60,000 tokens/minute&lt;br&gt;900 requests/hour&lt;br&gt;1,000,000 tokens/hour&lt;br&gt;14,400 requests/day&lt;br&gt;1,000,000 tokens/day&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Llama 4 Maverick&lt;/td&gt;&lt;td&gt;30 requests/minute&lt;br&gt;60,000 tokens/minute&lt;br&gt;900 requests/hour&lt;br&gt;1,000,000 tokens/hour&lt;br&gt;14,400 requests/day&lt;br&gt;1,000,000 tokens/day&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

### [Groq](https://console.groq.com)

&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Model Name&lt;/th&gt;&lt;th&gt;Model Limits&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;Allam 2 7B&lt;/td&gt;&lt;td&gt;7,000 requests/day&lt;br&gt;6,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;DeepSeek R1 Distill Llama 70B&lt;/td&gt;&lt;td&gt;1,000 requests/day&lt;br&gt;6,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Distil Whisper Large v3&lt;/td&gt;&lt;td&gt;7,200 audio-seconds/minute&lt;br&gt;2,000 requests/day&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Gemma 2 9B Instruct&lt;/td&gt;&lt;td&gt;14,400 requests/day&lt;br&gt;15,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Groq compound-beta&lt;/td&gt;&lt;td&gt;200 requests/day&lt;br&gt;70,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Groq compound-beta-mini&lt;/td&gt;&lt;td&gt;200 requests/day&lt;br&gt;70,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Llama 3 70B&lt;/td&gt;&lt;td&gt;14,400 requests/day&lt;br&gt;6,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Llama 3 8B&lt;/td&gt;&lt;td&gt;14,400 requests/day&lt;br&gt;6,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Llama 3.1 8B&lt;/td&gt;&lt;td&gt;14,400 requests/day&lt;br&gt;6,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Llama 3.3 70B&lt;/td&gt;&lt;td&gt;1,000 requests/day&lt;br&gt;12,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Llama 4 Maverick 17B 128E Instruct&lt;/td&gt;&lt;td&gt;1,000 requests/day&lt;br&gt;6,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Llama 4 Scout Instruct&lt;/td&gt;&lt;td&gt;1,000 requests/day&lt;br&gt;30,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Whisper Large v3&lt;/td&gt;&lt;td&gt;7,200 audio-seconds/minute&lt;br&gt;2,000 requests/day&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Whisper Large v3 Turbo&lt;/td&gt;&lt;td&gt;7,200 audio-seconds/minute&lt;br&gt;2,000 requests/day&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;meta-llama/llama-guard-4-12b&lt;/td&gt;&lt;td&gt;14,400 requests/day&lt;br&gt;15,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;meta-llama/llama-prompt-guard-2-22m&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;meta-llama/llama-prompt-guard-2-86m&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;moonshotai/kimi-k2-instruct&lt;/td&gt;&lt;td&gt;1,000 requests/day&lt;br&gt;10,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;openai/gpt-oss-120b&lt;/td&gt;&lt;td&gt;1,000 requests/day&lt;br&gt;8,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;openai/gpt-oss-20b&lt;/td&gt;&lt;td&gt;1,000 requests/day&lt;br&gt;8,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;qwen/qwen3-32b&lt;/td&gt;&lt;td&gt;1,000 requests/day&lt;br&gt;6,000 tokens/minute&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;

### [Together (Free)](https://together.ai)

**Limits:** Up to 60 requests/minute

- [Llama 3.2 11B Vision Instruct](https://together.ai/models/llama-3-2-11b-free)
- [Llama 3.3 70B Instruct](https://together.ai/models/llama-3-3-70b-free)
- [DeepSeek R1 Distil Llama 70B](https://together.ai/models/deepseek-r1-distilled-llama-70b-free)

### [Cohere](https://cohere.com)

**Limits:**

[20 requests/minute&lt;br&gt;1,000 requests/month](https://docs.cohere.com/docs/rate-limits)

Models share a common quota.

- Command-A
- Command-R7B
- Command-R+
- Command-R
- Aya Expanse 8B
- Aya Expanse 32B
- Aya Vision 8B
- Aya Vision 32B

### [GitHub Models](https://github.com/marketplace/models)

Extremely restrictive input/output token limits.

**Limits:** [Dependent on Copilot subscription tier (Free/Pro/Pro+/Business/Enterprise)](https://docs.github.com/en/github-models/prototyping-with-ai-models#rate-limits)

- AI21 Jamba 1.5 Large
- AI21 Jamba 1.5 Mini
- Codestral 25.01
- Cohere Command A
- Cohere Command R 08-2024
- Cohere Command R+ 08-2024
- Cohere Embed v3 English
- Cohere Embed v3 Multilingual
- DeepSeek-R1
- DeepSeek-R1-0528
- DeepSeek-V3-0324
- Grok 3
- Grok 3 Mini
- JAIS 30b Chat
- Llama 4 Maverick 17B 128E Instruct FP8
- Llama 4 Scout 17B 16E Instruct
- Llama-3.2-11B-Vision-Instruct
- Llama-3.2-90B-Vision-Instruct
- Llama-3.3-70B-Instruct
- MAI-DS-R1
- Meta-Llama-3.1-405B-Instruct
- Meta-Llama-3.1-8B-Instruct
- Ministral 3B
- Mistral Large 24.11
- Mistral Medium 3 (25.05)
- Mistral Nemo
- Mistral Small 3.1
- OpenAI GPT-4.1
- OpenAI GPT-4.1-mini
- OpenAI GPT-4.1-nano
- OpenAI GPT-4o
- OpenAI GPT-4o mini
- OpenAI Text Embedding 3 (large)
- OpenAI Text Embedding 3 (small)
- OpenAI gpt-5
- OpenAI gpt-5-chat (preview)
- OpenAI gpt-5-mini
- OpenAI gpt-5-nano
- OpenAI o1
- OpenAI o1-mini
- OpenAI o1-preview
- OpenAI o3
- OpenAI o3-mini
- OpenAI o4-mini
- Phi-3-medium instruct (128k)
- Phi-3-medium instruct (4k)
- Phi-3-mini instruct (128k)
- Phi-3-mini instruct (4k)
- Phi-3-small instruct (128k)
- Phi-3-small instruct (8k)
- Phi-3.5-MoE instruct (128k)
- Phi-3.5-mini instruct (128k)
- Phi-3.5-vision instruct (128k)
- Phi-4
- Phi-4-Reasoning
- Phi-4-mini-instruct
- Phi-4-mini-reasoning
- Phi-4-multimodal-instruct

### [Cloudflare Workers AI](https://developers.cloudflare.com/workers-ai)

**Limits:** [10,000 neurons/day](https://developers.cloudflare.com/workers-ai/platform/pricing/#free-allocation)

- @cf/openai/gpt-oss-120b
- @cf/openai/gpt-oss-20b
- DeepSeek R1 Distill Qwen 32B
- Deepseek Coder 6.7B Base (AWQ)
- Deepseek Coder 6.7B Instruct (AWQ)
- Deepseek Math 7B Instruct
- Discolm German 7B v1 (AWQ)
- Falcom 7B Instruct
- Gemma 2B Instruct (LoRA)
- Gemma 3 12B Instruct
- Gemma 7B Instruct
- Gemma 7B Instruct (LoRA)
- Hermes 2 Pro Mistral 7B
- Llama 2 13B Chat (AWQ)
- Llama 2 7B Chat (FP16)
- Llama 2 7B Chat (INT8)
- Llama 2 7B Chat (LoRA)
- Llama 3 8B Instruct
- Llama 3 8B Instruct
- Llama 3 8B Instruct (AWQ)
- Llama 3.1 8B Instruct (AWQ)
- Llama 3.1 8B Instruct (FP8)
- Llama 3.2 11B Vision Instruct
- Llama 3.2 1B Instruct
- Llama 3.2 3B Instruct
- Llama 3.3 70B Instruct (FP8)
- Llama 4 Scout Instruct
- Llama Guard 3 8B
- LlamaGuard 7B (AWQ)
- Mistral 7B Instruct v0.1
- Mistral 7B Instruct v0.1 (AWQ)
- Mistral 7B Instruct v0.2
- Mistral 7B Instruct v0.2 (LoRA)
- Mistral Small 3.1 24B Instruct
- Neural Chat 7B v3.1 (AWQ)
- OpenChat 3.5 0106
- OpenHermes 2.5 Mistral 7B (AWQ)
- Phi-2
- Qwen 1.5 0.5B Chat
- Qwen 1.5 1.8B Chat
- Qwen 1.5 14B Chat (AWQ)
- Qwen 1.5 7B Chat (AWQ)
- Qwen 2.5 Coder 32B Instruct
- Qwen QwQ 32B
- SQLCoder 7B 2
- Starling LM 7B Beta
- TinyLlama 1.1B Chat v1.0
- Una Cybertron 7B v2 (BF16)
- Zephyr 7B Beta (AWQ)

### [Google Cloud Vertex AI](https://console.cloud.google.com/vertex-ai/model-garden)

Very stringent payment verification for Google Cloud.

&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Model Name&lt;/th&gt;&lt;th&gt;Model Limits&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href=&quot;https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-3-2-90b-vision-instruct-maas&quot; target=&quot;_blank&quot;&gt;Llama 3.2 90B Vision Instruct&lt;/a&gt;&lt;/td&gt;&lt;td&gt;30 requests/minute&lt;br&gt;Free during preview&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href=&quot;https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-3-1-405b-instruct-maas&quot; target=&quot;_blank&quot;&gt;Llama 3.1 70B Instruct&lt;/a&gt;&lt;/td&gt;&lt;td&gt;60 requests/minute&lt;br&gt;Free during preview&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;a href=&quot;https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama-3-1-405b-instruct-maas&quot; target=&quot;_blank&quot;&gt;Llama 3.1 8B Instruct&lt;/a&gt;&lt;/td&gt;&lt;td&gt;60 requests/minute&lt;br&gt;Free during preview&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;



## Providers with trial credits

### [Together](https://together.ai)

**Credits:** $1 when you add a payment method

**Models:** [Various open models](https://together.ai/models)

### [Fireworks](https://fireworks.ai/)

**Credits:** $1

**Models:** [Various open models](https://fireworks.ai/models)

### [Baseten](https://app.baseten.co/)

**Credits:** $30

**Models:** [Any supported model - pay by compute time](https://www.baseten.co/library/)

### [Nebius](https://studio.nebius.com/)

**Credits:** $1

**Models:** [Various open models](https://studio.nebius.ai/models)

### [Novita](https://novita.ai/?ref=ytblmjc&amp;utm_source=affiliate)

**Credits:** $0.5 for 1 year

**Models:** [Various open models](https://novita.ai/models)

### [AI21](https://studio.ai21.com/)

**Credits:** $10 for 3 months

**Models:** Jamba family of models

### [Upstage](https://console.upstage.ai/)

**Credits:** $10 for 3 months

**Models:** Solar Pro/Mini

### [NLP Cloud](https://nlpcloud.com/home)

**Credits:** $15

**Requirements:** Phone number verification

**Models:** Various open models

### [Alibaba Cloud (International) Model Studio](https://bailian.console.alibabacloud.com/)

**Credits:** 1 million tokens/model

**Models:** [Various open and proprietary Qwen models](https://www.alibabacloud.com/en/product/modelstudio)

### [Modal](https://modal.com)

**Credits:** $5/month upon sign up, $30/month with payment method added

**Models:** Any supported model - pay by compute time

### [Inference.net](https://inference.net)

**Credits:** $1, $25 on responding to email survey

**Models:** Various open models

### [nCompass](https://ncompass.tech)

**Credits:** $1

**Models:** Various open models

### [Hyperbolic](https://app.hyperbolic.xyz/)

**Credits:** $1

**Models:**
- DeepSeek V3
- DeepSeek V3 0324
- Hermes 3 Llama 3.1 70B
- Llama 3 70B Instruct
- Llama 3.1 405B Base
- Llama 3.1 405B Base (FP8)
- Llama 3.1 405B Instruct
- Llama 3.1 70B Instruct
- Llama 3.1 8B Instruct
- Llama 3.2 3B Instruct
- Llama 3.3 70B Instruct
- Pixtral 12B (2409)
- Qwen QwQ 32B
- Qwen QwQ 32B Preview
- Qwen2.5 72B Instruct
- Qwen2.5 Coder 32B Instruct
- Qwen2.5 VL 72B Instruct
- Qwen2.5 VL 7B Instruct

### [SambaNova Cloud](https://cloud.sambanova.ai/)

**Credits:** $5 for 3 months

**Models:**
- E

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[TCM-Course-Resources/Practical-Ethical-Hacking-Resources]]></title>
            <link>https://github.com/TCM-Course-Resources/Practical-Ethical-Hacking-Resources</link>
            <guid>https://github.com/TCM-Course-Resources/Practical-Ethical-Hacking-Resources</guid>
            <pubDate>Wed, 13 Aug 2025 00:04:12 GMT</pubDate>
            <description><![CDATA[Compilation of Resources from TCM's Practical Ethical Hacking Udemy Course]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/TCM-Course-Resources/Practical-Ethical-Hacking-Resources">TCM-Course-Resources/Practical-Ethical-Hacking-Resources</a></h1>
            <p>Compilation of Resources from TCM's Practical Ethical Hacking Udemy Course</p>
            <p>Language: Python</p>
            <p>Stars: 5,832</p>
            <p>Forks: 1,436</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre># Practical-Ethical-Hacking-Resources

Compilation of Resources from TCM&#039;s Udemy Course 

### General Links 

Link to Website: https://www.thecybermentor.com/

Link to the course: 
* https://academy.tcm-sec.com/p/practical-ethical-hacking-the-complete-course (tcm academy)

Link to discord server: https://discord.gg/EM6tqPZ

FAQ: https://github.com/hmaverickadams/Practical-Ethical-Hacking-FAQ

### Note Keeping

Trilium: https://github.com/zadam/trilium

KeepNote: http://keepnote.org/

CherryTree: https://www.giuspen.com/cherrytree/

GreenShot: https://getgreenshot.org/downloads/

FlameShot: https://github.com/lupoDharkael/flameshot

OneNote: https://products.office.com/en-us/onenote/digital-note-taking-app?rtc=1

Joplin: https://github.com/laurent22/joplin

### Networking Refresher
Seven Second Subnetting: https://www.youtube.com/watch?v=ZxAwQB8TZsM

Subnet Guide: https://drive.google.com/file/d/1ETKH31-E7G-7ntEOlWGZcDZWuukmeHFe/view

### Setting up our Lab
VMware: https://www.vmware.com/products/workstation-player/workstation-player-evaluation.html

VirtualBox: https://www.virtualbox.org/wiki/Downloads

Kali Download: https://www.offensive-security.com/kali-linux-vm-vmware-virtualbox-image-download/

Official Offensive Security kali 2019.3 release: http://old.kali.org/kali-images/kali-2019.3/

Other Offical kali 2019 Releases: https://cdimage.kali.org/

### Mid-Course Capstone
New Capstone boxes: https://drive.google.com/drive/folders/1VXEuyySgzsSo-MYmyCareTnJ5rAeVKeH

Old Capstone boxes: https://youtu.be/JZN3JhoAdWo

Linux Priv Esc course: https://academy.tcm-sec.com/p/windows-privilege-escalation-for-beginners

Windows Priv Esc Course: https://academy.tcm-sec.com/p/linux-privilege-escalation

### Introduction to Exploit Development (Buffer Overflows)
Immunity Debugger: https://www.immunityinc.com/products/debugger/

Vulnserver: http://www.thegreycorner.com/p/vulnserver.html

Bad Chars: https://www.ins1gn1a.com/identifying-bad-characters/

### Attacking Active Directory: Initial Attack Vectors

Top Five Ways I Got DA on Your Internal Network Before Lunch: https://adam-toscher.medium.com/top-five-ways-i-got-domain-admin-on-your-internal-network-before-lunch-2018-edition-82259ab73aaa 

mitm6: https://blog.fox-it.com/2018/01/11/mitm6-compromising-ipv4-networks-via-ipv6/

Combining NTLM Relays and Kerberos Delegation: https://dirkjanm.io/worst-of-both-worlds-ntlm-relaying-and-kerberos-delegation/

### Attacking Active Directory: Post-Compromise Enumeration
PowerView Cheat Sheet: https://gist.github.com/HarmJ0y/184f9822b195c52dd50c379ed3117993

### Attacking Active Directory: Post-Compromise Attacks
Group Policy Pwnage: https://blog.rapid7.com/2016/07/27/pentesting-in-the-real-world-group-policy-pwnage/

Mimikatz: https://github.com/gentilkiwi/mimikatz

Active Directory Security Blog: https://adsecurity.org/

Harmj0y Blog: http://blog.harmj0y.net/

Pentester Academy Active Directory: https://www.pentesteracademy.com/activedirectorylab

Pentester Academy Red Team Labs: https://www.pentesteracademy.com/redteamlab

eLS PTX: https://www.elearnsecurity.com/course/penetration_testing_extreme/

### Web Application Enumeration, Revisited
sumrecon: https://github.com/thatonetester/sumrecon

### Testing the Top 10 Web Application Vulnerabilities
 OWASP Top 10: https://owasp.org/www-pdf-archive/OWASP_Top_10-2017_%28en%29.pdf.pdf
 
 OWASP Testing Checklist: https://github.com/tanprathan/OWASP-Testing-Checklist
 
 OWASP Testing Guide: https://owasp.org/www-pdf-archive/OTGv4.pdf
 
 Installing Docker on Kali: https://medium.com/@airman604/installing-docker-in-kali-linux-2017-1-fbaa4d1447fe
 
 OWASP Juice Shop: https://github.com/bkimminich/juice-shop
 
 OWASP A1-Injection: https://www.owasp.org/index.php/Top_10-2017_A1-Injection
 
 OWASP A2-Broken Authentication: https://www.owasp.org/index.php/Top_10-2017_A2-Broken_Authentication
 
 OWASP A3-Sensetive Data Exposure: https://www.owasp.org/index.php/Top_10-2017_A3-Sensitive_Data_Exposure
 
 OWASP A4-XML External Entities: https://www.owasp.org/index.php/Top_10-2017_A4-XML_External_Entities_(XXE)
 
 OWASP A5-Broken Access Control: https://www.owasp.org/index.php/Top_10-2017_A5-Broken_Access_Control
 
 OWASP A6-Security Misconfigurations: https://www.owasp.org/index.php/Top_10-2017_A6-Security_Misconfiguration
 
 OWASP A7-Cross Site Scripting: https://www.owasp.org/index.php/Top_10-2017_A7-Cross-Site_Scripting_(XSS)
 
 DOM Based XSS: https://www.scip.ch/en/?labs.20171214
 
 XSS Game: https://xss-game.appspot.com/
 
 OWASP A8-Insecure Deserialization: https://www.owasp.org/index.php/Top_10-2017_A8-Insecure_Deserialization
 
 OWASP A9-Using Components with Known Vulnerabilities: https://www.owasp.org/index.php/Top_10-2017_A9-Using_Components_with_Known_Vulnerabilities
 
 OWASP A10-Insufficient Logging &amp; Monitoring: https://owasp.org/www-project-top-ten/OWASP_Top_Ten_2017/Top_10-2017_A10-Insufficient_Logging%252526Monitoring.html
 
 ### Legal Documents and Report Writing
 Sample Pentest Report: https://github.com/hmaverickadams/TCM-Security-Sample-Pentest-Report
  
 ## Tools
 #### Pimpmykali
 * Github: https://github.com/Dewalt-arch/pimpmykali
 
 #### Hunter.io
 * Site: https://hunter.io/
 
 #### theHarvester
 * Github: https://github.com/laramies/theHarvester
 
 #### breach-parse
 * Github: https://github.com/hmaverickadams/breach-parse
 
 #### Hashcat: 
 * Github: https://github.com/hashcat/hashcat
 * Installing on Windows: https://www.erobber.in/2017/04/hashcat-for-windows.html
 
 #### mitm6: 
 * Github: https://github.com/fox-it/mitm6
 
 #### mimikatz:
 * Github: https://github.com/gentilkiwi/mimikatz
 
 #### sumrecon 
 * Github: https://github.com/thatonetester/sumrecon
 
 ### Setting up Your AD Lab Using Azure
 Building Free AD lab: https://medium.com/@kamran.bilgrami/ethical-hacking-lessons-building-free-active-directory-lab-in-azure-6c67a7eddd7f
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[BerriAI/litellm]]></title>
            <link>https://github.com/BerriAI/litellm</link>
            <guid>https://github.com/BerriAI/litellm</guid>
            <pubDate>Wed, 13 Aug 2025 00:04:11 GMT</pubDate>
            <description><![CDATA[Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq]]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/BerriAI/litellm">BerriAI/litellm</a></h1>
            <p>Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq]</p>
            <p>Language: Python</p>
            <p>Stars: 27,271</p>
            <p>Forks: 3,784</p>
            <p>Stars today: 54 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
        üöÖ LiteLLM
    &lt;/h1&gt;
    &lt;p align=&quot;center&quot;&gt;
        &lt;p align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://render.com/deploy?repo=https://github.com/BerriAI/litellm&quot; target=&quot;_blank&quot; rel=&quot;nofollow&quot;&gt;&lt;img src=&quot;https://render.com/images/deploy-to-render-button.svg&quot; alt=&quot;Deploy to Render&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://railway.app/template/HLP0Ub?referralCode=jch2ME&quot;&gt;
          &lt;img src=&quot;https://railway.app/button.svg&quot; alt=&quot;Deploy on Railway&quot;&gt;
        &lt;/a&gt;
        &lt;/p&gt;
        &lt;p align=&quot;center&quot;&gt;Call all LLM APIs using the OpenAI format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.]
        &lt;br&gt;
    &lt;/p&gt;
&lt;h4 align=&quot;center&quot;&gt;&lt;a href=&quot;https://docs.litellm.ai/docs/simple_proxy&quot; target=&quot;_blank&quot;&gt;LiteLLM Proxy Server (LLM Gateway)&lt;/a&gt; | &lt;a href=&quot;https://docs.litellm.ai/docs/hosted&quot; target=&quot;_blank&quot;&gt; Hosted Proxy (Preview)&lt;/a&gt; | &lt;a href=&quot;https://docs.litellm.ai/docs/enterprise&quot;target=&quot;_blank&quot;&gt;Enterprise Tier&lt;/a&gt;&lt;/h4&gt;
&lt;h4 align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://pypi.org/project/litellm/&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/pypi/v/litellm.svg&quot; alt=&quot;PyPI Version&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://www.ycombinator.com/companies/berriai&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/Y%20Combinator-W23-orange?style=flat-square&quot; alt=&quot;Y Combinator W23&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://wa.link/huol9n&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/static/v1?label=Chat%20on&amp;message=WhatsApp&amp;color=success&amp;logo=WhatsApp&amp;style=flat-square&quot; alt=&quot;Whatsapp&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://discord.gg/wuPM9dRgDw&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/static/v1?label=Chat%20on&amp;message=Discord&amp;color=blue&amp;logo=Discord&amp;style=flat-square&quot; alt=&quot;Discord&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://join.slack.com/share/enQtOTE0ODczMzk2Nzk4NC01YjUxNjY2YjBlYTFmNDRiZTM3NDFiYTM3MzVkODFiMDVjOGRjMmNmZTZkZTMzOWQzZGQyZWIwYjQ0MWExYmE3&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/static/v1?label=Chat%20on&amp;message=Slack&amp;color=black&amp;logo=Slack&amp;style=flat-square&quot; alt=&quot;Slack&quot;&gt;
    &lt;/a&gt;
&lt;/h4&gt;

LiteLLM manages:

- Translate inputs to provider&#039;s `completion`, `embedding`, and `image_generation` endpoints
- [Consistent output](https://docs.litellm.ai/docs/completion/output), text responses will always be available at `[&#039;choices&#039;][0][&#039;message&#039;][&#039;content&#039;]`
- Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - [Router](https://docs.litellm.ai/docs/routing)
- Set Budgets &amp; Rate limits per project, api key, model [LiteLLM Proxy Server (LLM Gateway)](https://docs.litellm.ai/docs/simple_proxy)

[**Jump to LiteLLM Proxy (LLM Gateway) Docs**](https://github.com/BerriAI/litellm?tab=readme-ov-file#openai-proxy---docs) &lt;br&gt;
[**Jump to Supported LLM Providers**](https://github.com/BerriAI/litellm?tab=readme-ov-file#supported-providers-docs)

üö® **Stable Release:** Use docker images with the `-stable` tag. These have undergone 12 hour load tests, before being published. [More information about the release cycle here](https://docs.litellm.ai/docs/proxy/release_cycle)

Support for more providers. Missing a provider or LLM Platform, raise a [feature request](https://github.com/BerriAI/litellm/issues/new?assignees=&amp;labels=enhancement&amp;projects=&amp;template=feature_request.yml&amp;title=%5BFeature%5D%3A+).

# Usage ([**Docs**](https://docs.litellm.ai/docs/))

&gt; [!IMPORTANT]
&gt; LiteLLM v1.0.0 now requires `openai&gt;=1.0.0`. Migration guide [here](https://docs.litellm.ai/docs/migration)
&gt; LiteLLM v1.40.14+ now requires `pydantic&gt;=2.0.0`. No changes required.

&lt;a target=&quot;_blank&quot; href=&quot;https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/liteLLM_Getting_Started.ipynb&quot;&gt;
  &lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot; alt=&quot;Open In Colab&quot;/&gt;
&lt;/a&gt;

```shell
pip install litellm
```

```python
from litellm import completion
import os

## set ENV variables
os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;your-openai-key&quot;
os.environ[&quot;ANTHROPIC_API_KEY&quot;] = &quot;your-anthropic-key&quot;

messages = [{ &quot;content&quot;: &quot;Hello, how are you?&quot;,&quot;role&quot;: &quot;user&quot;}]

# openai call
response = completion(model=&quot;openai/gpt-4o&quot;, messages=messages)

# anthropic call
response = completion(model=&quot;anthropic/claude-sonnet-4-20250514&quot;, messages=messages)
print(response)
```

### Response (OpenAI Format)

```json
{
    &quot;id&quot;: &quot;chatcmpl-1214900a-6cdd-4148-b663-b5e2f642b4de&quot;,
    &quot;created&quot;: 1751494488,
    &quot;model&quot;: &quot;claude-sonnet-4-20250514&quot;,
    &quot;object&quot;: &quot;chat.completion&quot;,
    &quot;system_fingerprint&quot;: null,
    &quot;choices&quot;: [
        {
            &quot;finish_reason&quot;: &quot;stop&quot;,
            &quot;index&quot;: 0,
            &quot;message&quot;: {
                &quot;content&quot;: &quot;Hello! I&#039;m doing well, thank you for asking. I&#039;m here and ready to help with whatever you&#039;d like to discuss or work on. How are you doing today?&quot;,
                &quot;role&quot;: &quot;assistant&quot;,
                &quot;tool_calls&quot;: null,
                &quot;function_call&quot;: null
            }
        }
    ],
    &quot;usage&quot;: {
        &quot;completion_tokens&quot;: 39,
        &quot;prompt_tokens&quot;: 13,
        &quot;total_tokens&quot;: 52,
        &quot;completion_tokens_details&quot;: null,
        &quot;prompt_tokens_details&quot;: {
            &quot;audio_tokens&quot;: null,
            &quot;cached_tokens&quot;: 0
        },
        &quot;cache_creation_input_tokens&quot;: 0,
        &quot;cache_read_input_tokens&quot;: 0
    }
}
```

Call any model supported by a provider, with `model=&lt;provider_name&gt;/&lt;model_name&gt;`. There might be provider-specific details here, so refer to [provider docs for more information](https://docs.litellm.ai/docs/providers)

## Async ([Docs](https://docs.litellm.ai/docs/completion/stream#async-completion))

```python
from litellm import acompletion
import asyncio

async def test_get_response():
    user_message = &quot;Hello, how are you?&quot;
    messages = [{&quot;content&quot;: user_message, &quot;role&quot;: &quot;user&quot;}]
    response = await acompletion(model=&quot;openai/gpt-4o&quot;, messages=messages)
    return response

response = asyncio.run(test_get_response())
print(response)
```

## Streaming ([Docs](https://docs.litellm.ai/docs/completion/stream))

liteLLM supports streaming the model response back, pass `stream=True` to get a streaming iterator in response.
Streaming is supported for all models (Bedrock, Huggingface, TogetherAI, Azure, OpenAI, etc.)

```python
from litellm import completion
response = completion(model=&quot;openai/gpt-4o&quot;, messages=messages, stream=True)
for part in response:
    print(part.choices[0].delta.content or &quot;&quot;)

# claude sonnet 4
response = completion(&#039;anthropic/claude-sonnet-4-20250514&#039;, messages, stream=True)
for part in response:
    print(part)
```

### Response chunk (OpenAI Format)

```json
{
    &quot;id&quot;: &quot;chatcmpl-fe575c37-5004-4926-ae5e-bfbc31f356ca&quot;,
    &quot;created&quot;: 1751494808,
    &quot;model&quot;: &quot;claude-sonnet-4-20250514&quot;,
    &quot;object&quot;: &quot;chat.completion.chunk&quot;,
    &quot;system_fingerprint&quot;: null,
    &quot;choices&quot;: [
        {
            &quot;finish_reason&quot;: null,
            &quot;index&quot;: 0,
            &quot;delta&quot;: {
                &quot;provider_specific_fields&quot;: null,
                &quot;content&quot;: &quot;Hello&quot;,
                &quot;role&quot;: &quot;assistant&quot;,
                &quot;function_call&quot;: null,
                &quot;tool_calls&quot;: null,
                &quot;audio&quot;: null
            },
            &quot;logprobs&quot;: null
        }
    ],
    &quot;provider_specific_fields&quot;: null,
    &quot;stream_options&quot;: null,
    &quot;citations&quot;: null
}
```

## Logging Observability ([Docs](https://docs.litellm.ai/docs/observability/callbacks))

LiteLLM exposes pre defined callbacks to send data to Lunary, MLflow, Langfuse, DynamoDB, s3 Buckets, Helicone, Promptlayer, Traceloop, Athina, Slack

```python
from litellm import completion

## set env variables for logging tools (when using MLflow, no API key set up is required)
os.environ[&quot;LUNARY_PUBLIC_KEY&quot;] = &quot;your-lunary-public-key&quot;
os.environ[&quot;HELICONE_API_KEY&quot;] = &quot;your-helicone-auth-key&quot;
os.environ[&quot;LANGFUSE_PUBLIC_KEY&quot;] = &quot;&quot;
os.environ[&quot;LANGFUSE_SECRET_KEY&quot;] = &quot;&quot;
os.environ[&quot;ATHINA_API_KEY&quot;] = &quot;your-athina-api-key&quot;

os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;your-openai-key&quot;

# set callbacks
litellm.success_callback = [&quot;lunary&quot;, &quot;mlflow&quot;, &quot;langfuse&quot;, &quot;athina&quot;, &quot;helicone&quot;] # log input/output to lunary, langfuse, supabase, athina, helicone etc

#openai call
response = completion(model=&quot;openai/gpt-4o&quot;, messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hi üëã - i&#039;m openai&quot;}])
```

# LiteLLM Proxy Server (LLM Gateway) - ([Docs](https://docs.litellm.ai/docs/simple_proxy))

Track spend + Load Balance across multiple projects

[Hosted Proxy (Preview)](https://docs.litellm.ai/docs/hosted)

The proxy provides:

1. [Hooks for auth](https://docs.litellm.ai/docs/proxy/virtual_keys#custom-auth)
2. [Hooks for logging](https://docs.litellm.ai/docs/proxy/logging#step-1---create-your-custom-litellm-callback-class)
3. [Cost tracking](https://docs.litellm.ai/docs/proxy/virtual_keys#tracking-spend)
4. [Rate Limiting](https://docs.litellm.ai/docs/proxy/users#set-rate-limits)

## üìñ Proxy Endpoints - [Swagger Docs](https://litellm-api.up.railway.app/)


## Quick Start Proxy - CLI

```shell
pip install &#039;litellm[proxy]&#039;
```

### Step 1: Start litellm proxy

```shell
$ litellm --model huggingface/bigcode/starcoder

#INFO: Proxy running on http://0.0.0.0:4000
```

### Step 2: Make ChatCompletions Request to Proxy


&gt; [!IMPORTANT]
&gt; üí° [Use LiteLLM Proxy with Langchain (Python, JS), OpenAI SDK (Python, JS) Anthropic SDK, Mistral SDK, LlamaIndex, Instructor, Curl](https://docs.litellm.ai/docs/proxy/user_keys)

```python
import openai # openai v1.0.0+
client = openai.OpenAI(api_key=&quot;anything&quot;,base_url=&quot;http://0.0.0.0:4000&quot;) # set proxy to base_url
# request sent to model set on litellm proxy, `litellm --model`
response = client.chat.completions.create(model=&quot;gpt-3.5-turbo&quot;, messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;this is a test request, write a short poem&quot;
    }
])

print(response)
```

## Proxy Key Management ([Docs](https://docs.litellm.ai/docs/proxy/virtual_keys))

Connect the proxy with a Postgres DB to create proxy keys

```bash
# Get the code
git clone https://github.com/BerriAI/litellm

# Go to folder
cd litellm

# Add the master key - you can change this after setup
echo &#039;LITELLM_MASTER_KEY=&quot;sk-1234&quot;&#039; &gt; .env

# Add the litellm salt key - you cannot change this after adding a model
# It is used to encrypt / decrypt your LLM API Key credentials
# We recommend - https://1password.com/password-generator/
# password generator to get a random hash for litellm salt key
echo &#039;LITELLM_SALT_KEY=&quot;sk-1234&quot;&#039; &gt;&gt; .env

source .env

# Start
docker-compose up
```


UI on `/ui` on your proxy server
![ui_3](https://github.com/BerriAI/litellm/assets/29436595/47c97d5e-b9be-4839-b28c-43d7f4f10033)

Set budgets and rate limits across multiple projects
`POST /key/generate`

### Request

```shell
curl &#039;http://0.0.0.0:4000/key/generate&#039; \
--header &#039;Authorization: Bearer sk-1234&#039; \
--header &#039;Content-Type: application/json&#039; \
--data-raw &#039;{&quot;models&quot;: [&quot;gpt-3.5-turbo&quot;, &quot;gpt-4&quot;, &quot;claude-2&quot;], &quot;duration&quot;: &quot;20m&quot;,&quot;metadata&quot;: {&quot;user&quot;: &quot;ishaan@berri.ai&quot;, &quot;team&quot;: &quot;core-infra&quot;}}&#039;
```

### Expected Response

```shell
{
    &quot;key&quot;: &quot;sk-kdEXbIqZRwEeEiHwdg7sFA&quot;, # Bearer token
    &quot;expires&quot;: &quot;2023-11-19T01:38:25.838000+00:00&quot; # datetime object
}
```

## Supported Providers ([Docs](https://docs.litellm.ai/docs/providers))

| Provider                                                                            | [Completion](https://docs.litellm.ai/docs/#basic-usage) | [Streaming](https://docs.litellm.ai/docs/completion/stream#streaming-responses) | [Async Completion](https://docs.litellm.ai/docs/completion/stream#async-completion) | [Async Streaming](https://docs.litellm.ai/docs/completion/stream#async-streaming) | [Async Embedding](https://docs.litellm.ai/docs/embedding/supported_embedding) | [Async Image Generation](https://docs.litellm.ai/docs/image_generation) |
|-------------------------------------------------------------------------------------|---------------------------------------------------------|---------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|-------------------------------------------------------------------------------|-------------------------------------------------------------------------|
| [openai](https://docs.litellm.ai/docs/providers/openai)                             | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             | ‚úÖ                                                                       |
| [Meta - Llama API](https://docs.litellm.ai/docs/providers/meta_llama)                               | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                              |                                                                        |
| [azure](https://docs.litellm.ai/docs/providers/azure)                               | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             | ‚úÖ                                                                       |
| [AI/ML API](https://docs.litellm.ai/docs/providers/aiml)                               | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             | ‚úÖ                                                                       |
| [aws - sagemaker](https://docs.litellm.ai/docs/providers/aws_sagemaker)             | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             |                                                                         |
| [aws - bedrock](https://docs.litellm.ai/docs/providers/bedrock)                     | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             |                                                                         |
| [google - vertex_ai](https://docs.litellm.ai/docs/providers/vertex)                 | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             | ‚úÖ                                                                       |
| [google - palm](https://docs.litellm.ai/docs/providers/palm)                        | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [google AI Studio - gemini](https://docs.litellm.ai/docs/providers/gemini)          | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [mistral ai api](https://docs.litellm.ai/docs/providers/mistral)                    | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             |                                                                         |
| [cloudflare AI Workers](https://docs.litellm.ai/docs/providers/cloudflare_workers)  | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [cohere](https://docs.litellm.ai/docs/providers/cohere)                             | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 | ‚úÖ                                                                             |                                                                         |
| [anthropic](https://docs.litellm.ai/docs/providers/anthropic)                       | ‚úÖ                                                       | ‚úÖ                                                                               | ‚úÖ                                                                                   | ‚úÖ                                                                                 |                                                                               |                                                                         |
| [empower](https://docs.litellm.ai/docs/providers/empower)                    | ‚úÖ                                                      | ‚úÖ                                                                              | ‚úÖ                                                                                  | ‚úÖ                                                                                |
| [huggingface](https://docs.litellm.ai/docs/providers/huggingface)                   | ‚úÖ                                                       | ‚úÖ                            

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[lipku/LiveTalking]]></title>
            <link>https://github.com/lipku/LiveTalking</link>
            <guid>https://github.com/lipku/LiveTalking</guid>
            <pubDate>Wed, 13 Aug 2025 00:04:10 GMT</pubDate>
            <description><![CDATA[Real time interactive streaming digital human]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lipku/LiveTalking">lipku/LiveTalking</a></h1>
            <p>Real time interactive streaming digital human</p>
            <p>Language: Python</p>
            <p>Stars: 6,207</p>
            <p>Forks: 956</p>
            <p>Stars today: 19 stars today</p>
            <h2>README</h2><pre> [English](./README-EN.md) | ‰∏≠ÊñáÁâà   
 ÂÆûÊó∂‰∫§‰∫íÊµÅÂºèÊï∞Â≠ó‰∫∫ÔºåÂÆûÁé∞Èü≥ËßÜÈ¢ëÂêåÊ≠•ÂØπËØù„ÄÇÂü∫Êú¨ÂèØ‰ª•ËææÂà∞ÂïÜÁî®ÊïàÊûú
[wav2lipÊïàÊûú](https://www.bilibili.com/video/BV1scwBeyELA/) | [ernerfÊïàÊûú](https://www.bilibili.com/video/BV1G1421z73r/) | [musetalkÊïàÊûú](https://www.bilibili.com/video/BV1gm421N7vQ/)  
ÂõΩÂÜÖÈïúÂÉèÂú∞ÂùÄ:&lt;https://gitee.com/lipku/LiveTalking&gt; 

## ‰∏∫ÈÅøÂÖç‰∏é3dÊï∞Â≠ó‰∫∫Ê∑∑Ê∑ÜÔºåÂéüÈ°πÁõÆmetahuman-streamÊîπÂêç‰∏∫livetalkingÔºåÂéüÊúâÈìæÊé•Âú∞ÂùÄÁªßÁª≠ÂèØÁî®

## News
- 2024.12.8 ÂÆåÂñÑÂ§öÂπ∂ÂèëÔºåÊòæÂ≠ò‰∏çÈöèÂπ∂ÂèëÊï∞Â¢ûÂä†
- 2024.12.21 Ê∑ªÂä†wav2lip„ÄÅmusetalkÊ®°ÂûãÈ¢ÑÁÉ≠ÔºåËß£ÂÜ≥Á¨¨‰∏ÄÊ¨°Êé®ÁêÜÂç°È°øÈóÆÈ¢ò„ÄÇÊÑüË∞¢[@heimaojinzhangyz](https://github.com/heimaojinzhangyz)
- 2024.12.28 Ê∑ªÂä†Êï∞Â≠ó‰∫∫Ê®°ÂûãUltralight-Digital-Human„ÄÇ ÊÑüË∞¢[@lijihua2017](https://github.com/lijihua2017)
- 2025.2.7 Ê∑ªÂä†fish-speech tts
- 2025.2.21 Ê∑ªÂä†wav2lip256ÂºÄÊ∫êÊ®°Âûã ÊÑüË∞¢@‰∏çË†¢‰∏çË†¢
- 2025.3.2 Ê∑ªÂä†ËÖæËÆØËØ≠Èü≥ÂêàÊàêÊúçÂä°
- 2025.3.16 ÊîØÊåÅmac gpuÊé®ÁêÜÔºåÊÑüË∞¢[@GcsSloop](https://github.com/GcsSloop) 
- 2025.5.1 Á≤æÁÆÄËøêË°åÂèÇÊï∞ÔºåernerfÊ®°ÂûãÁßªËá≥gitÂàÜÊîØernerf-rtmp
- 2025.6.7 Ê∑ªÂä†ËôöÊãüÊëÑÂÉèÂ§¥ËæìÂá∫
- 2025.7.5 Ê∑ªÂä†Ë±ÜÂåÖËØ≠Èü≥ÂêàÊàê, ÊÑüË∞¢[@ELK-milu](https://github.com/ELK-milu)
- 2025.7.26 ÊîØÊåÅmusetalk v1.5ÁâàÊú¨

## Features
1. ÊîØÊåÅÂ§öÁßçÊï∞Â≠ó‰∫∫Ê®°Âûã: ernerf„ÄÅmusetalk„ÄÅwav2lip„ÄÅUltralight-Digital-Human
2. ÊîØÊåÅÂ£∞Èü≥ÂÖãÈöÜ
3. ÊîØÊåÅÊï∞Â≠ó‰∫∫ËØ¥ËØùË¢´ÊâìÊñ≠
4. ÊîØÊåÅÂÖ®Ë∫´ËßÜÈ¢ëÊãºÊé•
5. ÊîØÊåÅwebrtc„ÄÅËôöÊãüÊëÑÂÉèÂ§¥ËæìÂá∫
6. ÊîØÊåÅÂä®‰ΩúÁºñÊéíÔºö‰∏çËØ¥ËØùÊó∂Êí≠ÊîæËá™ÂÆö‰πâËßÜÈ¢ë
7. ÊîØÊåÅÂ§öÂπ∂Âèë

## 1. Installation

Tested on Ubuntu 24.04, Python3.10, Pytorch 2.5.0 and CUDA 12.4

### 1.1 Install dependency

```bash
conda create -n nerfstream python=3.10
conda activate nerfstream
#Â¶ÇÊûúcudaÁâàÊú¨‰∏ç‰∏∫12.4(ËøêË°ånvidia-smiÁ°ÆËÆ§ÁâàÊú¨)ÔºåÊ†πÊçÆ&lt;https://pytorch.org/get-started/previous-versions/&gt;ÂÆâË£ÖÂØπÂ∫îÁâàÊú¨ÁöÑpytorch 
conda install pytorch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 pytorch-cuda=12.4 -c pytorch -c nvidia
pip install -r requirements.txt
#Â¶ÇÊûúÈúÄË¶ÅËÆ≠ÁªÉernerfÊ®°ÂûãÔºåÂÆâË£Ö‰∏ãÈù¢ÁöÑÂ∫ì
# pip install &quot;git+https://github.com/facebookresearch/pytorch3d.git&quot;
# pip install tensorflow-gpu==2.8.0
# pip install --upgrade &quot;protobuf&lt;=3.20.1&quot;
``` 
ÂÆâË£ÖÂ∏∏ËßÅÈóÆÈ¢ò[FAQ](https://livetalking-doc.readthedocs.io/zh-cn/latest/faq.html)  
linux cudaÁéØÂ¢ÉÊê≠Âª∫ÂèØ‰ª•ÂèÇËÄÉËøôÁØáÊñáÁ´† &lt;https://zhuanlan.zhihu.com/p/674972886&gt;  
ËßÜÈ¢ëËøû‰∏ç‰∏äËß£ÂÜ≥ÊñπÊ≥ï &lt;https://mp.weixin.qq.com/s/MVUkxxhV2cgMMHalphr2cg&gt;


## 2. Quick Start
- ‰∏ãËΩΩÊ®°Âûã  
Â§∏ÂÖã‰∫ëÁõò&lt;https://pan.quark.cn/s/83a750323ef0&gt;    
GoogleDriver &lt;https://drive.google.com/drive/folders/1FOC_MD6wdogyyX_7V1d4NDIO7P9NlSAJ?usp=sharing&gt;  
Â∞Üwav2lip256.pthÊã∑Âà∞Êú¨È°πÁõÆÁöÑmodels‰∏ã, ÈáçÂëΩÂêç‰∏∫wav2lip.pth;  
Â∞Üwav2lip256_avatar1.tar.gzËß£ÂéãÂêéÊï¥‰∏™Êñá‰ª∂Â§πÊã∑Âà∞Êú¨È°πÁõÆÁöÑdata/avatars‰∏ã
- ËøêË°å  
python app.py --transport webrtc --model wav2lip --avatar_id wav2lip256_avatar1  
&lt;font color=red&gt;ÊúçÂä°Á´ØÈúÄË¶ÅÂºÄÊîæÁ´ØÂè£ tcp:8010; udp:1-65536 &lt;/font&gt;  
ÂÆ¢Êà∑Á´ØÂèØ‰ª•ÈÄâÁî®‰ª•‰∏ã‰∏§ÁßçÊñπÂºè:  
(1)Áî®ÊµèËßàÂô®ÊâìÂºÄhttp://serverip:8010/webrtcapi.html , ÂÖàÁÇπ‚Äòstart&#039;,Êí≠ÊîæÊï∞Â≠ó‰∫∫ËßÜÈ¢ëÔºõÁÑ∂ÂêéÂú®ÊñáÊú¨Ê°ÜËæìÂÖ•‰ªªÊÑèÊñáÂ≠óÔºåÊèê‰∫§„ÄÇÊï∞Â≠ó‰∫∫Êí≠Êä•ËØ•ÊÆµÊñáÂ≠ó  
(2)Áî®ÂÆ¢Êà∑Á´ØÊñπÂºè, ‰∏ãËΩΩÂú∞ÂùÄ&lt;https://pan.quark.cn/s/d7192d8ac19b&gt;   

- Âø´ÈÄü‰ΩìÈ™å  
&lt;https://www.compshare.cn/images/4458094e-a43d-45fe-9b57-de79253befe4?referral_code=3XW3852OBmnD089hMMrtuU&amp;ytag=GPU_GitHub_livetalking&gt; Áî®ËØ•ÈïúÂÉèÂàõÂª∫ÂÆû‰æãÂç≥ÂèØËøêË°åÊàêÂäü

Â¶ÇÊûúËÆøÈóÆ‰∏ç‰∫ÜhuggingfaceÔºåÂú®ËøêË°åÂâç
```
export HF_ENDPOINT=https://hf-mirror.com
``` 


## 3. More Usage
‰ΩøÁî®ËØ¥Êòé: &lt;https://livetalking-doc.readthedocs.io/&gt;
  
## 4. Docker Run  
‰∏çÈúÄË¶ÅÂâçÈù¢ÁöÑÂÆâË£ÖÔºåÁõ¥Êé•ËøêË°å„ÄÇ
```
docker run --gpus all -it --network=host --rm registry.cn-zhangjiakou.aliyuncs.com/codewithgpu3/lipku-livetalking:toza2irpHZ
```
‰ª£Á†ÅÂú®/root/livetalkingÔºåÂÖàgit pullÊãâ‰∏Ä‰∏ãÊúÄÊñ∞‰ª£Á†ÅÔºåÁÑ∂ÂêéÊâßË°åÂëΩ‰ª§ÂêåÁ¨¨2„ÄÅ3Ê≠• 

Êèê‰æõÂ¶Ç‰∏ãÈïúÂÉè
- autodlÈïúÂÉè: &lt;https://www.codewithgpu.com/i/lipku/livetalking/base&gt;   
[autodlÊïôÁ®ã](https://livetalking-doc.readthedocs.io/en/latest/autodl/README.html)
- ucloudÈïúÂÉè: &lt;https://www.compshare.cn/images/4458094e-a43d-45fe-9b57-de79253befe4?referral_code=3XW3852OBmnD089hMMrtuU&amp;ytag=GPU_GitHub_livetalking&gt;  
ÂèØ‰ª•ÂºÄÊîæ‰ªªÊÑèÁ´ØÂè£Ôºå‰∏çÈúÄË¶ÅÂè¶Â§ñÈÉ®ÁΩ≤srsÊúçÂä°.  
[ucloudÊïôÁ®ã](https://livetalking-doc.readthedocs.io/en/latest/ucloud/ucloud.html) 


## 5. ÊÄßËÉΩ
- ÊÄßËÉΩ‰∏ªË¶ÅË∑ücpuÂíågpuÁõ∏ÂÖ≥ÔºåÊØèË∑ØËßÜÈ¢ëÂéãÁº©ÈúÄË¶ÅÊ∂àËÄócpuÔºåcpuÊÄßËÉΩ‰∏éËßÜÈ¢ëÂàÜËæ®ÁéáÊ≠£Áõ∏ÂÖ≥ÔºõÊØèË∑ØÂè£ÂûãÊé®ÁêÜË∑ügpuÊÄßËÉΩÁõ∏ÂÖ≥„ÄÇ  
- ‰∏çËØ¥ËØùÊó∂ÁöÑÂπ∂ÂèëÊï∞Ë∑ücpuÁõ∏ÂÖ≥ÔºåÂêåÊó∂ËØ¥ËØùÁöÑÂπ∂ÂèëÊï∞Ë∑ügpuÁõ∏ÂÖ≥„ÄÇ  
- ÂêéÁ´ØÊó•ÂøóinferfpsË°®Á§∫ÊòæÂç°Êé®ÁêÜÂ∏ßÁéáÔºåfinalfpsË°®Á§∫ÊúÄÁªàÊé®ÊµÅÂ∏ßÁéá„ÄÇ‰∏§ËÄÖÈÉΩË¶ÅÂú®25‰ª•‰∏äÊâçËÉΩÂÆûÊó∂„ÄÇÂ¶ÇÊûúinferfpsÂú®25‰ª•‰∏äÔºåfinalfpsËææ‰∏çÂà∞25Ë°®Á§∫cpuÊÄßËÉΩ‰∏çË∂≥„ÄÇ  
- ÂÆûÊó∂Êé®ÁêÜÊÄßËÉΩ  

Ê®°Âûã    |ÊòæÂç°ÂûãÂè∑   |fps
:----   |:---   |:---
wav2lip256 | 3060    | 60
wav2lip256 | 3080Ti  | 120
musetalk   | 3080Ti  | 42
musetalk   | 3090    | 45
musetalk   | 4090    | 72 

wav2lip256ÊòæÂç°3060‰ª•‰∏äÂç≥ÂèØÔºåmusetalkÈúÄË¶Å3080Ti‰ª•‰∏ä„ÄÇ 

## 6. ÂïÜ‰∏öÁâà
Êèê‰æõÂ¶Ç‰∏ãÊâ©Â±ïÂäüËÉΩÔºåÈÄÇÁî®‰∫éÂØπÂºÄÊ∫êÈ°πÁõÆÂ∑≤ÁªèÊØîËæÉÁÜüÊÇâÔºåÈúÄË¶ÅÊâ©Â±ï‰∫ßÂìÅÂäüËÉΩÁöÑÁî®Êà∑
1. È´òÊ∏Öwav2lipÊ®°Âûã
2. ÂÆåÂÖ®ËØ≠Èü≥‰∫§‰∫íÔºåÊï∞Â≠ó‰∫∫ÂõûÁ≠îËøáÁ®ã‰∏≠ÊîØÊåÅÈÄöËøáÂî§ÈÜíËØçÊàñËÄÖÊåâÈíÆÊâìÊñ≠ÊèêÈóÆ
3. ÂÆûÊó∂ÂêåÊ≠•Â≠óÂπïÔºåÁªôÂâçÁ´ØÊèê‰æõÊï∞Â≠ó‰∫∫ÊØèÂè•ËØùÊí≠Êä•ÂºÄÂßã„ÄÅÁªìÊùü‰∫ã‰ª∂
4. ÊØè‰∏™ËøûÊé•ÂèØ‰ª•ÊåáÂÆöÂØπÂ∫îavatarÂíåÈü≥Ëâ≤ÔºåavatarÂõæÁâáÂä†ËΩΩÂä†ÈÄü
5. Âä®‰ΩúÁºñÊéíÔºö‰∏çËØ¥ËØùÊó∂Âä®‰Ωú„ÄÅÂî§ÈÜíÊó∂Âä®‰Ωú„ÄÅÊÄùËÄÉÊó∂Âä®‰Ωú
6. ÊîØÊåÅ‰∏çÈôêÊó∂ÈïøÁöÑÊï∞Â≠ó‰∫∫ÂΩ¢Ë±°avatar
7. Êèê‰æõÂÆûÊó∂Èü≥È¢ëÊµÅËæìÂÖ•Êé•Âè£
8. Êï∞Â≠ó‰∫∫ÈÄèÊòéËÉåÊôØÔºåÂè†Âä†Âä®ÊÄÅËÉåÊôØ  

Êõ¥Â§öËØ¶ÊÉÖ&lt;https://livetalking-doc.readthedocs.io/zh-cn/latest/service.html#wav2lip&gt;

## 7. Â£∞Êòé
Âü∫‰∫éÊú¨È°πÁõÆÂºÄÂèëÂπ∂ÂèëÂ∏ÉÂú®BÁ´ô„ÄÅËßÜÈ¢ëÂè∑„ÄÅÊäñÈü≥Á≠âÁΩëÁ´ô‰∏äÁöÑËßÜÈ¢ëÈúÄÂ∏¶‰∏äLiveTalkingÊ∞¥Âç∞ÂíåÊ†áËØÜÔºåÂ¶ÇÈúÄÂéªÈô§ËØ∑ËÅîÁ≥ª‰ΩúËÄÖÂ§áÊ°àÊéàÊùÉ„ÄÇ

---
Â¶ÇÊûúÊú¨È°πÁõÆÂØπ‰Ω†ÊúâÂ∏ÆÂä©ÔºåÂ∏ÆÂøôÁÇπ‰∏™star„ÄÇ‰πüÊ¨¢ËøéÊÑüÂÖ¥Ë∂£ÁöÑÊúãÂèã‰∏ÄËµ∑Êù•ÂÆåÂñÑËØ•È°πÁõÆ.
* Áü•ËØÜÊòüÁêÉ: https://t.zsxq.com/7NMyO Ê≤âÊ∑ÄÈ´òË¥®ÈáèÂ∏∏ËßÅÈóÆÈ¢ò„ÄÅÊúÄ‰Ω≥ÂÆûË∑µÁªèÈ™å„ÄÅÈóÆÈ¢òËß£Á≠î  
* ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ÔºöÊï∞Â≠ó‰∫∫ÊäÄÊúØ  
![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/l3ZibgueFiaeyfaiaLZGuMGQXnhLWxibpJUS2gfs8Dje6JuMY8zu2tVyU9n8Zx1yaNncvKHBMibX0ocehoITy5qQEZg/640?wxfrom=12&amp;tp=wxpic&amp;usePicPrefetch=1&amp;wx_fmt=jpeg&amp;amp;from=appmsg)  

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[paperless-ngx/paperless-ngx]]></title>
            <link>https://github.com/paperless-ngx/paperless-ngx</link>
            <guid>https://github.com/paperless-ngx/paperless-ngx</guid>
            <pubDate>Wed, 13 Aug 2025 00:04:09 GMT</pubDate>
            <description><![CDATA[A community-supported supercharged document management system: scan, index and archive all your documents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/paperless-ngx/paperless-ngx">paperless-ngx/paperless-ngx</a></h1>
            <p>A community-supported supercharged document management system: scan, index and archive all your documents</p>
            <p>Language: Python</p>
            <p>Stars: 30,313</p>
            <p>Forks: 1,832</p>
            <p>Stars today: 203 stars today</p>
            <h2>README</h2><pre>[![ci](https://github.com/paperless-ngx/paperless-ngx/workflows/ci/badge.svg)](https://github.com/paperless-ngx/paperless-ngx/actions)
[![Crowdin](https://badges.crowdin.net/paperless-ngx/localized.svg)](https://crowdin.com/project/paperless-ngx)
[![Documentation Status](https://img.shields.io/github/deployments/paperless-ngx/paperless-ngx/github-pages?label=docs)](https://docs.paperless-ngx.com)
[![codecov](https://codecov.io/gh/paperless-ngx/paperless-ngx/branch/main/graph/badge.svg?token=VK6OUPJ3TY)](https://codecov.io/gh/paperless-ngx/paperless-ngx)
[![Chat on Matrix](https://matrix.to/img/matrix-badge.svg)](https://matrix.to/#/%23paperlessngx%3Amatrix.org)
[![demo](https://cronitor.io/badges/ve7ItY/production/W5E_B9jkelG9ZbDiNHUPQEVH3MY.svg)](https://demo.paperless-ngx.com)

&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/paperless-ngx/paperless-ngx/blob/main/resources/logo/web/png/White%20logo%20-%20no%20background.png&quot; width=&quot;50%&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png&quot; width=&quot;50%&quot;&gt;
    &lt;img src=&quot;https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png&quot; width=&quot;50%&quot;&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;!-- omit in toc --&gt;

# Paperless-ngx

Paperless-ngx is a document management system that transforms your physical documents into a searchable online archive so you can keep, well, _less paper_.

Paperless-ngx is the official successor to the original [Paperless](https://github.com/the-paperless-project/paperless) &amp; [Paperless-ng](https://github.com/jonaswinkler/paperless-ng) projects and is designed to distribute the responsibility of advancing and supporting the project among a team of people. [Consider joining us!](#community-support)

Thanks to the generous folks at [DigitalOcean](https://m.do.co/c/8d70b916d462), a demo is available at [demo.paperless-ngx.com](https://demo.paperless-ngx.com) using login `demo` / `demo`. _Note: demo content is reset frequently and confidential information should not be uploaded._

- [Features](#features)
- [Getting started](#getting-started)
- [Contributing](#contributing)
  - [Community Support](#community-support)
  - [Translation](#translation)
  - [Feature Requests](#feature-requests)
  - [Bugs](#bugs)
- [Related Projects](#related-projects)
- [Important Note](#important-note)

&lt;p align=&quot;right&quot;&gt;This project is supported by:&lt;br/&gt;
  &lt;a href=&quot;https://m.do.co/c/8d70b916d462&quot; style=&quot;padding-top: 4px; display: block;&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_white.svg&quot; width=&quot;140px&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg&quot; width=&quot;140px&quot;&gt;
      &lt;img src=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_black_.svg&quot; width=&quot;140px&quot;&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
&lt;/p&gt;

# Features

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards-dark.png&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png&quot;&gt;
&lt;/picture&gt;

A full list of [features](https://docs.paperless-ngx.com/#features) and [screenshots](https://docs.paperless-ngx.com/#screenshots) are available in the [documentation](https://docs.paperless-ngx.com/).

# Getting started

The easiest way to deploy paperless is `docker compose`. The files in the [`/docker/compose` directory](https://github.com/paperless-ngx/paperless-ngx/tree/main/docker/compose) are configured to pull the image from the GitHub container registry.

If you&#039;d like to jump right in, you can configure a `docker compose` environment with our install script:

```bash
bash -c &quot;$(curl -L https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/install-paperless-ngx.sh)&quot;
```

More details and step-by-step guides for alternative installation methods can be found in [the documentation](https://docs.paperless-ngx.com/setup/#installation).

Migrating from Paperless-ng is easy, just drop in the new docker image! See the [documentation on migrating](https://docs.paperless-ngx.com/setup/#migrating-to-paperless-ngx) for more details.

&lt;!-- omit in toc --&gt;

### Documentation

The documentation for Paperless-ngx is available at [https://docs.paperless-ngx.com](https://docs.paperless-ngx.com/).

# Contributing

If you feel like contributing to the project, please do! Bug fixes, enhancements, visual fixes etc. are always welcome. If you want to implement something big: Please start a discussion about that! The [documentation](https://docs.paperless-ngx.com/development/) has some basic information on how to get started.

## Community Support

People interested in continuing the work on paperless-ngx are encouraged to reach out here on github and in the [Matrix Room](https://matrix.to/#/#paperless:matrix.org). If you would like to contribute to the project on an ongoing basis there are multiple [teams](https://github.com/orgs/paperless-ngx/people) (frontend, ci/cd, etc) that could use your help so please reach out!

## Translation

Paperless-ngx is available in many languages that are coordinated on Crowdin. If you want to help out by translating paperless-ngx into your language, please head over to https://crowdin.com/project/paperless-ngx, and thank you! More details can be found in [CONTRIBUTING.md](https://github.com/paperless-ngx/paperless-ngx/blob/main/CONTRIBUTING.md#translating-paperless-ngx).

## Feature Requests

Feature requests can be submitted via [GitHub Discussions](https://github.com/paperless-ngx/paperless-ngx/discussions/categories/feature-requests), you can search for existing ideas, add your own and vote for the ones you care about.

## Bugs

For bugs please [open an issue](https://github.com/paperless-ngx/paperless-ngx/issues) or [start a discussion](https://github.com/paperless-ngx/paperless-ngx/discussions) if you have questions.

# Related Projects

Please see [the wiki](https://github.com/paperless-ngx/paperless-ngx/wiki/Related-Projects) for a user-maintained list of related projects and software that is compatible with Paperless-ngx.

# Important Note

&gt; Document scanners are typically used to scan sensitive documents like your social insurance number, tax records, invoices, etc. **Paperless-ngx should never be run on an untrusted host** because information is stored in clear text without encryption. No guarantees are made regarding security (but we do try!) and you use the app at your own risk.
&gt; **The safest way to run Paperless-ngx is on a local server in your own home with backups in place**.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[tadata-org/fastapi_mcp]]></title>
            <link>https://github.com/tadata-org/fastapi_mcp</link>
            <guid>https://github.com/tadata-org/fastapi_mcp</guid>
            <pubDate>Wed, 13 Aug 2025 00:04:08 GMT</pubDate>
            <description><![CDATA[Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/tadata-org/fastapi_mcp">tadata-org/fastapi_mcp</a></h1>
            <p>Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!</p>
            <p>Language: Python</p>
            <p>Stars: 7,797</p>
            <p>Forks: 630</p>
            <p>Stars today: 392 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/tadata-org/fastapi_mcp&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/7e44e98b-a0ba-4aff-a68a-4ffee3a6189c&quot; alt=&quot;fastapi-to-mcp&quot; height=100/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;span style=&quot;font-size: 0.85em; font-weight: normal;&quot;&gt;Built by &lt;a href=&quot;https://tadata.com&quot;&gt;Tadata&lt;/a&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;h1 align=&quot;center&quot;&gt;
  FastAPI-MCP
&lt;/h1&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/14064&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14064&quot; alt=&quot;tadata-org%2Ffastapi_mcp | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;

[![PyPI version](https://img.shields.io/pypi/v/fastapi-mcp?color=%2334D058&amp;label=pypi%20package)](https://pypi.org/project/fastapi-mcp/)
[![Python Versions](https://img.shields.io/pypi/pyversions/fastapi-mcp.svg)](https://pypi.org/project/fastapi-mcp/)
[![FastAPI](https://img.shields.io/badge/FastAPI-009485.svg?logo=fastapi&amp;logoColor=white)](#)
[![CI](https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml/badge.svg)](https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml)
[![Coverage](https://codecov.io/gh/tadata-org/fastapi_mcp/branch/main/graph/badge.svg)](https://codecov.io/gh/tadata-org/fastapi_mcp)

&lt;/div&gt;


&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/tadata-org/fastapi_mcp&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/b205adc6-28c0-4e3c-a68b-9c1a80eb7d0c&quot; alt=&quot;fastapi-mcp-usage&quot; height=&quot;400&quot;/&gt;&lt;/a&gt;&lt;/p&gt;


## Features

- **Authentication** built in, using your existing FastAPI dependencies!

- **FastAPI-native:** Not just another OpenAPI -&gt; MCP converter

- **Zero/Minimal configuration** required - just point it at your FastAPI app and it works

- **Preserving schemas** of your request models and response models

- **Preserve documentation** of all your endpoints, just as it is in Swagger

- **Flexible deployment** - Mount your MCP server to the same app, or deploy separately

- **ASGI transport** - Uses FastAPI&#039;s ASGI interface directly for efficient communication


## Hosted Solution

If you prefer a managed hosted solution check out [tadata.com](https://tadata.com).

## Installation

We recommend using [uv](https://docs.astral.sh/uv/), a fast Python package installer:

```bash
uv add fastapi-mcp
```

Alternatively, you can install with pip:

```bash
pip install fastapi-mcp
```

## Basic Usage

The simplest way to use FastAPI-MCP is to add an MCP server directly to your FastAPI application:

```python
from fastapi import FastAPI
from fastapi_mcp import FastApiMCP

app = FastAPI()

mcp = FastApiMCP(app)

# Mount the MCP server directly to your FastAPI app
mcp.mount()
```

That&#039;s it! Your auto-generated MCP server is now available at `https://app.base.url/mcp`.

## Documentation, Examples and Advanced Usage

FastAPI-MCP provides [comprehensive documentation](https://fastapi-mcp.tadata.com/). Additionaly, check out the [examples directory](examples) for code samples demonstrating these features in action.

## FastAPI-first Approach

FastAPI-MCP is designed as a native extension of FastAPI, not just a converter that generates MCP tools from your API. This approach offers several key advantages:

- **Native dependencies**: Secure your MCP endpoints using familiar FastAPI `Depends()` for authentication and authorization

- **ASGI transport**: Communicates directly with your FastAPI app using its ASGI interface, eliminating the need for HTTP calls from the MCP to your API

- **Unified infrastructure**: Your FastAPI app doesn&#039;t need to run separately from the MCP server (though [separate deployment](https://fastapi-mcp.tadata.com/advanced/deploy#deploying-separately-from-original-fastapi-app) is also supported)

This design philosophy ensures minimum friction when adding MCP capabilities to your existing FastAPI services.


## Development and Contributing

Thank you for considering contributing to FastAPI-MCP! We encourage the community to post Issues and create Pull Requests.

Before you get started, please see our [Contribution Guide](CONTRIBUTING.md).

## Community

Join [MCParty Slack community](https://join.slack.com/t/themcparty/shared_invite/zt-30yxr1zdi-2FG~XjBA0xIgYSYuKe7~Xg) to connect with other MCP enthusiasts, ask questions, and share your experiences with FastAPI-MCP.

## Requirements

- Python 3.10+ (Recommended 3.12)
- uv

## License

MIT License. Copyright (c) 2025 Tadata Inc.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[getzep/graphiti]]></title>
            <link>https://github.com/getzep/graphiti</link>
            <guid>https://github.com/getzep/graphiti</guid>
            <pubDate>Wed, 13 Aug 2025 00:04:07 GMT</pubDate>
            <description><![CDATA[Build Real-Time Knowledge Graphs for AI Agents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/getzep/graphiti">getzep/graphiti</a></h1>
            <p>Build Real-Time Knowledge Graphs for AI Agents</p>
            <p>Language: Python</p>
            <p>Stars: 16,684</p>
            <p>Forks: 1,445</p>
            <p>Stars today: 125 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.getzep.com/&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73&quot; width=&quot;150&quot; alt=&quot;Zep Logo&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt;
Graphiti
&lt;/h1&gt;
&lt;h2 align=&quot;center&quot;&gt; Build Real-Time Knowledge Graphs for AI Agents&lt;/h2&gt;
&lt;div align=&quot;center&quot;&gt;

[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)
[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)
[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)

![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)
[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?&amp;logo=discord&amp;logoColor=white)](https://discord.com/invite/W8Kw6bsgXQ)
[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)
[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&amp;label=Release&amp;color=limegreen)](https://github.com/getzep/graphiti/releases)

&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://trendshift.io/repositories/12986&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12986&quot; alt=&quot;getzep%2Fgraphiti | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;

:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_

&lt;br /&gt;

&gt; [!TIP]
&gt; Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.

Graphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.

Use Graphiti to:

- Integrate and maintain dynamic user interactions and business data.
- Facilitate state-based reasoning and task automation for agents.
- Query complex, evolving data with semantic, keyword, and graph-based search methods.

&lt;br /&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;images/graphiti-graph-intro.gif&quot; alt=&quot;Graphiti temporal walkthrough&quot; width=&quot;700px&quot;&gt;   
&lt;/p&gt;

&lt;br /&gt;

A knowledge graph is a network of interconnected facts, such as _&quot;Kendra loves Adidas shoes.&quot;_ Each fact is a &quot;triplet&quot; represented by two entities, or
nodes (&quot;Kendra&quot;, &quot;Adidas shoes&quot;), and their relationship, or edge (&quot;loves&quot;). Knowledge Graphs have been explored
extensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph
while handling changing relationships and maintaining historical context.

## Graphiti and Zep&#039;s Context Engineering Platform.

Graphiti powers the core of [Zep](https://www.getzep.com), a turn-key context engineering platform for AI Agents. Zep offers agent memory, Graph RAG for dynamic data, and context retrieval and assembly.

Using Graphiti, we&#039;ve demonstrated Zep is
the [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).

Read our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).

We&#039;re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2501.13956&quot;&gt;&lt;img src=&quot;images/arxiv-screenshot.png&quot; alt=&quot;Zep: A Temporal Knowledge Graph Architecture for Agent Memory&quot; width=&quot;700px&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

## Why Graphiti?

Traditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:

- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.
- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.
- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.
- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.
- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/images/graphiti-intro-slides-stock-2.gif&quot; alt=&quot;Graphiti structured + unstructured demo&quot; width=&quot;700px&quot;&gt;   
&lt;/p&gt;

## Graphiti vs. GraphRAG

| Aspect                     | GraphRAG                              | Graphiti                                         |
| -------------------------- | ------------------------------------- | ------------------------------------------------ |
| **Primary Use**            | Static document summarization         | Dynamic data management                          |
| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |
| **Knowledge Structure**    | Entity clusters &amp; community summaries | Episodic data, semantic entities, communities    |
| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |
| **Adaptability**           | Low                                   | High                                             |
| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |
| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |
| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |
| **Custom Entity Types**    | No                                    | Yes, customizable                                |
| **Scalability**            | Moderate                              | High, optimized for large datasets               |

Graphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.

## Installation

Requirements:

- Python 3.10 or higher
- Neo4j 5.26 / FalkorDB 1.1.2 or higher (serves as the embeddings storage backend)
- OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)

&gt; [!IMPORTANT]
&gt; Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).
&gt; Using other services may result in incorrect output schemas and ingestion failures. This is particularly
&gt; problematic when using smaller models.

Optional:

- Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)

&gt; [!TIP]
&gt; The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly
&gt; interface to manage Neo4j instances and databases.
&gt; Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:

```bash
docker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest

```

```bash
pip install graphiti-core
```

or

```bash
uv add graphiti-core
```

### Installing with FalkorDB Support

If you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:

```bash
pip install graphiti-core[falkordb]

# or with uv
uv add graphiti-core[falkordb]
```

### You can also install optional LLM providers as extras:

```bash
# Install with Anthropic support
pip install graphiti-core[anthropic]

# Install with Groq support
pip install graphiti-core[groq]

# Install with Google Gemini support
pip install graphiti-core[google-genai]

# Install with multiple providers
pip install graphiti-core[anthropic,groq,google-genai]

# Install with FalkorDB and LLM providers
pip install graphiti-core[falkordb,anthropic,google-genai]
```

## Default to Low Concurrency; LLM Provider 429 Rate Limit Errors

Graphiti&#039;s ingestion pipelines are designed for high concurrency. By default, concurrency is set low to avoid LLM Provider 429 Rate Limit Errors. If you find Graphiti slow, please increase concurrency as described below.

Concurrency controlled by the `SEMAPHORE_LIMIT` environment variable. By default, `SEMAPHORE_LIMIT` is set to `10` concurrent operations to help prevent `429` rate limit errors from your LLM provider. If you encounter such errors, try lowering this value.

If your LLM provider allows higher throughput, you can increase `SEMAPHORE_LIMIT` to boost episode ingestion performance.

## Quick Start

&gt; [!IMPORTANT]
&gt; Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your environment.
&gt; Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI
&gt; compatible APIs.

For a complete working example, see the [Quickstart Example](./examples/quickstart/README.md) in the examples directory. The quickstart demonstrates:

1. Connecting to a Neo4j or FalkorDB database
2. Initializing Graphiti indices and constraints
3. Adding episodes to the graph (both text and structured JSON)
4. Searching for relationships (edges) using hybrid search
5. Reranking search results using graph distance
6. Searching for nodes using predefined search recipes

The example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.

## MCP Server

The `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti&#039;s knowledge graph capabilities through the MCP protocol.

Key features of the MCP server include:

- Episode management (add, retrieve, delete)
- Entity management and relationship handling
- Semantic and hybrid search capabilities
- Group management for organizing related data
- Graph maintenance operations

The MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.

For detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md).

## REST Service

The `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.

Please see the [server README](./server/README.md) for more information.

## Optional Environment Variables

In addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.
If you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables
must be set.

### Database Configuration

Database names are configured directly in the driver constructors:

- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)
- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)

As of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.

#### Neo4j with Custom Database Name

```python
from graphiti_core import Graphiti
from graphiti_core.driver.neo4j_driver import Neo4jDriver

# Create a Neo4j driver with custom database name
driver = Neo4jDriver(
    uri=&quot;bolt://localhost:7687&quot;,
    user=&quot;neo4j&quot;,
    password=&quot;password&quot;,
    database=&quot;my_custom_database&quot;  # Custom database name
)

# Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver)
```

#### FalkorDB with Custom Database Name

```python
from graphiti_core import Graphiti
from graphiti_core.driver.falkordb_driver import FalkorDriver

# Create a FalkorDB driver with custom database name
driver = FalkorDriver(
    host=&quot;localhost&quot;,
    port=6379,
    username=&quot;falkor_user&quot;,  # Optional
    password=&quot;falkor_password&quot;,  # Optional
    database=&quot;my_custom_graph&quot;  # Custom database name
)

# Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver)
```


### Performance Configuration

`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish
to enable Neo4j&#039;s parallel runtime feature for several of our search queries.
Note that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,
as such this feature is off by default.

## Using Graphiti with Azure OpenAI

Graphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.

```python
from openai import AsyncAzureOpenAI
from graphiti_core import Graphiti
from graphiti_core.llm_client import LLMConfig, OpenAIClient
from graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig
from graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient

# Azure OpenAI configuration - use separate endpoints for different services
api_key = &quot;&lt;your-api-key&gt;&quot;
api_version = &quot;&lt;your-api-version&gt;&quot;
llm_endpoint = &quot;&lt;your-llm-endpoint&gt;&quot;  # e.g., &quot;https://your-llm-resource.openai.azure.com/&quot;
embedding_endpoint = &quot;&lt;your-embedding-endpoint&gt;&quot;  # e.g., &quot;https://your-embedding-resource.openai.azure.com/&quot;

# Create separate Azure OpenAI clients for different services
llm_client_azure = AsyncAzureOpenAI(
    api_key=api_key,
    api_version=api_version,
    azure_endpoint=llm_endpoint
)

embedding_client_azure = AsyncAzureOpenAI(
    api_key=api_key,
    api_version=api_version,
    azure_endpoint=embedding_endpoint
)

# Create LLM Config with your Azure deployment names
azure_llm_config = LLMConfig(
    small_model=&quot;gpt-4.1-nano&quot;,
    model=&quot;gpt-4.1-mini&quot;,
)

# Initialize Graphiti with Azure OpenAI clients
graphiti = Graphiti(
    &quot;bolt://localhost:7687&quot;,
    &quot;neo4j&quot;,
    &quot;password&quot;,
    llm_client=OpenAIClient(
        config=azure_llm_config,
        client=llm_client_azure
    ),
    embedder=OpenAIEmbedder(
        config=OpenAIEmbedderConfig(
            embedding_model=&quot;text-embedding-3-small-deployment&quot;  # Your Azure embedding deployment name
        ),
        client=embedding_client_azure
    ),
    cross_encoder=OpenAIRerankerClient(
        config=LLMConfig(
            model=azure_llm_config.small_model  # Use small model for reranking
        ),
        client=llm_client_azure
    )
)

# Now you can use Graphiti with Azure OpenAI
```

Make sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.

## Using Graphiti with Google Gemini

Graphiti supports Google&#039;s Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you&#039;ll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.

Install Graphiti:

```bash
uv add &quot;graphiti-core[google-genai]&quot;

# or

pip install &quot;graphiti-core[google-genai]&quot;
```

```python
from graphiti_core import Graphiti
from graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig
from graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig
from graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient

# Google API key configuration
api_key = &quot;&lt;your-google-api-key&gt;&quot;

# Initialize Graphiti with Gemini clients
graphiti = Graphiti(
    &quot;bolt://localhost:7687&quot;,
    &quot;neo4j&quot;,
    &quot;password&quot;,
    llm_client=GeminiClient(
        config=LLMConfig(
            api_key=api_key,
            model=&quot;gemini-2.0-flash&quot;
        )
    ),
    embedder=GeminiEmbedder(
        config=GeminiEmbedderConfig(
            api_key=api_key,
            embedding_model=&quot;embedding-001&quot;
        )
    ),
    cross_encoder=GeminiRerankerClient(
        config=LLMConfig(
            api_key=api_key,
            model=&quot;gemini-2.5-flash-lite-preview-06-17&quot;
        )
    )
)

# Now you can use Graphiti with Google Gemini for all components
```

The Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini&#039;s log probabilities feature to rank passage relevance.

## Using Graphiti with Ollama (Local LLM)

Graphiti supports Ollama for running local LLMs and embedding models via Ollama&#039;s OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.

Install the models:
ollama pull deepseek-r1:7b # LLM
ollama pull nomic-embed-text # embeddings

```python
from graphiti_core import Graphiti
from graphiti_core.llm_client.config import LLMConfig
from graphiti_core.llm_client.openai_client import OpenAIClient
from graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig
from graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient

# Configure Ollama LLM client
llm_config = LLMConfig(
    api_key=&quot;abc&quot;,  # Ollama doesn&#039;t require a real API key
    model=&quot;deepseek-r1:7b&quot;,
    small_model=&quot;deepseek-r1:7b&quot;,
    base_url=&quot;http://localhost:11434/v1&quot;, # Ollama provides this port
)

llm_client = OpenAIClient(config=llm_config)

# Initialize Graphiti with Ollama clients
graphiti = Graphiti(
    &quot;bolt://localhost:7687&quot;,
    &quot;neo4j&quot;,
    &quot;password&quot;,
    llm_client=llm_client,
    embedder=OpenAIEmbedder(
        config=OpenAIEmbedderConfig(
            api_key=&quot;abc&quot;,
            embedding_model=&quot;nomic-embed-text&quot;,
            embedding_dim=768,
            base_url=&quot;http://localhost:11434/v1&quot;,
        )
    ),
    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),
)

# Now you can use Graphiti with local Ollama models
```

Ensure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.

## Documentation

- [Guides and API documentation](https://help.getzep.com/graphiti).
- [Quick Start](https://help.getzep.com/graphiti/graphiti/quick-start)
- [Building an agent with LangChain&#039;s LangGraph and Graphiti](https://help.getzep.com/graphiti/graphiti/lang-graph-agent)

## Telemetry

Graphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for everyone. We believe transparency is important, so here&#039;s exactly what we collect and why.

### What We Collect

When you initialize a Graphiti instance, we collect:

- **Anonymous identifier**: A randomly generated UUID stored locally in `~/.cache/graphiti/telemetry_anon_id`
- **System information**: Operating system, Python version, and system architecture
- **Graphiti version**: The version you&#039;re using
- **Configuration choices**:
  - LLM provider type (OpenAI, Azure, Anthropic, etc.)
  - Database backend (Neo4j, FalkorDB)
  - Embedder provider (OpenAI, Azure, Voyage, etc.)

### What We Don&#039;t Collect

We are committed to protecting your privacy. We **never** collect:

- Personal information or identifiers
- API keys or credentials
- Your actual data, queries, or graph content
- IP addresses or hostnames
- File paths or system-specific information
- Any content from your episodes, nodes, or edges

### Why We Collect This Data

This information helps us:

- Understand which configurations are most popular to prioritize support and testing
- Identify which LLM 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[1Panel-dev/MaxKB]]></title>
            <link>https://github.com/1Panel-dev/MaxKB</link>
            <guid>https://github.com/1Panel-dev/MaxKB</guid>
            <pubDate>Wed, 13 Aug 2025 00:04:06 GMT</pubDate>
            <description><![CDATA[üî• MaxKB is an open-source platform for building enterprise-grade agents. MaxKB ÊòØÂº∫Â§ßÊòìÁî®ÁöÑÂºÄÊ∫ê‰ºÅ‰∏öÁ∫ßÊô∫ËÉΩ‰ΩìÂπ≥Âè∞„ÄÇ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/1Panel-dev/MaxKB">1Panel-dev/MaxKB</a></h1>
            <p>üî• MaxKB is an open-source platform for building enterprise-grade agents. MaxKB ÊòØÂº∫Â§ßÊòìÁî®ÁöÑÂºÄÊ∫ê‰ºÅ‰∏öÁ∫ßÊô∫ËÉΩ‰ΩìÂπ≥Âè∞„ÄÇ</p>
            <p>Language: Python</p>
            <p>Stars: 17,656</p>
            <p>Forks: 2,302</p>
            <p>Stars today: 27 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;img src= &quot;https://github.com/1Panel-dev/maxkb/assets/52996290/c0694996-0eed-40d8-b369-322bf2a380bf&quot; alt=&quot;MaxKB&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;
&lt;h3 align=&quot;center&quot;&gt;Open-source platform for building enterprise-grade agents&lt;/h3&gt;
&lt;h3 align=&quot;center&quot;&gt;Âº∫Â§ßÊòìÁî®ÁöÑ‰ºÅ‰∏öÁ∫ßÊô∫ËÉΩ‰ΩìÂπ≥Âè∞&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://trendshift.io/repositories/9113&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/9113&quot; alt=&quot;1Panel-dev%2FMaxKB | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.gnu.org/licenses/gpl-3.0.html#license-text&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/1Panel-dev/maxkb?color=%231890FF&quot; alt=&quot;License: GPL v3&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/1Panel-dev/maxkb/releases/latest&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/1Panel-dev/maxkb&quot; alt=&quot;Latest release&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/1Panel-dev/maxkb&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/1Panel-dev/maxkb?color=%231890FF&amp;style=flat-square&quot; alt=&quot;Stars&quot;&gt;&lt;/a&gt;    
  &lt;a href=&quot;https://hub.docker.com/r/1panel/maxkb&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/1panel/maxkb?label=downloads&quot; alt=&quot;Download&quot;&gt;&lt;/a&gt;&lt;br/&gt;
 [&lt;a href=&quot;/README_CN.md&quot;&gt;‰∏≠Êñá(ÁÆÄ‰Ωì)&lt;/a&gt;] | [&lt;a href=&quot;/README.md&quot;&gt;English&lt;/a&gt;] 
&lt;/p&gt;
&lt;hr/&gt;

MaxKB = Max Knowledge Brain, it is an open-source platform for building enterprise-grade agents. MaxKB integrates Retrieval-Augmented Generation (RAG) pipelines, supports robust workflows, and provides advanced MCP tool-use capabilities. MaxKB is widely applied in scenarios such as intelligent customer service, corporate internal knowledge bases, academic research, and education.

- **RAG Pipeline**: Supports direct uploading of documents / automatic crawling of online documents, with features for automatic text splitting, vectorization. This effectively reduces hallucinations in large models, providing a superior smart Q&amp;A interaction experience.
- **Agentic Workflow**: Equipped with a powerful workflow engine, function library and MCP tool-use, enabling the orchestration of AI processes to meet the needs of complex business scenarios. 
- **Seamless Integration**: Facilitates zero-coding rapid integration into third-party business systems, quickly equipping existing systems with intelligent Q&amp;A capabilities to enhance user satisfaction.
- **Model-Agnostic**: Supports various large models, including private models (such as DeepSeek, Llama, Qwen, etc.) and public models (like OpenAI, Claude, Gemini, etc.).
- **Multi Modal**: Native support for input and output text, image, audio and video.

## Quick start

Execute the script below to start a MaxKB container using Docker:

```bash
docker run -d --name=maxkb --restart=always -p 8080:8080 -v ~/.maxkb:/var/lib/postgresql/data -v ~/.python-packages:/opt/maxkb/app/sandbox/python-packages 1panel/maxkb:v1
```

Access MaxKB web interface at `http://your_server_ip:8080` with default admin credentials:

- username: admin
- password: MaxKB@123..

‰∏≠ÂõΩÁî®Êà∑Â¶ÇÈÅáÂà∞ Docker ÈïúÂÉè Pull Â§±Ë¥•ÈóÆÈ¢òÔºåËØ∑ÂèÇÁÖßËØ• [Á¶ªÁ∫øÂÆâË£ÖÊñáÊ°£](https://maxkb.cn/docs/v1/installation/offline_installtion/) ËøõË°åÂÆâË£Ö„ÄÇ

## Screenshots

&lt;table style=&quot;border-collapse: collapse; border: 1px solid black;&quot;&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://maxkb.hk/images/overview.png&quot; alt=&quot;MaxKB Demo1&quot;   /&gt;&lt;/td&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://maxkb.hk/images/screenshot-models.png&quot; alt=&quot;MaxKB Demo2&quot;   /&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://maxkb.hk/images/screenshot-knowledge.png&quot; alt=&quot;MaxKB Demo3&quot;   /&gt;&lt;/td&gt;
    &lt;td style=&quot;padding: 5px;background-color:#fff;&quot;&gt;&lt;img src= &quot;https://maxkb.hk/images/screenshot-function.png&quot; alt=&quot;MaxKB Demo4&quot;   /&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## Technical stack

- FrontendÔºö[Vue.js](https://vuejs.org/)
- BackendÔºö[Python / Django](https://www.djangoproject.com/)
- LLM FrameworkÔºö[LangChain](https://www.langchain.com/)
- DatabaseÔºö[PostgreSQL + pgvector](https://www.postgresql.org/)

## Feature Comparison

&lt;table style=&quot;width: 100%;&quot;&gt;
  &lt;tr&gt;
    &lt;th align=&quot;center&quot;&gt;Feature&lt;/th&gt;
    &lt;th align=&quot;center&quot;&gt;LangChain&lt;/th&gt;
    &lt;th align=&quot;center&quot;&gt;Dify.AI&lt;/th&gt;
    &lt;th align=&quot;center&quot;&gt;Flowise&lt;/th&gt;
    &lt;th align=&quot;center&quot;&gt;MaxKB &lt;br&gt;ÔºàBuilt upon LangChainÔºâ&lt;/th&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;Supported LLMs&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;Rich Variety&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;Rich Variety&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;Rich Variety&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;Rich Variety&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;RAG Engine&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;Agent&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚ùå&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;Workflow&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚ùå&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;Observability&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚ùå&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;SSO/Access control&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚ùå&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚ùå&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ (Pro)&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;On-premise Deployment&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;‚úÖ&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=1Panel-dev/MaxKB&amp;type=Date)](https://star-history.com/#1Panel-dev/MaxKB&amp;Date)

## License

Licensed under The GNU General Public License version 3 (GPLv3)  (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at

&lt;https://www.gnu.org/licenses/gpl-3.0.html&gt;

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>