<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sun, 28 Dec 2025 00:05:35 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[TheAlgorithms/Python]]></title>
            <link>https://github.com/TheAlgorithms/Python</link>
            <guid>https://github.com/TheAlgorithms/Python</guid>
            <pubDate>Sun, 28 Dec 2025 00:05:35 GMT</pubDate>
            <description><![CDATA[All Algorithms implemented in Python]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/TheAlgorithms/Python">TheAlgorithms/Python</a></h1>
            <p>All Algorithms implemented in Python</p>
            <p>Language: Python</p>
            <p>Stars: 215,546</p>
            <p>Forks: 49,740</p>
            <p>Stars today: 152 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;!-- Title: --&gt;
  &lt;a href=&quot;https://github.com/TheAlgorithms/&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/TheAlgorithms/website/1cd824df116b27029f17c2d1b42d81731f28a920/public/logo.svg&quot; height=&quot;100&quot;&gt;
  &lt;/a&gt;
  &lt;h1&gt;&lt;a href=&quot;https://github.com/TheAlgorithms/&quot;&gt;The Algorithms&lt;/a&gt; - Python&lt;/h1&gt;

&lt;!-- Labels: --&gt;
  &lt;!-- First row: --&gt;
  &lt;a href=&quot;https://gitpod.io/#https://github.com/TheAlgorithms/Python&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;Gitpod Ready-to-Code&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/TheAlgorithms/Python/blob/master/CONTRIBUTING.md&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/static/v1.svg?label=Contributions&amp;message=Welcome&amp;color=0059b3&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;Contributions Welcome&quot;&gt;
  &lt;/a&gt;
  &lt;img src=&quot;https://img.shields.io/github/repo-size/TheAlgorithms/Python.svg?label=Repo%20size&amp;style=flat-square&quot; height=&quot;20&quot;&gt;
  &lt;a href=&quot;https://the-algorithms.com/discord&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/discord/808045925556682782.svg?logo=discord&amp;colorB=7289DA&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;Discord chat&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://gitter.im/TheAlgorithms/community&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Chat-Gitter-ff69b4.svg?label=Chat&amp;logo=gitter&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;Gitter chat&quot;&gt;
  &lt;/a&gt;

  &lt;!-- Second row: --&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://github.com/TheAlgorithms/Python/actions&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/TheAlgorithms/Python/build.yml?branch=master&amp;label=CI&amp;logo=github&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;GitHub Workflow Status&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/pre-commit/pre-commit&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;logoColor=white&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;pre-commit&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://docs.astral.sh/ruff/formatter/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/static/v1?label=code%20style&amp;message=ruff&amp;color=black&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;code style: black&quot;&gt;
  &lt;/a&gt;

&lt;!-- Short description: --&gt;
  &lt;h3&gt;All algorithms implemented in Python - for education ğŸ“š&lt;/h3&gt;
&lt;/div&gt;

Implementations are for learning purposes only. They may be less efficient than the implementations in the Python standard library. Use them at your discretion.

## ğŸš€ Getting Started

ğŸ“‹ Read through our [Contribution Guidelines](CONTRIBUTING.md) before you contribute.

## ğŸŒ Community Channels

We are on [Discord](https://the-algorithms.com/discord) and [Gitter](https://gitter.im/TheAlgorithms/community)! Community channels are a great way for you to ask questions and get help. Please join us!

## ğŸ“œ List of Algorithms

See our [directory](DIRECTORY.md) for easier navigation and a better overview of the project.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[xerrors/Yuxi-Know]]></title>
            <link>https://github.com/xerrors/Yuxi-Know</link>
            <guid>https://github.com/xerrors/Yuxi-Know</guid>
            <pubDate>Sun, 28 Dec 2025 00:05:34 GMT</pubDate>
            <description><![CDATA[ç»“åˆLightRAG çŸ¥è¯†åº“çš„çŸ¥è¯†å›¾è°±æ™ºèƒ½ä½“å¹³å°ã€‚ An agent platform that integrates a LightRAG knowledge base and knowledge graphs. Build with LangChain v1 + Vue + FastAPI, support DeepAgentsã€MinerU PDFã€Neo4j ã€MCP.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/xerrors/Yuxi-Know">xerrors/Yuxi-Know</a></h1>
            <p>ç»“åˆLightRAG çŸ¥è¯†åº“çš„çŸ¥è¯†å›¾è°±æ™ºèƒ½ä½“å¹³å°ã€‚ An agent platform that integrates a LightRAG knowledge base and knowledge graphs. Build with LangChain v1 + Vue + FastAPI, support DeepAgentsã€MinerU PDFã€Neo4j ã€MCP.</p>
            <p>Language: Python</p>
            <p>Stars: 3,463</p>
            <p>Forks: 419</p>
            <p>Stars today: 35 stars today</p>
            <h2>README</h2><pre>
&lt;div align=&quot;center&quot;&gt;
&lt;img width=&quot;140&quot; height=&quot;140&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/299137b7-08d8-45b0-9feb-7b4ab35d7b48&quot; /&gt;

&lt;h1&gt;è¯­æ - åŸºäºå¤§æ¨¡å‹çš„çŸ¥è¯†åº“ä¸çŸ¥è¯†å›¾è°±æ™ºèƒ½ä½“å¼€å‘å¹³å°&lt;/h1&gt;

&lt;a href=&quot;https://trendshift.io/repositories/15845&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/15845&quot; alt=&quot;Yuxi-Know | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

[![Stable](https://img.shields.io/badge/stable-v0.4.0-blue.svg)](https://github.com/xerrors/Yuxi-Know/tree/v0.4.0)
[![](https://img.shields.io/badge/Docker-2496ED?style=flat&amp;logo=docker&amp;logoColor=ffffff)](https://github.com/xerrors/Yuxi-Know/blob/main/docker-compose.yml)
[![](https://img.shields.io/github/issues/xerrors/Yuxi-Know?color=F48D73)](https://github.com/xerrors/Yuxi-Know/issues)
[![License](https://img.shields.io/github/license/bitcookies/winrar-keygen.svg?logo=github)](https://github.com/xerrors/Yuxi-Know/blob/main/LICENSE)
[![DeepWiki](https://img.shields.io/badge/DeepWiki-blue.svg)](https://deepwiki.com/xerrors/Yuxi-Know)
[![zread](https://img.shields.io/badge/Ask_Zread-_.svg?style=flat&amp;color=00b0aa&amp;labelColor=000000&amp;logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdCb3g9IjAgMCAxNiAxNiIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTQuOTYxNTYgMS42MDAxSDIuMjQxNTZDMS44ODgxIDEuNjAwMSAxLjYwMTU2IDEuODg2NjQgMS42MDE1NiAyLjI0MDFWNC45NjAxQzEuNjAxNTYgNS4zMTM1NiAxLjg4ODEgNS42MDAxIDIuMjQxNTYgNS42MDAxSDQuOTYxNTZDNS4zMTUwMiA1LjYwMDEgNS42MDE1NiA1LjMxMzU2IDUuNjAxNTYgNC45NjAxVjIuMjQwMUM1LjYwMTU2IDEuODg2NjQgNS4zMTUwMiAxLjYwMDEgNC45NjE1NiAxLjYwMDFaIiBmaWxsPSIjZmZmIi8%2BCjxwYXRoIGQ9Ik00Ljk2MTU2IDEwLjM5OTlIMi4yNDE1NkMxLjg4ODEgMTAuMzk5OSAxLjYwMTU2IDEwLjY4NjQgMS42MDE1NiAxMS4wMzk5VjEzLjc1OTlDMS42MDE1NiAxNC4xMTM0IDEuODg4MSAxNC4zOTk5IDIuMjQxNTYgMTQuMzk5OUg0Ljk2MTU2QzUuMzE1MDIgMTQuMzk5OSA1LjYwMTU2IDE0LjExMzQgNS42MDE1NiAxMy43NTk5VjExLjAzOTlDNS42MDE1NiAxMC42ODY0IDUuMzE1MDIgMTAuMzk5OSA0Ljk2MTU2IDEwLjM5OTlaIiBmaWxsPSIjZmZmIi8%2BCjxwYXRoIGQ9Ik0xMy43NTg0IDEuNjAwMUgxMS4wMzg0QzEwLjY4NSAxLjYwMDEgMTAuMzk4NCAxLjg4NjY0IDEwLjM5ODQgMi4yNDAxVjQuOTYwMUMxMC4zOTg0IDUuMzEzNTYgMTAuNjg1IDUuNjAwMSAxMS4wMzg0IDUuNjAwMUgxMy43NTg0QzE0LjExMTkgNS42MDAxIDE0LjM5ODQgNS4zMTM1NiAxNC4zOTg0IDQuOTYwMVYyLjI0MDFDMTQuMzk4NCAxLjg4NjY0IDE0LjExMTkgMS42MDAxIDEzLjc1ODQgMS42MDAxWiIgZmlsbD0iI2ZmZiIvPgo8cGF0aCBkPSJNNCAxMkwxMiA0TDQgMTJaIiBmaWxsPSIjZmZmIi8%2BCjxwYXRoIGQ9Ik00IDEyTDEyIDQiIHN0cm9rZT0iI2ZmZiIgc3Ryb2tlLXdpZHRoPSIxLjUiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIvPgo8L3N2Zz4K&amp;logoColor=ffffff)](https://zread.ai/xerrors/Yuxi-Know)
[![demo](https://img.shields.io/badge/demo-00A1D6.svg?style=flat&amp;logo=bilibili&amp;logoColor=white)](https://www.bilibili.com/video/BV1DF14BTETq/)

ğŸ“„ [**æ–‡æ¡£ä¸­å¿ƒ**](https://xerrors.github.io/Yuxi-Know/) |
ğŸ“½ï¸ [**è§†é¢‘æ¼”ç¤º**](https://www.bilibili.com/video/BV1DF14BTETq/)

&lt;/div&gt;


è¯­ææ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„æ™ºèƒ½ä½“å¹³å°ï¼Œèåˆäº† RAG çŸ¥è¯†åº“ä¸çŸ¥è¯†å›¾è°±æŠ€æœ¯ï¼ŒåŸºäº LangGraph v1 + Vue.js + FastAPI + LightRAG æ¶æ„æ„å»ºã€‚

**äº®ç‚¹**ï¼šæä¾›å…¨å¥—çš„æ™ºèƒ½ä½“å¼€å‘å¥—ä»¶ï¼ŒåŸºäº MIT å¼€æºåè®®ï¼ŒæŠ€æœ¯æ ˆå‹å¥½ï¼Œé€‚åˆåŸºäºæ­¤é¡¹ç›®æ‰“é€ è‡ªå·±çš„æ™ºèƒ½ä½“å¹³å°ã€‚

&lt;img width=&quot;1632&quot; height=&quot;392&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/ec381fde-53dd-4845-a79f-116b823fe989&quot; /&gt;

---

**ğŸ‰ æœ€æ–°åŠ¨æ€**



- **[2025/12/17] v0.4.0-beta ç‰ˆæœ¬å‘å¸ƒ**
  &lt;details&gt;
  &lt;summary&gt;æŸ¥çœ‹è¯¦ç»†æ›´æ–°æ—¥å¿—&lt;/summary&gt;


  ### æ–°å¢
  - æ–°å¢å¯¹äºä¸Šä¼ é™„ä»¶çš„æ™ºèƒ½ä½“ä¸­é—´ä»¶ï¼Œè¯¦è§[æ–‡æ¡£](https://xerrors.github.io/Yuxi-Know/latest/advanced/agents-config.html#%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E4%B8%AD%E9%97%B4%E4%BB%B6)
  - æ–°å¢å¤šæ¨¡æ€æ¨¡å‹æ”¯æŒï¼ˆå½“å‰ä»…æ”¯æŒå›¾ç‰‡ï¼‰ï¼Œè¯¦è§[æ–‡æ¡£](https://xerrors.github.io/Yuxi-Know/latest/advanced/agents-config.html#%E5%A4%9A%E6%A8%A1%E6%80%81%E5%9B%BE%E7%89%87%E6%94%AF%E6%8C%81)
  - æ–°å»º DeepAgents æ™ºèƒ½ä½“ï¼ˆæ·±åº¦åˆ†ææ™ºèƒ½ä½“ï¼‰ï¼Œæ”¯æŒ todoï¼Œfiles ç­‰æ¸²æŸ“ï¼Œæ”¯æŒæ–‡ä»¶çš„ä¸‹è½½ã€‚
  - æ–°å¢åŸºäºçŸ¥è¯†åº“æ–‡ä»¶ç”Ÿæˆæ€ç»´å¯¼å›¾åŠŸèƒ½ï¼ˆ[#335](https://github.com/xerrors/Yuxi-Know/pull/335#issuecomment-3530976425)ï¼‰
  - æ–°å¢åŸºäºçŸ¥è¯†åº“æ–‡ä»¶ç”Ÿæˆç¤ºä¾‹é—®é¢˜åŠŸèƒ½ï¼ˆ[#335](https://github.com/xerrors/Yuxi-Know/pull/335#issuecomment-3530976425)ï¼‰
  - æ–°å¢çŸ¥è¯†åº“æ”¯æŒæ–‡ä»¶å¤¹/å‹ç¼©åŒ…ä¸Šä¼ çš„åŠŸèƒ½ï¼ˆ[#335](https://github.com/xerrors/Yuxi-Know/pull/335#issuecomment-3530976425)ï¼‰
  - æ–°å¢è‡ªå®šä¹‰æ¨¡å‹æ”¯æŒã€æ–°å¢ dashscope rerank/embeddings æ¨¡å‹çš„æ”¯æŒ
  - æ–°å¢æ–‡æ¡£è§£æçš„å›¾ç‰‡æ”¯æŒï¼Œå·²æ”¯æŒ MinerU Officicalã€Docsã€Markdown Zip æ ¼å¼
  - æ–°å¢æš—è‰²æ¨¡å¼æ”¯æŒå¹¶è°ƒæ•´æ•´ä½“ UIï¼ˆ[#343](https://github.com/xerrors/Yuxi-Know/pull/343)ï¼‰
  - æ–°å¢çŸ¥è¯†åº“è¯„ä¼°åŠŸèƒ½ï¼Œæ”¯æŒå¯¼å…¥è¯„ä¼°åŸºå‡†æˆ–è€…è‡ªåŠ¨æ„å»ºè¯„ä¼°åŸºå‡†ï¼ˆç›®å‰ä»…æ”¯æŒ Milvus ç±»å‹çŸ¥è¯†åº“ï¼‰è¯¦è§[æ–‡æ¡£](https://xerrors.github.io/Yuxi-Know/latest/intro/evaluation.html)
  - æ–°å¢åŒåæ–‡ä»¶å¤„ç†é€»è¾‘ï¼šé‡åˆ°åŒåæ–‡ä»¶åˆ™åœ¨ä¸Šä¼ åŒºåŸŸæç¤ºï¼Œæ˜¯å¦åˆ é™¤æ—§æ–‡ä»¶
  - æ–°å¢ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²è„šæœ¬ï¼Œå›ºå®š python ä¾èµ–ç‰ˆæœ¬ï¼Œæå‡éƒ¨ç½²ç¨³å®šæ€§
  - ä¼˜åŒ–å›¾è°±å¯è§†åŒ–æ–¹å¼ï¼Œç»Ÿä¸€å›¾è°±æ•°æ®ç»“æ„ï¼Œç»Ÿä¸€ä½¿ç”¨åŸºäº G6 çš„å¯è§†åŒ–æ–¹å¼ï¼ŒåŒæ—¶æ”¯æŒä¸Šä¼ å¸¦å±æ€§çš„å›¾è°±æ–‡ä»¶ï¼Œè¯¦è§[æ–‡æ¡£](https://xerrors.github.io/Yuxi-Know/latest/intro/knowledge-base.html#_1-%E4%BB%A5%E4%B8%89%E5%85%83%E7%BB%84%E5%BD%A2%E5%BC%8F%E5%AF%BC%E5%85%A5)
  - ä¼˜åŒ– DBManager / ConversationManagerï¼Œæ”¯æŒå¼‚æ­¥æ“ä½œ
  - ä¼˜åŒ– çŸ¥è¯†åº“è¯¦æƒ…é¡µé¢ï¼Œæ›´åŠ ç®€æ´æ¸…æ™°ï¼Œå¢å¼ºæ–‡ä»¶ä¸‹è½½åŠŸèƒ½

  ### ä¿®å¤
  - ä¿®å¤é‡æ’åºæ¨¡å‹å®é™…æœªç”Ÿæ•ˆçš„é—®é¢˜
  - ä¿®å¤æ¶ˆæ¯ä¸­æ–­åæ¶ˆæ¯æ¶ˆå¤±çš„é—®é¢˜ï¼Œå¹¶æ”¹å–„å¼‚å¸¸æ•ˆæœ
  - ä¿®å¤å½“å‰ç‰ˆæœ¬å¦‚æœè°ƒç”¨ç»“æœä¸ºç©ºçš„æ—¶å€™ï¼Œå·¥å…·è°ƒç”¨çŠ¶æ€ä¼šä¸€ç›´å¤„äºè°ƒç”¨çŠ¶æ€ï¼Œå°½ç®¡è°ƒç”¨æ˜¯æˆåŠŸçš„
  - ä¿®å¤æ£€ç´¢é…ç½®å®é™…æœªç”Ÿæ•ˆçš„é—®é¢˜

  ### ç ´åæ€§æ›´æ–°

  - ç§»é™¤ Chroma çš„æ”¯æŒï¼Œå½“å‰ç‰ˆæœ¬æ ‡è®°ä¸ºç§»é™¤
  - ç§»é™¤æ¨¡å‹é…ç½®é¢„è®¾çš„ TogetherAI
  &lt;/details&gt;

- **[2025/11/05] v0.3 ç‰ˆæœ¬å‘å¸ƒ**
  - å…¨é¢é€‚é… LangChain/LangGraph v1 ç‰ˆæœ¬çš„ç‰¹æ€§ï¼Œä½¿ç”¨ create_agent åˆ›å»ºæ™ºèƒ½ä½“å…¥å£ã€‚
  - æ–‡æ¡£è§£æå‡çº§ï¼Œé€‚é… mineru-2.6 ä»¥åŠ mineru-apiã€‚
  - æ›´å¤šæ™ºèƒ½ä½“å¼€å‘å¥—ä»¶ ä¸­é—´ä»¶ã€å­æ™ºèƒ½ä½“ï¼Œæ›´ç®€æ´ï¼Œæ›´æ˜“ä¸Šæ‰‹ã€‚


&lt;div align=&quot;center&quot;&gt;
  &lt;!-- è§†é¢‘ç¼©ç•¥å›¾ --&gt;
&lt;img width=&quot;4420&quot; height=&quot;2510&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/76d58c8f-e4ef-4373-8ab6-7c80da568910&quot; /&gt;



  &lt;br&gt;
  &lt;img width=&quot;5369&quot; height=&quot;2934&quot; alt=&quot;222&quot; src=&quot;https://github.com/user-attachments/assets/839978d4-dcb8-47bd-a629-2912d2867e7e&quot; /&gt;

  &lt;br&gt;
  &lt;img width=&quot;5369&quot; height=&quot;2934&quot; alt=&quot;333&quot; src=&quot;https://github.com/user-attachments/assets/6387abce-45ab-4cd2-bd2f-e478d59a145f&quot; /&gt;

  &lt;br&gt;
  &lt;img width=&quot;5369&quot; height=&quot;2934&quot; alt=&quot;444&quot; src=&quot;https://github.com/user-attachments/assets/aff737c4-4b58-4b2e-b7aa-b0ff92d84e9b&quot; /&gt;

  &lt;br&gt;
  &lt;img width=&quot;5369&quot; height=&quot;2934&quot; alt=&quot;555&quot; src=&quot;https://github.com/user-attachments/assets/3fda0e48-ff6e-4b35-88b2-f7f7bc99a436&quot; /&gt;

  &lt;br&gt;
  &lt;img width=&quot;5369&quot; height=&quot;2934&quot; alt=&quot;666&quot; src=&quot;https://github.com/user-attachments/assets/8fdf0407-056d-40e2-949c-8dd4247dc59d&quot; /&gt;


&lt;/div&gt;


## å‚ä¸è´¡çŒ®

æ„Ÿè°¢æ‰€æœ‰è´¡çŒ®è€…çš„æ”¯æŒï¼

&lt;a href=&quot;https://github.com/xerrors/Yuxi-Know/contributors&quot;&gt;
    &lt;img src=&quot;https://contributors.nn.ci/api?repo=xerrors/Yuxi-Know&quot; alt=&quot;è´¡çŒ®è€…åå•&quot;&gt;
&lt;/a&gt;


## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=xerrors/Yuxi-Know)](https://star-history.com/#xerrors/Yuxi-Know)

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

---

&lt;div align=&quot;center&quot;&gt;

**å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ä¸è¦å¿˜è®°ç»™æˆ‘ä»¬ä¸€ä¸ª â­ï¸**

[æŠ¥å‘Šé—®é¢˜](https://github.com/xerrors/Yuxi-Know/issues) | [åŠŸèƒ½è¯·æ±‚](https://github.com/xerrors/Yuxi-Know/issues) | [è®¨è®º](https://github.com/xerrors/Yuxi-Know/discussions)

&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Shubhamsaboo/awesome-llm-apps]]></title>
            <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
            <guid>https://github.com/Shubhamsaboo/awesome-llm-apps</guid>
            <pubDate>Sun, 28 Dec 2025 00:05:33 GMT</pubDate>
            <description><![CDATA[Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Shubhamsaboo/awesome-llm-apps">Shubhamsaboo/awesome-llm-apps</a></h1>
            <p>Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p>
            <p>Language: Python</p>
            <p>Stars: 84,491</p>
            <p>Forks: 12,010</p>
            <p>Stars today: 143 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;http://www.theunwindai.com&quot;&gt;
    &lt;img src=&quot;docs/banner/unwind_black.png&quot; width=&quot;900px&quot; alt=&quot;Unwind AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.linkedin.com/in/shubhamsaboo/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;style=flat-square&quot; alt=&quot;LinkedIn&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/Saboo_Shubham_&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/Shubham_Saboo&quot; alt=&quot;Twitter&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de&quot;&gt;Deutsch&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es&quot;&gt;EspaÃ±ol&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr&quot;&gt;franÃ§ais&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja&quot;&gt;æ—¥æœ¬èª&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko&quot;&gt;í•œêµ­ì–´&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt&quot;&gt;PortuguÃªs&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru&quot;&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh&quot;&gt;ä¸­æ–‡&lt;/a&gt;
&lt;/p&gt;

&lt;hr/&gt;

# ğŸŒŸ Awesome LLM Apps

A curated collection of **Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.** This repository features LLM apps that use models from &lt;img src=&quot;https://cdn.simpleicons.org/openai&quot;  alt=&quot;openai logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**OpenAI** , &lt;img src=&quot;https://cdn.simpleicons.org/anthropic&quot;  alt=&quot;anthropic logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**Anthropic**, &lt;img src=&quot;https://cdn.simpleicons.org/googlegemini&quot;  alt=&quot;google logo&quot; width=&quot;25&quot; height=&quot;18&quot;&gt;**Google**, &lt;img src=&quot;https://cdn.simpleicons.org/x&quot;  alt=&quot;X logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**xAI** and open-source models like &lt;img src=&quot;https://cdn.simpleicons.org/alibabacloud&quot;  alt=&quot;alibaba logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**Qwen** or  &lt;img src=&quot;https://cdn.simpleicons.org/meta&quot;  alt=&quot;meta logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**Llama** that you can run locally on your computer.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/9876&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/9876&quot; alt=&quot;Shubhamsaboo%2Fawesome-llm-apps | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## ğŸ¤” Why Awesome LLM Apps?

- ğŸ’¡ Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.
- ğŸ”¥ Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp; RAG.
- ğŸ“ Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.

## ğŸ™ Thanks to our sponsors

&lt;table align=&quot;center&quot; cellpadding=&quot;16&quot; cellspacing=&quot;12&quot;&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://tsdb.co/shubham-gh&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;Tiger Data&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsors/tigerdata.png&quot; alt=&quot;Tiger Data&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://tsdb.co/shubham-gh&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        Tiger Data MCP
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://github.com/speechmatics/speechmatics-academy&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;Speechmatics&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsors/speechmatics.png&quot; alt=&quot;Speechmatics&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://github.com/speechmatics/speechmatics-academy&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        Speechmatics
      &lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://okara.ai/?utm_source=oss&amp;utm_medium=sponsorship&amp;utm_campaign=awesome-llm-apps&quot; title=&quot;Okara&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsors/okara.png&quot; alt=&quot;Okara&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://okara.ai/?utm_source=oss&amp;utm_medium=sponsorship&amp;utm_campaign=awesome-llm-apps&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        Okara AI
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://sponsorunwindai.com/&quot; title=&quot;Become a Sponsor&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsor_awesome_llm_apps.png&quot; alt=&quot;Become a Sponsor&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://sponsorunwindai.com/&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        Become a Sponsor
      &lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## ğŸ“‚ Featured AI Projects

### AI Agents

### ğŸŒ± Starter AI Agents

*   [ğŸ™ï¸ AI Blog to Podcast Agent](starter_ai_agents/ai_blog_to_podcast_agent/)
*   [â¤ï¸â€ğŸ©¹ AI Breakup Recovery Agent](starter_ai_agents/ai_breakup_recovery_agent/)
*   [ğŸ“Š AI Data Analysis Agent](starter_ai_agents/ai_data_analysis_agent/)
*   [ğŸ©» AI Medical Imaging Agent](starter_ai_agents/ai_medical_imaging_agent/)
*   [ğŸ˜‚ AI Meme Generator Agent (Browser)](starter_ai_agents/ai_meme_generator_agent_browseruse/)
*   [ğŸµ AI Music Generator Agent](starter_ai_agents/ai_music_generator_agent/)
*   [ğŸ›« AI Travel Agent (Local &amp; Cloud)](starter_ai_agents/ai_travel_agent/)
*   [âœ¨ Gemini Multimodal Agent](starter_ai_agents/gemini_multimodal_agent_demo/)
*   [ğŸ”„ Mixture of Agents](starter_ai_agents/mixture_of_agents/)
*   [ğŸ“Š xAI Finance Agent](starter_ai_agents/xai_finance_agent/)
*   [ğŸ” OpenAI Research Agent](starter_ai_agents/opeani_research_agent/)
*   [ğŸ•¸ï¸ Web Scraping AI Agent (Local &amp; Cloud SDK)](starter_ai_agents/web_scrapping_ai_agent/)

### ğŸš€ Advanced AI Agents
*   [ğŸšï¸ ğŸŒ AI Home Renovation Agent with Nano Banana](advanced_ai_agents/multi_agent_apps/ai_home_renovation_agent)
*   [ğŸ” AI Deep Research Agent](advanced_ai_agents/single_agent_apps/ai_deep_research_agent/)
*   [ğŸ¤ AI Consultant Agent](advanced_ai_agents/single_agent_apps/ai_consultant_agent)
*   [ğŸ—ï¸ AI System Architect Agent](advanced_ai_agents/single_agent_apps/ai_system_architect_r1/)
*   [ğŸ’° AI Financial Coach Agent](advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/)
*   [ğŸ¬ AI Movie Production Agent](advanced_ai_agents/single_agent_apps/ai_movie_production_agent/)
*   [ğŸ“ˆ AI Investment Agent](advanced_ai_agents/single_agent_apps/ai_investment_agent/)
*   [ğŸ‹ï¸â€â™‚ï¸ AI Health &amp; Fitness Agent](advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/)
*   [ğŸš€ AI Product Launch Intelligence Agent](advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent)
*   [ğŸ—ï¸ AI Journalist Agent](advanced_ai_agents/single_agent_apps/ai_journalist_agent/)
*   [ğŸ§  AI Mental Wellbeing Agent](advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/)
*   [ğŸ“‘ AI Meeting Agent](advanced_ai_agents/single_agent_apps/ai_meeting_agent/)
*   [ğŸ§¬ AI Self-Evolving Agent](advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/)
*   [ğŸ§ AI Social Media News and Podcast Agent](advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/)

### ğŸ® Autonomous Game Playing Agents

*   [ğŸ® AI 3D Pygame Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/)
*   [â™œ AI Chess Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/)
*   [ğŸ² AI Tic-Tac-Toe Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/)

### ğŸ¤ Multi-agent Teams

*   [ğŸ§² AI Competitor Intelligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/)
*   [ğŸ’² AI Finance Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/)
*   [ğŸ¨ AI Game Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/)
*   [ğŸ‘¨â€âš–ï¸ AI Legal Agent Team (Cloud &amp; Local)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/)
*   [ğŸ’¼ AI Recruitment Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/)
*   [ğŸ  AI Real Estate Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_real_estate_agent_team)
*   [ğŸ‘¨â€ğŸ’¼ AI Services Agency (CrewAI)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/)
*   [ğŸ‘¨â€ğŸ« AI Teaching Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/)
*   [ğŸ’» Multimodal Coding Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/)
*   [âœ¨ Multimodal Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/)
*   [ğŸ¨ ğŸŒ Multimodal UI/UX Feedback Agent Team with Nano Banana](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_uiux_feedback_agent_team/)
*   [ğŸŒ AI Travel Planner Agent Team](/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/)

### ğŸ—£ï¸ Voice AI Agents

*   [ğŸ—£ï¸ AI Audio Tour Agent](voice_ai_agents/ai_audio_tour_agent/)
*   [ğŸ“ Customer Support Voice Agent](voice_ai_agents/customer_support_voice_agent/)
*   [ğŸ”Š Voice RAG Agent (OpenAI SDK)](voice_ai_agents/voice_rag_openaisdk/)


### &lt;img src=&quot;https://cdn.simpleicons.org/modelcontextprotocol&quot;  alt=&quot;mcp logo&quot; width=&quot;25&quot; height=&quot;20&quot;&gt; MCP AI Agents 

*   [â™¾ï¸ Browser MCP Agent](mcp_ai_agents/browser_mcp_agent/)
*   [ğŸ™ GitHub MCP Agent](mcp_ai_agents/github_mcp_agent/)
*   [ğŸ“‘ Notion MCP Agent](mcp_ai_agents/notion_mcp_agent) 
*   [ğŸŒ AI Travel Planner MCP Agent](mcp_ai_agents/ai_travel_planner_mcp_agent_team)

### ğŸ“€ RAG (Retrieval Augmented Generation)
*   [ğŸ”¥ Agentic RAG with Embedding Gemma](rag_tutorials/agentic_rag_embedding_gemma)
*   [ğŸ§ Agentic RAG with Reasoning](rag_tutorials/agentic_rag_with_reasoning/)
*   [ğŸ“° AI Blog Search (RAG)](rag_tutorials/ai_blog_search/)
*   [ğŸ” Autonomous RAG](rag_tutorials/autonomous_rag/)
*   [ğŸ”„ Contextual AI RAG Agent](rag_tutorials/contextualai_rag_agent/)
*   [ğŸ”„ Corrective RAG (CRAG)](rag_tutorials/corrective_rag/)
*   [ğŸ‹ Deepseek Local RAG Agent](rag_tutorials/deepseek_local_rag_agent/)
*   [ğŸ¤” Gemini Agentic RAG](rag_tutorials/gemini_agentic_rag/)
*   [ğŸ‘€ Hybrid Search RAG (Cloud)](rag_tutorials/hybrid_search_rag/)
*   [ğŸ”„ Llama 3.1 Local RAG](rag_tutorials/llama3.1_local_rag/)
*   [ğŸ–¥ï¸ Local Hybrid Search RAG](rag_tutorials/local_hybrid_search_rag/)
*   [ğŸ¦™ Local RAG Agent](rag_tutorials/local_rag_agent/)
*   [ğŸ§© RAG-as-a-Service](rag_tutorials/rag-as-a-service/)
*   [âœ¨ RAG Agent with Cohere](rag_tutorials/rag_agent_cohere/)
*   [â›“ï¸ Basic RAG Chain](rag_tutorials/rag_chain/)
*   [ğŸ“  RAG with Database Routing](rag_tutorials/rag_database_routing/)
*   [ğŸ–¼ï¸ Vision RAG](rag_tutorials/vision_rag/)

### ğŸ’¾ LLM Apps with Memory Tutorials

*   [ğŸ’¾ AI ArXiv Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/)
*   [ğŸ›©ï¸ AI Travel Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/)
*   [ğŸ’¬ Llama3 Stateful Chat](advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/)
*   [ğŸ“ LLM App with Personalized Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/)
*   [ğŸ—„ï¸ Local ChatGPT Clone with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/)
*   [ğŸ§  Multi-LLM Application with Shared Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/)


### ğŸ’¬ Chat with X Tutorials

*   [ğŸ’¬ Chat with GitHub (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_github/)
*   [ğŸ“¨ Chat with Gmail](advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/)
*   [ğŸ“„ Chat with PDF (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/)
*   [ğŸ“š Chat with Research Papers (ArXiv) (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/)
*   [ğŸ“ Chat with Substack](advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/)
*   [ğŸ“½ï¸ Chat with YouTube Videos](advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/)

### ğŸ¯ LLM Optimization Tools

*   [ğŸ¯ Toonify Token Optimization](advanced_llm_apps/llm_optimization_tools/toonify_token_optimization/) - Reduce LLM API costs by 30-60% using TOON format

### ğŸ”§ LLM Fine-tuning Tutorials

* &lt;img src=&quot;https://cdn.simpleicons.org/google&quot;  alt=&quot;google logo&quot; width=&quot;20&quot; height=&quot;15&quot;&gt; [Gemma 3 Fine-tuning](advanced_llm_apps/llm_finetuning_tutorials/gemma3_finetuning/)
* &lt;img src=&quot;https://cdn.simpleicons.org/meta&quot;  alt=&quot;meta logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt; [Llama 3.2 Fine-tuning](advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/)


### ğŸ§‘â€ğŸ« AI Agent Framework Crash Course

&lt;img src=&quot;https://cdn.simpleicons.org/google&quot;  alt=&quot;google logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt; [Google ADK Crash Course](ai_agent_framework_crash_course/google_adk_crash_course/)
  - Starter agent; modelâ€‘agnostic (OpenAI, Claude)
  - Structured outputs (Pydantic)
  - Tools: builtâ€‘in, function, thirdâ€‘party, MCP tools
  - Memory; callbacks; Plugins
  - Simple multiâ€‘agent; Multiâ€‘agent patterns

&lt;img src=&quot;https://cdn.simpleicons.org/openai&quot;  alt=&quot;openai logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt; [OpenAI Agents SDK Crash Course](ai_agent_framework_crash_course/openai_sdk_crash_course/)
  - Starter agent; function calling; structured outputs
  - Tools: builtâ€‘in, function, thirdâ€‘party integrations
  - Memory; callbacks; evaluation
  - Multiâ€‘agent patterns; agent handoffs
  - Swarm orchestration; routing logic

## ğŸš€ Getting Started

1. **Clone the repository** 

    ```bash 
    git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
    ```

2. **Navigate to the desired project directory**

    ```bash 
    cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
    ```

3. **Install the required dependencies**

    ```bash
    pip install -r requirements.txt
    ```

4. **Follow the project-specific instructions** in each project&#039;s `README.md` file to set up and run the app.


### &lt;img src=&quot;https://cdn.simpleicons.org/github&quot;  alt=&quot;github logo&quot; width=&quot;25&quot; height=&quot;20&quot;&gt; Thank You, Community, for the Support! ğŸ™

[![Star History Chart](https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;type=Date)](https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;Date)

ğŸŒŸ **Donâ€™t miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[rendercv/rendercv]]></title>
            <link>https://github.com/rendercv/rendercv</link>
            <guid>https://github.com/rendercv/rendercv</guid>
            <pubDate>Sun, 28 Dec 2025 00:05:32 GMT</pubDate>
            <description><![CDATA[CV/resume generator for academics and engineers, YAML to PDF]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/rendercv/rendercv">rendercv/rendercv</a></h1>
            <p>CV/resume generator for academics and engineers, YAML to PDF</p>
            <p>Language: Python</p>
            <p>Stars: 12,648</p>
            <p>Forks: 836</p>
            <p>Stars today: 634 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;h1&gt;RenderCV&lt;/h1&gt;

_CV/resume generator for academics and engineers_

[![test](https://github.com/rendercv/rendercv/actions/workflows/test.yaml/badge.svg?branch=main)](https://github.com/rendercv/rendercv/actions/workflows/test.yaml)
[![coverage](https://coverage-badge.samuelcolvin.workers.dev/rendercv/rendercv.svg)](https://coverage-badge.samuelcolvin.workers.dev/redirect/rendercv/rendercv)
[![docs](&lt;https://img.shields.io/badge/docs-mkdocs-rgb(0%2C79%2C144)&gt;)](https://docs.rendercv.com)
[![pypi-version](&lt;https://img.shields.io/pypi/v/rendercv?label=PyPI%20version&amp;color=rgb(0%2C79%2C144)&gt;)](https://pypi.python.org/pypi/rendercv)
[![pypi-downloads](&lt;https://img.shields.io/pepy/dt/rendercv?label=PyPI%20downloads&amp;color=rgb(0%2C%2079%2C%20144)&gt;)](https://pypistats.org/packages/rendercv)

&lt;/div&gt;

Write your CV or resume as YAML, then run RenderCV,

```bash
rendercv render John_Doe_CV.yaml
```

and get a PDF with perfect typography. No template wrestling. No broken layouts. Consistent spacing, every time.

With RenderCV, you can:

- Version-control your CV â€” it&#039;s just text.
- Focus on content â€” don&#039;t wory about the formatting.
- Get perfect typography â€” pixel-perfect alignment and spacing, handled for you.

A YAML file like this:

```yaml
cv:
  name: John Doe
  location: San Francisco, CA
  email: john.doe@email.com
  website: https://rendercv.com/
  social_networks:
    - network: LinkedIn
      username: rendercv
    - network: GitHub
      username: rendercv
  sections:
    Welcome to RenderCV:
      - RenderCV reads a CV written in a YAML file, and generates a PDF with professional typography.
      - See the [documentation](https://docs.rendercv.com) for more details.
    education:
      - institution: Princeton University
        area: Computer Science
        degree: PhD
        date:
        start_date: 2018-09
        end_date: 2023-05
        location: Princeton, NJ
        summary:
        highlights:
          - &quot;Thesis: Efficient Neural Architecture Search for Resource-Constrained Deployment&quot;
          - &quot;Advisor: Prof. Sanjeev Arora&quot;
          - NSF Graduate Research Fellowship, Siebel Scholar (Class of 2022)
    ...
```

becomes one of these PDFs. Click on the images to preview.

| [![Classic Theme Example of RenderCV](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/classic.png)](https://github.com/rendercv/rendercv/blob/main/examples/John_Doe_ClassicTheme_CV.pdf)    | [![Engineeringresumes Theme Example of RenderCV](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/engineeringresumes.png)](https://github.com/rendercv/rendercv/blob/main/examples/John_Doe_EngineeringresumesTheme_CV.pdf) | [![Sb2nov Theme Example of RenderCV](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/sb2nov.png)](https://github.com/rendercv/rendercv/blob/main/examples/John_Doe_Sb2novTheme_CV.pdf) |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [![Moderncv Theme Example of RenderCV](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/moderncv.png)](https://github.com/rendercv/rendercv/blob/main/examples/John_Doe_ModerncvTheme_CV.pdf) | [![Engineeringclassic Theme Example of RenderCV](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/engineeringclassic.png)](https://github.com/rendercv/rendercv/blob/main/examples/John_Doe_EngineeringclassicTheme_CV.pdf) | ![Custom themes can be added.](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/customtheme.png)                                                                                        |


## JSON Schema

RenderCV&#039;s JSON Schema lets you fill out the YAML interactively, with autocompletion and inline documentation.

![JSON Schema of RenderCV](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/json_schema.gif)


## Extensive Design Options

You have full control over every detail.

```yaml
design:
  theme: classic
  page:
    size: us-letter
    top_margin: 0.7in
    bottom_margin: 0.7in
    left_margin: 0.7in
    right_margin: 0.7in
    show_footer: true
    show_top_note: true
  colors:
    body: rgb(0, 0, 0)
    name: rgb(0, 79, 144)
    headline: rgb(0, 79, 144)
    connections: rgb(0, 79, 144)
    section_titles: rgb(0, 79, 144)
    links: rgb(0, 79, 144)
    footer: rgb(128, 128, 128)
    top_note: rgb(128, 128, 128)
  typography:
    line_spacing: 0.6em
    alignment: justified
    date_and_location_column_alignment: right
    font_family: Source Sans 3
  # ...and much more
```

![Design Options of RenderCV](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/design_options.gif)

&gt; [!TIP]
&gt; Want to set up a live preview environment like the one shown above? See [how to set up VS Code for RenderCV](https://docs.rendercv.com/user_guide/how_to/set_up_vs_code_for_rendercv).

## Strict Validation

No surprises. If something&#039;s wrong, you&#039;ll know exactly what and where. If it&#039;s valid, you get a perfect PDF.

![Strict Validation Feature of RenderCV](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/validation.gif)


## Any Language

Fill out the locale field for your language.

```yaml
locale:
  language: english
  last_updated: Last updated in
  month: month
  months: months
  year: year
  years: years
  present: present
  month_abbreviations:
    - Jan
    - Feb
    - Mar
  ...
```

## Get Started

Install RenderCV (Requires Python 3.12+):

```
pip install &quot;rendercv[full]&quot;
```

Create a new CV yaml file:

```
rendercv new &quot;John Doe&quot;
```

Edit the YAML, then render:

```
rendercv render &quot;John_Doe_CV.yaml&quot;
```

For more details, see the [user guide](https://docs.rendercv.com/user_guide/).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[topoteretes/cognee]]></title>
            <link>https://github.com/topoteretes/cognee</link>
            <guid>https://github.com/topoteretes/cognee</guid>
            <pubDate>Sun, 28 Dec 2025 00:05:31 GMT</pubDate>
            <description><![CDATA[Memory for AI Agents in 6 lines of code]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/topoteretes/cognee">topoteretes/cognee</a></h1>
            <p>Memory for AI Agents in 6 lines of code</p>
            <p>Language: Python</p>
            <p>Stars: 10,611</p>
            <p>Forks: 977</p>
            <p>Stars today: 75 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/topoteretes/cognee&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/dev/assets/cognee-logo-transparent.png&quot; alt=&quot;Cognee Logo&quot; height=&quot;60&quot;&gt;
  &lt;/a&gt;

  &lt;br /&gt;

  Cognee - Accurate and Persistent AI Memory

  &lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=1bezuvLwJmw&amp;t=2s&quot;&gt;Demo&lt;/a&gt;
  .
  &lt;a href=&quot;https://docs.cognee.ai/&quot;&gt;Docs&lt;/a&gt;
  .
  &lt;a href=&quot;https://cognee.ai&quot;&gt;Learn More&lt;/a&gt;
  Â·
  &lt;a href=&quot;https://discord.gg/NQPKmU5CCg&quot;&gt;Join Discord&lt;/a&gt;
  Â·
  &lt;a href=&quot;https://www.reddit.com/r/AIMemory/&quot;&gt;Join r/AIMemory&lt;/a&gt;
  .
  &lt;a href=&quot;https://github.com/topoteretes/cognee-community&quot;&gt;Community Plugins &amp; Add-ons&lt;/a&gt;
  &lt;/p&gt;


  [![GitHub forks](https://img.shields.io/github/forks/topoteretes/cognee.svg?style=social&amp;label=Fork&amp;maxAge=2592000)](https://GitHub.com/topoteretes/cognee/network/)
  [![GitHub stars](https://img.shields.io/github/stars/topoteretes/cognee.svg?style=social&amp;label=Star&amp;maxAge=2592000)](https://GitHub.com/topoteretes/cognee/stargazers/)
  [![GitHub commits](https://badgen.net/github/commits/topoteretes/cognee)](https://GitHub.com/topoteretes/cognee/commit/)
  [![GitHub tag](https://badgen.net/github/tag/topoteretes/cognee)](https://github.com/topoteretes/cognee/tags/)
  [![Downloads](https://static.pepy.tech/badge/cognee)](https://pepy.tech/project/cognee)
  [![License](https://img.shields.io/github/license/topoteretes/cognee?colorA=00C586&amp;colorB=000000)](https://github.com/topoteretes/cognee/blob/main/LICENSE)
  [![Contributors](https://img.shields.io/github/contributors/topoteretes/cognee?colorA=00C586&amp;colorB=000000)](https://github.com/topoteretes/cognee/graphs/contributors)
  &lt;a href=&quot;https://github.com/sponsors/topoteretes&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Sponsor-â¤ï¸-ff69b4.svg&quot; alt=&quot;Sponsor&quot;&gt;&lt;/a&gt;

&lt;p&gt;
  &lt;a href=&quot;https://www.producthunt.com/posts/cognee?embed=true&amp;utm_source=badge-top-post-badge&amp;utm_medium=badge&amp;utm_souce=badge-cognee&quot; target=&quot;_blank&quot; style=&quot;display:inline-block; margin-right:10px;&quot;&gt;
    &lt;img src=&quot;https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=946346&amp;theme=light&amp;period=daily&amp;t=1744472480704&quot; alt=&quot;cognee - Memory&amp;#0032;for&amp;#0032;AI&amp;#0032;Agents&amp;#0032;&amp;#0032;in&amp;#0032;5&amp;#0032;lines&amp;#0032;of&amp;#0032;code | Product Hunt&quot; width=&quot;250&quot; height=&quot;54&quot; /&gt;
  &lt;/a&gt;

  &lt;a href=&quot;https://trendshift.io/repositories/13955&quot; target=&quot;_blank&quot; style=&quot;display:inline-block;&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/13955&quot; alt=&quot;topoteretes%2Fcognee | Trendshift&quot; width=&quot;250&quot; height=&quot;55&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

Use your data to build personalized and dynamic memory for AI Agents. Cognee lets you replace RAG with scalable and modular ECL (Extract, Cognify, Load) pipelines.

  &lt;p align=&quot;center&quot;&gt;
  ğŸŒ Available Languages
  :
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://www.readme-i18n.com/topoteretes/cognee?lang=de&quot;&gt;Deutsch&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/topoteretes/cognee?lang=es&quot;&gt;EspaÃ±ol&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/topoteretes/cognee?lang=fr&quot;&gt;FranÃ§ais&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/topoteretes/cognee?lang=ja&quot;&gt;æ—¥æœ¬èª&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/topoteretes/cognee?lang=ko&quot;&gt;í•œêµ­ì–´&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/topoteretes/cognee?lang=pt&quot;&gt;PortuguÃªs&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/topoteretes/cognee?lang=ru&quot;&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/topoteretes/cognee?lang=zh&quot;&gt;ä¸­æ–‡&lt;/a&gt;
  &lt;/p&gt;


&lt;div style=&quot;text-align: center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_benefits.png&quot; alt=&quot;Why cognee?&quot; width=&quot;50%&quot; /&gt;
&lt;/div&gt;
&lt;/div&gt;

## About Cognee

Cognee is an open-source tool and platform that transforms your raw data into persistent and dynamic AI memory for Agents. It combines vector search with graph databases to make your documents both searchable by meaning and connected by relationships. 

You can use Cognee in two ways:

1. [Self-host Cognee Open Source](https://docs.cognee.ai/getting-started/installation), which stores all data locally by default.
2. [Connect to Cognee Cloud](https://platform.cognee.ai/), and get the same OSS stack on managed infrastructure for easier development and productionization. 

### Cognee Open Source (self-hosted):

- Interconnects any type of data â€” including past conversations, files, images, and audio transcriptions
- Replaces traditional RAG systems with a unified memory layer built on graphs and vectors
- Reduces developer effort and infrastructure cost while improving quality and precision
- Provides Pythonic data pipelines for ingestion from 30+ data sources
- Offers high customizability through user-defined tasks, modular pipelines, and built-in search endpoints

### Cognee Cloud (managed):
- Hosted web UI dashboard 
- Automatic version updates 
- Resource usage analytics
- GDPR compliant, enterprise-grade security

## Basic Usage &amp; Feature Guide

To learn more, [check out this short, end-to-end Colab walkthrough](https://colab.research.google.com/drive/12Vi9zID-M3fpKpKiaqDBvkk98ElkRPWy?usp=sharing) of Cognee&#039;s core features.

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/12Vi9zID-M3fpKpKiaqDBvkk98ElkRPWy?usp=sharing)

## Quickstart

Letâ€™s try Cognee in just a few lines of code. For detailed setup and configuration, see the [Cognee Docs](https://docs.cognee.ai/getting-started/installation#environment-configuration).

### Prerequisites

- Python 3.10 to 3.13

### Step 1: Install Cognee

You can install Cognee with **pip**, **poetry**, **uv**, or your preferred Python package manager.

```bash
uv pip install cognee
```

### Step 2: Configure the LLM
```python
import os
os.environ[&quot;LLM_API_KEY&quot;] = &quot;YOUR OPENAI_API_KEY&quot;
```
Alternatively, create a `.env` file using our [template](https://github.com/topoteretes/cognee/blob/main/.env.template).

To integrate other LLM providers, see our [LLM Provider Documentation](https://docs.cognee.ai/setup-configuration/llm-providers).

### Step 3: Run the Pipeline

Cognee will take your documents, generate a knowledge graph from them and then query the graph based on combined relationships. 

Now, run a minimal pipeline:

```python
import cognee
import asyncio
from pprint import pprint


async def main():
    # Add text to cognee
    await cognee.add(&quot;Cognee turns documents into AI memory.&quot;)

    # Generate the knowledge graph
    await cognee.cognify()

    # Add memory algorithms to the graph
    await cognee.memify()

    # Query the knowledge graph
    results = await cognee.search(&quot;What does Cognee do?&quot;)

    # Display the results
    for result in results:
        pprint(result)


if __name__ == &#039;__main__&#039;:
    asyncio.run(main())

```

As you can see, the output is generated from the document we previously stored in Cognee:

```bash
  Cognee turns documents into AI memory.
```

### Use the Cognee CLI 

As an alternative, you can get started with these essential commands:

```bash
cognee-cli add &quot;Cognee turns documents into AI memory.&quot;

cognee-cli cognify

cognee-cli search &quot;What does Cognee do?&quot;
cognee-cli delete --all

```

To open the local UI, run:
```bash
cognee-cli -ui
```

## Demos &amp; Examples

See Cognee in action:

### Persistent Agent Memory

[Cognee Memory for LangGraph Agents](https://github.com/user-attachments/assets/e113b628-7212-4a2b-b288-0be39a93a1c3)

### Simple GraphRAG

[Watch Demo](https://github.com/user-attachments/assets/f2186b2e-305a-42b0-9c2d-9f4473f15df8)

### Cognee with Ollama

[Watch Demo](https://github.com/user-attachments/assets/39672858-f774-4136-b957-1e2de67b8981)


## Community &amp; Support

### Contributing
We welcome contributions from the community! Your input helps make Cognee better for everyone. See [`CONTRIBUTING.md`](CONTRIBUTING.md) to get started.

### Code of Conduct

We&#039;re committed to fostering an inclusive and respectful community. Read our [Code of Conduct](https://github.com/topoteretes/cognee/blob/main/CODE_OF_CONDUCT.md) for guidelines.

## Research &amp; Citation

We recently published a research paper on optimizing knowledge graphs for LLM reasoning:

```bibtex
@misc{markovic2025optimizinginterfaceknowledgegraphs,
      title={Optimizing the Interface Between Knowledge Graphs and LLMs for Complex Reasoning},
      author={Vasilije Markovic and Lazar Obradovic and Laszlo Hajdu and Jovan Pavlovic},
      year={2025},
      eprint={2505.24478},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2505.24478},
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ModelTC/LightX2V]]></title>
            <link>https://github.com/ModelTC/LightX2V</link>
            <guid>https://github.com/ModelTC/LightX2V</guid>
            <pubDate>Sun, 28 Dec 2025 00:05:30 GMT</pubDate>
            <description><![CDATA[Light Video Generation Inference Framework]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ModelTC/LightX2V">ModelTC/LightX2V</a></h1>
            <p>Light Video Generation Inference Framework</p>
            <p>Language: Python</p>
            <p>Stars: 1,569</p>
            <p>Forks: 109</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; style=&quot;font-family: charter;&quot;&gt;
  &lt;h1&gt;âš¡ï¸ LightX2V:&lt;br&gt; Light Video Generation Inference Framework&lt;/h1&gt;

&lt;img alt=&quot;logo&quot; src=&quot;assets/img_lightx2v.png&quot; width=75%&gt;&lt;/img&gt;

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/ModelTC/lightx2v)
[![Doc](https://img.shields.io/badge/docs-English-99cc2)](https://lightx2v-en.readthedocs.io/en/latest)
[![Doc](https://img.shields.io/badge/æ–‡æ¡£-ä¸­æ–‡-99cc2)](https://lightx2v-zhcn.readthedocs.io/zh-cn/latest)
[![Papers](https://img.shields.io/badge/è®ºæ–‡é›†-ä¸­æ–‡-99cc2)](https://lightx2v-papers-zhcn.readthedocs.io/zh-cn/latest)
[![Docker](https://img.shields.io/badge/Docker-2496ED?style=flat&amp;logo=docker&amp;logoColor=white)](https://hub.docker.com/r/lightx2v/lightx2v/tags)

**\[ English | [ä¸­æ–‡](README_zh.md) \]**

&lt;/div&gt;

--------------------------------------------------------------------------------

**LightX2V** is an advanced lightweight image/video generation inference framework engineered to deliver efficient, high-performance image/video synthesis solutions. This unified platform integrates multiple state-of-the-art image/video generation techniques, supporting diverse generation tasks including text-to-video (T2V), image-to-video (I2V), text-to-image (T2I), image-editing (I2I). **X2V represents the transformation of different input modalities (X, such as text or images) into vision output (Vision)**.

&gt; ğŸŒ **Try it online now!** Experience LightX2V without installation: **[LightX2V Online Service](https://x2v.light-ai.top/login)** - Free, lightweight, and fast AI digital human video generation platform.

&gt; ğŸ‘‹ **Join us on [WeChat](https://light-ai.top/community.html).**

## ğŸ§¾ Community Code Contribution Guidelines

Before submitting, please ensure that the code format conforms to the project standard. You can use the following execution command to ensure the consistency of project code format.

```bash
pip install ruff pre-commit
pre-commit run --all-files
```

Besides the contributions from the LightX2V team, we have received contributions from some community developers, including but not limited to:

- [triple-Mu](https://github.com/triple-Mu)
- [vivienfanghuagood](https://github.com/vivienfanghuagood)
- [yeahdongcn](https://github.com/yeahdongcn)

## :fire: Latest News

- **December 27, 2025:** ğŸš€ Supported deployment on MThreads MUSA.

- **December 25, 2025:** ğŸš€ Supported deployment on AMD ROCm and Ascend 910B.

- **December 23, 2025:** ğŸš€ We support the [Qwen-Image-Edit-2511](https://huggingface.co/Qwen/Qwen-Image-Edit-2511) image editing model since Day 0. On a single H100 GPU, LightX2V delivers approximately 1.4Ã— speedup. We support for CFG parallelism, Ulysses parallelism, and efficient offloading technologies. Our [HuggingFace](https://huggingface.co/lightx2v/Qwen-Image-Edit-2511-Lightning) has been updated with CFG / step-distilled LoRA and FP8 weights. Usage examples can be found in the [Python scripts](https://github.com/ModelTC/LightX2V/tree/main/examples/qwen_image). Combined with LightX2V, 4-step CFG / step distillation, and the FP8 model, the maximum acceleration can reach up to approximately 42Ã—. Feel free to try [LightX2V Online Service](https://x2v.light-ai.top/login) with *Image to Image* and *Qwen-Image-Edit-2511* model.

- **December 22, 2025:** ğŸš€ Added **Wan2.1 NVFP4 quantization-aware 4-step distilled models**; weights are available on HuggingFace: [Wan-NVFP4](https://huggingface.co/lightx2v/Wan-NVFP4).

- **December 15, 2025:** ğŸš€ Supported deployment on Hygon DCU.

- **December 4, 2025:** ğŸš€ Supported GGUF format model inference &amp; deployment on Cambricon MLU590/MetaX C500.

- **November 24, 2025:** ğŸš€ We released 4-step distilled models for HunyuanVideo-1.5! These models enable **ultra-fast 4-step inference** without CFG requirements, achieving approximately **25x speedup** compared to standard 50-step inference. Both base and FP8 quantized versions are now available: [Hy1.5-Distill-Models](https://huggingface.co/lightx2v/Hy1.5-Distill-Models).

- **November 21, 2025:** ğŸš€ We support the [HunyuanVideo-1.5](https://huggingface.co/tencent/HunyuanVideo-1.5) video generation model since Day 0. With the same number of GPUs, LightX2V can achieve a speed improvement of over 2 times and supports deployment on GPUs with lower memory (such as the 24GB RTX 4090). It also supports CFG/Ulysses parallelism, efficient offloading, TeaCache/MagCache technologies, and more. We will soon update more models on our [HuggingFace page](https://huggingface.co/lightx2v), including step distillation, VAE distillation, and other related models. Quantized models and lightweight VAE models are now available: [Hy1.5-Quantized-Models](https://huggingface.co/lightx2v/Hy1.5-Quantized-Models) for quantized inference, and [LightTAE for HunyuanVideo-1.5](https://huggingface.co/lightx2v/Autoencoders/blob/main/lighttaehy1_5.safetensors) for fast VAE decoding. Refer to [this](https://github.com/ModelTC/LightX2V/tree/main/scripts/hunyuan_video_15) for usage tutorials, or check out the [examples directory](https://github.com/ModelTC/LightX2V/tree/main/examples) for code examples.


## ğŸ† Performance Benchmarks (Updated on 2025.12.01)

### ğŸ“Š Cross-Framework Performance Comparison (H100)

| Framework | GPUs | Step Time | Speedup |
|-----------|---------|---------|---------|
| Diffusers | 1 | 9.77s/it | 1x |
| xDiT | 1 | 8.93s/it | 1.1x |
| FastVideo | 1 | 7.35s/it | 1.3x |
| SGL-Diffusion | 1 | 6.13s/it | 1.6x |
| **LightX2V** | 1 | **5.18s/it** | **1.9x** ğŸš€ |
| FastVideo | 8 | 2.94s/it | 1x |
| xDiT | 8 | 2.70s/it | 1.1x |
| SGL-Diffusion | 8 | 1.19s/it | 2.5x |
| **LightX2V** | 8 | **0.75s/it** | **3.9x** ğŸš€ |

### ğŸ“Š Cross-Framework Performance Comparison (RTX 4090D)

| Framework | GPUs | Step Time | Speedup |
|-----------|---------|---------|---------|
| Diffusers | 1 | 30.50s/it | 1x |
| FastVideo | 1 | 22.66s/it | 1.3x |
| xDiT | 1 | OOM | OOM |
| SGL-Diffusion | 1 | OOM | OOM |
| **LightX2V** | 1 | **20.26s/it** | **1.5x** ğŸš€ |
| FastVideo | 8 | 15.48s/it | 1x |
| xDiT | 8 | OOM | OOM |
| SGL-Diffusion | 8 | OOM | OOM |
| **LightX2V** | 8 | **4.75s/it** | **3.3x** ğŸš€ |

### ğŸ“Š LightX2V Performance Comparison

| Framework | GPU | Configuration | Step Time | Speedup |
|-----------|-----|---------------|-----------|---------------|
| **LightX2V** | H100 | 8 GPUs + cfg | 0.75s/it | 1x |
| **LightX2V** | H100 | 8 GPUs + no cfg | 0.39s/it | 1.9x |
| **LightX2V** | H100 | **8 GPUs + no cfg + fp8** | **0.35s/it** | **2.1x** ğŸš€ |
| **LightX2V** | 4090D | 8 GPUs + cfg | 4.75s/it | 1x |
| **LightX2V** | 4090D | 8 GPUs + no cfg | 3.13s/it | 1.5x |
| **LightX2V** | 4090D | **8 GPUs + no cfg + fp8** | **2.35s/it** | **2.0x** ğŸš€ |

**Note**: All the above performance data were tested on Wan2.1-I2V-14B-480P(40 steps, 81 frames). In addition, we also provide 4-step distilled models on the [HuggingFace page](https://huggingface.co/lightx2v).


## ğŸ’¡ Quick Start

For comprehensive usage instructions, please refer to our documentation: **[English Docs](https://lightx2v-en.readthedocs.io/en/latest/) | [ä¸­æ–‡æ–‡æ¡£](https://lightx2v-zhcn.readthedocs.io/zh-cn/latest/)**

**We highly recommend using the Docker environment, as it is the simplest and fastest way to set up the environment. For details, please refer to the Quick Start section in the documentation.**

### Installation from Git
```bash
pip install -v git+https://github.com/ModelTC/LightX2V.git
```

### Building from Source
```bash
git clone https://github.com/ModelTC/LightX2V.git
cd LightX2V
uv pip install -v . # pip install -v .
```

### (Optional) Install Attention/Quantize Operators
For attention operators installation, please refer to our documentation: **[English Docs](https://lightx2v-en.readthedocs.io/en/latest/getting_started/quickstart.html#step-4-install-attention-operators) | [ä¸­æ–‡æ–‡æ¡£](https://lightx2v-zhcn.readthedocs.io/zh-cn/latest/getting_started/quickstart.html#id9)**

### Usage Example

```python
# examples/wan/wan_i2v.py
&quot;&quot;&quot;
Wan2.2 image-to-video generation example.
This example demonstrates how to use LightX2V with Wan2.2 model for I2V generation.
&quot;&quot;&quot;

from lightx2v import LightX2VPipeline

# Initialize pipeline for Wan2.2 I2V task
# For wan2.1, use model_cls=&quot;wan2.1&quot;
pipe = LightX2VPipeline(
    model_path=&quot;/path/to/Wan2.2-I2V-A14B&quot;,
    model_cls=&quot;wan2.2_moe&quot;,
    task=&quot;i2v&quot;,
)

# Alternative: create generator from config JSON file
# pipe.create_generator(
#     config_json=&quot;configs/wan22/wan_moe_i2v.json&quot;
# )

# Enable offloading to significantly reduce VRAM usage with minimal speed impact
# Suitable for RTX 30/40/50 consumer GPUs
pipe.enable_offload(
    cpu_offload=True,
    offload_granularity=&quot;block&quot;,  # For Wan models, supports both &quot;block&quot; and &quot;phase&quot;
    text_encoder_offload=True,
    image_encoder_offload=False,
    vae_offload=False,
)

# Create generator manually with specified parameters
pipe.create_generator(
    attn_mode=&quot;sage_attn2&quot;,
    infer_steps=40,
    height=480,  # Can be set to 720 for higher resolution
    width=832,  # Can be set to 1280 for higher resolution
    num_frames=81,
    guidance_scale=[3.5, 3.5],  # For wan2.1, guidance_scale is a scalar (e.g., 5.0)
    sample_shift=5.0,
)

# Generation parameters
seed = 42
prompt = &quot;Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline&#039;s intricate details and the refreshing atmosphere of the seaside.&quot;
negative_prompt = &quot;é•œå¤´æ™ƒåŠ¨ï¼Œè‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œæ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°&quot;
image_path=&quot;/path/to/img_0.jpg&quot;
save_result_path = &quot;/path/to/save_results/output.mp4&quot;

# Generate video
pipe.generate(
    seed=seed,
    image_path=image_path,
    prompt=prompt,
    negative_prompt=negative_prompt,
    save_result_path=save_result_path,
)
```

**NVFP4 (quantization-aware 4-step) resources**
- Inference examples: `examples/wan/wan_i2v_nvfp4.py` (I2V) and `examples/wan/wan_t2v_nvfp4.py` (T2V).
- NVFP4 operator build/install guide: see `lightx2v_kernel/README.md`.

&gt; ğŸ’¡ **More Examples**: For more usage examples including quantization, offloading, caching, and other advanced configurations, please refer to the [examples directory](https://github.com/ModelTC/LightX2V/tree/main/examples).



## ğŸ¤– Supported Model Ecosystem

### Official Open-Source Models
- âœ… [HunyuanVideo-1.5](https://huggingface.co/tencent/HunyuanVideo-1.5)
- âœ… [Wan2.1 &amp; Wan2.2](https://huggingface.co/Wan-AI/)
- âœ… [Qwen-Image](https://huggingface.co/Qwen/Qwen-Image)
- âœ… [Qwen-Image-Edit](https://huggingface.co/spaces/Qwen/Qwen-Image-Edit)
- âœ… [Qwen-Image-Edit-2509](https://huggingface.co/Qwen/Qwen-Image-Edit-2509)
- âœ… [Qwen-Image-Edit-2511](https://huggingface.co/Qwen/Qwen-Image-Edit-2511)

### Quantized and Distilled Models/LoRAs (**ğŸš€ Recommended: 4-step inference**)
- âœ… [Wan2.1-Distill-Models](https://huggingface.co/lightx2v/Wan2.1-Distill-Models)
- âœ… [Wan2.2-Distill-Models](https://huggingface.co/lightx2v/Wan2.2-Distill-Models)
- âœ… [Wan2.1-Distill-Loras](https://huggingface.co/lightx2v/Wan2.1-Distill-Loras)
- âœ… [Wan2.2-Distill-Loras](https://huggingface.co/lightx2v/Wan2.2-Distill-Loras)
- âœ… [Wan2.1-Distill-NVFP4](https://huggingface.co/lightx2v/Wan-NVFP4)
- âœ… [Qwen-Image-Edit-2511-Lightning](https://huggingface.co/lightx2v/Qwen-Image-Edit-2511-Lightning)

### Lightweight Autoencoder Models (**ğŸš€ Recommended: fast inference &amp; low memory usage**)
- âœ… [Autoencoders](https://huggingface.co/lightx2v/Autoencoders)

### Autoregressive Models
- âœ… [Wan2.1-T2V-CausVid](https://huggingface.co/lightx2v/Wan2.1-T2V-14B-CausVid)
- âœ… [Self-Forcing](https://github.com/guandeh17/Self-Forcing)
- âœ… [Matrix-Game-2.0](https://huggingface.co/Skywork/Matrix-Game-2.0)

ğŸ”” Follow our [HuggingFace page](https://huggingface.co/lightx2v) for the latest model releases from our team.

ğŸ’¡ Refer to the [Model Structure Documentation](https://lightx2v-en.readthedocs.io/en/latest/getting_started/model_structure.html) to quickly get started with LightX2V

## ğŸš€ Frontend Interfaces

We provide multiple frontend interface deployment options:

- **ğŸ¨ Gradio Interface**: Clean and user-friendly web interface, perfect for quick experience and prototyping
  - ğŸ“– [Gradio Deployment Guide](https://lightx2v-en.readthedocs.io/en/latest/deploy_guides/deploy_gradio.html)
- **ğŸ¯ ComfyUI Interface**: Powerful node-based workflow interface, supporting complex video generation tasks
  - ğŸ“– [ComfyUI Deployment Guide](https://lightx2v-en.readthedocs.io/en/latest/deploy_guides/deploy_comfyui.html)
- **ğŸš€ Windows One-Click Deployment**: Convenient deployment solution designed for Windows users, featuring automatic environment configuration and intelligent parameter optimization
  - ğŸ“– [Windows One-Click Deployment Guide](https://lightx2v-en.readthedocs.io/en/latest/deploy_guides/deploy_local_windows.html)

**ğŸ’¡ Recommended Solutions**:
- **First-time Users**: We recommend the Windows one-click deployment solution
- **Advanced Users**: We recommend the ComfyUI interface for more customization options
- **Quick Experience**: The Gradio interface provides the most intuitive operation experience

## ğŸš€ Core Features

### ğŸ¯ **Ultimate Performance Optimization**
- **ğŸ”¥ SOTA Inference Speed**: Achieve **~20x** acceleration via step distillation and system optimization (single GPU)
- **âš¡ï¸ Revolutionary 4-Step Distillation**: Compress original 40-50 step inference to just 4 steps without CFG requirements
- **ğŸ› ï¸ Advanced Operator Support**: Integrated with cutting-edge operators including [Sage Attention](https://github.com/thu-ml/SageAttention), [Flash Attention](https://github.com/Dao-AILab/flash-attention), [Radial Attention](https://github.com/mit-han-lab/radial-attention), [q8-kernel](https://github.com/KONAKONA666/q8_kernels), [sgl-kernel](https://github.com/sgl-project/sglang/tree/main/sgl-kernel), [vllm](https://github.com/vllm-project/vllm)

### ğŸ’¾ **Resource-Efficient Deployment**
- **ğŸ’¡ Breaking Hardware Barriers**: Run 14B models for 480P/720P video generation with only **8GB VRAM + 16GB RAM**
- **ğŸ”§ Intelligent Parameter Offloading**: Advanced disk-CPU-GPU three-tier offloading architecture with phase/block-level granular management
- **âš™ï¸ Comprehensive Quantization**: Support for `w8a8-int8`, `w8a8-fp8`, `w4a4-nvfp4` and other quantization strategies

### ğŸ¨ **Rich Feature Ecosystem**
- **ğŸ“ˆ Smart Feature Caching**: Intelligent caching mechanisms to eliminate redundant computations
- **ğŸ”„ Parallel Inference**: Multi-GPU parallel processing for enhanced performance
- **ğŸ“± Flexible Deployment Options**: Support for Gradio, service deployment, ComfyUI and other deployment methods
- **ğŸ›ï¸ Dynamic Resolution Inference**: Adaptive resolution adjustment for optimal generation quality
- **ğŸï¸ Video Frame Interpolation**: RIFE-based frame interpolation for smooth frame rate enhancement


## ğŸ“š Technical Documentation

### ğŸ“– **Method Tutorials**
- [Model Quantization](https://lightx2v-en.readthedocs.io/en/latest/method_tutorials/quantization.html) - Comprehensive guide to quantization strategies
- [Feature Caching](https://lightx2v-en.readthedocs.io/en/latest/method_tutorials/cache.html) - Intelligent caching mechanisms
- [Attention Mechanisms](https://lightx2v-en.readthedocs.io/en/latest/method_tutorials/attention.html) - State-of-the-art attention operators
- [Parameter Offloading](https://lightx2v-en.readthedocs.io/en/latest/method_tutorials/offload.html) - Three-tier storage architecture
- [Parallel Inference](https://lightx2v-en.readthedocs.io/en/latest/method_tutorials/parallel.html) - Multi-GPU acceleration strategies
- [Changing Resolution Inference](https://lightx2v-en.readthedocs.io/en/latest/method_tutorials/changing_resolution.html) - U-shaped resolution strategy
- [Step Distillation](https://lightx2v-en.readthedocs.io/en/latest/method_tutorials/step_distill.html) - 4-step inference technology
- [Video Frame Interpolation](https://lightx2v-en.readthedocs.io/en/latest/method_tutorials/video_frame_interpolation.html) - Base on the RIFE technology

### ğŸ› ï¸ **Deployment Guides**
- [Low-Resource Deployment](https://lightx2v-en.readthedocs.io/en/latest/deploy_guides/for_low_resource.html) - Optimized 8GB VRAM solutions
- [Low-Latency Deployment](https://lightx2v-en.readthedocs.io/en/latest/deploy_guides/for_low_latency.html) - Ultra-fast inference optimization
- [Gradio Deployment](https://lightx2v-en.readthedocs.io/en/latest/deploy_guides/deploy_gradio.html) - Web interface setup
- [Service Deployment](https://lightx2v-en.readthedocs.io/en/latest/deploy_guides/deploy_service.html) - Production API service deployment
- [Lora Model Deployment](https://lightx2v-en.readthedocs.io/en/latest/deploy_guides/lora_deploy.html) - Flexible Lora deployment

## ğŸ¤ Acknowledgments

We sincerely thank all the model repositories and research communities that inspired and promoted the development of LightX2V. This framework is built on the collective efforts of the open-source community. It includes but is not limited to:

- [Tencent-Hunyuan](https://github.com/Tencent-Hunyuan)
- [Wan-Video](https://github.com/Wan-Video)
- [Qwen-Image](https://github.com/QwenLM/Qwen-Image)
- [LightLLM](https://github.com/ModelTC/LightLLM)
- [sglang](https://github.com/sgl-project/sglang)
- [vllm](https://github.com/vllm-project/vllm)
- [flash-attention](https://github.com/Dao-AILab/flash-attention)
- [SageAttention](https://github.com/thu-ml/SageAttention)
- [flashinfer](https://github.com/flashinfer-ai/flashinfer)
- [MagiAttention](https://github.com/SandAI-org/MagiAttention)
- [radial-attention](https://github.com/mit-han-lab/radial-attention)
- [xDiT](https://github.com/xdit-project/xDiT)
- [FastVideo](https://github.com/hao-ai-lab/FastVideo)

## ğŸŒŸ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=ModelTC/lightx2v&amp;type=Timeline)](https://star-history.com/#ModelTC/lightx2v&amp;Timeline)

## âœï¸ Citation

If you find LightX2V useful in your research, please consider citing our work:

```bibtex
@misc{lightx2v,
 author = {LightX2V Contributors},
 title = {LightX2V: Light Video Generation Inference Framework},
 year = {2025},
 publisher = {GitHub},
 journal = {GitHub repository},
 howpublished = {\url{https://github.com/ModelTC/lightx2v}},
}
```

## ğŸ“ Contact &amp; Support

For questions, suggestions, or support, please feel free to reach out through:
- ğŸ› [GitHub Issues](https://github.com/ModelTC/lightx2v/issues) - Bug reports and feature requests

---

&lt;div align=&quot;center&quot;&gt;
Built with â¤ï¸ by the LightX2V team
&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[vibrantlabsai/ragas]]></title>
            <link>https://github.com/vibrantlabsai/ragas</link>
            <guid>https://github.com/vibrantlabsai/ragas</guid>
            <pubDate>Sun, 28 Dec 2025 00:05:29 GMT</pubDate>
            <description><![CDATA[Supercharge Your LLM Application Evaluations ğŸš€]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/vibrantlabsai/ragas">vibrantlabsai/ragas</a></h1>
            <p>Supercharge Your LLM Application Evaluations ğŸš€</p>
            <p>Language: Python</p>
            <p>Stars: 11,919</p>
            <p>Forks: 1,181</p>
            <p>Stars today: 20 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
  &lt;img style=&quot;vertical-align:middle&quot; height=&quot;200&quot;
  src=&quot;https://raw.githubusercontent.com/vibrantlabsai/ragas/main/docs/_static/imgs/logo.png&quot;&gt;
&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;i&gt;Supercharge Your LLM Application Evaluations ğŸš€&lt;/i&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://github.com/vibrantlabsai/ragas/releases&quot;&gt;
        &lt;img alt=&quot;Latest release&quot; src=&quot;https://img.shields.io/github/release/vibrantlabsai/ragas.svg&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://www.python.org/&quot;&gt;
        &lt;img alt=&quot;Made with Python&quot; src=&quot;https://img.shields.io/badge/Made%20with-Python-1f425f.svg?color=purple&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/vibrantlabsai/ragas/blob/master/LICENSE&quot;&gt;
        &lt;img alt=&quot;License Apache-2.0&quot; src=&quot;https://img.shields.io/github/license/vibrantlabsai/ragas.svg?color=green&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://pypi.org/project/ragas/&quot;&gt;
        &lt;img alt=&quot;Ragas Downloads per month&quot; src=&quot;https://static.pepy.tech/badge/ragas/month&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://discord.gg/5djav8GGNZ&quot;&gt;
        &lt;img alt=&quot;Join Ragas community on Discord&quot; src=&quot;https://img.shields.io/discord/1119637219561451644&quot;&gt;
    &lt;/a&gt;
    &lt;a target=&quot;_blank&quot; href=&quot;https://deepwiki.com/vibrantlabsai/ragas&quot;&gt;
      &lt;img 
        src=&quot;https://devin.ai/assets/deepwiki-badge.png&quot; 
        alt=&quot;Ask DeepWiki.com&quot; 
        height=&quot;20&quot; 
      /&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;h4 align=&quot;center&quot;&gt;
    &lt;p&gt;
        &lt;a href=&quot;https://docs.ragas.io/&quot;&gt;Documentation&lt;/a&gt; |
        &lt;a href=&quot;#fire-quickstart&quot;&gt;Quick start&lt;/a&gt; |
        &lt;a href=&quot;https://discord.gg/5djav8GGNZ&quot;&gt;Join Discord&lt;/a&gt; |
        &lt;a href=&quot;https://blog.ragas.io/&quot;&gt;Blog&lt;/a&gt; |
        &lt;a href=&quot;https://newsletter.ragas.io/&quot;&gt;NewsLetter&lt;/a&gt; |
        &lt;a href=&quot;https://www.ragas.io/careers&quot;&gt;Careers&lt;/a&gt;
    &lt;p&gt;
&lt;/h4&gt;

Objective metrics, intelligent test generation, and data-driven insights for LLM apps

Ragas is your ultimate toolkit for evaluating and optimizing Large Language Model (LLM) applications. Say goodbye to time-consuming, subjective assessments and hello to data-driven, efficient evaluation workflows.
Don&#039;t have a test dataset ready? We also do production-aligned test set generation.

&gt; [!NOTE]
&gt; Need help setting up Evals for your AI application? We&#039;d love to help! We are conducting Office Hours every week. You can sign up [here](https://cal.com/team/vibrantlabs/office-hours).

## Key Features

- ğŸ¯ Objective Metrics: Evaluate your LLM applications with precision using both LLM-based and traditional metrics.
- ğŸ§ª Test Data Generation: Automatically create comprehensive test datasets covering a wide range of scenarios.
- ğŸ”— Seamless Integrations: Works flawlessly with popular LLM frameworks like LangChain and major observability tools.
- ğŸ“Š Build feedback loops: Leverage production data to continually improve your LLM applications.

## :shield: Installation

Pypi:

```bash
pip install ragas
```

Alternatively, from source:

```bash
pip install git+https://github.com/vibrantlabsai/ragas
```

## :fire: Quickstart

### Clone a Complete Example Project

The fastest way to get started is to use the `ragas quickstart` command:

```bash
# List available templates
ragas quickstart

# Create a RAG evaluation project
ragas quickstart rag_eval

# Specify where you want to create it.
ragas quickstart rag_eval -o ./my-project
```

Available templates:
- `rag_eval` - Evaluate RAG systems

Coming Soon:
- `agent_evals` - Evaluate AI agents
- `benchmark_llm` - Benchmark and compare LLMs
- `prompt_evals` - Evaluate prompt variations
- `workflow_eval` - Evaluate complex workflows

### Evaluate your LLM App

This is a simple example evaluating a summary for accuracy:

```python
import asyncio
from ragas.metrics.collections import AspectCritic
from ragas.llms import llm_factory

# Setup your LLM
llm = llm_factory(&quot;gpt-4o&quot;)

# Create a metric
metric = AspectCritic(
    name=&quot;summary_accuracy&quot;,
    definition=&quot;Verify if the summary is accurate and captures key information.&quot;,
    llm=llm
)

# Evaluate
test_data = {
    &quot;user_input&quot;: &quot;summarise given text\nThe company reported an 8% rise in Q3 2024, driven by strong performance in the Asian market. Sales in this region have significantly contributed to the overall growth. Analysts attribute this success to strategic marketing and product localization. The positive trend in the Asian market is expected to continue into the next quarter.&quot;,
    &quot;response&quot;: &quot;The company experienced an 8% increase in Q3 2024, largely due to effective marketing strategies and product adaptation, with expectations of continued growth in the coming quarter.&quot;,
}

score = await metric.ascore(
    user_input=test_data[&quot;user_input&quot;],
    response=test_data[&quot;response&quot;]
)
print(f&quot;Score: {score.value}&quot;)
print(f&quot;Reason: {score.reason}&quot;)
```

&gt; **Note**: Make sure your `OPENAI_API_KEY` environment variable is set.

Find the complete [Quickstart Guide](https://docs.ragas.io/en/latest/getstarted/evals)

## Want help in improving your AI application using evals?

In the past 2 years, we have seen and helped improve many AI applications using evals. If you want help with improving and scaling up your AI application using evals.

ğŸ”— Book a [slot](https://cal.com/team/vibrantlabs/app) or drop us a line: [founders@vibrantlabs.com](mailto:founders@vibrantlabs.com).

## ğŸ«‚ Community

If you want to get more involved with Ragas, check out our [discord server](https://discord.gg/5qGUJ6mh7C). It&#039;s a fun community where we geek out about LLM, Retrieval, Production issues, and more.

## Contributors

```yml
+----------------------------------------------------------------------------+
|     +----------------------------------------------------------------+     |
|     | Developers: Those who built with `ragas`.                      |     |
|     | (You have `import ragas` somewhere in your project)            |     |
|     |     +----------------------------------------------------+     |     |
|     |     | Contributors: Those who make `ragas` better.       |     |     |
|     |     | (You make PR to this repo)                         |     |     |
|     |     +----------------------------------------------------+     |     |
|     +----------------------------------------------------------------+     |
+----------------------------------------------------------------------------+
```

We welcome contributions from the community! Whether it&#039;s bug fixes, feature additions, or documentation improvements, your input is valuable.

1. Fork the repository
2. Create your feature branch (git checkout -b feature/AmazingFeature)
3. Commit your changes (git commit -m &#039;Add some AmazingFeature&#039;)
4. Push to the branch (git push origin feature/AmazingFeature)
5. Open a Pull Request

## ğŸ” Open Analytics

At Ragas, we believe in transparency. We collect minimal, anonymized usage data to improve our product and guide our development efforts.

âœ… No personal or company-identifying information

âœ… Open-source data collection [code](./src/ragas/_analytics.py)

âœ… Publicly available aggregated [data](https://github.com/vibrantlabsai/ragas/issues/49)

To opt-out, set the `RAGAS_DO_NOT_TRACK` environment variable to `true`.

### Cite Us

```
@misc{ragas2024,
  author       = {VibrantLabs},
  title        = {Ragas: Supercharge Your LLM Application Evaluations},
  year         = {2024},
  howpublished = {\url{https://github.com/vibrantlabsai/ragas}},
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[facebookresearch/llm-transparency-tool]]></title>
            <link>https://github.com/facebookresearch/llm-transparency-tool</link>
            <guid>https://github.com/facebookresearch/llm-transparency-tool</guid>
            <pubDate>Sun, 28 Dec 2025 00:05:28 GMT</pubDate>
            <description><![CDATA[LLM Transparency Tool (LLM-TT), an open-source interactive toolkit for analyzing internal workings of Transformer-based language models. *Check out demo at* https://huggingface.co/spaces/facebook/llm-transparency-tool-demo]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/facebookresearch/llm-transparency-tool">facebookresearch/llm-transparency-tool</a></h1>
            <p>LLM Transparency Tool (LLM-TT), an open-source interactive toolkit for analyzing internal workings of Transformer-based language models. *Check out demo at* https://huggingface.co/spaces/facebook/llm-transparency-tool-demo</p>
            <p>Language: Python</p>
            <p>Stars: 1,185</p>
            <p>Forks: 100</p>
            <p>Stars today: 34 stars today</p>
            <h2>README</h2><pre>&lt;h1&gt;
  &lt;img width=&quot;500&quot; alt=&quot;LLM Transparency Tool&quot; src=&quot;https://github.com/facebookresearch/llm-transparency-tool/assets/1367529/795233be-5ef7-4523-8282-67486cf2e15f&quot;&gt;
&lt;/h1&gt;

&lt;img width=&quot;832&quot; alt=&quot;screenshot&quot; src=&quot;https://github.com/facebookresearch/llm-transparency-tool/assets/1367529/78f6f9e2-fe76-4ded-bb78-a57f64f4ac3a&quot;&gt;


## Key functionality

* Choose your model, choose or add your prompt, run the inference.
* Browse contribution graph.
    * Select the token to build the graph from.
    * Tune the contribution threshold.
* Select representation of any token after any block.
* For the representation, see its projection to the output vocabulary, see which tokens
were promoted/suppressed but the previous block.
* The following things are clickable:
  * Edges. That shows more info about the contributing attention head.
  * Heads when an edge is selected. You can see what this head is promoting/suppressing.
  * FFN blocks (little squares on the graph).
  * Neurons when an FFN block is selected.


## Installation

### Dockerized running
```bash
# From the repository root directory
docker build -t llm_transparency_tool .
docker run --rm -p 7860:7860 llm_transparency_tool
```

### Local Installation


```bash
# download
git clone git@github.com:facebookresearch/llm-transparency-tool.git
cd llm-transparency-tool

# install the necessary packages
conda env create --name llmtt -f env.yaml
# install the `llm_transparency_tool` package
pip install -e .

# now, we need to build the frontend
# don&#039;t worry, even `yarn` comes preinstalled by `env.yaml`
cd llm_transparency_tool/components/frontend
yarn install
yarn build
```

### Launch

```bash
streamlit run llm_transparency_tool/server/app.py -- config/local.json
```


## Adding support for your LLM

Initially, the tool allows you to select from just a handful of models. Here are the
options you can try for using your model in the tool, from least to most
effort.


### The model is already supported by TransformerLens

Full list of models is [here](https://github.com/neelnanda-io/TransformerLens/blob/0825c5eb4196e7ad72d28bcf4e615306b3897490/transformer_lens/loading_from_pretrained.py#L18).
In this case, the model can be added to the configuration json file.


### Tuned version of a model supported by TransformerLens

Add the official name of the model to the config along with the location to read the
weights from.


### The model is not supported by TransformerLens

In this case the UI wouldn&#039;t know how to create proper hooks for the model. You&#039;d need
to implement your version of [TransparentLlm](./llm_transparency_tool/models/transparent_llm.py#L28) class and alter the
Streamlit app to use your implementation.

## Citation
If you use the LLM Transparency Tool for your research, please consider citing:

```bibtex
@article{tufanov2024lm,
      title={LM Transparency Tool: Interactive Tool for Analyzing Transformer Language Models}, 
      author={Igor Tufanov and Karen Hambardzumyan and Javier Ferrando and Elena Voita},
      year={2024},
      journal={Arxiv},
      url={https://arxiv.org/abs/2404.07004}
}

@article{ferrando2024information,
    title={Information Flow Routes: Automatically Interpreting Language Models at Scale}, 
    author={Javier Ferrando and Elena Voita},
    year={2024},
    journal={Arxiv},
    url={https://arxiv.org/abs/2403.00824}
}
````

## License

This code is made available under a [CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/) license, as found in the LICENSE file.
However you may have other legal obligations that govern your use of other content, such as the terms of service for third-party models.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hsliuping/TradingAgents-CN]]></title>
            <link>https://github.com/hsliuping/TradingAgents-CN</link>
            <guid>https://github.com/hsliuping/TradingAgents-CN</guid>
            <pubDate>Sun, 28 Dec 2025 00:05:27 GMT</pubDate>
            <description><![CDATA[åŸºäºå¤šæ™ºèƒ½ä½“LLMçš„ä¸­æ–‡é‡‘èäº¤æ˜“æ¡†æ¶ - TradingAgentsä¸­æ–‡å¢å¼ºç‰ˆ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hsliuping/TradingAgents-CN">hsliuping/TradingAgents-CN</a></h1>
            <p>åŸºäºå¤šæ™ºèƒ½ä½“LLMçš„ä¸­æ–‡é‡‘èäº¤æ˜“æ¡†æ¶ - TradingAgentsä¸­æ–‡å¢å¼ºç‰ˆ</p>
            <p>Language: Python</p>
            <p>Stars: 14,216</p>
            <p>Forks: 3,127</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre># TradingAgents ä¸­æ–‡å¢å¼ºç‰ˆ

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python](https://img.shields.io/badge/Python-3.10%2B-blue.svg)](https://www.python.org/)
[![Version](https://img.shields.io/badge/Version-cn--0.1.15-green.svg)](./VERSION)
[![Documentation](https://img.shields.io/badge/docs-ä¸­æ–‡æ–‡æ¡£-green.svg)](./docs/)
[![Original](https://img.shields.io/badge/åŸºäº-TauricResearch/TradingAgents-orange.svg)](https://github.com/TauricResearch/TradingAgents)

&gt;
&gt; ğŸ“ **å­¦ä¹ ä¸­å¿ƒ**: AIåŸºç¡€ | æç¤ºè¯å·¥ç¨‹ | æ¨¡å‹é€‰æ‹© | å¤šæ™ºèƒ½ä½“åˆ†æåŸç† | é£é™©ä¸å±€é™ | æºé¡¹ç›®ä¸è®ºæ–‡ | å®æˆ˜æ•™ç¨‹ï¼ˆéƒ¨åˆ†ä¸ºå¤–é“¾ï¼‰ | å¸¸è§é—®é¢˜
&gt; ğŸ¯ **æ ¸å¿ƒåŠŸèƒ½**: åŸç”ŸOpenAIæ”¯æŒ | Google AIå…¨é¢é›†æˆ | è‡ªå®šä¹‰ç«¯ç‚¹é…ç½® | æ™ºèƒ½æ¨¡å‹é€‰æ‹© | å¤šLLMæä¾›å•†æ”¯æŒ | æ¨¡å‹é€‰æ‹©æŒä¹…åŒ– | Dockerå®¹å™¨åŒ–éƒ¨ç½² | ä¸“ä¸šæŠ¥å‘Šå¯¼å‡º | å®Œæ•´Aè‚¡æ”¯æŒ | ä¸­æ–‡æœ¬åœ°åŒ–

é¢å‘ä¸­æ–‡ç”¨æˆ·çš„**å¤šæ™ºèƒ½ä½“ä¸å¤§æ¨¡å‹è‚¡ç¥¨åˆ†æå­¦ä¹ å¹³å°**ã€‚å¸®åŠ©ä½ ç³»ç»ŸåŒ–å­¦ä¹ å¦‚ä½•ä½¿ç”¨å¤šæ™ºèƒ½ä½“äº¤æ˜“æ¡†æ¶ä¸ AI å¤§æ¨¡å‹è¿›è¡Œåˆè§„çš„è‚¡ç¥¨ç ”ç©¶ä¸ç­–ç•¥å®éªŒï¼Œä¸æä¾›å®ç›˜äº¤æ˜“æŒ‡ä»¤ï¼Œå¹³å°å®šä½ä¸ºå­¦ä¹ ä¸ç ”ç©¶ç”¨é€”ã€‚

## ğŸ™ è‡´æ•¬æºé¡¹ç›®

æ„Ÿè°¢ [Tauric Research](https://github.com/TauricResearch) å›¢é˜Ÿåˆ›é€ çš„é©å‘½æ€§å¤šæ™ºèƒ½ä½“äº¤æ˜“æ¡†æ¶ [TradingAgents](https://github.com/TauricResearch/TradingAgents)ï¼

**ğŸ¯ æˆ‘ä»¬çš„å®šä½ä¸ä½¿å‘½**: ä¸“æ³¨å­¦ä¹ ä¸ç ”ç©¶ï¼Œæä¾›ä¸­æ–‡åŒ–å­¦ä¹ ä¸­å¿ƒä¸å·¥å…·ï¼Œåˆè§„å‹å¥½ï¼Œæ”¯æŒ Aè‚¡/æ¸¯è‚¡/ç¾è‚¡ çš„åˆ†æä¸æ•™å­¦ï¼Œæ¨åŠ¨ AI é‡‘èæŠ€æœ¯åœ¨ä¸­æ–‡ç¤¾åŒºçš„æ™®åŠä¸æ­£ç¡®ä½¿ç”¨ã€‚

## ğŸ‰ v1.0.0-preview ç‰ˆæœ¬ä¸Šçº¿ - å…¨æ–°æ¶æ„å‡çº§

&gt; ğŸš€ **é‡ç£…å‘å¸ƒ**: v1.0.0-preview ç‰ˆæœ¬ç°å·²æ­£å¼ï¼å…¨æ–°çš„ FastAPI + Vue 3 æ¶æ„ï¼Œå¸¦æ¥ä¼ä¸šçº§çš„æ€§èƒ½å’Œä½“éªŒï¼

### âœ¨ æ ¸å¿ƒç‰¹æ€§

#### ğŸ—ï¸ **å…¨æ–°æŠ€æœ¯æ¶æ„**
- **åç«¯å‡çº§**: ä» Streamlit è¿ç§»åˆ° FastAPIï¼Œæä¾›æ›´å¼ºå¤§çš„ RESTful API
- **å‰ç«¯é‡æ„**: é‡‡ç”¨ Vue 3 + Element Plusï¼Œæ‰“é€ ç°ä»£åŒ–çš„å•é¡µåº”ç”¨
- **æ•°æ®åº“ä¼˜åŒ–**: MongoDB + Redis åŒæ•°æ®åº“æ¶æ„ï¼Œæ€§èƒ½æå‡ 10 å€
- **å®¹å™¨åŒ–éƒ¨ç½²**: å®Œæ•´çš„ Docker å¤šæ¶æ„æ”¯æŒï¼ˆamd64 + arm64ï¼‰

#### ğŸ¯ **ä¼ä¸šçº§åŠŸèƒ½**
- **ç”¨æˆ·æƒé™ç®¡ç†**: å®Œæ•´çš„ç”¨æˆ·è®¤è¯ã€è§’è‰²ç®¡ç†ã€æ“ä½œæ—¥å¿—ç³»ç»Ÿ
- **é…ç½®ç®¡ç†ä¸­å¿ƒ**: å¯è§†åŒ–çš„å¤§æ¨¡å‹é…ç½®ã€æ•°æ®æºç®¡ç†ã€ç³»ç»Ÿè®¾ç½®
- **ç¼“å­˜ç®¡ç†ç³»ç»Ÿ**: æ™ºèƒ½ç¼“å­˜ç­–ç•¥ï¼Œæ”¯æŒ MongoDB/Redis/æ–‡ä»¶å¤šçº§ç¼“å­˜
- **å®æ—¶é€šçŸ¥ç³»ç»Ÿ**: SSE+WebSocket åŒé€šé“æ¨é€ï¼Œå®æ—¶è·Ÿè¸ªåˆ†æè¿›åº¦å’Œç³»ç»ŸçŠ¶æ€
- **æ‰¹é‡åˆ†æåŠŸèƒ½**: æ”¯æŒå¤šåªè‚¡ç¥¨åŒæ—¶åˆ†æï¼Œæå‡å·¥ä½œæ•ˆç‡
- **æ™ºèƒ½è‚¡ç¥¨ç­›é€‰**: åŸºäºå¤šç»´åº¦æŒ‡æ ‡çš„è‚¡ç¥¨ç­›é€‰å’Œæ’åºç³»ç»Ÿ
- **è‡ªé€‰è‚¡ç®¡ç†**: ä¸ªäººè‡ªé€‰è‚¡æ”¶è—ã€åˆ†ç»„ç®¡ç†å’Œè·Ÿè¸ªåŠŸèƒ½
- **ä¸ªè‚¡è¯¦æƒ…é¡µ**: å®Œæ•´çš„ä¸ªè‚¡ä¿¡æ¯å±•ç¤ºå’Œå†å²åˆ†æè®°å½•
- **æ¨¡æ‹Ÿäº¤æ˜“ç³»ç»Ÿ**: è™šæ‹Ÿäº¤æ˜“ç¯å¢ƒï¼ŒéªŒè¯æŠ•èµ„ç­–ç•¥æ•ˆæœ

#### ğŸ¤– **æ™ºèƒ½åˆ†æå¢å¼º**
- **åŠ¨æ€ä¾›åº”å•†ç®¡ç†**: æ”¯æŒåŠ¨æ€æ·»åŠ å’Œé…ç½® LLM ä¾›åº”å•†
- **æ¨¡å‹èƒ½åŠ›ç®¡ç†**: æ™ºèƒ½æ¨¡å‹é€‰æ‹©ï¼Œæ ¹æ®ä»»åŠ¡è‡ªåŠ¨åŒ¹é…æœ€ä½³æ¨¡å‹
- **å¤šæ•°æ®æºåŒæ­¥**: ç»Ÿä¸€çš„æ•°æ®æºç®¡ç†ï¼Œæ”¯æŒ Tushareã€AkShareã€BaoStock
- **æŠ¥å‘Šå¯¼å‡ºåŠŸèƒ½**: æ”¯æŒ Markdown/Word/PDF å¤šæ ¼å¼ä¸“ä¸šæŠ¥å‘Šå¯¼å‡º

#### ï¿½ **é‡å¤§Bugä¿®å¤**
- **æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ä¿®å¤**: å½»åº•è§£å†³å¸‚åœºåˆ†æå¸ˆæŠ€æœ¯æŒ‡æ ‡è®¡ç®—ä¸å‡†ç¡®é—®é¢˜
- **åŸºæœ¬é¢æ•°æ®ä¿®å¤**: ä¿®å¤åŸºæœ¬é¢åˆ†æå¸ˆPEã€PBç­‰å…³é”®è´¢åŠ¡æ•°æ®è®¡ç®—é”™è¯¯
- **æ­»å¾ªç¯é—®é¢˜ä¿®å¤**: è§£å†³éƒ¨åˆ†ç”¨æˆ·åœ¨åˆ†æè¿‡ç¨‹ä¸­è§¦å‘çš„æ— é™å¾ªç¯é—®é¢˜
- **æ•°æ®ä¸€è‡´æ€§ä¼˜åŒ–**: ç¡®ä¿æ‰€æœ‰åˆ†æå¸ˆä½¿ç”¨ç»Ÿä¸€ã€å‡†ç¡®çš„æ•°æ®æº

#### ï¿½ğŸ³ **Docker å¤šæ¶æ„æ”¯æŒ**
- **è·¨å¹³å°éƒ¨ç½²**: æ”¯æŒ x86_64 å’Œ ARM64 æ¶æ„ï¼ˆApple Siliconã€æ ‘è“æ´¾ã€AWS Gravitonï¼‰
- **GitHub Actions**: è‡ªåŠ¨åŒ–æ„å»ºå’Œå‘å¸ƒ Docker é•œåƒ
- **ä¸€é”®éƒ¨ç½²**: å®Œæ•´çš„ Docker Compose é…ç½®ï¼Œ5 åˆ†é’Ÿå¿«é€Ÿå¯åŠ¨

### ğŸ“Š æŠ€æœ¯æ ˆå‡çº§

| ç»„ä»¶ | v0.1.x | v1.0.0-preview |
|------|--------|----------------|
| **åç«¯æ¡†æ¶** | Streamlit | FastAPI + Uvicorn |
| **å‰ç«¯æ¡†æ¶** | Streamlit | Vue 3 + Vite + Element Plus |
| **æ•°æ®åº“** | å¯é€‰ MongoDB | MongoDB + Redis |
| **API æ¶æ„** | å•ä½“åº”ç”¨ | RESTful API + WebSocket |
| **éƒ¨ç½²æ–¹å¼** | æœ¬åœ°/Docker | Docker å¤šæ¶æ„ + GitHub Actions |



#### ğŸ“¥ å®‰è£…éƒ¨ç½²

**ä¸‰ç§éƒ¨ç½²æ–¹å¼ï¼Œä»»é€‰å…¶ä¸€**ï¼š

| éƒ¨ç½²æ–¹å¼ | é€‚ç”¨åœºæ™¯ | éš¾åº¦ | æ–‡æ¡£é“¾æ¥ |
|---------|---------|------|---------|
| ğŸŸ¢ **ç»¿è‰²ç‰ˆ** | Windows ç”¨æˆ·ã€å¿«é€Ÿä½“éªŒ | â­ ç®€å• | [ç»¿è‰²ç‰ˆå®‰è£…æŒ‡å—](https://mp.weixin.qq.com/s/eoo_HeIGxaQZVT76LBbRJQ) |
| ğŸ³ **Dockerç‰ˆ** | ç”Ÿäº§ç¯å¢ƒã€è·¨å¹³å° | â­â­ ä¸­ç­‰ | [Docker éƒ¨ç½²æŒ‡å—](https://mp.weixin.qq.com/s/JkA0cOu8xJnoY_3LC5oXNw) |
| ğŸ’» **æœ¬åœ°ä»£ç ç‰ˆ** | å¼€å‘è€…ã€å®šåˆ¶éœ€æ±‚ | â­â­â­ è¾ƒéš¾ | [æœ¬åœ°å®‰è£…æŒ‡å—](https://mp.weixin.qq.com/s/cqUGf-sAzcBV19gdI4sYfA) |

âš ï¸ **é‡è¦æé†’**ï¼šåœ¨åˆ†æè‚¡ç¥¨ä¹‹å‰ï¼Œè¯·æŒ‰ç›¸å…³æ–‡æ¡£è¦æ±‚ï¼Œå°†è‚¡ç¥¨æ•°æ®åŒæ­¥å®Œæˆï¼Œå¦åˆ™åˆ†æç»“æœå°†ä¼šå‡ºç°æ•°æ®é”™è¯¯ã€‚



#### ğŸ“š ä½¿ç”¨æŒ‡å—

åœ¨ä½¿ç”¨å‰ï¼Œå»ºè®®å…ˆé˜…è¯»è¯¦ç»†çš„ä½¿ç”¨æŒ‡å—ï¼š
- **[0ã€ğŸ“˜ TradingAgents-CN v1.0.0-preview å¿«é€Ÿå…¥é—¨è§†é¢‘](https://www.bilibili.com/video/BV1i2CeBwEP7/?vd_source=5d790a5b8d2f46d2c10fd4e770be1594)**

- **[1ã€ğŸ“˜ TradingAgents-CN v1.0.0-preview ä½¿ç”¨æŒ‡å—](https://mp.weixin.qq.com/s/ppsYiBncynxlsfKFG8uEbw)**
- **[2ã€ğŸ“˜ ä½¿ç”¨ Docker Compose éƒ¨ç½²TradingAgents-CN v1.0.0-previewï¼ˆå®Œå…¨ç‰ˆï¼‰](https://mp.weixin.qq.com/s/JkA0cOu8xJnoY_3LC5oXNw)**
- **[3ã€ğŸ“˜ ä» Docker Hub æ›´æ–° TradingAgentsâ€‘CN é•œåƒ](https://mp.weixin.qq.com/s/WKYhW8J80Watpg8K6E_dSQ)**
- **[4ã€ğŸ“˜ TradingAgents-CN v1.0.0-previewç»¿è‰²ç‰ˆå®‰è£…å’Œå‡çº§æŒ‡å—](https://mp.weixin.qq.com/s/eoo_HeIGxaQZVT76LBbRJQ)**
- **[5ã€ğŸ“˜ TradingAgents-CN v1.0.0-previewç»¿è‰²ç‰ˆç«¯å£é…ç½®è¯´æ˜](https://mp.weixin.qq.com/s/o5QdNuh2-iKkIHzJXCj7vQ)**
- **[6ã€ğŸ“˜ TradingAgents v1.0.0-preview æºç ç‰ˆå®‰è£…æ‰‹å†Œï¼ˆä¿®è®¢ç‰ˆï¼‰](https://mp.weixin.qq.com/s/cqUGf-sAzcBV19gdI4sYfA)**
- **[7ã€ğŸ“˜ TradingAgents v1.0.0-preview æºç å®‰è£…è§†é¢‘æ•™ç¨‹](https://www.bilibili.com/video/BV1FxCtBHEte/?vd_source=5d790a5b8d2f46d2c10fd4e770be1594)**


ä½¿ç”¨æŒ‡å—åŒ…å«ï¼š
- âœ… å®Œæ•´çš„åŠŸèƒ½ä»‹ç»å’Œæ“ä½œæ¼”ç¤º
- âœ… è¯¦ç»†çš„é…ç½®è¯´æ˜å’Œæœ€ä½³å®è·µ
- âœ… å¸¸è§é—®é¢˜è§£ç­”å’Œæ•…éšœæ’é™¤
- âœ… å®é™…ä½¿ç”¨æ¡ˆä¾‹å’Œæ•ˆæœå±•ç¤º

#### å…³æ³¨å…¬ä¼—å·

1. **å…³æ³¨å…¬ä¼—å·**: å¾®ä¿¡æœç´¢ **&quot;TradingAgents-CN&quot;** å¹¶å…³æ³¨
2. å…¬ä¼—å·æ¯å¤©æ¨é€é¡¹ç›®æœ€æ–°è¿›å±•å’Œä½¿ç”¨æ•™ç¨‹


- **å¾®ä¿¡å…¬ä¼—å·**: TradingAgents-CNï¼ˆæ¨èï¼‰

  &lt;img src=&quot;assets/wexin.png&quot; alt=&quot;å¾®ä¿¡å…¬ä¼—å·&quot; width=&quot;200&quot;/&gt;


## ğŸ†š ä¸­æ–‡å¢å¼ºç‰¹è‰²

**ç›¸æ¯”åŸç‰ˆæ–°å¢**: æ™ºèƒ½æ–°é—»åˆ†æ | å¤šå±‚æ¬¡æ–°é—»è¿‡æ»¤ | æ–°é—»è´¨é‡è¯„ä¼° | ç»Ÿä¸€æ–°é—»å·¥å…· | å¤šLLMæä¾›å•†é›†æˆ | æ¨¡å‹é€‰æ‹©æŒä¹…åŒ– | å¿«é€Ÿåˆ‡æ¢æŒ‰é’® | | å®æ—¶è¿›åº¦æ˜¾ç¤º | æ™ºèƒ½ä¼šè¯ç®¡ç† | ä¸­æ–‡ç•Œé¢ | Aè‚¡æ•°æ® | å›½äº§LLM | Dockeréƒ¨ç½² | ä¸“ä¸šæŠ¥å‘Šå¯¼å‡º | ç»Ÿä¸€æ—¥å¿—ç®¡ç† | Webé…ç½®ç•Œé¢ | æˆæœ¬ä¼˜åŒ–

## ğŸ“¢ æ‹›å‹Ÿæµ‹è¯•å¿—æ„¿è€…

### ğŸ¯ æˆ‘ä»¬éœ€è¦ä½ çš„å¸®åŠ©ï¼

TradingAgentsCN å·²ç»è·å¾— **13,000+ stars**ï¼Œä½†ä¸€ç›´ç”±æˆ‘ä¸€ä¸ªäººå¼€å‘ç»´æŠ¤ã€‚æ¯æ¬¡å‘å¸ƒæ–°ç‰ˆæœ¬æ—¶ï¼Œå°½ç®¡æˆ‘ä¼šå°½åŠ›æµ‹è¯•ï¼Œä½†ä»ç„¶ä¼šæœ‰ä¸€äº›éšè—çš„ bug æ²¡æœ‰è¢«å‘ç°ã€‚

**æˆ‘éœ€è¦ä½ çš„å¸®åŠ©æ¥è®©è¿™ä¸ªé¡¹ç›®å˜å¾—æ›´å¥½ï¼**

### ğŸ™‹ æˆ‘ä»¬éœ€è¦ä»€ä¹ˆæ ·çš„å¿—æ„¿è€…ï¼Ÿ

- âœ… å¯¹è‚¡ç¥¨åˆ†ææˆ– AI åº”ç”¨æ„Ÿå…´è¶£
- âœ… æ„¿æ„åœ¨æ–°ç‰ˆæœ¬å‘å¸ƒå‰è¿›è¡Œæµ‹è¯•
- âœ… èƒ½å¤Ÿæ¸…æ™°æè¿°é‡åˆ°çš„é—®é¢˜
- âœ… æ¯å‘¨å¯ä»¥æŠ•å…¥ 2-4 å°æ—¶ï¼ˆå¼¹æ€§æ—¶é—´ï¼‰

**ä¸éœ€è¦ç¼–ç¨‹ç»éªŒï¼** åŠŸèƒ½æµ‹è¯•ã€æ–‡æ¡£æµ‹è¯•ã€ç”¨æˆ·ä½“éªŒæµ‹è¯•éƒ½éå¸¸æœ‰ä»·å€¼ã€‚

### ğŸ ä½ å°†è·å¾—ä»€ä¹ˆï¼Ÿ

1. **ä¼˜å…ˆä½“éªŒæƒ** - æå‰ä½“éªŒæ–°åŠŸèƒ½å’Œæ–°ç‰ˆæœ¬
2. **æŠ€æœ¯æˆé•¿** - æ·±å…¥äº†è§£å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå’Œ LLM åº”ç”¨å¼€å‘
3. **ç¤¾åŒºè®¤å¯** - åœ¨ README å’Œå‘å¸ƒè¯´æ˜ä¸­è‡´è°¢ï¼Œè·å¾— &quot;Core Tester&quot; æ ‡ç­¾
4. **å¼€æºè´¡çŒ®** - ä¸º 13,000+ stars çš„é¡¹ç›®åšå‡ºå®è´¨æ€§è´¡çŒ®
5. **æœªæ¥æœºä¼š** - å¦‚æœé¡¹ç›®å•†ä¸šåŒ–ï¼Œå¯èƒ½ä¼šæœ‰ç›¸åº”çš„æŠ¥é…¬

### ğŸš€ å¦‚ä½•åŠ å…¥ï¼Ÿ

**æ–¹å¼ä¸€ï¼šå¾®ä¿¡å…¬ä¼—å·ç”³è¯·ï¼ˆæ¨èï¼‰**
1. å…³æ³¨å¾®ä¿¡å…¬ä¼—å·ï¼š**TradingAgentsCN**
2. åœ¨å…¬ä¼—å·èœå•é€‰æ‹©&quot;æµ‹è¯•ç”³è¯·&quot;èœå•
3. å¡«å†™ç”³è¯·ä¿¡æ¯

**æ–¹å¼äºŒï¼šé‚®ä»¶ç”³è¯·**
- å‘é€é‚®ä»¶åˆ°ï¼šhsliup@163.com
- ä¸»é¢˜ï¼šæµ‹è¯•å¿—æ„¿è€…ç”³è¯·

### ğŸ“‹ æµ‹è¯•å†…å®¹ç¤ºä¾‹

- **æ—¥å¸¸æµ‹è¯•**ï¼ˆæ¯å‘¨ 2-4 å°æ—¶ï¼‰ï¼šæµ‹è¯•æ–°åŠŸèƒ½å’Œ bug ä¿®å¤ï¼Œåœ¨ä¸åŒç¯å¢ƒä¸‹éªŒè¯åŠŸèƒ½
- **ç‰ˆæœ¬å‘å¸ƒå‰æµ‹è¯•**ï¼ˆæ¯æœˆ 1-2 æ¬¡ï¼‰ï¼šå®Œæ•´çš„åŠŸèƒ½å›å½’æµ‹è¯•ã€å®‰è£…å’Œéƒ¨ç½²æµç¨‹æµ‹è¯•

### ğŸŒŸ ç‰¹åˆ«éœ€è¦çš„æµ‹è¯•æ–¹å‘

- ğŸªŸ **Windows ç”¨æˆ·** - æµ‹è¯• Windows å®‰è£…ç¨‹åºå’Œç»¿è‰²ç‰ˆ
- ğŸ **macOS ç”¨æˆ·** - æµ‹è¯• macOS å…¼å®¹æ€§
- ğŸ§ **Linux ç”¨æˆ·** - æµ‹è¯• Linux å…¼å®¹æ€§
- ğŸ³ **Docker ç”¨æˆ·** - æµ‹è¯• Docker éƒ¨ç½²
- ğŸ“Š **å¤šå¸‚åœºç”¨æˆ·** - æµ‹è¯• A è‚¡ã€æ¸¯è‚¡ã€ç¾è‚¡æ•°æ®æº
- ğŸ¤– **å¤š LLM ç”¨æˆ·** - æµ‹è¯•ä¸åŒ LLM æä¾›å•†ï¼ˆOpenAI/Gemini/DeepSeek/é€šä¹‰åƒé—®ç­‰ï¼‰

**è¯¦ç»†ä¿¡æ¯**: æŸ¥çœ‹å®Œæ•´æ‹›å‹Ÿå…¬å‘Š â†’ [ğŸ“¢ æµ‹è¯•å¿—æ„¿è€…æ‹›å‹Ÿ](docs/community/CALL_FOR_TESTERS.md)

## ğŸ¤ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿å„ç§å½¢å¼çš„è´¡çŒ®ï¼š

### è´¡çŒ®ç±»å‹

- ğŸ› **Bugä¿®å¤** - å‘ç°å¹¶ä¿®å¤é—®é¢˜
- âœ¨ **æ–°åŠŸèƒ½** - æ·»åŠ æ–°çš„åŠŸèƒ½ç‰¹æ€§
- ğŸ“š **æ–‡æ¡£æ”¹è¿›** - å®Œå–„æ–‡æ¡£å’Œæ•™ç¨‹
- ğŸŒ **æœ¬åœ°åŒ–** - ç¿»è¯‘å’Œæœ¬åœ°åŒ–å·¥ä½œ
- ğŸ¨ **ä»£ç ä¼˜åŒ–** - æ€§èƒ½ä¼˜åŒ–å’Œä»£ç é‡æ„

### è´¡çŒ®æµç¨‹

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m &#039;Add some AmazingFeature&#039;`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. åˆ›å»º Pull Request

### ğŸ“‹ æŸ¥çœ‹è´¡çŒ®è€…

æŸ¥çœ‹æ‰€æœ‰è´¡çŒ®è€…å’Œè¯¦ç»†è´¡çŒ®å†…å®¹ï¼š**[ğŸ¤ è´¡çŒ®è€…åå•](CONTRIBUTORS.md)**

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨**æ··åˆè®¸å¯è¯**æ¨¡å¼ï¼Œè¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ï¼š

### ğŸ”“ å¼€æºéƒ¨åˆ†ï¼ˆApache 2.0ï¼‰
- **é€‚ç”¨èŒƒå›´**ï¼šé™¤ `app/` å’Œ `frontend/` å¤–çš„æ‰€æœ‰æ–‡ä»¶
- **æƒé™**ï¼šå•†ä¸šä½¿ç”¨ âœ… | ä¿®æ”¹åˆ†å‘ âœ… | ç§äººä½¿ç”¨ âœ… | ä¸“åˆ©ä½¿ç”¨ âœ…
- **æ¡ä»¶**ï¼šä¿ç•™ç‰ˆæƒå£°æ˜ â— | åŒ…å«è®¸å¯è¯å‰¯æœ¬ â—

### ğŸ”’ ä¸“æœ‰éƒ¨åˆ†ï¼ˆéœ€å•†ä¸šæˆæƒï¼‰
- **é€‚ç”¨èŒƒå›´**ï¼š`app/`ï¼ˆFastAPIåç«¯ï¼‰å’Œ `frontend/`ï¼ˆVueå‰ç«¯ï¼‰ç›®å½•
- **å•†ä¸šä½¿ç”¨**ï¼šéœ€è¦å•ç‹¬è®¸å¯åè®®
- **è”ç³»æˆæƒ**ï¼š[hsliup@163.com](mailto:hsliup@163.com)

### ğŸ“‹ è®¸å¯è¯é€‰æ‹©å»ºè®®
- **ä¸ªäººå­¦ä¹ /ç ”ç©¶**ï¼šå¯è‡ªç”±ä½¿ç”¨å…¨éƒ¨åŠŸèƒ½
- **å•†ä¸šåº”ç”¨**ï¼šè¯·è”ç³»è·å–ä¸“æœ‰ç»„ä»¶æˆæƒ
- **å®šåˆ¶å¼€å‘**ï¼šæ¬¢è¿å’¨è¯¢å•†ä¸šåˆä½œæ–¹æ¡ˆ

## ğŸ™ è‡´è°¢ä¸æ„Ÿæ©

### ğŸŒŸ å‘æºé¡¹ç›®å¼€å‘è€…è‡´æ•¬

æˆ‘ä»¬å‘ [Tauric Research](https://github.com/TauricResearch) å›¢é˜Ÿè¡¨è¾¾æœ€æ·±çš„æ•¬æ„å’Œæ„Ÿè°¢ï¼š

- **ğŸ¯ æ„¿æ™¯é¢†å¯¼è€…**: æ„Ÿè°¢æ‚¨ä»¬åœ¨AIé‡‘èé¢†åŸŸçš„å‰ç»æ€§æ€è€ƒå’Œåˆ›æ–°å®è·µ
- **ğŸ’ çè´µæºç **: æ„Ÿè°¢æ‚¨ä»¬å¼€æºçš„æ¯ä¸€è¡Œä»£ç ï¼Œå®ƒä»¬å‡èšç€æ— æ•°çš„æ™ºæ…§å’Œå¿ƒè¡€
- **ğŸ—ï¸ æ¶æ„å¤§å¸ˆ**: æ„Ÿè°¢æ‚¨ä»¬è®¾è®¡äº†å¦‚æ­¤ä¼˜é›…ã€å¯æ‰©å±•çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶
- **ğŸ’¡ æŠ€æœ¯å…ˆé©±**: æ„Ÿè°¢æ‚¨ä»¬å°†å‰æ²¿AIæŠ€æœ¯ä¸é‡‘èå®åŠ¡å®Œç¾ç»“åˆ
- **ğŸ”„ æŒç»­è´¡çŒ®**: æ„Ÿè°¢æ‚¨ä»¬æŒç»­çš„ç»´æŠ¤ã€æ›´æ–°å’Œæ”¹è¿›å·¥ä½œ

### ğŸ¤ ç¤¾åŒºè´¡çŒ®è€…è‡´è°¢

æ„Ÿè°¢æ‰€æœ‰ä¸ºTradingAgents-CNé¡¹ç›®åšå‡ºè´¡çŒ®çš„å¼€å‘è€…å’Œç”¨æˆ·ï¼

è¯¦ç»†çš„è´¡çŒ®è€…åå•å’Œè´¡çŒ®å†…å®¹è¯·æŸ¥çœ‹ï¼š**[ğŸ“‹ è´¡çŒ®è€…åå•](CONTRIBUTORS.md)**

åŒ…æ‹¬ä½†ä¸é™äºï¼š

- ğŸ³ **Dockerå®¹å™¨åŒ–** - éƒ¨ç½²æ–¹æ¡ˆä¼˜åŒ–
- ğŸ“„ **æŠ¥å‘Šå¯¼å‡ºåŠŸèƒ½** - å¤šæ ¼å¼è¾“å‡ºæ”¯æŒ
- ğŸ› **Bugä¿®å¤** - ç³»ç»Ÿç¨³å®šæ€§æå‡
- ğŸ”§ **ä»£ç ä¼˜åŒ–** - ç”¨æˆ·ä½“éªŒæ”¹è¿›
- ğŸ“ **æ–‡æ¡£å®Œå–„** - ä½¿ç”¨æŒ‡å—å’Œæ•™ç¨‹
- ğŸŒ **ç¤¾åŒºå»ºè®¾** - é—®é¢˜åé¦ˆå’Œæ¨å¹¿
- **ğŸŒ å¼€æºè´¡çŒ®**: æ„Ÿè°¢æ‚¨ä»¬é€‰æ‹©Apache 2.0åè®®ï¼Œç»™äºˆå¼€å‘è€…æœ€å¤§çš„è‡ªç”±
- **ğŸ“š çŸ¥è¯†åˆ†äº«**: æ„Ÿè°¢æ‚¨ä»¬æä¾›çš„è¯¦ç»†æ–‡æ¡£å’Œæœ€ä½³å®è·µæŒ‡å¯¼

**ç‰¹åˆ«æ„Ÿè°¢**ï¼š[TradingAgents](https://github.com/TauricResearch/TradingAgents) é¡¹ç›®ä¸ºæˆ‘ä»¬æä¾›äº†åšå®çš„æŠ€æœ¯åŸºç¡€ã€‚è™½ç„¶Apache 2.0åè®®èµ‹äºˆäº†æˆ‘ä»¬ä½¿ç”¨æºç çš„æƒåˆ©ï¼Œä½†æˆ‘ä»¬æ·±çŸ¥æ¯ä¸€è¡Œä»£ç çš„çè´µä»·å€¼ï¼Œå°†æ°¸è¿œé“­è®°å¹¶æ„Ÿè°¢æ‚¨ä»¬çš„æ— ç§è´¡çŒ®ã€‚

### ğŸ‡¨ğŸ‡³ æ¨å¹¿ä½¿å‘½çš„åˆå¿ƒ

åˆ›å»ºè¿™ä¸ªä¸­æ–‡å¢å¼ºç‰ˆæœ¬ï¼Œæˆ‘ä»¬æ€€ç€ä»¥ä¸‹åˆå¿ƒï¼š

- **ğŸŒ‰ æŠ€æœ¯ä¼ æ’­**: è®©ä¼˜ç§€çš„TradingAgentsæŠ€æœ¯åœ¨ä¸­å›½å¾—åˆ°æ›´å¹¿æ³›çš„åº”ç”¨
- **ğŸ“ æ•™è‚²æ™®åŠ**: ä¸ºä¸­å›½çš„AIé‡‘èæ•™è‚²æä¾›æ›´å¥½çš„å·¥å…·å’Œèµ„æº
- **ğŸ¤ æ–‡åŒ–æ¡¥æ¢**: åœ¨ä¸­è¥¿æ–¹æŠ€æœ¯ç¤¾åŒºä¹‹é—´æ­å»ºäº¤æµåˆä½œçš„æ¡¥æ¢
- **ğŸš€ åˆ›æ–°æ¨åŠ¨**: æ¨åŠ¨ä¸­å›½é‡‘èç§‘æŠ€é¢†åŸŸçš„AIæŠ€æœ¯åˆ›æ–°å’Œåº”ç”¨

### ğŸŒ å¼€æºç¤¾åŒº

æ„Ÿè°¢æ‰€æœ‰ä¸ºæœ¬é¡¹ç›®è´¡çŒ®ä»£ç ã€æ–‡æ¡£ã€å»ºè®®å’Œåé¦ˆçš„å¼€å‘è€…å’Œç”¨æˆ·ã€‚æ­£æ˜¯å› ä¸ºæœ‰äº†å¤§å®¶çš„æ”¯æŒï¼Œæˆ‘ä»¬æ‰èƒ½æ›´å¥½åœ°æœåŠ¡ä¸­æ–‡ç”¨æˆ·ç¤¾åŒºã€‚

### ğŸ¤ åˆä½œå…±èµ¢

æˆ‘ä»¬æ‰¿è¯ºï¼š

- **å°Šé‡åŸåˆ›**: å§‹ç»ˆå°Šé‡æºé¡¹ç›®çš„çŸ¥è¯†äº§æƒå’Œå¼€æºåè®®
- **åé¦ˆè´¡çŒ®**: å°†æœ‰ä»·å€¼çš„æ”¹è¿›å’Œåˆ›æ–°åé¦ˆç»™æºé¡¹ç›®å’Œå¼€æºç¤¾åŒº
- **æŒç»­æ”¹è¿›**: ä¸æ–­å®Œå–„ä¸­æ–‡å¢å¼ºç‰ˆæœ¬ï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ
- **å¼€æ”¾åˆä½œ**: æ¬¢è¿ä¸æºé¡¹ç›®å›¢é˜Ÿå’Œå…¨çƒå¼€å‘è€…è¿›è¡ŒæŠ€æœ¯äº¤æµä¸åˆä½œ

## ğŸ“ˆ ç‰ˆæœ¬å†å²

- **v0.1.13** (2025-08-02): ğŸ¤– åŸç”ŸOpenAIæ”¯æŒä¸Google AIç”Ÿæ€ç³»ç»Ÿå…¨é¢é›†æˆ âœ¨ **æœ€æ–°ç‰ˆæœ¬**
- **v0.1.12** (2025-07-29): ğŸ§  æ™ºèƒ½æ–°é—»åˆ†ææ¨¡å—ä¸é¡¹ç›®ç»“æ„ä¼˜åŒ–
- **v0.1.11** (2025-07-27): ğŸ¤– å¤šLLMæä¾›å•†é›†æˆä¸æ¨¡å‹é€‰æ‹©æŒä¹…åŒ–
- **v0.1.10** (2025-07-18): ğŸš€ Webç•Œé¢å®æ—¶è¿›åº¦æ˜¾ç¤ºä¸æ™ºèƒ½ä¼šè¯ç®¡ç†
- **v0.1.9** (2025-07-16): ğŸ¯ CLIç”¨æˆ·ä½“éªŒé‡å¤§ä¼˜åŒ–ä¸ç»Ÿä¸€æ—¥å¿—ç®¡ç†
- **v0.1.8** (2025-07-15): ğŸ¨ Webç•Œé¢å…¨é¢ä¼˜åŒ–ä¸ç”¨æˆ·ä½“éªŒæå‡
- **v0.1.7** (2025-07-13): ğŸ³ å®¹å™¨åŒ–éƒ¨ç½²ä¸ä¸“ä¸šæŠ¥å‘Šå¯¼å‡º
- **v0.1.6** (2025-07-11): ğŸ”§ é˜¿é‡Œç™¾ç‚¼ä¿®å¤ä¸æ•°æ®æºå‡çº§
- **v0.1.5** (2025-07-08): ğŸ“Š æ·»åŠ Deepseekæ¨¡å‹æ”¯æŒ
- **v0.1.4** (2025-07-05): ğŸ—ï¸ æ¶æ„ä¼˜åŒ–ä¸é…ç½®ç®¡ç†é‡æ„
- **v0.1.3** (2025-06-28): ğŸ‡¨ğŸ‡³ Aè‚¡å¸‚åœºå®Œæ•´æ”¯æŒ
- **v0.1.2** (2025-06-15): ğŸŒ Webç•Œé¢å’Œé…ç½®ç®¡ç†
- **v0.1.1** (2025-06-01): ğŸ§  å›½äº§LLMé›†æˆ

ğŸ“‹ **è¯¦ç»†æ›´æ–°æ—¥å¿—**: [CHANGELOG.md](./docs/releases/CHANGELOG.md)

## ğŸ“ è”ç³»æ–¹å¼

- **GitHub Issues**: [æäº¤é—®é¢˜å’Œå»ºè®®](https://github.com/hsliuping/TradingAgents-CN/issues)
- **é‚®ç®±**: hsliup@163.com
- é¡¹ç›®ï¼±ï¼±ç¾¤ï¼š1009816091
- é¡¹ç›®å¾®ä¿¡å…¬ä¼—å·ï¼šTradingAgents-CN

  &lt;img src=&quot;assets/wexin.png&quot; alt=&quot;å¾®ä¿¡å…¬ä¼—å·&quot; width=&quot;200&quot;/&gt;

- **åŸé¡¹ç›®**: [TauricResearch/TradingAgents](https://github.com/TauricResearch/TradingAgents)
- **æ–‡æ¡£**: [å®Œæ•´æ–‡æ¡£ç›®å½•](docs/)

## âš ï¸ é£é™©æç¤º

**é‡è¦å£°æ˜**: æœ¬æ¡†æ¶ä»…ç”¨äºç ”ç©¶å’Œæ•™è‚²ç›®çš„ï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚

- ğŸ“Š äº¤æ˜“è¡¨ç°å¯èƒ½å› å¤šç§å› ç´ è€Œå¼‚
- ğŸ¤– AIæ¨¡å‹çš„é¢„æµ‹å­˜åœ¨ä¸ç¡®å®šæ€§
- ğŸ’° æŠ•èµ„æœ‰é£é™©ï¼Œå†³ç­–éœ€è°¨æ…
- ğŸ‘¨â€ğŸ’¼ å»ºè®®å’¨è¯¢ä¸“ä¸šè´¢åŠ¡é¡¾é—®

---

&lt;div align=&quot;center&quot;&gt;

**ğŸŒŸ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼**

[â­ Star this repo](https://github.com/hsliuping/TradingAgents-CN) | [ğŸ´ Fork this repo](https://github.com/hsliuping/TradingAgents-CN/fork) | [ğŸ“– Read the docs](./docs/)

&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>