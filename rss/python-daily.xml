<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Fri, 23 May 2025 00:04:31 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[All-Hands-AI/OpenHands]]></title>
            <link>https://github.com/All-Hands-AI/OpenHands</link>
            <guid>https://github.com/All-Hands-AI/OpenHands</guid>
            <pubDate>Fri, 23 May 2025 00:04:31 GMT</pubDate>
            <description><![CDATA[ğŸ™Œ OpenHands: Code Less, Make More]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/All-Hands-AI/OpenHands">All-Hands-AI/OpenHands</a></h1>
            <p>ğŸ™Œ OpenHands: Code Less, Make More</p>
            <p>Language: Python</p>
            <p>Stars: 55,164</p>
            <p>Forks: 6,202</p>
            <p>Stars today: 402 stars today</p>
            <h2>README</h2><pre>&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./docs/static/img/logo.png&quot; alt=&quot;Logo&quot; width=&quot;200&quot;&gt;
  &lt;h1 align=&quot;center&quot;&gt;OpenHands: Code Less, Make More&lt;/h1&gt;
&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/All-Hands-AI/OpenHands/graphs/contributors&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/All-Hands-AI/OpenHands?style=for-the-badge&amp;color=blue&quot; alt=&quot;Contributors&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/All-Hands-AI/OpenHands/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/All-Hands-AI/OpenHands?style=for-the-badge&amp;color=blue&quot; alt=&quot;Stargazers&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/All-Hands-AI/OpenHands/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/All-Hands-AI/OpenHands?style=for-the-badge&amp;color=blue&quot; alt=&quot;MIT License&quot;&gt;&lt;/a&gt;
  &lt;br/&gt;
  &lt;a href=&quot;https://join.slack.com/t/openhands-ai/shared_invite/zt-34zm4j0gj-Qz5kRHoca8DFCbqXPS~f_A&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Slack-Join%20Us-red?logo=slack&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Join our Slack community&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/ESHStjSjD4&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Join our Discord community&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/All-Hands-AI/OpenHands/blob/main/CREDITS.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Project-Credits-blue?style=for-the-badge&amp;color=FFE165&amp;logo=github&amp;logoColor=white&quot; alt=&quot;Credits&quot;&gt;&lt;/a&gt;
  &lt;br/&gt;
  &lt;a href=&quot;https://docs.all-hands.dev/modules/usage/getting-started&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;logoColor=FFE165&amp;style=for-the-badge&quot; alt=&quot;Check out the documentation&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://arxiv.org/abs/2407.16741&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Paper%20on%20Arxiv-000?logoColor=FFE165&amp;logo=arxiv&amp;style=for-the-badge&quot; alt=&quot;Paper on Arxiv&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1wOUdFCMyY6Nt0AIqF705KN4JKOWgeI4wUGUP60krXXs/edit?gid=0#gid=0&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Benchmark%20score-000?logoColor=FFE165&amp;logo=huggingface&amp;style=for-the-badge&quot; alt=&quot;Evaluation Benchmark Score&quot;&gt;&lt;/a&gt;
  &lt;hr&gt;
&lt;/div&gt;

Welcome to OpenHands (formerly OpenDevin), a platform for software development agents powered by AI.

OpenHands agents can do anything a human developer can: modify code, run commands, browse the web,
call APIs, and yesâ€”even copy code snippets from StackOverflow.

Learn more at [docs.all-hands.dev](https://docs.all-hands.dev), or [sign up for OpenHands Cloud](https://app.all-hands.dev) to get started.

&gt; [!IMPORTANT]
&gt; Using OpenHands for work? We&#039;d love to chat! Fill out
&gt; [this short form](https://docs.google.com/forms/d/e/1FAIpQLSet3VbGaz8z32gW9Wm-Grl4jpt5WgMXPgJ4EDPVmCETCBpJtQ/viewform)
&gt; to join our Design Partner program, where you&#039;ll get early access to commercial features and the opportunity to provide input on our product roadmap.

![App screenshot](./docs/static/img/screenshot.png)

## â˜ï¸ OpenHands Cloud
The easiest way to get started with OpenHands is on [OpenHands Cloud](https://app.all-hands.dev),
which comes with $50 in free credits for new users.

## ğŸ’» Running OpenHands Locally

OpenHands can also run on your local system using Docker.
See the [Running OpenHands](https://docs.all-hands.dev/modules/usage/installation) guide for
system requirements and more information.

&gt; [!WARNING]
&gt; On a public network? See our [Hardened Docker Installation Guide](https://docs.all-hands.dev/modules/usage/runtimes/docker#hardened-docker-installation)
&gt; to secure your deployment by restricting network binding and implementing additional security measures.


```bash
docker pull docker.all-hands.dev/all-hands-ai/runtime:0.39-nikolaik

docker run -it --rm --pull=always \
    -e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.39-nikolaik \
    -e LOG_ALL_EVENTS=true \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -v ~/.openhands-state:/.openhands-state \
    -p 3000:3000 \
    --add-host host.docker.internal:host-gateway \
    --name openhands-app \
    docker.all-hands.dev/all-hands-ai/openhands:0.39
```

You&#039;ll find OpenHands running at [http://localhost:3000](http://localhost:3000)!

When you open the application, you&#039;ll be asked to choose an LLM provider and add an API key.
[Anthropic&#039;s Claude 3.7 Sonnet](https://www.anthropic.com/api) (`anthropic/claude-3-7-sonnet-20250219`)
works best, but you have [many options](https://docs.all-hands.dev/modules/usage/llms).

## ğŸ’¡ Other ways to run OpenHands

&gt; [!CAUTION]
&gt; OpenHands is meant to be run by a single user on their local workstation.
&gt; It is not appropriate for multi-tenant deployments where multiple users share the same instance. There is no built-in authentication, isolation, or scalability.
&gt;
&gt; If you&#039;re interested in running OpenHands in a multi-tenant environment, please
&gt; [get in touch with us](https://docs.google.com/forms/d/e/1FAIpQLSet3VbGaz8z32gW9Wm-Grl4jpt5WgMXPgJ4EDPVmCETCBpJtQ/viewform)
&gt; for advanced deployment options.

You can also [connect OpenHands to your local filesystem](https://docs.all-hands.dev/modules/usage/runtimes/docker#connecting-to-your-filesystem),
run OpenHands in a scriptable [headless mode](https://docs.all-hands.dev/modules/usage/how-to/headless-mode),
interact with it via a [friendly CLI](https://docs.all-hands.dev/modules/usage/how-to/cli-mode),
or run it on tagged issues with [a github action](https://docs.all-hands.dev/modules/usage/how-to/github-action).

Visit [Running OpenHands](https://docs.all-hands.dev/modules/usage/installation) for more information and setup instructions.

If you want to modify the OpenHands source code, check out [Development.md](https://github.com/All-Hands-AI/OpenHands/blob/main/Development.md).

Having issues? The [Troubleshooting Guide](https://docs.all-hands.dev/modules/usage/troubleshooting) can help.

## ğŸ“– Documentation
  &lt;a href=&quot;https://deepwiki.com/All-Hands-AI/OpenHands&quot;&gt;&lt;img src=&quot;https://deepwiki.com/badge.svg&quot; alt=&quot;Ask DeepWiki&quot; title=&quot;Autogenerated Documentation by DeepWiki&quot;&gt;&lt;/a&gt;

To learn more about the project, and for tips on using OpenHands,
check out our [documentation](https://docs.all-hands.dev/modules/usage/getting-started).

There you&#039;ll find resources on how to use different LLM providers,
troubleshooting resources, and advanced configuration options.

## ğŸ¤ How to Join the Community

OpenHands is a community-driven project, and we welcome contributions from everyone. We do most of our communication
through Slack, so this is the best place to start, but we also are happy to have you contact us on Discord or Github:

- [Join our Slack workspace](https://join.slack.com/t/openhands-ai/shared_invite/zt-34zm4j0gj-Qz5kRHoca8DFCbqXPS~f_A) - Here we talk about research, architecture, and future development.
- [Join our Discord server](https://discord.gg/ESHStjSjD4) - This is a community-run server for general discussion, questions, and feedback.
- [Read or post Github Issues](https://github.com/All-Hands-AI/OpenHands/issues) - Check out the issues we&#039;re working on, or add your own ideas.

See more about the community in [COMMUNITY.md](./COMMUNITY.md) or find details on contributing in [CONTRIBUTING.md](./CONTRIBUTING.md).

## ğŸ“ˆ Progress

See the monthly OpenHands roadmap [here](https://github.com/orgs/All-Hands-AI/projects/1) (updated at the maintainer&#039;s meeting at the end of each month).

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://star-history.com/#All-Hands-AI/OpenHands&amp;Date&quot;&gt;
    &lt;img src=&quot;https://api.star-history.com/svg?repos=All-Hands-AI/OpenHands&amp;type=Date&quot; width=&quot;500&quot; alt=&quot;Star History Chart&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

## ğŸ“œ License

Distributed under the MIT License. See [`LICENSE`](./LICENSE) for more information.

## ğŸ™ Acknowledgements

OpenHands is built by a large number of contributors, and every contribution is greatly appreciated! We also build upon other open source projects, and we are deeply thankful for their work.

For a list of open source projects and licenses used in OpenHands, please see our [CREDITS.md](./CREDITS.md) file.

## ğŸ“š Cite

```
@misc{openhands,
      title={{OpenHands: An Open Platform for AI Software Developers as Generalist Agents}},
      author={Xingyao Wang and Boxuan Li and Yufan Song and Frank F. Xu and Xiangru Tang and Mingchen Zhuge and Jiayi Pan and Yueqi Song and Bowen Li and Jaskirat Singh and Hoang H. Tran and Fuqiang Li and Ren Ma and Mingzhang Zheng and Bill Qian and Yanjun Shao and Niklas Muennighoff and Yizhe Zhang and Binyuan Hui and Junyang Lin and Robert Brennan and Hao Peng and Heng Ji and Graham Neubig},
      year={2024},
      eprint={2407.16741},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2407.16741},
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[virattt/ai-hedge-fund]]></title>
            <link>https://github.com/virattt/ai-hedge-fund</link>
            <guid>https://github.com/virattt/ai-hedge-fund</guid>
            <pubDate>Fri, 23 May 2025 00:04:30 GMT</pubDate>
            <description><![CDATA[An AI Hedge Fund Team]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/virattt/ai-hedge-fund">virattt/ai-hedge-fund</a></h1>
            <p>An AI Hedge Fund Team</p>
            <p>Language: Python</p>
            <p>Stars: 32,295</p>
            <p>Forks: 5,543</p>
            <p>Stars today: 445 stars today</p>
            <h2>README</h2><pre># AI Hedge Fund

This is a proof of concept for an AI-powered hedge fund.  The goal of this project is to explore the use of AI to make trading decisions.  This project is for **educational** purposes only and is not intended for real trading or investment.

This system employs several agents working together:

1. Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation
2. Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety
3. Bill Ackman Agent - An activist investor, takes bold positions and pushes for change
4. Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption
5. Charlie Munger Agent - Warren Buffett&#039;s partner, only buys wonderful businesses at fair prices
6. Michael Burry Agent - The Big Short contrarian who hunts for deep value
7. Peter Lynch Agent - Practical investor who seeks &quot;ten-baggers&quot; in everyday businesses
8. Phil Fisher Agent - Meticulous growth investor who uses deep &quot;scuttlebutt&quot; research 
9. Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential
10. Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price
11. Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals
12. Sentiment Agent - Analyzes market sentiment and generates trading signals
13. Fundamentals Agent - Analyzes fundamental data and generates trading signals
14. Technicals Agent - Analyzes technical indicators and generates trading signals
15. Risk Manager - Calculates risk metrics and sets position limits
16. Portfolio Manager - Makes final trading decisions and generates orders
    
&lt;img width=&quot;1042&quot; alt=&quot;Screenshot 2025-03-22 at 6 19 07 PM&quot; src=&quot;https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4&quot; /&gt;


**Note**: the system simulates trading decisions, it does not actually trade.

[![Twitter Follow](https://img.shields.io/twitter/follow/virattt?style=social)](https://twitter.com/virattt)

## Disclaimer

This project is for **educational and research purposes only**.

- Not intended for real trading or investment
- No investment advice or guarantees provided
- Creator assumes no liability for financial losses
- Consult a financial advisor for investment decisions
- Past performance does not indicate future results

By using this software, you agree to use it solely for learning purposes.

## Table of Contents
- [Setup](#setup)
  - [Using Poetry](#using-poetry)
  - [Using Docker](#using-docker)
- [Usage](#usage)
  - [Running the Hedge Fund](#running-the-hedge-fund)
  - [Running the Backtester](#running-the-backtester)
- [Project Structure](#project-structure)
- [Contributing](#contributing)
- [Feature Requests](#feature-requests)
- [License](#license)

## Setup

### Using Poetry

Clone the repository:
```bash
git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
```

1. Install Poetry (if not already installed):
```bash
curl -sSL https://install.python-poetry.org | python3 -
```

2. Install dependencies:
```bash
poetry install
```

3. Set up your environment variables:
```bash
# Create .env file for your API keys
cp .env.example .env
```

4. Set your API keys:
```bash
# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
# Get your OpenAI API key from https://platform.openai.com/
OPENAI_API_KEY=your-openai-api-key

# For running LLMs hosted by groq (deepseek, llama3, etc.)
# Get your Groq API key from https://groq.com/
GROQ_API_KEY=your-groq-api-key

# For getting financial data to power the hedge fund
# Get your Financial Datasets API key from https://financialdatasets.ai/
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
```

### Using Docker

1. Make sure you have Docker installed on your system. If not, you can download it from [Docker&#039;s official website](https://www.docker.com/get-started).

2. Clone the repository:
```bash
git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
```

3. Set up your environment variables:
```bash
# Create .env file for your API keys
cp .env.example .env
```

4. Edit the .env file to add your API keys as described above.

5. Build the Docker image:
```bash
# On Linux/Mac:
./run.sh build

# On Windows:
run.bat build
```

**Important**: You must set `OPENAI_API_KEY`, `GROQ_API_KEY`, `ANTHROPIC_API_KEY`, or `DEEPSEEK_API_KEY` for the hedge fund to work.  If you want to use LLMs from all providers, you will need to set all API keys.

Financial data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key.

For any other ticker, you will need to set the `FINANCIAL_DATASETS_API_KEY` in the .env file.

## Usage

### Running the Hedge Fund

#### With Poetry
```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA
```

#### With Docker
```bash
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA main

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA main
```

**Example Output:**
&lt;img width=&quot;992&quot; alt=&quot;Screenshot 2025-01-06 at 5 50 17 PM&quot; src=&quot;https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b&quot; /&gt;

You can also specify a `--ollama` flag to run the AI hedge fund using local LLMs.

```bash
# With Poetry:
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama

# With Docker (on Linux/Mac):
./run.sh --ticker AAPL,MSFT,NVDA --ollama main

# With Docker (on Windows):
run.bat --ticker AAPL,MSFT,NVDA --ollama main
```

You can also specify a `--show-reasoning` flag to print the reasoning of each agent to the console.

```bash
# With Poetry:
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --show-reasoning

# With Docker (on Linux/Mac):
./run.sh --ticker AAPL,MSFT,NVDA --show-reasoning main

# With Docker (on Windows):
run.bat --ticker AAPL,MSFT,NVDA --show-reasoning main
```

You can optionally specify the start and end dates to make decisions for a specific time period.

```bash
# With Poetry:
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 

# With Docker (on Linux/Mac):
./run.sh --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 main

# With Docker (on Windows):
run.bat --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 main
```

### Running the Backtester

#### With Poetry
```bash
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
```

#### With Docker
```bash
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA backtest

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA backtest
```

**Example Output:**
&lt;img width=&quot;941&quot; alt=&quot;Screenshot 2025-01-06 at 5 47 52 PM&quot; src=&quot;https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47&quot; /&gt;


You can optionally specify the start and end dates to backtest over a specific time period.

```bash
# With Poetry:
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01

# With Docker (on Linux/Mac):
./run.sh --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 backtest

# With Docker (on Windows):
run.bat --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 backtest
```

You can also specify a `--ollama` flag to run the backtester using local LLMs.
```bash
# With Poetry:
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --ollama

# With Docker (on Linux/Mac):
./run.sh --ticker AAPL,MSFT,NVDA --ollama backtest

# With Docker (on Windows):
run.bat --ticker AAPL,MSFT,NVDA --ollama backtest
```


## Project Structure 
```
ai-hedge-fund/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/                   # Agent definitions and workflow
â”‚   â”‚   â”œâ”€â”€ bill_ackman.py        # Bill Ackman agent
â”‚   â”‚   â”œâ”€â”€ fundamentals.py       # Fundamental analysis agent
â”‚   â”‚   â”œâ”€â”€ portfolio_manager.py  # Portfolio management agent
â”‚   â”‚   â”œâ”€â”€ risk_manager.py       # Risk management agent
â”‚   â”‚   â”œâ”€â”€ sentiment.py          # Sentiment analysis agent
â”‚   â”‚   â”œâ”€â”€ technicals.py         # Technical analysis agent
â”‚   â”‚   â”œâ”€â”€ valuation.py          # Valuation analysis agent
â”‚   â”‚   â”œâ”€â”€ ...                   # Other agents
â”‚   â”‚   â”œâ”€â”€ warren_buffett.py     # Warren Buffett agent
â”‚   â”‚   â”œâ”€â”€ aswath_damodaran.py   # Aswath Damodaran agent
â”‚   â”‚   â”œâ”€â”€ ...                   # Other agents
â”‚   â”‚   â”œâ”€â”€ ...                   # Other agents
â”‚   â”œâ”€â”€ tools/                    # Agent tools
â”‚   â”‚   â”œâ”€â”€ api.py                # API tools
â”‚   â”œâ”€â”€ backtester.py             # Backtesting tools
â”‚   â”œâ”€â”€ main.py # Main entry point
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ ...
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

**Important**: Please keep your pull requests small and focused.  This will make it easier to review and merge.

## Feature Requests

If you have a feature request, please open an [issue](https://github.com/virattt/ai-hedge-fund/issues) and make sure it is tagged with `enhancement`.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pytorch/torchrec]]></title>
            <link>https://github.com/pytorch/torchrec</link>
            <guid>https://github.com/pytorch/torchrec</guid>
            <pubDate>Fri, 23 May 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[Pytorch domain library for recommendation systems]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pytorch/torchrec">pytorch/torchrec</a></h1>
            <p>Pytorch domain library for recommendation systems</p>
            <p>Language: Python</p>
            <p>Stars: 2,191</p>
            <p>Forks: 515</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/qlib]]></title>
            <link>https://github.com/microsoft/qlib</link>
            <guid>https://github.com/microsoft/qlib</guid>
            <pubDate>Fri, 23 May 2025 00:04:28 GMT</pubDate>
            <description><![CDATA[Qlib is an AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms. including supervised learning, market dynamics modeling, and RL.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/qlib">microsoft/qlib</a></h1>
            <p>Qlib is an AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms. including supervised learning, market dynamics modeling, and RL.</p>
            <p>Language: Python</p>
            <p>Stars: 19,763</p>
            <p>Forks: 3,233</p>
            <p>Stars today: 333 stars today</p>
            <h2>README</h2><pre>[![Python Versions](https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&amp;logoColor=white)](https://pypi.org/project/pyqlib/#files)
[![Platform](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey)](https://pypi.org/project/pyqlib/#files)
[![PypI Versions](https://img.shields.io/pypi/v/pyqlib)](https://pypi.org/project/pyqlib/#history)
[![Upload Python Package](https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/pyqlib/)
[![Github Actions Test Status](https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main)](https://github.com/microsoft/qlib/actions)
[![Documentation Status](https://readthedocs.org/projects/qlib/badge/?version=latest)](https://qlib.readthedocs.io/en/latest/?badge=latest)
[![License](https://img.shields.io/pypi/l/pyqlib)](LICENSE)
[![Join the chat at https://gitter.im/Microsoft/qlib](https://badges.gitter.im/Microsoft/qlib.svg)](https://gitter.im/Microsoft/qlib?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge)

## :newspaper: **What&#039;s NEW!** &amp;nbsp;   :sparkling_heart: 

Recent released features

### Introducing &lt;a href=&quot;https://github.com/microsoft/RD-Agent&quot;&gt;&lt;img src=&quot;docs/_static/img/rdagent_logo.png&quot; alt=&quot;RD_Agent&quot; style=&quot;height: 2em&quot;&gt;&lt;/a&gt;: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&amp;D

We are excited to announce the release of **RD-Agent**ğŸ“¢, a powerful tool that supports automated factor mining and model optimization in quant investment R&amp;D.

RD-Agent is now available on [GitHub](https://github.com/microsoft/RD-Agent), and we welcome your starğŸŒŸ!

To learn more, please visit our [â™¾ï¸Demo page](https://rdagent.azurewebsites.net/). Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.

We have prepared several demo videos for you:
| Scenario | Demo video (English) | Demo video (ä¸­æ–‡) |
| --                      | ------    | ------    |
| Quant Factor Mining | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=zh) |
| Quant Factor Mining from reports | [Link](https://rdagent.azurewebsites.net/report_factor?lang=en) | [Link](https://rdagent.azurewebsites.net/report_factor?lang=zh) |
| Quant Model Optimization | [Link](https://rdagent.azurewebsites.net/model_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/model_loop?lang=zh) |

- [R&amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization](https://arxiv.org/abs/2505.15155)
```BibTeX
@misc{li2025rdagentquant,
    title={R\&amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},
    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},
    year={2025},
    eprint={2505.15155},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
```
![image](https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d)

***

| Feature | Status |
| --                      | ------    |
| [R&amp;D-Agent-Quant](https://arxiv.org/abs/2505.15155) Published | Apply R&amp;D-Agent to Qlib for quant trading | 
| BPQP for End-to-end learning | ğŸ“ˆComing soon!([Under review](https://github.com/microsoft/qlib/pull/1863)) |
| ğŸ”¥LLM-driven Auto Quant FactoryğŸ”¥ | ğŸš€ Released in [â™¾ï¸RD-Agent](https://github.com/microsoft/RD-Agent) on Aug 8, 2024 |
| KRNN and Sandwich models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1414/) on May 26, 2023 |
| Release Qlib v0.9.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.9.0) on Dec 9, 2022 |
| RL Learning Framework | :hammer: :chart_with_upwards_trend: Released on Nov 10, 2022. [#1332](https://github.com/microsoft/qlib/pull/1332), [#1322](https://github.com/microsoft/qlib/pull/1322), [#1316](https://github.com/microsoft/qlib/pull/1316),[#1299](https://github.com/microsoft/qlib/pull/1299),[#1263](https://github.com/microsoft/qlib/pull/1263), [#1244](https://github.com/microsoft/qlib/pull/1244), [#1169](https://github.com/microsoft/qlib/pull/1169), [#1125](https://github.com/microsoft/qlib/pull/1125), [#1076](https://github.com/microsoft/qlib/pull/1076)|
| HIST and IGMTF models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1040) on Apr 10, 2022 |
| Qlib [notebook tutorial](https://github.com/microsoft/qlib/tree/main/examples/tutorial) | ğŸ“– [Released](https://github.com/microsoft/qlib/pull/1037) on Apr 7, 2022 | 
| Ibovespa index data | :rice: [Released](https://github.com/microsoft/qlib/pull/990) on Apr 6, 2022 |
| Point-in-Time database | :hammer: [Released](https://github.com/microsoft/qlib/pull/343) on Mar 10, 2022 |
| Arctic Provider Backend &amp; Orderbook data example | :hammer: [Released](https://github.com/microsoft/qlib/pull/744) on Jan 17, 2022 |
| Meta-Learning-based framework &amp; DDG-DA  | :chart_with_upwards_trend:  :hammer: [Released](https://github.com/microsoft/qlib/pull/743) on Jan 10, 2022 | 
| Planning-based portfolio optimization | :hammer: [Released](https://github.com/microsoft/qlib/pull/754) on Dec 28, 2021 | 
| Release Qlib v0.8.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.8.0) on Dec 8, 2021 |
| ADD model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/704) on Nov 22, 2021 |
| ADARNN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/689) on Nov 14, 2021 |
| TCN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/668) on Nov 4, 2021 |
| Nested Decision Framework | :hammer: [Released](https://github.com/microsoft/qlib/pull/438) on Oct 1, 2021. [Example](https://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/workflow.py) and [Doc](https://qlib.readthedocs.io/en/latest/component/highfreq.html) |
| Temporal Routing Adaptor (TRA) | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/531) on July 30, 2021 |
| Transformer &amp; Localformer | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/508) on July 22, 2021 |
| Release Qlib v0.7.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.7.0) on July 12, 2021 |
| TCTS Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/491) on July 1, 2021 |
| Online serving and automatic model rolling | :hammer:  [Released](https://github.com/microsoft/qlib/pull/290) on May 17, 2021 | 
| DoubleEnsemble Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/286) on Mar 2, 2021 | 
| High-frequency data processing example | :hammer: [Released](https://github.com/microsoft/qlib/pull/257) on Feb 5, 2021  |
| High-frequency trading example | :chart_with_upwards_trend: [Part of code released](https://github.com/microsoft/qlib/pull/227) on Jan 28, 2021  | 
| High-frequency data(1min) | :rice: [Released](https://github.com/microsoft/qlib/pull/221) on Jan 27, 2021 |
| Tabnet Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/205) on Jan 22, 2021 |

Features released before 2021 are not listed here.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/_static/img/logo/1.png&quot; /&gt;
&lt;/p&gt;

Qlib is an open-source, AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms, including supervised learning, market dynamics modeling, and reinforcement learning.

An increasing number of SOTA Quant research works/papers in diverse paradigms are being released in Qlib to collaboratively solve key challenges in quantitative investment. For example, 1) using supervised learning to mine the market&#039;s complex non-linear patterns from rich and heterogeneous financial data, 2) modeling the dynamic nature of the financial market using adaptive concept drift technology, and 3) using reinforcement learning to model continuous investment decisions and assist investors in optimizing their trading strategies.

It contains the full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution. 
For more details, please refer to our paper [&quot;Qlib: An AI-oriented Quantitative Investment Platform&quot;](https://arxiv.org/abs/2009.11189).


&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Frameworks, Tutorial, Data &amp; DevOps&lt;/th&gt;
      &lt;th&gt;Main Challenges &amp; Solutions in Quant Research&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;li&gt;&lt;a href=&quot;#plans&quot;&gt;&lt;strong&gt;Plans&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#framework-of-qlib&quot;&gt;Framework of Qlib&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#quick-start&quot;&gt;Quick Start&lt;/a&gt;&lt;/li&gt;
          &lt;ul dir=&quot;auto&quot;&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#installation&quot;&gt;Installation&lt;/a&gt; &lt;/li&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#data-preparation&quot;&gt;Data Preparation&lt;/a&gt;&lt;/li&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#auto-quant-research-workflow&quot;&gt;Auto Quant Research Workflow&lt;/a&gt;&lt;/li&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#building-customized-quant-research-workflow-by-code&quot;&gt;Building Customized Quant Research Workflow by Code&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
        &lt;li&gt;&lt;a href=&quot;#quant-dataset-zoo&quot;&gt;&lt;strong&gt;Quant Dataset Zoo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#learning-framework&quot;&gt;Learning Framework&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#more-about-qlib&quot;&gt;More About Qlib&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#offline-mode-and-online-mode&quot;&gt;Offline Mode and Online Mode&lt;/a&gt;
        &lt;ul&gt;
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#performance-of-qlib-data-server&quot;&gt;Performance of Qlib Data Server&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
        &lt;li&gt;&lt;a href=&quot;#related-reports&quot;&gt;Related Reports&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#contact-us&quot;&gt;Contact Us&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#contributing&quot;&gt;Contributing&lt;/a&gt;&lt;/li&gt;
      &lt;/td&gt;
      &lt;td valign=&quot;baseline&quot;&gt;
        &lt;li&gt;&lt;a href=&quot;#main-challenges--solutions-in-quant-research&quot;&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/a&gt;
          &lt;ul&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#forecasting-finding-valuable-signalspatterns&quot;&gt;Forecasting: Finding Valuable Signals/Patterns&lt;/a&gt;
              &lt;ul&gt;
                &lt;li type=&quot;disc&quot;&gt;&lt;a href=&quot;#quant-model-paper-zoo&quot;&gt;&lt;strong&gt;Quant Model (Paper) Zoo&lt;/strong&gt;&lt;/a&gt;
                  &lt;ul&gt;
                    &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#run-a-single-model&quot;&gt;Run a Single Model&lt;/a&gt;&lt;/li&gt;
                    &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#run-multiple-models&quot;&gt;Run Multiple Models&lt;/a&gt;&lt;/li&gt;
                  &lt;/ul&gt;
                &lt;/li&gt;
              &lt;/ul&gt;
            &lt;/li&gt;
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#adapting-to-market-dynamics&quot;&gt;Adapting to Market Dynamics&lt;/a&gt;&lt;/li&gt;
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#reinforcement-learning-modeling-continuous-decisions&quot;&gt;Reinforcement Learning: modeling continuous decisions&lt;/a&gt;&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

# Plans
New features under development(order by estimated release time).
Your feedbacks about the features are very important.
&lt;!-- | Feature                        | Status      | --&gt;
&lt;!-- | --                      | ------    | --&gt;

# Framework of Qlib

&lt;div style=&quot;align: center&quot;&gt;
&lt;img src=&quot;docs/_static/img/framework-abstract.jpg&quot; /&gt;
&lt;/div&gt;

The high-level framework of Qlib can be found above(users can find the [detailed framework](https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework) of Qlib&#039;s design when getting into nitty gritty).
The components are designed as loose-coupled modules, and each component could be used stand-alone.

Qlib provides a strong infrastructure to support Quant research. [Data](https://qlib.readthedocs.io/en/latest/component/data.html) is always an important part.
A strong learning framework is designed to support diverse learning paradigms (e.g. [reinforcement learning](https://qlib.readthedocs.io/en/latest/component/rl.html), [supervised learning](https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section)) and patterns at different levels(e.g. [market dynamic modeling](https://qlib.readthedocs.io/en/latest/component/meta.html)).
By modeling the market, [trading strategies](https://qlib.readthedocs.io/en/latest/component/strategy.html) will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be [nested to be optimized and run together](https://qlib.readthedocs.io/en/latest/component/highfreq.html).
At last, a comprehensive [analysis](https://qlib.readthedocs.io/en/latest/component/report.html) will be provided and the model can be [served online](https://qlib.readthedocs.io/en/latest/component/online.html) in a low cost.


# Quick Start

This quick start guide tries to demonstrate
1. It&#039;s very easy to build a complete Quant research workflow and try your ideas with _Qlib_.
2. Though with *public data* and *simple models*, machine learning technologies **work very well** in practical Quant investment.

Here is a quick **[demo](https://terminalizer.com/view/3f24561a4470)** shows how to install ``Qlib``, and run LightGBM with ``qrun``. **But**, please make sure you have already prepared the data following the [instruction](#data-preparation).


## Installation

This table demonstrates the supported Python version of `Qlib`:
|               | install with pip      | install from source  |        plot        |
| ------------- |:---------------------:|:--------------------:|:------------------:|
| Python 3.8    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.9    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.10   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.11   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.12   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |

**Note**: 
1. **Conda** is suggested for managing your Python environment. In some cases, using Python outside of a `conda` environment may result in missing header files, causing the installation failure of certain packages.
2. Please pay attention that installing cython in Python 3.6 will raise some error when installing ``Qlib`` from source. If users use Python 3.6 on their machines, it is recommended to *upgrade* Python to version 3.8 or higher, or use `conda`&#039;s Python to install ``Qlib`` from source.

### Install with pip
Users can easily install ``Qlib`` by pip according to the following command.

```bash
  pip install pyqlib
```

**Note**: pip will install the latest stable qlib. However, the main branch of qlib is in active development. If you want to test the latest scripts or functions in the main branch. Please install qlib with the methods below.

### Install from source
Also, users can install the latest dev version ``Qlib`` by the source code according to the following steps:

* Before installing ``Qlib`` from source, users need to install some dependencies:

  ```bash
  pip install numpy
  pip install --upgrade cython
  ```

* Clone the repository and install ``Qlib`` as follows.
    ```bash
    git clone https://github.com/microsoft/qlib.git &amp;&amp; cd qlib
    pip install .  # `pip install -e .[dev]` is recommended for development. check details in docs/developer/code_standard_and_dev_guide.rst
    ```

**Tips**: If you fail to install `Qlib` or run the examples in your environment,  comparing your steps and the [CI workflow](.github/workflows/test_qlib_from_source.yml) may help you find the problem.

**Tips for Mac**: If you are using Mac with M1, you might encounter issues in building the wheel for LightGBM, which is due to missing dependencies from OpenMP. To solve the problem, install openmp first with ``brew install libomp`` and then run ``pip install .`` to build it successfully. 

## Data Preparation
â— Due to more restrict data security policy. The offical dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.
Here is an example to download the latest data.
```bash
wget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz
mkdir -p ~/.qlib/qlib_data/cn_data
tar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1
rm -f qlib_bin.tar.gz
```

The official dataset below will resume in short future.


----

Load and prepare data by running the following code:

### Get with module
  ```bash
  # get 1d data
  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

  # get 1min data
  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

  ```

### Get from source

  ```bash
  # get 1d data
  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

  # get 1min data
  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

  ```

This dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in
the same repository.
Users could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)

*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.
We recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.

### Automatic update of daily frequency data (from yahoo finance)
  &gt; This step is *Optional* if users only want to try their models and strategies on history data.
  &gt; 
  &gt; It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.
  &gt;
  &gt; **NOTE**: Users can&#039;t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.
  &gt; 
  &gt; For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)

  * Automatic update of data to the &quot;qlib&quot; directory each trading day(Linux)
      * use *crontab*: `crontab -e`
      * set up timed tasks:

        ```
        * * * * 1-5 python &lt;script path&gt; update_data_to_bin --qlib_data_1d_dir &lt;user data dir&gt;
        ```
        * **script path**: *scripts/data_collector/yahoo/collector.py*

  * Manual update of data
      ```
      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir &lt;user data dir&gt; --trading_date &lt;start date&gt; --end_date &lt;end date&gt;
      ```
      * *trading_date*: start of trading day
      * *end_date*: end of trading day(not included)

### Checking the health of the data
  * We provide a script to check the health of the data, you can run the following commands to check whether the data is healthy or not.
    ```
    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data
    ```
  * Of course, you can also add some parameters to adjust the test results, such as this.
    ```
    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data --missing_data_num 30055 --large_step_threshold_volume 94

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[PaddlePaddle/PaddleOCR]]></title>
            <link>https://github.com/PaddlePaddle/PaddleOCR</link>
            <guid>https://github.com/PaddlePaddle/PaddleOCR</guid>
            <pubDate>Fri, 23 May 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[Awesome multilingual OCR toolkits based on PaddlePaddle (practical ultra lightweight OCR system, support 80+ languages recognition, provide data annotation and synthesis tools, support training and deployment among server, mobile, embedded and IoT devices)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/PaddlePaddle/PaddleOCR">PaddlePaddle/PaddleOCR</a></h1>
            <p>Awesome multilingual OCR toolkits based on PaddlePaddle (practical ultra lightweight OCR system, support 80+ languages recognition, provide data annotation and synthesis tools, support training and deployment among server, mobile, embedded and IoT devices)</p>
            <p>Language: Python</p>
            <p>Stars: 49,419</p>
            <p>Forks: 8,233</p>
            <p>Stars today: 95 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
      &lt;img width=&quot;100%&quot; src=&quot;./docs/images/Banner_cn.png&quot; alt=&quot;PaddleOCR Banner&quot;&gt;&lt;/a&gt;
  &lt;/p&gt;

&lt;!-- language --&gt;
ä¸­æ–‡ | [English](./README_en.md)

&lt;!-- icon --&gt;

[![stars](https://img.shields.io/github/stars/PaddlePaddle/PaddleOCR?color=ccf)](https://github.com/PaddlePaddle/PaddleOCR)
[![Downloads](https://img.shields.io/pypi/dm/paddleocr)](https://pypi.org/project/PaddleOCR/)
![python](https://img.shields.io/badge/python-3.8~3.12-aff.svg)
![os](https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg)
![hardware](https://img.shields.io/badge/hardware-cpu%2C%20gpu%2C%20xpu%2C%20npu-yellow.svg)


[![Website](https://img.shields.io/badge/Website-PaddleOCR-blue?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAABmmRkdj0AAAAASUVORK5CYII=)](https://www.paddleocr.ai/)
[![AI Studio](https://img.shields.io/badge/PP_OCRv5-AI_Studio-green)](https://aistudio.baidu.com/community/app/91660/webUI)
[![AI Studio](https://img.shields.io/badge/PP_StructureV3-AI_Studio-green)](https://aistudio.baidu.com/community/app/518494/webUI)
[![AI Studio](https://img.shields.io/badge/PP_ChatOCRv4-AI_Studio-green)](https://aistudio.baidu.com/community/app/518493/webUI)

&lt;/div&gt;

## ğŸš€ ç®€ä»‹
PaddleOCRè‡ªå‘å¸ƒä»¥æ¥å‡­å€Ÿå­¦æœ¯å‰æ²¿ç®—æ³•å’Œäº§ä¸šè½åœ°å®è·µï¼Œå—åˆ°äº†äº§å­¦ç ”å„æ–¹çš„å–œçˆ±ï¼Œå¹¶è¢«å¹¿æ³›åº”ç”¨äºä¼—å¤šçŸ¥åå¼€æºé¡¹ç›®ï¼Œä¾‹å¦‚ï¼šUmi-OCRã€OmniParserã€MinerUã€RAGFlowç­‰ï¼Œå·²æˆä¸ºå¹¿å¤§å¼€å‘è€…å¿ƒä¸­çš„å¼€æºOCRé¢†åŸŸçš„é¦–é€‰å·¥å…·ã€‚2025å¹´5æœˆ20æ—¥ï¼Œé£æ¡¨å›¢é˜Ÿå‘å¸ƒ**PaddleOCR 3.0**ï¼Œå…¨é¢é€‚é…**é£æ¡¨æ¡†æ¶3.0æ­£å¼ç‰ˆ**ï¼Œè¿›ä¸€æ­¥**æå‡æ–‡å­—è¯†åˆ«ç²¾åº¦**ï¼Œæ”¯æŒ**å¤šæ–‡å­—ç±»å‹è¯†åˆ«**å’Œ**æ‰‹å†™ä½“è¯†åˆ«**ï¼Œæ»¡è¶³å¤§æ¨¡å‹åº”ç”¨å¯¹**å¤æ‚æ–‡æ¡£é«˜ç²¾åº¦è§£æ**çš„æ—ºç››éœ€æ±‚ï¼Œç»“åˆ**æ–‡å¿ƒå¤§æ¨¡å‹4.5 Turbo**æ˜¾è‘—æå‡å…³é”®ä¿¡æ¯æŠ½å–ç²¾åº¦ï¼Œå¹¶æ–°å¢**å¯¹æ˜†ä»‘èŠ¯ã€æ˜‡è…¾ç­‰å›½äº§ç¡¬ä»¶**çš„æ”¯æŒã€‚

PaddleOCR 3.0**æ–°å¢**ä¸‰å¤§ç‰¹è‰²èƒ½åŠ›ï¼š
- å…¨åœºæ™¯æ–‡å­—è¯†åˆ«æ¨¡å‹[PP-OCRv5](docs/version3.x/algorithm/PP-OCRv5/PP-OCRv5.md)ï¼šå•æ¨¡å‹æ”¯æŒäº”ç§æ–‡å­—ç±»å‹å’Œå¤æ‚æ‰‹å†™ä½“è¯†åˆ«ï¼›æ•´ä½“è¯†åˆ«ç²¾åº¦ç›¸æ¯”ä¸Šä¸€ä»£**æå‡13ä¸ªç™¾åˆ†ç‚¹**ã€‚[åœ¨çº¿ä½“éªŒ](https://aistudio.baidu.com/community/app/91660/webUI)
- é€šç”¨æ–‡æ¡£è§£ææ–¹æ¡ˆ[PP-StructureV3](docs/version3.x/algorithm/PP-StructureV3/PP-StructureV3.md)ï¼šæ”¯æŒå¤šåœºæ™¯ã€å¤šç‰ˆå¼ PDF é«˜ç²¾åº¦è§£æï¼Œåœ¨å…¬å¼€è¯„æµ‹é›†ä¸­**é¢†å…ˆä¼—å¤šå¼€æºå’Œé—­æºæ–¹æ¡ˆ**ã€‚[åœ¨çº¿ä½“éªŒ](https://aistudio.baidu.com/community/app/518494/webUI)
- æ™ºèƒ½æ–‡æ¡£ç†è§£æ–¹æ¡ˆ[PP-ChatOCRv4](docs/version3.x/algorithm/PP-ChatOCRv4/PP-ChatOCRv4.md)ï¼šåŸç”Ÿæ”¯æŒæ–‡å¿ƒå¤§æ¨¡å‹4.5 Turboï¼Œç²¾åº¦ç›¸æ¯”ä¸Šä¸€ä»£**æå‡15ä¸ªç™¾åˆ†ç‚¹**ã€‚[åœ¨çº¿ä½“éªŒ](https://aistudio.baidu.com/community/app/518493/webUI)

PaddleOCR 3.0é™¤äº†æä¾›ä¼˜ç§€çš„æ¨¡å‹åº“å¤–ï¼Œè¿˜æä¾›å¥½å­¦æ˜“ç”¨çš„å·¥å…·ï¼Œè¦†ç›–æ¨¡å‹è®­ç»ƒã€æ¨ç†å’ŒæœåŠ¡åŒ–éƒ¨ç½²ï¼Œæ–¹ä¾¿å¼€å‘è€…å¿«é€Ÿè½åœ°AIåº”ç”¨ã€‚
&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
      &lt;img width=&quot;100%&quot; src=&quot;./docs/images/Arch_cn.png&quot; alt=&quot;PaddleOCR Architecture&quot;&gt;&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;


## ğŸ“£ æœ€æ–°åŠ¨æ€
ğŸ”¥ğŸ”¥2025.05.20: **PaddleOCR 3.0** æ­£å¼å‘å¸ƒï¼ŒåŒ…å«ï¼š
- **PP-OCRv5**: å…¨åœºæ™¯é«˜ç²¾åº¦æ–‡å­—è¯†åˆ«

   1. ğŸŒ å•æ¨¡å‹æ”¯æŒ**äº”ç§**æ–‡å­—ç±»å‹(**ç®€ä½“ä¸­æ–‡**ã€**ç¹ä½“ä¸­æ–‡**ã€**ä¸­æ–‡æ‹¼éŸ³**ã€**è‹±æ–‡**å’Œ**æ—¥æ–‡**)ã€‚
   2. âœï¸ æ”¯æŒå¤æ‚**æ‰‹å†™ä½“**è¯†åˆ«ï¼šå¤æ‚è¿ç¬”ã€éè§„èŒƒå­—è¿¹è¯†åˆ«æ€§èƒ½æ˜¾è‘—æå‡ã€‚
   3. ğŸ¯ æ•´ä½“è¯†åˆ«ç²¾åº¦æå‡ - å¤šç§åº”ç”¨åœºæ™¯è¾¾åˆ° SOTA ç²¾åº¦, ç›¸æ¯”ä¸Šä¸€ç‰ˆæœ¬PP-OCRv4ï¼Œè¯†åˆ«ç²¾åº¦**æå‡13ä¸ªç™¾åˆ†ç‚¹**ï¼

- **PP-StructureV3**: é€šç”¨æ–‡æ¡£è§£ææ–¹æ¡ˆ

   1. ğŸ§® æ”¯æŒå¤šåœºæ™¯ PDF é«˜ç²¾åº¦è§£æï¼Œåœ¨ OmniDocBench åŸºå‡†æµ‹è¯•ä¸­**é¢†å…ˆä¼—å¤šå¼€æºå’Œé—­æºæ–¹æ¡ˆ**ã€‚
   2. ğŸ§  å¤šé¡¹ä¸“ç²¾èƒ½åŠ›: **å°ç« è¯†åˆ«**ã€**å›¾è¡¨è½¬è¡¨æ ¼**ã€**åµŒå¥—å…¬å¼/å›¾ç‰‡çš„è¡¨æ ¼è¯†åˆ«**ã€**ç«–æ’æ–‡æœ¬è§£æ**åŠ**å¤æ‚è¡¨æ ¼ç»“æ„åˆ†æ**ç­‰ã€‚


- **PP-ChatOCRv4**: æ™ºèƒ½æ–‡æ¡£ç†è§£æ–¹æ¡ˆ
   1. ğŸ”¥ æ–‡æ¡£å›¾åƒï¼ˆPDF/PNG/JPGï¼‰å…³é”®ä¿¡æ¯æå–ç²¾åº¦ç›¸æ¯”ä¸Šä¸€ä»£**æå‡15ä¸ªç™¾åˆ†ç‚¹**ï¼
   2. ğŸ’» åŸç”Ÿæ”¯æŒ**æ–‡å¿ƒå¤§æ¨¡å‹4.5 Turbo**ï¼Œè¿˜å…¼å®¹ PaddleNLPã€Ollamaã€vLLM ç­‰å·¥å…·éƒ¨ç½²çš„å¤§æ¨¡å‹ã€‚
   3. ğŸ¤ é›†æˆ [PP-DocBee2](https://github.com/PaddlePaddle/PaddleMIX/tree/develop/paddlemix/examples/ppdocbee2)ï¼Œæ”¯æŒå°åˆ·æ–‡å­—ã€æ‰‹å†™ä½“æ–‡å­—ã€å°ç« ä¿¡æ¯ã€è¡¨æ ¼ã€å›¾è¡¨ç­‰å¸¸è§çš„å¤æ‚æ–‡æ¡£ä¿¡æ¯æŠ½å–å’Œç†è§£çš„èƒ½åŠ›ã€‚


## âš¡ å¿«é€Ÿå¼€å§‹
### 1. åœ¨çº¿ä½“éªŒ
[![AI Studio](https://img.shields.io/badge/PP_OCRv5-AI_Studio-green)](https://aistudio.baidu.com/community/app/91660/webUI)
[![AI Studio](https://img.shields.io/badge/PP_StructureV3-AI_Studio-green)](https://aistudio.baidu.com/community/app/518494/webUI)
[![AI Studio](https://img.shields.io/badge/PP_ChatOCRv4-AI_Studio-green)](https://aistudio.baidu.com/community/app/518493/webUI)

### 2. æœ¬åœ°å®‰è£…

è¯·å‚è€ƒ[å®‰è£…æŒ‡å—](https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation/docs/zh/develop/install/pip/linux-pip.html)å®Œæˆ**PaddlePaddle 3.0**çš„å®‰è£…ï¼Œç„¶åå®‰è£…paddleocrã€‚

```bash
# å®‰è£… paddleocr
pip install paddleocr==3.0.0
```

### 3. å‘½ä»¤è¡Œæ–¹å¼æ¨ç†
```bash
# è¿è¡Œ PP-OCRv5 æ¨ç†
paddleocr ocr -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png --use_doc_orientation_classify False --use_doc_unwarping False --use_textline_orientation False 

# è¿è¡Œ PP-StructureV3 æ¨ç†
paddleocr pp_structurev3 -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png --use_doc_orientation_classify False --use_doc_unwarping False

# è¿è¡Œ PP-ChatOCRv4 æ¨ç†å‰ï¼Œéœ€è¦å…ˆè·å¾—åƒå¸†API Key
paddleocr pp_chatocrv4_doc -i https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png -k é©¾é©¶å®¤å‡†ä¹˜äººæ•° --qianfan_api_key your_api_key --use_doc_orientation_classify False --use_doc_unwarping False 

# æŸ¥çœ‹ &quot;paddleocr ocr&quot; è¯¦ç»†å‚æ•°
paddleocr ocr --help
```
### 4. APIæ–¹å¼æ¨ç†

**4.1 PP-OCRv5 ç¤ºä¾‹**
```python
from paddleocr import PaddleOCR
# åˆå§‹åŒ– PaddleOCR å®ä¾‹
ocr = PaddleOCR(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False,
    use_textline_orientation=False)
# å¯¹ç¤ºä¾‹å›¾åƒæ‰§è¡Œ OCR æ¨ç† 
result = ocr.predict(
    input=&quot;https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/general_ocr_002.png&quot;)
# å¯è§†åŒ–ç»“æœå¹¶ä¿å­˜ json ç»“æœ
for res in result:
    res.print()
    res.save_to_img(&quot;output&quot;)
    res.save_to_json(&quot;output&quot;)
```

&lt;details&gt;
    &lt;summary&gt;&lt;strong&gt;4.2 PP-StructureV3 ç¤ºä¾‹&lt;/strong&gt;&lt;/summary&gt;

```python
from pathlib import Path
from paddleocr import PPStructureV3

pipeline = PPStructureV3(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

# For Image
output = pipeline.predict(
    input=&quot;https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/pp_structure_v3_demo.png&quot;,
    )

# å¯è§†åŒ–ç»“æœå¹¶ä¿å­˜ json ç»“æœ
for res in output:
    res.print() 
    res.save_to_json(save_path=&quot;output&quot;) 
    res.save_to_markdown(save_path=&quot;output&quot;) 
```

&lt;/details&gt;


&lt;details&gt;
   &lt;summary&gt;&lt;strong&gt;4.3 PP-ChatOCRv4 ç¤ºä¾‹&lt;/strong&gt;&lt;/summary&gt;

```python
from paddleocr import PPChatOCRv4Doc

chat_bot_config = {
    &quot;module_name&quot;: &quot;chat_bot&quot;,
    &quot;model_name&quot;: &quot;ernie-3.5-8k&quot;,
    &quot;base_url&quot;: &quot;https://qianfan.baidubce.com/v2&quot;,
    &quot;api_type&quot;: &quot;openai&quot;,
    &quot;api_key&quot;: &quot;api_key&quot;,  # your api_key
}

retriever_config = {
    &quot;module_name&quot;: &quot;retriever&quot;,
    &quot;model_name&quot;: &quot;embedding-v1&quot;,
    &quot;base_url&quot;: &quot;https://qianfan.baidubce.com/v2&quot;,
    &quot;api_type&quot;: &quot;qianfan&quot;,
    &quot;api_key&quot;: &quot;api_key&quot;,  # your api_key
}

mllm_chat_bot_config = {
    &quot;module_name&quot;: &quot;chat_bot&quot;,
    &quot;model_name&quot;: &quot;PP-DocBee&quot;,
    &quot;base_url&quot;: &quot;http://127.0.0.1:8080/&quot;,  # your local mllm service url
    &quot;api_type&quot;: &quot;openai&quot;,
    &quot;api_key&quot;: &quot;api_key&quot;,  # your api_key
}

pipeline = PPChatOCRv4Doc(
    use_doc_orientation_classify=False,
    use_doc_unwarping=False
)

visual_predict_res = pipeline.visual_predict(
    input=&quot;https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/vehicle_certificate-1.png&quot;,
    use_common_ocr=True,
    use_seal_recognition=True,
    use_table_recognition=True,
)

visual_info_list = []
for res in visual_predict_res:
    visual_info_list.append(res[&quot;visual_info&quot;])
    layout_parsing_result = res[&quot;layout_parsing_result&quot;]

vector_info = pipeline.build_vector(
    visual_info_list, flag_save_bytes_vector=True, retriever_config=retriever_config
)
mllm_predict_res = pipeline.mllm_pred(
    input=&quot;vehicle_certificate-1.png&quot;,
    key_list=[&quot;é©¾é©¶å®¤å‡†ä¹˜äººæ•°&quot;],
    mllm_chat_bot_config=mllm_chat_bot_config,
)
mllm_predict_info = mllm_predict_res[&quot;mllm_res&quot;]
chat_result = pipeline.chat(
    key_list=[&quot;é©¾é©¶å®¤å‡†ä¹˜äººæ•°&quot;],
    visual_info=visual_info_list,
    vector_info=vector_info,
    mllm_predict_info=mllm_predict_info,
    chat_bot_config=chat_bot_config,
    retriever_config=retriever_config,
)
print(chat_result)
```

&lt;/details&gt;


### 5. **å›½äº§åŒ–ç¡¬ä»¶ä½¿ç”¨**
- [æ˜†ä»‘èŠ¯å®‰è£…æŒ‡å—](https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_XPU.html)
- [æ˜‡è…¾å®‰è£…æŒ‡å—](https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/other_devices_support/paddlepaddle_install_NPU.html)
  
## â›°ï¸ è¿›é˜¶æŒ‡å—
- [PP-OCRv5 ä½¿ç”¨æ•™ç¨‹](https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/OCR.html)
- [PP-StructureV3 ä½¿ç”¨æ•™ç¨‹](https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-StructureV3.html)
- [PP-ChatOCRv4 ä½¿ç”¨æ•™ç¨‹](https://paddlepaddle.github.io/PaddleOCR/latest/version3.x/pipeline_usage/PP-ChatOCRv4.html)

## ğŸ”„ æ•ˆæœå±•ç¤º

&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
       &lt;img width=&quot;100%&quot; src=&quot;./docs/images/demo.gif&quot; alt=&quot;PP-OCRv5 Demo&quot;&gt;&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
      &lt;img width=&quot;100%&quot; src=&quot;./docs/images/blue_v3.gif&quot; alt=&quot;PP-StructureV3 Demo&quot;&gt;&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

## ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ å¼€å‘è€…ç¤¾åŒº

| æ‰«ç å…³æ³¨é£æ¡¨å…¬ä¼—å· | æ‰«ç åŠ å…¥æŠ€æœ¯äº¤æµç¾¤ |
| :---: | :---: |
| &lt;img src=&quot;https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qrcode_for_paddlepaddle_official_account.jpg&quot; width=&quot;150&quot;&gt; | &lt;img src=&quot;https://raw.githubusercontent.com/cuicheng01/PaddleX_doc_images/refs/heads/main/images/paddleocr/README/qr_code_for_the_questionnaire.jpg&quot; width=&quot;150&quot;&gt; |

## ğŸ† ä½¿ç”¨ PaddleOCR çš„ä¼˜ç§€é¡¹ç›®
PaddleOCR çš„å‘å±•ç¦»ä¸å¼€ç¤¾åŒºè´¡çŒ®ï¼ğŸ’—è¡·å¿ƒæ„Ÿè°¢æ‰€æœ‰å¼€å‘è€…ã€åˆä½œä¼™ä¼´ä¸è´¡çŒ®è€…ï¼
| é¡¹ç›®åç§° | ç®€ä»‹ |
| ------------ | ----------- |
| [RAGFlow](https://github.com/infiniflow/ragflow) &lt;a href=&quot;https://github.com/infiniflow/ragflow&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/infiniflow/ragflow&quot;&gt;&lt;/a&gt;|åŸºäºRAGçš„AIå·¥ä½œæµå¼•æ“|
| [MinerU](https://github.com/opendatalab/MinerU) &lt;a href=&quot;https://github.com/opendatalab/MinerU&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/opendatalab/MinerU&quot;&gt;&lt;/a&gt;|å¤šç±»å‹æ–‡æ¡£è½¬æ¢Markdownå·¥å…·|
| [Umi-OCR](https://github.com/hiroi-sora/Umi-OCR) &lt;a href=&quot;https://github.com/hiroi-sora/Umi-OCR&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/hiroi-sora/Umi-OCR&quot;&gt;&lt;/a&gt;|å¼€æºæ‰¹é‡ç¦»çº¿OCRè½¯ä»¶|
| [OmniParser](https://github.com/microsoft/OmniParser)&lt;a href=&quot;https://github.com/microsoft/OmniParser&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/microsoft/OmniParser&quot;&gt;&lt;/a&gt; |åŸºäºçº¯è§†è§‰çš„GUIæ™ºèƒ½ä½“å±å¹•è§£æå·¥å…·|
| [QAnything](https://github.com/netease-youdao/QAnything)&lt;a href=&quot;https://github.com/netease-youdao/QAnything&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/netease-youdao/QAnything&quot;&gt;&lt;/a&gt; |åŸºäºä»»æ„å†…å®¹çš„é—®ç­”ç³»ç»Ÿ|
| [PDF-Extract-Kit](https://github.com/opendatalab/PDF-Extract-Kit) &lt;a href=&quot;https://github.com/opendatalab/PDF-Extract-Kit&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/opendatalab/PDF-Extract-Kit&quot;&gt;&lt;/a&gt;|é«˜æ•ˆå¤æ‚PDFæ–‡æ¡£æå–å·¥å…·åŒ…|
| [Dango-Translator](https://github.com/PantsuDango/Dango-Translator)&lt;a href=&quot;https://github.com/PantsuDango/Dango-Translator&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/PantsuDango/Dango-Translator&quot;&gt;&lt;/a&gt; |å±å¹•å®æ—¶ç¿»è¯‘å·¥å…·|
| [æ›´å¤šé¡¹ç›®](./awesome_projects.md) | |

## ğŸ‘©â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦ è´¡çŒ®è€…

&lt;a href=&quot;https://github.com/PaddlePaddle/PaddleOCR/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=PaddlePaddle/PaddleOCR&amp;max=400&amp;columns=20&quot;  width=&quot;800&quot;/&gt;
&lt;/a&gt;


## ğŸŒŸ Star

[![Star History Chart](https://api.star-history.com/svg?repos=PaddlePaddle/PaddleOCR&amp;type=Date)](https://star-history.com/#PaddlePaddle/PaddleOCR&amp;Date)


## ğŸ“„ è®¸å¯åè®®
æœ¬é¡¹ç›®çš„å‘å¸ƒå—[Apache 2.0 license](LICENSE)è®¸å¯è®¤è¯ã€‚

## ğŸ“ å­¦æœ¯å¼•ç”¨

```
@misc{paddleocr2020,
title={PaddleOCR, Awesome multilingual OCR toolkits based on PaddlePaddle.},
author={PaddlePaddle Authors},
howpublished = {\url{https://github.com/PaddlePaddle/PaddleOCR}},
year={2020}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pathwaycom/pathway]]></title>
            <link>https://github.com/pathwaycom/pathway</link>
            <guid>https://github.com/pathwaycom/pathway</guid>
            <pubDate>Fri, 23 May 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pathwaycom/pathway">pathwaycom/pathway</a></h1>
            <p>Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.</p>
            <p>Language: Python</p>
            <p>Stars: 25,312</p>
            <p>Forks: 381</p>
            <p>Stars today: 101 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pathway.com/&quot;&gt;
    &lt;img src=&quot;https://pathway.com/logo-light.svg&quot;/&gt;
  &lt;/a&gt;
  &lt;br /&gt;&lt;br /&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/10388&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/10388&quot; alt=&quot;pathwaycom%2Fpathway | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
  &lt;br /&gt;&lt;br /&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml&quot;&gt;
        &lt;img src=&quot;https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml/badge.svg&quot; alt=&quot;ubuntu&quot;/&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/actions/workflows/release.yml&quot;&gt;
        &lt;img src=&quot;https://github.com/pathwaycom/pathway/actions/workflows/release.yml/badge.svg&quot; alt=&quot;Last release&quot;/&gt;&lt;/a&gt;
        &lt;a href=&quot;https://badge.fury.io/py/pathway&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/pathway.svg&quot; alt=&quot;PyPI version&quot; height=&quot;18&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://badge.fury.io/py/pathway&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/pathway&quot; alt=&quot;PyPI downloads&quot; height=&quot;18&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/blob/main/LICENSE.txt&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/license-BSL-green&quot; alt=&quot;License: BSL&quot;/&gt;&lt;/a&gt;
      &lt;br&gt;
        &lt;a href=&quot;https://discord.gg/pathway&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/discord/1042405378304004156?logo=discord&quot;
            alt=&quot;chat on Discord&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://twitter.com/intent/follow?screen_name=pathway_com&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/twitter/follow/pathwaycom&quot;
            alt=&quot;follow on Twitter&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://linkedin.com/company/pathway&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/pathway-0077B5?style=social&amp;logo=linkedin&quot; alt=&quot;follow on LinkedIn&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://github.com/dylanhogg/awesome-python/blob/main/README.md&quot;&gt;
      &lt;img src=&quot;https://awesome.re/badge.svg&quot; alt=&quot;Awesome Python&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://gurubase.io/g/pathway&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/badge/Gurubase-Ask%20Pathway%20Guru-006BFF&quot; alt=&quot;Pathway Guru&quot;&gt;&lt;/a&gt;
    &lt;br&gt;
    &lt;a href=&quot;#getting-started&quot;&gt;Getting Started&lt;/a&gt; |
    &lt;a href=&quot;#deployment&quot;&gt;Deployment&lt;/a&gt; |
    &lt;a href=&quot;#resources&quot;&gt;Documentation and Support&lt;/a&gt; |
    &lt;a href=&quot;https://pathway.com/blog/&quot;&gt;Blog&lt;/a&gt; |
    &lt;a href=&quot;#license&quot;&gt;License&lt;/a&gt;

  
&lt;/p&gt;

# Pathway&lt;a id=&quot;pathway&quot;&gt; Live Data Framework&lt;/a&gt;

[Pathway](https://pathway.com) is a Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.

Pathway comes with an **easy-to-use Python API**, allowing you to seamlessly integrate your favorite Python ML libraries.
Pathway code is versatile and robust: **you can use it in both development and production environments, handling both batch and streaming data effectively**.
The same code can be used for local development, CI/CD tests, running batch jobs, handling stream replays, and processing data streams.

Pathway is powered by a **scalable Rust engine** based on Differential Dataflow and performs incremental computation.
Your Pathway code, despite being written in Python, is run by the Rust engine, enabling multithreading, multiprocessing, and distributed computations.
All the pipeline is kept in memory and can be easily deployed with **Docker and Kubernetes**.

You can install Pathway with pip:
```
pip install -U pathway
```

For any questions, you will find the community and team behind the project [on Discord](https://discord.com/invite/pathway).

## Use-cases and templates

Ready to see what Pathway can do?

[Try one of our easy-to-run examples](https://pathway.com/developers/templates)!

Available in both notebook and docker formats, these ready-to-launch examples can be launched in just a few clicks. Pick one and start your hands-on experience with Pathway today!

### Event processing and real-time analytics pipelines
With its unified engine for batch and streaming and its full Python compatibility, Pathway makes data processing as easy as possible. It&#039;s the ideal solution for a wide range of data processing pipelines, including:

- [Showcase: Real-time ETL.](https://pathway.com/developers/templates/kafka-etl)
- [Showcase: Event-driven pipelines with alerting.](https://pathway.com/developers/templates/realtime-log-monitoring)
- [Showcase: Realtime analytics.](https://pathway.com/developers/templates/linear_regression_with_kafka/)
- [Docs: Switch from batch to streaming.](https://pathway.com/developers/user-guide/connecting-to-data/switch-from-batch-to-streaming)



### AI Pipelines

Pathway provides dedicated LLM tooling to build live LLM and RAG pipelines. Wrappers for most common LLM services and utilities are included, making working with LLMs and RAGs pipelines incredibly easy. Check out our [LLM xpack documentation](https://pathway.com/developers/user-guide/llm-xpack/overview).

Don&#039;t hesitate to try one of our runnable examples featuring LLM tooling.
You can find such examples [here](https://pathway.com/developers/user-guide/llm-xpack/llm-examples).

  - [Template: Unstructured data to SQL on-the-fly.](https://pathway.com/developers/templates/unstructured-to-structured/)
  - [Template: Private RAG with Ollama and Mistral AI](https://pathway.com/developers/templates/private-rag-ollama-mistral)
  - [Template: Adaptive RAG](https://pathway.com/developers/templates/adaptive-rag)
  - [Template: Multimodal RAG with gpt-4o](https://pathway.com/developers/templates/multimodal-rag)

## Features

- **A wide range of connectors**: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector.
- **Stateless and stateful transformations**: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data.
- **Persistence**: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!
- **Consistency**: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the &quot;at least once&quot; consistency while the enterprise version provides the &quot;exactly once&quot; consistency.
- **Scalable Rust engine**: with Pathway Rust engine, you are free from the usual limits imposed by Python. You can easily do multithreading, multiprocessing, and distributed computations.
- **LLM helpers**: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents.


## Getting started&lt;a id=&quot;getting-started&quot;&gt;&lt;/a&gt;

### Installation&lt;a id=&quot;installation&quot;&gt;&lt;/a&gt;

Pathway requires Python 3.10 or above.

You can install the current release of Pathway using `pip`:

```
$ pip install -U pathway
```

âš ï¸ Pathway is available on MacOS and Linux. Users of other systems should run Pathway on a Virtual Machine.


### Example: computing the sum of positive values in real time.&lt;a id=&quot;example&quot;&gt;&lt;/a&gt;

```python
import pathway as pw

# Define the schema of your data (Optional)
class InputSchema(pw.Schema):
  value: int

# Connect to your data using connectors
input_table = pw.io.csv.read(
  &quot;./input/&quot;,
  schema=InputSchema
)

#Define your operations on the data
filtered_table = input_table.filter(input_table.value&gt;=0)
result_table = filtered_table.reduce(
  sum_value = pw.reducers.sum(filtered_table.value)
)

# Load your results to external systems
pw.io.jsonlines.write(result_table, &quot;output.jsonl&quot;)

# Run the computation
pw.run()
```

Run Pathway [in Google Colab](https://colab.research.google.com/drive/1aBIJ2HCng-YEUOMrr0qtj0NeZMEyRz55?usp=sharing).

You can find more examples [here](https://github.com/pathwaycom/pathway/tree/main/examples).


## Deployment&lt;a id=&quot;deployment&quot;&gt;&lt;/a&gt;

### Locally&lt;a id=&quot;running-pathway-locally&quot;&gt;&lt;/a&gt;

To use Pathway, you only need to import it:

```python
import pathway as pw
```

Now, you can easily create your processing pipeline, and let Pathway handle the updates. Once your pipeline is created, you can launch the computation on streaming data with a one-line command:

```python
pw.run()
```

You can then run your Pathway project (say, `main.py`) just like a normal Python script: `$ python main.py`.
Pathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages. 

&lt;img src=&quot;https://d14l3brkh44201.cloudfront.net/pathway-dashboard.png&quot; width=&quot;1326&quot; alt=&quot;Pathway dashboard&quot;/&gt;

Alternatively, you can use the pathway&#039;ish version:

```
$ pathway spawn python main.py
```

Pathway natively supports multithreading.
To launch your application with 3 threads, you can do as follows:
```
$ pathway spawn --threads 3 python main.py
```

To jumpstart a Pathway project, you can use our [cookiecutter template](https://github.com/pathwaycom/cookiecutter-pathway).


### Docker&lt;a id=&quot;docker&quot;&gt;&lt;/a&gt;

You can easily run Pathway using docker.

#### Pathway image

You can use the [Pathway docker image](https://hub.docker.com/r/pathwaycom/pathway), using a Dockerfile:

```dockerfile
FROM pathwaycom/pathway:latest

WORKDIR /app

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD [ &quot;python&quot;, &quot;./your-script.py&quot; ]
```

You can then build and run the Docker image:

```console
docker build -t my-pathway-app .
docker run -it --rm --name my-pathway-app my-pathway-app
```

#### Run a single Python script

When dealing with single-file projects, creating a full-fledged `Dockerfile`
might seem unnecessary. In such scenarios, you can execute a
Python script directly using the Pathway Docker image. For example:

```console
docker run -it --rm --name my-pathway-app -v &quot;$PWD&quot;:/app pathwaycom/pathway:latest python my-pathway-app.py
```

#### Python docker image

You can also use a standard Python image and install Pathway using pip with a Dockerfile:

```dockerfile
FROM --platform=linux/x86_64 python:3.10

RUN pip install -U pathway
COPY ./pathway-script.py pathway-script.py

CMD [&quot;python&quot;, &quot;-u&quot;, &quot;pathway-script.py&quot;]
```

### Kubernetes and cloud&lt;a id=&quot;k8s&quot;&gt;&lt;/a&gt;

Docker containers are ideally suited for deployment on the cloud with Kubernetes.
If you want to scale your Pathway application, you may be interested in our Pathway for Enterprise.
Pathway for Enterprise is specially tailored towards end-to-end data processing and real time intelligent analytics.
It scales using distributed computing on the cloud and supports distributed Kubernetes deployment, with external persistence setup.

You can easily deploy Pathway using services like Render: see [how to deploy Pathway in a few clicks](https://pathway.com/developers/user-guide/deployment/render-deploy/).

If you are interested, don&#039;t hesitate to [contact us](mailto:contact@pathway.com) to learn more.

## Performance&lt;a id=&quot;performance&quot;&gt;&lt;/a&gt;

Pathway is made to outperform state-of-the-art technologies designed for streaming and batch data processing tasks, including: Flink, Spark, and Kafka Streaming. It also makes it possible to implement a lot of algorithms/UDF&#039;s in streaming mode which are not readily supported by other streaming frameworks (especially: temporal joins, iterative graph algorithms, machine learning routines).

If you are curious, here are [some benchmarks to play with](https://github.com/pathwaycom/pathway-benchmarks).

&lt;img src=&quot;https://github.com/pathwaycom/pathway-benchmarks/raw/main/images/bm-wordcount-lineplot.png&quot; width=&quot;1326&quot; alt=&quot;WordCount Graph&quot;/&gt;

## Documentation and Support&lt;a id=&quot;resources&quot;&gt;&lt;/a&gt;

The entire documentation of Pathway is available at [pathway.com/developers/](https://pathway.com/developers/user-guide/introduction/welcome), including the [API Docs](https://pathway.com/developers/api-docs/pathway).

If you have any question, don&#039;t hesitate to [open an issue on GitHub](https://github.com/pathwaycom/pathway/issues), join us on [Discord](https://discord.com/invite/pathway), or send us an email at [contact@pathway.com](mailto:contact@pathway.com).

## License&lt;a id=&quot;license&quot;&gt;&lt;/a&gt;

Pathway is distributed on a [BSL 1.1 License](https://github.com/pathwaycom/pathway/blob/main/LICENSE.txt) which allows for unlimited non-commercial use, as well as use of the Pathway package [for most commercial purposes](https://pathway.com/license/), free of charge. Code in this repository automatically converts to Open Source (Apache 2.0 License) after 4 years. Some [public repos](https://github.com/pathwaycom) which are complementary to this one (examples, libraries, connectors, etc.) are licensed as Open Source, under the MIT license.


## Contribution guidelines&lt;a id=&quot;contribution-guidelines&quot;&gt;&lt;/a&gt;

If you develop a library or connector which you would like to integrate with this repo, we suggest releasing it first as a separate repo on a MIT/Apache 2.0 license. 

For all concerns regarding core Pathway functionalities, Issues are encouraged. For further information, don&#039;t hesitate to engage with Pathway&#039;s [Discord community](https://discord.gg/pathway).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[bilibili/Index-anisora]]></title>
            <link>https://github.com/bilibili/Index-anisora</link>
            <guid>https://github.com/bilibili/Index-anisora</guid>
            <pubDate>Fri, 23 May 2025 00:04:25 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/bilibili/Index-anisora">bilibili/Index-anisora</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 768</p>
            <p>Forks: 36</p>
            <p>Stars today: 57 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;assets/index_icon.png&quot; width=&quot;250&quot;/&gt;

&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
       ğŸ–¥ï¸  &lt;a href=&quot;https://github.com/bilibili/Index-anisora/tree/main&quot;&gt;GitHub&lt;/a&gt; &amp;nbsp&amp;nbsp  |  &amp;nbsp&amp;nbspğŸ¤— &lt;a href=https://huggingface.co/IndexTeam/Index-anisora&gt;Hugging Face&lt;/a&gt;&amp;nbsp&amp;nbsp |  &amp;nbsp&amp;nbspğŸ¤– &lt;a href=https://www.modelscope.cn/organization/bilibili-index&gt;Model Scope&lt;/a&gt;&amp;nbsp&amp;nbsp | ğŸ“‘ &lt;a href=&#039;http://arxiv.org/abs/2412.10255&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/ArXiv-2412.10255-red&#039;&gt;&lt;/a&gt; &amp;nbsp&amp;nbsp ï½œ  ğŸ“‘ &lt;a href=&#039;http://arxiv.org/abs/2504.10044&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/ArXiv-2504.10044-red&#039;&gt;&lt;/a&gt; &amp;nbsp&amp;nbsp

**English** | [**ä¸­æ–‡ç®€ä½“**](./README_CN.md)

&lt;br&gt;

----

[**Index-AniSora:The Ultimate Open-Source Anime Video Generation Model**](http://arxiv.org/abs/2412.10255) &lt;be&gt; 

This Project presenting Bilibili&#039;s gift to the anime world - Index-AniSora, the most powerful open-source animated video generation model.
It enables one-click creation of video shots across diverse anime styles including series episodes, Chinese original animations, manga adaptations, VTuber content, anime PVs, mad-style parodies(é¬¼ç•œåŠ¨ç”»), and more!
Powered by our IJCAI&#039;25-accepted work  &lt;a href=&#039;http://arxiv.org/abs/2412.10255&#039;&gt;AniSora: Exploring the Frontiers of Animation Video Generation in the Sora Era &lt;/a&gt;

## Video Demos
&lt;div align=&quot;center&quot;&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/4351fc5e-f7fd-456b-807e-82fdcb321de2&quot; controls width=&quot;60%&quot; poster=&quot;&quot;&gt;&lt;/video&gt;
&lt;/div&gt;


## ğŸ“£ Updates

- `2025/05/12` ğŸ”¥ğŸ”¥Everything we build is open-source. Check Out Now!!!
- `2025/05/10` ğŸ”¥Our paper is accepted by IJCAI25. Camera Ready Version is updated. 
- `2024/12/19` We submitted our paper on arXiv and released our project with evaluation benchmark.

## Project Guide

### AniSoraV1.0
Find in ğŸ“ `anisoraV1_infer`

Trained on the CogVideoX-5B foundation model, with full training and inference code released. 
- Localized region guidance for video control
- Temporal guidance (first/last frame guidance, keyframe interpolation, multi-frame guidance)
- Full training and inference code release. Find in ğŸ“ `anisoraV1_train_npu`
- Cost-effective deployment on RTX 4090
- Covers 80% of application scenarios

### AniSoraV2.0
Find in ğŸ“ `anisoraV2_gpu`, `anisoraV2_npu` 

Powered by the enhanced Wan2.1-14B foundation model for superior stability.
- Distillation-accelerated inference without quality compromise, faster and cheaper
- Full training/inference code release
- Native support Huawei Ascend 910B NPUs (entirely trained on domestic chips) ğŸ“ `anisoraV2_npu`.
- High quality video shots generation, covers 90% of application scenarios

### Ecosystem Tools
Find in ğŸ“ `data_pipeline`

End-to-end dataset pipeline for rapid training data expansion.
- Animate data cleaning pipeline.

### Anime-optimized Benchmark System
Find in ğŸ“ `reward`

Specialized evaluation models and scoring algorithms for anime video generation, includes reward models suitable for reinforcement learning and benchmarking. 
- Tailored evaluation framework for animation generation
- Standard test dataset aligned with ACG aesthetics
- Human Preference Alignment

The benchmark dataset contains 948 animation video clips are collected and labeled with different actions. Each label contains 10-30 video clips. The corresponding text prompt is generated by Qwen-VL2 at first, then is corrected manually to guarantee the text-video alignment.
Fill the form and send PDF format to yangsiqian@bilibili.com or xubaohan@bilibili.com (links provided after agreeing with Bilibili)


### AniSoraV1.0_RL
Find in ğŸ“ `anisora_rl`

The first RLHF framework for anime video generation. 
- RL-optimized AniSoraV1.0 for enhanced anime-style output
- Methodology detailed in our preprint: &lt;a href=&#039;http://arxiv.org/abs/2504.10044&#039;&gt; Aligning Anime Video Generation with Human Feedback &lt;/a&gt;

## ğŸ“‘ Todo List
- AniSoraV2.0
    - [ ] Support 14B version, is excepted before the end of May.
- AniSora Dataset
    - [ ] **High quality training set open apply**
- Anisora Benchmark
    - [ ] Update latest SOTA models performance
    - [ ] Provide more video demos on AniSora benchmark. 

---
## ğŸ’¡ Abstract
Animation has gained significant interest in the recent film and TV industry. Despite the success of advanced video generation models like Sora, Kling, and CogVideoX in generating natural videos, they lack the same effectiveness in handling animation videos. Evaluating animation video generation is also a great challenge due to its unique artist styles, violating the laws of physics and exaggerated motions. In this paper, we present a comprehensive system, **AniSora**, designed for animation video generation, which includes a data processing pipeline, a controllable generation model, and an evaluation dataset. Supported by the data processing pipeline with over 10M high-quality data, the generation model incorporates a spatiotemporal mask module to facilitate key animation production functions such as image-to-video generation, frame interpolation, and localized image-guided animation. We also collect an evaluation benchmark of 948 various animation videos, the evaluation on VBench and human double-blind test demonstrates consistency in character and motion, achieving state-of-the-art results in animation video generation.

## ğŸ–¥ï¸ Method

The overview of Index-anisora is shown as follows.

&lt;picture&gt;
  &lt;img src=&quot;assets/framework.png&quot;  width=&quot;800&quot;/&gt;
&lt;/picture&gt;

Features:

1. We develop a comprehensive video processing system that significantly enhances preprocessing for video generation.

2. We propose a unified framework designed for animation video generation with a spatiotemporal mask module, enabling tasks such as image-to-video generation, frame interpolation, and localized image-guided animation.

3. We release a benchmark dataset specifically for evaluating animation video generation.


## ğŸï¸ Showcases


**Image-generated videos in different artistic styles:**

| prmopt | image  | Video  |
| --- | --- | --- |
|The figures in the picture are sitting in a forward moving car waving to the rear, their hair swaying from side to side in the wind| &lt;img src=&quot;assets/000000(225).png&quot; width=&quot;800&quot;/&gt; |![Demo](assets/000000(225).gif)|
|The scene shows two figures in red wedding clothes holding a red rope as they walk off into the distance|  &lt;img src=&quot;assets/000000(223).png&quot; width=&quot;800&quot;/&gt; |![Demo](assets/000000(223).gif)|
|The yellow-haired figure reaches out to touch the head of the kneeling figure, and the kneeling figure&#039;s body rises and falls as he gasps for breath.|  &lt;img src=&quot;assets/000000(232).png&quot; width=&quot;800&quot;/&gt; |![Demo](assets/000000(232).gif)|
|In the frame, a person sprints forward at high speed, their motion appearing slightly blurred from the velocity. | &lt;img src=&quot;assets/image_1.jpg&quot;  width=&quot;800&quot;/&gt; | ![Demo](assets/image_1_vid.gif)|
|In the frame, the character raises their arm, with gaseous currents visibly flowing along its surface. | &lt;img src=&quot;assets/image_2.jpg&quot;  width=&quot;800&quot;/&gt; | ![Demo](assets/image_2_vid.gif)|
|The old man&#039;s gaze locks onto the gemstone, his right hand subtly adjusting the magnifying glass as his lips moveâ€”as if it holds the key to unraveling some ancient knowledge or secret. | &lt;img src=&quot;assets/image_3.jpg&quot;  width=&quot;800&quot;/&gt; | ![Demo](assets/image_3_vid.gif)|
|The man on the left presses his lips tightly together, his face etched with fury and resolve. Every line of his expression radiates both profound frustration and unshakable conviction. Meanwhile, the other man&#039;s jaw hangs openâ€”poised as if to erupt into a shout or impassioned declaration. | &lt;img src=&quot;assets/image_4.jpg&quot;  width=&quot;800&quot;/&gt; | ![Demo](assets/image_4_vid.gif)|
|The scene depicts an exploding rock, erupting in blinding light as shattered fragments blast outward in all directions. | &lt;img src=&quot;assets/image_5.jpg&quot;  width=&quot;800&quot;/&gt; | ![Demo](assets/image_5_vid.gif)|


**Temporal Control:**

| prmopt | first frame | mid frame| last frame  | Video  |
| --- | --- | --- | --- | --- |
|In this video we see a scene from the animated film Beauty and the Beast with Belle and the Beast. Belle, with long blonde hair, is standing in a room with large windows, looking out the window and talking to it. She is wearing a purple dress with a purple top...| &lt;img src=&quot;assets/cartoon_films_ren_wu_shuo_hua_34_firstmidlast_first.png&quot; width=&quot;800&quot;/&gt; |&lt;img src=&quot;assets/cartoon_films_ren_wu_shuo_hua_34_firstmidlast_mid.png&quot; width=&quot;800&quot;/&gt; |&lt;img src=&quot;assets/cartoon_films_ren_wu_shuo_hua_34_firstmidlast_last.png&quot; width=&quot;800&quot;/&gt; |![Demo](assets/cartoon_films_ren_wu_shuo_hua_34_firstmidlast.gif)|
|In this video, a young woman with long blonde hair can be seen looking out from behind a car door at night. The car is parked under a starry sky with a full moon illuminating the scene. The woman appears to be in a state of worry, as evidenced by her facial expression and the way she grips the car door. |  &lt;img src=&quot;assets/motion_comics_tui_la_5_firstlast_first.png&quot; width=&quot;800&quot;/&gt; | None | &lt;img src=&quot;assets/motion_comics_tui_la_5_firstlast_last.jpeg&quot; width=&quot;800&quot;/&gt;|![Demo](assets/motion_comics_tui_la_5_firstlast.gif)|
|A cartoon cat is the central figure in this video, which appears to be in a state of mischief or curiosity. The cat&#039;s eyes are closed and its mouth is open, suggesting a moment of surprise or anticipation...|None | None |&lt;img src=&quot;assets/motion_comics_zhi_dong_xi_2_last.jpeg&quot; width=&quot;800&quot;/&gt;|![Demo](assets/motion_comics_zhi_dong_xi_2_last.gif)|

**Spatial Control:**

| prmopt | first frame | motion mask |  Video(with motion mask visualization)  |
| --- | --- | --- | --- | 
|In this vibrant underwater scene from the animated film Finding Nemo, Marlin and Nemo, two clownfish, talk near a large purple piece of coral...| &lt;img src=&quot;assets/132.png&quot; width=&quot;800&quot;/&gt; |&lt;img src=&quot;assets/132_mask.png&quot; width=&quot;800&quot;/&gt; |![Demo](assets/132.gif)|
|Same as above|  Same as above | &lt;img src=&quot;assets/133_mask.png&quot; width=&quot;800&quot;/&gt;|![Demo](assets/133.gif)|


**&lt;span style=&quot;font-size:18px;&quot;&gt; More videos are available in: [Video Gallery AniSoraV1.0](https://pwz4yo5eenw.feishu.cn/docx/XN9YdiOwCoqJuexLdCpcakSlnkg) &lt;/span&gt;**

## ğŸ“‘ Evaluation

Evaluation results on Vbench:

| Method                   | Motion Smoothness | Motion Score | Aesthetic Quality | Imaging Quality | I2V Subject | I2V Background | Overall Consistency | Subject Consistency |
|--------------------------|-------------------|--------------|-------------------|-----------------|-------------|----------------|---------------------|---------------------|
| Opensora-Plan(V1.3)  | 99.13            | 76.45        | 53.21            | 65.11           | 93.53       | 94.71          | 21.67              | 88.86              |
| Opensora(V1.2)       | 98.78            | 73.62        | 54.30            | 68.44           | 93.15       | 91.09          | 22.68              | 87.71              |
| Vidu                 | 97.71            | **77.51**        | 53.68            | 69.23           | 92.25       | 93.06          | 20.87              | 88.27              |
| Covideo(5B-V1)       | 97.67            | 71.47        | **54.87**            | 68.16           | 90.68       | 91.79          | 21.87              | 90.29              |
| MiniMax              | 99.20            | 66.53        | 54.56            | **71.67**           | 95.95       | **95.42**          | 21.82              | 93.62              |
| **AniSora**              | **99.34**        | 45.59        | 54.31            | 70.58           | **97.52**       | 95.04          | 21.15              | **96.99**              |
| AniSora-K            | 99.12            | 59.49        | 53.76            | 68.68           | 95.13       | 93.36          | 21.13              | 94.61              |
| AniSora-I            | 99.31            | 54.96        | 54.67            | 68.98           | 94.16       | 92.38          | 20.47              | 95.75              |
| GT                   | 98.72            | 56.05        | 52.70            | 70.50           | 96.02       | 95.03          | 21.29              | 94.37              |


Evaluation results on AniSora-Benchmark:

| Method                   | Human Evaluation | Visual Smooth | Visual Motion | Visual Appeal | Text-Video Consistency | Image-Video Consistency | Character Consistency |
|--------------------------|------------------|---------------|---------------|---------------|------------------------|-------------------------|-----------------------|
| Vidu-1.5                 | 60.98            | 55.37         | **78.95**     | 50.68         | 60.71                  | 66.85                   | 82.57                 |
| Opensora-V1.2            | 41.10            | 22.28         | 74.90         | 22.62         | 52.19                  | 55.67                   | 74.76                 |
| Opensora-Plan-V1.3       | 46.14            | 35.08         | 77.47         | 36.14         | 56.19                  | 59.42                   | 81.19                 |
| CogVideoX-5B-V1          | 53.29            | 39.91         | 73.07         | 39.59         | 67.98                  | 65.49                   | 83.07                 |
| MiniMax-I2V01            | 69.63            | 69.38         | 68.05         | **70.34**     | **76.14**              | 78.74                   | 89.47                 |
| **AniSora (Ours)**       | **70.13**        | **71.47**     | 47.94         | 64.44         | 72.92                  | **81.54**               | **94.54**             |
| AniSora (Interpolated Avg) | -             | 70.78         | 53.02         | 64.41         | 73.56                  | 80.62                   | 91.59                 |
| AniSora (KeyFrame Interp) | -             | 70.03         | 58.10         | 64.57         | 74.57                  | 80.78                   | 91.98                 |
| AniSora (KeyFrame Interp) | -             | 70.03         | 58.10         | 64.57         | 74.57                  | 80.78                   | 91.98                 |
| GT                       | -                | 92.20         | 58.27         | 89.72         | 92.51                  | 94.69                   | 95.08                 |



AniSora for our I2V results.

AniSora-K for the key frame interpolation results.

AniSora-I for the average results of frame interpolation conditions, including key frame, last frame, mid frame results.

## ğŸ³ Benchmark Dataset

The benchmark dataset contains 948 animation video clips are collected and labeled
with different actions. Each label contains 10-30 video clips. The corresponding text prompt is generated by Qwen-VL2 at first, then is corrected manually to guarantee the text-video alignment.

Fill the  &lt;a href=&quot;assets/anisora_benchmark_agreement_form.doc&quot;&gt;form&lt;/a&gt; and send PDF format to yangsiqian@bilibili.com or xubaohan@bilibili.com (links provided after agreeing with Bilibili)

## ğŸ¤— Acknowledgments
We would like to express our sincere thanks to the [CogVideoX](https://github.com/THUDM/CogVideo)ã€[Wan2.1](https://github.com/Wan-Video/Wan2.1)ã€[FasterCache](https://github.com/Vchitect/FasterCache) and [OSS](https://github.com/bebebe666/OptimalSteps) for their valuable work.

## ğŸ“š Citation

ğŸŒŸ If you find our work helpful, please leave us a star and cite our paper.

```
@article{jiang2024anisora,
  title={AniSora: Exploring the Frontiers of Animation Video Generation in the Sora Era},
  author={Yudong Jiang, Baohan Xu, Siqian Yang, Mingyu Yin, Jing Liu, Chao Xu, Siqi Wang, Yidi Wu, Bingwen Zhu, Xinwen Zhang, Xingyu Zheng,Jixuan Xu, Yue Zhang, Jinlong Hou and Huyang Sun},
  journal={arXiv preprint arXiv:2412.10255},
  year={2024}
}
```

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[frappe/hrms]]></title>
            <link>https://github.com/frappe/hrms</link>
            <guid>https://github.com/frappe/hrms</guid>
            <pubDate>Fri, 23 May 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[Open Source HR and Payroll Software]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/frappe/hrms">frappe/hrms</a></h1>
            <p>Open Source HR and Payroll Software</p>
            <p>Language: Python</p>
            <p>Stars: 2,254</p>
            <p>Forks: 1,073</p>
            <p>Stars today: 37 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
	&lt;a href=&quot;https://frappe.io/hr&quot;&gt;
		&lt;img src=&quot;.github/frappe-hr-logo.png&quot; height=&quot;80px&quot; width=&quot;80px&quot; alt=&quot;Frappe HR Logo&quot;&gt;
	&lt;/a&gt;
	&lt;h2&gt;Frappe HR&lt;/h2&gt;
	&lt;p align=&quot;center&quot;&gt;
		&lt;p&gt;Open Source, modern, and easy-to-use HR and Payroll Software&lt;/p&gt;
	&lt;/p&gt;

[![CI](https://github.com/frappe/hrms/actions/workflows/ci.yml/badge.svg?branch=develop)](https://github.com/frappe/hrms/actions/workflows/ci.yml)
[![codecov](https://codecov.io/gh/frappe/hrms/branch/develop/graph/badge.svg?token=0TwvyUg3I5)](https://codecov.io/gh/frappe/hrms)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
	&lt;img src=&quot;.github/hrms-hero.png&quot;/&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
	&lt;a href=&quot;https://frappe.io/hr&quot;&gt;Website&lt;/a&gt;
	-
	&lt;a href=&quot;https://docs.frappe.io/hr/introduction&quot;&gt;Documentation&lt;/a&gt;
&lt;/div&gt;

## Frappe HR

Frappe HR has everything you need to drive excellence within the company. It&#039;s a complete HRMS solution with over 13 different modules right from Employee Management, Onboarding, Leaves, to Payroll, Taxation, and more!

## Motivation
When Frappe team started growing in terms of size, we needed an open-source HR and Payroll software. We didn&#039;t find any &quot;true&quot; open-source HR software out there and so decided to build one ourselves.
Initially, it was a set of modules within ERPNext but version 14 onwards, as the modules became more mature, Frappe HR was created as a separate product.

## Key Features

- **Employee Lifecycle**: From onboarding employees, managing promotions and transfers, all the way to documenting feedback with exit interviews, make life easier for employees throughout their life cycle.
- **Leave and Attendance**: Configure leave policies, pull regional holidays with a click, check-in and check-out with geolocation capturing, track leave balances and attendance with reports.
- **Expense Claims and Advances**: Manage employee advances, claim expenses, configure multi-level approval workflows, all this with seamless integration with ERPNext accounting.
- **Performance Management**: Track goals, align goals with key result areas (KRAs), enable employees to evaluate themselves, make managing appraisal cycles easy.
- **Payroll &amp; Taxation**: Create salary structures, configure income tax slabs, run standard payroll, accomodate additional salaries and off cycle payments, view income breakup on salary slips and so much more.
- **Frappe HR Mobile App**: Apply for and approve leaves on the go, check-in and check-out, access employee profile right from the mobile app.

&lt;details open&gt;

&lt;summary&gt;View Screenshots&lt;/summary&gt;
	&lt;img src=&quot;.github/hrms-appraisal.png&quot;/&gt;
	&lt;img src=&quot;.github/hrms-requisition.png&quot;/&gt;
	&lt;img src=&quot;.github/hrms-attendance.png&quot;/&gt;
	&lt;img src=&quot;.github/hrms-salary.png&quot;/&gt;
	&lt;img src=&quot;.github/hrms-pwa.png&quot;/&gt;
&lt;/details&gt;

### Under the Hood

- [**Frappe Framework**](https://github.com/frappe/frappe): A full-stack web application framework written in Python and Javascript. The framework provides a robust foundation for building web applications, including a database abstraction layer, user authentication, and a REST API.

- [**Frappe UI**](https://github.com/frappe/frappe-ui): A Vue-based UI library, to provide a modern user interface. The Frappe UI library provides a variety of components that can be used to build single-page applications on top of the Frappe Framework.

## Production Setup

### Managed Hosting

You can try [Frappe Cloud](https://frappecloud.com), a simple, user-friendly and sophisticated [open-source](https://github.com/frappe/press) platform to host Frappe applications with peace of mind.

It takes care of installation, setup, upgrades, monitoring, maintenance and support of your Frappe deployments. It is a fully featured developer platform with an ability to manage and control multiple Frappe deployments.

&lt;div&gt;
	&lt;a href=&quot;https://frappecloud.com/hrms/signup&quot; target=&quot;_blank&quot;&gt;
		&lt;picture&gt;
			&lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://frappe.io/files/try-on-fc-white.png&quot;&gt;
			&lt;img src=&quot;https://frappe.io/files/try-on-fc-black.png&quot; alt=&quot;Try on Frappe Cloud&quot; height=&quot;28&quot; /&gt;
		&lt;/picture&gt;
	&lt;/a&gt;
&lt;/div&gt;


## Development setup
### Docker
You need Docker, docker-compose and git setup on your machine. Refer [Docker documentation](https://docs.docker.com/). After that, run the following commands:
```
git clone https://github.com/frappe/hrms
cd hrms/docker
docker-compose up
```

Wait for some time until the setup script creates a site. After that you can access `http://localhost:8000` in your browser and the login screen for HR should show up.

Use the following credentials to log in:

- Username: `Administrator`
- Password: `admin`

### Local

1. Set up bench by following the [Installation Steps](https://frappeframework.com/docs/user/en/installation) and start the server and keep it running
	```sh
	$ bench start
	```
2. In a separate terminal window, run the following commands
	```sh
	$ bench new-site hrms.local
	$ bench get-app erpnext
	$ bench get-app hrms
	$ bench --site hrms.local install-app hrms
	$ bench --site hrms.local add-to-hosts
	```
3. You can access the site at `http://hrms.local:8080`

## Learning and Community

1. [Frappe School](https://frappe.school) - Learn Frappe Framework and ERPNext from the various courses by the maintainers or from the community.
2. [Documentation](https://docs.frappe.io/hr) - Extensive documentation for Frappe HR.
3. [User Forum](https://discuss.erpnext.com/) - Engage with the community of ERPNext users and service providers.
4. [Telegram Group](https://t.me/frappehr) - Get instant help from the community of users.


## Contributing

1. [Issue Guidelines](https://github.com/frappe/erpnext/wiki/Issue-Guidelines)
1. [Report Security Vulnerabilities](https://erpnext.com/security)
1. [Pull Request Requirements](https://github.com/frappe/erpnext/wiki/Contribution-Guidelines)


## Logo and Trademark Policy

Please read our [Logo and Trademark Policy](TRADEMARK_POLICY.md).

&lt;br /&gt;
&lt;br /&gt;
&lt;div align=&quot;center&quot; style=&quot;padding-top: 0.75rem;&quot;&gt;
	&lt;a href=&quot;https://frappe.io&quot; target=&quot;_blank&quot;&gt;
		&lt;picture&gt;
			&lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://frappe.io/files/Frappe-white.png&quot;&gt;
			&lt;img src=&quot;https://frappe.io/files/Frappe-black.png&quot; alt=&quot;Frappe Technologies&quot; height=&quot;28&quot;/&gt;
		&lt;/picture&gt;
	&lt;/a&gt;
&lt;/div&gt;

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[real-stanford/diffusion_policy]]></title>
            <link>https://github.com/real-stanford/diffusion_policy</link>
            <guid>https://github.com/real-stanford/diffusion_policy</guid>
            <pubDate>Fri, 23 May 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[[RSS 2023] Diffusion Policy Visuomotor Policy Learning via Action Diffusion]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/real-stanford/diffusion_policy">real-stanford/diffusion_policy</a></h1>
            <p>[RSS 2023] Diffusion Policy Visuomotor Policy Learning via Action Diffusion</p>
            <p>Language: Python</p>
            <p>Stars: 2,540</p>
            <p>Forks: 464</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre># Diffusion Policy

[[Project page]](https://diffusion-policy.cs.columbia.edu/)
[[Paper]](https://diffusion-policy.cs.columbia.edu/#paper)
[[Data]](https://diffusion-policy.cs.columbia.edu/data/)
[[Colab (state)]](https://colab.research.google.com/drive/1gxdkgRVfM55zihY9TFLja97cSVZOZq2B?usp=sharing)
[[Colab (vision)]](https://colab.research.google.com/drive/18GIHeOQ5DyjMN8iIRZL2EKZ0745NLIpg?usp=sharing)


[Cheng Chi](http://cheng-chi.github.io/)&lt;sup&gt;1&lt;/sup&gt;,
[Siyuan Feng](https://www.cs.cmu.edu/~sfeng/)&lt;sup&gt;2&lt;/sup&gt;,
[Yilun Du](https://yilundu.github.io/)&lt;sup&gt;3&lt;/sup&gt;,
[Zhenjia Xu](https://www.zhenjiaxu.com/)&lt;sup&gt;1&lt;/sup&gt;,
[Eric Cousineau](https://www.eacousineau.com/)&lt;sup&gt;2&lt;/sup&gt;,
[Benjamin Burchfiel](http://www.benburchfiel.com/)&lt;sup&gt;2&lt;/sup&gt;,
[Shuran Song](https://www.cs.columbia.edu/~shurans/)&lt;sup&gt;1&lt;/sup&gt;

&lt;sup&gt;1&lt;/sup&gt;Columbia University,
&lt;sup&gt;2&lt;/sup&gt;Toyota Research Institute,
&lt;sup&gt;3&lt;/sup&gt;MIT

&lt;img src=&quot;media/teaser.png&quot; alt=&quot;drawing&quot; width=&quot;100%&quot;/&gt;
&lt;img src=&quot;media/multimodal_sim.png&quot; alt=&quot;drawing&quot; width=&quot;100%&quot;/&gt;

## ğŸ› Try it out!
Our self-contained Google Colab notebooks is the easiest way to play with Diffusion Policy. We provide separate notebooks for  [state-based environment](https://colab.research.google.com/drive/1gxdkgRVfM55zihY9TFLja97cSVZOZq2B?usp=sharing) and [vision-based environment](https://colab.research.google.com/drive/18GIHeOQ5DyjMN8iIRZL2EKZ0745NLIpg?usp=sharing).

## ğŸ§¾ Checkout our experiment logs!
For each experiment used to generate Table I,II and IV in the [paper](https://diffusion-policy.cs.columbia.edu/#paper), we provide:
1. A `config.yaml` that contains all parameters needed to reproduce the experiment.
2. Detailed training/eval `logs.json.txt` for every training step.
3. Checkpoints for the best `epoch=*-test_mean_score=*.ckpt` and last `latest.ckpt` epoch of each run.

Experiment logs are hosted on our website as nested directories in format:
`https://diffusion-policy.cs.columbia.edu/data/experiments/&lt;image|low_dim&gt;/&lt;task&gt;/&lt;method&gt;/`

Within each experiment directory you may find:
```
.
â”œâ”€â”€ config.yaml
â”œâ”€â”€ metrics
â”‚Â Â  â””â”€â”€ logs.json.txt
â”œâ”€â”€ train_0
â”‚Â Â  â”œâ”€â”€ checkpoints
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ epoch=0300-test_mean_score=1.000.ckpt
â”‚Â Â  â”‚Â Â  â””â”€â”€ latest.ckpt
â”‚Â Â  â””â”€â”€ logs.json.txt
â”œâ”€â”€ train_1
â”‚Â Â  â”œâ”€â”€ checkpoints
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ epoch=0250-test_mean_score=1.000.ckpt
â”‚Â Â  â”‚Â Â  â””â”€â”€ latest.ckpt
â”‚Â Â  â””â”€â”€ logs.json.txt
â””â”€â”€ train_2
    â”œâ”€â”€ checkpoints
    â”‚Â Â  â”œâ”€â”€ epoch=0250-test_mean_score=1.000.ckpt
    â”‚Â Â  â””â”€â”€ latest.ckpt
    â””â”€â”€ logs.json.txt
```
The `metrics/logs.json.txt` file aggregates evaluation metrics from all 3 training runs every 50 epochs using `multirun_metrics.py`. The numbers reported in the paper correspond to `max` and `k_min_train_loss` aggregation keys.

To download all files in a subdirectory, use:

```console
$ wget --recursive --no-parent --no-host-directories --relative --reject=&quot;index.html*&quot; https://diffusion-policy.cs.columbia.edu/data/experiments/low_dim/square_ph/diffusion_policy_cnn/
```

## ğŸ› ï¸ Installation
### ğŸ–¥ï¸ Simulation
To reproduce our simulation benchmark results, install our conda environment on a Linux machine with Nvidia GPU. On Ubuntu 20.04 you need to install the following apt packages for mujoco:
```console
$ sudo apt install -y libosmesa6-dev libgl1-mesa-glx libglfw3 patchelf
```

We recommend [Mambaforge](https://github.com/conda-forge/miniforge#mambaforge) instead of the standard anaconda distribution for faster installation: 
```console
$ mamba env create -f conda_environment.yaml
```

but you can use conda as well: 
```console
$ conda env create -f conda_environment.yaml
```

The `conda_environment_macos.yaml` file is only for development on MacOS and does not have full support for benchmarks.

### ğŸ¦¾ Real Robot
Hardware (for Push-T):
* 1x [UR5-CB3](https://www.universal-robots.com/cb3) or [UR5e](https://www.universal-robots.com/products/ur5-robot/) ([RTDE Interface](https://www.universal-robots.com/articles/ur/interface-communication/real-time-data-exchange-rtde-guide/) is required)
* 2x [RealSense D415](https://www.intelrealsense.com/depth-camera-d415/)
* 1x [3Dconnexion SpaceMouse](https://3dconnexion.com/us/product/spacemouse-wireless/) (for teleop)
* 1x [Millibar Robotics Manual Tool Changer](https://www.millibar.com/manual-tool-changer/) (only need robot side)
* 1x 3D printed [End effector](https://cad.onshape.com/documents/a818888644a15afa6cc68ee5/w/2885b48b018cda84f425beca/e/3e8771c2124cee024edd2fed?renderMode=0&amp;uiState=63ffcba6631ca919895e64e5)
* 1x 3D printed [T-block](https://cad.onshape.com/documents/f1140134e38f6ed6902648d5/w/a78cf81827600e4ff4058d03/e/f35f57fb7589f72e05c76caf?renderMode=0&amp;uiState=63ffcbc9af4a881b344898ee)
* USB-C cables and screws for RealSense

Software:
* Ubuntu 20.04.3 (tested)
* Mujoco dependencies: 
`sudo apt install libosmesa6-dev libgl1-mesa-glx libglfw3 patchelf`
* [RealSense SDK](https://github.com/IntelRealSense/librealsense/blob/master/doc/distribution_linux.md)
* Spacemouse dependencies: 
`sudo apt install libspnav-dev spacenavd; sudo systemctl start spacenavd`
* Conda environment `mamba env create -f conda_environment_real.yaml`

## ğŸ–¥ï¸ Reproducing Simulation Benchmark Results 
### Download Training Data
Under the repo root, create data subdirectory:
```console
[diffusion_policy]$ mkdir data &amp;&amp; cd data
```

Download the corresponding zip file from [https://diffusion-policy.cs.columbia.edu/data/training/](https://diffusion-policy.cs.columbia.edu/data/training/)
```console
[data]$ wget https://diffusion-policy.cs.columbia.edu/data/training/pusht.zip
```

Extract training data:
```console
[data]$ unzip pusht.zip &amp;&amp; rm -f pusht.zip &amp;&amp; cd ..
```

Grab config file for the corresponding experiment:
```console
[diffusion_policy]$ wget -O image_pusht_diffusion_policy_cnn.yaml https://diffusion-policy.cs.columbia.edu/data/experiments/image/pusht/diffusion_policy_cnn/config.yaml
```

### Running for a single seed
Activate conda environment and login to [wandb](https://wandb.ai) (if you haven&#039;t already).
```console
[diffusion_policy]$ conda activate robodiff
(robodiff)[diffusion_policy]$ wandb login
```

Launch training with seed 42 on GPU 0.
```console
(robodiff)[diffusion_policy]$ python train.py --config-dir=. --config-name=image_pusht_diffusion_policy_cnn.yaml training.seed=42 training.device=cuda:0 hydra.run.dir=&#039;data/outputs/${now:%Y.%m.%d}/${now:%H.%M.%S}_${name}_${task_name}&#039;
```

This will create a directory in format `data/outputs/yyyy.mm.dd/hh.mm.ss_&lt;method_name&gt;_&lt;task_name&gt;` where configs, logs and checkpoints are written to. The policy will be evaluated every 50 epochs with the success rate logged as `test/mean_score` on wandb, as well as videos for some rollouts.
```console
(robodiff)[diffusion_policy]$ tree data/outputs/2023.03.01/20.02.03_train_diffusion_unet_hybrid_pusht_image -I wandb
data/outputs/2023.03.01/20.02.03_train_diffusion_unet_hybrid_pusht_image
â”œâ”€â”€ checkpoints
â”‚   â”œâ”€â”€ epoch=0000-test_mean_score=0.134.ckpt
â”‚   â””â”€â”€ latest.ckpt
â”œâ”€â”€ .hydra
â”‚   â”œâ”€â”€ config.yaml
â”‚   â”œâ”€â”€ hydra.yaml
â”‚   â””â”€â”€ overrides.yaml
â”œâ”€â”€ logs.json.txt
â”œâ”€â”€ media
â”‚   â”œâ”€â”€ 2k5u6wli.mp4
â”‚   â”œâ”€â”€ 2kvovxms.mp4
â”‚   â”œâ”€â”€ 2pxd9f6b.mp4
â”‚   â”œâ”€â”€ 2q5gjt5f.mp4
â”‚   â”œâ”€â”€ 2sawbf6m.mp4
â”‚   â””â”€â”€ 538ubl79.mp4
â””â”€â”€ train.log

3 directories, 13 files
```

### Running for multiple seeds
Launch local ray cluster. For large scale experiments, you might want to setup an [AWS cluster with autoscaling](https://docs.ray.io/en/master/cluster/vms/user-guides/launching-clusters/aws.html). All other commands remain the same.
```console
(robodiff)[diffusion_policy]$ export CUDA_VISIBLE_DEVICES=0,1,2  # select GPUs to be managed by the ray cluster
(robodiff)[diffusion_policy]$ ray start --head --num-gpus=3
```

Launch a ray client which will start 3 training workers (3 seeds) and 1 metrics monitor worker.
```console
(robodiff)[diffusion_policy]$ python ray_train_multirun.py --config-dir=. --config-name=image_pusht_diffusion_policy_cnn.yaml --seeds=42,43,44 --monitor_key=test/mean_score -- multi_run.run_dir=&#039;data/outputs/${now:%Y.%m.%d}/${now:%H.%M.%S}_${name}_${task_name}&#039; multi_run.wandb_name_base=&#039;${now:%Y.%m.%d-%H.%M.%S}_${name}_${task_name}&#039;
```

In addition to the wandb log written by each training worker individually, the metrics monitor worker will log to wandb project `diffusion_policy_metrics` for the metrics aggregated from all 3 training runs. Local config, logs and checkpoints will be written to `data/outputs/yyyy.mm.dd/hh.mm.ss_&lt;method_name&gt;_&lt;task_name&gt;` in a directory structure identical to our [training logs](https://diffusion-policy.cs.columbia.edu/data/experiments/):
```console
(robodiff)[diffusion_policy]$ tree data/outputs/2023.03.01/22.13.58_train_diffusion_unet_hybrid_pusht_image -I &#039;wandb|media&#039;
data/outputs/2023.03.01/22.13.58_train_diffusion_unet_hybrid_pusht_image
â”œâ”€â”€ config.yaml
â”œâ”€â”€ metrics
â”‚   â”œâ”€â”€ logs.json.txt
â”‚   â”œâ”€â”€ metrics.json
â”‚   â””â”€â”€ metrics.log
â”œâ”€â”€ train_0
â”‚   â”œâ”€â”€ checkpoints
â”‚   â”‚   â”œâ”€â”€ epoch=0000-test_mean_score=0.174.ckpt
â”‚   â”‚   â””â”€â”€ latest.ckpt
â”‚   â”œâ”€â”€ logs.json.txt
â”‚   â””â”€â”€ train.log
â”œâ”€â”€ train_1
â”‚   â”œâ”€â”€ checkpoints
â”‚   â”‚   â”œâ”€â”€ epoch=0000-test_mean_score=0.131.ckpt
â”‚   â”‚   â””â”€â”€ latest.ckpt
â”‚   â”œâ”€â”€ logs.json.txt
â”‚   â””â”€â”€ train.log
â””â”€â”€ train_2
    â”œâ”€â”€ checkpoints
    â”‚   â”œâ”€â”€ epoch=0000-test_mean_score=0.105.ckpt
    â”‚   â””â”€â”€ latest.ckpt
    â”œâ”€â”€ logs.json.txt
    â””â”€â”€ train.log

7 directories, 16 files
```
### ğŸ†• Evaluate Pre-trained Checkpoints
Download a checkpoint from the published training log folders, such as [https://diffusion-policy.cs.columbia.edu/data/experiments/low_dim/pusht/diffusion_policy_cnn/train_0/checkpoints/epoch=0550-test_mean_score=0.969.ckpt](https://diffusion-policy.cs.columbia.edu/data/experiments/low_dim/pusht/diffusion_policy_cnn/train_0/checkpoints/epoch=0550-test_mean_score=0.969.ckpt).

Run the evaluation script:
```console
(robodiff)[diffusion_policy]$ python eval.py --checkpoint data/0550-test_mean_score=0.969.ckpt --output_dir data/pusht_eval_output --device cuda:0
```

This will generate the following directory structure:
```console
(robodiff)[diffusion_policy]$ tree data/pusht_eval_output
data/pusht_eval_output
â”œâ”€â”€ eval_log.json
â””â”€â”€ media
    â”œâ”€â”€ 1fxtno84.mp4
    â”œâ”€â”€ 224l7jqd.mp4
    â”œâ”€â”€ 2fo4btlf.mp4
    â”œâ”€â”€ 2in4cn7a.mp4
    â”œâ”€â”€ 34b3o2qq.mp4
    â””â”€â”€ 3p7jqn32.mp4

1 directory, 7 files
```

`eval_log.json` contains metrics that is logged to wandb during training:
```console
(robodiff)[diffusion_policy]$ cat data/pusht_eval_output/eval_log.json
{
  &quot;test/mean_score&quot;: 0.9150393806777066,
  &quot;test/sim_max_reward_4300000&quot;: 1.0,
  &quot;test/sim_max_reward_4300001&quot;: 0.9872969750774386,
...
  &quot;train/sim_video_1&quot;: &quot;data/pusht_eval_output//media/2fo4btlf.mp4&quot;
}
```

## ğŸ¦¾ Demo, Training and Eval on a Real Robot
Make sure your UR5 robot is running and accepting command from its network interface (emergency stop button within reach at all time), your RealSense cameras plugged in to your workstation (tested with `realsense-viewer`) and your SpaceMouse connected with the `spacenavd` daemon running (verify with `systemctl status spacenavd`).

Start the demonstration collection script. Press &quot;C&quot; to start recording. Use SpaceMouse to move the robot. Press &quot;S&quot; to stop recording. 
```console
(robodiff)[diffusion_policy]$ python demo_real_robot.py -o data/demo_pusht_real --robot_ip 192.168.0.204
```

This should result in a demonstration dataset in `data/demo_pusht_real` with in the same structure as our example [real Push-T training dataset](https://diffusion-policy.cs.columbia.edu/data/training/pusht_real.zip).

To train a Diffusion Policy, launch training with config:
```console
(robodiff)[diffusion_policy]$ python train.py --config-name=train_diffusion_unet_real_image_workspace task.dataset_path=data/demo_pusht_real
```
Edit [`diffusion_policy/config/task/real_pusht_image.yaml`](./diffusion_policy/config/task/real_pusht_image.yaml) if your camera setup is different.

Assuming the training has finished and you have a checkpoint at `data/outputs/blah/checkpoints/latest.ckpt`, launch the evaluation script with:
```console
python eval_real_robot.py -i data/outputs/blah/checkpoints/latest.ckpt -o data/eval_pusht_real --robot_ip 192.168.0.204
```
Press &quot;C&quot; to start evaluation (handing control over to the policy). Press &quot;S&quot; to stop the current episode.

## ğŸ—ºï¸ Codebase Tutorial
This codebase is structured under the requirement that:
1. implementing `N` tasks and `M` methods will only require `O(N+M)` amount of code instead of `O(N*M)`
2. while retaining maximum flexibility.

To achieve this requirement, we 
1. maintained a simple unified interface between tasks and methods and 
2. made the implementation of the tasks and the methods independent of each other. 

These design decisions come at the cost of code repetition between the tasks and the methods. However, we believe that the benefit of being able to add/modify task/methods without affecting the remainder and being able understand a task/method by reading the code linearly outweighs the cost of copying and pasting ğŸ˜Š.

### The Split
On the task side, we have:
* `Dataset`: adapts a (third-party) dataset to the interface.
* `EnvRunner`: executes a `Policy` that accepts the interface and produce logs and metrics.
* `config/task/&lt;task_name&gt;.yaml`: contains all information needed to construct `Dataset` and `EnvRunner`.
* (optional) `Env`: an `gym==0.21.0` compatible class that encapsulates the task environment.

On the policy side, we have:
* `Policy`: implements inference according to the interface and part of the training process.
* `Workspace`: manages the life-cycle of training and evaluation (interleaved) of a method. 
* `config/&lt;workspace_name&gt;.yaml`: contains all information needed to construct `Policy` and `Workspace`.

### The Interface
#### Low Dim
A [`LowdimPolicy`](./diffusion_policy/policy/base_lowdim_policy.py) takes observation dictionary:
- `&quot;obs&quot;:` Tensor of shape `(B,To,Do)`

and predicts action dictionary:
- `&quot;action&quot;: ` Tensor of shape `(B,Ta,Da)`

A [`LowdimDataset`](./diffusion_policy/dataset/base_dataset.py) returns a sample of dictionary:
- `&quot;obs&quot;:` Tensor of shape `(To, Do)`
- `&quot;action&quot;:` Tensor of shape `(Ta, Da)`

Its `get_normalizer` method returns a [`LinearNormalizer`](./diffusion_policy/model/common/normalizer.py) with keys `&quot;obs&quot;,&quot;action&quot;`.

The `Policy` handles normalization on GPU with its copy of the `LinearNormalizer`. The parameters of the `LinearNormalizer` is saved as part of the `Policy`&#039;s weights checkpoint.

#### Image
A [`ImagePolicy`](./diffusion_policy/policy/base_image_policy.py) takes observation dictionary:
- `&quot;key0&quot;:` Tensor of shape `(B,To,*)`
- `&quot;key1&quot;:` Tensor of shape e.g. `(B,To,H,W,3)` ([0,1] float32)

and predicts action dictionary:
- `&quot;action&quot;: ` Tensor of shape `(B,Ta,Da)`

A [`ImageDataset`](./diffusion_policy/dataset/base_dataset.py) returns a sample of dictionary:
- `&quot;obs&quot;:` Dict of
    - `&quot;key0&quot;:` Tensor of shape `(To, *)`
    - `&quot;key1&quot;:` Tensor fo shape `(To,H,W,3)`
- `&quot;action&quot;:` Tensor of shape `(Ta, Da)`

Its `get_normalizer` method returns a [`LinearNormalizer`](./diffusion_policy/model/common/normalizer.py) with keys `&quot;key0&quot;,&quot;key1&quot;,&quot;action&quot;`.

#### Example
```
To = 3
Ta = 4
T = 6
|o|o|o|
| | |a|a|a|a|
|o|o|
| |a|a|a|a|a|
| | | | |a|a|
```
Terminology in the paper: `varname` in the codebase
- Observation Horizon: `To|n_obs_steps`
- Action Horizon: `Ta|n_action_steps`
- Prediction Horizon: `T|horizon`

The classical (e.g. MDP) single step observation/action formulation is included as a special case where `To=1` and `Ta=1`.

## ğŸ”© Key Components
### `Workspace`
A `Workspace` object encapsulates all states and code needed to run an experiment. 
* Inherits from [`BaseWorkspace`](./diffusion_policy/workspace/base_workspace.py).
* A single `OmegaConf` config object generated by `hydra` should contain all information needed to construct the Workspace object and running experiments. This config correspond to `config/&lt;workspace_name&gt;.yaml` + hydra overrides.
* The `run` method contains the entire pipeline for the experiment.
* Checkpoints happen at the `Workspace` level. All training states implemented as object attributes are automatically saved by the `save_checkpoint` method.
* All other states for the experiment should be implemented as local variables in the `run` method.

The entrypoint for training is `train.py` which uses `@hydra.main` decorator. Read [hydra](https://hydra.cc/)&#039;s official documentation for command line arguments and config overrides. For example, the argument `task=&lt;task_name&gt;` will replace the `task` subtree of the config with the content of `config/task/&lt;task_name&gt;.yaml`, thereby selecting the task to run for this experiment.

### `Dataset`
A `Dataset` object:
* Inherits from `torch.utils.data.Dataset`.
* Returns a sample conforming to [the interface](#the-interface) depending on whether the task has Low Dim or Image observations.
* Has a method `get_normalizer` that returns a `LinearNormalizer` conforming to [the interface](#the-interface).

Normalization is a very common source of bugs during project development. It is sometimes helpful to print out the specific `scale` and `bias` vectors used for each key in the `LinearNormalizer`.

Most of our implementations of `Dataset` uses a combination of [`ReplayBuffer`](#replaybuffer) and [`SequenceSampler`](./diffusion_policy/common/sampler.py) to generate samples. Correctly handling padding at the beginning and the end of each demonstration episode according to `To` and `Ta` is important for good performance. Please read our [`SequenceSampler`](./diffusion_policy/common/sampler.py) before implementing your own sampling method.

### `Policy`
A `Policy` object:
* Inherits from `BaseLowdimPolicy` or `BaseImagePolicy`.
* Has a method `predict_action` that given observation dict, predicts actions conforming to [the interface](#the-interface).
* Has a method `set_normalizer` that takes in a `LinearNormalizer` and handles observation/action normalization internally in the policy.
* (optional) Might has a method `compute_loss` that takes in a batch and returns the loss to be optimized.
* (optional) Usually each `Policy` class correspond to a `Workspace` class due to the differences of training and evaluation process between methods.

### `EnvRunner`
A `EnvRunner` object abstracts away the subtle differences between different task environments.
* Has a method `run` that takes a `Policy` object for evaluation, and returns a dict of logs and metrics. Each value should be compatible with `wandb.log`. 

To maximize evaluation speed, we usually vectorize environments using our modification of [`gym.vector.AsyncVectorEnv`](./diffusion_policy/gym_util/async_vector_env.py) which runs each individual environment in a separate process (workaround python GIL). 

âš ï¸ Since subprocesses are launched using `fork` on linux, you need to be specially careful for environments that creates its OpenGL context during initialization (e.g. robosuite) which, once inherited by the child process memory space, often causes obscure bugs like segmentation fault. As a workaround, you can provide a `dummy_env_fn` that constructs an environment without initializing OpenGL.

### `ReplayBuffer`
The [`ReplayBuffer`](./diffusion_policy/common/replay_buffer.py) is a key data structure for storing a demonstration dataset both in-memory and on-disk with chunking and compression. It makes heavy use of the [`zarr`](https://zarr.readthedocs.io/en/stable/index.html) format but also has a `numpy` backend for lower access overhead.

On disk, it can be stored as a nested directory (e.g. `data/pusht_cchi_v7_replay.zarr`) or a zip file (e.g. `data/robomimic/datasets/square/mh/image_abs.hdf5.zarr.zip`).

Due to the relative small size of our datasets, it&#039;s often possible to store the entire image-based dataset in RAM with [`Jpeg2000` compression](./diffusion_policy/codecs/imagecodecs_numcodecs.py) which eliminates disk IO during training at the expense increasing of CPU workload.

Example:
```
data/pusht_cchi_v7_replay.zarr
 â”œâ”€

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[donnemartin/system-design-primer]]></title>
            <link>https://github.com/donnemartin/system-design-primer</link>
            <guid>https://github.com/donnemartin/system-design-primer</guid>
            <pubDate>Fri, 23 May 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[Learn how to design large-scale systems. Prep for the system design interview. Includes Anki flashcards.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/donnemartin/system-design-primer">donnemartin/system-design-primer</a></h1>
            <p>Learn how to design large-scale systems. Prep for the system design interview. Includes Anki flashcards.</p>
            <p>Language: Python</p>
            <p>Stars: 300,534</p>
            <p>Forks: 49,844</p>
            <p>Stars today: 342 stars today</p>
            <h2>README</h2><pre>*[English](README.md) âˆ™ [æ—¥æœ¬èª](README-ja.md) âˆ™ [ç®€ä½“ä¸­æ–‡](README-zh-Hans.md) âˆ™ [ç¹é«”ä¸­æ–‡](README-zh-TW.md) | [Ø§Ù„Ø¹ÙØ±ÙØ¨ÙÙŠÙÙ‘Ø©â€](https://github.com/donnemartin/system-design-primer/issues/170) âˆ™ [à¦¬à¦¾à¦‚à¦²à¦¾](https://github.com/donnemartin/system-design-primer/issues/220) âˆ™ [PortuguÃªs do Brasil](https://github.com/donnemartin/system-design-primer/issues/40) âˆ™ [Deutsch](https://github.com/donnemartin/system-design-primer/issues/186) âˆ™ [ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬](https://github.com/donnemartin/system-design-primer/issues/130) âˆ™ [×¢×‘×¨×™×ª](https://github.com/donnemartin/system-design-primer/issues/272) âˆ™ [Italiano](https://github.com/donnemartin/system-design-primer/issues/104) âˆ™ [í•œêµ­ì–´](https://github.com/donnemartin/system-design-primer/issues/102) âˆ™ [ÙØ§Ø±Ø³ÛŒ](https://github.com/donnemartin/system-design-primer/issues/110) âˆ™ [Polski](https://github.com/donnemartin/system-design-primer/issues/68) âˆ™ [Ñ€ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº](https://github.com/donnemartin/system-design-primer/issues/87) âˆ™ [EspaÃ±ol](https://github.com/donnemartin/system-design-primer/issues/136) âˆ™ [à¸ à¸²à¸©à¸²à¹„à¸—à¸¢](https://github.com/donnemartin/system-design-primer/issues/187) âˆ™ [TÃ¼rkÃ§e](https://github.com/donnemartin/system-design-primer/issues/39) âˆ™ [tiáº¿ng Viá»‡t](https://github.com/donnemartin/system-design-primer/issues/127) âˆ™ [FranÃ§ais](https://github.com/donnemartin/system-design-primer/issues/250) | [Add Translation](https://github.com/donnemartin/system-design-primer/issues/28)*

**Help [translate](TRANSLATIONS.md) this guide!**

# The System Design Primer

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/jj3A5N8.png&quot;&gt;
  &lt;br/&gt;
&lt;/p&gt;

## Motivation

&gt; Learn how to design large-scale systems.
&gt;
&gt; Prep for the system design interview.

### Learn how to design large-scale systems

Learning how to design scalable systems will help you become a better engineer.

System design is a broad topic.  There is a **vast amount of resources scattered throughout the web** on system design principles.

This repo is an **organized collection** of resources to help you learn how to build systems at scale.

### Learn from the open source community

This is a continually updated, open source project.

[Contributions](#contributing) are welcome!

### Prep for the system design interview

In addition to coding interviews, system design is a **required component** of the **technical interview process** at many tech companies.

**Practice common system design interview questions** and **compare** your results with **sample solutions**: discussions, code, and diagrams.

Additional topics for interview prep:

* [Study guide](#study-guide)
* [How to approach a system design interview question](#how-to-approach-a-system-design-interview-question)
* [System design interview questions, **with solutions**](#system-design-interview-questions-with-solutions)
* [Object-oriented design interview questions, **with solutions**](#object-oriented-design-interview-questions-with-solutions)
* [Additional system design interview questions](#additional-system-design-interview-questions)

## Anki flashcards

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/zdCAkB3.png&quot;&gt;
  &lt;br/&gt;
&lt;/p&gt;

The provided [Anki flashcard decks](https://apps.ankiweb.net/) use spaced repetition to help you retain key system design concepts.

* [System design deck](https://github.com/donnemartin/system-design-primer/tree/master/resources/flash_cards/System%20Design.apkg)
* [System design exercises deck](https://github.com/donnemartin/system-design-primer/tree/master/resources/flash_cards/System%20Design%20Exercises.apkg)
* [Object oriented design exercises deck](https://github.com/donnemartin/system-design-primer/tree/master/resources/flash_cards/OO%20Design.apkg)

Great for use while on-the-go.

### Coding Resource: Interactive Coding Challenges

Looking for resources to help you prep for the [**Coding Interview**](https://github.com/donnemartin/interactive-coding-challenges)?

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/b4YtAEN.png&quot;&gt;
  &lt;br/&gt;
&lt;/p&gt;

Check out the sister repo [**Interactive Coding Challenges**](https://github.com/donnemartin/interactive-coding-challenges), which contains an additional Anki deck:

* [Coding deck](https://github.com/donnemartin/interactive-coding-challenges/tree/master/anki_cards/Coding.apkg)

## Contributing

&gt; Learn from the community.

Feel free to submit pull requests to help:

* Fix errors
* Improve sections
* Add new sections
* [Translate](https://github.com/donnemartin/system-design-primer/issues/28)

Content that needs some polishing is placed [under development](#under-development).

Review the [Contributing Guidelines](CONTRIBUTING.md).

## Index of system design topics

&gt; Summaries of various system design topics, including pros and cons.  **Everything is a trade-off**.
&gt;
&gt; Each section contains links to more in-depth resources.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/jrUBAF7.png&quot;&gt;
  &lt;br/&gt;
&lt;/p&gt;

* [System design topics: start here](#system-design-topics-start-here)
    * [Step 1: Review the scalability video lecture](#step-1-review-the-scalability-video-lecture)
    * [Step 2: Review the scalability article](#step-2-review-the-scalability-article)
    * [Next steps](#next-steps)
* [Performance vs scalability](#performance-vs-scalability)
* [Latency vs throughput](#latency-vs-throughput)
* [Availability vs consistency](#availability-vs-consistency)
    * [CAP theorem](#cap-theorem)
        * [CP - consistency and partition tolerance](#cp---consistency-and-partition-tolerance)
        * [AP - availability and partition tolerance](#ap---availability-and-partition-tolerance)
* [Consistency patterns](#consistency-patterns)
    * [Weak consistency](#weak-consistency)
    * [Eventual consistency](#eventual-consistency)
    * [Strong consistency](#strong-consistency)
* [Availability patterns](#availability-patterns)
    * [Fail-over](#fail-over)
    * [Replication](#replication)
    * [Availability in numbers](#availability-in-numbers)
* [Domain name system](#domain-name-system)
* [Content delivery network](#content-delivery-network)
    * [Push CDNs](#push-cdns)
    * [Pull CDNs](#pull-cdns)
* [Load balancer](#load-balancer)
    * [Active-passive](#active-passive)
    * [Active-active](#active-active)
    * [Layer 4 load balancing](#layer-4-load-balancing)
    * [Layer 7 load balancing](#layer-7-load-balancing)
    * [Horizontal scaling](#horizontal-scaling)
* [Reverse proxy (web server)](#reverse-proxy-web-server)
    * [Load balancer vs reverse proxy](#load-balancer-vs-reverse-proxy)
* [Application layer](#application-layer)
    * [Microservices](#microservices)
    * [Service discovery](#service-discovery)
* [Database](#database)
    * [Relational database management system (RDBMS)](#relational-database-management-system-rdbms)
        * [Master-slave replication](#master-slave-replication)
        * [Master-master replication](#master-master-replication)
        * [Federation](#federation)
        * [Sharding](#sharding)
        * [Denormalization](#denormalization)
        * [SQL tuning](#sql-tuning)
    * [NoSQL](#nosql)
        * [Key-value store](#key-value-store)
        * [Document store](#document-store)
        * [Wide column store](#wide-column-store)
        * [Graph Database](#graph-database)
    * [SQL or NoSQL](#sql-or-nosql)
* [Cache](#cache)
    * [Client caching](#client-caching)
    * [CDN caching](#cdn-caching)
    * [Web server caching](#web-server-caching)
    * [Database caching](#database-caching)
    * [Application caching](#application-caching)
    * [Caching at the database query level](#caching-at-the-database-query-level)
    * [Caching at the object level](#caching-at-the-object-level)
    * [When to update the cache](#when-to-update-the-cache)
        * [Cache-aside](#cache-aside)
        * [Write-through](#write-through)
        * [Write-behind (write-back)](#write-behind-write-back)
        * [Refresh-ahead](#refresh-ahead)
* [Asynchronism](#asynchronism)
    * [Message queues](#message-queues)
    * [Task queues](#task-queues)
    * [Back pressure](#back-pressure)
* [Communication](#communication)
    * [Transmission control protocol (TCP)](#transmission-control-protocol-tcp)
    * [User datagram protocol (UDP)](#user-datagram-protocol-udp)
    * [Remote procedure call (RPC)](#remote-procedure-call-rpc)
    * [Representational state transfer (REST)](#representational-state-transfer-rest)
* [Security](#security)
* [Appendix](#appendix)
    * [Powers of two table](#powers-of-two-table)
    * [Latency numbers every programmer should know](#latency-numbers-every-programmer-should-know)
    * [Additional system design interview questions](#additional-system-design-interview-questions)
    * [Real world architectures](#real-world-architectures)
    * [Company architectures](#company-architectures)
    * [Company engineering blogs](#company-engineering-blogs)
* [Under development](#under-development)
* [Credits](#credits)
* [Contact info](#contact-info)
* [License](#license)

## Study guide

&gt; Suggested topics to review based on your interview timeline (short, medium, long).

![Imgur](images/OfVllex.png)

**Q: For interviews, do I need to know everything here?**

**A: No, you don&#039;t need to know everything here to prepare for the interview**.

What you are asked in an interview depends on variables such as:

* How much experience you have
* What your technical background is
* What positions you are interviewing for
* Which companies you are interviewing with
* Luck

More experienced candidates are generally expected to know more about system design.  Architects or team leads might be expected to know more than individual contributors.  Top tech companies are likely to have one or more design interview rounds.

Start broad and go deeper in a few areas.  It helps to know a little about various key system design topics.  Adjust the following guide based on your timeline, experience, what positions you are interviewing for, and which companies you are interviewing with.

* **Short timeline** - Aim for **breadth** with system design topics.  Practice by solving **some** interview questions.
* **Medium timeline** - Aim for **breadth** and **some depth** with system design topics.  Practice by solving **many** interview questions.
* **Long timeline** - Aim for **breadth** and **more depth** with system design topics.  Practice by solving **most** interview questions.

| | Short | Medium | Long |
|---|---|---|---|
| Read through the [System design topics](#index-of-system-design-topics) to get a broad understanding of how systems work | :+1: | :+1: | :+1: |
| Read through a few articles in the [Company engineering blogs](#company-engineering-blogs) for the companies you are interviewing with | :+1: | :+1: | :+1: |
| Read through a few [Real world architectures](#real-world-architectures) | :+1: | :+1: | :+1: |
| Review [How to approach a system design interview question](#how-to-approach-a-system-design-interview-question) | :+1: | :+1: | :+1: |
| Work through [System design interview questions with solutions](#system-design-interview-questions-with-solutions) | Some | Many | Most |
| Work through [Object-oriented design interview questions with solutions](#object-oriented-design-interview-questions-with-solutions) | Some | Many | Most |
| Review [Additional system design interview questions](#additional-system-design-interview-questions) | Some | Many | Most |

## How to approach a system design interview question

&gt; How to tackle a system design interview question.

The system design interview is an **open-ended conversation**.  You are expected to lead it.

You can use the following steps to guide the discussion.  To help solidify this process, work through the [System design interview questions with solutions](#system-design-interview-questions-with-solutions) section using the following steps.

### Step 1: Outline use cases, constraints, and assumptions

Gather requirements and scope the problem.  Ask questions to clarify use cases and constraints.  Discuss assumptions.

* Who is going to use it?
* How are they going to use it?
* How many users are there?
* What does the system do?
* What are the inputs and outputs of the system?
* How much data do we expect to handle?
* How many requests per second do we expect?
* What is the expected read to write ratio?

### Step 2: Create a high level design

Outline a high level design with all important components.

* Sketch the main components and connections
* Justify your ideas

### Step 3: Design core components

Dive into details for each core component.  For example, if you were asked to [design a url shortening service](solutions/system_design/pastebin/README.md), discuss:

* Generating and storing a hash of the full url
    * [MD5](solutions/system_design/pastebin/README.md) and [Base62](solutions/system_design/pastebin/README.md)
    * Hash collisions
    * SQL or NoSQL
    * Database schema
* Translating a hashed url to the full url
    * Database lookup
* API and object-oriented design

### Step 4: Scale the design

Identify and address bottlenecks, given the constraints.  For example, do you need the following to address scalability issues?

* Load balancer
* Horizontal scaling
* Caching
* Database sharding

Discuss potential solutions and trade-offs.  Everything is a trade-off.  Address bottlenecks using [principles of scalable system design](#index-of-system-design-topics).

### Back-of-the-envelope calculations

You might be asked to do some estimates by hand.  Refer to the [Appendix](#appendix) for the following resources:

* [Use back of the envelope calculations](http://highscalability.com/blog/2011/1/26/google-pro-tip-use-back-of-the-envelope-calculations-to-choo.html)
* [Powers of two table](#powers-of-two-table)
* [Latency numbers every programmer should know](#latency-numbers-every-programmer-should-know)

### Source(s) and further reading

Check out the following links to get a better idea of what to expect:

* [How to ace a systems design interview](https://www.palantir.com/2011/10/how-to-rock-a-systems-design-interview/)
* [The system design interview](http://www.hiredintech.com/system-design)
* [Intro to Architecture and Systems Design Interviews](https://www.youtube.com/watch?v=ZgdS0EUmn70)
* [System design template](https://leetcode.com/discuss/career/229177/My-System-Design-Template)

## System design interview questions with solutions

&gt; Common system design interview questions with sample discussions, code, and diagrams.
&gt;
&gt; Solutions linked to content in the `solutions/` folder.

| Question | |
|---|---|
| Design Pastebin.com (or Bit.ly) | [Solution](solutions/system_design/pastebin/README.md) |
| Design the Twitter timeline and search (or Facebook feed and search) | [Solution](solutions/system_design/twitter/README.md) |
| Design a web crawler | [Solution](solutions/system_design/web_crawler/README.md) |
| Design Mint.com | [Solution](solutions/system_design/mint/README.md) |
| Design the data structures for a social network | [Solution](solutions/system_design/social_graph/README.md) |
| Design a key-value store for a search engine | [Solution](solutions/system_design/query_cache/README.md) |
| Design Amazon&#039;s sales ranking by category feature | [Solution](solutions/system_design/sales_rank/README.md) |
| Design a system that scales to millions of users on AWS | [Solution](solutions/system_design/scaling_aws/README.md) |
| Add a system design question | [Contribute](#contributing) |

### Design Pastebin.com (or Bit.ly)

[View exercise and solution](solutions/system_design/pastebin/README.md)

![Imgur](images/4edXG0T.png)

### Design the Twitter timeline and search (or Facebook feed and search)

[View exercise and solution](solutions/system_design/twitter/README.md)

![Imgur](images/jrUBAF7.png)

### Design a web crawler

[View exercise and solution](solutions/system_design/web_crawler/README.md)

![Imgur](images/bWxPtQA.png)

### Design Mint.com

[View exercise and solution](solutions/system_design/mint/README.md)

![Imgur](images/V5q57vU.png)

### Design the data structures for a social network

[View exercise and solution](solutions/system_design/social_graph/README.md)

![Imgur](images/cdCv5g7.png)

### Design a key-value store for a search engine

[View exercise and solution](solutions/system_design/query_cache/README.md)

![Imgur](images/4j99mhe.png)

### Design Amazon&#039;s sales ranking by category feature

[View exercise and solution](solutions/system_design/sales_rank/README.md)

![Imgur](images/MzExP06.png)

### Design a system that scales to millions of users on AWS

[View exercise and solution](solutions/system_design/scaling_aws/README.md)

![Imgur](images/jj3A5N8.png)

## Object-oriented design interview questions with solutions

&gt; Common object-oriented design interview questions with sample discussions, code, and diagrams.
&gt;
&gt; Solutions linked to content in the `solutions/` folder.

&gt;**Note: This section is under development**

| Question | |
|---|---|
| Design a hash map | [Solution](solutions/object_oriented_design/hash_table/hash_map.ipynb)  |
| Design a least recently used cache | [Solution](solutions/object_oriented_design/lru_cache/lru_cache.ipynb)  |
| Design a call center | [Solution](solutions/object_oriented_design/call_center/call_center.ipynb)  |
| Design a deck of cards | [Solution](solutions/object_oriented_design/deck_of_cards/deck_of_cards.ipynb)  |
| Design a parking lot | [Solution](solutions/object_oriented_design/parking_lot/parking_lot.ipynb)  |
| Design a chat server | [Solution](solutions/object_oriented_design/online_chat/online_chat.ipynb)  |
| Design a circular array | [Contribute](#contributing)  |
| Add an object-oriented design question | [Contribute](#contributing) |

## System design topics: start here

New to system design?

First, you&#039;ll need a basic understanding of common principles, learning about what they are, how they are used, and their pros and cons.

### Step 1: Review the scalability video lecture

[Scalability Lecture at Harvard](https://www.youtube.com/watch?v=-W9F__D3oY4)

* Topics covered:
    * Vertical scaling
    * Horizontal scaling
    * Caching
    * Load balancing
    * Database replication
    * Database partitioning

### Step 2: Review the scalability article

[Scalability](https://web.archive.org/web/20221030091841/http://www.lecloud.net/tagged/scalability/chrono)

* Topics covered:
    * [Clones](https://web.archive.org/web/20220530193911/https://www.lecloud.net/post/7295452622/scalability-for-dummies-part-1-clones)
    * [Databases](https://web.archive.org/web/20220602114024/https://www.lecloud.net/post/7994751381/scalability-for-dummies-part-2-database)
    * [Caches](https://web.archive.org/web/20230126233752/https://www.lecloud.net/post/9246290032/scalability-for-dummies-part-3-cache)
    * [Asynchronism](https://web.archive.org/web/20220926171507/https://www.lecloud.net/post/9699762917/scalability-for-dummies-part-4-asynchronism)

### Next steps

Next, we&#039;ll look at high-level trade-offs:

* **Performance** vs **scalability**
* **Latency** vs **throughput**
* **Availability** vs **consistency**

Keep in mind that **everything is a trade-off**.

Then we&#039;ll dive into more specific topics such as DNS, CDNs, and load balancers.

## Performance vs scalability

A service is **scalable** if it results in increased **performance** in a manner proportional to resources added. Generally, increasing performance means serving more units of work, but it can also be to handle larger units of work, such as when datasets grow.&lt;sup&gt;&lt;a href=http://www.allthingsdistributed.com/2006/03/a_word_on_scalability.html&gt;1&lt;/a&gt;&lt;/sup&gt;

Another way to look at performance vs scalability:

* If you have a **performance** problem, your system is slow for a single user.
* If you have a **scalability** problem, your system is fast for a single user but slow under heavy load.

### Source(s) and further reading

* [A word on scalability](http://www.allthingsdistributed.com/2006/03/a_word_on_scalability.html)
* [Scalability, availability, s

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[yutiansut/QUANTAXIS]]></title>
            <link>https://github.com/yutiansut/QUANTAXIS</link>
            <guid>https://github.com/yutiansut/QUANTAXIS</guid>
            <pubDate>Fri, 23 May 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[QUANTAXIS æ”¯æŒä»»åŠ¡è°ƒåº¦ åˆ†å¸ƒå¼éƒ¨ç½²çš„ è‚¡ç¥¨/æœŸè´§/æœŸæƒ æ•°æ®/å›æµ‹/æ¨¡æ‹Ÿ/äº¤æ˜“/å¯è§†åŒ–/å¤šè´¦æˆ· çº¯æœ¬åœ°é‡åŒ–è§£å†³æ–¹æ¡ˆ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/yutiansut/QUANTAXIS">yutiansut/QUANTAXIS</a></h1>
            <p>QUANTAXIS æ”¯æŒä»»åŠ¡è°ƒåº¦ åˆ†å¸ƒå¼éƒ¨ç½²çš„ è‚¡ç¥¨/æœŸè´§/æœŸæƒ æ•°æ®/å›æµ‹/æ¨¡æ‹Ÿ/äº¤æ˜“/å¯è§†åŒ–/å¤šè´¦æˆ· çº¯æœ¬åœ°é‡åŒ–è§£å†³æ–¹æ¡ˆ</p>
            <p>Language: Python</p>
            <p>Stars: 8,803</p>
            <p>Forks: 3,073</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre># QUANTAXIS 2.0.0

[![Github workers](https://img.shields.io/github/watchers/quantaxis/quantaxis.svg?style=social&amp;label=Watchers&amp;)](https://github.com/quantaxis/quantaxis/watchers)
[![GitHub stars](https://img.shields.io/github/stars/quantaxis/quantaxis.svg?style=social&amp;label=Star&amp;)](https://github.com/quantaxis/quantaxis/stargazers)
[![GitHub forks](https://img.shields.io/github/forks/quantaxis/quantaxis.svg?style=social&amp;label=Fork&amp;)](https://github.com/quantaxis/quantaxis/fork)

[ç‚¹å‡»å³ä¸Šè§’Starå’ŒWatchæ¥è·Ÿè¸ªé¡¹ç›®è¿›å±•! ç‚¹å‡»Forkæ¥åˆ›å»ºå±äºä½ çš„QUANTAXIS!]

![QUANTAXIS_LOGO_LAST_small.jpg](http://picx.gulizhu.com/Fn0TPEcwu_uhraf58_93Ul5yfvAz)

![gvp](http://picx.gulizhu.com/gvp.jpg)


æ›´å¤šæ–‡æ¡£åœ¨[QABook Release](https://github.com/QUANTAXIS/QUANTAXIS/releases/download/latest/quantaxis.pdf)

Quantitative Financial FrameWork

æœ¬é¡¹ç›®åˆ†ä¸ºå‡ ä¸ªå¤§å—:


1. QASU/ QAFetch æ”¯æŒå¤šå¸‚åœºæ•°æ®å­˜å‚¨/ è‡ªåŠ¨è¿ç»´/ æ•°æ®è·å–(mongodb/ clickhouse)

2. QAUtil æ”¯æŒäº¤æ˜“æ—¶é—´, äº¤æ˜“æ—¥å†, æ—¶é—´å‘å‰å‘åæ¨ç®—, å¸‚åœºè¯†åˆ«, dataframe æ•°æ®è½¬æ¢ç­‰

3. QIFI/ QAMarket ä¸€å¥—ç»Ÿä¸€çš„å¤šå¸‚åœº å¤šè¯­è¨€è´¦æˆ·ä½“ç³»
    - qifiaccount qifi çš„æ ‡å‡†è´¦æˆ·ä½“ç³»,  åœ¨å¤šè¯­è¨€ä¸Šå’Œ rust/cpp ç‰ˆæœ¬çš„ qifi account ä¿æŒä¸€è‡´æ€§
    - qifimanager  qifi å¤šè´¦æˆ·ç®¡ç†ä½“ç³» æ”¯æŒå¤šä¸ªè¯­è¨€çš„è´¦æˆ·ç»Ÿä¸€ç®¡ç†
    - qaposition  å•æ ‡çš„ä»“ä½ç®¡ç†æ¨¡å—, æ”¯æŒå¯¹äºå•æ ‡çš„çš„ç²¾å‡†å¤šç©ºæ§åˆ¶(å¥—åˆ©åœºæ™¯/ cta åœºæ™¯/ è‚¡ç¥¨åœºæ™¯)
    - marketpreset å¸‚åœºé¢„åˆ¶åŸºç±», æ–¹ä¾¿æŸ¥è¯¢æœŸè´§/è‚¡ç¥¨/è™šæ‹Ÿè´§å¸ å“ç§ tick/ ä¿è¯é‡‘/ æ‰‹ç»­è´¹ç­‰

4. QAFactor å› å­ç ”ç©¶å¥—ä»¶
    - å•å› å­ç ”ç©¶å…¥åº“
    - å› å­ç®¡ç†, æµ‹è¯•
    - å› å­åˆå¹¶

    - [ ] ä¼˜åŒ–å™¨

5. QAData å¤šæ ‡çš„å¤šå¸‚åœºçš„æ•°æ®ç»“æ„, å¯ä»¥ä½œä¸ºå®æ—¶è®¡ç®—å’Œå›æµ‹çš„å†…å­˜æ•°æ®åº“ä½¿ç”¨

6. QAIndicator æ”¯æŒè‡ªå®šä¹‰æŒ‡æ ‡ç¼–å†™, æ‰¹é‡å…¨å¸‚åœº apply, æ”¯æŒå› å­è¡¨è¾¾å¼æ„å»º

7. QAEngine è‡ªå®šä¹‰çº¿ç¨‹è¿›ç¨‹åŸºç±», å¯ä»¥è‡ªè¡Œä¿®æ”¹è®¡ç®—çš„å¼‚æ­¥å’Œå±€åŸŸç½‘å†…åˆ†å¸ƒå¼è®¡ç®— agent

8. QAPubSub åŸºäº MQ çš„æ¶ˆæ¯é˜Ÿåˆ—, æ”¯æŒ 1-1 1-n n-n çš„æ¶ˆæ¯åˆ†å‘, å¯ç”¨äºè®¡ç®—ä»»åŠ¡åˆ†å‘æ”¶é›†, å®æ—¶è®¢å•æµç­‰åœºæ™¯

9. QAStrategy cta/å¥—åˆ©å›æµ‹å¥—ä»¶, æ”¯æŒ QIFI æ¨¡å¼

10. QAWebServer tornadobase çš„ webserver å¥—ä»¶, å¯ä»¥ä½œä¸ºä¸­å°å¾®æœåŠ¡æ„å»º

11. QASchedule åŸºäº QAWerbServer çš„åå°ä»»åŠ¡è°ƒåº¦ æ”¯æŒè‡ªåŠ¨è¿ç»´, è¿œç¨‹ä»»åŠ¡è°ƒåº¦ç­‰



æœ¬ç‰ˆæœ¬ä¸ºä¸å…¼å®¹å‡çº§çš„ 2.0 quantaxis, æ¶‰åŠä¸€äº›æ”¹å˜

## æ•°æ®éƒ¨åˆ†

- å¢åŠ  clickhouse client  è‡ªå»ºæ•°æ®æºåˆ†å‘

- å¢åŠ æ•°æ®æ ¼å¼ 
    - å¯¹äº tabular data çš„æ”¯æŒ
    - æ”¯æŒå› å­åŒ–çš„æ•°æ®ç»“æ„

- æ”¯æŒ tick/l2 order/transaction çš„æ•°æ®æ ¼å¼

## å¾®æœåŠ¡éƒ¨åˆ†

- å¢åŠ  QAWEBSEBVER

- æ”¯æŒåŠ¨æ€çš„ä»»åŠ¡æŒ‡æ´¾çš„ sechedule

- å¢åŠ  åŸºäº DAGæ¨¡å‹çš„pipeline

- å¢åŠ  QAPUBSUBæ¨¡å— æ”¯æŒ rabbitmq

## è´¦æˆ·éƒ¨åˆ†

- åˆ é™¤ QAARP ä¸å†ç»´æŠ¤è€ç‰ˆæœ¬ account ç³»ç»Ÿ

- å‡çº§å®Œæ•´çš„ qifi æ¨¡å— æ”¯æŒå¤šå¸‚åœº/è·¨å¸‚åœºçš„è´¦æˆ·æ¨¡å‹
    - æ”¯æŒä¿è¯é‡‘æ¨¡å‹
    - æ”¯æŒè‚¡ç¥¨
    - æ”¯æŒæœŸè´§

    - æœŸæƒ[å‡çº§ä¸­]


## å®ç›˜æ¨¡æ‹Ÿç›˜éƒ¨åˆ†

- ä½¿ç”¨ç¨³å®šçš„ qifi ç»“æ„å¯¹æ¥

-  æ”¯æŒ CTP æ¥å£çš„
    - æœŸè´§
    - æœŸæƒ
-  æ”¯æŒè‚¡ç¥¨éƒ¨åˆ†
    - QMT å¯¹æ¥

- æ¯å­è´¦æˆ·çš„è®¢å•åˆ†å‘è·Ÿè¸ª [OMS]

- ordergateway é£æ§è®¢å•æµè§„åˆ™

## å¤šè¯­è¨€éƒ¨åˆ†

- æ”¯æŒäº QUANTAXIS Rust ç‰ˆæœ¬çš„é€šä¿¡
- åŸºäº arrow åº“, ä½¿ç”¨å¤šè¯­è¨€æ”¯æŒçš„ pyarrow æ ¼å¼, å¯¹æ¥ arrow-rs, datafusion-rs, libarrow(CPP)

- æ”¯æŒ RUST/ CPP è´¦æˆ·
- æ”¯æŒå› å­åŒ–çš„ rust job worker

## ç¤¾åŒº/é¡¹ç›®æèµ 



### github

QUANTAXIS æ˜¯ä¸€ä¸ªå¼€æ”¾çš„é¡¹ç›®, åœ¨å¼€æºçš„3å¹´ä¸­æœ‰å¤§é‡çš„å°ä¼™ä¼´åŠ å…¥äº†æˆ‘, å¹¶æäº¤äº†ç›¸å…³çš„ä»£ç , æ„Ÿè°¢ä»¥ä¸‹çš„åŒå­¦ä»¬

&lt;a href=&quot;https://github.com/QUANTAXIS/QUANTAXIS/graphs/contributors&quot;&gt;&lt;img src=&quot;https://opencollective.com/QUANTAXIS/contributors.svg?width=890&amp;button=false&quot; /&gt;&lt;/a&gt;



è®¸å¤šé—®é¢˜ å¯ä»¥åœ¨ [GITHUB ISSUE](https://github.com/QUANTAXIS/QUANTAXIS/issues)ä¸­æ‰¾åˆ°, ä½ å¯ä»¥æå‡ºæ–°çš„issue


### æèµ 

å†™ä»£ç ä¸æ˜“...è¯·ä½œè€…å–æ¯å’–å•¡å‘—?


![](http://picx.gulizhu.com/alipay.png)

(PS: æ”¯ä»˜çš„æ—¶å€™ è¯·å¸¦ä¸Šä½ çš„åå­—/æ˜µç§°å‘€ ä¼šç»´æŠ¤ä¸€ä¸ªèµåŠ©åˆ—è¡¨~ )


### QQç¾¤

æ¬¢è¿åŠ ç¾¤è®¨è®º: 563280067 [ç¾¤é“¾æ¥](https://jq.qq.com/?_wv=1027&amp;k=4CEKGzn) 

DISCORD ç¤¾åŒº  https://discord.gg/mkk5RgN


QUANTAXIS å¼€å‘ç¾¤: 773602202 (å¦‚æœæƒ³è¦è´¡çŒ®ä»£ç  è¯·åŠ è¿™ä¸ªç¾¤ éœ€è¦å¤‡æ³¨ä½ çš„GITHUB ID)

QUANTAXIS æœŸè´§å®ç›˜å¤šè´¦æˆ·çš„æœ¬åœ°éƒ¨ç½²ç¾¤ (è¯·å‹¿æµªè´¹ç¾¤èµ„æº æ²¡æœ‰æœ¬åœ°å¤šè´¦æˆ·éƒ¨ç½²çš„è¯·å‹¿åŠ ): 945822690

### å…¬å…±å·

æ¬¢è¿å…³æ³¨å…¬ä¼—å·: ![å…¬ä¼—å·](http://picx.gulizhu.com/Fr0pHbwB7-zrq_HAKsvB8g2zaP_A)

QAPROå…¬å…±å·å…è´¹æä¾›äº†ä¸‹å•æ¨é€æ¥å£, å…³æ³¨å…¬å…±å·å›å¤tradeå³å¯ä½¿ç”¨

### è®ºå› QACLUB

QUANTAXIS å†…æµ‹ç‰ˆè®ºå› [QUANTAXISCLUBä¸Šçº¿](http://www.yutiansut.com:3000)

http://www.yutiansut.com:3000

å‡¡é€šè¿‡è®ºå›è¿›è¡Œæé—®çš„ å‡æœ‰æœ€é«˜çš„å›å¤ä¼˜å…ˆçº§
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[modelcontextprotocol/python-sdk]]></title>
            <link>https://github.com/modelcontextprotocol/python-sdk</link>
            <guid>https://github.com/modelcontextprotocol/python-sdk</guid>
            <pubDate>Fri, 23 May 2025 00:04:20 GMT</pubDate>
            <description><![CDATA[The official Python SDK for Model Context Protocol servers and clients]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/modelcontextprotocol/python-sdk">modelcontextprotocol/python-sdk</a></h1>
            <p>The official Python SDK for Model Context Protocol servers and clients</p>
            <p>Language: Python</p>
            <p>Stars: 12,820</p>
            <p>Forks: 1,494</p>
            <p>Stars today: 120 stars today</p>
            <h2>README</h2><pre># MCP Python SDK

&lt;div align=&quot;center&quot;&gt;

&lt;strong&gt;Python implementation of the Model Context Protocol (MCP)&lt;/strong&gt;

[![PyPI][pypi-badge]][pypi-url]
[![MIT licensed][mit-badge]][mit-url]
[![Python Version][python-badge]][python-url]
[![Documentation][docs-badge]][docs-url]
[![Specification][spec-badge]][spec-url]
[![GitHub Discussions][discussions-badge]][discussions-url]

&lt;/div&gt;

&lt;!-- omit in toc --&gt;
## Table of Contents

- [MCP Python SDK](#mcp-python-sdk)
  - [Overview](#overview)
  - [Installation](#installation)
    - [Adding MCP to your python project](#adding-mcp-to-your-python-project)
    - [Running the standalone MCP development tools](#running-the-standalone-mcp-development-tools)
  - [Quickstart](#quickstart)
  - [What is MCP?](#what-is-mcp)
  - [Core Concepts](#core-concepts)
    - [Server](#server)
    - [Resources](#resources)
    - [Tools](#tools)
    - [Prompts](#prompts)
    - [Images](#images)
    - [Context](#context)
  - [Running Your Server](#running-your-server)
    - [Development Mode](#development-mode)
    - [Claude Desktop Integration](#claude-desktop-integration)
    - [Direct Execution](#direct-execution)
    - [Mounting to an Existing ASGI Server](#mounting-to-an-existing-asgi-server)
  - [Examples](#examples)
    - [Echo Server](#echo-server)
    - [SQLite Explorer](#sqlite-explorer)
  - [Advanced Usage](#advanced-usage)
    - [Low-Level Server](#low-level-server)
    - [Writing MCP Clients](#writing-mcp-clients)
    - [MCP Primitives](#mcp-primitives)
    - [Server Capabilities](#server-capabilities)
  - [Documentation](#documentation)
  - [Contributing](#contributing)
  - [License](#license)

[pypi-badge]: https://img.shields.io/pypi/v/mcp.svg
[pypi-url]: https://pypi.org/project/mcp/
[mit-badge]: https://img.shields.io/pypi/l/mcp.svg
[mit-url]: https://github.com/modelcontextprotocol/python-sdk/blob/main/LICENSE
[python-badge]: https://img.shields.io/pypi/pyversions/mcp.svg
[python-url]: https://www.python.org/downloads/
[docs-badge]: https://img.shields.io/badge/docs-modelcontextprotocol.io-blue.svg
[docs-url]: https://modelcontextprotocol.io
[spec-badge]: https://img.shields.io/badge/spec-spec.modelcontextprotocol.io-blue.svg
[spec-url]: https://spec.modelcontextprotocol.io
[discussions-badge]: https://img.shields.io/github/discussions/modelcontextprotocol/python-sdk
[discussions-url]: https://github.com/modelcontextprotocol/python-sdk/discussions

## Overview

The Model Context Protocol allows applications to provide context for LLMs in a standardized way, separating the concerns of providing context from the actual LLM interaction. This Python SDK implements the full MCP specification, making it easy to:

- Build MCP clients that can connect to any MCP server
- Create MCP servers that expose resources, prompts and tools
- Use standard transports like stdio, SSE, and Streamable HTTP
- Handle all MCP protocol messages and lifecycle events

## Installation

### Adding MCP to your python project

We recommend using [uv](https://docs.astral.sh/uv/) to manage your Python projects. 

If you haven&#039;t created a uv-managed project yet, create one:

   ```bash
   uv init mcp-server-demo
   cd mcp-server-demo
   ```

   Then add MCP to your project dependencies:

   ```bash
   uv add &quot;mcp[cli]&quot;
   ```

Alternatively, for projects using pip for dependencies:
```bash
pip install &quot;mcp[cli]&quot;
```

### Running the standalone MCP development tools

To run the mcp command with uv:

```bash
uv run mcp
```

## Quickstart

Let&#039;s create a simple MCP server that exposes a calculator tool and some data:

```python
# server.py
from mcp.server.fastmcp import FastMCP

# Create an MCP server
mcp = FastMCP(&quot;Demo&quot;)


# Add an addition tool
@mcp.tool()
def add(a: int, b: int) -&gt; int:
    &quot;&quot;&quot;Add two numbers&quot;&quot;&quot;
    return a + b


# Add a dynamic greeting resource
@mcp.resource(&quot;greeting://{name}&quot;)
def get_greeting(name: str) -&gt; str:
    &quot;&quot;&quot;Get a personalized greeting&quot;&quot;&quot;
    return f&quot;Hello, {name}!&quot;
```

You can install this server in [Claude Desktop](https://claude.ai/download) and interact with it right away by running:
```bash
mcp install server.py
```

Alternatively, you can test it with the MCP Inspector:
```bash
mcp dev server.py
```

## What is MCP?

The [Model Context Protocol (MCP)](https://modelcontextprotocol.io) lets you build servers that expose data and functionality to LLM applications in a secure, standardized way. Think of it like a web API, but specifically designed for LLM interactions. MCP servers can:

- Expose data through **Resources** (think of these sort of like GET endpoints; they are used to load information into the LLM&#039;s context)
- Provide functionality through **Tools** (sort of like POST endpoints; they are used to execute code or otherwise produce a side effect)
- Define interaction patterns through **Prompts** (reusable templates for LLM interactions)
- And more!

## Core Concepts

### Server

The FastMCP server is your core interface to the MCP protocol. It handles connection management, protocol compliance, and message routing:

```python
# Add lifespan support for startup/shutdown with strong typing
from contextlib import asynccontextmanager
from collections.abc import AsyncIterator
from dataclasses import dataclass

from fake_database import Database  # Replace with your actual DB type

from mcp.server.fastmcp import Context, FastMCP

# Create a named server
mcp = FastMCP(&quot;My App&quot;)

# Specify dependencies for deployment and development
mcp = FastMCP(&quot;My App&quot;, dependencies=[&quot;pandas&quot;, &quot;numpy&quot;])


@dataclass
class AppContext:
    db: Database


@asynccontextmanager
async def app_lifespan(server: FastMCP) -&gt; AsyncIterator[AppContext]:
    &quot;&quot;&quot;Manage application lifecycle with type-safe context&quot;&quot;&quot;
    # Initialize on startup
    db = await Database.connect()
    try:
        yield AppContext(db=db)
    finally:
        # Cleanup on shutdown
        await db.disconnect()


# Pass lifespan to server
mcp = FastMCP(&quot;My App&quot;, lifespan=app_lifespan)


# Access type-safe lifespan context in tools
@mcp.tool()
def query_db(ctx: Context) -&gt; str:
    &quot;&quot;&quot;Tool that uses initialized resources&quot;&quot;&quot;
    db = ctx.request_context.lifespan_context.db
    return db.query()
```

### Resources

Resources are how you expose data to LLMs. They&#039;re similar to GET endpoints in a REST API - they provide data but shouldn&#039;t perform significant computation or have side effects:

```python
from mcp.server.fastmcp import FastMCP

mcp = FastMCP(&quot;My App&quot;)


@mcp.resource(&quot;config://app&quot;)
def get_config() -&gt; str:
    &quot;&quot;&quot;Static configuration data&quot;&quot;&quot;
    return &quot;App configuration here&quot;


@mcp.resource(&quot;users://{user_id}/profile&quot;)
def get_user_profile(user_id: str) -&gt; str:
    &quot;&quot;&quot;Dynamic user data&quot;&quot;&quot;
    return f&quot;Profile data for user {user_id}&quot;
```

### Tools

Tools let LLMs take actions through your server. Unlike resources, tools are expected to perform computation and have side effects:

```python
import httpx
from mcp.server.fastmcp import FastMCP

mcp = FastMCP(&quot;My App&quot;)


@mcp.tool()
def calculate_bmi(weight_kg: float, height_m: float) -&gt; float:
    &quot;&quot;&quot;Calculate BMI given weight in kg and height in meters&quot;&quot;&quot;
    return weight_kg / (height_m**2)


@mcp.tool()
async def fetch_weather(city: str) -&gt; str:
    &quot;&quot;&quot;Fetch current weather for a city&quot;&quot;&quot;
    async with httpx.AsyncClient() as client:
        response = await client.get(f&quot;https://api.weather.com/{city}&quot;)
        return response.text
```

### Prompts

Prompts are reusable templates that help LLMs interact with your server effectively:

```python
from mcp.server.fastmcp import FastMCP
from mcp.server.fastmcp.prompts import base

mcp = FastMCP(&quot;My App&quot;)


@mcp.prompt()
def review_code(code: str) -&gt; str:
    return f&quot;Please review this code:\n\n{code}&quot;


@mcp.prompt()
def debug_error(error: str) -&gt; list[base.Message]:
    return [
        base.UserMessage(&quot;I&#039;m seeing this error:&quot;),
        base.UserMessage(error),
        base.AssistantMessage(&quot;I&#039;ll help debug that. What have you tried so far?&quot;),
    ]
```

### Images

FastMCP provides an `Image` class that automatically handles image data:

```python
from mcp.server.fastmcp import FastMCP, Image
from PIL import Image as PILImage

mcp = FastMCP(&quot;My App&quot;)


@mcp.tool()
def create_thumbnail(image_path: str) -&gt; Image:
    &quot;&quot;&quot;Create a thumbnail from an image&quot;&quot;&quot;
    img = PILImage.open(image_path)
    img.thumbnail((100, 100))
    return Image(data=img.tobytes(), format=&quot;png&quot;)
```

### Context

The Context object gives your tools and resources access to MCP capabilities:

```python
from mcp.server.fastmcp import FastMCP, Context

mcp = FastMCP(&quot;My App&quot;)


@mcp.tool()
async def long_task(files: list[str], ctx: Context) -&gt; str:
    &quot;&quot;&quot;Process multiple files with progress tracking&quot;&quot;&quot;
    for i, file in enumerate(files):
        ctx.info(f&quot;Processing {file}&quot;)
        await ctx.report_progress(i, len(files))
        data, mime_type = await ctx.read_resource(f&quot;file://{file}&quot;)
    return &quot;Processing complete&quot;
```

### Authentication

Authentication can be used by servers that want to expose tools accessing protected resources.

`mcp.server.auth` implements an OAuth 2.0 server interface, which servers can use by
providing an implementation of the `OAuthServerProvider` protocol.

```
mcp = FastMCP(&quot;My App&quot;,
        auth_server_provider=MyOAuthServerProvider(),
        auth=AuthSettings(
            issuer_url=&quot;https://myapp.com&quot;,
            revocation_options=RevocationOptions(
                enabled=True,
            ),
            client_registration_options=ClientRegistrationOptions(
                enabled=True,
                valid_scopes=[&quot;myscope&quot;, &quot;myotherscope&quot;],
                default_scopes=[&quot;myscope&quot;],
            ),
            required_scopes=[&quot;myscope&quot;],
        ),
)
```

See [OAuthServerProvider](src/mcp/server/auth/provider.py) for more details.

## Running Your Server

### Development Mode

The fastest way to test and debug your server is with the MCP Inspector:

```bash
mcp dev server.py

# Add dependencies
mcp dev server.py --with pandas --with numpy

# Mount local code
mcp dev server.py --with-editable .
```

### Claude Desktop Integration

Once your server is ready, install it in Claude Desktop:

```bash
mcp install server.py

# Custom name
mcp install server.py --name &quot;My Analytics Server&quot;

# Environment variables
mcp install server.py -v API_KEY=abc123 -v DB_URL=postgres://...
mcp install server.py -f .env
```

### Direct Execution

For advanced scenarios like custom deployments:

```python
from mcp.server.fastmcp import FastMCP

mcp = FastMCP(&quot;My App&quot;)

if __name__ == &quot;__main__&quot;:
    mcp.run()
```

Run it with:
```bash
python server.py
# or
mcp run server.py
```

Note that `mcp run` or `mcp dev` only supports server using FastMCP and not the low-level server variant.

### Streamable HTTP Transport

&gt; **Note**: Streamable HTTP transport is superseding SSE transport for production deployments.

```python
from mcp.server.fastmcp import FastMCP

# Stateful server (maintains session state)
mcp = FastMCP(&quot;StatefulServer&quot;)

# Stateless server (no session persistence)
mcp = FastMCP(&quot;StatelessServer&quot;, stateless_http=True)

# Stateless server (no session persistence, no sse stream with supported client)
mcp = FastMCP(&quot;StatelessServer&quot;, stateless_http=True, json_response=True)

# Run server with streamable_http transport
mcp.run(transport=&quot;streamable-http&quot;)
```

You can mount multiple FastMCP servers in a FastAPI application:

```python
# echo.py
from mcp.server.fastmcp import FastMCP

mcp = FastMCP(name=&quot;EchoServer&quot;, stateless_http=True)


@mcp.tool(description=&quot;A simple echo tool&quot;)
def echo(message: str) -&gt; str:
    return f&quot;Echo: {message}&quot;
```

```python
# math.py
from mcp.server.fastmcp import FastMCP

mcp = FastMCP(name=&quot;MathServer&quot;, stateless_http=True)


@mcp.tool(description=&quot;A simple add tool&quot;)
def add_two(n: int) -&gt; int:
    return n + 2
```

```python
# main.py
import contextlib
from fastapi import FastAPI
from mcp.echo import echo
from mcp.math import math


# Create a combined lifespan to manage both session managers
@contextlib.asynccontextmanager
async def lifespan(app: FastAPI):
    async with contextlib.AsyncExitStack() as stack:
        await stack.enter_async_context(echo.mcp.session_manager.run())
        await stack.enter_async_context(math.mcp.session_manager.run())
        yield


app = FastAPI(lifespan=lifespan)
app.mount(&quot;/echo&quot;, echo.mcp.streamable_http_app())
app.mount(&quot;/math&quot;, math.mcp.streamable_http_app())
```

For low level server with Streamable HTTP implementations, see:
- Stateful server: [`examples/servers/simple-streamablehttp/`](examples/servers/simple-streamablehttp/)
- Stateless server: [`examples/servers/simple-streamablehttp-stateless/`](examples/servers/simple-streamablehttp-stateless/)



The streamable HTTP transport supports:
- Stateful and stateless operation modes
- Resumability with event stores
- JSON or SSE response formats  
- Better scalability for multi-node deployments


### Mounting to an Existing ASGI Server

&gt; **Note**: SSE transport is being superseded by [Streamable HTTP transport](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#streamable-http).

By default, SSE servers are mounted at `/sse` and Streamable HTTP servers are mounted at `/mcp`. You can customize these paths using the methods described below.

You can mount the SSE server to an existing ASGI server using the `sse_app` method. This allows you to integrate the SSE server with other ASGI applications.

```python
from starlette.applications import Starlette
from starlette.routing import Mount, Host
from mcp.server.fastmcp import FastMCP


mcp = FastMCP(&quot;My App&quot;)

# Mount the SSE server to the existing ASGI server
app = Starlette(
    routes=[
        Mount(&#039;/&#039;, app=mcp.sse_app()),
    ]
)

# or dynamically mount as host
app.router.routes.append(Host(&#039;mcp.acme.corp&#039;, app=mcp.sse_app()))
```

When mounting multiple MCP servers under different paths, you can configure the mount path in several ways:

```python
from starlette.applications import Starlette
from starlette.routing import Mount
from mcp.server.fastmcp import FastMCP

# Create multiple MCP servers
github_mcp = FastMCP(&quot;GitHub API&quot;)
browser_mcp = FastMCP(&quot;Browser&quot;)
curl_mcp = FastMCP(&quot;Curl&quot;)
search_mcp = FastMCP(&quot;Search&quot;)

# Method 1: Configure mount paths via settings (recommended for persistent configuration)
github_mcp.settings.mount_path = &quot;/github&quot;
browser_mcp.settings.mount_path = &quot;/browser&quot;

# Method 2: Pass mount path directly to sse_app (preferred for ad-hoc mounting)
# This approach doesn&#039;t modify the server&#039;s settings permanently

# Create Starlette app with multiple mounted servers
app = Starlette(
    routes=[
        # Using settings-based configuration
        Mount(&quot;/github&quot;, app=github_mcp.sse_app()),
        Mount(&quot;/browser&quot;, app=browser_mcp.sse_app()),
        # Using direct mount path parameter
        Mount(&quot;/curl&quot;, app=curl_mcp.sse_app(&quot;/curl&quot;)),
        Mount(&quot;/search&quot;, app=search_mcp.sse_app(&quot;/search&quot;)),
    ]
)

# Method 3: For direct execution, you can also pass the mount path to run()
if __name__ == &quot;__main__&quot;:
    search_mcp.run(transport=&quot;sse&quot;, mount_path=&quot;/search&quot;)
```

For more information on mounting applications in Starlette, see the [Starlette documentation](https://www.starlette.io/routing/#submounting-routes).

## Examples

### Echo Server

A simple server demonstrating resources, tools, and prompts:

```python
from mcp.server.fastmcp import FastMCP

mcp = FastMCP(&quot;Echo&quot;)


@mcp.resource(&quot;echo://{message}&quot;)
def echo_resource(message: str) -&gt; str:
    &quot;&quot;&quot;Echo a message as a resource&quot;&quot;&quot;
    return f&quot;Resource echo: {message}&quot;


@mcp.tool()
def echo_tool(message: str) -&gt; str:
    &quot;&quot;&quot;Echo a message as a tool&quot;&quot;&quot;
    return f&quot;Tool echo: {message}&quot;


@mcp.prompt()
def echo_prompt(message: str) -&gt; str:
    &quot;&quot;&quot;Create an echo prompt&quot;&quot;&quot;
    return f&quot;Please process this message: {message}&quot;
```

### SQLite Explorer

A more complex example showing database integration:

```python
import sqlite3

from mcp.server.fastmcp import FastMCP

mcp = FastMCP(&quot;SQLite Explorer&quot;)


@mcp.resource(&quot;schema://main&quot;)
def get_schema() -&gt; str:
    &quot;&quot;&quot;Provide the database schema as a resource&quot;&quot;&quot;
    conn = sqlite3.connect(&quot;database.db&quot;)
    schema = conn.execute(&quot;SELECT sql FROM sqlite_master WHERE type=&#039;table&#039;&quot;).fetchall()
    return &quot;\n&quot;.join(sql[0] for sql in schema if sql[0])


@mcp.tool()
def query_data(sql: str) -&gt; str:
    &quot;&quot;&quot;Execute SQL queries safely&quot;&quot;&quot;
    conn = sqlite3.connect(&quot;database.db&quot;)
    try:
        result = conn.execute(sql).fetchall()
        return &quot;\n&quot;.join(str(row) for row in result)
    except Exception as e:
        return f&quot;Error: {str(e)}&quot;
```

## Advanced Usage

### Low-Level Server

For more control, you can use the low-level server implementation directly. This gives you full access to the protocol and allows you to customize every aspect of your server, including lifecycle management through the lifespan API:

```python
from contextlib import asynccontextmanager
from collections.abc import AsyncIterator

from fake_database import Database  # Replace with your actual DB type

from mcp.server import Server


@asynccontextmanager
async def server_lifespan(server: Server) -&gt; AsyncIterator[dict]:
    &quot;&quot;&quot;Manage server startup and shutdown lifecycle.&quot;&quot;&quot;
    # Initialize resources on startup
    db = await Database.connect()
    try:
        yield {&quot;db&quot;: db}
    finally:
        # Clean up on shutdown
        await db.disconnect()


# Pass lifespan to server
server = Server(&quot;example-server&quot;, lifespan=server_lifespan)


# Access lifespan context in handlers
@server.call_tool()
async def query_db(name: str, arguments: dict) -&gt; list:
    ctx = server.get_context()
    db = ctx.lifespan_context[&quot;db&quot;]
    return await db.query(arguments[&quot;query&quot;])
```

The lifespan API provides:
- A way to initialize resources when the server starts and clean them up when it stops
- Access to initialized resources through the request context in handlers
- Type-safe context passing between lifespan and request handlers

```python
import mcp.server.stdio
import mcp.types as types
from mcp.server.lowlevel import NotificationOptions, Server
from mcp.server.models import InitializationOptions

# Create a server instance
server = Server(&quot;example-server&quot;)


@server.list_prompts()
async def handle_list_prompts() -&gt; list[types.Prompt]:
    return [
        types.Prompt(
            name=&quot;example-prompt&quot;,
            description=&quot;An example prompt template&quot;,
            arguments=[
                types.PromptArgument(
                    name=&quot;arg1&quot;, description=&quot;Example argument&quot;, required=True
                )
            ],
        )
    ]


@server.get_prompt()
async def handle_get_prompt(
    name: str, arguments: dict[str, str] | None
) -&gt; types.GetPromptResult:
    if name != &quot;example-prompt&quot;:
        raise ValueError(f&quot;Unknown prompt: {name}&quot;)

    return types.GetPromptResult(
        description=&quot;Example prompt&quot;,
        messages=[
            types.PromptMessage(
                role=&quot;user&quot;,
                content=types.TextContent(type=&quot;text&quot;, text=&quot;Example prompt text&quot;),
            )
        ],
    )


async def run():
    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
        await server.run(
            read_stream,
            write_stream,
            InitializationOptions(
                server_name=&quot;example&quot;,
                server_version=&quot;0.1.0&quot;,
                capabilities=server.get_capabilities(
                    notification_options=NotificationOptions(),
                    experimental_capabilities={},
                ),
            ),
        )


if __name__ == &quot;__main__&quot;:
    import asyncio

    asyncio.run(run())
```

Caution: The `mcp run` and `mcp dev` tool doesn&#039;t support low-level server.

### Writing MCP Clients

The SDK provides a high-level client interface for connecting to MCP servers using various [transports](https://modelcontextprotocol.io/sp

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mindsdb/mindsdb]]></title>
            <link>https://github.com/mindsdb/mindsdb</link>
            <guid>https://github.com/mindsdb/mindsdb</guid>
            <pubDate>Fri, 23 May 2025 00:04:19 GMT</pubDate>
            <description><![CDATA[AI's query engine - Platform for building AI that can answer questions over large scale federated data. - The only MCP Server you'll ever need]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mindsdb/mindsdb">mindsdb/mindsdb</a></h1>
            <p>AI's query engine - Platform for building AI that can answer questions over large scale federated data. - The only MCP Server you'll ever need</p>
            <p>Language: Python</p>
            <p>Stars: 28,639</p>
            <p>Forks: 5,022</p>
            <p>Stars today: 453 stars today</p>
            <h2>README</h2><pre>

&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
	&lt;a href=&quot;https://pypi.org/project/MindsDB/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/MindsDB.svg&quot; alt=&quot;MindsDB Release&quot;&gt;&lt;/a&gt;
	&lt;a href=&quot;https://www.python.org/downloads/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/python-3.10.x%7C%203.11.x-brightgreen.svg&quot; alt=&quot;Python supported&quot;&gt;&lt;/a&gt;
	&lt;a href=&quot;https://ossrank.com/p/630&quot;&gt;&lt;img src=&quot;https://shields.io/endpoint?url=https://ossrank.com/shield/630&quot;&gt;&lt;/a&gt;
	&lt;img alt=&quot;PyPI - Downloads&quot; src=&quot;https://img.shields.io/pypi/dm/Mindsdb&quot;&gt;
	&lt;a href=&quot;https://hub.docker.com/u/mindsdb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/mindsdb/mindsdb&quot; alt=&quot;Docker pulls&quot;&gt;&lt;/a&gt;

  &lt;br /&gt;
  &lt;br /&gt;

  &lt;a href=&quot;https://github.com/mindsdb/mindsdb&quot;&gt;
    &lt;img src=&quot;/docs/assets/mindsdb_logo.png&quot; alt=&quot;MindsDB&quot; width=&quot;300&quot;&gt;
  &lt;/a&gt;

  &lt;p align=&quot;center&quot;&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://www.mindsdb.com?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Website&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://docs.mindsdb.com?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Docs&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://mdb.ai/register&quot;&gt;Demo&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://mindsdb.com/joincommunity?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Community Slack&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

----------------------------------------


MindsDB is an AI data solution that enables humans, AI, agents, and applications to query data in natural language and SQL, and get highly accurate answers across disparate data sources and types.

![image](https://github.com/user-attachments/assets/03b779e8-7008-485e-989a-e8733cb94e4c)

A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. 

[MindsDB has an MCP server built in](https://docs.mindsdb.com/mcp/overview) that enables your MCP applications to connect, unify and respond to questions over large-scale federated dataâ€”spanning databases, data warehouses, and SaaS applications.

## Minds [Demo](https://mdb.ai/register)
Play with [Minds demo](https://mdb.ai/register), and see the power of MindsDB at answering questions from structured to unstructured data, whether it&#039;s scattered across SaaS applications, databases, or... hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered.
 
## Install MindsDB Server 

MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart&#039;s content.

  * [Using Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop). This is the fastest and recommended way to get started and have it all running.
  * [Using Docker](https://docs.mindsdb.com/setup/self-hosted/docker). This is also simple, but gives you more flexibility on how to further customize your server.
  * [Using PyPI](https://docs.mindsdb.com/contribute/install). This option enables you to contribute to MindsDB.

## Connect Your Data

You can connect to hundreds of [data sources (learn more)](https://docs.mindsdb.com/integrations/data-overview). This is just an example of a Postgres database.

```sql
-- Connect to demo postgres DB
CREATE DATABASE demo_postgres_db
WITH ENGINE = &quot;postgres&quot;,
PARAMETERS = {
  &quot;user&quot;: &quot;demo_user&quot;,
  &quot;password&quot;: &quot;demo_password&quot;,
  &quot;host&quot;: &quot;samples.mindsdb.com&quot;,
  &quot;port&quot;: &quot;5432&quot;,
  &quot;database&quot;: &quot;demo&quot;,
  &quot;schema&quot;: &quot;demo_data&quot;
};
```

Once you&#039;ve connected your data sources, you can [combine](https://docs.mindsdb.com/mindsdb_sql/sql/api/join-on), [slice it, dice it](https://docs.mindsdb.com/mindsdb_sql/sql/api/select), and [transform](https://docs.mindsdb.com/use-cases/data_enrichment/overview) it however your heart desires using good ol&#039; standard SQL [(learn more)](https://docs.mindsdb.com/mindsdb_sql/overview). 

After you&#039;ve whipped your data into shape, it&#039;s time to build AI that actually learns!

## Build AI Knowledge

Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager&#039;s bedroom, our Knowledge Base engine will figure out how to find the relevant information. 

**In this example** we will create a knowledge base that knows everything about amazon reviews. 

```sql
-- first create a knowledge base
CREATE KNOWLEDGE_BASE mindsdb.reviews_kb;

-- now insert everything from the amazon reviews table into it, so it can learn it
INSERT INTO mindsdb.reviews_kb (
  SELECT review as content FROM demo_pg_db.amazon_reviews
);

-- check the status of your loads here
SELECT * FROM information_schema.knowledge_bases;

-- query the content of the knowledge base
SELECT * FROM mindsdb.reviews_kb;
```

For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. [(Learn more about knowledge Bases)](https://docs.mindsdb.com/mindsdb_sql/agents/knowledge-bases)

+ Want to [hand-pick your embedding model? Go for it](https://docs.mindsdb.com/mindsdb_sql/agents/knowledge-bases#knowledge-base-with-openai-embedding-model)! 
+ Have strong [opinions about vector databases? We&#039;re here for it!](https://docs.mindsdb.com/mindsdb_sql/agents/knowledge-bases#knowledge-base-with-custom-vector-store). 

But if you&#039;d rather spend your time on other things (like finally building that billion-dollar AI App), that&#039;s perfectly fine too. By default, it&#039;s all handled automatically - you don&#039;t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc.

## Search 

Now that your knowledge base is loaded and ready. Let&#039;s hunt for some juicy info!

#### Via SQL

```sql
-- Find the reviews that about Iphone in beast of lights
SELECT *  FROM mindsdb.reviews_kb
WHERE content LIKE &#039;what are the best kindle reviews&#039;
LIMIT 10;
```

#### Via Python SDK

Install MindsDB SDK

```shell
pip install mindsdb_sdk
```

You can call this AI knowledge base from your app with the following code:

```python
import mindsdb_sdk


# connects to the specified host and port
server = mindsdb_sdk.connect(&#039;http://127.0.0.1:47334&#039;)

my_kb = server.knowledge_bases.get(&#039;mindsdb.reviews_kb&#039;);
df = my_kb.find(&#039;what are the best kindle reviews&#039;).fetch()

```

## ğŸ¤ Contribute

Interested in contributing to MindsDB? Follow our [installation guide for development](https://docs.mindsdb.com/contribute/install?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

You can find our [contribution guide here](https://docs.mindsdb.com/contribute/contribute?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

We welcome suggestions! Feel free to open new issues with your ideas, and weâ€™ll guide you.

This project adheres to a [Contributor Code of Conduct](https://github.com/mindsdb/mindsdb/blob/main/CODE_OF_CONDUCT.md). By participating, you agree to follow its terms.

Also, check out our [community rewards and programs](https://mindsdb.com/community?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

## ğŸ¤ Support

If you find a bug, please submit an [issue on GitHub](https://github.com/mindsdb/mindsdb/issues/new/choose).

Hereâ€™s how you can get community support:

* Ask a question in our [Slack Community](https://mindsdb.com/joincommunity?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).
* Join our [GitHub Discussions](https://github.com/mindsdb/mindsdb/discussions).
* Post on [Stack Overflow](https://stackoverflow.com/questions/tagged/mindsdb) with the MindsDB tag.

For commercial support, please [contact the MindsDB team](https://mindsdb.com/contact?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

## ğŸ’š Current Contributors

&lt;a href=&quot;https://github.com/mindsdb/mindsdb/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contributors-img.web.app/image?repo=mindsdb/mindsdb&quot; /&gt;
&lt;/a&gt;

Generated with [contributors-img](https://contributors-img.web.app).

## ğŸ”” Subscribe for Updates

Join our [Slack community](https://mindsdb.com/joincommunity)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/RD-Agent]]></title>
            <link>https://github.com/microsoft/RD-Agent</link>
            <guid>https://github.com/microsoft/RD-Agent</guid>
            <pubDate>Fri, 23 May 2025 00:04:18 GMT</pubDate>
            <description><![CDATA[Research and development (R&D) is crucial for the enhancement of industrial productivity, especially in the AI era, where the core aspects of R&D are mainly focused on data and models. We are committed to automating these high-value generic R&D processes through our open source R&D automation tool R&D-Agent, which lets AI drive data-driven AI.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/RD-Agent">microsoft/RD-Agent</a></h1>
            <p>Research and development (R&D) is crucial for the enhancement of industrial productivity, especially in the AI era, where the core aspects of R&D are mainly focused on data and models. We are committed to automating these high-value generic R&D processes through our open source R&D automation tool R&D-Agent, which lets AI drive data-driven AI.</p>
            <p>Language: Python</p>
            <p>Stars: 4,480</p>
            <p>Forks: 400</p>
            <p>Stars today: 63 stars today</p>
            <h2>README</h2><pre>&lt;h4 align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/_static/logo.png&quot; alt=&quot;RA-Agent logo&quot; style=&quot;width:70%; &quot;&gt;
  
  &lt;a href=&quot;https://rdagent.azurewebsites.net&quot; target=&quot;_blank&quot;&gt;ğŸ–¥ï¸ Live Demo&lt;/a&gt; |
  &lt;a href=&quot;https://rdagent.azurewebsites.net/factor_loop&quot; target=&quot;_blank&quot;&gt;ğŸ¥ Demo Video&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=JJ4JYO3HscM&amp;list=PLALmKB0_N3_i52fhUmPQiL4jsO354uopR&quot; target=&quot;_blank&quot;&gt;â–¶ï¸YouTube&lt;/a&gt;   |
  &lt;a href=&quot;https://rdagent.readthedocs.io/en/latest/index.html&quot; target=&quot;_blank&quot;&gt;ğŸ“– Documentation&lt;/a&gt; |
  &lt;a href=&quot;https://aka.ms/RD-Agent-Tech-Report&quot; target=&quot;_blank&quot;&gt;ğŸ“„ Tech Report&lt;/a&gt; |
  &lt;a href=&quot;#-paperwork-list&quot;&gt; ğŸ“ƒ Papers &lt;/a&gt;
&lt;/h3&gt;


[![CI](https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml)
[![CodeQL](https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql)
[![Dependabot Updates](https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates)
[![Lint PR Title](https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml)
[![Release.yml](https://github.com/microsoft/RD-Agent/actions/workflows/release.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/release.yml)
[![Platform](https://img.shields.io/badge/platform-Linux-blue)](https://pypi.org/project/rdagent/#files)
[![PyPI](https://img.shields.io/pypi/v/rdagent)](https://pypi.org/project/rdagent/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/rdagent)](https://pypi.org/project/rdagent/)
[![Release](https://img.shields.io/github/v/release/microsoft/RD-Agent)](https://github.com/microsoft/RD-Agent/releases)
[![GitHub](https://img.shields.io/github/license/microsoft/RD-Agent)](https://github.com/microsoft/RD-Agent/blob/main/LICENSE)
[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit)](https://github.com/pre-commit/pre-commit)
[![Checked with mypy](https://www.mypy-lang.org/static/mypy_badge.svg)](http://mypy-lang.org/)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
[![Chat](https://img.shields.io/badge/chat-discord-blue)](https://discord.gg/ybQ97B6Jjy)
[![Documentation Status](https://readthedocs.org/projects/rdagent/badge/?version=latest)](https://rdagent.readthedocs.io/en/latest/?badge=latest)
[![Readthedocs Preview](https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml) &lt;!-- this badge is too long, please place it in the last one to make it pretty --&gt; 
[![arXiv](https://img.shields.io/badge/arXiv-2505.14738-00ff00.svg)](https://arxiv.org/abs/2505.14738)



# ğŸ† The Best Machine Learning Engineering Agent!

[MLE-bench](https://github.com/openai/mle-bench) is a comprehensive benchmark evaluating the performance of AI agents on machine learning engineering tasks. Utilizing datasets from 75 Kaggle competitions, MLE-bench provides robust assessments of AI systems&#039; capabilities in real-world ML engineering scenarios.

R&amp;D-Agent currently leads as the top-performing machine learning engineering agent on MLE-bench:

| Agent | Low == Lite (%) | Medium (%) | High (%) | All (%) |
|---------|--------|-----------|---------|----------|
| R&amp;D-Agent o1-preview | 48.18 Â± 2.49 | 8.95 Â± 2.36 | 18.67 Â± 2.98 | 22.4 Â± 1.1 |
| R&amp;D-Agent o3(R)+GPT-4.1(D) | 51.52 Â± 6.21 | 7.89 Â± 3.33 | 16.67 Â± 3.65 | 22.45 Â± 2.45 |
| AIDE o1-preview | 34.3 Â± 2.4 | 8.8 Â± 1.1 | 10.0 Â± 1.9 | 16.9 Â± 1.1 |

**Notes:**
- **O3(R)+GPT-4.1(D)**: This version is designed to both reduce average time per loop and leverage a cost-effective combination of backend LLMs by seamlessly integrating Research Agent (o3) with Development Agent (GPT-4.1).
- **AIDE o1-preview**: Represents the previously best public result on MLE-bench as reported in the original MLE-bench paper.
- Average and standard deviation results for R&amp;D-Agent o1-preview is based on a independent of 5 seeds and for R&amp;D-Agent o3(R)+GPT-4.1(D) is based on 6 seeds.
- According to MLE-Bench, the 75 competitions are categorized into three levels of complexity: **Low==Lite** if we estimate that an experienced ML engineer can produce a sensible solution in under 2 hours, excluding the time taken to train any models; **Medium** if it takes between 2 and 10 hours; and **High** if it takes more than 10 hours.

You can inspect the detailed runs of the above results online.
- [R&amp;D-Agent o1-preview detailed runs](https://aka.ms/RD-Agent_MLE-Bench_O1-preview)
- [R&amp;D-Agent o3(R)+GPT-4.1(D) detailed runs](https://aka.ms/RD-Agent_MLE-Bench_O3_GPT41)


# ğŸ“° News
| ğŸ—ï¸ News        | ğŸ“ Description                 |
| --            | ------      |
| [Technical Report Release](#overall-technical-report) | Overall framework description and results on MLE-bench | 
| [R&amp;D-Agent-Quant Release](#deep-application-in-diverse-scenarios) | Apply R&amp;D-Agent to quant trading | 
| MLE-Bench Results Released | R&amp;D-Agent currently leads as the [top-performing machine learning engineering agent](#-the-best-machine-learning-engineering-agent) on MLE-bench |
| Support LiteLLM Backend | We now fully support **[LiteLLM](https://github.com/BerriAI/litellm)** as a backend for integration with multiple LLM providers. |
| More General Data Science Agent | ğŸš€Coming soon! |
| Kaggle Scenario release | We release **[Kaggle Agent](https://rdagent.readthedocs.io/en/latest/scens/kaggle_agent.html)**, try the new features!                  |
| Official WeChat group release  | We created a WeChat group, welcome to join! (ğŸ—ª[QR Code](https://github.com/microsoft/RD-Agent/issues/880)) |
| Official Discord release  | We launch our first chatting channel in Discord (ğŸ—ª[![Chat](https://img.shields.io/badge/chat-discord-blue)](https://discord.gg/ybQ97B6Jjy)) |
| First release | **R&amp;D-Agent** is released on GitHub |



# Data Science Agent Preview
Check out our demo video showcasing the current progress of our Data Science Agent under development:

https://github.com/user-attachments/assets/3eccbecb-34a4-4c81-bce4-d3f8862f7305

# ğŸŒŸ Introduction
&lt;div align=&quot;center&quot;&gt;
      &lt;img src=&quot;docs/_static/scen.png&quot; alt=&quot;Our focused scenario&quot; style=&quot;width:80%; &quot;&gt;
&lt;/div&gt;

R&amp;D-Agent aims to automate the most critical and valuable aspects of the industrial R&amp;D process, and we begin with focusing on the data-driven scenarios to streamline the development of models and data. 
Methodologically, we have identified a framework with two key components: &#039;R&#039; for proposing new ideas and &#039;D&#039; for implementing them.
We believe that the automatic evolution of R&amp;D will lead to solutions of significant industrial value.


&lt;!-- Tag Cloud --&gt;
R&amp;D is a very general scenario. The advent of R&amp;D-Agent can be your
- ğŸ’° **Automatic Quant Factory** ([ğŸ¥Demo Video](https://rdagent.azurewebsites.net/factor_loop)|[â–¶ï¸YouTube](https://www.youtube.com/watch?v=X4DK2QZKaKY&amp;t=6s))
- ğŸ¤– **Data Mining Agent:** Iteratively proposing data &amp; models ([ğŸ¥Demo Video 1](https://rdagent.azurewebsites.net/model_loop)|[â–¶ï¸YouTube](https://www.youtube.com/watch?v=dm0dWL49Bc0&amp;t=104s)) ([ğŸ¥Demo Video 2](https://rdagent.azurewebsites.net/dmm)|[â–¶ï¸YouTube](https://www.youtube.com/watch?v=VIaSTZuoZg4))  and implementing them by gaining knowledge from data.
- ğŸ¦¾ **Research Copilot:** Auto read research papers ([ğŸ¥Demo Video](https://rdagent.azurewebsites.net/report_model)|[â–¶ï¸YouTube](https://www.youtube.com/watch?v=BiA2SfdKQ7o)) / financial reports ([ğŸ¥Demo Video](https://rdagent.azurewebsites.net/report_factor)|[â–¶ï¸YouTube](https://www.youtube.com/watch?v=ECLTXVcSx-c)) and implement model structures or building datasets.
- ğŸ¤– **Kaggle Agent:** Auto Model Tuning and Feature Engineering([ğŸ¥Demo Video Coming Soon...]()) and implementing them to achieve more in competitions.
- ...

You can click the links above to view the demo. We&#039;re continuously adding more methods and scenarios to the project to enhance your R&amp;D processes and boost productivity. 

Additionally, you can take a closer look at the examples in our **[ğŸ–¥ï¸ Live Demo](https://rdagent.azurewebsites.net/)**.

&lt;div align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://rdagent.azurewebsites.net/&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;docs/_static/demo.png&quot; alt=&quot;Watch the demo&quot; width=&quot;80%&quot;&gt;
    &lt;/a&gt;
&lt;/div&gt;


# âš¡ Quick start

You can try above demos by running the following command:

### ğŸ³ Docker installation.
Users must ensure Docker is installed before attempting most scenarios. Please refer to the [official ğŸ³Docker page](https://docs.docker.com/engine/install/) for installation instructions.
Ensure the current user can run Docker commands **without using sudo**. You can verify this by executing `docker run hello-world`.

### ğŸ Create a Conda Environment
- Create a new conda environment with Python (3.10 and 3.11 are well-tested in our CI):
  ```sh
  conda create -n rdagent python=3.10
  ```
- Activate the environment:
  ```sh
  conda activate rdagent
  ```

### ğŸ› ï¸ Install the R&amp;D-Agent
- You can directly install the R&amp;D-Agent package from PyPI:
  ```sh
  pip install rdagent
  ```

### ğŸ’Š Health check
- rdagent provides a health check that currently checks two things.
  - whether the docker installation was successful.
  - whether the default port used by the [rdagent ui](https://github.com/microsoft/RD-Agent?tab=readme-ov-file#%EF%B8%8F-monitor-the-application-results) is occupied.
  ```sh
  rdagent health_check
  ```


### âš™ï¸ Configuration
- The demos requires following ability:
  - ChatCompletion
  - json_mode
  - embedding query

- For example: If you are using the `OpenAI API`, you have to configure your GPT model in the `.env` file like this.
  ```bash
  cat &lt;&lt; EOF  &gt; .env
  OPENAI_API_KEY=&lt;replace_with_your_openai_api_key&gt;
  # EMBEDDING_MODEL=text-embedding-3-small
  CHAT_MODEL=gpt-4-turbo
  EOF
  ```
- However, not every API services support these features by default. For example: `AZURE OpenAI`, you have to configure your GPT model in the `.env` file like this.
  ```bash
  cat &lt;&lt; EOF  &gt; .env
  USE_AZURE=True
  EMBEDDING_OPENAI_API_KEY=&lt;replace_with_your_azure_openai_api_key&gt;
  EMBEDDING_AZURE_API_BASE=&lt;replace_with_your_azure_endpoint&gt;
  EMBEDDING_AZURE_API_VERSION=&lt;replace_with_the_version_of_your_azure_openai_api&gt;
  EMBEDDING_MODEL=text-embedding-3-small
  CHAT_OPENAI_API_KEY=&lt;replace_with_your_azure_openai_api_key&gt;
  CHAT_AZURE_API_BASE=&lt;replace_with_your_azure_endpoint&gt;
  CHAT_AZURE_API_VERSION=&lt;replace_with_the_version_of_your_azure_openai_api&gt;
  CHAT_MODEL=&lt;replace_it_with_the_name_of_your_azure_chat_model&gt;
  EOF
  ```

- We now support LiteLLM as a backend for integration with multiple LLM providers. If you use LiteLLM Backend to use models, you can configure as follows:
  ```bash
  cat &lt;&lt; EOF  &gt; .env
  BACKEND=rdagent.oai.backend.LiteLLMAPIBackend
  # It can be modified to any model supported by LiteLLM.
  CHAT_MODEL=gpt-4o
  EMBEDDING_MODEL=text-embedding-3-small
  # The backend api_key fully follow the convention of litellm.
  OPENAI_API_KEY=&lt;replace_with_your_openai_api_key&gt;
  ```
  
- For more configuration information, please refer to the [documentation](https://rdagent.readthedocs.io/en/latest/installation_and_configuration.html).

### ğŸš€ Run the Application

The **[ğŸ–¥ï¸ Live Demo](https://rdagent.azurewebsites.net/)** is implemented by the following commands(each item represents one demo, you can select the one you prefer):

- Run the **Automated Quantitative Trading &amp; Iterative Factors Evolution**:  [Qlib](http://github.com/microsoft/qlib) self-loop factor proposal and implementation application
  ```sh
  rdagent fin_factor
  ```

- Run the **Automated Quantitative Trading &amp; Iterative Model Evolution**: [Qlib](http://github.com/microsoft/qlib) self-loop model proposal and implementation application
  ```sh
  rdagent fin_model
  ```

- Run the **Automated Medical Prediction Model Evolution**: Medical self-loop model proposal and implementation application
  &gt;(1) Apply for an account at [PhysioNet](https://physionet.org/). &lt;br /&gt; (2) Request access to FIDDLE preprocessed data: [FIDDLE Dataset](https://physionet.org/content/mimic-eicu-fiddle-feature/1.0.0/). &lt;br /&gt;
  (3) Place your username and password in `.env`.
  ```bash
  cat &lt;&lt; EOF  &gt;&gt; .env
  DM_USERNAME=&lt;your_username&gt;
  DM_PASSWORD=&lt;your_password&gt;
  EOF
  ```
  ```sh
  rdagent med_model
  ```

- Run the **Automated Quantitative Trading &amp; Factors Extraction from Financial Reports**:  Run the [Qlib](http://github.com/microsoft/qlib) factor extraction and implementation application based on financial reports
  ```sh
  # 1. Generally, you can run this scenario using the following command:
  rdagent fin_factor_report --report_folder=&lt;Your financial reports folder path&gt;

  # 2. Specifically, you need to prepare some financial reports first. You can follow this concrete example:
  wget https://github.com/SunsetWolf/rdagent_resource/releases/download/reports/all_reports.zip
  unzip all_reports.zip -d git_ignore_folder/reports
  rdagent fin_factor_report --report_folder=git_ignore_folder/reports
  ```

- Run the **Automated Model Research &amp; Development Copilot**: model extraction and implementation application
  ```sh
  # 1. Generally, you can run your own papers/reports with the following command:
  rdagent general_model &lt;Your paper URL&gt;

  # 2. Specifically, you can do it like this. For more details and additional paper examples, use `rdagent general_model -h`:
  rdagent general_model  &quot;https://arxiv.org/pdf/2210.09789&quot;
  ```

- Run the **Automated Kaggle Model Tuning &amp; Feature Engineering**:  self-loop model proposal and feature engineering implementation application &lt;br /&gt;
  &gt; Using **sf-crime** *(San Francisco Crime Classification)* as an example. &lt;br /&gt;
  &gt; 1. Register and login on the [Kaggle](https://www.kaggle.com/) website. &lt;br /&gt;
  &gt; 2. Configuring the Kaggle API. &lt;br /&gt;
  &gt; (1) Click on the avatar (usually in the top right corner of the page) -&gt; `Settings` -&gt; `Create New Token`, A file called `kaggle.json` will be downloaded. &lt;br /&gt;
  &gt; (2) Move `kaggle.json` to `~/.config/kaggle/` &lt;br /&gt;
  &gt; (3) Modify the permissions of the kaggle.json file. Reference command: `chmod 600 ~/.config/kaggle/kaggle.json` &lt;br /&gt;
  &gt; 3. Join the competition: Click `Join the competition` -&gt; `I Understand and Accept` at the bottom of the [competition details page](https://www.kaggle.com/competitions/sf-crime/data).
  ```bash
  # Generally, you can run the Kaggle competition program with the following command:
  rdagent kaggle --competition &lt;your competition name&gt;

  # Specifically, you need to create a folder for storing competition files (e.g., competition description file, competition datasets, etc.), and configure the path to the folder in your environment. In addition, you need to use chromedriver when you download the competition descriptors, which you can follow for this specific example:
  
  # 1. Install chromedriver.

  # 2. Add the competition description file path to the `.env` file.
  mkdir -p ./git_ignore_folder/kaggle_data
  dotenv set KG_LOCAL_DATA_PATH &quot;$(pwd)/git_ignore_folder/kaggle_data&quot;

  # 3. run the application
  rdagent kaggle --competition sf-crime
  ```
  &gt; **Description of the above example:** &lt;br /&gt;
  &gt; - Kaggle competition data is roughly divided into three sections: competition description file (json file) and complete dataset for the competition and simplified dataset for the competition. &lt;br /&gt;
  &gt; - The Kaggle competition data will be downloaded automatically, the download process depends on `chromedriver`, installation instructions can be found in the [documentation](https://rdagent.readthedocs.io/en/latest/scens/kaggle_agent.html#example-guide). &lt;br /&gt;

### ğŸ–¥ï¸ Monitor the Application Results
- You can run the following command for our demo program to see the run logs.

  ```sh
  rdagent ui --port 19899 --log_dir &lt;your log folder like &quot;log/&quot;&gt;
  ```

  **Note:** Although port 19899 is not commonly used, but before you run this demo, you need to check if port 19899 is occupied. If it is, please change it to another port that is not occupied.

  You can check if a port is occupied by running the following command.

  ```sh
  rdagent health_check
  ```

# ğŸ­ Scenarios

We have applied R&amp;D-Agent to multiple valuable data-driven industrial scenarios.


## ğŸ¯ Goal: Agent for Data-driven R&amp;D

In this project, we are aiming to build an Agent to automate Data-Driven R\&amp;D that can
+ ğŸ“„ Read real-world material (reports, papers, etc.) and **extract** key formulas, descriptions of interested **features** and **models**, which are the key components of data-driven R&amp;D .
+ ğŸ› ï¸ **Implement** the extracted formulas (e.g., features, factors, and models) in runnable codes.
   + Due to the limited ability of LLM in implementing at once, build an evolving process for the agent to improve performance by learning from feedback and knowledge.
+ ğŸ’¡ Propose **new ideas** based on current knowledge and observations.

&lt;!-- ![Data-Centric R&amp;D Overview](docs/_static/overview.png) --&gt;

## ğŸ“ˆ Scenarios/Demos

In the two key areas of data-driven scenarios, model implementation and data building, our system aims to serve two main roles: ğŸ¦¾Copilot and ğŸ¤–Agent. 
- The ğŸ¦¾Copilot follows human instructions to automate repetitive tasks. 
- The ğŸ¤–Agent, being more autonomous, actively proposes ideas for better results in the future.

The supported scenarios are listed below:

| Scenario/Target | Model Implementation                   | Data Building                                                                      |
| --              | --                                     | --                                                                                 |
| **ğŸ’¹ Finance**      | ğŸ¤– [Iteratively Proposing Ideas &amp; Evolving](https://rdagent.azurewebsites.net/model_loop)[â–¶ï¸YouTube](https://www.youtube.com/watch?v=dm0dWL49Bc0&amp;t=104s) |  ğŸ¤– [Iteratively Proposing Ideas &amp; Evolving](https://rdagent.azurewebsites.net/factor_loop) [â–¶ï¸YouTube](https://www.youtube.com/watch?v=X4DK2QZKaKY&amp;t=6s) &lt;br/&gt;   ğŸ¦¾ [Auto reports reading &amp; implementation](https://rdagent.azurewebsites.net/report_factor)[â–¶ï¸YouTube](https://www.youtube.com/watch?v=ECLTXVcSx-c)  |
| **ğŸ©º Medical**      | ğŸ¤– [Iteratively Proposing Ideas &amp; Evolving](https://rdagent.azurewebsites.net/dmm)[â–¶ï¸YouTube](https://www.youtube.com/watch?v=VIaSTZuoZg4) | -                                                                                  |
| **ğŸ­ General**      | ğŸ¦¾ [Auto paper reading &amp; implementation](https://rdagent.azurewebsites.net/report_model)[â–¶ï¸YouTube](https://www.youtube.com/watch?v=BiA2SfdKQ7o) &lt;br/&gt; ğŸ¤– Auto Kaggle Model Tuning   | ğŸ¤–Auto Kaggle feature Engineering |

- **[RoadMap](https://rdagent.readthedocs.io/en/latest/scens/kaggle_agent.html#roadmap)**: Currently, we are working hard to add new features to the Kaggle scenario.

Different scenarios vary in entrance and configuration. Please check the detailed setup tutorial in the scenarios documents.

Here is a gallery of [successful explorations](https://github.com/SunsetWolf/rdagent_resource/releases/download/demo_traces/demo_traces.zip) (5 traces showed in **[ğŸ–¥ï¸ Live Demo](https://rdagent.azurewebsites.net/)**). You can download and view the execution trace using [this command](https://github.com/microsoft/RD-Agent?tab=readme-ov-file#%EF%B8%8F-monitor-the-application-results) from the documentation.

Please refer to **[ğŸ“–readthedocs_scen](https://rdagent.readthedocs.io/en/latest/scens/catalog.html)** for more details of the scenarios.

# âš™ï¸ Framework

&lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;docs/_static/Framework-RDAgent.png&quot; alt=&quot;Framework-RDAgent&quot; width=&quot;85%&quot;&gt;
&lt;/div&gt;


... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hpcaitech/Open-Sora]]></title>
            <link>https://github.com/hpcaitech/Open-Sora</link>
            <guid>https://github.com/hpcaitech/Open-Sora</guid>
            <pubDate>Fri, 23 May 2025 00:04:17 GMT</pubDate>
            <description><![CDATA[Open-Sora: Democratizing Efficient Video Production for All]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hpcaitech/Open-Sora">hpcaitech/Open-Sora</a></h1>
            <p>Open-Sora: Democratizing Efficient Video Production for All</p>
            <p>Language: Python</p>
            <p>Stars: 26,493</p>
            <p>Forks: 2,562</p>
            <p>Stars today: 20 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/readme/icon.png&quot; width=&quot;250&quot;/&gt;
&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://github.com/hpcaitech/Open-Sora/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/hpcaitech/Open-Sora?style=social&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2503.09642v1&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Tech Report 2.0&amp;message=Arxiv&amp;color=red&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2412.20404&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Tech Report 1.2&amp;message=Arxiv&amp;color=red&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://hpcaitech.github.io/Open-Sora/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Gallery-View-orange?logo=&amp;amp&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://discord.gg/kZakZzrSUT&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-join-blueviolet?logo=discord&amp;amp&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://join.slack.com/t/colossalaiworkspace/shared_invite/zt-247ipg9fk-KRRYmUl~u2ll2637WRURVA&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Slack-ColossalAI-blueviolet?logo=slack&amp;amp&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://x.com/YangYou1991/status/1899973689460044010&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Twitter-Discuss-blue?logo=twitter&amp;amp&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/WeChat.png&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/å¾®ä¿¡-å°åŠ©æ‰‹åŠ ç¾¤-green?logo=wechat&amp;amp&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

## Open-Sora: Democratizing Efficient Video Production for All

We design and implement **Open-Sora**, an initiative dedicated to **efficiently** producing high-quality video. We hope to make the model,
tools and all details accessible to all. By embracing **open-source** principles,
Open-Sora not only democratizes access to advanced video generation techniques, but also offers a
streamlined and user-friendly platform that simplifies the complexities of video generation.
With Open-Sora, our goal is to foster innovation, creativity, and inclusivity within the field of content creation.

ğŸ¬ For a professional AI video-generation product, try [Video Ocean](https://video-ocean.com/) â€” powered by a superior model.
&lt;div align=&quot;center&quot;&gt;
   &lt;a href=&quot;https://video-ocean.com/&quot;&gt;
   &lt;img src=&quot;https://github.com/hpcaitech/public_assets/blob/main/colossalai/img/3.gif&quot; width=&quot;850&quot; /&gt;
   &lt;/a&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
   &lt;a href=&quot;https://hpc-ai.com/?utm_source=github&amp;utm_medium=social&amp;utm_campaign=promotion-opensora&quot;&gt;
   &lt;img src=&quot;https://github.com/hpcaitech/public_assets/blob/main/colossalai/img/1.gif&quot; width=&quot;850&quot; /&gt;
   &lt;/a&gt;
&lt;/div&gt;

&lt;!-- [[ä¸­æ–‡æ–‡æ¡£](/docs/zh_CN/README.md)] [[æ½æ™¨äº‘](https://cloud.luchentech.com/)|[OpenSoraé•œåƒ](https://cloud.luchentech.com/doc/docs/image/open-sora/)|[è§†é¢‘æ•™ç¨‹](https://www.bilibili.com/video/BV1ow4m1e7PX/?vd_source=c6b752764cd36ff0e535a768e35d98d2)] --&gt;

## ğŸ“° News

- **[2025.03.12]** ğŸ”¥ We released **Open-Sora 2.0** (11B). ğŸ¬ 11B model achieves [on-par performance](#evaluation) with 11B HunyuanVideo &amp; 30B Step-Video on ğŸ“VBench &amp; ğŸ“ŠHuman Preference. ğŸ› ï¸ Fully open-source: checkpoints and training codes for training with only **$200K**. [[report]](https://arxiv.org/abs/2503.09642v1)
- **[2025.02.20]** ğŸ”¥ We released **Open-Sora 1.3** (1B). With the upgraded VAE and Transformer architecture, the quality of our generated videos has been greatly improved ğŸš€. [[checkpoints]](#open-sora-13-model-weights) [[report]](/docs/report_04.md) [[demo]](https://huggingface.co/spaces/hpcai-tech/open-sora)
- **[2024.12.23]** The development cost of video generation models has saved by 50%! Open-source solutions are now available with H200 GPU vouchers. [[blog]](https://company.hpc-ai.com/blog/the-development-cost-of-video-generation-models-has-saved-by-50-open-source-solutions-are-now-available-with-h200-gpu-vouchers) [[code]](https://github.com/hpcaitech/Open-Sora/blob/main/scripts/train.py) [[vouchers]](https://colossalai.org/zh-Hans/docs/get_started/bonus/)
- **[2024.06.17]** We released **Open-Sora 1.2**, which includes **3D-VAE**, **rectified flow**, and **score condition**. The video quality is greatly improved. [[checkpoints]](#open-sora-12-model-weights) [[report]](/docs/report_03.md) [[arxiv]](https://arxiv.org/abs/2412.20404)
- **[2024.04.25]** ğŸ¤— We released the [Gradio demo for Open-Sora](https://huggingface.co/spaces/hpcai-tech/open-sora) on Hugging Face Spaces.
- **[2024.04.25]** We released **Open-Sora 1.1**, which supports **2s~15s, 144p to 720p, any aspect ratio** text-to-image, **text-to-video, image-to-video, video-to-video, infinite time** generation. In addition, a full video processing pipeline is released. [[checkpoints]](#open-sora-11-model-weights) [[report]](/docs/report_02.md)
- **[2024.03.18]** We released **Open-Sora 1.0**, a fully open-source project for video generation.
  Open-Sora 1.0 supports a full pipeline of video data preprocessing, training with
  &lt;a href=&quot;https://github.com/hpcaitech/ColossalAI&quot;&gt;&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/readme/colossal_ai.png&quot; width=&quot;8%&quot; &gt;&lt;/a&gt;
  acceleration,
  inference, and more. Our model can produce 2s 512x512 videos with only 3 days training. [[checkpoints]](#open-sora-10-model-weights)
  [[blog]](https://hpc-ai.com/blog/open-sora-v1.0) [[report]](/docs/report_01.md)
- **[2024.03.04]** Open-Sora provides training with 46% cost reduction.
  [[blog]](https://hpc-ai.com/blog/open-sora)

ğŸ“ Since Open-Sora is under active development, we remain different branches for different versions. The latest version is [main](https://github.com/hpcaitech/Open-Sora). Old versions include: [v1.0](https://github.com/hpcaitech/Open-Sora/tree/opensora/v1.0), [v1.1](https://github.com/hpcaitech/Open-Sora/tree/opensora/v1.1), [v1.2](https://github.com/hpcaitech/Open-Sora/tree/opensora/v1.2), [v1.3](https://github.com/hpcaitech/Open-Sora/tree/opensora/v1.3).

## ğŸ¥ Latest Demo

Demos are presented in compressed GIF format for convenience. For original quality samples and their corresponding prompts, please visit our [Gallery](https://hpcaitech.github.io/Open-Sora/).

| **5s 1024Ã—576**                                                                                                                                    | **5s 576Ã—1024**                                                                                                                                    | **5s 576Ã—1024**                                                                                                                                   |
| -------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------- |
| [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v2.0/ft_0001_1_1.gif&quot; width=&quot;&quot;&gt;](https://streamable.com/e/8g9y9h?autoplay=1) | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v2.0/movie_0160.gif&quot; width=&quot;&quot;&gt;](https://streamable.com/e/k50mnv?autoplay=1)  | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v2.0/movie_0017.gif&quot; width=&quot;&quot;&gt;](https://streamable.com/e/bzrn9n?autoplay=1) |
| [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v2.0/ft_0012_1_1.gif&quot; width=&quot;&quot;&gt;](https://streamable.com/e/dsv8da?autoplay=1) | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v2.0/douyin_0005.gif&quot; width=&quot;&quot;&gt;](https://streamable.com/e/3wif07?autoplay=1) | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v2.0/movie_0037.gif&quot; width=&quot;&quot;&gt;](https://streamable.com/e/us2w7h?autoplay=1) |
| [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v2.0/ft_0055_1_1.gif&quot; width=&quot;&quot;&gt;](https://streamable.com/e/yfwk8i?autoplay=1) | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v2.0/sora_0019.gif&quot; width=&quot;&quot;&gt;](https://streamable.com/e/jgjil0?autoplay=1)   | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v2.0/movie_0463.gif&quot; width=&quot;&quot;&gt;](https://streamable.com/e/lsoai1?autoplay=1) |

&lt;details&gt;
&lt;summary&gt;OpenSora 1.3 Demo&lt;/summary&gt;

| **5s 720Ã—1280**                                                                                                                                                        | **5s 720Ã—1280**                                                                                                                                                           | **5s 720Ã—1280**                                                                                                                                                              |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.3/demo_tomato.gif&quot; width=&quot;&quot;&gt;](https://streamable.com/e/r0imrp?quality=highest&amp;amp;autoplay=1) | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.3/demo_fisherman.gif&quot; width=&quot;&quot;&gt;](https://streamable.com/e/hfvjkh?quality=highest&amp;amp;autoplay=1) | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.3/demo_girl2.gif&quot; width=&quot;&quot;&gt;](https://streamable.com/e/kutmma?quality=highest&amp;amp;autoplay=1)        |
| [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.3/demo_grape.gif&quot; width=&quot;&quot;&gt;](https://streamable.com/e/osn1la?quality=highest&amp;amp;autoplay=1)  | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.3/demo_mushroom.gif&quot; width=&quot;&quot;&gt;](https://streamable.com/e/l1pzws?quality=highest&amp;amp;autoplay=1)  | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.3/demo_parrot.gif&quot; width=&quot;&quot;&gt;](https://streamable.com/e/2vqari?quality=highest&amp;amp;autoplay=1)       |
| [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.3/demo_trans.gif&quot; width=&quot;&quot;&gt;](https://streamable.com/e/1in7d6?quality=highest&amp;amp;autoplay=1)  | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.3/demo_bear.gif&quot; width=&quot;&quot;&gt;](https://streamable.com/e/e9bi4o?quality=highest&amp;amp;autoplay=1)      | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.3/demo_futureflower.gif&quot; width=&quot;&quot;&gt;](https://streamable.com/e/09z7xi?quality=highest&amp;amp;autoplay=1) |
| [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.3/demo_fire.gif&quot; width=&quot;&quot;&gt;](https://streamable.com/e/16c3hk?quality=highest&amp;amp;autoplay=1)   | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.3/demo_man.gif&quot; width=&quot;&quot;&gt;](https://streamable.com/e/wi250w?quality=highest&amp;amp;autoplay=1)       | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.3/demo_black.gif&quot; width=&quot;&quot;&gt;](https://streamable.com/e/vw5b64?quality=highest&amp;amp;autoplay=1)        |

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;OpenSora 1.2 Demo&lt;/summary&gt;

| **4s 720Ã—1280**                                                                                                                                                                                     | **4s 720Ã—1280**                                                                                                                                                                                     | **4s 720Ã—1280**                                                                                                                                                                                     |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.2/sample_0013.gif&quot; width=&quot;&quot;&gt;](https://github.com/hpcaitech/Open-Sora/assets/99191637/7895aab6-ed23-488c-8486-091480c26327) | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.2/sample_1718.gif&quot; width=&quot;&quot;&gt;](https://github.com/hpcaitech/Open-Sora/assets/99191637/20f07c7b-182b-4562-bbee-f1df74c86c9a) | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.2/sample_0087.gif&quot; width=&quot;&quot;&gt;](https://github.com/hpcaitech/Open-Sora/assets/99191637/3d897e0d-dc21-453a-b911-b3bda838acc2) |
| [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.2/sample_0052.gif&quot; width=&quot;&quot;&gt;](https://github.com/hpcaitech/Open-Sora/assets/99191637/644bf938-96ce-44aa-b797-b3c0b513d64c) | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.2/sample_1719.gif&quot; width=&quot;&quot;&gt;](https://github.com/hpcaitech/Open-Sora/assets/99191637/272d88ac-4b4a-484d-a665-8d07431671d0) | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.2/sample_0002.gif&quot; width=&quot;&quot;&gt;](https://github.com/hpcaitech/Open-Sora/assets/99191637/ebbac621-c34e-4bb4-9543-1c34f8989764) |
| [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.2/sample_0011.gif&quot; width=&quot;&quot;&gt;](https://github.com/hpcaitech/Open-Sora/assets/99191637/a1e3a1a3-4abd-45f5-8df2-6cced69da4ca) | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.2/sample_0004.gif&quot; width=&quot;&quot;&gt;](https://github.com/hpcaitech/Open-Sora/assets/99191637/d6ce9c13-28e1-4dff-9644-cc01f5f11926) | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.2/sample_0061.gif&quot; width=&quot;&quot;&gt;](https://github.com/hpcaitech/Open-Sora/assets/99191637/561978f8-f1b0-4f4d-ae7b-45bec9001b4a) |

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;OpenSora 1.1 Demo&lt;/summary&gt;

| **2s 240Ã—426**                                                                                                                                                                                                  | **2s 240Ã—426**                                                                                                                                                                                                 |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.1/sample_16x240x426_9.gif&quot; width=&quot;&quot;&gt;](https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/c31ebc52-de39-4a4e-9b1e-9211d45e05b2) | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.1/sora_16x240x426_26.gif&quot; width=&quot;&quot;&gt;](https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/c31ebc52-de39-4a4e-9b1e-9211d45e05b2) |
| [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.1/sora_16x240x426_27.gif&quot; width=&quot;&quot;&gt;](https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/f7ce4aaa-528f-40a8-be7a-72e61eaacbbd)  | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.1/sora_16x240x426_40.gif&quot; width=&quot;&quot;&gt;](https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/5d58d71e-1fda-4d90-9ad3-5f2f7b75c6a9) |

| **2s 426Ã—240**                                                                                                                                                                                                 | **4s 480Ã—854**                                                                                                                                                                                                  |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.1/sora_16x426x240_24.gif&quot; width=&quot;&quot;&gt;](https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/34ecb4a0-4eef-4286-ad4c-8e3a87e5a9fd) | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.1/sample_32x480x854_9.gif&quot; width=&quot;&quot;&gt;](https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/c1619333-25d7-42ba-a91c-18dbc1870b18) |

| **16s 320Ã—320**                                                                                                                                                                                            | **16s 224Ã—448**                                                                                                                                                                                            | **2s 426Ã—240**                                                                                                                                                                                                |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.1/sample_16s_320x320.gif&quot; width=&quot;&quot;&gt;](https://github.com/hpcaitech/Open-Sora/assets/99191637/3cab536e-9b43-4b33-8da8-a0f9cf842ff2) | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.1/sample_16s_224x448.gif&quot; width=&quot;&quot;&gt;](https://github.com/hpcaitech/Open-Sora/assets/99191637/9fb0b9e0-c6f4-4935-b29e-4cac10b373c4) | [&lt;img src=&quot;https://github.com/hpcaitech/Open-Sora-Demo/blob/main/demo/v1.1/sora_16x426x240_3.gif&quot; width=&quot;&quot;&gt;](https://github.com/hpcaitech/Open-Sora-dev/assets/99191637/3e892ad2-9543-4049-b005-643a4c1bf3bf) |

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;OpenSora 1.0 Demo&lt;/summary&gt;

| **2s 512Ã—512**                                                                                                                                                                                   | **2s 512Ã—512**                                                                                                                                                                                   | **2s 512Ã—512**                                                                                                                                                                                   |
| --------------------

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[lastmile-ai/mcp-agent]]></title>
            <link>https://github.com/lastmile-ai/mcp-agent</link>
            <guid>https://github.com/lastmile-ai/mcp-agent</guid>
            <pubDate>Fri, 23 May 2025 00:04:16 GMT</pubDate>
            <description><![CDATA[Build effective agents using Model Context Protocol and simple workflow patterns]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lastmile-ai/mcp-agent">lastmile-ai/mcp-agent</a></h1>
            <p>Build effective agents using Model Context Protocol and simple workflow patterns</p>
            <p>Language: Python</p>
            <p>Stars: 4,674</p>
            <p>Forks: 421</p>
            <p>Stars today: 86 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/6f4e40c4-dc88-47b6-b965-5856b69416d2&quot; alt=&quot;Logo&quot; width=&quot;300&quot; /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;em&gt;Build effective agents with Model Context Protocol using simple, composable patterns.&lt;/em&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/lastmile-ai/mcp-agent/tree/main/examples&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/a&gt;
  |
  &lt;a href=&quot;https://www.anthropic.com/research/building-effective-agents&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;Building Effective Agents&lt;/strong&gt;&lt;/a&gt;
  |
  &lt;a href=&quot;https://modelcontextprotocol.io/introduction&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;MCP&lt;/strong&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://pypi.org/project/mcp-agent/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/mcp-agent?color=%2334D058&amp;label=pypi&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/lastmile-ai/mcp-agent/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-raw/lastmile-ai/mcp-agent&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://lmai.link/discord/mcp-agent&quot;&gt;&lt;img src=&quot;https://shields.io/discord/1089284610329952357&quot; alt=&quot;discord&quot; /&gt;&lt;/a&gt;
&lt;img alt=&quot;Pepy Total Downloads&quot; src=&quot;https://img.shields.io/pepy/dt/mcp-agent?label=pypi%20%7C%20downloads&quot;/&gt;
&lt;a href=&quot;https://github.com/lastmile-ai/mcp-agent/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/l/mcp-agent&quot; /&gt;&lt;/a&gt;
&lt;/p&gt;

## Overview

**`mcp-agent`** is a simple, composable framework to build agents using [Model Context Protocol](https://modelcontextprotocol.io/introduction).

**Inspiration**: Anthropic announced 2 foundational updates for AI application developers:

1. [Model Context Protocol](https://www.anthropic.com/news/model-context-protocol) - a standardized interface to let any software be accessible to AI assistants via MCP servers.
2. [Building Effective Agents](https://www.anthropic.com/research/building-effective-agents) - a seminal writeup on simple, composable patterns for building production-ready AI agents.

`mcp-agent` puts these two foundational pieces into an AI application framework:

1. It handles the pesky business of managing the lifecycle of MCP server connections so you don&#039;t have to.
2. It implements every pattern described in Building Effective Agents, and does so in a _composable_ way, allowing you to chain these patterns together.
3. **Bonus**: It implements [OpenAI&#039;s Swarm](https://github.com/openai/swarm) pattern for multi-agent orchestration, but in a model-agnostic way.

Altogether, this is the simplest and easiest way to build robust agent applications. Much like MCP, this project is in early development.
We welcome all kinds of [contributions](/CONTRIBUTING.md), feedback and your help in growing this to become a new standard.

## Get Started

We recommend using [uv](https://docs.astral.sh/uv/) to manage your Python projects:

```bash
uv add &quot;mcp-agent&quot;
```

Alternatively:

```bash
pip install mcp-agent
```

### Quickstart

&gt; [!TIP]
&gt; The [`examples`](/examples) directory has several example applications to get started with.
&gt; To run an example, clone this repo, then:
&gt;
&gt; ```bash
&gt; cd examples/basic/mcp_basic_agent # Or any other example
&gt; cp mcp_agent.secrets.yaml.example mcp_agent.secrets.yaml # Update API keys
&gt; uv run main.py
&gt; ```

Here is a basic &quot;finder&quot; agent that uses the fetch and filesystem servers to look up a file, read a blog and write a tweet. [Example link](./examples/basic/mcp_basic_agent/):

&lt;details open&gt;
&lt;summary&gt;finder_agent.py&lt;/summary&gt;

```python
import asyncio
import os

from mcp_agent.app import MCPApp
from mcp_agent.agents.agent import Agent
from mcp_agent.workflows.llm.augmented_llm_openai import OpenAIAugmentedLLM

app = MCPApp(name=&quot;hello_world_agent&quot;)

async def example_usage():
    async with app.run() as mcp_agent_app:
        logger = mcp_agent_app.logger
        # This agent can read the filesystem or fetch URLs
        finder_agent = Agent(
            name=&quot;finder&quot;,
            instruction=&quot;&quot;&quot;You can read local files or fetch URLs.
                Return the requested information when asked.&quot;&quot;&quot;,
            server_names=[&quot;fetch&quot;, &quot;filesystem&quot;], # MCP servers this Agent can use
        )

        async with finder_agent:
            # Automatically initializes the MCP servers and adds their tools for LLM use
            tools = await finder_agent.list_tools()
            logger.info(f&quot;Tools available:&quot;, data=tools)

            # Attach an OpenAI LLM to the agent (defaults to GPT-4o)
            llm = await finder_agent.attach_llm(OpenAIAugmentedLLM)

            # This will perform a file lookup and read using the filesystem server
            result = await llm.generate_str(
                message=&quot;Show me what&#039;s in README.md verbatim&quot;
            )
            logger.info(f&quot;README.md contents: {result}&quot;)

            # Uses the fetch server to fetch the content from URL
            result = await llm.generate_str(
                message=&quot;Print the first two paragraphs from https://www.anthropic.com/research/building-effective-agents&quot;
            )
            logger.info(f&quot;Blog intro: {result}&quot;)

            # Multi-turn interactions by default
            result = await llm.generate_str(&quot;Summarize that in a 128-char tweet&quot;)
            logger.info(f&quot;Tweet: {result}&quot;)

if __name__ == &quot;__main__&quot;:
    asyncio.run(example_usage())

```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;mcp_agent.config.yaml&lt;/summary&gt;

```yaml
execution_engine: asyncio
logger:
  transports: [console] # You can use [file, console] for both
  level: debug
  path: &quot;logs/mcp-agent.jsonl&quot; # Used for file transport
  # For dynamic log filenames:
  # path_settings:
  #   path_pattern: &quot;logs/mcp-agent-{unique_id}.jsonl&quot;
  #   unique_id: &quot;timestamp&quot;  # Or &quot;session_id&quot;
  #   timestamp_format: &quot;%Y%m%d_%H%M%S&quot;

mcp:
  servers:
    fetch:
      command: &quot;uvx&quot;
      args: [&quot;mcp-server-fetch&quot;]
    filesystem:
      command: &quot;npx&quot;
      args:
        [
          &quot;-y&quot;,
          &quot;@modelcontextprotocol/server-filesystem&quot;,
          &quot;&lt;add_your_directories&gt;&quot;,
        ]

openai:
  # Secrets (API keys, etc.) are stored in an mcp_agent.secrets.yaml file which can be gitignored
  default_model: gpt-4o
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Agent output&lt;/summary&gt;
&lt;img width=&quot;2398&quot; alt=&quot;Image&quot; src=&quot;https://github.com/user-attachments/assets/eaa60fdf-bcc6-460b-926e-6fa8534e9089&quot; /&gt;
&lt;/details&gt;

## Table of Contents

- [Why use mcp-agent?](#why-use-mcp-agent)
- [Example Applications](#examples)
  - [Claude Desktop](#claude-desktop)
  - [Streamlit](#streamlit)
    - [Gmail Agent](#gmail-agent)
    - [RAG](#simple-rag-chatbot)
  - [Marimo](#marimo)
  - [Python](#python)
    - [Swarm (CLI)](#swarm)
- [Core Concepts](#core-components)
- [Workflows Patterns](#workflows)
  - [Augmented LLM](#augmentedllm)
  - [Parallel](#parallel)
  - [Router](#router)
  - [Intent-Classifier](#intentclassifier)
  - [Orchestrator-Workers](#orchestrator-workers)
  - [Evaluator-Optimizer](#evaluator-optimizer)
  - [OpenAI Swarm](#swarm-1)
- [Advanced](#advanced)
  - [Composing multiple workflows](#composability)
  - [Signaling and Human input](#signaling-and-human-input)
  - [App Config](#app-config)
  - [MCP Server Management](#mcp-server-management)
- [Contributing](#contributing)
- [Roadmap](#roadmap)
- [FAQs](#faqs)

## Why use `mcp-agent`?

There are too many AI frameworks out there already. But `mcp-agent` is the only one that is purpose-built for a shared protocol - [MCP](https://modelcontextprotocol.io/introduction). It is also the most lightweight, and is closer to an agent pattern library than a framework.

As [more services become MCP-aware](https://github.com/punkpeye/awesome-mcp-servers), you can use mcp-agent to build robust and controllable AI agents that can leverage those services out-of-the-box.

## Examples

Before we go into the core concepts of mcp-agent, let&#039;s show what you can build with it.

In short, you can build any kind of AI application with mcp-agent: multi-agent collaborative workflows, human-in-the-loop workflows, RAG pipelines and more.

### Claude Desktop

You can integrate mcp-agent apps into MCP clients like Claude Desktop.

#### mcp-agent server

This app wraps an mcp-agent application inside an MCP server, and exposes that server to Claude Desktop.
The app exposes agents and workflows that Claude Desktop can invoke to service of the user&#039;s request.

https://github.com/user-attachments/assets/7807cffd-dba7-4f0c-9c70-9482fd7e0699

This demo shows a multi-agent evaluation task where each agent evaluates aspects of an input poem, and
then an aggregator summarizes their findings into a final response.

**Details**: Starting from a user&#039;s request over text, the application:

- dynamically defines agents to do the job
- uses the appropriate workflow to orchestrate those agents (in this case the Parallel workflow)

**Link to code**: [examples/basic/mcp_agent_server](./examples/basic/mcp_agent_server)

&gt; [!NOTE]
&gt; Huge thanks to [Jerron Lim (@StreetLamb)](https://github.com/StreetLamb)
&gt; for developing and contributing this example!

### Streamlit

You can deploy mcp-agent apps using Streamlit.

#### Gmail agent

This app is able to perform read and write actions on gmail using text prompts -- i.e. read, delete, send emails, mark as read/unread, etc.
It uses an MCP server for Gmail.

https://github.com/user-attachments/assets/54899cac-de24-4102-bd7e-4b2022c956e3

**Link to code**: [gmail-mcp-server](https://github.com/jasonsum/gmail-mcp-server/blob/add-mcp-agent-streamlit/streamlit_app.py)

&gt; [!NOTE]
&gt; Huge thanks to [Jason Summer (@jasonsum)](https://github.com/jasonsum)
&gt; for developing and contributing this example!

#### Simple RAG Chatbot

This app uses a Qdrant vector database (via an MCP server) to do Q&amp;A over a corpus of text.

https://github.com/user-attachments/assets/f4dcd227-cae9-4a59-aa9e-0eceeb4acaf4

**Link to code**: [examples/usecases/streamlit_mcp_rag_agent](./examples/usecases/streamlit_mcp_rag_agent/)

&gt; [!NOTE]
&gt; Huge thanks to [Jerron Lim (@StreetLamb)](https://github.com/StreetLamb)
&gt; for developing and contributing this example!

### Marimo

[Marimo](https://github.com/marimo-team/marimo) is a reactive Python notebook that replaces Jupyter and Streamlit.
Here&#039;s the &quot;file finder&quot; agent from [Quickstart](#quickstart) implemented in Marimo:

&lt;img src=&quot;https://github.com/user-attachments/assets/139a95a5-e3ac-4ea7-9c8f-bad6577e8597&quot; width=&quot;400&quot;/&gt;

**Link to code**: [examples/usecases/marimo_mcp_basic_agent](./examples/usecases/marimo_mcp_basic_agent/)

&gt; [!NOTE]
&gt; Huge thanks to [Akshay Agrawal (@akshayka)](https://github.com/akshayka)
&gt; for developing and contributing this example!

### Python

You can write mcp-agent apps as Python scripts or Jupyter notebooks.

#### Swarm

This example demonstrates a multi-agent setup for handling different customer service requests in an airline context using the Swarm workflow pattern. The agents can triage requests, handle flight modifications, cancellations, and lost baggage cases.

https://github.com/user-attachments/assets/b314d75d-7945-4de6-965b-7f21eb14a8bd

**Link to code**: [examples/workflows/workflow_swarm](./examples/workflows/workflow_swarm/)

## Core Components

The following are the building blocks of the mcp-agent framework:

- **[MCPApp](./src/mcp_agent/app.py)**: global state and app configuration
- **MCP server management**: [`gen_client`](./src/mcp_agent/mcp/gen_client.py) and [`MCPConnectionManager`](./src/mcp_agent/mcp/mcp_connection_manager.py) to easily connect to MCP servers.
- **[Agent](./src/mcp_agent/agents/agent.py)**: An Agent is an entity that has access to a set of MCP servers and exposes them to an LLM as tool calls. It has a name and purpose (instruction).
- **[AugmentedLLM](./src/mcp_agent/workflows/llm/augmented_llm.py)**: An LLM that is enhanced with tools provided from a collection of MCP servers. Every Workflow pattern described below is an `AugmentedLLM` itself, allowing you to compose and chain them together.

Everything in the framework is a derivative of these core capabilities.

## Workflows

mcp-agent provides implementations for every pattern in Anthropicâ€™s [Building Effective Agents](https://www.anthropic.com/research/building-effective-agents), as well as the OpenAI [Swarm](https://github.com/openai/swarm) pattern.
Each pattern is model-agnostic, and exposed as an `AugmentedLLM`, making everything very composable.

### AugmentedLLM

[AugmentedLLM](./src/mcp_agent/workflows/llm/augmented_llm.py) is an LLM that has access to MCP servers and functions via Agents.

LLM providers implement the AugmentedLLM interface to expose 3 functions:

- `generate`: Generate message(s) given a prompt, possibly over multiple iterations and making tool calls as needed.
- `generate_str`: Calls `generate` and returns result as a string output.
- `generate_structured`: Uses [Instructor](https://github.com/instructor-ai/instructor) to return the generated result as a Pydantic model.

Additionally, `AugmentedLLM` has memory, to keep track of long or short-term history.

&lt;details&gt;
&lt;summary&gt;Example&lt;/summary&gt;

```python
from mcp_agent.agents.agent import Agent
from mcp_agent.workflows.llm.augmented_llm_anthropic import AnthropicAugmentedLLM

finder_agent = Agent(
    name=&quot;finder&quot;,
    instruction=&quot;You are an agent with filesystem + fetch access. Return the requested file or URL contents.&quot;,
    server_names=[&quot;fetch&quot;, &quot;filesystem&quot;],
)

async with finder_agent:
   llm = await finder_agent.attach_llm(AnthropicAugmentedLLM)

   result = await llm.generate_str(
      message=&quot;Print the first 2 paragraphs of https://www.anthropic.com/research/building-effective-agents&quot;,
      # Can override model, tokens and other defaults
   )
   logger.info(f&quot;Result: {result}&quot;)

   # Multi-turn conversation
   result = await llm.generate_str(
      message=&quot;Summarize those paragraphs in a 128 character tweet&quot;,
   )
   logger.info(f&quot;Result: {result}&quot;)
```

&lt;/details&gt;

### [Parallel](src/mcp_agent/workflows/parallel/parallel_llm.py)

![Parallel workflow (Image credit: Anthropic)](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F406bb032ca007fd1624f261af717d70e6ca86286-2401x1000.png&amp;w=3840&amp;q=75)

Fan-out tasks to multiple sub-agents and fan-in the results. Each subtask is an AugmentedLLM, as is the overall Parallel workflow, meaning each subtask can optionally be a more complex workflow itself.

&gt; [!NOTE]
&gt;
&gt; **[Link to full example](examples/workflows/workflow_parallel/main.py)**

&lt;details&gt;
&lt;summary&gt;Example&lt;/summary&gt;

```python
proofreader = Agent(name=&quot;proofreader&quot;, instruction=&quot;Review grammar...&quot;)
fact_checker = Agent(name=&quot;fact_checker&quot;, instruction=&quot;Check factual consistency...&quot;)
style_enforcer = Agent(name=&quot;style_enforcer&quot;, instruction=&quot;Enforce style guidelines...&quot;)

grader = Agent(name=&quot;grader&quot;, instruction=&quot;Combine feedback into a structured report.&quot;)

parallel = ParallelLLM(
    fan_in_agent=grader,
    fan_out_agents=[proofreader, fact_checker, style_enforcer],
    llm_factory=OpenAIAugmentedLLM,
)

result = await parallel.generate_str(&quot;Student short story submission: ...&quot;, RequestParams(model=&quot;gpt4-o&quot;))
```

&lt;/details&gt;

### [Router](src/mcp_agent/workflows/router/)

![Router workflow (Image credit: Anthropic)](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F5c0c0e9fe4def0b584c04d37849941da55e5e71c-2401x1000.png&amp;w=3840&amp;q=75)

Given an input, route to the `top_k` most relevant categories. A category can be an Agent, an MCP server or a regular function.

mcp-agent provides several router implementations, including:

- [`EmbeddingRouter`](src/mcp_agent/workflows/router/router_embedding.py): uses embedding models for classification
- [`LLMRouter`](src/mcp_agent/workflows/router/router_llm.py): uses LLMs for classification

&gt; [!NOTE]
&gt;
&gt; **[Link to full example](examples/workflows/workflow_router/main.py)**

&lt;details&gt;
&lt;summary&gt;Example&lt;/summary&gt;

```python
def print_hello_world:
     print(&quot;Hello, world!&quot;)

finder_agent = Agent(name=&quot;finder&quot;, server_names=[&quot;fetch&quot;, &quot;filesystem&quot;])
writer_agent = Agent(name=&quot;writer&quot;, server_names=[&quot;filesystem&quot;])

llm = OpenAIAugmentedLLM()
router = LLMRouter(
    llm=llm,
    agents=[finder_agent, writer_agent],
    functions=[print_hello_world],
)

results = await router.route( # Also available: route_to_agent, route_to_server
    request=&quot;Find and print the contents of README.md verbatim&quot;,
    top_k=1
)
chosen_agent = results[0].result
async with chosen_agent:
    ...
```

&lt;/details&gt;

### [IntentClassifier](src/mcp_agent/workflows/intent_classifier/)

A close sibling of Router, the Intent Classifier pattern identifies the `top_k` Intents that most closely match a given input.
Just like a Router, mcp-agent provides both an [embedding](src/mcp_agent/workflows/intent_classifier/intent_classifier_embedding.py) and [LLM-based](src/mcp_agent/workflows/intent_classifier/intent_classifier_llm.py) intent classifier.

### [Evaluator-Optimizer](src/mcp_agent/workflows/evaluator_optimizer/evaluator_optimizer.py)

![Evaluator-optimizer workflow (Image credit: Anthropic)](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F14f51e6406ccb29e695da48b17017e899a6119c7-2401x1000.png&amp;w=3840&amp;q=75)

One LLM (the â€œoptimizerâ€) refines a response, another (the â€œevaluatorâ€) critiques it until a response exceeds a quality criteria.

&gt; [!NOTE]
&gt;
&gt; **[Link to full example](examples/workflows/workflow_evaluator_optimizer/main.py)**

&lt;details&gt;
&lt;summary&gt;Example&lt;/summary&gt;

```python
optimizer = Agent(name=&quot;cover_letter_writer&quot;, server_names=[&quot;fetch&quot;], instruction=&quot;Generate a cover letter ...&quot;)
evaluator = Agent(name=&quot;critiquer&quot;, instruction=&quot;Evaluate clarity, specificity, relevance...&quot;)

llm = EvaluatorOptimizerLLM(
    optimizer=optimizer,
    evaluator=evaluator,
    llm_factory=OpenAIAugmentedLLM,
    min_rating=QualityRating.EXCELLENT, # Keep iterating until the minimum quality bar is reached
)

result = await eo_llm.generate_str(&quot;Write a job cover letter for an AI framework developer role at LastMile AI.&quot;)
print(&quot;Final refined cover letter:&quot;, result)
```

&lt;/details&gt;

### [Orchestrator-workers](src/mcp_agent/workflows/orchestrator/orchestrator.py)

![Orchestrator workflow (Image credit: Anthropic)](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F8985fc683fae4780fb34eab1365ab78c7e51bc8e-2401x1000.png&amp;w=3840&amp;q=75)

A higher-level LLM generates a plan, then assigns them to sub-agents, and synthesizes the results.
The Orchestrator workflow automatically parallelizes steps that can be done in parallel, and blocks on dependencies.

&gt; [!NOTE]
&gt;
&gt; **[Link to full example](examples/workflows/workflow_orchestrator_worker/main.py)**

&lt;details&gt;
&lt;summary&gt;Example&lt;/summary&gt;

```python
finder_agent = Agent(name=&quot;finder&quot;, server_names=[&quot;fetch&quot;, &quot;filesystem&quot;])
writer_agent = Agent(name=&quot;writer&quot;, server_names=[&quot;filesystem&quot;])
proofreader = Agent(name=&quot;proofreader&quot;, ...)
fact_checker = Agent(name=&quot;fact_checker&quot;, ...)
style_enforcer = Agent(name=&quot;style_enforcer&quot;, instructions=&quot;Use APA style guide from ...&quot;, server_names=[&quot;fetch&quot;])

orchestrator = Orchestrator(
    llm_factory=AnthropicAugmentedLLM,
    available_agents=[finder_agent, writer_agent, proofreader, fact_checker, style_enforcer],
)

task = &quot;Load short_story.md, evaluate it, produce a graded_report.md with multiple feedback aspects.&quot;
result = await orchestrator.generate_str(task, RequestParams(model=&quot;gpt-4o&quot;))
print(result)
```

&lt;/details&gt;

### [Swarm](src/mcp_agent/workflows/swarm/swarm.py)

OpenAI has an experimental multi-agent pattern called [Swarm](https://github.com/openai/swarm), which we provide a model-agnostic reference implementation for in mcp-agent.

&lt;img src=&quot;https://github.com/openai/swarm/blob/main/assets/swarm_diagram.png?raw=true&quot; width=500 /&gt;

The mcp-agent Swarm pattern works seamlessly with MCP servers, and is exposed as an `AugmentedLLM`, allowing for composability with other patterns above

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jingyaogong/minimind]]></title>
            <link>https://github.com/jingyaogong/minimind</link>
            <guid>https://github.com/jingyaogong/minimind</guid>
            <pubDate>Fri, 23 May 2025 00:04:15 GMT</pubDate>
            <description><![CDATA[ğŸš€ğŸš€ ã€Œå¤§æ¨¡å‹ã€2å°æ—¶å®Œå…¨ä»0è®­ç»ƒ26Mçš„å°å‚æ•°GPTï¼ğŸŒ Train a 26M-parameter GPT from scratch in just 2h!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jingyaogong/minimind">jingyaogong/minimind</a></h1>
            <p>ğŸš€ğŸš€ ã€Œå¤§æ¨¡å‹ã€2å°æ—¶å®Œå…¨ä»0è®­ç»ƒ26Mçš„å°å‚æ•°GPTï¼ğŸŒ Train a 26M-parameter GPT from scratch in just 2h!</p>
            <p>Language: Python</p>
            <p>Stars: 21,163</p>
            <p>Forks: 2,478</p>
            <p>Stars today: 62 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

![logo](./images/logo.png)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

![visitors](https://visitor-badge.laobi.icu/badge?page_id=jingyaogong/minimind)
[![GitHub Repo stars](https://img.shields.io/github/stars/jingyaogong/minimind?style=social)](https://github.com/jingyaogong/minimind/stargazers)
[![GitHub Code License](https://img.shields.io/github/license/jingyaogong/minimind)](LICENSE)
[![GitHub last commit](https://img.shields.io/github/last-commit/jingyaogong/minimind)](https://github.com/jingyaogong/minimind/commits/master)
[![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)](https://github.com/jingyaogong/minimind/pulls)
[![Collection](https://img.shields.io/badge/ğŸ¤—-MiniMind%20%20Collection-blue)](https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;h3&gt;&quot;å¤§é“è‡³ç®€&quot;&lt;/h3&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

ä¸­æ–‡ | [English](./README_en.md)

&lt;/div&gt;

* æ­¤å¼€æºé¡¹ç›®æ—¨åœ¨å®Œå…¨ä»0å¼€å§‹ï¼Œä»…ç”¨3å—é’±æˆæœ¬ + 2å°æ—¶ï¼å³å¯è®­ç»ƒå‡ºä»…ä¸º25.8Mçš„è¶…å°è¯­è¨€æ¨¡å‹**MiniMind**ã€‚
* **MiniMind**ç³»åˆ—æå…¶è½»é‡ï¼Œæœ€å°ç‰ˆæœ¬ä½“ç§¯æ˜¯ GPT-3 çš„ $\frac{1}{7000}$ï¼ŒåŠ›æ±‚åšåˆ°æœ€æ™®é€šçš„ä¸ªäººGPUä¹Ÿå¯å¿«é€Ÿè®­ç»ƒã€‚
* é¡¹ç›®åŒæ—¶å¼€æºäº†å¤§æ¨¡å‹çš„æç®€ç»“æ„-åŒ…å«æ‹“å±•å…±äº«æ··åˆä¸“å®¶(MoE)ã€æ•°æ®é›†æ¸…æ´—ã€é¢„è®­ç»ƒ(Pretrain)ã€ç›‘ç£å¾®è°ƒ(SFT)ã€LoRAå¾®è°ƒï¼Œ
  ç›´æ¥åå¥½å¼ºåŒ–å­¦ä¹ (DPO)ç®—æ³•ã€æ¨¡å‹è’¸é¦ç®—æ³•ç­‰å…¨è¿‡ç¨‹ä»£ç ã€‚
* **MiniMind**åŒæ—¶æ‹“å±•äº†è§†è§‰å¤šæ¨¡æ€çš„VLM: [MiniMind-V](https://github.com/jingyaogong/minimind-v)ã€‚
* é¡¹ç›®æ‰€æœ‰æ ¸å¿ƒç®—æ³•ä»£ç å‡ä»0ä½¿ç”¨PyTorchåŸç”Ÿé‡æ„ï¼ä¸ä¾èµ–ç¬¬ä¸‰æ–¹åº“æä¾›çš„æŠ½è±¡æ¥å£ã€‚
* è¿™ä¸ä»…æ˜¯å¤§è¯­è¨€æ¨¡å‹çš„å…¨é˜¶æ®µå¼€æºå¤ç°ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªå…¥é—¨LLMçš„æ•™ç¨‹ã€‚
* å¸Œæœ›æ­¤é¡¹ç›®èƒ½ä¸ºæ‰€æœ‰äººæä¾›ä¸€ä¸ªæŠ›ç –å¼•ç‰çš„ç¤ºä¾‹ï¼Œä¸€èµ·æ„Ÿå—åˆ›é€ çš„ä¹è¶£ï¼æ¨åŠ¨æ›´å¹¿æ³›AIç¤¾åŒºçš„è¿›æ­¥ï¼

&gt; ä¸ºé˜²æ­¢è¯¯è§£ï¼Œâ€œ2å°æ—¶â€ åŸºäºNVIDIA 3090ç¡¬ä»¶è®¾å¤‡ï¼ˆå•å¡ï¼‰æµ‹è¯•ï¼Œâ€œ3å—é’±â€
&gt; æŒ‡GPUæœåŠ¡å™¨ç§Ÿç”¨æˆæœ¬ï¼Œå…·ä½“è§„æ ¼è¯¦æƒ…è§ä¸‹æ–‡ã€‚

---


&lt;div align=&quot;center&quot;&gt;

![minimind2](./images/minimind2.gif)

[ğŸ”—ğŸ“æ¨ç†æ¨¡å‹](https://www.modelscope.cn/studios/gongjy/MiniMind-Reasoning) | [ğŸ”—ğŸ¤–å¸¸è§„æ¨¡å‹](https://www.modelscope.cn/studios/gongjy/MiniMind) | [ğŸ”—ğŸï¸è§†é¢‘ä»‹ç»](https://www.bilibili.com/video/BV12dHPeqE72/?share_source=copy_web&amp;vd_source=670c2504f88726f8cf4a21ef6147c0e8)


&lt;div align=&quot;center&quot;&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5&quot; style=&quot;text-decoration: none;&quot;&gt;
          &lt;img src=&quot;./images/and_huggingface.png&quot; alt=&quot;Hugging Face Logo&quot; style=&quot;vertical-align: middle; width: auto; max-width: 100%;&quot; /&gt;
        &lt;/a&gt;
      &lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://www.modelscope.cn/profile/gongjy&quot; style=&quot;text-decoration: none;&quot;&gt;
          &lt;img src=&quot;./images/and_modelscope.png&quot; alt=&quot;ModelScope Logo&quot; style=&quot;vertical-align: middle; width: auto; max-width: 100%;&quot; /&gt;
        &lt;/a&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;


&lt;/div&gt;

# ğŸ“Œ Introduction

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Model, LLMï¼‰çš„å‡ºç°å¼•å‘äº†å…¨ä¸–ç•Œå¯¹AIçš„ç©ºå‰å…³æ³¨ã€‚
æ— è®ºæ˜¯ChatGPTã€DeepSeekè¿˜æ˜¯Qwenï¼Œéƒ½ä»¥å…¶æƒŠè‰³çš„æ•ˆæœä»¤äººå¹ä¸ºè§‚æ­¢ã€‚
ç„¶è€Œï¼ŒåŠ¨è¾„æ•°ç™¾äº¿å‚æ•°çš„åºå¤§è§„æ¨¡ï¼Œä½¿å¾—å®ƒä»¬å¯¹ä¸ªäººè®¾å¤‡è€Œè¨€ä¸ä»…éš¾ä»¥è®­ç»ƒï¼Œç”šè‡³è¿éƒ¨ç½²éƒ½æ˜¾å¾—é¥ä¸å¯åŠã€‚
æ‰“å¼€å¤§æ¨¡å‹çš„â€œé»‘ç›’å­â€ï¼Œæ¢ç´¢å…¶å†…éƒ¨è¿ä½œæœºåˆ¶ï¼Œå¤šä¹ˆä»¤äººå¿ƒæ½®æ¾æ¹ƒï¼
é—æ†¾çš„æ˜¯ï¼Œ99%çš„æ¢ç´¢åªèƒ½æ­¢æ­¥äºä½¿ç”¨LoRAç­‰æŠ€æœ¯å¯¹ç°æœ‰å¤§æ¨¡å‹è¿›è¡Œå°‘é‡å¾®è°ƒï¼Œå­¦ä¹ ä¸€äº›æ–°æŒ‡ä»¤æˆ–ä»»åŠ¡ã€‚
è¿™å°±å¥½æ¯”æ•™ç‰›é¡¿å¦‚ä½•ä½¿ç”¨21ä¸–çºªçš„æ™ºèƒ½æ‰‹æœºâ€”â€”è™½ç„¶æœ‰è¶£ï¼Œå´å®Œå…¨åç¦»äº†ç†è§£ç‰©ç†æœ¬è´¨çš„åˆè¡·ã€‚
ä¸æ­¤åŒæ—¶ï¼Œç¬¬ä¸‰æ–¹çš„å¤§æ¨¡å‹æ¡†æ¶å’Œå·¥å…·åº“ï¼Œå¦‚transformers+trlï¼Œå‡ ä¹åªæš´éœ²äº†é«˜åº¦æŠ½è±¡çš„æ¥å£ã€‚
é€šè¿‡çŸ­çŸ­10è¡Œä»£ç ï¼Œå°±èƒ½å®Œæˆâ€œåŠ è½½æ¨¡å‹+åŠ è½½æ•°æ®é›†+æ¨ç†+å¼ºåŒ–å­¦ä¹ â€çš„å…¨æµç¨‹è®­ç»ƒã€‚
è¿™ç§é«˜æ•ˆçš„å°è£…å›ºç„¶ä¾¿åˆ©ï¼Œä½†ä¹Ÿåƒä¸€æ¶é«˜é€Ÿé£èˆ¹ï¼Œå°†æˆ‘ä»¬ä¸åº•å±‚å®ç°éš”ç¦»å¼€æ¥ï¼Œé˜»ç¢äº†æ·±å…¥æ¢ç©¶LLMæ ¸å¿ƒä»£ç çš„æœºä¼šã€‚
ç„¶è€Œï¼Œâ€œç”¨ä¹é«˜æ‹¼å‡ºä¸€æ¶é£æœºï¼Œè¿œæ¯”ååœ¨å¤´ç­‰èˆ±é‡Œé£è¡Œæ›´è®©äººå…´å¥‹ï¼â€ã€‚
æ›´ç³Ÿç³•çš„æ˜¯ï¼Œäº’è”ç½‘ä¸Šå……æ–¥ç€å¤§é‡ä»˜è´¹è¯¾ç¨‹å’Œè¥é”€å·ï¼Œä»¥æ¼æ´ç™¾å‡ºã€ä¸€çŸ¥åŠè§£çš„å†…å®¹æ¨é”€AIæ•™ç¨‹ã€‚
æ­£å› å¦‚æ­¤ï¼Œæœ¬é¡¹ç›®åˆè¡·æ˜¯æ‹‰ä½LLMçš„å­¦ä¹ é—¨æ§›ï¼Œè®©æ¯ä¸ªäººéƒ½èƒ½ä»ç†è§£æ¯ä¸€è¡Œä»£ç å¼€å§‹ï¼Œ
ä»é›¶å¼€å§‹äº²æ‰‹è®­ç»ƒä¸€ä¸ªæå°çš„è¯­è¨€æ¨¡å‹ã€‚æ˜¯çš„ï¼Œä»**é›¶å¼€å§‹è®­ç»ƒ**ï¼Œè€Œä¸æ˜¯ä»…ä»…è¿›è¡Œ**æ¨ç†**ï¼
æœ€ä½åªéœ€3å—é’±ä¸åˆ°çš„æœåŠ¡å™¨æˆæœ¬ï¼Œå°±èƒ½äº²èº«ä½“éªŒä»0åˆ°1æ„å»ºä¸€ä¸ªè¯­è¨€æ¨¡å‹çš„å…¨è¿‡ç¨‹ã€‚
ä¸€èµ·æ„Ÿå—åˆ›é€ çš„ä¹è¶£å§ï¼

&gt; [!NOTE]
&gt; ï¼ˆæˆªè‡³2025-02-07ï¼‰MiniMindç³»åˆ—å·²å®Œæˆå¤šä¸ªå‹å·æ¨¡å‹çš„é¢„è®­ç»ƒï¼Œæœ€å°ä»…éœ€25.8Mï¼ˆ0.02Bï¼‰ï¼Œå³å¯å…·å¤‡æµç•…å¯¹è¯èƒ½åŠ›ï¼

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Models List&lt;/summary&gt;

| æ¨¡å‹ (å¤§å°)                 | æ¨ç†å ç”¨ (çº¦) | Release    | 
|-------------------------|----------|------------|
| MiniMind2-small (26M)   | 0.5 GB   | 2025.04.26 |
| MiniMind2-MoE (145M)    | 1.0 GB   | 2025.04.26 |
| MiniMind2 (104M)        | 1.0 GB   | 2025.04.26 |
| minimind-v1-small (26M) | 0.5 GB   | 2024.08.28 |
| minimind-v1-moe (4Ã—26M) | 1.0 GB   | 2024.09.17 |
| minimind-v1 (108M)      | 1.0 GB   | 2024.09.01 |

&lt;/details&gt;

**é¡¹ç›®åŒ…å«**

- MiniMind-LLMç»“æ„çš„å…¨éƒ¨ä»£ç ï¼ˆDense+MoEæ¨¡å‹ï¼‰ã€‚
- åŒ…å«Tokenizeråˆ†è¯å™¨è¯¦ç»†è®­ç»ƒä»£ç ã€‚
- åŒ…å«Pretrainã€SFTã€LoRAã€RLHF-DPOã€æ¨¡å‹è’¸é¦çš„å…¨è¿‡ç¨‹è®­ç»ƒä»£ç ã€‚
- æ”¶é›†ã€è’¸é¦ã€æ•´ç†å¹¶æ¸…æ´—å»é‡æ‰€æœ‰é˜¶æ®µçš„é«˜è´¨é‡æ•°æ®é›†ï¼Œä¸”å…¨éƒ¨å¼€æºã€‚
- ä»0å®ç°é¢„è®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒã€LoRAã€DPOå¼ºåŒ–å­¦ä¹ ï¼Œç™½ç›’æ¨¡å‹è’¸é¦ã€‚å…³é”®ç®—æ³•å‡ ä¹ä¸ä¾èµ–ç¬¬ä¸‰æ–¹å°è£…çš„æ¡†æ¶ï¼Œä¸”å…¨éƒ¨å¼€æºã€‚
- åŒæ—¶å…¼å®¹`transformers`ã€`trl`ã€`peft`ç­‰ç¬¬ä¸‰æ–¹ä¸»æµæ¡†æ¶ã€‚
- è®­ç»ƒæ”¯æŒå•æœºå•å¡ã€å•æœºå¤šå¡(DDPã€DeepSpeed)è®­ç»ƒï¼Œæ”¯æŒwandbå¯è§†åŒ–è®­ç»ƒæµç¨‹ã€‚æ”¯æŒåŠ¨æ€å¯åœè®­ç»ƒã€‚
- åœ¨ç¬¬ä¸‰æ–¹æµ‹è¯„æ¦œï¼ˆC-Evalã€C-MMLUã€OpenBookQAç­‰ï¼‰è¿›è¡Œæ¨¡å‹æµ‹è¯•ã€‚
- å®ç°Openai-Apiåè®®çš„æç®€æœåŠ¡ç«¯ï¼Œä¾¿äºé›†æˆåˆ°ç¬¬ä¸‰æ–¹ChatUIä½¿ç”¨ï¼ˆFastGPTã€Open-WebUIç­‰ï¼‰ã€‚
- åŸºäºstreamlitå®ç°æœ€ç®€èŠå¤©WebUIå‰ç«¯ã€‚
- å…¨é¢å…¼å®¹ç¤¾åŒºçƒ­é—¨`llama.cpp`ã€`vllm`ã€`ollama`æ¨ç†å¼•æ“æˆ–`Llama-Factory`è®­ç»ƒæ¡†æ¶ã€‚
- å¤ç°(è’¸é¦/RL)å¤§å‹æ¨ç†æ¨¡å‹DeepSeek-R1çš„MiniMind-Reasonæ¨¡å‹ï¼Œ**æ•°æ®+æ¨¡å‹**å…¨éƒ¨å¼€æºï¼

å¸Œæœ›æ­¤å¼€æºé¡¹ç›®å¯ä»¥å¸®åŠ©LLMåˆå­¦è€…å¿«é€Ÿå…¥é—¨ï¼

### ğŸ‘‰**æ›´æ–°æ—¥å¿—**

&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2025-04-26 (newest ğŸ‰ğŸ‰ğŸ‰)&lt;/b&gt; &lt;/summary&gt;

- é‡è¦æ›´æ–°
- å¦‚æœ‰å…¼å®¹æ€§éœ€è¦ï¼Œå¯è®¿é—®[ğŸ”—æ—§ä»“åº“å†…å®¹ğŸ”—](https://github.com/jingyaogong/minimind/tree/7da201a944a90ed49daef8a0265c959288dff83a)ã€‚
- MiniMindæ¨¡å‹å‚æ•°å®Œå…¨æ”¹åï¼Œå¯¹é½Transformersåº“æ¨¡å‹ï¼ˆç»Ÿä¸€å‘½åï¼‰ã€‚
- generateæ–¹å¼é‡æ„ï¼Œç»§æ‰¿è‡ªGenerationMixinç±»ã€‚
- ğŸ”¥æ”¯æŒllama.cppã€vllmã€ollamaç­‰çƒ­é—¨ä¸‰æ–¹ç”Ÿæ€ã€‚
- è§„èŒƒä»£ç å’Œç›®å½•ç»“æ„ã€‚
- æ”¹åŠ¨è¯è¡¨`&lt;s&gt;&lt;/s&gt;`-&gt;`&lt;|im_start|&gt;&lt;|im_end|&gt;`
```text
ä¸ºå…¼å®¹ç¬¬ä¸‰æ–¹æ¨ç†æ¡†æ¶llama.cppã€vllmï¼Œæœ¬æ¬¡æ›´æ–°éœ€ä»˜å‡ºä¸€äº›å¯è§‚ä»£ä»·ã€‚
æœ¬æ¬¡æ›´æ–°ä¸å†æ”¯æŒã€Œç›´æ¥ã€åŠ è½½25-04-26ä»¥å‰çš„æ—§æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚
ç”±äºLlamaä½ç½®ç¼–ç æ–¹å¼ä¸minimindå­˜åœ¨åŒºåˆ«ï¼Œå¯¼è‡´æ˜ å°„Llamaæ¨¡å‹åQKå€¼å­˜åœ¨å·®å¼‚
MiniMind2ç³»åˆ—æ—§æ¨¡å‹å‡ç»è¿‡æƒé‡æ˜ å°„+ï¼ˆå¾®è°ƒè®­ç»ƒï¼‰QKVOçº¿æ€§å±‚æ ¡å‡†æ¢å¤è€Œæ¥ã€‚
æœ¬æ¬¡æ›´æ–°åå°†æ”¾å¼ƒå¯¹`minimind-v1`å…¨ç³»åˆ—çš„ç»´æŠ¤ï¼Œå¹¶åœ¨ä»“åº“ä¸­ä¸‹çº¿ã€‚
```
&lt;/details&gt;

&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2025-02-09&lt;/b&gt; &lt;/summary&gt;

- è¿æ¥å‘å¸ƒä»¥æ¥é‡å¤§æ›´æ–°ï¼ŒRelease MiniMind2 Seriesã€‚
- ä»£ç å‡ ä¹å…¨éƒ¨é‡æ„ï¼Œä½¿ç”¨æ›´ç®€æ´æ˜äº†çš„ç»Ÿä¸€ç»“æ„ã€‚
  å¦‚æœ‰æ—§ä»£ç çš„å…¼å®¹æ€§éœ€è¦ï¼Œå¯è®¿é—®[ğŸ”—æ—§ä»“åº“å†…å®¹ğŸ”—](https://github.com/jingyaogong/minimind/tree/6e9cd28ef9b34a0a10afbdf6f59e65cb6e628efb)ã€‚
- å…å»æ•°æ®é¢„å¤„ç†æ­¥éª¤ã€‚ç»Ÿä¸€æ•°æ®é›†æ ¼å¼ï¼Œæ›´æ¢ä¸º`jsonl`æ ¼å¼æœç»æ•°æ®é›†ä¸‹è½½æ··ä¹±çš„é—®é¢˜ã€‚
- MiniMind2ç³»åˆ—æ•ˆæœç›¸æ¯”MiniMind-V1æ˜¾è‘—æå‡ã€‚
- å°é—®é¢˜ï¼š{kv-cacheå†™æ³•æ›´æ ‡å‡†ã€MoEçš„è´Ÿè½½å‡è¡¡lossè¢«è€ƒè™‘ç­‰ç­‰}
- æä¾›æ¨¡å‹è¿ç§»åˆ°ç§æœ‰æ•°æ®é›†çš„è®­ç»ƒæ–¹æ¡ˆï¼ˆåŒ»ç–—æ¨¡å‹ã€è‡ªæˆ‘è®¤çŸ¥æ ·ä¾‹ï¼‰ã€‚
- ç²¾ç®€é¢„è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶å¤§å¹…æå‡é¢„è®­ç»ƒæ•°æ®è´¨é‡ï¼Œå¤§å¹…ç¼©çŸ­ä¸ªäººå¿«é€Ÿè®­ç»ƒæ‰€éœ€æ—¶é—´ï¼Œå•å¡3090å³å¯2å°æ—¶å¤ç°ï¼
- æ›´æ–°ï¼šLoRAå¾®è°ƒè„±ç¦»peftåŒ…è£…ï¼Œä»0å®ç°LoRAè¿‡ç¨‹ï¼›DPOç®—æ³•ä»0ä½¿ç”¨PyTorchåŸç”Ÿå®ç°ï¼›æ¨¡å‹ç™½ç›’è’¸é¦åŸç”Ÿå®ç°ã€‚
- MiniMind2-DeepSeek-R1ç³»åˆ—è’¸é¦æ¨¡å‹è¯ç”Ÿï¼
- MiniMind2å…·å¤‡ä¸€å®šçš„è‹±æ–‡èƒ½åŠ›ï¼
- æ›´æ–°MiniMind2ä¸ç¬¬ä¸‰æ–¹æ¨¡å‹çš„åŸºäºæ›´å¤šå¤§æ¨¡å‹æ¦œå•æµ‹è¯•æ€§èƒ½çš„ç»“æœã€‚

&lt;/details&gt;



&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2024-10-05&lt;/b&gt; &lt;/summary&gt;

- ä¸ºMiniMindæ‹“å±•äº†å¤šæ¨¡æ€èƒ½åŠ›ä¹‹---è§†è§‰
- ç§»æ­¥å­ªç”Ÿé¡¹ç›®[minimind-v](https://github.com/jingyaogong/minimind-v)æŸ¥çœ‹è¯¦æƒ…ï¼

&lt;/details&gt;



&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2024-09-27&lt;/b&gt; &lt;/summary&gt;

- 09-27æ›´æ–°pretrainæ•°æ®é›†çš„é¢„å¤„ç†æ–¹å¼ï¼Œä¸ºäº†ä¿è¯æ–‡æœ¬å®Œæ•´æ€§ï¼Œæ”¾å¼ƒé¢„å¤„ç†æˆ.binè®­ç»ƒçš„å½¢å¼ï¼ˆè½»å¾®ç‰ºç‰²è®­ç»ƒé€Ÿåº¦ï¼‰ã€‚
- ç›®å‰pretrainé¢„å¤„ç†åçš„æ–‡ä»¶å‘½åä¸ºï¼špretrain_data.csvã€‚
- åˆ é™¤äº†ä¸€äº›å†—ä½™çš„ä»£ç ã€‚

&lt;/details&gt;


&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2024-09-17&lt;/b&gt; &lt;/summary&gt;

- æ›´æ–°minimind-v1-moeæ¨¡å‹
- ä¸ºäº†é˜²æ­¢æ­§ä¹‰ï¼Œä¸å†ä½¿ç”¨mistral_tokenizeråˆ†è¯ï¼Œå…¨éƒ¨é‡‡ç”¨è‡ªå®šä¹‰çš„minimind_tokenizerä½œä¸ºåˆ†è¯å™¨ã€‚

&lt;/details&gt;


&lt;details close&gt;
&lt;summary&gt; &lt;b&gt;2024-09-01&lt;/b&gt; &lt;/summary&gt;

- æ›´æ–°minimind-v1 (108M)æ¨¡å‹ï¼Œé‡‡ç”¨minimind_tokenizerï¼Œé¢„è®­ç»ƒè½®æ¬¡3 + SFTè½®æ¬¡10ï¼Œæ›´å……åˆ†è®­ç»ƒï¼Œæ€§èƒ½æ›´å¼ºã€‚
- é¡¹ç›®å·²éƒ¨ç½²è‡³ModelScopeåˆ›ç©ºé—´ï¼Œå¯ä»¥åœ¨æ­¤ç½‘ç«™ä¸Šä½“éªŒï¼š
- [ğŸ”—ModelScopeåœ¨çº¿ä½“éªŒğŸ”—](https://www.modelscope.cn/studios/gongjy/minimind)

&lt;/details&gt;


&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2024-08-27&lt;/b&gt; &lt;/summary&gt;

- é¡¹ç›®é¦–æ¬¡å¼€æº

&lt;/details&gt;

# ğŸ“Œ å¿«é€Ÿå¼€å§‹

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;åˆ†äº«æœ¬äººçš„è½¯ç¡¬ä»¶é…ç½®ï¼ˆä»…ä¾›å‚è€ƒï¼‰&lt;/summary&gt;

* CPU: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz
* RAM: 128 GB
* GPU: NVIDIA GeForce RTX 3090(24GB) * 8
* Ubuntu==20.04
* CUDA==12.2
* Python==3.10.16
* [requirements.txt](./requirements.txt)

&lt;/details&gt;

### ç¬¬0æ­¥

```bash
git clone https://github.com/jingyaogong/minimind.git
```

## â…  æµ‹è¯•å·²æœ‰æ¨¡å‹æ•ˆæœ

### 1.ç¯å¢ƒå‡†å¤‡

```bash
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

### 2.ä¸‹è½½æ¨¡å‹
åˆ°é¡¹ç›®æ ¹ç›®å½•
```bash
git clone https://huggingface.co/jingyaogong/MiniMind2
```

### ï¼ˆå¯é€‰ï¼‰å‘½ä»¤è¡Œé—®ç­”

```bash
# load=0: load from pytorch model, load=1: load from transformers-hf model
python eval_model.py --load 1 --model_mode 2
```

### ï¼ˆå¯é€‰ï¼‰å¯åŠ¨WebUI

```bash
# å¯èƒ½éœ€è¦`python&gt;=3.10` å®‰è£… `pip install streamlit`
# cd scripts
streamlit run web_demo.py
```

### ï¼ˆå¯é€‰ï¼‰ç¬¬ä¸‰æ–¹æ¨ç†æ¡†æ¶

```bash
# ollama
ollama run jingyaogong/minimind2
# vllm
vllm serve ./MiniMind2/ --served-model-name &quot;minimind&quot;
```

## â…¡ ä»0å¼€å§‹è‡ªå·±è®­ç»ƒ

### 1.ç¯å¢ƒå‡†å¤‡

```bash
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;æ³¨ï¼šæå‰æµ‹è¯•Torchæ˜¯å¦å¯ç”¨cuda&lt;/summary&gt;

```bash
import torch
print(torch.cuda.is_available())
```

å¦‚æœä¸å¯ç”¨ï¼Œè¯·è‡ªè¡Œå»[torch_stable](https://download.pytorch.org/whl/torch_stable.html)
ä¸‹è½½whlæ–‡ä»¶å®‰è£…ã€‚å‚è€ƒ[é“¾æ¥](https://blog.csdn.net/weixin_45456738/article/details/141029610?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=%E5%AE%89%E8%A3%85torch&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-141029610.nonecase&amp;spm=1018.2226.3001.4187)

&lt;/details&gt;

### 2.æ•°æ®ä¸‹è½½

ä»ä¸‹æ–‡æä¾›çš„[æ•°æ®é›†ä¸‹è½½é“¾æ¥](https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files)
ä¸‹è½½éœ€è¦çš„æ•°æ®æ–‡ä»¶ï¼ˆåˆ›å»º`./dataset`ç›®å½•ï¼‰å¹¶æ”¾åˆ°`./dataset`ä¸‹

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;æ³¨ï¼šæ•°æ®é›†é¡»çŸ¥&lt;/summary&gt;

é»˜è®¤æ¨èä¸‹è½½`pretrain_hq.jsonl` + `sft_mini_512.jsonl`æœ€å¿«é€Ÿåº¦å¤ç°ZeroèŠå¤©æ¨¡å‹ã€‚

æ•°æ®æ–‡ä»¶å¯è‡ªç”±é€‰æ‹©ï¼Œä¸‹æ–‡æä¾›äº†å¤šç§æ­é…æ–¹æ¡ˆï¼Œå¯æ ¹æ®è‡ªå·±æ‰‹å¤´çš„è®­ç»ƒéœ€æ±‚å’ŒGPUèµ„æºè¿›è¡Œé€‚å½“ç»„åˆã€‚

&lt;/details&gt;

### 3.å¼€å§‹è®­ç»ƒ

ç›®å½•ä½äº`trainer`

**3.1 é¢„è®­ç»ƒï¼ˆå­¦çŸ¥è¯†ï¼‰**

```bash
python train_pretrain.py
```

&gt; æ‰§è¡Œé¢„è®­ç»ƒï¼Œå¾—åˆ° `pretrain_*.pth` ä½œä¸ºé¢„è®­ç»ƒçš„è¾“å‡ºæƒé‡ï¼ˆå…¶ä¸­*ä¸ºæ¨¡å‹çš„dimensionï¼Œé»˜è®¤ä¸º512ï¼‰


**3.2 ç›‘ç£å¾®è°ƒï¼ˆå­¦å¯¹è¯æ–¹å¼ï¼‰**

```bash
python train_full_sft.py
```

&gt; æ‰§è¡Œç›‘ç£å¾®è°ƒï¼Œå¾—åˆ° `full_sft_*.pth` ä½œä¸ºæŒ‡ä»¤å¾®è°ƒçš„è¾“å‡ºæƒé‡ï¼ˆå…¶ä¸­`full`å³ä¸ºå…¨å‚æ•°å¾®è°ƒï¼‰

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;æ³¨ï¼šè®­ç»ƒé¡»çŸ¥&lt;/summary&gt;

æ‰€æœ‰è®­ç»ƒè¿‡ç¨‹é»˜è®¤æ¯éš”100æ­¥ä¿å­˜1æ¬¡å‚æ•°åˆ°æ–‡ä»¶`./out/***.pth`ï¼ˆæ¯æ¬¡ä¼šè¦†ç›–æ‰æ—§æƒé‡æ–‡ä»¶ï¼‰ã€‚

ç®€å•èµ·è§ï¼Œæ­¤å¤„åªå†™æ˜ä¸¤ä¸ªé˜¶æ®µè®­ç»ƒè¿‡ç¨‹ã€‚å¦‚éœ€å…¶å®ƒè®­ç»ƒ (LoRA, è’¸é¦, å¼ºåŒ–å­¦ä¹ , å¾®è°ƒæ¨ç†ç­‰) å¯å‚è€ƒä¸‹æ–‡ã€å®éªŒã€‘å°èŠ‚çš„è¯¦ç»†è¯´æ˜ã€‚

&lt;/details&gt;


---

### 4.æµ‹è¯•æ¨¡å‹æ•ˆæœ

ç¡®ä¿éœ€è¦æµ‹è¯•çš„æ¨¡å‹`*.pth`æ–‡ä»¶ä½äº`./out/`ç›®å½•ä¸‹ã€‚
ä¹Ÿå¯ä»¥ç›´æ¥å»[æ­¤å¤„](https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch/files)ä¸‹è½½ä½¿ç”¨æˆ‘è®­ç»ƒçš„`*.pth`æ–‡ä»¶ã€‚

```bash
python eval_model.py --model_mode 1 # é»˜è®¤ä¸º0ï¼šæµ‹è¯•pretrainæ¨¡å‹æ•ˆæœï¼Œè®¾ç½®ä¸º1ï¼šæµ‹è¯•full_sftæ¨¡å‹æ•ˆæœ
```

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;æ³¨ï¼šæµ‹è¯•é¡»çŸ¥&lt;/summary&gt;

å¦‚éœ€è¯¦æƒ…ï¼ŒæŸ¥çœ‹`eval_model.py`è„šæœ¬ä»£ç å³å¯ã€‚model_modeåˆ†ä¸º 0: é¢„è®­ç»ƒæ¨¡å‹ï¼Œ1: SFT-Chatæ¨¡å‹ï¼Œ2: RLHF-Chatæ¨¡å‹ï¼Œ3: Reasonæ¨¡å‹

&lt;/details&gt;


---

&gt; [!TIP]
&gt; æ‰€æœ‰è®­ç»ƒè„šæœ¬å‡ä¸ºPytorchåŸç”Ÿæ¡†æ¶ï¼Œå‡æ”¯æŒå¤šå¡åŠ é€Ÿï¼Œå‡è®¾ä½ çš„è®¾å¤‡æœ‰N (Nï¼1) å¼ æ˜¾å¡ï¼š

å•æœºNå¡å¯åŠ¨è®­ç»ƒæ–¹å¼ (DDP, æ”¯æŒå¤šæœºå¤šå¡é›†ç¾¤)

```bash
torchrun --nproc_per_node N train_xxx.py
```

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;æ³¨ï¼šå…¶å®ƒé¡»çŸ¥&lt;/summary&gt;

å•æœºNå¡å¯åŠ¨è®­ç»ƒ (DeepSpeed)

```bash
deepspeed --master_port 29500 --num_gpus=N train_xxx.py
```

å¯æ ¹æ®éœ€è¦å¼€å¯wandbè®°å½•è®­ç»ƒè¿‡ç¨‹

```bash
# éœ€è¦ç™»å½•: wandb login
torchrun --nproc_per_node N train_xxx.py --use_wandb
# and
python train_xxx.py --use_wandb
```

é€šè¿‡æ·»åŠ `--use_wandb`å‚æ•°ï¼Œå¯ä»¥è®°å½•è®­ç»ƒè¿‡ç¨‹ï¼Œè®­ç»ƒå®Œæˆåï¼Œå¯ä»¥åœ¨wandbç½‘ç«™ä¸ŠæŸ¥çœ‹è®­ç»ƒè¿‡ç¨‹ã€‚é€šè¿‡ä¿®æ”¹`wandb_project`
å’Œ`wandb_run_name`å‚æ•°ï¼Œå¯ä»¥æŒ‡å®šé¡¹ç›®åç§°å’Œè¿è¡Œåç§°ã€‚

&lt;/details&gt;

# ğŸ“Œ æ•°æ®ä»‹ç»

## â…  Tokenizer

åˆ†è¯å™¨å°†å•è¯ä»è‡ªç„¶è¯­è¨€é€šè¿‡â€œè¯å…¸â€æ˜ å°„åˆ°`0, 1, 36`è¿™æ ·çš„æ•°å­—ï¼Œå¯ä»¥ç†è§£ä¸ºæ•°å­—å°±ä»£è¡¨äº†å•è¯åœ¨â€œè¯å…¸â€ä¸­çš„é¡µç ã€‚
å¯ä»¥é€‰æ‹©è‡ªå·±æ„é€ è¯è¡¨è®­ç»ƒä¸€ä¸ªâ€œè¯å…¸â€ï¼Œä»£ç å¯è§`./scripts/train_tokenizer.py`ï¼ˆä»…ä¾›å­¦ä¹ å‚è€ƒï¼Œè‹¥éå¿…è¦æ— éœ€å†è‡ªè¡Œè®­ç»ƒï¼ŒMiniMindå·²è‡ªå¸¦tokenizerï¼‰ã€‚
æˆ–è€…é€‰æ‹©æ¯”è¾ƒå‡ºåçš„å¼€æºå¤§æ¨¡å‹åˆ†è¯å™¨ï¼Œ
æ­£å¦‚åŒç›´æ¥ç”¨æ–°å/ç‰›æ´¥è¯å…¸çš„ä¼˜ç‚¹æ˜¯tokenç¼–ç å‹ç¼©ç‡å¾ˆå¥½ï¼Œç¼ºç‚¹æ˜¯é¡µæ•°å¤ªå¤šï¼ŒåŠ¨è¾„æ•°åä¸‡ä¸ªè¯æ±‡çŸ­è¯­ï¼›
è‡ªå·±è®­ç»ƒçš„åˆ†è¯å™¨ï¼Œä¼˜ç‚¹æ˜¯è¯è¡¨é•¿åº¦å’Œå†…å®¹éšæ„æ§åˆ¶ï¼Œç¼ºç‚¹æ˜¯å‹ç¼©ç‡å¾ˆä½ï¼ˆä¾‹å¦‚&quot;hello&quot;ä¹Ÿè®¸ä¼šè¢«æ‹†åˆ†ä¸º&quot;h e l l o&quot;
äº”ä¸ªç‹¬ç«‹çš„tokenï¼‰ï¼Œä¸”ç”Ÿåƒ»è¯éš¾ä»¥è¦†ç›–ã€‚
â€œè¯å…¸â€çš„é€‰æ‹©å›ºç„¶å¾ˆé‡è¦ï¼ŒLLMçš„è¾“å‡ºæœ¬è´¨ä¸Šæ˜¯SoftMaxåˆ°è¯å…¸Nä¸ªè¯çš„å¤šåˆ†ç±»é—®é¢˜ï¼Œç„¶åé€šè¿‡â€œè¯å…¸â€è§£ç åˆ°è‡ªç„¶è¯­è¨€ã€‚
å› ä¸ºMiniMindä½“ç§¯éœ€è¦ä¸¥æ ¼æ§åˆ¶ï¼Œä¸ºäº†é¿å…æ¨¡å‹å¤´é‡è„šè½»ï¼ˆè¯åµŒå…¥embeddingå±‚å‚æ•°åœ¨LLMå æ¯”å¤ªé«˜ï¼‰ï¼Œæ‰€ä»¥è¯è¡¨é•¿åº¦çŸ­çŸ­ç›Šå–„ã€‚

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Tokenizerä»‹ç»&lt;/summary&gt;

ç¬¬ä¸‰æ–¹å¼ºå¤§çš„å¼€æºæ¨¡å‹ä¾‹å¦‚Yiã€qwenã€chatglmã€mistralã€Llama3çš„tokenizerè¯è¡¨é•¿åº¦å¦‚ä¸‹ï¼š

&lt;table&gt;
  &lt;tr&gt;&lt;th&gt;Tokenizeræ¨¡å‹&lt;/th&gt;&lt;th&gt;è¯è¡¨å¤§å°&lt;/th&gt;&lt;th&gt;æ¥æº&lt;/th&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;yi tokenizer&lt;/td&gt;&lt;td&gt;64,000&lt;/td&gt;&lt;td&gt;01ä¸‡ç‰©ï¼ˆä¸­å›½ï¼‰&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;qwen2 tokenizer&lt;/td&gt;&lt;td&gt;151,643&lt;/td&gt;&lt;td&gt;é˜¿é‡Œäº‘ï¼ˆä¸­å›½ï¼‰&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;glm tokenizer&lt;/td&gt;&lt;td&gt;151,329&lt;/td&gt;&lt;td&gt;æ™ºè°±AIï¼ˆä¸­å›½ï¼‰&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;mistral tokenizer&lt;/td&gt;&lt;td&gt;32,000&lt;/td&gt;&lt;td&gt;Mistral AIï¼ˆæ³•å›½ï¼‰&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;llama3 tokenizer&lt;/td&gt;&lt;td&gt;128,000&lt;/td&gt;&lt;td&gt;Metaï¼ˆç¾å›½ï¼‰&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;minimind tokenizer&lt;/td&gt;&lt;td&gt;6,400&lt;/td&gt;&lt;td&gt;è‡ªå®šä¹‰&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&gt; ğŸ‘‰2024-09-17æ›´æ–°ï¼šä¸ºäº†é˜²æ­¢è¿‡å»çš„ç‰ˆæœ¬æ­§ä¹‰&amp;æ§åˆ¶ä½“ç§¯ï¼Œminimindæ‰€æœ‰æ¨¡å‹å‡ä½¿ç”¨minimind_tokenizeråˆ†è¯ï¼ŒåºŸå¼ƒæ‰€æœ‰mistral_tokenizerç‰ˆæœ¬ã€‚

```
# ä¸€äº›è‡ªè¨€è‡ªè¯­
&gt; å°½ç®¡minimind_tokenizeré•¿åº¦å¾ˆå°ï¼Œç¼–è§£ç æ•ˆç‡å¼±äºqwen2ã€glmç­‰ä¸­æ–‡å‹å¥½å‹åˆ†è¯å™¨ã€‚
&gt; ä½†minimindæ¨¡å‹é€‰æ‹©äº†è‡ªå·±è®­ç»ƒçš„minimind_tokenizerä½œä¸ºåˆ†è¯å™¨ï¼Œä»¥ä¿æŒæ•´ä½“å‚æ•°è½»é‡ï¼Œé¿å…ç¼–ç å±‚å’Œè®¡ç®—å±‚å æ¯”å¤±è¡¡ï¼Œå¤´é‡è„šè½»ï¼Œå› ä¸ºminimindçš„è¯è¡¨å¤§å°åªæœ‰6400ã€‚
&gt; ä¸”minimindåœ¨å®é™…æµ‹è¯•ä¸­æ²¡æœ‰å‡ºç°è¿‡ç”Ÿåƒ»è¯æ±‡è§£ç å¤±è´¥çš„æƒ…å†µï¼Œæ•ˆæœè‰¯å¥½ã€‚
&gt; ç”±äºè‡ªå®šä¹‰è¯è¡¨å‹ç¼©é•¿åº¦åˆ°6400ï¼Œä½¿å¾—LLMæ€»å‚æ•°é‡æœ€ä½åªæœ‰25.8Mã€‚
&gt; è®­ç»ƒæ•°æ®`tokenizer_train.jsonl`å‡æ¥è‡ªäº`åŒ æ•°å¤§æ¨¡å‹æ•°æ®é›†`ï¼Œè¿™éƒ¨åˆ†æ•°æ®ç›¸å¯¹æ¬¡è¦ï¼Œå¦‚éœ€è®­ç»ƒå¯ä»¥è‡ªç”±é€‰æ‹©ã€‚
```

&lt;/details&gt;

## â…¡ Pretrainæ•°æ®

ç»å†äº†MiniMind-V1çš„ä½è´¨é‡é¢„è®­ç»ƒæ•°æ®ï¼Œå¯¼è‡´æ¨¡å‹èƒ¡è¨€ä¹±è¯­çš„æ•™è®­ï¼Œ`2025-02-05` ä¹‹åå†³å®šä¸å†é‡‡ç”¨å¤§è§„æ¨¡æ— ç›‘ç£çš„æ•°æ®é›†åšé¢„è®­ç»ƒã€‚
è¿›è€Œå°è¯•æŠŠ[åŒ æ•°å¤§æ¨¡å‹æ•°æ®é›†](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)çš„ä¸­æ–‡éƒ¨åˆ†æå–å‡ºæ¥ï¼Œ
æ¸…æ´—å‡ºå­—ç¬¦`&lt;512`é•¿åº¦çš„å¤§çº¦1.6GBçš„è¯­æ–™ç›´æ¥æ‹¼æ¥æˆé¢„è®­ç»ƒæ•°æ® `pretrain_hq.jsonl`ï¼Œhqå³ä¸ºhigh
qualityï¼ˆå½“ç„¶ä¹Ÿè¿˜ä¸ç®—highï¼Œæå‡æ•°æ®è´¨é‡æ— æ­¢å°½ï¼‰ã€‚

æ–‡ä»¶`pretrain_hq.jsonl` æ•°æ®æ ¼å¼ä¸º

```bash
{&quot;text&quot;: &quot;å¦‚ä½•æ‰èƒ½æ‘†è„±æ‹–å»¶ç—‡ï¼Ÿ æ²»æ„ˆæ‹–å»¶ç—‡å¹¶ä¸å®¹æ˜“ï¼Œä½†ä»¥ä¸‹å»ºè®®å¯èƒ½æœ‰æ‰€å¸®åŠ©...&quot;}
```

## â…¢ SFTæ•°æ®

[åŒ æ•°å¤§æ¨¡å‹SFTæ•°æ®é›†](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)
â€œæ˜¯ä¸€ä¸ªå®Œæ•´ã€æ ¼å¼ç»Ÿä¸€ã€å®‰å…¨çš„å¤§æ¨¡å‹è®­ç»ƒå’Œç ”ç©¶èµ„æºã€‚
ä»ç½‘ç»œä¸Šçš„å…¬å¼€æ•°æ®æºæ”¶é›†å¹¶æ•´ç†äº†å¤§é‡å¼€æºæ•°æ®é›†ï¼Œå¯¹å…¶è¿›è¡Œäº†æ ¼å¼ç»Ÿä¸€ï¼Œæ•°æ®æ¸…æ´—ï¼Œ
åŒ…å«10Mæ¡æ•°æ®çš„ä¸­æ–‡æ•°æ®é›†å’ŒåŒ…å«2Mæ¡æ•°æ®çš„è‹±æ–‡æ•°æ®é›†ã€‚â€
ä»¥ä¸Šæ˜¯å®˜æ–¹ä»‹ç»ï¼Œä¸‹è½½æ–‡ä»¶åçš„æ•°æ®æ€»é‡å¤§çº¦åœ¨4B tokensï¼Œè‚¯å®šæ˜¯é€‚åˆä½œä¸ºä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹çš„SFTæ•°æ®çš„ã€‚
ä½†æ˜¯å®˜æ–¹æä¾›çš„æ•°æ®æ ¼å¼å¾ˆä¹±ï¼Œå…¨éƒ¨ç”¨æ¥sftä»£ä»·å¤ªå¤§ã€‚
æˆ‘å°†æŠŠå®˜æ–¹æ•°æ®é›†è¿›è¡Œäº†äºŒæ¬¡æ¸…æ´—ï¼ŒæŠŠå«æœ‰ç¬¦å·æ±¡æŸ“å’Œå™ªå£°çš„æ¡ç›®å»é™¤ï¼›å¦å¤–ä¾ç„¶åªä¿ç•™äº†æ€»é•¿åº¦`&lt;512`
çš„å†…å®¹ï¼Œæ­¤é˜¶æ®µå¸Œæœ›é€šè¿‡å¤§é‡å¯¹è¯è¡¥å……é¢„è®­ç»ƒé˜¶æ®µæ¬ ç¼ºçš„çŸ¥è¯†ã€‚
å¯¼å‡ºæ–‡ä»¶ä¸º`sft_512.jsonl`(~7.5GB)ã€‚

[Magpie-SFTæ•°æ®é›†](https://www.modelscope.cn/organization/Magpie-Align)
æ”¶é›†äº†~1Mæ¡æ¥è‡ªQwen2/2.5çš„é«˜è´¨é‡å¯¹è¯ï¼Œæˆ‘å°†è¿™éƒ¨åˆ†æ•°æ®è¿›ä¸€æ­¥æ¸…æ´—ï¼ŒæŠŠæ€»é•¿åº¦`&lt;2048`çš„éƒ¨åˆ†å¯¼å‡ºä¸º`sft_2048.jsonl`(~9GB)ã€‚
é•¿åº¦`&lt;1024`çš„éƒ¨åˆ†å¯¼å‡ºä¸º`sft_1024.jsonl`(~5.5GB)ï¼Œç”¨å¤§æ¨¡å‹å¯¹è¯æ•°æ®ç›´æ¥è¿›è¡Œsftå°±å±äºâ€œé»‘ç›’è’¸é¦â€çš„èŒƒç•´ã€‚

è¿›ä¸€æ­¥æ¸…æ´—å‰ä¸¤æ­¥sftçš„æ•°æ®ï¼ˆåªä¿ç•™ä¸­æ–‡å­—ç¬¦å æ¯”é«˜çš„å†…å®¹ï¼‰ï¼Œç­›é€‰é•¿åº¦`&lt;512`çš„å¯¹è¯ï¼Œå¾—åˆ°`sft_mini_512.jsonl`(~1.2GB)ã€‚

æ‰€æœ‰sftæ–‡ä»¶ `sft_X.jsonl` æ•°æ®æ ¼å¼å‡ä¸º

```text
{
    &quot;conversations&quot;: [
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;ä½ å¥½&quot;},
        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;ä½ å¥½ï¼&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;å†è§&quot;},
        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;å†è§ï¼&quot;}
    ]
}
```

## â…£ RLHFæ•°æ®

æ¥è‡ª[Magpie-DPOæ•°æ®é›†](https://www.modelscope.cn/datasets/Magpie-Align/MagpieLM-DPO-Data-v0.1)
å¤§çº¦200kæ¡åå¥½æ•°æ®ï¼ˆå‡æ˜¯è‹±æ–‡ï¼‰ç”Ÿæˆè‡ªLlama3.1-70B/8Bï¼Œå¯ä»¥ç”¨äºè®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œä¼˜åŒ–æ¨¡å‹å›å¤è´¨é‡ï¼Œä½¿å…¶æ›´åŠ ç¬¦åˆäººç±»åå¥½ã€‚
è¿™é‡Œå°†æ•°æ®æ€»é•¿åº¦`&lt;3000`çš„å†…å®¹é‡ç»„ä¸º`dpo.jsonl`(~0.9GB)ï¼ŒåŒ…å«`chosen`å’Œ`rejected`ä¸¤ä¸ªå­—æ®µï¼Œ`chosen`
ä¸ºåå¥½çš„å›å¤ï¼Œ`rejected`ä¸ºæ‹’ç»çš„å›å¤ã€‚

æ–‡ä»¶ `dpo.jsonl` æ•°æ®æ ¼å¼ä¸º

```text
{
  &quot;chosen&quot;: [
    {&quot;content&quot;: &quot;Q&quot;, &quot;role&quot;: &quot;user&quot;}, 
    {&quot;content&quot;: &quot;good answer&quot;, &quot;role&quot;: &quot;assistant&quot;}
  ], 
  &quot;rejected&quot;: [
    {&quot;content&quot;: &quot;Q&quot;, &quot;role&quot;: &quot;user&quot;}, 
    {&quot;content&quot;: &quot;bad answer&quot;, &quot;role&quot;: &quot;assistant&quot;}
  ]
}
```

## â…¤ Reasonæ•°æ®é›†ï¼š

ä¸å¾—ä¸è¯´2025å¹´2æœˆè°èƒ½ç«çš„è¿‡DeepSeek...
ä¹Ÿæ¿€å‘äº†æˆ‘å¯¹RLå¼•å¯¼çš„æ¨ç†æ¨¡å‹çš„æµ“åšå…´è¶£ï¼Œç›®å‰å·²ç»ç”¨Qwen2.5å¤ç°äº†R1-Zeroã€‚
å¦‚æœæœ‰æ—¶é—´+æ•ˆæœworkï¼ˆä½†99%åŸºæ¨¡èƒ½åŠ›ä¸è¶³ï¼‰æˆ‘ä¼šåœ¨ä¹‹åæ›´æ–°MiniMindåŸºäºRLè®­ç»ƒçš„æ¨ç†æ¨¡å‹è€Œä¸æ˜¯è’¸é¦æ¨¡å‹ã€‚
æ—¶é—´æœ‰é™ï¼Œæœ€å¿«çš„ä½æˆæœ¬æ–¹æ¡ˆä¾ç„¶æ˜¯ç›´æ¥è’¸é¦ï¼ˆé»‘ç›’æ–¹å¼ï¼‰ã€‚
è€ä¸ä½R1å¤ªç«ï¼ŒçŸ­çŸ­å‡ å¤©å°±å·²ç»å­˜åœ¨ä¸€äº›R1çš„è’¸é¦æ•°æ®é›†[R1-Llama-70B](https://www.modelscope.cn/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B)ã€[R1-Distill-SFT](https://www.modelscope.cn/datasets/AI-ModelScope/R1-Distill-SFT)ã€
[Alpaca-Distill-R1](https://huggingface.co/datasets/shareAI/Alpaca-Distill-R1-ZH)ã€
[deepseek_r1_zh](https://huggingface.co/datasets/jinliuxi/deepseek_r1_zh)ç­‰ç­‰ï¼Œçº¯ä¸­æ–‡çš„æ•°æ®å¯èƒ½æ¯”è¾ƒå°‘ã€‚
æœ€ç»ˆæ•´åˆå®ƒä»¬ï¼Œå¯¼å‡ºæ–‡ä»¶ä¸º`r1_mix_1024.jsonl`ï¼Œæ•°æ®æ ¼å¼å’Œ`sft_X.jsonl`ä¸€è‡´ã€‚

## â…¥ æ›´å¤šæ•°æ®é›†

ç›®å‰å·²ç»æœ‰[HqWu-HITCS/Awesome-Chinese-LLM](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM)
åœ¨æ”¶é›†å’Œæ¢³ç†ä¸­æ–‡LLMç›¸å…³çš„å¼€æºæ¨¡å‹ã€åº”ç”¨ã€æ•°æ®é›†åŠæ•™ç¨‹ç­‰èµ„æ–™ï¼Œå¹¶æŒç»­æ›´æ–°è¿™æ–¹é¢çš„æœ€æ–°è¿›å±•ã€‚å…¨é¢ä¸”ä¸“ä¸šï¼ŒRespectï¼

---

## â…§ MiniMindè®­ç»ƒæ•°æ®é›†

&gt; [!NOTE]
&gt; 2025-02-05åï¼Œå¼€æºMiniMindæœ€ç»ˆè®­ç»ƒæ‰€ç”¨çš„æ‰€æœ‰æ•°æ®é›†ï¼Œå› æ­¤æ— éœ€å†è‡ªè¡Œé¢„å¤„ç†å¤§è§„æ¨¡æ•°æ®é›†ï¼Œé¿å…é‡å¤æ€§çš„æ•°æ®å¤„ç†å·¥ä½œã€‚

MiniMindè®­ç»ƒæ•°æ®é›†ä¸‹è½½åœ°å€ï¼š [ModelScope](https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files) | [HuggingFace](https://huggingface.co/datasets/jingyaogong/minimind_dataset/tree/main)

&gt; æ— éœ€å…¨éƒ¨cloneï¼Œå¯å•ç‹¬ä¸‹è½½æ‰€éœ€çš„æ–‡ä»¶

å°†ä¸‹è½½çš„æ•°æ®é›†æ–‡ä»¶æ”¾åˆ°`./dataset/`ç›®å½•ä¸‹ï¼ˆâœ¨ä¸ºæ¨èçš„å¿…é¡»é¡¹ï¼‰

```bash
./dataset/
â”œâ”€â”€ dpo.jsonl (909MB)
â”œâ”€â”€ lora_identity.jsonl (22.8KB)
â”œâ”€â”€ lora_medical.jsonl (34MB)
â”œâ”€â”€ pretrain_hq.jsonl (1.6GB, âœ¨)
â”œâ”€â”€ r1_mix_1024.jsonl (340MB)
â”œâ”€â”€ sft_1024.jsonl (5.6GB)
â”œâ”€â”€ sft_2048.jsonl (9GB)
â”œâ”€â”€ sft_512.jsonl (7.5GB)
â”œâ”€â”€ sft_mini_512.jsonl (1.2GB, âœ¨)
â””â”€â”€ tokenizer_train.jsonl (1GB)
```

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;æ³¨ï¼šå„æ•°æ®é›†ç®€ä»‹&lt;/summary&gt;

* `dpo.jsonl` --RLHFé˜¶æ®µæ•°æ®é›†
* `lora_identity.jsonl` --è‡ªæˆ‘è®¤çŸ¥æ•°æ®é›†ï¼ˆä¾‹å¦‚ï¼šä½ æ˜¯è°ï¼Ÿæˆ‘æ˜¯minimind...ï¼‰ï¼Œæ¨èç”¨äºloraè®­ç»ƒï¼ˆäº¦å¯ç”¨äºå…¨å‚SFTï¼Œå‹¿è¢«åå­—å±€é™ï¼‰
* `lora_medical.jsonl` --åŒ»ç–—é—®ç­”æ•°æ®é›†ï¼Œæ¨èç”¨äºloraè®­ç»ƒï¼ˆäº¦å¯ç”¨äºå…¨å‚SFTï¼Œå‹¿è¢«åå­—å±€é™ï¼‰
* `pretrain_hq.jsonl`âœ¨ --é¢„è®­ç»ƒæ•°æ®é›†ï¼Œæ•´åˆè‡ªjiangshuç§‘æŠ€
* `r1_mix_1024.jsonl` --DeepSeek-R1-1.5Bè’¸é¦æ•°æ®ï¼Œæ¯æ¡æ•°æ®å­—ç¬¦æœ€å¤§é•¿åº¦ä¸º1024ï¼ˆå› æ­¤è®­ç»ƒæ—¶è®¾ç½®max_seq_len=1024ï¼‰
* `sft_1024.jsonl` --æ•´åˆè‡ªQwen2.5è’¸é¦æ•°æ®ï¼ˆæ˜¯sft_2048çš„å­é›†ï¼‰ï¼Œæ¯æ¡æ•°æ®å­—ç¬¦æœ€å¤§é•¿åº¦ä¸º1024ï¼ˆå› æ­¤è®­ç»ƒæ—¶è®¾ç½®max_seq_len=1024ï¼‰
* `sft_2048.jsonl` --æ•´åˆè‡ªQwen2.5è’¸é¦æ•°æ®ï¼Œæ¯æ¡æ•°æ®å­—ç¬¦æœ€å¤§é•¿åº¦ä¸º2048ï¼ˆå› æ­¤è®­ç»ƒæ—¶è®¾ç½®max_seq_len=2048ï¼‰
* `sft_512.jsonl` --æ•´åˆè‡ªåŒ æ•°ç§‘æŠ€SFTæ•°æ®ï¼Œæ¯æ¡æ•°æ®å­—ç¬¦æœ€å¤§é•¿åº¦ä¸º512ï¼ˆå› æ­¤è®­ç»ƒæ—¶è®¾ç½®max_seq_len=512ï¼‰
* `sft_mini_512.jsonl`âœ¨ --æç®€æ•´åˆè‡ªåŒ æ•°ç§‘æŠ€SFTæ•°æ®+Qwen2.5è’¸é¦æ•°æ®ï¼ˆç”¨äºå¿«é€Ÿè®­ç»ƒZeroæ¨¡å‹ï¼‰ï¼Œæ¯æ¡æ•°æ®å­—ç¬¦æœ€å¤§é•¿åº¦ä¸º512ï¼ˆå› æ­¤è®­ç»ƒæ—¶è®¾ç½®max_seq_len=512ï¼‰
* `tokenizer_train.jsonl` --å‡æ¥è‡ªäº`åŒ æ•°å¤§æ¨¡å‹æ•°æ®é›†`ï¼Œè¿™éƒ¨åˆ†æ•°æ®ç›¸å¯¹æ¬¡è¦ï¼Œï¼ˆä¸æ¨èè‡ªå·±é‡å¤è®­ç»ƒtokenizerï¼Œç†ç”±å¦‚ä¸Šï¼‰å¦‚éœ€è‡ªå·±è®­ç»ƒtokenizerå¯ä»¥è‡ªç”±é€‰æ‹©æ•°æ®é›†ã€‚

&lt;/details&gt;


![dataset](./images/dataset.jpg)

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;è¯´æ˜ &amp; æ¨èè®­ç»ƒæ–¹æ¡ˆ&lt;/summary&gt;

* MiniMind2 Serieså‡ç»è¿‡å…±çº¦20GBè¯­æ–™è®­ç»ƒï¼Œå¤§çº¦4B tokensï¼Œå³å¯¹åº”ä¸Šé¢çš„æ•°æ®ç»„åˆè®­ç»ƒç»“æœï¼ˆå¼€é”€ï¼šğŸ’°ğŸ’°ğŸ’°ğŸ’°ğŸ’°ğŸ’°ğŸ’°ğŸ’°ï¼Œæ•ˆæœï¼šğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜Šï¼‰

* æƒ³è¦æœ€å¿«é€Ÿåº¦ä»0å®ç°Zeroæ¨¡å‹ï¼Œæ¨èä½¿ç”¨`pretrain_hq.jsonl` + `sft_mini_512.jsonl` çš„æ•°æ®ç»„åˆï¼Œå…·ä½“èŠ±é”€å’Œæ•ˆæœå¯æŸ¥çœ‹ä¸‹æ–‡è¡¨æ ¼ï¼ˆå¼€é”€ï¼šğŸ’°ï¼Œæ•ˆæœï¼šğŸ˜ŠğŸ˜Šï¼‰

* æ¨èå…·å¤‡ä¸€å®šç®—åŠ›èµ„æºæˆ–æ›´åœ¨æ„æ•ˆæœçš„æœ‹å‹å¯ä»¥è€ƒè™‘å‰è€…å®Œæ•´å¤ç°MiniMind2ï¼›ä»…æœ‰å•å¡GPUæˆ–åœ¨ä¹çŸ­æ—¶é—´å¿«é€Ÿå¤ç°çš„æœ‹å‹å¼ºçƒˆæ¨èåè€…ï¼›

* ã€æŠ˜ä¸­æ–¹æ¡ˆã€‘äº¦å¯é€‰æ‹©ä¾‹å¦‚`sft_mini_512.jsonl`ã€`sft_1024.jsonl`ä¸­ç­‰è§„æ¨¡æ•°æ®è¿›è¡Œè‡ªç”±ç»„åˆè®­ç»ƒï¼ˆå¼€é”€ï¼šğŸ’°ğŸ’°ğŸ’°ï¼Œæ•ˆæœï¼šğŸ˜ŠğŸ˜ŠğŸ˜ŠğŸ˜Šï¼‰ã€‚

&lt;/details&gt;

# ğŸ“Œ Model Structure

MiniMind-Denseï¼ˆå’Œ[Llama3.1](https://ai.meta.com/blog/meta-llama-3-1/)ä¸€æ ·ï¼‰ä½¿ç”¨äº†Transformerçš„Decoder-Onlyç»“æ„ï¼Œè·ŸGPT-3çš„åŒºåˆ«åœ¨äºï¼š

* é‡‡ç”¨äº†GPT-3çš„é¢„æ ‡å‡†åŒ–æ–¹æ³•ï¼Œä¹Ÿå°±æ˜¯åœ¨æ¯ä¸ªTransformerå­å±‚çš„è¾“å…¥ä¸Šè¿›è¡Œå½’ä¸€åŒ–ï¼Œè€Œä¸æ˜¯åœ¨è¾“å‡ºä¸Šã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨çš„æ˜¯RMSNormå½’ä¸€åŒ–å‡½æ•°ã€‚
* ç”¨SwiGLUæ¿€æ´»å‡½æ•°æ›¿ä»£äº†ReLUï¼Œè¿™æ ·åšæ˜¯ä¸ºäº†æé«˜æ€§èƒ½ã€‚
* åƒGPT-Neoä¸€æ ·ï¼Œå»æ‰äº†ç»å¯¹ä½ç½®åµŒå…¥ï¼Œæ”¹ç”¨äº†æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰ï¼Œè¿™æ ·åœ¨å¤„ç†è¶…å‡ºè®­ç»ƒé•¿åº¦çš„æ¨ç†æ—¶æ•ˆæœæ›´å¥½ã€‚

---

MiniMind-MoEæ¨¡å‹ï¼Œå®ƒçš„ç»“æ„åŸºäºLlama3å’Œ[Deepseek-V2/3](https://arxiv.org/pdf/2405.04434)ä¸­çš„MixFFNæ··åˆä¸“å®¶æ¨¡å—ã€‚

* DeepSeek-V2åœ¨å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰æ–¹é¢ï¼Œé‡‡ç”¨äº†æ›´ç»†ç²’åº¦çš„ä¸“å®¶åˆ†å‰²å’Œå…±äº«çš„ä¸“å®¶éš”ç¦»æŠ€æœ¯ï¼Œä»¥æé«˜Expertsçš„æ•ˆæœã€‚

---

MiniMindçš„æ•´ä½“ç»“æ„ä¸€è‡´ï¼Œåªæ˜¯åœ¨RoPEè®¡ç®—ã€æ¨ç†å‡½æ•°å’ŒFFNå±‚çš„ä»£ç ä¸Šåšäº†ä¸€äº›å°è°ƒæ•´ã€‚
å…¶ç»“æ„å¦‚ä¸‹å›¾ï¼ˆé‡ç»˜ç‰ˆï¼‰ï¼š

![structure](./images/LLM-structure.png)
![structure-moe](./images/LLM-structure-moe.png)

ä¿®æ”¹æ¨¡å‹é…ç½®è§[./model/LMConfig.py](./model/LMConfig.py)ã€‚
å‚è€ƒæ¨¡å‹å‚æ•°ç‰ˆæœ¬è§ä¸‹è¡¨ï¼š

| Model Name        | params | len_vocab | rope_theta | n_layers | d_model | kv_heads | q_heads | share+route |
|-------------------|--------|-----------|------------|----------|---------|----------|---------|-------------|
| MiniMind2-Small   | 26M    | 6400      | 1e6        | 8        | 512     | 2        | 8       | -           |
| MiniMind2-MoE     | 145M   | 6400      | 1e6        | 8        | 640     | 2        | 8       | 1+4         |
| MiniMind2         | 104M   | 6400      | 1e6        | 16       | 768     | 2        | 8       | -           |
| minimind-v1-small | 26M    | 6400      | 1e4        | 8        | 512     | 8        | 16      | -           |
| minimind-v1-moe   | 4Ã—26M  | 6400      | 1e4        | 8        | 512     | 8        | 16      | 1+4         |
| minimind-v1       | 108M   | 6400      | 1e4        | 16       | 768     | 8        | 16      | -           |

# ğŸ“Œ Experiment

## â…  è®­ç»ƒå¼€é”€

- **æ—¶é—´å•ä½**ï¼šå°æ—¶ (h)ã€‚
- **æˆæœ¬å•ä½**ï¼šäººæ°‘å¸ (ï¿¥)ï¼›7ï¿¥ â‰ˆ 1ç¾å…ƒã€‚
- **3090 ç§Ÿå¡å•ä»·**ï¼šâ‰ˆ1.3ï¿¥/hï¼ˆå¯è‡ªè¡Œå‚è€ƒå®æ—¶å¸‚ä»·ï¼‰ã€‚
- **å‚è€ƒæ ‡å‡†**ï¼šè¡¨æ ¼ä»…å®æµ‹ `pretrain` å’Œ `sft_mini_512` ä¸¤ä¸ªæ•°æ®é›†çš„è®­ç»ƒæ—¶é—´ï¼Œå…¶å®ƒè€—æ—¶æ ¹æ®æ•°æ®é›†å¤§å°ä¼°ç®—ï¼ˆå¯èƒ½å­˜åœ¨äº›è®¸å‡ºå…¥ï¼‰ã€‚

&gt; åŸºäº 3090 ï¼ˆå•å¡ï¼‰æˆæœ¬è®¡ç®—

| Model Name      | params | pretrain         | sft_mini_512     | sft_512       | sft_1024          | sft_2048         | RLHF          |
|-----------------|--------|------------------|------------------|---------------|-------------------|------------------|---------------|
| MiniMind2-Small | 26M    | â‰ˆ1.1h&lt;br/&gt;â‰ˆ1.43ï¿¥ | â‰ˆ1h&lt;br/&gt;â‰ˆ1.3ï¿¥    | â‰ˆ6h&lt;br/&gt;â‰ˆ7.8ï¿¥ | â‰ˆ4.58h&lt;br/&gt;â‰ˆ5.95ï¿¥ | â‰ˆ7.5h&lt;br/&gt;â‰ˆ9.75ï¿¥ | â‰ˆ1h&lt;br/&gt;â‰ˆ1.3ï¿¥ |
| MiniMind2       | 104M   | â‰ˆ3.9h&lt;br/&gt;â‰ˆ5.07ï¿¥ | â‰ˆ3.3h&lt;br/&gt;â‰ˆ4.29ï¿¥ | â‰ˆ20h&lt;br/&gt;â‰ˆ26ï¿¥ | â‰ˆ15h&lt;br/&gt;â‰ˆ19.5ï¿¥   | â‰ˆ25h&lt;br/&gt;â‰ˆ32.5ï¿¥  | â‰ˆ3h&lt;br/&gt;â‰ˆ3.9ï¿¥ |

---

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;è®­ç»ƒå¼€é”€æ€»ç»“&amp;é¢„æµ‹&lt;/summary&gt;


&gt; MiniMind2-Smallå‚æ•°
&gt;&gt; `pretrain_hq`+`sft_mini_512`æ•°æ®é›†
&lt;br/&gt;å•å¡3090 (1 epoch) + 2.1å°æ—¶ + èŠ±è´¹2.73å…ƒäººæ°‘å¸
&lt;br/&gt;å³å¯ä»0è®­ç»ƒå‡ºMiniMind-Zero-0.025Bæ¨¡å‹!!!

&gt; MiniMind2-Smallå‚æ•°
&gt;&gt; `pretrain_hq`+`sft_512`+`sft_2048`+`dpo`æ•°æ®é›†
&lt;br/&gt;å•å¡3090 (2 epochs) + å¤§çº¦38.16å°æ—¶ + èŠ±è´¹49.61å…ƒäººæ°‘å¸
&lt;br/&gt;å³å¯ä»0è®­ç»ƒå‡ºMiniMind2-Small-0.025Bæ¨¡å‹!!!

&gt; MiniMind2å‚æ•°
&gt;&gt; `pretrain_hq`+`sft_512`+`sft_2048`+`dpo`æ•°æ®é›†
&lt;br/&gt;å•å¡3090 (2 epochs) + å¤§çº¦122å°æ—¶ + èŠ±è´¹158.6å…ƒäººæ°‘å¸
&lt;br/&gt;å³å¯ä»0è®­ç»ƒå‡ºMiniMind2-0.1Bæ¨¡å‹!!!

&lt;/details&gt;



âœ¨åŸºäºå•å¡NVIDIA 3090çš„`MiniMind-Zero`ä»0è®­ç»ƒä»…éœ€`2å°æ—¶` + `3å—é’±`ï¼Œå®ç°ChatBotæ•ˆæœï¼

âœ¨PSï¼šè‹¥é‡‡ç”¨8å¡4090è®­ç»ƒï¼Œæ€»ç”¨æ—¶ç”šè‡³å¯ä»¥å‹ç¼©åˆ°10åˆ†é’Ÿä»¥å†…ï¼ï¼ˆç”±äºæ—¶é—´æ›´çŸ­ï¼ŒèŠ±è´¹åŒæ ·3å…ƒå·¦å³ï¼Œä¸å•å¡æˆæœ¬ç›¸å½“ï¼‰

âœ¨ä»¥æä½æä½çš„é—¨æ§›ï¼Œå®ç°äººäººå¯ç©çš„å¤§æ¨¡å‹è‡ªç”±ï¼è¿™æ­£æ˜¯MiniMindç³»åˆ—çš„è¯ç”Ÿåˆè¡·ï¼

âœ¨ä»…ä»·å€¼`3å—é’±`æˆæœ¬çš„`MiniMind-Zero`å¹¶ä¸æ˜¯å™±å¤´ï¼Chatæµ‹è¯•ï¼š

```text
ğŸ‘¶: è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚
ğŸ¤–ï¸: ä½œä¸ºäººå·¥æ™ºèƒ½ï¼Œæˆ‘æ²¡æœ‰å®é™…çš„ç”Ÿæ´»ï¼Œä¹Ÿæ²¡æœ‰è‡ªæˆ‘æ„è¯†ï¼Œæ‰€ä»¥æ²¡æœ‰è‡ªå·±çš„ç”Ÿæ´»ã€‚æˆ‘è¢«è®¾è®¡æˆèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·è§£ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯ã€è¿›è¡Œå¯¹è¯ç­‰ã€‚æˆ‘çš„è®¾è®¡å’ŒåŠŸèƒ½æ˜¯ç”±è®¡ç®—æœºç§‘å­¦ã€æœºå™¨å­¦ä¹ ã€äººå·¥æ™ºèƒ½æŠ€æœ¯å’Œç®—æ³•æ‰€æ„å»ºçš„ï¼Œæˆ‘é€šè¿‡ç¼–ç¨‹å’Œç®—æ³•å®ç°ã€‚

ğŸ‘¶: æ¨èä¸€äº›æ­å·çš„ç‰¹è‰²ç¾é£Ÿå§ã€‚
ğŸ¤–ï¸: æ­å·æœ‰å¾ˆå¤šç¾å‘³çš„ç‰¹è‰²ç¾é£Ÿï¼Œæ¯”å¦‚é¸¡è‚‰ä¸²ã€é³—é±¼å¤´ã€æ°´ç…®é±¼ã€è±†è…è„‘ç­‰ã€‚è¿™äº›ç¾é£Ÿéƒ½éå¸¸ç¾å‘³ï¼Œæœ‰ç€ç‹¬ç‰¹çš„é£å‘³å’Œå£æ„Ÿï¼Œç»å¯¹æ˜¯å€¼å¾—ä¸€è¯•çš„ç¾é£Ÿã€‚

ğŸ‘¶: è¯·ä¸ºæˆ‘è®²è§£â€œå¤§è¯­è¨€æ¨¡å‹â€è¿™ä¸ªæ¦‚å¿µã€‚
ğŸ¤–ï¸: å¤§è¯­è¨€æ¨¡å‹æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œå®ƒä½¿ç”¨å¤§é‡çš„æ–‡æœ¬æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ï¼Œç„¶ååœ¨ä¸æ–­çš„è¿­ä»£ä¸­ä¸æ–­ä¼˜åŒ–æ¨¡å‹ã€‚å®ƒçš„ç›®æ ‡æ˜¯æ ¹æ®è¾“å…¥çš„æ•°æ®æ¥ç”Ÿæˆç¬¦åˆç‰¹å®šéœ€æ±‚çš„æ–‡æœ¬ï¼Œè¿™äº›æ–‡æœ¬å¯ä»¥æ˜¯ä»»ä½•ä½ æ„Ÿå…´è¶£çš„ä¸»é¢˜ã€‚

ğŸ‘¶: Introduce the history of the United States, please.
ğŸ¤–ï¸: æ‚¨æåˆ°çš„â€œIntrook&#039;s the believeations of theument.&quot; è¿™ä¸ªåå­—æ¥æºäºä¸­å›½å¤ä»£çš„&quot;groty of of the change.&quot;
```

æé€Ÿä¸”åˆå…·æ•ˆæœï¼Œç”šè‡³ä»ç„¶å¯ä»¥è¿›ä¸€æ­¥å‹ç¼©è·å–æ›´å°æ›´ä¼˜è´¨çš„è®­ç»ƒæ•°æ®ã€‚
Zeroæ¨¡å‹æƒé‡ä¿å­˜ä¸º `full_sft_512_zero.pth`ï¼ˆè§ä¸‹æ–‡MiniMindæ¨¡å‹æ–‡ä»¶é“¾æ¥ï¼‰ï¼Œå¦‚æœ‰å…´è¶£å¯ä¸‹è½½æ£€éªŒæ­¤æ¨¡å‹æ•ˆæœã€‚


---

## â…¡ ä¸»è¦è®­ç»ƒæ­¥éª¤

&gt; æ‰€æœ‰è®­ç»ƒè„šæœ¬å‡ `cd ./trainer` ç›®å½•æ‰§è¡Œ

### **1. é¢„è®­ç»ƒ(Pretrain)**:

LLMé¦–å…ˆè¦å­¦ä¹ çš„å¹¶éç›´æ¥ä¸äººäº¤æµï¼Œè€Œæ˜¯è®©ç½‘ç»œå‚æ•°ä¸­å……æ»¡çŸ¥è¯†çš„å¢¨æ°´ï¼Œâ€œå¢¨æ°´â€ ç†è®ºä¸Šå–çš„è¶Šé¥±è¶Šå¥½ï¼Œäº§ç”Ÿå¤§é‡çš„å¯¹ä¸–ç•Œçš„çŸ¥è¯†ç§¯ç´¯ã€‚
é¢„è®­ç»ƒå°±æ˜¯è®©Modelå…ˆåŸ‹å¤´è‹¦å­¦å¤§é‡åŸºæœ¬çš„çŸ¥è¯†ï¼Œä¾‹å¦‚ä»Wikiç™¾ç§‘ã€æ–°é—»ã€ä¹¦ç±æ•´ç†å¤§è§„æ¨¡çš„é«˜è´¨é‡è®­ç»ƒæ•°æ®ã€‚
è¿™ä¸ªè¿‡ç¨‹æ˜¯â€œæ— ç›‘ç£â€çš„ï¼Œå³äººç±»ä¸éœ€è¦åœ¨è¿‡ç¨‹ä¸­åšä»»ä½•â€œæœ‰ç›‘ç£â€çš„æ ¡æ­£ï¼Œè€Œæ˜¯ç”±æ¨¡å‹è‡ªå·±ä»å¤§é‡æ–‡æœ¬ä¸­æ€»ç»“è§„å¾‹å­¦ä¹ çŸ¥è¯†ç‚¹ã€‚
æ¨¡å‹æ­¤é˜¶æ®µç›®çš„åªæœ‰ä¸€ä¸ªï¼š**å­¦ä¼šè¯è¯­æ¥é¾™**ã€‚ä¾‹å¦‚æˆ‘ä»¬è¾“å…¥â€œç§¦å§‹çš‡â€å››ä¸ªå­—ï¼Œå®ƒå¯ä»¥æ¥é¾™â€œæ˜¯ä¸­å›½çš„ç¬¬ä¸€ä½çš‡å¸â€ã€‚

```bash
torchrun --nproc_per_node 1 train_pretrain.py # 1å³ä¸ºå•å¡è®­ç»ƒï¼Œå¯æ ¹æ®ç¡¬ä»¶æƒ…å†µè‡ªè¡Œè°ƒæ•´ (è®¾ç½®&gt;=2)
# or
python train_pretrain.py
```

&gt; 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[getsentry/sentry]]></title>
            <link>https://github.com/getsentry/sentry</link>
            <guid>https://github.com/getsentry/sentry</guid>
            <pubDate>Fri, 23 May 2025 00:04:14 GMT</pubDate>
            <description><![CDATA[Developer-first error tracking and performance monitoring]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/getsentry/sentry">getsentry/sentry</a></h1>
            <p>Developer-first error tracking and performance monitoring</p>
            <p>Language: Python</p>
            <p>Stars: 40,913</p>
            <p>Forks: 4,346</p>
            <p>Stars today: 17 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://sentry.io/?utm_source=github&amp;utm_medium=logo&quot; target=&quot;_blank&quot;&gt;
      &lt;img src=&quot;https://sentry-brand.storage.googleapis.com/sentry-wordmark-dark-280x84.png&quot; alt=&quot;Sentry&quot; width=&quot;280&quot; height=&quot;84&quot; /&gt;
    &lt;/a&gt;
  &lt;/p&gt;
  &lt;p align=&quot;center&quot;&gt;
    Users and logs provide clues. Sentry provides answers.
  &lt;/p&gt;
&lt;/p&gt;

# What&#039;s Sentry?

Sentry is a developer-first error tracking and performance monitoring platform that helps developers see what actually matters, solve quicker, and learn continuously about their applications.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/projects.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/issue-details.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/transaction-summary.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/releases.png&quot; width=&quot;270&quot; /&gt;
&lt;/p&gt;

## Official Sentry SDKs

- [JavaScript](https://github.com/getsentry/sentry-javascript)
- [Electron](https://github.com/getsentry/sentry-electron/)
- [React-Native](https://github.com/getsentry/sentry-react-native)
- [Python](https://github.com/getsentry/sentry-python)
- [Ruby](https://github.com/getsentry/sentry-ruby)
- [PHP](https://github.com/getsentry/sentry-php)
- [Laravel](https://github.com/getsentry/sentry-laravel)
- [Go](https://github.com/getsentry/sentry-go)
- [Rust](https://github.com/getsentry/sentry-rust)
- [Java/Kotlin](https://github.com/getsentry/sentry-java)
- [Objective-C/Swift](https://github.com/getsentry/sentry-cocoa)
- [C\#/F\#](https://github.com/getsentry/sentry-dotnet)
- [C/C++](https://github.com/getsentry/sentry-native)
- [Dart](https://github.com/getsentry/sentry-dart)
- [Perl](https://github.com/getsentry/perl-raven)
- [Clojure](https://github.com/getsentry/sentry-clj/)
- [Elixir](https://github.com/getsentry/sentry-elixir)
- [Unity](https://github.com/getsentry/sentry-unity)
- [Unreal Engine](https://github.com/getsentry/sentry-unreal)
- [PowerShell](https://github.com/getsentry/sentry-powershell)

# Resources

- [Documentation](https://docs.sentry.io/)
- [Discussions](https://github.com/getsentry/sentry/discussions) (Bugs, feature requests,
  general questions)
- [Discord](https://discord.gg/PXa5Apfe7K)
- [Contributing](https://docs.sentry.io/internal/contributing/)
- [Bug Tracker](https://github.com/getsentry/sentry/issues)
- [Code](https://github.com/getsentry/sentry)
- [Transifex](https://www.transifex.com/getsentry/sentry/) (Translate
  Sentry\!)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>