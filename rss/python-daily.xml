<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Tue, 03 Feb 2026 00:07:50 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[OpenBMB/ChatDev]]></title>
            <link>https://github.com/OpenBMB/ChatDev</link>
            <guid>https://github.com/OpenBMB/ChatDev</guid>
            <pubDate>Tue, 03 Feb 2026 00:07:50 GMT</pubDate>
            <description><![CDATA[ChatDev 2.0: Dev All through LLM-powered Multi-Agent Collaboration]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenBMB/ChatDev">OpenBMB/ChatDev</a></h1>
            <p>ChatDev 2.0: Dev All through LLM-powered Multi-Agent Collaboration</p>
            <p>Language: Python</p>
            <p>Stars: 29,373</p>
            <p>Forks: 3,664</p>
            <p>Stars today: 93 stars today</p>
            <h2>README</h2><pre># ChatDev 2.0 - DevAll

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;frontend/public/media/logo.png&quot; alt=&quot;DevAll Logo&quot; width=&quot;500&quot;/&gt;
&lt;/p&gt;


&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;A Zero-Code Multi-Agent Platform for Developing Everything&lt;/strong&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  „Äê&lt;a href=&quot;./README.md&quot;&gt;English&lt;/a&gt; | &lt;a href=&quot;./README-zh.md&quot;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;„Äë
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    „Äêüìö &lt;a href=&quot;#developers&quot;&gt;Developers&lt;/a&gt; | üë• &lt;a href=&quot;#primary-contributors&quot;&gt;Contributors&lt;/a&gt;ÔΩú‚≠êÔ∏è &lt;a href=&quot;https://github.com/OpenBMB/ChatDev/tree/chatdev1.0&quot;&gt;ChatDev 1.0 (Legacy)&lt;/a&gt;„Äë
&lt;/p&gt;

## üìñ Overview
ChatDev has evolved from a specialized software development multi-agent system into a comprehensive multi-agent orchestration platform.

- &lt;a href=&quot;https://github.com/OpenBMB/ChatDev/tree/main&quot;&gt;**ChatDev 2.0 (DevAll)**&lt;/a&gt; is a **Zero-Code Multi-Agent Platform** for &quot;Developing Everything&quot;. It empowers users to rapidly build and execute customized multi-agent systems through simple configuration. No coding is required‚Äîusers can define agents, workflows, and tasks to orchestrate complex scenarios such as data visualization, 3D generation, and deep research.
- &lt;a href=&quot;https://github.com/OpenBMB/ChatDev/tree/chatdev1.0&quot;&gt;**ChatDev 1.0 (Legacy)**&lt;/a&gt; operates as a **Virtual Software Company**. It utilizes various intelligent agents (e.g., CEO, CTO, Programmer) participating in specialized functional seminars to automate the entire software development life cycle‚Äîincluding designing, coding, testing, and documenting. It serves as the foundational paradigm for communicative agent collaboration.

## üéâ News
‚Ä¢ **Jan 07, 2026: üöÄ We are excited to announce the official release of ChatDev 2.0 (DevAll)!** This version introduces a zero-code multi-agent orchestration platform. The classic ChatDev (v1.x) has been moved to the [`chatdev1.0`](https://github.com/OpenBMB/ChatDev/tree/chatdev1.0) branch for maintenance. More details about ChatDev 2.0 can be found on [our official post](https://x.com/OpenBMB/status/2008916790399701335).

&lt;details&gt;
&lt;summary&gt;Old News&lt;/summary&gt;

‚Ä¢Sep 24, 2025: üéâ Our paper [Multi-Agent Collaboration via Evolving Orchestration](https://arxiv.org/abs/2505.19591) has been accepted to NeurIPS 2025. The implementation is available in the `puppeteer` branch of this repository.

‚Ä¢May 26, 2025: üéâ We propose a novel puppeteer-style paradigm for multi-agent collaboration among large language model based agents. By leveraging a learnable central orchestrator optimized with reinforcement learning, our method dynamically activates and sequences agents to construct efficient, context-aware reasoning paths. This approach not only improves reasoning quality but also reduces computational costs, enabling scalable and adaptable multi-agent cooperation in complex tasks.
See our paper in [Multi-Agent Collaboration via Evolving Orchestration](https://arxiv.org/abs/2505.19591).
  &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&#039;./assets/puppeteer.png&#039; width=800&gt;
  &lt;/p&gt;

‚Ä¢June 25, 2024: üéâTo foster development in LLM-powered multi-agent collaborationü§ñü§ñ and related fields, the ChatDev team has curated a collection of seminal papersüìÑ presented in a [open-source](https://github.com/OpenBMB/ChatDev/tree/main/MultiAgentEbook) interactive e-booküìö format. Now you can explore the latest advancements on the [Ebook Website](https://thinkwee.top/multiagent_ebook) and download the [paper list](https://github.com/OpenBMB/ChatDev/blob/main/MultiAgentEbook/papers.csv).
  &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&#039;./assets/ebook.png&#039; width=800&gt;
  &lt;/p&gt;
  
‚Ä¢June 12, 2024: We introduced Multi-Agent Collaboration Networks (MacNet) üéâ, which utilize directed acyclic graphs to facilitate effective task-oriented collaboration among agents through linguistic interactions ü§ñü§ñ. MacNet supports co-operation across various topologies and among more than a thousand agents without exceeding context limits. More versatile and scalable, MacNet can be considered as a more advanced version of ChatDev&#039;s chain-shaped topology. Our preprint paper is available at [https://arxiv.org/abs/2406.07155](https://arxiv.org/abs/2406.07155). This technique has been incorporated into the [macnet](https://github.com/OpenBMB/ChatDev/tree/macnet) branch, enhancing support for diverse organizational structures and offering richer solutions beyond software development (e.g., logical reasoning, data analysis, story generation, and more).
  &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&#039;./assets/macnet.png&#039; width=500&gt;
  &lt;/p&gt;

‚Ä¢ May 07, 2024, we introduced &quot;Iterative Experience Refinement&quot; (IER), a novel method where instructor and assistant agents enhance shortcut-oriented experiences to efficiently adapt to new tasks. This approach encompasses experience acquisition, utilization, propagation and elimination across a series of tasks and making the pricess shorter and efficient. Our preprint paper is available at https://arxiv.org/abs/2405.04219, and this technique will soon be incorporated into ChatDev.
  &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&#039;./assets/ier.png&#039; width=220&gt;
  &lt;/p&gt;

‚Ä¢ January 25, 2024: We have integrated Experiential Co-Learning Module into ChatDev. Please see the [Experiential Co-Learning Guide](wiki.md#co-tracking).

‚Ä¢ December 28, 2023: We present Experiential Co-Learning, an innovative approach where instructor and assistant agents accumulate shortcut-oriented experiences to effectively solve new tasks, reducing repetitive errors and enhancing efficiency.  Check out our preprint paper at https://arxiv.org/abs/2312.17025 and this technique will soon be integrated into ChatDev.
  &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&#039;./assets/ecl.png&#039; width=860&gt;
  &lt;/p&gt;
‚Ä¢ November 15, 2023: We launched ChatDev as a SaaS platform that enables software developers and innovative entrepreneurs to build software efficiently at a very low cost and remove the barrier to entry. Try it out at https://chatdev.modelbest.cn/.
  &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&#039;./assets/saas.png&#039; width=560&gt;
  &lt;/p&gt;

‚Ä¢ November 2, 2023: ChatDev is now supported with a new feature: incremental development, which allows agents to develop upon existing codes. Try ```--config &quot;incremental&quot; --path &quot;[source_code_directory_path]&quot;``` to start it.
  &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&#039;./assets/increment.png&#039; width=700&gt;
  &lt;/p&gt;

‚Ä¢ October 26, 2023: ChatDev is now supported with Docker for safe execution (thanks to contribution from [ManindraDeMel](https://github.com/ManindraDeMel)). Please see [Docker Start Guide](wiki.md#docker-start).
  &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&#039;./assets/docker.png&#039; width=400&gt;
  &lt;/p&gt;
  
‚Ä¢ September 25, 2023: The **Git** mode is now available, enabling the programmer &lt;img src=&#039;visualizer/static/figures/programmer.png&#039; height=20&gt; to utilize Git for version control. To enable this feature, simply set ``&quot;git_management&quot;`` to ``&quot;True&quot;`` in ``ChatChainConfig.json``. See [guide](wiki.md#git-mode).
  &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&#039;./assets/github.png&#039; width=600&gt;
  &lt;/p&gt;

‚Ä¢ September 20, 2023: The **Human-Agent-Interaction** mode is now available! You can get involved with the ChatDev team by playing the role of reviewer &lt;img src=&#039;visualizer/static/figures/reviewer.png&#039; height=20&gt; and making suggestions to the programmer &lt;img src=&#039;visualizer/static/figures/programmer.png&#039; height=20&gt;;
  try ``python3 run.py --task [description_of_your_idea] --config &quot;Human&quot;``. See [guide](wiki.md#human-agent-interaction) and [example](WareHouse/Gomoku_HumanAgentInteraction_20230920135038).
  &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&#039;./assets/Human_intro.png&#039; width=600&gt;
  &lt;/p&gt;

‚Ä¢ September 1, 2023: The **Art** mode is available now! You can activate the designer agent &lt;img src=&#039;visualizer/static/figures/designer.png&#039; height=20&gt; to generate images used in the software;
  try ``python3 run.py --task [description_of_your_idea] --config &quot;Art&quot;``. See [guide](wiki.md#art) and [example](WareHouse/gomokugameArtExample_THUNLP_20230831122822).
  
‚Ä¢ August 28, 2023: The system is publicly available.

‚Ä¢ August 17, 2023: The v1.0.0 version was ready for release.

‚Ä¢ July 30, 2023: Users can customize ChatChain, Phasea and Role settings. Additionally, both online Log mode and replay
  mode are now supported.

‚Ä¢ July 16, 2023: The [preprint paper](https://arxiv.org/abs/2307.07924) associated with this project was published.

‚Ä¢ June 30, 2023: The initial version of the ChatDev repository was released.
&lt;/details&gt;


## üöÄ Quick Start

### üìã Prerequisites

*   **OS**: macOS / Linux / WSL / Windows
*   **Python**: 3.12+
*   **Node.js**: 18+
*   **Package Manager**: [uv](https://docs.astral.sh/uv/)

### üì¶ Installation

1.  **Backend Dependencies** (Python managed by `uv`):
    ```bash
    uv sync
    ```

2.  **Frontend Dependencies** (Vite + Vue 3):
    ```bash
    cd frontend &amp;&amp; npm install
    ```

### ‚ö°Ô∏è Run the Application

1.  **Start Backend** :
    ```bash
    # Run from the project root
    uv run python server_main.py --port 6400 --reload
    ```
    &gt; Remove `--reload` if output files (e.g., GameDev) trigger restarts, which interrupts tasks and loses progress.

2.  **Start Frontend**:
    ```bash
    cd frontend
    VITE_API_BASE_URL=http://localhost:6400 npm run dev
    ```
    &gt; Then access the Web Console at **[http://localhost:5173](http://localhost:5173)**. 
    
    
    &gt; **üí° Tip**: If the frontend fails to connect to the backend, the default port `6400` may already be occupied.
    &gt; Please switch both services to an available port, for example:
    &gt;
    &gt; * **Backend**: start with `--port 6401`
    &gt; * **Frontend**: set `VITE_API_BASE_URL=http://localhost:6401`


### üîë Configuration

*   **Environment Variables**: Create a `.env` file in the project root.
*   **Model Keys**: Set `API_KEY` and `BASE_URL` in `.env` for your LLM provider.
*   **YAML placeholders**: Use `${VAR}`Ôºàe.g., `${API_KEY}`Ôºâin configuration files to reference these variables.

---

## üí° How to Use

### üñ•Ô∏è Web Console

The DevAll interface provides a seamless experience for both construction and execution

*   **Tutorial**: Comprehensive step-by-step guides and documentation integrated directly into the platform to help you get started quickly.
&lt;img src=&quot;assets/tutorial-en.png&quot;/&gt; 

*   **Workflow**: A visual canvas to design your multi-agent systems. Configure node parameters, define context flows, and orchestrate complex agent interactions with drag-and-drop ease.
&lt;img src=&quot;assets/workflow.gif&quot;/&gt;

*   **Launch**: Initiate workflows, monitor real-time logs, inspect intermediate artifacts, and provide human-in-the-loop feedback.
&lt;img src=&quot;assets/launch.gif&quot;/&gt;

### üß∞ Python SDK
For automation and batch processing, use our lightweight Python SDK to execute workflows programmatically and retrieve results directly.

```python
from runtime.sdk import run_workflow

# Execute a workflow and get the final node message
result = run_workflow(
    yaml_file=&quot;yaml_instance/demo.yaml&quot;,
    task_prompt=&quot;Summarize the attached document in one sentence.&quot;,
    attachments=[&quot;/path/to/document.pdf&quot;],
    variables={&quot;API_KEY&quot;: &quot;sk-xxxx&quot;} # Override .env variables if needed
)

if result.final_message:
    print(f&quot;Output: {result.final_message.text_content()}&quot;)
```

---

&lt;a id=&quot;developers&quot;&gt;&lt;/a&gt;
## ‚öôÔ∏è For Developers

**For secondary development and extensions, please proceed with this section.**

Extend DevAll with new nodes, providers, and tools.
The project is organized into a modular structure:
*   **Core Systems**: `server/` hosts the FastAPI backend, while `runtime/` manages agent abstraction and tool execution.
*   **Orchestration**: `workflow/` handles the multi-agent logic, driven by configurations in `entity/`.
*   **Frontend**: `frontend/` contains the Vue 3 Web Console.
*   **Extensibility**: `functions/` is the place for custom Python tools.

Relevant reference documentation:
*   **Getting Started**: [Start Guide](./docs/user_guide/en/index.md)
*   **Core Modules**: [Workflow Authoring](./docs/user_guide/en/workflow_authoring.md), [Memory](./docs/user_guide/en/modules/memory.md), and [Tooling](./docs/user_guide/en/modules/tooling/index.md)

---

## üåü Featured Workflows
We provide robust, out-of-the-box templates for common scenarios. All runnable workflow configs are located in `yaml_instance/`.
*   **Demos**: Files named `demo_*.yaml` showcase specific features or modules.
*   **Implementations**: Files named directly (e.g., `ChatDev_v1.yaml`) are full in-house or recreated workflows. As follows:

### üìã Workflow Collection

| Category | Workflow                                                                                                    | Case | 
| :--- |:------------------------------------------------------------------------------------------------------------| :--- | 
| **üìà Data Visualization** | `data_visualization_basic.yaml`&lt;br&gt;`data_visualization_enhanced.yaml`                                       | &lt;img src=&quot;assets/cases/data_analysis/data_analysis.gif&quot; width=&quot;100%&quot;&gt;&lt;br&gt;Prompt: *&quot;Create 4‚Äì6 high-quality PNG charts for my large real-estate transactions dataset.&quot;* |
| **üõ†Ô∏è 3D Generation**&lt;br&gt;*(Requires [Blender](https://www.blender.org/) &amp; [blender-mcp](https://github.com/ahujasid/blender-mcp))* | `blender_3d_builder_simple.yaml`&lt;br&gt;`blender_3d_builder_hub.yaml`&lt;br&gt;`blender_scientific_illustration.yaml` | &lt;img src=&quot;assets/cases/3d_generation/3d.gif&quot; width=&quot;100%&quot;&gt;&lt;br&gt;Prompt: *&quot;Please build a Christmas tree.&quot;* |
| **üéÆ Game Dev** | `GameDev_v1.yaml`&lt;br&gt;`ChatDev_v1.yaml`                                                                      | &lt;img src=&quot;assets/cases/game_development/game.gif&quot; width=&quot;100%&quot;&gt;&lt;br&gt;Prompt: *&quot;Please help me design and develop a Tank Battle game.&quot;* |
| **üìö Deep Research** | `deep_research_v1.yaml`                                                                                     | &lt;img src=&quot;assets/cases/deep_research/deep_research.gif&quot; width=&quot;85%&quot;&gt;&lt;br&gt;Prompt: *&quot;Research about recent advances in the field of LLM-based agent RL&quot;* |
| **üéì Teach Video** | `teach_video.yaml` (Please run command `uv add manim` before running this workflow)                         | &lt;img src=&quot;assets/cases/video_generation/video.gif&quot; width=&quot;140%&quot;&gt;&lt;br&gt;Prompt: *&quot;ËÆ≤‰∏Ä‰∏ã‰ªÄ‰πàÊòØÂá∏‰ºòÂåñ&quot;* |

---

### üí° Usage Guide
For those implementations, you can use the **Launch** tab to execute them.
1.  **Select**: Choose a workflow in the **Launch** tab.
2.  **Upload**: Upload necessary files (e.g., `.csv` for data analysis) if required.
3.  **Prompt**: Enter your request (e.g., *&quot;Visualize the sales trends&quot;* or *&quot;Design a snake game&quot;*).

---

## ü§ù Contributing

We welcome contributions from the community! Whether you&#039;re fixing bugs, adding new workflow templates, or sharing high-quality cases/artifacts produced by DevAll, your help is much appreciated. Feel free to contribute by submitting **Issues** or **Pull Requests**.

By contributing to DevAll, you&#039;ll be recognized in our **Contributors** list below. Check out our [Developer Guide](#developers) to get started!

### üë• Contributors

#### Primary Contributors

&lt;table&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/NA-Wen&quot;&gt;&lt;img src=&quot;https://github.com/NA-Wen.png?size=100&quot; width=&quot;64px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;NA-Wen&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/zxrys&quot;&gt;&lt;img src=&quot;https://github.com/zxrys.png?size=100&quot; width=&quot;64px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;zxrys&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/swugi&quot;&gt;&lt;img src=&quot;https://github.com/swugi.png?size=100&quot; width=&quot;64px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;swugi&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/huatl98&quot;&gt;&lt;img src=&quot;https://github.com/huatl98.png?size=100&quot; width=&quot;64px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;huatl98&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

#### Contributors
&lt;table&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/shiowen&quot;&gt;&lt;img src=&quot;https://github.com/shiowen.png?size=100&quot; width=&quot;64px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;shiowen&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/kilo2127&quot;&gt;&lt;img src=&quot;https://github.com/kilo2127.png?size=100&quot; width=&quot;64px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;kilo2127&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/AckerlyLau&quot;&gt;&lt;img src=&quot;https://github.com/AckerlyLau.png?size=100&quot; width=&quot;64px;&quot; alt=&quot;&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;AckerlyLau&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/table&gt;

## ü§ù Acknowledgments

&lt;a href=&quot;http://nlp.csai.tsinghua.edu.cn/&quot;&gt;&lt;img src=&quot;assets/thunlp.png&quot; height=50pt&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
&lt;a href=&quot;https://modelbest.cn/&quot;&gt;&lt;img src=&quot;assets/modelbest.png&quot; height=50pt&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
&lt;a href=&quot;https://github.com/OpenBMB/AgentVerse/&quot;&gt;&lt;img src=&quot;assets/agentverse.png&quot; height=50pt&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;
&lt;a href=&quot;https://github.com/OpenBMB/RepoAgent&quot;&gt;&lt;img src=&quot;assets/repoagent.png&quot;  height=50pt&gt;&lt;/a&gt;
&lt;a href=&quot;https://app.commanddash.io/agent?github=https://github.com/OpenBMB/ChatDev&quot;&gt;&lt;img src=&quot;assets/CommandDash.png&quot; height=50pt&gt;&lt;/a&gt;
&lt;a href=&quot;www.teachmaster.cn&quot;&gt;&lt;img src=&quot;assets/teachmaster.png&quot; height=50pt&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/OpenBMB/AppCopilot&quot;&gt;&lt;img src=&quot;assets/appcopilot.png&quot; height=50pt&gt;&lt;/a&gt;

## üîé Citation

```
@article{chatdev,
    title = {ChatDev: Communicative Agents for Software Development},
    author = {Chen Qian and Wei Liu and Hongzhang Liu and Nuo Chen and Yufan Dang and Jiahao Li and Cheng Yang and Weize Chen and Yusheng Su and Xin Cong and Juyuan Xu and Dahai Li and Zhiyuan Liu and Maosong Sun},
    journal = {arXiv preprint arXiv:2307.07924},
    url = {https://arxiv.org/abs/2307.07924},
    year = {2023}
}

@article{colearning,
    title = {Experiential Co-Learning of Software-Developing Agents},
    author = {Chen Qian and Yufan Dang and Jiahao Li and Wei Liu and Zihao Xie and Yifei Wang and Weize Chen and Cheng Yang and Xin Cong and Xiaoyin Che and Zhiyuan Liu and Maosong Sun},
    journal = {arXiv preprint arXiv:2312.17025},
    url = {https://arxiv.org/abs/2312.17025},
    year = {2023}
}

@article{macnet,
    title={Scaling Large-Language-Model-based Multi-Agent Collaboration},
    author={Chen Qian and Zihao Xie and Yifei Wang and Wei Liu and Yufan Dang and Zhuoyun Du and Weize Chen and Cheng Yang and Zhiyuan Liu and Maosong Sun}
    journal={arXiv preprint arXiv:2406.07155},
    url = {https://arxiv.org/abs/2406.07155},
    year={2024}
}

@article{iagents,
    title={Autonomous Agents for Collaborative Task under Information Asymmetry},
    author={Wei Liu and Chenxi Wang and Yifei Wang and Zihao Xie and Rennai Qiu and Yufan Dnag and Zhuoyun Du and Weize Chen and Cheng Yang and Chen Qian},
    journal={arXiv preprint arXiv:2406.14928},
    url = {https://arxiv.org/abs/2406.14928},
    year={2024}
}

@article{puppeteer,
      title={Multi-Agent Collaboration via Evolving Orchestration}, 
      author={Yufan Dang and Chen Qian and Xueheng Luo and Jingru Fan and Zihao Xie and Ruijie Shi and Weize Chen and Cheng Yang and Xiaoyin Che and Ye Tian and Xuantang Xiong and Lei Han and Zhiyuan Liu and Maosong Sun},
      journal={arXiv preprint arXiv:2505.19591},
      url={https://arxiv.org/abs/2505.19591},
      year={2025}
}
```

## üì¨ Contact

If you have any questions, feedback, or would like to get in touch, please feel free to reach out to us via email at [qianc62@gmail.com](mailto:qianc62@gmail.com)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[VectifyAI/PageIndex]]></title>
            <link>https://github.com/VectifyAI/PageIndex</link>
            <guid>https://github.com/VectifyAI/PageIndex</guid>
            <pubDate>Tue, 03 Feb 2026 00:07:49 GMT</pubDate>
            <description><![CDATA[üìë PageIndex: Document Index for Vectorless, Reasoning-based RAG]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/VectifyAI/PageIndex">VectifyAI/PageIndex</a></h1>
            <p>üìë PageIndex: Document Index for Vectorless, Reasoning-based RAG</p>
            <p>Language: Python</p>
            <p>Stars: 12,479</p>
            <p>Forks: 889</p>
            <p>Stars today: 793 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  
&lt;a href=&quot;https://vectify.ai/pageindex&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/46201e72-675b-43bc-bfbd-081cc6b65a1d&quot; alt=&quot;PageIndex Banner&quot; /&gt;
&lt;/a&gt;

&lt;br/&gt;
&lt;br/&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/14736&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14736&quot; alt=&quot;VectifyAI%2FPageIndex | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

# PageIndex: Vectorless, Reasoning-based RAG

&lt;p align=&quot;center&quot;&gt;&lt;b&gt;Reasoning-based RAG&amp;nbsp; ‚ó¶ &amp;nbsp;No Vector DB&amp;nbsp; ‚ó¶ &amp;nbsp;No Chunking&amp;nbsp; ‚ó¶ &amp;nbsp;Human-like Retrieval&lt;/b&gt;&lt;/p&gt;

&lt;h4 align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://vectify.ai&quot;&gt;üè† Homepage&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp;
  &lt;a href=&quot;https://chat.pageindex.ai&quot;&gt;üñ•Ô∏è Chat Platform&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp;
  &lt;a href=&quot;https://pageindex.ai/mcp&quot;&gt;üîå MCP&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp;
  &lt;a href=&quot;https://docs.pageindex.ai&quot;&gt;üìö Docs&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp;
  &lt;a href=&quot;https://discord.com/invite/VuXuf29EUj&quot;&gt;üí¨ Discord&lt;/a&gt;&amp;nbsp; ‚Ä¢ &amp;nbsp;
  &lt;a href=&quot;https://ii2abc2jejf.typeform.com/to/tK3AXl8T&quot;&gt;‚úâÔ∏è Contact&lt;/a&gt;&amp;nbsp;
&lt;/h4&gt;
  
&lt;/div&gt;


&lt;details open&gt;
&lt;summary&gt;&lt;h3&gt;üì¢ Latest Updates&lt;/h3&gt;&lt;/summary&gt;

 **üî• Releases:**
- [**PageIndex Chat**](https://chat.pageindex.ai): The first human-like document-analysis agent [platform](https://chat.pageindex.ai) built for professional long documents. Can also be integrated via [MCP](https://pageindex.ai/mcp) or [API](https://docs.pageindex.ai/quickstart) (beta).
&lt;!-- - [**PageIndex Chat API**](https://docs.pageindex.ai/quickstart): An API that brings PageIndex&#039;s advanced long-document intelligence directly into your applications and workflows. --&gt;
&lt;!-- - [PageIndex MCP](https://pageindex.ai/mcp): Bring PageIndex into Claude, Cursor, or any MCP-enabled agent. Chat with long PDFs in a reasoning-based, human-like way. --&gt;
 
 **üìù Articles:**
- [**PageIndex Framework**](https://pageindex.ai/blog/pageindex-intro): Introduces the PageIndex framework ‚Äî an *agentic, in-context* *tree index* that enables LLMs to perform *reasoning-based*, *human-like retrieval* over long documents, without vector DB or chunking.
&lt;!-- - [Do We Still Need OCR?](https://pageindex.ai/blog/do-we-need-ocr): Explores how vision-based, reasoning-native RAG challenges the traditional OCR pipeline, and why the future of document AI might be *vectorless* and *vision-based*. --&gt;

 **üß™ Cookbooks:**
- [Vectorless RAG](https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex): A minimal, hands-on example of reasoning-based RAG using PageIndex. No vectors, no chunking, and human-like retrieval.
- [Vision-based Vectorless RAG](https://docs.pageindex.ai/cookbook/vision-rag-pageindex): OCR-free, vision-only RAG with PageIndex&#039;s reasoning-native retrieval workflow that works directly over PDF page images.
&lt;/details&gt;

---

# üìë Introduction to PageIndex

Are you frustrated with vector database retrieval accuracy for long professional documents? Traditional vector-based RAG relies on semantic *similarity* rather than true *relevance*. But **similarity ‚â† relevance** ‚Äî what we truly need in retrieval is **relevance**, and that requires **reasoning**. When working with professional documents that demand domain expertise and multi-step reasoning, similarity search often falls short.

Inspired by AlphaGo, we propose **[PageIndex](https://vectify.ai/pageindex)** ‚Äî a **vectorless**, **reasoning-based RAG** system that builds a **hierarchical tree index** from long documents and uses LLMs to **reason** *over that index* for **agentic, context-aware retrieval**.
It simulates how *human experts* navigate and extract knowledge from complex documents through *tree search*, enabling LLMs to *think* and *reason* their way to the most relevant document sections. PageIndex performs retrieval in two steps:

1. Generate a ‚ÄúTable-of-Contents‚Äù **tree structure index** of documents
2. Perform reasoning-based retrieval through **tree search**

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pageindex.ai/blog/pageindex-intro&quot; target=&quot;_blank&quot; title=&quot;The PageIndex Framework&quot;&gt;
    &lt;img src=&quot;https://docs.pageindex.ai/images/cookbook/vectorless-rag.png&quot; width=&quot;70%&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

### üéØ Core Features 

Compared to traditional vector-based RAG, **PageIndex** features:
- **No Vector DB**: Uses document structure and LLM reasoning for retrieval, instead of vector similarity search.
- **No Chunking**: Documents are organized into natural sections, not artificial chunks.
- **Human-like Retrieval**: Simulates how human experts navigate and extract knowledge from complex documents.
- **Better Explainability and Traceability**: Retrieval is based on reasoning ‚Äî traceable and interpretable, with page and section references. No more opaque, approximate vector search (‚Äúvibe retrieval‚Äù).

PageIndex powers a reasoning-based RAG system that achieved **state-of-the-art** [98.7% accuracy](https://github.com/VectifyAI/Mafin2.5-FinanceBench) on FinanceBench, demonstrating superior performance over vector-based RAG solutions in professional document analysis (see our [blog post](https://vectify.ai/blog/Mafin2.5) for details).

### üìç Explore PageIndex

To learn more, please see a detailed introduction of the [PageIndex framework](https://pageindex.ai/blog/pageindex-intro). Check out this GitHub repo for open-source code, and the [cookbooks](https://docs.pageindex.ai/cookbook), [tutorials](https://docs.pageindex.ai/tutorials), and [blog](https://pageindex.ai/blog) for additional usage guides and examples. 

The PageIndex service is available as a ChatGPT-style [chat platform](https://chat.pageindex.ai), or can be integrated via [MCP](https://pageindex.ai/mcp) or [API](https://docs.pageindex.ai/quickstart).

### üõ†Ô∏è Deployment Options
- Self-host ‚Äî run locally with this open-source repo.
- Cloud Service ‚Äî try instantly with our [Chat Platform](https://chat.pageindex.ai/), or integrate with [MCP](https://pageindex.ai/mcp) or [API](https://docs.pageindex.ai/quickstart).
- _Enterprise_ ‚Äî private or on-prem deployment. [Contact us](https://ii2abc2jejf.typeform.com/to/tK3AXl8T) or [book a demo](https://calendly.com/pageindex/meet) for more details.

### üß™ Quick Hands-on

- Try the [**Vectorless RAG**](https://github.com/VectifyAI/PageIndex/blob/main/cookbook/pageindex_RAG_simple.ipynb) notebook ‚Äî a *minimal*, hands-on example of reasoning-based RAG using PageIndex.
- Experiment with [*Vision-based Vectorless RAG*](https://github.com/VectifyAI/PageIndex/blob/main/cookbook/vision_RAG_pageindex.ipynb) ‚Äî no OCR; a minimal, reasoning-native RAG pipeline that works directly over page images.
  
&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/pageindex_RAG_simple.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Open_In_Colab-Vectorless_RAG-orange?style=for-the-badge&amp;logo=googlecolab&quot; alt=&quot;Open in Colab: Vectorless RAG&quot; /&gt;
  &lt;/a&gt;
  &amp;nbsp;&amp;nbsp;
  &lt;a href=&quot;https://colab.research.google.com/github/VectifyAI/PageIndex/blob/main/cookbook/vision_RAG_pageindex.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Open_In_Colab-Vision_RAG-orange?style=for-the-badge&amp;logo=googlecolab&quot; alt=&quot;Open in Colab: Vision RAG&quot; /&gt;
  &lt;/a&gt;
&lt;/div&gt;

---

# üå≤ PageIndex Tree Structure
PageIndex can transform lengthy PDF documents into a semantic **tree structure**, similar to a _&quot;table of contents&quot;_ but optimized for use with Large Language Models (LLMs). It&#039;s ideal for: financial reports, regulatory filings, academic textbooks, legal or technical manuals, and any document that exceeds LLM context limits.

Below is an example PageIndex tree structure. Also see more example [documents](https://github.com/VectifyAI/PageIndex/tree/main/tests/pdfs) and generated [tree structures](https://github.com/VectifyAI/PageIndex/tree/main/tests/results).

```jsonc
...
{
  &quot;title&quot;: &quot;Financial Stability&quot;,
  &quot;node_id&quot;: &quot;0006&quot;,
  &quot;start_index&quot;: 21,
  &quot;end_index&quot;: 22,
  &quot;summary&quot;: &quot;The Federal Reserve ...&quot;,
  &quot;nodes&quot;: [
    {
      &quot;title&quot;: &quot;Monitoring Financial Vulnerabilities&quot;,
      &quot;node_id&quot;: &quot;0007&quot;,
      &quot;start_index&quot;: 22,
      &quot;end_index&quot;: 28,
      &quot;summary&quot;: &quot;The Federal Reserve&#039;s monitoring ...&quot;
    },
    {
      &quot;title&quot;: &quot;Domestic and International Cooperation and Coordination&quot;,
      &quot;node_id&quot;: &quot;0008&quot;,
      &quot;start_index&quot;: 28,
      &quot;end_index&quot;: 31,
      &quot;summary&quot;: &quot;In 2023, the Federal Reserve collaborated ...&quot;
    }
  ]
}
...
```

You can generate the PageIndex tree structure with this open-source repo, or use our [API](https://docs.pageindex.ai/quickstart) 

---

# ‚öôÔ∏è Package Usage

You can follow these steps to generate a PageIndex tree from a PDF document.

### 1. Install dependencies

```bash
pip3 install --upgrade -r requirements.txt
```

### 2. Set your OpenAI API key

Create a `.env` file in the root directory and add your API key:

```bash
CHATGPT_API_KEY=your_openai_key_here
```

### 3. Run PageIndex on your PDF

```bash
python3 run_pageindex.py --pdf_path /path/to/your/document.pdf
```

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Optional parameters&lt;/strong&gt;&lt;/summary&gt;
&lt;br&gt;
You can customize the processing with additional optional arguments:

```
--model                 OpenAI model to use (default: gpt-4o-2024-11-20)
--toc-check-pages       Pages to check for table of contents (default: 20)
--max-pages-per-node    Max pages per node (default: 10)
--max-tokens-per-node   Max tokens per node (default: 20000)
--if-add-node-id        Add node ID (yes/no, default: yes)
--if-add-node-summary   Add node summary (yes/no, default: yes)
--if-add-doc-description Add doc description (yes/no, default: yes)
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Markdown support&lt;/strong&gt;&lt;/summary&gt;
&lt;br&gt;
We also provide markdown support for PageIndex. You can use the `-md_path` flag to generate a tree structure for a markdown file.

```bash
python3 run_pageindex.py --md_path /path/to/your/document.md
```

&gt; Note: in this function, we use &quot;#&quot; to determine node heading and their levels. For example, &quot;##&quot; is level 2, &quot;###&quot; is level 3, etc. Make sure your markdown file is formatted correctly. If your Markdown file was converted from a PDF or HTML, we don&#039;t recommend using this function, since most existing conversion tools cannot preserve the original hierarchy. Instead, use our [PageIndex OCR](https://pageindex.ai/blog/ocr), which is designed to preserve the original hierarchy, to convert the PDF to a markdown file and then use this function.
&lt;/details&gt;

&lt;!-- 
# ‚òÅÔ∏è Improved Tree Generation with PageIndex OCR

This repo is designed for generating PageIndex tree structure for simple PDFs, but many real-world use cases involve complex PDFs that are hard to parse by classic Python tools. However, extracting high-quality text from PDF documents remains a non-trivial challenge. Most OCR tools only extract page-level content, losing the broader document context and hierarchy.

To address this, we introduced PageIndex OCR ‚Äî the first long-context OCR model designed to preserve the global structure of documents. PageIndex OCR significantly outperforms other leading OCR tools, such as those from Mistral and Contextual AI, in recognizing true hierarchy and semantic relationships across document pages.

- Experience next-level OCR quality with PageIndex OCR at our [Dashboard](https://dash.pageindex.ai/).
- Integrate PageIndex OCR seamlessly into your stack via our [API](https://docs.pageindex.ai/quickstart).

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/eb35d8ae-865c-4e60-a33b-ebbd00c41732&quot; width=&quot;80%&quot;&gt;
&lt;/p&gt;
--&gt;

---

# üìà Case Study: PageIndex Leads Finance QA Benchmark

[Mafin 2.5](https://vectify.ai/mafin) is a reasoning-based RAG system for financial document analysis, powered by **PageIndex**. It achieved a state-of-the-art [**98.7% accuracy**](https://vectify.ai/blog/Mafin2.5) on the [FinanceBench](https://arxiv.org/abs/2311.11944) benchmark, significantly outperforming traditional vector-based RAG systems.

PageIndex&#039;s hierarchical indexing and reasoning-driven retrieval enable precise navigation and extraction of relevant context from complex financial reports, such as SEC filings and earnings disclosures.

Explore the full [benchmark results](https://github.com/VectifyAI/Mafin2.5-FinanceBench) and our [blog post](https://vectify.ai/blog/Mafin2.5) for detailed comparisons and performance metrics.

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/VectifyAI/Mafin2.5-FinanceBench&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/571aa074-d803-43c7-80c4-a04254b782a3&quot; width=&quot;70%&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

---

# üß≠ Resources

* üß™ [Cookbooks](https://docs.pageindex.ai/cookbook/vectorless-rag-pageindex): hands-on, runnable examples and advanced use cases.
* üìñ [Tutorials](https://docs.pageindex.ai/doc-search): practical guides and strategies, including *Document Search* and *Tree Search*.
* üìù [Blog](https://pageindex.ai/blog): technical articles, research insights, and product updates.
* üîå [MCP setup](https://pageindex.ai/mcp#quick-setup) &amp; [API docs](https://docs.pageindex.ai/quickstart): integration details and configuration options.

---

# ‚≠ê Support Us

Leave us a star üåü if you like our project. Thank you!  

&lt;p&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/eae4ff38-48ae-4a7c-b19f-eab81201d794&quot; width=&quot;80%&quot;&gt;
&lt;/p&gt;

### Connect with Us

[![Twitter](https://img.shields.io/badge/Twitter-000000?style=for-the-badge&amp;logo=x&amp;logoColor=white)](https://x.com/PageIndexAI)&amp;nbsp;
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&amp;logo=linkedin&amp;logoColor=white)](https://www.linkedin.com/company/vectify-ai/)&amp;nbsp;
[![Discord](https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;logo=discord&amp;logoColor=white)](https://discord.com/invite/VuXuf29EUj)&amp;nbsp;
[![Contact Us](https://img.shields.io/badge/Contact_Us-3B82F6?style=for-the-badge&amp;logo=envelope&amp;logoColor=white)](https://ii2abc2jejf.typeform.com/to/tK3AXl8T)

---

¬© 2025 [Vectify AI](https://vectify.ai)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[karpathy/nanochat]]></title>
            <link>https://github.com/karpathy/nanochat</link>
            <guid>https://github.com/karpathy/nanochat</guid>
            <pubDate>Tue, 03 Feb 2026 00:07:48 GMT</pubDate>
            <description><![CDATA[The best ChatGPT that $100 can buy.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/karpathy/nanochat">karpathy/nanochat</a></h1>
            <p>The best ChatGPT that $100 can buy.</p>
            <p>Language: Python</p>
            <p>Stars: 41,613</p>
            <p>Forks: 5,386</p>
            <p>Stars today: 254 stars today</p>
            <h2>README</h2><pre># nanochat

![nanochat logo](dev/nanochat.png)
![scaling laws](dev/scaling_laws_jan26.png)

nanochat is the simplest experimental harness for training LLMs. It is designed to run on a single GPU node, the code is minimal/hackable, and it covers all major LLM stages including tokenization, pretraining, finetuning, evaluation, inference, and a chat UI. For example, you can train your own GPT-2 capability LLM (which cost ~$50,000 to train in 2019) for only $73 (3 hours of 8XH100 GPU node) and then talk to it in a familiar ChatGPT-like web UI.

For questions about the repo, I recommend either using [DeepWiki](https://deepwiki.com/karpathy/nanochat) from Devin/Cognition to ask questions about the repo, or use the [Discussions tab](https://github.com/karpathy/nanochat/discussions), or come by the [#nanochat](https://discord.com/channels/1020383067459821711/1427295580895314031) channel on Discord.

## Updates

- (Jan 31 2026) Major revamp of all scripts/README ongoing, deleting midtraining stage, might be a bit messy briefly...
- (Jan 30 2026) With all the latest improvements we&#039;re able to train GPT-2 grade LLM in about $73. The [runs/speedrun.sh](runs/speedrun.sh) script will become the refernece way to train GPT-2 grade model and talk to it.

## Leaderboard

| # | Record time | Description | Date | Commit | Contributors |
|---|-------------|-------------|------|--------|--------------|
| 1 | 3.04 hours | d24 baseline, slightly overtrained | Jan 29 2026 | 348fbb3 | @karpathy |

The primary metric we care about is &quot;time to GPT-2&quot; - the wall clock time needed to outperform the GPT-2 (1.6B) CORE metric on an 8XH100 GPU node. In 2019, the training of GPT-2 cost approximately $50,000 so it is incredible that due to many advances over 7 years across the stack, we can now do so in 3 hours or less, for ~$73 and below. Once your repo is set up (see the [runs/speedrun.sh](runs/speedrun.sh) script for reference), e.g. the way I kicked off the jan29 run is as follows:

```
OMP_NUM_THREADS=1 torchrun --standalone --nproc_per_node=8 -m scripts.base_train -- \
    --depth=24 \
    --run=d24-jan29 \
    --model-tag=d24_jan29 \
    --device-batch-size=16 \
    --sample-every=-1 \
    --save-every=-1 \
    --core-metric-max-per-task=-1 \
    --core-metric-every=3000 \
    --target-param-data-ratio=12
```

After 3 hours we get output like this:

```
...
wandb: Run summary:
wandb:          core_metric 0.25851
wandb:                 step 16704
wandb: total_training_flops 4.330784131228946e+19
wandb:  total_training_time 10949.46713
```

The GPT-2 CORE score (i.e. the target to beat) is 0.256525. So we see that this d24 CORE score is higher (0.25851). Then we look at the `total_training_time`, which is the time of the training iterations alone, excluding all the evaluations and logging, in seconds. We get: `10949/60/60 ~= 3.04` hours, the current record.

## Getting started

### Reproduce and talk to GPT-2

The most fun you can have is to train your own GPT-2 and talk to it. The entire pipeline to do so is contained in the single file [runs/speedrun.sh](runs/speedrun.sh), which is designed to be run on an 8XH100 GPU node. Currently, at ~$24/hour for these nodes, pretraining GPT-2 grade model takes approximately 3 hours and will set you back about $75. Boot up a new 8XH100 GPU box from your favorite provider (e.g. I use and like [Lambda](https://lambda.ai/service/gpu-cloud)), and kick off the training script:

```bash
bash runs/speedrun.sh
```

You mish to do so in a screen session as this will take ~3 hours to run. Once it&#039;s done, you can talk to it via the ChatGPT-like web UI. Make sure again that your local uv virtual environment is active (run `source .venv/bin/activate`), and serve it:

```bash
python -m scripts.chat_web
```

And then visit the URL shown. Make sure to access it correctly, e.g. on Lambda use the public IP of the node you&#039;re on, followed by the port, so for example [http://209.20.xxx.xxx:8000/](http://209.20.xxx.xxx:8000/), etc. Then talk to your LLM as you&#039;d normally talk to ChatGPT! Get it to write stories or poems. Ask it to tell you who you are to see a hallucination. Ask it why the sky is blue. Or why it&#039;s green. The speedrun is a 4e19 FLOPs capability model so it&#039;s a bit like talking to a kindergartener :).

---

&lt;img width=&quot;2672&quot; height=&quot;1520&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/ed39ddf8-2370-437a-bedc-0f39781e76b5&quot; /&gt;

---

A few more notes:

- The code will run just fine on the Ampere 8XA100 GPU node as well, but a bit slower.
- All code will run just fine on even a single GPU by omitting `torchrun`, and will produce ~identical results (code will automatically switch to gradient accumulation), but you&#039;ll have to wait 8 times longer.
- If your GPU(s) have less than 80GB, you&#039;ll have to tune some of the hyperparameters or you will OOM / run out of VRAM. Look for `--device_batch_size` in the scripts and reduce it until things fit. E.g. from 32 (default) to 16, 8, 4, 2, or even 1. Less than that you&#039;ll have to know a bit more what you&#039;re doing and get more creative.
- Most of the code is fairly vanilla PyTorch so it should run on anything that supports that - xpu, mps, or etc, but I haven&#039;t personally exercised all of these code paths so there might be sharp edges.

## Research

If you are a researcher and wish to help improve nanochat, two scripts of interest are [runs/scaling_laws.sh](runs/scaling_laws.sh) and [runs/miniseries.sh](runs/miniseries.sh). See [Jan 7 miniseries v1](https://github.com/karpathy/nanochat/discussions/420) for related documentation. For quick experimentation (~5 min pretraining runs) my favorite scale is to train a 12-layer model (GPT-1 sized), e.g. like this:

```
OMP_NUM_THREADS=1 torchrun --standalone --nproc_per_node=8 -m scripts.base_train -- \
    --depth=12 \
    --run=&quot;d12&quot; \
    --model-tag=&quot;d12&quot; \
    --core-metric-every=999999 \
    --sample-every=-1 \
    --save-every=-1 \
```

This uses wandb (run name &quot;d12&quot;), only runs the CORE metric on last step, and it doesn&#039;t sample and save intermediate checkpoints. I like to change something in the code, re-run a d12 (or a d16 etc) and see if it helped, in an iteration loop.

The overall approach is to treat the depth of the model as the single dial of complexity. By sweeping out the depth, we get increasingly more powerful models. We determine the scaling laws, set the data budget to a compute optimal setting, train a whole miniseries of models of increasing sizes, and compare them to the GPT-2 and GPT-3 miniseries. Right now, beating GPT-2 specifically faster and faster is the most interesting target.

## Running on CPU / MPS

The script [runs/runcpu.sh](runs/runcpu.sh) shows a very simple example of running on CPU or Apple Silicon. It dramatically shrinks the LLM tha tis being trained to make things fit into a reasonable time interval of a few ten minutes of training. You will not get strong results in this way.

## Guides

I&#039;ve published a number of guides that might contain helpful information:

- [Oct 13 2025 original nanochat post](https://github.com/karpathy/nanochat/discussions/1) introducing nanochat, though now it contains some deprecated information and the model is a lot older (with worse results) than current master.
- [Jan 7 miniseries v1](https://github.com/karpathy/nanochat/discussions/420) documents the first nanochat miniseries of models.
- To customize your nanochat, see [Guide: infusing identity to your nanochat](https://github.com/karpathy/nanochat/discussions/139) in Discussions, which describes how you can tune your nanochat&#039;s personality through synthetic data generation and mixing that data into the SFT stage.
- To add new abilities to nanochat, see [Guide: counting r in strawberry (and how to add abilities generally)](https://github.com/karpathy/nanochat/discussions/164).

## File structure

```
.
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ dev
‚îÇ   ‚îú‚îÄ‚îÄ gen_synthetic_data.py       # Example synthetic data for identity
‚îÇ   ‚îú‚îÄ‚îÄ generate_logo.html
‚îÇ   ‚îú‚îÄ‚îÄ nanochat.png
‚îÇ   ‚îî‚îÄ‚îÄ repackage_data_reference.py # Pretraining data shard generation
‚îú‚îÄ‚îÄ nanochat
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                 # empty
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint_manager.py       # Save/Load model checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ common.py                   # Misc small utilities, quality of life
‚îÇ   ‚îú‚îÄ‚îÄ core_eval.py                # Evaluates base model CORE score (DCLM paper)
‚îÇ   ‚îú‚îÄ‚îÄ dataloader.py               # Tokenizing Distributed Data Loader
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py                  # Download/read utils for pretraining data
‚îÇ   ‚îú‚îÄ‚îÄ engine.py                   # Efficient model inference with KV Cache
‚îÇ   ‚îú‚îÄ‚îÄ execution.py                # Allows the LLM to execute Python code as tool
‚îÇ   ‚îú‚îÄ‚îÄ gpt.py                      # The GPT nn.Module Transformer
‚îÇ   ‚îú‚îÄ‚îÄ logo.svg
‚îÇ   ‚îú‚îÄ‚îÄ loss_eval.py                # Evaluate bits per byte (instead of loss)
‚îÇ   ‚îú‚îÄ‚îÄ optim.py                    # AdamW + Muon optimizer, 1GPU and distributed
‚îÇ   ‚îú‚îÄ‚îÄ report.py                   # Utilities for writing the nanochat Report
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.py                # BPE Tokenizer wrapper in style of GPT-4
‚îÇ   ‚îî‚îÄ‚îÄ ui.html                     # HTML/CSS/JS for nanochat frontend
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ runs
‚îÇ   ‚îú‚îÄ‚îÄ miniseries.sh               # Miniseries training script
‚îÇ   ‚îú‚îÄ‚îÄ runcpu.sh                   # Small example of how to run on CPU/MPS
‚îÇ   ‚îú‚îÄ‚îÄ scaling_laws.sh             # Scaling laws experiments
‚îÇ   ‚îî‚îÄ‚îÄ speedrun.sh                 # Train the ~$100 nanochat d20
‚îú‚îÄ‚îÄ scripts
‚îÇ   ‚îú‚îÄ‚îÄ base_eval.py                # Base model: CORE score, bits per byte, samples
‚îÇ   ‚îú‚îÄ‚îÄ base_train.py               # Base model: train
‚îÇ   ‚îú‚îÄ‚îÄ chat_cli.py                 # Chat model: talk to over CLI
‚îÇ   ‚îú‚îÄ‚îÄ chat_eval.py                # Chat model: eval tasks
‚îÇ   ‚îú‚îÄ‚îÄ chat_rl.py                  # Chat model: reinforcement learning
‚îÇ   ‚îú‚îÄ‚îÄ chat_sft.py                 # Chat model: train SFT
‚îÇ   ‚îú‚îÄ‚îÄ chat_web.py                 # Chat model: talk to over WebUI
‚îÇ   ‚îú‚îÄ‚îÄ tok_eval.py                 # Tokenizer: evaluate compression rate
‚îÇ   ‚îî‚îÄ‚îÄ tok_train.py                # Tokenizer: train it
‚îú‚îÄ‚îÄ tasks
‚îÇ   ‚îú‚îÄ‚îÄ arc.py                      # Multiple choice science questions
‚îÇ   ‚îú‚îÄ‚îÄ common.py                   # TaskMixture | TaskSequence
‚îÇ   ‚îú‚îÄ‚îÄ customjson.py               # Make Task from arbitrary jsonl convos
‚îÇ   ‚îú‚îÄ‚îÄ gsm8k.py                    # 8K Grade School Math questions
‚îÇ   ‚îú‚îÄ‚îÄ humaneval.py                # Misnomer; Simple Python coding task
‚îÇ   ‚îú‚îÄ‚îÄ mmlu.py                     # Multiple choice questions, broad topics
‚îÇ   ‚îú‚îÄ‚îÄ smoltalk.py                 # Conglomerate dataset of SmolTalk from HF
‚îÇ   ‚îî‚îÄ‚îÄ spellingbee.py              # Task teaching model to spell/count letters
‚îú‚îÄ‚îÄ tests
‚îÇ   ‚îî‚îÄ‚îÄ test_engine.py
‚îî‚îÄ‚îÄ uv.lock
```

## Contributing

The goal of nanochat is to improve the state of the art in micro models that are accessible to work with end to end on budgets of &lt; $1000 dollars. Accessibility is about overall cost but also about cognitive complexity - nanochat is not an exhaustively configurable LLM &quot;framework&quot;; there are no giant configuration objects, model factories, or if-then-else monsters in the code base. It is a single, cohesive, minimal, readable, hackable, maximally-forkable &quot;strong baseline&quot; codebase designed to run start to end and produce a ChatGPT model you can talk to. Currently, the most interesting part personally is speeding up the latency to GPT-2 (i.e. getting a CORE score above 0.256525). Currently this takes ~3 hours, but by improving the pretraining stage we can improve this further.

Current AI policy: disclosure. When submitting a PR, please declare any parts that had substantial LLM contribution and that you have not written or that you do not fully understand.

## Acknowledgements

- The name (nanochat) derives from my earlier project [nanoGPT](https://github.com/karpathy/nanoGPT), which only covered pretraining.
- nanochat is also inspired by [modded-nanoGPT](https://github.com/KellerJordan/modded-nanogpt), which gamified the nanoGPT repo with clear metrics and a leaderboard, and borrows a lot of its ideas and some implementation for pretraining.
- Thank you to [HuggingFace](https://huggingface.co/) for fineweb and smoltalk.
- Thank you [Lambda](https://lambda.ai/service/gpu-cloud) for the compute used in developing this project.
- Thank you to chief LLM whisperer üßô‚Äç‚ôÇÔ∏è Alec Radford for advice/guidance.
- Thank you to the repo czar Sofie [@svlandeg](https://github.com/svlandeg) for help with managing issues, pull requests and discussions of nanochat.

## Cite

If you find nanochat helpful in your research cite simply as:

```bibtex
@misc{nanochat,
  author = {Andrej Karpathy},
  title = {nanochat: The best ChatGPT that \$100 can buy},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/karpathy/nanochat}
}
```

## License

MIT
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[kovidgoyal/calibre]]></title>
            <link>https://github.com/kovidgoyal/calibre</link>
            <guid>https://github.com/kovidgoyal/calibre</guid>
            <pubDate>Tue, 03 Feb 2026 00:07:47 GMT</pubDate>
            <description><![CDATA[The official source code repository for the calibre ebook manager]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/kovidgoyal/calibre">kovidgoyal/calibre</a></h1>
            <p>The official source code repository for the calibre ebook manager</p>
            <p>Language: Python</p>
            <p>Stars: 23,791</p>
            <p>Forks: 2,542</p>
            <p>Stars today: 183 stars today</p>
            <h2>README</h2><pre># calibre

&lt;img align=&quot;left&quot; src=&quot;https://raw.githubusercontent.com/kovidgoyal/calibre/master/resources/images/lt.png&quot; height=&quot;200&quot; width=&quot;200&quot;/&gt;

calibre is an e-book manager. It can view, convert, edit and catalog e-books 
in all of the major e-book formats. It can also talk to e-book reader 
devices. It can go out to the internet and fetch metadata for your books. 
It can download newspapers and convert them into e-books for convenient 
reading. It is cross platform, running on Linux, Windows and macOS.

For more information, see the [calibre About page](https://calibre-ebook.com/about).

[![Build Status](https://github.com/kovidgoyal/calibre/workflows/CI/badge.svg)](https://github.com/kovidgoyal/calibre/actions?query=workflow%3ACI)

## Screenshots  

[Screenshots page](https://calibre-ebook.com/demo)

## Usage

See the [User Manual](https://manual.calibre-ebook.com).

## Development

[Setting up a development environment for calibre](https://manual.calibre-ebook.com/develop.html).

A [tarball of the source code](https://calibre-ebook.com/dist/src) for the 
current calibre release.

## Bugs

Bug reports and feature requests should be made in the calibre bug tracker at [Launchpad](https://bugs.launchpad.net/calibre).
GitHub is only used for code hosting and pull requests.

## Support calibre

calibre is a result of the efforts of many volunteers from all over the world.
If you find it useful, please consider contributing to support its development.
[Donate to support calibre development](https://calibre-ebook.com/donate).

## Building calibre binaries

See [Build instructions](bypy/README.rst) for instructions on how to build the
calibre binaries and installers for all the platforms calibre supports.

## calibre package versions in various repositories

[![Packaging Status](https://repology.org/badge/vertical-allrepos/calibre.svg?columns=3&amp;header=calibre)](https://repology.org/project/calibre/versions)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/agent-lightning]]></title>
            <link>https://github.com/microsoft/agent-lightning</link>
            <guid>https://github.com/microsoft/agent-lightning</guid>
            <pubDate>Tue, 03 Feb 2026 00:07:46 GMT</pubDate>
            <description><![CDATA[The absolute trainer to light up AI agents.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/agent-lightning">microsoft/agent-lightning</a></h1>
            <p>The absolute trainer to light up AI agents.</p>
            <p>Language: Python</p>
            <p>Stars: 13,288</p>
            <p>Forks: 1,098</p>
            <p>Stars today: 369 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/assets/readme-banner.svg&quot; alt=&quot;Agent-lightning-banner&quot; style=&quot;width:600px&quot;/&gt;
&lt;/p&gt;

# Agent Lightning‚ö°

[![Unit Tests](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml)
[![Documentation](https://img.shields.io/badge/GitHub%20Pages-Documentation-blue)](https://microsoft.github.io/agent-lightning/)
[![PyPI version](https://badge.fury.io/py/agentlightning.svg)](https://badge.fury.io/py/agentlightning)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/microsoft/agent-lightning)
[![Discord](https://img.shields.io/badge/Discord-Join-5865F2?logo=discord&amp;logoColor=white)](https://discord.gg/RYk7CdvDR7)

**The absolute trainer to light up AI agents.**

Join our [Discord community](https://discord.gg/RYk7CdvDR7) to connect with other users and contributors.

## ‚ö° Core Features

- Turn your agent into an optimizable beast with **ZERO CODE CHANGE** (almost)! üí§
- Build with **ANY** agent framework (LangChain, OpenAI Agent SDK, AutoGen, CrewAI, Microsoft Agent Framework...); or even WITHOUT agent framework (Python OpenAI). You name it! ü§ñ
- **Selectively** optimize one or more agents in a multi-agent system. üéØ
- Embraces **Algorithms** like Reinforcement Learning, Automatic Prompt Optimization, Supervised Fine-tuning and more. ü§ó

Read more on our [documentation website](https://microsoft.github.io/agent-lightning/).

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/assets/readme-diff.svg&quot; alt=&quot;Agent-Lightning Core Quickstart&quot; style=&quot;width:100%&quot;/&gt;
&lt;/p&gt;

## ‚ö° Installation

```bash
pip install agentlightning
```

For the latest nightly build (cutting-edge features), you can install from Test PyPI:

```bash
pip install --upgrade --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ --pre agentlightning
```

Please refer to our [installation guide](https://microsoft.github.io/agent-lightning/stable/tutorials/installation/) for more details.

To start using Agent-lightning, check out our [documentation](https://microsoft.github.io/agent-lightning/) and [examples](./examples).

## ‚ö° Articles

- 12/17/2025 [Adopting the Trajectory Level Aggregation for Faster Training](https://agent-lightning.github.io/posts/trajectory_level_aggregation/) Agent-lightning blog.
- 11/4/2025 [Tuning ANY AI agent with Tinker ‚úï Agent-lightning](https://medium.com/@yugez/tuning-any-ai-agent-with-tinker-agent-lightning-part-1-1d8c9a397f0e) Medium. See also [Part 2](https://medium.com/@yugez/tuning-any-ai-agent-with-tinker-agent-lightning-part-2-332c5437f0dc).
- 10/22/2025 [No More Retokenization Drift: Returning Token IDs via the OpenAI Compatible API Matters in Agent RL](https://blog.vllm.ai/2025/10/22/agent-lightning.html) vLLM blog. See also [Zhihu writeup](https://zhuanlan.zhihu.com/p/1965067274642785725).
- 8/11/2025 [Training AI Agents to Write and Self-correct SQL with Reinforcement Learning](https://medium.com/@yugez/training-ai-agents-to-write-and-self-correct-sql-with-reinforcement-learning-571ed31281ad) Medium.
- 8/5/2025 [Agent Lightning: Train ANY AI Agents with Reinforcement Learning](https://arxiv.org/abs/2508.03680) arXiv paper.
- 7/26/2025 [We discovered an approach to train any AI agent with RL, with (almost) zero code changes.](https://www.reddit.com/r/LocalLLaMA/comments/1m9m670/we_discovered_an_approach_to_train_any_ai_agent/) Reddit.
- 6/6/2025 [Agent Lightning - Microsoft Research](https://www.microsoft.com/en-us/research/project/agent-lightning/) Project page.

## ‚ö° Community Projects

- [DeepWerewolf](https://github.com/af-74413592/DeepWerewolf) ‚Äî A case study of agent RL training for the Chinese Werewolf game built with AgentScope and Agent Lightning.
- [AgentFlow](https://agentflow.stanford.edu/) ‚Äî A modular multi-agent framework that combines planner, executor, verifier, and generator agents with the Flow-GRPO algorithm to tackle long-horizon, sparse-reward tasks.
- [Youtu-Agent](https://github.com/TencentCloudADP/Youtu-agent) ‚Äî Youtu-Agent lets you build and train your agent with ease. Built with [a modified branch](https://github.com/microsoft/agent-lightning/tree/contrib/youtu-agent-lightning) of Agent Lightning, Youtu-Agent has verified up to 128 GPUs RL training on maths/code and search capabilities with steady convergence. Also check [the recipe](https://github.com/TencentCloudADP/youtu-agent/tree/rl/agl) and their blog [*Stop Wrestling with Your Agent RL: How Youtu-Agent Achieved Stable, 128-GPU Scaling Without Breaking a Sweat*](https://spotted-coconut-df8.notion.site/Stop-Wrestling-with-Your-Agent-RL-How-Youtu-Agent-Achieved-Stable-128-GPU-Scaling-Without-Breaking-2ca5e8f089ba80539a98c582b65e0233).

## ‚ö° Architecture

Agent Lightning keeps the moving parts to a minimum so you can focus on your idea, not the plumbing. Your agent continues to run as usual; you can still use any agent framework you like; you drop in the lightweight `agl.emit_xxx()` helper, or let the tracer collect every prompt, tool call, and reward. Those events become structured spans that flow into the LightningStore, a central hub that keeps tasks, resources, and traces in sync.

On the other side of the store sits the algorithm you choose, or write yourself. The algorithm reads spans, learns from them, and posts updated resources such as refined prompt templates or new policy weights. The Trainer ties it all together: it streams datasets to runners, ferries resources between the store and the algorithm, and updates the inference engine when improvements land. You can either stop there, or simply let the same loop keep turning.

No rewrites, no lock-in, just a clear path from first rollout to steady improvement.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/assets/readme-architecture.svg&quot; alt=&quot;Agent-lightning Architecture&quot; style=&quot;width:100%&quot;/&gt;
&lt;/p&gt;

## ‚ö° CI Status

| Workflow | Status |
|----------|--------|
| CPU Tests | [![tests workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml) |
| Full Tests | [![tests summary workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml) |
| UI Tests | [![UI Tests](https://github.com/microsoft/agent-lightning/actions/workflows/dashboard.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/dashboard.yml) |
| Examples Integration | [![examples summary workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml) |
| Latest Dependency Compatibility | [![latest summary workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml) |
| Legacy Examples Compatibility | [![compat summary workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-compat.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/badge-compat.yml) |

## ‚ö° Citation

If you find Agent Lightning useful in your research or projects, please cite our paper:

```bibtex
@misc{luo2025agentlightningtrainai,
      title={Agent Lightning: Train ANY AI Agents with Reinforcement Learning},
      author={Xufang Luo and Yuge Zhang and Zhiyuan He and Zilong Wang and Siyun Zhao and Dongsheng Li and Luna K. Qiu and Yuqing Yang},
      year={2025},
      eprint={2508.03680},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.03680},
}
```

## ‚ö° Contributing

This project welcomes contributions and suggestions. Start by reading the [Contributing Guide](docs/community/contributing.md) for recommended contribution points, environment setup, branching conventions, and pull request expectations. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## ‚ö° Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft&#039;s Trademark &amp; Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#039;s policies.

## ‚ö° Responsible AI

This project has been evaluated and certified to comply with the Microsoft Responsible AI Standard. The team will continue to monitor and maintain the repository, addressing any severe issues, including potential harms, if they arise.

## ‚ö° License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[EbookFoundation/free-programming-books]]></title>
            <link>https://github.com/EbookFoundation/free-programming-books</link>
            <guid>https://github.com/EbookFoundation/free-programming-books</guid>
            <pubDate>Tue, 03 Feb 2026 00:07:45 GMT</pubDate>
            <description><![CDATA[üìö Freely available programming books]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/EbookFoundation/free-programming-books">EbookFoundation/free-programming-books</a></h1>
            <p>üìö Freely available programming books</p>
            <p>Language: Python</p>
            <p>Stars: 381,926</p>
            <p>Forks: 65,880</p>
            <p>Stars today: 335 stars today</p>
            <h2>README</h2><pre># List of Free Learning Resources In Many Languages

&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)&amp;#160;
[![License: CC BY 4.0](https://img.shields.io/github/license/EbookFoundation/free-programming-books)](https://creativecommons.org/licenses/by/4.0/)&amp;#160;
[![Hacktoberfest 2025 stats](https://img.shields.io/github/hacktoberfest/2025/EbookFoundation/free-programming-books?label=Hacktoberfest+2025)](https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged+created%3A2025-10-01..2025-10-31)

&lt;/div&gt;

Search the list at [https://ebookfoundation.github.io/free-programming-books-search/](https://ebookfoundation.github.io/free-programming-books-search/) [![https://ebookfoundation.github.io/free-programming-books-search/](https://img.shields.io/website?style=flat&amp;logo=www&amp;logoColor=whitesmoke&amp;label=Dynamic%20search%20site&amp;down_color=red&amp;down_message=down&amp;up_color=green&amp;up_message=up&amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books-search%2F)](https://ebookfoundation.github.io/free-programming-books-search/).

This page is available as an easy-to-read website. Access it by clicking on [![https://ebookfoundation.github.io/free-programming-books/](https://img.shields.io/website?style=flat&amp;logo=www&amp;logoColor=whitesmoke&amp;label=Static%20site&amp;down_color=red&amp;down_message=down&amp;up_color=green&amp;up_message=up&amp;url=https%3A%2F%2Febookfoundation.github.io%2Ffree-programming-books%2F)](https://ebookfoundation.github.io/free-programming-books/).

&lt;div align=&quot;center&quot;&gt;
  &lt;form action=&quot;https://ebookfoundation.github.io/free-programming-books-search&quot;&gt;
    &lt;input type=&quot;text&quot; id=&quot;fpbSearch&quot; name=&quot;search&quot; required placeholder=&quot;Search Book or Author&quot;/&gt;
    &lt;label for=&quot;submit&quot;&gt; &lt;/label&gt;
    &lt;input type=&quot;submit&quot; id=&quot;submit&quot; name=&quot;submit&quot; value=&quot;Search&quot; /&gt;
  &lt;/form&gt;
&lt;/div&gt;

## Intro

This list was originally a clone of [StackOverflow - List of Freely Available Programming Books](https://web.archive.org/web/20140606191453/http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books/392926) with contributions from Karan Bhangui and George Stocker.

The list was moved to GitHub by Victor Felder for collaborative updating and maintenance. It has grown to become one of [GitHub&#039;s most popular repositories](https://octoverse.github.com/).

&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;

[![GitHub repo forks](https://img.shields.io/github/forks/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Forks)](https://github.com/EbookFoundation/free-programming-books/network)&amp;#160;
[![GitHub repo stars](https://img.shields.io/github/stars/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Stars)](https://github.com/EbookFoundation/free-programming-books/stargazers)&amp;#160;
[![GitHub repo contributors](https://img.shields.io/github/contributors-anon/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Contributors)](https://github.com/EbookFoundation/free-programming-books/graphs/contributors)    
[![GitHub org sponsors](https://img.shields.io/github/sponsors/EbookFoundation?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Sponsors)](https://github.com/sponsors/EbookFoundation)&amp;#160;
[![GitHub repo watchers](https://img.shields.io/github/watchers/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Watchers)](https://github.com/EbookFoundation/free-programming-books/watchers)&amp;#160;
[![GitHub repo size](https://img.shields.io/github/repo-size/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=whitesmoke&amp;label=Repo%20Size)](https://github.com/EbookFoundation/free-programming-books/archive/refs/heads/main.zip)

&lt;/div&gt;

The [Free Ebook Foundation](https://ebookfoundation.org) now administers the repo, a not-for-profit organization devoted to promoting the creation, distribution, archiving, and sustainability of free ebooks. [Donations](https://ebookfoundation.org/contributions.html) to the Free Ebook Foundation are tax-deductible in the US.


## How To Contribute

Please read [CONTRIBUTING](docs/CONTRIBUTING.md). If you&#039;re new to GitHub, [welcome](docs/HOWTO.md)! Remember to abide by our adapted from ![Contributor Covenant 1.3](https://img.shields.io/badge/Contributor%20Covenant-1.3-4baaaa.svg) [Code of Conduct](docs/CODE_OF_CONDUCT.md) too ([translations](#translations) also available).

Click on these badges to see how you might be able to help:

&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;

[![GitHub repo Issues](https://img.shields.io/github/issues/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=red&amp;label=Issues)](https://github.com/EbookFoundation/free-programming-books/issues)&amp;#160;
[![GitHub repo Good Issues for newbies](https://img.shields.io/github/issues/EbookFoundation/free-programming-books/good%20first%20issue?style=flat&amp;logo=github&amp;logoColor=green&amp;label=Good%20First%20issues)](https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)&amp;#160;
[![GitHub Help Wanted issues](https://img.shields.io/github/issues/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;logo=github&amp;logoColor=b545d1&amp;label=%22Help%20Wanted%22%20issues)](https://github.com/EbookFoundation/free-programming-books/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22)    
[![GitHub repo PRs](https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=orange&amp;label=PRs)](https://github.com/EbookFoundation/free-programming-books/pulls)&amp;#160;
[![GitHub repo Merged PRs](https://img.shields.io/github/issues-search/EbookFoundation/free-programming-books?style=flat&amp;logo=github&amp;logoColor=green&amp;label=Merged%20PRs&amp;query=is%3Amerged)](https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Apr+is%3Amerged)&amp;#160;
[![GitHub Help Wanted PRs](https://img.shields.io/github/issues-pr/EbookFoundation/free-programming-books/help%20wanted?style=flat&amp;logo=github&amp;logoColor=b545d1&amp;label=%22Help%20Wanted%22%20PRs)](https://github.com/EbookFoundation/free-programming-books/pulls?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22)

&lt;/div&gt;

## How To Share

&lt;div align=&quot;left&quot; markdown=&quot;1&quot;&gt;
&lt;a href=&quot;https://www.facebook.com/share.php?u=https%3A%2F%2Fgithub.com%2FEbookFoundation%2Ffree-programming-books&amp;p[images][0]=&amp;p[title]=Free%20Programming%20Books&amp;p[summary]=&quot;&gt;Share on Facebook&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;http://www.linkedin.com/shareArticle?mini=true&amp;url=https://github.com/EbookFoundation/free-programming-books&amp;title=Free%20Programming%20Books&amp;summary=&amp;source=&quot;&gt;Share on LinkedIn&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;https://toot.kytta.dev/?text=https://github.com/EbookFoundation/free-programming-books&quot;&gt;Share on Mastodon/Fediverse&lt;/a&gt;&lt;br&gt;    
&lt;a href=&quot;https://t.me/share/url?url=https://github.com/EbookFoundation/free-programming-books&quot;&gt;Share on Telegram&lt;/a&gt;&lt;br&gt;
&lt;a href=&quot;https://twitter.com/intent/tweet?text=https://github.com/EbookFoundation/free-programming-books%0AFree%20Programming%20Books&quot;&gt;Share on ùïè (Twitter)&lt;/a&gt;&lt;br&gt;
&lt;/div&gt;

## Resources

This project lists books and other resources grouped by genres:

### Books

[English, By Programming Language](books/free-programming-books-langs.md)

[English, By Subject](books/free-programming-books-subjects.md)

#### Other Languages

+ [Arabic / al arabiya / ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](books/free-programming-books-ar.md)
+ [Armenian / ’Ä’°’µ’•÷Ä’•’∂](books/free-programming-books-hy.md)
+ [Azerbaijani / –ê–∑”ô—Ä–±–∞—ò“π–∞–Ω –¥–∏–ª–∏ / ÿ¢ÿ∞ÿ±ÿ®ÿßŸäÿ¨ÿßŸÜÿ¨ÿß ÿØŸäŸÑŸä](books/free-programming-books-az.md)
+ [Bengali / ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ](books/free-programming-books-bn.md)
+ [Bulgarian / –±—ä–ª–≥–∞—Ä—Å–∫–∏](books/free-programming-books-bg.md)
+ [Burmese / ·Äô·Äº·Äî·Ä∫·Äô·Ä¨·Äò·Ä¨·Äû·Ä¨](books/free-programming-books-my.md)
+ [Chinese / ‰∏≠Êñá](books/free-programming-books-zh.md)
+ [Czech / ƒçe≈°tina / ƒçesk√Ω jazyk](books/free-programming-books-cs.md)
+ [Catalan / catalan / catal√†](books/free-programming-books-ca.md)
+ [Danish / dansk](books/free-programming-books-da.md)
+ [Dutch / Nederlands](books/free-programming-books-nl.md)
+ [Estonian / eesti keel](books/free-programming-books-et.md)
+ [Finnish / suomi / suomen kieli](books/free-programming-books-fi.md)
+ [French / fran√ßais](books/free-programming-books-fr.md)
+ [German / Deutsch](books/free-programming-books-de.md)
+ [Greek / ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨](books/free-programming-books-el.md)
+ [Hebrew / ◊¢◊ë◊®◊ô◊™](books/free-programming-books-he.md)
+ [Hindi / ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä](books/free-programming-books-hi.md)
+ [Hungarian / magyar / magyar nyelv](books/free-programming-books-hu.md)
+ [Indonesian / Bahasa Indonesia](books/free-programming-books-id.md)
+ [Italian / italiano](books/free-programming-books-it.md)
+ [Japanese / Êó•Êú¨Ë™û](books/free-programming-books-ja.md)
+ [Korean / ÌïúÍµ≠Ïñ¥](books/free-programming-books-ko.md)
+ [Latvian / Latvie≈°u](books/free-programming-books-lv.md)
+ [Malayalam / ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç](books/free-programming-books-ml.md)
+ [Norwegian / Norsk](books/free-programming-books-no.md)
+ [Persian / Farsi (Iran) / ŸÅÿßÿ±ÿ≥Ÿâ](books/free-programming-books-fa_IR.md)
+ [Polish / polski / jƒôzyk polski / polszczyzna](books/free-programming-books-pl.md)
+ [Portuguese (Brazil)](books/free-programming-books-pt_BR.md)
+ [Portuguese (Portugal)](books/free-programming-books-pt_PT.md)
+ [Romanian (Romania) / limba rom√¢nƒÉ / rom√¢n](books/free-programming-books-ro.md)
+ [Russian / –†—É—Å—Å–∫–∏–π —è–∑—ã–∫](books/free-programming-books-ru.md)
+ [Serbian / —Å—Ä–ø—Å–∫–∏ —ò–µ–∑–∏–∫ / srpski jezik](books/free-programming-books-sr.md)
+ [Slovak / slovenƒçina](books/free-programming-books-sk.md)
+ [Slovenian / Sloven≈°ƒçina](books/free-programming-books-sl.md)
+ [Spanish / espa√±ol / castellano](books/free-programming-books-es.md)
+ [Swedish / Svenska](books/free-programming-books-sv.md)
+ [Tamil / ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç](books/free-programming-books-ta.md)
+ [Telugu / ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å](books/free-programming-books-te.md)
+ [Thai / ‡πÑ‡∏ó‡∏¢](books/free-programming-books-th.md)
+ [Turkish / T√ºrk√ße](books/free-programming-books-tr.md)
+ [Ukrainian / –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞](books/free-programming-books-uk.md)
+ [Urdu / ÿßÿ±ÿØŸà](books/free-programming-books-ur.md)
+ [Vietnamese / Ti·∫øng Vi·ªát](books/free-programming-books-vi.md)

### Cheat Sheets

+ [All Languages](more/free-programming-cheatsheets.md)

### Free Online Courses

+ [Arabic / al arabiya / ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](courses/free-courses-ar.md)
+ [Bengali / ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ](courses/free-courses-bn.md)
+ [Bulgarian / –±—ä–ª–≥–∞—Ä—Å–∫–∏](courses/free-courses-bg.md)
+ [Burmese / ·Äô·Äº·Äî·Ä∫·Äô·Ä¨·Äò·Ä¨·Äû·Ä¨](courses/free-courses-my.md)
+ [Chinese / ‰∏≠Êñá](courses/free-courses-zh.md)
+ [English](courses/free-courses-en.md)
+ [Finnish / suomi / suomen kieli](courses/free-courses-fi.md)
+ [French / fran√ßais](courses/free-courses-fr.md)
+ [German / Deutsch](courses/free-courses-de.md)
+ [Greek / ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨](courses/free-courses-el.md)
+ [Hebrew / ◊¢◊ë◊®◊ô◊™](courses/free-courses-he.md)
+ [Hindi / ‡§π‡§ø‡§Ç‡§¶‡•Ä](courses/free-courses-hi.md)
+ [Indonesian / Bahasa Indonesia](courses/free-courses-id.md)
+ [Italian / italiano](courses/free-courses-it.md)
+ [Japanese / Êó•Êú¨Ë™û](courses/free-courses-ja.md)
+ [Kannada / ‡≤ï‡≤®‡≥ç‡≤®‡≤°](courses/free-courses-kn.md)
+ [Kazakh / “õ–∞–∑–∞“õ—à–∞](courses/free-courses-kk.md)
+ [Khmer / ·ûó·û∂·ûü·û∂·ûÅ·üí·ûò·üÇ·ûö](courses/free-courses-km.md)
+ [Korean / ÌïúÍµ≠Ïñ¥](courses/free-courses-ko.md)
+ [Malayalam / ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç](courses/free-courses-ml.md)
+ [Marathi / ‡§Æ‡§∞‡§æ‡§†‡•Ä](courses/free-courses-mr.md)
+ [Nepali / ‡§®‡•á‡§™‡§æ‡§≤‡•Ä](courses/free-courses-ne.md)
+ [Norwegian / Norsk](courses/free-courses-no.md)
+ [Persian / Farsi (Iran) / ŸÅÿßÿ±ÿ≥Ÿâ](courses/free-courses-fa_IR.md)
+ [Polish / polski / jƒôzyk polski / polszczyzna](courses/free-courses-pl.md)
+ [Portuguese (Brazil)](courses/free-courses-pt_BR.md)
+ [Portuguese (Portugal)](courses/free-courses-pt_PT.md)
+ [Punjabi / ‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä / ŸæŸÜÿ¨ÿßÿ®€å](courses/free-courses-pa.md)
+ [Romanian (Romania) / limba rom√¢nƒÉ / rom√¢n](courses/free-courses-ro.md)
+ [Russian / –†—É—Å—Å–∫–∏–π —è–∑—ã–∫](courses/free-courses-ru.md)
+ [Sinhala / ‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω](courses/free-courses-si.md)
+ [Spanish / espa√±ol / castellano](courses/free-courses-es.md)
+ [Swedish / svenska](courses/free-courses-sv.md)
+ [Tamil / ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç](courses/free-courses-ta.md)
+ [Telugu / ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å](courses/free-courses-te.md)
+ [Thai / ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢](courses/free-courses-th.md)
+ [Turkish / T√ºrk√ße](courses/free-courses-tr.md)
+ [Ukrainian / –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞](courses/free-courses-uk.md)
+ [Urdu / ÿßÿ±ÿØŸà](courses/free-courses-ur.md)
+ [Vietnamese / Ti·∫øng Vi·ªát](courses/free-courses-vi.md)


### Interactive Programming Resources

+ [Chinese / ‰∏≠Êñá](more/free-programming-interactive-tutorials-zh.md)
+ [English](more/free-programming-interactive-tutorials-en.md)
+ [German / Deutsch](more/free-programming-interactive-tutorials-de.md)
+ [Japanese / Êó•Êú¨Ë™û](more/free-programming-interactive-tutorials-ja.md)
+ [Russian / –†—É—Å—Å–∫–∏–π —è–∑—ã–∫](more/free-programming-interactive-tutorials-ru.md)


### Problem Sets and Competitive Programming

+ [Problem Sets](more/problem-sets-competitive-programming.md)


### Podcast - Screencast

Free Podcasts and Screencasts:

+ [Arabic / al Arabiya / ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](casts/free-podcasts-screencasts-ar.md)
+ [Burmese / ·Äô·Äº·Äî·Ä∫·Äô·Ä¨·Äò·Ä¨·Äû·Ä¨](casts/free-podcasts-screencasts-my.md)
+ [Chinese / ‰∏≠Êñá](casts/free-podcasts-screencasts-zh.md)
+ [Czech / ƒçe≈°tina / ƒçesk√Ω jazyk](casts/free-podcasts-screencasts-cs.md)
+ [Dutch / Nederlands](casts/free-podcasts-screencasts-nl.md)
+ [English](casts/free-podcasts-screencasts-en.md)
+ [Finnish / Suomi](casts/free-podcasts-screencasts-fi.md)
+ [French / fran√ßais](casts/free-podcasts-screencasts-fr.md)
+ [German / Deutsch](casts/free-podcasts-screencasts-de.md)
+ [Hebrew / ◊¢◊ë◊®◊ô◊™](casts/free-podcasts-screencasts-he.md)
+ [Indonesian / Bahasa Indonesia](casts/free-podcasts-screencasts-id.md)
+ [Persian / Farsi (Iran) / ŸÅÿßÿ±ÿ≥Ÿâ](casts/free-podcasts-screencasts-fa_IR.md)
+ [Polish / polski / jƒôzyk polski / polszczyzna](casts/free-podcasts-screencasts-pl.md)
+ [Portuguese (Brazil)](casts/free-podcasts-screencasts-pt_BR.md)
+ [Portuguese (Portugal)](casts/free-podcasts-screencasts-pt_PT.md)
+ [Russian / –†—É—Å—Å–∫–∏–π —è–∑—ã–∫](casts/free-podcasts-screencasts-ru.md)
+ [Sinhala / ‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω](casts/free-podcasts-screencasts-si.md)
+ [Spanish / espa√±ol / castellano](casts/free-podcasts-screencasts-es.md)
+ [Swedish / Svenska](casts/free-podcasts-screencasts-sv.md)
+ [Turkish / T√ºrk√ße](casts/free-podcasts-screencasts-tr.md)
+ [Ukrainian / –£–∫—Ä–∞—ó–Ω—Å—å–∫–∞](casts/free-podcasts-screencasts-uk.md)


### Programming Playgrounds

Write, compile, and run your code within a browser. Try it out!

+ [Chinese / ‰∏≠Êñá](more/free-programming-playgrounds-zh.md)
+ [English](more/free-programming-playgrounds.md)
+ [German / Deutsch](more/free-programming-playgrounds-de.md)

## Translations

Volunteers have translated many of our Contributing, How-to, and Code of Conduct documents into languages covered by our lists.

+ English
  + [Code of Conduct](docs/CODE_OF_CONDUCT.md)
  + [Contributing](docs/CONTRIBUTING.md)
  + [How-to](docs/HOWTO.md)
+ ... *[More languages](docs/README.md#translations)* ...

You might notice that there are [some missing translations here](docs/README.md#translations) - perhaps you would like to help out by [contributing a translation](docs/CONTRIBUTING.md#help-out-by-contributing-a-translation)?


## License

Each file included in this repository is licensed under the [CC BY License](LICENSE).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/BitNet]]></title>
            <link>https://github.com/microsoft/BitNet</link>
            <guid>https://github.com/microsoft/BitNet</guid>
            <pubDate>Tue, 03 Feb 2026 00:07:44 GMT</pubDate>
            <description><![CDATA[Official inference framework for 1-bit LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/BitNet">microsoft/BitNet</a></h1>
            <p>Official inference framework for 1-bit LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 27,643</p>
            <p>Forks: 2,242</p>
            <p>Stars today: 99 stars today</p>
            <h2>README</h2><pre># bitnet.cpp
[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)
![version](https://img.shields.io/badge/version-1.0-blue)

[&lt;img src=&quot;./assets/header_model_release.png&quot; alt=&quot;BitNet Model on Hugging Face&quot; width=&quot;800&quot;/&gt;](https://huggingface.co/microsoft/BitNet-b1.58-2B-4T)

Try it out via this [demo](https://bitnet-demo.azurewebsites.net/), or build and run it on your own [CPU](https://github.com/microsoft/BitNet?tab=readme-ov-file#build-from-source) or [GPU](https://github.com/microsoft/BitNet/blob/main/gpu/README.md).

bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support **fast** and **lossless** inference of 1.58-bit models on CPU and GPU (NPU support will coming next).

The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of **1.37x** to **5.07x** on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by **55.4%** to **70.0%**, further boosting overall efficiency. On x86 CPUs, speedups range from **2.37x** to **6.17x** with energy reductions between **71.9%** to **82.2%**. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. Please refer to the [technical report](https://arxiv.org/abs/2410.16144) for more details.

**Latest optimization** introduces parallel kernel implementations with configurable tiling and embedding quantization support, achieving **1.15x to 2.1x** additional speedup over the original implementation across different hardware platforms and workloads. For detailed technical information, see the [optimization guide](src/README.md).

&lt;img src=&quot;./assets/performance.png&quot; alt=&quot;performance_comparison&quot; width=&quot;800&quot;/&gt;


## Demo

A demo of bitnet.cpp running a BitNet b1.58 3B model on Apple M2:

https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1

## What&#039;s New:
- 01/15/2026 [BitNet CPU Inference Optimization](https://github.com/microsoft/BitNet/blob/main/src/README.md) ![NEW](https://img.shields.io/badge/NEW-red)
- 05/20/2025 [BitNet Official GPU inference kernel](https://github.com/microsoft/BitNet/blob/main/gpu/README.md)
- 04/14/2025 [BitNet Official 2B Parameter Model on Hugging Face](https://huggingface.co/microsoft/BitNet-b1.58-2B-4T)
- 02/18/2025 [Bitnet.cpp: Efficient Edge Inference for Ternary LLMs](https://arxiv.org/abs/2502.11880)
- 11/08/2024 [BitNet a4.8: 4-bit Activations for 1-bit LLMs](https://arxiv.org/abs/2411.04965)
- 10/21/2024 [1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs](https://arxiv.org/abs/2410.16144)
- 10/17/2024 bitnet.cpp 1.0 released.
- 03/21/2024 [The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ](https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf)
- 02/27/2024 [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764)
- 10/17/2023 [BitNet: Scaling 1-bit Transformers for Large Language Models](https://arxiv.org/abs/2310.11453)

## Acknowledgements

This project is based on the [llama.cpp](https://github.com/ggerganov/llama.cpp) framework. We would like to thank all the authors for their contributions to the open-source community. Also, bitnet.cpp&#039;s kernels are built on top of the Lookup Table methodologies pioneered in [T-MAC](https://github.com/microsoft/T-MAC/). For inference of general low-bit LLMs beyond ternary models, we recommend using T-MAC.
## Official Models
&lt;table&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th rowspan=&quot;2&quot;&gt;Model&lt;/th&gt;
        &lt;th rowspan=&quot;2&quot;&gt;Parameters&lt;/th&gt;
        &lt;th rowspan=&quot;2&quot;&gt;CPU&lt;/th&gt;
        &lt;th colspan=&quot;3&quot;&gt;Kernel&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;I2_S&lt;/th&gt;
        &lt;th&gt;TL1&lt;/th&gt;
        &lt;th&gt;TL2&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/microsoft/BitNet-b1.58-2B-4T&quot;&gt;BitNet-b1.58-2B-4T&lt;/a&gt;&lt;/td&gt;
        &lt;td rowspan=&quot;2&quot;&gt;2.4B&lt;/td&gt;
        &lt;td&gt;x86&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ARM&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

## Supported Models
‚ùóÔ∏è**We use existing 1-bit LLMs available on [Hugging Face](https://huggingface.co/) to demonstrate the inference capabilities of bitnet.cpp. We hope the release of bitnet.cpp will inspire the development of 1-bit LLMs in large-scale settings in terms of model size and training tokens.**

&lt;table&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th rowspan=&quot;2&quot;&gt;Model&lt;/th&gt;
        &lt;th rowspan=&quot;2&quot;&gt;Parameters&lt;/th&gt;
        &lt;th rowspan=&quot;2&quot;&gt;CPU&lt;/th&gt;
        &lt;th colspan=&quot;3&quot;&gt;Kernel&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;I2_S&lt;/th&gt;
        &lt;th&gt;TL1&lt;/th&gt;
        &lt;th&gt;TL2&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/1bitLLM/bitnet_b1_58-large&quot;&gt;bitnet_b1_58-large&lt;/a&gt;&lt;/td&gt;
        &lt;td rowspan=&quot;2&quot;&gt;0.7B&lt;/td&gt;
        &lt;td&gt;x86&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ARM&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/1bitLLM/bitnet_b1_58-3B&quot;&gt;bitnet_b1_58-3B&lt;/a&gt;&lt;/td&gt;
        &lt;td rowspan=&quot;2&quot;&gt;3.3B&lt;/td&gt;
        &lt;td&gt;x86&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ARM&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens&quot;&gt;Llama3-8B-1.58-100B-tokens&lt;/a&gt;&lt;/td&gt;
        &lt;td rowspan=&quot;2&quot;&gt;8.0B&lt;/td&gt;
        &lt;td&gt;x86&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ARM&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/collections/tiiuae/falcon3-67605ae03578be86e4e87026&quot;&gt;Falcon3 Family&lt;/a&gt;&lt;/td&gt;
        &lt;td rowspan=&quot;2&quot;&gt;1B-10B&lt;/td&gt;
        &lt;td&gt;x86&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ARM&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td rowspan=&quot;2&quot;&gt;&lt;a href=&quot;https://huggingface.co/collections/tiiuae/falcon-edge-series-6804fd13344d6d8a8fa71130&quot;&gt;Falcon-E Family&lt;/a&gt;&lt;/td&gt;
        &lt;td rowspan=&quot;2&quot;&gt;1B-3B&lt;/td&gt;
        &lt;td&gt;x86&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;ARM&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#9989;&lt;/td&gt;
        &lt;td&gt;&amp;#10060;&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;



## Installation

### Requirements
- python&gt;=3.9
- cmake&gt;=3.22
- clang&gt;=18
    - For Windows users, install [Visual Studio 2022](https://visualstudio.microsoft.com/downloads/). In the installer, toggle on at least the following options(this also automatically installs the required additional tools like CMake):
        -  Desktop-development with C++
        -  C++-CMake Tools for Windows
        -  Git for Windows
        -  C++-Clang Compiler for Windows
        -  MS-Build Support for LLVM-Toolset (clang)
    - For Debian/Ubuntu users, you can download with [Automatic installation script](https://apt.llvm.org/)

        `bash -c &quot;$(wget -O - https://apt.llvm.org/llvm.sh)&quot;`
- conda (highly recommend)

### Build from source

&gt; [!IMPORTANT]
&gt; If you are using Windows, please remember to always use a Developer Command Prompt / PowerShell for VS2022 for the following commands. Please refer to the FAQs below if you see any issues.

1. Clone the repo
```bash
git clone --recursive https://github.com/microsoft/BitNet.git
cd BitNet
```
2. Install the dependencies
```bash
# (Recommended) Create a new conda environment
conda create -n bitnet-cpp python=3.9
conda activate bitnet-cpp

pip install -r requirements.txt
```
3. Build the project
```bash
# Manually download the model and run with local path
huggingface-cli download microsoft/BitNet-b1.58-2B-4T-gguf --local-dir models/BitNet-b1.58-2B-4T
python setup_env.py -md models/BitNet-b1.58-2B-4T -q i2_s

```
&lt;pre&gt;
usage: setup_env.py [-h] [--hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}] [--model-dir MODEL_DIR] [--log-dir LOG_DIR] [--quant-type {i2_s,tl1}] [--quant-embd]
                    [--use-pretuned]

Setup the environment for running inference

optional arguments:
  -h, --help            show this help message and exit
  --hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}, -hr {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens,tiiuae/Falcon3-1B-Instruct-1.58bit,tiiuae/Falcon3-3B-Instruct-1.58bit,tiiuae/Falcon3-7B-Instruct-1.58bit,tiiuae/Falcon3-10B-Instruct-1.58bit}
                        Model used for inference
  --model-dir MODEL_DIR, -md MODEL_DIR
                        Directory to save/load the model
  --log-dir LOG_DIR, -ld LOG_DIR
                        Directory to save the logging info
  --quant-type {i2_s,tl1}, -q {i2_s,tl1}
                        Quantization type
  --quant-embd          Quantize the embeddings to f16
  --use-pretuned, -p    Use the pretuned kernel parameters
&lt;/pre&gt;
## Usage
### Basic usage
```bash
# Run inference with the quantized model
python run_inference.py -m models/BitNet-b1.58-2B-4T/ggml-model-i2_s.gguf -p &quot;You are a helpful assistant&quot; -cnv
```
&lt;pre&gt;
usage: run_inference.py [-h] [-m MODEL] [-n N_PREDICT] -p PROMPT [-t THREADS] [-c CTX_SIZE] [-temp TEMPERATURE] [-cnv]

Run inference

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Path to model file
  -n N_PREDICT, --n-predict N_PREDICT
                        Number of tokens to predict when generating text
  -p PROMPT, --prompt PROMPT
                        Prompt to generate text from
  -t THREADS, --threads THREADS
                        Number of threads to use
  -c CTX_SIZE, --ctx-size CTX_SIZE
                        Size of the prompt context
  -temp TEMPERATURE, --temperature TEMPERATURE
                        Temperature, a hyperparameter that controls the randomness of the generated text
  -cnv, --conversation  Whether to enable chat mode or not (for instruct models.)
                        (When this option is turned on, the prompt specified by -p will be used as the system prompt.)
&lt;/pre&gt;

### Benchmark
We provide scripts to run the inference benchmark providing a model.

```  
usage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  
   
Setup the environment for running the inference  
   
required arguments:  
  -m MODEL, --model MODEL  
                        Path to the model file. 
   
optional arguments:  
  -h, --help  
                        Show this help message and exit. 
  -n N_TOKEN, --n-token N_TOKEN  
                        Number of generated tokens. 
  -p N_PROMPT, --n-prompt N_PROMPT  
                        Prompt to generate text from. 
  -t THREADS, --threads THREADS  
                        Number of threads to use. 
```  
   
Here&#039;s a brief explanation of each argument:  
   
- `-m`, `--model`: The path to the model file. This is a required argument that must be provided when running the script.  
- `-n`, `--n-token`: The number of tokens to generate during the inference. It is an optional argument with a default value of 128.  
- `-p`, `--n-prompt`: The number of prompt tokens to use for generating text. This is an optional argument with a default value of 512.  
- `-t`, `--threads`: The number of threads to use for running the inference. It is an optional argument with a default value of 2.  
- `-h`, `--help`: Show the help message and exit. Use this argument to display usage information.  
   
For example:  
   
```sh  
python utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  
```  
   
This command would run the inference benchmark using the model located at `/path/to/model`, generating 200 tokens from a 256 token prompt, utilizing 4 threads.  

For the model layout that do not supported by any public model, we provide scripts to generate a dummy model with the given model layout, and run the benchmark on your machine:

```bash
python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M

# Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate
python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128
```

### Convert from `.safetensors` Checkpoints

```sh
# Prepare the .safetensors model file
huggingface-cli download microsoft/bitnet-b1.58-2B-4T-bf16 --local-dir ./models/bitnet-b1.58-2B-4T-bf16

# Convert to gguf model
python ./utils/convert-helper-bitnet.py ./models/bitnet-b1.58-2B-4T-bf16
```

### FAQ (Frequently Asked Questions)üìå 

#### Q1: The build dies with errors building llama.cpp due to issues with std::chrono in log.cpp?

**A:**
This is an issue introduced in recent version of llama.cpp. Please refer to this [commit](https://github.com/tinglou/llama.cpp/commit/4e3db1e3d78cc1bcd22bcb3af54bd2a4628dd323) in the [discussion](https://github.com/abetlen/llama-cpp-python/issues/1942) to fix this issue.

#### Q2: How to build with clang in conda environment on windows?

**A:** 
Before building the project, verify your clang installation and access to Visual Studio tools by running:
```
clang -v
```

This command checks that you are using the correct version of clang and that the Visual Studio tools are available. If you see an error message such as:
```
&#039;clang&#039; is not recognized as an internal or external command, operable program or batch file.
```

It indicates that your command line window is not properly initialized for Visual Studio tools.

‚Ä¢ If you are using Command Prompt, run:
```
&quot;C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\VsDevCmd.bat&quot; -startdir=none -arch=x64 -host_arch=x64
```

‚Ä¢ If you are using Windows PowerShell, run the following commands:
```
Import-Module &quot;C:\Program Files\Microsoft Visual Studio\2022\Professional\Common7\Tools\Microsoft.VisualStudio.DevShell.dll&quot; Enter-VsDevShell 3f0e31ad -SkipAutomaticLocation -DevCmdArguments &quot;-arch=x64 -host_arch=x64&quot;
```

These steps will initialize your environment and allow you to use the correct Visual Studio tools.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[davila7/claude-code-templates]]></title>
            <link>https://github.com/davila7/claude-code-templates</link>
            <guid>https://github.com/davila7/claude-code-templates</guid>
            <pubDate>Tue, 03 Feb 2026 00:07:43 GMT</pubDate>
            <description><![CDATA[CLI tool for configuring and monitoring Claude Code]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/davila7/claude-code-templates">davila7/claude-code-templates</a></h1>
            <p>CLI tool for configuring and monitoring Claude Code</p>
            <p>Language: Python</p>
            <p>Stars: 19,282</p>
            <p>Forks: 1,794</p>
            <p>Stars today: 113 stars today</p>
            <h2>README</h2><pre>[![npm version](https://img.shields.io/npm/v/claude-code-templates.svg)](https://www.npmjs.com/package/claude-code-templates)
[![npm downloads](https://img.shields.io/npm/dt/claude-code-templates.svg)](https://www.npmjs.com/package/claude-code-templates)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![Sponsored by Z.AI](https://img.shields.io/badge/Sponsored%20by-Z.AI-2563eb?style=flat&amp;logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTEyIDJMMiAyMkgyMkwxMiAyWiIgZmlsbD0id2hpdGUiLz4KPC9zdmc+)](https://z.ai/subscribe?ic=8JVLJQFSKB&amp;utm_source=github&amp;utm_medium=badge&amp;utm_campaign=readme)
[![Neon Open Source Program](https://img.shields.io/badge/Neon-Open%20Source%20Program-00E599?style=flat)](https://get.neon.com/4eCjZDz)
[![Buy Me A Coffee](https://img.shields.io/badge/Buy%20Me%20A%20Coffee-support-yellow?style=flat&amp;logo=buy-me-a-coffee)](https://buymeacoffee.com/daniavila)
[![GitHub stars](https://img.shields.io/github/stars/davila7/claude-code-templates.svg?style=social&amp;label=Star)](https://github.com/davila7/claude-code-templates)

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/15113&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/15113&quot; alt=&quot;davila7%2Fclaude-code-templates | Trendshift&quot; style=&quot;width: 200px; height: 40px;&quot; width=&quot;125&quot; height=&quot;40&quot;/&gt;
  &lt;/a&gt;
  &lt;br /&gt;
  &lt;br /&gt;
  &lt;a href=&quot;https://vercel.com/oss&quot;&gt;
  &lt;img alt=&quot;Vercel OSS Program&quot; src=&quot;https://vercel.com/oss/program-badge.svg&quot; /&gt;
  &lt;/a&gt;
  &amp;nbsp;&amp;nbsp;
  &lt;a href=&quot;https://get.neon.com/4eCjZDz&quot;&gt;
  &lt;img alt=&quot;Neon Open Source Program&quot; src=&quot;https://img.shields.io/badge/Neon-Open%20Source%20Program-00E599?style=for-the-badge&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

---

&lt;div align=&quot;center&quot;&gt;
  &lt;h3&gt;ü§ù Partnership&lt;/h3&gt;
  &lt;p&gt;
    &lt;strong&gt;This project is sponsored by &lt;a href=&quot;https://z.ai&quot; target=&quot;_blank&quot;&gt;Z.AI&lt;/a&gt;&lt;/strong&gt;&lt;br/&gt;
    Supporting Claude Code Templates with the &lt;strong&gt;GLM CODING PLAN&lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
    &lt;a href=&quot;https://z.ai/subscribe?ic=8JVLJQFSKB&amp;utm_source=github&amp;utm_medium=readme&amp;utm_campaign=partnership&quot; target=&quot;_blank&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/badge/Get%2010%25%20OFF-GLM%20Coding%20Plan-2563eb?style=for-the-badge&quot; alt=&quot;GLM Coding Plan&quot; /&gt;
    &lt;/a&gt;
  &lt;/p&gt;
  &lt;p&gt;
    &lt;em&gt;Top-tier coding performance powered by GLM-4.6 ‚Ä¢ Starting at $3/month&lt;/em&gt;&lt;br/&gt;
    &lt;em&gt;Seamlessly integrates with Claude Code, Cursor, Cline &amp; 10+ AI coding tools&lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
    &lt;code&gt;npx claude-code-templates@latest --setting partnerships/glm-coding-plan --yes&lt;/code&gt;
  &lt;/p&gt;
&lt;/div&gt;

---

# Claude Code Templates ([aitmpl.com](https://aitmpl.com))

**Ready-to-use configurations for Anthropic&#039;s Claude Code.** A comprehensive collection of AI agents, custom commands, settings, hooks, external integrations (MCPs), and project templates to enhance your development workflow.

## Browse &amp; Install Components and Templates

**[Browse All Templates](https://aitmpl.com)** - Interactive web interface to explore and install 100+ agents, commands, settings, hooks, and MCPs.

&lt;img width=&quot;1049&quot; height=&quot;855&quot; alt=&quot;Screenshot 2025-08-19 at 08 09 24&quot; src=&quot;https://github.com/user-attachments/assets/e3617410-9b1c-4731-87b7-a3858800b737&quot; /&gt;

## üöÄ Quick Installation

```bash
# Install a complete development stack
npx claude-code-templates@latest --agent development-team/frontend-developer --command testing/generate-tests --mcp development/github-integration --yes

# Browse and install interactively
npx claude-code-templates@latest

# Install specific components
npx claude-code-templates@latest --agent development-tools/code-reviewer --yes
npx claude-code-templates@latest --command performance/optimize-bundle --yes
npx claude-code-templates@latest --setting performance/mcp-timeouts --yes
npx claude-code-templates@latest --hook git/pre-commit-validation --yes
npx claude-code-templates@latest --mcp database/postgresql-integration --yes
```

## What You Get

| Component | Description | Examples |
|-----------|-------------|----------|
| **ü§ñ Agents** | AI specialists for specific domains | Security auditor, React performance optimizer, database architect |
| **‚ö° Commands** | Custom slash commands | `/generate-tests`, `/optimize-bundle`, `/check-security` |
| **üîå MCPs** | External service integrations | GitHub, PostgreSQL, Stripe, AWS, OpenAI |
| **‚öôÔ∏è Settings** | Claude Code configurations | Timeouts, memory settings, output styles |
| **ü™ù Hooks** | Automation triggers | Pre-commit validation, post-completion actions |
| **üé® Skills** | Reusable capabilities with progressive disclosure | PDF processing, Excel automation, custom workflows |

## üõ†Ô∏è Additional Tools

Beyond the template catalog, Claude Code Templates includes powerful development tools:

### üìä Claude Code Analytics
Monitor your AI-powered development sessions in real-time with live state detection and performance metrics.

```bash
npx claude-code-templates@latest --analytics
```

### üí¨ Conversation Monitor  
Mobile-optimized interface to view Claude responses in real-time with secure remote access.

```bash
# Local access
npx claude-code-templates@latest --chats

# Secure remote access via Cloudflare Tunnel
npx claude-code-templates@latest --chats --tunnel
```

### üîç Health Check
Comprehensive diagnostics to ensure your Claude Code installation is optimized.

```bash
npx claude-code-templates@latest --health-check
```

### üîå Plugin Dashboard
View marketplaces, installed plugins, and manage permissions from a unified interface.

```bash
npx claude-code-templates@latest --plugins
```

## üìñ Documentation

**[üìö docs.aitmpl.com](https://docs.aitmpl.com/)** - Complete guides, examples, and API reference for all components and tools.

## Contributing

We welcome contributions! **[Browse existing templates](https://aitmpl.com)** to see what&#039;s available, then check our [contributing guidelines](CONTRIBUTING.md) to add your own agents, commands, MCPs, settings, or hooks.

**Please read our [Code of Conduct](CODE_OF_CONDUCT.md) before contributing.**

## Attribution

This collection includes components from multiple sources:

**Scientific Skills:**
- **[K-Dense-AI/claude-scientific-skills](https://github.com/K-Dense-AI/claude-scientific-skills)** by K-Dense Inc. - MIT License (139 scientific skills for biology, chemistry, medicine, and computational research)

**Official Anthropic:**
- **[anthropics/skills](https://github.com/anthropics/skills)** - Official Anthropic skills (21 skills)
- **[anthropics/claude-code](https://github.com/anthropics/claude-code)** - Development guides and examples (10 skills)

**Community Skills &amp; Agents:**
- **[obra/superpowers](https://github.com/obra/superpowers)** by Jesse Obra - MIT License (14 workflow skills)
- **[alirezarezvani/claude-skills](https://github.com/alirezarezvani/claude-skills)** by Alireza Rezvani - MIT License (36 professional role skills)
- **[wshobson/agents](https://github.com/wshobson/agents)** by wshobson - MIT License (48 agents)
- **NerdyChefsAI Skills** - Community contribution - MIT License (specialized enterprise skills)

**Commands &amp; Tools:**
- **[awesome-claude-code](https://github.com/hesreallyhim/awesome-claude-code)** by hesreallyhim - CC0 1.0 Universal (21 commands)
- **[awesome-claude-skills](https://github.com/mehdi-lamrani/awesome-claude-skills)** - Apache 2.0 (community skills)
- **move-code-quality-skill** - MIT License
- **cocoindex-claude** - Apache 2.0

Each of these resources retains its **original license and attribution**, as defined by their respective authors.
We respect and credit all original creators for their work and contributions to the Claude ecosystem.

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üîó Links

- **üåê Browse Templates**: [aitmpl.com](https://aitmpl.com)
- **üìö Documentation**: [docs.aitmpl.com](https://docs.aitmpl.com)
- **üí¨ Community**: [GitHub Discussions](https://github.com/davila7/claude-code-templates/discussions)
- **üêõ Issues**: [GitHub Issues](https://github.com/davila7/claude-code-templates/issues)

## Stargazers over time
[![Stargazers over time](https://starchart.cc/davila7/claude-code-templates.svg?variant=adaptive)](https://starchart.cc/davila7/claude-code-templates)

---

**‚≠ê Found this useful? Give us a star to support the project!**

[![Buy Me A Coffee](https://img.buymeacoffee.com/button-api/?text=Buy%20me%20a%20coffee&amp;slug=daniavila&amp;button_colour=FFDD00&amp;font_colour=000000&amp;font_family=Cookie&amp;outline_colour=000000&amp;coffee_colour=ffffff)](https://buymeacoffee.com/daniavila)</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[lllyasviel/Fooocus]]></title>
            <link>https://github.com/lllyasviel/Fooocus</link>
            <guid>https://github.com/lllyasviel/Fooocus</guid>
            <pubDate>Tue, 03 Feb 2026 00:07:42 GMT</pubDate>
            <description><![CDATA[Focus on prompting and generating]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lllyasviel/Fooocus">lllyasviel/Fooocus</a></h1>
            <p>Focus on prompting and generating</p>
            <p>Language: Python</p>
            <p>Stars: 47,652</p>
            <p>Forks: 7,774</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[GreyDGL/PentestGPT]]></title>
            <link>https://github.com/GreyDGL/PentestGPT</link>
            <guid>https://github.com/GreyDGL/PentestGPT</guid>
            <pubDate>Tue, 03 Feb 2026 00:07:41 GMT</pubDate>
            <description><![CDATA[Automated Penetration Testing Agentic Framework Powered by Large Language Models]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/GreyDGL/PentestGPT">GreyDGL/PentestGPT</a></h1>
            <p>Automated Penetration Testing Agentic Framework Powered by Large Language Models</p>
            <p>Language: Python</p>
            <p>Stars: 11,352</p>
            <p>Forks: 1,869</p>
            <p>Stars today: 19 stars today</p>
            <h2>README</h2><pre>&lt;!-- Improved compatibility of back to top link: See: https://github.com/othneildrew/Best-README-Template/pull/73 --&gt;
&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;!-- PROJECT SHIELDS --&gt;
[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]
[![MIT License][license-shield]][license-url]
[![Discord][discord-shield]][discord-url]

&lt;!-- PROJECT LOGO --&gt;
&lt;br /&gt;
&lt;div align=&quot;center&quot;&gt;

&lt;h3 align=&quot;center&quot;&gt;PentestGPT&lt;/h3&gt;

  &lt;p align=&quot;center&quot;&gt;
    AI-Powered Autonomous Penetration Testing Agent
    &lt;br /&gt;
    &lt;strong&gt;Published at USENIX Security 2024&lt;/strong&gt;
    &lt;br /&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://pentestgpt.com&quot;&gt;&lt;strong&gt;Official Website: pentestgpt.com ¬ª&lt;/strong&gt;&lt;/a&gt;
    &lt;br /&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://www.usenix.org/conference/usenixsecurity24/presentation/deng&quot;&gt;Research Paper&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://github.com/GreyDGL/PentestGPT/issues&quot;&gt;Report Bug&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://github.com/GreyDGL/PentestGPT/issues&quot;&gt;Request Feature&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

&lt;!-- ABOUT THE PROJECT --&gt;
&lt;a href=&quot;https://trendshift.io/repositories/3770&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/3770&quot; alt=&quot;GreyDGL%2FPentestGPT | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

---

## Demo

### Installation
[![Installation Demo](https://asciinema.org/a/761661.svg)](https://asciinema.org/a/761661)

[Watch on YouTube](https://www.youtube.com/watch?v=RUNmoXqBwVg)

### PentestGPT in Action
[![PentestGPT Demo](https://asciinema.org/a/761663.svg)](https://asciinema.org/a/761663)

[Watch on YouTube](https://www.youtube.com/watch?v=cWi3Yb7RmZA)

---

## What&#039;s New in v1.0 (Agentic Upgrade)

- **Autonomous Agent** - Agentic pipeline for intelligent, autonomous penetration testing
- **Session Persistence** - Save and resume penetration testing sessions
- **Docker-First** - Isolated, reproducible environment with security tools pre-installed

&gt; **In Progress**: Multi-model support for OpenAI, Gemini, and other LLM providers

---

## Features

- **AI-Powered Challenge Solver** - Leverages LLM advanced reasoning to perform penetration testing and CTFs
- **Live Walkthrough** - Tracks steps in real-time as the agent works through challenges
- **Multi-Category Support** - Web, Crypto, Reversing, Forensics, PWN, Privilege Escalation
- **Real-Time Feedback** - Watch the AI work with live activity updates
- **Extensible Architecture** - Clean, modular design ready for future enhancements

---

## Quick Start

### Prerequisites

- **Docker** (required) - [Install Docker](https://docs.docker.com/get-docker/)
- **LLM Provider** (choose one):
  - Anthropic API Key from [console.anthropic.com](https://console.anthropic.com/)
  - Claude OAuth Login (requires Claude subscription)
  - OpenRouter for alternative models at [openrouter.ai](https://openrouter.ai/keys)
  - [Tutorial: Using Local Models with Claude Code](https://docs.google.com/document/d/1ixK7x-wlr5t5TYZJdfm75UME5KnPCpS46boLkUXKg1w/edit?usp=sharing)


### Installation

```bash
# Clone and build
git clone --recurse-submodules https://github.com/GreyDGL/PentestGPT.git
cd PentestGPT
make install

# Configure authentication (first time only)
make config

# Connect to container
make connect
```

&gt; **Note**: The `--recurse-submodules` flag downloads the benchmark suite. If you already cloned without it, run: `git submodule update --init --recursive`

### Try a Benchmark

```bash
cd benchmark/standalone-xbow-benchmark-runner
python3 run_benchmarks.py --range 1-1 --pattern-flag
```

See [Benchmark Documentation](benchmark/README.md) for detailed usage.

### Commands Reference

| Command | Description |
|---------|-------------|
| `make install` | Build the Docker image |
| `make config` | Configure API key (first-time setup) |
| `make connect` | Connect to container (main entry point) |
| `make stop` | Stop container (config persists) |
| `make clean-docker` | Remove everything including config |


---

## Usage

```bash
# Interactive TUI mode (default)
pentestgpt --target 10.10.11.234

# Non-interactive mode
pentestgpt --target 10.10.11.100 --non-interactive

# With challenge context
pentestgpt --target 10.10.11.50 --instruction &quot;WordPress site, focus on plugin vulnerabilities&quot;
```

**Keyboard Shortcuts:** `F1` Help | `Ctrl+P` Pause/Resume | `Ctrl+Q` Quit

---

## Using Local LLMs

PentestGPT supports routing requests to local LLM servers (LM Studio, Ollama, text-generation-webui, etc.) running on your host machine.

### Prerequisites

- Local LLM server with an OpenAI-compatible API endpoint
  - **LM Studio**: Enable server mode (default port 1234)
  - **Ollama**: Run `ollama serve` (default port 11434)

### Setup

```bash
# Configure PentestGPT for local LLM
make config
# Select option 4: Local LLM

# Start your local LLM server on the host machine
# Then connect to the container
make connect
```

### Customizing Models

Edit `scripts/ccr-config-template.json` to customize:

- **`localLLM.api_base_url`**: Your LLM server URL (default: `host.docker.internal:1234`)
- **`localLLM.models`**: Available model names on your server
- **Router section**: Which models handle which operations

| Route | Purpose | Default Model |
|-------|---------|---------------|
| `default` | General tasks | openai/gpt-oss-20b |
| `background` | Background operations | openai/gpt-oss-20b |
| `think` | Reasoning-heavy tasks | qwen/qwen3-coder-30b |
| `longContext` | Large context handling | qwen/qwen3-coder-30b |
| `webSearch` | Web search operations | openai/gpt-oss-20b |

### Troubleshooting

- **Connection refused**: Ensure your LLM server is running and listening on the configured port
- **Docker networking**: Use `host.docker.internal` (not `localhost`) to access host services from Docker
- **Check CCR logs**: Inside the container, run `cat /tmp/ccr.log`

---

## Telemetry

PentestGPT collects anonymous usage data to help improve the tool. This data is sent to our [Langfuse](https://langfuse.com) project and includes:
- Session metadata (target type, duration, completion status)
- Tool execution patterns (which tools are used, not the actual commands)
- Flag detection events (that a flag was found, not the flag content)

**No sensitive data is collected** - command outputs, credentials, or actual flag values are never transmitted.

### Opting Out

```bash
# Via command line flag
pentestgpt --target 10.10.11.234 --no-telemetry

# Via environment variable
export LANGFUSE_ENABLED=false
```

---

## Benchmarks

PentestGPT includes 104 XBOW validation benchmarks for comprehensive testing and evaluation.

```bash
cd benchmark/standalone-xbow-benchmark-runner

python3 run_benchmarks.py --range 1-10 --pattern-flag   # Run benchmarks 1-10
python3 run_benchmarks.py --all --pattern-flag          # Run all 104 benchmarks
python3 run_benchmarks.py --retry-failed                # Retry failed benchmarks
python3 run_benchmarks.py --dry-run --range 1-5         # Preview without executing
```

### Performance Highlights

PentestGPT achieved an **86.5% success rate** (90/104 benchmarks) on the XBOW validation suite:

- **Cost**: Average $1.11, Median $0.42 per successful benchmark
- **Time**: Average 6.1 minutes, Median 3.3 minutes per successful benchmark
- **Success rates by difficulty**:
  - Level 1: 91.1%
  - Level 2: 74.5%
  - Level 3: 62.5%

For detailed benchmark results, analysis, and automated testing instructions, see the **[Benchmark Documentation](benchmark/README.md)**.

---

## Legacy Version

The previous multi-LLM version (v0.15) supporting OpenAI, Gemini, Deepseek, and Ollama is archived in [`legacy/`](legacy/):

```bash
cd legacy &amp;&amp; pip install -e . &amp;&amp; pentestgpt --reasoning gpt-4o
```

---

## Citation

If you use PentestGPT in your research, please cite our paper:

```bibtex
@inproceedings{299699,
  author = {Gelei Deng and Yi Liu and V√≠ctor Mayoral-Vilches and Peng Liu and Yuekang Li and Yuan Xu and Tianwei Zhang and Yang Liu and Martin Pinzger and Stefan Rass},
  title = {{PentestGPT}: Evaluating and Harnessing Large Language Models for Automated Penetration Testing},
  booktitle = {33rd USENIX Security Symposium (USENIX Security 24)},
  year = {2024},
  isbn = {978-1-939133-44-1},
  address = {Philadelphia, PA},
  pages = {847--864},
  url = {https://www.usenix.org/conference/usenixsecurity24/presentation/deng},
  publisher = {USENIX Association},
  month = aug
}
```

---

## License

Distributed under the MIT License. See `LICENSE.md` for more information.

**Disclaimer**: This tool is for educational purposes and authorized security testing only. The authors do not condone any illegal use. Use at your own risk.

---

## Acknowledgments

- Research supported by [Quantstamp](https://www.quantstamp.com/) and [NTU Singapore](https://www.ntu.edu.sg/)

&lt;p align=&quot;right&quot;&gt;(&lt;a href=&quot;#readme-top&quot;&gt;back to top&lt;/a&gt;)&lt;/p&gt;

&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt;
[contributors-shield]: https://img.shields.io/github/contributors/GreyDGL/PentestGPT.svg?style=for-the-badge
[contributors-url]: https://github.com/GreyDGL/PentestGPT/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/GreyDGL/PentestGPT.svg?style=for-the-badge
[forks-url]: https://github.com/GreyDGL/PentestGPT/network/members
[stars-shield]: https://img.shields.io/github/stars/GreyDGL/PentestGPT.svg?style=for-the-badge
[stars-url]: https://github.com/GreyDGL/PentestGPT/stargazers
[issues-shield]: https://img.shields.io/github/issues/GreyDGL/PentestGPT.svg?style=for-the-badge
[issues-url]: https://github.com/GreyDGL/PentestGPT/issues
[license-shield]: https://img.shields.io/github/license/GreyDGL/PentestGPT.svg?style=for-the-badge
[license-url]: https://github.com/GreyDGL/PentestGPT/blob/master/LICENSE.md
[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&amp;logo=linkedin&amp;colorB=555
[linkedin-url]: https://www.linkedin.com/in/gelei-deng-225a10112/
[linkedin-url2]: https://www.linkedin.com/in/vmayoral/
[discord-shield]: https://dcbadge.vercel.app/api/server/eC34CEfEkK
[discord-url]: https://discord.gg/eC34CEfEkK</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[langchain-ai/open_deep_research]]></title>
            <link>https://github.com/langchain-ai/open_deep_research</link>
            <guid>https://github.com/langchain-ai/open_deep_research</guid>
            <pubDate>Tue, 03 Feb 2026 00:07:40 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/langchain-ai/open_deep_research">langchain-ai/open_deep_research</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 10,443</p>
            <p>Forks: 1,531</p>
            <p>Stars today: 39 stars today</p>
            <h2>README</h2><pre># üî¨ Open Deep Research

&lt;img width=&quot;1388&quot; height=&quot;298&quot; alt=&quot;full_diagram&quot; src=&quot;https://github.com/user-attachments/assets/12a2371b-8be2-4219-9b48-90503eb43c69&quot; /&gt;

Deep research has broken out as one of the most popular agent applications. This is a simple, configurable, fully open source deep research agent that works across many model providers, search tools, and MCP servers. It&#039;s performance is on par with many popular deep research agents ([see Deep Research Bench leaderboard](https://huggingface.co/spaces/Ayanami0730/DeepResearch-Leaderboard)).

&lt;img width=&quot;817&quot; height=&quot;666&quot; alt=&quot;Screenshot 2025-07-13 at 11 21 12‚ÄØPM&quot; src=&quot;https://github.com/user-attachments/assets/052f2ed3-c664-4a4f-8ec2-074349dcaa3f&quot; /&gt;

### üî• Recent Updates

**August 14, 2025**: See our free course [here](https://academy.langchain.com/courses/deep-research-with-langgraph) (and course repo [here](https://github.com/langchain-ai/deep_research_from_scratch)) on building open deep research.

**August 7, 2025**: Added GPT-5 and updated the Deep Research Bench evaluation w/ GPT-5 results.

**August 2, 2025**: Achieved #6 ranking on the [Deep Research Bench Leaderboard](https://huggingface.co/spaces/Ayanami0730/DeepResearch-Leaderboard) with an overall score of 0.4344. 

**July 30, 2025**: Read about the evolution from our original implementations to the current version in our [blog post](https://rlancemartin.github.io/2025/07/30/bitter_lesson/).

**July 16, 2025**: Read more in our [blog](https://blog.langchain.com/open-deep-research/) and watch our [video](https://www.youtube.com/watch?v=agGiWUpxkhg) for a quick overview.

### üöÄ Quickstart

1. Clone the repository and activate a virtual environment:
```bash
git clone https://github.com/langchain-ai/open_deep_research.git
cd open_deep_research
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

2. Install dependencies:
```bash
uv sync
# or
uv pip install -r pyproject.toml
```

3. Set up your `.env` file to customize the environment variables (for model selection, search tools, and other configuration settings):
```bash
cp .env.example .env
```

4. Launch agent with the LangGraph server locally:

```bash
# Install dependencies and start the LangGraph server
uvx --refresh --from &quot;langgraph-cli[inmem]&quot; --with-editable . --python 3.11 langgraph dev --allow-blocking
```

This will open the LangGraph Studio UI in your browser.

```
- üöÄ API: http://127.0.0.1:2024
- üé® Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024
- üìö API Docs: http://127.0.0.1:2024/docs
```

Ask a question in the `messages` input field and click `Submit`. Select different configuration in the &quot;Manage Assistants&quot; tab.

### ‚öôÔ∏è Configurations

#### LLM :brain:

Open Deep Research supports a wide range of LLM providers via the [init_chat_model() API](https://python.langchain.com/docs/how_to/chat_models_universal_init/). It uses LLMs for a few different tasks. See the below model fields in the [configuration.py](https://github.com/langchain-ai/open_deep_research/blob/main/src/open_deep_research/configuration.py) file for more details. This can be accessed via the LangGraph Studio UI. 

- **Summarization** (default: `openai:gpt-4.1-mini`): Summarizes search API results
- **Research** (default: `openai:gpt-4.1`): Power the search agent
- **Compression** (default: `openai:gpt-4.1`): Compresses research findings
- **Final Report Model** (default: `openai:gpt-4.1`): Write the final report

&gt; Note: the selected model will need to support [structured outputs](https://python.langchain.com/docs/integrations/chat/) and [tool calling](https://python.langchain.com/docs/how_to/tool_calling/).

&gt; Note: For OpenRouter: Follow [this guide](https://github.com/langchain-ai/open_deep_research/issues/75#issuecomment-2811472408) and for local models via Ollama  see [setup instructions](https://github.com/langchain-ai/open_deep_research/issues/65#issuecomment-2743586318).

#### Search API :mag:

Open Deep Research supports a wide range of search tools. By default it uses the [Tavily](https://www.tavily.com/) search API. Has full MCP compatibility and work native web search for Anthropic and OpenAI. See the `search_api` and `mcp_config` fields in the [configuration.py](https://github.com/langchain-ai/open_deep_research/blob/main/src/open_deep_research/configuration.py) file for more details. This can be accessed via the LangGraph Studio UI. 

#### Other 

See the fields in the [configuration.py](https://github.com/langchain-ai/open_deep_research/blob/main/src/open_deep_research/configuration.py) for various other settings to customize the behavior of Open Deep Research. 

### üìä Evaluation

Open Deep Research is configured for evaluation with [Deep Research Bench](https://huggingface.co/spaces/Ayanami0730/DeepResearch-Leaderboard). This benchmark has 100 PhD-level research tasks (50 English, 50 Chinese), crafted by domain experts across 22 fields (e.g., Science &amp; Tech, Business &amp; Finance) to mirror real-world deep-research needs. It has 2 evaluation metrics, but the leaderboard is based on the RACE score. This uses LLM-as-a-judge (Gemini) to evaluate research reports against a golden set of reports compiled by experts across a set of metrics.

#### Usage

&gt; Warning: Running across the 100 examples can cost ~$20-$100 depending on the model selection.

The dataset is available on [LangSmith via this link](https://smith.langchain.com/public/c5e7a6ad-fdba-478c-88e6-3a388459ce8b/d). To kick off evaluation, run the following command:

```bash
# Run comprehensive evaluation on LangSmith datasets
python tests/run_evaluate.py
```

This will provide a link to a LangSmith experiment, which will have a name `YOUR_EXPERIMENT_NAME`. Once this is done, extract the results to a JSONL file that can be submitted to the Deep Research Bench.

```bash
python tests/extract_langsmith_data.py --project-name &quot;YOUR_EXPERIMENT_NAME&quot; --model-name &quot;you-model-name&quot; --dataset-name &quot;deep_research_bench&quot;
```

This creates `tests/expt_results/deep_research_bench_model-name.jsonl` with the required format. Move the generated JSONL file to a local clone of the Deep Research Bench repository and follow their [Quick Start guide](https://github.com/Ayanami0730/deep_research_bench?tab=readme-ov-file#quick-start) for evaluation submission.

#### Results 

| Name | Commit | Summarization | Research | Compression | Total Cost | Total Tokens | RACE Score | Experiment |
|------|--------|---------------|----------|-------------|------------|--------------|------------|------------|
| GPT-5 | [ca3951d](https://github.com/langchain-ai/open_deep_research/pull/168/commits) | openai:gpt-4.1-mini | openai:gpt-5 | openai:gpt-4.1 |  | 204,640,896 | 0.4943 | [Link](https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/6e4766ca-613c-4bda-8bde-f64f0422bbf3/compare?selectedSessions=4d5941c8-69ce-4f3d-8b3e-e3c99dfbd4cc&amp;baseline=undefined) |
| Defaults | [6532a41](https://github.com/langchain-ai/open_deep_research/commit/6532a4176a93cc9bb2102b3d825dcefa560c85d9) | openai:gpt-4.1-mini | openai:gpt-4.1 | openai:gpt-4.1 | $45.98 | 58,015,332 | 0.4309 | [Link](https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/6e4766ca-6[‚Ä¶]ons=cf4355d7-6347-47e2-a774-484f290e79bc&amp;baseline=undefined) |
| Claude Sonnet 4 | [f877ea9](https://github.com/langchain-ai/open_deep_research/pull/163/commits/f877ea93641680879c420ea991e998b47aab9bcc) | openai:gpt-4.1-mini | anthropic:claude-sonnet-4-20250514 | openai:gpt-4.1 | $187.09 | 138,917,050 | 0.4401 | [Link](https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/6e4766ca-6[‚Ä¶]ons=04f6002d-6080-4759-bcf5-9a52e57449ea&amp;baseline=undefined) |
| Deep Research Bench Submission | [c0a160b](https://github.com/langchain-ai/open_deep_research/commit/c0a160b57a9b5ecd4b8217c3811a14d8eff97f72) | openai:gpt-4.1-nano | openai:gpt-4.1 | openai:gpt-4.1 | $87.83 | 207,005,549 | 0.4344 | [Link](https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/6e4766ca-6[‚Ä¶]ons=e6647f74-ad2f-4cb9-887e-acb38b5f73c0&amp;baseline=undefined) |

### üöÄ Deployments and Usage

#### LangGraph Studio

Follow the [quickstart](#-quickstart) to start LangGraph server locally and test the agent out on LangGraph Studio.

#### Hosted deployment
 
You can easily deploy to [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/#deployment-options). 

#### Open Agent Platform

Open Agent Platform (OAP) is a UI from which non-technical users can build and configure their own agents. OAP is great for allowing users to configure the Deep Researcher with different MCP tools and search APIs that are best suited to their needs and the problems that they want to solve.

We&#039;ve deployed Open Deep Research to our public demo instance of OAP. All you need to do is add your API Keys, and you can test out the Deep Researcher for yourself! Try it out [here](https://oap.langchain.com)

You can also deploy your own instance of OAP, and make your own custom agents (like Deep Researcher) available on it to your users.
1. [Deploy Open Agent Platform](https://docs.oap.langchain.com/quickstart)
2. [Add Deep Researcher to OAP](https://docs.oap.langchain.com/setup/agents)

### Legacy Implementations üèõÔ∏è

The `src/legacy/` folder contains two earlier implementations that provide alternative approaches to automated research. They are less performant than the current implementation, but provide alternative ideas understanding the different approaches to deep research.

#### 1. Workflow Implementation (`legacy/graph.py`)
- **Plan-and-Execute**: Structured workflow with human-in-the-loop planning
- **Sequential Processing**: Creates sections one by one with reflection
- **Interactive Control**: Allows feedback and approval of report plans
- **Quality Focused**: Emphasizes accuracy through iterative refinement

#### 2. Multi-Agent Implementation (`legacy/multi_agent.py`)  
- **Supervisor-Researcher Architecture**: Coordinated multi-agent system
- **Parallel Processing**: Multiple researchers work simultaneously
- **Speed Optimized**: Faster report generation through concurrency
- **MCP Support**: Extensive Model Context Protocol integration
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jingyaogong/minimind]]></title>
            <link>https://github.com/jingyaogong/minimind</link>
            <guid>https://github.com/jingyaogong/minimind</guid>
            <pubDate>Tue, 03 Feb 2026 00:07:39 GMT</pubDate>
            <description><![CDATA[üöÄüöÄ „ÄåÂ§ßÊ®°Âûã„Äç2Â∞èÊó∂ÂÆåÂÖ®‰ªé0ËÆ≠ÁªÉ26MÁöÑÂ∞èÂèÇÊï∞GPTÔºÅüåè Train a 26M-parameter GPT from scratch in just 2h!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jingyaogong/minimind">jingyaogong/minimind</a></h1>
            <p>üöÄüöÄ „ÄåÂ§ßÊ®°Âûã„Äç2Â∞èÊó∂ÂÆåÂÖ®‰ªé0ËÆ≠ÁªÉ26MÁöÑÂ∞èÂèÇÊï∞GPTÔºÅüåè Train a 26M-parameter GPT from scratch in just 2h!</p>
            <p>Language: Python</p>
            <p>Stars: 38,557</p>
            <p>Forks: 4,629</p>
            <p>Stars today: 136 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

![logo](./images/logo.png)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

![visitors](https://visitor-badge.laobi.icu/badge?page_id=jingyaogong/minimind)
[![GitHub Repo stars](https://img.shields.io/github/stars/jingyaogong/minimind?style=social)](https://github.com/jingyaogong/minimind/stargazers)
[![GitHub Code License](https://img.shields.io/github/license/jingyaogong/minimind)](LICENSE)
[![GitHub last commit](https://img.shields.io/github/last-commit/jingyaogong/minimind)](https://github.com/jingyaogong/minimind/commits/master)
[![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)](https://github.com/jingyaogong/minimind/pulls)
[![Collection](https://img.shields.io/badge/ü§ó-MiniMind%20%20Collection-blue)](https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

![GitHub Trend](https://trendshift.io/api/badge/repositories/12586)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;h3&gt;&quot;Â§ßÈÅìËá≥ÁÆÄ&quot;&lt;/h3&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

‰∏≠Êñá | [English](./README_en.md)

&lt;/div&gt;

* Ê≠§ÂºÄÊ∫êÈ°πÁõÆÊó®Âú®ÂÆåÂÖ®‰ªé0ÂºÄÂßãÔºå‰ªÖÁî®3ÂùóÈí±ÊàêÊú¨ + 2Â∞èÊó∂ÔºÅÂç≥ÂèØËÆ≠ÁªÉÂá∫‰ªÖ‰∏∫25.8MÁöÑË∂ÖÂ∞èËØ≠Ë®ÄÊ®°Âûã**MiniMind**„ÄÇ
* **MiniMind**Á≥ªÂàóÊûÅÂÖ∂ËΩªÈáèÔºåÊúÄÂ∞èÁâàÊú¨‰ΩìÁßØÊòØ GPT-3 ÁöÑ $\frac{1}{7000}$ÔºåÂäõÊ±ÇÂÅöÂà∞ÊúÄÊôÆÈÄöÁöÑ‰∏™‰∫∫GPU‰πüÂèØÂø´ÈÄüËÆ≠ÁªÉ„ÄÇ
* È°πÁõÆÂêåÊó∂ÂºÄÊ∫ê‰∫ÜÂ§ßÊ®°ÂûãÁöÑÊûÅÁÆÄÁªìÊûÑ-ÂåÖÂê´ÊãìÂ±ïÂÖ±‰∫´Ê∑∑Âêà‰∏ìÂÆ∂(MoE)„ÄÅÊï∞ÊçÆÈõÜÊ∏ÖÊ¥ó„ÄÅÈ¢ÑËÆ≠ÁªÉ(Pretrain)„ÄÅÁõëÁù£ÂæÆË∞É(SFT)„ÄÅLoRAÂæÆË∞É„ÄÅÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñ(DPO)„ÄÅÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉ(RLAIF: PPO/GRPOÁ≠â)„ÄÅÊ®°ÂûãËí∏È¶èÁ≠âÂÖ®ËøáÁ®ã‰ª£Á†Å„ÄÇ
* **MiniMind**ÂêåÊó∂ÊãìÂ±ï‰∫ÜËßÜËßâÂ§öÊ®°ÊÄÅÁöÑVLM: [MiniMind-V](https://github.com/jingyaogong/minimind-v)„ÄÇ
* È°πÁõÆÊâÄÊúâÊ†∏ÂøÉÁÆóÊ≥ï‰ª£Á†ÅÂùá‰ªé0‰ΩøÁî®PyTorchÂéüÁîüÈáçÊûÑÔºÅ‰∏ç‰æùËµñÁ¨¨‰∏âÊñπÂ∫ìÊèê‰æõÁöÑÊäΩË±°Êé•Âè£„ÄÇ
* Ëøô‰∏ç‰ªÖÊòØÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÖ®Èò∂ÊÆµÂºÄÊ∫êÂ§çÁé∞Ôºå‰πüÊòØ‰∏Ä‰∏™ÂÖ•Èó®LLMÁöÑÊïôÁ®ã„ÄÇ
* Â∏åÊúõÊ≠§È°πÁõÆËÉΩ‰∏∫ÊâÄÊúâ‰∫∫Êèê‰æõ‰∏Ä‰∏™ÊäõÁ†ñÂºïÁéâÁöÑÁ§∫‰æãÔºå‰∏ÄËµ∑ÊÑüÂèóÂàõÈÄ†ÁöÑ‰πêË∂£ÔºÅÊé®Âä®Êõ¥ÂπøÊ≥õAIÁ§æÂå∫ÁöÑËøõÊ≠•ÔºÅ

&gt; ‰∏∫Èò≤Ê≠¢ËØØËß£Ôºå‚Äú2Â∞èÊó∂‚Äù Âü∫‰∫éNVIDIA 3090Á°¨‰ª∂ËÆæÂ§áÔºàÂçïÂç°ÔºâÊµãËØïÔºå‚Äú3ÂùóÈí±‚ÄùÊåáGPUÊúçÂä°Âô®ÁßüÁî®ÊàêÊú¨ÔºåÂÖ∑‰ΩìËßÑÊ†ºËØ¶ÊÉÖËßÅ‰∏ãÊñá„ÄÇ

---


&lt;div align=&quot;center&quot;&gt;

![minimind2](./images/minimind2.gif)

[üîóüçìÊé®ÁêÜÊ®°Âûã](https://www.modelscope.cn/studios/gongjy/MiniMind-Reasoning) | [üîóü§ñÂ∏∏ËßÑÊ®°Âûã](https://www.modelscope.cn/studios/gongjy/MiniMind) | [üîóüéûÔ∏èËßÜÈ¢ë‰ªãÁªç](https://www.bilibili.com/video/BV12dHPeqE72/?share_source=copy_web&amp;vd_source=670c2504f88726f8cf4a21ef6147c0e8)


&lt;div align=&quot;center&quot;&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5&quot; style=&quot;text-decoration: none;&quot;&gt;
          &lt;img src=&quot;./images/and_huggingface.png&quot; alt=&quot;Hugging Face Logo&quot; style=&quot;vertical-align: middle; width: auto; max-width: 100%;&quot; /&gt;
        &lt;/a&gt;
      &lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://www.modelscope.cn/profile/gongjy&quot; style=&quot;text-decoration: none;&quot;&gt;
          &lt;img src=&quot;./images/and_modelscope.png&quot; alt=&quot;ModelScope Logo&quot; style=&quot;vertical-align: middle; width: auto; max-width: 100%;&quot; /&gt;
        &lt;/a&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;


&lt;/div&gt;

# üìå Introduction

Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLarge Language Model, LLMÔºâÁöÑÂá∫Áé∞ÂºïÂèë‰∫ÜÂÖ®‰∏ñÁïåÂØπAIÁöÑÁ©∫ÂâçÂÖ≥Ê≥®„ÄÇ
Êó†ËÆ∫ÊòØChatGPT„ÄÅDeepSeekËøòÊòØQwenÔºåÈÉΩ‰ª•ÂÖ∂ÊÉäËâ≥ÁöÑÊïàÊûú‰ª§‰∫∫Âèπ‰∏∫ËßÇÊ≠¢„ÄÇ
ÁÑ∂ËÄåÔºåÂä®ËæÑÊï∞Áôæ‰∫øÂèÇÊï∞ÁöÑÂ∫ûÂ§ßËßÑÊ®°Ôºå‰ΩøÂæóÂÆÉ‰ª¨ÂØπ‰∏™‰∫∫ËÆæÂ§áËÄåË®Ä‰∏ç‰ªÖÈöæ‰ª•ËÆ≠ÁªÉÔºåÁîöËá≥ËøûÈÉ®ÁΩ≤ÈÉΩÊòæÂæóÈÅ•‰∏çÂèØÂèä„ÄÇ
ÊâìÂºÄÂ§ßÊ®°ÂûãÁöÑ‚ÄúÈªëÁõíÂ≠ê‚ÄùÔºåÊé¢Á¥¢ÂÖ∂ÂÜÖÈÉ®Ëøê‰ΩúÊú∫Âà∂ÔºåÂ§ö‰πà‰ª§‰∫∫ÂøÉÊΩÆÊæéÊπÉÔºÅ
ÈÅóÊÜæÁöÑÊòØÔºå99%ÁöÑÊé¢Á¥¢Âè™ËÉΩÊ≠¢Ê≠•‰∫é‰ΩøÁî®LoRAÁ≠âÊäÄÊúØÂØπÁé∞ÊúâÂ§ßÊ®°ÂûãËøõË°åÂ∞ëÈáèÂæÆË∞ÉÔºåÂ≠¶‰π†‰∏Ä‰∫õÊñ∞Êåá‰ª§Êàñ‰ªªÂä°„ÄÇ
ËøôÂ∞±Â•ΩÊØîÊïôÁâõÈ°øÂ¶Ç‰Ωï‰ΩøÁî®21‰∏ñÁ∫™ÁöÑÊô∫ËÉΩÊâãÊú∫‚Äî‚ÄîËôΩÁÑ∂ÊúâË∂£ÔºåÂç¥ÂÆåÂÖ®ÂÅèÁ¶ª‰∫ÜÁêÜËß£Áâ©ÁêÜÊú¨Ë¥®ÁöÑÂàùË°∑„ÄÇ
‰∏éÊ≠§ÂêåÊó∂ÔºåÁ¨¨‰∏âÊñπÁöÑÂ§ßÊ®°ÂûãÊ°ÜÊû∂ÂíåÂ∑•ÂÖ∑Â∫ìÔºåÂ¶Çtransformers+trlÔºåÂá†‰πéÂè™Êö¥Èú≤‰∫ÜÈ´òÂ∫¶ÊäΩË±°ÁöÑÊé•Âè£„ÄÇ
ÈÄöËøáÁü≠Áü≠10Ë°å‰ª£Á†ÅÔºåÂ∞±ËÉΩÂÆåÊàê‚ÄúÂä†ËΩΩÊ®°Âûã+Âä†ËΩΩÊï∞ÊçÆÈõÜ+Êé®ÁêÜ+Âº∫ÂåñÂ≠¶‰π†‚ÄùÁöÑÂÖ®ÊµÅÁ®ãËÆ≠ÁªÉ„ÄÇ
ËøôÁßçÈ´òÊïàÁöÑÂ∞ÅË£ÖÂõ∫ÁÑ∂‰æøÂà©Ôºå‰ΩÜ‰πüÂÉè‰∏ÄÊû∂È´òÈÄüÈ£ûËàπÔºåÂ∞ÜÂºÄÂèëËÄÖ‰∏éÂ∫ïÂ±ÇÂÆûÁé∞ÈöîÁ¶ªÂºÄÊù•ÔºåÈòªÁ¢ç‰∫ÜÊ∑±ÂÖ•Êé¢Á©∂LLMÊ†∏ÂøÉ‰ª£Á†ÅÁöÑÊú∫‰ºö„ÄÇ
ÁÑ∂ËÄåÔºå‚ÄúÁî®‰πêÈ´òÊãºÂá∫‰∏ÄÊû∂È£ûÊú∫ÔºåËøúÊØîÂùêÂú®Â§¥Á≠âËà±ÈáåÈ£ûË°åÊõ¥ËÆ©‰∫∫ÂÖ¥Â•ãÔºÅ‚Äù„ÄÇ
Êõ¥Á≥üÁ≥ïÁöÑÊòØÔºå‰∫íËÅîÁΩë‰∏äÂÖÖÊñ•ÁùÄÂ§ßÈáè‰ªòË¥πËØæÁ®ãÂíåËê•ÈîÄÂè∑Ôºå‰ª•ÊºèÊ¥ûÁôæÂá∫„ÄÅ‰∏ÄÁü•ÂçäËß£ÁöÑÂÜÖÂÆπÊé®ÈîÄAIÊïôÁ®ã„ÄÇ
Ê≠£Âõ†Â¶ÇÊ≠§ÔºåÊú¨È°πÁõÆÂàùË°∑ÊòØÊãâ‰ΩéLLMÁöÑÂ≠¶‰π†Èó®ÊßõÔºåËÆ©ÊØè‰∏™‰∫∫ÈÉΩËÉΩ‰ªéÁêÜËß£ÊØè‰∏ÄË°å‰ª£Á†ÅÂºÄÂßãÔºå
‰ªéÈõ∂ÂºÄÂßã‰∫≤ÊâãËÆ≠ÁªÉ‰∏Ä‰∏™ÊûÅÂ∞èÁöÑËØ≠Ë®ÄÊ®°Âûã„ÄÇÊòØÁöÑÔºå‰ªé**Èõ∂ÂºÄÂßãËÆ≠ÁªÉ**ÔºåËÄå‰∏çÊòØ‰ªÖ‰ªÖËøõË°å**Êé®ÁêÜ**ÔºÅ
ÊúÄ‰ΩéÂè™ÈúÄ3ÂùóÈí±‰∏çÂà∞ÁöÑÊúçÂä°Âô®ÊàêÊú¨ÔºåÂ∞±ËÉΩ‰∫≤Ë∫´‰ΩìÈ™å‰ªé0Âà∞1ÊûÑÂª∫‰∏Ä‰∏™ËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÖ®ËøáÁ®ã„ÄÇ
‰∏ÄËµ∑ÊÑüÂèóÂàõÈÄ†ÁöÑ‰πêË∂£ÂêßÔºÅ

&gt; [!NOTE]
&gt; ÔºàÊà™Ëá≥2025-10ÔºâMiniMindÁ≥ªÂàóÂ∑≤ÂÆåÊàêÂ§ö‰∏™ÂûãÂè∑Ê®°ÂûãÁöÑÈ¢ÑËÆ≠ÁªÉÔºåÊúÄÂ∞è‰ªÖÈúÄ25.8MÔºà0.02BÔºâÔºåÂç≥ÂèØÂÖ∑Â§áÊµÅÁïÖÂØπËØùËÉΩÂäõÔºÅ

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Models List&lt;/summary&gt;

| Ê®°Âûã (Â§ßÂ∞è)                 | Êé®ÁêÜÂç†Áî® (Á∫¶) | Release    | 
|-------------------------|----------|------------|
| MiniMind2-small (26M)   | 0.5 GB   | 2025.04.26 |
| MiniMind2-MoE (145M)    | 1.0 GB   | 2025.04.26 |
| MiniMind2 (104M)        | 1.0 GB   | 2025.04.26 |
| minimind-v1-small (26M) | 0.5 GB   | 2024.08.28 |
| minimind-v1-moe (4√ó26M) | 1.0 GB   | 2024.09.17 |
| minimind-v1 (108M)      | 1.0 GB   | 2024.09.01 |

&lt;/details&gt;

**È°πÁõÆÂåÖÂê´**

- MiniMind-LLMÁªìÊûÑÁöÑÂÖ®ÈÉ®‰ª£Á†ÅÔºàDense+MoEÊ®°ÂûãÔºâ„ÄÇ
- ÂåÖÂê´TokenizerÂàÜËØçÂô®ËØ¶ÁªÜËÆ≠ÁªÉ‰ª£Á†Å„ÄÇ
- ÂåÖÂê´Pretrain„ÄÅSFT„ÄÅLoRA„ÄÅRLHF-DPO„ÄÅRLAIF(PPO/GRPO/SPO)„ÄÅÊ®°ÂûãËí∏È¶èÁöÑÂÖ®ËøáÁ®ãËÆ≠ÁªÉ‰ª£Á†Å„ÄÇ
- Êî∂ÈõÜ„ÄÅËí∏È¶è„ÄÅÊï¥ÁêÜÂπ∂Ê∏ÖÊ¥óÂéªÈáçÊâÄÊúâÈò∂ÊÆµÁöÑÈ´òË¥®ÈáèÊï∞ÊçÆÈõÜÔºå‰∏îÂÖ®ÈÉ®ÂºÄÊ∫ê„ÄÇ
- ‰ªé0ÂÆûÁé∞È¢ÑËÆ≠ÁªÉ„ÄÅÊåá‰ª§ÂæÆË∞É„ÄÅLoRA„ÄÅDPO/PPO/GRPO/SPOÂº∫ÂåñÂ≠¶‰π†ÔºåÁôΩÁõíÊ®°ÂûãËí∏È¶è„ÄÇÂÖ≥ÈîÆÁÆóÊ≥ïÂá†‰πé‰∏ç‰æùËµñÁ¨¨‰∏âÊñπÂ∞ÅË£ÖÁöÑÊ°ÜÊû∂Ôºå‰∏îÂÖ®ÈÉ®ÂºÄÊ∫ê„ÄÇ
- ÂêåÊó∂ÂÖºÂÆπ`transformers`„ÄÅ`trl`„ÄÅ`peft`Á≠âÁ¨¨‰∏âÊñπ‰∏ªÊµÅÊ°ÜÊû∂„ÄÇ
- ËÆ≠ÁªÉÊîØÊåÅÂçïÊú∫ÂçïÂç°„ÄÅÂçïÊú∫Â§öÂç°(DDP„ÄÅDeepSpeed)ËÆ≠ÁªÉÔºåÊîØÊåÅwandb/swanlabÂèØËßÜÂåñËÆ≠ÁªÉÊµÅÁ®ã„ÄÇÊîØÊåÅÂä®ÊÄÅÂêØÂÅúËÆ≠ÁªÉ„ÄÇ
- Âú®Á¨¨‰∏âÊñπÊµãËØÑÊ¶úÔºàC-Eval„ÄÅC-MMLU„ÄÅOpenBookQAÁ≠âÔºâËøõË°åÊ®°ÂûãÊµãËØïÔºåÊîØÊåÅYaRNÁÆóÊ≥ïÊâßË°åRoPEÈïøÊñáÊú¨Â§ñÊé®„ÄÇ
- ÂÆûÁé∞Openai-ApiÂçèËÆÆÁöÑÊûÅÁÆÄÊúçÂä°Á´ØÔºå‰æø‰∫éÈõÜÊàêÂà∞Á¨¨‰∏âÊñπChatUI‰ΩøÁî®ÔºàFastGPT„ÄÅOpen-WebUIÁ≠âÔºâ„ÄÇ
- Âü∫‰∫éstreamlitÂÆûÁé∞ÊúÄÁÆÄËÅäÂ§©WebUIÂâçÁ´Ø„ÄÇ
- ÂÖ®Èù¢ÂÖºÂÆπÁ§æÂå∫ÁÉ≠Èó®`llama.cpp`„ÄÅ`vllm`„ÄÅ`ollama`Êé®ÁêÜÂºïÊìéÊàñ`Llama-Factory`ËÆ≠ÁªÉÊ°ÜÊû∂„ÄÇ
- Â§çÁé∞(Ëí∏È¶è/RL)Â§ßÂûãÊé®ÁêÜÊ®°ÂûãDeepSeek-R1ÁöÑMiniMind-ReasonÊ®°ÂûãÔºå**Êï∞ÊçÆ+Ê®°Âûã**ÂÖ®ÈÉ®ÂºÄÊ∫êÔºÅ

Â∏åÊúõÊ≠§ÂºÄÊ∫êÈ°πÁõÆÂèØ‰ª•Â∏ÆÂä©LLMÂàùÂ≠¶ËÄÖÂø´ÈÄüÂÖ•Èó®ÔºÅ

### üëâ**Êõ¥Êñ∞Êó•Âøó**

&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2025-10-24&lt;/b&gt; &lt;/summary&gt;

- üî• Êñ∞Â¢ûRLAIFËÆ≠ÁªÉÁÆóÊ≥ïÔºöPPO„ÄÅGRPO„ÄÅSPOÔºà‰ªé0ÂéüÁîüÂÆûÁé∞Ôºâ
- Êñ∞Â¢ûÊñ≠ÁÇπÁª≠ËÆ≠ÂäüËÉΩÔºöÊîØÊåÅËÆ≠ÁªÉËá™Âä®ÊÅ¢Â§ç„ÄÅË∑®GPUÊï∞ÈáèÊÅ¢Â§ç„ÄÅwandbËÆ∞ÂΩïËøûÁª≠ÊÄß
- Êñ∞Â¢ûRLAIFÊï∞ÊçÆÈõÜÔºörlaif-mini.jsonlÔºà‰ªéSFTÊï∞ÊçÆÈöèÊú∫ÈááÊ†∑1‰∏áÊù°ÔºâÔºõÁÆÄÂåñDPOÊï∞ÊçÆÈõÜÔºåÂä†ÂÖ•‰∏≠ÊñáÊï∞ÊçÆ
- Êñ∞Â¢ûYaRNÁÆóÊ≥ïÔºöÊîØÊåÅRoPEÈïøÊñáÊú¨Â§ñÊé®ÔºåÊèêÂçáÈïøÂ∫èÂàóÂ§ÑÁêÜËÉΩÂäõ
- Adaptive ThinkingÔºöReasonÊ®°ÂûãÂèØÈÄâÊòØÂê¶ÂêØÁî®ÊÄùËÄÉÈìæ
- chat_templateÂÖ®Èù¢ÊîØÊåÅTool CallingÂíåReasoningÊ†áÁ≠æÔºà`&lt;tool_call&gt;`„ÄÅ`&lt;think&gt;`Á≠âÔºâ
- Êñ∞Â¢ûRLAIFÂÆåÊï¥Á´†ËäÇ„ÄÅËÆ≠ÁªÉÊõ≤Á∫øÂØπÊØî„ÄÅÁÆóÊ≥ïÂéüÁêÜÊäòÂè†ËØ¥Êòé
- [SwanLab](https://swanlab.cn/)Êõø‰ª£WandBÔºàÂõΩÂÜÖËÆøÈóÆÂèãÂ•ΩÔºåAPIÂÆåÂÖ®ÂÖºÂÆπÔºâ
- ËßÑËåÉÂåñÊâÄÊúâ‰ª£Á†Å &amp; ‰øÆÂ§ç‰∏Ä‰∫õÂ∑≤Áü•bugs

&lt;/details&gt;

&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2025-04-26&lt;/b&gt; &lt;/summary&gt;

- ÈáçË¶ÅÊõ¥Êñ∞
- Â¶ÇÊúâÂÖºÂÆπÊÄßÈúÄË¶ÅÔºåÂèØËÆøÈóÆ[üîóÊóß‰ªìÂ∫ìÂÜÖÂÆπüîó](https://github.com/jingyaogong/minimind/tree/7da201a944a90ed49daef8a0265c959288dff83a)„ÄÇ
- MiniMindÊ®°ÂûãÂèÇÊï∞ÂÆåÂÖ®ÊîπÂêçÔºåÂØπÈΩêTransformersÂ∫ìÊ®°ÂûãÔºàÁªü‰∏ÄÂëΩÂêçÔºâ„ÄÇ
- generateÊñπÂºèÈáçÊûÑÔºåÁªßÊâøËá™GenerationMixinÁ±ª„ÄÇ
- üî•ÊîØÊåÅllama.cpp„ÄÅvllm„ÄÅollamaÁ≠âÁÉ≠Èó®‰∏âÊñπÁîüÊÄÅ„ÄÇ
- ËßÑËåÉ‰ª£Á†ÅÂíåÁõÆÂΩïÁªìÊûÑ„ÄÇ
- ÊîπÂä®ËØçË°®`&lt;s&gt;&lt;/s&gt;`-&gt;`&lt;|im_start|&gt;&lt;|im_end|&gt;`

```text
‰∏∫ÂÖºÂÆπÁ¨¨‰∏âÊñπÊé®ÁêÜÊ°ÜÊû∂llama.cpp„ÄÅvllmÔºåÊú¨Ê¨°Êõ¥Êñ∞ÈúÄ‰ªòÂá∫‰∏Ä‰∫õÂèØËßÇ‰ª£‰ª∑„ÄÇ
Êú¨Ê¨°Êõ¥Êñ∞‰∏çÂÜçÊîØÊåÅ„ÄåÁõ¥Êé•„ÄçÂä†ËΩΩ25-04-26‰ª•ÂâçÁöÑÊóßÊ®°ÂûãËøõË°åÊé®ÁêÜ„ÄÇ
Áî±‰∫éLlama‰ΩçÁΩÆÁºñÁ†ÅÊñπÂºè‰∏éminimindÂ≠òÂú®Âå∫Âà´ÔºåÂØºËá¥Êò†Â∞ÑLlamaÊ®°ÂûãÂêéQKÂÄºÂ≠òÂú®Â∑ÆÂºÇ
MiniMind2Á≥ªÂàóÊóßÊ®°ÂûãÂùáÁªèËøáÊùÉÈáçÊò†Â∞Ñ+ÔºàÂæÆË∞ÉËÆ≠ÁªÉÔºâQKVOÁ∫øÊÄßÂ±ÇÊ†°ÂáÜÊÅ¢Â§çËÄåÊù•„ÄÇ
Êú¨Ê¨°Êõ¥Êñ∞ÂêéÂ∞ÜÊîæÂºÉÂØπ`minimind-v1`ÂÖ®Á≥ªÂàóÁöÑÁª¥Êä§ÔºåÂπ∂Âú®‰ªìÂ∫ì‰∏≠‰∏ãÁ∫ø„ÄÇ
```

&lt;/details&gt;

&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2025-02-09&lt;/b&gt; &lt;/summary&gt;

- ËøéÊù•ÂèëÂ∏É‰ª•Êù•ÈáçÂ§ßÊõ¥Êñ∞ÔºåRelease MiniMind2 Series„ÄÇ
- ‰ª£Á†ÅÂá†‰πéÂÖ®ÈÉ®ÈáçÊûÑÔºå‰ΩøÁî®Êõ¥ÁÆÄÊ¥ÅÊòé‰∫ÜÁöÑÁªü‰∏ÄÁªìÊûÑ„ÄÇ
  Â¶ÇÊúâÊóß‰ª£Á†ÅÁöÑÂÖºÂÆπÊÄßÈúÄË¶ÅÔºåÂèØËÆøÈóÆ[üîóÊóß‰ªìÂ∫ìÂÜÖÂÆπüîó](https://github.com/jingyaogong/minimind/tree/6e9cd28ef9b34a0a10afbdf6f59e65cb6e628efb)„ÄÇ
- ÂÖçÂéªÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÊ≠•È™§„ÄÇÁªü‰∏ÄÊï∞ÊçÆÈõÜÊ†ºÂºèÔºåÊõ¥Êç¢‰∏∫`jsonl`Ê†ºÂºèÊùúÁªùÊï∞ÊçÆÈõÜ‰∏ãËΩΩÊ∑∑‰π±ÁöÑÈóÆÈ¢ò„ÄÇ
- MiniMind2Á≥ªÂàóÊïàÊûúÁõ∏ÊØîMiniMind-V1ÊòæËëóÊèêÂçá„ÄÇ
- Â∞èÈóÆÈ¢òÔºö{kv-cacheÂÜôÊ≥ïÊõ¥Ê†áÂáÜ„ÄÅMoEÁöÑË¥üËΩΩÂùáË°°lossË¢´ËÄÉËôëÁ≠âÁ≠â}
- Êèê‰æõÊ®°ÂûãËøÅÁßªÂà∞ÁßÅÊúâÊï∞ÊçÆÈõÜÁöÑËÆ≠ÁªÉÊñπÊ°àÔºàÂåªÁñóÊ®°Âûã„ÄÅËá™ÊàëËÆ§Áü•Ê†∑‰æãÔºâ„ÄÇ
- Á≤æÁÆÄÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºåÂπ∂Â§ßÂπÖÊèêÂçáÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆË¥®ÈáèÔºåÂ§ßÂπÖÁº©Áü≠‰∏™‰∫∫Âø´ÈÄüËÆ≠ÁªÉÊâÄÈúÄÊó∂Èó¥ÔºåÂçïÂç°3090Âç≥ÂèØ2Â∞èÊó∂Â§çÁé∞ÔºÅ
- Êõ¥Êñ∞ÔºöLoRAÂæÆË∞ÉËÑ±Á¶ªpeftÂåÖË£ÖÔºå‰ªé0ÂÆûÁé∞LoRAËøáÁ®ãÔºõDPOÁÆóÊ≥ï‰ªé0‰ΩøÁî®PyTorchÂéüÁîüÂÆûÁé∞ÔºõÊ®°ÂûãÁôΩÁõíËí∏È¶èÂéüÁîüÂÆûÁé∞„ÄÇ
- MiniMind2-DeepSeek-R1Á≥ªÂàóËí∏È¶èÊ®°ÂûãËØûÁîüÔºÅ
- MiniMind2ÂÖ∑Â§á‰∏ÄÂÆöÁöÑËã±ÊñáËÉΩÂäõÔºÅ
- Êõ¥Êñ∞MiniMind2‰∏éÁ¨¨‰∏âÊñπÊ®°ÂûãÁöÑÂü∫‰∫éÊõ¥Â§öÂ§ßÊ®°ÂûãÊ¶úÂçïÊµãËØïÊÄßËÉΩÁöÑÁªìÊûú„ÄÇ

&lt;/details&gt;

&lt;details close&gt;
&lt;summary&gt; &lt;b&gt;More...&lt;/b&gt; &lt;/summary&gt;

**2024-10-05**
- ‰∏∫MiniMindÊãìÂ±ï‰∫ÜÂ§öÊ®°ÊÄÅËÉΩÂäõ‰πã---ËßÜËßâ
- ÁßªÊ≠•Â≠™ÁîüÈ°πÁõÆ[minimind-v](https://github.com/jingyaogong/minimind-v)Êü•ÁúãËØ¶ÊÉÖÔºÅ

**2024-09-27**
- 09-27Êõ¥Êñ∞pretrainÊï∞ÊçÆÈõÜÁöÑÈ¢ÑÂ§ÑÁêÜÊñπÂºèÔºå‰∏∫‰∫Ü‰øùËØÅÊñáÊú¨ÂÆåÊï¥ÊÄßÔºåÊîæÂºÉÈ¢ÑÂ§ÑÁêÜÊàê.binËÆ≠ÁªÉÁöÑÂΩ¢ÂºèÔºàËΩªÂæÆÁâ∫Áâ≤ËÆ≠ÁªÉÈÄüÂ∫¶Ôºâ„ÄÇ
- ÁõÆÂâçpretrainÈ¢ÑÂ§ÑÁêÜÂêéÁöÑÊñá‰ª∂ÂëΩÂêç‰∏∫Ôºöpretrain_data.csv„ÄÇ
- Âà†Èô§‰∫Ü‰∏Ä‰∫õÂÜó‰ΩôÁöÑ‰ª£Á†Å„ÄÇ

**2024-09-17**
- Êõ¥Êñ∞minimind-v1-moeÊ®°Âûã
- ‰∏∫‰∫ÜÈò≤Ê≠¢Ê≠ß‰πâÔºå‰∏çÂÜç‰ΩøÁî®mistral_tokenizerÂàÜËØçÔºåÂÖ®ÈÉ®ÈááÁî®Ëá™ÂÆö‰πâÁöÑminimind_tokenizer‰Ωú‰∏∫ÂàÜËØçÂô®„ÄÇ

**2024-09-01**
- Êõ¥Êñ∞minimind-v1 (108M)Ê®°ÂûãÔºåÈááÁî®minimind_tokenizerÔºåÈ¢ÑËÆ≠ÁªÉËΩÆÊ¨°3 + SFTËΩÆÊ¨°10ÔºåÊõ¥ÂÖÖÂàÜËÆ≠ÁªÉÔºåÊÄßËÉΩÊõ¥Âº∫„ÄÇ
- È°πÁõÆÂ∑≤ÈÉ®ÁΩ≤Ëá≥ModelScopeÂàõÁ©∫Èó¥ÔºåÂèØ‰ª•Âú®Ê≠§ÁΩëÁ´ô‰∏ä‰ΩìÈ™åÔºö
- [üîóModelScopeÂú®Á∫ø‰ΩìÈ™åüîó](https://www.modelscope.cn/studios/gongjy/minimind)

**2024-08-27**
- È°πÁõÆÈ¶ñÊ¨°ÂºÄÊ∫ê

&lt;/details&gt;

# üìå Âø´ÈÄüÂºÄÂßã

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;ÂàÜ‰∫´Êú¨‰∫∫ÁöÑËΩØÁ°¨‰ª∂ÈÖçÁΩÆÔºà‰ªÖ‰æõÂèÇËÄÉÔºâ&lt;/summary&gt;

* CPU: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz
* RAM: 128 GB
* GPU: NVIDIA GeForce RTX 3090(24GB) * 8
* Ubuntu==20.04
* CUDA==12.2
* Python==3.10.16
* [requirements.txt](./requirements.txt)

&lt;/details&gt;

### Á¨¨0Ê≠•

```bash
git clone https://github.com/jingyaogong/minimind.git
```

## ‚Ö† ÊµãËØïÂ∑≤ÊúâÊ®°ÂûãÊïàÊûú

### 1.ÁéØÂ¢ÉÂáÜÂ§á

```bash
pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple
```

### 2.‰∏ãËΩΩÊ®°Âûã

Âà∞È°πÁõÆÊ†πÁõÆÂΩï

```bash
git clone https://huggingface.co/jingyaogong/MiniMind2 # or https://www.modelscope.cn/models/gongjy/MiniMind2
```

### ÔºàÂèØÈÄâÔºâÂëΩ‰ª§Ë°åÈóÆÁ≠î

```bash
# ‰ΩøÁî®transformersÊ†ºÂºèÊ®°Âûã
python eval_llm.py --load_from ./MiniMind2
```

### ÔºàÂèØÈÄâÔºâÂêØÂä®WebUI

```bash
# ÂèØËÉΩÈúÄË¶Å`python&gt;=3.10` ÂÆâË£Ö `pip install streamlit`
# cd scripts
streamlit run web_demo.py
```

### ÔºàÂèØÈÄâÔºâÁ¨¨‰∏âÊñπÊé®ÁêÜÊ°ÜÊû∂

```bash
# ollama
ollama run jingyaogong/minimind2
# vllm
vllm serve ./MiniMind2/ --served-model-name &quot;minimind&quot;
```

## ‚Ö° ‰ªé0ÂºÄÂßãËá™Â∑±ËÆ≠ÁªÉ

### 1.ÁéØÂ¢ÉÂáÜÂ§á

```bash
pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple
```

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöÊèêÂâçÊµãËØïTorchÊòØÂê¶ÂèØÁî®cuda&lt;/summary&gt;

```bash
import torch
print(torch.cuda.is_available())
```

Â¶ÇÊûú‰∏çÂèØÁî®ÔºåËØ∑Ëá™Ë°åÂéª[torch_stable](https://download.pytorch.org/whl/torch_stable.html)
‰∏ãËΩΩwhlÊñá‰ª∂ÂÆâË£Ö„ÄÇÂèÇËÄÉ[ÈìæÊé•](https://blog.csdn.net/weixin_45456738/article/details/141029610?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=%E5%AE%89%E8%A3%85torch&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-141029610.nonecase&amp;spm=1018.2226.3001.4187)

&lt;/details&gt;

### 2.Êï∞ÊçÆ‰∏ãËΩΩ

‰ªé‰∏ãÊñáÊèê‰æõÁöÑ[Êï∞ÊçÆÈõÜ‰∏ãËΩΩÈìæÊé•](https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files)
‰∏ãËΩΩÈúÄË¶ÅÁöÑÊï∞ÊçÆÊñá‰ª∂ÔºàÂàõÂª∫`./dataset`ÁõÆÂΩïÔºâÂπ∂ÊîæÂà∞`./dataset`‰∏ã

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöÊï∞ÊçÆÈõÜÈ°ªÁü•&lt;/summary&gt;

ÈªòËÆ§Êé®Ëçê‰∏ãËΩΩ`pretrain_hq.jsonl` + `sft_mini_512.jsonl`ÊúÄÂø´ÈÄüÂ∫¶Â§çÁé∞ZeroËÅäÂ§©Ê®°Âûã„ÄÇ

Êï∞ÊçÆÊñá‰ª∂ÂèØËá™Áî±ÈÄâÊã©Ôºå‰∏ãÊñáÊèê‰æõ‰∫ÜÂ§öÁßçÊê≠ÈÖçÊñπÊ°àÔºåÂèØÊ†πÊçÆËá™Â∑±ÊâãÂ§¥ÁöÑËÆ≠ÁªÉÈúÄÊ±ÇÂíåGPUËµÑÊ∫êËøõË°åÈÄÇÂΩìÁªÑÂêà„ÄÇ

&lt;/details&gt;

### 3.ÂºÄÂßãËÆ≠ÁªÉ

ÁõÆÂΩï‰Ωç‰∫é`trainer`

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;üí° Ê£ÄÊü•ÁÇπÊöÇÂÅúÁª≠ËÆ≠&lt;/summary&gt;

ÊâÄÊúâËÆ≠ÁªÉËÑöÊú¨ÂùáËá™Âä®‰øùÂ≠òÊ£ÄÊü•ÁÇπÔºåÂè™ÈúÄÊ∑ªÂä† `--from_resume 1` ÂèÇÊï∞Âç≥ÂèØËá™Âä®Ê£ÄÊµãÂä†ËΩΩ&amp;ÊÅ¢Â§çËÆ≠ÁªÉÔºö

```bash
python train_pretrain.py --from_resume 1
python train_full_sft.py --from_resume 1
...
```

**Êñ≠ÁÇπÁª≠ËÆ≠Êú∫Âà∂ËØ¥ÊòéÔºö**
- ËÆ≠ÁªÉËøáÁ®ãËá™Âä®Âú® `./checkpoints/` ÁõÆÂΩï‰øùÂ≠òÂÆåÊï¥Ê£ÄÊü•ÁÇπÔºàÊ®°Âûã„ÄÅ‰ºòÂåñÂô®„ÄÅËÆ≠ÁªÉËøõÂ∫¶Á≠âÔºâ
- Ê£ÄÊü•ÁÇπÊñá‰ª∂ÂëΩÂêçÔºö`&lt;ÊùÉÈáçÂêç&gt;_&lt;Áª¥Â∫¶&gt;_resume.pth`ÔºàÂ¶ÇÔºö`full_sft_512_resume.pth`Ôºâ
- ÊîØÊåÅË∑®‰∏çÂêåGPUÊï∞ÈáèÊÅ¢Â§çÔºàËá™Âä®Ë∞ÉÊï¥stepÔºâ
- ÊîØÊåÅwandbËÆ≠ÁªÉËÆ∞ÂΩïËøûÁª≠ÊÄßÔºàËá™Âä®ÊÅ¢Â§çÂêå‰∏Ä‰∏™runÔºâ

&gt; ÈÄÇÂêàÈïøÊó∂Èó¥ËÆ≠ÁªÉÊàñ‰∏çÁ®≥ÂÆöÁéØÂ¢ÉÔºåÊó†ÈúÄÊãÖÂøÉËÆ≠ÁªÉ‰∏≠Êñ≠ÂØºËá¥ËøõÂ∫¶‰∏¢Â§±

&lt;/details&gt;

**3.1 È¢ÑËÆ≠ÁªÉÔºàÂ≠¶Áü•ËØÜÔºâ**

```bash
python train_pretrain.py
```

&gt; ÊâßË°åÈ¢ÑËÆ≠ÁªÉÔºåÂæóÂà∞ `pretrain_*.pth` ‰Ωú‰∏∫È¢ÑËÆ≠ÁªÉÁöÑËæìÂá∫ÊùÉÈáçÔºàÂÖ∂‰∏≠*‰∏∫Ê®°ÂûãÁöÑdimensionÔºåÈªòËÆ§‰∏∫512Ôºâ


**3.2 ÁõëÁù£ÂæÆË∞ÉÔºàÂ≠¶ÂØπËØùÊñπÂºèÔºâ**

```bash
python train_full_sft.py
```

&gt; ÊâßË°åÁõëÁù£ÂæÆË∞ÉÔºåÂæóÂà∞ `full_sft_*.pth` ‰Ωú‰∏∫Êåá‰ª§ÂæÆË∞ÉÁöÑËæìÂá∫ÊùÉÈáçÔºàÂÖ∂‰∏≠`full`Âç≥‰∏∫ÂÖ®ÂèÇÊï∞ÂæÆË∞ÉÔºâ

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöËÆ≠ÁªÉÈ°ªÁü•&lt;/summary&gt;

ÊâÄÊúâËÆ≠ÁªÉËøáÁ®ãÈªòËÆ§ÊØèÈöî100Ê≠•‰øùÂ≠ò1Ê¨°ÂèÇÊï∞Âà∞Êñá‰ª∂`./out/***.pth`ÔºàÊØèÊ¨°‰ºöË¶ÜÁõñÊéâÊóßÊùÉÈáçÊñá‰ª∂Ôºâ„ÄÇ

ÁÆÄÂçïËµ∑ËßÅÔºåÊ≠§Â§ÑÂè™ÂÜôÊòé‰∏§‰∏™Èò∂ÊÆµËÆ≠ÁªÉËøáÁ®ã„ÄÇÂ¶ÇÈúÄÂÖ∂ÂÆÉËÆ≠ÁªÉ (LoRA, Ëí∏È¶è, Âº∫ÂåñÂ≠¶‰π†, ÂæÆË∞ÉÊé®ÁêÜÁ≠â) ÂèØÂèÇËÄÉ‰∏ãÊñá„ÄêÂÆûÈ™å„ÄëÂ∞èËäÇÁöÑËØ¶ÁªÜËØ¥Êòé„ÄÇ

&lt;/details&gt;


---

### 4.ÊµãËØïËá™Â∑±ËÆ≠ÁªÉÁöÑÊ®°ÂûãÊïàÊûú

Á°Æ‰øùÈúÄË¶ÅÊµãËØïÁöÑÊ®°Âûã`*.pth`Êñá‰ª∂‰Ωç‰∫é`./out/`ÁõÆÂΩï‰∏ã„ÄÇ
‰πüÂèØ‰ª•Áõ¥Êé•Âéª[Ê≠§Â§Ñ](https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch/files)‰∏ãËΩΩ‰ΩøÁî®ÊàëËÆ≠ÁªÉÁöÑ`*.pth`Êñá‰ª∂„ÄÇ

```bash
python eval_llm.py --weight full_sft # Êàñ pretrain/dpo/ppo/grpo...
```

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöÊµãËØïÈ°ªÁü•&lt;/summary&gt;

`--weight` ÂèÇÊï∞ÊåáÂÆöÊùÉÈáçÂêçÁß∞ÂâçÁºÄÔºåÂèØÈÄâÔºö`pretrain`, `full_sft`, `dpo`, `reason`, `ppo_actor`, `grpo`, `spo` Á≠â

ÂÖ∂‰ªñÂ∏∏Áî®ÂèÇÊï∞Ôºö
- `--load_from`: Ê®°ÂûãÂä†ËΩΩË∑ØÂæÑÔºà`model`=ÂéüÁîütorchÊùÉÈáçÔºåÂÖ∂‰ªñË∑ØÂæÑ=transformersÊ†ºÂºèÔºâ
- `--save_dir`: Ê®°ÂûãÊùÉÈáçÁõÆÂΩïÔºàÈªòËÆ§`out`Ôºâ
- `--lora_weight`: LoRAÊùÉÈáçÂêçÁß∞Ôºà`None`Ë°®Á§∫‰∏ç‰ΩøÁî®Ôºâ
- `--historys`: Êê∫Â∏¶ÂéÜÂè≤ÂØπËØùËΩÆÊï∞ÔºàÈúÄ‰∏∫ÂÅ∂Êï∞Ôºå0Ë°®Á§∫‰∏çÊê∫Â∏¶ÂéÜÂè≤Ôºâ
- `--max_new_tokens`: ÊúÄÂ§ßÁîüÊàêÈïøÂ∫¶ÔºàÈªòËÆ§8192Ôºâ
- `--temperature`: ÁîüÊàêÊ∏©Â∫¶ÔºàÈªòËÆ§0.85Ôºâ
- `--top_p`: nucleusÈááÊ†∑ÈòàÂÄºÔºàÈªòËÆ§0.85Ôºâ


‰ΩøÁî®ÊñπÂºèÁõ¥Êé•Êü•Áúã`eval_llm.py`‰ª£Á†ÅÂç≥ÂèØ„ÄÇ

&lt;/details&gt;


---

&gt; [!TIP]
&gt; ÊâÄÊúâËÆ≠ÁªÉËÑöÊú¨Âùá‰∏∫PytorchÂéüÁîüÊ°ÜÊû∂ÔºåÂùáÊîØÊåÅÂ§öÂç°Âä†ÈÄüÔºåÂÅáËÆæ‰Ω†ÁöÑËÆæÂ§áÊúâN (NÔºû1) Âº†ÊòæÂç°Ôºö

ÂçïÊú∫NÂç°ÂêØÂä®ËÆ≠ÁªÉÊñπÂºè (DDP, ÊîØÊåÅÂ§öÊú∫Â§öÂç°ÈõÜÁæ§)

```bash
torchrun --nproc_per_node N train_xxx.py
```

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöÂÖ∂ÂÆÉÈ°ªÁü•&lt;/summary&gt;

&lt;del&gt;
ÂçïÊú∫NÂç°ÂêØÂä®ËÆ≠ÁªÉ (DeepSpeed)

```bash
deepspeed --master_port 29500 --num_gpus=N train_xxx.py
```
&lt;/del&gt;

ÂèØÊ†πÊçÆÈúÄË¶ÅÂºÄÂêØwandbËÆ∞ÂΩïËÆ≠ÁªÉËøáÁ®ãÔºàÈúÄÂèØÁõ¥ËøûÔºâ

```bash
# ÈúÄË¶ÅÁôªÂΩï: wandb login
torchrun --nproc_per_node N train_xxx.py --use_wandb
# and
python train_xxx.py --use_wandb
```

ÈÄöËøáÊ∑ªÂä†`--use_wandb`ÂèÇÊï∞ÔºåÂèØ‰ª•ËÆ∞ÂΩïËÆ≠ÁªÉËøáÁ®ãÔºåËÆ≠ÁªÉÂÆåÊàêÂêéÔºåÂèØ‰ª•Âú®wandbÁΩëÁ´ô‰∏äÊü•ÁúãËÆ≠ÁªÉËøáÁ®ã„ÄÇÈÄöËøá‰øÆÊîπ`wandb_project`
Âíå`wandb_run_name`ÂèÇÊï∞ÔºåÂèØ‰ª•ÊåáÂÆöÈ°πÁõÆÂêçÁß∞ÂíåËøêË°åÂêçÁß∞„ÄÇ

„ÄêÊ≥®„ÄëÔºö25Âπ¥6ÊúàÂêéÔºåÂõΩÂÜÖÁΩëÁªúÁéØÂ¢ÉÊó†Ê≥ïÁõ¥ËøûWandBÔºåMiniMindÈ°πÁõÆÈªòËÆ§ËΩ¨‰∏∫‰ΩøÁî®[SwanLab](https://swanlab.cn/)‰Ωú‰∏∫ËÆ≠ÁªÉÂèØËßÜÂåñÂ∑•ÂÖ∑ÔºàÂÆåÂÖ®ÂÖºÂÆπWandB APIÔºâÔºåÂç≥`import wandb`Êîπ‰∏∫`import swanlab as wandb`Âç≥ÂèØÔºåÂÖ∂‰ªñÂùáÊó†ÈúÄÊîπÂä®„ÄÇ

&lt;/details&gt;

# üìå Êï∞ÊçÆ‰ªãÁªç

## ‚Ö† Tokenizer

ÂàÜËØçÂô®Â∞ÜÂçïËØç‰ªéËá™ÁÑ∂ËØ≠Ë®ÄÈÄöËøá‚ÄúËØçÂÖ∏‚ÄùÊò†Â∞ÑÂà∞`0, 1, 36`ËøôÊ†∑ÁöÑÊï∞Â≠óÔºåÂèØ‰ª•ÁêÜËß£‰∏∫Êï∞Â≠óÂ∞±‰ª£Ë°®‰∫ÜÂçïËØçÂú®‚ÄúËØçÂÖ∏‚Äù‰∏≠ÁöÑÈ°µÁ†Å„ÄÇ
ÂèØ‰ª•ÈÄâÊã©Ëá™Â∑±ÊûÑÈÄ†ËØçË°®ËÆ≠ÁªÉ‰∏Ä‰∏™‚ÄúËØçÂÖ∏‚ÄùÔºå‰ª£Á†ÅÂèØËßÅ`./trainer/train_tokenizer.py`Ôºà‰ªÖ‰æõÂ≠¶‰π†ÂèÇËÄÉÔºåËã•ÈùûÂøÖË¶ÅÊó†ÈúÄÂÜçËá™Ë°åËÆ≠ÁªÉÔºåMiniMindÂ∑≤Ëá™Â∏¶tokenizerÔºâ„ÄÇ
ÊàñËÄÖÈÄâÊã©ÊØîËæÉÂá∫ÂêçÁöÑÂºÄÊ∫êÂ§ßÊ®°ÂûãÂàÜËØçÂô®Ôºå
Ê≠£Â¶ÇÂêåÁõ¥Êé•Áî®Êñ∞Âçé/ÁâõÊ¥•ËØçÂÖ∏ÁöÑ‰ºòÁÇπÊòØtokenÁºñÁ†ÅÂéãÁº©ÁéáÂæàÂ•ΩÔºåÁº∫ÁÇπÊòØÈ°µÊï∞Â§™Â§öÔºåÂä®ËæÑÊï∞ÂçÅ‰∏á‰∏™ËØçÊ±áÁü≠ËØ≠Ôºõ
Ëá™Â∑±ËÆ≠ÁªÉÁöÑÂàÜËØçÂô®Ôºå‰ºòÁÇπÊòØËØçË°®ÈïøÂ∫¶ÂíåÂÜÖÂÆπÈöèÊÑèÊéßÂà∂ÔºåÁº∫ÁÇπÊòØÂéãÁº©ÁéáÂæà‰ΩéÔºà‰æãÂ¶Ç&quot;hello&quot;‰πüËÆ∏‰ºöË¢´ÊãÜÂàÜ‰∏∫&quot;h e l l o&quot;
‰∫î‰∏™Áã¨Á´ãÁöÑtokenÔºâÔºå‰∏îÁîüÂÉªËØçÈöæ‰ª•Ë¶ÜÁõñ„ÄÇ
‚ÄúËØçÂÖ∏‚ÄùÁöÑÈÄâÊã©Âõ∫ÁÑ∂ÂæàÈáçË¶ÅÔºåLLMÁöÑËæìÂá∫Êú¨Ë¥®‰∏äÊòØSoftMaxÂà∞ËØçÂÖ∏N‰∏™ËØçÁöÑÂ§öÂàÜÁ±ªÈóÆÈ¢òÔºåÁÑ∂ÂêéÈÄöËøá‚ÄúËØçÂÖ∏‚ÄùËß£Á†ÅÂà∞Ëá™ÁÑ∂ËØ≠Ë®Ä„ÄÇ
Âõ†‰∏∫MiniMind‰ΩìÁßØÈúÄË¶Å‰∏•Ê†ºÊéßÂà∂Ôºå‰∏∫‰∫ÜÈÅøÂÖçÊ®°ÂûãÂ§¥ÈáçËÑöËΩªÔºàËØçÂµåÂÖ•embeddingÂ±ÇÂèÇÊï∞Âú®LLMÂç†ÊØîÂ§™È´òÔºâÔºåÊâÄ‰ª•ËØçË°®ÈïøÂ∫¶Áü≠Áü≠ÁõäÂñÑ„ÄÇ

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Tokenizer‰ªãÁªç&lt;/summary&gt;

Á¨¨‰∏âÊñπÂº∫Â§ßÁöÑÂºÄÊ∫êÊ®°Âûã‰æãÂ¶ÇYi„ÄÅqwen„ÄÅchatglm„ÄÅmistral„ÄÅLlama3ÁöÑtokenizerËØçË°®ÈïøÂ∫¶Â¶Ç‰∏ãÔºö

&lt;table&gt;
  &lt;tr&gt;&lt;th&gt;TokenizerÊ®°Âûã&lt;/th&gt;&lt;th&gt;ËØçË°®Â§ßÂ∞è&lt;/th&gt;&lt;th&gt;Êù•Ê∫ê&lt;/th&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;yi tokenizer&lt;/td&gt;&lt;td&gt;64,000&lt;/td&gt;&lt;td&gt;01‰∏áÁâ©Ôºà‰∏≠ÂõΩÔºâ&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;qwen2 tokenizer&lt;/td&gt;&lt;td&gt;151,643&lt;/td&gt;&lt;td&gt;ÈòøÈáå‰∫ëÔºà‰∏≠ÂõΩÔºâ&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;glm tokenizer&lt;/td&gt;&lt;td&gt;151,329&lt;/td&gt;&lt;td&gt;Êô∫Ë∞±AIÔºà‰∏≠ÂõΩÔºâ&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;mistral tokenizer&lt;/td&gt;&lt;td&gt;32,000&lt;/td&gt;&lt;td&gt;Mistral AIÔºàÊ≥ïÂõΩÔºâ&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;llama3 tokenizer&lt;/td&gt;&lt;td&gt;128,000&lt;/td&gt;&lt;td&gt;MetaÔºàÁæéÂõΩÔºâ&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;minimind tokenizer&lt;/td&gt;&lt;td&gt;6,400&lt;/td&gt;&lt;td&gt;Ëá™ÂÆö‰πâ&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&gt; üëâ2024-09-17Êõ¥Êñ∞Ôºö‰∏∫‰∫ÜÈò≤Ê≠¢ËøáÂéªÁöÑÁâàÊú¨Ê≠ß‰πâ&amp;ÊéßÂà∂‰ΩìÁßØÔºåminimindÊâÄÊúâÊ®°ÂûãÂùá‰ΩøÁî®minimind_tokenizerÂàÜËØçÔºåÂ∫üÂºÉÊâÄÊúâmistral_tokenizerÁâàÊú¨„ÄÇ

```
# ‰∏Ä‰∫õËá™Ë®ÄËá™ËØ≠
&gt; Â∞ΩÁÆ°minimind_tokenizerÈïøÂ∫¶ÂæàÂ∞èÔºåÁºñËß£Á†ÅÊïàÁéáÂº±‰∫éqwen2„ÄÅglmÁ≠â‰∏≠ÊñáÂèãÂ•ΩÂûãÂàÜËØçÂô®„ÄÇ
&gt; ‰ΩÜminimindÊ®°ÂûãÈÄâÊã©‰∫ÜËá™Â∑±ËÆ≠ÁªÉÁöÑminimind_tokenizer‰Ωú‰∏∫ÂàÜËØçÂô®Ôºå‰ª•‰øùÊåÅÊï¥‰ΩìÂèÇÊï∞ËΩªÈáèÔºåÈÅøÂÖçÁºñÁ†ÅÂ±ÇÂíåËÆ°ÁÆóÂ±ÇÂç†ÊØîÂ§±Ë°°ÔºåÂ§¥ÈáçËÑöËΩªÔºåÂõ†‰∏∫minimindÁöÑËØçË°®Â§ßÂ∞èÂè™Êúâ6400„ÄÇ
&gt; ‰∏îminimindÂú®ÂÆûÈôÖÊµãËØï‰∏≠Ê≤°ÊúâÂá∫Áé∞ËøáÁîüÂÉªËØçÊ±áËß£Á†ÅÂ§±Ë¥•ÁöÑÊÉÖÂÜµÔºåÊïàÊûúËâØÂ•Ω„ÄÇ
&gt; Áî±‰∫éËá™ÂÆö‰πâËØçË°®ÂéãÁº©ÈïøÂ∫¶Âà∞6400Ôºå‰ΩøÂæóLLMÊÄªÂèÇÊï∞ÈáèÊúÄ‰ΩéÂè™Êúâ25.8M„ÄÇ
&gt; ËÆ≠ÁªÉÊï∞ÊçÆ`pretrain_hq.jsonl`ÂùáÊù•Ëá™‰∫é`Âå†Êï∞Â§ßÊ®°ÂûãÊï∞ÊçÆÈõÜ`ÔºåËøôÈÉ®ÂàÜÊï∞ÊçÆÁõ∏ÂØπÊ¨°Ë¶ÅÔºåÂ¶ÇÈúÄËÆ≠ÁªÉÂèØ‰ª•Ëá™Áî±ÈÄâÊã©„ÄÇ
```

&lt;/details&gt;

## ‚Ö° PretrainÊï∞ÊçÆ

ÁªèÂéÜ‰∫ÜMiniMind-V1ÁöÑ‰ΩéË¥®ÈáèÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÔºåÂØºËá¥Ê®°ÂûãËÉ°Ë®Ä‰π±ËØ≠ÁöÑÊïôËÆ≠Ôºå`2025-02-05` ‰πãÂêéÂÜ≥ÂÆö‰∏çÂÜçÈááÁî®Â§ßËßÑÊ®°Êó†ÁõëÁù£ÁöÑÊï∞ÊçÆÈõÜÂÅöÈ¢ÑËÆ≠ÁªÉ„ÄÇ
ËøõËÄåÂ∞ùËØïÊää[Âå†Êï∞Â§ßÊ®°ÂûãÊï∞ÊçÆÈõÜ](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)ÁöÑ‰∏≠ÊñáÈÉ®ÂàÜÊèêÂèñÂá∫Êù•Ôºå
Ê∏ÖÊ¥óÂá∫Â≠óÁ¨¶`&lt;512`ÈïøÂ∫¶ÁöÑÂ§ßÁ∫¶1.6GBÁöÑËØ≠ÊñôÁõ¥Êé•ÊãºÊé•ÊàêÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆ `pretrain_hq.jsonl`ÔºåhqÂç≥‰∏∫high
qualityÔºàÂΩìÁÑ∂‰πüËøò‰∏çÁÆóhighÔºåÊèêÂçáÊï∞ÊçÆË¥®ÈáèÊó†Ê≠¢Â∞ΩÔºâ„ÄÇ

Êñá‰ª∂`pretrain_hq.jsonl` Êï∞ÊçÆÊ†ºÂºè‰∏∫

```json
{&quot;text&quot;: &quot;Â¶Ç‰ΩïÊâçËÉΩÊëÜËÑ±ÊãñÂª∂ÁóáÔºü Ê≤ªÊÑàÊãñÂª∂ÁóáÂπ∂‰∏çÂÆπÊòìÔºå‰ΩÜ‰ª•‰∏ãÂª∫ËÆÆÂèØËÉΩÊúâÊâÄÂ∏ÆÂä©...&quot;}
```

## ‚Ö¢ SFTÊï∞ÊçÆ

[Âå†Êï∞Â§ßÊ®°ÂûãSFTÊï∞ÊçÆÈõÜ](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)
‚ÄúÊòØ‰∏Ä‰∏™ÂÆåÊï¥„ÄÅÊ†ºÂºèÁªü‰∏Ä„ÄÅÂÆâÂÖ®ÁöÑÂ§ßÊ®°ÂûãËÆ≠ÁªÉÂíåÁ†îÁ©∂ËµÑÊ∫ê„ÄÇ
‰ªéÁΩëÁªú‰∏äÁöÑÂÖ¨ÂºÄÊï∞ÊçÆÊ∫êÊî∂ÈõÜÂπ∂Êï¥ÁêÜ‰∫ÜÂ§ßÈáèÂºÄÊ∫êÊï∞ÊçÆÈõÜÔºåÂØπÂÖ∂ËøõË°å‰∫ÜÊ†ºÂºèÁªü‰∏ÄÔºåÊï∞ÊçÆÊ∏ÖÊ¥óÔºå
ÂåÖÂê´10MÊù°Êï∞ÊçÆÁöÑ‰∏≠ÊñáÊï∞ÊçÆÈõÜÂíåÂåÖÂê´2MÊù°Êï∞ÊçÆÁöÑËã±ÊñáÊï∞ÊçÆÈõÜ„ÄÇ‚Äù
‰ª•‰∏äÊòØÂÆòÊñπ‰ªãÁªçÔºå‰∏ãËΩΩÊñá‰ª∂ÂêéÁöÑÊï∞ÊçÆÊÄªÈáèÂ§ßÁ∫¶Âú®4B tokensÔºåËÇØÂÆöÊòØÈÄÇÂêà‰Ωú‰∏∫‰∏≠ÊñáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑSFTÊï∞ÊçÆÁöÑ„ÄÇ
‰ΩÜÊòØÂÆòÊñπÊèê‰æõÁöÑÊï∞ÊçÆÊ†ºÂºèÂæà‰π±ÔºåÂÖ®ÈÉ®Áî®Êù•sft‰ª£‰ª∑Â§™Â§ß„ÄÇ
ÊàëÂ∞ÜÊääÂÆòÊñπÊï∞ÊçÆÈõÜËøõË°å‰∫Ü‰∫åÊ¨°Ê∏ÖÊ¥óÔºåÊääÂê´ÊúâÁ¨¶Âè∑Ê±°ÊüìÂíåÂô™Â£∞ÁöÑÊù°ÁõÆÂéªÈô§ÔºõÂè¶Â§ñ‰æùÁÑ∂Âè™‰øùÁïô‰∫ÜÊÄªÈïøÂ∫¶`&lt;512`
ÁöÑÂÜÖÂÆπÔºåÊ≠§Èò∂ÊÆµÂ∏åÊúõÈÄöËøáÂ§ßÈáèÂØπËØùË°•ÂÖÖÈ¢ÑËÆ≠ÁªÉÈò∂ÊÆµÊ¨†Áº∫ÁöÑÁü•ËØÜ„ÄÇ
ÂØºÂá∫Êñá‰ª∂‰∏∫`sft_512.jsonl`(~7.5GB)„ÄÇ

[Magpie-SFTÊï∞ÊçÆÈõÜ](https://www.modelscope.cn/organization/Magpie-Align)
Êî∂ÈõÜ‰∫Ü~1MÊù°Êù•Ëá™Qwen2/2.5ÁöÑÈ´òË¥®ÈáèÂØπËØùÔºåÊàëÂ∞ÜËøôÈÉ®ÂàÜÊï∞ÊçÆËøõ‰∏ÄÊ≠•Ê∏ÖÊ¥óÔºåÊääÊÄªÈïøÂ∫¶`&lt;2048`ÁöÑÈÉ®ÂàÜÂØºÂá∫‰∏∫`sft_2048.jsonl`(~9GB)„ÄÇ
ÈïøÂ∫¶`&lt;1024`ÁöÑÈÉ®ÂàÜÂØºÂá∫‰∏∫`sft_1024.jsonl`(~5.5GB)ÔºåÁî®Â§ßÊ®°ÂûãÂØπËØùÊï∞ÊçÆÁõ¥Êé•ËøõË°åsftÂ∞±Â±û‰∫é‚ÄúÈªëÁõíËí∏È¶è‚ÄùÁöÑËåÉÁï¥„ÄÇ

Ëøõ‰∏ÄÊ≠•Ê∏ÖÊ¥óÂâç‰∏§Ê≠•sftÁöÑÊï∞ÊçÆÔºàÂè™‰øùÁïô‰∏≠ÊñáÂ≠óÁ¨¶Âç†ÊØîÈ´òÁöÑÂÜÖÂÆπÔºâÔºåÁ≠õÈÄâÈïøÂ∫¶`&lt;512`ÁöÑÂØπËØùÔºåÂæóÂà∞`sft_mini_512.jsonl`(~1.2GB)„ÄÇ

ÊâÄÊúâsftÊñá‰ª∂ `sft_X.jsonl` Êï∞ÊçÆÊ†ºÂºèÂùá‰∏∫

```text
{
    &quot;conversations&quot;: [
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;‰Ω†Â•Ω&quot;},
        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;‰Ω†Â•ΩÔºÅ&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;ÂÜçËßÅ&quot;},
        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;ÂÜçËßÅÔºÅ&quot;}
    ]
}
```

## ‚Ö£ RLHFÊï∞ÊçÆ

Êù•Ëá™[Magpie-DPOÊï∞ÊçÆÈõÜ](https://www.modelscope.cn/datasets/Magpie-Align/MagpieLM-DPO-Data-v0.1)
Â§ßÁ∫¶200kÊù°ÂÅèÂ•ΩÊï∞ÊçÆÔºàÂùáÊòØËã±ÊñáÔºâÁîüÊàêËá™Llama3.1-70B/8BÔºåÂèØ‰ª•Áî®‰∫éËÆ≠ÁªÉÂ•ñÂä±Ê®°ÂûãÔºå‰ºòÂåñÊ®°ÂûãÂõûÂ§çË¥®ÈáèÔºå‰ΩøÂÖ∂Êõ¥Âä†Á¨¶Âêà‰∫∫Á±ªÂÅèÂ•Ω„ÄÇ
ËøôÈáåÂ∞ÜÊï∞ÊçÆÊÄªÈïøÂ∫¶`&lt;3000`ÁöÑÂÜÖÂÆπÈáçÁªÑ‰∏∫`dpo.jsonl`(~0.9GB)ÔºåÂåÖÂê´`chosen`Âíå`rejected`‰∏§‰∏™Â≠óÊÆµÔºå`chosen`
‰∏∫ÂÅèÂ•ΩÁöÑÂõûÂ§çÔºå`rejected`‰∏∫ÊãíÁªùÁöÑÂõûÂ§ç„ÄÇ

Êñá‰ª∂ `dpo.jsonl` Êï∞ÊçÆÊ†ºÂºè‰∏∫

```text
{
  &quot;chosen&quot;: [
    {&quot;content&quot;: &quot;Q&quot;, &quot;role&quot;: &quot;user&quot;}, 
    {&quot;content&quot;: &quot;good answer&quot;, &quot;role&quot;: &quot;assistant&quot;}
  ], 
  &quot;rejected&quot;: [
    {&quot;content&quot;: &quot;Q&quot;, &quot;role&quot;: &quot;user&quot;}, 
    {&quot;content&quot;: &quot;bad answer&quot;, &quot;role&quot;: &quot;assistant&quot;}
  ]
}
```

## ‚Ö§ ReasonÊï∞ÊçÆÈõÜÔºö

‰∏çÂæó‰∏çËØ¥2025Âπ¥2ÊúàË∞ÅËÉΩÁÅ´ÁöÑËøáDeepSeek...
‰πüÊøÄÂèë‰∫ÜÊàëÂØπRLÂºïÂØºÁöÑÊé®ÁêÜÊ®°ÂûãÁöÑÊµìÂéöÂÖ¥Ë∂£ÔºåÁõÆÂâçÂ∑≤ÁªèÁî®Qwen2.5Â§çÁé∞‰∫ÜR1-Zero„ÄÇ
Â¶ÇÊûúÊúâÊó∂Èó¥+ÊïàÊûúworkÔºà‰ΩÜ99%Âü∫Ê®°ËÉΩÂäõ‰∏çË∂≥ÔºâÊàë‰ºöÂú®‰πãÂêéÊõ¥Êñ∞MiniMindÂü∫‰∫éRLËÆ≠ÁªÉÁöÑÊé®ÁêÜÊ®°ÂûãËÄå‰∏çÊòØËí∏È¶èÊ®°Âûã„ÄÇ
Êó∂Èó¥ÊúâÈôêÔºåÊúÄÂø´ÁöÑ‰ΩéÊàêÊú¨ÊñπÊ°à‰æùÁÑ∂ÊòØÁõ¥Êé•Ëí∏È¶èÔºàÈªëÁõíÊñπÂºèÔºâ„ÄÇ
ËÄê‰∏ç‰ΩèR1Â§™ÁÅ´ÔºåÁü≠Áü≠Âá†Â§©Â∞±Â∑≤ÁªèÂ≠òÂú®‰∏Ä‰∫õR1ÁöÑËí∏È¶èÊï∞ÊçÆÈõÜ[R1-Llama-70B](https://www.modelscope.cn/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B)„ÄÅ[R1-Distill-SFT](https://www.modelscope.cn/datasets/AI-ModelScope/R1-Distill-SFT)„ÄÅ
[Alpaca-Distill-R1](https://huggingface.co/datasets/shareAI/Alpaca-Distill-R1-ZH)„ÄÅ
[deepseek_r1_zh](https://huggingface.co/datasets/jinliuxi/deepseek_r1_zh)Á≠âÁ≠âÔºåÁ∫Ø‰∏≠ÊñáÁöÑÊï∞ÊçÆÂèØËÉΩÊØîËæÉÂ∞ë„ÄÇ
ÊúÄÁªàÊï¥ÂêàÂÆÉ‰ª¨ÔºåÂØºÂá∫Êñá‰ª∂‰∏∫`r1_mix_1024.jsonl`ÔºåÊï∞ÊçÆÊ†ºÂºèÂíå`sft_X.jsonl`‰∏ÄËá¥„ÄÇ

## ‚Ö• Êõ¥Â§öÊï∞ÊçÆÈõÜ

ÁõÆÂâçÂ∑≤ÁªèÊúâ[HqWu-HITCS/Awesome-Chinese-LLM](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM)
Âú®Êî∂ÈõÜÂíåÊ¢≥ÁêÜ‰∏≠ÊñáLLMÁõ∏ÂÖ≥ÁöÑÂºÄÊ∫êÊ®°Âûã„ÄÅÂ∫îÁî®„ÄÅÊï∞ÊçÆÈõÜÂèäÊïôÁ®ãÁ≠âËµÑÊñôÔºåÂπ∂ÊåÅÁª≠Êõ¥Êñ∞ËøôÊñπÈù¢ÁöÑÊúÄÊñ∞ËøõÂ±ï„ÄÇÂÖ®Èù¢‰∏î‰∏ì‰∏öÔºåRespectÔºÅ

---

## ‚Öß MiniMindËÆ≠ÁªÉÊï∞ÊçÆÈõÜ

&gt; [!NOTE]
&gt; 2025-02-05ÂêéÔºåÂºÄÊ∫êMiniMindÊúÄÁªàËÆ≠ÁªÉÊâÄÁî®ÁöÑÊâÄÊúâÊï∞ÊçÆÈõÜÔºåÂõ†Ê≠§Êó†ÈúÄÂÜçËá™Ë°åÈ¢ÑÂ§ÑÁêÜÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÈÅøÂÖçÈáçÂ§çÊÄßÁöÑÊï∞ÊçÆÂ§ÑÁêÜÂ∑•‰Ωú„ÄÇ

MiniMindËÆ≠ÁªÉÊï∞ÊçÆÈõÜ‰∏ãËΩΩÂú∞ÂùÄÔºö [ModelScope](https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files) | [HuggingFace](https://huggingface.co/datasets/jingyaogong/minimind_dataset/tree/main)

&gt; Êó†ÈúÄÂÖ®ÈÉ®cloneÔºåÂèØÂçïÁã¨‰∏ãËΩΩÊâÄÈúÄÁöÑÊñá‰ª∂

Â∞Ü‰∏ãËΩΩÁöÑÊï∞ÊçÆÈõÜÊñá‰ª∂ÊîæÂà∞`./dataset/`ÁõÆÂΩï‰∏ãÔºà‚ú®‰∏∫Êé®ËçêÁöÑÂøÖÈ°ªÈ°πÔºâ

```bash
./dataset/
‚îú‚îÄ‚îÄ dpo.jsonl (55MB, ‚ú®)
‚îú‚îÄ‚îÄ lora_identity.jsonl (22.8KB)
‚îú‚îÄ‚îÄ lora_medical.jsonl (34MB)
‚îú‚îÄ‚îÄ pretrain_hq.jsonl (1.6GB, ‚ú®)
‚îú‚îÄ‚îÄ r1_mix_1024.jsonl (340MB)
‚îú‚îÄ‚îÄ rlaif-mini.jsonl (1MB, ‚ú®)
‚îú‚îÄ‚îÄ sft_1024.jsonl (5.6GB)
‚îú‚îÄ‚îÄ sft_2048.jsonl (9GB)
‚îú‚îÄ‚îÄ sft_512.jsonl (7.5GB)
‚îî‚îÄ‚îÄ sft_mini_512.jsonl (1.2GB, ‚ú®)
```

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöÂêÑÊï∞ÊçÆÈõÜÁÆÄ‰ªã&lt;/summary&gt;

* `dpo.jsonl`‚ú® --RLHFÈò∂ÊÆµÊï∞ÊçÆÈõÜÔºàÂ∑≤Á≤æÁÆÄ‰ºòÂåñÔºåÈÄÇÂêàÂø´ÈÄüËÆ≠ÁªÉÔºâ
* `lora_identity.jsonl` --Ëá™ÊàëËÆ§Áü•Êï∞ÊçÆÈõÜÔºà‰æãÂ¶ÇÔºö‰Ω†ÊòØË∞ÅÔºüÊàëÊòØminimind...ÔºâÔºåÊé®ËçêÁî®‰∫éloraËÆ≠ÁªÉÔºà‰∫¶ÂèØÁî®‰∫éÂÖ®ÂèÇSFTÔºåÂãøË¢´ÂêçÂ≠óÂ±ÄÈôêÔºâ
* `lora_medical.jsonl` --ÂåªÁñóÈóÆÁ≠îÊï∞ÊçÆÈõÜÔºåÊé®ËçêÁî®‰∫éloraËÆ≠ÁªÉÔºà‰∫¶ÂèØÁî®‰∫éÂÖ®ÂèÇSFTÔºåÂãøË¢´ÂêçÂ≠óÂ±ÄÈôêÔºâ
* `pretrain_hq.jsonl`‚ú® --È¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºåÊï¥ÂêàËá™Âå†Êï∞ÁßëÊäÄÔºàÊé®ËçêËÆæÁΩÆ`max_seq_len‚âà320`Ôºâ
* `r1_mix_1024.jsonl` --DeepSeek-R1-1.5BËí∏È¶èÊï∞ÊçÆÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫1024ÔºàÊé®ËçêËÆæÁΩÆ`max_seq_len‚âà720`Ôºâ
* `rlaif-mini.jsonl` --RLAIFËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºå‰ªéSFTÊï∞ÊçÆÈõÜ‰∏≠ÈöèÊú∫ÈááÊ†∑1‰∏áÊù°È´òË¥®ÈáèÂØπËØùÔºåÁî®‰∫éPPO/GRPO/SPOÁ≠âÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïËÆ≠ÁªÉ
* `sft_1024.jsonl` --Êï¥ÂêàËá™Qwen2.5Ëí∏È¶èÊï∞ÊçÆÔºàÊòØsft_2048ÁöÑÂ≠êÈõÜÔºâÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫1024ÔºàÊé®ËçêËÆæÁΩÆ`max_seq_len‚âà650`Ôºâ
* `sft_2048.jsonl` --Êï¥ÂêàËá™Qwen2.5Ëí∏È¶èÊï∞ÊçÆÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫2048ÔºàÊé®ËçêËÆæÁΩÆ`max_seq_len‚âà1400`Ôºâ
* `sft_512.jsonl` --Êï¥ÂêàËá™Âå†Êï∞ÁßëÊäÄSFTÊï∞ÊçÆÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫512ÔºàÊé®ËçêËÆæÁΩÆ`max_seq_len‚âà350`Ôºâ
* `sft_mini_512.jsonl`‚ú® --ÊûÅÁÆÄÊï¥ÂêàËá™Âå†Êï∞ÁßëÊäÄSFTÊï∞ÊçÆ+Qwen2.5Ëí∏È¶èÊï∞ÊçÆÔºàÁî®‰∫éÂø´ÈÄüËÆ≠ÁªÉZeroÊ®°ÂûãÔºâÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫512ÔºàÊé®ËçêËÆæÁΩÆ`max_seq_len‚âà340`Ôºâ


ËÆ≠ÁªÉÂèÇÊï∞`max_seq_len`ÁõÆÂâçÊåáÁöÑÊòØtokensÈïøÂ∫¶ÔºåËÄåÈùûÁªùÂØπÂ≠óÁ¨¶Êï∞„ÄÇ
Êú¨È°πÁõÆtokenizerÂú®‰∏≠ÊñáÊñáÊú¨‰∏äÂ§ßÁ∫¶`1.5~1.7 Â≠óÁ¨¶/token`ÔºåÁ∫ØËã±ÊñáÁöÑÂéãÁº©ÊØîÂú®`4~5 Â≠óÁ¨¶/token`Ôºå‰∏çÂêåÊï∞ÊçÆÂàÜÂ∏É‰ºöÊúâÊ≥¢Âä®„ÄÇ
Êï∞ÊçÆÈõÜÂëΩÂêçÊ†áÊ≥®ÁöÑ‚ÄúÊúÄÂ§ßÈïøÂ∫¶‚ÄùÂùá‰∏∫Â≠óÁ¨¶Êï∞Ôºå100ÈïøÂ∫¶ÁöÑÂ≠óÁ¨¶‰∏≤ÂèØÁ≤óÁï•Êç¢ÁÆóÊàê`100/1.5‚âà67`ÁöÑtokensÈïøÂ∫¶„ÄÇ

‰æãÂ¶ÇÔºö

* ‰∏≠ÊñáÔºö`ÁôΩÊó•‰æùÂ±±Â∞Ω`5‰∏™Â≠óÁ¨¶ÂèØËÉΩË¢´ÊãÜÂàÜ‰∏∫[`ÁôΩÊó•`,`‰æù`,`Â±±`,`Â∞Ω`] 4‰∏™tokensÔºõ
* Ëã±ÊñáÔºö`The sun sets in the west`24‰∏™Â≠óÁ¨¶ÂèØËÉΩË¢´ÊãÜÂàÜ‰∏∫[`The `,`sun `,`sets `,`in `,`the`,`west`] 6‰∏™tokens

‚ÄúÊé®ËçêËÆæÁΩÆ‚ÄùÁªôÂá∫‰∫ÜÂêÑ‰∏™Êï∞ÊçÆÈõÜ‰∏äÊúÄÂ§ßtokensÈïøÂ∫¶ÁöÑÁ≤óÁï•‰º∞ËÆ°„ÄÇ
È°ªÁü•max_seq_lenÂèØ‰ª•ÊøÄËøõ/‰øùÂÆà/ÂùáË°°Âú∞Ë∞ÉÊï¥ÔºåÂõ†‰∏∫Êõ¥Â§ßÊàñÊõ¥Â∞èÂùáÊó†Ê≥ïÈÅøÂÖçÂâØ‰ΩúÁî®Ôºö‰∏Ä‰∫õÊ†∑Êú¨Áü≠‰∫émax_seq_lenÂêéË¢´paddingÊµ™Ë¥πÁÆóÂäõÔºå‰∏Ä‰∫õÊ†∑Êú¨Èïø‰∫émax_seq_lenÂêéË¢´Êà™Êñ≠ËØ≠ÊÑè„ÄÇ

Âú® `ÁÆóÂäõÊïàÁéá` &lt;---&gt; `ËØ≠‰πâÂÆåÊï¥ÊÄß` ‰πãÈó¥ÊâæÂà∞‰∏Ä‰∏™Âπ≥Ë°°ÁÇπÂç≥ÂèØ

&lt;/details&gt;


![dataset](./images/dataset.jpg)

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;ËØ¥Êòé &amp; Êé®ËçêËÆ≠ÁªÉÊñπÊ°à&lt;/summary&gt;

* MiniMind2 SeriesÂùáÁªèËøáÂÖ±Á∫¶20GBËØ≠ÊñôËÆ≠ÁªÉÔºåÂ§ßÁ∫¶4B tokensÔºåÂç≥ÂØπÂ∫î‰∏äÈù¢ÁöÑÊï∞ÊçÆÁªÑÂêàËÆ≠ÁªÉÁªìÊûúÔºàÂºÄÈîÄÔºöüí∞üí∞üí∞üí∞üí∞üí∞üí∞üí∞ÔºåÊïàÊûúÔºöüòäüòäüòäüòäüòäüòäÔºâ

* ÊÉ≥Ë¶ÅÊúÄÂø´ÈÄüÂ∫¶‰ªé0ÂÆûÁé∞ZeroÊ®°ÂûãÔºåÊé®Ëçê‰ΩøÁî®`pretrain_hq.jsonl` + `sft_mini_512.jsonl` ÁöÑÊï∞ÊçÆÁªÑÂêàÔºåÂÖ∑‰ΩìËä±ÈîÄÂíåÊïàÊûúÂèØÊü•Áúã‰∏ãÊñáË°®Ê†ºÔºàÂºÄÈîÄÔºöüí∞ÔºåÊïàÊûúÔºöüòäüòäÔºâ

* Êé®ËçêÂÖ∑Â§á‰∏ÄÂÆöÁÆóÂäõËµÑÊ∫êÊàñÊõ¥Âú®ÊÑèÊïàÊûúÁöÑÊúãÂèãÂèØ‰ª•ËÄÉËôëÂâçËÄÖÂÆåÊï¥Â§çÁé∞MiniMind2Ôºõ‰ªÖÊúâÂçïÂç°GPUÊàñÂú®‰πéÁü≠Êó∂Èó¥Âø´ÈÄüÂ§çÁé∞ÁöÑÊúãÂèãÂº∫ÁÉàÊé®ËçêÂêéËÄÖÔºõ

* „ÄêÊäò‰∏≠ÊñπÊ°à„Äë‰∫¶ÂèØÈÄâÊã©‰æãÂ¶Ç`sft_mini_512.jsonl`„ÄÅ`sft_1024.jsonl`‰∏≠Á≠âËßÑÊ®°Êï∞ÊçÆËøõË°åËá™Áî±ÁªÑÂêàËÆ≠ÁªÉÔºàÂºÄÈîÄÔºöüí∞üí∞üí∞ÔºåÊïàÊûúÔºöüòäüòäüòäüòäÔºâ„ÄÇ

&lt;/details&gt;

# üìå Model

## Structure

MiniMind-DenseÔºàÂíå[Llama3.1](https://ai.meta.com/blog/meta-llama-3-1/)‰∏ÄÊ†∑Ôºâ‰ΩøÁî®‰∫ÜTransformerÁöÑDecoder-OnlyÁªìÊûÑÔºåË∑üGPT-3ÁöÑÂå∫Âà´Âú®‰∫éÔºö

* ÈááÁî®‰∫ÜGPT-3ÁöÑÈ¢ÑÊ†áÂáÜÂåñÊñπÊ≥ïÔºå‰πüÂ∞±ÊòØÂú®ÊØè‰∏™TransformerÂ≠êÂ±ÇÁöÑËæìÂÖ•‰∏äËøõË°åÂΩí‰∏ÄÂåñÔºåËÄå‰∏çÊòØÂú®ËæìÂá∫‰∏ä„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºå‰ΩøÁî®ÁöÑÊòØRMSNormÂΩí‰∏ÄÂåñÂáΩÊï∞„ÄÇ
* Áî®SwiGLUÊøÄÊ¥ªÂáΩÊï∞Êõø‰ª£‰∫ÜReLUÔºåËøôÊ†∑ÂÅöÊòØ‰∏∫‰∫ÜÊèêÈ´òÊÄßËÉΩ„ÄÇ
* ÂÉèGPT-Neo‰∏ÄÊ†∑ÔºåÂéªÊéâ‰∫ÜÁªùÂØπ‰ΩçÁΩÆÂµåÂÖ•ÔºåÊîπÁî®‰∫ÜÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÔºàRoPEÔºâÔºåËøôÊ†∑Âú®Â§ÑÁêÜË∂ÖÂá∫ËÆ≠ÁªÉÈïøÂ∫¶ÁöÑÊé®ÁêÜÊó∂ÊïàÊûúÊõ¥Â•Ω„ÄÇ

---

MiniMind-MoEÊ®°ÂûãÔºåÂÆÉÁöÑÁªìÊûÑÂü∫‰∫éLlama3Âíå[Deepseek-V2/3](https://arxiv.org/pdf/2405.04434)‰∏≠ÁöÑMixFFNÊ∑∑Âêà‰∏ìÂÆ∂Ê®°Âùó„ÄÇ

* DeepSeek-V2Âú®ÂâçÈ¶àÁΩëÁªúÔºàFFNÔºâÊñπÈù¢ÔºåÈááÁî®‰∫ÜÊõ¥ÁªÜÁ≤íÂ∫¶ÁöÑ‰∏ìÂÆ∂ÂàÜÂâ≤ÂíåÂÖ±‰∫´ÁöÑ‰∏ìÂÆ∂ÈöîÁ¶ªÊäÄÊúØÔºå‰ª•ÊèêÈ´òExpertsÁöÑÊïàÊûú„ÄÇ

---

MiniMindÁöÑÊï¥‰ΩìÁªìÊûÑ‰∏ÄËá¥ÔºåÂè™ÊòØÂú®RoPEËÆ°ÁÆó„ÄÅÊé®ÁêÜÂáΩÊï∞ÂíåFFNÂ±ÇÁöÑ‰ª£Á†Å‰∏äÂÅö‰∫Ü‰∏Ä‰∫õÂ∞èË∞ÉÊï¥„ÄÇ
ÂÖ∂ÁªìÊûÑÂ¶Ç‰∏ãÂõæÔºàÈáçÁªòÁâàÔºâÔºö

![structure](./images/LLM-structure.png)
![structure-moe](./images/LLM-structure-moe.png)

‰øÆÊîπÊ®°ÂûãÈÖçÁΩÆËßÅ[./model/model_minimind.py](./model/model_minimind.py)„ÄÇ
ÂèÇËÄÉÊ®°ÂûãÂèÇÊï∞ÁâàÊú¨ËßÅ‰∏ãË°®Ôºö

| Model Name        | params | len_vocab | rope_theta | n_layers | d_model | kv_heads | q_heads | share+route |
|-------------------|--------|-----------|------------|----------|---------|----------|---------|-------------|
| MiniMind2-Small   | 26M    | 6400      | 1e6        | 8        | 512     | 2        | 8       | -           |
| MiniMind2-MoE     | 145M   | 6400      | 1e6        | 8        | 640     | 2        | 8       | 1+4         |
| MiniMind2         | 104M   | 6400      | 1e6        | 16       | 768     | 2        | 8       | -           |
| minimind-v1-small | 26M    | 6400      | 1e4        | 8        | 512     | 8        | 16      | -           |
| minimind-v1-moe   | 4√ó26M  | 6400      | 1e4        | 8        | 512     | 8        | 16      | 1+4         |
| minimind-v1       | 108M   | 6400      | 1e4        | 16       | 768     | 8        | 16      | -           |


## Model Configuration

üìãÂÖ≥‰∫éLLMÁöÑÂèÇÊï∞ÈÖçÁΩÆÔºåÊúâ‰∏ÄÁØáÂæàÊúâÊÑèÊÄùÁöÑËÆ∫Êñá[MobileLLM](https://arxiv.org/pdf/2402.14905)ÂÅö‰∫ÜËØ¶ÁªÜÁöÑÁ†îÁ©∂ÂíåÂÆûÈ™å„ÄÇ
Scaling LawÂú®Â∞èÊ®°Âûã‰∏≠ÊúâËá™Â∑±Áã¨ÁâπÁöÑËßÑÂæã„ÄÇ
ÂºïËµ∑TransformerÂèÇÊï∞ÊàêËßÑÊ®°ÂèòÂåñÁöÑÂèÇÊï∞Âá†‰πéÂè™ÂèñÂÜ≥‰∫é`d_model`Âíå`n_layers`„ÄÇ

* `d_model`‚Üë + `n_layers`‚Üì -&gt; ÁüÆËÉñÂ≠ê
* `d_model`‚Üì + `n_layers`‚Üë -&gt; Áò¶È´ò‰∏™

2020Âπ¥ÊèêÂá∫Scaling LawÁöÑËÆ∫ÊñáËÆ§‰∏∫ÔºåËÆ≠ÁªÉÊï∞ÊçÆÈáè„ÄÅÂèÇÊï∞Èáè‰ª•ÂèäËÆ≠ÁªÉËø≠‰ª£Ê¨°Êï∞ÊâçÊòØÂÜ≥ÂÆöÊÄßËÉΩÁöÑÂÖ≥ÈîÆÂõ†Á¥†ÔºåËÄåÊ®°ÂûãÊû∂ÊûÑÁöÑÂΩ±ÂìçÂá†‰πéÂèØ‰ª•ÂøΩËßÜ„ÄÇ
ÁÑ∂ËÄå‰ºº‰πéËøô‰∏™ÂÆöÂæãÂØπÂ∞èÊ®°ÂûãÂπ∂‰∏çÂÆåÂÖ®ÈÄÇÁî®„ÄÇ
MobileLLMÊèêÂá∫Êû∂ÊûÑÁöÑÊ∑±Â∫¶ÊØîÂÆΩÂ∫¶Êõ¥ÈáçË¶ÅÔºå„ÄåÊ∑±ËÄåÁ™Ñ„ÄçÁöÑ„ÄåÁò¶Èïø„ÄçÊ®°ÂûãÂèØ‰ª•Â≠¶‰π†Âà∞ÊØî„ÄåÂÆΩËÄåÊµÖ„ÄçÊ®°ÂûãÊõ¥Â§öÁöÑÊäΩË±°Ê¶ÇÂøµ„ÄÇ
‰æãÂ¶ÇÂΩìÊ®°ÂûãÂèÇÊï∞Âõ∫ÂÆöÂú®125MÊàñËÄÖ350MÊó∂Ôºå30ÔΩû42Â±ÇÁöÑ„ÄåÁã≠Èïø„ÄçÊ®°ÂûãÊòéÊòæÊØî12Â±ÇÂ∑¶Âè≥ÁöÑ„ÄåÁüÆËÉñ„ÄçÊ®°ÂûãÊúâÊõ¥‰ºòË∂äÁöÑÊÄßËÉΩÔºå
Âú®Â∏∏ËØÜÊé®ÁêÜ„ÄÅÈóÆÁ≠î„ÄÅÈòÖËØªÁêÜËß£Á≠â8‰∏™Âü∫ÂáÜÊµãËØï‰∏äÈÉΩÊúâÁ±ª‰ººÁöÑË∂ãÂäø„ÄÇ
ËøôÂÖ∂ÂÆûÊòØÈùûÂ∏∏ÊúâË∂£ÁöÑÂèëÁé∞ÔºåÂõ†‰∏∫‰ª•ÂæÄ‰∏∫100MÂ∑¶Âè≥ÈáèÁ∫ßÁöÑÂ∞èÊ®°ÂûãËÆæËÆ°Êû∂ÊûÑÊó∂ÔºåÂá†‰πéÊ≤°‰∫∫Â∞ùËØïËøáÂè†Âä†Ë∂ÖËøá12Â±Ç„ÄÇ
Ëøô‰∏éMiniMindÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÊ®°ÂûãÂèÇÊï∞ÈáèÂú®`d_model`Âíå`n_layers`‰πãÈó¥ËøõË°åË∞ÉÊï¥ÂÆûÈ™åËßÇÂØüÂà∞ÁöÑÊïàÊûúÊòØ‰∏ÄËá¥ÁöÑ„ÄÇ
ÁÑ∂ËÄå„ÄåÊ∑±ËÄåÁ™Ñ„ÄçÁöÑ„ÄåÁ™Ñ„Äç‰πüÊòØÊúâÁª¥Â∫¶ÊûÅÈôêÁöÑÔºåÂΩìd_model&lt;512Êó∂ÔºåËØçÂµåÂÖ•Áª¥Â∫¶ÂùçÂ°åÁöÑÂä£ÂäøÈùûÂ∏∏ÊòéÊòæÔºå
Â¢ûÂä†ÁöÑlayersÂπ∂‰∏çËÉΩÂº•Ë°•ËØçÂµåÂÖ•Âú®Âõ∫ÂÆöq_headÂ∏¶Êù•d_head‰∏çË∂≥ÁöÑ

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[yt-dlp/yt-dlp]]></title>
            <link>https://github.com/yt-dlp/yt-dlp</link>
            <guid>https://github.com/yt-dlp/yt-dlp</guid>
            <pubDate>Tue, 03 Feb 2026 00:07:38 GMT</pubDate>
            <description><![CDATA[A feature-rich command-line audio/video downloader]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/yt-dlp/yt-dlp">yt-dlp/yt-dlp</a></h1>
            <p>A feature-rich command-line audio/video downloader</p>
            <p>Language: Python</p>
            <p>Stars: 145,486</p>
            <p>Forks: 11,776</p>
            <p>Stars today: 183 stars today</p>
            <h2>README</h2><pre>&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
&lt;div align=&quot;center&quot;&gt;

[![YT-DLP](https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/.github/banner.svg)](#readme)

[![Release version](https://img.shields.io/github/v/release/yt-dlp/yt-dlp?color=brightgreen&amp;label=Download&amp;style=for-the-badge)](#installation &quot;Installation&quot;)
[![PyPI](https://img.shields.io/badge/-PyPI-blue.svg?logo=pypi&amp;labelColor=555555&amp;style=for-the-badge)](https://pypi.org/project/yt-dlp &quot;PyPI&quot;)
[![Donate](https://img.shields.io/badge/_-Donate-red.svg?logo=githubsponsors&amp;labelColor=555555&amp;style=for-the-badge)](Maintainers.md#maintainers &quot;Donate&quot;)
[![Discord](https://img.shields.io/discord/807245652072857610?color=blue&amp;labelColor=555555&amp;label=&amp;logo=discord&amp;style=for-the-badge)](https://discord.gg/H5MNcFW63r &quot;Discord&quot;)
[![Supported Sites](https://img.shields.io/badge/-Supported_Sites-brightgreen.svg?style=for-the-badge)](supportedsites.md &quot;Supported Sites&quot;)
[![License: Unlicense](https://img.shields.io/badge/-Unlicense-blue.svg?style=for-the-badge)](LICENSE &quot;License&quot;)
[![CI Status](https://img.shields.io/github/actions/workflow/status/yt-dlp/yt-dlp/core.yml?branch=master&amp;label=Tests&amp;style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/actions &quot;CI Status&quot;)
[![Commits](https://img.shields.io/github/commit-activity/m/yt-dlp/yt-dlp?label=commits&amp;style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/commits &quot;Commit History&quot;)
[![Last Commit](https://img.shields.io/github/last-commit/yt-dlp/yt-dlp/master?label=&amp;style=for-the-badge&amp;display_timestamp=committer)](https://github.com/yt-dlp/yt-dlp/pulse/monthly &quot;Last activity&quot;)

&lt;/div&gt;
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

yt-dlp is a feature-rich command-line audio/video downloader with support for [thousands of sites](supportedsites.md). The project is a fork of [youtube-dl](https://github.com/ytdl-org/youtube-dl) based on the now inactive [youtube-dlc](https://github.com/blackjack4494/yt-dlc).

&lt;!-- MANPAGE: MOVE &quot;USAGE AND OPTIONS&quot; SECTION HERE --&gt;

&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
* [INSTALLATION](#installation)
    * [Detailed instructions](https://github.com/yt-dlp/yt-dlp/wiki/Installation)
    * [Release Files](#release-files)
    * [Update](#update)
    * [Dependencies](#dependencies)
    * [Compile](#compile)
* [USAGE AND OPTIONS](#usage-and-options)
    * [General Options](#general-options)
    * [Network Options](#network-options)
    * [Geo-restriction](#geo-restriction)
    * [Video Selection](#video-selection)
    * [Download Options](#download-options)
    * [Filesystem Options](#filesystem-options)
    * [Thumbnail Options](#thumbnail-options)
    * [Internet Shortcut Options](#internet-shortcut-options)
    * [Verbosity and Simulation Options](#verbosity-and-simulation-options)
    * [Workarounds](#workarounds)
    * [Video Format Options](#video-format-options)
    * [Subtitle Options](#subtitle-options)
    * [Authentication Options](#authentication-options)
    * [Post-processing Options](#post-processing-options)
    * [SponsorBlock Options](#sponsorblock-options)
    * [Extractor Options](#extractor-options)
    * [Preset Aliases](#preset-aliases)
* [CONFIGURATION](#configuration)
    * [Configuration file encoding](#configuration-file-encoding)
    * [Authentication with netrc](#authentication-with-netrc)
    * [Notes about environment variables](#notes-about-environment-variables)
* [OUTPUT TEMPLATE](#output-template)
    * [Output template examples](#output-template-examples)
* [FORMAT SELECTION](#format-selection)
    * [Filtering Formats](#filtering-formats)
    * [Sorting Formats](#sorting-formats)
    * [Format Selection examples](#format-selection-examples)
* [MODIFYING METADATA](#modifying-metadata)
    * [Modifying metadata examples](#modifying-metadata-examples)
* [EXTRACTOR ARGUMENTS](#extractor-arguments)
* [PLUGINS](#plugins)
    * [Installing Plugins](#installing-plugins)
    * [Developing Plugins](#developing-plugins)
* [EMBEDDING YT-DLP](#embedding-yt-dlp)
    * [Embedding examples](#embedding-examples)
* [CHANGES FROM YOUTUBE-DL](#changes-from-youtube-dl)
    * [New features](#new-features)
    * [Differences in default behavior](#differences-in-default-behavior)
    * [Deprecated options](#deprecated-options)
* [CONTRIBUTING](CONTRIBUTING.md#contributing-to-yt-dlp)
    * [Opening an Issue](CONTRIBUTING.md#opening-an-issue)
    * [Developer Instructions](CONTRIBUTING.md#developer-instructions)
* [WIKI](https://github.com/yt-dlp/yt-dlp/wiki)
    * [FAQ](https://github.com/yt-dlp/yt-dlp/wiki/FAQ)
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;


# INSTALLATION

&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
[![Windows](https://img.shields.io/badge/-Windows_x64-blue.svg?style=for-the-badge&amp;logo=windows)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe)
[![Unix](https://img.shields.io/badge/-Linux/BSD-red.svg?style=for-the-badge&amp;logo=linux)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp)
[![MacOS](https://img.shields.io/badge/-MacOS-lightblue.svg?style=for-the-badge&amp;logo=apple)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos)
[![PyPI](https://img.shields.io/badge/-PyPI-blue.svg?logo=pypi&amp;labelColor=555555&amp;style=for-the-badge)](https://pypi.org/project/yt-dlp)
[![Source Tarball](https://img.shields.io/badge/-Source_tar-green.svg?style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)
[![Other variants](https://img.shields.io/badge/-Other-grey.svg?style=for-the-badge)](#release-files)
[![All versions](https://img.shields.io/badge/-All_Versions-lightgrey.svg?style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/releases)
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

You can install yt-dlp using [the binaries](#release-files), [pip](https://pypi.org/project/yt-dlp) or one using a third-party package manager. See [the wiki](https://github.com/yt-dlp/yt-dlp/wiki/Installation) for detailed instructions


&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
## RELEASE FILES

#### Recommended

File|Description
:---|:---
[yt-dlp](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp)|Platform-independent [zipimport](https://docs.python.org/3/library/zipimport.html) binary. Needs Python (recommended for **Linux/BSD**)
[yt-dlp.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe)|Windows (Win8+) standalone x64 binary (recommended for **Windows**)
[yt-dlp_macos](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos)|Universal MacOS (10.15+) standalone executable (recommended for **MacOS**)

#### Alternatives

File|Description
:---|:---
[yt-dlp_linux](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux)|Linux (glibc 2.17+) standalone x86_64 binary
[yt-dlp_linux.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux.zip)|Unpackaged Linux (glibc 2.17+) x86_64 executable (no auto-update)
[yt-dlp_linux_aarch64](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_aarch64)|Linux (glibc 2.17+) standalone aarch64 binary
[yt-dlp_linux_aarch64.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_aarch64.zip)|Unpackaged Linux (glibc 2.17+) aarch64 executable (no auto-update)
[yt-dlp_linux_armv7l.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_armv7l.zip)|Unpackaged Linux (glibc 2.31+) armv7l executable (no auto-update)
[yt-dlp_musllinux](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux)|Linux (musl 1.2+) standalone x86_64 binary
[yt-dlp_musllinux.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux.zip)|Unpackaged Linux (musl 1.2+) x86_64 executable (no auto-update)
[yt-dlp_musllinux_aarch64](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux_aarch64)|Linux (musl 1.2+) standalone aarch64 binary
[yt-dlp_musllinux_aarch64.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux_aarch64.zip)|Unpackaged Linux (musl 1.2+) aarch64 executable (no auto-update)
[yt-dlp_x86.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_x86.exe)|Windows (Win8+) standalone x86 (32-bit) binary
[yt-dlp_win_x86.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win_x86.zip)|Unpackaged Windows (Win8+) x86 (32-bit) executable (no auto-update)
[yt-dlp_arm64.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_arm64.exe)|Windows (Win10+) standalone ARM64 binary
[yt-dlp_win_arm64.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win_arm64.zip)|Unpackaged Windows (Win10+) ARM64 executable (no auto-update)
[yt-dlp_win.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win.zip)|Unpackaged Windows (Win8+) x64 executable (no auto-update)
[yt-dlp_macos.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos.zip)|Unpackaged MacOS (10.15+) executable (no auto-update)

#### Misc

File|Description
:---|:---
[yt-dlp.tar.gz](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)|Source tarball
[SHA2-512SUMS](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS)|GNU-style SHA512 sums
[SHA2-512SUMS.sig](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS.sig)|GPG signature file for SHA512 sums
[SHA2-256SUMS](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS)|GNU-style SHA256 sums
[SHA2-256SUMS.sig](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS.sig)|GPG signature file for SHA256 sums

The public key that can be used to verify the GPG signatures is [available here](https://github.com/yt-dlp/yt-dlp/blob/master/public.key)
Example usage:
```
curl -L https://github.com/yt-dlp/yt-dlp/raw/master/public.key | gpg --import
gpg --verify SHA2-256SUMS.sig SHA2-256SUMS
gpg --verify SHA2-512SUMS.sig SHA2-512SUMS
```

#### Licensing

While yt-dlp is licensed under the [Unlicense](LICENSE), many of the release files contain code from other projects with different licenses.

Most notably, the PyInstaller-bundled executables include GPLv3+ licensed code, and as such the combined work is licensed under [GPLv3+](https://www.gnu.org/licenses/gpl-3.0.html).

The zipimport Unix executable (`yt-dlp`) contains [ISC](https://github.com/meriyah/meriyah/blob/main/LICENSE.md) licensed code from [`meriyah`](https://github.com/meriyah/meriyah) and [MIT](https://github.com/davidbonnet/astring/blob/main/LICENSE) licensed code from [`astring`](https://github.com/davidbonnet/astring).

See [THIRD_PARTY_LICENSES.txt](THIRD_PARTY_LICENSES.txt) for more details.

The git repository, the source tarball (`yt-dlp.tar.gz`), the PyPI source distribution and the PyPI built distribution (wheel) only contain code licensed under the [Unlicense](LICENSE).

&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

**Note**: The manpages, shell completion (autocomplete) files etc. are available inside the [source tarball](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)


## UPDATE
You can use `yt-dlp -U` to update if you are using the [release binaries](#release-files)

If you [installed with pip](https://github.com/yt-dlp/yt-dlp/wiki/Installation#with-pip), simply re-run the same command that was used to install the program

For other third-party package managers, see [the wiki](https://github.com/yt-dlp/yt-dlp/wiki/Installation#third-party-package-managers) or refer to their documentation

&lt;a id=&quot;update-channels&quot;&gt;&lt;/a&gt;

There are currently three release channels for binaries: `stable`, `nightly` and `master`.

* `stable` is the default channel, and many of its changes have been tested by users of the `nightly` and `master` channels.
* The `nightly` channel has releases scheduled to build every day around midnight UTC, for a snapshot of the project&#039;s new patches and changes. This is the **recommended channel for regular users** of yt-dlp. The `nightly` releases are available from [yt-dlp/yt-dlp-nightly-builds](https://github.com/yt-dlp/yt-dlp-nightly-builds/releases) or as development releases of the `yt-dlp` PyPI package (which can be installed with pip&#039;s `--pre` flag).
* The `master` channel features releases that are built after each push to the master branch, and these will have the very latest fixes and additions, but may also be more prone to regressions. They are available from [yt-dlp/yt-dlp-master-builds](https://github.com/yt-dlp/yt-dlp-master-builds/releases).

When using `--update`/`-U`, a release binary will only update to its current channel.
`--update-to CHANNEL` can be used to switch to a different channel when a newer version is available. `--update-to [CHANNEL@]TAG` can also be used to upgrade or downgrade to specific tags from a channel.

You may also use `--update-to &lt;repository&gt;` (`&lt;owner&gt;/&lt;repository&gt;`) to update to a channel on a completely different repository. Be careful with what repository you are updating to though, there is no verification done for binaries from different repositories.

Example usage:

* `yt-dlp --update-to master` switch to the `master` channel and update to its latest release
* `yt-dlp --update-to stable@2023.07.06` upgrade/downgrade to release to `stable` channel tag `2023.07.06`
* `yt-dlp --update-to 2023.10.07` upgrade/downgrade to tag `2023.10.07` if it exists on the current channel
* `yt-dlp --update-to example/yt-dlp@2023.09.24` upgrade/downgrade to the release from the `example/yt-dlp` repository, tag `2023.09.24`

**Important**: Any user experiencing an issue with the `stable` release should install or update to the `nightly` release before submitting a bug report:
```
# To update to nightly from stable executable/binary:
yt-dlp --update-to nightly

# To install nightly with pip:
python -m pip install -U --pre &quot;yt-dlp[default]&quot;
```

When running a yt-dlp version that is older than 90 days, you will see a warning message suggesting to update to the latest version.
You can suppress this warning by adding `--no-update` to your command or configuration file.

## DEPENDENCIES
Python versions 3.10+ (CPython) and 3.11+ (PyPy) are supported. Other versions and implementations may or may not work correctly.

&lt;!-- Python 3.5+ uses VC++14 and it is already embedded in the binary created
&lt;!x-- https://www.microsoft.com/en-us/download/details.aspx?id=26999 --x&gt;
On Windows, [Microsoft Visual C++ 2010 SP1 Redistributable Package (x86)](https://download.microsoft.com/download/1/6/5/165255E7-1014-4D0A-B094-B6A430A6BFFC/vcredist_x86.exe) is also necessary to run yt-dlp. You probably already have this, but if the executable throws an error due to missing `MSVCR100.dll` you need to install it manually.
--&gt;

While all the other dependencies are optional, `ffmpeg`, `ffprobe`, `yt-dlp-ejs` and a supported JavaScript runtime/engine are highly recommended

### Strongly recommended

* [**ffmpeg** and **ffprobe**](https://www.ffmpeg.org) - Required for [merging separate video and audio files](#format-selection), as well as for various [post-processing](#post-processing-options) tasks. License [depends on the build](https://www.ffmpeg.org/legal.html)

    There are bugs in ffmpeg that cause various issues when used alongside yt-dlp. Since ffmpeg is such an important dependency, we provide [custom builds](https://github.com/yt-dlp/FFmpeg-Builds#ffmpeg-static-auto-builds) with patches for some of these issues at [yt-dlp/FFmpeg-Builds](https://github.com/yt-dlp/FFmpeg-Builds). See [the readme](https://github.com/yt-dlp/FFmpeg-Builds#patches-applied) for details on the specific issues solved by these builds

    **Important**: What you need is ffmpeg *binary*, **NOT** [the Python package of the same name](https://pypi.org/project/ffmpeg)

* [**yt-dlp-ejs**](https://github.com/yt-dlp/ejs) - Required for full YouTube support. Licensed under [Unlicense](https://github.com/yt-dlp/ejs/blob/main/LICENSE), bundles [MIT](https://github.com/davidbonnet/astring/blob/main/LICENSE) and [ISC](https://github.com/meriyah/meriyah/blob/main/LICENSE.md) components.

    A JavaScript runtime/engine like [**deno**](https://deno.land) (recommended), [**node.js**](https://nodejs.org), [**bun**](https://bun.sh), or [**QuickJS**](https://bellard.org/quickjs/) is also required to run yt-dlp-ejs. See [the wiki](https://github.com/yt-dlp/yt-dlp/wiki/EJS).

### Networking
* [**certifi**](https://github.com/certifi/python-certifi)\* - Provides Mozilla&#039;s root certificate bundle. Licensed under [MPLv2](https://github.com/certifi/python-certifi/blob/master/LICENSE)
* [**brotli**](https://github.com/google/brotli)\* or [**brotlicffi**](https://github.com/python-hyper/brotlicffi) - [Brotli](https://en.wikipedia.org/wiki/Brotli) content encoding support. Both licensed under MIT &lt;sup&gt;[1](https://github.com/google/brotli/blob/master/LICENSE) [2](https://github.com/python-hyper/brotlicffi/blob/master/LICENSE) &lt;/sup&gt;
* [**websockets**](https://github.com/aaugustin/websockets)\* - For downloading over websocket. Licensed under [BSD-3-Clause](https://github.com/aaugustin/websockets/blob/main/LICENSE)
* [**requests**](https://github.com/psf/requests)\* - HTTP library. For HTTPS proxy and persistent connections support. Licensed under [Apache-2.0](https://github.com/psf/requests/blob/main/LICENSE)

#### Impersonation

The following provide support for impersonating browser requests. This may be required for some sites that employ TLS fingerprinting.

* [**curl_cffi**](https://github.com/lexiforest/curl_cffi) (recommended) - Python binding for [curl-impersonate](https://github.com/lexiforest/curl-impersonate). Provides impersonation targets for Chrome, Edge and Safari. Licensed under [MIT](https://github.com/lexiforest/curl_cffi/blob/main/LICENSE)
  * Can be installed with the `curl-cffi` extra, e.g. `pip install &quot;yt-dlp[default,curl-cffi]&quot;`
  * Currently included in most builds *except* `yt-dlp` (Unix zipimport binary), `yt-dlp_x86` (Windows 32-bit) and `yt-dlp_musllinux_aarch64`


### Metadata

* [**mutagen**](https://github.com/quodlibet/mutagen)\* - For `--embed-thumbnail` in certain formats. Licensed under [GPLv2+](https://github.com/quodlibet/mutagen/blob/master/COPYING)
* [**AtomicParsley**](https://github.com/wez/atomicparsley) - For `--embed-thumbnail` in `mp4`/`m4a` files when `mutagen`/`ffmpeg` cannot. Licensed under [GPLv2+](https://github.com/wez/atomicparsley/blob/master/COPYING)
* [**xattr**](https://github.com/xattr/xattr), [**pyxattr**](https://github.com/iustin/pyxattr) or [**setfattr**](http://savannah.nongnu.org/projects/attr) - For writing xattr metadata (`--xattrs`) on **Mac** and **BSD**. Licensed under [MIT](https://github.com/xattr/xattr/blob/master/LICENSE.txt), [LGPL2.1](https://github.com/iustin/pyxattr/blob/master/COPYING) and [GPLv2+](http://git.savannah.nongnu.org/cgit/attr.git/tree/doc/COPYING) respectively

### Misc

* [**pycryptodomex**](https://github.com/Legrandin/pycryptodome)\* - For decrypting AES-128 HLS streams and various other data. Licensed under [BSD-2-Clause](https://github.com/Legrandin/pycryptodome/blob/master/LICENSE.rst)
* [**phantomjs**](https://github.com/ariya/phantomjs) - Used in some extractors where JavaScript needs to be run. No longer used for YouTube. To be deprecated in the near future. Licensed under [BSD-3-Clause](https://github.com/ariya/phantomjs/blob/master/LICENSE.BSD)
* [**secretstorage**](https://github.com/mitya57/secretstorage)\* - For `--cookies-from-browser` to access the **Gnome** keyring while decrypting cookies of **Chromium**-based browsers on **Linux**. Licensed under [BSD-3-Clause](https://github.com/mitya57/secretstorage/blob/master/LICENSE)
* Any external downloader that you want to use with `--downloader`

### Deprecated

* [**rtmpdump**](http://rtmpdump.mplayerhq.hu) - For downloading `rtmp` streams. ffmpeg can be used instead with `--downloader ffmpeg`. Licensed under [GPLv2+](http://rtmpdump.mplayerhq.hu)
* [**mplayer**](http://mplayerhq.hu/design7/info.html) or [**mpv**](https://

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[home-assistant/core]]></title>
            <link>https://github.com/home-assistant/core</link>
            <guid>https://github.com/home-assistant/core</guid>
            <pubDate>Tue, 03 Feb 2026 00:07:37 GMT</pubDate>
            <description><![CDATA[üè° Open source home automation that puts local control and privacy first.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/home-assistant/core">home-assistant/core</a></h1>
            <p>üè° Open source home automation that puts local control and privacy first.</p>
            <p>Language: Python</p>
            <p>Stars: 84,548</p>
            <p>Forks: 36,673</p>
            <p>Stars today: 29 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[happycola233/tchMaterial-parser]]></title>
            <link>https://github.com/happycola233/tchMaterial-parser</link>
            <guid>https://github.com/happycola233/tchMaterial-parser</guid>
            <pubDate>Tue, 03 Feb 2026 00:07:36 GMT</pubDate>
            <description><![CDATA[ÂõΩÂÆ∂‰∏≠Â∞èÂ≠¶Êô∫ÊÖßÊïôËÇ≤Âπ≥Âè∞ ÁîµÂ≠êËØæÊú¨‰∏ãËΩΩÂ∑•ÂÖ∑ÔºåÂ∏ÆÂä©ÊÇ®‰ªéÊô∫ÊÖßÊïôËÇ≤Âπ≥Âè∞‰∏≠Ëé∑ÂèñÁîµÂ≠êËØæÊú¨ÁöÑ PDF Êñá‰ª∂ÁΩëÂùÄÂπ∂ËøõË°å‰∏ãËΩΩÔºåËÆ©ÊÇ®Êõ¥Êñπ‰æøÂú∞Ëé∑ÂèñËØæÊú¨ÂÜÖÂÆπ„ÄÇ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/happycola233/tchMaterial-parser">happycola233/tchMaterial-parser</a></h1>
            <p>ÂõΩÂÆ∂‰∏≠Â∞èÂ≠¶Êô∫ÊÖßÊïôËÇ≤Âπ≥Âè∞ ÁîµÂ≠êËØæÊú¨‰∏ãËΩΩÂ∑•ÂÖ∑ÔºåÂ∏ÆÂä©ÊÇ®‰ªéÊô∫ÊÖßÊïôËÇ≤Âπ≥Âè∞‰∏≠Ëé∑ÂèñÁîµÂ≠êËØæÊú¨ÁöÑ PDF Êñá‰ª∂ÁΩëÂùÄÂπ∂ËøõË°å‰∏ãËΩΩÔºåËÆ©ÊÇ®Êõ¥Êñπ‰æøÂú∞Ëé∑ÂèñËØæÊú¨ÂÜÖÂÆπ„ÄÇ</p>
            <p>Language: Python</p>
            <p>Stars: 4,443</p>
            <p>Forks: 535</p>
            <p>Stars today: 34 stars today</p>
            <h2>README</h2><pre># [ÂõΩÂÆ∂‰∏≠Â∞èÂ≠¶Êô∫ÊÖßÊïôËÇ≤Âπ≥Âè∞ ÁîµÂ≠êËØæÊú¨](https://basic.smartedu.cn/tchMaterial/)‰∏ãËΩΩÂ∑•ÂÖ∑

![Python Version](https://img.shields.io/badge/Python-3.x-blue.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Made With Love‚ù§Ô∏è](https://img.shields.io/badge/Made_With-%E2%9D%A4-red.svg)

Êú¨Â∑•ÂÖ∑ÂèØ‰ª•Â∏ÆÂä©ÊÇ®‰ªé[**ÂõΩÂÆ∂‰∏≠Â∞èÂ≠¶Êô∫ÊÖßÊïôËÇ≤Âπ≥Âè∞**](https://basic.smartedu.cn/)Ëé∑ÂèñÁîµÂ≠êËØæÊú¨ÁöÑ PDF Êñá‰ª∂ÁΩëÂùÄÂπ∂ËøõË°å‰∏ãËΩΩÔºåËÆ©ÊÇ®Êõ¥Êñπ‰æøÂú∞Ëé∑ÂèñËØæÊú¨ÂÜÖÂÆπ„ÄÇ

## ‚ú® Â∑•ÂÖ∑ÁâπÁÇπ

- üìö**ÊîØÊåÅÊâπÈáè‰∏ãËΩΩ**Ôºö‰∏ÄÊ¨°ËæìÂÖ•Â§ö‰∏™ÁîµÂ≠êËØæÊú¨È¢ÑËßàÈ°µÈù¢ÁΩëÂùÄÔºåÂç≥ÂèØÊâπÈáè‰∏ãËΩΩ PDF ËØæÊú¨Êñá‰ª∂„ÄÇ
- üìÇ**Ëá™Âä®ÂëΩÂêçÊñá‰ª∂**ÔºöÂ∑•ÂÖ∑‰ºöËá™Âä®‰ΩøÁî®ÁîµÂ≠êËØæÊú¨ÁöÑÂêçÁß∞‰Ωú‰∏∫ÈªòËÆ§Êñá‰ª∂ÂêçÔºåÊñπ‰æøÁÆ°ÁêÜ‰∏ãËΩΩÁöÑËØæÊú¨Êñá‰ª∂„ÄÇ
- üîñ**Ëá™Âä®Ê∑ªÂä†‰π¶Á≠æ**ÔºöËã•ÂãæÈÄâ‰∫Ü ‚ÄúÊ∑ªÂä†‰π¶Á≠æ‚Äù ÈÄâÈ°πÔºåÂàô‰ºöÂú®‰∏ãËΩΩÂÆåÊàêÂêé‰∏∫ÁîµÂ≠êËØæÊú¨Ê∑ªÂä†‰π¶Á≠æÔºåÂú®Êü•Áúã PDF Êó∂ÂèØÊõ¥Êñπ‰æøÂú∞Ë∑≥ËΩ¨Âà∞ÊåáÂÆö‰ΩçÁΩÆ„ÄÇ
- üîë**ÊîØÊåÅ Access Token**ÔºöÊîØÊåÅÁî®Êà∑[ÊâãÂä®ËæìÂÖ• Access Token](#2--ËÆæÁΩÆ-access-tokenÂèØÈÄâ) Âπ∂Ëá™Âä®‰øùÂ≠òÔºå‰∏ãÊ¨°ÂêØÂä®ÂèØËá™Âä®Âä†ËΩΩ„ÄÇ
- üñ•Ô∏è**È´ò DPI ÈÄÇÈÖç**Ôºö‰ºòÂåñ UI ‰ª•ÈÄÇÈÖçÈ´òÂàÜËæ®ÁéáÂ±èÂπïÔºåÈÅøÂÖçÁïåÈù¢Ê®°Á≥äÈóÆÈ¢ò„ÄÇ
- üíª**Ë∑®Âπ≥Âè∞ÊîØÊåÅ**ÔºöÊîØÊåÅ Windows„ÄÅLinux„ÄÅmacOS Á≠âÊìç‰ΩúÁ≥ªÁªüÔºàÈúÄË¶ÅÂõæÂΩ¢ÁïåÈù¢Ôºâ„ÄÇ

![Â∑•ÂÖ∑Êà™Âõæ](./res/main.png)

## üì• ‰∏ãËΩΩ‰∏éÂÆâË£ÖÊñπÊ≥ï

### GitHub Releases

Êú¨È°πÁõÆÁöÑ [GitHub Releases È°µÈù¢](https://github.com/happycola233/tchMaterial-parser/releases)‰ºöÂèëÂ∏É**ÈÄÇÁî®‰∫é Windows„ÄÅLinux ÁöÑ x86_64 Êû∂ÊûÑ**‰∏é**ÈÄÇÁî®‰∫é Linux„ÄÅmacOS ÁöÑ Arm64 Êû∂ÊûÑ**ÁöÑÁ®ãÂ∫è„ÄÇ

‰∏ãËΩΩÂÆåÊàê‰πãÂêé‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÁöÑÂÆâË£ÖÊ≠•È™§„ÄÇWindows Âíå Linux ÂèØÁõ¥Êé•ËøêË°åÊú¨Á®ãÂ∫è„ÄÇ

&gt; [!WARNING]
&gt; Âú® macOS Êìç‰ΩúÁ≥ªÁªü‰∏≠ÔºåÁî±‰∫éÊ≤°ÊúâÁ≠æÂêçÔºåÁ≥ªÁªü‰ºöÊä•ÂëäÊñá‰ª∂Â∑≤Ë¢´ÊçüÂùèÔºåÂõ†Ê≠§ÈúÄË¶ÅÂÖàËøêË°å `xattr -cr /path/to/tchMaterial-parser.app` Êù•ÁßªÈô§Â∫îÁî®ÁöÑ ‚ÄúÈöîÁ¶ª‚Äù Â±ûÊÄß„ÄÇ‰∏∫‰∫Ü‰øùËØÅ Access Token ÁöÑÊåÅ‰πÖÂåñÔºåÂª∫ËÆÆÂ∞ÜÂ∫îÁî®ÁßªÂä®Âà∞ `/Applications` ÁõÆÂΩï‰∏ãÂÜçËøêË°å„ÄÇ

### Arch Áî®Êà∑ËΩØ‰ª∂‰ªìÂ∫ìÔºàAURÔºâ

ÂØπ‰∫é **Arch Linux** Êìç‰ΩúÁ≥ªÁªüÔºåÊú¨Á®ãÂ∫èÂ∑≤ÂèëÂ∏ÉËá≥[Arch Áî®Êà∑ËΩØ‰ª∂‰ªìÂ∫ì](https://aur.archlinux.org/packages/tchmaterial-parser)ÔºåÂõ†Ê≠§ÊÇ®ËøòÂèØ‰ª•ÈÄöËøáÂú®ÁªàÁ´Ø‰∏≠ËæìÂÖ•‰ª•‰∏ãÂëΩ‰ª§ÂÆâË£ÖÔºö

```sh
yay -S tchmaterial-parser
```

ÊÑüË∞¢ [@iamzhz](https://github.com/iamzhz) ‰∏∫Êú¨Â∑•ÂÖ∑Âà∂‰Ωú‰∫ÜÂèëË°åÂåÖÔºà[#26](../../issues/26)ÔºâÔºÅ

## üõ†Ô∏è ‰ΩøÁî®ÊñπÊ≥ï

### 1. ‚å®Ô∏è ËæìÂÖ•ÁîµÂ≠êËØæÊú¨ÈìæÊé•

Â∞ÜÁîµÂ≠êËØæÊú¨ÁöÑ**È¢ÑËßàÈ°µÈù¢ÁΩëÂùÄ**Á≤òË¥¥Âà∞Â∑•ÂÖ∑ÊñáÊú¨Ê°Ü‰∏≠ÔºåÊîØÊåÅÂ§ö‰∏™ URLÔºàÊØèË°å‰∏Ä‰∏™Ôºâ„ÄÇ

**Á§∫‰æãÁΩëÂùÄ**Ôºö

```text
https://basic.smartedu.cn/tchMaterial/detail?contentType=assets_document&amp;contentId=XXXXXX&amp;catalogType=tchMaterial&amp;subCatalog=tchMaterial
```

### 2. üîë ËÆæÁΩÆ Access TokenÔºàÂèØÈÄâÔºâ

&gt; [!TIP]
&gt; Ëá™ v3.1 ÁâàÊú¨Ëµ∑ÔºåËøô‰∏ÄÊ≠•Êìç‰ΩúÂ∑≤Áªè**‰∏çÂÜçÂøÖË¶Å**ÔºåÂΩìÊú™ËÆæÁΩÆ Access Token Êó∂Â∑•ÂÖ∑‰ºö‰ΩøÁî®ÂÖ∂‰ªñÊñπÊ≥ï‰∏ãËΩΩËµÑÊ∫ê„ÄÇÁÑ∂ËÄåÔºåËøô‰∏ÄÊñπÊ≥ï**Âπ∂‰∏çÈïøÊúüÊúâÊïà**ÔºåÂõ†Ê≠§‰ªçÁÑ∂Âª∫ËÆÆÊÇ®ËøõË°åËøô‰∏ÄÊ≠•Êìç‰Ωú„ÄÇ

1. **ÊâìÂºÄÊµèËßàÂô®**ÔºåËÆøÈóÆ[ÂõΩÂÆ∂‰∏≠Â∞èÂ≠¶Êô∫ÊÖßÊïôËÇ≤Âπ≥Âè∞](https://auth.smartedu.cn/uias/login)Âπ∂**ÁôªÂΩïË¥¶Âè∑**„ÄÇ
2. Êåâ‰∏ã **F12** Êàñ **Ctrl+Shift+I**ÔºåÊàñÂè≥ÈîÆ‚Äî‚ÄîÊ£ÄÊü•ÔºàÂÆ°Êü•ÂÖÉÁ¥†ÔºâÊâìÂºÄ**ÂºÄÂèëËÄÖÂ∑•ÂÖ∑**ÔºåÈÄâÊã©**ÊéßÂà∂Âè∞ÔºàConsoleÔºâ**„ÄÇ
3. Âú®ÊéßÂà∂Âè∞Á≤òË¥¥‰ª•‰∏ã‰ª£Á†ÅÂêéÂõûËΩ¶ÔºàEnterÔºâÔºö

   ```js
   (function () {
     const authKey = Object.keys(localStorage).find((key) =&gt;
       key.startsWith(&quot;ND_UC_AUTH&quot;),
     );
     if (!authKey) {
       console.error(&quot;Êú™ÊâæÂà∞ Access TokenÔºåËØ∑Á°Æ‰øùÂ∑≤ÁôªÂΩïÔºÅ&quot;);
       return;
     }
     const tokenData = JSON.parse(localStorage.getItem(authKey));
     const accessToken = JSON.parse(tokenData.value).access_token;
     console.log(
       &quot;%cAccess Token:&quot;,
       &quot;color: green; font-weight: bold&quot;,
       accessToken,
     );
   })();
   ```

4. Â§çÂà∂ÊéßÂà∂Âè∞ËæìÂá∫ÁöÑ **Access Token**ÔºåÁÑ∂ÂêéÂú®Êú¨Â∑•ÂÖ∑‰∏≠ÁÇπÂáª ‚Äú**ËÆæÁΩÆ Token**‚Äù ÊåâÈíÆÔºåÁ≤òË¥¥Âπ∂‰øùÂ≠ò Token„ÄÇ

&gt; [!NOTE]
&gt; Access Token ÂèØËÉΩ‰ºöËøáÊúüÔºåËã•‰∏ãËΩΩÂ§±Ë¥•ÔºåËØ∑ÈáçÊñ∞Ëé∑ÂèñÂπ∂ËÆæÁΩÆÊñ∞ÁöÑ Token„ÄÇ

### 3. üöÄ ÂºÄÂßã‰∏ãËΩΩ

ÁÇπÂáª ‚Äú**‰∏ãËΩΩ**‚Äù ÊåâÈíÆÔºåÂ∑•ÂÖ∑Â∞ÜËá™Âä®Ëß£ÊûêÂπ∂‰∏ãËΩΩ PDF ËØæÊú¨„ÄÇ

Êú¨Â∑•ÂÖ∑ÊîØÊåÅ**ÊâπÈáè‰∏ãËΩΩ**ÔºåÊâÄÊúâ PDF Êñá‰ª∂‰ºöËá™Âä®ÊåâËØæÊú¨ÂêçÁß∞ÂëΩÂêçÂπ∂‰øùÂ≠òÂú®ÈÄâÂÆöÁõÆÂΩï‰∏≠„ÄÇ

Ëã•ÊÇ®ÂãæÈÄâ‰∫Ü ‚Äú**ËÆæÁΩÆ‰π¶Á≠æ**‚Äù Â§çÈÄâÊ°ÜÔºåÂàôÊú¨Â∑•ÂÖ∑‰ºöÂú®ËØæÊú¨‰∏ãËΩΩÂÆåÊàêÂêéËá™Âä®‰∏∫ÂÖ∂Ê∑ªÂä†‰π¶Á≠æÔºåÂú®Êü•Áúã PDF Êó∂ÂèØÂø´ÈÄüË∑≥ËΩ¨Âà∞ÊåáÂÆö‰ΩçÁΩÆ„ÄÇ

![Ê∑ªÂä†‰∫Ü‰π¶Á≠æÁöÑ PDF Êñá‰ª∂](./res/bookmark.png)

## ‚ùì Â∏∏ËßÅÈóÆÈ¢ò

### 1. ‚ö†Ô∏è ‰∏∫‰ªÄ‰πà‰∏ãËΩΩÂ§±Ë¥•Ôºü

- Â¶ÇÊûúÊÇ®Ê≤°ÊúâËÆæÁΩÆ Access TokenÔºåÂèØËÉΩÊòØÊú¨Â∑•ÂÖ∑‰ΩøÁî®ÁöÑÊñπÊ≥ïÂ§±Êïà‰∫ÜÔºåËØ∑[**ËÆæÁΩÆ Access Token**](#2--ËÆæÁΩÆ-access-tokenÂèØÈÄâ)üîë„ÄÇ
- Â¶ÇÊûúÊÇ®ËÆæÁΩÆ‰∫Ü Access TokenÔºåÁî±‰∫éÂÖ∂ÂÖ∑ÊúâÊó∂ÊïàÊÄßÔºà‰∏ÄËà¨‰∏∫ 7 Â§©ÔºâÔºåÂõ†Ê≠§ÊûÅÊúâÂèØËÉΩÊòØ **Access Token ËøáÊúü‰∫Ü**ÔºåËØ∑ÈáçÊñ∞Ëé∑ÂèñÊñ∞ÁöÑ Access Token„ÄÇ
- **Á°ÆËÆ§ÁΩëÁªúËøûÊé•ÊòØÂê¶Ê≠£Â∏∏**üåêÔºåÊúâÊó∂ÁΩëÁªú‰∏çÁ®≥ÂÆöÂèØËÉΩÂØºËá¥‰∏ãËΩΩÂ§±Ë¥•„ÄÇ
- **Á°Æ‰øùËæìÂÖ•ÁöÑÁΩëÂùÄÊúâÊïà**üîóÔºåÈÉ®ÂàÜÊóßËµÑÊ∫êÂèØËÉΩÂ∑≤Ë¢´ÁßªÈô§„ÄÇ

### 2. üíæAccess Token ‰øùÂ≠òÂú®Âì™ÈáåÔºü

- **Windows**ÔºöToken ‰ºöÂ≠òÂÇ®Âú®**Ê≥®ÂÜåË°®** `HKEY_CURRENT_USER\Software\tchMaterial-parser` È°π‰∏≠ÁöÑ `AccessToken` ÂÄº„ÄÇ
- **Linux**: Token ‰ºöÂ≠òÂÇ®Âú®**Êñá‰ª∂** `~/.config/tchMaterial-parser/data.json` ‰∏≠„ÄÇ
- **macOS**ÔºöToken ‰ºöÂ≠òÂÇ®Âú®**Êñá‰ª∂** `~/Library/Application Support/tchMaterial-parser/data.json` ‰∏≠„ÄÇ
- **ÂÖ∂‰ªñÊìç‰ΩúÁ≥ªÁªü**ÔºöToken ‰ªÖÂú®ËøêË°åÊó∂‰∏¥Êó∂Â≠òÂÇ®‰∫éÂÜÖÂ≠òÔºå‰∏ç‰ºöËá™Âä®‰øùÂ≠òÔºåÁ®ãÂ∫èÈáçÂêØÂêéÈúÄÈáçÊñ∞ËæìÂÖ•ÔºåÁõÆÂâçÊàë‰ª¨Ê≠£Âú®Âä™ÂäõÊîπËøõËØ•ÂäüËÉΩ„ÄÇ

### 3. üîêToken ‰ºö‰∏ç‰ºöÊ≥ÑÈú≤Ôºü

- Êú¨Â∑•ÂÖ∑**‰∏ç‰ºö‰∏ä‰º†** TokenÔºå‰πü‰∏ç‰ºöÂ≠òÂÇ®Âú®‰∫ëÁ´ØÔºå‰ªÖÁî®‰∫éÊú¨Âú∞ËØ∑Ê±ÇÊéàÊùÉ„ÄÇ
- **ËØ∑ÂãøÂú®ÂÖ¨ÂºÄÂú∫ÂêàÂàÜ‰∫´ Token**Ôºå‰ª•ÂÖçÊÇ®ÁöÑË¥¶Âè∑Ë¢´‰ªñ‰∫∫‰ΩøÁî®ÔºåÈÄ†Êàê‰∏•ÈáçÂêéÊûú„ÄÇ

## ‚≠êStar History

[![Star History Chart](https://api.star-history.com/svg?repos=happycola233/tchMaterial-parser&amp;type=Date)](https://star-history.com/#happycola233/tchMaterial-parser&amp;Date)

## ü§ù Ë¥°ÁåÆÊåáÂçó

Â¶ÇÊûúÊÇ®ÂèëÁé∞ Bug ÊàñÊúâÊîπËøõÂª∫ËÆÆÔºåÊ¨¢ËøéÊèê‰∫§ **Issue** Êàñ **Pull Request**ÔºåËÆ©Êàë‰ª¨‰∏ÄËµ∑ÂÆåÂñÑÊú¨Â∑•ÂÖ∑ÔºÅ

## üìú ËÆ∏ÂèØËØÅ

Êú¨È°πÁõÆÂü∫‰∫é [MIT ËÆ∏ÂèØËØÅ](LICENSE)ÔºåÊ¨¢ËøéËá™Áî±‰ΩøÁî®Âíå‰∫åÊ¨°ÂºÄÂèë„ÄÇ

## üíå ÂèãÊÉÖÈìæÊé•

- üìö ÊÇ®‰πüÂèØ‰ª•Âú® [ChinaTextbook](https://github.com/TapXWorld/ChinaTextbook) È°πÁõÆ‰∏≠‰∏ãËΩΩÂΩíÊ°£ÁöÑÁîµÂ≠êËØæÊú¨ PDF„ÄÇ
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Zie619/n8n-workflows]]></title>
            <link>https://github.com/Zie619/n8n-workflows</link>
            <guid>https://github.com/Zie619/n8n-workflows</guid>
            <pubDate>Tue, 03 Feb 2026 00:07:35 GMT</pubDate>
            <description><![CDATA[all of the workflows of n8n i could find (also from the site itself)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Zie619/n8n-workflows">Zie619/n8n-workflows</a></h1>
            <p>all of the workflows of n8n i could find (also from the site itself)</p>
            <p>Language: Python</p>
            <p>Stars: 50,799</p>
            <p>Forks: 6,261</p>
            <p>Stars today: 68 stars today</p>
            <h2>README</h2><pre># üöÄ n8n Workflow Collection

&lt;div align=&quot;center&quot;&gt;

![n8n Workflows](https://img.shields.io/badge/n8n-Workflows-orange?style=for-the-badge&amp;logo=n8n)
![Workflows](https://img.shields.io/badge/Workflows-4343+-blue?style=for-the-badge)
![Integrations](https://img.shields.io/badge/Integrations-365+-green?style=for-the-badge)
![License](https://img.shields.io/badge/License-MIT-purple?style=for-the-badge)
[![Buy Me a Coffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-FFDD00?style=for-the-badge&amp;logo=buy-me-a-coffee&amp;logoColor=black)](https://www.buymeacoffee.com/zie619)

### üåü The Ultimate Collection of n8n Automation Workflows

**[üîç Browse Online](https://zie619.github.io/n8n-workflows)** ‚Ä¢ **[üìö Documentation](#documentation)** ‚Ä¢ **[ü§ù Contributing](#contributing)** ‚Ä¢ **[üìÑ License](#license)**

&lt;/div&gt;

---

## ‚ú® What&#039;s New

### üéâ Latest Updates (November 2025)
- **üîí Enhanced Security**: Full security audit completed, all CVEs resolved
- **üê≥ Docker Support**: Multi-platform builds for linux/amd64 and linux/arm64
- **üìä GitHub Pages**: Live searchable interface at [zie619.github.io/n8n-workflows](https://zie619.github.io/n8n-workflows)
- **‚ö° Performance**: 100x faster search with SQLite FTS5 integration
- **üé® Modern UI**: Completely redesigned interface with dark/light mode

---

## üåê Quick Access

### üî• Use Online (No Installation)
Visit **[zie619.github.io/n8n-workflows](https://zie619.github.io/n8n-workflows)** for instant access to:
- üîç **Smart Search** - Find workflows instantly
- üìÇ **15+ Categories** - Browse by use case
- üì± **Mobile Ready** - Works on any device
- ‚¨áÔ∏è **Direct Downloads** - Get workflow JSONs instantly

---

## üöÄ Features

&lt;table&gt;
&lt;tr&gt;
&lt;td width=&quot;50%&quot;&gt;

### üìä By The Numbers
- **4,343** Production-Ready Workflows
- **365** Unique Integrations
- **29,445** Total Nodes
- **15** Organized Categories
- **100%** Import Success Rate

&lt;/td&gt;
&lt;td width=&quot;50%&quot;&gt;

### ‚ö° Performance
- **&lt; 100ms** Search Response
- **&lt; 50MB** Memory Usage
- **700x** Smaller Than v1
- **10x** Faster Load Times
- **40x** Less RAM Usage

&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

---

## üíª Local Installation

### Prerequisites
- Python 3.9+
- pip (Python package manager)
- 100MB free disk space

### Quick Start
```bash
# Clone the repository
git clone https://github.com/Zie619/n8n-workflows.git
cd n8n-workflows

# Install dependencies
pip install -r requirements.txt

# Start the server
python run.py

# Open in browser
# http://localhost:8000
```

### üê≥ Docker Installation
```bash
# Using Docker Hub
docker run -p 8000:8000 zie619/n8n-workflows:latest

# Or build locally
docker build -t n8n-workflows .
docker run -p 8000:8000 n8n-workflows
```

---

## üìö Documentation

### API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Web interface |
| `/api/search` | GET | Search workflows |
| `/api/stats` | GET | Repository statistics |
| `/api/workflow/{id}` | GET | Get workflow JSON |
| `/api/categories` | GET | List all categories |
| `/api/export` | GET | Export workflows |

### Search Features
- **Full-text search** across names, descriptions, and nodes
- **Category filtering** (Marketing, Sales, DevOps, etc.)
- **Complexity filtering** (Low, Medium, High)
- **Trigger type filtering** (Webhook, Schedule, Manual, etc.)
- **Service filtering** (365+ integrations)

---

## üèóÔ∏è Architecture

```mermaid
graph LR
    A[User] --&gt; B[Web Interface]
    B --&gt; C[FastAPI Server]
    C --&gt; D[SQLite FTS5]
    D --&gt; E[Workflow Database]
    C --&gt; F[Static Files]
    F --&gt; G[Workflow JSONs]
```

### Tech Stack
- **Backend**: Python, FastAPI, SQLite with FTS5
- **Frontend**: Vanilla JS, Tailwind CSS
- **Database**: SQLite with Full-Text Search
- **Deployment**: Docker, GitHub Actions, GitHub Pages
- **Security**: Trivy scanning, CORS protection, Input validation

---

## üìÇ Repository Structure

```
n8n-workflows/
‚îú‚îÄ‚îÄ workflows/           # 4,343 workflow JSON files
‚îÇ   ‚îî‚îÄ‚îÄ [category]/     # Organized by integration
‚îú‚îÄ‚îÄ docs/               # GitHub Pages site
‚îú‚îÄ‚îÄ src/                # Python source code
‚îú‚îÄ‚îÄ scripts/            # Utility scripts
‚îú‚îÄ‚îÄ api_server.py       # FastAPI application
‚îú‚îÄ‚îÄ run.py              # Server launcher
‚îú‚îÄ‚îÄ workflow_db.py      # Database manager
‚îî‚îÄ‚îÄ requirements.txt    # Python dependencies
```

---

## ü§ù Contributing

We love contributions! Here&#039;s how you can help:

### Ways to Contribute
- üêõ **Report bugs** via [Issues](https://github.com/Zie619/n8n-workflows/issues)
- üí° **Suggest features** in [Discussions](https://github.com/Zie619/n8n-workflows/discussions)
- üìù **Improve documentation**
- üîß **Submit workflow fixes**
- ‚≠ê **Star the repository**

### Development Setup
```bash
# Fork and clone
git clone https://github.com/YOUR_USERNAME/n8n-workflows.git

# Create branch
git checkout -b feature/amazing-feature

# Make changes and test
python run.py --debug

# Commit and push
git add .
git commit -m &quot;feat: add amazing feature&quot;
git push origin feature/amazing-feature

# Open PR
```

---

## üîí Security

### Security Features
- ‚úÖ **Path traversal protection**
- ‚úÖ **Input validation &amp; sanitization**
- ‚úÖ **CORS protection**
- ‚úÖ **Rate limiting**
- ‚úÖ **Docker security hardening**
- ‚úÖ **Non-root container user**
- ‚úÖ **Regular security scanning**

### Reporting Security Issues
Please report security vulnerabilities to the maintainers via [Security Advisory](https://github.com/Zie619/n8n-workflows/security/advisories/new).

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

```
MIT License

Copyright (c) 2025 Zie619

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the &quot;Software&quot;), to deal
in the Software without restriction...
```

---

## üíñ Support

If you find this project helpful, please consider:

&lt;div align=&quot;center&quot;&gt;

[![Buy Me a Coffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-FFDD00?style=for-the-badge&amp;logo=buy-me-a-coffee&amp;logoColor=black)](https://www.buymeacoffee.com/zie619)
[![Star on GitHub](https://img.shields.io/badge/Star%20on%20GitHub-181717?style=for-the-badge&amp;logo=github)](https://github.com/Zie619/n8n-workflows)
[![Follow](https://img.shields.io/badge/Follow-1DA1F2?style=for-the-badge&amp;logo=twitter&amp;logoColor=white)](https://twitter.com/zie619)

&lt;/div&gt;

---

## üìä Stats &amp; Badges

&lt;div align=&quot;center&quot;&gt;

![GitHub stars](https://img.shields.io/github/stars/Zie619/n8n-workflows?style=social)
![GitHub forks](https://img.shields.io/github/forks/Zie619/n8n-workflows?style=social)
![GitHub watchers](https://img.shields.io/github/watchers/Zie619/n8n-workflows?style=social)
![GitHub issues](https://img.shields.io/github/issues/Zie619/n8n-workflows)
![GitHub pull requests](https://img.shields.io/github/issues-pr/Zie619/n8n-workflows)
![GitHub last commit](https://img.shields.io/github/last-commit/Zie619/n8n-workflows)
![GitHub repo size](https://img.shields.io/github/repo-size/Zie619/n8n-workflows)

&lt;/div&gt;

---

## üôè Acknowledgments

- **n8n** - For creating an amazing automation platform
- **Contributors** - Everyone who has helped improve this collection
- **Community** - For feedback and support
- **You** - For using and supporting this project!

---

&lt;div align=&quot;center&quot;&gt;

### ‚≠ê Star us on GitHub ‚Äî it motivates us a lot!

Made with ‚ù§Ô∏è by [Zie619](https://github.com/Zie619) and [contributors](https://github.com/Zie619/n8n-workflows/graphs/contributors)

&lt;/div&gt;</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[serengil/deepface]]></title>
            <link>https://github.com/serengil/deepface</link>
            <guid>https://github.com/serengil/deepface</guid>
            <pubDate>Tue, 03 Feb 2026 00:07:34 GMT</pubDate>
            <description><![CDATA[A Lightweight Face Recognition and Facial Attribute Analysis (Age, Gender, Emotion and Race) Library for Python]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/serengil/deepface">serengil/deepface</a></h1>
            <p>A Lightweight Face Recognition and Facial Attribute Analysis (Age, Gender, Emotion and Race) Library for Python</p>
            <p>Language: Python</p>
            <p>Stars: 22,109</p>
            <p>Forks: 3,017</p>
            <p>Stars today: 36 stars today</p>
            <h2>README</h2><pre># deepface

&lt;div align=&quot;center&quot;&gt;

[![Downloads](https://static.pepy.tech/personalized-badge/deepface?period=total&amp;units=international_system&amp;left_color=grey&amp;right_color=blue&amp;left_text=downloads)](https://pepy.tech/project/deepface)
[![Stars](https://img.shields.io/github/stars/serengil/deepface?color=yellow&amp;style=flat&amp;label=%E2%AD%90%20stars)](https://github.com/serengil/deepface/stargazers)
[![Pulls](https://img.shields.io/docker/pulls/serengil/deepface?logo=docker)](https://hub.docker.com/r/serengil/deepface)
[![License](http://img.shields.io/:license-MIT-green.svg?style=flat)](https://github.com/serengil/deepface/blob/master/LICENSE)
[![Tests](https://github.com/serengil/deepface/actions/workflows/tests.yml/badge.svg)](https://github.com/serengil/deepface/actions/workflows/tests.yml)
[![DOI](http://img.shields.io/:DOI-10.17671/gazibtd.1399077-blue.svg?style=flat)](https://doi.org/10.17671/gazibtd.1399077)

[![Blog](https://img.shields.io/:blog-sefiks.com-blue.svg?style=flat&amp;logo=wordpress)](https://sefiks.com)
[![YouTube](https://img.shields.io/:youtube-@sefiks-red.svg?style=flat&amp;logo=youtube)](https://www.youtube.com/@sefiks?sub_confirmation=1)
[![Twitter](https://img.shields.io/:follow-@serengil-blue.svg?style=flat&amp;logo=x)](https://twitter.com/intent/user?screen_name=serengil)

[![Patreon](https://img.shields.io/:become-patron-f96854.svg?style=flat&amp;logo=patreon)](https://www.patreon.com/serengil?repo=deepface)
[![GitHub Sponsors](https://img.shields.io/github/sponsors/serengil?logo=GitHub&amp;color=lightgray)](https://github.com/sponsors/serengil)
[![Buy Me a Coffee](https://img.shields.io/badge/-buy_me_a%C2%A0coffee-gray?logo=buy-me-a-coffee)](https://buymeacoffee.com/serengil)

&lt;!--
[![Hacker News](https://img.shields.io/badge/dynamic/json?color=orange&amp;label=Hacker%20News&amp;query=score&amp;url=https%3A%2F%2Fhacker-news.firebaseio.com%2Fv0%2Fitem%2F46608519.json&amp;logo=y-combinator)](https://news.ycombinator.com/item?id=46608519)
--&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/4227&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/4227&quot; alt=&quot;serengil%2Fdeepface | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-icon-labeled.png&quot; width=&quot;200&quot; height=&quot;240&quot;&gt;&lt;/p&gt;

DeepFace is a lightweight [face recognition](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/) and facial attribute analysis ([age](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/), [gender](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/), [emotion](https://sefiks.com/2018/01/01/facial-expression-recognition-with-keras/) and [race](https://sefiks.com/2019/11/11/race-and-ethnicity-prediction-in-keras/)) framework for python. It is a hybrid face recognition framework wrapping **state-of-the-art** models: [`VGG-Face`](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/), [`FaceNet`](https://sefiks.com/2018/09/03/face-recognition-with-facenet-in-keras/), [`OpenFace`](https://sefiks.com/2019/07/21/face-recognition-with-openface-in-keras/), [`DeepFace`](https://sefiks.com/2020/02/17/face-recognition-with-facebook-deepface-in-keras/), [`DeepID`](https://sefiks.com/2020/06/16/face-recognition-with-deepid-in-keras/), [`ArcFace`](https://sefiks.com/2020/12/14/deep-face-recognition-with-arcface-in-keras-and-python/), [`Dlib`](https://sefiks.com/2020/07/11/face-recognition-with-dlib-in-python/), `SFace`, `GhostFaceNet`, `Buffalo_L`.

[A modern face recognition pipeline](https://sefiks.com/2020/05/01/a-gentle-introduction-to-face-recognition-in-deep-learning/) consists of 5 common stages: [detect](https://sefiks.com/2020/08/25/deep-face-detection-with-opencv-in-python/), [align](https://sefiks.com/2020/02/23/face-alignment-for-face-recognition-in-python-within-opencv/), [normalize](https://sefiks.com/2020/11/20/facial-landmarks-for-face-recognition-with-dlib/), [represent](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/) and [verify](https://sefiks.com/2020/05/22/fine-tuning-the-threshold-in-face-recognition/). While DeepFace handles all these common stages in the background, you don‚Äôt need to acquire in-depth knowledge about all the processes behind it. You can just call its verification, find or analysis function with a single line of code.

[`Experiments`](https://github.com/serengil/deepface/tree/master/benchmarks) show that **human beings have 97.53% accuracy** on facial recognition tasks whereas those models already reached and passed that accuracy level.

## Installation [![PyPI](https://img.shields.io/pypi/v/deepface.svg)](https://pypi.org/project/deepface/)

The easiest way to install deepface is to download it from [`PyPI`](https://pypi.org/project/deepface/). It&#039;s going to install the library itself and its prerequisites as well.

```shell
$ pip install deepface
```

Alternatively, you can also install deepface from its source code. Source code may have new features not published in pip release yet.

```shell
$ git clone https://github.com/serengil/deepface.git
$ cd deepface
$ pip install -e .
```

Once you installed the library, then you will be able to import it and use its functionalities.

```python
from deepface import DeepFace
```

**Face Verification** - [`Demo`](https://youtu.be/KRCvkNCOphE)

This function determines whether two facial images belong to the same person or to different individuals. The function returns a dictionary, where the key of interest is `verified`: True indicates the images are of the same person, while False means they are of different people.

```python
result: dict = DeepFace.verify(img1_path = &quot;img1.jpg&quot;, img2_path = &quot;img2.jpg&quot;)
```

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/serengil/deepface/master/icon/verify-credit.jpg&quot; width=&quot;99%&quot;&gt;&lt;/p&gt;

**Face recognition** - [`Tutorial`](https://sefiks.com/2026/01/01/introducing-brand-new-face-recognition-in-deepface/), [`Demo`](https://youtu.be/Hrjp-EStM_s)

[Face recognition](https://sefiks.com/2020/05/25/large-scale-face-recognition-for-deep-learning/) requires applying face verification many times. DeepFace provides an out-of-the-box `find` function that searches for the identity of an input image within a specified database path.

```python
dfs: List[pd.DataFrame] = DeepFace.find(img_path = &quot;img1.jpg&quot;, db_path = &quot;C:/my_db&quot;)
```

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-6-v2.jpg&quot; width=&quot;95%&quot;&gt;&lt;/p&gt;

Here, the `find` function relies on a directory-based face datastore and stores embeddings on disk. Alternatively, DeepFace provides a database-backed [`search`](https://sefiks.com/2026/01/01/introducing-brand-new-face-recognition-in-deepface/) functionality where embeddings are explicitly registered and queried. Currently, [postgres](https://sefiks.com/2023/06/22/vector-similarity-search-in-postgresql/), [mongo](https://sefiks.com/2021/01/22/deep-face-recognition-with-mongodb/), [neo4j](https://sefiks.com/2021/04/03/deep-face-recognition-with-neo4j/), [pgvector](https://sefiks.com/2024/07/05/postgres-as-a-vector-database-billion-scale-vector-similarity-search-with-pgvector/), [pinecone](https://sefiks.com/2021/05/19/large-scale-face-recognition-with-pinecone-vector-database/) and weaviate are supported as backend databases.

```python
# register an image into the database
DeepFace.register(img = &quot;img1.jpg&quot;)

# perform exact search
dfs: List[pd.DataFrame] = DeepFace.search(img = &quot;target.jpg&quot;)
```

If you want to perform [`approximate nearest neighbor`](https://sefiks.com/2023/12/31/a-step-by-step-approximate-nearest-neighbor-example-in-python-from-scratch/) search instead of exact search to achieve faster results on [large-scale databases](https://www.youtube.com/playlist?list=PLsS_1RYmYQQGSJu_Z3OVhXhGmZ86_zuIm), you can build an index beforehand and explicitly enable ANN search. Here, [Faiss](https://sefiks.com/2020/09/17/large-scale-face-recognition-with-facebook-faiss/) is used to index embeddings in postgres and mongo; whereas vector databases such as pgvector, weaviate, pinecone and neo4j handle indexing internally.

```python
# build index on registered embeddings (for postgres and mongo only)
DeepFace.build_index()

# perform approximate nearest neighbor search
dfs: List[pd.DataFrame] = DeepFace.search(img = &quot;target.jpg&quot;, search_method = &quot;ann&quot;)
```

**Facial Attribute Analysis** - [`Demo`](https://youtu.be/GT2UeN85BdA)

DeepFace also comes with a strong facial attribute analysis module including [`age`](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/), [`gender`](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/), [`facial expression`](https://sefiks.com/2018/01/01/facial-expression-recognition-with-keras/) (including angry, fear, neutral, sad, disgust, happy and surprise) and [`race`](https://sefiks.com/2019/11/11/race-and-ethnicity-prediction-in-keras/) (including asian, white, middle eastern, indian, latino and black) predictions.

```python
objs: List[dict] = DeepFace.analyze(
  img_path = &quot;img4.jpg&quot;, actions = [&#039;age&#039;, &#039;gender&#039;, &#039;race&#039;, &#039;emotion&#039;]
)
```

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-2.jpg&quot; width=&quot;95%&quot;&gt;&lt;/p&gt;

Age model got ¬± 4.65 MAE; gender model got 97.44% accuracy, 96.29% precision and 95.05% recall as mentioned in its [tutorial](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/).

**Real Time Analysis** - [`Demo`](https://youtu.be/-c9sSJcx6wI), [`React Demo part-i`](https://youtu.be/IXoah6rhxac), [`React Demo part-ii`](https://youtu.be/_waBA-cH2D4)

You can run deepface for real time videos as well. Stream function will access your webcam and apply both face recognition and facial attribute analysis. The function starts to analyze a frame if it can focus a face sequentially 5 frames. Then, it shows results 5 seconds.

```python
DeepFace.stream(db_path = &quot;C:/database&quot;)
```

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-3.jpg&quot; width=&quot;90%&quot;&gt;&lt;/p&gt;

Even though face recognition is based on one-shot learning, you can use multiple face pictures of a person as well. You should rearrange your directory structure as illustrated below.

```bash
user
‚îú‚îÄ‚îÄ database
‚îÇ   ‚îú‚îÄ‚îÄ Alice
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Alice1.jpg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Alice2.jpg
‚îÇ   ‚îú‚îÄ‚îÄ Bob
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Bob.jpg
```

If you intend to perform face verification or analysis tasks directly from your browser, [`deepface-react-ui`](https://github.com/serengil/deepface-react-ui) is a separate repository built using ReactJS depending on deepface api.

Here, you can also find some real time demos for various facial recognition models:

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/serengil/deepface/refs/heads/master/icon/deepface-realtime.jpg&quot; width=&quot;90%&quot;&gt;&lt;/p&gt;

| Task                 | Model    | Demo                                    |
| ---                  | ---      | ---                                     |
| Facial Recognition   | DeepFace | [`Video`](https://youtu.be/YjYIMs5ZOfc) |
| Facial Recognition   | FaceNet  | [`Video`](https://youtu.be/vB1I5vWgTQg) |
| Facial Recognition   | VGG-Face | [`Video`](https://youtu.be/tSU_lNi0gQQ) |
| Facial Recognition   | OpenFace | [`Video`](https://youtu.be/-4z2sL6wzP8) |
| Age &amp; Gender         | Default  | [`Video`](https://youtu.be/tFI7vZn3P7E) |
| Race &amp; Ethnicity     | Default  | [`Video`](https://youtu.be/-ztiy5eJha8) |
| Emotion              | Default  | [`Video`](https://youtu.be/Y7DfLvLKScs) |
| Celebrity Look-Alike | Default  | [`Video`](https://youtu.be/RMgIKU1H8DY) |

**Embeddings** - [`Tutorial`](https://sefiks.com/2025/06/28/what-are-vector-embeddings-and-why-they-matter-in-ai/), [`Demo`](https://youtu.be/OYialFo7Qo4)

Face recognition models basically represent facial images as multi-dimensional vectors. Sometimes, you need those embedding vectors directly. DeepFace comes with a dedicated representation function.

```python
embedding_objs: List[dict] = DeepFace.represent(img_path = &quot;img.jpg&quot;)
```

Embeddings can be [plotted](https://sefiks.com/2020/05/01/a-gentle-introduction-to-face-recognition-in-deep-learning/) as below. Each slot is corresponding to a dimension value and dimension value is emphasized with colors. Similar to 2D barcodes, vertical dimension stores no information in the illustration.

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/serengil/deepface/master/icon/embedding.jpg&quot; width=&quot;95%&quot;&gt;&lt;/p&gt;

In summary, the distance between vector embeddings of the same person should be smaller than that between embeddings of different people. When reduced to two-dimensional space, the clusters become clearly distinguishable.

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/serengil/deepface/refs/heads/master/icon/facenet-pca.png&quot; width=&quot;95%&quot;&gt;&lt;/p&gt;

**Face recognition models** - [`Demo`](https://youtu.be/eKOZawGR3y0)

DeepFace is a **hybrid** face recognition package. It currently wraps many **state-of-the-art** face recognition models: [`VGG-Face`](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/) , [`FaceNet`](https://sefiks.com/2018/09/03/face-recognition-with-facenet-in-keras/), [`OpenFace`](https://sefiks.com/2019/07/21/face-recognition-with-openface-in-keras/), [`DeepFace`](https://sefiks.com/2020/02/17/face-recognition-with-facebook-deepface-in-keras/), [`DeepID`](https://sefiks.com/2020/06/16/face-recognition-with-deepid-in-keras/), [`ArcFace`](https://sefiks.com/2020/12/14/deep-face-recognition-with-arcface-in-keras-and-python/), [`Dlib`](https://sefiks.com/2020/07/11/face-recognition-with-dlib-in-python/), `SFace`, `GhostFaceNet` and `Buffalo_L`. The default configuration uses VGG-Face model.

```python
models = [
    &quot;VGG-Face&quot;, &quot;Facenet&quot;, &quot;Facenet512&quot;, &quot;OpenFace&quot;, &quot;DeepFace&quot;,
    &quot;DeepID&quot;, &quot;ArcFace&quot;, &quot;Dlib&quot;, &quot;SFace&quot;, &quot;GhostFaceNet&quot;,
    &quot;Buffalo_L&quot;,
]

result = DeepFace.verify(
  img1_path = &quot;img1.jpg&quot;, img2_path = &quot;img2.jpg&quot;, model_name = models[0]
)

dfs = DeepFace.find(
  img_path = &quot;img1.jpg&quot;, db_path = &quot;C:/my_db&quot;, model_name = models[1]
)

embeddings = DeepFace.represent(
  img_path = &quot;img.jpg&quot;, model_name = models[2]
)
```

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/serengil/deepface/master/icon/model-portfolio-20240316.jpg&quot; width=&quot;95%&quot;&gt;&lt;/p&gt;

FaceNet, VGG-Face, ArcFace and Dlib are overperforming ones based on experiments - see [`BENCHMARKS`](https://github.com/serengil/deepface/tree/master/benchmarks) for more details. You can find the measured scores of various models in DeepFace and the reported scores from their original studies in the following table.

| Model          | Measured Score | Declared Score     |
| -------------- | -------------- | ------------------ |
| Facenet512     | 98.4%          | 99.6%              |
| Human-beings   | 97.5%          | 97.5%              |
| Facenet        | 97.4%          | 99.2%              |
| Dlib           | 96.8%          | 99.3 %             |
| VGG-Face       | 96.7%          | 98.9%              |
| ArcFace        | 96.7%          | 99.5%              |
| GhostFaceNet   | 93.3%          | 99.7%              |
| SFace          | 93.0%          | 99.5%              |
| OpenFace       | 78.7%          | 92.9%              |
| DeepFace       | 69.0%          | 97.3%              |
| DeepID         | 66.5%          | 97.4%              |

Conducting experiments with those models within DeepFace may reveal disparities compared to the original studies, owing to the adoption of distinct detection or normalization techniques. Furthermore, some models have been released solely with their backbones, lacking pre-trained weights. Thus, we are utilizing their re-implementations instead of the original pre-trained weights.

**Face Detection and Alignment** - [`Demo`](https://youtu.be/GZ2p2hj2H5k)

Face detection and alignment are important early stages of a modern face recognition pipeline. [Experiments](https://github.com/serengil/deepface/tree/master/benchmarks) show that detection increases the face recognition accuracy up to 42%, while alignment increases it up to 6%. [`OpenCV`](https://sefiks.com/2020/02/23/face-alignment-for-face-recognition-in-python-within-opencv/), [`Ssd`](https://sefiks.com/2020/08/25/deep-face-detection-with-opencv-in-python/), [`Dlib`](https://sefiks.com/2020/07/11/face-recognition-with-dlib-in-python/),  [`MtCnn`](https://sefiks.com/2020/09/09/deep-face-detection-with-mtcnn-in-python/), `Faster MtCnn`, [`RetinaFace`](https://sefiks.com/2021/04/27/deep-face-detection-with-retinaface-in-python/), [`MediaPipe`](https://sefiks.com/2022/01/14/deep-face-detection-with-mediapipe/), `Yolo`, `YuNet` and `CenterFace` detectors are wrapped in deepface.

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/serengil/deepface/master/icon/detector-portfolio-v6.jpg&quot; width=&quot;95%&quot;&gt;&lt;/p&gt;

All deepface functions accept optional detector backend and align input arguments. You can switch among those detectors and alignment modes with these arguments. OpenCV is the default detector and alignment is on by default.

```python
backends = [
    &#039;opencv&#039;, &#039;ssd&#039;, &#039;dlib&#039;, &#039;mtcnn&#039;, &#039;fastmtcnn&#039;,
    &#039;retinaface&#039;, &#039;mediapipe&#039;, &#039;yolov8n&#039;, &#039;yolov8m&#039;, 
    &#039;yolov8l&#039;, &#039;yolov11n&#039;, &#039;yolov11s&#039;, &#039;yolov11m&#039;,
    &#039;yolov11l&#039;, &#039;yolov12n&#039;, &#039;yolov12s&#039;, &#039;yolov12m&#039;,
    &#039;yolov12l&#039;, &#039;yunet&#039;, &#039;centerface&#039;,
]
detector = backends[3]
align = True

obj = DeepFace.verify(
  img1_path = &quot;img1.jpg&quot;, img2_path = &quot;img2.jpg&quot;, detector_backend = detector, align = align
)

dfs = DeepFace.find(
  img_path = &quot;img.jpg&quot;, db_path = &quot;my_db&quot;, detector_backend = detector, align = align
)

embedding_objs = DeepFace.represent(
  img_path = &quot;img.jpg&quot;, detector_backend = detector, align = align
)

demographies = DeepFace.analyze(
  img_path = &quot;img4.jpg&quot;, detector_backend = detector, align = align
)

face_objs = DeepFace.extract_faces(
  img_path = &quot;img.jpg&quot;, detector_backend = detector, align = align
)
```

Face recognition models are actually CNN models and they expect standard sized inputs. So, resizing is required before representation. To avoid deformation, deepface adds black padding pixels according to the target size argument after detection and alignment.

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/serengil/deepface/master/icon/detector-outputs-20240414.jpg&quot; width=&quot;90%&quot;&gt;&lt;/p&gt;

[RetinaFace](https://sefiks.com/2021/04/27/deep-face-detection-with-retinaface-in-python/) and [MtCnn](https://sefiks.com/2020/09/09/deep-face-detection-with-mtcnn-in-python/) seem to overperform in detection and alignment stages but they are much slower. If the speed of your pipeline is more important, then you should use opencv or ssd. On the other hand, if you consider the accuracy, then you should use retinaface or mtcnn.

The performance of RetinaFace is very satisfactory even in the crowd as seen in the following illustration. Besides, it comes with an incredible facial landmark detection performance. Highlighted red points show some facial landmarks such as eyes, nose and mouth. That&#039;s why, alignment score of RetinaFace is high as well.

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/serengil/deepface/master/icon/retinaface-results.jpeg&quot; width=&quot;90%&quot;&gt;
&lt;br&gt;&lt;em&gt;The Yellow Angels - Fenerbahce Women&#039;s Volleyball Team&lt;/em&gt;
&lt;/p&gt;

You can find out more about RetinaFace on this [repo](https://github.com/serengil/retinaface).

**Face Anti Spoofing** - [`Demo`](https://youtu.be/UiK1aIjOBlQ)

DeepFace also includes an anti-spoofing analysis module to understand given image is real or fake. To activate this feature, set the `anti_spoofing` argument to True in any DeepFace tasks.

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/serengil/deepface/master/icon/face-anti-spoofing.jpg&quot; width=&quot;40%&quot;&gt;&lt;/p&gt;

```python
# anti spoofing test in fa

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>