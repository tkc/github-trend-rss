<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sat, 29 Mar 2025 00:04:02 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[Shubhamsaboo/awesome-llm-apps]]></title>
            <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
            <guid>https://github.com/Shubhamsaboo/awesome-llm-apps</guid>
            <pubDate>Sat, 29 Mar 2025 00:04:02 GMT</pubDate>
            <description><![CDATA[Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Shubhamsaboo/awesome-llm-apps">Shubhamsaboo/awesome-llm-apps</a></h1>
            <p>Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p>
            <p>Language: Python</p>
            <p>Stars: 23,872</p>
            <p>Forks: 2,765</p>
            <p>Stars today: 1,340 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;http://www.theunwindai.com&quot;&gt;
    &lt;img src=&quot;docs/banner/unwind_black.png&quot; width=&quot;900px&quot; alt=&quot;Unwind AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.linkedin.com/in/shubhamsaboo/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;style=flat-square&quot; alt=&quot;LinkedIn&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/Saboo_Shubham_&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/Shubham_Saboo&quot; alt=&quot;Twitter&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;hr/&gt;

# 🌟 Awesome LLM Apps

A curated collection of awesome LLM apps built with RAG and AI agents. This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/9876&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/9876&quot; alt=&quot;Shubhamsaboo%2Fawesome-llm-apps | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## 🤔 Why Awesome LLM Apps?

- 💡 Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.
- 🔥 Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with RAG and AI Agents.
- 🎓 Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.

## 🚨 Open Source AI Agent Hackathon! 🚨

We&#039;re launching a Global AI Agent Hackathon in collaboration with AI Agent ecosystem partners — open to all developers, builders, and startups working on agents, RAG, tool use, or multi-agent systems.

### 💰 Win up to $20,000 in cash by building Agents

- 🏅 10 winners: $300 each
- 🥉 10 winners: $500 each
- 🥈 5 winners: $1,000 each
- 🥇 1 winner: $2,000
- 🏆 GRAND PRIZE: $5,000 🏆

### 🎁 Bonus
- Top 5 projects will be featured in the top trending [Awesome LLM Apps](https://github.com/Shubhamsaboo/awesome-llm-apps) repo.

### 🤝 Partners

[Unwind AI](https://www.theunwindai.com), [Agno](https://www.agno.com) and more Agent ecosystem companies joining soon.

### 📅 Here&#039;s the timeline:

- April 3rd - Final dates revealed
- April 10th - Prize and success criteria announced
- April 15th (tentative) - Hackathon starts
- May 30th (tentative) - Hackathon ends

Join us for a month of building Agents!

&gt; Prizes will be distributed on an ongoing basis and continue till all prizes are awarded.

⭐ Star this repo and follow along to stay updated.

### 🤝 Want to join us as a partner or judge?

If you&#039;re a company in the AI agent ecosystem or would like to judge the hackathon, reach out to [Shubham Saboo](https://x.com/Saboo_Shubham_) or [Ashpreet Bedi](https://x.com/ashpreetbedi) on X to partner. Let’s make this the biggest open source AI Agent hackathon.

## 📂 Featured AI Projects

### AI Agents
- [💼 AI Customer Support Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_customer_support_agent)
- [📈 AI Investment Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_investment_agent)
- [👨‍⚖️ AI Legal Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_legal_agent_team)
- [💼 AI Recruitment Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_recruitment_agent_team)
- [👨‍💼 AI Services Agency](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_services_agency)
- [🧲 AI Competitor Intelligence Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_competitor_intelligence_agent_team)
- [🏋️‍♂️ AI Health &amp; Fitness Planner Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_health_fitness_agent)
- [📈 AI Startup Trend Analysis Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_startup_trend_analysis_agent)
- [🗞️ AI Journalist Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_journalist_agent)
- [💲 AI Finance Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_finance_agent_team)
- [🧲 AI Competitor Intelligence Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_competitor_intelligence_agent_team)
- [🎯 AI Lead Generation Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_lead_generation_agent)
- [💰 AI Personal Finance Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_personal_finance_agent)
- [🩻 AI Medical Scan Diagnosis Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_medical_imaging_agent)
- [👨‍🏫 AI Teaching Agent Team](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_teaching_agent_team)
- [🛫 AI Travel Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_travel_agent)
- [🎬 AI Movie Production Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_movie_production_agent)
- [📰 Multi-Agent AI Researcher](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/multi_agent_researcher)
- [💻 Multimodal AI Coding Agent Team with o3-mini and Gemini](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_coding_agent_o3-mini)
- [📑 AI Meeting Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_meeting_agent)
- [♜ AI Chess Agent Game](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_chess_agent)
- [🏠 AI Real Estate Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_real_estate_agent)
- [🌐 Local News Agent OpenAI Swarm](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/local_news_agent_openai_swarm)
- [📊 AI Finance Agent with xAI Grok](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/xai_finance_agent)
- [🎮 AI 3D PyGame Visualizer with DeepSeek R1](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_3dpygame_r1)
- [🧠 AI Reasoning Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/ai_reasoning_agent)
- [🧬 Multimodal AI Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/ai_agent_tutorials/multimodal_ai_agent)

### RAG (Retrieval Augmented Generation)
- [🔍 Autonomous RAG](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/autonomous_rag)
- [🔗 Agentic RAG](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/agentic_rag)
- [🤔 Agentic RAG with Gemini Flash Thinking](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/gemini_agentic_rag)
- [🐋 Deepseek Local RAG Reasoning Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/deepseek_local_rag_agent)
- [🔄 Llama3.1 Local RAG](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/llama3.1_local_rag)
- [🧩 RAG-as-a-Service](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/rag-as-a-service)
- [🦙 Local RAG Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/local_rag_agent)
- [👀 RAG App with Hybrid Search](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/hybrid_search_rag)
- [🖥️ Local RAG App with Hybrid Search](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/local_hybrid_search_rag)
- [📠 RAG Agent with Database Routing](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/rag_database_routing)
- [🔄 Corrective RAG Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/rag_tutorials/corrective_rag)

### MCP AI Agents
- [🐙 MCP GitHub Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/mcp_ai_agents/github_mcp_agent)

### LLM Apps with Memory
- [💾 AI Arxiv Agent with Memory](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory)
- [📝 LLM App with Personalized Memory](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/llm_app_personalized_memory)
- [🛩️ AI Travel Agent with Memory](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/ai_travel_agent_memory)
- [🗄️ Local ChatGPT with Memory](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_apps_with_memory_tutorials/local_chatgpt_with_memory)

### Chat with X
- [💬 Chat with GitHub Repo](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_github)
- [📨 Chat with Gmail](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_gmail)
- [📄 Chat with PDF](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_pdf)
- [📚 Chat with Research Papers](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_research_papers)
- [📝 Chat with Substack Newsletter](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_substack)
- [📽️ Chat with YouTube Videos](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/chat_with_X_tutorials/chat_with_youtube_videos)

### LLM Finetuning
- [🌐 Llama3.2 Finetuning](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/llm_finetuning_tutorials/llama3.2_finetuning)

### Advanced Tools and Frameworks
- [🧪 Gemini Multimodal Chatbot](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/gemini_multimodal_chatbot)
- [🔄 Mixture of Agents](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/mixture_of_agents)
- [🌐 MultiLLM Chat Playground](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/multillm_chat_playground)
- [🔗 LLM Router App](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/llm_router_app)
- [💬 Local ChatGPT Clone](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/local_chatgpt_clone)
- [🌍 Web Scraping AI Agent](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/web_scrapping_ai_agent)
- [🔍 Web Search AI Assistant](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/web_search_ai_assistant)
- [🧪 Cursor AI Experiments](https://github.com/Shubhamsaboo/awesome-llm-apps/tree/main/advanced_tools_frameworks/cursor_ai_experiments)

## 🚀 Getting Started

1. **Clone the repository** 

    ```bash 
    git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
    ```

2. **Navigate to the desired project directory**

    ```bash 
    cd awesome-llm-apps/chat_with_X_tutorials/chat_with_gmail
    ```

3. **Install the required dependencies**

    ```bash
    pip install -r requirements.txt
    ```

4. **Follow the project-specific instructions** in each project&#039;s `README.md` file to set up and run the app.

## 🤝 Contributing to Open Source

Contributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new [GitHub Issue](https://github.com/Shubhamsaboo/awesome-llm-apps/issues) or submit a pull request. Make sure to follow the existing project structure and include a detailed `README.md` for each new app.

### Thank You, Community, for the Support! 🙏

[![Star History Chart](https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;type=Date)](https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;Date)

🌟 **Don’t miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[khoj-ai/khoj]]></title>
            <link>https://github.com/khoj-ai/khoj</link>
            <guid>https://github.com/khoj-ai/khoj</guid>
            <pubDate>Sat, 29 Mar 2025 00:04:01 GMT</pubDate>
            <description><![CDATA[Your AI second brain. Self-hostable. Get answers from the web or your docs. Build custom agents, schedule automations, do deep research. Turn any online or local LLM into your personal, autonomous AI (gpt, claude, gemini, llama, qwen, mistral). Get started - free.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/khoj-ai/khoj">khoj-ai/khoj</a></h1>
            <p>Your AI second brain. Self-hostable. Get answers from the web or your docs. Build custom agents, schedule automations, do deep research. Turn any online or local LLM into your personal, autonomous AI (gpt, claude, gemini, llama, qwen, mistral). Get started - free.</p>
            <p>Language: Python</p>
            <p>Stars: 28,154</p>
            <p>Forks: 1,559</p>
            <p>Stars today: 392 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://assets.khoj.dev/khoj-logo-sideways-1200x540.png&quot; width=&quot;230&quot; alt=&quot;Khoj Logo&quot;&gt;&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

[![test](https://github.com/khoj-ai/khoj/actions/workflows/test.yml/badge.svg)](https://github.com/khoj-ai/khoj/actions/workflows/test.yml)
[![docker](https://github.com/khoj-ai/khoj/actions/workflows/dockerize.yml/badge.svg)](https://github.com/khoj-ai/khoj/pkgs/container/khoj)
[![pypi](https://github.com/khoj-ai/khoj/actions/workflows/pypi.yml/badge.svg)](https://pypi.org/project/khoj/)
[![discord](https://img.shields.io/discord/1112065956647284756?style=plastic&amp;label=discord)](https://discord.gg/BDgyabRM6e)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;b&gt;Your AI second brain&lt;/b&gt;
&lt;/div&gt;

&lt;br /&gt;

&lt;div align=&quot;center&quot;&gt;

[📑 Docs](https://docs.khoj.dev)
&lt;span&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp;&lt;/span&gt;
[🌐 Web](https://khoj.dev)
&lt;span&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp;&lt;/span&gt;
[🔥 App](https://app.khoj.dev)
&lt;span&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp;&lt;/span&gt;
[💬 Discord](https://discord.gg/BDgyabRM6e)
&lt;span&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp;&lt;/span&gt;
[✍🏽 Blog](https://blog.khoj.dev)

&lt;/div&gt;

***

### 🎁 New
* Start any message with `/research` to try out the experimental research mode with Khoj.
* Anyone can now [create custom agents](https://blog.khoj.dev/posts/create-agents-on-khoj/) with tunable personality, tools and knowledge bases.
* [Read](https://blog.khoj.dev/posts/evaluate-khoj-quality/) about Khoj&#039;s excellent performance on modern retrieval and reasoning benchmarks.

***

## Overview

[Khoj](https://khoj.dev) is a personal AI app to extend your capabilities. It smoothly scales up from an on-device personal AI to a cloud-scale enterprise AI.

- Chat with any local or online LLM (e.g llama3, qwen, gemma, mistral, gpt, claude, gemini).
- Get answers from the internet and your docs (including image, pdf, markdown, org-mode, word, notion files).
- Access it from your Browser, Obsidian, Emacs, Desktop, Phone or Whatsapp.
- Create agents with custom knowledge, persona, chat model and tools to take on any role.
- Automate away repetitive research. Get personal newsletters and smart notifications delivered to your inbox.
- Find relevant docs quickly and easily using our advanced semantic search.
- Generate images, talk out loud, play your messages.
- Khoj is open-source, self-hostable. Always.
- Run it privately on [your computer](https://docs.khoj.dev/get-started/setup) or try it on our [cloud app](https://app.khoj.dev).

***

## See it in action

![demo_chat](https://github.com/khoj-ai/khoj/blob/master/documentation/assets/img/quadratic_equation_khoj_web.gif?raw=true)

Go to https://app.khoj.dev to see Khoj live.

## Full feature list
You can see the full feature list [here](https://docs.khoj.dev/category/features).

## Self-Host

To get started with self-hosting Khoj, [read the docs](https://docs.khoj.dev/get-started/setup).

## Enterprise

Khoj is available as a cloud service, on-premises, or as a hybrid solution. To learn more about Khoj Enterprise, [visit our website](https://khoj.dev/teams).

## Contributors
Cheers to our awesome contributors! 🎉

&lt;a href=&quot;https://github.com/khoj-ai/khoj/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=khoj-ai/khoj&quot; /&gt;
&lt;/a&gt;

Made with [contrib.rocks](https://contrib.rocks).

### Interested in Contributing?

We are always looking for contributors to help us build new features, improve the project documentation, or fix bugs. If you&#039;re interested, please see our [Contributing Guidelines](https://docs.khoj.dev/contributing/development) and check out our [Contributors Project Board](https://github.com/orgs/khoj-ai/projects/4).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ansible/ansible]]></title>
            <link>https://github.com/ansible/ansible</link>
            <guid>https://github.com/ansible/ansible</guid>
            <pubDate>Sat, 29 Mar 2025 00:04:00 GMT</pubDate>
            <description><![CDATA[Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain. Automate everything from code deployment to network configuration to cloud management, in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ansible/ansible">ansible/ansible</a></h1>
            <p>Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain. Automate everything from code deployment to network configuration to cloud management, in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com.</p>
            <p>Language: Python</p>
            <p>Stars: 64,497</p>
            <p>Forks: 24,003</p>
            <p>Stars today: 35 stars today</p>
            <h2>README</h2><pre>[![PyPI version](https://img.shields.io/pypi/v/ansible-core.svg)](https://pypi.org/project/ansible-core)
[![Docs badge](https://img.shields.io/badge/docs-latest-brightgreen.svg)](https://docs.ansible.com/ansible/latest/)
[![Chat badge](https://img.shields.io/badge/chat-IRC-brightgreen.svg)](https://docs.ansible.com/ansible/devel/community/communication.html)
[![Build Status](https://dev.azure.com/ansible/ansible/_apis/build/status/CI?branchName=devel)](https://dev.azure.com/ansible/ansible/_build/latest?definitionId=20&amp;branchName=devel)
[![Ansible Code of Conduct](https://img.shields.io/badge/code%20of%20conduct-Ansible-silver.svg)](https://docs.ansible.com/ansible/devel/community/code_of_conduct.html)
[![Ansible mailing lists](https://img.shields.io/badge/mailing%20lists-Ansible-orange.svg)](https://docs.ansible.com/ansible/devel/community/communication.html#mailing-list-information)
[![Repository License](https://img.shields.io/badge/license-GPL%20v3.0-brightgreen.svg)](COPYING)
[![Ansible CII Best Practices certification](https://bestpractices.coreinfrastructure.org/projects/2372/badge)](https://bestpractices.coreinfrastructure.org/projects/2372)

# Ansible

Ansible is a radically simple IT automation system. It handles
configuration management, application deployment, cloud provisioning,
ad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complex
changes like zero-downtime rolling updates with load balancers easy. More information on the Ansible [website](https://ansible.com/).

## Design Principles

* Have an extremely simple setup process with a minimal learning curve.
* Manage machines quickly and in parallel.
* Avoid custom-agents and additional open ports, be agentless by
  leveraging the existing SSH daemon.
* Describe infrastructure in a language that is both machine and human
  friendly.
* Focus on security and easy auditability/review/rewriting of content.
* Manage new remote machines instantly, without bootstrapping any
  software.
* Allow module development in any dynamic language, not just Python.
* Be usable as non-root.
* Be the easiest IT automation system to use, ever.

## Use Ansible

You can install a released version of Ansible with `pip` or a package manager. See our
[installation guide](https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html) for details on installing Ansible
on a variety of platforms.

Power users and developers can run the `devel` branch, which has the latest
features and fixes, directly. Although it is reasonably stable, you are more likely to encounter
breaking changes when running the `devel` branch. We recommend getting involved
in the Ansible community if you want to run the `devel` branch.

## Communication

Join the Ansible forum to ask questions, get help, and interact with the
community.

* [Get Help](https://forum.ansible.com/c/help/6): Find help or share your Ansible knowledge to help others.
  Use tags to filter and subscribe to posts, such as the following:
  * Posts tagged with [ansible](https://forum.ansible.com/tag/ansible)
  * Posts tagged with [ansible-core](https://forum.ansible.com/tag/ansible-core)
  * Posts tagged with [playbook](https://forum.ansible.com/tag/playbook)
* [Social Spaces](https://forum.ansible.com/c/chat/4): Meet and interact with fellow enthusiasts.
* [News &amp; Announcements](https://forum.ansible.com/c/news/5): Track project-wide announcements including social events.
* [Bullhorn newsletter](https://docs.ansible.com/ansible/devel/community/communication.html#the-bullhorn): Get release announcements and important changes.

For more ways to get in touch, see [Communicating with the Ansible community](https://docs.ansible.com/ansible/devel/community/communication.html).

## Contribute to Ansible

* Check out the [Contributor&#039;s Guide](./.github/CONTRIBUTING.md).
* Read [Community Information](https://docs.ansible.com/ansible/devel/community) for all
  kinds of ways to contribute to and interact with the project,
  including how to submit bug reports and code to Ansible.
* Submit a proposed code update through a pull request to the `devel` branch.
* Talk to us before making larger changes
  to avoid duplicate efforts. This not only helps everyone
  know what is going on, but it also helps save time and effort if we decide
  some changes are needed.

## Coding Guidelines

We document our Coding Guidelines in the [Developer Guide](https://docs.ansible.com/ansible/devel/dev_guide/). We particularly suggest you review:

* [Contributing your module to Ansible](https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_checklist.html)
* [Conventions, tips, and pitfalls](https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_best_practices.html)

## Branch Info

* The `devel` branch corresponds to the release actively under development.
* The `stable-2.X` branches correspond to stable releases.
* Create a branch based on `devel` and set up a [dev environment](https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_general.html#common-environment-setup) if you want to open a PR.
* See the [Ansible release and maintenance](https://docs.ansible.com/ansible/devel/reference_appendices/release_and_maintenance.html) page for information about active branches.

## Roadmap

Based on team and community feedback, an initial roadmap will be published for a major or minor version (ex: 2.7, 2.8).
The [Ansible Roadmap page](https://docs.ansible.com/ansible/devel/roadmap/) details what is planned and how to influence the roadmap.

## Authors

Ansible was created by [Michael DeHaan](https://github.com/mpdehaan)
and has contributions from over 5000 users (and growing). Thanks everyone!

[Ansible](https://www.ansible.com) is sponsored by [Red Hat, Inc.](https://www.redhat.com)

## License

GNU General Public License v3.0 or later

See [COPYING](COPYING) to see the full text.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[huggingface/lerobot]]></title>
            <link>https://github.com/huggingface/lerobot</link>
            <guid>https://github.com/huggingface/lerobot</guid>
            <pubDate>Sat, 29 Mar 2025 00:03:59 GMT</pubDate>
            <description><![CDATA[🤗 LeRobot: Making AI for Robotics more accessible with end-to-end learning]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/huggingface/lerobot">huggingface/lerobot</a></h1>
            <p>🤗 LeRobot: Making AI for Robotics more accessible with end-to-end learning</p>
            <p>Language: Python</p>
            <p>Stars: 11,204</p>
            <p>Forks: 1,228</p>
            <p>Stars today: 92 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;media/lerobot-logo-thumbnail.png&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;media/lerobot-logo-thumbnail.png&quot;&gt;
    &lt;img alt=&quot;LeRobot, Hugging Face Robotics Library&quot; src=&quot;media/lerobot-logo-thumbnail.png&quot; style=&quot;max-width: 100%;&quot;&gt;
  &lt;/picture&gt;
  &lt;br/&gt;
  &lt;br/&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

[![Tests](https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml/badge.svg?branch=main)](https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml?query=branch%3Amain)
[![Coverage](https://codecov.io/gh/huggingface/lerobot/branch/main/graph/badge.svg?token=TODO)](https://codecov.io/gh/huggingface/lerobot)
[![Python versions](https://img.shields.io/pypi/pyversions/lerobot)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/huggingface/lerobot/blob/main/LICENSE)
[![Status](https://img.shields.io/pypi/status/lerobot)](https://pypi.org/project/lerobot/)
[![Version](https://img.shields.io/pypi/v/lerobot)](https://pypi.org/project/lerobot/)
[![Examples](https://img.shields.io/badge/Examples-green.svg)](https://github.com/huggingface/lerobot/tree/main/examples)
[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.1%20adopted-ff69b4.svg)](https://github.com/huggingface/lerobot/blob/main/CODE_OF_CONDUCT.md)
[![Discord](https://dcbadge.vercel.app/api/server/C5P34WJ68S?style=flat)](https://discord.gg/s3KuuzsPFb)

&lt;/div&gt;

&lt;h2 align=&quot;center&quot;&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/huggingface/lerobot/blob/main/examples/10_use_so100.md&quot;&gt;
        Build Your Own SO-100 Robot!&lt;/a&gt;&lt;/p&gt;
&lt;/h2&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/so100/leader_follower.webp?raw=true&quot; alt=&quot;SO-100 leader and follower arms&quot; title=&quot;SO-100 leader and follower arms&quot; width=&quot;50%&quot;&gt;

  &lt;p&gt;&lt;strong&gt;Meet the SO-100 – Just $110 per arm!&lt;/strong&gt;&lt;/p&gt;
  &lt;p&gt;Train it in minutes with a few simple moves on your laptop.&lt;/p&gt;
  &lt;p&gt;Then sit back and watch your creation act autonomously! 🤯&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;https://github.com/huggingface/lerobot/blob/main/examples/10_use_so100.md&quot;&gt;
      Get the full SO-100 tutorial here.&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;Want to take it to the next level? Make your SO-100 mobile by building LeKiwi!&lt;/p&gt;
  &lt;p&gt;Check out the &lt;a href=&quot;https://github.com/huggingface/lerobot/blob/main/examples/11_use_lekiwi.md&quot;&gt;LeKiwi tutorial&lt;/a&gt; and bring your robot to life on wheels.&lt;/p&gt;

  &lt;img src=&quot;media/lekiwi/kiwi.webp?raw=true&quot; alt=&quot;LeKiwi mobile robot&quot; title=&quot;LeKiwi mobile robot&quot; width=&quot;50%&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;

&lt;h3 align=&quot;center&quot;&gt;
    &lt;p&gt;LeRobot: State-of-the-art AI for real-world robotics&lt;/p&gt;
&lt;/h3&gt;

---


🤗 LeRobot aims to provide models, datasets, and tools for real-world robotics in PyTorch. The goal is to lower the barrier to entry to robotics so that everyone can contribute and benefit from sharing datasets and pretrained models.

🤗 LeRobot contains state-of-the-art approaches that have been shown to transfer to the real-world with a focus on imitation learning and reinforcement learning.

🤗 LeRobot already provides a set of pretrained models, datasets with human collected demonstrations, and simulation environments to get started without assembling a robot. In the coming weeks, the plan is to add more and more support for real-world robotics on the most affordable and capable robots out there.

🤗 LeRobot hosts pretrained models and datasets on this Hugging Face community page: [huggingface.co/lerobot](https://huggingface.co/lerobot)

#### Examples of pretrained models on simulation environments

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;img src=&quot;media/gym/aloha_act.gif&quot; width=&quot;100%&quot; alt=&quot;ACT policy on ALOHA env&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;media/gym/simxarm_tdmpc.gif&quot; width=&quot;100%&quot; alt=&quot;TDMPC policy on SimXArm env&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;media/gym/pusht_diffusion.gif&quot; width=&quot;100%&quot; alt=&quot;Diffusion policy on PushT env&quot;/&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;ACT policy on ALOHA env&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;TDMPC policy on SimXArm env&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;Diffusion policy on PushT env&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

### Acknowledgment

- Thanks to Tony Zhao, Zipeng Fu and colleagues for open sourcing ACT policy, ALOHA environments and datasets. Ours are adapted from [ALOHA](https://tonyzhaozh.github.io/aloha) and [Mobile ALOHA](https://mobile-aloha.github.io).
- Thanks to Cheng Chi, Zhenjia Xu and colleagues for open sourcing Diffusion policy, Pusht environment and datasets, as well as UMI datasets. Ours are adapted from [Diffusion Policy](https://diffusion-policy.cs.columbia.edu) and [UMI Gripper](https://umi-gripper.github.io).
- Thanks to Nicklas Hansen, Yunhai Feng and colleagues for open sourcing TDMPC policy, Simxarm environments and datasets. Ours are adapted from [TDMPC](https://github.com/nicklashansen/tdmpc) and [FOWM](https://www.yunhaifeng.com/FOWM).
- Thanks to Antonio Loquercio and Ashish Kumar for their early support.
- Thanks to [Seungjae (Jay) Lee](https://sjlee.cc/), [Mahi Shafiullah](https://mahis.life/) and colleagues for open sourcing [VQ-BeT](https://sjlee.cc/vq-bet/) policy and helping us adapt the codebase to our repository. The policy is adapted from [VQ-BeT repo](https://github.com/jayLEE0301/vq_bet_official).


## Installation

Download our source code:
```bash
git clone https://github.com/huggingface/lerobot.git
cd lerobot
```

Create a virtual environment with Python 3.10 and activate it, e.g. with [`miniconda`](https://docs.anaconda.com/free/miniconda/index.html):
```bash
conda create -y -n lerobot python=3.10
conda activate lerobot
```

When using `miniconda`, if you don&#039;t have `ffmpeg` in your environment:
```bash
conda install ffmpeg
```

Install 🤗 LeRobot:
```bash
pip install --no-binary=av -e .
```

&gt; **NOTE:** If you encounter build errors, you may need to install additional dependencies (`cmake`, `build-essential`, and `ffmpeg libs`). On Linux, run:
`sudo apt-get install cmake build-essential python-dev pkg-config libavformat-dev libavcodec-dev libavdevice-dev libavutil-dev libswscale-dev libswresample-dev libavfilter-dev pkg-config`. For other systems, see: [Compiling PyAV](https://pyav.org/docs/develop/overview/installation.html#bring-your-own-ffmpeg)

For simulations, 🤗 LeRobot comes with gymnasium environments that can be installed as extras:
- [aloha](https://github.com/huggingface/gym-aloha)
- [xarm](https://github.com/huggingface/gym-xarm)
- [pusht](https://github.com/huggingface/gym-pusht)

For instance, to install 🤗 LeRobot with aloha and pusht, use:
```bash
pip install --no-binary=av -e &quot;.[aloha, pusht]&quot;
```

To use [Weights and Biases](https://docs.wandb.ai/quickstart) for experiment tracking, log in with
```bash
wandb login
```

(note: you will also need to enable WandB in the configuration. See below.)

## Walkthrough

```
.
├── examples             # contains demonstration examples, start here to learn about LeRobot
|   └── advanced         # contains even more examples for those who have mastered the basics
├── lerobot
|   ├── configs          # contains config classes with all options that you can override in the command line
|   ├── common           # contains classes and utilities
|   |   ├── datasets       # various datasets of human demonstrations: aloha, pusht, xarm
|   |   ├── envs           # various sim environments: aloha, pusht, xarm
|   |   ├── policies       # various policies: act, diffusion, tdmpc
|   |   ├── robot_devices  # various real devices: dynamixel motors, opencv cameras, koch robots
|   |   └── utils          # various utilities
|   └── scripts          # contains functions to execute via command line
|       ├── eval.py                 # load policy and evaluate it on an environment
|       ├── train.py                # train a policy via imitation learning and/or reinforcement learning
|       ├── control_robot.py        # teleoperate a real robot, record data, run a policy
|       ├── push_dataset_to_hub.py  # convert your dataset into LeRobot dataset format and upload it to the Hugging Face hub
|       └── visualize_dataset.py    # load a dataset and render its demonstrations
├── outputs               # contains results of scripts execution: logs, videos, model checkpoints
└── tests                 # contains pytest utilities for continuous integration
```

### Visualize datasets

Check out [example 1](./examples/1_load_lerobot_dataset.py) that illustrates how to use our dataset class which automatically downloads data from the Hugging Face hub.

You can also locally visualize episodes from a dataset on the hub by executing our script from the command line:
```bash
python lerobot/scripts/visualize_dataset.py \
    --repo-id lerobot/pusht \
    --episode-index 0
```

or from a dataset in a local folder with the `root` option and the `--local-files-only` (in the following case the dataset will be searched for in `./my_local_data_dir/lerobot/pusht`)
```bash
python lerobot/scripts/visualize_dataset.py \
    --repo-id lerobot/pusht \
    --root ./my_local_data_dir \
    --local-files-only 1 \
    --episode-index 0
```


It will open `rerun.io` and display the camera streams, robot states and actions, like this:

https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20240505T172924Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;X-Amz-SignedHeaders=host&amp;actor_id=24889239&amp;key_id=0&amp;repo_id=748713144


Our script can also visualize datasets stored on a distant server. See `python lerobot/scripts/visualize_dataset.py --help` for more instructions.

### The `LeRobotDataset` format

A dataset in `LeRobotDataset` format is very simple to use. It can be loaded from a repository on the Hugging Face hub or a local folder simply with e.g. `dataset = LeRobotDataset(&quot;lerobot/aloha_static_coffee&quot;)` and can be indexed into like any Hugging Face and PyTorch dataset. For instance `dataset[0]` will retrieve a single temporal frame from the dataset containing observation(s) and an action as PyTorch tensors ready to be fed to a model.

A specificity of `LeRobotDataset` is that, rather than retrieving a single frame by its index, we can retrieve several frames based on their temporal relationship with the indexed frame, by setting `delta_timestamps` to a list of relative times with respect to the indexed frame. For example, with `delta_timestamps = {&quot;observation.image&quot;: [-1, -0.5, -0.2, 0]}`  one can retrieve, for a given index, 4 frames: 3 &quot;previous&quot; frames 1 second, 0.5 seconds, and 0.2 seconds before the indexed frame, and the indexed frame itself (corresponding to the 0 entry). See example [1_load_lerobot_dataset.py](examples/1_load_lerobot_dataset.py) for more details on `delta_timestamps`.

Under the hood, the `LeRobotDataset` format makes use of several ways to serialize data which can be useful to understand if you plan to work more closely with this format. We tried to make a flexible yet simple dataset format that would cover most type of features and specificities present in reinforcement learning and robotics, in simulation and in real-world, with a focus on cameras and robot states but easily extended to other types of sensory inputs as long as they can be represented by a tensor.

Here are the important details and internal structure organization of a typical `LeRobotDataset` instantiated with `dataset = LeRobotDataset(&quot;lerobot/aloha_static_coffee&quot;)`. The exact features will change from dataset to dataset but not the main aspects:

```
dataset attributes:
  ├ hf_dataset: a Hugging Face dataset (backed by Arrow/parquet). Typical features example:
  │  ├ observation.images.cam_high (VideoFrame):
  │  │   VideoFrame = {&#039;path&#039;: path to a mp4 video, &#039;timestamp&#039; (float32): timestamp in the video}
  │  ├ observation.state (list of float32): position of an arm joints (for instance)
  │  ... (more observations)
  │  ├ action (list of float32): goal position of an arm joints (for instance)
  │  ├ episode_index (int64): index of the episode for this sample
  │  ├ frame_index (int64): index of the frame for this sample in the episode ; starts at 0 for each episode
  │  ├ timestamp (float32): timestamp in the episode
  │  ├ next.done (bool): indicates the end of en episode ; True for the last frame in each episode
  │  └ index (int64): general index in the whole dataset
  ├ episode_data_index: contains 2 tensors with the start and end indices of each episode
  │  ├ from (1D int64 tensor): first frame index for each episode — shape (num episodes,) starts with 0
  │  └ to: (1D int64 tensor): last frame index for each episode — shape (num episodes,)
  ├ stats: a dictionary of statistics (max, mean, min, std) for each feature in the dataset, for instance
  │  ├ observation.images.cam_high: {&#039;max&#039;: tensor with same number of dimensions (e.g. `(c, 1, 1)` for images, `(c,)` for states), etc.}
  │  ...
  ├ info: a dictionary of metadata on the dataset
  │  ├ codebase_version (str): this is to keep track of the codebase version the dataset was created with
  │  ├ fps (float): frame per second the dataset is recorded/synchronized to
  │  ├ video (bool): indicates if frames are encoded in mp4 video files to save space or stored as png files
  │  └ encoding (dict): if video, this documents the main options that were used with ffmpeg to encode the videos
  ├ videos_dir (Path): where the mp4 videos or png images are stored/accessed
  └ camera_keys (list of string): the keys to access camera features in the item returned by the dataset (e.g. `[&quot;observation.images.cam_high&quot;, ...]`)
```

A `LeRobotDataset` is serialised using several widespread file formats for each of its parts, namely:
- hf_dataset stored using Hugging Face datasets library serialization to parquet
- videos are stored in mp4 format to save space
- metadata are stored in plain json/jsonl files

Dataset can be uploaded/downloaded from the HuggingFace hub seamlessly. To work on a local dataset, you can specify its location with the `root` argument if it&#039;s not in the default `~/.cache/huggingface/lerobot` location.

### Evaluate a pretrained policy

Check out [example 2](./examples/2_evaluate_pretrained_policy.py) that illustrates how to download a pretrained policy from Hugging Face hub, and run an evaluation on its corresponding environment.

We also provide a more capable script to parallelize the evaluation over multiple environments during the same rollout. Here is an example with a pretrained model hosted on [lerobot/diffusion_pusht](https://huggingface.co/lerobot/diffusion_pusht):
```bash
python lerobot/scripts/eval.py \
    --policy.path=lerobot/diffusion_pusht \
    --env.type=pusht \
    --eval.batch_size=10 \
    --eval.n_episodes=10 \
    --policy.use_amp=false \
    --policy.device=cuda
```

Note: After training your own policy, you can re-evaluate the checkpoints with:

```bash
python lerobot/scripts/eval.py --policy.path={OUTPUT_DIR}/checkpoints/last/pretrained_model
```

See `python lerobot/scripts/eval.py --help` for more instructions.

### Train your own policy

Check out [example 3](./examples/3_train_policy.py) that illustrate how to train a model using our core library in python, and [example 4](./examples/4_train_policy_with_script.md) that shows how to use our training script from command line.

To use wandb for logging training and evaluation curves, make sure you&#039;ve run `wandb login` as a one-time setup step. Then, when running the training command above, enable WandB in the configuration by adding `--wandb.enable=true`.

A link to the wandb logs for the run will also show up in yellow in your terminal. Here is an example of what they look like in your browser. Please also check [here](./examples/4_train_policy_with_script.md#typical-logs-and-metrics) for the explanation of some commonly used metrics in logs.

![](media/wandb.png)

Note: For efficiency, during training every checkpoint is evaluated on a low number of episodes. You may use `--eval.n_episodes=500` to evaluate on more episodes than the default. Or, after training, you may want to re-evaluate your best checkpoints on more episodes or change the evaluation settings. See `python lerobot/scripts/eval.py --help` for more instructions.

#### Reproduce state-of-the-art (SOTA)

We provide some pretrained policies on our [hub page](https://huggingface.co/lerobot) that can achieve state-of-the-art performances.
You can reproduce their training by loading the config from their run. Simply running:
```bash
python lerobot/scripts/train.py --config_path=lerobot/diffusion_pusht
```
reproduces SOTA results for Diffusion Policy on the PushT task.

## Contribute

If you would like to contribute to 🤗 LeRobot, please check out our [contribution guide](https://github.com/huggingface/lerobot/blob/main/CONTRIBUTING.md).

&lt;!-- ### Add a new dataset

To add a dataset to the hub, you need to login using a write-access token, which can be generated from the [Hugging Face settings](https://huggingface.co/settings/tokens):
```bash
huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential
```

Then point to your raw dataset folder (e.g. `data/aloha_static_pingpong_test_raw`), and push your dataset to the hub with:
```bash
python lerobot/scripts/push_dataset_to_hub.py \
--raw-dir data/aloha_static_pingpong_test_raw \
--out-dir data \
--repo-id lerobot/aloha_static_pingpong_test \
--raw-format aloha_hdf5
```

See `python lerobot/scripts/push_dataset_to_hub.py --help` for more instructions.

If your dataset format is not supported, implement your own in `lerobot/common/datasets/push_dataset_to_hub/${raw_format}_format.py` by copying examples like [pusht_zarr](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/pusht_zarr_format.py), [umi_zarr](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/umi_zarr_format.py), [aloha_hdf5](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/aloha_hdf5_format.py), or [xarm_pkl](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/xarm_pkl_format.py). --&gt;


### Add a pretrained policy

Once you have trained a policy you may upload it to the Hugging Face hub using a hub id that looks like `${hf_user}/${repo_name}` (e.g. [lerobot/diffusion_pusht](https://huggingface.co/lerobot/diffusion_pusht)).

You first need to find the checkpoint folder located inside your experiment directory (e.g. `outputs/train/2024-05-05/20-21-12_aloha_act_default/checkpoints/002500`). Within that there is a `pretrained_model` directory which should contain:
- `config.json`: A serialized version of the policy configuration (following the policy&#039;s dataclass config).
- `model.safetensors`: A set of `torch.nn.Module` parameters, saved in [Hugging Face Safetensors](https://huggingface.co/docs/safetensors/index) format.
- `train_config.json`: A consolidated configuration containing all parameter userd for training. The policy configuration should match `config.json` exactly. Thisis useful for anyone who wants to evaluate your policy or for reproducibility.

To upload these to the hub, run the following:
```bash
huggingface-cli upload ${hf_user}/${repo_name} path/to/pretrained_model
```

See [eval.py](https://github.com/huggingface/lerobot/blob/main/lerobot/scripts/eval.py) for an example of how other people may use your policy.


### Improve your code with profiling

An example of a code snippet to profile the evaluation of a policy:
```python
from torch.profiler import profile, record_function, ProfilerActivity

def trace_handler(prof):
    prof.export_chrome_trace(f&quot;tmp/trace_schedule_{prof.step_num}.json&quot;)

with profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[RasaHQ/rasa]]></title>
            <link>https://github.com/RasaHQ/rasa</link>
            <guid>https://github.com/RasaHQ/rasa</guid>
            <pubDate>Sat, 29 Mar 2025 00:03:58 GMT</pubDate>
            <description><![CDATA[💬 Open source machine learning framework to automate text- and voice-based conversations: NLU, dialogue management, connect to Slack, Facebook, and more - Create chatbots and voice assistants]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/RasaHQ/rasa">RasaHQ/rasa</a></h1>
            <p>💬 Open source machine learning framework to automate text- and voice-based conversations: NLU, dialogue management, connect to Slack, Facebook, and more - Create chatbots and voice assistants</p>
            <p>Language: Python</p>
            <p>Stars: 19,836</p>
            <p>Forks: 4,745</p>
            <p>Stars today: 41 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;Rasa Open Source&lt;/h1&gt;

&lt;div align=&quot;center&quot;&gt;

[![Join the chat on Rasa Community Forum](https://img.shields.io/badge/forum-join%20discussions-brightgreen.svg)](https://forum.rasa.com/?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge)
[![PyPI version](https://badge.fury.io/py/rasa.svg)](https://badge.fury.io/py/rasa)
[![Supported Python Versions](https://img.shields.io/pypi/pyversions/rasa.svg)](https://pypi.python.org/pypi/rasa)
[![Build Status](https://github.com/RasaHQ/rasa/workflows/Continuous%20Integration/badge.svg)](https://github.com/RasaHQ/rasa/actions)
[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=RasaHQ_rasa&amp;metric=alert_status)](https://sonarcloud.io/summary/new_code?id=RasaHQ_rasa)
[![Documentation Status](https://img.shields.io/badge/docs-stable-brightgreen.svg)](https://rasa.com/docs)
![Documentation Build](https://img.shields.io/netlify/d2e447e4-5a5e-4dc7-be5d-7c04ae7ff706?label=Documentation%20Build)
[![FOSSA Status](https://app.fossa.com/api/projects/custom%2B8141%2Fgit%40github.com%3ARasaHQ%2Frasa.git.svg?type=shield)](https://app.fossa.com/projects/custom%2B8141%2Fgit%40github.com%3ARasaHQ%2Frasa.git?ref=badge_shield)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](https://github.com/orgs/RasaHQ/projects/23)

&lt;/div&gt;

&lt;hr /&gt;

💡 **We&#039;re migrating issues to Jira** 💡

Starting January 2023, issues for Rasa Open Source are located in
[this Jira board](https://rasa-open-source.atlassian.net/browse/OSS). You can browse issues without being logged in;
if you want to create issues, you&#039;ll need to create a Jira account.

&lt;hr /&gt;

&lt;img align=&quot;right&quot; height=&quot;255&quot; src=&quot;https://www.rasa.com/assets/img/sara/sara-open-source-2.0.png&quot; alt=&quot;An image of Sara, the Rasa mascot bird, holding a flag that reads Open Source with one wing, and a wrench in the other&quot; title=&quot;Rasa Open Source&quot;&gt;

Rasa is an open source machine learning framework to automate text and voice-based conversations. With Rasa, you can build contextual assistants on:
- Facebook Messenger
- Slack
- Google Hangouts
- Webex Teams
- Microsoft Bot Framework
- Rocket.Chat
- Mattermost
- Telegram
- Twilio
- Your own custom conversational channels

or voice assistants as:
- Alexa Skills
- Google Home Actions

Rasa helps you build contextual assistants capable of having layered conversations with
lots of back-and-forth. In order for a human to have a meaningful exchange with a contextual
assistant, the assistant needs to be able to use context to build on things that were previously
discussed – Rasa enables you to build assistants that can do this in a scalable way.

There&#039;s a lot more background information in this
[blog post](https://medium.com/rasa-blog/a-new-approach-to-conversational-software-2e64a5d05f2a).

---
- 🤔 [Learn more about Rasa](https://rasa.community/)

- 🤓 [Read The Docs](https://rasa.com/docs/rasa/)

- 😁 [Install Rasa](https://rasa.com/docs/rasa/installation/environment-set-up)

- 🚀 [Dive deeper in the learning center](https://learning.rasa.com/)

- 🤗 [Contribute](#how-to-contribute)

- ❓ [Get enterprise-grade support](https://rasa.com/support/)

- 🏢 [Explore the features of our commercial platform](https://rasa.com/product/rasa-platform/)

- 📚 [Learn more about research papers that leverage Rasa](https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;authuser=1&amp;cites=16243802403383697687,353275993797024115,14567308604105196228,9067977709825839723,9855847065463746011&amp;as_sdt=5)



---
## Where to get help

There is extensive documentation in the [Rasa Docs](https://rasa.com/docs/rasa).
Make sure to select the correct version so you are looking at
the docs for the version you installed.

Please use [Rasa Community Forum](https://forum.rasa.com) for quick answers to
questions.

### README Contents:
- [How to contribute](#how-to-contribute)
- [Development Internals](#development-internals)
- [Releases](#releases)
- [License](#license)

### How to contribute
We are very happy to receive and merge your contributions into this repository!

To contribute via pull request, follow these steps:

1. Create an issue describing the bug/improvement you want to work on or pick up an
   existing issue in [Jira](https://rasa-open-source.atlassian.net/jira/software/c/projects/OSS/boards/1)
2. Follow our Pull Request guidelines: write code, test, documentation, changelog and follow our [Code Style](#code-style)
3. Create a pull request describing your changes

For more detailed instructions on how to contribute code, check out these [code contributor guidelines](CONTRIBUTING.md).

You can find more information about how to contribute to Rasa (in lots of
different ways!) [on our website.](http://rasa.community).

Your pull request will be reviewed by a maintainer, who will get
back to you about any necessary changes or questions. You will
also be asked to sign a
[Contributor License Agreement](https://cla-assistant.io/RasaHQ/rasa).


## Development Internals

### Installing Poetry

Rasa uses Poetry for packaging and dependency management. If you want to build it from source,
you have to install Poetry first. Please follow
[the official guide](https://python-poetry.org/docs/#installation) to see all possible options.

To update an existing poetry version to the [version](.github/poetry_version.txt), currently used in rasa, run:
```shell
    poetry self update &lt;version&gt;
```

### Managing environments

The official [Poetry guide](https://python-poetry.org/docs/managing-environments/) suggests to use
[pyenv](https://github.com/pyenv/pyenv) or any other similar tool to easily switch between Python versions.
This is how it can be done:

```bash
pyenv install 3.10.10
pyenv local 3.10.10  # Activate Python 3.10.10 for the current project
```
*Note*: If you have trouble installing a specific version of python on your system
it might be worth trying other supported versions.

By default, Poetry will try to use the currently activated Python version to create the virtual environment
for the current project automatically. You can also create and activate a virtual environment manually — in this
case, Poetry should pick it up and use it to install the dependencies. For example:

```bash
python -m venv .venv
source .venv/bin/activate
```

You can make sure that the environment is picked up by executing

```bash
poetry env info
```

### Building from source

To install dependencies and `rasa` itself in editable mode execute

```bash
make install
```

*Note for macOS users*: under macOS Big Sur we&#039;ve seen some compiler issues for
dependencies. Using `export SYSTEM_VERSION_COMPAT=1` before the installation helped.


#### Installing optional dependencies

In order to install rasa&#039;s optional dependencies, you need to run:

```bash
make install-full
```

*Note for macOS users*: The command `make install-full` could result in a failure while installing `tokenizers`
(issue described in depth [here](https://github.com/huggingface/tokenizers/issues/1050)).

In order to resolve it, you must follow these steps to install a Rust compiler:
```bash
brew install rustup
rustup-init
```

After initialising the Rust compiler, you should restart the console and check its installation:
```bash
rustc --version
```

In case the PATH variable had not been automatically setup, run:
```bash
export PATH=&quot;$HOME/.cargo/bin:$PATH&quot;
```


### Running and changing the documentation

First of all, install all the required dependencies:

```bash
make install install-docs
```

After the installation has finished, you can run and view the documentation
locally using:

```bash
make livedocs
```

It should open a new tab with the local version of the docs in your browser;
if not, visit http://localhost:3000 in your browser.
You can now change the docs locally and the web page will automatically reload
and apply your changes.

### Running the Tests

In order to run the tests, make sure that you have the development requirements installed:

```bash
make prepare-tests-ubuntu # Only on Ubuntu and Debian based systems
make prepare-tests-macos  # Only on macOS
```

Then, run the tests:

```bash
make test
```

They can also be run at multiple jobs to save some time:

```bash
JOBS=[n] make test
```

Where `[n]` is the number of jobs desired. If omitted, `[n]` will be automatically chosen by pytest.


### Running the Integration Tests

In order to run the integration tests, make sure that you have the development requirements installed:

```bash
make prepare-tests-ubuntu # Only on Ubuntu and Debian based systems
make prepare-tests-macos  # Only on macOS
```

Then, you&#039;ll need to start services with the following command which uses
[Docker Compose](https://docs.docker.com/compose/install/):

```bash
make run-integration-containers
```

Finally, you can run the integration tests like this:

```bash
make test-integration
```


### Resolving merge conflicts

Poetry doesn&#039;t include any solution that can help to resolve merge conflicts in
the lock file `poetry.lock` by default.
However, there is a great tool called [poetry-merge-lock](https://poetry-merge-lock.readthedocs.io/en/latest/).
Here is how you can install it:

```bash
pip install poetry-merge-lock
```

Just execute this command to resolve merge conflicts in `poetry.lock` automatically:

```bash
poetry-merge-lock
```

### Build a Docker image locally

In order to build a Docker image on your local machine execute the following command:

```bash
make build-docker
```

The Docker image is available on your local machine as `rasa:localdev`.

### Code Style

To ensure a standardized code style we use the formatter [black](https://github.com/ambv/black).
To ensure our type annotations are correct we use the type checker [pytype](https://github.com/google/pytype).
If your code is not formatted properly or doesn&#039;t type check, GitHub will fail to build.

#### Formatting

If you want to automatically format your code on every commit, you can use [pre-commit](https://pre-commit.com/).
Just install it via `pip install pre-commit` and execute `pre-commit install` in the root folder.
This will add a hook to the repository, which reformats files on every commit.

If you want to set it up manually, install black via `poetry install`.
To reformat files execute
```
make formatter
```

#### Type Checking

If you want to check types on the codebase, install `mypy` using `poetry install`.
To check the types execute
```
make types
```

### Deploying documentation updates

We use `Docusaurus v2` to build docs for tagged versions and for the `main` branch.
To run Docusaurus, install `Node.js 12.x`.
The static site that gets built is pushed to the `documentation` branch of this repo.

We host the site on netlify. On `main` branch builds (see `.github/workflows/documentation.yml`), we push the built docs to
the `documentation` branch. Netlify automatically re-deploys the docs pages whenever there is a change to that branch.

## Releases
Rasa has implemented robust policies governing version naming, as well as release pace for major, minor, and patch releases.

The values for a given version number (MAJOR.MINOR.PATCH) are incremented as follows:
- MAJOR version for incompatible API changes or other breaking changes.
- MINOR version for functionality added in a backward compatible manner.
- PATCH version for backward compatible bug fixes.

The following table describes the version types and their expected *release cadence*:

| Version Type |                                                                  Description                                                                  |  Target Cadence |
|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------|-----------------|
| Major        | For significant changes, or when any backward-incompatible changes are introduced to the API or data model.                                   | Every 1 - 2 yrs |
| Minor        | For when new backward-compatible functionality is introduced, a minor feature is introduced, or when a set of smaller features is rolled out. | +/- Quarterly   |
| Patch        | For backward-compatible bug fixes that fix incorrect behavior.                                                                                | As needed       |

While this table represents our target release frequency, we reserve the right to modify it based on changing market conditions and technical requirements.

### Maintenance Policy
Our End of Life policy defines how long a given release is considered supported, as well as how long a release is
considered to be still in active development or maintenance.

The maintenance duration and end of life for every release are shown on our website as part of the [Product Release and Maintenance Policy](https://rasa.com/rasa-product-release-and-maintenance-policy/).

### Cutting a Major / Minor release
#### A week before release day

1. **Make sure the [milestone](https://github.com/RasaHQ/rasa/milestones) already exists and is scheduled for the
correct date.**
2. **Take a look at the issues &amp; PRs that are in the milestone**: does it look about right for the release highlights
we are planning to ship? Does it look like anything is missing? Don&#039;t worry about being aware of every PR that should
be in, but it&#039;s useful to take a moment to evaluate what&#039;s assigned to the milestone.
3. **Post a message on the engineering Slack channel**, letting the team know you&#039;ll be the one cutting the upcoming
release, as well as:
    1. Providing the link to the appropriate milestone
    2. Reminding everyone to go over their issues and PRs and please assign them to the milestone
    3. Reminding everyone of the scheduled date for the release

#### A day before release day

1. **Go over the milestone and evaluate the status of any PR merging that&#039;s happening. Follow up with people on their
bugs and fixes.** If the release introduces new bugs or regressions that can&#039;t be fixed in time, we should discuss on
Slack about this and take a decision on how to move forward. If the issue is not ready to be merged in time, we remove the issue / PR from the milestone and notify the PR owner and the product manager on Slack about it. The PR / issue owners are responsible for
communicating any issues which might be release relevant. Postponing the release should be considered as an edge case scenario.

#### Release day! 🚀

1. **At the start of the day, post a small message on slack announcing release day!** Communicate you&#039;ll be handling
the release, and the time you&#039;re aiming to start releasing (again, no later than 4pm, as issues may arise and
cause delays). This message should be posted early in the morning and before moving forward with any of the steps of the release,
   in order to give enough time to people to check their PRs and issues. That way they can plan any remaining work. A template of the slack message can be found [here](https://rasa-hq.slack.com/archives/C36SS4N8M/p1613032208137500?thread_ts=1612876410.068400&amp;cid=C36SS4N8M).
   The release time should be communicated transparently so that others can plan potentially necessary steps accordingly. If there are bigger changes this should be communicated.
2. Make sure the milestone is empty (everything has been either merged or moved to the next milestone)
3. Once everything in the milestone is taken care of, post a small message on Slack communicating you are about to
start the release process (in case anything is missing).
4. **You may now do the release by following the instructions outlined in the
[Rasa Open Source README](#steps-to-release-a-new-version) !**

#### After a Major release

After a Major release has been completed, please follow [these instructions to complete the documentation update](./docs/README.md#manual-steps-after-a-new-version).

### Steps to release a new version
Releasing a new version is quite simple, as the packages are build and distributed by GitHub Actions.

*Release steps*:
1. Make sure all dependencies are up to date (**especially Rasa SDK**)
    - For Rasa SDK, except in the case of a patch release, that means first creating a [new Rasa SDK release](https://github.com/RasaHQ/rasa-sdk#steps-to-release-a-new-version) (make sure the version numbers between the new Rasa and Rasa SDK releases match)
    - Once the tag with the new Rasa SDK release is pushed and the package appears on [pypi](https://pypi.org/project/rasa-sdk/), the dependency in the rasa repository can be resolved (see below).
2. If this is a minor / major release: Make sure all fixes from currently supported minor versions have been merged from their respective release branches (e.g. 3.3.x) back into main.
3. In case of a minor release, create a new branch that corresponds to the new release, e.g.
   ```bash
    git checkout -b 1.2.x
    git push origin 1.2.x
    ```
4. Switch to the branch you want to cut the release from (`main` in case of a major, the `&lt;major&gt;.&lt;minor&gt;.x` branch for minors and patches)
    - Update the `rasa-sdk` entry in `pyproject.toml` with the new release version and run `poetry update`. This creates a new `poetry.lock` file with all dependencies resolved.
    - Commit the changes with `git commit -am &quot;bump rasa-sdk dependency&quot;` but do not push them. They will be automatically picked up by the following step.
5. If this is a major release, update the list of actively maintained versions [in the README](#actively-maintained-versions) and in [the docs](./docs/docs/actively-maintained-versions.mdx).
6. Run `make release`
7. Create a PR against the release branch (e.g. `1.2.x`)
8. Once your PR is merged, tag a new release (this SHOULD always happen on the release branch), e.g. using
    ```bash
    git checkout 1.2.x
    git pull origin 1.2.x
    git tag 1.2.0 -m &quot;next release&quot;
    git push origin 1.2.0 --tags
    ```
    GitHub will build this tag and publish the build artifacts.
9. After all the steps are completed and if everything goes well then we should see a message automatically posted in the company&#039;s Slack (`product` channel) like this [one](https://rasa-hq.slack.com/archives/C7B08Q5FX/p1614354499046600)
10. If no message appears in the channel then you can do the following checks:
    - Check the workflows in [Github Actions](https://github.com/RasaHQ/rasa/actions) and make sure that the merged PR of the current release is completed successfully. To easily find your PR you can use the filters `event: push` and `branch: &lt;version number&gt;` (example on release 2.4 you can see [here](https://github.com/RasaHQ/rasa/actions/runs/643344876))
    - If the workflow is not completed, then try to re run the workflow in case that solves the problem
    - If the problem persists, check also the log files and try to find the root cause of the issue
    - If you still cannot resolve the error, contact the infrastructure team by providing any helpful information from your investigation
11.  After the message is posted correctly in the `product` channel, check also in the `product-engineering-alerts` channel if there are any alerts related to the Rasa Open Source release like this [one](https://rasa-hq.slack.com/archives/C01585AN2NP/p1615486087001000)

### Cutting a Patch release

Patch releases are simpler to cut, since they are meant to contain only bugfixes.

**The only things you need to do to cut a patch release are:**

1. Notify the engineering team on Slack that you are planning to cut a patch, in case someone has an important fix
to add.
2. Make sure the bugfix(es) are in the release branch you will use (p.e if you are cutting a `2.0.4` patch, you will
need your fixes to be on the `2.0.x` release branch). All patch releases must come from a `.x` branch!
3. Once you&#039;re ready to release the Rasa Open Source patch, checkout the branch, run `make release` and follow the
steps + get the PR merged.
4. Once the PR is in, pull the `.x` branch 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[chengazhen/cursor-auto-free]]></title>
            <link>https://github.com/chengazhen/cursor-auto-free</link>
            <guid>https://github.com/chengazhen/cursor-auto-free</guid>
            <pubDate>Sat, 29 Mar 2025 00:03:57 GMT</pubDate>
            <description><![CDATA[auto sign cursor]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/chengazhen/cursor-auto-free">chengazhen/cursor-auto-free</a></h1>
            <p>auto sign cursor</p>
            <p>Language: Python</p>
            <p>Stars: 7,470</p>
            <p>Forks: 1,094</p>
            <p>Stars today: 62 stars today</p>
            <h2>README</h2><pre># Cursor Pro 自动化工具使用说明


[English doc](./README.EN.md)


## 在线文档
[cursor-auto-free-doc.vercel.app](https://cursor-auto-free-doc.vercel.app)


## 公众号 回复 1 获取 qq 群

![公众号](./screen/qrcode_for_gh_c985615b5f2b_258.jpg)

## 英文名字集
https://github.com/toniprada/usa-names-dataset

## 许可证声明
本项目采用 [CC BY-NC-ND 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/) 许可证。
这意味着您可以：
- 分享 — 在任何媒介以任何形式复制、发行本作品
但必须遵守以下条件：
- 非商业性使用 — 您不得将本作品用于商业目的

## 声明
- 本项目仅供学习交流使用，请勿用于商业用途。
- 本项目不承担任何法律责任，使用本项目造成的任何后果，由使用者自行承担。



## 骗子
海豚


## 感谢 linuxDo 这个开源社区(一个真正的技术社区)
https://linux.do/

## 特别鸣谢
本项目的开发过程中得到了众多开源项目和社区成员的支持与帮助，在此特别感谢：

### 开源项目
- [go-cursor-help](https://github.com/yuaotian/go-cursor-help) - 一个优秀的 Cursor 机器码重置工具，本项目的机器码重置功能使用该项目实现。该项目目前已获得 9.1k Stars，是最受欢迎的 Cursor 辅助工具之一。

## 请我喝杯茶 | buy me a cup of tea
&lt;img src=&quot;./screen/image.png&quot; width=&quot;300&quot;/&gt;
&lt;img src=&quot;./screen/28613e3f3f23a935b66a7ba31ff4e3f.jpg&quot; width=&quot;300&quot;/&gt;
 &lt;img src=&quot;./screen/mm_facetoface_collect_qrcode_1738583247120.png&quot; width=&quot;300&quot;/&gt;


</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mlflow/mlflow]]></title>
            <link>https://github.com/mlflow/mlflow</link>
            <guid>https://github.com/mlflow/mlflow</guid>
            <pubDate>Sat, 29 Mar 2025 00:03:56 GMT</pubDate>
            <description><![CDATA[Open source platform for the machine learning lifecycle]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mlflow/mlflow">mlflow/mlflow</a></h1>
            <p>Open source platform for the machine learning lifecycle</p>
            <p>Language: Python</p>
            <p>Stars: 19,955</p>
            <p>Forks: 4,428</p>
            <p>Stars today: 20 stars today</p>
            <h2>README</h2><pre># MLflow: A Machine Learning Lifecycle Platform

[![Latest Docs](https://img.shields.io/badge/docs-latest-success.svg?style=for-the-badge)](https://mlflow.org/docs/latest/index.html)
[![Apache 2 License](https://img.shields.io/badge/license-Apache%202-brightgreen.svg?style=for-the-badge&amp;logo=apache)](https://github.com/mlflow/mlflow/blob/master/LICENSE.txt)
[![Total Downloads](https://img.shields.io/pypi/dw/mlflow?style=for-the-badge&amp;logo=pypi&amp;logoColor=white)](https://pepy.tech/project/mlflow)
[![Slack](https://img.shields.io/badge/slack-@mlflow--users-CF0E5B.svg?logo=slack&amp;logoColor=white&amp;labelColor=3F0E40&amp;style=for-the-badge)](https://mlflow.org/community/#slack)
[![Twitter](https://img.shields.io/twitter/follow/MLflow?style=for-the-badge&amp;labelColor=00ACEE&amp;logo=twitter&amp;logoColor=white)](https://twitter.com/MLflow)

MLflow is an open-source platform, purpose-built to assist machine learning practitioners and teams in handling the complexities of the machine learning process. MLflow focuses on the full lifecycle for machine learning projects, ensuring that each phase is manageable, traceable, and reproducible

---

The core components of MLflow are:

- [Experiment Tracking](https://mlflow.org/docs/latest/tracking.html) 📝: A set of APIs to log models, params, and results in ML experiments and compare them using an interactive UI.
- [Model Packaging](https://mlflow.org/docs/latest/models.html) 📦: A standard format for packaging a model and its metadata, such as dependency versions, ensuring reliable deployment and strong reproducibility.
- [Model Registry](https://mlflow.org/docs/latest/model-registry.html) 💾: A centralized model store, set of APIs, and UI, to collaboratively manage the full lifecycle of MLflow Models.
- [Serving](https://mlflow.org/docs/latest/deployment/index.html) 🚀: Tools for seamless model deployment to batch and real-time scoring on platforms like Docker, Kubernetes, Azure ML, and AWS SageMaker.
- [Evaluation](https://mlflow.org/docs/latest/model-evaluation/index.html) 📊: A suite of automated model evaluation tools, seamlessly integrated with experiment tracking to record model performance and visually compare results across multiple models.
- [Observability](https://mlflow.org/docs/latest/llms/tracing/index.html) 🔍: Tracing integrations with various GenAI libraries and a Python SDK for manual instrumentation, offering smoother debugging experience and supporting online monitoring.

&lt;img src=&quot;https://mlflow.org/img/hero.png&quot; alt=&quot;MLflow Hero&quot; width=100%&gt;

## Installation

To install the MLflow Python package, run the following command:

```
pip install mlflow
```

Alternatively, you can install MLflow from on different package hosting platforms:

|               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| ------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| PyPI          | [![PyPI - mlflow](https://img.shields.io/pypi/v/mlflow.svg?style=for-the-badge&amp;logo=pypi&amp;logoColor=white&amp;label=mlflow)](https://pypi.org/project/mlflow/) [![PyPI - mlflow-skinny](https://img.shields.io/pypi/v/mlflow-skinny.svg?style=for-the-badge&amp;logo=pypi&amp;logoColor=white&amp;label=mlflow-skinny)](https://pypi.org/project/mlflow-skinny/)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| conda-forge   | [![Conda - mlflow](https://img.shields.io/conda/vn/conda-forge/mlflow.svg?style=for-the-badge&amp;logo=anaconda&amp;label=mlflow)](https://anaconda.org/conda-forge/mlflow) [![Conda - mlflow-skinny](https://img.shields.io/conda/vn/conda-forge/mlflow.svg?style=for-the-badge&amp;logo=anaconda&amp;label=mlflow-skinny)](https://anaconda.org/conda-forge/mlflow-skinny)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| CRAN          | [![CRAN - mlflow](https://img.shields.io/cran/v/mlflow.svg?style=for-the-badge&amp;logo=r&amp;label=mlflow)](https://cran.r-project.org/package=mlflow)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| Maven Central | [![Maven Central - mlflow-client](https://img.shields.io/maven-central/v/org.mlflow/mlflow-client.svg?style=for-the-badge&amp;logo=apache-maven&amp;label=mlflow-client)](https://mvnrepository.com/artifact/org.mlflow/mlflow-client) [![Maven Central - mlflow-parent](https://img.shields.io/maven-central/v/org.mlflow/mlflow-parent.svg?style=for-the-badge&amp;logo=apache-maven&amp;label=mlflow-parent)](https://mvnrepository.com/artifact/org.mlflow/mlflow-parent) [![Maven Central - mlflow-scoring](https://img.shields.io/maven-central/v/org.mlflow/mlflow-scoring.svg?style=for-the-badge&amp;logo=apache-maven&amp;label=mlflow-scoring)](https://mvnrepository.com/artifact/org.mlflow/mlflow-scoring) [![Maven Central - mlflow-spark](https://img.shields.io/maven-central/v/org.mlflow/mlflow-spark.svg?style=for-the-badge&amp;logo=apache-maven&amp;label=mlflow-spark)](https://mvnrepository.com/artifact/org.mlflow/mlflow-spark) |

## Documentation 📘

Official documentation for MLflow can be found at [here](https://mlflow.org/docs/latest/index.html).

## Running Anywhere 🌐

You can run MLflow on many different environments, including local development, Amazon SageMaker, AzureML, and Databricks. Please refer to [this guidance](https://mlflow.org/docs/latest/index.html#running-mlflow-anywhere) for how to setup MLflow on your environment.

## Usage

### Experiment Tracking ([Doc](https://mlflow.org/docs/latest/tracking.html))

The following examples trains a simple regression model with scikit-learn, while enabling MLflow&#039;s [autologging](https://mlflow.org/docs/latest/tracking/autolog.html) feature for experiment tracking.

```python
import mlflow

from sklearn.model_selection import train_test_split
from sklearn.datasets import load_diabetes
from sklearn.ensemble import RandomForestRegressor

# Enable MLflow&#039;s automatic experiment tracking for scikit-learn
mlflow.sklearn.autolog()

# Load the training dataset
db = load_diabetes()
X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)

rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)
# MLflow triggers logging automatically upon model fitting
rf.fit(X_train, y_train)
```

Once the above code finishes, run the following command in a separate terminal and access the MLflow UI via the printed URL. An MLflow **Run** should be automatically created, which tracks the training dataset, hyper parameters, performance metrics, the trained model, dependencies, and even more.

```
mlflow ui
```

### Serving Models ([Doc](https://mlflow.org/docs/latest/deployment/index.html))

You can deploy the logged model to a local inference server by a one-line command using the MLflow CLI. Visit the documentation for how to deploy models to other hosting platforms.

```bash
mlflow models serve --model-uri runs:/&lt;run-id&gt;/model
```

### Evaluating Models ([Doc](https://mlflow.org/docs/latest/model-evaluation/index.html))

The following example runs automatic evaluation for question-answering tasks with several built-in metrics.

```python
import mlflow
import pandas as pd

# Evaluation set contains (1) input question (2) model outputs (3) ground truth
df = pd.DataFrame(
    {
        &quot;inputs&quot;: [&quot;What is MLflow?&quot;, &quot;What is Spark?&quot;],
        &quot;outputs&quot;: [
            &quot;MLflow is an innovative fully self-driving airship powered by AI.&quot;,
            &quot;Sparks is an American pop and rock duo formed in Los Angeles.&quot;,
        ],
        &quot;ground_truth&quot;: [
            &quot;MLflow is an open-source platform for managing the end-to-end machine learning (ML) &quot;
            &quot;lifecycle.&quot;,
            &quot;Apache Spark is an open-source, distributed computing system designed for big data &quot;
            &quot;processing and analytics.&quot;,
        ],
    }
)
eval_dataset = mlflow.data.from_pandas(
    df, predictions=&quot;outputs&quot;, targets=&quot;ground_truth&quot;
)

# Start an MLflow Run to record the evaluation results to
with mlflow.start_run(run_name=&quot;evaluate_qa&quot;):
    # Run automatic evaluation with a set of built-in metrics for question-answering models
    results = mlflow.evaluate(
        data=eval_dataset,
        model_type=&quot;question-answering&quot;,
    )

print(results.tables[&quot;eval_results_table&quot;])
```

### Observability ([Doc](https://mlflow.org/docs/latest/llms/tracing/index.html))

MLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call `mlflow.xyz.autolog()` before running your models. Refer to the documentation for customization and manual instrumentation.

```python
import mlflow
from openai import OpenAI

# Enable tracing for OpenAI
mlflow.openai.autolog()

# Query OpenAI LLM normally
response = OpenAI().chat.completions.create(
    model=&quot;gpt-4o-mini&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hi!&quot;}],
    temperature=0.1,
)
```

Then navigate to the &quot;Traces&quot; tab in the MLflow UI to find the trace records OpenAI query.

## Community

- For help or questions about MLflow usage (e.g. &quot;how do I do X?&quot;) visit the [docs](https://mlflow.org/docs/latest/index.html)
  or [Stack Overflow](https://stackoverflow.com/questions/tagged/mlflow).
- Alternatively, you can ask the question to our AI-powered chat bot. Visit the doc website and click on the **&quot;Ask AI&quot;** button at the right bottom to start chatting with the bot.
- To report a bug, file a documentation issue, or submit a feature request, please [open a GitHub issue](https://github.com/mlflow/mlflow/issues/new/choose).
- For release announcements and other discussions, please subscribe to our mailing list (mlflow-users@googlegroups.com)
  or join us on [Slack](https://mlflow.org/slack).

## Contributing

We happily welcome contributions to MLflow! We are also seeking contributions to items on the
[MLflow Roadmap](https://github.com/mlflow/mlflow/milestone/3). Please see our
[contribution guide](CONTRIBUTING.md) to learn more about contributing to MLflow.

## Core Members

MLflow is currently maintained by the following core members with significant contributions from hundreds of exceptionally talented community members.

- [Ben Wilson](https://github.com/BenWilson2)
- [Corey Zumar](https://github.com/dbczumar)
- [Daniel Lok](https://github.com/daniellok-db)
- [Gabriel Fu](https://github.com/gabrielfu)
- [Harutaka Kawamura](https://github.com/harupy)
- [Serena Ruan](https://github.com/serena-ruan)
- [Weichen Xu](https://github.com/WeichenXu123)
- [Yuki Watanabe](https://github.com/B-Step62)
- [Tomu Hirata](https://github.com/TomeHirata)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[deepseek-ai/DeepSeek-V3]]></title>
            <link>https://github.com/deepseek-ai/DeepSeek-V3</link>
            <guid>https://github.com/deepseek-ai/DeepSeek-V3</guid>
            <pubDate>Sat, 29 Mar 2025 00:03:55 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/deepseek-ai/DeepSeek-V3">deepseek-ai/DeepSeek-V3</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 94,526</p>
            <p>Forks: 15,282</p>
            <p>Stars today: 350 stars today</p>
            <h2>README</h2><pre>&lt;!-- markdownlint-disable first-line-h1 --&gt;
&lt;!-- markdownlint-disable html --&gt;
&lt;!-- markdownlint-disable no-duplicate-header --&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true&quot; width=&quot;60%&quot; alt=&quot;DeepSeek-V3&quot; /&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div align=&quot;center&quot; style=&quot;line-height: 1;&quot;&gt;
  &lt;a href=&quot;https://www.deepseek.com/&quot;&gt;&lt;img alt=&quot;Homepage&quot;
    src=&quot;https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://chat.deepseek.com/&quot;&gt;&lt;img alt=&quot;Chat&quot;
    src=&quot;https://img.shields.io/badge/🤖%20Chat-DeepSeek%20V3-536af5?color=536af5&amp;logoColor=white&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://huggingface.co/deepseek-ai&quot;&gt;&lt;img alt=&quot;Hugging Face&quot;
    src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;logoColor=white&quot;/&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://discord.gg/Tc7c45Zzu5&quot;&gt;&lt;img alt=&quot;Discord&quot;
    src=&quot;https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;logoColor=white&amp;color=7289da&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true&quot;&gt;&lt;img alt=&quot;Wechat&quot;
    src=&quot;https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&amp;logoColor=white&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/deepseek_ai&quot;&gt;&lt;img alt=&quot;Twitter Follow&quot;
    src=&quot;https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;logoColor=white&quot;/&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE&quot;&gt;&lt;img alt=&quot;Code License&quot;
    src=&quot;https://img.shields.io/badge/Code_License-MIT-f5de53?&amp;color=f5de53&quot;/&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL&quot;&gt;&lt;img alt=&quot;Model License&quot;
    src=&quot;https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&amp;color=f5de53&quot;/&gt;&lt;/a&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://arxiv.org/pdf/2412.19437&quot;&gt;&lt;b&gt;Paper Link&lt;/b&gt;👁️&lt;/a&gt;
&lt;/div&gt;

## Table of Contents

1. [Introduction](#1-introduction)
2. [Model Summary](#2-model-summary)
3. [Model Downloads](#3-model-downloads)
4. [Evaluation Results](#4-evaluation-results)
5. [Chat Website &amp; API Platform](#5-chat-website--api-platform)
6. [How to Run Locally](#6-how-to-run-locally)
7. [License](#7-license)
8. [Citation](#8-citation)
9. [Contact](#9-contact)


## 1. Introduction

We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. 
To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. 
Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. 
We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. 
Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.
Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.
In addition, its training process is remarkably stable. 
Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. 
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;80%&quot; src=&quot;figures/benchmark.png&quot;&gt;
&lt;/p&gt;

## 2. Model Summary

---

**Architecture: Innovative Load Balancing Strategy and Training Objective**

- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.
-  We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. 
    It can also be used for speculative decoding for inference acceleration. 

---

**Pre-Training: Towards Ultimate Training Efficiency**

- We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.  
- Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.  
  This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.  
- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.

---

**Post-Training: Knowledge Distillation from DeepSeek-R1**

-   We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.

---


## 3. Model Downloads

&lt;div align=&quot;center&quot;&gt;

| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |
| :------------: | :------------: | :------------: | :------------: | :------------: |
| DeepSeek-V3-Base | 671B | 37B | 128K   | [🤗 Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V3-Base)   |
| DeepSeek-V3   | 671B | 37B |  128K   | [🤗 Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V3)   |

&lt;/div&gt;

&gt; [!NOTE]
&gt; The total size of DeepSeek-V3 models on Hugging Face is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.

To ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6: [How_to Run_Locally](#6-how-to-run-locally).

For developers looking to dive deeper, we recommend exploring [README_WEIGHTS.md](./README_WEIGHTS.md) for details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.

## 4. Evaluation Results
### Base Model
#### Standard Benchmarks

&lt;div align=&quot;center&quot;&gt;


|  | Benchmark (Metric) | # Shots | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |
|---|-------------------|----------|--------|-------------|---------------|---------|
| | Architecture | - | MoE | Dense | Dense | MoE |
| | # Activated Params | - | 21B | 72B | 405B | 37B |
| | # Total Params | - | 236B | 72B | 405B | 671B |
| English | Pile-test (BPB) | - | 0.606 | 0.638 | **0.542** | 0.548 |
| | BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | **87.5** |
| | MMLU (Acc.) | 5-shot | 78.4 | 85.0 | 84.4 | **87.1** |
| | MMLU-Redux (Acc.) | 5-shot | 75.6 | 83.2 | 81.3 | **86.2** |
| | MMLU-Pro (Acc.) | 5-shot | 51.4 | 58.3 | 52.8 | **64.4** |
| | DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | **89.0** |
| | ARC-Easy (Acc.) | 25-shot | 97.6 | 98.4 | 98.4 | **98.9** |
| | ARC-Challenge (Acc.) | 25-shot | 92.2 | 94.5 | **95.3** | **95.3** |
| | HellaSwag (Acc.) | 10-shot | 87.1 | 84.8 | **89.2** | 88.9 |
| | PIQA (Acc.) | 0-shot | 83.9 | 82.6 | **85.9** | 84.7 |
| | WinoGrande (Acc.) | 5-shot | **86.3** | 82.3 | 85.2 | 84.9 |
| | RACE-Middle (Acc.) | 5-shot | 73.1 | 68.1 | **74.2** | 67.1 |
| | RACE-High (Acc.) | 5-shot | 52.6 | 50.3 | **56.8** | 51.3 |
| | TriviaQA (EM) | 5-shot | 80.0 | 71.9 | 82.7 | **82.9** |
| | NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | **41.5** | 40.0 |
| | AGIEval (Acc.) | 0-shot | 57.5 | 75.8 | 60.6 | **79.6** |
| Code | HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | **65.2** |
| | MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | **75.4** |
| | LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | **19.4** |
| | CRUXEval-I (Acc.) | 2-shot | 52.5 | 59.1 | 58.5 | **67.3** |
| | CRUXEval-O (Acc.) | 2-shot | 49.8 | 59.9 | 59.9 | **69.8** |
| Math | GSM8K (EM) | 8-shot | 81.6 | 88.3 | 83.5 | **89.3** |
| | MATH (EM) | 4-shot | 43.4 | 54.4 | 49.0 | **61.6** |
| | MGSM (EM) | 8-shot | 63.6 | 76.2 | 69.9 | **79.8** |
| | CMath (EM) | 3-shot | 78.7 | 84.5 | 77.3 | **90.7** |
| Chinese | CLUEWSC (EM) | 5-shot | 82.0 | 82.5 | **83.0** | 82.7 |
| | C-Eval (Acc.) | 5-shot | 81.4 | 89.2 | 72.5 | **90.1** |
| | CMMLU (Acc.) | 5-shot | 84.0 | **89.5** | 73.7 | 88.8 |
| | CMRC (EM) | 1-shot | **77.4** | 75.8 | 76.0 | 76.3 |
| | C3 (Acc.) | 0-shot | 77.4 | 76.7 | **79.7** | 78.6 |
| | CCPM (Acc.) | 0-shot | **93.0** | 88.5 | 78.6 | 92.0 |
| Multilingual | MMMLU-non-English (Acc.) | 5-shot | 64.0 | 74.8 | 73.8 | **79.4** |

&lt;/div&gt;

&gt; [!NOTE]
&gt; Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.
&gt; For more evaluation details, please check our paper. 

#### Context Window
&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;80%&quot; src=&quot;figures/niah.png&quot;&gt;
&lt;/p&gt;

Evaluation results on the ``Needle In A Haystack`` (NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to **128K**. 

### Chat Model
#### Standard Benchmarks (Models larger than 67B)
&lt;div align=&quot;center&quot;&gt;

| | **Benchmark (Metric)** | **DeepSeek V2-0506** | **DeepSeek V2.5-0905** | **Qwen2.5 72B-Inst.** | **Llama3.1 405B-Inst.** | **Claude-3.5-Sonnet-1022** | **GPT-4o 0513** | **DeepSeek V3** |
|---|---------------------|---------------------|----------------------|---------------------|----------------------|---------------------------|----------------|----------------|
| | Architecture | MoE | MoE | Dense | Dense | - | - | MoE |
| | # Activated Params | 21B | 21B | 72B | 405B | - | - | 37B |
| | # Total Params | 236B | 236B | 72B | 405B | - | - | 671B |
| English | MMLU (EM) | 78.2 | 80.6 | 85.3 | **88.6** | **88.3** | 87.2 | **88.5** |
| | MMLU-Redux (EM) | 77.9 | 80.3 | 85.6 | 86.2 | **88.9** | 88.0 | **89.1** |
| | MMLU-Pro (EM) | 58.5 | 66.2 | 71.6 | 73.3 | **78.0** | 72.6 | 75.9 |
| | DROP (3-shot F1) | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | **91.6** |
| | IF-Eval (Prompt Strict) | 57.7 | 80.6 | 84.1 | 86.0 | **86.5** | 84.3 | 86.1 |
| | GPQA-Diamond (Pass@1) | 35.3 | 41.3 | 49.0 | 51.1 | **65.0** | 49.9 | 59.1 |
| | SimpleQA (Correct) | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | **38.2** | 24.9 |
| | FRAMES (Acc.) | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | **80.5** | 73.3 |
| | LongBench v2 (Acc.) | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | **48.7** |
| Code | HumanEval-Mul (Pass@1) | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | **82.6** |
| | LiveCodeBench (Pass@1-COT) | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | **40.5** |
| | LiveCodeBench (Pass@1) | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | **37.6** |
| | Codeforces (Percentile) | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | **51.6** |
| | SWE Verified (Resolved) | - | 22.6 | 23.8 | 24.5 | **50.8** | 38.8 | 42.0 |
| | Aider-Edit (Acc.) | 60.3 | 71.6 | 65.4 | 63.9 | **84.2** | 72.9 | 79.7 |
| | Aider-Polyglot (Acc.) | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | **49.6** |
| Math | AIME 2024 (Pass@1) | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | **39.2** |
| | MATH-500 (EM) | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | **90.2** |
| | CNMO 2024 (Pass@1) | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | **43.2** |
| Chinese | CLUEWSC (EM) | 89.9 | 90.4 | **91.4** | 84.7 | 85.4 | 87.9 | 90.9 |
| | C-Eval (EM) | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | **86.5** |
| | C-SimpleQA (Correct) | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | **64.8** |

&lt;/div&gt;

&gt; [!NOTE]
&gt; All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.


####  Open Ended Generation Evaluation

&lt;div align=&quot;center&quot;&gt;



| Model | Arena-Hard | AlpacaEval 2.0 |
|-------|------------|----------------|
| DeepSeek-V2.5-0905 | 76.2 | 50.5 |
| Qwen2.5-72B-Instruct | 81.2 | 49.1 |
| LLaMA-3.1 405B | 69.3 | 40.5 |
| GPT-4o-0513 | 80.4 | 51.1 |
| Claude-Sonnet-3.5-1022 | 85.2 | 52.0 |
| DeepSeek-V3 | **85.5** | **70.0** |

&lt;/div&gt;

&gt; [!NOTE]
&gt; English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.


## 5. Chat Website &amp; API Platform
You can chat with DeepSeek-V3 on DeepSeek&#039;s official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in)

We also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)

## 6. How to Run Locally

DeepSeek-V3 can be deployed locally using the following hardware and open-source community software:

1. **DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference.
2. **SGLang**: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes, with Multi-Token Prediction [coming soon](https://github.com/sgl-project/sglang/issues/2591).
3. **LMDeploy**: Enables efficient FP8 and BF16 inference for local and cloud deployment.
4. **TensorRT-LLM**: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.
5. **vLLM**: Support DeepSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.
6. **AMD GPU**: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.
7. **Huawei Ascend NPU**: Supports running DeepSeek-V3 on Huawei Ascend devices.

Since FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.

Here is an example of converting FP8 weights to BF16:

```shell
cd inference
python fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weights
```

&gt; [!NOTE]
&gt; Hugging Face&#039;s Transformers has not been directly supported yet.

### 6.1 Inference with DeepSeek-Infer Demo (example only)

#### System Requirements

&gt; [!NOTE] 
&gt; Linux with Python 3.10 only. Mac and Windows are not supported.

Dependencies:
```pip-requirements
torch==2.4.1
triton==3.0.0
transformers==4.46.3
safetensors==0.4.5
```
#### Model Weights &amp; Demo Code Preparation

First, clone our DeepSeek-V3 GitHub repository:

```shell
git clone https://github.com/deepseek-ai/DeepSeek-V3.git
```

Navigate to the `inference` folder and install dependencies listed in `requirements.txt`. Easiest way is to use a package manager like `conda` or `uv` to create a new virtual environment and install the dependencies.

```shell
cd DeepSeek-V3/inference
pip install -r requirements.txt
```

Download the model weights from Hugging Face, and put them into `/path/to/DeepSeek-V3` folder.

#### Model Weights Conversion

Convert Hugging Face model weights to a specific format:

```shell
python convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16
```

#### Run

Then you can chat with DeepSeek-V3:

```shell
torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200
```

Or batch inference on a given file:

```shell
torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR generate.py --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE
```

### 6.2 Inference with SGLang (recommended)

[SGLang](https://github.com/sgl-project/sglang) currently supports [MLA optimizations](https://lmsys.org/blog/2024-09-04-sglang-v0-3/#deepseek-multi-head-latent-attention-mla-throughput-optimizations), [DP Attention](https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models), FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.

Notably, [SGLang v0.4.1](https://github.com/sgl-project/sglang/releases/tag/v0.4.1) fully supports running DeepSeek-V3 on both **NVIDIA and AMD GPUs**, making it a highly versatile and robust solution.

SGLang also supports [multi-node tensor parallelism](https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3#example-serving-with-2-h208), enabling you to run this model on multiple network-connected machines.

Multi-Token Prediction (MTP) is in development, and progress can be tracked in the [optimization plan](https://github.com/sgl-project/sglang/issues/2591).

Here are the launch instructions from the SGLang team: https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3

### 6.3 Inference with LMDeploy (recommended)
[LMDeploy](https://github.com/InternLM/lmdeploy), a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.

For comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here: https://github.com/InternLM/lmdeploy/issues/2960


### 6.4 Inference with TRT-LLM (recommended)

[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) now supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly: https://github.com/NVIDIA/TensorRT-LLM/tree/deepseek/examples/deepseek_v3. 


### 6.5 Inference with vLLM (recommended)

[vLLM](https://github.com/vllm-project/vllm) v0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offers _pipeline parallelism_ allowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to the [vLLM instructions](https://docs.vllm.ai/en/latest/serving/distributed_serving.html). Please feel free to follow [the enhancement plan](https://github.com/vllm-project/vllm/issues/11539) as well.

### 6.6 Recommended Inference Functionality with AMD GPUs

In collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to the [SGLang instructions](#63-inference-with-lmdeploy-recommended).

### 6.7 Recommended Inference Functionality with Huawei Ascend NPUs
The [MindIE](https://www.hiascend.com/en/software/mindie) framework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow the [instructions here](https://modelers.cn/models/MindIE/deepseekv3).


## 7. License
This code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use.

## 8. Citation
```
@misc{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[odoo/odoo]]></title>
            <link>https://github.com/odoo/odoo</link>
            <guid>https://github.com/odoo/odoo</guid>
            <pubDate>Sat, 29 Mar 2025 00:03:54 GMT</pubDate>
            <description><![CDATA[Odoo. Open Source Apps To Grow Your Business.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/odoo/odoo">odoo/odoo</a></h1>
            <p>Odoo. Open Source Apps To Grow Your Business.</p>
            <p>Language: Python</p>
            <p>Stars: 41,636</p>
            <p>Forks: 27,028</p>
            <p>Stars today: 34 stars today</p>
            <h2>README</h2><pre>[![Build Status](https://runbot.odoo.com/runbot/badge/flat/1/master.svg)](https://runbot.odoo.com/runbot)
[![Tech Doc](https://img.shields.io/badge/master-docs-875A7B.svg?style=flat&amp;colorA=8F8F8F)](https://www.odoo.com/documentation/master)
[![Help](https://img.shields.io/badge/master-help-875A7B.svg?style=flat&amp;colorA=8F8F8F)](https://www.odoo.com/forum/help-1)
[![Nightly Builds](https://img.shields.io/badge/master-nightly-875A7B.svg?style=flat&amp;colorA=8F8F8F)](https://nightly.odoo.com/)

Odoo
----

Odoo is a suite of web based open source business apps.

The main Odoo Apps include an &lt;a href=&quot;https://www.odoo.com/page/crm&quot;&gt;Open Source CRM&lt;/a&gt;,
&lt;a href=&quot;https://www.odoo.com/app/website&quot;&gt;Website Builder&lt;/a&gt;,
&lt;a href=&quot;https://www.odoo.com/app/ecommerce&quot;&gt;eCommerce&lt;/a&gt;,
&lt;a href=&quot;https://www.odoo.com/app/inventory&quot;&gt;Warehouse Management&lt;/a&gt;,
&lt;a href=&quot;https://www.odoo.com/app/project&quot;&gt;Project Management&lt;/a&gt;,
&lt;a href=&quot;https://www.odoo.com/app/accounting&quot;&gt;Billing &amp;amp; Accounting&lt;/a&gt;,
&lt;a href=&quot;https://www.odoo.com/app/point-of-sale-shop&quot;&gt;Point of Sale&lt;/a&gt;,
&lt;a href=&quot;https://www.odoo.com/app/employees&quot;&gt;Human Resources&lt;/a&gt;,
&lt;a href=&quot;https://www.odoo.com/app/social-marketing&quot;&gt;Marketing&lt;/a&gt;,
&lt;a href=&quot;https://www.odoo.com/app/manufacturing&quot;&gt;Manufacturing&lt;/a&gt;,
&lt;a href=&quot;https://www.odoo.com/&quot;&gt;...&lt;/a&gt;

Odoo Apps can be used as stand-alone applications, but they also integrate seamlessly so you get
a full-featured &lt;a href=&quot;https://www.odoo.com&quot;&gt;Open Source ERP&lt;/a&gt; when you install several Apps.

Getting started with Odoo
-------------------------

For a standard installation please follow the &lt;a href=&quot;https://www.odoo.com/documentation/master/administration/install/install.html&quot;&gt;Setup instructions&lt;/a&gt;
from the documentation.

To learn the software, we recommend the &lt;a href=&quot;https://www.odoo.com/slides&quot;&gt;Odoo eLearning&lt;/a&gt;, or &lt;a href=&quot;https://www.odoo.com/page/scale-up-business-game&quot;&gt;Scale-up&lt;/a&gt;, the &lt;a href=&quot;https://www.odoo.com/page/scale-up-business-game&quot;&gt;business game&lt;/a&gt;. Developers can start with &lt;a href=&quot;https://www.odoo.com/documentation/master/developer/howtos.html&quot;&gt;the developer tutorials&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[disposable-email-domains/disposable-email-domains]]></title>
            <link>https://github.com/disposable-email-domains/disposable-email-domains</link>
            <guid>https://github.com/disposable-email-domains/disposable-email-domains</guid>
            <pubDate>Sat, 29 Mar 2025 00:03:53 GMT</pubDate>
            <description><![CDATA[a list of disposable email domains]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/disposable-email-domains/disposable-email-domains">disposable-email-domains/disposable-email-domains</a></h1>
            <p>a list of disposable email domains</p>
            <p>Language: Python</p>
            <p>Stars: 3,559</p>
            <p>Forks: 622</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre>List of disposable email domains
========================
This repo contains a [list of disposable and temporary email address domains](disposable_email_blocklist.conf) often used to register dummy users in order to spam or abuse some services.

We cannot guarantee all of these can still be considered disposable but we do basic checking so chances are they were disposable at one point in time.

&gt; One of the most impactful mechanisms we currently have is prohibiting known &quot;throw-away&quot; email domains from creating accounts on the index. We currently use the `disposable-email-domains` list as well as our own internal list to block registration with －or association of － such domains for PyPI accounts.

-- Ee Durbin, PyPI Admin, Director of Infrastructure (PSF) [link](https://blog.pypi.org/posts/2024-06-16-prohibiting-msn-emails/)

Allowlist
=========
The file [allowlist.conf](allowlist.conf) gathers email domains that are often identified as disposable but in fact are not.

Contributing
============
Feel free to create PR with additions or request removal of some domain (with reasons).

**Specifically, please cite in your PR where one can generate a disposable email address which uses that domain, so the maintainers can verify it.**

Please add new disposable domains directly into [disposable_email_blocklist.conf](disposable_email_blocklist.conf) in the same format (only second level domains on new line without @), then run [maintain.sh](maintain.sh). The shell script will help you convert uppercase to lowercase, sort, remove duplicates and remove allowlisted domains.

License
=======
You can copy, modify, distribute and use the work, even for commercial purposes, all without asking permission.

[![Licensed under CC0](https://licensebuttons.net/p/zero/1.0/88x31.png)](https://creativecommons.org/publicdomain/zero/1.0/) 

Changelog
============

* 1/9/25 Enabled [GitHub sponsorhip](https://github.com/sponsors/disposable-email-domains) for this work. Everybody can do it, but currently only one person does it. Send them $2 for a coffee if you care.

* 2/11/21 We created a github [org account](https://github.com/disposable-email-domains) and transferred the repository to it.

* 4/18/19 [@di](https://github.com/di) [joined](https://github.com/martenson/disposable-email-domains/issues/205) as a core maintainer of this project. Thank you!

* 7/31/17 [@deguif](https://github.com/deguif) [joined](https://github.com/martenson/disposable-email-domains/issues/106) as a core maintainer of this project. Thanks!

* 12/6/16 - Available as [PyPI module](https://pypi.org/project/disposable-email-domains) thanks to [@di](https://github.com/di)

* 7/27/16 - Converted all domains to the second level. This means that starting from [this commit](https://github.com/martenson/disposable-email-domains/commit/61ae67aacdab0b19098de2e13069d7c35b74017a) the implementers should take care of matching the second level domain names properly i.e. `@xxx.yyy.zzz` should match `yyy.zzz` in blocklist where `zzz` is a [public suffix](https://publicsuffix.org/). More info in [#46](https://github.com/martenson/disposable-email-domains/issues/46)

* 9/2/14 - First commit [393c21f5](https://github.com/disposable-email-domains/disposable-email-domains/commit/393c21f56b5186f8db7d197b11cf1d7c5490a6f9)
  
Example Usage
=============

TOC: [Python](#python), [PHP](#php), [Go](#go), [Ruby on Rails](#ruby-on-rails), [NodeJS](#nodejs), [C#](#c), [bash](#bash), [Java](#java), [Swift](#swift)

### Python
```Python
with open(&#039;disposable_email_blocklist.conf&#039;) as blocklist:
    blocklist_content = {line.rstrip() for line in blocklist.readlines()}
if email.partition(&#039;@&#039;)[2] in blocklist_content:
    message = &quot;Please enter your permanent email address.&quot;
    return (False, message)
else:
    return True
```

Available as [PyPI module](https://pypi.org/project/disposable-email-domains) thanks to [@di](https://github.com/di)
```python
&gt;&gt;&gt; from disposable_email_domains import blocklist
&gt;&gt;&gt; &#039;bearsarefuzzy.com&#039; in blocklist
True
```

### PHP
contributed by [@txt3rob](https://github.com/txt3rob), [@deguif](https://github.com/deguif), [@pjebs](https://github.com/pjebs) and [@Wruczek](https://github.com/Wruczek)

1. Make sure the passed email is valid. You can check that with [filter_var](https://secure.php.net/manual/en/function.filter-var.php)
2. Make sure you have the mbstring extension installed on your server
```php
function isDisposableEmail($email, $blocklist_path = null) {
    if (!$blocklist_path) $blocklist_path = __DIR__ . &#039;/disposable_email_blocklist.conf&#039;;
    $disposable_domains = file($blocklist_path, FILE_IGNORE_NEW_LINES | FILE_SKIP_EMPTY_LINES);
    $domain = mb_strtolower(explode(&#039;@&#039;, trim($email))[1]);
    return in_array($domain, $disposable_domains);
}
```

Alternatively check out Composer package https://github.com/elliotjreed/disposable-emails-filter-php.

### Go
contributed by [@pjebs](https://github.com/pjebs)

```go
import (&quot;bufio&quot;; &quot;os&quot;; &quot;strings&quot;;)
var disposableList = make(map[string]struct{}, 3500)
func init() {
	f, _ := os.Open(&quot;disposable_email_blocklist.conf&quot;)
	for scanner := bufio.NewScanner(f); scanner.Scan(); {
		disposableList[scanner.Text()] = struct{}{}
	}
	f.Close()
}

func isDisposableEmail(email string) (disposable bool) {
	segs := strings.Split(email, &quot;@&quot;)
	_, disposable = disposableList[strings.ToLower(segs[len(segs)-1])]
	return
}
```

Alternatively check out Go package https://github.com/rocketlaunchr/anti-disposable-email.

### Ruby on Rails
contributed by [@MitsunChieh](https://github.com/MitsunChieh)

In the resource model, usually it is `user.rb`:

```Ruby
before_validation :reject_email_blocklist

def reject_email_blocklist
  blocklist = File.read(&#039;config/disposable_email_blocklist.conf&#039;).split(&quot;\n&quot;)

  if blocklist.include?(email.split(&#039;@&#039;)[1])
    errors[:email] &lt;&lt; &#039;invalid email&#039;
    return false
  else
    return true
  end
end
```

### Node.js
contributed by [@boywithkeyboard](https://github.com/boywithkeyboard)

```js
import { readFile } from &#039;node:fs/promises&#039;

let blocklist

async function isDisposable(email) {
  if (!blocklist) {
    const content = await readFile(&#039;disposable_email_blocklist.conf&#039;, { encoding: &#039;utf-8&#039; })

    blocklist = content.split(&#039;\r\n&#039;).slice(0, -1)
  }

  return blocklist.includes(email.split(&#039;@&#039;)[1])
}
```

Alternatively check out NPM package https://github.com/mziyut/disposable-email-domains-js.

### C#
```C#
private static readonly Lazy&lt;HashSet&lt;string&gt;&gt; _emailBlockList = new Lazy&lt;HashSet&lt;string&gt;&gt;(() =&gt;
{
  var lines = File.ReadLines(&quot;disposable_email_blocklist.conf&quot;)
    .Where(line =&gt; !string.IsNullOrWhiteSpace(line) &amp;&amp; !line.TrimStart().StartsWith(&quot;//&quot;));
  return new HashSet&lt;string&gt;(lines, StringComparer.OrdinalIgnoreCase);
});

private static bool IsBlocklisted(string domain) =&gt; _emailBlockList.Value.Contains(domain);

...

var addr = new MailAddress(email);
if (IsBlocklisted(addr.Host)))
  throw new ApplicationException(&quot;Email is blocklisted.&quot;);
```

### Bash

```
#!/bin/bash

# This script checks if an email address is temporary.

# Read blocklist file into a bash array
mapfile -t blocklist &lt; disposable_email_blocklist.conf

# Check if email domain is in blocklist
if [[ &quot; ${blocklist[@]} &quot; =~ &quot; ${email#*@} &quot; ]]; then
    message=&quot;Please enter your permanent email address.&quot;
    return_value=false
else
    return_value=true
fi

# Return result
echo &quot;$return_value&quot;
```

### Java

Code assumes that you have added `disposable_email_blocklist.conf` next to your class as classpath resource.

```Java
private static final Set&lt;String&gt; DISPOSABLE_EMAIL_DOMAINS;

static {
    Set&lt;String&gt; domains = new HashSet&lt;&gt;();
    try (BufferedReader in = new BufferedReader(
            new InputStreamReader(
                EMailChecker.class.getResourceAsStream(&quot;disposable_email_blocklist.conf&quot;), StandardCharsets.UTF_8))) {
        String line;
        while ((line = in.readLine()) != null) {
            line = line.trim();
            if (line.isEmpty()) {
                continue;
            }
            
            domains.add(line);
        }
    } catch (IOException ex) {
        LOG.error(&quot;Failed to load list of disposable email domains.&quot;, ex);
    }
    DISPOSABLE_EMAIL_DOMAINS = domains;
}

public static boolean isDisposable(String email) throws AddressException {
    InternetAddress contact = new InternetAddress(email);
    return isDisposable(contact);
}

public static boolean isDisposable(InternetAddress contact) throws AddressException {
    String address = contact.getAddress();
    int domainSep = address.indexOf(&#039;@&#039;);
    String domain = (domainSep &gt;= 0) ? address.substring(domainSep + 1) : address;
    return DISPOSABLE_EMAIL_DOMAINS.contains(domain);
}
```

### Swift
contributed by [@1998code](https://github.com/1998code)

```swift
func checkBlockList(email: String, completion: @escaping (Bool) -&gt; Void) {
    let url = URL(string: &quot;https://raw.githubusercontent.com/disposable-email-domains/disposable-email-domains/master/disposable_email_blocklist.conf&quot;)!
    let task = URLSession.shared.dataTask(with: url) { data, response, error in
        if let data = data {
            if let string = String(data: data, encoding: .utf8) {
                let lines = string.components(separatedBy: &quot;\n&quot;)
                for line in lines {
                    if email.contains(line) {
                        completion(true)
                        return
                    }
                }
            }
        }
        completion(false)
    }
    task.resume()
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[networkx/networkx]]></title>
            <link>https://github.com/networkx/networkx</link>
            <guid>https://github.com/networkx/networkx</guid>
            <pubDate>Sat, 29 Mar 2025 00:03:52 GMT</pubDate>
            <description><![CDATA[Network Analysis in Python]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/networkx/networkx">networkx/networkx</a></h1>
            <p>Network Analysis in Python</p>
            <p>Language: Python</p>
            <p>Stars: 15,560</p>
            <p>Forks: 3,316</p>
            <p>Stars today: 24 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[huggingface/accelerate]]></title>
            <link>https://github.com/huggingface/accelerate</link>
            <guid>https://github.com/huggingface/accelerate</guid>
            <pubDate>Sat, 29 Mar 2025 00:03:51 GMT</pubDate>
            <description><![CDATA[🚀 A simple way to launch, train, and use PyTorch models on almost any device and distributed configuration, automatic mixed precision (including fp8), and easy-to-configure FSDP and DeepSpeed support]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/huggingface/accelerate">huggingface/accelerate</a></h1>
            <p>🚀 A simple way to launch, train, and use PyTorch models on almost any device and distributed configuration, automatic mixed precision (including fp8), and easy-to-configure FSDP and DeepSpeed support</p>
            <p>Language: Python</p>
            <p>Stars: 8,547</p>
            <p>Forks: 1,058</p>
            <p>Stars today: 4 stars today</p>
            <h2>README</h2><pre>&lt;!---
Copyright 2021 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
--&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;br&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/huggingface/accelerate/main/docs/source/imgs/accelerate_logo.png&quot; width=&quot;400&quot;/&gt;
    &lt;br&gt;
&lt;p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;!-- Uncomment when CircleCI is set up
    &lt;a href=&quot;https://circleci.com/gh/huggingface/accelerate&quot;&gt;&lt;img alt=&quot;Build&quot; src=&quot;https://img.shields.io/circleci/build/github/huggingface/transformers/master&quot;&gt;&lt;/a&gt;
    --&gt;
    &lt;a href=&quot;https://github.com/huggingface/accelerate/blob/main/LICENSE&quot;&gt;&lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/github/license/huggingface/accelerate.svg?color=blue&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://huggingface.co/docs/accelerate/index.html&quot;&gt;&lt;img alt=&quot;Documentation&quot; src=&quot;https://img.shields.io/website/http/huggingface.co/docs/accelerate/index.html.svg?down_color=red&amp;down_message=offline&amp;up_message=online&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/huggingface/accelerate/releases&quot;&gt;&lt;img alt=&quot;GitHub release&quot; src=&quot;https://img.shields.io/github/release/huggingface/accelerate.svg&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/huggingface/accelerate/blob/main/CODE_OF_CONDUCT.md&quot;&gt;&lt;img alt=&quot;Contributor Covenant&quot; src=&quot;https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;h3 align=&quot;center&quot;&gt;
&lt;p&gt;Run your *raw* PyTorch training script on any kind of device
&lt;/h3&gt;

&lt;h3 align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://hf.co/course&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/huggingface/accelerate/main/docs/source/imgs/course_banner.png&quot;&gt;&lt;/a&gt;
&lt;/h3&gt;

## Easy to integrate

🤗 Accelerate was created for PyTorch users who like to write the training loop of PyTorch models but are reluctant to write and maintain the boilerplate code needed to use multi-GPUs/TPU/fp16.

🤗 Accelerate abstracts exactly and only the boilerplate code related to multi-GPUs/TPU/fp16 and leaves the rest of your code unchanged.

Here is an example:

```diff
  import torch
  import torch.nn.functional as F
  from datasets import load_dataset
+ from accelerate import Accelerator

+ accelerator = Accelerator()
- device = &#039;cpu&#039;
+ device = accelerator.device

  model = torch.nn.Transformer().to(device)
  optimizer = torch.optim.Adam(model.parameters())

  dataset = load_dataset(&#039;my_dataset&#039;)
  data = torch.utils.data.DataLoader(dataset, shuffle=True)

+ model, optimizer, data = accelerator.prepare(model, optimizer, data)

  model.train()
  for epoch in range(10):
      for source, targets in data:
          source = source.to(device)
          targets = targets.to(device)

          optimizer.zero_grad()

          output = model(source)
          loss = F.cross_entropy(output, targets)

-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
```

As you can see in this example, by adding 5-lines to any standard PyTorch training script you can now run on any kind of single or distributed node setting (single CPU, single GPU, multi-GPUs and TPUs) as well as with or without mixed precision (fp8, fp16, bf16).

In particular, the same code can then be run without modification on your local machine for debugging or your training environment.

🤗 Accelerate even handles the device placement for you (which requires a few more changes to your code, but is safer in general), so you can even simplify your training loop further:

```diff
  import torch
  import torch.nn.functional as F
  from datasets import load_dataset
+ from accelerate import Accelerator

- device = &#039;cpu&#039;
+ accelerator = Accelerator()

- model = torch.nn.Transformer().to(device)
+ model = torch.nn.Transformer()
  optimizer = torch.optim.Adam(model.parameters())

  dataset = load_dataset(&#039;my_dataset&#039;)
  data = torch.utils.data.DataLoader(dataset, shuffle=True)

+ model, optimizer, data = accelerator.prepare(model, optimizer, data)

  model.train()
  for epoch in range(10):
      for source, targets in data:
-         source = source.to(device)
-         targets = targets.to(device)

          optimizer.zero_grad()

          output = model(source)
          loss = F.cross_entropy(output, targets)

-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
```

Want to learn more? Check out the [documentation](https://huggingface.co/docs/accelerate) or have a look at our [examples](https://github.com/huggingface/accelerate/tree/main/examples).

## Launching script

🤗 Accelerate also provides an optional CLI tool that allows you to quickly configure and test your training environment before launching the scripts. No need to remember how to use `torch.distributed.run` or to write a specific launcher for TPU training!
On your machine(s) just run:

```bash
accelerate config
```

and answer the questions asked. This will generate a config file that will be used automatically to properly set the default options when doing

```bash
accelerate launch my_script.py --args_to_my_script
``` 

For instance, here is how you would run the GLUE example on the MRPC task (from the root of the repo):

```bash
accelerate launch examples/nlp_example.py
```

This CLI tool is **optional**, and you can still use `python my_script.py` or `python -m torchrun my_script.py` at your convenience.

You can also directly pass in the arguments you would to `torchrun` as arguments to `accelerate launch` if you wish to not run` accelerate config`.

For example, here is how to launch on two GPUs:

```bash
accelerate launch --multi_gpu --num_processes 2 examples/nlp_example.py
```

To learn more, check the CLI documentation available [here](https://huggingface.co/docs/accelerate/package_reference/cli).

Or view the configuration zoo [here](https://github.com/huggingface/accelerate/blob/main/examples/config_yaml_templates/)

## Launching multi-CPU run using MPI

🤗 Here is another way to launch multi-CPU run using MPI. You can learn how to install Open MPI on [this page](https://www.open-mpi.org/faq/?category=building#easy-build). You can use Intel MPI or MVAPICH as well.
Once you have MPI setup on your cluster, just run:
```bash
accelerate config
```
Answer the questions that are asked, selecting to run using multi-CPU, and answer &quot;yes&quot; when asked if you want accelerate to launch mpirun.
Then, use `accelerate launch` with your script like:
```bash
accelerate launch examples/nlp_example.py
```
Alternatively, you can use mpirun directly, without using the CLI like:
```bash
mpirun -np 2 python examples/nlp_example.py
```

## Launching training using DeepSpeed

🤗 Accelerate supports training on single/multiple GPUs using DeepSpeed. To use it, you don&#039;t need to change anything in your training code; you can set everything using just `accelerate config`. However, if you desire to tweak your DeepSpeed related args from your Python script, we provide you the `DeepSpeedPlugin`.

```python
from accelerate import Accelerator, DeepSpeedPlugin

# deepspeed needs to know your gradient accumulation steps beforehand, so don&#039;t forget to pass it
# Remember you still need to do gradient accumulation by yourself, just like you would have done without deepspeed
deepspeed_plugin = DeepSpeedPlugin(zero_stage=2, gradient_accumulation_steps=2)
accelerator = Accelerator(mixed_precision=&#039;fp16&#039;, deepspeed_plugin=deepspeed_plugin)

# How to save your 🤗 Transformer?
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(save_dir, save_function=accelerator.save, state_dict=accelerator.get_state_dict(model))
```

Note: DeepSpeed support is experimental for now. In case you get into some problem, please open an issue.

## Launching your training from a notebook

🤗 Accelerate also provides a `notebook_launcher` function you can use in a notebook to launch a distributed training. This is especially useful for Colab or Kaggle notebooks with a TPU backend. Just define your training loop in a `training_function` then in your last cell, add:

```python
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

An example can be found in [this notebook](https://github.com/huggingface/notebooks/blob/main/examples/accelerate_examples/simple_nlp_example.ipynb). [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/accelerate_examples/simple_nlp_example.ipynb)

## Why should I use 🤗 Accelerate?

You should use 🤗 Accelerate when you want to easily run your training scripts in a distributed environment without having to renounce full control over your training loop. This is not a high-level framework above PyTorch, just a thin wrapper so you don&#039;t have to learn a new library. In fact, the whole API of 🤗 Accelerate is in one class, the `Accelerator` object.

## Why shouldn&#039;t I use 🤗 Accelerate?

You shouldn&#039;t use 🤗 Accelerate if you don&#039;t want to write a training loop yourself. There are plenty of high-level libraries above PyTorch that will offer you that, 🤗 Accelerate is not one of them.

## Frameworks using 🤗 Accelerate

If you like the simplicity of 🤗 Accelerate but would prefer a higher-level abstraction around its capabilities, some frameworks and libraries that are built on top of 🤗 Accelerate are listed below:

* [Amphion](https://github.com/open-mmlab/Amphion) is a toolkit for Audio, Music, and Speech Generation. Its purpose is to support reproducible research and help junior researchers and engineers get started in the field of audio, music, and speech generation research and development.
* [Animus](https://github.com/Scitator/animus) is a minimalistic framework to run machine learning experiments. Animus highlights common &quot;breakpoints&quot; in ML experiments and provides a unified interface for them within [IExperiment](https://github.com/Scitator/animus/blob/main/animus/core.py#L76).
* [Catalyst](https://github.com/catalyst-team/catalyst#getting-started) is a PyTorch framework for Deep Learning Research and Development. It focuses on reproducibility, rapid experimentation, and codebase reuse so you can create something new rather than write yet another train loop. Catalyst provides a [Runner](https://catalyst-team.github.io/catalyst/api/core.html#runner) to connect all parts of the experiment: hardware backend, data transformations, model training, and inference logic.
* [fastai](https://github.com/fastai/fastai#installing) is a PyTorch framework for Deep Learning that simplifies training fast and accurate neural nets using modern best practices. fastai provides a [Learner](https://docs.fast.ai/learner.html#Learner) to handle the training, fine-tuning, and inference of deep learning algorithms.
* [Finetuner](https://github.com/jina-ai/finetuner) is a service that enables models to create higher-quality embeddings for semantic search, visual similarity search, cross-modal text&lt;-&gt;image search, recommendation systems, clustering, duplication detection, anomaly detection, or other uses.
* [InvokeAI](https://github.com/invoke-ai/InvokeAI) is a creative engine for Stable Diffusion models, offering industry-leading WebUI, terminal usage support, and serves as the foundation for many commercial products.
* [Kornia](https://kornia.readthedocs.io/en/latest/get-started/introduction.html) is a differentiable library that allows classical computer vision to be integrated into deep learning models. Kornia provides a [Trainer](https://kornia.readthedocs.io/en/latest/x.html#kornia.x.Trainer) with the specific purpose to train and fine-tune the supported deep learning algorithms within the library.
* [Open Assistant](https://projects.laion.ai/Open-Assistant/) is a chat-based assistant that understands tasks, can interact with their party systems, and retrieve information dynamically to do so. 
* [pytorch-accelerated](https://github.com/Chris-hughes10/pytorch-accelerated) is a lightweight training library, with a streamlined feature set centered around a general-purpose [Trainer](https://pytorch-accelerated.readthedocs.io/en/latest/trainer.html), that places a huge emphasis on simplicity and transparency; enabling users to understand exactly what is going on under the hood, but without having to write and maintain the boilerplate themselves!
* [Stable Diffusion web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is an open-source browser-based easy-to-use interface based on the Gradio library for Stable Diffusion.
* [torchkeras](https://github.com/lyhue1991/torchkeras) is a simple tool for training pytorch model just in a keras style, a dynamic and beautiful plot is provided in notebook to monitor your loss or metric.
* [transformers](https://github.com/huggingface/transformers) as a tool for helping train state-of-the-art machine learning models in PyTorch, Tensorflow, and JAX. (Accelerate is the backend for the PyTorch side).


## Installation

This repository is tested on Python 3.8+ and PyTorch 1.10.0+

You should install 🤗 Accelerate in a [virtual environment](https://docs.python.org/3/library/venv.html). If you&#039;re unfamiliar with Python virtual environments, check out the [user guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).

First, create a virtual environment with the version of Python you&#039;re going to use and activate it.

Then, you will need to install PyTorch: refer to the [official installation page](https://pytorch.org/get-started/locally/#start-locally) regarding the specific install command for your platform. Then 🤗 Accelerate can be installed using pip as follows:

```bash
pip install accelerate
```

## Supported integrations

- CPU only
- multi-CPU on one node (machine)
- multi-CPU on several nodes (machines)
- single GPU
- multi-GPU on one node (machine)
- multi-GPU on several nodes (machines)
- TPU
- FP16/BFloat16 mixed precision
- FP8 mixed precision with [Transformer Engine](https://github.com/NVIDIA/TransformerEngine) or [MS-AMP](https://github.com/Azure/MS-AMP/)
- DeepSpeed support (Experimental)
- PyTorch Fully Sharded Data Parallel (FSDP) support (Experimental)
- Megatron-LM support (Experimental)

## Citing 🤗 Accelerate

If you use 🤗 Accelerate in your publication, please cite it by using the following BibTeX entry.

```bibtex
@Misc{accelerate,
  title =        {Accelerate: Training and inference at scale made simple, efficient and adaptable.},
  author =       {Sylvain Gugger and Lysandre Debut and Thomas Wolf and Philipp Schmid and Zachary Mueller and Sourab Mangrulkar and Marc Sun and Benjamin Bossan},
  howpublished = {\url{https://github.com/huggingface/accelerate}},
  year =         {2022}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[getsentry/sentry]]></title>
            <link>https://github.com/getsentry/sentry</link>
            <guid>https://github.com/getsentry/sentry</guid>
            <pubDate>Sat, 29 Mar 2025 00:03:50 GMT</pubDate>
            <description><![CDATA[Developer-first error tracking and performance monitoring]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/getsentry/sentry">getsentry/sentry</a></h1>
            <p>Developer-first error tracking and performance monitoring</p>
            <p>Language: Python</p>
            <p>Stars: 40,411</p>
            <p>Forks: 4,297</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://sentry.io/?utm_source=github&amp;utm_medium=logo&quot; target=&quot;_blank&quot;&gt;
      &lt;img src=&quot;https://sentry-brand.storage.googleapis.com/sentry-wordmark-dark-280x84.png&quot; alt=&quot;Sentry&quot; width=&quot;280&quot; height=&quot;84&quot; /&gt;
    &lt;/a&gt;
  &lt;/p&gt;
  &lt;p align=&quot;center&quot;&gt;
    Users and logs provide clues. Sentry provides answers.
  &lt;/p&gt;
&lt;/p&gt;

# What&#039;s Sentry?

Sentry is a developer-first error tracking and performance monitoring platform that helps developers see what actually matters, solve quicker, and learn continuously about their applications.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/projects.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/issue-details.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/transaction-summary.png&quot; width=&quot;270&quot; /&gt;
  &lt;img src=&quot;https://github.com/getsentry/sentry/raw/master/.github/screenshots/releases.png&quot; width=&quot;270&quot; /&gt;
&lt;/p&gt;

## Official Sentry SDKs

- [JavaScript](https://github.com/getsentry/sentry-javascript)
- [Electron](https://github.com/getsentry/sentry-electron/)
- [React-Native](https://github.com/getsentry/sentry-react-native)
- [Python](https://github.com/getsentry/sentry-python)
- [Ruby](https://github.com/getsentry/sentry-ruby)
- [PHP](https://github.com/getsentry/sentry-php)
- [Laravel](https://github.com/getsentry/sentry-laravel)
- [Go](https://github.com/getsentry/sentry-go)
- [Rust](https://github.com/getsentry/sentry-rust)
- [Java/Kotlin](https://github.com/getsentry/sentry-java)
- [Objective-C/Swift](https://github.com/getsentry/sentry-cocoa)
- [C\#/F\#](https://github.com/getsentry/sentry-dotnet)
- [C/C++](https://github.com/getsentry/sentry-native)
- [Dart](https://github.com/getsentry/sentry-dart)
- [Perl](https://github.com/getsentry/perl-raven)
- [Clojure](https://github.com/getsentry/sentry-clj/)
- [Elixir](https://github.com/getsentry/sentry-elixir)
- [Unity](https://github.com/getsentry/sentry-unity)
- [Unreal Engine](https://github.com/getsentry/sentry-unreal)
- [PowerShell](https://github.com/getsentry/sentry-powershell)

# Resources

- [Documentation](https://docs.sentry.io/)
- [Discussions](https://github.com/getsentry/sentry/discussions) (Bugs, feature requests,
  general questions)
- [Discord](https://discord.gg/PXa5Apfe7K)
- [Contributing](https://docs.sentry.io/internal/contributing/)
- [Bug Tracker](https://github.com/getsentry/sentry/issues)
- [Code](https://github.com/getsentry/sentry)
- [Transifex](https://www.transifex.com/getsentry/sentry/) (Translate
  Sentry\!)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Agenta-AI/agenta]]></title>
            <link>https://github.com/Agenta-AI/agenta</link>
            <guid>https://github.com/Agenta-AI/agenta</guid>
            <pubDate>Sat, 29 Mar 2025 00:03:49 GMT</pubDate>
            <description><![CDATA[The open-source LLMOps platform: prompt playground, prompt management, LLM evaluation, and LLM Observability all in one place.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Agenta-AI/agenta">Agenta-AI/agenta</a></h1>
            <p>The open-source LLMOps platform: prompt playground, prompt management, LLM evaluation, and LLM Observability all in one place.</p>
            <p>Language: Python</p>
            <p>Stars: 2,276</p>
            <p>Forks: 280</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://agenta.ai?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=readme&quot;&gt;
      &lt;picture &gt;
        &lt;source width=&quot;275&quot; media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/Agenta-AI/agenta/assets/4510758/cdddf5ad-2352-4920-b1d9-ae7f8d9d7735&quot;  &gt;
        &lt;source width=&quot;275&quot; media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/Agenta-AI/agenta/assets/4510758/ab75cbac-b807-496f-aab3-57463a33f726&quot;  &gt;
        &lt;img alt=&quot;Shows the logo of agenta&quot; src=&quot;https://github.com/Agenta-AI/agenta/assets/4510758/68e055d4-d7b8-4943-992f-761558c64253&quot; &gt;
      &lt;/picture&gt;
  &lt;/a&gt;
&lt;/p&gt;
  &lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://docs.agenta.ai?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=readme&quot;&gt;Documentation&lt;/a&gt; |
    &lt;a href=&quot;https://agenta.ai?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=readme&quot;&gt;Website&lt;/a&gt; |
    &lt;a href=&quot;https://join.slack.com/t/agenta-hq/shared_invite/zt-2yewk6o2b-DmhyA4h_lkKwecDtIsj1AQ&quot;&gt;Slack&lt;/a&gt;
  &lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;strong&gt; &lt;h1&gt; The Open source LLMOps Platform &lt;/h1&gt;&lt;/strong&gt;
  Prompt playground, prompt management, evaluation, and observability
&lt;/div&gt;
&lt;/br&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/license-MIT-blue.svg&quot; alt=&quot;MIT license.&quot; /&gt;
  &lt;a href=&quot;https://docs.agenta.ai?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=readme&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Doc-online-green&quot; alt=&quot;Doc&quot;&gt;
  &lt;/a&gt;

  &lt;a href=&quot;https://github.com/Agenta-AI/agenta/blob/main/CONTRIBUTING.md&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/PRs-Welcome-brightgreen&quot; alt=&quot;PRs welcome&quot; /&gt;
  &lt;/a&gt;
  &lt;img src=&quot;https://img.shields.io/github/contributors/Agenta-AI/agenta&quot; alt=&quot;Contributors&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/github/last-commit/Agenta-AI/agenta&quot; alt=&quot;Last Commit&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/commit-activity/m/agenta-ai/agenta&quot; alt=&quot;Commits per month&quot;&gt;

  &lt;a href=&quot;https://pypi.org/project/agenta/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/dm/agenta&quot; alt=&quot;PyPI - Downloads&quot;&gt;
  &lt;/a&gt;
&lt;/br&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://join.slack.com/t/agenta-hq/shared_invite/zt-2yewk6o2b-DmhyA4h_lkKwecDtIsj1AQ&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/JOIN US ON SLACK-4A154B?style=for-the-badge&amp;logo=slack&amp;logoColor=white&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://www.linkedin.com/company/agenta-ai/&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&amp;logo=linkedin&amp;logoColor=white&quot; /&gt;
    &lt;/a&gt;
    &lt;a  href=&quot;https://twitter.com/agenta_ai&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/twitter/follow/agenta_ai?style=social&quot; height=&quot;28&quot; /&gt;
    &lt;/a&gt;
&lt;/p&gt;

  &lt;br /&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://cloud.agenta.ai?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=readme&quot;&gt;
      &lt;picture &gt;
        &lt;source width=&quot;275&quot; media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/user-attachments/assets/b8912ecb-c7a0-47bd-8507-29b12382fef6&quot;  &gt;
        &lt;source width=&quot;275&quot; media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/user-attachments/assets/f133dd08-04a3-4b20-b047-22f8f841cfbb&quot;  &gt;
        &lt;img alt=&quot;Try Agenta Live Demo&quot; src=&quot;https://github.com/Agenta-AI/agenta/assets/4510758/68e055d4-d7b8-4943-992f-761558c64253&quot; &gt;
      &lt;/picture&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;br/&gt;
  &lt;br /&gt;
      &lt;div align=&quot;center&quot; &gt;
        &lt;a href=&quot;https://cloud.agenta.ai?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=readme&quot;&gt;
          &lt;picture &gt;
            &lt;img width=&quot;800&quot; alt=&quot;Screenshot Agenta&quot; src=&quot;https://github.com/user-attachments/assets/32e95ddb-e001-4462-b92e-72bf4cc78597&quot; &gt;
          &lt;/picture&gt;
        &lt;/a&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;br /&gt;
&lt;br /&gt;

---

&lt;h3 align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://docs.agenta.ai?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=readme&quot;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; &amp;bull;
  &lt;a href=&quot;https://docs.agenta.ai/changelog/main?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=readme&quot;&gt;&lt;b&gt;Changelog&lt;/b&gt;&lt;/a&gt; &amp;bull;
  &lt;a href=&quot;https://agenta.ai?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=readme&quot;&gt;&lt;b&gt;Website&lt;/b&gt;&lt;/a&gt; &amp;bull;
  &lt;a href=&quot;https://cloud.agenta.ai?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=readme&quot;&gt;&lt;b&gt;Agenta Cloud&lt;/b&gt;&lt;/a&gt;

&lt;/h3&gt;

---

## What is Agenta?

Agenta is a platform for building production-grade LLM applications. It helps **engineering and product teams** create reliable LLM apps faster.


Agenta provides end-to-end tools for the entire LLMOps workflow:  building (**LLM playground**, **evaluation**), deploying (**prompt and configuration management**), and monitoring (**LLM observability and tracing**).

## Features
- **Prompt Playground**: Experiment, iterate on prompts, and compare outputs from over 50 LLM models side by side ([docs](https://docs.agenta.ai/prompt-management/using-the-playground?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=readme))
- **Custom Workflows**: Build a playground for any custom LLM workflow, such as RAG or agents. Enable all the team to easily iterate on its parameters and evaluate it from the web UI.
- **LLM evaluation**: Run evaluation suite from the webUI using predefined evaluators like LLM-as-a-judge, RAG evaluators, or custom code evaluators. ([docs](https://docs.agenta.ai/evaluation/overview?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=readme))
- **Human evaluation**: Collaborate with subject matter experts for human annotation evaluation, including A/B testing and annotating golden test sets.
- **Prompt Management**: Version your prompts and manage them across different environments ([docs](https://docs.agenta.ai/prompt-management/overview?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=readme), [quick start](https://docs.agenta.ai/prompt-management/quick-start?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=readme))
- **LLM Tracing**: Observe and debug your apps with integrations to most providers and frameworks ([docs](https://docs.agenta.ai/observability/overview?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=readme), [quick start](https://docs.agenta.ai/observability/quickstart?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=readme))
- **LLM Monitoring**: Track cost and latency and compare different deployments.

  
## Getting Started 
### Agenta Cloud:
The easiest way to get started is through Agenta Cloud. It is free to signup, and comes with a generous free-tier.

  &lt;a href=&quot;https://cloud.agenta.ai?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=readme&quot;&gt;
      &lt;picture &gt;
        &lt;source width=&quot;160&quot; media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/user-attachments/assets/759422d8-01bc-4503-bf3c-b5871c99359a&quot;  &gt;
        &lt;source width=&quot;160&quot; media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/user-attachments/assets/ffa9af5f-0981-4e95-9272-cb35eedb6780&quot;  &gt;
        &lt;img alt=&quot;Get Started with Agenta Cloud&quot; src=&quot;https://github.com/user-attachments/assets/ffa9af5f-0981-4e95-9272-cb35eedb6780&quot; &gt;
      &lt;/picture&gt;
  &lt;/a&gt;
  
   
### Self-hosting Agenta

1. Clone Agenta:
```bash
git clone https://github.com/Agenta-AI/agenta &amp;&amp; cd agenta
```

2. Edit `hosting/docker-compose/oss/.env.oss.gh` and add your LLM provider API keys.

3. Start Agenta services:
```bash
docker compose -f hosting/docker-compose/oss/docker-compose.gh.yml --env-file hosting/docker-compose/oss/.env.oss.gh --profile with-web up -d
```

4. Access Agenta at `http://localhost`.

For deploying on a remote host, or using different ports refers to our [self-hosting](https://docs.agenta.ai/self-host/host-locally?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=readme) and [remote deployment documentation](https://docs.agenta.ai/self-host/host-remotely?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=readme).

## Disabling Anonymized Tracking

By default, Agenta automatically reports anonymized basic usage statistics. This helps us understand how Agenta is used and track its overall usage and growth. This data does not include any sensitive information. To disable anonymized telemetry set `TELEMETRY_ENABLED` to `false` in your `.env` file.


## Contributing

We warmly welcome contributions to Agenta. Feel free to submit issues, fork the repository, and send pull requests.

We are usually hanging in our Slack. Feel free to [join our Slack and ask us anything](https://join.slack.com/t/agenta-hq/shared_invite/zt-2yewk6o2b-DmhyA4h_lkKwecDtIsj1AQ)

Check out our [Contributing Guide](https://docs.agenta.ai/misc/contributing/getting-started?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=readme) for more information.

### Contributors ✨

&lt;!-- ALL-CONTRIBUTORS-BADGE:START - Do not remove or modify this section --&gt;
[![All Contributors](https://img.shields.io/badge/all_contributors-49-orange.svg?style=flat-square)](#contributors-)
&lt;!-- ALL-CONTRIBUTORS-BADGE:END --&gt;

Thanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):

&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt;
&lt;!-- prettier-ignore-start --&gt;
&lt;!-- markdownlint-disable --&gt;
&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/SamMethnani&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/57623556?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Sameh Methnani&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Sameh Methnani&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=SamMethnani&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt; &lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=SamMethnani&quot; title=&quot;Documentation&quot;&gt;📖&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/suadsuljovic&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/8658374?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Suad Suljovic&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Suad Suljovic&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=suadsuljovic&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt; &lt;a href=&quot;#design-suadsuljovic&quot; title=&quot;Design&quot;&gt;🎨&lt;/a&gt; &lt;a href=&quot;#mentoring-suadsuljovic&quot; title=&quot;Mentoring&quot;&gt;🧑‍🏫&lt;/a&gt; &lt;a href=&quot;https://github.com/Agenta-AI/agenta/pulls?q=is%3Apr+reviewed-by%3Asuadsuljovic&quot; title=&quot;Reviewed Pull Requests&quot;&gt;👀&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/burtenshaw&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/19620375?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;burtenshaw&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;burtenshaw&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=burtenshaw&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;http://abram.tech&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/55067204?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Abram&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Abram&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=aybruhm&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt; &lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=aybruhm&quot; title=&quot;Documentation&quot;&gt;📖&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;http://israelabebe.com&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/7479824?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Israel Abebe&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Israel Abebe&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/issues?q=author%3Avernu&quot; title=&quot;Bug reports&quot;&gt;🐛&lt;/a&gt; &lt;a href=&quot;#design-vernu&quot; title=&quot;Design&quot;&gt;🎨&lt;/a&gt; &lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=vernu&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/SohaibAnwaar&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/29427728?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Master X&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Master X&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=SohaibAnwaar&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://main-portfolio-26wv6oglp-witehound.vercel.app/&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/26417477?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;corinthian&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;corinthian&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=witehound&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt; &lt;a href=&quot;#design-witehound&quot; title=&quot;Design&quot;&gt;🎨&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/Pajko97&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/25198892?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Pavle Janjusevic&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Pavle Janjusevic&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;#infra-Pajko97&quot; title=&quot;Infrastructure (Hosting, Build-Tools, etc)&quot;&gt;🚇&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;http://kaosiso-ezealigo.netlify.app&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/99529776?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Kaosi Ezealigo&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Kaosi Ezealigo&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/issues?q=author%3Abekossy&quot; title=&quot;Bug reports&quot;&gt;🐛&lt;/a&gt; &lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=bekossy&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/albnunes&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/46302915?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Alberto Nunes&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Alberto Nunes&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/issues?q=author%3Aalbnunes&quot; title=&quot;Bug reports&quot;&gt;🐛&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://www.linkedin.com/in/mohammed-maaz-6290b0116/&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/17180132?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Maaz Bin Khawar&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Maaz Bin Khawar&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=MohammedMaaz&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt; &lt;a href=&quot;https://github.com/Agenta-AI/agenta/pulls?q=is%3Apr+reviewed-by%3AMohammedMaaz&quot; title=&quot;Reviewed Pull Requests&quot;&gt;👀&lt;/a&gt; &lt;a href=&quot;#mentoring-MohammedMaaz&quot; title=&quot;Mentoring&quot;&gt;🧑‍🏫&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/devgenix&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/56418363?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Nehemiah Onyekachukwu Emmanuel&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Nehemiah Onyekachukwu Emmanuel&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=devgenix&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt; &lt;a href=&quot;#example-devgenix&quot; title=&quot;Examples&quot;&gt;💡&lt;/a&gt; &lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=devgenix&quot; title=&quot;Documentation&quot;&gt;📖&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/philipokiokio&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/55271518?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Philip Okiokio&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Philip Okiokio&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=philipokiokio&quot; title=&quot;Documentation&quot;&gt;📖&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://sweetdevil144.github.io/My-Website/&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/117591942?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Abhinav Pandey&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Abhinav Pandey&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=Sweetdevil144&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/RamchandraWarang9822&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/92023869?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Ramchandra Warang&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Ramchandra Warang&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=RamchandraWarang9822&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt; &lt;a href=&quot;https://github.com/Agenta-AI/agenta/issues?q=author%3ARamchandraWarang9822&quot; title=&quot;Bug reports&quot;&gt;🐛&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/lazyfuhrer&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/64888892?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Biswarghya Biswas&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Biswarghya Biswas&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=lazyfuhrer&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/okieLoki&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/96105929?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Uddeepta Raaj Kashyap&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Uddeepta Raaj Kashyap&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=okieLoki&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;http://www.linkedin.com/in/nayeem-abdullah-317098141&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/32274108?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Nayeem Abdullah&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Nayeem Abdullah&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=nayeem01&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/kangsuhyun-yanolja&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/124246127?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Kang Suhyun&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Kang Suhyun&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=kangsuhyun-yanolja&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/yeokyeong-yanolja&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/128676129?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Yoon&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Yoon&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=yeokyeong-yanolja&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://mrkirthi24.netlify.app/&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/53830546?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Kirthi Bagrecha Jain&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Kirthi Bagrecha Jain&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=mrkirthi-24&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/navdeep1840&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/80774259?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Navdeep&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Navdeep&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=navdeep1840&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://www.linkedin.com/in/rhythm-sharma-708a421a8/&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/64489317?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Rhythm Sharma&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Rhythm Sharma&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=Rhythm-08&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://osinachi.me&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/40396070?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Osinachi Chukwujama &quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Osinachi Chukwujama &lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=vicradon&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://liduos.com/&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/47264881?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;莫尔索&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;莫尔索&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=morsoli&quot; title=&quot;Documentation&quot;&gt;📖&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;http://luccithedev.com&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/22600781?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Agunbiade Adedeji&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Agunbiade Adedeji&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=dejongbaba&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://techemmy.github.io/&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/43725109?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Emmanuel Oloyede&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Emmanuel Oloyede&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br /&gt;&lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=techemmy&quot; title=&quot;Code&quot;&gt;💻&lt;/a&gt; &lt;a href=&quot;https://github.com/Agenta-AI/agenta/commits?author=techemmy&quot; title=&quot;Documentation&quot;&gt;📖&lt;/a&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot; valign=&quot;top&quot; width=&quot;14.28%&quot;&gt;&lt;a href=&quot;https://github.com/Dhaneshwarguiyan&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/116065351?v=4?s=100&quot; width=&quot;100px;&quot; alt=&quot;Dhaneshwarguiyan&quot;/&gt;&lt;br /&gt;&lt;sub&gt;&lt;b&gt;Dhaneshwarguiyan&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mindsdb/mindsdb]]></title>
            <link>https://github.com/mindsdb/mindsdb</link>
            <guid>https://github.com/mindsdb/mindsdb</guid>
            <pubDate>Sat, 29 Mar 2025 00:03:48 GMT</pubDate>
            <description><![CDATA[AI's query engine - Platform for building AI that can learn and answer questions over large scale federated data.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mindsdb/mindsdb">mindsdb/mindsdb</a></h1>
            <p>AI's query engine - Platform for building AI that can learn and answer questions over large scale federated data.</p>
            <p>Language: Python</p>
            <p>Stars: 27,509</p>
            <p>Forks: 4,937</p>
            <p>Stars today: 48 stars today</p>
            <h2>README</h2><pre>

&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
	&lt;a href=&quot;https://pypi.org/project/MindsDB/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/MindsDB.svg&quot; alt=&quot;MindsDB Release&quot;&gt;&lt;/a&gt;
	&lt;a href=&quot;https://www.python.org/downloads/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/python-3.9.x%7C%203.10.x%7C%203.11.x-brightgreen.svg&quot; alt=&quot;Python supported&quot;&gt;&lt;/a&gt;
	&lt;a href=&quot;https://ossrank.com/p/630&quot;&gt;&lt;img src=&quot;https://shields.io/endpoint?url=https://ossrank.com/shield/630&quot;&gt;&lt;/a&gt;
	&lt;img alt=&quot;PyPI - Downloads&quot; src=&quot;https://img.shields.io/pypi/dm/Mindsdb&quot;&gt;
	&lt;a href=&quot;https://hub.docker.com/u/mindsdb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/mindsdb/mindsdb&quot; alt=&quot;Docker pulls&quot;&gt;&lt;/a&gt;

  &lt;br /&gt;
  &lt;br /&gt;

  &lt;a href=&quot;https://github.com/mindsdb/mindsdb&quot;&gt;
    &lt;img src=&quot;/docs/assets/mindsdb_logo.png&quot; alt=&quot;MindsDB&quot; width=&quot;300&quot;&gt;
  &lt;/a&gt;

  &lt;p align=&quot;center&quot;&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://www.mindsdb.com?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Website&lt;/a&gt;
    ·
    &lt;a href=&quot;https://docs.mindsdb.com?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Docs&lt;/a&gt;
    ·
    &lt;a href=&quot;https://mdb.ai/register&quot;&gt;Demo&lt;/a&gt;
    ·
    &lt;a href=&quot;https://mindsdb.com/joincommunity?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Community Slack&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

----------------------------------------


MindsDB is the world&#039;s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;/docs/assets/cloud/main_mdb.png&quot;/&gt;
&lt;/p&gt;

A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. 

## Minds [Demo](https://mdb.ai/register)
Play with [Minds demo](https://mdb.ai/register), and see the power of MindsDB at answering questions from structured to unstructured data, whether it&#039;s scattered across SaaS applications, databases, or... hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered.
 
## Install MindsDB Server 

MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart&#039;s content.

  * [Using Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop). This is the fastest and recommended way to get started and have it all running.
  * [Using Docker](https://docs.mindsdb.com/setup/self-hosted/docker). This is also simple, but gives you more flexibility on how to further customize your server.
  * [Using PyPI](https://docs.mindsdb.com/contribute/install). This option enables you to contribute to MindsDB.

## Connect Your Data

You can connect to hundreds of [data sources (learn more)](https://docs.mindsdb.com/integrations/data-overview). This is just an example of a Postgres database.

```sql
-- Connect to demo postgres DB
CREATE DATABASE demo_postgres_db
WITH ENGINE = &quot;postgres&quot;,
PARAMETERS = {
  &quot;user&quot;: &quot;demo_user&quot;,
  &quot;password&quot;: &quot;demo_password&quot;,
  &quot;host&quot;: &quot;samples.mindsdb.com&quot;,
  &quot;port&quot;: &quot;5432&quot;,
  &quot;database&quot;: &quot;demo&quot;,
  &quot;schema&quot;: &quot;demo_data&quot;
};
```

Once you&#039;ve connected your data sources, you can [combine](https://docs.mindsdb.com/mindsdb_sql/sql/api/join-on), [slice it, dice it](https://docs.mindsdb.com/mindsdb_sql/sql/api/select), and [transform](https://docs.mindsdb.com/use-cases/data_enrichment/overview) it however your heart desires using good ol&#039; standard SQL [(learn more)](https://docs.mindsdb.com/mindsdb_sql/overview). 

After you&#039;ve whipped your data into shape, it&#039;s time to build AI that actually learns!

## Build AI Knowledge

Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager&#039;s bedroom, our Knowledge Base engine will figure out how to find the relevant information. 

**In this example** we will create a knowledge base that knows everything about amazon reviews. 

```sql
-- first create a knowledge base
CREATE KNOWLEDGE_BASE mindsdb.reviews_kb;

-- now insert everything from the amazon reviews table into it, so it can learn it
INSERT INTO mindsdb.reviews_kb (
  SELECT review as content FROM demo_pg_db.amazon_reviews
);

-- check the status of your loads here
SELECT * FROM information_schema.knowledge_bases;

-- query the content of the knowledge base
SELECT * FROM mindsdb.reviews_kb;
```

For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. [(Learn more about knowledge Bases)](https://docs.mindsdb.com/mindsdb_sql/agents/knowledge-bases)

+ Want to [hand-pick your embedding model? Go for it](https://docs.mindsdb.com/mindsdb_sql/agents/knowledge-bases#knowledge-base-with-openai-embedding-model)! 
+ Have strong [opinions about vector databases? We&#039;re here for it!](https://docs.mindsdb.com/mindsdb_sql/agents/knowledge-bases#knowledge-base-with-custom-vector-store). 

But if you&#039;d rather spend your time on other things (like finally building that billion-dollar AI App), that&#039;s perfectly fine too. By default, it&#039;s all handled automatically - you don&#039;t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc.

## Search 

Now that your knowledge base is loaded and ready. Let&#039;s hunt for some juicy info!

#### Via SQL

```sql
-- Find the reviews that about Iphone in beast of lights
SELECT *  FROM mindsdb.reviews_kb
WHERE content LIKE &#039;what are the best kindle reviews&#039;
LIMIT 10;
```

#### Via Python SDK

Install MindsDB SDK

```shell
pip install mindsdb_sdk
```

You can call this AI knowledge base from your app with the following code:

```python
import mindsdb_sdk


# connects to the specified host and port
server = mindsdb_sdk.connect(&#039;http://127.0.0.1:47334&#039;)

wiki_kb = server.knowledge_bases.get(&#039;mindsdb.reviews_kb&#039;);
df = my_kb.find(&#039;what are the best kindle reviews&#039;).fetch()

```

## 🤝 Contribute

Interested in contributing to MindsDB? Follow our [installation guide for development](https://docs.mindsdb.com/contribute/install?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

You can find our [contribution guide here](https://docs.mindsdb.com/contribute/contribute?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

We welcome suggestions! Feel free to open new issues with your ideas, and we’ll guide you.

This project adheres to a [Contributor Code of Conduct](https://github.com/mindsdb/mindsdb/blob/main/CODE_OF_CONDUCT.md). By participating, you agree to follow its terms.

Also, check out our [community rewards and programs](https://mindsdb.com/community?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

## 🤍 Support

If you find a bug, please submit an [issue on GitHub](https://github.com/mindsdb/mindsdb/issues/new/choose).

Here’s how you can get community support:

* Ask a question in our [Slack Community](https://mindsdb.com/joincommunity?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).
* Join our [GitHub Discussions](https://github.com/mindsdb/mindsdb/discussions).
* Post on [Stack Overflow](https://stackoverflow.com/questions/tagged/mindsdb) with the MindsDB tag.

For commercial support, please [contact the MindsDB team](https://mindsdb.com/contact?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

## 💚 Current Contributors

&lt;a href=&quot;https://github.com/mindsdb/mindsdb/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contributors-img.web.app/image?repo=mindsdb/mindsdb&quot; /&gt;
&lt;/a&gt;

Generated with [contributors-img](https://contributors-img.web.app).

## 🔔 Subscribe for Updates

Join our [Slack community](https://mindsdb.com/joincommunity)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[agno-agi/agno]]></title>
            <link>https://github.com/agno-agi/agno</link>
            <guid>https://github.com/agno-agi/agno</guid>
            <pubDate>Sat, 29 Mar 2025 00:03:47 GMT</pubDate>
            <description><![CDATA[Agno is a lightweight library for building Multimodal Agents. Use it to give LLMs superpowers like memory, knowledge, tools and reasoning.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/agno-agi/agno">agno-agi/agno</a></h1>
            <p>Agno is a lightweight library for building Multimodal Agents. Use it to give LLMs superpowers like memory, knowledge, tools and reasoning.</p>
            <p>Language: Python</p>
            <p>Stars: 22,857</p>
            <p>Forks: 2,964</p>
            <p>Stars today: 351 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; id=&quot;top&quot;&gt;
  &lt;a href=&quot;https://docs.agno.com&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-dark.svg&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-light.svg&quot;&gt;
      &lt;img src=&quot;https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-light.svg&quot; alt=&quot;Agno&quot;&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://docs.agno.com&quot;&gt;📚 Documentation&lt;/a&gt; &amp;nbsp;|&amp;nbsp;
  &lt;a href=&quot;https://docs.agno.com/examples/introduction&quot;&gt;💡 Examples&lt;/a&gt; &amp;nbsp;|&amp;nbsp;
  &lt;a href=&quot;https://github.com/agno-agi/agno/stargazers&quot;&gt;🌟 Star Us&lt;/a&gt;
&lt;/div&gt;

## Introduction

[Agno](https://docs.agno.com) is a lightweight library for building Multimodal Agents. It exposes LLMs as a unified API and gives them superpowers like memory, knowledge, tools and reasoning.

- Build lightning-fast Agents that can generate text, image, audio and video.
- Add memory, knowledge, tools and reasoning as needed.
- Run anywhere, Agno is open-source.

Here&#039;s an Agent that can search the web:

```python websearch_agent.py
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    tools=[DuckDuckGoTools()],
    markdown=True
)
agent.print_response(&quot;What&#039;s happening in New York?&quot;, stream=True)
```

## 🚨 Open Source AI Agent Hackathon! 🚨

We&#039;re launching a Global AI Agent Hackathon in collaboration with AI Agent ecosystem partners — open to all developers, builders, and startups working on agents, RAG, tool use, or multi-agent systems.

### 💰 Win up to $20,000 in cash by building Agents

- 🏅 10 winners: $300 each
- 🥉 10 winners: $500 each
- 🥈 5 winners: $1,000 each
- 🥇 1 winner: $2,000
- 🏆 GRAND PRIZE: $5,000 🏆

### 🎁 Bonus
- Top 5 projects will be featured in the top trending [Awesome LLM Apps](https://github.com/Shubhamsaboo/awesome-llm-apps) repo.

### 🤝 Partners

[Agno](https://www.agno.com), [Unwind AI](https://www.theunwindai.com) and more Agent ecosystem companies joining soon.

### 📅 Here&#039;s the timeline:

- April 3rd - Final dates revealed
- April 10th - Prize and success criteria announced
- April 15th (tentative) - Hackathon starts
- May 30th (tentative) - Hackathon ends

Join us for a month of building Agents!

&gt; Prizes will be distributed on an ongoing basis and continue till all prizes are awarded.

⭐ Star this repo and follow along to stay updated.

### 🤝 Want to join us as a partner or judge?

If you&#039;re a company in the AI agent ecosystem or would like to judge the hackathon, reach out to [Shubham Saboo](https://x.com/Saboo_Shubham_) or [Ashpreet Bedi](https://x.com/ashpreetbedi) on X to partner. Let’s make this the biggest open source AI Agent hackathon.

## Key features

Agno is simple, fast and model agnostic. Here are some key features:

- **Lightning Fast**: Agent creation is 10,000x faster than LangGraph (see [performance](#performance)).
- **Model Agnostic**: Use any model, any provider, no lock-in.
- **Multi Modal**: Native support for text, image, audio and video.
- **Multi Agent**: Build teams of specialized agents.
- **Memory Management**: Store agent sessions and state in a database.
- **Knowledge Stores**: Use vector databases for RAG or dynamic few-shot learning.
- **Structured Outputs**: Make Agents respond in a structured format.
- **Monitoring**: Track agent sessions and performance in real-time on [agno.com](https://app.agno.com).

## Getting Started

- Start by [building your first Agent](https://docs.agno.com/introduction/agents)
- Check out the [examples](https://docs.agno.com/examples/introduction)
- Read the [documentation](https://docs.agno.com)

## Installation

```shell
pip install -U agno
```

## What are Agents?

**Agents** are intelligent programs that solve problems autonomously.

Agents have memory, domain knowledge and the ability to use tools (like searching the web, querying a database, making API calls). Unlike traditional programs that follow a predefined execution path, Agents dynamically adapt their approach based on the context and tool results.

Instead of a rigid binary definition, let&#039;s think of Agents in terms of agency and autonomy.
- **Level 0**: Agents with no tools (basic inference tasks).
- **Level 1**: Agents with tools for autonomous task execution.
- **Level 2**: Agents with knowledge, combining memory and reasoning.
- **Level 3**: Teams of specialized agents collaborating on complex workflows.

## Example - Basic Agent

The simplest Agent is just an inference task, no tools, no memory, no knowledge.

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat

agent = Agent(
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    description=&quot;You are an enthusiastic news reporter with a flair for storytelling!&quot;,
    markdown=True
)
agent.print_response(&quot;Tell me about a breaking news story from New York.&quot;, stream=True)
```

To run the agent, install dependencies and export your `OPENAI_API_KEY`.

```shell
pip install agno openai

export OPENAI_API_KEY=sk-xxxx

python basic_agent.py
```

[View this example in the cookbook](./cookbook/getting_started/01_basic_agent.py)

## Example - Agent with tools

This basic agent will obviously make up a story, lets give it a tool to search the web.

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    description=&quot;You are an enthusiastic news reporter with a flair for storytelling!&quot;,
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True
)
agent.print_response(&quot;Tell me about a breaking news story from New York.&quot;, stream=True)
```

Install dependencies and run the Agent:

```shell
pip install duckduckgo-search

python agent_with_tools.py
```

Now you should see a much more relevant result.

[View this example in the cookbook](./cookbook/getting_started/02_agent_with_tools.py)

## Example - Agent with knowledge

Agents can store knowledge in a vector database and use it for RAG or dynamic few-shot learning.

**Agno agents use Agentic RAG** by default, which means they will search their knowledge base for the specific information they need to achieve their task.

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.embedder.openai import OpenAIEmbedder
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.vectordb.lancedb import LanceDb, SearchType

agent = Agent(
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    description=&quot;You are a Thai cuisine expert!&quot;,
    instructions=[
        &quot;Search your knowledge base for Thai recipes.&quot;,
        &quot;If the question is better suited for the web, search the web to fill in gaps.&quot;,
        &quot;Prefer the information in your knowledge base over the web results.&quot;
    ],
    knowledge=PDFUrlKnowledgeBase(
        urls=[&quot;https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf&quot;],
        vector_db=LanceDb(
            uri=&quot;tmp/lancedb&quot;,
            table_name=&quot;recipes&quot;,
            search_type=SearchType.hybrid,
            embedder=OpenAIEmbedder(id=&quot;text-embedding-3-small&quot;),
        ),
    ),
    tools=[DuckDuckGoTools()],
    show_tool_calls=True,
    markdown=True
)

# Comment out after the knowledge base is loaded
if agent.knowledge is not None:
    agent.knowledge.load()

agent.print_response(&quot;How do I make chicken and galangal in coconut milk soup&quot;, stream=True)
agent.print_response(&quot;What is the history of Thai curry?&quot;, stream=True)
```

Install dependencies and run the Agent:

```shell
pip install lancedb tantivy pypdf duckduckgo-search

python agent_with_knowledge.py
```

[View this example in the cookbook](./cookbook/getting_started/03_agent_with_knowledge.py)

## Example - Multi Agent Teams

Agents work best when they have a singular purpose, a narrow scope and a small number of tools. When the number of tools grows beyond what the language model can handle or the tools belong to different categories, use a team of agents to spread the load.

```python
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools
from agno.team import Team

web_agent = Agent(
    name=&quot;Web Agent&quot;,
    role=&quot;Search the web for information&quot;,
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    tools=[DuckDuckGoTools()],
    instructions=&quot;Always include sources&quot;,
    show_tool_calls=True,
    markdown=True,
)

finance_agent = Agent(
    name=&quot;Finance Agent&quot;,
    role=&quot;Get financial data&quot;,
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    tools=[YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)],
    instructions=&quot;Use tables to display data&quot;,
    show_tool_calls=True,
    markdown=True,
)

agent_team = Team(
    mode=&quot;coordinate&quot;,
    members=[web_agent, finance_agent],
    model=OpenAIChat(id=&quot;gpt-4o&quot;),
    success_criteria=&quot;A comprehensive financial news report with clear sections and data-driven insights.&quot;,
    instructions=[&quot;Always include sources&quot;, &quot;Use tables to display data&quot;],
    show_tool_calls=True,
    markdown=True,
)

agent_team.print_response(&quot;What&#039;s the market outlook and financial performance of AI semiconductor companies?&quot;, stream=True)
```

Install dependencies and run the Agent team:

```shell
pip install duckduckgo-search yfinance

python agent_team.py
```

[View this example in the cookbook](./cookbook/getting_started/05_agent_team.py)

## Performance

At Agno, we&#039;re obsessed with performance. Why? because even simple AI workflows can spawn thousands of Agents to achieve their goals. Scale that to a modest number of users and performance becomes a bottleneck. Agno is designed to power high performance agentic systems:

- Agent instantiation: ~2μs on average (~10,000x faster than LangGraph).
- Memory footprint: ~3.75Kib on average (~50x less memory than LangGraph).

&gt; Tested on an Apple M4 Mackbook Pro.

While an Agent&#039;s run-time is bottlenecked by inference, we must do everything possible to minimize execution time, reduce memory usage, and parallelize tool calls. These numbers may seem trivial at first, but our experience shows that they add up even at a reasonably small scale.

### Instantiation time

Let&#039;s measure the time it takes for an Agent with 1 tool to start up. We&#039;ll run the evaluation 1000 times to get a baseline measurement.

You should run the evaluation yourself on your own machine, please, do not take these results at face value.

```shell
# Setup virtual environment
./scripts/perf_setup.sh
source .venvs/perfenv/bin/activate
# OR Install dependencies manually
# pip install openai agno langgraph langchain_openai

# Agno
python evals/performance/instantiation_with_tool.py

# LangGraph
python evals/performance/other/langgraph_instantiation.py
```

&gt; The following evaluation is run on an Apple M4 Mackbook Pro. It also runs as a Github action on this repo.

LangGraph is on the right, **let&#039;s start it first and give it a head start**.

Agno is on the left, notice how it finishes before LangGraph gets 1/2 way through the runtime measurement, and hasn&#039;t even started the memory measurement. That&#039;s how fast Agno is.

https://github.com/user-attachments/assets/ba466d45-75dd-45ac-917b-0a56c5742e23

Dividing the average time of a Langgraph Agent by the average time of an Agno Agent:

```
0.020526s / 0.000002s ~ 10,263
```

In this particular run, **Agno Agents startup is roughly 10,000 times faster than Langgraph Agents**. The numbers continue to favor Agno as the number of tools grow, and we add memory and knowledge stores.

### Memory usage

To measure memory usage, we use the `tracemalloc` library. We first calculate a baseline memory usage by running an empty function, then run the Agent 1000x times and calculate the difference. This gives a (reasonably) isolated measurement of the memory usage of the Agent.

We recommend running the evaluation yourself on your own machine, and digging into the code to see how it works. If we&#039;ve made a mistake, please let us know.

Dividing the average memory usage of a Langgraph Agent by the average memory usage of an Agno Agent:

```
0.137273/0.002528 ~ 54.3
```

**Langgraph Agents use ~50x more memory than Agno Agents**. In our opinion, memory usage is a much more important metric than instantiation time. As we start running thousands of Agents in production, these numbers directly start affecting the cost of running the Agents.

### Conclusion

Agno agents are designed for performance and while we do share some benchmarks against other frameworks, we should be mindful that accuracy and reliability are more important than speed.

We&#039;ll be publishing accuracy and reliability benchmarks running on Github actions in the coming weeks. Given that each framework is different and we won&#039;t be able to tune their performance like we do with Agno, for future benchmarks we&#039;ll only be comparing against ourselves.

## Cursor Setup

When building Agno agents, using Agno documentation as a source in Cursor is a great way to speed up your development.

1. In Cursor, go to the settings or preferences section.
2. Find the section to manage documentation sources.
3. Add `https://docs.agno.com` to the list of documentation URLs.
4. Save the changes.

Now, Cursor will have access to the Agno documentation.

## Documentation, Community &amp; More examples

- Docs: &lt;a href=&quot;https://docs.agno.com&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;docs.agno.com&lt;/a&gt;
- Getting Started Examples: &lt;a href=&quot;https://github.com/agno-agi/agno/tree/main/cookbook/getting_started&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Getting Started Cookbook&lt;/a&gt;
- All Examples: &lt;a href=&quot;https://github.com/agno-agi/agno/tree/main/cookbook&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Cookbook&lt;/a&gt;
- Community forum: &lt;a href=&quot;https://community.agno.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;community.agno.com&lt;/a&gt;
- Chat: &lt;a href=&quot;https://discord.gg/4MtYHHrgA8&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;discord&lt;/a&gt;

## Contributions

We welcome contributions, read our [contributing guide](https://github.com/agno-agi/agno/blob/main/CONTRIBUTING.md) to get started.

## Telemetry

Agno logs which model an agent used so we can prioritize updates to the most popular providers. You can disable this by setting `AGNO_TELEMETRY=false` in your environment.

&lt;p align=&quot;left&quot;&gt;
  &lt;a href=&quot;#top&quot;&gt;⬆️ Back to Top&lt;/a&gt;
&lt;/p&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Akkudoktor-EOS/EOS]]></title>
            <link>https://github.com/Akkudoktor-EOS/EOS</link>
            <guid>https://github.com/Akkudoktor-EOS/EOS</guid>
            <pubDate>Sat, 29 Mar 2025 00:03:46 GMT</pubDate>
            <description><![CDATA[This repository features an Energy Optimization System (EOS) that optimizes energy distribution, usage for batteries, heat pumps& household devices. It includes predictive models for electricity prices (planned), load forecasting& dynamic optimization to maximize energy efficiency & minimize costs. Founder Dr. Andreas Schmitz (YouTube @akkudoktor)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Akkudoktor-EOS/EOS">Akkudoktor-EOS/EOS</a></h1>
            <p>This repository features an Energy Optimization System (EOS) that optimizes energy distribution, usage for batteries, heat pumps& household devices. It includes predictive models for electricity prices (planned), load forecasting& dynamic optimization to maximize energy efficiency & minimize costs. Founder Dr. Andreas Schmitz (YouTube @akkudoktor)</p>
            <p>Language: Python</p>
            <p>Stars: 933</p>
            <p>Forks: 78</p>
            <p>Stars today: 77 stars today</p>
            <h2>README</h2><pre># Energy System Simulation and Optimization

This project provides a comprehensive solution for simulating and optimizing an energy system based on renewable energy sources. With a focus on photovoltaic (PV) systems, battery storage (batteries), load management (consumer requirements), heat pumps, electric vehicles, and consideration of electricity price data, this system enables forecasting and optimization of energy flow and costs over a specified period.

Documentation can be found at [Akkudoktor-EOS](https://akkudoktor-eos.readthedocs.io/en/latest/).

## Getting Involved

See [CONTRIBUTING.md](CONTRIBUTING.md).

## System requirements

- Python &gt;= 3.11, &lt; 3.13
- Architecture: amd64, aarch64 (armv8)
- OS: Linux, Windows, macOS

Note: For Python 3.13 some dependencies (e.g. [Pendulum](https://github.com/python-pendulum/Pendulum)) are not yet available on https://pypi.org and have to be manually compiled (a recent [Rust](https://www.rust-lang.org/tools/install) installation is required).

Other architectures (e.g. armv6, armv7) are unsupported for now, because a multitude of dependencies are not available on https://piwheels.org and have to be built manually (a recent Rust installation and [GCC](https://gcc.gnu.org/) are required, Python 3.11 is recommended).

## Installation

Docker images (amd64/aarch64) can be found at [akkudoktor/eos](https://hub.docker.com/r/akkudoktor/eos).

Following sections describe how to locally start the EOS server on `http://localhost:8503`.

### Run from source

Install dependencies in virtual environment:

Linux:

```bash
python -m venv .venv
.venv/bin/pip install -r requirements.txt
.venv/bin/pip install -e .
```

Windows:

```cmd
python -m venv .venv
 .venv\Scripts\pip install -r requirements.txt
 .venv\Scripts\pip install -e .
```

Finally, start the EOS server to access it at `http://localhost:8503` (API docs at `http://localhost:8503/docs`):

Linux:

```bash
.venv/bin/python src/akkudoktoreos/server/eos.py
```

Windows:

```cmd
.venv\Scripts\python src/akkudoktoreos/server/eos.py
```

### Docker

Start EOS with following command to access it at `http://localhost:8503` (API docs at `http://localhost:8503/docs`):

```bash
docker compose up
```

## Configuration

This project uses the `EOS.config.json` file to manage configuration settings.

### Default Configuration

A default configuration file `default.config.json` is provided. This file contains all the necessary configuration keys with their default values.

### Custom Configuration

Users can specify a custom configuration directory by setting the environment variable `EOS_DIR`.

- If the directory specified by `EOS_DIR` contains an existing `config.json` file, the application will use this configuration file.
- If the `EOS.config.json` file does not exist in the specified directory, the `default.config.json` file will be copied to the directory as `EOS.config.json`.

### Configuration Updates

If the configuration keys in the `EOS.config.json` file are missing or different from those in `default.config.json`, they will be automatically updated to match the default settings, ensuring that all required keys are present.

## Classes and Functionalities

This project uses various classes to simulate and optimize the components of an energy system. Each class represents a specific aspect of the system, as described below:

- `Battery`: Simulates a battery storage system, including capacity, state of charge, and now charge and discharge losses.

- `PVForecast`: Provides forecast data for photovoltaic generation, based on weather data and historical generation data.

- `Load`: Models the load requirements of a household or business, enabling the prediction of future energy demand.

- `Heatpump`: Simulates a heat pump, including its energy consumption and efficiency under various operating conditions.

- `Strompreis`: Provides information on electricity prices, enabling optimization of energy consumption and generation based on tariff information.

- `EMS`: The Energy Management System (EMS) coordinates the interaction between the various components, performs optimization, and simulates the operation of the entire energy system.

These classes work together to enable a detailed simulation and optimization of the energy system. For each class, specific parameters and settings can be adjusted to test different scenarios and strategies.

### Customization and Extension

Each class is designed to be easily customized and extended to integrate additional functions or improvements. For example, new methods can be added for more accurate modeling of PV system or battery behavior. Developers are invited to modify and extend the system according to their needs.

## Server API

See the Swagger API documentation for detailed information: [EOS OpenAPI Spec](https://petstore3.swagger.io/?url=https://raw.githubusercontent.com/Akkudoktor-EOS/EOS/refs/heads/main/openapi.json)

## Further resources

- [Installation guide (de)](https://meintechblog.de/2024/09/05/andreas-schmitz-joerg-installiert-mein-energieoptimierungssystem/)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>