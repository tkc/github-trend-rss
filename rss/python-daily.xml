<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sun, 28 Sep 2025 00:04:35 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[onyx-dot-app/onyx]]></title>
            <link>https://github.com/onyx-dot-app/onyx</link>
            <guid>https://github.com/onyx-dot-app/onyx</guid>
            <pubDate>Sun, 28 Sep 2025 00:04:35 GMT</pubDate>
            <description><![CDATA[Open Source AI Platform - AI Chat with advanced features that works with every LLM]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/onyx-dot-app/onyx">onyx-dot-app/onyx</a></h1>
            <p>Open Source AI Platform - AI Chat with advanced features that works with every LLM</p>
            <p>Language: Python</p>
            <p>Stars: 14,351</p>
            <p>Forks: 1,954</p>
            <p>Stars today: 224 stars today</p>
            <h2>README</h2><pre>&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;h2 align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://www.onyx.app/&quot;&gt; &lt;img width=&quot;50%&quot; src=&quot;https://github.com/onyx-dot-app/onyx/blob/logo/OnyxLogoCropped.jpg?raw=true)&quot; /&gt;&lt;/a&gt;
&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;Open Source AI Platform&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://discord.gg/TDJ59cGV2X&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/discord-join-blue.svg?logo=discord&amp;logoColor=white&quot; alt=&quot;Discord&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://docs.onyx.app/&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/docs-view-blue&quot; alt=&quot;Documentation&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://docs.onyx.app/&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/website?url=https://www.onyx.app&amp;up_message=visit&amp;up_color=blue&quot; alt=&quot;Documentation&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/onyx-dot-app/onyx/blob/main/LICENSE&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/static/v1?label=license&amp;message=MIT&amp;color=blue&quot; alt=&quot;License&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;



**[Onyx](https://www.onyx.app/)** is a feature-rich, self-hostable Chat UI that works with any LLM. It is easy to deploy and can run in a completely airgapped environment.

Onyx comes loaded with advanced features like Agents, Web Search, RAG, MCP, Deep Research, Connectors to 40+ knowledge sources, and more.

&gt; [!TIP]
&gt; Run Onyx with one command (or see deployment section below):
&gt; ```
&gt; curl -fsSL https://raw.githubusercontent.com/onyx-dot-app/onyx/main/deployment/docker_compose/install.sh &gt; install.sh &amp;&amp; chmod +x install.sh &amp;&amp; ./install.sh
&gt; ```

****

![Onyx Chat Silent Demo](https://github.com/onyx-dot-app/onyx/releases/download/v0.21.1/OnyxChatSilentDemo.gif)



## ‚≠ê Features
- **ü§ñ Custom Agents:** Build AI Agents with unique instructions, knowledge and actions.
- **üåç Web Search:** Browse the web with Google PSE, Exa, and Serper as well as an in-house scraper or Firecrawl.
- **üîç RAG:** Best in class hybrid-search + knowledge graph for uploaded files and ingested documents from connectors. 
- **üîÑ Connectors:** Pull knowledge, metadata, and access information from over 40 applications.
- **üî¨ Deep Research:** Get in depth answers with an agentic multi-step search.
- **‚ñ∂Ô∏è Actions &amp; MCP:** Give AI Agents the ability to interact with external systems.
- **üíª Code Interpreter:** Execute code to analyze data, render graphs and create files.
- **üé® Image Generation:** Generate images based on user prompts.
- **üë• Collaboration:** Chat sharing, feedback gathering, user management, usage analytics, and more.

Onyx works with all LLMs (like OpenAI, Anthropic, Gemini, etc.) and self-hosted LLMs (like Ollama, vLLM, etc.)

To learn more about the features, check out our [documentation](https://docs.onyx.app/welcome)!



## üöÄ Deployment
Onyx supports deployments in Docker, Kubernetes, Terraform, along with guides for major cloud providers.

See guides below:
- [Docker](https://docs.onyx.app/deployment/local/docker) or [Quickstart](https://docs.onyx.app/deployment/getting_started/quickstart) (best for most users)
- [Kubernetes](https://docs.onyx.app/deployment/local/kubernetes) (best for large teams)
- [Terraform](https://docs.onyx.app/deployment/local/terraform) (best for teams already using Terraform)
- Cloud specific guides (best if specifically using [AWS EKS](https://docs.onyx.app/deployment/cloud/aws/eks), [Azure VMs](https://docs.onyx.app/deployment/cloud/azure), etc.)

&gt; [!TIP]  
&gt; **To try Onyx for free without deploying, check out [Onyx Cloud](https://cloud.onyx.app/signup)**.



## üîç Other Notable Benefits
Onyx is built for teams of all sizes, from individual users to the largest global enterprises.

- **Enterprise Search**: far more than simple RAG, Onyx has custom indexing and retrieval that remains performant and accurate for scales of up to tens of millions of documents.
- **Security**: SSO (OIDC/SAML/OAuth2), RBAC, encryption of credentials, etc.
- **Management UI**: different user roles such as basic, curator, and admin.
- **Document Permissioning**: mirrors user access from external apps for RAG use cases.



## üöß Roadmap
To see ongoing and upcoming projects, check out our [roadmap](https://github.com/orgs/onyx-dot-app/projects/2)!



## üìö Licensing
There are two editions of Onyx:

- Onyx Community Edition (CE) is available freely under the MIT license.
- Onyx Enterprise Edition (EE) includes extra features that are primarily useful for larger organizations.
For feature details, check out [our website](https://www.onyx.app/pricing).



## üë™ Community
Join our open source community on **[Discord](https://discord.gg/TDJ59cGV2X)**!



## üí° Contributing
Looking to contribute? Please check out the [Contribution Guide](CONTRIBUTING.md) for more details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[HKUDS/RAG-Anything]]></title>
            <link>https://github.com/HKUDS/RAG-Anything</link>
            <guid>https://github.com/HKUDS/RAG-Anything</guid>
            <pubDate>Sun, 28 Sep 2025 00:04:34 GMT</pubDate>
            <description><![CDATA["RAG-Anything: All-in-One RAG Framework"]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/HKUDS/RAG-Anything">HKUDS/RAG-Anything</a></h1>
            <p>"RAG-Anything: All-in-One RAG Framework"</p>
            <p>Language: Python</p>
            <p>Stars: 6,866</p>
            <p>Forks: 776</p>
            <p>Stars today: 415 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;div style=&quot;margin: 20px 0;&quot;&gt;
  &lt;img src=&quot;./assets/logo.png&quot; width=&quot;120&quot; height=&quot;120&quot; alt=&quot;RAG-Anything Logo&quot; style=&quot;border-radius: 20px; box-shadow: 0 8px 32px rgba(0, 217, 255, 0.3);&quot;&gt;
&lt;/div&gt;

# üöÄ RAG-Anything: All-in-One RAG Framework

&lt;a href=&quot;https://trendshift.io/repositories/14959&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14959&quot; alt=&quot;HKUDS%2FRAG-Anything | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://readme-typing-svg.herokuapp.com?font=Orbitron&amp;size=24&amp;duration=3000&amp;pause=1000&amp;color=00D9FF&amp;center=true&amp;vCenter=true&amp;width=600&amp;lines=Welcome+to+RAG-Anything;Next-Gen+Multimodal+RAG+System;Powered+by+Advanced+AI+Technology&quot; alt=&quot;Typing Animation&quot; /&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;div style=&quot;background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; padding: 25px; text-align: center;&quot;&gt;
    &lt;p&gt;
      &lt;a href=&#039;https://github.com/HKUDS/RAG-Anything&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/üî•Project-Page-00d9ff?style=for-the-badge&amp;logo=github&amp;logoColor=white&amp;labelColor=1a1a2e&#039;&gt;&lt;/a&gt;
      &lt;a href=&#039;https://arxiv.org/abs/2410.05779&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/üìÑarXiv-2410.05779-ff6b6b?style=for-the-badge&amp;logo=arxiv&amp;logoColor=white&amp;labelColor=1a1a2e&#039;&gt;&lt;/a&gt;
      &lt;a href=&#039;https://github.com/HKUDS/LightRAG&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/‚ö°Based%20on-LightRAG-4ecdc4?style=for-the-badge&amp;logo=lightning&amp;logoColor=white&amp;labelColor=1a1a2e&#039;&gt;&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://github.com/HKUDS/RAG-Anything/stargazers&quot;&gt;&lt;img src=&#039;https://img.shields.io/github/stars/HKUDS/RAG-Anything?color=00d9ff&amp;style=for-the-badge&amp;logo=star&amp;logoColor=white&amp;labelColor=1a1a2e&#039; /&gt;&lt;/a&gt;
      &lt;img src=&quot;https://img.shields.io/badge/üêçPython-3.10-4ecdc4?style=for-the-badge&amp;logo=python&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;
      &lt;a href=&quot;https://pypi.org/project/raganything/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/raganything.svg?style=for-the-badge&amp;logo=pypi&amp;logoColor=white&amp;labelColor=1a1a2e&amp;color=ff6b6b&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://github.com/astral-sh/uv&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/‚ö°uv-Ready-ff6b6b?style=for-the-badge&amp;logo=python&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;a href=&quot;https://discord.gg/yF2MmDJyGJ&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/üí¨Discord-Community-7289da?style=for-the-badge&amp;logo=discord&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://github.com/HKUDS/RAG-Anything/issues/7&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/üí¨WeChat-Group-07c160?style=for-the-badge&amp;logo=wechat&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;a href=&quot;README_zh.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/üá®üá≥‰∏≠ÊñáÁâà-1a1a2e?style=for-the-badge&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;README.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/üá∫üá∏English-1a1a2e?style=for-the-badge&quot;&gt;&lt;/a&gt;
    &lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;div style=&quot;width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);&quot;&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;#-quick-start&quot; style=&quot;text-decoration: none;&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Quick%20Start-Get%20Started%20Now-00d9ff?style=for-the-badge&amp;logo=rocket&amp;logoColor=white&amp;labelColor=1a1a2e&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

---

## üéâ News
- [X] [2025.08.12]üéØüì¢ üîç RAG-Anything now features **VLM-Enhanced Query** mode! When documents include images, the system seamlessly integrates them into VLM for advanced multimodal analysis, combining visual and textual context for deeper insights.
- [X] [2025.07.05]üéØüì¢ RAG-Anything now features a [context configuration module](docs/context_aware_processing.md), enabling intelligent integration of relevant contextual information to enhance multimodal content processing.
- [X] [2025.07.04]üéØüì¢ üöÄ RAG-Anything now supports multimodal query capabilities, enabling enhanced RAG with seamless processing of text, images, tables, and equations.
- [X] [2025.07.03]üéØüì¢ üéâ RAG-Anything has reached 1küåü stars on GitHub! Thank you for your incredible support and valuable contributions to the project.

---

## üåü System Overview

*Next-Generation Multimodal Intelligence*

&lt;div style=&quot;background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%); border-radius: 15px; padding: 25px; margin: 20px 0; border: 2px solid #00d9ff; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);&quot;&gt;

Modern documents increasingly contain diverse multimodal content‚Äîtext, images, tables, equations, charts, and multimedia‚Äîthat traditional text-focused RAG systems cannot effectively process. **RAG-Anything** addresses this challenge as a comprehensive **All-in-One Multimodal Document Processing RAG system** built on [LightRAG](https://github.com/HKUDS/LightRAG).

As a unified solution, RAG-Anything **eliminates the need for multiple specialized tools**. It provides **seamless processing and querying across all content modalities** within a single integrated framework. Unlike conventional RAG approaches that struggle with non-textual elements, our all-in-one system delivers **comprehensive multimodal retrieval capabilities**.

Users can query documents containing **interleaved text**, **visual diagrams**, **structured tables**, and **mathematical formulations** through **one cohesive interface**. This consolidated approach makes RAG-Anything particularly valuable for academic research, technical documentation, financial reports, and enterprise knowledge management where rich, mixed-content documents demand a **unified processing framework**.

&lt;img src=&quot;assets/rag_anything_framework.png&quot; alt=&quot;RAG-Anything&quot; /&gt;

&lt;/div&gt;

### üéØ Key Features

&lt;div style=&quot;background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%); border-radius: 15px; padding: 25px; margin: 20px 0;&quot;&gt;

- **üîÑ End-to-End Multimodal Pipeline** - Complete workflow from document ingestion and parsing to intelligent multimodal query answering
- **üìÑ Universal Document Support** - Seamless processing of PDFs, Office documents, images, and diverse file formats
- **üß† Specialized Content Analysis** - Dedicated processors for images, tables, mathematical equations, and heterogeneous content types
- **üîó Multimodal Knowledge Graph** - Automatic entity extraction and cross-modal relationship discovery for enhanced understanding
- **‚ö° Adaptive Processing Modes** - Flexible MinerU-based parsing or direct multimodal content injection workflows
- **üìã Direct Content List Insertion** - Bypass document parsing by directly inserting pre-parsed content lists from external sources
- **üéØ Hybrid Intelligent Retrieval** - Advanced search capabilities spanning textual and multimodal content with contextual understanding

&lt;/div&gt;

---

## üèóÔ∏è Algorithm &amp; Architecture

&lt;div style=&quot;background: linear-gradient(135deg, #0f0f23 0%, #1a1a2e 100%); border-radius: 15px; padding: 25px; margin: 20px 0; border-left: 5px solid #00d9ff;&quot;&gt;

### Core Algorithm

**RAG-Anything** implements an effective **multi-stage multimodal pipeline** that fundamentally extends traditional RAG architectures to seamlessly handle diverse content modalities through intelligent orchestration and cross-modal understanding.

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;div style=&quot;width: 100%; max-width: 600px; margin: 20px auto; padding: 20px; background: linear-gradient(135deg, rgba(0, 217, 255, 0.1) 0%, rgba(0, 217, 255, 0.05) 100%); border-radius: 15px; border: 1px solid rgba(0, 217, 255, 0.2);&quot;&gt;
    &lt;div style=&quot;display: flex; justify-content: space-around; align-items: center; flex-wrap: wrap; gap: 20px;&quot;&gt;
      &lt;div style=&quot;text-align: center;&quot;&gt;
        &lt;div style=&quot;font-size: 24px; margin-bottom: 10px;&quot;&gt;üìÑ&lt;/div&gt;
        &lt;div style=&quot;font-size: 14px; color: #00d9ff;&quot;&gt;Document Parsing&lt;/div&gt;
      &lt;/div&gt;
      &lt;div style=&quot;font-size: 20px; color: #00d9ff;&quot;&gt;‚Üí&lt;/div&gt;
      &lt;div style=&quot;text-align: center;&quot;&gt;
        &lt;div style=&quot;font-size: 24px; margin-bottom: 10px;&quot;&gt;üß†&lt;/div&gt;
        &lt;div style=&quot;font-size: 14px; color: #00d9ff;&quot;&gt;Content Analysis&lt;/div&gt;
      &lt;/div&gt;
      &lt;div style=&quot;font-size: 20px; color: #00d9ff;&quot;&gt;‚Üí&lt;/div&gt;
      &lt;div style=&quot;text-align: center;&quot;&gt;
        &lt;div style=&quot;font-size: 24px; margin-bottom: 10px;&quot;&gt;üîç&lt;/div&gt;
        &lt;div style=&quot;font-size: 14px; color: #00d9ff;&quot;&gt;Knowledge Graph&lt;/div&gt;
      &lt;/div&gt;
      &lt;div style=&quot;font-size: 20px; color: #00d9ff;&quot;&gt;‚Üí&lt;/div&gt;
      &lt;div style=&quot;text-align: center;&quot;&gt;
        &lt;div style=&quot;font-size: 24px; margin-bottom: 10px;&quot;&gt;üéØ&lt;/div&gt;
        &lt;div style=&quot;font-size: 14px; color: #00d9ff;&quot;&gt;Intelligent Retrieval&lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;

### 1. Document Parsing Stage

&lt;div style=&quot;background: linear-gradient(90deg, #1a1a2e 0%, #16213e 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #4ecdc4;&quot;&gt;

The system provides high-fidelity document extraction through adaptive content decomposition. It intelligently segments heterogeneous elements while preserving contextual relationships. Universal format compatibility is achieved via specialized optimized parsers.

**Key Components:**

- **‚öôÔ∏è MinerU Integration**: Leverages [MinerU](https://github.com/opendatalab/MinerU) for high-fidelity document structure extraction and semantic preservation across complex layouts.

- **üß© Adaptive Content Decomposition**: Automatically segments documents into coherent text blocks, visual elements, structured tables, mathematical equations, and specialized content types while preserving contextual relationships.

- **üìÅ Universal Format Support**: Provides comprehensive handling of PDFs, Office documents (DOC/DOCX/PPT/PPTX/XLS/XLSX), images, and emerging formats through specialized parsers with format-specific optimization.

&lt;/div&gt;

### 2. Multi-Modal Content Understanding &amp; Processing

&lt;div style=&quot;background: linear-gradient(90deg, #16213e 0%, #0f3460 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #ff6b6b;&quot;&gt;

The system automatically categorizes and routes content through optimized channels. It uses concurrent pipelines for parallel text and multimodal processing. Document hierarchy and relationships are preserved during transformation.

**Key Components:**

- **üéØ Autonomous Content Categorization and Routing**: Automatically identify, categorize, and route different content types through optimized execution channels.

- **‚ö° Concurrent Multi-Pipeline Architecture**: Implements concurrent execution of textual and multimodal content through dedicated processing pipelines. This approach maximizes throughput efficiency while preserving content integrity.

- **üèóÔ∏è Document Hierarchy Extraction**: Extracts and preserves original document hierarchy and inter-element relationships during content transformation.

&lt;/div&gt;

### 3. Multimodal Analysis Engine

&lt;div style=&quot;background: linear-gradient(90deg, #0f3460 0%, #1a1a2e 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #00d9ff;&quot;&gt;

The system deploys modality-aware processing units for heterogeneous data modalities:

**Specialized Analyzers:**

- **üîç Visual Content Analyzer**:
  - Integrate vision model for image analysis.
  - Generates context-aware descriptive captions based on visual semantics.
  - Extracts spatial relationships and hierarchical structures between visual elements.

- **üìä Structured Data Interpreter**:
  - Performs systematic interpretation of tabular and structured data formats.
  - Implements statistical pattern recognition algorithms for data trend analysis.
  - Identifies semantic relationships and dependencies across multiple tabular datasets.

- **üìê Mathematical Expression Parser**:
  - Parses complex mathematical expressions and formulas with high accuracy.
  - Provides native LaTeX format support for seamless integration with academic workflows.
  - Establishes conceptual mappings between mathematical equations and domain-specific knowledge bases.

- **üîß Extensible Modality Handler**:
  - Provides configurable processing framework for custom and emerging content types.
  - Enables dynamic integration of new modality processors through plugin architecture.
  - Supports runtime configuration of processing pipelines for specialized use cases.

&lt;/div&gt;

### 4. Multimodal Knowledge Graph Index

&lt;div style=&quot;background: linear-gradient(90deg, #1a1a2e 0%, #16213e 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #4ecdc4;&quot;&gt;

The multi-modal knowledge graph construction module transforms document content into structured semantic representations. It extracts multimodal entities, establishes cross-modal relationships, and preserves hierarchical organization. The system applies weighted relevance scoring for optimized knowledge retrieval.

**Core Functions:**

- **üîç Multi-Modal Entity Extraction**: Transforms significant multimodal elements into structured knowledge graph entities. The process includes semantic annotations and metadata preservation.

- **üîó Cross-Modal Relationship Mapping**: Establishes semantic connections and dependencies between textual entities and multimodal components. This is achieved through automated relationship inference algorithms.

- **üèóÔ∏è Hierarchical Structure Preservation**: Maintains original document organization through &quot;belongs_to&quot; relationship chains. These chains preserve logical content hierarchy and sectional dependencies.

- **‚öñÔ∏è Weighted Relationship Scoring**: Assigns quantitative relevance scores to relationship types. Scoring is based on semantic proximity and contextual significance within the document structure.

&lt;/div&gt;

### 5. Modality-Aware Retrieval

&lt;div style=&quot;background: linear-gradient(90deg, #16213e 0%, #0f3460 100%); border-radius: 10px; padding: 20px; margin: 15px 0; border-left: 4px solid #ff6b6b;&quot;&gt;

The hybrid retrieval system combines vector similarity search with graph traversal algorithms for comprehensive content retrieval. It implements modality-aware ranking mechanisms and maintains relational coherence between retrieved elements to ensure contextually integrated information delivery.

**Retrieval Mechanisms:**

- **üîÄ Vector-Graph Fusion**: Integrates vector similarity search with graph traversal algorithms. This approach leverages both semantic embeddings and structural relationships for comprehensive content retrieval.

- **üìä Modality-Aware Ranking**: Implements adaptive scoring mechanisms that weight retrieval results based on content type relevance. The system adjusts rankings according to query-specific modality preferences.

- **üîó Relational Coherence Maintenance**: Maintains semantic and structural relationships between retrieved elements. This ensures coherent information delivery and contextual integrity.

&lt;/div&gt;

---

## üöÄ Quick Start

*Initialize Your AI Journey*

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://user-images.githubusercontent.com/74038190/212284158-e840e285-664b-44d7-b79b-e264b5e54825.gif&quot; width=&quot;400&quot;&gt;
&lt;/div&gt;

### Installation

#### Option 1: Install from PyPI (Recommended)

```bash
# Basic installation
pip install raganything

# With optional dependencies for extended format support:
pip install &#039;raganything[all]&#039;              # All optional features
pip install &#039;raganything[image]&#039;            # Image format conversion (BMP, TIFF, GIF, WebP)
pip install &#039;raganything[text]&#039;             # Text file processing (TXT, MD)
pip install &#039;raganything[image,text]&#039;       # Multiple features
```

#### Option 2: Install from Source
```bash
# Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Clone and setup the project with uv
git clone https://github.com/HKUDS/RAG-Anything.git
cd RAG-Anything

# Install the package and dependencies in a virtual environment
uv sync

# If you encounter network timeouts (especially for opencv packages):
# UV_HTTP_TIMEOUT=120 uv sync

# Run commands directly with uv (recommended approach)
uv run python examples/raganything_example.py --help

# Install with optional dependencies
uv sync --extra image --extra text  # Specific extras
uv sync --all-extras                 # All optional features
```

#### Optional Dependencies

- **`[image]`** - Enables processing of BMP, TIFF, GIF, WebP image formats (requires Pillow)
- **`[text]`** - Enables processing of TXT and MD files (requires ReportLab)
- **`[all]`** - Includes all Python optional dependencies

&gt; **‚ö†Ô∏è Office Document Processing Requirements:**
&gt; - Office documents (.doc, .docx, .ppt, .pptx, .xls, .xlsx) require **LibreOffice** installation
&gt; - Download from [LibreOffice official website](https://www.libreoffice.org/download/download/)
&gt; - **Windows**: Download installer from official website
&gt; - **macOS**: `brew install --cask libreoffice`
&gt; - **Ubuntu/Debian**: `sudo apt-get install libreoffice`
&gt; - **CentOS/RHEL**: `sudo yum install libreoffice`

**Check MinerU installation:**

```bash
# Verify installation
mineru --version

# Check if properly configured
python -c &quot;from raganything import RAGAnything; rag = RAGAnything(); print(&#039;‚úÖ MinerU installed properly&#039; if rag.check_parser_installation() else &#039;‚ùå MinerU installation issue&#039;)&quot;
```

Models are downloaded automatically on first use. For manual download, refer to [MinerU Model Source Configuration](https://github.com/opendatalab/MinerU/blob/master/README.md#22-model-source-configuration).

### Usage Examples

#### 1. End-to-End Document Processing

```python
import asyncio
from raganything import RAGAnything, RAGAnythingConfig
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc

async def main():
    # Set up API configuration
    api_key = &quot;your-api-key&quot;
    base_url = &quot;your-base-url&quot;  # Optional

    # Create RAGAnything configuration
    config = RAGAnythingConfig(
        working_dir=&quot;./rag_storage&quot;,
        parser=&quot;mineru&quot;,  # Parser selection: mineru or docling
        parse_method=&quot;auto&quot;,  # Parse method: auto, ocr, or txt
        enable_image_processing=True,
        enable_table_processing=True,
        enable_equation_processing=True,
    )

    # Define LLM model function
    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
        return openai_complete_if_cache(
            &quot;gpt-4o-mini&quot;,
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )

    # Define vision model function for image processing
    def vision_model_func(
        prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs
    ):
        # If messages format is provided (for multimodal VLM enhanced query), use it directly
        if messages:
            return openai_complete_if_cache(
                &quot;gpt-4o&quot;,
                &quot;&quot;,
                system_prompt=None,
                history_messages=[],
                messages=messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Traditional single image format
        elif image_data:
            return openai_complete_if_cache(
                &quot;gpt-4o&quot;,
                &quot;&quot;,
                system_prompt=None,
                history_messages=[],
                messages=[
                    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_prompt}
                    if system_prompt
                    else None,
                    {
                        &quot;role&quot;: &quot;user&quot;,
                        &quot;content&quot;: [
                            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: prompt},
                            {
                                &quot;type&quot;: &quot;image_url&quot;,
                                &quot;image_url&quot;: {
                                    &quot;url&quot;: f&quot;data:image/jpeg;base64,{image_data}&quot;
                                },
                            },
                        ],
                    }
                    if image_data
                    

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[harry0703/MoneyPrinterTurbo]]></title>
            <link>https://github.com/harry0703/MoneyPrinterTurbo</link>
            <guid>https://github.com/harry0703/MoneyPrinterTurbo</guid>
            <pubDate>Sun, 28 Sep 2025 00:04:33 GMT</pubDate>
            <description><![CDATA[Âà©Áî®AIÂ§ßÊ®°ÂûãÔºå‰∏ÄÈîÆÁîüÊàêÈ´òÊ∏ÖÁü≠ËßÜÈ¢ë Generate short videos with one click using AI LLM.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/harry0703/MoneyPrinterTurbo">harry0703/MoneyPrinterTurbo</a></h1>
            <p>Âà©Áî®AIÂ§ßÊ®°ÂûãÔºå‰∏ÄÈîÆÁîüÊàêÈ´òÊ∏ÖÁü≠ËßÜÈ¢ë Generate short videos with one click using AI LLM.</p>
            <p>Language: Python</p>
            <p>Stars: 40,293</p>
            <p>Forks: 5,872</p>
            <p>Stars today: 37 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;h1 align=&quot;center&quot;&gt;MoneyPrinterTurbo üí∏&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&quot; alt=&quot;Stargazers&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&quot; alt=&quot;Issues&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/network/members&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&quot; alt=&quot;Forks&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/harry0703/MoneyPrinterTurbo/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge&quot; alt=&quot;License&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;h3&gt;ÁÆÄ‰Ωì‰∏≠Êñá | &lt;a href=&quot;README-en.md&quot;&gt;English&lt;/a&gt;&lt;/h3&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/8731&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/8731&quot; alt=&quot;harry0703%2FMoneyPrinterTurbo | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;br&gt;
Âè™ÈúÄÊèê‰æõ‰∏Ä‰∏™ËßÜÈ¢ë &lt;b&gt;‰∏ªÈ¢ò&lt;/b&gt; Êàñ &lt;b&gt;ÂÖ≥ÈîÆËØç&lt;/b&gt; ÔºåÂ∞±ÂèØ‰ª•ÂÖ®Ëá™Âä®ÁîüÊàêËßÜÈ¢ëÊñáÊ°à„ÄÅËßÜÈ¢ëÁ¥†Êùê„ÄÅËßÜÈ¢ëÂ≠óÂπï„ÄÅËßÜÈ¢ëËÉåÊôØÈü≥‰πêÔºåÁÑ∂ÂêéÂêàÊàê‰∏Ä‰∏™È´òÊ∏ÖÁöÑÁü≠ËßÜÈ¢ë„ÄÇ
&lt;br&gt;

&lt;h4&gt;WebÁïåÈù¢&lt;/h4&gt;

![](docs/webui.jpg)

&lt;h4&gt;APIÁïåÈù¢&lt;/h4&gt;

![](docs/api.jpg)

&lt;/div&gt;

## ÁâπÂà´ÊÑüË∞¢ üôè

Áî±‰∫éËØ•È°πÁõÆÁöÑ **ÈÉ®ÁΩ≤** Âíå **‰ΩøÁî®**ÔºåÂØπ‰∫é‰∏Ä‰∫õÂ∞èÁôΩÁî®Êà∑Êù•ËØ¥ÔºåËøòÊòØ **Êúâ‰∏ÄÂÆöÁöÑÈó®Êßõ**ÔºåÂú®Ê≠§ÁâπÂà´ÊÑüË∞¢
**ÂΩïÂíñÔºàAIÊô∫ËÉΩ Â§öÂ™í‰ΩìÊúçÂä°Âπ≥Âè∞Ôºâ** ÁΩëÁ´ôÂü∫‰∫éËØ•È°πÁõÆÔºåÊèê‰æõÁöÑÂÖçË¥π`AIËßÜÈ¢ëÁîüÊàêÂô®`ÊúçÂä°ÔºåÂèØ‰ª•‰∏çÁî®ÈÉ®ÁΩ≤ÔºåÁõ¥Êé•Âú®Á∫ø‰ΩøÁî®ÔºåÈùûÂ∏∏Êñπ‰æø„ÄÇ

- ‰∏≠ÊñáÁâàÔºöhttps://reccloud.cn
- Ëã±ÊñáÁâàÔºöhttps://reccloud.com

![](docs/reccloud.cn.jpg)

## ÊÑüË∞¢ËµûÂä© üôè

ÊÑüË∞¢‰ΩêÁ≥ñ https://picwish.cn ÂØπËØ•È°πÁõÆÁöÑÊîØÊåÅÂíåËµûÂä©Ôºå‰ΩøÂæóËØ•È°πÁõÆËÉΩÂ§üÊåÅÁª≠ÁöÑÊõ¥Êñ∞ÂíåÁª¥Êä§„ÄÇ

‰ΩêÁ≥ñ‰∏ìÊ≥®‰∫é**ÂõæÂÉèÂ§ÑÁêÜÈ¢ÜÂüü**ÔºåÊèê‰æõ‰∏∞ÂØåÁöÑ**ÂõæÂÉèÂ§ÑÁêÜÂ∑•ÂÖ∑**ÔºåÂ∞ÜÂ§çÊùÇÊìç‰ΩúÊûÅËá¥ÁÆÄÂåñÔºåÁúüÊ≠£ÂÆûÁé∞ËÆ©ÂõæÂÉèÂ§ÑÁêÜÊõ¥ÁÆÄÂçï„ÄÇ

![picwish.jpg](docs/picwish.jpg)

## ÂäüËÉΩÁâπÊÄß üéØ

- [x] ÂÆåÊï¥ÁöÑ **MVCÊû∂ÊûÑ**Ôºå‰ª£Á†Å **ÁªìÊûÑÊ∏ÖÊô∞**ÔºåÊòì‰∫éÁª¥Êä§ÔºåÊîØÊåÅ `API` Âíå `WebÁïåÈù¢`
- [x] ÊîØÊåÅËßÜÈ¢ëÊñáÊ°à **AIËá™Âä®ÁîüÊàê**Ôºå‰πüÂèØ‰ª•**Ëá™ÂÆö‰πâÊñáÊ°à**
- [x] ÊîØÊåÅÂ§öÁßç **È´òÊ∏ÖËßÜÈ¢ë** Â∞∫ÂØ∏
    - [x] Á´ñÂ±è 9:16Ôºå`1080x1920`
    - [x] Ê®™Â±è 16:9Ôºå`1920x1080`
- [x] ÊîØÊåÅ **ÊâπÈáèËßÜÈ¢ëÁîüÊàê**ÔºåÂèØ‰ª•‰∏ÄÊ¨°ÁîüÊàêÂ§ö‰∏™ËßÜÈ¢ëÔºåÁÑ∂ÂêéÈÄâÊã©‰∏Ä‰∏™ÊúÄÊª°ÊÑèÁöÑ
- [x] ÊîØÊåÅ **ËßÜÈ¢ëÁâáÊÆµÊó∂Èïø** ËÆæÁΩÆÔºåÊñπ‰æøË∞ÉËäÇÁ¥†ÊùêÂàáÊç¢È¢ëÁéá
- [x] ÊîØÊåÅ **‰∏≠Êñá** Âíå **Ëã±Êñá** ËßÜÈ¢ëÊñáÊ°à
- [x] ÊîØÊåÅ **Â§öÁßçËØ≠Èü≥** ÂêàÊàêÔºåÂèØ **ÂÆûÊó∂ËØïÂê¨** ÊïàÊûú
- [x] ÊîØÊåÅ **Â≠óÂπïÁîüÊàê**ÔºåÂèØ‰ª•Ë∞ÉÊï¥ `Â≠ó‰Ωì`„ÄÅ`‰ΩçÁΩÆ`„ÄÅ`È¢úËâ≤`„ÄÅ`Â§ßÂ∞è`ÔºåÂêåÊó∂ÊîØÊåÅ`Â≠óÂπïÊèèËæπ`ËÆæÁΩÆ
- [x] ÊîØÊåÅ **ËÉåÊôØÈü≥‰πê**ÔºåÈöèÊú∫ÊàñËÄÖÊåáÂÆöÈü≥‰πêÊñá‰ª∂ÔºåÂèØËÆæÁΩÆ`ËÉåÊôØÈü≥‰πêÈü≥Èáè`
- [x] ËßÜÈ¢ëÁ¥†ÊùêÊù•Ê∫ê **È´òÊ∏Ö**ÔºåËÄå‰∏î **Êó†ÁâàÊùÉ**Ôºå‰πüÂèØ‰ª•‰ΩøÁî®Ëá™Â∑±ÁöÑ **Êú¨Âú∞Á¥†Êùê**
- [x] ÊîØÊåÅ **OpenAI**„ÄÅ**Moonshot**„ÄÅ**Azure**„ÄÅ**gpt4free**„ÄÅ**one-api**„ÄÅ**ÈÄö‰πâÂçÉÈóÆ**„ÄÅ**Google Gemini**„ÄÅ**Ollama**„ÄÅ**DeepSeek**„ÄÅ **ÊñáÂøÉ‰∏ÄË®Ä**, **Pollinations** Á≠âÂ§öÁßçÊ®°ÂûãÊé•ÂÖ•
    - ‰∏≠ÂõΩÁî®Êà∑Âª∫ËÆÆ‰ΩøÁî® **DeepSeek** Êàñ **Moonshot** ‰Ωú‰∏∫Â§ßÊ®°ÂûãÊèê‰æõÂïÜÔºàÂõΩÂÜÖÂèØÁõ¥Êé•ËÆøÈóÆÔºå‰∏çÈúÄË¶ÅVPN„ÄÇÊ≥®ÂÜåÂ∞±ÈÄÅÈ¢ùÂ∫¶ÔºåÂü∫Êú¨Â§üÁî®Ôºâ


### ÂêéÊúüËÆ°Âàí üìÖ

- [ ] GPT-SoVITS ÈÖçÈü≥ÊîØÊåÅ
- [ ] ‰ºòÂåñËØ≠Èü≥ÂêàÊàêÔºåÂà©Áî®Â§ßÊ®°ÂûãÔºå‰ΩøÂÖ∂ÂêàÊàêÁöÑÂ£∞Èü≥ÔºåÊõ¥Âä†Ëá™ÁÑ∂ÔºåÊÉÖÁª™Êõ¥Âä†‰∏∞ÂØå
- [ ] Â¢ûÂä†ËßÜÈ¢ëËΩ¨Âú∫ÊïàÊûúÔºå‰ΩøÂÖ∂ÁúãËµ∑Êù•Êõ¥Âä†ÁöÑÊµÅÁïÖ
- [ ] Â¢ûÂä†Êõ¥Â§öËßÜÈ¢ëÁ¥†ÊùêÊù•Ê∫êÔºå‰ºòÂåñËßÜÈ¢ëÁ¥†ÊùêÂíåÊñáÊ°àÁöÑÂåπÈÖçÂ∫¶
- [ ] Â¢ûÂä†ËßÜÈ¢ëÈïøÂ∫¶ÈÄâÈ°πÔºöÁü≠„ÄÅ‰∏≠„ÄÅÈïø
- [ ] ÊîØÊåÅÊõ¥Â§öÁöÑËØ≠Èü≥ÂêàÊàêÊúçÂä°ÂïÜÔºåÊØîÂ¶Ç OpenAI TTS
- [ ] Ëá™Âä®‰∏ä‰º†Âà∞YouTubeÂπ≥Âè∞

## ËßÜÈ¢ëÊºîÁ§∫ üì∫

### Á´ñÂ±è 9:16

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;‚ñ∂Ô∏è&lt;/g-emoji&gt; „ÄäÂ¶Ç‰ΩïÂ¢ûÂä†ÁîüÊ¥ªÁöÑ‰πêË∂£„Äã&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;‚ñ∂Ô∏è&lt;/g-emoji&gt; „ÄäÈáëÈí±ÁöÑ‰ΩúÁî®„Äã&lt;br&gt;Êõ¥ÁúüÂÆûÁöÑÂêàÊàêÂ£∞Èü≥&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;‚ñ∂Ô∏è&lt;/g-emoji&gt; „ÄäÁîüÂëΩÁöÑÊÑè‰πâÊòØ‰ªÄ‰πà„Äã&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/a84d33d5-27a2-4aba-8fd0-9fb2bd91c6a6&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/af2f3b0b-002e-49fe-b161-18ba91c055e8&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/112c9564-d52b-4472-99ad-970b75f66476&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

### Ê®™Â±è 16:9

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;‚ñ∂Ô∏è&lt;/g-emoji&gt;„ÄäÁîüÂëΩÁöÑÊÑè‰πâÊòØ‰ªÄ‰πà„Äã&lt;/th&gt;
&lt;th align=&quot;center&quot;&gt;&lt;g-emoji class=&quot;g-emoji&quot; alias=&quot;arrow_forward&quot;&gt;‚ñ∂Ô∏è&lt;/g-emoji&gt;„Ää‰∏∫‰ªÄ‰πàË¶ÅËøêÂä®„Äã&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/346ebb15-c55f-47a9-a653-114f08bb8073&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;&lt;video src=&quot;https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/271f2fae-8283-44a0-8aa0-0ed8f9a6fa87&quot;&gt;&lt;/video&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

## ÈÖçÁΩÆË¶ÅÊ±Ç üì¶

- Âª∫ËÆÆÊúÄ‰Ωé CPU **4Ê†∏** Êàñ‰ª•‰∏äÔºåÂÜÖÂ≠ò **4G** Êàñ‰ª•‰∏äÔºåÊòæÂç°ÈùûÂøÖÈ°ª
- Windows 10 Êàñ MacOS 11.0 ‰ª•‰∏äÁ≥ªÁªü


## Âø´ÈÄüÂºÄÂßã üöÄ

### Âú® Google Colab ‰∏≠ËøêË°å
ÂÖçÂéªÊú¨Âú∞ÁéØÂ¢ÉÈÖçÁΩÆÔºåÁÇπÂáªÁõ¥Êé•Âú® Google Colab ‰∏≠Âø´ÈÄü‰ΩìÈ™å MoneyPrinterTurbo

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harry0703/MoneyPrinterTurbo/blob/main/docs/MoneyPrinterTurbo.ipynb)


### Windows‰∏ÄÈîÆÂêØÂä®ÂåÖ

‰∏ãËΩΩ‰∏ÄÈîÆÂêØÂä®ÂåÖÔºåËß£ÂéãÁõ¥Êé•‰ΩøÁî®ÔºàË∑ØÂæÑ‰∏çË¶ÅÊúâ **‰∏≠Êñá**„ÄÅ**ÁâπÊÆäÂ≠óÁ¨¶**„ÄÅ**Á©∫Ê†º**Ôºâ

- ÁôæÂ∫¶ÁΩëÁõòÔºàv1.2.6Ôºâ: https://pan.baidu.com/s/1wg0UaIyXpO3SqIpaq790SQ?pwd=sbqx ÊèêÂèñÁ†Å: sbqx
- Google Drive (v1.2.6): https://drive.google.com/file/d/1HsbzfT7XunkrCrHw5ncUjFX8XX4zAuUh/view?usp=sharing

‰∏ãËΩΩÂêéÔºåÂª∫ËÆÆÂÖà**ÂèåÂáªÊâßË°å** `update.bat` Êõ¥Êñ∞Âà∞**ÊúÄÊñ∞‰ª£Á†Å**ÔºåÁÑ∂ÂêéÂèåÂáª `start.bat` ÂêØÂä®

ÂêØÂä®ÂêéÔºå‰ºöËá™Âä®ÊâìÂºÄÊµèËßàÂô®ÔºàÂ¶ÇÊûúÊâìÂºÄÊòØÁ©∫ÁôΩÔºåÂª∫ËÆÆÊç¢Êàê **Chrome** ÊàñËÄÖ **Edge** ÊâìÂºÄÔºâ

## ÂÆâË£ÖÈÉ®ÁΩ≤ üì•

### ÂâçÊèêÊù°‰ª∂

- Â∞ΩÈáè‰∏çË¶Å‰ΩøÁî® **‰∏≠ÊñáË∑ØÂæÑ**ÔºåÈÅøÂÖçÂá∫Áé∞‰∏Ä‰∫õÊó†Ê≥ïÈ¢ÑÊñôÁöÑÈóÆÈ¢ò
- ËØ∑Á°Æ‰øù‰Ω†ÁöÑ **ÁΩëÁªú** ÊòØÊ≠£Â∏∏ÁöÑÔºåVPNÈúÄË¶ÅÊâìÂºÄ`ÂÖ®Â±ÄÊµÅÈáè`Ê®°Âºè

#### ‚ë† ÂÖãÈöÜ‰ª£Á†Å

```shell
git clone https://github.com/harry0703/MoneyPrinterTurbo.git
```

#### ‚ë° ‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂ÔºàÂèØÈÄâÔºåÂª∫ËÆÆÂêØÂä®Âêé‰πüÂèØ‰ª•Âú® WebUI ÈáåÈù¢ÈÖçÁΩÆÔºâ

- Â∞Ü `config.example.toml` Êñá‰ª∂Â§çÂà∂‰∏Ä‰ªΩÔºåÂëΩÂêç‰∏∫ `config.toml`
- ÊåâÁÖß `config.toml` Êñá‰ª∂‰∏≠ÁöÑËØ¥ÊòéÔºåÈÖçÁΩÆÂ•Ω `pexels_api_keys` Âíå `llm_provider`ÔºåÂπ∂Ê†πÊçÆ llm_provider ÂØπÂ∫îÁöÑÊúçÂä°ÂïÜÔºåÈÖçÁΩÆÁõ∏ÂÖ≥ÁöÑ
  API Key

### DockerÈÉ®ÁΩ≤ üê≥

#### ‚ë† ÂêØÂä®Docker

Â¶ÇÊûúÊú™ÂÆâË£Ö DockerÔºåËØ∑ÂÖàÂÆâË£Ö https://www.docker.com/products/docker-desktop/

Â¶ÇÊûúÊòØWindowsÁ≥ªÁªüÔºåËØ∑ÂèÇËÄÉÂæÆËΩØÁöÑÊñáÊ°£Ôºö

1. https://learn.microsoft.com/zh-cn/windows/wsl/install
2. https://learn.microsoft.com/zh-cn/windows/wsl/tutorials/wsl-containers

```shell
cd MoneyPrinterTurbo
docker-compose up
```

&gt; Ê≥®ÊÑèÔºöÊúÄÊñ∞ÁâàÁöÑdockerÂÆâË£ÖÊó∂‰ºöËá™Âä®‰ª•Êèí‰ª∂ÁöÑÂΩ¢ÂºèÂÆâË£Ödocker composeÔºåÂêØÂä®ÂëΩ‰ª§Ë∞ÉÊï¥‰∏∫docker compose up

#### ‚ë° ËÆøÈóÆWebÁïåÈù¢

ÊâìÂºÄÊµèËßàÂô®ÔºåËÆøÈóÆ http://0.0.0.0:8501

#### ‚ë¢ ËÆøÈóÆAPIÊñáÊ°£

ÊâìÂºÄÊµèËßàÂô®ÔºåËÆøÈóÆ http://0.0.0.0:8080/docs ÊàñËÄÖ http://0.0.0.0:8080/redoc

### ÊâãÂä®ÈÉ®ÁΩ≤ üì¶

&gt; ËßÜÈ¢ëÊïôÁ®ã

- ÂÆåÊï¥ÁöÑ‰ΩøÁî®ÊºîÁ§∫Ôºöhttps://v.douyin.com/iFhnwsKY/
- Â¶Ç‰ΩïÂú®Windows‰∏äÈÉ®ÁΩ≤Ôºöhttps://v.douyin.com/iFyjoW3M

#### ‚ë† ÂàõÂª∫ËôöÊãüÁéØÂ¢É

Âª∫ËÆÆ‰ΩøÁî® [conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html) ÂàõÂª∫ python ËôöÊãüÁéØÂ¢É

```shell
git clone https://github.com/harry0703/MoneyPrinterTurbo.git
cd MoneyPrinterTurbo
conda create -n MoneyPrinterTurbo python=3.11
conda activate MoneyPrinterTurbo
pip install -r requirements.txt
```

#### ‚ë° ÂÆâË£ÖÂ•Ω ImageMagick

- Windows:
    - ‰∏ãËΩΩ https://imagemagick.org/script/download.php ÈÄâÊã©WindowsÁâàÊú¨ÔºåÂàáËÆ∞‰∏ÄÂÆöË¶ÅÈÄâÊã© **ÈùôÊÄÅÂ∫ì** ÁâàÊú¨ÔºåÊØîÂ¶Ç
      ImageMagick-7.1.1-32-Q16-x64-**static**.exe
    - ÂÆâË£Ö‰∏ãËΩΩÂ•ΩÁöÑ ImageMagickÔºå**Ê≥®ÊÑè‰∏çË¶Å‰øÆÊîπÂÆâË£ÖË∑ØÂæÑ**
    - ‰øÆÊîπ `ÈÖçÁΩÆÊñá‰ª∂ config.toml` ‰∏≠ÁöÑ `imagemagick_path` ‰∏∫‰Ω†ÁöÑ **ÂÆûÈôÖÂÆâË£ÖË∑ØÂæÑ**

- MacOS:
  ```shell
  brew install imagemagick
  ````
- Ubuntu
  ```shell
  sudo apt-get install imagemagick
  ```
- CentOS
  ```shell
  sudo yum install ImageMagick
  ```

#### ‚ë¢ ÂêØÂä®WebÁïåÈù¢ üåê

Ê≥®ÊÑèÈúÄË¶ÅÂà∞ MoneyPrinterTurbo È°πÁõÆ `Ê†πÁõÆÂΩï` ‰∏ãÊâßË°å‰ª•‰∏ãÂëΩ‰ª§

###### Windows

```bat
webui.bat
```

###### MacOS or Linux

```shell
sh webui.sh
```

ÂêØÂä®ÂêéÔºå‰ºöËá™Âä®ÊâìÂºÄÊµèËßàÂô®ÔºàÂ¶ÇÊûúÊâìÂºÄÊòØÁ©∫ÁôΩÔºåÂª∫ËÆÆÊç¢Êàê **Chrome** ÊàñËÄÖ **Edge** ÊâìÂºÄÔºâ

#### ‚ë£ ÂêØÂä®APIÊúçÂä° üöÄ

```shell
python main.py
```

ÂêØÂä®ÂêéÔºåÂèØ‰ª•Êü•Áúã `APIÊñáÊ°£` http://127.0.0.1:8080/docs ÊàñËÄÖ http://127.0.0.1:8080/redoc Áõ¥Êé•Âú®Á∫øË∞ÉËØïÊé•Âè£ÔºåÂø´ÈÄü‰ΩìÈ™å„ÄÇ

## ËØ≠Èü≥ÂêàÊàê üó£

ÊâÄÊúâÊîØÊåÅÁöÑÂ£∞Èü≥ÂàóË°®ÔºåÂèØ‰ª•Êü•ÁúãÔºö[Â£∞Èü≥ÂàóË°®](./docs/voice-list.txt)

2024-04-16 v1.1.2 Êñ∞Â¢û‰∫Ü9ÁßçAzureÁöÑËØ≠Èü≥ÂêàÊàêÂ£∞Èü≥ÔºåÈúÄË¶ÅÈÖçÁΩÆAPI KEYÔºåËØ•Â£∞Èü≥ÂêàÊàêÁöÑÊõ¥Âä†ÁúüÂÆû„ÄÇ

## Â≠óÂπïÁîüÊàê üìú

ÂΩìÂâçÊîØÊåÅ2ÁßçÂ≠óÂπïÁîüÊàêÊñπÂºèÔºö

- **edge**: ÁîüÊàê`ÈÄüÂ∫¶Âø´`ÔºåÊÄßËÉΩÊõ¥Â•ΩÔºåÂØπÁîµËÑëÈÖçÁΩÆÊ≤°ÊúâË¶ÅÊ±ÇÔºå‰ΩÜÊòØË¥®ÈáèÂèØËÉΩ‰∏çÁ®≥ÂÆö
- **whisper**: ÁîüÊàê`ÈÄüÂ∫¶ÊÖ¢`ÔºåÊÄßËÉΩËæÉÂ∑ÆÔºåÂØπÁîµËÑëÈÖçÁΩÆÊúâ‰∏ÄÂÆöË¶ÅÊ±ÇÔºå‰ΩÜÊòØ`Ë¥®ÈáèÊõ¥ÂèØÈù†`„ÄÇ

ÂèØ‰ª•‰øÆÊîπ `config.toml` ÈÖçÁΩÆÊñá‰ª∂‰∏≠ÁöÑ `subtitle_provider` ËøõË°åÂàáÊç¢

Âª∫ËÆÆ‰ΩøÁî® `edge` Ê®°ÂºèÔºåÂ¶ÇÊûúÁîüÊàêÁöÑÂ≠óÂπïË¥®Èáè‰∏çÂ•ΩÔºåÂÜçÂàáÊç¢Âà∞ `whisper` Ê®°Âºè

&gt; Ê≥®ÊÑèÔºö

1. whisper Ê®°Âºè‰∏ãÈúÄË¶ÅÂà∞ HuggingFace ‰∏ãËΩΩ‰∏Ä‰∏™Ê®°ÂûãÊñá‰ª∂ÔºåÂ§ßÁ∫¶ 3GB Â∑¶Âè≥ÔºåËØ∑Á°Æ‰øùÁΩëÁªúÈÄöÁïÖ
2. Â¶ÇÊûúÁïôÁ©∫ÔºåË°®Á§∫‰∏çÁîüÊàêÂ≠óÂπï„ÄÇ

&gt; Áî±‰∫éÂõΩÂÜÖÊó†Ê≥ïËÆøÈóÆ HuggingFaceÔºåÂèØ‰ª•‰ΩøÁî®‰ª•‰∏ãÊñπÊ≥ï‰∏ãËΩΩ `whisper-large-v3` ÁöÑÊ®°ÂûãÊñá‰ª∂

‰∏ãËΩΩÂú∞ÂùÄÔºö

- ÁôæÂ∫¶ÁΩëÁõò: https://pan.baidu.com/s/11h3Q6tsDtjQKTjUu3sc5cA?pwd=xjs9
- Â§∏ÂÖãÁΩëÁõòÔºöhttps://pan.quark.cn/s/3ee3d991d64b

Ê®°Âûã‰∏ãËΩΩÂêéËß£ÂéãÔºåÊï¥‰∏™ÁõÆÂΩïÊîæÂà∞ `.\MoneyPrinterTurbo\models` ÈáåÈù¢Ôºå
ÊúÄÁªàÁöÑÊñá‰ª∂Ë∑ØÂæÑÂ∫îËØ•ÊòØËøôÊ†∑: `.\MoneyPrinterTurbo\models\whisper-large-v3`

```
MoneyPrinterTurbo  
  ‚îú‚îÄmodels
  ‚îÇ   ‚îî‚îÄwhisper-large-v3
  ‚îÇ          config.json
  ‚îÇ          model.bin
  ‚îÇ          preprocessor_config.json
  ‚îÇ          tokenizer.json
  ‚îÇ          vocabulary.json
```

## ËÉåÊôØÈü≥‰πê üéµ

Áî®‰∫éËßÜÈ¢ëÁöÑËÉåÊôØÈü≥‰πêÔºå‰Ωç‰∫éÈ°πÁõÆÁöÑ `resource/songs` ÁõÆÂΩï‰∏ã„ÄÇ
&gt; ÂΩìÂâçÈ°πÁõÆÈáåÈù¢Êîæ‰∫Ü‰∏Ä‰∫õÈªòËÆ§ÁöÑÈü≥‰πêÔºåÊù•Ëá™‰∫é YouTube ËßÜÈ¢ëÔºåÂ¶ÇÊúâ‰æµÊùÉÔºåËØ∑Âà†Èô§„ÄÇ

## Â≠óÂπïÂ≠ó‰Ωì üÖ∞

Áî®‰∫éËßÜÈ¢ëÂ≠óÂπïÁöÑÊ∏≤ÊüìÔºå‰Ωç‰∫éÈ°πÁõÆÁöÑ `resource/fonts` ÁõÆÂΩï‰∏ãÔºå‰Ω†‰πüÂèØ‰ª•ÊîæËøõÂéªËá™Â∑±ÁöÑÂ≠ó‰Ωì„ÄÇ

## Â∏∏ËßÅÈóÆÈ¢ò ü§î

### ‚ùìRuntimeError: No ffmpeg exe could be found

ÈÄöÂ∏∏ÊÉÖÂÜµ‰∏ãÔºåffmpeg ‰ºöË¢´Ëá™Âä®‰∏ãËΩΩÔºåÂπ∂‰∏î‰ºöË¢´Ëá™Âä®Ê£ÄÊµãÂà∞„ÄÇ
‰ΩÜÊòØÂ¶ÇÊûú‰Ω†ÁöÑÁéØÂ¢ÉÊúâÈóÆÈ¢òÔºåÊó†Ê≥ïËá™Âä®‰∏ãËΩΩÔºåÂèØËÉΩ‰ºöÈÅáÂà∞Â¶Ç‰∏ãÈîôËØØÔºö

```
RuntimeError: No ffmpeg exe could be found.
Install ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable.
```

Ê≠§Êó∂‰Ω†ÂèØ‰ª•‰ªé https://www.gyan.dev/ffmpeg/builds/ ‰∏ãËΩΩffmpegÔºåËß£ÂéãÂêéÔºåËÆæÁΩÆ `ffmpeg_path` ‰∏∫‰Ω†ÁöÑÂÆûÈôÖÂÆâË£ÖË∑ØÂæÑÂç≥ÂèØ„ÄÇ

```toml
[app]
# ËØ∑Ê†πÊçÆ‰Ω†ÁöÑÂÆûÈôÖË∑ØÂæÑËÆæÁΩÆÔºåÊ≥®ÊÑè Windows Ë∑ØÂæÑÂàÜÈöîÁ¨¶‰∏∫ \\
ffmpeg_path = &quot;C:\\Users\\harry\\Downloads\\ffmpeg.exe&quot;
```

### ‚ùìImageMagickÁöÑÂÆâÂÖ®Á≠ñÁï•ÈòªÊ≠¢‰∫Ü‰∏é‰∏¥Êó∂Êñá‰ª∂@/tmp/tmpur5hyyto.txtÁõ∏ÂÖ≥ÁöÑÊìç‰Ωú

ÂèØ‰ª•Âú®ImageMagickÁöÑÈÖçÁΩÆÊñá‰ª∂policy.xml‰∏≠ÊâæÂà∞Ëøô‰∫õÁ≠ñÁï•„ÄÇ
Ëøô‰∏™Êñá‰ª∂ÈÄöÂ∏∏‰Ωç‰∫é /etc/ImageMagick-`X`/ Êàñ ImageMagick ÂÆâË£ÖÁõÆÂΩïÁöÑÁ±ª‰ºº‰ΩçÁΩÆ„ÄÇ
‰øÆÊîπÂåÖÂê´`pattern=&quot;@&quot;`ÁöÑÊù°ÁõÆÔºåÂ∞Ü`rights=&quot;none&quot;`Êõ¥Êîπ‰∏∫`rights=&quot;read|write&quot;`‰ª•ÂÖÅËÆ∏ÂØπÊñá‰ª∂ÁöÑËØªÂÜôÊìç‰Ωú„ÄÇ

### ‚ùìOSError: [Errno 24] Too many open files

Ëøô‰∏™ÈóÆÈ¢òÊòØÁî±‰∫éÁ≥ªÁªüÊâìÂºÄÊñá‰ª∂Êï∞ÈôêÂà∂ÂØºËá¥ÁöÑÔºåÂèØ‰ª•ÈÄöËøá‰øÆÊîπÁ≥ªÁªüÁöÑÊñá‰ª∂ÊâìÂºÄÊï∞ÈôêÂà∂Êù•Ëß£ÂÜ≥„ÄÇ

Êü•ÁúãÂΩìÂâçÈôêÂà∂

```shell
ulimit -n
```

Â¶ÇÊûúËøá‰ΩéÔºåÂèØ‰ª•Ë∞ÉÈ´ò‰∏Ä‰∫õÔºåÊØîÂ¶Ç

```shell
ulimit -n 10240
```

### ‚ùìWhisper Ê®°Âûã‰∏ãËΩΩÂ§±Ë¥•ÔºåÂá∫Áé∞Â¶Ç‰∏ãÈîôËØØ

LocalEntryNotfoundEror: Cannot find an appropriate cached snapshotfolderfor the specified revision on the local disk and
outgoing trafic has been disabled.
To enablerepo look-ups and downloads online, pass &#039;local files only=False&#039; as input.

ÊàñËÄÖ

An error occured while synchronizing the model Systran/faster-whisper-large-v3 from the Hugging Face Hub:
An error happened while trying to locate the files on the Hub and we cannot find the appropriate snapshot folder for the
specified revision on the local disk. Please check your internet connection and try again.
Trying to load the model directly from the local cache, if it exists.

Ëß£ÂÜ≥ÊñπÊ≥ïÔºö[ÁÇπÂáªÊü•ÁúãÂ¶Ç‰Ωï‰ªéÁΩëÁõòÊâãÂä®‰∏ãËΩΩÊ®°Âûã](#%E5%AD%97%E5%B9%95%E7%94%9F%E6%88%90-)

## ÂèçÈ¶àÂª∫ËÆÆ üì¢

- ÂèØ‰ª•Êèê‰∫§ [issue](https://github.com/harry0703/MoneyPrinterTurbo/issues)
  ÊàñËÄÖ [pull request](https://github.com/harry0703/MoneyPrinterTurbo/pulls)„ÄÇ

## ËÆ∏ÂèØËØÅ üìù

ÁÇπÂáªÊü•Áúã [`LICENSE`](LICENSE) Êñá‰ª∂

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=harry0703/MoneyPrinterTurbo&amp;type=Date)](https://star-history.com/#harry0703/MoneyPrinterTurbo&amp;Date)</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[frappe/erpnext]]></title>
            <link>https://github.com/frappe/erpnext</link>
            <guid>https://github.com/frappe/erpnext</guid>
            <pubDate>Sun, 28 Sep 2025 00:04:32 GMT</pubDate>
            <description><![CDATA[Free and Open Source Enterprise Resource Planning (ERP)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/frappe/erpnext">frappe/erpnext</a></h1>
            <p>Free and Open Source Enterprise Resource Planning (ERP)</p>
            <p>Language: Python</p>
            <p>Stars: 28,762</p>
            <p>Forks: 9,409</p>
            <p>Stars today: 107 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/frappe/design/blob/master/logos/logo-2019/erpnext-logo.png&quot; height=&quot;128&quot;&gt;
    &lt;h2&gt;ERPNext&lt;/h2&gt;
    &lt;p align=&quot;center&quot;&gt;
        &lt;p&gt;ERP made simple&lt;/p&gt;
    &lt;/p&gt;

[![Build Status](https://travis-ci.com/frappe/erpnext.png)](https://travis-ci.com/frappe/erpnext)
[![Open Source Helpers](https://www.codetriage.com/frappe/erpnext/badges/users.svg)](https://www.codetriage.com/frappe/erpnext)
[![Coverage Status](https://coveralls.io/repos/github/frappe/erpnext/badge.svg?branch=develop)](https://coveralls.io/github/frappe/erpnext?branch=develop)

[https://erpnext.com](https://erpnext.com)

&lt;/div&gt;

Includes: Accounting, Inventory, Manufacturing, CRM, Sales, Purchase, Project Management, HRMS. Requires MariaDB.

ERPNext is built on the [Frappe](https://github.com/frappe/frappe) Framework, a full-stack web app framework in Python &amp; JavaScript.

- [User Guide](https://erpnext.com/docs/user)
- [Discussion Forum](https://discuss.erpnext.com/)

---

### Full Install

The Easy Way: our install script for bench will install all dependencies (e.g. MariaDB). See https://github.com/frappe/bench for more details.

New passwords will be created for the ERPNext &quot;Administrator&quot; user, the MariaDB root user, and the frappe user (the script displays the passwords and saves them to ~/frappe_passwords.txt).

### Virtual Image

You can download a virtual image to run ERPNext in a virtual machine on your local system.

- [ERPNext Download](http://erpnext.com/download)

System and user credentials are listed on the download page.

---

## License

GNU/General Public License (see [license.txt](license.txt))

The ERPNext code is licensed as GNU General Public License (v3) and the Documentation is licensed as Creative Commons (CC-BY-SA-3.0) and the copyright is owned by Frappe Technologies Pvt Ltd (Frappe) and Contributors.

---

## Contributing

1. [Issue Guidelines](https://github.com/frappe/erpnext/wiki/Issue-Guidelines)
1. [Report Security Vulnerabilities](https://erpnext.com/report)
1. [Pull Request Requirements](https://github.com/frappe/erpnext/wiki/Contribution-Guidelines)
1. [Translations](https://translate.erpnext.com)
1. [Chart of Accounts](https://charts.erpnext.com)

---

## Logo and Trademark

The brand name ERPNext and the logo are trademarks of Frappe Technologies Pvt. Ltd.

### Introduction

Frappe Technologies Pvt. Ltd. (Frappe) owns and oversees the trademarks for the ERPNext name and logos. We have developed this trademark usage policy with the following goals in mind:

- We‚Äôd like to make it easy for anyone to use the ERPNext name or logo for community-oriented efforts that help spread and improve ERPNext.
- We‚Äôd like to make it clear how ERPNext-related businesses and projects can (and cannot) use the ERPNext name and logo.
- We‚Äôd like to make it hard for anyone to use the ERPNext name and logo to unfairly profit from, trick or confuse people who are looking for official ERPNext resources.

### Frappe Trademark Usage Policy

Permission from Frappe is required to use the ERPNext name or logo as part of any project, product, service, domain or company name.

We will grant permission to use the ERPNext name and logo for projects that meet the following criteria:

- The primary purpose of your project is to promote the spread and improvement of the ERPNext software.
- Your project is non-commercial in nature (it can make money to cover its costs or contribute to non-profit entities, but it cannot be run as a for-profit project or business).
Your project neither promotes nor is associated with entities that currently fail to comply with the GPL license under which ERPNext is distributed.
- If your project meets these criteria, you will be permitted to use the ERPNext name and logo to promote your project in any way you see fit with one exception: Please do not use ERPNext as part of a domain name.

Use of the ERPNext name and logo is additionally allowed in the following situations:

All other ERPNext-related businesses or projects can use the ERPNext name and logo to refer to and explain their services, but they cannot use them as part of a product, project, service, domain, or company name and they cannot use them in any way that suggests an affiliation with or endorsement by ERPNext or Frappe Technologies or the ERPNext open source project. For example, a consulting company can describe its business as ‚Äú123 Web Services, offering ERPNext consulting for small businesses,‚Äù but cannot call its business ‚ÄúThe ERPNext Consulting Company.‚Äù

Similarly, it‚Äôs OK to use the ERPNext logo as part of a page that describes your products or services, but it is not OK to use it as part of your company or product logo or branding itself. Under no circumstances is it permitted to use ERPNext as part of a top-level domain name.

We do not allow the use of the trademark in advertising, including AdSense/AdWords.

Please note that it is not the goal of this policy to limit commercial activity around ERPNext. We encourage ERPNext-based businesses, and we would love to see hundreds of them.

When in doubt about your use of the ERPNext name or logo, please contact Frappe Technologies for clarification.

(inspired by WordPress)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Olow304/memvid]]></title>
            <link>https://github.com/Olow304/memvid</link>
            <guid>https://github.com/Olow304/memvid</guid>
            <pubDate>Sun, 28 Sep 2025 00:04:31 GMT</pubDate>
            <description><![CDATA[Video-based AI memory library. Store millions of text chunks in MP4 files with lightning-fast semantic search. No database needed.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Olow304/memvid">Olow304/memvid</a></h1>
            <p>Video-based AI memory library. Store millions of text chunks in MP4 files with lightning-fast semantic search. No database needed.</p>
            <p>Language: Python</p>
            <p>Stars: 9,358</p>
            <p>Forks: 777</p>
            <p>Stars today: 86 stars today</p>
            <h2>README</h2><pre>## What to expect in v2

&gt; **Early-access notice**  
&gt; Memvid v1 is still experimental. The file format and API may change until we lock in a stable release.
&gt; 
&gt; **Memvid v2 ‚Äì what&#039;s next**  
&gt; - **Living-Memory Engine** ‚Äì keep adding new data and let LLMs remember it across sessions.  
&gt; - **Capsule Context** ‚Äì shareable `.mv2` capsules, each with its own rules and expiry.  
&gt; - **Time-Travel Debugging** ‚Äì rewind or branch any chat to review or test.  
&gt; - **Smart Recall** ‚Äì local cache guesses what you‚Äôll need and loads it in under 5 ms.  
&gt; - **Codec Intelligence** ‚Äì auto-tunes AV1 now and future codecs later, so files keep shrinking.  
&gt; - **CLI &amp; Dashboard** ‚Äì simple tools for branching, analytics, and one-command cloud publish.  

Sneak peek of Memvid v2 - a living memory engine that can be used to chat with your knowledge base.
![Memvid v2 Preview](assets/mv2.png)


---

## Memvid v1



[![PyPI](https://img.shields.io/pypi/v/memvid)](https://pypi.org/project/memvid/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![GitHub Stars](https://img.shields.io/github/stars/olow304/memvid)](https://github.com/olow304/memvid)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

# Memvid - Turn millions of text chunks into a single, searchable video file

Memvid compresses an entire knowledge base into **MP4** files while keeping millisecond-level semantic search. Think of it as *SQLite for AI memory* portable, efficient, and self-contained. By encoding text as **QR codes in video frames**, we deliver **50-100√ó** smaller storage than vector databases with **zero infrastructure**.

---

## Why Video Compression Changes Everything üöÄ

| What it enables | How video codecs make it possible |
|---------|-------------------|
| **50-100√ó smaller storage** | Modern video codecs compress repetitive visual patterns (QR codes) far better than raw embeddings |
| **Sub-100ms retrieval** | Direct frame seek via index ‚Üí QR decode ‚Üí your text. No server round-trips |
| **Zero infrastructure** | Just Python and MP4 files-no DB clusters, no Docker, no ops |
| **True portability** | Copy or stream `memory.mp4`-it works anywhere video plays |
| **Offline-first design** | After encoding, everything runs without internet |

---

## Under the Hood - Memvid v1 üîç

1. **Text ‚Üí QR ‚Üí Frame**  
   Each text chunk becomes a QR code, packed into video frames. Modern codecs excel at compressing these repetitive patterns.

2. **Smart indexing**  
   Embeddings map queries ‚Üí frame numbers. One seek, one decode, millisecond results.

3. **Codec leverage**  
   30 years of video R&amp;D means your text gets compressed better than any custom algorithm could achieve.

4. **Future-proof**  
   Next-gen codecs (AV1, H.266) automatically make your memories smaller and faster-no code changes needed.

---

## Installation
```bash
pip install memvid
# For PDF support
pip install memvid PyPDF2
```

## Quick Start
```python
from memvid import MemvidEncoder, MemvidChat

# Create video memory from text
chunks = [&quot;NASA founded 1958&quot;, &quot;Apollo 11 landed 1969&quot;, &quot;ISS launched 1998&quot;]
encoder = MemvidEncoder()
encoder.add_chunks(chunks)
encoder.build_video(&quot;space.mp4&quot;, &quot;space_index.json&quot;)

# Chat with your memory
chat = MemvidChat(&quot;space.mp4&quot;, &quot;space_index.json&quot;)
response = chat.chat(&quot;When did humans land on the moon?&quot;)
print(response)  # References Apollo 11 in 1969
```

## Real-World Examples

### Documentation Assistant
```python
from memvid import MemvidEncoder
import os

encoder = MemvidEncoder(chunk_size=512)

# Index all markdown files
for file in os.listdir(&quot;docs&quot;):
    if file.endswith(&quot;.md&quot;):
        with open(f&quot;docs/{file}&quot;) as f:
            encoder.add_text(f.read(), metadata={&quot;file&quot;: file})

encoder.build_video(&quot;docs.mp4&quot;, &quot;docs_index.json&quot;)
```

### PDF Library Search
```python
# Index multiple PDFs
encoder = MemvidEncoder()
encoder.add_pdf(&quot;deep_learning.pdf&quot;)
encoder.add_pdf(&quot;machine_learning.pdf&quot;) 
encoder.build_video(&quot;ml_library.mp4&quot;, &quot;ml_index.json&quot;)

# Semantic search across all books
from memvid import MemvidRetriever
retriever = MemvidRetriever(&quot;ml_library.mp4&quot;, &quot;ml_index.json&quot;)
results = retriever.search(&quot;backpropagation&quot;, top_k=5)
```

### Interactive Web UI
```python
from memvid import MemvidInteractive

# Launch at http://localhost:7860
interactive = MemvidInteractive(&quot;knowledge.mp4&quot;, &quot;index.json&quot;)
interactive.run()
```

## Advanced Features

### Scale Optimization
```python
# Maximum compression for huge datasets
encoder.build_video(
    &quot;compressed.mp4&quot;,
    &quot;index.json&quot;, 
    fps=60,              # More frames/second
    frame_size=256,      # Smaller QR codes
    video_codec=&#039;h265&#039;,  # Better compression
    crf=28              # Quality tradeoff
)
```

### Custom Embeddings
```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer(&#039;all-mpnet-base-v2&#039;)
encoder = MemvidEncoder(embedding_model=model)
```

### Parallel Processing
```python
encoder = MemvidEncoder(n_workers=8)
encoder.add_chunks_parallel(million_chunks)
```

## CLI Usage
```bash
# Process documents
python examples/file_chat.py --input-dir /docs --provider openai

# Advanced codecs
python examples/file_chat.py --files doc.pdf --codec h265

# Load existing
python examples/file_chat.py --load-existing output/memory
```

## Performance

- **Indexing**: ~10K chunks/second on modern CPUs
- **Search**: &lt;100ms for 1M chunks (includes decode)
- **Storage**: 100MB text ‚Üí 1-2MB video
- **Memory**: Constant 500MB RAM regardless of size

## What&#039;s Coming in v2

- **Delta encoding**: Time-travel through knowledge versions
- **Streaming ingest**: Add to videos in real-time
- **Cloud dashboard**: Web UI with API management
- **Smart codecs**: Auto-select AV1/HEVC per content
- **GPU boost**: 100√ó faster bulk encoding

## Get Involved

Memvid is redefining AI memory. Join us:

- ‚≠ê Star on [GitHub](https://github.com/olow304/memvid)
- üêõ Report issues or request features
- üîß Submit PRs (we review quickly!)
- üí¨ Discuss video-based AI memory

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[imputnet/helium]]></title>
            <link>https://github.com/imputnet/helium</link>
            <guid>https://github.com/imputnet/helium</guid>
            <pubDate>Sun, 28 Sep 2025 00:04:30 GMT</pubDate>
            <description><![CDATA[Private, fast, and honest web browser]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/imputnet/helium">imputnet/helium</a></h1>
            <p>Private, fast, and honest web browser</p>
            <p>Language: Python</p>
            <p>Stars: 3,501</p>
            <p>Forks: 39</p>
            <p>Stars today: 289 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
    &lt;br/&gt;
    &lt;p&gt;
        &lt;img src=&quot;resources/branding/app_icon/raw.png&quot;
            title=&quot;Helium&quot; alt=&quot;Helium logo&quot; width=&quot;120&quot; /&gt;
        &lt;h1&gt;Helium&lt;/h1&gt;
    &lt;/p&gt;
    &lt;p width=&quot;120&quot;&gt;
        The Chromium-based web browser made for people, with love.
        &lt;br&gt;
        Best privacy by default, unbiased ad-blocking, no bloat and no noise.
    &lt;/p&gt;
    &lt;a href=&quot;https://helium.computer/&quot;&gt;
        helium.computer
    &lt;/a&gt;
    &lt;br/&gt;
&lt;/div&gt;

## Downloads
&gt; [!NOTE]
&gt; Helium is still in beta, so unexpected issues may occur. We are not responsible
for any damage caused by usage of beta software.

Best way to download Helium is to open [helium.computer](https://helium.computer/) on your computer.
It&#039;ll pick the right build for your OS and architecture automatically.

If you wish to download builds &quot;straight from the tap&quot; with all options in one place,
you can do it on GitHub in the Releases section in each platform&#039;s repo:
- [macOS](https://github.com/imputnet/helium-macos/releases/latest)
- [Linux](https://github.com/imputnet/helium-linux/releases/latest) (AppImage)
- [Windows](https://github.com/imputnet/helium-windows/releases/latest) (no auto-updates yet)

## Platform packaging
Helium is available on all major desktop platforms, with entirety of source code
for all of them published here:
- [Helium for macOS](https://github.com/imputnet/helium-macos)
- [Helium for Linux](https://github.com/imputnet/helium-linux)
- [Helium for Windows](https://github.com/imputnet/helium-windows)

## Other Helium repos
Along with the main repo and platform packaging, these projects are also a part of Helium:
- [Helium services](https://github.com/imputnet/helium-services)
- [Helium onboarding](https://github.com/imputnet/helium-onboarding) (the onboarding page seen in Helium at `helium://setup`)
- [uBlock Origin packaging](https://github.com/imputnet/ublock-origin-crx)

## Credits
### ungoogled-chromium
Helium is proudly based on [ungoogled-chromium](https://github.com/ungoogled-software/ungoogled-chromium).
It wouldn&#039;t be possible for us to get rid of Google&#039;s bloat and get a development+building pipeline this fast without it.
Huge shout-out to everyone behind this amazing project!
(and we intend to contribute even more stuff upstream in the future)

### The Chromium project
[The Chromium Project](https://www.chromium.org/) is obviously at the core of Helium,
making it possible to exist in the first place.

### ungoogled-chromium&#039;s dependencies
- [Inox patchset](https://github.com/gcarq/inox-patchset)
- [Debian](https://tracker.debian.org/pkg/chromium-browser)
- [Bromite](https://github.com/bromite/bromite)
- [Iridium Browser](https://iridiumbrowser.de/)

## License
All code, patches, modified portions of imported code or patches, and
any other content that is unique to Helium and not imported from other
repositories is licensed under GPL-3.0. See [LICENSE](LICENSE).

Any content imported from other projects retains its original license (for
example, any original unmodified code imported from ungoogled-chromium remains
licensed under their [BSD 3-Clause license](LICENSE.ungoogled_chromium)).

## More documentation (soon)
&gt; [!NOTE]
&gt; We will add more documentation along with design and motivation guidelines in the future.
All docs will be linked here along with other related content.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[roboflow/supervision]]></title>
            <link>https://github.com/roboflow/supervision</link>
            <guid>https://github.com/roboflow/supervision</guid>
            <pubDate>Sun, 28 Sep 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[We write your reusable computer vision tools. üíú]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/roboflow/supervision">roboflow/supervision</a></h1>
            <p>We write your reusable computer vision tools. üíú</p>
            <p>Language: Python</p>
            <p>Stars: 35,289</p>
            <p>Forks: 2,901</p>
            <p>Stars today: 81 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
    &lt;a align=&quot;center&quot; href=&quot;&quot; target=&quot;https://supervision.roboflow.com&quot;&gt;
      &lt;img
        width=&quot;100%&quot;
        src=&quot;https://media.roboflow.com/open-source/supervision/rf-supervision-banner.png?updatedAt=1678995927529&quot;
      &gt;
    &lt;/a&gt;
  &lt;/p&gt;

&lt;br&gt;

[notebooks](https://github.com/roboflow/notebooks) | [inference](https://github.com/roboflow/inference) | [autodistill](https://github.com/autodistill/autodistill) | [maestro](https://github.com/roboflow/multimodal-maestro)

&lt;br&gt;

[![version](https://badge.fury.io/py/supervision.svg)](https://badge.fury.io/py/supervision)
[![downloads](https://img.shields.io/pypi/dm/supervision)](https://pypistats.org/packages/supervision)
[![snyk](https://snyk.io/advisor/python/supervision/badge.svg)](https://snyk.io/advisor/python/supervision)
[![license](https://img.shields.io/pypi/l/supervision)](https://github.com/roboflow/supervision/blob/main/LICENSE.md)
[![python-version](https://img.shields.io/pypi/pyversions/supervision)](https://badge.fury.io/py/supervision)
[![colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow/supervision/blob/main/demo.ipynb)
[![gradio](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/Roboflow/Annotators)
[![discord](https://img.shields.io/discord/1159501506232451173?logo=discord&amp;label=discord&amp;labelColor=fff&amp;color=5865f2&amp;link=https%3A%2F%2Fdiscord.gg%2FGbfgXGJ8Bk)](https://discord.gg/GbfgXGJ8Bk)
[![built-with-material-for-mkdocs](https://img.shields.io/badge/Material_for_MkDocs-526CFE?logo=MaterialForMkDocs&amp;logoColor=white)](https://squidfunk.github.io/mkdocs-material/)

  &lt;div align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://trendshift.io/repositories/124&quot;  target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/124&quot; alt=&quot;roboflow%2Fsupervision | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
  &lt;/div&gt;

&lt;/div&gt;

## üëã hello

**We write your reusable computer vision tools.** Whether you need to load your dataset from your hard drive, draw detections on an image or video, or count how many detections are in a zone. You can count on us! ü§ù

## üíª install

Pip install the supervision package in a
[**Python&gt;=3.9**](https://www.python.org/) environment.

```bash
pip install supervision
```

Read more about conda, mamba, and installing from source in our [guide](https://roboflow.github.io/supervision/).

## üî• quickstart

### models

Supervision was designed to be model agnostic. Just plug in any classification, detection, or segmentation model. For your convenience, we have created [connectors](https://supervision.roboflow.com/latest/detection/core/#detections) for the most popular libraries like Ultralytics, Transformers, or MMDetection.

```python
import cv2
import supervision as sv
from ultralytics import YOLO

image = cv2.imread(...)
model = YOLO(&quot;yolov8s.pt&quot;)
result = model(image)[0]
detections = sv.Detections.from_ultralytics(result)

len(detections)
# 5
```

&lt;details&gt;
&lt;summary&gt;üëâ more model connectors&lt;/summary&gt;

- inference

  Running with [Inference](https://github.com/roboflow/inference) requires a [Roboflow API KEY](https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key).

  ```python
  import cv2
  import supervision as sv
  from inference import get_model

  image = cv2.imread(...)
  model = get_model(model_id=&quot;yolov8s-640&quot;, api_key=&lt;ROBOFLOW API KEY&gt;)
  result = model.infer(image)[0]
  detections = sv.Detections.from_inference(result)

  len(detections)
  # 5
  ```

&lt;/details&gt;

### annotators

Supervision offers a wide range of highly customizable [annotators](https://supervision.roboflow.com/latest/detection/annotators/), allowing you to compose the perfect visualization for your use case.

```python
import cv2
import supervision as sv

image = cv2.imread(...)
detections = sv.Detections(...)

box_annotator = sv.BoxAnnotator()
annotated_frame = box_annotator.annotate(
  scene=image.copy(),
  detections=detections)
```

https://github.com/roboflow/supervision/assets/26109316/691e219c-0565-4403-9218-ab5644f39bce

### datasets

Supervision provides a set of [utils](https://supervision.roboflow.com/latest/datasets/core/) that allow you to load, split, merge, and save datasets in one of the supported formats.

```python
import supervision as sv
from roboflow import Roboflow

project = Roboflow().workspace(&lt;WORKSPACE_ID&gt;).project(&lt;PROJECT_ID&gt;)
dataset = project.version(&lt;PROJECT_VERSION&gt;).download(&quot;coco&quot;)

ds = sv.DetectionDataset.from_coco(
    images_directory_path=f&quot;{dataset.location}/train&quot;,
    annotations_path=f&quot;{dataset.location}/train/_annotations.coco.json&quot;,
)

path, image, annotation = ds[0]
    # loads image on demand

for path, image, annotation in ds:
    # loads image on demand
```

&lt;details close&gt;
&lt;summary&gt;üëâ more dataset utils&lt;/summary&gt;

- load

  ```python
  dataset = sv.DetectionDataset.from_yolo(
      images_directory_path=...,
      annotations_directory_path=...,
      data_yaml_path=...
  )

  dataset = sv.DetectionDataset.from_pascal_voc(
      images_directory_path=...,
      annotations_directory_path=...
  )

  dataset = sv.DetectionDataset.from_coco(
      images_directory_path=...,
      annotations_path=...
  )
  ```

- split

  ```python
  train_dataset, test_dataset = dataset.split(split_ratio=0.7)
  test_dataset, valid_dataset = test_dataset.split(split_ratio=0.5)

  len(train_dataset), len(test_dataset), len(valid_dataset)
  # (700, 150, 150)
  ```

- merge

  ```python
  ds_1 = sv.DetectionDataset(...)
  len(ds_1)
  # 100
  ds_1.classes
  # [&#039;dog&#039;, &#039;person&#039;]

  ds_2 = sv.DetectionDataset(...)
  len(ds_2)
  # 200
  ds_2.classes
  # [&#039;cat&#039;]

  ds_merged = sv.DetectionDataset.merge([ds_1, ds_2])
  len(ds_merged)
  # 300
  ds_merged.classes
  # [&#039;cat&#039;, &#039;dog&#039;, &#039;person&#039;]
  ```

- save

  ```python
  dataset.as_yolo(
      images_directory_path=...,
      annotations_directory_path=...,
      data_yaml_path=...
  )

  dataset.as_pascal_voc(
      images_directory_path=...,
      annotations_directory_path=...
  )

  dataset.as_coco(
      images_directory_path=...,
      annotations_path=...
  )
  ```

- convert

  ```python
  sv.DetectionDataset.from_yolo(
      images_directory_path=...,
      annotations_directory_path=...,
      data_yaml_path=...
  ).as_pascal_voc(
      images_directory_path=...,
      annotations_directory_path=...
  )
  ```

&lt;/details&gt;

## üé¨ tutorials

Want to learn how to use Supervision? Explore our [how-to guides](https://supervision.roboflow.com/develop/how_to/detect_and_annotate/), [end-to-end examples](https://github.com/roboflow/supervision/tree/develop/examples), [cheatsheet](https://roboflow.github.io/cheatsheet-supervision/), and [cookbooks](https://supervision.roboflow.com/develop/cookbooks/)!

&lt;br/&gt;

&lt;p align=&quot;left&quot;&gt;
&lt;a href=&quot;https://youtu.be/hAWpsIuem10&quot; title=&quot;Dwell Time Analysis with Computer Vision | Real-Time Stream Processing&quot;&gt;&lt;img src=&quot;https://github.com/SkalskiP/SkalskiP/assets/26109316/a742823d-c158-407d-b30f-063a5d11b4e1&quot; alt=&quot;Dwell Time Analysis with Computer Vision | Real-Time Stream Processing&quot; width=&quot;300px&quot; align=&quot;left&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://youtu.be/hAWpsIuem10&quot; title=&quot;Dwell Time Analysis with Computer Vision | Real-Time Stream Processing&quot;&gt;&lt;strong&gt;Dwell Time Analysis with Computer Vision | Real-Time Stream Processing&lt;/strong&gt;&lt;/a&gt;
&lt;div&gt;&lt;strong&gt;Created: 5 Apr 2024&lt;/strong&gt;&lt;/div&gt;
&lt;br/&gt;Learn how to use computer vision to analyze wait times and optimize processes. This tutorial covers object detection, tracking, and calculating time spent in designated zones. Use these techniques to improve customer experience in retail, traffic management, or other scenarios.&lt;/p&gt;

&lt;br/&gt;

&lt;p align=&quot;left&quot;&gt;
&lt;a href=&quot;https://youtu.be/uWP6UjDeZvY&quot; title=&quot;Speed Estimation &amp; Vehicle Tracking | Computer Vision | Open Source&quot;&gt;&lt;img src=&quot;https://github.com/SkalskiP/SkalskiP/assets/26109316/61a444c8-b135-48ce-b979-2a5ab47c5a91&quot; alt=&quot;Speed Estimation &amp; Vehicle Tracking | Computer Vision | Open Source&quot; width=&quot;300px&quot; align=&quot;left&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://youtu.be/uWP6UjDeZvY&quot; title=&quot;Speed Estimation &amp; Vehicle Tracking | Computer Vision | Open Source&quot;&gt;&lt;strong&gt;Speed Estimation &amp; Vehicle Tracking | Computer Vision | Open Source&lt;/strong&gt;&lt;/a&gt;
&lt;div&gt;&lt;strong&gt;Created: 11 Jan 2024&lt;/strong&gt;&lt;/div&gt;
&lt;br/&gt;Learn how to track and estimate the speed of vehicles using YOLO, ByteTrack, and Roboflow Inference. This comprehensive tutorial covers object detection, multi-object tracking, filtering detections, perspective transformation, speed estimation, visualization improvements, and more.&lt;/p&gt;

## üíú built with supervision

Did you build something cool using supervision? [Let us know!](https://github.com/roboflow/supervision/discussions/categories/built-with-supervision)

https://user-images.githubusercontent.com/26109316/207858600-ee862b22-0353-440b-ad85-caa0c4777904.mp4

https://github.com/roboflow/supervision/assets/26109316/c9436828-9fbf-4c25-ae8c-60e9c81b3900

https://github.com/roboflow/supervision/assets/26109316/3ac6982f-4943-4108-9b7f-51787ef1a69f

## üìö documentation

Visit our [documentation](https://roboflow.github.io/supervision) page to learn how supervision can help you build computer vision applications faster and more reliably.

## üèÜ contribution

We love your input! Please see our [contributing guide](https://github.com/roboflow/supervision/blob/main/CONTRIBUTING.md) to get started. Thank you üôè to all our contributors!

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://github.com/roboflow/supervision/graphs/contributors&quot;&gt;
      &lt;img src=&quot;https://contrib.rocks/image?repo=roboflow/supervision&quot; /&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;br&gt;

&lt;div align=&quot;center&quot;&gt;

&lt;div align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://youtube.com/roboflow&quot;&gt;
          &lt;img
            src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1672949634652&quot;
            width=&quot;3%&quot;
          /&gt;
      &lt;/a&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&quot; width=&quot;3%&quot;/&gt;
      &lt;a href=&quot;https://roboflow.com&quot;&gt;
          &lt;img
            src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1672949746649&quot;
            width=&quot;3%&quot;
          /&gt;
      &lt;/a&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&quot; width=&quot;3%&quot;/&gt;
      &lt;a href=&quot;https://www.linkedin.com/company/roboflow-ai/&quot;&gt;
          &lt;img
            src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1672949633691&quot;
            width=&quot;3%&quot;
          /&gt;
      &lt;/a&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&quot; width=&quot;3%&quot;/&gt;
      &lt;a href=&quot;https://docs.roboflow.com&quot;&gt;
          &lt;img
            src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1672949634511&quot;
            width=&quot;3%&quot;
          /&gt;
      &lt;/a&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&quot; width=&quot;3%&quot;/&gt;
      &lt;a href=&quot;https://discuss.roboflow.com&quot;&gt;
          &lt;img
            src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1672949633584&quot;
            width=&quot;3%&quot;
          /&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&quot; width=&quot;3%&quot;/&gt;
      &lt;a href=&quot;https://blog.roboflow.com&quot;&gt;
          &lt;img
            src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&amp;updatedAt=1672949633605&quot;
            width=&quot;3%&quot;
          /&gt;
      &lt;/a&gt;
      &lt;/a&gt;
  &lt;/div&gt;
&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Tencent-Hunyuan/Hunyuan3D-2.1]]></title>
            <link>https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1</link>
            <guid>https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1</guid>
            <pubDate>Sun, 28 Sep 2025 00:04:28 GMT</pubDate>
            <description><![CDATA[From Images to High-Fidelity 3D Assets with Production-Ready PBR Material]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1">Tencent-Hunyuan/Hunyuan3D-2.1</a></h1>
            <p>From Images to High-Fidelity 3D Assets with Production-Ready PBR Material</p>
            <p>Language: Python</p>
            <p>Stars: 2,166</p>
            <p>Forks: 279</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre>
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/images/teaser.jpg&quot;&gt;
&lt;/p&gt;


&lt;div align=&quot;center&quot;&gt;
  &lt;a href=https://3d.hunyuan.tencent.com target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage height=22px&gt;&lt;/a&gt;
  &lt;a href=https://huggingface.co/spaces/tencent/Hunyuan3D-2.1  target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/%F0%9F%A4%97%20Demo-276cb4.svg height=22px&gt;&lt;/a&gt;
  &lt;a href=https://huggingface.co/tencent/Hunyuan3D-2.1 target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px&gt;&lt;/a&gt;
  &lt;a href=https://3d-models.hunyuan.tencent.com/ target=&quot;_blank&quot;&gt;&lt;img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px&gt;&lt;/a&gt;
  &lt;a href=https://discord.gg/dNBrdrGGMa target=&quot;_blank&quot;&gt;&lt;img src= https://img.shields.io/badge/Discord-white.svg?logo=discord height=22px&gt;&lt;/a&gt;
  &lt;a href=https://arxiv.org/pdf/2506.15442 target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px&gt;&lt;/a&gt;
  &lt;a href=https://x.com/TencentHunyuan target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px&gt;&lt;/a&gt;
 &lt;a href=&quot;#community-resources&quot; target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/Community-lavender.svg?logo=homeassistantcommunitystore height=22px&gt;&lt;/a&gt;
&lt;/div&gt;

[//]: # (  &lt;a href=# target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px&gt;&lt;/a&gt;)

[//]: # (  &lt;a href=# target=&quot;_blank&quot;&gt;&lt;img src= https://img.shields.io/badge/Colab-8f2628.svg?logo=googlecolab height=22px&gt;&lt;/a&gt;)

[//]: # (  &lt;a href=&quot;#&quot;&gt;&lt;img alt=&quot;PyPI - Downloads&quot; src=&quot;https://img.shields.io/pypi/v/mulankit?logo=pypi&quot;  height=22px&gt;&lt;/a&gt;)
&lt;br&gt;

## üî• News

- Jul 26, 2025: ü§ó We release the first open-source, simulation-capable, immersive 3D world generation model, [HunyuanWorld-1.0](https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0)!
- Jun 19, 2025: üëã We present the [technical report](https://arxiv.org/pdf/2506.15442) of Hunyuan3D-2.1, please check out the details and spark some discussion!
- Jun 13, 2025: ü§ó We release the first production-ready 3D asset generation model, Hunyuan3D-2.1!

&gt; Join our **[Wechat](#)** and **[Discord](https://discord.gg/dNBrdrGGMa)** group to discuss and find help from us.

| Wechat Group                                     | Xiaohongshu                                           | X                                           | Discord                                           |
|--------------------------------------------------|-------------------------------------------------------|---------------------------------------------|---------------------------------------------------|
| &lt;img src=&quot;assets/qrcode/wechat.png&quot;  height=140&gt; | &lt;img src=&quot;assets/qrcode/xiaohongshu.png&quot;  height=140&gt; | &lt;img src=&quot;assets/qrcode/x.png&quot;  height=140&gt; | &lt;img src=&quot;assets/qrcode/discord.png&quot;  height=140&gt; |        

## ü§ó Community Contribution Leaderboard
1. By [@visualbruno](https://github.com/visualbruno)
  - ComfyUI-Hunyuan3d-2-1: https://github.com/visualbruno/ComfyUI-Hunyuan3d-2-1
2. By [@VR-Jobs](https://github.com/VR-Jobs)
  - Hunyuan3d-2-1 Unity Support: https://github.com/VR-Jobs/Hunyuan3D-2.1-Unity-XR-PC-Phone

## ‚òØÔ∏è **Hunyuan3D 2.1**

### Architecture

Tencent Hunyuan3D-2.1 is a scalable 3D asset creation system that advances state-of-the-art 3D generation through two pivotal innovations: Fully Open-Source Framework and  Physically-Based Rendering (PBR) Texture Synthesis. For the first time, the system releases full model weights and training code, enabling community developers to directly fine-tune and extend the model for diverse downstream applications. This transparency accelerates academic research and industrial deployment. Moreover, replacing the prior RGB-based texture model, the upgraded PBR pipeline leverages  physics-grounded material simulation  to generate textures with photorealistic light interaction (e.g., metallic reflections, subsurface scattering).

&lt;p align=&quot;left&quot;&gt;
  &lt;img src=&quot;assets/images/pipeline.png&quot;&gt;
&lt;/p&gt;

### Performance

We have evaluated Hunyuan3D 2.1 with other open-source as well as close-source 3d-generation methods.
The numerical results indicate that Hunyuan3D 2.1 surpasses all baselines in the quality of generated textured 3D assets
and the condition following ability.

| Model                   | ULIP-T(‚¨Ü)   | ULIP-I(‚¨Ü) | Uni3D-T(‚¨Ü)      | Uni3D-I(‚¨Ü) |
|-------------------------|-----------|-------------|-------------|---------------|
| Michelangelo  | 0.0752     | 0.1152      | 0.2133     | 0.2611         |
| Craftsman | 0.0745     | 0.1296      | 0.2375     | 0.2987         |
| TripoSG | 0.0767     | 0.1225      | 0.2506     | 0.3129       |
| Step1X-3D | 0.0735     | 0.1183      | 0.2554     | 0.3195         |
| Trellis | 0.0769     | 0.1267      | 0.2496     | 0.3116         |
| Direct3D-S2 | 0.0706     | 0.1134      | 0.2346     | 0.2930         |
| Hunyuan3D-Shape-2.1           | **0.0774** | **0.1395**  | **0.2556** | **0.3213** |


| Model                   | CLIP-FiD(‚¨á)   | CMMD(‚¨á) | CLIP-I(‚¨Ü)      | LPIPS(‚¨á) |
|-------------------------|-----------|-------------|-------------|---------------|
| SyncMVD-IPA  | 28.39     | 2.397      | 0.8823     | 0.1423         |
| TexGen | 28.24     | 2.448      | 0.8818     | 0.1331         |
| Hunyuan3D-2.0 | 26.44     | 2.318     | 0.8893     | 0.1261         |
| Hunyuan3D-Paint-2.1           | **24.78** | **2.191**  | **0.9207** | **0.1211**     |



## üéÅ Models Zoo

It takes 10 GB VRAM for shape generation, 21GB for texture generation and 29GB for shape and texture generation in total.


| Model                      | Description                 | Date       | Size | Huggingface                                                                               |
|----------------------------|-----------------------------|------------|------|-------------------------------------------------------------------------------------------| 
| Hunyuan3D-Shape-v2-1         | Image to Shape Model        | 2025-06-14 | 3.3B | [Download](https://huggingface.co/tencent/Hunyuan3D-2.1/tree/main/hunyuan3d-dit-v2-1)         |
| Hunyuan3D-Paint-v2-1       | Texture Generation Model    | 2025-06-14 | 2B | [Download](https://huggingface.co/tencent/Hunyuan3D-2.1/tree/main/hunyuan3d-paintpbr-v2-1)       |


## ü§ó Get Started with Hunyuan3D 2.1

Hunyuan3D 2.1 supports Macos, Windows, Linux. You may follow the next steps to use Hunyuan3D 2.1 via:

### Install Requirements
We test our model with Python 3.10 and PyTorch 2.5.1+cu124.
```bash
pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124
pip install -r requirements.txt

cd hy3dpaint/custom_rasterizer
pip install -e .
cd ../..
cd hy3dpaint/DifferentiableRenderer
bash compile_mesh_painter.sh
cd ../..

wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P hy3dpaint/ckpt
```

### Code Usage

We designed a diffusers-like API to use our shape generation model - Hunyuan3D-Shape and texture synthesis model -
Hunyuan3D-Paint.

```python
import sys
sys.path.insert(0, &#039;./hy3dshape&#039;)
sys.path.insert(0, &#039;./hy3dpaint&#039;)
from textureGenPipeline import Hunyuan3DPaintPipeline
from textureGenPipeline import Hunyuan3DPaintPipeline, Hunyuan3DPaintConfig
from hy3dshape.pipelines import Hunyuan3DDiTFlowMatchingPipeline

# let&#039;s generate a mesh first
shape_pipeline = Hunyuan3DDiTFlowMatchingPipeline.from_pretrained(&#039;tencent/Hunyuan3D-2.1&#039;)
mesh_untextured = shape_pipeline(image=&#039;assets/demo.png&#039;)[0]

paint_pipeline = Hunyuan3DPaintPipeline(Hunyuan3DPaintConfig(max_num_view=6, resolution=512))
mesh_textured = paint_pipeline(mesh_path, image_path=&#039;assets/demo.png&#039;)
```


### Gradio App

You could also host a [Gradio](https://www.gradio.app/) App in your own computer via:


```bash
python3 gradio_app.py \
  --model_path tencent/Hunyuan3D-2.1 \
  --subfolder hunyuan3d-dit-v2-1 \
  --texgen_model_path tencent/Hunyuan3D-2.1 \
  --low_vram_mode
```


## üîó BibTeX

If you found this repository helpful, please cite our reports:

```bibtex
@misc{hunyuan3d2025hunyuan3d,
    title={Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with Production-Ready PBR Material},
    author={Tencent Hunyuan3D Team},
    year={2025},
    eprint={2506.15442},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{hunyuan3d22025tencent,
    title={Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation},
    author={Tencent Hunyuan3D Team},
    year={2025},
    eprint={2501.12202},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{yang2024hunyuan3d,
    title={Hunyuan3D 1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation},
    author={Tencent Hunyuan3D Team},
    year={2024},
    eprint={2411.02293},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
```

## Acknowledgements

We would like to thank the contributors to
the [TripoSG](https://github.com/VAST-AI-Research/TripoSG), [Trellis](https://github.com/microsoft/TRELLIS),  [DINOv2](https://github.com/facebookresearch/dinov2), [Stable Diffusion](https://github.com/Stability-AI/stablediffusion), [FLUX](https://github.com/black-forest-labs/flux), [diffusers](https://github.com/huggingface/diffusers), [HuggingFace](https://huggingface.co), [CraftsMan3D](https://github.com/wyysf-98/CraftsMan3D), [Michelangelo](https://github.com/NeuralCarver/Michelangelo/tree/main), [Hunyuan-DiT](https://github.com/Tencent-Hunyuan/HunyuanDiT), and [HunyuanVideo](https://github.com/Tencent-Hunyuan/HunyuanVideo) repositories, for their open research and
exploration.

## Star History

&lt;a href=&quot;https://star-history.com/#Tencent-Hunyuan/Hunyuan3D-2.1&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Tencent-Hunyuan/Hunyuan3D-2.1&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Tencent-Hunyuan/Hunyuan3D-2.1&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=Tencent-Hunyuan/Hunyuan3D-2.1&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Asabeneh/30-Days-Of-Python]]></title>
            <link>https://github.com/Asabeneh/30-Days-Of-Python</link>
            <guid>https://github.com/Asabeneh/30-Days-Of-Python</guid>
            <pubDate>Sun, 28 Sep 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[30 days of Python programming challenge is a step-by-step guide to learn the Python programming language in 30 days. This challenge may take more than100 days, follow your own pace. These videos may help too: https://www.youtube.com/channel/UC7PNRuno1rzYPb1xLa4yktw]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Asabeneh/30-Days-Of-Python">Asabeneh/30-Days-Of-Python</a></h1>
            <p>30 days of Python programming challenge is a step-by-step guide to learn the Python programming language in 30 days. This challenge may take more than100 days, follow your own pace. These videos may help too: https://www.youtube.com/channel/UC7PNRuno1rzYPb1xLa4yktw</p>
            <p>Language: Python</p>
            <p>Stars: 50,337</p>
            <p>Forks: 9,586</p>
            <p>Stars today: 67 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[confident-ai/deepeval]]></title>
            <link>https://github.com/confident-ai/deepeval</link>
            <guid>https://github.com/confident-ai/deepeval</guid>
            <pubDate>Sun, 28 Sep 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[The LLM Evaluation Framework]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/confident-ai/deepeval">confident-ai/deepeval</a></h1>
            <p>The LLM Evaluation Framework</p>
            <p>Language: Python</p>
            <p>Stars: 11,287</p>
            <p>Forks: 973</p>
            <p>Stars today: 22 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/confident-ai/deepeval/blob/main/docs/static/img/deepeval.png&quot; alt=&quot;DeepEval Logo&quot; width=&quot;100%&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;h1 align=&quot;center&quot;&gt;The LLM Evaluation Framework&lt;/h1&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/5917&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/5917&quot; alt=&quot;confident-ai%2Fdeepeval | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://discord.gg/3SEyvpgu2f&quot;&gt;
        &lt;img alt=&quot;discord-invite&quot; src=&quot;https://dcbadge.vercel.app/api/server/3SEyvpgu2f?style=flat&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;h4 align=&quot;center&quot;&gt;
    &lt;p&gt;
        &lt;a href=&quot;https://deepeval.com/docs/getting-started?utm_source=GitHub&quot;&gt;Documentation&lt;/a&gt; |
        &lt;a href=&quot;#-metrics-and-features&quot;&gt;Metrics and Features&lt;/a&gt; |
        &lt;a href=&quot;#-quickstart&quot;&gt;Getting Started&lt;/a&gt; |
        &lt;a href=&quot;#-integrations&quot;&gt;Integrations&lt;/a&gt; |
        &lt;a href=&quot;https://confident-ai.com?utm_source=GitHub&quot;&gt;DeepEval Platform&lt;/a&gt;
    &lt;p&gt;
&lt;/h4&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://github.com/confident-ai/deepeval/releases&quot;&gt;
        &lt;img alt=&quot;GitHub release&quot; src=&quot;https://img.shields.io/github/release/confident-ai/deepeval.svg?color=violet&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://colab.research.google.com/drive/1PPxYEBa6eu__LquGoFFJZkhYgWVYE6kh?usp=sharing&quot;&gt;
        &lt;img alt=&quot;Try Quickstart in Colab&quot; src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/confident-ai/deepeval/blob/master/LICENSE.md&quot;&gt;
        &lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/github/license/confident-ai/deepeval.svg?color=yellow&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://x.com/deepeval&quot;&gt;
        &lt;img alt=&quot;Twitter Follow&quot; src=&quot;https://img.shields.io/twitter/follow/deepeval?style=social&amp;logo=x&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=de&quot;&gt;Deutsch&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=es&quot;&gt;Espa√±ol&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=fr&quot;&gt;fran√ßais&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=ja&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=ko&quot;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=pt&quot;&gt;Portugu√™s&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=ru&quot;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=zh&quot;&gt;‰∏≠Êñá&lt;/a&gt;
&lt;/p&gt;

**DeepEval** is a simple-to-use, open-source LLM evaluation framework, for evaluating and testing large-language model systems. It is similar to Pytest but specialized for unit testing LLM outputs. DeepEval incorporates the latest research to evaluate LLM outputs based on metrics such as G-Eval, hallucination, answer relevancy, RAGAS, etc., which uses LLMs and various other NLP models that runs **locally on your machine** for evaluation.

Whether your LLM applications are RAG pipelines, chatbots, AI agents, implemented via LangChain or LlamaIndex, DeepEval has you covered. With it, you can easily determine the optimal models, prompts, and architecture to improve your RAG pipeline, agentic workflows, prevent prompt drifting, or even transition from OpenAI to hosting your own Deepseek R1 with confidence.

&gt; [!IMPORTANT]
&gt; Need a place for your DeepEval testing data to live üè°‚ù§Ô∏è? [Sign up to the DeepEval platform](https://confident-ai.com?utm_source=GitHub) to compare iterations of your LLM app, generate &amp; share testing reports, and more.
&gt;
&gt; ![Demo GIF](assets/demo.gif)

&gt; Want to talk LLM evaluation, need help picking metrics, or just to say hi? [Come join our discord.](https://discord.com/invite/3SEyvpgu2f)

&lt;br /&gt;

# üî• Metrics and Features

&gt; ü•≥ You can now share DeepEval&#039;s test results on the cloud directly on [Confident AI](https://confident-ai.com?utm_source=GitHub)&#039;s infrastructure

- Supports both end-to-end and component-level LLM evaluation.
- Large variety of ready-to-use LLM evaluation metrics (all with explanations) powered by **ANY** LLM of your choice, statistical methods, or NLP models that runs **locally on your machine**:
  - G-Eval
  - DAG ([deep acyclic graph](https://deepeval.com/docs/metrics-dag))
  - **RAG metrics:**
    - Answer Relevancy
    - Faithfulness
    - Contextual Recall
    - Contextual Precision
    - Contextual Relevancy
    - RAGAS
  - **Agentic metrics:**
    - Task Completion
    - Tool Correctness
  - **Others:**
    - Hallucination
    - Summarization
    - Bias
    - Toxicity
  - **Conversational metrics:**
    - Knowledge Retention
    - Conversation Completeness
    - Conversation Relevancy
    - Role Adherence
  - etc.
- Build your own custom metrics that are automatically integrated with DeepEval&#039;s ecosystem.
- Generate synthetic datasets for evaluation.
- Integrates seamlessly with **ANY** CI/CD environment.
- [Red team your LLM application](https://deepeval.com/docs/red-teaming-introduction) for 40+ safety vulnerabilities in a few lines of code, including:
  - Toxicity
  - Bias
  - SQL Injection
  - etc., using advanced 10+ attack enhancement strategies such as prompt injections.
- Easily benchmark **ANY** LLM on popular LLM benchmarks in [under 10 lines of code.](https://deepeval.com/docs/benchmarks-introduction?utm_source=GitHub), which includes:
  - MMLU
  - HellaSwag
  - DROP
  - BIG-Bench Hard
  - TruthfulQA
  - HumanEval
  - GSM8K
- [100% integrated with Confident AI](https://confident-ai.com?utm_source=GitHub) for the full evaluation lifecycle:
  - Curate/annotate evaluation datasets on the cloud
  - Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best
  - Fine-tune metrics for custom results
  - Debug evaluation results via LLM traces
  - Monitor &amp; evaluate LLM responses in product to improve datasets with real-world data
  - Repeat until perfection

&gt; [!NOTE]
&gt; Confident AI is the DeepEval platform. Create an account [here.](https://app.confident-ai.com?utm_source=GitHub)

&lt;br /&gt;

# üîå Integrations

- ü¶Ñ LlamaIndex, to [**unit test RAG applications in CI/CD**](https://www.deepeval.com/integrations/frameworks/llamaindex?utm_source=GitHub)
- ü§ó Hugging Face, to [**enable real-time evaluations during LLM fine-tuning**](https://www.deepeval.com/integrations/frameworks/huggingface?utm_source=GitHub)

&lt;br /&gt;

# üöÄ QuickStart

Let&#039;s pretend your LLM application is a RAG based customer support chatbot; here&#039;s how DeepEval can help test what you&#039;ve built.

## Installation

Deepeval works with **Python&gt;=3.9+**.

```
pip install -U deepeval
```

## Create an account (highly recommended)

Using the `deepeval` platform will allow you to generate sharable testing reports on the cloud. It is free, takes no additional code to setup, and we highly recommend giving it a try.

To login, run:

```
deepeval login
```

Follow the instructions in the CLI to create an account, copy your API key, and paste it into the CLI. All test cases will automatically be logged (find more information on data privacy [here](https://deepeval.com/docs/data-privacy?utm_source=GitHub)).

## Writing your first test case

Create a test file:

```bash
touch test_chatbot.py
```

Open `test_chatbot.py` and write your first test case to run an **end-to-end** evaluation using DeepEval, which treats your LLM app as a black-box:

```python
import pytest
from deepeval import assert_test
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams

def test_case():
    correctness_metric = GEval(
        name=&quot;Correctness&quot;,
        criteria=&quot;Determine if the &#039;actual output&#039; is correct based on the &#039;expected output&#039;.&quot;,
        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],
        threshold=0.5
    )
    test_case = LLMTestCase(
        input=&quot;What if these shoes don&#039;t fit?&quot;,
        # Replace this with the actual output from your LLM application
        actual_output=&quot;You have 30 days to get a full refund at no extra cost.&quot;,
        expected_output=&quot;We offer a 30-day full refund at no extra costs.&quot;,
        retrieval_context=[&quot;All customers are eligible for a 30 day full refund at no extra costs.&quot;]
    )
    assert_test(test_case, [correctness_metric])
```

Set your `OPENAI_API_KEY` as an environment variable (you can also evaluate using your own custom model, for more details visit [this part of our docs](https://deepeval.com/docs/metrics-introduction#using-a-custom-llm?utm_source=GitHub)):

```
export OPENAI_API_KEY=&quot;...&quot;
```

And finally, run `test_chatbot.py` in the CLI:

```
deepeval test run test_chatbot.py
```

**Congratulations! Your test case should have passed ‚úÖ** Let&#039;s breakdown what happened.

- The variable `input` mimics a user input, and `actual_output` is a placeholder for what your application&#039;s supposed to output based on this input.
- The variable `expected_output` represents the ideal answer for a given `input`, and [`GEval`](https://deepeval.com/docs/metrics-llm-evals) is a research-backed metric provided by `deepeval` for you to evaluate your LLM output&#039;s on any custom with human-like accuracy.
- In this example, the metric `criteria` is correctness of the `actual_output` based on the provided `expected_output`.
- All metric scores range from 0 - 1, which the `threshold=0.5` threshold ultimately determines if your test have passed or not.

[Read our documentation](https://deepeval.com/docs/getting-started?utm_source=GitHub) for more information on more options to run end-to-end evaluation, how to use additional metrics, create your own custom metrics, and tutorials on how to integrate with other tools like LangChain and LlamaIndex.

&lt;br /&gt;

## Evaluating Nested Components

If you wish to evaluate individual components within your LLM app, you need to run **component-level** evals - a powerful way to evaluate any component within an LLM system.

Simply trace &quot;components&quot; such as LLM calls, retrievers, tool calls, and agents within your LLM application using the `@observe` decorator to apply metrics on a component-level. Tracing with `deepeval` is non-instrusive (learn more [here](https://deepeval.com/docs/evaluation-llm-tracing#dont-be-worried-about-tracing)) and helps you avoid rewriting your codebase just for evals:

```python
from deepeval.tracing import observe, update_current_span
from deepeval.test_case import LLMTestCase
from deepeval.dataset import Golden
from deepeval.metrics import GEval
from deepeval import evaluate

correctness = GEval(name=&quot;Correctness&quot;, criteria=&quot;Determine if the &#039;actual output&#039; is correct based on the &#039;expected output&#039;.&quot;, evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT])

@observe(metrics=[correctness])
def inner_component():
    # Component can be anything from an LLM call, retrieval, agent, tool use, etc.
    update_current_span(test_case=LLMTestCase(input=&quot;...&quot;, actual_output=&quot;...&quot;))
    return

@observe
def llm_app(input: str):
    inner_component()
    return

evaluate(observed_callback=llm_app, goldens=[Golden(input=&quot;Hi!&quot;)])
```

You can learn everything about component-level evaluations [here.](https://www.deepeval.com/docs/evaluation-component-level-llm-evals)

&lt;br /&gt;

## Evaluating Without Pytest Integration

Alternatively, you can evaluate without Pytest, which is more suited for a notebook environment.

```python
from deepeval import evaluate
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)
test_case = LLMTestCase(
    input=&quot;What if these shoes don&#039;t fit?&quot;,
    # Replace this with the actual output from your LLM application
    actual_output=&quot;We offer a 30-day full refund at no extra costs.&quot;,
    retrieval_context=[&quot;All customers are eligible for a 30 day full refund at no extra costs.&quot;]
)
evaluate([test_case], [answer_relevancy_metric])
```

## Using Standalone Metrics

DeepEval is extremely modular, making it easy for anyone to use any of our metrics. Continuing from the previous example:

```python
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)
test_case = LLMTestCase(
    input=&quot;What if these shoes don&#039;t fit?&quot;,
    # Replace this with the actual output from your LLM application
    actual_output=&quot;We offer a 30-day full refund at no extra costs.&quot;,
    retrieval_context=[&quot;All customers are eligible for a 30 day full refund at no extra costs.&quot;]
)

answer_relevancy_metric.measure(test_case)
print(answer_relevancy_metric.score)
# All metrics also offer an explanation
print(answer_relevancy_metric.reason)
```

Note that some metrics are for RAG pipelines, while others are for fine-tuning. Make sure to use our docs to pick the right one for your use case.

## Evaluating a Dataset / Test Cases in Bulk

In DeepEval, a dataset is simply a collection of test cases. Here is how you can evaluate these in bulk:

```python
import pytest
from deepeval import assert_test
from deepeval.dataset import EvaluationDataset, Golden
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

dataset = EvaluationDataset(goldens=[Golden(input=&quot;What&#039;s the weather like today?&quot;)])

for golden in dataset.goldens:
    test_case = LLMTestCase(
        input=golden.input,
        actual_output=your_llm_app(golden.input)
    )
    dataset.add_test_case(test_case)

@pytest.mark.parametrize(
    &quot;test_case&quot;,
    dataset,
)
def test_customer_chatbot(test_case: LLMTestCase):
    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)
    assert_test(test_case, [answer_relevancy_metric])
```

```bash
# Run this in the CLI, you can also add an optional -n flag to run tests in parallel
deepeval test run test_&lt;filename&gt;.py -n 4
```

&lt;br/&gt;

Alternatively, although we recommend using `deepeval test run`, you can evaluate a dataset/test cases without using our Pytest integration:

```python
from deepeval import evaluate
...

evaluate(dataset, [answer_relevancy_metric])
# or
dataset.evaluate([answer_relevancy_metric])
```

## A Note on Env Variables (.env / .env.local)

DeepEval auto-loads `.env.local` then `.env` from the current working directory **at import time**.
**Precedence:** process env -&gt; `.env.local` -&gt; `.env`.
Opt out with `DEEPEVAL_DISABLE_DOTENV=1`.

```bash
cp .env.example .env.local
# then edit .env.local (ignored by git)
```

# DeepEval With Confident AI

DeepEval&#039;s cloud platform, [Confident AI](https://confident-ai.com?utm_source=Github), allows you to:

1. Curate/annotate evaluation datasets on the cloud
2. Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best
3. Fine-tune metrics for custom results
4. Debug evaluation results via LLM traces
5. Monitor &amp; evaluate LLM responses in product to improve datasets with real-world data
6. Repeat until perfection

Everything on Confident AI, including how to use Confident is available [here](https://www.confident-ai.com/docs?utm_source=GitHub).

To begin, login from the CLI:

```bash
deepeval login
```

Follow the instructions to log in, create your account, and paste your API key into the CLI.

Now, run your test file again:

```bash
deepeval test run test_chatbot.py
```

You should see a link displayed in the CLI once the test has finished running. Paste it into your browser to view the results!

![Demo GIF](assets/demo.gif)

&lt;br /&gt;

## Configuration

### Environment variables via .env files

Using `.env.local` or `.env` is optional. If they are missing, DeepEval uses your existing environment variables. When present, dotenv environment variables are auto-loaded at import time (unless you set `DEEPEVAL_DISABLE_DOTENV=1`).

**Precedence:** process env -&gt; `.env.local` -&gt; `.env`

```bash
cp .env.example .env.local
# then edit .env.local (ignored by git)

&lt;br /&gt;

# Contributing

Please read [CONTRIBUTING.md](https://github.com/confident-ai/deepeval/blob/main/CONTRIBUTING.md) for details on our code of conduct, and the process for submitting pull requests to us.

&lt;br /&gt;

# Roadmap

Features:

- [x] Integration with Confident AI
- [x] Implement G-Eval
- [x] Implement RAG metrics
- [x] Implement Conversational metrics
- [x] Evaluation Dataset Creation
- [x] Red-Teaming
- [ ] DAG custom metrics
- [ ] Guardrails

&lt;br /&gt;

# Authors

Built by the founders of Confident AI. Contact jeffreyip@confident-ai.com for all enquiries.

&lt;br /&gt;

# License

DeepEval is licensed under Apache 2.0 - see the [LICENSE.md](https://github.com/confident-ai/deepeval/blob/main/LICENSE.md) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[yt-dlp/yt-dlp]]></title>
            <link>https://github.com/yt-dlp/yt-dlp</link>
            <guid>https://github.com/yt-dlp/yt-dlp</guid>
            <pubDate>Sun, 28 Sep 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[A feature-rich command-line audio/video downloader]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/yt-dlp/yt-dlp">yt-dlp/yt-dlp</a></h1>
            <p>A feature-rich command-line audio/video downloader</p>
            <p>Language: Python</p>
            <p>Stars: 128,334</p>
            <p>Forks: 10,278</p>
            <p>Stars today: 200 stars today</p>
            <h2>README</h2><pre>&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
&lt;div align=&quot;center&quot;&gt;

[![YT-DLP](https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/.github/banner.svg)](#readme)

[![Release version](https://img.shields.io/github/v/release/yt-dlp/yt-dlp?color=brightgreen&amp;label=Download&amp;style=for-the-badge)](#installation &quot;Installation&quot;)
[![PyPI](https://img.shields.io/badge/-PyPI-blue.svg?logo=pypi&amp;labelColor=555555&amp;style=for-the-badge)](https://pypi.org/project/yt-dlp &quot;PyPI&quot;)
[![Donate](https://img.shields.io/badge/_-Donate-red.svg?logo=githubsponsors&amp;labelColor=555555&amp;style=for-the-badge)](Collaborators.md#collaborators &quot;Donate&quot;)
[![Discord](https://img.shields.io/discord/807245652072857610?color=blue&amp;labelColor=555555&amp;label=&amp;logo=discord&amp;style=for-the-badge)](https://discord.gg/H5MNcFW63r &quot;Discord&quot;)
[![Supported Sites](https://img.shields.io/badge/-Supported_Sites-brightgreen.svg?style=for-the-badge)](supportedsites.md &quot;Supported Sites&quot;)
[![License: Unlicense](https://img.shields.io/badge/-Unlicense-blue.svg?style=for-the-badge)](LICENSE &quot;License&quot;)
[![CI Status](https://img.shields.io/github/actions/workflow/status/yt-dlp/yt-dlp/core.yml?branch=master&amp;label=Tests&amp;style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/actions &quot;CI Status&quot;)
[![Commits](https://img.shields.io/github/commit-activity/m/yt-dlp/yt-dlp?label=commits&amp;style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/commits &quot;Commit History&quot;)
[![Last Commit](https://img.shields.io/github/last-commit/yt-dlp/yt-dlp/master?label=&amp;style=for-the-badge&amp;display_timestamp=committer)](https://github.com/yt-dlp/yt-dlp/pulse/monthly &quot;Last activity&quot;)

&lt;/div&gt;
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

yt-dlp is a feature-rich command-line audio/video downloader with support for [thousands of sites](supportedsites.md). The project is a fork of [youtube-dl](https://github.com/ytdl-org/youtube-dl) based on the now inactive [youtube-dlc](https://github.com/blackjack4494/yt-dlc).

&lt;!-- MANPAGE: MOVE &quot;USAGE AND OPTIONS&quot; SECTION HERE --&gt;

&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
* [INSTALLATION](#installation)
    * [Detailed instructions](https://github.com/yt-dlp/yt-dlp/wiki/Installation)
    * [Release Files](#release-files)
    * [Update](#update)
    * [Dependencies](#dependencies)
    * [Compile](#compile)
* [USAGE AND OPTIONS](#usage-and-options)
    * [General Options](#general-options)
    * [Network Options](#network-options)
    * [Geo-restriction](#geo-restriction)
    * [Video Selection](#video-selection)
    * [Download Options](#download-options)
    * [Filesystem Options](#filesystem-options)
    * [Thumbnail Options](#thumbnail-options)
    * [Internet Shortcut Options](#internet-shortcut-options)
    * [Verbosity and Simulation Options](#verbosity-and-simulation-options)
    * [Workarounds](#workarounds)
    * [Video Format Options](#video-format-options)
    * [Subtitle Options](#subtitle-options)
    * [Authentication Options](#authentication-options)
    * [Post-processing Options](#post-processing-options)
    * [SponsorBlock Options](#sponsorblock-options)
    * [Extractor Options](#extractor-options)
    * [Preset Aliases](#preset-aliases)
* [CONFIGURATION](#configuration)
    * [Configuration file encoding](#configuration-file-encoding)
    * [Authentication with netrc](#authentication-with-netrc)
    * [Notes about environment variables](#notes-about-environment-variables)
* [OUTPUT TEMPLATE](#output-template)
    * [Output template examples](#output-template-examples)
* [FORMAT SELECTION](#format-selection)
    * [Filtering Formats](#filtering-formats)
    * [Sorting Formats](#sorting-formats)
    * [Format Selection examples](#format-selection-examples)
* [MODIFYING METADATA](#modifying-metadata)
    * [Modifying metadata examples](#modifying-metadata-examples)
* [EXTRACTOR ARGUMENTS](#extractor-arguments)
* [PLUGINS](#plugins)
    * [Installing Plugins](#installing-plugins)
    * [Developing Plugins](#developing-plugins)
* [EMBEDDING YT-DLP](#embedding-yt-dlp)
    * [Embedding examples](#embedding-examples)
* [CHANGES FROM YOUTUBE-DL](#changes-from-youtube-dl)
    * [New features](#new-features)
    * [Differences in default behavior](#differences-in-default-behavior)
    * [Deprecated options](#deprecated-options)
* [CONTRIBUTING](CONTRIBUTING.md#contributing-to-yt-dlp)
    * [Opening an Issue](CONTRIBUTING.md#opening-an-issue)
    * [Developer Instructions](CONTRIBUTING.md#developer-instructions)
* [WIKI](https://github.com/yt-dlp/yt-dlp/wiki)
    * [FAQ](https://github.com/yt-dlp/yt-dlp/wiki/FAQ)
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;


# INSTALLATION

&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
[![Windows](https://img.shields.io/badge/-Windows_x64-blue.svg?style=for-the-badge&amp;logo=windows)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe)
[![Unix](https://img.shields.io/badge/-Linux/BSD-red.svg?style=for-the-badge&amp;logo=linux)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp)
[![MacOS](https://img.shields.io/badge/-MacOS-lightblue.svg?style=for-the-badge&amp;logo=apple)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos)
[![PyPI](https://img.shields.io/badge/-PyPI-blue.svg?logo=pypi&amp;labelColor=555555&amp;style=for-the-badge)](https://pypi.org/project/yt-dlp)
[![Source Tarball](https://img.shields.io/badge/-Source_tar-green.svg?style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)
[![Other variants](https://img.shields.io/badge/-Other-grey.svg?style=for-the-badge)](#release-files)
[![All versions](https://img.shields.io/badge/-All_Versions-lightgrey.svg?style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/releases)
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

You can install yt-dlp using [the binaries](#release-files), [pip](https://pypi.org/project/yt-dlp) or one using a third-party package manager. See [the wiki](https://github.com/yt-dlp/yt-dlp/wiki/Installation) for detailed instructions


&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
## RELEASE FILES

#### Recommended

File|Description
:---|:---
[yt-dlp](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp)|Platform-independent [zipimport](https://docs.python.org/3/library/zipimport.html) binary. Needs Python (recommended for **Linux/BSD**)
[yt-dlp.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe)|Windows (Win8+) standalone x64 binary (recommended for **Windows**)
[yt-dlp_macos](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos)|Universal MacOS (10.15+) standalone executable (recommended for **MacOS**)

#### Alternatives

File|Description
:---|:---
[yt-dlp_linux](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux)|Linux (glibc 2.17+) standalone x86_64 binary
[yt-dlp_linux.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux.zip)|Unpackaged Linux (glibc 2.17+) x86_64 executable (no auto-update)
[yt-dlp_linux_aarch64](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_aarch64)|Linux (glibc 2.17+) standalone aarch64 binary
[yt-dlp_linux_aarch64.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_aarch64.zip)|Unpackaged Linux (glibc 2.17+) aarch64 executable (no auto-update)
[yt-dlp_linux_armv7l.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_armv7l.zip)|Unpackaged Linux (glibc 2.31+) armv7l executable (no auto-update)
[yt-dlp_musllinux](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux)|Linux (musl 1.2+) standalone x86_64 binary
[yt-dlp_musllinux.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux.zip)|Unpackaged Linux (musl 1.2+) x86_64 executable (no auto-update)
[yt-dlp_musllinux_aarch64](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux_aarch64)|Linux (musl 1.2+) standalone aarch64 binary
[yt-dlp_musllinux_aarch64.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux_aarch64.zip)|Unpackaged Linux (musl 1.2+) aarch64 executable (no auto-update)
[yt-dlp_x86.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_x86.exe)|Windows (Win8+) standalone x86 (32-bit) binary
[yt-dlp_win_x86.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win_x86.zip)|Unpackaged Windows (Win8+) x86 (32-bit) executable (no auto-update)
[yt-dlp_arm64.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_arm64.exe)|Windows (Win10+) standalone ARM64 binary
[yt-dlp_win_arm64.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win_arm64.zip)|Unpackaged Windows (Win10+) ARM64 executable (no auto-update)
[yt-dlp_win.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win.zip)|Unpackaged Windows (Win8+) x64 executable (no auto-update)
[yt-dlp_macos.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos.zip)|Unpackaged MacOS (10.15+) executable (no auto-update)

#### Misc

File|Description
:---|:---
[yt-dlp.tar.gz](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)|Source tarball
[SHA2-512SUMS](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS)|GNU-style SHA512 sums
[SHA2-512SUMS.sig](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS.sig)|GPG signature file for SHA512 sums
[SHA2-256SUMS](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS)|GNU-style SHA256 sums
[SHA2-256SUMS.sig](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS.sig)|GPG signature file for SHA256 sums

The public key that can be used to verify the GPG signatures is [available here](https://github.com/yt-dlp/yt-dlp/blob/master/public.key)
Example usage:
```
curl -L https://github.com/yt-dlp/yt-dlp/raw/master/public.key | gpg --import
gpg --verify SHA2-256SUMS.sig SHA2-256SUMS
gpg --verify SHA2-512SUMS.sig SHA2-512SUMS
```

#### Licensing

While yt-dlp is licensed under the [Unlicense](LICENSE), many of the release files contain code from other projects with different licenses.

Most notably, the PyInstaller-bundled executables include GPLv3+ licensed code, and as such the combined work is licensed under [GPLv3+](https://www.gnu.org/licenses/gpl-3.0.html).

See [THIRD_PARTY_LICENSES.txt](THIRD_PARTY_LICENSES.txt) for details.

The zipimport binary (`yt-dlp`), the source tarball (`yt-dlp.tar.gz`), and the PyPI source distribution &amp; wheel only contain code licensed under the [Unlicense](LICENSE).

&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

**Note**: The manpages, shell completion (autocomplete) files etc. are available inside the [source tarball](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)


## UPDATE
You can use `yt-dlp -U` to update if you are using the [release binaries](#release-files)

If you [installed with pip](https://github.com/yt-dlp/yt-dlp/wiki/Installation#with-pip), simply re-run the same command that was used to install the program

For other third-party package managers, see [the wiki](https://github.com/yt-dlp/yt-dlp/wiki/Installation#third-party-package-managers) or refer to their documentation

&lt;a id=&quot;update-channels&quot;&gt;&lt;/a&gt;

There are currently three release channels for binaries: `stable`, `nightly` and `master`.

* `stable` is the default channel, and many of its changes have been tested by users of the `nightly` and `master` channels.
* The `nightly` channel has releases scheduled to build every day around midnight UTC, for a snapshot of the project&#039;s new patches and changes. This is the **recommended channel for regular users** of yt-dlp. The `nightly` releases are available from [yt-dlp/yt-dlp-nightly-builds](https://github.com/yt-dlp/yt-dlp-nightly-builds/releases) or as development releases of the `yt-dlp` PyPI package (which can be installed with pip&#039;s `--pre` flag).
* The `master` channel features releases that are built after each push to the master branch, and these will have the very latest fixes and additions, but may also be more prone to regressions. They are available from [yt-dlp/yt-dlp-master-builds](https://github.com/yt-dlp/yt-dlp-master-builds/releases).

When using `--update`/`-U`, a release binary will only update to its current channel.
`--update-to CHANNEL` can be used to switch to a different channel when a newer version is available. `--update-to [CHANNEL@]TAG` can also be used to upgrade or downgrade to specific tags from a channel.

You may also use `--update-to &lt;repository&gt;` (`&lt;owner&gt;/&lt;repository&gt;`) to update to a channel on a completely different repository. Be careful with what repository you are updating to though, there is no verification done for binaries from different repositories.

Example usage:

* `yt-dlp --update-to master` switch to the `master` channel and update to its latest release
* `yt-dlp --update-to stable@2023.07.06` upgrade/downgrade to release to `stable` channel tag `2023.07.06`
* `yt-dlp --update-to 2023.10.07` upgrade/downgrade to tag `2023.10.07` if it exists on the current channel
* `yt-dlp --update-to example/yt-dlp@2023.09.24` upgrade/downgrade to the release from the `example/yt-dlp` repository, tag `2023.09.24`

**Important**: Any user experiencing an issue with the `stable` release should install or update to the `nightly` release before submitting a bug report:
```
# To update to nightly from stable executable/binary:
yt-dlp --update-to nightly

# To install nightly with pip:
python3 -m pip install -U --pre &quot;yt-dlp[default]&quot;
```

When running a yt-dlp version that is older than 90 days, you will see a warning message suggesting to update to the latest version.
You can suppress this warning by adding `--no-update` to your command or configuration file.

## DEPENDENCIES
Python versions 3.9+ (CPython) and 3.11+ (PyPy) are supported. Other versions and implementations may or may not work correctly.

&lt;!-- Python 3.5+ uses VC++14 and it is already embedded in the binary created
&lt;!x-- https://www.microsoft.com/en-us/download/details.aspx?id=26999 --x&gt;
On Windows, [Microsoft Visual C++ 2010 SP1 Redistributable Package (x86)](https://download.microsoft.com/download/1/6/5/165255E7-1014-4D0A-B094-B6A430A6BFFC/vcredist_x86.exe) is also necessary to run yt-dlp. You probably already have this, but if the executable throws an error due to missing `MSVCR100.dll` you need to install it manually.
--&gt;

While all the other dependencies are optional, `ffmpeg` and `ffprobe` are highly recommended

### Strongly recommended

* [**ffmpeg** and **ffprobe**](https://www.ffmpeg.org) - Required for [merging separate video and audio files](#format-selection), as well as for various [post-processing](#post-processing-options) tasks. License [depends on the build](https://www.ffmpeg.org/legal.html)

    There are bugs in ffmpeg that cause various issues when used alongside yt-dlp. Since ffmpeg is such an important dependency, we provide [custom builds](https://github.com/yt-dlp/FFmpeg-Builds#ffmpeg-static-auto-builds) with patches for some of these issues at [yt-dlp/FFmpeg-Builds](https://github.com/yt-dlp/FFmpeg-Builds). See [the readme](https://github.com/yt-dlp/FFmpeg-Builds#patches-applied) for details on the specific issues solved by these builds

    **Important**: What you need is ffmpeg *binary*, **NOT** [the Python package of the same name](https://pypi.org/project/ffmpeg)

### Networking
* [**certifi**](https://github.com/certifi/python-certifi)\* - Provides Mozilla&#039;s root certificate bundle. Licensed under [MPLv2](https://github.com/certifi/python-certifi/blob/master/LICENSE)
* [**brotli**](https://github.com/google/brotli)\* or [**brotlicffi**](https://github.com/python-hyper/brotlicffi) - [Brotli](https://en.wikipedia.org/wiki/Brotli) content encoding support. Both licensed under MIT &lt;sup&gt;[1](https://github.com/google/brotli/blob/master/LICENSE) [2](https://github.com/python-hyper/brotlicffi/blob/master/LICENSE) &lt;/sup&gt;
* [**websockets**](https://github.com/aaugustin/websockets)\* - For downloading over websocket. Licensed under [BSD-3-Clause](https://github.com/aaugustin/websockets/blob/main/LICENSE)
* [**requests**](https://github.com/psf/requests)\* - HTTP library. For HTTPS proxy and persistent connections support. Licensed under [Apache-2.0](https://github.com/psf/requests/blob/main/LICENSE)

#### Impersonation

The following provide support for impersonating browser requests. This may be required for some sites that employ TLS fingerprinting.

* [**curl_cffi**](https://github.com/lexiforest/curl_cffi) (recommended) - Python binding for [curl-impersonate](https://github.com/lexiforest/curl-impersonate). Provides impersonation targets for Chrome, Edge and Safari. Licensed under [MIT](https://github.com/lexiforest/curl_cffi/blob/main/LICENSE)
  * Can be installed with the `curl-cffi` group, e.g. `pip install &quot;yt-dlp[default,curl-cffi]&quot;`
  * Currently included in most builds *except* `yt-dlp` (Unix zipimport binary), `yt-dlp_x86` (Windows 32-bit) and `yt-dlp_musllinux_aarch64`


### Metadata

* [**mutagen**](https://github.com/quodlibet/mutagen)\* - For `--embed-thumbnail` in certain formats. Licensed under [GPLv2+](https://github.com/quodlibet/mutagen/blob/master/COPYING)
* [**AtomicParsley**](https://github.com/wez/atomicparsley) - For `--embed-thumbnail` in `mp4`/`m4a` files when `mutagen`/`ffmpeg` cannot. Licensed under [GPLv2+](https://github.com/wez/atomicparsley/blob/master/COPYING)
* [**xattr**](https://github.com/xattr/xattr), [**pyxattr**](https://github.com/iustin/pyxattr) or [**setfattr**](http://savannah.nongnu.org/projects/attr) - For writing xattr metadata (`--xattrs`) on **Mac** and **BSD**. Licensed under [MIT](https://github.com/xattr/xattr/blob/master/LICENSE.txt), [LGPL2.1](https://github.com/iustin/pyxattr/blob/master/COPYING) and [GPLv2+](http://git.savannah.nongnu.org/cgit/attr.git/tree/doc/COPYING) respectively

### Misc

* [**pycryptodomex**](https://github.com/Legrandin/pycryptodome)\* - For decrypting AES-128 HLS streams and various other data. Licensed under [BSD-2-Clause](https://github.com/Legrandin/pycryptodome/blob/master/LICENSE.rst)
* [**phantomjs**](https://github.com/ariya/phantomjs) - Used in extractors where javascript needs to be run. Licensed under [BSD-3-Clause](https://github.com/ariya/phantomjs/blob/master/LICENSE.BSD)
* [**secretstorage**](https://github.com/mitya57/secretstorage)\* - For `--cookies-from-browser` to access the **Gnome** keyring while decrypting cookies of **Chromium**-based browsers on **Linux**. Licensed under [BSD-3-Clause](https://github.com/mitya57/secretstorage/blob/master/LICENSE)
* Any external downloader that you want to use with `--downloader`

### Deprecated

* [**rtmpdump**](http://rtmpdump.mplayerhq.hu) - For downloading `rtmp` streams. ffmpeg can be used instead with `--downloader ffmpeg`. Licensed under [GPLv2+](http://rtmpdump.mplayerhq.hu)
* [**mplayer**](http://mplayerhq.hu/design7/info.html) or [**mpv**](https://mpv.io) - For downloading `rstp`/`mms` streams. ffmpeg can be used instead with `--downloader ffmpeg`. Licensed under [GPLv2+](https://github.com/mpv-player/mpv/blob/master/Copyright)

To use or redistribute the dependencies, you must agree to their respective licensing terms.

The standalone release binaries are built with the Python interpreter and the packages marked with **\*** included.

If you do not have the necessary dependencies for a task you are attempting, yt-dlp will warn you. All the currently available dependencies are visible at the top of the `--verbose` output


## COMPILE

### Standalone PyInstaller Builds
To build the standalone executable, you must have Python and `pyinstaller` (plus any of yt-dlp&#039;s [optional dependencies](#dependencies) if needed). The executable will be built for the same CPU architecture as the Python used.

You can run the following commands:

```
python3 devscripts/install_deps.py --include pyinstaller
python3 devscripts/make_lazy_extractors.py
python3 -m bundle.pyinstaller
```

On some systems, you may n

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hacksider/Deep-Live-Cam]]></title>
            <link>https://github.com/hacksider/Deep-Live-Cam</link>
            <guid>https://github.com/hacksider/Deep-Live-Cam</guid>
            <pubDate>Sun, 28 Sep 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[real time face swap and one-click video deepfake with only a single image]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hacksider/Deep-Live-Cam">hacksider/Deep-Live-Cam</a></h1>
            <p>real time face swap and one-click video deepfake with only a single image</p>
            <p>Language: Python</p>
            <p>Stars: 73,439</p>
            <p>Forks: 10,667</p>
            <p>Stars today: 51 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;Deep-Live-Cam&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  Real-time face swap and video deepfake with a single click and only a single image.
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/11395&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11395&quot; alt=&quot;hacksider%2FDeep-Live-Cam | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/demo.gif&quot; alt=&quot;Demo GIF&quot; width=&quot;800&quot;&gt;
&lt;/p&gt;

##  Disclaimer

This deepfake software is designed to be a productive tool for the AI-generated media industry. It can assist artists in animating custom characters, creating engaging content, and even using models for clothing design.

We are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to the law and ethics. We may shut down the project or add watermarks if legally required.

- Ethical Use: Users are expected to use this software responsibly and legally. If using a real person&#039;s face, obtain their consent and clearly label any output as a deepfake when sharing online.

- Content Restrictions: The software includes built-in checks to prevent processing inappropriate media, such as nudity, graphic content, or sensitive material.

- Legal Compliance: We adhere to all relevant laws and ethical guidelines. If legally required, we may shut down the project or add watermarks to the output.

- User Responsibility: We are not responsible for end-user actions. Users must ensure their use of the software aligns with ethical standards and legal requirements.

By using this software, you agree to these terms and commit to using it in a manner that respects the rights and dignity of others.

Users are expected to use this software responsibly and legally. If using a real person&#039;s face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user actions.

## Exclusive v2.2 Quick Start - Pre-built (Windows/Mac Silicon)

  &lt;a href=&quot;https://deeplivecam.net/index.php/quickstart&quot;&gt; &lt;img src=&quot;media/Download.png&quot; width=&quot;285&quot; height=&quot;77&quot; /&gt;

##### This is the fastest build you can get if you have a discrete NVIDIA or AMD GPU or Mac Silicon, And you&#039;ll receive special priority support.
 
###### These Pre-builts are perfect for non-technical users or those who don&#039;t have time to, or can&#039;t manually install all the requirements. Just a heads-up: this is an open-source project, so you can also install it manually. 

## TLDR; Live Deepfake in just 3 Clicks
![easysteps](https://github.com/user-attachments/assets/af825228-852c-411b-b787-ffd9aac72fc6)
1. Select a face
2. Select which camera to use
3. Press live!

## Features &amp; Uses - Everything is in real-time

### Mouth Mask

**Retain your original mouth for accurate movement using Mouth Mask**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/ludwig.gif&quot; alt=&quot;resizable-gif&quot;&gt;
&lt;/p&gt;

### Face Mapping

**Use different faces on multiple subjects simultaneously**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/streamers.gif&quot; alt=&quot;face_mapping_source&quot;&gt;
&lt;/p&gt;

### Your Movie, Your Face

**Watch movies with any face in real-time**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/movie.gif&quot; alt=&quot;movie&quot;&gt;
&lt;/p&gt;

### Live Show

**Run Live shows and performances**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/live_show.gif&quot; alt=&quot;show&quot;&gt;
&lt;/p&gt;

### Memes

**Create Your Most Viral Meme Yet**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/meme.gif&quot; alt=&quot;show&quot; width=&quot;450&quot;&gt; 
  &lt;br&gt;
  &lt;sub&gt;Created using Many Faces feature in Deep-Live-Cam&lt;/sub&gt;
&lt;/p&gt;

### Omegle

**Surprise people on Omegle**

&lt;p align=&quot;center&quot;&gt;
  &lt;video src=&quot;https://github.com/user-attachments/assets/2e9b9b82-fa04-4b70-9f56-b1f68e7672d0&quot; width=&quot;450&quot; controls&gt;&lt;/video&gt;
&lt;/p&gt;

## Installation (Manual)

**Please be aware that the installation requires technical skills and is not for beginners. Consider downloading the quickstart version.**

&lt;details&gt;
&lt;summary&gt;Click to see the process&lt;/summary&gt;

### Installation

This is more likely to work on your computer but will be slower as it utilizes the CPU.

**1. Set up Your Platform**

-   Python (3.11 recommended)
-   pip
-   git
-   [ffmpeg](https://www.youtube.com/watch?v=OlNWCpFdVMA) - ```iex (irm ffmpeg.tc.ht)```
-   [Visual Studio 2022 Runtimes (Windows)](https://visualstudio.microsoft.com/visual-cpp-build-tools/)

**2. Clone the Repository**

```bash
git clone https://github.com/hacksider/Deep-Live-Cam.git
cd Deep-Live-Cam
```

**3. Download the Models**

1. [GFPGANv1.4](https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth)
2. [inswapper\_128\_fp16.onnx](https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx)

Place these files in the &quot;**models**&quot; folder.

**4. Install Dependencies**

We highly recommend using a `venv` to avoid issues.


For Windows:
```bash
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
```
For Linux:
```bash
# Ensure you use the installed Python 3.10
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

**For macOS:**

Apple Silicon (M1/M2/M3) requires specific setup:

```bash
# Install Python 3.11 (specific version is important)
brew install python@3.11

# Install tkinter package (required for the GUI)
brew install python-tk@3.10

# Create and activate virtual environment with Python 3.11
python3.11 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

** In case something goes wrong and you need to reinstall the virtual environment **

```bash
# Deactivate the virtual environment
rm -rf venv

# Reinstall the virtual environment
python -m venv venv
source venv/bin/activate

# install the dependencies again
pip install -r requirements.txt

# gfpgan and basicsrs issue fix
pip install git+https://github.com/xinntao/BasicSR.git@master
pip uninstall gfpgan -y
pip install git+https://github.com/TencentARC/GFPGAN.git@master
```

**Run:** If you don&#039;t have a GPU, you can run Deep-Live-Cam using `python run.py`. Note that initial execution will download models (~300MB).

### GPU Acceleration

**CUDA Execution Provider (Nvidia)**

1. Install [CUDA Toolkit 12.8.0](https://developer.nvidia.com/cuda-12-8-0-download-archive)
2. Install [cuDNN v8.9.7 for CUDA 12.x](https://developer.nvidia.com/rdp/cudnn-archive) (required for onnxruntime-gpu):
   - Download cuDNN v8.9.7 for CUDA 12.x
   - Make sure the cuDNN bin directory is in your system PATH
3. Install dependencies:

```bash
pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu==1.21.0
```

3. Usage:

```bash
python run.py --execution-provider cuda
```

**CoreML Execution Provider (Apple Silicon)**

Apple Silicon (M1/M2/M3) specific installation:

1. Make sure you&#039;ve completed the macOS setup above using Python 3.10.
2. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-silicon
pip install onnxruntime-silicon==1.13.1
```

3. Usage (important: specify Python 3.10):

```bash
python3.10 run.py --execution-provider coreml
```

**Important Notes for macOS:**
- You **must** use Python 3.10, not newer versions like 3.11 or 3.13
- Always run with `python3.10` command not just `python` if you have multiple Python versions installed
- If you get error about `_tkinter` missing, reinstall the tkinter package: `brew reinstall python-tk@3.10`
- If you get model loading errors, check that your models are in the correct folder
- If you encounter conflicts with other Python versions, consider uninstalling them:
  ```bash
  # List all installed Python versions
  brew list | grep python
  
  # Uninstall conflicting versions if needed
  brew uninstall --ignore-dependencies python@3.11 python@3.13
  
  # Keep only Python 3.11
  brew cleanup
  ```

**CoreML Execution Provider (Apple Legacy)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-coreml
pip install onnxruntime-coreml==1.21.0
```

2. Usage:

```bash
python run.py --execution-provider coreml
```

**DirectML Execution Provider (Windows)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-directml
pip install onnxruntime-directml==1.21.0
```

2. Usage:

```bash
python run.py --execution-provider directml
```

**OpenVINO‚Ñ¢ Execution Provider (Intel)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-openvino
pip install onnxruntime-openvino==1.21.0
```

2. Usage:

```bash
python run.py --execution-provider openvino
```
&lt;/details&gt;

## Usage

**1. Image/Video Mode**

-   Execute `python run.py`.
-   Choose a source face image and a target image/video.
-   Click &quot;Start&quot;.
-   The output will be saved in a directory named after the target video.

**2. Webcam Mode**

-   Execute `python run.py`.
-   Select a source face image.
-   Click &quot;Live&quot;.
-   Wait for the preview to appear (10-30 seconds).
-   Use a screen capture tool like OBS to stream.
-   To change the face, select a new source image.

## Command Line Arguments (Unmaintained)

```
options:
  -h, --help                                               show this help message and exit
  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image
  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video
  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory
  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)
  --keep-fps                                               keep original fps
  --keep-audio                                             keep original audio
  --keep-frames                                            keep temporary frames
  --many-faces                                             process every face
  --map-faces                                              map source target faces
  --mouth-mask                                             mask the mouth region
  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder
  --video-quality [0-51]                                   adjust output video quality
  --live-mirror                                            the live camera display as you see it in the front-facing camera frame
  --live-resizable                                         the live camera frame is resizable
  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB
  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)
  --execution-threads EXECUTION_THREADS                    number of execution threads
  -v, --version                                            show program&#039;s version number and exit
```

Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.

## Press

**We are always open to criticism and are ready to improve, that&#039;s why we didn&#039;t cherry-pick anything.**

 - [*&quot;Deep-Live-Cam goes viral, allowing anyone to become a digital doppelganger&quot;*](https://arstechnica.com/information-technology/2024/08/new-ai-tool-enables-real-time-face-swapping-on-webcams-raising-fraud-concerns/) - Ars Technica
 - [*&quot;Thanks Deep Live Cam, shapeshifters are among us now&quot;*](https://dataconomy.com/2024/08/15/what-is-deep-live-cam-github-deepfake/) - Dataconomy
 - [*&quot;This free AI tool lets you become anyone during video-calls&quot;*](https://www.newsbytesapp.com/news/science/deep-live-cam-ai-impersonation-tool-goes-viral/story) - NewsBytes
 - [*&quot;OK, this viral AI live stream software is truly terrifying&quot;*](https://www.creativebloq.com/ai/ok-this-viral-ai-live-stream-software-is-truly-terrifying) - Creative Bloq
 - [*&quot;Deepfake AI Tool Lets You Become Anyone in a Video Call With Single Photo&quot;*](https://petapixel.com/2024/08/14/deep-live-cam-deepfake-ai-tool-lets-you-become-anyone-in-a-video-call-with-single-photo-mark-zuckerberg-jd-vance-elon-musk/) - PetaPixel
 - [*&quot;Deep-Live-Cam Uses AI to Transform Your Face in Real-Time, Celebrities Included&quot;*](https://www.techeblog.com/deep-live-cam-ai-transform-face/) - TechEBlog
 - [*&quot;An AI tool that &quot;makes you look like anyone&quot; during a video call is going viral online&quot;*](https://telegrafi.com/en/a-tool-that-makes-you-look-like-anyone-during-a-video-call-is-going-viral-on-the-Internet/) - Telegrafi
 - [*&quot;This Deepfake Tool Turning Images Into Livestreams is Topping the GitHub Charts&quot;*](https://decrypt.co/244565/this-deepfake-tool-turning-images-into-livestreams-is-topping-the-github-charts) - Emerge
 - [*&quot;New Real-Time Face-Swapping AI Allows Anyone to Mimic Famous Faces&quot;*](https://www.digitalmusicnews.com/2024/08/15/face-swapping-ai-real-time-mimic/) - Digital Music News
 - [*&quot;This real-time webcam deepfake tool raises alarms about the future of identity theft&quot;*](https://www.diyphotography.net/this-real-time-webcam-deepfake-tool-raises-alarms-about-the-future-of-identity-theft/) - DIYPhotography
 - [*&quot;That&#039;s Crazy, Oh God. That&#039;s Fucking Freaky Dude... That&#039;s So Wild Dude&quot;*](https://www.youtube.com/watch?time_continue=1074&amp;v=py4Tc-Y8BcY) - SomeOrdinaryGamers
 - [*&quot;Alright look look look, now look chat, we can do any face we want to look like chat&quot;*](https://www.youtube.com/live/mFsCe7AIxq8?feature=shared&amp;t=2686) - IShowSpeed
 - [*&quot;They do a pretty good job matching poses, expression and even the lighting&quot;*](https://www.youtube.com/watch?v=wnCghLjqv3s&amp;t=551s) - TechLinked (LTT)
 - [*&quot;Als Sean Connery an der Redaktionskonferenz teilnahm&quot;*](https://www.golem.de/news/deepfakes-als-sean-connery-an-der-redaktionskonferenz-teilnahm-2408-188172.html) - Golem.de (German)


## Credits

-   [ffmpeg](https://ffmpeg.org/): for making video-related operations easy
-   [deepinsight](https://github.com/deepinsight): for their [insightface](https://github.com/deepinsight/insightface) project which provided a well-made library and models. Please be reminded that the [use of the model is for non-commercial research purposes only](https://github.com/deepinsight/insightface?tab=readme-ov-file#license).
-   [havok2-htwo](https://github.com/havok2-htwo): for sharing the code for webcam
-   [GosuDRM](https://github.com/GosuDRM): for the open version of roop
-   [pereiraroland26](https://github.com/pereiraroland26): Multiple faces support
-   [vic4key](https://github.com/vic4key): For supporting/contributing to this project
-   [kier007](https://github.com/kier007): for improving the user experience
-   [qitianai](https://github.com/qitianai): for multi-lingual support
-   and [all developers](https://github.com/hacksider/Deep-Live-Cam/graphs/contributors) behind libraries used in this project.
-   Footnote: Please be informed that the base author of the code is [s0md3v](https://github.com/s0md3v/roop)
-   All the wonderful users who helped make this project go viral by starring the repo ‚ù§Ô∏è

[![Stargazers](https://reporoster.com/stars/hacksider/Deep-Live-Cam)](https://github.com/hacksider/Deep-Live-Cam/stargazers)

## Contributions

![Alt](https://repobeats.axiom.co/api/embed/fec8e29c45dfdb9c5916f3a7830e1249308d20e1.svg &quot;Repobeats analytics image&quot;)

## Stars to the Moon üöÄ

&lt;a href=&quot;https://star-history.com/#hacksider/deep-live-cam&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[qodo-ai/pr-agent]]></title>
            <link>https://github.com/qodo-ai/pr-agent</link>
            <guid>https://github.com/qodo-ai/pr-agent</guid>
            <pubDate>Sun, 28 Sep 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[üöÄ PR-Agent: An AI-Powered ü§ñ Tool for Automated Pull Request Analysis, Feedback, Suggestions and More! üíªüîç (For more advanced: check Qodo Merge)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/qodo-ai/pr-agent">qodo-ai/pr-agent</a></h1>
            <p>üöÄ PR-Agent: An AI-Powered ü§ñ Tool for Automated Pull Request Analysis, Feedback, Suggestions and More! üíªüîç (For more advanced: check Qodo Merge)</p>
            <p>Language: Python</p>
            <p>Stars: 9,057</p>
            <p>Forks: 1,105</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;div align=&quot;center&quot;&gt;

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://www.qodo.ai/wp-content/uploads/2025/02/PR-Agent-Purple-2.png&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://www.qodo.ai/wp-content/uploads/2025/02/PR-Agent-Purple-2.png&quot;&gt;
  &lt;img src=&quot;https://codium.ai/images/pr_agent/logo-light.png&quot; alt=&quot;logo&quot; width=&quot;330&quot;&gt;

&lt;/picture&gt;
&lt;br/&gt;

[Installation Guide](https://qodo-merge-docs.qodo.ai/installation/) |
[Usage Guide](https://qodo-merge-docs.qodo.ai/usage-guide/) |
[Tools Guide](https://qodo-merge-docs.qodo.ai/tools/) |
[Qodo Merge](https://qodo-merge-docs.qodo.ai/overview/pr_agent_pro/) üíé

## Open-Source AI-Powered Code Review Tool

**PR-Agent** is a free, open-source tool that provides AI-powered code review and PR assistance. Run it locally, on your CI/CD, or self-host it.

**[Qodo Merge](https://qodo-merge-docs.qodo.ai/overview/pr_agent_pro/) üíé** is the hosted, enterprise version with additional features, zero-setup, and priority support.

---

### Quick Start Options

| **Option** | **Best For** | **Setup Time** | **Cost** |
|------------|--------------|----------------|----------|
| **[PR-Agent (Open Source)](#-quick-start-for-pr-agent-open-source)** | Developers who want full control, self-hosting, or custom integrations | 5-15 minutes | Free |
| **[Qodo Merge](#-try-qodo-merge-zero-setup)** | Teams wanting zero-setup, enhancing the open-source features, additional enterprise features, and managed hosting | 2 minutes | Free tier available |
&lt;/div&gt;

[![Static Badge](https://img.shields.io/badge/Chrome-Extension-violet)](https://chromewebstore.google.com/detail/qodo-merge-ai-powered-cod/ephlnjeghhogofkifjloamocljapahnl)
[![Static Badge](https://img.shields.io/badge/Pro-App-blue)](https://github.com/apps/qodo-merge-pro/)
[![Static Badge](https://img.shields.io/badge/OpenSource-App-red)](https://github.com/apps/qodo-merge-pro-for-open-source/)
[![Discord](https://badgen.net/badge/icon/discord?icon=discord&amp;label&amp;color=purple)](https://discord.com/invite/SgSxuQ65GF)
&lt;a href=&quot;https://github.com/Codium-ai/pr-agent/commits/main&quot;&gt;
&lt;img alt=&quot;GitHub&quot; src=&quot;https://img.shields.io/github/last-commit/Codium-ai/pr-agent/main?style=for-the-badge&quot; height=&quot;20&quot;&gt;
&lt;/a&gt;
&lt;/div&gt;

## Table of Contents

- [PR-Agent vs Qodo Merge](#pr-agent-vs-qodo-merge)
- [Getting Started](#getting-started)
- [Why Use PR-Agent?](#why-use-pr-agent)
- [Features](#features)
- [See It in Action](#see-it-in-action)
- [Try It Now](#try-it-now)
- [Qodo Merge üíé](#qodo-merge-)
- [How It Works](#how-it-works)
- [Data Privacy](#data-privacy)
- [Contributing](#contributing)
- [Links](#links)

## PR-Agent vs Qodo Merge

### PR-Agent (This Repository) - Open Source

‚úÖ **What you get:**
- Complete source code access and customization
- Self-hosted deployment options
- All core AI review tools (`/describe`, `/review`, `/improve`, `/ask`)
- Support for GitHub, GitLab, BitBucket, Azure DevOps
- CLI usage for local development
- Free forever

‚öôÔ∏è **What you need to manage:**
- Your own API keys (OpenAI, Claude, etc.)
- Infrastructure and hosting
- Updates and maintenance
- Configuration management

### Qodo Merge - Hosted Solution

‚úÖ **What you get (everything above plus):**
- Zero-setup installation (2-minute GitHub app install)
- Managed infrastructure and automatic updates
- Advanced features: CI feedback, code suggestions tracking, custom prompts
- Priority support and feature requests
- Enhanced privacy with zero data retention
- Free tier: 75 PR reviews/month per organization

üí∞ **Pricing:**
- Free tier available
- Paid plans for unlimited usage
- [View pricing details](https://www.qodo.ai/pricing/)

**üë®‚Äçüíª Developer Recommendation:** Start with PR-Agent if you want to experiment, customize heavily, or have specific self-hosting requirements. Choose Qodo Merge if you want to focus on coding rather than tool maintenance.

## Getting Started

### üöÄ Quick Start for PR-Agent (Open Source)

#### 1. Try it Instantly (No Setup)
Test PR-Agent on any public GitHub repository by commenting `@CodiumAI-Agent /improve`

#### 2. GitHub Action (Recommended)
Add automated PR reviews to your repository with a simple workflow file:
```yaml
# .github/workflows/pr-agent.yml
name: PR Agent
on:
  pull_request:
    types: [opened, synchronize]
jobs:
  pr_agent_job:
    runs-on: ubuntu-latest
    steps:
    - name: PR Agent action step
      uses: Codium-ai/pr-agent@main
      env:
        OPENAI_KEY: ${{ secrets.OPENAI_KEY }}
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
```
[Full GitHub Action setup guide](https://qodo-merge-docs.qodo.ai/installation/github/#run-as-a-github-action)

#### 3. CLI Usage (Local Development)
Run PR-Agent locally on your repository:
```bash
pip install pr-agent
export OPENAI_KEY=your_key_here
pr-agent --pr_url https://github.com/owner/repo/pull/123 review
```
[Complete CLI setup guide](https://qodo-merge-docs.qodo.ai/usage-guide/automations_and_usage/#local-repo-cli)

#### 4. Other Platforms
- [GitLab webhook setup](https://qodo-merge-docs.qodo.ai/installation/gitlab/)
- [BitBucket app installation](https://qodo-merge-docs.qodo.ai/installation/bitbucket/)
- [Azure DevOps setup](https://qodo-merge-docs.qodo.ai/installation/azure/)

### üíé Try Qodo Merge (Zero Setup)

If you prefer a hosted solution without managing infrastructure:

1. **[Install Qodo Merge GitHub App](https://github.com/marketplace/qodo-merge-pro)** (2 minutes)
2. **[FREE for Open Source](https://github.com/marketplace/qodo-merge-pro-for-open-source)**: Full features, zero cost for public repos
3. **Free Tier**: 75 PR reviews/month for private repos
4. **[View Plans &amp; Pricing](https://www.qodo.ai/pricing/)**

[Complete Qodo Merge setup guide](https://qodo-merge-docs.qodo.ai/installation/qodo_merge/)

### üíª Local IDE Integration
Receive automatic feedback in your IDE after each commit: [Qodo Merge post-commit agent](https://github.com/qodo-ai/agents/tree/main/agents/qodo-merge-post-commit)


[//]: # (## News and Updates)

[//]: # ()
[//]: # (## Aug 8, 2025)

[//]: # ()
[//]: # (Added full support for GPT-5 models. View the [benchmark results]&amp;#40;https://qodo-merge-docs.qodo.ai/pr_benchmark/#pr-benchmark-results&amp;#41; for details on the performance of GPT-5 models in PR-Agent.)

[//]: # ()
[//]: # ()
[//]: # (## Jul 17, 2025)

[//]: # ()
[//]: # (Introducing `/compliance`, a new Qodo Merge üíé tool that runs comprehensive checks for security, ticket requirements, codebase duplication, and custom organizational rules. )

[//]: # ()
[//]: # (&lt;img width=&quot;384&quot; alt=&quot;compliance-image&quot; src=&quot;https://codium.ai/images/pr_agent/compliance_partial.png&quot;/&gt;)

[//]: # ()
[//]: # (Read more about it [here]&amp;#40;https://qodo-merge-docs.qodo.ai/tools/compliance/&amp;#41;)

[//]: # ()
[//]: # ()
[//]: # (## Jul 1, 2025)

[//]: # (You can now receive automatic feedback from Qodo Merge in your local IDE after each commit. Read more about it [here]&amp;#40;https://github.com/qodo-ai/agents/tree/main/agents/qodo-merge-post-commit&amp;#41;.)

[//]: # ()
[//]: # ()
[//]: # (## Jun 21, 2025)

[//]: # ()
[//]: # (v0.30 was [released]&amp;#40;https://github.com/qodo-ai/pr-agent/releases&amp;#41;)

[//]: # ()
[//]: # ()
[//]: # (## Jun 3, 2025)

[//]: # ()
[//]: # (Qodo Merge now offers a simplified free tier üíé.)

[//]: # (Organizations can use Qodo Merge at no cost, with a [monthly limit]&amp;#40;https://qodo-merge-docs.qodo.ai/installation/qodo_merge/#cloud-users&amp;#41; of 75 PR reviews per organization.)

[//]: # ()
[//]: # ()
[//]: # (## Apr 30, 2025)

[//]: # ()
[//]: # (A new feature is now available in the `/improve` tool for Qodo Merge üíé - Chat on code suggestions.)

[//]: # ()
[//]: # (&lt;img width=&quot;512&quot; alt=&quot;image&quot; src=&quot;https://codium.ai/images/pr_agent/improve_chat_on_code_suggestions_ask.png&quot; /&gt;)

[//]: # ()
[//]: # (Read more about it [here]&amp;#40;https://qodo-merge-docs.qodo.ai/tools/improve/#chat-on-code-suggestions&amp;#41;.)

[//]: # ()
[//]: # ()
[//]: # (## Apr 16, 2025)

[//]: # ()
[//]: # (New tool for Qodo Merge üíé - `/scan_repo_discussions`.)

[//]: # ()
[//]: # (&lt;img width=&quot;635&quot; alt=&quot;image&quot; src=&quot;https://codium.ai/images/pr_agent/scan_repo_discussions_2.png&quot; /&gt;)

[//]: # ()
[//]: # (Read more about it [here]&amp;#40;https://qodo-merge-docs.qodo.ai/tools/scan_repo_discussions/&amp;#41;.)

## Why Use PR-Agent?

### üéØ Built for Real Development Teams

**Fast &amp; Affordable**: Each tool (`/review`, `/improve`, `/ask`) uses a single LLM call (~30 seconds, low cost)

**Handles Any PR Size**: Our [PR Compression strategy](https://qodo-merge-docs.qodo.ai/core-abilities/#pr-compression-strategy) effectively processes both small and large PRs

**Highly Customizable**: JSON-based prompting allows easy customization of review categories and behavior via [configuration files](pr_agent/settings/configuration.toml)

**Platform Agnostic**: 
- **Git Providers**: GitHub, GitLab, BitBucket, Azure DevOps, Gitea
- **Deployment**: CLI, GitHub Actions, Docker, self-hosted, webhooks
- **AI Models**: OpenAI GPT, Claude, Deepseek, and more

**Open Source Benefits**:
- Full control over your data and infrastructure
- Customize prompts and behavior for your team&#039;s needs
- No vendor lock-in
- Community-driven development

## Features

&lt;div style=&quot;text-align:left;&quot;&gt;

PR-Agent and Qodo Merge offer comprehensive pull request functionalities integrated with various git providers:

|                                                         |                                                                                        | GitHub | GitLab | Bitbucket | Azure DevOps | Gitea |
|---------------------------------------------------------|----------------------------------------------------------------------------------------|:------:|:------:|:---------:|:------------:|:-----:|
| [TOOLS](https://qodo-merge-docs.qodo.ai/tools/)         | [Describe](https://qodo-merge-docs.qodo.ai/tools/describe/)                            |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |  ‚úÖ   |
|                                                         | [Review](https://qodo-merge-docs.qodo.ai/tools/review/)                                |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |  ‚úÖ   |
|                                                         | [Improve](https://qodo-merge-docs.qodo.ai/tools/improve/)                              |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |  ‚úÖ   |
|                                                         | [Ask](https://qodo-merge-docs.qodo.ai/tools/ask/)                                      |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |       |
|                                                         | ‚Æë [Ask on code lines](https://qodo-merge-docs.qodo.ai/tools/ask/#ask-lines)            |   ‚úÖ   |   ‚úÖ   |           |              |       |
|                                                         | [Help Docs](https://qodo-merge-docs.qodo.ai/tools/help_docs/?h=auto#auto-approval)     |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |              |       |
|                                                         | [Update CHANGELOG](https://qodo-merge-docs.qodo.ai/tools/update_changelog/)            |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |       |
|                                                         | [Add Documentation](https://qodo-merge-docs.qodo.ai/tools/documentation/) üíé           |   ‚úÖ   |   ‚úÖ   |           |              |       |
|                                                         | [Analyze](https://qodo-merge-docs.qodo.ai/tools/analyze/) üíé                           |   ‚úÖ   |   ‚úÖ   |           |              |       |
|                                                         | [Auto-Approve](https://qodo-merge-docs.qodo.ai/tools/improve/?h=auto#auto-approval) üíé |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |              |       |
|                                                         | [CI Feedback](https://qodo-merge-docs.qodo.ai/tools/ci_feedback/) üíé                   |   ‚úÖ   |        |           |              |       |
|                                                         | [Compliance](https://qodo-merge-docs.qodo.ai/tools/compliance/) üíé                     |   ‚úÖ   |    ‚úÖ   |    ‚úÖ     |              |       |
|                                                         | [Custom Prompt](https://qodo-merge-docs.qodo.ai/tools/custom_prompt/) üíé                                            |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |              |       |
|                                                         | [Generate Custom Labels](https://qodo-merge-docs.qodo.ai/tools/custom_labels/) üíé                                   |   ‚úÖ   |   ‚úÖ   |           |              |       |
|                                                         | [Generate Tests](https://qodo-merge-docs.qodo.ai/tools/test/) üíé                                                    |   ‚úÖ   |   ‚úÖ   |           |              |       |
|                                                         | [Implement](https://qodo-merge-docs.qodo.ai/tools/implement/) üíé                                                    |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |              |       |
|                                                         | [Scan Repo Discussions](https://qodo-merge-docs.qodo.ai/tools/scan_repo_discussions/) üíé                            |   ‚úÖ   |        |           |              |       |
|                                                         | [Similar Code](https://qodo-merge-docs.qodo.ai/tools/similar_code/) üíé                                              |   ‚úÖ   |        |           |              |       |
|                                                         | [Utilizing Best Practices](https://qodo-merge-docs.qodo.ai/tools/improve/#best-practices) üíé                        |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |              |       |
|                                                         | [PR Chat](https://qodo-merge-docs.qodo.ai/chrome-extension/features/#pr-chat) üíé                                    |   ‚úÖ   |        |           |              |       |
|                                                         | [PR to Ticket](https://qodo-merge-docs.qodo.ai/tools/pr_to_ticket/) üíé                                              |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |              |       |
|                                                         | [Suggestion Tracking](https://qodo-merge-docs.qodo.ai/tools/improve/#suggestion-tracking) üíé                        |   ‚úÖ   |   ‚úÖ   |           |              |       |
|                                                         |                                                                                                                     |        |        |           |              |       |
| [USAGE](https://qodo-merge-docs.qodo.ai/usage-guide/)   | [CLI](https://qodo-merge-docs.qodo.ai/usage-guide/automations_and_usage/#local-repo-cli)                            |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |  ‚úÖ   |
|                                                         | [App / webhook](https://qodo-merge-docs.qodo.ai/usage-guide/automations_and_usage/#github-app)                      |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |  ‚úÖ   |
|                                                         | [Tagging bot](https://github.com/Codium-ai/pr-agent#try-it-now)                                                     |   ‚úÖ   |        |           |              |       |
|                                                         | [Actions](https://qodo-merge-docs.qodo.ai/installation/github/#run-as-a-github-action)                              |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |       |
|                                                         |                                                                                                                     |        |        |           |              |       |
| [CORE](https://qodo-merge-docs.qodo.ai/core-abilities/) | [Adaptive and token-aware file patch fitting](https://qodo-merge-docs.qodo.ai/core-abilities/compression_strategy/) |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |       |
|                                                         | [Auto Best Practices üíé](https://qodo-merge-docs.qodo.ai/core-abilities/auto_best_practices/)                       |   ‚úÖ   |      |         |            |   |
|                                                         | [Chat on code suggestions](https://qodo-merge-docs.qodo.ai/core-abilities/chat_on_code_suggestions/)                |   ‚úÖ   |  ‚úÖ   |           |              |       |
|                                                         | [Code Validation üíé](https://qodo-merge-docs.qodo.ai/core-abilities/code_validation/)                               |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |       |
|                                                         | [Dynamic context](https://qodo-merge-docs.qodo.ai/core-abilities/dynamic_context/)                                  |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |       |
|                                                         | [Fetching ticket context](https://qodo-merge-docs.qodo.ai/core-abilities/fetching_ticket_context/)                  |   ‚úÖ    |  ‚úÖ    |     ‚úÖ     |              |       |
|                                                         | [Global and wiki configurations](https://qodo-merge-docs.qodo.ai/usage-guide/configuration_options/) üíé             |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |              |       |
|                                                         | [Impact Evaluation](https://qodo-merge-docs.qodo.ai/core-abilities/impact_evaluation/) üíé                           |   ‚úÖ   |   ‚úÖ   |           |              |       |
|                                                         | [Incremental Update](https://qodo-merge-docs.qodo.ai/core-abilities/incremental_update/)                            |   ‚úÖ    |       |           |              |       |
|                                                         | [Interactivity](https://qodo-merge-docs.qodo.ai/core-abilities/interactivity/)                                      |   ‚úÖ   |  ‚úÖ   |           |              |       |
|                                                         | [Local and global metadata](https://qodo-merge-docs.qodo.ai/core-abilities/metadata/)                               |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |       |
|                                                         | [Multiple models support](https://qodo-merge-docs.qodo.ai/usage-guide/changing_a_model/)                            |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |       |
|                                                         | [PR compression](https://qodo-merge-docs.qodo.ai/core-abilities/compression_strategy/)                              |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |       |
|                                                         | [PR interactive actions](https://www.qodo.ai/images/pr_agent/pr-actions.mp4) üíé                                     |   ‚úÖ   |   ‚úÖ   |           |              |       |
|                                                         | [RAG context enrichment](https://qodo-merge-docs.qodo.ai/core-abilities/rag_context_enrichment/)                    |   ‚úÖ    |       |    ‚úÖ     |              |       |
|                                                         | [Self reflection](https://qodo-merge-docs.qodo.ai/core-abilities/self_reflection/)                                  |   ‚úÖ   |   ‚úÖ   |    ‚úÖ     |      ‚úÖ      |       |
|                                                         | [Static code analysis](https://qodo-merge-docs.qodo.ai/core-abilities/static_code_analysis/) üíé                     |   ‚úÖ   |   ‚úÖ   |           |              |       |
- üíé means this feature is available only in [Qodo Merge](https://www.qodo.ai/pricing/)

[//]: # (- Support for additional git providers is described in [here]&amp;#40;./docs/Full_environments.md&amp;#41;)
___

## See It in Action

&lt;/div&gt;
&lt;h4&gt;&lt;a href=&quot;https://github.com/Codium-ai/pr-agent/pull/530&quot;&gt;/descri

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[streamlit/streamlit]]></title>
            <link>https://github.com/streamlit/streamlit</link>
            <guid>https://github.com/streamlit/streamlit</guid>
            <pubDate>Sun, 28 Sep 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[Streamlit ‚Äî A faster way to build and share data apps.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/streamlit/streamlit">streamlit/streamlit</a></h1>
            <p>Streamlit ‚Äî A faster way to build and share data apps.</p>
            <p>Language: Python</p>
            <p>Stars: 41,578</p>
            <p>Forks: 3,745</p>
            <p>Stars today: 12 stars today</p>
            <h2>README</h2><pre># Welcome to Streamlit :wave:

**The fastest way to build and share data apps.**

Streamlit lets you turn data scripts into shareable web apps in minutes, not weeks. It‚Äôs all Python, open-source, and free! And once you‚Äôve created an app you can use our¬†[Community Cloud platform](https://streamlit.io/cloud)¬†to deploy, manage, and share your app!

![Example of live coding an app in Streamlit|635x380](https://raw.githubusercontent.com/streamlit/docs/main/public/images/Streamlit_overview.gif)

## Installation

```bash
pip install streamlit
streamlit hello
```

Streamlit can also be installed in a virtual environment on [Windows](https://github.com/streamlit/streamlit/wiki/Installing-in-a-virtual-environment#on-windows), [Mac](https://github.com/streamlit/streamlit/wiki/Installing-in-a-virtual-environment#on-mac--linux), and [Linux](https://github.com/streamlit/streamlit/wiki/Installing-in-a-virtual-environment#on-mac--linux).

## A little example

Streamlit makes it incredibly easy to build interactive apps:

```python
import streamlit as st

x = st.slider(&#039;Select a value&#039;)
st.write(x, &#039;squared is&#039;, x * x)
```

&lt;img src=&quot;https://raw.githubusercontent.com/streamlit/docs/main/public/images/simple_example.png&quot;/&gt;

## A bigger example

Streamlit&#039;s simple and focused API lets you build incredibly rich and powerful tools.¬† [This demo project](https://github.com/streamlit/demo-self-driving) lets you browse the entire [Udacity self-driving-car dataset](https://github.com/udacity/self-driving-car) and run inference in real-time using the [YOLO object detection net](https://pjreddie.com/darknet/yolo).

![Final App Animation](https://raw.githubusercontent.com/streamlit/docs/main/public/images/complex_app_example.gif)

The complete demo is implemented in less than 300 lines of Python. In fact, the app contains [only 23 Streamlit calls](https://github.com/streamlit/demo-self-driving/blob/master/streamlit_app.py) which illustrates all the major building blocks of Streamlit. You can try it right now at [share.streamlit.io/streamlit/demo-self-driving](https://share.streamlit.io/streamlit/demo-self-driving).

## The Streamlit GitHub badge

Streamlit&#039;s GitHub badge helps others find and play with your Streamlit app.

[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://share.streamlit.io/streamlit/demo-face-gan)

Once you deploy your app, you can embed this badge right into your GitHub readme.md as follows:

```markdown
[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://share.streamlit.io/yourGitHubName/yourRepo/yourApp/)
```

## More Information

- Our [launch post](https://towardsdatascience.com/coding-ml-tools-like-you-code-ml-models-ddba3357eace?source=friends_link&amp;sk=f7774c54571148b33cde3ba6c6310086) explaining why we created Streamlit
- Our [Community Cloud platform announcement](https://blog.streamlit.io/introducing-streamlit-cloud)
- Our amazing [community](https://discuss.streamlit.io/) where Streamlit users share apps, ask questions, and help each other out
- Streamlit [documentation](https://docs.streamlit.io/) and [blog](https://blog.streamlit.io) for the latest Streamlit info
- More [demo projects](https://github.com/streamlit/) to inspire you
- And if you would like to contribute, see [instructions here](https://github.com/streamlit/streamlit/wiki/Contributing)

## Community Cloud

With [Community Cloud](https://streamlit.io/cloud) you can deploy, manage, and share your apps with the world, directly from Streamlit ‚Äî all for free. Sign-up [here](https://share.streamlit.io/signup).

## License

Streamlit is completely free and open-source and licensed under the [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) license.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[strands-agents/sdk-python]]></title>
            <link>https://github.com/strands-agents/sdk-python</link>
            <guid>https://github.com/strands-agents/sdk-python</guid>
            <pubDate>Sun, 28 Sep 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[A model-driven approach to building AI agents in just a few lines of code.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/strands-agents/sdk-python">strands-agents/sdk-python</a></h1>
            <p>A model-driven approach to building AI agents in just a few lines of code.</p>
            <p>Language: Python</p>
            <p>Stars: 3,529</p>
            <p>Forks: 397</p>
            <p>Stars today: 21 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;div&gt;
    &lt;a href=&quot;https://strandsagents.com&quot;&gt;
      &lt;img src=&quot;https://strandsagents.com/latest/assets/logo-github.svg&quot; alt=&quot;Strands Agents&quot; width=&quot;55px&quot; height=&quot;105px&quot;&gt;
    &lt;/a&gt;
  &lt;/div&gt;

  &lt;h1&gt;
    Strands Agents
  &lt;/h1&gt;

  &lt;h2&gt;
    A model-driven approach to building AI agents in just a few lines of code.
  &lt;/h2&gt;

  &lt;div align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://github.com/strands-agents/sdk-python/graphs/commit-activity&quot;&gt;&lt;img alt=&quot;GitHub commit activity&quot; src=&quot;https://img.shields.io/github/commit-activity/m/strands-agents/sdk-python&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/strands-agents/sdk-python/issues&quot;&gt;&lt;img alt=&quot;GitHub open issues&quot; src=&quot;https://img.shields.io/github/issues/strands-agents/sdk-python&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/strands-agents/sdk-python/pulls&quot;&gt;&lt;img alt=&quot;GitHub open pull requests&quot; src=&quot;https://img.shields.io/github/issues-pr/strands-agents/sdk-python&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/strands-agents/sdk-python/blob/main/LICENSE&quot;&gt;&lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/github/license/strands-agents/sdk-python&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://pypi.org/project/strands-agents/&quot;&gt;&lt;img alt=&quot;PyPI version&quot; src=&quot;https://img.shields.io/pypi/v/strands-agents&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://python.org&quot;&gt;&lt;img alt=&quot;Python versions&quot; src=&quot;https://img.shields.io/pypi/pyversions/strands-agents&quot;/&gt;&lt;/a&gt;
  &lt;/div&gt;
  
  &lt;p&gt;
    &lt;a href=&quot;https://strandsagents.com/&quot;&gt;Documentation&lt;/a&gt;
    ‚óÜ &lt;a href=&quot;https://github.com/strands-agents/samples&quot;&gt;Samples&lt;/a&gt;
    ‚óÜ &lt;a href=&quot;https://github.com/strands-agents/sdk-python&quot;&gt;Python SDK&lt;/a&gt;
    ‚óÜ &lt;a href=&quot;https://github.com/strands-agents/tools&quot;&gt;Tools&lt;/a&gt;
    ‚óÜ &lt;a href=&quot;https://github.com/strands-agents/agent-builder&quot;&gt;Agent Builder&lt;/a&gt;
    ‚óÜ &lt;a href=&quot;https://github.com/strands-agents/mcp-server&quot;&gt;MCP Server&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

Strands Agents is a simple yet powerful SDK that takes a model-driven approach to building and running AI agents. From simple conversational assistants to complex autonomous workflows, from local development to production deployment, Strands Agents scales with your needs.

## Feature Overview

- **Lightweight &amp; Flexible**: Simple agent loop that just works and is fully customizable
- **Model Agnostic**: Support for Amazon Bedrock, Anthropic, Gemini, LiteLLM, Llama, Ollama, OpenAI, Writer, and custom providers
- **Advanced Capabilities**: Multi-agent systems, autonomous agents, and streaming support
- **Built-in MCP**: Native support for Model Context Protocol (MCP) servers, enabling access to thousands of pre-built tools

## Quick Start

```bash
# Install Strands Agents
pip install strands-agents strands-agents-tools
```

```python
from strands import Agent
from strands_tools import calculator
agent = Agent(tools=[calculator])
agent(&quot;What is the square root of 1764&quot;)
```

&gt; **Note**: For the default Amazon Bedrock model provider, you&#039;ll need AWS credentials configured and model access enabled for Claude 4 Sonnet in the us-west-2 region. See the [Quickstart Guide](https://strandsagents.com/) for details on configuring other model providers.

## Installation

Ensure you have Python 3.10+ installed, then:

```bash
# Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows use: .venv\Scripts\activate

# Install Strands and tools
pip install strands-agents strands-agents-tools
```

## Features at a Glance

### Python-Based Tools

Easily build tools using Python decorators:

```python
from strands import Agent, tool

@tool
def word_count(text: str) -&gt; int:
    &quot;&quot;&quot;Count words in text.

    This docstring is used by the LLM to understand the tool&#039;s purpose.
    &quot;&quot;&quot;
    return len(text.split())

agent = Agent(tools=[word_count])
response = agent(&quot;How many words are in this sentence?&quot;)
```

**Hot Reloading from Directory:**
Enable automatic tool loading and reloading from the `./tools/` directory:

```python
from strands import Agent

# Agent will watch ./tools/ directory for changes
agent = Agent(load_tools_from_directory=True)
response = agent(&quot;Use any tools you find in the tools directory&quot;)
```

### MCP Support

Seamlessly integrate Model Context Protocol (MCP) servers:

```python
from strands import Agent
from strands.tools.mcp import MCPClient
from mcp import stdio_client, StdioServerParameters

aws_docs_client = MCPClient(
    lambda: stdio_client(StdioServerParameters(command=&quot;uvx&quot;, args=[&quot;awslabs.aws-documentation-mcp-server@latest&quot;]))
)

with aws_docs_client:
   agent = Agent(tools=aws_docs_client.list_tools_sync())
   response = agent(&quot;Tell me about Amazon Bedrock and how to use it with Python&quot;)
```

### Multiple Model Providers

Support for various model providers:

```python
from strands import Agent
from strands.models import BedrockModel
from strands.models.ollama import OllamaModel
from strands.models.llamaapi import LlamaAPIModel
from strands.models.gemini import GeminiModel
from strands.models.llamacpp import LlamaCppModel

# Bedrock
bedrock_model = BedrockModel(
  model_id=&quot;us.amazon.nova-pro-v1:0&quot;,
  temperature=0.3,
  streaming=True, # Enable/disable streaming
)
agent = Agent(model=bedrock_model)
agent(&quot;Tell me about Agentic AI&quot;)

# Google Gemini
gemini_model = GeminiModel(
  api_key=&quot;your_gemini_api_key&quot;,
  model_id=&quot;gemini-2.5-flash&quot;,
  params={&quot;temperature&quot;: 0.7}
)
agent = Agent(model=gemini_model)
agent(&quot;Tell me about Agentic AI&quot;)

# Ollama
ollama_model = OllamaModel(
  host=&quot;http://localhost:11434&quot;,
  model_id=&quot;llama3&quot;
)
agent = Agent(model=ollama_model)
agent(&quot;Tell me about Agentic AI&quot;)

# Llama API
llama_model = LlamaAPIModel(
    model_id=&quot;Llama-4-Maverick-17B-128E-Instruct-FP8&quot;,
)
agent = Agent(model=llama_model)
response = agent(&quot;Tell me about Agentic AI&quot;)
```

Built-in providers:
 - [Amazon Bedrock](https://strandsagents.com/latest/user-guide/concepts/model-providers/amazon-bedrock/)
 - [Anthropic](https://strandsagents.com/latest/user-guide/concepts/model-providers/anthropic/)
 - [Gemini](https://strandsagents.com/latest/user-guide/concepts/model-providers/gemini/)
 - [Cohere](https://strandsagents.com/latest/user-guide/concepts/model-providers/cohere/)
 - [LiteLLM](https://strandsagents.com/latest/user-guide/concepts/model-providers/litellm/)
 - [llama.cpp](https://strandsagents.com/latest/user-guide/concepts/model-providers/llamacpp/)
 - [LlamaAPI](https://strandsagents.com/latest/user-guide/concepts/model-providers/llamaapi/)
 - [MistralAI](https://strandsagents.com/latest/user-guide/concepts/model-providers/mistral/)
 - [Ollama](https://strandsagents.com/latest/user-guide/concepts/model-providers/ollama/)
 - [OpenAI](https://strandsagents.com/latest/user-guide/concepts/model-providers/openai/)
 - [SageMaker](https://strandsagents.com/latest/user-guide/concepts/model-providers/sagemaker/)
 - [Writer](https://strandsagents.com/latest/user-guide/concepts/model-providers/writer/)

Custom providers can be implemented using [Custom Providers](https://strandsagents.com/latest/user-guide/concepts/model-providers/custom_model_provider/)

### Example tools

Strands offers an optional strands-agents-tools package with pre-built tools for quick experimentation:

```python
from strands import Agent
from strands_tools import calculator
agent = Agent(tools=[calculator])
agent(&quot;What is the square root of 1764&quot;)
```

It&#039;s also available on GitHub via [strands-agents/tools](https://github.com/strands-agents/tools).

## Documentation

For detailed guidance &amp; examples, explore our documentation:

- [User Guide](https://strandsagents.com/)
- [Quick Start Guide](https://strandsagents.com/latest/user-guide/quickstart/)
- [Agent Loop](https://strandsagents.com/latest/user-guide/concepts/agents/agent-loop/)
- [Examples](https://strandsagents.com/latest/examples/)
- [API Reference](https://strandsagents.com/latest/api-reference/agent/)
- [Production &amp; Deployment Guide](https://strandsagents.com/latest/user-guide/deploy/operating-agents-in-production/)

## Contributing ‚ù§Ô∏è

We welcome contributions! See our [Contributing Guide](CONTRIBUTING.md) for details on:
- Reporting bugs &amp; features
- Development setup
- Contributing via Pull Requests
- Code of Conduct
- Reporting of security issues

## License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[modelcontextprotocol/python-sdk]]></title>
            <link>https://github.com/modelcontextprotocol/python-sdk</link>
            <guid>https://github.com/modelcontextprotocol/python-sdk</guid>
            <pubDate>Sun, 28 Sep 2025 00:04:20 GMT</pubDate>
            <description><![CDATA[The official Python SDK for Model Context Protocol servers and clients]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/modelcontextprotocol/python-sdk">modelcontextprotocol/python-sdk</a></h1>
            <p>The official Python SDK for Model Context Protocol servers and clients</p>
            <p>Language: Python</p>
            <p>Stars: 18,838</p>
            <p>Forks: 2,528</p>
            <p>Stars today: 31 stars today</p>
            <h2>README</h2><pre># MCP Python SDK

&lt;div align=&quot;center&quot;&gt;

&lt;strong&gt;Python implementation of the Model Context Protocol (MCP)&lt;/strong&gt;

[![PyPI][pypi-badge]][pypi-url]
[![MIT licensed][mit-badge]][mit-url]
[![Python Version][python-badge]][python-url]
[![Documentation][docs-badge]][docs-url]
[![Specification][spec-badge]][spec-url]
[![GitHub Discussions][discussions-badge]][discussions-url]

&lt;/div&gt;

&lt;!-- omit in toc --&gt;
## Table of Contents

- [MCP Python SDK](#mcp-python-sdk)
  - [Overview](#overview)
  - [Installation](#installation)
    - [Adding MCP to your python project](#adding-mcp-to-your-python-project)
    - [Running the standalone MCP development tools](#running-the-standalone-mcp-development-tools)
  - [Quickstart](#quickstart)
  - [What is MCP?](#what-is-mcp)
  - [Core Concepts](#core-concepts)
    - [Server](#server)
    - [Resources](#resources)
    - [Tools](#tools)
      - [Structured Output](#structured-output)
    - [Prompts](#prompts)
    - [Images](#images)
    - [Context](#context)
      - [Getting Context in Functions](#getting-context-in-functions)
      - [Context Properties and Methods](#context-properties-and-methods)
    - [Completions](#completions)
    - [Elicitation](#elicitation)
    - [Sampling](#sampling)
    - [Logging and Notifications](#logging-and-notifications)
    - [Authentication](#authentication)
    - [FastMCP Properties](#fastmcp-properties)
    - [Session Properties and Methods](#session-properties-and-methods)
    - [Request Context Properties](#request-context-properties)
  - [Running Your Server](#running-your-server)
    - [Development Mode](#development-mode)
    - [Claude Desktop Integration](#claude-desktop-integration)
    - [Direct Execution](#direct-execution)
    - [Streamable HTTP Transport](#streamable-http-transport)
      - [CORS Configuration for Browser-Based Clients](#cors-configuration-for-browser-based-clients)
    - [Mounting to an Existing ASGI Server](#mounting-to-an-existing-asgi-server)
      - [StreamableHTTP servers](#streamablehttp-servers)
        - [Basic mounting](#basic-mounting)
        - [Host-based routing](#host-based-routing)
        - [Multiple servers with path configuration](#multiple-servers-with-path-configuration)
        - [Path configuration at initialization](#path-configuration-at-initialization)
      - [SSE servers](#sse-servers)
  - [Advanced Usage](#advanced-usage)
    - [Low-Level Server](#low-level-server)
      - [Structured Output Support](#structured-output-support)
    - [Pagination (Advanced)](#pagination-advanced)
    - [Writing MCP Clients](#writing-mcp-clients)
    - [Client Display Utilities](#client-display-utilities)
    - [OAuth Authentication for Clients](#oauth-authentication-for-clients)
    - [Parsing Tool Results](#parsing-tool-results)
    - [MCP Primitives](#mcp-primitives)
    - [Server Capabilities](#server-capabilities)
  - [Documentation](#documentation)
  - [Contributing](#contributing)
  - [License](#license)

[pypi-badge]: https://img.shields.io/pypi/v/mcp.svg
[pypi-url]: https://pypi.org/project/mcp/
[mit-badge]: https://img.shields.io/pypi/l/mcp.svg
[mit-url]: https://github.com/modelcontextprotocol/python-sdk/blob/main/LICENSE
[python-badge]: https://img.shields.io/pypi/pyversions/mcp.svg
[python-url]: https://www.python.org/downloads/
[docs-badge]: https://img.shields.io/badge/docs-modelcontextprotocol.io-blue.svg
[docs-url]: https://modelcontextprotocol.io
[spec-badge]: https://img.shields.io/badge/spec-spec.modelcontextprotocol.io-blue.svg
[spec-url]: https://spec.modelcontextprotocol.io
[discussions-badge]: https://img.shields.io/github/discussions/modelcontextprotocol/python-sdk
[discussions-url]: https://github.com/modelcontextprotocol/python-sdk/discussions

## Overview

The Model Context Protocol allows applications to provide context for LLMs in a standardized way, separating the concerns of providing context from the actual LLM interaction. This Python SDK implements the full MCP specification, making it easy to:

- Build MCP clients that can connect to any MCP server
- Create MCP servers that expose resources, prompts and tools
- Use standard transports like stdio, SSE, and Streamable HTTP
- Handle all MCP protocol messages and lifecycle events

## Installation

### Adding MCP to your python project

We recommend using [uv](https://docs.astral.sh/uv/) to manage your Python projects.

If you haven&#039;t created a uv-managed project yet, create one:

   ```bash
   uv init mcp-server-demo
   cd mcp-server-demo
   ```

   Then add MCP to your project dependencies:

   ```bash
   uv add &quot;mcp[cli]&quot;
   ```

Alternatively, for projects using pip for dependencies:

```bash
pip install &quot;mcp[cli]&quot;
```

### Running the standalone MCP development tools

To run the mcp command with uv:

```bash
uv run mcp
```

## Quickstart

Let&#039;s create a simple MCP server that exposes a calculator tool and some data:

&lt;!-- snippet-source examples/snippets/servers/fastmcp_quickstart.py --&gt;
```python
&quot;&quot;&quot;
FastMCP quickstart example.

cd to the `examples/snippets/clients` directory and run:
    uv run server fastmcp_quickstart stdio
&quot;&quot;&quot;

from mcp.server.fastmcp import FastMCP

# Create an MCP server
mcp = FastMCP(&quot;Demo&quot;)


# Add an addition tool
@mcp.tool()
def add(a: int, b: int) -&gt; int:
    &quot;&quot;&quot;Add two numbers&quot;&quot;&quot;
    return a + b


# Add a dynamic greeting resource
@mcp.resource(&quot;greeting://{name}&quot;)
def get_greeting(name: str) -&gt; str:
    &quot;&quot;&quot;Get a personalized greeting&quot;&quot;&quot;
    return f&quot;Hello, {name}!&quot;


# Add a prompt
@mcp.prompt()
def greet_user(name: str, style: str = &quot;friendly&quot;) -&gt; str:
    &quot;&quot;&quot;Generate a greeting prompt&quot;&quot;&quot;
    styles = {
        &quot;friendly&quot;: &quot;Please write a warm, friendly greeting&quot;,
        &quot;formal&quot;: &quot;Please write a formal, professional greeting&quot;,
        &quot;casual&quot;: &quot;Please write a casual, relaxed greeting&quot;,
    }

    return f&quot;{styles.get(style, styles[&#039;friendly&#039;])} for someone named {name}.&quot;
```

_Full example: [examples/snippets/servers/fastmcp_quickstart.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/fastmcp_quickstart.py)_
&lt;!-- /snippet-source --&gt;

You can install this server in [Claude Desktop](https://claude.ai/download) and interact with it right away by running:

```bash
uv run mcp install server.py
```

Alternatively, you can test it with the MCP Inspector:

```bash
uv run mcp dev server.py
```

## What is MCP?

The [Model Context Protocol (MCP)](https://modelcontextprotocol.io) lets you build servers that expose data and functionality to LLM applications in a secure, standardized way. Think of it like a web API, but specifically designed for LLM interactions. MCP servers can:

- Expose data through **Resources** (think of these sort of like GET endpoints; they are used to load information into the LLM&#039;s context)
- Provide functionality through **Tools** (sort of like POST endpoints; they are used to execute code or otherwise produce a side effect)
- Define interaction patterns through **Prompts** (reusable templates for LLM interactions)
- And more!

## Core Concepts

### Server

The FastMCP server is your core interface to the MCP protocol. It handles connection management, protocol compliance, and message routing:

&lt;!-- snippet-source examples/snippets/servers/lifespan_example.py --&gt;
```python
&quot;&quot;&quot;Example showing lifespan support for startup/shutdown with strong typing.&quot;&quot;&quot;

from collections.abc import AsyncIterator
from contextlib import asynccontextmanager
from dataclasses import dataclass

from mcp.server.fastmcp import Context, FastMCP
from mcp.server.session import ServerSession


# Mock database class for example
class Database:
    &quot;&quot;&quot;Mock database class for example.&quot;&quot;&quot;

    @classmethod
    async def connect(cls) -&gt; &quot;Database&quot;:
        &quot;&quot;&quot;Connect to database.&quot;&quot;&quot;
        return cls()

    async def disconnect(self) -&gt; None:
        &quot;&quot;&quot;Disconnect from database.&quot;&quot;&quot;
        pass

    def query(self) -&gt; str:
        &quot;&quot;&quot;Execute a query.&quot;&quot;&quot;
        return &quot;Query result&quot;


@dataclass
class AppContext:
    &quot;&quot;&quot;Application context with typed dependencies.&quot;&quot;&quot;

    db: Database


@asynccontextmanager
async def app_lifespan(server: FastMCP) -&gt; AsyncIterator[AppContext]:
    &quot;&quot;&quot;Manage application lifecycle with type-safe context.&quot;&quot;&quot;
    # Initialize on startup
    db = await Database.connect()
    try:
        yield AppContext(db=db)
    finally:
        # Cleanup on shutdown
        await db.disconnect()


# Pass lifespan to server
mcp = FastMCP(&quot;My App&quot;, lifespan=app_lifespan)


# Access type-safe lifespan context in tools
@mcp.tool()
def query_db(ctx: Context[ServerSession, AppContext]) -&gt; str:
    &quot;&quot;&quot;Tool that uses initialized resources.&quot;&quot;&quot;
    db = ctx.request_context.lifespan_context.db
    return db.query()
```

_Full example: [examples/snippets/servers/lifespan_example.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/lifespan_example.py)_
&lt;!-- /snippet-source --&gt;

### Resources

Resources are how you expose data to LLMs. They&#039;re similar to GET endpoints in a REST API - they provide data but shouldn&#039;t perform significant computation or have side effects:

&lt;!-- snippet-source examples/snippets/servers/basic_resource.py --&gt;
```python
from mcp.server.fastmcp import FastMCP

mcp = FastMCP(name=&quot;Resource Example&quot;)


@mcp.resource(&quot;file://documents/{name}&quot;)
def read_document(name: str) -&gt; str:
    &quot;&quot;&quot;Read a document by name.&quot;&quot;&quot;
    # This would normally read from disk
    return f&quot;Content of {name}&quot;


@mcp.resource(&quot;config://settings&quot;)
def get_settings() -&gt; str:
    &quot;&quot;&quot;Get application settings.&quot;&quot;&quot;
    return &quot;&quot;&quot;{
  &quot;theme&quot;: &quot;dark&quot;,
  &quot;language&quot;: &quot;en&quot;,
  &quot;debug&quot;: false
}&quot;&quot;&quot;
```

_Full example: [examples/snippets/servers/basic_resource.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/basic_resource.py)_
&lt;!-- /snippet-source --&gt;

### Tools

Tools let LLMs take actions through your server. Unlike resources, tools are expected to perform computation and have side effects:

&lt;!-- snippet-source examples/snippets/servers/basic_tool.py --&gt;
```python
from mcp.server.fastmcp import FastMCP

mcp = FastMCP(name=&quot;Tool Example&quot;)


@mcp.tool()
def sum(a: int, b: int) -&gt; int:
    &quot;&quot;&quot;Add two numbers together.&quot;&quot;&quot;
    return a + b


@mcp.tool()
def get_weather(city: str, unit: str = &quot;celsius&quot;) -&gt; str:
    &quot;&quot;&quot;Get weather for a city.&quot;&quot;&quot;
    # This would normally call a weather API
    return f&quot;Weather in {city}: 22degrees{unit[0].upper()}&quot;
```

_Full example: [examples/snippets/servers/basic_tool.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/basic_tool.py)_
&lt;!-- /snippet-source --&gt;

Tools can optionally receive a Context object by including a parameter with the `Context` type annotation. This context is automatically injected by the FastMCP framework and provides access to MCP capabilities:

&lt;!-- snippet-source examples/snippets/servers/tool_progress.py --&gt;
```python
from mcp.server.fastmcp import Context, FastMCP
from mcp.server.session import ServerSession

mcp = FastMCP(name=&quot;Progress Example&quot;)


@mcp.tool()
async def long_running_task(task_name: str, ctx: Context[ServerSession, None], steps: int = 5) -&gt; str:
    &quot;&quot;&quot;Execute a task with progress updates.&quot;&quot;&quot;
    await ctx.info(f&quot;Starting: {task_name}&quot;)

    for i in range(steps):
        progress = (i + 1) / steps
        await ctx.report_progress(
            progress=progress,
            total=1.0,
            message=f&quot;Step {i + 1}/{steps}&quot;,
        )
        await ctx.debug(f&quot;Completed step {i + 1}&quot;)

    return f&quot;Task &#039;{task_name}&#039; completed&quot;
```

_Full example: [examples/snippets/servers/tool_progress.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/tool_progress.py)_
&lt;!-- /snippet-source --&gt;

#### Structured Output

Tools will return structured results by default, if their return type
annotation is compatible. Otherwise, they will return unstructured results.

Structured output supports these return types:

- Pydantic models (BaseModel subclasses)
- TypedDicts
- Dataclasses and other classes with type hints
- `dict[str, T]` (where T is any JSON-serializable type)
- Primitive types (str, int, float, bool, bytes, None) - wrapped in `{&quot;result&quot;: value}`
- Generic types (list, tuple, Union, Optional, etc.) - wrapped in `{&quot;result&quot;: value}`

Classes without type hints cannot be serialized for structured output. Only
classes with properly annotated attributes will be converted to Pydantic models
for schema generation and validation.

Structured results are automatically validated against the output schema
generated from the annotation. This ensures the tool returns well-typed,
validated data that clients can easily process.

**Note:** For backward compatibility, unstructured results are also
returned. Unstructured results are provided for backward compatibility
with previous versions of the MCP specification, and are quirks-compatible
with previous versions of FastMCP in the current version of the SDK.

**Note:** In cases where a tool function&#039;s return type annotation
causes the tool to be classified as structured _and this is undesirable_,
the  classification can be suppressed by passing `structured_output=False`
to the `@tool` decorator.

&lt;!-- snippet-source examples/snippets/servers/structured_output.py --&gt;
```python
&quot;&quot;&quot;Example showing structured output with tools.&quot;&quot;&quot;

from typing import TypedDict

from pydantic import BaseModel, Field

from mcp.server.fastmcp import FastMCP

mcp = FastMCP(&quot;Structured Output Example&quot;)


# Using Pydantic models for rich structured data
class WeatherData(BaseModel):
    &quot;&quot;&quot;Weather information structure.&quot;&quot;&quot;

    temperature: float = Field(description=&quot;Temperature in Celsius&quot;)
    humidity: float = Field(description=&quot;Humidity percentage&quot;)
    condition: str
    wind_speed: float


@mcp.tool()
def get_weather(city: str) -&gt; WeatherData:
    &quot;&quot;&quot;Get weather for a city - returns structured data.&quot;&quot;&quot;
    # Simulated weather data
    return WeatherData(
        temperature=22.5,
        humidity=45.0,
        condition=&quot;sunny&quot;,
        wind_speed=5.2,
    )


# Using TypedDict for simpler structures
class LocationInfo(TypedDict):
    latitude: float
    longitude: float
    name: str


@mcp.tool()
def get_location(address: str) -&gt; LocationInfo:
    &quot;&quot;&quot;Get location coordinates&quot;&quot;&quot;
    return LocationInfo(latitude=51.5074, longitude=-0.1278, name=&quot;London, UK&quot;)


# Using dict[str, Any] for flexible schemas
@mcp.tool()
def get_statistics(data_type: str) -&gt; dict[str, float]:
    &quot;&quot;&quot;Get various statistics&quot;&quot;&quot;
    return {&quot;mean&quot;: 42.5, &quot;median&quot;: 40.0, &quot;std_dev&quot;: 5.2}


# Ordinary classes with type hints work for structured output
class UserProfile:
    name: str
    age: int
    email: str | None = None

    def __init__(self, name: str, age: int, email: str | None = None):
        self.name = name
        self.age = age
        self.email = email


@mcp.tool()
def get_user(user_id: str) -&gt; UserProfile:
    &quot;&quot;&quot;Get user profile - returns structured data&quot;&quot;&quot;
    return UserProfile(name=&quot;Alice&quot;, age=30, email=&quot;alice@example.com&quot;)


# Classes WITHOUT type hints cannot be used for structured output
class UntypedConfig:
    def __init__(self, setting1, setting2):  # type: ignore[reportMissingParameterType]
        self.setting1 = setting1
        self.setting2 = setting2


@mcp.tool()
def get_config() -&gt; UntypedConfig:
    &quot;&quot;&quot;This returns unstructured output - no schema generated&quot;&quot;&quot;
    return UntypedConfig(&quot;value1&quot;, &quot;value2&quot;)


# Lists and other types are wrapped automatically
@mcp.tool()
def list_cities() -&gt; list[str]:
    &quot;&quot;&quot;Get a list of cities&quot;&quot;&quot;
    return [&quot;London&quot;, &quot;Paris&quot;, &quot;Tokyo&quot;]
    # Returns: {&quot;result&quot;: [&quot;London&quot;, &quot;Paris&quot;, &quot;Tokyo&quot;]}


@mcp.tool()
def get_temperature(city: str) -&gt; float:
    &quot;&quot;&quot;Get temperature as a simple float&quot;&quot;&quot;
    return 22.5
    # Returns: {&quot;result&quot;: 22.5}
```

_Full example: [examples/snippets/servers/structured_output.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/structured_output.py)_
&lt;!-- /snippet-source --&gt;

### Prompts

Prompts are reusable templates that help LLMs interact with your server effectively:

&lt;!-- snippet-source examples/snippets/servers/basic_prompt.py --&gt;
```python
from mcp.server.fastmcp import FastMCP
from mcp.server.fastmcp.prompts import base

mcp = FastMCP(name=&quot;Prompt Example&quot;)


@mcp.prompt(title=&quot;Code Review&quot;)
def review_code(code: str) -&gt; str:
    return f&quot;Please review this code:\n\n{code}&quot;


@mcp.prompt(title=&quot;Debug Assistant&quot;)
def debug_error(error: str) -&gt; list[base.Message]:
    return [
        base.UserMessage(&quot;I&#039;m seeing this error:&quot;),
        base.UserMessage(error),
        base.AssistantMessage(&quot;I&#039;ll help debug that. What have you tried so far?&quot;),
    ]
```

_Full example: [examples/snippets/servers/basic_prompt.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/basic_prompt.py)_
&lt;!-- /snippet-source --&gt;

### Icons

MCP servers can provide icons for UI display. Icons can be added to the server implementation, tools, resources, and prompts:

```python
from mcp.server.fastmcp import FastMCP, Icon

# Create an icon from a file path or URL
icon = Icon(
    src=&quot;icon.png&quot;,
    mimeType=&quot;image/png&quot;,
    sizes=&quot;64x64&quot;
)

# Add icons to server
mcp = FastMCP(
    &quot;My Server&quot;,
    website_url=&quot;https://example.com&quot;,
    icons=[icon]
)

# Add icons to tools, resources, and prompts
@mcp.tool(icons=[icon])
def my_tool():
    &quot;&quot;&quot;Tool with an icon.&quot;&quot;&quot;
    return &quot;result&quot;

@mcp.resource(&quot;demo://resource&quot;, icons=[icon])
def my_resource():
    &quot;&quot;&quot;Resource with an icon.&quot;&quot;&quot;
    return &quot;content&quot;
```

_Full example: [examples/fastmcp/icons_demo.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/fastmcp/icons_demo.py)_

### Images

FastMCP provides an `Image` class that automatically handles image data:

&lt;!-- snippet-source examples/snippets/servers/images.py --&gt;
```python
&quot;&quot;&quot;Example showing image handling with FastMCP.&quot;&quot;&quot;

from PIL import Image as PILImage

from mcp.server.fastmcp import FastMCP, Image

mcp = FastMCP(&quot;Image Example&quot;)


@mcp.tool()
def create_thumbnail(image_path: str) -&gt; Image:
    &quot;&quot;&quot;Create a thumbnail from an image&quot;&quot;&quot;
    img = PILImage.open(image_path)
    img.thumbnail((100, 100))
    return Image(data=img.tobytes(), format=&quot;png&quot;)
```

_Full example: [examples/snippets/servers/images.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/images.py)_
&lt;!-- /snippet-source --&gt;

### Context

The Context object is automatically injected into tool and resource functions that request it via type hints. It provides access to MCP capabilities like logging, progress reporting, resource reading, user interaction, and request metadata.

#### Getting Context in Functions

To use context in a tool or resource function, add a parameter with the `Context` type annotation:

```python
from mcp.server.fastmcp import Context, FastMCP

mcp = FastMCP(name=&quot;Context Example&quot;)


@mcp.tool()
async def my_tool(x: int, ctx: Context) -&gt; str:
    &quot;&quot;&quot;Tool that uses context capabilities.&quot;&quot;&quot;
    # The context parameter can have any name as long as it&#039;s type-annotated
    return await process_with_context(x, ctx)
```

#### Context Properties and Methods

The Context object provides the following capabilities:

- `ctx.request_id` - Unique ID for the current request
- `ctx.client_id` - Client ID if available
- `ctx.fastmcp` - Access to the FastMCP server instance (see [FastMCP Properties](#fastmcp-properties))
- `ctx.session` - Access to the underlying session for advanced communication (see [Session Properties and Methods](#session-properties-and-methods))
- `ctx.request_context` - Access to request-specific data and lifespan resources (see [Request Context Properties](#request-context-properties))
- `await ctx.debug(message)` - Send debug log message
- `await ctx.info(message)` - Send info log message  
- `await ctx.warning(message)` - Send warning log message
- `await ctx.error(mes

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[crewAIInc/crewAI]]></title>
            <link>https://github.com/crewAIInc/crewAI</link>
            <guid>https://github.com/crewAIInc/crewAI</guid>
            <pubDate>Sun, 28 Sep 2025 00:04:19 GMT</pubDate>
            <description><![CDATA[Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/crewAIInc/crewAI">crewAIInc/crewAI</a></h1>
            <p>Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.</p>
            <p>Language: Python</p>
            <p>Stars: 38,553</p>
            <p>Forks: 5,107</p>
            <p>Stars today: 40 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI&quot;&gt;
    &lt;img src=&quot;docs/images/crewai_logo.png&quot; width=&quot;600px&quot; alt=&quot;Open source Multi-AI Agent orchestration framework&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot; style=&quot;display: flex; justify-content: center; gap: 20px; align-items: center;&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/11239&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/11239&quot; alt=&quot;crewAIInc%2FcrewAI | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://crewai.com&quot;&gt;Homepage&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://docs.crewai.com&quot;&gt;Docs&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://app.crewai.com&quot;&gt;Start Cloud Trial&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://blog.crewai.com&quot;&gt;Blog&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://community.crewai.com&quot;&gt;Forum&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/crewAIInc/crewAI&quot; alt=&quot;GitHub Repo stars&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI/network/members&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/forks/crewAIInc/crewAI&quot; alt=&quot;GitHub forks&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI/issues&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/issues/crewAIInc/crewAI&quot; alt=&quot;GitHub issues&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/crewAIInc/crewAI/pulls&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/issues-pr/crewAIInc/crewAI&quot; alt=&quot;GitHub pull requests&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/License-MIT-green.svg&quot; alt=&quot;License: MIT&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pypi.org/project/crewai/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/crewai&quot; alt=&quot;PyPI version&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/crewai/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/dm/crewai&quot; alt=&quot;PyPI downloads&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/crewAIInc&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/crewAIInc?style=social&quot; alt=&quot;Twitter Follow&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

### Fast and Flexible Multi-Agent Automation Framework

&gt; CrewAI is a lean, lightning-fast Python framework built entirely from scratch‚Äîcompletely **independent of LangChain or other agent frameworks**.
&gt; It empowers developers with both high-level simplicity and precise low-level control, ideal for creating autonomous AI agents tailored to any scenario.

- **CrewAI Crews**: Optimize for autonomy and collaborative intelligence.
- **CrewAI Flows**: Enable granular, event-driven control, single LLM calls for precise task orchestration and supports Crews natively

With over 100,000 developers certified through our community courses at [learn.crewai.com](https://learn.crewai.com), CrewAI is rapidly becoming the
standard for enterprise-ready AI automation.

# CrewAI Enterprise Suite

CrewAI Enterprise Suite is a comprehensive bundle tailored for organizations that require secure, scalable, and easy-to-manage agent-driven automation.

You can try one part of the suite the [Crew Control Plane for free](https://app.crewai.com)

## Crew Control Plane Key Features:

- **Tracing &amp; Observability**: Monitor and track your AI agents and workflows in real-time, including metrics, logs, and traces.
- **Unified Control Plane**: A centralized platform for managing, monitoring, and scaling your AI agents and workflows.
- **Seamless Integrations**: Easily connect with existing enterprise systems, data sources, and cloud infrastructure.
- **Advanced Security**: Built-in robust security and compliance measures ensuring safe deployment and management.
- **Actionable Insights**: Real-time analytics and reporting to optimize performance and decision-making.
- **24/7 Support**: Dedicated enterprise support to ensure uninterrupted operation and quick resolution of issues.
- **On-premise and Cloud Deployment Options**: Deploy CrewAI Enterprise on-premise or in the cloud, depending on your security and compliance requirements.

CrewAI Enterprise is designed for enterprises seeking a powerful, reliable solution to transform complex business processes into efficient,
intelligent automations.

## Table of contents

- [Why CrewAI?](#why-crewai)
- [Getting Started](#getting-started)
- [Key Features](#key-features)
- [Understanding Flows and Crews](#understanding-flows-and-crews)
- [CrewAI vs LangGraph](#how-crewai-compares)
- [Examples](#examples)
  - [Quick Tutorial](#quick-tutorial)
  - [Write Job Descriptions](#write-job-descriptions)
  - [Trip Planner](#trip-planner)
  - [Stock Analysis](#stock-analysis)
  - [Using Crews and Flows Together](#using-crews-and-flows-together)
- [Connecting Your Crew to a Model](#connecting-your-crew-to-a-model)
- [How CrewAI Compares](#how-crewai-compares)
- [Frequently Asked Questions (FAQ)](#frequently-asked-questions-faq)
- [Contribution](#contribution)
- [Telemetry](#telemetry)
- [License](#license)

## Why CrewAI?

&lt;div align=&quot;center&quot; style=&quot;margin-bottom: 30px;&quot;&gt;
  &lt;img src=&quot;docs/images/asset.png&quot; alt=&quot;CrewAI Logo&quot; width=&quot;100%&quot;&gt;
&lt;/div&gt;

CrewAI unlocks the true potential of multi-agent automation, delivering the best-in-class combination of speed, flexibility, and control with either Crews of AI Agents or Flows of Events:

- **Standalone Framework**: Built from scratch, independent of LangChain or any other agent framework.
- **High Performance**: Optimized for speed and minimal resource usage, enabling faster execution.
- **Flexible Low Level Customization**: Complete freedom to customize at both high and low levels - from overall workflows and system architecture to granular agent behaviors, internal prompts, and execution logic.
- **Ideal for Every Use Case**: Proven effective for both simple tasks and highly complex, real-world, enterprise-grade scenarios.
- **Robust Community**: Backed by a rapidly growing community of over **100,000 certified** developers offering comprehensive support and resources.

CrewAI empowers developers and enterprises to confidently build intelligent automations, bridging the gap between simplicity, flexibility, and performance.

## Getting Started

Setup and run your first CrewAI agents by following this tutorial.

[![CrewAI Getting Started Tutorial](https://img.youtube.com/vi/-kSOTtYzgEw/hqdefault.jpg)](https://www.youtube.com/watch?v=-kSOTtYzgEw &quot;CrewAI Getting Started Tutorial&quot;)

###
 Learning Resources

Learn CrewAI through our comprehensive courses:

- [Multi AI Agent Systems with CrewAI](https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/) - Master the fundamentals of multi-agent systems
- [Practical Multi AI Agents and Advanced Use Cases](https://www.deeplearning.ai/short-courses/practical-multi-ai-agents-and-advanced-use-cases-with-crewai/) - Deep dive into advanced implementations

### Understanding Flows and Crews

CrewAI offers two powerful, complementary approaches that work seamlessly together to build sophisticated AI applications:

1. **Crews**: Teams of AI agents with true autonomy and agency, working together to accomplish complex tasks through role-based collaboration. Crews enable:

   - Natural, autonomous decision-making between agents
   - Dynamic task delegation and collaboration
   - Specialized roles with defined goals and expertise
   - Flexible problem-solving approaches
2. **Flows**: Production-ready, event-driven workflows that deliver precise control over complex automations. Flows provide:

   - Fine-grained control over execution paths for real-world scenarios
   - Secure, consistent state management between tasks
   - Clean integration of AI agents with production Python code
   - Conditional branching for complex business logic

The true power of CrewAI emerges when combining Crews and Flows. This synergy allows you to:

- Build complex, production-grade applications
- Balance autonomy with precise control
- Handle sophisticated real-world scenarios
- Maintain clean, maintainable code structure

### Getting Started with Installation

To get started with CrewAI, follow these simple steps:

### 1. Installation

Ensure you have Python &gt;=3.10 &lt;3.14 installed on your system. CrewAI uses [UV](https://docs.astral.sh/uv/) for dependency management and package handling, offering a seamless setup and execution experience.

First, install CrewAI:

```shell
pip install crewai
```

If you want to install the &#039;crewai&#039; package along with its optional features that include additional tools for agents, you can do so by using the following command:

```shell
pip install &#039;crewai[tools]&#039;
```

The command above installs the basic package and also adds extra components which require more dependencies to function.

### Troubleshooting Dependencies

If you encounter issues during installation or usage, here are some common solutions:

#### Common Issues

1. **ModuleNotFoundError: No module named &#039;tiktoken&#039;**

   - Install tiktoken explicitly: `pip install &#039;crewai[embeddings]&#039;`
   - If using embedchain or other tools: `pip install &#039;crewai[tools]&#039;`
2. **Failed building wheel for tiktoken**

   - Ensure Rust compiler is installed (see installation steps above)
   - For Windows: Verify Visual C++ Build Tools are installed
   - Try upgrading pip: `pip install --upgrade pip`
   - If issues persist, use a pre-built wheel: `pip install tiktoken --prefer-binary`

### 2. Setting Up Your Crew with the YAML Configuration

To create a new CrewAI project, run the following CLI (Command Line Interface) command:

```shell
crewai create crew &lt;project_name&gt;
```

This command creates a new project folder with the following structure:

```
my_project/
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ .env
‚îî‚îÄ‚îÄ src/
    ‚îî‚îÄ‚îÄ my_project/
        ‚îú‚îÄ‚îÄ __init__.py
        ‚îú‚îÄ‚îÄ main.py
        ‚îú‚îÄ‚îÄ crew.py
        ‚îú‚îÄ‚îÄ tools/
        ‚îÇ   ‚îú‚îÄ‚îÄ custom_tool.py
        ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
        ‚îî‚îÄ‚îÄ config/
            ‚îú‚îÄ‚îÄ agents.yaml
            ‚îî‚îÄ‚îÄ tasks.yaml
```

You can now start developing your crew by editing the files in the `src/my_project` folder. The `main.py` file is the entry point of the project, the `crew.py` file is where you define your crew, the `agents.yaml` file is where you define your agents, and the `tasks.yaml` file is where you define your tasks.

#### To customize your project, you can:

- Modify `src/my_project/config/agents.yaml` to define your agents.
- Modify `src/my_project/config/tasks.yaml` to define your tasks.
- Modify `src/my_project/crew.py` to add your own logic, tools, and specific arguments.
- Modify `src/my_project/main.py` to add custom inputs for your agents and tasks.
- Add your environment variables into the `.env` file.

#### Example of a simple crew with a sequential process:

Instantiate your crew:

```shell
crewai create crew latest-ai-development
```

Modify the files as needed to fit your use case:

**agents.yaml**

```yaml
# src/my_project/config/agents.yaml
researcher:
  role: &gt;
    {topic} Senior Data Researcher
  goal: &gt;
    Uncover cutting-edge developments in {topic}
  backstory: &gt;
    You&#039;re a seasoned researcher with a knack for uncovering the latest
    developments in {topic}. Known for your ability to find the most relevant
    information and present it in a clear and concise manner.

reporting_analyst:
  role: &gt;
    {topic} Reporting Analyst
  goal: &gt;
    Create detailed reports based on {topic} data analysis and research findings
  backstory: &gt;
    You&#039;re a meticulous analyst with a keen eye for detail. You&#039;re known for
    your ability to turn complex data into clear and concise reports, making
    it easy for others to understand and act on the information you provide.
```

**tasks.yaml**

```yaml
# src/my_project/config/tasks.yaml
research_task:
  description: &gt;
    Conduct a thorough research about {topic}
    Make sure you find any interesting and relevant information given
    the current year is 2025.
  expected_output: &gt;
    A list with 10 bullet points of the most relevant information about {topic}
  agent: researcher

reporting_task:
  description: &gt;
    Review the context you got and expand each topic into a full section for a report.
    Make sure the report is detailed and contains any and all relevant information.
  expected_output: &gt;
    A fully fledge reports with the mains topics, each with a full section of information.
    Formatted as markdown without &#039;```&#039;
  agent: reporting_analyst
  output_file: report.md
```

**crew.py**

```python
# src/my_project/crew.py
from crewai import Agent, Crew, Process, Task
from crewai.project import CrewBase, agent, crew, task
from crewai_tools import SerperDevTool
from crewai.agents.agent_builder.base_agent import BaseAgent
from typing import List

@CrewBase
class LatestAiDevelopmentCrew():
	&quot;&quot;&quot;LatestAiDevelopment crew&quot;&quot;&quot;
	agents: List[BaseAgent]
	tasks: List[Task]

	@agent
	def researcher(self) -&gt; Agent:
		return Agent(
			config=self.agents_config[&#039;researcher&#039;],
			verbose=True,
			tools=[SerperDevTool()]
		)

	@agent
	def reporting_analyst(self) -&gt; Agent:
		return Agent(
			config=self.agents_config[&#039;reporting_analyst&#039;],
			verbose=True
		)

	@task
	def research_task(self) -&gt; Task:
		return Task(
			config=self.tasks_config[&#039;research_task&#039;],
		)

	@task
	def reporting_task(self) -&gt; Task:
		return Task(
			config=self.tasks_config[&#039;reporting_task&#039;],
			output_file=&#039;report.md&#039;
		)

	@crew
	def crew(self) -&gt; Crew:
		&quot;&quot;&quot;Creates the LatestAiDevelopment crew&quot;&quot;&quot;
		return Crew(
			agents=self.agents, # Automatically created by the @agent decorator
			tasks=self.tasks, # Automatically created by the @task decorator
			process=Process.sequential,
			verbose=True,
		)
```

**main.py**

```python
#!/usr/bin/env python
# src/my_project/main.py
import sys
from latest_ai_development.crew import LatestAiDevelopmentCrew

def run():
    &quot;&quot;&quot;
    Run the crew.
    &quot;&quot;&quot;
    inputs = {
        &#039;topic&#039;: &#039;AI Agents&#039;
    }
    LatestAiDevelopmentCrew().crew().kickoff(inputs=inputs)
```

### 3. Running Your Crew

Before running your crew, make sure you have the following keys set as environment variables in your `.env` file:

- An [OpenAI API key](https://platform.openai.com/account/api-keys) (or other LLM API key): `OPENAI_API_KEY=sk-...`
- A [Serper.dev](https://serper.dev/) API key: `SERPER_API_KEY=YOUR_KEY_HERE`

Lock the dependencies and install them by using the CLI command but first, navigate to your project directory:

```shell
cd my_project
crewai install (Optional)
```

To run your crew, execute the following command in the root of your project:

```bash
crewai run
```

or

```bash
python src/my_project/main.py
```

If an error happens due to the usage of poetry, please run the following command to update your crewai package:

```bash
crewai update
```

You should see the output in the console and the `report.md` file should be created in the root of your project with the full final report.

In addition to the sequential process, you can use the hierarchical process, which automatically assigns a manager to the defined crew to properly coordinate the planning and execution of tasks through delegation and validation of results. [See more about the processes here](https://docs.crewai.com/core-concepts/Processes/).

## Key Features

CrewAI stands apart as a lean, standalone, high-performance multi-AI Agent framework delivering simplicity, flexibility, and precise control‚Äîfree from the complexity and limitations found in other agent frameworks.

- **Standalone &amp; Lean**: Completely independent from other frameworks like LangChain, offering faster execution and lighter resource demands.
- **Flexible &amp; Precise**: Easily orchestrate autonomous agents through intuitive [Crews](https://docs.crewai.com/concepts/crews) or precise [Flows](https://docs.crewai.com/concepts/flows), achieving perfect balance for your needs.
- **Seamless Integration**: Effortlessly combine Crews (autonomy) and Flows (precision) to create complex, real-world automations.
- **Deep Customization**: Tailor every aspect‚Äîfrom high-level workflows down to low-level internal prompts and agent behaviors.
- **Reliable Performance**: Consistent results across simple tasks and complex, enterprise-level automations.
- **Thriving Community**: Backed by robust documentation and over 100,000 certified developers, providing exceptional support and guidance.

Choose CrewAI to easily build powerful, adaptable, and production-ready AI automations.

## Examples

You can test different real life examples of AI crews in the [CrewAI-examples repo](https://github.com/crewAIInc/crewAI-examples?tab=readme-ov-file):

- [Landing Page Generator](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/landing_page_generator)
- [Having Human input on the execution](https://docs.crewai.com/how-to/Human-Input-on-Execution)
- [Trip Planner](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/trip_planner)
- [Stock Analysis](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/stock_analysis)

### Quick Tutorial

[![CrewAI Tutorial](https://img.youtube.com/vi/tnejrr-0a94/maxresdefault.jpg)](https://www.youtube.com/watch?v=tnejrr-0a94 &quot;CrewAI Tutorial&quot;)

### Write Job Descriptions

[Check out code for this example](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/job-posting) or watch a video below:

[![Jobs postings](https://img.youtube.com/vi/u98wEMz-9to/maxresdefault.jpg)](https://www.youtube.com/watch?v=u98wEMz-9to &quot;Jobs postings&quot;)

### Trip Planner

[Check out code for this example](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/trip_planner) or watch a video below:

[![Trip Planner](https://img.youtube.com/vi/xis7rWp-hjs/maxresdefault.jpg)](https://www.youtube.com/watch?v=xis7rWp-hjs &quot;Trip Planner&quot;)

### Stock Analysis

[Check out code for this example](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/stock_analysis) or watch a video below:

[![Stock Analysis](https://img.youtube.com/vi/e0Uj4yWdaAg/maxresdefault.jpg)](https://www.youtube.com/watch?v=e0Uj4yWdaAg &quot;Stock Analysis&quot;)

### Using Crews and Flows Together

CrewAI&#039;s power truly shines when combining Crews with Flows to create sophisticated automation pipelines.
CrewAI flows support logical operators like `or_` and `and_` to combine multiple conditions. This can be used with `@start`, `@listen`, or `@router` decorators to create complex triggering conditions.

- `or_`: Triggers when any of the specified conditions are met.
- `and_`Triggers when all of the specified conditions are met.

Here&#039;s how you can orchestrate multiple Crews within a Flow:

```python
from crewai.flow.flow import Flow, listen, start, router, or_
from crewai import Crew, Agent, Task, Process
from pydantic import BaseModel

# Define structured state for precise control
class MarketState(BaseModel):
    sentiment: str = &quot;neutral&quot;
    confidence: float = 0.0
    recommendations: list = []

class AdvancedAnalysisFlow(Flow[MarketState]):
    @start()
    def fetch_market_data(self):
        # Demonstrate low-level control with structured state
        self.state.sentiment = &quot;analyzing&quot;
        return {&quot;sector&quot;: &quot;tech&quot;, &quot;timeframe&quot;: &quot;1W&quot;}  # These parameters match the task description template

    @listen(fetch_market_data)
    def analyze_with_crew(self, market_data):
        # Show crew agency through specialized roles
        analyst = Agent(
            role=&quot;Senior Market Analyst&quot;,
            goal=&quot;Conduct deep market analysis with expert insight&quot;,
            backstory=&quot;You&#039;re a veteran analyst known for identifying subtle market patterns&quot;
        )
        researcher = Agent(
            role=&quot;Data Researcher&quot;,
            goal=&quot;Gather and validate supporting market data&quot;,
            backstory=&quot;You excel at finding and correlating multiple data sources&quot;
        )

        analysis_task = Task(
            description=&quot;Analyze {sector} sector data for the past {timeframe}&quot;,
            expected_output=&quot;Detailed market analysis with confidence score&quot;,
            agent=analyst
        )
        research_task = Task(
            description=&quot;Fi

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Significant-Gravitas/AutoGPT]]></title>
            <link>https://github.com/Significant-Gravitas/AutoGPT</link>
            <guid>https://github.com/Significant-Gravitas/AutoGPT</guid>
            <pubDate>Sun, 28 Sep 2025 00:04:18 GMT</pubDate>
            <description><![CDATA[AutoGPT is the vision of accessible AI for everyone, to use and to build on. Our mission is to provide the tools, so that you can focus on what matters.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Significant-Gravitas/AutoGPT">Significant-Gravitas/AutoGPT</a></h1>
            <p>AutoGPT is the vision of accessible AI for everyone, to use and to build on. Our mission is to provide the tools, so that you can focus on what matters.</p>
            <p>Language: Python</p>
            <p>Stars: 178,724</p>
            <p>Forks: 45,985</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre># AutoGPT: Build, Deploy, and Run AI Agents

[![Discord Follow](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2Fautogpt%3Fwith_counts%3Dtrue&amp;query=%24.approximate_member_count&amp;label=total%20members&amp;logo=discord&amp;logoColor=white&amp;color=7289da)](https://discord.gg/autogpt) &amp;ensp;
[![Twitter Follow](https://img.shields.io/twitter/follow/Auto_GPT?style=social)](https://twitter.com/Auto_GPT) &amp;ensp;

&lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
[Deutsch](https://zdoc.app/de/Significant-Gravitas/AutoGPT) | 
[Espa√±ol](https://zdoc.app/es/Significant-Gravitas/AutoGPT) | 
[fran√ßais](https://zdoc.app/fr/Significant-Gravitas/AutoGPT) | 
[Êó•Êú¨Ë™û](https://zdoc.app/ja/Significant-Gravitas/AutoGPT) | 
[ÌïúÍµ≠Ïñ¥](https://zdoc.app/ko/Significant-Gravitas/AutoGPT) | 
[Portugu√™s](https://zdoc.app/pt/Significant-Gravitas/AutoGPT) | 
[–†—É—Å—Å–∫–∏–π](https://zdoc.app/ru/Significant-Gravitas/AutoGPT) | 
[‰∏≠Êñá](https://zdoc.app/zh/Significant-Gravitas/AutoGPT)

**AutoGPT** is a powerful platform that allows you to create, deploy, and manage continuous AI agents that automate complex workflows. 

## Hosting Options 
   - Download to self-host (Free!)
   - [Join the Waitlist](https://bit.ly/3ZDijAI) for the cloud-hosted beta (Closed Beta - Public release Coming Soon!)

## How to Self-Host the AutoGPT Platform
&gt; [!NOTE]
&gt; Setting up and hosting the AutoGPT Platform yourself is a technical process. 
&gt; If you&#039;d rather something that just works, we recommend [joining the waitlist](https://bit.ly/3ZDijAI) for the cloud-hosted beta.

### System Requirements

Before proceeding with the installation, ensure your system meets the following requirements:

#### Hardware Requirements
- CPU: 4+ cores recommended
- RAM: Minimum 8GB, 16GB recommended
- Storage: At least 10GB of free space

#### Software Requirements
- Operating Systems:
  - Linux (Ubuntu 20.04 or newer recommended)
  - macOS (10.15 or newer)
  - Windows 10/11 with WSL2
- Required Software (with minimum versions):
  - Docker Engine (20.10.0 or newer)
  - Docker Compose (2.0.0 or newer)
  - Git (2.30 or newer)
  - Node.js (16.x or newer)
  - npm (8.x or newer)
  - VSCode (1.60 or newer) or any modern code editor

#### Network Requirements
- Stable internet connection
- Access to required ports (will be configured in Docker)
- Ability to make outbound HTTPS connections

### Updated Setup Instructions:
We&#039;ve moved to a fully maintained and regularly updated documentation site.

üëâ [Follow the official self-hosting guide here](https://docs.agpt.co/platform/getting-started/)


This tutorial assumes you have Docker, VSCode, git and npm installed.

---

#### ‚ö° Quick Setup with One-Line Script (Recommended for Local Hosting)

Skip the manual steps and get started in minutes using our automatic setup script.

For macOS/Linux:
```
curl -fsSL https://setup.agpt.co/install.sh -o install.sh &amp;&amp; bash install.sh
```

For Windows (PowerShell):
```
powershell -c &quot;iwr https://setup.agpt.co/install.bat -o install.bat; ./install.bat&quot;
```

This will install dependencies, configure Docker, and launch your local instance ‚Äî all in one go.

### üß± AutoGPT Frontend

The AutoGPT frontend is where users interact with our powerful AI automation platform. It offers multiple ways to engage with and leverage our AI agents. This is the interface where you&#039;ll bring your AI automation ideas to life:

   **Agent Builder:** For those who want to customize, our intuitive, low-code interface allows you to design and configure your own AI agents. 
   
   **Workflow Management:** Build, modify, and optimize your automation workflows with ease. You build your agent by connecting blocks, where each block     performs a single action.
   
   **Deployment Controls:** Manage the lifecycle of your agents, from testing to production.
   
   **Ready-to-Use Agents:** Don&#039;t want to build? Simply select from our library of pre-configured agents and put them to work immediately.
   
   **Agent Interaction:** Whether you&#039;ve built your own or are using pre-configured agents, easily run and interact with them through our user-friendly      interface.

   **Monitoring and Analytics:** Keep track of your agents&#039; performance and gain insights to continually improve your automation processes.

[Read this guide](https://docs.agpt.co/platform/new_blocks/) to learn how to build your own custom blocks.

### üíΩ AutoGPT Server

The AutoGPT Server is the powerhouse of our platform This is where your agents run. Once deployed, agents can be triggered by external sources and can operate continuously. It contains all the essential components that make AutoGPT run smoothly.

   **Source Code:** The core logic that drives our agents and automation processes.
   
   **Infrastructure:** Robust systems that ensure reliable and scalable performance.
   
   **Marketplace:** A comprehensive marketplace where you can find and deploy a wide range of pre-built agents.

### üêô Example Agents

Here are two examples of what you can do with AutoGPT:

1. **Generate Viral Videos from Trending Topics**
   - This agent reads topics on Reddit.
   - It identifies trending topics.
   - It then automatically creates a short-form video based on the content. 

2. **Identify Top Quotes from Videos for Social Media**
   - This agent subscribes to your YouTube channel.
   - When you post a new video, it transcribes it.
   - It uses AI to identify the most impactful quotes to generate a summary.
   - Then, it writes a post to automatically publish to your social media. 

These examples show just a glimpse of what you can achieve with AutoGPT! You can create customized workflows to build agents for any use case.

---

### **License Overview:**

üõ°Ô∏è **Polyform Shield License:**
All code and content within the `autogpt_platform` folder is licensed under the Polyform Shield License. This new project is our in-developlemt platform for building, deploying and managing agents.&lt;/br&gt;_[Read more about this effort](https://agpt.co/blog/introducing-the-autogpt-platform)_

ü¶â **MIT License:**
All other portions of the AutoGPT repository (i.e., everything outside the `autogpt_platform` folder) are licensed under the MIT License. This includes the original stand-alone AutoGPT Agent, along with projects such as [Forge](https://github.com/Significant-Gravitas/AutoGPT/tree/master/classic/forge), [agbenchmark](https://github.com/Significant-Gravitas/AutoGPT/tree/master/classic/benchmark) and the [AutoGPT Classic GUI](https://github.com/Significant-Gravitas/AutoGPT/tree/master/classic/frontend).&lt;/br&gt;We also publish additional work under the MIT Licence in other repositories, such as [GravitasML](https://github.com/Significant-Gravitas/gravitasml) which is developed for and used in the AutoGPT Platform. See also our MIT Licenced [Code Ability](https://github.com/Significant-Gravitas/AutoGPT-Code-Ability) project.

---
### Mission
Our mission is to provide the tools, so that you can focus on what matters:

- üèóÔ∏è **Building** - Lay the foundation for something amazing.
- üß™ **Testing** - Fine-tune your agent to perfection.
- ü§ù **Delegating** - Let AI work for you, and have your ideas come to life.

Be part of the revolution! **AutoGPT** is here to stay, at the forefront of AI innovation.

**üìñ [Documentation](https://docs.agpt.co)**
&amp;ensp;|&amp;ensp;
**üöÄ [Contributing](CONTRIBUTING.md)**

---
## ü§ñ AutoGPT Classic
&gt; Below is information about the classic version of AutoGPT.

**üõ†Ô∏è [Build your own Agent - Quickstart](classic/FORGE-QUICKSTART.md)**

### üèóÔ∏è Forge

**Forge your own agent!** &amp;ndash; Forge is a ready-to-go toolkit to build your own agent application. It handles most of the boilerplate code, letting you channel all your creativity into the things that set *your* agent apart. All tutorials are located [here](https://medium.com/@aiedge/autogpt-forge-e3de53cc58ec). Components from [`forge`](/classic/forge/) can also be used individually to speed up development and reduce boilerplate in your agent project.

üöÄ [**Getting Started with Forge**](https://github.com/Significant-Gravitas/AutoGPT/blob/master/classic/forge/tutorials/001_getting_started.md) &amp;ndash;
This guide will walk you through the process of creating your own agent and using the benchmark and user interface.

üìò [Learn More](https://github.com/Significant-Gravitas/AutoGPT/tree/master/classic/forge) about Forge

### üéØ Benchmark

**Measure your agent&#039;s performance!** The `agbenchmark` can be used with any agent that supports the agent protocol, and the integration with the project&#039;s [CLI] makes it even easier to use with AutoGPT and forge-based agents. The benchmark offers a stringent testing environment. Our framework allows for autonomous, objective performance evaluations, ensuring your agents are primed for real-world action.

&lt;!-- TODO: insert visual demonstrating the benchmark --&gt;

üì¶ [`agbenchmark`](https://pypi.org/project/agbenchmark/) on Pypi
&amp;ensp;|&amp;ensp;
üìò [Learn More](https://github.com/Significant-Gravitas/AutoGPT/tree/master/classic/benchmark) about the Benchmark

### üíª UI

**Makes agents easy to use!** The `frontend` gives you a user-friendly interface to control and monitor your agents. It connects to agents through the [agent protocol](#-agent-protocol), ensuring compatibility with many agents from both inside and outside of our ecosystem.

&lt;!-- TODO: insert screenshot of front end --&gt;

The frontend works out-of-the-box with all agents in the repo. Just use the [CLI] to run your agent of choice!

üìò [Learn More](https://github.com/Significant-Gravitas/AutoGPT/tree/master/classic/frontend) about the Frontend

### ‚å®Ô∏è CLI

[CLI]: #-cli

To make it as easy as possible to use all of the tools offered by the repository, a CLI is included at the root of the repo:

```shell
$ ./run
Usage: cli.py [OPTIONS] COMMAND [ARGS]...

Options:
  --help  Show this message and exit.

Commands:
  agent      Commands to create, start and stop agents
  benchmark  Commands to start the benchmark and list tests and categories
  setup      Installs dependencies needed for your system.
```

Just clone the repo, install dependencies with `./run setup`, and you should be good to go!

## ü§î Questions? Problems? Suggestions?

### Get help - [Discord üí¨](https://discord.gg/autogpt)

[![Join us on Discord](https://invidget.switchblade.xyz/autogpt)](https://discord.gg/autogpt)

To report a bug or request a feature, create a [GitHub Issue](https://github.com/Significant-Gravitas/AutoGPT/issues/new/choose). Please ensure someone else hasn&#039;t created an issue for the same topic.

## ü§ù Sister projects

### üîÑ Agent Protocol

To maintain a uniform standard and ensure seamless compatibility with many current and future applications, AutoGPT employs the [agent protocol](https://agentprotocol.ai/) standard by the AI Engineer Foundation. This standardizes the communication pathways from your agent to the frontend and benchmark.

---

## Stars stats

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://star-history.com/#Significant-Gravitas/AutoGPT&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Significant-Gravitas/AutoGPT&amp;type=Date&amp;theme=dark&quot; /&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Significant-Gravitas/AutoGPT&amp;type=Date&quot; /&gt;
    &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=Significant-Gravitas/AutoGPT&amp;type=Date&quot; /&gt;
  &lt;/picture&gt;
&lt;/a&gt;
&lt;/p&gt;


## ‚ö° Contributors

&lt;a href=&quot;https://github.com/Significant-Gravitas/AutoGPT/graphs/contributors&quot; alt=&quot;View Contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=Significant-Gravitas/AutoGPT&amp;max=1000&amp;columns=10&quot; alt=&quot;Contributors&quot; /&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[dataease/SQLBot]]></title>
            <link>https://github.com/dataease/SQLBot</link>
            <guid>https://github.com/dataease/SQLBot</guid>
            <pubDate>Sun, 28 Sep 2025 00:04:17 GMT</pubDate>
            <description><![CDATA[üî• Âü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü„ÄÇText-to-SQL Generation via LLMs using RAG.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/dataease/SQLBot">dataease/SQLBot</a></h1>
            <p>üî• Âü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü„ÄÇText-to-SQL Generation via LLMs using RAG.</p>
            <p>Language: Python</p>
            <p>Stars: 3,285</p>
            <p>Forks: 331</p>
            <p>Stars today: 211 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://resource-fit2cloud-com.oss-cn-hangzhou.aliyuncs.com/sqlbot/sqlbot.png&quot; alt=&quot;SQLBot&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;
&lt;h3 align=&quot;center&quot;&gt;Âü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/dataease/SQLBot/releases/latest&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/dataease/SQLBot&quot; alt=&quot;Latest release&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/dataease/SQLBot&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/dataease/SQLBot?color=%231890FF&amp;style=flat-square&quot; alt=&quot;Stars&quot;&gt;&lt;/a&gt;    
  &lt;a href=&quot;https://hub.docker.com/r/dataease/SQLbot&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/dataease/sqlbot?label=downloads&quot; alt=&quot;Download&quot;&gt;&lt;/a&gt;&lt;br/&gt;

&lt;/p&gt;
&lt;hr/&gt;

SQLBot ÊòØ‰∏ÄÊ¨æÂü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü„ÄÇSQLBot ÁöÑ‰ºòÂäøÂåÖÊã¨Ôºö

- **ÂºÄÁÆ±Âç≥Áî®**: Âè™ÈúÄÈÖçÁΩÆÂ§ßÊ®°ÂûãÂíåÊï∞ÊçÆÊ∫êÂç≥ÂèØÂºÄÂêØÈóÆÊï∞‰πãÊóÖÔºåÈÄöËøáÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÁªìÂêàÊù•ÂÆûÁé∞È´òË¥®ÈáèÁöÑ text2sqlÔºõ
- **Êòì‰∫éÈõÜÊàê**: ÊîØÊåÅÂø´ÈÄüÂµåÂÖ•Âà∞Á¨¨‰∏âÊñπ‰∏öÂä°Á≥ªÁªüÔºå‰πüÊîØÊåÅË¢´ n8n„ÄÅMaxKB„ÄÅDify„ÄÅCoze Á≠â AI Â∫îÁî®ÂºÄÂèëÂπ≥Âè∞ÈõÜÊàêË∞ÉÁî®ÔºåËÆ©ÂêÑÁ±ªÂ∫îÁî®Âø´ÈÄüÊã•ÊúâÊô∫ËÉΩÈóÆÊï∞ËÉΩÂäõÔºõ
- **ÂÆâÂÖ®ÂèØÊéß**: Êèê‰æõÂü∫‰∫éÂ∑•‰ΩúÁ©∫Èó¥ÁöÑËµÑÊ∫êÈöîÁ¶ªÊú∫Âà∂ÔºåËÉΩÂ§üÂÆûÁé∞ÁªÜÁ≤íÂ∫¶ÁöÑÊï∞ÊçÆÊùÉÈôêÊéßÂà∂„ÄÇ

## Â∑•‰ΩúÂéüÁêÜ

&lt;img width=&quot;1105&quot; height=&quot;577&quot; alt=&quot;system-arch&quot; src=&quot;https://github.com/user-attachments/assets/462603fc-980b-4b8b-a6d4-a821c070a048&quot; /&gt;

## Âø´ÈÄüÂºÄÂßã

### ÂÆâË£ÖÈÉ®ÁΩ≤

ÂáÜÂ§á‰∏ÄÂè∞ Linux ÊúçÂä°Âô®ÔºåÂÆâË£ÖÂ•Ω [Docker](https://docs.docker.com/get-docker/)ÔºåÊâßË°å‰ª•‰∏ã‰∏ÄÈîÆÂÆâË£ÖËÑöÊú¨Ôºö

```bash
docker run -d \
  --name sqlbot \
  --restart unless-stopped \
  -p 8000:8000 \
  -p 8001:8001 \
  -v ./data/sqlbot/excel:/opt/sqlbot/data/excel \
  -v ./data/sqlbot/file:/opt/sqlbot/data/file \
  -v ./data/sqlbot/images:/opt/sqlbot/images \
  -v ./data/sqlbot/logs:/opt/sqlbot/app/logs \
  -v ./data/postgresql:/var/lib/postgresql/data \
  --privileged=true \
  dataease/sqlbot
```

‰Ω†‰πüÂèØ‰ª•ÈÄöËøá [1Panel Â∫îÁî®ÂïÜÂ∫ó](https://apps.fit2cloud.com/1panel) Âø´ÈÄüÈÉ®ÁΩ≤ SQLBot„ÄÇ

Â¶ÇÊûúÊòØÂÜÖÁΩëÁéØÂ¢ÉÔºå‰Ω†ÂèØ‰ª•ÈÄöËøá [Á¶ªÁ∫øÂÆâË£ÖÂåÖÊñπÂºè](https://community.fit2cloud.com/#/products/sqlbot/downloads) ÈÉ®ÁΩ≤ SQLBot„ÄÇ

### ËÆøÈóÆÊñπÂºè

- Âú®ÊµèËßàÂô®‰∏≠ÊâìÂºÄ: http://&lt;‰Ω†ÁöÑÊúçÂä°Âô®IP&gt;:8000/
- Áî®Êà∑Âêç: admin
- ÂØÜÁ†Å: SQLBot@123456

### ËÅîÁ≥ªÊàë‰ª¨

Â¶Ç‰Ω†ÊúâÊõ¥Â§öÈóÆÈ¢òÔºåÂèØ‰ª•Âä†ÂÖ•Êàë‰ª¨ÁöÑÊäÄÊúØ‰∫§ÊµÅÁæ§‰∏éÊàë‰ª¨‰∫§ÊµÅ„ÄÇ

&lt;img width=&quot;180&quot; height=&quot;180&quot; alt=&quot;contact_me_qr&quot; src=&quot;https://github.com/user-attachments/assets/2594ff29-5426-4457-b051-279855610030&quot; /&gt;

## UI Â±ïÁ§∫

  &lt;tr&gt;
    &lt;img alt=&quot;q&amp;a&quot; src=&quot;https://github.com/user-attachments/assets/55526514-52f3-4cfe-98ec-08a986259280&quot;   /&gt;
  &lt;/tr&gt;

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=dataease/sqlbot&amp;type=Date)](https://www.star-history.com/#dataease/sqlbot&amp;Date)

## È£ûËá¥‰∫ëÊóó‰∏ãÁöÑÂÖ∂‰ªñÊòéÊòüÈ°πÁõÆ

- [DataEase](https://github.com/dataease/dataease/) - ‰∫∫‰∫∫ÂèØÁî®ÁöÑÂºÄÊ∫ê BI Â∑•ÂÖ∑
- [1Panel](https://github.com/1panel-dev/1panel/) - Áé∞‰ª£Âåñ„ÄÅÂºÄÊ∫êÁöÑ Linux ÊúçÂä°Âô®ËøêÁª¥ÁÆ°ÁêÜÈù¢Êùø
- [MaxKB](https://github.com/1panel-dev/MaxKB/) - Âº∫Â§ßÊòìÁî®ÁöÑ‰ºÅ‰∏öÁ∫ßÊô∫ËÉΩ‰ΩìÂπ≥Âè∞
- [JumpServer](https://github.com/jumpserver/jumpserver/) - ÂπøÂèóÊ¨¢ËøéÁöÑÂºÄÊ∫êÂ†°ÂûíÊú∫
- [Cordys CRM](https://github.com/1Panel-dev/CordysCRM) - Êñ∞‰∏Ä‰ª£ÁöÑÂºÄÊ∫ê AI CRM Á≥ªÁªü
- [Halo](https://github.com/halo-dev/halo/) - Âº∫Â§ßÊòìÁî®ÁöÑÂºÄÊ∫êÂª∫Á´ôÂ∑•ÂÖ∑
- [MeterSphere](https://github.com/metersphere/metersphere/) - Êñ∞‰∏Ä‰ª£ÁöÑÂºÄÊ∫êÊåÅÁª≠ÊµãËØïÂ∑•ÂÖ∑

## License

Êú¨‰ªìÂ∫ìÈÅµÂæ™ [FIT2CLOUD Open Source License](LICENSE) ÂºÄÊ∫êÂçèËÆÆÔºåËØ•ËÆ∏ÂèØËØÅÊú¨Ë¥®‰∏äÊòØ GPLv3Ôºå‰ΩÜÊúâ‰∏Ä‰∫õÈ¢ùÂ§ñÁöÑÈôêÂà∂„ÄÇ

‰Ω†ÂèØ‰ª•Âü∫‰∫é SQLBot ÁöÑÊ∫ê‰ª£Á†ÅËøõË°å‰∫åÊ¨°ÂºÄÂèëÔºå‰ΩÜÊòØÈúÄË¶ÅÈÅµÂÆà‰ª•‰∏ãËßÑÂÆöÔºö

- ‰∏çËÉΩÊõøÊç¢Âíå‰øÆÊîπ SQLBot ÁöÑ Logo ÂíåÁâàÊùÉ‰ø°ÊÅØÔºõ
- ‰∫åÊ¨°ÂºÄÂèëÂêéÁöÑË°çÁîü‰ΩúÂìÅÂøÖÈ°ªÈÅµÂÆà GPL V3 ÁöÑÂºÄÊ∫ê‰πâÂä°„ÄÇ

Â¶ÇÈúÄÂïÜ‰∏öÊéàÊùÉÔºåËØ∑ËÅîÁ≥ª support@fit2cloud.com „ÄÇ
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>