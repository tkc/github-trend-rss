<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Mon, 10 Nov 2025 00:04:40 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[usestrix/strix]]></title>
            <link>https://github.com/usestrix/strix</link>
            <guid>https://github.com/usestrix/strix</guid>
            <pubDate>Mon, 10 Nov 2025 00:04:40 GMT</pubDate>
            <description><![CDATA[‚ú® Open-source AI hackers for your apps üë®üèª‚Äçüíª]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/usestrix/strix">usestrix/strix</a></h1>
            <p>‚ú® Open-source AI hackers for your apps üë®üèª‚Äçüíª</p>
            <p>Language: Python</p>
            <p>Stars: 6,709</p>
            <p>Forks: 662</p>
            <p>Stars today: 2,131 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://usestrix.com/&quot;&gt;
    &lt;img src=&quot;.github/logo.png&quot; width=&quot;150&quot; alt=&quot;Strix Logo&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt;
Strix
&lt;/h1&gt;

&lt;h2 align=&quot;center&quot;&gt;Open-source AI Hackers to secure your Apps&lt;/h2&gt;

&lt;div align=&quot;center&quot;&gt;

[![Python](https://img.shields.io/pypi/pyversions/strix-agent?color=3776AB)](https://pypi.org/project/strix-agent/)
[![PyPI](https://img.shields.io/pypi/v/strix-agent?color=10b981)](https://pypi.org/project/strix-agent/)
[![PyPI Downloads](https://static.pepy.tech/personalized-badge/strix-agent?period=total&amp;units=INTERNATIONAL_SYSTEM&amp;left_color=GREY&amp;right_color=RED&amp;left_text=Downloads)](https://pepy.tech/projects/strix-agent)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)

[![GitHub Stars](https://img.shields.io/github/stars/usestrix/strix)](https://github.com/usestrix/strix)
[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?&amp;logo=discord&amp;logoColor=white)](https://discord.gg/YjKFvEZSdZ)
[![Website](https://img.shields.io/badge/Website-usestrix.com-2d3748.svg)](https://usestrix.com)

&lt;a href=&quot;https://trendshift.io/repositories/15362&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/15362&quot; alt=&quot;usestrix%2Fstrix | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

:star: _Love Strix? Give us a star to help other developers discover it!_

&lt;br /&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;.github/screenshot.png&quot; alt=&quot;Strix Demo&quot; width=&quot;800&quot; style=&quot;border-radius: 16px; box-shadow: 0 20px 40px rgba(0, 0, 0, 0.3), 0 0 0 1px rgba(255, 255, 255, 0.1), inset 0 1px 0 rgba(255, 255, 255, 0.2); transform: perspective(1000px) rotateX(2deg); transition: transform 0.3s ease;&quot;&gt;
&lt;/div&gt;

&gt; [!TIP]
&gt; **New!** Strix now integrates seamlessly with GitHub Actions and CI/CD pipelines. Automatically scan for vulnerabilities on every pull request and block insecure code before it reaches production!

---

## ü¶â Strix Overview

Strix are autonomous AI agents that act just like real hackers - they run your code dynamically, find vulnerabilities, and validate them through actual proof-of-concepts. Built for developers and security teams who need fast, accurate security testing without the overhead of manual pentesting or the false positives of static analysis tools.

- **Full hacker toolkit** out of the box
- **Teams of agents** that collaborate and scale
- **Real validation** with PoCs, not false positives
- **Developer‚Äëfirst** CLI with actionable reports
- **Auto‚Äëfix &amp; reporting** to accelerate remediation

---

### üéØ Use Cases

- Detect and validate critical vulnerabilities in your applications.
- Get penetration tests done in hours, not weeks, with compliance reports.
- Automate bug bounty research and generate PoCs for faster reporting.
- Run tests in CI/CD to block vulnerabilities before reaching production.

---

### üöÄ Quick Start

Prerequisites:
- Docker (running)
- Python 3.12+
- An LLM provider key (or a local LLM)

```bash
# Install
pipx install strix-agent

# Configure AI provider
export STRIX_LLM=&quot;openai/gpt-5&quot;
export LLM_API_KEY=&quot;your-api-key&quot;

# Run security assessment
strix --target ./app-directory
```

First run pulls the sandbox Docker image. Results are saved under `agent_runs/&lt;run-name&gt;`.

### ‚òÅÔ∏è Cloud Hosted

Want to skip the setup? Try our cloud-hosted version: **[usestrix.com](https://usestrix.com)**

## ‚ú® Features

### üõ†Ô∏è Agentic Security Tools

- **üîå Full HTTP Proxy** - Full request/response manipulation and analysis
- **üåê Browser Automation** - Multi-tab browser for testing of XSS, CSRF, auth flows
- **üíª Terminal Environments** - Interactive shells for command execution and testing
- **üêç Python Runtime** - Custom exploit development and validation
- **üîç Reconnaissance** - Automated OSINT and attack surface mapping
- **üìÅ Code Analysis** - Static and dynamic analysis capabilities
- **üìù Knowledge Management** - Structured findings and attack documentation

### üéØ Comprehensive Vulnerability Detection

- **Access Control** - IDOR, privilege escalation, auth bypass
- **Injection Attacks** - SQL, NoSQL, command injection
- **Server-Side** - SSRF, XXE, deserialization flaws
- **Client-Side** - XSS, prototype pollution, DOM vulnerabilities
- **Business Logic** - Race conditions, workflow manipulation
- **Authentication** - JWT vulnerabilities, session management
- **Infrastructure** - Misconfigurations, exposed services

### üï∏Ô∏è Graph of Agents

- **Distributed Workflows** - Specialized agents for different attacks and assets
- **Scalable Testing** - Parallel execution for fast comprehensive coverage
- **Dynamic Coordination** - Agents collaborate and share discoveries


## üíª Usage Examples

```bash
# Local codebase analysis
strix --target ./app-directory

# Repository security review
strix --target https://github.com/org/repo

# Web application assessment
strix --target https://your-app.com

# Multi-target white-box testing (source code + deployed app)
strix -t https://github.com/org/app -t https://your-app.com

# Test multiple environments simultaneously
strix -t https://dev.your-app.com -t https://staging.your-app.com -t https://prod.your-app.com

# Focused testing with instructions
strix --target api.your-app.com --instruction &quot;Prioritize authentication and authorization testing&quot;

# Testing with credentials
strix --target https://your-app.com --instruction &quot;Test with credentials: testuser/testpass. Focus on privilege escalation and access control bypasses.&quot;
```

### ‚öôÔ∏è Configuration

```bash
export STRIX_LLM=&quot;openai/gpt-5&quot;
export LLM_API_KEY=&quot;your-api-key&quot;

# Optional
export LLM_API_BASE=&quot;your-api-base-url&quot;  # if using a local model, e.g. Ollama, LMStudio
export PERPLEXITY_API_KEY=&quot;your-api-key&quot;  # for search capabilities
```

[üìö View supported AI models](https://docs.litellm.ai/docs/providers)

### ü§ñ Headless Mode

Run Strix programmatically without interactive UI using the `-n/--non-interactive` flag‚Äîperfect for servers and automated jobs. The CLI prints real-time vulnerability findings, and the final report before exiting. Exits with non-zero code when vulnerabilities are found.

```bash
strix -n --target https://your-app.com --instruction &quot;Focus on authentication and authorization vulnerabilities&quot;
```

### üîÑ CI/CD (GitHub Actions)

Strix can be added to your pipeline to run a security test on pull requests with a lightweight GitHub Actions workflow:

```yaml
name: strix-penetration-test

on:
  pull_request:

jobs:
  security-scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install Strix
        run: pipx install strix-agent

      - name: Run Strix
        env:
          STRIX_LLM: ${{ secrets.STRIX_LLM }}
          LLM_API_KEY: ${{ secrets.LLM_API_KEY }}

        run: strix -n -t ./
```

## üèÜ Enterprise Platform

Our managed platform provides:

- **üìà Executive Dashboards**
- **üß† Custom Fine-Tuned Models**
- **‚öôÔ∏è CI/CD Integration**
- **üîç Large-Scale Scanning**
- **üîå Third-Party Integrations**
- **üéØ Enterprise Support**

[**Get Enterprise Demo ‚Üí**](https://usestrix.com)

## üîí Security Architecture

- **Container Isolation** - All testing in sandboxed Docker environments
- **Local Processing** - Testing runs locally, no data sent to external services

&gt; [!WARNING]
&gt; Only test systems you own or have permission to test. You are responsible for using Strix ethically and legally.

## ü§ù Contributing

We welcome contributions from the community! There are several ways to contribute:

### Code Contributions
See our [Contributing Guide](CONTRIBUTING.md) for details on:
- Setting up your development environment
- Running tests and quality checks
- Submitting pull requests
- Code style guidelines

### Prompt Modules Collection
Help expand our collection of specialized prompt modules for AI agents:
- Advanced testing techniques for vulnerabilities, frameworks, and technologies
- See [Prompt Modules Documentation](strix/prompts/README.md) for guidelines
- Submit via [pull requests](https://github.com/usestrix/strix/pulls) or [issues](https://github.com/usestrix/strix/issues)

## üåü Support the Project

**Love Strix?** Give us a ‚≠ê on GitHub!

## üë• Join Our Community

Have questions? Found a bug? Want to contribute? **[Join our Discord!](https://discord.gg/YjKFvEZSdZ)**

&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[thinking-machines-lab/tinker-cookbook]]></title>
            <link>https://github.com/thinking-machines-lab/tinker-cookbook</link>
            <guid>https://github.com/thinking-machines-lab/tinker-cookbook</guid>
            <pubDate>Mon, 10 Nov 2025 00:04:39 GMT</pubDate>
            <description><![CDATA[Post-training with Tinker]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/thinking-machines-lab/tinker-cookbook">thinking-machines-lab/tinker-cookbook</a></h1>
            <p>Post-training with Tinker</p>
            <p>Language: Python</p>
            <p>Stars: 1,615</p>
            <p>Forks: 130</p>
            <p>Stars today: 76 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;Tinker Cookbook&lt;/h1&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/tinker-cover.png&quot; width=&quot;60%&quot; /&gt;
&lt;/div&gt;

We provide two libraries for the broader community to customize their language models: `tinker` and `tinker-cookbook`.

- `tinker` is a training SDK for researchers and developers to fine-tune language models. You send API requests to us and we handle the complexities of distributed training.
- `tinker-cookbook` includes realistic examples of fine-tuning language models. It builds on the Tinker API and provides common abstractions to fine-tune language models.

## Installation

1. Sign up for Tinker through the [waitlist](https://thinkingmachines.ai/tinker).
2. Once you have access, create an API key from the [console](https://tinker-console.thinkingmachines.ai) and export it as environment variable `TINKER_API_KEY`.
3. Install tinker python client via `pip install tinker`
4. We recommend installing `tinker-cookbook` in a virtual env either with `conda` or `uv`. For running most examples, you can install via `pip install -e .`.

## Tinker

Refer to the [docs](https://tinker-docs.thinkingmachines.ai/training-sampling) to start from basics.
Here we introduce a few Tinker primitives - the basic components to fine-tune LLMs:

```python
service_client = tinker.ServiceClient()
training_client = service_client.create_lora_training_client(
  base_model=&quot;meta-llama/Llama-3.2-1B&quot;, rank=32,
)
training_client.forward_backward(...)
training_client.optim_step(...)
training_client.save_state(...)
training_client.load_state(...)

sampling_client = training_client.save_weights_and_get_sampling_client(name=&quot;my_model&quot;)
sampling_client.sample(...)
```

See [tinker_cookbook/recipes/sl_loop.py](tinker_cookbook/recipes/sl_loop.py) and [tinker_cookbook/recipes/rl_loop.py](tinker_cookbook/recipes/rl_loop.py) for minimal examples of using these primitives to fine-tune LLMs.

To download the weights of any model:
```python
rest_client = service_client.create_rest_client()
future = rest_client.download_checkpoint_archive_from_tinker_path(sampling_client.model_path)
with open(f&quot;model-checkpoint.tar.gz&quot;, &quot;wb&quot;) as f:
    f.write(future.result())
```

### Tinker Cookbook

Besides these primitives, we also offer **Tinker Cookbook** (a.k.a. this repo), a library of a wide range of abstractions to help you customize training environments.
[`tinker_cookbook/recipes/sl_basic.py`](tinker_cookbook/recipes/sl_basic.py) and [`tinker_cookbook/recipes/rl_basic.py`](tinker_cookbook/recipes/rl_basic.py) contain minimal examples to configure supervised learning and reinforcement learning.

We also include a wide range of more sophisticated examples in the [`tinker_cookbook/recipes/`](tinker_cookbook/recipes/) folder:
1. **[Chat supervised learning](tinker_cookbook/recipes/chat_sl/)**: supervised fine-tuning on conversational datasets like Tulu3.
2. **[Math reasoning](tinker_cookbook/recipes/math_rl/)**: improve LLM reasoning capability by rewarding it for answering math questions correctly.
3. **[Preference learning](tinker_cookbook/recipes/preference/)**: showcase a three-stage RLHF pipeline: 1) supervised fine-tuning, 2) learning a reward model, 3) RL against the reward model.
4. **[Tool use](tinker_cookbook/recipes/tool_use/)**: train LLMs to better use retrieval tools to answer questions more accurately.
5. **[Prompt distillation](tinker_cookbook/recipes/prompt_distillation/)**: internalize long and complex instructions into LLMs.
6. **[Multi-Agent](tinker_cookbook/recipes/multiplayer_rl/)**: optimize LLMs to play against another LLM or themselves.

These examples are located in each subfolder, and their `README.md` files will walk you through the key implementation details, the commands to run them, and the expected performance.

### Import our utilities

Tinker cookbook includes several utilities. Here&#039;s a quick overview:
- [`renderers`](tinker_cookbook/renderers.py) converts tokens from/to structured chat message objects
- [`hyperparam_utils`](tinker_cookbook/hyperparam_utils.py) helps calculate hyperparameters suitable for LoRAs
- [`evaluation`](tinker_cookbook/eval/evaluators.py) provides abstractions for evaluating Tinker models and [`inspect_evaluation`](tinker_cookbook/eval/inspect_evaluators.py) shows how to integrate with InspectAI to make evaluating on standard benchmarks easy.

## Contributing

This project is built in the spirit of open science and collaborative development. We believe that the best tools emerge through community involvement and shared learning.

We welcome PR contributions after our private beta is over. If you have any feedback, please email us at tinker@thinkingmachines.ai.

## Citation
If you use Tinker for your research, please cite it as:
```
Thinking Machines Lab, 2025. Tinker. https://thinkingmachines.ai/tinker/.
```

Or use this BibTeX citation:
```
@misc{tml2025tinker,
  author = {Thinking Machines Lab},
  title = {Tinker},
  year = {2025},
  url = {https://thinkingmachines.ai/tinker/},
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[airweave-ai/airweave]]></title>
            <link>https://github.com/airweave-ai/airweave</link>
            <guid>https://github.com/airweave-ai/airweave</guid>
            <pubDate>Mon, 10 Nov 2025 00:04:38 GMT</pubDate>
            <description><![CDATA[Context retrieval for AI agents across apps and databases]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/airweave-ai/airweave">airweave-ai/airweave</a></h1>
            <p>Context retrieval for AI agents across apps and databases</p>
            <p>Language: Python</p>
            <p>Stars: 4,919</p>
            <p>Forks: 588</p>
            <p>Stars today: 172 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;frontend/public/logo-airweave-darkbg.svg&quot;/&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;frontend/public/logo-airweave-lightbg.svg&quot;/&gt;
  &lt;img width=&quot;837&quot; alt=&quot;airweave-lettermark&quot; style=&quot;padding-bottom: 12px;&quot; src=&quot;frontend/public/logo-airweave-darkbg.svg&quot;/&gt;
&lt;/picture&gt;

# Context Retrieval for AI Agents across Apps &amp; Databases

[![Ruff](https://github.com/airweave-ai/airweave/actions/workflows/ruff.yml/badge.svg)](https://github.com/airweave-ai/airweave/actions/workflows/ruff.yml)
[![ESLint](https://github.com/airweave-ai/airweave/actions/workflows/eslint.yml/badge.svg)](https://github.com/airweave-ai/airweave/actions/workflows/eslint.yml)
[![System Tests](https://github.com/airweave-ai/airweave/actions/workflows/test-public-api.yml/badge.svg)](https://github.com/airweave-ai/airweave/actions/workflows/test-public-api.yml)
[![PyPI Downloads](https://static.pepy.tech/personalized-badge/airweave-sdk?period=total&amp;units=INTERNATIONAL_SYSTEM&amp;left_color=GRAY&amp;right_color=BRIGHTGREEN&amp;left_text=downloads)](https://pepy.tech/projects/airweave-sdk)
[![Discord](https://img.shields.io/discord/1323415085011701870?label=Discord&amp;logo=discord&amp;logoColor=white&amp;style=flat-square)](https://discord.gg/gDuebsWGkn)
&lt;br&gt;
&lt;div style=&quot;padding-top: 16px;&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/13748&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13748&quot; alt=&quot;airweave-ai%2Fairweave | Trendshift&quot; style=&quot;width: 250px; height: 55px; margin-right: 24px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://www.ycombinator.com/launches/NX7-airweave-let-agents-search-any-app&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://www.ycombinator.com/launches/NX7-airweave-let-agents-search-any-app/upvote_embed.svg&quot; alt=&quot;Launch YC: Airweave - Let Agents Search Any App&quot; style=&quot;margin-left: 12px;&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;br&gt;

‚≠ê **Help us reach more developers and grow the Airweave community. Star this repo!**

&lt;/div&gt;

## What is Airweave?

[Airweave](https://app.airweave.ai/) is a fully open-source context retrieval layer for AI agents across apps and databases. It connects to apps, productivity tools, databases, or document stores and transforms their contents into searchable knowledge bases, accessible through a standardized interface for agents.

The search interface is exposed via REST API or MCP. When using MCP, Airweave essentially builds a semantically searchable MCP server. The platform handles everything from auth and extraction to embedding and serving. You can find our documentation [here](https://docs.airweave.ai/welcome).

üì∫ Check out a quick demo of Airweave below:

&lt;video width=&quot;100%&quot; src=&quot;https://github.com/user-attachments/assets/995e4a36-3f88-4d8e-b401-6ca43db0c7bf&quot; controls&gt;&lt;/video&gt;

[**üîó Example notebooks**](https://github.com/airweave-ai/airweave/tree/main/examples)

## Table of Contents

- [Airweave](#airweave)
  - [Overview](#overview)
  - [Table of Contents](#table-of-contents)
  - [üöÄ Quick Start](#-quick-start)
  - [üîå Supported Integrations](#-supported-integrations)
  - [üíª Usage](#-usage)
    - [Frontend](#frontend)
    - [API](#api)
  - [üì¶ SDKs](#-sdks)
    - [Python](#python)
    - [TypeScript/JavaScript](#typescriptjavascript)
  - [üîë Key Features](#-key-features)
  - [üîß Technology Stack](#-tech-stack)
  - [üë• Contributing](#-contributing)
  - [üìÑ License](#-license)
  - [üîó Connect](#-connect)

## üöÄ Quick Start

### Managed Service: [Airweave Cloud](https://app.airweave.ai/)

### Self-hosted:

Make sure docker and docker-compose are installed, then...

```bash
# 1. Clone the repository
git clone https://github.com/airweave-ai/airweave.git
cd airweave

# 2. Build and run
chmod +x start.sh
./start.sh
```

That&#039;s it! Access the dashboard at http://localhost:8080

## üîå Supported Integrations

&lt;!-- START_APP_GRID --&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/airtable.svg&quot; alt=&quot;Airtable&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/asana.svg&quot; alt=&quot;Asana&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/attio.svg&quot; alt=&quot;Attio&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/bitbucket.svg&quot; alt=&quot;Bitbucket&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/box.svg&quot; alt=&quot;Box&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/clickup.svg&quot; alt=&quot;ClickUp&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/confluence.svg&quot; alt=&quot;Confluence&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/ctti.svg&quot; alt=&quot;CTTI&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/dropbox.svg&quot; alt=&quot;Dropbox&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/excel.svg&quot; alt=&quot;Excel&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/github.svg&quot; alt=&quot;Github&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/gitlab.svg&quot; alt=&quot;Gitlab&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/gmail.svg&quot; alt=&quot;Gmail&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/google_calendar.svg&quot; alt=&quot;Google Calendar&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/google_docs.svg&quot; alt=&quot;Google Docs&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/google_drive.svg&quot; alt=&quot;Google Drive&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/google_slides.svg&quot; alt=&quot;Google Slides&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/hubspot.svg&quot; alt=&quot;Hubspot&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/jira.svg&quot; alt=&quot;Jira&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/linear.svg&quot; alt=&quot;Linear&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/monday.svg&quot; alt=&quot;Monday&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/notion.svg&quot; alt=&quot;Notion&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/onedrive.svg&quot; alt=&quot;Onedrive&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/onenote.svg&quot; alt=&quot;OneNote&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/outlook_calendar.svg&quot; alt=&quot;Outlook Calendar&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/outlook_mail.svg&quot; alt=&quot;Outlook Mail&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/postgresql.svg&quot; alt=&quot;Postgresql&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/salesforce.svg&quot; alt=&quot;Salesforce&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/sharepoint.svg&quot; alt=&quot;Sharepoint&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/slack.svg&quot; alt=&quot;Slack&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/stripe.svg&quot; alt=&quot;Stripe&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/teams.svg&quot; alt=&quot;Teams&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/todoist.svg&quot; alt=&quot;Todoist&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/trello.svg&quot; alt=&quot;Trello&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/word.svg&quot; alt=&quot;Word&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/zendesk.svg&quot; alt=&quot;Zendesk&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;/p&gt;

&lt;!-- END_APP_GRID --&gt;

## üíª Usage

### Frontend
- Access the UI at `http://localhost:8080`
- Connect sources, configure syncs, and query data

### API
- Swagger docs: `http://localhost:8001/docs`
- Create connections, trigger syncs, and search data

## üì¶ SDKs

### Python

```bash
pip install airweave-sdk
```

```python
from airweave import AirweaveSDK

# Initialize client
client = AirweaveSDK(
    api_key=&quot;YOUR_API_KEY&quot;,
    base_url=&quot;http://localhost:8001&quot;
)

# Create a collection
collection = client.collections.create(name=&quot;My Collection&quot;)

# Add a source connection
source = client.source_connections.create(
    name=&quot;My Stripe Connection&quot;,
    short_name=&quot;stripe&quot;,
    readable_collection_id=collection.readable_id,
    authentication={
        &quot;credentials&quot;: {&quot;api_key&quot;: &quot;your_stripe_api_key&quot;}
    }
)

# Semantic search (default)
results = client.collections.search(
    readable_id=collection.readable_id,
    query=&quot;Find recent failed payments&quot;
)

# Hybrid search (semantic + keyword)
results = client.collections.search(
    readable_id=collection.readable_id,
    query=&quot;customer invoices Q4 2024&quot;,
    search_type=&quot;hybrid&quot;
)

# With query expansion and reranking
results = client.collections.search(
    readable_id=collection.readable_id,
    query=&quot;technical documentation&quot;,
    enable_query_expansion=True,
    enable_reranking=True,
    top_k=20
)

# Search with recency bias (prioritize recent results)
results = client.collections.search(
    readable_id=collection.readable_id,
    query=&quot;critical bugs&quot;,
    recency_bias=0.8,  # 0.0 to 1.0, higher = more recent
    limit=10
)

# Get AI-generated answer instead of raw results
answer = client.collections.search(
    readable_id=collection.readable_id,
    query=&quot;What are our customer refund policies?&quot;,
    response_type=&quot;completion&quot;,
    enable_reranking=True
)
```

### TypeScript/JavaScript
```bash
npm install @airweave/sdk
# or
yarn add @airweave/sdk
```

```typescript
import { AirweaveSDKClient, AirweaveSDKEnvironment } from &quot;@airweave/sdk&quot;;

// Initialize client
const client = new AirweaveSDKClient({
    apiKey: &quot;YOUR_API_KEY&quot;,
    environment: AirweaveSDKEnvironment.Local
});

// Create a collection
const collection = await client.collections.create({
    name: &quot;My Collection&quot;
});

// Add a source connection
const source = await client.sourceConnections.create({
    name: &quot;My Stripe Connection&quot;,
    shortName: &quot;stripe&quot;,
    readableCollectionId: collection.readableId,
    authentication: {
        credentials: { apiKey: &quot;your_stripe_api_key&quot; }
    }
});

// Semantic search (default)
const results = await client.collections.search(
    collection.readableId,
    { query: &quot;Find recent failed payments&quot; }
);

// Hybrid search (semantic + keyword)
const hybridResults = await client.collections.search(
    collection.readableId,
    {
        query: &quot;customer invoices Q4 2024&quot;,
        searchType: &quot;hybrid&quot;
    }
);

// With query expansion and reranking
const advancedResults = await client.collections.search(
    collection.readableId,
    {
        query: &quot;technical documentation&quot;,
        enableQueryExpansion: true,
        enableReranking: true,
        topK: 20
    }
);

// Search with recency bias (prioritize recent results)
const recentResults = await client.collections.search(
    collection.readableId,
    {
        query: &quot;critical bugs&quot;,
        recencyBias: 0.8,  // 0.0 to 1.0, higher = more recent
        limit: 10
    }
);

// Get AI-generated answer instead of raw results
const answer = await client.collections.search(
    collection.readableId,
    {
        query: &quot;What are our customer refund policies?&quot;,
        responseType: &quot;completion&quot;,
        enableReranking: true
    }
);
```

## üîë Key Features

- **Data synchronization** from 30+ sources with minimal config
- **Entity extraction** and transformation pipeline
- **Multi-tenant** architecture with OAuth2
- **Incremental updates** using content hashing
- **Semantic search** for agent queries
- **Versioning** for data changes

## üîß Tech Stack

- **Frontend**: React/TypeScript with ShadCN
- **Backend**: FastAPI (Python)
- **Databases**: PostgreSQL (metadata), Qdrant (vectors)
- **Workers**: Temporal (workflow orchestration), Redis (pub/sub)
- **Deployment**: Docker Compose (dev), Kubernetes (prod)

## üë• Contributing

We welcome contributions! Please check [CONTRIBUTING.md](https://github.com/airweave-ai/airweave/blob/main/CONTRIBUTING.md) for details.

## üìÑ License

Airweave is released under the [MIT](LICENSE) license.

## üîó Connect

- **[Discord](https://discord.com/invite/484HY9Ehxt)** - Get help and discuss features
- **[GitHub Issues](https://github.com/airweave-ai/airweave/issues)** - Report bugs or request features
- **[Twitter](https://x.com/airweave_ai)** - Follow for updates
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[AUTOMATIC1111/stable-diffusion-webui]]></title>
            <link>https://github.com/AUTOMATIC1111/stable-diffusion-webui</link>
            <guid>https://github.com/AUTOMATIC1111/stable-diffusion-webui</guid>
            <pubDate>Mon, 10 Nov 2025 00:04:37 GMT</pubDate>
            <description><![CDATA[Stable Diffusion web UI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">AUTOMATIC1111/stable-diffusion-webui</a></h1>
            <p>Stable Diffusion web UI</p>
            <p>Language: Python</p>
            <p>Stars: 157,992</p>
            <p>Forks: 29,329</p>
            <p>Stars today: 43 stars today</p>
            <h2>README</h2><pre># Stable Diffusion web UI
A web interface for Stable Diffusion, implemented using Gradio library.

![](screenshot.png)

## Features
[Detailed feature showcase with images](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features):
- Original txt2img and img2img modes
- One click install and run script (but you still must install python and git)
- Outpainting
- Inpainting
- Color Sketch
- Prompt Matrix
- Stable Diffusion Upscale
- Attention, specify parts of text that the model should pay more attention to
    - a man in a `((tuxedo))` - will pay more attention to tuxedo
    - a man in a `(tuxedo:1.21)` - alternative syntax
    - select text and press `Ctrl+Up` or `Ctrl+Down` (or `Command+Up` or `Command+Down` if you&#039;re on a MacOS) to automatically adjust attention to selected text (code contributed by anonymous user)
- Loopback, run img2img processing multiple times
- X/Y/Z plot, a way to draw a 3 dimensional plot of images with different parameters
- Textual Inversion
    - have as many embeddings as you want and use any names you like for them
    - use multiple embeddings with different numbers of vectors per token
    - works with half precision floating point numbers
    - train embeddings on 8GB (also reports of 6GB working)
- Extras tab with:
    - GFPGAN, neural network that fixes faces
    - CodeFormer, face restoration tool as an alternative to GFPGAN
    - RealESRGAN, neural network upscaler
    - ESRGAN, neural network upscaler with a lot of third party models
    - SwinIR and Swin2SR ([see here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/2092)), neural network upscalers
    - LDSR, Latent diffusion super resolution upscaling
- Resizing aspect ratio options
- Sampling method selection
    - Adjust sampler eta values (noise multiplier)
    - More advanced noise setting options
- Interrupt processing at any time
- 4GB video card support (also reports of 2GB working)
- Correct seeds for batches
- Live prompt token length validation
- Generation parameters
     - parameters you used to generate images are saved with that image
     - in PNG chunks for PNG, in EXIF for JPEG
     - can drag the image to PNG info tab to restore generation parameters and automatically copy them into UI
     - can be disabled in settings
     - drag and drop an image/text-parameters to promptbox
- Read Generation Parameters Button, loads parameters in promptbox to UI
- Settings page
- Running arbitrary python code from UI (must run with `--allow-code` to enable)
- Mouseover hints for most UI elements
- Possible to change defaults/mix/max/step values for UI elements via text config
- Tiling support, a checkbox to create images that can be tiled like textures
- Progress bar and live image generation preview
    - Can use a separate neural network to produce previews with almost none VRAM or compute requirement
- Negative prompt, an extra text field that allows you to list what you don&#039;t want to see in generated image
- Styles, a way to save part of prompt and easily apply them via dropdown later
- Variations, a way to generate same image but with tiny differences
- Seed resizing, a way to generate same image but at slightly different resolution
- CLIP interrogator, a button that tries to guess prompt from an image
- Prompt Editing, a way to change prompt mid-generation, say to start making a watermelon and switch to anime girl midway
- Batch Processing, process a group of files using img2img
- Img2img Alternative, reverse Euler method of cross attention control
- Highres Fix, a convenience option to produce high resolution pictures in one click without usual distortions
- Reloading checkpoints on the fly
- Checkpoint Merger, a tab that allows you to merge up to 3 checkpoints into one
- [Custom scripts](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Custom-Scripts) with many extensions from community
- [Composable-Diffusion](https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/), a way to use multiple prompts at once
     - separate prompts using uppercase `AND`
     - also supports weights for prompts: `a cat :1.2 AND a dog AND a penguin :2.2`
- No token limit for prompts (original stable diffusion lets you use up to 75 tokens)
- DeepDanbooru integration, creates danbooru style tags for anime prompts
- [xformers](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers), major speed increase for select cards: (add `--xformers` to commandline args)
- via extension: [History tab](https://github.com/yfszzx/stable-diffusion-webui-images-browser): view, direct and delete images conveniently within the UI
- Generate forever option
- Training tab
     - hypernetworks and embeddings options
     - Preprocessing images: cropping, mirroring, autotagging using BLIP or deepdanbooru (for anime)
- Clip skip
- Hypernetworks
- Loras (same as Hypernetworks but more pretty)
- A separate UI where you can choose, with preview, which embeddings, hypernetworks or Loras to add to your prompt
- Can select to load a different VAE from settings screen
- Estimated completion time in progress bar
- API
- Support for dedicated [inpainting model](https://github.com/runwayml/stable-diffusion#inpainting-with-stable-diffusion) by RunwayML
- via extension: [Aesthetic Gradients](https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients), a way to generate images with a specific aesthetic by using clip images embeds (implementation of [https://github.com/vicgalle/stable-diffusion-aesthetic-gradients](https://github.com/vicgalle/stable-diffusion-aesthetic-gradients))
- [Stable Diffusion 2.0](https://github.com/Stability-AI/stablediffusion) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#stable-diffusion-20) for instructions
- [Alt-Diffusion](https://arxiv.org/abs/2211.06679) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#alt-diffusion) for instructions
- Now without any bad letters!
- Load checkpoints in safetensors format
- Eased resolution restriction: generated image&#039;s dimensions must be a multiple of 8 rather than 64
- Now with a license!
- Reorder elements in the UI from settings screen
- [Segmind Stable Diffusion](https://huggingface.co/segmind/SSD-1B) support

## Installation and Running
Make sure the required [dependencies](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Dependencies) are met and follow the instructions available for:
- [NVidia](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs) (recommended)
- [AMD](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-AMD-GPUs) GPUs.
- [Intel CPUs, Intel GPUs (both integrated and discrete)](https://github.com/openvinotoolkit/stable-diffusion-webui/wiki/Installation-on-Intel-Silicon) (external wiki page)
- [Ascend NPUs](https://github.com/wangshuai09/stable-diffusion-webui/wiki/Install-and-run-on-Ascend-NPUs) (external wiki page)

Alternatively, use online services (like Google Colab):

- [List of Online Services](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services)

### Installation on Windows 10/11 with NVidia-GPUs using release package
1. Download `sd.webui.zip` from [v1.0.0-pre](https://github.com/AUTOMATIC1111/stable-diffusion-webui/releases/tag/v1.0.0-pre) and extract its contents.
2. Run `update.bat`.
3. Run `run.bat`.
&gt; For more details see [Install-and-Run-on-NVidia-GPUs](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs)

### Automatic Installation on Windows
1. Install [Python 3.10.6](https://www.python.org/downloads/release/python-3106/) (Newer version of Python does not support torch), checking &quot;Add Python to PATH&quot;.
2. Install [git](https://git-scm.com/download/win).
3. Download the stable-diffusion-webui repository, for example by running `git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git`.
4. Run `webui-user.bat` from Windows Explorer as normal, non-administrator, user.

### Automatic Installation on Linux
1. Install the dependencies:
```bash
# Debian-based:
sudo apt install wget git python3 python3-venv libgl1 libglib2.0-0
# Red Hat-based:
sudo dnf install wget git python3 gperftools-libs libglvnd-glx
# openSUSE-based:
sudo zypper install wget git python3 libtcmalloc4 libglvnd
# Arch-based:
sudo pacman -S wget git python3
```
If your system is very new, you need to install python3.11 or python3.10:
```bash
# Ubuntu 24.04
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt update
sudo apt install python3.11

# Manjaro/Arch
sudo pacman -S yay
yay -S python311 # do not confuse with python3.11 package

# Only for 3.11
# Then set up env variable in launch script
export python_cmd=&quot;python3.11&quot;
# or in webui-user.sh
python_cmd=&quot;python3.11&quot;
```
2. Navigate to the directory you would like the webui to be installed and execute the following command:
```bash
wget -q https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh
```
Or just clone the repo wherever you want:
```bash
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui
```

3. Run `webui.sh`.
4. Check `webui-user.sh` for options.
### Installation on Apple Silicon

Find the instructions [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Installation-on-Apple-Silicon).

## Contributing
Here&#039;s how to add code to this repo: [Contributing](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)

## Documentation

The documentation was moved from this README over to the project&#039;s [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki).

For the purposes of getting Google and other search engines to crawl the wiki, here&#039;s a link to the (not for humans) [crawlable wiki](https://github-wiki-see.page/m/AUTOMATIC1111/stable-diffusion-webui/wiki).

## Credits
Licenses for borrowed code can be found in `Settings -&gt; Licenses` screen, and also in `html/licenses.html` file.

- Stable Diffusion - https://github.com/Stability-AI/stablediffusion, https://github.com/CompVis/taming-transformers, https://github.com/mcmonkey4eva/sd3-ref
- k-diffusion - https://github.com/crowsonkb/k-diffusion.git
- Spandrel - https://github.com/chaiNNer-org/spandrel implementing
  - GFPGAN - https://github.com/TencentARC/GFPGAN.git
  - CodeFormer - https://github.com/sczhou/CodeFormer
  - ESRGAN - https://github.com/xinntao/ESRGAN
  - SwinIR - https://github.com/JingyunLiang/SwinIR
  - Swin2SR - https://github.com/mv-lab/swin2sr
- LDSR - https://github.com/Hafiidz/latent-diffusion
- MiDaS - https://github.com/isl-org/MiDaS
- Ideas for optimizations - https://github.com/basujindal/stable-diffusion
- Cross Attention layer optimization - Doggettx - https://github.com/Doggettx/stable-diffusion, original idea for prompt editing.
- Cross Attention layer optimization - InvokeAI, lstein - https://github.com/invoke-ai/InvokeAI (originally http://github.com/lstein/stable-diffusion)
- Sub-quadratic Cross Attention layer optimization - Alex Birch (https://github.com/Birch-san/diffusers/pull/1), Amin Rezaei (https://github.com/AminRezaei0x443/memory-efficient-attention)
- Textual Inversion - Rinon Gal - https://github.com/rinongal/textual_inversion (we&#039;re not using his code, but we are using his ideas).
- Idea for SD upscale - https://github.com/jquesnelle/txt2imghd
- Noise generation for outpainting mk2 - https://github.com/parlance-zz/g-diffuser-bot
- CLIP interrogator idea and borrowing some code - https://github.com/pharmapsychotic/clip-interrogator
- Idea for Composable Diffusion - https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch
- xformers - https://github.com/facebookresearch/xformers
- DeepDanbooru - interrogator for anime diffusers https://github.com/KichangKim/DeepDanbooru
- Sampling in float32 precision from a float16 UNet - marunine for the idea, Birch-san for the example Diffusers implementation (https://github.com/Birch-san/diffusers-play/tree/92feee6)
- Instruct pix2pix - Tim Brooks (star), Aleksander Holynski (star), Alexei A. Efros (no star) - https://github.com/timothybrooks/instruct-pix2pix
- Security advice - RyotaK
- UniPC sampler - Wenliang Zhao - https://github.com/wl-zhao/UniPC
- TAESD - Ollin Boer Bohan - https://github.com/madebyollin/taesd
- LyCORIS - KohakuBlueleaf
- Restart sampling - lambertae - https://github.com/Newbeeer/diffusion_restart_sampling
- Hypertile - tfernd - https://github.com/tfernd/HyperTile
- Initial Gradio script - posted on 4chan by an Anonymous user. Thank you Anonymous user.
- (You)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Shubhamsaboo/awesome-llm-apps]]></title>
            <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
            <guid>https://github.com/Shubhamsaboo/awesome-llm-apps</guid>
            <pubDate>Mon, 10 Nov 2025 00:04:36 GMT</pubDate>
            <description><![CDATA[Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Shubhamsaboo/awesome-llm-apps">Shubhamsaboo/awesome-llm-apps</a></h1>
            <p>Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p>
            <p>Language: Python</p>
            <p>Stars: 76,219</p>
            <p>Forks: 10,048</p>
            <p>Stars today: 417 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;http://www.theunwindai.com&quot;&gt;
    &lt;img src=&quot;docs/banner/unwind_black.png&quot; width=&quot;900px&quot; alt=&quot;Unwind AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.linkedin.com/in/shubhamsaboo/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;style=flat-square&quot; alt=&quot;LinkedIn&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/Saboo_Shubham_&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/Shubham_Saboo&quot; alt=&quot;Twitter&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de&quot;&gt;Deutsch&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es&quot;&gt;Espa√±ol&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr&quot;&gt;fran√ßais&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko&quot;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt&quot;&gt;Portugu√™s&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru&quot;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh&quot;&gt;‰∏≠Êñá&lt;/a&gt;
&lt;/p&gt;

&lt;hr/&gt;

# üåü Awesome LLM Apps

A curated collection of **Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.** This repository features LLM apps that use models from &lt;img src=&quot;https://cdn.simpleicons.org/openai&quot;  alt=&quot;openai logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**OpenAI** , &lt;img src=&quot;https://cdn.simpleicons.org/anthropic&quot;  alt=&quot;anthropic logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**Anthropic**, &lt;img src=&quot;https://cdn.simpleicons.org/googlegemini&quot;  alt=&quot;google logo&quot; width=&quot;25&quot; height=&quot;18&quot;&gt;**Google**, &lt;img src=&quot;https://cdn.simpleicons.org/x&quot;  alt=&quot;X logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**xAI** and open-source models like &lt;img src=&quot;https://cdn.simpleicons.org/alibabacloud&quot;  alt=&quot;alibaba logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**Qwen** or  &lt;img src=&quot;https://cdn.simpleicons.org/meta&quot;  alt=&quot;meta logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt;**Llama** that you can run locally on your computer.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/9876&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/9876&quot; alt=&quot;Shubhamsaboo%2Fawesome-llm-apps | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## ü§î Why Awesome LLM Apps?

- üí° Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.
- üî• Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp; RAG.
- üéì Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.

## üôè Thanks to our sponsors

&lt;table align=&quot;center&quot; cellpadding=&quot;16&quot; cellspacing=&quot;12&quot;&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://getunblocked.com/unblocked-mcp/?utm_source=oss&amp;utm_medium=sponsorship&amp;utm_campaign=awesome-llm-apps&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;Unblocked&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsors/unblocked.png&quot; alt=&quot;Unblocked&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://getunblocked.com/unblocked-mcp/?utm_source=oss&amp;utm_medium=sponsorship&amp;utm_campaign=awesome-llm-apps&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        Unblocked
      &lt;/a&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://okara.ai/?utm_source=oss&amp;utm_medium=sponsorship&amp;utm_campaign=awesome-llm-apps&quot; title=&quot;Okara&quot;&gt;
        &lt;img src=&quot;docs/banner/sponsors/okara.png&quot; alt=&quot;Okara&quot; width=&quot;500&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;a href=&quot;https://okara.ai/?utm_source=oss&amp;utm_medium=sponsorship&amp;utm_campaign=awesome-llm-apps&quot; style=&quot;text-decoration: none; color: #333; font-weight: bold; font-size: 18px;&quot;&gt;
        Okara AI
      &lt;/a&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://sponsorunwindai.com&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/_Sponsor_Us-FF69B4?style=for-the-badge&amp;logo=github-sponsors&amp;logoColor=white&quot; alt=&quot;Sponsor Us&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

## üìÇ Featured AI Projects

### AI Agents

### üå± Starter AI Agents

*   [üéôÔ∏è AI Blog to Podcast Agent](starter_ai_agents/ai_blog_to_podcast_agent/)
*   [‚ù§Ô∏è‚Äçü©π AI Breakup Recovery Agent](starter_ai_agents/ai_breakup_recovery_agent/)
*   [üìä AI Data Analysis Agent](starter_ai_agents/ai_data_analysis_agent/)
*   [ü©ª AI Medical Imaging Agent](starter_ai_agents/ai_medical_imaging_agent/)
*   [üòÇ AI Meme Generator Agent (Browser)](starter_ai_agents/ai_meme_generator_agent_browseruse/)
*   [üéµ AI Music Generator Agent](starter_ai_agents/ai_music_generator_agent/)
*   [üõ´ AI Travel Agent (Local &amp; Cloud)](starter_ai_agents/ai_travel_agent/)
*   [‚ú® Gemini Multimodal Agent](starter_ai_agents/gemini_multimodal_agent_demo/)
*   [üîÑ Mixture of Agents](starter_ai_agents/mixture_of_agents/)
*   [üìä xAI Finance Agent](starter_ai_agents/xai_finance_agent/)
*   [üîç OpenAI Research Agent](starter_ai_agents/opeani_research_agent/)
*   [üï∏Ô∏è Web Scrapping AI Agent (Local &amp; Cloud)](starter_ai_agents/web_scrapping_ai_agent/)

### üöÄ Advanced AI Agents
*   [üèöÔ∏è üçå AI Home Renovation Agent with Nano Banana](advanced_ai_agents/multi_agent_apps/ai_home_renovation_agent)
*   [üîç AI Deep Research Agent](advanced_ai_agents/single_agent_apps/ai_deep_research_agent/)
*   [ü§ù AI Consultant Agent](advanced_ai_agents/single_agent_apps/ai_consultant_agent)
*   [üèóÔ∏è AI System Architect Agent](advanced_ai_agents/single_agent_apps/ai_system_architect_r1/)
*   [üéØ AI Lead Generation Agent](advanced_ai_agents/single_agent_apps/ai_lead_generation_agent/)
*   [üí∞ AI Financial Coach Agent](advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/)
*   [üé¨ AI Movie Production Agent](advanced_ai_agents/single_agent_apps/ai_movie_production_agent/)
*   [üìà AI Investment Agent](advanced_ai_agents/single_agent_apps/ai_investment_agent/)
*   [üèãÔ∏è‚Äç‚ôÇÔ∏è AI Health &amp; Fitness Agent](advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/)
*   [üöÄ AI Product Launch Intelligence Agent](advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent)
*   [üóûÔ∏è AI Journalist Agent](advanced_ai_agents/single_agent_apps/ai_journalist_agent/)
*   [üß† AI Mental Wellbeing Agent](advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/)
*   [üìë AI Meeting Agent](advanced_ai_agents/single_agent_apps/ai_meeting_agent/)
*   [üß¨ AI Self-Evolving Agent](advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/)
*   [üéß AI Social Media News and Podcast Agent](advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/)

### üéÆ Autonomous Game Playing Agents

*   [üéÆ AI 3D Pygame Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/)
*   [‚ôú AI Chess Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/)
*   [üé≤ AI Tic-Tac-Toe Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/)

### ü§ù Multi-agent Teams

*   [üß≤ AI Competitor Intelligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/)
*   [üí≤ AI Finance Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/)
*   [üé® AI Game Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/)
*   [üë®‚Äç‚öñÔ∏è AI Legal Agent Team (Cloud &amp; Local)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/)
*   [üíº AI Recruitment Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/)
*   [üè† AI Real Estate Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_real_estate_agent_team)
*   [üë®‚Äçüíº AI Services Agency (CrewAI)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/)
*   [üë®‚Äçüè´ AI Teaching Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/)
*   [üíª Multimodal Coding Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/)
*   [‚ú® Multimodal Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/)
*   [üé® üçå Multimodal UI/UX Feedback Agent Team with Nano Banana](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_uiux_feedback_agent_team/)
*   [üåè AI Travel Planner Agent Team](/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/)

### üó£Ô∏è Voice AI Agents

*   [üó£Ô∏è AI Audio Tour Agent](voice_ai_agents/ai_audio_tour_agent/)
*   [üìû Customer Support Voice Agent](voice_ai_agents/customer_support_voice_agent/)
*   [üîä Voice RAG Agent (OpenAI SDK)](voice_ai_agents/voice_rag_openaisdk/)


### &lt;img src=&quot;https://cdn.simpleicons.org/modelcontextprotocol&quot;  alt=&quot;mcp logo&quot; width=&quot;25&quot; height=&quot;20&quot;&gt; MCP AI Agents 

*   [‚ôæÔ∏è Browser MCP Agent](mcp_ai_agents/browser_mcp_agent/)
*   [üêô GitHub MCP Agent](mcp_ai_agents/github_mcp_agent/)
*   [üìë Notion MCP Agent](mcp_ai_agents/notion_mcp_agent) 
*   [üåç AI Travel Planner MCP Agent](mcp_ai_agents/ai_travel_planner_mcp_agent_team)

### üìÄ RAG (Retrieval Augmented Generation)
*   [üî• Agentic RAG with Embedding Gemma](rag_tutorials/agentic_rag_embedding_gemma)
*   [üßê Agentic RAG with Reasoning](rag_tutorials/agentic_rag_with_reasoning/)
*   [üì∞ AI Blog Search (RAG)](rag_tutorials/ai_blog_search/)
*   [üîç Autonomous RAG](rag_tutorials/autonomous_rag/)
*   [üîÑ Contextual AI RAG Agent](rag_tutorials/contextualai_rag_agent/)
*   [üîÑ Corrective RAG (CRAG)](rag_tutorials/corrective_rag/)
*   [üêã Deepseek Local RAG Agent](rag_tutorials/deepseek_local_rag_agent/)
*   [ü§î Gemini Agentic RAG](rag_tutorials/gemini_agentic_rag/)
*   [üëÄ Hybrid Search RAG (Cloud)](rag_tutorials/hybrid_search_rag/)
*   [üîÑ Llama 3.1 Local RAG](rag_tutorials/llama3.1_local_rag/)
*   [üñ•Ô∏è Local Hybrid Search RAG](rag_tutorials/local_hybrid_search_rag/)
*   [ü¶ô Local RAG Agent](rag_tutorials/local_rag_agent/)
*   [üß© RAG-as-a-Service](rag_tutorials/rag-as-a-service/)
*   [‚ú® RAG Agent with Cohere](rag_tutorials/rag_agent_cohere/)
*   [‚õìÔ∏è Basic RAG Chain](rag_tutorials/rag_chain/)
*   [üì† RAG with Database Routing](rag_tutorials/rag_database_routing/)
*   [üñºÔ∏è Vision RAG](rag_tutorials/vision_rag/)

### üíæ LLM Apps with Memory Tutorials

*   [üíæ AI ArXiv Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/)
*   [üõ©Ô∏è AI Travel Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/)
*   [üí¨ Llama3 Stateful Chat](advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/)
*   [üìù LLM App with Personalized Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/)
*   [üóÑÔ∏è Local ChatGPT Clone with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/)
*   [üß† Multi-LLM Application with Shared Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/)


### üí¨ Chat with X Tutorials

*   [üí¨ Chat with GitHub (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_github/)
*   [üì® Chat with Gmail](advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/)
*   [üìÑ Chat with PDF (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/)
*   [üìö Chat with Research Papers (ArXiv) (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/)
*   [üìù Chat with Substack](advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/)
*   [üìΩÔ∏è Chat with YouTube Videos](advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/)

### üîß LLM Fine-tuning Tutorials

* &lt;img src=&quot;https://cdn.simpleicons.org/google&quot;  alt=&quot;google logo&quot; width=&quot;20&quot; height=&quot;15&quot;&gt; [Gemma 3 Fine-tuning](advanced_llm_apps/llm_finetuning_tutorials/gemma3_finetuning/)
* &lt;img src=&quot;https://cdn.simpleicons.org/meta&quot;  alt=&quot;meta logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt; [Llama 3.2 Fine-tuning](advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/)


### üßë‚Äçüè´ AI Agent Framework Crash Course

&lt;img src=&quot;https://cdn.simpleicons.org/google&quot;  alt=&quot;google logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt; [Google ADK Crash Course](ai_agent_framework_crash_course/google_adk_crash_course/)
  - Starter agent; model‚Äëagnostic (OpenAI, Claude)
  - Structured outputs (Pydantic)
  - Tools: built‚Äëin, function, third‚Äëparty, MCP tools
  - Memory; callbacks; Plugins
  - Simple multi‚Äëagent; Multi‚Äëagent patterns

&lt;img src=&quot;https://cdn.simpleicons.org/openai&quot;  alt=&quot;openai logo&quot; width=&quot;25&quot; height=&quot;15&quot;&gt; [OpenAI Agents SDK Crash Course](ai_agent_framework_crash_course/openai_sdk_crash_course/)
  - Starter agent; function calling; structured outputs
  - Tools: built‚Äëin, function, third‚Äëparty integrations
  - Memory; callbacks; evaluation
  - Multi‚Äëagent patterns; agent handoffs
  - Swarm orchestration; routing logic

## üöÄ Getting Started

1. **Clone the repository** 

    ```bash 
    git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
    ```

2. **Navigate to the desired project directory**

    ```bash 
    cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
    ```

3. **Install the required dependencies**

    ```bash
    pip install -r requirements.txt
    ```

4. **Follow the project-specific instructions** in each project&#039;s `README.md` file to set up and run the app.


### &lt;img src=&quot;https://cdn.simpleicons.org/github&quot;  alt=&quot;github logo&quot; width=&quot;25&quot; height=&quot;20&quot;&gt; Thank You, Community, for the Support! üôè

[![Star History Chart](https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;type=Date)](https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;Date)

üåü **Don‚Äôt miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[inventree/InvenTree]]></title>
            <link>https://github.com/inventree/InvenTree</link>
            <guid>https://github.com/inventree/InvenTree</guid>
            <pubDate>Mon, 10 Nov 2025 00:04:35 GMT</pubDate>
            <description><![CDATA[Open Source Inventory Management System]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/inventree/InvenTree">inventree/InvenTree</a></h1>
            <p>Open Source Inventory Management System</p>
            <p>Language: Python</p>
            <p>Stars: 5,981</p>
            <p>Forks: 1,145</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/images/logo/inventree.png&quot; alt=&quot;InvenTree logo&quot; width=&quot;200&quot; height=&quot;auto&quot; /&gt;
  &lt;h1&gt;InvenTree&lt;/h1&gt;
  &lt;p&gt;Open Source Inventory Management System &lt;/p&gt;

&lt;!-- Badges --&gt;
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/license/MIT)![GitHub tag (latest SemVer)](https://img.shields.io/github/v/tag/inventree/inventree)
![CI](https://github.com/inventree/inventree/actions/workflows/qc_checks.yaml/badge.svg)
[![Documentation Status](https://readthedocs.org/projects/inventree/badge/?version=latest)](https://inventree.readthedocs.io/en/latest/?badge=latest)
![Docker Build](https://github.com/inventree/inventree/actions/workflows/docker.yaml/badge.svg)
[![Netlify Status](https://api.netlify.com/api/v1/badges/9bbb2101-0a4d-41e7-ad56-b63fb6053094/deploy-status)](https://app.netlify.com/sites/inventree/deploys)
[![Performance Testing](https://dev.azure.com/InvenTree/InvenTree%20test%20statistics/_apis/build/status%2Fmatmair.InvenTree?branchName=testing)](https://dev.azure.com/InvenTree/InvenTree%20test%20statistics/_build/latest?definitionId=3&amp;branchName=testing)

[![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/7179/badge)](https://bestpractices.coreinfrastructure.org/projects/7179)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/inventree/InvenTree/badge)](https://securityscorecards.dev/viewer/?uri=github.com/inventree/InvenTree)
[![Maintainability Rating](https://sonarcloud.io/api/project_badges/measure?project=inventree_InvenTree&amp;metric=sqale_rating)](https://sonarcloud.io/summary/new_code?id=inventree_InvenTree)

[![codecov](https://codecov.io/gh/inventree/InvenTree/graph/badge.svg?token=9DZRGUUV7B)](https://codecov.io/gh/inventree/InvenTree)
[![Crowdin](https://badges.crowdin.net/inventree/localized.svg)](https://crowdin.com/project/inventree)
![GitHub commit activity](https://img.shields.io/github/commit-activity/m/inventree/inventree)
[![Docker Pulls](https://img.shields.io/docker/pulls/inventree/inventree)](https://hub.docker.com/r/inventree/inventree)

[![GitHub Org&#039;s stars](https://img.shields.io/github/stars/inventree?style=social)](https://github.com/inventree/InvenTree/)
[![Twitter Follow](https://img.shields.io/twitter/follow/inventreedb?style=social)](https://twitter.com/inventreedb)
[![Subreddit subscribers](https://img.shields.io/reddit/subreddit-subscribers/inventree?style=social)](https://www.reddit.com/r/InvenTree/)
[![Mastdon](https://img.shields.io/badge/dynamic/json?label=Mastodon&amp;query=followers_count&amp;url=https%3A%2F%2Fchaos.social%2Fapi%2Fv1%2Faccounts%2Flookup%3Facct=InvenTree&amp;logo=mastodon&amp;style=social)](https://chaos.social/@InvenTree)

&lt;h4&gt;
    &lt;a href=&quot;https://demo.inventree.org/&quot;&gt;View Demo&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
    &lt;a href=&quot;https://docs.inventree.org/en/latest/&quot;&gt;Documentation&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
    &lt;a href=&quot;https://github.com/inventree/InvenTree/issues/new?template=bug_report.md&amp;title=[BUG]&quot;&gt;Report Bug&lt;/a&gt;
  &lt;span&gt; ¬∑ &lt;/span&gt;
    &lt;a href=&quot;https://github.com/inventree/InvenTree/issues/new?template=feature_request.md&amp;title=[FR]&quot;&gt;Request Feature&lt;/a&gt;
  &lt;/h4&gt;
&lt;/div&gt;

&lt;!-- About the Project --&gt;
## :star2: About the Project

InvenTree is an open-source Inventory Management System which provides powerful low-level stock control and part tracking. The core of the InvenTree system is a Python/Django database backend which provides an admin interface (web-based) and a REST API for interaction with external interfaces and applications. A powerful plugin system provides support for custom applications and extensions.

Check out [our website](https://inventree.org) for more details.

&lt;!-- Roadmap --&gt;
### :compass: Roadmap

Want to see what we are working on? Check out the [roadmap tag](https://github.com/inventree/InvenTree/issues?q=is%3Aopen+is%3Aissue+label%3Aroadmap) and [horizon milestone](https://github.com/inventree/InvenTree/milestone/42).

&lt;!-- Integration --&gt;
### :hammer_and_wrench: Integration

InvenTree is designed to be **extensible**, and provides multiple options for **integration** with external applications or addition of custom plugins:

* [InvenTree API](https://docs.inventree.org/en/latest/api/)
* [Python module](https://docs.inventree.org/en/latest/api/python/)
* [Plugin interface](https://docs.inventree.org/en/latest/plugins/)
* [Third party tools](https://docs.inventree.org/en/latest/plugins/integrate/)

&lt;!-- TechStack --&gt;
### :space_invader: Tech Stack

&lt;details&gt;
  &lt;summary&gt;Server&lt;/summary&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.python.org/&quot;&gt;Python&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.djangoproject.com/&quot;&gt;Django&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.django-rest-framework.org/&quot;&gt;DRF&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://django-q.readthedocs.io/&quot;&gt;Django Q&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://docs.allauth.org/&quot;&gt;Django-Allauth&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Database&lt;/summary&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.postgresql.org/&quot;&gt;PostgreSQL&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.mysql.com/&quot;&gt;MySQL&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.sqlite.org/&quot;&gt;SQLite&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://redis.io/&quot;&gt;Redis&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;Client&lt;/summary&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://react.dev/&quot;&gt;React&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://lingui.dev/&quot;&gt;Lingui&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://reactrouter.com/&quot;&gt;React Router&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://tanstack.com/query/&quot;&gt;TanStack Query&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://github.com/pmndrs/zustand&quot;&gt;Zustand&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://mantine.dev/&quot;&gt;Mantine&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://icflorescu.github.io/mantine-datatable/&quot;&gt;Mantine Data Table&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://codemirror.net/&quot;&gt;CodeMirror&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;DevOps&lt;/summary&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://hub.docker.com/r/inventree/inventree&quot;&gt;Docker&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://crowdin.com/project/inventree&quot;&gt;Crowdin&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://app.codecov.io/gh/inventree/InvenTree&quot;&gt;Codecov&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://sonarcloud.io/project/overview?id=inventree_InvenTree&quot;&gt;SonarCloud&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://packager.io/gh/inventree/InvenTree&quot;&gt;Packager.io&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/details&gt;

&lt;!-- Getting Started --&gt;
## 	:toolbox: Deployment / Getting Started

There are several options to deploy InvenTree.

&lt;div align=&quot;center&quot;&gt;&lt;h4&gt;
    &lt;a href=&quot;https://docs.inventree.org/en/latest/start/docker/&quot;&gt;Docker&lt;/a&gt;
    &lt;span&gt; ¬∑ &lt;/span&gt;
    &lt;a href=&quot;https://inventree.org/digitalocean&quot;&gt;&lt;img src=&quot;https://www.deploytodo.com/do-btn-blue-ghost.svg&quot; alt=&quot;Deploy to DO&quot; width=&quot;auto&quot; height=&quot;40&quot; /&gt;&lt;/a&gt;
    &lt;span&gt; ¬∑ &lt;/span&gt;
    &lt;a href=&quot;https://docs.inventree.org/en/latest/start/install/&quot;&gt;Bare Metal&lt;/a&gt;
&lt;/h4&gt;&lt;/div&gt;

Single line install - read [the docs](https://docs.inventree.org/en/latest/start/installer/) for supported distros and details about the function:
```bash
wget -qO install.sh https://get.inventree.org &amp;&amp; bash install.sh
```

Refer to the [getting started guide](https://docs.inventree.org/en/latest/start/install/) for a full set of installation and setup instructions.

&lt;!-- Mobile App --&gt;
## 	:iphone: Mobile App

InvenTree is supported by a [companion mobile app](https://docs.inventree.org/en/latest/app/) which allows users access to stock control information and functionality.

&lt;div align=&quot;center&quot;&gt;&lt;h4&gt;
    &lt;a href=&quot;https://play.google.com/store/apps/details?id=inventree.inventree_app&quot;&gt;Android Play Store&lt;/a&gt;
     &lt;span&gt; ¬∑ &lt;/span&gt;
    &lt;a href=&quot;https://apps.apple.com/au/app/inventree/id1581731101#?platform=iphone&quot;&gt;Apple App Store&lt;/a&gt;
&lt;/h4&gt;&lt;/div&gt;

&lt;!-- Security --&gt;
## :lock: Code of Conduct &amp; Security Policy

The InvenTree project team is committed to providing a safe and welcoming environment for all users. Please read our [Code of Conduct](CODE_OF_CONDUCT.md) for more information.

InvenTree is following industry best practices for security. Our security policy is included [in this repo](SECURITY.md). We provide dedicated security pages on [our documentation site](https://docs.inventree.org/en/latest/security/).

&lt;!-- Contributing --&gt;
## :wave: Contributing

Contributions are welcomed and encouraged. Please help to make this project even better! Refer to the [contribution page](https://docs.inventree.org/en/latest/develop/contributing/).

&lt;!-- Translation --&gt;
## :scroll: Translation

Native language translation of the InvenTree web application is [community contributed via crowdin](https://crowdin.com/project/inventree). **Contributions are welcomed and encouraged**.

&lt;!-- Sponsor --&gt;
## :money_with_wings: Sponsor

If you use InvenTree and find it to be useful, please consider [sponsoring the project](https://github.com/sponsors/inventree).

&lt;!-- Acknowledgments --&gt;
## :gem: Acknowledgements

We want to acknowledge [PartKeepr](https://github.com/partkeepr/PartKeepr) as a valuable predecessor and inspiration.
Find a full list of used third-party libraries in the license information dialog of your instance.

## :heart: Support

&lt;p&gt;This project is supported by the following sponsors:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/MartinLoeper&quot;&gt;&lt;img src=&quot;https://github.com/MartinLoeper.png&quot; width=&quot;60px&quot; alt=&quot;Martin L√∂per&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/lippoliv&quot;&gt;&lt;img src=&quot;https://github.com/lippoliv.png&quot; width=&quot;60px&quot; alt=&quot;Oliver Lippert&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/lfg-seth&quot;&gt;&lt;img src=&quot;https://github.com/lfg-seth.png&quot; width=&quot;60px&quot; alt=&quot;Seth Smith&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/snorkrat&quot;&gt;&lt;img src=&quot;https://github.com/snorkrat.png&quot; width=&quot;60px&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/spacequest-ltd&quot;&gt;&lt;img src=&quot;https://github.com/spacequest-ltd.png&quot; width=&quot;60px&quot; alt=&quot;SpaceQuest Ltd&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/appwrite&quot;&gt;&lt;img src=&quot;https://github.com/appwrite.png&quot; width=&quot;60px&quot; alt=&quot;Appwrite&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/PricelessToolkit&quot;&gt;&lt;img src=&quot;https://github.com/PricelessToolkit.png&quot; width=&quot;60px&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/cabottech&quot;&gt;&lt;img src=&quot;https://github.com/cabottech.png&quot; width=&quot;60px&quot; alt=&quot;Cabot Technologies&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/markus-k&quot;&gt;&lt;img src=&quot;https://github.com/markus-k.png&quot; width=&quot;60px&quot; alt=&quot;Markus Kasten&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/jefffhaynes&quot;&gt;&lt;img src=&quot;https://github.com/jefffhaynes.png&quot; width=&quot;60px&quot; alt=&quot;Jeff Haynes&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/dnviti&quot;&gt;&lt;img src=&quot;https://github.com/dnviti.png&quot; width=&quot;60px&quot; alt=&quot;Daniele Viti&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/Islendur&quot;&gt;&lt;img src=&quot;https://github.com/Islendur.png&quot; width=&quot;60px&quot; alt=&quot;Islendur&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/Gibeon-NL&quot;&gt;&lt;img src=&quot;https://github.com/Gibeon-NL.png&quot; width=&quot;60px&quot; alt=&quot;Gibeon-NL&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/Motrac-Research-Engineering&quot;&gt;&lt;img src=&quot;https://github.com/Motrac-Research-Engineering.png&quot; width=&quot;60px&quot; alt=&quot;Motrac Research&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/trytuna&quot;&gt;&lt;img src=&quot;https://github.com/trytuna.png&quot; width=&quot;60px&quot; alt=&quot;Timo Scrappe&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/ATLAS2246&quot;&gt;&lt;img src=&quot;https://github.com/ATLAS2246.png&quot; width=&quot;60px&quot; alt=&quot;ATLAS2246&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/Kedarius&quot;&gt;&lt;img src=&quot;https://github.com/Kedarius.png&quot; width=&quot;60px&quot; alt=&quot;Radek Hladik&quot; /&gt;&lt;/a&gt;

&lt;/p&gt;

&lt;p&gt;With ongoing resources provided by:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://depot.dev?utm_source=inventree&quot;&gt;&lt;img src=&quot;https://depot.dev/badges/built-with-depot.svg&quot; alt=&quot;Built with Depot&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://inventree.org/digitalocean&quot;&gt;
    &lt;img src=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg&quot; width=&quot;201px&quot; alt=&quot;Servers by Digital Ocean&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.netlify.com&quot;&gt; &lt;img src=&quot;https://www.netlify.com/v3/img/components/netlify-color-bg.svg&quot; alt=&quot;Deploys by Netlify&quot; /&gt; &lt;/a&gt;
  &lt;a href=&quot;https://crowdin.com&quot;&gt; &lt;img src=&quot;https://crowdin.com/images/crowdin-logo.svg&quot; alt=&quot;Translation by Crowdin&quot; /&gt; &lt;/a&gt; &lt;br&gt;
&lt;/p&gt;


&lt;!-- License --&gt;
## :warning: License

Distributed under the [MIT](https://choosealicense.com/licenses/mit/) License. See [LICENSE.txt](https://github.com/inventree/InvenTree/blob/master/LICENSE) for more information.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[awslabs/mcp]]></title>
            <link>https://github.com/awslabs/mcp</link>
            <guid>https://github.com/awslabs/mcp</guid>
            <pubDate>Mon, 10 Nov 2025 00:04:34 GMT</pubDate>
            <description><![CDATA[AWS MCP Servers ‚Äî helping you get the most out of AWS, wherever you use MCP.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/awslabs/mcp">awslabs/mcp</a></h1>
            <p>AWS MCP Servers ‚Äî helping you get the most out of AWS, wherever you use MCP.</p>
            <p>Language: Python</p>
            <p>Stars: 7,333</p>
            <p>Forks: 1,055</p>
            <p>Stars today: 173 stars today</p>
            <h2>README</h2><pre># AWS MCP Servers

A suite of specialized MCP servers that help you get the most out of AWS, wherever you use MCP.

[![GitHub](https://img.shields.io/badge/github-awslabs/mcp-blue.svg?style=flat&amp;logo=github)](https://github.com/awslabs/mcp)
[![License](https://img.shields.io/badge/license-Apache--2.0-brightgreen)](LICENSE)
[![Codecov](https://img.shields.io/codecov/c/github/awslabs/mcp)](https://app.codecov.io/gh/awslabs/mcp)
[![OSSF-Scorecard Score](https://img.shields.io/ossf-scorecard/github.com/awslabs/mcp)](https://scorecard.dev/viewer/?uri=github.com/awslabs/mcp)

## Table of Contents

- [AWS MCP Servers](#aws-mcp-servers)
  - [Table of Contents](#table-of-contents)
  - [What is the Model Context Protocol (MCP) and how does it work with AWS MCP Servers?](#what-is-the-model-context-protocol-mcp-and-how-does-it-work-with-aws-mcp-servers)
  - [AWS MCP Servers Transport Mechanisms](#aws-mcp-servers-transport-mechanisms)
    - [Supported transport mechanisms](#supported-transport-mechanisms)
    - [Server Sent Events Support Removal](#server-sent-events-support-removal)
  - [Why AWS MCP Servers?](#why-aws-mcp-servers)
  - [Available MCP Servers: Quick Installation](#available-mcp-servers-quick-installation)
    - [üöÄGetting Started with AWS](#-getting-started-with-aws)
    - [Browse by What You&#039;re Building](#browse-by-what-youre-building)
      - [üìö Real-time access to official AWS documentation](#-real-time-access-to-official-aws-documentation)
      - [üèóÔ∏è Infrastructure \&amp; Deployment](#Ô∏è-infrastructure--deployment)
        - [Infrastructure as Code](#infrastructure-as-code)
        - [Container Platforms](#container-platforms)
        - [Serverless \&amp; Functions](#serverless--functions)
        - [Support](#support)
      - [ü§ñ AI \&amp; Machine Learning](#-ai--machine-learning)
      - [üìä Data \&amp; Analytics](#-data--analytics)
        - [SQL \&amp; NoSQL Databases](#sql--nosql-databases)
        - [Search \&amp; Analytics](#search--analytics)
        - [Caching \&amp; Performance](#caching--performance)
      - [üõ†Ô∏è Developer Tools \&amp; Support](#Ô∏è-developer-tools--support)
      - [üì° Integration \&amp; Messaging](#-integration--messaging)
      - [üí∞ Cost \&amp; Operations](#-cost--operations)
      - [üß¨ Healthcare \&amp; Lifesciences](#-healthcare--lifesciences)
    - [Browse by How You&#039;re Working](#browse-by-how-youre-working)
      - [üë®‚Äçüíª Vibe Coding \&amp; Development](#-vibe-coding--development)
        - [Core Development Workflow](#core-development-workflow)
        - [Infrastructure as Code](#infrastructure-as-code-1)
        - [Application Development](#application-development)
        - [Container \&amp; Serverless Development](#container--serverless-development)
        - [Testing \&amp; Data](#testing--data)
        - [Lifesciences Workflow Development](#lifesciences-workflow-development)
      - [üí¨ Conversational Assistants](#-conversational-assistants)
        - [Knowledge \&amp; Search](#knowledge--search)
        - [Content Processing \&amp; Generation](#content-processing--generation)
        - [Business Services](#business-services)
      - [ü§ñ Autonomous Background Agents](#-autonomous-background-agents)
        - [Data Operations \&amp; ETL](#data-operations--etl)
        - [Caching \&amp; Performance](#caching--performance-1)
        - [Workflow \&amp; Integration](#workflow--integration)
        - [Operations \&amp; Monitoring](#operations--monitoring)
  - [MCP AWS Lambda Handler Module](#mcp-aws-lambda-handler-module)
  - [When to use Local vs Remote MCP Servers?](#when-to-use-local-vs-remote-mcp-servers)
    - [Local MCP Servers](#local-mcp-servers)
    - [Remote MCP Servers](#remote-mcp-servers)
  - [Use Cases for the Servers](#use-cases-for-the-servers)
  - [Installation and Setup](#installation-and-setup)
    - [Running MCP servers in containers](#running-mcp-servers-in-containers)
    - [Getting Started with Amazon Q Developer CLI](#getting-started-with-amazon-q-developer-cli)
      - [`~/.aws/amazonq/mcp.json`](#awsamazonqmcpjson)
    - [Getting Started with Kiro](#getting-started-with-kiro)
      - [`kiro_mcp_settings.json`](#kiro_mcp_settingsjson)
    - [Getting Started with Cline and Amazon Bedrock](#getting-started-with-cline-and-amazon-bedrock)
      - [`cline_mcp_settings.json`](#cline_mcp_settingsjson)
    - [Getting Started with Cursor](#getting-started-with-cursor)
      - [`.cursor/mcp.json`](#cursormcpjson)
    - [Getting Started with Windsurf](#getting-started-with-windsurf)
      - [`~/.codeium/windsurf/mcp_config.json`](#codeiumwindsurfmcp_configjson)
    - [Getting Started with VS Code](#getting-started-with-vs-code)
      - [`.vscode/mcp.json`](#vscodemcpjson)
    - [Getting Started with Claude Code](#getting-started-with-claude-code)
      - [`.mcp.json`](#mcpjson)
  - [Samples](#samples)
  - [Vibe coding](#vibe-coding)
  - [Additional Resources](#additional-resources)
  - [Security](#security)
  - [Contributing](#contributing)
  - [Developer guide](#developer-guide)
  - [License](#license)
  - [Disclaimer](#disclaimer)

## What is the Model Context Protocol (MCP) and how does it work with AWS MCP Servers?

&gt; The Model Context Protocol (MCP) is an open protocol that enables seamless integration between LLM applications and external data sources and tools. Whether you&#039;re building an AI-powered IDE, enhancing a chat interface, or creating custom AI workflows, MCP provides a standardized way to connect LLMs with the context they need.
&gt;
&gt; &amp;mdash; [Model Context Protocol README](https://github.com/modelcontextprotocol#:~:text=The%20Model%20Context,context%20they%20need.)

An MCP Server is a lightweight program that exposes specific capabilities through the standardized Model Context Protocol. Host applications (such as chatbots, IDEs, and other AI tools) have MCP clients that maintain 1:1 connections with MCP servers. Common MCP clients include agentic AI coding assistants (like Q Developer, Cline, Cursor, Windsurf) as well as chatbot applications like Claude Desktop, with more clients coming soon. MCP servers can access local data sources and remote services to provide additional context that improves the generated outputs from the models.

AWS MCP Servers use this protocol to provide AI applications access to AWS documentation, contextual guidance, and best practices. Through the standardized MCP client-server architecture, AWS capabilities become an intelligent extension of your development environment or AI application.

AWS MCP servers enable enhanced cloud-native development, infrastructure management, and development workflows‚Äîmaking AI-assisted cloud computing more accessible and efficient.

The Model Context Protocol is an open source project run by Anthropic, PBC. and open to contributions from the entire community. For more information on MCP, you can find further documentation [here](https://modelcontextprotocol.io/introduction)

## AWS MCP Servers Transport Mechanisms

### Supported transport mechanisms

The MCP protocol currently defines two standard transport mechanisms for client-server communication:
- stdio, communication over standard in and standard out
- streamable HTTP

These AWS MCP Servers are designed to support stdio only.

You are responsible for ensuring that your use of these servers comply with the terms governing them, and any laws, rules, regulations, policies, or standards that apply to you.

### Server Sent Events Support Removal

**Important Notice:** On May 26th, 2025, Server Sent Events (SSE) support was removed from all MCP servers in their latest major versions. This change aligns with the Model Context Protocol specification&#039;s [backwards compatibility guidelines](https://modelcontextprotocol.io/specification/2025-03-26/basic/transports#backwards-compatibility).

We are actively working towards supporting [Streamable HTTP](https://modelcontextprotocol.io/specification/draft/basic/transports#streamable-http), which will provide improved transport capabilities for future versions.

For applications still requiring SSE support, please use the previous major version of the respective MCP server until you can migrate to alternative transport methods.

### Why AWS MCP Servers?

MCP servers enhance the capabilities of foundation models (FMs) in several key ways:

- **Improved Output Quality**: By providing relevant information directly in the model&#039;s context, MCP servers significantly improve model responses for specialized domains like AWS services. This approach reduces hallucinations, provides more accurate technical details, enables more precise code generation, and ensures recommendations align with current AWS best practices and service capabilities.

- **Access to Latest Documentation**: FMs may not have knowledge of recent releases, APIs, or SDKs. MCP servers bridge this gap by pulling in up-to-date documentation, ensuring your AI assistant always works with the latest AWS capabilities.

- **Workflow Automation**: MCP servers convert common workflows into tools that foundation models can use directly. Whether it&#039;s CDK, Terraform, or other AWS-specific workflows, these tools enable AI assistants to perform complex tasks with greater accuracy and efficiency.

- **Specialized Domain Knowledge**: MCP servers provide deep, contextual knowledge about AWS services that might not be fully represented in foundation models&#039; training data, enabling more accurate and helpful responses for cloud development tasks.

## Available MCP Servers: Quick Installation

Get started quickly with one-click installation buttons for popular MCP clients. Click the buttons below to install servers directly in Cursor or VS Code:

### üöÄ Getting Started with AWS

For general AWS interactions and comprehensive API support, we recommend starting with:

| Server Name | Description | Install |
|-------------|-------------|---------|
| [AWS API MCP Server](src/aws-api-mcp-server) | Start here for general AWS interactions! Comprehensive AWS API support with command validation, security controls, and access to all AWS services. Perfect for managing infrastructure, exploring resources, and executing AWS operations through natural language. | [![Install](https://img.shields.io/badge/Install-Cursor-blue?style=flat-square&amp;logo=cursor)](https://cursor.com/en/install-mcp?name=awslabs.aws-api-mcp-server&amp;config=eyJjb21tYW5kIjoidXZ4IGF3c2xhYnMuYXdzLWFwaS1tY3Atc2VydmVyQGxhdGVzdCIsImVudiI6eyJBV1NfUkVHSU9OIjoidXMtZWFzdC0xIn0sImRpc2FibGVkIjpmYWxzZSwiYXV0b0FwcHJvdmUiOltdfQ%3D%3D)&lt;br/&gt;[![Install VS Code](https://img.shields.io/badge/Install-VS_Code-FF9900?style=flat-square&amp;logo=visualstudiocode&amp;logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=AWS%20API%20MCP%20Server&amp;config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22awslabs.aws-api-mcp-server%40latest%22%5D%2C%22env%22%3A%7B%22AWS_REGION%22%3A%22us-east-1%22%7D%2C%22type%22%3A%22stdio%22%7D) |
| [AWS Knowledge MCP Server](src/aws-knowledge-mcp-server) | A remote, fully-managed MCP server hosted by AWS that provides access to the latest AWS docs, API references, What&#039;s New Posts, Getting Started information, Builder Center, Blog posts, Architectural references, and Well-Architected guidance. | [![Install](https://img.shields.io/badge/Install-Cursor-blue?style=flat-square&amp;logo=cursor)](https://cursor.com/en/install-mcp?name=aws-knowledge-mcp&amp;config=eyJ1cmwiOiJodHRwczovL2tub3dsZWRnZS1tY3AuZ2xvYmFsLmFwaS5hd3MifQ==) &lt;br/&gt;[![Install on VS Code](https://img.shields.io/badge/Install-VS_Code-FF9900?style=flat-square&amp;logo=visualstudiocode&amp;logoColor=white)](https://vscode.dev/redirect/mcp/install?name=aws-knowledge-mcp&amp;config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Fknowledge-mcp.global.api.aws%22%7D) |

### Browse by What You&#039;re Building

#### üìö Real-time access to official AWS documentation

| Server Name | Description | Install |
|-------------|-------------|---------|
| [AWS Knowledge MCP Server](src/aws-knowledge-mcp-server) | A remote, fully-managed MCP server hosted by AWS that provides access to the latest AWS docs, API references, What&#039;s New Posts, Getting Started information, Builder Center, Blog posts, Architectural references, and Well-Architected guidance. | [![Install](https://img.shields.io/badge/Install-Cursor-blue?style=flat-square&amp;logo=cursor)](https://cursor.com/en/install-mcp?name=aws-knowledge-mcp&amp;config=eyJ1cmwiOiJodHRwczovL2tub3dsZWRnZS1tY3AuZ2xvYmFsLmFwaS5hd3MifQ==) &lt;br/&gt;[![Install on VS Code](https://img.shields.io/badge/Install-VS_Code-FF9900?style=flat-square&amp;logo=visualstudiocode&amp;logoColor=white)](https://vscode.dev/redirect/mcp/install?name=aws-knowledge-mcp&amp;config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Fknowledge-mcp.global.api.aws%22%7D) |
| [AWS Documentation MCP Server](src/aws-documentation-mcp-server) | Get latest AWS docs and API references | [![Install](https://img.shields.io/badge/Install-Cursor-blue?style=flat-square&amp;logo=cursor)](https://cursor.com/en/install-mcp?name=awslabs.aws-documentation-mcp-server&amp;config=eyJjb21tYW5kIjoidXZ4IGF3c2xhYnMuYXdzLWRvY3VtZW50YXRpb24tbWNwLXNlcnZlckBsYXRlc3QiLCJlbnYiOnsiRkFTVE1DUF9MT0dfTEVWRUwiOiJFUlJPUiIsIkFXU19ET0NVTUVOVEFUSU9OX1BBUlRJVElPTiI6ImF3cyJ9LCJkaXNhYmxlZCI6ZmFsc2UsImF1dG9BcHByb3ZlIjpbXX0%3D) &lt;br/&gt;[![Install on VS Code](https://img.shields.io/badge/Install-VS_Code-FF9900?style=flat-square&amp;logo=visualstudiocode&amp;logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=AWS%20Documentation%20MCP%20Server&amp;config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22awslabs.aws-documentation-mcp-server%40latest%22%5D%2C%22env%22%3A%7B%22FASTMCP_LOG_LEVEL%22%3A%22ERROR%22%2C%22AWS_DOCUMENTATION_PARTITION%22%3A%22aws%22%7D%2C%22disabled%22%3Afalse%2C%22autoApprove%22%3A%5B%5D%7D) |


### üèóÔ∏è Infrastructure &amp; Deployment

Build, deploy, and manage cloud infrastructure with Infrastructure as Code best practices.

| Server Name | Description | Install |
|-------------|-------------|---------|
| [AWS Cloud Control API MCP Server](src/ccapi-mcp-server) | Direct AWS resource management with security scanning and best practices | [![Install](https://img.shields.io/badge/Install-Cursor-blue?style=flat-square&amp;logo=cursor)](https://cursor.com/en/install-mcp?name=awslabs.ccapi-mcp-server&amp;config=eyJjb21tYW5kIjoidXZ4IGF3c2xhYnMuY2NhcGktbWNwLXNlcnZlckBsYXRlc3QiLCJlbnYiOnsiQVdTX1BST0ZJTEUiOiJ5b3VyLWF3cy1wcm9maWxlIiwiQVdTX1JFR0lPTiI6InVzLWVhc3QtMSIsIkZBU1RNQ1BfTE9HX0xFVkVMIjoiRVJST1IifSwiZGlzYWJsZWQiOmZhbHNlLCJhdXRvQXBwcm92ZSI6W119) &lt;br/&gt;[![Install on VS Code](https://img.shields.io/badge/Install-VS_Code-FF9900?style=flat-square&amp;logo=visualstudiocode&amp;logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=AWS%20Cloud%20Control%20API%20MCP%20Server&amp;config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22awslabs.ccapi-mcp-server%40latest%22%5D%2C%22env%22%3A%7B%22AWS_PROFILE%22%3A%22your-aws-profile%22%2C%22AWS_REGION%22%3A%22us-east-1%22%2C%22FASTMCP_LOG_LEVEL%22%3A%22ERROR%22%7D%2C%22disabled%22%3Afalse%2C%22autoApprove%22%3A%5B%5D%7D) |
| [AWS CDK MCP Server](src/cdk-mcp-server) | AWS CDK development with security compliance and best practices | [![Install](https://img.shields.io/badge/Install-Cursor-blue?style=flat-square&amp;logo=cursor)](https://cursor.com/en/install-mcp?name=awslabs.cdk-mcp-server&amp;config=eyJjb21tYW5kIjoidXZ4IGF3c2xhYnMuY2RrLW1jcC1zZXJ2ZXJAbGF0ZXN0IiwiZW52Ijp7IkZBU1RNQ1BfTE9HX0xFVkVMIjoiRVJST1IifSwiZGlzYWJsZWQiOmZhbHNlLCJhdXRvQXBwcm92ZSI6W119) &lt;br/&gt;[![Install on VS Code](https://img.shields.io/badge/Install-VS_Code-FF9900?style=flat-square&amp;logo=visualstudiocode&amp;logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=CDK%20MCP%20Server&amp;config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22awslabs.cdk-mcp-server%40latest%22%5D%2C%22env%22%3A%7B%22FASTMCP_LOG_LEVEL%22%3A%22ERROR%22%7D%2C%22disabled%22%3Afalse%2C%22autoApprove%22%3A%5B%5D%7D) |
| [AWS Terraform MCP Server](src/terraform-mcp-server) | Terraform workflows with integrated security scanning | [![Install](https://img.shields.io/badge/Install-Cursor-blue?style=flat-square&amp;logo=cursor)](https://cursor.com/en/install-mcp?name=awslabs.terraform-mcp-server&amp;config=eyJjb21tYW5kIjoidXZ4IGF3c2xhYnMudGVycmFmb3JtLW1jcC1zZXJ2ZXJAbGF0ZXN0IiwiZW52Ijp7IkZBU1RNQ1BfTE9HX0xFVkVMIjoiRVJST1IifSwiZGlzYWJsZWQiOmZhbHNlLCJhdXRvQXBwcm92ZSI6W119) &lt;br/&gt;[![Install on VS Code](https://img.shields.io/badge/Install-VS_Code-FF9900?style=flat-square&amp;logo=visualstudiocode&amp;logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=Terraform%20MCP%20Server&amp;config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22awslabs.terraform-mcp-server%40latest%22%5D%2C%22env%22%3A%7B%22FASTMCP_LOG_LEVEL%22%3A%22ERROR%22%7D%2C%22disabled%22%3Afalse%2C%22autoApprove%22%3A%5B%5D%7D) |
| [AWS CloudFormation MCP Server](src/cfn-mcp-server) | Direct CloudFormation resource management via Cloud Control API | [![Install](https://img.shields.io/badge/Install-Cursor-blue?style=flat-square&amp;logo=cursor)](https://cursor.com/en/install-mcp?name=awslabs.cfn-mcp-server&amp;config=eyJjb21tYW5kIjoidXZ4IGF3c2xhYnMuY2ZuLW1jcC1zZXJ2ZXJAbGF0ZXN0IiwiZW52Ijp7IkFXU19QUk9GSUxFIjoieW91ci1uYW1lZC1wcm9maWxlIn0sImRpc2FibGVkIjpmYWxzZSwiYXV0b0FwcHJvdmUiOltdfQ%3D%3D) &lt;br/&gt;[![Install on VS Code](https://img.shields.io/badge/Install-VS_Code-FF9900?style=flat-square&amp;logo=visualstudiocode&amp;logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=CloudFormation%20MCP%20Server&amp;config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22awslabs.cfn-mcp-server%40latest%22%5D%2C%22env%22%3A%7B%22AWS_PROFILE%22%3A%22your-named-profile%22%7D%2C%22disabled%22%3Afalse%2C%22autoApprove%22%3A%5B%5D%7D) |

#### Container Platforms

| Server Name | Description | Install |
|-------------|-------------|---------|
| [Amazon EKS MCP Server](src/eks-mcp-server) | Kubernetes cluster management and application deployment | [![Install](https://img.shields.io/badge/Install-Cursor-blue?style=flat-square&amp;logo=cursor)](https://cursor.com/en/install-mcp?name=awslabs.eks-mcp-server&amp;config=eyJhdXRvQXBwcm92ZSI6W10sImRpc2FibGVkIjpmYWxzZSwiY29tbWFuZCI6InV2eCBhd3NsYWJzLmVrcy1tY3Atc2VydmVyQGxhdGVzdCAtLWFsbG93LXdyaXRlIC0tYWxsb3ctc2Vuc2l0aXZlLWRhdGEtYWNjZXNzIiwiZW52Ijp7IkZBU1RNQ1BfTE9HX0xFVkVMIjoiRVJST1IifSwidHJhbnNwb3J0VHlwZSI6InN0ZGlvIn0%3D) &lt;br/&gt;[![Install on VS Code](https://img.shields.io/badge/Install-VS_Code-FF9900?style=flat-square&amp;logo=visualstudiocode&amp;logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=EKS%20MCP%20Server&amp;config=%7B%22autoApprove%22%3A%5B%5D%2C%22disabled%22%3Afalse%2C%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22awslabs.eks-mcp-server%40latest%22%2C%22--allow-write%22%2C%22--allow-sensitive-data-access%22%5D%2C%22env%22%3A%7B%22FASTMCP_LOG_LEVEL%22%3A%22ERROR%22%7D%2C%22transportType%22%3A%22stdio%22%7D) |
| [Amazon ECS MCP Server](src/ecs-mcp-server) | Container orchestration and ECS application deployment | [![Install](https://img.shields.io/badge/Install-Cursor-blue?style=flat-square&amp;logo=cursor)](https://cursor.com/en/install-mcp?name=awslabs.ecs-mcp-server&amp;config=eyJjb21tYW5kIjoidXZ4IC0tZnJvbSBhd3NsYWJzLWVjcy1tY3Atc2VydmVyIGVjcy1tY3Atc2VydmVyIiwiZW52Ijp7IkFXU19QUk9GSUxFIjoieW91ci1hd3MtcHJvZmlsZSIsIkFXU19SRUdJT04iOiJ5b3VyLWF3cy1yZWdpb24iLCJGQVNUTUNQX0xPR19MRVZFTCI6IkVSUk9SIiwiRkFTVE1DUF9MT0dfRklMRSI6Ii9wYXRoL3RvL2Vjcy1tY3Atc2VydmVyLmxvZyIsIkFMTE9XX1dSSVRFIjoiZmFsc2UiLCJBTExPV19TRU5TSVRJVkVfREFUQSI6ImZhbHNlIn19) &lt;br/&gt;[![Install on VS Code](https://img.shields.io/badge/Install-VS_Code-FF9900?style=flat-square&amp;logo=visualstudiocode&amp;logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=ECS%20MCP%20Server&amp;config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22--from%22%2C%22awslabs-ecs-mcp-server%22%2C%22ecs-mcp-server%22%5D%2C%22env%22%3A%7B%22AWS_PROFILE%22%3A%22your-aws-profile%22%2C%22AWS_REGION%22%3A%22your-aws-region%22%2C%22FASTMCP_LOG_LEVEL%22%3A%22ERROR%22%2C%22FASTMCP_LOG_FILE%22%3A%22%2Fpath%2Fto%2Fecs-mcp-server.log%22%2C%22ALLOW_WRITE%22%3A%22false%22%2C%22ALLOW_SENSITIVE_DAT

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[yichuan-w/LEANN]]></title>
            <link>https://github.com/yichuan-w/LEANN</link>
            <guid>https://github.com/yichuan-w/LEANN</guid>
            <pubDate>Mon, 10 Nov 2025 00:04:33 GMT</pubDate>
            <description><![CDATA[RAG on Everything with LEANN. Enjoy 97% storage savings while running a fast, accurate, and 100% private RAG application on your personal device.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/yichuan-w/LEANN">yichuan-w/LEANN</a></h1>
            <p>RAG on Everything with LEANN. Enjoy 97% storage savings while running a fast, accurate, and 100% private RAG application on your personal device.</p>
            <p>Language: Python</p>
            <p>Stars: 3,782</p>
            <p>Forks: 385</p>
            <p>Stars today: 175 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/logo-text.png&quot; alt=&quot;LEANN Logo&quot; width=&quot;400&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Python-3.9%20%7C%203.10%20%7C%203.11%20%7C%203.12%20%7C%203.13-blue.svg&quot; alt=&quot;Python Versions&quot;&gt;
  &lt;img src=&quot;https://github.com/yichuan-w/LEANN/actions/workflows/build-and-publish.yml/badge.svg&quot; alt=&quot;CI Status&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Platform-Ubuntu%20%26%20Arch%20%26%20WSL%20%7C%20macOS%20(ARM64%2FIntel)-lightgrey&quot; alt=&quot;Platform&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/License-MIT-green.svg&quot; alt=&quot;MIT License&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/MCP-Native%20Integration-blue&quot; alt=&quot;MCP Integration&quot;&gt;
  &lt;a href=&quot;https://join.slack.com/t/leann-e2u9779/shared_invite/zt-3ckd2f6w1-OX08~NN4gkWhh10PRVBj1Q&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Slack-Join-4A154B?logo=slack&amp;logoColor=white&quot; alt=&quot;Join Slack&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;assets/wechat_user_group.JPG&quot; title=&quot;Join WeChat group&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/WeChat-Join-2DC100?logo=wechat&amp;logoColor=white&quot; alt=&quot;Join WeChat group&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;h2 align=&quot;center&quot; tabindex=&quot;-1&quot; class=&quot;heading-element&quot; dir=&quot;auto&quot;&gt;
    The smallest vector index in the world. RAG Everything with LEANN!
&lt;/h2&gt;

LEANN is an innovative vector database that democratizes personal AI. Transform your laptop into a powerful RAG system that can index and search through millions of documents while using **97% less storage** than traditional solutions **without accuracy loss**.

LEANN achieves this through *graph-based selective recomputation* with *high-degree preserving pruning*, computing embeddings on-demand instead of storing them all. [Illustration Fig ‚Üí](#Ô∏è-architecture--how-it-works) | [Paper ‚Üí](https://arxiv.org/abs/2506.08276)

**Ready to RAG Everything?** Transform your laptop into a personal AI assistant that can semantic search your **[file system](#-personal-data-manager-process-any-documents-pdf-txt-md)**, **[emails](#-your-personal-email-secretary-rag-on-apple-mail)**, **[browser history](#-time-machine-for-the-web-rag-your-entire-browser-history)**, **[chat history](#-wechat-detective-unlock-your-golden-memories)** ([WeChat](#-wechat-detective-unlock-your-golden-memories), [iMessage](#-imessage-history-your-personal-conversation-archive)), **[agent memory](#-chatgpt-chat-history-your-personal-ai-conversation-archive)** ([ChatGPT](#-chatgpt-chat-history-your-personal-ai-conversation-archive), [Claude](#-claude-chat-history-your-personal-ai-conversation-archive)), **[live data](#mcp-integration-rag-on-live-data-from-any-platform)** ([Slack](#mcp-integration-rag-on-live-data-from-any-platform), [Twitter](#mcp-integration-rag-on-live-data-from-any-platform)), **[codebase](#-claude-code-integration-transform-your-development-workflow)**\* , or external knowledge bases (i.e., 60M documents) - all on your laptop, with zero cloud costs and complete privacy.


\* Claude Code only supports basic `grep`-style keyword search. **LEANN** is a drop-in **semantic search MCP service fully compatible with Claude Code**, unlocking intelligent retrieval without changing your workflow. üî• Check out [the easy setup ‚Üí](packages/leann-mcp/README.md)



## Why LEANN?

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/effects.png&quot; alt=&quot;LEANN vs Traditional Vector DB Storage Comparison&quot; width=&quot;70%&quot;&gt;
&lt;/p&gt;

&gt; **The numbers speak for themselves:** Index 60 million text chunks in just 6GB instead of 201GB. From emails to browser history, everything fits on your laptop. [See detailed benchmarks for different applications below ‚Üì](#-storage-comparison)


üîí **Privacy:** Your data never leaves your laptop. No OpenAI, no cloud, no &quot;terms of service&quot;.

ü™∂ **Lightweight:** Graph-based recomputation eliminates heavy embedding storage, while smart graph pruning and CSR format minimize graph storage overhead. Always less storage, less memory usage!

üì¶ **Portable:** Transfer your entire knowledge base between devices (even with others) with minimal cost - your personal AI memory travels with you.

üìà **Scalability:** Handle messy personal data that would crash traditional vector DBs, easily managing your growing personalized data and agent generated memory!

‚ú® **No Accuracy Loss:** Maintain the same search quality as heavyweight solutions while using 97% less storage.

## Installation

### üì¶ Prerequisites: Install uv

[Install uv](https://docs.astral.sh/uv/getting-started/installation/#installation-methods) first if you don&#039;t have it. Typically, you can install it with:

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### üöÄ Quick Install

Clone the repository to access all examples and try amazing applications,

```bash
git clone https://github.com/yichuan-w/LEANN.git leann
cd leann
```

and install LEANN from [PyPI](https://pypi.org/project/leann/) to run them immediately:

```bash
uv venv
source .venv/bin/activate
uv pip install leann
```

&lt;!--
&gt; Low-resource? See &quot;Low-resource setups&quot; in the [Configuration Guide](docs/configuration-guide.md#low-resource-setups). --&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;strong&gt;üîß Build from Source (Recommended for development)&lt;/strong&gt;
&lt;/summary&gt;



```bash
git clone https://github.com/yichuan-w/LEANN.git leann
cd leann
git submodule update --init --recursive
```

**macOS:**

Note: DiskANN requires MacOS 13.3 or later.

```bash
brew install libomp boost protobuf zeromq pkgconf
uv sync --extra diskann
```

**Linux (Ubuntu/Debian):**

Note: On Ubuntu 20.04, you may need to build a newer Abseil and pin Protobuf (e.g., v3.20.x) for building DiskANN. See [Issue #30](https://github.com/yichuan-w/LEANN/issues/30) for a step-by-step note.

You can manually install [Intel oneAPI MKL](https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html) instead of `libmkl-full-dev` for DiskANN. You can also use `libopenblas-dev` for building HNSW only, by removing `--extra diskann` in the command below.

```bash
sudo apt-get update &amp;&amp; sudo apt-get install -y \
  libomp-dev libboost-all-dev protobuf-compiler libzmq3-dev \
  pkg-config libabsl-dev libaio-dev libprotobuf-dev \
  libmkl-full-dev

uv sync --extra diskann
```

**Linux (Arch Linux):**

```bash
sudo pacman -Syu &amp;&amp; sudo pacman -S --needed base-devel cmake pkgconf git gcc \
  boost boost-libs protobuf abseil-cpp libaio zeromq

# For MKL in DiskANN
sudo pacman -S --needed base-devel git
git clone https://aur.archlinux.org/paru-bin.git
cd paru-bin &amp;&amp; makepkg -si
paru -S intel-oneapi-mkl intel-oneapi-compiler
source /opt/intel/oneapi/setvars.sh

uv sync --extra diskann
```

**Linux (RHEL / CentOS Stream / Oracle / Rocky / AlmaLinux):**

See [Issue #50](https://github.com/yichuan-w/LEANN/issues/50) for more details.

```bash
sudo dnf groupinstall -y &quot;Development Tools&quot;
sudo dnf install -y libomp-devel boost-devel protobuf-compiler protobuf-devel \
  abseil-cpp-devel libaio-devel zeromq-devel pkgconf-pkg-config

# For MKL in DiskANN
sudo dnf install -y intel-oneapi-mkl intel-oneapi-mkl-devel \
  intel-oneapi-openmp || sudo dnf install -y intel-oneapi-compiler
source /opt/intel/oneapi/setvars.sh

uv sync --extra diskann
```

&lt;/details&gt;


## Quick Start

Our declarative API makes RAG as easy as writing a config file.

Check out [demo.ipynb](demo.ipynb) or [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yichuan-w/LEANN/blob/main/demo.ipynb)

```python
from leann import LeannBuilder, LeannSearcher, LeannChat
from pathlib import Path
INDEX_PATH = str(Path(&quot;./&quot;).resolve() / &quot;demo.leann&quot;)

# Build an index
builder = LeannBuilder(backend_name=&quot;hnsw&quot;)
builder.add_text(&quot;LEANN saves 97% storage compared to traditional vector databases.&quot;)
builder.add_text(&quot;Tung Tung Tung Sahur called‚Äîthey need their banana‚Äëcrocodile hybrid back&quot;)
builder.build_index(INDEX_PATH)

# Search
searcher = LeannSearcher(INDEX_PATH)
results = searcher.search(&quot;fantastical AI-generated creatures&quot;, top_k=1)

# Chat with your data
chat = LeannChat(INDEX_PATH, llm_config={&quot;type&quot;: &quot;hf&quot;, &quot;model&quot;: &quot;Qwen/Qwen3-0.6B&quot;})
response = chat.ask(&quot;How much storage does LEANN save?&quot;, top_k=1)
```

## RAG on Everything!

LEANN supports RAG on various data sources including documents (`.pdf`, `.txt`, `.md`), Apple Mail, Google Search History, WeChat, ChatGPT conversations, Claude conversations, iMessage conversations, and **live data from any platform through MCP (Model Context Protocol) servers** - including Slack, Twitter, and more.



### Generation Model Setup

#### LLM Backend

LEANN supports many LLM providers for text generation (HuggingFace, Ollama, and Any OpenAI compatible API).


&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üîë OpenAI API Setup (Default)&lt;/strong&gt;&lt;/summary&gt;

Set your OpenAI API key as an environment variable:

```bash
export OPENAI_API_KEY=&quot;your-api-key-here&quot;
```

Make sure to use `--llm openai` flag when using the CLI.
You can also specify the model name with `--llm-model &lt;model-name&gt;` flag.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üõ†Ô∏è Supported LLM &amp; Embedding Providers (via OpenAI Compatibility)&lt;/strong&gt;&lt;/summary&gt;

Thanks to the widespread adoption of the OpenAI API format, LEANN is compatible out-of-the-box with a vast array of LLM and embedding providers. Simply set the `OPENAI_BASE_URL` and `OPENAI_API_KEY` environment variables to connect to your preferred service.

```sh
export OPENAI_API_KEY=&quot;xxx&quot;
export OPENAI_BASE_URL=&quot;http://localhost:1234/v1&quot; # base url of the provider
```

To use OpenAI compatible endpoint with the CLI interface:

If you are using it for text generation, make sure to use `--llm openai` flag and specify the model name with `--llm-model &lt;model-name&gt;` flag.

If you are using it for embedding, set the `--embedding-mode openai` flag and specify the model name with `--embedding-model &lt;MODEL&gt;`.

-----


Below is a list of base URLs for common providers to get you started.


### üñ•Ô∏è Local Inference Engines (Recommended for full privacy)

| Provider         | Sample Base URL             |
| ---------------- | --------------------------- |
| **Ollama** | `http://localhost:11434/v1` |
| **LM Studio** | `http://localhost:1234/v1`  |
| **vLLM** | `http://localhost:8000/v1`  |
| **llama.cpp** | `http://localhost:8080/v1`  |
| **SGLang** | `http://localhost:30000/v1` |
| **LiteLLM** | `http://localhost:4000`     |

-----

### ‚òÅÔ∏è Cloud Providers

&gt; **üö® A Note on Privacy:** Before choosing a cloud provider, carefully review their privacy and data retention policies. Depending on their terms, your data may be used for their own purposes, including but not limited to human reviews and model training, which can lead to serious consequences if not handled properly.


| Provider         | Base URL                                                   |
| ---------------- | ---------------------------------------------------------- |
| **OpenAI** | `https://api.openai.com/v1`                                |
| **OpenRouter** | `https://openrouter.ai/api/v1`                             |
| **Gemini** | `https://generativelanguage.googleapis.com/v1beta/openai/` |
| **x.AI (Grok)** | `https://api.x.ai/v1`                                      |
| **Groq AI** | `https://api.groq.com/openai/v1`                           |
| **DeepSeek** | `https://api.deepseek.com/v1`                              |
| **SiliconFlow** | `https://api.siliconflow.cn/v1`                            |
| **Zhipu (BigModel)** | `https://open.bigmodel.cn/api/paas/v4/`                |
| **Mistral AI** | `https://api.mistral.ai/v1`                                |




If your provider isn&#039;t on this list, don&#039;t worry! Check their documentation for an OpenAI-compatible endpoint‚Äîchances are, it&#039;s OpenAI Compatible too!

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üîß Ollama Setup (Recommended for full privacy)&lt;/strong&gt;&lt;/summary&gt;

**macOS:**

First, [download Ollama for macOS](https://ollama.com/download/mac).

```bash
# Pull a lightweight model (recommended for consumer hardware)
ollama pull llama3.2:1b
```

**Linux:**

```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama service manually
ollama serve &amp;

# Pull a lightweight model (recommended for consumer hardware)
ollama pull llama3.2:1b
```

&lt;/details&gt;


## ‚≠ê Flexible Configuration

LEANN provides flexible parameters for embedding models, search strategies, and data processing to fit your specific needs.

üìö **Need configuration best practices?** Check our [Configuration Guide](docs/configuration-guide.md) for detailed optimization tips, model selection advice, and solutions to common issues like slow embeddings or poor search quality.

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üìã Click to expand: Common Parameters (Available in All Examples)&lt;/strong&gt;&lt;/summary&gt;

All RAG examples share these common parameters. **Interactive mode** is available in all examples - simply run without `--query` to start a continuous Q&amp;A session where you can ask multiple questions. Type &#039;quit&#039; to exit.

```bash
# Core Parameters (General preprocessing for all examples)
--index-dir DIR              # Directory to store the index (default: current directory)
--query &quot;YOUR QUESTION&quot;      # Single query mode. Omit for interactive chat (type &#039;quit&#039; to exit), and now you can play with your index interactively
--max-items N                # Limit data preprocessing (default: -1, process all data)
--force-rebuild              # Force rebuild index even if it exists

# Embedding Parameters
--embedding-model MODEL      # e.g., facebook/contriever, text-embedding-3-small, mlx-community/Qwen3-Embedding-0.6B-8bit or nomic-embed-text
--embedding-mode MODE        # sentence-transformers, openai, mlx, or ollama

# LLM Parameters (Text generation models)
--llm TYPE                   # LLM backend: openai, ollama, or hf (default: openai)
--llm-model MODEL            # Model name (default: gpt-4o) e.g., gpt-4o-mini, llama3.2:1b, Qwen/Qwen2.5-1.5B-Instruct
--thinking-budget LEVEL      # Thinking budget for reasoning models: low/medium/high (supported by o3, o3-mini, GPT-Oss:20b, and other reasoning models)

# Search Parameters
--top-k N                    # Number of results to retrieve (default: 20)
--search-complexity N        # Search complexity for graph traversal (default: 32)

# Chunking Parameters
--chunk-size N               # Size of text chunks (default varies by source: 256 for most, 192 for WeChat)
--chunk-overlap N            # Overlap between chunks (default varies: 25-128 depending on source)

# Index Building Parameters
--backend-name NAME          # Backend to use: hnsw or diskann (default: hnsw)
--graph-degree N             # Graph degree for index construction (default: 32)
--build-complexity N         # Build complexity for index construction (default: 64)
--compact / --no-compact     # Use compact storage (default: true). Must be `no-compact` for `no-recompute` build.
--recompute / --no-recompute # Enable/disable embedding recomputation (default: enabled). Should not do a `no-recompute` search in a `recompute` build.
```

&lt;/details&gt;

### üìÑ Personal Data Manager: Process Any Documents (`.pdf`, `.txt`, `.md`)!

Ask questions directly about your personal PDFs, documents, and any directory containing your files!

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;videos/paper_clear.gif&quot; alt=&quot;LEANN Document Search Demo&quot; width=&quot;600&quot;&gt;
&lt;/p&gt;

The example below asks a question about summarizing our paper (uses default data in `data/`, which is a directory with diverse data sources: two papers, Pride and Prejudice, and a Technical report about LLM in Huawei in Chinese), and this is the **easiest example** to run here:

```bash
source .venv/bin/activate # Don&#039;t forget to activate the virtual environment
python -m apps.document_rag --query &quot;What are the main techniques LEANN explores?&quot;
```

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üìã Click to expand: Document-Specific Arguments&lt;/strong&gt;&lt;/summary&gt;

#### Parameters
```bash
--data-dir DIR           # Directory containing documents to process (default: data)
--file-types .ext .ext   # Filter by specific file types (optional - all LlamaIndex supported types if omitted)
```

#### Example Commands
```bash
# Process all documents with larger chunks for academic papers
python -m apps.document_rag --data-dir &quot;~/Documents/Papers&quot; --chunk-size 1024

# Filter only markdown and Python files with smaller chunks
python -m apps.document_rag --data-dir &quot;./docs&quot; --chunk-size 256 --file-types .md .py

# Enable AST-aware chunking for code files
python -m apps.document_rag --enable-code-chunking --data-dir &quot;./my_project&quot;

# Or use the specialized code RAG for better code understanding
python -m apps.code_rag --repo-dir &quot;./my_codebase&quot; --query &quot;How does authentication work?&quot;
```

&lt;/details&gt;

### üìß Your Personal Email Secretary: RAG on Apple Mail!

&gt; **Note:** The examples below currently support macOS only. Windows support coming soon.


&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;videos/mail_clear.gif&quot; alt=&quot;LEANN Email Search Demo&quot; width=&quot;600&quot;&gt;
&lt;/p&gt;

Before running the example below, you need to grant full disk access to your terminal/VS Code in System Preferences ‚Üí Privacy &amp; Security ‚Üí Full Disk Access.

```bash
python -m apps.email_rag --query &quot;What&#039;s the food I ordered by DoorDash or Uber Eats mostly?&quot;
```
**780K email chunks ‚Üí 78MB storage.** Finally, search your email like you search Google.

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üìã Click to expand: Email-Specific Arguments&lt;/strong&gt;&lt;/summary&gt;

#### Parameters
```bash
--mail-path PATH         # Path to specific mail directory (auto-detects if omitted)
--include-html          # Include HTML content in processing (useful for newsletters)
```

#### Example Commands
```bash
# Search work emails from a specific account
python -m apps.email_rag --mail-path &quot;~/Library/Mail/V10/WORK_ACCOUNT&quot;

# Find all receipts and order confirmations (includes HTML)
python -m apps.email_rag --query &quot;receipt order confirmation invoice&quot; --include-html
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üìã Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt;

Once the index is built, you can ask questions like:
- &quot;Find emails from my boss about deadlines&quot;
- &quot;What did John say about the project timeline?&quot;
- &quot;Show me emails about travel expenses&quot;
&lt;/details&gt;

### üîç Time Machine for the Web: RAG Your Entire Chrome Browser History!

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;videos/google_clear.gif&quot; alt=&quot;LEANN Browser History Search Demo&quot; width=&quot;600&quot;&gt;
&lt;/p&gt;

```bash
python -m apps.browser_rag --query &quot;Tell me my browser history about machine learning?&quot;
```
**38K browser entries ‚Üí 6MB storage.** Your browser history becomes your personal search engine.

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üìã Click to expand: Browser-Specific Arguments&lt;/strong&gt;&lt;/summary&gt;

#### Parameters
```bash
--chrome-profile PATH    # Path to Chrome profile directory (auto-detects if omitted)
```

#### Example Commands
```bash
# Search academic research from your browsing history
python -m apps.browser_rag --query &quot;arxiv papers machine learning transformer architecture&quot;

# Track competitor analysis across work profile
python -m apps.browser_rag --chrome-profile &quot;~/Library/Application Support/Google/Chrome/Work Profile&quot; --max-items 5000
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üìã Click to expand: How to find your Chrome profile&lt;/strong&gt;&lt;/summary&gt;

The default Chrome profile path is configured for a typical macOS setup. If you need to find your specific Chrome profile:

1. Open Terminal
2. Run: `ls ~/Library/Application\ Support/Google/Chrome/`
3. Look for folders like &quot;Default&quot;, &quot;Profile 1&quot;, &quot;Profile 2&quot;, etc.
4. Use the full path as your `--chrome-profile` argument

**Common Chrome profile locations:**
- macOS: `~/Library/Application Support/Google/Chrome/Default`
- Linux: `~/.config/google-chrome/Default`

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üí¨ Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt;

Once the index is built, you can ask questions like:

- &quot;What websites did I visit about machine learning?&quot;
- &quot;Find my search history about programming&quot;
- &quot;What YouTube videos did I watch re

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hanxi/xiaomusic]]></title>
            <link>https://github.com/hanxi/xiaomusic</link>
            <guid>https://github.com/hanxi/xiaomusic</guid>
            <pubDate>Mon, 10 Nov 2025 00:04:32 GMT</pubDate>
            <description><![CDATA[‰ΩøÁî®Â∞èÁà±Èü≥ÁÆ±Êí≠ÊîæÈü≥‰πêÔºåÈü≥‰πê‰ΩøÁî® yt-dlp ‰∏ãËΩΩ„ÄÇ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hanxi/xiaomusic">hanxi/xiaomusic</a></h1>
            <p>‰ΩøÁî®Â∞èÁà±Èü≥ÁÆ±Êí≠ÊîæÈü≥‰πêÔºåÈü≥‰πê‰ΩøÁî® yt-dlp ‰∏ãËΩΩ„ÄÇ</p>
            <p>Language: Python</p>
            <p>Stars: 7,499</p>
            <p>Forks: 767</p>
            <p>Stars today: 148 stars today</p>
            <h2>README</h2><pre># XiaoMusic: Êó†ÈôêÂê¨Ê≠åÔºåËß£ÊîæÂ∞èÁà±Èü≥ÁÆ±

[![GitHub License](https://img.shields.io/github/license/hanxi/xiaomusic)](https://github.com/hanxi/xiaomusic)
[![Docker Image Version](https://img.shields.io/docker/v/hanxi/xiaomusic?sort=semver&amp;label=docker%20image)](https://hub.docker.com/r/hanxi/xiaomusic)
[![Docker Pulls](https://img.shields.io/docker/pulls/hanxi/xiaomusic)](https://hub.docker.com/r/hanxi/xiaomusic)
[![PyPI - Version](https://img.shields.io/pypi/v/xiaomusic)](https://pypi.org/project/xiaomusic/)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/xiaomusic)](https://pypi.org/project/xiaomusic/)
[![Python Version from PEP 621 TOML](https://img.shields.io/python/required-version-toml?tomlFilePath=https%3A%2F%2Fraw.githubusercontent.com%2Fhanxi%2Fxiaomusic%2Fmain%2Fpyproject.toml)](https://pypi.org/project/xiaomusic/)
[![GitHub Release](https://img.shields.io/github/v/release/hanxi/xiaomusic)](https://github.com/hanxi/xiaomusic/releases)
[![Visitors](https://api.visitorbadge.io/api/daily?path=hanxi%2Fxiaomusic&amp;label=daily%20visitor&amp;countColor=%232ccce4&amp;style=flat)](https://visitorbadge.io/status?path=hanxi%2Fxiaomusic)
[![Visitors](https://api.visitorbadge.io/api/visitors?path=hanxi%2Fxiaomusic&amp;label=total%20visitor&amp;countColor=%232ccce4&amp;style=flat)](https://visitorbadge.io/status?path=hanxi%2Fxiaomusic)

‰ΩøÁî®Â∞èÁà±Èü≥ÁÆ±Êí≠ÊîæÈü≥‰πêÔºåÈü≥‰πê‰ΩøÁî® yt-dlp ‰∏ãËΩΩ„ÄÇ

&lt;https://github.com/hanxi/xiaomusic&gt;

ÊñáÊ°£: &lt;https://xdocs.hanxi.cc/&gt;

&gt; [!TIP]
&gt; ÂàùÊ¨°ÂÆâË£ÖÈÅáÂà∞ÈóÆÈ¢òËØ∑Êü•ÈòÖ [üí¨ FAQÈóÆÈ¢òÈõÜÂêà](https://github.com/hanxi/xiaomusic/issues/99) Ôºå‰∏ÄËà¨ÈÅáÂà∞ÁöÑÈóÆÈ¢òÈÉΩÂ∑≤ÁªèÊúâËß£ÂÜ≥ÂäûÊ≥ï„ÄÇ

## üëã ÊúÄÁÆÄÈÖçÁΩÆËøêË°å

Â∑≤ÁªèÊîØÊåÅÂú® web È°µÈù¢ÈÖçÁΩÆÂÖ∂‰ªñÂèÇÊï∞Ôºådocker ÂêØÂä®ÂëΩ‰ª§Â¶Ç‰∏ã:

```bash
docker run -p 58090:8090 -e XIAOMUSIC_PUBLIC_PORT=58090 -v /xiaomusic_music:/app/music -v /xiaomusic_conf:/app/conf hanxi/xiaomusic
```

üî• ÂõΩÂÜÖÔºö

```bash
docker run -p 58090:8090 -e XIAOMUSIC_PUBLIC_PORT=58090 -v /xiaomusic_music:/app/music -v /xiaomusic_conf:/app/conf docker.hanxi.cc/hanxi/xiaomusic
```

ÂØπÂ∫îÁöÑ docker compose ÈÖçÁΩÆÂ¶Ç‰∏ãÔºö

```yaml
services:
  xiaomusic:
    image: hanxi/xiaomusic
    container_name: xiaomusic
    restart: unless-stopped
    ports:
      - 58090:8090
    environment:
      XIAOMUSIC_PUBLIC_PORT: 58090
    volumes:
      - /xiaomusic_music:/app/music
      - /xiaomusic_conf:/app/conf
```

üî• ÂõΩÂÜÖÔºö

```yaml
services:
  xiaomusic:
    image: docker.hanxi.cc/hanxi/xiaomusic
    container_name: xiaomusic
    restart: unless-stopped
    ports:
      - 58090:8090
    environment:
      XIAOMUSIC_PUBLIC_PORT: 58090
    volumes:
      - /xiaomusic_music:/app/music
      - /xiaomusic_conf:/app/conf
```

- ÂÖ∂‰∏≠ conf ÁõÆÂΩï‰∏∫ÈÖçÁΩÆÊñá‰ª∂Â≠òÊîæÁõÆÂΩïÔºåmusic ÁõÆÂΩï‰∏∫Èü≥‰πêÂ≠òÊîæÁõÆÂΩïÔºåÂª∫ËÆÆÂàÜÂºÄÈÖçÁΩÆ‰∏∫‰∏çÂêåÁöÑÁõÆÂΩï„ÄÇ
- /xiaomusic_music Âíå /xiaomusic_conf ÊòØ docker ÊâÄÂú®ÁöÑ‰∏ªÊú∫ÁöÑÁõÆÂΩïÔºåÂèØ‰ª•‰øÆÊîπ‰∏∫ÂÖ∂‰ªñÁõÆÂΩï„ÄÇÂ¶ÇÊûúÊä•ÈîôÊâæ‰∏çÂà∞ /xiaomusic_music ÁõÆÂΩïÔºåÂèØ‰ª•ÂÖàÊâßË°å `mkdir -p /xiaomusic_{music,conf}` ÂëΩ‰ª§Êñ∞Âª∫ÁõÆÂΩï„ÄÇ
- /app/music Âíå /app/conf ÊòØ docker ÂÆπÂô®ÈáåÁöÑÁõÆÂΩïÔºå‰∏çË¶ÅÂéª‰øÆÊîπ„ÄÇ
- XIAOMUSIC_PUBLIC_PORT ÊòØÁî®Êù•ÈÖçÁΩÆ NAS Êú¨Âú∞Á´ØÂè£ÁöÑ„ÄÇ8090 ÊòØÂÆπÂô®Á´ØÂè£Ôºå‰∏çË¶ÅÂéª‰øÆÊîπ„ÄÇ
- ÂêéÂè∞ËÆøÈóÆÂú∞ÂùÄ‰∏∫Ôºö http://NAS_IP:58090

&gt; [!NOTE]
&gt; docker Âíå docker compose ‰∫åÈÄâ‰∏ÄÂç≥ÂèØÔºåÂêØÂä®ÊàêÂäüÂêéÔºåÂú® web È°µÈù¢ÂèØ‰ª•ÈÖçÁΩÆÂÖ∂‰ªñÂèÇÊï∞ÔºåÂ∏¶Êúâ `*` Âè∑ÁöÑÈÖçÁΩÆÊòØÂøÖÈ°ªË¶ÅÈÖçÁΩÆÁöÑÔºåÂÖ∂‰ªñÁöÑÁî®‰∏ç‰∏äÊó∂‰∏çÁî®‰øÆÊîπ„ÄÇÂàùÊ¨°ÈÖçÁΩÆÊó∂ÈúÄË¶ÅÂú®È°µÈù¢‰∏äËæìÂÖ•Â∞èÁ±≥Ë¥¶Âè∑ÂíåÂØÜÁ†Å‰øùÂ≠òÂêéÊâçËÉΩËé∑ÂèñÂà∞ËÆæÂ§áÂàóË°®„ÄÇ

&gt; [!TIP]
&gt; ÁõÆÂâçÂÆâË£ÖÊ≠•È™§Â∑≤ÁªèÊòØÊúÄÁÆÄÂåñ‰∫ÜÔºåÂ¶ÇÊûúËøòÊòØÂ´åÂÆâË£ÖÈ∫ªÁÉ¶ÔºåÂèØ‰ª•ÂæÆ‰ø°ÊàñËÄÖ QQ Á∫¶ÊàëËøúÁ®ãÂÆâË£ÖÔºåÊàë‰∏ÄËà¨Âë®Êú´ÂíåÊôö‰∏äÊâçÊúâÊó∂Èó¥ÔºåÈúÄË¶ÅËµûÂä©‰∏™ËæõËã¶Ë¥π :moneybag: 50 ÂÖÉ‰∏ÄÊ¨°„ÄÇ

ÈÅáÂà∞ÈóÆÈ¢òÂèØ‰ª•Âéª web ËÆæÁΩÆÈ°µÈù¢Â∫ïÈÉ®ÁÇπÂáª„Äê‰∏ãËΩΩÊó•ÂøóÊñá‰ª∂„ÄëÊåâÈíÆÔºåÁÑ∂ÂêéÊêúÁ¥¢‰∏Ä‰∏ãÊó•ÂøóÊñá‰ª∂ÂÜÖÂÆπÁ°Æ‰øùÈáåÈù¢Ê≤°ÊúâË¥¶Âè∑ÂØÜÁ†Å‰ø°ÊÅØÂêé(ÊúâÂ∞±Âà†Èô§Ëøô‰∫õÊïèÊÑü‰ø°ÊÅØ)ÔºåÁÑ∂ÂêéÂú®Êèê issues ÂèçÈ¶àÈóÆÈ¢òÊó∂Êää‰∏ãËΩΩÁöÑÊó•ÂøóÊñá‰ª∂Â∏¶‰∏ä„ÄÇ

&gt; [!TIP]
&gt; ‰ΩúËÄÖÁöÑÂè¶‰∏Ä‰∏™ÈÄÇÁî®‰∫é NAS ‰∏äÂÆâË£ÖÁöÑÂºÄÊ∫êÂ∑•ÂÖ∑Ôºö &lt;https://github.com/hanxi/tiny-nav&gt;

&gt; [!TIP]
&gt;
&gt; ÂñúÊ¨¢Âê¨‰π¶ÁöÑÂèØ‰ª•ÈÖçÂêàËøô‰∏™Â∑•ÂÖ∑‰ΩøÁî® &lt;https://github.com/hanxi/epub2mp3&gt;

&gt; [!TIP]
&gt;
&gt; - üî•„ÄêÂπøÂëä:ÂèØÁî®‰∫éÂÆâË£Ö frp ÂÆûÁé∞ÂÜÖÁΩëÁ©øÈÄè„Äë
&gt; - üî• Êµ∑Â§ñ RackNerd VPS Êú∫Âô®Êé®ËçêÔºåÂèØÊîØ‰ªòÂÆù‰ªòÊ¨æ„ÄÇ
&gt; - &lt;a href=&quot;https://my.racknerd.com/aff.php?aff=11177&quot;&gt;&lt;img src=&quot;https://racknerd.com/banners/320x50.gif&quot; alt=&quot;RackNerd Mobile Leaderboard Banner&quot; width=&quot;320&quot; height=&quot;50&quot;&gt;&lt;/a&gt;
&gt; - ‰∏çÁü•ÈÅìÈÄâÂì™‰∏™Â•óÈ§êÂèØ‰ª•Áõ¥Êé•‰π∞Ëøô‰∏™ÊúÄ‰æøÂÆúÁöÑ &lt;https://my.racknerd.com/aff.php?aff=11177&amp;pid=917&gt;
&gt; - ‰πüÂèØ‰ª•Áî®Êù•ÈÉ®ÁΩ≤‰ª£ÁêÜÔºådocker ÈÉ®ÁΩ≤ÊñπÊ≥ïËßÅ &lt;https://github.com/hanxi/blog/issues/96&gt;

&gt; [!TIP]
&gt;
&gt; - üî•„ÄêÂπøÂëä: Êê≠Âª∫ÊÇ®ÁöÑ‰∏ìÂ±ûÂ§ßÊ®°Âûã‰∏ªÈ°µ
ÂëäÂà´ÁπÅÁêêÈÖçÁΩÆÈöæÈ¢òÔºå‰∏ÄÈîÆÂç≥ÂèØÁïÖ‰∫´Á®≥ÂÆöÊµÅÁïÖÁöÑAI‰ΩìÈ™åÔºÅ„Äë&lt;https://university.aliyun.com/mobile?userCode=szqvatm6&gt;

&gt; [!TIP]
&gt; - ÂÖçË¥π‰∏ªÊú∫
&gt; - &lt;a href=&quot;https://dartnode.com?aff=SnappyPigeon570&quot;&gt;&lt;img src=&quot;https://dartnode.com/branding/DN-Open-Source-sm.png&quot; alt=&quot;Powered by DartNode - Free VPS for Open Source&quot; width=&quot;320&quot;&gt;&lt;/a&gt;


### ü§ê ÊîØÊåÅËØ≠Èü≥Âè£‰ª§

- „ÄêÊí≠ÊîæÊ≠åÊõ≤„ÄëÔºåÊí≠ÊîæÊú¨Âú∞ÁöÑÊ≠åÊõ≤
- „ÄêÊí≠ÊîæÊ≠åÊõ≤+Ê≠åÂêç„ÄëÔºåÊØîÂ¶ÇÔºöÊí≠ÊîæÊ≠åÊõ≤Âë®Êù∞‰º¶Êô¥Â§©
- „Äê‰∏ä‰∏ÄÈ¶ñ„Äë
- „Äê‰∏ã‰∏ÄÈ¶ñ„Äë
- „ÄêÂçïÊõ≤Âæ™ÁéØ„Äë
- „ÄêÂÖ®ÈÉ®Âæ™ÁéØ„Äë
- „ÄêÈöèÊú∫Êí≠Êîæ„Äë
- „ÄêÂÖ≥Êú∫„ÄëÔºå„ÄêÂÅúÊ≠¢Êí≠Êîæ„ÄëÔºå‰∏§‰∏™ÊïàÊûúÊòØ‰∏ÄÊ†∑ÁöÑ„ÄÇ
- „ÄêÂà∑Êñ∞ÂàóË°®„ÄëÔºåÂΩìÂ§çÂà∂‰∫ÜÊ≠åÊõ≤Ëøõ music ÁõÆÂΩïÂêéÔºåÂèØ‰ª•Áî®Ëøô‰∏™Âè£‰ª§Âà∑Êñ∞Ê≠åÂçï„ÄÇ
- „ÄêÊí≠ÊîæÂàóË°®+ÂàóË°®Âêç„ÄëÔºåÊØîÂ¶ÇÔºöÊí≠ÊîæÂàóË°®ÂÖ∂‰ªñ„ÄÇ
- „ÄêÂä†ÂÖ•Êî∂Ëóè„ÄëÔºåÊääÂΩìÂâçÊí≠ÊîæÁöÑÊ≠åÊõ≤Âä†ÂÖ•Êî∂ËóèÊ≠åÂçï„ÄÇ
- „ÄêÂèñÊ∂àÊî∂Ëóè„ÄëÔºåÊääÂΩìÂâçÊí≠ÊîæÁöÑÊ≠åÊõ≤‰ªéÊî∂ËóèÊ≠åÂçïÈáåÁßªÈô§„ÄÇ
- „ÄêÊí≠ÊîæÂàóË°®Êî∂Ëóè„ÄëÔºåËøô‰∏™Áî®‰∫éÊí≠ÊîæÊî∂ËóèÊ≠åÂçï„ÄÇ
- ~„ÄêÊí≠ÊîæÊú¨Âú∞Ê≠åÊõ≤+Ê≠åÂêç„ÄëÔºåËøô‰∏™Âè£‰ª§ÂíåÊí≠ÊîæÊ≠åÊõ≤ÁöÑÂå∫Âà´ÊòØÊú¨Âú∞Êâæ‰∏çÂà∞‰πü‰∏ç‰ºöÂéª‰∏ãËΩΩ„ÄÇ~
- „ÄêÊí≠ÊîæÂàóË°®Á¨¨Âá†‰∏™+ÂàóË°®Âêç„ÄëÔºåÂÖ∑‰ΩìËßÅÔºö &lt;https://github.com/hanxi/xiaomusic/issues/158&gt;
- „ÄêÊêúÁ¥¢Êí≠Êîæ+ÂÖ≥ÈîÆËØç„ÄëÔºå‰ºöÊêúÁ¥¢ÂÖ≥ÈîÆËØç‰Ωú‰∏∫‰∏¥Êó∂ÊêúÁ¥¢ÂàóË°®Êí≠ÊîæÔºåÊØîÂ¶ÇËØ¥„ÄêÊêúÁ¥¢Êí≠ÊîæÊûó‰øäÊù∞„ÄëÔºå‰ºöÊí≠ÊîæÊâÄÊúâÊûó‰øäÊù∞ÁöÑÊ≠å„ÄÇ
- „ÄêÊú¨Âú∞ÊêúÁ¥¢Êí≠Êîæ+ÂÖ≥ÈîÆËØç„ÄëÔºåË∑üÊêúÁ¥¢Êí≠ÊîæÁöÑÂå∫Âà´ÊòØÊú¨Âú∞Êâæ‰∏çÂà∞‰πü‰∏ç‰ºöÂéª‰∏ãËΩΩ„ÄÇ

&gt; [!TIP]
&gt; ÈöêËóèÁé©Ê≥ï: ÂØπÂ∞èÁà±ÂêåÂ≠¶ËØ¥Êí≠ÊîæÊ≠åÊõ≤Â∞èÁå™‰Ω©Â•áÁöÑÊïÖ‰∫ãÔºå‰ºöÂÖà‰∏ãËΩΩÂ∞èÁå™‰Ω©Â•áÁöÑÊïÖ‰∫ãÔºåÁÑ∂ÂêéÂÜçÊí≠ÊîæÂ∞èÁå™‰Ω©Â•áÁöÑÊïÖ‰∫ã„ÄÇ

## üõ†Ô∏è pip ÊñπÂºèÂÆâË£ÖËøêË°å

```shell
&gt; pip install -U xiaomusic
&gt; xiaomusic --help
 __  __  _                   __  __                 _
 \ \/ / (_)   __ _    ___   |  \/  |  _   _   ___  (_)   ___
  \  /  | |  / _` |  / _ \  | |\/| | | | | | / __| | |  / __|
  /  \  | | | (_| | | (_) | | |  | | | |_| | \__ \ | | | (__
 /_/\_\ |_|  \__,_|  \___/  |_|  |_|  \__,_| |___/ |_|  \___|
          XiaoMusic v0.3.69 by: github.com/hanxi

usage: xiaomusic [-h] [--port PORT] [--hardware HARDWARE] [--account ACCOUNT]
                 [--password PASSWORD] [--cookie COOKIE] [--verbose]
                 [--config CONFIG] [--ffmpeg_location FFMPEG_LOCATION]

options:
  -h, --help            show this help message and exit
  --port PORT           ÁõëÂê¨Á´ØÂè£
  --hardware HARDWARE   Â∞èÁà±Èü≥ÁÆ±ÂûãÂè∑
  --account ACCOUNT     xiaomi account
  --password PASSWORD   xiaomi password
  --cookie COOKIE       xiaomi cookie
  --verbose             show info
  --config CONFIG       config file path
  --ffmpeg_location FFMPEG_LOCATION
                        ffmpeg bin path
&gt; xiaomusic --config config.json
```

ÂÖ∂‰∏≠ `config.json` Êñá‰ª∂ÂèØ‰ª•ÂèÇËÄÉ `config-example.json` Êñá‰ª∂ÈÖçÁΩÆ„ÄÇËßÅ &lt;https://github.com/hanxi/xiaomusic/issues/94&gt;

‰∏ç‰øÆÊîπÈªòËÆ§Á´ØÂè£ 8090 ÁöÑÊÉÖÂÜµ‰∏ãÔºåÂè™ÈúÄË¶ÅÊâßË°å `xiaomusic` Âç≥ÂèØÂêØÂä®„ÄÇ

## üî© ÂºÄÂèëÁéØÂ¢ÉËøêË°å

- ‰ΩøÁî® install_dependencies.sh ‰∏ãËΩΩ‰æùËµñ
- ‰ΩøÁî® pdm ÂÆâË£ÖÁéØÂ¢É
- ÈªòËÆ§ÁõëÂê¨‰∫ÜÁ´ØÂè£ 8090 , ‰ΩøÁî®ÂÖ∂‰ªñÁ´ØÂè£Ëá™Ë°å‰øÆÊîπ„ÄÇ

```shell
pdm run xiaomusic.py
````

Â¶ÇÊûúÊòØÂºÄÂèëÂâçÁ´ØÁïåÈù¢ÔºåÂèØ‰ª•ÈÄöËøá &lt;http://localhost:8090/docs&gt;
Êü•ÁúãÊúâ‰ªÄ‰πàÊé•Âè£„ÄÇÁõÆÂâçÁöÑ web ÊéßÂà∂Âè∞ÈùûÂ∏∏ÁÆÄÈôãÔºåÊ¨¢ËøéÊúâÂÖ¥Ë∂£ÁöÑÊúãÂèãÂ∏ÆÂøôÂÆûÁé∞‰∏Ä‰∏™ÊºÇ‰∫ÆÁöÑÂâçÁ´ØÔºåÈúÄË¶Å‰ªÄ‰πàÊé•Âè£ÂèØ‰ª•ÈöèÊó∂ÊèêÈúÄÊ±Ç„ÄÇ

### üö¶ ‰ª£Á†ÅÊèê‰∫§ËßÑËåÉ

Êèê‰∫§ÂâçËØ∑ÊâßË°å

```
pdm lintfmt
```

Áî®‰∫éÊ£ÄÊü•‰ª£Á†ÅÂíåÊ†ºÂºèÂåñ‰ª£Á†Å„ÄÇ

### Êú¨Âú∞ÁºñËØë Docker Image

```shell
docker build -t xiaomusic .
```

### ÊäÄÊúØÊ†à

- ÂêéÁ´Ø‰ª£Á†Å‰ΩøÁî® Python ËØ≠Ë®ÄÁºñÂÜô„ÄÇ
- HTTP ÊúçÂä°‰ΩøÁî®ÁöÑÊòØ FastAPI Ê°ÜÊû∂Ôºå~~Êó©ÊúüÁâàÊú¨‰ΩøÁî®ÁöÑÊòØ Flask~~„ÄÇ
- ‰ΩøÁî®‰∫Ü Docker ÔºåÂú® NAS ‰∏äÂÆâË£ÖÊõ¥Êñπ‰æø„ÄÇ
- ÈªòËÆ§ÁöÑÂâçÁ´Ø‰∏ªÈ¢ò‰ΩøÁî®‰∫Ü jQuery „ÄÇ

## Â∑≤ÊµãËØïÊîØÊåÅÁöÑËÆæÂ§á

| ÂûãÂè∑   | ÂêçÁß∞                                                                                             |
| ---- | ---------------------------------------------------------------------------------------------- |
| L06A | [Â∞èÁà±Èü≥ÁÆ±](https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.l06a)             |
| L07A | [RedmiÂ∞èÁà±Èü≥ÁÆ± Play](https://home.mi.com/webapp/content/baike/product/index.html?model=xiaomi.wifispeaker.l7a)                     |
| S12/S12A/MDZ-25-DA | [Â∞èÁ±≥AIÈü≥ÁÆ±](https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.s12)            |
| LX5A | [Â∞èÁà±Èü≥ÁÆ± ‰∏áËÉΩÈÅ•ÊéßÁâà](https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.lx5a)       |
| LX05 | [Â∞èÁà±Èü≥ÁÆ±PlayÔºà2019Ê¨æÔºâ](https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.lx05)  |
| L15A | [Â∞èÁ±≥AIÈü≥ÁÆ±ÔºàÁ¨¨‰∫å‰ª£Ôºâ](https://home.mi.com/webapp/content/baike/product/index.html?model=xiaomi.wifispeaker.l15a#/) |
| L16A | [Xiaomi Sound](https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.l16a)     |
| L17A | [Xiaomi Sound Pro](https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.l17a) |
| LX06 | [Â∞èÁà±Èü≥ÁÆ±Pro](https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.lx06)          |
| LX01 | [Â∞èÁà±Èü≥ÁÆ±mini](https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.lx01)         |
| L05B | [Â∞èÁà±Èü≥ÁÆ±Play](https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.l05b)         |
| L05C | [Â∞èÁ±≥Â∞èÁà±Èü≥ÁÆ±Play Â¢ûÂº∫Áâà](https://home.mi.com/baike/index.html#/detail?model=xiaomi.wifispeaker.l05c)   |
| L09A | [Â∞èÁ±≥Èü≥ÁÆ±Art](https://home.mi.com/webapp/content/baike/product/index.html?model=xiaomi.wifispeaker.l09a) |
| LX04 X10A X08A | Â∑≤ÁªèÊîØÊåÅÁöÑËß¶Â±èÁâà |
| X08C X08E X8F | Â∑≤Áªè‰∏çÈúÄË¶ÅËÆæÁΩÆ‰∫Ü. ~ÈúÄË¶ÅËÆæÁΩÆ„ÄêÂûãÂè∑ÂÖºÂÆπÊ®°Âºè„ÄëÈÄâÈ°π‰∏∫ true~ |
| M01/XMYX01JY | Â∞èÁ±≥Â∞èÁà±Èü≥ÁÆ±HD ÈúÄË¶ÅËÆæÁΩÆ„ÄêÁâπÊÆäÂûãÂè∑Ëé∑ÂèñÂØπËØùËÆ∞ÂΩï„ÄëÈÄâÈ°π‰∏∫ true ÊâçËÉΩËØ≠Èü≥Êí≠Êîæ|
| OH2P | XIAOMI Êô∫ËÉΩÈü≥ÁÆ± Pro |
| OH2 | XIAOMI Êô∫ËÉΩÈü≥ÁÆ± |

ÂûãÂè∑‰∏é‰∫ßÂìÅÂêçÁß∞ÂØπÁÖßÂèØ‰ª•Âú®ËøôÈáåÊü•ËØ¢ &lt;https://home.miot-spec.com/s/xiaomi.wifispeaker&gt;

&gt; [!NOTE]
&gt; Â¶ÇÊûú‰Ω†ÁöÑËÆæÂ§áÊîØÊåÅÊí≠ÊîæÔºåËØ∑ÂèçÈ¶àÁªôÊàëÊ∑ªÂä†Âà∞ÊîØÊåÅÂàóË°®ÈáåÔºåË∞¢Ë∞¢„ÄÇ
&gt; ÁõÆÂâçÂ∫îËØ•ÊâÄÊúâËÆæÂ§áÁ±ªÂûãÈÉΩÂ∑≤ÁªèÊîØÊåÅÊí≠ÊîæÔºåÊúâÈóÆÈ¢òÈöèÊó∂ÂèçÈ¶à„ÄÇ
&gt; ÂÖ∂‰ªñËß¶Â±èÁâà‰∏çËÉΩÊí≠ÊîæÂèØ‰ª•ËÆæÁΩÆ„ÄêÂûãÂè∑ÂÖºÂÆπÊ®°Âºè„ÄëÈÄâÈ°π‰∏∫ true ËØïËØï„ÄÇËßÅ &lt;https://github.com/hanxi/xiaomusic/issues/30&gt;

## üéµ ÊîØÊåÅÈü≥‰πêÊ†ºÂºè

- mp3
- flac
- wav
- ape
- ogg
- m4a

&gt; [!NOTE]
&gt; Êú¨Âú∞Èü≥‰πê‰ºöÊêúÁ¥¢ÁõÆÂΩï‰∏ã‰∏äÈù¢Ê†ºÂºèÁöÑÊñá‰ª∂Ôºå‰∏ãËΩΩÁöÑÊ≠åÊõ≤ÊòØ mp3 Ê†ºÂºèÁöÑ„ÄÇ
&gt; Â∑≤Áü• L05B L05C LX06 L16A ‰∏çÊîØÊåÅ flac Ê†ºÂºè„ÄÇ
&gt; Â¶ÇÊûúÊ†ºÂºè‰∏çËÉΩÊí≠ÊîæÂèØ‰ª•ÊâìÂºÄ„ÄêËΩ¨Êç¢‰∏∫MP3„ÄëÂíå„ÄêÂûãÂè∑ÂÖºÂÆπÊ®°Âºè„ÄëÈÄâÈ°π„ÄÇÂÖ∑‰ΩìËßÅ &lt;https://github.com/hanxi/xiaomusic/issues/153#issuecomment-2328168689&gt;

## üåè ÁΩëÁªúÊ≠åÂçïÂäüËÉΩ

ÂèØ‰ª•ÈÖçÁΩÆ‰∏Ä‰∏™ json Ê†ºÂºèÁöÑÊ≠åÂçïÔºåÊîØÊåÅÁîµÂè∞ÂíåÊ≠åÊõ≤Ôºå‰πüÂèØ‰ª•Áõ¥Êé•Áî®Âà´‰∫∫ÂàÜ‰∫´ÁöÑÈìæÊé•ÔºåÂêåÊó∂ÈÖçÂ§á‰∫Ü m3u Êñá‰ª∂Ê†ºÂºèËΩ¨Êç¢Â∑•ÂÖ∑ÔºåÂèØ‰ª•ÂæàÊñπ‰æøÁöÑÊää m3u ÁîµÂè∞Êñá‰ª∂ËΩ¨Êç¢ÊàêÁΩëÁªúÊ≠åÂçïÊ†ºÂºèÁöÑ json Êñá‰ª∂ÔºåÂÖ∑‰ΩìÁî®Ê≥ïËßÅ  &lt;https://github.com/hanxi/xiaomusic/issues/78&gt;

&gt; [!NOTE]
&gt; Ê¨¢ËøéÊúâÊÉ≥Ê≥ïÁöÑÊúãÂèã‰ª¨Âà∂‰ΩúÊõ¥Â§öÁöÑÊ≠åÂçïËΩ¨Êç¢Â∑•ÂÖ∑„ÄÇ

## üç∫ Êõ¥Â§öÂÖ∂‰ªñÂèØÈÄâÈÖçÁΩÆ

ËßÅ &lt;https://github.com/hanxi/xiaomusic/issues/333&gt;

## ‚ö†Ô∏è ÂÆâÂÖ®ÊèêÈÜí

&gt; [!IMPORTANT]
&gt;
&gt; 1. Â¶ÇÊûúÈÖçÁΩÆ‰∫ÜÂÖ¨ÁΩëËÆøÈóÆ xiaomusic ÔºåËØ∑‰∏ÄÂÆöË¶ÅÂºÄÂêØÂØÜÁ†ÅÁôªÈôÜÔºåÂπ∂ËÆæÁΩÆÂ§çÊùÇÁöÑÂØÜÁ†Å„ÄÇ‰∏î‰∏çË¶ÅÂú®ÂÖ¨ÂÖ±Âú∫ÊâÄÁöÑ WiFi ÁéØÂ¢É‰∏ã‰ΩøÁî®ÔºåÂê¶ÂàôÂèØËÉΩÈÄ†ÊàêÂ∞èÁ±≥Ë¥¶Âè∑ÂØÜÁ†ÅÊ≥ÑÈú≤„ÄÇ
&gt; 2. Âº∫ÁÉà‰∏çÂª∫ËÆÆÂ∞ÜÂ∞èÁà±Èü≥ÁÆ±ÁöÑÂ∞èÁ±≥Ë¥¶Âè∑ÁªëÂÆöÊëÑÂÉèÂ§¥Ôºå‰ª£Á†ÅÈöæÂÖç‰ºöÊúâ bug Ôºå‰∏ÄÊó¶Â∞èÁ±≥Ë¥¶Âè∑ÂØÜÁ†ÅÊ≥ÑÈú≤ÔºåÂèØËÉΩÁõëÊéßÂΩïÂÉè‰πü‰ºöÊ≥ÑÈú≤„ÄÇ

## ü§î È´òÁ∫ßÁØá

- Ëá™ÂÆö‰πâÂè£‰ª§ÂäüËÉΩ &lt;https://github.com/hanxi/xiaomusic/issues/105&gt;
- &lt;https://github.com/hanxi/xiaomusic/issues/312&gt;
- &lt;https://github.com/hanxi/xiaomusic/issues/269&gt;
- &lt;https://github.com/hanxi/xiaomusic/issues/159&gt;

## üì¢ ËÆ®ËÆ∫Âå∫

- [ÁÇπÂáªÈìæÊé•Âä†ÂÖ•QQÈ¢ëÈÅì„Äêxiaomusic„Äë](https://pd.qq.com/s/e2jybz0ss)
- [ÁÇπÂáªÈìæÊé•Âä†ÂÖ•Áæ§ËÅä„ÄêxiaomusicÂÆòÊñπ‰∫§ÊµÅÁæ§3„Äë 1072151477](https://qm.qq.com/q/lxIhquqbza)
- &lt;https://github.com/hanxi/xiaomusic/issues&gt;
- [ÂæÆ‰ø°Áæ§‰∫åÁª¥Á†Å](https://github.com/hanxi/xiaomusic/issues/86)

## ‚ù§Ô∏è ÊÑüË∞¢

- [xiaomi](https://www.mi.com/)
- [PDM](https://pdm.fming.dev/latest/)
- [xiaogpt](https://github.com/yihong0618/xiaogpt)
- [MiService](https://github.com/yihong0618/MiService)
- [ÂÆûÁé∞ÂéüÁêÜ](https://github.com/yihong0618/gitblog/issues/258)
- [yt-dlp](https://github.com/yt-dlp/yt-dlp)
- [awesome-xiaoai](https://github.com/zzz6519003/awesome-xiaoai)
- [ÂæÆ‰ø°Â∞èÁ®ãÂ∫è: ÂçØÂçØÈü≥‰πê](https://github.com/F-loat/xiaoplayer)
- [pure ‰∏ªÈ¢ò xiaomusicUI](https://github.com/52fisher/xiaomusicUI)
- [ÁßªÂä®Á´ØÁöÑÊí≠ÊîæÂô®‰∏ªÈ¢ò](https://github.com/52fisher/XMusicPlayer)
- [Tailwind‰∏ªÈ¢ò](https://github.com/clarencejh/xiaomusic)
- [SoundScape‰∏ªÈ¢ò](https://github.com/jhao0413/SoundScape)
- [‰∏Ä‰∏™Á¨¨‰∏âÊñπÁöÑ‰∏ªÈ¢ò](https://github.com/DarrenWen/xiaomusicui)
- [Umami ÁªüËÆ°](https://github.com/umami-software/umami)
- [Sentry Êä•ÈîôÁõëÊéß](https://github.com/getsentry/sentry)
- ÊâÄÊúâÂ∏ÆÂøôË∞ÉËØïÂíåÊµãËØïÁöÑÊúãÂèã
- ÊâÄÊúâÂèçÈ¶àÈóÆÈ¢òÂíåÂª∫ËÆÆÁöÑÊúãÂèã

### üëâ ÂÖ∂‰ªñÊïôÁ®ã

Êõ¥Â§öÂäüËÉΩËßÅ [üìù ÊñáÊ°£Ê±áÊÄª](https://github.com/hanxi/xiaomusic/issues/211)

## üö® ÂÖçË¥£Â£∞Êòé

Êú¨È°πÁõÆ‰ªÖ‰æõÂ≠¶‰π†ÂíåÁ†îÁ©∂ÁõÆÁöÑÔºå‰∏çÂæóÁî®‰∫é‰ªª‰ΩïÂïÜ‰∏öÊ¥ªÂä®„ÄÇÁî®Êà∑Âú®‰ΩøÁî®Êú¨È°πÁõÆÊó∂Â∫îÈÅµÂÆàÊâÄÂú®Âú∞Âå∫ÁöÑÊ≥ïÂæãÊ≥ïËßÑÔºåÂØπ‰∫éËøùÊ≥ï‰ΩøÁî®ÊâÄÂØºËá¥ÁöÑÂêéÊûúÔºåÊú¨È°πÁõÆÂèä‰ΩúËÄÖ‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªª„ÄÇ
Êú¨È°πÁõÆÂèØËÉΩÂ≠òÂú®Êú™Áü•ÁöÑÁº∫Èô∑ÂíåÈ£éÈô©ÔºàÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éËÆæÂ§áÊçüÂùèÂíåË¥¶Âè∑Â∞ÅÁ¶ÅÁ≠âÔºâÔºå‰ΩøÁî®ËÄÖÂ∫îËá™Ë°åÊâøÊãÖ‰ΩøÁî®Êú¨È°πÁõÆÊâÄ‰∫ßÁîüÁöÑÊâÄÊúâÈ£éÈô©ÂèäË¥£‰ªª„ÄÇ
‰ΩúËÄÖ‰∏ç‰øùËØÅÊú¨È°πÁõÆÁöÑÂáÜÁ°ÆÊÄß„ÄÅÂÆåÊï¥ÊÄß„ÄÅÂèäÊó∂ÊÄß„ÄÅÂèØÈù†ÊÄßÔºå‰πü‰∏çÊâøÊãÖ‰ªª‰ΩïÂõ†‰ΩøÁî®Êú¨È°πÁõÆËÄå‰∫ßÁîüÁöÑ‰ªª‰ΩïÊçüÂ§±ÊàñÊçüÂÆ≥Ë¥£‰ªª„ÄÇ
‰ΩøÁî®Êú¨È°πÁõÆÂç≥Ë°®Á§∫ÊÇ®Â∑≤ÈòÖËØªÂπ∂ÂêåÊÑèÊú¨ÂÖçË¥£Â£∞ÊòéÁöÑÂÖ®ÈÉ®ÂÜÖÂÆπ„ÄÇ

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=hanxi/xiaomusic&amp;type=Date)](https://star-history.com/#hanxi/xiaomusic&amp;Date)

## ËµûËµè

- :moneybag: Áà±ÂèëÁîµ &lt;https://afdian.com/a/imhanxi&gt;
- ÁÇπ‰∏™ Star :star:
- Ë∞¢Ë∞¢ :heart:
- ![ÂñùÊùØÂ•∂Ëå∂](https://i.v2ex.co/7Q03axO5l.png)

## License

[MIT](https://github.com/hanxi/xiaomusic/blob/main/LICENSE) License ¬© 2024 Ê∂µÊõ¶
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[python/cpython]]></title>
            <link>https://github.com/python/cpython</link>
            <guid>https://github.com/python/cpython</guid>
            <pubDate>Mon, 10 Nov 2025 00:04:31 GMT</pubDate>
            <description><![CDATA[The Python programming language]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/python/cpython">python/cpython</a></h1>
            <p>The Python programming language</p>
            <p>Language: Python</p>
            <p>Stars: 69,766</p>
            <p>Forks: 33,346</p>
            <p>Stars today: 22 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[OpenHands/OpenHands]]></title>
            <link>https://github.com/OpenHands/OpenHands</link>
            <guid>https://github.com/OpenHands/OpenHands</guid>
            <pubDate>Mon, 10 Nov 2025 00:04:30 GMT</pubDate>
            <description><![CDATA[üôå OpenHands: Code Less, Make More]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenHands/OpenHands">OpenHands/OpenHands</a></h1>
            <p>üôå OpenHands: Code Less, Make More</p>
            <p>Language: Python</p>
            <p>Stars: 64,830</p>
            <p>Forks: 7,877</p>
            <p>Stars today: 35 stars today</p>
            <h2>README</h2><pre>&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/All-Hands-AI/docs/main/openhands/static/img/logo.png&quot; alt=&quot;Logo&quot; width=&quot;200&quot;&gt;
  &lt;h1 align=&quot;center&quot;&gt;OpenHands: Code Less, Make More&lt;/h1&gt;
&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/OpenHands/OpenHands/graphs/contributors&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/OpenHands/OpenHands?style=for-the-badge&amp;color=blue&quot; alt=&quot;Contributors&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/OpenHands/OpenHands/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/OpenHands/OpenHands?style=for-the-badge&amp;color=blue&quot; alt=&quot;Stargazers&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/OpenHands/OpenHands/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/OpenHands/OpenHands?style=for-the-badge&amp;color=blue&quot; alt=&quot;MIT License&quot;&gt;&lt;/a&gt;
  &lt;br/&gt;
  &lt;a href=&quot;https://all-hands.dev/joinslack&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Slack-Join%20Us-red?logo=slack&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Join our Slack community&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/OpenHands/OpenHands/blob/main/CREDITS.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Project-Credits-blue?style=for-the-badge&amp;color=FFE165&amp;logo=github&amp;logoColor=white&quot; alt=&quot;Credits&quot;&gt;&lt;/a&gt;
  &lt;br/&gt;
  &lt;a href=&quot;https://docs.all-hands.dev/usage/getting-started&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;logoColor=FFE165&amp;style=for-the-badge&quot; alt=&quot;Check out the documentation&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://arxiv.org/abs/2407.16741&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Paper%20on%20Arxiv-000?logoColor=FFE165&amp;logo=arxiv&amp;style=for-the-badge&quot; alt=&quot;Paper on Arxiv&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1wOUdFCMyY6Nt0AIqF705KN4JKOWgeI4wUGUP60krXXs/edit?gid=0#gid=0&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Benchmark%20score-000?logoColor=FFE165&amp;logo=huggingface&amp;style=for-the-badge&quot; alt=&quot;Evaluation Benchmark Score&quot;&gt;&lt;/a&gt;

  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://www.readme-i18n.com/OpenHands/OpenHands?lang=de&quot;&gt;Deutsch&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/OpenHands/OpenHands?lang=es&quot;&gt;Espa√±ol&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/OpenHands/OpenHands?lang=fr&quot;&gt;fran√ßais&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/OpenHands/OpenHands?lang=ja&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/OpenHands/OpenHands?lang=ko&quot;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/OpenHands/OpenHands?lang=pt&quot;&gt;Portugu√™s&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/OpenHands/OpenHands?lang=ru&quot;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/OpenHands/OpenHands?lang=zh&quot;&gt;‰∏≠Êñá&lt;/a&gt;

  &lt;hr&gt;
&lt;/div&gt;

Welcome to OpenHands (formerly OpenDevin), a platform for software development agents powered by AI.

OpenHands agents can do anything a human developer can: modify code, run commands, browse the web,
call APIs, and yes‚Äîeven copy code snippets from StackOverflow.

Learn more at [docs.all-hands.dev](https://docs.all-hands.dev), or [sign up for OpenHands Cloud](https://app.all-hands.dev) to get started.


&gt; [!IMPORTANT]
&gt; **Upcoming change**: We are renaming our GitHub Org from `All-Hands-AI` to `OpenHands` on October 20th, 2025.
&gt; Check the [tracking issue](https://github.com/All-Hands-AI/OpenHands/issues/11376) for more information.


&gt; [!IMPORTANT]
&gt; Using OpenHands for work? We&#039;d love to chat! Fill out
&gt; [this short form](https://docs.google.com/forms/d/e/1FAIpQLSet3VbGaz8z32gW9Wm-Grl4jpt5WgMXPgJ4EDPVmCETCBpJtQ/viewform)
&gt; to join our Design Partner program, where you&#039;ll get early access to commercial features and the opportunity to provide input on our product roadmap.

## ‚òÅÔ∏è OpenHands Cloud
The easiest way to get started with OpenHands is on [OpenHands Cloud](https://app.all-hands.dev),
which comes with $10 in free credits for new users.

## üíª Running OpenHands Locally

### Option 1: CLI Launcher (Recommended)

The easiest way to run OpenHands locally is using the CLI launcher with [uv](https://docs.astral.sh/uv/). This provides better isolation from your current project&#039;s virtual environment and is required for OpenHands&#039; default MCP servers.

**Install uv** (if you haven&#039;t already):

See the [uv installation guide](https://docs.astral.sh/uv/getting-started/installation/) for the latest installation instructions for your platform.

**Launch OpenHands**:
```bash
# Launch the GUI server
uvx --python 3.12 openhands serve

# Or launch the CLI
uvx --python 3.12 openhands
```

You&#039;ll find OpenHands running at [http://localhost:3000](http://localhost:3000) (for GUI mode)!

### Option 2: Docker

&lt;details&gt;
&lt;summary&gt;Click to expand Docker command&lt;/summary&gt;

You can also run OpenHands directly with Docker:

```bash
docker pull docker.openhands.dev/openhands/runtime:0.61-nikolaik

docker run -it --rm --pull=always \
    -e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.openhands.dev/openhands/runtime:0.61-nikolaik \
    -e LOG_ALL_EVENTS=true \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -v ~/.openhands:/.openhands \
    -p 3000:3000 \
    --add-host host.docker.internal:host-gateway \
    --name openhands-app \
    docker.openhands.dev/openhands/openhands:0.61
```

&lt;/details&gt;

&gt; **Note**: If you used OpenHands before version 0.44, you may want to run `mv ~/.openhands-state ~/.openhands` to migrate your conversation history to the new location.

&gt; [!WARNING]
&gt; On a public network? See our [Hardened Docker Installation Guide](https://docs.all-hands.dev/usage/runtimes/docker#hardened-docker-installation)
&gt; to secure your deployment by restricting network binding and implementing additional security measures.

### Getting Started

When you open the application, you&#039;ll be asked to choose an LLM provider and add an API key.
[Anthropic&#039;s Claude Sonnet 4.5](https://www.anthropic.com/api) (`anthropic/claude-sonnet-4-5-20250929`)
works best, but you have [many options](https://docs.all-hands.dev/usage/llms).

See the [Running OpenHands](https://docs.all-hands.dev/usage/installation) guide for
system requirements and more information.

## üí° Other ways to run OpenHands

&gt; [!WARNING]
&gt; OpenHands is meant to be run by a single user on their local workstation.
&gt; It is not appropriate for multi-tenant deployments where multiple users share the same instance. There is no built-in authentication, isolation, or scalability.
&gt;
&gt; If you&#039;re interested in running OpenHands in a multi-tenant environment, check out the source-available, commercially-licensed
&gt; [OpenHands Cloud Helm Chart](https://github.com/openHands/OpenHands-cloud)

You can [connect OpenHands to your local filesystem](https://docs.all-hands.dev/usage/runtimes/docker#connecting-to-your-filesystem),
interact with it via a [friendly CLI](https://docs.all-hands.dev/usage/how-to/cli-mode),
run OpenHands in a scriptable [headless mode](https://docs.all-hands.dev/usage/how-to/headless-mode),
or run it on tagged issues with [a github action](https://docs.all-hands.dev/usage/how-to/github-action).

Visit [Running OpenHands](https://docs.all-hands.dev/usage/installation) for more information and setup instructions.

If you want to modify the OpenHands source code, check out [Development.md](https://github.com/OpenHands/OpenHands/blob/main/Development.md).

Having issues? The [Troubleshooting Guide](https://docs.all-hands.dev/usage/troubleshooting) can help.

## üìñ Documentation

To learn more about the project, and for tips on using OpenHands,
check out our [documentation](https://docs.all-hands.dev/usage/getting-started).

There you&#039;ll find resources on how to use different LLM providers,
troubleshooting resources, and advanced configuration options.

## ü§ù How to Join the Community

OpenHands is a community-driven project, and we welcome contributions from everyone. We do most of our communication
through Slack, so this is the best place to start, but we also are happy to have you contact us on Github:

- [Join our Slack workspace](https://all-hands.dev/joinslack) - Here we talk about research, architecture, and future development.
- [Read or post Github Issues](https://github.com/OpenHands/OpenHands/issues) - Check out the issues we&#039;re working on, or add your own ideas.

See more about the community in [COMMUNITY.md](./COMMUNITY.md) or find details on contributing in [CONTRIBUTING.md](./CONTRIBUTING.md).

## üìà Progress

See the monthly OpenHands roadmap [here](https://github.com/orgs/OpenHands/projects/1) (updated at the maintainer&#039;s meeting at the end of each month).

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://star-history.com/#OpenHands/OpenHands&amp;Date&quot;&gt;
    &lt;img src=&quot;https://api.star-history.com/svg?repos=OpenHands/OpenHands&amp;type=Date&quot; width=&quot;500&quot; alt=&quot;Star History Chart&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

## üìú License

Distributed under the MIT License, with the exception of the `enterprise/` folder. See [`LICENSE`](./LICENSE) for more information.

## üôè Acknowledgements

OpenHands is built by a large number of contributors, and every contribution is greatly appreciated! We also build upon other open source projects, and we are deeply thankful for their work.

For a list of open source projects and licenses used in OpenHands, please see our [CREDITS.md](./CREDITS.md) file.

## üìö Cite

```
@inproceedings{
  wang2025openhands,
  title={OpenHands: An Open Platform for {AI} Software Developers as Generalist Agents},
  author={Xingyao Wang and Boxuan Li and Yufan Song and Frank F. Xu and Xiangru Tang and Mingchen Zhuge and Jiayi Pan and Yueqi Song and Bowen Li and Jaskirat Singh and Hoang H. Tran and Fuqiang Li and Ren Ma and Mingzhang Zheng and Bill Qian and Yanjun Shao and Niklas Muennighoff and Yizhe Zhang and Binyuan Hui and Junyang Lin and Robert Brennan and Hao Peng and Heng Ji and Graham Neubig},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=OJd3ayDDoF}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[AI-Hypercomputer/maxtext]]></title>
            <link>https://github.com/AI-Hypercomputer/maxtext</link>
            <guid>https://github.com/AI-Hypercomputer/maxtext</guid>
            <pubDate>Mon, 10 Nov 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[A simple, performant and scalable Jax LLM!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/AI-Hypercomputer/maxtext">AI-Hypercomputer/maxtext</a></h1>
            <p>A simple, performant and scalable Jax LLM!</p>
            <p>Language: Python</p>
            <p>Stars: 1,975</p>
            <p>Forks: 420</p>
            <p>Stars today: 2 stars today</p>
            <h2>README</h2><pre>&lt;!--
 # Copyright 2023‚Äì2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
 --&gt;

# MaxText

[![MaxText Package Tests](https://github.com/AI-Hypercomputer/maxtext/actions/workflows/RunTests.yml/badge.svg)](https://github.com/AI-Hypercomputer/maxtext/actions/workflows/build_and_test_maxtext.yml)

MaxText is a high performance, highly scalable, open-source LLM library and reference implementation written in pure Python/[JAX](https://docs.jax.dev/en/latest/jax-101.html) and targeting Google Cloud TPUs and GPUs for training. 

MaxText provides a library of high performance models to choose from, including Gemma, Llama, DeepSeek, Qwen, and Mistral. For each of these models, MaxText supports pre-training (up to tens of thousands of chips) and scalable post-training, with popular techniques like Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO, a type of Reinforcement Learning). 

MaxText achieves high Model FLOPs Utilization (MFU) and tokens/second from single host to very large clusters while staying simple and largely &quot;optimization-free&quot; thanks to the power of JAX and the XLA compiler.

MaxText is the launching point for ambitious LLM projects both in research and production. We encourage you to start by experimenting with MaxText out of the box and then fork and modify MaxText to meet your needs.

Check out our [Read The Docs site](https://maxtext.readthedocs.io/en/latest/) or directly [Get Started](https://maxtext.readthedocs.io/en/latest/tutorials/first_run.html) with your first MaxText run. If you‚Äôre interested in Diffusion models (Wan 2.1, Flux, etc), see the [MaxDiffusion](https://github.com/AI-Hypercomputer/maxdiffusion) repository in our AI Hypercomputer GitHub organization. 

## Installation

See our installation guide to [install MaxText with pip](https://maxtext.readthedocs.io/en/latest/guides/install_maxtext.html).

## üî• Latest news üî•

* \[September 26, 2025\] Vocabulary tiling ([PR](https://github.com/AI-Hypercomputer/maxtext/pull/2242)) is now supported in MaxText! Adjust config `num_vocab_tiling` to unlock more efficient memory usage.
* \[September 24, 2025\] The GPT-OSS family of models (20B, 120B) is now supported.
* \[September 15, 2025\] MaxText is now available as a [PyPI package](https://pypi.org/project/maxtext). Users can now [install maxtext through pip](https://maxtext.readthedocs.io/en/latest/guides/install_maxtext.html).
* \[September 5, 2025\] MaxText has moved to an `src` layout as part of [RESTRUCTURE.md](RESTRUCTURE.md). For existing environments, please run `pip install -e .` from MaxText root.
* \[August 13, 2025\] The Qwen3 2507 MoE family of models is now supported: MoEs: 235B Thinking &amp; 280B Coder as well as existing dense models: 0.6B, 4B, 8B, 14B, and 32B.  
* \[July 27, 2025\] Updated TFLOPS/s calculation ([PR](https://github.com/AI-Hypercomputer/maxtext/pull/1988)) to account for causal attention, dividing the attention flops in half. Accounted for sliding window and chunked attention reduced attention flops in [PR](https://github.com/AI-Hypercomputer/maxtext/pull/2009) and [PR](https://github.com/AI-Hypercomputer/maxtext/pull/2030). Changes impact large sequence configs, as explained in this [doc](https://github.com/AI-Hypercomputer/maxtext/blob/main/docs/guides/performance_metrics.md)  
* \[July 16, 2025\] We will be restructuring the MaxText repository for improved organization and clarity. Please review the [proposed structure](https://github.com/AI-Hypercomputer/maxtext/blob/main/RESTRUCTURE.md) and provide feedback.  
* \[July 11, 2025\] Multi-Token Prediction (MTP) training support\! Adds an auxiliary loss based on predicting multiple future tokens, inspired by [DeepSeek-V3 paper](https://arxiv.org/html/2412.19437v1), to enhance training efficiency.  
* \[June 25, 2025\] DeepSeek R1-0528 variant is now supported.  
* \[April 24, 2025\] Llama 4 Maverick models are now supported.

## Use cases

MaxText provides a library of models and demonstrates how to perform pre-training or post-training with high performance and scale. 

MaxText leverages [JAX AI libraries](https://docs.jaxstack.ai/en/latest/getting_started.html) and presents a cohesive and comprehensive demonstration of training at scale by using [Flax](https://flax.readthedocs.io/en/latest/) (neural networks), [Tunix](https://github.com/google/tunix) (post-training), [Orbax](https://orbax.readthedocs.io/en/latest/) (checkpointing), [Optax](https://optax.readthedocs.io/en/latest/) (optimization), and [Grain](https://google-grain.readthedocs.io/en/latest/) (dataloading).

In addition to pure text-based LLMs, we also support multi-modal training with Gemma 3 and Llama 4 VLMs.

### Pre-training

If you‚Äôre building models from scratch, MaxText can serve as a reference implementation for experimentation, ideation, and inspiration \- just fork and modify MaxText to train your model, whether it‚Äôs a small dense model like Llama 8B, or a large MoE like DeepSeek-V3. Experiment with configs and model design to build the most efficient model on TPU or GPU. 

MaxText provides opinionated implementations for how to achieve optimal performance across a wide variety of dimensions like sharding, quantization, and checkpointing. 

### Post-training

If you are post-training a model, whether it is proprietary or open source, MaxText provides a scalable framework using Tunix. For RL (like GRPO), we leverage vLLM for sampling and Pathways (soon) for multi-host. 

Our goal is to provide a variety of models (dimension ‚Äúa‚Äù) and techniques (dimension ‚Äúb‚Äù), so you can easily explore (a) \* (b) combinations and efficiently train the perfect model for your use case.

Check out these getting started guides:

* [SFT](https://github.com/AI-Hypercomputer/maxtext/blob/main/end_to_end/tpu/llama3.1/8b/run_sft.sh) (Supervised Fine Tuning)  
* [GRPO](https://maxtext.readthedocs.io/en/latest/tutorials/grpo.html) (Group Relative Policy Optimization)

### Model library

MaxText aims to provide you with the best OSS models, whether as a reference implementation, or to post-train and then serve with vLLM. 

**Supported JAX models in MaxText**

* Google  
  * Gemma 3 (4B, 12B, 27B)  
  * Gemma 2 (2B, 9B, 27B)  
  * Gemma 1 (2B, 7B)  
* Alibaba  
  * Qwen 3 MoE 2507 (235B, 480B)  
  * Qwen 3 MoE (30B, 235B)  
  * Qwen 3 Dense (0.6B, 1.7B, 4B, 8B, 14B, 32B)  
* DeepSeek  
  * DeepSeek-V3 0324 (671B) &amp; DeepSeek-R1 0528 (671B)
  * DeepSeek-V2 (16B, 236B)  
* Meta  
  * Llama 4 Scout (109B) &amp; Maverick (400B)  
  * Llama 3.3 70B, 3.1 (8B, 70B, 405B), 3.0 (8B, 70B, 405B)  
  * Llama 2 (7B, 13B, 70B)  
* Open AI  
  * GPT-OSS (20B, 120B)
  * GPT3 (52K, 6B, 22B, 175B)  
* Mistral  
  * Mixtral (8x7B, 8x22B)  
  * Mistral (7B)  
* Diffusion Models  
  * See [MaxDiffusion](https://github.com/AI-Hypercomputer/maxdiffusion) (LTXV, Wan 2.1, Flux, SDXL, etc)

## Get involved

Please join our [Discord Channel](https://discord.com/invite/2H9PhvTcDU) and if you have feedback, you can file a feature request, documentation request, or bug report [here](https://github.com/AI-Hypercomputer/maxtext/issues/new/choose).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[huggingface/transformers]]></title>
            <link>https://github.com/huggingface/transformers</link>
            <guid>https://github.com/huggingface/transformers</guid>
            <pubDate>Mon, 10 Nov 2025 00:04:28 GMT</pubDate>
            <description><![CDATA[ü§ó Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/huggingface/transformers">huggingface/transformers</a></h1>
            <p>ü§ó Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training.</p>
            <p>Language: Python</p>
            <p>Stars: 152,302</p>
            <p>Forks: 31,097</p>
            <p>Stars today: 38 stars today</p>
            <h2>README</h2><pre>&lt;!---
Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
--&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg&quot;&gt;
    &lt;img alt=&quot;Hugging Face Transformers Library&quot; src=&quot;https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg&quot; width=&quot;352&quot; height=&quot;59&quot; style=&quot;max-width: 100%;&quot;&gt;
  &lt;/picture&gt;
  &lt;br/&gt;
  &lt;br/&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://huggingface.com/models&quot;&gt;&lt;img alt=&quot;Checkpoints on Hub&quot; src=&quot;https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&amp;color=brightgreen&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://circleci.com/gh/huggingface/transformers&quot;&gt;&lt;img alt=&quot;Build&quot; src=&quot;https://img.shields.io/circleci/build/github/huggingface/transformers/main&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/LICENSE&quot;&gt;&lt;img alt=&quot;GitHub&quot; src=&quot;https://img.shields.io/github/license/huggingface/transformers.svg?color=blue&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://huggingface.co/docs/transformers/index&quot;&gt;&lt;img alt=&quot;Documentation&quot; src=&quot;https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&amp;down_message=offline&amp;up_message=online&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/huggingface/transformers/releases&quot;&gt;&lt;img alt=&quot;GitHub release&quot; src=&quot;https://img.shields.io/github/release/huggingface/transformers.svg&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md&quot;&gt;&lt;img alt=&quot;Contributor Covenant&quot; src=&quot;https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://zenodo.org/badge/latestdoi/155220641&quot;&gt;&lt;img src=&quot;https://zenodo.org/badge/155220641.svg&quot; alt=&quot;DOI&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;h4 align=&quot;center&quot;&gt;
    &lt;p&gt;
        &lt;b&gt;English&lt;/b&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md&quot;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md&quot;&gt;ÁπÅÈ´î‰∏≠Êñá&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md&quot;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_es.md&quot;&gt;Espa√±ol&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md&quot;&gt;‡§π‡§ø‡§®‡•ç‡§¶‡•Ä&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md&quot;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md&quot;&gt;Portugu√™s&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_te.md&quot;&gt;‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md&quot;&gt;Fran√ßais&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_de.md&quot;&gt;Deutsch&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_it.md&quot;&gt;Italiano&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md&quot;&gt;Ti·∫øng Vi·ªát&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md&quot;&gt;ÿßŸÑÿπÿ±ÿ®Ÿäÿ©&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md&quot;&gt;ÿßÿ±ÿØŸà&lt;/a&gt; |
        &lt;a href=&quot;https://github.com/huggingface/transformers/blob/main/i18n/README_bn.md&quot;&gt;‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ&lt;/a&gt; |
    &lt;/p&gt;
&lt;/h4&gt;

&lt;h3 align=&quot;center&quot;&gt;
    &lt;p&gt;State-of-the-art pretrained models for inference and training&lt;/p&gt;
&lt;/h3&gt;

&lt;h3 align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_as_a_model_definition.png&quot;/&gt;
&lt;/h3&gt;

Transformers acts as the model-definition framework for state-of-the-art machine learning with text, computer
vision, audio, video, and multimodal models, for both inference and training.

It centralizes the model definition so that this definition is agreed upon across the ecosystem. `transformers` is the
pivot across frameworks: if a model definition is supported, it will be compatible with the majority of training
frameworks (Axolotl, Unsloth, DeepSpeed, FSDP, PyTorch-Lightning, ...), inference engines (vLLM, SGLang, TGI, ...),
and adjacent modeling libraries (llama.cpp, mlx, ...) which leverage the model definition from `transformers`.

We pledge to help support new state-of-the-art models and democratize their usage by having their model definition be
simple, customizable, and efficient.

There are over 1M+ Transformers [model checkpoints](https://huggingface.co/models?library=transformers&amp;sort=trending) on the [Hugging Face Hub](https://huggingface.com/models) you can use.

Explore the [Hub](https://huggingface.com/) today to find a model and use Transformers to help you get started right away.

## Installation

Transformers works with Python 3.9+, and [PyTorch](https://pytorch.org/get-started/locally/) 2.1+.

Create and activate a virtual environment with [venv](https://docs.python.org/3/library/venv.html) or [uv](https://docs.astral.sh/uv/), a fast Rust-based Python package and project manager.

```py
# venv
python -m venv .my-env
source .my-env/bin/activate
# uv
uv venv .my-env
source .my-env/bin/activate
```

Install Transformers in your virtual environment.

```py
# pip
pip install &quot;transformers[torch]&quot;

# uv
uv pip install &quot;transformers[torch]&quot;
```

Install Transformers from source if you want the latest changes in the library or are interested in contributing. However, the *latest* version may not be stable. Feel free to open an [issue](https://github.com/huggingface/transformers/issues) if you encounter an error.

```shell
git clone https://github.com/huggingface/transformers.git
cd transformers

# pip
pip install &#039;.[torch]&#039;

# uv
uv pip install &#039;.[torch]&#039;
```

## Quickstart

Get started with Transformers right away with the [Pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial) API. The `Pipeline` is a high-level inference class that supports text, audio, vision, and multimodal tasks. It handles preprocessing the input and returns the appropriate output.

Instantiate a pipeline and specify model to use for text generation. The model is downloaded and cached so you can easily reuse it again. Finally, pass some text to prompt the model.

```py
from transformers import pipeline

pipeline = pipeline(task=&quot;text-generation&quot;, model=&quot;Qwen/Qwen2.5-1.5B&quot;)
pipeline(&quot;the secret to baking a really good cake is &quot;)
[{&#039;generated_text&#039;: &#039;the secret to baking a really good cake is 1) to use the right ingredients and 2) to follow the recipe exactly. the recipe for the cake is as follows: 1 cup of sugar, 1 cup of flour, 1 cup of milk, 1 cup of butter, 1 cup of eggs, 1 cup of chocolate chips. if you want to make 2 cakes, how much sugar do you need? To make 2 cakes, you will need 2 cups of sugar.&#039;}]
```

To chat with a model, the usage pattern is the same. The only difference is you need to construct a chat history (the input to `Pipeline`) between you and the system.

&gt; [!TIP]
&gt; You can also chat with a model directly from the command line.
&gt; ```shell
&gt; transformers chat Qwen/Qwen2.5-0.5B-Instruct
&gt; ```

```py
import torch
from transformers import pipeline

chat = [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986.&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hey, can you tell me any fun things to do in New York?&quot;}
]

pipeline = pipeline(task=&quot;text-generation&quot;, model=&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;, dtype=torch.bfloat16, device_map=&quot;auto&quot;)
response = pipeline(chat, max_new_tokens=512)
print(response[0][&quot;generated_text&quot;][-1][&quot;content&quot;])
```

Expand the examples below to see how `Pipeline` works for different modalities and tasks.

&lt;details&gt;
&lt;summary&gt;Automatic speech recognition&lt;/summary&gt;

```py
from transformers import pipeline

pipeline = pipeline(task=&quot;automatic-speech-recognition&quot;, model=&quot;openai/whisper-large-v3&quot;)
pipeline(&quot;https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac&quot;)
{&#039;text&#039;: &#039; I have a dream that one day this nation will rise up and live out the true meaning of its creed.&#039;}
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Image classification&lt;/summary&gt;

&lt;h3 align=&quot;center&quot;&gt;
    &lt;a&gt;&lt;img src=&quot;https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png&quot;&gt;&lt;/a&gt;
&lt;/h3&gt;

```py
from transformers import pipeline

pipeline = pipeline(task=&quot;image-classification&quot;, model=&quot;facebook/dinov2-small-imagenet1k-1-layer&quot;)
pipeline(&quot;https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png&quot;)
[{&#039;label&#039;: &#039;macaw&#039;, &#039;score&#039;: 0.997848391532898},
 {&#039;label&#039;: &#039;sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita&#039;,
  &#039;score&#039;: 0.0016551691805943847},
 {&#039;label&#039;: &#039;lorikeet&#039;, &#039;score&#039;: 0.00018523589824326336},
 {&#039;label&#039;: &#039;African grey, African gray, Psittacus erithacus&#039;,
  &#039;score&#039;: 7.85409429227002e-05},
 {&#039;label&#039;: &#039;quail&#039;, &#039;score&#039;: 5.502637941390276e-05}]
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Visual question answering&lt;/summary&gt;

&lt;h3 align=&quot;center&quot;&gt;
    &lt;a&gt;&lt;img src=&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg&quot;&gt;&lt;/a&gt;
&lt;/h3&gt;

```py
from transformers import pipeline

pipeline = pipeline(task=&quot;visual-question-answering&quot;, model=&quot;Salesforce/blip-vqa-base&quot;)
pipeline(
    image=&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg&quot;,
    question=&quot;What is in the image?&quot;,
)
[{&#039;answer&#039;: &#039;statue of liberty&#039;}]
```

&lt;/details&gt;

## Why should I use Transformers?

1. Easy-to-use state-of-the-art models:
    - High performance on natural language understanding &amp; generation, computer vision, audio, video, and multimodal tasks.
    - Low barrier to entry for researchers, engineers, and developers.
    - Few user-facing abstractions with just three classes to learn.
    - A unified API for using all our pretrained models.

1. Lower compute costs, smaller carbon footprint:
    - Share trained models instead of training from scratch.
    - Reduce compute time and production costs.
    - Dozens of model architectures with 1M+ pretrained checkpoints across all modalities.

1. Choose the right framework for every part of a models lifetime:
    - Train state-of-the-art models in 3 lines of code.
    - Move a single model between PyTorch/JAX/TF2.0 frameworks at will.
    - Pick the right framework for training, evaluation, and production.

1. Easily customize a model or an example to your needs:
    - We provide examples for each architecture to reproduce the results published by its original authors.
    - Model internals are exposed as consistently as possible.
    - Model files can be used independently of the library for quick experiments.

&lt;a target=&quot;_blank&quot; href=&quot;https://huggingface.co/enterprise&quot;&gt;
    &lt;img alt=&quot;Hugging Face Enterprise Hub&quot; src=&quot;https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925&quot;&gt;
&lt;/a&gt;&lt;br&gt;

## Why shouldn&#039;t I use Transformers?

- This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.
- The training API is optimized to work with PyTorch models provided by Transformers. For generic machine learning loops, you should use another library like [Accelerate](https://huggingface.co/docs/accelerate).
- The [example scripts](https://github.com/huggingface/transformers/tree/main/examples) are only *examples*. They may not necessarily work out-of-the-box on your specific use case and you&#039;ll need to adapt the code for it to work.

## 100 projects using Transformers

Transformers is more than a toolkit to use pretrained models, it&#039;s a community of projects built around it and the
Hugging Face Hub. We want Transformers to enable developers, researchers, students, professors, engineers, and anyone
else to build their dream projects.

In order to celebrate Transformers 100,000 stars, we wanted to put the spotlight on the
community with the [awesome-transformers](./awesome-transformers.md) page which lists 100
incredible projects built with Transformers.

If you own or use a project that you believe should be part of the list, please open a PR to add it!

## Example models

You can test most of our models directly on their [Hub model pages](https://huggingface.co/models).

Expand each modality below to see a few example models for various use cases.

&lt;details&gt;
&lt;summary&gt;Audio&lt;/summary&gt;

- Audio classification with [Whisper](https://huggingface.co/openai/whisper-large-v3-turbo)
- Automatic speech recognition with [Moonshine](https://huggingface.co/UsefulSensors/moonshine)
- Keyword spotting with [Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)
- Speech to speech generation with [Moshi](https://huggingface.co/kyutai/moshiko-pytorch-bf16)
- Text to audio with [MusicGen](https://huggingface.co/facebook/musicgen-large)
- Text to speech with [Bark](https://huggingface.co/suno/bark)

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Computer vision&lt;/summary&gt;

- Automatic mask generation with [SAM](https://huggingface.co/facebook/sam-vit-base)
- Depth estimation with [DepthPro](https://huggingface.co/apple/DepthPro-hf)
- Image classification with [DINO v2](https://huggingface.co/facebook/dinov2-base)
- Keypoint detection with [SuperPoint](https://huggingface.co/magic-leap-community/superpoint)
- Keypoint matching with [SuperGlue](https://huggingface.co/magic-leap-community/superglue_outdoor)
- Object detection with [RT-DETRv2](https://huggingface.co/PekingU/rtdetr_v2_r50vd)
- Pose Estimation with [VitPose](https://huggingface.co/usyd-community/vitpose-base-simple)
- Universal segmentation with [OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_swin_large)
- Video classification with [VideoMAE](https://huggingface.co/MCG-NJU/videomae-large)

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Multimodal&lt;/summary&gt;

- Audio or text to text with [Qwen2-Audio](https://huggingface.co/Qwen/Qwen2-Audio-7B)
- Document question answering with [LayoutLMv3](https://huggingface.co/microsoft/layoutlmv3-base)
- Image or text to text with [Qwen-VL](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)
- Image captioning [BLIP-2](https://huggingface.co/Salesforce/blip2-opt-2.7b)
- OCR-based document understanding with [GOT-OCR2](https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf)
- Table question answering with [TAPAS](https://huggingface.co/google/tapas-base)
- Unified multimodal understanding and generation with [Emu3](https://huggingface.co/BAAI/Emu3-Gen)
- Vision to text with [Llava-OneVision](https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf)
- Visual question answering with [Llava](https://huggingface.co/llava-hf/llava-1.5-7b-hf)
- Visual referring expression segmentation with [Kosmos-2](https://huggingface.co/microsoft/kosmos-2-patch14-224)

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;NLP&lt;/summary&gt;

- Masked word completion with [ModernBERT](https://huggingface.co/answerdotai/ModernBERT-base)
- Named entity recognition with [Gemma](https://huggingface.co/google/gemma-2-2b)
- Question answering with [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)
- Summarization with [BART](https://huggingface.co/facebook/bart-large-cnn)
- Translation with [T5](https://huggingface.co/google-t5/t5-base)
- Text generation with [Llama](https://huggingface.co/meta-llama/Llama-3.2-1B)
- Text classification with [Qwen](https://huggingface.co/Qwen/Qwen2.5-0.5B)

&lt;/details&gt;

## Citation

We now have a [paper](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) you can cite for the ü§ó Transformers library:
```bibtex
@inproceedings{wolf-etal-2020-transformers,
    title = &quot;Transformers: State-of-the-Art Natural Language Processing&quot;,
    author = &quot;Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R√©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush&quot;,
    booktitle = &quot;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations&quot;,
    month = oct,
    year = &quot;2020&quot;,
    address = &quot;Online&quot;,
    publisher = &quot;Association for Computational Linguistics&quot;,
    url = &quot;https://www.aclweb.org/anthology/2020.emnlp-demos.6&quot;,
    pages = &quot;38--45&quot;
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mitmproxy/mitmproxy]]></title>
            <link>https://github.com/mitmproxy/mitmproxy</link>
            <guid>https://github.com/mitmproxy/mitmproxy</guid>
            <pubDate>Mon, 10 Nov 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[An interactive TLS-capable intercepting HTTP proxy for penetration testers and software developers.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mitmproxy/mitmproxy">mitmproxy/mitmproxy</a></h1>
            <p>An interactive TLS-capable intercepting HTTP proxy for penetration testers and software developers.</p>
            <p>Language: Python</p>
            <p>Stars: 41,164</p>
            <p>Forks: 4,357</p>
            <p>Stars today: 20 stars today</p>
            <h2>README</h2><pre># mitmproxy

[![Continuous Integration Status](https://github.com/mitmproxy/mitmproxy/actions/workflows/main.yml/badge.svg?branch=main)](https://github.com/mitmproxy/mitmproxy/actions?query=branch%3Amain)
[![Codacy Badge](https://app.codacy.com/project/badge/Grade/a38b0325dfb944839c0c8da354f70b1b)](https://app.codacy.com/gh/mitmproxy/mitmproxy/dashboard)
[![autofix.ci: enabled](https://shields.mitmproxy.org/badge/autofix.ci-yes-success?logo=data:image/svg+xml;base64,PHN2ZyBmaWxsPSIjZmZmIiB2aWV3Qm94PSIwIDAgMTI4IDEyOCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCB0cmFuc2Zvcm09InNjYWxlKDAuMDYxLC0wLjA2MSkgdHJhbnNsYXRlKC0yNTAsLTE3NTApIiBkPSJNMTMyNSAtMzQwcS0xMTUgMCAtMTY0LjUgMzIuNXQtNDkuNSAxMTQuNXEwIDMyIDUgNzAuNXQxMC41IDcyLjV0NS41IDU0djIyMHEtMzQgLTkgLTY5LjUgLTE0dC03MS41IC01cS0xMzYgMCAtMjUxLjUgNjJ0LTE5MSAxNjl0LTkyLjUgMjQxcS05MCAxMjAgLTkwIDI2NnEwIDEwOCA0OC41IDIwMC41dDEzMiAxNTUuNXQxODguNSA4MXExNSA5OSAxMDAuNSAxODAuNXQyMTcgMTMwLjV0MjgyLjUgNDlxMTM2IDAgMjU2LjUgLTQ2IHQyMDkgLTEyNy41dDEyOC41IC0xODkuNXExNDkgLTgyIDIyNyAtMjEzLjV0NzggLTI5OS41cTAgLTEzNiAtNTggLTI0NnQtMTY1LjUgLTE4NC41dC0yNTYuNSAtMTAzLjVsLTI0MyAtMzAwdi01MnEwIC0yNyAzLjUgLTU2LjV0Ni41IC01Ny41dDMgLTUycTAgLTg1IC00MS41IC0xMTguNXQtMTU3LjUgLTMzLjV6TTEzMjUgLTI2MHE3NyAwIDk4IDE0LjV0MjEgNTcuNXEwIDI5IC0zIDY4dC02LjUgNzN0LTMuNSA0OHY2NGwyMDcgMjQ5IHEtMzEgMCAtNjAgNS41dC01NCAxMi41bC0xMDQgLTEyM3EtMSAzNCAtMiA2My41dC0xIDU0LjVxMCA2OSA5IDEyM2wzMSAyMDBsLTExNSAtMjhsLTQ2IC0yNzFsLTIwNSAyMjZxLTE5IC0xNSAtNDMgLTI4LjV0LTU1IC0yNi41bDIxOSAtMjQydi0yNzZxMCAtMjAgLTUuNSAtNjB0LTEwLjUgLTc5dC01IC01OHEwIC00MCAzMCAtNTMuNXQxMDQgLTEzLjV6TTEyNjIgNjE2cS0xMTkgMCAtMjI5LjUgMzQuNXQtMTkzLjUgOTYuNWw0OCA2NCBxNzMgLTU1IDE3MC41IC04NXQyMDQuNSAtMzBxMTM3IDAgMjQ5IDQ1LjV0MTc5IDEyMXQ2NyAxNjUuNWg4MHEwIC0xMTQgLTc3LjUgLTIwNy41dC0yMDggLTE0OXQtMjg5LjUgLTU1LjV6TTgwMyA1OTVxODAgMCAxNDkgMjkuNXQxMDggNzIuNWwyMjEgLTY3bDMwOSA4NnE0NyAtMzIgMTA0LjUgLTUwdDExNy41IC0xOHE5MSAwIDE2NSAzOHQxMTguNSAxMDMuNXQ0NC41IDE0Ni41cTAgNzYgLTM0LjUgMTQ5dC05NS41IDEzNHQtMTQzIDk5IHEtMzcgMTA3IC0xMTUuNSAxODMuNXQtMTg2IDExNy41dC0yMzAuNSA0MXEtMTAzIDAgLTE5Ny41IC0yNnQtMTY5IC03Mi41dC0xMTcuNSAtMTA4dC00MyAtMTMxLjVxMCAtMzQgMTQuNSAtNjIuNXQ0MC41IC01MC41bC01NSAtNTlxLTM0IDI5IC01NCA2NS41dC0yNSA4MS41cS04MSAtMTggLTE0NSAtNzB0LTEwMSAtMTI1LjV0LTM3IC0xNTguNXEwIC0xMDIgNDguNSAtMTgwLjV0MTI5LjUgLTEyM3QxNzkgLTQ0LjV6Ii8+PC9zdmc+)](https://autofix.ci)
[![Coverage Status](https://shields.mitmproxy.org/codecov/c/github/mitmproxy/mitmproxy/main.svg?label=codecov)](https://codecov.io/gh/mitmproxy/mitmproxy)
[![Latest Version](https://shields.mitmproxy.org/pypi/v/mitmproxy.svg)](https://pypi.python.org/pypi/mitmproxy)
[![Supported Python versions](https://shields.mitmproxy.org/pypi/pyversions/mitmproxy.svg)](https://pypi.python.org/pypi/mitmproxy)

``mitmproxy`` is an interactive, SSL/TLS-capable intercepting proxy with a console
interface for HTTP/1, HTTP/2, and WebSockets.

``mitmdump`` is the command-line version of mitmproxy. Think tcpdump for HTTP.

``mitmweb`` is a web-based interface for mitmproxy.

## Installation

The installation instructions are [here](https://docs.mitmproxy.org/stable/overview-installation).
If you want to install from source, see [CONTRIBUTING.md](./CONTRIBUTING.md).

## Documentation &amp; Help

General information, tutorials, and precompiled binaries can be found on the mitmproxy website.

[![mitmproxy.org](https://shields.mitmproxy.org/badge/https%3A%2F%2F-mitmproxy.org-blue.svg)](https://mitmproxy.org/)

The documentation for mitmproxy is available on our website:

[![mitmproxy documentation stable](https://shields.mitmproxy.org/badge/docs-stable-brightgreen.svg)](https://docs.mitmproxy.org/stable/)
[![mitmproxy documentation dev](https://shields.mitmproxy.org/badge/docs-dev-brightgreen.svg)](https://docs.mitmproxy.org/dev/)

If you have questions on how to use mitmproxy, please
use GitHub Discussions!

[![mitmproxy discussions](https://shields.mitmproxy.org/badge/help-github%20discussions-orange.svg)](https://github.com/mitmproxy/mitmproxy/discussions)

## Contributing

As an open source project, mitmproxy welcomes contributions of all forms.

[![Dev Guide](https://shields.mitmproxy.org/badge/dev_docs-CONTRIBUTING.md-blue)](./CONTRIBUTING.md)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Olow304/memvid]]></title>
            <link>https://github.com/Olow304/memvid</link>
            <guid>https://github.com/Olow304/memvid</guid>
            <pubDate>Mon, 10 Nov 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[Video-based AI memory library. Store millions of text chunks in MP4 files with lightning-fast semantic search. No database needed.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Olow304/memvid">Olow304/memvid</a></h1>
            <p>Video-based AI memory library. Store millions of text chunks in MP4 files with lightning-fast semantic search. No database needed.</p>
            <p>Language: Python</p>
            <p>Stars: 10,343</p>
            <p>Forks: 878</p>
            <p>Stars today: 20 stars today</p>
            <h2>README</h2><pre>## What to expect in v2

&gt; **Early-access notice**  
&gt; Memvid v1 is still experimental. The file format and API may change until we lock in a stable release.
&gt; 
&gt; **Memvid v2 ‚Äì what&#039;s next**  
&gt; - **Living-Memory Engine** ‚Äì keep adding new data and let LLMs remember it across sessions.  
&gt; - **Capsule Context** ‚Äì shareable `.mv2` capsules, each with its own rules and expiry.  
&gt; - **Time-Travel Debugging** ‚Äì rewind or branch any chat to review or test.  
&gt; - **Smart Recall** ‚Äì local cache guesses what you‚Äôll need and loads it in under 5 ms.  
&gt; - **Codec Intelligence** ‚Äì auto-tunes AV1 now and future codecs later, so files keep shrinking.  
&gt; - **CLI &amp; Dashboard** ‚Äì simple tools for branching, analytics, and one-command cloud publish.  

Sneak peek of Memvid v2 - a living memory engine that can be used to chat with your knowledge base.
![Memvid v2 Preview](assets/mv2.png)


---

## Memvid v1



[![PyPI](https://img.shields.io/pypi/v/memvid)](https://pypi.org/project/memvid/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![GitHub Stars](https://img.shields.io/github/stars/olow304/memvid)](https://github.com/olow304/memvid)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

# Memvid - Turn millions of text chunks into a single, searchable video file

Memvid compresses an entire knowledge base into **MP4** files while keeping millisecond-level semantic search. Think of it as *SQLite for AI memory* portable, efficient, and self-contained. By encoding text as **QR codes in video frames**, we deliver **50-100√ó** smaller storage than vector databases with **zero infrastructure**.

---

## Why Video Compression Changes Everything üöÄ

| What it enables | How video codecs make it possible |
|---------|-------------------|
| **50-100√ó smaller storage** | Modern video codecs compress repetitive visual patterns (QR codes) far better than raw embeddings |
| **Sub-100ms retrieval** | Direct frame seek via index ‚Üí QR decode ‚Üí your text. No server round-trips |
| **Zero infrastructure** | Just Python and MP4 files-no DB clusters, no Docker, no ops |
| **True portability** | Copy or stream `memory.mp4`-it works anywhere video plays |
| **Offline-first design** | After encoding, everything runs without internet |

---

## Under the Hood - Memvid v1 üîç

1. **Text ‚Üí QR ‚Üí Frame**  
   Each text chunk becomes a QR code, packed into video frames. Modern codecs excel at compressing these repetitive patterns.

2. **Smart indexing**  
   Embeddings map queries ‚Üí frame numbers. One seek, one decode, millisecond results.

3. **Codec leverage**  
   30 years of video R&amp;D means your text gets compressed better than any custom algorithm could achieve.

4. **Future-proof**  
   Next-gen codecs (AV1, H.266) automatically make your memories smaller and faster-no code changes needed.

---

## Installation
```bash
pip install memvid
# For PDF support
pip install memvid PyPDF2
```

## Quick Start
```python
from memvid import MemvidEncoder, MemvidChat

# Create video memory from text
chunks = [&quot;NASA founded 1958&quot;, &quot;Apollo 11 landed 1969&quot;, &quot;ISS launched 1998&quot;]
encoder = MemvidEncoder()
encoder.add_chunks(chunks)
encoder.build_video(&quot;space.mp4&quot;, &quot;space_index.json&quot;)

# Chat with your memory
chat = MemvidChat(&quot;space.mp4&quot;, &quot;space_index.json&quot;)
response = chat.chat(&quot;When did humans land on the moon?&quot;)
print(response)  # References Apollo 11 in 1969
```

## Real-World Examples

### Documentation Assistant
```python
from memvid import MemvidEncoder
import os

encoder = MemvidEncoder(chunk_size=512)

# Index all markdown files
for file in os.listdir(&quot;docs&quot;):
    if file.endswith(&quot;.md&quot;):
        with open(f&quot;docs/{file}&quot;) as f:
            encoder.add_text(f.read(), metadata={&quot;file&quot;: file})

encoder.build_video(&quot;docs.mp4&quot;, &quot;docs_index.json&quot;)
```

### PDF Library Search
```python
# Index multiple PDFs
encoder = MemvidEncoder()
encoder.add_pdf(&quot;deep_learning.pdf&quot;)
encoder.add_pdf(&quot;machine_learning.pdf&quot;) 
encoder.build_video(&quot;ml_library.mp4&quot;, &quot;ml_index.json&quot;)

# Semantic search across all books
from memvid import MemvidRetriever
retriever = MemvidRetriever(&quot;ml_library.mp4&quot;, &quot;ml_index.json&quot;)
results = retriever.search(&quot;backpropagation&quot;, top_k=5)
```

### Interactive Web UI
```python
from memvid import MemvidInteractive

# Launch at http://localhost:7860
interactive = MemvidInteractive(&quot;knowledge.mp4&quot;, &quot;index.json&quot;)
interactive.run()
```

## Advanced Features

### Scale Optimization
```python
# Maximum compression for huge datasets
encoder.build_video(
    &quot;compressed.mp4&quot;,
    &quot;index.json&quot;, 
    fps=60,              # More frames/second
    frame_size=256,      # Smaller QR codes
    video_codec=&#039;h265&#039;,  # Better compression
    crf=28              # Quality tradeoff
)
```

### Custom Embeddings
```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer(&#039;all-mpnet-base-v2&#039;)
encoder = MemvidEncoder(embedding_model=model)
```

### Parallel Processing
```python
encoder = MemvidEncoder(n_workers=8)
encoder.add_chunks_parallel(million_chunks)
```

## CLI Usage
```bash
# Process documents
python examples/file_chat.py --input-dir /docs --provider openai

# Advanced codecs
python examples/file_chat.py --files doc.pdf --codec h265

# Load existing
python examples/file_chat.py --load-existing output/memory
```

## Performance

- **Indexing**: ~10K chunks/second on modern CPUs
- **Search**: &lt;100ms for 1M chunks (includes decode)
- **Storage**: 100MB text ‚Üí 1-2MB video
- **Memory**: Constant 500MB RAM regardless of size

## What&#039;s Coming in v2

- **Delta encoding**: Time-travel through knowledge versions
- **Streaming ingest**: Add to videos in real-time
- **Cloud dashboard**: Web UI with API management
- **Smart codecs**: Auto-select AV1/HEVC per content
- **GPU boost**: 100√ó faster bulk encoding

## Get Involved

Memvid is redefining AI memory. Join us:

- ‚≠ê Star on [GitHub](https://github.com/olow304/memvid)
- üêõ Report issues or request features
- üîß Submit PRs (we review quickly!)
- üí¨ Discuss video-based AI memory

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[fastapi/fastapi]]></title>
            <link>https://github.com/fastapi/fastapi</link>
            <guid>https://github.com/fastapi/fastapi</guid>
            <pubDate>Mon, 10 Nov 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[FastAPI framework, high performance, easy to learn, fast to code, ready for production]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/fastapi/fastapi">fastapi/fastapi</a></h1>
            <p>FastAPI framework, high performance, easy to learn, fast to code, ready for production</p>
            <p>Language: Python</p>
            <p>Stars: 91,713</p>
            <p>Forks: 8,195</p>
            <p>Stars today: 30 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://fastapi.tiangolo.com&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/logo-margin/logo-teal.png&quot; alt=&quot;FastAPI&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;em&gt;FastAPI framework, high performance, easy to learn, fast to code, ready for production&lt;/em&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/fastapi/fastapi/actions?query=workflow%3ATest+event%3Apush+branch%3Amaster&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://github.com/fastapi/fastapi/actions/workflows/test.yml/badge.svg?event=push&amp;branch=master&quot; alt=&quot;Test&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://coverage-badge.samuelcolvin.workers.dev/redirect/fastapi/fastapi&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://coverage-badge.samuelcolvin.workers.dev/fastapi/fastapi.svg&quot; alt=&quot;Coverage&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://pypi.org/project/fastapi&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/fastapi?color=%2334D058&amp;label=pypi%20package&quot; alt=&quot;Package version&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://pypi.org/project/fastapi&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/pyversions/fastapi.svg?color=%2334D058&quot; alt=&quot;Supported Python versions&quot;&gt;
&lt;/a&gt;
&lt;/p&gt;

---

**Documentation**: &lt;a href=&quot;https://fastapi.tiangolo.com&quot; target=&quot;_blank&quot;&gt;https://fastapi.tiangolo.com&lt;/a&gt;

**Source Code**: &lt;a href=&quot;https://github.com/fastapi/fastapi&quot; target=&quot;_blank&quot;&gt;https://github.com/fastapi/fastapi&lt;/a&gt;

---

FastAPI is a modern, fast (high-performance), web framework for building APIs with Python based on standard Python type hints.

The key features are:

* **Fast**: Very high performance, on par with **NodeJS** and **Go** (thanks to Starlette and Pydantic). [One of the fastest Python frameworks available](#performance).
* **Fast to code**: Increase the speed to develop features by about 200% to 300%. *
* **Fewer bugs**: Reduce about 40% of human (developer) induced errors. *
* **Intuitive**: Great editor support. &lt;abbr title=&quot;also known as auto-complete, autocompletion, IntelliSense&quot;&gt;Completion&lt;/abbr&gt; everywhere. Less time debugging.
* **Easy**: Designed to be easy to use and learn. Less time reading docs.
* **Short**: Minimize code duplication. Multiple features from each parameter declaration. Fewer bugs.
* **Robust**: Get production-ready code. With automatic interactive documentation.
* **Standards-based**: Based on (and fully compatible with) the open standards for APIs: &lt;a href=&quot;https://github.com/OAI/OpenAPI-Specification&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;OpenAPI&lt;/a&gt; (previously known as Swagger) and &lt;a href=&quot;https://json-schema.org/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;JSON Schema&lt;/a&gt;.

&lt;small&gt;* estimation based on tests on an internal development team, building production applications.&lt;/small&gt;

## Sponsors

&lt;!-- sponsors --&gt;

&lt;a href=&quot;https://blockbee.io?ref=fastapi&quot; target=&quot;_blank&quot; title=&quot;BlockBee Cryptocurrency Payment Gateway&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/blockbee.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/scalar/scalar/?utm_source=fastapi&amp;utm_medium=website&amp;utm_campaign=main-badge&quot; target=&quot;_blank&quot; title=&quot;Scalar: Beautiful Open-Source API References from Swagger/OpenAPI files&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/scalar.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.propelauth.com/?utm_source=fastapi&amp;utm_campaign=1223&amp;utm_medium=mainbadge&quot; target=&quot;_blank&quot; title=&quot;Auth, user management and more for your B2B product&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/propelauth.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://zuplo.link/fastapi-gh&quot; target=&quot;_blank&quot; title=&quot;Zuplo: Deploy, Secure, Document, and Monetize your FastAPI&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/zuplo.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://liblab.com?utm_source=fastapi&quot; target=&quot;_blank&quot; title=&quot;liblab - Generate SDKs from FastAPI&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/liblab.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://docs.render.com/deploy-fastapi?utm_source=deploydoc&amp;utm_medium=referral&amp;utm_campaign=fastapi&quot; target=&quot;_blank&quot; title=&quot;Deploy &amp; scale any full-stack web app on Render. Focus on building apps, not infra.&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/render.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.coderabbit.ai/?utm_source=fastapi&amp;utm_medium=badge&amp;utm_campaign=fastapi&quot; target=&quot;_blank&quot; title=&quot;Cut Code Review Time &amp; Bugs in Half with CodeRabbit&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/coderabbit.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://subtotal.com/?utm_source=fastapi&amp;utm_medium=sponsorship&amp;utm_campaign=open-source&quot; target=&quot;_blank&quot; title=&quot;The Gold Standard in Retail Account Linking&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/subtotal.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://docs.railway.com/guides/fastapi?utm_medium=integration&amp;utm_source=docs&amp;utm_campaign=fastapi&quot; target=&quot;_blank&quot; title=&quot;Deploy enterprise applications at startup speed&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/railway.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://serpapi.com/?utm_source=fastapi_website&quot; target=&quot;_blank&quot; title=&quot;SerpApi: Web Search API&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/serpapi.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://databento.com/?utm_source=fastapi&amp;utm_medium=sponsor&amp;utm_content=display&quot; target=&quot;_blank&quot; title=&quot;Pay as you go for market data&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/databento.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://speakeasy.com/editor?utm_source=fastapi+repo&amp;utm_medium=github+sponsorship&quot; target=&quot;_blank&quot; title=&quot;SDKs for your API | Speakeasy&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/speakeasy.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.svix.com/&quot; target=&quot;_blank&quot; title=&quot;Svix - Webhooks as a service&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/svix.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.stainlessapi.com/?utm_source=fastapi&amp;utm_medium=referral&quot; target=&quot;_blank&quot; title=&quot;Stainless | Generate best-in-class SDKs&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/stainless.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.permit.io/blog/implement-authorization-in-fastapi?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=fastapi&quot; target=&quot;_blank&quot; title=&quot;Fine-Grained Authorization for FastAPI&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/permit.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.interviewpal.com/?utm_source=fastapi&amp;utm_medium=open-source&amp;utm_campaign=dev-hiring&quot; target=&quot;_blank&quot; title=&quot;InterviewPal - AI Interview Coach for Engineers and Devs&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/interviewpal.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://dribia.com/en/&quot; target=&quot;_blank&quot; title=&quot;Dribia - Data Science within your reach&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/dribia.png&quot;&gt;&lt;/a&gt;

&lt;!-- /sponsors --&gt;

&lt;a href=&quot;https://fastapi.tiangolo.com/fastapi-people/#sponsors&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Other sponsors&lt;/a&gt;

## Opinions

&quot;_[...] I&#039;m using **FastAPI** a ton these days. [...] I&#039;m actually planning to use it for all of my team&#039;s **ML services at Microsoft**. Some of them are getting integrated into the core **Windows** product and some **Office** products._&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Kabir Khan - &lt;strong&gt;Microsoft&lt;/strong&gt; &lt;a href=&quot;https://github.com/fastapi/fastapi/pull/26&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_We adopted the **FastAPI** library to spawn a **REST** server that can be queried to obtain **predictions**. [for Ludwig]_&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Piero Molino, Yaroslav Dudin, and Sai Sumanth Miryala - &lt;strong&gt;Uber&lt;/strong&gt; &lt;a href=&quot;https://eng.uber.com/ludwig-v0-2/&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_**Netflix** is pleased to announce the open-source release of our **crisis management** orchestration framework: **Dispatch**! [built with **FastAPI**]_&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Kevin Glisson, Marc Vilanova, Forest Monsen - &lt;strong&gt;Netflix&lt;/strong&gt; &lt;a href=&quot;https://netflixtechblog.com/introducing-dispatch-da4b8a2a8072&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_I‚Äôm over the moon excited about **FastAPI**. It‚Äôs so fun!_&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Brian Okken - &lt;strong&gt;&lt;a href=&quot;https://pythonbytes.fm/episodes/show/123/time-to-right-the-py-wrongs?time_in_sec=855&quot; target=&quot;_blank&quot;&gt;Python Bytes&lt;/a&gt; podcast host&lt;/strong&gt; &lt;a href=&quot;https://x.com/brianokken/status/1112220079972728832&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_Honestly, what you&#039;ve built looks super solid and polished. In many ways, it&#039;s what I wanted **Hug** to be - it&#039;s really inspiring to see someone build that._&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Timothy Crosley - &lt;strong&gt;&lt;a href=&quot;https://github.com/hugapi/hug&quot; target=&quot;_blank&quot;&gt;Hug&lt;/a&gt; creator&lt;/strong&gt; &lt;a href=&quot;https://news.ycombinator.com/item?id=19455465&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_If you&#039;re looking to learn one **modern framework** for building REST APIs, check out **FastAPI** [...] It&#039;s fast, easy to use and easy to learn [...]_&quot;

&quot;_We&#039;ve switched over to **FastAPI** for our **APIs** [...] I think you&#039;ll like it [...]_&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Ines Montani - Matthew Honnibal - &lt;strong&gt;&lt;a href=&quot;https://explosion.ai&quot; target=&quot;_blank&quot;&gt;Explosion AI&lt;/a&gt; founders - &lt;a href=&quot;https://spacy.io&quot; target=&quot;_blank&quot;&gt;spaCy&lt;/a&gt; creators&lt;/strong&gt; &lt;a href=&quot;https://x.com/_inesmontani/status/1144173225322143744&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt; - &lt;a href=&quot;https://x.com/honnibal/status/1144031421859655680&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_If anyone is looking to build a production Python API, I would highly recommend **FastAPI**. It is **beautifully designed**, **simple to use** and **highly scalable**, it has become a **key component** in our API first development strategy and is driving many automations and services such as our Virtual TAC Engineer._&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Deon Pillsbury - &lt;strong&gt;Cisco&lt;/strong&gt; &lt;a href=&quot;https://www.linkedin.com/posts/deonpillsbury_cisco-cx-python-activity-6963242628536487936-trAp/&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

## **Typer**, the FastAPI of CLIs

&lt;a href=&quot;https://typer.tiangolo.com&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://typer.tiangolo.com/img/logo-margin/logo-margin-vector.svg&quot; style=&quot;width: 20%;&quot;&gt;&lt;/a&gt;

If you are building a &lt;abbr title=&quot;Command Line Interface&quot;&gt;CLI&lt;/abbr&gt; app to be used in the terminal instead of a web API, check out &lt;a href=&quot;https://typer.tiangolo.com/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;**Typer**&lt;/a&gt;.

**Typer** is FastAPI&#039;s little sibling. And it&#039;s intended to be the **FastAPI of CLIs**. ‚å®Ô∏è üöÄ

## Requirements

FastAPI stands on the shoulders of giants:

* &lt;a href=&quot;https://www.starlette.dev/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Starlette&lt;/a&gt; for the web parts.
* &lt;a href=&quot;https://docs.pydantic.dev/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Pydantic&lt;/a&gt; for the data parts.

## Installation

Create and activate a &lt;a href=&quot;https://fastapi.tiangolo.com/virtual-environments/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;virtual environment&lt;/a&gt; and then install FastAPI:

&lt;div class=&quot;termy&quot;&gt;

```console
$ pip install &quot;fastapi[standard]&quot;

---&gt; 100%
```

&lt;/div&gt;

**Note**: Make sure you put `&quot;fastapi[standard]&quot;` in quotes to ensure it works in all terminals.

## Example

### Create it

Create a file `main.py` with:

```Python
from typing import Union

from fastapi import FastAPI

app = FastAPI()


@app.get(&quot;/&quot;)
def read_root():
    return {&quot;Hello&quot;: &quot;World&quot;}


@app.get(&quot;/items/{item_id}&quot;)
def read_item(item_id: int, q: Union[str, None] = None):
    return {&quot;item_id&quot;: item_id, &quot;q&quot;: q}
```

&lt;details markdown=&quot;1&quot;&gt;
&lt;summary&gt;Or use &lt;code&gt;async def&lt;/code&gt;...&lt;/summary&gt;

If your code uses `async` / `await`, use `async def`:

```Python hl_lines=&quot;9  14&quot;
from typing import Union

from fastapi import FastAPI

app = FastAPI()


@app.get(&quot;/&quot;)
async def read_root():
    return {&quot;Hello&quot;: &quot;World&quot;}


@app.get(&quot;/items/{item_id}&quot;)
async def read_item(item_id: int, q: Union[str, None] = None):
    return {&quot;item_id&quot;: item_id, &quot;q&quot;: q}
```

**Note**:

If you don&#039;t know, check the _&quot;In a hurry?&quot;_ section about &lt;a href=&quot;https://fastapi.tiangolo.com/async/#in-a-hurry&quot; target=&quot;_blank&quot;&gt;`async` and `await` in the docs&lt;/a&gt;.

&lt;/details&gt;

### Run it

Run the server with:

&lt;div class=&quot;termy&quot;&gt;

```console
$ fastapi dev main.py

 ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FastAPI CLI - Development mode ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
 ‚îÇ                                                     ‚îÇ
 ‚îÇ  Serving at: http://127.0.0.1:8000                  ‚îÇ
 ‚îÇ                                                     ‚îÇ
 ‚îÇ  API docs: http://127.0.0.1:8000/docs               ‚îÇ
 ‚îÇ                                                     ‚îÇ
 ‚îÇ  Running in development mode, for production use:   ‚îÇ
 ‚îÇ                                                     ‚îÇ
 ‚îÇ  fastapi run                                        ‚îÇ
 ‚îÇ                                                     ‚îÇ
 ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ

INFO:     Will watch for changes in these directories: [&#039;/home/user/code/awesomeapp&#039;]
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [2248755] using WatchFiles
INFO:     Started server process [2248757]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
```

&lt;/div&gt;

&lt;details markdown=&quot;1&quot;&gt;
&lt;summary&gt;About the command &lt;code&gt;fastapi dev main.py&lt;/code&gt;...&lt;/summary&gt;

The command `fastapi dev` reads your `main.py` file, detects the **FastAPI** app in it, and starts a server using &lt;a href=&quot;https://www.uvicorn.dev&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Uvicorn&lt;/a&gt;.

By default, `fastapi dev` will start with auto-reload enabled for local development.

You can read more about it in the &lt;a href=&quot;https://fastapi.tiangolo.com/fastapi-cli/&quot; target=&quot;_blank&quot;&gt;FastAPI CLI docs&lt;/a&gt;.

&lt;/details&gt;

### Check it

Open your browser at &lt;a href=&quot;http://127.0.0.1:8000/items/5?q=somequery&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/items/5?q=somequery&lt;/a&gt;.

You will see the JSON response as:

```JSON
{&quot;item_id&quot;: 5, &quot;q&quot;: &quot;somequery&quot;}
```

You already created an API that:

* Receives HTTP requests in the _paths_ `/` and `/items/{item_id}`.
* Both _paths_ take `GET` &lt;em&gt;operations&lt;/em&gt; (also known as HTTP _methods_).
* The _path_ `/items/{item_id}` has a _path parameter_ `item_id` that should be an `int`.
* The _path_ `/items/{item_id}` has an optional `str` _query parameter_ `q`.

### Interactive API docs

Now go to &lt;a href=&quot;http://127.0.0.1:8000/docs&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/docs&lt;/a&gt;.

You will see the automatic interactive API documentation (provided by &lt;a href=&quot;https://github.com/swagger-api/swagger-ui&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Swagger UI&lt;/a&gt;):

![Swagger UI](https://fastapi.tiangolo.com/img/index/index-01-swagger-ui-simple.png)

### Alternative API docs

And now, go to &lt;a href=&quot;http://127.0.0.1:8000/redoc&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/redoc&lt;/a&gt;.

You will see the alternative automatic documentation (provided by &lt;a href=&quot;https://github.com/Rebilly/ReDoc&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;ReDoc&lt;/a&gt;):

![ReDoc](https://fastapi.tiangolo.com/img/index/index-02-redoc-simple.png)

## Example upgrade

Now modify the file `main.py` to receive a body from a `PUT` request.

Declare the body using standard Python types, thanks to Pydantic.

```Python hl_lines=&quot;4  9-12  25-27&quot;
from typing import Union

from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()


class Item(BaseModel):
    name: str
    price: float
    is_offer: Union[bool, None] = None


@app.get(&quot;/&quot;)
def read_root():
    return {&quot;Hello&quot;: &quot;World&quot;}


@app.get(&quot;/items/{item_id}&quot;)
def read_item(item_id: int, q: Union[str, None] = None):
    return {&quot;item_id&quot;: item_id, &quot;q&quot;: q}


@app.put(&quot;/items/{item_id}&quot;)
def update_item(item_id: int, item: Item):
    return {&quot;item_name&quot;: item.name, &quot;item_id&quot;: item_id}
```

The `fastapi dev` server should reload automatically.

### Interactive API docs upgrade

Now go to &lt;a href=&quot;http://127.0.0.1:8000/docs&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/docs&lt;/a&gt;.

* The interactive API documentation will be automatically updated, including the new body:

![Swagger UI](https://fastapi.tiangolo.com/img/index/index-03-swagger-02.png)

* Click on the button &quot;Try it out&quot;, it allows you to fill the parameters and directly interact with the API:

![Swagger UI interaction](https://fastapi.tiangolo.com/img/index/index-04-swagger-03.png)

* Then click on the &quot;Execute&quot; button, the user interface will communicate with your API, send the parameters, get the results and show them on the screen:

![Swagger UI interaction](https://fastapi.tiangolo.com/img/index/index-05-swagger-04.png)

### Alternative API docs upgrade

And now, go to &lt;a href=&quot;http://127.0.0.1:8000/redoc&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/redoc&lt;/a&gt;.

* The alternative documentation will also reflect the new query parameter and body:

![ReDoc](https://fastapi.tiangolo.com/img/index/index-06-redoc-02.png)

### Recap

In summary, you declare **once** the types of parameters, body, etc. as function parameters.

You do that with standard modern Python types.

You don&#039;t have to learn a new syntax, the methods or classes of a specific library, etc.

Just standard **Python**.

For example, for an `int`:

```Python
item_id: int
```

or for a more complex `Item` model:

```Python
item: Item
```

...and with that single declaration you get:

* Editor support, including:
    * Completion.
    * Type checks.
* Validation of data:
    * Automatic and clear errors when the data is invalid.
    * Validation even for deeply nested JSON objects.
* &lt;abbr title=&quot;also known as: serialization, parsing, marshalling&quot;&gt;Conversion&lt;/abbr&gt; of input data: coming from the network to Python data and types. Reading from:
    * JSON.
    * Path parameters.
    * Query parameters.
    * Cookies.
    * Headers.
    * Forms.
    * Files.
* &lt;abbr title=&quot;also known as: serialization, parsing, marshalling&quot;&gt;Conversion&lt;/abbr&gt; of output data: converting from Python data and types to network data (as JSON):
    * Convert Python types (`str`, `int`, `float`, `bool`, `list`, etc).
    * `datetime` objects.
    * `UUID` objects.
    * Database models.
    * ...and many more.
* Automatic interactive API documentation, including 2 alternative user interfaces:
    * Swagger UI.
    * ReDoc.

---

Coming back to the previous code example, **FastAPI** will:

* Validate that there is an `item_id` in the path for `GET` and `PUT` requests.
* Validate that the `item_id` is of type `int` for `GET` and `PUT` requests.
    * If it is not, the client will see a useful, clear error.
* Check if there is an optional query parameter named `q` (as in `http://127.0.0.1:8000/items/foo?q=somequery`) for `GET` requests.
    * As the `q` parameter is declared with `= None`, it is optional.
    * Without the `None` it would be required (as is the body in the case with `PUT`).
* For `PUT` requests to `/items/{item_id}`, read the body as JSON:
    * Check that it has a required attribute `name` that should be a `str`.
    * Check that it has a required attribute `price` that has to be a `float`.
    * Check that it has an optional attribute `is_offer`, that should be a `bool`, if present.
    * All this would also work for deeply nested JSON objects.
* Convert from and to JSON automatically.
* Document everything with OpenAPI, that can be used by:
    * Interactive documentation systems.
    * Automatic client code generation systems, for many languages.
* Provide 2 interactive documentation web interfaces directly.

---

We just scratched the surface, but you already get the idea of how it all works.

Try changing the line with:

```Python
    return {&quot;item_name&quot;: item.name, &quot;item_id&quot;: item_id}
```

...from:

```Python
        ... &quot;item_name&quot;: item.name ...
```

...to:

```Python
        ... &quot;item_price&quot;: item.price ...
```

...and see how your editor will auto-complete the attributes and know their types:

![editor support](https://fastapi.tiangolo.com/img/vscode-completion.png)

For a more comple

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[openai/evals]]></title>
            <link>https://github.com/openai/evals</link>
            <guid>https://github.com/openai/evals</guid>
            <pubDate>Mon, 10 Nov 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[Evals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/openai/evals">openai/evals</a></h1>
            <p>Evals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks.</p>
            <p>Language: Python</p>
            <p>Stars: 17,251</p>
            <p>Forks: 2,832</p>
            <p>Stars today: 6 stars today</p>
            <h2>README</h2><pre># OpenAI Evals

&gt; You can now configure and run Evals directly in the OpenAI Dashboard. [Get started ‚Üí](https://platform.openai.com/docs/guides/evals)

Evals provide a framework for evaluating large language models (LLMs) or systems built using LLMs. We offer an existing registry of evals to test different dimensions of OpenAI models and the ability to write your own custom evals for use cases you care about. You can also use your data to build private evals which represent the common LLMs patterns in your workflow without exposing any of that data publicly.

If you are building with LLMs, creating high quality evals is one of the most impactful things you can do. Without evals, it can be very difficult and time intensive to understand how different model versions might affect your use case. In the words of [OpenAI&#039;s President Greg Brockman](https://twitter.com/gdb/status/1733553161884127435):

&lt;img width=&quot;596&quot; alt=&quot;https://x.com/gdb/status/1733553161884127435?s=20&quot; src=&quot;https://github.com/openai/evals/assets/35577566/ce7840ff-43a8-4d88-bb2f-6b207410333b&quot;&gt;

## Setup

To run evals, you will need to set up and specify your [OpenAI API key](https://platform.openai.com/account/api-keys). After you obtain an API key, specify it using the [`OPENAI_API_KEY` environment variable](https://platform.openai.com/docs/quickstart/step-2-setup-your-api-key). Please be aware of the [costs](https://openai.com/pricing) associated with using the API when running evals. You can also run and create evals using [Weights &amp; Biases](https://wandb.ai/wandb_fc/openai-evals/reports/OpenAI-Evals-Demo-Using-W-B-Prompts-to-Run-Evaluations--Vmlldzo0MTI4ODA3).

**Minimum Required Version: Python 3.9**

### Downloading evals

Our evals registry is stored using [Git-LFS](https://git-lfs.com/). Once you have downloaded and installed LFS, you can fetch the evals (from within your local copy of the evals repo) with:
```sh
cd evals
git lfs fetch --all
git lfs pull
```

This will populate all the pointer files under `evals/registry/data`.

You may just want to fetch data for a select eval. You can achieve this via:
```sh
git lfs fetch --include=evals/registry/data/${your eval}
git lfs pull
```

### Making evals

If you are going to be creating evals, we suggest cloning this repo directly from GitHub and installing the requirements using the following command:

```sh
pip install -e .
```

Using `-e`, changes you make to your eval will be reflected immediately without having to reinstall.

Optionally, you can install the formatters for pre-committing with:

```sh
pip install -e .[formatters]
```

Then run `pre-commit install` to install pre-commit into your git hooks. pre-commit will now run on every commit.

If you want to manually run all pre-commit hooks on a repository, run `pre-commit run --all-files`. To run individual hooks use `pre-commit run &lt;hook_id&gt;`.

## Running evals

If you don&#039;t want to contribute new evals, but simply want to run them locally, you can install the evals package via pip:

```sh
pip install evals
```

You can find the full instructions to run existing evals in [`run-evals.md`](docs/run-evals.md) and our existing eval templates in [`eval-templates.md`](docs/eval-templates.md). For more advanced use cases like prompt chains or tool-using agents, you can use our [Completion Function Protocol](docs/completion-fns.md).

We provide the option for you to log your eval results to a Snowflake database, if you have one or wish to set one up. For this option, you will further have to specify the `SNOWFLAKE_ACCOUNT`, `SNOWFLAKE_DATABASE`, `SNOWFLAKE_USERNAME`, and `SNOWFLAKE_PASSWORD` environment variables.

## Writing evals

We suggest getting starting by: 

- Walking through the process for building an eval: [`build-eval.md`](docs/build-eval.md)
- Exploring an example of implementing custom eval logic: [`custom-eval.md`](docs/custom-eval.md)
- Writing your own completion functions: [`completion-fns.md`](docs/completion-fns.md)
- Review our starter guide for writing evals: [Getting Started with OpenAI Evals](https://cookbook.openai.com/examples/evaluation/getting_started_with_openai_evals)

Please note that we are currently not accepting evals with custom code! While we ask you to not submit such evals at the moment, you can still submit model-graded evals with custom model-graded YAML files.

If you think you have an interesting eval, please open a pull request with your contribution. OpenAI staff actively review these evals when considering improvements to upcoming models.

## FAQ

Do you have any examples of how to build an eval from start to finish?

- Yes! These are in the `examples` folder. We recommend that you also read through [`build-eval.md`](docs/build-eval.md) in order to gain a deeper understanding of what is happening in these examples.

Do you have any examples of evals implemented in multiple different ways?

- Yes! In particular, see `evals/registry/evals/coqa.yaml`. We have implemented small subsets of the [CoQA](https://stanfordnlp.github.io/coqa/) dataset for various eval templates to help illustrate the differences.

When I run an eval, it sometimes hangs at the very end (after the final report). What&#039;s going on?

- This is a known issue, but you should be able to interrupt it safely and the eval should finish immediately after.

There&#039;s a lot of code, and I just want to spin up a quick eval. Help? OR,

I am a world-class prompt engineer. I choose not to code. How can I contribute my wisdom?

- If you follow an existing [eval template](docs/eval-templates.md) to build a basic or model-graded eval, you don&#039;t need to write any evaluation code at all! Just provide your data in JSON format and specify your eval parameters in YAML. [build-eval.md](docs/build-eval.md) walks you through these steps, and you can supplement these instructions with the Jupyter notebooks in the `examples` folder to help you get started quickly. Keep in mind, though, that a good eval will inevitably require careful thought and rigorous experimentation!

## Disclaimer

By contributing to evals, you are agreeing to make your evaluation logic and data under the same MIT license as this repository. You must have adequate rights to upload any data used in an eval. OpenAI reserves the right to use this data in future service improvements to our product. Contributions to OpenAI evals will be subject to our usual Usage Policies: https://platform.openai.com/docs/usage-policies.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[public-apis/public-apis]]></title>
            <link>https://github.com/public-apis/public-apis</link>
            <guid>https://github.com/public-apis/public-apis</guid>
            <pubDate>Mon, 10 Nov 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[A collective list of free APIs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/public-apis/public-apis">public-apis/public-apis</a></h1>
            <p>A collective list of free APIs</p>
            <p>Language: Python</p>
            <p>Stars: 377,388</p>
            <p>Forks: 39,947</p>
            <p>Stars today: 529 stars today</p>
            <h2>README</h2><pre># Try Public APIs for free
The Public APIs repository is manually curated by community members like you and folks working at [APILayer](https://apilayer.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo). It includes an extensive list of public APIs from many domains that you can use for your own products. Consider it a treasure trove of APIs well-managed by the community over the years.

&lt;br &gt;

&lt;p&gt;
    &lt;a href=&quot;https://apilayer.com&quot;&gt;
        &lt;div&gt;
            &lt;img src=&quot;.github/cs1586-APILayerLogoUpdate2022-LJ_v2-HighRes.png&quot; width=&quot;100%&quot; alt=&quot;APILayer Logo&quot; /&gt;
        &lt;/div&gt;
    &lt;/a&gt;
  &lt;/p&gt;

APILayer is the fastest way to integrate APIs into any product. Explore [APILayer APIs](https://apilayer.com/products/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo) here for your next project.

Join our [Discord server](https://discord.com/invite/hgjA78638n/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo) to get updates, ask questions, get answers, random community calls, and more.

&lt;br &gt;

## APILayer APIs
| API | Description | Call this API |
|:---|:---|:---|
| [IPstack](https://ipstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Locate and Identify Website Visitors by IP Address | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-55145132-244c-448c-8e6f-8780866e4862?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-55145132-244c-448c-8e6f-8780866e4862%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Marketstack](https://marketstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Free, easy-to-use REST API interface delivering worldwide stock market data in JSON format | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Weatherstack](https://weatherstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Retrieve instant, accurate weather information for any location in the world in lightweight JSON format | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Numverify](https://numverify.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers ) | Global Phone Number Validation &amp; Lookup JSON API |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Fixer](https://fixer.io/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Fixer is a simple and lightweight API for current and historical foreign exchange (forex) rates. |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Aviationstack](https://aviationstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Free, real-time flight status and global Aviation data API |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-72ee0d35-018e-4370-a2b6-a66d3ebd5b5a?action=collection/fork)|

&lt;br &gt;

## Learn more about Public APIs

&lt;strong&gt;Get Involved&lt;/strong&gt;

* [Contributing Guide](CONTRIBUTING.md)
* [API for this project](https://github.com/davemachado/public-api)
* [Issues](https://github.com/public-apis/public-apis/issues)
* [Pull Requests](https://github.com/public-apis/public-apis/pulls)
* [LICENSE](LICENSE) 

&lt;br /&gt;

## Index

* [Animals](#animals)
* [Anime](#anime)
* [Anti-Malware](#anti-malware)
* [Art &amp; Design](#art--design)
* [Authentication &amp; Authorization](#authentication--authorization)
* [Blockchain](#blockchain)
* [Books](#books)
* [Business](#business)
* [Calendar](#calendar)
* [Cloud Storage &amp; File Sharing](#cloud-storage--file-sharing)
* [Continuous Integration](#continuous-integration)
* [Cryptocurrency](#cryptocurrency)
* [Currency Exchange](#currency-exchange)
* [Data Validation](#data-validation)
* [Development](#development)
* [Dictionaries](#dictionaries)
* [Documents &amp; Productivity](#documents--productivity)
* [Email](#email)
* [Entertainment](#entertainment)
* [Environment](#environment)
* [Events](#events)
* [Finance](#finance)
* [Food &amp; Drink](#food--drink)
* [Games &amp; Comics](#games--comics)
* [Geocoding](#geocoding)
* [Government](#government)
* [Health](#health)
* [Jobs](#jobs)
* [Machine Learning](#machine-learning)
* [Music](#music)
* [News](#news)
* [Open Data](#open-data)
* [Open Source Projects](#open-source-projects)
* [Patent](#patent)
* [Personality](#personality)
* [Phone](#phone)
* [Photography](#photography)
* [Programming](#programming)
* [Science &amp; Math](#science--math)
* [Security](#security)
* [Shopping](#shopping)
* [Social](#social)
* [Sports &amp; Fitness](#sports--fitness)
* [Test Data](#test-data)
* [Text Analysis](#text-analysis)
* [Tracking](#tracking)
* [Transportation](#transportation)
* [URL Shorteners](#url-shorteners)
* [Vehicle](#vehicle)
* [Video](#video)
* [Weather](#weather)
&lt;br &gt;

### Animals
API | Description | Auth | HTTPS | CORS 
|:---|:---|:---|:---|:---|
| [AdoptAPet](https://www.adoptapet.com/public/apis/pet_list.html) | Resource to help get pets adopted | `apiKey` | Yes | Yes |
| [Axolotl](https://theaxolotlapi.netlify.app/) | Collection of axolotl pictures and facts | No | Yes | No |
| [Cat Facts](https://alexwohlbruck.github.io/cat-facts/) | Daily cat facts | No | Yes | No | |
| [Cataas](https://cataas.com/) | Cat as a service (cats pictures and gifs) | No | Yes | No |
| [Cats](https://docs.thecatapi.com/) | Pictures of cats from Tumblr | `apiKey` | Yes | No |
| [Dog Facts](https://dukengn.github.io/Dog-facts-API/) | Random dog facts | No | Yes | Yes |
| [Dog Facts](https://kinduff.github.io/dog-api/) | Random facts of Dogs | No | Yes | Yes |
| [Dogs](https://dog.ceo/dog-api/) | Based on the Stanford Dogs Dataset | No | Yes | Yes |
| [eBird](https://documenter.getpostman.com/view/664302/S1ENwy59) | Retrieve recent or notable birding observations within a region | `apiKey` | Yes | No |
| [FishWatch](https://www.fishwatch.gov/developers) | Information and pictures about individual fish species | No | Yes | Yes |
| [HTTP Cat](https://http.cat/) | Cat for every HTTP Status | No | Yes | Yes |
| [HTTP Dog](https://http.dog/) | Dogs for every HTTP response status code | No | Yes | Yes |
| [IUCN](http://apiv3.iucnredlist.org/api/v3/docs) | IUCN Red List of Threatened Species | `apiKey` | No | No |
| [MeowFacts](https://github.com/wh-iterabb-it/meowfacts) | Get random cat facts | No | Yes | No |
| [Movebank](https://github.com/movebank/movebank-api-doc) | Movement and Migration data of animals | No | Yes | Yes |
| [Petfinder](https://www.petfinder.com/developers/) | Petfinder is dedicated to helping pets find homes, another resource to get pets adopted | `apiKey` | Yes | Yes |
| [PlaceBear](https://placebear.com/) | Placeholder bear pictures | No | Yes | Yes |
| [PlaceDog](https://place.dog) | Placeholder Dog pictures | No | Yes | Yes |
| [PlaceKitten](https://placekitten.com/) | Placeholder Kitten pictures | No | Yes | Yes |
| [RandomDog](https://random.dog/woof.json) | Random pictures of dogs | No | Yes | Yes |
| [RandomDuck](https://random-d.uk/api) | Random pictures of ducks | No | Yes | No |
| [RandomFox](https://randomfox.ca/floof/) | Random pictures of foxes | No | Yes | No |
| [RescueGroups](https://userguide.rescuegroups.org/display/APIDG/API+Developers+Guide+Home) | Adoption | No | Yes | Unknown |
| [Shibe.Online](http://shibe.online/) | Random pictures of Shiba Inu, cats or birds | No | Yes | Yes |
| [The Dog](https://thedogapi.com/) | A public service all about Dogs, free to use when making your fancy new App, Website or Service | `apiKey` | Yes | No |
| [xeno-canto](https://xeno-canto.org/explore/api) | Bird recordings | No | Yes | Unknown |
| [Zoo Animals](https://zoo-animal-api.herokuapp.com/) | Facts and pictures of zoo animals | No | Yes | Yes |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;

### Anime
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AniAPI](https://aniapi.com/docs/) | Anime discovery, streaming &amp; syncing with trackers | `OAuth` | Yes | Yes |
| [AniDB](https://wiki.anidb.net/HTTP_API_Definition) | Anime Database | `apiKey` | No | Unknown |
| [AniList](https://github.com/AniList/ApiV2-GraphQL-Docs) | Anime discovery &amp; tracking | `OAuth` | Yes | Unknown |
| [AnimeChan](https://github.com/RocktimSaikia/anime-chan) | Anime quotes (over 10k+) | No | Yes | No |
| [AnimeFacts](https://chandan-02.github.io/anime-facts-rest-api/) | Anime Facts (over 100+) | No | Yes | Yes |
| [AnimeNewsNetwork](https://www.animenewsnetwork.com/encyclopedia/api.php) | Anime industry news | No | Yes | Yes |
| [Catboy](https://catboys.com/api) | Neko images, funny GIFs &amp; more | No | Yes | Yes |
| [Danbooru Anime](https://danbooru.donmai.us/wiki_pages/help:api) | Thousands of anime artist database to find good anime art | `apiKey` | Yes | Yes |
| [Jikan](https://jikan.moe) | Unofficial MyAnimeList API | No | Yes | Yes |
| [Kitsu](https://kitsu.docs.apiary.io/) | Anime discovery platform | `OAuth` | Yes | Yes |
| [MangaDex](https://api.mangadex.org/docs.html) | Manga Database and Community | `apiKey` | Yes | Unknown |
| [Mangapi](https://rapidapi.com/pierre.carcellermeunier/api/mangapi3/) | Translate manga pages from one language to another | `apiKey` | Yes | Unknown |
| [MyAnimeList](https://myanimelist.net/clubs.php?cid=13727) | Anime and Manga Database and Community | `OAuth` | Yes | Unknown |
| [NekosBest](https://docs.nekos.best) | Neko Images &amp; Anime roleplaying GIFs | No | Yes | Yes |
| [Shikimori](https://shikimori.one/api/doc) | Anime discovery, tracking, forum, rates | `OAuth` | Yes | Unknown |
| [Studio Ghibli](https://ghibliapi.herokuapp.com) | Resources from Studio Ghibli films | No | Yes | Yes |
| [Trace Moe](https://soruly.github.io/trace.moe-api/#/) | A useful tool to get the exact scene of an anime from a screenshot | No | Yes | No |
| [Waifu.im](https://waifu.im/docs) | Get waifu pictures from an archive of over 4000 images and multiple tags | No | Yes | Yes |
| [Waifu.pics](https://waifu.pics/docs) | Image sharing platform for anime images | No | Yes | No |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;

### Anti-Malware
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AbuseIPDB](https://docs.abuseipdb.com/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [AlienVault Open Threat Exchange (OTX)](https://otx.alienvault.com/api) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [CAPEsandbox](https://capev2.readthedocs.io/en/latest/usage/api.html) | Malware execution and analysis | `apiKey` | Yes | Unknown |
| [Google Safe Browsing](https://developers.google.com/safe-browsing/) | Google Link/Domain Flagging | `apiKey` | Yes | Unknown |
| [MalDatabase](https://maldatabase.com/api-doc.html) | Provide malware datasets and threat intelligence feeds | `apiKey` | Yes | Unknown |
| [MalShare](https://malshare.com/doc.php) | Malware Archive / file sourcing | `apiKey` | Yes | No |
| [MalwareBazaar](https://bazaar.abuse.ch/api/) | Collect and share malware samples | `apiKey` | Yes | Unknown |
| [Metacert](https://metacert.com/) | Metacert Link Flagging | `apiKey` | Yes | Unknown |
| [NoPhishy](https://rapidapi.com/Amiichu/api/exerra-phishing-check/) | Check links to see if they&#039;re known phishing attempts | `apiKey` | Yes | Yes |
| [Phisherman](https://phisherman.gg/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [Scanii](https://docs.scanii.com/) | Simple REST API that can scan submitted documents/files for the presence of threats | `apiKey` | Yes | Yes |
| [URLhaus](https://urlhaus-api.abuse.ch/) | Bulk queries and Download Malware Samples | No | Yes | Yes |
| [URLScan.io](https://urlscan.io/about-api/) | Scan and Analyse URLs | `apiKey` | Yes | Unknown |
| [VirusTotal](https://www.virustotal.com/en/documentation/public-api/) | VirusTotal File/URL Analysis | `apiKey` | Yes | Unknown |
| [Web of Trust](https://support.mywot.com/hc/en-us/sections/360004477734-API-) | IP/domain/URL reputation | `apiKey` | Yes | Unknown | 

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Art &amp; Design
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [Am√©thyste](https://api.amethyste.moe/) | Generate images for Discord users | `apiKey` | Yes | Unknown |
| [Art Institute of Chicago](https://api.artic.edu/docs/) | Art | No | Yes | Yes |
| [Colormind](http://colormind.io/api-access/) | Color scheme generator | No | No | Unknown |
| [ColourLovers](http://www.colourlovers.com/api) | Get various patterns, palettes and images | No | No | Unknown |
| [Cooper Hewitt](https://collection.cooperhewitt.org/api) | Smithsonian Design Museum | `apiKey` | Yes | Unknown |
| [Dribbble](https://developer.dribbble.com) | Discover the world‚Äôs top designers &amp; creatives | `OAuth` | Yes | Unknown |
| [EmojiHub](https://github.com/cheatsnake/emojihub) | Get emojis by categories and groups | No | Yes | Yes |
| [Europeana](https://pro.europeana.eu/resources/apis/search) | European Museum and Galleries content | `apiKey` | Yes | Unknown |
| [Harvard Art Museums](https://github.com/harvardartmuseums/api-docs) | Art | `apiKey` | No | Unknown |
| [Icon Horse](https://icon.horse) | Favicons for any website, with fallbacks | No | Yes | Yes |
| [Iconfinder](https://developer.iconfinder.com) | Icons | `apiKey` | Yes | Unknown |
| [Icons8](https://img.icons8.com/) | Icons (find &quot;search icon&quot; hyperlink in page) | No | Yes | Unknown |
| [Lordicon](https://lordicon.com/) | Icons with predone Animations | No | Yes | Yes |
| [Metropolitan Museum of Art](https://metmuseum.github.io/) | Met Museum of Art | No | Yes | No |
| [Noun Project](http://api.thenounproject.com/index.html) | Icons | `OAuth` | No | Unknown |
| [PHP-Noise](https://php-noise.com/) | Noise Background Image Generator | No | Yes | Yes |
| [Pixel Encounter](https://pixelencounter.com/api) | SVG Icon Generator | No | Yes | No |
| [Rijksmuseum](https://data.rijksmuseum.nl/object-metadata/api/) | RijksMuseum Data | `apiKey` | Yes | Unknown |
| [Word Cloud](https://wordcloudapi.com/) | Easily create word clouds | `apiKey` | Yes | Unknown |
| [xColors](https://x-colors.herokuapp.com/) | Generate &amp; convert colors | No | Yes | Yes |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Authentication &amp; Authorization
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [Auth0](https://auth0.com) | Easy to implement, adaptable authentication and authorization platform | `apiKey` | Yes | Yes |
| [GetOTP](https://otp.dev/en/docs/) | Implement OTP flow quickly | `apiKey` | Yes | No |
| [Micro User Service](https://m3o.com/user) | User management and authentication | `apiKey` | Yes | No |
| [MojoAuth](https://mojoauth.com) | Secure and modern passwordless authentication platform | `apiKey` | Yes | Yes |
| [SAWO Labs](https://sawolabs.com) | Simplify login and improve user experience by integrating passwordless authentication in your app | `apiKey` | Yes | Yes |
| [Stytch](https://stytch.com/) | User infrastructure for modern applications | `apiKey` | Yes | No |
| [Warrant](https://warrant.dev/) | APIs for authorization and access control | `apiKey` | Yes | Yes |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Blockchain
| API | Description | Auth | HTTPS | CORS |
|---|:---|:---|:---|:---|
| [Bitquery](https://graphql.bitquery.io/ide) | Onchain GraphQL APIs &amp; DEX APIs | `apiKey` | Yes | Yes |
| [Chainlink](https://chain.link/developer-resources) | Build hybrid smart contracts with Chainlink | No | Yes | Unknown |
| [Chainpoint](https://tierion.com/chainpoint/) | Chainpoint is a global network for anchoring data to the Bitcoin blockchain | No | Yes | Unknown |
| [Covalent](https://www.covalenthq.com/docs/api/) | Multi-blockchain data aggregator platform | `apiKey` | Yes | Unknown |
| [Etherscan](https://etherscan.io/apis) | Ethereum explorer API | `apiKey` | Yes | Yes |
| [Helium](https://docs.helium.com/api/blockchain/introduction/) | Helium is a global, distributed network of Hotspots that create public, long-range wireless coverage | No | Yes | Unknown |
| [Nownodes](https://nownodes.io/) | Blockchain-as-a-service solution that provides high-quality connection via API | `apiKey` | Yes | Unknown |
| [Steem](https://developers.steem.io/) | Blockchain-based blogging and social media website | No | No | No |
| [The Graph](https://thegraph.com) | Indexing protocol for querying networks like Ethereum with GraphQL | `apiKey` | Yes | Unknown |
| [Walltime](https://walltime.info/api.html) | To retrieve Walltime&#039;s market info | No | Yes | Unknown |
| [Watchdata](https://docs.watchdata.io) | Provide simple and reliable API access to Ethereum blockchain | `apiKey` | Yes | Unknown |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Books
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [A B√≠blia Digital](https://www.abibliadigital.com.br/en) | Do not worry about managing the multiple versions of the Bible | `apiKey` | Yes | No |
| [Bhagavad Gita](https://docs.bhagavadgitaapi.in) | Open Source Shrimad Bhagavad Gita API including 21+ authors translation in Sanskrit/English/Hindi | `apiKey` | Yes | Yes |
| [Bhagavad Gita](https://bhagavadgita.io/api) | Bhagavad Gita text | `OAuth` | Yes | Yes |
| [Bhagavad Gita telugu](https://gita-api.vercel.app) | Bhagavad Gita API in telugu and odia languages | No | Yes | Yes |
| [Bible-api](https://bible-api.com/) | Free Bible API with multiple languages | No | Yes | Yes |
| [British National Bibliography](http://bnb.data.bl.uk/) | Books | No | No | Unknown |
| [Crossref Metadata Search](https://github.com/CrossRef/rest-api-doc) | Books &amp; Articles Metadata | No | Yes | Unknown |
| [Ganjoor](https://api.ganjoor.net) | Classic Persian poetry works including access to related manuscripts, recitations and music tracks | `OAuth` | Yes | Yes |
| [Google Books](https://developers.google.com/books/) | Books | `OAuth` | Yes | Unknown |
| [GurbaniNow](https://github.com/GurbaniNow/api) | Fast and Accurate Gurbani RESTful API | No | Yes | Unknown |
| [Gutendex](https://gutendex.com/) | Web-API for fetching data from Project Gutenberg Books Library | No | Yes | Unknown |
| [Open Library](https://openlibrary.org/developers/api) | Books, book covers and related data | No | Yes | No |
| [Penguin Publishing](http://www.penguinrandomhouse.biz/webservices/rest/) | Books, book covers and related data | No | Yes | Yes |
| [PoetryDB](https://github.com/thundercomb/poetrydb#readme) | Enables you to get instant data from our vast p

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[zulip/zulip]]></title>
            <link>https://github.com/zulip/zulip</link>
            <guid>https://github.com/zulip/zulip</guid>
            <pubDate>Mon, 10 Nov 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[Zulip server and web application. Open-source team chat that helps teams stay productive and focused.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/zulip/zulip">zulip/zulip</a></h1>
            <p>Zulip server and web application. Open-source team chat that helps teams stay productive and focused.</p>
            <p>Language: Python</p>
            <p>Stars: 23,764</p>
            <p>Forks: 8,902</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre># Zulip overview

[Zulip](https://zulip.com) is an open-source team collaboration tool with unique
[topic-based threading][why-zulip] that combines the best of email and chat to
make remote work productive and delightful. Fortune 500 companies, [leading open
source projects][rust-case-study], and thousands of other organizations use
Zulip every day. Zulip is the only [modern team chat app][features] that is
designed for both live and asynchronous conversations.

Zulip is built by a distributed community of developers from all around the
world, with 97+ people who have each contributed 100+ commits. With
over 1,500 contributors merging over 500 commits a month, Zulip is the
largest and fastest growing open source team chat project.

Come find us on the [development community chat](https://zulip.com/development-community/)!

[![GitHub Actions build status](https://github.com/zulip/zulip/actions/workflows/zulip-ci.yml/badge.svg)](https://github.com/zulip/zulip/actions/workflows/zulip-ci.yml?query=branch%3Amain)
[![coverage status](https://img.shields.io/codecov/c/github/zulip/zulip/main.svg)](https://codecov.io/gh/zulip/zulip)
[![Mypy coverage](https://img.shields.io/badge/mypy-100%25-green.svg)][mypy-coverage]
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
[![code style: prettier](https://img.shields.io/badge/code_style-prettier-ff69b4.svg)](https://github.com/prettier/prettier)
[![GitHub release](https://img.shields.io/github/release/zulip/zulip.svg)](https://github.com/zulip/zulip/releases/latest)
[![docs](https://readthedocs.org/projects/zulip/badge/?version=latest)](https://zulip.readthedocs.io/en/latest/)
[![Zulip chat](https://img.shields.io/badge/zulip-join_chat-brightgreen.svg)](https://chat.zulip.org)
[![Twitter](https://img.shields.io/badge/twitter-@zulip-blue.svg?style=flat)](https://twitter.com/zulip)
[![GitHub Sponsors](https://img.shields.io/github/sponsors/zulip)](https://github.com/sponsors/zulip)

[mypy-coverage]: https://blog.zulip.org/2016/10/13/static-types-in-python-oh-mypy/
[why-zulip]: https://zulip.com/why-zulip/
[rust-case-study]: https://zulip.com/case-studies/rust/
[features]: https://zulip.com/features/

## Getting started

- **Contributing code**. Check out our [guide for new
  contributors](https://zulip.readthedocs.io/en/latest/contributing/contributing.html)
  to get started. We have invested in making Zulip‚Äôs code highly
  readable, thoughtfully tested, and easy to modify. Beyond that, we
  have written an extraordinary 185K words of documentation for Zulip
  contributors.

- **Contributing non-code**. [Report an
  issue](https://zulip.readthedocs.io/en/latest/contributing/contributing.html#reporting-issues),
  [translate](https://zulip.readthedocs.io/en/latest/translating/translating.html)
  Zulip into your language, or [give us
  feedback](https://zulip.readthedocs.io/en/latest/contributing/suggesting-features.html).
  We&#039;d love to hear from you, whether you&#039;ve been using Zulip for years, or are just
  trying it out for the first time.

- **Checking Zulip out**. The best way to see Zulip in action is to drop by the
  [Zulip community server](https://zulip.com/development-community/). We also
  recommend reading about Zulip&#039;s [unique
  approach](https://zulip.com/why-zulip/) to organizing conversations.

- **Running a Zulip server**. Self-host Zulip directly on Ubuntu or Debian
  Linux, in [Docker](https://github.com/zulip/docker-zulip), or with prebuilt
  images for [Digital Ocean](https://marketplace.digitalocean.com/apps/zulip) and
  [Render](https://render.com/docs/deploy-zulip).
  Learn more about [self-hosting Zulip](https://zulip.com/self-hosting/).

- **Using Zulip without setting up a server**. Learn about [Zulip
  Cloud](https://zulip.com/plans/) hosting options. Zulip sponsors free [Zulip
  Cloud Standard](https://zulip.com/plans/) for hundreds of worthy
  organizations, including [fellow open-source
  projects](https://zulip.com/for/open-source/).

- **Participating in [outreach
  programs](https://zulip.readthedocs.io/en/latest/contributing/contributing.html#outreach-programs)**
  like [Google Summer of Code](https://developers.google.com/open-source/gsoc/)
  and [Outreachy](https://www.outreachy.org/).

- **Supporting Zulip**. Advocate for your organization to use Zulip, become a
  [sponsor](https://github.com/sponsors/zulip), write a review in the mobile app
  stores, or [help others find
  Zulip](https://zulip.readthedocs.io/en/latest/contributing/contributing.html#help-others-find-zulip).

You may also be interested in reading our [blog](https://blog.zulip.org/), and
following us on [Twitter](https://twitter.com/zulip) and
[LinkedIn](https://www.linkedin.com/company/zulip-project/).

Zulip is distributed under the
[Apache 2.0](https://github.com/zulip/zulip/blob/main/LICENSE) license.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[openai/whisper]]></title>
            <link>https://github.com/openai/whisper</link>
            <guid>https://github.com/openai/whisper</guid>
            <pubDate>Mon, 10 Nov 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[Robust Speech Recognition via Large-Scale Weak Supervision]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/openai/whisper">openai/whisper</a></h1>
            <p>Robust Speech Recognition via Large-Scale Weak Supervision</p>
            <p>Language: Python</p>
            <p>Stars: 90,600</p>
            <p>Forks: 11,354</p>
            <p>Stars today: 45 stars today</p>
            <h2>README</h2><pre># Whisper

[[Blog]](https://openai.com/blog/whisper)
[[Paper]](https://arxiv.org/abs/2212.04356)
[[Model card]](https://github.com/openai/whisper/blob/main/model-card.md)
[[Colab example]](https://colab.research.google.com/github/openai/whisper/blob/master/notebooks/LibriSpeech.ipynb)

Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.


## Approach

![Approach](https://raw.githubusercontent.com/openai/whisper/main/approach.png)

A Transformer sequence-to-sequence model is trained on various speech processing tasks, including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. These tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing a single model to replace many stages of a traditional speech-processing pipeline. The multitask training format uses a set of special tokens that serve as task specifiers or classification targets.


## Setup

We used Python 3.9.9 and [PyTorch](https://pytorch.org/) 1.10.1 to train and test our models, but the codebase is expected to be compatible with Python 3.8-3.11 and recent PyTorch versions. The codebase also depends on a few Python packages, most notably [OpenAI&#039;s tiktoken](https://github.com/openai/tiktoken) for their fast tokenizer implementation. You can download and install (or update to) the latest release of Whisper with the following command:

    pip install -U openai-whisper

Alternatively, the following command will pull and install the latest commit from this repository, along with its Python dependencies:

    pip install git+https://github.com/openai/whisper.git 

To update the package to the latest version of this repository, please run:

    pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git

It also requires the command-line tool [`ffmpeg`](https://ffmpeg.org/) to be installed on your system, which is available from most package managers:

```bash
# on Ubuntu or Debian
sudo apt update &amp;&amp; sudo apt install ffmpeg

# on Arch Linux
sudo pacman -S ffmpeg

# on MacOS using Homebrew (https://brew.sh/)
brew install ffmpeg

# on Windows using Chocolatey (https://chocolatey.org/)
choco install ffmpeg

# on Windows using Scoop (https://scoop.sh/)
scoop install ffmpeg
```

You may need [`rust`](http://rust-lang.org) installed as well, in case [tiktoken](https://github.com/openai/tiktoken) does not provide a pre-built wheel for your platform. If you see installation errors during the `pip install` command above, please follow the [Getting started page](https://www.rust-lang.org/learn/get-started) to install Rust development environment. Additionally, you may need to configure the `PATH` environment variable, e.g. `export PATH=&quot;$HOME/.cargo/bin:$PATH&quot;`. If the installation fails with `No module named &#039;setuptools_rust&#039;`, you need to install `setuptools_rust`, e.g. by running:

```bash
pip install setuptools-rust
```


## Available models and languages

There are six model sizes, four with English-only versions, offering speed and accuracy tradeoffs.
Below are the names of the available models and their approximate memory requirements and inference speed relative to the large model.
The relative speeds below are measured by transcribing English speech on a A100, and the real-world speed may vary significantly depending on many factors including the language, the speaking speed, and the available hardware.

|  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |
|:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|
|  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~10x      |
|  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~7x       |
| small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~4x       |
| medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |
| large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |
| turbo  |   809 M    |        N/A         |      `turbo`       |     ~6 GB     |      ~8x       |

The `.en` models for English-only applications tend to perform better, especially for the `tiny.en` and `base.en` models. We observed that the difference becomes less significant for the `small.en` and `medium.en` models.
Additionally, the `turbo` model is an optimized version of `large-v3` that offers faster transcription speed with a minimal degradation in accuracy.

Whisper&#039;s performance varies widely depending on the language. The figure below shows a performance breakdown of `large-v3` and `large-v2` models by language, using WERs (word error rates) or CER (character error rates, shown in *Italic*) evaluated on the Common Voice 15 and Fleurs datasets. Additional WER/CER metrics corresponding to the other models and datasets can be found in Appendix D.1, D.2, and D.4 of [the paper](https://arxiv.org/abs/2212.04356), as well as the BLEU (Bilingual Evaluation Understudy) scores for translation in Appendix D.3.

![WER breakdown by language](https://github.com/openai/whisper/assets/266841/f4619d66-1058-4005-8f67-a9d811b77c62)

## Command-line usage

The following command will transcribe speech in audio files, using the `turbo` model:

```bash
whisper audio.flac audio.mp3 audio.wav --model turbo
```

The default setting (which selects the `turbo` model) works well for transcribing English. However, **the `turbo` model is not trained for translation tasks**. If you need to **translate non-English speech into English**, use one of the **multilingual models** (`tiny`, `base`, `small`, `medium`, `large`) instead of `turbo`. 

For example, to transcribe an audio file containing non-English speech, you can specify the language:

```bash
whisper japanese.wav --language Japanese
```

To **translate** speech into English, use:

```bash
whisper japanese.wav --model medium --language Japanese --task translate
```

&gt; **Note:** The `turbo` model will return the original language even if `--task translate` is specified. Use `medium` or `large` for the best translation results.

Run the following to view all available options:

```bash
whisper --help
```

See [tokenizer.py](https://github.com/openai/whisper/blob/main/whisper/tokenizer.py) for the list of all available languages.


## Python usage

Transcription can also be performed within Python: 

```python
import whisper

model = whisper.load_model(&quot;turbo&quot;)
result = model.transcribe(&quot;audio.mp3&quot;)
print(result[&quot;text&quot;])
```

Internally, the `transcribe()` method reads the entire file and processes the audio with a sliding 30-second window, performing autoregressive sequence-to-sequence predictions on each window.

Below is an example usage of `whisper.detect_language()` and `whisper.decode()` which provide lower-level access to the model.

```python
import whisper

model = whisper.load_model(&quot;turbo&quot;)

# load audio and pad/trim it to fit 30 seconds
audio = whisper.load_audio(&quot;audio.mp3&quot;)
audio = whisper.pad_or_trim(audio)

# make log-Mel spectrogram and move to the same device as the model
mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)

# detect the spoken language
_, probs = model.detect_language(mel)
print(f&quot;Detected language: {max(probs, key=probs.get)}&quot;)

# decode the audio
options = whisper.DecodingOptions()
result = whisper.decode(model, mel, options)

# print the recognized text
print(result.text)
```

## More examples

Please use the [üôå Show and tell](https://github.com/openai/whisper/discussions/categories/show-and-tell) category in Discussions for sharing more example usages of Whisper and third-party extensions such as web demos, integrations with other tools, ports for different platforms, etc.


## License

Whisper&#039;s code and model weights are released under the MIT License. See [LICENSE](https://github.com/openai/whisper/blob/main/LICENSE) for further details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mozilla-ai/any-llm]]></title>
            <link>https://github.com/mozilla-ai/any-llm</link>
            <guid>https://github.com/mozilla-ai/any-llm</guid>
            <pubDate>Mon, 10 Nov 2025 00:04:20 GMT</pubDate>
            <description><![CDATA[Communicate with an LLM provider using a single interface]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mozilla-ai/any-llm">mozilla-ai/any-llm</a></h1>
            <p>Communicate with an LLM provider using a single interface</p>
            <p>Language: Python</p>
            <p>Stars: 1,248</p>
            <p>Forks: 96</p>
            <p>Stars today: 38 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/mozilla-ai/any-llm/refs/heads/main/docs/images/any-llm-logo-mark.png&quot; width=&quot;20%&quot; alt=&quot;Project logo&quot;/&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

# any-llm

[![Read the Blog Post](https://img.shields.io/badge/Read%20the%20Blog%20Post-red.svg)](https://blog.mozilla.ai/introducing-any-llm-a-unified-api-to-access-any-llm-provider/)

[![Docs](https://github.com/mozilla-ai/any-llm/actions/workflows/docs.yaml/badge.svg)](https://github.com/mozilla-ai/any-llm/actions/workflows/docs.yaml/)

[![Linting](https://github.com/mozilla-ai/any-llm/actions/workflows/lint.yaml/badge.svg)](https://github.com/mozilla-ai/any-llm/actions/workflows/lint.yaml/)
[![Unit Tests](https://github.com/mozilla-ai/any-llm/actions/workflows/tests-unit.yaml/badge.svg)](https://github.com/mozilla-ai/any-llm/actions/workflows/tests-unit.yaml/)
[![Integration Tests](https://github.com/mozilla-ai/any-llm/actions/workflows/tests-integration.yaml/badge.svg)](https://github.com/mozilla-ai/any-llm/actions/workflows/tests-integration.yaml/)

![Python 3.11+](https://img.shields.io/badge/python-3.11%2B-blue.svg)
[![PyPI](https://img.shields.io/pypi/v/any-llm-sdk)](https://pypi.org/project/any-llm-sdk/)
&lt;a href=&quot;https://discord.gg/4gf3zXrQUc&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/static/v1?label=Chat%20on&amp;message=Discord&amp;color=blue&amp;logo=Discord&amp;style=flat-square&quot; alt=&quot;Discord&quot;&gt;
&lt;/a&gt;

**Communicate with any LLM provider using a single, unified interface.**
Switch between OpenAI, Anthropic, Mistral, Ollama, and more without changing your code.

[Documentation](https://mozilla-ai.github.io/any-llm/) | [Try the Demos](#-try-it) | [Contributing](#-contributing)

&lt;/div&gt;

## Quickstart

```python
pip install &#039;any-llm-sdk[mistral,ollama]&#039;

export MISTRAL_API_KEY=&quot;YOUR_KEY_HERE&quot;  # or OPENAI_API_KEY, etc
from any_llm import completion
import os

# Make sure you have the appropriate environment variable set
assert os.environ.get(&#039;MISTRAL_API_KEY&#039;)

response = completion(
    model=&quot;mistral-small-latest&quot;,
    provider=&quot;mistral&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}]
)
print(response.choices[0].message.content)
```
**That&#039;s it!** Change the provider name and add provider-specific keys to switch between LLM providers.


## Installation

### Requirements

- Python 3.11 or newer
- API keys for whichever LLM providers you want to use

### Basic Installation

Install support for specific providers:

```bash
pip install &#039;any-llm-sdk[openai]&#039;           # Just OpenAI
pip install &#039;any-llm-sdk[mistral,ollama]&#039;   # Multiple providers
pip install &#039;any-llm-sdk[all]&#039;              # All supported providers
```

See our [list of supported providers](https://mozilla-ai.github.io/any-llm/providers/) to choose which ones you need.

### Setting Up API Keys

Set environment variables for your chosen providers:

```bash
export OPENAI_API_KEY=&quot;your-key-here&quot;
export ANTHROPIC_API_KEY=&quot;your-key-here&quot;
export MISTRAL_API_KEY=&quot;your-key-here&quot;
# ... etc
```

Alternatively, pass API keys directly in your code (see [Usage](#usage) examples).

&gt; **Note:** For production deployments requiring budget management and usage tracking, see the [Any-LLM Gateway](#any-llm-gateway) section below.


## Any-LLM Gateway

any-llm-gateway is an **optional** FastAPI-based proxy server that adds enterprise-grade features on top of the core library:

- **Budget Management** - Enforce spending limits with automatic daily, weekly, or monthly resets
- **API Key Management** - Issue, revoke, and monitor virtual API keys without exposing provider credentials
- **Usage Analytics** - Track every request with full token counts, costs, and metadata
- **Multi-tenant Support** - Manage access and budgets across users and teams

The gateway sits between your applications and LLM providers, exposing an OpenAI-compatible API that works with any supported provider.

### Quick Start
```bash
docker run \
  -e GATEWAY_MASTER_KEY=&quot;your-secure-master-key&quot; \
  -e OPENAI_API_KEY=&quot;your-api-key&quot; \
  -p 8000:8000 \
  ghcr.io/mozilla-ai/any-llm/gateway:latest
```

**Perfect for:**
- SaaS applications with tiered pricing
- Research teams tracking costs per user
- Enterprise deployments with centralized LLM access
- Development teams needing temporary, scoped API keys

See the [Gateway Documentation](https://mozilla-ai.github.io/any-llm/gateway/overview/) for complete setup and deployment instructions.

## Why choose `any-llm`?

- **Simple, unified interface** - Single function for all providers, switch models with just a string change
- **Developer friendly** - Full type hints for better IDE support and clear, actionable error messages
- **Leverages official provider SDKs** - Ensures maximum compatibility
- **Stays framework-agnostic** so it can be used across different projects and use cases
- **Battle-tested** - Powers our own production tools ([any-agent](https://github.com/mozilla-ai/any-agent))
- **Flexible deployment** - Direct connections for simplicity, or optional any-llm-gateway for production budget and access control

## Usage

`any-llm` offers two main approaches for interacting with LLM providers:

#### Option 1: Direct API Functions (Recommended for Bootstrapping and Experimentation)

**Recommended approach:** Use separate `provider` and `model` parameters:

```python
from any_llm import completion
import os

# Make sure you have the appropriate environment variable set
assert os.environ.get(&#039;MISTRAL_API_KEY&#039;)

response = completion(
    model=&quot;mistral-small-latest&quot;,
    provider=&quot;mistral&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}]
)
print(response.choices[0].message.content)
```

**Alternative syntax:** Use combined `provider:model` format:

```python
response = completion(
    model=&quot;mistral:mistral-small-latest&quot;, # &lt;provider_id&gt;:&lt;model_id&gt;
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}]
)
```

#### Option 2: AnyLLM Class (Recommended for Production)

For applications that need to reuse providers, perform multiple operations, or require more control:

```python
from any_llm import AnyLLM

llm = AnyLLM.create(&quot;mistral&quot;, api_key=&quot;your-mistral-api-key&quot;)

response = llm.completion(
    model=&quot;mistral-small-latest&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}]
)

```

#### When to Use Which Approach

| Approach | Best For | Connection Handling |
|----------|----------|---------------------|
| **Direct API Functions** (`completion`) | Scripts, notebooks, single requests | New client per call (stateless) |
| **AnyLLM Class** (`AnyLLM.create`) | Production apps, multiple requests | Reuses client (connection pooling) |

Both approaches support identical features: streaming, tools, responses API, etc.

### Responses API

For providers that implement the OpenAI-style Responses API, use [`responses`](https://mozilla-ai.github.io/any-llm/api/responses/) or `aresponses`:

```python
from any_llm import responses

result = responses(
    model=&quot;gpt-4o-mini&quot;,
    provider=&quot;openai&quot;,
    input_data=[
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Summarize this in one sentence.&quot;}
        ]}
    ],
)

# Non-streaming returns an OpenAI-compatible Responses object alias
print(result.output_text)
```

### Finding the Right Model

The `provider_id` should match our [supported provider names](https://mozilla-ai.github.io/any-llm/providers/).

The `model_id` is passed directly to the provider. To find available models:
- Check the provider&#039;s documentation
- Use our `list_models` API (if the provider supports it)


## Try It

Try `any-llm` in action with our interactive demos:

### üí¨ Chat Demo
**[üìÇ Run the Chat Demo](./demos/chat/README.md)**

An interactive chat interface showcasing streaming completions and provider switching:
- Real-time streaming responses
- Easy switching between multiple LLM providers
- Collapsible &quot;thinking&quot; content display for supported models
- Auto-scrolling chat interface

### üîç Model Finder Demo
**[üìÇ Run the Model Finder Demo](./demos/finder/README.md)**

A model discovery tool featuring:
- Search and filter models across all your configured providers
- Provider status dashboard
- API configuration checker

## Motivation

The landscape of LLM provider interfaces is fragmented. While OpenAI&#039;s API has become the de facto standard, providers implement slight variations in parameter names, response formats, and feature sets. This creates a need for light wrappers that gracefully handle these differences while maintaining a consistent interface.

**Existing Solutions and Their Limitations:**

- **[LiteLLM](https://github.com/BerriAI/litellm)**: Popular but reimplements provider interfaces rather than leveraging official SDKs, leading to potential compatibility issues.
- **[AISuite](https://github.com/andrewyng/aisuite/issues)**: Clean, modular approach but lacks active maintenance, comprehensive testing, and modern Python typing standards.
- **[Framework-specific solutions](https://github.com/agno-agi/agno/tree/main/libs/agno/agno/models)**: Some agent frameworks either depend on LiteLLM or implement their own provider integrations, creating fragmentation
- **[Proxy Only Solutions](https://openrouter.ai/)**: solutions like [OpenRouter](https://openrouter.ai/) and [Portkey](https://github.com/Portkey-AI/portkey-python-sdk) require a hosted proxy between your code and the LLM provider.

`any-llm` addresses these challenges by leveraging official SDKs when available, maintaining framework-agnostic design, and requiring no proxy servers.

## Documentation
- **[Full Documentation](https://mozilla-ai.github.io/any-llm/)** - Complete guides and API reference
- **[Supported Providers](https://mozilla-ai.github.io/any-llm/providers/)** - List of all supported LLM providers
- **[Cookbook Examples](https://mozilla-ai.github.io/any-llm/cookbook/)** - In-depth usage examples


## Contributing
We welcome contributions from developers of all skill levels! Please see our [Contributing Guide](CONTRIBUTING.md) or open an issue to discuss changes.

## License
This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE.md) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>