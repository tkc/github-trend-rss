<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Wed, 12 Nov 2025 00:04:38 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[sansan0/TrendRadar]]></title>
            <link>https://github.com/sansan0/TrendRadar</link>
            <guid>https://github.com/sansan0/TrendRadar</guid>
            <pubDate>Wed, 12 Nov 2025 00:04:38 GMT</pubDate>
            <description><![CDATA[üéØ ÂëäÂà´‰ø°ÊÅØËøáËΩΩÔºåAI Âä©‰Ω†ÁúãÊáÇÊñ∞ÈóªËµÑËÆØÁÉ≠ÁÇπÔºåÁÆÄÂçïÁöÑËàÜÊÉÖÁõëÊéßÂàÜÊûê - Â§öÂπ≥Âè∞ÁÉ≠ÁÇπËÅöÂêà+Âü∫‰∫é MCP ÁöÑAIÂàÜÊûêÂ∑•ÂÖ∑„ÄÇÁõëÊéß35‰∏™Âπ≥Âè∞ÔºàÊäñÈü≥„ÄÅÁü•‰πé„ÄÅBÁ´ô„ÄÅÂçéÂ∞îË°óËßÅÈóª„ÄÅË¥¢ËÅîÁ§æÁ≠âÔºâÔºåÊô∫ËÉΩÁ≠õÈÄâ+Ëá™Âä®Êé®ÈÄÅ+AIÂØπËØùÂàÜÊûêÔºàÁî®Ëá™ÁÑ∂ËØ≠Ë®ÄÊ∑±Â∫¶ÊåñÊéòÊñ∞ÈóªÔºöË∂ãÂäøËøΩË∏™„ÄÅÊÉÖÊÑüÂàÜÊûê„ÄÅÁõ∏‰ººÊ£ÄÁ¥¢Á≠â13ÁßçÂ∑•ÂÖ∑Ôºâ„ÄÇÊîØÊåÅ‰ºÅ‰∏öÂæÆ‰ø°/È£û‰π¶/ÈíâÈíâ/Telegram/ÈÇÆ‰ª∂/ntfyÊé®ÈÄÅÔºå30ÁßíÁΩëÈ°µÈÉ®ÁΩ≤Ôºå1ÂàÜÈíüÊâãÊú∫ÈÄöÁü•ÔºåÊó†ÈúÄÁºñÁ®ã„ÄÇÊîØÊåÅDockerÈÉ®ÁΩ≤‚≠ê ËÆ©ÁÆóÊ≥ï‰∏∫‰Ω†ÊúçÂä°ÔºåÁî®AIÁêÜËß£ÁÉ≠ÁÇπ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sansan0/TrendRadar">sansan0/TrendRadar</a></h1>
            <p>üéØ ÂëäÂà´‰ø°ÊÅØËøáËΩΩÔºåAI Âä©‰Ω†ÁúãÊáÇÊñ∞ÈóªËµÑËÆØÁÉ≠ÁÇπÔºåÁÆÄÂçïÁöÑËàÜÊÉÖÁõëÊéßÂàÜÊûê - Â§öÂπ≥Âè∞ÁÉ≠ÁÇπËÅöÂêà+Âü∫‰∫é MCP ÁöÑAIÂàÜÊûêÂ∑•ÂÖ∑„ÄÇÁõëÊéß35‰∏™Âπ≥Âè∞ÔºàÊäñÈü≥„ÄÅÁü•‰πé„ÄÅBÁ´ô„ÄÅÂçéÂ∞îË°óËßÅÈóª„ÄÅË¥¢ËÅîÁ§æÁ≠âÔºâÔºåÊô∫ËÉΩÁ≠õÈÄâ+Ëá™Âä®Êé®ÈÄÅ+AIÂØπËØùÂàÜÊûêÔºàÁî®Ëá™ÁÑ∂ËØ≠Ë®ÄÊ∑±Â∫¶ÊåñÊéòÊñ∞ÈóªÔºöË∂ãÂäøËøΩË∏™„ÄÅÊÉÖÊÑüÂàÜÊûê„ÄÅÁõ∏‰ººÊ£ÄÁ¥¢Á≠â13ÁßçÂ∑•ÂÖ∑Ôºâ„ÄÇÊîØÊåÅ‰ºÅ‰∏öÂæÆ‰ø°/È£û‰π¶/ÈíâÈíâ/Telegram/ÈÇÆ‰ª∂/ntfyÊé®ÈÄÅÔºå30ÁßíÁΩëÈ°µÈÉ®ÁΩ≤Ôºå1ÂàÜÈíüÊâãÊú∫ÈÄöÁü•ÔºåÊó†ÈúÄÁºñÁ®ã„ÄÇÊîØÊåÅDockerÈÉ®ÁΩ≤‚≠ê ËÆ©ÁÆóÊ≥ï‰∏∫‰Ω†ÊúçÂä°ÔºåÁî®AIÁêÜËß£ÁÉ≠ÁÇπ</p>
            <p>Language: Python</p>
            <p>Stars: 6,384</p>
            <p>Forks: 4,598</p>
            <p>Stars today: 444 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[usestrix/strix]]></title>
            <link>https://github.com/usestrix/strix</link>
            <guid>https://github.com/usestrix/strix</guid>
            <pubDate>Wed, 12 Nov 2025 00:04:37 GMT</pubDate>
            <description><![CDATA[‚ú® Open-source AI hackers for your apps üë®üèª‚Äçüíª]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/usestrix/strix">usestrix/strix</a></h1>
            <p>‚ú® Open-source AI hackers for your apps üë®üèª‚Äçüíª</p>
            <p>Language: Python</p>
            <p>Stars: 9,598</p>
            <p>Forks: 865</p>
            <p>Stars today: 1,137 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://usestrix.com/&quot;&gt;
    &lt;img src=&quot;.github/logo.png&quot; width=&quot;150&quot; alt=&quot;Strix Logo&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt;
Strix
&lt;/h1&gt;

&lt;h2 align=&quot;center&quot;&gt;Open-source AI Hackers to secure your Apps&lt;/h2&gt;

&lt;div align=&quot;center&quot;&gt;

[![Python](https://img.shields.io/pypi/pyversions/strix-agent?color=3776AB)](https://pypi.org/project/strix-agent/)
[![PyPI](https://img.shields.io/pypi/v/strix-agent?color=10b981)](https://pypi.org/project/strix-agent/)
[![PyPI Downloads](https://static.pepy.tech/personalized-badge/strix-agent?period=total&amp;units=INTERNATIONAL_SYSTEM&amp;left_color=GREY&amp;right_color=RED&amp;left_text=Downloads)](https://pepy.tech/projects/strix-agent)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)

[![GitHub Stars](https://img.shields.io/github/stars/usestrix/strix)](https://github.com/usestrix/strix)
[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?&amp;logo=discord&amp;logoColor=white)](https://discord.gg/YjKFvEZSdZ)
[![Website](https://img.shields.io/badge/Website-usestrix.com-2d3748.svg)](https://usestrix.com)

&lt;a href=&quot;https://trendshift.io/repositories/15362&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/15362&quot; alt=&quot;usestrix%2Fstrix | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

:star: _Love Strix? Give us a star to help other developers discover it!_

&lt;br /&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;.github/screenshot.png&quot; alt=&quot;Strix Demo&quot; width=&quot;800&quot; style=&quot;border-radius: 16px;&quot;&gt;
&lt;/div&gt;

&gt; [!TIP]
&gt; **New!** Strix now integrates seamlessly with GitHub Actions and CI/CD pipelines. Automatically scan for vulnerabilities on every pull request and block insecure code before it reaches production!

&gt; [!WARNING]
&gt; Only test systems you own or have permission to test. You are responsible for using Strix ethically and legally.

---

## ü¶â Strix Overview

Strix are autonomous AI agents that act just like real hackers - they run your code dynamically, find vulnerabilities, and validate them through actual proof-of-concepts. Built for developers and security teams who need fast, accurate security testing without the overhead of manual pentesting or the false positives of static analysis tools.

- **Full hacker toolkit** out of the box
- **Teams of agents** that collaborate and scale
- **Real validation** with PoCs, not false positives
- **Developer‚Äëfirst** CLI with actionable reports
- **Auto‚Äëfix &amp; reporting** to accelerate remediation

---

### üéØ Use Cases

- Detect and validate critical vulnerabilities in your applications.
- Get penetration tests done in hours, not weeks, with compliance reports.
- Automate bug bounty research and generate PoCs for faster reporting.
- Run tests in CI/CD to block vulnerabilities before reaching production.

---

### üöÄ Quick Start

Prerequisites:
- Docker (running)
- Python 3.12+
- An LLM provider key (or a local LLM)

```bash
# Install
pipx install strix-agent

# Configure AI provider
export STRIX_LLM=&quot;openai/gpt-5&quot;
export LLM_API_KEY=&quot;your-api-key&quot;

# Run security assessment
strix --target ./app-directory
```

First run pulls the sandbox Docker image. Results are saved under `agent_runs/&lt;run-name&gt;`.

### üèÜ Enterprise Platform

Want to skip the setup? Try our cloud-hosted version: **[usestrix.com](https://usestrix.com)**

Our managed platform provides:

- **üìà Executive Dashboards**
- **üß† Custom Fine-Tuned Models**
- **‚öôÔ∏è CI/CD Integration**
- **üîç Large-Scale Scanning**
- **üîå Third-Party Integrations**
- **üéØ Enterprise Support**

[**Get Enterprise Demo ‚Üí**](https://usestrix.com)

## ‚ú® Features

### üõ†Ô∏è Agentic Security Tools

- **üîå Full HTTP Proxy** - Full request/response manipulation and analysis
- **üåê Browser Automation** - Multi-tab browser for testing of XSS, CSRF, auth flows
- **üíª Terminal Environments** - Interactive shells for command execution and testing
- **üêç Python Runtime** - Custom exploit development and validation
- **üîç Reconnaissance** - Automated OSINT and attack surface mapping
- **üìÅ Code Analysis** - Static and dynamic analysis capabilities
- **üìù Knowledge Management** - Structured findings and attack documentation

### üéØ Comprehensive Vulnerability Detection

- **Access Control** - IDOR, privilege escalation, auth bypass
- **Injection Attacks** - SQL, NoSQL, command injection
- **Server-Side** - SSRF, XXE, deserialization flaws
- **Client-Side** - XSS, prototype pollution, DOM vulnerabilities
- **Business Logic** - Race conditions, workflow manipulation
- **Authentication** - JWT vulnerabilities, session management
- **Infrastructure** - Misconfigurations, exposed services

### üï∏Ô∏è Graph of Agents

- **Distributed Workflows** - Specialized agents for different attacks and assets
- **Scalable Testing** - Parallel execution for fast comprehensive coverage
- **Dynamic Coordination** - Agents collaborate and share discoveries

## üíª Usage Examples

### Default Usage

```bash
# Local codebase analysis
strix --target ./app-directory

# Repository security review
strix --target https://github.com/org/repo

# Black-Box Web application assessment
strix --target https://your-app.com

# Grey-Box Security Assesment
strix --target https://your-app.com --instructions &quot;Perform authenticated testing using the following credentials user:pass&quot;

# Multi-target white-box testing (source code + deployed app)
strix -t https://github.com/org/app -t https://your-app.com

# Focused testing with instructions
strix --target api.your-app.com --instruction &quot;Focus on business logic flaws and IDOR vulnerabilities&quot;
```

### ü§ñ Headless Mode

Run Strix programmatically without interactive UI using the `-n/--non-interactive` flag‚Äîperfect for servers and automated jobs. The CLI prints real-time vulnerability findings, and the final report before exiting. Exits with non-zero code when vulnerabilities are found.

```bash
strix -n --target https://your-app.com
```

### üîÑ CI/CD (GitHub Actions)

Strix can be added to your pipeline to run a security test on pull requests with a lightweight GitHub Actions workflow:

```yaml
name: strix-penetration-test

on:
  pull_request:

jobs:
  security-scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install Strix
        run: pipx install strix-agent

      - name: Run Strix
        env:
          STRIX_LLM: ${{ secrets.STRIX_LLM }}
          LLM_API_KEY: ${{ secrets.LLM_API_KEY }}

        run: strix -n -t ./
```

### ‚öôÔ∏è Configuration

```bash
export STRIX_LLM=&quot;openai/gpt-5&quot;
export LLM_API_KEY=&quot;your-api-key&quot;

# Optional
export LLM_API_BASE=&quot;your-api-base-url&quot;  # if using a local model, e.g. Ollama, LMStudio
export PERPLEXITY_API_KEY=&quot;your-api-key&quot;  # for search capabilities
```

[üìö View supported AI models](https://docs.litellm.ai/docs/providers)

## ü§ù Contributing

We welcome contributions from the community! There are several ways to contribute:

### Code Contributions
See our [Contributing Guide](CONTRIBUTING.md) for details on:
- Setting up your development environment
- Running tests and quality checks
- Submitting pull requests
- Code style guidelines

### Prompt Modules Collection
Help expand our collection of specialized prompt modules for AI agents:
- Advanced testing techniques for vulnerabilities, frameworks, and technologies
- See [Prompt Modules Documentation](strix/prompts/README.md) for guidelines
- Submit via [pull requests](https://github.com/usestrix/strix/pulls) or [issues](https://github.com/usestrix/strix/issues)

## üë• Join Our Community

Have questions? Found a bug? Want to contribute? **[Join our Discord!](https://discord.gg/YjKFvEZSdZ)**

## üåü Support the Project

**Love Strix?** Give us a ‚≠ê on GitHub!

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://api.star-history.com/svg?repos=usestrix/strix&amp;type=date&amp;legend=top-left&quot; alt=&quot;Star History Chart&quot; width=&quot;800&quot; style=&quot;border-radius: 16px;&quot;&gt;
&lt;/div&gt;

&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[yichuan-w/LEANN]]></title>
            <link>https://github.com/yichuan-w/LEANN</link>
            <guid>https://github.com/yichuan-w/LEANN</guid>
            <pubDate>Wed, 12 Nov 2025 00:04:36 GMT</pubDate>
            <description><![CDATA[RAG on Everything with LEANN. Enjoy 97% storage savings while running a fast, accurate, and 100% private RAG application on your personal device.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/yichuan-w/LEANN">yichuan-w/LEANN</a></h1>
            <p>RAG on Everything with LEANN. Enjoy 97% storage savings while running a fast, accurate, and 100% private RAG application on your personal device.</p>
            <p>Language: Python</p>
            <p>Stars: 4,056</p>
            <p>Forks: 412</p>
            <p>Stars today: 70 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/logo-text.png&quot; alt=&quot;LEANN Logo&quot; width=&quot;400&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Python-3.9%20%7C%203.10%20%7C%203.11%20%7C%203.12%20%7C%203.13-blue.svg&quot; alt=&quot;Python Versions&quot;&gt;
  &lt;img src=&quot;https://github.com/yichuan-w/LEANN/actions/workflows/build-and-publish.yml/badge.svg&quot; alt=&quot;CI Status&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Platform-Ubuntu%20%26%20Arch%20%26%20WSL%20%7C%20macOS%20(ARM64%2FIntel)-lightgrey&quot; alt=&quot;Platform&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/License-MIT-green.svg&quot; alt=&quot;MIT License&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/MCP-Native%20Integration-blue&quot; alt=&quot;MCP Integration&quot;&gt;
  &lt;a href=&quot;https://join.slack.com/t/leann-e2u9779/shared_invite/zt-3ckd2f6w1-OX08~NN4gkWhh10PRVBj1Q&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Slack-Join-4A154B?logo=slack&amp;logoColor=white&quot; alt=&quot;Join Slack&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;assets/wechat_user_group.JPG&quot; title=&quot;Join WeChat group&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/WeChat-Join-2DC100?logo=wechat&amp;logoColor=white&quot; alt=&quot;Join WeChat group&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;h2 align=&quot;center&quot; tabindex=&quot;-1&quot; class=&quot;heading-element&quot; dir=&quot;auto&quot;&gt;
    The smallest vector index in the world. RAG Everything with LEANN!
&lt;/h2&gt;

LEANN is an innovative vector database that democratizes personal AI. Transform your laptop into a powerful RAG system that can index and search through millions of documents while using **97% less storage** than traditional solutions **without accuracy loss**.

LEANN achieves this through *graph-based selective recomputation* with *high-degree preserving pruning*, computing embeddings on-demand instead of storing them all. [Illustration Fig ‚Üí](#Ô∏è-architecture--how-it-works) | [Paper ‚Üí](https://arxiv.org/abs/2506.08276)

**Ready to RAG Everything?** Transform your laptop into a personal AI assistant that can semantic search your **[file system](#-personal-data-manager-process-any-documents-pdf-txt-md)**, **[emails](#-your-personal-email-secretary-rag-on-apple-mail)**, **[browser history](#-time-machine-for-the-web-rag-your-entire-browser-history)**, **[chat history](#-wechat-detective-unlock-your-golden-memories)** ([WeChat](#-wechat-detective-unlock-your-golden-memories), [iMessage](#-imessage-history-your-personal-conversation-archive)), **[agent memory](#-chatgpt-chat-history-your-personal-ai-conversation-archive)** ([ChatGPT](#-chatgpt-chat-history-your-personal-ai-conversation-archive), [Claude](#-claude-chat-history-your-personal-ai-conversation-archive)), **[live data](#mcp-integration-rag-on-live-data-from-any-platform)** ([Slack](#mcp-integration-rag-on-live-data-from-any-platform), [Twitter](#mcp-integration-rag-on-live-data-from-any-platform)), **[codebase](#-claude-code-integration-transform-your-development-workflow)**\* , or external knowledge bases (i.e., 60M documents) - all on your laptop, with zero cloud costs and complete privacy.


\* Claude Code only supports basic `grep`-style keyword search. **LEANN** is a drop-in **semantic search MCP service fully compatible with Claude Code**, unlocking intelligent retrieval without changing your workflow. üî• Check out [the easy setup ‚Üí](packages/leann-mcp/README.md)



## Why LEANN?

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/effects.png&quot; alt=&quot;LEANN vs Traditional Vector DB Storage Comparison&quot; width=&quot;70%&quot;&gt;
&lt;/p&gt;

&gt; **The numbers speak for themselves:** Index 60 million text chunks in just 6GB instead of 201GB. From emails to browser history, everything fits on your laptop. [See detailed benchmarks for different applications below ‚Üì](#-storage-comparison)


üîí **Privacy:** Your data never leaves your laptop. No OpenAI, no cloud, no &quot;terms of service&quot;.

ü™∂ **Lightweight:** Graph-based recomputation eliminates heavy embedding storage, while smart graph pruning and CSR format minimize graph storage overhead. Always less storage, less memory usage!

üì¶ **Portable:** Transfer your entire knowledge base between devices (even with others) with minimal cost - your personal AI memory travels with you.

üìà **Scalability:** Handle messy personal data that would crash traditional vector DBs, easily managing your growing personalized data and agent generated memory!

‚ú® **No Accuracy Loss:** Maintain the same search quality as heavyweight solutions while using 97% less storage.

## Installation

### üì¶ Prerequisites: Install uv

[Install uv](https://docs.astral.sh/uv/getting-started/installation/#installation-methods) first if you don&#039;t have it. Typically, you can install it with:

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### üöÄ Quick Install

Clone the repository to access all examples and try amazing applications,

```bash
git clone https://github.com/yichuan-w/LEANN.git leann
cd leann
```

and install LEANN from [PyPI](https://pypi.org/project/leann/) to run them immediately:

```bash
uv venv
source .venv/bin/activate
uv pip install leann
```

&lt;!--
&gt; Low-resource? See &quot;Low-resource setups&quot; in the [Configuration Guide](docs/configuration-guide.md#low-resource-setups). --&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;strong&gt;üîß Build from Source (Recommended for development)&lt;/strong&gt;
&lt;/summary&gt;



```bash
git clone https://github.com/yichuan-w/LEANN.git leann
cd leann
git submodule update --init --recursive
```

**macOS:**

Note: DiskANN requires MacOS 13.3 or later.

```bash
brew install libomp boost protobuf zeromq pkgconf
uv sync --extra diskann
```

**Linux (Ubuntu/Debian):**

Note: On Ubuntu 20.04, you may need to build a newer Abseil and pin Protobuf (e.g., v3.20.x) for building DiskANN. See [Issue #30](https://github.com/yichuan-w/LEANN/issues/30) for a step-by-step note.

You can manually install [Intel oneAPI MKL](https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html) instead of `libmkl-full-dev` for DiskANN. You can also use `libopenblas-dev` for building HNSW only, by removing `--extra diskann` in the command below.

```bash
sudo apt-get update &amp;&amp; sudo apt-get install -y \
  libomp-dev libboost-all-dev protobuf-compiler libzmq3-dev \
  pkg-config libabsl-dev libaio-dev libprotobuf-dev \
  libmkl-full-dev

uv sync --extra diskann
```

**Linux (Arch Linux):**

```bash
sudo pacman -Syu &amp;&amp; sudo pacman -S --needed base-devel cmake pkgconf git gcc \
  boost boost-libs protobuf abseil-cpp libaio zeromq

# For MKL in DiskANN
sudo pacman -S --needed base-devel git
git clone https://aur.archlinux.org/paru-bin.git
cd paru-bin &amp;&amp; makepkg -si
paru -S intel-oneapi-mkl intel-oneapi-compiler
source /opt/intel/oneapi/setvars.sh

uv sync --extra diskann
```

**Linux (RHEL / CentOS Stream / Oracle / Rocky / AlmaLinux):**

See [Issue #50](https://github.com/yichuan-w/LEANN/issues/50) for more details.

```bash
sudo dnf groupinstall -y &quot;Development Tools&quot;
sudo dnf install -y libomp-devel boost-devel protobuf-compiler protobuf-devel \
  abseil-cpp-devel libaio-devel zeromq-devel pkgconf-pkg-config

# For MKL in DiskANN
sudo dnf install -y intel-oneapi-mkl intel-oneapi-mkl-devel \
  intel-oneapi-openmp || sudo dnf install -y intel-oneapi-compiler
source /opt/intel/oneapi/setvars.sh

uv sync --extra diskann
```

&lt;/details&gt;


## Quick Start

Our declarative API makes RAG as easy as writing a config file.

Check out [demo.ipynb](demo.ipynb) or [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yichuan-w/LEANN/blob/main/demo.ipynb)

```python
from leann import LeannBuilder, LeannSearcher, LeannChat
from pathlib import Path
INDEX_PATH = str(Path(&quot;./&quot;).resolve() / &quot;demo.leann&quot;)

# Build an index
builder = LeannBuilder(backend_name=&quot;hnsw&quot;)
builder.add_text(&quot;LEANN saves 97% storage compared to traditional vector databases.&quot;)
builder.add_text(&quot;Tung Tung Tung Sahur called‚Äîthey need their banana‚Äëcrocodile hybrid back&quot;)
builder.build_index(INDEX_PATH)

# Search
searcher = LeannSearcher(INDEX_PATH)
results = searcher.search(&quot;fantastical AI-generated creatures&quot;, top_k=1)

# Chat with your data
chat = LeannChat(INDEX_PATH, llm_config={&quot;type&quot;: &quot;hf&quot;, &quot;model&quot;: &quot;Qwen/Qwen3-0.6B&quot;})
response = chat.ask(&quot;How much storage does LEANN save?&quot;, top_k=1)
```

## RAG on Everything!

LEANN supports RAG on various data sources including documents (`.pdf`, `.txt`, `.md`), Apple Mail, Google Search History, WeChat, ChatGPT conversations, Claude conversations, iMessage conversations, and **live data from any platform through MCP (Model Context Protocol) servers** - including Slack, Twitter, and more.



### Generation Model Setup

#### LLM Backend

LEANN supports many LLM providers for text generation (HuggingFace, Ollama, and Any OpenAI compatible API).


&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üîë OpenAI API Setup (Default)&lt;/strong&gt;&lt;/summary&gt;

Set your OpenAI API key as an environment variable:

```bash
export OPENAI_API_KEY=&quot;your-api-key-here&quot;
```

Make sure to use `--llm openai` flag when using the CLI.
You can also specify the model name with `--llm-model &lt;model-name&gt;` flag.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üõ†Ô∏è Supported LLM &amp; Embedding Providers (via OpenAI Compatibility)&lt;/strong&gt;&lt;/summary&gt;

Thanks to the widespread adoption of the OpenAI API format, LEANN is compatible out-of-the-box with a vast array of LLM and embedding providers. Simply set the `OPENAI_BASE_URL` and `OPENAI_API_KEY` environment variables to connect to your preferred service.

```sh
export OPENAI_API_KEY=&quot;xxx&quot;
export OPENAI_BASE_URL=&quot;http://localhost:1234/v1&quot; # base url of the provider
```

To use OpenAI compatible endpoint with the CLI interface:

If you are using it for text generation, make sure to use `--llm openai` flag and specify the model name with `--llm-model &lt;model-name&gt;` flag.

If you are using it for embedding, set the `--embedding-mode openai` flag and specify the model name with `--embedding-model &lt;MODEL&gt;`.

-----


Below is a list of base URLs for common providers to get you started.


### üñ•Ô∏è Local Inference Engines (Recommended for full privacy)

| Provider         | Sample Base URL             |
| ---------------- | --------------------------- |
| **Ollama** | `http://localhost:11434/v1` |
| **LM Studio** | `http://localhost:1234/v1`  |
| **vLLM** | `http://localhost:8000/v1`  |
| **llama.cpp** | `http://localhost:8080/v1`  |
| **SGLang** | `http://localhost:30000/v1` |
| **LiteLLM** | `http://localhost:4000`     |

-----

### ‚òÅÔ∏è Cloud Providers

&gt; **üö® A Note on Privacy:** Before choosing a cloud provider, carefully review their privacy and data retention policies. Depending on their terms, your data may be used for their own purposes, including but not limited to human reviews and model training, which can lead to serious consequences if not handled properly.


| Provider         | Base URL                                                   |
| ---------------- | ---------------------------------------------------------- |
| **OpenAI** | `https://api.openai.com/v1`                                |
| **OpenRouter** | `https://openrouter.ai/api/v1`                             |
| **Gemini** | `https://generativelanguage.googleapis.com/v1beta/openai/` |
| **x.AI (Grok)** | `https://api.x.ai/v1`                                      |
| **Groq AI** | `https://api.groq.com/openai/v1`                           |
| **DeepSeek** | `https://api.deepseek.com/v1`                              |
| **SiliconFlow** | `https://api.siliconflow.cn/v1`                            |
| **Zhipu (BigModel)** | `https://open.bigmodel.cn/api/paas/v4/`                |
| **Mistral AI** | `https://api.mistral.ai/v1`                                |




If your provider isn&#039;t on this list, don&#039;t worry! Check their documentation for an OpenAI-compatible endpoint‚Äîchances are, it&#039;s OpenAI Compatible too!

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üîß Ollama Setup (Recommended for full privacy)&lt;/strong&gt;&lt;/summary&gt;

**macOS:**

First, [download Ollama for macOS](https://ollama.com/download/mac).

```bash
# Pull a lightweight model (recommended for consumer hardware)
ollama pull llama3.2:1b
```

**Linux:**

```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama service manually
ollama serve &amp;

# Pull a lightweight model (recommended for consumer hardware)
ollama pull llama3.2:1b
```

&lt;/details&gt;


## ‚≠ê Flexible Configuration

LEANN provides flexible parameters for embedding models, search strategies, and data processing to fit your specific needs.

üìö **Need configuration best practices?** Check our [Configuration Guide](docs/configuration-guide.md) for detailed optimization tips, model selection advice, and solutions to common issues like slow embeddings or poor search quality.

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üìã Click to expand: Common Parameters (Available in All Examples)&lt;/strong&gt;&lt;/summary&gt;

All RAG examples share these common parameters. **Interactive mode** is available in all examples - simply run without `--query` to start a continuous Q&amp;A session where you can ask multiple questions. Type &#039;quit&#039; to exit.

```bash
# Core Parameters (General preprocessing for all examples)
--index-dir DIR              # Directory to store the index (default: current directory)
--query &quot;YOUR QUESTION&quot;      # Single query mode. Omit for interactive chat (type &#039;quit&#039; to exit), and now you can play with your index interactively
--max-items N                # Limit data preprocessing (default: -1, process all data)
--force-rebuild              # Force rebuild index even if it exists

# Embedding Parameters
--embedding-model MODEL      # e.g., facebook/contriever, text-embedding-3-small, mlx-community/Qwen3-Embedding-0.6B-8bit or nomic-embed-text
--embedding-mode MODE        # sentence-transformers, openai, mlx, or ollama

# LLM Parameters (Text generation models)
--llm TYPE                   # LLM backend: openai, ollama, or hf (default: openai)
--llm-model MODEL            # Model name (default: gpt-4o) e.g., gpt-4o-mini, llama3.2:1b, Qwen/Qwen2.5-1.5B-Instruct
--thinking-budget LEVEL      # Thinking budget for reasoning models: low/medium/high (supported by o3, o3-mini, GPT-Oss:20b, and other reasoning models)

# Search Parameters
--top-k N                    # Number of results to retrieve (default: 20)
--search-complexity N        # Search complexity for graph traversal (default: 32)

# Chunking Parameters
--chunk-size N               # Size of text chunks (default varies by source: 256 for most, 192 for WeChat)
--chunk-overlap N            # Overlap between chunks (default varies: 25-128 depending on source)

# Index Building Parameters
--backend-name NAME          # Backend to use: hnsw or diskann (default: hnsw)
--graph-degree N             # Graph degree for index construction (default: 32)
--build-complexity N         # Build complexity for index construction (default: 64)
--compact / --no-compact     # Use compact storage (default: true). Must be `no-compact` for `no-recompute` build.
--recompute / --no-recompute # Enable/disable embedding recomputation (default: enabled). Should not do a `no-recompute` search in a `recompute` build.
```

&lt;/details&gt;

### üìÑ Personal Data Manager: Process Any Documents (`.pdf`, `.txt`, `.md`)!

Ask questions directly about your personal PDFs, documents, and any directory containing your files!

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;videos/paper_clear.gif&quot; alt=&quot;LEANN Document Search Demo&quot; width=&quot;600&quot;&gt;
&lt;/p&gt;

The example below asks a question about summarizing our paper (uses default data in `data/`, which is a directory with diverse data sources: two papers, Pride and Prejudice, and a Technical report about LLM in Huawei in Chinese), and this is the **easiest example** to run here:

```bash
source .venv/bin/activate # Don&#039;t forget to activate the virtual environment
python -m apps.document_rag --query &quot;What are the main techniques LEANN explores?&quot;
```

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üìã Click to expand: Document-Specific Arguments&lt;/strong&gt;&lt;/summary&gt;

#### Parameters
```bash
--data-dir DIR           # Directory containing documents to process (default: data)
--file-types .ext .ext   # Filter by specific file types (optional - all LlamaIndex supported types if omitted)
```

#### Example Commands
```bash
# Process all documents with larger chunks for academic papers
python -m apps.document_rag --data-dir &quot;~/Documents/Papers&quot; --chunk-size 1024

# Filter only markdown and Python files with smaller chunks
python -m apps.document_rag --data-dir &quot;./docs&quot; --chunk-size 256 --file-types .md .py

# Enable AST-aware chunking for code files
python -m apps.document_rag --enable-code-chunking --data-dir &quot;./my_project&quot;

# Or use the specialized code RAG for better code understanding
python -m apps.code_rag --repo-dir &quot;./my_codebase&quot; --query &quot;How does authentication work?&quot;
```

&lt;/details&gt;

### üìß Your Personal Email Secretary: RAG on Apple Mail!

&gt; **Note:** The examples below currently support macOS only. Windows support coming soon.


&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;videos/mail_clear.gif&quot; alt=&quot;LEANN Email Search Demo&quot; width=&quot;600&quot;&gt;
&lt;/p&gt;

Before running the example below, you need to grant full disk access to your terminal/VS Code in System Preferences ‚Üí Privacy &amp; Security ‚Üí Full Disk Access.

```bash
python -m apps.email_rag --query &quot;What&#039;s the food I ordered by DoorDash or Uber Eats mostly?&quot;
```
**780K email chunks ‚Üí 78MB storage.** Finally, search your email like you search Google.

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üìã Click to expand: Email-Specific Arguments&lt;/strong&gt;&lt;/summary&gt;

#### Parameters
```bash
--mail-path PATH         # Path to specific mail directory (auto-detects if omitted)
--include-html          # Include HTML content in processing (useful for newsletters)
```

#### Example Commands
```bash
# Search work emails from a specific account
python -m apps.email_rag --mail-path &quot;~/Library/Mail/V10/WORK_ACCOUNT&quot;

# Find all receipts and order confirmations (includes HTML)
python -m apps.email_rag --query &quot;receipt order confirmation invoice&quot; --include-html
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üìã Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt;

Once the index is built, you can ask questions like:
- &quot;Find emails from my boss about deadlines&quot;
- &quot;What did John say about the project timeline?&quot;
- &quot;Show me emails about travel expenses&quot;
&lt;/details&gt;

### üîç Time Machine for the Web: RAG Your Entire Chrome Browser History!

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;videos/google_clear.gif&quot; alt=&quot;LEANN Browser History Search Demo&quot; width=&quot;600&quot;&gt;
&lt;/p&gt;

```bash
python -m apps.browser_rag --query &quot;Tell me my browser history about machine learning?&quot;
```
**38K browser entries ‚Üí 6MB storage.** Your browser history becomes your personal search engine.

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üìã Click to expand: Browser-Specific Arguments&lt;/strong&gt;&lt;/summary&gt;

#### Parameters
```bash
--chrome-profile PATH    # Path to Chrome profile directory (auto-detects if omitted)
```

#### Example Commands
```bash
# Search academic research from your browsing history
python -m apps.browser_rag --query &quot;arxiv papers machine learning transformer architecture&quot;

# Track competitor analysis across work profile
python -m apps.browser_rag --chrome-profile &quot;~/Library/Application Support/Google/Chrome/Work Profile&quot; --max-items 5000
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üìã Click to expand: How to find your Chrome profile&lt;/strong&gt;&lt;/summary&gt;

The default Chrome profile path is configured for a typical macOS setup. If you need to find your specific Chrome profile:

1. Open Terminal
2. Run: `ls ~/Library/Application\ Support/Google/Chrome/`
3. Look for folders like &quot;Default&quot;, &quot;Profile 1&quot;, &quot;Profile 2&quot;, etc.
4. Use the full path as your `--chrome-profile` argument

**Common Chrome profile locations:**
- macOS: `~/Library/Application Support/Google/Chrome/Default`
- Linux: `~/.config/google-chrome/Default`

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üí¨ Click to expand: Example queries you can try&lt;/strong&gt;&lt;/summary&gt;

Once the index is built, you can ask questions like:

- &quot;What websites did I visit about machine learning?&quot;
- &quot;Find my search history about programming&quot;
- &quot;What YouTube videos did I watch re

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[dgtlmoon/changedetection.io]]></title>
            <link>https://github.com/dgtlmoon/changedetection.io</link>
            <guid>https://github.com/dgtlmoon/changedetection.io</guid>
            <pubDate>Wed, 12 Nov 2025 00:04:35 GMT</pubDate>
            <description><![CDATA[Best and simplest tool for website change detection, web page monitoring, and website change alerts. Perfect for tracking content changes, price drops, restock alerts, and website defacement monitoring‚Äîall for free or enjoy our SaaS plan!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/dgtlmoon/changedetection.io">dgtlmoon/changedetection.io</a></h1>
            <p>Best and simplest tool for website change detection, web page monitoring, and website change alerts. Perfect for tracking content changes, price drops, restock alerts, and website defacement monitoring‚Äîall for free or enjoy our SaaS plan!</p>
            <p>Language: Python</p>
            <p>Stars: 28,547</p>
            <p>Forks: 1,583</p>
            <p>Stars today: 18 stars today</p>
            <h2>README</h2><pre># Detect Website Changes Automatically ‚Äî Monitor Web Page Changes in Real Time

Monitor websites for updates ‚Äî get notified via Discord, Email, Slack, Telegram, Webhook and many more.

**Detect web page content changes and get instant alerts.**  

Ideal for monitoring price changes, content edits, conditional changes and more.


[&lt;img src=&quot;https://raw.githubusercontent.com/dgtlmoon/changedetection.io/master/docs/screenshot.png&quot; style=&quot;max-width:100%;&quot; alt=&quot;Web site page change monitoring&quot;  title=&quot;Web site page change monitoring&quot;  /&gt;](https://changedetection.io?src=github)

[![Release Version][release-shield]][release-link] [![Docker Pulls][docker-pulls]][docker-link] [![License][license-shield]](LICENSE.md)

![changedetection.io](https://github.com/dgtlmoon/changedetection.io/actions/workflows/test-only.yml/badge.svg?branch=master)

[**Get started with website page change monitoring straight away. Don&#039;t have time? Try our $8.99/month subscription, use our proxies and support!**](https://changedetection.io) , _half the price of other website change monitoring services!_


- Chrome browser included.
- Nothing to install, access via browser login after signup.
- Super fast, no registration needed setup.
- Get started watching and receiving website change notifications straight away.
- See our [tutorials and how-to page for more inspiration](https://changedetection.io/tutorials) 

### Target specific parts of the webpage using the Visual Selector tool.

Available when connected to a &lt;a href=&quot;https://github.com/dgtlmoon/changedetection.io/wiki/Playwright-content-fetcher&quot;&gt;playwright content fetcher&lt;/a&gt; (included as part of our subscription service)

[&lt;img src=&quot;https://raw.githubusercontent.com/dgtlmoon/changedetection.io/master/docs/visualselector-anim.gif&quot; style=&quot;max-width:100%;&quot; alt=&quot;Select parts and elements of a web page to monitor for changes&quot;  title=&quot;Select parts and elements of a web page to monitor for changes&quot; /&gt;](https://changedetection.io?src=github)

### Easily see what changed, examine by word, line, or individual character.

[&lt;img src=&quot;https://raw.githubusercontent.com/dgtlmoon/changedetection.io/master/docs/screenshot-diff.png&quot; style=&quot;max-width:100%;&quot; alt=&quot;Self-hosted web page change monitoring context difference &quot;  title=&quot;Self-hosted web page change monitoring context difference &quot; /&gt;](https://changedetection.io?src=github)


### Perform interactive browser steps

Fill in text boxes, click buttons and more, setup your changedetection scenario. 

Using the **Browser Steps** configuration, add basic steps before performing change detection, such as logging into websites, adding a product to a cart, accept cookie logins, entering dates and refining searches.

[&lt;img src=&quot;docs/browsersteps-anim.gif&quot; style=&quot;max-width:100%;&quot; alt=&quot;Website change detection with interactive browser steps, detect changes behind login and password, search queries and more&quot;  title=&quot;Website change detection with interactive browser steps, detect changes behind login and password, search queries and more&quot; /&gt;](https://changedetection.io?src=github)

After **Browser Steps** have been run, then visit the **Visual Selector** tab to refine the content you&#039;re interested in.
Requires Playwright to be enabled.

### Awesome restock and price change notifications

Enable the _&quot;Re-stock &amp; Price detection for single product pages&quot;_ option to activate the best way to monitor product pricing, this will extract any meta-data in the HTML page and give you many options to follow the pricing of the product.

Easily organise and monitor prices for products from the dashboard, get alerts and notifications when the price of a product changes or comes back in stock again!

[&lt;img src=&quot;docs/restock-overview.png&quot; style=&quot;max-width:100%;&quot; alt=&quot;Easily keep an eye on product price changes directly from the UI&quot;  title=&quot;Easily keep an eye on product price changes directly from the UI&quot; /&gt;](https://changedetection.io?src=github)

Set price change notification parameters, upper and lower price, price change percentage and more.
Always know when a product for sale drops in price.

[&lt;img src=&quot;docs/restock-settings.png&quot; style=&quot;max-width:100%;&quot; alt=&quot;Set upper lower and percentage price change notification values&quot;  title=&quot;Set upper lower and percentage price change notification values&quot; /&gt;](https://changedetection.io?src=github)



### Example use cases

- Products and services have a change in pricing
- _Out of stock notification_ and _Back In stock notification_
- Monitor and track PDF file changes, know when a PDF file has text changes.
- Governmental department updates (changes are often only on their websites)
- New software releases, security advisories when you&#039;re not on their mailing list.
- Festivals with changes
- Discogs restock alerts and monitoring
- Realestate listing changes
- Know when your favourite whiskey is on sale, or other special deals are announced before anyone else
- COVID related news from government websites
- University/organisation news from their website
- Detect and monitor changes in JSON API responses 
- JSON API monitoring and alerting
- Changes in legal and other documents
- Trigger API calls via notifications when text appears on a website
- Glue together APIs using the JSON filter and JSON notifications
- Create RSS feeds based on changes in web content
- Monitor HTML source code for unexpected changes, strengthen your PCI compliance
- You have a very sensitive list of URLs to watch and you do _not_ want to use the paid alternatives. (Remember, _you_ are the product)
- Get notified when certain keywords appear in Twitter search results
- Proactively search for jobs, get notified when companies update their careers page, search job portals for keywords.
- Get alerts when new job positions are open on Bamboo HR and other job platforms
- Website defacement monitoring
- Pok√©mon Card Restock Tracker / Pok√©mon TCG Tracker
- RegTech - stay ahead of regulatory changes, regulatory compliance

_Need an actual Chrome runner with Javascript support? We support fetching via WebDriver and Playwright!&lt;/a&gt;_

#### Key Features

- Lots of trigger filters, such as &quot;Trigger on text&quot;, &quot;Remove text by selector&quot;, &quot;Ignore text&quot;, &quot;Extract text&quot;, also using regular-expressions!
- Target elements with xPath 1 and xPath 2, CSS Selectors, Easily monitor complex JSON with JSONPath or jq
- Switch between fast non-JS and Chrome JS based &quot;fetchers&quot;
- Track changes in PDF files (Monitor text changed in the PDF, Also monitor PDF filesize and checksums)
- Easily specify how often a site should be checked
- Execute JS before extracting text (Good for logging in, see examples in the UI!)
- Override Request Headers, Specify `POST` or `GET` and other methods
- Use the &quot;Visual Selector&quot; to help target specific elements
- Configurable [proxy per watch](https://github.com/dgtlmoon/changedetection.io/wiki/Proxy-configuration)
- Send a screenshot with the notification when a change is detected in the web page

We [recommend and use Bright Data](https://brightdata.grsm.io/n0r16zf7eivq) global proxy services, Bright Data will match any first deposit up to $150 using our signup link.

Please :star: star :star: this project and help it grow! https://github.com/dgtlmoon/changedetection.io/

### Conditional web page changes

Easily [configure conditional actions](https://changedetection.io/tutorial/conditional-actions-web-page-changes), for example, only trigger when a price is above or below a preset amount, or [when a web page includes (or does not include) a keyword](https://changedetection.io/tutorial/how-monitor-keywords-any-website)

&lt;img src=&quot;./docs/web-page-change-conditions.png&quot; style=&quot;max-width:80%;&quot; alt=&quot;Conditional web page changes&quot;  title=&quot;Conditional web page changes&quot;  /&gt;

### Schedule web page watches in any timezone, limit by day of week and time.

Easily set a re-check schedule, for example you could limit the web page change detection to only operate during business hours.
Or perhaps based on a foreign timezone (for example, you want to check for the latest news-headlines in a foreign country at 0900 AM),

&lt;img src=&quot;./docs/scheduler.png&quot; style=&quot;max-width:80%;&quot; alt=&quot;How to monitor web page changes according to a schedule&quot;  title=&quot;How to monitor web page changes according to a schedule&quot;  /&gt;

Includes quick short-cut buttons to setup a schedule for **business hours only**, or **weekends**.

### We have a Chrome extension!

Easily add the current web page to your changedetection.io tool, simply install the extension and click &quot;Sync&quot; to connect it to your existing changedetection.io install.

[&lt;img src=&quot;./docs/chrome-extension-screenshot.png&quot; style=&quot;max-width:80%;&quot; alt=&quot;Chrome Extension to easily add the current web-page to detect a change.&quot;  title=&quot;Chrome Extension to easily add the current web-page to detect a change.&quot;  /&gt;](https://chromewebstore.google.com/detail/changedetectionio-website/kefcfmgmlhmankjmnbijimhofdjekbop)

[Goto the Chrome Webstore to download the extension.](https://chromewebstore.google.com/detail/changedetectionio-website/kefcfmgmlhmankjmnbijimhofdjekbop) ( Or check out the [GitHub repo](https://github.com/dgtlmoon/changedetection.io-browser-extension) ) 

## Installation

### Docker

With Docker composer, just clone this repository and..

```bash
$ docker compose up -d
```

Docker standalone
```bash
$ docker run -d --restart always -p &quot;127.0.0.1:5000:5000&quot; -v datastore-volume:/datastore --name changedetection.io dgtlmoon/changedetection.io
```

`:latest` tag is our latest stable release, `:dev` tag is our bleeding edge `master` branch.

Alternative docker repository over at ghcr - [ghcr.io/dgtlmoon/changedetection.io](https://ghcr.io/dgtlmoon/changedetection.io)

### Windows

See the install instructions at the wiki https://github.com/dgtlmoon/changedetection.io/wiki/Microsoft-Windows

### Python Pip

Check out our pypi page https://pypi.org/project/changedetection.io/

```bash
$ pip3 install changedetection.io
$ changedetection.io -d /path/to/empty/data/dir -p 5000
```

Then visit http://127.0.0.1:5000 , You should now be able to access the UI.

_Now with per-site configurable support for using a fast built in HTTP fetcher or use a Chrome based fetcher for monitoring of JavaScript websites!_

## Updating changedetection.io

### Docker
```
docker pull dgtlmoon/changedetection.io
docker kill $(docker ps -a -f name=changedetection.io -q)
docker rm $(docker ps -a -f name=changedetection.io -q)
docker run -d --restart always -p &quot;127.0.0.1:5000:5000&quot; -v datastore-volume:/datastore --name changedetection.io dgtlmoon/changedetection.io
```

### docker compose

```bash
docker compose pull &amp;&amp; docker compose up -d
```

See the wiki for more information https://github.com/dgtlmoon/changedetection.io/wiki


## Filters

XPath(1.0), JSONPath, jq, and CSS support comes baked in! You can be as specific as you need, use XPath exported from various XPath element query creation tools. 
(We support LXML `re:test`, `re:match` and `re:replace`.)

## Notifications

ChangeDetection.io supports a massive amount of notifications (including email, office365, custom APIs, etc) when a web-page has a change detected thanks to the &lt;a href=&quot;https://github.com/caronc/apprise&quot;&gt;apprise&lt;/a&gt; library.
Simply set one or more notification URL&#039;s in the _[edit]_ tab of that watch.

Just some examples

    discord://webhook_id/webhook_token
    flock://app_token/g:channel_id
    gitter://token/room
    gchat://workspace/key/token
    msteams://TokenA/TokenB/TokenC/
    o365://TenantID:AccountEmail/ClientID/ClientSecret/TargetEmail
    rocket://user:password@hostname/#Channel
    mailto://user:pass@example.com?to=receivingAddress@example.com
    json://someserver.com/custom-api
    syslog://
 
&lt;a href=&quot;https://github.com/caronc/apprise#popular-notification-services&quot;&gt;And everything else in this list!&lt;/a&gt;

&lt;img src=&quot;https://raw.githubusercontent.com/dgtlmoon/changedetection.io/master/docs/screenshot-notifications.png&quot; style=&quot;max-width:100%;&quot; alt=&quot;Self-hosted web page change monitoring notifications&quot;  title=&quot;Self-hosted web page change monitoring notifications&quot;  /&gt;

Now you can also customise your notification content and use &lt;a target=&quot;_new&quot; href=&quot;https://jinja.palletsprojects.com/en/3.0.x/templates/&quot;&gt;Jinja2 templating&lt;/a&gt; for their title and body!

## JSON API Monitoring

Detect changes and monitor data in JSON API&#039;s by using either JSONPath or jq to filter, parse, and restructure JSON as needed.

![image](https://raw.githubusercontent.com/dgtlmoon/changedetection.io/master/docs/json-filter-field-example.png)

This will re-parse the JSON and apply formatting to the text, making it super easy to monitor and detect changes in JSON API results

![image](https://raw.githubusercontent.com/dgtlmoon/changedetection.io/master/docs/json-diff-example.png)

### JSONPath or jq?

For more complex parsing, filtering, and modifying of JSON data, jq is recommended due to the built-in operators and functions. Refer to the [documentation](https://stedolan.github.io/jq/manual/) for more specific information on jq.

One big advantage of `jq` is that you can use logic in your JSON filter, such as filters to only show items that have a value greater than/less than etc.

See the wiki https://github.com/dgtlmoon/changedetection.io/wiki/JSON-Selector-Filter-help for more information and examples

### Parse JSON embedded in HTML!

When you enable a `json:` or `jq:` filter, you can even automatically extract and parse embedded JSON inside a HTML page! Amazingly handy for sites that build content based on JSON, such as many e-commerce websites. 

```
&lt;html&gt;
...
&lt;script type=&quot;application/ld+json&quot;&gt;

{
   &quot;@context&quot;:&quot;http://schema.org/&quot;,
   &quot;@type&quot;:&quot;Product&quot;,
   &quot;offers&quot;:{
      &quot;@type&quot;:&quot;Offer&quot;,
      &quot;availability&quot;:&quot;http://schema.org/InStock&quot;,
      &quot;price&quot;:&quot;3949.99&quot;,
      &quot;priceCurrency&quot;:&quot;USD&quot;,
      &quot;url&quot;:&quot;https://www.newegg.com/p/3D5-000D-001T1&quot;
   },
   &quot;description&quot;:&quot;Cobratype King Cobra Hero Desktop Gaming PC&quot;,
   &quot;name&quot;:&quot;Cobratype King Cobra Hero Desktop Gaming PC&quot;,
   &quot;sku&quot;:&quot;3D5-000D-001T1&quot;,
   &quot;itemCondition&quot;:&quot;NewCondition&quot;
}
&lt;/script&gt;
```  

`json:$..price` or `jq:..price` would give `3949.99`, or you can extract the whole structure (use a JSONpath test website to validate with)

The application also supports notifying you that it can follow this information automatically


## Proxy Configuration

See the wiki https://github.com/dgtlmoon/changedetection.io/wiki/Proxy-configuration , we also support using [Bright Data proxy services where possible](https://github.com/dgtlmoon/changedetection.io/wiki/Proxy-configuration#brightdata-proxy-support) and [Oxylabs](https://oxylabs.go2cloud.org/SH2d) proxy services.

## Raspberry Pi support?

Raspberry Pi and linux/arm/v6 linux/arm/v7 arm64 devices are supported! See the wiki for [details](https://github.com/dgtlmoon/changedetection.io/wiki/Fetching-pages-with-WebDriver)

## Import support

Easily [import your list of websites to watch for changes in Excel .xslx file format](https://changedetection.io/tutorial/how-import-your-website-change-detection-lists-excel), or paste in lists of website URLs as plaintext. 

Excel import is recommended - that way you can better organise tags/groups of websites and other features.


## API Support

Full REST API for programmatic management of watches, tags, notifications and more. 

- **[Interactive API Documentation](https://changedetection.io/docs/api_v1/index.html)** - Complete API reference with live testing
- **[OpenAPI Specification](docs/api-spec.yaml)** - Generate SDKs for any programming language

## Support us

Do you use changedetection.io to make money? does it save you time or money? Does it make your life easier? less stressful? Remember, we write this software when we should be doing actual paid work, we have to buy food and pay rent just like you.


Consider taking out an officially supported [website change detection subscription](https://changedetection.io?src=github) , even if you don&#039;t use it, you still get the warm fuzzy feeling of helping out the project. (And who knows, you might just use it!)

## Commercial Support

I offer commercial support, this software is depended on by network security, aerospace , data-science and data-journalist professionals just to name a few, please reach out at dgtlmoon@gmail.com for any enquiries, I am more than glad to work with your organisation to further the possibilities of what can be done with changedetection.io


[release-shield]: https://img.shields.io:/github/v/release/dgtlmoon/changedetection.io?style=for-the-badge
[docker-pulls]: https://img.shields.io/docker/pulls/dgtlmoon/changedetection.io?style=for-the-badge
[test-shield]: https://github.com/dgtlmoon/changedetection.io/actions/workflows/test-only.yml/badge.svg?branch=master

[license-shield]: https://img.shields.io/github/license/dgtlmoon/changedetection.io.svg?style=for-the-badge
[release-link]: https://github.com/dgtlmoon/changedetection.io/releases
[docker-link]: https://hub.docker.com/r/dgtlmoon/changedetection.io

## Commercial Licencing

If you are reselling this software either in part or full as part of any commercial arrangement, you must abide by our COMMERCIAL_LICENCE.md found in our code repository, please contact dgtlmoon@gmail.com and contact@changedetection.io .

## Third-party licenses

changedetectionio.html_tools.elementpath_tostring: Copyright (c), 2018-2021, SISSA (Scuola Internazionale Superiore di Studi Avanzati), Licensed under [MIT license](https://github.com/sissaschool/elementpath/blob/master/LICENSE)

## Contributors

Recognition of fantastic contributors to the project

- Constantin Hong https://github.com/Constantin1489
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[AtsushiSakai/PythonRobotics]]></title>
            <link>https://github.com/AtsushiSakai/PythonRobotics</link>
            <guid>https://github.com/AtsushiSakai/PythonRobotics</guid>
            <pubDate>Wed, 12 Nov 2025 00:04:34 GMT</pubDate>
            <description><![CDATA[Python sample codes and textbook for robotics algorithms.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/AtsushiSakai/PythonRobotics">AtsushiSakai/PythonRobotics</a></h1>
            <p>Python sample codes and textbook for robotics algorithms.</p>
            <p>Language: Python</p>
            <p>Stars: 26,397</p>
            <p>Forks: 6,953</p>
            <p>Stars today: 32 stars today</p>
            <h2>README</h2><pre>&lt;img src=&quot;https://github.com/AtsushiSakai/PythonRobotics/raw/master/icon.png?raw=true&quot; align=&quot;right&quot; width=&quot;300&quot; alt=&quot;header pic&quot;/&gt;

# PythonRobotics
![GitHub_Action_Linux_CI](https://github.com/AtsushiSakai/PythonRobotics/workflows/Linux_CI/badge.svg)
![GitHub_Action_MacOS_CI](https://github.com/AtsushiSakai/PythonRobotics/workflows/MacOS_CI/badge.svg)
![GitHub_Action_Windows_CI](https://github.com/AtsushiSakai/PythonRobotics/workflows/Windows_CI/badge.svg)
[![Build status](https://ci.appveyor.com/api/projects/status/sb279kxuv1be391g?svg=true)](https://ci.appveyor.com/project/AtsushiSakai/pythonrobotics)

Python codes and [textbook](https://atsushisakai.github.io/PythonRobotics/index.html) for robotics algorithm.


# Table of Contents
   * [What is this?](#what-is-this)
   * [Requirements](#requirements)
   * [Documentation](#documentation)
   * [How to use](#how-to-use)
   * [Localization](#localization)
      * [Extended Kalman Filter localization](#extended-kalman-filter-localization)
      * [Particle filter localization](#particle-filter-localization)
      * [Histogram filter localization](#histogram-filter-localization)
   * [Mapping](#mapping)
      * [Gaussian grid map](#gaussian-grid-map)
      * [Ray casting grid map](#ray-casting-grid-map)
      * [Lidar to grid map](#lidar-to-grid-map)
      * [k-means object clustering](#k-means-object-clustering)
      * [Rectangle fitting](#rectangle-fitting)
   * [SLAM](#slam)
      * [Iterative Closest Point (ICP) Matching](#iterative-closest-point-icp-matching)
      * [FastSLAM 1.0](#fastslam-10)
   * [Path Planning](#path-planning)
      * [Dynamic Window Approach](#dynamic-window-approach)
      * [Grid based search](#grid-based-search)
         * [Dijkstra algorithm](#dijkstra-algorithm)
         * [A* algorithm](#a-algorithm)
         * [D* algorithm](#d-algorithm)
         * [D* Lite algorithm](#d-lite-algorithm)
         * [Potential Field algorithm](#potential-field-algorithm)
         * [Grid based coverage path planning](#grid-based-coverage-path-planning)
         * [Particle Swarm Optimization (PSO)](#particle-swarm-optimization-pso)  
      * [State Lattice Planning](#state-lattice-planning)
         * [Biased polar sampling](#biased-polar-sampling)
         * [Lane sampling](#lane-sampling)
      * [Probabilistic Road-Map (PRM) planning](#probabilistic-road-map-prm-planning)
      * [Rapidly-Exploring Random Trees (RRT)](#rapidly-exploring-random-trees-rrt)
         * [RRT*](#rrt)
         * [RRT* with reeds-shepp path](#rrt-with-reeds-shepp-path)
         * [LQR-RRT*](#lqr-rrt)
      * [Quintic polynomials planning](#quintic-polynomials-planning)
      * [Reeds Shepp planning](#reeds-shepp-planning)
      * [LQR based path planning](#lqr-based-path-planning)
      * [Optimal Trajectory in a Frenet Frame](#optimal-trajectory-in-a-frenet-frame)
   * [Path Tracking](#path-tracking)
      * [move to a pose control](#move-to-a-pose-control)
      * [Stanley control](#stanley-control)
      * [Rear wheel feedback control](#rear-wheel-feedback-control)
      * [Linear‚Äìquadratic regulator (LQR) speed and steering control](#linearquadratic-regulator-lqr-speed-and-steering-control)
      * [Model predictive speed and steering control](#model-predictive-speed-and-steering-control)
      * [Nonlinear Model predictive control with C-GMRES](#nonlinear-model-predictive-control-with-c-gmres)
   * [Arm Navigation](#arm-navigation)
      * [N joint arm to point control](#n-joint-arm-to-point-control)
      * [Arm navigation with obstacle avoidance](#arm-navigation-with-obstacle-avoidance)
   * [Aerial Navigation](#aerial-navigation)
      * [drone 3d trajectory following](#drone-3d-trajectory-following)
      * [rocket powered landing](#rocket-powered-landing)
   * [Bipedal](#bipedal)
      * [bipedal planner with inverted pendulum](#bipedal-planner-with-inverted-pendulum)
   * [License](#license)
   * [Use-case](#use-case)
   * [Contribution](#contribution)
   * [Citing](#citing)
   * [Support](#support)
   * [Sponsors](#sponsors)
      * [JetBrains](#JetBrains)
      * [1Password](#1password)
   * [Authors](#authors)

# What is PythonRobotics?

PythonRobotics is a Python code collection and a [textbook](https://atsushisakai.github.io/PythonRobotics/index.html) of robotics algorithms.

Features:

1. Easy to read for understanding each algorithm&#039;s basic idea.

2. Widely used and practical algorithms are selected.

3. Minimum dependency.

See this documentation 

- [Getting Started ‚Äî PythonRobotics documentation](https://atsushisakai.github.io/PythonRobotics/modules/0_getting_started/1_what_is_python_robotics.html)

or this Youtube video:

- [PythonRobotics project audio overview](https://www.youtube.com/watch?v=uMeRnNoJAfU)

or this paper for more details:

- [\[1808\.10703\] PythonRobotics: a Python code collection of robotics algorithms](https://arxiv.org/abs/1808.10703) ([BibTeX](https://github.com/AtsushiSakai/PythonRoboticsPaper/blob/master/python_robotics.bib))


# Requirements to run the code

For running each sample code:

- [Python 3.13.x](https://www.python.org/)
 
- [NumPy](https://numpy.org/)
 
- [SciPy](https://scipy.org/)
 
- [Matplotlib](https://matplotlib.org/)
 
- [cvxpy](https://www.cvxpy.org/) 

For development:
  
- [pytest](https://pytest.org/) (for unit tests)
  
- [pytest-xdist](https://pypi.org/project/pytest-xdist/) (for parallel unit tests)
  
- [mypy](https://mypy-lang.org/) (for type check)
  
- [sphinx](https://www.sphinx-doc.org/) (for document generation)
  
- [pycodestyle](https://pypi.org/project/pycodestyle/) (for code style check)

# Documentation (Textbook)

This README only shows some examples of this project. 

If you are interested in other examples or mathematical backgrounds of each algorithm, 

You can check the full documentation (textbook) online: [Welcome to PythonRobotics‚Äôs documentation\! ‚Äî PythonRobotics documentation](https://atsushisakai.github.io/PythonRobotics/index.html)

All animation gifs are stored here: [AtsushiSakai/PythonRoboticsGifs: Animation gifs of PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs)

# How to use

1. Clone this repo.

   ```terminal
   git clone https://github.com/AtsushiSakai/PythonRobotics.git
   ```


2. Install the required libraries.

- using conda :

  ```terminal
  conda env create -f requirements/environment.yml
  ```
 
- using pip :

  ```terminal
  pip install -r requirements/requirements.txt
  ```


3. Execute python script in each directory.

4. Add star to this repo if you like it :smiley:. 

# Localization

## Extended Kalman Filter localization

&lt;img src=&quot;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/extended_kalman_filter/animation.gif&quot; width=&quot;640&quot; alt=&quot;EKF pic&quot;&gt;

Reference

- [documentation](https://atsushisakai.github.io/PythonRobotics/modules/2_localization/extended_kalman_filter_localization_files/extended_kalman_filter_localization.html)

## Particle filter localization

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/particle_filter/animation.gif)

This is a sensor fusion localization with Particle Filter(PF).

The blue line is true trajectory, the black line is dead reckoning trajectory,

and the red line is an estimated trajectory with PF.

It is assumed that the robot can measure a distance from landmarks (RFID).

These measurements are used for PF localization.

Reference

- [PROBABILISTIC ROBOTICS](http://www.probabilistic-robotics.org/)


## Histogram filter localization

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/histogram_filter/animation.gif)

This is a 2D localization example with Histogram filter.

The red cross is true position, black points are RFID positions.

The blue grid shows a position probability of histogram filter.  

In this simulation, x,y are unknown, yaw is known.

The filter integrates speed input and range observations from RFID for localization.

Initial position is not needed.

Reference

- [PROBABILISTIC ROBOTICS](http://www.probabilistic-robotics.org/)

# Mapping

## Gaussian grid map

This is a 2D Gaussian grid mapping example.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/gaussian_grid_map/animation.gif)

## Ray casting grid map

This is a 2D ray casting grid mapping example.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/raycasting_grid_map/animation.gif)

## Lidar to grid map

This example shows how to convert a 2D range measurement to a grid map.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/lidar_to_grid_map/animation.gif)

## k-means object clustering

This is a 2D object clustering with k-means algorithm.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/kmeans_clustering/animation.gif)

## Rectangle fitting

This is a 2D rectangle fitting for vehicle detection.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/rectangle_fitting/animation.gif)


# SLAM

Simultaneous Localization and Mapping(SLAM) examples

## Iterative Closest Point (ICP) Matching

This is a 2D ICP matching example with singular value decomposition.

It can calculate a rotation matrix, and a translation vector between points and points.

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/iterative_closest_point/animation.gif)

Reference

- [Introduction to Mobile Robotics: Iterative Closest Point Algorithm](https://cs.gmu.edu/~kosecka/cs685/cs685-icp.pdf)


## FastSLAM 1.0

This is a feature based SLAM example using FastSLAM 1.0.

The blue line is ground truth, the black line is dead reckoning, the red line is the estimated trajectory with FastSLAM.

The red points are particles of FastSLAM.

Black points are landmarks, blue crosses are estimated landmark positions by FastSLAM.


![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/FastSLAM1/animation.gif)


Reference

- [PROBABILISTIC ROBOTICS](http://www.probabilistic-robotics.org/)

- [SLAM simulations by Tim Bailey](http://www-personal.acfr.usyd.edu.au/tbailey/software/slam_simulations.htm)


# Path Planning

## Dynamic Window Approach

This is a 2D navigation sample code with Dynamic Window Approach.

- [The Dynamic Window Approach to Collision Avoidance](https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf)

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DynamicWindowApproach/animation.gif)


## Grid based search

### Dijkstra algorithm

This is a 2D grid based the shortest path planning with Dijkstra&#039;s algorithm.

![PythonRobotics/figure_1.png at master ¬∑ AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/Dijkstra/animation.gif)

In the animation, cyan points are searched nodes.

### A\* algorithm

This is a 2D grid based the shortest path planning with A star algorithm.

![PythonRobotics/figure_1.png at master ¬∑ AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/AStar/animation.gif)

In the animation, cyan points are searched nodes.

Its heuristic is 2D Euclid distance.

### D\* algorithm

This is a 2D grid based the shortest path planning with D star algorithm.

![figure at master ¬∑ nirnayroy/intelligentrobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DStar/animation.gif)

The animation shows a robot finding its path avoiding an obstacle using the D* search algorithm.

Reference

- [D* Algorithm Wikipedia](https://en.wikipedia.org/wiki/D*)

### D\* Lite algorithm

This algorithm finds the shortest path between two points while rerouting when obstacles are discovered. It has been implemented here for a 2D grid.

![D* Lite](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DStarLite/animation.gif)

The animation shows a robot finding its path and rerouting to avoid obstacles as they are discovered using the D* Lite search algorithm.

Refs:

- [D* Lite](http://idm-lab.org/bib/abstracts/papers/aaai02b.pdf)
- [Improved Fast Replanning for Robot Navigation in Unknown Terrain](http://www.cs.cmu.edu/~maxim/files/dlite_icra02.pdf)

### Potential Field algorithm

This is a 2D grid based path planning with Potential Field algorithm.

![PotentialField](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/PotentialFieldPlanning/animation.gif)

In the animation, the blue heat map shows potential value on each grid.

Reference

- [Robotic Motion Planning:Potential Functions](https://www.cs.cmu.edu/~motionplanning/lecture/Chap4-Potential-Field_howie.pdf)

### Grid based coverage path planning

This is a 2D grid based coverage path planning simulation.

![PotentialField](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/GridBasedSweepCPP/animation.gif)

### Particle Swarm Optimization (PSO)

This is a 2D path planning simulation using the Particle Swarm Optimization algorithm.

![PSO](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ParticleSwarmOptimization/animation.gif)

PSO is a metaheuristic optimization algorithm inspired by bird flocking behavior. In path planning, particles explore the search space to find collision-free paths while avoiding obstacles.

The animation shows particles (blue dots) converging towards the optimal path (yellow line) from start (green area) to goal (red star).

References

- [Particle swarm optimization - Wikipedia](https://en.wikipedia.org/wiki/Particle_swarm_optimization)

- [Kennedy, J.; Eberhart, R. (1995). &quot;Particle Swarm Optimization&quot;](https://ieeexplore.ieee.org/document/488968)



## State Lattice Planning

This script is a path planning code with state lattice planning.

This code uses the model predictive trajectory generator to solve boundary problem.

Reference 

- [Optimal rough terrain trajectory generation for wheeled mobile robots](https://journals.sagepub.com/doi/pdf/10.1177/0278364906075328)

- [State Space Sampling of Feasible Motions for High-Performance Mobile Robot Navigation in Complex Environments](https://www.cs.cmu.edu/~alonzo/pubs/papers/JFR_08_SS_Sampling.pdf)


### Biased polar sampling

![PythonRobotics/figure_1.png at master ¬∑ AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/BiasedPolarSampling.gif)


### Lane sampling

![PythonRobotics/figure_1.png at master ¬∑ AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/LaneSampling.gif)

## Probabilistic Road-Map (PRM) planning 

![PRM](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ProbabilisticRoadMap/animation.gif)

This PRM planner uses Dijkstra method for graph search.

In the animation, blue points are sampled points,

Cyan crosses means searched points with Dijkstra method,

The red line is the final path of PRM.

Reference

- [Probabilistic roadmap \- Wikipedia](https://en.wikipedia.org/wiki/Probabilistic_roadmap)

„ÄÄ„ÄÄ

## Rapidly-Exploring Random Trees (RRT)

### RRT\*

![PythonRobotics/figure_1.png at master ¬∑ AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTstar/animation.gif)

This is a path planning code with RRT\*

Black circles are obstacles, green line is a searched tree, red crosses are start and goal positions.

Reference

- [Incremental Sampling-based Algorithms for Optimal Motion Planning](https://arxiv.org/abs/1005.0416)

- [Sampling-based Algorithms for Optimal Motion Planning](https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=bddbc99f97173430aa49a0ada53ab5bade5902fa)

### RRT\* with reeds-shepp path

![Robotics/animation.gif at master ¬∑ AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTStarReedsShepp/animation.gif)

Path planning for a car robot with RRT\* and reeds shepp path planner.

### LQR-RRT\*

This is a path planning simulation with LQR-RRT\*.

A double integrator motion model is used for LQR local planner.

![LQR_RRT](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRRRTStar/animation.gif)

Reference

- [LQR\-RRT\*: Optimal Sampling\-Based Motion Planning with Automatically Derived Extension Heuristics](https://lis.csail.mit.edu/pubs/perez-icra12.pdf)

- [MahanFathi/LQR\-RRTstar: LQR\-RRT\* method is used for random motion planning of a simple pendulum in its phase plot](https://github.com/MahanFathi/LQR-RRTstar)


## Quintic polynomials planning

Motion planning with quintic polynomials.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/QuinticPolynomialsPlanner/animation.gif)

It can calculate a 2D path, velocity, and acceleration profile based on quintic polynomials.

Reference

- [Local Path Planning And Motion Control For Agv In Positioning](https://ieeexplore.ieee.org/document/637936/)

## Reeds Shepp planning

A sample code with Reeds Shepp path planning.

![RSPlanning](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ReedsSheppPath/animation.gif?raw=true)

Reference

- [15.3.2 Reeds\-Shepp Curves](http://planning.cs.uiuc.edu/node822.html) 

- [optimal paths for a car that goes both forwards and backwards](https://pdfs.semanticscholar.org/932e/c495b1d0018fd59dee12a0bf74434fac7af4.pdf)

- [ghliu/pyReedsShepp: Implementation of Reeds Shepp curve\.](https://github.com/ghliu/pyReedsShepp)


## LQR based path planning

A sample code using LQR based path planning for double integrator model.

![RSPlanning](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRPlanner/animation.gif?raw=true)


## Optimal Trajectory in a Frenet Frame 

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/FrenetOptimalTrajectory/animation.gif)

This is optimal trajectory generation in a Frenet Frame.

The cyan line is the target course and black crosses are obstacles.

The red line is the predicted path.

Reference

- [Optimal Trajectory Generation for Dynamic Street Scenarios in a Frenet Frame](https://www.researchgate.net/profile/Moritz_Werling/publication/224156269_Optimal_Trajectory_Generation_for_Dynamic_Street_Scenarios_in_a_Frenet_Frame/links/54f749df0cf210398e9277af.pdf)

- [Optimal trajectory generation for dynamic street scenarios in a Frenet Frame](https://www.youtube.com/watch?v=Cj6tAQe7UCY)


# Path Tracking

## move to a pose control

This is a simulation of moving to a pose control

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Control/move_to_pose/animation.gif)

Reference

- [P. I. Corke, &quot;Robotics, Vision and Control&quot; \| SpringerLink p102](https://link.springer.com/book/10.1007/978-3-642-20144-8)


## Stanley control

Path tracking simulation with Stanley steering control and PID speed control.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/stanley_controller/animation.gif)

Reference

- [Stanley: The robot that won the DARPA grand challenge](http://robots.stanford.edu/papers/thrun.stanley05.pdf)

- [Automatic Steering Methods for Autonomous Automobile Path Tracking](https://www.ri.cmu.edu/pub_files/2009/2/Automatic_Steering_Methods_for_Autonomous_Automobile_Path_Tracking.pdf)



## Rear wheel feedback control

Path tracking simulation with rear wheel feedback steering control and PID speed control.

![PythonRobotics/figure_1.png at master ¬∑ AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/rear_wheel_feedback/animation.gif)

Reference

- [A Survey of Motion Planning and Control Techniques for Self-driving Urban Vehicles](https://arxiv.org/abs/1604.07446)


## Linear‚Äìquadratic regulator (LQR) speed and steering control

Path tracking simulation with LQR speed

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jingyaogong/minimind]]></title>
            <link>https://github.com/jingyaogong/minimind</link>
            <guid>https://github.com/jingyaogong/minimind</guid>
            <pubDate>Wed, 12 Nov 2025 00:04:33 GMT</pubDate>
            <description><![CDATA[üöÄüöÄ „ÄåÂ§ßÊ®°Âûã„Äç2Â∞èÊó∂ÂÆåÂÖ®‰ªé0ËÆ≠ÁªÉ26MÁöÑÂ∞èÂèÇÊï∞GPTÔºÅüåè Train a 26M-parameter GPT from scratch in just 2h!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jingyaogong/minimind">jingyaogong/minimind</a></h1>
            <p>üöÄüöÄ „ÄåÂ§ßÊ®°Âûã„Äç2Â∞èÊó∂ÂÆåÂÖ®‰ªé0ËÆ≠ÁªÉ26MÁöÑÂ∞èÂèÇÊï∞GPTÔºÅüåè Train a 26M-parameter GPT from scratch in just 2h!</p>
            <p>Language: Python</p>
            <p>Stars: 33,293</p>
            <p>Forks: 3,876</p>
            <p>Stars today: 300 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

![logo](./images/logo.png)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

![visitors](https://visitor-badge.laobi.icu/badge?page_id=jingyaogong/minimind)
[![GitHub Repo stars](https://img.shields.io/github/stars/jingyaogong/minimind?style=social)](https://github.com/jingyaogong/minimind/stargazers)
[![GitHub Code License](https://img.shields.io/github/license/jingyaogong/minimind)](LICENSE)
[![GitHub last commit](https://img.shields.io/github/last-commit/jingyaogong/minimind)](https://github.com/jingyaogong/minimind/commits/master)
[![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)](https://github.com/jingyaogong/minimind/pulls)
[![Collection](https://img.shields.io/badge/ü§ó-MiniMind%20%20Collection-blue)](https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

![GitHub Trend](https://trendshift.io/api/badge/repositories/12586)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;h3&gt;&quot;Â§ßÈÅìËá≥ÁÆÄ&quot;&lt;/h3&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

‰∏≠Êñá | [English](./README_en.md)

&lt;/div&gt;

* Ê≠§ÂºÄÊ∫êÈ°πÁõÆÊó®Âú®ÂÆåÂÖ®‰ªé0ÂºÄÂßãÔºå‰ªÖÁî®3ÂùóÈí±ÊàêÊú¨ + 2Â∞èÊó∂ÔºÅÂç≥ÂèØËÆ≠ÁªÉÂá∫‰ªÖ‰∏∫25.8MÁöÑË∂ÖÂ∞èËØ≠Ë®ÄÊ®°Âûã**MiniMind**„ÄÇ
* **MiniMind**Á≥ªÂàóÊûÅÂÖ∂ËΩªÈáèÔºåÊúÄÂ∞èÁâàÊú¨‰ΩìÁßØÊòØ GPT-3 ÁöÑ $\frac{1}{7000}$ÔºåÂäõÊ±ÇÂÅöÂà∞ÊúÄÊôÆÈÄöÁöÑ‰∏™‰∫∫GPU‰πüÂèØÂø´ÈÄüËÆ≠ÁªÉ„ÄÇ
* È°πÁõÆÂêåÊó∂ÂºÄÊ∫ê‰∫ÜÂ§ßÊ®°ÂûãÁöÑÊûÅÁÆÄÁªìÊûÑ-ÂåÖÂê´ÊãìÂ±ïÂÖ±‰∫´Ê∑∑Âêà‰∏ìÂÆ∂(MoE)„ÄÅÊï∞ÊçÆÈõÜÊ∏ÖÊ¥ó„ÄÅÈ¢ÑËÆ≠ÁªÉ(Pretrain)„ÄÅÁõëÁù£ÂæÆË∞É(SFT)„ÄÅLoRAÂæÆË∞É„ÄÅÁõ¥Êé•ÂÅèÂ•Ω‰ºòÂåñ(DPO)„ÄÅÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉ(RLAIF: PPO/GRPOÁ≠â)„ÄÅÊ®°ÂûãËí∏È¶èÁ≠âÂÖ®ËøáÁ®ã‰ª£Á†Å„ÄÇ
* **MiniMind**ÂêåÊó∂ÊãìÂ±ï‰∫ÜËßÜËßâÂ§öÊ®°ÊÄÅÁöÑVLM: [MiniMind-V](https://github.com/jingyaogong/minimind-v)„ÄÇ
* È°πÁõÆÊâÄÊúâÊ†∏ÂøÉÁÆóÊ≥ï‰ª£Á†ÅÂùá‰ªé0‰ΩøÁî®PyTorchÂéüÁîüÈáçÊûÑÔºÅ‰∏ç‰æùËµñÁ¨¨‰∏âÊñπÂ∫ìÊèê‰æõÁöÑÊäΩË±°Êé•Âè£„ÄÇ
* Ëøô‰∏ç‰ªÖÊòØÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÖ®Èò∂ÊÆµÂºÄÊ∫êÂ§çÁé∞Ôºå‰πüÊòØ‰∏Ä‰∏™ÂÖ•Èó®LLMÁöÑÊïôÁ®ã„ÄÇ
* Â∏åÊúõÊ≠§È°πÁõÆËÉΩ‰∏∫ÊâÄÊúâ‰∫∫Êèê‰æõ‰∏Ä‰∏™ÊäõÁ†ñÂºïÁéâÁöÑÁ§∫‰æãÔºå‰∏ÄËµ∑ÊÑüÂèóÂàõÈÄ†ÁöÑ‰πêË∂£ÔºÅÊé®Âä®Êõ¥ÂπøÊ≥õAIÁ§æÂå∫ÁöÑËøõÊ≠•ÔºÅ

&gt; ‰∏∫Èò≤Ê≠¢ËØØËß£Ôºå‚Äú2Â∞èÊó∂‚Äù Âü∫‰∫éNVIDIA 3090Á°¨‰ª∂ËÆæÂ§áÔºàÂçïÂç°ÔºâÊµãËØïÔºå‚Äú3ÂùóÈí±‚ÄùÊåáGPUÊúçÂä°Âô®ÁßüÁî®ÊàêÊú¨ÔºåÂÖ∑‰ΩìËßÑÊ†ºËØ¶ÊÉÖËßÅ‰∏ãÊñá„ÄÇ

---


&lt;div align=&quot;center&quot;&gt;

![minimind2](./images/minimind2.gif)

[üîóüçìÊé®ÁêÜÊ®°Âûã](https://www.modelscope.cn/studios/gongjy/MiniMind-Reasoning) | [üîóü§ñÂ∏∏ËßÑÊ®°Âûã](https://www.modelscope.cn/studios/gongjy/MiniMind) | [üîóüéûÔ∏èËßÜÈ¢ë‰ªãÁªç](https://www.bilibili.com/video/BV12dHPeqE72/?share_source=copy_web&amp;vd_source=670c2504f88726f8cf4a21ef6147c0e8)


&lt;div align=&quot;center&quot;&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://huggingface.co/collections/jingyaogong/minimind-66caf8d999f5c7fa64f399e5&quot; style=&quot;text-decoration: none;&quot;&gt;
          &lt;img src=&quot;./images/and_huggingface.png&quot; alt=&quot;Hugging Face Logo&quot; style=&quot;vertical-align: middle; width: auto; max-width: 100%;&quot; /&gt;
        &lt;/a&gt;
      &lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://www.modelscope.cn/profile/gongjy&quot; style=&quot;text-decoration: none;&quot;&gt;
          &lt;img src=&quot;./images/and_modelscope.png&quot; alt=&quot;ModelScope Logo&quot; style=&quot;vertical-align: middle; width: auto; max-width: 100%;&quot; /&gt;
        &lt;/a&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;


&lt;/div&gt;

# üìå Introduction

Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLarge Language Model, LLMÔºâÁöÑÂá∫Áé∞ÂºïÂèë‰∫ÜÂÖ®‰∏ñÁïåÂØπAIÁöÑÁ©∫ÂâçÂÖ≥Ê≥®„ÄÇ
Êó†ËÆ∫ÊòØChatGPT„ÄÅDeepSeekËøòÊòØQwenÔºåÈÉΩ‰ª•ÂÖ∂ÊÉäËâ≥ÁöÑÊïàÊûú‰ª§‰∫∫Âèπ‰∏∫ËßÇÊ≠¢„ÄÇ
ÁÑ∂ËÄåÔºåÂä®ËæÑÊï∞Áôæ‰∫øÂèÇÊï∞ÁöÑÂ∫ûÂ§ßËßÑÊ®°Ôºå‰ΩøÂæóÂÆÉ‰ª¨ÂØπ‰∏™‰∫∫ËÆæÂ§áËÄåË®Ä‰∏ç‰ªÖÈöæ‰ª•ËÆ≠ÁªÉÔºåÁîöËá≥ËøûÈÉ®ÁΩ≤ÈÉΩÊòæÂæóÈÅ•‰∏çÂèØÂèä„ÄÇ
ÊâìÂºÄÂ§ßÊ®°ÂûãÁöÑ‚ÄúÈªëÁõíÂ≠ê‚ÄùÔºåÊé¢Á¥¢ÂÖ∂ÂÜÖÈÉ®Ëøê‰ΩúÊú∫Âà∂ÔºåÂ§ö‰πà‰ª§‰∫∫ÂøÉÊΩÆÊæéÊπÉÔºÅ
ÈÅóÊÜæÁöÑÊòØÔºå99%ÁöÑÊé¢Á¥¢Âè™ËÉΩÊ≠¢Ê≠•‰∫é‰ΩøÁî®LoRAÁ≠âÊäÄÊúØÂØπÁé∞ÊúâÂ§ßÊ®°ÂûãËøõË°åÂ∞ëÈáèÂæÆË∞ÉÔºåÂ≠¶‰π†‰∏Ä‰∫õÊñ∞Êåá‰ª§Êàñ‰ªªÂä°„ÄÇ
ËøôÂ∞±Â•ΩÊØîÊïôÁâõÈ°øÂ¶Ç‰Ωï‰ΩøÁî®21‰∏ñÁ∫™ÁöÑÊô∫ËÉΩÊâãÊú∫‚Äî‚ÄîËôΩÁÑ∂ÊúâË∂£ÔºåÂç¥ÂÆåÂÖ®ÂÅèÁ¶ª‰∫ÜÁêÜËß£Áâ©ÁêÜÊú¨Ë¥®ÁöÑÂàùË°∑„ÄÇ
‰∏éÊ≠§ÂêåÊó∂ÔºåÁ¨¨‰∏âÊñπÁöÑÂ§ßÊ®°ÂûãÊ°ÜÊû∂ÂíåÂ∑•ÂÖ∑Â∫ìÔºåÂ¶Çtransformers+trlÔºåÂá†‰πéÂè™Êö¥Èú≤‰∫ÜÈ´òÂ∫¶ÊäΩË±°ÁöÑÊé•Âè£„ÄÇ
ÈÄöËøáÁü≠Áü≠10Ë°å‰ª£Á†ÅÔºåÂ∞±ËÉΩÂÆåÊàê‚ÄúÂä†ËΩΩÊ®°Âûã+Âä†ËΩΩÊï∞ÊçÆÈõÜ+Êé®ÁêÜ+Âº∫ÂåñÂ≠¶‰π†‚ÄùÁöÑÂÖ®ÊµÅÁ®ãËÆ≠ÁªÉ„ÄÇ
ËøôÁßçÈ´òÊïàÁöÑÂ∞ÅË£ÖÂõ∫ÁÑ∂‰æøÂà©Ôºå‰ΩÜ‰πüÂÉè‰∏ÄÊû∂È´òÈÄüÈ£ûËàπÔºåÂ∞ÜÂºÄÂèëËÄÖ‰∏éÂ∫ïÂ±ÇÂÆûÁé∞ÈöîÁ¶ªÂºÄÊù•ÔºåÈòªÁ¢ç‰∫ÜÊ∑±ÂÖ•Êé¢Á©∂LLMÊ†∏ÂøÉ‰ª£Á†ÅÁöÑÊú∫‰ºö„ÄÇ
ÁÑ∂ËÄåÔºå‚ÄúÁî®‰πêÈ´òÊãºÂá∫‰∏ÄÊû∂È£ûÊú∫ÔºåËøúÊØîÂùêÂú®Â§¥Á≠âËà±ÈáåÈ£ûË°åÊõ¥ËÆ©‰∫∫ÂÖ¥Â•ãÔºÅ‚Äù„ÄÇ
Êõ¥Á≥üÁ≥ïÁöÑÊòØÔºå‰∫íËÅîÁΩë‰∏äÂÖÖÊñ•ÁùÄÂ§ßÈáè‰ªòË¥πËØæÁ®ãÂíåËê•ÈîÄÂè∑Ôºå‰ª•ÊºèÊ¥ûÁôæÂá∫„ÄÅ‰∏ÄÁü•ÂçäËß£ÁöÑÂÜÖÂÆπÊé®ÈîÄAIÊïôÁ®ã„ÄÇ
Ê≠£Âõ†Â¶ÇÊ≠§ÔºåÊú¨È°πÁõÆÂàùË°∑ÊòØÊãâ‰ΩéLLMÁöÑÂ≠¶‰π†Èó®ÊßõÔºåËÆ©ÊØè‰∏™‰∫∫ÈÉΩËÉΩ‰ªéÁêÜËß£ÊØè‰∏ÄË°å‰ª£Á†ÅÂºÄÂßãÔºå
‰ªéÈõ∂ÂºÄÂßã‰∫≤ÊâãËÆ≠ÁªÉ‰∏Ä‰∏™ÊûÅÂ∞èÁöÑËØ≠Ë®ÄÊ®°Âûã„ÄÇÊòØÁöÑÔºå‰ªé**Èõ∂ÂºÄÂßãËÆ≠ÁªÉ**ÔºåËÄå‰∏çÊòØ‰ªÖ‰ªÖËøõË°å**Êé®ÁêÜ**ÔºÅ
ÊúÄ‰ΩéÂè™ÈúÄ3ÂùóÈí±‰∏çÂà∞ÁöÑÊúçÂä°Âô®ÊàêÊú¨ÔºåÂ∞±ËÉΩ‰∫≤Ë∫´‰ΩìÈ™å‰ªé0Âà∞1ÊûÑÂª∫‰∏Ä‰∏™ËØ≠Ë®ÄÊ®°ÂûãÁöÑÂÖ®ËøáÁ®ã„ÄÇ
‰∏ÄËµ∑ÊÑüÂèóÂàõÈÄ†ÁöÑ‰πêË∂£ÂêßÔºÅ

&gt; [!NOTE]
&gt; ÔºàÊà™Ëá≥2025-10ÔºâMiniMindÁ≥ªÂàóÂ∑≤ÂÆåÊàêÂ§ö‰∏™ÂûãÂè∑Ê®°ÂûãÁöÑÈ¢ÑËÆ≠ÁªÉÔºåÊúÄÂ∞è‰ªÖÈúÄ25.8MÔºà0.02BÔºâÔºåÂç≥ÂèØÂÖ∑Â§áÊµÅÁïÖÂØπËØùËÉΩÂäõÔºÅ

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Models List&lt;/summary&gt;

| Ê®°Âûã (Â§ßÂ∞è)                 | Êé®ÁêÜÂç†Áî® (Á∫¶) | Release    | 
|-------------------------|----------|------------|
| MiniMind2-small (26M)   | 0.5 GB   | 2025.04.26 |
| MiniMind2-MoE (145M)    | 1.0 GB   | 2025.04.26 |
| MiniMind2 (104M)        | 1.0 GB   | 2025.04.26 |
| minimind-v1-small (26M) | 0.5 GB   | 2024.08.28 |
| minimind-v1-moe (4√ó26M) | 1.0 GB   | 2024.09.17 |
| minimind-v1 (108M)      | 1.0 GB   | 2024.09.01 |

&lt;/details&gt;

**È°πÁõÆÂåÖÂê´**

- MiniMind-LLMÁªìÊûÑÁöÑÂÖ®ÈÉ®‰ª£Á†ÅÔºàDense+MoEÊ®°ÂûãÔºâ„ÄÇ
- ÂåÖÂê´TokenizerÂàÜËØçÂô®ËØ¶ÁªÜËÆ≠ÁªÉ‰ª£Á†Å„ÄÇ
- ÂåÖÂê´Pretrain„ÄÅSFT„ÄÅLoRA„ÄÅRLHF-DPO„ÄÅRLAIF(PPO/GRPO/SPO)„ÄÅÊ®°ÂûãËí∏È¶èÁöÑÂÖ®ËøáÁ®ãËÆ≠ÁªÉ‰ª£Á†Å„ÄÇ
- Êî∂ÈõÜ„ÄÅËí∏È¶è„ÄÅÊï¥ÁêÜÂπ∂Ê∏ÖÊ¥óÂéªÈáçÊâÄÊúâÈò∂ÊÆµÁöÑÈ´òË¥®ÈáèÊï∞ÊçÆÈõÜÔºå‰∏îÂÖ®ÈÉ®ÂºÄÊ∫ê„ÄÇ
- ‰ªé0ÂÆûÁé∞È¢ÑËÆ≠ÁªÉ„ÄÅÊåá‰ª§ÂæÆË∞É„ÄÅLoRA„ÄÅDPO/PPO/GRPO/SPOÂº∫ÂåñÂ≠¶‰π†ÔºåÁôΩÁõíÊ®°ÂûãËí∏È¶è„ÄÇÂÖ≥ÈîÆÁÆóÊ≥ïÂá†‰πé‰∏ç‰æùËµñÁ¨¨‰∏âÊñπÂ∞ÅË£ÖÁöÑÊ°ÜÊû∂Ôºå‰∏îÂÖ®ÈÉ®ÂºÄÊ∫ê„ÄÇ
- ÂêåÊó∂ÂÖºÂÆπ`transformers`„ÄÅ`trl`„ÄÅ`peft`Á≠âÁ¨¨‰∏âÊñπ‰∏ªÊµÅÊ°ÜÊû∂„ÄÇ
- ËÆ≠ÁªÉÊîØÊåÅÂçïÊú∫ÂçïÂç°„ÄÅÂçïÊú∫Â§öÂç°(DDP„ÄÅDeepSpeed)ËÆ≠ÁªÉÔºåÊîØÊåÅwandb/swanlabÂèØËßÜÂåñËÆ≠ÁªÉÊµÅÁ®ã„ÄÇÊîØÊåÅÂä®ÊÄÅÂêØÂÅúËÆ≠ÁªÉ„ÄÇ
- Âú®Á¨¨‰∏âÊñπÊµãËØÑÊ¶úÔºàC-Eval„ÄÅC-MMLU„ÄÅOpenBookQAÁ≠âÔºâËøõË°åÊ®°ÂûãÊµãËØïÔºåÊîØÊåÅYaRNÁÆóÊ≥ïÊâßË°åRoPEÈïøÊñáÊú¨Â§ñÊé®„ÄÇ
- ÂÆûÁé∞Openai-ApiÂçèËÆÆÁöÑÊûÅÁÆÄÊúçÂä°Á´ØÔºå‰æø‰∫éÈõÜÊàêÂà∞Á¨¨‰∏âÊñπChatUI‰ΩøÁî®ÔºàFastGPT„ÄÅOpen-WebUIÁ≠âÔºâ„ÄÇ
- Âü∫‰∫éstreamlitÂÆûÁé∞ÊúÄÁÆÄËÅäÂ§©WebUIÂâçÁ´Ø„ÄÇ
- ÂÖ®Èù¢ÂÖºÂÆπÁ§æÂå∫ÁÉ≠Èó®`llama.cpp`„ÄÅ`vllm`„ÄÅ`ollama`Êé®ÁêÜÂºïÊìéÊàñ`Llama-Factory`ËÆ≠ÁªÉÊ°ÜÊû∂„ÄÇ
- Â§çÁé∞(Ëí∏È¶è/RL)Â§ßÂûãÊé®ÁêÜÊ®°ÂûãDeepSeek-R1ÁöÑMiniMind-ReasonÊ®°ÂûãÔºå**Êï∞ÊçÆ+Ê®°Âûã**ÂÖ®ÈÉ®ÂºÄÊ∫êÔºÅ

Â∏åÊúõÊ≠§ÂºÄÊ∫êÈ°πÁõÆÂèØ‰ª•Â∏ÆÂä©LLMÂàùÂ≠¶ËÄÖÂø´ÈÄüÂÖ•Èó®ÔºÅ

### üëâ**Êõ¥Êñ∞Êó•Âøó**

&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2025-10-24&lt;/b&gt; &lt;/summary&gt;

- üî• Êñ∞Â¢ûRLAIFËÆ≠ÁªÉÁÆóÊ≥ïÔºöPPO„ÄÅGRPO„ÄÅSPOÔºà‰ªé0ÂéüÁîüÂÆûÁé∞Ôºâ
- Êñ∞Â¢ûÊñ≠ÁÇπÁª≠ËÆ≠ÂäüËÉΩÔºöÊîØÊåÅËÆ≠ÁªÉËá™Âä®ÊÅ¢Â§ç„ÄÅË∑®GPUÊï∞ÈáèÊÅ¢Â§ç„ÄÅwandbËÆ∞ÂΩïËøûÁª≠ÊÄß
- Êñ∞Â¢ûRLAIFÊï∞ÊçÆÈõÜÔºörlaif-mini.jsonlÔºà‰ªéSFTÊï∞ÊçÆÈöèÊú∫ÈááÊ†∑1‰∏áÊù°ÔºâÔºõÁÆÄÂåñDPOÊï∞ÊçÆÈõÜÔºåÂä†ÂÖ•‰∏≠ÊñáÊï∞ÊçÆ
- Êñ∞Â¢ûYaRNÁÆóÊ≥ïÔºöÊîØÊåÅRoPEÈïøÊñáÊú¨Â§ñÊé®ÔºåÊèêÂçáÈïøÂ∫èÂàóÂ§ÑÁêÜËÉΩÂäõ
- Adaptive ThinkingÔºöReasonÊ®°ÂûãÂèØÈÄâÊòØÂê¶ÂêØÁî®ÊÄùËÄÉÈìæ
- chat_templateÂÖ®Èù¢ÊîØÊåÅTool CallingÂíåReasoningÊ†áÁ≠æÔºà`&lt;tool_call&gt;`„ÄÅ`&lt;think&gt;`Á≠âÔºâ
- Êñ∞Â¢ûRLAIFÂÆåÊï¥Á´†ËäÇ„ÄÅËÆ≠ÁªÉÊõ≤Á∫øÂØπÊØî„ÄÅÁÆóÊ≥ïÂéüÁêÜÊäòÂè†ËØ¥Êòé
- [SwanLab](https://swanlab.cn/)Êõø‰ª£WandBÔºàÂõΩÂÜÖËÆøÈóÆÂèãÂ•ΩÔºåAPIÂÆåÂÖ®ÂÖºÂÆπÔºâ
- ËßÑËåÉÂåñÊâÄÊúâ‰ª£Á†Å &amp; ‰øÆÂ§ç‰∏Ä‰∫õÂ∑≤Áü•bugs

&lt;/details&gt;

&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2025-04-26&lt;/b&gt; &lt;/summary&gt;

- ÈáçË¶ÅÊõ¥Êñ∞
- Â¶ÇÊúâÂÖºÂÆπÊÄßÈúÄË¶ÅÔºåÂèØËÆøÈóÆ[üîóÊóß‰ªìÂ∫ìÂÜÖÂÆπüîó](https://github.com/jingyaogong/minimind/tree/7da201a944a90ed49daef8a0265c959288dff83a)„ÄÇ
- MiniMindÊ®°ÂûãÂèÇÊï∞ÂÆåÂÖ®ÊîπÂêçÔºåÂØπÈΩêTransformersÂ∫ìÊ®°ÂûãÔºàÁªü‰∏ÄÂëΩÂêçÔºâ„ÄÇ
- generateÊñπÂºèÈáçÊûÑÔºåÁªßÊâøËá™GenerationMixinÁ±ª„ÄÇ
- üî•ÊîØÊåÅllama.cpp„ÄÅvllm„ÄÅollamaÁ≠âÁÉ≠Èó®‰∏âÊñπÁîüÊÄÅ„ÄÇ
- ËßÑËåÉ‰ª£Á†ÅÂíåÁõÆÂΩïÁªìÊûÑ„ÄÇ
- ÊîπÂä®ËØçË°®`&lt;s&gt;&lt;/s&gt;`-&gt;`&lt;|im_start|&gt;&lt;|im_end|&gt;`

```text
‰∏∫ÂÖºÂÆπÁ¨¨‰∏âÊñπÊé®ÁêÜÊ°ÜÊû∂llama.cpp„ÄÅvllmÔºåÊú¨Ê¨°Êõ¥Êñ∞ÈúÄ‰ªòÂá∫‰∏Ä‰∫õÂèØËßÇ‰ª£‰ª∑„ÄÇ
Êú¨Ê¨°Êõ¥Êñ∞‰∏çÂÜçÊîØÊåÅ„ÄåÁõ¥Êé•„ÄçÂä†ËΩΩ25-04-26‰ª•ÂâçÁöÑÊóßÊ®°ÂûãËøõË°åÊé®ÁêÜ„ÄÇ
Áî±‰∫éLlama‰ΩçÁΩÆÁºñÁ†ÅÊñπÂºè‰∏éminimindÂ≠òÂú®Âå∫Âà´ÔºåÂØºËá¥Êò†Â∞ÑLlamaÊ®°ÂûãÂêéQKÂÄºÂ≠òÂú®Â∑ÆÂºÇ
MiniMind2Á≥ªÂàóÊóßÊ®°ÂûãÂùáÁªèËøáÊùÉÈáçÊò†Â∞Ñ+ÔºàÂæÆË∞ÉËÆ≠ÁªÉÔºâQKVOÁ∫øÊÄßÂ±ÇÊ†°ÂáÜÊÅ¢Â§çËÄåÊù•„ÄÇ
Êú¨Ê¨°Êõ¥Êñ∞ÂêéÂ∞ÜÊîæÂºÉÂØπ`minimind-v1`ÂÖ®Á≥ªÂàóÁöÑÁª¥Êä§ÔºåÂπ∂Âú®‰ªìÂ∫ì‰∏≠‰∏ãÁ∫ø„ÄÇ
```

&lt;/details&gt;

&lt;details close&gt; 
&lt;summary&gt; &lt;b&gt;2025-02-09&lt;/b&gt; &lt;/summary&gt;

- ËøéÊù•ÂèëÂ∏É‰ª•Êù•ÈáçÂ§ßÊõ¥Êñ∞ÔºåRelease MiniMind2 Series„ÄÇ
- ‰ª£Á†ÅÂá†‰πéÂÖ®ÈÉ®ÈáçÊûÑÔºå‰ΩøÁî®Êõ¥ÁÆÄÊ¥ÅÊòé‰∫ÜÁöÑÁªü‰∏ÄÁªìÊûÑ„ÄÇ
  Â¶ÇÊúâÊóß‰ª£Á†ÅÁöÑÂÖºÂÆπÊÄßÈúÄË¶ÅÔºåÂèØËÆøÈóÆ[üîóÊóß‰ªìÂ∫ìÂÜÖÂÆπüîó](https://github.com/jingyaogong/minimind/tree/6e9cd28ef9b34a0a10afbdf6f59e65cb6e628efb)„ÄÇ
- ÂÖçÂéªÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÊ≠•È™§„ÄÇÁªü‰∏ÄÊï∞ÊçÆÈõÜÊ†ºÂºèÔºåÊõ¥Êç¢‰∏∫`jsonl`Ê†ºÂºèÊùúÁªùÊï∞ÊçÆÈõÜ‰∏ãËΩΩÊ∑∑‰π±ÁöÑÈóÆÈ¢ò„ÄÇ
- MiniMind2Á≥ªÂàóÊïàÊûúÁõ∏ÊØîMiniMind-V1ÊòæËëóÊèêÂçá„ÄÇ
- Â∞èÈóÆÈ¢òÔºö{kv-cacheÂÜôÊ≥ïÊõ¥Ê†áÂáÜ„ÄÅMoEÁöÑË¥üËΩΩÂùáË°°lossË¢´ËÄÉËôëÁ≠âÁ≠â}
- Êèê‰æõÊ®°ÂûãËøÅÁßªÂà∞ÁßÅÊúâÊï∞ÊçÆÈõÜÁöÑËÆ≠ÁªÉÊñπÊ°àÔºàÂåªÁñóÊ®°Âûã„ÄÅËá™ÊàëËÆ§Áü•Ê†∑‰æãÔºâ„ÄÇ
- Á≤æÁÆÄÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºåÂπ∂Â§ßÂπÖÊèêÂçáÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆË¥®ÈáèÔºåÂ§ßÂπÖÁº©Áü≠‰∏™‰∫∫Âø´ÈÄüËÆ≠ÁªÉÊâÄÈúÄÊó∂Èó¥ÔºåÂçïÂç°3090Âç≥ÂèØ2Â∞èÊó∂Â§çÁé∞ÔºÅ
- Êõ¥Êñ∞ÔºöLoRAÂæÆË∞ÉËÑ±Á¶ªpeftÂåÖË£ÖÔºå‰ªé0ÂÆûÁé∞LoRAËøáÁ®ãÔºõDPOÁÆóÊ≥ï‰ªé0‰ΩøÁî®PyTorchÂéüÁîüÂÆûÁé∞ÔºõÊ®°ÂûãÁôΩÁõíËí∏È¶èÂéüÁîüÂÆûÁé∞„ÄÇ
- MiniMind2-DeepSeek-R1Á≥ªÂàóËí∏È¶èÊ®°ÂûãËØûÁîüÔºÅ
- MiniMind2ÂÖ∑Â§á‰∏ÄÂÆöÁöÑËã±ÊñáËÉΩÂäõÔºÅ
- Êõ¥Êñ∞MiniMind2‰∏éÁ¨¨‰∏âÊñπÊ®°ÂûãÁöÑÂü∫‰∫éÊõ¥Â§öÂ§ßÊ®°ÂûãÊ¶úÂçïÊµãËØïÊÄßËÉΩÁöÑÁªìÊûú„ÄÇ

&lt;/details&gt;

&lt;details close&gt;
&lt;summary&gt; &lt;b&gt;More...&lt;/b&gt; &lt;/summary&gt;

**2024-10-05**
- ‰∏∫MiniMindÊãìÂ±ï‰∫ÜÂ§öÊ®°ÊÄÅËÉΩÂäõ‰πã---ËßÜËßâ
- ÁßªÊ≠•Â≠™ÁîüÈ°πÁõÆ[minimind-v](https://github.com/jingyaogong/minimind-v)Êü•ÁúãËØ¶ÊÉÖÔºÅ

**2024-09-27**
- 09-27Êõ¥Êñ∞pretrainÊï∞ÊçÆÈõÜÁöÑÈ¢ÑÂ§ÑÁêÜÊñπÂºèÔºå‰∏∫‰∫Ü‰øùËØÅÊñáÊú¨ÂÆåÊï¥ÊÄßÔºåÊîæÂºÉÈ¢ÑÂ§ÑÁêÜÊàê.binËÆ≠ÁªÉÁöÑÂΩ¢ÂºèÔºàËΩªÂæÆÁâ∫Áâ≤ËÆ≠ÁªÉÈÄüÂ∫¶Ôºâ„ÄÇ
- ÁõÆÂâçpretrainÈ¢ÑÂ§ÑÁêÜÂêéÁöÑÊñá‰ª∂ÂëΩÂêç‰∏∫Ôºöpretrain_data.csv„ÄÇ
- Âà†Èô§‰∫Ü‰∏Ä‰∫õÂÜó‰ΩôÁöÑ‰ª£Á†Å„ÄÇ

**2024-09-17**
- Êõ¥Êñ∞minimind-v1-moeÊ®°Âûã
- ‰∏∫‰∫ÜÈò≤Ê≠¢Ê≠ß‰πâÔºå‰∏çÂÜç‰ΩøÁî®mistral_tokenizerÂàÜËØçÔºåÂÖ®ÈÉ®ÈááÁî®Ëá™ÂÆö‰πâÁöÑminimind_tokenizer‰Ωú‰∏∫ÂàÜËØçÂô®„ÄÇ

**2024-09-01**
- Êõ¥Êñ∞minimind-v1 (108M)Ê®°ÂûãÔºåÈááÁî®minimind_tokenizerÔºåÈ¢ÑËÆ≠ÁªÉËΩÆÊ¨°3 + SFTËΩÆÊ¨°10ÔºåÊõ¥ÂÖÖÂàÜËÆ≠ÁªÉÔºåÊÄßËÉΩÊõ¥Âº∫„ÄÇ
- È°πÁõÆÂ∑≤ÈÉ®ÁΩ≤Ëá≥ModelScopeÂàõÁ©∫Èó¥ÔºåÂèØ‰ª•Âú®Ê≠§ÁΩëÁ´ô‰∏ä‰ΩìÈ™åÔºö
- [üîóModelScopeÂú®Á∫ø‰ΩìÈ™åüîó](https://www.modelscope.cn/studios/gongjy/minimind)

**2024-08-27**
- È°πÁõÆÈ¶ñÊ¨°ÂºÄÊ∫ê

&lt;/details&gt;

# üìå Âø´ÈÄüÂºÄÂßã

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;ÂàÜ‰∫´Êú¨‰∫∫ÁöÑËΩØÁ°¨‰ª∂ÈÖçÁΩÆÔºà‰ªÖ‰æõÂèÇËÄÉÔºâ&lt;/summary&gt;

* CPU: Intel(R) Core(TM) i9-10980XE CPU @ 3.00GHz
* RAM: 128 GB
* GPU: NVIDIA GeForce RTX 3090(24GB) * 8
* Ubuntu==20.04
* CUDA==12.2
* Python==3.10.16
* [requirements.txt](./requirements.txt)

&lt;/details&gt;

### Á¨¨0Ê≠•

```bash
git clone https://github.com/jingyaogong/minimind.git
```

## ‚Ö† ÊµãËØïÂ∑≤ÊúâÊ®°ÂûãÊïàÊûú

### 1.ÁéØÂ¢ÉÂáÜÂ§á

```bash
pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple
```

### 2.‰∏ãËΩΩÊ®°Âûã

Âà∞È°πÁõÆÊ†πÁõÆÂΩï

```bash
git clone https://huggingface.co/jingyaogong/MiniMind2 # or https://www.modelscope.cn/models/gongjy/MiniMind2
```

### ÔºàÂèØÈÄâÔºâÂëΩ‰ª§Ë°åÈóÆÁ≠î

```bash
# ‰ΩøÁî®transformersÊ†ºÂºèÊ®°Âûã
python eval_llm.py --load_from ./MiniMind2
```

### ÔºàÂèØÈÄâÔºâÂêØÂä®WebUI

```bash
# ÂèØËÉΩÈúÄË¶Å`python&gt;=3.10` ÂÆâË£Ö `pip install streamlit`
# cd scripts
streamlit run web_demo.py
```

### ÔºàÂèØÈÄâÔºâÁ¨¨‰∏âÊñπÊé®ÁêÜÊ°ÜÊû∂

```bash
# ollama
ollama run jingyaogong/minimind2
# vllm
vllm serve ./MiniMind2/ --served-model-name &quot;minimind&quot;
```

## ‚Ö° ‰ªé0ÂºÄÂßãËá™Â∑±ËÆ≠ÁªÉ

### 1.ÁéØÂ¢ÉÂáÜÂ§á

```bash
pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple
```

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöÊèêÂâçÊµãËØïTorchÊòØÂê¶ÂèØÁî®cuda&lt;/summary&gt;

```bash
import torch
print(torch.cuda.is_available())
```

Â¶ÇÊûú‰∏çÂèØÁî®ÔºåËØ∑Ëá™Ë°åÂéª[torch_stable](https://download.pytorch.org/whl/torch_stable.html)
‰∏ãËΩΩwhlÊñá‰ª∂ÂÆâË£Ö„ÄÇÂèÇËÄÉ[ÈìæÊé•](https://blog.csdn.net/weixin_45456738/article/details/141029610?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=%E5%AE%89%E8%A3%85torch&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-141029610.nonecase&amp;spm=1018.2226.3001.4187)

&lt;/details&gt;

### 2.Êï∞ÊçÆ‰∏ãËΩΩ

‰ªé‰∏ãÊñáÊèê‰æõÁöÑ[Êï∞ÊçÆÈõÜ‰∏ãËΩΩÈìæÊé•](https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files)
‰∏ãËΩΩÈúÄË¶ÅÁöÑÊï∞ÊçÆÊñá‰ª∂ÔºàÂàõÂª∫`./dataset`ÁõÆÂΩïÔºâÂπ∂ÊîæÂà∞`./dataset`‰∏ã

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöÊï∞ÊçÆÈõÜÈ°ªÁü•&lt;/summary&gt;

ÈªòËÆ§Êé®Ëçê‰∏ãËΩΩ`pretrain_hq.jsonl` + `sft_mini_512.jsonl`ÊúÄÂø´ÈÄüÂ∫¶Â§çÁé∞ZeroËÅäÂ§©Ê®°Âûã„ÄÇ

Êï∞ÊçÆÊñá‰ª∂ÂèØËá™Áî±ÈÄâÊã©Ôºå‰∏ãÊñáÊèê‰æõ‰∫ÜÂ§öÁßçÊê≠ÈÖçÊñπÊ°àÔºåÂèØÊ†πÊçÆËá™Â∑±ÊâãÂ§¥ÁöÑËÆ≠ÁªÉÈúÄÊ±ÇÂíåGPUËµÑÊ∫êËøõË°åÈÄÇÂΩìÁªÑÂêà„ÄÇ

&lt;/details&gt;

### 3.ÂºÄÂßãËÆ≠ÁªÉ

ÁõÆÂΩï‰Ωç‰∫é`trainer`

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;üí° Ê£ÄÊü•ÁÇπÊöÇÂÅúÁª≠ËÆ≠&lt;/summary&gt;

ÊâÄÊúâËÆ≠ÁªÉËÑöÊú¨ÂùáËá™Âä®‰øùÂ≠òÊ£ÄÊü•ÁÇπÔºåÂè™ÈúÄÊ∑ªÂä† `--from_resume 1` ÂèÇÊï∞Âç≥ÂèØËá™Âä®Ê£ÄÊµãÂä†ËΩΩ&amp;ÊÅ¢Â§çËÆ≠ÁªÉÔºö

```bash
python train_pretrain.py --from_resume 1
python train_full_sft.py --from_resume 1
...
```

**Êñ≠ÁÇπÁª≠ËÆ≠Êú∫Âà∂ËØ¥ÊòéÔºö**
- ËÆ≠ÁªÉËøáÁ®ãËá™Âä®Âú® `./checkpoints/` ÁõÆÂΩï‰øùÂ≠òÂÆåÊï¥Ê£ÄÊü•ÁÇπÔºàÊ®°Âûã„ÄÅ‰ºòÂåñÂô®„ÄÅËÆ≠ÁªÉËøõÂ∫¶Á≠âÔºâ
- Ê£ÄÊü•ÁÇπÊñá‰ª∂ÂëΩÂêçÔºö`&lt;ÊùÉÈáçÂêç&gt;_&lt;Áª¥Â∫¶&gt;_resume.pth`ÔºàÂ¶ÇÔºö`full_sft_512_resume.pth`Ôºâ
- ÊîØÊåÅË∑®‰∏çÂêåGPUÊï∞ÈáèÊÅ¢Â§çÔºàËá™Âä®Ë∞ÉÊï¥stepÔºâ
- ÊîØÊåÅwandbËÆ≠ÁªÉËÆ∞ÂΩïËøûÁª≠ÊÄßÔºàËá™Âä®ÊÅ¢Â§çÂêå‰∏Ä‰∏™runÔºâ

&gt; ÈÄÇÂêàÈïøÊó∂Èó¥ËÆ≠ÁªÉÊàñ‰∏çÁ®≥ÂÆöÁéØÂ¢ÉÔºåÊó†ÈúÄÊãÖÂøÉËÆ≠ÁªÉ‰∏≠Êñ≠ÂØºËá¥ËøõÂ∫¶‰∏¢Â§±

&lt;/details&gt;

**3.1 È¢ÑËÆ≠ÁªÉÔºàÂ≠¶Áü•ËØÜÔºâ**

```bash
python train_pretrain.py
```

&gt; ÊâßË°åÈ¢ÑËÆ≠ÁªÉÔºåÂæóÂà∞ `pretrain_*.pth` ‰Ωú‰∏∫È¢ÑËÆ≠ÁªÉÁöÑËæìÂá∫ÊùÉÈáçÔºàÂÖ∂‰∏≠*‰∏∫Ê®°ÂûãÁöÑdimensionÔºåÈªòËÆ§‰∏∫512Ôºâ


**3.2 ÁõëÁù£ÂæÆË∞ÉÔºàÂ≠¶ÂØπËØùÊñπÂºèÔºâ**

```bash
python train_full_sft.py
```

&gt; ÊâßË°åÁõëÁù£ÂæÆË∞ÉÔºåÂæóÂà∞ `full_sft_*.pth` ‰Ωú‰∏∫Êåá‰ª§ÂæÆË∞ÉÁöÑËæìÂá∫ÊùÉÈáçÔºàÂÖ∂‰∏≠`full`Âç≥‰∏∫ÂÖ®ÂèÇÊï∞ÂæÆË∞ÉÔºâ

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöËÆ≠ÁªÉÈ°ªÁü•&lt;/summary&gt;

ÊâÄÊúâËÆ≠ÁªÉËøáÁ®ãÈªòËÆ§ÊØèÈöî100Ê≠•‰øùÂ≠ò1Ê¨°ÂèÇÊï∞Âà∞Êñá‰ª∂`./out/***.pth`ÔºàÊØèÊ¨°‰ºöË¶ÜÁõñÊéâÊóßÊùÉÈáçÊñá‰ª∂Ôºâ„ÄÇ

ÁÆÄÂçïËµ∑ËßÅÔºåÊ≠§Â§ÑÂè™ÂÜôÊòé‰∏§‰∏™Èò∂ÊÆµËÆ≠ÁªÉËøáÁ®ã„ÄÇÂ¶ÇÈúÄÂÖ∂ÂÆÉËÆ≠ÁªÉ (LoRA, Ëí∏È¶è, Âº∫ÂåñÂ≠¶‰π†, ÂæÆË∞ÉÊé®ÁêÜÁ≠â) ÂèØÂèÇËÄÉ‰∏ãÊñá„ÄêÂÆûÈ™å„ÄëÂ∞èËäÇÁöÑËØ¶ÁªÜËØ¥Êòé„ÄÇ

&lt;/details&gt;


---

### 4.ÊµãËØïËá™Â∑±ËÆ≠ÁªÉÁöÑÊ®°ÂûãÊïàÊûú

Á°Æ‰øùÈúÄË¶ÅÊµãËØïÁöÑÊ®°Âûã`*.pth`Êñá‰ª∂‰Ωç‰∫é`./out/`ÁõÆÂΩï‰∏ã„ÄÇ
‰πüÂèØ‰ª•Áõ¥Êé•Âéª[Ê≠§Â§Ñ](https://www.modelscope.cn/models/gongjy/MiniMind2-PyTorch/files)‰∏ãËΩΩ‰ΩøÁî®ÊàëËÆ≠ÁªÉÁöÑ`*.pth`Êñá‰ª∂„ÄÇ

```bash
python eval_llm.py --weight full_sft # Êàñ pretrain/dpo/ppo/grpo...
```

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöÊµãËØïÈ°ªÁü•&lt;/summary&gt;

`--weight` ÂèÇÊï∞ÊåáÂÆöÊùÉÈáçÂêçÁß∞ÂâçÁºÄÔºåÂèØÈÄâÔºö`pretrain`, `full_sft`, `dpo`, `reason`, `ppo_actor`, `grpo`, `spo` Á≠â

ÂÖ∂‰ªñÂ∏∏Áî®ÂèÇÊï∞Ôºö
- `--load_from`: Ê®°ÂûãÂä†ËΩΩË∑ØÂæÑÔºà`model`=ÂéüÁîütorchÊùÉÈáçÔºåÂÖ∂‰ªñË∑ØÂæÑ=transformersÊ†ºÂºèÔºâ
- `--save_dir`: Ê®°ÂûãÊùÉÈáçÁõÆÂΩïÔºàÈªòËÆ§`out`Ôºâ
- `--lora_weight`: LoRAÊùÉÈáçÂêçÁß∞Ôºà`None`Ë°®Á§∫‰∏ç‰ΩøÁî®Ôºâ
- `--historys`: Êê∫Â∏¶ÂéÜÂè≤ÂØπËØùËΩÆÊï∞ÔºàÈúÄ‰∏∫ÂÅ∂Êï∞Ôºå0Ë°®Á§∫‰∏çÊê∫Â∏¶ÂéÜÂè≤Ôºâ
- `--max_new_tokens`: ÊúÄÂ§ßÁîüÊàêÈïøÂ∫¶ÔºàÈªòËÆ§8192Ôºâ
- `--temperature`: ÁîüÊàêÊ∏©Â∫¶ÔºàÈªòËÆ§0.85Ôºâ
- `--top_p`: nucleusÈááÊ†∑ÈòàÂÄºÔºàÈªòËÆ§0.85Ôºâ


‰ΩøÁî®ÊñπÂºèÁõ¥Êé•Êü•Áúã`eval_llm.py`‰ª£Á†ÅÂç≥ÂèØ„ÄÇ

&lt;/details&gt;


---

&gt; [!TIP]
&gt; ÊâÄÊúâËÆ≠ÁªÉËÑöÊú¨Âùá‰∏∫PytorchÂéüÁîüÊ°ÜÊû∂ÔºåÂùáÊîØÊåÅÂ§öÂç°Âä†ÈÄüÔºåÂÅáËÆæ‰Ω†ÁöÑËÆæÂ§áÊúâN (NÔºû1) Âº†ÊòæÂç°Ôºö

ÂçïÊú∫NÂç°ÂêØÂä®ËÆ≠ÁªÉÊñπÂºè (DDP, ÊîØÊåÅÂ§öÊú∫Â§öÂç°ÈõÜÁæ§)

```bash
torchrun --nproc_per_node N train_xxx.py
```

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöÂÖ∂ÂÆÉÈ°ªÁü•&lt;/summary&gt;

&lt;del&gt;
ÂçïÊú∫NÂç°ÂêØÂä®ËÆ≠ÁªÉ (DeepSpeed)

```bash
deepspeed --master_port 29500 --num_gpus=N train_xxx.py
```
&lt;/del&gt;

ÂèØÊ†πÊçÆÈúÄË¶ÅÂºÄÂêØwandbËÆ∞ÂΩïËÆ≠ÁªÉËøáÁ®ãÔºàÈúÄÂèØÁõ¥ËøûÔºâ

```bash
# ÈúÄË¶ÅÁôªÂΩï: wandb login
torchrun --nproc_per_node N train_xxx.py --use_wandb
# and
python train_xxx.py --use_wandb
```

ÈÄöËøáÊ∑ªÂä†`--use_wandb`ÂèÇÊï∞ÔºåÂèØ‰ª•ËÆ∞ÂΩïËÆ≠ÁªÉËøáÁ®ãÔºåËÆ≠ÁªÉÂÆåÊàêÂêéÔºåÂèØ‰ª•Âú®wandbÁΩëÁ´ô‰∏äÊü•ÁúãËÆ≠ÁªÉËøáÁ®ã„ÄÇÈÄöËøá‰øÆÊîπ`wandb_project`
Âíå`wandb_run_name`ÂèÇÊï∞ÔºåÂèØ‰ª•ÊåáÂÆöÈ°πÁõÆÂêçÁß∞ÂíåËøêË°åÂêçÁß∞„ÄÇ

„ÄêÊ≥®„ÄëÔºö25Âπ¥6ÊúàÂêéÔºåÂõΩÂÜÖÁΩëÁªúÁéØÂ¢ÉÊó†Ê≥ïÁõ¥ËøûWandBÔºåMiniMindÈ°πÁõÆÈªòËÆ§ËΩ¨‰∏∫‰ΩøÁî®[SwanLab](https://swanlab.cn/)‰Ωú‰∏∫ËÆ≠ÁªÉÂèØËßÜÂåñÂ∑•ÂÖ∑ÔºàÂÆåÂÖ®ÂÖºÂÆπWandB APIÔºâÔºåÂç≥`import wandb`Êîπ‰∏∫`import swanlab as wandb`Âç≥ÂèØÔºåÂÖ∂‰ªñÂùáÊó†ÈúÄÊîπÂä®„ÄÇ

&lt;/details&gt;

# üìå Êï∞ÊçÆ‰ªãÁªç

## ‚Ö† Tokenizer

ÂàÜËØçÂô®Â∞ÜÂçïËØç‰ªéËá™ÁÑ∂ËØ≠Ë®ÄÈÄöËøá‚ÄúËØçÂÖ∏‚ÄùÊò†Â∞ÑÂà∞`0, 1, 36`ËøôÊ†∑ÁöÑÊï∞Â≠óÔºåÂèØ‰ª•ÁêÜËß£‰∏∫Êï∞Â≠óÂ∞±‰ª£Ë°®‰∫ÜÂçïËØçÂú®‚ÄúËØçÂÖ∏‚Äù‰∏≠ÁöÑÈ°µÁ†Å„ÄÇ
ÂèØ‰ª•ÈÄâÊã©Ëá™Â∑±ÊûÑÈÄ†ËØçË°®ËÆ≠ÁªÉ‰∏Ä‰∏™‚ÄúËØçÂÖ∏‚ÄùÔºå‰ª£Á†ÅÂèØËßÅ`./scripts/train_tokenizer.py`Ôºà‰ªÖ‰æõÂ≠¶‰π†ÂèÇËÄÉÔºåËã•ÈùûÂøÖË¶ÅÊó†ÈúÄÂÜçËá™Ë°åËÆ≠ÁªÉÔºåMiniMindÂ∑≤Ëá™Â∏¶tokenizerÔºâ„ÄÇ
ÊàñËÄÖÈÄâÊã©ÊØîËæÉÂá∫ÂêçÁöÑÂºÄÊ∫êÂ§ßÊ®°ÂûãÂàÜËØçÂô®Ôºå
Ê≠£Â¶ÇÂêåÁõ¥Êé•Áî®Êñ∞Âçé/ÁâõÊ¥•ËØçÂÖ∏ÁöÑ‰ºòÁÇπÊòØtokenÁºñÁ†ÅÂéãÁº©ÁéáÂæàÂ•ΩÔºåÁº∫ÁÇπÊòØÈ°µÊï∞Â§™Â§öÔºåÂä®ËæÑÊï∞ÂçÅ‰∏á‰∏™ËØçÊ±áÁü≠ËØ≠Ôºõ
Ëá™Â∑±ËÆ≠ÁªÉÁöÑÂàÜËØçÂô®Ôºå‰ºòÁÇπÊòØËØçË°®ÈïøÂ∫¶ÂíåÂÜÖÂÆπÈöèÊÑèÊéßÂà∂ÔºåÁº∫ÁÇπÊòØÂéãÁº©ÁéáÂæà‰ΩéÔºà‰æãÂ¶Ç&quot;hello&quot;‰πüËÆ∏‰ºöË¢´ÊãÜÂàÜ‰∏∫&quot;h e l l o&quot;
‰∫î‰∏™Áã¨Á´ãÁöÑtokenÔºâÔºå‰∏îÁîüÂÉªËØçÈöæ‰ª•Ë¶ÜÁõñ„ÄÇ
‚ÄúËØçÂÖ∏‚ÄùÁöÑÈÄâÊã©Âõ∫ÁÑ∂ÂæàÈáçË¶ÅÔºåLLMÁöÑËæìÂá∫Êú¨Ë¥®‰∏äÊòØSoftMaxÂà∞ËØçÂÖ∏N‰∏™ËØçÁöÑÂ§öÂàÜÁ±ªÈóÆÈ¢òÔºåÁÑ∂ÂêéÈÄöËøá‚ÄúËØçÂÖ∏‚ÄùËß£Á†ÅÂà∞Ëá™ÁÑ∂ËØ≠Ë®Ä„ÄÇ
Âõ†‰∏∫MiniMind‰ΩìÁßØÈúÄË¶Å‰∏•Ê†ºÊéßÂà∂Ôºå‰∏∫‰∫ÜÈÅøÂÖçÊ®°ÂûãÂ§¥ÈáçËÑöËΩªÔºàËØçÂµåÂÖ•embeddingÂ±ÇÂèÇÊï∞Âú®LLMÂç†ÊØîÂ§™È´òÔºâÔºåÊâÄ‰ª•ËØçË°®ÈïøÂ∫¶Áü≠Áü≠ÁõäÂñÑ„ÄÇ

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Tokenizer‰ªãÁªç&lt;/summary&gt;

Á¨¨‰∏âÊñπÂº∫Â§ßÁöÑÂºÄÊ∫êÊ®°Âûã‰æãÂ¶ÇYi„ÄÅqwen„ÄÅchatglm„ÄÅmistral„ÄÅLlama3ÁöÑtokenizerËØçË°®ÈïøÂ∫¶Â¶Ç‰∏ãÔºö

&lt;table&gt;
  &lt;tr&gt;&lt;th&gt;TokenizerÊ®°Âûã&lt;/th&gt;&lt;th&gt;ËØçË°®Â§ßÂ∞è&lt;/th&gt;&lt;th&gt;Êù•Ê∫ê&lt;/th&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;yi tokenizer&lt;/td&gt;&lt;td&gt;64,000&lt;/td&gt;&lt;td&gt;01‰∏áÁâ©Ôºà‰∏≠ÂõΩÔºâ&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;qwen2 tokenizer&lt;/td&gt;&lt;td&gt;151,643&lt;/td&gt;&lt;td&gt;ÈòøÈáå‰∫ëÔºà‰∏≠ÂõΩÔºâ&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;glm tokenizer&lt;/td&gt;&lt;td&gt;151,329&lt;/td&gt;&lt;td&gt;Êô∫Ë∞±AIÔºà‰∏≠ÂõΩÔºâ&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;mistral tokenizer&lt;/td&gt;&lt;td&gt;32,000&lt;/td&gt;&lt;td&gt;Mistral AIÔºàÊ≥ïÂõΩÔºâ&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;llama3 tokenizer&lt;/td&gt;&lt;td&gt;128,000&lt;/td&gt;&lt;td&gt;MetaÔºàÁæéÂõΩÔºâ&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;minimind tokenizer&lt;/td&gt;&lt;td&gt;6,400&lt;/td&gt;&lt;td&gt;Ëá™ÂÆö‰πâ&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&gt; üëâ2024-09-17Êõ¥Êñ∞Ôºö‰∏∫‰∫ÜÈò≤Ê≠¢ËøáÂéªÁöÑÁâàÊú¨Ê≠ß‰πâ&amp;ÊéßÂà∂‰ΩìÁßØÔºåminimindÊâÄÊúâÊ®°ÂûãÂùá‰ΩøÁî®minimind_tokenizerÂàÜËØçÔºåÂ∫üÂºÉÊâÄÊúâmistral_tokenizerÁâàÊú¨„ÄÇ

```
# ‰∏Ä‰∫õËá™Ë®ÄËá™ËØ≠
&gt; Â∞ΩÁÆ°minimind_tokenizerÈïøÂ∫¶ÂæàÂ∞èÔºåÁºñËß£Á†ÅÊïàÁéáÂº±‰∫éqwen2„ÄÅglmÁ≠â‰∏≠ÊñáÂèãÂ•ΩÂûãÂàÜËØçÂô®„ÄÇ
&gt; ‰ΩÜminimindÊ®°ÂûãÈÄâÊã©‰∫ÜËá™Â∑±ËÆ≠ÁªÉÁöÑminimind_tokenizer‰Ωú‰∏∫ÂàÜËØçÂô®Ôºå‰ª•‰øùÊåÅÊï¥‰ΩìÂèÇÊï∞ËΩªÈáèÔºåÈÅøÂÖçÁºñÁ†ÅÂ±ÇÂíåËÆ°ÁÆóÂ±ÇÂç†ÊØîÂ§±Ë°°ÔºåÂ§¥ÈáçËÑöËΩªÔºåÂõ†‰∏∫minimindÁöÑËØçË°®Â§ßÂ∞èÂè™Êúâ6400„ÄÇ
&gt; ‰∏îminimindÂú®ÂÆûÈôÖÊµãËØï‰∏≠Ê≤°ÊúâÂá∫Áé∞ËøáÁîüÂÉªËØçÊ±áËß£Á†ÅÂ§±Ë¥•ÁöÑÊÉÖÂÜµÔºåÊïàÊûúËâØÂ•Ω„ÄÇ
&gt; Áî±‰∫éËá™ÂÆö‰πâËØçË°®ÂéãÁº©ÈïøÂ∫¶Âà∞6400Ôºå‰ΩøÂæóLLMÊÄªÂèÇÊï∞ÈáèÊúÄ‰ΩéÂè™Êúâ25.8M„ÄÇ
&gt; ËÆ≠ÁªÉÊï∞ÊçÆ`pretrain_hq.jsonl`ÂùáÊù•Ëá™‰∫é`Âå†Êï∞Â§ßÊ®°ÂûãÊï∞ÊçÆÈõÜ`ÔºåËøôÈÉ®ÂàÜÊï∞ÊçÆÁõ∏ÂØπÊ¨°Ë¶ÅÔºåÂ¶ÇÈúÄËÆ≠ÁªÉÂèØ‰ª•Ëá™Áî±ÈÄâÊã©„ÄÇ
```

&lt;/details&gt;

## ‚Ö° PretrainÊï∞ÊçÆ

ÁªèÂéÜ‰∫ÜMiniMind-V1ÁöÑ‰ΩéË¥®ÈáèÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÔºåÂØºËá¥Ê®°ÂûãËÉ°Ë®Ä‰π±ËØ≠ÁöÑÊïôËÆ≠Ôºå`2025-02-05` ‰πãÂêéÂÜ≥ÂÆö‰∏çÂÜçÈááÁî®Â§ßËßÑÊ®°Êó†ÁõëÁù£ÁöÑÊï∞ÊçÆÈõÜÂÅöÈ¢ÑËÆ≠ÁªÉ„ÄÇ
ËøõËÄåÂ∞ùËØïÊää[Âå†Êï∞Â§ßÊ®°ÂûãÊï∞ÊçÆÈõÜ](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)ÁöÑ‰∏≠ÊñáÈÉ®ÂàÜÊèêÂèñÂá∫Êù•Ôºå
Ê∏ÖÊ¥óÂá∫Â≠óÁ¨¶`&lt;512`ÈïøÂ∫¶ÁöÑÂ§ßÁ∫¶1.6GBÁöÑËØ≠ÊñôÁõ¥Êé•ÊãºÊé•ÊàêÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆ `pretrain_hq.jsonl`ÔºåhqÂç≥‰∏∫high
qualityÔºàÂΩìÁÑ∂‰πüËøò‰∏çÁÆóhighÔºåÊèêÂçáÊï∞ÊçÆË¥®ÈáèÊó†Ê≠¢Â∞ΩÔºâ„ÄÇ

Êñá‰ª∂`pretrain_hq.jsonl` Êï∞ÊçÆÊ†ºÂºè‰∏∫

```json
{&quot;text&quot;: &quot;Â¶Ç‰ΩïÊâçËÉΩÊëÜËÑ±ÊãñÂª∂ÁóáÔºü Ê≤ªÊÑàÊãñÂª∂ÁóáÂπ∂‰∏çÂÆπÊòìÔºå‰ΩÜ‰ª•‰∏ãÂª∫ËÆÆÂèØËÉΩÊúâÊâÄÂ∏ÆÂä©...&quot;}
```

## ‚Ö¢ SFTÊï∞ÊçÆ

[Âå†Êï∞Â§ßÊ®°ÂûãSFTÊï∞ÊçÆÈõÜ](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)
‚ÄúÊòØ‰∏Ä‰∏™ÂÆåÊï¥„ÄÅÊ†ºÂºèÁªü‰∏Ä„ÄÅÂÆâÂÖ®ÁöÑÂ§ßÊ®°ÂûãËÆ≠ÁªÉÂíåÁ†îÁ©∂ËµÑÊ∫ê„ÄÇ
‰ªéÁΩëÁªú‰∏äÁöÑÂÖ¨ÂºÄÊï∞ÊçÆÊ∫êÊî∂ÈõÜÂπ∂Êï¥ÁêÜ‰∫ÜÂ§ßÈáèÂºÄÊ∫êÊï∞ÊçÆÈõÜÔºåÂØπÂÖ∂ËøõË°å‰∫ÜÊ†ºÂºèÁªü‰∏ÄÔºåÊï∞ÊçÆÊ∏ÖÊ¥óÔºå
ÂåÖÂê´10MÊù°Êï∞ÊçÆÁöÑ‰∏≠ÊñáÊï∞ÊçÆÈõÜÂíåÂåÖÂê´2MÊù°Êï∞ÊçÆÁöÑËã±ÊñáÊï∞ÊçÆÈõÜ„ÄÇ‚Äù
‰ª•‰∏äÊòØÂÆòÊñπ‰ªãÁªçÔºå‰∏ãËΩΩÊñá‰ª∂ÂêéÁöÑÊï∞ÊçÆÊÄªÈáèÂ§ßÁ∫¶Âú®4B tokensÔºåËÇØÂÆöÊòØÈÄÇÂêà‰Ωú‰∏∫‰∏≠ÊñáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑSFTÊï∞ÊçÆÁöÑ„ÄÇ
‰ΩÜÊòØÂÆòÊñπÊèê‰æõÁöÑÊï∞ÊçÆÊ†ºÂºèÂæà‰π±ÔºåÂÖ®ÈÉ®Áî®Êù•sft‰ª£‰ª∑Â§™Â§ß„ÄÇ
ÊàëÂ∞ÜÊääÂÆòÊñπÊï∞ÊçÆÈõÜËøõË°å‰∫Ü‰∫åÊ¨°Ê∏ÖÊ¥óÔºåÊääÂê´ÊúâÁ¨¶Âè∑Ê±°ÊüìÂíåÂô™Â£∞ÁöÑÊù°ÁõÆÂéªÈô§ÔºõÂè¶Â§ñ‰æùÁÑ∂Âè™‰øùÁïô‰∫ÜÊÄªÈïøÂ∫¶`&lt;512`
ÁöÑÂÜÖÂÆπÔºåÊ≠§Èò∂ÊÆµÂ∏åÊúõÈÄöËøáÂ§ßÈáèÂØπËØùË°•ÂÖÖÈ¢ÑËÆ≠ÁªÉÈò∂ÊÆµÊ¨†Áº∫ÁöÑÁü•ËØÜ„ÄÇ
ÂØºÂá∫Êñá‰ª∂‰∏∫`sft_512.jsonl`(~7.5GB)„ÄÇ

[Magpie-SFTÊï∞ÊçÆÈõÜ](https://www.modelscope.cn/organization/Magpie-Align)
Êî∂ÈõÜ‰∫Ü~1MÊù°Êù•Ëá™Qwen2/2.5ÁöÑÈ´òË¥®ÈáèÂØπËØùÔºåÊàëÂ∞ÜËøôÈÉ®ÂàÜÊï∞ÊçÆËøõ‰∏ÄÊ≠•Ê∏ÖÊ¥óÔºåÊääÊÄªÈïøÂ∫¶`&lt;2048`ÁöÑÈÉ®ÂàÜÂØºÂá∫‰∏∫`sft_2048.jsonl`(~9GB)„ÄÇ
ÈïøÂ∫¶`&lt;1024`ÁöÑÈÉ®ÂàÜÂØºÂá∫‰∏∫`sft_1024.jsonl`(~5.5GB)ÔºåÁî®Â§ßÊ®°ÂûãÂØπËØùÊï∞ÊçÆÁõ¥Êé•ËøõË°åsftÂ∞±Â±û‰∫é‚ÄúÈªëÁõíËí∏È¶è‚ÄùÁöÑËåÉÁï¥„ÄÇ

Ëøõ‰∏ÄÊ≠•Ê∏ÖÊ¥óÂâç‰∏§Ê≠•sftÁöÑÊï∞ÊçÆÔºàÂè™‰øùÁïô‰∏≠ÊñáÂ≠óÁ¨¶Âç†ÊØîÈ´òÁöÑÂÜÖÂÆπÔºâÔºåÁ≠õÈÄâÈïøÂ∫¶`&lt;512`ÁöÑÂØπËØùÔºåÂæóÂà∞`sft_mini_512.jsonl`(~1.2GB)„ÄÇ

ÊâÄÊúâsftÊñá‰ª∂ `sft_X.jsonl` Êï∞ÊçÆÊ†ºÂºèÂùá‰∏∫

```text
{
    &quot;conversations&quot;: [
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;‰Ω†Â•Ω&quot;},
        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;‰Ω†Â•ΩÔºÅ&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;ÂÜçËßÅ&quot;},
        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;ÂÜçËßÅÔºÅ&quot;}
    ]
}
```

## ‚Ö£ RLHFÊï∞ÊçÆ

Êù•Ëá™[Magpie-DPOÊï∞ÊçÆÈõÜ](https://www.modelscope.cn/datasets/Magpie-Align/MagpieLM-DPO-Data-v0.1)
Â§ßÁ∫¶200kÊù°ÂÅèÂ•ΩÊï∞ÊçÆÔºàÂùáÊòØËã±ÊñáÔºâÁîüÊàêËá™Llama3.1-70B/8BÔºåÂèØ‰ª•Áî®‰∫éËÆ≠ÁªÉÂ•ñÂä±Ê®°ÂûãÔºå‰ºòÂåñÊ®°ÂûãÂõûÂ§çË¥®ÈáèÔºå‰ΩøÂÖ∂Êõ¥Âä†Á¨¶Âêà‰∫∫Á±ªÂÅèÂ•Ω„ÄÇ
ËøôÈáåÂ∞ÜÊï∞ÊçÆÊÄªÈïøÂ∫¶`&lt;3000`ÁöÑÂÜÖÂÆπÈáçÁªÑ‰∏∫`dpo.jsonl`(~0.9GB)ÔºåÂåÖÂê´`chosen`Âíå`rejected`‰∏§‰∏™Â≠óÊÆµÔºå`chosen`
‰∏∫ÂÅèÂ•ΩÁöÑÂõûÂ§çÔºå`rejected`‰∏∫ÊãíÁªùÁöÑÂõûÂ§ç„ÄÇ

Êñá‰ª∂ `dpo.jsonl` Êï∞ÊçÆÊ†ºÂºè‰∏∫

```text
{
  &quot;chosen&quot;: [
    {&quot;content&quot;: &quot;Q&quot;, &quot;role&quot;: &quot;user&quot;}, 
    {&quot;content&quot;: &quot;good answer&quot;, &quot;role&quot;: &quot;assistant&quot;}
  ], 
  &quot;rejected&quot;: [
    {&quot;content&quot;: &quot;Q&quot;, &quot;role&quot;: &quot;user&quot;}, 
    {&quot;content&quot;: &quot;bad answer&quot;, &quot;role&quot;: &quot;assistant&quot;}
  ]
}
```

## ‚Ö§ ReasonÊï∞ÊçÆÈõÜÔºö

‰∏çÂæó‰∏çËØ¥2025Âπ¥2ÊúàË∞ÅËÉΩÁÅ´ÁöÑËøáDeepSeek...
‰πüÊøÄÂèë‰∫ÜÊàëÂØπRLÂºïÂØºÁöÑÊé®ÁêÜÊ®°ÂûãÁöÑÊµìÂéöÂÖ¥Ë∂£ÔºåÁõÆÂâçÂ∑≤ÁªèÁî®Qwen2.5Â§çÁé∞‰∫ÜR1-Zero„ÄÇ
Â¶ÇÊûúÊúâÊó∂Èó¥+ÊïàÊûúworkÔºà‰ΩÜ99%Âü∫Ê®°ËÉΩÂäõ‰∏çË∂≥ÔºâÊàë‰ºöÂú®‰πãÂêéÊõ¥Êñ∞MiniMindÂü∫‰∫éRLËÆ≠ÁªÉÁöÑÊé®ÁêÜÊ®°ÂûãËÄå‰∏çÊòØËí∏È¶èÊ®°Âûã„ÄÇ
Êó∂Èó¥ÊúâÈôêÔºåÊúÄÂø´ÁöÑ‰ΩéÊàêÊú¨ÊñπÊ°à‰æùÁÑ∂ÊòØÁõ¥Êé•Ëí∏È¶èÔºàÈªëÁõíÊñπÂºèÔºâ„ÄÇ
ËÄê‰∏ç‰ΩèR1Â§™ÁÅ´ÔºåÁü≠Áü≠Âá†Â§©Â∞±Â∑≤ÁªèÂ≠òÂú®‰∏Ä‰∫õR1ÁöÑËí∏È¶èÊï∞ÊçÆÈõÜ[R1-Llama-70B](https://www.modelscope.cn/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B)„ÄÅ[R1-Distill-SFT](https://www.modelscope.cn/datasets/AI-ModelScope/R1-Distill-SFT)„ÄÅ
[Alpaca-Distill-R1](https://huggingface.co/datasets/shareAI/Alpaca-Distill-R1-ZH)„ÄÅ
[deepseek_r1_zh](https://huggingface.co/datasets/jinliuxi/deepseek_r1_zh)Á≠âÁ≠âÔºåÁ∫Ø‰∏≠ÊñáÁöÑÊï∞ÊçÆÂèØËÉΩÊØîËæÉÂ∞ë„ÄÇ
ÊúÄÁªàÊï¥ÂêàÂÆÉ‰ª¨ÔºåÂØºÂá∫Êñá‰ª∂‰∏∫`r1_mix_1024.jsonl`ÔºåÊï∞ÊçÆÊ†ºÂºèÂíå`sft_X.jsonl`‰∏ÄËá¥„ÄÇ

## ‚Ö• Êõ¥Â§öÊï∞ÊçÆÈõÜ

ÁõÆÂâçÂ∑≤ÁªèÊúâ[HqWu-HITCS/Awesome-Chinese-LLM](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM)
Âú®Êî∂ÈõÜÂíåÊ¢≥ÁêÜ‰∏≠ÊñáLLMÁõ∏ÂÖ≥ÁöÑÂºÄÊ∫êÊ®°Âûã„ÄÅÂ∫îÁî®„ÄÅÊï∞ÊçÆÈõÜÂèäÊïôÁ®ãÁ≠âËµÑÊñôÔºåÂπ∂ÊåÅÁª≠Êõ¥Êñ∞ËøôÊñπÈù¢ÁöÑÊúÄÊñ∞ËøõÂ±ï„ÄÇÂÖ®Èù¢‰∏î‰∏ì‰∏öÔºåRespectÔºÅ

---

## ‚Öß MiniMindËÆ≠ÁªÉÊï∞ÊçÆÈõÜ

&gt; [!NOTE]
&gt; 2025-02-05ÂêéÔºåÂºÄÊ∫êMiniMindÊúÄÁªàËÆ≠ÁªÉÊâÄÁî®ÁöÑÊâÄÊúâÊï∞ÊçÆÈõÜÔºåÂõ†Ê≠§Êó†ÈúÄÂÜçËá™Ë°åÈ¢ÑÂ§ÑÁêÜÂ§ßËßÑÊ®°Êï∞ÊçÆÈõÜÔºåÈÅøÂÖçÈáçÂ§çÊÄßÁöÑÊï∞ÊçÆÂ§ÑÁêÜÂ∑•‰Ωú„ÄÇ

MiniMindËÆ≠ÁªÉÊï∞ÊçÆÈõÜ‰∏ãËΩΩÂú∞ÂùÄÔºö [ModelScope](https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files) | [HuggingFace](https://huggingface.co/datasets/jingyaogong/minimind_dataset/tree/main)

&gt; Êó†ÈúÄÂÖ®ÈÉ®cloneÔºåÂèØÂçïÁã¨‰∏ãËΩΩÊâÄÈúÄÁöÑÊñá‰ª∂

Â∞Ü‰∏ãËΩΩÁöÑÊï∞ÊçÆÈõÜÊñá‰ª∂ÊîæÂà∞`./dataset/`ÁõÆÂΩï‰∏ãÔºà‚ú®‰∏∫Êé®ËçêÁöÑÂøÖÈ°ªÈ°πÔºâ

```bash
./dataset/
‚îú‚îÄ‚îÄ dpo.jsonl (55MB, ‚ú®)
‚îú‚îÄ‚îÄ lora_identity.jsonl (22.8KB)
‚îú‚îÄ‚îÄ lora_medical.jsonl (34MB)
‚îú‚îÄ‚îÄ pretrain_hq.jsonl (1.6GB, ‚ú®)
‚îú‚îÄ‚îÄ r1_mix_1024.jsonl (340MB)
‚îú‚îÄ‚îÄ rlaif-mini.jsonl (1MB)
‚îú‚îÄ‚îÄ sft_1024.jsonl (5.6GB)
‚îú‚îÄ‚îÄ sft_2048.jsonl (9GB)
‚îú‚îÄ‚îÄ sft_512.jsonl (7.5GB)
‚îî‚îÄ‚îÄ sft_mini_512.jsonl (1.2GB, ‚ú®)
```

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;Ê≥®ÔºöÂêÑÊï∞ÊçÆÈõÜÁÆÄ‰ªã&lt;/summary&gt;

* `dpo.jsonl`‚ú® --RLHFÈò∂ÊÆµÊï∞ÊçÆÈõÜÔºàÂ∑≤Á≤æÁÆÄ‰ºòÂåñÔºåÈÄÇÂêàÂø´ÈÄüËÆ≠ÁªÉÔºâ
* `lora_identity.jsonl` --Ëá™ÊàëËÆ§Áü•Êï∞ÊçÆÈõÜÔºà‰æãÂ¶ÇÔºö‰Ω†ÊòØË∞ÅÔºüÊàëÊòØminimind...ÔºâÔºåÊé®ËçêÁî®‰∫éloraËÆ≠ÁªÉÔºà‰∫¶ÂèØÁî®‰∫éÂÖ®ÂèÇSFTÔºåÂãøË¢´ÂêçÂ≠óÂ±ÄÈôêÔºâ
* `lora_medical.jsonl` --ÂåªÁñóÈóÆÁ≠îÊï∞ÊçÆÈõÜÔºåÊé®ËçêÁî®‰∫éloraËÆ≠ÁªÉÔºà‰∫¶ÂèØÁî®‰∫éÂÖ®ÂèÇSFTÔºåÂãøË¢´ÂêçÂ≠óÂ±ÄÈôêÔºâ
* `pretrain_hq.jsonl`‚ú® --È¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºåÊï¥ÂêàËá™Âå†Êï∞ÁßëÊäÄ
* `r1_mix_1024.jsonl` --DeepSeek-R1-1.5BËí∏È¶èÊï∞ÊçÆÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫1024ÔºàÂõ†Ê≠§ËÆ≠ÁªÉÊó∂ËÆæÁΩÆmax_seq_len=1024Ôºâ
* `rlaif-mini.jsonl` --RLAIFËÆ≠ÁªÉÊï∞ÊçÆÈõÜÔºå‰ªéSFTÊï∞ÊçÆÈõÜ‰∏≠ÈöèÊú∫ÈááÊ†∑1‰∏áÊù°È´òË¥®ÈáèÂØπËØùÔºåÁî®‰∫éPPO/GRPO/SPOÁ≠âÂº∫ÂåñÂ≠¶‰π†ÁÆóÊ≥ïËÆ≠ÁªÉ
* `sft_1024.jsonl` --Êï¥ÂêàËá™Qwen2.5Ëí∏È¶èÊï∞ÊçÆÔºàÊòØsft_2048ÁöÑÂ≠êÈõÜÔºâÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫1024ÔºàÂõ†Ê≠§ËÆ≠ÁªÉÊó∂ËÆæÁΩÆmax_seq_len=1024Ôºâ
* `sft_2048.jsonl` --Êï¥ÂêàËá™Qwen2.5Ëí∏È¶èÊï∞ÊçÆÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫2048ÔºàÂõ†Ê≠§ËÆ≠ÁªÉÊó∂ËÆæÁΩÆmax_seq_len=2048Ôºâ
* `sft_512.jsonl` --Êï¥ÂêàËá™Âå†Êï∞ÁßëÊäÄSFTÊï∞ÊçÆÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫512ÔºàÂõ†Ê≠§ËÆ≠ÁªÉÊó∂ËÆæÁΩÆmax_seq_len=512Ôºâ
* `sft_mini_512.jsonl`‚ú® --ÊûÅÁÆÄÊï¥ÂêàËá™Âå†Êï∞ÁßëÊäÄSFTÊï∞ÊçÆ+Qwen2.5Ëí∏È¶èÊï∞ÊçÆÔºàÁî®‰∫éÂø´ÈÄüËÆ≠ÁªÉZeroÊ®°ÂûãÔºâÔºåÊØèÊù°Êï∞ÊçÆÂ≠óÁ¨¶ÊúÄÂ§ßÈïøÂ∫¶‰∏∫512ÔºàÂõ†Ê≠§ËÆ≠ÁªÉÊó∂ËÆæÁΩÆmax_seq_len=512Ôºâ

&lt;/details&gt;


![dataset](./images/dataset.jpg)

&lt;details style=&quot;color:rgb(128,128,128)&quot;&gt;
&lt;summary&gt;ËØ¥Êòé &amp; Êé®ËçêËÆ≠ÁªÉÊñπÊ°à&lt;/summary&gt;

* MiniMind2 SeriesÂùáÁªèËøáÂÖ±Á∫¶20GBËØ≠ÊñôËÆ≠ÁªÉÔºåÂ§ßÁ∫¶4B tokensÔºåÂç≥ÂØπÂ∫î‰∏äÈù¢ÁöÑÊï∞ÊçÆÁªÑÂêàËÆ≠ÁªÉÁªìÊûúÔºàÂºÄÈîÄÔºöüí∞üí∞üí∞üí∞üí∞üí∞üí∞üí∞ÔºåÊïàÊûúÔºöüòäüòäüòäüòäüòäüòäÔºâ

* ÊÉ≥Ë¶ÅÊúÄÂø´ÈÄüÂ∫¶‰ªé0ÂÆûÁé∞ZeroÊ®°ÂûãÔºåÊé®Ëçê‰ΩøÁî®`pretrain_hq.jsonl` + `sft_mini_512.jsonl` ÁöÑÊï∞ÊçÆÁªÑÂêàÔºåÂÖ∑‰ΩìËä±ÈîÄÂíåÊïàÊûúÂèØÊü•Áúã‰∏ãÊñáË°®Ê†ºÔºàÂºÄÈîÄÔºöüí∞ÔºåÊïàÊûúÔºöüòäüòäÔºâ

* Êé®ËçêÂÖ∑Â§á‰∏ÄÂÆöÁÆóÂäõËµÑÊ∫êÊàñÊõ¥Âú®ÊÑèÊïàÊûúÁöÑÊúãÂèãÂèØ‰ª•ËÄÉËôëÂâçËÄÖÂÆåÊï¥Â§çÁé∞MiniMind2Ôºõ‰ªÖÊúâÂçïÂç°GPUÊàñÂú®‰πéÁü≠Êó∂Èó¥Âø´ÈÄüÂ§çÁé∞ÁöÑÊúãÂèãÂº∫ÁÉàÊé®ËçêÂêéËÄÖÔºõ

* „ÄêÊäò‰∏≠ÊñπÊ°à„Äë‰∫¶ÂèØÈÄâÊã©‰æãÂ¶Ç`sft_mini_512.jsonl`„ÄÅ`sft_1024.jsonl`‰∏≠Á≠âËßÑÊ®°Êï∞ÊçÆËøõË°åËá™Áî±ÁªÑÂêàËÆ≠ÁªÉÔºàÂºÄÈîÄÔºöüí∞üí∞üí∞ÔºåÊïàÊûúÔºöüòäüòäüòäüòäÔºâ„ÄÇ

&lt;/details&gt;

# üìå Model

## Structure

MiniMind-DenseÔºàÂíå[Llama3.1](https://ai.meta.com/blog/meta-llama-3-1/)‰∏ÄÊ†∑Ôºâ‰ΩøÁî®‰∫ÜTransformerÁöÑDecoder-OnlyÁªìÊûÑÔºåË∑üGPT-3ÁöÑÂå∫Âà´Âú®‰∫éÔºö

* ÈááÁî®‰∫ÜGPT-3ÁöÑÈ¢ÑÊ†áÂáÜÂåñÊñπÊ≥ïÔºå‰πüÂ∞±ÊòØÂú®ÊØè‰∏™TransformerÂ≠êÂ±ÇÁöÑËæìÂÖ•‰∏äËøõË°åÂΩí‰∏ÄÂåñÔºåËÄå‰∏çÊòØÂú®ËæìÂá∫‰∏ä„ÄÇÂÖ∑‰ΩìÊù•ËØ¥Ôºå‰ΩøÁî®ÁöÑÊòØRMSNormÂΩí‰∏ÄÂåñÂáΩÊï∞„ÄÇ
* Áî®SwiGLUÊøÄÊ¥ªÂáΩÊï∞Êõø‰ª£‰∫ÜReLUÔºåËøôÊ†∑ÂÅöÊòØ‰∏∫‰∫ÜÊèêÈ´òÊÄßËÉΩ„ÄÇ
* ÂÉèGPT-Neo‰∏ÄÊ†∑ÔºåÂéªÊéâ‰∫ÜÁªùÂØπ‰ΩçÁΩÆÂµåÂÖ•ÔºåÊîπÁî®‰∫ÜÊóãËΩ¨‰ΩçÁΩÆÂµåÂÖ•ÔºàRoPEÔºâÔºåËøôÊ†∑Âú®Â§ÑÁêÜË∂ÖÂá∫ËÆ≠ÁªÉÈïøÂ∫¶ÁöÑÊé®ÁêÜÊó∂ÊïàÊûúÊõ¥Â•Ω„ÄÇ

---

MiniMind-MoEÊ®°ÂûãÔºåÂÆÉÁöÑÁªìÊûÑÂü∫‰∫éLlama3Âíå[Deepseek-V2/3](https://arxiv.org/pdf/2405.04434)‰∏≠ÁöÑMixFFNÊ∑∑Âêà‰∏ìÂÆ∂Ê®°Âùó„ÄÇ

* DeepSeek-V2Âú®ÂâçÈ¶àÁΩëÁªúÔºàFFNÔºâÊñπÈù¢ÔºåÈááÁî®‰∫ÜÊõ¥ÁªÜÁ≤íÂ∫¶ÁöÑ‰∏ìÂÆ∂ÂàÜÂâ≤ÂíåÂÖ±‰∫´ÁöÑ‰∏ìÂÆ∂ÈöîÁ¶ªÊäÄÊúØÔºå‰ª•ÊèêÈ´òExpertsÁöÑÊïàÊûú„ÄÇ

---

MiniMindÁöÑÊï¥‰ΩìÁªìÊûÑ‰∏ÄËá¥ÔºåÂè™ÊòØÂú®RoPEËÆ°ÁÆó„ÄÅÊé®ÁêÜÂáΩÊï∞ÂíåFFNÂ±ÇÁöÑ‰ª£Á†Å‰∏äÂÅö‰∫Ü‰∏Ä‰∫õÂ∞èË∞ÉÊï¥„ÄÇ
ÂÖ∂ÁªìÊûÑÂ¶Ç‰∏ãÂõæÔºàÈáçÁªòÁâàÔºâÔºö

![structure](./images/LLM-structure.png)
![structure-moe](./images/LLM-structure-moe.png)

‰øÆÊîπÊ®°ÂûãÈÖçÁΩÆËßÅ[./model/model_minimind.py](./model/model_minimind.py)„ÄÇ
ÂèÇËÄÉÊ®°ÂûãÂèÇÊï∞ÁâàÊú¨ËßÅ‰∏ãË°®Ôºö

| Model Name        | params | len_vocab | rope_theta | n_layers | d_model | kv_heads | q_heads | share+route |
|-------------------|--------|-----------|------------|----------|---------|----------|---------|-------------|
| MiniMind2-Small   | 26M    | 6400      | 1e6        | 8        | 512     | 2        | 8       | -           |
| MiniMind2-MoE     | 145M   | 6400      | 1e6        | 8        | 640     | 2        | 8       | 1+4         |
| MiniMind2         | 104M   | 6400      | 1e6        | 16       | 768     | 2        | 8       | -           |
| minimind-v1-small | 26M    | 6400      | 1e4        | 8        | 512     | 8        | 16      | -           |
| minimind-v1-moe   | 4√ó26M  | 6400      | 1e4        | 8        | 512     | 8        | 16      | 1+4         |
| minimind-v1       | 108M   | 6400      | 1e4        | 16       | 768     | 8        | 16      | -           |


## Model Configuration

üìãÂÖ≥‰∫éLLMÁöÑÂèÇÊï∞ÈÖçÁΩÆÔºåÊúâ‰∏ÄÁØáÂæàÊúâÊÑèÊÄùÁöÑËÆ∫Êñá[MobileLLM](https://arxiv.org/pdf/2402.14905)ÂÅö‰∫ÜËØ¶ÁªÜÁöÑÁ†îÁ©∂ÂíåÂÆûÈ™å„ÄÇ
Scaling LawÂú®Â∞èÊ®°Âûã‰∏≠ÊúâËá™Â∑±Áã¨ÁâπÁöÑËßÑÂæã„ÄÇ
ÂºïËµ∑TransformerÂèÇÊï∞ÊàêËßÑÊ®°ÂèòÂåñÁöÑÂèÇÊï∞Âá†‰πéÂè™ÂèñÂÜ≥‰∫é`d_model`Âíå`n_layers`„ÄÇ

* `d_model`‚Üë + `n_layers`‚Üì -&gt; ÁüÆËÉñÂ≠ê
* `d_model`‚Üì + `n_layers`‚Üë -&gt; Áò¶È´ò‰∏™

2020Âπ¥ÊèêÂá∫Scaling LawÁöÑËÆ∫ÊñáËÆ§‰∏∫ÔºåËÆ≠ÁªÉÊï∞ÊçÆÈáè„ÄÅÂèÇÊï∞Èáè‰ª•ÂèäËÆ≠ÁªÉËø≠‰ª£Ê¨°Êï∞ÊâçÊòØÂÜ≥ÂÆöÊÄßËÉΩÁöÑÂÖ≥ÈîÆÂõ†Á¥†ÔºåËÄåÊ®°ÂûãÊû∂ÊûÑÁöÑÂΩ±ÂìçÂá†‰πéÂèØ‰ª•ÂøΩËßÜ„ÄÇ
ÁÑ∂ËÄå‰ºº‰πéËøô‰∏™ÂÆöÂæãÂØπÂ∞èÊ®°ÂûãÂπ∂‰∏çÂÆåÂÖ®ÈÄÇÁî®„ÄÇ
MobileLLMÊèêÂá∫Êû∂ÊûÑÁöÑÊ∑±Â∫¶ÊØîÂÆΩÂ∫¶Êõ¥ÈáçË¶ÅÔºå„ÄåÊ∑±ËÄåÁ™Ñ„ÄçÁöÑ„ÄåÁò¶Èïø„ÄçÊ®°ÂûãÂèØ‰ª•Â≠¶‰π†Âà∞ÊØî„ÄåÂÆΩËÄåÊµÖ„ÄçÊ®°ÂûãÊõ¥Â§öÁöÑÊäΩË±°Ê¶ÇÂøµ„ÄÇ
‰æãÂ¶ÇÂΩìÊ®°ÂûãÂèÇÊï∞Âõ∫ÂÆöÂú®125MÊàñËÄÖ350MÊó∂Ôºå30ÔΩû42Â±ÇÁöÑ„ÄåÁã≠Èïø„ÄçÊ®°ÂûãÊòéÊòæÊØî12Â±ÇÂ∑¶Âè≥ÁöÑ„ÄåÁüÆËÉñ„ÄçÊ®°ÂûãÊúâÊõ¥‰ºòË∂äÁöÑÊÄßËÉΩÔºå
Âú®Â∏∏ËØÜÊé®ÁêÜ„ÄÅÈóÆÁ≠î„ÄÅÈòÖËØªÁêÜËß£Á≠â8‰∏™Âü∫ÂáÜÊµãËØï‰∏äÈÉΩÊúâÁ±ª‰ººÁöÑË∂ãÂäø„ÄÇ
ËøôÂÖ∂ÂÆûÊòØÈùûÂ∏∏ÊúâË∂£ÁöÑÂèëÁé∞ÔºåÂõ†‰∏∫‰ª•ÂæÄ‰∏∫100MÂ∑¶Âè≥ÈáèÁ∫ßÁöÑÂ∞èÊ®°ÂûãËÆæËÆ°Êû∂ÊûÑÊó∂ÔºåÂá†‰πéÊ≤°‰∫∫Â∞ùËØïËøáÂè†Âä†Ë∂ÖËøá12Â±Ç„ÄÇ
Ëøô‰∏éMiniMindÂú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåÊ®°ÂûãÂèÇÊï∞ÈáèÂú®`d_model`Âíå`n_layers`‰πãÈó¥ËøõË°åË∞ÉÊï¥ÂÆûÈ™åËßÇÂØüÂà∞ÁöÑÊïàÊûúÊòØ‰∏ÄËá¥ÁöÑ„ÄÇ
ÁÑ∂ËÄå„ÄåÊ∑±ËÄåÁ™Ñ„ÄçÁöÑ„ÄåÁ™Ñ„Äç‰πüÊòØÊúâÁª¥Â∫¶ÊûÅÈôêÁöÑÔºåÂΩìd_model&lt;512Êó∂ÔºåËØçÂµåÂÖ•Áª¥Â∫¶ÂùçÂ°åÁöÑÂä£ÂäøÈùûÂ∏∏ÊòéÊòæÔºå
Â¢ûÂä†ÁöÑlayersÂπ∂‰∏çËÉΩÂº•Ë°•ËØçÂµåÂÖ•Âú®Âõ∫ÂÆöq_headÂ∏¶Êù•d_head‰∏çË∂≥ÁöÑÂä£Âäø„ÄÇ
ÂΩìd_model&gt;1536Êó∂ÔºålayersÁöÑÂ¢ûÂä†‰ºº‰πéÊØîd_modelÁöÑ‰ºòÂÖàÁ∫ßÊõ¥È´òÔºåÊõ¥ËÉΩÂ∏¶Êù•ÂÖ∑Êúâ&quot;ÊÄß‰ª∑ÊØî&quot;ÁöÑÂèÇÊï∞-&gt;ÊïàÊûúÂ¢ûÁõä„ÄÇ

* Âõ†Ê≠§MiniMindËÆæÂÆösmallÊ®°Âûãdim=512Ôºån_layers=8Êù•Ëé∑ÂèñÁöÑ„ÄåÊûÅÂ∞è‰ΩìÁßØ&lt;-&gt;Êõ¥Â•ΩÊïàÊûú„ÄçÁöÑÂπ≥Ë°°„ÄÇ
* ËÆæÂÆödim=768Ôºån_layers=16Êù•Ëé∑ÂèñÊïàÊûúÁöÑÊõ¥Â§ßÊî∂ÁõäÔºåÊõ¥Âä†Á¨¶ÂêàÂ∞èÊ®°ÂûãScaling-LawÁöÑÂèòÂåñÊõ≤Á∫ø„ÄÇ

‰Ωú‰∏∫ÂèÇËÄÉÔºåGPT3ÁöÑÂèÇÊï∞ËÆæÂÆöËßÅ‰∏ãË°®Ôºö
![gpt3_config.png](./images/gpt3_config.png)

---

# üìå Experiment

## ‚Ö† ËÆ≠ÁªÉÂºÄÈîÄ

- **Êó∂Èó¥Âçï‰Ωç**ÔºöÂ∞èÊó∂ (h)„ÄÇ
- **ÊàêÊú¨Âçï‰Ωç**Ôºö‰∫∫Ê∞ëÂ∏Å (Ôø•)Ôºõ7Ôø• ‚âà 1ÁæéÂÖÉ„ÄÇ
- **3090 ÁßüÂç°Âçï‰ª∑**Ôºö‚âà1.3Ôø•/hÔºàÂèØËá™Ë°åÂèÇËÄÉÂÆûÊó∂Â∏Ç‰ª∑Ôºâ„ÄÇ
- **ÂèÇËÄÉÊ†áÂáÜ**ÔºöË°®Ê†º‰ªÖÂÆûÊµã `pretrain` Âíå `sft_mini_512` ‰∏§‰∏™Êï∞ÊçÆÈõÜÁöÑËÆ≠ÁªÉÊó∂Èó¥ÔºåÂÖ∂ÂÆÉËÄóÊó∂Ê†πÊçÆÊï∞ÊçÆÈõÜÂ§ßÂ∞è‰º∞ÁÆóÔºàÂèØËÉΩÂ≠òÂú®‰∫õËÆ∏Âá∫ÂÖ•Ôºâ„ÄÇ

&gt; Âü∫‰∫é 3090 ÔºàÂçïÂç°ÔºâÊàêÊú¨ËÆ°ÁÆó

| Model Name      | params | pretrain  

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/call-center-ai]]></title>
            <link>https://github.com/microsoft/call-center-ai</link>
            <guid>https://github.com/microsoft/call-center-ai</guid>
            <pubDate>Wed, 12 Nov 2025 00:04:32 GMT</pubDate>
            <description><![CDATA[Send a phone call from AI agent, in an API call. Or, directly call the bot from the configured phone number!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/call-center-ai">microsoft/call-center-ai</a></h1>
            <p>Send a phone call from AI agent, in an API call. Or, directly call the bot from the configured phone number!</p>
            <p>Language: Python</p>
            <p>Stars: 2,087</p>
            <p>Forks: 322</p>
            <p>Stars today: 544 stars today</p>
            <h2>README</h2><pre># Call Center AI

AI-powered call center solution with Azure and OpenAI GPT.

&lt;!-- github.com badges --&gt;
[![Last release date](https://img.shields.io/github/release-date/clemlesne/call-center-ai)](https://github.com/clemlesne/call-center-ai/releases)
[![Project license](https://img.shields.io/github/license/clemlesne/call-center-ai)](https://github.com/clemlesne/call-center-ai/blob/main/LICENSE)

&lt;!-- GitHub Codespaces badge --&gt;
[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/call-center-ai?quickstart=1)

## Overview

Send a phone call from AI agent, in an API call. Or, directly call the bot from the configured phone number!

Insurance, IT support, customer service, and more. The bot can be customized in few hours (really) to fit your needs.

```bash
# Ask the bot to call a phone number
data=&#039;{
  &quot;bot_company&quot;: &quot;Contoso&quot;,
  &quot;bot_name&quot;: &quot;Am√©lie&quot;,
  &quot;phone_number&quot;: &quot;+11234567890&quot;,
  &quot;task&quot;: &quot;Help the customer with their digital workplace. Assistant is working for the IT support department. The objective is to help the customer with their issue and gather information in the claim.&quot;,
  &quot;agent_phone_number&quot;: &quot;+33612345678&quot;,
  &quot;claim&quot;: [
    {
      &quot;name&quot;: &quot;hardware_info&quot;,
      &quot;type&quot;: &quot;text&quot;
    },
    {
      &quot;name&quot;: &quot;first_seen&quot;,
      &quot;type&quot;: &quot;datetime&quot;
    },
    {
      &quot;name&quot;: &quot;building_location&quot;,
      &quot;type&quot;: &quot;text&quot;
    }
  ]
}&#039;

curl \
  --header &#039;Content-Type: application/json&#039; \
  --request POST \
  --url https://xxx/call \
  --data $data
```

### Features

- **Enhanced communication and user experience**: Integrates inbound and outbound calls with a dedicated phone number, supports multiple languages and voice tones, and allows users to provide or receive information via SMS. Conversations are **streamed in real-time** to avoid delays, can be **resumed after disconnections**, and are **stored for future reference**. This ensures an **improved customer experience**, enabling 24/7 communication and handling of low to medium complexity calls, all in a more accessible and user-friendly manner.

- **Advanced intelligence and data management**: Leverages **gpt-4.1** and **gpt-4.1-nano** (known for higher performance and a 10‚Äì15x cost premium) to achieve nuanced comprehension. It can discuss **private and sensitive data**, including customer-specific information, while following **retrieval-augmented generation (RAG)** best practices to ensure secure and compliant handling of internal documents. The system understands domain-specific terms, follows a structured claim schema, generates automated to-do lists, filters inappropriate content, and detects jailbreak attempts. Historical conversations and past interactions can also be used to **fine-tune the LLM**, improving accuracy and personalization over time. Redis caching further enhances efficiency.

- **Customization, oversight, and scalability**: Offers **customizable prompts**, feature flags for controlled experimentation, human agent fallback, and call recording for quality assurance. Integrates Application Insights for monitoring and tracing, provides publicly accessible claim data, and plans future enhancements such as automated callbacks and IVR-like workflows. It also enables the creation of a **brand-specific custom voice**, allowing the assistant‚Äôs voice to reflect the company‚Äôs identity and improve brand consistency.

- **Cloud-native deployment and resource management**: Deployed on **Azure** with a containerized, serverless architecture for low maintenance and elastic scaling. This approach optimizes costs based on usage, ensuring flexibility and affordability over time. Seamless integration with **Azure Communication Services**, **Cognitive Services**, and **OpenAI resources** provides a secure environment suitable for rapid iteration, continuous improvement, and accommodating variable workloads in the call center.

### Demo

A French demo is avaialble on YouTube. Do not hesitate to watch the demo in x1.5 speed to get a quick overview of the project. Voice is hesitant on purpose to show the bot can handle it. All the infrastructure is deployed on Azure, mostly in serverless mode. Provisionning of the LLM resources can be done to reduce the latency.

[![French demo](https://img.youtube.com/vi/i_qhNdUUxSI/maxresdefault.jpg)](https://youtube.com/watch?v=i_qhNdUUxSI)

Main interactions shown in the demo:

1. User calls the call center
2. The bot answers and the conversation starts
3. The bot stores conversation, claim and todo list in the database

Extract of the data stored during the call:

```json
{
  &quot;claim&quot;: {
    &quot;incident_description&quot;: &quot;Collision avec un autre v√©hicule, voiture dans le foss√©, pas de bless√©s&quot;,
    &quot;incident_location&quot;: &quot;Nationale 17&quot;,
    &quot;involved_parties&quot;: &quot;Dujardin, Madame Lesn√©&quot;,
    &quot;policy_number&quot;: &quot;DEC1748&quot;
  },
  &quot;messages&quot;: [
    {
      &quot;created_at&quot;: &quot;2024-12-10T15:51:04.566727Z&quot;,
      &quot;action&quot;: &quot;talk&quot;,
      &quot;content&quot;: &quot;Non, je pense que c&#039;est pas mal. Vous avez r√©pondu √† mes questions et l√† j&#039;attends la d√©paneuse. Merci beaucoup.&quot;,
      &quot;persona&quot;: &quot;human&quot;,
      &quot;style&quot;: &quot;none&quot;,
      &quot;tool_calls&quot;: []
    },
    {
      &quot;created_at&quot;: &quot;2024-12-10T15:51:06.040451Z&quot;,
      &quot;action&quot;: &quot;talk&quot;,
      &quot;content&quot;: &quot;Je suis ravi d&#039;avoir pu vous aider! Si vous avez besoin de quoi que ce soit d&#039;autre, n&#039;h√©sitez pas √† nous contacter. Je vous souhaite une bonne journ√©e et j&#039;esp√®re que tout se passera bien avec la d√©panneuse. Au revoir!&quot;,
      &quot;persona&quot;: &quot;assistant&quot;,
      &quot;style&quot;: &quot;none&quot;,
      &quot;tool_calls&quot;: []
    }
  ],
  &quot;next&quot;: {
    &quot;action&quot;: &quot;case_closed&quot;,
    &quot;justification&quot;: &quot;The customer has provided all necessary information for the insurance claim, and a reminder has been set for a follow-up call. The customer is satisfied with the assistance provided and is waiting for the tow truck. The case can be closed for now.&quot;
  },
  &quot;reminders&quot;: [
    {
      &quot;created_at&quot;: &quot;2024-12-10T15:50:09.507903Z&quot;,
      &quot;description&quot;: &quot;Rappeler le client pour faire le point sur l&#039;accident et l&#039;avancement du dossier.&quot;,
      &quot;due_date_time&quot;: &quot;2024-12-11T14:30:00&quot;,
      &quot;owner&quot;: &quot;assistant&quot;,
      &quot;title&quot;: &quot;Rappel client sur l&#039;accident&quot;
    }
  ],
  &quot;synthesis&quot;: {
    &quot;long&quot;: &quot;During our call, you reported an accident involving your vehicle on the Nationale 17. You mentioned that there were no injuries, but both your car and the other vehicle ended up in a ditch. The other party involved is named Dujardin, and your vehicle is a 4x4 Ford. I have updated your claim with these details, including the license plates: yours is U837GE and the other vehicle&#039;s is GA837IA. A reminder has been set for a follow-up call tomorrow at 14:30 to discuss the progress of your claim. If you need further assistance, please feel free to reach out.&quot;,
    &quot;satisfaction&quot;: &quot;high&quot;,
    &quot;short&quot;: &quot;the accident on Nationale 17&quot;,
    &quot;improvement_suggestions&quot;: &quot;To improve the customer experience, it would be beneficial to ensure that the call connection is stable to avoid interruptions. Additionally, providing a clear step-by-step guide on what information is needed for the claim could help streamline the process and reduce any confusion for the customer.&quot;
  }
  ...
}
```

### User report after the call

A report is available at `https://[your_domain]/report/[phone_number]` (like `http://localhost:8080/report/%2B133658471534`). It shows the conversation history, claim data and reminders.

![User report](./docs/user_report.png)

## Architecture

### High level architecture

```mermaid
---
title: System diagram (C4 model)
---
graph
  user([&quot;User&quot;])
  agent([&quot;Agent&quot;])

  app[&quot;Call Center AI&quot;]

  app -- Transfer to --&gt; agent
  app -. Send voice .-&gt; user
  user -- Call --&gt; app
```

### Component level architecture

```mermaid
---
title: Claim AI component diagram (C4 model)
---
graph LR
  agent([&quot;Agent&quot;])
  user([&quot;User&quot;])

  subgraph &quot;Claim AI&quot;
    ada[&quot;Embedding&lt;br&gt;(ADA)&quot;]
    app[&quot;App&lt;br&gt;(Container App)&quot;]
    communication_services[&quot;Call &amp; SMS gateway&lt;br&gt;(Communication Services)&quot;]
    db[(&quot;Conversations and claims&lt;br&gt;(Cosmos DB)&quot;)]
    eventgrid[&quot;Broker&lt;br&gt;(Event Grid)&quot;]
    gpt[&quot;LLM&lt;br&gt;(gpt-4.1, gpt-4.1-nano)&quot;]
    queues[(&quot;Queues&lt;br&gt;(Azure Storage)&quot;)]
    redis[(&quot;Cache&lt;br&gt;(Redis)&quot;)]
    search[(&quot;RAG&lt;br&gt;(AI Search)&quot;)]
    sounds[(&quot;Sounds&lt;br&gt;(Azure Storage)&quot;)]
    sst[&quot;Speech-to-text&lt;br&gt;(Cognitive Services)&quot;]
    translation[&quot;Translation&lt;br&gt;(Cognitive Services)&quot;]
    tts[&quot;Text-to-speech&lt;br&gt;(Cognitive Services)&quot;]
  end

  app -- Translate static TTS --&gt; translation
  app -- Sezarch RAG data --&gt; search
  app -- Generate completion --&gt; gpt
  gpt -. Answer with completion .-&gt; app
  app -- Generate voice --&gt; tts
  tts -. Answer with voice .-&gt; app
  app -- Get cached data --&gt; redis
  app -- Save conversation --&gt; db
  app -- Transform voice --&gt; sst
  sst -. Answer with text .-&gt; app
  app &lt;-. Exchange audio .-&gt; communication_services
  app -. Watch .-&gt; queues

  communication_services -- Load sound --&gt; sounds
  communication_services -- Notifies --&gt; eventgrid
  communication_services -- Transfer to --&gt; agent
  communication_services &lt;-. Exchange audio .-&gt; agent
  communication_services &lt;-. Exchange audio .-&gt; user

  eventgrid -- Push to --&gt; queues

  search -- Generate embeddings --&gt; ada

  user -- Call --&gt; communication_services
```

## Deployment

&gt; [!NOTE]
&gt; This project is a proof of concept. It is not intended to be used in production. This demonstrates how can be combined Azure Communication Services, Azure Cognitive Services and Azure OpenAI to build an automated call center solution.

### Prerequisites

[Prefer using GitHub Codespaces for a quick start.](https://codespaces.new/microsoft/call-center-ai?quickstart=1) The environment will setup automatically with all the required tools.

In macOS, with [Homebrew](https://brew.sh), simply type `make brew`.

For other systems, make sure you have the following installed:

- [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli)
- [Twilio CLI](https://www.twilio.com/docs/twilio-cli/getting-started/install) (optional)
- [yq](https://github.com/mikefarah/yq?tab=readme-ov-file#install)
- Bash compatible shell, like `bash` or `zsh`
- Make, `apt install make` (Ubuntu), `yum install make` (CentOS), `brew install make` (macOS)

Then, Azure resources are needed:

#### 1. [Create a new resource group](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/manage-resource-groups-portal)

- Prefer to use lowercase and no special characters other than dashes (e.g. `ccai-customer-a`)

#### 2. [Create a Communication Services resource](https://learn.microsoft.com/en-us/azure/communication-services/quickstarts/create-communication-resource?tabs=linux&amp;pivots=platform-azp)

- Same name as the resource group
- Enable system managed identity

#### 3. [Buy a phone number](https://learn.microsoft.com/en-us/azure/communication-services/quickstarts/telephony/get-phone-number?tabs=linux&amp;pivots=platform-azp-new)

- From the Communication Services resource
- Allow inbound and outbound communication
- Enable voice (required) and SMS (optional) capabilities

Now that the prerequisites are configured (local + Azure), the deployment can be done.

### Remote (on Azure)

A pre-built container image is available on GitHub Actions, it will be used to deploy the solution on Azure:

- Latest version from a branch: `ghcr.io/clemlesne/call-center-ai:main`
- Specific tag: `ghcr.io/clemlesne/call-center-ai:0.1.0` (recommended)

#### 1. Create the light config file

Fill the template from the example at [`config-remote-example.yaml`](./config-remote-example.yaml). The file should be placed at the root of the project under the name `config.yaml`. It will be used by install scripts (incl. Makefile and Bicep) to configure the Azure resources.

#### 2. Connect to your Azure environment

```zsh
az login
```

#### 3. Run deployment automation

&gt; [!TIP]
&gt; Specify the release version under the `image_version` parameter (default is `main`). For example, `image_version=16.0.0` or `image_version=sha-7ca2c0c`. This will ensure any future project breaking changes won&#039;t affect your deployment.

```zsh
make deploy name=my-rg-name
```

Wait for the deployment to finish.

#### 4. Get the logs

```zsh
make logs name=my-rg-name
```

### Local (on your machine)

#### 1. Prerequisites

If you skiped the `make brew` command from the first install section, make sure you have the following installed:

- [Rust](https://rust-lang.org)
- [uv](https://docs.astral.sh/uv)

Finally, run `make install` to setup Python environment.

#### 2. Create the full config file

If the application is already deployed on Azure, you can run `make name=my-rg-name sync-local-config` to copy the configuration from remote to your local machine.

&gt; [!TIP]
&gt; To use a Service Principal to authenticate to Azure, you can also add the following in a `.env` file:
&gt;
&gt; ```dotenv
&gt; AZURE_CLIENT_ID=xxx
&gt; AZURE_CLIENT_SECRET=xxx
&gt; AZURE_TENANT_ID=xxx
&gt; ```

If the solution is not running online, fill the template from the example at [`config-local-example.yaml`](./config-local-example.yaml). The file should be placed at the root of the project under the name `config.yaml`.

#### 3. Run the deployment automation

Execute if the solution is not yet deployed on Azure.

```zsh
make deploy-bicep deploy-post name=my-rg-name
```

- This will deploy the Azure resources without the API server, allowing you to test the bot locally
- Wait for the deployment to finish

#### 4. Connect to Azure Dev tunnels

&gt; [!IMPORTANT]
&gt; Tunnel requires to be run in a separate terminal, because it needs to be running all the time

```zsh
# Log in once
devtunnel login

# Start the tunnel
make tunnel
```

#### 5. Iterate quickly with the code

&gt; [!NOTE]
&gt; To override a specific configuration value, you can use environment variables. For example, to override the `llm.fast.endpoint` value, you can use the `LLM__FAST__ENDPOINT` variable:
&gt;
&gt; ```dotenv
&gt; LLM__FAST__ENDPOINT=https://xxx.openai.azure.com
&gt; ```

&gt; [!NOTE]
&gt; Also, `local.py` script is available to test the application without the need of a phone call (= without Communication Services). Run the script with:
&gt;
&gt; ```bash
&gt; python3 -m tests.local
&gt; ```

```zsh
make dev
```

- Code is automatically reloaded on file changes, no need to restart the server
- The API server is available at `http://localhost:8080`

## Advanced usage

### Enable call recording

Call recording is disabled by default. To enable it:

1. Create a new container in the Azure Storage account (i.e. `recordings`), it is already done if you deployed the solution on Azure
2. Update the feature flag `recording_enabled` in App Configuration to `true`

### Add my custom training data with AI Search

Training data is stored on AI Search to be retrieved by the bot, on demand.

Required index schema:

| **Field Name** | `Type` | Retrievable | Searchable | Dimensions | Vectorizer |
|-|-|-|-|-|-|
| **answer** | `Edm.String` | Yes | Yes | | |
| **context** | `Edm.String` | Yes | Yes | | |
| **created_at** | `Edm.String` | Yes | No | | |
| **document_synthesis** | `Edm.String` | Yes | Yes | | |
| **file_path** | `Edm.String` | Yes | No | | |
| **id** | `Edm.String` | Yes | No | | |
| **question** | `Edm.String` | Yes | Yes | | |
| **vectors** | `Collection(Edm.Single)` | No | Yes | 1536 | *OpenAI ADA* |

Software to fill the index is included [on Synthetic RAG Index](https://github.com/clemlesne/rag-index) repository.

### Customize the languages

The bot can be used in multiple languages. It can understand the language the user chose.

See the [list of supported languages](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support?tabs=tts#supported-languages) for the Text-to-Speech service.

```yaml
# config.yaml
conversation:
  initiate:
    lang:
      default_short_code: fr-FR
      availables:
        - pronunciations_en: [&quot;French&quot;, &quot;FR&quot;, &quot;France&quot;]
          short_code: fr-FR
          voice: fr-FR-DeniseNeural
        - pronunciations_en: [&quot;Chinese&quot;, &quot;ZH&quot;, &quot;China&quot;]
          short_code: zh-CN
          voice: zh-CN-XiaoqiuNeural
```

If you built and deployed an [Azure Speech Custom Neural Voice (CNV)](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/custom-neural-voice), add field `custom_voice_endpoint_id` on the language configuration:

```yaml
# config.yaml
conversation:
  initiate:
    lang:
      default_short_code: fr-FR
      availables:
        - pronunciations_en: [&quot;French&quot;, &quot;FR&quot;, &quot;France&quot;]
          short_code: fr-FR
          voice: xxx
          custom_voice_endpoint_id: xxx
```

### Customize the moderation levels

Levels are defined for each category of Content Safety. The higher the score, the more strict the moderation is, from 0 to 7. Moderation is applied on all bot data, including the web page and the conversation. Configure them in Azure OpenAI Content Filters.

### Customize the claim data schema

Customization of the data schema is fully supported. You can add or remove fields as needed, depending on the requirements.

By default, the schema of composed of:

- `caller_email` (`email`)
- `caller_name` (`text`)
- `caller_phone` (`phone_number`)

Values are validated to ensure the data format commit to your schema. They can be either:

- `datetime`
- `email`
- `phone_number` (`E164` format)
- `text`

Finally, an optional description can be provided. The description must be short and meaningful, it will be passed to the LLM.

Default schema, for inbound calls, is defined in the configuration:

```yaml
# config.yaml
conversation:
  default_initiate:
    claim:
      - name: additional_notes
        type: text
        # description: xxx
      - name: device_info
        type: text
        # description: xxx
      - name: incident_datetime
        type: datetime
        # description: xxx
```

Claim schema can be customized for each call, by adding the `claim` field in the `POST /call` API call.

### Customize the call objective

The objective is a description of what the bot will do during the call. It is used to give a context to the LLM. It should be short, meaningful, and written in English.

This solution is priviledged instead of overriding the LLM prompt.

Default task, for inbound calls, is defined in the configuration:

```yaml
# config.yaml
conversation:
  initiate:
    task: |
      Help the customer with their insurance claim. Assistant requires data from the customer to fill the claim. The latest claim data will be given. Assistant role is not over until all the relevant data is gathered.
```

Task can be customized for each call, by adding the `task` field in the `POST /call` API call.

### Customize the conversation

Conversation options are represented as features. They can be configured from App Configuration, without the need to redeploy or restart the application. Once a feature is updated, a delay of 60 secs is needed to make the change effective.

By default, values are refreshed every 60 seconds. Refresh is not sync across all instances, so it can take up to 60 seconds to see the change on all users. Update this in the `app_configuration.ttl_sec` field.

| Name | Description | Type | Default |
|-|-|-|-|
| `answer_hard_timeout_sec` | Time waiting the LLM before aborting the answer with an error message. | `int` | 15 |
| `answer_soft_timeout_sec` | Time waiting the LLM before sending a waiting message. | `int` | 4 |
| `callback_timeout_hour` | The timeout for a callback in hours. Set 0 to disable. | `int` | 3 |
| `phone_silence_timeout_sec` | Amount of silence in secs to trigger a warning message from the assistant. | `int` | 20 |
| `recognition_retry_max` | TThe maximum number of retries for voice recognition. Minimum of 1. | `int` | 3 |
| `recognition_stt_complete_timeout_ms` | The timeout for STT completion in milliseconds. | `int` | 100 |
| `recording_enabled` | Whether call recording is enabled. | `bool` | false |
| `slow_llm_for_chat` | Whether to use the slow LLM 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[thinking-machines-lab/tinker-cookbook]]></title>
            <link>https://github.com/thinking-machines-lab/tinker-cookbook</link>
            <guid>https://github.com/thinking-machines-lab/tinker-cookbook</guid>
            <pubDate>Wed, 12 Nov 2025 00:04:31 GMT</pubDate>
            <description><![CDATA[Post-training with Tinker]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/thinking-machines-lab/tinker-cookbook">thinking-machines-lab/tinker-cookbook</a></h1>
            <p>Post-training with Tinker</p>
            <p>Language: Python</p>
            <p>Stars: 1,811</p>
            <p>Forks: 145</p>
            <p>Stars today: 95 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;Tinker Cookbook&lt;/h1&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/tinker-cover.png&quot; width=&quot;60%&quot; /&gt;
&lt;/div&gt;

We provide two libraries for the broader community to customize their language models: `tinker` and `tinker-cookbook`.

- `tinker` is a training SDK for researchers and developers to fine-tune language models. You send API requests to us and we handle the complexities of distributed training.
- `tinker-cookbook` includes realistic examples of fine-tuning language models. It builds on the Tinker API and provides common abstractions to fine-tune language models.

## Installation

1. Sign up for Tinker through the [waitlist](https://thinkingmachines.ai/tinker).
2. Once you have access, create an API key from the [console](https://tinker-console.thinkingmachines.ai) and export it as environment variable `TINKER_API_KEY`.
3. Install tinker python client via `pip install tinker`
4. We recommend installing `tinker-cookbook` in a virtual env either with `conda` or `uv`. For running most examples, you can install via `pip install -e .`.

## Tinker

Refer to the [docs](https://tinker-docs.thinkingmachines.ai/training-sampling) to start from basics.
Here we introduce a few Tinker primitives - the basic components to fine-tune LLMs:

```python
service_client = tinker.ServiceClient()
training_client = service_client.create_lora_training_client(
  base_model=&quot;meta-llama/Llama-3.2-1B&quot;, rank=32,
)
training_client.forward_backward(...)
training_client.optim_step(...)
training_client.save_state(...)
training_client.load_state(...)

sampling_client = training_client.save_weights_and_get_sampling_client(name=&quot;my_model&quot;)
sampling_client.sample(...)
```

See [tinker_cookbook/recipes/sl_loop.py](tinker_cookbook/recipes/sl_loop.py) and [tinker_cookbook/recipes/rl_loop.py](tinker_cookbook/recipes/rl_loop.py) for minimal examples of using these primitives to fine-tune LLMs.

To download the weights of any model:
```python
rest_client = service_client.create_rest_client()
future = rest_client.download_checkpoint_archive_from_tinker_path(sampling_client.model_path)
with open(f&quot;model-checkpoint.tar.gz&quot;, &quot;wb&quot;) as f:
    f.write(future.result())
```

### Tinker Cookbook

Besides these primitives, we also offer **Tinker Cookbook** (a.k.a. this repo), a library of a wide range of abstractions to help you customize training environments.
[`tinker_cookbook/recipes/sl_basic.py`](tinker_cookbook/recipes/sl_basic.py) and [`tinker_cookbook/recipes/rl_basic.py`](tinker_cookbook/recipes/rl_basic.py) contain minimal examples to configure supervised learning and reinforcement learning.

We also include a wide range of more sophisticated examples in the [`tinker_cookbook/recipes/`](tinker_cookbook/recipes/) folder:
1. **[Chat supervised learning](tinker_cookbook/recipes/chat_sl/)**: supervised fine-tuning on conversational datasets like Tulu3.
2. **[Math reasoning](tinker_cookbook/recipes/math_rl/)**: improve LLM reasoning capability by rewarding it for answering math questions correctly.
3. **[Preference learning](tinker_cookbook/recipes/preference/)**: showcase a three-stage RLHF pipeline: 1) supervised fine-tuning, 2) learning a reward model, 3) RL against the reward model.
4. **[Tool use](tinker_cookbook/recipes/tool_use/)**: train LLMs to better use retrieval tools to answer questions more accurately.
5. **[Prompt distillation](tinker_cookbook/recipes/prompt_distillation/)**: internalize long and complex instructions into LLMs.
6. **[Multi-Agent](tinker_cookbook/recipes/multiplayer_rl/)**: optimize LLMs to play against another LLM or themselves.

These examples are located in each subfolder, and their `README.md` files will walk you through the key implementation details, the commands to run them, and the expected performance.

### Import our utilities

Tinker cookbook includes several utilities. Here&#039;s a quick overview:
- [`renderers`](tinker_cookbook/renderers.py) converts tokens from/to structured chat message objects
- [`hyperparam_utils`](tinker_cookbook/hyperparam_utils.py) helps calculate hyperparameters suitable for LoRAs
- [`evaluation`](tinker_cookbook/eval/evaluators.py) provides abstractions for evaluating Tinker models and [`inspect_evaluation`](tinker_cookbook/eval/inspect_evaluators.py) shows how to integrate with InspectAI to make evaluating on standard benchmarks easy.

## Contributing

This project is built in the spirit of open science and collaborative development. We believe that the best tools emerge through community involvement and shared learning.

We welcome PR contributions after our private beta is over. If you have any feedback, please email us at tinker@thinkingmachines.ai.

## Citation
If you use Tinker for your research, please cite it as:
```
Thinking Machines Lab, 2025. Tinker. https://thinkingmachines.ai/tinker/.
```

Or use this BibTeX citation:
```
@misc{tml2025tinker,
  author = {Thinking Machines Lab},
  title = {Tinker},
  year = {2025},
  url = {https://thinkingmachines.ai/tinker/},
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Zie619/n8n-workflows]]></title>
            <link>https://github.com/Zie619/n8n-workflows</link>
            <guid>https://github.com/Zie619/n8n-workflows</guid>
            <pubDate>Wed, 12 Nov 2025 00:04:30 GMT</pubDate>
            <description><![CDATA[all of the workflows of n8n i could find (also from the site itself)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Zie619/n8n-workflows">Zie619/n8n-workflows</a></h1>
            <p>all of the workflows of n8n i could find (also from the site itself)</p>
            <p>Language: Python</p>
            <p>Stars: 39,742</p>
            <p>Forks: 3,894</p>
            <p>Stars today: 473 stars today</p>
            <h2>README</h2><pre># üöÄ n8n Workflow Collection

&lt;div align=&quot;center&quot;&gt;

![n8n Workflows](https://img.shields.io/badge/n8n-Workflows-orange?style=for-the-badge&amp;logo=n8n)
![Workflows](https://img.shields.io/badge/Workflows-4343+-blue?style=for-the-badge)
![Integrations](https://img.shields.io/badge/Integrations-365+-green?style=for-the-badge)
![License](https://img.shields.io/badge/License-MIT-purple?style=for-the-badge)
[![Buy Me a Coffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-FFDD00?style=for-the-badge&amp;logo=buy-me-a-coffee&amp;logoColor=black)](https://www.buymeacoffee.com/zie619)

### üåü The Ultimate Collection of n8n Automation Workflows

**[üîç Browse Online](https://zie619.github.io/n8n-workflows)** ‚Ä¢ **[üìö Documentation](#documentation)** ‚Ä¢ **[ü§ù Contributing](#contributing)** ‚Ä¢ **[üìÑ License](#license)**

&lt;/div&gt;

---

## ‚ú® What&#039;s New

### üéâ Latest Updates (November 2025)
- **üîí Enhanced Security**: Full security audit completed, all CVEs resolved
- **üê≥ Docker Support**: Multi-platform builds for linux/amd64 and linux/arm64
- **üìä GitHub Pages**: Live searchable interface at [zie619.github.io/n8n-workflows](https://zie619.github.io/n8n-workflows)
- **‚ö° Performance**: 100x faster search with SQLite FTS5 integration
- **üé® Modern UI**: Completely redesigned interface with dark/light mode

---

## üåê Quick Access

### üî• Use Online (No Installation)
Visit **[zie619.github.io/n8n-workflows](https://zie619.github.io/n8n-workflows)** for instant access to:
- üîç **Smart Search** - Find workflows instantly
- üìÇ **15+ Categories** - Browse by use case
- üì± **Mobile Ready** - Works on any device
- ‚¨áÔ∏è **Direct Downloads** - Get workflow JSONs instantly

---

## üöÄ Features

&lt;table&gt;
&lt;tr&gt;
&lt;td width=&quot;50%&quot;&gt;

### üìä By The Numbers
- **4,343** Production-Ready Workflows
- **365** Unique Integrations
- **29,445** Total Nodes
- **15** Organized Categories
- **100%** Import Success Rate

&lt;/td&gt;
&lt;td width=&quot;50%&quot;&gt;

### ‚ö° Performance
- **&lt; 100ms** Search Response
- **&lt; 50MB** Memory Usage
- **700x** Smaller Than v1
- **10x** Faster Load Times
- **40x** Less RAM Usage

&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

---

## üíª Local Installation

### Prerequisites
- Python 3.9+
- pip (Python package manager)
- 100MB free disk space

### Quick Start
```bash
# Clone the repository
git clone https://github.com/Zie619/n8n-workflows.git
cd n8n-workflows

# Install dependencies
pip install -r requirements.txt

# Start the server
python run.py

# Open in browser
# http://localhost:8000
```

### üê≥ Docker Installation
```bash
# Using Docker Hub
docker run -p 8000:8000 zie619/n8n-workflows:latest

# Or build locally
docker build -t n8n-workflows .
docker run -p 8000:8000 n8n-workflows
```

---

## üìö Documentation

### API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | Web interface |
| `/api/search` | GET | Search workflows |
| `/api/stats` | GET | Repository statistics |
| `/api/workflow/{id}` | GET | Get workflow JSON |
| `/api/categories` | GET | List all categories |
| `/api/export` | GET | Export workflows |

### Search Features
- **Full-text search** across names, descriptions, and nodes
- **Category filtering** (Marketing, Sales, DevOps, etc.)
- **Complexity filtering** (Low, Medium, High)
- **Trigger type filtering** (Webhook, Schedule, Manual, etc.)
- **Service filtering** (365+ integrations)

---

## üèóÔ∏è Architecture

```mermaid
graph LR
    A[User] --&gt; B[Web Interface]
    B --&gt; C[FastAPI Server]
    C --&gt; D[SQLite FTS5]
    D --&gt; E[Workflow Database]
    C --&gt; F[Static Files]
    F --&gt; G[Workflow JSONs]
```

### Tech Stack
- **Backend**: Python, FastAPI, SQLite with FTS5
- **Frontend**: Vanilla JS, Tailwind CSS
- **Database**: SQLite with Full-Text Search
- **Deployment**: Docker, GitHub Actions, GitHub Pages
- **Security**: Trivy scanning, CORS protection, Input validation

---

## üìÇ Repository Structure

```
n8n-workflows/
‚îú‚îÄ‚îÄ workflows/           # 4,343 workflow JSON files
‚îÇ   ‚îî‚îÄ‚îÄ [category]/     # Organized by integration
‚îú‚îÄ‚îÄ docs/               # GitHub Pages site
‚îú‚îÄ‚îÄ src/                # Python source code
‚îú‚îÄ‚îÄ scripts/            # Utility scripts
‚îú‚îÄ‚îÄ api_server.py       # FastAPI application
‚îú‚îÄ‚îÄ run.py              # Server launcher
‚îú‚îÄ‚îÄ workflow_db.py      # Database manager
‚îî‚îÄ‚îÄ requirements.txt    # Python dependencies
```

---

## ü§ù Contributing

We love contributions! Here&#039;s how you can help:

### Ways to Contribute
- üêõ **Report bugs** via [Issues](https://github.com/Zie619/n8n-workflows/issues)
- üí° **Suggest features** in [Discussions](https://github.com/Zie619/n8n-workflows/discussions)
- üìù **Improve documentation**
- üîß **Submit workflow fixes**
- ‚≠ê **Star the repository**

### Development Setup
```bash
# Fork and clone
git clone https://github.com/YOUR_USERNAME/n8n-workflows.git

# Create branch
git checkout -b feature/amazing-feature

# Make changes and test
python run.py --debug

# Commit and push
git add .
git commit -m &quot;feat: add amazing feature&quot;
git push origin feature/amazing-feature

# Open PR
```

---

## üîí Security

### Security Features
- ‚úÖ **Path traversal protection**
- ‚úÖ **Input validation &amp; sanitization**
- ‚úÖ **CORS protection**
- ‚úÖ **Rate limiting**
- ‚úÖ **Docker security hardening**
- ‚úÖ **Non-root container user**
- ‚úÖ **Regular security scanning**

### Reporting Security Issues
Please report security vulnerabilities to the maintainers via [Security Advisory](https://github.com/Zie619/n8n-workflows/security/advisories/new).

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

```
MIT License

Copyright (c) 2025 Zie619

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the &quot;Software&quot;), to deal
in the Software without restriction...
```

---

## üíñ Support

If you find this project helpful, please consider:

&lt;div align=&quot;center&quot;&gt;

[![Buy Me a Coffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-FFDD00?style=for-the-badge&amp;logo=buy-me-a-coffee&amp;logoColor=black)](https://www.buymeacoffee.com/zie619)
[![Star on GitHub](https://img.shields.io/badge/Star%20on%20GitHub-181717?style=for-the-badge&amp;logo=github)](https://github.com/Zie619/n8n-workflows)
[![Follow](https://img.shields.io/badge/Follow-1DA1F2?style=for-the-badge&amp;logo=twitter&amp;logoColor=white)](https://twitter.com/zie619)

&lt;/div&gt;

---

## üìä Stats &amp; Badges

&lt;div align=&quot;center&quot;&gt;

![GitHub stars](https://img.shields.io/github/stars/Zie619/n8n-workflows?style=social)
![GitHub forks](https://img.shields.io/github/forks/Zie619/n8n-workflows?style=social)
![GitHub watchers](https://img.shields.io/github/watchers/Zie619/n8n-workflows?style=social)
![GitHub issues](https://img.shields.io/github/issues/Zie619/n8n-workflows)
![GitHub pull requests](https://img.shields.io/github/issues-pr/Zie619/n8n-workflows)
![GitHub last commit](https://img.shields.io/github/last-commit/Zie619/n8n-workflows)
![GitHub repo size](https://img.shields.io/github/repo-size/Zie619/n8n-workflows)

&lt;/div&gt;

---

## üôè Acknowledgments

- **n8n** - For creating an amazing automation platform
- **Contributors** - Everyone who has helped improve this collection
- **Community** - For feedback and support
- **You** - For using and supporting this project!

---

&lt;div align=&quot;center&quot;&gt;

### ‚≠ê Star us on GitHub ‚Äî it motivates us a lot!

Made with ‚ù§Ô∏è by [Zie619](https://github.com/Zie619) and [contributors](https://github.com/Zie619/n8n-workflows/graphs/contributors)

&lt;/div&gt;</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[GoogleCloudPlatform/agent-starter-pack]]></title>
            <link>https://github.com/GoogleCloudPlatform/agent-starter-pack</link>
            <guid>https://github.com/GoogleCloudPlatform/agent-starter-pack</guid>
            <pubDate>Wed, 12 Nov 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[A collection of production-ready Generative AI Agent templates built for Google Cloud. It accelerates development by providing a holistic, production-ready solution, addressing common challenges (Deployment & Operations, Evaluation, Customization, Observability) in building and deploying GenAI agents.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/GoogleCloudPlatform/agent-starter-pack">GoogleCloudPlatform/agent-starter-pack</a></h1>
            <p>A collection of production-ready Generative AI Agent templates built for Google Cloud. It accelerates development by providing a holistic, production-ready solution, addressing common challenges (Deployment & Operations, Evaluation, Customization, Observability) in building and deploying GenAI agents.</p>
            <p>Language: Python</p>
            <p>Stars: 2,846</p>
            <p>Forks: 852</p>
            <p>Stars today: 24 stars today</p>
            <h2>README</h2><pre># üöÄ Agent Starter Pack

![Version](https://img.shields.io/pypi/v/agent-starter-pack?color=blue) [![1-Minute Video Overview](https://img.shields.io/badge/1--Minute%20Overview-gray)](https://youtu.be/jHt-ZVD660g) [![Docs](https://img.shields.io/badge/Documentation-gray)](https://googlecloudplatform.github.io/agent-starter-pack/) &lt;a href=&quot;https://studio.firebase.google.com/new?template=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fagent-starter-pack%2Ftree%2Fmain%2Fagent_starter_pack%2Fresources%2Fidx&quot;&gt;
  &lt;picture&gt;
    &lt;source
      media=&quot;(prefers-color-scheme: dark)&quot;
      srcset=&quot;https://cdn.firebasestudio.dev/btn/try_light_20.svg&quot;&gt;
    &lt;source
      media=&quot;(prefers-color-scheme: light)&quot;
      srcset=&quot;https://cdn.firebasestudio.dev/btn/try_dark_20.svg&quot;&gt;
    &lt;img
      height=&quot;20&quot;
      alt=&quot;Try in Firebase Studio&quot;
      src=&quot;https://cdn.firebasestudio.dev/btn/try_blue_20.svg&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt; [![Launch in Cloud Shell](https://img.shields.io/badge/Launch-in_Cloud_Shell-white)](https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Feliasecchig%2Fasp-open-in-cloud-shell&amp;cloudshell_print=open-in-cs) ![Stars](https://img.shields.io/github/stars/GoogleCloudPlatform/agent-starter-pack?color=yellow)

A Python package that provides **production-ready templates** for GenAI agents on Google Cloud.

Focus on your agent logic‚Äîthe starter pack provides everything else: infrastructure, CI/CD, observability, and security.

| ‚ö°Ô∏è Launch | üß™ Experiment  | ‚úÖ Deploy | üõ†Ô∏è Customize |
|---|---|---|---|
| [Pre-built agent templates](./agent_starter_pack/agents/) (ReAct, RAG, multi-agent, Live API). | [Vertex AI evaluation](https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview) and an interactive playground. | Production-ready infra with [monitoring, observability](https://googlecloudplatform.github.io/agent-starter-pack/guide/observability), and [CI/CD](https://googlecloudplatform.github.io/agent-starter-pack/guide/deployment) on [Cloud Run](https://cloud.google.com/run) or [Agent Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview). | Extend and customize templates according to your needs. üÜï Now integrating with [Gemini CLI](https://github.com/google-gemini/gemini-cli) |

---

## ‚ö° Get Started in 1 Minute

**From zero to production-ready agent in 60 seconds using [`uv`](https://docs.astral.sh/uv/getting-started/installation/):**

```bash
uvx agent-starter-pack create my-awesome-agent
```

&lt;details&gt;
&lt;summary&gt; ‚ú® Alternative: Using pip&lt;/summary&gt;

If you don&#039;t have [`uv`](https://github.com/astral-sh/uv) installed, you can use pip:
```bash
# Create and activate a Python virtual environment
python -m venv .venv &amp;&amp; source .venv/bin/activate

# Install the agent starter pack
pip install --upgrade agent-starter-pack

# Create a new agent project
agent-starter-pack create my-awesome-agent
```
&lt;/details&gt;

**That&#039;s it!** You now have a fully functional agent project‚Äîcomplete with backend, frontend, and deployment infrastructure‚Äîready for you to explore and customize.

### üîß Enhance Existing Agents

Already have an agent? Add production-ready deployment and infrastructure by running this command in your project&#039;s root folder:

```bash
uvx agent-starter-pack enhance
```

See [Installation Guide](https://googlecloudplatform.github.io/agent-starter-pack/guide/installation) for more options, or try with zero setup in [Firebase Studio](https://studio.firebase.google.com/new?template=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fagent-starter-pack%2Ftree%2Fmain%2Fsrc%2Fresources%2Fidx) or [Cloud Shell](https://shell.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Feliasecchig%2Fasp-open-in-cloud-shell&amp;cloudshell_print=open-in-cs).

---

## ü§ñ Agents

| Agent Name                  | Description                                                                                                                       |
|-----------------------------|-----------------------------------------------------------------------------------------------------------------------------------|
| `adk_base`      | A base ReAct agent implemented using Google&#039;s [Agent Development Kit](https://github.com/google/adk-python) |
| `adk_a2a_base`  | An ADK agent with [Agent2Agent (A2A) Protocol](https://a2a-protocol.org/) support for distributed agent communication and interoperability |
| `agentic_rag` | A RAG agent for document retrieval and Q&amp;A. Supporting [Vertex AI Search](https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction) and [Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview).       |
| `langgraph_base_react`      | An agent implementing a base ReAct agent using LangGraph |
| `crewai_coding_crew`       | A multi-agent system implemented with CrewAI created to support coding activities       |
| `adk_live`       | A real-time multimodal RAG agent powered by Gemini, supporting audio/video/text chat     |

**More agents are on the way!** We are continuously expanding our [agent library](https://googlecloudplatform.github.io/agent-starter-pack/agents/overview). Have a specific agent type in mind? [Raise an issue as a feature request!](https://github.com/GoogleCloudPlatform/agent-starter-pack/issues/new?labels=enhancement)

**üîç ADK Samples**

Looking to explore more ADK examples? Check out the [ADK Samples Repository](https://github.com/google/adk-samples) for additional examples and use cases demonstrating ADK&#039;s capabilities.

---

## üåü Community Showcase

Explore amazing projects built with the Agent Starter Pack! 

**[View Community Showcase ‚Üí](https://googlecloudplatform.github.io/agent-starter-pack/guide/community-showcase)**

## Key Features

The `agent-starter-pack` offers key features to accelerate and simplify the development of your agent:
- **üîÑ [CI/CD Automation](https://googlecloudplatform.github.io/agent-starter-pack/cli/setup_cicd)** - A single command to set up a complete CI/CD pipeline for all environments, supporting both **Google Cloud Build** and **GitHub Actions**.
- **üì• [Data Pipeline for RAG with Terraform/CI-CD](https://googlecloudplatform.github.io/agent-starter-pack/guide/data-ingestion)** - Seamlessly integrate a data pipeline to process embeddings for RAG into your agent system. Supporting [Vertex AI Search](https://cloud.google.com/generative-ai-app-builder/docs/enterprise-search-introduction) and [Vector Search](https://cloud.google.com/vertex-ai/docs/vector-search/overview).
- **[Remote Templates](docs/guide/remote-templating.md)**: Create and share your own agent starter packs templates from any Git repository.
- **ü§ñ Gemini CLI Integration** - Use the [Gemini CLI](https://github.com/google-gemini/gemini-cli) and the included `GEMINI.md` context file to ask questions about your template, agent architecture, and the path to production. Get instant guidance and code examples directly in your terminal.

## High-Level Architecture

This starter pack covers all aspects of Agent development, from prototyping and evaluation to deployment and monitoring.

![High Level Architecture](docs/images/ags_high_level_architecture.png &quot;Architecture&quot;)

---

## üîß Requirements

- Python 3.10+
- [Google Cloud SDK](https://cloud.google.com/sdk/docs/install)
- [Terraform](https://developer.hashicorp.com/terraform/downloads) (for deployment)
- [Make](https://www.gnu.org/software/make/) (for development tasks)


## üìö Documentation

Visit our [documentation site](https://googlecloudplatform.github.io/agent-starter-pack/) for comprehensive guides and references!

- [Getting Started Guide](https://googlecloudplatform.github.io/agent-starter-pack/guide/getting-started) - First steps with agent-starter-pack
- [Installation Guide](https://googlecloudplatform.github.io/agent-starter-pack/guide/installation) - Setting up your environment
- [Deployment Guide](https://googlecloudplatform.github.io/agent-starter-pack/guide/deployment) - Taking your agent to production
- [Agent Templates Overview](https://googlecloudplatform.github.io/agent-starter-pack/agents/overview) - Explore available agent patterns
- [CLI Reference](https://googlecloudplatform.github.io/agent-starter-pack/cli/) - Command-line tool documentation


### Video Walkthrough:

- **[Exploring the Agent Starter Pack](https://www.youtube.com/watch?v=9zqwym-N3lg)**: A comprehensive tutorial demonstrating how to rapidly deploy AI Agents using the Agent Starter Pack, covering architecture, templates, and step-by-step deployment.

- **[Deploy Your First ADK Agent in Under 5 Minutes](https://www.youtube.com/watch?v=_PU0fNGIvUw)** (January 2025): Learn how to create and deploy your first AI agent in under 5 minutes. This tutorial from PracticalGCP explains what&#039;s happening behind the scenes, covering project structure, ADK Playground, testing notebooks, and detailed explanations of all the make commands.

- **[6-minute introduction](https://www.youtube.com/live/eZ-8UQ_t4YM?feature=shared&amp;t=2791)** (April 2024): Explaining the Agent Starter Pack and demonstrating its key features. Part of the Kaggle GenAI intensive course.

- **[120-minute livestream demo](https://www.youtube.com/watch?v=yIRIT_EtALs&amp;t=235s)** (March 6, 2025): Watch us build 3 Agents in under 30 minutes using the `agent-starter-pack`!


Looking for more examples and resources for Generative AI on Google Cloud? Check out the [GoogleCloudPlatform/generative-ai](https://github.com/GoogleCloudPlatform/generative-ai) repository for notebooks, code samples, and more!

## Contributing

Contributions are welcome! See the [Contributing Guide](CONTRIBUTING.md).

## Feedback

We value your input! Your feedback helps us improve this starter pack and make it more useful for the community.

### Getting Help

If you encounter any issues or have specific suggestions, please first consider [raising an issue](https://github.com/GoogleCloudPlatform/generative-ai/issues) on our GitHub repository.

### Share Your Experience

For other types of feedback, or if you&#039;d like to share a positive experience or success story using this starter pack, we&#039;d love to hear from you! You can reach out to us at &lt;a href=&quot;mailto:agent-starter-pack@google.com&quot;&gt;agent-starter-pack@google.com&lt;/a&gt;.

Thank you for your contributions!

## Disclaimer

This repository is for demonstrative purposes only and is not an officially supported Google product.

## Terms of Service

The agent-starter-pack templating CLI and the templates in this starter pack leverage Google Cloud APIs. When you use this starter pack, you&#039;ll be deploying resources in your own Google Cloud project and will be responsible for those resources. Please review the [Google Cloud Service Terms](https://cloud.google.com/terms/service-terms) for details on the terms of service associated with these APIs.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[lzhoang2801/OpCore-Simplify]]></title>
            <link>https://github.com/lzhoang2801/OpCore-Simplify</link>
            <guid>https://github.com/lzhoang2801/OpCore-Simplify</guid>
            <pubDate>Wed, 12 Nov 2025 00:04:28 GMT</pubDate>
            <description><![CDATA[A tool designed to simplify the creation of OpenCore EFI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lzhoang2801/OpCore-Simplify">lzhoang2801/OpCore-Simplify</a></h1>
            <p>A tool designed to simplify the creation of OpenCore EFI</p>
            <p>Language: Python</p>
            <p>Stars: 2,339</p>
            <p>Forks: 222</p>
            <p>Stars today: 132 stars today</p>
            <h2>README</h2><pre>&lt;br/&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;h3 align=&quot;center&quot;&gt;OpCore Simplify&lt;/h3&gt;

  &lt;p align=&quot;center&quot;&gt;
    A specialized tool that streamlines &lt;a href=&quot;https://github.com/acidanthera/OpenCorePkg&quot;&gt;OpenCore&lt;/a&gt; EFI creation by automating the essential setup process and providing standardized configurations. Designed to reduce manual effort while ensuring accuracy in your Hackintosh journey.
    &lt;br /&gt;
    &lt;br /&gt;
    &lt;a href=&quot;#-features&quot;&gt;Features&lt;/a&gt; ‚Ä¢
    &lt;a href=&quot;#-how-to-use&quot;&gt;How To Use&lt;/a&gt; ‚Ä¢
    &lt;a href=&quot;#-contributing&quot;&gt;Contributing&lt;/a&gt; ‚Ä¢
    &lt;a href=&quot;#-license&quot;&gt;License&lt;/a&gt; ‚Ä¢
    &lt;a href=&quot;#-credits&quot;&gt;Credits&lt;/a&gt; ‚Ä¢
    &lt;a href=&quot;#-contact&quot;&gt;Contact&lt;/a&gt;
  &lt;/p&gt;
  
  &lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://trendshift.io/repositories/15410&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/15410&quot; alt=&quot;lzhoang2801%2FOpCore-Simplify | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

&gt; [!CAUTION]
&gt; **DO NOT TRUST ANY HACKINTOSH INFORMATION FROM AI/LLM SOURCES**
&gt; 
&gt; They often provide incorrect information about Hackintosh. Always rely on official sources like the [Dortania Guide](https://dortania.github.io/OpenCore-Install-Guide/) and the Hackintosh community for accurate information.

&gt; [!WARNING]
&gt; **OUTDATED SECTIONS IN DORTANIA GUIDE**
&gt; 
&gt; While the Dortania Guide is a valuable resource, some sections may be outdated. Always:
&gt; - Verify information with the Hackintosh community
&gt; - Test configurations yourself
&gt; - Prefer reading documentation directly from the GitHub repositories of bootloaders and kexts you plan to use

&gt; [!IMPORTANT]
&gt; If the installation process is successful using OpCore Simplify, please confirm it at [Successful Hackintosh Setup with OpCore Simplify](https://github.com/lzhoang2801/OpCore-Simplify/discussions/23). 
&gt; This will greatly assist others in the community.
&gt;
&gt; OpCore Simplify is the ONLY tool that builds OpenCore EFI based on your complete hardware configuration, not just predefined options. This fundamental difference sets us apart from other tools in the Hackintosh community.

&gt; [!NOTE]
&gt; While OpCore Simplify significantly reduces setup time, the Hackintosh journey still requires:
&gt; - Understanding basic concepts from the [Dortania Guide](https://dortania.github.io/OpenCore-Install-Guide/)
&gt; - Testing and troubleshooting during the installation process
&gt; - Patience and persistence in resolving any issues that arise
&gt;
&gt; Our tool doesn&#039;t eliminate these steps, but it ensures you start with a solid foundation.

## ‚ú® **Features**

1. **Comprehensive Hardware and macOS Support**  
   Fully supports modern hardware. Use `Compatibility Checker` to check supported/unsupported devices and macOS version supported.

   | **Component**  | **Supported**                                                                                       |
   |----------------|-----------------------------------------------------------------------------------------------------|
   | **CPU**        | Intel: Nehalem and Westmere (1nd Gen) ‚Üí Arrow Lake (15th Gen/Core Ultra Series 2) &lt;br&gt; AMD: Ryzen and Threadripper with [AMD Vanilla](https://github.com/AMD-OSX/AMD_Vanilla) |
   | **GPU**        | Intel iGPU: Iron Lake (1nd Gen) ‚Üí Ice Lake (10th Gen) &lt;br&gt; AMD APU: The entire Vega Raven ASIC family (Ryzen 1xxx ‚Üí 5xxx, 7x30 series) &lt;br&gt; AMD dGPU: Navi 23, Navi 22, Navi 21 generations, and older series &lt;br&gt; NVIDIA: Kepler, Pascal, Maxwell, Fermi, Tesla generations |
   | **macOS**      | macOS High Sierra ‚Üí macOS Tahoe |

2. **ACPI Patches and Kexts**  
   Automatically detects and adds ACPI patches and kexts based on hardware configuration.
   
   - Integrated with [SSDTTime](https://github.com/corpnewt/SSDTTime) for common patches (e.g., FakeEC, FixHPET, PLUG, RTCAWAC).
   - Includes custom patches:
      - Prevent kernel panics by directing the first CPU entry to an active CPU, disabling the UNC0 device, and creating a new RTC device for HEDT systems.
      - Disable unsupported or unused PCI devices, such as the GPU (using Optimus and Bumblebee methods or adding the disable-gpu property), Wi-Fi card, and NVMe storage controller.
      - Fix sleep state values in _PRW methods (GPRW, UPRW, HP special) to prevent immediate wake.
      - Add devices including ALS0, BUS0, MCHC, PMCR, PNLF, RMNE, IMEI, USBX, XOSI, along with a Surface Patch.
      - Enable ALSD and GPI0 devices.

3. **Automatic Updates**  
    Automatically checks for and updates OpenCorePkg and kexts from [Dortania Builds](https://dortania.github.io/builds/) and GitHub releases before each EFI build.
            
4. **EFI Configuration**  
   Apply additional customization based on both widely used sources and personal experience.

   - Spoof GPU IDs for certain AMD GPUs not recognized in macOS.
   - Use CpuTopologyRebuild kext for Intel CPUs with P-cores and E-cores to enhance performance.
   - Disable System Integrity Protection (SIP).
   - Spoof CPU IDs for Intel Pentium, Celeron, Core, and Xeon processors.
   - Add custom CPU names for AMD CPUs, as well as Intel Pentium, Celeron, Xeon, and Core lines from the Rocket Lake (11th) generation and newer.
   - Add a patch to allow booting macOS with unsupported SMBIOS.
   - Add NVRAM entries to bypass checking the internal Bluetooth controller.
   - Properly configure ResizeAppleGpuBars based on specific Resizable BAR information.
   - Allow flexible iGPU configuration between headless and driving a display when a supported discrete GPU is present.
   - Force Intel GPUs into VESA mode with HDMI and DVI connectors to simplify installation process.
   - Provide configuration required for using OpenCore Legacy Patcher.
   - Add built-in device property for network devices (fix &#039;Could not communicate with the server&#039; when using iServices) and storage controllers (fix internal drives shown as external).
   - Prioritize SMBIOS optimized for both power management and performance.
   - Re-enable CPU power management on legacy Intel CPUs in macOS Ventura 13 and newer.
   - Apply WiFi profiles for itlwm kext to enable auto WiFi connections at boot time.

   and more...

5. **Easy Customization**  
   In addition to the default settings applied, users can easily make further customizations if desired.

   - Custom ACPI patches, kexts, and SMBIOS adjustments (**not recommended**).
   - Force load kexts on unsupported macOS versions.

## üöÄ **How To Use**

1. **Download OpCore Simplify**:
   - Click **Code** ‚Üí **Download ZIP**, or download directly via this [link](https://github.com/lzhoang2801/OpCore-Simplify/archive/refs/heads/main.zip).  
   - Extract the downloaded ZIP file to your desired location.

   ![Download OpCore Simplify](https://i.imgur.com/mcE7OSX.png)

2. **Running OpCore Simplify**:
   - On **Windows**, run `OpCore-Simplify.bat`.
   - On **macOS**, run `OpCore-Simplify.command`.

   ![OpCore Simplify Menu](https://i.imgur.com/vTr1V9D.png)

3. **Selecting hardware report**:
   - On Windows, there will be an option for `E. Export hardware report`. It&#039;s recommended to use this for the best results with your hardware configuration and BIOS at the time of building.
   - Alternatively, use [**Hardware Sniffer**](https://github.com/lzhoang2801/Hardware-Sniffer) to create a `Report.json` and ACPI dump for configuration manully.

   ![Selecting hardware report](https://i.imgur.com/MbRmIGJ.png)

   ![Loading ACPI Tables](https://i.imgur.com/SbL6N6v.png)

   ![Compatibility Checker](https://i.imgur.com/kuDGMmp.png)

4. **Selecting macOS Version and Customizing OpenCore EFI**:
   - By default, the latest compatible macOS version will be selected for your hardware.
   - OpCore Simplify will automatically apply essential ACPI patches and kexts. 
   - You can manually review and customize these settings as needed.

   ![OpCore Simplify Menu](https://i.imgur.com/TSk9ejy.png)

5. **Building OpenCore EFI**:
   - Once you&#039;ve customized all options, select **Build OpenCore EFI** to generate your EFI.
   - The tool will automatically download the necessary bootloader and kexts, which may take a few minutes.

   ![WiFi Profile Extractor](https://i.imgur.com/71TkJkD.png)

   ![Choosing Codec Layout ID](https://i.imgur.com/Mcm20EQ.png)

   ![Building OpenCore EFI](https://i.imgur.com/deyj5de.png)

6. **USB Mapping**:
   - After building your EFI, follow the steps for mapping USB ports.

   ![Results](https://i.imgur.com/MIPigPF.png)

7. **Create USB and Install macOS**: 
   - Use [**UnPlugged**](https://github.com/corpnewt/UnPlugged) on Windows to create a USB macOS installer, or follow [this guide](https://dortania.github.io/OpenCore-Install-Guide/installer-guide/mac-install.html) for macOS.
   - For troubleshooting, refer to the [OpenCore Troubleshooting Guide](https://dortania.github.io/OpenCore-Install-Guide/troubleshooting/troubleshooting.html).

&gt; [!NOTE]
&gt; 1. After a successful installation, if OpenCore Legacy Patcher is required, simply apply root patches to activate the missing features (such as modern Broadcom Wi-Fi card and graphics acceleration).
&gt; 
&gt; 2. For AMD GPUs, after applying root patches from OpenCore Legacy Patcher, you need to remove the boot argument `-radvesa`/`-amd_no_dgpu_accel` for graphics acceleration to work.

## ü§ù **Contributing**

Contributions are **highly appreciated**! If you have ideas to improve this project, feel free to fork the repo and create a pull request, or open an issue with the &quot;enhancement&quot; tag.

Don&#039;t forget to ‚≠ê star the project! Thank you for your support! üåü

## üìú **License**

Distributed under the BSD 3-Clause License. See `LICENSE` for more information.

## üôå **Credits**

- [OpenCorePkg](https://github.com/acidanthera/OpenCorePkg) and [kexts](https://github.com/lzhoang2801/OpCore-Simplify/blob/main/Scripts/datasets/kext_data.py) ‚Äì The backbone of this project.
- [SSDTTime](https://github.com/corpnewt/SSDTTime) ‚Äì SSDT patching utilities.

## üìû **Contact**

**Hoang Hong Quan**
&gt; Facebook [@macforce2601](https://facebook.com/macforce2601) &amp;nbsp;&amp;middot;&amp;nbsp;
&gt; Telegram [@lzhoang2601](https://t.me/lzhoang2601) &amp;nbsp;&amp;middot;&amp;nbsp;
&gt; Email: lzhoang2601@gmail.com

## üåü **Star History**

[![Star History Chart](https://api.star-history.com/svg?repos=lzhoang2801/OpCore-Simplify&amp;type=Date)](https://star-history.com/#lzhoang2801/OpCore-Simplify&amp;Date)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[google/adk-python]]></title>
            <link>https://github.com/google/adk-python</link>
            <guid>https://github.com/google/adk-python</guid>
            <pubDate>Wed, 12 Nov 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[An open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google/adk-python">google/adk-python</a></h1>
            <p>An open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control.</p>
            <p>Language: Python</p>
            <p>Stars: 14,805</p>
            <p>Forks: 2,291</p>
            <p>Stars today: 238 stars today</p>
            <h2>README</h2><pre># Agent Development Kit (ADK)

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](LICENSE)
[![PyPI](https://img.shields.io/pypi/v/google-adk)](https://pypi.org/project/google-adk/)
[![Python Unit Tests](https://github.com/google/adk-python/actions/workflows/python-unit-tests.yml/badge.svg)](https://github.com/google/adk-python/actions/workflows/python-unit-tests.yml)
[![r/agentdevelopmentkit](https://img.shields.io/badge/Reddit-r%2Fagentdevelopmentkit-FF4500?style=flat&amp;logo=reddit&amp;logoColor=white)](https://www.reddit.com/r/agentdevelopmentkit/)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/google/adk-python)

&lt;html&gt;
    &lt;h2 align=&quot;center&quot;&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/google/adk-python/main/assets/agent-development-kit.png&quot; width=&quot;256&quot;/&gt;
    &lt;/h2&gt;
    &lt;h3 align=&quot;center&quot;&gt;
      An open-source, code-first Python framework for building, evaluating, and deploying sophisticated AI agents with flexibility and control.
    &lt;/h3&gt;
    &lt;h3 align=&quot;center&quot;&gt;
      Important Links:
      &lt;a href=&quot;https://google.github.io/adk-docs/&quot;&gt;Docs&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/google/adk-samples&quot;&gt;Samples&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/google/adk-java&quot;&gt;Java ADK&lt;/a&gt;,
      &lt;a href=&quot;https://github.com/google/adk-go&quot;&gt;Go ADK&lt;/a&gt; &amp;
      &lt;a href=&quot;https://github.com/google/adk-web&quot;&gt;ADK Web&lt;/a&gt;.
    &lt;/h3&gt;
&lt;/html&gt;

Agent Development Kit (ADK) is a flexible and modular framework that applies
software development principles to AI agent creation. It is designed to
simplify building, deploying, and orchestrating agent workflows, from simple
tasks to complex systems. While optimized for Gemini, ADK is model-agnostic,
deployment-agnostic, and compatible with other frameworks.

---

## üî• What&#039;s new

- **Custom Service Registration**: Add a service registry to provide a generic way to register custom service implementations to be used in FastAPI server. See [short instruction](https://github.com/google/adk-python/discussions/3175#discussioncomment-14745120). ([391628f](https://github.com/google/adk-python/commit/391628fcdc7b950c6835f64ae3ccab197163c990))

- **Rewind**: Add the ability to rewind a session to before a previous invocation ([9dce06f](https://github.com/google/adk-python/commit/9dce06f9b00259ec42241df4f6638955e783a9d1)).

- **New CodeExecutor**: Introduces a new AgentEngineSandboxCodeExecutor class that supports executing agent-generated code using the Vertex AI Code Execution Sandbox API ([ee39a89](https://github.com/google/adk-python/commit/ee39a891106316b790621795b5cc529e89815a98))

## ‚ú® Key Features

- **Rich Tool Ecosystem**: Utilize pre-built tools, custom functions,
  OpenAPI specs, MCP tools or integrate existing tools to give agents diverse
  capabilities, all for tight integration with the Google ecosystem.

- **Code-First Development**: Define agent logic, tools, and orchestration
  directly in Python for ultimate flexibility, testability, and versioning.

- **Agent Config**: Build agents without code. Check out the
  [Agent Config](https://google.github.io/adk-docs/agents/config/) feature.

- **Tool Confirmation**: A [tool confirmation flow(HITL)](https://google.github.io/adk-docs/tools/confirmation/) that can guard tool execution with explicit confirmation and custom input.

- **Modular Multi-Agent Systems**: Design scalable applications by composing
  multiple specialized agents into flexible hierarchies.

- **Deploy Anywhere**: Easily containerize and deploy agents on Cloud Run or
  scale seamlessly with Vertex AI Agent Engine.

## üöÄ Installation

### Stable Release (Recommended)

You can install the latest stable version of ADK using `pip`:

```bash
pip install google-adk
```

The release cadence is roughly bi-weekly.

This version is recommended for most users as it represents the most recent official release.

### Development Version
Bug fixes and new features are merged into the main branch on GitHub first. If you need access to changes that haven&#039;t been included in an official PyPI release yet, you can install directly from the main branch:

```bash
pip install git+https://github.com/google/adk-python.git@main
```

Note: The development version is built directly from the latest code commits. While it includes the newest fixes and features, it may also contain experimental changes or bugs not present in the stable release. Use it primarily for testing upcoming changes or accessing critical fixes before they are officially released.

## ü§ñ Agent2Agent (A2A) Protocol and ADK Integration

For remote agent-to-agent communication, ADK integrates with the
[A2A protocol](https://github.com/google-a2a/A2A/).
See this [example](https://github.com/a2aproject/a2a-samples/tree/main/samples/python/agents)
for how they can work together.

## üìö Documentation

Explore the full documentation for detailed guides on building, evaluating, and
deploying agents:

* **[Documentation](https://google.github.io/adk-docs)**

## üèÅ Feature Highlight

### Define a single agent:

```python
from google.adk.agents import Agent
from google.adk.tools import google_search

root_agent = Agent(
    name=&quot;search_assistant&quot;,
    model=&quot;gemini-2.5-flash&quot;, # Or your preferred Gemini model
    instruction=&quot;You are a helpful assistant. Answer user questions using Google Search when needed.&quot;,
    description=&quot;An assistant that can search the web.&quot;,
    tools=[google_search]
)
```

### Define a multi-agent system:

Define a multi-agent system with coordinator agent, greeter agent, and task execution agent. Then ADK engine and the model will guide the agents works together to accomplish the task.

```python
from google.adk.agents import LlmAgent, BaseAgent

# Define individual agents
greeter = LlmAgent(name=&quot;greeter&quot;, model=&quot;gemini-2.5-flash&quot;, ...)
task_executor = LlmAgent(name=&quot;task_executor&quot;, model=&quot;gemini-2.5-flash&quot;, ...)

# Create parent agent and assign children via sub_agents
coordinator = LlmAgent(
    name=&quot;Coordinator&quot;,
    model=&quot;gemini-2.5-flash&quot;,
    description=&quot;I coordinate greetings and tasks.&quot;,
    sub_agents=[ # Assign sub_agents here
        greeter,
        task_executor
    ]
)
```

### Development UI

A built-in development UI to help you test, evaluate, debug, and showcase your agent(s).

&lt;img src=&quot;https://raw.githubusercontent.com/google/adk-python/main/assets/adk-web-dev-ui-function-call.png&quot;/&gt;

###  Evaluate Agents

```bash
adk eval \
    samples_for_testing/hello_world \
    samples_for_testing/hello_world/hello_world_eval_set_001.evalset.json
```

## ü§ù Contributing

We welcome contributions from the community! Whether it&#039;s bug reports, feature requests, documentation improvements, or code contributions, please see our
- [General contribution guideline and flow](https://google.github.io/adk-docs/contributing-guide/).
- Then if you want to contribute code, please read [Code Contributing Guidelines](./CONTRIBUTING.md) to get started.

## Community Repo

We have [adk-python-community repo](https://github.com/google/adk-python-community)that is home to a growing ecosystem of community-contributed tools, third-party
service integrations, and deployment scripts that extend the core capabilities
of the ADK.

## Vibe Coding

If you are to develop agent via vibe coding the [llms.txt](./llms.txt) and the [llms-full.txt](./llms-full.txt) can be used as context to LLM. While the former one is a summarized one and the later one has the full information in case your LLM has big enough context window.

## Community Events

- [Completed] ADK&#039;s 1st community meeting on Wednesday, October 15, 2025. Remember to [join our group](https://groups.google.com/g/adk-community) to get access to the [recording](https://drive.google.com/file/d/1rpXDq5NSH8-MyMeYI6_5pZ3Lhn0X9BQf/view), and [deck](https://docs.google.com/presentation/d/1_b8LG4xaiadbUUDzyNiapSFyxanc9ZgFdw7JQ6zmZ9Q/edit?slide=id.g384e60cdaca_0_658&amp;resourcekey=0-tjFFv0VBQhpXBPCkZr0NOg#slide=id.g384e60cdaca_0_658).

## üìÑ License

This project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.

---

*Happy Agent Building!*
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[airweave-ai/airweave]]></title>
            <link>https://github.com/airweave-ai/airweave</link>
            <guid>https://github.com/airweave-ai/airweave</guid>
            <pubDate>Wed, 12 Nov 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[Context retrieval for AI agents across apps and databases]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/airweave-ai/airweave">airweave-ai/airweave</a></h1>
            <p>Context retrieval for AI agents across apps and databases</p>
            <p>Language: Python</p>
            <p>Stars: 5,112</p>
            <p>Forks: 603</p>
            <p>Stars today: 46 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;frontend/public/logo-airweave-darkbg.svg&quot;/&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;frontend/public/logo-airweave-lightbg.svg&quot;/&gt;
  &lt;img width=&quot;837&quot; alt=&quot;airweave-lettermark&quot; style=&quot;padding-bottom: 12px;&quot; src=&quot;frontend/public/logo-airweave-darkbg.svg&quot;/&gt;
&lt;/picture&gt;

# Context Retrieval for AI Agents across Apps &amp; Databases

[![Ruff](https://github.com/airweave-ai/airweave/actions/workflows/ruff.yml/badge.svg)](https://github.com/airweave-ai/airweave/actions/workflows/ruff.yml)
[![ESLint](https://github.com/airweave-ai/airweave/actions/workflows/eslint.yml/badge.svg)](https://github.com/airweave-ai/airweave/actions/workflows/eslint.yml)
[![System Tests](https://github.com/airweave-ai/airweave/actions/workflows/test-public-api.yml/badge.svg)](https://github.com/airweave-ai/airweave/actions/workflows/test-public-api.yml)
[![PyPI Downloads](https://static.pepy.tech/personalized-badge/airweave-sdk?period=total&amp;units=INTERNATIONAL_SYSTEM&amp;left_color=GRAY&amp;right_color=BRIGHTGREEN&amp;left_text=downloads)](https://pepy.tech/projects/airweave-sdk)
[![Discord](https://img.shields.io/discord/1323415085011701870?label=Discord&amp;logo=discord&amp;logoColor=white&amp;style=flat-square)](https://discord.gg/gDuebsWGkn)
&lt;br&gt;
&lt;div style=&quot;padding-top: 16px;&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/13748&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13748&quot; alt=&quot;airweave-ai%2Fairweave | Trendshift&quot; style=&quot;width: 250px; height: 55px; margin-right: 24px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://www.ycombinator.com/launches/NX7-airweave-let-agents-search-any-app&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://www.ycombinator.com/launches/NX7-airweave-let-agents-search-any-app/upvote_embed.svg&quot; alt=&quot;Launch YC: Airweave - Let Agents Search Any App&quot; style=&quot;margin-left: 12px;&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;br&gt;

‚≠ê **Help us reach more developers and grow the Airweave community. Star this repo!**

&lt;/div&gt;

## What is Airweave?

[Airweave](https://app.airweave.ai/) is a fully open-source context retrieval layer for AI agents across apps and databases. It connects to apps, productivity tools, databases, or document stores and transforms their contents into searchable knowledge bases, accessible through a standardized interface for agents.

The search interface is exposed via REST API or MCP. When using MCP, Airweave essentially builds a semantically searchable MCP server. The platform handles everything from auth and extraction to embedding and serving. You can find our documentation [here](https://docs.airweave.ai/welcome).

üì∫ Check out a quick demo of Airweave below:

&lt;video width=&quot;100%&quot; src=&quot;https://github.com/user-attachments/assets/995e4a36-3f88-4d8e-b401-6ca43db0c7bf&quot; controls&gt;&lt;/video&gt;

[**üîó Example notebooks**](https://github.com/airweave-ai/airweave/tree/main/examples)

## Table of Contents

- [Airweave](#airweave)
  - [Overview](#overview)
  - [Table of Contents](#table-of-contents)
  - [üöÄ Quick Start](#-quick-start)
  - [üîå Supported Integrations](#-supported-integrations)
  - [üíª Usage](#-usage)
    - [Frontend](#frontend)
    - [API](#api)
  - [üì¶ SDKs](#-sdks)
    - [Python](#python)
    - [TypeScript/JavaScript](#typescriptjavascript)
  - [üîë Key Features](#-key-features)
  - [üîß Technology Stack](#-tech-stack)
  - [üë• Contributing](#-contributing)
  - [üìÑ License](#-license)
  - [üîó Connect](#-connect)

## üöÄ Quick Start

### Managed Service: [Airweave Cloud](https://app.airweave.ai/)

### Self-hosted:

Make sure docker and docker-compose are installed, then...

```bash
# 1. Clone the repository
git clone https://github.com/airweave-ai/airweave.git
cd airweave

# 2. Build and run
chmod +x start.sh
./start.sh
```

That&#039;s it! Access the dashboard at http://localhost:8080

## üîå Supported Integrations

&lt;!-- START_APP_GRID --&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/airtable.svg&quot; alt=&quot;Airtable&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/asana.svg&quot; alt=&quot;Asana&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/attio.svg&quot; alt=&quot;Attio&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/bitbucket.svg&quot; alt=&quot;Bitbucket&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/box.svg&quot; alt=&quot;Box&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/clickup.svg&quot; alt=&quot;ClickUp&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/confluence.svg&quot; alt=&quot;Confluence&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/ctti.svg&quot; alt=&quot;CTTI&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/dropbox.svg&quot; alt=&quot;Dropbox&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/excel.svg&quot; alt=&quot;Excel&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/github.svg&quot; alt=&quot;Github&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/gitlab.svg&quot; alt=&quot;Gitlab&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/gmail.svg&quot; alt=&quot;Gmail&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/google_calendar.svg&quot; alt=&quot;Google Calendar&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/google_docs.svg&quot; alt=&quot;Google Docs&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/google_drive.svg&quot; alt=&quot;Google Drive&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/google_slides.svg&quot; alt=&quot;Google Slides&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/hubspot.svg&quot; alt=&quot;Hubspot&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/jira.svg&quot; alt=&quot;Jira&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/linear.svg&quot; alt=&quot;Linear&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/monday.svg&quot; alt=&quot;Monday&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/notion.svg&quot; alt=&quot;Notion&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/onedrive.svg&quot; alt=&quot;Onedrive&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/onenote.svg&quot; alt=&quot;OneNote&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/outlook_calendar.svg&quot; alt=&quot;Outlook Calendar&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/outlook_mail.svg&quot; alt=&quot;Outlook Mail&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/postgresql.svg&quot; alt=&quot;Postgresql&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/salesforce.svg&quot; alt=&quot;Salesforce&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/sharepoint.svg&quot; alt=&quot;Sharepoint&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/slack.svg&quot; alt=&quot;Slack&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/stripe.svg&quot; alt=&quot;Stripe&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/teams.svg&quot; alt=&quot;Teams&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/todoist.svg&quot; alt=&quot;Todoist&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/trello.svg&quot; alt=&quot;Trello&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/word.svg&quot; alt=&quot;Word&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;img src=&quot;frontend/src/components/icons/apps/zendesk.svg&quot; alt=&quot;Zendesk&quot; width=&quot;50&quot; height=&quot;50&quot; style=&quot;margin: 8px;&quot; /&gt;
&lt;/p&gt;

&lt;!-- END_APP_GRID --&gt;

## üíª Usage

### Frontend
- Access the UI at `http://localhost:8080`
- Connect sources, configure syncs, and query data

### API
- Swagger docs: `http://localhost:8001/docs`
- Create connections, trigger syncs, and search data

## üì¶ SDKs

### Python

```bash
pip install airweave-sdk
```

```python
from airweave import AirweaveSDK

# Initialize client
client = AirweaveSDK(
    api_key=&quot;YOUR_API_KEY&quot;,
    base_url=&quot;http://localhost:8001&quot;
)

# Create a collection
collection = client.collections.create(name=&quot;My Collection&quot;)

# Add a source connection
source = client.source_connections.create(
    name=&quot;My Stripe Connection&quot;,
    short_name=&quot;stripe&quot;,
    readable_collection_id=collection.readable_id,
    authentication={
        &quot;credentials&quot;: {&quot;api_key&quot;: &quot;your_stripe_api_key&quot;}
    }
)

# Semantic search (default)
results = client.collections.search(
    readable_id=collection.readable_id,
    query=&quot;Find recent failed payments&quot;
)

# Hybrid search (semantic + keyword)
results = client.collections.search(
    readable_id=collection.readable_id,
    query=&quot;customer invoices Q4 2024&quot;,
    search_type=&quot;hybrid&quot;
)

# With query expansion and reranking
results = client.collections.search(
    readable_id=collection.readable_id,
    query=&quot;technical documentation&quot;,
    enable_query_expansion=True,
    enable_reranking=True,
    top_k=20
)

# Search with recency bias (prioritize recent results)
results = client.collections.search(
    readable_id=collection.readable_id,
    query=&quot;critical bugs&quot;,
    recency_bias=0.8,  # 0.0 to 1.0, higher = more recent
    limit=10
)

# Get AI-generated answer instead of raw results
answer = client.collections.search(
    readable_id=collection.readable_id,
    query=&quot;What are our customer refund policies?&quot;,
    response_type=&quot;completion&quot;,
    enable_reranking=True
)
```

### TypeScript/JavaScript
```bash
npm install @airweave/sdk
# or
yarn add @airweave/sdk
```

```typescript
import { AirweaveSDKClient, AirweaveSDKEnvironment } from &quot;@airweave/sdk&quot;;

// Initialize client
const client = new AirweaveSDKClient({
    apiKey: &quot;YOUR_API_KEY&quot;,
    environment: AirweaveSDKEnvironment.Local
});

// Create a collection
const collection = await client.collections.create({
    name: &quot;My Collection&quot;
});

// Add a source connection
const source = await client.sourceConnections.create({
    name: &quot;My Stripe Connection&quot;,
    shortName: &quot;stripe&quot;,
    readableCollectionId: collection.readableId,
    authentication: {
        credentials: { apiKey: &quot;your_stripe_api_key&quot; }
    }
});

// Semantic search (default)
const results = await client.collections.search(
    collection.readableId,
    { query: &quot;Find recent failed payments&quot; }
);

// Hybrid search (semantic + keyword)
const hybridResults = await client.collections.search(
    collection.readableId,
    {
        query: &quot;customer invoices Q4 2024&quot;,
        searchType: &quot;hybrid&quot;
    }
);

// With query expansion and reranking
const advancedResults = await client.collections.search(
    collection.readableId,
    {
        query: &quot;technical documentation&quot;,
        enableQueryExpansion: true,
        enableReranking: true,
        topK: 20
    }
);

// Search with recency bias (prioritize recent results)
const recentResults = await client.collections.search(
    collection.readableId,
    {
        query: &quot;critical bugs&quot;,
        recencyBias: 0.8,  // 0.0 to 1.0, higher = more recent
        limit: 10
    }
);

// Get AI-generated answer instead of raw results
const answer = await client.collections.search(
    collection.readableId,
    {
        query: &quot;What are our customer refund policies?&quot;,
        responseType: &quot;completion&quot;,
        enableReranking: true
    }
);
```

## üîë Key Features

- **Data synchronization** from 30+ sources with minimal config
- **Entity extraction** and transformation pipeline
- **Multi-tenant** architecture with OAuth2
- **Incremental updates** using content hashing
- **Semantic search** for agent queries
- **Versioning** for data changes

## üîß Tech Stack

- **Frontend**: React/TypeScript with ShadCN
- **Backend**: FastAPI (Python)
- **Databases**: PostgreSQL (metadata), Qdrant (vectors)
- **Workers**: Temporal (workflow orchestration), Redis (pub/sub)
- **Deployment**: Docker Compose (dev), Kubernetes (prod)

## üë• Contributing

We welcome contributions! Please check [CONTRIBUTING.md](https://github.com/airweave-ai/airweave/blob/main/CONTRIBUTING.md) for details.

## üìÑ License

Airweave is released under the [MIT](LICENSE) license.

## üîó Connect

- **[Discord](https://discord.com/invite/484HY9Ehxt)** - Get help and discuss features
- **[GitHub Issues](https://github.com/airweave-ai/airweave/issues)** - Report bugs or request features
- **[Twitter](https://x.com/airweave_ai)** - Follow for updates
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[GibsonAI/Memori]]></title>
            <link>https://github.com/GibsonAI/Memori</link>
            <guid>https://github.com/GibsonAI/Memori</guid>
            <pubDate>Wed, 12 Nov 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[Open-Source Memory Engine for LLMs, AI Agents & Multi-Agent Systems]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/GibsonAI/Memori">GibsonAI/Memori</a></h1>
            <p>Open-Source Memory Engine for LLMs, AI Agents & Multi-Agent Systems</p>
            <p>Language: Python</p>
            <p>Stars: 2,191</p>
            <p>Forks: 202</p>
            <p>Stars today: 383 stars today</p>
            <h2>README</h2><pre>[![Memori Labs](https://s3.us-east-1.amazonaws.com/images.memorilabs.ai/banner.png)](https://memorilabs.ai/)

&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;An open-source SQL-Native memory engine for AI

&lt;/strong&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;i&gt;One line of code to give any LLM persistent, queryable memory using standard SQL databases&lt;/i&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://badge.fury.io/py/memorisdk&quot;&gt;
    &lt;img src=&quot;https://badge.fury.io/py/memori.svg&quot; alt=&quot;PyPI version&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://pepy.tech/projects/memorisdk&quot;&gt;
    &lt;img src=&quot;https://static.pepy.tech/badge/memorisdk&quot; alt=&quot;Downloads&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://opensource.org/license/apache-2-0&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/license-Apache%202.0-blue&quot; alt=&quot;License&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.python.org/downloads/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/python-3.8+-blue.svg&quot; alt=&quot;Python 3.8+&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/abD4eGym6v&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/discord/1042405378304004156?logo=discord&quot; alt=&quot;Discord&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/GibsonAI/memori/stargazers&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/‚≠ê%20Give%20a%20Star-Support%20the%20project-orange?style=for-the-badge&quot; alt=&quot;Give a Star&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

---

## What is Memori

Memori enables any LLM to remember conversations, learn from interactions, and maintain context across sessions with a single line: `memori.enable()`. Memory is stored in standard SQL databases (SQLite, PostgreSQL, MySQL) that you fully own and control.

**Why Memori?**
- **One-line integration** - Works with OpenAI, Anthropic, LiteLLM, LangChain, and any LLM framework
- **SQL-native storage** - Portable, queryable, and auditable memory in databases you control
- **80-90% cost savings** - No expensive vector databases required
- **Zero vendor lock-in** - Export your memory as SQLite and move anywhere
- **Intelligent memory** - Automatic entity extraction, relationship mapping, and context prioritization

[Documentation](https://www.gibsonai.com/docs/memori) | [Examples](#examples) | [Discord](https://discord.gg/abD4eGym6v)

---

## Quick Start

```bash
pip install memorisdk
```

```python
from memori import Memori
from openai import OpenAI

# Initialize
memori = Memori(conscious_ingest=True)
memori.enable()

client = OpenAI()

# First conversation
response = client.chat.completions.create(
    model=&quot;gpt-4o-mini&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;I&#039;m building a FastAPI project&quot;}]
)

# Later conversation - Memori automatically provides context
response = client.chat.completions.create(
    model=&quot;gpt-4o-mini&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Help me add authentication&quot;}]
)
# LLM automatically knows about your FastAPI project
```


---

## Database Support

Memori works with any SQL database you already use:

| Database | Connection String Example |
|----------|--------------------------|
| **SQLite** | `sqlite:///my_memory.db` |
| **PostgreSQL** | `postgresql://user:pass@localhost/memori` |
| **MySQL** | `mysql://user:pass@localhost/memori` |
| **Neon** | `postgresql://user:pass@ep-*.neon.tech/memori` |
| **Supabase** | `postgresql://postgres:pass@db.*.supabase.co/postgres` |

---

## LLM Framework Support

Works with any LLM framework through LiteLLM&#039;s native callback system:

| Framework | Status | Usage |
|-----------|--------|-------|
| **OpenAI** | ‚úì Native | `from openai import OpenAI` |
| **Anthropic** | ‚úì Native | `from anthropic import Anthropic` |
| **LiteLLM** | ‚úì Native | `from litellm import completion` |
| **LangChain** | ‚úì Supported | Use with LiteLLM integration |
| **Azure OpenAI** | ‚úì Supported | Configure with `ProviderConfig.from_azure()` |
| **100+ Models** | ‚úì Supported | Any LiteLLM-compatible provider |

---

## Configuration

### Persistent Storage

```python
from memori import Memori

memori = Memori(
    database_connect=&quot;postgresql://user:pass@localhost/memori&quot;,
    conscious_ingest=True,  # Short-term working memory
    auto_ingest=True,       # Dynamic search per query
    openai_api_key=&quot;sk-...&quot;
)
memori.enable()
```

### Memory Modes

**Conscious Mode** - One-shot working memory injection
```python
memori = Memori(conscious_ingest=True)
```

**Auto Mode** - Dynamic search per query
```python
memori = Memori(auto_ingest=True)
```

**Combined Mode** - Best of both
```python
memori = Memori(conscious_ingest=True, auto_ingest=True)
```

### Using ConfigManager

```python
from memori import Memori, ConfigManager

config = ConfigManager()
config.auto_load()  # Loads from environment or config files

memori = Memori()
memori.enable()
```

Set environment variables:
```bash
export MEMORI_DATABASE__CONNECTION_STRING=&quot;postgresql://...&quot;
export MEMORI_AGENTS__OPENAI_API_KEY=&quot;sk-...&quot;
export MEMORI_MEMORY__NAMESPACE=&quot;production&quot;
```

---

## Architecture Overview

Memori works by **intercepting** LLM calls - injecting context before the call and recording after:

```mermaid
graph LR
    A[Your App] --&gt;|1. client.chat.completions.create| B[Memori Interceptor]
    B --&gt;|2. Get Context| C[(SQL Database)]
    C --&gt;|3. Relevant Memories| B
    B --&gt;|4. Inject Context + Call| D[OpenAI/Anthropic/etc]
    D --&gt;|5. Response| B
    B --&gt;|6. Extract &amp; Store| C
    B --&gt;|7. Return Response| A

    E[Conscious Agent] -.-&gt;|Background: Analyze &amp; Promote| C
```

### How It Works

**Pre-Call (Context Injection)**

1. Your app calls `client.chat.completions.create(messages=[...])`
2. Memori intercepts the call transparently
3. **Retrieval Agent** (auto mode) or **Conscious Agent** (conscious mode) retrieves relevant memories
4. Context injected into messages before sending to the LLM provider

**Post-Call (Recording)**

5. LLM provider returns response
6. **Memory Agent** extracts entities, categorizes (facts, preferences, skills, rules, context)
7. Conversation stored in SQL database with full-text search indexes
8. Original response returned to your app

**Background (every 6 hours)**

- **Conscious Agent** analyzes patterns and promotes essential memories from long-term to short-term storage

For detailed architecture documentation, see [docs/architecture.md](https://www.gibsonai.com/docs/memori/architecture).

---

## Examples

**Basic Examples**
- [Basic Usage](./examples/basic_usage.py) - Simple memory setup
- [Personal Assistant](./examples/personal_assistant.py) - AI assistant with memory
- [Memory Retrieval](./memory_retrival_example.py) - Function calling
- [Advanced Config](./examples/advanced_config.py) - Production setup

**Multi-User**
- [Simple Multi-User](./examples/multiple-users/simple_multiuser.py) - User memory isolation
- [FastAPI Multi-User App](./examples/multiple-users/fastapi_multiuser_app.py) - REST API with Swagger

---

## Framework Integrations

| Framework | Description |
|-----------|-------------|
| [AgentOps](./examples/integrations/agentops_example.py) | Memory operation tracking with observability |
| [Agno](./examples/integrations/agno_example.py) | Agent framework with persistent conversations |
| [AWS Strands](./examples/integrations/aws_strands_example.py) | Strands SDK with persistent memory |
| [Azure AI Foundry](./examples/integrations/azure_ai_foundry_example.py) | Enterprise AI agents with Azure |
| [AutoGen](./examples/integrations/autogen_example.py) | Multi-agent group chat memory |
| [CamelAI](./examples/integrations/camelai_example.py) | Multi-agent communication framework |
| [CrewAI](./examples/integrations/crewai_example.py) | Multi-agent shared memory |
| [Digital Ocean AI](./examples/integrations/digital_ocean_example.py) | Customer support with history |
| [LangChain](./examples/integrations/langchain_example.py) | Enterprise agent framework |
| [OpenAI Agent](./examples/integrations/openai_agent_example.py) | Function calling with preferences |
| [Swarms](./examples/integrations/swarms_example.py) | Multi-agent persistent memory |

---

## Interactive Demos

| Demo | Description | Live |
|------|-------------|------|
| [Personal Diary](./demos/personal_diary_assistant/) | Mood tracking and pattern analysis | [Try it](https://personal-diary-assistant.streamlit.app/) |
| [Researcher](./demos/researcher_agent/) | Research assistant with web search | [Try it](https://researcher-agent-memori.streamlit.app/) |

---

## Contributing

We welcome contributions from the community! Please see our [Contributing Guidelines](./CONTRIBUTING.md) for details on:

- Setting up your development environment
- Code style and standards
- Submitting pull requests
- Reporting issues

---

## Support

- **Documentation**: [https://www.gibsonai.com/docs/memori](https://www.gibsonai.com/docs/memori)
- **Discord**: [https://discord.gg/abD4eGym6v](https://discord.gg/abD4eGym6v)
- **Issues**: [GitHub Issues](https://github.com/GibsonAI/memori/issues)

---

## License

Apache 2.0 - see [LICENSE](./LICENSE)

---

**Star us on GitHub** to support the project

[![Star History](https://api.star-history.com/svg?repos=GibsonAI/memori&amp;type=date)](https://star-history.com/#GibsonAI/memori)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/markitdown]]></title>
            <link>https://github.com/microsoft/markitdown</link>
            <guid>https://github.com/microsoft/markitdown</guid>
            <pubDate>Wed, 12 Nov 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[Python tool for converting files and office documents to Markdown.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/markitdown">microsoft/markitdown</a></h1>
            <p>Python tool for converting files and office documents to Markdown.</p>
            <p>Language: Python</p>
            <p>Stars: 82,884</p>
            <p>Forks: 4,694</p>
            <p>Stars today: 78 stars today</p>
            <h2>README</h2><pre># MarkItDown

[![PyPI](https://img.shields.io/pypi/v/markitdown.svg)](https://pypi.org/project/markitdown/)
![PyPI - Downloads](https://img.shields.io/pypi/dd/markitdown)
[![Built by AutoGen Team](https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue)](https://github.com/microsoft/autogen)

&gt; [!TIP]
&gt; MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See [markitdown-mcp](https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp) for more information.

&gt; [!IMPORTANT]
&gt; Breaking changes between 0.0.1 to 0.1.0:
&gt; * Dependencies are now organized into optional feature-groups (further details below). Use `pip install &#039;markitdown[all]&#039;` to have backward-compatible behavior. 
&gt; * convert\_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.
&gt; * The DocumentConverter class interface has changed to read from file-like streams rather than file paths. *No temporary files are created anymore*. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.

MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to [textract](https://github.com/deanmalmgren/textract), but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.

MarkItDown currently supports the conversion from:

- PDF
- PowerPoint
- Word
- Excel
- Images (EXIF metadata and OCR)
- Audio (EXIF metadata and speech transcription)
- HTML
- Text-based formats (CSV, JSON, XML)
- ZIP files (iterates over contents)
- Youtube URLs
- EPubs
- ... and more!

## Why Markdown?

Markdown is extremely close to plain text, with minimal markup or formatting, but still
provides a way to represent important document structure. Mainstream LLMs, such as
OpenAI&#039;s GPT-4o, natively &quot;_speak_&quot; Markdown, and often incorporate Markdown into their
responses unprompted. This suggests that they have been trained on vast amounts of
Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions
are also highly token-efficient.

## Prerequisites
MarkItDown requires Python 3.10 or higher. It is recommended to use a virtual environment to avoid dependency conflicts.

With the standard Python installation, you can create and activate a virtual environment using the following commands:

```bash
python -m venv .venv
source .venv/bin/activate
```

If using `uv`, you can create a virtual environment with:

```bash
uv venv --python=3.12 .venv
source .venv/bin/activate
# NOTE: Be sure to use &#039;uv pip install&#039; rather than just &#039;pip install&#039; to install packages in this virtual environment
```

If you are using Anaconda, you can create a virtual environment with:

```bash
conda create -n markitdown python=3.12
conda activate markitdown
```

## Installation

To install MarkItDown, use pip: `pip install &#039;markitdown[all]&#039;`. Alternatively, you can install it from the source:

```bash
git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e &#039;packages/markitdown[all]&#039;
```

## Usage

### Command-Line

```bash
markitdown path-to-file.pdf &gt; document.md
```

Or use `-o` to specify the output file:

```bash
markitdown path-to-file.pdf -o document.md
```

You can also pipe content:

```bash
cat path-to-file.pdf | markitdown
```

### Optional Dependencies
MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the `[all]` option. However, you can also install them individually for more control. For example:

```bash
pip install &#039;markitdown[pdf, docx, pptx]&#039;
```

will install only the dependencies for PDF, DOCX, and PPTX files.

At the moment, the following optional dependencies are available:

* `[all]` Installs all optional dependencies
* `[pptx]` Installs dependencies for PowerPoint files
* `[docx]` Installs dependencies for Word files
* `[xlsx]` Installs dependencies for Excel files
* `[xls]` Installs dependencies for older Excel files
* `[pdf]` Installs dependencies for PDF files
* `[outlook]` Installs dependencies for Outlook messages
* `[az-doc-intel]` Installs dependencies for Azure Document Intelligence
* `[audio-transcription]` Installs dependencies for audio transcription of wav and mp3 files
* `[youtube-transcription]` Installs dependencies for fetching YouTube video transcription

### Plugins

MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:

```bash
markitdown --list-plugins
```

To enable plugins use:

```bash
markitdown --use-plugins path-to-file.pdf
```

To find available plugins, search GitHub for the hashtag `#markitdown-plugin`. To develop a plugin, see `packages/markitdown-sample-plugin`.

### Azure Document Intelligence

To use Microsoft Document Intelligence for conversion:

```bash
markitdown path-to-file.pdf -o document.md -d -e &quot;&lt;document_intelligence_endpoint&gt;&quot;
```

More information about how to set up an Azure Document Intelligence Resource can be found [here](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0)

### Python API

Basic usage in Python:

```python
from markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert(&quot;test.xlsx&quot;)
print(result.text_content)
```

Document Intelligence conversion in Python:

```python
from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint=&quot;&lt;document_intelligence_endpoint&gt;&quot;)
result = md.convert(&quot;test.pdf&quot;)
print(result.text_content)
```

To use Large Language Models for image descriptions (currently only for pptx and image files), provide `llm_client` and `llm_model`:

```python
from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model=&quot;gpt-4o&quot;, llm_prompt=&quot;optional custom prompt&quot;)
result = md.convert(&quot;example.jpg&quot;)
print(result.text_content)
```

### Docker

```sh
docker build -t markitdown:latest .
docker run --rm -i markitdown:latest &lt; ~/your-file.pdf &gt; output.md
```

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

### How to Contribute

You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as &#039;open for contribution&#039; and &#039;open for reviewing&#039; to help facilitate community contributions. These are of course just suggestions and you are welcome to contribute in any way you like.

&lt;div align=&quot;center&quot;&gt;

|            | All                                                          | Especially Needs Help from Community                                                                                                      |
| ---------- | ------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------- |
| **Issues** | [All Issues](https://github.com/microsoft/markitdown/issues) | [Issues open for contribution](https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22) |
| **PRs**    | [All PRs](https://github.com/microsoft/markitdown/pulls)     | [PRs open for reviewing](https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22)              |

&lt;/div&gt;

### Running Tests and Checks

- Navigate to the MarkItDown package:

  ```sh
  cd packages/markitdown
  ```

- Install `hatch` in your environment and run tests:

  ```sh
  pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
  hatch shell
  hatch test
  ```

  (Alternative) Use the Devcontainer which has all the dependencies installed:

  ```sh
  # Reopen the project in Devcontainer and run:
  hatch test
  ```

- Run pre-commit checks before submitting a PR: `pre-commit run --all-files`

### Contributing 3rd-party Plugins

You can also contribute by creating and sharing 3rd party plugins. See `packages/markitdown-sample-plugin` for more details.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
[Microsoft&#039;s Trademark &amp; Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party&#039;s policies.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[alibaba/ROLL]]></title>
            <link>https://github.com/alibaba/ROLL</link>
            <guid>https://github.com/alibaba/ROLL</guid>
            <pubDate>Wed, 12 Nov 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[An Efficient and User-Friendly Scaling Library for Reinforcement Learning with Large Language Models]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/alibaba/ROLL">alibaba/ROLL</a></h1>
            <p>An Efficient and User-Friendly Scaling Library for Reinforcement Learning with Large Language Models</p>
            <p>Language: Python</p>
            <p>Stars: 2,271</p>
            <p>Forks: 144</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;assets/roll.jpeg&quot; width=&quot;40%&quot; alt=&quot;ROLL Logo&quot;&gt;

# ROLL: Reinforcement Learning Optimization for Large-Scale Learning

&lt;h4&gt;üöÄ An Efficient and User-Friendly Scaling Library for Reinforcement Learning with Large Language Models üöÄ&lt;/h4&gt;

&lt;p&gt;
  &lt;a href=&quot;https://github.com/alibaba/ROLL/blob/main/LICENSE&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/license-Apache%202.0-blue.svg&quot; alt=&quot;License&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/alibaba/ROLL/issues&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/issues/alibaba/ROLL&quot; alt=&quot;GitHub issues&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/alibaba/ROLL/stargazers&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/alibaba/ROLL?style=social&quot; alt=&quot;Repo stars&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://arxiv.org/abs/2506.06122&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=arXiv&amp;message=Paper&amp;color=red&quot;&gt;&lt;/a&gt;
  &lt;!-- ÁªÑÁªá‰∏ªÈ°µÔºöÁÇπÂáªË∑≥ËΩ¨Âà∞ https://github.com/alibaba --&gt;
  &lt;a href=&quot;./assets/roll_wechat.png&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/WeChat-green?logo=wechat&quot; alt=&quot;WeChat QR&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://deepwiki.com/alibaba/ROLL&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://deepwiki.com/badge.svg&quot; alt=&quot;Ask DeepWiki&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;/div&gt;

ROLL is an efficient and user-friendly RL library designed for Large Language Models (LLMs) utilizing Large Scale GPU resources. It significantly enhances LLM performance in key areas such as human preference alignment, complex reasoning, and multi-turn agentic interaction scenarios.

Leveraging a multi-role distributed architecture with Ray for flexible resource allocation and heterogeneous task scheduling, ROLL integrates cutting-edge technologies like Megatron-Core, SGLang and vLLM to accelerate model training and inference.



---

## üì¢ News

| üì£   Updates                                                                                                                                                                                                                                                                                                                            |
|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **[11/08/2025]** üéâ Our [ROCK: Reinforcement Open Construction Kit](https://github.com/alibaba/ROCK) released, Explore the new capabilities!.                                                                                                                                                                                           |
| **[10/23/2025]** üéâ Our Papers released, see [Asymmetric Proximal Policy Optimization: mini-critics boost LLM reasoning](https://arxiv.org/abs/2510.01656) and [Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization](https://arxiv.org/abs/2510.13554).                         |
| **[10/14/2025]** üéâ Our Paper released, see [Part II: ROLL Flash -- Accelerating RLVR and Agentic Training with Asynchrony](https://arxiv.org/abs/2510.11345).                                                                                                                                                                          |
| **[09/28/2025]** üéâ Ascend NPU support ‚Äî see [usage guide](https://alibaba.github.io/ROLL/docs/English/UserGuide/ascend/ascend_usage).                                                                                                                                                                                                  |
| **[09/25/2025]** üéâ Our Paper released, see [RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training](https://arxiv.org/abs/2509.21009)                                                                                                                                                                        |
| **[09/24/2025]** üéâ Support [Wan2_2 Reward FL pipeline](examples/wan2.2-14B-reward_fl_ds/reward_fl_config.yaml). Explore the new capabilities!                                                                                                                                                                                          |
| **[09/23/2025]** üéâ ROLL aligns with GEM environment definition, providing agentic Tool Use training capabilities, [ToolUse docs](docs_roll/docs/English/UserGuide/agentic/Tool_Use.md).                                                                                                                                                |
| **[09/16/2025]** üéâ Qwen3-Next model training is supported, refer to [configuration](examples/qwen3-next-80BA3B-rlvr_megatron/rlvr_config.yaml).                                                                                                                                                                                        |
| **[09/04/2025]** üéâ ROLL supports vLLM dynamic FP8 rollout and remove_padding for acceleration.                                                                                                                                                                                                                                         |
| **[08/28/2025]** üéâ ROLL supports SFT pipeline, refer to [configuration](examples/qwen2.5-7B-sft_megatron/sft_config.yaml).                                                                                                                                                                                                             |
| **[08/13/2025]** üéâ ROLL supports AMD GPUs with out-of-box image docker and Dockerfile and specific yamls under `examples/` directory. Please refer to [Installation](https://alibaba.github.io/ROLL/docs/English/QuickStart/installation).                                                                                             |
| **[08/11/2025]** üéâ Our Paper released, see [Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning](https://arxiv.org/abs/2508.08221).                                                                                                                                                                                         |
| **[08/10/2025]** üéâ Agentic RL supports [stepwise learning](examples/qwen2.5-0.5B-agentic/agent_val_frozen_lake_gigpo.yaml), like [GiGPO](https://arxiv.org/abs/2505.10978); Distill supports [VLM](examples/qwen2.5-vl-7B-distill/distill_vl_megatron.yaml). Explore the new capabilities!                                             |
| **[08/06/2025]** üéâ ROLL PPT is now available, [Slides](assets/ROLL%20È´òÊïà‰∏îÁî®Êà∑ÂèãÂ•ΩÁöÑÂ§ßÊ®°ÂûãRLËÆ≠ÁªÉÊ°ÜÊû∂.pdf).                                                                                                                                                                                                                                           |
| **[07/31/2025]** üéâ Refactor agentic rl design. Support agentic rl [async training](examples/qwen2.5-0.5B-agentic/agent_val_frozen_lake_async.yaml). Explore the new capabilities!                                                                                                                                                      |
| **[07/31/2025]** üéâ Support [DistillPipeline](examples/qwen2.5-7B-distill_megatron/run_distill_pipeline.sh)/[DpoPipeline](examples/qwen2.5-3B-dpo_megatron/run_dpo_pipeline.sh). Support [lora](examples/qwen2.5-7B-rlvr_megatron/rlvr_lora_zero3.yaml). Support [GSPO](https://arxiv.org/abs/2507.18071)                               |
| **[06/25/2025]** üéâ Support thread env for env scaling and support [qwen2.5 VL agentic pipeline](examples/qwen2.5-vl-3B-agentic/agentic_val_sokoban.yaml).                                                                                                                                                                              |
| **[06/13/2025]** üéâ Support [Qwen2.5 VL rlvr pipeline](examples/qwen2.5-vl-7B-rlvr/rlvr_megatron.yaml) and upgrade mcore to 0.12 version.                                                                                                                                                                                               |
| **[06/09/2025]** üéâ ROLL tech report is now available! Access the report [here](https://arxiv.org/abs/2506.06122).                                                                                                                                                                                                                      |
| **[06/08/2025]** üéâSupports  Qwen3([8B](examples/qwen3-8B-rlvr_megatron/rlvr_config.yaml)/14B/32B), Qwen3-MoE([30A3](examples/qwen3-30BA3B-rlvr_megatron/rlvr_config.yaml)/[235A22](examples/qwen3-235BA22B-rlvr_megatron/rlvr_config.yaml)), Qwen2.5([7B](examples/qwen2.5-7B-rlvr_megatron/rlvr_config.yaml)/14B/32B/72B) LLM models. |
| **[05/30/2025]** üéâ Training [RLVR](examples/qwen2.5-7B-rlvr_megatron/rlvr_config.yaml) and [Agentic RL](examples/qwen2.5-0.5B-agentic/agent_val_frozen_lake.yaml) with ROLL is now available! Explore the new capabilities.                                                                                                            |
---


## üöÄ Get Started

[Documents](https://alibaba.github.io/ROLL/)

### Quick Start
[Installation](https://alibaba.github.io/ROLL/docs/English/QuickStart/installation)  
[Config System Explanation](https://alibaba.github.io/ROLL/docs/English/QuickStart/config_system)  
[Debugging Guide](https://alibaba.github.io/ROLL/docs/English/QuickStart/debugging_guide_en)  
[Trackers and Metrics](https://alibaba.github.io/ROLL/docs/English/UserGuide/trackers_and_metrics)  
[Checkpoint Saving and Resuming Guide](https://alibaba.github.io/ROLL/docs/English/UserGuide/checkpoint_and_resume)  
[Converting MCoreAdapter Models to Hugging Face Format](https://alibaba.github.io/ROLL/docs/English/UserGuide/megatron_convert_2_hf)  
[Quick Start: Single-Node Deployment Guide](https://alibaba.github.io/ROLL/docs/English/QuickStart/single_node_quick_start)  
[Quick Start: Multi-Node Deployment Guide](https://alibaba.github.io/ROLL/docs/English/QuickStart/multi_node_quick_start)  
[Frequently Asked Questions](https://alibaba.github.io/ROLL/docs/English/QuickStart/qa_issues)

### UserGuide

#### Pipeline Step by Step
[RLVR Pipeline](https://alibaba.github.io/ROLL/docs/English/UserGuide/pipeline/rlvr_pipeline_start)  
[Agentic Pipeline](https://alibaba.github.io/ROLL/docs/English/UserGuide/pipeline/agentic_pipeline_start)  
[Agentic Comprehensive Guide](https://alibaba.github.io/ROLL/docs/English/UserGuide/pipeline/agent_pipeline_start)  
[Distill Pipeline](https://alibaba.github.io/ROLL/docs/English/UserGuide/pipeline/distill_pipeline_start)

#### Algorithms
[Reinforce++](https://alibaba.github.io/ROLL/docs/English/UserGuide/algorithms/Reinforce_Plus_Plus)  
[TOPR](https://alibaba.github.io/ROLL/docs/English/UserGuide/algorithms/TOPR)  
[GiGPO](https://alibaba.github.io/ROLL/docs/English/UserGuide/algorithms/agentic_GiGPO)  
[PPO](https://alibaba.github.io/ROLL/docs/English/UserGuide/algorithms/PPO)  
[Lite PPO](https://alibaba.github.io/ROLL/docs/English/UserGuide/algorithms/LitePPO)  
[GRPO](https://alibaba.github.io/ROLL/docs/English/UserGuide/algorithms/GRPO)  
[GSPO](https://alibaba.github.io/ROLL/docs/English/UserGuide/algorithms/GSPO)  
[RAFT++](https://alibaba.github.io/ROLL/docs/English/UserGuide/algorithms/RAFT_Plus_Plus)  
[StarPO](https://alibaba.github.io/ROLL/docs/English/UserGuide/algorithms/agentic_StarPO)   
[RewardFL](https://alibaba.github.io/ROLL/docs/English/UserGuide/algorithms/Reward_FL)

#### Backend
[DeepSeed](https://alibaba.github.io/ROLL/docs/English/UserGuide/backend/deepspeed)  
[Megatron](https://alibaba.github.io/ROLL/docs/English/UserGuide/backend/megatron)   
[vLLM](https://alibaba.github.io/ROLL/docs/English/UserGuide/backend/vllm)  
[SGLang](https://alibaba.github.io/ROLL/docs/English/UserGuide/backend/sglang)

#### Advanced Features
[Asynchronous Parallel Rollout](https://alibaba.github.io/ROLL/docs/English/UserGuide/async_parallel_rollout)  
[Asynchronous Training Feature](https://alibaba.github.io/ROLL/docs/English/UserGuide/async_training)  

#### Performance Optimization &amp; Resource Management 
[Resource Config](https://alibaba.github.io/ROLL/docs/English/UserGuide/device_mapping)   
[GPU Time-Division Multiplexing Control](https://alibaba.github.io/ROLL/docs/English/UserGuide/offload_reload_control)  

#### ROLL x Ascend
[Ascend Usage Guide](https://alibaba.github.io/ROLL/docs/English/UserGuide/ascend/ascend_usage)

---

## ‚ú® Key Features
*   **Multi-task RL Training (RLVR):** Covers mathematics, coding, general reasoning, open-ended Q&amp;A, instruction following, etc.
    *   Flexible `domain_batch_size` distribution control.
    *   **Sample-level asynchronous parallel Rollout**, asynchronous reward calculation, and dynamic sampling.
    *   Asynchronous training under implementation.
*   **Agentic RL:** Multi-turn interaction capabilities for games, multi-turn dialogues, tool use, etc.
    *   Environment-level **asynchronous parallel rollout**.
    *   Supports **asynchronous training**.
    *   Multi-turn interaction rollout supports **local debugging**, improving multi-turn interaction business development efficiency.
    *   Supports **TrajectoryWise (StartPO)** and **StepWise (GiGPO)** training paradigms.
*   **Algorithm-Friendly:** Provides flexible and rich RL strategy configurations by default.
    *   Over 20 rich reinforcement learning strategy options, such as reward normalization, reward clipping, various advantage estimation methods, etc.
    *   Out-of-the-box support for reinforcement learning algorithms, such as **PPO, GRPO, Reinforce++, TOPR, RAFT++, GSPO**, etc.
*   **Rich Training and Inference Engine:** Ray-based multi-role distributed architecture; Strategy abstraction unifies various backends, enabling easy operation from single machines to thousands-of-GPU clusters.
    *   Inference/Generation supports vLLM, SGLang.
    *   Training supports DeepSpeed (ZeRO), Megatron-LM 5D parallelism (mcore-adapter, dp/tp/pp/cp/ep), FSDP under implementation.
    *   Extreme offload/reload capabilities.
    *   Supports [LoRA](https://alibaba.github.io/ROLL/docs/English/UserGuide/backend/lora) training.
    *   Supports FP8 rollout (FP8 inference for LLM as judge, FP8 rollout with BF16 training under development).
*   **AutoDeviceMapping:** Supports custom device mapping for different roles, flexibly managing colocated and disaggregated deployments.
*   **Observability:** Integrated with SwanLab / WandB / TensorBoard, tracking of performance for each domain and reward type.
*   **Rich Post-training Technical Support:**
    *   Agentic RL LLM &amp; VLM
    *   RLVR LLM &amp; VLM
    *   Distill Pipeline LLM &amp; VLM
    *   DPO Pipeline
    *   SFT Pipeline under development



---

## üîÆ Upcoming Features

We are continuously working to expand ROLL&#039;s capabilities:
* ‚è±Ô∏è **Async RLVR pipeline**: For even more efficient and streamlined asynchronous operations.
* ‚öôÔ∏è **FSDP2**: Integrating the latest Fully Sharded Data Parallel techniques.
* üîç **Support DeepseekV3**: Adding compatibility for the newest Deepseek models.

---

## üèÜ Notable work based on ROLL
- [IPRO](https://arxiv.org/abs/2510.14255): A novel video diffusion framework using reinforcement learning to enhance identity preservation in human-centric I2V generation, optimizing diffusion models with face identity scorer and KL-divergence regularization.
- [TaoSR-SHE](https://arxiv.org/abs/2510.07972): Stepwise Hybrid Examination Reinforcement Learning Framework for Taobao Search Relevance, with SRPO (hybrid reward model + offline verifier), diversified data filtering, and multi-stage curriculum learning.
- [EARL](https://arxiv.org/abs/2510.05943): Efficient Agentic RL Systems for LLMs, introducing a dynamic parallelism selector and a layout-aware data dispatcher to boost throughput, reduce memory and data movement bottlenecks, enabling stable large-scale agentic RL without hard context-length limits.
- [LiveThinking](https://arxiv.org/abs/2510.07685): Real-time reasoning for AI-powered livestreaming by distilling a 670B teacher LLM to a 30B MoE (3B active) via Rejection Sampling Fine-Tuning, then compressing reasoning with GRPO; delivers sub-second latency and ~30x compute reduction, with gains in response correctness (3.3%), helpfulness (21.8%), and GMV in Taobao Live.
- [TaoSR-AGRL](https://www.arxiv.org/abs/2510.08048): Adaptive Guided Reinforcement Learning for LLM-based e-commerce relevance, introducing Rule-aware Reward Shaping and Adaptive Guided Replay to improve long-horizon reasoning, rule adherence, and training stability in Taobao Search; deployed in main search handling hundreds of millions of users.
- [RecGPT](https://www.arxiv.org/abs/2507.22879): a next-generation, LLM-driven framework that places user intent at the core of recommender systems, fostering a more sustainable and mutually beneficial ecosystem.
- [TaoSR1](https://arxiv.org/abs/2508.12365): A novel LLM framework directly deploying Chain-of-Thought (CoT) reasoning for e-commerce query-product relevance prediction, overcoming deployment challenges for superior performance.
- [AIGB-Pearl](https://www.arxiv.org/abs/2509.15927): a novel auto-bidding method that integrates generative planning and policy optimization, utilizing an LLM-enhanced trajectory evaluator to iteratively refine bidding strategies for state-of-the-art advertising performance.
-----

## üôè Citation and Acknowledgement

ROLL is inspired by the design of OpenRLHF, VeRL, Nemo-Aligner, and RAGEN.
The project is developed by Alibaba TAOBAO &amp; TMALL Group and Alibaba Group. The code is distributed under the Apache License (Version 2.0). This product contains various third-party components under other open-source licenses. See the `NOTICE` file for more information.

The following repositories have been used in ROLL, either in their close-to-original form or as an inspiration:

  * [NVIDIA/Megatron-LM](https://github.com/NVIDIA/Megatron-LM)
  * [microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed)
  * [sgl-project/sglang](https://github.com/sgl-project/sglang)
  * [vllm-project/vllm](https://github.com/vllm-project/vllm)
  * [modelscope/DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio)

If you use ROLL in your research or project, please consider citing us:

```bibtex
@article{wang2025reinforcement,
  title={Reinforcement Learning Optimization for Large-Scale Learning: An Efficient and User-Friendly Scaling Library},
  author={Wang, Weixun and Xiong, Shaopan and Chen, Gengru and Gao, Wei and Guo, Sheng and He, Yancheng and Huang, Ju and Liu, Jiaheng and Li, Zhendong and Li, Xiaoyang and others},
  journal={arXiv preprint arXiv:2506.06122},
  year={2025}
}
```



-----

## ü§ù About [ROCK &amp; ROLL Team]
ROLL is a project jointly developed by Taotian Future Living Lab and Alibaba AI Engine Team, with a strong emphasis on pioneering the future of Reinforcement Learning (RL). Our mission is to explore and shape innovative forms of future living powered by advanced RL technologies. If you are passionate about the future of RL and want to be part of its evolution, we warmly welcome you to join us!üëá

&lt;a href=&quot;./assets/roll_wechat.png&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/WeChat-green?logo=wechat&quot; alt=&quot;WeChat QR&quot;&gt;
&lt;/a&gt;

-----
We are HIRING! 
- Post Training Infra Á†îÂèëÂ∑•Á®ãÂ∏à [JD link](https://talent-holding.alibaba.com/off-campus/position-detail?lang=zh&amp;positionId=7000016304)
- Â§ßÊ®°ÂûãËÆ≠ÁªÉ‰∏ìÂÆ∂Ôºö 
  - ÔºàÁ§æÊãõÔºâ[JD link](https://talent.taotian.com/off-campus/position-detail?lang=zh&amp;positionId=7000024203)
  - ÔºàÊ†°ÊãõÔºâ[JD link](https://talent.taotian.com/campus/position-detail?positionI

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[google/magika]]></title>
            <link>https://github.com/google/magika</link>
            <guid>https://github.com/google/magika</guid>
            <pubDate>Wed, 12 Nov 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[Fast and accurate AI powered file content types detection]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google/magika">google/magika</a></h1>
            <p>Fast and accurate AI powered file content types detection</p>
            <p>Language: Python</p>
            <p>Stars: 9,520</p>
            <p>Forks: 469</p>
            <p>Stars today: 49 stars today</p>
            <h2>README</h2><pre># Magika

[![image](https://img.shields.io/pypi/v/magika.svg)](https://pypi.python.org/pypi/magika)
[![NPM Version](https://img.shields.io/npm/v/magika)](https://npmjs.com/package/magika)
[![image](https://img.shields.io/pypi/l/magika.svg)](https://pypi.python.org/pypi/magika)
[![image](https://img.shields.io/pypi/pyversions/magika.svg)](https://pypi.python.org/pypi/magika)
&lt;!-- [![OpenSSF Scorecard](https://api.scorecard.dev/projects/github.com/google/magika/badge)](https://scorecard.dev/viewer/?uri=github.com/google/magika) --&gt;
[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/8706/badge)](https://www.bestpractices.dev/en/projects/8706)
![CodeQL](https://github.com/google/magika/workflows/CodeQL/badge.svg)
[![Actions status](https://github.com/google/magika/actions/workflows/python-build-and-release-package.yml/badge.svg)](https://github.com/google/magika/actions)
[![PyPI Monthly Downloads](https://static.pepy.tech/badge/magika/month)](https://pepy.tech/projects/magika)
[![PyPI Downloads](https://static.pepy.tech/badge/magika)](https://pepy.tech/projects/magika)

Magika is a novel AI-powered file type detection tool that relies on the recent advance of deep learning to provide accurate detection. Under the hood, Magika employs a custom, highly optimized model that only weighs about a few MBs, and enables precise file identification within milliseconds, even when running on a single CPU. Magika has been trained and evaluated on a dataset of ~100M samples across 200+ content types (covering both binary and textual file formats), and it achieves an average ~99% accuracy on our test set.

Here is an example of what Magika command line output looks like:

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;./assets/magika-screenshot.png&quot; width=&quot;600&quot;&gt;
&lt;/p&gt;

Magika is used at scale to help improve Google users&#039; safety by routing Gmail, Drive, and Safe Browsing files to the proper security and content policy scanners, processing hundreds billions samples on a weekly basis. Magika has also been integrated with [VirusTotal](https://www.virustotal.com/) ([example](./assets/magika-vt.png)) and [abuse.ch](https://bazaar.abuse.ch/) ([example](./assets/magika-abusech.png)).

For more context you can read our initial [announcement post on Google&#039;s OSS blog](https://opensource.googleblog.com/2024/02/magika-ai-powered-fast-and-efficient-file-type-identification.html), you can consult [Magika&#039;s website](https://securityresearch.google/magika/), and you can read more in our [research paper](https://securityresearch.google/magika/additional-resources/research-papers-and-citation/), published at the IEEE/ACM International Conference on Software Engineering (ICSE) 2025.

You can try Magika without installing anything by using our [web demo](https://securityresearch.google/magika/demo/magika-demo/), which runs locally in your browser!


# Highlights

- Available as a command line tool written in Rust, a Python API, and additional bindings for Rust, JavaScript/TypeScript (with an experimental npm package, which powers our [web demo](https://securityresearch.google/magika/demo/magika-demo/)), and GoLang (WIP).
- Trained and evaluated on a dataset of ~100M files across [200+ content types](./assets/models/standard_v3_3/README.md).
- On our test set, Magika achieves ~99% average precision and recall, outperforming existing approaches -- especially on textual content types.
- After the model is loaded (which is a one-off overhead), the inference time is about 5ms per file, even when run on a single CPU.
- You can invoke Magika with even thousands of files at the same time. You can also use `-r` for recursively scanning a directory.
- Near-constant inference time, independently from the file size; Magika only uses a limited subset of the file&#039;s content.
- Magika uses a per-content-type threshold system that determines whether to &quot;trust&quot; the prediction for the model, or whether to return a generic label, such as &quot;Generic text document&quot; or &quot;Unknown binary data&quot;.
- The tolerance to errors can be controlled via different prediction modes, such as `high-confidence`, `medium-confidence`, and `best-guess`.
- The client and the bindings are already open source, and more is coming soon!

# Table of Contents

1. [Getting Started](#getting-started)
   1. [Installation](#installation)
   1. [Quick Start](#quick-start)
1. [Documentation](#documentation)
1. [Security Vulnerabilities](#security-vulnerabilities)
1. [License](#license)
1. [Disclaimer](#disclaimer)

# Getting Started

## Installation

### Command Line Tool

Magika ships a CLI written in Rust, and can be installed in several ways.

Via `magika` python package:
```shell
pipx install magika
```

Via installer script:
```shell
curl -LsSf https://securityresearch.google/magika/install.sh | sh
```

or

```shell
powershell -ExecutionPolicy Bypass -c &quot;irm https://securityresearch.google/magika/install.ps1 | iex&quot;
```


### Python package

```shell
pip install magika
```

### JavaScript package

```shell
npm install magika
```


## Quick Start

Here you can find a number of quick examples just to get you started.

To learn about Magika&#039;s inner workings, see the [Core Concepts](https://securityresearch.google/magika/core-concepts/) section of Magika&#039;s website.

### Command Line Tool Examples

```shell
% cd tests_data/basic &amp;&amp; magika -r * | head
asm/code.asm: Assembly (code)
batch/simple.bat: DOS batch file (code)
c/code.c: C source (code)
css/code.css: CSS source (code)
csv/magika_test.csv: CSV document (code)
dockerfile/Dockerfile: Dockerfile (code)
docx/doc.docx: Microsoft Word 2007+ document (document)
docx/magika_test.docx: Microsoft Word 2007+ document (document)
eml/sample.eml: RFC 822 mail (text)
empty/empty_file: Empty file (inode)
```

```shell
% magika ./tests_data/basic/python/code.py --json
[
  {
    &quot;path&quot;: &quot;./tests_data/basic/python/code.py&quot;,
    &quot;result&quot;: {
      &quot;status&quot;: &quot;ok&quot;,
      &quot;value&quot;: {
        &quot;dl&quot;: {
          &quot;description&quot;: &quot;Python source&quot;,
          &quot;extensions&quot;: [
            &quot;py&quot;,
            &quot;pyi&quot;
          ],
          &quot;group&quot;: &quot;code&quot;,
          &quot;is_text&quot;: true,
          &quot;label&quot;: &quot;python&quot;,
          &quot;mime_type&quot;: &quot;text/x-python&quot;
        },
        &quot;output&quot;: {
          &quot;description&quot;: &quot;Python source&quot;,
          &quot;extensions&quot;: [
            &quot;py&quot;,
            &quot;pyi&quot;
          ],
          &quot;group&quot;: &quot;code&quot;,
          &quot;is_text&quot;: true,
          &quot;label&quot;: &quot;python&quot;,
          &quot;mime_type&quot;: &quot;text/x-python&quot;
        },
        &quot;score&quot;: 0.996999979019165
      }
    }
  }
]
```

```shell
% cat tests_data/basic/ini/doc.ini | magika -
-: INI configuration file (text)
```

```shell
% magika --help
Determines file content types using AI

Usage: magika [OPTIONS] [PATH]...

Arguments:
  [PATH]...
          List of paths to the files to analyze.

          Use a dash (-) to read from standard input (can only be used once).

Options:
  -r, --recursive
          Identifies files within directories instead of identifying the directory itself

      --no-dereference
          Identifies symbolic links as is instead of identifying their content by following them

      --colors
          Prints with colors regardless of terminal support

      --no-colors
          Prints without colors regardless of terminal support

  -s, --output-score
          Prints the prediction score in addition to the content type

  -i, --mime-type
          Prints the MIME type instead of the content type description

  -l, --label
          Prints a simple label instead of the content type description

      --json
          Prints in JSON format

      --jsonl
          Prints in JSONL format

      --format &lt;CUSTOM&gt;
          Prints using a custom format (use --help for details).

          The following placeholders are supported:

            %p  The file path
            %l  The unique label identifying the content type
            %d  The description of the content type
            %g  The group of the content type
            %m  The MIME type of the content type
            %e  Possible file extensions for the content type
            %s  The score of the content type for the file
            %S  The score of the content type for the file in percent
            %b  The model output if overruled (empty otherwise)
            %%  A literal %

  -h, --help
          Print help (see a summary with &#039;-h&#039;)

  -V, --version
          Print version
```

For more examples and documentation about the CLI, see https://crates.io/crates/magika-cli.


### Python Examples

```python
&gt;&gt;&gt; from magika import Magika
&gt;&gt;&gt; m = Magika()
&gt;&gt;&gt; res = m.identify_bytes(b&#039;function log(msg) {console.log(msg);}&#039;)
&gt;&gt;&gt; print(res.output.label)
javascript
```

```python
&gt;&gt;&gt; from magika import Magika
&gt;&gt;&gt; m = Magika()
&gt;&gt;&gt; res = m.identify_path(&#039;./tests_data/basic/ini/doc.ini&#039;)
&gt;&gt;&gt; print(res.output.label)
ini
```

```python
&gt;&gt;&gt; from magika import Magika
&gt;&gt;&gt; m = Magika()
&gt;&gt;&gt; with open(&#039;./tests_data/basic/ini/doc.ini&#039;, &#039;rb&#039;) as f:
&gt;&gt;&gt;     res = m.identify_stream(f)
&gt;&gt;&gt; print(res.output.label)
ini
```

For more examples and documentation about the Python module, see the [Python `Magika` module](https://securityresearch.google/magika/cli-and-bindings/python/) section.


# Documentation

Please consult [Magika&#039;s website](https://securityresearch.google/magika) for detailed documentation about:
- Core Concepts
  - How Magika works
  - Models &amp; content types
  - Prediction modes
  - Understanding the output
- CLI &amp; Bindings (Python module, JavaScript module, ...)
- Contributing
- FAQ
- ...


# Security Vulnerabilities

Please contact us directly at magika-dev@google.com.


# License

Apache 2.0; see [`LICENSE`](LICENSE) for details.


# Disclaimer

This project is not an official Google project. It is not supported by
Google and Google specifically disclaims all warranties as to its quality,
merchantability, or fitness for a particular purpose.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>