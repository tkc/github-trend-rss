<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Thu, 11 Dec 2025 00:04:29 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[datawhalechina/hello-agents]]></title>
            <link>https://github.com/datawhalechina/hello-agents</link>
            <guid>https://github.com/datawhalechina/hello-agents</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[ğŸ“š ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹â€”â€”ä»é›¶å¼€å§‹çš„æ™ºèƒ½ä½“åŸç†ä¸å®è·µæ•™ç¨‹]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/datawhalechina/hello-agents">datawhalechina/hello-agents</a></h1>
            <p>ğŸ“š ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹â€”â€”ä»é›¶å¼€å§‹çš„æ™ºèƒ½ä½“åŸç†ä¸å®è·µæ•™ç¨‹</p>
            <p>Language: Python</p>
            <p>Stars: 6,969</p>
            <p>Forks: 787</p>
            <p>Stars today: 458 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;right&quot;&gt;
  &lt;a href=&quot;./README_EN.md&quot;&gt;English&lt;/a&gt; | ä¸­æ–‡
&lt;/div&gt;

&lt;div align=&#039;center&#039;&gt;
  &lt;img src=&quot;./docs/images/hello-agents.png&quot; alt=&quot;alt text&quot; width=&quot;100%&quot;&gt;
  &lt;h1&gt;Hello-Agents&lt;/h1&gt;
  &lt;h3&gt;ğŸ¤– ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹&lt;/h3&gt;
  &lt;p&gt;&lt;em&gt;ä»åŸºç¡€ç†è®ºåˆ°å®é™…åº”ç”¨ï¼Œå…¨é¢æŒæ¡æ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡ä¸å®ç°&lt;/em&gt;&lt;/p&gt;
  &lt;img src=&quot;https://img.shields.io/github/stars/datawhalechina/Hello-Agents?style=flat&amp;logo=github&quot; alt=&quot;GitHub stars&quot;/&gt;
  &lt;img src=&quot;https://img.shields.io/github/forks/datawhalechina/Hello-Agents?style=flat&amp;logo=github&quot; alt=&quot;GitHub forks&quot;/&gt;
  &lt;img src=&quot;https://img.shields.io/badge/language-Chinese-brightgreen?style=flat&quot; alt=&quot;Language&quot;/&gt;
  &lt;a href=&quot;https://github.com/datawhalechina/Hello-Agents&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-Project-blue?style=flat&amp;logo=github&quot; alt=&quot;GitHub Project&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://datawhalechina.github.io/hello-agents/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/åœ¨çº¿é˜…è¯»-Online%20Reading-green?style=flat&amp;logo=gitbook&quot; alt=&quot;Online Reading&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

---

## ğŸ¯ é¡¹ç›®ä»‹ç»

&amp;emsp;&amp;emsp;å¦‚æœè¯´ 2024 å¹´æ˜¯&quot;ç™¾æ¨¡å¤§æˆ˜&quot;çš„å…ƒå¹´ï¼Œé‚£ä¹ˆ 2025 å¹´æ— ç–‘å¼€å¯äº†&quot;Agent å…ƒå¹´&quot;ã€‚æŠ€æœ¯çš„ç„¦ç‚¹æ­£ä»è®­ç»ƒæ›´å¤§çš„åŸºç¡€æ¨¡å‹ï¼Œè½¬å‘æ„å»ºæ›´èªæ˜çš„æ™ºèƒ½ä½“åº”ç”¨ã€‚ç„¶è€Œï¼Œå½“å‰ç³»ç»Ÿæ€§ã€é‡å®è·µçš„æ•™ç¨‹å´æåº¦åŒ®ä¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å‘èµ·äº† Hello-Agents é¡¹ç›®ï¼Œå¸Œæœ›èƒ½ä¸ºç¤¾åŒºæä¾›ä¸€æœ¬ä»é›¶å¼€å§‹ã€ç†è®ºä¸å®æˆ˜å¹¶é‡çš„æ™ºèƒ½ä½“ç³»ç»Ÿæ„å»ºæŒ‡å—ã€‚

&amp;emsp;&amp;emsp;Hello-Agents æ˜¯ Datawhale ç¤¾åŒºçš„&lt;strong&gt;ç³»ç»Ÿæ€§æ™ºèƒ½ä½“å­¦ä¹ æ•™ç¨‹&lt;/strong&gt;ã€‚å¦‚ä»Š Agent æ„å»ºä¸»è¦åˆ†ä¸ºä¸¤æ´¾ï¼Œä¸€æ´¾æ˜¯ Difyï¼ŒCozeï¼Œn8n è¿™ç±»è½¯ä»¶å·¥ç¨‹ç±» Agentï¼Œå…¶æœ¬è´¨æ˜¯æµç¨‹é©±åŠ¨çš„è½¯ä»¶å¼€å‘ï¼ŒLLM ä½œä¸ºæ•°æ®å¤„ç†çš„åç«¯ï¼›å¦ä¸€æ´¾åˆ™æ˜¯ AI åŸç”Ÿçš„ Agentï¼Œå³çœŸæ­£ä»¥ AI é©±åŠ¨çš„ Agentã€‚æœ¬æ•™ç¨‹æ—¨åœ¨å¸¦é¢†å¤§å®¶æ·±å…¥ç†è§£å¹¶æ„å»ºåè€…â€”â€”çœŸæ­£çš„ AI Native Agentã€‚æ•™ç¨‹å°†å¸¦é¢†ä½ ç©¿é€æ¡†æ¶è¡¨è±¡ï¼Œä»æ™ºèƒ½ä½“çš„æ ¸å¿ƒåŸç†å‡ºå‘ï¼Œæ·±å…¥å…¶æ ¸å¿ƒæ¶æ„ï¼Œç†è§£å…¶ç»å…¸èŒƒå¼ï¼Œå¹¶æœ€ç»ˆäº²æ‰‹æ„å»ºèµ·å±äºè‡ªå·±çš„å¤šæ™ºèƒ½ä½“åº”ç”¨ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæœ€å¥½çš„å­¦ä¹ æ–¹å¼å°±æ˜¯åŠ¨æ‰‹å®è·µã€‚å¸Œæœ›è¿™æœ¬æ•™ç¨‹èƒ½æˆä¸ºä½ æ¢ç´¢æ™ºèƒ½ä½“ä¸–ç•Œçš„èµ·ç‚¹ï¼Œèƒ½å¤Ÿä»ä¸€åå¤§è¯­è¨€æ¨¡å‹çš„&quot;ä½¿ç”¨è€…&quot;ï¼Œèœ•å˜ä¸ºä¸€åæ™ºèƒ½ä½“ç³»ç»Ÿçš„&quot;æ„å»ºè€…&quot;ã€‚

## ğŸ“š å¿«é€Ÿå¼€å§‹

### åœ¨çº¿é˜…è¯»
**[ğŸŒ ç‚¹å‡»è¿™é‡Œå¼€å§‹åœ¨çº¿é˜…è¯»](https://datawhalechina.github.io/hello-agents/)** - æ— éœ€ä¸‹è½½ï¼Œéšæ—¶éšåœ°å­¦ä¹ 

**[ğŸ“– Cookbook(æµ‹è¯•ç‰ˆ)](https://book.heterocat.com.cn/)**

### æœ¬åœ°é˜…è¯»
å¦‚æœæ‚¨å¸Œæœ›åœ¨æœ¬åœ°é˜…è¯»æˆ–è´¡çŒ®å†…å®¹ï¼Œè¯·å‚è€ƒä¸‹æ–¹çš„å­¦ä¹ æŒ‡å—ã€‚

### âœ¨ ä½ å°†æ”¶è·ä»€ä¹ˆï¼Ÿ

- ğŸ“– &lt;strong&gt;Datawhale å¼€æºå…è´¹&lt;/strong&gt; å®Œå…¨å…è´¹å­¦ä¹ æœ¬é¡¹ç›®æ‰€æœ‰å†…å®¹ï¼Œä¸ç¤¾åŒºå…±åŒæˆé•¿
- ğŸ” &lt;strong&gt;ç†è§£æ ¸å¿ƒåŸç†&lt;/strong&gt; æ·±å…¥ç†è§£æ™ºèƒ½ä½“çš„æ¦‚å¿µã€å†å²ä¸ç»å…¸èŒƒå¼
- ğŸ—ï¸ &lt;strong&gt;äº²æ‰‹å®ç°&lt;/strong&gt; æŒæ¡çƒ­é—¨ä½ä»£ç å¹³å°å’Œæ™ºèƒ½ä½“ä»£ç æ¡†æ¶çš„ä½¿ç”¨
- ğŸ› ï¸ &lt;strong&gt;è‡ªç ”æ¡†æ¶[HelloAgents](https://github.com/jjyaoao/helloagents)&lt;/strong&gt; åŸºäº Openai åŸç”Ÿ API ä»é›¶æ„å»ºä¸€ä¸ªè‡ªå·±çš„æ™ºèƒ½ä½“æ¡†æ¶
- âš™ï¸ &lt;strong&gt;æŒæ¡é«˜çº§æŠ€èƒ½&lt;/strong&gt; ä¸€æ­¥æ­¥å®ç°ä¸Šä¸‹æ–‡å·¥ç¨‹ã€Memoryã€åè®®ã€è¯„ä¼°ç­‰ç³»ç»Ÿæ€§æŠ€æœ¯
- ğŸ¤ &lt;strong&gt;æ¨¡å‹è®­ç»ƒ&lt;/strong&gt; æŒæ¡ Agentic RLï¼Œä» SFT åˆ° GRPO çš„å…¨æµç¨‹å®æˆ˜è®­ç»ƒ LLM
- ğŸš€ &lt;strong&gt;é©±åŠ¨çœŸå®æ¡ˆä¾‹&lt;/strong&gt; å®æˆ˜å¼€å‘æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹ã€èµ›åšå°é•‡ç­‰ç»¼åˆé¡¹ç›®
- ğŸ“– &lt;strong&gt;æ±‚èŒé¢è¯•&lt;/strong&gt; å­¦ä¹ æ™ºèƒ½ä½“æ±‚èŒç›¸å…³é¢è¯•é—®é¢˜

## ğŸ“– å†…å®¹å¯¼èˆª

| ç« èŠ‚                                                                                        | å…³é”®å†…å®¹                                      | çŠ¶æ€ |
| ------------------------------------------------------------------------------------------- | --------------------------------------------- | ---- |
| [å‰è¨€](./docs/å‰è¨€.md)                                                                      | é¡¹ç›®çš„ç¼˜èµ·ã€èƒŒæ™¯åŠè¯»è€…å»ºè®®                    | âœ…    |
| &lt;strong&gt;ç¬¬ä¸€éƒ¨åˆ†ï¼šæ™ºèƒ½ä½“ä¸è¯­è¨€æ¨¡å‹åŸºç¡€&lt;/strong&gt;                                             |                                               |      |
| [ç¬¬ä¸€ç«  åˆè¯†æ™ºèƒ½ä½“](./docs/chapter1/ç¬¬ä¸€ç« %20åˆè¯†æ™ºèƒ½ä½“.md)                                 | æ™ºèƒ½ä½“å®šä¹‰ã€ç±»å‹ã€èŒƒå¼ä¸åº”ç”¨                  | âœ…    |
| [ç¬¬äºŒç«  æ™ºèƒ½ä½“å‘å±•å²](./docs/chapter2/ç¬¬äºŒç« %20æ™ºèƒ½ä½“å‘å±•å².md)                             | ä»ç¬¦å·ä¸»ä¹‰åˆ° LLM é©±åŠ¨çš„æ™ºèƒ½ä½“æ¼”è¿›             | âœ…    |
| [ç¬¬ä¸‰ç«  å¤§è¯­è¨€æ¨¡å‹åŸºç¡€](./docs/chapter3/ç¬¬ä¸‰ç« %20å¤§è¯­è¨€æ¨¡å‹åŸºç¡€.md)                         | Transformerã€æç¤ºã€ä¸»æµ LLM åŠå…¶å±€é™          | âœ…    |
| &lt;strong&gt;ç¬¬äºŒéƒ¨åˆ†ï¼šæ„å»ºä½ çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“&lt;/strong&gt;                                         |                                               |      |
| [ç¬¬å››ç«  æ™ºèƒ½ä½“ç»å…¸èŒƒå¼æ„å»º](./docs/chapter4/ç¬¬å››ç« %20æ™ºèƒ½ä½“ç»å…¸èŒƒå¼æ„å»º.md)                 | æ‰‹æŠŠæ‰‹å®ç° ReActã€Plan-and-Solveã€Reflection  | âœ…    |
| [ç¬¬äº”ç«  åŸºäºä½ä»£ç å¹³å°çš„æ™ºèƒ½ä½“æ­å»º](./docs/chapter5/ç¬¬äº”ç« %20åŸºäºä½ä»£ç å¹³å°çš„æ™ºèƒ½ä½“æ­å»º.md) | äº†è§£ Cozeã€Difyã€n8n ç­‰ä½ä»£ç æ™ºèƒ½ä½“å¹³å°ä½¿ç”¨   | âœ…    |
| [ç¬¬å…­ç«  æ¡†æ¶å¼€å‘å®è·µ](./docs/chapter6/ç¬¬å…­ç« %20æ¡†æ¶å¼€å‘å®è·µ.md)                             | AutoGenã€AgentScopeã€LangGraph ç­‰ä¸»æµæ¡†æ¶åº”ç”¨ | âœ…    |
| [ç¬¬ä¸ƒç«  æ„å»ºä½ çš„Agentæ¡†æ¶](./docs/chapter7/ç¬¬ä¸ƒç« %20æ„å»ºä½ çš„Agentæ¡†æ¶.md)                   | ä» 0 å¼€å§‹æ„å»ºæ™ºèƒ½ä½“æ¡†æ¶                       | âœ…    |
| &lt;strong&gt;ç¬¬ä¸‰éƒ¨åˆ†ï¼šé«˜çº§çŸ¥è¯†æ‰©å±•&lt;/strong&gt;                                                     |                                               |      |
| [ç¬¬å…«ç«  è®°å¿†ä¸æ£€ç´¢](./docs/chapter8/ç¬¬å…«ç« %20è®°å¿†ä¸æ£€ç´¢.md)                                 | è®°å¿†ç³»ç»Ÿï¼ŒRAGï¼Œå­˜å‚¨                           | âœ…    |
| [ç¬¬ä¹ç«  ä¸Šä¸‹æ–‡å·¥ç¨‹](./docs/chapter9/ç¬¬ä¹ç« %20ä¸Šä¸‹æ–‡å·¥ç¨‹.md)                                 | æŒç»­äº¤äº’çš„&quot;æƒ…å¢ƒç†è§£&quot;                          | âœ…    |
| [ç¬¬åç«  æ™ºèƒ½ä½“é€šä¿¡åè®®](./docs/chapter10/ç¬¬åç« %20æ™ºèƒ½ä½“é€šä¿¡åè®®.md)                        | MCPã€A2Aã€ANP ç­‰åè®®è§£æ                      | âœ…    |
| [ç¬¬åä¸€ç«  Agentic-RL](./docs/chapter11/ç¬¬åä¸€ç« %20Agentic-RL.md)                            | ä» SFT åˆ° GRPO çš„ LLM è®­ç»ƒå®æˆ˜                | âœ…    |
| [ç¬¬åäºŒç«  æ™ºèƒ½ä½“æ€§èƒ½è¯„ä¼°](./docs/chapter12/ç¬¬åäºŒç« %20æ™ºèƒ½ä½“æ€§èƒ½è¯„ä¼°.md)                    | æ ¸å¿ƒæŒ‡æ ‡ã€åŸºå‡†æµ‹è¯•ä¸è¯„ä¼°æ¡†æ¶                  | âœ…    |
| &lt;strong&gt;ç¬¬å››éƒ¨åˆ†ï¼šç»¼åˆæ¡ˆä¾‹è¿›é˜¶&lt;/strong&gt;                                                     |                                               |      |
| [ç¬¬åä¸‰ç«  æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹](./docs/chapter13/ç¬¬åä¸‰ç« %20æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹.md)                        | MCP ä¸å¤šæ™ºèƒ½ä½“åä½œçš„çœŸå®ä¸–ç•Œåº”ç”¨              | âœ…    |
| [ç¬¬åå››ç«  è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“](./docs/chapter14/ç¬¬åå››ç« %20è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“.md)        | DeepResearch Agent å¤ç°ä¸è§£æ                 | âœ…    |
| [ç¬¬åäº”ç«  æ„å»ºèµ›åšå°é•‡](./docs/chapter15/ç¬¬åäº”ç« %20æ„å»ºèµ›åšå°é•‡.md)                        | Agent ä¸æ¸¸æˆçš„ç»“åˆï¼Œæ¨¡æ‹Ÿç¤¾ä¼šåŠ¨æ€              | âœ…    |
| &lt;strong&gt;ç¬¬äº”éƒ¨åˆ†ï¼šæ¯•ä¸šè®¾è®¡åŠæœªæ¥å±•æœ›&lt;/strong&gt;                                               |                                               |      |
| [ç¬¬åå…­ç«  æ¯•ä¸šè®¾è®¡](./docs/chapter16/ç¬¬åå…­ç« %20æ¯•ä¸šè®¾è®¡.md)                                | æ„å»ºå±äºä½ çš„å®Œæ•´å¤šæ™ºèƒ½ä½“åº”ç”¨                  | âœ…    |

### ç¤¾åŒºè´¡çŒ®ç²¾é€‰ (Community Blog)

&amp;emsp;&amp;emsp;æ¬¢è¿å¤§å®¶å°†åœ¨å­¦ä¹  Hello-Agents æˆ– Agent ç›¸å…³æŠ€æœ¯ä¸­çš„ç‹¬åˆ°è§è§£ã€å®è·µæ€»ç»“ï¼Œä»¥ PR çš„å½¢å¼è´¡çŒ®åˆ°ç¤¾åŒºç²¾é€‰ã€‚å¦‚æœæ˜¯ç‹¬ç«‹äºæ­£æ–‡çš„å†…å®¹ï¼Œä¹Ÿå¯ä»¥æŠ•ç¨¿è‡³ Extra-Chapterï¼&lt;strong&gt;æœŸå¾…ä½ çš„ç¬¬ä¸€æ¬¡è´¡çŒ®ï¼&lt;/strong&gt;

| ç¤¾åŒºç²¾é€‰                                                                                                                                      | å†…å®¹æ€»ç»“                 |
| --------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------ |
| [01-Agenté¢è¯•é¢˜æ€»ç»“](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra01-é¢è¯•é—®é¢˜æ€»ç»“.md)                          | Agent å²—ä½ç›¸å…³é¢è¯•é—®é¢˜   |
| [01-Agenté¢è¯•é¢˜ç­”æ¡ˆ](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra01-å‚è€ƒç­”æ¡ˆ.md)                              | ç›¸å…³é¢è¯•é—®é¢˜ç­”æ¡ˆ         |
| [02-ä¸Šä¸‹æ–‡å·¥ç¨‹å†…å®¹è¡¥å……](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra02-ä¸Šä¸‹æ–‡å·¥ç¨‹è¡¥å……çŸ¥è¯†.md)                 | ä¸Šä¸‹æ–‡å·¥ç¨‹å†…å®¹æ‰©å±•       |
| [03-Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ•™ç¨‹](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra03-Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ“ä½œæµç¨‹.md) | Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ•™ç¨‹ |
| [04-Hello-agentsè¯¾ç¨‹å¸¸è§é—®é¢˜](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra04-DatawhaleFAQ.md)                 | Datawhaleè¯¾ç¨‹å¸¸è§é—®é¢˜    |

### PDF ç‰ˆæœ¬ä¸‹è½½

&amp;emsp;&amp;emsp;*&lt;strong&gt;æœ¬ Hello-Agents PDF æ•™ç¨‹å®Œå…¨å¼€æºå…è´¹ã€‚ä¸ºé˜²æ­¢å„ç±»è¥é”€å·åŠ æ°´å°åè´©å–ç»™å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåˆå­¦è€…ï¼Œæˆ‘ä»¬ç‰¹åœ°åœ¨ PDF æ–‡ä»¶ä¸­é¢„å…ˆæ·»åŠ äº†ä¸å½±å“é˜…è¯»çš„ Datawhale å¼€æºæ ‡å¿—æ°´å°ï¼Œæ•¬è¯·è°…è§£ï½&lt;/strong&gt;*

&gt; *Hello-Agents PDF : https://github.com/datawhalechina/hello-agents/releases/tag/V1.0.0*  
&gt; *Hello-Agents PDF å›½å†…ä¸‹è½½åœ°å€ : https://www.datawhale.cn/learn/summary/239* 

## ğŸ’¡ å¦‚ä½•å­¦ä¹ 

&amp;emsp;&amp;emsp;æ¬¢è¿ä½ ï¼Œæœªæ¥çš„æ™ºèƒ½ç³»ç»Ÿæ„å»ºè€…ï¼åœ¨å¼€å¯è¿™æ®µæ¿€åŠ¨äººå¿ƒçš„æ—…ç¨‹ä¹‹å‰ï¼Œè¯·å…è®¸æˆ‘ä»¬ç»™ä½ ä¸€äº›æ¸…æ™°çš„æŒ‡å¼•ã€‚

&amp;emsp;&amp;emsp;æœ¬é¡¹ç›®å†…å®¹å…¼é¡¾ç†è®ºä¸å®æˆ˜ï¼Œæ—¨åœ¨å¸®åŠ©ä½ ç³»ç»Ÿæ€§åœ°æŒæ¡ä»å•ä¸ªæ™ºèƒ½ä½“åˆ°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡ä¸å¼€å‘å…¨æµç¨‹ã€‚å› æ­¤ï¼Œå°¤å…¶é€‚åˆæœ‰ä¸€å®šç¼–ç¨‹åŸºç¡€çš„ &lt;strong&gt;AI å¼€å‘è€…ã€è½¯ä»¶å·¥ç¨‹å¸ˆã€åœ¨æ ¡å­¦ç”Ÿ&lt;/strong&gt; ä»¥åŠå¯¹å‰æ²¿ AI æŠ€æœ¯æŠ±æœ‰æµ“åšå…´è¶£çš„ &lt;strong&gt;è‡ªå­¦è€…&lt;/strong&gt;ã€‚åœ¨å­¦ä¹ æœ¬é¡¹ç›®ä¹‹å‰ï¼Œæˆ‘ä»¬å¸Œæœ›ä½ å…·å¤‡åŸºç¡€çš„ Python ç¼–ç¨‹èƒ½åŠ›ï¼Œå¹¶å¯¹å¤§è¯­è¨€æ¨¡å‹æœ‰åŸºæœ¬çš„æ¦‚å¿µæ€§äº†è§£ï¼ˆä¾‹å¦‚ï¼ŒçŸ¥é“å¦‚ä½•é€šè¿‡ API è°ƒç”¨ä¸€ä¸ª LLMï¼‰ã€‚é¡¹ç›®çš„é‡ç‚¹æ˜¯åº”ç”¨ä¸æ„å»ºï¼Œå› æ­¤ä½ æ— éœ€å…·å¤‡æ·±åšçš„ç®—æ³•æˆ–æ¨¡å‹è®­ç»ƒèƒŒæ™¯ã€‚

&amp;emsp;&amp;emsp;é¡¹ç›®åˆ†ä¸ºäº”å¤§éƒ¨åˆ†ï¼Œæ¯ä¸€éƒ¨åˆ†éƒ½æ˜¯é€šå¾€ä¸‹ä¸€é˜¶æ®µçš„åšå®é˜¶æ¢¯ï¼š

- &lt;strong&gt;ç¬¬ä¸€éƒ¨åˆ†ï¼šæ™ºèƒ½ä½“ä¸è¯­è¨€æ¨¡å‹åŸºç¡€&lt;/strong&gt;ï¼ˆç¬¬ä¸€ç« ï½ç¬¬ä¸‰ç« ï¼‰ï¼Œæˆ‘ä»¬å°†ä»æ™ºèƒ½ä½“çš„å®šä¹‰ã€ç±»å‹ä¸å‘å±•å†å²è®²èµ·ï¼Œä¸ºä½ æ¢³ç†&quot;æ™ºèƒ½ä½“&quot;è¿™ä¸€æ¦‚å¿µçš„æ¥é¾™å»è„‰ã€‚éšåï¼Œæˆ‘ä»¬ä¼šå¿«é€Ÿå·©å›ºå¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œä¸ºä½ çš„å®è·µä¹‹æ—…æ‰“ä¸‹åšå®çš„ç†è®ºåœ°åŸºã€‚

- &lt;strong&gt;ç¬¬äºŒéƒ¨åˆ†ï¼šæ„å»ºä½ çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“&lt;/strong&gt;ï¼ˆç¬¬å››ç« ï½ç¬¬ä¸ƒç« ï¼‰ï¼Œè¿™æ˜¯ä½ åŠ¨æ‰‹å®è·µçš„èµ·ç‚¹ã€‚ä½ å°†äº²æ‰‹å®ç° ReAct ç­‰ç»å…¸èŒƒå¼ï¼Œä½“éªŒ Coze ç­‰ä½ä»£ç å¹³å°çš„ä¾¿æ·ï¼Œå¹¶æŒæ¡ Langgraph ç­‰ä¸»æµæ¡†æ¶çš„åº”ç”¨ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è¿˜ä¼šå¸¦ä½ ä»é›¶å¼€å§‹æ„å»ºä¸€ä¸ªå±äºè‡ªå·±çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œè®©ä½ å…¼å…·â€œç”¨è½®å­â€ä¸â€œé€ è½®å­â€çš„èƒ½åŠ›ã€‚

- &lt;strong&gt;ç¬¬ä¸‰éƒ¨åˆ†ï¼šé«˜çº§çŸ¥è¯†æ‰©å±•&lt;/strong&gt;ï¼ˆç¬¬å…«ç« ï½ç¬¬åäºŒç« ï¼‰ï¼Œåœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œä½ çš„æ™ºèƒ½ä½“å°†â€œå­¦ä¼šâ€æ€è€ƒä¸åä½œã€‚æˆ‘ä»¬å°†ä½¿ç”¨ç¬¬äºŒéƒ¨åˆ†çš„è‡ªç ”æ¡†æ¶ï¼Œæ·±å…¥æ¢ç´¢è®°å¿†ä¸æ£€ç´¢ã€ä¸Šä¸‹æ–‡å·¥ç¨‹ã€Agent è®­ç»ƒç­‰æ ¸å¿ƒæŠ€æœ¯ï¼Œå¹¶å­¦ä¹ å¤šæ™ºèƒ½ä½“é—´çš„é€šä¿¡åè®®ã€‚æœ€ç»ˆï¼Œä½ å°†æŒæ¡è¯„ä¼°æ™ºèƒ½ä½“ç³»ç»Ÿæ€§èƒ½çš„ä¸“ä¸šæ–¹æ³•ã€‚

- &lt;strong&gt;ç¬¬å››éƒ¨åˆ†ï¼šç»¼åˆæ¡ˆä¾‹è¿›é˜¶&lt;/strong&gt;ï¼ˆç¬¬åä¸‰ç« ï½ç¬¬åäº”ç« ï¼‰ï¼Œè¿™é‡Œæ˜¯ç†è®ºä¸å®è·µçš„äº¤æ±‡ç‚¹ã€‚ä½ å°†æŠŠæ‰€å­¦èä¼šè´¯é€šï¼Œäº²æ‰‹æ‰“é€ æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹ã€è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“ï¼Œä¹ƒè‡³ä¸€ä¸ªæ¨¡æ‹Ÿç¤¾ä¼šåŠ¨æ€çš„èµ›åšå°é•‡ï¼Œåœ¨çœŸå®æœ‰è¶£çš„é¡¹ç›®ä¸­æ·¬ç‚¼ä½ çš„æ„å»ºèƒ½åŠ›ã€‚

- &lt;strong&gt;ç¬¬äº”éƒ¨åˆ†ï¼šæ¯•ä¸šè®¾è®¡åŠæœªæ¥å±•æœ›&lt;/strong&gt;ï¼ˆç¬¬åå…­ç« ï¼‰ï¼Œåœ¨æ—…ç¨‹çš„ç»ˆç‚¹ï¼Œä½ å°†è¿æ¥ä¸€ä¸ªæ¯•ä¸šè®¾è®¡ï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„ã€å±äºä½ è‡ªå·±çš„å¤šæ™ºèƒ½ä½“åº”ç”¨ï¼Œå…¨é¢æ£€éªŒä½ çš„å­¦ä¹ æˆæœã€‚æˆ‘ä»¬è¿˜å°†ä¸ä½ ä¸€åŒå±•æœ›æ™ºèƒ½ä½“çš„æœªæ¥ï¼Œæ¢ç´¢æ¿€åŠ¨äººå¿ƒçš„å‰æ²¿æ–¹å‘ã€‚


&amp;emsp;&amp;emsp;æ™ºèƒ½ä½“æ˜¯ä¸€ä¸ªé£é€Ÿå‘å±•ä¸”æåº¦ä¾èµ–å®è·µçš„é¢†åŸŸã€‚ä¸ºäº†è·å¾—æœ€ä½³çš„å­¦ä¹ æ•ˆæœï¼Œæˆ‘ä»¬åœ¨é¡¹ç›®çš„`code`æ–‡ä»¶å¤¹å†…æä¾›äº†é…å¥—çš„å…¨éƒ¨ä»£ç ï¼Œå¼ºçƒˆå»ºè®®ä½ &lt;strong&gt;å°†ç†è®ºä¸å®è·µç›¸ç»“åˆ&lt;/strong&gt;ã€‚è¯·åŠ¡å¿…äº²æ‰‹è¿è¡Œã€è°ƒè¯•ç”šè‡³ä¿®æ”¹é¡¹ç›®é‡Œæä¾›çš„æ¯ä¸€ä»½ä»£ç ã€‚æ¬¢è¿ä½ éšæ—¶å…³æ³¨ Datawhale ä»¥åŠå…¶ä»– Agent ç›¸å…³ç¤¾åŒºï¼Œå½“é‡åˆ°é—®é¢˜æ—¶ï¼Œä½ å¯ä»¥éšæ—¶åœ¨æœ¬é¡¹ç›®çš„ issue åŒºæé—®ã€‚

&amp;emsp;&amp;emsp;ç°åœ¨ï¼Œå‡†å¤‡å¥½è¿›å…¥æ™ºèƒ½ä½“çš„å¥‡å¦™ä¸–ç•Œäº†å—ï¼Ÿè®©æˆ‘ä»¬å³åˆ»å¯ç¨‹ï¼

## ä¸‹ä¸€æ­¥è§„åˆ’
- []è‹±æ–‡ç‰ˆæ•™ç¨‹
- []åŒè¯­è§†é¢‘è¯¾ç¨‹[è‹±æ–‡+ä¸­æ–‡]ï¼ˆå°†ä¼šæ›´åŠ ç»†è‡´ï¼Œå®è·µè¯¾å¸¦é¢†å¤§å®¶ä»è®¾è®¡æ€è·¯åˆ°å®æ–½ï¼Œæˆäººä»¥é±¼ä¹Ÿæˆäººä»¥æ¸”ï¼‰
- []å…±åˆ›ç¬¬16ç« ï¼ˆæ‰“é€ å„ç±»Agentåº”ç”¨,æ›´æ‰“é€ Agentç”Ÿæ€ï¼‰
  
## ğŸ¤ å¦‚ä½•è´¡çŒ®

æˆ‘ä»¬æ˜¯ä¸€ä¸ªå¼€æ”¾çš„å¼€æºç¤¾åŒºï¼Œæ¬¢è¿ä»»ä½•å½¢å¼çš„è´¡çŒ®ï¼

- ğŸ› &lt;strong&gt;æŠ¥å‘Š Bug&lt;/strong&gt; - å‘ç°å†…å®¹æˆ–ä»£ç é—®é¢˜ï¼Œè¯·æäº¤ Issue
- ğŸ’¡ &lt;strong&gt;æå‡ºå»ºè®®&lt;/strong&gt; - å¯¹é¡¹ç›®æœ‰å¥½æƒ³æ³•ï¼Œæ¬¢è¿å‘èµ·è®¨è®º
- ğŸ“ &lt;strong&gt;å®Œå–„å†…å®¹&lt;/strong&gt; - å¸®åŠ©æ”¹è¿›æ•™ç¨‹ï¼Œæäº¤ä½ çš„ Pull Request
- âœï¸ &lt;strong&gt;åˆ†äº«å®è·µ&lt;/strong&gt; - åœ¨&quot;ç¤¾åŒºè´¡çŒ®ç²¾é€‰&quot;ä¸­åˆ†äº«ä½ çš„å­¦ä¹ ç¬”è®°å’Œé¡¹ç›®

## ğŸ™ è‡´è°¢

### æ ¸å¿ƒè´¡çŒ®è€…
- [é™ˆæ€å·-é¡¹ç›®è´Ÿè´£äºº](https://github.com/jjyaoao) (Datawhale æˆå‘˜, å…¨æ–‡å†™ä½œå’Œæ ¡å¯¹)
- [å­™éŸ¬-é¡¹ç›®è´Ÿè´£äºº](https://github.com/fengju0213) (Datawhale æˆå‘˜, ç¬¬ä¹ç« å†…å®¹å’Œæ ¡å¯¹)  
- [å§œèˆ’å‡¡-é¡¹ç›®è´Ÿè´£äºº](https://github.com/Tsumugii24)ï¼ˆDatawhale æˆå‘˜, ç« èŠ‚ä¹ é¢˜è®¾è®¡å’Œæ ¡å¯¹ï¼‰
- [é»„ä½©æ—-Datawhaleæ„å‘æˆå‘˜](https://github.com/HeteroCat) (Agent å¼€å‘å·¥ç¨‹å¸ˆ, ç¬¬äº”ç« å†…å®¹è´¡çŒ®è€…)
- [æ›¾é‘«æ°‘-Agentå·¥ç¨‹å¸ˆ](https://github.com/fancyboi999) (ç‰›å®¢ç§‘æŠ€, ç¬¬åå››ç« æ¡ˆä¾‹å¼€å‘)
- [æœ±ä¿¡å¿ -æŒ‡å¯¼ä¸“å®¶](https://xinzhongzhu.github.io/) (Datawhaleé¦–å¸­ç§‘å­¦å®¶-æµ™æ±Ÿå¸ˆèŒƒå¤§å­¦æ­å·äººå·¥æ™ºèƒ½ç ”ç©¶é™¢æ•™æˆ)
### Extra-Chapter è´¡çŒ®è€…
- [WH](https://github.com/WHQAQ11) (å†…å®¹è´¡çŒ®è€…)
- [å‘¨å¥¥æ°-DWè´¡çŒ®è€…å›¢é˜Ÿ](https://github.com/thunderbolt-fire) (è¥¿å®‰äº¤é€šå¤§å­¦, Extra02 å†…å®¹è´¡çŒ®)
- [å¼ å®¸æ—­-ä¸ªäººå¼€å‘è€…](https://github.com/Tasselszcx)(å¸å›½ç†å·¥å­¦é™¢, Extra03 å†…å®¹è´¡çŒ®)
- [é»„å®æ™—-DWè´¡çŒ®è€…å›¢é˜Ÿ](https://github.com/XiaoMa-PM) (æ·±åœ³å¤§å­¦, Extra04 å†…å®¹è´¡çŒ®)

### ç‰¹åˆ«æ„Ÿè°¢
- æ„Ÿè°¢ [@Sm1les](https://github.com/Sm1les) å¯¹æœ¬é¡¹ç›®çš„å¸®åŠ©ä¸æ”¯æŒ
- æ„Ÿè°¢æ‰€æœ‰ä¸ºæœ¬é¡¹ç›®åšå‡ºè´¡çŒ®çš„å¼€å‘è€…ä»¬ â¤ï¸

&lt;div align=center style=&quot;margin-top: 30px;&quot;&gt;
  &lt;a href=&quot;https://github.com/datawhalechina/Hello-Agents/graphs/contributors&quot;&gt;
    &lt;img src=&quot;https://contrib.rocks/image?repo=datawhalechina/Hello-Agents&quot; /&gt;
  &lt;/a&gt;
&lt;/div&gt;

## Star History

&lt;div align=&#039;center&#039;&gt;
    &lt;img src=&quot;./docs/images/star-history-2025128.png&quot; alt=&quot;Datawhale&quot; width=&quot;90%&quot;&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼&lt;/p&gt;
&lt;/div&gt;

## å…³äº Datawhale

&lt;div align=&#039;center&#039;&gt;
    &lt;img src=&quot;./docs/images/datawhale.png&quot; alt=&quot;Datawhale&quot; width=&quot;30%&quot;&gt;
    &lt;p&gt;æ‰«æäºŒç»´ç å…³æ³¨ Datawhale å…¬ä¼—å·ï¼Œè·å–æ›´å¤šä¼˜è´¨å¼€æºå†…å®¹&lt;/p&gt;
&lt;/div&gt;

---

## ğŸ“œ å¼€æºåè®®

æœ¬ä½œå“é‡‡ç”¨[çŸ¥è¯†å…±äº«ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç›¸åŒæ–¹å¼å…±äº« 4.0 å›½é™…è®¸å¯åè®®](http://creativecommons.org/licenses/by-nc-sa/4.0/)è¿›è¡Œè®¸å¯ã€‚
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/VibeVoice]]></title>
            <link>https://github.com/microsoft/VibeVoice</link>
            <guid>https://github.com/microsoft/VibeVoice</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:28 GMT</pubDate>
            <description><![CDATA[Open-Source Frontier Voice AI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/VibeVoice">microsoft/VibeVoice</a></h1>
            <p>Open-Source Frontier Voice AI</p>
            <p>Language: Python</p>
            <p>Stars: 16,992</p>
            <p>Forks: 1,852</p>
            <p>Stars today: 966 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

## ğŸ™ï¸ VibeVoice: Open-Source Frontier Voice AI
[![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=microsoft)](https://microsoft.github.io/VibeVoice)
[![Hugging Face](https://img.shields.io/badge/HuggingFace-Collection-orange?logo=huggingface)](https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f)
[![Technical Report](https://img.shields.io/badge/Technical-Report-red?logo=adobeacrobatreader)](https://arxiv.org/pdf/2508.19205)


&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;Figures/VibeVoice_logo_white.png&quot;&gt;
  &lt;img src=&quot;Figures/VibeVoice_logo.png&quot; alt=&quot;VibeVoice Logo&quot; width=&quot;300&quot;&gt;
&lt;/picture&gt;
&lt;/div&gt;

&lt;div align=&quot;left&quot;&gt;

&lt;h3&gt;ğŸ“° News&lt;/h3&gt;

&lt;img src=&quot;https://img.shields.io/badge/Status-New-brightgreen?style=flat&quot; alt=&quot;New&quot; /&gt;
&lt;img src=&quot;https://img.shields.io/badge/Feature-Realtime_TTS-blue?style=flat&amp;logo=soundcharts&quot; alt=&quot;Realtime TTS&quot; /&gt;

&lt;strong&gt;2025-12-03: ğŸ“£ We open-sourced &lt;a href=&quot;docs/vibevoice-realtime-0.5b.md&quot;&gt;&lt;strong&gt;VibeVoiceâ€‘Realtimeâ€‘0.5B&lt;/strong&gt;&lt;/a&gt;, a realâ€‘time textâ€‘toâ€‘speech model that supports streaming text input and robust long-form speech generation. Try it on [Colab](https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb).&lt;/strong&gt;

&lt;strong&gt;2025-12-09: ğŸ“£ Weâ€™ve added experimental speakers in nine languages (DE, FR, IT, JP, KR, NL, PL, PT, ES) for explorationâ€”welcome to try them out and share your feedback.&lt;/strong&gt;

To mitigate deepfake risks and ensure low latency for the first speech chunk, voice prompts are provided in an embedded format. For users requiring voice customization, please reach out to our team. We will also be expanding the range of available speakers.
&lt;br&gt;

https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc

&gt; (Launch your own realtime demo via the websocket example in [Usage](docs/vibevoice-realtime-0.5b.md#usage-1-launch-real-time-websocket-demo)).

&lt;/div&gt;

2025-09-05: VibeVoice is an open-source research framework intended to advance collaboration in the speech synthesis community. After release, we discovered instances where the tool was used in ways inconsistent with the stated intent. Since responsible use of AI is one of Microsoftâ€™s guiding principles, we have disabled this repo until we are confident that out-of-scope use is no longer possible.


### Overview

VibeVoice is a novel framework designed for generating **expressive**, **long-form**, **multi-speaker** conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking.

VibeVoice currently includes two model variants:

- **Long-form multi-speaker model**: Synthesizes conversational/single-speaker speech up to **90 minutes** with up to **4 distinct speakers**, surpassing the typical 1â€“2 speaker limits of many prior models.
- **[Realtime streaming TTS model](docs/vibevoice-realtime-0.5b.md)**: Produces initial audible speech in ~**300 ms** and supports **streaming text input** for single-speaker **real-time** speech generation; designed for low-latency generation.

A core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a [next-token diffusion](https://arxiv.org/abs/2412.08635) framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.


&lt;p align=&quot;left&quot;&gt;
  &lt;img src=&quot;Figures/MOS-preference.png&quot; alt=&quot;MOS Preference Results&quot; height=&quot;260px&quot;&gt;
  &lt;img src=&quot;Figures/VibeVoice.jpg&quot; alt=&quot;VibeVoice Overview&quot; height=&quot;250px&quot; style=&quot;margin-right: 10px;&quot;&gt;
&lt;/p&gt;


### ğŸµ Demo Examples


**Video Demo**

We produced this video with [Wan2.2](https://github.com/Wan-Video/Wan2.2). We sincerely appreciate the Wan-Video team for their great work.

**English**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784

&lt;/div&gt;


**Chinese**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f

&lt;/div&gt;

**Cross-Lingual**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722

&lt;/div&gt;

**Spontaneous Singing**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730

&lt;/div&gt;


**Long Conversation with 4 people**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727

&lt;/div&gt;

For more examples, see the [Project Page](https://microsoft.github.io/VibeVoice).



## Risks and limitations

While efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release).
Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.

English and Chinese only: Transcripts in languages other than English or Chinese may result in unexpected audio outputs.

Non-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects.

Overlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations.

We do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.

## Star History

![Star History Chart](https://api.star-history.com/svg?repos=Microsoft/vibevoice&amp;type=date&amp;legend=top-left)</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[666ghj/BettaFish]]></title>
            <link>https://github.com/666ghj/BettaFish</link>
            <guid>https://github.com/666ghj/BettaFish</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[å¾®èˆ†ï¼šäººäººå¯ç”¨çš„å¤šAgentèˆ†æƒ…åˆ†æåŠ©æ‰‹ï¼Œæ‰“ç ´ä¿¡æ¯èŒ§æˆ¿ï¼Œè¿˜åŸèˆ†æƒ…åŸè²Œï¼Œé¢„æµ‹æœªæ¥èµ°å‘ï¼Œè¾…åŠ©å†³ç­–ï¼ä»0å®ç°ï¼Œä¸ä¾èµ–ä»»ä½•æ¡†æ¶ã€‚]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/666ghj/BettaFish">666ghj/BettaFish</a></h1>
            <p>å¾®èˆ†ï¼šäººäººå¯ç”¨çš„å¤šAgentèˆ†æƒ…åˆ†æåŠ©æ‰‹ï¼Œæ‰“ç ´ä¿¡æ¯èŒ§æˆ¿ï¼Œè¿˜åŸèˆ†æƒ…åŸè²Œï¼Œé¢„æµ‹æœªæ¥èµ°å‘ï¼Œè¾…åŠ©å†³ç­–ï¼ä»0å®ç°ï¼Œä¸ä¾èµ–ä»»ä½•æ¡†æ¶ã€‚</p>
            <p>Language: Python</p>
            <p>Stars: 32,303</p>
            <p>Forks: 6,210</p>
            <p>Stars today: 326 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;static/image/logo_compressed.png&quot; alt=&quot;BettaFish Logo&quot; width=&quot;100%&quot;&gt;

&lt;a href=&quot;https://trendshift.io/repositories/15286&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/15286&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;a href=&quot;https://aihubmix.com/?aff=8Ds9&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_aihubmix.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;&amp;ensp;
&lt;a href=&quot;https://lioncc.ai/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_loincc.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;&amp;ensp;
&lt;a href=&quot;https://share.302.ai/P66Qe3&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_302ai.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;

&lt;a href=&quot;https://open.anspire.cn/?share_code=3E1FUOUH&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_anspire.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;50&quot;/&gt;&lt;/a&gt;

[![GitHub Stars](https://img.shields.io/github/stars/666ghj/BettaFish?style=flat-square)](https://github.com/666ghj/BettaFish/stargazers)
[![GitHub Watchers](https://img.shields.io/github/watchers/666ghj/BettaFish?style=flat-square)](https://github.com/666ghj/BettaFish/watchers)
[![GitHub Forks](https://img.shields.io/github/forks/666ghj/BettaFish?style=flat-square)](https://github.com/666ghj/BettaFish/network)
[![GitHub Issues](https://img.shields.io/github/issues/666ghj/BettaFish?style=flat-square)](https://github.com/666ghj/BettaFish/issues)
[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/666ghj/BettaFish?style=flat-square)](https://github.com/666ghj/BettaFish/pulls)

[![GitHub License](https://img.shields.io/github/license/666ghj/BettaFish?style=flat-square)](https://github.com/666ghj/BettaFish/blob/main/LICENSE)
[![Version](https://img.shields.io/badge/version-v1.2.1-green.svg?style=flat-square)](https://github.com/666ghj/BettaFish)
[![Docker](https://img.shields.io/badge/Docker-Build-2496ED?style=flat-square&amp;logo=docker&amp;logoColor=white)](https://hub.docker.com/)



[English](./README-EN.md) | [ä¸­æ–‡æ–‡æ¡£](./README.md)

&lt;/div&gt;

## âš¡ é¡¹ç›®æ¦‚è¿°

â€œ**å¾®èˆ†**â€ æ˜¯ä¸€ä¸ªä»0å®ç°çš„åˆ›æ–°å‹ å¤šæ™ºèƒ½ä½“ èˆ†æƒ…åˆ†æç³»ç»Ÿï¼Œå¸®åŠ©å¤§å®¶ç ´é™¤ä¿¡æ¯èŒ§æˆ¿ï¼Œè¿˜åŸèˆ†æƒ…åŸè²Œï¼Œé¢„æµ‹æœªæ¥èµ°å‘ï¼Œè¾…åŠ©å†³ç­–ã€‚ç”¨æˆ·åªéœ€åƒèŠå¤©ä¸€æ ·æå‡ºåˆ†æéœ€æ±‚ï¼Œæ™ºèƒ½ä½“å¼€å§‹å…¨è‡ªåŠ¨åˆ†æ å›½å†…å¤–30+ä¸»æµç¤¾åª’ ä¸ æ•°ç™¾ä¸‡æ¡å¤§ä¼—è¯„è®ºã€‚

&gt; â€œå¾®èˆ†â€è°éŸ³â€œå¾®é±¼â€ï¼ŒBettaFishæ˜¯ä¸€ç§ä½“å‹å¾ˆå°ä½†éå¸¸å¥½æ–—ã€æ¼‚äº®çš„é±¼ï¼Œå®ƒè±¡å¾ç€â€œå°è€Œå¼ºå¤§ï¼Œä¸ç•æŒ‘æˆ˜â€

æŸ¥çœ‹ç³»ç»Ÿä»¥â€œæ­¦æ±‰å¤§å­¦èˆ†æƒ…â€ä¸ºä¾‹ï¼Œç”Ÿæˆçš„ç ”ç©¶æŠ¥å‘Šï¼š[æ­¦æ±‰å¤§å­¦å“ç‰Œå£°èª‰æ·±åº¦åˆ†ææŠ¥å‘Š](./final_reports/final_report__20250827_131630.html)

æŸ¥çœ‹ç³»ç»Ÿä»¥â€œæ­¦æ±‰å¤§å­¦èˆ†æƒ…â€ä¸ºä¾‹ï¼Œä¸€æ¬¡å®Œæ•´è¿è¡Œçš„è§†é¢‘ï¼š[è§†é¢‘-æ­¦æ±‰å¤§å­¦å“ç‰Œå£°èª‰æ·±åº¦åˆ†ææŠ¥å‘Š](https://www.bilibili.com/video/BV1TH1WBxEWN/?vd_source=da3512187e242ce17dceee4c537ec7a6#reply279744466833)

ä¸ä»…ä»…ä½“ç°åœ¨æŠ¥å‘Šè´¨é‡ä¸Šï¼Œç›¸æ¯”åŒç±»äº§å“ï¼Œæˆ‘ä»¬æ‹¥æœ‰ğŸš€å…­å¤§ä¼˜åŠ¿ï¼š

1. **AIé©±åŠ¨çš„å…¨åŸŸç›‘æ§**ï¼šAIçˆ¬è™«é›†ç¾¤7x24å°æ—¶ä¸é—´æ–­ä½œä¸šï¼Œå…¨é¢è¦†ç›–å¾®åšã€å°çº¢ä¹¦ã€æŠ–éŸ³ã€å¿«æ‰‹ç­‰10+å›½å†…å¤–å…³é”®ç¤¾åª’ã€‚ä¸ä»…å®æ—¶æ•è·çƒ­ç‚¹å†…å®¹ï¼Œæ›´èƒ½ä¸‹é’»è‡³æµ·é‡ç”¨æˆ·è¯„è®ºï¼Œè®©æ‚¨å¬åˆ°æœ€çœŸå®ã€æœ€å¹¿æ³›çš„å¤§ä¼—å£°éŸ³ã€‚

2. **è¶…è¶ŠLLMçš„å¤åˆåˆ†æå¼•æ“**ï¼šæˆ‘ä»¬ä¸ä»…ä¾èµ–è®¾è®¡çš„5ç±»ä¸“ä¸šAgentï¼Œæ›´èåˆäº†å¾®è°ƒæ¨¡å‹ã€ç»Ÿè®¡æ¨¡å‹ç­‰ä¸­é—´ä»¶ã€‚é€šè¿‡å¤šæ¨¡å‹ååŒå·¥ä½œï¼Œç¡®ä¿äº†åˆ†æç»“æœçš„æ·±åº¦ã€å‡†åº¦ä¸å¤šç»´è§†è§’ã€‚

3. **å¼ºå¤§çš„å¤šæ¨¡æ€èƒ½åŠ›**ï¼šçªç ´å›¾æ–‡é™åˆ¶ï¼Œèƒ½æ·±åº¦è§£ææŠ–éŸ³ã€å¿«æ‰‹ç­‰çŸ­è§†é¢‘å†…å®¹ï¼Œå¹¶ç²¾å‡†æå–ç°ä»£æœç´¢å¼•æ“ä¸­çš„å¤©æ°”ã€æ—¥å†ã€è‚¡ç¥¨ç­‰ç»“æ„åŒ–å¤šæ¨¡æ€ä¿¡æ¯å¡ç‰‡ï¼Œè®©æ‚¨å…¨é¢æŒæ¡èˆ†æƒ…åŠ¨æ€ã€‚

4. **Agentâ€œè®ºå›â€åä½œæœºåˆ¶**ï¼šä¸ºä¸åŒAgentèµ‹äºˆç‹¬ç‰¹çš„å·¥å…·é›†ä¸æ€ç»´æ¨¡å¼ï¼Œå¼•å…¥è¾©è®ºä¸»æŒäººæ¨¡å‹ï¼Œé€šè¿‡â€œè®ºå›â€æœºåˆ¶è¿›è¡Œé“¾å¼æ€ç»´ç¢°æ’ä¸è¾©è®ºã€‚è¿™ä¸ä»…é¿å…äº†å•ä¸€æ¨¡å‹çš„æ€ç»´å±€é™ä¸äº¤æµå¯¼è‡´çš„åŒè´¨åŒ–ï¼Œæ›´å‚¬ç”Ÿå‡ºæ›´é«˜è´¨é‡çš„é›†ä½“æ™ºèƒ½ä¸å†³ç­–æ”¯æŒã€‚

5. **å…¬ç§åŸŸæ•°æ®æ— ç¼èåˆ**ï¼šå¹³å°ä¸ä»…åˆ†æå…¬å¼€èˆ†æƒ…ï¼Œè¿˜æä¾›é«˜å®‰å…¨æ€§çš„æ¥å£ï¼Œæ”¯æŒæ‚¨å°†å†…éƒ¨ä¸šåŠ¡æ•°æ®åº“ä¸èˆ†æƒ…æ•°æ®æ— ç¼é›†æˆã€‚æ‰“é€šæ•°æ®å£å’ï¼Œä¸ºå‚ç›´ä¸šåŠ¡æä¾›â€œå¤–éƒ¨è¶‹åŠ¿+å†…éƒ¨æ´å¯Ÿâ€çš„å¼ºå¤§åˆ†æèƒ½åŠ›ã€‚

6. **è½»é‡åŒ–ä¸é«˜æ‰©å±•æ€§æ¡†æ¶**ï¼šåŸºäºçº¯Pythonæ¨¡å—åŒ–è®¾è®¡ï¼Œå®ç°è½»é‡åŒ–ã€ä¸€é”®å¼éƒ¨ç½²ã€‚ä»£ç ç»“æ„æ¸…æ™°ï¼Œå¼€å‘è€…å¯è½»æ¾é›†æˆè‡ªå®šä¹‰æ¨¡å‹ä¸ä¸šåŠ¡é€»è¾‘ï¼Œå®ç°å¹³å°çš„å¿«é€Ÿæ‰©å±•ä¸æ·±åº¦å®šåˆ¶ã€‚

**å§‹äºèˆ†æƒ…ï¼Œè€Œä¸æ­¢äºèˆ†æƒ…**ã€‚â€œå¾®èˆ†â€çš„ç›®æ ‡ï¼Œæ˜¯æˆä¸ºé©±åŠ¨ä¸€åˆ‡ä¸šåŠ¡åœºæ™¯çš„ç®€æ´é€šç”¨çš„æ•°æ®åˆ†æå¼•æ“ã€‚

&gt; ä¸¾ä¸ªä¾‹å­. ä½ åªéœ€ç®€å•ä¿®æ”¹Agentå·¥å…·é›†çš„apiå‚æ•°ä¸promptï¼Œå°±å¯ä»¥æŠŠä»–å˜æˆä¸€ä¸ªé‡‘èé¢†åŸŸçš„å¸‚åœºåˆ†æç³»ç»Ÿ
&gt;
&gt; é™„ä¸€ä¸ªæ¯”è¾ƒæ´»è·ƒçš„Lç«™é¡¹ç›®è®¨è®ºå¸–ï¼šhttps://linux.do/t/topic/1009280
&gt;
&gt; æŸ¥çœ‹Lç«™ä½¬å‹åšçš„æµ‹è¯„ [å¼€æºé¡¹ç›®(å¾®èˆ†)ä¸manus|minimax|ChatGPTå¯¹æ¯”](https://linux.do/t/topic/1148040)

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;static/image/system_schematic.png&quot; alt=&quot;banner&quot; width=&quot;800&quot;&gt;

å‘Šåˆ«ä¼ ç»Ÿçš„æ•°æ®çœ‹æ¿ï¼Œåœ¨â€œå¾®èˆ†â€ï¼Œä¸€åˆ‡ç”±ä¸€ä¸ªç®€å•çš„é—®é¢˜å¼€å§‹ï¼Œæ‚¨åªéœ€åƒå¯¹è¯ä¸€æ ·ï¼Œæå‡ºæ‚¨çš„åˆ†æéœ€æ±‚
&lt;/div&gt;

## ğŸª„ èµåŠ©å•†

LLMæ¨¡å‹APIèµåŠ©ï¼š&lt;a href=&quot;https://aihubmix.com/?aff=8Ds9&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_aihubmix.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;

&lt;details&gt;
&lt;summary&gt;æœ‰èµåŠ©LLMç®—åŠ›ç¦åˆ©ï¼ç¼–ç¨‹æ‹¼è½¦codecodex.aiï¼›ç¼–ç¨‹ç®—åŠ›VibeCodingAPI.aiï¼š&lt;/a&gt;&lt;span style=&quot;margin-left: 10px&quot;&gt;&lt;a href=&quot;https://codecodex.ai/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_loincc.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;&lt;/summary&gt;

1. æ‰€ç½—é—¨åšå®¢LionCC.aiå·²æ›´æ–°ã€ŠBettaFish å¾®èˆ†ç³»ç»Ÿ - LionCC API éƒ¨ç½²é…ç½®å®Œå…¨æŒ‡å—ã€‹æ­£åœ¨äºŒå¼€ä¼˜åŒ–ä¸€é”®éƒ¨ç½²å’Œäº‘æœåŠ¡å™¨è°ƒç”¨æ–¹æ¡ˆã€‚
2. VibeCodingapi.aiç‹®å­ç®—åŠ›å¹³å°å·²ç»é€‚é…ã€ŠBettaFish å¾®èˆ†ç³»ç»Ÿã€‹æ‰€æœ‰LLMæ¨¡å‹å«claude codeå’Œopenai codexå’Œgemini cliç¼–ç¨‹å¼€å‘ä¸‰å·¨å¤´ç®—åŠ›ã€‚é¢åº¦ä»·æ ¼ï¼Œåªè¦ä¸€æ¯”ä¸€ï¼ˆ100å…ƒç­‰äº100ç¾åˆ€é¢åº¦ï¼‰
3. Codecodex.aiç‹®å­ç¼–ç¨‹æ‹¼è½¦ç³»ç»Ÿï¼Œå·²å®ç°æ— IPé—¨æ§›ç»•è¿‡claude codeå’Œopenai codexå°é”ï¼ŒæŒ‰å®˜æ–¹éƒ¨ç½²æ•™ç¨‹ååˆ‡æ¢BASE_URLè°ƒç”¨åœ°å€å’ŒToken keyè°ƒç”¨å¯†é’¥å³å¯ä½¿ç”¨æœ€å¼ºç¼–ç¨‹æ¨¡å‹ã€‚

æ‰€ç½—é—¨LionCCèµåŠ©BettaFish å¾®èˆ†ç¦åˆ©ï¼šæ‰“å¼€codecodex.aiç‹®å­ç¼–ç¨‹é¢‘é“æ‰«ç åŠ å…¥å¾®ä¿¡ç¤¾ç¾¤ï¼Œæ³¨å†ŒVibeCodingapi.aiç‹®å­ç®—åŠ›ï¼Œç»Ÿä¸€é€20åˆ€APIé¢åº¦ï¼ˆä»…é™å‰ä¸€åƒåï¼‰
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;æŒ‰ç”¨é‡ä»˜è´¹çš„ä¼ä¸šçº§AIèµ„æºå¹³å°ï¼Œæä¾›å¸‚åœºä¸Šå…¨é¢çš„AIæ¨¡å‹å’ŒAPIï¼Œä»¥åŠå¤šç§åœ¨çº¿AIåº”ç”¨ï¼š&lt;/a&gt;&lt;span style=&quot;margin-left: 10px&quot;&gt;&lt;a href=&quot;https://share.302.ai/P66Qe3&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_302ai.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;&lt;/summary&gt;
&lt;img src=&quot;static/image/banner_302ai_ch.jpg&quot; alt=&quot;banner&quot;&gt;302.AIæ˜¯ä¸€ä¸ªæŒ‰ç”¨é‡ä»˜è´¹çš„ä¼ä¸šçº§AIèµ„æºå¹³å°ï¼Œæä¾›å¸‚åœºä¸Šæœ€æ–°ã€æœ€å…¨é¢çš„AIæ¨¡å‹å’ŒAPIï¼Œä»¥åŠå¤šç§å¼€ç®±å³ç”¨çš„åœ¨çº¿AIåº”ç”¨ã€‚
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;AIè”ç½‘æœç´¢ã€æ–‡ä»¶è§£æåŠç½‘é¡µå†…å®¹æŠ“å–ç­‰æ™ºèƒ½ä½“æ ¸å¿ƒèƒ½åŠ›æä¾›å•†ï¼š&lt;/a&gt;&lt;span style=&quot;margin-left: 10px&quot;&gt;&lt;a href=&quot;https://open.anspire.cn/?share_code=3E1FUOUH&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_anspire.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;50&quot;/&gt;&lt;/a&gt;&lt;/summary&gt;
å®‰æ€æ´¾å¼€æ”¾å¹³å°(Anspire Open)æ˜¯é¢å‘æ™ºèƒ½ä½“æ—¶ä»£çš„é¢†å…ˆçš„åŸºç¡€è®¾æ–½æä¾›å•†ã€‚æˆ‘ä»¬ä¸ºå¼€å‘è€…æä¾›æ„å»ºå¼ºå¤§æ™ºèƒ½ä½“æ‰€éœ€çš„æ ¸å¿ƒèƒ½åŠ›æ ˆï¼Œç°å·²ä¸Šçº¿AIè”ç½‘æœç´¢ã€å¤šç‰ˆæœ¬ï¼Œæå…·ç«äº‰åŠ›çš„ä»·æ ¼ã€‘ã€æ–‡ä»¶è§£æã€é™å…ã€‘åŠç½‘é¡µå†…å®¹æŠ“å–ã€é™å…ã€‘ã€äº‘ç«¯æµè§ˆå™¨è‡ªåŠ¨åŒ–ï¼ˆAnspire Browser Agentï¼‰ã€å†…æµ‹ã€‘ã€å¤šè½®æ”¹å†™ç­‰æœåŠ¡ï¼ŒæŒç»­ä¸ºæ™ºèƒ½ä½“è¿æ¥å¹¶æ“ä½œå¤æ‚çš„æ•°å­—ä¸–ç•Œæä¾›åšå®åŸºç¡€ã€‚å¯æ— ç¼é›†æˆè‡³Difyã€Cozeã€å…ƒå™¨ç­‰ä¸»æµæ™ºèƒ½ä½“å¹³å°ã€‚é€šè¿‡é€æ˜ç‚¹æ•°è®¡è´¹ä½“ç³»ä¸æ¨¡å—åŒ–è®¾è®¡ï¼Œä¸ºä¼ä¸šæä¾›é«˜æ•ˆã€ä½æˆæœ¬çš„å®šåˆ¶åŒ–æ”¯æŒï¼ŒåŠ é€Ÿæ™ºèƒ½åŒ–å‡çº§è¿›ç¨‹ã€‚
&lt;/details&gt;

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### æ•´ä½“æ¶æ„å›¾

**Insight Agent** ç§æœ‰æ•°æ®åº“æŒ–æ˜ï¼šç§æœ‰èˆ†æƒ…æ•°æ®åº“æ·±åº¦åˆ†æAIä»£ç†

**Media Agent** å¤šæ¨¡æ€å†…å®¹åˆ†æï¼šå…·å¤‡å¼ºå¤§å¤šæ¨¡æ€èƒ½åŠ›çš„AIä»£ç†

**Query Agent** ç²¾å‡†ä¿¡æ¯æœç´¢ï¼šå…·å¤‡å›½å†…å¤–ç½‘é¡µæœç´¢èƒ½åŠ›çš„AIä»£ç†

**Report Agent** æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆï¼šå†…ç½®æ¨¡æ¿çš„å¤šè½®æŠ¥å‘Šç”ŸæˆAIä»£ç†

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;static/image/framework.png&quot; alt=&quot;banner&quot; width=&quot;800&quot;&gt;
&lt;/div&gt;

### ä¸€æ¬¡å®Œæ•´åˆ†ææµç¨‹

| æ­¥éª¤ | é˜¶æ®µåç§° | ä¸»è¦æ“ä½œ | å‚ä¸ç»„ä»¶ | å¾ªç¯ç‰¹æ€§ |
|------|----------|----------|----------|----------|
| 1 | ç”¨æˆ·æé—® | Flaskä¸»åº”ç”¨æ¥æ”¶æŸ¥è¯¢ | Flaskä¸»åº”ç”¨ | - |
| 2 | å¹¶è¡Œå¯åŠ¨ | ä¸‰ä¸ªAgentåŒæ—¶å¼€å§‹å·¥ä½œ | Query Agentã€Media Agentã€Insight Agent | - |
| 3 | åˆæ­¥åˆ†æ | å„Agentä½¿ç”¨ä¸“å±å·¥å…·è¿›è¡Œæ¦‚è§ˆæœç´¢ | å„Agent + ä¸“å±å·¥å…·é›† | - |
| 4 | ç­–ç•¥åˆ¶å®š | åŸºäºåˆæ­¥ç»“æœåˆ¶å®šåˆ†å—ç ”ç©¶ç­–ç•¥ | å„Agentå†…éƒ¨å†³ç­–æ¨¡å— | - |
| 5-N | **å¾ªç¯é˜¶æ®µ** | **è®ºå›åä½œ + æ·±åº¦ç ”ç©¶** | **ForumEngine + æ‰€æœ‰Agent** | **å¤šè½®å¾ªç¯** |
| 5.1 | æ·±åº¦ç ”ç©¶ | å„AgentåŸºäºè®ºå›ä¸»æŒäººå¼•å¯¼è¿›è¡Œä¸“é¡¹æœç´¢ | å„Agent + åæ€æœºåˆ¶ + è®ºå›å¼•å¯¼ | æ¯è½®å¾ªç¯ |
| 5.2 | è®ºå›åä½œ | ForumEngineç›‘æ§Agentå‘è¨€å¹¶ç”Ÿæˆä¸»æŒäººå¼•å¯¼ | ForumEngine + LLMä¸»æŒäºº | æ¯è½®å¾ªç¯ |
| 5.3 | äº¤æµèåˆ | å„Agentæ ¹æ®è®¨è®ºè°ƒæ•´ç ”ç©¶æ–¹å‘ | å„Agent + forum_readerå·¥å…· | æ¯è½®å¾ªç¯ |
| N+1 | ç»“æœæ•´åˆ | Report Agentæ”¶é›†æ‰€æœ‰åˆ†æç»“æœå’Œè®ºå›å†…å®¹ | Report Agent | - |
| N+2 | IRä¸­é—´è¡¨ç¤º | åŠ¨æ€é€‰æ‹©æ¨¡æ¿å’Œæ ·å¼ï¼Œå¤šè½®ç”Ÿæˆå…ƒæ•°æ®ï¼Œè£…è®¢ä¸ºIRä¸­é—´è¡¨ç¤º | Report Agent + æ¨¡æ¿å¼•æ“ | - |
| N+3 | æŠ¥å‘Šç”Ÿæˆ | åˆ†å—è¿›è¡Œè´¨é‡æ£€æµ‹ï¼ŒåŸºäºIRæ¸²æŸ“æˆäº¤äº’å¼ HTML æŠ¥å‘Š | Report Agent + è£…è®¢å¼•æ“ | - |

### é¡¹ç›®ä»£ç ç»“æ„æ ‘

```
BettaFish/
â”œâ”€â”€ QueryEngine/                            # å›½å†…å¤–æ–°é—»å¹¿åº¦æœç´¢Agent
â”‚   â”œâ”€â”€ agent.py                            # Agentä¸»é€»è¾‘ï¼Œåè°ƒæœç´¢ä¸åˆ†ææµç¨‹
â”‚   â”œâ”€â”€ llms/                               # LLMæ¥å£å°è£…
â”‚   â”œâ”€â”€ nodes/                              # å¤„ç†èŠ‚ç‚¹ï¼šæœç´¢ã€æ ¼å¼åŒ–ã€æ€»ç»“ç­‰
â”‚   â”œâ”€â”€ tools/                              # å›½å†…å¤–æ–°é—»æœç´¢å·¥å…·é›†
â”‚   â”œâ”€â”€ utils/                              # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ state/                              # çŠ¶æ€ç®¡ç†
â”‚   â”œâ”€â”€ prompts/                            # æç¤ºè¯æ¨¡æ¿
â”‚   â””â”€â”€ ...
â”œâ”€â”€ MediaEngine/                            # å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£Agent
â”‚   â”œâ”€â”€ agent.py                            # Agentä¸»é€»è¾‘ï¼Œå¤„ç†è§†é¢‘/å›¾ç‰‡ç­‰å¤šæ¨¡æ€å†…å®¹
â”‚   â”œâ”€â”€ llms/                               # LLMæ¥å£å°è£…
â”‚   â”œâ”€â”€ nodes/                              # å¤„ç†èŠ‚ç‚¹ï¼šæœç´¢ã€æ ¼å¼åŒ–ã€æ€»ç»“ç­‰
â”‚   â”œâ”€â”€ tools/                              # å¤šæ¨¡æ€æœç´¢å·¥å…·é›†
â”‚   â”œâ”€â”€ utils/                              # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ state/                              # çŠ¶æ€ç®¡ç†
â”‚   â”œâ”€â”€ prompts/                            # æç¤ºè¯æ¨¡æ¿
â”‚   â””â”€â”€ ...
â”œâ”€â”€ InsightEngine/                          # ç§æœ‰æ•°æ®åº“æŒ–æ˜Agent
â”‚   â”œâ”€â”€ agent.py                            # Agentä¸»é€»è¾‘ï¼Œåè°ƒæ•°æ®åº“æŸ¥è¯¢ä¸åˆ†æ
â”‚   â”œâ”€â”€ llms/                               # LLMæ¥å£å°è£…
â”‚   â”‚   â””â”€â”€ base.py                         # ç»Ÿä¸€çš„OpenAIå…¼å®¹å®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ nodes/                              # å¤„ç†èŠ‚ç‚¹ï¼šæœç´¢ã€æ ¼å¼åŒ–ã€æ€»ç»“ç­‰
â”‚   â”‚   â”œâ”€â”€ base_node.py                    # åŸºç¡€èŠ‚ç‚¹ç±»
â”‚   â”‚   â”œâ”€â”€ search_node.py                  # æœç´¢èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ formatting_node.py              # æ ¼å¼åŒ–èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ report_structure_node.py        # æŠ¥å‘Šç»“æ„èŠ‚ç‚¹
â”‚   â”‚   â””â”€â”€ summary_node.py                 # æ€»ç»“èŠ‚ç‚¹
â”‚   â”œâ”€â”€ tools/                              # æ•°æ®åº“æŸ¥è¯¢å’Œåˆ†æå·¥å…·é›†
â”‚   â”‚   â”œâ”€â”€ keyword_optimizer.py            # Qwenå…³é”®è¯ä¼˜åŒ–ä¸­é—´ä»¶
â”‚   â”‚   â”œâ”€â”€ search.py                       # æ•°æ®åº“æ“ä½œå·¥å…·é›†ï¼ˆè¯é¢˜æœç´¢ã€è¯„è®ºè·å–ç­‰ï¼‰
â”‚   â”‚   â””â”€â”€ sentiment_analyzer.py           # æƒ…æ„Ÿåˆ†æé›†æˆå·¥å…·
â”‚   â”œâ”€â”€ utils/                              # å·¥å…·å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ config.py                       # é…ç½®ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ db.py                           # SQLAlchemyå¼‚æ­¥å¼•æ“ä¸åªè¯»æŸ¥è¯¢å°è£…
â”‚   â”‚   â””â”€â”€ text_processing.py              # æ–‡æœ¬å¤„ç†å·¥å…·
â”‚   â”œâ”€â”€ state/                              # çŠ¶æ€ç®¡ç†
â”‚   â”‚   â””â”€â”€ state.py                        # AgentçŠ¶æ€å®šä¹‰
â”‚   â”œâ”€â”€ prompts/                            # æç¤ºè¯æ¨¡æ¿
â”‚   â”‚   â””â”€â”€ prompts.py                      # å„ç±»æç¤ºè¯
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ReportEngine/                           # å¤šè½®æŠ¥å‘Šç”ŸæˆAgent
â”‚   â”œâ”€â”€ agent.py                            # æ€»è°ƒåº¦å™¨ï¼šæ¨¡æ¿é€‰æ‹©â†’å¸ƒå±€â†’ç¯‡å¹…â†’ç« èŠ‚â†’æ¸²æŸ“
â”‚   â”œâ”€â”€ flask_interface.py                  # Flask/SSEå…¥å£ï¼Œç®¡ç†ä»»åŠ¡æ’é˜Ÿä¸æµå¼äº‹ä»¶
â”‚   â”œâ”€â”€ llms/                               # OpenAIå…¼å®¹LLMå°è£…
â”‚   â”‚   â””â”€â”€ base.py                         # ç»Ÿä¸€çš„æµå¼/é‡è¯•å®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ core/                               # æ ¸å¿ƒåŠŸèƒ½ï¼šæ¨¡æ¿è§£æã€ç« èŠ‚å­˜å‚¨ã€æ–‡æ¡£è£…è®¢
â”‚   â”‚   â”œâ”€â”€ template_parser.py              # Markdownæ¨¡æ¿åˆ‡ç‰‡ä¸slugç”Ÿæˆ
â”‚   â”‚   â”œâ”€â”€ chapter_storage.py              # ç« èŠ‚runç›®å½•ã€manifestä¸rawæµå†™å…¥
â”‚   â”‚   â””â”€â”€ stitcher.py                     # Document IRè£…è®¢å™¨ï¼Œè¡¥é½é”šç‚¹/å…ƒæ•°æ®
â”‚   â”œâ”€â”€ ir/                                 # æŠ¥å‘Šä¸­é—´è¡¨ç¤ºï¼ˆIRï¼‰å¥‘çº¦ä¸æ ¡éªŒ
â”‚   â”‚   â”œâ”€â”€ schema.py                       # å—/æ ‡è®°Schemaå¸¸é‡å®šä¹‰
â”‚   â”‚   â””â”€â”€ validator.py                    # ç« èŠ‚JSONç»“æ„æ ¡éªŒå™¨
â”‚   â”œâ”€â”€ nodes/                              # å…¨æµç¨‹æ¨ç†èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ base_node.py                    # èŠ‚ç‚¹åŸºç±»+æ—¥å¿—/çŠ¶æ€é’©å­
â”‚   â”‚   â”œâ”€â”€ template_selection_node.py      # æ¨¡æ¿å€™é€‰æ”¶é›†ä¸LLMç­›é€‰
â”‚   â”‚   â”œâ”€â”€ document_layout_node.py         # æ ‡é¢˜/ç›®å½•/ä¸»é¢˜è®¾è®¡
â”‚   â”‚   â”œâ”€â”€ word_budget_node.py             # ç¯‡å¹…è§„åˆ’ä¸ç« èŠ‚æŒ‡ä»¤ç”Ÿæˆ
â”‚   â”‚   â””â”€â”€ chapter_generation_node.py      # ç« èŠ‚çº§JSONç”Ÿæˆ+æ ¡éªŒ
â”‚   â”œâ”€â”€ prompts/                            # æç¤ºè¯åº“ä¸Schemaè¯´æ˜
â”‚   â”‚   â””â”€â”€ prompts.py                      # æ¨¡æ¿é€‰æ‹©/å¸ƒå±€/ç¯‡å¹…/ç« èŠ‚æç¤ºè¯
â”‚   â”œâ”€â”€ renderers/                          # IRæ¸²æŸ“å™¨
â”‚   â”‚   â”œâ”€â”€ html_renderer.py                # Document IRâ†’äº¤äº’å¼HTML
â”‚   â”‚   â”œâ”€â”€ pdf_renderer.py                 # HTMLâ†’PDFå¯¼å‡ºï¼ˆWeasyPrintï¼‰
â”‚   â”‚   â”œâ”€â”€ pdf_layout_optimizer.py         # PDFå¸ƒå±€ä¼˜åŒ–å™¨
â”‚   â”‚   â””â”€â”€ chart_to_svg.py                 # å›¾è¡¨è½¬SVGå·¥å…·
â”‚   â”œâ”€â”€ state/                              # ä»»åŠ¡/å…ƒæ•°æ®çŠ¶æ€æ¨¡å‹
â”‚   â”‚   â””â”€â”€ state.py                        # ReportStateä¸åºåˆ—åŒ–å·¥å…·
â”‚   â”œâ”€â”€ utils/                              # é…ç½®ä¸è¾…åŠ©å·¥å…·
â”‚   â”‚   â”œâ”€â”€ config.py                       # Pydantic Settingsä¸æ‰“å°åŠ©æ‰‹
â”‚   â”‚   â”œâ”€â”€ dependency_check.py             # ä¾èµ–æ£€æŸ¥å·¥å…·
â”‚   â”‚   â”œâ”€â”€ json_parser.py                  # JSONè§£æå·¥å…·
â”‚   â”‚   â”œâ”€â”€ chart_validator.py              # å›¾è¡¨æ ¡éªŒå·¥å…·
â”‚   â”‚   â””â”€â”€ chart_repair_api.py             # å›¾è¡¨ä¿®å¤API
â”‚   â”œâ”€â”€ report_template/                    # Markdownæ¨¡æ¿åº“
â”‚   â”‚   â”œâ”€â”€ ä¼ä¸šå“ç‰Œå£°èª‰åˆ†ææŠ¥å‘Š.md
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ForumEngine/                            # è®ºå›å¼•æ“ï¼šAgentåä½œæœºåˆ¶
â”‚   â”œâ”€â”€ monitor.py                          # æ—¥å¿—ç›‘æ§å’Œè®ºå›ç®¡ç†æ ¸å¿ƒ
â”‚   â”œâ”€â”€ llm_host.py                         # è®ºå›ä¸»æŒäººLLMæ¨¡å—
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ MindSpider/                             # ç¤¾äº¤åª’ä½“çˆ¬è™«ç³»ç»Ÿ
â”‚   â”œâ”€â”€ main.py                             # çˆ¬è™«ä¸»ç¨‹åºå…¥å£
â”‚   â”œâ”€â”€ config.py                           # çˆ¬è™«é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ BroadTopicExtraction/               # è¯é¢˜æå–æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ main.py                         # è¯é¢˜æå–ä¸»ç¨‹åº
â”‚   â”‚   â”œâ”€â”€ database_manager.py             # æ•°æ®åº“ç®¡ç†å™¨
â”‚   â”‚   â”œâ”€â”€ get_today_news.py               # ä»Šæ—¥æ–°é—»è·å–
â”‚   â”‚   â””â”€â”€ topic_extractor.py              # è¯é¢˜æå–å™¨
â”‚   â”œâ”€â”€ DeepSentimentCrawling/              # æ·±åº¦èˆ†æƒ…çˆ¬å–æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ main.py                         # æ·±åº¦çˆ¬å–ä¸»ç¨‹åº
â”‚   â”‚   â”œâ”€â”€ keyword_manager.py              # å…³é”®è¯ç®¡ç†å™¨
â”‚   â”‚   â”œâ”€â”€ platform_crawler.py             # å¹³å°çˆ¬è™«ç®¡ç†
â”‚   â”‚   â””â”€â”€ MediaCrawler/                   # ç¤¾åª’çˆ¬è™«æ ¸å¿ƒ
â”‚   â”‚       â”œâ”€â”€ main.py
â”‚   â”‚       â”œâ”€â”€ config/                     # å„å¹³å°é…ç½®
â”‚   â”‚       â”œâ”€â”€ media_platform/             # å„å¹³å°çˆ¬è™«å®ç°
â”‚   â”‚       â””â”€â”€ ...
â”‚   â””â”€â”€ schema/                             # æ•°æ®åº“ç»“æ„å®šä¹‰
â”‚       â”œâ”€â”€ db_manager.py                   # æ•°æ®åº“ç®¡ç†å™¨
â”‚       â”œâ”€â”€ init_database.py                # æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬
â”‚       â”œâ”€â”€ mindspider_tables.sql           # æ•°æ®åº“è¡¨ç»“æ„SQL
â”‚       â”œâ”€â”€ models_bigdata.py               # å¤§è§„æ¨¡åª’ä½“èˆ†æƒ…è¡¨çš„SQLAlchemyæ˜ å°„
â”‚       â””â”€â”€ models_sa.py                    # DailyTopic/Taskç­‰æ‰©å±•è¡¨ORMæ¨¡å‹
â”œâ”€â”€ SentimentAnalysisModel/                 # æƒ…æ„Ÿåˆ†ææ¨¡å‹é›†åˆ
â”‚   â”œâ”€â”€ WeiboSentiment_Finetuned/           # å¾®è°ƒBERT/GPT-2æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ BertChinese-Lora/               # BERTä¸­æ–‡LoRAå¾®è°ƒ
â”‚   â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”‚   â”œâ”€â”€ predict.py
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â””â”€â”€ GPT2-Lora/                      # GPT-2 LoRAå¾®è°ƒ
â”‚   â”‚       â”œâ”€â”€ train.py
â”‚   â”‚       â”œâ”€â”€ predict.py
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”œâ”€â”€ WeiboMultilingualSentiment/         # å¤šè¯­è¨€æƒ…æ„Ÿåˆ†æ
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”œâ”€â”€ predict.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ WeiboSentiment_SmallQwen/           # å°å‚æ•°Qwen3å¾®è°ƒ
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”œâ”€â”€ predict_universal.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ WeiboSentiment_MachineLearning/     # ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•
â”‚       â”œâ”€â”€ train.py
â”‚       â”œâ”€â”€ predict.py
â”‚       â””â”€â”€ ...
â”œâ”€â”€ SingleEngineApp/                        # å•ç‹¬Agentçš„Streamlitåº”ç”¨
â”‚   â”œâ”€â”€ query_engine_streamlit_app.py       # QueryEngineç‹¬ç«‹åº”ç”¨
â”‚   â”œâ”€â”€ media_engine_streamlit_app.py       # MediaEngineç‹¬ç«‹åº”ç”¨
â”‚   â””â”€â”€ insight_engine_streamlit_app.py     # InsightEngineç‹¬ç«‹åº”ç”¨
â”œâ”€â”€ query_engine_streamlit_reports/         # QueryEngineå•åº”ç”¨è¿è¡Œè¾“å‡º
â”œâ”€â”€ media_engine_streamlit_reports/         # MediaEngineå•åº”ç”¨è¿è¡Œè¾“å‡º
â”œâ”€â”€ insight_engine_streamlit_reports/       # InsightEngineå•åº”ç”¨è¿è¡Œè¾“å‡º
â”œâ”€â”€ templates/                              # Flaskå‰ç«¯æ¨¡æ¿
â”‚   â””â”€â”€ index.html                          # ä¸»ç•Œé¢HTML
â”œâ”€â”€ static/                                 # é™æ€èµ„æº
â”‚   â””â”€â”€ image/                              # å›¾ç‰‡èµ„æº
â”‚       â”œâ”€â”€ logo_compressed.png
â”‚       â”œâ”€â”€ framework.png
â”‚       â””â”€â”€ ...
â”œâ”€â”€ logs/                                   # è¿è¡Œæ—¥å¿—ç›®å½•
â”œâ”€â”€ final_reports/                          # æœ€ç»ˆç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶
â”‚   â”œâ”€â”€ ir/                                 # æŠ¥å‘ŠIR JSONæ–‡ä»¶
â”‚   â””â”€â”€ *.html                              # æœ€ç»ˆHTMLæŠ¥å‘Š
â”œâ”€â”€ utils/                                  # é€šç”¨å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ forum_reader.py                     # Agenté—´è®ºå›é€šä¿¡å·¥å…·
â”‚   â”œâ”€â”€ github_issues.py                    # ç»Ÿä¸€ç”ŸæˆGitHub Issueé“¾æ¥ä¸é”™è¯¯æç¤º
â”‚   â””â”€â”€ retry_helper.py                     # ç½‘ç»œè¯·æ±‚é‡è¯•æœºåˆ¶å·¥å…·
â”œâ”€â”€ tests/                                  # å•å…ƒæµ‹è¯•ä¸é›†æˆæµ‹è¯•
â”‚   â”œâ”€â”€ run_tests.py                        # pytestå…¥å£è„šæœ¬
â”‚   â”œâ”€â”€ test_monitor.py                     # ForumEngineç›‘æ§å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ test_report_engine_sanitization.py  # ReportEngineå®‰å…¨æ€§æµ‹è¯•
â”‚   â””â”€â”€ ...
â”œâ”€â”€ app.py                                  # Flaskä¸»åº”ç”¨å…¥å£
â”œâ”€â”€ config.py                               # å…¨å±€é…ç½®æ–‡ä»¶
â”œâ”€â”€ .env.example                            # ç¯å¢ƒå˜é‡ç¤ºä¾‹æ–‡ä»¶
â”œâ”€â”€ docker-compose.yml                      # Dockerå¤šæœåŠ¡ç¼–æ’é…ç½®
â”œâ”€â”€ Dockerfile                              # Dockeré•œåƒæ„å»ºæ–‡ä»¶
â”œâ”€â”€ requirements.txt                        # Pythonä¾èµ–åŒ…æ¸…å•
â”œâ”€â”€ regenerate_latest_pdf.py                # PDFé‡æ–°ç”Ÿæˆå·¥å…·è„šæœ¬
â”œâ”€â”€ report_engine_only.py                   # Report Engineå‘½ä»¤è¡Œç‰ˆæœ¬
â”œâ”€â”€ README.md                               # ä¸­æ–‡è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ README-EN.md                            # è‹±æ–‡è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ CONTRIBUTING.md                         # ä¸­æ–‡è´¡çŒ®æŒ‡å—
â”œâ”€â”€ CONTRIBUTING-EN.md                      # è‹±æ–‡è´¡çŒ®æŒ‡å—
â””â”€â”€ LICENSE                                 # GPL-2.0å¼€æºè®¸å¯è¯
```

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆDockerï¼‰

### 1. å¯åŠ¨é¡¹ç›®

å¤åˆ¶ä¸€ä»½ `.env.example` æ–‡ä»¶ï¼Œå‘½åä¸º `.env` ï¼Œå¹¶æŒ‰éœ€é…ç½® `.env` æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤åœ¨åå°å¯åŠ¨æ‰€æœ‰æœåŠ¡ï¼š

```bash
docker compose up -d
```

&gt; **æ³¨ï¼šé•œåƒæ‹‰å–é€Ÿåº¦æ…¢**ï¼Œåœ¨åŸ `docker-compose.yml` æ–‡ä»¶ä¸­ï¼Œæˆ‘ä»¬å·²ç»é€šè¿‡**æ³¨é‡Š**çš„æ–¹å¼æä¾›äº†å¤‡ç”¨é•œåƒåœ°å€ä¾›æ‚¨æ›¿æ¢

### 2. é…ç½®è¯´æ˜

#### æ•°æ®åº“é…ç½®ï¼ˆPostgreSQLï¼‰

è¯·æŒ‰ç…§ä»¥ä¸‹å‚æ•°é…ç½®æ•°æ®åº“è¿æ¥ä¿¡æ¯ï¼Œä¹Ÿæ”¯æŒMysqlå¯è‡ªè¡Œä¿®æ”¹ï¼š

| é…ç½®é¡¹ | å¡«å†™å€¼ | è¯´æ˜ |
| :--- | :--- | :--- |
| `DB_HOST` | `db` | æ•°æ®åº“æœåŠ¡åç§° (å¯¹åº” `docker-compose.yml` ä¸­çš„æœåŠ¡å) |
| `DB_PORT` | `5432` | é»˜è®¤ PostgreSQL ç«¯å£ |
| `DB_USER` | `bettafish` | æ•°æ®åº“ç”¨æˆ·å |
| `DB_PASSWORD` | `bettafish` | æ•°æ®åº“å¯†ç  |
| `DB_NAME` | `bettafish` | æ•°æ®åº“åç§° |
| **å…¶ä»–** | **ä¿æŒé»˜è®¤** | æ•°æ®åº“è¿æ¥æ± ç­‰å…¶ä»–å‚æ•°è¯·ä¿æŒé»˜è®¤è®¾ç½®ã€‚ |

#### å¤§æ¨¡å‹é…ç½®

&gt; æˆ‘ä»¬æ‰€æœ‰ LLM è°ƒç”¨ä½¿ç”¨ OpenAI çš„ API æ¥å£æ ‡å‡†

åœ¨å®Œæˆæ•°æ®åº“é…ç½®åï¼Œè¯·æ­£å¸¸é…ç½®**æ‰€æœ‰å¤§æ¨¡å‹ç›¸å…³çš„å‚æ•°**ï¼Œç¡®ä¿ç³»ç»Ÿèƒ½å¤Ÿè¿æ¥åˆ°æ‚¨é€‰æ‹©çš„å¤§æ¨¡å‹æœåŠ¡ã€‚

å®Œæˆä¸Šè¿°æ‰€æœ‰é…ç½®å¹¶ä¿å­˜åï¼Œç³»ç»Ÿå³å¯æ­£å¸¸è¿è¡Œã€‚

## ğŸ”§ æºç å¯åŠ¨æŒ‡å—

&gt; å¦‚æœä½ æ˜¯åˆæ¬¡å­¦ä¹ ä¸€ä¸ªAgentç³»ç»Ÿçš„æ­å»ºï¼Œå¯ä»¥ä»ä¸€ä¸ªéå¸¸ç®€å•çš„demoå¼€å§‹ï¼š[Deep Search Agent Demo](https://github.com/666ghj/DeepSearchAgent-Demo)

### ç¯å¢ƒè¦æ±‚

- **æ“ä½œç³»ç»Ÿ**: Windowsã€Linuxã€MacOS
- **Pythonç‰ˆæœ¬**: 3.9+
- **Conda**: Anacondaæˆ–Miniconda
- **æ•°æ®åº“**: PostgreSQLï¼ˆæ¨èï¼‰æˆ–MySQL
- **å†…å­˜**: å»ºè®®2GBä»¥ä¸Š

### 1. åˆ›å»ºç¯å¢ƒ

#### å¦‚æœä½¿ç”¨Conda

```bash
# åˆ›å»ºcondaç¯å¢ƒ
conda create -n your_conda_name python=3.11
conda activate your_conda_name
```

#### å¦‚æœä½¿ç”¨uv

```bash
# åˆ›å»ºuvç¯å¢ƒ
uv venv --python 3.11 # åˆ›å»º3.11ç¯å¢ƒ
```

### 2. å®‰è£… PDF å¯¼å‡ºæ‰€éœ€ç³»ç»Ÿä¾èµ–ï¼ˆå¯é€‰ï¼‰

è¿™éƒ¨åˆ†æœ‰è¯¦ç»†çš„é…ç½®è¯´æ˜ï¼š[é…ç½®æ‰€éœ€ä¾èµ–](./static/Partial%20README%20for%20PDF%20Exporting/README.md)

### 3. å®‰è£…ä¾èµ–åŒ…

&gt; å¦‚æœè·³è¿‡äº†æ­¥éª¤2ï¼Œweasyprintåº“å¯èƒ½æ— æ³•å®‰è£…ï¼ŒPDFåŠŸèƒ½å¯èƒ½æ— æ³•æ­£å¸¸ä½¿ç”¨ã€‚

```bash
# åŸºç¡€ä¾èµ–å®‰è£…
pip install -r requirements.txt

# uvç‰ˆæœ¬å‘½ä»¤ï¼ˆæ›´å¿«é€Ÿå®‰è£…ï¼‰
uv pip install -r requirements.txt
# å¦‚æœä¸æƒ³ä½¿ç”¨æœ¬åœ°æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼ˆç®—åŠ›éœ€æ±‚å¾ˆå°ï¼Œé»˜è®¤å®‰è£…cpuç‰ˆæœ¬ï¼‰ï¼Œå¯ä»¥å°†è¯¥æ–‡ä»¶ä¸­çš„&quot;æœºå™¨å­¦ä¹ &quot;éƒ¨åˆ†æ³¨é‡Šæ‰å†æ‰§è¡ŒæŒ‡ä»¤
```

### 4. å®‰è£…Playwrightæµè§ˆå™¨é©±åŠ¨

```bash
# å®‰è£…æµè§ˆå™¨é©±åŠ¨ï¼ˆç”¨äºçˆ¬è™«åŠŸèƒ½ï¼‰
playwright install chromium
```

### 5. é…ç½®LLMä¸æ•°æ®åº“

å¤åˆ¶ä¸€ä»½é¡¹ç›®æ ¹ç›®å½• `.env.example` æ–‡ä»¶ï¼Œå‘½åä¸º `.env`

ç¼–è¾‘ `.env` æ–‡ä»¶ï¼Œå¡«å…¥æ‚¨çš„APIå¯†é’¥ï¼ˆæ‚¨ä¹Ÿå¯ä»¥é€‰æ‹©è‡ªå·±çš„æ¨¡å‹ã€æœç´¢ä»£ç†ï¼Œè¯¦æƒ…è§æ ¹ç›®å½•.env.exampleæ–‡ä»¶å†…æˆ–æ ¹ç›®å½•config.pyä¸­çš„è¯´æ˜ï¼‰ï¼š

```yml
# ====================== æ•°æ®åº“é…ç½® ======================
# æ•°æ®åº“ä¸»æœºï¼Œä¾‹å¦‚localhost æˆ– 127.0.0.1
DB_HOST=your_db_host
# æ•°æ®åº“ç«¯å£å·ï¼Œé»˜è®¤ä¸º3306
DB_PORT=3306
# æ•°æ®åº“ç”¨æˆ·å
DB_USER=your_db_user
# æ•°æ®åº“å¯†ç 
DB_PASSWORD=your_db_password
# æ•°æ®åº“åç§°
DB_NAME=your_db_name
# æ•°æ®åº“å­—ç¬¦é›†ï¼Œæ¨èutf8mb4ï¼Œå…¼å®¹emoji
DB_CHARSET=utf8mb4
# æ•°æ®åº“ç±»å‹postgresqlæˆ–mysql
DB_DIALECT=postgresql
# æ•°æ®åº“ä¸éœ€è¦åˆå§‹åŒ–ï¼Œæ‰§è¡Œapp.pyæ—¶ä¼šè‡ªåŠ¨æ£€æµ‹

# ====================== LLMé…ç½® ======================
# æ‚¨å¯ä»¥æ›´æ”¹æ¯ä¸ªéƒ¨åˆ†LLMä½¿ç”¨çš„APIï¼Œåªè¦å…¼å®¹OpenAIè¯·æ±‚æ ¼å¼éƒ½å¯ä»¥
# é…ç½®æ–‡ä»¶å†…éƒ¨ç»™äº†æ¯ä¸€ä¸ªAgentçš„æ¨èLLMï¼Œåˆæ¬¡éƒ¨ç½²è¯·å…ˆå‚è€ƒæ¨èè®¾ç½®

# Insight Agent
INSIGHT_ENGINE_API_KEY=
INSIGHT_ENGINE_BASE_URL=
INSIGHT_ENGINE_MODEL_NAME=

# Media Agent
...
```

### 6. å¯åŠ¨ç³»ç»Ÿ

#### 6.1 å®Œæ•´ç³»ç»Ÿå¯åŠ¨ï¼ˆæ¨èï¼‰

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ï¼Œæ¿€æ´»condaç¯å¢ƒ
conda activate your_conda_name

# å¯åŠ¨ä¸»åº”ç”¨å³å¯
python app.py
```

uv ç‰ˆæœ¬å¯åŠ¨å‘½ä»¤ 
```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ï¼Œæ¿€æ´»uvç¯å¢ƒ
.venv\Scripts\activate

# å¯åŠ¨ä¸»åº”ç”¨å³å¯
python app.py
```

&gt; æ³¨1ï¼šä¸€æ¬¡è¿è¡Œç»ˆæ­¢åï¼Œstreamlit appå¯èƒ½ç»“æŸå¼‚å¸¸ä»ç„¶å ç”¨ç«¯å£ï¼Œæ­¤æ—¶æœç´¢å ç”¨ç«¯å£çš„è¿›ç¨‹killæ‰å³å¯

&gt; æ³¨2ï¼šæ•°æ®çˆ¬å–éœ€è¦å•ç‹¬æ“ä½œï¼Œè§6.3æŒ‡å¼•

è®¿é—® http://localhost:5000 å³å¯ä½¿ç”¨å®Œæ•´ç³»ç»Ÿ

#### 6.2 å•ç‹¬å¯åŠ¨æŸä¸ªAgent

```bash
# å¯åŠ¨QueryEngine
streamlit run SingleEngineApp/query_engine_streamlit_app.py --server.port 8503

# å¯åŠ¨MediaEngine  
streamlit run SingleEngineApp/media_engine_streamlit_app.py --server.port 8502

# å¯åŠ¨InsightEngine
streamlit run SingleEngineApp/insight_engine_streamlit_app.py --server.port 8501
```

#### 6.3 çˆ¬è™«ç³»ç»Ÿå•ç‹¬ä½¿ç”¨

è¿™éƒ¨åˆ†æœ‰è¯¦ç»†çš„é…ç½®æ–‡æ¡£ï¼š[MindSpiderä½¿ç”¨è¯´æ˜](./MindSpider/README.md)

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;MindSpider\img\example.png&quot; alt=&quot;banner&quot; width=&quot;600&quot;&gt;

MindSpider è¿è¡Œç¤ºä¾‹
&lt;/div&gt;

```bash
# è¿›å…¥çˆ¬è™«ç›®å½•
cd MindSpider

# é¡¹ç›®åˆå§‹åŒ–
python main.py --setup

# è¿è¡Œè¯é¢˜æå–ï¼ˆè·å–çƒ­ç‚¹æ–°é—»å’Œå…³é”®è¯ï¼‰
python main.py --broad-topic

# è¿è¡Œå®Œæ•´çˆ¬è™«æµç¨‹
python main.py --complete --date 2024-01-20

# ä»…è¿è¡Œè¯é¢˜æå–
python main.py --broad-topic --date 2024-01-20

# ä»…è¿è¡Œæ·±åº¦çˆ¬å–
python main.py --deep-sentiment --platforms xhs dy wb
```

#### 6.4 å‘½ä»¤è¡ŒæŠ¥å‘Šç”Ÿæˆå·¥å…·

è¯¥å·¥å…·ä¼šè·³è¿‡ä¸‰ä¸ªåˆ†æå¼•æ“çš„è¿è¡Œé˜¶æ®µï¼Œç›´æ¥è¯»å–å®ƒä»¬çš„æœ€æ–°æ—¥å¿—æ–‡ä»¶ï¼Œå¹¶åœ¨æ— éœ€ Web ç•Œé¢çš„æƒ…å†µä¸‹ç”Ÿæˆç»¼åˆæŠ¥å‘Šï¼ˆåŒæ—¶çœç•¥æ–‡ä»¶å¢é‡æ ¡éªŒæ­¥éª¤ï¼‰ã€‚é€šå¸¸ç”¨äºå¯¹æŠ¥å‘Šç”Ÿæˆç»“æœä¸æ»¡æ„ã€éœ€è¦å¿«é€Ÿé‡è¯•çš„åœºæ™¯ï¼Œæˆ–åœ¨è°ƒè¯• Report Engine æ—¶å¯ç”¨ã€‚

```bash
# åŸºæœ¬ä½¿ç”¨ï¼ˆè‡ªåŠ¨ä»æ–‡ä»¶åæå–ä¸»é¢˜ï¼‰
python report_engine_only.py

# æŒ‡å®šæŠ¥å‘Šä¸»é¢˜
python report_engine_only.py --query &quot;åœŸæœ¨å·¥ç¨‹è¡Œä¸šåˆ†æ&quot;

# è·³è¿‡PDFç”Ÿæˆï¼ˆå³ä½¿ç³»ç»Ÿæ”¯æŒï¼‰
python report_engine_only.py --skip-pdf

# æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
python report_engine_only.py --verbose

# æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯
python report_engine_only.py --help
```

**åŠŸèƒ½è¯´æ˜ï¼š**

1. **è‡ªåŠ¨æ£€æŸ¥ä¾èµ–**ï¼šç¨‹åºä¼šè‡ªåŠ¨æ£€æŸ¥PDFç”Ÿæˆæ‰€éœ€çš„ç³»ç»Ÿä¾èµ–ï¼Œå¦‚æœç¼ºå¤±ä¼šç»™å‡ºå®‰è£…æç¤º
2. **è·å–æœ€æ–°æ–‡ä»¶**ï¼šè‡ªåŠ¨ä»ä¸‰ä¸ªå¼•æ“ç›®å½•ï¼ˆ`insight_engine_streamlit_reports`ã€`media_engine_streamlit_reports`ã€`query_engine_streamlit_reports`ï¼‰è·å–æœ€æ–°çš„åˆ†ææŠ¥å‘Š
3. **æ–‡ä»¶ç¡®è®¤**ï¼šæ˜¾ç¤ºæ‰€æœ‰é€‰æ‹©çš„æ–‡ä»¶åã€è·¯å¾„å’Œä¿®æ”¹æ—¶é—´ï¼Œç­‰å¾…ç”¨æˆ·ç¡®è®¤ï¼ˆé»˜è®¤è¾“å…¥ `y` ç»§ç»­ï¼Œè¾“å…¥ `n` é€€å‡ºï¼‰
4. **ç›´æ¥ç”ŸæˆæŠ¥å‘Š**ï¼šè·³è¿‡æ–‡ä»¶å¢åŠ å®¡æ ¸ç¨‹åºï¼Œç›´æ¥è°ƒç”¨Report Engineç”Ÿæˆç»¼åˆæŠ¥å‘Š
5. **è‡ªåŠ¨ä¿å­˜æ–‡ä»¶**ï¼š
   - HTMLæŠ¥å‘Šä¿å­˜åˆ° `final_reports/` ç›®å½•
   - PDFæŠ¥å‘Šï¼ˆå¦‚æœæœ‰ä¾èµ–ï¼‰ä¿å­˜åˆ° `final_reports/pdf/` ç›®å½•
   - æ–‡ä»¶å‘½åæ ¼å¼ï¼š`final_report_{ä¸»é¢˜}_{æ—¶é—´æˆ³}.html/pdf`

**æ³¨æ„äº‹é¡¹ï¼š**

- ç¡®ä¿ä¸‰ä¸ªå¼•æ“ç›®å½•ä¸­è‡³å°‘æœ‰ä¸€ä¸ªåŒ…å«`.md`æŠ¥å‘Šæ–‡ä»¶
- å‘½ä»¤è¡Œå·¥å…·ä¸Webç•Œé¢ç›¸äº’ç‹¬ç«‹ï¼Œä¸ä¼šç›¸äº’å½±å“
- PDFç”Ÿæˆéœ€è¦å®‰è£…ç³»ç»Ÿä¾èµ–ï¼Œè¯¦è§ä¸Šæ–‡&quot;å®‰è£… PDF å¯¼å‡ºæ‰€éœ€ç³»ç»Ÿä¾èµ–&quot;éƒ¨åˆ†

## âš™ï¸ é«˜çº§é…ç½®ï¼ˆå·²è¿‡æ—¶ï¼Œå·²ç»ç»Ÿä¸€ä¸ºé¡¹ç›®æ ¹ç›®å½•.envæ–‡ä»¶ç®¡ç†ï¼Œå…¶ä»–å­agentè‡ªåŠ¨ç»§æ‰¿æ ¹ç›®å½•é…ç½®ï¼‰

### ä¿®æ”¹å…³é”®å‚æ•°

#### Agenté…ç½®å‚æ•°

æ¯ä¸ªAgentéƒ½æœ‰ä¸“é—¨çš„é…ç½®æ–‡ä»¶ï¼Œå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼Œä¸‹é¢æ˜¯éƒ¨åˆ†ç¤ºä¾‹ï¼š

```python
# QueryEngine/utils/config.py
class Config:
    max_reflections = 2           # åæ€è½®æ¬¡
    max_search_results = 15       # æœ€å¤§æœç´¢ç»“æœæ•°
    max_content_length = 8000     # æœ€å¤§å†…å®¹é•¿åº¦
    
# MediaEngine/utils/config.py  
class Config:
    comprehensive_search_limit = 10  # ç»¼åˆæœç´¢é™åˆ¶
    web_search_limit = 15           # ç½‘é¡µæœç´¢é™åˆ¶
  

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[infiniflow/ragflow]]></title>
            <link>https://github.com/infiniflow/ragflow</link>
            <guid>https://github.com/infiniflow/ragflow</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[RAGFlow is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/infiniflow/ragflow">infiniflow/ragflow</a></h1>
            <p>RAGFlow is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 69,387</p>
            <p>Forks: 7,527</p>
            <p>Stars today: 174 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://demo.ragflow.io/&quot;&gt;
&lt;img src=&quot;web/src/assets/logo-with-text.svg&quot; width=&quot;520&quot; alt=&quot;ragflow logo&quot;&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;./README.md&quot;&gt;&lt;img alt=&quot;README in English&quot; src=&quot;https://img.shields.io/badge/English-DBEDFA&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_zh.md&quot;&gt;&lt;img alt=&quot;ç®€ä½“ä¸­æ–‡ç‰ˆè‡ªè¿°æ–‡ä»¶&quot; src=&quot;https://img.shields.io/badge/ç®€ä½“ä¸­æ–‡-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_tzh.md&quot;&gt;&lt;img alt=&quot;ç¹é«”ç‰ˆä¸­æ–‡è‡ªè¿°æ–‡ä»¶&quot; src=&quot;https://img.shields.io/badge/ç¹é«”ä¸­æ–‡-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_ja.md&quot;&gt;&lt;img alt=&quot;æ—¥æœ¬èªã®README&quot; src=&quot;https://img.shields.io/badge/æ—¥æœ¬èª-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_ko.md&quot;&gt;&lt;img alt=&quot;í•œêµ­ì–´&quot; src=&quot;https://img.shields.io/badge/í•œêµ­ì–´-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_id.md&quot;&gt;&lt;img alt=&quot;Bahasa Indonesia&quot; src=&quot;https://img.shields.io/badge/Bahasa Indonesia-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_pt_br.md&quot;&gt;&lt;img alt=&quot;PortuguÃªs(Brasil)&quot; src=&quot;https://img.shields.io/badge/PortuguÃªs(Brasil)-DFE0E5&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://x.com/intent/follow?screen_name=infiniflowai&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/twitter/follow/infiniflow?logo=X&amp;color=%20%23f5f5f5&quot; alt=&quot;follow on X(Twitter)&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://demo.ragflow.io&quot; target=&quot;_blank&quot;&gt;
        &lt;img alt=&quot;Static Badge&quot; src=&quot;https://img.shields.io/badge/Online-Demo-4e6b99&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://hub.docker.com/r/infiniflow/ragflow&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/docker/pulls/infiniflow/ragflow?label=Docker%20Pulls&amp;color=0db7ed&amp;logo=docker&amp;logoColor=white&amp;style=flat-square&quot; alt=&quot;docker pull infiniflow/ragflow:v0.22.1&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/infiniflow/ragflow/releases/latest&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/github/v/release/infiniflow/ragflow?color=blue&amp;label=Latest%20Release&quot; alt=&quot;Latest Release&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/infiniflow/ragflow/blob/main/LICENSE&quot;&gt;
        &lt;img height=&quot;21&quot; src=&quot;https://img.shields.io/badge/License-Apache--2.0-ffffff?labelColor=d4eaf7&amp;color=2e6cc4&quot; alt=&quot;license&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://deepwiki.com/infiniflow/ragflow&quot;&gt;
        &lt;img alt=&quot;Ask DeepWiki&quot; src=&quot;https://deepwiki.com/badge.svg&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;h4 align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://ragflow.io/docs/dev/&quot;&gt;Document&lt;/a&gt; |
  &lt;a href=&quot;https://github.com/infiniflow/ragflow/issues/4214&quot;&gt;Roadmap&lt;/a&gt; |
  &lt;a href=&quot;https://twitter.com/infiniflowai&quot;&gt;Twitter&lt;/a&gt; |
  &lt;a href=&quot;https://discord.gg/NjYzJD3GM3&quot;&gt;Discord&lt;/a&gt; |
  &lt;a href=&quot;https://demo.ragflow.io&quot;&gt;Demo&lt;/a&gt;
&lt;/h4&gt;

&lt;div align=&quot;center&quot; style=&quot;margin-top:20px;margin-bottom:20px;&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/ragflow-octoverse.png&quot; width=&quot;1200&quot;/&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/9064&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/9064&quot; alt=&quot;infiniflow%2Fragflow | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;details open&gt;
&lt;summary&gt;&lt;b&gt;ğŸ“• Table of Contents&lt;/b&gt;&lt;/summary&gt;

- ğŸ’¡ [What is RAGFlow?](#-what-is-ragflow)
- ğŸ® [Demo](#-demo)
- ğŸ“Œ [Latest Updates](#-latest-updates)
- ğŸŒŸ [Key Features](#-key-features)
- ğŸ” [System Architecture](#-system-architecture)
- ğŸ¬ [Get Started](#-get-started)
- ğŸ”§ [Configurations](#-configurations)
- ğŸ”§ [Build a Docker image](#-build-a-docker-image)
- ğŸ”¨ [Launch service from source for development](#-launch-service-from-source-for-development)
- ğŸ“š [Documentation](#-documentation)
- ğŸ“œ [Roadmap](#-roadmap)
- ğŸ„ [Community](#-community)
- ğŸ™Œ [Contributing](#-contributing)

&lt;/details&gt;

## ğŸ’¡ What is RAGFlow?

[RAGFlow](https://ragflow.io/) is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs. It offers a streamlined RAG workflow adaptable to enterprises of any scale. Powered by a converged context engine and pre-built agent templates, RAGFlow enables developers to transform complex data into high-fidelity, production-ready AI systems with exceptional efficiency and precision.

## ğŸ® Demo

Try our demo at [https://demo.ragflow.io](https://demo.ragflow.io).

&lt;div align=&quot;center&quot; style=&quot;margin-top:20px;margin-bottom:20px;&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/chunking.gif&quot; width=&quot;1200&quot;/&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/agentic-dark.gif&quot; width=&quot;1200&quot;/&gt;
&lt;/div&gt;

## ğŸ”¥ Latest Updates

- 2025-11-19 Supports Gemini 3 Pro.
- 2025-11-12 Supports data synchronization from Confluence, S3, Notion, Discord, Google Drive.
- 2025-10-23 Supports MinerU &amp; Docling as document parsing methods.
- 2025-10-15 Supports orchestrable ingestion pipeline.
- 2025-08-08 Supports OpenAI&#039;s latest GPT-5 series models.
- 2025-08-01 Supports agentic workflow and MCP.
- 2025-05-23 Adds a Python/JavaScript code executor component to Agent.
- 2025-05-05 Supports cross-language query.
- 2025-03-19 Supports using a multi-modal model to make sense of images within PDF or DOCX files.

## ğŸ‰ Stay Tuned

â­ï¸ Star our repository to stay up-to-date with exciting new features and improvements! Get instant notifications for new
releases! ğŸŒŸ

&lt;div align=&quot;center&quot; style=&quot;margin-top:20px;margin-bottom:20px;&quot;&gt;
&lt;img src=&quot;https://github.com/user-attachments/assets/18c9707e-b8aa-4caf-a154-037089c105ba&quot; width=&quot;1200&quot;/&gt;
&lt;/div&gt;

## ğŸŒŸ Key Features

### ğŸ­ **&quot;Quality in, quality out&quot;**

- [Deep document understanding](./deepdoc/README.md)-based knowledge extraction from unstructured data with complicated
  formats.
- Finds &quot;needle in a data haystack&quot; of literally unlimited tokens.

### ğŸ± **Template-based chunking**

- Intelligent and explainable.
- Plenty of template options to choose from.

### ğŸŒ± **Grounded citations with reduced hallucinations**

- Visualization of text chunking to allow human intervention.
- Quick view of the key references and traceable citations to support grounded answers.

### ğŸ” **Compatibility with heterogeneous data sources**

- Supports Word, slides, excel, txt, images, scanned copies, structured data, web pages, and more.

### ğŸ›€ **Automated and effortless RAG workflow**

- Streamlined RAG orchestration catered to both personal and large businesses.
- Configurable LLMs as well as embedding models.
- Multiple recall paired with fused re-ranking.
- Intuitive APIs for seamless integration with business.

## ğŸ” System Architecture

&lt;div align=&quot;center&quot; style=&quot;margin-top:20px;margin-bottom:20px;&quot;&gt;
&lt;img src=&quot;https://github.com/user-attachments/assets/31b0dd6f-ca4f-445a-9457-70cb44a381b2&quot; width=&quot;1000&quot;/&gt;
&lt;/div&gt;

## ğŸ¬ Get Started

### ğŸ“ Prerequisites

- CPU &gt;= 4 cores
- RAM &gt;= 16 GB
- Disk &gt;= 50 GB
- Docker &gt;= 24.0.0 &amp; Docker Compose &gt;= v2.26.1
- [gVisor](https://gvisor.dev/docs/user_guide/install/): Required only if you intend to use the code executor (sandbox) feature of RAGFlow.

&gt; [!TIP]
&gt; If you have not installed Docker on your local machine (Windows, Mac, or Linux), see [Install Docker Engine](https://docs.docker.com/engine/install/).

### ğŸš€ Start up the server

1. Ensure `vm.max_map_count` &gt;= 262144:

   &gt; To check the value of `vm.max_map_count`:
   &gt;
   &gt; ```bash
   &gt; $ sysctl vm.max_map_count
   &gt; ```
   &gt;
   &gt; Reset `vm.max_map_count` to a value at least 262144 if it is not.
   &gt;
   &gt; ```bash
   &gt; # In this case, we set it to 262144:
   &gt; $ sudo sysctl -w vm.max_map_count=262144
   &gt; ```
   &gt;
   &gt; This change will be reset after a system reboot. To ensure your change remains permanent, add or update the
   &gt; `vm.max_map_count` value in **/etc/sysctl.conf** accordingly:
   &gt;
   &gt; ```bash
   &gt; vm.max_map_count=262144
   &gt; ```
   &gt;
2. Clone the repo:

   ```bash
   $ git clone https://github.com/infiniflow/ragflow.git
   ```
3. Start up the server using the pre-built Docker images:

&gt; [!CAUTION]
&gt; All Docker images are built for x86 platforms. We don&#039;t currently offer Docker images for ARM64.
&gt; If you are on an ARM64 platform, follow [this guide](https://ragflow.io/docs/dev/build_docker_image) to build a Docker image compatible with your system.

&gt; The command below downloads the `v0.22.1` edition of the RAGFlow Docker image. See the following table for descriptions of different RAGFlow editions. To download a RAGFlow edition different from `v0.22.1`, update the `RAGFLOW_IMAGE` variable accordingly in **docker/.env** before using `docker compose` to start the server.

```bash
   $ cd ragflow/docker
  
   # git checkout v0.22.1
   # Optional: use a stable tag (see releases: https://github.com/infiniflow/ragflow/releases)
   # This step ensures the **entrypoint.sh** file in the code matches the Docker image version.
   
   # Use CPU for DeepDoc tasks:
   $ docker compose -f docker-compose.yml up -d

   # To use GPU to accelerate DeepDoc tasks:
   # sed -i &#039;1i DEVICE=gpu&#039; .env
   # docker compose -f docker-compose.yml up -d
```

&gt; Note: Prior to `v0.22.0`, we provided both images with embedding models and slim images without embedding models. Details as follows:

| RAGFlow image tag | Image size (GB) | Has embedding models? | Stable?                  |
| ----------------- | --------------- | --------------------- | ------------------------ |
| v0.21.1           | &amp;approx;9       | âœ”ï¸                    | Stable release           |
| v0.21.1-slim      | &amp;approx;2       | âŒ                    | Stable release           |

&gt; Starting with `v0.22.0`, we ship only the slim edition and no longer append the **-slim** suffix to the image tag.

4. Check the server status after having the server up and running:

   ```bash
   $ docker logs -f docker-ragflow-cpu-1
   ```

   _The following output confirms a successful launch of the system:_

   ```bash

         ____   ___    ______ ______ __
        / __ \ /   |  / ____// ____// /____  _      __
       / /_/ // /| | / / __ / /_   / // __ \| | /| / /
      / _, _// ___ |/ /_/ // __/  / // /_/ /| |/ |/ /
     /_/ |_|/_/  |_|\____//_/    /_/ \____/ |__/|__/

    * Running on all addresses (0.0.0.0)
   ```

   &gt; If you skip this confirmation step and directly log in to RAGFlow, your browser may prompt a `network anormal`
   &gt; error because, at that moment, your RAGFlow may not be fully initialized.
   &gt;
5. In your web browser, enter the IP address of your server and log in to RAGFlow.

   &gt; With the default settings, you only need to enter `http://IP_OF_YOUR_MACHINE` (**sans** port number) as the default
   &gt; HTTP serving port `80` can be omitted when using the default configurations.
   &gt;
6. In [service_conf.yaml.template](./docker/service_conf.yaml.template), select the desired LLM factory in `user_default_llm` and update
   the `API_KEY` field with the corresponding API key.

   &gt; See [llm_api_key_setup](https://ragflow.io/docs/dev/llm_api_key_setup) for more information.
   &gt;

   _The show is on!_

## ğŸ”§ Configurations

When it comes to system configurations, you will need to manage the following files:

- [.env](./docker/.env): Keeps the fundamental setups for the system, such as `SVR_HTTP_PORT`, `MYSQL_PASSWORD`, and
  `MINIO_PASSWORD`.
- [service_conf.yaml.template](./docker/service_conf.yaml.template): Configures the back-end services. The environment variables in this file will be automatically populated when the Docker container starts. Any environment variables set within the Docker container will be available for use, allowing you to customize service behavior based on the deployment environment.
- [docker-compose.yml](./docker/docker-compose.yml): The system relies on [docker-compose.yml](./docker/docker-compose.yml) to start up.

&gt; The [./docker/README](./docker/README.md) file provides a detailed description of the environment settings and service
&gt; configurations which can be used as `${ENV_VARS}` in the [service_conf.yaml.template](./docker/service_conf.yaml.template) file.

To update the default HTTP serving port (80), go to [docker-compose.yml](./docker/docker-compose.yml) and change `80:80`
to `&lt;YOUR_SERVING_PORT&gt;:80`.

Updates to the above configurations require a reboot of all containers to take effect:

&gt; ```bash
&gt; $ docker compose -f docker-compose.yml up -d
&gt; ```

### Switch doc engine from Elasticsearch to Infinity

RAGFlow uses Elasticsearch by default for storing full text and vectors. To switch to [Infinity](https://github.com/infiniflow/infinity/), follow these steps:

1. Stop all running containers:

   ```bash
   $ docker compose -f docker/docker-compose.yml down -v
   ```

&gt; [!WARNING]
&gt; `-v` will delete the docker container volumes, and the existing data will be cleared.

2. Set `DOC_ENGINE` in **docker/.env** to `infinity`.
3. Start the containers:

   ```bash
   $ docker compose -f docker-compose.yml up -d
   ```

&gt; [!WARNING]
&gt; Switching to Infinity on a Linux/arm64 machine is not yet officially supported.

## ğŸ”§ Build a Docker image

This image is approximately 2 GB in size and relies on external LLM and embedding services.

```bash
git clone https://github.com/infiniflow/ragflow.git
cd ragflow/
docker build --platform linux/amd64 -f Dockerfile -t infiniflow/ragflow:nightly .
```

## ğŸ”¨ Launch service from source for development

1. Install `uv` and `pre-commit`, or skip this step if they are already installed:

   ```bash
   pipx install uv pre-commit
   ```
2. Clone the source code and install Python dependencies:

   ```bash
   git clone https://github.com/infiniflow/ragflow.git
   cd ragflow/
   uv sync --python 3.12 # install RAGFlow dependent python modules
   uv run download_deps.py
   pre-commit install
   ```
3. Launch the dependent services (MinIO, Elasticsearch, Redis, and MySQL) using Docker Compose:

   ```bash
   docker compose -f docker/docker-compose-base.yml up -d
   ```

   Add the following line to `/etc/hosts` to resolve all hosts specified in **docker/.env** to `127.0.0.1`:

   ```
   127.0.0.1       es01 infinity mysql minio redis sandbox-executor-manager
   ```
4. If you cannot access HuggingFace, set the `HF_ENDPOINT` environment variable to use a mirror site:

   ```bash
   export HF_ENDPOINT=https://hf-mirror.com
   ```
5. If your operating system does not have jemalloc, please install it as follows:

   ```bash
   # Ubuntu
   sudo apt-get install libjemalloc-dev
   # CentOS
   sudo yum install jemalloc
   # OpenSUSE
   sudo zypper install jemalloc
   # macOS
   sudo brew install jemalloc
   ```
6. Launch backend service:

   ```bash
   source .venv/bin/activate
   export PYTHONPATH=$(pwd)
   bash docker/launch_backend_service.sh
   ```
7. Install frontend dependencies:

   ```bash
   cd web
   npm install
   ```
8. Launch frontend service:

   ```bash
   npm run dev
   ```

   _The following output confirms a successful launch of the system:_

   ![](https://github.com/user-attachments/assets/0daf462c-a24d-4496-a66f-92533534e187)
9. Stop RAGFlow front-end and back-end service after development is complete:

   ```bash
   pkill -f &quot;ragflow_server.py|task_executor.py&quot;
   ```

## ğŸ“š Documentation

- [Quickstart](https://ragflow.io/docs/dev/)
- [Configuration](https://ragflow.io/docs/dev/configurations)
- [Release notes](https://ragflow.io/docs/dev/release_notes)
- [User guides](https://ragflow.io/docs/dev/category/guides)
- [Developer guides](https://ragflow.io/docs/dev/category/developers)
- [References](https://ragflow.io/docs/dev/category/references)
- [FAQs](https://ragflow.io/docs/dev/faq)

## ğŸ“œ Roadmap

See the [RAGFlow Roadmap 2025](https://github.com/infiniflow/ragflow/issues/4214)

## ğŸ„ Community

- [Discord](https://discord.gg/NjYzJD3GM3)
- [Twitter](https://twitter.com/infiniflowai)
- [GitHub Discussions](https://github.com/orgs/infiniflow/discussions)

## ğŸ™Œ Contributing

RAGFlow flourishes via open-source collaboration. In this spirit, we embrace diverse contributions from the community.
If you would like to be a part, review our [Contribution Guidelines](https://ragflow.io/docs/dev/contributing) first.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[google/adk-samples]]></title>
            <link>https://github.com/google/adk-samples</link>
            <guid>https://github.com/google/adk-samples</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[A collection of sample agents built with Agent Development Kit (ADK)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google/adk-samples">google/adk-samples</a></h1>
            <p>A collection of sample agents built with Agent Development Kit (ADK)</p>
            <p>Language: Python</p>
            <p>Stars: 7,329</p>
            <p>Forks: 2,026</p>
            <p>Stars today: 282 stars today</p>
            <h2>README</h2><pre># Agent Development Kit (ADK) Samples

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](LICENSE)

&lt;img src=&quot;https://github.com/google/adk-docs/blob/main/docs/assets/agent-development-kit.png&quot; alt=&quot;Agent Development Kit Logo&quot; width=&quot;150&quot;&gt;

Welcome to the ADK Sample Agents repository! This collection provides ready-to-use agents built on top of the [Agent Development Kit](https://google.github.io/adk-docs/), designed to accelerate your development process. These agents cover a range of common use cases and complexities, from simple conversational bots to complex multi-agent workflows.

## âœ¨ Getting Started
This repo contains ADK sample agents for **Python**, **Go** and **Java.** Navigate to the **[Python](python/)**, **[Go](go/)**, and **[Java](java/)** subfolders to see language-specific setup instructions, and learn more about the available sample agents.

&gt; [!IMPORTANT]
&gt; The agents in this repository are built using the **Agent Development Kit (ADK)**. Before you can run any of the samples, you must have the ADK installed. For instructions, please refer to the [**ADK Installation Guide**](https://google.github.io/adk-docs/get-started).

To learn more, check out the [ADK Documentation](https://google.github.io/adk-docs/), and the GitHub repositories for each language:
- [ADK Python](https://github.com/google/adk-python)
- [ADK Go](https://github.com/google/adk-go)
- [ADK Java](https://github.com/google/adk-java)

## ğŸŒ³ Repository Structure
```bash
â”œâ”€â”€ go
â”‚Â Â  â”œâ”€â”€ agents
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm-auditor
â”‚Â Â  â””â”€â”€ README.md
â”œâ”€â”€ java
â”‚Â Â  â”œâ”€â”€ agents
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ software-bug-assistant
â”‚Â Â  â”‚Â Â  â””â”€â”€ time-series-forecasting
â”‚Â Â  â””â”€â”€ README.md
â”œâ”€â”€ python
â”‚Â Â  â”œâ”€â”€ agents
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ academic-research
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ antom-payment
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ blog-writer
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ brand-search-optimization
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ camel
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ customer-service
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ data-engineering
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ data-science
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ financial-advisor
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ fomc-research
â”‚   â”‚   â”œâ”€â”€ gemini-fullstack
â”‚   â”‚   â”œâ”€â”€ deep-search
â”‚   â”‚   â”œâ”€â”€ google-trends-agent
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ image-scoring
â”‚   â”‚   â”œâ”€â”€ llm-auditor
â”‚   â”‚   â”œâ”€â”€ machine-learning-engineering
â”‚   â”‚   â”œâ”€â”€ marketing-agency
â”‚   â”‚   â”œâ”€â”€ medical-pre-authorization
â”‚   â”‚   â”œâ”€â”€ personalized-shopping
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ plumber-data-engineering-assistant
â”‚   â”‚   â”œâ”€â”€ RAG
â”‚   â”‚   â”œâ”€â”€ realtime-conversational-agent
â”‚   â”‚   â”œâ”€â”€ safety-plugins
â”‚   â”‚   â”œâ”€â”€ short-movie-agents
â”‚   â”‚   â”œâ”€â”€ software-bug-assistant
â”‚   â”‚   â”œâ”€â”€ travel-concierge
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â””â”€â”€ README.md
â””â”€â”€ README.md
```

## â„¹ï¸ Getting help

If you have any questions or if you found any problems with this repository, please report through [GitHub issues](https://github.com/google/adk-samples/issues).

## ğŸ¤ Contributing

We welcome contributions from the community! Whether it&#039;s bug reports, feature requests, documentation improvements, or code contributions, please see our [**Contributing Guidelines**](https://github.com/google/adk-samples/blob/main/CONTRIBUTING.md) to get started.

## ğŸ“„ License

This project is licensed under the Apache 2.0 License - see the [LICENSE](https://github.com/google/adk-samples/blob/main/LICENSE) file for details.

## Disclaimers

This is not an officially supported Google product. This project is not eligible for the [Google Open Source Software Vulnerability Rewards Program](https://bughunters.google.com/open-source-security).

This project is intended for demonstration purposes only. It is not intended for use in a production environment.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[AsyncFuncAI/deepwiki-open]]></title>
            <link>https://github.com/AsyncFuncAI/deepwiki-open</link>
            <guid>https://github.com/AsyncFuncAI/deepwiki-open</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[Open Source DeepWiki: AI-Powered Wiki Generator for GitHub/Gitlab/Bitbucket Repositories. Join the discord: https://discord.gg/gMwThUMeme]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/AsyncFuncAI/deepwiki-open">AsyncFuncAI/deepwiki-open</a></h1>
            <p>Open Source DeepWiki: AI-Powered Wiki Generator for GitHub/Gitlab/Bitbucket Repositories. Join the discord: https://discord.gg/gMwThUMeme</p>
            <p>Language: Python</p>
            <p>Stars: 12,731</p>
            <p>Forks: 1,397</p>
            <p>Stars today: 31 stars today</p>
            <h2>README</h2><pre># DeepWiki-Open

![DeepWiki Banner](screenshots/Deepwiki.png)

**DeepWiki** is my own implementation attempt of DeepWiki, automatically creates beautiful, interactive wikis for any GitHub, GitLab, or BitBucket repository! Just enter a repo name, and DeepWiki will:

1. Analyze the code structure
2. Generate comprehensive documentation
3. Create visual diagrams to explain how everything works
4. Organize it all into an easy-to-navigate wiki

[![&quot;Buy Me A Coffee&quot;](https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png)](https://buymeacoffee.com/sheing)
[![Tip in Crypto](https://tip.md/badge.svg)](https://tip.md/sng-asyncfunc)
[![Twitter/X](https://img.shields.io/badge/Twitter-1DA1F2?style=for-the-badge&amp;logo=twitter&amp;logoColor=white)](https://x.com/sashimikun_void)
[![Discord](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&amp;logo=discord&amp;logoColor=white)](https://discord.com/invite/VQMBGR8u5v)

[English](./README.md) | [ç®€ä½“ä¸­æ–‡](./README.zh.md) | [ç¹é«”ä¸­æ–‡](./README.zh-tw.md) | [æ—¥æœ¬èª](./README.ja.md) | [EspaÃ±ol](./README.es.md) | [í•œêµ­ì–´](./README.kr.md) | [Tiáº¿ng Viá»‡t](./README.vi.md) | [PortuguÃªs Brasileiro](./README.pt-br.md) | [FranÃ§ais](./README.fr.md) | [Ğ ÑƒÑÑĞºĞ¸Ğ¹](./README.ru.md)

## âœ¨ Features

- **Instant Documentation**: Turn any GitHub, GitLab or BitBucket repo into a wiki in seconds
- **Private Repository Support**: Securely access private repositories with personal access tokens
- **Smart Analysis**: AI-powered understanding of code structure and relationships
- **Beautiful Diagrams**: Automatic Mermaid diagrams to visualize architecture and data flow
- **Easy Navigation**: Simple, intuitive interface to explore the wiki
- **Ask Feature**: Chat with your repository using RAG-powered AI to get accurate answers
- **DeepResearch**: Multi-turn research process that thoroughly investigates complex topics
- **Multiple Model Providers**: Support for Google Gemini, OpenAI, OpenRouter, and local Ollama models
- **Flexible Embeddings**: Choose between OpenAI, Google AI, or local Ollama embeddings for optimal performance

## ğŸš€ Quick Start (Super Easy!)

### Option 1: Using Docker

```bash
# Clone the repository
git clone https://github.com/AsyncFuncAI/deepwiki-open.git
cd deepwiki-open

# Create a .env file with your API keys
echo &quot;GOOGLE_API_KEY=your_google_api_key&quot; &gt; .env
echo &quot;OPENAI_API_KEY=your_openai_api_key&quot; &gt;&gt; .env
# Optional: Use Google AI embeddings instead of OpenAI (recommended if using Google models)
echo &quot;DEEPWIKI_EMBEDDER_TYPE=google&quot; &gt;&gt; .env
# Optional: Add OpenRouter API key if you want to use OpenRouter models
echo &quot;OPENROUTER_API_KEY=your_openrouter_api_key&quot; &gt;&gt; .env
# Optional: Add Ollama host if not local. defaults to http://localhost:11434
echo &quot;OLLAMA_HOST=your_ollama_host&quot; &gt;&gt; .env
# Optional: Add Azure API key, endpoint and version if you want to use azure openai models
echo &quot;AZURE_OPENAI_API_KEY=your_azure_openai_api_key&quot; &gt;&gt; .env
echo &quot;AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint&quot; &gt;&gt; .env
echo &quot;AZURE_OPENAI_VERSION=your_azure_openai_version&quot; &gt;&gt; .env
# Run with Docker Compose
docker-compose up
```

For detailed instructions on using DeepWiki with Ollama and Docker, see [Ollama Instructions](Ollama-instruction.md).

&gt; ğŸ’¡ **Where to get these keys:**
&gt; - Get a Google API key from [Google AI Studio](https://makersuite.google.com/app/apikey)
&gt; - Get an OpenAI API key from [OpenAI Platform](https://platform.openai.com/api-keys)
&gt; - Get Azure OpenAI credentials from [Azure Portal](https://portal.azure.com/) - create an Azure OpenAI resource and get the API key, endpoint, and API version

### Option 2: Manual Setup (Recommended)

#### Step 1: Set Up Your API Keys

Create a `.env` file in the project root with these keys:

```
GOOGLE_API_KEY=your_google_api_key
OPENAI_API_KEY=your_openai_api_key
# Optional: Use Google AI embeddings (recommended if using Google models)
DEEPWIKI_EMBEDDER_TYPE=google
# Optional: Add this if you want to use OpenRouter models
OPENROUTER_API_KEY=your_openrouter_api_key
# Optional: Add this if you want to use Azure OpenAI models
AZURE_OPENAI_API_KEY=your_azure_openai_api_key
AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint
AZURE_OPENAI_VERSION=your_azure_openai_version
# Optional: Add Ollama host if not local. default: http://localhost:11434
OLLAMA_HOST=your_ollama_host
```

#### Step 2: Start the Backend

```bash
# Install Python dependencies
python -m pip install poetry==1.8.2 &amp;&amp; poetry install -C api

# Start the API server
python -m api.main
```

#### Step 3: Start the Frontend

```bash
# Install JavaScript dependencies
npm install
# or
yarn install

# Start the web app
npm run dev
# or
yarn dev
```

#### Step 4: Use DeepWiki!

1. Open [http://localhost:3000](http://localhost:3000) in your browser
2. Enter a GitHub, GitLab, or Bitbucket repository (like `https://github.com/openai/codex`, `https://github.com/microsoft/autogen`, `https://gitlab.com/gitlab-org/gitlab`, or `https://bitbucket.org/redradish/atlassian_app_versions`)
3. For private repositories, click &quot;+ Add access tokens&quot; and enter your GitHub or GitLab personal access token
4. Click &quot;Generate Wiki&quot; and watch the magic happen!

## ğŸ” How It Works

DeepWiki uses AI to:

1. Clone and analyze the GitHub, GitLab, or Bitbucket repository (including private repos with token authentication)
2. Create embeddings of the code for smart retrieval
3. Generate documentation with context-aware AI (using Google Gemini, OpenAI, OpenRouter, Azure OpenAI, or local Ollama models)
4. Create visual diagrams to explain code relationships
5. Organize everything into a structured wiki
6. Enable intelligent Q&amp;A with the repository through the Ask feature
7. Provide in-depth research capabilities with DeepResearch

```mermaid
graph TD
    A[User inputs GitHub/GitLab/Bitbucket repo] --&gt; AA{Private repo?}
    AA --&gt;|Yes| AB[Add access token]
    AA --&gt;|No| B[Clone Repository]
    AB --&gt; B
    B --&gt; C[Analyze Code Structure]
    C --&gt; D[Create Code Embeddings]

    D --&gt; M{Select Model Provider}
    M --&gt;|Google Gemini| E1[Generate with Gemini]
    M --&gt;|OpenAI| E2[Generate with OpenAI]
    M --&gt;|OpenRouter| E3[Generate with OpenRouter]
    M --&gt;|Local Ollama| E4[Generate with Ollama]
    M --&gt;|Azure| E5[Generate with Azure]

    E1 --&gt; E[Generate Documentation]
    E2 --&gt; E
    E3 --&gt; E
    E4 --&gt; E
    E5 --&gt; E

    D --&gt; F[Create Visual Diagrams]
    E --&gt; G[Organize as Wiki]
    F --&gt; G
    G --&gt; H[Interactive DeepWiki]

    classDef process stroke-width:2px;
    classDef data stroke-width:2px;
    classDef result stroke-width:2px;
    classDef decision stroke-width:2px;

    class A,D data;
    class AA,M decision;
    class B,C,E,F,G,AB,E1,E2,E3,E4,E5 process;
    class H result;
```

## ğŸ› ï¸ Project Structure

```
deepwiki/
â”œâ”€â”€ api/                  # Backend API server
â”‚   â”œâ”€â”€ main.py           # API entry point
â”‚   â”œâ”€â”€ api.py            # FastAPI implementation
â”‚   â”œâ”€â”€ rag.py            # Retrieval Augmented Generation
â”‚   â”œâ”€â”€ data_pipeline.py  # Data processing utilities
â”‚   â””â”€â”€ requirements.txt  # Python dependencies
â”‚
â”œâ”€â”€ src/                  # Frontend Next.js app
â”‚   â”œâ”€â”€ app/              # Next.js app directory
â”‚   â”‚   â””â”€â”€ page.tsx      # Main application page
â”‚   â””â”€â”€ components/       # React components
â”‚       â””â”€â”€ Mermaid.tsx   # Mermaid diagram renderer
â”‚
â”œâ”€â”€ public/               # Static assets
â”œâ”€â”€ package.json          # JavaScript dependencies
â””â”€â”€ .env                  # Environment variables (create this)
```

## ğŸ¤– Provider-Based Model Selection System

DeepWiki now implements a flexible provider-based model selection system supporting multiple LLM providers:

### Supported Providers and Models

- **Google**: Default `gemini-2.5-flash`, also supports `gemini-2.5-flash-lite`, `gemini-2.5-pro`, etc.
- **OpenAI**: Default `gpt-5-nano`, also supports `gpt-5`, `4o`, etc.
- **OpenRouter**: Access to multiple models via a unified API, including Claude, Llama, Mistral, etc.
- **Azure OpenAI**: Default `gpt-4o`, also supports `o4-mini`, etc.
- **Ollama**: Support for locally running open-source models like `llama3`

### Environment Variables

Each provider requires its corresponding API key environment variables:

```
# API Keys
GOOGLE_API_KEY=your_google_api_key        # Required for Google Gemini models
OPENAI_API_KEY=your_openai_api_key        # Required for OpenAI models
OPENROUTER_API_KEY=your_openrouter_api_key # Required for OpenRouter models
AZURE_OPENAI_API_KEY=your_azure_openai_api_key  #Required for Azure OpenAI models
AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint  #Required for Azure OpenAI models
AZURE_OPENAI_VERSION=your_azure_openai_version  #Required for Azure OpenAI models

# OpenAI API Base URL Configuration
OPENAI_BASE_URL=https://custom-api-endpoint.com/v1  # Optional, for custom OpenAI API endpoints

# Ollama host
OLLAMA_HOST=your_ollama_host # Optional, if Ollama is not local. default: http://localhost:11434

# Configuration Directory
DEEPWIKI_CONFIG_DIR=/path/to/custom/config/dir  # Optional, for custom config file location
```

### Configuration Files

DeepWiki uses JSON configuration files to manage various aspects of the system:

1. **`generator.json`**: Configuration for text generation models
   - Defines available model providers (Google, OpenAI, OpenRouter, Azure, Ollama)
   - Specifies default and available models for each provider
   - Contains model-specific parameters like temperature and top_p

2. **`embedder.json`**: Configuration for embedding models and text processing
   - Defines embedding models for vector storage
   - Contains retriever configuration for RAG
   - Specifies text splitter settings for document chunking

3. **`repo.json`**: Configuration for repository handling
   - Contains file filters to exclude certain files and directories
   - Defines repository size limits and processing rules

By default, these files are located in the `api/config/` directory. You can customize their location using the `DEEPWIKI_CONFIG_DIR` environment variable.

### Custom Model Selection for Service Providers

The custom model selection feature is specifically designed for service providers who need to:

- You can offer multiple AI model choices to users within your organization
- You can quickly adapt to the rapidly evolving LLM landscape without code changes
- You can support specialized or fine-tuned models that aren&#039;t in the predefined list

Service providers can implement their model offerings by selecting from the predefined options or entering custom model identifiers in the frontend interface.

### Base URL Configuration for Enterprise Private Channels

The OpenAI Client&#039;s base_url configuration is designed primarily for enterprise users with private API channels. This feature:

- Enables connection to private or enterprise-specific API endpoints
- Allows organizations to use their own self-hosted or custom-deployed LLM services
- Supports integration with third-party OpenAI API-compatible services

**Coming Soon**: In future updates, DeepWiki will support a mode where users need to provide their own API keys in requests. This will allow enterprise customers with private channels to use their existing API arrangements without sharing credentials with the DeepWiki deployment.

## ğŸ§© Using OpenAI-Compatible Embedding Models (e.g., Alibaba Qwen)

If you want to use embedding models compatible with the OpenAI API (such as Alibaba Qwen), follow these steps:

1. Replace the contents of `api/config/embedder.json` with those from `api/config/embedder_openai_compatible.json`.
2. In your project root `.env` file, set the relevant environment variables, for example:
   ```
   OPENAI_API_KEY=your_api_key
   OPENAI_BASE_URL=your_openai_compatible_endpoint
   ```
3. The program will automatically substitute placeholders in embedder.json with the values from your environment variables.

This allows you to seamlessly switch to any OpenAI-compatible embedding service without code changes.

## ğŸ§  Using Google AI Embeddings

DeepWiki now supports Google AI&#039;s latest embedding models as an alternative to OpenAI embeddings. This provides better integration when you&#039;re already using Google Gemini models for text generation.

### Features

- **Latest Model**: Uses Google&#039;s `text-embedding-004` model
- **Same API Key**: Uses your existing `GOOGLE_API_KEY` (no additional setup required)
- **Better Integration**: Optimized for use with Google Gemini text generation models
- **Task-Specific**: Supports semantic similarity, retrieval, and classification tasks
- **Batch Processing**: Efficient processing of multiple texts

### How to Enable Google AI Embeddings

**Option 1: Environment Variable (Recommended)**

Set the embedder type in your `.env` file:

```bash
# Your existing Google API key
GOOGLE_API_KEY=your_google_api_key

# Enable Google AI embeddings
DEEPWIKI_EMBEDDER_TYPE=google
```

**Option 2: Docker Environment**

```bash
docker run -p 8001:8001 -p 3000:3000 \
  -e GOOGLE_API_KEY=your_google_api_key \
  -e DEEPWIKI_EMBEDDER_TYPE=google \
  -v ~/.adalflow:/root/.adalflow \
  ghcr.io/asyncfuncai/deepwiki-open:latest
```

**Option 3: Docker Compose**

Add to your `.env` file:

```bash
GOOGLE_API_KEY=your_google_api_key
DEEPWIKI_EMBEDDER_TYPE=google
```

Then run:

```bash
docker-compose up
```

### Available Embedder Types

| Type | Description | API Key Required | Notes |
|------|-------------|------------------|-------|
| `openai` | OpenAI embeddings (default) | `OPENAI_API_KEY` | Uses `text-embedding-3-small` model |
| `google` | Google AI embeddings | `GOOGLE_API_KEY` | Uses `text-embedding-004` model |
| `ollama` | Local Ollama embeddings | None | Requires local Ollama installation |

### Why Use Google AI Embeddings?

- **Consistency**: If you&#039;re using Google Gemini for text generation, using Google embeddings provides better semantic consistency
- **Performance**: Google&#039;s latest embedding model offers excellent performance for retrieval tasks
- **Cost**: Competitive pricing compared to OpenAI
- **No Additional Setup**: Uses the same API key as your text generation models

### Switching Between Embedders

You can easily switch between different embedding providers:

```bash
# Use OpenAI embeddings (default)
export DEEPWIKI_EMBEDDER_TYPE=openai

# Use Google AI embeddings
export DEEPWIKI_EMBEDDER_TYPE=google

# Use local Ollama embeddings
export DEEPWIKI_EMBEDDER_TYPE=ollama
```

**Note**: When switching embedders, you may need to regenerate your repository embeddings as different models produce different vector spaces.

### Logging

DeepWiki uses Python&#039;s built-in `logging` module for diagnostic output. You can configure the verbosity and log file destination via environment variables:

| Variable        | Description                                                        | Default                      |
|-----------------|--------------------------------------------------------------------|------------------------------|
| `LOG_LEVEL`     | Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL).             | INFO                         |
| `LOG_FILE_PATH` | Path to the log file. If set, logs will be written to this file.   | `api/logs/application.log`   |

To enable debug logging and direct logs to a custom file:
```bash
export LOG_LEVEL=DEBUG
export LOG_FILE_PATH=./debug.log
python -m api.main
```
Or with Docker Compose:
```bash
LOG_LEVEL=DEBUG LOG_FILE_PATH=./debug.log docker-compose up
```

When running with Docker Compose, the container&#039;s `api/logs` directory is bind-mounted to `./api/logs` on your host (see the `volumes` section in `docker-compose.yml`), ensuring log files persist across restarts.

Alternatively, you can store these settings in your `.env` file:

```bash
LOG_LEVEL=DEBUG
LOG_FILE_PATH=./debug.log
```
Then simply run:

```bash
docker-compose up
```

**Logging Path Security Considerations:** In production environments, ensure the `api/logs` directory and any custom log file path are secured with appropriate filesystem permissions and access controls. The application enforces that `LOG_FILE_PATH` resides within the project&#039;s `api/logs` directory to prevent path traversal or unauthorized writes.

## ğŸ› ï¸ Advanced Setup

### Environment Variables

| Variable             | Description                                                  | Required | Note                                                                                                     |
|----------------------|--------------------------------------------------------------|----------|----------------------------------------------------------------------------------------------------------|
| `GOOGLE_API_KEY`     | Google Gemini API key for AI generation and embeddings      | No | Required for Google Gemini models and Google AI embeddings                                               
| `OPENAI_API_KEY`     | OpenAI API key for embeddings and models                     | Conditional | Required if using OpenAI embeddings or models                                                            |
| `OPENROUTER_API_KEY` | OpenRouter API key for alternative models                    | No | Required only if you want to use OpenRouter models                                                       |
| `AZURE_OPENAI_API_KEY` | Azure OpenAI API key                    | No | Required only if you want to use Azure OpenAI models                                                       |
| `AZURE_OPENAI_ENDPOINT` | Azure OpenAI endpoint                    | No | Required only if you want to use Azure OpenAI models                                                       |
| `AZURE_OPENAI_VERSION` | Azure OpenAI version                     | No | Required only if you want to use Azure OpenAI models                                                       |
| `OLLAMA_HOST`        | Ollama Host (default: http://localhost:11434)                | No | Required only if you want to use external Ollama server                                                  |
| `DEEPWIKI_EMBEDDER_TYPE` | Embedder type: `openai`, `google`, or `ollama` (default: `openai`) | No | Controls which embedding provider to use                                                              |
| `PORT`               | Port for the API server (default: 8001)                      | No | If you host API and frontend on the same machine, make sure change port of `SERVER_BASE_URL` accordingly |
| `SERVER_BASE_URL`    | Base URL for the API server (default: http://localhost:8001) | No |
| `DEEPWIKI_AUTH_MODE` | Set to `true` or `1` to enable authorization mode. | No | Defaults to `false`. If enabled, `DEEPWIKI_AUTH_CODE` is required. |
| `DEEPWIKI_AUTH_CODE` | The secret code required for wiki generation when `DEEPWIKI_AUTH_MODE` is enabled. | No | Only used if `DEEPWIKI_AUTH_MODE` is `true` or `1`. |

**API Key Requirements:**
- If using `DEEPWIKI_EMBEDDER_TYPE=openai` (default): `OPENAI_API_KEY` is required
- If using `DEEPWIKI_EMBEDDER_TYPE=google`: `GOOGLE_API_KEY` is required  
- If using `DEEPWIKI_EMBEDDER_TYPE=ollama`: No API key required (local processing)

Other API keys are only required when configuring and using models from the corresponding providers.

## Authorization Mode

DeepWiki can be configured to run in an authorization mode, where wiki generation requires a valid authorization code. This is useful if you want to control who can use the generation feature.
Restricts frontend initiation and protects cache deletion, but doesn&#039;t fully prevent backend generation if API endpoints are hit directly.

To enable authorization mode, set the following environment variables:

- `DEEPWIKI_AUTH_MODE`: Set this to `true` or `1`. When enabled, the frontend will display an input field for the authorization code.
- `DEEPWIKI_AUTH_CODE`: Set this to the desired secret code. Restricts frontend initiation and protects cache deletion, but doesn&#039;t fully prevent backend generation if API endpoints are hit directly.

If `DEEPWIKI_AUTH_MODE` is not set or is set to `false` (or a

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[zhu-xlab/GlobalBuildingAtlas]]></title>
            <link>https://github.com/zhu-xlab/GlobalBuildingAtlas</link>
            <guid>https://github.com/zhu-xlab/GlobalBuildingAtlas</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:23 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/zhu-xlab/GlobalBuildingAtlas">zhu-xlab/GlobalBuildingAtlas</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 1,289</p>
            <p>Forks: 128</p>
            <p>Stars today: 197 stars today</p>
            <h2>README</h2><pre># GlobalBuildingAtlas

## Introduction
In this project, we provide the level of detail 1 (LoD1) data of buildings across the globe.

A overview of the dataset is illustrated bellow:

&lt;img src=&quot;figures/overview.png&quot; width=&quot;800&quot;&gt;


## Access to the Data
### Web Feature Service (WFS)
A WFS is provided so that one can access the data using other websites or GIS softwares such as QGIS and ArcGIS.

Url: `https://tubvsig-so2sat-vm1.srv.mwn.de/geoserver/ows?`

### Web Viewer
A web interface for viewing the data is available at: [website](https://tubvsig-so2sat-vm1.srv.mwn.de)

Note: Over the past few days, our web viewer has received nearly 280,000 access requests. Due to this unusually high traffic, some data may not load completely, which may result in a significant portion of buildings not being displayed.

### Full Data Download
The full data can be downloaded from [mediaTUM](https://mediatum.ub.tum.de/1782307)

## Development Code
### Global Building Polygon Generation using Satellite Data (Sec. 4.3)
For codes related to building map extraction, regularization, polygonization, and simplification, i.e., generating building polygons from satellite images (Sec. 4.3.2, Sec. 4.3.3, and Sec. 4.3.4), please refer to `./im2bf`.

### Global Building Height Estimation (Sec. 4.4)
1. For codes related to monocular height estimation using HTC-DC Net (Sec. 4.4.2), please refer to `./im2bh`.
2. For codes related to the global inference and uncertainty quantification (Sec. 4.4.3), please refer to `./infer_height`

### Global LoD1 Building Model Generation (Sec. 4.5)
1. For codes related to quality-guided building polygon fusion (Sec. 4.5.1), please refer to `./fuse_bf`.
2. For codes related to LoD1 building model generation (Sec. 4.5.2), please refer to `./make_lod1`.

## Visualization Code
For codes to reproduce the plots in the manuscript, please refer to `./make_plots`.

## Code License
MIT with Commons Clause (no commercial use allowed). See [LICENSE](https://github.com/zhu-xlab/GlobalBuildingAtlas/blob/main/LICENSE).

## How to cite
If you find this dataset helpful in your work, please cite the following paper.
```
@Article{essd-17-6647-2025,
AUTHOR = {Zhu, X. X. and Chen, S. and Zhang, F. and Shi, Y. and Wang, Y.},
TITLE = {GlobalBuildingAtlas: an open global and complete dataset of building polygons, heights and LoD1 3D models},
JOURNAL = {Earth System Science Data},
VOLUME = {17},
YEAR = {2025},
NUMBER = {12},
PAGES = {6647--6668},
URL = {https://essd.copernicus.org/articles/17/6647/2025/},
DOI = {10.5194/essd-17-6647-2025}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[rany2/edge-tts]]></title>
            <link>https://github.com/rany2/edge-tts</link>
            <guid>https://github.com/rany2/edge-tts</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[Use Microsoft Edge's online text-to-speech service from Python WITHOUT needing Microsoft Edge or Windows or an API key]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/rany2/edge-tts">rany2/edge-tts</a></h1>
            <p>Use Microsoft Edge's online text-to-speech service from Python WITHOUT needing Microsoft Edge or Windows or an API key</p>
            <p>Language: Python</p>
            <p>Stars: 9,530</p>
            <p>Forks: 906</p>
            <p>Stars today: 8 stars today</p>
            <h2>README</h2><pre># edge-tts

`edge-tts` is a Python module that allows you to use Microsoft Edge&#039;s online text-to-speech service from within your Python code or using the provided `edge-tts` or `edge-playback` command.

## Installation

To install it, run the following command:

    $ pip install edge-tts

If you only want to use the `edge-tts` and `edge-playback` commands, it would be better to use `pipx`:

    $ pipx install edge-tts

## Usage

### Basic usage

If you want to use the `edge-tts` command, you can simply run it with the following command:

    $ edge-tts --text &quot;Hello, world!&quot; --write-media hello.mp3 --write-subtitles hello.srt

If you wish to play it back immediately with subtitles, you could use the `edge-playback` command:

    $ edge-playback --text &quot;Hello, world!&quot;

Note that `edge-playback` requires the installation of the [`mpv` command line player](https://mpv.io/), except on Windows.

All `edge-tts` commands work with `edge-playback` with the exception of the `--write-media`, `--write-subtitles` and `--list-voices` options.

### Changing the voice

You can change the voice used by the text-to-speech service by using the `--voice` option. The `--list-voices` option can be used to list all available voices.

    $ edge-tts --list-voices
    Name                               Gender    ContentCategories      VoicePersonalities
    ---------------------------------  --------  ---------------------  --------------------------------------
    af-ZA-AdriNeural                   Female    General                Friendly, Positive
    af-ZA-WillemNeural                 Male      General                Friendly, Positive
    am-ET-AmehaNeural                  Male      General                Friendly, Positive
    am-ET-MekdesNeural                 Female    General                Friendly, Positive
    ar-AE-FatimaNeural                 Female    General                Friendly, Positive
    ar-AE-HamdanNeural                 Male      General                Friendly, Positive
    ar-BH-AliNeural                    Male      General                Friendly, Positive
    ar-BH-LailaNeural                  Female    General                Friendly, Positive
    ar-DZ-AminaNeural                  Female    General                Friendly, Positive
    ar-DZ-IsmaelNeural                 Male      General                Friendly, Positive
    ar-EG-SalmaNeural                  Female    General                Friendly, Positive
    ...

    $ edge-tts --voice ar-EG-SalmaNeural --text &quot;Ù…Ø±Ø­Ø¨Ø§ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ&quot; --write-media hello_in_arabic.mp3 --write-subtitles hello_in_arabic.srt

### Custom SSML

Support for custom SSML was removed because Microsoft prevents the use of any SSML that could not be generated by Microsoft Edge itself. This means that all the cases where custom SSML would be useful cannot be supported as the service only permits a single `&lt;voice&gt;` tag with a single `&lt;prosody&gt;` tag inside it. Any available customization options that could be used in the `&lt;prosody&gt;` tag are already available from the library or the command line itself.

### Changing rate, volume and pitch

You can change the rate, volume and pitch of the generated speech by using the `--rate`, `--volume` and `--pitch` options. When using a negative value, you will need to use `--[option]=-50%` instead of `--[option] -50%` to avoid the option being interpreted as a command line option.

    $ edge-tts --rate=-50% --text &quot;Hello, world!&quot; --write-media hello_with_rate_lowered.mp3 --write-subtitles hello_with_rate_lowered.srt
    $ edge-tts --volume=-50% --text &quot;Hello, world!&quot; --write-media hello_with_volume_lowered.mp3 --write-subtitles hello_with_volume_lowered.srt
    $ edge-tts --pitch=-50Hz --text &quot;Hello, world!&quot; --write-media hello_with_pitch_lowered.mp3 --write-subtitles hello_with_pitch_lowered.srt

## Python module

It is possible to use the `edge-tts` module directly from Python. Examples from the project itself include:

* [/examples/](/examples/)
* [/src/edge_tts/util.py](/src/edge_tts/util.py)

Other projects that use the `edge-tts` module include:

* [hass-edge-tts](https://github.com/hasscc/hass-edge-tts/blob/main/custom_components/edge_tts/tts.py)
* [Podcastfy](https://github.com/souzatharsis/podcastfy/blob/main/podcastfy/tts/providers/edge.py)
* [tts-samples](https://github.com/yaph/tts-samples/blob/main/bin/create_sound_samples.py) - a collection of [mp3 sound samples](https://github.com/yaph/tts-samples/tree/main/mp3) to facilitate picking a voice for your project.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NVIDIA/cutile-python]]></title>
            <link>https://github.com/NVIDIA/cutile-python</link>
            <guid>https://github.com/NVIDIA/cutile-python</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[cuTile is a programming model for writing parallel kernels for NVIDIA GPUs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NVIDIA/cutile-python">NVIDIA/cutile-python</a></h1>
            <p>cuTile is a programming model for writing parallel kernels for NVIDIA GPUs</p>
            <p>Language: Python</p>
            <p>Stars: 1,346</p>
            <p>Forks: 64</p>
            <p>Stars today: 183 stars today</p>
            <h2>README</h2><pre>&lt;!--- SPDX-FileCopyrightText: Copyright (c) &lt;2025&gt; NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved. --&gt;
&lt;!--- SPDX-License-Identifier: Apache-2.0 --&gt;

cuTile Python
=============

cuTile Python is a programming language for NVIDIA GPUs. The official documentation can be found
on [docs.nvidia.com](https://docs.nvidia.com/cuda/cutile-python),
or built from source located in the [docs](docs/) folder.


Example
-------
```python
# This examples uses CuPy which can be installed via `pip install cupy-cuda13x`
# Make sure cuda toolkit 13.1+ is installed: https://developer.nvidia.com/cuda-downloads

import cuda.tile as ct
import cupy

TILE_SIZE = 16

# cuTile kernel for adding two dense vectors. It runs in parallel on the GPU.
@ct.kernel
def vector_add_kernel(a, b, result):
    block_id = ct.bid(0)
    a_tile = ct.load(a, index=(block_id,), shape=(TILE_SIZE,))
    b_tile = ct.load(b, index=(block_id,), shape=(TILE_SIZE,))
    result_tile = a_tile + b_tile
    ct.store(result, index=(block_id,), tile=result_tile)

# Host-side function that launches the above kernel.
def vector_add(a: cupy.ndarray, b: cupy.ndarray, result: cupy.ndarray):
    assert a.shape == b.shape == result.shape
    grid = (ct.cdiv(a.shape[0], TILE_SIZE), 1, 1)
    ct.launch(cupy.cuda.get_current_stream(), grid, vector_add_kernel, (a, b, result))


import numpy as np

def test_vector_add():
    a = cupy.random.uniform(-5, 5, 128)
    b = cupy.random.uniform(-5, 5, 128)
    result = cupy.zeros_like(a)

    vector_add(a, b, result)

    a_np = cupy.asnumpy(a)
    b_np = cupy.asnumpy(b)
    result_np = cupy.asnumpy(result)

    expected = a_np + b_np
    np.testing.assert_array_almost_equal(result_np, expected)

test_vector_add()
```

System Requirements
-------------------
cuTile Python generates kernels based on [Tile IR](https://docs.nvidia.com/cuda/tile-ir/)
which requries NVIDIA Driver r580 or later to run.
Furthermore, the `tileiras` compiler only supports Blackwell GPU with 13.1 release, but the
restriction will be removed in the coming versions.
Checkout the [prerequisites](https://docs.nvidia.com/cuda/cutile-python/quickstart.html#prerequisites)
for full list of requirements.


Installing from PyPI
--------------------
cuTile Python is published on [PyPI](https://pypi.org/) under the
[cuda-tile](https://pypi.org/project/cuda-tile/) package name and can be installed with `pip`:
```
pip install cuda-tile
```
Currently, the [CUDA Toolkit 13.1+](https://developer.nvidia.com/cuda-downloads) is required
and needs to be installed separately. On a Debian-based system, use `apt-get install
cuda-tileiras-13.1 cuda-compiler-13.1` instead of `apt-get install cuda-toolkit-13.1`
if you wish to avoid installing the full CUDA Toolkit.


Building from Source
--------------------
cuTile is written mostly in Python, but includes a C++ extension which needs to be built.
You will need:
- A C++17-capable compiler, such as GNU C++ or MSVC;
- CMake 3.18+;
- GNU Make on Linux or msbuild on Windows;
- Python 3.10+ with development headers (`venv` module is recommended but optional);
- [CUDA Toolkit 13.1+](https://developer.nvidia.com/cuda-downloads)

On an Ubuntu system, the first four dependencies can be installed with APT:
```
sudo apt-get update &amp;&amp; sudo apt-get install build-essential cmake python3-dev python3-venv
```

The CMakeLists.txt script will also automatically download
the [DLPack](https://github.com/dmlc/dlpack) dependency from GitHub.
If you wish to disable this behavior and provide your own copy of DLPack,
set the `CUDA_TILE_CMAKE_DLPACK_PATH` environment variable to a local path
to the DLPack source tree.

Unless you are already using a Python virtual environment, it is recommended to create one
in order to avoid installing cuTile globally:

```
python3 -m venv env
source env/bin/activate
```

Once the build dependencies are in place, the simplest way to build cuTile is to install it
in editable mode by running the following command in the source root directory:

```
pip install -e .
```

This will create the `build` directory and invoke the CMake-based build process.
In editable mode, the compiled extension module will be placed in the build directory,
and then a symbolic link to it will be created in the source directory.
This makes sure that the `pip install -e .` command above is needed only once, and recompiling
the extension after making changes to the C++ code can be done with `make -C build`
which is much faster. This logic is defined in [setup.py](./setup.py).

Running Tests
-------------
cuTile uses the [pytest](https://pytest.org) framework for testing.
Tests have extra dependencies, such as PyTorch, which can be installed with
```
pip install -r test/requirements.txt
```

The tests are located in the [test/](test/) directory. To run a specific test file,
for example `test_copy.py`, use the following command:
```
pytest test/test_copy.py
```

Copyright and License Information
---------------------------------
Copyright Â© 2025 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.

cuTile-Python is licensed under the Apache 2.0 license. See the [LICENSES](LICENSES/) folder for the full license text.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[srbhr/Resume-Matcher]]></title>
            <link>https://github.com/srbhr/Resume-Matcher</link>
            <guid>https://github.com/srbhr/Resume-Matcher</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:20 GMT</pubDate>
            <description><![CDATA[Improve your resumes with Resume Matcher. Get insights, keyword suggestions and tune your resumes to job descriptions.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/srbhr/Resume-Matcher">srbhr/Resume-Matcher</a></h1>
            <p>Improve your resumes with Resume Matcher. Get insights, keyword suggestions and tune your resumes to job descriptions.</p>
            <p>Language: Python</p>
            <p>Stars: 24,557</p>
            <p>Forks: 4,546</p>
            <p>Stars today: 323 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

[![Resume Matcher](assets/page_2.png)](https://www.resumematcher.fyi)

# Resume Matcher

[ğ™¹ğš˜ğš’ğš— ğ™³ğš’ğšœğšŒğš˜ğš›ğš](https://dsc.gg/resume-matcher) âœ¦ [ğš†ğšğš‹ğšœğš’ğšğš](https://resumematcher.fyi) âœ¦ [ğ™·ğš˜ğš  ğšğš˜ ğ™¸ğš—ğšœğšğšŠğš•ğš•](#how-to-install) âœ¦ [ğ™²ğš˜ğš—ğšğš›ğš’ğš‹ğšğšğš˜ğš›ğšœ](#contributors) âœ¦ [ğ™³ğš˜ğš—ğšŠğšğš](#support-the-development-by-donating) âœ¦ [ğšƒğš ğš’ğšğšğšğš›/ğš‡](https://twitter.com/ssrbhr) âœ¦ [ğ™»ğš’ğš—ğš”ğšğšğ™¸ğš—](https://www.linkedin.com/company/resume-matcher/)

**Stop getting auto-rejected by ATS bots.** Resume Matcher is the AI-powered platform that reverse-engineers hiring algorithms to show you exactly how to tailor your resume. Get the keywords, formatting, and insights that actually get you past the first screen and into human hands.

Hoping to make this, **VS Code for making resumes**.

&lt;/div&gt;

&lt;br&gt;

&lt;div align=&quot;center&quot;&gt;

![Stars](https://img.shields.io/github/stars/srbhr/Resume-Matcher?labelColor=black&amp;style=for-the-badge&amp;color=c20a71)
![Apache 2.0](https://img.shields.io/github/license/srbhr/Resume-Matcher?labelColor=black&amp;style=for-the-badge&amp;color=c20a71) ![Forks](https://img.shields.io/github/forks/srbhr/Resume-Matcher?labelColor=black&amp;style=for-the-badge&amp;color=c20a71) ![version](https://img.shields.io/badge/Version-0.1%20Veridis%20Quo-FFF?labelColor=black&amp;logo=LinkedIn&amp;style=for-the-badge&amp;color=c20a71)

[![Discord](https://img.shields.io/discord/1122069176962531400?labelColor=black&amp;logo=discord&amp;logoColor=c20a71&amp;style=for-the-badge&amp;color=c20a71)](https://dsc.gg/resume-matcher) [![Website](https://img.shields.io/badge/website-Resume%20Matcher-FFF?labelColor=black&amp;style=for-the-badge&amp;color=c20a71)](https://resumematcher.fyi) [![LinkedIn](https://img.shields.io/badge/LinkedIn-Resume%20Matcher-FFF?labelColor=black&amp;logo=LinkedIn&amp;style=for-the-badge&amp;color=c20a71)](https://www.linkedin.com/company/resume-matcher/)

&lt;a href=&quot;https://trendshift.io/repositories/565&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/565&quot; alt=&quot;srbhr%2FResume-Matcher | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

![Vercel OSS Program](https://vercel.com/oss/program-badge.svg)

&lt;/div&gt;

&gt; \[!IMPORTANT]
&gt;
&gt; This project is in active development. New features are being added continuously, and we welcome contributions from the community. There are some breaking changes on the `main` branch. If you have any suggestions or feature requests, please feel free to open an issue on GitHub or discuss it on our [Discord](https://dsc.gg/resume-matcher) server.

## Getting started with Resume Matcher

Resume Matcher is designed to help you optimize your resume with the aim to highlight your skills and experience in a way that resonates with potential employers.

We&#039;re actively working on improving the platform, building towards a **VS Code for making resumes**, and adding new features. The best way to stay updated is to join the Discord discussion and be part of the active development community.

&gt; Join our [Discord](https://dsc.gg/resume-matcher) community ğŸ‘‡
[![Discord](assets/resume_matcher_discord.png)](https://dsc.gg/resume-matcher)

&gt; Follow us on [LinkedIn](https://www.linkedin.com/company/resume-matcher/) âœ¨
[![LinkedIn](assets/resume_matcher_linkedin.png)](https://www.linkedin.com/company/resume-matcher/)

&gt; â­ Star Resume Matcher to support the development and get updates on GitHub.
![Star Resume Matcher](assets/star_resume_matcher.png)

## Key Features

![resume_matcher_features](assets/resume_matcher_features.png)

- **Works locally**: No need to upload your resume to a server. Everything runs on your machine with open source AI models by Ollama.
- **ATS Compatibility**: Get a detailed analysis of your resume&#039;s compatibility with ATS systems.
- **Instant Match Score**: Upload resume &amp; job description for a quick match score and key improvement areas.
- **Keyword Optimizer**: Align your resume with job keywords and identify critical content gaps.
- **Guided Improvements**: Get clear suggestions to make your resume stand out.

### Roadmap

If you have any suggestions or feature requests, please feel free to open an issue on GitHub. And discuss it on our [Discord](https://dsc.gg/resume-matcher) server.

- Visual keyword highlighting.
- AI Canvas, which can help to craft impactful, metric-driven resume content.
- Multi-job description optimization.

## How to Install

![Installation](assets/how_to_install_resumematcher.png)

Follow the instructions in the [SETUP.md](SETUP.md) file to set up the project locally. The setup script will install all the necessary dependencies and configure your environment.

The project is built using:

- FastAPI for the backend.
- Next.js for the frontend.
- Ollama for local AI model serving.
- Tailwind CSS for styling.
- SQLite for the database.

| Technology   | Info/Version                               |
|--------------|---------------------------------------|
| Python      | 3.12+                   |
| Next.js      | 15+                   |
| Ollama       |        0.6.7        |

## Join Us and Contribute

![how to contribute](assets/how_to_contribute.png)

We welcome contributions from everyone! Whether you&#039;re a developer, designer, or just someone who wants to help out. All the contributors are listed in the [about page](https://resumematcher.fyi/about) on our website and on the GitHub Readme here.

Check out the roadmap if you would like to work on the features that are planned for the future. If you have any suggestions or feature requests, please feel free to open an issue on GitHub and discuss it on our [Discord](https://dsc.gg/resume-matcher) server.

## Contributors

![Contributors](assets/contributors.png)

&lt;a href=&quot;https://github.com/srbhr/Resume-Matcher/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=srbhr/Resume-Matcher&quot; /&gt;
&lt;/a&gt;

## Support the Development by Donating

![donate](assets/supporting_resume_matcher.png)

If you would like to support the development of Resume Matcher, you can do so by donating. Your contributions will help us keep the project alive and continue adding new features.

| Platform  | Link                                   |
|-----------|----------------------------------------|
| GitHub    | [![GitHub Sponsors](https://img.shields.io/github/sponsors/srbhr?style=for-the-badge&amp;color=c20a71&amp;labelColor=black&amp;logo=github)](https://github.com/sponsors/srbhr) |
| Buy Me a Coffee | [![BuyMeACoffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=for-the-badge&amp;logo=buy-me-a-coffee&amp;color=c20a72&amp;logoColor=white)](https://www.buymeacoffee.com/srbhr) |

&lt;details&gt;
  &lt;summary&gt;&lt;kbd&gt;Star History&lt;/kbd&gt;&lt;/summary&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=srbhr/resume-matcher&amp;theme=dark&amp;type=Date&quot;&gt;
    &lt;img width=&quot;100%&quot; src=&quot;https://api.star-history.com/svg?repos=srbhr/resume-matcher&amp;theme=dark&amp;type=Date&quot;&gt;
  &lt;/picture&gt;
&lt;/details&gt;

## Resume Matcher is a part of [Vercel Open Source Program](https://vercel.com/oss)

![Vercel OSS Program](https://vercel.com/oss/program-badge.svg)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[strands-agents/sdk-python]]></title>
            <link>https://github.com/strands-agents/sdk-python</link>
            <guid>https://github.com/strands-agents/sdk-python</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:19 GMT</pubDate>
            <description><![CDATA[A model-driven approach to building AI agents in just a few lines of code.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/strands-agents/sdk-python">strands-agents/sdk-python</a></h1>
            <p>A model-driven approach to building AI agents in just a few lines of code.</p>
            <p>Language: Python</p>
            <p>Stars: 4,337</p>
            <p>Forks: 532</p>
            <p>Stars today: 43 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;div&gt;
    &lt;a href=&quot;https://strandsagents.com&quot;&gt;
      &lt;img src=&quot;https://strandsagents.com/latest/assets/logo-github.svg&quot; alt=&quot;Strands Agents&quot; width=&quot;55px&quot; height=&quot;105px&quot;&gt;
    &lt;/a&gt;
  &lt;/div&gt;

  &lt;h1&gt;
    Strands Agents
  &lt;/h1&gt;

  &lt;h2&gt;
    A model-driven approach to building AI agents in just a few lines of code.
  &lt;/h2&gt;

  &lt;div align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://github.com/strands-agents/sdk-python/graphs/commit-activity&quot;&gt;&lt;img alt=&quot;GitHub commit activity&quot; src=&quot;https://img.shields.io/github/commit-activity/m/strands-agents/sdk-python&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/strands-agents/sdk-python/issues&quot;&gt;&lt;img alt=&quot;GitHub open issues&quot; src=&quot;https://img.shields.io/github/issues/strands-agents/sdk-python&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/strands-agents/sdk-python/pulls&quot;&gt;&lt;img alt=&quot;GitHub open pull requests&quot; src=&quot;https://img.shields.io/github/issues-pr/strands-agents/sdk-python&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/strands-agents/sdk-python/blob/main/LICENSE&quot;&gt;&lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/github/license/strands-agents/sdk-python&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://pypi.org/project/strands-agents/&quot;&gt;&lt;img alt=&quot;PyPI version&quot; src=&quot;https://img.shields.io/pypi/v/strands-agents&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://python.org&quot;&gt;&lt;img alt=&quot;Python versions&quot; src=&quot;https://img.shields.io/pypi/pyversions/strands-agents&quot;/&gt;&lt;/a&gt;
  &lt;/div&gt;
  
  &lt;p&gt;
    &lt;a href=&quot;https://strandsagents.com/&quot;&gt;Documentation&lt;/a&gt;
    â—† &lt;a href=&quot;https://github.com/strands-agents/samples&quot;&gt;Samples&lt;/a&gt;
    â—† &lt;a href=&quot;https://github.com/strands-agents/sdk-python&quot;&gt;Python SDK&lt;/a&gt;
    â—† &lt;a href=&quot;https://github.com/strands-agents/tools&quot;&gt;Tools&lt;/a&gt;
    â—† &lt;a href=&quot;https://github.com/strands-agents/agent-builder&quot;&gt;Agent Builder&lt;/a&gt;
    â—† &lt;a href=&quot;https://github.com/strands-agents/mcp-server&quot;&gt;MCP Server&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

Strands Agents is a simple yet powerful SDK that takes a model-driven approach to building and running AI agents. From simple conversational assistants to complex autonomous workflows, from local development to production deployment, Strands Agents scales with your needs.

## Feature Overview

- **Lightweight &amp; Flexible**: Simple agent loop that just works and is fully customizable
- **Model Agnostic**: Support for Amazon Bedrock, Anthropic, Gemini, LiteLLM, Llama, Ollama, OpenAI, Writer, and custom providers
- **Advanced Capabilities**: Multi-agent systems, autonomous agents, and streaming support
- **Built-in MCP**: Native support for Model Context Protocol (MCP) servers, enabling access to thousands of pre-built tools

## Quick Start

```bash
# Install Strands Agents
pip install strands-agents strands-agents-tools
```

```python
from strands import Agent
from strands_tools import calculator
agent = Agent(tools=[calculator])
agent(&quot;What is the square root of 1764&quot;)
```

&gt; **Note**: For the default Amazon Bedrock model provider, you&#039;ll need AWS credentials configured and model access enabled for Claude 4 Sonnet in the us-west-2 region. See the [Quickstart Guide](https://strandsagents.com/) for details on configuring other model providers.

## Installation

Ensure you have Python 3.10+ installed, then:

```bash
# Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows use: .venv\Scripts\activate

# Install Strands and tools
pip install strands-agents strands-agents-tools
```

## Features at a Glance

### Python-Based Tools

Easily build tools using Python decorators:

```python
from strands import Agent, tool

@tool
def word_count(text: str) -&gt; int:
    &quot;&quot;&quot;Count words in text.

    This docstring is used by the LLM to understand the tool&#039;s purpose.
    &quot;&quot;&quot;
    return len(text.split())

agent = Agent(tools=[word_count])
response = agent(&quot;How many words are in this sentence?&quot;)
```

**Hot Reloading from Directory:**
Enable automatic tool loading and reloading from the `./tools/` directory:

```python
from strands import Agent

# Agent will watch ./tools/ directory for changes
agent = Agent(load_tools_from_directory=True)
response = agent(&quot;Use any tools you find in the tools directory&quot;)
```

### MCP Support

Seamlessly integrate Model Context Protocol (MCP) servers:

```python
from strands import Agent
from strands.tools.mcp import MCPClient
from mcp import stdio_client, StdioServerParameters

aws_docs_client = MCPClient(
    lambda: stdio_client(StdioServerParameters(command=&quot;uvx&quot;, args=[&quot;awslabs.aws-documentation-mcp-server@latest&quot;]))
)

with aws_docs_client:
   agent = Agent(tools=aws_docs_client.list_tools_sync())
   response = agent(&quot;Tell me about Amazon Bedrock and how to use it with Python&quot;)
```

### Multiple Model Providers

Support for various model providers:

```python
from strands import Agent
from strands.models import BedrockModel
from strands.models.ollama import OllamaModel
from strands.models.llamaapi import LlamaAPIModel
from strands.models.gemini import GeminiModel
from strands.models.llamacpp import LlamaCppModel

# Bedrock
bedrock_model = BedrockModel(
  model_id=&quot;us.amazon.nova-pro-v1:0&quot;,
  temperature=0.3,
  streaming=True, # Enable/disable streaming
)
agent = Agent(model=bedrock_model)
agent(&quot;Tell me about Agentic AI&quot;)

# Google Gemini
gemini_model = GeminiModel(
  client_args={
    &quot;api_key&quot;: &quot;your_gemini_api_key&quot;,
  },
  model_id=&quot;gemini-2.5-flash&quot;,
  params={&quot;temperature&quot;: 0.7}
)
agent = Agent(model=gemini_model)
agent(&quot;Tell me about Agentic AI&quot;)

# Ollama
ollama_model = OllamaModel(
  host=&quot;http://localhost:11434&quot;,
  model_id=&quot;llama3&quot;
)
agent = Agent(model=ollama_model)
agent(&quot;Tell me about Agentic AI&quot;)

# Llama API
llama_model = LlamaAPIModel(
    model_id=&quot;Llama-4-Maverick-17B-128E-Instruct-FP8&quot;,
)
agent = Agent(model=llama_model)
response = agent(&quot;Tell me about Agentic AI&quot;)
```

Built-in providers:
 - [Amazon Bedrock](https://strandsagents.com/latest/user-guide/concepts/model-providers/amazon-bedrock/)
 - [Anthropic](https://strandsagents.com/latest/user-guide/concepts/model-providers/anthropic/)
 - [Gemini](https://strandsagents.com/latest/user-guide/concepts/model-providers/gemini/)
 - [Cohere](https://strandsagents.com/latest/user-guide/concepts/model-providers/cohere/)
 - [LiteLLM](https://strandsagents.com/latest/user-guide/concepts/model-providers/litellm/)
 - [llama.cpp](https://strandsagents.com/latest/user-guide/concepts/model-providers/llamacpp/)
 - [LlamaAPI](https://strandsagents.com/latest/user-guide/concepts/model-providers/llamaapi/)
 - [MistralAI](https://strandsagents.com/latest/user-guide/concepts/model-providers/mistral/)
 - [Ollama](https://strandsagents.com/latest/user-guide/concepts/model-providers/ollama/)
 - [OpenAI](https://strandsagents.com/latest/user-guide/concepts/model-providers/openai/)
 - [SageMaker](https://strandsagents.com/latest/user-guide/concepts/model-providers/sagemaker/)
 - [Writer](https://strandsagents.com/latest/user-guide/concepts/model-providers/writer/)

Custom providers can be implemented using [Custom Providers](https://strandsagents.com/latest/user-guide/concepts/model-providers/custom_model_provider/)

### Example tools

Strands offers an optional strands-agents-tools package with pre-built tools for quick experimentation:

```python
from strands import Agent
from strands_tools import calculator
agent = Agent(tools=[calculator])
agent(&quot;What is the square root of 1764&quot;)
```

It&#039;s also available on GitHub via [strands-agents/tools](https://github.com/strands-agents/tools).

### Bidirectional Streaming

&gt; **âš ï¸ Experimental Feature**: Bidirectional streaming is currently in experimental status. APIs may change in future releases as we refine the feature based on user feedback and evolving model capabilities.

Build real-time voice and audio conversations with persistent streaming connections. Unlike traditional request-response patterns, bidirectional streaming maintains long-running conversations where users can interrupt, provide continuous input, and receive real-time audio responses. Get started with your first BidiAgent by following the [Quickstart](https://strandsagents.com/latest/documentation/docs/user-guide/concepts/experimental/bidirectional-streaming/quickstart) guide. 

**Supported Model Providers:**
- Amazon Nova Sonic (`amazon.nova-sonic-v1:0`)
- Google Gemini Live (`gemini-2.5-flash-native-audio-preview-09-2025`)
- OpenAI Realtime API (`gpt-realtime`)

**Quick Example:**

```python
import asyncio
from strands.experimental.bidi import BidiAgent
from strands.experimental.bidi.models import BidiNovaSonicModel
from strands.experimental.bidi.io import BidiAudioIO, BidiTextIO
from strands.experimental.bidi.tools import stop_conversation
from strands_tools import calculator

async def main():
    # Create bidirectional agent with audio model
    model = BidiNovaSonicModel()
    agent = BidiAgent(model=model, tools=[calculator, stop_conversation])

    # Setup audio and text I/O
    audio_io = BidiAudioIO()
    text_io = BidiTextIO()

    # Run with real-time audio streaming
    # Say &quot;stop conversation&quot; to gracefully end the conversation
    await agent.run(
        inputs=[audio_io.input()],
        outputs=[audio_io.output(), text_io.output()]
    )

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

**Configuration Options:**

```python
# Configure audio settings
model = BidiNovaSonicModel(
    provider_config={
        &quot;audio&quot;: {
            &quot;input_rate&quot;: 16000,
            &quot;output_rate&quot;: 16000,
            &quot;voice&quot;: &quot;matthew&quot;
        },
        &quot;inference&quot;: {
            &quot;max_tokens&quot;: 2048,
            &quot;temperature&quot;: 0.7
        }
    }
)

# Configure I/O devices
audio_io = BidiAudioIO(
    input_device_index=0,  # Specific microphone
    output_device_index=1,  # Specific speaker
    input_buffer_size=10,
    output_buffer_size=10
)
```

## Documentation

For detailed guidance &amp; examples, explore our documentation:

- [User Guide](https://strandsagents.com/)
- [Quick Start Guide](https://strandsagents.com/latest/user-guide/quickstart/)
- [Agent Loop](https://strandsagents.com/latest/user-guide/concepts/agents/agent-loop/)
- [Examples](https://strandsagents.com/latest/examples/)
- [API Reference](https://strandsagents.com/latest/api-reference/agent/)
- [Production &amp; Deployment Guide](https://strandsagents.com/latest/user-guide/deploy/operating-agents-in-production/)

## Contributing â¤ï¸

We welcome contributions! See our [Contributing Guide](CONTRIBUTING.md) for details on:
- Reporting bugs &amp; features
- Development setup
- Contributing via Pull Requests
- Code of Conduct
- Reporting of security issues

## License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[anthropics/claude-quickstarts]]></title>
            <link>https://github.com/anthropics/claude-quickstarts</link>
            <guid>https://github.com/anthropics/claude-quickstarts</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:18 GMT</pubDate>
            <description><![CDATA[A collection of projects designed to help developers quickly get started with building deployable applications using the Claude API]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/anthropics/claude-quickstarts">anthropics/claude-quickstarts</a></h1>
            <p>A collection of projects designed to help developers quickly get started with building deployable applications using the Claude API</p>
            <p>Language: Python</p>
            <p>Stars: 11,827</p>
            <p>Forks: 2,061</p>
            <p>Stars today: 108 stars today</p>
            <h2>README</h2><pre># Claude Quickstarts

Claude Quickstarts is a collection of projects designed to help developers quickly get started with building  applications using the Claude API. Each quickstart provides a foundation that you can easily build upon and customize for your specific needs.

## Getting Started

To use these quickstarts, you&#039;ll need an Claude API key. If you don&#039;t have one yet, you can sign up for free at [console.anthropic.com](https://console.anthropic.com).

## Available Quickstarts

### Customer Support Agent

A customer support agent powered by Claude. This project demonstrates how to leverage Claude&#039;s natural language understanding and generation capabilities to create an AI-assisted customer support system with access to a knowledge base.

[Go to Customer Support Agent Quickstart](./customer-support-agent)

### Financial Data Analyst

A financial data analyst powered by Claude. This project demonstrates how to leverage Claude&#039;s capabilities with interactive data visualization to analyze financial data via chat.

[Go to Financial Data Analyst Quickstart](./financial-data-analyst)

### Computer Use Demo

An environment and tools that Claude can use to control a desktop computer. This project demonstrates how to leverage the computer use capabilities of Claude, including support for the latest `computer_use_20251124` tool version with zoom actions.

[Go to Computer Use Demo Quickstart](./computer-use-demo)

### Autonomous Coding Agent

An autonomous coding agent powered by the Claude Agent SDK. This project demonstrates a two-agent pattern (initializer + coding agent) that can build complete applications over multiple sessions, with progress persisted via git and a feature list that the agent works through incrementally.

[Go to Autonomous Coding Agent Quickstart](./autonomous-coding)

## General Usage

Each quickstart project comes with its own README and setup instructions. Generally, you&#039;ll follow these steps:

1. Clone this repository
2. Navigate to the specific quickstart directory
3. Install the required dependencies
4. Set up your Claude API key as an environment variable
5. Run the quickstart application

## Explore Further

To deepen your understanding of working with Claude and the Claude API, check out these resources:

- [Claude API Documentation](https://docs.claude.com)
- [Claude Cookbooks](https://github.com/anthropics/claude-cookbooks) - A collection of code snippets and guides for common tasks
- [Claude API Fundamentals Course](https://github.com/anthropics/courses/tree/master/anthropic_api_fundamentals)

## Contributing

We welcome contributions to the Claude Quickstarts repository! If you have ideas for new quickstart projects or improvements to existing ones, please open an issue or submit a pull request.

## Community and Support

- Join our [Anthropic Discord community](https://www.anthropic.com/discord) for discussions and support
- Check out the [Anthropic support documentation](https://support.anthropic.com) for additional help

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[newren/git-filter-repo]]></title>
            <link>https://github.com/newren/git-filter-repo</link>
            <guid>https://github.com/newren/git-filter-repo</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:17 GMT</pubDate>
            <description><![CDATA[Quickly rewrite git repository history (filter-branch replacement)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/newren/git-filter-repo">newren/git-filter-repo</a></h1>
            <p>Quickly rewrite git repository history (filter-branch replacement)</p>
            <p>Language: Python</p>
            <p>Stars: 11,282</p>
            <p>Forks: 892</p>
            <p>Stars today: 8 stars today</p>
            <h2>README</h2><pre>git filter-repo is a versatile tool for rewriting history, which includes
[capabilities I have not found anywhere
else](#design-rationale-behind-filter-repo).  It roughly falls into the
same space of tool as [git
filter-branch](https://git-scm.com/docs/git-filter-branch) but without the
capitulation-inducing poor
[performance](https://public-inbox.org/git/CABPp-BGOz8nks0+Tdw5GyGqxeYR-3FF6FT5JcgVqZDYVRQ6qog@mail.gmail.com/),
with far more capabilities, and with a design that scales usability-wise
beyond trivial rewriting cases.  [git filter-repo is now recommended by the
git project](https://git-scm.com/docs/git-filter-branch#_warning) instead
of git filter-branch.

While most users will probably just use filter-repo as a simple command
line tool (and likely only use a few of its flags), at its core filter-repo
contains a library for creating history rewriting tools.  As such, users
with specialized needs can leverage it to quickly create [entirely new
history rewriting tools](contrib/filter-repo-demos).

# Table of Contents

  * [Prerequisites](#prerequisites)
  * [How do I install it?](#how-do-i-install-it)
  * [How do I use it?](#how-do-i-use-it)
  * [Why filter-repo instead of other alternatives?](#why-filter-repo-instead-of-other-alternatives)
    * [filter-branch](#filter-branch)
    * [BFG Repo Cleaner](#bfg-repo-cleaner)
  * [Simple example, with comparisons](#simple-example-with-comparisons)
    * [Solving this with filter-repo](#solving-this-with-filter-repo)
    * [Solving this with BFG Repo Cleaner](#solving-this-with-bfg-repo-cleaner)
    * [Solving this with filter-branch](#solving-this-with-filter-branch)
    * [Solving this with fast-export/fast-import](#solving-this-with-fast-exportfast-import)
  * [Design rationale behind filter-repo](#design-rationale-behind-filter-repo)
  * [How do I contribute?](#how-do-i-contribute)
  * [Is there a Code of Conduct?](#is-there-a-code-of-conduct)
  * [Upstream Improvements](#upstream-improvements)

# Prerequisites

filter-repo requires:

  * git &gt;= 2.36.0
  * python3 &gt;= 3.6

# How do I install it?

While the `git-filter-repo` repository has many files, the main logic
is all contained in a single-file python script named
`git-filter-repo`, which was done to make installation for basic use
on many systems trivial: just place that one file into your $PATH.

See [INSTALL.md](INSTALL.md) for things beyond basic usage or special
cases.  The more involved instructions are only needed if one of the
following apply:

  * you do not find the above comment about trivial installation intuitively
    obvious
  * you are working with a python3 executable named something other than
    &quot;python3&quot;
  * you want to install documentation (beyond the builtin docs shown with -h)
  * you want to run some of the [contrib](contrib/filter-repo-demos/) examples
  * you want to create your own python filtering scripts using filter-repo as
    a module/library

# How do I use it?

For comprehensive documentation:
  * see the [user manual](https://htmlpreview.github.io/?https://github.com/newren/git-filter-repo/blob/docs/html/git-filter-repo.html)
  * alternative formating of the user manual is available on various
    external sites
    ([example](https://www.mankier.com/1/git-filter-repo)), for those
    that don&#039;t like the htmlpreview.github.io layout, though it may
    only be up-to-date as of the latest release

If you prefer learning from examples:
  * there is a [cheat sheet for converting filter-branch
    commands](Documentation/converting-from-filter-branch.md#cheat-sheet-conversion-of-examples-from-the-filter-branch-manpage),
    which covers every example from the filter-branch manual
  * there is a [cheat sheet for converting BFG Repo Cleaner
    commands](Documentation/converting-from-bfg-repo-cleaner.md#cheat-sheet-conversion-of-examples-from-bfg),
    which covers every example from the BFG website
  * the [simple example](#simple-example-with-comparisons) below may
    be of interest
  * the user manual has an extensive [examples
section](https://htmlpreview.github.io/?https://github.com/newren/git-filter-repo/blob/docs/html/git-filter-repo.html#EXAMPLES)
  * I have collected a set of [example filterings based on user-filed issues](Documentation/examples-from-user-filed-issues.md)

In either case, you may also find the [Frequently Answered Questions](Documentation/FAQ.md) useful.

# Why filter-repo instead of other alternatives?

This was covered in more detail in a [Git Rev News article on
filter-repo](https://git.github.io/rev_news/2019/08/21/edition-54/#an-introduction-to-git-filter-repo--written-by-elijah-newren),
but some highlights for the main competitors:

## filter-branch

  * filter-branch is [extremely to unusably
    slow](https://public-inbox.org/git/CABPp-BGOz8nks0+Tdw5GyGqxeYR-3FF6FT5JcgVqZDYVRQ6qog@mail.gmail.com/)
    ([multiple orders of magnitude slower than it should
    be](https://git-scm.com/docs/git-filter-branch#PERFORMANCE))
    for non-trivial repositories.

  * [filter-branch is riddled with
    gotchas](https://git-scm.com/docs/git-filter-branch#SAFETY) that can
    silently corrupt your rewrite or at least thwart your &quot;cleanup&quot;
    efforts by giving you something more problematic and messy than what
    you started with.

  * filter-branch is [very onerous](#simple-example-with-comparisons)
    [to
    use](https://github.com/newren/git-filter-repo/blob/a6a6a1b0f62d365bbe2e76f823e1621857ec4dbd/contrib/filter-repo-demos/filter-lamely#L9-L61)
    for any rewrite which is even slightly non-trivial.

  * the git project has stated that the above issues with filter-branch
    cannot be backward compatibly fixed; they recommend that you [stop
    using
    filter-branch](https://git-scm.com/docs/git-filter-branch#_warning)

  * die-hard fans of filter-branch may be interested in
    [filter-lamely](contrib/filter-repo-demos/filter-lamely)
    (a.k.a. [filter-branch-ish](contrib/filter-repo-demos/filter-branch-ish)),
    a reimplementation of filter-branch based on filter-repo which is
    more performant (though not nearly as fast or safe as
    filter-repo).

  * a [cheat
    sheet](Documentation/converting-from-filter-branch.md#cheat-sheet-conversion-of-examples-from-the-filter-branch-manpage)
    is available showing how to convert example commands from the manual of
    filter-branch into filter-repo commands.

## BFG Repo Cleaner

  * great tool for its time, but while it makes some things simple, it
    is limited to a few kinds of rewrites.

  * its architecture is not amenable to handling more types of
    rewrites.

  * its architecture presents some shortcomings and bugs even for its
    intended usecase.

  * fans of bfg may be interested in
    [bfg-ish](contrib/filter-repo-demos/bfg-ish), a reimplementation of bfg
    based on filter-repo which includes several new features and bugfixes
    relative to bfg.

  * a [cheat
    sheet](Documentation/converting-from-bfg-repo-cleaner.md#cheat-sheet-conversion-of-examples-from-bfg)
    is available showing how to convert example commands from the manual of
    BFG Repo Cleaner into filter-repo commands.

# Simple example, with comparisons

Let&#039;s say that we want to extract a piece of a repository, with the intent
on merging just that piece into some other bigger repo.  For extraction, we
want to:

  * extract the history of a single directory, src/.  This means that only
    paths under src/ remain in the repo, and any commits that only touched
    paths outside this directory will be removed.
  * rename all files to have a new leading directory, my-module/ (e.g. so that
    src/foo.c becomes my-module/src/foo.c)
  * rename any tags in the extracted repository to have a &#039;my-module-&#039;
    prefix (to avoid any conflicts when we later merge this repo into
    something else)

## Solving this with filter-repo

Doing this with filter-repo is as simple as the following command:
```shell
  git filter-repo --path src/ --to-subdirectory-filter my-module --tag-rename &#039;&#039;:&#039;my-module-&#039;
```
(the single quotes are unnecessary, but make it clearer to a human that we
are replacing the empty string as a prefix with `my-module-`)

## Solving this with BFG Repo Cleaner

BFG Repo Cleaner is not capable of this kind of rewrite; in fact, all
three types of wanted changes are outside of its capabilities.

## Solving this with filter-branch

filter-branch comes with a pile of caveats (more on that below) even
once you figure out the necessary invocation(s):

```shell
  git filter-branch \
      --tree-filter &#039;mkdir -p my-module &amp;&amp; \
                     git ls-files \
                         | grep -v ^src/ \
                         | xargs git rm -f -q &amp;&amp; \
                     ls -d * \
                         | grep -v my-module \
                         | xargs -I files mv files my-module/&#039; \
          --tag-name-filter &#039;echo &quot;my-module-$(cat)&quot;&#039; \
	  --prune-empty -- --all
  git clone file://$(pwd) newcopy
  cd newcopy
  git for-each-ref --format=&quot;delete %(refname)&quot; refs/tags/ \
      | grep -v refs/tags/my-module- \
      | git update-ref --stdin
  git gc --prune=now
```

Some might notice that the above filter-branch invocation will be really
slow due to using --tree-filter; you could alternatively use the
--index-filter option of filter-branch, changing the above commands to:

```shell
  git filter-branch \
      --index-filter &#039;git ls-files \
                          | grep -v ^src/ \
                          | xargs git rm -q --cached;
                      git ls-files -s \
                          | sed &quot;s%$(printf \\t)%&amp;my-module/%&quot; \
                          | git update-index --index-info;
                      git ls-files \
                          | grep -v ^my-module/ \
                          | xargs git rm -q --cached&#039; \
      --tag-name-filter &#039;echo &quot;my-module-$(cat)&quot;&#039; \
      --prune-empty -- --all
  git clone file://$(pwd) newcopy
  cd newcopy
  git for-each-ref --format=&quot;delete %(refname)&quot; refs/tags/ \
      | grep -v refs/tags/my-module- \
      | git update-ref --stdin
  git gc --prune=now
```

However, for either filter-branch command there are a pile of caveats.
First, some may be wondering why I list five commands here for
filter-branch.  Despite the use of --all and --tag-name-filter, and
filter-branch&#039;s manpage claiming that a clone is enough to get rid of
old objects, the extra steps to delete the other tags and do another
gc are still required to clean out the old objects and avoid mixing
new and old history before pushing somewhere.  Other caveats:
  * Commit messages are not rewritten; so if some of your commit
    messages refer to prior commits by (abbreviated) sha1, after the
    rewrite those messages will now refer to commits that are no longer
    part of the history.  It would be better to rewrite those
    (abbreviated) sha1 references to refer to the new commit ids.
  * The --prune-empty flag sometimes misses commits that should be
    pruned, and it will also prune commits that *started* empty rather
    than just ended empty due to filtering.  For repositories that
    intentionally use empty commits for versioning and publishing
    related purposes, this can be detrimental.
  * The commands above are OS-specific.  GNU vs. BSD issues for sed,
    xargs, and other commands often trip up users; I think I failed to
    get most folks to use --index-filter since the only example in the
    filter-branch manpage that both uses it and shows how to move
    everything into a subdirectory is linux-specific, and it is not
    obvious to the reader that it has a portability issue since it
    silently misbehaves rather than failing loudly.
  * The --index-filter version of the filter-branch command may be two to
    three times faster than the --tree-filter version, but both
    filter-branch commands are going to be multiple orders of magnitude
    slower than filter-repo.
  * Both commands assume all filenames are composed entirely of ascii
    characters (even special ascii characters such as tabs or double
    quotes will wreak havoc and likely result in missing files or
    misnamed files)

## Solving this with fast-export/fast-import

One can kind of hack this together with something like:

```shell
  git fast-export --no-data --reencode=yes --mark-tags --fake-missing-tagger \
      --signed-tags=strip --tag-of-filtered-object=rewrite --all \
      | grep -vP &#039;^M [0-9]+ [0-9a-f]+ (?!src/)&#039; \
      | grep -vP &#039;^D (?!src/)&#039; \
      | perl -pe &#039;s%^(M [0-9]+ [0-9a-f]+ )(.*)$%\1my-module/\2%&#039; \
      | perl -pe &#039;s%^(D )(.*)$%\1my-module/\2%&#039; \
      | perl -pe s%refs/tags/%refs/tags/my-module-% \
      | git -c core.ignorecase=false fast-import --date-format=raw-permissive \
            --force --quiet
  git for-each-ref --format=&quot;delete %(refname)&quot; refs/tags/ \
      | grep -v refs/tags/my-module- \
      | git update-ref --stdin
  git reset --hard
  git reflog expire --expire=now --all
  git gc --prune=now
```

But this comes with some nasty caveats and limitations:
  * The various greps and regex replacements operate on the entire
    fast-export stream and thus might accidentally corrupt unintended
    portions of it, such as commit messages.  If you needed to edit
    file contents and thus dropped the --no-data flag, it could also
    end up corrupting file contents.
  * This command assumes all filenames in the repository are composed
    entirely of ascii characters, and also exclude special characters
    such as tabs or double quotes.  If such a special filename exists
    within the old src/ directory, it will be pruned even though it
    was intended to be kept.  (In slightly different repository
    rewrites, this type of editing also risks corrupting filenames
    with special characters by adding extra double quotes near the end
    of the filename and in some leading directory name.)
  * This command will leave behind huge numbers of useless empty
    commits, and has no realistic way of pruning them.  (And if you
    tried to combine this technique with another tool to prune the
    empty commits, then you now have no way to distinguish between
    commits which were made empty by the filtering that you want to
    remove, and commits which were empty before the filtering process
    and which you thus may want to keep.)
  * Commit messages which reference other commits by hash will now
    reference old commits that no longer exist.  Attempting to edit
    the commit messages to update them is extraordinarily difficult to
    add to this kind of direct rewrite.

# Design rationale behind filter-repo

None of the existing repository filtering tools did what I wanted;
they all came up short for my needs. No tool provided any of the
first eight traits below I wanted, and no tool provided more than
two of the last four traits either:

  1. [Starting report] Provide user an analysis of their repo to help
     them get started on what to prune or rename, instead of expecting
     them to guess or find other tools to figure it out.  (Triggered, e.g.
     by running the first time with a special flag, such as --analyze.)

  1. [Keep vs. remove] Instead of just providing a way for users to
     easily remove selected paths, also provide flags for users to
     only *keep* certain paths.  Sure, users could workaround this by
     specifying to remove all paths other than the ones they want to
     keep, but the need to specify all paths that *ever* existed in
     **any** version of the repository could sometimes be quite
     painful.  For filter-branch, using pipelines like `git ls-files |
     grep -v ... | xargs -r git rm` might be a reasonable workaround
     but can get unwieldy and isn&#039;t as straightforward for users; plus
     those commands are often operating-system specific (can you spot
     the GNUism in the snippet I provided?).

  1. [Renaming] It should be easy to rename paths.  For example, in
     addition to allowing one to treat some subdirectory as the root
     of the repository, also provide options for users to make the
     root of the repository just become a subdirectory.  And more
     generally allow files and directories to be easily renamed.
     Provide sanity checks if renaming causes multiple files to exist
     at the same path.  (And add special handling so that if a commit
     merely copied oldname-&gt;newname without modification, then
     filtering oldname-&gt;newname doesn&#039;t trigger the sanity check and
     die on that commit.)

  1. [More intelligent safety] Writing copies of the original refs to
     a special namespace within the repo does not provide a
     user-friendly recovery mechanism.  Many would struggle to recover
     using that.  Almost everyone I&#039;ve ever seen do a repository
     filtering operation has done so with a fresh clone, because
     wiping out the clone in case of error is a vastly easier recovery
     mechanism.  Strongly encourage that workflow by [detecting and
     bailing if we&#039;re not in a fresh
     clone](https://htmlpreview.github.io/?https://github.com/newren/git-filter-repo/blob/docs/html/git-filter-repo.html#FRESHCLONE),
     unless the user overrides with --force.

  1. [Auto shrink] Automatically remove old cruft and repack the
     repository for the user after filtering (unless overridden); this
     simplifies things for the user, helps avoid mixing old and new
     history together, and avoids problems where the multi-step
     process for shrinking the repo documented in the manpage doesn&#039;t
     actually work in some cases.  (I&#039;m looking at you,
     filter-branch.)

  1. [Clean separation] Avoid confusing users (and prevent accidental
     re-pushing of old stuff) due to mixing old repo and rewritten
     repo together.  (This is particularly a problem with filter-branch
     when using the --tag-name-filter option, and sometimes also an
     issue when only filtering a subset of branches.)

  1. [Versatility] Provide the user the ability to extend the tool or
     even write new tools that leverage existing capabilities, and
     provide this extensibility in a way that (a) avoids the need to
     fork separate processes (which would destroy performance), (b)
     avoids making the user specify OS-dependent shell commands (which
     would prevent users from sharing commands with each other), (c)
     takes advantage of rich data structures (because hashes, dicts,
     lists, and arrays are prohibitively difficult in shell) and (d)
     provides reasonable string manipulation capabilities (which are
     sorely lacking in shell).

  1. [Old commit references] Provide a way for users to use old commit
     IDs with the new repository (in particular via mapping from old to
     new hashes with refs/replace/ references).

  1. [Commit message consistency] If commit messages refer to other
     commits by ID (e.g. &quot;this reverts commit 01234567890abcdef&quot;, &quot;In
     commit 0013deadbeef9a...&quot;), those commit messages should be
     rewritten to refer to the new commit IDs.

  1. [Become-empty pruning] Commits which become empty due to filtering
     should be pruned.  If the parent of a commit is pruned, the first
     non-pruned ancestor needs to become the new parent.  If no
     non-pruned ancestor exists and the commit was not a merge, then it
     becomes a new root commit.  If no non-pruned ancestor exists and
     the commit was a merge, then the merge will have one less parent
     (and thus make it likely to become a non-merge commit which would
     itself be pruned if it had no file changes of its own).  One
     special thing to note here is that we prune commits which become
     empty, NOT commits which start empty.  Some projects intentionally
     create empty commits for versioning or publishing reasons, and
     these should not be removed.  (As a special case, commits which
     started empty but whose parent was pruned away will also be
     considered to have &quot;become empt

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[vllm-project/vllm]]></title>
            <link>https://github.com/vllm-project/vllm</link>
            <guid>https://github.com/vllm-project/vllm</guid>
            <pubDate>Thu, 11 Dec 2025 00:04:16 GMT</pubDate>
            <description><![CDATA[A high-throughput and memory-efficient inference and serving engine for LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/vllm-project/vllm">vllm-project/vllm</a></h1>
            <p>A high-throughput and memory-efficient inference and serving engine for LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 65,091</p>
            <p>Forks: 11,872</p>
            <p>Stars today: 97 stars today</p>
            <h2>README</h2><pre>&lt;!-- markdownlint-disable MD001 MD041 --&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-dark.png&quot;&gt;
    &lt;img alt=&quot;vLLM&quot; src=&quot;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-light.png&quot; width=55%&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;h3 align=&quot;center&quot;&gt;
Easy, fast, and cheap LLM serving for everyone
&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
| &lt;a href=&quot;https://docs.vllm.ai&quot;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://blog.vllm.ai/&quot;&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://arxiv.org/abs/2309.06180&quot;&gt;&lt;b&gt;Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://x.com/vllm_project&quot;&gt;&lt;b&gt;Twitter/X&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://discuss.vllm.ai&quot;&gt;&lt;b&gt;User Forum&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://slack.vllm.ai&quot;&gt;&lt;b&gt;Developer Slack&lt;/b&gt;&lt;/a&gt; |
&lt;/p&gt;

---
Join us at the [PyTorch Conference, October 22-23](https://events.linuxfoundation.org/pytorch-conference/) and [Ray Summit, November 3-5](https://www.anyscale.com/ray-summit/2025) in San Francisco for our latest updates on vLLM and to meet the vLLM team! Register now for the largest vLLM community events of the year!

---

*Latest News* ğŸ”¥

- [2025/11] We hosted [vLLM Bangkok Meetup](https://luma.com/v0f647nv). We explored vLLM and LMCache inference and low-resource language adaptation with speakers from Embedded LLM, AMD, and Red Hat. Please find the meetup slides [here](https://drive.google.com/drive/folders/1H0DS57F8HQ5q3kSOSoRmucPJWL3E0A_X?usp=sharing).
- [2025/11] We hosted [the first vLLM Europe Meetup in Zurich](https://luma.com/0gls27kb) focused on quantization, distributed inference, and reinforcement learning at scale with speakers from Mistral, IBM, and Red Hat. Please find the meetup slides [here](https://docs.google.com/presentation/d/1UC9PTLCHYXQpOmJDSFg6Sljra3iVXzc09DeEI7dnxMc/edit?usp=sharing) and recording [here](https://www.youtube.com/watch?v=6m6ZE6yVEDI)
- [2025/11] We hosted [vLLM Beijing Meetup](https://mp.weixin.qq.com/s/xSrYXjNgr1HbCP4ExYNG1w) focusing on distributed inference and diverse accelerator support with vLLM! Please find the meetup slides [here](https://drive.google.com/drive/folders/1nQJ8ZkLSjKxvu36sSHaceVXtttbLvvu-?usp=drive_link).
- [2025/10] We hosted [vLLM Shanghai Meetup](https://mp.weixin.qq.com/s/__xb4OyOsImz-9eAVrdlcg) focused on hands-on vLLM inference optimization! Please find the meetup slides [here](https://drive.google.com/drive/folders/1KqwjsFJLfEsC8wlDugnrR61zsWHt94Q6).
- [2025/09] We hosted [vLLM Toronto Meetup](https://luma.com/e80e0ymm) focused on tackling inference at scale and speculative decoding with speakers from NVIDIA and Red Hat! Please find the meetup slides [here](https://docs.google.com/presentation/d/1IYJYmJcu9fLpID5N5RbW_vO0XLo0CGOR14IXOjB61V8/edit?usp=sharing).
- [2025/08] We hosted [vLLM Shenzhen Meetup](https://mp.weixin.qq.com/s/k8ZBO1u2_2odgiKWH_GVTQ) focusing on the ecosystem around vLLM! Please find the meetup slides [here](https://drive.google.com/drive/folders/1Ua2SVKVSu-wp5vou_6ElraDt2bnKhiEA).
- [2025/08] We hosted [vLLM Singapore Meetup](https://www.sginnovate.com/event/vllm-sg-meet). We shared V1 updates, disaggregated serving and MLLM speedups with speakers from Embedded LLM, AMD, WekaIO, and A*STAR. Please find the meetup slides [here](https://drive.google.com/drive/folders/1ncf3GyqLdqFaB6IeB834E5TZJPLAOiXZ?usp=sharing).
- [2025/08] We hosted [vLLM Shanghai Meetup](https://mp.weixin.qq.com/s/pDmAXHcN7Iqc8sUKgJgGtg) focusing on building, developing, and integrating with vLLM! Please find the meetup slides [here](https://drive.google.com/drive/folders/1OvLx39wnCGy_WKq8SiVKf7YcxxYI3WCH).
- [2025/05] vLLM is now a hosted project under PyTorch Foundation! Please find the announcement [here](https://pytorch.org/blog/pytorch-foundation-welcomes-vllm/).
- [2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post [here](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html).

&lt;details&gt;
&lt;summary&gt;Previous News&lt;/summary&gt;

- [2025/08] We hosted [vLLM Korea Meetup](https://luma.com/cgcgprmh) with Red Hat and Rebellions! We shared the latest advancements in vLLM along with project spotlights from the vLLM Korea community. Please find the meetup slides [here](https://drive.google.com/file/d/1bcrrAE1rxUgx0mjIeOWT6hNe2RefC5Hm/view).
- [2025/08] We hosted [vLLM Beijing Meetup](https://mp.weixin.qq.com/s/dgkWg1WFpWGO2jCdTqQHxA) focusing on large-scale LLM deployment! Please find the meetup slides [here](https://drive.google.com/drive/folders/1Pid6NSFLU43DZRi0EaTcPgXsAzDvbBqF) and the recording [here](https://www.chaspark.com/#/live/1166916873711665152).
- [2025/05] We hosted [NYC vLLM Meetup](https://lu.ma/c1rqyf1f)! Please find the meetup slides [here](https://docs.google.com/presentation/d/1_q_aW_ioMJWUImf1s1YM-ZhjXz8cUeL0IJvaquOYBeA/edit?usp=sharing).
- [2025/04] We hosted [Asia Developer Day](https://www.sginnovate.com/event/limited-availability-morning-evening-slots-remaining-inaugural-vllm-asia-developer-day)! Please find the meetup slides from the vLLM team [here](https://docs.google.com/presentation/d/19cp6Qu8u48ihB91A064XfaXruNYiBOUKrBxAmDOllOo/edit?usp=sharing).
- [2025/03] We hosted [vLLM x Ollama Inference Night](https://lu.ma/vllm-ollama)! Please find the meetup slides from the vLLM team [here](https://docs.google.com/presentation/d/16T2PDD1YwRnZ4Tu8Q5r6n53c5Lr5c73UV9Vd2_eBo4U/edit?usp=sharing).
- [2025/03] We hosted [the first vLLM China Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg)! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1REHvfQMKGnvz6p3Fd23HhSO4c8j5WPGZV0bKYLwnHyQ/edit?usp=sharing).
- [2025/03] We hosted [the East Coast vLLM Meetup](https://lu.ma/7mu4k4xx)! Please find the meetup slides [here](https://docs.google.com/presentation/d/1NHiv8EUFF1NLd3fEYODm56nDmL26lEeXCaDgyDlTsRs/edit#slide=id.g31441846c39_0_0).
- [2025/02] We hosted [the ninth vLLM meetup](https://lu.ma/h7g3kuj9) with Meta! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1jzC_PZVXrVNSFVCW-V4cFXb6pn7zZ2CyP_Flwo05aqg/edit?usp=sharing) and AMD [here](https://drive.google.com/file/d/1Zk5qEJIkTmlQ2eQcXQZlljAx3m9s7nwn/view?usp=sharing). The slides from Meta will not be posted.
- [2025/01] We hosted [the eighth vLLM meetup](https://lu.ma/zep56hui) with Google Cloud! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing), and Google Cloud team [here](https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing).
- [2024/12] vLLM joins [pytorch ecosystem](https://pytorch.org/blog/vllm-joins-pytorch)! Easy, Fast, and Cheap LLM Serving for Everyone!
- [2024/11] We hosted [the seventh vLLM meetup](https://lu.ma/h0qvrajz) with Snowflake! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing), and Snowflake team [here](https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing).
- [2024/10] We have just created a developer slack ([slack.vllm.ai](https://slack.vllm.ai)) focusing on coordinating contributions and discussing features. Please feel free to join us there!
- [2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team [here](https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing). Learn more from the [talks](https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR) from other vLLM contributors and users!
- [2024/09] We hosted [the sixth vLLM meetup](https://lu.ma/87q3nvnh) with NVIDIA! Please find the meetup slides [here](https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing).
- [2024/07] We hosted [the fifth vLLM meetup](https://lu.ma/lp0gyjqr) with AWS! Please find the meetup slides [here](https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing).
- [2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post [here](https://blog.vllm.ai/2024/07/23/llama31.html).
- [2024/06] We hosted [the fourth vLLM meetup](https://lu.ma/agivllm) with Cloudflare and BentoML! Please find the meetup slides [here](https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing).
- [2024/04] We hosted [the third vLLM meetup](https://robloxandvllmmeetup2024.splashthat.com/) with Roblox! Please find the meetup slides [here](https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing).
- [2024/01] We hosted [the second vLLM meetup](https://lu.ma/ygxbpzhl) with IBM! Please find the meetup slides [here](https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing).
- [2023/10] We hosted [the first vLLM meetup](https://lu.ma/first-vllm-meetup) with a16z! Please find the meetup slides [here](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing).
- [2023/08] We would like to express our sincere gratitude to [Andreessen Horowitz](https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/) (a16z) for providing a generous grant to support the open-source development and research of vLLM.
- [2023/06] We officially released vLLM! FastChat-vLLM integration has powered [LMSYS Vicuna and Chatbot Arena](https://chat.lmsys.org) since mid-April. Check out our [blog post](https://vllm.ai).

&lt;/details&gt;

---

## About

vLLM is a fast and easy-to-use library for LLM inference and serving.

Originally developed in the [Sky Computing Lab](https://sky.cs.berkeley.edu) at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.

vLLM is fast with:

- State-of-the-art serving throughput
- Efficient management of attention key and value memory with [**PagedAttention**](https://blog.vllm.ai/2023/06/20/vllm.html)
- Continuous batching of incoming requests
- Fast model execution with CUDA/HIP graph
- Quantizations: [GPTQ](https://arxiv.org/abs/2210.17323), [AWQ](https://arxiv.org/abs/2306.00978), [AutoRound](https://arxiv.org/abs/2309.05516), INT4, INT8, and FP8
- Optimized CUDA kernels, including integration with FlashAttention and FlashInfer
- Speculative decoding
- Chunked prefill

vLLM is flexible and easy to use with:

- Seamless integration with popular Hugging Face models
- High-throughput serving with various decoding algorithms, including *parallel sampling*, *beam search*, and more
- Tensor, pipeline, data and expert parallelism support for distributed inference
- Streaming outputs
- OpenAI-compatible API server
- Support for NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, Arm CPUs, and TPU. Additionally, support for diverse hardware plugins such as Intel Gaudi, IBM Spyre and Huawei Ascend.
- Prefix caching support
- Multi-LoRA support

vLLM seamlessly supports most popular open-source models on HuggingFace, including:

- Transformer-like LLMs (e.g., Llama)
- Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)
- Embedding Models (e.g., E5-Mistral)
- Multi-modal LLMs (e.g., LLaVA)

Find the full list of supported models [here](https://docs.vllm.ai/en/latest/models/supported_models.html).

## Getting Started

Install vLLM with `pip` or [from source](https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source):

```bash
pip install vllm
```

Visit our [documentation](https://docs.vllm.ai/en/latest/) to learn more.

- [Installation](https://docs.vllm.ai/en/latest/getting_started/installation.html)
- [Quickstart](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)
- [List of Supported Models](https://docs.vllm.ai/en/latest/models/supported_models.html)

## Contributing

We welcome and value any contributions and collaborations.
Please check out [Contributing to vLLM](https://docs.vllm.ai/en/latest/contributing/index.html) for how to get involved.

## Sponsors

vLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!

&lt;!-- Note: Please sort them in alphabetical order. --&gt;
&lt;!-- Note: Please keep these consistent with docs/community/sponsors.md --&gt;
Cash Donations:

- a16z
- Dropbox
- Sequoia Capital
- Skywork AI
- ZhenFund

Compute Resources:

- Alibaba Cloud
- AMD
- Anyscale
- Arm
- AWS
- Crusoe Cloud
- Databricks
- DeepInfra
- Google Cloud
- Intel
- Lambda Lab
- Nebius
- Novita AI
- NVIDIA
- Replicate
- Roblox
- RunPod
- Trainy
- UC Berkeley
- UC San Diego
- Volcengine

Slack Sponsor: Anyscale

We also have an official fundraising venue through [OpenCollective](https://opencollective.com/vllm). We plan to use the fund to support the development, maintenance, and adoption of vLLM.

## Citation

If you use vLLM for your research, please cite our [paper](https://arxiv.org/abs/2309.06180):

```bibtex
@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}
```

## Contact Us

&lt;!-- --8&lt;-- [start:contact-us] --&gt;
- For technical questions and feature requests, please use GitHub [Issues](https://github.com/vllm-project/vllm/issues)
- For discussing with fellow users, please use the [vLLM Forum](https://discuss.vllm.ai)
- For coordinating contributions and development, please use [Slack](https://slack.vllm.ai)
- For security disclosures, please use GitHub&#039;s [Security Advisories](https://github.com/vllm-project/vllm/security/advisories) feature
- For collaborations and partnerships, please contact us at [vllm-questions@lists.berkeley.edu](mailto:vllm-questions@lists.berkeley.edu)
&lt;!-- --8&lt;-- [end:contact-us] --&gt;

## Media Kit

- If you wish to use vLLM&#039;s logo, please refer to [our media kit repo](https://github.com/vllm-project/media-kit)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>