<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Thu, 29 May 2025 00:04:27 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[Fosowl/agenticSeek]]></title>
            <link>https://github.com/Fosowl/agenticSeek</link>
            <guid>https://github.com/Fosowl/agenticSeek</guid>
            <pubDate>Thu, 29 May 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[Fully Local Manus AI. No APIs, No $200 monthly bills. Enjoy an autonomous agent that thinks, browses the web, and code for the sole cost of electricity.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Fosowl/agenticSeek">Fosowl/agenticSeek</a></h1>
            <p>Fully Local Manus AI. No APIs, No $200 monthly bills. Enjoy an autonomous agent that thinks, browses the web, and code for the sole cost of electricity.</p>
            <p>Language: Python</p>
            <p>Stars: 11,654</p>
            <p>Forks: 964</p>
            <p>Stars today: 2,379 stars today</p>
            <h2>README</h2><pre># AgenticSeek: Private, Local Manus Alternative.

&lt;p align=&quot;center&quot;&gt;
&lt;img align=&quot;center&quot; src=&quot;./media/agentic_seek_logo.png&quot; width=&quot;300&quot; height=&quot;300&quot; alt=&quot;Agentic Seek Logo&quot;&gt;
&lt;p&gt;

  English | [‰∏≠Êñá](./README_CHS.md) | [ÁπÅÈ´î‰∏≠Êñá](./README_CHT.md) | [Fran√ßais](./README_FR.md) | [Êó•Êú¨Ë™û](./README_JP.md)

*A **100% local alternative to Manus AI**, this voice-enabled AI assistant autonomously browses the web, writes code, and plans tasks while keeping all data on your device. Tailored for local reasoning models, it runs entirely on your hardware, ensuring complete privacy and zero cloud dependency.*

[![Visit AgenticSeek](https://img.shields.io/static/v1?label=Website&amp;message=AgenticSeek&amp;color=blue&amp;style=flat-square)](https://fosowl.github.io/agenticSeek.html) ![License](https://img.shields.io/badge/license-GPL--3.0-green) [![Discord](https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&amp;logoColor=white)](https://discord.gg/8hGDaME3TC) [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&amp;label=Update%20%40Fosowl)](https://x.com/Martin993886460) [![GitHub stars](https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social)](https://github.com/Fosowl/agenticSeek/stargazers)

### Why AgenticSeek ?

* üîí Fully Local &amp; Private - Everything runs on your machine ‚Äî no cloud, no data sharing. Your files, conversations, and searches stay private.

* üåê Smart Web Browsing - AgenticSeek can browse the internet by itself ‚Äî search, read, extract info, fill web form ‚Äî all hands-free.

* üíª Autonomous Coding Assistant - Need code? It can write, debug, and run programs in Python, C, Go, Java, and more ‚Äî all without supervision.

* üß† Smart Agent Selection - You ask, it figures out the best agent for the job automatically. Like having a team of experts ready to help.

* üìã Plans &amp; Executes Complex Tasks - From trip planning to complex projects ‚Äî it can split big tasks into steps and get things done using multiple AI agents.

* üéôÔ∏è Voice-Enabled - Clean, fast, futuristic voice and speech to text allowing you to talk to it like it&#039;s your personal AI from a sci-fi movie

### **Demo**

&gt; *Can you search for the agenticSeek project, learn what skills are required, then open the CV_candidates.zip and then tell me which match best the project*

https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316

Disclaimer: This demo, including all the files that appear (e.g: CV_candidates.zip), are entirely fictional. We are not a corporation, we seek open-source contributors not candidates.

&gt; üõ†‚ö†Ô∏èÔ∏è **Active Work in Progress** ‚Äì Please note that Code/Bash is not dockerized yet but will be soon (see docker_deployement branch) - Do not deploy over network or production.

&gt; üôè Please also understand that this project began as a side experiment, with no roadmap and no expectations, we didn&#039;t expect to end in Github trending. Financial backing is exactly $1/month (shoutout to my single sponsor). Contributions, feedback, and patience are deeply appreciated.

## Installation

Make sure you have chrome driver, docker and python3.10 installed.

We highly advice you use exactly python3.10 for the setup. Dependencies error might happen otherwise.

For issues related to chrome driver, see the **Chromedriver** section.

### 1Ô∏è‚É£ **Clone the repository and setup**

```sh
git clone https://github.com/Fosowl/agenticSeek.git
cd agenticSeek
mv .env.example .env
```

### 2Ô∏è **Create a virtual env**

```sh
python3 -m venv agentic_seek_env
source agentic_seek_env/bin/activate
# On Windows: agentic_seek_env\Scripts\activate
```

### 3Ô∏è‚É£ **Install package**

Ensure Python, Docker and docker compose, and Google chrome are installed.

We recommand Python 3.10.0.

**Automatic Installation (Recommanded):**

For Linux/Macos:
```sh
./install.sh
```

For windows:

```sh
./install.bat
```

**Manually:**

**Note: For any OS, ensure the ChromeDriver you install matches your installed Chrome version. Run `google-chrome --version`. See known issues if you have chrome &gt;135**

- *Linux*: 

Update Package List: `sudo apt update`

Install Dependencies: `sudo apt install -y alsa-utils portaudio19-dev python3-pyaudio libgtk-3-dev libnotify-dev libgconf-2-4 libnss3 libxss1`

Install ChromeDriver matching your Chrome browser version:
`sudo apt install -y chromium-chromedriver`

Install requirements: `pip3 install -r requirements.txt`

- *Macos*:

Update brew : `brew update`

Install chromedriver : `brew install --cask chromedriver`

Install portaudio: `brew install portaudio`

Upgrade pip : `python3 -m pip install --upgrade pip`

Upgrade wheel : : `pip3 install --upgrade setuptools wheel`

Install requirements: `pip3 install -r requirements.txt`

- *Windows*:

Install pyreadline3 `pip install pyreadline3`

Install portaudio manually (e.g., via vcpkg or prebuilt binaries) and then run: `pip install pyaudio`

Download and install chromedriver manually from: https://sites.google.com/chromium.org/driver/getting-started

Place chromedriver in a directory included in your PATH.

Install requirements: `pip3 install -r requirements.txt`

---

## Setup for running LLM locally on your machine

**Hardware Requirements:**

To run LLMs locally, you&#039;ll need sufficient hardware. At a minimum, a GPU capable of running Qwen/Deepseek 14B is required. See the FAQ for detailed model/performance recommendations.

**Setup your local provider**  

Start your local provider, for example with ollama:

```sh
ollama serve
```

See below for a list of local supported provider.

**Update the config.ini**

Change the config.ini file to set the provider_name to a supported provider and provider_model to a LLM supported by your provider. We recommand reasoning model such as *Qwen* or *Deepseek*.

See the **FAQ** at the end of the README for required hardware.

```sh
[MAIN]
is_local = True # Whenever you are running locally or with remote provider.
provider_name = ollama # or lm-studio, openai, etc..
provider_model = deepseek-r1:14b # choose a model that fit your hardware
provider_server_address = 127.0.0.1:11434
agent_name = Jarvis # name of your AI
recover_last_session = True # whenever to recover the previous session
save_session = True # whenever to remember the current session
speak = True # text to speech
listen = False # Speech to text, only for CLI
work_dir =  /Users/mlg/Documents/workspace # The workspace for AgenticSeek.
jarvis_personality = False # Whenever to use a more &quot;Jarvis&quot; like personality (experimental)
languages = en zh # The list of languages, Text to speech will default to the first language on the list
[BROWSER]
headless_browser = True # Whenever to use headless browser, recommanded only if you use web interface.
stealth_mode = True # Use undetected selenium to reduce browser detection
```

Warning: Do *NOT* set provider_name to `openai` if using LM-studio for running LLMs. Set it to `lm-studio`.

Note: Some provider (eg: lm-studio) require you to have `http://` in front of the IP. For example `http://127.0.0.1:1234`

**List of local providers**

| Provider  | Local? | Description                                               |
|-----------|--------|-----------------------------------------------------------|
| ollama    | Yes    | Run LLMs locally with ease using ollama as a LLM provider |
| lm-studio  | Yes    | Run LLM locally with LM studio (set `provider_name` to `lm-studio`)|
| openai    | Yes     |  Use openai compatible API (eg: llama.cpp server)  |

Next step: [Start services and run AgenticSeek](#Start-services-and-Run)  

*See the **Known issues** section if you are having issues*

*See the **Run with an API** section if your hardware can&#039;t run deepseek locally*

*See the **Config** section for detailled config file explanation.*

---

## Setup to run with an API

Set the desired provider in the `config.ini`. See below for a list of API providers.

```sh
[MAIN]
is_local = False
provider_name = google
provider_model = gemini-2.0-flash
provider_server_address = 127.0.0.1:5000 # doesn&#039;t matter
```
Warning: Make sure there is not trailing space in the config.

Export your API key: `export &lt;&lt;PROVIDER&gt;&gt;_API_KEY=&quot;xxx&quot;`

Example: export `TOGETHER_API_KEY=&quot;xxxxx&quot;`

**List of API providers**
  
| Provider  | Local? | Description                                               |
|-----------|--------|-----------------------------------------------------------|
| openai    | Depends  | Use ChatGPT API  |
| deepseek  | No     | Deepseek API (non-private)                            |
| huggingface| No    | Hugging-Face API (non-private)                            |
| togetherAI | No    | Use together AI API (non-private)                         |
| google | No    | Use google gemini API (non-private)                         |

*We advice against using gpt-4o or other closedAI models*, performance are poor for web browsing and task planning.

Please also note that coding/bash might fail with gemini, it seem to ignore our prompt for format to respect, which are optimized for deepseek r1.

Next step: [Start services and run AgenticSeek](#Start-services-and-Run)

*See the **Known issues** section if you are having issues*

*See the **Config** section for detailled config file explanation.*

---

## Start services and Run

Activate your python env if needed.
```sh
source agentic_seek_env/bin/activate
```

Start required services. This will start all services from the docker-compose.yml, including:
    - searxng
    - redis (required by searxng)
    - frontend

```sh
sudo ./start_services.sh # MacOS
start ./start_services.cmd # Window
```

**Options 1:** Run with the CLI interface.

```sh
python3 cli.py
```

We advice you set `headless_browser` to False in the config.ini for CLI mode.

**Options 2:** Run with the Web interface.

Start the backend.

```sh
python3 api.py
```

Go to `http://localhost:3000/` and you should see the web interface.

---

## Usage

Make sure the services are up and running with `./start_services.sh` and run the AgenticSeek with `python3 cli.py` for CLI mode or `python3 api.py` then go to `localhost:3000` for web interface.

You can also use speech to text by setting `listen = True` in the config. Only for CLI mode.

To exit, simply say/type `goodbye`.

Here are some example usage:

&gt; *Make a snake game in python!*

&gt; *Search the web for top cafes in Rennes, France, and save a list of three with their addresses in rennes_cafes.txt.*

&gt; *Write a Go program to calculate the factorial of a number, save it as factorial.go in your workspace*

&gt; *Search my summer_pictures folder for all JPG files, rename them with today‚Äôs date, and save a list of renamed files in photos_list.txt*

&gt; *Search online for popular sci-fi movies from 2024 and pick three to watch tonight. Save the list in movie_night.txt.*

&gt; *Search the web for the latest AI news articles from 2025, select three, and write a Python script to scrape their titles and summaries. Save the script as news_scraper.py and the summaries in ai_news.txt in /home/projects*

&gt; *Friday, search the web for a free stock price API, register with supersuper7434567@gmail.com then write a Python script to fetch using the API daily prices for Tesla, and save the results in stock_prices.csv*

*Note that form filling capabilities are still experimental and might fail.*



After you type your query, AgenticSeek will allocate the best agent for the task.

Because this is an early prototype, the agent routing system might not always allocate the right agent based on your query.

Therefore, you should be very explicit in what you want and how the AI might proceed for example if you want it to conduct a web search, do not say:

`Do you know some good countries for solo-travel?`

Instead, ask:

`Do a web search and find out which are the best country for solo-travel`

---

## **Setup to run the LLM on your own server**  

If you have a powerful computer or a server that you can use, but you want to use it from your laptop you have the options to run the LLM on a remote server using our custom llm server. 

On your &quot;server&quot; that will run the AI model, get the ip address

```sh
ip a | grep &quot;inet &quot; | grep -v 127.0.0.1 | awk &#039;{print $2}&#039; | cut -d/ -f1 # local ip
curl https://ipinfo.io/ip # public ip
```

Note: For Windows or macOS, use ipconfig or ifconfig respectively to find the IP address.

Clone the repository and enter the `server/`folder.


```sh
git clone --depth 1 https://github.com/Fosowl/agenticSeek.git
cd agenticSeek/llm_server/
```

Install server specific requirements:

```sh
pip3 install -r requirements.txt
```

Run the server script.

```sh
python3 app.py --provider ollama --port 3333
```

You have the choice between using `ollama` and `llamacpp` as a LLM service.


Now on your personal computer:

Change the `config.ini` file to set the `provider_name` to `server` and `provider_model` to `deepseek-r1:xxb`.
Set the `provider_server_address` to the ip address of the machine that will run the model.

```sh
[MAIN]
is_local = False
provider_name = server
provider_model = deepseek-r1:70b
provider_server_address = x.x.x.x:3333
```


Next step: [Start services and run AgenticSeek](#Start-services-and-Run)  

---

## Speech to Text

Please note that currently speech to text only work in english.

The speech-to-text functionality is disabled by default. To enable it, set the listen option to True in the config.ini file:

```
listen = True
```

When enabled, the speech-to-text feature listens for a trigger keyword, which is the agent&#039;s name, before it begins processing your input. You can customize the agent&#039;s name by updating the `agent_name` value in the *config.ini* file:

```
agent_name = Friday
```

For optimal recognition, we recommend using a common English name like &quot;John&quot; or &quot;Emma&quot; as the agent name

Once you see the transcript start to appear, say the agent&#039;s name aloud to wake it up (e.g., &quot;Friday&quot;).

Speak your query clearly.

End your request with a confirmation phrase to signal the system to proceed. Examples of confirmation phrases include:
```
&quot;do it&quot;, &quot;go ahead&quot;, &quot;execute&quot;, &quot;run&quot;, &quot;start&quot;, &quot;thanks&quot;, &quot;would ya&quot;, &quot;please&quot;, &quot;okay?&quot;, &quot;proceed&quot;, &quot;continue&quot;, &quot;go on&quot;, &quot;do that&quot;, &quot;go it&quot;, &quot;do you understand?&quot;
```

## Config

Example config:
```
[MAIN]
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = 127.0.0.1:11434
agent_name = Friday
recover_last_session = False
save_session = False
speak = False
listen = False
work_dir =  /Users/mlg/Documents/ai_folder
jarvis_personality = False
languages = en zh
[BROWSER]
headless_browser = False
stealth_mode = False
```

**Explanation**:

- is_local -&gt; Runs the agent locally (True) or on a remote server (False).

- provider_name -&gt; The provider to use (one of: `ollama`, `server`, `lm-studio`, `deepseek-api`)

- provider_model -&gt; The model used, e.g., deepseek-r1:32b.

- provider_server_address -&gt; Server address, e.g., 127.0.0.1:11434 for local. Set to anything for non-local API.

- agent_name -&gt; Name of the agent, e.g., Friday. Used as a trigger word for TTS.

- recover_last_session -&gt; Restarts from last session (True) or not (False).

- save_session -&gt; Saves session data (True) or not (False).

- speak -&gt; Enables voice output (True) or not (False).

- listen -&gt; listen to voice input (True) or not (False).

- work_dir -&gt; Folder the AI will have access to. eg: /Users/user/Documents/.

- jarvis_personality -&gt; Uses a JARVIS-like personality (True) or not (False). This simply change the prompt file.

- languages -&gt; The list of supported language, needed for the llm router to work properly, avoid putting too many or too similar languages.

- headless_browser -&gt; Runs browser without a visible window (True) or not (False).

- stealth_mode -&gt; Make bot detector time harder. Only downside is you have to manually install the anticaptcha extension.

- languages -&gt; List of supported languages. Required for agent routing system. The longer the languages list the more model will be downloaded.

## Providers

The table below show the available providers:

| Provider  | Local? | Description                                               |
|-----------|--------|-----------------------------------------------------------|
| ollama    | Yes    | Run LLMs locally with ease using ollama as a LLM provider |
| server    | Yes    | Host the model on another machine, run your local machine |
| lm-studio  | Yes    | Run LLM locally with LM studio (`lm-studio`)             |
| openai    | Depends  | Use ChatGPT API (non-private) or openai compatible API  |
| deepseek-api  | No     | Deepseek API (non-private)                            |
| huggingface| No    | Hugging-Face API (non-private)                            |
| togetherAI | No    | Use together AI API (non-private)                         |
| google | No    | Use google gemini API (non-private)                         |

To select a provider change the config.ini:

```
is_local = True
provider_name = ollama
provider_model = deepseek-r1:32b
provider_server_address = 127.0.0.1:5000
```
`is_local`: should be True for any locally running LLM, otherwise False.

`provider_name`: Select the provider to use by it&#039;s name, see the provider list above.

`provider_model`: Set the model to use by the agent.

`provider_server_address`: can be set to anything if you are not using the server provider.

# Known issues

## Chromedriver Issues

**Known error #1:** *chromedriver mismatch*

`Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113
Current browser version is 134.0.6998.89 with binary path`

This happen if there is a mismatch between your browser and chromedriver version.

You need to navigate to download the latest version:

https://developer.chrome.com/docs/chromedriver/downloads

If you&#039;re using Chrome version 115 or newer go to:

https://googlechromelabs.github.io/chrome-for-testing/

And download the chromedriver version matching your OS.

![alt text](./media/chromedriver_readme.png)

If this section is incomplete please raise an issue.

##  connection adapters Issues

```
Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for &#039;127.0.0.1:11434/v1/chat/completions&#039;
```

Make sure you have `http://` in front of the provider IP address :

`provider_server_address = http://127.0.0.1:11434`

## SearxNG base URL must be provided

```
raise ValueError(&quot;SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.&quot;)
ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.
```

Maybe you didn&#039;t move `.env.example` as `.env` ? You can also export SEARXNG_BASE_URL:

`export  SEARXNG_BASE_URL=&quot;http://127.0.0.1:8080&quot;`

## FAQ

**Q: What hardware do I need?**  

| Model Size  | GPU  | Comment                                               |
|-----------|--------|-----------------------------------------------------------|
| 7B        | 8GB Vram | ‚ö†Ô∏è Not recommended. Performance is poor, frequent hallucinations, and planner agents will likely fail. |
| 14B        | 12 GB VRAM (e.g. RTX 3060) | ‚úÖ Usable for simple tasks. May struggle with web browsing and planning tasks. |
| 32B        | 24+ GB VRAM (e.g. RTX 4090) | üöÄ Success with most tasks, might still struggle with task planning |
| 70B+        | 48+ GB Vram (eg. mac studio) | üí™ Excellent. Recommended for advanced use cases. |

**Q: Why Deepseek R1 over other models?**  

Deepseek R1 excels at reasoning and tool use for its size. We think it‚Äôs a solid fit for our needs other models work fine, but Deepseek is our primary pick.

**Q: I get an error running `cli.py`. What do I do?**  

Ensure local is running (`ollama serve`), your `config.ini` matches your provider, and dependencies are installed. If none work feel free to raise an issue.

**Q: Can it really run 100% locally?**  

Yes with Ollama, lm-studio or serv

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/qlib]]></title>
            <link>https://github.com/microsoft/qlib</link>
            <guid>https://github.com/microsoft/qlib</guid>
            <pubDate>Thu, 29 May 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[Qlib is an AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms. including supervised learning, market dynamics modeling, and RL.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/qlib">microsoft/qlib</a></h1>
            <p>Qlib is an AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms. including supervised learning, market dynamics modeling, and RL.</p>
            <p>Language: Python</p>
            <p>Stars: 22,938</p>
            <p>Forks: 3,560</p>
            <p>Stars today: 351 stars today</p>
            <h2>README</h2><pre>[![Python Versions](https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&amp;logoColor=white)](https://pypi.org/project/pyqlib/#files)
[![Platform](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey)](https://pypi.org/project/pyqlib/#files)
[![PypI Versions](https://img.shields.io/pypi/v/pyqlib)](https://pypi.org/project/pyqlib/#history)
[![Upload Python Package](https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/pyqlib/)
[![Github Actions Test Status](https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main)](https://github.com/microsoft/qlib/actions)
[![Documentation Status](https://readthedocs.org/projects/qlib/badge/?version=latest)](https://qlib.readthedocs.io/en/latest/?badge=latest)
[![License](https://img.shields.io/pypi/l/pyqlib)](LICENSE)
[![Join the chat at https://gitter.im/Microsoft/qlib](https://badges.gitter.im/Microsoft/qlib.svg)](https://gitter.im/Microsoft/qlib?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge)

## :newspaper: **What&#039;s NEW!** &amp;nbsp;   :sparkling_heart: 

Recent released features

### Introducing &lt;a href=&quot;https://github.com/microsoft/RD-Agent&quot;&gt;&lt;img src=&quot;docs/_static/img/rdagent_logo.png&quot; alt=&quot;RD_Agent&quot; style=&quot;height: 2em&quot;&gt;&lt;/a&gt;: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&amp;D

We are excited to announce the release of **RD-Agent**üì¢, a powerful tool that supports automated factor mining and model optimization in quant investment R&amp;D.

RD-Agent is now available on [GitHub](https://github.com/microsoft/RD-Agent), and we welcome your starüåü!

To learn more, please visit our [‚ôæÔ∏èDemo page](https://rdagent.azurewebsites.net/). Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.

We have prepared several demo videos for you:
| Scenario | Demo video (English) | Demo video (‰∏≠Êñá) |
| --                      | ------    | ------    |
| Quant Factor Mining | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=zh) |
| Quant Factor Mining from reports | [Link](https://rdagent.azurewebsites.net/report_factor?lang=en) | [Link](https://rdagent.azurewebsites.net/report_factor?lang=zh) |
| Quant Model Optimization | [Link](https://rdagent.azurewebsites.net/model_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/model_loop?lang=zh) |

- üìÉ**Paper**: [R&amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization](https://arxiv.org/abs/2505.15155)
- üëæ**Code**: https://github.com/microsoft/RD-Agent/
```BibTeX
@misc{li2025rdagentquant,
    title={R\&amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},
    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},
    year={2025},
    eprint={2505.15155},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
```
![image](https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d)

***

| Feature | Status |
| --                      | ------    |
| [R&amp;D-Agent-Quant](https://arxiv.org/abs/2505.15155) Published | Apply R&amp;D-Agent to Qlib for quant trading | 
| BPQP for End-to-end learning | üìàComing soon!([Under review](https://github.com/microsoft/qlib/pull/1863)) |
| üî•LLM-driven Auto Quant Factoryüî• | üöÄ Released in [‚ôæÔ∏èRD-Agent](https://github.com/microsoft/RD-Agent) on Aug 8, 2024 |
| KRNN and Sandwich models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1414/) on May 26, 2023 |
| Release Qlib v0.9.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.9.0) on Dec 9, 2022 |
| RL Learning Framework | :hammer: :chart_with_upwards_trend: Released on Nov 10, 2022. [#1332](https://github.com/microsoft/qlib/pull/1332), [#1322](https://github.com/microsoft/qlib/pull/1322), [#1316](https://github.com/microsoft/qlib/pull/1316),[#1299](https://github.com/microsoft/qlib/pull/1299),[#1263](https://github.com/microsoft/qlib/pull/1263), [#1244](https://github.com/microsoft/qlib/pull/1244), [#1169](https://github.com/microsoft/qlib/pull/1169), [#1125](https://github.com/microsoft/qlib/pull/1125), [#1076](https://github.com/microsoft/qlib/pull/1076)|
| HIST and IGMTF models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1040) on Apr 10, 2022 |
| Qlib [notebook tutorial](https://github.com/microsoft/qlib/tree/main/examples/tutorial) | üìñ [Released](https://github.com/microsoft/qlib/pull/1037) on Apr 7, 2022 | 
| Ibovespa index data | :rice: [Released](https://github.com/microsoft/qlib/pull/990) on Apr 6, 2022 |
| Point-in-Time database | :hammer: [Released](https://github.com/microsoft/qlib/pull/343) on Mar 10, 2022 |
| Arctic Provider Backend &amp; Orderbook data example | :hammer: [Released](https://github.com/microsoft/qlib/pull/744) on Jan 17, 2022 |
| Meta-Learning-based framework &amp; DDG-DA  | :chart_with_upwards_trend:  :hammer: [Released](https://github.com/microsoft/qlib/pull/743) on Jan 10, 2022 | 
| Planning-based portfolio optimization | :hammer: [Released](https://github.com/microsoft/qlib/pull/754) on Dec 28, 2021 | 
| Release Qlib v0.8.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.8.0) on Dec 8, 2021 |
| ADD model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/704) on Nov 22, 2021 |
| ADARNN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/689) on Nov 14, 2021 |
| TCN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/668) on Nov 4, 2021 |
| Nested Decision Framework | :hammer: [Released](https://github.com/microsoft/qlib/pull/438) on Oct 1, 2021. [Example](https://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/workflow.py) and [Doc](https://qlib.readthedocs.io/en/latest/component/highfreq.html) |
| Temporal Routing Adaptor (TRA) | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/531) on July 30, 2021 |
| Transformer &amp; Localformer | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/508) on July 22, 2021 |
| Release Qlib v0.7.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.7.0) on July 12, 2021 |
| TCTS Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/491) on July 1, 2021 |
| Online serving and automatic model rolling | :hammer:  [Released](https://github.com/microsoft/qlib/pull/290) on May 17, 2021 | 
| DoubleEnsemble Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/286) on Mar 2, 2021 | 
| High-frequency data processing example | :hammer: [Released](https://github.com/microsoft/qlib/pull/257) on Feb 5, 2021  |
| High-frequency trading example | :chart_with_upwards_trend: [Part of code released](https://github.com/microsoft/qlib/pull/227) on Jan 28, 2021  | 
| High-frequency data(1min) | :rice: [Released](https://github.com/microsoft/qlib/pull/221) on Jan 27, 2021 |
| Tabnet Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/205) on Jan 22, 2021 |

Features released before 2021 are not listed here.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/_static/img/logo/1.png&quot; /&gt;
&lt;/p&gt;

Qlib is an open-source, AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms, including supervised learning, market dynamics modeling, and reinforcement learning.

An increasing number of SOTA Quant research works/papers in diverse paradigms are being released in Qlib to collaboratively solve key challenges in quantitative investment. For example, 1) using supervised learning to mine the market&#039;s complex non-linear patterns from rich and heterogeneous financial data, 2) modeling the dynamic nature of the financial market using adaptive concept drift technology, and 3) using reinforcement learning to model continuous investment decisions and assist investors in optimizing their trading strategies.

It contains the full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution. 
For more details, please refer to our paper [&quot;Qlib: An AI-oriented Quantitative Investment Platform&quot;](https://arxiv.org/abs/2009.11189).


&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Frameworks, Tutorial, Data &amp; DevOps&lt;/th&gt;
      &lt;th&gt;Main Challenges &amp; Solutions in Quant Research&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;li&gt;&lt;a href=&quot;#plans&quot;&gt;&lt;strong&gt;Plans&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#framework-of-qlib&quot;&gt;Framework of Qlib&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#quick-start&quot;&gt;Quick Start&lt;/a&gt;&lt;/li&gt;
          &lt;ul dir=&quot;auto&quot;&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#installation&quot;&gt;Installation&lt;/a&gt; &lt;/li&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#data-preparation&quot;&gt;Data Preparation&lt;/a&gt;&lt;/li&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#auto-quant-research-workflow&quot;&gt;Auto Quant Research Workflow&lt;/a&gt;&lt;/li&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#building-customized-quant-research-workflow-by-code&quot;&gt;Building Customized Quant Research Workflow by Code&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
        &lt;li&gt;&lt;a href=&quot;#quant-dataset-zoo&quot;&gt;&lt;strong&gt;Quant Dataset Zoo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#learning-framework&quot;&gt;Learning Framework&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#more-about-qlib&quot;&gt;More About Qlib&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#offline-mode-and-online-mode&quot;&gt;Offline Mode and Online Mode&lt;/a&gt;
        &lt;ul&gt;
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#performance-of-qlib-data-server&quot;&gt;Performance of Qlib Data Server&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
        &lt;li&gt;&lt;a href=&quot;#related-reports&quot;&gt;Related Reports&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#contact-us&quot;&gt;Contact Us&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#contributing&quot;&gt;Contributing&lt;/a&gt;&lt;/li&gt;
      &lt;/td&gt;
      &lt;td valign=&quot;baseline&quot;&gt;
        &lt;li&gt;&lt;a href=&quot;#main-challenges--solutions-in-quant-research&quot;&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/a&gt;
          &lt;ul&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#forecasting-finding-valuable-signalspatterns&quot;&gt;Forecasting: Finding Valuable Signals/Patterns&lt;/a&gt;
              &lt;ul&gt;
                &lt;li type=&quot;disc&quot;&gt;&lt;a href=&quot;#quant-model-paper-zoo&quot;&gt;&lt;strong&gt;Quant Model (Paper) Zoo&lt;/strong&gt;&lt;/a&gt;
                  &lt;ul&gt;
                    &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#run-a-single-model&quot;&gt;Run a Single Model&lt;/a&gt;&lt;/li&gt;
                    &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#run-multiple-models&quot;&gt;Run Multiple Models&lt;/a&gt;&lt;/li&gt;
                  &lt;/ul&gt;
                &lt;/li&gt;
              &lt;/ul&gt;
            &lt;/li&gt;
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#adapting-to-market-dynamics&quot;&gt;Adapting to Market Dynamics&lt;/a&gt;&lt;/li&gt;
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#reinforcement-learning-modeling-continuous-decisions&quot;&gt;Reinforcement Learning: modeling continuous decisions&lt;/a&gt;&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

# Plans
New features under development(order by estimated release time).
Your feedbacks about the features are very important.
&lt;!-- | Feature                        | Status      | --&gt;
&lt;!-- | --                      | ------    | --&gt;

# Framework of Qlib

&lt;div style=&quot;align: center&quot;&gt;
&lt;img src=&quot;docs/_static/img/framework-abstract.jpg&quot; /&gt;
&lt;/div&gt;

The high-level framework of Qlib can be found above(users can find the [detailed framework](https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework) of Qlib&#039;s design when getting into nitty gritty).
The components are designed as loose-coupled modules, and each component could be used stand-alone.

Qlib provides a strong infrastructure to support Quant research. [Data](https://qlib.readthedocs.io/en/latest/component/data.html) is always an important part.
A strong learning framework is designed to support diverse learning paradigms (e.g. [reinforcement learning](https://qlib.readthedocs.io/en/latest/component/rl.html), [supervised learning](https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section)) and patterns at different levels(e.g. [market dynamic modeling](https://qlib.readthedocs.io/en/latest/component/meta.html)).
By modeling the market, [trading strategies](https://qlib.readthedocs.io/en/latest/component/strategy.html) will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be [nested to be optimized and run together](https://qlib.readthedocs.io/en/latest/component/highfreq.html).
At last, a comprehensive [analysis](https://qlib.readthedocs.io/en/latest/component/report.html) will be provided and the model can be [served online](https://qlib.readthedocs.io/en/latest/component/online.html) in a low cost.


# Quick Start

This quick start guide tries to demonstrate
1. It&#039;s very easy to build a complete Quant research workflow and try your ideas with _Qlib_.
2. Though with *public data* and *simple models*, machine learning technologies **work very well** in practical Quant investment.

Here is a quick **[demo](https://terminalizer.com/view/3f24561a4470)** shows how to install ``Qlib``, and run LightGBM with ``qrun``. **But**, please make sure you have already prepared the data following the [instruction](#data-preparation).


## Installation

This table demonstrates the supported Python version of `Qlib`:
|               | install with pip      | install from source  |        plot        |
| ------------- |:---------------------:|:--------------------:|:------------------:|
| Python 3.8    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.9    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.10   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.11   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.12   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |

**Note**: 
1. **Conda** is suggested for managing your Python environment. In some cases, using Python outside of a `conda` environment may result in missing header files, causing the installation failure of certain packages.
2. Please pay attention that installing cython in Python 3.6 will raise some error when installing ``Qlib`` from source. If users use Python 3.6 on their machines, it is recommended to *upgrade* Python to version 3.8 or higher, or use `conda`&#039;s Python to install ``Qlib`` from source.

### Install with pip
Users can easily install ``Qlib`` by pip according to the following command.

```bash
  pip install pyqlib
```

**Note**: pip will install the latest stable qlib. However, the main branch of qlib is in active development. If you want to test the latest scripts or functions in the main branch. Please install qlib with the methods below.

### Install from source
Also, users can install the latest dev version ``Qlib`` by the source code according to the following steps:

* Before installing ``Qlib`` from source, users need to install some dependencies:

  ```bash
  pip install numpy
  pip install --upgrade cython
  ```

* Clone the repository and install ``Qlib`` as follows.
    ```bash
    git clone https://github.com/microsoft/qlib.git &amp;&amp; cd qlib
    pip install .  # `pip install -e .[dev]` is recommended for development. check details in docs/developer/code_standard_and_dev_guide.rst
    ```

**Tips**: If you fail to install `Qlib` or run the examples in your environment,  comparing your steps and the [CI workflow](.github/workflows/test_qlib_from_source.yml) may help you find the problem.

**Tips for Mac**: If you are using Mac with M1, you might encounter issues in building the wheel for LightGBM, which is due to missing dependencies from OpenMP. To solve the problem, install openmp first with ``brew install libomp`` and then run ``pip install .`` to build it successfully. 

## Data Preparation
‚ùó Due to more restrict data security policy. The offical dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.
Here is an example to download the latest data.
```bash
wget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz
mkdir -p ~/.qlib/qlib_data/cn_data
tar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1
rm -f qlib_bin.tar.gz
```

The official dataset below will resume in short future.


----

Load and prepare data by running the following code:

### Get with module
  ```bash
  # get 1d data
  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

  # get 1min data
  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

  ```

### Get from source

  ```bash
  # get 1d data
  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

  # get 1min data
  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

  ```

This dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in
the same repository.
Users could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)

*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.
We recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.

### Automatic update of daily frequency data (from yahoo finance)
  &gt; This step is *Optional* if users only want to try their models and strategies on history data.
  &gt; 
  &gt; It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.
  &gt;
  &gt; **NOTE**: Users can&#039;t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.
  &gt; 
  &gt; For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)

  * Automatic update of data to the &quot;qlib&quot; directory each trading day(Linux)
      * use *crontab*: `crontab -e`
      * set up timed tasks:

        ```
        * * * * 1-5 python &lt;script path&gt; update_data_to_bin --qlib_data_1d_dir &lt;user data dir&gt;
        ```
        * **script path**: *scripts/data_collector/yahoo/collector.py*

  * Manual update of data
      ```
      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir &lt;user data dir&gt; --trading_date &lt;start date&gt; --end_date &lt;end date&gt;
      ```
      * *trading_date*: start of trading day
      * *end_date*: end of trading day(not included)

### Checking the health of the data
  * We provide a script to check the health of the data, you can run the following commands to check whether the data is healthy or not.
    ```
    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data
    ```
  * Of course, you can also add some parameters to adjust the test results, such as this.
    ```
    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[langflow-ai/langflow]]></title>
            <link>https://github.com/langflow-ai/langflow</link>
            <guid>https://github.com/langflow-ai/langflow</guid>
            <pubDate>Thu, 29 May 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[Langflow is a powerful tool for building and deploying AI-powered agents and workflows.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/langflow-ai/langflow">langflow-ai/langflow</a></h1>
            <p>Langflow is a powerful tool for building and deploying AI-powered agents and workflows.</p>
            <p>Language: Python</p>
            <p>Stars: 65,352</p>
            <p>Forks: 6,614</p>
            <p>Stars today: 577 stars today</p>
            <h2>README</h2><pre>&lt;!-- markdownlint-disable MD030 --&gt;

![Langflow logo](./docs/static/img/langflow-logo-color-black-solid.svg)


[![Release Notes](https://img.shields.io/github/release/langflow-ai/langflow?style=flat-square)](https://github.com/langflow-ai/langflow/releases)
[![PyPI - License](https://img.shields.io/badge/license-MIT-orange)](https://opensource.org/licenses/MIT)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/langflow?style=flat-square)](https://pypistats.org/packages/langflow)
[![GitHub star chart](https://img.shields.io/github/stars/langflow-ai/langflow?style=flat-square)](https://star-history.com/#langflow-ai/langflow)
[![Open Issues](https://img.shields.io/github/issues-raw/langflow-ai/langflow?style=flat-square)](https://github.com/langflow-ai/langflow/issues)
[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langflow-ai.svg?style=social&amp;label=Follow%20%40Langflow)](https://twitter.com/langflow_ai)
[![YouTube Channel](https://img.shields.io/youtube/channel/subscribers/UCn2bInQrjdDYKEEmbpwblLQ?label=Subscribe)](https://www.youtube.com/@Langflow)
[![Discord Server](https://img.shields.io/discord/1116803230643527710?logo=discord&amp;style=social&amp;label=Join)](https://discord.gg/EqksyE2EX9)


[Langflow](https://langflow.org) is a powerful tool for building and deploying AI-powered agents and workflows. It provides developers with both a visual authoring experience and a built-in API server that turns every agent into an API endpoint that can be integrated into applications built on any framework or stack. Langflow comes with batteries included and supports all major LLMs, vector databases and a growing library of AI tools.

## ‚ú® Highlight features

1. **Visual Builder** to get started quickly and iterate. 
1. **Access to Code** so developers can tweak any component using Python.
1. **Playground** to immediately test and iterate on their flows with step-by-step control.
1. **Multi-agent** orchestration and conversation management and retrieval.
1. **Deploy as an API** or export as JSON for Python apps.
1. **Observability** with LangSmith, LangFuse and other integrations.
1. **Enterprise-ready** security and scalability.

## ‚ö°Ô∏è Quickstart

Langflow works with Python 3.10 to 3.13.

Install with uv **(recommended)** 

```shell
uv pip install langflow
```

Install with pip

```shell
pip install langflow
```

## üì¶ Deployment

### Self-managed

Langflow is completely open source and you can deploy it to all major deployment clouds. Follow this [guide](https://docs.langflow.org/deployment-docker) to learn how to use Docker to deploy Langflow.

### Fully-managed by DataStax

DataStax Langflow is a full-managed environment with zero setup. Developers can [sign up for a free account](https://astra.datastax.com/signup?type=langflow) to get started.

## ‚≠ê Stay up-to-date

Star Langflow on GitHub to be instantly notified of new releases.

![Star Langflow](https://github.com/user-attachments/assets/03168b17-a11d-4b2a-b0f7-c1cce69e5a2c)

## üëã Contribute

We welcome contributions from developers of all levels. If you&#039;d like to contribute, please check our [contributing guidelines](./CONTRIBUTING.md) and help make Langflow more accessible.

---

[![Star History Chart](https://api.star-history.com/svg?repos=langflow-ai/langflow&amp;type=Timeline)](https://star-history.com/#langflow-ai/langflow&amp;Date)

## ‚ù§Ô∏è Contributors

[![langflow contributors](https://contrib.rocks/image?repo=langflow-ai/langflow)](https://github.com/langflow-ai/langflow/graphs/contributors)

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[zhayujie/chatgpt-on-wechat]]></title>
            <link>https://github.com/zhayujie/chatgpt-on-wechat</link>
            <guid>https://github.com/zhayujie/chatgpt-on-wechat</guid>
            <pubDate>Thu, 29 May 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[Âü∫‰∫éÂ§ßÊ®°ÂûãÊê≠Âª∫ÁöÑËÅäÂ§©Êú∫Âô®‰∫∫ÔºåÂêåÊó∂ÊîØÊåÅ ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®„ÄÅÈ£û‰π¶„ÄÅÈíâÈíâ Á≠âÊé•ÂÖ•ÔºåÂèØÈÄâÊã©GPT4.1/GPT-4o/GPT-o1/ DeepSeek/Claude/ÊñáÂøÉ‰∏ÄË®Ä/ËÆØÈ£ûÊòüÁÅ´/ÈÄö‰πâÂçÉÈóÆ/ Gemini/GLM-4/Kimi/LinkAIÔºåËÉΩÂ§ÑÁêÜÊñáÊú¨„ÄÅËØ≠Èü≥ÂíåÂõæÁâáÔºåËÆøÈóÆÊìç‰ΩúÁ≥ªÁªüÂíå‰∫íËÅîÁΩëÔºåÊîØÊåÅÂü∫‰∫éËá™ÊúâÁü•ËØÜÂ∫ìËøõË°åÂÆöÂà∂‰ºÅ‰∏öÊô∫ËÉΩÂÆ¢Êúç„ÄÇ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/zhayujie/chatgpt-on-wechat">zhayujie/chatgpt-on-wechat</a></h1>
            <p>Âü∫‰∫éÂ§ßÊ®°ÂûãÊê≠Âª∫ÁöÑËÅäÂ§©Êú∫Âô®‰∫∫ÔºåÂêåÊó∂ÊîØÊåÅ ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®„ÄÅÈ£û‰π¶„ÄÅÈíâÈíâ Á≠âÊé•ÂÖ•ÔºåÂèØÈÄâÊã©GPT4.1/GPT-4o/GPT-o1/ DeepSeek/Claude/ÊñáÂøÉ‰∏ÄË®Ä/ËÆØÈ£ûÊòüÁÅ´/ÈÄö‰πâÂçÉÈóÆ/ Gemini/GLM-4/Kimi/LinkAIÔºåËÉΩÂ§ÑÁêÜÊñáÊú¨„ÄÅËØ≠Èü≥ÂíåÂõæÁâáÔºåËÆøÈóÆÊìç‰ΩúÁ≥ªÁªüÂíå‰∫íËÅîÁΩëÔºåÊîØÊåÅÂü∫‰∫éËá™ÊúâÁü•ËØÜÂ∫ìËøõË°åÂÆöÂà∂‰ºÅ‰∏öÊô∫ËÉΩÂÆ¢Êúç„ÄÇ</p>
            <p>Language: Python</p>
            <p>Stars: 37,065</p>
            <p>Forks: 9,237</p>
            <p>Stars today: 53 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;img src= &quot;https://github.com/user-attachments/assets/31fb4eab-3be4-477d-aa76-82cf62bfd12c&quot; alt=&quot;Chatgpt-on-Wechat&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
   &lt;a href=&quot;https://github.com/zhayujie/chatgpt-on-wechat/releases/latest&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/zhayujie/chatgpt-on-wechat&quot; alt=&quot;Latest release&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/zhayujie/chatgpt-on-wechat/blob/master/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/zhayujie/chatgpt-on-wechat&quot; alt=&quot;License: MIT&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/zhayujie/chatgpt-on-wechat&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/zhayujie/chatgpt-on-wechat?style=flat-square&quot; alt=&quot;Stars&quot;&gt;&lt;/a&gt; &lt;br/&gt;
&lt;/p&gt;

chatgpt-on-wechatÔºàÁÆÄÁß∞CoWÔºâÈ°πÁõÆÊòØÂü∫‰∫éÂ§ßÊ®°ÂûãÁöÑÊô∫ËÉΩÂØπËØùÊú∫Âô®‰∫∫ÔºåÊîØÊåÅÂæÆ‰ø°ÂÖ¨‰ºóÂè∑„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®„ÄÅÈ£û‰π¶„ÄÅÈíâÈíâÊé•ÂÖ•ÔºåÂèØÈÄâÊã©GPT3.5/GPT4.0/Claude/Gemini/LinkAI/ChatGLM/KIMI/ÊñáÂøÉ‰∏ÄË®Ä/ËÆØÈ£ûÊòüÁÅ´/ÈÄö‰πâÂçÉÈóÆ/LinkAI/ModelScopeÔºåËÉΩÂ§ÑÁêÜÊñáÊú¨„ÄÅËØ≠Èü≥ÂíåÂõæÁâáÔºåÈÄöËøáÊèí‰ª∂ËÆøÈóÆÊìç‰ΩúÁ≥ªÁªüÂíå‰∫íËÅîÁΩëÁ≠âÂ§ñÈÉ®ËµÑÊ∫êÔºåÊîØÊåÅÂü∫‰∫éËá™ÊúâÁü•ËØÜÂ∫ìÂÆöÂà∂‰ºÅ‰∏öAIÂ∫îÁî®„ÄÇ

# ÁÆÄ‰ªã

ÊúÄÊñ∞ÁâàÊú¨ÊîØÊåÅÁöÑÂäüËÉΩÂ¶Ç‰∏ãÔºö

-  ‚úÖ   **Â§öÁ´ØÈÉ®ÁΩ≤Ôºö** ÊúâÂ§öÁßçÈÉ®ÁΩ≤ÊñπÂºèÂèØÈÄâÊã©‰∏îÂäüËÉΩÂÆåÂ§áÔºåÁõÆÂâçÂ∑≤ÊîØÊåÅÂæÆ‰ø°ÂÖ¨‰ºóÂè∑„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®„ÄÅÈ£û‰π¶„ÄÅÈíâÈíâÁ≠âÈÉ®ÁΩ≤ÊñπÂºè
-  ‚úÖ   **Âü∫Á°ÄÂØπËØùÔºö** ÁßÅËÅäÂèäÁæ§ËÅäÁöÑÊ∂àÊÅØÊô∫ËÉΩÂõûÂ§çÔºåÊîØÊåÅÂ§öËΩÆ‰ºöËØù‰∏ä‰∏ãÊñáËÆ∞ÂøÜÔºåÊîØÊåÅ GPT-4oÁ≥ªÂàó, GPT-4.1Á≥ªÂàó, Claude, Gemini, ÊñáÂøÉ‰∏ÄË®Ä, ËÆØÈ£ûÊòüÁÅ´, ÈÄö‰πâÂçÉÈóÆÔºåChatGLM-4ÔºåKimi, MiniMax, GiteeAI, ModelScope
-  ‚úÖ   **ËØ≠Èü≥ËÉΩÂäõÔºö** ÂèØËØÜÂà´ËØ≠Èü≥Ê∂àÊÅØÔºåÈÄöËøáÊñáÂ≠óÊàñËØ≠Èü≥ÂõûÂ§çÔºåÊîØÊåÅ azure, baidu, google, openai(whisper/tts) Á≠âÂ§öÁßçËØ≠Èü≥Ê®°Âûã
-  ‚úÖ   **ÂõæÂÉèËÉΩÂäõÔºö** ÊîØÊåÅÂõæÁâáÁîüÊàê„ÄÅÂõæÁâáËØÜÂà´„ÄÅÂõæÁîüÂõæÔºàÂ¶ÇÁÖßÁâá‰øÆÂ§çÔºâÔºåÂèØÈÄâÊã© Dall-E-3, stable diffusion, replicate, midjourney, CogView-3, visionÊ®°Âûã
-  ‚úÖ   **‰∏∞ÂØåÊèí‰ª∂Ôºö** ÊîØÊåÅ‰∏™ÊÄßÂåñÊèí‰ª∂Êâ©Â±ïÔºåÂ∑≤ÂÆûÁé∞Â§öËßíËâ≤ÂàáÊç¢„ÄÅÊñáÂ≠óÂÜíÈô©„ÄÅÊïèÊÑüËØçËøáÊª§„ÄÅËÅäÂ§©ËÆ∞ÂΩïÊÄªÁªì„ÄÅÊñáÊ°£ÊÄªÁªìÂíåÂØπËØù„ÄÅËÅîÁΩëÊêúÁ¥¢Á≠âÊèí‰ª∂
-  ‚úÖ   **Áü•ËØÜÂ∫ìÔºö** ÈÄöËøá‰∏ä‰º†Áü•ËØÜÂ∫ìÊñá‰ª∂Ëá™ÂÆö‰πâ‰∏ìÂ±ûÊú∫Âô®‰∫∫ÔºåÂèØ‰Ωú‰∏∫Êï∞Â≠óÂàÜË∫´„ÄÅÊô∫ËÉΩÂÆ¢Êúç„ÄÅÁßÅÂüüÂä©Êâã‰ΩøÁî®ÔºåÂü∫‰∫é [LinkAI](https://link-ai.tech) ÂÆûÁé∞

## Â£∞Êòé

1. Êú¨È°πÁõÆÈÅµÂæ™ [MITÂºÄÊ∫êÂçèËÆÆ](/LICENSE)Ôºå‰ªÖÁî®‰∫éÊäÄÊúØÁ†îÁ©∂ÂíåÂ≠¶‰π†Ôºå‰ΩøÁî®Êú¨È°πÁõÆÊó∂ÈúÄÈÅµÂÆàÊâÄÂú®Âú∞Ê≥ïÂæãÊ≥ïËßÑ„ÄÅÁõ∏ÂÖ≥ÊîøÁ≠ñ‰ª•Âèä‰ºÅ‰∏öÁ´†Á®ãÔºåÁ¶ÅÊ≠¢Áî®‰∫é‰ªª‰ΩïËøùÊ≥ïÊàñ‰æµÁäØ‰ªñ‰∫∫ÊùÉÁõäÁöÑË°å‰∏∫
2. Â¢ÉÂÜÖ‰ΩøÁî®ËØ•È°πÁõÆÊó∂ÔºåËØ∑‰ΩøÁî®ÂõΩÂÜÖÂéÇÂïÜÁöÑÂ§ßÊ®°ÂûãÊúçÂä°ÔºåÂπ∂ËøõË°åÂøÖË¶ÅÁöÑÂÜÖÂÆπÂÆâÂÖ®ÂÆ°Ê†∏ÂèäËøáÊª§
3. Êú¨È°πÁõÆ‰∏ªË¶ÅÊé•ÂÖ•ÂçèÂêåÂäûÂÖ¨Âπ≥Âè∞ÔºåÊé®Ëçê‰ΩøÁî®ÂÖ¨‰ºóÂè∑„ÄÅ‰ºÅÂæÆËá™Âª∫Â∫îÁî®„ÄÅÈíâÈíâ„ÄÅÈ£û‰π¶Á≠âÊé•ÂÖ•ÈÄöÈÅìÔºåÂÖ∂‰ªñÈÄöÈÅì‰∏∫ÂéÜÂè≤‰∫ßÁâ©Â∑≤‰∏çÁª¥Êä§
4. ‰ªª‰Ωï‰∏™‰∫∫„ÄÅÂõ¢ÈòüÂíå‰ºÅ‰∏öÔºåÊó†ËÆ∫‰ª•‰ΩïÁßçÊñπÂºè‰ΩøÁî®ËØ•È°πÁõÆ„ÄÅÂØπ‰ΩïÂØπË±°Êèê‰æõÊúçÂä°ÔºåÊâÄ‰∫ßÁîüÁöÑ‰∏ÄÂàáÂêéÊûúÔºåÊú¨È°πÁõÆÂùá‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªª

## ÊºîÁ§∫

DEMOËßÜÈ¢ëÔºöhttps://cdn.link-ai.tech/doc/cow_demo.mp4

## Á§æÂå∫

Ê∑ªÂä†Â∞èÂä©ÊâãÂæÆ‰ø°Âä†ÂÖ•ÂºÄÊ∫êÈ°πÁõÆ‰∫§ÊµÅÁæ§Ôºö

&lt;img width=&quot;160&quot; src=&quot;https://img-1317903499.cos.ap-guangzhou.myqcloud.com/docs/open-community.png&quot;&gt;

&lt;br&gt;

# ‰ºÅ‰∏öÊúçÂä°

&lt;a href=&quot;https://link-ai.tech&quot; target=&quot;_blank&quot;&gt;&lt;img width=&quot;800&quot; src=&quot;https://cdn.link-ai.tech/image/link-ai-intro.jpg&quot;&gt;&lt;/a&gt;

&gt; [LinkAI](https://link-ai.tech/) ÊòØÈù¢Âêë‰ºÅ‰∏öÂíåÂºÄÂèëËÄÖÁöÑ‰∏ÄÁ´ôÂºèAIÂ∫îÁî®Âπ≥Âè∞ÔºåËÅöÂêàÂ§öÊ®°ÊÄÅÂ§ßÊ®°Âûã„ÄÅÁü•ËØÜÂ∫ì„ÄÅAgent Êèí‰ª∂„ÄÅÂ∑•‰ΩúÊµÅÁ≠âËÉΩÂäõÔºåÊîØÊåÅ‰∏ÄÈîÆÊé•ÂÖ•‰∏ªÊµÅÂπ≥Âè∞Âπ∂ËøõË°åÁÆ°ÁêÜÔºåÊîØÊåÅSaaS„ÄÅÁßÅÊúâÂåñÈÉ®ÁΩ≤Â§öÁßçÊ®°Âºè„ÄÇ
&gt;
&gt; LinkAI ÁõÆÂâç Â∑≤Âú®ÁßÅÂüüËøêËê•„ÄÅÊô∫ËÉΩÂÆ¢Êúç„ÄÅ‰ºÅ‰∏öÊïàÁéáÂä©ÊâãÁ≠âÂú∫ÊôØÁßØÁ¥Ø‰∫Ü‰∏∞ÂØåÁöÑ AI Ëß£ÂÜ≥ÊñπÊ°àÔºå Âú®ÁîµÂïÜ„ÄÅÊñáÊïô„ÄÅÂÅ•Â∫∑„ÄÅÊñ∞Ê∂àË¥π„ÄÅÁßëÊäÄÂà∂ÈÄ†Á≠âÂêÑË°å‰∏öÊ≤âÊ∑Ä‰∫ÜÂ§ßÊ®°ÂûãËêΩÂú∞Â∫îÁî®ÁöÑÊúÄ‰Ω≥ÂÆûË∑µÔºåËá¥Âäõ‰∫éÂ∏ÆÂä©Êõ¥Â§ö‰ºÅ‰∏öÂíåÂºÄÂèëËÄÖÊã•Êä± AI Áîü‰∫ßÂäõ„ÄÇ

**‰ºÅ‰∏öÊúçÂä°Âíå‰∫ßÂìÅÂí®ËØ¢** ÂèØËÅîÁ≥ª‰∫ßÂìÅÈ°æÈóÆÔºö

&lt;img width=&quot;160&quot; src=&quot;https://cdn.link-ai.tech/consultant-s.jpg&quot;&gt;

&lt;br&gt;

# üè∑ Êõ¥Êñ∞Êó•Âøó

&gt;**2025.05.23Ôºö** [1.7.6ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.6) ‰ºòÂåñwebÁΩëÈ°µchannel„ÄÅÊñ∞Â¢û[AgentMeshÂ§öÊô∫ËÉΩ‰ΩìÊèí‰ª∂](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/plugins/agent/README.md)„ÄÅÁôæÂ∫¶ËØ≠Èü≥ÂêàÊàê‰ºòÂåñ„ÄÅ‰ºÅÂæÆÂ∫îÁî®`access_token`Ëé∑Âèñ‰ºòÂåñ„ÄÅÊîØÊåÅ`claude-4-sonnet`Âíå`claude-4-opus`Ê®°Âûã

&gt;**2025.04.11Ôºö** [1.7.5ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.5) Êñ∞Â¢ûÊîØÊåÅ [wechatferry](https://github.com/zhayujie/chatgpt-on-wechat/pull/2562) ÂçèËÆÆ„ÄÅÊñ∞Â¢û deepseek Ê®°Âûã„ÄÅÊñ∞Â¢ûÊîØÊåÅËÖæËÆØ‰∫ëËØ≠Èü≥ËÉΩÂäõ„ÄÅÊñ∞Â¢ûÊîØÊåÅ ModelScope Âíå Gitee-AI APIÊé•Âè£

&gt;**2024.12.13Ôºö** [1.7.4ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.4) Êñ∞Â¢û Gemini 2.0 Ê®°Âûã„ÄÅÊñ∞Â¢ûweb channel„ÄÅËß£ÂÜ≥ÂÜÖÂ≠òÊ≥ÑÊºèÈóÆÈ¢ò„ÄÅËß£ÂÜ≥ `#reloadp` ÂëΩ‰ª§ÈáçËΩΩ‰∏çÁîüÊïàÈóÆÈ¢ò

&gt;**2024.10.31Ôºö** [1.7.3ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.3) Á®ãÂ∫èÁ®≥ÂÆöÊÄßÊèêÂçá„ÄÅÊï∞ÊçÆÂ∫ìÂäüËÉΩ„ÄÅClaudeÊ®°Âûã‰ºòÂåñ„ÄÅlinkaiÊèí‰ª∂‰ºòÂåñ„ÄÅÁ¶ªÁ∫øÈÄöÁü•

&gt;**2024.09.26Ôºö** [1.7.2ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.2)  Âíå [1.7.1ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.1) ÊñáÂøÉÔºåËÆØÈ£ûÁ≠âÊ®°Âûã‰ºòÂåñ„ÄÅo1 Ê®°Âûã„ÄÅÂø´ÈÄüÂÆâË£ÖÂíåÁÆ°ÁêÜËÑöÊú¨

&gt;**2024.08.02Ôºö** [1.7.0ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.0) Êñ∞Â¢û ËÆØÈ£û4.0 Ê®°Âûã„ÄÅÁü•ËØÜÂ∫ìÂºïÁî®Êù•Ê∫êÂ±ïÁ§∫„ÄÅÁõ∏ÂÖ≥Êèí‰ª∂‰ºòÂåñ

&gt;**2024.07.19Ôºö** [1.6.9ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.6.9) Êñ∞Â¢û gpt-4o-mini Ê®°Âûã„ÄÅÈòøÈáåËØ≠Èü≥ËØÜÂà´„ÄÅ‰ºÅÂæÆÂ∫îÁî®Ê∏†ÈÅìË∑ØÁî±‰ºòÂåñ

&gt;**2024.07.05Ôºö** [1.6.8ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.6.8) Âíå [1.6.7ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.6.7)ÔºåClaude3.5, Gemini 1.5 Pro, MiniMaxÊ®°Âûã„ÄÅÂ∑•‰ΩúÊµÅÂõæÁâáËæìÂÖ•„ÄÅÊ®°ÂûãÂàóË°®ÂÆåÂñÑ

&gt;**2024.06.04Ôºö** [1.6.6ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.6.6) Âíå [1.6.5ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.6.5)Ôºågpt-4oÊ®°Âûã„ÄÅÈíâÈíâÊµÅÂºèÂç°Áâá„ÄÅËÆØÈ£ûËØ≠Èü≥ËØÜÂà´/ÂêàÊàê

&gt;**2024.04.26Ôºö** [1.6.0ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.6.0)ÔºåÊñ∞Â¢û Kimi Êé•ÂÖ•„ÄÅgpt-4-turboÁâàÊú¨ÂçáÁ∫ß„ÄÅÊñá‰ª∂ÊÄªÁªìÂíåËØ≠Èü≥ËØÜÂà´ÈóÆÈ¢ò‰øÆÂ§ç

&gt;**2024.03.26Ôºö** [1.5.8ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.8) Âíå [1.5.7ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.7)ÔºåÊñ∞Â¢û GLM-4„ÄÅClaude-3 Ê®°ÂûãÔºåedge-tts ËØ≠Èü≥ÊîØÊåÅ

&gt;**2024.01.26Ôºö** [1.5.6ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.6) Âíå [1.5.5ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.5)ÔºåÈíâÈíâÊé•ÂÖ•ÔºåtoolÊèí‰ª∂ÂçáÁ∫ßÔºå4-turboÊ®°ÂûãÊõ¥Êñ∞

&gt;**2023.11.11Ôºö** [1.5.3ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.3) Âíå [1.5.4ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.4)ÔºåÊñ∞Â¢ûÈÄö‰πâÂçÉÈóÆÊ®°Âûã„ÄÅGoogle Gemini

&gt;**2023.11.10Ôºö** [1.5.2ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.2)ÔºåÊñ∞Â¢ûÈ£û‰π¶ÈÄöÈÅì„ÄÅÂõæÂÉèËØÜÂà´ÂØπËØù„ÄÅÈªëÂêçÂçïÈÖçÁΩÆ

&gt;**2023.11.10Ôºö** [1.5.0ÁâàÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.5.0)ÔºåÊñ∞Â¢û `gpt-4-turbo`, `dall-e-3`, `tts` Ê®°ÂûãÊé•ÂÖ•ÔºåÂÆåÂñÑÂõæÂÉèÁêÜËß£&amp;ÁîüÊàê„ÄÅËØ≠Èü≥ËØÜÂà´&amp;ÁîüÊàêÁöÑÂ§öÊ®°ÊÄÅËÉΩÂäõ

&gt;**2023.10.16Ôºö** ÊîØÊåÅÈÄöËøáÊÑèÂõæËØÜÂà´‰ΩøÁî®LinkAIËÅîÁΩëÊêúÁ¥¢„ÄÅÊï∞Â≠¶ËÆ°ÁÆó„ÄÅÁΩëÈ°µËÆøÈóÆÁ≠âÊèí‰ª∂ÔºåÂèÇËÄÉ[Êèí‰ª∂ÊñáÊ°£](https://docs.link-ai.tech/platform/plugins)

&gt;**2023.09.26Ôºö** Êèí‰ª∂Â¢ûÂä† Êñá‰ª∂/ÊñáÁ´†ÈìæÊé• ‰∏ÄÈîÆÊÄªÁªìÂíåÂØπËØùÁöÑÂäüËÉΩÔºå‰ΩøÁî®ÂèÇËÄÉÔºö[Êèí‰ª∂ËØ¥Êòé](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins/linkai#3%E6%96%87%E6%A1%A3%E6%80%BB%E7%BB%93%E5%AF%B9%E8%AF%9D%E5%8A%9F%E8%83%BD)

&gt;**2023.08.08Ôºö** Êé•ÂÖ•ÁôæÂ∫¶ÊñáÂøÉ‰∏ÄË®ÄÊ®°ÂûãÔºåÈÄöËøá [Êèí‰ª∂](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins/linkai) ÊîØÊåÅ Midjourney ÁªòÂõæ

&gt;**2023.06.12Ôºö** Êé•ÂÖ• [LinkAI](https://link-ai.tech/console) Âπ≥Âè∞ÔºåÂèØÂú®Á∫øÂàõÂª∫È¢ÜÂüüÁü•ËØÜÂ∫ìÔºåÊâìÈÄ†‰∏ìÂ±ûÂÆ¢ÊúçÊú∫Âô®‰∫∫„ÄÇ‰ΩøÁî®ÂèÇËÄÉ [Êé•ÂÖ•ÊñáÊ°£](https://link-ai.tech/platform/link-app/wechat)„ÄÇ

Êõ¥Êó©Êõ¥Êñ∞Êó•ÂøóÊü•Áúã: [ÂΩíÊ°£Êó•Âøó](/docs/version/old-version.md)

&lt;br&gt;

# üöÄ Âø´ÈÄüÂºÄÂßã

- Âø´ÈÄüÂºÄÂßãËØ¶ÁªÜÊñáÊ°£Ôºö[È°πÁõÆÊê≠Âª∫ÊñáÊ°£](https://docs.link-ai.tech/cow/quick-start)

- Âø´ÈÄüÂÆâË£ÖËÑöÊú¨ÔºåËØ¶ÁªÜ‰ΩøÁî®ÊåáÂØºÔºö[‰∏ÄÈîÆÂÆâË£ÖÂêØÂä®ËÑöÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/wiki/%E4%B8%80%E9%94%AE%E5%AE%89%E8%A3%85%E5%90%AF%E5%8A%A8%E8%84%9A%E6%9C%AC)
```bash
bash &lt;(curl -sS https://cdn.link-ai.tech/code/cow/install.sh)
```
- È°πÁõÆÁÆ°ÁêÜËÑöÊú¨ÔºåËØ¶ÁªÜ‰ΩøÁî®ÊåáÂØºÔºö[È°πÁõÆÁÆ°ÁêÜËÑöÊú¨](https://github.com/zhayujie/chatgpt-on-wechat/wiki/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E8%84%9A%E6%9C%AC)
## ‰∏Ä„ÄÅÂáÜÂ§á

### 1. Ë¥¶Âè∑Ê≥®ÂÜå

È°πÁõÆÈªòËÆ§‰ΩøÁî®OpenAIÊé•Âè£ÔºåÈúÄÂâçÂæÄ [OpenAIÊ≥®ÂÜåÈ°µÈù¢](https://beta.openai.com/signup) ÂàõÂª∫Ë¥¶Âè∑ÔºåÂàõÂª∫ÂÆåË¥¶Âè∑ÂàôÂâçÂæÄ [APIÁÆ°ÁêÜÈ°µÈù¢](https://beta.openai.com/account/api-keys) ÂàõÂª∫‰∏Ä‰∏™ API Key Âπ∂‰øùÂ≠ò‰∏ãÊù•ÔºåÂêéÈù¢ÈúÄË¶ÅÂú®È°πÁõÆ‰∏≠ÈÖçÁΩÆËøô‰∏™key„ÄÇÊé•Âè£ÈúÄË¶ÅÊµ∑Â§ñÁΩëÁªúËÆøÈóÆÂèäÁªëÂÆö‰ø°Áî®Âç°ÊîØ‰ªò„ÄÇ

&gt; ÈªòËÆ§ÂØπËØùÊ®°ÂûãÊòØ openai ÁöÑ gpt-3.5-turboÔºåËÆ°Ë¥πÊñπÂºèÊòØÁ∫¶ÊØè 1000tokens (Á∫¶750‰∏™Ëã±ÊñáÂçïËØç Êàñ 500Ê±âÂ≠óÔºåÂåÖÂê´ËØ∑Ê±ÇÂíåÂõûÂ§ç) Ê∂àËÄó $0.002ÔºåÂõæÁâáÁîüÊàêÊòØDell EÊ®°ÂûãÔºåÊØèÂº†Ê∂àËÄó $0.016„ÄÇ

È°πÁõÆÂêåÊó∂‰πüÊîØÊåÅ‰ΩøÁî® LinkAI Êé•Âè£ÔºåÊó†ÈúÄ‰ª£ÁêÜÔºåÂèØ‰ΩøÁî® Kimi„ÄÅÊñáÂøÉ„ÄÅËÆØÈ£û„ÄÅGPT-3.5„ÄÅGPT-4o Á≠âÊ®°ÂûãÔºåÊîØÊåÅ ÂÆöÂà∂ÂåñÁü•ËØÜÂ∫ì„ÄÅËÅîÁΩëÊêúÁ¥¢„ÄÅMJÁªòÂõæ„ÄÅÊñáÊ°£ÊÄªÁªì„ÄÅÂ∑•‰ΩúÊµÅÁ≠âËÉΩÂäõ„ÄÇ‰øÆÊîπÈÖçÁΩÆÂç≥ÂèØ‰∏ÄÈîÆ‰ΩøÁî®ÔºåÂèÇËÄÉ [Êé•ÂÖ•ÊñáÊ°£](https://link-ai.tech/platform/link-app/wechat)„ÄÇ

### 2.ËøêË°åÁéØÂ¢É

ÊîØÊåÅ Linux„ÄÅMacOS„ÄÅWindows Á≥ªÁªüÔºàÂèØÂú®LinuxÊúçÂä°Âô®‰∏äÈïøÊúüËøêË°å)ÔºåÂêåÊó∂ÈúÄÂÆâË£Ö `Python`„ÄÇ
&gt; Âª∫ËÆÆPythonÁâàÊú¨Âú® 3.7.1~3.9.X ‰πãÈó¥ÔºåÊé®Ëçê3.8ÁâàÊú¨Ôºå3.10Âèä‰ª•‰∏äÁâàÊú¨Âú® MacOS ÂèØÁî®ÔºåÂÖ∂‰ªñÁ≥ªÁªü‰∏ä‰∏çÁ°ÆÂÆöËÉΩÂê¶Ê≠£Â∏∏ËøêË°å„ÄÇ

&gt; Ê≥®ÊÑèÔºöDocker Êàñ Railway ÈÉ®ÁΩ≤Êó†ÈúÄÂÆâË£ÖpythonÁéØÂ¢ÉÂíå‰∏ãËΩΩÊ∫êÁ†ÅÔºåÂèØÁõ¥Êé•Âø´ËøõÂà∞‰∏ã‰∏ÄËäÇ„ÄÇ

**(1) ÂÖãÈöÜÈ°πÁõÆ‰ª£Á†ÅÔºö**

```bash
git clone https://github.com/zhayujie/chatgpt-on-wechat
cd chatgpt-on-wechat/
```

Ê≥®: Â¶ÇÈÅáÂà∞ÁΩëÁªúÈóÆÈ¢òÂèØÈÄâÊã©ÂõΩÂÜÖÈïúÂÉè https://gitee.com/zhayujie/chatgpt-on-wechat

**(2) ÂÆâË£ÖÊ†∏ÂøÉ‰æùËµñ (ÂøÖÈÄâ)Ôºö**
&gt; ËÉΩÂ§ü‰ΩøÁî®`itchat`ÂàõÂª∫Êú∫Âô®‰∫∫ÔºåÂπ∂ÂÖ∑ÊúâÊñáÂ≠ó‰∫§ÊµÅÂäüËÉΩÊâÄÈúÄÁöÑÊúÄÂ∞è‰æùËµñÈõÜÂêà„ÄÇ
```bash
pip3 install -r requirements.txt
```

**(3) ÊãìÂ±ï‰æùËµñ (ÂèØÈÄâÔºåÂª∫ËÆÆÂÆâË£Ö)Ôºö**

```bash
pip3 install -r requirements-optional.txt
```
&gt; Â¶ÇÊûúÊüêÈ°π‰æùËµñÂÆâË£ÖÂ§±Ë¥•ÂèØÊ≥®ÈáäÊéâÂØπÂ∫îÁöÑË°åÂÜçÁªßÁª≠

## ‰∫å„ÄÅÈÖçÁΩÆ

ÈÖçÁΩÆÊñá‰ª∂ÁöÑÊ®°ÊùøÂú®Ê†πÁõÆÂΩïÁöÑ`config-template.json`‰∏≠ÔºåÈúÄÂ§çÂà∂ËØ•Ê®°ÊùøÂàõÂª∫ÊúÄÁªàÁîüÊïàÁöÑ `config.json` Êñá‰ª∂Ôºö

```bash
  cp config-template.json config.json
```

ÁÑ∂ÂêéÂú®`config.json`‰∏≠Â°´ÂÖ•ÈÖçÁΩÆÔºå‰ª•‰∏ãÊòØÂØπÈªòËÆ§ÈÖçÁΩÆÁöÑËØ¥ÊòéÔºåÂèØÊ†πÊçÆÈúÄË¶ÅËøõË°åËá™ÂÆö‰πâ‰øÆÊîπÔºàÊ≥®ÊÑèÂÆûÈôÖ‰ΩøÁî®Êó∂ËØ∑ÂéªÊéâÊ≥®ÈáäÔºå‰øùËØÅJSONÊ†ºÂºèÁöÑÂÆåÊï¥ÔºâÔºö

```bash
# config.jsonÊñá‰ª∂ÂÜÖÂÆπÁ§∫‰æã
{
  &quot;model&quot;: &quot;gpt-4o-mini&quot;,                                     # Ê®°ÂûãÂêçÁß∞, ÊîØÊåÅ gpt-4o-mini, gpt-4.1, gpt-4o, wenxin, xunfei, glm-4, claude-3-7-sonnet-latest, moonshotÁ≠â
  &quot;open_ai_api_key&quot;: &quot;YOUR API KEY&quot;,                          # Â¶ÇÊûú‰ΩøÁî®openAIÊ®°ÂûãÂàôÂ°´ÂÖ•‰∏äÈù¢ÂàõÂª∫ÁöÑ OpenAI API KEY
  &quot;open_ai_api_base&quot;: &quot;https://api.openai.com/v1&quot;,            # OpenAIÊé•Âè£‰ª£ÁêÜÂú∞ÂùÄ
  &quot;proxy&quot;: &quot;&quot;,                                                # ‰ª£ÁêÜÂÆ¢Êà∑Á´ØÁöÑipÂíåÁ´ØÂè£ÔºåÂõΩÂÜÖÁéØÂ¢ÉÂºÄÂêØ‰ª£ÁêÜÁöÑÈúÄË¶ÅÂ°´ÂÜôËØ•È°πÔºåÂ¶Ç &quot;127.0.0.1:7890&quot;
  &quot;single_chat_prefix&quot;: [&quot;bot&quot;, &quot;@bot&quot;],                      # ÁßÅËÅäÊó∂ÊñáÊú¨ÈúÄË¶ÅÂåÖÂê´ËØ•ÂâçÁºÄÊâçËÉΩËß¶ÂèëÊú∫Âô®‰∫∫ÂõûÂ§ç
  &quot;single_chat_reply_prefix&quot;: &quot;[bot] &quot;,                       # ÁßÅËÅäÊó∂Ëá™Âä®ÂõûÂ§çÁöÑÂâçÁºÄÔºåÁî®‰∫éÂå∫ÂàÜÁúü‰∫∫
  &quot;group_chat_prefix&quot;: [&quot;@bot&quot;],                              # Áæ§ËÅäÊó∂ÂåÖÂê´ËØ•ÂâçÁºÄÂàô‰ºöËß¶ÂèëÊú∫Âô®‰∫∫ÂõûÂ§ç
  &quot;group_name_white_list&quot;: [&quot;ChatGPTÊµãËØïÁæ§&quot;, &quot;ChatGPTÊµãËØïÁæ§2&quot;], # ÂºÄÂêØËá™Âä®ÂõûÂ§çÁöÑÁæ§ÂêçÁß∞ÂàóË°®
  &quot;group_chat_in_one_session&quot;: [&quot;ChatGPTÊµãËØïÁæ§&quot;],              # ÊîØÊåÅ‰ºöËØù‰∏ä‰∏ãÊñáÂÖ±‰∫´ÁöÑÁæ§ÂêçÁß∞  
  &quot;image_create_prefix&quot;: [&quot;Áîª&quot;, &quot;Áúã&quot;, &quot;Êâæ&quot;],                   # ÂºÄÂêØÂõæÁâáÂõûÂ§çÁöÑÂâçÁºÄ
  &quot;conversation_max_tokens&quot;: 1000,                            # ÊîØÊåÅ‰∏ä‰∏ãÊñáËÆ∞ÂøÜÁöÑÊúÄÂ§öÂ≠óÁ¨¶Êï∞
  &quot;speech_recognition&quot;: false,                                # ÊòØÂê¶ÂºÄÂêØËØ≠Èü≥ËØÜÂà´
  &quot;group_speech_recognition&quot;: false,                          # ÊòØÂê¶ÂºÄÂêØÁæ§ÁªÑËØ≠Èü≥ËØÜÂà´
  &quot;voice_reply_voice&quot;: false,                                 # ÊòØÂê¶‰ΩøÁî®ËØ≠Èü≥ÂõûÂ§çËØ≠Èü≥
  &quot;character_desc&quot;: &quot;‰Ω†ÊòØÂü∫‰∫éÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑAIÊô∫ËÉΩÂä©ÊâãÔºåÊó®Âú®ÂõûÁ≠îÂπ∂Ëß£ÂÜ≥‰∫∫‰ª¨ÁöÑ‰ªª‰ΩïÈóÆÈ¢òÔºåÂπ∂‰∏îÂèØ‰ª•‰ΩøÁî®Â§öÁßçËØ≠Ë®Ä‰∏é‰∫∫‰∫§ÊµÅ„ÄÇ&quot;,  # ‰∫∫Ê†ºÊèèËø∞
  # ËÆ¢ÈòÖÊ∂àÊÅØÔºåÂÖ¨‰ºóÂè∑Âíå‰ºÅ‰∏öÂæÆ‰ø°channel‰∏≠ËØ∑Â°´ÂÜôÔºåÂΩìË¢´ËÆ¢ÈòÖÊó∂‰ºöËá™Âä®ÂõûÂ§çÔºåÂèØ‰ΩøÁî®ÁâπÊÆäÂç†‰ΩçÁ¨¶„ÄÇÁõÆÂâçÊîØÊåÅÁöÑÂç†‰ΩçÁ¨¶Êúâ{trigger_prefix}ÔºåÂú®Á®ãÂ∫è‰∏≠ÂÆÉ‰ºöËá™Âä®ÊõøÊç¢ÊàêbotÁöÑËß¶ÂèëËØç„ÄÇ
  &quot;subscribe_msg&quot;: &quot;ÊÑüË∞¢ÊÇ®ÁöÑÂÖ≥Ê≥®ÔºÅ\nËøôÈáåÊòØChatGPTÔºåÂèØ‰ª•Ëá™Áî±ÂØπËØù„ÄÇ\nÊîØÊåÅËØ≠Èü≥ÂØπËØù„ÄÇ\nÊîØÊåÅÂõæÁâáËæìÂá∫ÔºåÁîªÂ≠óÂºÄÂ§¥ÁöÑÊ∂àÊÅØÂ∞ÜÊåâË¶ÅÊ±ÇÂàõ‰ΩúÂõæÁâá„ÄÇ\nÊîØÊåÅËßíËâ≤ÊâÆÊºîÂíåÊñáÂ≠óÂÜíÈô©Á≠â‰∏∞ÂØåÊèí‰ª∂„ÄÇ\nËæìÂÖ•{trigger_prefix}#help Êü•ÁúãËØ¶ÁªÜÊåá‰ª§„ÄÇ&quot;,
  &quot;use_linkai&quot;: false,                                        # ÊòØÂê¶‰ΩøÁî®LinkAIÊé•Âè£ÔºåÈªòËÆ§ÂÖ≥Èó≠ÔºåÂºÄÂêØÂêéÂèØÂõΩÂÜÖËÆøÈóÆÔºå‰ΩøÁî®Áü•ËØÜÂ∫ìÂíåMJ
  &quot;linkai_api_key&quot;: &quot;&quot;,                                       # LinkAI Api Key
  &quot;linkai_app_code&quot;: &quot;&quot;                                       # LinkAI Â∫îÁî®ÊàñÂ∑•‰ΩúÊµÅcode
}
```
**ÈÖçÁΩÆËØ¥ÊòéÔºö**

**1.‰∏™‰∫∫ËÅäÂ§©**

+ ‰∏™‰∫∫ËÅäÂ§©‰∏≠ÔºåÈúÄË¶Å‰ª• &quot;bot&quot;Êàñ&quot;@bot&quot; ‰∏∫ÂºÄÂ§¥ÁöÑÂÜÖÂÆπËß¶ÂèëÊú∫Âô®‰∫∫ÔºåÂØπÂ∫îÈÖçÁΩÆÈ°π `single_chat_prefix` (Â¶ÇÊûú‰∏çÈúÄË¶Å‰ª•ÂâçÁºÄËß¶ÂèëÂèØ‰ª•Â°´ÂÜô  `&quot;single_chat_prefix&quot;: [&quot;&quot;]`)
+ Êú∫Âô®‰∫∫ÂõûÂ§çÁöÑÂÜÖÂÆπ‰ºö‰ª• &quot;[bot] &quot; ‰Ωú‰∏∫ÂâçÁºÄÔºå ‰ª•Âå∫ÂàÜÁúü‰∫∫ÔºåÂØπÂ∫îÁöÑÈÖçÁΩÆÈ°π‰∏∫ `single_chat_reply_prefix` (Â¶ÇÊûú‰∏çÈúÄË¶ÅÂâçÁºÄÂèØ‰ª•Â°´ÂÜô `&quot;single_chat_reply_prefix&quot;: &quot;&quot;`)

**2.Áæ§ÁªÑËÅäÂ§©**

+ Áæ§ÁªÑËÅäÂ§©‰∏≠ÔºåÁæ§ÂêçÁß∞ÈúÄÈÖçÁΩÆÂú® `group_name_white_list ` ‰∏≠ÊâçËÉΩÂºÄÂêØÁæ§ËÅäËá™Âä®ÂõûÂ§ç„ÄÇÂ¶ÇÊûúÊÉ≥ÂØπÊâÄÊúâÁæ§ËÅäÁîüÊïàÔºåÂèØ‰ª•Áõ¥Êé•Â°´ÂÜô `&quot;group_name_white_list&quot;: [&quot;ALL_GROUP&quot;]`
+ ÈªòËÆ§Âè™Ë¶ÅË¢´‰∫∫ @ Â∞±‰ºöËß¶ÂèëÊú∫Âô®‰∫∫Ëá™Âä®ÂõûÂ§çÔºõÂè¶Â§ñÁæ§ËÅäÂ§©‰∏≠Âè™Ë¶ÅÊ£ÄÊµãÂà∞‰ª• &quot;@bot&quot; ÂºÄÂ§¥ÁöÑÂÜÖÂÆπÔºåÂêåÊ†∑‰ºöËá™Âä®ÂõûÂ§çÔºàÊñπ‰æøËá™Â∑±Ëß¶ÂèëÔºâÔºåËøôÂØπÂ∫îÈÖçÁΩÆÈ°π `group_chat_prefix`
+ ÂèØÈÄâÈÖçÁΩÆ: `group_name_keyword_white_list`ÈÖçÁΩÆÈ°πÊîØÊåÅÊ®°Á≥äÂåπÈÖçÁæ§ÂêçÁß∞Ôºå`group_chat_keyword`ÈÖçÁΩÆÈ°πÂàôÊîØÊåÅÊ®°Á≥äÂåπÈÖçÁæ§Ê∂àÊÅØÂÜÖÂÆπÔºåÁî®Ê≥ï‰∏é‰∏äËø∞‰∏§‰∏™ÈÖçÁΩÆÈ°πÁõ∏Âêå„ÄÇÔºàContributed by [evolay](https://github.com/evolay))
+ `group_chat_in_one_session`Ôºö‰ΩøÁæ§ËÅäÂÖ±‰∫´‰∏Ä‰∏™‰ºöËØù‰∏ä‰∏ãÊñáÔºåÈÖçÁΩÆ `[&quot;ALL_GROUP&quot;]` Âàô‰ΩúÁî®‰∫éÊâÄÊúâÁæ§ËÅä

**3.ËØ≠Èü≥ËØÜÂà´**

+ Ê∑ªÂä† `&quot;speech_recognition&quot;: true` Â∞ÜÂºÄÂêØËØ≠Èü≥ËØÜÂà´ÔºåÈªòËÆ§‰ΩøÁî®openaiÁöÑwhisperÊ®°ÂûãËØÜÂà´‰∏∫ÊñáÂ≠óÔºåÂêåÊó∂‰ª•ÊñáÂ≠óÂõûÂ§çÔºåËØ•ÂèÇÊï∞‰ªÖÊîØÊåÅÁßÅËÅä (Ê≥®ÊÑèÁî±‰∫éËØ≠Èü≥Ê∂àÊÅØÊó†Ê≥ïÂåπÈÖçÂâçÁºÄÔºå‰∏ÄÊó¶ÂºÄÂêØÂ∞ÜÂØπÊâÄÊúâËØ≠Èü≥Ëá™Âä®ÂõûÂ§çÔºåÊîØÊåÅËØ≠Èü≥Ëß¶ÂèëÁîªÂõæ)Ôºõ
+ Ê∑ªÂä† `&quot;group_speech_recognition&quot;: true` Â∞ÜÂºÄÂêØÁæ§ÁªÑËØ≠Èü≥ËØÜÂà´ÔºåÈªòËÆ§‰ΩøÁî®openaiÁöÑwhisperÊ®°ÂûãËØÜÂà´‰∏∫ÊñáÂ≠óÔºåÂêåÊó∂‰ª•ÊñáÂ≠óÂõûÂ§çÔºåÂèÇÊï∞‰ªÖÊîØÊåÅÁæ§ËÅä (‰ºöÂåπÈÖçgroup_chat_prefixÂíågroup_chat_keyword, ÊîØÊåÅËØ≠Èü≥Ëß¶ÂèëÁîªÂõæ)Ôºõ
+ Ê∑ªÂä† `&quot;voice_reply_voice&quot;: true` Â∞ÜÂºÄÂêØËØ≠Èü≥ÂõûÂ§çËØ≠Èü≥ÔºàÂêåÊó∂‰ΩúÁî®‰∫éÁßÅËÅäÂíåÁæ§ËÅäÔºâ

**4.ÂÖ∂‰ªñÈÖçÁΩÆ**

+ `model`: Ê®°ÂûãÂêçÁß∞ÔºåÁõÆÂâçÊîØÊåÅ `gpt-4o-mini`, `gpt-4.1`, `gpt-4o`, `gpt-3.5-turbo`, `wenxin` , `claude` , `gemini`, `glm-4`,  `xunfei`, `moonshot`Á≠âÔºåÂÖ®ÈÉ®Ê®°ÂûãÂêçÁß∞ÂèÇËÄÉ[common/const.py](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/common/const.py)Êñá‰ª∂
+ `temperature`,`frequency_penalty`,`presence_penalty`: Chat APIÊé•Âè£ÂèÇÊï∞ÔºåËØ¶ÊÉÖÂèÇËÄÉ[OpenAIÂÆòÊñπÊñáÊ°£„ÄÇ](https://platform.openai.com/docs/api-reference/chat)
+ `proxy`ÔºöÁî±‰∫éÁõÆÂâç `openai` Êé•Âè£ÂõΩÂÜÖÊó†Ê≥ïËÆøÈóÆÔºåÈúÄÈÖçÁΩÆ‰ª£ÁêÜÂÆ¢Êà∑Á´ØÁöÑÂú∞ÂùÄÔºåËØ¶ÊÉÖÂèÇËÄÉ  [#351](https://github.com/zhayujie/chatgpt-on-wechat/issues/351)
+ ÂØπ‰∫éÂõæÂÉèÁîüÊàêÔºåÂú®Êª°Ë∂≥‰∏™‰∫∫ÊàñÁæ§ÁªÑËß¶ÂèëÊù°‰ª∂Â§ñÔºåËøòÈúÄË¶ÅÈ¢ùÂ§ñÁöÑÂÖ≥ÈîÆËØçÂâçÁºÄÊù•Ëß¶ÂèëÔºåÂØπÂ∫îÈÖçÁΩÆ `image_create_prefix `
+ ÂÖ≥‰∫éOpenAIÂØπËØùÂèäÂõæÁâáÊé•Âè£ÁöÑÂèÇÊï∞ÈÖçÁΩÆÔºàÂÜÖÂÆπËá™Áî±Â∫¶„ÄÅÂõûÂ§çÂ≠óÊï∞ÈôêÂà∂„ÄÅÂõæÁâáÂ§ßÂ∞èÁ≠âÔºâÔºåÂèØ‰ª•ÂèÇËÄÉ [ÂØπËØùÊé•Âè£](https://beta.openai.com/docs/api-reference/completions) Âíå [ÂõæÂÉèÊé•Âè£](https://beta.openai.com/docs/api-reference/completions)  ÊñáÊ°£ÔºåÂú®[`config.py`](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/config.py)‰∏≠Ê£ÄÊü•Âì™‰∫õÂèÇÊï∞Âú®Êú¨È°πÁõÆ‰∏≠ÊòØÂèØÈÖçÁΩÆÁöÑ„ÄÇ
+ `conversation_max_tokens`ÔºöË°®Á§∫ËÉΩÂ§üËÆ∞ÂøÜÁöÑ‰∏ä‰∏ãÊñáÊúÄÂ§ßÂ≠óÊï∞Ôºà‰∏ÄÈóÆ‰∏ÄÁ≠î‰∏∫‰∏ÄÁªÑÂØπËØùÔºåÂ¶ÇÊûúÁ¥ØÁßØÁöÑÂØπËØùÂ≠óÊï∞Ë∂ÖÂá∫ÈôêÂà∂ÔºåÂ∞±‰ºö‰ºòÂÖàÁßªÈô§ÊúÄÊó©ÁöÑ‰∏ÄÁªÑÂØπËØùÔºâ
+ `rate_limit_chatgpt`Ôºå`rate_limit_dalle`ÔºöÊØèÂàÜÈíüÊúÄÈ´òÈóÆÁ≠îÈÄüÁéá„ÄÅÁîªÂõæÈÄüÁéáÔºåË∂ÖÈÄüÂêéÊéíÈòüÊåâÂ∫èÂ§ÑÁêÜ„ÄÇ
+ `clear_memory_commands`: ÂØπËØùÂÜÖÊåá‰ª§Ôºå‰∏ªÂä®Ê∏ÖÁ©∫ÂâçÊñáËÆ∞ÂøÜÔºåÂ≠óÁ¨¶‰∏≤Êï∞ÁªÑÂèØËá™ÂÆö‰πâÊåá‰ª§Âà´Âêç„ÄÇ
+ `hot_reload`: Á®ãÂ∫èÈÄÄÂá∫ÂêéÔºåÊöÇÂ≠òÁ≠â‰∫éÁä∂ÊÄÅÔºåÈªòËÆ§ÂÖ≥Èó≠„ÄÇ
+ `character_desc` ÈÖçÁΩÆ‰∏≠‰øùÂ≠òÁùÄ‰Ω†ÂØπÊú∫Âô®‰∫∫ËØ¥ÁöÑ‰∏ÄÊÆµËØùÔºå‰ªñ‰ºöËÆ∞‰ΩèËøôÊÆµËØùÂπ∂‰Ωú‰∏∫‰ªñÁöÑËÆæÂÆöÔºå‰Ω†ÂèØ‰ª•‰∏∫‰ªñÂÆöÂà∂‰ªª‰Ωï‰∫∫Ê†º      (ÂÖ≥‰∫é‰ºöËØù‰∏ä‰∏ãÊñáÁöÑÊõ¥Â§öÂÜÖÂÆπÂèÇËÄÉËØ• [issue](https://github.com/zhayujie/chatgpt-on-wechat/issues/43))
+ `subscribe_msg`ÔºöËÆ¢ÈòÖÊ∂àÊÅØÔºåÂÖ¨‰ºóÂè∑Âíå‰ºÅ‰∏öÂæÆ‰ø°channel‰∏≠ËØ∑Â°´ÂÜôÔºåÂΩìË¢´ËÆ¢ÈòÖÊó∂‰ºöËá™Âä®ÂõûÂ§çÔºå ÂèØ‰ΩøÁî®ÁâπÊÆäÂç†‰ΩçÁ¨¶„ÄÇÁõÆÂâçÊîØÊåÅÁöÑÂç†‰ΩçÁ¨¶Êúâ{trigger_prefix}ÔºåÂú®Á®ãÂ∫è‰∏≠ÂÆÉ‰ºöËá™Âä®ÊõøÊç¢ÊàêbotÁöÑËß¶ÂèëËØç„ÄÇ

**5.LinkAIÈÖçÁΩÆ (ÂèØÈÄâ)**

+ `use_linkai`: ÊòØÂê¶‰ΩøÁî®LinkAIÊé•Âè£ÔºåÂºÄÂêØÂêéÂèØÂõΩÂÜÖËÆøÈóÆÔºå‰ΩøÁî®Áü•ËØÜÂ∫ìÂíå `Midjourney` ÁªòÁîª, ÂèÇËÄÉ [ÊñáÊ°£](https://link-ai.tech/platform/link-app/wechat)
+ `linkai_api_key`: LinkAI Api KeyÔºåÂèØÂú® [ÊéßÂà∂Âè∞](https://link-ai.tech/console/interface) ÂàõÂª∫
+ `linkai_app_code`: LinkAI Â∫îÁî®ÊàñÂ∑•‰ΩúÊµÅÁöÑcodeÔºåÈÄâÂ°´

**Êú¨ËØ¥ÊòéÊñáÊ°£ÂèØËÉΩ‰ºöÊú™ÂèäÊó∂Êõ¥Êñ∞ÔºåÂΩìÂâçÊâÄÊúâÂèØÈÄâÁöÑÈÖçÁΩÆÈ°πÂùáÂú®ËØ•[`config.py`](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/config.py)‰∏≠ÂàóÂá∫„ÄÇ**

## ‰∏â„ÄÅËøêË°å

### 1.Êú¨Âú∞ËøêË°å

Â¶ÇÊûúÊòØÂºÄÂèëÊú∫ **Êú¨Âú∞ËøêË°å**ÔºåÁõ¥Êé•Âú®È°πÁõÆÊ†πÁõÆÂΩï‰∏ãÊâßË°åÔºö

```bash
python3 app.py                                    # windowsÁéØÂ¢É‰∏ãËØ•ÂëΩ‰ª§ÈÄöÂ∏∏‰∏∫ python app.py
```

ÁªàÁ´ØËæìÂá∫‰∫åÁª¥Á†ÅÂêéÔºåËøõË°åÊâ´Á†ÅÁôªÂΩïÔºåÂΩìËæìÂá∫ &quot;Start auto replying&quot; Êó∂Ë°®Á§∫Ëá™Âä®ÂõûÂ§çÁ®ãÂ∫èÂ∑≤ÁªèÊàêÂäüËøêË°å‰∫ÜÔºàÊ≥®ÊÑèÔºöÁî®‰∫éÁôªÂΩïÁöÑË¥¶Âè∑ÈúÄË¶ÅÂú®ÊîØ‰ªòÂ§ÑÂ∑≤ÂÆåÊàêÂÆûÂêçËÆ§ËØÅÔºâ„ÄÇÊâ´Á†ÅÁôªÂΩïÂêé‰Ω†ÁöÑË¥¶Âè∑Â∞±Êàê‰∏∫Êú∫Âô®‰∫∫‰∫ÜÔºåÂèØ‰ª•Âú®ÊâãÊú∫Á´ØÈÄöËøáÈÖçÁΩÆÁöÑÂÖ≥ÈîÆËØçËß¶ÂèëËá™Âä®ÂõûÂ§ç (‰ªªÊÑèÂ•ΩÂèãÂèëÈÄÅÊ∂àÊÅØÁªô‰Ω†ÔºåÊàñÊòØËá™Â∑±ÂèëÊ∂àÊÅØÁªôÂ•ΩÂèã)ÔºåÂèÇËÄÉ[#142](https://github.com/zhayujie/chatgpt-on-wechat/issues/142)„ÄÇ

### 2.ÊúçÂä°Âô®ÈÉ®ÁΩ≤

‰ΩøÁî®nohupÂëΩ‰ª§Âú®ÂêéÂè∞ËøêË°åÁ®ãÂ∫èÔºö

```bash
nohup python3 app.py &amp; tail -f nohup.out          # Âú®ÂêéÂè∞ËøêË°åÁ®ãÂ∫èÂπ∂ÈÄöËøáÊó•ÂøóËæìÂá∫‰∫åÁª¥Á†Å
```
Êâ´Á†ÅÁôªÂΩïÂêéÁ®ãÂ∫èÂç≥ÂèØËøêË°å‰∫éÊúçÂä°Âô®ÂêéÂè∞ÔºåÊ≠§Êó∂ÂèØÈÄöËøá `ctrl+c` ÂÖ≥Èó≠Êó•ÂøóÔºå‰∏ç‰ºöÂΩ±ÂìçÂêéÂè∞Á®ãÂ∫èÁöÑËøêË°å„ÄÇ‰ΩøÁî® `ps -ef | grep app.py | grep -v grep` ÂëΩ‰ª§ÂèØÊü•ÁúãËøêË°å‰∫éÂêéÂè∞ÁöÑËøõÁ®ãÔºåÂ¶ÇÊûúÊÉ≥Ë¶ÅÈáçÊñ∞ÂêØÂä®Á®ãÂ∫èÂèØ‰ª•ÂÖà `kill` ÊéâÂØπÂ∫îÁöÑËøõÁ®ã„ÄÇÊó•ÂøóÂÖ≥Èó≠ÂêéÂ¶ÇÊûúÊÉ≥Ë¶ÅÂÜçÊ¨°ÊâìÂºÄÂè™ÈúÄËæìÂÖ•¬†`tail -f nohup.out`„ÄÇÊ≠§Â§ñÔºå`scripts` ÁõÆÂΩï‰∏ãÊúâ‰∏ÄÈîÆËøêË°å„ÄÅÂÖ≥Èó≠Á®ãÂ∫èÁöÑËÑöÊú¨‰æõ‰ΩøÁî®„ÄÇ

&gt; **Â§öË¥¶Âè∑ÊîØÊåÅÔºö** Â∞ÜÈ°πÁõÆÂ§çÂà∂Â§ö‰ªΩÔºåÂàÜÂà´ÂêØÂä®Á®ãÂ∫èÔºåÁî®‰∏çÂêåË¥¶Âè∑Êâ´Á†ÅÁôªÂΩïÂç≥ÂèØÂÆûÁé∞ÂêåÊó∂ËøêË°å„ÄÇ

&gt; **ÁâπÊÆäÊåá‰ª§Ôºö** Áî®Êà∑ÂêëÊú∫Âô®‰∫∫ÂèëÈÄÅ **#reset** Âç≥ÂèØÊ∏ÖÁ©∫ËØ•Áî®Êà∑ÁöÑ‰∏ä‰∏ãÊñáËÆ∞ÂøÜ„ÄÇ


### 3.DockerÈÉ®ÁΩ≤

&gt; ‰ΩøÁî®dockerÈÉ®ÁΩ≤Êó†ÈúÄ‰∏ãËΩΩÊ∫êÁ†ÅÂíåÂÆâË£Ö‰æùËµñÔºåÂè™ÈúÄË¶ÅËé∑Âèñ docker-compose.yml ÈÖçÁΩÆÊñá‰ª∂Âπ∂ÂêØÂä®ÂÆπÂô®Âç≥ÂèØ„ÄÇ

&gt; ÂâçÊèêÊòØÈúÄË¶ÅÂÆâË£ÖÂ•Ω `docker` Âèä `docker-compose`ÔºåÂÆâË£ÖÊàêÂäüÁöÑË°®Áé∞ÊòØÊâßË°å `docker -v` Âíå `docker-compose version` (Êàñ docker compose version) ÂèØ‰ª•Êü•ÁúãÂà∞ÁâàÊú¨Âè∑ÔºåÂèØÂâçÂæÄ [dockerÂÆòÁΩë](https://docs.docker.com/engine/install/) ËøõË°å‰∏ãËΩΩ„ÄÇ

**(1) ‰∏ãËΩΩ docker-compose.yml Êñá‰ª∂**

```bash
wget https://open-1317903499.cos.ap-guangzhou.myqcloud.com/docker-compose.yml
```

‰∏ãËΩΩÂÆåÊàêÂêéÊâìÂºÄ `docker-compose.yml` ‰øÆÊîπÊâÄÈúÄÈÖçÁΩÆÔºåÂ¶Ç `OPEN_AI_API_KEY` Âíå `GROUP_NAME_WHITE_LIST` Á≠â„ÄÇ

**(2) ÂêØÂä®ÂÆπÂô®**

Âú® `docker-compose.yml` ÊâÄÂú®ÁõÆÂΩï‰∏ãÊâßË°å‰ª•‰∏ãÂëΩ‰ª§ÂêØÂä®ÂÆπÂô®Ôºö

```bash
sudo docker compose up -d
```

ËøêË°å `sudo docker ps` ËÉΩÊü•ÁúãÂà∞ NAMES ‰∏∫ chatgpt-on-wechat ÁöÑÂÆπÂô®Âç≥Ë°®Á§∫ËøêË°åÊàêÂäü„ÄÇ

Ê≥®ÊÑèÔºö

 - Â¶ÇÊûú `docker-compose` ÊòØ 1.X ÁâàÊú¨ ÂàôÈúÄË¶ÅÊâßË°å `sudo  docker-compose up -d` Êù•ÂêØÂä®ÂÆπÂô®
 - ËØ•ÂëΩ‰ª§‰ºöËá™Âä®Âéª [docker hub](https://hub.docker.com/r/zhayujie/chatgpt-on-wechat) ÊãâÂèñ latest ÁâàÊú¨ÁöÑÈïúÂÉèÔºålatest ÈïúÂÉè‰ºöÂú®ÊØèÊ¨°È°πÁõÆ release Êñ∞ÁöÑÁâàÊú¨Êó∂ÁîüÊàê

ÊúÄÂêéËøêË°å‰ª•‰∏ãÂëΩ‰ª§ÂèØÊü•ÁúãÂÆπÂô®ËøêË°åÊó•ÂøóÔºåÊâ´ÊèèÊó•Âøó‰∏≠ÁöÑ‰∫åÁª¥Á†ÅÂç≥ÂèØÂÆåÊàêÁôªÂΩïÔºö

```bash
sudo docker logs -f chatgpt-on-wechat
```

**(3) Êèí‰ª∂‰ΩøÁî®**

Â¶ÇÊûúÈúÄË¶ÅÂú®dockerÂÆπÂô®‰∏≠‰øÆÊîπÊèí‰ª∂ÈÖçÁΩÆÔºåÂèØÈÄöËøáÊåÇËΩΩÁöÑÊñπÂºèÂÆåÊàêÔºåÂ∞Ü [Êèí‰ª∂ÈÖçÁΩÆÊñá‰ª∂](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/plugins/config.json.template)
ÈáçÂëΩÂêç‰∏∫ `config.json`ÔºåÊîæÁΩÆ‰∫é `docker-compose.yml` Áõ∏ÂêåÁõÆÂΩï‰∏ãÔºåÂπ∂Âú® `docker-compose.yml` ‰∏≠ÁöÑ `chatgpt-on-wechat` ÈÉ®ÂàÜ‰∏ãÊ∑ªÂä† `volumes` Êò†Â∞Ñ:

```
volumes:
  - ./config.json:/app/plugins/config.json
```
**Ê≥®**ÔºöÈááÁî®dockerÊñπÂºèÈÉ®ÁΩ≤ÁöÑËØ¶ÁªÜÊïôÁ®ãÂèØ‰ª•ÂèÇËÄÉÔºö[dockerÈÉ®ÁΩ≤CoWÈ°πÁõÆ](https://www.wangpc.cc/ai/docker-deploy-cow/)
### 4. RailwayÈÉ®ÁΩ≤

&gt; Railway ÊØèÊúàÊèê‰æõ5ÂàÄÂíåÊúÄÂ§ö500Â∞èÊó∂ÁöÑÂÖçË¥πÈ¢ùÂ∫¶„ÄÇ (07.11Êõ¥Êñ∞: ÁõÆÂâçÂ§ßÈÉ®ÂàÜË¥¶Âè∑Â∑≤Êó†Ê≥ïÂÖçË¥πÈÉ®ÁΩ≤)

1. ËøõÂÖ• [Railway](https://railway.app/template/qApznZ?referralCode=RC3znh)
2. ÁÇπÂáª `Deploy Now` ÊåâÈíÆ„ÄÇ
3. ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáèÊù•ÈáçËΩΩÁ®ãÂ∫èËøêË°åÁöÑÂèÇÊï∞Ôºå‰æãÂ¶Ç`open_ai_api_key`, `character_desc`„ÄÇ

**‰∏ÄÈîÆÈÉ®ÁΩ≤:**
  
  [![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template/qApznZ?referralCode=RC3znh)

&lt;br&gt;

# üîé Â∏∏ËßÅÈóÆÈ¢ò

FAQsÔºö &lt;https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs&gt;

ÊàñÁõ¥Êé•Âú®Á∫øÂí®ËØ¢ [È°πÁõÆÂ∞èÂä©Êâã](https://link-ai.tech/app/Kv2fXJcH)  (ËØ≠ÊñôÊåÅÁª≠ÂÆåÂñÑ‰∏≠ÔºåÂõûÂ§ç‰ªÖ‰æõÂèÇËÄÉ)

# üõ†Ô∏è ÂºÄÂèë

Ê¨¢ËøéÊé•ÂÖ•Êõ¥Â§öÂ∫îÁî®ÔºåÂèÇËÄÉ [Terminal‰ª£Á†Å](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/channel/terminal/terminal_channel.py) ÂÆûÁé∞Êé•Êî∂ÂíåÂèëÈÄÅÊ∂àÊÅØÈÄªËæëÂç≥ÂèØÊé•ÂÖ•„ÄÇ ÂêåÊó∂Ê¨¢ËøéÂ¢ûÂä†Êñ∞ÁöÑÊèí‰ª∂ÔºåÂèÇËÄÉ [Êèí‰ª∂ËØ¥ÊòéÊñáÊ°£](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins)„ÄÇ

# ‚úâ ËÅîÁ≥ª

Ê¨¢ËøéÊèê‰∫§PR„ÄÅIssuesÔºå‰ª•ÂèäStarÊîØÊåÅ‰∏Ä‰∏ã„ÄÇÁ®ãÂ∫èËøêË°åÈÅáÂà∞ÈóÆÈ¢òÂèØ‰ª•Êü•Áúã [Â∏∏ËßÅÈóÆÈ¢òÂàóË°®](https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs) ÔºåÂÖ∂Ê¨°ÂâçÂæÄ [Issues](https://github.com/zhayujie/chatgpt-on-wechat/issues) ‰∏≠ÊêúÁ¥¢„ÄÇ‰∏™‰∫∫ÂºÄÂèëËÄÖÂèØÂä†ÂÖ•ÂºÄÊ∫ê‰∫§ÊµÅÁæ§ÂèÇ‰∏éÊõ¥Â§öËÆ®ËÆ∫Ôºå‰ºÅ‰∏öÁî®Êà∑ÂèØËÅîÁ≥ª[‰∫ßÂìÅÈ°æÈóÆ](https://img-1317903499.cos.ap-guangzhou.myqcloud.com/docs/product-manager-qrcode.jpg)Âí®ËØ¢„ÄÇ

# üåü Ë¥°ÁåÆËÄÖ

![cow contributors](https://contrib.rocks/image?repo=zhayujie/chatgpt-on-wechat&amp;max=1000)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/RD-Agent]]></title>
            <link>https://github.com/microsoft/RD-Agent</link>
            <guid>https://github.com/microsoft/RD-Agent</guid>
            <pubDate>Thu, 29 May 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[Research and development (R&D) is crucial for the enhancement of industrial productivity, especially in the AI era, where the core aspects of R&D are mainly focused on data and models. We are committed to automating these high-value generic R&D processes through R&D-Agent, which lets AI drive data-driven AI. üîóhttps://aka.ms/RD-Agent-Tech-Report]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/RD-Agent">microsoft/RD-Agent</a></h1>
            <p>Research and development (R&D) is crucial for the enhancement of industrial productivity, especially in the AI era, where the core aspects of R&D are mainly focused on data and models. We are committed to automating these high-value generic R&D processes through R&D-Agent, which lets AI drive data-driven AI. üîóhttps://aka.ms/RD-Agent-Tech-Report</p>
            <p>Language: Python</p>
            <p>Stars: 5,158</p>
            <p>Forks: 460</p>
            <p>Stars today: 234 stars today</p>
            <h2>README</h2><pre>&lt;h4 align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/_static/logo.png&quot; alt=&quot;RA-Agent logo&quot; style=&quot;width:70%; &quot;&gt;
  
  &lt;a href=&quot;https://rdagent.azurewebsites.net&quot; target=&quot;_blank&quot;&gt;üñ•Ô∏è Live Demo&lt;/a&gt; |
  &lt;a href=&quot;https://rdagent.azurewebsites.net/factor_loop&quot; target=&quot;_blank&quot;&gt;üé• Demo Video&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=JJ4JYO3HscM&amp;list=PLALmKB0_N3_i52fhUmPQiL4jsO354uopR&quot; target=&quot;_blank&quot;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;   |
  &lt;a href=&quot;https://rdagent.readthedocs.io/en/latest/index.html&quot; target=&quot;_blank&quot;&gt;üìñ Documentation&lt;/a&gt; |
  &lt;a href=&quot;https://aka.ms/RD-Agent-Tech-Report&quot; target=&quot;_blank&quot;&gt;üìÑ Tech Report&lt;/a&gt; |
  &lt;a href=&quot;#-paperwork-list&quot;&gt; üìÉ Papers &lt;/a&gt;
&lt;/h3&gt;


[![CI](https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml)
[![CodeQL](https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql)
[![Dependabot Updates](https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates)
[![Lint PR Title](https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml)
[![Release.yml](https://github.com/microsoft/RD-Agent/actions/workflows/release.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/release.yml)
[![Platform](https://img.shields.io/badge/platform-Linux-blue)](https://pypi.org/project/rdagent/#files)
[![PyPI](https://img.shields.io/pypi/v/rdagent)](https://pypi.org/project/rdagent/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/rdagent)](https://pypi.org/project/rdagent/)
[![Release](https://img.shields.io/github/v/release/microsoft/RD-Agent)](https://github.com/microsoft/RD-Agent/releases)
[![GitHub](https://img.shields.io/github/license/microsoft/RD-Agent)](https://github.com/microsoft/RD-Agent/blob/main/LICENSE)
[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit)](https://github.com/pre-commit/pre-commit)
[![Checked with mypy](https://www.mypy-lang.org/static/mypy_badge.svg)](http://mypy-lang.org/)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
[![Chat](https://img.shields.io/badge/chat-discord-blue)](https://discord.gg/ybQ97B6Jjy)
[![Documentation Status](https://readthedocs.org/projects/rdagent/badge/?version=latest)](https://rdagent.readthedocs.io/en/latest/?badge=latest)
[![Readthedocs Preview](https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml) &lt;!-- this badge is too long, please place it in the last one to make it pretty --&gt; 
[![arXiv](https://img.shields.io/badge/arXiv-2505.14738-00ff00.svg)](https://arxiv.org/abs/2505.14738)



# üèÜ The Best Machine Learning Engineering Agent!

[MLE-bench](https://github.com/openai/mle-bench) is a comprehensive benchmark evaluating the performance of AI agents on machine learning engineering tasks. Utilizing datasets from 75 Kaggle competitions, MLE-bench provides robust assessments of AI systems&#039; capabilities in real-world ML engineering scenarios.

R&amp;D-Agent currently leads as the top-performing machine learning engineering agent on MLE-bench:

| Agent | Low == Lite (%) | Medium (%) | High (%) | All (%) |
|---------|--------|-----------|---------|----------|
| R&amp;D-Agent o1-preview | 48.18 ¬± 2.49 | 8.95 ¬± 2.36 | 18.67 ¬± 2.98 | 22.4 ¬± 1.1 |
| R&amp;D-Agent o3(R)+GPT-4.1(D) | 51.52 ¬± 6.21 | 7.89 ¬± 3.33 | 16.67 ¬± 3.65 | 22.45 ¬± 2.45 |
| AIDE o1-preview | 34.3 ¬± 2.4 | 8.8 ¬± 1.1 | 10.0 ¬± 1.9 | 16.9 ¬± 1.1 |

**Notes:**
- **O3(R)+GPT-4.1(D)**: This version is designed to both reduce average time per loop and leverage a cost-effective combination of backend LLMs by seamlessly integrating Research Agent (o3) with Development Agent (GPT-4.1).
- **AIDE o1-preview**: Represents the previously best public result on MLE-bench as reported in the original MLE-bench paper.
- Average and standard deviation results for R&amp;D-Agent o1-preview is based on a independent of 5 seeds and for R&amp;D-Agent o3(R)+GPT-4.1(D) is based on 6 seeds.
- According to MLE-Bench, the 75 competitions are categorized into three levels of complexity: **Low==Lite** if we estimate that an experienced ML engineer can produce a sensible solution in under 2 hours, excluding the time taken to train any models; **Medium** if it takes between 2 and 10 hours; and **High** if it takes more than 10 hours.

You can inspect the detailed runs of the above results online.
- [R&amp;D-Agent o1-preview detailed runs](https://aka.ms/RD-Agent_MLE-Bench_O1-preview)
- [R&amp;D-Agent o3(R)+GPT-4.1(D) detailed runs](https://aka.ms/RD-Agent_MLE-Bench_O3_GPT41)

For running R&amp;D-Agent on MLE-bench, refer to **[MLE-bench Guide: Running ML Engineering via MLE-bench](https://rdagent.readthedocs.io/en/latest/scens/data_science.html)**


# üì∞ News
| üóûÔ∏è News        | üìù Description                 |
| --            | ------      |
| [Technical Report Release](#overall-technical-report) | Overall framework description and results on MLE-bench | 
| [R&amp;D-Agent-Quant Release](#deep-application-in-diverse-scenarios) | Apply R&amp;D-Agent to quant trading | 
| MLE-Bench Results Released | R&amp;D-Agent currently leads as the [top-performing machine learning engineering agent](#-the-best-machine-learning-engineering-agent) on MLE-bench |
| Support LiteLLM Backend | We now fully support **[LiteLLM](https://github.com/BerriAI/litellm)** as a backend for integration with multiple LLM providers. |
| More General Data Science Agent | üöÄComing soon! |
| Kaggle Scenario release | We release **[Kaggle Agent](https://rdagent.readthedocs.io/en/latest/scens/kaggle_agent.html)**, try the new features!                  |
| Official WeChat group release  | We created a WeChat group, welcome to join! (üó™[QR Code](https://github.com/microsoft/RD-Agent/issues/880)) |
| Official Discord release  | We launch our first chatting channel in Discord (üó™[![Chat](https://img.shields.io/badge/chat-discord-blue)](https://discord.gg/ybQ97B6Jjy)) |
| First release | **R&amp;D-Agent** is released on GitHub |



# Data Science Agent Preview
Check out our demo video showcasing the current progress of our Data Science Agent under development:

https://github.com/user-attachments/assets/3eccbecb-34a4-4c81-bce4-d3f8862f7305

# üåü Introduction
&lt;div align=&quot;center&quot;&gt;
      &lt;img src=&quot;docs/_static/scen.png&quot; alt=&quot;Our focused scenario&quot; style=&quot;width:80%; &quot;&gt;
&lt;/div&gt;

R&amp;D-Agent aims to automate the most critical and valuable aspects of the industrial R&amp;D process, and we begin with focusing on the data-driven scenarios to streamline the development of models and data. 
Methodologically, we have identified a framework with two key components: &#039;R&#039; for proposing new ideas and &#039;D&#039; for implementing them.
We believe that the automatic evolution of R&amp;D will lead to solutions of significant industrial value.


&lt;!-- Tag Cloud --&gt;
R&amp;D is a very general scenario. The advent of R&amp;D-Agent can be your
- üí∞ **Automatic Quant Factory** ([üé•Demo Video](https://rdagent.azurewebsites.net/factor_loop)|[‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=X4DK2QZKaKY&amp;t=6s))
- ü§ñ **Data Mining Agent:** Iteratively proposing data &amp; models ([üé•Demo Video 1](https://rdagent.azurewebsites.net/model_loop)|[‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=dm0dWL49Bc0&amp;t=104s)) ([üé•Demo Video 2](https://rdagent.azurewebsites.net/dmm)|[‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=VIaSTZuoZg4))  and implementing them by gaining knowledge from data.
- ü¶æ **Research Copilot:** Auto read research papers ([üé•Demo Video](https://rdagent.azurewebsites.net/report_model)|[‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=BiA2SfdKQ7o)) / financial reports ([üé•Demo Video](https://rdagent.azurewebsites.net/report_factor)|[‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=ECLTXVcSx-c)) and implement model structures or building datasets.
- ü§ñ **Kaggle Agent:** Auto Model Tuning and Feature Engineering([üé•Demo Video Coming Soon...]()) and implementing them to achieve more in competitions.
- ...

You can click the links above to view the demo. We&#039;re continuously adding more methods and scenarios to the project to enhance your R&amp;D processes and boost productivity. 

Additionally, you can take a closer look at the examples in our **[üñ•Ô∏è Live Demo](https://rdagent.azurewebsites.net/)**.

&lt;div align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://rdagent.azurewebsites.net/&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;docs/_static/demo.png&quot; alt=&quot;Watch the demo&quot; width=&quot;80%&quot;&gt;
    &lt;/a&gt;
&lt;/div&gt;


# ‚ö° Quick start

You can try above demos by running the following command:

### üê≥ Docker installation.
Users must ensure Docker is installed before attempting most scenarios. Please refer to the [official üê≥Docker page](https://docs.docker.com/engine/install/) for installation instructions.
Ensure the current user can run Docker commands **without using sudo**. You can verify this by executing `docker run hello-world`.

### üêç Create a Conda Environment
- Create a new conda environment with Python (3.10 and 3.11 are well-tested in our CI):
  ```sh
  conda create -n rdagent python=3.10
  ```
- Activate the environment:
  ```sh
  conda activate rdagent
  ```

### üõ†Ô∏è Install the R&amp;D-Agent
- You can directly install the R&amp;D-Agent package from PyPI:
  ```sh
  pip install rdagent
  ```

### üíä Health check
- rdagent provides a health check that currently checks two things.
  - whether the docker installation was successful.
  - whether the default port used by the [rdagent ui](https://github.com/microsoft/RD-Agent?tab=readme-ov-file#%EF%B8%8F-monitor-the-application-results) is occupied.
  ```sh
  rdagent health_check
  ```


### ‚öôÔ∏è Configuration
- The demos requires following ability:
  - ChatCompletion
  - json_mode
  - embedding query

- For example: If you are using the `OpenAI API`, you have to configure your GPT model in the `.env` file like this.
  ```bash
  cat &lt;&lt; EOF  &gt; .env
  OPENAI_API_KEY=&lt;replace_with_your_openai_api_key&gt;
  # EMBEDDING_MODEL=text-embedding-3-small
  CHAT_MODEL=gpt-4-turbo
  EOF
  ```
- However, not every API services support these features by default. For example: `AZURE OpenAI`, you have to configure your GPT model in the `.env` file like this.
  ```bash
  cat &lt;&lt; EOF  &gt; .env
  USE_AZURE=True
  EMBEDDING_OPENAI_API_KEY=&lt;replace_with_your_azure_openai_api_key&gt;
  EMBEDDING_AZURE_API_BASE=&lt;replace_with_your_azure_endpoint&gt;
  EMBEDDING_AZURE_API_VERSION=&lt;replace_with_the_version_of_your_azure_openai_api&gt;
  EMBEDDING_MODEL=text-embedding-3-small
  CHAT_OPENAI_API_KEY=&lt;replace_with_your_azure_openai_api_key&gt;
  CHAT_AZURE_API_BASE=&lt;replace_with_your_azure_endpoint&gt;
  CHAT_AZURE_API_VERSION=&lt;replace_with_the_version_of_your_azure_openai_api&gt;
  CHAT_MODEL=&lt;replace_it_with_the_name_of_your_azure_chat_model&gt;
  EOF
  ```

- We now support LiteLLM as a backend for integration with multiple LLM providers. If you use LiteLLM Backend to use models, you can configure as follows:
  ```bash
  cat &lt;&lt; EOF  &gt; .env
  BACKEND=rdagent.oai.backend.LiteLLMAPIBackend
  # It can be modified to any model supported by LiteLLM.
  CHAT_MODEL=gpt-4o
  EMBEDDING_MODEL=text-embedding-3-small
  # The backend api_key fully follow the convention of litellm.
  OPENAI_API_KEY=&lt;replace_with_your_openai_api_key&gt;
  ```
  
- For more configuration information, please refer to the [documentation](https://rdagent.readthedocs.io/en/latest/installation_and_configuration.html).

### üöÄ Run the Application

The **[üñ•Ô∏è Live Demo](https://rdagent.azurewebsites.net/)** is implemented by the following commands(each item represents one demo, you can select the one you prefer):

- Run the **Automated Quantitative Trading &amp; Iterative Factors Evolution**:  [Qlib](http://github.com/microsoft/qlib) self-loop factor proposal and implementation application
  ```sh
  rdagent fin_factor
  ```

- Run the **Automated Quantitative Trading &amp; Iterative Model Evolution**: [Qlib](http://github.com/microsoft/qlib) self-loop model proposal and implementation application
  ```sh
  rdagent fin_model
  ```

- Run the **Automated Medical Prediction Model Evolution**: Medical self-loop model proposal and implementation application
  &gt;(1) Apply for an account at [PhysioNet](https://physionet.org/). &lt;br /&gt; (2) Request access to FIDDLE preprocessed data: [FIDDLE Dataset](https://physionet.org/content/mimic-eicu-fiddle-feature/1.0.0/). &lt;br /&gt;
  (3) Place your username and password in `.env`.
  ```bash
  cat &lt;&lt; EOF  &gt;&gt; .env
  DM_USERNAME=&lt;your_username&gt;
  DM_PASSWORD=&lt;your_password&gt;
  EOF
  ```
  ```sh
  rdagent med_model
  ```

- Run the **Automated Quantitative Trading &amp; Factors Extraction from Financial Reports**:  Run the [Qlib](http://github.com/microsoft/qlib) factor extraction and implementation application based on financial reports
  ```sh
  # 1. Generally, you can run this scenario using the following command:
  rdagent fin_factor_report --report_folder=&lt;Your financial reports folder path&gt;

  # 2. Specifically, you need to prepare some financial reports first. You can follow this concrete example:
  wget https://github.com/SunsetWolf/rdagent_resource/releases/download/reports/all_reports.zip
  unzip all_reports.zip -d git_ignore_folder/reports
  rdagent fin_factor_report --report_folder=git_ignore_folder/reports
  ```

- Run the **Automated Model Research &amp; Development Copilot**: model extraction and implementation application
  ```sh
  # 1. Generally, you can run your own papers/reports with the following command:
  rdagent general_model &lt;Your paper URL&gt;

  # 2. Specifically, you can do it like this. For more details and additional paper examples, use `rdagent general_model -h`:
  rdagent general_model  &quot;https://arxiv.org/pdf/2210.09789&quot;
  ```

- Run the **Automated Kaggle Model Tuning &amp; Feature Engineering**:  self-loop model proposal and feature engineering implementation application &lt;br /&gt;
  &gt; Using **sf-crime** *(San Francisco Crime Classification)* as an example. &lt;br /&gt;
  &gt; 1. Register and login on the [Kaggle](https://www.kaggle.com/) website. &lt;br /&gt;
  &gt; 2. Configuring the Kaggle API. &lt;br /&gt;
  &gt; (1) Click on the avatar (usually in the top right corner of the page) -&gt; `Settings` -&gt; `Create New Token`, A file called `kaggle.json` will be downloaded. &lt;br /&gt;
  &gt; (2) Move `kaggle.json` to `~/.config/kaggle/` &lt;br /&gt;
  &gt; (3) Modify the permissions of the kaggle.json file. Reference command: `chmod 600 ~/.config/kaggle/kaggle.json` &lt;br /&gt;
  &gt; 3. Join the competition: Click `Join the competition` -&gt; `I Understand and Accept` at the bottom of the [competition details page](https://www.kaggle.com/competitions/sf-crime/data).
  ```bash
  # Generally, you can run the Kaggle competition program with the following command:
  rdagent kaggle --competition &lt;your competition name&gt;

  # Specifically, you need to create a folder for storing competition files (e.g., competition description file, competition datasets, etc.), and configure the path to the folder in your environment. In addition, you need to use chromedriver when you download the competition descriptors, which you can follow for this specific example:
  
  # 1. Install chromedriver.

  # 2. Add the competition description file path to the `.env` file.
  mkdir -p ./git_ignore_folder/kaggle_data
  dotenv set KG_LOCAL_DATA_PATH &quot;$(pwd)/git_ignore_folder/kaggle_data&quot;

  # 3. run the application
  rdagent kaggle --competition sf-crime
  ```
  &gt; **Description of the above example:** &lt;br /&gt;
  &gt; - Kaggle competition data is roughly divided into three sections: competition description file (json file) and complete dataset for the competition and simplified dataset for the competition. &lt;br /&gt;
  &gt; - The Kaggle competition data will be downloaded automatically, the download process depends on `chromedriver`, installation instructions can be found in the [documentation](https://rdagent.readthedocs.io/en/latest/scens/kaggle_agent.html#example-guide). &lt;br /&gt;

### üñ•Ô∏è Monitor the Application Results
- You can run the following command for our demo program to see the run logs.

  ```sh
  rdagent ui --port 19899 --log_dir &lt;your log folder like &quot;log/&quot;&gt;
  ```

  **Note:** Although port 19899 is not commonly used, but before you run this demo, you need to check if port 19899 is occupied. If it is, please change it to another port that is not occupied.

  You can check if a port is occupied by running the following command.

  ```sh
  rdagent health_check
  ```

# üè≠ Scenarios

We have applied R&amp;D-Agent to multiple valuable data-driven industrial scenarios.


## üéØ Goal: Agent for Data-driven R&amp;D

In this project, we are aiming to build an Agent to automate Data-Driven R\&amp;D that can
+ üìÑ Read real-world material (reports, papers, etc.) and **extract** key formulas, descriptions of interested **features** and **models**, which are the key components of data-driven R&amp;D .
+ üõ†Ô∏è **Implement** the extracted formulas (e.g., features, factors, and models) in runnable codes.
   + Due to the limited ability of LLM in implementing at once, build an evolving process for the agent to improve performance by learning from feedback and knowledge.
+ üí° Propose **new ideas** based on current knowledge and observations.

&lt;!-- ![Data-Centric R&amp;D Overview](docs/_static/overview.png) --&gt;

## üìà Scenarios/Demos

In the two key areas of data-driven scenarios, model implementation and data building, our system aims to serve two main roles: ü¶æCopilot and ü§ñAgent. 
- The ü¶æCopilot follows human instructions to automate repetitive tasks. 
- The ü§ñAgent, being more autonomous, actively proposes ideas for better results in the future.

The supported scenarios are listed below:

| Scenario/Target | Model Implementation                   | Data Building                                                                      |
| --              | --                                     | --                                                                                 |
| **üíπ Finance**      | ü§ñ [Iteratively Proposing Ideas &amp; Evolving](https://rdagent.azurewebsites.net/model_loop)[‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=dm0dWL49Bc0&amp;t=104s) |  ü§ñ [Iteratively Proposing Ideas &amp; Evolving](https://rdagent.azurewebsites.net/factor_loop) [‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=X4DK2QZKaKY&amp;t=6s) &lt;br/&gt;   ü¶æ [Auto reports reading &amp; implementation](https://rdagent.azurewebsites.net/report_factor)[‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=ECLTXVcSx-c)  |
| **ü©∫ Medical**      | ü§ñ [Iteratively Proposing Ideas &amp; Evolving](https://rdagent.azurewebsites.net/dmm)[‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=VIaSTZuoZg4) | -                                                                                  |
| **üè≠ General**      | ü¶æ [Auto paper reading &amp; implementation](https://rdagent.azurewebsites.net/report_model)[‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=BiA2SfdKQ7o) &lt;br/&gt; ü§ñ Auto Kaggle Model Tuning   | ü§ñAuto Kaggle feature Engineering |

- **[RoadMap](https://rdagent.readthedocs.io/en/latest/scens/kaggle_agent.html#roadmap)**: Currently, we are working hard to add new features to the Kaggle scenario.

Different scenarios vary in entrance and configuration. Please check the detailed setup tutorial in the scenarios documents.

Here is a gallery of [successful explorations](https://github.com/SunsetWolf/rdagent_resource/releases/download/demo_traces/demo_traces.zip) (5 traces showed in **[üñ•Ô∏è Live Demo](https://rdagent.azurewebsites.net/)**). You can download and view the execution trace using [this command](https://github.com/microsoft/RD-Agent?tab=readme-ov-file#%EF%B8%8F-monitor-the-application-results) from the documentation.

Please refer to **[üìñreadthedocs_scen](https://rdagent.readthedocs.io/en/latest/scens/catalog.htm

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ccxt/ccxt]]></title>
            <link>https://github.com/ccxt/ccxt</link>
            <guid>https://github.com/ccxt/ccxt</guid>
            <pubDate>Thu, 29 May 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[A JavaScript / TypeScript / Python / C# / PHP / Go cryptocurrency trading API with support for more than 100 bitcoin/altcoin exchanges]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ccxt/ccxt">ccxt/ccxt</a></h1>
            <p>A JavaScript / TypeScript / Python / C# / PHP / Go cryptocurrency trading API with support for more than 100 bitcoin/altcoin exchanges</p>
            <p>Language: Python</p>
            <p>Stars: 36,406</p>
            <p>Forks: 7,919</p>
            <p>Stars today: 46 stars today</p>
            <h2>README</h2><pre># CCXT ‚Äì CryptoCurrency eXchange Trading Library

[![NPM Downloads](https://img.shields.io/npm/dy/ccxt.svg)](https://www.npmjs.com/package/ccxt) [![npm](https://img.shields.io/npm/v/ccxt.svg)](https://npmjs.com/package/ccxt) [![PyPI](https://img.shields.io/pypi/v/ccxt.svg)](https://pypi.python.org/pypi/ccxt) [![NuGet version](https://img.shields.io/nuget/v/ccxt)](https://www.nuget.org/packages/ccxt) [![GoDoc](https://pkg.go.dev/badge/github.com/ccxt/ccxt/go/v4?utm_source=godoc)](https://godoc.org/github.com/ccxt/ccxt/go/v4) [![Discord](https://img.shields.io/discord/690203284119617602?logo=discord&amp;logoColor=white)](https://discord.gg/ccxt) [![Supported Exchanges](https://img.shields.io/badge/exchanges-104-blue.svg)](https://github.com/ccxt/ccxt/wiki/Exchange-Markets) [![Follow CCXT at x.com](https://img.shields.io/twitter/follow/ccxt_official.svg?style=social&amp;label=CCXT)](https://x.com/ccxt_official)

A `JavaScript` / `Python` / `PHP` / `C#` / `Go` library for cryptocurrency trading and e-commerce with support for many bitcoin/ether/altcoin exchange markets and merchant APIs.

### [Install](#install) ¬∑ [Usage](#usage) ¬∑ [Manual](https://github.com/ccxt/ccxt/wiki) ¬∑ [FAQ](https://github.com/ccxt/ccxt/wiki/FAQ) ¬∑ [Examples](https://github.com/ccxt/ccxt/tree/master/examples) ¬∑ [Contributing](https://github.com/ccxt/ccxt/blob/master/CONTRIBUTING.md) ¬∑ [Social](#social)

The **CCXT** library is used to connect and trade with cryptocurrency exchanges and payment processing services worldwide. It provides quick access to market data for storage, analysis, visualization, indicator development, algorithmic trading, strategy backtesting, bot programming, and related software engineering.

It is intended to be used by **coders, developers, technically-skilled traders, data-scientists and financial analysts** for building trading algorithms.

Current feature list:

- support for many cryptocurrency exchanges ‚Äî more coming soon
- fully implemented public and private APIs
- optional normalized data for cross-exchange analytics and arbitrage
- an out of the box unified API that is extremely easy to integrate
- works in Node 10.4+, Python 3, PHP 8.1+, netstandard2.0/2.1, Go 1.20+ and web browsers


## See Also

- &lt;sub&gt;[![TabTrader](https://user-images.githubusercontent.com/1294454/66755907-9c3e8880-eea1-11e9-846e-0bff349ceb87.png)](https://tab-trader.com/?utm_source=ccxt)&lt;/sub&gt; **[TabTrader](https://tab-trader.com/?utm_source=ccxt)** ‚Äì trading on all exchanges in one app. Available on **[Android](https://play.google.com/store/apps/details?id=com.tabtrader.android&amp;referrer=utm_source%3Dccxt)** and **[iOS](https://itunes.apple.com/app/apple-store/id1095716562?mt=8)**!
- &lt;sub&gt;[![Freqtrade](https://user-images.githubusercontent.com/1294454/114340585-8e35fa80-9b60-11eb-860f-4379125e2db6.png)](https://www.freqtrade.io)&lt;/sub&gt; **[Freqtrade](https://www.freqtrade.io)** ‚Äì leading opensource cryptocurrency algorithmic trading software!
- &lt;sub&gt;[![OctoBot](https://user-images.githubusercontent.com/1294454/132113722-007fc092-7530-4b41-b929-b8ed380b7b2e.png)](https://www.octobot.online)&lt;/sub&gt; **[OctoBot](https://www.octobot.online)** ‚Äì cryptocurrency trading bot with an advanced web interface.
- &lt;sub&gt;[![TokenBot](https://user-images.githubusercontent.com/1294454/152720975-0522b803-70f0-4f18-a305-3c99b37cd990.png)](https://tokenbot.com/?utm_source=github&amp;utm_medium=ccxt&amp;utm_campaign=algodevs)&lt;/sub&gt; **[TokenBot](https://tokenbot.com/?utm_source=github&amp;utm_medium=ccxt&amp;utm_campaign=algodevs)** ‚Äì discover and copy the best algorithmic traders in the world.

## Certified Cryptocurrency Exchanges


|logo                                                                                                                                                                         |id             |name                                                                                     |ver                                                                                                                               |type                                                                                                    |certified                                                                                                                    |pro                                                                           |discount                                                                                                                                                                                                          |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|-----------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------:|--------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------:|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [![binance](https://github.com/user-attachments/assets/e9419b93-ccb0-46aa-9bff-c883f096274b)](https://accounts.binance.com/en/register?ref=D7YA7CLY)                        | binance       | [Binance](https://accounts.binance.com/en/register?ref=D7YA7CLY)                        | [![API Version *](https://img.shields.io/badge/*-lightgray)](https://developers.binance.com/en)                                  | ![CEX ‚Äì Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX ‚Äì Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) | [![Sign up with Binance using CCXT&#039;s referral link for a 10% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d10%25&amp;color=orange)](https://accounts.binance.com/en/register?ref=D7YA7CLY)        |
| [![binancecoinm](https://github.com/user-attachments/assets/387cfc4e-5f33-48cd-8f5c-cd4854dabf0c)](https://accounts.binance.com/en/register?ref=D7YA7CLY)                   | binancecoinm  | [Binance COIN-M](https://accounts.binance.com/en/register?ref=D7YA7CLY)                 | [![API Version *](https://img.shields.io/badge/*-lightgray)](https://binance-docs.github.io/apidocs/delivery/en/)                | ![CEX ‚Äì Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX ‚Äì Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) | [![Sign up with Binance COIN-M using CCXT&#039;s referral link for a 10% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d10%25&amp;color=orange)](https://accounts.binance.com/en/register?ref=D7YA7CLY) |
| [![binanceusdm](https://github.com/user-attachments/assets/871cbea7-eebb-4b28-b260-c1c91df0487a)](https://accounts.binance.com/en/register?ref=D7YA7CLY)                    | binanceusdm   | [Binance USD‚ìà-M](https://accounts.binance.com/en/register?ref=D7YA7CLY)                 | [![API Version *](https://img.shields.io/badge/*-lightgray)](https://binance-docs.github.io/apidocs/futures/en/)                 | ![CEX ‚Äì Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX ‚Äì Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) | [![Sign up with Binance USD‚ìà-M using CCXT&#039;s referral link for a 10% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d10%25&amp;color=orange)](https://accounts.binance.com/en/register?ref=D7YA7CLY) |
| [![bingx](https://github-production-user-asset-6210df.s3.amazonaws.com/1294454/253675376-6983b72e-4999-4549-b177-33b374c195e3.jpg)](https://bingx.com/invite/OHETOM)        | bingx         | [BingX](https://bingx.com/invite/OHETOM)                                                | [![API Version 1](https://img.shields.io/badge/1-lightgray)](https://bingx-api.github.io/docs/)                                  | ![CEX ‚Äì Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX ‚Äì Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) |                                                                                                                                                                                                                  |
| [![bitget](https://github.com/user-attachments/assets/fbaa10cc-a277-441d-a5b7-997dd9a87658)](https://www.bitget.com/expressly?languageType=0&amp;channelCode=ccxt&amp;vipCode=tg9j) | bitget        | [Bitget](https://www.bitget.com/expressly?languageType=0&amp;channelCode=ccxt&amp;vipCode=tg9j) | [![API Version 2](https://img.shields.io/badge/2-lightgray)](https://www.bitget.com/api-doc/common/intro)                        | ![CEX ‚Äì Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX ‚Äì Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) |                                                                                                                                                                                                                  |
| [![bitmart](https://github.com/user-attachments/assets/0623e9c4-f50e-48c9-82bd-65c3908c3a14)](http://www.bitmart.com/?r=rQCFLh)                                             | bitmart       | [BitMart](http://www.bitmart.com/?r=rQCFLh)                                             | [![API Version 2](https://img.shields.io/badge/2-lightgray)](https://developer-pro.bitmart.com/)                                 | ![CEX ‚Äì Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX ‚Äì Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) | [![Sign up with BitMart using CCXT&#039;s referral link for a 30% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d30%25&amp;color=orange)](http://www.bitmart.com/?r=rQCFLh)                             |
| [![bitmex](https://github.com/user-attachments/assets/c78425ab-78d5-49d6-bd14-db7734798f04)](https://www.bitmex.com/app/register/NZTR1q)                                    | bitmex        | [BitMEX](https://www.bitmex.com/app/register/NZTR1q)                                    | [![API Version 1](https://img.shields.io/badge/1-lightgray)](https://www.bitmex.com/app/apiOverview)                             | ![CEX ‚Äì Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX ‚Äì Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) | [![Sign up with BitMEX using CCXT&#039;s referral link for a 10% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d10%25&amp;color=orange)](https://www.bitmex.com/app/register/NZTR1q)                    |
| [![bybit](https://github.com/user-attachments/assets/97a5d0b3-de10-423d-90e1-6620960025ed)](https://www.bybit.com/register?affiliate_id=35953)                              | bybit         | [Bybit](https://www.bybit.com/register?affiliate_id=35953)                              | [![API Version 5](https://img.shields.io/badge/5-lightgray)](https://bybit-exchange.github.io/docs/inverse/)                     | ![CEX ‚Äì Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX ‚Äì Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) |                                                                                                                                                                                                                  |
| [![coinex](https://user-images.githubusercontent.com/51840849/87182089-1e05fa00-c2ec-11ea-8da9-cc73b45abbbc.jpg)](https://www.coinex.com/register?refer_code=yw5fz)         | coinex        | [CoinEx](https://www.coinex.com/register?refer_code=yw5fz)                              | [![API Version 2](https://img.shields.io/badge/2-lightgray)](https://docs.coinex.com/api/v2)                                     | ![CEX ‚Äì Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX ‚Äì Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) |                                                                                                                                                                                                                  |
| [![cryptocom](https://user-images.githubusercontent.com/1294454/147792121-38ed5e36-c229-48d6-b49a-48d05fc19ed4.jpeg)](https://crypto.com/exch/kdacthrnxt)                   | cryptocom     | [Crypto.com](https://crypto.com/exch/kdacthrnxt)                                        | [![API Version 2](https://img.shields.io/badge/2-lightgray)](https://exchange-docs.crypto.com/exchange/v1/rest-ws/index.html)    | ![CEX ‚Äì Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX ‚Äì Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) | [![Sign up with Crypto.com using CCXT&#039;s referral link for a 75% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d75%25&amp;color=orange)](https://crypto.com/exch/kdacthrnxt)                        |
| [![gate](https://github.com/user-attachments/assets/64f988c5-07b6-4652-b5c1-679a6bf67c85)](https://www.gate.io/signup/2436035)                                              | gate          | [Gate.io](https://www.gate.io/signup/2436035)                                           | [![API Version 4](https://img.shields.io/badge/4-lightgray)](https://www.gate.io/docs/developers/apiv4/en/)                      | ![CEX ‚Äì Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX ‚Äì Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) | [![Sign up with Gate.io using CCXT&#039;s referral link for a 20% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d20%25&amp;color=orange)](https://www.gate.io/signup/2436035)                           |
| [![hashkey](https://github.com/user-attachments/assets/6dd6127b-cc19-4a13-9b29-a98d81f80e98)](https://global.hashkey.com/en-US/register/invite?invite_code=82FQUN)          | hashkey       | [HashKey Global](https://global.hashkey.com/en-US/register/invite?invite_code=82FQUN)   | [![API Version 1](https://img.shields.io/badge/1-lightgray)](https://hashkeyglobal-apidoc.readme.io/)                            | ![CEX ‚Äì Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX ‚Äì Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) |                                                                                                                                                                                                                  |
| [![htx](https://user-images.githubusercontent.com/1294454/76137448-22748a80-604e-11ea-8069-6e389271911d.jpg)](https://www.htx.com.vc/invite/en-us/1h?invite_code=6rmm2223)  | htx           | [HTX](https://www.htx.com.vc/invite/en-us/1h?invite_code=6rmm2223)                      | [![API Version 1](https://img.shields.io/badge/1-lightgray)](https://huobiapi.github.io/docs/spot/v1/en/)                        | ![CEX ‚Äì Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX ‚Äì Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) | [![Sign up with HTX using CCXT&#039;s referral link for a 15% discount!](https://img.shields.io/static/v1?label=Fee&amp;message=%2d15%25&amp;color=orange)](https://www.htx.com.vc/invite/en-us/1h?invite_code=6rmm2223)      |
| [![hyperliquid](https://github.com/ccxt/ccxt/assets/43336371/b371bc6c-4a8c-489f-87f4-20a913dd8d4b)](https://app.hyperliquid.xyz/)                                           | hyperliquid   | [Hyperliquid](https://app.hyperliquid.xyz/)                                             | [![API Version 1](https://img.shields.io/badge/1-lightgray)](https://hyperliquid.gitbook.io/hyperliquid-docs/for-developers/api) | ![DEX - Distributed EXchange](https://img.shields.io/badge/DEX-blue.svg &quot;DEX - Distributed EXchange&quot;)  | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) |                                                                                                                                                                                                                  |
| [![kucoin](https://user-images.githubusercontent.com/51840849/87295558-132aaf80-c50e-11ea-9801-a2fb0c57c799.jpg)](https://www.kucoin.com/ucenter/signup?rcode=E5wkqe)       | kucoin        | [KuCoin](https://www.kucoin.com/ucenter/signup?rcode=E5wkqe)                            | [![API Version 2](https://img.shields.io/badge/2-lightgray)](https://docs.kucoin.com)                                            | ![CEX ‚Äì Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX ‚Äì Centralized EXchange&quot;) | [![CCXT Certified](https://img.shields.io/badge/CCXT-Certified-green.svg)](https://github.com/ccxt/ccxt/wiki/Certification) | [![CCXT Pro](https://img.shields.io/badge/CCXT-Pro-black)](https://ccxt.pro) |                                                                                                                                                                                                                  |
| [![kucoinfutures](https://user-images.githubusercontent.com/1294454/147508995-9e35030a-d046-43a1-a006-6fabd981b554.jpg)](https://futures.kucoin.com/?rcode=E5wkqe)          | kucoinfutures | [KuCoin Futures](https://futures.kucoin.com/?rcode=E5wkqe)                              | [![API Version 1](https://img.shields.io/badge/1-lightgray)](https://docs.kucoin.com/futures)                                    | ![CEX ‚Äì Centralized EXchange](https://img.shields.io/badge/CEX-green.svg &quot;CEX ‚Äì C

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sktime/sktime]]></title>
            <link>https://github.com/sktime/sktime</link>
            <guid>https://github.com/sktime/sktime</guid>
            <pubDate>Thu, 29 May 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[A unified framework for machine learning with time series]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sktime/sktime">sktime/sktime</a></h1>
            <p>A unified framework for machine learning with time series</p>
            <p>Language: Python</p>
            <p>Stars: 8,868</p>
            <p>Forks: 1,587</p>
            <p>Stars today: 282 stars today</p>
            <h2>README</h2><pre>
## Welcome to sktime

&lt;a href=&quot;https://www.sktime.net&quot;&gt;&lt;img src=&quot;https://github.com/sktime/sktime/blob/main/docs/source/images/sktime-logo.svg&quot; width=&quot;175&quot; align=&quot;right&quot; /&gt;&lt;/a&gt;

&gt; A unified interface for machine learning with time series

:rocket: **Version 0.37.0 out now!** [Check out the release notes here](https://www.sktime.net/en/latest/changelog.html).

sktime is a library for time series analysis in Python. It provides a unified interface for multiple time series learning tasks. Currently, this includes forecasting, time series classification, clustering, anomaly/changepoint detection, and other tasks. It comes with [time series algorithms](https://www.sktime.net/en/stable/estimator_overview.html) and [scikit-learn] compatible tools to build, tune, and validate time series models.

[scikit-learn]: https://scikit-learn.org/stable/

|  | **[Documentation](https://www.sktime.net/en/stable/users.html)** ¬∑ **[Tutorials](https://www.sktime.net/en/stable/examples.html)** ¬∑ **[Release Notes](https://www.sktime.net/en/stable/changelog.html)** |
|---|---|
| **Open&amp;#160;Source** | [![BSD 3-clause](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](https://github.com/sktime/sktime/blob/main/LICENSE) |
| **Tutorials** | [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/sktime/sktime/main?filepath=examples) [![!youtube](https://img.shields.io/static/v1?logo=youtube&amp;label=YouTube&amp;message=tutorials&amp;color=red)](https://www.youtube.com/playlist?list=PLKs3UgGjlWHqNzu0LEOeLKvnjvvest2d0) |
| **Community** | [![!discord](https://img.shields.io/static/v1?logo=discord&amp;label=discord&amp;message=chat&amp;color=lightgreen)](https://discord.com/invite/54ACzaFsn7) [![!slack](https://img.shields.io/static/v1?logo=linkedin&amp;label=LinkedIn&amp;message=news&amp;color=lightblue)](https://www.linkedin.com/company/scikit-time/)  |
| **CI/CD** | [![github-actions](https://img.shields.io/github/actions/workflow/status/sktime/sktime/wheels.yml?logo=github)](https://github.com/sktime/sktime/actions/workflows/wheels.yml) [![readthedocs](https://img.shields.io/readthedocs/sktime?logo=readthedocs)](https://www.sktime.net/en/latest/?badge=latest) [![platform](https://img.shields.io/conda/pn/conda-forge/sktime)](https://github.com/sktime/sktime) |
| **Code** |  [![!pypi](https://img.shields.io/pypi/v/sktime?color=orange)](https://pypi.org/project/sktime/) [![!conda](https://img.shields.io/conda/vn/conda-forge/sktime)](https://anaconda.org/conda-forge/sktime) [![!python-versions](https://img.shields.io/pypi/pyversions/sktime)](https://www.python.org/) [![!black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)  |
| **Downloads** | ![PyPI - Downloads](https://img.shields.io/pypi/dw/sktime) ![PyPI - Downloads](https://img.shields.io/pypi/dm/sktime) [![Downloads](https://static.pepy.tech/personalized-badge/sktime?period=total&amp;units=international_system&amp;left_color=grey&amp;right_color=blue&amp;left_text=cumulative%20(pypi))](https://pepy.tech/project/sktime) |
| **Citation** | [![!zenodo](https://zenodo.org/badge/DOI/10.5281/zenodo.3749000.svg)](https://doi.org/10.5281/zenodo.3749000) |

## :books: Documentation

| Documentation                        |                                                                |
|--------------------------------------| -------------------------------------------------------------- |
| :star: **[Tutorials]**               | New to sktime? Here&#039;s everything you need to know!              |
| :clipboard: **[Binder Notebooks]**   | Example notebooks to play with in your browser.              |
| :woman_technologist: **[Examples]**  | How to use sktime and its features.                             |
| :scissors: **[Extension Templates]** | How to build your own estimator using sktime&#039;s API.            |
| :control_knobs: **[API Reference]**  | The detailed reference for sktime&#039;s API.                        |
| :tv: **[Video Tutorial]**            | Our video tutorial from 2021 PyData Global.      |
| :hammer_and_wrench: **[Changelog]**  | Changes and version history.                                   |
| :deciduous_tree: **[Roadmap]**       | sktime&#039;s software and community development plan.                                   |
| :pencil: **[Related Software]**      | A list of related software. |

[tutorials]: https://www.sktime.net/en/latest/tutorials.html
[binder notebooks]: https://mybinder.org/v2/gh/sktime/sktime/main?filepath=examples
[examples]: https://www.sktime.net/en/latest/examples.html
[video tutorial]: https://github.com/sktime/sktime-tutorial-pydata-global-2021
[api reference]: https://www.sktime.net/en/latest/api_reference.html
[changelog]: https://www.sktime.net/en/latest/changelog.html
[roadmap]: https://www.sktime.net/en/latest/roadmap.html
[related software]: https://www.sktime.net/en/latest/related_software.html

## :speech_balloon: Where to ask questions

Questions and feedback are extremely welcome! We strongly believe in the value of sharing help publicly, as it allows a wider audience to benefit from it.

| Type                            | Platforms                               |
| ------------------------------- | --------------------------------------- |
| :bug: **Bug Reports**              | [GitHub Issue Tracker]                  |
| :sparkles: **Feature Requests &amp; Ideas** | [GitHub Issue Tracker]                       |
| :woman_technologist: **Usage Questions**          | [GitHub Discussions] ¬∑ [Stack Overflow] |
| :speech_balloon: **General Discussion**        | [GitHub Discussions] |
| :factory: **Contribution &amp; Development** | `dev-chat` channel ¬∑ [Discord] |
| :globe_with_meridians: **Meet-ups and collaboration sessions** | [Discord] - Fridays 13 UTC, dev/meet-ups channel |

[github issue tracker]: https://github.com/sktime/sktime/issues
[github discussions]: https://github.com/sktime/sktime/discussions
[stack overflow]: https://stackoverflow.com/questions/tagged/sktime
[discord]: https://discord.com/invite/54ACzaFsn7

## :dizzy: Features
Our objective is to enhance the interoperability and usability of the time series analysis ecosystem in its entirety. sktime provides a __unified interface for distinct but related time series learning tasks__. It features [__dedicated time series algorithms__](https://www.sktime.net/en/stable/estimator_overview.html) and __tools for composite model building__,  such as pipelining, ensembling, tuning, and reduction, empowering users to apply algorithms designed for one task to another.

sktime also provides **interfaces to related libraries**, for example [scikit-learn], [statsmodels], [tsfresh], [PyOD], and [fbprophet], among others.

[statsmodels]: https://www.statsmodels.org/stable/index.html
[tsfresh]: https://tsfresh.readthedocs.io/en/latest/
[pyod]: https://pyod.readthedocs.io/en/latest/
[fbprophet]: https://facebook.github.io/prophet/

| Module | Status | Links |
|---|---|---|
| **[Forecasting]** | stable | [Tutorial](https://www.sktime.net/en/latest/examples/01_forecasting.html) ¬∑ [API Reference](https://www.sktime.net/en/latest/api_reference/forecasting.html) ¬∑ [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/forecasting.py)  |
| **[Time Series Classification]** | stable | [Tutorial](https://github.com/sktime/sktime/blob/main/examples/02_classification.ipynb) ¬∑ [API Reference](https://www.sktime.net/en/latest/api_reference/classification.html) ¬∑ [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/classification.py) |
| **[Time Series Regression]** | stable | [API Reference](https://www.sktime.net/en/latest/api_reference/regression.html) |
| **[Transformations]** | stable | [Tutorial](https://github.com/sktime/sktime/blob/main/examples/03_transformers.ipynb) ¬∑ [API Reference](https://www.sktime.net/en/latest/api_reference/transformations.html) ¬∑ [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/transformer.py)  |
| **[Detection tasks]** | maturing | [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/detection.py) |
| **[Parameter fitting]** | maturing | [API Reference](https://www.sktime.net/en/latest/api_reference/param_est.html) ¬∑ [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/transformer.py)  |
| **[Time Series Clustering]** | maturing | [API Reference](https://www.sktime.net/en/latest/api_reference/clustering.html) ¬∑  [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/clustering.py) |
| **[Time Series Distances/Kernels]** | maturing | [Tutorial](https://github.com/sktime/sktime/blob/main/examples/03_transformers.ipynb) ¬∑ [API Reference](https://www.sktime.net/en/latest/api_reference/dists_kernels.html) ¬∑ [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/dist_kern_panel.py) |
| **[Time Series Alignment]** | experimental | [API Reference](https://www.sktime.net/en/latest/api_reference/alignment.html) ¬∑ [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/alignment.py) |
| **[Time Series Splitters]** | maturing | [Extension Template](https://github.com/sktime/sktime/blob/main/extension_templates/split.py) | |
| **[Distributions and simulation]** | experimental |  |

[forecasting]: https://github.com/sktime/sktime/tree/main/sktime/forecasting
[time series classification]: https://github.com/sktime/sktime/tree/main/sktime/classification
[time series regression]: https://github.com/sktime/sktime/tree/main/sktime/regression
[time series clustering]: https://github.com/sktime/sktime/tree/main/sktime/clustering
[detection tasks]: https://github.com/sktime/sktime/tree/main/sktime/detection
[time series distances/kernels]: https://github.com/sktime/sktime/tree/main/sktime/dists_kernels
[time series alignment]: https://github.com/sktime/sktime/tree/main/sktime/alignment
[transformations]: https://github.com/sktime/sktime/tree/main/sktime/transformations
[distributions and simulation]: https://github.com/sktime/sktime/tree/main/sktime/proba
[time series splitters]: https://github.com/sktime/sktime/tree/main/sktime/split
[parameter fitting]: https://github.com/sktime/sktime/tree/main/sktime/param_est


## :hourglass_flowing_sand: Install sktime
For troubleshooting and detailed installation instructions, see the [documentation](https://www.sktime.net/en/latest/installation.html).

- **Operating system**: macOS X ¬∑ Linux ¬∑ Windows 8.1 or higher
- **Python version**: Python 3.8, 3.9, 3.10, 3.11, and 3.12 (only 64-bit)
- **Package managers**: [pip] ¬∑ [conda] (via `conda-forge`)

[pip]: https://pip.pypa.io/en/stable/
[conda]: https://docs.conda.io/en/latest/

### pip
Using pip, sktime releases are available as source packages and binary wheels.
Available wheels are listed [here](https://pypi.org/simple/sktime/).

```bash
pip install sktime
```

or, with maximum dependencies,

```bash
pip install sktime[all_extras]
```

For curated sets of soft dependencies for specific learning tasks:

```bash
pip install sktime[forecasting]  # for selected forecasting dependencies
pip install sktime[forecasting,transformations]  # forecasters and transformers
```

or similar. Valid sets are:

* `forecasting`
* `transformations`
* `classification`
* `regression`
* `clustering`
* `param_est`
* `networks`
* `detection`
* `alignment`

Cave: in general, not all soft dependencies for a learning task are installed,
only a curated selection.

### conda
You can also install sktime from `conda` via the `conda-forge` channel.
The feedstock including the build recipe and configuration is maintained
in [this conda-forge repository](https://github.com/conda-forge/sktime-feedstock).

```bash
conda install -c conda-forge sktime
```

or, with maximum dependencies,

```bash
conda install -c conda-forge sktime-all-extras
```

(as `conda` does not support dependency sets,
flexible choice of soft dependencies is unavailable via `conda`)

## :zap: Quickstart

### Forecasting

``` python
from sktime.datasets import load_airline
from sktime.forecasting.base import ForecastingHorizon
from sktime.forecasting.theta import ThetaForecaster
from sktime.split import temporal_train_test_split
from sktime.performance_metrics.forecasting import mean_absolute_percentage_error

y = load_airline()
y_train, y_test = temporal_train_test_split(y)
fh = ForecastingHorizon(y_test.index, is_relative=False)
forecaster = ThetaForecaster(sp=12)  # monthly seasonal periodicity
forecaster.fit(y_train)
y_pred = forecaster.predict(fh)
mean_absolute_percentage_error(y_test, y_pred)
&gt;&gt;&gt; 0.08661467738190656
```

### Time Series Classification

```python
from sktime.classification.interval_based import TimeSeriesForestClassifier
from sktime.datasets import load_arrow_head
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X, y = load_arrow_head()
X_train, X_test, y_train, y_test = train_test_split(X, y)
classifier = TimeSeriesForestClassifier()
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
accuracy_score(y_test, y_pred)
&gt;&gt;&gt; 0.8679245283018868
```

## :wave: How to get involved

There are many ways to join the sktime community. We follow the [all-contributors](https://github.com/all-contributors/all-contributors) specification: all kinds of contributions are welcome - not just code.

| Documentation              |                                                                |
| -------------------------- | --------------------------------------------------------------        |
| :gift_heart: **[Contribute]**        | How to contribute to sktime.          |
| :school_satchel:  **[Mentoring]** | New to open source? Apply to our mentoring program! |
| :date: **[Meetings]** | Join our discussions, tutorials, workshops, and sprints! |
| :woman_mechanic:  **[Developer Guides]**      | How to further develop sktime&#039;s code base.                             |
| :construction: **[Enhancement Proposals]** | Design a new feature for sktime. |
| :medal_sports: **[Contributors]** | A list of all contributors. |
| :raising_hand: **[Roles]** | An overview of our core community roles. |
| :money_with_wings: **[Donate]** | Fund sktime maintenance and development. |
| :classical_building: **[Governance]** | How and by whom decisions are made in sktime&#039;s community.   |

[contribute]: https://www.sktime.net/en/latest/get_involved/contributing.html
[donate]: https://opencollective.com/sktime
[extension templates]: https://github.com/sktime/sktime/tree/main/extension_templates
[developer guides]: https://www.sktime.net/en/latest/developer_guide.html
[contributors]: https://github.com/sktime/sktime/blob/main/CONTRIBUTORS.md
[governance]: https://www.sktime.net/en/latest/get_involved/governance.html
[mentoring]: https://github.com/sktime/mentoring
[meetings]: https://calendar.google.com/calendar/u/0/embed?src=sktime.toolbox@gmail.com&amp;ctz=UTC
[enhancement proposals]: https://github.com/sktime/enhancement-proposals
[roles]: https://www.sktime.net/en/latest/about/team.html

## :trophy: Hall of fame

Thanks to all our community for all your wonderful contributions, PRs, issues, ideas.

&lt;a href=&quot;https://github.com/sktime/sktime/graphs/contributors&quot;&gt;
&lt;img src=&quot;https://opencollective.com/sktime/contributors.svg?width=600&amp;button=false&quot; /&gt;
&lt;/a&gt;
&lt;br&gt;

## :bulb: Project vision

* **By the community, for the community** -- developed by a friendly and collaborative community.
* The **right tool for the right task** -- helping users to diagnose their learning problem and suitable scientific model types.
* **Embedded in state-of-art ecosystems** and **provider of interoperable interfaces** -- interoperable with [scikit-learn], [statsmodels], [tsfresh], and other community favorites.
* **Rich model composition and reduction functionality** -- build tuning and feature extraction pipelines, solve forecasting tasks with [scikit-learn] regressors.
* **Clean, descriptive specification syntax** -- based on modern object-oriented design principles for data science.
* **Fair model assessment and benchmarking** -- build your models, inspect your models, check your models, and avoid pitfalls.
* **Easily extensible** -- easy extension templates to add your own algorithms compatible with sktime&#039;s API.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[allenai/olmocr]]></title>
            <link>https://github.com/allenai/olmocr</link>
            <guid>https://github.com/allenai/olmocr</guid>
            <pubDate>Thu, 29 May 2025 00:04:20 GMT</pubDate>
            <description><![CDATA[Toolkit for linearizing PDFs for LLM datasets/training]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/allenai/olmocr">allenai/olmocr</a></h1>
            <p>Toolkit for linearizing PDFs for LLM datasets/training</p>
            <p>Language: Python</p>
            <p>Stars: 12,570</p>
            <p>Forks: 883</p>
            <p>Stars today: 40 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;!-- &lt;img src=&quot;https://github.com/allenai/OLMo/assets/8812459/774ac485-a535-4768-8f7c-db7be20f5cc3&quot; width=&quot;300&quot;/&gt; --&gt;
&lt;img src=&quot;https://github.com/user-attachments/assets/d70c8644-3e64-4230-98c3-c52fddaeccb6&quot; alt=&quot;olmOCR Logo&quot; width=&quot;300&quot;/&gt;
&lt;hr/&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/allenai/OLMo/blob/main/LICENSE&quot;&gt;
    &lt;img alt=&quot;GitHub License&quot; src=&quot;https://img.shields.io/github/license/allenai/OLMo&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/allenai/olmocr/releases&quot;&gt;
    &lt;img alt=&quot;GitHub release&quot; src=&quot;https://img.shields.io/github/release/allenai/olmocr.svg&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://olmocr.allenai.org/papers/olmocr.pdf&quot;&gt;
    &lt;img alt=&quot;Tech Report&quot; src=&quot;https://img.shields.io/badge/Paper-olmOCR-blue&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://olmocr.allenai.org&quot;&gt;
    &lt;img alt=&quot;Demo&quot; src=&quot;https://img.shields.io/badge/Ai2-Demo-F0529C&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/sZq3jTNVNG&quot;&gt;
    &lt;img alt=&quot;Discord&quot; src=&quot;https://img.shields.io/badge/Discord%20-%20blue?style=flat&amp;logo=discord&amp;label=Ai2&amp;color=%235B65E9&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

A toolkit for converting PDFs and other image-based document formats into clean, readable, plain text format.

Try the online demo: [https://olmocr.allenai.org/](https://olmocr.allenai.org/)

Features:
 - Convert PDF, PNG, and JPEG based documents into clean Markdown
 - Support for equations, tables, handwriting, and complex formatting
 - Automatically removes headers and footers
 - Convert into text with a natural reading order, even in the presence of
   figures, multi-column layouts, and insets
 - Efficient, less than $200 USD per million pages converted
 - (Based on a 7B parameter VLM, so it requires a GPU)

### News
 - May 23, 2025 - v0.1.70 - Official docker support and images are now available! [See Docker usage](#using-docker)
 - May 19, 2025 - v0.1.68 - [olmOCR-Bench](https://github.com/allenai/olmocr/tree/main/olmocr/bench) launch, scoring 77.4. Launch includes 2 point performance boost in olmOCR pipeline due to bug fixes with prompts.
 - Mar 17, 2025 - v0.1.60 - Performance improvements due to better temperature selection in sampling.
 - Feb 25, 2025 - v0.1.58 -  Initial public launch and demo.

### Benchmark

[**olmOCR-Bench**](https://github.com/allenai/olmocr/tree/main/olmocr/bench):
We also ship a comprehensive benchmark suite covering over 7,000 test cases across 1,400 documents to help measure performance of OCR systems. 

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th align=&quot;left&quot;&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt;
      &lt;th align=&quot;center&quot;&gt;AR&lt;/th&gt;
      &lt;th align=&quot;center&quot;&gt;OSM&lt;/th&gt;
      &lt;th align=&quot;center&quot;&gt;TA&lt;/th&gt;
      &lt;th align=&quot;center&quot;&gt;OS&lt;/th&gt;
      &lt;th align=&quot;center&quot;&gt;HF&lt;/th&gt;
      &lt;th align=&quot;center&quot;&gt;MC&lt;/th&gt;
      &lt;th align=&quot;center&quot;&gt;LTT&lt;/th&gt;
      &lt;th align=&quot;center&quot;&gt;Base&lt;/th&gt;
      &lt;th align=&quot;center&quot;&gt;Overall Score&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td align=&quot;left&quot;&gt;Marker v1.6.2&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;24.3&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;22.1&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;69.8&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;24.3&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;87.1&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;71.0&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;76.9&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;99.5&lt;/strong&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;59.4 ¬± 1.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;left&quot;&gt;MinerU v1.3.10&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;75.4&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;47.4&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;60.9&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;17.3&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;96.6&lt;/strong&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;59.0&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;39.1&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;96.6&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;61.5 ¬± 1.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;left&quot;&gt;Mistral OCR API&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;77.2&lt;/strong&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;67.5&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;60.6&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;29.3&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;93.6&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;71.3&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;77.1&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;99.4&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;72.0 ¬± 1.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td align=&quot;left&quot;&gt;olmOCR v0.1.68 (pipeline.py)&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;75.6&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;75.1&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;70.2&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;44.5&lt;/strong&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;93.4&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;79.4&lt;/strong&gt;&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;81.7&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;99.0&lt;/td&gt;
      &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;77.4 ¬± 1.0&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

### Installation

Requirements:
 - Recent NVIDIA GPU (tested on RTX 4090, L40S, A100, H100) with at least 20 GB of GPU RAM
 - 30GB of free disk space

You will need to install poppler-utils and additional fonts for rendering PDF images.

Install dependencies (Ubuntu/Debian)
```bash
sudo apt-get update
sudo apt-get install poppler-utils ttf-mscorefonts-installer msttcorefonts fonts-crosextra-caladea fonts-crosextra-carlito gsfonts lcdf-typetools
```

Set up a conda environment and install olmocr. The requirements for running olmOCR
are difficult to install in an existing python environment, so please do make a clean python environment to install into.
```bash
conda create -n olmocr python=3.11
conda activate olmocr

# For CPU-only operations, ex running the benchmark
pip install olmocr[bench]

# For actually converting the files with your own GPU
pip install olmocr[gpu] --find-links https://flashinfer.ai/whl/cu124/torch2.4/flashinfer/
```

### Local Usage Example

For quick testing, try the [web demo](https://olmocr.allen.ai/). To run locally, a GPU is required, as inference is powered by [sglang](https://github.com/sgl-project/sglang) under the hood.

Convert a Single PDF:
```bash
# Download a sample PDF
curl -o olmocr-sample.pdf https://olmocr.allenai.org/papers/olmocr_3pg_sample.pdf

# Convert it to markdown
python -m olmocr.pipeline ./localworkspace --markdown --pdfs olmocr-sample.pdf
```

Convert an Image file:
```bash
python -m olmocr.pipeline ./localworkspace --markdown --pdfs random_page.png
```

Convert Multiple PDFs:
```bash
python -m olmocr.pipeline ./localworkspace --markdown --pdfs tests/gnarly_pdfs/*.pdf
```

With the addition of the `--markdown` flag, results will be stored as markdown files inside of `./localworkspace/markdown/`. 

#### Viewing Results

The `./localworkspace/` workspace folder will then have both [Dolma](https://github.com/allenai/dolma) and markdown files (if using `--markdown`).


```bash
cat localworkspace/markdown/olmocr-sample.md 
```

```
olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models
...
```

### Multi-node / Cluster Usage

If you want to convert millions of PDFs, using multiple nodes running in parallel, then olmOCR supports
reading your PDFs from AWS S3, and coordinating work using an AWS S3 output bucket.

For example, you can start this command on your first worker node, and it will set up
a simple work queue in your AWS bucket and start converting PDFs.

```bash
python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace --pdfs s3://my_s3_bucket/jakep/gnarly_pdfs/*.pdf
```

Now on any subsequent nodes, just run this and they will start grabbing items from the same workspace queue.
```bash
python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace
```

If you are at Ai2 and want to linearize millions of PDFs efficiently using [beaker](https://www.beaker.org), just add the `--beaker`
flag. This will prepare the workspace on your local machine, and then launch N GPU workers in the cluster to start
converting PDFs.

For example:
```bash
python -m olmocr.pipeline s3://my_s3_bucket/pdfworkspaces/exampleworkspace --pdfs s3://my_s3_bucket/jakep/gnarly_pdfs/*.pdf --beaker --beaker_gpus 4
```

### Using Docker

Pull the Docker image.
```bash
docker pull alleninstituteforai/olmocr:latest
```

To run the container interactively:
```bash
docker run -it --gpus all --name olmocr_container alleninstituteforai/olmocr:latest /bin/bash
```

If you want to access your local files inside the container, use volume mounting:
```bash
docker run -it --gpus all \
  -v /path/to/your/local/files:/local_files \
  --name olmocr_container \
  alleninstituteforai/olmocr:latest /bin/bash
```

All dependencies are already installed. Once you‚Äôre inside the container, you can run olmOCR commands. For example:

```bash
curl -o olmocr-sample.pdf https://olmocr.allenai.org/papers/olmocr_3pg_sample.pdf

python -m olmocr.pipeline ./localworkspace --markdown --pdfs olmocr-sample.pdf
```
&gt; You can also visit our Docker repository on [Docker Hub](https://hub.docker.com/r/alleninstituteforai/olmocr).

### Full documentation for the pipeline

```bash
python -m olmocr.pipeline --help
usage: pipeline.py [-h] [--pdfs PDFS] [--workspace_profile WORKSPACE_PROFILE] [--pdf_profile PDF_PROFILE] [--pages_per_group PAGES_PER_GROUP]
                   [--max_page_retries MAX_PAGE_RETRIES] [--max_page_error_rate MAX_PAGE_ERROR_RATE] [--workers WORKERS] [--apply_filter] [--stats] [--model MODEL]
                   [--model_max_context MODEL_MAX_CONTEXT] [--model_chat_template MODEL_CHAT_TEMPLATE] [--target_longest_image_dim TARGET_LONGEST_IMAGE_DIM]
                   [--target_anchor_text_len TARGET_ANCHOR_TEXT_LEN] [--beaker] [--beaker_workspace BEAKER_WORKSPACE] [--beaker_cluster BEAKER_CLUSTER]
                   [--beaker_gpus BEAKER_GPUS] [--beaker_priority BEAKER_PRIORITY]
                   workspace

Manager for running millions of PDFs through a batch inference pipeline

positional arguments:
  workspace             The filesystem path where work will be stored, can be a local folder, or an s3 path if coordinating work with many workers, s3://bucket/prefix/

options:
  -h, --help            show this help message and exit
  --pdfs PDFS           Path to add pdfs stored in s3 to the workspace, can be a glob path s3://bucket/prefix/*.pdf or path to file containing list of pdf paths
  --workspace_profile WORKSPACE_PROFILE
                        S3 configuration profile for accessing the workspace
  --pdf_profile PDF_PROFILE
                        S3 configuration profile for accessing the raw pdf documents
  --pages_per_group PAGES_PER_GROUP
                        Aiming for this many pdf pages per work item group
  --max_page_retries MAX_PAGE_RETRIES
                        Max number of times we will retry rendering a page
  --max_page_error_rate MAX_PAGE_ERROR_RATE
                        Rate of allowable failed pages in a document, 1/250 by default
  --workers WORKERS     Number of workers to run at a time
  --apply_filter        Apply basic filtering to English pdfs which are not forms, and not likely seo spam
  --stats               Instead of running any job, reports some statistics about the current workspace
  --markdown            Also write natural text to markdown files preserving the folder structure of the input pdfs
  --model MODEL         List of paths where you can find the model to convert this pdf. You can specify several different paths here, and the script will try to use the
                        one which is fastest to access
  --model_max_context MODEL_MAX_CONTEXT
                        Maximum context length that the model was fine tuned under
  --model_chat_template MODEL_CHAT_TEMPLATE
                        Chat template to pass to sglang server
  --target_longest_image_dim TARGET_LONGEST_IMAGE_DIM
                        Dimension on longest side to use for rendering the pdf pages
  --target_anchor_text_len TARGET_ANCHOR_TEXT_LEN
                        Maximum amount of anchor text to use (characters)
  --beaker              Submit this job to beaker instead of running locally
  --beaker_workspace BEAKER_WORKSPACE
                        Beaker workspace to submit to
  --beaker_cluster BEAKER_CLUSTER
                        Beaker clusters you want to run on
  --beaker_gpus BEAKER_GPUS
                        Number of gpu replicas to run
  --beaker_priority BEAKER_PRIORITY
                        Beaker priority level for the job
```

## Code overview

There are some nice reusable pieces of the code that may be useful for your own projects:
 - A prompting strategy to get really good natural text parsing using ChatGPT 4o - [buildsilver.py](https://github.com/allenai/olmocr/blob/main/olmocr/data/buildsilver.py)
 - An side-by-side eval toolkit for comparing different pipeline versions - [runeval.py](https://github.com/allenai/olmocr/blob/main/olmocr/eval/runeval.py)
 - Basic filtering by language and SEO spam removal - [filter.py](https://github.com/allenai/olmocr/blob/main/olmocr/filter/filter.py)
 - Finetuning code for Qwen2-VL and Molmo-O - [train.py](https://github.com/allenai/olmocr/blob/main/olmocr/train/train.py)
 - Processing millions of PDFs through a finetuned model using Sglang - [pipeline.py](https://github.com/allenai/olmocr/blob/main/olmocr/pipeline.py)
 - Viewing [Dolma docs](https://github.com/allenai/dolma) created from PDFs - [dolmaviewer.py](https://github.com/allenai/olmocr/blob/main/olmocr/viewer/dolmaviewer.py)



## Team

&lt;!-- start team --&gt;

**olmOCR** is developed and maintained by the AllenNLP team, backed by [the Allen Institute for Artificial Intelligence (AI2)](https://allenai.org/).
AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering.
To learn more about who specifically contributed to this codebase, see [our contributors](https://github.com/allenai/olmocr/graphs/contributors) page.

&lt;!-- end team --&gt;

## License

&lt;!-- start license --&gt;

**olmOCR** is licensed under [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0).
A full copy of the license can be found [on GitHub](https://github.com/allenai/olmocr/blob/main/LICENSE).

&lt;!-- end license --&gt;

## Citing

```bibtex
@misc{olmocr,
      title={{olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models}},
      author={Jake Poznanski and Jon Borchardt and Jason Dunkelberger and Regan Huff and Daniel Lin and Aman Rangapur and Christopher Wilhelm and Kyle Lo and Luca Soldaini},
      year={2025},
      eprint={2502.18443},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.18443},
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[kubernetes-client/python]]></title>
            <link>https://github.com/kubernetes-client/python</link>
            <guid>https://github.com/kubernetes-client/python</guid>
            <pubDate>Thu, 29 May 2025 00:04:19 GMT</pubDate>
            <description><![CDATA[Official Python client library for kubernetes]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/kubernetes-client/python">kubernetes-client/python</a></h1>
            <p>Official Python client library for kubernetes</p>
            <p>Language: Python</p>
            <p>Stars: 7,114</p>
            <p>Forks: 3,336</p>
            <p>Stars today: 5 stars today</p>
            <h2>README</h2><pre># Kubernetes Python Client

[![Build Status](https://travis-ci.org/kubernetes-client/python.svg?branch=master)](https://travis-ci.org/kubernetes-client/python)
[![PyPI version](https://badge.fury.io/py/kubernetes.svg)](https://badge.fury.io/py/kubernetes)
[![codecov](https://codecov.io/gh/kubernetes-client/python/branch/master/graph/badge.svg)](https://codecov.io/gh/kubernetes-client/python &quot;Non-generated packages only&quot;)
[![pypi supported versions](https://img.shields.io/pypi/pyversions/kubernetes.svg)](https://pypi.python.org/pypi/kubernetes)
[![Client Capabilities](https://img.shields.io/badge/Kubernetes%20client-Silver-blue.svg?style=flat&amp;colorB=C0C0C0&amp;colorA=306CE8)](http://bit.ly/kubernetes-client-capabilities-badge)
[![Client Support Level](https://img.shields.io/badge/kubernetes%20client-beta-green.svg?style=flat&amp;colorA=306CE8)](http://bit.ly/kubernetes-client-support-badge)

Python client for the [kubernetes](http://kubernetes.io/) API.

## Installation

From source:

```
git clone --recursive https://github.com/kubernetes-client/python.git
cd python
python setup.py install
```

From [PyPI](https://pypi.python.org/pypi/kubernetes/) directly:

```
pip install kubernetes
```

## Examples

list all pods:

```python
from kubernetes import client, config

# Configs can be set in Configuration class directly or using helper utility
config.load_kube_config()

v1 = client.CoreV1Api()
print(&quot;Listing pods with their IPs:&quot;)
ret = v1.list_pod_for_all_namespaces(watch=False)
for i in ret.items:
    print(&quot;%s\t%s\t%s&quot; % (i.status.pod_ip, i.metadata.namespace, i.metadata.name))
```

watch on namespace object:

```python
from kubernetes import client, config, watch

# Configs can be set in Configuration class directly or using helper utility
config.load_kube_config()

v1 = client.CoreV1Api()
count = 10
w = watch.Watch()
for event in w.stream(v1.list_namespace, _request_timeout=60):
    print(&quot;Event: %s %s&quot; % (event[&#039;type&#039;], event[&#039;object&#039;].metadata.name))
    count -= 1
    if not count:
        w.stop()

print(&quot;Ended.&quot;)
```

More examples can be found in [examples](examples/) folder. To run examples, run this command:

```shell
python -m examples.example1
```

(replace example1 with one of the filenames in the examples folder)

## Documentation

All APIs and Models&#039; documentation can be found at the [Generated client&#039;s README file](kubernetes/README.md)

## Compatibility

`client-python` follows [semver](http://semver.org/), so until the major version of
client-python gets increased, your code will continue to work with explicitly
supported versions of Kubernetes clusters.

#### Compatibility matrix of supported client versions

- [client 9.y.z](https://pypi.org/project/kubernetes/9.0.1/): Kubernetes 1.12 or below (+-), Kubernetes 1.13 (‚úì), Kubernetes 1.14 or above (+-)
- [client 10.y.z](https://pypi.org/project/kubernetes/10.1.0/): Kubernetes 1.13 or below (+-), Kubernetes 1.14 (‚úì), Kubernetes 1.14 or above (+-)
- [client 11.y.z](https://pypi.org/project/kubernetes/11.0.0/): Kubernetes 1.14 or below (+-), Kubernetes 1.15 (‚úì), Kubernetes 1.16 or above (+-)
- [client 12.y.z](https://pypi.org/project/kubernetes/12.0.1/): Kubernetes 1.15 or below (+-), Kubernetes 1.16 (‚úì), Kubernetes 1.17 or above (+-)
- [client 17.y.z](https://pypi.org/project/kubernetes/17.17.0/): Kubernetes 1.16 or below (+-), Kubernetes 1.17 (‚úì), Kubernetes 1.18 or above (+-)
- [client 18.y.z](https://pypi.org/project/kubernetes/18.20.0/): Kubernetes 1.17 or below (+-), Kubernetes 1.18 (‚úì), Kubernetes 1.19 or above (+-)
- [client 19.y.z](https://pypi.org/project/kubernetes/19.15.0/): Kubernetes 1.18 or below (+-), Kubernetes 1.19 (‚úì), Kubernetes 1.20 or above (+-)
- [client 20.y.z](https://pypi.org/project/kubernetes/20.13.0/): Kubernetes 1.19 or below (+-), Kubernetes 1.20 (‚úì), Kubernetes 1.21 or above (+-)
- [client 21.y.z](https://pypi.org/project/kubernetes/21.7.0/): Kubernetes 1.20 or below (+-), Kubernetes 1.21 (‚úì), Kubernetes 1.22 or above (+-)
- [client 22.y.z](https://pypi.org/project/kubernetes/22.6.0/): Kubernetes 1.21 or below (+-), Kubernetes 1.22 (‚úì), Kubernetes 1.23 or above (+-)
- [client 23.y.z](https://pypi.org/project/kubernetes/23.6.0/): Kubernetes 1.22 or below (+-), Kubernetes 1.23 (‚úì), Kubernetes 1.24 or above (+-)
- [client 24.y.z](https://pypi.org/project/kubernetes/24.2.0/): Kubernetes 1.23 or below (+-), Kubernetes 1.24 (‚úì), Kubernetes 1.25 or above (+-)
- [client 25.y.z](https://pypi.org/project/kubernetes/25.3.0/): Kubernetes 1.24 or below (+-), Kubernetes 1.25 (‚úì), Kubernetes 1.26 or above (+-)
- [client 26.y.z](https://pypi.org/project/kubernetes/26.1.0/): Kubernetes 1.25 or below (+-), Kubernetes 1.26 (‚úì), Kubernetes 1.27 or above (+-)
- [client 27.y.z](https://pypi.org/project/kubernetes/27.2.0/): Kubernetes 1.26 or below (+-), Kubernetes 1.27 (‚úì), Kubernetes 1.28 or above (+-)
- [client 28.y.z](https://pypi.org/project/kubernetes/28.1.0/): Kubernetes 1.27 or below (+-), Kubernetes 1.28 (‚úì), Kubernetes 1.29 or above (+-)
- [client 29.y.z](https://pypi.org/project/kubernetes/29.0.0/): Kubernetes 1.28 or below (+-), Kubernetes 1.29 (‚úì), Kubernetes 1.30 or above (+-)
- [client 30.y.z](https://pypi.org/project/kubernetes/30.1.0/): Kubernetes 1.29 or below (+-), Kubernetes 1.30 (‚úì), Kubernetes 1.31 or above (+-)
- [client 31.y.z](https://pypi.org/project/kubernetes/31.0.0/): Kubernetes 1.30 or below (+-), Kubernetes 1.31 (‚úì), Kubernetes 1.32 or above (+-)
- [client 32.y.z](https://pypi.org/project/kubernetes/32.0.1/): Kubernetes 1.31 or below (+-), Kubernetes 1.32 (‚úì), Kubernetes 1.33 or above (+-)
- [client 33.y.z](https://pypi.org/project/kubernetes/33.0.1/): Kubernetes 1.32 or below (+-), Kubernetes 1.33 (‚úì), Kubernetes 1.34 or above (+-)


&gt; See [here](#homogenizing-the-kubernetes-python-client-versions) for an explanation of why there is no v13-v16 release.

Key:

* `‚úì` Exactly the same features / API objects in both client-python and the Kubernetes
  version.
* `+` client-python has features or API objects that may not be present in the Kubernetes
 cluster, either due to that client-python has additional new API, or that the server has
 removed old API. However, everything they have in common (i.e., most APIs) will work.
 Please note that alpha APIs may vanish or change significantly in a single release.
* `-` The Kubernetes cluster has features the client-python library can&#039;t use, either due
 to the server has additional new API, or that client-python has removed old API. However,
 everything they share in common (i.e., most APIs) will work.

See the [CHANGELOG](./CHANGELOG.md) for a detailed description of changes
between client-python versions.

| Client version  | Canonical source for OpenAPI spec    | Maintenance status            |
|-----------------|--------------------------------------|-------------------------------|
| 5.0 Alpha/Beta  | Kubernetes main repo, 1.9 branch     | ‚úó                             |
| 5.0             | Kubernetes main repo, 1.9 branch     | ‚úó                             |
| 6.0 Alpha/Beta  | Kubernetes main repo, 1.10 branch    | ‚úó                             |
| 6.0             | Kubernetes main repo, 1.10 branch    | ‚úó                             |
| 7.0 Alpha/Beta  | Kubernetes main repo, 1.11 branch    | ‚úó                             |
| 7.0             | Kubernetes main repo, 1.11 branch    | ‚úó                             |
| 8.0 Alpha/Beta  | Kubernetes main repo, 1.12 branch    | ‚úó                             |
| 8.0             | Kubernetes main repo, 1.12 branch    | ‚úó                             |
| 9.0 Alpha/Beta  | Kubernetes main repo, 1.13 branch    | ‚úó                             |
| 9.0             | Kubernetes main repo, 1.13 branch    | ‚úó                             |
| 10.0 Alpha/Beta | Kubernetes main repo, 1.14 branch    | ‚úó                             |
| 10.0            | Kubernetes main repo, 1.14 branch    | ‚úó                             |
| 11.0 Alpha/Beta | Kubernetes main repo, 1.15 branch    | ‚úó                             |
| 11.0            | Kubernetes main repo, 1.15 branch    | ‚úó                             |
| 12.0 Alpha/Beta | Kubernetes main repo, 1.16 branch    | ‚úó                             |
| 12.0            | Kubernetes main repo, 1.16 branch    | ‚úó                             |
| 17.0 Alpha/Beta | Kubernetes main repo, 1.17 branch    | ‚úó                             |
| 17.0            | Kubernetes main repo, 1.17 branch    | ‚úó                             |
| 18.0 Alpha/Beta | Kubernetes main repo, 1.18 branch    | ‚úó                             |
| 18.0            | Kubernetes main repo, 1.18 branch    | ‚úó                             |
| 19.0 Alpha/Beta | Kubernetes main repo, 1.19 branch    | ‚úó                             |
| 19.0            | Kubernetes main repo, 1.19 branch    | ‚úó                             |
| 20.0 Alpha/Beta | Kubernetes main repo, 1.20 branch    | ‚úó                             |
| 20.0            | Kubernetes main repo, 1.20 branch    | ‚úó                             |
| 21.0 Alpha/Beta | Kubernetes main repo, 1.21 branch    | ‚úó                             |
| 21.0            | Kubernetes main repo, 1.21 branch    | ‚úó                             |
| 22.0 Alpha/Beta | Kubernetes main repo, 1.22 branch    | ‚úó                             |
| 22.0            | Kubernetes main repo, 1.22 branch    | ‚úó                             |
| 23.0 Alpha/Beta | Kubernetes main repo, 1.23 branch    | ‚úó                             |
| 23.0            | Kubernetes main repo, 1.23 branch    | ‚úó                             |
| 24.0 Alpha/Beta | Kubernetes main repo, 1.24 branch    | ‚úó                             |
| 24.0            | Kubernetes main repo, 1.24 branch    | ‚úó                             |
| 25.0 Alpha/Beta | Kubernetes main repo, 1.25 branch    | ‚úó                             |
| 25.0            | Kubernetes main repo, 1.25 branch    | ‚úó                             |
| 26.0 Alpha/Beta | Kubernetes main repo, 1.26 branch    | ‚úó                             |
| 26.0            | Kubernetes main repo, 1.26 branch    | ‚úó                             |
| 27.0 Alpha/Beta | Kubernetes main repo, 1.27 branch    | ‚úó                             |
| 27.0            | Kubernetes main repo, 1.27 branch    | ‚úó                             |
| 28.0 Alpha/Beta | Kubernetes main repo, 1.28 branch    | ‚úó                             |
| 28.0            | Kubernetes main repo, 1.28 branch    | ‚úó                             |
| 29.0 Alpha/Beta | Kubernetes main repo, 1.29 branch    | ‚úó                             |
| 29.0            | Kubernetes main repo, 1.29 branch    | ‚úó                             |
| 30.0 Alpha/Beta | Kubernetes main repo, 1.30 branch    | ‚úó                             |
| 30.0            | Kubernetes main repo, 1.30 branch    | ‚úì                             |
| 31.0 Alpha/Beta | Kubernetes main repo, 1.31 branch    | ‚úó                             |
| 31.0            | Kubernetes main repo, 1.31 branch    | ‚úì                             |
| 32.0 Alpha/Beta | Kubernetes main repo, 1.32 branch    | ‚úó                             |
| 32.1            | Kubernetes main repo, 1.32 branch    | ‚úì                             |
| 33.0 Alpha/Beta | Kubernetes main repo, 1.33 branch    | ‚úì                             |

&gt; See [here](#homogenizing-the-kubernetes-python-client-versions) for an explanation of why there is no v13-v16 release.

Key:

* `‚úì` Changes in main Kubernetes repo are manually ([should be automated](https://github.com/kubernetes-client/python/issues/177)) published to client-python when they are available.
* `‚úó` No longer maintained; please upgrade.

Kubernetes supports [three minor releases](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md#supported-releases-and-component-skew) at a time. &quot;Support&quot; means we expect users to be running that version in production, though we may not port fixes back before the latest minor version. For example, when v1.3 comes out, v1.0 will no longer be supported. In consistent with Kubernetes support policy, we expect to support **three GA major releases** (corresponding to three Kubernetes minor releases) at a time.

Note: There would be no maintenance for alpha/beta releases except the latest one.

**Exception to the above support rule:** Since we are running behind on releases, we will support Alpha/Beta releases for a greater number of clients until we catch up with the upstream version.

## Homogenizing the Kubernetes Python Client versions

The client releases v12 and before following a versioning schema where the major version was 4 integer positions behind the Kubernetes minor on which the client is based on. For example, v12.0.0 is based on Kubernetes v1.16, v11.0.0 is based on Kubernetes v1.15 and so on.

This created a lot of confusion tracking two different version numbers for each client release. It was decided to homogenize the version scheme starting from the Kubernetes Python client based on Kubernetes v1.17. The versioning scheme of the client from this release would be vY.Z.P where Y and Z are the Kubernetes minor and patch release numbers from Kubernets v1.Y.Z and P is the client specific patch release numbers to accommodate changes and fixes done specifically to the client. For more details, refer [this issue](https://github.com/kubernetes-client/python/issues/1244).

## Community, Support, Discussion

If you have any problem on using the package or any suggestions, please start with reaching the [Kubernetes clients slack channel](https://kubernetes.slack.com/messages/C76GB48RK/), or filing an [issue](https://github.com/kubernetes-client/python/issues) to let us know. You can also reach the maintainers of this project at [SIG API Machinery](https://github.com/kubernetes/community/tree/master/sig-api-machinery), where this project falls under.

### Code of Conduct

Participation in the Kubernetes community is governed by the [CNCF Code of Conduct](https://github.com/cncf/foundation/blob/master/code-of-conduct.md).

## Troubleshooting

### SSLError on macOS

If you get an SSLError, you likely need to update your version of python. The
version that ships with macOS may not be supported.

Install the latest version of python with [brew](https://brew.sh/):

```
brew install python
```

Once installed, you can query the version of OpenSSL like so:

```
python -c &quot;import ssl; print (ssl.OPENSSL_VERSION)&quot;
```

You&#039;ll need a version with OpenSSL version 1.0.0 or later.

### Hostname doesn&#039;t match

If you get an `ssl.CertificateError` complaining about hostname match, your installed packages does not meet version [requirements](requirements.txt).
Specifically check `ipaddress` and `urllib3` package versions to make sure they met requirements in [requirements.txt](requirements.txt) file.

### Why Exec/Attach calls doesn&#039;t work

Starting from 4.0 release, we do not support directly calling exec or attach calls. you should use stream module to call them. so instead
of `resp = api.connect_get_namespaced_pod_exec(name, ...` you should call `resp = stream(api.connect_get_namespaced_pod_exec, name, ...`.

Using Stream will overwrite the requests protocol in _core_v1_api.CoreV1Api()_
This will cause a failure in  non-exec/attach calls. If you reuse your api client object, you will need to
recreate it between api calls that use _stream_ and other api calls.

See more at [exec example](examples/pod_exec.py).

## Enabling Debug Logging

To enable debug logging in the Kubernetes Python client, follow these steps:

### 1. Import the `logging` module

```python
import logging

# Set the logging level to DEBUG
logging.basicConfig(level=logging.DEBUG)

# To see the HTTP requests and responses sent to the Kubernetes API (for debugging network-related issues), configure the urllib3 logger:
logging.getLogger(&quot;urllib3&quot;).setLevel(logging.DEBUG)
```

**[‚¨Ü back to top](#Installation)**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[volcengine/verl]]></title>
            <link>https://github.com/volcengine/verl</link>
            <guid>https://github.com/volcengine/verl</guid>
            <pubDate>Thu, 29 May 2025 00:04:18 GMT</pubDate>
            <description><![CDATA[verl: Volcano Engine Reinforcement Learning for LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/volcengine/verl">volcengine/verl</a></h1>
            <p>verl: Volcano Engine Reinforcement Learning for LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 8,629</p>
            <p>Forks: 1,079</p>
            <p>Stars today: 73 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
 üëã Hi, everyone! 
    verl is a RL training library initiated by &lt;b&gt;ByteDance Seed team&lt;/b&gt; and maintained by the verl community.
    &lt;br&gt;
    &lt;br&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

[&lt;img src=&quot;https://devin.ai/assets/deepwiki-badge.png&quot; alt=&quot;Ask DeepWiki.com&quot; height=&quot;20&quot;/&gt;](https://deepwiki.com/volcengine/verl)
[![GitHub Repo stars](https://img.shields.io/github/stars/volcengine/verl)](https://github.com/volcengine/verl/stargazers)
[![Twitter](https://img.shields.io/twitter/follow/verl_project)](https://twitter.com/verl_project)
&lt;a href=&quot;https://join.slack.com/t/verlgroup/shared_invite/zt-2w5p9o4c3-yy0x2Q56s_VlGLsJ93A6vA&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Slack-verl-blueviolet?logo=slack&amp;amp&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://arxiv.org/pdf/2409.19256&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=EuroSys&amp;message=Paper&amp;color=red&quot;&gt;&lt;/a&gt;
[![Documentation](https://img.shields.io/badge/documentation-blue)](https://verl.readthedocs.io/en/latest/)
&lt;a href=&quot;https://raw.githubusercontent.com/eric-haibin-lin/verl-community/refs/heads/main/WeChat.JPG&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ÂæÆ‰ø°-green?logo=wechat&amp;amp&quot;&gt;&lt;/a&gt;

&lt;/div&gt;

![seed logo](https://github.com/user-attachments/assets/c42e675e-497c-4508-8bb9-093ad4d1f216)

&lt;h1 style=&quot;text-align: center;&quot;&gt;verl: Volcano Engine Reinforcement Learning for LLMs&lt;/h1&gt;

verl is a flexible, efficient and production-ready RL training library for large language models (LLMs).

verl is the open-source version of **[HybridFlow: A Flexible and Efficient RLHF Framework](https://arxiv.org/abs/2409.19256v2)** paper.

verl is flexible and easy to use with:

- **Easy extension of diverse RL algorithms**: The hybrid-controller programming model enables flexible representation and efficient execution of complex Post-Training dataflows. Build RL dataflows such as GRPO, PPO in a few lines of code.

- **Seamless integration of existing LLM infra with modular APIs**: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as FSDP, Megatron-LM, vLLM, SGLang, etc

- **Flexible device mapping**: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.

- Ready integration with popular HuggingFace models

verl is fast with:

- **State-of-the-art throughput**: SOTA LLM training and inference engine integrations and SOTA RL throughput.

- **Efficient actor model resharding with 3D-HybridEngine**: Eliminates memory redundancy and significantly reduces communication overhead during transitions between training and generation phases.

&lt;/p&gt;

## News

- [2025/06] verl team will provide latest project updates at [PyTorch Day China](https://www.lfasiallc.com/pytorch-day-china/) on June 7th. Meet our dev team in Beijing!
- [2025/05] [PF-PPO](https://arxiv.org/abs/2409.06957), accepted to ICML 2025, is now supported in verl! PF-PPO enhances policy learning efficiency and robustness by filtering potentially noisy reward signals and reusing high-quality experiences via a replay buffer.
- [2025/04] We will give a tutorial about latest post-training techniques and programming guide for verl at [ICLR 2025 Expo](https://iclr.cc/virtual/2025/calendar?filter_events=Expo+Talk+Panel&amp;filter_rooms=), [SCI-FM workshop](https://open-foundation-model.github.io/) and [LMSys afterparty](https://lu.ma/d23nyynm). Talk materials available [here](https://github.com/eric-haibin-lin/verl-community/tree/main/iclr25).
- [2025/04] [Seed-Thinking-v1.5](https://github.com/ByteDance-Seed/Seed-Thinking-v1.5/blob/main/seed-thinking-v1.5.pdf) tech report is released! Trained with verl, Seed-Thinking-v1.5 achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on GPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond reasoning tasks, the method demonstrates notable generalization across diverse domains.
- [2025/04] We are working on open source recipe for [VAPO](https://arxiv.org/pdf/2504.05118) (value-based augmented PPO), our latest RL method for reasoning models. Trained from Qwen-32B-base model, VAPO achieves 60.4 on AIME 2024, outperforming DeepSeek-zero-32B and DAPO-32B.
- [2025/03] verl v0.3.0.post1 is released! See [release note](https://github.com/volcengine/verl/releases/) for details. It achieves [~1.4x speedup](https://tongyx361.github.io/blogs/posts/verl-intro/#/verl-flexible-and-efficient-rl-for-llms) compared to prev versions.
- [2025/03] [DAPO](https://dapo-sia.github.io/) is the open-sourced SOTA RL algorithm that achieves 50 points on AIME 2024 based on the Qwen2.5-32B pre-trained model, surpassing the previous SOTA achieved by DeepSeek&#039;s GRPO (DeepSeek-R1-Zero-Qwen-32B). DAPO&#039;s training is fully powered by verl and the reproduction code is available in `recipe/dapo` now.
&lt;details&gt;&lt;summary&gt; more... &lt;/summary&gt;
&lt;ul&gt;
  &lt;li&gt;[2025/05] verl will be presented at [A2M Shanghai](https://a2m.msup.com.cn/home/?aid=4488&amp;city=shanghai) on 5/16 - 5/17.&lt;/li&gt;
  &lt;li&gt;[2025/05] verl will be presented at [GOSIM x PyTorch Day 2025](https://paris2025.gosim.org/). See you in Paris! &lt;/li&gt;
  &lt;li&gt;[2025/03] We introduced the programming model of verl at the [vLLM Beijing Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg) and [verl intro and updates](https://github.com/eric-haibin-lin/verl-community/blob/main/slides/verl-lmsys-meetup.pdf) at the [SGLang-LMSYS Org Meetup](https://lu.ma/ntjrr7ig) in Sunnyvale mid-March.&lt;/li&gt;
  &lt;li&gt;[2025/02] verl v0.2.0.post2 is released!&lt;/li&gt;
  &lt;li&gt;[2025/01] [Doubao-1.5-pro](https://team.doubao.com/zh/special/doubao_1_5_pro) is released with SOTA-level performance on LLM &amp; VLM. The RL scaling preview model is trained using verl, reaching OpenAI O1-level performance on math benchmarks (70.0 pass@1 on AIME).&lt;/li&gt;
  &lt;li&gt;[2025/03] We will present verl(HybridFlow) at EuroSys 2025. See you in Rotterdam!&lt;/li&gt;
  &lt;li&gt;[2025/02] We presented verl in the &lt;a href=&quot;https://lu.ma/ji7atxux&quot;&gt;Bytedance/NVIDIA/Anyscale Ray Meetup&lt;/a&gt;. See you in San Jose!&lt;/li&gt;
  &lt;li&gt;[2024/12] verl is presented at Ray Forward 2024. Slides available &lt;a href=&quot;https://github.com/eric-haibin-lin/verl-community/blob/main/slides/Ray_Forward_2024_%E5%B7%AB%E9%94%A1%E6%96%8C.pdf&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[2024/10] verl is presented at Ray Summit. &lt;a href=&quot;https://www.youtube.com/watch?v=MrhMcXkXvJU&amp;list=PLzTswPQNepXntmT8jr9WaNfqQ60QwW7-U&amp;index=37&quot;&gt;Youtube video&lt;/a&gt; available.&lt;/li&gt;
  &lt;li&gt;[2024/12] The team presented &lt;a href=&quot;https://neurips.cc/Expo/Conferences/2024/workshop/100677&quot;&gt;Post-training LLMs: From Algorithms to Infrastructure&lt;/a&gt; at NeurIPS 2024. &lt;a href=&quot;https://github.com/eric-haibin-lin/verl-data/tree/neurips&quot;&gt;Slides&lt;/a&gt; and &lt;a href=&quot;https://neurips.cc/Expo/Conferences/2024/workshop/100677&quot;&gt;video&lt;/a&gt; available.&lt;/li&gt;
  &lt;li&gt;[2024/08] HybridFlow (verl) is accepted to EuroSys 2025.&lt;/li&gt;
&lt;/ul&gt;   
&lt;/details&gt;

## Key Features

- **FSDP**, **FSDP2** and **Megatron-LM** for training.
- **vLLM**, **SGLang** and **HF Transformers** for rollout generation.
- Compatible with Hugging Face Transformers and Modelscope Hub: [Qwen-3](https://github.com/volcengine/verl/blob/main/examples/grpo_trainer/run_qwen3-8b.sh), Qwen-2.5, Llama3.1, Gemma2, DeepSeek-LLM, etc
- Supervised fine-tuning.
- Reinforcement learning with [PPO](examples/ppo_trainer/), [GRPO](examples/grpo_trainer/), [ReMax](examples/remax_trainer/), [REINFORCE++](https://verl.readthedocs.io/en/latest/examples/config.html#algorithm), [RLOO](examples/rloo_trainer/), [PRIME](recipe/prime/), [DAPO](recipe/dapo/), [DrGRPO](recipe/drgrpo), etc.
  - Support model-based reward and function-based reward (verifiable reward) for math, [coding](https://github.com/volcengine/verl/tree/main/recipe/dapo), etc
  - Support vision-language models (VLMs) and [multi-modal RL](examples/grpo_trainer/run_qwen2_5_vl-7b.sh)
  - [Multi-turn with tool calling](https://github.com/volcengine/verl/tree/main/examples/sglang_multiturn)
- LLM alignment recipes such as [Self-play preference optimization (SPPO)](https://github.com/volcengine/verl/tree/main/recipe/sppo)
- Flash attention 2, [sequence packing](examples/ppo_trainer/run_qwen2-7b_seq_balance.sh), [sequence parallelism](examples/ppo_trainer/run_deepseek7b_llm_sp2.sh) support via DeepSpeed Ulysses, [LoRA](examples/sft/gsm8k/run_qwen_05_peft.sh), [Liger-kernel](examples/sft/gsm8k/run_qwen_05_sp2_liger.sh).
- Scales up to 70B models and hundreds of GPUs.
- Lora RL support to save memory.
- Experiment tracking with wandb, swanlab, mlflow and tensorboard.

## Upcoming Features and Changes

- Roadmap https://github.com/volcengine/verl/issues/710
- DeepSeek 671b optimizations with Megatron v0.11 https://github.com/volcengine/verl/issues/708
- Multi-turn rollout optimizations https://github.com/volcengine/verl/pull/1037 https://github.com/volcengine/verl/pull/1138
- Environment interactions https://github.com/volcengine/verl/issues/1172
- List of breaking changes since v0.3 https://github.com/volcengine/verl/discussions/943
- Lora for RL https://github.com/volcengine/verl/pull/1127 

## Getting Started

&lt;a href=&quot;https://verl.readthedocs.io/en/latest/index.html&quot;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt;

**Quickstart:**

- [Installation](https://verl.readthedocs.io/en/latest/start/install.html)
- [Quickstart](https://verl.readthedocs.io/en/latest/start/quickstart.html)
- [Programming Guide](https://verl.readthedocs.io/en/latest/hybrid_flow.html)

**Running a PPO example step-by-step:**

- Data and Reward Preparation
  - [Prepare Data for Post-Training](https://verl.readthedocs.io/en/latest/preparation/prepare_data.html)
  - [Implement Reward Function for Dataset](https://verl.readthedocs.io/en/latest/preparation/reward_function.html)
- Understanding the PPO Example
  - [PPO Example Architecture](https://verl.readthedocs.io/en/latest/examples/ppo_code_architecture.html)
  - [Config Explanation](https://verl.readthedocs.io/en/latest/examples/config.html)
  - [Run GSM8K Example](https://verl.readthedocs.io/en/latest/examples/gsm8k_example.html)

**Reproducible algorithm baselines:**

- [PPO, GRPO, ReMax](https://verl.readthedocs.io/en/latest/experiment/ppo.html)

**For code explanation and advance usage (extension):**

- PPO Trainer and Workers
  - [PPO Ray Trainer](https://verl.readthedocs.io/en/latest/workers/ray_trainer.html)
  - [PyTorch FSDP Backend](https://verl.readthedocs.io/en/latest/workers/fsdp_workers.html)
  - [Megatron-LM Backend](https://verl.readthedocs.io/en/latest/index.html)
- Advance Usage and Extension
  - [Multi-turn Rollout Support](https://verl.readthedocs.io/en/latest/sglang_multiturn/multiturn.html)
  - [Ray API design tutorial](https://verl.readthedocs.io/en/latest/advance/placement.html)
  - [Extend to Other RL(HF) algorithms](https://verl.readthedocs.io/en/latest/advance/dpo_extension.html)
  - [Add Models with the FSDP Backend](https://verl.readthedocs.io/en/latest/advance/fsdp_extension.html)
  - [Add Models with the Megatron-LM Backend](https://verl.readthedocs.io/en/latest/advance/megatron_extension.html)
  - [Deployment using Separate GPU Resources](https://github.com/volcengine/verl/tree/main/examples/split_placement)

**Blogs from the community**

- [SGLang, verl, OpenBMB and Tsinghua University: Pioneering End-to-End Multi-Turn RLHF](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/verl-multiturn-rollout-Release.md)
- [Reinforcement Learning from Human Feedback on AMD GPUs with verl and ROCm Integration](https://rocm.blogs.amd.com/artificial-intelligence/verl-large-scale/README.html)
- [veMLP x verl ÔºöÁé©ËΩ¨Âº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉ](https://mp.weixin.qq.com/s/7nbqxk4knMGd-hQE9ls2tA)
- [‰ΩøÁî® verl ËøõË°å GRPO ÂàÜÂ∏ÉÂºèÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÊúÄ‰Ω≥ÂÆûË∑µ](https://www.volcengine.com/docs/6459/1463942)
- [HybridFlow verl ÂéüÊñáÊµÖÊûê](https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/readme.md)
- [ÊúÄÈ´òÊèêÂçá 20 ÂÄçÂêûÂêêÈáèÔºÅË±ÜÂåÖÂ§ßÊ®°ÂûãÂõ¢ÈòüÂèëÂ∏ÉÂÖ®Êñ∞ RLHF Ê°ÜÊû∂ÔºåÁé∞Â∑≤ÂºÄÊ∫êÔºÅ](https://team.doubao.com/en/blog/%E6%9C%80%E9%AB%98%E6%8F%90%E5%8D%8720%E5%80%8D%E5%90%9E%E5%90%90%E9%87%8F-%E8%B1%86%E5%8C%85%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%A2%E9%98%9F%E5%8F%91%E5%B8%83%E5%85%A8%E6%96%B0-rlhf-%E6%A1%86%E6%9E%B6-%E7%8E%B0%E5%B7%B2%E5%BC%80%E6%BA%90)

## Performance Tuning Guide

The performance is essential for on-policy RL algorithm. We have written a detailed [performance tuning guide](https://verl.readthedocs.io/en/latest/perf/perf_tuning.html) to help you optimize performance.

## Upgrade to vLLM &gt;= v0.8.2

verl now supports vLLM&gt;=0.8.2 when using FSDP as the training backend. Please refer to [this document](https://github.com/volcengine/verl/blob/main/docs/README_vllm0.8.md) for the installation guide and more information. Please avoid vllm 0.7.x, which contains bugs that may lead to OOMs and unexpected errors.

## Use Latest SGLang

SGLang is fully supported with verl, and SGLang RL Group is working extensively on building unique features, including multi-turn agentic RL, VLM RLHF, server-based RL, and partial rollout. Please refer to [this document](https://verl.readthedocs.io/en/latest/workers/sglang_worker.html) for the installation guide and more information.

## Upgrade to FSDP2

verl is fully embracing FSDP2! FSDP2 is recommended by torch distributed team, providing better throughput and memory usage, and is composible with other features (e.g. torch.compile). To enable FSDP2, simply use verl main and set the following options:
```
actor_rollout_ref.ref.strategy=fsdp2
actor_rollout_ref.actor.strategy=fsdp2
critic.strategy=fsdp2 
reward_model.strategy=fsdp2 
```
Furthermore, FSDP2 cpu offloading is compatible with gradient accumulation. You can turn it on to save memory with `actor_rollout_ref.actor.offload_policy=True`. For more details, see https://github.com/volcengine/verl/pull/1026

## [Hardware] Support AMD (ROCm Kernel)

verl now supports FSDP as the training engine (Megatron support coming soon) and both integrates with vLLM and SGLang as inference engines. Please refer to [this document](https://github.com/volcengine/verl/blob/main/docs/amd_tutorial/amd_build_dockerfile_page.rst) for the installation guide and more information, and [this document](https://github.com/volcengine/verl/blob/main/docs/amd_tutorial/amd_vllm_page.rst) for the vLLM performance tuning for ROCm.


## Citation and acknowledgement

If you find the project helpful, please cite:

- [HybridFlow: A Flexible and Efficient RLHF Framework](https://arxiv.org/abs/2409.19256v2)
- [A Framework for Training Large Language Models for Code Generation via Proximal Policy Optimization](https://i.cs.hku.hk/~cwu/papers/gmsheng-NL2Code24.pdf)

```bibtex
@article{sheng2024hybridflow,
  title   = {HybridFlow: A Flexible and Efficient RLHF Framework},
  author  = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2409.19256}
}
```

verl is inspired by the design of Nemo-Aligner, Deepspeed-chat and OpenRLHF. The project is adopted and contributed by Bytedance, Anyscale, LMSys.org, [Alibaba Qwen team](https://github.com/QwenLM/), Shanghai AI Lab, Tsinghua University, UC Berkeley, UCLA, UIUC, University of Hong Kong, ke.com, [All Hands AI](https://www.all-hands.dev/), [ModelBest](http://modelbest.cn/), [OpenPipe](https://openpipe.ai/), JD AI Lab, Microsoft Research, [StepFun](https://www.stepfun.com/), Amazon, Linkedin, Meituan, [Camel-AI](https://www.camel-ai.org/), [OpenManus](https://github.com/OpenManus), Xiaomi, Prime Intellect, NVIDIA research, [Baichuan](https://www.baichuan-ai.com/home), [RedNote](https://www.xiaohongshu.com/), [SwissAI](https://www.swiss-ai.org/), and many more.

## Awesome work using verl

- [TinyZero](https://github.com/Jiayi-Pan/TinyZero): a reproduction of **DeepSeek R1 Zero** recipe for reasoning tasks ![GitHub Repo stars](https://img.shields.io/github/stars/Jiayi-Pan/TinyZero)
- [SkyThought](https://github.com/NovaSky-AI/SkyThought): RL training for Sky-T1-7B by NovaSky AI team. ![GitHub Repo stars](https://img.shields.io/github/stars/NovaSky-AI/SkyThought)
- [simpleRL-reason](https://github.com/hkust-nlp/simpleRL-reason): SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild ![GitHub Repo stars](https://img.shields.io/github/stars/hkust-nlp/simpleRL-reason)
- [Easy-R1](https://github.com/hiyouga/EasyR1): **Multi-modal** RL training framework ![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/EasyR1)
- [OpenManus-RL](https://github.com/OpenManus/OpenManus-RL): LLM Agents RL tunning framework for multiple agent environments. ![GitHub Repo stars](https://img.shields.io/github/stars/OpenManus/OpenManus-RL)
- [rllm](https://github.com/agentica-project/rllm): async RL training with [verl-pipeline](https://github.com/agentica-project/verl-pipeline) ![GitHub Repo stars](https://img.shields.io/github/stars/agentica-project/rllm)
- [PRIME](https://github.com/PRIME-RL/PRIME): Process reinforcement through implicit rewards ![GitHub Repo stars](https://img.shields.io/github/stars/PRIME-RL/PRIME)
- [RAGEN](https://github.com/ZihanWang314/ragen): a general-purpose reasoning **agent** training framework ![GitHub Repo stars](https://img.shields.io/github/stars/ZihanWang314/ragen)
- [Logic-RL](https://github.com/Unakar/Logic-RL): a reproduction of DeepSeek R1 Zero on 2K Tiny Logic Puzzle Dataset. ![GitHub Repo stars](https://img.shields.io/github/stars/Unakar/Logic-RL)
- [Search-R1](https://github.com/PeterGriffinJin/Search-R1): RL with reasoning and **searching (tool-call)** interleaved LLMs ![GitHub Repo stars](https://img.shields.io/github/stars/PeterGriffinJin/Search-R1)
- [DeepRetrieval](https://github.com/pat-jj/DeepRetrieval): RL Training of **Search Agent** with **Search/Retrieval Outcome** ![GitHub Repo stars](https://img.shields.io/github/stars/pat-jj/DeepRetrieval)
- [ReSearch](https://github.com/Agent-RL/ReSearch): Learning to **Re**ason with **Search** for LLMs via Reinforcement Learning ![GitHub Repo stars](https://img.shields.io/github/stars/Agent-RL/ReSearch)
- [Code-R1](https://github.com/ganler/code-r1): Reproducing R1 for **Code** with Reliable Rewards ![GitHub Repo stars](https://img.shields.io/github/stars/ganler/code-r1)
- [Skywork-OR1](https://github.com/SkyworkAI/Skywork-OR1): Skywork open reaonser series ![GitHub Repo stars](https://img.shields.io/github/stars/SkyworkAI/Skywork-OR1)
- [ToRL](https://github.com/GAIR-NLP/ToRL): Scaling tool-integrated RL ![GitHub Repo stars](https://img.shields.io/github/stars/GAIR-NLP/ToRL)
- [verl-agent](https://github.com/langfengQ/verl-agent): A scalable training framework for **long-horizon LLM/VLM agents**, along with a new algorithm **GiGPO** ![GitHub Repo stars](https://img.shields.io/github/stars/langfengQ/verl-agent)
- [PF-PPO](https://arxiv.org/abs/2409.06957): Policy Filtration for PPO based on the reliability of reward signals for more efficient and robust RLHF.
- [GUI-R1](https://github.com/ritzz-ai/GUI-R1): **GUI-R1**: A Generalist R1-style Vision-Language Action Model For **GUI Agents** ![GitHub Repo stars](https://img.shields.io/github/stars/ritzz-ai/GUI-R1)
- [DeepResearcher](https://github.com/GAIR-NLP/DeepResearcher): Scaling deep research via reinforcement learning in real-world environments ![GitHub Repo stars](https://img.shields.io/github/stars/GAIR-NLP/DeepResearcher)
- [VAGEN](https://github.com/RAGEN-AI/VAGEN): Training VLM agents with multi-turn reinforcement learning ![GitHub Repo stars](https://img.shields.io/github/stars/RAGEN-AI/VAGEN)
- [ReTool](https://retool-rl.github.io/): ReTool: reinforcement learning for strategic tool use in LLMs
- [Seed-Coder](https://github.com/ByteDance-Seed/Seed-Coder): RL training of Seed-Coder boosts performance on competitive programming ![GitHub Repo stars](https://img.shields.io/github/stars/ByteDance-Seed/Seed-Coder)
- [all-hands/openhands-lm-32b-v0.1](https://www.all-hands.dev/blog/introducing-openhands-lm-32b----a-strong-open-coding-agent-model): A str

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[vllm-project/vllm]]></title>
            <link>https://github.com/vllm-project/vllm</link>
            <guid>https://github.com/vllm-project/vllm</guid>
            <pubDate>Thu, 29 May 2025 00:04:17 GMT</pubDate>
            <description><![CDATA[A high-throughput and memory-efficient inference and serving engine for LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/vllm-project/vllm">vllm-project/vllm</a></h1>
            <p>A high-throughput and memory-efficient inference and serving engine for LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 48,423</p>
            <p>Forks: 7,649</p>
            <p>Stars today: 199 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-dark.png&quot;&gt;
    &lt;img alt=&quot;vLLM&quot; src=&quot;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-light.png&quot; width=55%&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;h3 align=&quot;center&quot;&gt;
Easy, fast, and cheap LLM serving for everyone
&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
| &lt;a href=&quot;https://docs.vllm.ai&quot;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://blog.vllm.ai/&quot;&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://arxiv.org/abs/2309.06180&quot;&gt;&lt;b&gt;Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://x.com/vllm_project&quot;&gt;&lt;b&gt;Twitter/X&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://discuss.vllm.ai&quot;&gt;&lt;b&gt;User Forum&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://slack.vllm.ai&quot;&gt;&lt;b&gt;Developer Slack&lt;/b&gt;&lt;/a&gt; |
&lt;/p&gt;

---

*Latest News* üî•
- [2025/05] We hosted [NYC vLLM Meetup](https://lu.ma/c1rqyf1f)! Please find the meetup slides [here](https://docs.google.com/presentation/d/1_q_aW_ioMJWUImf1s1YM-ZhjXz8cUeL0IJvaquOYBeA/edit?usp=sharing).
- [2025/05] vLLM is now a hosted project under PyTorch Foundation! Please find the announcement [here](https://pytorch.org/blog/pytorch-foundation-welcomes-vllm/).
- [2025/04] We hosted [Asia Developer Day](https://www.sginnovate.com/event/limited-availability-morning-evening-slots-remaining-inaugural-vllm-asia-developer-day)! Please find the meetup slides from the vLLM team [here](https://docs.google.com/presentation/d/19cp6Qu8u48ihB91A064XfaXruNYiBOUKrBxAmDOllOo/edit?usp=sharing).
- [2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post [here](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html).

&lt;details&gt;
&lt;summary&gt;Previous News&lt;/summary&gt;

- [2025/03] We hosted [vLLM x Ollama Inference Night](https://lu.ma/vllm-ollama)! Please find the meetup slides from the vLLM team [here](https://docs.google.com/presentation/d/16T2PDD1YwRnZ4Tu8Q5r6n53c5Lr5c73UV9Vd2_eBo4U/edit?usp=sharing).
- [2025/03] We hosted [the first vLLM China Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg)! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1REHvfQMKGnvz6p3Fd23HhSO4c8j5WPGZV0bKYLwnHyQ/edit?usp=sharing).
- [2025/03] We hosted [the East Coast vLLM Meetup](https://lu.ma/7mu4k4xx)! Please find the meetup slides [here](https://docs.google.com/presentation/d/1NHiv8EUFF1NLd3fEYODm56nDmL26lEeXCaDgyDlTsRs/edit#slide=id.g31441846c39_0_0).
- [2025/02] We hosted [the ninth vLLM meetup](https://lu.ma/h7g3kuj9) with Meta! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1jzC_PZVXrVNSFVCW-V4cFXb6pn7zZ2CyP_Flwo05aqg/edit?usp=sharing) and AMD [here](https://drive.google.com/file/d/1Zk5qEJIkTmlQ2eQcXQZlljAx3m9s7nwn/view?usp=sharing). The slides from Meta will not be posted.
- [2025/01] We hosted [the eighth vLLM meetup](https://lu.ma/zep56hui) with Google Cloud! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing), and Google Cloud team [here](https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing).
- [2024/12] vLLM joins [pytorch ecosystem](https://pytorch.org/blog/vllm-joins-pytorch)! Easy, Fast, and Cheap LLM Serving for Everyone!
- [2024/11] We hosted [the seventh vLLM meetup](https://lu.ma/h0qvrajz) with Snowflake! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing), and Snowflake team [here](https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing).
- [2024/10] We have just created a developer slack ([slack.vllm.ai](https://slack.vllm.ai)) focusing on coordinating contributions and discussing features. Please feel free to join us there!
- [2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team [here](https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing). Learn more from the [talks](https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR) from other vLLM contributors and users!
- [2024/09] We hosted [the sixth vLLM meetup](https://lu.ma/87q3nvnh) with NVIDIA! Please find the meetup slides [here](https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing).
- [2024/07] We hosted [the fifth vLLM meetup](https://lu.ma/lp0gyjqr) with AWS! Please find the meetup slides [here](https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing).
- [2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post [here](https://blog.vllm.ai/2024/07/23/llama31.html).
- [2024/06] We hosted [the fourth vLLM meetup](https://lu.ma/agivllm) with Cloudflare and BentoML! Please find the meetup slides [here](https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing).
- [2024/04] We hosted [the third vLLM meetup](https://robloxandvllmmeetup2024.splashthat.com/) with Roblox! Please find the meetup slides [here](https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing).
- [2024/01] We hosted [the second vLLM meetup](https://lu.ma/ygxbpzhl) with IBM! Please find the meetup slides [here](https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing).
- [2023/10] We hosted [the first vLLM meetup](https://lu.ma/first-vllm-meetup) with a16z! Please find the meetup slides [here](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing).
- [2023/08] We would like to express our sincere gratitude to [Andreessen Horowitz](https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/) (a16z) for providing a generous grant to support the open-source development and research of vLLM.
- [2023/06] We officially released vLLM! FastChat-vLLM integration has powered [LMSYS Vicuna and Chatbot Arena](https://chat.lmsys.org) since mid-April. Check out our [blog post](https://vllm.ai).

&lt;/details&gt;

---
## About

vLLM is a fast and easy-to-use library for LLM inference and serving.

Originally developed in the [Sky Computing Lab](https://sky.cs.berkeley.edu) at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.

vLLM is fast with:

- State-of-the-art serving throughput
- Efficient management of attention key and value memory with [**PagedAttention**](https://blog.vllm.ai/2023/06/20/vllm.html)
- Continuous batching of incoming requests
- Fast model execution with CUDA/HIP graph
- Quantizations: [GPTQ](https://arxiv.org/abs/2210.17323), [AWQ](https://arxiv.org/abs/2306.00978), [AutoRound](https://arxiv.org/abs/2309.05516),INT4, INT8, and FP8.
- Optimized CUDA kernels, including integration with FlashAttention and FlashInfer.
- Speculative decoding
- Chunked prefill

**Performance benchmark**: We include a performance benchmark at the end of [our blog post](https://blog.vllm.ai/2024/09/05/perf-update.html). It compares the performance of vLLM against other LLM serving engines ([TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM), [SGLang](https://github.com/sgl-project/sglang) and [LMDeploy](https://github.com/InternLM/lmdeploy)). The implementation is under [nightly-benchmarks folder](.buildkite/nightly-benchmarks/) and you can [reproduce](https://github.com/vllm-project/vllm/issues/8176) this benchmark using our one-click runnable script.

vLLM is flexible and easy to use with:

- Seamless integration with popular Hugging Face models
- High-throughput serving with various decoding algorithms, including *parallel sampling*, *beam search*, and more
- Tensor parallelism and pipeline parallelism support for distributed inference
- Streaming outputs
- OpenAI-compatible API server
- Support NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, TPU, and AWS Neuron.
- Prefix caching support
- Multi-LoRA support

vLLM seamlessly supports most popular open-source models on HuggingFace, including:
- Transformer-like LLMs (e.g., Llama)
- Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)
- Embedding Models (e.g. E5-Mistral)
- Multi-modal LLMs (e.g., LLaVA)

Find the full list of supported models [here](https://docs.vllm.ai/en/latest/models/supported_models.html).

## Getting Started

Install vLLM with `pip` or [from source](https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source):

```bash
pip install vllm
```

Visit our [documentation](https://docs.vllm.ai/en/latest/) to learn more.
- [Installation](https://docs.vllm.ai/en/latest/getting_started/installation.html)
- [Quickstart](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)
- [List of Supported Models](https://docs.vllm.ai/en/latest/models/supported_models.html)

## Contributing

We welcome and value any contributions and collaborations.
Please check out [Contributing to vLLM](https://docs.vllm.ai/en/latest/contributing/index.html) for how to get involved.

## Sponsors

vLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!

&lt;!-- Note: Please sort them in alphabetical order. --&gt;
&lt;!-- Note: Please keep these consistent with docs/community/sponsors.md --&gt;
Cash Donations:
- a16z
- Dropbox
- Sequoia Capital
- Skywork AI
- ZhenFund

Compute Resources:
- AMD
- Anyscale
- AWS
- Crusoe Cloud
- Databricks
- DeepInfra
- Google Cloud
- Intel
- Lambda Lab
- Nebius
- Novita AI
- NVIDIA
- Replicate
- Roblox
- RunPod
- Trainy
- UC Berkeley
- UC San Diego

Slack Sponsor: Anyscale

We also have an official fundraising venue through [OpenCollective](https://opencollective.com/vllm). We plan to use the fund to support the development, maintenance, and adoption of vLLM.

## Citation

If you use vLLM for your research, please cite our [paper](https://arxiv.org/abs/2309.06180):

```bibtex
@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}
```

## Contact Us

- For technical questions and feature requests, please use GitHub [Issues](https://github.com/vllm-project/vllm/issues) or [Discussions](https://github.com/vllm-project/vllm/discussions)
- For discussing with fellow users, please use the [vLLM Forum](https://discuss.vllm.ai)
- coordinating contributions and development, please use [Slack](https://slack.vllm.ai)
- For security disclosures, please use GitHub&#039;s [Security Advisories](https://github.com/vllm-project/vllm/security/advisories) feature
- For collaborations and partnerships, please contact us at [vllm-questions@lists.berkeley.edu](mailto:vllm-questions@lists.berkeley.edu)

## Media Kit

- If you wish to use vLLM&#039;s logo, please refer to [our media kit repo](https://github.com/vllm-project/media-kit).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Lightricks/LTX-Video]]></title>
            <link>https://github.com/Lightricks/LTX-Video</link>
            <guid>https://github.com/Lightricks/LTX-Video</guid>
            <pubDate>Thu, 29 May 2025 00:04:16 GMT</pubDate>
            <description><![CDATA[Official repository for LTX-Video]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Lightricks/LTX-Video">Lightricks/LTX-Video</a></h1>
            <p>Official repository for LTX-Video</p>
            <p>Language: Python</p>
            <p>Stars: 6,177</p>
            <p>Forks: 504</p>
            <p>Stars today: 69 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# LTX-Video

This is the official repository for LTX-Video.

[Website](https://www.lightricks.com/ltxv) |
[Model](https://huggingface.co/Lightricks/LTX-Video) |
[Demo](https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b) |
[Paper](https://arxiv.org/abs/2501.00103) |
[Trainer](https://github.com/Lightricks/LTX-Video-Trainer) |
[Discord](https://discord.gg/Mn8BRgUKKy)

&lt;/div&gt;

## Table of Contents

- [Introduction](#introduction)
- [What&#039;s new](#news)
- [Models &amp; Workflows](#models--workflows)
- [Quick Start Guide](#quick-start-guide)
  - [Use online](#online-inference)
  - [Run locally](#run-locally)
    - [Installation](#installation)
    - [Inference](#inference)
  - [ComfyUI Integration](#comfyui-integration)
  - [Diffusers Integration](#diffusers-integration)
- [Model User Guide](#model-user-guide)
- [Community Contribution](#community-contribution)
- [Training](#‚ö°Ô∏è-training)
- [Join Us!](#üöÄ-join-us)
- [Acknowledgement](#acknowledgement)

# Introduction

LTX-Video is the first DiT-based video generation model that can generate high-quality videos in *real-time*.
It can generate 30 FPS videos at 1216√ó704 resolution, faster than it takes to watch them.
The model is trained on a large-scale dataset of diverse videos and can generate high-resolution videos
with realistic and diverse content.

The model supports text-to-image, image-to-video, keyframe-based animation, video extension (both forward and backward), video-to-video transformations, and any combination of these features.

### Image to video examples
| | | |
|:---:|:---:|:---:|
| ![example1](./docs/_static/ltx-video_i2v_example_00001.gif) | ![example2](./docs/_static/ltx-video_i2v_example_00002.gif) | ![example3](./docs/_static/ltx-video_i2v_example_00003.gif) |
| ![example4](./docs/_static/ltx-video_i2v_example_00004.gif) | ![example5](./docs/_static/ltx-video_i2v_example_00005.gif) |  ![example6](./docs/_static/ltx-video_i2v_example_00006.gif) |
| ![example7](./docs/_static/ltx-video_i2v_example_00007.gif) |  ![example8](./docs/_static/ltx-video_i2v_example_00008.gif) | ![example9](./docs/_static/ltx-video_i2v_example_00009.gif) |


### Text to video examples
| | | |
|:---:|:---:|:---:|
| ![example1](./docs/_static/ltx-video_example_00001.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A woman with long brown hair and light skin smiles at another woman...&lt;/summary&gt;A woman with long brown hair and light skin smiles at another woman with long blonde hair. The woman with brown hair wears a black jacket and has a small, barely noticeable mole on her right cheek. The camera angle is a close-up, focused on the woman with brown hair&#039;s face. The lighting is warm and natural, likely from the setting sun, casting a soft glow on the scene. The scene appears to be real-life footage.&lt;/details&gt; | ![example10](./docs/_static/ltx-video_example_00010.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A clear, turquoise river flows through a rocky canyon...&lt;/summary&gt;A clear, turquoise river flows through a rocky canyon, cascading over a small waterfall and forming a pool of water at the bottom.The river is the main focus of the scene, with its clear water reflecting the surrounding trees and rocks. The canyon walls are steep and rocky, with some vegetation growing on them. The trees are mostly pine trees, with their green needles contrasting with the brown and gray rocks. The overall tone of the scene is one of peace and tranquility.&lt;/details&gt; | ![example3](./docs/_static/ltx-video_example_00015.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;Two police officers in dark blue uniforms and matching hats...&lt;/summary&gt;Two police officers in dark blue uniforms and matching hats enter a dimly lit room through a doorway on the left side of the frame. The first officer, with short brown hair and a mustache, steps inside first, followed by his partner, who has a shaved head and a goatee. Both officers have serious expressions and maintain a steady pace as they move deeper into the room. The camera remains stationary, capturing them from a slightly low angle as they enter. The room has exposed brick walls and a corrugated metal ceiling, with a barred window visible in the background. The lighting is low-key, casting shadows on the officers&#039; faces and emphasizing the grim atmosphere. The scene appears to be from a film or television show.&lt;/details&gt; |
| ![example5](./docs/_static/ltx-video_example_00005.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A woman with light skin, wearing a blue jacket and a black hat...&lt;/summary&gt;A woman with light skin, wearing a blue jacket and a black hat with a veil, looks down and to her right, then back up as she speaks; she has brown hair styled in an updo, light brown eyebrows, and is wearing a white collared shirt under her jacket; the camera remains stationary on her face as she speaks; the background is out of focus, but shows trees and people in period clothing; the scene is captured in real-life footage.&lt;/details&gt; | ![example6](./docs/_static/ltx-video_example_00006.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A man in a dimly lit room talks on a vintage telephone...&lt;/summary&gt;A man in a dimly lit room talks on a vintage telephone, hangs up, and looks down with a sad expression. He holds the black rotary phone to his right ear with his right hand, his left hand holding a rocks glass with amber liquid. He wears a brown suit jacket over a white shirt, and a gold ring on his left ring finger. His short hair is neatly combed, and he has light skin with visible wrinkles around his eyes. The camera remains stationary, focused on his face and upper body. The room is dark, lit only by a warm light source off-screen to the left, casting shadows on the wall behind him. The scene appears to be from a movie.&lt;/details&gt; | ![example7](./docs/_static/ltx-video_example_00007.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A prison guard unlocks and opens a cell door...&lt;/summary&gt;A prison guard unlocks and opens a cell door to reveal a young man sitting at a table with a woman. The guard, wearing a dark blue uniform with a badge on his left chest, unlocks the cell door with a key held in his right hand and pulls it open; he has short brown hair, light skin, and a neutral expression. The young man, wearing a black and white striped shirt, sits at a table covered with a white tablecloth, facing the woman; he has short brown hair, light skin, and a neutral expression. The woman, wearing a dark blue shirt, sits opposite the young man, her face turned towards him; she has short blonde hair and light skin. The camera remains stationary, capturing the scene from a medium distance, positioned slightly to the right of the guard. The room is dimly lit, with a single light fixture illuminating the table and the two figures. The walls are made of large, grey concrete blocks, and a metal door is visible in the background. The scene is captured in real-life footage.&lt;/details&gt; |
| ![example2](./docs/_static/ltx-video_example_00014.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A man walks towards a window, looks out, and then turns around...&lt;/summary&gt;A man walks towards a window, looks out, and then turns around. He has short, dark hair, dark skin, and is wearing a brown coat over a red and gray scarf. He walks from left to right towards a window, his gaze fixed on something outside. The camera follows him from behind at a medium distance. The room is brightly lit, with white walls and a large window covered by a white curtain. As he approaches the window, he turns his head slightly to the left, then back to the right. He then turns his entire body to the right, facing the window. The camera remains stationary as he stands in front of the window. The scene is captured in real-life footage.&lt;/details&gt; | ![example13](./docs/_static/ltx-video_example_00013.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;The camera pans across a cityscape of tall buildings...&lt;/summary&gt;The camera pans across a cityscape of tall buildings with a circular building in the center. The camera moves from left to right, showing the tops of the buildings and the circular building in the center. The buildings are various shades of gray and white, and the circular building has a green roof. The camera angle is high, looking down at the city. The lighting is bright, with the sun shining from the upper left, casting shadows from the buildings. The scene is computer-generated imagery.&lt;/details&gt; | ![example11](./docs/_static/ltx-video_example_00011.gif)&lt;br&gt;&lt;details style=&quot;max-width: 300px; margin: auto;&quot;&gt;&lt;summary&gt;A man in a suit enters a room and speaks to two women...&lt;/summary&gt;A man in a suit enters a room and speaks to two women sitting on a couch. The man, wearing a dark suit with a gold tie, enters the room from the left and walks towards the center of the frame. He has short gray hair, light skin, and a serious expression. He places his right hand on the back of a chair as he approaches the couch. Two women are seated on a light-colored couch in the background. The woman on the left wears a light blue sweater and has short blonde hair. The woman on the right wears a white sweater and has short blonde hair. The camera remains stationary, focusing on the man as he enters the room. The room is brightly lit, with warm tones reflecting off the walls and furniture. The scene appears to be from a film or television show.&lt;/details&gt; |

# News

## May, 14th, 2025: New distilled model 13B v0.9.7:
- Release a new 13B distilled model [ltxv-13b-0.9.7-distilled](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled.safetensors)
    * Amazing for iterative work - generates HD videos in 10 seconds, with low-res preview after just 3 seconds (on H100)!
    * Does not require classifier-free guidance and spatio-temporal guidance.
    * Supports sampling with 8 (recommended), or less diffusion steps.
    * Also released a LoRA version of the distilled model, [ltxv-13b-0.9.7-distilled-lora128](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-lora128.safetensors)
        * Requires only 1GB of VRAM
        * Can be used with the full 13B model for fast inference
- Release a new quantized distilled model [ltxv-13b-0.9.7-distilled-fp8](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-fp8.safetensors) for *real-time* generation (on H100) with even less VRAM (Supported in the [official ComfyUI workflow](https://github.com/Lightricks/ComfyUI-LTXVideo/))

## May, 5th, 2025: New model 13B v0.9.7:
- Release a new 13B model [ltxv-13b-0.9.7-dev](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev.safetensors)
- Release a new quantized model [ltxv-13b-0.9.7-dev-fp8](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev-fp8.safetensors) for faster inference with less VRAM (Supported in the [official ComfyUI workflow](https://github.com/Lightricks/ComfyUI-LTXVideo/))
- Release a new upscalers
  * [ltxv-temporal-upscaler-0.9.7](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-temporal-upscaler-0.9.7.safetensors)
  * [ltxv-spatial-upscaler-0.9.7](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-spatial-upscaler-0.9.7.safetensors)
- Breakthrough prompt adherence and physical understanding.
- New Pipeline for multi-scale video rendering for fast and high quality results


## April, 15th, 2025: New checkpoints v0.9.6:
- Release a new checkpoint [ltxv-2b-0.9.6-dev-04-25](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-dev-04-25.safetensors) with improved quality
- Release a new distilled model [ltxv-2b-0.9.6-distilled-04-25](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-distilled-04-25.safetensors)
    * 15x faster inference than non-distilled model.
    * Does not require classifier-free guidance and spatio-temporal guidance.
    * Supports sampling with 8 (recommended), or less diffusion steps.
- Improved prompt adherence, motion quality and fine details.
- New default resolution and FPS: 1216 √ó 704 pixels at 30 FPS
    * Still real time on H100 with the distilled model.
    * Other resolutions and FPS are still supported.
- Support stochastic inference (can improve visual quality when using the distilled model)

## March, 5th, 2025: New checkpoint v0.9.5
- New license for commercial use ([OpenRail-M](https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.5.license.txt))
- Release a new checkpoint v0.9.5 with improved quality
- Support keyframes and video extension
- Support higher resolutions
- Improved prompt understanding
- Improved VAE
- New online web app in [LTX-Studio](https://app.ltx.studio/ltx-video)
- Automatic prompt enhancement

## February, 20th, 2025: More inference options
- Improve STG (Spatiotemporal Guidance) for LTX-Video
- Support MPS on macOS with PyTorch 2.3.0
- Add support for 8-bit model, LTX-VideoQ8
- Add TeaCache for LTX-Video
- Add [ComfyUI-LTXTricks](#comfyui-integration)
- Add Diffusion-Pipe

## December 31st, 2024: Research paper
- Release the [research paper](https://arxiv.org/abs/2501.00103)

## December 20th, 2024: New checkpoint v0.9.1
- Release a new checkpoint v0.9.1 with improved quality
- Support for STG / PAG
- Support loading checkpoints of LTX-Video in Diffusers format (conversion is done on-the-fly)
- Support offloading unused parts to CPU
- Support the new timestep-conditioned VAE decoder
- Reference contributions from the community in the readme file
- Relax transformers dependency

## November 21th, 2024: Initial release v0.9.0
- Initial release of LTX-Video
- Support text-to-video and image-to-video generation


# Models &amp; Workflows

| Name                    | Notes                                                                                      | inference.py config                                                                                                                                      | ComfyUI workflow (Recommended) |
|-------------------------|--------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------|
| ltxv-13b-0.9.7-dev                   | Highest quality, requires more VRAM                                                        | [ltxv-13b-0.9.7-dev.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.7-dev.yaml)                                             | [ltxv-13b-i2v-base.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/ltxv-13b-i2v-base.json)             |
| [ltxv-13b-0.9.7-mix](https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b)            | Mix ltxv-13b-dev and ltxv-13b-distilled in the same multi-scale rendering workflow for balanced speed-quality | N/A                                             | [ltxv-13b-i2v-mixed-multiscale.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/ltxv-13b-i2v-mixed-multiscale.json)             |
 [ltxv-13b-0.9.7-distilled](https://app.ltx.studio/motion-workspace?videoModel=ltxv)        | Faster, less VRAM usage, slight quality reduction compared to 13b. Ideal for rapid iterations | [ltxv-13b-0.9.7-distilled.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.7-dev.yaml)                                    | [ltxv-13b-dist-i2v-base.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base.json) |
| [ltxv-13b-0.9.7-distilled-lora128](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-distilled-lora128.safetensors)         | LoRA to make ltxv-13b-dev behave like the distilled model | N/A                                    | N/A |
| ltxv-13b-0.9.7-fp8               | Quantized version of ltxv-13b | Coming soon | [ltxv-13b-i2v-base-fp8.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/ltxv-13b-i2v-base-fp8.json) |
| ltxv-13b-0.9.7-distilled-fp8     | Quantized version of ltxv-13b-distilled | Coming soon | [ltxv-13b-dist-i2v-base-fp8.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base-fp8.json) |
| ltxv-2b-0.9.6                     | Good quality, lower VRAM requirement than ltxv-13b                                         | [ltxv-2b-0.9.6-dev.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.6-dev.yaml)                                                 | [ltxvideo-i2v.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/low_level/ltxvideo-i2v.json)             |
| ltxv-2b-0.9.6-distilled         | 15√ó faster, real-time capable, fewer steps needed, no STG/CFG required                     | [ltxv-2b-0.9.6-distilled.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.6-distilled.yaml)                                     | [ltxvideo-i2v-distilled.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/low_level/ltxvideo-i2v-distilled.json)             |


# Quick Start Guide

## Online inference
The model is accessible right away via the following links:
- [LTX-Studio image-to-video (13B-mix)](https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b)
- [LTX-Studio image-to-video (13B distilled)](https://app.ltx.studio/motion-workspace?videoModel=ltxv)
- [Fal.ai text-to-video](https://fal.ai/models/fal-ai/ltx-video)
- [Fal.ai image-to-video](https://fal.ai/models/fal-ai/ltx-video/image-to-video)
- [Replicate text-to-video and image-to-video](https://replicate.com/lightricks/ltx-video)

## Run locally

### Installation
The codebase was tested with Python 3.10.5, CUDA version 12.2, and supports PyTorch &gt;= 2.1.2.
On macos, MPS was tested with PyTorch 2.3.0, and should support PyTorch == 2.3 or &gt;= 2.6.

```bash
git clone https://github.com/Lightricks/LTX-Video.git
cd LTX-Video

# create env
python -m venv env
source env/bin/activate
python -m pip install -e .\[inference-script\]
```

### Inference

üìù **Note:** For best results, we recommend using our [ComfyUI](#comfyui-integration) workflow. We‚Äôre working on updating the inference.py script to match the high quality and output fidelity of ComfyUI.

To use our model, please follow the inference code in [inference.py](./inference.py):

#### For text-to-video generation:

```bash
python inference.py --prompt &quot;PROMPT&quot; --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.7-distilled.yaml
```

#### For image-to-video generation:

```bash
python inference.py --prompt &quot;PROMPT&quot; --conditioning_media_paths IMAGE_PATH --conditioning_start_frames 0 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.7-distilled.yaml
```

#### Extending a video:

üìù **Note:** Input video segments must contain a multiple of 8 frames plus 1 (e.g., 9, 17, 25, etc.), and the target frame number should be a multiple of 8.


```bash
python inference.py --prompt &quot;PROMPT&quot; --conditioning_media_paths VIDEO_PATH --conditioning_start_frames START_FRAME --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.7-distilled.yaml
```

#### For video generation with multiple conditions:

You can now generate a video conditioned on a set of images and/or short video segments.
Simply provide a list of paths to the images or video segments you want to condition on, along with their target frame numbers in the generated video. You can also specify the conditioning strength for each item (default: 1.0).

```bash
python 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[eosphoros-ai/DB-GPT]]></title>
            <link>https://github.com/eosphoros-ai/DB-GPT</link>
            <guid>https://github.com/eosphoros-ai/DB-GPT</guid>
            <pubDate>Thu, 29 May 2025 00:04:15 GMT</pubDate>
            <description><![CDATA[AI Native Data App Development framework with AWEL(Agentic Workflow Expression Language) and Agents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/eosphoros-ai/DB-GPT">eosphoros-ai/DB-GPT</a></h1>
            <p>AI Native Data App Development framework with AWEL(Agentic Workflow Expression Language) and Agents</p>
            <p>Language: Python</p>
            <p>Stars: 16,616</p>
            <p>Forks: 2,278</p>
            <p>Stars today: 18 stars today</p>
            <h2>README</h2><pre># DB-GPT: AI Native Data App Development framework with AWEL(Agentic Workflow Expression Language) and Agents

&lt;p align=&quot;left&quot;&gt;
  &lt;img src=&quot;./assets/LOGO.png&quot; width=&quot;100%&quot; /&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
    &lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT&quot;&gt;
        &lt;img alt=&quot;stars&quot; src=&quot;https://img.shields.io/github/stars/eosphoros-ai/db-gpt?style=social&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT&quot;&gt;
        &lt;img alt=&quot;forks&quot; src=&quot;https://img.shields.io/github/forks/eosphoros-ai/db-gpt?style=social&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;
      &lt;img alt=&quot;License: MIT&quot; src=&quot;https://img.shields.io/badge/License-MIT-yellow.svg&quot; /&gt;
    &lt;/a&gt;
     &lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT/releases&quot;&gt;
      &lt;img alt=&quot;Release Notes&quot; src=&quot;https://img.shields.io/github/release/eosphoros-ai/DB-GPT&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT/issues&quot;&gt;
      &lt;img alt=&quot;Open Issues&quot; src=&quot;https://img.shields.io/github/issues-raw/eosphoros-ai/DB-GPT&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://discord.gg/7uQnPuveTY&quot;&gt;
      &lt;img alt=&quot;Discord&quot; src=&quot;https://dcbadge.vercel.app/api/server/7uQnPuveTY?compact=true&amp;style=flat&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://join.slack.com/t/slack-inu2564/shared_invite/zt-29rcnyw2b-N~ubOD9kFc7b7MDOAM1otA&quot;&gt;
      &lt;img alt=&quot;Slack&quot; src=&quot;https://badgen.net/badge/Slack/Join%20DB-GPT/0abd59?icon=slack&quot; /&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://codespaces.new/eosphoros-ai/DB-GPT&quot;&gt;
      &lt;img alt=&quot;Open in GitHub Codespaces&quot; src=&quot;https://github.com/codespaces/badge.svg&quot; /&gt;
    &lt;/a&gt;
  &lt;/p&gt;


[**ÁÆÄ‰Ωì‰∏≠Êñá**](README.zh.md) | [**Êó•Êú¨Ë™û**](README.ja.md) | [**Discord**](https://discord.gg/7uQnPuveTY) | [**Documents**](https://docs.dbgpt.site) | [**ÂæÆ‰ø°**](https://github.com/eosphoros-ai/DB-GPT/blob/main/README.zh.md#%E8%81%94%E7%B3%BB%E6%88%91%E4%BB%AC) | [**Community**](https://github.com/eosphoros-ai/community) | [**Paper**](https://arxiv.org/pdf/2312.17449.pdf)

&lt;/div&gt;

## What is DB-GPT?

ü§ñ **DB-GPT is an open source AI native data app development framework with AWEL(Agentic Workflow Expression Language) and agents**. 

The purpose is to build infrastructure in the field of large models, through the development of multiple technical capabilities such as multi-model management (SMMF), Text2SQL effect optimization, RAG framework and optimization, Multi-Agents framework collaboration, AWEL (agent workflow orchestration), etc. Which makes large model applications with data simpler and more convenient.

üöÄ **In the Data 3.0 era, based on models and databases, enterprises and developers can build their own bespoke applications with less code.**

### DISCKAIMER
- [disckaimer](./DISCKAIMER.md)

### AI-Native Data App 
---
- üî•üî•üî• [Released V0.7.0 | A set of significant upgrades](http://docs.dbgpt.cn/blog/db-gpt-v070-release)
  - [Support MCP Protocol](https://github.com/eosphoros-ai/DB-GPT/pull/2497)
  - [Support DeepSeek R1](https://github.com/deepseek-ai/DeepSeek-R1)
  - [Support QwQ-32B](https://huggingface.co/Qwen/QwQ-32B)
  - [Refactor the basic modules]()
    - [dbgpt-app](./packages/dbgpt-app)
    - [dbgpt-core](./packages/dbgpt-core)
    - [dbgpt-serve](./packages/dbgpt-serve)
    - [dbgpt-client](./packages/dbgpt-client)
    - [dbgpt-accelerator](./packages/dbgpt-accelerator)
    - [dbgpt-ext](./packages/dbgpt-ext)
---

![app_chat_v0 6](https://github.com/user-attachments/assets/a2f0a875-df8c-4f0d-89a3-eed321c02113)

![app_manage_chat_data_v0 6](https://github.com/user-attachments/assets/c8cc85bb-e3c2-4fab-8fb9-7b4b469d0611)

![chat_dashboard_display_v0 6](https://github.com/user-attachments/assets/b15d6ebe-54c4-4527-a16d-02fbbaf20dc9)

![agent_prompt_awel_v0 6](https://github.com/user-attachments/assets/40761507-a1e1-49d4-b49a-3dd9a5ea41cc)

## Contents
- [Introduction](#introduction)
- [Install](#install)
- [Features](#features)
- [Contribution](#contribution)
- [Contact](#contact-information)

## Introduction 
The architecture of DB-GPT is shown in the following figure:

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./assets/dbgpt.png&quot; width=&quot;800&quot; /&gt;
&lt;/p&gt;

The core capabilities include the following parts:

- **RAG (Retrieval Augmented Generation)**: RAG is currently the most practically implemented and urgently needed domain. DB-GPT has already implemented a framework based on RAG, allowing users to build knowledge-based applications using the RAG capabilities of DB-GPT.

- **GBI (Generative Business Intelligence)**: Generative BI is one of the core capabilities of the DB-GPT project, providing the foundational data intelligence technology to build enterprise report analysis and business insights.

- **Fine-tuning Framework**: Model fine-tuning is an indispensable capability for any enterprise to implement in vertical and niche domains. DB-GPT provides a complete fine-tuning framework that integrates seamlessly with the DB-GPT project. In recent fine-tuning efforts, an accuracy rate based on the Spider dataset has been achieved at 82.5%.

- **Data-Driven Multi-Agents Framework**: DB-GPT offers a data-driven self-evolving multi-agents framework, aiming to continuously make decisions and execute based on data.

- **Data Factory**: The Data Factory is mainly about cleaning and processing trustworthy knowledge and data in the era of large models.

- **Data Sources**: Integrating various data sources to seamlessly connect production business data to the core capabilities of DB-GPT.

### SubModule
- [DB-GPT-Hub](https://github.com/eosphoros-ai/DB-GPT-Hub) Text-to-SQL workflow with high performance by applying Supervised Fine-Tuning (SFT) on Large Language Models (LLMs).

- [dbgpts](https://github.com/eosphoros-ai/dbgpts)  dbgpts is the official repository which contains some data apps„ÄÅAWEL operators„ÄÅAWEL workflow templates and agents which build upon DB-GPT.

#### Text2SQL Finetune
- support llms
  - [x] LLaMA
  - [x] LLaMA-2
  - [x] BLOOM
  - [x] BLOOMZ
  - [x] Falcon
  - [x] Baichuan
  - [x] Baichuan2
  - [x] InternLM
  - [x] Qwen
  - [x] XVERSE
  - [x] ChatGLM2

[More Information about Text2SQL finetune](https://github.com/eosphoros-ai/DB-GPT-Hub)

- [DB-GPT-Plugins](https://github.com/eosphoros-ai/DB-GPT-Plugins) DB-GPT Plugins that can run Auto-GPT plugin directly
- [GPT-Vis](https://github.com/eosphoros-ai/GPT-Vis) Visualization protocol

## Install 
![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&amp;logo=docker&amp;logoColor=white)
![Linux](https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&amp;logo=linux&amp;logoColor=black)
![macOS](https://img.shields.io/badge/mac%20os-000000?style=for-the-badge&amp;logo=macos&amp;logoColor=F0F0F0)
![Windows](https://img.shields.io/badge/Windows-0078D6?style=for-the-badge&amp;logo=windows&amp;logoColor=white)

[**Usage Tutorial**](http://docs.dbgpt.cn/docs/overview)
- [**Install**](http://docs.dbgpt.cn/docs/installation)
  - [Docker](http://docs.dbgpt.cn/docs/installation/docker)
  - [Source Code](http://docs.dbgpt.cn/docs/installation/sourcecode)
- [**Quickstart**](http://docs.dbgpt.cn/docs/quickstart)
- [**Application**](http://docs.dbgpt.cn/docs/operation_manual)
  - [Development Guide](http://docs.dbgpt.cn/docs/cookbook/app/data_analysis_app_develop) 
  - [App Usage](http://docs.dbgpt.cn/docs/application/app_usage)
  - [AWEL Flow Usage](http://docs.dbgpt.cn/docs/application/awel_flow_usage)
- [**Debugging**](http://docs.dbgpt.cn/docs/operation_manual/advanced_tutorial/debugging)
- [**Advanced Usage**](http://docs.dbgpt.cn/docs/application/advanced_tutorial/cli)
  - [SMMF](http://docs.dbgpt.cn/docs/application/advanced_tutorial/smmf)
  - [Finetune](http://docs.dbgpt.cn/docs/application/fine_tuning_manual/dbgpt_hub)
  - [AWEL](http://docs.dbgpt.cn/docs/awel/tutorial)


## Features

At present, we have introduced several key features to showcase our current capabilities:
- **Private Domain Q&amp;A &amp; Data Processing**

  The DB-GPT project offers a range of functionalities designed to improve knowledge base construction and enable efficient storage and retrieval of both structured and unstructured data. These functionalities include built-in support for uploading multiple file formats, the ability to integrate custom data extraction plug-ins, and unified vector storage and retrieval capabilities for effectively managing large volumes of information.

- **Multi-Data Source &amp; GBI(Generative Business intelligence)**

  The DB-GPT project facilitates seamless natural language interaction with diverse data sources, including Excel, databases, and data warehouses. It simplifies the process of querying and retrieving information from these sources, empowering users to engage in intuitive conversations and gain insights. Moreover, DB-GPT supports the generation of analytical reports, providing users with valuable data summaries and interpretations.

- **Multi-Agents&amp;Plugins**

  It offers support for custom plug-ins to perform various tasks and natively integrates the Auto-GPT plug-in model. The Agents protocol adheres to the Agent Protocol standard.

- **Automated Fine-tuning text2SQL**

  We&#039;ve also developed an automated fine-tuning lightweight framework centred on large language models (LLMs), Text2SQL datasets, LoRA/QLoRA/Pturning, and other fine-tuning methods. This framework simplifies Text-to-SQL fine-tuning, making it as straightforward as an assembly line process. [DB-GPT-Hub](https://github.com/eosphoros-ai/DB-GPT-Hub)

- **SMMF(Service-oriented Multi-model Management Framework)**

  We offer extensive model support, including dozens of large language models (LLMs) from both open-source and API agents, such as LLaMA/LLaMA2, Baichuan, ChatGLM, Wenxin, Tongyi, Zhipu, and many more. 

  - News
    - üî•üî•üî•  [Qwen3-235B-A22B](https://huggingface.co/Qwen/Qwen3-235B-A22B)
    - üî•üî•üî•  [Qwen3-30B-A3B](https://huggingface.co/Qwen/Qwen3-30B-A3B)
    - üî•üî•üî•  [Qwen3-32B](https://huggingface.co/Qwen/Qwen3-32B)
    - üî•üî•üî•  [GLM-Z1-32B-0414](https://huggingface.co/THUDM/GLM-Z1-32B-0414)
    - üî•üî•üî•  [GLM-4-32B-0414](https://huggingface.co/THUDM/GLM-4-32B-0414)
    - üî•üî•üî•  [QwQ-32B](https://huggingface.co/Qwen/QwQ-32B)
    - üî•üî•üî•  [DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)
    - üî•üî•üî•  [DeepSeek-V3](https://huggingface.co/deepseek-ai/DeepSeek-V3)
    - üî•üî•üî•  [DeepSeek-R1-Distill-Llama-70B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)
    - üî•üî•üî•  [DeepSeek-R1-Distill-Qwen-32B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)
    - üî•üî•üî•  [DeepSeek-R1-Distill-Qwen-14B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)
    - üî•üî•üî•  [DeepSeek-R1-Distill-Llama-8B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)
    - üî•üî•üî•  [DeepSeek-R1-Distill-Qwen-7B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)
    - üî•üî•üî•  [DeepSeek-R1-Distill-Qwen-1.5B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)
    - üî•üî•üî•  [Qwen2.5-Coder-32B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct)
    - üî•üî•üî•  [Qwen2.5-Coder-14B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct)
    - üî•üî•üî•  [Qwen2.5-72B-Instruct](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct)
    - üî•üî•üî•  [Qwen2.5-32B-Instruct](https://huggingface.co/Qwen/Qwen2.5-32B-Instruct)
    - üî•üî•üî•  [Qwen2.5-14B-Instruct](https://huggingface.co/Qwen/Qwen2.5-14B-Instruct)
    - üî•üî•üî•  [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)
    - üî•üî•üî•  [Qwen2.5-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct)
    - üî•üî•üî•  [Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct)
    - üî•üî•üî•  [Qwen2.5-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct)
    - üî•üî•üî•  [Qwen2.5-Coder-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct)
    - üî•üî•üî•  [Qwen2.5-Coder-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct)
    - üî•üî•üî•  [Meta-Llama-3.1-405B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct)
    - üî•üî•üî•  [Meta-Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct)
    - üî•üî•üî•  [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)
    - üî•üî•üî•  [gemma-2-27b-it](https://huggingface.co/google/gemma-2-27b-it)
    - üî•üî•üî•  [gemma-2-9b-it](https://huggingface.co/google/gemma-2-9b-it)
    - üî•üî•üî•  [DeepSeek-Coder-V2-Instruct](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct)
    - üî•üî•üî•  [DeepSeek-Coder-V2-Lite-Instruct](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct)
    - üî•üî•üî•  [Qwen2-57B-A14B-Instruct](https://huggingface.co/Qwen/Qwen2-57B-A14B-Instruct)
    - üî•üî•üî•  [Qwen2-72B-Instruct](https://huggingface.co/Qwen/Qwen2-72B-Instruct)
    - üî•üî•üî•  [Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct)
    - üî•üî•üî•  [Qwen2-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct)
    - üî•üî•üî•  [Qwen2-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2-0.5B-Instruct)
    - üî•üî•üî•  [glm-4-9b-chat](https://huggingface.co/THUDM/glm-4-9b-chat)
    - üî•üî•üî•  [Phi-3](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3)
    - üî•üî•üî•  [Yi-1.5-34B-Chat](https://huggingface.co/01-ai/Yi-1.5-34B-Chat)
    - üî•üî•üî•  [Yi-1.5-9B-Chat](https://huggingface.co/01-ai/Yi-1.5-9B-Chat)
    - üî•üî•üî•  [Yi-1.5-6B-Chat](https://huggingface.co/01-ai/Yi-1.5-6B-Chat)
    - üî•üî•üî•  [Qwen1.5-110B-Chat](https://huggingface.co/Qwen/Qwen1.5-110B-Chat)
    - üî•üî•üî•  [Qwen1.5-MoE-A2.7B-Chat](https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B-Chat)
    - üî•üî•üî•  [Meta-Llama-3-70B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct)
    - üî•üî•üî•  [Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)
    - üî•üî•üî•  [CodeQwen1.5-7B-Chat](https://huggingface.co/Qwen/CodeQwen1.5-7B-Chat)
    - üî•üî•üî•  [Qwen1.5-32B-Chat](https://huggingface.co/Qwen/Qwen1.5-32B-Chat)
    - üî•üî•üî•  [Starling-LM-7B-beta](https://huggingface.co/Nexusflow/Starling-LM-7B-beta)
    - üî•üî•üî•  [gemma-7b-it](https://huggingface.co/google/gemma-7b-it)
    - üî•üî•üî•  [gemma-2b-it](https://huggingface.co/google/gemma-2b-it)
    - üî•üî•üî•  [SOLAR-10.7B](https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0)
    - üî•üî•üî•  [Mixtral-8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)
    - üî•üî•üî•  [Qwen-72B-Chat](https://huggingface.co/Qwen/Qwen-72B-Chat)
    - üî•üî•üî•  [Yi-34B-Chat](https://huggingface.co/01-ai/Yi-34B-Chat)
  - [More Supported LLMs](http://docs.dbgpt.site/docs/modules/smmf)

- **Privacy and Security**
  
  We ensure the privacy and security of data through the implementation of various technologies, including privatized large models and proxy desensitization.

- Support Datasources
  - [Datasources](http://docs.dbgpt.cn/docs/modules/connections)

## Image
üåê [AutoDL Image](https://www.codewithgpu.com/i/eosphoros-ai/DB-GPT/dbgpt)


### Language Switching
    In the .env configuration file, modify the LANGUAGE parameter to switch to different languages. The default is English (Chinese: zh, English: en, other languages to be added later).

## Contribution

- To check detailed guidelines for new contributions, please refer [how to contribute](https://github.com/eosphoros-ai/DB-GPT/blob/main/CONTRIBUTING.md)

### Contributors Wall
&lt;a href=&quot;https://github.com/eosphoros-ai/DB-GPT/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=eosphoros-ai/DB-GPT&amp;max=200&quot; /&gt;
&lt;/a&gt;


## Licence
The MIT License (MIT)

## Citation
If you want to understand the overall architecture of DB-GPT, please cite &lt;a href=&quot;https://arxiv.org/abs/2312.17449&quot; target=&quot;_blank&quot;&gt;paper&lt;/a&gt; and &lt;a href=&quot;https:// arxiv.org/abs/2404.10209&quot; target=&quot;_blank&quot;&gt;Paper&lt;/a&gt;

If you want to learn about using DB-GPT for Agent development, please cite the &lt;a href=&quot;https://arxiv.org/abs/2412.13520&quot; target=&quot;_blank&quot;&gt;paper&lt;/a&gt;
```bibtex
@article{xue2023dbgpt,
      title={DB-GPT: Empowering Database Interactions with Private Large Language Models}, 
      author={Siqiao Xue and Caigao Jiang and Wenhui Shi and Fangyin Cheng and Keting Chen and Hongjun Yang and Zhiping Zhang and Jianshan He and Hongyang Zhang and Ganglin Wei and Wang Zhao and Fan Zhou and Danrui Qi and Hong Yi and Shaodong Liu and Faqiang Chen},
      year={2023},
      journal={arXiv preprint arXiv:2312.17449},
      url={https://arxiv.org/abs/2312.17449}
}
@misc{huang2024romasrolebasedmultiagentdatabase,
      title={ROMAS: A Role-Based Multi-Agent System for Database monitoring and Planning}, 
      author={Yi Huang and Fangyin Cheng and Fan Zhou and Jiahui Li and Jian Gong and Hongjun Yang and Zhidong Fan and Caigao Jiang and Siqiao Xue and Faqiang Chen},
      year={2024},
      eprint={2412.13520},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2412.13520}, 
}
@inproceedings{xue2024demonstration,
      title={Demonstration of DB-GPT: Next Generation Data Interaction System Empowered by Large Language Models}, 
      author={Siqiao Xue and Danrui Qi and Caigao Jiang and Wenhui Shi and Fangyin Cheng and Keting Chen and Hongjun Yang and Zhiping Zhang and Jianshan He and Hongyang Zhang and Ganglin Wei and Wang Zhao and Fan Zhou and Hong Yi and Shaodong Liu and Hongjun Yang and Faqiang Chen},
      year={2024},
      booktitle = &quot;Proceedings of the VLDB Endowment&quot;,
      url={https://arxiv.org/abs/2404.10209}
}
```


## Contact Information
We are working on building a community, if you have any ideas for building the community, feel free to contact us.
[![](https://dcbadge.vercel.app/api/server/7uQnPuveTY?compact=true&amp;style=flat)](https://discord.gg/7uQnPuveTY)

[![Star History Chart](https://api.star-history.com/svg?repos=csunny/DB-GPT&amp;type=Date)](https://star-history.com/#csunny/DB-GPT)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[seleniumbase/SeleniumBase]]></title>
            <link>https://github.com/seleniumbase/SeleniumBase</link>
            <guid>https://github.com/seleniumbase/SeleniumBase</guid>
            <pubDate>Thu, 29 May 2025 00:04:14 GMT</pubDate>
            <description><![CDATA[Python APIs for web automation, testing, and bypassing bot-detection.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/seleniumbase/SeleniumBase">seleniumbase/SeleniumBase</a></h1>
            <p>Python APIs for web automation, testing, and bypassing bot-detection.</p>
            <p>Language: Python</p>
            <p>Stars: 10,612</p>
            <p>Forks: 1,328</p>
            <p>Stars today: 84 stars today</p>
            <h2>README</h2><pre>&lt;!-- SeleniumBase Docs --&gt;

&lt;meta property=&quot;og:site_name&quot; content=&quot;SeleniumBase&quot;&gt;
&lt;meta property=&quot;og:title&quot; content=&quot;SeleniumBase: Python Web Automation and E2E Testing&quot; /&gt;
&lt;meta property=&quot;og:description&quot; content=&quot;Fast, easy, and reliable Web/UI testing with Python.&quot; /&gt;
&lt;meta property=&quot;og:keywords&quot; content=&quot;Python, pytest, selenium, webdriver, testing, automation, seleniumbase, framework, dashboard, recorder, reports, screenshots&quot;&gt;
&lt;meta property=&quot;og:image&quot; content=&quot;https://seleniumbase.github.io/cdn/img/mac_sb_logo_5b.png&quot; /&gt;
&lt;link rel=&quot;icon&quot; href=&quot;https://seleniumbase.github.io/img/logo7.png&quot; /&gt;

&lt;h1&gt;SeleniumBase&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/&quot;&gt;&lt;img src=&quot;https://seleniumbase.github.io/cdn/img/super_logo_sb3.png&quot; alt=&quot;SeleniumBase&quot; title=&quot;SeleniumBase&quot; width=&quot;350&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot; class=&quot;hero__title&quot;&gt;&lt;b&gt;All-in-one Browser Automation Framework:&lt;br /&gt;Web Crawling / Testing / Scraping / Stealth&lt;/b&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://pypi.python.org/pypi/seleniumbase&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/seleniumbase.svg?color=3399EE&quot; alt=&quot;PyPI version&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/releases&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/seleniumbase/SeleniumBase.svg?color=22AAEE&quot; alt=&quot;GitHub version&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://seleniumbase.io&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/docs-seleniumbase.io-11BBAA.svg&quot; alt=&quot;SeleniumBase Docs&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/actions&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://github.com/seleniumbase/SeleniumBase/workflows/CI%20build/badge.svg&quot; alt=&quot;SeleniumBase GitHub Actions&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/EdhQTn3EyE&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/727927627830001734?color=7289DA&amp;label=Discord&amp;logo=discord&amp;logoColor=white&quot;/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;#python_installation&quot;&gt;üöÄ Start&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/features_list.md&quot;&gt;üè∞ Features&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/customizing_test_runs.md&quot;&gt;üéõÔ∏è Options&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/ReadMe.md&quot;&gt;üìö Examples&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/seleniumbase/console_scripts/ReadMe.md&quot;&gt;üå† Scripts&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/mobile_testing.md&quot;&gt;üì± Mobile&lt;/a&gt;
&lt;br /&gt;
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/method_summary.md&quot;&gt;üìò APIs&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/syntax_formats.md&quot;&gt; üî† Formats&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/recorder_mode.md&quot;&gt;üî¥ Recorder&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/example_logs/ReadMe.md&quot;&gt;üìä Dashboard&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/locale_codes.md&quot;&gt;üóæ Locales&lt;/a&gt; |
&lt;a href=&quot;https://seleniumbase.io/devices/?url=seleniumbase.com&quot;&gt;üíª Farm&lt;/a&gt;
&lt;br /&gt;
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/commander.md&quot;&gt;üéñÔ∏è GUI&lt;/a&gt; |
&lt;a href=&quot;https://seleniumbase.io/demo_page&quot;&gt;üì∞ TestPage&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/uc_mode.md&quot;&gt;üë§ UC Mode&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/cdp_mode/ReadMe.md&quot;&gt;üêô CDP Mode&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/chart_maker/ReadMe.md&quot;&gt;üì∂ Charts&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/seleniumbase/utilities/selenium_grid/ReadMe.md&quot;&gt;üåê Grid&lt;/a&gt;
&lt;br /&gt;
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/how_it_works.md&quot;&gt;üëÅÔ∏è How&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/tree/master/examples/migration/raw_selenium&quot;&gt;üöù Migrate&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/case_plans.md&quot;&gt;üóÇÔ∏è CasePlans&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/tree/master/examples/boilerplates&quot;&gt;‚ôªÔ∏è Template&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/master_qa/ReadMe.md&quot;&gt;üß¨ Hybrid&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/tour_examples/ReadMe.md&quot;&gt;üöé Tours&lt;/a&gt;
&lt;br /&gt;
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/integrations/github/workflows/ReadMe.md&quot;&gt;ü§ñ CI/CD&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/js_package_manager.md&quot;&gt;üïπÔ∏è JSMgr&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/translations.md&quot;&gt;üåè Translator&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/presenter/ReadMe.md&quot;&gt;üéûÔ∏è Presenter&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/dialog_boxes/ReadMe.md&quot;&gt;üõÇ Dialog&lt;/a&gt; |
&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/visual_testing/ReadMe.md&quot;&gt;üñºÔ∏è Visual&lt;/a&gt;
&lt;br /&gt;
&lt;/p&gt;

&lt;p&gt;SeleniumBase is the professional toolkit for web automation activities. Built for testing websites, bypassing CAPTCHAs, enhancing productivity, completing tasks, and scaling your business.&lt;/p&gt;

--------

üìö Learn from [**over 200 examples** in the **SeleniumBase/examples/** folder](https://github.com/seleniumbase/SeleniumBase/tree/master/examples).

üêô Note that &lt;a translate=&quot;no&quot; href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/uc_mode.md&quot;&gt;&lt;b&gt;UC Mode&lt;/b&gt;&lt;/a&gt; / &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/cdp_mode/ReadMe.md&quot;&gt;&lt;b&gt;CDP Mode&lt;/b&gt;&lt;/a&gt; (Stealth Mode) have their own ReadMe files.

‚ÑπÔ∏è Most scripts run with raw &lt;code translate=&quot;no&quot;&gt;&lt;b&gt;python&lt;/b&gt;&lt;/code&gt;, although some scripts use &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/syntax_formats.md&quot;&gt;Syntax Formats&lt;/a&gt; that expect &lt;a href=&quot;https://docs.pytest.org/en/latest/how-to/usage.html&quot; translate=&quot;no&quot;&gt;&lt;b&gt;pytest&lt;/b&gt;&lt;/a&gt; (a Python unit-testing framework included with SeleniumBase that can discover, collect, and run tests automatically).

--------

&lt;p align=&quot;left&quot;&gt;üìó Here&#039;s &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/raw_google.py&quot;&gt;raw_google.py&lt;/a&gt;, which performs a Google search:&lt;/p&gt;

```python
from seleniumbase import SB

with SB(test=True, uc=True) as sb:
    sb.open(&quot;https://google.com/ncr&quot;)
    sb.type(&#039;[title=&quot;Search&quot;]&#039;, &quot;SeleniumBase GitHub page\n&quot;)
    sb.click(&#039;[href*=&quot;github.com/seleniumbase/&quot;]&#039;)
    sb.save_screenshot_to_logs()  # ./latest_logs/
    print(sb.get_page_title())
```

&gt; `python raw_google.py`

&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/raw_google.py&quot;&gt;&lt;img src=&quot;https://seleniumbase.github.io/cdn/gif/google_search.gif&quot; alt=&quot;SeleniumBase Test&quot; title=&quot;SeleniumBase Test&quot; width=&quot;480&quot; /&gt;&lt;/a&gt;

--------

&lt;p align=&quot;left&quot;&gt;üìó Here&#039;s an example of bypassing Cloudflare&#039;s challenge page: &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/cdp_mode/raw_gitlab.py&quot;&gt;SeleniumBase/examples/cdp_mode/raw_gitlab.py&lt;/a&gt;&lt;/p&gt;

```python
from seleniumbase import SB

with SB(uc=True, test=True, locale=&quot;en&quot;) as sb:
    url = &quot;https://gitlab.com/users/sign_in&quot;
    sb.activate_cdp_mode(url)
    sb.uc_gui_click_captcha()
    sb.sleep(2)
```

&lt;img src=&quot;https://seleniumbase.github.io/other/cf_sec.jpg&quot; title=&quot;SeleniumBase&quot; width=&quot;332&quot;&gt; &lt;img src=&quot;https://seleniumbase.github.io/other/gitlab_bypass.png&quot; title=&quot;SeleniumBase&quot; width=&quot;288&quot;&gt;

--------

&lt;p align=&quot;left&quot;&gt;üìó Here&#039;s &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/test_get_swag.py&quot;&gt;test_get_swag.py&lt;/a&gt;, which tests an e-commerce site:&lt;/p&gt;

```python
from seleniumbase import BaseCase
BaseCase.main(__name__, __file__)  # Call pytest

class MyTestClass(BaseCase):
    def test_swag_labs(self):
        self.open(&quot;https://www.saucedemo.com&quot;)
        self.type(&quot;#user-name&quot;, &quot;standard_user&quot;)
        self.type(&quot;#password&quot;, &quot;secret_sauce\n&quot;)
        self.assert_element(&quot;div.inventory_list&quot;)
        self.click(&#039;button[name*=&quot;backpack&quot;]&#039;)
        self.click(&quot;#shopping_cart_container a&quot;)
        self.assert_text(&quot;Backpack&quot;, &quot;div.cart_item&quot;)
        self.click(&quot;button#checkout&quot;)
        self.type(&quot;input#first-name&quot;, &quot;SeleniumBase&quot;)
        self.type(&quot;input#last-name&quot;, &quot;Automation&quot;)
        self.type(&quot;input#postal-code&quot;, &quot;77123&quot;)
        self.click(&quot;input#continue&quot;)
        self.click(&quot;button#finish&quot;)
        self.assert_text(&quot;Thank you for your order!&quot;)
```

&gt; `pytest test_get_swag.py`

&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/test_get_swag.py&quot;&gt;&lt;img src=&quot;https://seleniumbase.github.io/cdn/gif/fast_swag_2.gif&quot; alt=&quot;SeleniumBase Test&quot; title=&quot;SeleniumBase Test&quot; width=&quot;480&quot; /&gt;&lt;/a&gt;

&gt; (The default browser is ``--chrome`` if not set.)

--------

&lt;p align=&quot;left&quot;&gt;üìó Here&#039;s &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/test_coffee_cart.py&quot; target=&quot;_blank&quot;&gt;test_coffee_cart.py&lt;/a&gt;, which verifies an e-commerce site:&lt;/p&gt;

```bash
pytest test_coffee_cart.py --demo
```

&lt;p align=&quot;left&quot;&gt;&lt;a href=&quot;https://seleniumbase.io/coffee/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://seleniumbase.github.io/cdn/gif/coffee_cart.gif&quot; width=&quot;480&quot; alt=&quot;SeleniumBase Coffee Cart Test&quot; title=&quot;SeleniumBase Coffee Cart Test&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&gt; &lt;p&gt;(&lt;code translate=&quot;no&quot;&gt;--demo&lt;/code&gt; mode slows down tests and highlights actions)&lt;/p&gt;

--------

&lt;a id=&quot;multiple_examples&quot;&gt;&lt;/a&gt;

&lt;p align=&quot;left&quot;&gt;üìó Here&#039;s &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/test_demo_site.py&quot; target=&quot;_blank&quot;&gt;test_demo_site.py&lt;/a&gt;, which covers several actions:&lt;/p&gt;

```bash
pytest test_demo_site.py
```

&lt;p align=&quot;left&quot;&gt;&lt;a href=&quot;https://seleniumbase.io/demo_page&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://seleniumbase.github.io/cdn/gif/demo_page_5.gif&quot; width=&quot;480&quot; alt=&quot;SeleniumBase Example&quot; title=&quot;SeleniumBase Example&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&gt; Easy to type, click, select, toggle, drag &amp; drop, and more.

(For more examples, see the &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/ReadMe.md&quot;&gt;SeleniumBase/examples/&lt;/a&gt; folder.)

--------

&lt;p align=&quot;left&quot;&gt;&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/&quot;&gt;&lt;img src=&quot;https://seleniumbase.github.io/cdn/img/super_logo_sb3.png&quot; alt=&quot;SeleniumBase&quot; title=&quot;SeleniumBase&quot; width=&quot;232&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p dir=&quot;auto&quot;&gt;&lt;strong&gt;Explore the README:&lt;/strong&gt;&lt;/p&gt;
&lt;ul dir=&quot;auto&quot;&gt;
&lt;li&gt;&lt;a href=&quot;#install_seleniumbase&quot;   &gt;&lt;strong&gt;Get Started / Installation&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#basic_example_and_usage&quot;&gt;&lt;strong&gt;Basic Example / Usage&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#common_methods&quot;         &gt;&lt;strong&gt;Common Test Methods&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#fun_facts&quot;              &gt;&lt;strong&gt;Fun Facts / Learn More&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#demo_mode_and_debugging&quot;&gt;&lt;strong&gt;Demo Mode / Debugging&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#command_line_options&quot;   &gt;&lt;strong&gt;Command-line Options&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#directory_configuration&quot;&gt;&lt;strong&gt;Directory Configuration&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#seleniumbase_dashboard&quot; &gt;&lt;strong&gt;SeleniumBase Dashboard&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#creating_visual_reports&quot;&gt;&lt;strong&gt;Generating Test Reports&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

--------

&lt;details&gt;
&lt;summary&gt; ‚ñ∂Ô∏è How is &lt;b&gt;SeleniumBase&lt;/b&gt; different from raw Selenium? (&lt;b&gt;click to expand&lt;/b&gt;)&lt;/summary&gt;
&lt;div&gt;

&lt;p&gt;üí° SeleniumBase is a Python framework for browser automation and testing. SeleniumBase uses &lt;a href=&quot;https://www.w3.org/TR/webdriver2/#endpoints&quot; target=&quot;_blank&quot;&gt;Selenium/WebDriver&lt;/a&gt; APIs and incorporates test-runners such as &lt;code translate=&quot;no&quot;&gt;pytest&lt;/code&gt;, &lt;code translate=&quot;no&quot;&gt;pynose&lt;/code&gt;, and &lt;code translate=&quot;no&quot;&gt;behave&lt;/code&gt; to provide organized structure, test discovery, test execution, test state (&lt;i&gt;eg. passed, failed, or skipped&lt;/i&gt;), and command-line options for changing default settings (&lt;i&gt;eg. browser selection&lt;/i&gt;). With raw Selenium, you would need to set up your own options-parser for configuring tests from the command-line.&lt;/p&gt;

&lt;p&gt;üí° SeleniumBase&#039;s driver manager gives you more control over automatic driver downloads. (Use &lt;code translate=&quot;no&quot;&gt;--driver-version=VER&lt;/code&gt; with your &lt;code translate=&quot;no&quot;&gt;pytest&lt;/code&gt; run command to specify the version.) By default, SeleniumBase will download a driver version that matches your major browser version if not set.&lt;/p&gt;

&lt;p&gt;üí° SeleniumBase automatically detects between CSS Selectors and XPath, which means you don&#039;t need to specify the type of selector in your commands (&lt;i&gt;but optionally you could&lt;/i&gt;).&lt;/p&gt;

&lt;p&gt;üí° SeleniumBase methods often perform multiple actions in a single method call. For example, &lt;code translate=&quot;no&quot;&gt;self.type(selector, text)&lt;/code&gt; does the following:&lt;br /&gt;1. Waits for the element to be visible.&lt;br /&gt;2. Waits for the element to be interactive.&lt;br /&gt;3. Clears the text field.&lt;br /&gt;4. Types in the new text.&lt;br /&gt;5. Presses Enter/Submit if the text ends in &lt;code translate=&quot;no&quot;&gt;&quot;\n&quot;&lt;/code&gt;.&lt;br /&gt;With raw Selenium, those actions require multiple method calls.&lt;/p&gt;

&lt;p&gt;üí° SeleniumBase uses default timeout values when not set:&lt;br /&gt;
‚úÖ &lt;code translate=&quot;no&quot;&gt;self.click(&quot;button&quot;)&lt;/code&gt;&lt;br /&gt;
With raw Selenium, methods would fail instantly (&lt;i&gt;by default&lt;/i&gt;) if an element needed more time to load:&lt;br /&gt;
‚ùå &lt;code translate=&quot;no&quot;&gt;self.driver.find_element(by=&quot;css selector&quot;, value=&quot;button&quot;).click()&lt;/code&gt;&lt;br /&gt;
(Reliable code is better than unreliable code.)&lt;/p&gt;

&lt;p&gt;üí° SeleniumBase lets you change the explicit timeout values of methods:&lt;br /&gt;
‚úÖ &lt;code translate=&quot;no&quot;&gt;self.click(&quot;button&quot;, timeout=10)&lt;/code&gt;&lt;br /&gt;
With raw Selenium, that requires more code:&lt;br /&gt;
‚ùå &lt;code translate=&quot;no&quot;&gt;WebDriverWait(driver, 10).until(EC.element_to_be_clickable(&quot;css selector&quot;, &quot;button&quot;)).click()&lt;/code&gt;&lt;br /&gt;
(Simple code is better than complex code.)&lt;/p&gt;

&lt;p&gt;üí° SeleniumBase gives you clean error output when a test fails. With raw Selenium, error messages can get very messy.&lt;/p&gt;

&lt;p&gt;üí° SeleniumBase gives you the option to generate a dashboard and reports for tests. It also saves screenshots from failing tests to the &lt;code translate=&quot;no&quot;&gt;./latest_logs/&lt;/code&gt; folder. Raw &lt;a href=&quot;https://www.selenium.dev/documentation/webdriver/&quot; translate=&quot;no&quot; target=&quot;_blank&quot;&gt;Selenium&lt;/a&gt; does not have these options out-of-the-box.&lt;/p&gt;

&lt;p&gt;üí° SeleniumBase includes desktop GUI apps for running tests, such as &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/commander.md&quot; translate=&quot;no&quot;&gt;SeleniumBase Commander&lt;/a&gt; for &lt;code translate=&quot;no&quot;&gt;pytest&lt;/code&gt; and &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/behave_bdd/ReadMe.md&quot; translate=&quot;no&quot;&gt;SeleniumBase Behave GUI&lt;/a&gt; for &lt;code translate=&quot;no&quot;&gt;behave&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;üí° SeleniumBase has its own &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/recorder_mode.md&quot;&gt;Recorder / Test Generator&lt;/a&gt; for creating tests from manual browser actions.&lt;/p&gt;

&lt;p&gt;üí° SeleniumBase comes with &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/case_plans.md&quot;&gt;test case management software, (&quot;CasePlans&quot;)&lt;/a&gt;, for organizing tests and step descriptions.&lt;/p&gt;

&lt;p&gt;üí° SeleniumBase includes tools for &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/chart_maker/ReadMe.md&quot;&gt;building data apps, (&quot;ChartMaker&quot;)&lt;/a&gt;, which can generate JavaScript from Python.&lt;/p&gt;

&lt;/div&gt;
&lt;/details&gt;

--------

&lt;p&gt;üìö &lt;b&gt;Learn about different ways of writing tests:&lt;/b&gt;&lt;/p&gt;

&lt;p align=&quot;left&quot;&gt;üìóüìù Here&#039;s &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/test_simple_login.py&quot;&gt;test_simple_login.py&lt;/a&gt;, which uses &lt;code translate=&quot;no&quot;&gt;&lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/seleniumbase/fixtures/base_case.py&quot;&gt;BaseCase&lt;/a&gt;&lt;/code&gt; class inheritance, and runs with &lt;a href=&quot;https://docs.pytest.org/en/latest/how-to/usage.html&quot;&gt;pytest&lt;/a&gt; or &lt;a href=&quot;https://github.com/mdmintz/pynose&quot;&gt;pynose&lt;/a&gt;. (Use &lt;code translate=&quot;no&quot;&gt;self.driver&lt;/code&gt; to access Selenium&#039;s raw &lt;code translate=&quot;no&quot;&gt;driver&lt;/code&gt;.)&lt;/p&gt;

```python
from seleniumbase import BaseCase
BaseCase.main(__name__, __file__)

class TestSimpleLogin(BaseCase):
    def test_simple_login(self):
        self.open(&quot;seleniumbase.io/simple/login&quot;)
        self.type(&quot;#username&quot;, &quot;demo_user&quot;)
        self.type(&quot;#password&quot;, &quot;secret_pass&quot;)
        self.click(&#039;a:contains(&quot;Sign in&quot;)&#039;)
        self.assert_exact_text(&quot;Welcome!&quot;, &quot;h1&quot;)
        self.assert_element(&quot;img#image1&quot;)
        self.highlight(&quot;#image1&quot;)
        self.click_link(&quot;Sign out&quot;)
        self.assert_text(&quot;signed out&quot;, &quot;#top_message&quot;)
```

&lt;p align=&quot;left&quot;&gt;üìòüìù Here&#039;s &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/raw_login_sb.py&quot;&gt;raw_login_sb.py&lt;/a&gt;, which uses the &lt;b&gt;&lt;code translate=&quot;no&quot;&gt;SB&lt;/code&gt;&lt;/b&gt; Context Manager. Runs with pure &lt;code translate=&quot;no&quot;&gt;python&lt;/code&gt;. (Use &lt;code translate=&quot;no&quot;&gt;sb.driver&lt;/code&gt; to access Selenium&#039;s raw &lt;code translate=&quot;no&quot;&gt;driver&lt;/code&gt;.)&lt;/p&gt;

```python
from seleniumbase import SB

with SB() as sb:
    sb.open(&quot;seleniumbase.io/simple/login&quot;)
    sb.type(&quot;#username&quot;, &quot;demo_user&quot;)
    sb.type(&quot;#password&quot;, &quot;secret_pass&quot;)
    sb.click(&#039;a:contains(&quot;Sign in&quot;)&#039;)
    sb.assert_exact_text(&quot;Welcome!&quot;, &quot;h1&quot;)
    sb.assert_element(&quot;img#image1&quot;)
    sb.highlight(&quot;#image1&quot;)
    sb.click_link(&quot;Sign out&quot;)
    sb.assert_text(&quot;signed out&quot;, &quot;#top_message&quot;)
```

&lt;p align=&quot;left&quot;&gt;üìôüìù Here&#039;s &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/examples/raw_login_driver.py&quot;&gt;raw_login_driver.py&lt;/a&gt;, which uses the &lt;b&gt;&lt;code translate=&quot;no&quot;&gt;Driver&lt;/code&gt;&lt;/b&gt; Manager. Runs with pure &lt;code translate=&quot;no&quot;&gt;python&lt;/code&gt;. (The &lt;code&gt;driver&lt;/code&gt; is an improved version of Selenium&#039;s raw &lt;code translate=&quot;no&quot;&gt;driver&lt;/code&gt;, with more methods.)&lt;/p&gt;

```python
from seleniumbase import Driver

driver = Driver()
try:
    driver.open(&quot;seleniumbase.io/simple/login&quot;)
    driver.type(&quot;#username&quot;, &quot;demo_user&quot;)
    driver.type(&quot;#password&quot;, &quot;secret_pass&quot;)
    driver.click(&#039;a:contains(&quot;Sign in&quot;)&#039;)
    driver.assert_exact_text(&quot;Welcome!&quot;, &quot;h1&quot;)
    driver.assert_element(&quot;img#image1&quot;)
    driver.highlight(&quot;#image1&quot;)
    driver.click_link(&quot;Sign out&quot;)
    driver.assert_text(&quot;signed out&quot;, &quot;#top_message&quot;)
finally:
    driver.quit()
```

--------

&lt;a id=&quot;python_installation&quot;&gt;&lt;/a&gt;
&lt;h2&gt;&lt;img src=&quot;https://seleniumbase.github.io/cdn/img/python_logo.png&quot; title=&quot;SeleniumBase&quot; width=&quot;42&quot; /&gt; Set up Python &amp; Git:&lt;/h2&gt;

&lt;a href=&quot;https://www.python.org/downloads/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/pyversions/seleniumbase.svg?color=FACE42&quot; title=&quot;Supported Python Versions&quot; /&gt;&lt;/a&gt;

üîµ Add &lt;b&gt;&lt;a href=&quot;https://www.python.org/downloads/&quot;&gt;Python&lt;/a&gt;&lt;/b&gt; and &lt;b&gt;&lt;a href=&quot;https://git-scm.com/&quot;&gt;Git&lt;/a&gt;&lt;/b&gt; to your System PATH.

üîµ Using a &lt;a href=&quot;https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/virtualenv_instructions.md&quot;&gt;Python virtual env&lt;/a&gt; is recommended.

&lt;a id=&quot;install_seleniumbase&quot;&gt;&lt;/a&gt;
&lt;h2&gt;&lt;img src=&quot;https://seleniumbase.github.io/img/logo7.png&quot; title=&quot;SeleniumBase&quot; width=&quot;32&quot; /&gt; Install SeleniumBase:&lt;/h2&gt;

**You can install ``seleniumbase`` from [PyPI](https://pypi.org/project/seleniumbase/) or [GitHub](https://github.com/seleniumbase/SeleniumBase):**

üîµ **How to install ``seleniumbase`` from PyPI:**

```bash
pip install seleniumbase
```

* (Add ``--upgrade`` OR ``-U`` to upgrade SeleniumBase.)
* (Add ``--force-reinstall`` to upgrade indirect packages.)
* (Use ``pip3`` if multiple versions of Python are present.)

üîµ **How to install ``seleniumbase`` from a GitHub clone:**

```bash
git clone https://github.com/seleniumbase/SeleniumBase.git
cd SeleniumBase/
pip install -e .
```

üîµ **How to upgrade an existing install from a GitHub clone:**

```bash
git pull
pip install -e .
```

üîµ **Type ``seleniumbase`` or ``sbase`` to verify that SeleniumBase was installed successfully:**



... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[DLR-RM/stable-baselines3]]></title>
            <link>https://github.com/DLR-RM/stable-baselines3</link>
            <guid>https://github.com/DLR-RM/stable-baselines3</guid>
            <pubDate>Thu, 29 May 2025 00:04:13 GMT</pubDate>
            <description><![CDATA[PyTorch version of Stable Baselines, reliable implementations of reinforcement learning algorithms.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/DLR-RM/stable-baselines3">DLR-RM/stable-baselines3</a></h1>
            <p>PyTorch version of Stable Baselines, reliable implementations of reinforcement learning algorithms.</p>
            <p>Language: Python</p>
            <p>Stars: 10,745</p>
            <p>Forks: 1,839</p>
            <p>Stars today: 22 stars today</p>
            <h2>README</h2><pre>&lt;!-- [![pipeline status](https://gitlab.com/araffin/stable-baselines3/badges/master/pipeline.svg)](https://gitlab.com/araffin/stable-baselines3/-/commits/master) --&gt;
[![CI](https://github.com/DLR-RM/stable-baselines3/workflows/CI/badge.svg)](https://github.com/DLR-RM/stable-baselines3/actions/workflows/ci.yml)
[![Documentation Status](https://readthedocs.org/projects/stable-baselines/badge/?version=master)](https://stable-baselines3.readthedocs.io/en/master/?badge=master) [![coverage report](https://gitlab.com/araffin/stable-baselines3/badges/master/coverage.svg)](https://github.com/DLR-RM/stable-baselines3/actions/workflows/ci.yml)
[![codestyle](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)


# Stable Baselines3

&lt;img src=&quot;docs/\_static/img/logo.png&quot; align=&quot;right&quot; width=&quot;40%&quot;/&gt;

Stable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It is the next major version of [Stable Baselines](https://github.com/hill-a/stable-baselines).

You can read a detailed presentation of Stable Baselines3 in the [v1.0 blog post](https://araffin.github.io/post/sb3/) or our [JMLR paper](https://jmlr.org/papers/volume22/20-1364/20-1364.pdf).


These algorithms will make it easier for the research community and industry to replicate, refine, and identify new ideas, and will create good baselines to build projects on top of. We expect these tools will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. We also hope that the simplicity of these tools will allow beginners to experiment with a more advanced toolset, without being buried in implementation details.

**Note: Despite its simplicity of use, Stable Baselines3 (SB3) assumes you have some knowledge about Reinforcement Learning (RL).** You should not utilize this library without some practice. To that extent, we provide good resources in the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/rl.html) to get started with RL.

## Main Features

**The performance of each algorithm was tested** (see *Results* section in their respective page),
you can take a look at the issues [#48](https://github.com/DLR-RM/stable-baselines3/issues/48) and [#49](https://github.com/DLR-RM/stable-baselines3/issues/49) for more details.

We also provide detailed logs and reports on the [OpenRL Benchmark](https://wandb.ai/openrlbenchmark/sb3) platform.


| **Features**                | **Stable-Baselines3** |
| --------------------------- | ----------------------|
| State of the art RL methods | :heavy_check_mark: |
| Documentation               | :heavy_check_mark: |
| Custom environments         | :heavy_check_mark: |
| Custom policies             | :heavy_check_mark: |
| Common interface            | :heavy_check_mark: |
| `Dict` observation space support  | :heavy_check_mark: |
| Ipython / Notebook friendly | :heavy_check_mark: |
| Tensorboard support         | :heavy_check_mark: |
| PEP8 code style             | :heavy_check_mark: |
| Custom callback             | :heavy_check_mark: |
| High code coverage          | :heavy_check_mark: |
| Type hints                  | :heavy_check_mark: |


### Planned features

Since most of the features from the [original roadmap](https://github.com/DLR-RM/stable-baselines3/issues/1) have been implemented, there are no major changes planned for SB3, it is now *stable*.
If you want to contribute, you can search in the issues for the ones where [help is welcomed](https://github.com/DLR-RM/stable-baselines3/labels/help%20wanted) and the other [proposed enhancements](https://github.com/DLR-RM/stable-baselines3/labels/enhancement).

While SB3 development is now focused on bug fixes and maintenance (doc update, user experience, ...), there is more active development going on in the associated repositories:
- newer algorithms are regularly added to the [SB3 Contrib](https://github.com/Stable-Baselines-Team/stable-baselines3-contrib) repository
- faster variants are developed in the [SBX (SB3 + Jax)](https://github.com/araffin/sbx) repository
- the training framework for SB3, the RL Zoo, has an active [roadmap](https://github.com/DLR-RM/rl-baselines3-zoo/issues/299)

## Migration guide: from Stable-Baselines (SB2) to Stable-Baselines3 (SB3)

A migration guide from SB2 to SB3 can be found in the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html).

## Documentation

Documentation is available online: [https://stable-baselines3.readthedocs.io/](https://stable-baselines3.readthedocs.io/)

## Integrations

Stable-Baselines3 has some integration with other libraries/services like Weights &amp; Biases for experiment tracking or Hugging Face for storing/sharing trained models. You can find out more in the [dedicated section](https://stable-baselines3.readthedocs.io/en/master/guide/integrations.html) of the documentation.


## RL Baselines3 Zoo: A Training Framework for Stable Baselines3 Reinforcement Learning Agents

[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a training framework for Reinforcement Learning (RL).

It provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.

In addition, it includes a collection of tuned hyperparameters for common environments and RL algorithms, and agents trained with those settings.

Goals of this repository:

1. Provide a simple interface to train and enjoy RL agents
2. Benchmark the different Reinforcement Learning algorithms
3. Provide tuned hyperparameters for each environment and RL algorithm
4. Have fun with the trained agents!

Github repo: https://github.com/DLR-RM/rl-baselines3-zoo

Documentation: https://rl-baselines3-zoo.readthedocs.io/en/master/

## SB3-Contrib: Experimental RL Features

We implement experimental features in a separate contrib repository: [SB3-Contrib](https://github.com/Stable-Baselines-Team/stable-baselines3-contrib)

This allows SB3 to maintain a stable and compact core, while still providing the latest features, like Recurrent PPO (PPO LSTM), CrossQ, Truncated Quantile Critics (TQC), Quantile Regression DQN (QR-DQN) or PPO with invalid action masking (Maskable PPO).

Documentation is available online: [https://sb3-contrib.readthedocs.io/](https://sb3-contrib.readthedocs.io/)

## Stable-Baselines Jax (SBX)

[Stable Baselines Jax (SBX)](https://github.com/araffin/sbx) is a proof of concept version of Stable-Baselines3 in Jax, with recent algorithms like DroQ or CrossQ.

It provides a minimal number of features compared to SB3 but can be much faster (up to 20x times!): https://twitter.com/araffin2/status/1590714558628253698


## Installation

**Note:** Stable-Baselines3 supports PyTorch &gt;= 2.3

### Prerequisites
Stable Baselines3 requires Python 3.9+.

#### Windows

To install stable-baselines on Windows, please look at the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/install.html#prerequisites).


### Install using pip
Install the Stable Baselines3 package:
```sh
pip install &#039;stable-baselines3[extra]&#039;
```

This includes optional dependencies like Tensorboard, OpenCV or `ale-py` to train on atari games. If you do not need those, you can use:
```sh
pip install stable-baselines3
```

Please read the [documentation](https://stable-baselines3.readthedocs.io/) for more details and alternatives (from source, using docker).


## Example

Most of the code in the library tries to follow a sklearn-like syntax for the Reinforcement Learning algorithms.

Here is a quick example of how to train and run PPO on a cartpole environment:
```python
import gymnasium as gym

from stable_baselines3 import PPO

env = gym.make(&quot;CartPole-v1&quot;, render_mode=&quot;human&quot;)

model = PPO(&quot;MlpPolicy&quot;, env, verbose=1)
model.learn(total_timesteps=10_000)

vec_env = model.get_env()
obs = vec_env.reset()
for i in range(1000):
    action, _states = model.predict(obs, deterministic=True)
    obs, reward, done, info = vec_env.step(action)
    vec_env.render()
    # VecEnv resets automatically
    # if done:
    #   obs = env.reset()

env.close()
```

Or just train a model with a one liner if [the environment is registered in Gymnasium](https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/#registering-envs) and if [the policy is registered](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html):

```python
from stable_baselines3 import PPO

model = PPO(&quot;MlpPolicy&quot;, &quot;CartPole-v1&quot;).learn(10_000)
```

Please read the [documentation](https://stable-baselines3.readthedocs.io/) for more examples.


## Try it online with Colab Notebooks !

All the following examples can be executed online using Google Colab notebooks:

- [Full Tutorial](https://github.com/araffin/rl-tutorial-jnrr19)
- [All Notebooks](https://github.com/Stable-Baselines-Team/rl-colab-notebooks/tree/sb3)
- [Getting Started](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/stable_baselines_getting_started.ipynb)
- [Training, Saving, Loading](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/saving_loading_dqn.ipynb)
- [Multiprocessing](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/multiprocessing_rl.ipynb)
- [Monitor Training and Plotting](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/monitor_training.ipynb)
- [Atari Games](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/atari_games.ipynb)
- [RL Baselines Zoo](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/rl-baselines-zoo.ipynb)
- [PyBullet](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/pybullet.ipynb)


## Implemented Algorithms

| **Name**         | **Recurrent**      | `Box`          | `Discrete`     | `MultiDiscrete` | `MultiBinary`  | **Multi Processing**              |
| ------------------- | ------------------ | ------------------ | ------------------ | ------------------- | ------------------ | --------------------------------- |
| ARS&lt;sup&gt;[1](#f1)&lt;/sup&gt;   | :x: | :heavy_check_mark: | :heavy_check_mark: | :x: | :x: | :heavy_check_mark: |
| A2C   | :x: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |
| CrossQ&lt;sup&gt;[1](#f1)&lt;/sup&gt;   | :x: | :heavy_check_mark: | :x:                | :x:                 | :x:                | :heavy_check_mark: |
| DDPG  | :x: | :heavy_check_mark: | :x:                | :x:                 | :x:                | :heavy_check_mark: |
| DQN   | :x: | :x: | :heavy_check_mark: | :x:                 | :x:                | :heavy_check_mark: |
| HER   | :x: | :heavy_check_mark: | :heavy_check_mark: | :x: | :x: | :heavy_check_mark: |
| PPO   | :x: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark:  | :heavy_check_mark: | :heavy_check_mark: |
| QR-DQN&lt;sup&gt;[1](#f1)&lt;/sup&gt;  | :x: | :x: | :heavy_check_mark: | :x:                 | :x:                | :heavy_check_mark: |
| RecurrentPPO&lt;sup&gt;[1](#f1)&lt;/sup&gt;   | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark:  | :heavy_check_mark: | :heavy_check_mark: |
| SAC   | :x: | :heavy_check_mark: | :x:                | :x:                 | :x:                | :heavy_check_mark: |
| TD3   | :x: | :heavy_check_mark: | :x:                | :x:                 | :x:                | :heavy_check_mark: |
| TQC&lt;sup&gt;[1](#f1)&lt;/sup&gt;   | :x: | :heavy_check_mark: | :x:                | :x:                 | :x: | :heavy_check_mark: |
| TRPO&lt;sup&gt;[1](#f1)&lt;/sup&gt;  | :x: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark:  | :heavy_check_mark: | :heavy_check_mark: |
| Maskable PPO&lt;sup&gt;[1](#f1)&lt;/sup&gt;   | :x: | :x: | :heavy_check_mark: | :heavy_check_mark:  | :heavy_check_mark: | :heavy_check_mark:  |

&lt;b id=&quot;f1&quot;&gt;1&lt;/b&gt;: Implemented in [SB3 Contrib](https://github.com/Stable-Baselines-Team/stable-baselines3-contrib) GitHub repository.

Actions `gymnasium.spaces`:
 * `Box`: A N-dimensional box that contains every point in the action space.
 * `Discrete`: A list of possible actions, where each timestep only one of the actions can be used.
 * `MultiDiscrete`: A list of possible actions, where each timestep only one action of each discrete set can be used.
 * `MultiBinary`: A list of possible actions, where each timestep any of the actions can be used in any combination.



## Testing the installation
### Install dependencies
```sh
pip install -e .[docs,tests,extra]
```
### Run tests
All unit tests in stable baselines3 can be run using `pytest` runner:
```sh
make pytest
```
To run a single test file:
```sh
python3 -m pytest -v tests/test_env_checker.py
```
To run a single test:
```sh
python3 -m pytest -v -k &#039;test_check_env_dict_action&#039;
```

You can also do a static type check using `mypy`:
```sh
pip install mypy
make type
```

Codestyle check with `ruff`:
```sh
pip install ruff
make lint
```

## Projects Using Stable-Baselines3

We try to maintain a list of projects using stable-baselines3 in the [documentation](https://stable-baselines3.readthedocs.io/en/master/misc/projects.html),
please tell us if you want your project to appear on this page ;)

## Citing the Project

To cite this repository in publications:

```bibtex
@article{stable-baselines3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}
```

Note: If you need to refer to a specific version of SB3, you can also use the [Zenodo DOI](https://doi.org/10.5281/zenodo.8123988).

## Maintainers

Stable-Baselines3 is currently maintained by [Ashley Hill](https://github.com/hill-a) (aka @hill-a), [Antonin Raffin](https://araffin.github.io/) (aka [@araffin](https://github.com/araffin)), [Maximilian Ernestus](https://github.com/ernestum) (aka @ernestum), [Adam Gleave](https://github.com/adamgleave) (@AdamGleave), [Anssi Kanervisto](https://github.com/Miffyli) (@Miffyli) and [Quentin Gallou√©dec](https://gallouedec.com/) (@qgallouedec).

**Important Note: We do not provide technical support, or consulting** and do not answer personal questions via email.
Please post your question on the [RL Discord](https://discord.com/invite/xhfNqQv), [Reddit](https://www.reddit.com/r/reinforcementlearning/), or [Stack Overflow](https://stackoverflow.com/) in that case.


## How To Contribute

To any interested in making the baselines better, there is still some documentation that needs to be done.
If you want to contribute, please read [**CONTRIBUTING.md**](./CONTRIBUTING.md) guide first.

## Acknowledgments

The initial work to develop Stable Baselines3 was partially funded by the project *Reduced Complexity Models* from the *Helmholtz-Gemeinschaft Deutscher Forschungszentren*, and by the EU&#039;s Horizon 2020 Research and Innovation Programme under grant number 951992 ([VeriDream](https://www.veridream.eu/)).

The original version, Stable Baselines, was created in the [robotics lab U2IS](http://u2is.ensta-paristech.fr/index.php?lang=en) ([INRIA Flowers](https://flowers.inria.fr/) team) at [ENSTA ParisTech](http://www.ensta-paristech.fr/en).


Logo credits: [L.M. Tenkes](https://www.instagram.com/lucillehue/)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>