<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Mon, 19 Jan 2026 00:05:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[yt-dlp/yt-dlp]]></title>
            <link>https://github.com/yt-dlp/yt-dlp</link>
            <guid>https://github.com/yt-dlp/yt-dlp</guid>
            <pubDate>Mon, 19 Jan 2026 00:05:00 GMT</pubDate>
            <description><![CDATA[A feature-rich command-line audio/video downloader]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/yt-dlp/yt-dlp">yt-dlp/yt-dlp</a></h1>
            <p>A feature-rich command-line audio/video downloader</p>
            <p>Language: Python</p>
            <p>Stars: 142,459</p>
            <p>Forks: 11,506</p>
            <p>Stars today: 149 stars today</p>
            <h2>README</h2><pre>&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
&lt;div align=&quot;center&quot;&gt;

[![YT-DLP](https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/.github/banner.svg)](#readme)

[![Release version](https://img.shields.io/github/v/release/yt-dlp/yt-dlp?color=brightgreen&amp;label=Download&amp;style=for-the-badge)](#installation &quot;Installation&quot;)
[![PyPI](https://img.shields.io/badge/-PyPI-blue.svg?logo=pypi&amp;labelColor=555555&amp;style=for-the-badge)](https://pypi.org/project/yt-dlp &quot;PyPI&quot;)
[![Donate](https://img.shields.io/badge/_-Donate-red.svg?logo=githubsponsors&amp;labelColor=555555&amp;style=for-the-badge)](Maintainers.md#maintainers &quot;Donate&quot;)
[![Discord](https://img.shields.io/discord/807245652072857610?color=blue&amp;labelColor=555555&amp;label=&amp;logo=discord&amp;style=for-the-badge)](https://discord.gg/H5MNcFW63r &quot;Discord&quot;)
[![Supported Sites](https://img.shields.io/badge/-Supported_Sites-brightgreen.svg?style=for-the-badge)](supportedsites.md &quot;Supported Sites&quot;)
[![License: Unlicense](https://img.shields.io/badge/-Unlicense-blue.svg?style=for-the-badge)](LICENSE &quot;License&quot;)
[![CI Status](https://img.shields.io/github/actions/workflow/status/yt-dlp/yt-dlp/core.yml?branch=master&amp;label=Tests&amp;style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/actions &quot;CI Status&quot;)
[![Commits](https://img.shields.io/github/commit-activity/m/yt-dlp/yt-dlp?label=commits&amp;style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/commits &quot;Commit History&quot;)
[![Last Commit](https://img.shields.io/github/last-commit/yt-dlp/yt-dlp/master?label=&amp;style=for-the-badge&amp;display_timestamp=committer)](https://github.com/yt-dlp/yt-dlp/pulse/monthly &quot;Last activity&quot;)

&lt;/div&gt;
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

yt-dlp is a feature-rich command-line audio/video downloader with support for [thousands of sites](supportedsites.md). The project is a fork of [youtube-dl](https://github.com/ytdl-org/youtube-dl) based on the now inactive [youtube-dlc](https://github.com/blackjack4494/yt-dlc).

&lt;!-- MANPAGE: MOVE &quot;USAGE AND OPTIONS&quot; SECTION HERE --&gt;

&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
* [INSTALLATION](#installation)
    * [Detailed instructions](https://github.com/yt-dlp/yt-dlp/wiki/Installation)
    * [Release Files](#release-files)
    * [Update](#update)
    * [Dependencies](#dependencies)
    * [Compile](#compile)
* [USAGE AND OPTIONS](#usage-and-options)
    * [General Options](#general-options)
    * [Network Options](#network-options)
    * [Geo-restriction](#geo-restriction)
    * [Video Selection](#video-selection)
    * [Download Options](#download-options)
    * [Filesystem Options](#filesystem-options)
    * [Thumbnail Options](#thumbnail-options)
    * [Internet Shortcut Options](#internet-shortcut-options)
    * [Verbosity and Simulation Options](#verbosity-and-simulation-options)
    * [Workarounds](#workarounds)
    * [Video Format Options](#video-format-options)
    * [Subtitle Options](#subtitle-options)
    * [Authentication Options](#authentication-options)
    * [Post-processing Options](#post-processing-options)
    * [SponsorBlock Options](#sponsorblock-options)
    * [Extractor Options](#extractor-options)
    * [Preset Aliases](#preset-aliases)
* [CONFIGURATION](#configuration)
    * [Configuration file encoding](#configuration-file-encoding)
    * [Authentication with netrc](#authentication-with-netrc)
    * [Notes about environment variables](#notes-about-environment-variables)
* [OUTPUT TEMPLATE](#output-template)
    * [Output template examples](#output-template-examples)
* [FORMAT SELECTION](#format-selection)
    * [Filtering Formats](#filtering-formats)
    * [Sorting Formats](#sorting-formats)
    * [Format Selection examples](#format-selection-examples)
* [MODIFYING METADATA](#modifying-metadata)
    * [Modifying metadata examples](#modifying-metadata-examples)
* [EXTRACTOR ARGUMENTS](#extractor-arguments)
* [PLUGINS](#plugins)
    * [Installing Plugins](#installing-plugins)
    * [Developing Plugins](#developing-plugins)
* [EMBEDDING YT-DLP](#embedding-yt-dlp)
    * [Embedding examples](#embedding-examples)
* [CHANGES FROM YOUTUBE-DL](#changes-from-youtube-dl)
    * [New features](#new-features)
    * [Differences in default behavior](#differences-in-default-behavior)
    * [Deprecated options](#deprecated-options)
* [CONTRIBUTING](CONTRIBUTING.md#contributing-to-yt-dlp)
    * [Opening an Issue](CONTRIBUTING.md#opening-an-issue)
    * [Developer Instructions](CONTRIBUTING.md#developer-instructions)
* [WIKI](https://github.com/yt-dlp/yt-dlp/wiki)
    * [FAQ](https://github.com/yt-dlp/yt-dlp/wiki/FAQ)
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;


# INSTALLATION

&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
[![Windows](https://img.shields.io/badge/-Windows_x64-blue.svg?style=for-the-badge&amp;logo=windows)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe)
[![Unix](https://img.shields.io/badge/-Linux/BSD-red.svg?style=for-the-badge&amp;logo=linux)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp)
[![MacOS](https://img.shields.io/badge/-MacOS-lightblue.svg?style=for-the-badge&amp;logo=apple)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos)
[![PyPI](https://img.shields.io/badge/-PyPI-blue.svg?logo=pypi&amp;labelColor=555555&amp;style=for-the-badge)](https://pypi.org/project/yt-dlp)
[![Source Tarball](https://img.shields.io/badge/-Source_tar-green.svg?style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)
[![Other variants](https://img.shields.io/badge/-Other-grey.svg?style=for-the-badge)](#release-files)
[![All versions](https://img.shields.io/badge/-All_Versions-lightgrey.svg?style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/releases)
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

You can install yt-dlp using [the binaries](#release-files), [pip](https://pypi.org/project/yt-dlp) or one using a third-party package manager. See [the wiki](https://github.com/yt-dlp/yt-dlp/wiki/Installation) for detailed instructions


&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
## RELEASE FILES

#### Recommended

File|Description
:---|:---
[yt-dlp](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp)|Platform-independent [zipimport](https://docs.python.org/3/library/zipimport.html) binary. Needs Python (recommended for **Linux/BSD**)
[yt-dlp.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe)|Windows (Win8+) standalone x64 binary (recommended for **Windows**)
[yt-dlp_macos](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos)|Universal MacOS (10.15+) standalone executable (recommended for **MacOS**)

#### Alternatives

File|Description
:---|:---
[yt-dlp_linux](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux)|Linux (glibc 2.17+) standalone x86_64 binary
[yt-dlp_linux.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux.zip)|Unpackaged Linux (glibc 2.17+) x86_64 executable (no auto-update)
[yt-dlp_linux_aarch64](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_aarch64)|Linux (glibc 2.17+) standalone aarch64 binary
[yt-dlp_linux_aarch64.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_aarch64.zip)|Unpackaged Linux (glibc 2.17+) aarch64 executable (no auto-update)
[yt-dlp_linux_armv7l.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_armv7l.zip)|Unpackaged Linux (glibc 2.31+) armv7l executable (no auto-update)
[yt-dlp_musllinux](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux)|Linux (musl 1.2+) standalone x86_64 binary
[yt-dlp_musllinux.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux.zip)|Unpackaged Linux (musl 1.2+) x86_64 executable (no auto-update)
[yt-dlp_musllinux_aarch64](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux_aarch64)|Linux (musl 1.2+) standalone aarch64 binary
[yt-dlp_musllinux_aarch64.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux_aarch64.zip)|Unpackaged Linux (musl 1.2+) aarch64 executable (no auto-update)
[yt-dlp_x86.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_x86.exe)|Windows (Win8+) standalone x86 (32-bit) binary
[yt-dlp_win_x86.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win_x86.zip)|Unpackaged Windows (Win8+) x86 (32-bit) executable (no auto-update)
[yt-dlp_arm64.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_arm64.exe)|Windows (Win10+) standalone ARM64 binary
[yt-dlp_win_arm64.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win_arm64.zip)|Unpackaged Windows (Win10+) ARM64 executable (no auto-update)
[yt-dlp_win.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win.zip)|Unpackaged Windows (Win8+) x64 executable (no auto-update)
[yt-dlp_macos.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos.zip)|Unpackaged MacOS (10.15+) executable (no auto-update)

#### Misc

File|Description
:---|:---
[yt-dlp.tar.gz](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)|Source tarball
[SHA2-512SUMS](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS)|GNU-style SHA512 sums
[SHA2-512SUMS.sig](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS.sig)|GPG signature file for SHA512 sums
[SHA2-256SUMS](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS)|GNU-style SHA256 sums
[SHA2-256SUMS.sig](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS.sig)|GPG signature file for SHA256 sums

The public key that can be used to verify the GPG signatures is [available here](https://github.com/yt-dlp/yt-dlp/blob/master/public.key)
Example usage:
```
curl -L https://github.com/yt-dlp/yt-dlp/raw/master/public.key | gpg --import
gpg --verify SHA2-256SUMS.sig SHA2-256SUMS
gpg --verify SHA2-512SUMS.sig SHA2-512SUMS
```

#### Licensing

While yt-dlp is licensed under the [Unlicense](LICENSE), many of the release files contain code from other projects with different licenses.

Most notably, the PyInstaller-bundled executables include GPLv3+ licensed code, and as such the combined work is licensed under [GPLv3+](https://www.gnu.org/licenses/gpl-3.0.html).

The zipimport Unix executable (`yt-dlp`) contains [ISC](https://github.com/meriyah/meriyah/blob/main/LICENSE.md) licensed code from [`meriyah`](https://github.com/meriyah/meriyah) and [MIT](https://github.com/davidbonnet/astring/blob/main/LICENSE) licensed code from [`astring`](https://github.com/davidbonnet/astring).

See [THIRD_PARTY_LICENSES.txt](THIRD_PARTY_LICENSES.txt) for more details.

The git repository, the source tarball (`yt-dlp.tar.gz`), the PyPI source distribution and the PyPI built distribution (wheel) only contain code licensed under the [Unlicense](LICENSE).

&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

**Note**: The manpages, shell completion (autocomplete) files etc. are available inside the [source tarball](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)


## UPDATE
You can use `yt-dlp -U` to update if you are using the [release binaries](#release-files)

If you [installed with pip](https://github.com/yt-dlp/yt-dlp/wiki/Installation#with-pip), simply re-run the same command that was used to install the program

For other third-party package managers, see [the wiki](https://github.com/yt-dlp/yt-dlp/wiki/Installation#third-party-package-managers) or refer to their documentation

&lt;a id=&quot;update-channels&quot;&gt;&lt;/a&gt;

There are currently three release channels for binaries: `stable`, `nightly` and `master`.

* `stable` is the default channel, and many of its changes have been tested by users of the `nightly` and `master` channels.
* The `nightly` channel has releases scheduled to build every day around midnight UTC, for a snapshot of the project&#039;s new patches and changes. This is the **recommended channel for regular users** of yt-dlp. The `nightly` releases are available from [yt-dlp/yt-dlp-nightly-builds](https://github.com/yt-dlp/yt-dlp-nightly-builds/releases) or as development releases of the `yt-dlp` PyPI package (which can be installed with pip&#039;s `--pre` flag).
* The `master` channel features releases that are built after each push to the master branch, and these will have the very latest fixes and additions, but may also be more prone to regressions. They are available from [yt-dlp/yt-dlp-master-builds](https://github.com/yt-dlp/yt-dlp-master-builds/releases).

When using `--update`/`-U`, a release binary will only update to its current channel.
`--update-to CHANNEL` can be used to switch to a different channel when a newer version is available. `--update-to [CHANNEL@]TAG` can also be used to upgrade or downgrade to specific tags from a channel.

You may also use `--update-to &lt;repository&gt;` (`&lt;owner&gt;/&lt;repository&gt;`) to update to a channel on a completely different repository. Be careful with what repository you are updating to though, there is no verification done for binaries from different repositories.

Example usage:

* `yt-dlp --update-to master` switch to the `master` channel and update to its latest release
* `yt-dlp --update-to stable@2023.07.06` upgrade/downgrade to release to `stable` channel tag `2023.07.06`
* `yt-dlp --update-to 2023.10.07` upgrade/downgrade to tag `2023.10.07` if it exists on the current channel
* `yt-dlp --update-to example/yt-dlp@2023.09.24` upgrade/downgrade to the release from the `example/yt-dlp` repository, tag `2023.09.24`

**Important**: Any user experiencing an issue with the `stable` release should install or update to the `nightly` release before submitting a bug report:
```
# To update to nightly from stable executable/binary:
yt-dlp --update-to nightly

# To install nightly with pip:
python -m pip install -U --pre &quot;yt-dlp[default]&quot;
```

When running a yt-dlp version that is older than 90 days, you will see a warning message suggesting to update to the latest version.
You can suppress this warning by adding `--no-update` to your command or configuration file.

## DEPENDENCIES
Python versions 3.10+ (CPython) and 3.11+ (PyPy) are supported. Other versions and implementations may or may not work correctly.

&lt;!-- Python 3.5+ uses VC++14 and it is already embedded in the binary created
&lt;!x-- https://www.microsoft.com/en-us/download/details.aspx?id=26999 --x&gt;
On Windows, [Microsoft Visual C++ 2010 SP1 Redistributable Package (x86)](https://download.microsoft.com/download/1/6/5/165255E7-1014-4D0A-B094-B6A430A6BFFC/vcredist_x86.exe) is also necessary to run yt-dlp. You probably already have this, but if the executable throws an error due to missing `MSVCR100.dll` you need to install it manually.
--&gt;

While all the other dependencies are optional, `ffmpeg`, `ffprobe`, `yt-dlp-ejs` and a supported JavaScript runtime/engine are highly recommended

### Strongly recommended

* [**ffmpeg** and **ffprobe**](https://www.ffmpeg.org) - Required for [merging separate video and audio files](#format-selection), as well as for various [post-processing](#post-processing-options) tasks. License [depends on the build](https://www.ffmpeg.org/legal.html)

    There are bugs in ffmpeg that cause various issues when used alongside yt-dlp. Since ffmpeg is such an important dependency, we provide [custom builds](https://github.com/yt-dlp/FFmpeg-Builds#ffmpeg-static-auto-builds) with patches for some of these issues at [yt-dlp/FFmpeg-Builds](https://github.com/yt-dlp/FFmpeg-Builds). See [the readme](https://github.com/yt-dlp/FFmpeg-Builds#patches-applied) for details on the specific issues solved by these builds

    **Important**: What you need is ffmpeg *binary*, **NOT** [the Python package of the same name](https://pypi.org/project/ffmpeg)

* [**yt-dlp-ejs**](https://github.com/yt-dlp/ejs) - Required for deciphering YouTube n/sig values. Licensed under [Unlicense](https://github.com/yt-dlp/ejs/blob/main/LICENSE), bundles [MIT](https://github.com/davidbonnet/astring/blob/main/LICENSE) and [ISC](https://github.com/meriyah/meriyah/blob/main/LICENSE.md) components.

    A JavaScript runtime/engine like [**deno**](https://deno.land) (recommended), [**node.js**](https://nodejs.org), [**bun**](https://bun.sh), or [**QuickJS**](https://bellard.org/quickjs/) is also required to run yt-dlp-ejs. See [the wiki](https://github.com/yt-dlp/yt-dlp/wiki/EJS).

### Networking
* [**certifi**](https://github.com/certifi/python-certifi)\* - Provides Mozilla&#039;s root certificate bundle. Licensed under [MPLv2](https://github.com/certifi/python-certifi/blob/master/LICENSE)
* [**brotli**](https://github.com/google/brotli)\* or [**brotlicffi**](https://github.com/python-hyper/brotlicffi) - [Brotli](https://en.wikipedia.org/wiki/Brotli) content encoding support. Both licensed under MIT &lt;sup&gt;[1](https://github.com/google/brotli/blob/master/LICENSE) [2](https://github.com/python-hyper/brotlicffi/blob/master/LICENSE) &lt;/sup&gt;
* [**websockets**](https://github.com/aaugustin/websockets)\* - For downloading over websocket. Licensed under [BSD-3-Clause](https://github.com/aaugustin/websockets/blob/main/LICENSE)
* [**requests**](https://github.com/psf/requests)\* - HTTP library. For HTTPS proxy and persistent connections support. Licensed under [Apache-2.0](https://github.com/psf/requests/blob/main/LICENSE)

#### Impersonation

The following provide support for impersonating browser requests. This may be required for some sites that employ TLS fingerprinting.

* [**curl_cffi**](https://github.com/lexiforest/curl_cffi) (recommended) - Python binding for [curl-impersonate](https://github.com/lexiforest/curl-impersonate). Provides impersonation targets for Chrome, Edge and Safari. Licensed under [MIT](https://github.com/lexiforest/curl_cffi/blob/main/LICENSE)
  * Can be installed with the `curl-cffi` extra, e.g. `pip install &quot;yt-dlp[default,curl-cffi]&quot;`
  * Currently included in most builds *except* `yt-dlp` (Unix zipimport binary), `yt-dlp_x86` (Windows 32-bit) and `yt-dlp_musllinux_aarch64`


### Metadata

* [**mutagen**](https://github.com/quodlibet/mutagen)\* - For `--embed-thumbnail` in certain formats. Licensed under [GPLv2+](https://github.com/quodlibet/mutagen/blob/master/COPYING)
* [**AtomicParsley**](https://github.com/wez/atomicparsley) - For `--embed-thumbnail` in `mp4`/`m4a` files when `mutagen`/`ffmpeg` cannot. Licensed under [GPLv2+](https://github.com/wez/atomicparsley/blob/master/COPYING)
* [**xattr**](https://github.com/xattr/xattr), [**pyxattr**](https://github.com/iustin/pyxattr) or [**setfattr**](http://savannah.nongnu.org/projects/attr) - For writing xattr metadata (`--xattrs`) on **Mac** and **BSD**. Licensed under [MIT](https://github.com/xattr/xattr/blob/master/LICENSE.txt), [LGPL2.1](https://github.com/iustin/pyxattr/blob/master/COPYING) and [GPLv2+](http://git.savannah.nongnu.org/cgit/attr.git/tree/doc/COPYING) respectively

### Misc

* [**pycryptodomex**](https://github.com/Legrandin/pycryptodome)\* - For decrypting AES-128 HLS streams and various other data. Licensed under [BSD-2-Clause](https://github.com/Legrandin/pycryptodome/blob/master/LICENSE.rst)
* [**phantomjs**](https://github.com/ariya/phantomjs) - Used in some extractors where JavaScript needs to be run. No longer used for YouTube. To be deprecated in the near future. Licensed under [BSD-3-Clause](https://github.com/ariya/phantomjs/blob/master/LICENSE.BSD)
* [**secretstorage**](https://github.com/mitya57/secretstorage)\* - For `--cookies-from-browser` to access the **Gnome** keyring while decrypting cookies of **Chromium**-based browsers on **Linux**. Licensed under [BSD-3-Clause](https://github.com/mitya57/secretstorage/blob/master/LICENSE)
* Any external downloader that you want to use with `--downloader`

### Deprecated

* [**rtmpdump**](http://rtmpdump.mplayerhq.hu) - For downloading `rtmp` streams. ffmpeg can be used instead with `--downloader ffmpeg`. Licensed under [GPLv2+](http://rtmpdump.mplayerhq.hu)
* [**mplayer**](http://mplayerhq.hu/design7/info.html) or [**mpv

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[google/langextract]]></title>
            <link>https://github.com/google/langextract</link>
            <guid>https://github.com/google/langextract</guid>
            <pubDate>Mon, 19 Jan 2026 00:04:59 GMT</pubDate>
            <description><![CDATA[A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google/langextract">google/langextract</a></h1>
            <p>A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.</p>
            <p>Language: Python</p>
            <p>Stars: 22,034</p>
            <p>Forks: 1,526</p>
            <p>Stars today: 395 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/google/langextract&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/google/langextract/main/docs/_static/logo.svg&quot; alt=&quot;LangExtract Logo&quot; width=&quot;128&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

# LangExtract

[![PyPI version](https://img.shields.io/pypi/v/langextract.svg)](https://pypi.org/project/langextract/)
[![GitHub stars](https://img.shields.io/github/stars/google/langextract.svg?style=social&amp;label=Star)](https://github.com/google/langextract)
![Tests](https://github.com/google/langextract/actions/workflows/ci.yaml/badge.svg)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.17015089.svg)](https://doi.org/10.5281/zenodo.17015089)

## Table of Contents

- [Introduction](#introduction)
- [Why LangExtract?](#why-langextract)
- [Quick Start](#quick-start)
- [Installation](#installation)
- [API Key Setup for Cloud Models](#api-key-setup-for-cloud-models)
- [Adding Custom Model Providers](#adding-custom-model-providers)
- [Using OpenAI Models](#using-openai-models)
- [Using Local LLMs with Ollama](#using-local-llms-with-ollama)
- [More Examples](#more-examples)
  - [*Romeo and Juliet* Full Text Extraction](#romeo-and-juliet-full-text-extraction)
  - [Medication Extraction](#medication-extraction)
  - [Radiology Report Structuring: RadExtract](#radiology-report-structuring-radextract)
- [Community Providers](#community-providers)
- [Contributing](#contributing)
- [Testing](#testing)
- [Disclaimer](#disclaimer)

## Introduction

LangExtract is a Python library that uses LLMs to extract structured information from unstructured text documents based on user-defined instructions. It processes materials such as clinical notes or reports, identifying and organizing key details while ensuring the extracted data corresponds to the source text.

## Why LangExtract?

1.  **Precise Source Grounding:** Maps every extraction to its exact location in the source text, enabling visual highlighting for easy traceability and verification.
2.  **Reliable Structured Outputs:** Enforces a consistent output schema based on your few-shot examples, leveraging controlled generation in supported models like Gemini to guarantee robust, structured results.
3.  **Optimized for Long Documents:** Overcomes the &quot;needle-in-a-haystack&quot; challenge of large document extraction by using an optimized strategy of text chunking, parallel processing, and multiple passes for higher recall.
4.  **Interactive Visualization:** Instantly generates a self-contained, interactive HTML file to visualize and review thousands of extracted entities in their original context.
5.  **Flexible LLM Support:** Supports your preferred models, from cloud-based LLMs like the Google Gemini family to local open-source models via the built-in Ollama interface.
6.  **Adaptable to Any Domain:** Define extraction tasks for any domain using just a few examples. LangExtract adapts to your needs without requiring any model fine-tuning.
7.  **Leverages LLM World Knowledge:** Utilize precise prompt wording and few-shot examples to influence how the extraction task may utilize LLM knowledge. The accuracy of any inferred information and its adherence to the task specification are contingent upon the selected LLM, the complexity of the task, the clarity of the prompt instructions, and the nature of the prompt examples.

## Quick Start

&gt; **Note:** Using cloud-hosted models like Gemini requires an API key. See the [API Key Setup](#api-key-setup-for-cloud-models) section for instructions on how to get and configure your key.

Extract structured information with just a few lines of code.

### 1. Define Your Extraction Task

First, create a prompt that clearly describes what you want to extract. Then, provide a high-quality example to guide the model.

```python
import langextract as lx
import textwrap

# 1. Define the prompt and extraction rules
prompt = textwrap.dedent(&quot;&quot;&quot;\
    Extract characters, emotions, and relationships in order of appearance.
    Use exact text for extractions. Do not paraphrase or overlap entities.
    Provide meaningful attributes for each entity to add context.&quot;&quot;&quot;)

# 2. Provide a high-quality example to guide the model
examples = [
    lx.data.ExampleData(
        text=&quot;ROMEO. But soft! What light through yonder window breaks? It is the east, and Juliet is the sun.&quot;,
        extractions=[
            lx.data.Extraction(
                extraction_class=&quot;character&quot;,
                extraction_text=&quot;ROMEO&quot;,
                attributes={&quot;emotional_state&quot;: &quot;wonder&quot;}
            ),
            lx.data.Extraction(
                extraction_class=&quot;emotion&quot;,
                extraction_text=&quot;But soft!&quot;,
                attributes={&quot;feeling&quot;: &quot;gentle awe&quot;}
            ),
            lx.data.Extraction(
                extraction_class=&quot;relationship&quot;,
                extraction_text=&quot;Juliet is the sun&quot;,
                attributes={&quot;type&quot;: &quot;metaphor&quot;}
            ),
        ]
    )
]
```

&gt; **Note:** Examples drive model behavior. Each `extraction_text` should ideally be verbatim from the example&#039;s `text` (no paraphrasing), listed in order of appearance. LangExtract raises `Prompt alignment` warnings by default if examples don&#039;t follow this pattern‚Äîresolve these for best results.

### 2. Run the Extraction

Provide your input text and the prompt materials to the `lx.extract` function.

```python
# The input text to be processed
input_text = &quot;Lady Juliet gazed longingly at the stars, her heart aching for Romeo&quot;

# Run the extraction
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gemini-2.5-flash&quot;,
)
```

&gt; **Model Selection**: `gemini-2.5-flash` is the recommended default, offering an excellent balance of speed, cost, and quality. For highly complex tasks requiring deeper reasoning, `gemini-2.5-pro` may provide superior results. For large-scale or production use, a Tier 2 Gemini quota is suggested to increase throughput and avoid rate limits. See the [rate-limit documentation](https://ai.google.dev/gemini-api/docs/rate-limits#tier-2) for details.
&gt;
&gt; **Model Lifecycle**: Note that Gemini models have a lifecycle with defined retirement dates. Users should consult the [official model version documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/model-versions) to stay informed about the latest stable and legacy versions.

### 3. Visualize the Results

The extractions can be saved to a `.jsonl` file, a popular format for working with language model data. LangExtract can then generate an interactive HTML visualization from this file to review the entities in context.

```python
# Save the results to a JSONL file
lx.io.save_annotated_documents([result], output_name=&quot;extraction_results.jsonl&quot;, output_dir=&quot;.&quot;)

# Generate the visualization from the file
html_content = lx.visualize(&quot;extraction_results.jsonl&quot;)
with open(&quot;visualization.html&quot;, &quot;w&quot;) as f:
    if hasattr(html_content, &#039;data&#039;):
        f.write(html_content.data)  # For Jupyter/Colab
    else:
        f.write(html_content)
```

This creates an animated and interactive HTML file:

![Romeo and Juliet Basic Visualization ](https://raw.githubusercontent.com/google/langextract/main/docs/_static/romeo_juliet_basic.gif)

&gt; **Note on LLM Knowledge Utilization:** This example demonstrates extractions that stay close to the text evidence - extracting &quot;longing&quot; for Lady Juliet&#039;s emotional state and identifying &quot;yearning&quot; from &quot;gazed longingly at the stars.&quot; The task could be modified to generate attributes that draw more heavily from the LLM&#039;s world knowledge (e.g., adding `&quot;identity&quot;: &quot;Capulet family daughter&quot;` or `&quot;literary_context&quot;: &quot;tragic heroine&quot;`). The balance between text-evidence and knowledge-inference is controlled by your prompt instructions and example attributes.

### Scaling to Longer Documents

For larger texts, you can process entire documents directly from URLs with parallel processing and enhanced sensitivity:

```python
# Process Romeo &amp; Juliet directly from Project Gutenberg
result = lx.extract(
    text_or_documents=&quot;https://www.gutenberg.org/files/1513/1513-0.txt&quot;,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gemini-2.5-flash&quot;,
    extraction_passes=3,    # Improves recall through multiple passes
    max_workers=20,         # Parallel processing for speed
    max_char_buffer=1000    # Smaller contexts for better accuracy
)
```

This approach can extract hundreds of entities from full novels while maintaining high accuracy. The interactive visualization seamlessly handles large result sets, making it easy to explore hundreds of entities from the output JSONL file. **[See the full *Romeo and Juliet* extraction example ‚Üí](https://github.com/google/langextract/blob/main/docs/examples/longer_text_example.md)** for detailed results and performance insights.

### Vertex AI Batch Processing

Save costs on large-scale tasks by enabling Vertex AI Batch API: `language_model_params={&quot;vertexai&quot;: True, &quot;batch&quot;: {&quot;enabled&quot;: True}}`.

See an example of the Vertex AI Batch API usage in [this example](docs/examples/batch_api_example.md).

## Installation

### From PyPI

```bash
pip install langextract
```

*Recommended for most users. For isolated environments, consider using a virtual environment:*

```bash
python -m venv langextract_env
source langextract_env/bin/activate  # On Windows: langextract_env\Scripts\activate
pip install langextract
```

### From Source

LangExtract uses modern Python packaging with `pyproject.toml` for dependency management:

*Installing with `-e` puts the package in development mode, allowing you to modify the code without reinstalling.*


```bash
git clone https://github.com/google/langextract.git
cd langextract

# For basic installation:
pip install -e .

# For development (includes linting tools):
pip install -e &quot;.[dev]&quot;

# For testing (includes pytest):
pip install -e &quot;.[test]&quot;
```

### Docker

```bash
docker build -t langextract .
docker run --rm -e LANGEXTRACT_API_KEY=&quot;your-api-key&quot; langextract python your_script.py
```

## API Key Setup for Cloud Models

When using LangExtract with cloud-hosted models (like Gemini or OpenAI), you&#039;ll need to
set up an API key. On-device models don&#039;t require an API key. For developers
using local LLMs, LangExtract offers built-in support for Ollama and can be
extended to other third-party APIs by updating the inference endpoints.

### API Key Sources

Get API keys from:

*   [AI Studio](https://aistudio.google.com/app/apikey) for Gemini models
*   [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/sdks/overview) for enterprise use
*   [OpenAI Platform](https://platform.openai.com/api-keys) for OpenAI models

### Setting up API key in your environment

**Option 1: Environment Variable**

```bash
export LANGEXTRACT_API_KEY=&quot;your-api-key-here&quot;
```

**Option 2: .env File (Recommended)**

Add your API key to a `.env` file:

```bash
# Add API key to .env file
cat &gt;&gt; .env &lt;&lt; &#039;EOF&#039;
LANGEXTRACT_API_KEY=your-api-key-here
EOF

# Keep your API key secure
echo &#039;.env&#039; &gt;&gt; .gitignore
```

In your Python code:
```python
import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=&quot;Extract information...&quot;,
    examples=[...],
    model_id=&quot;gemini-2.5-flash&quot;
)
```

**Option 3: Direct API Key (Not Recommended for Production)**

You can also provide the API key directly in your code, though this is not recommended for production use:

```python
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=&quot;Extract information...&quot;,
    examples=[...],
    model_id=&quot;gemini-2.5-flash&quot;,
    api_key=&quot;your-api-key-here&quot;  # Only use this for testing/development
)
```

**Option 4: Vertex AI (Service Accounts)**

Use [Vertex AI](https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform) for authentication with service accounts:

```python
result = lx.extract(
    text_or_documents=input_text,
    prompt_description=&quot;Extract information...&quot;,
    examples=[...],
    model_id=&quot;gemini-2.5-flash&quot;,
    language_model_params={
        &quot;vertexai&quot;: True,
        &quot;project&quot;: &quot;your-project-id&quot;,
        &quot;location&quot;: &quot;global&quot;  # or regional endpoint
    }
)
```

## Adding Custom Model Providers

LangExtract supports custom LLM providers via a lightweight plugin system. You can add support for new models without changing core code.

- Add new model support independently of the core library
- Distribute your provider as a separate Python package
- Keep custom dependencies isolated
- Override or extend built-in providers via priority-based resolution

See the detailed guide in [Provider System Documentation](langextract/providers/README.md) to learn how to:

- Register a provider with `@registry.register(...)`
- Publish an entry point for discovery
- Optionally provide a schema with `get_schema_class()` for structured output
- Integrate with the factory via `create_model(...)`

## Using OpenAI Models

LangExtract supports OpenAI models (requires optional dependency: `pip install langextract[openai]`):

```python
import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gpt-4o&quot;,  # Automatically selects OpenAI provider
    api_key=os.environ.get(&#039;OPENAI_API_KEY&#039;),
    fence_output=True,
    use_schema_constraints=False
)
```

Note: OpenAI models require `fence_output=True` and `use_schema_constraints=False` because LangExtract doesn&#039;t implement schema constraints for OpenAI yet.

## Using Local LLMs with Ollama
LangExtract supports local inference using Ollama, allowing you to run models without API keys:

```python
import langextract as lx

result = lx.extract(
    text_or_documents=input_text,
    prompt_description=prompt,
    examples=examples,
    model_id=&quot;gemma2:2b&quot;,  # Automatically selects Ollama provider
    model_url=&quot;http://localhost:11434&quot;,
    fence_output=False,
    use_schema_constraints=False
)
```

**Quick setup:** Install Ollama from [ollama.com](https://ollama.com/), run `ollama pull gemma2:2b`, then `ollama serve`.

For detailed installation, Docker setup, and examples, see [`examples/ollama/`](examples/ollama/).

## More Examples

Additional examples of LangExtract in action:

### *Romeo and Juliet* Full Text Extraction

LangExtract can process complete documents directly from URLs. This example demonstrates extraction from the full text of *Romeo and Juliet* from Project Gutenberg (147,843 characters), showing parallel processing, sequential extraction passes, and performance optimization for long document processing.

**[View *Romeo and Juliet* Full Text Example ‚Üí](https://github.com/google/langextract/blob/main/docs/examples/longer_text_example.md)**

### Medication Extraction

&gt; **Disclaimer:** This demonstration is for illustrative purposes of LangExtract&#039;s baseline capability only. It does not represent a finished or approved product, is not intended to diagnose or suggest treatment of any disease or condition, and should not be used for medical advice.

LangExtract excels at extracting structured medical information from clinical text. These examples demonstrate both basic entity recognition (medication names, dosages, routes) and relationship extraction (connecting medications to their attributes), showing LangExtract&#039;s effectiveness for healthcare applications.

**[View Medication Examples ‚Üí](https://github.com/google/langextract/blob/main/docs/examples/medication_examples.md)**

### Radiology Report Structuring: RadExtract

Explore RadExtract, a live interactive demo on HuggingFace Spaces that shows how LangExtract can automatically structure radiology reports. Try it directly in your browser with no setup required.

**[View RadExtract Demo ‚Üí](https://huggingface.co/spaces/google/radextract)**

## Community Providers

Extend LangExtract with custom model providers! Check out our [Community Provider Plugins](COMMUNITY_PROVIDERS.md) registry to discover providers created by the community or add your own.

For detailed instructions on creating a provider plugin, see the [Custom Provider Plugin Example](examples/custom_provider_plugin/).

## Contributing

Contributions are welcome! See [CONTRIBUTING.md](https://github.com/google/langextract/blob/main/CONTRIBUTING.md) to get started
with development, testing, and pull requests. You must sign a
[Contributor License Agreement](https://cla.developers.google.com/about)
before submitting patches.



## Testing

To run tests locally from the source:

```bash
# Clone the repository
git clone https://github.com/google/langextract.git
cd langextract

# Install with test dependencies
pip install -e &quot;.[test]&quot;

# Run all tests
pytest tests
```

Or reproduce the full CI matrix locally with tox:

```bash
tox  # runs pylint + pytest on Python 3.10 and 3.11
```

### Ollama Integration Testing

If you have Ollama installed locally, you can run integration tests:

```bash
# Test Ollama integration (requires Ollama running with gemma2:2b model)
tox -e ollama-integration
```

This test will automatically detect if Ollama is available and run real inference tests.

## Development

### Code Formatting

This project uses automated formatting tools to maintain consistent code style:

```bash
# Auto-format all code
./autoformat.sh

# Or run formatters separately
isort langextract tests --profile google --line-length 80
pyink langextract tests --config pyproject.toml
```

### Pre-commit Hooks

For automatic formatting checks:
```bash
pre-commit install  # One-time setup
pre-commit run --all-files  # Manual run
```

### Linting

Run linting before submitting PRs:

```bash
pylint --rcfile=.pylintrc langextract tests
```

See [CONTRIBUTING.md](CONTRIBUTING.md) for full development guidelines.

## Disclaimer

This is not an officially supported Google product. If you use
LangExtract in production or publications, please cite accordingly and
acknowledge usage. Use is subject to the [Apache 2.0 License](https://github.com/google/langextract/blob/main/LICENSE).
For health-related applications, use of LangExtract is also subject to the
[Health AI Developer Foundations Terms of Use](https://developers.google.com/health-ai-developer-foundations/terms).

---

**Happy Extracting!**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[OpenBMB/VoxCPM]]></title>
            <link>https://github.com/OpenBMB/VoxCPM</link>
            <guid>https://github.com/OpenBMB/VoxCPM</guid>
            <pubDate>Mon, 19 Jan 2026 00:04:58 GMT</pubDate>
            <description><![CDATA[VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenBMB/VoxCPM">OpenBMB/VoxCPM</a></h1>
            <p>VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning</p>
            <p>Language: Python</p>
            <p>Stars: 4,215</p>
            <p>Forks: 509</p>
            <p>Stars today: 238 stars today</p>
            <h2>README</h2><pre>## üéôÔ∏è VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning


[![Project Page](https://img.shields.io/badge/Project%20Page-GitHub-blue)](https://github.com/OpenBMB/VoxCPM/) [![Technical Report](https://img.shields.io/badge/Technical%20Report-Arxiv-red)](https://arxiv.org/abs/2509.24650)[![Live Playground](https://img.shields.io/badge/Live%20PlayGround-Demo-orange)](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) [![Samples](https://img.shields.io/badge/Audio%20Samples-Page-green)](https://openbmb.github.io/VoxCPM-demopage)

#### VoxCPM1.5 Model Weights

 [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-OpenBMB-yellow)](https://huggingface.co/openbmb/VoxCPM1.5) [![ModelScope](https://img.shields.io/badge/ModelScope-OpenBMB-purple)](https://modelscope.cn/models/OpenBMB/VoxCPM1.5)  



&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/voxcpm_logo.png&quot; alt=&quot;VoxCPM Logo&quot; width=&quot;40%&quot;&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

üëã Contact us on [WeChat](assets/wechat.png)

&lt;/div&gt;

## News 
* [2025.12.05] üéâ üéâ üéâ  We Open Source the VoxCPM1.5 [weights](https://huggingface.co/openbmb/VoxCPM1.5)! The model now supports both full-parameter fine-tuning and efficient LoRA fine-tuning, empowering you to create your own tailored version. See [Release Notes](docs/release_note.md) for details.
* [2025.09.30] üî• üî• üî•  We Release VoxCPM [Technical Report](https://arxiv.org/abs/2509.24650)!
* [2025.09.16] üî• üî• üî•  We Open Source the VoxCPM-0.5B [weights](https://huggingface.co/openbmb/VoxCPM-0.5B)!
* [2025.09.16] üéâ üéâ üéâ  We Provide the [Gradio PlayGround](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) for VoxCPM-0.5B, try it now! 

## Overview

VoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization and enables two flagship capabilities: context-aware speech generation and true-to-life zero-shot voice cloning.

Unlike mainstream approaches that convert speech to discrete tokens, VoxCPM uses an end-to-end diffusion autoregressive architecture that directly generates continuous speech representations from text. Built on [MiniCPM-4](https://huggingface.co/openbmb/MiniCPM4-0.5B) backbone, it achieves implicit semantic-acoustic decoupling through hierachical language modeling and FSQ constraints, greatly enhancing both expressiveness and generation stability.

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/voxcpm_model.png&quot; alt=&quot;VoxCPM Model Architecture&quot; width=&quot;90%&quot;&gt;
&lt;/div&gt;


###  üöÄ Key Features
- **Context-Aware, Expressive Speech Generation** - VoxCPM comprehends text to infer and generate appropriate prosody, delivering speech with remarkable expressiveness and natural flow. It spontaneously adapts speaking style based on content, producing highly fitting vocal expression trained on a massive 1.8 million-hour bilingual corpus.
- **True-to-Life Voice Cloning** - With only a short reference audio clip, VoxCPM performs accurate zero-shot voice cloning, capturing not only the speaker&#039;s timbre but also fine-grained characteristics such as accent, emotional tone, rhythm, and pacing to create a faithful and natural replica.
- **High-Efficiency Synthesis** - VoxCPM supports streaming synthesis with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, making it possible for real-time applications.

### üì¶ Model Versions
See [Release Notes](docs/release_note.md) for details
- **VoxCPM1.5** (Latest): 
  - Model Params: 800M
  - Sampling rate of AudioVAE: 44100
  - Token rate in LM Backbone: 6.25Hz (patch-size=4)
  - RTF in a single NVIDIA-RTX 4090 GPU: ~0.15

- **VoxCPM-0.5B** (Original):
  - Model Params: 640M
  - Sampling rate of AudioVAE: 16000
  - Token rate in LM Backbone: 12.5Hz (patch-size=2)
  - RTF in a single NVIDIA-RTX 4090 GPU: 0.17



##  Quick Start

### üîß Install from PyPI
``` sh
pip install voxcpm
```
### 1.  Model Download (Optional)
By default, when you first run the script, the model will be downloaded automatically, but you can also download the model in advance.
- Download VoxCPM1.5
    ```
    from huggingface_hub import snapshot_download
    snapshot_download(&quot;openbmb/VoxCPM1.5&quot;)
    ```

- Or Download VoxCPM-0.5B
    ```
    from huggingface_hub import snapshot_download
    snapshot_download(&quot;openbmb/VoxCPM-0.5B&quot;)
    ```
- Download ZipEnhancer and SenseVoice-Small. We use ZipEnhancer to enhance speech prompts and SenseVoice-Small for speech prompt ASR in the web demo. 
    ```
    from modelscope import snapshot_download
    snapshot_download(&#039;iic/speech_zipenhancer_ans_multiloss_16k_base&#039;)
    snapshot_download(&#039;iic/SenseVoiceSmall&#039;)
    ```

### 2. Basic Usage
```python
import soundfile as sf
import numpy as np
from voxcpm import VoxCPM

model = VoxCPM.from_pretrained(&quot;openbmb/VoxCPM1.5&quot;)

# Non-streaming
wav = model.generate(
    text=&quot;VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.&quot;,
    prompt_wav_path=None,      # optional: path to a prompt speech for voice cloning
    prompt_text=None,          # optional: reference text
    cfg_value=2.0,             # LM guidance on LocDiT, higher for better adherence to the prompt, but maybe worse
    inference_timesteps=10,   # LocDiT inference timesteps, higher for better result, lower for fast speed
    normalize=False,           # enable external TN tool, but will disable native raw text support
    denoise=False,             # enable external Denoise tool, but it may cause some distortion and restrict the sampling rate to 16kHz
    retry_badcase=True,        # enable retrying mode for some bad cases (unstoppable)
    retry_badcase_max_times=3,  # maximum retrying times
    retry_badcase_ratio_threshold=6.0, # maximum length restriction for bad case detection (simple but effective), it could be adjusted for slow pace speech
)

sf.write(&quot;output.wav&quot;, wav, model.tts_model.sample_rate)
print(&quot;saved: output.wav&quot;)

# Streaming
chunks = []
for chunk in model.generate_streaming(
    text = &quot;Streaming text to speech is easy with VoxCPM!&quot;,
    # supports same args as above
):
    chunks.append(chunk)
wav = np.concatenate(chunks)

sf.write(&quot;output_streaming.wav&quot;, wav, model.tts_model.sample_rate)
print(&quot;saved: output_streaming.wav&quot;)
```

### 3. CLI Usage

After installation, the entry point is `voxcpm` (or use `python -m voxcpm.cli`).

```bash
# 1) Direct synthesis (single text)
voxcpm --text &quot;VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.&quot; --output out.wav

# 2) Voice cloning (reference audio + transcript)
voxcpm --text &quot;VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.&quot; \
  --prompt-audio path/to/voice.wav \
  --prompt-text &quot;reference transcript&quot; \
  --output out.wav \
  # --denoise

# (Optinal) Voice cloning (reference audio + transcript file)
voxcpm --text &quot;VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.&quot; \
  --prompt-audio path/to/voice.wav \
  --prompt-file &quot;/path/to/text-file&quot; \
  --output out.wav \
  # --denoise

# 3) Batch processing (one text per line)
voxcpm --input examples/input.txt --output-dir outs
# (optional) Batch + cloning
voxcpm --input examples/input.txt --output-dir outs \
  --prompt-audio path/to/voice.wav \
  --prompt-text &quot;reference transcript&quot; \
  # --denoise

# 4) Inference parameters (quality/speed)
voxcpm --text &quot;...&quot; --output out.wav \
  --cfg-value 2.0 --inference-timesteps 10 --normalize

# 5) Model loading
# Prefer local path
voxcpm --text &quot;...&quot; --output out.wav --model-path /path/to/VoxCPM_model_dir
# Or from Hugging Face (auto download/cache)
voxcpm --text &quot;...&quot; --output out.wav \
  --hf-model-id openbmb/VoxCPM1.5 --cache-dir ~/.cache/huggingface --local-files-only

# 6) Denoiser control
voxcpm --text &quot;...&quot; --output out.wav \
  --no-denoiser --zipenhancer-path iic/speech_zipenhancer_ans_multiloss_16k_base

# 7) Help
voxcpm --help
python -m voxcpm.cli --help
```

### 4. Start web demo

You can start the UI interface by running `python app.py`, which allows you to perform Voice Cloning and Voice Creation.

### 5. Fine-tuning

VoxCPM1.5 supports both full fine-tuning (SFT) and LoRA fine-tuning, allowing you to train personalized voice models on your own data. See the [Fine-tuning Guide](docs/finetune.md) for detailed instructions.

**Quick Start:**
```bash
# Full fine-tuning
python scripts/train_voxcpm_finetune.py \
    --config_path conf/voxcpm_v1.5/voxcpm_finetune_all.yaml

# LoRA fine-tuning
python scripts/train_voxcpm_finetune.py \
    --config_path conf/voxcpm_v1.5/voxcpm_finetune_lora.yaml
```

## üìö Documentation

- **[Usage Guide](docs/usage_guide.md)** - Detailed guide on how to use VoxCPM effectively, including text input modes, voice cloning tips, and parameter tuning
- **[Fine-tuning Guide](docs/finetune.md)** - Complete guide for fine-tuning VoxCPM models with SFT and LoRA
- **[Release Notes](docs/release_note.md)** - Version history and updates
- **[Performance Benchmarks](docs/performance.md)** - Detailed performance comparisons on public benchmarks

---

## üìö More Information

###  üåü Community Projects
We&#039;re excited to see the VoxCPM community growing! Here are some amazing projects and features built by our community:
- **[ComfyUI-VoxCPM](https://github.com/wildminder/ComfyUI-VoxCPM)** A VoxCPM extension for ComfyUI.
- **[ComfyUI-VoxCPMTTS](https://github.com/1038lab/ComfyUI-VoxCPMTTS)** A VoxCPM extension for ComfyUI.
- **[WebUI-VoxCPM](https://github.com/rsxdalv/tts_webui_extension.vox_cpm)** A template extension for TTS WebUI.
- **[PR: Streaming API Support (by AbrahamSanders)](https://github.com/OpenBMB/VoxCPM/pull/26)** 
- **[VoxCPM-NanoVLLM](https://github.com/a710128/nanovllm-voxcpm)** NanoVLLM integration for VoxCPM for faster, high-throughput inference on GPU.
- **[VoxCPM-ONNX](https://github.com/bluryar/VoxCPM-ONNX)** ONNX export for VoxCPM supports faster CPU inference.
- **[VoxCPMANE](https://github.com/0seba/VoxCPMANE)** VoxCPM TTS with Apple Neural Engine backend server.
- **[PR: LoRA finetune web UI (by Ayin1412)](https://github.com/OpenBMB/VoxCPM/pull/100)**
- **[voxcpm_rs](https://github.com/madushan1000/voxcpm_rs)** A re-implementation of VoxCPM-0.5B in Rust.

*Note: The projects are not officially maintained by OpenBMB.*



*Have you built something cool with VoxCPM? We&#039;d love to feature it here! Please open an issue or pull request to add your project.*

### üìä Performance Highlights

VoxCPM achieves competitive results on public zero-shot TTS benchmarks. See [Performance Benchmarks](docs/performance.md) for detailed comparison tables.



## ‚ö†Ô∏è Risks and limitations
- General Model Behavior: While VoxCPM has been trained on a large-scale dataset, it may still produce outputs that are unexpected, biased, or contain artifacts.
- Potential for Misuse of Voice Cloning: VoxCPM&#039;s powerful zero-shot voice cloning capability can generate highly realistic synthetic speech. This technology could be misused for creating convincing deepfakes for purposes of impersonation, fraud, or spreading disinformation. Users of this model must not use it to create content that infringes upon the rights of individuals. It is strictly forbidden to use VoxCPM for any illegal or unethical purposes. We strongly recommend that any publicly shared content generated with this model be clearly marked as AI-generated.
- Current Technical Limitations: Although generally stable, the model may occasionally exhibit instability, especially with very long or expressive inputs. Furthermore, the current version offers limited direct control over specific speech attributes like emotion or speaking style.
- Bilingual Model: VoxCPM is trained primarily on Chinese and English data. Performance on other languages is not guaranteed and may result in unpredictable or low-quality audio.
- This model is released for research and development purposes only. We do not recommend its use in production or commercial applications without rigorous testing and safety evaluations. Please use VoxCPM responsibly.

---

## üìù TO-DO List
Please stay tuned for updates!
- [x] Release the VoxCPM technical report.
- [x] Support higher sampling rate (44.1kHz in VoxCPM-1.5).
- [x] Support SFT and LoRA fine-tuning.
- [] Multilingual Support (besides ZH/EN).
- [] Controllable Speech Generation by Human Instruction.



## üìÑ License
The VoxCPM model weights and code are open-sourced under the [Apache-2.0](LICENSE) license.

## üôè Acknowledgments

We extend our sincere gratitude to the following works and resources for their inspiration and contributions:

- [DiTAR](https://arxiv.org/abs/2502.03930) for the diffusion autoregressive backbone used in speech generation
- [MiniCPM-4](https://github.com/OpenBMB/MiniCPM) for serving as the language model foundation
- [CosyVoice](https://github.com/FunAudioLLM/CosyVoice) for the implementation of Flow Matching-based LocDiT
- [DAC](https://github.com/descriptinc/descript-audio-codec) for providing the Audio VAE backbone

## Institutions

This project is developed by the following institutions:
- &lt;img src=&quot;assets/modelbest_logo.png&quot; width=&quot;28px&quot;&gt; [ModelBest](https://modelbest.cn/)

- &lt;img src=&quot;assets/thuhcsi_logo.png&quot; width=&quot;28px&quot;&gt; [THUHCSI](https://github.com/thuhcsi)


## ‚≠ê Star History
 [![Star History Chart](https://api.star-history.com/svg?repos=OpenBMB/VoxCPM&amp;type=Date)](https://star-history.com/#OpenBMB/VoxCPM&amp;Date)


## üìö Citation

If you find our model helpful, please consider citing our projects üìù and staring us ‚≠êÔ∏èÔºÅ

```bib
@article{voxcpm2025,
  title        = {VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning},
  author       = {Zhou, Yixuan and Zeng, Guoyang and Liu, Xin and Li, Xiang and Yu, Renjie and Wang, Ziyang and Ye, Runchuan and Sun, Weiyue and Gui, Jiancheng and Li, Kehan and Wu, Zhiyong  and Liu, Zhiyuan},
  journal      = {arXiv preprint arXiv:2509.24650},
  year         = {2025},
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[yichuan-w/LEANN]]></title>
            <link>https://github.com/yichuan-w/LEANN</link>
            <guid>https://github.com/yichuan-w/LEANN</guid>
            <pubDate>Mon, 19 Jan 2026 00:04:57 GMT</pubDate>
            <description><![CDATA[RAG on Everything with LEANN. Enjoy 97% storage savings while running a fast, accurate, and 100% private RAG application on your personal device.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/yichuan-w/LEANN">yichuan-w/LEANN</a></h1>
            <p>RAG on Everything with LEANN. Enjoy 97% storage savings while running a fast, accurate, and 100% private RAG application on your personal device.</p>
            <p>Language: Python</p>
            <p>Stars: 8,958</p>
            <p>Forks: 783</p>
            <p>Stars today: 144 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/logo-text.png&quot; alt=&quot;LEANN Logo&quot; width=&quot;400&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Python-3.9%20%7C%203.10%20%7C%203.11%20%7C%203.12%20%7C%203.13-blue.svg&quot; alt=&quot;Python Versions&quot;&gt;
  &lt;img src=&quot;https://github.com/yichuan-w/LEANN/actions/workflows/build-and-publish.yml/badge.svg&quot; alt=&quot;CI Status&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Platform-Ubuntu%20%26%20Arch%20%26%20WSL%20%7C%20macOS%20(ARM64%2FIntel)-lightgrey&quot; alt=&quot;Platform&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/License-MIT-green.svg&quot; alt=&quot;MIT License&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/MCP-Native%20Integration-blue&quot; alt=&quot;MCP Integration&quot;&gt;
  &lt;a href=&quot;https://join.slack.com/t/leann-e2u9779/shared_invite/zt-3ckd2f6w1-OX08~NN4gkWhh10PRVBj1Q&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Slack-Join-4A154B?logo=slack&amp;logoColor=white&quot; alt=&quot;Join Slack&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;assets/wechat_user_group.JPG&quot; title=&quot;Join WeChat group&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/WeChat-Join-2DC100?logo=wechat&amp;logoColor=white&quot; alt=&quot;Join WeChat group&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://forms.gle/rDbZf864gMNxhpTq8&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/üì£_Community_Survey-Help_Shape_v0.4-007ec6?style=for-the-badge&amp;logo=google-forms&amp;logoColor=white&quot; alt=&quot;Take Survey&quot;&gt;
  &lt;/a&gt;
  &lt;p&gt;
    We track &lt;b&gt;zero telemetry&lt;/b&gt;. This survey is the ONLY way to tell us if you want &lt;br&gt;
    &lt;b&gt;GPU Acceleration&lt;/b&gt; or &lt;b&gt;More Integrations&lt;/b&gt; next.&lt;br&gt;
    üëâ &lt;a href=&quot;https://forms.gle/rDbZf864gMNxhpTq8&quot;&gt;&lt;b&gt;Click here to cast your vote (2 mins)&lt;/b&gt;&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

&lt;h2 align=&quot;center&quot; tabindex=&quot;-1&quot; class=&quot;heading-element&quot; dir=&quot;auto&quot;&gt;
    The smallest vector index in the world. RAG Everything with LEANN!
&lt;/h2&gt;

LEANN is an innovative vector database that democratizes personal AI. Transform your laptop into a powerful RAG system that can index and search through millions of documents while using **97% less storage** than traditional solutions **without accuracy loss**.


LEANN achieves this through *graph-based selective recomputation* with *high-degree preserving pruning*, computing embeddings on-demand instead of storing them all. [Illustration Fig ‚Üí](#Ô∏è-architecture--how-it-works) | [Paper ‚Üí](https://arxiv.org/abs/2506.08276)

**Ready to RAG Everything?** Transform your laptop into a personal AI assistant that can semantic search your **[file system](#-personal-data-manager-process-any-documents-pdf-txt-md)**, **[emails](#-your-personal-email-secretary-rag-on-apple-mail)**, **[browser history](#-time-machine-for-the-web-rag-your-entire-browser-history)**, **[chat history](#-wechat-detective-unlock-your-golden-memories)** ([WeChat](#-wechat-detective-unlock-your-golden-memories), [iMessage](#-imessage-history-your-personal-conversation-archive)), **[agent memory](#-chatgpt-chat-history-your-personal-ai-conversation-archive)** ([ChatGPT](#-chatgpt-chat-history-your-personal-ai-conversation-archive), [Claude](#-claude-chat-history-your-personal-ai-conversation-archive)), **[live data](#mcp-integration-rag-on-live-data-from-any-platform)** ([Slack](#slack-messages-search-your-team-conversations), [Twitter](#-twitter-bookmarks-your-personal-tweet-library)), **[codebase](#-claude-code-integration-transform-your-development-workflow)**\* , or external knowledge bases (i.e., 60M documents) - all on your laptop, with zero cloud costs and complete privacy.


\* Claude Code only supports basic `grep`-style keyword search. **LEANN** is a drop-in **semantic search MCP service fully compatible with Claude Code**, unlocking intelligent retrieval without changing your workflow. üî• Check out [the easy setup ‚Üí](packages/leann-mcp/README.md)



## Why LEANN?

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/effects.png&quot; alt=&quot;LEANN vs Traditional Vector DB Storage Comparison&quot; width=&quot;70%&quot;&gt;
&lt;/p&gt;

&gt; **The numbers speak for themselves:** Index 60 million text chunks in just 6GB instead of 201GB. From emails to browser history, everything fits on your laptop. [See detailed benchmarks for different applications below ‚Üì](#-storage-comparison)


üîí **Privacy:** Your data never leaves your laptop. No OpenAI, no cloud, no &quot;terms of service&quot;.

ü™∂ **Lightweight:** Graph-based recomputation eliminates heavy embedding storage, while smart graph pruning and CSR format minimize graph storage overhead. Always less storage, less memory usage!

üì¶ **Portable:** Transfer your entire knowledge base between devices (even with others) with minimal cost - your personal AI memory travels with you.

üìà **Scalability:** Handle messy personal data that would crash traditional vector DBs, easily managing your growing personalized data and agent generated memory!

‚ú® **No Accuracy Loss:** Maintain the same search quality as heavyweight solutions while using 97% less storage.

## Installation

### üì¶ Prerequisites: Install uv

[Install uv](https://docs.astral.sh/uv/getting-started/installation/#installation-methods) first if you don&#039;t have it. Typically, you can install it with:

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### üöÄ Quick Install

Clone the repository to access all examples and try amazing applications,

```bash
git clone https://github.com/yichuan-w/LEANN.git leann
cd leann
```

and install LEANN from [PyPI](https://pypi.org/project/leann/) to run them immediately:

```bash
uv venv
source .venv/bin/activate
uv pip install leann
```

&lt;!--
&gt; Low-resource? See &quot;Low-resource setups&quot; in the [Configuration Guide](docs/configuration-guide.md#low-resource-setups). --&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;strong&gt;üîß Build from Source (Recommended for development)&lt;/strong&gt;
&lt;/summary&gt;



```bash
git clone https://github.com/yichuan-w/LEANN.git leann
cd leann
git submodule update --init --recursive
```

**macOS:**

Note: DiskANN requires MacOS 13.3 or later.

```bash
brew install libomp boost protobuf zeromq pkgconf
uv sync --extra diskann
```

**Linux (Ubuntu/Debian):**

Note: On Ubuntu 20.04, you may need to build a newer Abseil and pin Protobuf (e.g., v3.20.x) for building DiskANN. See [Issue #30](https://github.com/yichuan-w/LEANN/issues/30) for a step-by-step note.

You can manually install [Intel oneAPI MKL](https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html) instead of `libmkl-full-dev` for DiskANN. You can also use `libopenblas-dev` for building HNSW only, by removing `--extra diskann` in the command below.

```bash
sudo apt-get update &amp;&amp; sudo apt-get install -y \
  libomp-dev libboost-all-dev protobuf-compiler libzmq3-dev \
  pkg-config libabsl-dev libaio-dev libprotobuf-dev \
  libmkl-full-dev

uv sync --extra diskann
```

**Linux (Arch Linux):**

```bash
sudo pacman -Syu &amp;&amp; sudo pacman -S --needed base-devel cmake pkgconf git gcc \
  boost boost-libs protobuf abseil-cpp libaio zeromq

# For MKL in DiskANN
sudo pacman -S --needed base-devel git
git clone https://aur.archlinux.org/paru-bin.git
cd paru-bin &amp;&amp; makepkg -si
paru -S intel-oneapi-mkl intel-oneapi-compiler
source /opt/intel/oneapi/setvars.sh

uv sync --extra diskann
```

**Linux (RHEL / CentOS Stream / Oracle / Rocky / AlmaLinux):**

See [Issue #50](https://github.com/yichuan-w/LEANN/issues/50) for more details.

```bash
sudo dnf groupinstall -y &quot;Development Tools&quot;
sudo dnf install -y libomp-devel boost-devel protobuf-compiler protobuf-devel \
  abseil-cpp-devel libaio-devel zeromq-devel pkgconf-pkg-config

# For MKL in DiskANN
sudo dnf install -y intel-oneapi-mkl intel-oneapi-mkl-devel \
  intel-oneapi-openmp || sudo dnf install -y intel-oneapi-compiler
source /opt/intel/oneapi/setvars.sh

uv sync --extra diskann
```

&lt;/details&gt;


## Quick Start

Our declarative API makes RAG as easy as writing a config file.

Check out [demo.ipynb](demo.ipynb) or [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yichuan-w/LEANN/blob/main/demo.ipynb)

```python
from leann import LeannBuilder, LeannSearcher, LeannChat
from pathlib import Path
INDEX_PATH = str(Path(&quot;./&quot;).resolve() / &quot;demo.leann&quot;)

# Build an index
builder = LeannBuilder(backend_name=&quot;hnsw&quot;)
builder.add_text(&quot;LEANN saves 97% storage compared to traditional vector databases.&quot;)
builder.add_text(&quot;Tung Tung Tung Sahur called‚Äîthey need their banana‚Äëcrocodile hybrid back&quot;)
builder.build_index(INDEX_PATH)

# Search
searcher = LeannSearcher(INDEX_PATH)
results = searcher.search(&quot;fantastical AI-generated creatures&quot;, top_k=1)

# Chat with your data
chat = LeannChat(INDEX_PATH, llm_config={&quot;type&quot;: &quot;hf&quot;, &quot;model&quot;: &quot;Qwen/Qwen3-0.6B&quot;})
response = chat.ask(&quot;How much storage does LEANN save?&quot;, top_k=1)
```

## RAG on Everything!

LEANN supports RAG on various data sources including documents (`.pdf`, `.txt`, `.md`), Apple Mail, Google Search History, WeChat, ChatGPT conversations, Claude conversations, iMessage conversations, and **live data from any platform through MCP (Model Context Protocol) servers** - including Slack, Twitter, and more.



### Generation Model Setup

#### LLM Backend

LEANN supports many LLM providers for text generation (HuggingFace, Ollama, Anthropic, and Any OpenAI compatible API).


&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üîë OpenAI API Setup (Default)&lt;/strong&gt;&lt;/summary&gt;

Set your OpenAI API key as an environment variable:

```bash
export OPENAI_API_KEY=&quot;your-api-key-here&quot;
```

Make sure to use `--llm openai` flag when using the CLI.
You can also specify the model name with `--llm-model &lt;model-name&gt;` flag.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üõ†Ô∏è Supported LLM &amp; Embedding Providers (via OpenAI Compatibility)&lt;/strong&gt;&lt;/summary&gt;

Thanks to the widespread adoption of the OpenAI API format, LEANN is compatible out-of-the-box with a vast array of LLM and embedding providers. Simply set the `OPENAI_BASE_URL` and `OPENAI_API_KEY` environment variables to connect to your preferred service.

```sh
export OPENAI_API_KEY=&quot;xxx&quot;
export OPENAI_BASE_URL=&quot;http://localhost:1234/v1&quot; # base url of the provider
```

To use OpenAI compatible endpoint with the CLI interface:

If you are using it for text generation, make sure to use `--llm openai` flag and specify the model name with `--llm-model &lt;model-name&gt;` flag.

If you are using it for embedding, set the `--embedding-mode openai` flag and specify the model name with `--embedding-model &lt;MODEL&gt;`.

-----


Below is a list of base URLs for common providers to get you started.


### üñ•Ô∏è Local Inference Engines (Recommended for full privacy)

| Provider         | Sample Base URL             |
| ---------------- | --------------------------- |
| **Ollama** | `http://localhost:11434/v1` |
| **LM Studio** | `http://localhost:1234/v1`  |
| **vLLM** | `http://localhost:8000/v1`  |
| **llama.cpp** | `http://localhost:8080/v1`  |
| **SGLang** | `http://localhost:30000/v1` |
| **LiteLLM** | `http://localhost:4000`     |

-----

### ‚òÅÔ∏è Cloud Providers

&gt; **üö® A Note on Privacy:** Before choosing a cloud provider, carefully review their privacy and data retention policies. Depending on their terms, your data may be used for their own purposes, including but not limited to human reviews and model training, which can lead to serious consequences if not handled properly.


| Provider         | Base URL                                                   |
| ---------------- | ---------------------------------------------------------- |
| **OpenAI** | `https://api.openai.com/v1`                                |
| **OpenRouter** | `https://openrouter.ai/api/v1`                             |
| **Gemini** | `https://generativelanguage.googleapis.com/v1beta/openai/` |
| **x.AI (Grok)** | `https://api.x.ai/v1`                                      |
| **Groq AI** | `https://api.groq.com/openai/v1`                           |
| **DeepSeek** | `https://api.deepseek.com/v1`                              |
| **SiliconFlow** | `https://api.siliconflow.cn/v1`                            |
| **Zhipu (BigModel)** | `https://open.bigmodel.cn/api/paas/v4/`                |
| **Mistral AI** | `https://api.mistral.ai/v1`                                |
| **Anthropic** | `https://api.anthropic.com/v1`                             |
| **Jina AI** (Embeddings) | `https://api.jina.ai/v1`                         |

&gt; **üí° Tip: Separate Embedding Provider**
&gt;
&gt; To use a different provider for embeddings (e.g., Jina AI) while using another for LLM, use `--embedding-api-base` and `--embedding-api-key`:
&gt; ```bash
&gt; leann build my-index --docs ./docs \
&gt;   --embedding-mode openai \
&gt;   --embedding-model jina-embeddings-v3 \
&gt;   --embedding-api-base https://api.jina.ai/v1 \
&gt;   --embedding-api-key $JINA_API_KEY
&gt; ```

If your provider isn&#039;t on this list, don&#039;t worry! Check their documentation for an OpenAI-compatible endpoint‚Äîchances are, it&#039;s OpenAI Compatible too!

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üîß Ollama Setup (Recommended for full privacy)&lt;/strong&gt;&lt;/summary&gt;

**macOS:**

First, [download Ollama for macOS](https://ollama.com/download/mac).

```bash
# Pull a lightweight model (recommended for consumer hardware)
ollama pull llama3.2:1b
```

**Linux:**

```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama service manually
ollama serve &amp;

# Pull a lightweight model (recommended for consumer hardware)
ollama pull llama3.2:1b
```

&lt;/details&gt;


## ‚≠ê Flexible Configuration

LEANN provides flexible parameters for embedding models, search strategies, and data processing to fit your specific needs.

üìö **Need configuration best practices?** Check our [Configuration Guide](docs/configuration-guide.md) for detailed optimization tips, model selection advice, and solutions to common issues like slow embeddings or poor search quality.

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üìã Click to expand: Common Parameters (Available in All Examples)&lt;/strong&gt;&lt;/summary&gt;

All RAG examples share these common parameters. **Interactive mode** is available in all examples - simply run without `--query` to start a continuous Q&amp;A session where you can ask multiple questions. Type &#039;quit&#039; to exit.

```bash
# Environment Variables (GPU Device Selection)
LEANN_EMBEDDING_DEVICE       # GPU for embedding model (e.g., cuda:0, cuda:1, cpu)
LEANN_LLM_DEVICE             # GPU for HFChat LLM (e.g., cuda:1, or &quot;cuda&quot; for multi-GPU auto)

# Core Parameters (General preprocessing for all examples)
--index-dir DIR              # Directory to store the index (default: current directory)
--query &quot;YOUR QUESTION&quot;      # Single query mode. Omit for interactive chat (type &#039;quit&#039; to exit), and now you can play with your index interactively
--max-items N                # Limit data preprocessing (default: -1, process all data)
--force-rebuild              # Force rebuild index even if it exists

# Embedding Parameters
--embedding-model MODEL      # e.g., facebook/contriever, text-embedding-3-small, mlx-community/Qwen3-Embedding-0.6B-8bit or nomic-embed-text
--embedding-mode MODE        # sentence-transformers, openai, mlx, or ollama

# LLM Parameters (Text generation models)
--llm TYPE                   # LLM backend: openai, ollama, hf, or anthropic (default: openai)
--llm-model MODEL            # Model name (default: gpt-4o) e.g., gpt-4o-mini, llama3.2:1b, Qwen/Qwen2.5-1.5B-Instruct
--thinking-budget LEVEL      # Thinking budget for reasoning models: low/medium/high (supported by o3, o3-mini, GPT-Oss:20b, and other reasoning models)

# Search Parameters
--top-k N                    # Number of results to retrieve (default: 20)
--search-complexity N        # Search complexity for graph traversal (default: 32)

# Chunking Parameters
--chunk-size N               # Size of text chunks (default varies by source: 256 for most, 192 for WeChat)
--chunk-overlap N            # Overlap between chunks (default varies: 25-128 depending on source)

# Index Building Parameters
--backend-name NAME          # Backend to use: hnsw or diskann (default: hnsw)
--graph-degree N             # Graph degree for index construction (default: 32)
--build-complexity N         # Build complexity for index construction (default: 64)
--compact / --no-compact     # Use compact storage (default: true). Must be `no-compact` for `no-recompute` build.
--recompute / --no-recompute # Enable/disable embedding recomputation (default: enabled). Should not do a `no-recompute` search in a `recompute` build.
```

&lt;/details&gt;

### üìÑ Personal Data Manager: Process Any Documents (`.pdf`, `.txt`, `.md`)!

Ask questions directly about your personal PDFs, documents, and any directory containing your files!

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;videos/paper_clear.gif&quot; alt=&quot;LEANN Document Search Demo&quot; width=&quot;600&quot;&gt;
&lt;/p&gt;

The example below asks a question about summarizing our paper (uses default data in `data/`, which is a directory with diverse data sources: two papers, Pride and Prejudice, and a Technical report about LLM in Huawei in Chinese), and this is the **easiest example** to run here:

```bash
source .venv/bin/activate # Don&#039;t forget to activate the virtual environment
python -m apps.document_rag --query &quot;What are the main techniques LEANN explores?&quot;
```

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üìã Click to expand: Document-Specific Arguments&lt;/strong&gt;&lt;/summary&gt;

#### Parameters
```bash
--data-dir DIR           # Directory containing documents to process (default: data)
--file-types .ext .ext   # Filter by specific file types (optional - all LlamaIndex supported types if omitted)
```

#### Example Commands
```bash
# Process all documents with larger chunks for academic papers
python -m apps.document_rag --data-dir &quot;~/Documents/Papers&quot; --chunk-size 1024

# Filter only markdown and Python files with smaller chunks
python -m apps.document_rag --data-dir &quot;./docs&quot; --chunk-size 256 --file-types .md .py

# Enable AST-aware chunking for code files
python -m apps.document_rag --enable-code-chunking --data-dir &quot;./my_project&quot;

# Or use the specialized code RAG for better code understanding
python -m apps.code_rag --repo-dir &quot;./my_codebase&quot; --query &quot;How does authentication work?&quot;
```

&lt;/details&gt;

### üé® ColQwen: Multimodal PDF Retrieval with Vision-Language Models

Search through PDFs using both text and visual understanding with ColQwen2/ColPali models. Perfect for research papers, technical documents, and any PDFs with complex layouts, figures, or diagrams.

&gt; **üçé Mac Users**: ColQwen is optimized for Apple Silicon with MPS acceleration for faster inference!

```bash
# Build index from PDFs
python -m apps.colqwen_rag build --pdfs ./my_papers/ --index research_papers

# Search with text queries
python -m apps.colqwen_rag search research_papers &quot;How does attention mechanism work?&quot;

# Interactive Q&amp;A
python -m apps.colqwen_rag ask research_papers --interactive
```

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;üìã Click to expand: ColQwen Setup &amp; Usage&lt;/strong&gt;&lt;/summary&gt;

#### Prerequisites
```bash
# Install dependencies
uv pip install colpali_engine pdf2image pillow matplotlib qwen_vl_utils einops seaborn
brew install poppler  # macOS only, for PDF processing
```

#### Build Index
```bash
python -m apps.colqwen_rag build \
  --pdfs ./pdf_directory/ \
  --index my_index \
  --model colqwen2  # or colpali
```

#### Search
```bash
python -m apps.colqwen_rag search my_index &quot;your question here&quot; --top-k 5
```

#### Models
- **ColQwen2** (`colqwen2`): Latest vision-language model with improved performance
- **ColPali** (`colpali`): Proven multimodal retriever

For detailed usage, see the [ColQwen Guide](docs/COLQWEN_GUIDE.md).

&lt;/details&gt;

### üìß Your Personal Email Secretary: RAG on Apple Mail!

&gt; **Note:** The examples below currently support macOS only. Windows support coming soon.


&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;videos/mail_clear.gif&quot; alt=&quot;LEANN Email Search Demo&quot; width=&quot;600&quot;&gt;
&lt;/p&gt;

Before running the example below, you need to grant full disk access to your terminal/VS Code in System Preferences ‚Üí Privacy &amp; Security ‚Üí Full Disk Access.

```bash
python -m apps.email_rag --query &quot;What&#039;s the food I ordered by DoorDash or Uber Eats mostly?&quot;
```
**780K email chunks ‚Üí 78MB storage.** Finall

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Mebus/cupp]]></title>
            <link>https://github.com/Mebus/cupp</link>
            <guid>https://github.com/Mebus/cupp</guid>
            <pubDate>Mon, 19 Jan 2026 00:04:56 GMT</pubDate>
            <description><![CDATA[Common User Passwords Profiler (CUPP)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Mebus/cupp">Mebus/cupp</a></h1>
            <p>Common User Passwords Profiler (CUPP)</p>
            <p>Language: Python</p>
            <p>Stars: 5,548</p>
            <p>Forks: 1,757</p>
            <p>Stars today: 31 stars today</p>
            <h2>README</h2><pre># CUPP - Common User Passwords Profiler

[![Build Status](https://travis-ci.org/Mebus/cupp.svg?branch=master)](https://travis-ci.org/Mebus/cupp)
[![Coverage Status](https://coveralls.io/repos/github/Mebus/cupp/badge.svg)](https://coveralls.io/github/Mebus/cupp)
[![Codacy Badge](https://app.codacy.com/project/badge/Grade/be081b6a20b043ce9d79fd3d48b40009)](https://app.codacy.com/gh/Mebus/cupp/dashboard?utm_source=gh&amp;utm_medium=referral&amp;utm_content=&amp;utm_campaign=Badge_grade)
[![Rawsec&#039;s CyberSecurity Inventory](https://inventory.raw.pm/img/badges/Rawsec-inventoried-FF5050_plastic.svg)](https://inventory.raw.pm/)

 
## About

  The most common form of authentication is the combination of a username
  and a password or passphrase. If both match values stored within a locally
  stored table, the user is authenticated for a connection. Password strength is
  a measure of the difficulty involved in guessing or breaking the password
  through cryptographic techniques or library-based automated testing of
  alternate values.

  A weak password might be very short or only use alphanumberic characters,
  making decryption simple. A weak password can also be one that is easily
  guessed by someone profiling the user, such as a birthday, nickname, address,
  name of a pet or relative, or a common word such as God, love, money or password.

  That is why CUPP was born, and it can be used in situations like legal
  penetration tests or forensic crime investigations.


Requirements
------------

You need Python 3 to run CUPP.

Quick start
-----------

    $ python3 cupp.py -h

## Options

  Usage: cupp.py [OPTIONS]

        -h      this menu

        -i      Interactive questions for user password profiling

        -w      Use this option to profile existing dictionary,
                or WyD.pl output to make some pwnsauce :)

        -l      Download huge wordlists from repository

        -a      Parse default usernames and passwords directly from Alecto DB.
                Project Alecto uses purified databases of Phenoelit and CIRT which where merged and enhanced.

        -v      Version of the program



## Configuration

   CUPP has configuration file cupp.cfg with instructions.

## Example (Fast forwarded)

![cupp-example](screenshots/cupp-example.gif)

## License

  This program is free software; you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation; either version 3 of the License, or
  any later version.

  This program is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with this program; if not, write to the Free Software
  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA

  See &#039;./LICENSE&#039; for more information.

## Github import

This project was imported into https://github.com/Mebus/cupp by Mebus from:  
http://www.remote-exploit.org/content/cupp-3.0.tar.gz  
http://www.remote-exploit.org/articles/misc_research__amp_code/index.html  
to encourage further development of the tool.

## Original author

  Muris Kurgas aka j0rgan  
  j0rgan@remote-exploit.org  
  http://www.remote-exploit.org  
  http://www.azuzi.me  


## Contributors

  * Bosko Petrovic aka bolexxx  
  bole_loser@hotmail.com  
  http://www.offensive-security.com  
  http://www.bolexxx.net  

  * Mebus  
    https://github.com/Mebus/  

  * Abhro  
    https://github.com/Abhro/  

  * Andrea Giacomo  
    https://github.com/codepr

  * quantumcore  
    https://github.com/quantumcore
    

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sansan0/TrendRadar]]></title>
            <link>https://github.com/sansan0/TrendRadar</link>
            <guid>https://github.com/sansan0/TrendRadar</guid>
            <pubDate>Mon, 19 Jan 2026 00:04:55 GMT</pubDate>
            <description><![CDATA[‚≠êAI-driven public opinion & trend monitor with multi-platform aggregation, RSS, and smart alerts.üéØ ÂëäÂà´‰ø°ÊÅØËøáËΩΩÔºå‰Ω†ÁöÑ AI ËàÜÊÉÖÁõëÊéßÂä©Êâã‰∏éÁÉ≠ÁÇπÁ≠õÈÄâÂ∑•ÂÖ∑ÔºÅËÅöÂêàÂ§öÂπ≥Âè∞ÁÉ≠ÁÇπ + RSS ËÆ¢ÈòÖÔºåÊîØÊåÅÂÖ≥ÈîÆËØçÁ≤æÂáÜÁ≠õÈÄâ„ÄÇAI ÁøªËØë + AI ÂàÜÊûêÁÆÄÊä•Áõ¥Êé®ÊâãÊú∫Ôºå‰πüÊîØÊåÅÊé•ÂÖ• MCP Êû∂ÊûÑÔºåËµãËÉΩ AI Ëá™ÁÑ∂ËØ≠Ë®ÄÂØπËØùÂàÜÊûê„ÄÅÊÉÖÊÑüÊ¥ûÂØü‰∏éË∂ãÂäøÈ¢ÑÊµãÁ≠â„ÄÇÊîØÊåÅ Docker ÔºåÊï∞ÊçÆÊú¨Âú∞/‰∫ëÁ´ØËá™ÊåÅ„ÄÇÈõÜÊàêÂæÆ‰ø°/È£û‰π¶/ÈíâÈíâ/Telegram/ÈÇÆ‰ª∂/ntfy/bark/slack Á≠âÊ∏†ÈÅìÊô∫ËÉΩÊé®ÈÄÅ„ÄÇ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sansan0/TrendRadar">sansan0/TrendRadar</a></h1>
            <p>‚≠êAI-driven public opinion & trend monitor with multi-platform aggregation, RSS, and smart alerts.üéØ ÂëäÂà´‰ø°ÊÅØËøáËΩΩÔºå‰Ω†ÁöÑ AI ËàÜÊÉÖÁõëÊéßÂä©Êâã‰∏éÁÉ≠ÁÇπÁ≠õÈÄâÂ∑•ÂÖ∑ÔºÅËÅöÂêàÂ§öÂπ≥Âè∞ÁÉ≠ÁÇπ + RSS ËÆ¢ÈòÖÔºåÊîØÊåÅÂÖ≥ÈîÆËØçÁ≤æÂáÜÁ≠õÈÄâ„ÄÇAI ÁøªËØë + AI ÂàÜÊûêÁÆÄÊä•Áõ¥Êé®ÊâãÊú∫Ôºå‰πüÊîØÊåÅÊé•ÂÖ• MCP Êû∂ÊûÑÔºåËµãËÉΩ AI Ëá™ÁÑ∂ËØ≠Ë®ÄÂØπËØùÂàÜÊûê„ÄÅÊÉÖÊÑüÊ¥ûÂØü‰∏éË∂ãÂäøÈ¢ÑÊµãÁ≠â„ÄÇÊîØÊåÅ Docker ÔºåÊï∞ÊçÆÊú¨Âú∞/‰∫ëÁ´ØËá™ÊåÅ„ÄÇÈõÜÊàêÂæÆ‰ø°/È£û‰π¶/ÈíâÈíâ/Telegram/ÈÇÆ‰ª∂/ntfy/bark/slack Á≠âÊ∏†ÈÅìÊô∫ËÉΩÊé®ÈÄÅ„ÄÇ</p>
            <p>Language: Python</p>
            <p>Stars: 43,678</p>
            <p>Forks: 21,628</p>
            <p>Stars today: 180 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; id=&quot;trendradar&quot;&gt;

&lt;a href=&quot;https://github.com/sansan0/TrendRadar&quot; title=&quot;TrendRadar&quot;&gt;
  &lt;img src=&quot;/_image/banner.webp&quot; alt=&quot;TrendRadar Banner&quot; width=&quot;80%&quot;&gt;
&lt;/a&gt;

ÊúÄÂø´&lt;strong&gt;30Áßí&lt;/strong&gt;ÈÉ®ÁΩ≤ÁöÑÁÉ≠ÁÇπÂä©Êâã ‚Äî‚Äî ÂëäÂà´Êó†ÊïàÂà∑Â±èÔºåÂè™ÁúãÁúüÊ≠£ÂÖ≥ÂøÉÁöÑÊñ∞ÈóªËµÑËÆØ

&lt;a href=&quot;https://trendshift.io/repositories/14726&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14726&quot; alt=&quot;sansan0%2FTrendRadar | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;a href=&quot;https://shandianshuo.cn&quot; target=&quot;_blank&quot; title=&quot;AI ËØ≠Èü≥ËæìÂÖ•ÔºåÊØîÊâìÂ≠óÂø´ 4 ÂÄç ‚ö°&quot;&gt;&lt;img src=&quot;_image/shandianshuo.png&quot; alt=&quot;Èó™ÁîµËØ¥ logo&quot; height=&quot;50&quot;/&gt;&lt;/a&gt;

[![GitHub Stars](https://img.shields.io/github/stars/sansan0/TrendRadar?style=flat-square&amp;logo=github&amp;color=yellow)](https://github.com/sansan0/TrendRadar/stargazers)
[![GitHub Forks](https://img.shields.io/github/forks/sansan0/TrendRadar?style=flat-square&amp;logo=github&amp;color=blue)](https://github.com/sansan0/TrendRadar/network/members)
[![License](https://img.shields.io/badge/license-GPL--3.0-blue.svg?style=flat-square)](LICENSE)
[![Version](https://img.shields.io/badge/version-v5.2.0-blue.svg)](https://github.com/sansan0/TrendRadar)
[![MCP](https://img.shields.io/badge/MCP-v3.1.6-green.svg)](https://github.com/sansan0/TrendRadar)
[![RSS](https://img.shields.io/badge/RSS-ËÆ¢ÈòÖÊ∫êÊîØÊåÅ-orange.svg?style=flat-square&amp;logo=rss&amp;logoColor=white)](https://github.com/sansan0/TrendRadar)
[![AIÁøªËØë](https://img.shields.io/badge/AI-Â§öËØ≠Ë®ÄÊé®ÈÄÅ-purple.svg?style=flat-square)](https://github.com/sansan0/TrendRadar)

[![‰ºÅ‰∏öÂæÆ‰ø°ÈÄöÁü•](https://img.shields.io/badge/‰ºÅ‰∏öÂæÆ‰ø°-ÈÄöÁü•-00D4AA?style=flat-square)](https://work.weixin.qq.com/)
[![‰∏™‰∫∫ÂæÆ‰ø°ÈÄöÁü•](https://img.shields.io/badge/‰∏™‰∫∫ÂæÆ‰ø°-ÈÄöÁü•-00D4AA?style=flat-square)](https://weixin.qq.com/)
[![TelegramÈÄöÁü•](https://img.shields.io/badge/Telegram-ÈÄöÁü•-00D4AA?style=flat-square)](https://telegram.org/)
[![dingtalkÈÄöÁü•](https://img.shields.io/badge/ÈíâÈíâ-ÈÄöÁü•-00D4AA?style=flat-square)](#)
[![È£û‰π¶ÈÄöÁü•](https://img.shields.io/badge/È£û‰π¶-ÈÄöÁü•-00D4AA?style=flat-square)](https://www.feishu.cn/)
[![ÈÇÆ‰ª∂ÈÄöÁü•](https://img.shields.io/badge/Email-ÈÄöÁü•-00D4AA?style=flat-square)](#)
[![ntfyÈÄöÁü•](https://img.shields.io/badge/ntfy-ÈÄöÁü•-00D4AA?style=flat-square)](https://github.com/binwiederhier/ntfy)
[![BarkÈÄöÁü•](https://img.shields.io/badge/Bark-ÈÄöÁü•-00D4AA?style=flat-square)](https://github.com/Finb/Bark)
[![SlackÈÄöÁü•](https://img.shields.io/badge/Slack-ÈÄöÁü•-00D4AA?style=flat-square)](https://slack.com/)
[![ÈÄöÁî®Webhook](https://img.shields.io/badge/ÈÄöÁî®-Webhook-607D8B?style=flat-square&amp;logo=webhook&amp;logoColor=white)](#)


[![GitHub Actions](https://img.shields.io/badge/GitHub_Actions-Ëá™Âä®Âåñ-2088FF?style=flat-square&amp;logo=github-actions&amp;logoColor=white)](https://github.com/sansan0/TrendRadar)
[![GitHub Pages](https://img.shields.io/badge/GitHub_Pages-ÈÉ®ÁΩ≤-4285F4?style=flat-square&amp;logo=github&amp;logoColor=white)](https://sansan0.github.io/TrendRadar)
[![Docker](https://img.shields.io/badge/Docker-ÈÉ®ÁΩ≤-2496ED?style=flat-square&amp;logo=docker&amp;logoColor=white)](https://hub.docker.com/r/wantcat/trendradar)
[![MCP Support](https://img.shields.io/badge/MCP-AIÂàÜÊûêÊîØÊåÅ-FF6B6B?style=flat-square&amp;logo=ai&amp;logoColor=white)](https://modelcontextprotocol.io/)
[![AIÂàÜÊûêÊé®ÈÄÅ](https://img.shields.io/badge/AI-ÂàÜÊûêÊé®ÈÄÅ-FF6B6B?style=flat-square&amp;logo=openai&amp;logoColor=white)](#)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

**‰∏≠Êñá** | **[English](README-EN.md)**

&lt;/div&gt;

&gt; Êú¨È°πÁõÆ‰ª•ËΩªÈáèÔºåÊòìÈÉ®ÁΩ≤‰∏∫ÁõÆÊ†á

&lt;br&gt;

## üìë Âø´ÈÄüÂØºËà™

&gt; üí° **ÁÇπÂáª‰∏ãÊñπÈìæÊé•**ÂèØÂø´ÈÄüË∑≥ËΩ¨Âà∞ÂØπÂ∫îÁ´†ËäÇ„ÄÇÈÉ®ÁΩ≤Êé®Ëçê‰ªé„Äå**Âø´ÈÄüÂºÄÂßã**„ÄçÂÖ•ÊâãÔºåÈúÄË¶ÅËØ¶ÁªÜËá™ÂÆö‰πâËØ∑Áúã„Äå**ÈÖçÁΩÆËØ¶Ëß£**„Äç

&lt;div align=&quot;center&quot;&gt;

|   |   |   |
|:---:|:---:|:---:|
| [üöÄ **Âø´ÈÄüÂºÄÂßã**](#-Âø´ÈÄüÂºÄÂßã) | [AI Êô∫ËÉΩÂàÜÊûê](#-ai-Êô∫ËÉΩÂàÜÊûê) | [‚öôÔ∏è **ÈÖçÁΩÆËØ¶Ëß£**](#ÈÖçÁΩÆËØ¶Ëß£) |
| [DockerÈÉ®ÁΩ≤](#6-docker-ÈÉ®ÁΩ≤) | [MCPÂÆ¢Êà∑Á´Ø](#-mcp-ÂÆ¢Êà∑Á´Ø) | [üìù **Êõ¥Êñ∞Êó•Âøó**](#-Êõ¥Êñ∞Êó•Âøó) |
| [üéØ **Ê†∏ÂøÉÂäüËÉΩ**](#-Ê†∏ÂøÉÂäüËÉΩ) | [‚òï **ÊîØÊåÅÈ°πÁõÆ**](#-ÊîØÊåÅÈ°πÁõÆ) | [üìö **È°πÁõÆÁõ∏ÂÖ≥**](#-È°πÁõÆÁõ∏ÂÖ≥) |

&lt;/div&gt;

&lt;br&gt;

- ÊÑüË∞¢**‰∏∫È°πÁõÆÁÇπ star** ÁöÑËßÇ‰ºó‰ª¨Ôºå**fork** ‰Ω†ÊâÄÊ¨≤‰πüÔºå**star** ÊàëÊâÄÊ¨≤‰πüÔºå‰∏§ËÄÖÂæóÂÖºüòçÊòØÂØπÂºÄÊ∫êÁ≤æÁ•ûÊúÄÂ•ΩÁöÑÊîØÊåÅ

&lt;details&gt;
&lt;summary&gt;üëâ ÁÇπÂáªÂ±ïÂºÄÔºö&lt;strong&gt;Ëá¥Ë∞¢ÂêçÂçï&lt;/strong&gt; (Â§©‰ΩøËΩÆËç£Ë™âÊ¶ú üî•73+üî• ‰Ωç)&lt;/summary&gt;

### Êó©ÊúüÊîØÊåÅËÄÖËá¥Ë∞¢

&gt; üí° **ÁâπÂà´ËØ¥Êòé**Ôºö
&gt;
&gt; 1. **ÂÖ≥‰∫éÂêçÂçï**Ôºö‰∏ãÊñπË°®Ê†ºËÆ∞ÂΩï‰∫ÜÈ°πÁõÆËµ∑Ê≠•Èò∂ÊÆµÔºàÂ§©‰ΩøËΩÆÔºâÁöÑÊîØÊåÅËÄÖ„ÄÇÂõ†Êó©Êúü‰∫∫Â∑•ÁªüËÆ°ÁπÅÁêêÔºå**ÈöæÂÖçÂ≠òÂú®ÁñèÊºèÊàñËÆ∞ÂΩï‰∏çÂÖ®ÁöÑÊÉÖÂÜµÔºåÂ¶ÇÊúâÈÅóÊºèÔºåÂÆûÈùûÊú¨ÊÑèÔºå‰∏áÊúõÊµ∑Ê∂µ**„ÄÇ
&gt; 2. **Êú™Êù•ËßÑÂàí**Ôºö‰∏∫‰∫ÜÂ∞ÜÊúâÈôêÁöÑÁ≤æÂäõÂõûÂΩí‰ª£Á†Å‰∏éÂäüËÉΩËø≠‰ª£Ôºå**Âç≥Êó•Ëµ∑‰∏çÂÜç‰∫∫Â∑•Áª¥Êä§Ê≠§ÂêçÂçï**„ÄÇ
&gt;
&gt; Êó†ËÆ∫ÂêçÂ≠óÊòØÂê¶‰∏äÊ¶úÔºå‰Ω†‰ª¨ÁöÑÊØè‰∏Ä‰ªΩÊîØÊåÅÈÉΩÊòØ TrendRadar ËÉΩÂ§üËµ∞Âà∞‰ªäÂ§©ÁöÑÂü∫Áü≥„ÄÇüôè

### Âü∫Á°ÄËÆæÊñΩÊîØÊåÅ

ÊÑüË∞¢ **GitHub** ÂÖçË¥πÊèê‰æõÁöÑÂü∫Á°ÄËÆæÊñΩÔºåËøôÊòØÊú¨È°πÁõÆÂæó‰ª•**‰∏ÄÈîÆ fork**‰æøÊç∑ËøêË°åÁöÑÊúÄÂ§ßÂâçÊèê„ÄÇ

### Êï∞ÊçÆÊîØÊåÅ

Êú¨È°πÁõÆ‰ΩøÁî® [newsnow](https://github.com/ourongxing/newsnow) È°πÁõÆÁöÑ API Ëé∑ÂèñÂ§öÂπ≥Âè∞Êï∞ÊçÆÔºåÁâπÂà´ÊÑüË∞¢‰ΩúËÄÖÊèê‰æõÁöÑÊúçÂä°„ÄÇ

ÁªèËÅîÁ≥ªÔºå‰ΩúËÄÖË°®Á§∫Êó†ÈúÄÊãÖÂøÉÊúçÂä°Âô®ÂéãÂäõÔºå‰ΩÜËøôÊòØÂü∫‰∫é‰ªñÁöÑÂñÑÊÑèÂíå‰ø°‰ªª„ÄÇËØ∑Â§ßÂÆ∂Ôºö
- **ÂâçÂæÄ [newsnow È°πÁõÆ](https://github.com/ourongxing/newsnow) ÁÇπ star ÊîØÊåÅ**
- Docker ÈÉ®ÁΩ≤Êó∂ÔºåËØ∑ÂêàÁêÜÊéßÂà∂Êé®ÈÄÅÈ¢ëÁéáÔºåÂãøÁ´≠Ê≥ΩËÄåÊ∏î

### Êé®ÂπøÂä©Âäõ

&gt; ÊÑüË∞¢‰ª•‰∏ãÂπ≥Âè∞Âíå‰∏™‰∫∫ÁöÑÊé®Ëçê(ÊåâÊó∂Èó¥ÊéíÂàó)

- [Â∞è‰ºóËΩØ‰ª∂](https://mp.weixin.qq.com/s/fvutkJ_NPUelSW9OGK39aA) - ÂºÄÊ∫êËΩØ‰ª∂Êé®ËçêÂπ≥Âè∞
- [LinuxDo Á§æÂå∫](https://linux.do/) - ÊäÄÊúØÁà±Â•ΩËÄÖÁöÑËÅöÈõÜÂú∞
- [ÈòÆ‰∏ÄÂ≥∞Âë®Âàä](https://github.com/ruanyf/weekly) - ÊäÄÊúØÂúàÊúâÂΩ±ÂìçÂäõÁöÑÂë®Âàä

### ËßÇ‰ºóÊîØÊåÅ

&gt; ÊÑüË∞¢**Áªô‰∫àËµÑÈáëÊîØÊåÅ**ÁöÑÊúãÂèã‰ª¨Ôºå‰Ω†‰ª¨ÁöÑÊÖ∑ÊÖ®Â∑≤ÂåñË∫´‰∏∫ÈîÆÁõòÊóÅÁöÑÈõ∂È£üÈ•ÆÊñôÔºåÈô™‰º¥ÁùÄÈ°πÁõÆÁöÑÊØè‰∏ÄÊ¨°Ëø≠‰ª£„ÄÇ
&gt;
&gt; **ÂÖ≥‰∫é&quot;‰∏ÄÂÖÉÁÇπËµû&quot;ÁöÑÂõûÂΩí**Ôºö
&gt; ÈöèÁùÄ v5.0.0 ÁâàÊú¨ÁöÑÂèëÂ∏ÉÔºåÈ°πÁõÆËøàÂÖ•‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÈò∂ÊÆµ„ÄÇ‰∏∫‰∫ÜÊîØÊåÅÊó•ÁõäÂ¢ûÈïøÁöÑ API ÊàêÊú¨ÂíåÂíñÂï°Âõ†Ê∂àËÄóÔºå&quot;‰∏ÄÂÖÉÁÇπËµû&quot;ÈÄöÈÅìÁé∞Â∑≤ÈáçÊñ∞ÂºÄÂêØ„ÄÇ‰Ω†ÁöÑÊØè‰∏Ä‰ªΩÂøÉÊÑèÔºåÈÉΩÂ∞ÜËΩ¨Âåñ‰∏∫‰ª£Á†Å‰∏ñÁïåÈáåÁöÑ Token ÂíåÂä®Âäõ„ÄÇüöÄ [ÂâçÂæÄÊîØÊåÅ](#-ÊîØÊåÅÈ°πÁõÆ)

|           ÁÇπËµû‰∫∫            |  ÈáëÈ¢ù  |  Êó•Êúü  |             Â§áÊ≥®             |
| :-------------------------: | :----: | :----: | :-----------------------: |
|           D*5          |  1.8 * 3 | 2025.11.24  |    | 
|           *È¨º          |  1 | 2025.11.17  |    | 
|           *Ë∂Ö          |  10 | 2025.11.17  |    | 
|           R*w          |  10 | 2025.11.17  | Ëøô agent ÂÅöÁöÑÁâõÈÄºÂïä,ÂÖÑÂºü    | 
|           J*o          |  1 | 2025.11.17  | ÊÑüË∞¢ÂºÄÊ∫ê,Á•ùÂ§ß‰Ω¨‰∫ã‰∏öÊúâÊàê    | 
|           *Êô®          |  8.88  | 2025.11.16  | È°πÁõÆ‰∏çÈîô,Á†îÁ©∂Â≠¶‰π†‰∏≠    | 
|           *Êµ∑          |  1  | 2025.11.15  |    | 
|           *Âæ∑          |  1.99  | 2025.11.15  |    | 
|           *Áñè          |  8.8  | 2025.11.14  |  ÊÑüË∞¢ÂºÄÊ∫êÔºåÈ°πÁõÆÂæàÊ£íÔºåÊîØÊåÅ‰∏Ä‰∏ã   | 
|           M*e          |  10  | 2025.11.14  |  ÂºÄÊ∫ê‰∏çÊòìÔºåÂ§ß‰Ω¨ËæõËã¶‰∫Ü   | 
|           **ÊüØ          |  1  | 2025.11.14  |     | 
|           *‰∫ë          |  88  | 2025.11.13  |    Â•ΩÈ°πÁõÆÔºåÊÑüË∞¢ÂºÄÊ∫ê  | 
|           *W          |  6  | 2025.11.13  |      | 
|           *ÂáØ          |  1  | 2025.11.13  |      | 
|           ÂØπ*.          |  1  | 2025.11.13  |    Thanks for your TrendRadar  | 
|           s*y          |  1  | 2025.11.13  |      | 
|           **Áøî          |  10  | 2025.11.13  |   Â•ΩÈ°πÁõÆÔºåÁõ∏ËßÅÊÅ®ÊôöÔºåÊÑüË∞¢ÂºÄÊ∫êÔºÅ     | 
|           *Èü¶          |  9.9  | 2025.11.13  |   TrendRadarË∂ÖËµûÔºåËØ∑ËÄÅÂ∏àÂñùÂíñÂï°~     | 
|           h*p          |  5  | 2025.11.12  |   ÊîØÊåÅ‰∏≠ÂõΩÂºÄÊ∫êÂäõÈáèÔºåÂä†Ê≤πÔºÅ     | 
|           c*r          |  6  | 2025.11.12  |        | 
|           a*n          |  5  | 2025.11.12  |        | 
|           „ÄÇ*c          |  1  | 2025.11.12  |    ÊÑüË∞¢ÂºÄÊ∫êÂàÜ‰∫´    | 
|           *ËÆ∞          |  1  | 2025.11.11  |        | 
|           *‰∏ª          |  1  | 2025.11.10  |        | 
|           *‰∫Ü          |  10  | 2025.11.09  |        | 
|           *Êù∞          |  5  | 2025.11.08  |        | 
|           *ÁÇπ          |  8.80  | 2025.11.07  |   ÂºÄÂèë‰∏çÊòìÔºåÊîØÊåÅ‰∏Ä‰∏ã„ÄÇ     | 
|           Q*Q          |  6.66  | 2025.11.07  |   ÊÑüË∞¢ÂºÄÊ∫êÔºÅ     | 
|           C*e          |  1  | 2025.11.05  |        | 
|           Peter Fan          |  20  | 2025.10.29  |        | 
|           M*n          |  1  | 2025.10.27  |      ÊÑüË∞¢ÂºÄÊ∫ê  | 
|           *ËÆ∏          |  8.88  | 2025.10.23  |      ËÄÅÂ∏à Â∞èÁôΩ‰∏ÄÊûöÔºåÊë∏‰∫ÜÂá†Â§©‰∫ÜËøòÊ≤°Êï¥Ëµ∑Êù•ÔºåÊ±ÇÊïô  | 
|           Eason           |  1  | 2025.10.22  |      ËøòÊ≤°Êï¥ÊòéÁôΩÔºå‰ΩÜ‰Ω†Âú®ÂÅöÂ•Ω‰∫ã  | 
|           P*n           |  1  | 2025.10.20  |          |
|           *Êù∞           |  1  | 2025.10.19  |          |
|           *Âæê           |  1  | 2025.10.18  |          |
|           *Âøó           |  1  | 2025.10.17  |          |
|           *üòÄ           |  10  | 2025.10.16  |     ÁÇπËµû     |
|           **Êù∞           |  10  | 2025.10.16  |          |
|           *Âï∏           |  10  | 2025.10.16  |          |
|           *Á∫™           |  5  | 2025.10.14  | TrendRadar         |
|           J*d           |  1  | 2025.10.14  | Ë∞¢Ë∞¢‰Ω†ÁöÑÂ∑•ÂÖ∑ÔºåÂæàÂ•ΩÁé©...          |
|           *H           |  1  | 2025.10.14  |           |
|           ÈÇ£*O           |  10  | 2025.10.13  |           |
|           *ÂúÜ           |  1  | 2025.10.13  |           |
|           P*g           |  6  | 2025.10.13  |           |
|           Ocean           |  20  | 2025.10.12  |  ...ÁúüÁöÑÂ§™Ê£í‰∫ÜÔºÅÔºÅÔºÅÂ∞èÁôΩÁ∫ßÂà´‰πüËÉΩÁõ¥Êé•Áî®...         |
|           **Âüπ           |  5.2  | 2025.10.2  |  github-yzyf1312:ÂºÄÊ∫ê‰∏áÂ≤Å         |
|           *Ê§ø           |  3  | 2025.9.23  |  Âä†Ê≤πÔºåÂæà‰∏çÈîô         |
|           *üçç           |  10  | 2025.9.21  |           |
|           E*f           |  1  | 2025.9.20  |           |
|           *ËÆ∞            |  1  | 2025.9.20  |           |
|           z*u            |  2  | 2025.9.19  |           |
|           **Êòä            |  5  | 2025.9.17  |           |
|           *Âè∑            |  1  | 2025.9.15  |           |
|           T*T            |  2  | 2025.9.15  |  ÁÇπËµû         |
|           *ÂÆ∂            |  10  | 2025.9.10  |           |
|           *X            |  1.11  | 2025.9.3  |           |
|           *È£ô            |  20  | 2025.8.31  |  Êù•Ëá™ËÄÅÁ´•Ë∞¢Ë∞¢         |
|           *‰∏ã            |  1  | 2025.8.30  |           |
|           2*D            |  88  | 2025.8.13 ‰∏ãÂçà |           |
|           2*D            |  1  | 2025.8.13 ‰∏äÂçà |           |
|           S*o            |  1  | 2025.8.05 |   ÊîØÊåÅ‰∏Ä‰∏ã        |
|           *‰æ†            |  10  | 2025.8.04 |           |
|           x*x            |  2  | 2025.8.03 |  trendRadar Â•ΩÈ°πÁõÆ ÁÇπËµû          |
|           *Ëøú            |  1  | 2025.8.01 |            |
|           *ÈÇ™            |  5  | 2025.8.01 |            |
|           *Ê¢¶            |  0.1  | 2025.7.30 |            |
|           **Èæô            |  10  | 2025.7.29 |      ÊîØÊåÅ‰∏Ä‰∏ã      |


&lt;/details&gt;

&lt;br&gt;

## ü™Ñ ËµûÂä©ÂïÜ

&gt; ÊØèÂ§©ÂÜôÊä•Âëä„ÄÅÂõûÂ§çÊ∂àÊÅØÊòØÂê¶ËÆ©ÊâãËÖïÁñ≤ÊÉ´ÔºüËØïËØï„ÄåÈó™ÁîµËØ¥„ÄçAI ËØ≠Èü≥ËæìÂÖ•Ê≥ï ‚Äî‚Äî ËØ¥ËØùÔºåÊØîÊâìÂ≠óÂø´ 4 ÂÄç ‚ö° 

&lt;div align=&quot;center&quot;&gt;

[![Mac‰∏ãËΩΩ](https://img.shields.io/badge/Mac-ÂÖçË¥π‰∏ãËΩΩ-FF6B6B?style=for-the-badge&amp;logo=apple&amp;logoColor=white)](https://shandianshuo.cn) [![Windows‰∏ãËΩΩ](https://img.shields.io/badge/Windows-ÂÖçË¥π‰∏ãËΩΩ-FF6B6B?style=for-the-badge&amp;logo=lightning&amp;logoColor=white)](https://shandianshuo.cn)
&lt;a href=&quot;https://shandianshuo.cn&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;_image/banner-shandianshuo.png&quot; alt=&quot;Èó™ÁîµËØ¥&quot; width=&quot;600&quot;/&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;br&gt;

## ‚òï ÊîØÊåÅÈ°πÁõÆ

&gt; Â¶ÇÊûúÊú¨È°πÁõÆÂØπ‰Ω†ÊúâÂ∏ÆÂä©Ôºå‰Ω†ÂèØ‰ª•ÈÄâÊã©‰ª•‰∏ãÊñπÂºèÊîØÊåÅÔºö
&gt; 1. **ÂÖ¨ÁõäÂä©Â≠¶**ÔºöÂæÆ‰ø°ÊêúÁ¥¢**ËÖæËÆØÂÖ¨Áõä**ÔºåÂØπÈáåÈù¢ÁöÑ**Âä©Â≠¶**Áõ∏ÂÖ≥ÁöÑÈ°πÁõÆÈöèÂøÉÊçê„ÄÇ
&gt;
&gt; 2. **ËµûÂä©ÂºÄÂèëËÄÖ**Ôºö‰Ω†ÁöÑËµûÂä©Â∞ÜÁî®‰∫éË°•ÂÖÖÁ¢≥Âü∫ÁîüÁâ©ÁöÑÂíñÂï°Âõ†ÂíåÁ°ÖÂü∫ÁîüÁâ©ÁöÑ Token Ê∂àËÄó„ÄÇ


- **GitHub Issues**ÔºöÈÄÇÂêàÈíàÂØπÊÄßÂº∫ÁöÑËß£Á≠î„ÄÇÊèêÈóÆÊó∂ËØ∑Êèê‰æõÂÆåÊï¥‰ø°ÊÅØÔºàÊà™Âõæ„ÄÅÈîôËØØÊó•Âøó„ÄÅÁ≥ªÁªüÁéØÂ¢ÉÁ≠âÔºâ„ÄÇ
- **ÂÖ¨‰ºóÂè∑‰∫§ÊµÅ**ÔºöÈÄÇÂêàÂø´ÈÄüÂí®ËØ¢„ÄÇÂª∫ËÆÆ‰ºòÂÖàÂú®Áõ∏ÂÖ≥ÊñáÁ´†‰∏ãÁöÑÂÖ¨ÂÖ±ÁïôË®ÄÂå∫‰∫§ÊµÅÔºåÂ¶ÇÁßÅ‰ø°ÔºåËØ∑ÊñáÊòéÁ§ºË≤åÁî®ËØ≠üòâ
- **ËÅîÁ≥ªÊñπÂºè**Ôºöpath@linux.do


|ÂÖ¨‰ºóÂè∑ÂÖ≥Ê≥® |ÂæÆ‰ø°ÁÇπËµû | ÊîØ‰ªòÂÆùÁÇπËµû |
|:---:|:---:|:---:|
| &lt;img src=&quot;_image/weixin.png&quot; width=&quot;300&quot; title=&quot;Á°ÖÂü∫Ëå∂Ê∞¥Èó¥&quot;/&gt; | &lt;img src=&quot;https://cdn-1258574687.cos.ap-shanghai.myqcloud.com/img/%2F2025%2F07%2F17%2F2ae0a88d98079f7e876c2b4dc85233c6-9e8025.JPG&quot; width=&quot;300&quot; title=&quot;ÂæÆ‰ø°ÊîØ‰ªò&quot;/&gt; | &lt;img src=&quot;https://cdn-1258574687.cos.ap-shanghai.myqcloud.com/img/%2F2025%2F07%2F17%2F1ed4f20ab8e35be51f8e84c94e6e239b4-fe4947.JPG&quot; width=&quot;300&quot; title=&quot;ÊîØ‰ªòÂÆùÊîØ‰ªò&quot;/&gt; |

&lt;br&gt;

## üìù Êõ¥Êñ∞Êó•Âøó

&gt; **üìå Êü•ÁúãÊúÄÊñ∞Êõ¥Êñ∞**Ôºö**[Âéü‰ªìÂ∫ìÊõ¥Êñ∞Êó•Âøó](https://github.com/sansan0/TrendRadar?tab=readme-ov-file#-Êõ¥Êñ∞Êó•Âøó)** Ôºö
- **ÊèêÁ§∫**ÔºöÂª∫ËÆÆÊü•Áúã„ÄêÂéÜÂè≤Êõ¥Êñ∞„ÄëÔºåÊòéÁ°ÆÂÖ∑‰ΩìÁöÑ„ÄêÂäüËÉΩÂÜÖÂÆπ„Äë

### 2026/01/17 - v5.2.0

&gt; ‰∏ªË¶ÅËßÅ config.yaml ÊèèËø∞

**üåê AI ÁøªËØëÂäüËÉΩ**

- **Â§öËØ≠Ë®ÄÁøªËØë**ÔºöÊîØÊåÅÂ∞ÜÊé®ÈÄÅÂÜÖÂÆπÁøªËØë‰∏∫‰ªªÊÑèËØ≠Ë®Ä
- **ÊâπÈáèÁøªËØë**ÔºöÊô∫ËÉΩÊâπÈáèÂ§ÑÁêÜÔºåÂáèÂ∞ë API Ë∞ÉÁî®Ê¨°Êï∞
- **Ëá™ÂÆö‰πâÊèêÁ§∫ËØç**ÔºöÊîØÊåÅËá™ÂÆö‰πâÁøªËØëÈ£éÊ†º

**üîß ÈÖçÁΩÆÊû∂ÊûÑ‰ºòÂåñ**

- **AI Ê®°ÂûãÈÖçÁΩÆÁã¨Á´ã**ÔºöÂàÜÊûêÂíåÁøªËØëÂÖ±‰∫´Ê®°ÂûãÈÖçÁΩÆ
- **Âå∫ÂüüÂºÄÂÖ≥Áªü‰∏Ä**ÔºöÁªü‰∏ÄÁÆ°ÁêÜÊé®ÈÄÅÂå∫ÂüüÊòæÁ§∫
- **Âå∫ÂüüÊéíÂ∫èËá™ÂÆö‰πâ**ÔºöÊîØÊåÅËá™ÂÆö‰πâÂêÑÂå∫ÂüüÁöÑÊòæÁ§∫È°∫Â∫è

**‚ú® AI ÂàÜÊûêÂ¢ûÂº∫**

- **AI ÂàÜÊûêÂµåÂÖ• HTML**ÔºöÂàÜÊûêÁªìÊûúÁõ¥Êé•ÂµåÂÖ• HTML Êä•ÂëäÔºåÈÇÆ‰ª∂ÈÄöÁü•Áõ¥Êé•‰ΩøÁî®
- **ÂØåÊ†∑Âºè AI Âå∫Âùó**ÔºöÊ∏êÂèòËìùËâ≤ËÉåÊôØÂç°ÁâáÂºèÂ∏ÉÂ±ÄÔºåÊ∏ÖÊô∞ÂàÜÈöîÂêÑÂàÜÊûêÁª¥Â∫¶
- **ÊéíÂêçÊó∂Èó¥Á∫øÊîØÊåÅ**ÔºöAI ÂèØËé∑ÂèñÊØèÊù°Êñ∞ÈóªÂú®ÊØè‰∏™ÊäìÂèñÊó∂Èó¥ÁÇπÁöÑÁ≤æÁ°ÆÊéíÂêç
- **ÊùøÂùóÈáçÁªÑ (7‚Üí4)**ÔºöÊï¥Âêà‰∏∫Ê†∏ÂøÉÁÉ≠ÁÇπÊÄÅÂäø„ÄÅËàÜËÆ∫È£éÂêë‰∫âËÆÆ„ÄÅÂºÇÂä®‰∏éÂº±‰ø°Âè∑„ÄÅÁ†îÂà§Á≠ñÁï•Âª∫ËÆÆ

**üîß Â§öÊ®°ÂûãÈÄÇÈÖç**

- **ÈÄöÁî®ÂèÇÊï∞ÈÄè‰º†**ÔºöÊîØÊåÅÂêë API ÈÄè‰º†‰ªªÊÑèÈ´òÁ∫ßÂèÇÊï∞
- **Gemini ÈÄÇÈÖç**ÔºöÂéüÁîüÂèÇÊï∞ÊîØÊåÅÔºåÂÜÖÁΩÆÂÆâÂÖ®Á≠ñÁï•ÊîæÂÆΩ

**üêõ Bug ‰øÆÂ§ç**

- ‰øÆÂ§çËã•Âπ≤Â∑≤Áü•ÈóÆÈ¢òÔºåÊèêÂçáÁ≥ªÁªüÁ®≥ÂÆöÊÄß


### 2026/01/10 - mcp-v3.0.0~v3.1.5

- **Breaking Change**ÔºöÊâÄÊúâÂ∑•ÂÖ∑ËøîÂõûÂÄºÁªü‰∏Ä‰∏∫ `{success, summary, data, error}` ÁªìÊûÑ
- **ÂºÇÊ≠•‰∏ÄËá¥ÊÄß**ÔºöÊâÄÊúâ 21 ‰∏™Â∑•ÂÖ∑ÂáΩÊï∞‰ΩøÁî® `asyncio.to_thread()` ÂåÖË£ÖÂêåÊ≠•Ë∞ÉÁî®
- **MCP Resources**ÔºöÊñ∞Â¢û 4 ‰∏™ËµÑÊ∫êÔºàplatforms„ÄÅrss-feeds„ÄÅavailable-dates„ÄÅkeywordsÔºâ
- **RSS Â¢ûÂº∫**Ôºö`get_latest_rss` ÊîØÊåÅÂ§öÊó•Êü•ËØ¢Ôºàdays ÂèÇÊï∞ÔºâÔºåË∑®Êó•Êúü URL ÂéªÈáç
- **Ê≠£ÂàôÂåπÈÖç‰øÆÂ§ç**Ôºö`get_trending_topics` ÊîØÊåÅ `/pattern/` Ê≠£ÂàôËØ≠Ê≥ïÂíå `display_name`
- **ÁºìÂ≠ò‰ºòÂåñ**ÔºöÊñ∞Â¢û `make_cache_key()` ÂáΩÊï∞ÔºåÂèÇÊï∞ÊéíÂ∫è+MD5 ÂìàÂ∏åÁ°Æ‰øù‰∏ÄËá¥ÊÄß
- **Êñ∞Â¢û check_version Â∑•ÂÖ∑**ÔºöÊîØÊåÅÂêåÊó∂Ê£ÄÊü• TrendRadar Âíå MCP Server ÁâàÊú¨Êõ¥Êñ∞


&lt;details&gt;
&lt;summary&gt;üëâ ÁÇπÂáªÂ±ïÂºÄÔºö&lt;strong&gt;ÂéÜÂè≤Êõ¥Êñ∞&lt;/strong&gt;&lt;/summary&gt;


### 2026/01/10 - v5.0.0

&gt; **ÂºÄÂèëÂ∞èÊèíÊõ≤**Ôºö
&gt; Ëá¥Êï¨ÈÇ£‰∏™Èô™‰º¥Êàë‰∏§Âπ¥Â§ö„ÄÅÂç¥Âú®ÂàöÁª≠Ë¥πÂêéÂèçÊâãÂºπÂá∫ `&quot;This organization has been disabled&quot;` ÁöÑÊüê C ÂéÇÊ®°Âûã

**‚ú® Êé®ÈÄÅÂÜÖÂÆπ&quot;‰∫îÂ§ßÊùøÂùó&quot;ÈáçÊûÑ**

Êú¨Ê¨°Êõ¥Êñ∞ÂØπÊé®ÈÄÅÊ∂àÊÅØËøõË°å‰∫ÜÂå∫ÂüüÂåñÈáçÊûÑÔºåÁé∞Âú®Êé®ÈÄÅÂÜÖÂÆπÊ∏ÖÊô∞Âú∞ÂàíÂàÜ‰∏∫‰∫îÂ§ßÊ†∏ÂøÉÊùøÂùóÔºö

1.  **üìä ÁÉ≠Ê¶úÊñ∞Èóª**ÔºöÊ†πÊçÆÊÇ®ÁöÑÂÖ≥ÈîÆËØçÁ≤æÂáÜÁ≠õÈÄâÂêéÁöÑÂÖ®ÁΩëÁÉ≠ÁÇπËÅöÂêà„ÄÇ
2.  **üì∞ RSS ËÆ¢ÈòÖ**ÔºöÊÇ®ÁöÑ‰∏™ÊÄßÂåñËÆ¢ÈòÖÊ∫êÂÜÖÂÆπÔºåÊîØÊåÅÊåâÂÖ≥ÈîÆËØçÂàÜÁªÑ„ÄÇ
3.  **üÜï Êú¨Ê¨°Êñ∞Â¢û**ÔºöÂÆûÊó∂ÊçïÊçâËá™‰∏äÊ¨°ËøêË°å‰ª•Êù•ÁöÑÂÖ®Êñ∞ÁÉ≠ÁÇπÔºàÂ∏¶ üÜï Ê†áËÆ∞Ôºâ„ÄÇ
4.  **üìã Áã¨Á´ãÂ±ïÁ§∫Âå∫**ÔºöÊåáÂÆöÂπ≥Âè∞ÁöÑÂÆåÊï¥ÁÉ≠Ê¶úÊàñ RSS Ê∫êÂ±ïÁ§∫Ôºå**ÂÆåÂÖ®‰∏çÂèóÂÖ≥ÈîÆËØçËøáÊª§ÈôêÂà∂**„ÄÇ
5.  **‚ú® AI ÂàÜÊûêÊùøÂùó**ÔºöÁî± AI È©±Âä®ÁöÑÊ∑±Â∫¶Ê¥ûÂØüÔºåÂåÖÂê´Ë∂ãÂäøÊ¶ÇËø∞„ÄÅÁÉ≠Â∫¶Ëµ∞ÂäøÂèä**ÊûÅÂÖ∂ÈáçË¶Å**ÁöÑÊÉÖÊÑüÂÄæÂêëÂàÜÊûê„ÄÇ

**‚ú® AI Êô∫ËÉΩÂàÜÊûêÊé®ÈÄÅÂäüËÉΩ**

- **AI ÂàÜÊûêÈõÜÊàê**Ôºö‰ΩøÁî® AI Â§ßÊ®°ÂûãÂØπÊé®ÈÄÅÂÜÖÂÆπËøõË°åÊ∑±Â∫¶ÂàÜÊûêÔºåËá™Âä®ÁîüÊàêÁÉ≠ÁÇπË∂ãÂäøÊ¶ÇËø∞„ÄÅÂÖ≥ÈîÆËØçÁÉ≠Â∫¶ÂàÜÊûê„ÄÅË∑®Âπ≥Âè∞ÂÖ≥ËÅî„ÄÅÊΩúÂú®ÂΩ±ÂìçËØÑ‰º∞Á≠â
- **ÊÉÖÊÑüÂÄæÂêëÂàÜÊûê**ÔºöÊñ∞Â¢ûÊ∑±Â∫¶ÊÉÖÊÑüËØÜÂà´ÔºåÁ≤æÂáÜÊçïÊçâËàÜËÆ∫ÁöÑÊ≠£Ë¥üÈù¢„ÄÅ‰∫âËÆÆÊàñÊãÖÂøßÊÉÖÁª™
- **Â§ö AI Êèê‰æõÂïÜÊîØÊåÅ**ÔºöÊîØÊåÅ DeepSeekÔºàÈªòËÆ§ÔºåÊÄß‰ª∑ÊØîÈ´òÔºâ„ÄÅOpenAI„ÄÅGoogle Gemini Âèä‰ªªÊÑè OpenAI ÂÖºÂÆπÊé•Âè£
- **‰∏§ÁßçÊé®ÈÄÅÊ®°Âºè**Ôºö`only_analysis`Ôºà‰ªÖ AI ÂàÜÊûêÔºâ„ÄÅ`both`Ôºà‰∏§ËÄÖÈÉΩÊé®ÈÄÅÔºâ
- **Ëá™ÂÆö‰πâÊèêÁ§∫ËØç**ÔºöÈÄöËøá `config/ai_analysis_prompt.txt` Êñá‰ª∂Ëá™ÂÆö‰πâ AI ÂàÜÊûêËßíËâ≤ÂíåËæìÂá∫Ê†ºÂºè
- **Â§öÁª¥Â∫¶Êï∞ÊçÆÂàÜÊûê**ÔºöAI ÂèØÂàÜÊûêÊéíÂêçÂèòÂåñ„ÄÅÁÉ≠Â∫¶ÊåÅÁª≠Êó∂Èó¥„ÄÅË∑®Âπ≥Âè∞Ë°®Áé∞„ÄÅË∂ãÂäøÈ¢ÑÊµãÁ≠â

**üìã Áã¨Á´ãÂ±ïÁ§∫Âå∫ÂäüËÉΩ**

- **ÂÆåÊï¥ÁÉ≠Ê¶úÂ±ïÁ§∫**ÔºöÊåáÂÆöÂπ≥Âè∞ÁöÑÂÆåÊï¥ÁÉ≠Ê¶úÂçïÁã¨Â±ïÁ§∫Ôºå‰∏çÂèóÂÖ≥ÈîÆËØçËøáÊª§ÂΩ±Âìç
- **RSS Áã¨Á´ãÂ±ïÁ§∫**ÔºöRSS Ê∫êÂÜÖÂÆπÂèØÂÆåÊï¥Â±ïÁ§∫ÔºåÈÄÇÂêàÂÜÖÂÆπËæÉÂ∞ëÁöÑËÆ¢ÈòÖÊ∫ê
- **ÁÅµÊ¥ªÈÖçÁΩÆ**ÔºöÊîØÊåÅÈÖçÁΩÆÂ±ïÁ§∫Âπ≥Âè∞ÂàóË°®„ÄÅRSS Ê∫êÂàóË°®„ÄÅÊúÄÂ§ßÂ±ïÁ§∫Êù°Êï∞

**üìä Êé®ÈÄÅ‰ΩìÈ™åÈáçÊûÑ**

- **ÊéíÁâàÂçáÁ∫ß**ÔºöÈáçÊñ∞ËÆæËÆ°Âπ∂Áªü‰∏ÄÂêÑÊ∏†ÈÅìÁªüËÆ°Â§¥ÈÉ®ÔºåÂº∫ÂåñÂå∫ÂùóÁªÑÁªáÔºåÊ∂àÊÅØÂ±ÇÊ¨°‰∏ÄÁõÆ‰∫ÜÁÑ∂
- **ÈÖçÁΩÆÁÆÄÂåñ**Ôºö‰ºòÂåñÈ£û‰π¶Á≠âÈÄöÁü•Ê∏†ÈÅìÁöÑÈÖçÁΩÆÈÄªËæëÔºå‰∏äÊâãÊõ¥ÁÆÄÂçï
- **ÁÉ≠Â∫¶Ë∂ãÂäøÁÆ≠Â§¥**ÔºöÊñ∞Â¢û üî∫(‰∏äÂçá)„ÄÅüîª(‰∏ãÈôç)„ÄÅ‚ûñ(ÊåÅÂπ≥) Ë∂ãÂäøÊ†áËØÜÔºåÁõ¥ËßÇÂ±ïÁ§∫ÁÉ≠Â∫¶ÂèòÂåñ
- **ÈÄöÁî® Webhook**ÔºöÊîØÊåÅËá™ÂÆö‰πâ Webhook URL Âíå JSON Ê®°ÊùøÔºåËΩªÊùæÈÄÇÈÖç Discord„ÄÅMatrix„ÄÅIFTTT Á≠â‰ªªÊÑèÂπ≥Âè∞

**üîß ÈÖçÁΩÆ‰ºòÂåñ**

- **È¢ëÁéáËØçÈÖçÁΩÆÂ¢ûÂº∫**ÔºöÊñ∞Â¢û `[ÁªÑÂà´Âêç]` ËØ≠Ê≥ïÔºåÊîØÊåÅ `#` Ê≥®ÈáäË°åÔºåÈÖçÁΩÆÊõ¥Ê∏ÖÊô∞ÔºàÊÑüË∞¢ [@songge8](https://github.com/sansan0/TrendRadar/issues/752) ÊèêÂá∫ÁöÑÂª∫ËÆÆÔºâ
- **ÁéØÂ¢ÉÂèòÈáèÊîØÊåÅ**ÔºöAI ÂàÜÊûêÁõ∏ÂÖ≥ÈÖçÁΩÆÊîØÊåÅÁéØÂ¢ÉÂèòÈáèË¶ÜÁõñÔºà`AI_API_KEY`„ÄÅ`AI_PROVIDER` Á≠âÔºâ

&gt; üí° ËØ¶ÁªÜÈÖçÁΩÆÊïôÁ®ãËßÅ [ËÆ© AI Â∏ÆÊàëÂàÜÊûêÁÉ≠ÁÇπ](#12-ËÆ©-ai-Â∏ÆÊàëÂàÜÊûêÁÉ≠ÁÇπ)


### 2026/01/02 - v4.7.0

- **‰øÆÂ§ç RSS HTML ÊòæÁ§∫**Ôºö‰øÆÂ§ç RSS Êï∞ÊçÆÊ†ºÂºè‰∏çÂåπÈÖçÂØºËá¥ÁöÑÊ∏≤ÊüìÈóÆÈ¢òÔºåÁé∞Âú®ÊåâÂÖ≥ÈîÆËØçÂàÜÁªÑÊ≠£Á°ÆÊòæÁ§∫
- **Êñ∞Â¢ûÊ≠£ÂàôË°®ËææÂºèËØ≠Ê≥ï**ÔºöÂÖ≥ÈîÆËØçÈÖçÁΩÆÊîØÊåÅ `/pattern/` Ê≠£ÂàôËØ≠Ê≥ïÔºåËß£ÂÜ≥Ëã±ÊñáÂ≠êÂ≠óÁ¨¶‰∏≤ËØØÂåπÈÖçÈóÆÈ¢òÔºàÂ¶Ç `ai` ÂåπÈÖç `training`Ôºâ[üìñ Êü•ÁúãËØ≠Ê≥ïËØ¶Ëß£](#ÂÖ≥ÈîÆËØçÂü∫Á°ÄËØ≠Ê≥ï)
- **Êñ∞Â¢ûÊòæÁ§∫ÂêçÁß∞ËØ≠Ê≥ï**Ôºö‰ΩøÁî® `=&gt; Â§áÊ≥®` ÁªôÂ§çÊùÇÁöÑÊ≠£ÂàôË°®ËææÂºèËµ∑‰∏™Â•ΩËÆ∞ÁöÑÂêçÂ≠óÔºåÊé®ÈÄÅÊ∂àÊÅØÊòæÁ§∫Êõ¥Ê∏ÖÊô∞ÔºàÂ¶Ç `/\bai\b/ =&gt; AIÁõ∏ÂÖ≥`Ôºâ
- **‰∏ç‰ºöÂÜôÊ≠£ÂàôÔºü** README Êñ∞Â¢û AI ÁîüÊàêÊ≠£ÂàôÁöÑÂºïÂØºÔºåÂëäËØâ ChatGPT/Gemini/DeepSeek ‰Ω†ÊÉ≥ÂåπÈÖç‰ªÄ‰πàÔºåËÆ© AI Â∏Æ‰Ω†ÂÜô


### 2025/12/30 - mcp-v2.0.0

- **Êû∂ÊûÑË∞ÉÊï¥**ÔºöÁßªÈô§ TXT ÊîØÊåÅÔºåÁªü‰∏Ä‰ΩøÁî® SQLite Êï∞ÊçÆÂ∫ì
- **RSS Êü•ËØ¢**ÔºöÊñ∞Â¢û `get_latest_rss`„ÄÅ`search_rss`„ÄÅ`get_rss_feeds_status`
- **Áªü‰∏ÄÊêúÁ¥¢**Ôºö`search_news` ÊîØÊåÅ `include_rss` ÂèÇÊï∞ÂêåÊó∂ÊêúÁ¥¢ÁÉ≠Ê¶úÂíå RSS


### 2026/01/01 - v4.6.0

- **‰øÆÂ§ç RSS HTML ÊòæÁ§∫**ÔºöÂ∞Ü RSS ÂÜÖÂÆπÂêàÂπ∂Âà∞ÁÉ≠Ê¶ú HTML È°µÈù¢ÔºåÊåâÊ∫êÂàÜÁªÑÊòæÁ§∫
- **Êñ∞Â¢û display_mode ÈÖçÁΩÆ**ÔºöÊîØÊåÅ `keyword`ÔºàÊåâÂÖ≥ÈîÆËØçÂàÜÁªÑÔºâÂíå `platform`ÔºàÊåâÂπ≥Âè∞ÂàÜÁªÑÔºâ‰∏§ÁßçÊòæÁ§∫Ê®°Âºè


### 2025/12/30 - v4.5.0

- **RSS ËÆ¢ÈòÖÊ∫êÊîØÊåÅ**ÔºöÊñ∞Â¢û RSS/Atom ÊäìÂèñÔºåÊåâÂÖ≥ÈîÆËØçÂàÜÁªÑÁªüËÆ°Ôºà‰∏éÁÉ≠Ê¶úÊ†ºÂºè‰∏ÄËá¥Ôºâ
- **Â≠òÂÇ®ÁªìÊûÑÈáçÊûÑ**ÔºöÊâÅÂπ≥ÂåñÁõÆÂΩïÁªìÊûÑ `output/{type}/{date}.db`
- **Áªü‰∏ÄÊéíÂ∫èÈÖçÁΩÆ**Ôºö`sort_by_position_first` ÂêåÊó∂ÂΩ±ÂìçÁÉ≠Ê¶úÂíå RSS
- **ÈÖçÁΩÆÁªìÊûÑÈáçÊûÑ**Ôºö`config.yaml` ÈáçÊñ∞ÁªÑÁªá‰∏∫ 7 ‰∏™ÈÄªËæëÂàÜÁªÑÔºàapp„ÄÅreport„ÄÅnotification„ÄÅstorage„ÄÅplatforms„ÄÅrss„ÄÅadvancedÔºâÔºåÈÖçÁΩÆË∑ØÂæÑÊõ¥Ê∏ÖÊô∞


### 2025/12/26 - mcp-v1.2.0

  **MCP Ê®°ÂùóÊõ¥Êñ∞ - ‰ºòÂåñÂ∑•ÂÖ∑ÈõÜÔºåÊñ∞Â¢ûËÅöÂêàÂØπÊØîÂäüËÉΩÔºåÂêàÂπ∂ÂÜó‰ΩôÂ∑•ÂÖ∑:**
  - Êñ∞Â¢û `aggregate_news` Â∑•ÂÖ∑ - Ë∑®Âπ≥Âè∞Êñ∞ÈóªÂéªÈáçËÅöÂêà
  - Êñ∞Â¢û `compare_periods` Â∑•ÂÖ∑ - Êó∂ÊúüÂØπÊØîÂàÜÊûêÔºàÂë®ÁéØÊØî/ÊúàÁéØÊØîÔºâ
  - ÂêàÂπ∂ `find_similar_news` + `search_related_news_history` ‚Üí `find_related_news`
  - Â¢ûÂº∫ `get_trending_topics` - Êñ∞Â¢û `auto_extract` Ê®°ÂºèËá™Âä®ÊèêÂèñÁÉ≠ÁÇπ
  - ‰øÆÂ§çËã•Âπ≤bug
  - ÂêåÊ≠•Êõ¥Êñ∞ README-MCP-FAQ.md ÊñáÊ°£ÁöÑ‰∏≠Ëã±ÊñáÁâà (Q1-Q18)


### 2025/12/20 - v4.0.3

- Êñ∞Â¢û URL Ê†áÂáÜÂåñÂäüËÉΩÔºåËß£ÂÜ≥ÂæÆÂçöÁ≠âÂπ≥Âè∞Âõ†Âä®ÊÄÅÂèÇÊï∞ÔºàÂ¶Ç `band_rank`ÔºâÂØºËá¥ÁöÑÈáçÂ§çÊé®ÈÄÅÈóÆÈ¢ò
- ‰øÆÂ§çÂ¢ûÈáèÊ®°ÂºèÊ£ÄÊµãÈÄªËæëÔºåÊ≠£Á°ÆËØÜÂà´ÂéÜÂè≤Ê†áÈ¢ò


### 2025/12/17 - v4.0.1

- StorageManager Ê∑ªÂä†Êé®ÈÄÅËÆ∞ÂΩï‰ª£ÁêÜÊñπÊ≥ï
- S3 ÂÆ¢Êà∑Á´ØÂàáÊç¢Ëá≥ virtual-hosted style ‰ª•ÊèêÂçáÂÖºÂÆπÊÄßÔºàÊîØÊåÅËÖæËÆØ‰∫ë COS Á≠âÊõ¥Â§öÊúçÂä°Ôºâ


### 2025/12/13 - mcp-v1.1.0

  **MCP Ê®°ÂùóÊõ¥Êñ∞:**
  - ÈÄÇÈÖç v4.0.0ÔºåÂêåÊó∂‰πüÂÖºÂÆπ v3.x ÁöÑÊï∞ÊçÆ
  - Êñ∞Â¢ûÂ≠òÂÇ®ÂêåÊ≠•Â∑•ÂÖ∑Ôºö`sync_from_remote`„ÄÅ`get_storage_status`„ÄÅ`list_available_dates`


### 2025/12/13 - v4.0.0

**üéâ ÈáçÂ§ßÊõ¥Êñ∞ÔºöÂÖ®Èù¢ÈáçÊûÑÂ≠òÂÇ®ÂíåÊ†∏ÂøÉÊû∂ÊûÑ**

- **Â§öÂ≠òÂÇ®ÂêéÁ´ØÊîØÊåÅ**ÔºöÂºïÂÖ•ÂÖ®Êñ∞ÁöÑÂ≠òÂÇ®Ê®°ÂùóÔºåÊîØÊåÅÊú¨Âú∞ SQLite ÂíåËøúÁ®ã‰∫ëÂ≠òÂÇ®ÔºàS3 ÂÖºÂÆπÂçèËÆÆÔºå‰æãÂ¶Ç Cloudflare R2ÔºâÔºåÈÄÇÂ∫î GitHub Actions„ÄÅDocker ÂíåÊú¨Âú∞ÁéØÂ¢É„ÄÇ
- **Êï∞ÊçÆÂ∫ìÁªìÊûÑ‰ºòÂåñ**ÔºöÈáçÊûÑ SQLite Êï∞ÊçÆÂ∫ìË°®ÁªìÊûÑÔºåÊèêÂçáÊï∞ÊçÆÊïàÁéáÂíåÊü•ËØ¢ËÉΩÂäõ„ÄÇ
- **Ê†∏ÂøÉ‰ª£Á†ÅÊ®°ÂùóÂåñ**ÔºöÂ∞Ü‰∏ªÁ®ãÂ∫èÈÄªËæëÊãÜÂàÜ‰∏∫ trendradar ÂåÖÁöÑÂ§ö‰∏™Ê®°ÂùóÔºåÊòæËëóÊèêÂçá‰ª£Á†ÅÂèØÁª¥Êä§ÊÄß„ÄÇ
- **Â¢ûÂº∫ÂäüËÉΩ**ÔºöÂÆûÁé∞Êó•ÊúüÊ†ºÂºèÊ†áÂáÜÂåñ„ÄÅÊï∞ÊçÆ‰øùÁïôÁ≠ñÁï•„ÄÅÊó∂Âå∫ÈÖçÁΩÆÊîØÊåÅ„ÄÅÊó∂Èó¥ÊòæÁ§∫‰ºòÂåñÔºåÂπ∂‰øÆÂ§çËøúÁ®ãÂ≠òÂÇ®Êï∞ÊçÆÊåÅ‰πÖÂåñÈóÆÈ¢òÔºåÁ°Æ‰øùÊï∞ÊçÆÂêàÂπ∂ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ
- **Ê∏ÖÁêÜÂíåÂÖºÂÆπ**ÔºöÁßªÈô§‰∫ÜÂ§ßÈÉ®ÂàÜÂéÜÂè≤ÂÖºÂÆπ‰ª£Á†ÅÔºåÁªü‰∏Ä‰∫ÜÊï∞ÊçÆÂ≠òÂÇ®ÂíåËØªÂèñÊñπÂºè„ÄÇ


### 2025/12/03 - v3.5.0

**üéâ Ê†∏ÂøÉÂäüËÉΩÂ¢ûÂº∫**

1. **Â§öË¥¶Âè∑Êé®ÈÄÅÊîØÊåÅ**
   - ÊâÄÊúâÊé®ÈÄÅÊ∏†ÈÅìÔºàÈ£û‰π¶„ÄÅÈíâÈíâ„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°„ÄÅTelegram„ÄÅntfy„ÄÅBark„ÄÅSlackÔºâÊîØÊåÅÂ§öË¥¶Âè∑ÈÖçÁΩÆ
   - ‰ΩøÁî®ÂàÜÂè∑ `;` ÂàÜÈöîÂ§ö‰∏™Ë¥¶Âè∑Ôºå‰æãÂ¶ÇÔºö`FEISHU_WEBHOOK_URL=url1;url2`
   - Ëá™Âä®È™åËØÅÈÖçÂØπÈÖçÁΩÆÔºàÂ¶Ç Telegram ÁöÑ token Âíå chat_idÔºâÊï∞Èáè‰∏ÄËá¥ÊÄß

2. **Êé®ÈÄÅÂÜÖÂÆπÈ°∫Â∫èÂèØÈÖçÁΩÆ**
   - Êñ∞Â¢û `reverse_content_order` ÈÖçÁΩÆÈ°π
   - ÊîØÊåÅËá™ÂÆö‰πâÁÉ≠ÁÇπËØçÊ±áÁªüËÆ°‰∏éÊñ∞Â¢ûÁÉ≠ÁÇπÊñ∞ÈóªÁöÑÊòæÁ§∫È°∫Â∫è

3. **ÂÖ®Â±ÄËøáÊª§ÂÖ≥ÈîÆËØç**
   - Êñ∞Â¢û `[GLOBAL_FILTER]` Âå∫ÂüüÊ†áËÆ∞ÔºåÊîØÊåÅÂÖ®Â±ÄËøáÊª§‰∏çÊÉ≥ÁúãÂà∞ÁöÑÂÜÖÂÆπ
   - ÈÄÇÁî®Âú∫ÊôØÔºöËøáÊª§ÂπøÂëä„ÄÅËê•ÈîÄ„ÄÅ‰ΩéË¥®ÂÜÖÂÆπÁ≠â

**üê≥ Docker ÂèåË∑ØÂæÑ HTML ÁîüÊàê‰ºòÂåñ**

- **ÈóÆÈ¢ò‰øÆÂ§ç**ÔºöËß£ÂÜ≥ Docker ÁéØÂ¢É‰∏ã `index.html` Êó†Ê≥ïÂêåÊ≠•Âà∞ÂÆø‰∏ªÊú∫ÁöÑÈóÆÈ¢ò
- **ÂèåË∑ØÂæÑÁîüÊàê**ÔºöÂΩìÊó•Ê±áÊÄª HTML ÂêåÊó∂ÁîüÊàêÂà∞‰∏§‰∏™‰ΩçÁΩÆ
  - `index.html`ÔºàÈ°πÁõÆÊ†πÁõÆÂΩïÔºâÔºö‰æõ GitHub Pages ËÆøÈóÆ
  - `output/index.html`ÔºöÈÄöËøá Docker Volume ÊåÇËΩΩÔºåÂÆø‰∏ªÊú∫ÂèØÁõ¥Êé•ËÆøÈóÆ
- **ÂÖºÂÆπÊÄß**ÔºöÁ°Æ‰øù Docker„ÄÅGitHub Actions„ÄÅÊú¨Âú∞ËøêË°åÁéØÂ¢ÉÂùáËÉΩÊ≠£Â∏∏ËÆøÈóÆÁΩëÈ°µÁâàÊä•Âëä

**üê≥ Docker MCP ÈïúÂÉèÊîØÊåÅ**

- Êñ∞Â¢ûÁã¨Á´ãÁöÑ MCP ÊúçÂä°ÈïúÂÉè `wantcat/trendradar-mcp`
- ÊîØÊåÅ Docker ÈÉ®ÁΩ≤ AI ÂàÜÊûêÂäüËÉΩÔºåÈÄöËøá HTTP Êé•Âè£ÔºàÁ´ØÂè£ 3333ÔºâÊèê‰æõÊúçÂä°
- ÂèåÂÆπÂô®Êû∂ÊûÑÔºöÊñ∞ÈóªÊé®ÈÄÅÊúçÂä°‰∏é MCP ÊúçÂä°Áã¨Á´ãËøêË°åÔºåÂèØÂàÜÂà´Êâ©Â±ïÂíåÈáçÂêØ
- ËØ¶ËßÅ [Docker ÈÉ®ÁΩ≤ - MCP ÊúçÂä°](#6-docker-ÈÉ®ÁΩ≤)

**üåê Web ÊúçÂä°Âô®ÊîØÊåÅ**

- Êñ∞Â¢ûÂÜÖÁΩÆ Web ÊúçÂä°Âô®ÔºåÊîØÊåÅÈÄöËøáÊµèËßàÂô®ËÆøÈóÆÁîüÊàêÁöÑÊä•Âëä
- ÈÄöËøá `manage.py` ÂëΩ‰ª§ÊéßÂà∂ÂêØÂä®/ÂÅúÊ≠¢Ôºö`docker exec -it trendradar python manage.py start_webserver`
- ËÆøÈóÆÂú∞ÂùÄÔºö`http://localhost:8080`ÔºàÁ´ØÂè£ÂèØÈÖçÁΩÆÔºâ
- ÂÆâÂÖ®ÁâπÊÄßÔºöÈùôÊÄÅÊñá‰ª∂ÊúçÂä°„ÄÅÁõÆÂΩïÈôêÂà∂„ÄÅÊú¨Âú∞ËÆøÈóÆ
- ÊîØÊåÅËá™Âä®ÂêØÂä®ÂíåÊâãÂä®ÊéßÂà∂‰∏§ÁßçÊ®°Âºè

**üìñ ÊñáÊ°£‰ºòÂåñ**

- Êñ∞Â¢û [Êé®ÈÄÅÂÜÖÂÆπÊÄé‰πàÊòæÁ§∫Ôºü](#7-Êé®ÈÄÅÂÜÖÂÆπÊÄé‰πàÊòæÁ§∫) Á´†ËäÇÔºöËá™ÂÆö‰πâÊé®ÈÄÅÊ†∑ÂºèÂíåÂÜÖÂÆπ
- Êñ∞Â¢û [‰ªÄ‰πàÊó∂ÂÄôÁªôÊàëÊé®ÈÄÅÔºü](#8-‰ªÄ‰πàÊó∂ÂÄôÁªôÊàëÊé®ÈÄÅ) Á´†ËäÇÔºöËÆæÁΩÆÊé®ÈÄÅÊó∂Èó¥ÊÆµ
- Êñ∞Â¢û [Â§ö‰πÖËøêË°å‰∏ÄÊ¨°Ôºü](#9-Â§ö‰πÖËøêË°å‰∏ÄÊ¨°) Á´†ËäÇÔºöËÆæÁΩÆËá™Âä®ËøêË°åÈ¢ëÁéá
- Êñ∞Â¢û [Êé®ÈÄÅÂà∞Â§ö‰∏™Áæ§/ËÆæÂ§á](#10-Êé®ÈÄÅÂà∞Â§ö‰∏™Áæ§ËÆæÂ§á) Á´†ËäÇÔºöÂêåÊó∂Êé®ÈÄÅÁªôÂ§ö‰∏™Êé•Êî∂ËÄÖ
- ‰ºòÂåñÂêÑÈÖçÁΩÆÁ´†ËäÇÔºöÁªü‰∏ÄÊ∑ªÂä†&quot;ÈÖçÁΩÆ‰ΩçÁΩÆ&quot;ËØ¥Êòé
- ÁÆÄÂåñÂø´ÈÄüÂºÄÂßãÈÖçÁΩÆËØ¥ÊòéÔºö‰∏â‰∏™Ê†∏ÂøÉÊñá‰ª∂‰∏ÄÁõÆ‰∫ÜÁÑ∂
- ‰ºòÂåñ [Docker ÈÉ®ÁΩ≤](#6-docker-ÈÉ®ÁΩ≤) Á´†ËäÇÔºöÊñ∞Â¢ûÈïúÂÉèËØ¥Êòé„ÄÅÊé®Ëçê git clone ÈÉ®ÁΩ≤„ÄÅÈáçÁªÑÈÉ®ÁΩ≤ÊñπÂºè

**üîß ÂçáÁ∫ßËØ¥Êòé**Ôºö
- **GitHub Fork Áî®Êà∑**ÔºöÊõ¥Êñ∞ `main.py`„ÄÅ`config/config.yaml`ÔºàÊñ∞Â¢ûÂ§öË¥¶Âè∑Êé®ÈÄÅÊîØÊåÅÔºåÊó†ÈúÄ‰øÆÊîπÁé∞ÊúâÈÖçÁΩÆÔºâ
- **Â§öË¥¶Âè∑Êé®ÈÄÅ**ÔºöÊñ∞ÂäüËÉΩÔºåÈªòËÆ§‰∏çÂêØÁî®ÔºåÁé∞ÊúâÂçïË¥¶Âè∑ÈÖçÁΩÆ‰∏çÂèóÂΩ±Âìç


### 2025/11/26 - mcp-v1.0.3

  **MCP Ê®°ÂùóÊõ¥Êñ∞:**
  - Êñ∞Â¢ûÊó•ÊúüËß£ÊûêÂ∑•ÂÖ∑ resolve_date_range,Ëß£ÂÜ≥ AI Ê®°ÂûãËÆ°ÁÆóÊó•Êúü‰∏ç‰∏ÄËá¥ÁöÑÈóÆÈ¢ò
  - ÊîØÊåÅËá™ÁÑ∂ËØ≠Ë®ÄÊó•ÊúüË°®ËææÂºèËß£Êûê(Êú¨Âë®„ÄÅÊúÄËøë7Â§©„ÄÅ‰∏äÊúàÁ≠â)
  - Â∑•ÂÖ∑ÊÄªÊï∞‰ªé 13 ‰∏™Â¢ûÂä†Âà∞ 14 ‰∏™


### 2025/11/28 - v3.4.1

**üîß Ê†ºÂºè‰ºòÂåñ**

1. **Bark Êé®ÈÄÅÂ¢ûÂº∫**
   - Bark Áé∞ÊîØÊåÅ Markdown Ê∏≤Êüì
   - ÂêØÁî®ÂéüÁîü Markdown Ê†ºÂºèÔºöÁ≤ó‰Ωì„ÄÅÈìæÊé•„ÄÅÂàóË°®„ÄÅ‰ª£Á†ÅÂùóÁ≠â
   - ÁßªÈô§Á∫ØÊñáÊú¨ËΩ¨Êç¢ÔºåÂÖÖÂàÜÂà©Áî® Bark ÂéüÁîüÊ∏≤ÊüìËÉΩÂäõ

2. **Slack Ê†ºÂºèÁ≤æÂáÜÂåñ**
   - ‰ΩøÁî®‰∏ìÁî® mrkdwn Ê†ºÂºèÂ§ÑÁêÜÂàÜÊâπÂÜÖÂÆπ
   - ÊèêÂçáÂ≠óËäÇÂ§ßÂ∞è‰º∞ÁÆóÂáÜÁ°ÆÊÄßÔºàÈÅøÂÖçÊ∂àÊÅØË∂ÖÈôêÔºâ
   - ‰ºòÂåñÈìæÊé•Ê†ºÂºèÔºö`&lt;url|text&gt;` ÂíåÂä†Á≤óËØ≠Ê≥ïÔºö`*text*`

3. **ÊÄßËÉΩÊèêÂçá**
   - Ê†ºÂºèËΩ¨Êç¢Âú®ÂàÜÊâπËøáÁ®ã‰∏≠ÂÆåÊàêÔºåÈÅøÂÖç‰∫åÊ¨°Â§ÑÁêÜ
   - ÂáÜÁ°Æ‰º∞ÁÆóÊ∂àÊÅØÂ§ßÂ∞èÔºåÂáèÂ∞ëÂèëÈÄÅÂ§±Ë¥•Áéá

**üîß ÂçáÁ∫ßËØ¥Êòé**Ôºö
- **GitHub Fork Áî®Êà∑**ÔºöÊõ¥Êñ∞ `main.py`Ôºå`config.yaml`


### 2025/11/25 - v3.4.0

**üéâ Êñ∞Â¢û Slack Êé®ÈÄÅÊîØÊåÅ**

1. **Âõ¢ÈòüÂçè‰ΩúÊé®ÈÄÅÊ∏†ÈÅì**
   - ÊîØÊåÅ Slack Incoming WebhooksÔºàÂÖ®ÁêÉÊµÅË°åÁöÑÂõ¢ÈòüÂçè‰ΩúÂ∑•ÂÖ∑Ôºâ
   - Ê∂àÊÅØÈõÜ‰∏≠ÁÆ°ÁêÜÔºåÈÄÇÂêàÂõ¢ÈòüÂÖ±‰∫´ÁÉ≠ÁÇπËµÑËÆØ
   - ÊîØÊåÅ mrkdwn Ê†ºÂºèÔºàÁ≤ó‰Ωì„ÄÅÈìæÊé•Á≠âÔºâ

2. **Â§öÁßçÈÉ®ÁΩ≤ÊñπÂºè**
   - GitHub ActionsÔºöÈÖçÁΩÆ `SLACK_WEBHOOK_URL` Secret
   - DockerÔºöÁéØÂ¢ÉÂèòÈáè `SLACK_WEBHOOK_URL`
   - Êú¨Âú∞ËøêË°åÔºö`config/config.yaml` ÈÖçÁΩÆÊñá‰ª∂


&gt; üìñ **ËØ¶ÁªÜÈÖçÁΩÆÊïôÁ®ã**Ôºö[Âø´ÈÄüÂºÄÂßã - Slack Êé®ÈÄÅ](#-Âø´ÈÄüÂºÄÂßã)

- ‰ºòÂåñ setup-windows.bat Âíå setup-windows-en.bat ‰∏ÄÈîÆÂÆâË£Ö MCP ÁöÑ‰ΩìÈ™å

**üîß ÂçáÁ∫ßËØ¥Êòé**Ôºö
- **GitHub Fork Áî®Êà∑**ÔºöÊõ¥Êñ∞ `main.py`„ÄÅ`config/config.yaml`„ÄÅ`.github/workflows/crawler.yml`


### 2025/11/24 - v3.3.0

**üéâ Êñ∞Â¢û Bark Êé®ÈÄÅÊîØÊåÅ**

1. **iOS ‰∏ìÂ±ûÊé®ÈÄÅÊ∏†ÈÅì**
   - ÊîØÊåÅ Bark Êé®ÈÄÅÔºàÂü∫‰∫é APNsÔºåiOS Âπ≥Âè∞Ôºâ
   - ÂÖçË¥πÂºÄÊ∫êÔºåÁÆÄÊ¥ÅÈ´òÊïàÔºåÊó†ÂπøÂëäÂπ≤Êâ∞
   - ÊîØÊåÅÂÆòÊñπÊúçÂä°Âô®ÂíåËá™Âª∫ÊúçÂä°Âô®‰∏§ÁßçÊñπÂºè

2. **Â§öÁßçÈÉ®ÁΩ≤ÊñπÂºè**
   - GitHub ActionsÔºöÈÖçÁΩÆ `BARK_URL` Secret
   - DockerÔºöÁéØÂ¢ÉÂèòÈáè `BARK_URL`
   - Êú¨Âú∞ËøêË°åÔºö`config/config.yaml` ÈÖçÁΩÆÊñá‰ª∂

&gt; üìñ **ËØ¶ÁªÜÈÖçÁΩÆÊïôÁ®ã**Ôºö[Âø´ÈÄüÂºÄÂßã - Bark Êé®ÈÄÅ](#-Âø´ÈÄüÂºÄÂßã)

**üêõ Bug ‰øÆÂ§ç**
- ‰øÆÂ§ç `config.yaml` ‰∏≠ `ntfy_server_url` ÈÖçÁΩÆ‰∏çÁîüÊïàÁöÑÈóÆÈ¢ò ([#345](https://github.com/sansan0/TrendRadar/issues/345))

**üîß ÂçáÁ∫ßËØ¥Êòé**Ôºö
- **GitHub Fork Áî®Êà∑**ÔºöÊõ¥Êñ∞ `main.py`„ÄÅ`config/config.yaml`„ÄÅ`.github/workflows/crawler.yml`

### 2025/11/23 - v3.2.0

**üéØ Êñ∞Â¢ûÈ´òÁ∫ßÂÆöÂà∂ÂäüËÉΩ**

1. **ÂÖ≥ÈîÆËØçÊéíÂ∫è‰ºòÂÖàÁ∫ßÈÖçÁΩÆ**
   - ÊîØÊåÅ‰∏§ÁßçÊéíÂ∫èÁ≠ñÁï•ÔºöÁÉ≠Â∫¶‰ºòÂÖà vs ÈÖçÁΩÆÈ°∫Â∫è‰ºòÂÖà
   - Êª°Ë∂≥‰∏çÂêå‰ΩøÁî®Âú∫ÊôØÔºöÁÉ≠ÁÇπËøΩË∏™ or ‰∏™ÊÄßÂåñÂÖ≥Ê≥®

2. **ÊòæÁ§∫Êï∞ÈáèÁ≤æÂáÜÊéßÂà∂**
   - ÂÖ®Â±ÄÈÖçÁΩÆÔºöÁªü‰∏ÄÈôêÂà∂ÊâÄÊúâÂÖ≥ÈîÆËØçÊòæÁ§∫Êï∞Èáè
   - ÂçïÁã¨ÈÖçÁΩÆÔºö‰ΩøÁî® `@Êï∞Â≠ó` ËØ≠Ê≥ï‰∏∫ÁâπÂÆöÂÖ≥ÈîÆËØçËÆæÁΩÆÈôêÂà∂
   - ÊúâÊïàÊéßÂà∂Êé®ÈÄÅÈïøÂ∫¶ÔºåÁ™ÅÂá∫ÈáçÁÇπÂÜÖÂÆπ

&gt; üìñ **ËØ¶ÁªÜÈÖçÁΩÆÊïôÁ®ã**Ôºö[ÂÖ≥ÈîÆËØçÈÖçÁΩÆ - È´òÁ∫ßÈÖçÁΩÆ](#ÂÖ≥ÈîÆËØçÈ´òÁ∫ßÈÖçÁΩÆ)

**üîß ÂçáÁ∫ßËØ¥Êòé**Ôºö
- **GitHub Fork Áî®Êà∑**ÔºöÊõ¥Êñ∞ `main.py`„ÄÅ`config/config.yaml`


### 2025/11/18 - mcp-v1.0.2

  **MCP Ê®°ÂùóÊõ¥Êñ∞:**
  - ‰ºòÂåñÊü•ËØ¢‰ªäÊó•Êñ∞ÈóªÂç¥ÂèØËÉΩÈîôËØØËøîÂõûËøáÂéªÊó•ÊúüÁöÑÊÉÖÂÜµ


### 2025/11/22 - v3.1.1

- **‰øÆÂ§çÊï∞ÊçÆÂºÇÂ∏∏ÂØºËá¥ÁöÑÂ¥©Ê∫ÉÈóÆÈ¢ò**ÔºöËß£ÂÜ≥ÈÉ®ÂàÜÁî®Êà∑Âú® GitHub Actions ÁéØÂ¢É‰∏≠ÈÅáÂà∞ÁöÑ `&#039;float&#039; object has no attribute &#039;lower&#039;` ÈîôËØØ
- Êñ∞Â¢ûÂèåÈáçÈò≤Êä§Êú∫Âà∂ÔºöÂú®Êï∞ÊçÆËé∑ÂèñÈò∂ÊÆµËøáÊª§Êó†ÊïàÊ†áÈ¢òÔºàNone„ÄÅfloat„ÄÅÁ©∫Â≠óÁ¨¶‰∏≤ÔºâÔºåÂêåÊó∂Âú®ÂáΩÊï∞Ë∞ÉÁî®Â§ÑÊ∑ªÂä†Á±ªÂûãÊ£ÄÊü•
- ÊèêÂçáÁ≥ªÁªüÁ®≥ÂÆöÊÄßÔºåÁ°Æ‰øùÂú®Êï∞ÊçÆÊ∫êËøîÂõûÂºÇÂ∏∏Ê†ºÂºèÊó∂‰ªçËÉΩÊ≠£Â∏∏ËøêË°å

**ÂçáÁ∫ßËØ¥Êòé**ÔºàGitHub Fork Áî®Êà∑ÔºâÔºö
- ÂøÖÈ°ªÊõ¥Êñ∞Ôºö`main.py`
- Âª∫ËÆÆ‰ΩøÁî®Â∞èÁâàÊú¨ÂçáÁ∫ßÊñπÂºèÔºöÂ§çÂà∂ÊõøÊç¢‰∏äËø∞Êñá‰ª∂


### 2025/11/20 - v3.1.0

- **Êñ∞Â¢û‰∏™‰∫∫ÂæÆ‰ø°Êé®ÈÄÅÊîØÊåÅ**Ôºö‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®ÂèØÊé®ÈÄÅÂà∞‰∏™‰∫∫ÂæÆ‰ø°ÔºåÊó†ÈúÄÂÆâË£Ö‰ºÅ‰∏öÂæÆ‰ø° APP
- ÊîØÊåÅ‰∏§ÁßçÊ∂àÊÅØÊ†ºÂºèÔºö`markdown`Ôºà‰ºÅ‰∏öÂæÆ‰ø°Áæ§Êú∫Âô®‰∫∫ÔºâÂíå `text`Ôºà‰∏™‰∫∫ÂæÆ‰ø°Â∫îÁî®Ôºâ
- Êñ∞Â¢û `WEWORK_MSG_TYPE` ÁéØÂ¢ÉÂèòÈáèÈÖçÁΩÆÔºåÊîØÊåÅ GitHub Actions„ÄÅDocker„ÄÅdocker compose Á≠âÂ§öÁßçÈÉ®ÁΩ≤ÊñπÂºè
- `text` Ê®°ÂºèËá™Âä®Ê∏ÖÈô§ Markdown ËØ≠Ê≥ïÔºåÊèê‰æõÁ∫ØÊñáÊú¨Êé®ÈÄÅÊïàÊûú
- ËØ¶ËßÅÂø´ÈÄüÂºÄÂßã‰∏≠ÁöÑ„Äå‰∏™‰∫∫ÂæÆ‰ø°Êé®ÈÄÅ„ÄçÈÖçÁΩÆËØ¥Êòé

**ÂçáÁ∫ßËØ¥Êòé**ÔºàGitHub Fork Áî®Êà∑ÔºâÔºö
- ÂøÖÈ°ªÊõ¥Êñ∞Ôºö`main.py`„ÄÅ`config/config.yaml`
- ÂèØÈÄâÊõ¥Êñ∞Ôºö`.github/workflows/crawler.yml`ÔºàÂ¶Ç‰ΩøÁî® GitHub Actions ÈÉ®ÁΩ≤Ôºâ
- Âª∫ËÆÆ‰ΩøÁî®Â∞èÁâàÊú¨ÂçáÁ∫ßÊñπÂºèÔºöÂ§çÂà∂ÊõøÊç¢‰∏äËø∞Êñá‰ª∂

### 2025/11/12 - v3.0.5

- ‰øÆÂ§çÈÇÆ‰ª∂ÂèëÈÄÅ SSL/TLS Á´ØÂè£ÈÖçÁΩÆÈÄªËæëÈîôËØØ
- ‰ºòÂåñÈÇÆÁÆ±ÊúçÂä°ÂïÜÔºàQQ/163/126ÔºâÈªòËÆ§‰ΩøÁî® 465 Á´ØÂè£ÔºàSSLÔºâ
- **Êñ∞Â¢û Docker ÁéØÂ¢ÉÂèòÈáèÊîØÊåÅ**ÔºöÊ†∏ÂøÉÈÖçÁΩÆÈ°πÔºà`enable_crawler`„ÄÅ`report_mode`„ÄÅ`push_window` Á≠âÔºâÊîØÊåÅÈÄöËøáÁéØÂ¢ÉÂèòÈáèË¶ÜÁõñÔºåËß£ÂÜ≥ NAS Áî®Êà∑‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂‰∏çÁîüÊïàÁöÑÈóÆÈ¢òÔºàËØ¶ËßÅ [üê≥ Docker ÈÉ®ÁΩ≤](#-docker-ÈÉ®ÁΩ≤) Á´†ËäÇÔºâ


### 2025/10/26 - mcp-v1.0.1

  **MCP Ê®°ÂùóÊõ¥Êñ∞:**
  - ‰øÆÂ§çÊó•ÊúüÊü•ËØ¢ÂèÇÊï∞‰º†ÈÄíÈîôËØØ
  - Áªü‰∏ÄÊâÄÊúâÂ∑•ÂÖ∑ÁöÑÊó∂Èó¥ÂèÇÊï∞Ê†ºÂºè


### 2025/10/31 - v3.0.4

- Ëß£ÂÜ≥È£û‰π¶Âõ†Êé®ÈÄÅÂÜÖÂÆπËøáÈïøËÄå‰∫ßÁîüÁöÑÈîôËØØÔºåÂÆûÁé∞‰∫ÜÂàÜÊâπÊé®ÈÄÅ


### 2025/10/23 - v3.0.3

- Êâ©Â§ß ntfy ÈîôËØØ‰ø°ÊÅØÊòæÁ§∫ËåÉÂõ¥


### 2025/10/21 - v3.0.2

- ‰øÆÂ§ç ntfy Êé®ÈÄÅÁºñÁ†ÅÈóÆÈ¢ò

### 2025/10/20 - v3.0.0

**ÈáçÂ§ßÊõ¥Êñ∞ - AI ÂàÜÊûêÂäüËÉΩ‰∏äÁ∫ø** ‚ú®

- **Ê†∏ÂøÉÂäüËÉΩ**Ôºö
  - Êñ∞Â¢ûÂü∫‰∫é MCP (Model Context Protocol) ÁöÑ AI ÂàÜÊûêÊúçÂä°Âô®
  - ÊîØÊåÅ17ÁßçÊô∫ËÉΩÂàÜÊûêÂ∑•ÂÖ∑ÔºöÂü∫Á°ÄÊü•ËØ¢„ÄÅÊô∫ËÉΩÊ£ÄÁ¥¢„ÄÅÈ´òÁ∫ßÂàÜÊûê„ÄÅRSS Êü•ËØ¢„ÄÅÁ≥ªÁªüÁÆ°ÁêÜ
  - Ëá™ÁÑ∂ËØ≠Ë®Ä‰∫§‰∫íÔºöÈÄöËøáÂØπËØùÊñπÂºèÊü•ËØ¢ÂíåÂàÜÊûêÊñ∞ÈóªÊï∞ÊçÆ
  - Â§öÂÆ¢Êà∑Á´ØÊîØÊåÅÔºöClaude Desktop„ÄÅCherry Studio„ÄÅCursor„ÄÅCline Á≠â

- **ÂàÜÊûêËÉΩÂäõ**Ôºö
  - ËØùÈ¢òË∂ãÂäøÂàÜÊûêÔºàÁÉ≠Â∫¶ËøΩË∏™„ÄÅÁîüÂëΩÂë®Êúü„ÄÅÁàÜÁÅ´Ê£ÄÊµã„ÄÅË∂ãÂäøÈ¢ÑÊµãÔºâ
  - Êï∞ÊçÆÊ¥ûÂØüÔºàÂπ≥Âè∞ÂØπÊØî„ÄÅÊ¥ªË∑ÉÂ∫¶ÁªüËÆ°„ÄÅÂÖ≥ÈîÆËØçÂÖ±Áé∞Ôºâ
  - ÊÉÖÊÑüÂàÜÊûê„ÄÅÁõ∏‰ººÊñ∞ÈóªÊü•Êâæ„ÄÅÊô∫ËÉΩÊëòË¶ÅÁîüÊàê
  - ÂéÜÂè≤Áõ∏ÂÖ≥Êñ∞ÈóªÊ£ÄÁ¥¢„ÄÅÂ§öÊ®°ÂºèÊêúÁ¥¢

- **Êõ¥Êñ∞ÊèêÁ§∫**Ôºö
  - ËøôÊòØÁã¨Á´ãÁöÑ AI ÂàÜÊûêÂäüËÉΩÔºå

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NVlabs/PyCuVSLAM]]></title>
            <link>https://github.com/NVlabs/PyCuVSLAM</link>
            <guid>https://github.com/NVlabs/PyCuVSLAM</guid>
            <pubDate>Mon, 19 Jan 2026 00:04:54 GMT</pubDate>
            <description><![CDATA[Highly accurate and efficient VSLAM system for Python]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NVlabs/PyCuVSLAM">NVlabs/PyCuVSLAM</a></h1>
            <p>Highly accurate and efficient VSLAM system for Python</p>
            <p>Language: Python</p>
            <p>Stars: 796</p>
            <p>Forks: 64</p>
            <p>Stars today: 42 stars today</p>
            <h2>README</h2><pre># PyCuVSLAM: CUDA-Accelerated Visual Odometry and Mapping

![Demo](pycuvslam.gif)

### [ArXiv paper](https://www.arxiv.org/abs/2506.04359) | [Python API](https://nvlabs.github.io/PyCuVSLAM/api.html) | [ROS2](https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_visual_slam)

## Overview


PyCuVSLAM is the official Python wrapper for the NVIDIA cuVSLAM library, providing various Visual Tracking Camera modes and Simultaneous Localization and Mapping (SLAM) capabilities. Leveraging CUDA acceleration and a rich set of features, PyCuVSLAM delivers highly accurate, computationally efficient, real-time performance.

![Overview](assets/pycuvslam_overview.png)

## Table of Contents

- üõ†Ô∏è [System Requirements and Setup](#system-requirements-and-setup)
- [üíª Examples and Guides](#examples-and-guides)
- [ü§ñ ROS2 Support](#ros2-support)
- [üìö API Documentation and Technical Report](#api-documentation-and-technical-report)
- [‚öôÔ∏è Performance and Troubleshooting](#performance-and-troubleshooting)
- [‚öñÔ∏è License](#license)
- [üéì Citation](#citation)

## System Requirements and Setup

PyCuVSLAM is supported on the following OS and platforms, with the system requirements and installation methods listed below:

| OS                                | Architecture | System Requirements                          | Supported Installation Method                   |
|-----------------------------------|--------------|----------------------------------------------|-------------------------------------------------|
| Ubuntu 22.04 (Desktop/Laptop)     | x86_64       | Python 3.10, Nvidia GPU with CUDA 12.6       | [Native][3], [Venv][4], [Conda][5], [Docker][6] |
| Ubuntu 24.04 (Desktop/Laptop)     | x86_64       | Nvidia GPU with CUDA 12.6                    | [Conda][5], [Docker][6]                         |
| Ubuntu 22.04 ([Nvidia Jetson][1]) | aarch64      | [Jetpack 6.1/6.2][2], Python 3.10, CUDA 12.6 | [Native][3], [Venv][4], [Conda][5], [Docker][6] |

[1]: https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/
[2]: https://docs.nvidia.com/jetson/archives/jetpack-archived/jetpack-62/
[3]: #option-1-native-install
[4]: #option-2-using-venv
[5]: #option-3-using-conda
[6]: #option-4-using-docker


### CUDA Toolkit
Make sure you have the CUDA Toolkit installed, you can download the toolkit from the [NVIDIA website](https://developer.nvidia.com/cuda-downloads). If you install the CUDA toolkit for the first time, make sure to restart your computer.

### Environment setup
Depending on your OS and platform and the supported installation method, follow the instructions below to environment setup and PyCuVSLAM installation.

&gt;**Note:**  to correctly clone PyCuVSLAM binaries, Git LFS is required before cloning the repository. Please install it by running:
&gt;```bash
&gt;sudo apt-get install git-lfs
&gt;```

#### Option 1: Native Install
**Important**: This option is only available for Ubuntu 22.04 x86_64 and Jetpack 6.1/6.2 aarch64.

There are no special instructions for native install, proceed to [PyCuVSLAM installation](#pycuvslam-installation).

#### Option 2: Using Venv
**Important**: This option is only available for Ubuntu 22.04 x86_64 and Jetpack 6.1/6.2 aarch64.  

Create a virtual environment:

```bash
python3 -m venv .venv
source .venv/bin/activate
```
Proceed to [PyCuVSLAM installation](#pycuvslam-installation)  

#### Option 3: Using Conda

**Important**: This option has been tested on Ubuntu 22.04 x86_64 and Ubuntu 24.04 x86_64

Create a conda environment and install the required packages:

```bash
conda create -n pycuvslam python==3.10 pip
conda activate pycuvslam
conda install -c conda-forge libstdcxx-ng
export LD_LIBRARY_PATH=$CONDA_PREFIX/lib:$LD_LIBRARY_PATH
```
&gt;**Note**: for ubuntu 22.04 use `libstdcxx-ng=12.2.0` version

The `LD_LIBRARY_PATH` environment variable must be set every time you activate the
conda environment to ensure that the correct `libpython` library is loaded.

Proceed to [PyCuVSLAM installation](#pycuvslam-installation)

#### Option 4: Using Docker

PyCuVSLAM provides Docker support for both x86_64 and Jetson platforms with RealSense camera integration.

1. **Setup NGC (NVIDIA GPU Cloud):**
    ```bash
    docker login nvcr.io --username &#039;$oauthtoken&#039;
    ```
    For password, enter your NGC API key from: https://org.ngc.nvidia.com/setup/api-keys

2. **Clone the repository:**
    ```bash
    git clone https://github.com/NVlabs/pycuvslam.git
    cd pycuvslam
    ```

**For x86_64 (Desktop/Laptop):**

3. Build the x86 Docker image:
    ```bash
    docker build -f docker/Dockerfile.realsense-x86 -t pycuvslam:realsense-x86 .
    ```

4. Run the x86 container:
    ```bash
    bash docker/run_docker_x86.sh
    ```

**For Jetson (aarch64):**

3. Build the Jetson Docker image:
    ```bash
    docker build -f docker/Dockerfile.realsense-jetson -t pycuvslam:realsense-jetson .
    ```

4. Run the Jetson container:
    ```bash
    bash docker/run_docker_jetson.sh
    ```

**Features:**
- CUDA 12.6.1 support (Ubuntu 22.04)
- RealSense camera integration with librealsense
- X11 forwarding for GUI applications
- Automatic pycuvslam package installation
- USB device passthrough for camera access

### PyCuVSLAM Installation

1. Clone the PyCuVSLAM repository.
    ```bash
    git clone https://github.com/NVlabs/pycuvslam.git
    cd pycuvslam
    ```

2. Install the PyCuVSLAM package.
    ```bash
    pip install -e bin/x86_64
    ```
    For Jetson, use the following command:
    ```bash
    pip install -e bin/aarch64
    ```
3. Install PyCuVSLAM using one of the installation methods mentioned above, and then install the
   required packages for the examples:
    ```bash
    pip install -r examples/requirements.txt
    ```

## Examples and Guides

Explore various examples to quickly get started with PyCuVSLAM:

### Visual Tracking Mode Examples

- **Monocular Visual Odometry**
    - [EuRoC Dataset](examples/euroc/README.md)

- **Monocular-Depth Visual Odometry**
    - [TUM Dataset](examples/tum/README.md#running-monocular-depth-odometry)
    - [RealSense Live Camera](examples/realsense/README.md#running-monocular-depth-visual-odometry)
    - [ZED Live Camera](examples/zed/README.md#running-monocular-depth-visual-odometry)

- **Stereo Visual Odometry**
    - [KITTI Dataset](examples/kitti/README.md#running-pycuvslam-stereo-visual-odometry)
    - [RealSense Live Camera](examples/realsense/README.md#running-stereo-visual-odometry)
    - [ZED Live Camera](examples/zed/README.md#running-stereo-visual-odometry)
    - [OAK-D Live Camera](examples/oak-d/README.md#running-stereo-visual-odometry)

- **Stereo Visual-Inertial Odometry**
    - [EuRoC Dataset](examples/euroc/README.md#running-stereo-inertial-odometry)
    - [RealSense Live Camera](examples/realsense/README.md#running-stereo-inertial-odometry)

- **Multi-Camera Stereo Visual Odometry**
    - [Tartan Ground Dataset](examples/multicamera_edex/README.md#tartan-ground-dataset)
    - [R2B Galileo Dataset](examples/multicamera_edex/README.md#r2b-galileo-dataset)
    - [RealSense Live Camera](examples/realsense/README.md#running-multicamera-odometry)

### SLAM Examples

- [**Visual Mapping, Localization, and Map Saving/Loading**](examples/kitti/README.md#slam-mapping-collecting-storing-loading-and-localization)

### Advanced Features and Guides

- **Distorted Images**
    - [EuRoC Dataset](examples/euroc/README.md#distortion-models)
    - [OAK-D Live Camera](examples/oak-d/README.md#running-stereo-visual-odometry)
    - [ZED Live Camera](examples/zed/README.md#using-distorted-images)

- **Image Masking**
    - [Static Masks](examples/tum/README.md#masking-regions-to-prevent-feature-selection)
    - [Dynamic Masks](examples/kitti/README.md#dynamic-masks-with-pytorch-tensors)

- [**PyTorch GPU Tensor Handling**](examples/kitti/README.md#example-real-time-car-segmentation)

- [**RealSense Multi-Camera Assembly Guide**](examples/realsense/multicamera_hardware_assembly.md)

- [**Nvblox live 3D reconstruction**](https://nvidia-isaac.github.io/nvblox/pages/torch_examples_realsense.html#realsense-live-example)

## ROS2 Support

If you would like to use cuVSLAM in a ROS2 environment, please refer to the following links:
* [Isaac ROS cuVSLAM GitHub](https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_visual_slam)
* [Isaac ROS cuVSLAM Documentation](https://nvidia-isaac-ros.github.io/concepts/visual_slam/cuvslam/index.html)
* [Isaac ROS cuVSLAM User Manual](https://nvidia-isaac-ros.github.io/repositories_and_packages/isaac_ros_visual_slam/isaac_ros_visual_slam/index.html)

## API Documentation and Technical Report

- For detailed API documentation, please visit the [PyCuVSLAM API Documentation](https://nvlabs.github.io/PyCuVSLAM/api.html)

- For technical details on the cuVSLAM algorithms, validation, and benchmarking results, refer to our [Technical Report](https://www.arxiv.org/abs/2506.04359)

## Performance and Troubleshooting

cuVSLAM is a highly optimized visual tracking library validated across numerous public datasets and popular robotic camera setups. For detailed benchmarking and validation results, please refer to our [technical report](https://arxiv.org/html/2506.04359v3#S3).

&lt;img src=&quot;./assets/cuvslam_performance.png&quot; alt=&quot;cuVSLAM performance&quot; width=&quot;800&quot; /&gt;

The accuracy and robustness of cuVSLAM can be influenced by several factors. If you experience performance issues, please check your system against these common causes:

- **Hardware Overload**: Hardware overload can negatively impact visual tracking, resulting in dropped frames or insufficient computational resources for cuVSLAM. Disable intensive visualization or image-saving operations to improve performance. For expected performance metrics on Jetson embedded platforms, see our [technical report](https://arxiv.org/html/2506.04359v3#A1.F13)

- **Intrinsic and Extrinsic Calibration**: Accurate camera calibration is crucial. Ensure your calibration parameters are precise. For more details, refer to our guide on [image undistortion](examples/euroc/README.md#distortion-models). If you&#039;re new to calibration, consider working with an [experienced vendors](https://nvidia-isaac-ros.github.io/getting_started/hardware_setup/sensors/amr_extrinsic_calibration.html#extrinsic-calibration-of-sensors-in-custom-locations)

- **Synchronization and Timestamps**: Accurate synchronization significantly impacts cuVSLAM performance. Make sure multi-camera images are captured simultaneously‚Äîideally through hardware synchronization‚Äîand verify correct relative timestamps across cameras. Refer to our [multi-camera hardware assembly guide](examples/realsense/multicamera_hardware_assembly.md) for building a rig with synchronized RealSense cameras

- **Frame Rate**: Frame rate significantly affects performance. The ideal frame rate depends on translational and rotational velocities. Typically, 30 FPS is suitable for most &quot;human-speed&quot; motions. Adjust accordingly for faster movements

- **Resolution**: Image resolution matters. VGA resolution or higher is recommended. cuVSLAM efficiently handles relatively high-resolution images due to CUDA acceleration

- **Image Quality**: Ensure good image quality by using suitable lenses, correct exposure, and proper white balance to avoid clipping large image regions. For significant distortion or external objects within the camera&#039;s field of view, please refer to our guide on [static masking](examples/kitti/README.md#static-masks)

- **Motion Blur**: Excessive motion blur can negatively impact tracking. Ensure that exposure times are short enough to minimize motion blur. If avoiding motion blur isn&#039;t feasible, consider increasing the frame rate or try the following [Mono-Depth](examples/realsense/README.md#running-monocular-depth-visual-odometry) or [Stereo Inertial](examples/realsense/README.md#running-stereo-inertial-odometry) tracking modes

### Troubleshooting FAQ

**Q**: When trying to run examples I get `ImportError: pycuvslam/cuvslam/x86/cuvslam/pycuvslam.so: invalid ELF header` 

**A**: You need Git LFS to correctly pull binary files: 
```bash
sudo apt-get install git-lfs
# in the repo directory:
git lfs install
git lfs pull
```

**Q**: Can I run PyCuVSLAM with Python 3.x?

**A**: We are working on supporting wider range of systems, but current version is only built for Python 3.10. We recommend using Docker or Conda for now.

### Reporting other issues
Are you having problems running PyCuVSLAM? Do you have any suggestions? We&#039;d love to hear your feedback in the [issues](https://github.com/NVlabs/pycuvslam/issues) tab.

## License
This project is licensed under a non-commercial NVIDIA license, for details refer to the [LICENCE](LICENSE) file.

## Citation
If you find this work useful in your research, please consider citing:
```bibtex
@article{korovko2025cuvslam,
      title={cuVSLAM: CUDA accelerated visual odometry and mapping}, 
      author={Alexander Korovko and Dmitry Slepichev and Alexander Efitorov and Aigul Dzhumamuratova and Viktor Kuznetsov and Hesam Rabeti and Joydeep Biswas and Soha Pouya},
      year={2025},
      eprint={2506.04359},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2506.04359}, 
}
```</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[freemocap/freemocap]]></title>
            <link>https://github.com/freemocap/freemocap</link>
            <guid>https://github.com/freemocap/freemocap</guid>
            <pubDate>Mon, 19 Jan 2026 00:04:53 GMT</pubDate>
            <description><![CDATA[Free Motion Capture for Everyone üíÄ‚ú®]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/freemocap/freemocap">freemocap/freemocap</a></h1>
            <p>Free Motion Capture for Everyone üíÄ‚ú®</p>
            <p>Language: Python</p>
            <p>Stars: 4,440</p>
            <p>Forks: 360</p>
            <p>Stars today: 47 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/freemocap/freemocap/assets/15314521/da1af7fe-f808-43dc-8f59-c579715d6593&quot; height=&quot;240&quot; alt=&quot;Project Logo&quot;&gt;
&lt;/p&gt; 


&lt;h3 align=&quot;center&quot;&gt;The FreeMoCap Project&lt;/h3&gt;
&lt;h4 align=&quot;center&quot;&gt; A free-and-open-source, hardware-and-software-agnostic, minimal-cost, research-grade, motion capture
system and platform for decentralized scientific research, education, and training&lt;/h2&gt;


&lt;p align=&quot;center&quot;&gt;

&lt;a href=&quot;https://doi.org/10.5281/zenodo.7233714&quot;&gt;
    &lt;img src=&quot;https://zenodo.org/badge/DOI/10.5281/zenodo.7233714.svg&quot; alt=DOI-via-Zenodo.org&gt;
  &lt;/a&gt;

&lt;a href=&quot;https://github.com/psf/black&quot;&gt;
    &lt;img alt=&quot;https://img.shields.io/badge/code%20style-black-000000.svg&quot; src=&quot;https://img.shields.io/badge/code%20style-black-000000.svg&quot;&gt;
  &lt;/a&gt;

&lt;a href=&quot;https://github.com/freemocap/freemocap/releases/latest&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/github/release/freemocap/freemocap.svg&quot; alt=&quot;Latest Release&quot;&gt;
    &lt;/a&gt;

&lt;a href=&quot;https://github.com/freemocap/freemocap/blob/main/LICENSE&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/license-AGPL-blue.svg&quot; alt=&quot;AGPLv3&quot;&gt;
    &lt;/a&gt;

&lt;a href=&quot;https://github.com/freemocap/freemocap/issues&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/contributions-welcome-ff69b4.svg&quot; alt=&quot;Contributions Welcome&quot;&gt;
    &lt;/a&gt;

&lt;a href=&quot;https://github.com/psf/black&quot;&gt;
    &lt;img alt=&quot;https://img.shields.io/badge/code%20style-black-000000.svg&quot; src=&quot;https://img.shields.io/badge/code%20style-black-000000.svg&quot;&gt;
  &lt;/a&gt;

&lt;a href=&quot;https://discord.gg/SgdnzbHDTG&quot;&gt;
    &lt;img alt=&quot;Discord Community Server&quot; src=&quot;https://dcbadge.vercel.app/api/server/SgdnzbHDTG?style=flat&quot;&gt;
  &lt;/a&gt;


&lt;/p&gt;


https://user-images.githubusercontent.com/15314521/192062522-2a8d9305-f181-4869-a4b9-1aa068e094c9.mp4





--
## QUICKSTART

&gt; [!NOTE] 
&gt; For  detailed installation instructions, see our [official documentation&#039;s Installation page](https://freemocap.github.io/documentation/installation.html#detailed-pip-installation-instructions)


#### 0. Create a a Python 3.10 through 3.12 environment (python3.12 recommended)
#### 1. Install software via [pip](https://pypi.org/project/freemocap/#description):

```
pip install freemocap
```

#### 2. Launch the GUI by entering the command:

```
freemocap
``` 

####  3. A GUI should pop up that looks like this: 

   &lt;img width=&quot;1457&quot; alt=&quot;image&quot; src=&quot;https://github.com/freemocap/freemocap/assets/15314521/90ef7e7b-48f3-4f46-8d4a-5b5bcc3254b3&quot;&gt;

#### 4. Have fun! See the [Beginner Tutorials](https://freemocap.github.io/documentation/your-first-recording.html) on our official docs for detailed instructions.

#### 5. [Join the Discord and let us know how it went!](https://discord.gg/nxv5dNTfKT)



___
## Install/run from source code (i.e. the code in this repo)

Open an [Anaconda-enabled command prompt](https://www.anaconda.org) (or your preferred method of environment management) and enter the following commands:

1) Create a `Python` environment (Recommended version  is `python3.11`)

```bash
conda create -n freemocap-env python=3.11
```

2) Activate that newly created environment

```bash
conda activate freemocap-env
```

3) Clone the repository

```bash
git clone https://github.com/freemocap/freemocap
```

4) Navigate into the newly cloned/downloaded `freemocap` folder

```bash
cd freemocap
```

5) Install the package via the `pyproject.toml` file

```bash
pip install -e .
```

6) Launch the GUI (via the `freemocap.__main__.py` entry point)

```bash
python -m freemocap
```

A GUI should pop up!

___

## Documentation 

Our documentation is hosted at: https://freemocap.github.io/documentation

That site is built using `writerside` from this repository: https://github.com/freemocap/documentation

___



### Contribution Guidelines

Please read our contribution doc: [CONTRIBUTING.md](CONTRIBUTING.md)


## Related

[//]: # (* [project-name]&amp;#40;#&amp;#41; - Project description)

## Maintainers

* [Jon Matthis](https://github.com/jonmatthis)
* [Endurance Idehen](https://github.com/endurance)

## License

This project is licensed under the APGL License - see the [LICENSE](LICENSE) file for details.

If the AGPL does not work for your needs, we are happy to discuss terms to license this software to you with a different
agreement at a price point that increases exponentially as you
move [spiritually](https://www.gnu.org/philosophy/open-source-misses-the-point.en.html) away from the `AGPL`

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[lllyasviel/Fooocus]]></title>
            <link>https://github.com/lllyasviel/Fooocus</link>
            <guid>https://github.com/lllyasviel/Fooocus</guid>
            <pubDate>Mon, 19 Jan 2026 00:04:52 GMT</pubDate>
            <description><![CDATA[Focus on prompting and generating]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/lllyasviel/Fooocus">lllyasviel/Fooocus</a></h1>
            <p>Focus on prompting and generating</p>
            <p>Language: Python</p>
            <p>Stars: 47,534</p>
            <p>Forks: 7,759</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[SkyworkAI/SkyReels-V2]]></title>
            <link>https://github.com/SkyworkAI/SkyReels-V2</link>
            <guid>https://github.com/SkyworkAI/SkyReels-V2</guid>
            <pubDate>Mon, 19 Jan 2026 00:04:51 GMT</pubDate>
            <description><![CDATA[SkyReels-V2: Infinite-length Film Generative model]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/SkyworkAI/SkyReels-V2">SkyworkAI/SkyReels-V2</a></h1>
            <p>SkyReels-V2: Infinite-length Film Generative model</p>
            <p>Language: Python</p>
            <p>Stars: 5,778</p>
            <p>Forks: 1,127</p>
            <p>Stars today: 34 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/logo2.png&quot; alt=&quot;SkyReels Logo&quot; width=&quot;50%&quot;&gt;
&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt;SkyReels V2: Infinite-Length Film Generative Model&lt;/h1&gt; 

&lt;p align=&quot;center&quot;&gt;
üìë &lt;a href=&quot;https://arxiv.org/pdf/2504.13074&quot;&gt;Technical Report&lt;/a&gt; ¬∑ üëã &lt;a href=&quot;https://www.skyreels.ai/home?utm_campaign=github_SkyReels_V2&quot; target=&quot;_blank&quot;&gt;Playground&lt;/a&gt; ¬∑ üí¨ &lt;a href=&quot;https://discord.gg/PwM6NYtccQ&quot; target=&quot;_blank&quot;&gt;Discord&lt;/a&gt; ¬∑ ü§ó &lt;a href=&quot;https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9&quot; target=&quot;_blank&quot;&gt;Hugging Face&lt;/a&gt; ¬∑ ü§ñ &lt;a href=&quot;https://www.modelscope.cn/collections/SkyReels-V2-f665650130b144&quot; target=&quot;_blank&quot;&gt;ModelScope&lt;/a&gt;
&lt;/p&gt;

---
Welcome to the **SkyReels V2** repository! Here, you&#039;ll find the model weights and inference code for our infinite-length film generative models. To the best of our knowledge, it represents the first open-source video generative model employing **AutoRegressive Diffusion-Forcing architecture** that achieves the **SOTA performance** among publicly available models.


## üî•üî•üî• News!!
* Jun 1, 2025: üéâ We published the technical report, [SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video Diffusion Transformers](https://arxiv.org/pdf/2506.00830).
* May 16, 2025: üî• We release the inference code for [video extension](#ve) and [start/end frame control](#se) in diffusion forcing model.
* Apr 24, 2025: üî• We release the 720P models, [SkyReels-V2-DF-14B-720P](https://huggingface.co/Skywork/SkyReels-V2-DF-14B-720P) and [SkyReels-V2-I2V-14B-720P](https://huggingface.co/Skywork/SkyReels-V2-I2V-14B-720P). The former facilitates infinite-length autoregressive video generation, and the latter focuses on Image2Video synthesis.
* Apr 21, 2025: üëã We release the inference code and model weights of [SkyReels-V2](https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9) Series Models and the video captioning model [SkyCaptioner-V1](https://huggingface.co/Skywork/SkyCaptioner-V1) .
* Apr 3, 2025: üî• We also release [SkyReels-A2](https://github.com/SkyworkAI/SkyReels-A2). This is an open-sourced controllable video generation framework capable of assembling arbitrary visual elements.
* Feb 18, 2025: üî• we released [SkyReels-A1](https://github.com/SkyworkAI/SkyReels-A1). This is an open-sourced and effective framework for portrait image animation.
* Feb 18, 2025: üî• We released [SkyReels-V1](https://github.com/SkyworkAI/SkyReels-V1). This is the first and most advanced open-source human-centric video foundation model.

## üé• Demos
&lt;table&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;video src=&quot;https://github.com/user-attachments/assets/f6f9f9a7-5d5f-433c-9d73-d8d593b7ad25&quot; width=&quot;100%&quot;&gt;&lt;/video&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;video src=&quot;https://github.com/user-attachments/assets/0eb13415-f4d9-4aaf-bcd3-3031851109b9&quot; width=&quot;100%&quot;&gt;&lt;/video&gt;
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
      &lt;video src=&quot;https://github.com/user-attachments/assets/dcd16603-5bf4-4786-8e4d-1ed23889d07a&quot; width=&quot;100%&quot;&gt;&lt;/video&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;
The demos above showcase 30-second videos generated using our SkyReels-V2 Diffusion Forcing model.


## üìë TODO List

- [x] &lt;a href=&quot;https://arxiv.org/pdf/2504.13074&quot;&gt;Technical Report&lt;/a&gt;
- [x] Checkpoints of the 14B and 1.3B Models Series
- [x] Single-GPU &amp; Multi-GPU Inference Code
- [x] &lt;a href=&quot;https://huggingface.co/Skywork/SkyCaptioner-V1&quot;&gt;SkyCaptioner-V1&lt;/a&gt;: A Video Captioning Model
- [x] Prompt Enhancer
- [x] Diffusers integration
- [ ] Checkpoints of the 5B Models Series
- [ ] Checkpoints of the Camera Director Models
- [ ] Checkpoints of the Step &amp; Guidance Distill Model


## üöÄ Quickstart

#### Installation
```shell
# clone the repository.
git clone https://github.com/SkyworkAI/SkyReels-V2
cd SkyReels-V2
# Install dependencies. Test environment uses Python 3.10.12.
pip install -r requirements.txt
```

#### Model Download
You can download our models from Hugging Face:
&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Type&lt;/th&gt;
      &lt;th&gt;Model Variant&lt;/th&gt;
      &lt;th&gt;Recommended Height/Width/Frame&lt;/th&gt;
      &lt;th&gt;Link&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;5&quot;&gt;Diffusion Forcing&lt;/td&gt;
      &lt;td&gt;1.3B-540P&lt;/td&gt;
      &lt;td&gt;544 * 960 * 97f&lt;/td&gt;
      &lt;td&gt;ü§ó &lt;a href=&quot;https://huggingface.co/Skywork/SkyReels-V2-DF-1.3B-540P&quot;&gt;Huggingface&lt;/a&gt; ü§ñ &lt;a href=&quot;https://www.modelscope.cn/models/Skywork/SkyReels-V2-DF-1.3B-540P&quot;&gt;ModelScope&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5B-540P&lt;/td&gt;
      &lt;td&gt;544 * 960 * 97f&lt;/td&gt;
      &lt;td&gt;Coming Soon&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5B-720P&lt;/td&gt;
      &lt;td&gt;720 * 1280 * 121f&lt;/td&gt;
      &lt;td&gt;Coming Soon&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;14B-540P&lt;/td&gt;
      &lt;td&gt;544 * 960 * 97f&lt;/td&gt;
      &lt;td&gt;ü§ó &lt;a href=&quot;https://huggingface.co/Skywork/SkyReels-V2-DF-14B-540P&quot;&gt;Huggingface&lt;/a&gt; ü§ñ &lt;a href=&quot;https://www.modelscope.cn/models/Skywork/SkyReels-V2-DF-14B-540P&quot;&gt;ModelScope&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;14B-720P&lt;/td&gt;
      &lt;td&gt;720 * 1280 * 121f&lt;/td&gt;
      &lt;td&gt;ü§ó &lt;a href=&quot;https://huggingface.co/Skywork/SkyReels-V2-DF-14B-720P&quot;&gt;Huggingface&lt;/a&gt; ü§ñ &lt;a href=&quot;https://www.modelscope.cn/models/Skywork/SkyReels-V2-DF-14B-720P&quot;&gt;ModelScope&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;5&quot;&gt;Text-to-Video&lt;/td&gt;
      &lt;td&gt;1.3B-540P&lt;/td&gt;
      &lt;td&gt;544 * 960 * 97f&lt;/td&gt;
      &lt;td&gt;Coming Soon&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5B-540P&lt;/td&gt;
      &lt;td&gt;544 * 960 * 97f&lt;/td&gt;
      &lt;td&gt;Coming Soon&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5B-720P&lt;/td&gt;
      &lt;td&gt;720 * 1280 * 121f&lt;/td&gt;
      &lt;td&gt;Coming Soon&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;14B-540P&lt;/td&gt;
      &lt;td&gt;544 * 960 * 97f&lt;/td&gt;
      &lt;td&gt;ü§ó &lt;a href=&quot;https://huggingface.co/Skywork/SkyReels-V2-T2V-14B-540P&quot;&gt;Huggingface&lt;/a&gt; ü§ñ &lt;a href=&quot;https://www.modelscope.cn/models/Skywork/SkyReels-V2-T2V-14B-540P&quot;&gt;ModelScope&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;14B-720P&lt;/td&gt;
      &lt;td&gt;720 * 1280 * 121f&lt;/td&gt;
      &lt;td&gt;ü§ó &lt;a href=&quot;https://huggingface.co/Skywork/SkyReels-V2-T2V-14B-720P&quot;&gt;Huggingface&lt;/a&gt; ü§ñ &lt;a href=&quot;https://www.modelscope.cn/models/Skywork/SkyReels-V2-T2V-14B-720P&quot;&gt;ModelScope&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;5&quot;&gt;Image-to-Video&lt;/td&gt;
      &lt;td&gt;1.3B-540P&lt;/td&gt;
      &lt;td&gt;544 * 960 * 97f&lt;/td&gt;
      &lt;td&gt;ü§ó &lt;a href=&quot;https://huggingface.co/Skywork/SkyReels-V2-I2V-1.3B-540P&quot;&gt;Huggingface&lt;/a&gt; ü§ñ &lt;a href=&quot;https://www.modelscope.cn/models/Skywork/SkyReels-V2-I2V-1.3B-540P&quot;&gt;ModelScope&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5B-540P&lt;/td&gt;
      &lt;td&gt;544 * 960 * 97f&lt;/td&gt;
      &lt;td&gt;Coming Soon&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5B-720P&lt;/td&gt;
      &lt;td&gt;720 * 1280 * 121f&lt;/td&gt;
      &lt;td&gt;Coming Soon&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;14B-540P&lt;/td&gt;
      &lt;td&gt;544 * 960 * 97f&lt;/td&gt;
      &lt;td&gt;ü§ó &lt;a href=&quot;https://huggingface.co/Skywork/SkyReels-V2-I2V-14B-540P&quot;&gt;Huggingface&lt;/a&gt; ü§ñ &lt;a href=&quot;https://www.modelscope.cn/models/Skywork/SkyReels-V2-I2V-14B-540P&quot;&gt;ModelScope&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;14B-720P&lt;/td&gt;
      &lt;td&gt;720 * 1280 * 121f&lt;/td&gt;
      &lt;td&gt;ü§ó &lt;a href=&quot;https://huggingface.co/Skywork/SkyReels-V2-I2V-14B-720P&quot;&gt;Huggingface&lt;/a&gt; ü§ñ &lt;a href=&quot;https://www.modelscope.cn/models/Skywork/SkyReels-V2-I2V-14B-720P&quot;&gt;ModelScope&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td rowspan=&quot;3&quot;&gt;Camera Director&lt;/td&gt;
      &lt;td&gt;5B-540P&lt;/td&gt;
      &lt;td&gt;544 * 960 * 97f&lt;/td&gt;
      &lt;td&gt;Coming Soon&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5B-720P&lt;/td&gt;
      &lt;td&gt;720 * 1280 * 121f&lt;/td&gt;
      &lt;td&gt;Coming Soon&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;14B-720P&lt;/td&gt;
      &lt;td&gt;720 * 1280 * 121f&lt;/td&gt;
      &lt;td&gt;Coming Soon&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

After downloading, set the model path in your generation commands:


#### Single GPU Inference

- **Diffusion Forcing for Long Video Generation**

The &lt;a href=&quot;https://arxiv.org/abs/2407.01392&quot;&gt;**Diffusion Forcing**&lt;/a&gt; version model allows us to generate Infinite-Length videos. This model supports both **text-to-video (T2V)** and **image-to-video (I2V)** tasks, and it can perform inference in both synchronous and asynchronous modes. Here we demonstrate 2 running scripts as examples for long video generation. If you want to adjust the inference parameters, e.g., the duration of video, inference mode, read the Note below first.

synchronous generation for 10s video
```shell
model_id=Skywork/SkyReels-V2-DF-14B-540P
# synchronous inference
python3 generate_video_df.py \
  --model_id ${model_id} \
  --resolution 540P \
  --ar_step 0 \
  --base_num_frames 97 \
  --num_frames 257 \
  --overlap_history 17 \
  --prompt &quot;A graceful white swan with a curved neck and delicate feathers swimming in a serene lake at dawn, its reflection perfectly mirrored in the still water as mist rises from the surface, with the swan occasionally dipping its head into the water to feed.&quot; \
  --addnoise_condition 20 \
  --offload \
  --teacache \
  --use_ret_steps \
  --teacache_thresh 0.3
```

asynchronous generation for 30s video
```shell
model_id=Skywork/SkyReels-V2-DF-14B-540P
# asynchronous inference
python3 generate_video_df.py \
  --model_id ${model_id} \
  --resolution 540P \
  --ar_step 5 \
  --causal_block_size 5 \
  --base_num_frames 97 \
  --num_frames 737 \
  --overlap_history 17 \
  --prompt &quot;A graceful white swan with a curved neck and delicate feathers swimming in a serene lake at dawn, its reflection perfectly mirrored in the still water as mist rises from the surface, with the swan occasionally dipping its head into the water to feed.&quot; \
  --addnoise_condition 20 \
  --offload
```

Text-to-video with `diffusers`:
```py
import torch
from diffusers import AutoModel, SkyReelsV2DiffusionForcingPipeline, UniPCMultistepScheduler
from diffusers.utils import export_to_video

vae = AutoModel.from_pretrained(&quot;Skywork/SkyReels-V2-DF-14B-540P-Diffusers&quot;, subfolder=&quot;vae&quot;, torch_dtype=torch.float32)

pipeline = SkyReelsV2DiffusionForcingPipeline.from_pretrained(
    &quot;Skywork/SkyReels-V2-DF-14B-540P-Diffusers&quot;,
    vae=vae,
    torch_dtype=torch.bfloat16
)
flow_shift = 8.0  # 8.0 for T2V, 5.0 for I2V
pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config, flow_shift=flow_shift)
pipeline = pipeline.to(&quot;cuda&quot;)

prompt = &quot;A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window.&quot;

output = pipeline(
    prompt=prompt,
    num_inference_steps=30,
    height=544,  # 720 for 720P
    width=960,   # 1280 for 720P
    num_frames=97,
    base_num_frames=97,  # 121 for 720P
    ar_step=5,  # Controls asynchronous inference (0 for synchronous mode)
    causal_block_size=5,  # Number of frames in each block for asynchronous processing
    overlap_history=None,  # Number of frames to overlap for smooth transitions in long videos; 17 for long video generations
    addnoise_condition=20,  # Improves consistency in long video generation
).frames[0]
export_to_video(output, &quot;T2V.mp4&quot;, fps=24, quality=8)
```

Image-to-video with `diffusers`:
```py
import numpy as np
import torch
import torchvision.transforms.functional as TF
from diffusers import AutoencoderKLWan, SkyReelsV2DiffusionForcingImageToVideoPipeline, UniPCMultistepScheduler
from diffusers.utils import export_to_video, load_image

model_id = &quot;Skywork/SkyReels-V2-DF-14B-720P-Diffusers&quot;
vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=&quot;vae&quot;, torch_dtype=torch.float32)
pipeline = SkyReelsV2DiffusionForcingImageToVideoPipeline.from_pretrained(
    model_id, vae=vae, torch_dtype=torch.bfloat16
)
flow_shift = 5.0  # 8.0 for T2V, 5.0 for I2V
pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config, flow_shift=flow_shift)
pipeline.to(&quot;cuda&quot;)

first_frame = load_image(&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/flf2v_input_first_frame.png&quot;)
last_frame = load_image(&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/flf2v_input_last_frame.png&quot;)

def aspect_ratio_resize(image, pipeline, max_area=720 * 1280):
    aspect_ratio = image.height / image.width
    mod_value = pipeline.vae_scale_factor_spatial * pipeline.transformer.config.patch_size[1]
    height = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value
    width = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value
    image = image.resize((width, height))
    return image, height, width

def center_crop_resize(image, height, width):
    # Calculate resize ratio to match first frame dimensions
    resize_ratio = max(width / image.width, height / image.height)

    # Resize the image
    width = round(image.width * resize_ratio)
    height = round(image.height * resize_ratio)
    size = [width, height]
    image = TF.center_crop(image, size)

    return image, height, width

first_frame, height, width = aspect_ratio_resize(first_frame, pipeline)
if last_frame.size != first_frame.size:
    last_frame, _, _ = center_crop_resize(last_frame, height, width)

prompt = &quot;CG animation style, a small blue bird takes off from the ground, flapping its wings. The bird&#039;s feathers are delicate, with a unique pattern on its chest. The background shows a blue sky with white clouds under bright sunshine. The camera follows the bird upward, capturing its flight and the vastness of the sky from a close-up, low-angle perspective.&quot;

output = pipeline(
    image=first_frame, last_image=last_frame, prompt=prompt, height=height, width=width, guidance_scale=5.0
).frames[0]
export_to_video(output, &quot;output.mp4&quot;, fps=24, quality=8)
```

&gt; **Note**: 
&gt; - If you want to run the **image-to-video (I2V)** task, add `--image ${image_path}` to your command and it is also better to use **text-to-video (T2V)**-like prompt which includes some descriptions of the first-frame image.
&gt; - For long video generation, you can just switch the `--num_frames`, e.g., `--num_frames 257` for 10s video, `--num_frames 377` for 15s video, `--num_frames 737` for 30s video, `--num_frames 1457` for 60s video. The number is not strictly aligned with the logical frame number for specified time duration, but it is aligned with some training parameters, which means it may perform better. When you use asynchronous inference with causal_block_size &gt; 1, the `--num_frames` should be carefully set.
&gt; - You can use `--ar_step 5` to enable asynchronous inference. When asynchronous inference, `--causal_block_size 5` is recommended while it is not supposed to be set for synchronous generation. REMEMBER that the frame latent number inputted into the model in every iteration, e.g., base frame latent number (e.g., (97-1)//4+1=25 for base_num_frames=97) and (e.g., (237-97-(97-17)x1+17-1)//4+1=20 for base_num_frames=97, num_frames=237, overlap_history=17) for the last iteration, MUST be divided by causal_block_size. If you find it too hard to calculate and set proper values, just use our recommended setting above :). Asynchronous inference will take more steps to diffuse the whole sequence which means it will be SLOWER than synchronous mode. In our experiments, asynchronous inference may improve the instruction following and visual consistent performance.
&gt; - To reduce peak VRAM, just lower the `--base_num_frames`, e.g., to 77 or 57, while keeping the same generative length `--num_frames` you want to generate. This may slightly reduce video quality, and it should not be set too small.
&gt; - `--addnoise_condition` is used to help smooth the long video generation by adding some noise to the clean condition. Too large noise can cause the inconsistency as well. 20 is a recommended value, and you may try larger ones, but it is recommended to not exceed 50.
&gt; - Generating a 540P video using the 1.3B model requires approximately 14.7GB peak VRAM, while the same resolution video using the 14B model demands around 51.2GB peak VRAM.

- **&lt;span id=&quot;ve&quot;&gt;Video Extention&lt;/span&gt;**
```shell
model_id=Skywork/SkyReels-V2-DF-14B-540P
# video extention
python3 generate_video_df.py \
  --model_id ${model_id} \
  --resolution 540P \
  --ar_step 0 \
  --base_num_frames 97 \
  --num_frames 120 \
  --overlap_history 17 \
  --prompt ${prompt} \
  --addnoise_condition 20 \
  --offload \
  --use_ret_steps \
  --teacache \
  --teacache_thresh 0.3 \
  --video_path ${video_path}
```
&gt; **Note**: 
&gt; - When performing video extension, you need to pass the `--video_path  ${video_path}` parameter to specify the video to be extended.

- **&lt;span id=&quot;se&quot;&gt;Start/End Frame Control&lt;/span&gt;**
```shell
model_id=Skywork/SkyReels-V2-DF-14B-540P
# start/end frame control
python3 generate_video_df.py \
  --model_id ${model_id} \
  --resolution 540P \
  --ar_step 0 \
  --base_num_frames 97 \
  --num_frames 97 \
  --overlap_history 17 \
  --prompt ${prompt} \
  --addnoise_condition 20 \
  --offload \
  --use_ret_steps \
  --teacache \
  --teacache_thresh 0.3 \
  --image ${image} \
  --end_image ${end_image}
```
&gt; **Note**:
&gt; - When controlling the start and end frames, you need to pass the `--image  ${image}` parameter to control the generation of the start frame and the `--end_image  ${end_image}` parameter to control the generation of the end frame.

Video extension with `diffusers`:
```py
import numpy as np
import torch
import torchvision.transforms.functional as TF
from diffusers import AutoencoderKLWan, SkyReelsV2DiffusionForcingVideoToVideoPipeline, UniPCMultistepScheduler
from diffusers.utils import export_to_video, load_video

model_id = &quot;Skywork/SkyReels-V2-DF-14B-540P-Diffusers&quot;
vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=&quot;vae&quot;, torch_dtype=torch.float32)
pipeline = SkyReelsV2DiffusionForcingVideoToVideoPipeline.from_pretrained(
    model_id, vae=vae, torch_dtype=torch.bfloat16
)
flow_shift = 5.0  # 8.0 for T2V, 5.0 for I2V
pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config, flow_shift=flow_shift)
pipeline.to(&quot;cuda&quot;)

video = load_video(&quot;input_video.mp4&quot;)

prompt = &quot;CG animation style, a small blue bird takes off from the ground, flapping its wings. The bird&#039;s feathers are delicate, with a unique pattern on its chest. The background shows a blue sky with white clouds under bright sunshine. The camera follows the bird upward, capturing its flight and the vastness of the sky from a close-up, low-angle perspective.&quot;

output = pipeline(
    video=video, prompt=prompt, height=544, width=960, guidance_scale=5.0,
    num_inference_steps=30, num_frames=257, base_num_frames=97#, ar_step=5, causal_block_size=5,
).frames[0]
export_to_video(output, &quot;output.mp4&quot;, fps=24, quality=8)
# Total frames will be the number of frames of given video + 257
```

- **Text To Video &amp; Image To Video**

```shell
# run Text-to-Video Generation
model_id=Skywork/SkyReels-V2-T2V-14B-540P
python3 generate_video.py \
  --model_id ${model_id} \
  --resolution 540P \
  --num_frames 97 \
  --guidance_scale 6.0 \
  --shift 8.0 \
  --fps 24 \
  --prompt &quot;A serene lake surrounded by towering mountains, with a few swans gracefully gliding across the water and sunlight dancing on the surface.&quot; \
  --offload \
  --teacache \
  --use_ret_steps \
  --teacache_thresh 0.3
```
&gt; **Note**: 
&gt; - When using an **image-to-video (I2V)** model, you must provide an input image using the `--image  ${image_path}` parameter. The `--guidance_scale 5.0` and `--shift 3.0` is recommended for I2V model.
&gt; - Generating a 540P video using the 1.3B model requires approximately 14.7GB peak VRAM, while the same resolution video using the 14B model demands around 43.4GB peak VRAM.

T2V models with `diffusers`:
```py
import torch
from diffusers import (
    SkyReelsV2Pipeline,
    UniPCMultistepScheduler,
    AutoencoderKLWan,
)
from diffusers.utils import export_to_video

# Load the pipeline
# Available models:
# - Skywork/SkyReels-V2-T2V-14B-540P-Diffusers
# - Skywork/SkyReels-V2-T2V-14B-720P-Diffusers
vae = AutoencoderKLWan.from_pretrained(
    &quot;Skywork/SkyReels-V2-T2V-14B-720P-Diffusers&quot;,
    subfolder=&quot;vae&quot;,
    torch_dtype=torch.float32,
)
pipe = SkyReelsV2Pipeline.from_pretrained(
    &quot;Skywork/SkyReels-V2-T2V-14B-720P-Diffusers&quot;,
    vae=

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[modelcontextprotocol/python-sdk]]></title>
            <link>https://github.com/modelcontextprotocol/python-sdk</link>
            <guid>https://github.com/modelcontextprotocol/python-sdk</guid>
            <pubDate>Mon, 19 Jan 2026 00:04:50 GMT</pubDate>
            <description><![CDATA[The official Python SDK for Model Context Protocol servers and clients]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/modelcontextprotocol/python-sdk">modelcontextprotocol/python-sdk</a></h1>
            <p>The official Python SDK for Model Context Protocol servers and clients</p>
            <p>Language: Python</p>
            <p>Stars: 21,181</p>
            <p>Forks: 3,005</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre># MCP Python SDK

&lt;div align=&quot;center&quot;&gt;

&lt;strong&gt;Python implementation of the Model Context Protocol (MCP)&lt;/strong&gt;

[![PyPI][pypi-badge]][pypi-url]
[![MIT licensed][mit-badge]][mit-url]
[![Python Version][python-badge]][python-url]
[![Documentation][docs-badge]][docs-url]
[![Protocol][protocol-badge]][protocol-url]
[![Specification][spec-badge]][spec-url]

&lt;/div&gt;

&gt; [!IMPORTANT]
&gt; **This is the `main` branch which contains v2 of the SDK (currently in development, pre-alpha).**
&gt;
&gt; We anticipate a stable v2 release in Q1 2026. Until then, **v1.x remains the recommended version** for production use. v1.x will continue to receive bug fixes and security updates for at least 6 months after v2 ships to give people time to upgrade.
&gt;
&gt; For v1 documentation and code, see the [`v1.x` branch](https://github.com/modelcontextprotocol/python-sdk/tree/v1.x).

&lt;!-- omit in toc --&gt;
## Table of Contents

- [MCP Python SDK](#mcp-python-sdk)
  - [Overview](#overview)
  - [Installation](#installation)
    - [Adding MCP to your python project](#adding-mcp-to-your-python-project)
    - [Running the standalone MCP development tools](#running-the-standalone-mcp-development-tools)
  - [Quickstart](#quickstart)
  - [What is MCP?](#what-is-mcp)
  - [Core Concepts](#core-concepts)
    - [Server](#server)
    - [Resources](#resources)
    - [Tools](#tools)
      - [Structured Output](#structured-output)
    - [Prompts](#prompts)
    - [Images](#images)
    - [Context](#context)
      - [Getting Context in Functions](#getting-context-in-functions)
      - [Context Properties and Methods](#context-properties-and-methods)
    - [Completions](#completions)
    - [Elicitation](#elicitation)
    - [Sampling](#sampling)
    - [Logging and Notifications](#logging-and-notifications)
    - [Authentication](#authentication)
    - [FastMCP Properties](#fastmcp-properties)
    - [Session Properties and Methods](#session-properties-and-methods)
    - [Request Context Properties](#request-context-properties)
  - [Running Your Server](#running-your-server)
    - [Development Mode](#development-mode)
    - [Claude Desktop Integration](#claude-desktop-integration)
    - [Direct Execution](#direct-execution)
    - [Streamable HTTP Transport](#streamable-http-transport)
      - [CORS Configuration for Browser-Based Clients](#cors-configuration-for-browser-based-clients)
    - [Mounting to an Existing ASGI Server](#mounting-to-an-existing-asgi-server)
      - [StreamableHTTP servers](#streamablehttp-servers)
        - [Basic mounting](#basic-mounting)
        - [Host-based routing](#host-based-routing)
        - [Multiple servers with path configuration](#multiple-servers-with-path-configuration)
        - [Path configuration at initialization](#path-configuration-at-initialization)
      - [SSE servers](#sse-servers)
  - [Advanced Usage](#advanced-usage)
    - [Low-Level Server](#low-level-server)
      - [Structured Output Support](#structured-output-support)
    - [Pagination (Advanced)](#pagination-advanced)
    - [Writing MCP Clients](#writing-mcp-clients)
    - [Client Display Utilities](#client-display-utilities)
    - [OAuth Authentication for Clients](#oauth-authentication-for-clients)
    - [Parsing Tool Results](#parsing-tool-results)
    - [MCP Primitives](#mcp-primitives)
    - [Server Capabilities](#server-capabilities)
  - [Documentation](#documentation)
  - [Contributing](#contributing)
  - [License](#license)

[pypi-badge]: https://img.shields.io/pypi/v/mcp.svg
[pypi-url]: https://pypi.org/project/mcp/
[mit-badge]: https://img.shields.io/pypi/l/mcp.svg
[mit-url]: https://github.com/modelcontextprotocol/python-sdk/blob/main/LICENSE
[python-badge]: https://img.shields.io/pypi/pyversions/mcp.svg
[python-url]: https://www.python.org/downloads/
[docs-badge]: https://img.shields.io/badge/docs-python--sdk-blue.svg
[docs-url]: https://modelcontextprotocol.github.io/python-sdk/
[protocol-badge]: https://img.shields.io/badge/protocol-modelcontextprotocol.io-blue.svg
[protocol-url]: https://modelcontextprotocol.io
[spec-badge]: https://img.shields.io/badge/spec-spec.modelcontextprotocol.io-blue.svg
[spec-url]: https://modelcontextprotocol.io/specification/latest

## Overview

The Model Context Protocol allows applications to provide context for LLMs in a standardized way, separating the concerns of providing context from the actual LLM interaction. This Python SDK implements the full MCP specification, making it easy to:

- Build MCP clients that can connect to any MCP server
- Create MCP servers that expose resources, prompts and tools
- Use standard transports like stdio, SSE, and Streamable HTTP
- Handle all MCP protocol messages and lifecycle events

## Installation

### Adding MCP to your python project

We recommend using [uv](https://docs.astral.sh/uv/) to manage your Python projects.

If you haven&#039;t created a uv-managed project yet, create one:

   ```bash
   uv init mcp-server-demo
   cd mcp-server-demo
   ```

   Then add MCP to your project dependencies:

   ```bash
   uv add &quot;mcp[cli]&quot;
   ```

Alternatively, for projects using pip for dependencies:

```bash
pip install &quot;mcp[cli]&quot;
```

### Running the standalone MCP development tools

To run the mcp command with uv:

```bash
uv run mcp
```

## Quickstart

Let&#039;s create a simple MCP server that exposes a calculator tool and some data:

&lt;!-- snippet-source examples/snippets/servers/fastmcp_quickstart.py --&gt;
```python
&quot;&quot;&quot;FastMCP quickstart example.

Run from the repository root:
    uv run examples/snippets/servers/fastmcp_quickstart.py
&quot;&quot;&quot;

from mcp.server.fastmcp import FastMCP

# Create an MCP server
mcp = FastMCP(&quot;Demo&quot;)


# Add an addition tool
@mcp.tool()
def add(a: int, b: int) -&gt; int:
    &quot;&quot;&quot;Add two numbers&quot;&quot;&quot;
    return a + b


# Add a dynamic greeting resource
@mcp.resource(&quot;greeting://{name}&quot;)
def get_greeting(name: str) -&gt; str:
    &quot;&quot;&quot;Get a personalized greeting&quot;&quot;&quot;
    return f&quot;Hello, {name}!&quot;


# Add a prompt
@mcp.prompt()
def greet_user(name: str, style: str = &quot;friendly&quot;) -&gt; str:
    &quot;&quot;&quot;Generate a greeting prompt&quot;&quot;&quot;
    styles = {
        &quot;friendly&quot;: &quot;Please write a warm, friendly greeting&quot;,
        &quot;formal&quot;: &quot;Please write a formal, professional greeting&quot;,
        &quot;casual&quot;: &quot;Please write a casual, relaxed greeting&quot;,
    }

    return f&quot;{styles.get(style, styles[&#039;friendly&#039;])} for someone named {name}.&quot;


# Run with streamable HTTP transport
if __name__ == &quot;__main__&quot;:
    mcp.run(transport=&quot;streamable-http&quot;, json_response=True)
```

_Full example: [examples/snippets/servers/fastmcp_quickstart.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/fastmcp_quickstart.py)_
&lt;!-- /snippet-source --&gt;

You can install this server in [Claude Code](https://docs.claude.com/en/docs/claude-code/mcp) and interact with it right away. First, run the server:

```bash
uv run --with mcp examples/snippets/servers/fastmcp_quickstart.py
```

Then add it to Claude Code:

```bash
claude mcp add --transport http my-server http://localhost:8000/mcp
```

Alternatively, you can test it with the MCP Inspector. Start the server as above, then in a separate terminal:

```bash
npx -y @modelcontextprotocol/inspector
```

In the inspector UI, connect to `http://localhost:8000/mcp`.

## What is MCP?

The [Model Context Protocol (MCP)](https://modelcontextprotocol.io) lets you build servers that expose data and functionality to LLM applications in a secure, standardized way. Think of it like a web API, but specifically designed for LLM interactions. MCP servers can:

- Expose data through **Resources** (think of these sort of like GET endpoints; they are used to load information into the LLM&#039;s context)
- Provide functionality through **Tools** (sort of like POST endpoints; they are used to execute code or otherwise produce a side effect)
- Define interaction patterns through **Prompts** (reusable templates for LLM interactions)
- And more!

## Core Concepts

### Server

The FastMCP server is your core interface to the MCP protocol. It handles connection management, protocol compliance, and message routing:

&lt;!-- snippet-source examples/snippets/servers/lifespan_example.py --&gt;
```python
&quot;&quot;&quot;Example showing lifespan support for startup/shutdown with strong typing.&quot;&quot;&quot;

from collections.abc import AsyncIterator
from contextlib import asynccontextmanager
from dataclasses import dataclass

from mcp.server.fastmcp import Context, FastMCP
from mcp.server.session import ServerSession


# Mock database class for example
class Database:
    &quot;&quot;&quot;Mock database class for example.&quot;&quot;&quot;

    @classmethod
    async def connect(cls) -&gt; &quot;Database&quot;:
        &quot;&quot;&quot;Connect to database.&quot;&quot;&quot;
        return cls()

    async def disconnect(self) -&gt; None:
        &quot;&quot;&quot;Disconnect from database.&quot;&quot;&quot;
        pass

    def query(self) -&gt; str:
        &quot;&quot;&quot;Execute a query.&quot;&quot;&quot;
        return &quot;Query result&quot;


@dataclass
class AppContext:
    &quot;&quot;&quot;Application context with typed dependencies.&quot;&quot;&quot;

    db: Database


@asynccontextmanager
async def app_lifespan(server: FastMCP) -&gt; AsyncIterator[AppContext]:
    &quot;&quot;&quot;Manage application lifecycle with type-safe context.&quot;&quot;&quot;
    # Initialize on startup
    db = await Database.connect()
    try:
        yield AppContext(db=db)
    finally:
        # Cleanup on shutdown
        await db.disconnect()


# Pass lifespan to server
mcp = FastMCP(&quot;My App&quot;, lifespan=app_lifespan)


# Access type-safe lifespan context in tools
@mcp.tool()
def query_db(ctx: Context[ServerSession, AppContext]) -&gt; str:
    &quot;&quot;&quot;Tool that uses initialized resources.&quot;&quot;&quot;
    db = ctx.request_context.lifespan_context.db
    return db.query()
```

_Full example: [examples/snippets/servers/lifespan_example.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/lifespan_example.py)_
&lt;!-- /snippet-source --&gt;

### Resources

Resources are how you expose data to LLMs. They&#039;re similar to GET endpoints in a REST API - they provide data but shouldn&#039;t perform significant computation or have side effects:

&lt;!-- snippet-source examples/snippets/servers/basic_resource.py --&gt;
```python
from mcp.server.fastmcp import FastMCP

mcp = FastMCP(name=&quot;Resource Example&quot;)


@mcp.resource(&quot;file://documents/{name}&quot;)
def read_document(name: str) -&gt; str:
    &quot;&quot;&quot;Read a document by name.&quot;&quot;&quot;
    # This would normally read from disk
    return f&quot;Content of {name}&quot;


@mcp.resource(&quot;config://settings&quot;)
def get_settings() -&gt; str:
    &quot;&quot;&quot;Get application settings.&quot;&quot;&quot;
    return &quot;&quot;&quot;{
  &quot;theme&quot;: &quot;dark&quot;,
  &quot;language&quot;: &quot;en&quot;,
  &quot;debug&quot;: false
}&quot;&quot;&quot;
```

_Full example: [examples/snippets/servers/basic_resource.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/basic_resource.py)_
&lt;!-- /snippet-source --&gt;

### Tools

Tools let LLMs take actions through your server. Unlike resources, tools are expected to perform computation and have side effects:

&lt;!-- snippet-source examples/snippets/servers/basic_tool.py --&gt;
```python
from mcp.server.fastmcp import FastMCP

mcp = FastMCP(name=&quot;Tool Example&quot;)


@mcp.tool()
def sum(a: int, b: int) -&gt; int:
    &quot;&quot;&quot;Add two numbers together.&quot;&quot;&quot;
    return a + b


@mcp.tool()
def get_weather(city: str, unit: str = &quot;celsius&quot;) -&gt; str:
    &quot;&quot;&quot;Get weather for a city.&quot;&quot;&quot;
    # This would normally call a weather API
    return f&quot;Weather in {city}: 22degrees{unit[0].upper()}&quot;
```

_Full example: [examples/snippets/servers/basic_tool.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/basic_tool.py)_
&lt;!-- /snippet-source --&gt;

Tools can optionally receive a Context object by including a parameter with the `Context` type annotation. This context is automatically injected by the FastMCP framework and provides access to MCP capabilities:

&lt;!-- snippet-source examples/snippets/servers/tool_progress.py --&gt;
```python
from mcp.server.fastmcp import Context, FastMCP
from mcp.server.session import ServerSession

mcp = FastMCP(name=&quot;Progress Example&quot;)


@mcp.tool()
async def long_running_task(task_name: str, ctx: Context[ServerSession, None], steps: int = 5) -&gt; str:
    &quot;&quot;&quot;Execute a task with progress updates.&quot;&quot;&quot;
    await ctx.info(f&quot;Starting: {task_name}&quot;)

    for i in range(steps):
        progress = (i + 1) / steps
        await ctx.report_progress(
            progress=progress,
            total=1.0,
            message=f&quot;Step {i + 1}/{steps}&quot;,
        )
        await ctx.debug(f&quot;Completed step {i + 1}&quot;)

    return f&quot;Task &#039;{task_name}&#039; completed&quot;
```

_Full example: [examples/snippets/servers/tool_progress.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/tool_progress.py)_
&lt;!-- /snippet-source --&gt;

#### Structured Output

Tools will return structured results by default, if their return type
annotation is compatible. Otherwise, they will return unstructured results.

Structured output supports these return types:

- Pydantic models (BaseModel subclasses)
- TypedDicts
- Dataclasses and other classes with type hints
- `dict[str, T]` (where T is any JSON-serializable type)
- Primitive types (str, int, float, bool, bytes, None) - wrapped in `{&quot;result&quot;: value}`
- Generic types (list, tuple, Union, Optional, etc.) - wrapped in `{&quot;result&quot;: value}`

Classes without type hints cannot be serialized for structured output. Only
classes with properly annotated attributes will be converted to Pydantic models
for schema generation and validation.

Structured results are automatically validated against the output schema
generated from the annotation. This ensures the tool returns well-typed,
validated data that clients can easily process.

**Note:** For backward compatibility, unstructured results are also
returned. Unstructured results are provided for backward compatibility
with previous versions of the MCP specification, and are quirks-compatible
with previous versions of FastMCP in the current version of the SDK.

**Note:** In cases where a tool function&#039;s return type annotation
causes the tool to be classified as structured _and this is undesirable_,
the  classification can be suppressed by passing `structured_output=False`
to the `@tool` decorator.

##### Advanced: Direct CallToolResult

For full control over tool responses including the `_meta` field (for passing data to client applications without exposing it to the model), you can return `CallToolResult` directly:

&lt;!-- snippet-source examples/snippets/servers/direct_call_tool_result.py --&gt;
```python
&quot;&quot;&quot;Example showing direct CallToolResult return for advanced control.&quot;&quot;&quot;

from typing import Annotated

from pydantic import BaseModel

from mcp.server.fastmcp import FastMCP
from mcp.types import CallToolResult, TextContent

mcp = FastMCP(&quot;CallToolResult Example&quot;)


class ValidationModel(BaseModel):
    &quot;&quot;&quot;Model for validating structured output.&quot;&quot;&quot;

    status: str
    data: dict[str, int]


@mcp.tool()
def advanced_tool() -&gt; CallToolResult:
    &quot;&quot;&quot;Return CallToolResult directly for full control including _meta field.&quot;&quot;&quot;
    return CallToolResult(
        content=[TextContent(type=&quot;text&quot;, text=&quot;Response visible to the model&quot;)],
        _meta={&quot;hidden&quot;: &quot;data for client applications only&quot;},
    )


@mcp.tool()
def validated_tool() -&gt; Annotated[CallToolResult, ValidationModel]:
    &quot;&quot;&quot;Return CallToolResult with structured output validation.&quot;&quot;&quot;
    return CallToolResult(
        content=[TextContent(type=&quot;text&quot;, text=&quot;Validated response&quot;)],
        structured_content={&quot;status&quot;: &quot;success&quot;, &quot;data&quot;: {&quot;result&quot;: 42}},
        _meta={&quot;internal&quot;: &quot;metadata&quot;},
    )


@mcp.tool()
def empty_result_tool() -&gt; CallToolResult:
    &quot;&quot;&quot;For empty results, return CallToolResult with empty content.&quot;&quot;&quot;
    return CallToolResult(content=[])
```

_Full example: [examples/snippets/servers/direct_call_tool_result.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/direct_call_tool_result.py)_
&lt;!-- /snippet-source --&gt;

**Important:** `CallToolResult` must always be returned (no `Optional` or `Union`). For empty results, use `CallToolResult(content=[])`. For optional simple types, use `str | None` without `CallToolResult`.

&lt;!-- snippet-source examples/snippets/servers/structured_output.py --&gt;
```python
&quot;&quot;&quot;Example showing structured output with tools.&quot;&quot;&quot;

from typing import TypedDict

from pydantic import BaseModel, Field

from mcp.server.fastmcp import FastMCP

mcp = FastMCP(&quot;Structured Output Example&quot;)


# Using Pydantic models for rich structured data
class WeatherData(BaseModel):
    &quot;&quot;&quot;Weather information structure.&quot;&quot;&quot;

    temperature: float = Field(description=&quot;Temperature in Celsius&quot;)
    humidity: float = Field(description=&quot;Humidity percentage&quot;)
    condition: str
    wind_speed: float


@mcp.tool()
def get_weather(city: str) -&gt; WeatherData:
    &quot;&quot;&quot;Get weather for a city - returns structured data.&quot;&quot;&quot;
    # Simulated weather data
    return WeatherData(
        temperature=22.5,
        humidity=45.0,
        condition=&quot;sunny&quot;,
        wind_speed=5.2,
    )


# Using TypedDict for simpler structures
class LocationInfo(TypedDict):
    latitude: float
    longitude: float
    name: str


@mcp.tool()
def get_location(address: str) -&gt; LocationInfo:
    &quot;&quot;&quot;Get location coordinates&quot;&quot;&quot;
    return LocationInfo(latitude=51.5074, longitude=-0.1278, name=&quot;London, UK&quot;)


# Using dict[str, Any] for flexible schemas
@mcp.tool()
def get_statistics(data_type: str) -&gt; dict[str, float]:
    &quot;&quot;&quot;Get various statistics&quot;&quot;&quot;
    return {&quot;mean&quot;: 42.5, &quot;median&quot;: 40.0, &quot;std_dev&quot;: 5.2}


# Ordinary classes with type hints work for structured output
class UserProfile:
    name: str
    age: int
    email: str | None = None

    def __init__(self, name: str, age: int, email: str | None = None):
        self.name = name
        self.age = age
        self.email = email


@mcp.tool()
def get_user(user_id: str) -&gt; UserProfile:
    &quot;&quot;&quot;Get user profile - returns structured data&quot;&quot;&quot;
    return UserProfile(name=&quot;Alice&quot;, age=30, email=&quot;alice@example.com&quot;)


# Classes WITHOUT type hints cannot be used for structured output
class UntypedConfig:
    def __init__(self, setting1, setting2):  # type: ignore[reportMissingParameterType]
        self.setting1 = setting1
        self.setting2 = setting2


@mcp.tool()
def get_config() -&gt; UntypedConfig:
    &quot;&quot;&quot;This returns unstructured output - no schema generated&quot;&quot;&quot;
    return UntypedConfig(&quot;value1&quot;, &quot;value2&quot;)


# Lists and other types are wrapped automatically
@mcp.tool()
def list_cities() -&gt; list[str]:
    &quot;&quot;&quot;Get a list of cities&quot;&quot;&quot;
    return [&quot;London&quot;, &quot;Paris&quot;, &quot;Tokyo&quot;]
    # Returns: {&quot;result&quot;: [&quot;London&quot;, &quot;Paris&quot;, &quot;Tokyo&quot;]}


@mcp.tool()
def get_temperature(city: str) -&gt; float:
    &quot;&quot;&quot;Get temperature as a simple float&quot;&quot;&quot;
    return 22.5
    # Returns: {&quot;result&quot;: 22.5}
```

_Full example: [examples/snippets/servers/structured_output.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/structured_output.py)_
&lt;!-- /snippet-source --&gt;

### Prompts

Prompts are reusable templates that help LLMs interact with your server effectively:

&lt;!-- snippet-source examples/snippets/servers/basic_prompt.py --&gt;
```python
from mcp.server.fastmcp import FastMCP
from mcp.server.fastmcp.prompts import base

mcp = FastMCP(name=&quot;Prompt Example&quot;)


@mcp.prompt(title=&quot;Code Review&quot;)
def review_code(code: str) -&gt; str:
    return f&quot;Please review this code:\n\n{code}&quot;


@mcp.prompt(title=&quot;Debug Assistant&quot;)
def debug_error(error: str) -&gt; list[base.Message]:
    return [
        base.UserMessage(&quot;I&#039;m seeing this error:&quot;),
        base.UserMessage(error),
        base.AssistantMessage(&quot;I&#039;ll help debug that. What have you tried so far?&quot;),
    ]
```

_Full example: [examples/snippets/servers/basic_prompt.py](https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/snippets/servers/basic_prompt.py)_
&lt;!-- /snippet-source --&gt;

### Icons

MCP servers can provide icons for UI display. Icons can be added to the server implementation, tools, resources, and prompts:

```python
from mcp.server.fastmcp import FastMCP, Icon

# Create 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[funstory-ai/BabelDOC]]></title>
            <link>https://github.com/funstory-ai/BabelDOC</link>
            <guid>https://github.com/funstory-ai/BabelDOC</guid>
            <pubDate>Mon, 19 Jan 2026 00:04:49 GMT</pubDate>
            <description><![CDATA[Yet Another Document Translator]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/funstory-ai/BabelDOC">funstory-ai/BabelDOC</a></h1>
            <p>Yet Another Document Translator</p>
            <p>Language: Python</p>
            <p>Stars: 7,320</p>
            <p>Forks: 574</p>
            <p>Stars today: 127 stars today</p>
            <h2>README</h2><pre>&lt;!-- # Yet Another Document Translator --&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;!-- &lt;img src=&quot;https://s.immersivetranslate.com/assets/r2-uploads/images/babeldoc-banner.png&quot; width=&quot;320px&quot;  alt=&quot;YADT&quot;/&gt; --&gt;

&lt;br/&gt;

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://s.immersivetranslate.com/assets/uploads/babeldoc-big-logo-darkmode-with-transparent-background-IKuNO1.svg&quot; width=&quot;320px&quot; alt=&quot;BabelDOC&quot;/&gt;
  &lt;img src=&quot;https://s.immersivetranslate.com/assets/uploads/babeldoc-big-logo-with-transparent-background-2xweBr.svg&quot; width=&quot;320px&quot; alt=&quot;BabelDOC&quot;/&gt;
&lt;/picture&gt;

&lt;!-- &lt;h2 id=&quot;title&quot;&gt;BabelDOC&lt;/h2&gt; --&gt;

&lt;p&gt;
  &lt;!-- PyPI --&gt;
  &lt;a href=&quot;https://pypi.org/project/BabelDOC/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/BabelDOC&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pepy.tech/projects/BabelDOC&quot;&gt;
    &lt;img src=&quot;https://static.pepy.tech/badge/BabelDOC&quot;&gt;&lt;/a&gt;
  &lt;!-- &lt;a href=&quot;https://github.com/funstory-ai/BabelDOC/pulls&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/contributions-welcome-green&quot;&gt;&lt;/a&gt; --&gt;
  &lt;!-- License --&gt;
  &lt;a href=&quot;./LICENSE&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/license/funstory-ai/BabelDOC&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://t.me/+Z9_SgnxmsmA5NzBl&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Telegram-2CA5E0?style=flat-squeare&amp;logo=telegram&amp;logoColor=white&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://deepwiki.com/funstory-ai/BabelDOC&quot;&gt;&lt;img src=&quot;https://deepwiki.com/badge.svg&quot; alt=&quot;Ask DeepWiki&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;a href=&quot;https://trendshift.io/repositories/13358&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13358&quot; alt=&quot;funstory-ai%2FBabelDOC | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;

PDF scientific paper translation and bilingual comparison library.

- **Online Service**: Beta version launched [Immersive Translate - BabelDOC](https://app.immersivetranslate.com/babel-doc/) Free usage quota is available; please refer to the FAQ section on the page for details.
- **Self-deployment**: [PDFMathTranslate-next](https://github.com/PDFMathTranslate-next/PDFMathTranslate-next) support for BabelDOC, available for self-deployment + WebUI with more translation services.
- Provides a simple [command line interface](#getting-started).
- Provides a [Python API](#python-api).
- Mainly designed to be embedded into other programs, but can also be used directly for simple translation tasks.

&gt; [!TIP]
&gt;
&gt; How to use BabelDOC in Zotero
&gt;
&gt; 1. Immersive Translate Pro members can use the [immersive-translate/zotero-immersivetranslate](https://github.com/immersive-translate/zotero-immersivetranslate) plugin
&gt;
&gt; 2. PDFMathTranslate self-deployed users can use the [guaguastandup/zotero-pdf2zh](https://github.com/guaguastandup/zotero-pdf2zh) plugin

[Supported Language](https://funstory-ai.github.io/BabelDOC/supported_languages/)

## Preview

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;https://s.immersivetranslate.com/assets/r2-uploads/images/babeldoc-preview.png&quot; width=&quot;80%&quot;/&gt;
&lt;/div&gt;

## We are hiring

See details: [EN](https://github.com/funstory-ai/jobs) | [ZH](https://github.com/funstory-ai/jobs/blob/main/README_ZH.md)

## Getting Started

### Install from PyPI

We recommend using the Tool feature of [uv](https://github.com/astral-sh/uv) to install yadt.

1. First, you need to refer to [uv installation](https://github.com/astral-sh/uv#installation) to install uv and set up the `PATH` environment variable as prompted.

2. Use the following command to install yadt:

```bash
uv tool install --python 3.12 BabelDOC

babeldoc --help
```

3. Use the `babeldoc` command. For example:

```bash
babeldoc --openai --openai-model &quot;gpt-4o-mini&quot; --openai-base-url &quot;https://api.openai.com/v1&quot; --openai-api-key &quot;your-api-key-here&quot;  --files example.pdf

# multiple files
babeldoc --openai --openai-model &quot;gpt-4o-mini&quot; --openai-base-url &quot;https://api.openai.com/v1&quot; --openai-api-key &quot;your-api-key-here&quot;  --files example1.pdf --files example2.pdf
```

### Install from Source

We still recommend using [uv](https://github.com/astral-sh/uv) to manage virtual environments.

1. First, you need to refer to [uv installation](https://github.com/astral-sh/uv#installation) to install uv and set up the `PATH` environment variable as prompted.

2. Use the following command to install yadt:

```bash
# clone the project
git clone https://github.com/funstory-ai/BabelDOC

# enter the project directory
cd BabelDOC

# install dependencies and run babeldoc
uv run babeldoc --help
```

3. Use the `uv run babeldoc` command. For example:

```bash
uv run babeldoc --files example.pdf --openai --openai-model &quot;gpt-4o-mini&quot; --openai-base-url &quot;https://api.openai.com/v1&quot; --openai-api-key &quot;your-api-key-here&quot;

# multiple files
uv run babeldoc --files example.pdf --files example2.pdf --openai --openai-model &quot;gpt-4o-mini&quot; --openai-base-url &quot;https://api.openai.com/v1&quot; --openai-api-key &quot;your-api-key-here&quot;
```

&gt; [!TIP]
&gt; The absolute path is recommended.

## Advanced Options

&gt; [!NOTE]
&gt; This CLI is mainly for debugging purposes. Although end users can use this CLI to translate files, we do not provide any technical support for this purpose.
&gt;
&gt; End users should directly use **Online Service**: Beta version launched [Immersive Translate - BabelDOC](https://app.immersivetranslate.com/babel-doc/) 1000 free pages per month.
&gt;
&gt; End users who need self-deployment should use [PDFMathTranslate 2.0](https://github.com/PDFMathTranslate/PDFMathTranslate-next)
&gt; 
&gt; If you find that an option is not listed below, it means that this option is a debugging option for maintainers. Please do not use these options.


### Language Options

- `--lang-in`, `-li`: Source language code (default: en)
- `--lang-out`, `-lo`: Target language code (default: zh)

&gt; [!TIP]
&gt; Currently, this project mainly focuses on English-to-Chinese translation, and other scenarios have not been tested yet.
&gt; 
&gt; (2025.3.1 update): Basic English target language support has been added, primarily to minimize line breaks within words([0-9A-Za-z]+).
&gt; 
&gt; [HELP WANTED: Collecting word regular expressions for more languages](https://github.com/funstory-ai/BabelDOC/issues/129)

### PDF Processing Options

- `--files`: One or more file paths to input PDF documents.
- `--pages`, `-p`: Specify pages to translate (e.g., &quot;1,2,1-,-3,3-5&quot;). If not set, translate all pages
- `--split-short-lines`: Force split short lines into different paragraphs (may cause poor typesetting &amp; bugs)
- `--short-line-split-factor`: Split threshold factor (default: 0.8). The actual threshold is the median length of all lines on the current page \* this factor
- `--skip-clean`: Skip PDF cleaning step
- `--dual-translate-first`: Put translated pages first in dual PDF mode (default: original pages first)
- `--disable-rich-text-translate`: Disable rich text translation (may help improve compatibility with some PDFs)
- `--enhance-compatibility`: Enable all compatibility enhancement options (equivalent to --skip-clean --dual-translate-first --disable-rich-text-translate)
- `--use-alternating-pages-dual`: Use alternating pages mode for dual PDF. When enabled, original and translated pages are arranged in alternate order. When disabled (default), original and translated pages are shown side by side on the same page.
- `--watermark-output-mode`: Control watermark output mode: &#039;watermarked&#039; (default) adds watermark to translated PDF, &#039;no_watermark&#039; doesn&#039;t add watermark, &#039;both&#039; outputs both versions.
- `--max-pages-per-part`: Maximum number of pages per part for split translation. If not set, no splitting will be performed.
- `--no-watermark`: [DEPRECATED] Use --watermark-output-mode=no_watermark instead.
- `--translate-table-text`: Translate table text (experimental, default: False)
- `--formular-font-pattern`: Font pattern to identify formula text (default: None)
- `--formular-char-pattern`: Character pattern to identify formula text (default: None)
- `--show-char-box`: Show character bounding boxes (debug only, default: False)
- `--skip-scanned-detection`: Skip scanned document detection (default: False). When using split translation, only the first part performs detection if not skipped.
- `--ocr-workaround`: Use OCR workaround (default: False). Only suitable for documents with black text on white background. When enabled, white rectangular blocks will be added below the translation to cover the original text content, and all text will be forced to black color.
- `--auto-enable-ocr-workaround`: Enable automatic OCR workaround (default: False). If a document is detected as heavily scanned, this will attempt to enable OCR processing and skip further scan detection. See &quot;Important Interaction Note&quot; below for crucial details on how this interacts with `--ocr-workaround` and `--skip-scanned-detection`.
- `--primary-font-family`: Override primary font family for translated text. Choices: &#039;serif&#039; for serif fonts, &#039;sans-serif&#039; for sans-serif fonts, &#039;script&#039; for script/italic fonts. If not specified, uses automatic font selection based on original text properties.
- `--only-include-translated-page`: Only include translated pages in the output PDF. This option is only effective when `--pages` is used. (default: False)
- `--merge-alternating-line-numbers`: Enable post-processing to merge alternating line-number layouts (keep the number paragraph as an independent paragraph b; merge adjacent text paragraphs a and c across it when `layout_id` and `xobj_id` match, digits are ASCII and spaces only). Default: off.
- `--skip-form-render`: Skip form rendering (default: False). When enabled, PDF forms will not be rendered in the output.
- `--skip-curve-render`: Skip curve rendering (default: False). When enabled, PDF curves will not be rendered in the output.
- `--only-parse-generate-pdf`: Only parse PDF and generate output PDF without translation (default: False). This skips all translation-related processing including layout analysis, paragraph finding, style processing, and translation itself. Useful for testing PDF parsing and reconstruction functionality.
- `--remove-non-formula-lines`: Remove non-formula lines from paragraph areas (default: False). This removes decorative lines that are not part of formulas, while protecting lines in figure/table areas. Useful for cleaning up documents with decorative elements that interfere with text flow.
- `--non-formula-line-iou-threshold`: IoU threshold for detecting paragraph overlap when removing non-formula lines (default: 0.9). Higher values are more conservative and will remove fewer lines.
- `--figure-table-protection-threshold`: IoU threshold for protecting lines in figure/table areas when removing non-formula lines (default: 0.9). Higher values provide more protection for structural elements in figures and tables.

- `--rpc-doclayout`: RPC service host address for document layout analysis (default: None)
- `--working-dir`: Working directory for translation. If not set, use temp directory.
- `--no-auto-extract-glossary`: Disable automatic term extraction. If this flag is present, the step is skipped. Defaults to enabled.
- `--save-auto-extracted-glossary`: Save automatically extracted glossary to the specified file. If not set, the glossary will not be saved.

&gt; [!TIP]
&gt; - Both `--skip-clean` and `--dual-translate-first` may help improve compatibility with some PDF readers
&gt; - `--disable-rich-text-translate` can also help with compatibility by simplifying translation input
&gt; - However, using `--skip-clean` will result in larger file sizes
&gt; - If you encounter any compatibility issues, try using `--enhance-compatibility` first
&gt; - Use `--max-pages-per-part` for large documents to split them into smaller parts for translation and automatically merge them back.
&gt; - Use `--skip-scanned-detection` to speed up processing when you know your document is not a scanned PDF.
&gt; - Use `--ocr-workaround` to fill background for scanned PDF. (Current assumption: background is pure white, text is pure black, this option will also auto enable `--skip-scanned-detection`)

### Translation Service Options

- `--qps`: QPS (Queries Per Second) limit for translation service (default: 4)
- `--ignore-cache`: Ignore translation cache and force retranslation
- `--no-dual`: Do not output bilingual PDF files
- `--no-mono`: Do not output monolingual PDF files
- `--min-text-length`: Minimum text length to translate (default: 5)
- `--openai`: Use OpenAI for translation (default: False)
- `--custom-system-prompt`: Custom system prompt for translation.
- `--add-formula-placehold-hint`: Add formula placeholder hint for translation. (Currently not recommended, it may affect translation quality, default: False)
- `--disable-same-text-fallback`: Disable fallback translation when LLM output matches input text. (default: False)
- `--pool-max-workers`: Maximum number of worker threads for internal task processing pools. If not specified, defaults to QPS value. This parameter directly sets the worker count, replacing previous QPS-based dynamic calculations.
- `--no-auto-extract-glossary`: Disable automatic term extraction. If this flag is present, the step is skipped. Defaults to enabled.

&gt; [!TIP]
&gt;
&gt; 1. Currently, only OpenAI-compatible LLM is supported. For more translator support, please use [PDFMathTranslate 2.0](https://github.com/PDFMathTranslate/PDFMathTranslate-next).
&gt; 2. It is recommended to use models with strong compatibility with OpenAI, such as: `glm-4-flash`, `deepseek-chat`, etc.
&gt; 3. Currently, it has not been optimized for traditional translation engines like Bing/Google, it is recommended to use LLMs.
&gt; 4. You can use [litellm](https://github.com/BerriAI/litellm) to access multiple models.
&gt; 5. `--custom-system-prompt`: It is mainly used to add the `/no_think` instruction of Qwen 3 in the prompt. For example: `--custom-system-prompt &quot;/no_think You are a professional, authentic machine translation engine.&quot;`

### OpenAI Specific Options

- `--openai-model`: OpenAI model to use (default: gpt-4o-mini)
- `--openai-base-url`: Base URL for OpenAI API
- `--openai-api-key`: API key for OpenAI service
- `--enable-json-mode-if-requested`: Enable JSON mode for OpenAI requests (default: False)
- `--term-pool-max-workers`: Maximum number of worker threads dedicated to automatic term extraction. If not specified, this defaults to the value of `--pool-max-workers`, which itself defaults to the QPS value when unset.

&gt; [!TIP]
&gt;
&gt; 1. This tool supports any OpenAI-compatible API endpoints. Just set the correct base URL and API key. (e.g. `https://xxx.custom.xxx/v1`)
&gt; 2. For local models like Ollama, you can use any value as the API key (e.g. `--openai-api-key a`).

### Glossary Options

- `--glossary-files`: Comma-separated paths to glossary CSV files.
  - Each CSV file should have the columns: `source`, `target`, and an optional `tgt_lng`.
  - The `source` column contains the term in the original language.
  - The `target` column contains the term in the target language.
  - The `tgt_lng` column (optional) specifies the target language for that specific entry (e.g., &quot;zh-CN&quot;, &quot;en-US&quot;).
    - If `tgt_lng` is provided for an entry, that entry will only be loaded and used if its (normalized) `tgt_lng` matches the (normalized) overall target language specified by `--lang-out`. Normalization involves lowercasing and replacing hyphens (`-`) with underscores (`_`).
    - If `tgt_lng` is omitted for an entry, that entry is considered applicable for any `--lang-out`.
  - The name of each glossary (used in LLM prompts) is derived from its filename (without the .csv extension).
  - During translation, the system will check the input text against the loaded glossaries. If terms from a glossary are found in the current text segment, that glossary (with the relevant terms) will be included in the prompt to the language model, along with an instruction to adhere to it.

### Output Control

- `--output`, `-o`: Output directory for translated files. If not set, use current working directory.
- `--debug`: Enable debug logging level and export detailed intermediate results in `~/.cache/yadt/working`.
- `--report-interval`: Progress report interval in seconds (default: 0.1).

### General Options

- `--warmup`: Only download and verify required assets then exit (default: False)

### Offline Assets Management

- `--generate-offline-assets`: Generate an offline assets package in the specified directory. This creates a zip file containing all required models and fonts.
- `--restore-offline-assets`: Restore an offline assets package from the specified file. This extracts models and fonts from a previously generated package.

&gt; [!TIP]
&gt; 
&gt; 1. Offline assets packages are useful for environments without internet access or to speed up installation on multiple machines.
&gt; 2. Generate a package once with `babeldoc --generate-offline-assets /path/to/output/dir` and then distribute it.
&gt; 3. Restore the package on target machines with `babeldoc --restore-offline-assets /path/to/offline_assets_*.zip`.
&gt; 4. The offline assets package name cannot be modified because the file list hash is encoded in the name.
&gt; 5. If you provide a directory path to `--restore-offline-assets`, the tool will automatically look for the correct offline assets package file in that directory.
&gt; 6. The package contains all necessary fonts and models required for document processing, ensuring consistent results across different environments.
&gt; 7. The integrity of all assets is verified using SHA3-256 hashes during both packaging and restoration.
&gt; 8. If you&#039;re deploying in an air-gapped environment, make sure to generate the package on a machine with internet access first.

### Configuration File

- `--config`, `-c`: Configuration file path. Use the TOML format.

Example Configuration:

```toml
[babeldoc]
# Basic settings
debug = true
lang-in = &quot;en-US&quot;
lang-out = &quot;zh-CN&quot;
qps = 10
output = &quot;/path/to/output/dir&quot;

# PDF processing options
split-short-lines = false
short-line-split-factor = 0.8
skip-clean = false
dual-translate-first = false
disable-rich-text-translate = false
use-alternating-pages-dual = false
watermark-output-mode = &quot;watermarked&quot;  # Choices: &quot;watermarked&quot;, &quot;no_watermark&quot;, &quot;both&quot;
max-pages-per-part = 50  # Automatically split the document for translation and merge it back.
only_include_translated_page = false # Only include translated pages in the output PDF. Effective only when `pages` is used.
# no-watermark = false  # DEPRECATED: Use watermark-output-mode instead
skip-scanned-detection = false  # Skip scanned document detection for faster processing
auto_extract_glossary = true # Set to false to disable automatic term extraction
formular_font_pattern = &quot;&quot; # Font pattern for formula text
formular_char_pattern = &quot;&quot; # Character pattern for formula text
show_char_box = false # Show character bounding boxes (debug)
ocr_workaround = false # Use OCR workaround for scanned PDFs
rpc_doclayout = &quot;&quot; # RPC service host for document layout analysis
working_dir = &quot;&quot; # Working directory for translation
auto_enable_ocr_workaround = false # Enable automatic OCR workaround for scanned PDFs. See docs for interaction with ocr_workaround and skip_scanned_detection.
skip_form_render = false # Skip form rendering (default: False)
skip_curve_render = false # Skip curve rendering (default: False)
only_parse_generate_pdf = false # Only parse PDF and generate output PDF without translation (default: False)
remove_non_formula_lines = false # Remove non-formula lines from paragraph areas (default: False)
non_formula_line_iou_threshold = 0.2 # IoU threshold for paragraph overlap detection (default: 0.2)
figure_table_protection_threshold = 0.3 # IoU threshold for figure/table protection (default: 0.3)

# Translation service
openai = true
openai-model = &quot;gpt-4o-mini&quot;
openai-base-url = &quot;https://api.openai.com/v1&quot;
openai-api-key = &quot;your-api-key-here&quot;
enable-json-mode-if-requested = false  # Enable JSON mode when requested (default: false)
disable_same_text_fallback = false # Disable fallback translation when LLM output matches input text (default: false

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>