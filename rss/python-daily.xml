<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Mon, 15 Dec 2025 00:04:58 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[Mebus/cupp]]></title>
            <link>https://github.com/Mebus/cupp</link>
            <guid>https://github.com/Mebus/cupp</guid>
            <pubDate>Mon, 15 Dec 2025 00:04:58 GMT</pubDate>
            <description><![CDATA[Common User Passwords Profiler (CUPP)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Mebus/cupp">Mebus/cupp</a></h1>
            <p>Common User Passwords Profiler (CUPP)</p>
            <p>Language: Python</p>
            <p>Stars: 5,195</p>
            <p>Forks: 1,335</p>
            <p>Stars today: 19 stars today</p>
            <h2>README</h2><pre># CUPP - Common User Passwords Profiler

[![Build Status](https://travis-ci.org/Mebus/cupp.svg?branch=master)](https://travis-ci.org/Mebus/cupp)
[![Coverage Status](https://coveralls.io/repos/github/Mebus/cupp/badge.svg)](https://coveralls.io/github/Mebus/cupp)
[![Codacy Badge](https://api.codacy.com/project/badge/Grade/a578dde078ef481e97a0e7eac0c8d312)](https://app.codacy.com/app/Mebus/cupp?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=Mebus/cupp&amp;utm_campaign=Badge_Grade_Dashboard)
[![Rawsec&#039;s CyberSecurity Inventory](https://inventory.raw.pm/img/badges/Rawsec-inventoried-FF5050_plastic.svg)](https://inventory.raw.pm/)

 
## About

  The most common form of authentication is the combination of a username
  and a password or passphrase. If both match values stored within a locally
  stored table, the user is authenticated for a connection. Password strength is
  a measure of the difficulty involved in guessing or breaking the password
  through cryptographic techniques or library-based automated testing of
  alternate values.

  A weak password might be very short or only use alphanumberic characters,
  making decryption simple. A weak password can also be one that is easily
  guessed by someone profiling the user, such as a birthday, nickname, address,
  name of a pet or relative, or a common word such as God, love, money or password.

  That is why CUPP was born, and it can be used in situations like legal
  penetration tests or forensic crime investigations.


Requirements
------------

You need Python 3 to run CUPP.

Quick start
-----------

    $ python3 cupp.py -h

## Options

  Usage: cupp.py [OPTIONS]

        -h      this menu

        -i      Interactive questions for user password profiling

        -w      Use this option to profile existing dictionary,
                or WyD.pl output to make some pwnsauce :)

        -l      Download huge wordlists from repository

        -a      Parse default usernames and passwords directly from Alecto DB.
                Project Alecto uses purified databases of Phenoelit and CIRT which where merged and enhanced.

        -v      Version of the program



## Configuration

   CUPP has configuration file cupp.cfg with instructions.

## Example (Fast forwarded)

![cupp-example](screenshots/cupp-example.gif)

## License

  This program is free software; you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation; either version 3 of the License, or
  any later version.

  This program is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with this program; if not, write to the Free Software
  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA

  See &#039;./LICENSE&#039; for more information.

## Github import

This project was imported into https://github.com/Mebus/cupp by Mebus from:  
http://www.remote-exploit.org/content/cupp-3.0.tar.gz  
http://www.remote-exploit.org/articles/misc_research__amp_code/index.html  
to encourage further development of the tool.

## Original author

  Muris Kurgas aka j0rgan  
  j0rgan@remote-exploit.org  
  http://www.remote-exploit.org  
  http://www.azuzi.me  


## Contributors

  * Bosko Petrovic aka bolexxx  
  bole_loser@hotmail.com  
  http://www.offensive-security.com  
  http://www.bolexxx.net  

  * Mebus  
    https://github.com/Mebus/  

  * Abhro  
    https://github.com/Abhro/  

  * Andrea Giacomo  
    https://github.com/codepr

  * quantumcore  
    https://github.com/quantumcore
    

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[datawhalechina/hello-agents]]></title>
            <link>https://github.com/datawhalechina/hello-agents</link>
            <guid>https://github.com/datawhalechina/hello-agents</guid>
            <pubDate>Mon, 15 Dec 2025 00:04:57 GMT</pubDate>
            <description><![CDATA[ğŸ“š ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹â€”â€”ä»é›¶å¼€å§‹çš„æ™ºèƒ½ä½“åŸç†ä¸å®è·µæ•™ç¨‹]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/datawhalechina/hello-agents">datawhalechina/hello-agents</a></h1>
            <p>ğŸ“š ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹â€”â€”ä»é›¶å¼€å§‹çš„æ™ºèƒ½ä½“åŸç†ä¸å®è·µæ•™ç¨‹</p>
            <p>Language: Python</p>
            <p>Stars: 9,050</p>
            <p>Forks: 965</p>
            <p>Stars today: 354 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;right&quot;&gt;
  &lt;a href=&quot;./README_EN.md&quot;&gt;English&lt;/a&gt; | ä¸­æ–‡
&lt;/div&gt;

&lt;div align=&#039;center&#039;&gt;
  &lt;img src=&quot;./docs/images/hello-agents.png&quot; alt=&quot;alt text&quot; width=&quot;100%&quot;&gt;
  &lt;h1&gt;Hello-Agents&lt;/h1&gt;
  &lt;h3&gt;ğŸ¤– ã€Šä»é›¶å¼€å§‹æ„å»ºæ™ºèƒ½ä½“ã€‹&lt;/h3&gt;
  &lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/15520&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/15520&quot; alt=&quot;datawhalechina%2Fhello-agents | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
  &lt;/a&gt;
  &lt;/div&gt;
  &lt;p&gt;&lt;em&gt;ä»åŸºç¡€ç†è®ºåˆ°å®é™…åº”ç”¨ï¼Œå…¨é¢æŒæ¡æ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡ä¸å®ç°&lt;/em&gt;&lt;/p&gt;
  &lt;img src=&quot;https://img.shields.io/github/stars/datawhalechina/Hello-Agents?style=flat&amp;logo=github&quot; alt=&quot;GitHub stars&quot;/&gt;
  &lt;img src=&quot;https://img.shields.io/github/forks/datawhalechina/Hello-Agents?style=flat&amp;logo=github&quot; alt=&quot;GitHub forks&quot;/&gt;
  &lt;img src=&quot;https://img.shields.io/badge/language-Chinese-brightgreen?style=flat&quot; alt=&quot;Language&quot;/&gt;
  &lt;a href=&quot;https://github.com/datawhalechina/Hello-Agents&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-Project-blue?style=flat&amp;logo=github&quot; alt=&quot;GitHub Project&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://datawhalechina.github.io/hello-agents/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/åœ¨çº¿é˜…è¯»-Online%20Reading-green?style=flat&amp;logo=gitbook&quot; alt=&quot;Online Reading&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

---

## ğŸ¯ é¡¹ç›®ä»‹ç»

&amp;emsp;&amp;emsp;å¦‚æœè¯´ 2024 å¹´æ˜¯&quot;ç™¾æ¨¡å¤§æˆ˜&quot;çš„å…ƒå¹´ï¼Œé‚£ä¹ˆ 2025 å¹´æ— ç–‘å¼€å¯äº†&quot;Agent å…ƒå¹´&quot;ã€‚æŠ€æœ¯çš„ç„¦ç‚¹æ­£ä»è®­ç»ƒæ›´å¤§çš„åŸºç¡€æ¨¡å‹ï¼Œè½¬å‘æ„å»ºæ›´èªæ˜çš„æ™ºèƒ½ä½“åº”ç”¨ã€‚ç„¶è€Œï¼Œå½“å‰ç³»ç»Ÿæ€§ã€é‡å®è·µçš„æ•™ç¨‹å´æåº¦åŒ®ä¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å‘èµ·äº† Hello-Agents é¡¹ç›®ï¼Œå¸Œæœ›èƒ½ä¸ºç¤¾åŒºæä¾›ä¸€æœ¬ä»é›¶å¼€å§‹ã€ç†è®ºä¸å®æˆ˜å¹¶é‡çš„æ™ºèƒ½ä½“ç³»ç»Ÿæ„å»ºæŒ‡å—ã€‚

&amp;emsp;&amp;emsp;Hello-Agents æ˜¯ Datawhale ç¤¾åŒºçš„&lt;strong&gt;ç³»ç»Ÿæ€§æ™ºèƒ½ä½“å­¦ä¹ æ•™ç¨‹&lt;/strong&gt;ã€‚å¦‚ä»Š Agent æ„å»ºä¸»è¦åˆ†ä¸ºä¸¤æ´¾ï¼Œä¸€æ´¾æ˜¯ Difyï¼ŒCozeï¼Œn8n è¿™ç±»è½¯ä»¶å·¥ç¨‹ç±» Agentï¼Œå…¶æœ¬è´¨æ˜¯æµç¨‹é©±åŠ¨çš„è½¯ä»¶å¼€å‘ï¼ŒLLM ä½œä¸ºæ•°æ®å¤„ç†çš„åç«¯ï¼›å¦ä¸€æ´¾åˆ™æ˜¯ AI åŸç”Ÿçš„ Agentï¼Œå³çœŸæ­£ä»¥ AI é©±åŠ¨çš„ Agentã€‚æœ¬æ•™ç¨‹æ—¨åœ¨å¸¦é¢†å¤§å®¶æ·±å…¥ç†è§£å¹¶æ„å»ºåè€…â€”â€”çœŸæ­£çš„ AI Native Agentã€‚æ•™ç¨‹å°†å¸¦é¢†ä½ ç©¿é€æ¡†æ¶è¡¨è±¡ï¼Œä»æ™ºèƒ½ä½“çš„æ ¸å¿ƒåŸç†å‡ºå‘ï¼Œæ·±å…¥å…¶æ ¸å¿ƒæ¶æ„ï¼Œç†è§£å…¶ç»å…¸èŒƒå¼ï¼Œå¹¶æœ€ç»ˆäº²æ‰‹æ„å»ºèµ·å±äºè‡ªå·±çš„å¤šæ™ºèƒ½ä½“åº”ç”¨ã€‚æˆ‘ä»¬ç›¸ä¿¡ï¼Œæœ€å¥½çš„å­¦ä¹ æ–¹å¼å°±æ˜¯åŠ¨æ‰‹å®è·µã€‚å¸Œæœ›è¿™æœ¬æ•™ç¨‹èƒ½æˆä¸ºä½ æ¢ç´¢æ™ºèƒ½ä½“ä¸–ç•Œçš„èµ·ç‚¹ï¼Œèƒ½å¤Ÿä»ä¸€åå¤§è¯­è¨€æ¨¡å‹çš„&quot;ä½¿ç”¨è€…&quot;ï¼Œèœ•å˜ä¸ºä¸€åæ™ºèƒ½ä½“ç³»ç»Ÿçš„&quot;æ„å»ºè€…&quot;ã€‚

## ğŸ“š å¿«é€Ÿå¼€å§‹

### åœ¨çº¿é˜…è¯»
**[ğŸŒ ç‚¹å‡»è¿™é‡Œå¼€å§‹åœ¨çº¿é˜…è¯»](https://datawhalechina.github.io/hello-agents/)** - æ— éœ€ä¸‹è½½ï¼Œéšæ—¶éšåœ°å­¦ä¹ 

**[ğŸ“– Cookbook(æµ‹è¯•ç‰ˆ)](https://book.heterocat.com.cn/)**

### æœ¬åœ°é˜…è¯»
å¦‚æœæ‚¨å¸Œæœ›åœ¨æœ¬åœ°é˜…è¯»æˆ–è´¡çŒ®å†…å®¹ï¼Œè¯·å‚è€ƒä¸‹æ–¹çš„å­¦ä¹ æŒ‡å—ã€‚

### âœ¨ ä½ å°†æ”¶è·ä»€ä¹ˆï¼Ÿ

- ğŸ“– &lt;strong&gt;Datawhale å¼€æºå…è´¹&lt;/strong&gt; å®Œå…¨å…è´¹å­¦ä¹ æœ¬é¡¹ç›®æ‰€æœ‰å†…å®¹ï¼Œä¸ç¤¾åŒºå…±åŒæˆé•¿
- ğŸ” &lt;strong&gt;ç†è§£æ ¸å¿ƒåŸç†&lt;/strong&gt; æ·±å…¥ç†è§£æ™ºèƒ½ä½“çš„æ¦‚å¿µã€å†å²ä¸ç»å…¸èŒƒå¼
- ğŸ—ï¸ &lt;strong&gt;äº²æ‰‹å®ç°&lt;/strong&gt; æŒæ¡çƒ­é—¨ä½ä»£ç å¹³å°å’Œæ™ºèƒ½ä½“ä»£ç æ¡†æ¶çš„ä½¿ç”¨
- ğŸ› ï¸ &lt;strong&gt;è‡ªç ”æ¡†æ¶[HelloAgents](https://github.com/jjyaoao/helloagents)&lt;/strong&gt; åŸºäº Openai åŸç”Ÿ API ä»é›¶æ„å»ºä¸€ä¸ªè‡ªå·±çš„æ™ºèƒ½ä½“æ¡†æ¶
- âš™ï¸ &lt;strong&gt;æŒæ¡é«˜çº§æŠ€èƒ½&lt;/strong&gt; ä¸€æ­¥æ­¥å®ç°ä¸Šä¸‹æ–‡å·¥ç¨‹ã€Memoryã€åè®®ã€è¯„ä¼°ç­‰ç³»ç»Ÿæ€§æŠ€æœ¯
- ğŸ¤ &lt;strong&gt;æ¨¡å‹è®­ç»ƒ&lt;/strong&gt; æŒæ¡ Agentic RLï¼Œä» SFT åˆ° GRPO çš„å…¨æµç¨‹å®æˆ˜è®­ç»ƒ LLM
- ğŸš€ &lt;strong&gt;é©±åŠ¨çœŸå®æ¡ˆä¾‹&lt;/strong&gt; å®æˆ˜å¼€å‘æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹ã€èµ›åšå°é•‡ç­‰ç»¼åˆé¡¹ç›®
- ğŸ“– &lt;strong&gt;æ±‚èŒé¢è¯•&lt;/strong&gt; å­¦ä¹ æ™ºèƒ½ä½“æ±‚èŒç›¸å…³é¢è¯•é—®é¢˜

## ğŸ“– å†…å®¹å¯¼èˆª

| ç« èŠ‚                                                                                        | å…³é”®å†…å®¹                                      | çŠ¶æ€ |
| ------------------------------------------------------------------------------------------- | --------------------------------------------- | ---- |
| [å‰è¨€](./docs/å‰è¨€.md)                                                                      | é¡¹ç›®çš„ç¼˜èµ·ã€èƒŒæ™¯åŠè¯»è€…å»ºè®®                    | âœ…    |
| &lt;strong&gt;ç¬¬ä¸€éƒ¨åˆ†ï¼šæ™ºèƒ½ä½“ä¸è¯­è¨€æ¨¡å‹åŸºç¡€&lt;/strong&gt;                                             |                                               |      |
| [ç¬¬ä¸€ç«  åˆè¯†æ™ºèƒ½ä½“](./docs/chapter1/ç¬¬ä¸€ç« %20åˆè¯†æ™ºèƒ½ä½“.md)                                 | æ™ºèƒ½ä½“å®šä¹‰ã€ç±»å‹ã€èŒƒå¼ä¸åº”ç”¨                  | âœ…    |
| [ç¬¬äºŒç«  æ™ºèƒ½ä½“å‘å±•å²](./docs/chapter2/ç¬¬äºŒç« %20æ™ºèƒ½ä½“å‘å±•å².md)                             | ä»ç¬¦å·ä¸»ä¹‰åˆ° LLM é©±åŠ¨çš„æ™ºèƒ½ä½“æ¼”è¿›             | âœ…    |
| [ç¬¬ä¸‰ç«  å¤§è¯­è¨€æ¨¡å‹åŸºç¡€](./docs/chapter3/ç¬¬ä¸‰ç« %20å¤§è¯­è¨€æ¨¡å‹åŸºç¡€.md)                         | Transformerã€æç¤ºã€ä¸»æµ LLM åŠå…¶å±€é™          | âœ…    |
| &lt;strong&gt;ç¬¬äºŒéƒ¨åˆ†ï¼šæ„å»ºä½ çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“&lt;/strong&gt;                                         |                                               |      |
| [ç¬¬å››ç«  æ™ºèƒ½ä½“ç»å…¸èŒƒå¼æ„å»º](./docs/chapter4/ç¬¬å››ç« %20æ™ºèƒ½ä½“ç»å…¸èŒƒå¼æ„å»º.md)                 | æ‰‹æŠŠæ‰‹å®ç° ReActã€Plan-and-Solveã€Reflection  | âœ…    |
| [ç¬¬äº”ç«  åŸºäºä½ä»£ç å¹³å°çš„æ™ºèƒ½ä½“æ­å»º](./docs/chapter5/ç¬¬äº”ç« %20åŸºäºä½ä»£ç å¹³å°çš„æ™ºèƒ½ä½“æ­å»º.md) | äº†è§£ Cozeã€Difyã€n8n ç­‰ä½ä»£ç æ™ºèƒ½ä½“å¹³å°ä½¿ç”¨   | âœ…    |
| [ç¬¬å…­ç«  æ¡†æ¶å¼€å‘å®è·µ](./docs/chapter6/ç¬¬å…­ç« %20æ¡†æ¶å¼€å‘å®è·µ.md)                             | AutoGenã€AgentScopeã€LangGraph ç­‰ä¸»æµæ¡†æ¶åº”ç”¨ | âœ…    |
| [ç¬¬ä¸ƒç«  æ„å»ºä½ çš„Agentæ¡†æ¶](./docs/chapter7/ç¬¬ä¸ƒç« %20æ„å»ºä½ çš„Agentæ¡†æ¶.md)                   | ä» 0 å¼€å§‹æ„å»ºæ™ºèƒ½ä½“æ¡†æ¶                       | âœ…    |
| &lt;strong&gt;ç¬¬ä¸‰éƒ¨åˆ†ï¼šé«˜çº§çŸ¥è¯†æ‰©å±•&lt;/strong&gt;                                                     |                                               |      |
| [ç¬¬å…«ç«  è®°å¿†ä¸æ£€ç´¢](./docs/chapter8/ç¬¬å…«ç« %20è®°å¿†ä¸æ£€ç´¢.md)                                 | è®°å¿†ç³»ç»Ÿï¼ŒRAGï¼Œå­˜å‚¨                           | âœ…    |
| [ç¬¬ä¹ç«  ä¸Šä¸‹æ–‡å·¥ç¨‹](./docs/chapter9/ç¬¬ä¹ç« %20ä¸Šä¸‹æ–‡å·¥ç¨‹.md)                                 | æŒç»­äº¤äº’çš„&quot;æƒ…å¢ƒç†è§£&quot;                          | âœ…    |
| [ç¬¬åç«  æ™ºèƒ½ä½“é€šä¿¡åè®®](./docs/chapter10/ç¬¬åç« %20æ™ºèƒ½ä½“é€šä¿¡åè®®.md)                        | MCPã€A2Aã€ANP ç­‰åè®®è§£æ                      | âœ…    |
| [ç¬¬åä¸€ç«  Agentic-RL](./docs/chapter11/ç¬¬åä¸€ç« %20Agentic-RL.md)                            | ä» SFT åˆ° GRPO çš„ LLM è®­ç»ƒå®æˆ˜                | âœ…    |
| [ç¬¬åäºŒç«  æ™ºèƒ½ä½“æ€§èƒ½è¯„ä¼°](./docs/chapter12/ç¬¬åäºŒç« %20æ™ºèƒ½ä½“æ€§èƒ½è¯„ä¼°.md)                    | æ ¸å¿ƒæŒ‡æ ‡ã€åŸºå‡†æµ‹è¯•ä¸è¯„ä¼°æ¡†æ¶                  | âœ…    |
| &lt;strong&gt;ç¬¬å››éƒ¨åˆ†ï¼šç»¼åˆæ¡ˆä¾‹è¿›é˜¶&lt;/strong&gt;                                                     |                                               |      |
| [ç¬¬åä¸‰ç«  æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹](./docs/chapter13/ç¬¬åä¸‰ç« %20æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹.md)                        | MCP ä¸å¤šæ™ºèƒ½ä½“åä½œçš„çœŸå®ä¸–ç•Œåº”ç”¨              | âœ…    |
| [ç¬¬åå››ç«  è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“](./docs/chapter14/ç¬¬åå››ç« %20è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“.md)        | DeepResearch Agent å¤ç°ä¸è§£æ                 | âœ…    |
| [ç¬¬åäº”ç«  æ„å»ºèµ›åšå°é•‡](./docs/chapter15/ç¬¬åäº”ç« %20æ„å»ºèµ›åšå°é•‡.md)                        | Agent ä¸æ¸¸æˆçš„ç»“åˆï¼Œæ¨¡æ‹Ÿç¤¾ä¼šåŠ¨æ€              | âœ…    |
| &lt;strong&gt;ç¬¬äº”éƒ¨åˆ†ï¼šæ¯•ä¸šè®¾è®¡åŠæœªæ¥å±•æœ›&lt;/strong&gt;                                               |                                               |      |
| [ç¬¬åå…­ç«  æ¯•ä¸šè®¾è®¡](./docs/chapter16/ç¬¬åå…­ç« %20æ¯•ä¸šè®¾è®¡.md)                                | æ„å»ºå±äºä½ çš„å®Œæ•´å¤šæ™ºèƒ½ä½“åº”ç”¨                  | âœ…    |

### ç¤¾åŒºè´¡çŒ®ç²¾é€‰ (Community Blog)

&amp;emsp;&amp;emsp;æ¬¢è¿å¤§å®¶å°†åœ¨å­¦ä¹  Hello-Agents æˆ– Agent ç›¸å…³æŠ€æœ¯ä¸­çš„ç‹¬åˆ°è§è§£ã€å®è·µæ€»ç»“ï¼Œä»¥ PR çš„å½¢å¼è´¡çŒ®åˆ°ç¤¾åŒºç²¾é€‰ã€‚å¦‚æœæ˜¯ç‹¬ç«‹äºæ­£æ–‡çš„å†…å®¹ï¼Œä¹Ÿå¯ä»¥æŠ•ç¨¿è‡³ Extra-Chapterï¼&lt;strong&gt;æœŸå¾…ä½ çš„ç¬¬ä¸€æ¬¡è´¡çŒ®ï¼&lt;/strong&gt;

| ç¤¾åŒºç²¾é€‰                                                                                                                                      | å†…å®¹æ€»ç»“                 |
| --------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------ |
| [01-Agenté¢è¯•é¢˜æ€»ç»“](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra01-é¢è¯•é—®é¢˜æ€»ç»“.md)                          | Agent å²—ä½ç›¸å…³é¢è¯•é—®é¢˜   |
| [01-Agenté¢è¯•é¢˜ç­”æ¡ˆ](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra01-å‚è€ƒç­”æ¡ˆ.md)                              | ç›¸å…³é¢è¯•é—®é¢˜ç­”æ¡ˆ         |
| [02-ä¸Šä¸‹æ–‡å·¥ç¨‹å†…å®¹è¡¥å……](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra02-ä¸Šä¸‹æ–‡å·¥ç¨‹è¡¥å……çŸ¥è¯†.md)                 | ä¸Šä¸‹æ–‡å·¥ç¨‹å†…å®¹æ‰©å±•       |
| [03-Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ•™ç¨‹](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra03-Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ“ä½œæµç¨‹.md) | Difyæ™ºèƒ½ä½“åˆ›å»ºä¿å§†çº§æ•™ç¨‹ |
| [04-Hello-agentsè¯¾ç¨‹å¸¸è§é—®é¢˜](https://github.com/datawhalechina/hello-agents/blob/main/Extra-Chapter/Extra04-DatawhaleFAQ.md)                 | Datawhaleè¯¾ç¨‹å¸¸è§é—®é¢˜    |

### PDF ç‰ˆæœ¬ä¸‹è½½

&amp;emsp;&amp;emsp;*&lt;strong&gt;æœ¬ Hello-Agents PDF æ•™ç¨‹å®Œå…¨å¼€æºå…è´¹ã€‚ä¸ºé˜²æ­¢å„ç±»è¥é”€å·åŠ æ°´å°åè´©å–ç»™å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåˆå­¦è€…ï¼Œæˆ‘ä»¬ç‰¹åœ°åœ¨ PDF æ–‡ä»¶ä¸­é¢„å…ˆæ·»åŠ äº†ä¸å½±å“é˜…è¯»çš„ Datawhale å¼€æºæ ‡å¿—æ°´å°ï¼Œæ•¬è¯·è°…è§£ï½&lt;/strong&gt;*

&gt; *Hello-Agents PDF : https://github.com/datawhalechina/hello-agents/releases/tag/V1.0.0*  
&gt; *Hello-Agents PDF å›½å†…ä¸‹è½½åœ°å€ : https://www.datawhale.cn/learn/summary/239* 

## ğŸ’¡ å¦‚ä½•å­¦ä¹ 

&amp;emsp;&amp;emsp;æ¬¢è¿ä½ ï¼Œæœªæ¥çš„æ™ºèƒ½ç³»ç»Ÿæ„å»ºè€…ï¼åœ¨å¼€å¯è¿™æ®µæ¿€åŠ¨äººå¿ƒçš„æ—…ç¨‹ä¹‹å‰ï¼Œè¯·å…è®¸æˆ‘ä»¬ç»™ä½ ä¸€äº›æ¸…æ™°çš„æŒ‡å¼•ã€‚

&amp;emsp;&amp;emsp;æœ¬é¡¹ç›®å†…å®¹å…¼é¡¾ç†è®ºä¸å®æˆ˜ï¼Œæ—¨åœ¨å¸®åŠ©ä½ ç³»ç»Ÿæ€§åœ°æŒæ¡ä»å•ä¸ªæ™ºèƒ½ä½“åˆ°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡ä¸å¼€å‘å…¨æµç¨‹ã€‚å› æ­¤ï¼Œå°¤å…¶é€‚åˆæœ‰ä¸€å®šç¼–ç¨‹åŸºç¡€çš„ &lt;strong&gt;AI å¼€å‘è€…ã€è½¯ä»¶å·¥ç¨‹å¸ˆã€åœ¨æ ¡å­¦ç”Ÿ&lt;/strong&gt; ä»¥åŠå¯¹å‰æ²¿ AI æŠ€æœ¯æŠ±æœ‰æµ“åšå…´è¶£çš„ &lt;strong&gt;è‡ªå­¦è€…&lt;/strong&gt;ã€‚åœ¨å­¦ä¹ æœ¬é¡¹ç›®ä¹‹å‰ï¼Œæˆ‘ä»¬å¸Œæœ›ä½ å…·å¤‡åŸºç¡€çš„ Python ç¼–ç¨‹èƒ½åŠ›ï¼Œå¹¶å¯¹å¤§è¯­è¨€æ¨¡å‹æœ‰åŸºæœ¬çš„æ¦‚å¿µæ€§äº†è§£ï¼ˆä¾‹å¦‚ï¼ŒçŸ¥é“å¦‚ä½•é€šè¿‡ API è°ƒç”¨ä¸€ä¸ª LLMï¼‰ã€‚é¡¹ç›®çš„é‡ç‚¹æ˜¯åº”ç”¨ä¸æ„å»ºï¼Œå› æ­¤ä½ æ— éœ€å…·å¤‡æ·±åšçš„ç®—æ³•æˆ–æ¨¡å‹è®­ç»ƒèƒŒæ™¯ã€‚

&amp;emsp;&amp;emsp;é¡¹ç›®åˆ†ä¸ºäº”å¤§éƒ¨åˆ†ï¼Œæ¯ä¸€éƒ¨åˆ†éƒ½æ˜¯é€šå¾€ä¸‹ä¸€é˜¶æ®µçš„åšå®é˜¶æ¢¯ï¼š

- &lt;strong&gt;ç¬¬ä¸€éƒ¨åˆ†ï¼šæ™ºèƒ½ä½“ä¸è¯­è¨€æ¨¡å‹åŸºç¡€&lt;/strong&gt;ï¼ˆç¬¬ä¸€ç« ï½ç¬¬ä¸‰ç« ï¼‰ï¼Œæˆ‘ä»¬å°†ä»æ™ºèƒ½ä½“çš„å®šä¹‰ã€ç±»å‹ä¸å‘å±•å†å²è®²èµ·ï¼Œä¸ºä½ æ¢³ç†&quot;æ™ºèƒ½ä½“&quot;è¿™ä¸€æ¦‚å¿µçš„æ¥é¾™å»è„‰ã€‚éšåï¼Œæˆ‘ä»¬ä¼šå¿«é€Ÿå·©å›ºå¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒçŸ¥è¯†ï¼Œä¸ºä½ çš„å®è·µä¹‹æ—…æ‰“ä¸‹åšå®çš„ç†è®ºåœ°åŸºã€‚

- &lt;strong&gt;ç¬¬äºŒéƒ¨åˆ†ï¼šæ„å»ºä½ çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“&lt;/strong&gt;ï¼ˆç¬¬å››ç« ï½ç¬¬ä¸ƒç« ï¼‰ï¼Œè¿™æ˜¯ä½ åŠ¨æ‰‹å®è·µçš„èµ·ç‚¹ã€‚ä½ å°†äº²æ‰‹å®ç° ReAct ç­‰ç»å…¸èŒƒå¼ï¼Œä½“éªŒ Coze ç­‰ä½ä»£ç å¹³å°çš„ä¾¿æ·ï¼Œå¹¶æŒæ¡ Langgraph ç­‰ä¸»æµæ¡†æ¶çš„åº”ç”¨ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è¿˜ä¼šå¸¦ä½ ä»é›¶å¼€å§‹æ„å»ºä¸€ä¸ªå±äºè‡ªå·±çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œè®©ä½ å…¼å…·â€œç”¨è½®å­â€ä¸â€œé€ è½®å­â€çš„èƒ½åŠ›ã€‚

- &lt;strong&gt;ç¬¬ä¸‰éƒ¨åˆ†ï¼šé«˜çº§çŸ¥è¯†æ‰©å±•&lt;/strong&gt;ï¼ˆç¬¬å…«ç« ï½ç¬¬åäºŒç« ï¼‰ï¼Œåœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œä½ çš„æ™ºèƒ½ä½“å°†â€œå­¦ä¼šâ€æ€è€ƒä¸åä½œã€‚æˆ‘ä»¬å°†ä½¿ç”¨ç¬¬äºŒéƒ¨åˆ†çš„è‡ªç ”æ¡†æ¶ï¼Œæ·±å…¥æ¢ç´¢è®°å¿†ä¸æ£€ç´¢ã€ä¸Šä¸‹æ–‡å·¥ç¨‹ã€Agent è®­ç»ƒç­‰æ ¸å¿ƒæŠ€æœ¯ï¼Œå¹¶å­¦ä¹ å¤šæ™ºèƒ½ä½“é—´çš„é€šä¿¡åè®®ã€‚æœ€ç»ˆï¼Œä½ å°†æŒæ¡è¯„ä¼°æ™ºèƒ½ä½“ç³»ç»Ÿæ€§èƒ½çš„ä¸“ä¸šæ–¹æ³•ã€‚

- &lt;strong&gt;ç¬¬å››éƒ¨åˆ†ï¼šç»¼åˆæ¡ˆä¾‹è¿›é˜¶&lt;/strong&gt;ï¼ˆç¬¬åä¸‰ç« ï½ç¬¬åäº”ç« ï¼‰ï¼Œè¿™é‡Œæ˜¯ç†è®ºä¸å®è·µçš„äº¤æ±‡ç‚¹ã€‚ä½ å°†æŠŠæ‰€å­¦èä¼šè´¯é€šï¼Œäº²æ‰‹æ‰“é€ æ™ºèƒ½æ—…è¡ŒåŠ©æ‰‹ã€è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“ï¼Œä¹ƒè‡³ä¸€ä¸ªæ¨¡æ‹Ÿç¤¾ä¼šåŠ¨æ€çš„èµ›åšå°é•‡ï¼Œåœ¨çœŸå®æœ‰è¶£çš„é¡¹ç›®ä¸­æ·¬ç‚¼ä½ çš„æ„å»ºèƒ½åŠ›ã€‚

- &lt;strong&gt;ç¬¬äº”éƒ¨åˆ†ï¼šæ¯•ä¸šè®¾è®¡åŠæœªæ¥å±•æœ›&lt;/strong&gt;ï¼ˆç¬¬åå…­ç« ï¼‰ï¼Œåœ¨æ—…ç¨‹çš„ç»ˆç‚¹ï¼Œä½ å°†è¿æ¥ä¸€ä¸ªæ¯•ä¸šè®¾è®¡ï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„ã€å±äºä½ è‡ªå·±çš„å¤šæ™ºèƒ½ä½“åº”ç”¨ï¼Œå…¨é¢æ£€éªŒä½ çš„å­¦ä¹ æˆæœã€‚æˆ‘ä»¬è¿˜å°†ä¸ä½ ä¸€åŒå±•æœ›æ™ºèƒ½ä½“çš„æœªæ¥ï¼Œæ¢ç´¢æ¿€åŠ¨äººå¿ƒçš„å‰æ²¿æ–¹å‘ã€‚


&amp;emsp;&amp;emsp;æ™ºèƒ½ä½“æ˜¯ä¸€ä¸ªé£é€Ÿå‘å±•ä¸”æåº¦ä¾èµ–å®è·µçš„é¢†åŸŸã€‚ä¸ºäº†è·å¾—æœ€ä½³çš„å­¦ä¹ æ•ˆæœï¼Œæˆ‘ä»¬åœ¨é¡¹ç›®çš„`code`æ–‡ä»¶å¤¹å†…æä¾›äº†é…å¥—çš„å…¨éƒ¨ä»£ç ï¼Œå¼ºçƒˆå»ºè®®ä½ &lt;strong&gt;å°†ç†è®ºä¸å®è·µç›¸ç»“åˆ&lt;/strong&gt;ã€‚è¯·åŠ¡å¿…äº²æ‰‹è¿è¡Œã€è°ƒè¯•ç”šè‡³ä¿®æ”¹é¡¹ç›®é‡Œæä¾›çš„æ¯ä¸€ä»½ä»£ç ã€‚æ¬¢è¿ä½ éšæ—¶å…³æ³¨ Datawhale ä»¥åŠå…¶ä»– Agent ç›¸å…³ç¤¾åŒºï¼Œå½“é‡åˆ°é—®é¢˜æ—¶ï¼Œä½ å¯ä»¥éšæ—¶åœ¨æœ¬é¡¹ç›®çš„ issue åŒºæé—®ã€‚

&amp;emsp;&amp;emsp;ç°åœ¨ï¼Œå‡†å¤‡å¥½è¿›å…¥æ™ºèƒ½ä½“çš„å¥‡å¦™ä¸–ç•Œäº†å—ï¼Ÿè®©æˆ‘ä»¬å³åˆ»å¯ç¨‹ï¼

## ä¸‹ä¸€æ­¥è§„åˆ’
- []è‹±æ–‡ç‰ˆæ•™ç¨‹
- []åŒè¯­è§†é¢‘è¯¾ç¨‹[è‹±æ–‡+ä¸­æ–‡]ï¼ˆå°†ä¼šæ›´åŠ ç»†è‡´ï¼Œå®è·µè¯¾å¸¦é¢†å¤§å®¶ä»è®¾è®¡æ€è·¯åˆ°å®æ–½ï¼Œæˆäººä»¥é±¼ä¹Ÿæˆäººä»¥æ¸”ï¼‰
- []å…±åˆ›ç¬¬16ç« ï¼ˆæ‰“é€ å„ç±»Agentåº”ç”¨,æ›´æ‰“é€ Agentç”Ÿæ€ï¼‰
  
## ğŸ¤ å¦‚ä½•è´¡çŒ®

æˆ‘ä»¬æ˜¯ä¸€ä¸ªå¼€æ”¾çš„å¼€æºç¤¾åŒºï¼Œæ¬¢è¿ä»»ä½•å½¢å¼çš„è´¡çŒ®ï¼

- ğŸ› &lt;strong&gt;æŠ¥å‘Š Bug&lt;/strong&gt; - å‘ç°å†…å®¹æˆ–ä»£ç é—®é¢˜ï¼Œè¯·æäº¤ Issue
- ğŸ’¡ &lt;strong&gt;æå‡ºå»ºè®®&lt;/strong&gt; - å¯¹é¡¹ç›®æœ‰å¥½æƒ³æ³•ï¼Œæ¬¢è¿å‘èµ·è®¨è®º
- ğŸ“ &lt;strong&gt;å®Œå–„å†…å®¹&lt;/strong&gt; - å¸®åŠ©æ”¹è¿›æ•™ç¨‹ï¼Œæäº¤ä½ çš„ Pull Request
- âœï¸ &lt;strong&gt;åˆ†äº«å®è·µ&lt;/strong&gt; - åœ¨&quot;ç¤¾åŒºè´¡çŒ®ç²¾é€‰&quot;ä¸­åˆ†äº«ä½ çš„å­¦ä¹ ç¬”è®°å’Œé¡¹ç›®

## ğŸ™ è‡´è°¢

### æ ¸å¿ƒè´¡çŒ®è€…
- [é™ˆæ€å·-é¡¹ç›®è´Ÿè´£äºº](https://github.com/jjyaoao) (Datawhale æˆå‘˜, å…¨æ–‡å†™ä½œå’Œæ ¡å¯¹)
- [å­™éŸ¬-é¡¹ç›®è´Ÿè´£äºº](https://github.com/fengju0213) (Datawhale æˆå‘˜, ç¬¬ä¹ç« å†…å®¹å’Œæ ¡å¯¹)  
- [å§œèˆ’å‡¡-é¡¹ç›®è´Ÿè´£äºº](https://github.com/Tsumugii24)ï¼ˆDatawhale æˆå‘˜, ç« èŠ‚ä¹ é¢˜è®¾è®¡å’Œæ ¡å¯¹ï¼‰
- [é»„ä½©æ—-Datawhaleæ„å‘æˆå‘˜](https://github.com/HeteroCat) (Agent å¼€å‘å·¥ç¨‹å¸ˆ, ç¬¬äº”ç« å†…å®¹è´¡çŒ®è€…)
- [æ›¾é‘«æ°‘-Agentå·¥ç¨‹å¸ˆ](https://github.com/fancyboi999) (ç‰›å®¢ç§‘æŠ€, ç¬¬åå››ç« æ¡ˆä¾‹å¼€å‘)
- [æœ±ä¿¡å¿ -æŒ‡å¯¼ä¸“å®¶](https://xinzhongzhu.github.io/) (Datawhaleé¦–å¸­ç§‘å­¦å®¶-æµ™æ±Ÿå¸ˆèŒƒå¤§å­¦æ­å·äººå·¥æ™ºèƒ½ç ”ç©¶é™¢æ•™æˆ)
### Extra-Chapter è´¡çŒ®è€…
- [WH](https://github.com/WHQAQ11) (å†…å®¹è´¡çŒ®è€…)
- [å‘¨å¥¥æ°-DWè´¡çŒ®è€…å›¢é˜Ÿ](https://github.com/thunderbolt-fire) (è¥¿å®‰äº¤é€šå¤§å­¦, Extra02 å†…å®¹è´¡çŒ®)
- [å¼ å®¸æ—­-ä¸ªäººå¼€å‘è€…](https://github.com/Tasselszcx)(å¸å›½ç†å·¥å­¦é™¢, Extra03 å†…å®¹è´¡çŒ®)
- [é»„å®æ™—-DWè´¡çŒ®è€…å›¢é˜Ÿ](https://github.com/XiaoMa-PM) (æ·±åœ³å¤§å­¦, Extra04 å†…å®¹è´¡çŒ®)

### ç‰¹åˆ«æ„Ÿè°¢
- æ„Ÿè°¢ [@Sm1les](https://github.com/Sm1les) å¯¹æœ¬é¡¹ç›®çš„å¸®åŠ©ä¸æ”¯æŒ
- æ„Ÿè°¢æ‰€æœ‰ä¸ºæœ¬é¡¹ç›®åšå‡ºè´¡çŒ®çš„å¼€å‘è€…ä»¬ â¤ï¸

&lt;div align=center style=&quot;margin-top: 30px;&quot;&gt;
  &lt;a href=&quot;https://github.com/datawhalechina/Hello-Agents/graphs/contributors&quot;&gt;
    &lt;img src=&quot;https://contrib.rocks/image?repo=datawhalechina/Hello-Agents&quot; /&gt;
  &lt;/a&gt;
&lt;/div&gt;

## Star History

&lt;div align=&#039;center&#039;&gt;
    &lt;img src=&quot;./docs/images/star-history-20251212.png&quot; alt=&quot;Datawhale&quot; width=&quot;90%&quot;&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼&lt;/p&gt;
&lt;/div&gt;

## å…³äº Datawhale

&lt;div align=&#039;center&#039;&gt;
    &lt;img src=&quot;./docs/images/datawhale.png&quot; alt=&quot;Datawhale&quot; width=&quot;30%&quot;&gt;
    &lt;p&gt;æ‰«æäºŒç»´ç å…³æ³¨ Datawhale å…¬ä¼—å·ï¼Œè·å–æ›´å¤šä¼˜è´¨å¼€æºå†…å®¹&lt;/p&gt;
&lt;/div&gt;

---

## ğŸ“œ å¼€æºåè®®

æœ¬ä½œå“é‡‡ç”¨[çŸ¥è¯†å…±äº«ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç›¸åŒæ–¹å¼å…±äº« 4.0 å›½é™…è®¸å¯åè®®](http://creativecommons.org/licenses/by-nc-sa/4.0/)è¿›è¡Œè®¸å¯ã€‚
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[thinking-machines-lab/tinker-cookbook]]></title>
            <link>https://github.com/thinking-machines-lab/tinker-cookbook</link>
            <guid>https://github.com/thinking-machines-lab/tinker-cookbook</guid>
            <pubDate>Mon, 15 Dec 2025 00:04:56 GMT</pubDate>
            <description><![CDATA[Post-training with Tinker]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/thinking-machines-lab/tinker-cookbook">thinking-machines-lab/tinker-cookbook</a></h1>
            <p>Post-training with Tinker</p>
            <p>Language: Python</p>
            <p>Stars: 2,468</p>
            <p>Forks: 229</p>
            <p>Stars today: 38 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;Tinker Cookbook&lt;/h1&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/tinker-cover.png&quot; width=&quot;60%&quot; /&gt;
&lt;/div&gt;

We provide two libraries for the broader community to customize their language models: `tinker` and `tinker-cookbook`.

- `tinker` is a training SDK for researchers and developers to fine-tune language models. You send API requests to us and we handle the complexities of distributed training.
- `tinker-cookbook` includes realistic examples of fine-tuning language models. It builds on the Tinker API and provides common abstractions to fine-tune language models.

## Installation

1. Sign up for Tinker through the [waitlist](https://thinkingmachines.ai/tinker).
2. Once you have access, create an API key from the [console](https://tinker-console.thinkingmachines.ai) and export it as environment variable `TINKER_API_KEY`.
3. Install tinker python client via `pip install tinker`
4. We recommend installing `tinker-cookbook` in a virtual env either with `conda` or `uv`. For running most examples, you can install via `pip install -e .`.

## Tinker

Refer to the [docs](https://tinker-docs.thinkingmachines.ai/training-sampling) to start from basics.
Here we introduce a few Tinker primitives - the basic components to fine-tune LLMs:

```python
import tinker
service_client = tinker.ServiceClient()
training_client = service_client.create_lora_training_client(
  base_model=&quot;meta-llama/Llama-3.2-1B&quot;, rank=32,
)
training_client.forward_backward(...)
training_client.optim_step(...)
training_client.save_state(...)
training_client.load_state(...)

sampling_client = training_client.save_weights_and_get_sampling_client(name=&quot;my_model&quot;)
sampling_client.sample(...)
```

See [tinker_cookbook/recipes/sl_loop.py](tinker_cookbook/recipes/sl_loop.py) and [tinker_cookbook/recipes/rl_loop.py](tinker_cookbook/recipes/rl_loop.py) for minimal examples of using these primitives to fine-tune LLMs.

To download the weights of any model:
```python
rest_client = service_client.create_rest_client()
future = rest_client.get_checkpoint_archive_url_from_tinker_path(sampling_client.model_path)
with open(f&quot;model-checkpoint.tar.gz&quot;, &quot;wb&quot;) as f:
    f.write(future.result())
```

### Tinker Cookbook

Besides these primitives, we also offer **Tinker Cookbook** (a.k.a. this repo), a library of a wide range of abstractions to help you customize training environments.
[`tinker_cookbook/recipes/sl_basic.py`](tinker_cookbook/recipes/sl_basic.py) and [`tinker_cookbook/recipes/rl_basic.py`](tinker_cookbook/recipes/rl_basic.py) contain minimal examples to configure supervised learning and reinforcement learning.

We also include a wide range of more sophisticated examples in the [`tinker_cookbook/recipes/`](tinker_cookbook/recipes/) folder:
1. **[Chat supervised learning](tinker_cookbook/recipes/chat_sl/)**: supervised fine-tuning on conversational datasets like Tulu3.
2. **[Math reasoning](tinker_cookbook/recipes/math_rl/)**: improve LLM reasoning capability by rewarding it for answering math questions correctly.
3. **[Preference learning](tinker_cookbook/recipes/preference/)**: showcase a three-stage RLHF pipeline: 1) supervised fine-tuning, 2) learning a reward model, 3) RL against the reward model.
4. **[Tool use](tinker_cookbook/recipes/tool_use/)**: train LLMs to better use retrieval tools to answer questions more accurately.
5. **[Prompt distillation](tinker_cookbook/recipes/prompt_distillation/)**: internalize long and complex instructions into LLMs.
6. **[Multi-Agent](tinker_cookbook/recipes/multiplayer_rl/)**: optimize LLMs to play against another LLM or themselves.

These examples are located in each subfolder, and their `README.md` files will walk you through the key implementation details, the commands to run them, and the expected performance.

### Documentation

The `docs/` directory contains a mirror of the Tinker documentation. These files are synced from our internal documentation site.

**Note:** The documentation files use MDX format (Markdown with JSX), which includes some syntax that isn&#039;t standard Markdown. You may see things like `import` statements, `&lt;Callout&gt;` components, or curly-brace expressions. These are artifacts of our documentation framework - the actual content should still be readable as Markdown.

If you find errors or want to improve the documentation, feel free to submit a PR editing files in `docs/`. We&#039;ll sync the changes back to our documentation site.

For the rendered documentation, visit [tinker-docs.thinkingmachines.ai](https://tinker-docs.thinkingmachines.ai).

### Import our utilities

Tinker cookbook includes several utilities. Here&#039;s a quick overview:
- [`renderers`](tinker_cookbook/renderers.py) converts tokens from/to structured chat message objects
- [`hyperparam_utils`](tinker_cookbook/hyperparam_utils.py) helps calculate hyperparameters suitable for LoRAs
- [`evaluation`](tinker_cookbook/eval/evaluators.py) provides abstractions for evaluating Tinker models and [`inspect_evaluation`](tinker_cookbook/eval/inspect_evaluators.py) shows how to integrate with InspectAI to make evaluating on standard benchmarks easy.

## Contributing

This project is built in the spirit of open science and collaborative development. We believe that the best tools emerge through community involvement and shared learning.

We welcome PR contributions after our private beta is over. If you have any feedback, please email us at tinker@thinkingmachines.ai.

## Citation
If you use Tinker for your research, please cite it as:
```
Thinking Machines Lab, 2025. Tinker. https://thinkingmachines.ai/tinker/.
```

Or use this BibTeX citation:
```
@misc{tml2025tinker,
  author = {Thinking Machines Lab},
  title = {Tinker},
  year = {2025},
  url = {https://thinkingmachines.ai/tinker/},
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[virattt/ai-hedge-fund]]></title>
            <link>https://github.com/virattt/ai-hedge-fund</link>
            <guid>https://github.com/virattt/ai-hedge-fund</guid>
            <pubDate>Mon, 15 Dec 2025 00:04:55 GMT</pubDate>
            <description><![CDATA[An AI Hedge Fund Team]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/virattt/ai-hedge-fund">virattt/ai-hedge-fund</a></h1>
            <p>An AI Hedge Fund Team</p>
            <p>Language: Python</p>
            <p>Stars: 42,728</p>
            <p>Forks: 7,604</p>
            <p>Stars today: 31 stars today</p>
            <h2>README</h2><pre># AI Hedge Fund

This is a proof of concept for an AI-powered hedge fund.  The goal of this project is to explore the use of AI to make trading decisions.  This project is for **educational** purposes only and is not intended for real trading or investment.

This system employs several agents working together:

1. Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation
2. Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety
3. Bill Ackman Agent - An activist investor, takes bold positions and pushes for change
4. Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption
5. Charlie Munger Agent - Warren Buffett&#039;s partner, only buys wonderful businesses at fair prices
6. Michael Burry Agent - The Big Short contrarian who hunts for deep value
7. Mohnish Pabrai Agent - The Dhandho investor, who looks for doubles at low risk
8. Peter Lynch Agent - Practical investor who seeks &quot;ten-baggers&quot; in everyday businesses
9. Phil Fisher Agent - Meticulous growth investor who uses deep &quot;scuttlebutt&quot; research 
10. Rakesh Jhunjhunwala Agent - The Big Bull of India
11. Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential
12. Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price
13. Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals
14. Sentiment Agent - Analyzes market sentiment and generates trading signals
15. Fundamentals Agent - Analyzes fundamental data and generates trading signals
16. Technicals Agent - Analyzes technical indicators and generates trading signals
17. Risk Manager - Calculates risk metrics and sets position limits
18. Portfolio Manager - Makes final trading decisions and generates orders

&lt;img width=&quot;1042&quot; alt=&quot;Screenshot 2025-03-22 at 6 19 07 PM&quot; src=&quot;https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4&quot; /&gt;

Note: the system does not actually make any trades.

[![Twitter Follow](https://img.shields.io/twitter/follow/virattt?style=social)](https://twitter.com/virattt)

## Disclaimer

This project is for **educational and research purposes only**.

- Not intended for real trading or investment
- No investment advice or guarantees provided
- Creator assumes no liability for financial losses
- Consult a financial advisor for investment decisions
- Past performance does not indicate future results

By using this software, you agree to use it solely for learning purposes.

## Table of Contents
- [How to Install](#how-to-install)
- [How to Run](#how-to-run)
  - [âŒ¨ï¸ Command Line Interface](#ï¸-command-line-interface)
  - [ğŸ–¥ï¸ Web Application](#ï¸-web-application)
- [How to Contribute](#how-to-contribute)
- [Feature Requests](#feature-requests)
- [License](#license)

## How to Install

Before you can run the AI Hedge Fund, you&#039;ll need to install it and set up your API keys. These steps are common to both the full-stack web application and command line interface.

### 1. Clone the Repository

```bash
git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
```

### 2. Set up API keys

Create a `.env` file for your API keys:
```bash
# Create .env file for your API keys (in the root directory)
cp .env.example .env
```

Open and edit the `.env` file to add your API keys:
```bash
# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
OPENAI_API_KEY=your-openai-api-key

# For getting financial data to power the hedge fund
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
```

**Important**: You must set at least one LLM API key (e.g. `OPENAI_API_KEY`, `GROQ_API_KEY`, `ANTHROPIC_API_KEY`, or `DEEPSEEK_API_KEY`) for the hedge fund to work. 

**Financial Data**: Data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key. For any other ticker, you will need to set the `FINANCIAL_DATASETS_API_KEY` in the .env file.

## How to Run

### âŒ¨ï¸ Command Line Interface

You can run the AI Hedge Fund directly via terminal. This approach offers more granular control and is useful for automation, scripting, and integration purposes.

&lt;img width=&quot;992&quot; alt=&quot;Screenshot 2025-01-06 at 5 50 17 PM&quot; src=&quot;https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b&quot; /&gt;

#### Quick Start

1. Install Poetry (if not already installed):
```bash
curl -sSL https://install.python-poetry.org | python3 -
```

2. Install dependencies:
```bash
poetry install
```

#### Run the AI Hedge Fund
```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA
```

You can also specify a `--ollama` flag to run the AI hedge fund using local LLMs.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama
```

You can optionally specify the start and end dates to make decisions over a specific time period.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
```

#### Run the Backtester
```bash
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
```

**Example Output:**
&lt;img width=&quot;941&quot; alt=&quot;Screenshot 2025-01-06 at 5 47 52 PM&quot; src=&quot;https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47&quot; /&gt;


Note: The `--ollama`, `--start-date`, and `--end-date` flags work for the backtester, as well!

### ğŸ–¥ï¸ Web Application

The new way to run the AI Hedge Fund is through our web application that provides a user-friendly interface. This is recommended for users who prefer visual interfaces over command line tools.

Please see detailed instructions on how to install and run the web application [here](https://github.com/virattt/ai-hedge-fund/tree/main/app).

&lt;img width=&quot;1721&quot; alt=&quot;Screenshot 2025-06-28 at 6 41 03â€¯PM&quot; src=&quot;https://github.com/user-attachments/assets/b95ab696-c9f4-416c-9ad1-51feb1f5374b&quot; /&gt;


## How to Contribute

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

**Important**: Please keep your pull requests small and focused.  This will make it easier to review and merge.

## Feature Requests

If you have a feature request, please open an [issue](https://github.com/virattt/ai-hedge-fund/issues) and make sure it is tagged with `enhancement`.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[spipm/Depixelization_poc]]></title>
            <link>https://github.com/spipm/Depixelization_poc</link>
            <guid>https://github.com/spipm/Depixelization_poc</guid>
            <pubDate>Mon, 15 Dec 2025 00:04:54 GMT</pubDate>
            <description><![CDATA[Depix is a PoC for a technique to recover plaintext from pixelized screenshots.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/spipm/Depixelization_poc">spipm/Depixelization_poc</a></h1>
            <p>Depix is a PoC for a technique to recover plaintext from pixelized screenshots.</p>
            <p>Language: Python</p>
            <p>Stars: 3,676</p>
            <p>Forks: 277</p>
            <p>Stars today: 129 stars today</p>
            <h2>README</h2><pre># Depix

Depix is a PoC for a technique to recover plaintext from pixelized screenshots.

This implementation works on pixelized images that were created with a linear box filter.
In [this article](https://www.spipm.nl/2030.html) I cover background information on pixelization and similar research.

## Example

![image](docs/img/Recovering_prototype_latest.png)

## Updates

* 24 dec &#039;24: Made repo private, changed the name and made it public again. It just had a ridiculous amount of stars because of the media hype, which didn&#039;t feel right. I made this as a quick PoC for a company back in the day, because someone pixelated part of a password for an account with Domain Admin rights. The hype got running by the catchy image and eventually this repo had 26152 stars. If I ever get this much stars again, I want it to be for a project that I&#039;m that hyped about as well.
![image](images/stars.png)
* 27 nov &#039;23: Refactored and removed all this pip stuff. I like scripts I can just run. If a package can&#039;t be found, just install it. Also added `tool_show_boxes.py` to show how bad the box detector is (you have to really cut out the pixels exactly). Made a TODO to create a version that just cuts out boxes of static size.

## Installation

* Install the dependencies
* Run Depix:

```sh
python3 depix.py \
    -p /path/to/your/input/image.png \
    -s images/searchimages/debruinseq_notepad_Windows10_closeAndSpaced.png \
    -o /path/to/your/output.png
```

## Example usage

* Depixelize example image created with Notepad and pixelized with Greenshot. Greenshot averages by averaging the gamma-encoded 0-255 values, which is Depix&#039;s default mode.

```sh
python3 depix.py \
    -p images/testimages/testimage3_pixels.png \
    -s images/searchimages/debruinseq_notepad_Windows10_closeAndSpaced.png
```

Result: ![image](docs/img/example_output_multiword.png)

* Depixelize example image created with Sublime and pixelized with Gimp, where averaging is done in linear sRGB. The backgroundcolor option filters out the background color of the editor.

```sh
python3 depix.py \
    -p images/testimages/sublime_screenshot_pixels_gimp.png \
    -s images/searchimages/debruin_sublime_Linux_small.png \
    --backgroundcolor 40,41,35 \
    --averagetype linear
```

Result: ![image](docs/img/output_depixelizedExample_linear.png)

* (Optional) You can view if the box detector thingie finds your pixels with `tool_show_boxes.py`. Consider a smaller batch of pixels if this looks all mangled. Example of good looking boxes:

```sh
python3 tool_show_boxes.py \ 
    -p images/testimages/testimage3_pixels.png \
    -s images/searchimages/debruinseq_notepad_Windows10_closeAndSpaced.png
```

* (Optional) You can create pixelized image by using `tool_gen_pixelated.py`.

```sh
python3 tool_gen_pixelated.py -i /path/to/image.png -o pixed_output.png
```

* For a detailed explanation, please try to run `$ python3 depix.py -h` and `tool_gen_pixelated.py`.

## About

### Making a Search Image

* Cut out the pixelated blocks from the screenshot as a single rectangle.
* Paste a [De Bruijn sequence](https://en.wikipedia.org/wiki/De_Bruijn_sequence) with expected characters in an editor with the same font settings as your input image (Same text size, similar font, same colors).
* Make a screenshot of the sequence.
* Move that screenshot into a folder like `images/searchimages/`.
* Run Depix with the `-s` flag set to the location of this screenshot.

### Making a Pixelized Image

* Cut out the pixelized blocks exactly. See the `testimages` for examples.
* It tries to detect blocks but it doesn&#039;t do an amazing job. Play with the `tool_show_boxes.py` script and different cutouts if your blocks aren&#039;t properly detected.

### Algorithm

The algorithm uses the fact that the linear box filter processes every block separately. For every block it pixelizes all blocks in the search image to check for direct matches.

For some pixelized images Depix manages to find single-match results. It assumes these are correct. The matches of surrounding multi-match blocks are then compared to be geometrically at the same distance as in the pixelized image. Matches are also treated as correct. This process is repeated a couple of times.

After correct blocks have no more geometrical matches, it will output all correct blocks directly. For multi-match blocks, it outputs the average of all matches.

### Known limitations

* The algorithm matches by integer block-boundaries. As a result, it has the underlying assumption that for all characters rendered (both in the de Brujin sequence and the pixelated image), the text positioning is done at pixel level. However, some modern text rasterizers position text [at sub-pixel accuracies](http://agg.sourceforge.net/antigrain.com/research/font_rasterization/).
* You need to know the font specifications and in some cases the screen settings with which the screenshot was taken. However, if there is enough plaintext in the original image you might be able to use the original as a search image.
* This approach doesn&#039;t work if additional image compression is performed, because it messes up the colors of a block.

### Future development

* Implement more filter functions

Create more averaging filters that work like some popular editors do.

* Create a new tool that utilizes HMMs

Still, anyone who is passionate about this type of depixelization is encouraged to implement their own HMM-based version and share it.

### Other sources and tools

After creating this program, someone pointed me to a [research document](https://www.researchgate.net/publication/305423573_On_the_Ineffectiveness_of_Mosaicing_and_Blurring_as_Tools_for_Document_Redaction) from 2016 where a group of researchers managed to create a similar tool. Their tool has better precision and works across many different fonts. While their original source code is not public, an open-source implementation exists at [DepixHMM](https://github.com/JonasSchatz/DepixHMM).

Edit 16 Feb &#039;22: [Dan Petro](https://bishopfox.com/authors/dan-petro) created the tool UnRedacter ([write-up](https://bishopfox.com/blog/unredacter-tool-never-pixelation), [source](https://github.com/BishopFox/unredacter)) to crack a [challenge](https://labs.jumpsec.com/can-depix-deobfuscate-your-data/) that was created as a response to Depix!

Edit 16 Apr &#039;25: Jeff Geerling created a [challenge](https://www.jeffgeerling.com/blog/2025/its-easier-ever-de-censor-videos) for depixelating pixelated folder content in a moving image. Three people were able to do it. [Here](https://github.com/KoKuToru/de-pixelate_gaV-O6NPWrI) is a repo from KoKuToru showing how to do this with TensorFlow! Amazing!
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[zhinianboke/xianyu-auto-reply]]></title>
            <link>https://github.com/zhinianboke/xianyu-auto-reply</link>
            <guid>https://github.com/zhinianboke/xianyu-auto-reply</guid>
            <pubDate>Mon, 15 Dec 2025 00:04:53 GMT</pubDate>
            <description><![CDATA[é—²é±¼è‡ªåŠ¨å›å¤ç®¡ç†ç³»ç»Ÿæ˜¯ä¸€ä¸ªåŸºäº Python + FastAPI å¼€å‘çš„è‡ªåŠ¨åŒ–å®¢æœç³»ç»Ÿï¼Œä¸“ä¸ºé—²é±¼å¹³å°è®¾è®¡ã€‚ç³»ç»Ÿé€šè¿‡ WebSocket è¿æ¥é—²é±¼æœåŠ¡å™¨ï¼Œå®æ—¶æ¥æ”¶å’Œå¤„ç†æ¶ˆæ¯ï¼Œæä¾›æ™ºèƒ½åŒ–çš„è‡ªåŠ¨å›å¤æœåŠ¡ã€‚]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/zhinianboke/xianyu-auto-reply">zhinianboke/xianyu-auto-reply</a></h1>
            <p>é—²é±¼è‡ªåŠ¨å›å¤ç®¡ç†ç³»ç»Ÿæ˜¯ä¸€ä¸ªåŸºäº Python + FastAPI å¼€å‘çš„è‡ªåŠ¨åŒ–å®¢æœç³»ç»Ÿï¼Œä¸“ä¸ºé—²é±¼å¹³å°è®¾è®¡ã€‚ç³»ç»Ÿé€šè¿‡ WebSocket è¿æ¥é—²é±¼æœåŠ¡å™¨ï¼Œå®æ—¶æ¥æ”¶å’Œå¤„ç†æ¶ˆæ¯ï¼Œæä¾›æ™ºèƒ½åŒ–çš„è‡ªåŠ¨å›å¤æœåŠ¡ã€‚</p>
            <p>Language: Python</p>
            <p>Stars: 3,383</p>
            <p>Forks: 916</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre># ğŸŸ é—²é±¼è‡ªåŠ¨å›å¤ç³»ç»Ÿ

[![GitHub](https://img.shields.io/badge/GitHub-zhinianboke%2Fxianyu--auto--reply-blue?logo=github)](https://github.com/zhinianboke/xianyu-auto-reply)
[![Docker](https://img.shields.io/badge/Docker-ä¸€é”®éƒ¨ç½²-blue?logo=docker)](https://github.com/zhinianboke/xianyu-auto-reply#-å¿«é€Ÿå¼€å§‹)
[![Python](https://img.shields.io/badge/Python-3.11+-green?logo=python)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-ä»…ä¾›å­¦ä¹ -red.svg)](#ï¸-ç‰ˆæƒå£°æ˜ä¸ä½¿ç”¨æ¡æ¬¾)

## æœ€æ–°ä»£ç è·å–åœ°å€ï¼ˆå°½é‡è½¬å­˜ï¼‰

æˆ‘ç”¨å¤¸å…‹ç½‘ç›˜åˆ†äº«äº†ã€Œè‡ªåŠ¨å‘è´§ã€ï¼Œç‚¹å‡»é“¾æ¥å³å¯ä¿å­˜ã€‚æ‰“å¼€ã€Œå¤¸å…‹APPã€ï¼Œæ— éœ€ä¸‹è½½åœ¨çº¿æ’­æ”¾è§†é¢‘ï¼Œç•…äº«åŸç”»5å€é€Ÿï¼Œæ”¯æŒç”µè§†æŠ•å±ã€‚
é“¾æ¥ï¼šhttps://pan.quark.cn/s/447e909f4107

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

ä¸€ä¸ªåŠŸèƒ½å®Œæ•´çš„é—²é±¼è‡ªåŠ¨å›å¤å’Œç®¡ç†ç³»ç»Ÿï¼Œé‡‡ç”¨ç°ä»£åŒ–çš„æŠ€æœ¯æ¶æ„ï¼Œæ”¯æŒå¤šç”¨æˆ·ã€å¤šè´¦å·ç®¡ç†ï¼Œå…·å¤‡æ™ºèƒ½å›å¤ã€è‡ªåŠ¨å‘è´§ã€è‡ªåŠ¨ç¡®è®¤å‘è´§ã€å•†å“ç®¡ç†ç­‰ä¼ä¸šçº§åŠŸèƒ½ã€‚ç³»ç»ŸåŸºäºPythonå¼‚æ­¥ç¼–ç¨‹ï¼Œä½¿ç”¨FastAPIæä¾›RESTful APIï¼ŒSQLiteæ•°æ®åº“å­˜å‚¨ï¼Œæ”¯æŒDockerä¸€é”®éƒ¨ç½²ã€‚

&gt; **âš ï¸ é‡è¦æç¤ºï¼šæœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ ç ”ç©¶ä½¿ç”¨ï¼Œä¸¥ç¦å•†ä¸šç”¨é€”ï¼ä½¿ç”¨å‰è¯·ä»”ç»†é˜…è¯»[ç‰ˆæƒå£°æ˜](#ï¸-ç‰ˆæƒå£°æ˜ä¸ä½¿ç”¨æ¡æ¬¾)ã€‚**

## ğŸ—ï¸ æŠ€æœ¯æ¶æ„

### æ ¸å¿ƒæŠ€æœ¯æ ˆ
- **åç«¯æ¡†æ¶**: FastAPI + Python 3.11+ å¼‚æ­¥ç¼–ç¨‹
- **æ•°æ®åº“**: SQLite 3 + å¤šç”¨æˆ·æ•°æ®éš”ç¦» + è‡ªåŠ¨è¿ç§»
- **å‰ç«¯**: Bootstrap 5 + Vanilla JavaScript + å“åº”å¼è®¾è®¡
- **é€šä¿¡åè®®**: WebSocket + RESTful API + å®æ—¶é€šä¿¡
- **éƒ¨ç½²æ–¹å¼**: Docker + Docker Compose + ä¸€é”®éƒ¨ç½²
- **æ—¥å¿—ç³»ç»Ÿ**: Loguru + æ–‡ä»¶è½®è½¬ + å®æ—¶æ”¶é›†
- **å®‰å…¨è®¤è¯**: JWT + å›¾å½¢éªŒè¯ç  + é‚®ç®±éªŒè¯ + æƒé™æ§åˆ¶

### ç³»ç»Ÿæ¶æ„ç‰¹ç‚¹
- **å¾®æœåŠ¡è®¾è®¡**: æ¨¡å—åŒ–æ¶æ„ï¼Œæ˜“äºç»´æŠ¤å’Œæ‰©å±•
- **å¼‚æ­¥å¤„ç†**: åŸºäºasyncioçš„é«˜æ€§èƒ½å¼‚æ­¥å¤„ç†
- **å¤šç”¨æˆ·éš”ç¦»**: å®Œå…¨çš„æ•°æ®éš”ç¦»å’Œæƒé™æ§åˆ¶
- **å®¹å™¨åŒ–éƒ¨ç½²**: Dockerå®¹å™¨åŒ–ï¼Œæ”¯æŒä¸€é”®éƒ¨ç½²
- **å®æ—¶ç›‘æ§**: WebSocketå®æ—¶é€šä¿¡å’ŒçŠ¶æ€ç›‘æ§
- **è‡ªåŠ¨åŒ–è¿ç»´**: è‡ªåŠ¨é‡è¿ã€å¼‚å¸¸æ¢å¤ã€æ—¥å¿—è½®è½¬

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### ğŸ” å¤šç”¨æˆ·ç³»ç»Ÿ
- **ç”¨æˆ·æ³¨å†Œç™»å½•** - æ”¯æŒé‚®ç®±éªŒè¯ç æ³¨å†Œï¼Œå›¾å½¢éªŒè¯ç ä¿æŠ¤
- **æ•°æ®å®Œå…¨éš”ç¦»** - æ¯ä¸ªç”¨æˆ·çš„æ•°æ®ç‹¬ç«‹å­˜å‚¨ï¼Œäº’ä¸å¹²æ‰°
- **æƒé™ç®¡ç†** - ä¸¥æ ¼çš„ç”¨æˆ·æƒé™æ§åˆ¶å’ŒJWTè®¤è¯
- **å®‰å…¨ä¿æŠ¤** - é˜²æš´åŠ›ç ´è§£ã€ä¼šè¯ç®¡ç†ã€å®‰å…¨æ—¥å¿—
- **æˆæƒæœŸé™ç®¡ç†** - æ ¸å¿ƒæ»‘å—éªŒè¯æ¨¡å—åŒ…å«æˆæƒæœŸé™éªŒè¯ï¼Œç¡®ä¿åˆè§„ä½¿ç”¨

### ğŸ“± å¤šè´¦å·ç®¡ç†
- **æ— é™è´¦å·æ”¯æŒ** - æ¯ä¸ªç”¨æˆ·å¯ç®¡ç†å¤šä¸ªé—²é±¼è´¦å·
- **ç‹¬ç«‹è¿è¡Œ** - æ¯ä¸ªè´¦å·ç‹¬ç«‹ç›‘æ§ï¼Œäº’ä¸å½±å“
- **å®æ—¶çŠ¶æ€** - è´¦å·è¿æ¥çŠ¶æ€å®æ—¶ç›‘æ§
- **æ‰¹é‡æ“ä½œ** - æ”¯æŒæ‰¹é‡å¯åŠ¨ã€åœæ­¢è´¦å·ä»»åŠ¡

### ğŸ¤– æ™ºèƒ½å›å¤ç³»ç»Ÿ
- **å…³é”®è¯åŒ¹é…** - æ”¯æŒç²¾ç¡®å…³é”®è¯åŒ¹é…å›å¤
- **æŒ‡å®šå•†å“å›å¤** - æ”¯æŒä¸ºç‰¹å®šå•†å“è®¾ç½®ä¸“é—¨çš„å›å¤å†…å®¹ï¼Œä¼˜å…ˆçº§æœ€é«˜
- **å•†å“ä¸“ç”¨å…³é”®è¯** - æ”¯æŒä¸ºç‰¹å®šå•†å“è®¾ç½®ä¸“ç”¨å…³é”®è¯å›å¤
- **é€šç”¨å…³é”®è¯** - æ”¯æŒå…¨å±€é€šç”¨å…³é”®è¯ï¼Œé€‚ç”¨äºæ‰€æœ‰å•†å“
- **æ‰¹é‡å¯¼å…¥å¯¼å‡º** - æ”¯æŒExcelæ ¼å¼çš„å…³é”®è¯æ‰¹é‡å¯¼å…¥å¯¼å‡º
- **AIæ™ºèƒ½å›å¤** - é›†æˆOpenAI APIï¼Œæ”¯æŒä¸Šä¸‹æ–‡ç†è§£
- **å˜é‡æ›¿æ¢** - å›å¤å†…å®¹æ”¯æŒåŠ¨æ€å˜é‡ï¼ˆç”¨æˆ·åã€å•†å“ä¿¡æ¯ã€å•†å“IDç­‰ï¼‰
- **ä¼˜å…ˆçº§ç­–ç•¥** - æŒ‡å®šå•†å“å›å¤ &gt; å•†å“ä¸“ç”¨å…³é”®è¯ &gt; é€šç”¨å…³é”®è¯ &gt; é»˜è®¤å›å¤ &gt; AIå›å¤

### ğŸšš è‡ªåŠ¨å‘è´§åŠŸèƒ½
- **æ™ºèƒ½åŒ¹é…** - åŸºäºå•†å“ä¿¡æ¯è‡ªåŠ¨åŒ¹é…å‘è´§è§„åˆ™
- **å¤šè§„æ ¼æ”¯æŒ** - æ”¯æŒåŒä¸€å•†å“çš„ä¸åŒè§„æ ¼è‡ªåŠ¨åŒ¹é…å¯¹åº”å¡åˆ¸
- **ç²¾ç¡®åŒ¹é…+å…œåº•æœºåˆ¶** - ä¼˜å…ˆç²¾ç¡®åŒ¹é…è§„æ ¼ï¼Œå¤±è´¥æ—¶è‡ªåŠ¨é™çº§åˆ°æ™®é€šå¡åˆ¸
- **å»¶æ—¶å‘è´§** - æ”¯æŒè®¾ç½®å‘è´§å»¶æ—¶æ—¶é—´ï¼ˆ0-3600ç§’ï¼‰
- **å¤šç§è§¦å‘** - æ”¯æŒä»˜æ¬¾æ¶ˆæ¯ã€å°åˆ€æ¶ˆæ¯ç­‰å¤šç§è§¦å‘æ¡ä»¶
- **é˜²é‡å¤å‘è´§** - æ™ºèƒ½é˜²é‡å¤æœºåˆ¶ï¼Œé¿å…é‡å¤å‘è´§
- **å¤šç§å‘è´§æ–¹å¼** - æ”¯æŒå›ºå®šæ–‡å­—ã€æ‰¹é‡æ•°æ®ã€APIè°ƒç”¨ã€å›¾ç‰‡å‘è´§ç­‰æ–¹å¼
- **å›¾ç‰‡å‘è´§** - æ”¯æŒä¸Šä¼ å›¾ç‰‡å¹¶è‡ªåŠ¨å‘é€ç»™ä¹°å®¶ï¼Œå›¾ç‰‡è‡ªåŠ¨ä¸Šä¼ åˆ°CDN
- **è‡ªåŠ¨ç¡®è®¤å‘è´§** - æ£€æµ‹åˆ°ä»˜æ¬¾åè‡ªåŠ¨è°ƒç”¨é—²é±¼APIç¡®è®¤å‘è´§ï¼Œæ”¯æŒé”æœºåˆ¶é˜²å¹¶å‘
- **é˜²é‡å¤ç¡®è®¤** - æ™ºèƒ½é˜²é‡å¤ç¡®è®¤æœºåˆ¶ï¼Œé¿å…é‡å¤APIè°ƒç”¨
- **è®¢å•è¯¦æƒ…ç¼“å­˜** - è®¢å•è¯¦æƒ…è·å–æ”¯æŒæ•°æ®åº“ç¼“å­˜ï¼Œå¤§å¹…æå‡æ€§èƒ½
- **å‘è´§ç»Ÿè®¡** - å®Œæ•´çš„å‘è´§è®°å½•å’Œç»Ÿè®¡åŠŸèƒ½

### ğŸ›ï¸ å•†å“ç®¡ç†
- **è‡ªåŠ¨æ”¶é›†** - æ¶ˆæ¯è§¦å‘æ—¶è‡ªåŠ¨æ”¶é›†å•†å“ä¿¡æ¯
- **APIè·å–** - é€šè¿‡é—²é±¼APIè·å–å®Œæ•´å•†å“è¯¦æƒ…
- **å¤šè§„æ ¼æ”¯æŒ** - æ”¯æŒå¤šè§„æ ¼å•†å“çš„è§„æ ¼ä¿¡æ¯ç®¡ç†
- **æ‰¹é‡ç®¡ç†** - æ”¯æŒæ‰¹é‡æŸ¥çœ‹ã€ç¼–è¾‘ã€åˆ‡æ¢å¤šè§„æ ¼çŠ¶æ€
- **æ™ºèƒ½å»é‡** - è‡ªåŠ¨å»é‡ï¼Œé¿å…é‡å¤å­˜å‚¨

### ğŸ” å•†å“æœç´¢åŠŸèƒ½
- **çœŸå®æ•°æ®è·å–** - åŸºäºPlaywrightæŠ€æœ¯è·å–çœŸå®é—²é±¼å•†å“æ•°æ®
- **æ™ºèƒ½æ’åº** - æŒ‰&quot;äººæƒ³è¦&quot;æ•°é‡è‡ªåŠ¨å€’åºæ’åˆ—
- **å¤šé¡µæœç´¢** - æ”¯æŒä¸€æ¬¡æ€§è·å–å¤šé¡µå•†å“æ•°æ®
- **å‰ç«¯åˆ†é¡µ** - çµæ´»çš„å‰ç«¯åˆ†é¡µæ˜¾ç¤º
- **å•†å“è¯¦æƒ…** - æ”¯æŒæŸ¥çœ‹å®Œæ•´å•†å“è¯¦æƒ…ä¿¡æ¯

### ğŸ“Š ç³»ç»Ÿç›‘æ§
- **å®æ—¶æ—¥å¿—** - å®Œæ•´çš„æ“ä½œæ—¥å¿—è®°å½•å’ŒæŸ¥çœ‹
- **æ€§èƒ½ç›‘æ§** - ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µç›‘æ§
- **å¥åº·æ£€æŸ¥** - æœåŠ¡çŠ¶æ€å¥åº·æ£€æŸ¥

### ğŸ“ æ•°æ®ç®¡ç†
- **Excelå¯¼å…¥å¯¼å‡º** - æ”¯æŒå…³é”®è¯æ•°æ®çš„Excelæ ¼å¼å¯¼å…¥å¯¼å‡º
- **æ¨¡æ¿ç”Ÿæˆ** - è‡ªåŠ¨ç”ŸæˆåŒ…å«ç¤ºä¾‹æ•°æ®çš„å¯¼å…¥æ¨¡æ¿
- **æ‰¹é‡æ“ä½œ** - æ”¯æŒæ‰¹é‡æ·»åŠ ã€æ›´æ–°å…³é”®è¯æ•°æ®
- **æ•°æ®éªŒè¯** - å¯¼å…¥æ—¶è‡ªåŠ¨éªŒè¯æ•°æ®æ ¼å¼å’Œé‡å¤æ€§
- **å¤šè§„æ ¼å¡åˆ¸ç®¡ç†** - æ”¯æŒåˆ›å»ºå’Œç®¡ç†å¤šè§„æ ¼å¡åˆ¸
- **å‘è´§è§„åˆ™ç®¡ç†** - æ”¯æŒå¤šè§„æ ¼å‘è´§è§„åˆ™çš„åˆ›å»ºå’Œç®¡ç†
- **æ•°æ®å¤‡ä»½** - è‡ªåŠ¨æ•°æ®å¤‡ä»½å’Œæ¢å¤
- **ä¸€é”®éƒ¨ç½²** - æä¾›é¢„æ„å»ºDockeré•œåƒï¼Œæ— éœ€ç¼–è¯‘å³å¯å¿«é€Ÿéƒ¨ç½²

## ğŸ“ é¡¹ç›®ç»“æ„

&lt;details&gt;
&lt;summary&gt;ç‚¹å‡»å±•å¼€æŸ¥çœ‹è¯¦ç»†é¡¹ç›®ç»“æ„&lt;/summary&gt;

```
xianyu-auto-reply/
â”œâ”€â”€ ğŸ“„ æ ¸å¿ƒæ–‡ä»¶
â”‚   â”œâ”€â”€ Start.py                    # é¡¹ç›®å¯åŠ¨å…¥å£ï¼Œåˆå§‹åŒ–æ‰€æœ‰æœåŠ¡
â”‚   â”œâ”€â”€ XianyuAutoAsync.py         # é—²é±¼WebSocketè¿æ¥å’Œæ¶ˆæ¯å¤„ç†æ ¸å¿ƒ
â”‚   â”œâ”€â”€ reply_server.py            # FastAPI WebæœåŠ¡å™¨å’Œå®Œæ•´APIæ¥å£
â”‚   â”œâ”€â”€ db_manager.py              # SQLiteæ•°æ®åº“ç®¡ç†ï¼Œæ”¯æŒå¤šç”¨æˆ·æ•°æ®éš”ç¦»
â”‚   â”œâ”€â”€ cookie_manager.py          # å¤šè´¦å·Cookieç®¡ç†å’Œä»»åŠ¡è°ƒåº¦
â”‚   â”œâ”€â”€ ai_reply_engine.py         # AIæ™ºèƒ½å›å¤å¼•æ“ï¼Œæ”¯æŒå¤šç§AIæ¨¡å‹
â”‚   â”œâ”€â”€ order_status_handler.py    # è®¢å•çŠ¶æ€å¤„ç†å’Œæ›´æ–°æ¨¡å—
â”‚   â”œâ”€â”€ file_log_collector.py      # å®æ—¶æ—¥å¿—æ”¶é›†å’Œç®¡ç†ç³»ç»Ÿ
â”‚   â”œâ”€â”€ config.py                  # å…¨å±€é…ç½®æ–‡ä»¶ç®¡ç†å™¨
â”‚   â”œâ”€â”€ usage_statistics.py        # ç”¨æˆ·ç»Ÿè®¡å’Œæ•°æ®åˆ†ææ¨¡å—
â”‚   â”œâ”€â”€ simple_stats_server.py     # ç®€å•ç»Ÿè®¡æœåŠ¡å™¨ï¼ˆå¯é€‰ï¼‰
â”‚   â”œâ”€â”€ secure_confirm_ultra.py    # è‡ªåŠ¨ç¡®è®¤å‘è´§æ¨¡å—ï¼ˆå¤šå±‚åŠ å¯†ä¿æŠ¤ï¼‰
â”‚   â”œâ”€â”€ secure_confirm_decrypted.py # è‡ªåŠ¨ç¡®è®¤å‘è´§æ¨¡å—ï¼ˆè§£å¯†ç‰ˆæœ¬ï¼‰
â”‚   â”œâ”€â”€ secure_freeshipping_ultra.py # è‡ªåŠ¨å…æ‹¼å‘è´§æ¨¡å—ï¼ˆå¤šå±‚åŠ å¯†ä¿æŠ¤ï¼‰
â”‚   â””â”€â”€ secure_freeshipping_decrypted.py # è‡ªåŠ¨å…æ‹¼å‘è´§æ¨¡å—ï¼ˆè§£å¯†ç‰ˆæœ¬ï¼‰
â”œâ”€â”€ ğŸ› ï¸ å·¥å…·æ¨¡å—
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ xianyu_utils.py        # é—²é±¼APIå·¥å…·å‡½æ•°ï¼ˆåŠ å¯†ã€ç­¾åã€è§£æï¼‰
â”‚       â”œâ”€â”€ message_utils.py       # æ¶ˆæ¯æ ¼å¼åŒ–å’Œå¤„ç†å·¥å…·
â”‚       â”œâ”€â”€ ws_utils.py            # WebSocketå®¢æˆ·ç«¯å°è£…
â”‚       â”œâ”€â”€ image_utils.py         # å›¾ç‰‡å¤„ç†å’Œç®¡ç†å·¥å…·
â”‚       â”œâ”€â”€ image_uploader.py      # å›¾ç‰‡ä¸Šä¼ åˆ°é—²é±¼CDN
â”‚       â”œâ”€â”€ image_utils.py         # å›¾ç‰‡å¤„ç†å·¥å…·ï¼ˆå‹ç¼©ã€æ ¼å¼è½¬æ¢ï¼‰
â”‚       â”œâ”€â”€ item_search.py         # å•†å“æœç´¢åŠŸèƒ½ï¼ˆåŸºäºPlaywrightï¼Œæ— å¤´æ¨¡å¼ï¼‰
â”‚       â”œâ”€â”€ order_detail_fetcher.py # è®¢å•è¯¦æƒ…è·å–å·¥å…·
â”‚       â””â”€â”€ qr_login.py            # äºŒç»´ç ç™»å½•åŠŸèƒ½
â”œâ”€â”€ ğŸŒ å‰ç«¯ç•Œé¢
â”‚   â””â”€â”€ static/
â”‚       â”œâ”€â”€ index.html             # ä¸»ç®¡ç†ç•Œé¢ï¼ˆé›†æˆæ‰€æœ‰åŠŸèƒ½æ¨¡å—ï¼‰
â”‚       â”œâ”€â”€ login.html             # ç”¨æˆ·ç™»å½•é¡µé¢
â”‚       â”œâ”€â”€ register.html          # ç”¨æˆ·æ³¨å†Œé¡µé¢ï¼ˆé‚®ç®±éªŒè¯ï¼‰
â”‚       â”œâ”€â”€ js/
â”‚       â”‚   â””â”€â”€ app.js             # ä¸»è¦JavaScripté€»è¾‘å’Œæ‰€æœ‰åŠŸèƒ½æ¨¡å—
â”‚       â”œâ”€â”€ css/
â”‚       â”‚   â”œâ”€â”€ variables.css      # CSSå˜é‡å®šä¹‰
â”‚       â”‚   â”œâ”€â”€ layout.css         # å¸ƒå±€æ ·å¼
â”‚       â”‚   â”œâ”€â”€ components.css     # ç»„ä»¶æ ·å¼
â”‚       â”‚   â”œâ”€â”€ accounts.css       # è´¦å·ç®¡ç†æ ·å¼
â”‚       â”‚   â”œâ”€â”€ keywords.css       # å…³é”®è¯ç®¡ç†æ ·å¼
â”‚       â”‚   â”œâ”€â”€ items.css          # å•†å“ç®¡ç†æ ·å¼
â”‚       â”‚   â”œâ”€â”€ logs.css           # æ—¥å¿—ç®¡ç†æ ·å¼
â”‚       â”‚   â”œâ”€â”€ notifications.css  # é€šçŸ¥æ ·å¼
â”‚       â”‚   â”œâ”€â”€ dashboard.css      # ä»ªè¡¨æ¿æ ·å¼
â”‚       â”‚   â”œâ”€â”€ admin.css          # ç®¡ç†å‘˜æ ·å¼
â”‚       â”‚   â””â”€â”€ app.css            # ä¸»åº”ç”¨æ ·å¼
â”‚       â”œâ”€â”€ lib/
â”‚       â”‚   â”œâ”€â”€ bootstrap/         # Bootstrapæ¡†æ¶
â”‚       â”‚   â””â”€â”€ bootstrap-icons/   # Bootstrapå›¾æ ‡
â”‚       â”œâ”€â”€ uploads/
â”‚       â”‚   â””â”€â”€ images/            # ä¸Šä¼ çš„å›¾ç‰‡æ–‡ä»¶
â”‚       â”œâ”€â”€ xianyu_js_version_2.js # é—²é±¼JavaScriptå·¥å…·åº“
â”‚       â”œâ”€â”€ wechat-group.png       # å¾®ä¿¡ç¾¤äºŒç»´ç 
â”‚       â””â”€â”€ qq-group.png           # QQç¾¤äºŒç»´ç 
â”œâ”€â”€ ğŸ³ Dockeréƒ¨ç½²
â”‚   â”œâ”€â”€ Dockerfile                 # Dockeré•œåƒæ„å»ºæ–‡ä»¶ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
â”‚   â”œâ”€â”€ Dockerfile-cn             # å›½å†…ä¼˜åŒ–ç‰ˆDockeré•œåƒæ„å»ºæ–‡ä»¶
â”‚   â”œâ”€â”€ docker-compose.yml        # Docker Composeä¸€é”®éƒ¨ç½²é…ç½®
â”‚   â”œâ”€â”€ docker-compose-cn.yml     # å›½å†…ä¼˜åŒ–ç‰ˆDocker Composeé…ç½®
â”‚   â”œâ”€â”€ docker-deploy.sh          # Dockeréƒ¨ç½²ç®¡ç†è„šæœ¬ï¼ˆLinux/macOSï¼‰
â”‚   â”œâ”€â”€ docker-deploy.bat         # Dockeréƒ¨ç½²ç®¡ç†è„šæœ¬ï¼ˆWindowsï¼‰
â”‚   â”œâ”€â”€ entrypoint.sh              # Dockerå®¹å™¨å¯åŠ¨è„šæœ¬
â”‚   â””â”€â”€ .dockerignore             # Dockeræ„å»ºå¿½ç•¥æ–‡ä»¶
â”œâ”€â”€ ğŸŒ Nginxé…ç½®
â”‚   â””â”€â”€ nginx/
â”‚       â”œâ”€â”€ nginx.conf            # Nginxåå‘ä»£ç†é…ç½®
â”‚       â””â”€â”€ ssl/                  # SSLè¯ä¹¦ç›®å½•
â”œâ”€â”€ ğŸ“‹ é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ global_config.yml         # å…¨å±€é…ç½®æ–‡ä»¶ï¼ˆWebSocketã€APIç­‰ï¼‰
â”‚   â”œâ”€â”€ requirements.txt          # Pythonä¾èµ–åŒ…åˆ—è¡¨ï¼ˆç²¾ç®€ç‰ˆï¼Œæ— å†…ç½®æ¨¡å—ï¼‰
â”‚   â”œâ”€â”€ .gitignore                # Gitå¿½ç•¥æ–‡ä»¶é…ç½®ï¼ˆå®Œæ•´ç‰ˆï¼‰
â”‚   â””â”€â”€ README.md                 # é¡¹ç›®è¯´æ˜æ–‡æ¡£ï¼ˆæœ¬æ–‡ä»¶ï¼‰
â””â”€â”€ ğŸ“Š æ•°æ®ç›®å½•ï¼ˆè¿è¡Œæ—¶åˆ›å»ºï¼‰
    â”œâ”€â”€ data/                     # æ•°æ®ç›®å½•ï¼ˆDockeræŒ‚è½½ï¼Œè‡ªåŠ¨åˆ›å»ºï¼‰
    â”‚   â”œâ”€â”€ xianyu_data.db        # SQLiteä¸»æ•°æ®åº“æ–‡ä»¶
    â”‚   â”œâ”€â”€ user_stats.db         # ç”¨æˆ·ç»Ÿè®¡æ•°æ®åº“
    â”‚   â””â”€â”€ xianyu_data_backup_*.db # æ•°æ®åº“å¤‡ä»½æ–‡ä»¶
    â”œâ”€â”€ logs/                     # æŒ‰æ—¥æœŸåˆ†å‰²çš„æ—¥å¿—æ–‡ä»¶
    â””â”€â”€ backups/                  # å…¶ä»–å¤‡ä»½æ–‡ä»¶
```

&lt;/details&gt;

## ğŸ†• æœ€æ–°æ›´æ–°

### 2025å¹´1æœˆæ›´æ–°

**ğŸ”¥ æ€§èƒ½ä¸å®‰å…¨å¢å¼º**
- âœ… æ–°å¢ Nuitka äºŒè¿›åˆ¶ç¼–è¯‘æ”¯æŒï¼Œæ ¸å¿ƒæ¨¡å—å¯ç¼–è¯‘ä¸º .pyd/.so æå‡æ€§èƒ½å’Œå®‰å…¨æ€§
- âœ… æ»‘å—éªŒè¯æ¨¡å—å¢åŠ æˆæƒæœŸé™éªŒè¯æœºåˆ¶ï¼Œç¡®ä¿åˆè§„ä½¿ç”¨
- âœ… Docker æ„å»ºä¼˜åŒ–ï¼Œè‡ªåŠ¨ç¼–è¯‘äºŒè¿›åˆ¶æ¨¡å—ï¼Œæå‡å®¹å™¨å¯åŠ¨æ•ˆç‡
- âœ… å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶ï¼Œæå‡ç³»ç»Ÿç¨³å®šæ€§
- âœ… ä¿®å¤æ»‘å—éªŒè¯æ¨¡å—å†…å­˜æ³„æ¼é—®é¢˜ï¼Œæµè§ˆå™¨èµ„æºæ­£ç¡®é‡Šæ”¾

**ğŸ“¦ æ•°æ®ç®¡ç†ä¼˜åŒ–**
- âœ… æ•°æ®åº“æ–‡ä»¶ç»Ÿä¸€è¿ç§»åˆ° `data/` ç›®å½•ï¼Œæ›´å¥½çš„ç»„ç»‡å’Œç®¡ç†
- âœ… å¯åŠ¨æ—¶è‡ªåŠ¨æ£€æµ‹å¹¶è¿ç§»æ—§æ•°æ®åº“æ–‡ä»¶ï¼Œæ— éœ€æ‰‹åŠ¨æ“ä½œ
- âœ… å¤‡ä»½æ–‡ä»¶è‡ªåŠ¨æ•´ç†åˆ°æ•°æ®ç›®å½•ï¼Œä¾¿äºé›†ä¸­ç®¡ç†
- âœ… DockeræŒ‚è½½æ›´ç®€æ´ï¼Œä¸€ä¸ªdataç›®å½•åŒ…å«æ‰€æœ‰æ•°æ®

**ğŸ› ï¸ é…ç½®æ–‡ä»¶ä¼˜åŒ–**
- âœ… å®Œå–„ `.gitignore`ï¼Œæ–°å¢ç¼–è¯‘äº§ç‰©ã€æµè§ˆå™¨ç¼“å­˜ç­‰è§„åˆ™
- âœ… å®Œå–„ `.dockerignore`ï¼Œä¼˜åŒ–Dockeræ„å»ºé€Ÿåº¦å’Œé•œåƒä½“ç§¯
- âœ… å¢å¼º `entrypoint.sh`ï¼Œæ·»åŠ ç¯å¢ƒéªŒè¯å’Œè¯¦ç»†å¯åŠ¨æ—¥å¿—
- âœ… æ¸…ç†æµ‹è¯•æ–‡ä»¶å’Œä¸´æ—¶æ–‡ä»¶ï¼Œä¿æŒä»£ç åº“æ•´æ´

**ğŸ“¦ ä¾èµ–ç®¡ç†**
- âœ… `requirements.txt` ä¼˜åŒ–ï¼Œç§»é™¤Pythonå†…ç½®æ¨¡å—ï¼ŒæŒ‰åŠŸèƒ½åˆ†ç±»
- âœ… æ·»åŠ  Nuitka ç¼–è¯‘å·¥å…·é“¾ï¼ˆå¯é€‰ï¼‰
- âœ… è¯¦ç»†çš„ä¾èµ–è¯´æ˜å’Œå®‰è£…æŒ‡å—

**ğŸ› Bugä¿®å¤**
- âœ… ä¿®å¤æµè§ˆå™¨èµ„æºæ³„æ¼é—®é¢˜ï¼ŒDockerå®¹å™¨RAMä½¿ç”¨ç¨³å®š
- âœ… ä¼˜åŒ–å†å²è®°å½•å­˜å‚¨ï¼Œå‡å°‘90%ç£ç›˜å’Œå†…å­˜å ç”¨
- âœ… æ·»åŠ ææ„å‡½æ•°ç¡®ä¿èµ„æºé‡Šæ”¾

**ğŸ—ï¸ å¤šæ¶æ„æ”¯æŒ**
- âœ… Dockeré•œåƒæ”¯æŒAMD64å’ŒARM64åŒæ¶æ„
- âœ… GitHub Actionsè‡ªåŠ¨æ„å»ºå¹¶æ¨é€åˆ°åŒé•œåƒä»“åº“
- âœ… æ”¯æŒOracle Cloudã€AWS Gravitonç­‰ARMæœåŠ¡å™¨
- âœ… Dockerè‡ªåŠ¨é€‰æ‹©åŒ¹é…çš„æ¶æ„ï¼Œæ— éœ€æ‰‹åŠ¨æŒ‡å®š
- âœ… å›½å†…å¤–åŒé•œåƒæºï¼Œç¡®ä¿ä¸‹è½½é€Ÿåº¦

## ğŸš€ äº‘æœåŠ¡å™¨æ¨è

### ã€åˆ’ç®—äº‘ã€‘å›½å†…å¤–äº‘æœåŠ¡å™¨ã€å…¨çƒCDNã€æŒ‚æœºå®  www.hsykj.com


## ğŸš€ å¿«é€Ÿå¼€å§‹

**âš¡ æœ€å¿«éƒ¨ç½²æ–¹å¼ï¼ˆæ¨èï¼‰**ï¼šä½¿ç”¨é¢„æ„å»ºé•œåƒï¼Œæ— éœ€ä¸‹è½½æºç ï¼Œä¸€æ¡å‘½ä»¤å³å¯å¯åŠ¨ï¼

### æ–¹å¼ä¸€ï¼šDocker ä¸€é”®éƒ¨ç½²ï¼ˆæœ€ç®€å•ï¼‰â­

**å›½å†…ç”¨æˆ·ï¼ˆé˜¿é‡Œäº‘é•œåƒï¼Œæ¨èï¼‰**ï¼š
```bash
# 1. åˆ›å»ºæ•°æ®ç›®å½•
mkdir -p xianyu-auto-reply

# 2. ä¸€é”®å¯åŠ¨å®¹å™¨ï¼ˆæ”¯æŒAMD64/ARM64ï¼Œè‡ªåŠ¨é€‰æ‹©æ¶æ„ï¼‰
docker run -d \
  -p 8080:8080 \
  --restart always \
  -v $PWD/xianyu-auto-reply/:/app/data/ \
  --name xianyu-auto-reply \
  registry.cn-shanghai.aliyuncs.com/zhinian-software/xianyu-auto-reply:latest

# 3. è®¿é—®ç³»ç»Ÿ
# http://localhost:8080
```

**å›½é™…ç”¨æˆ·ï¼ˆDocker Hubé•œåƒï¼‰**ï¼š
```bash
# ä½¿ç”¨Docker Hubå›½é™…é•œåƒ
docker run -d \
  -p 8080:8080 \
  --restart always \
  -v $PWD/xianyu-auto-reply/:/app/data/ \
  --name xianyu-auto-reply \
  zhinianblog/xianyu-auto-reply:latest
```

**Windowsç”¨æˆ·**ï¼š
```powershell
# åˆ›å»ºæ•°æ®ç›®å½•
mkdir xianyu-auto-reply

# å›½å†…ç”¨æˆ·ï¼ˆé˜¿é‡Œäº‘ï¼‰
docker run -d -p 8080:8080 --restart always -v %cd%/xianyu-auto-reply/:/app/data/ --name xianyu-auto-reply registry.cn-shanghai.aliyuncs.com/zhinian-software/xianyu-auto-reply:latest

# å›½é™…ç”¨æˆ·ï¼ˆDocker Hubï¼‰
docker run -d -p 8080:8080 --restart always -v %cd%/xianyu-auto-reply/:/app/data/ --name xianyu-auto-reply zhinianblog/xianyu-auto-reply:latest
```

**ARM64æœåŠ¡å™¨** (Oracle Cloud, AWS Gravitonç­‰)ï¼š
```bash
# Dockerä¼šè‡ªåŠ¨é€‰æ‹©ARM64é•œåƒï¼Œæ— éœ€ç‰¹æ®Šé…ç½®
docker run -d \
  -p 8080:8080 \
  --restart always \
  -v $PWD/xianyu-auto-reply/:/app/data/ \
  --name xianyu-auto-reply \
  registry.cn-shanghai.aliyuncs.com/zhinian-software/xianyu-auto-reply:latest
```

### æ–¹å¼äºŒï¼šä»æºç æ„å»ºéƒ¨ç½²

#### ğŸŒ å›½é™…ç‰ˆï¼ˆæ¨èæµ·å¤–ç”¨æˆ·ï¼‰
```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/zhinianboke/xianyu-auto-reply.git
cd xianyu-auto-reply

# 2. ä½¿ç”¨å®Œæ•´ç‰ˆé…ç½®ï¼ˆåŒ…å«Redisç¼“å­˜ç­‰å¢å¼ºåŠŸèƒ½ï¼‰
docker-compose up -d --build

# 3. è®¿é—®ç³»ç»Ÿ
# http://localhost:8080
```

#### ğŸ‡¨ğŸ‡³ ä¸­å›½ç‰ˆï¼ˆæ¨èå›½å†…ç”¨æˆ·ï¼‰
```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/zhinianboke/xianyu-auto-reply.git
cd xianyu-auto-reply

# 2. ä½¿ç”¨ä¸­å›½é•œåƒæºé…ç½®ï¼ˆä¸‹è½½é€Ÿåº¦æ›´å¿«ï¼‰
docker-compose -f docker-compose-cn.yml up -d --build

# 3. è®¿é—®ç³»ç»Ÿ
# http://localhost:8080
```

**Windowsç”¨æˆ·**ï¼š
```cmd
# å›½é™…ç‰ˆ
docker-compose up -d --build

# ä¸­å›½ç‰ˆï¼ˆæ¨èï¼‰
docker-compose -f docker-compose-cn.yml up -d --build
```

### æ–¹å¼ä¸‰ï¼šæœ¬åœ°å¼€å‘éƒ¨ç½²

```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/zhinianboke/xianyu-auto-reply.git
cd xianyu-auto-reply

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
python -m venv venv
source venv/bin/activate  # Linux/macOS
# æˆ– venv\Scripts\activate  # Windows

# 3. å®‰è£…Pythonä¾èµ–
pip install --upgrade pip
pip install -r requirements.txt

# 4. å®‰è£…Playwrightæµè§ˆå™¨
playwright install chromium
playwright install-deps chromium  # Linuxéœ€è¦

# 5. å¯åŠ¨ç³»ç»Ÿ
python Start.py

# 6. è®¿é—®ç³»ç»Ÿ
# http://localhost:8080
```

### ğŸ“‹ ç¯å¢ƒè¦æ±‚

- **Python**: 3.11+
- **Node.js**: 16+ (ç”¨äºJavaScriptæ‰§è¡Œ)
- **ç³»ç»Ÿ**: Windows/Linux/macOS
- **æ¶æ„**: x86_64 (amd64) / ARM64 (aarch64)
- **å†…å­˜**: å»ºè®®2GB+
- **å­˜å‚¨**: å»ºè®®10GB+
- **Docker**: 20.10+ (Dockeréƒ¨ç½²)
- **Docker Compose**: 2.0+ (Dockeréƒ¨ç½²)

### ğŸ–¥ï¸ å¤šæ¶æ„æ”¯æŒ

**æ”¯æŒçš„æ¶æ„**:
- âœ… **linux/amd64** - Intel/AMDå¤„ç†å™¨ï¼ˆä¼ ç»ŸæœåŠ¡å™¨ã€PCã€è™šæ‹Ÿæœºï¼‰
- âœ… **linux/arm64** - ARM64å¤„ç†å™¨ï¼ˆARMæœåŠ¡å™¨ã€æ ‘è“æ´¾4+ã€Apple Mç³»åˆ—ï¼‰

**é•œåƒä»“åº“**:
- ğŸ‡¨ğŸ‡³ **é˜¿é‡Œäº‘**: `registry.cn-shanghai.aliyuncs.com/zhinian-software/xianyu-auto-reply:latest`
- ğŸŒ **Docker Hub**: `zhinianblog/xianyu-auto-reply:latest`

**è‡ªåŠ¨æ„å»º**: GitHub Actionsè‡ªåŠ¨æ„å»ºå¹¶æ¨é€å¤šæ¶æ„é•œåƒåˆ°ä¸¤ä¸ªé•œåƒä»“åº“ï¼ŒDockerä¼šè‡ªåŠ¨é€‰æ‹©åŒ¹é…çš„æ¶æ„

**é€‚ç”¨çš„ARMäº‘æœåŠ¡å™¨**:
- Oracle Cloud - Ampere A1 (æ°¸ä¹…å…è´¹4æ ¸24GB)
- AWS - Graviton2/3å®ä¾‹
- é˜¿é‡Œäº‘ - å€šå¤©710å®ä¾‹
- è…¾è®¯äº‘ - æ˜Ÿæ˜Ÿæµ·ARMå®ä¾‹
- åä¸ºäº‘ - é²²é¹ARMå®ä¾‹

### âš™ï¸ ç¯å¢ƒå˜é‡é…ç½®ï¼ˆå¯é€‰ï¼‰

ç³»ç»Ÿæ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡è¿›è¡Œé…ç½®ï¼Œä¸»è¦é…ç½®é¡¹åŒ…æ‹¬ï¼š

```bash
# åŸºç¡€é…ç½®
WEB_PORT=8080                          # WebæœåŠ¡ç«¯å£
API_HOST=0.0.0.0                       # APIæœåŠ¡ä¸»æœº
TZ=Asia/Shanghai                       # æ—¶åŒºè®¾ç½®

# æ•°æ®åº“é…ç½®
DB_PATH=data/xianyu_data.db            # æ•°æ®åº“æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤åœ¨dataç›®å½•ï¼‰

# ç®¡ç†å‘˜é…ç½®
ADMIN_USERNAME=admin                   # ç®¡ç†å‘˜ç”¨æˆ·å
ADMIN_PASSWORD=admin123                # ç®¡ç†å‘˜å¯†ç ï¼ˆè¯·ä¿®æ”¹ï¼‰
JWT_SECRET_KEY=your-secret-key         # JWTå¯†é’¥ï¼ˆè¯·ä¿®æ”¹ï¼‰

# åŠŸèƒ½å¼€å…³
AUTO_REPLY_ENABLED=true                # å¯ç”¨è‡ªåŠ¨å›å¤
AUTO_DELIVERY_ENABLED=true             # å¯ç”¨è‡ªåŠ¨å‘è´§
AI_REPLY_ENABLED=false                 # å¯ç”¨AIå›å¤

# æ—¥å¿—é…ç½®
LOG_LEVEL=INFO                         # æ—¥å¿—çº§åˆ«
SQL_LOG_ENABLED=true                   # SQLæ—¥å¿—

# èµ„æºé™åˆ¶
MEMORY_LIMIT=2048                      # å†…å­˜é™åˆ¶(MB)
CPU_LIMIT=2.0                          # CPUé™åˆ¶(æ ¸å¿ƒæ•°)

# æ›´å¤šé…ç½®è¯·å‚è€ƒ docker-compose.yml æ–‡ä»¶
```

&gt; ğŸ’¡ **æç¤º**ï¼šæ‰€æœ‰é…ç½®é¡¹éƒ½æœ‰é»˜è®¤å€¼ï¼Œå¯æ ¹æ®éœ€è¦é€‰æ‹©æ€§é…ç½®



### ğŸŒ è®¿é—®ç³»ç»Ÿ

éƒ¨ç½²å®Œæˆåï¼Œæ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è®¿é—®ç³»ç»Ÿï¼š

- **Webç®¡ç†ç•Œé¢**ï¼šhttp://localhost:8080
- **é»˜è®¤ç®¡ç†å‘˜è´¦å·**ï¼š
  - ç”¨æˆ·åï¼š`admin`
  - å¯†ç ï¼š`admin123`
- **APIæ–‡æ¡£**ï¼šhttp://localhost:8080/docs
- **å¥åº·æ£€æŸ¥**ï¼šhttp://localhost:8080/health

&gt; âš ï¸ **å®‰å…¨æç¤º**ï¼šé¦–æ¬¡ç™»å½•åè¯·ç«‹å³ä¿®æ”¹é»˜è®¤å¯†ç ï¼


## ğŸ“‹ ç³»ç»Ÿä½¿ç”¨

### 1. ç”¨æˆ·æ³¨å†Œ
- è®¿é—® `http://localhost:8080/register.html`
- å¡«å†™ç”¨æˆ·ä¿¡æ¯ï¼Œå®Œæˆé‚®ç®±éªŒè¯
- è¾“å…¥å›¾å½¢éªŒè¯ç å®Œæˆæ³¨å†Œ

### 2. æ·»åŠ é—²é±¼è´¦å·
- ç™»å½•ç³»ç»Ÿåè¿›å…¥ä¸»ç•Œé¢
- ç‚¹å‡»&quot;æ·»åŠ æ–°è´¦å·&quot;
- è¾“å…¥è´¦å·IDå’Œå®Œæ•´çš„Cookieå€¼
- ç³»ç»Ÿè‡ªåŠ¨å¯åŠ¨è´¦å·ç›‘æ§ä»»åŠ¡

### 3. é…ç½®è‡ªåŠ¨å›å¤
- **å…³é”®è¯å›å¤**ï¼šè®¾ç½®å…³é”®è¯å’Œå¯¹åº”å›å¤å†…å®¹
- **AIå›å¤**ï¼šé…ç½®OpenAI APIå¯†é’¥å¯ç”¨æ™ºèƒ½å›å¤
- **é»˜è®¤å›å¤**ï¼šè®¾ç½®æœªåŒ¹é…æ—¶çš„é»˜è®¤å›å¤

### 4. è®¾ç½®è‡ªåŠ¨å‘è´§
- æ·»åŠ å‘è´§è§„åˆ™ï¼Œè®¾ç½®å•†å“å…³é”®è¯å’Œå‘è´§å†…å®¹
- æ”¯æŒæ–‡æœ¬å†…å®¹å’Œå¡å¯†æ–‡ä»¶ä¸¤ç§å‘è´§æ–¹å¼
- ç³»ç»Ÿæ£€æµ‹åˆ°ä»˜æ¬¾æ¶ˆæ¯æ—¶è‡ªåŠ¨ç¡®è®¤å‘è´§å¹¶è‡ªåŠ¨å‘è´§

### 5. ä½¿ç”¨å•†å“æœç´¢åŠŸèƒ½
- è®¿é—®å•†å“æœç´¢é¡µé¢ï¼ˆéœ€è¦ç™»å½•ï¼‰
- è¾“å…¥æœç´¢å…³é”®è¯å’ŒæŸ¥è¯¢é¡µæ•°
- ç³»ç»Ÿè‡ªåŠ¨è·å–çœŸå®é—²é±¼å•†å“æ•°æ®
- å•†å“æŒ‰&quot;äººæƒ³è¦&quot;æ•°é‡è‡ªåŠ¨æ’åº
- æ”¯æŒæŸ¥çœ‹å•†å“è¯¦æƒ…å’Œè·³è½¬åˆ°é—²é±¼é¡µé¢

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Webç•Œé¢ (FastAPI)         â”‚
â”‚         ç”¨æˆ·ç®¡ç† + åŠŸèƒ½ç•Œé¢          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        CookieManager               â”‚
â”‚         å¤šè´¦å·ä»»åŠ¡ç®¡ç†              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      XianyuLive (å¤šå®ä¾‹)           â”‚
â”‚     WebSocketè¿æ¥ + æ¶ˆæ¯å¤„ç†        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        SQLiteæ•°æ®åº“                â”‚
â”‚   ç”¨æˆ·æ•°æ® + å•†å“ä¿¡æ¯ + é…ç½®æ•°æ®     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ æ ¸å¿ƒåŠŸèƒ½ç‰¹æ€§

### ğŸš€ è‡ªåŠ¨å›å¤ç³»ç»Ÿ
- **æ™ºèƒ½å…³é”®è¯åŒ¹é…** - æ”¯æŒç²¾ç¡®åŒ¹é…å’Œæ¨¡ç³ŠåŒ¹é…ï¼Œçµæ´»é…ç½®å›å¤è§„åˆ™
- **AIæ™ºèƒ½å›å¤** - é›†æˆå¤šç§AIæ¨¡å‹ï¼ˆé€šä¹‰åƒé—®ã€GPTç­‰ï¼‰ï¼Œæ™ºèƒ½ç†è§£ç”¨æˆ·æ„å›¾
- **å¤šè´¦å·ç®¡ç†** - æ”¯æŒåŒæ—¶ç®¡ç†å¤šä¸ªé—²é±¼è´¦å·ï¼Œç‹¬ç«‹é…ç½®å’Œè¿è¡Œ
- **å®æ—¶æ¶ˆæ¯å¤„ç†** - WebSocketé•¿è¿æ¥ï¼Œæ¯«ç§’çº§å“åº”ç”¨æˆ·æ¶ˆæ¯
- **è‡ªå®šä¹‰å›å¤æ¨¡æ¿** - æ”¯æŒå ä½ç¬¦å’ŒåŠ¨æ€å†…å®¹ï¼Œä¸ªæ€§åŒ–å›å¤ä½“éªŒ

### ğŸ›’ è‡ªåŠ¨å‘è´§ç³»ç»Ÿ
- **æ™ºèƒ½è®¢å•è¯†åˆ«** - è‡ªåŠ¨è¯†åˆ«è™šæ‹Ÿå•†å“è®¢å•ï¼Œç²¾å‡†åŒ¹é…å‘è´§è§„åˆ™
- **å¤šé‡å®‰å…¨éªŒè¯** - è¶…çº§åŠ å¯†ä¿æŠ¤ï¼Œé˜²æ­¢è¯¯æ“ä½œå’Œæ•°æ®æ³„éœ²
- **æ‰¹é‡å¤„ç†èƒ½åŠ›** - æ”¯æŒæ‰¹é‡ç¡®è®¤å‘è´§ï¼Œæé«˜å¤„ç†æ•ˆç‡
- **å¼‚å¸¸å¤„ç†æœºåˆ¶** - å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶ï¼Œç¡®ä¿å‘è´§æˆåŠŸ
- **å¤šæ¸ é“é€šçŸ¥** - æ”¯æŒQQã€é’‰é’‰ã€é£ä¹¦ã€Barkã€é‚®ä»¶ç­‰å¤šç§å‘è´§é€šçŸ¥æ–¹å¼

### ğŸ‘¥ å¤šç”¨æˆ·ç³»ç»Ÿ
- **ç”¨æˆ·æ³¨å†Œç™»å½•** - æ”¯æŒé‚®ç®±éªŒè¯å’Œå›¾å½¢éªŒè¯ç ï¼Œå®‰å…¨å¯é 
- **æƒé™ç®¡ç†** - ç®¡ç†å‘˜å’Œæ™®é€šç”¨æˆ·æƒé™åˆ†ç¦»ï¼Œç²¾ç»†åŒ–æƒé™æ§åˆ¶
- **æ•°æ®éš”ç¦»** - æ¯ä¸ªç”¨æˆ·çš„æ•°æ®å®Œå…¨éš”ç¦»ï¼Œä¿æŠ¤éšç§å®‰å…¨
- **ä¼šè¯ç®¡ç†** - JWT Tokenè®¤è¯ï¼Œæ”¯æŒè‡ªåŠ¨ç»­æœŸå’Œå®‰å…¨ç™»å‡º

### ğŸ“Š æ•°æ®ç®¡ç†
- **å•†å“ä¿¡æ¯ç®¡ç†** - è‡ªåŠ¨è·å–å’ŒåŒæ­¥å•†å“ä¿¡æ¯ï¼Œå®æ—¶æ›´æ–°çŠ¶æ€
- **è®¢å•æ•°æ®ç»Ÿè®¡** - è¯¦ç»†çš„è®¢å•æ•°æ®åˆ†æå’Œå¯è§†åŒ–å›¾è¡¨
- **å…³é”®è¯ç®¡ç†** - çµæ´»çš„å…³é”®è¯é…ç½®ï¼Œæ”¯æŒæ­£åˆ™è¡¨è¾¾å¼
- **æ•°æ®å¯¼å…¥å¯¼å‡º** - æ”¯æŒExcelæ ¼å¼çš„æ‰¹é‡æ•°æ®æ“ä½œ
- **è‡ªåŠ¨å¤‡ä»½** - å®šæœŸè‡ªåŠ¨å¤‡ä»½é‡è¦æ•°æ®ï¼Œé˜²æ­¢æ•°æ®ä¸¢å¤±

### ğŸ” å•†å“æœç´¢
- **çœŸå®æ•°æ®è·å–** - åŸºäºPlaywrightæŠ€æœ¯ï¼Œæ— å¤´æ¨¡å¼è·å–çœŸå®é—²é±¼å•†å“æ•°æ®
- **å¤šé¡µæœç´¢** - æ”¯æŒåˆ†é¡µæœç´¢å’Œæ‰¹é‡è·å–ï¼Œæ— é™åˆ¶æ•°æ®é‡‡é›†
- **æ™ºèƒ½æ’åº** - æŒ‰&quot;äººæƒ³è¦&quot;æ•°é‡è‡ªåŠ¨å€’åºæ’åˆ—ï¼Œä¼˜å…ˆæ˜¾ç¤ºçƒ­é—¨å•†å“
- **æ•°æ®å¯è§†åŒ–** - ç¾è§‚çš„å•†å“å±•ç¤ºç•Œé¢ï¼Œæ”¯æŒæ’åºå’Œç­›é€‰
- **å‰ç«¯åˆ†é¡µ** - çµæ´»çš„å‰ç«¯åˆ†é¡µæ˜¾ç¤ºï¼Œæå‡ç”¨æˆ·ä½“éªŒ
- **è´¦å·çŠ¶æ€éªŒè¯** - è‡ªåŠ¨æ£€æŸ¥cookieså¯ç”¨çŠ¶æ€ï¼Œç¡®ä¿æœç´¢åŠŸèƒ½æ­£å¸¸

### ğŸ“± é€šçŸ¥ç³»ç»Ÿ
- **å¤šæ¸ é“æ”¯æŒ** - QQã€é’‰é’‰ã€é£ä¹¦ã€Barkã€é‚®ä»¶ã€å¾®ä¿¡ã€Telegramç­‰8ç§é€šçŸ¥æ–¹å¼
- **æ™ºèƒ½é…ç½®** - å¯è§†åŒ–é…ç½®ç•Œé¢ï¼Œæ”¯æŒå¤æ‚å‚æ•°å’ŒåŠ å¯†è®¾ç½®
- **å®æ—¶æ¨é€** - é‡è¦äº‹ä»¶å®æ—¶é€šçŸ¥ï¼ŒåŠæ—¶äº†è§£ç³»ç»ŸçŠ¶æ€
- **é€šçŸ¥æ¨¡æ¿** - è‡ªå®šä¹‰é€šçŸ¥å†…å®¹å’Œæ ¼å¼ï¼Œä¸ªæ€§åŒ–æ¶ˆæ¯æ¨é€
- **ç§»åŠ¨ç«¯æ”¯æŒ** - Bark iOSæ¨é€ï¼Œéšæ—¶éšåœ°æ¥æ”¶é€šçŸ¥

### ğŸ” å®‰å…¨ç‰¹æ€§
- **Cookieå®‰å…¨ç®¡ç†** - åŠ å¯†å­˜å‚¨ç”¨æˆ·å‡­è¯ï¼Œå®šæœŸè‡ªåŠ¨åˆ·æ–°
- **Tokenè‡ªåŠ¨åˆ·æ–°** - æ™ºèƒ½æ£€æµ‹å’Œåˆ·æ–°è¿‡æœŸTokenï¼Œä¿æŒè¿æ¥ç¨³å®š
- **æ“ä½œæ—¥å¿—** - è¯¦ç»†è®°å½•æ‰€æœ‰æ“ä½œæ—¥å¿—ï¼Œæ”¯æŒå®¡è®¡å’Œè¿½è¸ª
- **å¼‚å¸¸ç›‘æ§** - å®æ—¶ç›‘æ§ç³»ç»Ÿå¼‚å¸¸å’Œé”™è¯¯ï¼Œä¸»åŠ¨é¢„è­¦

### ğŸ¨ ç”¨æˆ·ç•Œé¢
- **ç°ä»£åŒ–è®¾è®¡** - åŸºäºBootstrap 5çš„å“åº”å¼ç•Œé¢ï¼Œç¾è§‚æ˜“ç”¨
- **å¤šä¸»é¢˜æ”¯æŒ** - æ”¯æŒæ˜æš—ä¸»é¢˜åˆ‡æ¢ï¼Œä¸ªæ€§åŒ–ç•Œé¢ä½“éªŒ
- **ç§»åŠ¨ç«¯é€‚é…** - å®Œç¾é€‚é…æ‰‹æœºå’Œå¹³æ¿è®¾å¤‡ï¼Œéšæ—¶éšåœ°ç®¡ç†
- **å®æ—¶æ›´æ–°** - ç•Œé¢æ•°æ®å®æ—¶æ›´æ–°ï¼Œæ— éœ€æ‰‹åŠ¨åˆ·æ–°

## ğŸ“ æ ¸å¿ƒæ–‡ä»¶åŠŸèƒ½è¯´æ˜

### ğŸš€ æ ¸å¿ƒå¯åŠ¨æ¨¡å—
- **`Start.py`** - é¡¹ç›®å¯åŠ¨å…¥å£ï¼Œåˆå§‹åŒ–CookieManagerå’ŒFastAPIæœåŠ¡ï¼Œä»æ•°æ®åº“åŠ è½½è´¦å·ä»»åŠ¡å¹¶å¯åŠ¨åå°APIæœåŠ¡ï¼Œæ”¯æŒç¯å¢ƒå˜é‡é…ç½®
- **`XianyuAutoAsync.py`** - é—²é±¼WebSocketè¿æ¥æ ¸å¿ƒï¼Œå¤„ç†æ¶ˆæ¯æ”¶å‘ã€è‡ªåŠ¨å›å¤ã€æŒ‡å®šå•†å“å›å¤ã€è‡ªåŠ¨å‘è´§ã€å•†å“ä¿¡æ¯æ”¶é›†ã€AIå›å¤
- **`reply_server.py`** - FastAPI WebæœåŠ¡å™¨ï¼Œæä¾›å®Œæ•´çš„ç®¡ç†ç•Œé¢å’ŒRESTful APIæ¥å£ï¼Œæ”¯æŒå¤šç”¨æˆ·ç³»ç»Ÿã€JWTè®¤è¯ã€æƒé™ç®¡ç†
- **`cookie_manager.py`** - å¤šè´¦å·Cookieç®¡ç†å™¨ï¼Œè´Ÿè´£è´¦å·ä»»åŠ¡çš„å¯åŠ¨ã€åœæ­¢ã€çŠ¶æ€ç®¡ç†å’Œçº¿ç¨‹å®‰å…¨æ“ä½œï¼Œæ”¯æŒæ•°æ®åº“æŒä¹…åŒ–

### ğŸ—„ï¸ æ•°æ®å’Œé…ç½®ç®¡ç†
- **`db_manager.py`** - SQLiteæ•°æ®åº“ç®¡ç†å™¨ï¼Œæ”¯æŒå¤šç”¨æˆ·æ•°æ®éš”ç¦»ã€è‡ªåŠ¨è¿ç§»ã€ç‰ˆæœ¬ç®¡ç†ã€å®Œæ•´çš„CRUDæ“ä½œã€é‚®ç®±éªŒè¯ã€ç³»ç»Ÿè®¾ç½®
- **`config.py`** - å…¨å±€é…ç½®æ–‡ä»¶ç®¡ç†å™¨ï¼ŒåŠ è½½YAMLé…ç½®å’Œç¯å¢ƒå˜é‡ï¼Œæä¾›é…ç½®é¡¹è®¿é—®æ¥å£ï¼Œæ”¯æŒåŠ¨æ€é…ç½®æ›´æ–°
- **`global_config.yml`** - å…¨å±€é…ç½®æ–‡ä»¶ï¼ŒåŒ…å«WebSocketã€APIã€è‡ªåŠ¨å›å¤ã€AIã€é€šçŸ¥ç­‰æ‰€æœ‰ç³»ç»Ÿé…ç½®é¡¹

### ğŸ¤– æ™ºèƒ½åŠŸèƒ½æ¨¡å—
- **`ai_reply_engine.py`** - AIæ™ºèƒ½å›å¤å¼•æ“ï¼Œæ”¯æŒOpenAIã€é€šä¹‰åƒé—®ç­‰å¤šç§AIæ¨¡å‹ï¼Œæ„å›¾è¯†åˆ«ã€ä¸Šä¸‹æ–‡ç®¡ç†ã€ä¸ªæ€§åŒ–å›å¤
- **`secure_confirm_ultra.py`** - è‡ªåŠ¨ç¡®è®¤å‘è´§æ¨¡å—ï¼Œé‡‡ç”¨å¤šå±‚åŠ å¯†ä¿æŠ¤ï¼Œè°ƒç”¨é—²é±¼APIç¡®è®¤å‘è´§çŠ¶æ€ï¼Œæ”¯æŒé”æœºåˆ¶é˜²å¹¶å‘
- **`secure_freeshipping_ultra.py`** - è‡ªåŠ¨å…æ‹¼å‘è´§æ¨¡å—ï¼Œæ”¯æŒæ‰¹é‡å¤„ç†ã€å¼‚å¸¸æ¢å¤ã€æ™ºèƒ½åŒ¹é…ã€è§„æ ¼è¯†åˆ«
- **`file_log_collector.py`** - å®æ—¶æ—¥å¿—æ”¶é›†å™¨ï¼Œæä¾›Webç•Œé¢æ—¥å¿—æŸ¥çœ‹ã€æœç´¢ã€è¿‡æ»¤ã€ä¸‹è½½å’Œç®¡ç†åŠŸèƒ½

### ğŸ› ï¸ å·¥å…·æ¨¡å— (`utils/`)
- **`xianyu_utils.py`** - é—²é±¼APIæ ¸å¿ƒå·¥å…·ï¼ŒåŒ…å«åŠ å¯†ç®—æ³•ã€ç­¾åç”Ÿæˆã€æ•°æ®è§£æã€Cookieå¤„ç†ã€è¯·æ±‚å°è£…
- **`message_utils.py`** - æ¶ˆæ¯å¤„ç†å·¥å…·ï¼Œæ ¼å¼åŒ–æ¶ˆæ¯å†…å®¹ã€å˜é‡æ›¿æ¢ã€å†…å®¹è¿‡æ»¤ã€æ¨¡æ¿æ¸²æŸ“ã€è¡¨æƒ…å¤„ç†
- **`ws_utils.py`** - WebSocketå®¢æˆ·ç«¯å°è£…ï¼Œå¤„ç†è¿æ¥ç®¡ç†ã€å¿ƒè·³æ£€æµ‹ã€é‡è¿æœºåˆ¶ã€æ¶ˆæ¯é˜Ÿåˆ—ã€å¼‚å¸¸æ¢å¤
- **`qr_login.py`** - äºŒç»´ç ç™»å½•åŠŸèƒ½ï¼Œç”Ÿæˆç™»å½•äºŒç»´ç ã€çŠ¶æ€æ£€æµ‹ã€Cookieè·å–ã€éªŒè¯ã€è‡ªåŠ¨åˆ·æ–°
- **`item_search.py`** - å•†å“æœç´¢åŠŸèƒ½ï¼ŒåŸºäºPlaywrightè·å–çœŸå®é—²é±¼å•†å“æ•°æ®ï¼Œæ”¯æŒåˆ†é¡µã€è¿‡æ»¤ã€æ’åº
- **`order_detail_fetcher.py`** - è®¢å•è¯¦æƒ…è·å–å·¥å…·ï¼Œè§£æè®¢å•ä¿¡æ¯ã€ä¹°å®¶ä¿¡æ¯ã€SKUè¯¦æƒ…ï¼Œæ”¯æŒç¼“å­˜ä¼˜åŒ–ã€é”æœºåˆ¶
- **`image_utils.py`** - å›¾ç‰‡å¤„ç†å·¥å…·ï¼Œæ”¯æŒå‹ç¼©ã€æ ¼å¼è½¬æ¢ã€å°ºå¯¸è°ƒæ•´ã€æ°´å°æ·»åŠ ã€è´¨é‡ä¼˜åŒ–
- **`image_uploader.py`** - å›¾ç‰‡ä¸Šä¼ å·¥å…·ï¼Œæ”¯æŒå¤šç§CDNæœåŠ¡å•†ã€è‡ªåŠ¨å‹ç¼©ã€æ ¼å¼ä¼˜åŒ–ã€æ‰¹é‡ä¸Šä¼ 
- **`xianyu_slider_stealth.py`** - å¢å¼ºç‰ˆæ»‘å—éªŒè¯æ¨¡å—ï¼Œé‡‡ç”¨é«˜çº§åæ£€æµ‹æŠ€æœ¯ï¼Œæ”¯æŒå¯†ç ç™»å½•ã€è‡ªåŠ¨é‡è¯•ã€å¹¶å‘æ§åˆ¶ï¼ŒåŒ…å«æˆæƒæœŸé™éªŒè¯æœºåˆ¶ï¼ˆå¯ç¼–è¯‘ä¸ºäºŒè¿›åˆ¶æ¨¡å—ä»¥æå‡æ€§èƒ½å’Œå®‰å…¨æ€§ï¼‰
- **`refresh_util.py`** - Cookieåˆ·æ–°å·¥å…·ï¼Œè‡ªåŠ¨æ£€æµ‹å’Œåˆ·æ–°è¿‡æœŸçš„Cookieï¼Œä¿æŒè´¦å·è¿æ¥çŠ¶æ€

### ğŸŒ å‰ç«¯ç•Œé¢ (`static/`)
- **`index.html`** - ä¸»ç®¡ç†ç•Œé¢ï¼Œé›†æˆæ‰€æœ‰åŠŸèƒ½æ¨¡å—ï¼šè´¦å·ç®¡ç†ã€å…³é”®è¯ç®¡ç†ã€å•†å“ç®¡ç†ã€å‘è´§ç®¡ç†ã€ç³»ç»Ÿç›‘æ§ã€ç”¨æˆ·ç®¡ç†ç­‰
- **`login.html`** - ç”¨æˆ·ç™»å½•é¡µé¢ï¼Œæ”¯æŒå›¾å½¢éªŒè¯ç ã€è®°ä½ç™»å½•çŠ¶æ€ã€å¤šé‡å®‰å…¨éªŒè¯
- **`register.html`** - ç”¨æˆ·æ³¨å†Œé¡µé¢ï¼Œæ”¯æŒé‚®ç®±éªŒè¯ç ã€å®æ—¶éªŒè¯ã€å¯†ç å¼ºåº¦æ£€æµ‹
- **`js/app.js`** - ä¸»è¦JavaScripté€»è¾‘ï¼ŒåŒ…å«æ‰€æœ‰åŠŸèƒ½æ¨¡å—ï¼šå‰ç«¯äº¤äº’ã€APIè°ƒç”¨ã€å®æ—¶æ›´æ–°ã€æ•°æ®ç®¡ç†ã€ç”¨æˆ·ç•Œé¢æ§åˆ¶
- **`css/`** - æ¨¡å—åŒ–æ ·å¼æ–‡ä»¶ï¼ŒåŒ…å«å¸ƒå±€ã€ç»„ä»¶ã€ä¸»é¢˜ç­‰åˆ†ç±»æ ·å¼ï¼Œå“åº”å¼è®¾è®¡ï¼Œæ”¯æŒæ˜æš—ä¸»é¢˜åˆ‡æ¢
- **`xianyu_js_version_2.js`** - é—²é±¼JavaScriptå·¥å…·åº“ï¼ŒåŠ å¯†è§£å¯†ã€æ•°æ®å¤„ç†ã€APIå°è£…
- **`lib/`** - å‰ç«¯ä¾èµ–åº“ï¼ŒåŒ…å«Bootstrap 5ã€Bootstrap Iconsç­‰ç¬¬ä¸‰æ–¹åº“
- **`uploads/images/`** - å›¾ç‰‡ä¸Šä¼ ç›®å½•ï¼Œæ”¯æŒå‘è´§å›¾ç‰‡å’Œå…¶ä»–åª’ä½“æ–‡ä»¶å­˜å‚¨

### ğŸ³ éƒ¨ç½²é…ç½®
- **`Dockerfile`** - Dockeré•œåƒæ„å»ºæ–‡ä»¶ï¼ŒåŸºäºPython 3.11-slimï¼ŒåŒ…å«Playwrightæµè§ˆå™¨ã€Cç¼–è¯‘å™¨ï¼ˆæ”¯æŒNuitkaç¼–è¯‘ï¼‰ã€ç³»ç»Ÿä¾èµ–ï¼Œæ”¯æŒæ— å¤´æ¨¡å¼è¿è¡Œï¼Œä¼˜åŒ–æ„å»ºå±‚çº§ï¼Œè‡ªåŠ¨ç¼–è¯‘æ€§èƒ½å…³é”®æ¨¡å—
- **`Dockerfile-cn`** - å›½å†…ä¼˜åŒ–ç‰ˆDockeré•œåƒæ„å»ºæ–‡ä»¶ï¼Œä½¿ç”¨å›½å†…é•œåƒæºåŠ é€Ÿæ„å»ºï¼Œé€‚åˆå›½å†…ç½‘ç»œç¯å¢ƒ
- **`docker-compose.yml`** - Docker Composeé…ç½®ï¼Œæ”¯æŒä¸€é”®éƒ¨ç½²ã€å®Œæ•´ç¯å¢ƒå˜é‡é…ç½®ã€èµ„æºé™åˆ¶ã€å¥åº·æ£€æŸ¥ã€å¯é€‰Nginxä»£ç†
- **`docker-compose-cn.yml`** - å›½å†…ä¼˜åŒ–ç‰ˆDocker Composeé…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨å›½å†…é•œåƒæº
- **`docker-deploy.sh`** - Dockeréƒ¨ç½²ç®¡ç†è„šæœ¬ï¼Œæä¾›æ„å»ºã€å¯åŠ¨ã€åœæ­¢ã€é‡å¯ã€ç›‘æ§ã€æ—¥å¿—æŸ¥çœ‹ç­‰åŠŸèƒ½ï¼ˆLinux/macOSï¼‰
- **`docker-deploy.bat`** - Windowsç‰ˆæœ¬éƒ¨ç½²è„šæœ¬ï¼Œæ”¯æŒWindowsç¯å¢ƒä¸€é”®éƒ¨ç½²å’Œç®¡ç†
- **`entrypoint.sh`** - Dockerå®¹å™¨å¯åŠ¨è„šæœ¬ï¼Œå¢å¼ºç‰ˆåŒ…å«ç¯å¢ƒéªŒè¯ã€ä¾èµ–æ£€æŸ¥ã€ç›®å½•åˆ›å»ºã€æƒé™è®¾ç½®å’Œè¯¦ç»†å¯åŠ¨æ—¥å¿—
- **`nginx/nginx.conf`** - Nginxåå‘ä»£ç†é…ç½®ï¼Œæ”¯æŒè´Ÿè½½å‡è¡¡ã€SSLç»ˆç«¯ã€WebSocketä»£ç†ã€é™æ€æ–‡ä»¶æœåŠ¡
- **`requirements.txt`** - Pythonä¾èµ–åŒ…åˆ—è¡¨ï¼Œç²¾ç®€ç‰ˆæœ¬æ— å†…ç½®æ¨¡å—ï¼ŒæŒ‰åŠŸèƒ½åˆ†ç±»ç»„ç»‡ï¼ŒåŒ…å«è¯¦ç»†ç‰ˆæœ¬è¯´æ˜å’Œå®‰è£…æŒ‡å—
- **`.gitignore`** - Gitå¿½ç•¥æ–‡ä»¶é…ç½®ï¼Œå®Œæ•´è¦†ç›–Pythonã€Dockerã€å‰ç«¯ã€æµ‹è¯•ã€ä¸´æ—¶æ–‡ä»¶ç­‰ï¼Œ2025å¹´æ›´æ–°åŒ…å«ç¼–è¯‘äº§ç‰©ã€æµè§ˆå™¨ç¼“å­˜ã€ç»Ÿè®¡æ•°æ®ç­‰æ–°è§„åˆ™
- **`.dockerignore`** - Dockeræ„å»ºå¿½ç•¥æ–‡ä»¶ï¼Œä¼˜åŒ–æ„å»ºä¸Šä¸‹æ–‡å¤§å°å’Œæ„å»ºé€Ÿåº¦ï¼Œæ’é™¤ä¸å¿…è¦çš„æ–‡ä»¶å’Œç›®å½•ï¼Œ2025å¹´æ›´æ–°åŒ…å«æµè§ˆå™¨æ•°æ®ç­‰æ–°è§„åˆ™

## ğŸ—ï¸ è¯¦ç»†æŠ€æœ¯æ¶æ„

### ğŸ“Š ç³»ç»Ÿæ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Webå‰ç«¯ç•Œé¢                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ç”¨æˆ·ç®¡ç†   â”‚ â”‚  è´¦å·ç®¡ç†   â”‚ â”‚  å…³é”®è¯ç®¡ç† â”‚ â”‚  å•†å“ç®¡ç†   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  æ—¥å¿—ç®¡ç†   â”‚ â”‚  æ•°æ®ç®¡ç†   â”‚ â”‚  å•†å“æœç´¢   â”‚ â”‚  ç³»ç»Ÿç›‘æ§   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FastAPI WebæœåŠ¡å™¨                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ç”¨æˆ·è®¤è¯   â”‚ â”‚  æƒé™ç®¡ç†   â”‚ â”‚  APIæ¥å£    â”‚ â”‚  æ–‡ä»¶ä¸Šä¼    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  é‚®ç®±éªŒè¯   â”‚ â”‚  å›¾å½¢éªŒè¯ç  â”‚ â”‚  å®æ—¶æ—¥å¿—   â”‚ â”‚  å¥åº·æ£€æŸ¥   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CookieManager å¤šè´¦å·ç®¡ç†å™¨                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  ä»»åŠ¡è°ƒåº¦   â”‚ â”‚  çŠ¶æ€ç›‘æ§   â”‚ â”‚  çº¿ç¨‹ç®¡ç†   â”‚ â”‚  å¼‚å¸¸å¤„ç†   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                XianyuLive å®ä¾‹é›†ç¾¤ (å¤šå®ä¾‹å¹¶è¡Œ)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  è´¦å·Aå®ä¾‹  â”‚ â”‚  è´¦å·Bå®ä¾‹  â”‚ â”‚  è´¦å·Cå®ä¾‹  â”‚ â”‚    ...      â”‚ â”‚
â”‚  â”‚ WebSocket   â”‚ â”‚ WebSocket   â”‚ â”‚ WebSocket   â”‚ â”‚             â”‚ â”‚
â”‚  â”‚ æ¶ˆæ¯å¤„ç†    â”‚ â”‚ æ¶ˆæ¯å¤„ç†    â”‚ â”‚ æ¶ˆæ¯å¤„ç†    â”‚ â”‚             â”‚ â”‚
â”‚  â”‚ è‡ªåŠ¨å›å¤    â”‚ â”‚ è‡ªåŠ¨å›å¤    â”‚ â”‚ è‡ªåŠ¨å›å¤    â”‚ â”‚             â”‚ â”‚
â”‚  â”‚ è‡ªåŠ¨å‘è´§    â”‚ â”‚ è‡ªåŠ¨å‘è´§    â”‚ â”‚ è‡ªåŠ¨å‘è´§    â”‚ â”‚             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      è¾…åŠ©æœåŠ¡æ¨¡å—                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ AIå›å¤å¼•æ“  â”‚ â”‚ å›¾ç‰‡å¤„ç†    â”‚ â”‚ å•†å“æœç´¢    â”‚ â”‚ è®¢å•å¤„ç†    â”‚ â”‚

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[freqtrade/freqtrade]]></title>
            <link>https://github.com/freqtrade/freqtrade</link>
            <guid>https://github.com/freqtrade/freqtrade</guid>
            <pubDate>Mon, 15 Dec 2025 00:04:52 GMT</pubDate>
            <description><![CDATA[Free, open source crypto trading bot]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/freqtrade/freqtrade">freqtrade/freqtrade</a></h1>
            <p>Free, open source crypto trading bot</p>
            <p>Language: Python</p>
            <p>Stars: 45,272</p>
            <p>Forks: 9,399</p>
            <p>Stars today: 22 stars today</p>
            <h2>README</h2><pre># ![freqtrade](https://raw.githubusercontent.com/freqtrade/freqtrade/develop/docs/assets/freqtrade_poweredby.svg)

[![Freqtrade CI](https://github.com/freqtrade/freqtrade/actions/workflows/ci.yml/badge.svg?branch=develop)](https://github.com/freqtrade/freqtrade/actions/workflows/ci.yml)
[![DOI](https://joss.theoj.org/papers/10.21105/joss.04864/status.svg)](https://doi.org/10.21105/joss.04864)
[![Coverage Status](https://coveralls.io/repos/github/freqtrade/freqtrade/badge.svg?branch=develop&amp;service=github)](https://coveralls.io/github/freqtrade/freqtrade?branch=develop)
[![Documentation](https://readthedocs.org/projects/freqtrade/badge/)](https://www.freqtrade.io)

Freqtrade is a free and open source crypto trading bot written in Python. It is designed to support all major exchanges and be controlled via Telegram or webUI. It contains backtesting, plotting and money management tools as well as strategy optimization by machine learning.

![freqtrade](https://raw.githubusercontent.com/freqtrade/freqtrade/develop/docs/assets/freqtrade-screenshot.png)

## Disclaimer

This software is for educational purposes only. Do not risk money which
you are afraid to lose. USE THE SOFTWARE AT YOUR OWN RISK. THE AUTHORS
AND ALL AFFILIATES ASSUME NO RESPONSIBILITY FOR YOUR TRADING RESULTS.

Always start by running a trading bot in Dry-Run and do not engage money
before you understand how it works and what profit/loss you should
expect.

We strongly recommend you to have coding and Python knowledge. Do not
hesitate to read the source code and understand the mechanism of this bot.

## Supported Exchange marketplaces

Please read the [exchange-specific notes](docs/exchanges.md) to learn about special configurations that maybe needed for each exchange.

- [X] [Binance](https://www.binance.com/)
- [X] [BingX](https://bingx.com/invite/0EM9RX)
- [X] [Bitget](https://www.bitget.com/)
- [X] [Bitmart](https://bitmart.com/)
- [X] [Bybit](https://bybit.com/)
- [X] [Gate.io](https://www.gate.io/ref/6266643)
- [X] [HTX](https://www.htx.com/)
- [X] [Hyperliquid](https://hyperliquid.xyz/) (A decentralized exchange, or DEX)
- [X] [Kraken](https://kraken.com/)
- [X] [OKX](https://okx.com/)
- [X] [MyOKX](https://okx.com/) (OKX EEA)
- [ ] [potentially many others](https://github.com/ccxt/ccxt/). _(We cannot guarantee they will work)_

### Supported Futures Exchanges (experimental)

- [X] [Binance](https://www.binance.com/)
- [X] [Bitget](https://www.bitget.com/)
- [X] [Gate.io](https://www.gate.io/ref/6266643)
- [X] [Hyperliquid](https://hyperliquid.xyz/) (A decentralized exchange, or DEX)
- [X] [OKX](https://okx.com/)
- [X] [Bybit](https://bybit.com/)

Please make sure to read the [exchange specific notes](docs/exchanges.md), as well as the [trading with leverage](docs/leverage.md) documentation before diving in.

### Community tested

Exchanges confirmed working by the community:

- [X] [Bitvavo](https://bitvavo.com/)
- [X] [Kucoin](https://www.kucoin.com/)

## Documentation

We invite you to read the bot documentation to ensure you understand how the bot is working.

Please find the complete documentation on the [freqtrade website](https://www.freqtrade.io).

## Features

- [x] **Based on Python 3.11+**: For botting on any operating system - Windows, macOS and Linux.
- [x] **Persistence**: Persistence is achieved through sqlite.
- [x] **Dry-run**: Run the bot without paying money.
- [x] **Backtesting**: Run a simulation of your buy/sell strategy.
- [x] **Strategy Optimization by machine learning**: Use machine learning to optimize your buy/sell strategy parameters with real exchange data.
- [X] **Adaptive prediction modeling**: Build a smart strategy with FreqAI that self-trains to the market via adaptive machine learning methods. [Learn more](https://www.freqtrade.io/en/stable/freqai/)
- [x] **Whitelist crypto-currencies**: Select which crypto-currency you want to trade or use dynamic whitelists.
- [x] **Blacklist crypto-currencies**: Select which crypto-currency you want to avoid.
- [x] **Builtin WebUI**: Builtin web UI to manage your bot.
- [x] **Manageable via Telegram**: Manage the bot with Telegram.
- [x] **Display profit/loss in fiat**: Display your profit/loss in fiat currency.
- [x] **Performance status report**: Provide a performance status of your current trades.

## Quick start

Please refer to the [Docker Quickstart documentation](https://www.freqtrade.io/en/stable/docker_quickstart/) on how to get started quickly.

For further (native) installation methods, please refer to the [Installation documentation page](https://www.freqtrade.io/en/stable/installation/).

## Basic Usage

### Bot commands

```
usage: freqtrade [-h] [-V]
                 {trade,create-userdir,new-config,show-config,new-strategy,download-data,convert-data,convert-trade-data,trades-to-ohlcv,list-data,backtesting,backtesting-show,backtesting-analysis,edge,hyperopt,hyperopt-list,hyperopt-show,list-exchanges,list-markets,list-pairs,list-strategies,list-hyperoptloss,list-freqaimodels,list-timeframes,show-trades,test-pairlist,convert-db,install-ui,plot-dataframe,plot-profit,webserver,strategy-updater,lookahead-analysis,recursive-analysis}
                 ...

Free, open source crypto trading bot

positional arguments:
  {trade,create-userdir,new-config,show-config,new-strategy,download-data,convert-data,convert-trade-data,trades-to-ohlcv,list-data,backtesting,backtesting-show,backtesting-analysis,edge,hyperopt,hyperopt-list,hyperopt-show,list-exchanges,list-markets,list-pairs,list-strategies,list-hyperoptloss,list-freqaimodels,list-timeframes,show-trades,test-pairlist,convert-db,install-ui,plot-dataframe,plot-profit,webserver,strategy-updater,lookahead-analysis,recursive-analysis}
    trade               Trade module.
    create-userdir      Create user-data directory.
    new-config          Create new config
    show-config         Show resolved config
    new-strategy        Create new strategy
    download-data       Download backtesting data.
    convert-data        Convert candle (OHLCV) data from one format to
                        another.
    convert-trade-data  Convert trade data from one format to another.
    trades-to-ohlcv     Convert trade data to OHLCV data.
    list-data           List downloaded data.
    backtesting         Backtesting module.
    backtesting-show    Show past Backtest results
    backtesting-analysis
                        Backtest Analysis module.
    hyperopt            Hyperopt module.
    hyperopt-list       List Hyperopt results
    hyperopt-show       Show details of Hyperopt results
    list-exchanges      Print available exchanges.
    list-markets        Print markets on exchange.
    list-pairs          Print pairs on exchange.
    list-strategies     Print available strategies.
    list-hyperoptloss   Print available hyperopt loss functions.
    list-freqaimodels   Print available freqAI models.
    list-timeframes     Print available timeframes for the exchange.
    show-trades         Show trades.
    test-pairlist       Test your pairlist configuration.
    convert-db          Migrate database to different system
    install-ui          Install FreqUI
    plot-dataframe      Plot candles with indicators.
    plot-profit         Generate plot showing profits.
    webserver           Webserver module.
    strategy-updater    updates outdated strategy files to the current version
    lookahead-analysis  Check for potential look ahead bias.
    recursive-analysis  Check for potential recursive formula issue.

options:
  -h, --help            show this help message and exit
  -V, --version         show program&#039;s version number and exit
```

### Telegram RPC commands

Telegram is not mandatory. However, this is a great way to control your bot. More details and the full command list on the [documentation](https://www.freqtrade.io/en/latest/telegram-usage/)

- `/start`: Starts the trader.
- `/stop`: Stops the trader.
- `/stopentry`: Stop entering new trades.
- `/status &lt;trade_id&gt;|[table]`: Lists all or specific open trades.
- `/profit [&lt;n&gt;]`: Lists cumulative profit from all finished trades, over the last n days.
- `/profit_long [&lt;n&gt;]`: Lists cumulative profit from all finished long trades, over the last n days.
- `/profit_short [&lt;n&gt;]`: Lists cumulative profit from all finished short trades, over the last n days.
- `/forceexit &lt;trade_id&gt;|all`: Instantly exits the given trade (Ignoring `minimum_roi`).
- `/fx &lt;trade_id&gt;|all`: Alias to `/forceexit`
- `/performance`: Show performance of each finished trade grouped by pair
- `/balance`: Show account balance per currency.
- `/daily &lt;n&gt;`: Shows profit or loss per day, over the last n days.
- `/help`: Show help message.
- `/version`: Show version.


## Development branches

The project is currently setup in two main branches:

- `develop` - This branch has often new features, but might also contain breaking changes. We try hard to keep this branch as stable as possible.
- `stable` - This branch contains the latest stable release. This branch is generally well tested.
- `feat/*` - These are feature branches, which are being worked on heavily. Please don&#039;t use these unless you want to test a specific feature.

## Support

### Help / Discord

For any questions not covered by the documentation or for further information about the bot, or to simply engage with like-minded individuals, we encourage you to join the Freqtrade [discord server](https://discord.gg/p7nuUNVfP7).

### [Bugs / Issues](https://github.com/freqtrade/freqtrade/issues?q=is%3Aissue)

If you discover a bug in the bot, please
[search the issue tracker](https://github.com/freqtrade/freqtrade/issues?q=is%3Aissue)
first. If it hasn&#039;t been reported, please
[create a new issue](https://github.com/freqtrade/freqtrade/issues/new/choose) and
ensure you follow the template guide so that the team can assist you as
quickly as possible.

For every [issue](https://github.com/freqtrade/freqtrade/issues/new/choose) created, kindly follow up and mark satisfaction or reminder to close issue when equilibrium ground is reached.

--Maintain github&#039;s [community policy](https://docs.github.com/en/site-policy/github-terms/github-community-code-of-conduct)--

### [Feature Requests](https://github.com/freqtrade/freqtrade/labels/enhancement)

Have you a great idea to improve the bot you want to share? Please,
first search if this feature was not [already discussed](https://github.com/freqtrade/freqtrade/labels/enhancement).
If it hasn&#039;t been requested, please
[create a new request](https://github.com/freqtrade/freqtrade/issues/new/choose)
and ensure you follow the template guide so that it does not get lost
in the bug reports.

### [Pull Requests](https://github.com/freqtrade/freqtrade/pulls)

Feel like the bot is missing a feature? We welcome your pull requests!

Please read the
[Contributing document](https://github.com/freqtrade/freqtrade/blob/develop/CONTRIBUTING.md)
to understand the requirements before sending your pull-requests.

Coding is not a necessity to contribute - maybe start with improving the documentation?
Issues labeled [good first issue](https://github.com/freqtrade/freqtrade/labels/good%20first%20issue) can be good first contributions, and will help get you familiar with the codebase.

**Note** before starting any major new feature work, *please open an issue describing what you are planning to do* or talk to us on [discord](https://discord.gg/p7nuUNVfP7) (please use the #dev channel for this). This will ensure that interested parties can give valuable feedback on the feature, and let others know that you are working on it.

**Important:** Always create your PR against the `develop` branch, not `stable`.

## Requirements

### Up-to-date clock

The clock must be accurate, synchronized to a NTP server very frequently to avoid problems with communication to the exchanges.

### Minimum hardware required

To run this bot we recommend you a cloud instance with a minimum of:

- Minimal (advised) system requirements: 2GB RAM, 1GB disk space, 2vCPU

### Software requirements

- [Python &gt;= 3.11](http://docs.python-guide.org/en/latest/starting/installation/)
- [pip](https://pip.pypa.io/en/stable/installing/)
- [git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)
- [TA-Lib](https://ta-lib.github.io/ta-lib-python/)
- [virtualenv](https://virtualenv.pypa.io/en/stable/installation.html) (Recommended)
- [Docker](https://www.docker.com/products/docker) (Recommended)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Mirrowel/LLM-API-Key-Proxy]]></title>
            <link>https://github.com/Mirrowel/LLM-API-Key-Proxy</link>
            <guid>https://github.com/Mirrowel/LLM-API-Key-Proxy</guid>
            <pubDate>Mon, 15 Dec 2025 00:04:51 GMT</pubDate>
            <description><![CDATA[Universal LLM Gateway: One API, every LLM. OpenAI-compatible endpoints with multi-provider translation and intelligent load-balancing.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Mirrowel/LLM-API-Key-Proxy">Mirrowel/LLM-API-Key-Proxy</a></h1>
            <p>Universal LLM Gateway: One API, every LLM. OpenAI-compatible endpoints with multi-provider translation and intelligent load-balancing.</p>
            <p>Language: Python</p>
            <p>Stars: 85</p>
            <p>Forks: 22</p>
            <p>Stars today: 22 stars today</p>
            <h2>README</h2><pre># Universal LLM API Proxy &amp; Resilience Library 
[![ko-fi](https://ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/C0C0UZS4P)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/Mirrowel/LLM-API-Key-Proxy) [![zread](https://img.shields.io/badge/Ask_Zread-_.svg?style=flat&amp;color=00b0aa&amp;labelColor=000000&amp;logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdCb3g9IjAgMCAxNiAxNiIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTQuOTYxNTYgMS42MDAxSDIuMjQxNTZDMS44ODgxIDEuNjAwMSAxLjYwMTU2IDEuODg2NjQgMS42MDE1NiAyLjI0MDFWNC45NjAxQzEuNjAxNTYgNS4zMTM1NiAxLjg4ODEgNS42MDAxIDIuMjQxNTYgNS42MDAxSDQuOTYxNTZDNS4zMTUwMiA1LjYwMDEgNS42MDE1NiA1LjMxMzU2IDUuNjAxNTYgNC45NjAxVjIuMjQwMUM1LjYwMTU2IDEuODg2NjQgNS4zMTUwMiAxLjYwMDEgNC45NjE1NiAxLjYwMDFaIiBmaWxsPSIjZmZmIi8%2BCjxwYXRoIGQ9Ik00Ljk2MTU2IDEwLjM5OTlIMi4yNDE1NkMxLjg4ODEgMTAuMzk5OSAxLjYwMTU2IDEwLjY4NjQgMS42MDE1NiAxMS4wMzk5VjEzLjc1OTlDMS42MDE1NiAxNC4xMTM0IDEuODg4MSAxNC4zOTk5IDIuMjQxNTYgMTQuMzk5OUg0Ljk2MTU2QzUuMzE1MDIgMTQuMzk5OSA1LjYwMTU2IDE0LjExMzQgNS42MDE1NiAxMy43NTk5VjExLjAzOTlDNS42MDE1NiAxMC42ODY0IDUuMzE1MDIgMTAuMzk5OSA0Ljk2MTU2IDEwLjM5OTlaIiBmaWxsPSIjZmZmIi8%2BCjxwYXRoIGQ9Ik0xMy43NTg0IDEuNjAwMUgxMS4wMzg0QzEwLjY4NSAxLjYwMDEgMTAuMzk4NCAxLjg4NjY0IDEwLjM5ODQgMi4yNDAxVjQuOTYwMUMxMC4zOTg0IDUuMzEzNTYgMTAuNjg1IDUuNjAwMSAxMS4wMzg0IDUuNjAwMUgxMy43NTg0QzE0LjExMTkgNS42MDAxIDE0LjM5ODQgNS4zMTM1NiAxNC4zOTg0IDQuOTYwMVYyLjI0MDFDMTQuMzk4NCAxLjg4NjY0IDE0LjExMTkgMS42MDAxIDEzLjc1ODQgMS42MDAxWiIgZmlsbD0iI2ZmZiIvPgo8cGF0aCBkPSJNNCAxMkwxMiA0TDQgMTJaIiBmaWxsPSIjZmZmIi8%2BCjxwYXRoIGQ9Ik00IDEyTDEyIDQiIHN0cm9rZT0iI2ZmZiIgc3Ryb2tlLXdpZHRoPSIxLjUiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIvPgo8L3N2Zz4K&amp;logoColor=ffffff)](https://zread.ai/Mirrowel/LLM-API-Key-Proxy)

**One proxy. Any LLM provider. Zero code changes.**

A self-hosted proxy that provides a single, OpenAI-compatible API endpoint for all your LLM providers. Works with any application that supports custom OpenAI base URLsâ€”no code changes required in your existing tools.

This project consists of two components:
1. **The API Proxy** â€” A FastAPI application providing a universal `/v1/chat/completions` endpoint
2. **The Resilience Library** â€” A reusable Python library for intelligent API key management, rotation, and failover

---

## Why Use This?

- **Universal Compatibility** â€” Works with any app supporting OpenAI-compatible APIs: Opencode, Continue, Roo/Kilo Code, JanitorAI, SillyTavern, custom applications, and more
- **One Endpoint, Many Providers** â€” Configure Gemini, OpenAI, Anthropic, and [any LiteLLM-supported provider](https://docs.litellm.ai/docs/providers) once. Access them all through a single API key
- **Built-in Resilience** â€” Automatic key rotation, failover on errors, rate limit handling, and intelligent cooldowns
- **Exclusive Provider Support** â€” Includes custom providers not available elsewhere: **Antigravity** (Gemini 3 + Claude Sonnet/Opus 4.5), **Gemini CLI**, **Qwen Code**, and **iFlow**

---

## Quick Start

### Windows

1. **Download** the latest release from [GitHub Releases](https://github.com/Mirrowel/LLM-API-Key-Proxy/releases/latest)
2. **Unzip** the downloaded file
3. **Run** `proxy_app.exe` â€” the interactive TUI launcher opens

&lt;!-- TODO: Add TUI main menu screenshot here --&gt;

### macOS / Linux

```bash
# Download and extract the release for your platform
chmod +x proxy_app
./proxy_app
```

### From Source

```bash
git clone https://github.com/Mirrowel/LLM-API-Key-Proxy.git
cd LLM-API-Key-Proxy
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
python src/proxy_app/main.py
```

&gt; **Tip:** Running with command-line arguments (e.g., `--host 0.0.0.0 --port 8000`) bypasses the TUI and starts the proxy directly.

---

## Connecting to the Proxy

Once the proxy is running, configure your application with these settings:

| Setting | Value |
|---------|-------|
| **Base URL / API Endpoint** | `http://127.0.0.1:8000/v1` |
| **API Key** | Your `PROXY_API_KEY` |

### Model Format: `provider/model_name`

**Important:** Models must be specified in the format `provider/model_name`. The `provider/` prefix tells the proxy which backend to route the request to.

```
gemini/gemini-2.5-flash          â† Gemini API
openai/gpt-4o                    â† OpenAI API
anthropic/claude-3-5-sonnet      â† Anthropic API
openrouter/anthropic/claude-3-opus  â† OpenRouter
gemini_cli/gemini-2.5-pro        â† Gemini CLI (OAuth)
antigravity/gemini-3-pro-preview â† Antigravity (Gemini 3, Claude Opus 4.5)
```

### Usage Examples

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Python (OpenAI Library)&lt;/b&gt;&lt;/summary&gt;

```python
from openai import OpenAI

client = OpenAI(
    base_url=&quot;http://127.0.0.1:8000/v1&quot;,
    api_key=&quot;your-proxy-api-key&quot;
)

response = client.chat.completions.create(
    model=&quot;gemini/gemini-2.5-flash&quot;,  # provider/model format
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}]
)
print(response.choices[0].message.content)
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;curl&lt;/b&gt;&lt;/summary&gt;

```bash
curl -X POST http://127.0.0.1:8000/v1/chat/completions \
  -H &quot;Content-Type: application/json&quot; \
  -H &quot;Authorization: Bearer your-proxy-api-key&quot; \
  -d &#039;{
    &quot;model&quot;: &quot;gemini/gemini-2.5-flash&quot;,
    &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is the capital of France?&quot;}]
  }&#039;
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;JanitorAI / SillyTavern / Other Chat UIs&lt;/b&gt;&lt;/summary&gt;

1. Go to **API Settings**
2. Select **&quot;Proxy&quot;** or **&quot;Custom OpenAI&quot;** mode
3. Configure:
   - **API URL:** `http://127.0.0.1:8000/v1`
   - **API Key:** Your `PROXY_API_KEY`
   - **Model:** `provider/model_name` (e.g., `gemini/gemini-2.5-flash`)
4. Save and start chatting

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Continue / Cursor / IDE Extensions&lt;/b&gt;&lt;/summary&gt;

In your configuration file (e.g., `config.json`):

```json
{
  &quot;models&quot;: [{
    &quot;title&quot;: &quot;Gemini via Proxy&quot;,
    &quot;provider&quot;: &quot;openai&quot;,
    &quot;model&quot;: &quot;gemini/gemini-2.5-flash&quot;,
    &quot;apiBase&quot;: &quot;http://127.0.0.1:8000/v1&quot;,
    &quot;apiKey&quot;: &quot;your-proxy-api-key&quot;
  }]
}
```

&lt;/details&gt;

### API Endpoints

| Endpoint | Description |
|----------|-------------|
| `GET /` | Status check â€” confirms proxy is running |
| `POST /v1/chat/completions` | Chat completions (main endpoint) |
| `POST /v1/embeddings` | Text embeddings |
| `GET /v1/models` | List all available models with pricing &amp; capabilities |
| `GET /v1/models/{model_id}` | Get details for a specific model |
| `GET /v1/providers` | List configured providers |
| `POST /v1/token-count` | Calculate token count for a payload |
| `POST /v1/cost-estimate` | Estimate cost based on token counts |

&gt; **Tip:** The `/v1/models` endpoint is useful for discovering available models in your client. Many apps can fetch this list automatically. Add `?enriched=false` for a minimal response without pricing data.

---

## Managing Credentials

The proxy includes an interactive tool for managing all your API keys and OAuth credentials.

### Using the TUI

&lt;!-- TODO: Add TUI credentials menu screenshot here --&gt;

1. Run the proxy without arguments to open the TUI
2. Select **&quot;ğŸ”‘ Manage Credentials&quot;**
3. Choose to add API keys or OAuth credentials

### Using the Command Line

```bash
python -m rotator_library.credential_tool
```

### Credential Types

| Type | Providers | How to Add |
|------|-----------|------------|
| **API Keys** | Gemini, OpenAI, Anthropic, OpenRouter, Groq, Mistral, NVIDIA, Cohere, Chutes | Enter key in TUI or add to `.env` |
| **OAuth** | Gemini CLI, Antigravity, Qwen Code, iFlow | Interactive browser login via credential tool |

### The `.env` File

Credentials are stored in a `.env` file. You can edit it directly or use the TUI:

```env
# Required: Authentication key for YOUR proxy
PROXY_API_KEY=&quot;your-secret-proxy-key&quot;

# Provider API Keys (add multiple with _1, _2, etc.)
GEMINI_API_KEY_1=&quot;your-gemini-key&quot;
GEMINI_API_KEY_2=&quot;another-gemini-key&quot;
OPENAI_API_KEY_1=&quot;your-openai-key&quot;
ANTHROPIC_API_KEY_1=&quot;your-anthropic-key&quot;
```

&gt; Copy `.env.example` to `.env` as a starting point.

---

## The Resilience Library

The proxy is powered by a standalone Python library that you can use directly in your own applications.

### Key Features

- **Async-native** with `asyncio` and `httpx`
- **Intelligent key selection** with tiered, model-aware locking
- **Deadline-driven requests** with configurable global timeout
- **Automatic failover** between keys on errors
- **OAuth support** for Gemini CLI, Antigravity, Qwen, iFlow
- **Stateless deployment ready** â€” load credentials from environment variables

### Basic Usage

```python
from rotator_library import RotatingClient

client = RotatingClient(
    api_keys={&quot;gemini&quot;: [&quot;key1&quot;, &quot;key2&quot;], &quot;openai&quot;: [&quot;key3&quot;]},
    global_timeout=30,
    max_retries=2
)

async with client:
    response = await client.acompletion(
        model=&quot;gemini/gemini-2.5-flash&quot;,
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}]
    )
```

### Library Documentation

See the [Library README](src/rotator_library/README.md) for complete documentation including:
- All initialization parameters
- Streaming support
- Error handling and cooldown strategies
- Provider plugin system
- Credential prioritization

---

## Interactive TUI

The proxy includes a powerful text-based UI for configuration and management.

&lt;!-- TODO: Add TUI main menu screenshot here --&gt;

### TUI Features

- **ğŸš€ Run Proxy** â€” Start the server with saved settings
- **âš™ï¸ Configure Settings** â€” Host, port, API key, request logging
- **ğŸ”‘ Manage Credentials** â€” Add/edit API keys and OAuth credentials
- **ğŸ“Š View Status** â€” See configured providers and credential counts
- **ğŸ”§ Advanced Settings** â€” Custom providers, model definitions, concurrency

### Configuration Files

| File | Contents |
|------|----------|
| `.env` | All credentials and advanced settings |
| `launcher_config.json` | TUI-specific settings (host, port, logging) |

---

## Features

### Core Capabilities

- **Universal OpenAI-compatible endpoint** for all providers
- **Multi-provider support** via [LiteLLM](https://docs.litellm.ai/docs/providers) fallback
- **Automatic key rotation** and load balancing
- **Interactive TUI** for easy configuration
- **Detailed request logging** for debugging

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;ğŸ›¡ï¸ Resilience &amp; High Availability&lt;/b&gt;&lt;/summary&gt;

- **Global timeout** with deadline-driven retries
- **Escalating cooldowns** per model (10s â†’ 30s â†’ 60s â†’ 120s)
- **Key-level lockouts** for consistently failing keys
- **Stream error detection** and graceful recovery
- **Batch embedding aggregation** for improved throughput
- **Automatic daily resets** for cooldowns and usage stats

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;ğŸ”‘ Credential Management&lt;/b&gt;&lt;/summary&gt;

- **Auto-discovery** of API keys from environment variables
- **OAuth discovery** from standard paths (`~/.gemini/`, `~/.qwen/`, `~/.iflow/`)
- **Duplicate detection** warns when same account added multiple times
- **Credential prioritization** â€” paid tier used before free tier
- **Stateless deployment** â€” export OAuth to environment variables
- **Local-first storage** â€” credentials isolated in `oauth_creds/` directory

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;âš™ï¸ Advanced Configuration&lt;/b&gt;&lt;/summary&gt;

- **Model whitelists/blacklists** with wildcard support
- **Per-provider concurrency limits** (`MAX_CONCURRENT_REQUESTS_PER_KEY_&lt;PROVIDER&gt;`)
- **Rotation modes** â€” balanced (distribute load) or sequential (use until exhausted)
- **Priority multipliers** â€” higher concurrency for paid credentials
- **Model quota groups** â€” shared cooldowns for related models
- **Temperature override** â€” prevent tool hallucination issues
- **Weighted random rotation** â€” unpredictable selection patterns

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;ğŸ”Œ Provider-Specific Features&lt;/b&gt;&lt;/summary&gt;

**Gemini CLI:**
- Zero-config Google Cloud project discovery
- Internal API access with higher rate limits
- Automatic fallback to preview models on rate limit
- Paid vs free tier detection

**Antigravity:**
- Gemini 3 Pro with `thinkingLevel` support
- Claude Opus 4.5 (thinking mode)
- Claude Sonnet 4.5 (thinking and non-thinking)
- Thought signature caching for multi-turn conversations
- Tool hallucination prevention

**Qwen Code:**
- Dual auth (API key + OAuth Device Flow)
- `&lt;think&gt;` tag parsing as `reasoning_content`
- Tool schema cleaning

**iFlow:**
- Dual auth (API key + OAuth Authorization Code)
- Hybrid auth with separate API key fetch
- Tool schema cleaning

**NVIDIA NIM:**
- Dynamic model discovery
- DeepSeek thinking support

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;ğŸ“ Logging &amp; Debugging&lt;/b&gt;&lt;/summary&gt;

- **Per-request file logging** with `--enable-request-logging`
- **Unique request directories** with full transaction details
- **Streaming chunk capture** for debugging
- **Performance metadata** (duration, tokens, model used)
- **Provider-specific logs** for Qwen, iFlow, Antigravity

&lt;/details&gt;

---

## Advanced Configuration

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Environment Variables Reference&lt;/b&gt;&lt;/summary&gt;

### Proxy Settings

| Variable | Description | Default |
|----------|-------------|---------|
| `PROXY_API_KEY` | Authentication key for your proxy | Required |
| `OAUTH_REFRESH_INTERVAL` | Token refresh check interval (seconds) | `600` |
| `SKIP_OAUTH_INIT_CHECK` | Skip interactive OAuth setup on startup | `false` |

### Per-Provider Settings

| Pattern | Description | Example |
|---------|-------------|---------|
| `&lt;PROVIDER&gt;_API_KEY_&lt;N&gt;` | API key for provider | `GEMINI_API_KEY_1` |
| `MAX_CONCURRENT_REQUESTS_PER_KEY_&lt;PROVIDER&gt;` | Concurrent request limit | `MAX_CONCURRENT_REQUESTS_PER_KEY_OPENAI=3` |
| `ROTATION_MODE_&lt;PROVIDER&gt;` | `balanced` or `sequential` | `ROTATION_MODE_GEMINI=sequential` |
| `IGNORE_MODELS_&lt;PROVIDER&gt;` | Blacklist (comma-separated, supports `*`) | `IGNORE_MODELS_OPENAI=*-preview*` |
| `WHITELIST_MODELS_&lt;PROVIDER&gt;` | Whitelist (overrides blacklist) | `WHITELIST_MODELS_GEMINI=gemini-2.5-pro` |

### Advanced Features

| Variable | Description |
|----------|-------------|
| `ROTATION_TOLERANCE` | `0.0`=deterministic, `3.0`=weighted random (default) |
| `CONCURRENCY_MULTIPLIER_&lt;PROVIDER&gt;_PRIORITY_&lt;N&gt;` | Concurrency multiplier per priority tier |
| `QUOTA_GROUPS_&lt;PROVIDER&gt;_&lt;GROUP&gt;` | Models sharing quota limits |
| `OVERRIDE_TEMPERATURE_ZERO` | `remove` or `set` to prevent tool hallucination |

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Model Filtering (Whitelists &amp; Blacklists)&lt;/b&gt;&lt;/summary&gt;

Control which models are exposed through your proxy.

### Blacklist Only
```env
# Hide all preview models
IGNORE_MODELS_OPENAI=&quot;*-preview*&quot;
```

### Pure Whitelist Mode
```env
# Block all, then allow specific models
IGNORE_MODELS_GEMINI=&quot;*&quot;
WHITELIST_MODELS_GEMINI=&quot;gemini-2.5-pro,gemini-2.5-flash&quot;
```

### Exemption Mode
```env
# Block preview models, but allow one specific preview
IGNORE_MODELS_OPENAI=&quot;*-preview*&quot;
WHITELIST_MODELS_OPENAI=&quot;gpt-4o-2024-08-06-preview&quot;
```

**Logic order:** Whitelist check â†’ Blacklist check â†’ Default allow

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Concurrency &amp; Rotation Settings&lt;/b&gt;&lt;/summary&gt;

### Concurrency Limits

```env
# Allow 3 concurrent requests per OpenAI key
MAX_CONCURRENT_REQUESTS_PER_KEY_OPENAI=3

# Default is 1 (no concurrency)
MAX_CONCURRENT_REQUESTS_PER_KEY_GEMINI=1
```

### Rotation Modes

```env
# balanced (default): Distribute load evenly - best for per-minute rate limits
ROTATION_MODE_OPENAI=balanced

# sequential: Use until exhausted - best for daily/weekly quotas
ROTATION_MODE_GEMINI=sequential
```

### Priority Multipliers

Paid credentials can handle more concurrent requests:

```env
# Priority 1 (paid ultra): 10x concurrency
CONCURRENCY_MULTIPLIER_ANTIGRAVITY_PRIORITY_1=10

# Priority 2 (standard paid): 3x
CONCURRENCY_MULTIPLIER_ANTIGRAVITY_PRIORITY_2=3
```

### Model Quota Groups

Models sharing quota limits:

```env
# Claude models share quota - when one hits limit, both cool down
QUOTA_GROUPS_ANTIGRAVITY_CLAUDE=&quot;claude-sonnet-4-5,claude-opus-4-5&quot;
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Timeout Configuration&lt;/b&gt;&lt;/summary&gt;

Fine-grained control over HTTP timeouts:

```env
TIMEOUT_CONNECT=30              # Connection establishment
TIMEOUT_WRITE=30                # Request body send
TIMEOUT_POOL=60                 # Connection pool acquisition
TIMEOUT_READ_STREAMING=180      # Between streaming chunks (3 min)
TIMEOUT_READ_NON_STREAMING=600  # Full response wait (10 min)
```

**Recommendations:**
- Long thinking tasks: Increase `TIMEOUT_READ_STREAMING` to 300-360s
- Unstable network: Increase `TIMEOUT_CONNECT` to 60s
- Large outputs: Increase `TIMEOUT_READ_NON_STREAMING` to 900s+

&lt;/details&gt;

---

## OAuth Providers

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Gemini CLI&lt;/b&gt;&lt;/summary&gt;

Uses Google OAuth to access internal Gemini endpoints with higher rate limits.

**Setup:**
1. Run `python -m rotator_library.credential_tool`
2. Select &quot;Add OAuth Credential&quot; â†’ &quot;Gemini CLI&quot;
3. Complete browser authentication
4. Credentials saved to `oauth_creds/gemini_cli_oauth_1.json`

**Features:**
- Zero-config project discovery
- Automatic free-tier project onboarding
- Paid vs free tier detection
- Smart fallback on rate limits

**Environment Variables (for stateless deployment):**
```env
GEMINI_CLI_ACCESS_TOKEN=&quot;ya29.your-access-token&quot;
GEMINI_CLI_REFRESH_TOKEN=&quot;1//your-refresh-token&quot;
GEMINI_CLI_EXPIRY_DATE=&quot;1234567890000&quot;
GEMINI_CLI_EMAIL=&quot;your-email@gmail.com&quot;
GEMINI_CLI_PROJECT_ID=&quot;your-gcp-project-id&quot;  # Optional
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Antigravity (Gemini 3 + Claude Opus 4.5)&lt;/b&gt;&lt;/summary&gt;

Access Google&#039;s internal Antigravity API for cutting-edge models.

**Supported Models:**
- **Gemini 3 Pro** â€” with `thinkingLevel` support (low/high)
- **Claude Opus 4.5** â€” Anthropic&#039;s most powerful model (thinking mode only)
- **Claude Sonnet 4.5** â€” supports both thinking and non-thinking modes
- Gemini 2.5 Pro/Flash

**Setup:**
1. Run `python -m rotator_library.credential_tool`
2. Select &quot;Add OAuth Credential&quot; â†’ &quot;Antigravity&quot;
3. Complete browser authentication

**Advanced Features:**
- Thought signature caching for multi-turn conversations
- Tool hallucination prevention via parameter signature injection
- Automatic thinking block sanitization for Claude
- Credential prioritization (paid resets every 5 hours, free weekly)

**Environment Variables:**
```env
ANTIGRAVITY_ACCESS_TOKEN=&quot;ya29.your-access-token&quot;
ANTIGRAVITY_REFRESH_TOKEN=&quot;1//your-refresh-token&quot;
ANTIGRAVITY_EXPIRY_DATE=&quot;1234567890000&quot;
ANTIGRAVITY_EMAIL=&quot;your-email@gmail.com&quot;

# Feature toggles
ANTIGRAVITY_ENABLE_SIGNATURE_CACHE=true
ANTIGRAVITY_GEMINI3_TOOL_FIX=true
```

&gt; **Note:** Gemini 3 models require a paid-tier Google Cloud project.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Qwen Code&lt;/b&gt;&lt;/summary&gt;

Uses OAuth Device Flow for Qwen/Dashscope APIs.

**Setup:**
1. Run the credential tool
2. Select &quot;Add OAuth Credential&quot; â†’ &quot;Qwen Code&quot;
3. Enter the code displayed in your browser
4. Or add API key directly: `QWEN_CODE_API_KEY_1=&quot;your-key&quot;`

**Features:**
- Dual auth (API key or OAuth)
- `&lt;think&gt;` tag parsing as `reasoning_content`
- Automatic tool schema cleaning
- Custom models via `QWEN_CODE_MODELS` env var

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;iFlow&lt;/b&gt;&lt;/summary&gt;

Uses OAuth Authorization Code flow with local callback server.

**Setup:**
1. Run the credential tool
2. Select &quot;Add OAuth Credential&quot; â†’ &quot;iFlow&quot;
3. Complete browser authentication (callback on port 11451)
4. Or add API key directly: `IFLOW_API_KEY_1=&quot;sk-your-key&quot;`

**Features:**
- Dual auth (API key or OAuth)
- Hybrid auth (OAuth token fetches separate API key)
- Automatic tool schema cleaning
- Custom models via `IFLOW_MODELS` env var

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Stateless Deployment (Export to Environment Variables)&lt;/b&gt;&lt;/summary&gt;

For platforms without file persistence (Railway, Render, Vercel):

1. **Set up credentials locally:**
   ```bash
   python -m rotator_library.credential_tool
   # Complete OAuth flows
   ```

2. **Export to environment variables:**
   ```bash
   python -m rotator_library.credential_tool
  

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[RVC-Boss/GPT-SoVITS]]></title>
            <link>https://github.com/RVC-Boss/GPT-SoVITS</link>
            <guid>https://github.com/RVC-Boss/GPT-SoVITS</guid>
            <pubDate>Mon, 15 Dec 2025 00:04:50 GMT</pubDate>
            <description><![CDATA[1 min voice data can also be used to train a good TTS model! (few shot voice cloning)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/RVC-Boss/GPT-SoVITS">RVC-Boss/GPT-SoVITS</a></h1>
            <p>1 min voice data can also be used to train a good TTS model! (few shot voice cloning)</p>
            <p>Language: Python</p>
            <p>Stars: 53,132</p>
            <p>Forks: 5,817</p>
            <p>Stars today: 93 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;h1&gt;GPT-SoVITS-WebUI&lt;/h1&gt;
A Powerful Few-shot Voice Conversion and Text-to-Speech WebUI.&lt;br&gt;&lt;br&gt;

[![madewithlove](https://img.shields.io/badge/made_with-%E2%9D%A4-red?style=for-the-badge&amp;labelColor=orange)](https://github.com/RVC-Boss/GPT-SoVITS)

&lt;a href=&quot;https://trendshift.io/repositories/7033&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/7033&quot; alt=&quot;RVC-Boss%2FGPT-SoVITS | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;!-- img src=&quot;https://counter.seku.su/cmoe?name=gptsovits&amp;theme=r34&quot; /&gt;&lt;br&gt; --&gt;

[![Python](https://img.shields.io/badge/python-3.10--3.12-blue?style=for-the-badge&amp;logo=python)](https://www.python.org)
[![GitHub release](https://img.shields.io/github/v/release/RVC-Boss/gpt-sovits?style=for-the-badge&amp;logo=github)](https://github.com/RVC-Boss/gpt-sovits/releases)

[![Train In Colab](https://img.shields.io/badge/Colab-Training-F9AB00?style=for-the-badge&amp;logo=googlecolab)](https://colab.research.google.com/github/RVC-Boss/GPT-SoVITS/blob/main/Colab-WebUI.ipynb)
[![Huggingface](https://img.shields.io/badge/å…è´¹åœ¨çº¿ä½“éªŒ-free_online_demo-yellow.svg?style=for-the-badge&amp;logo=huggingface)](https://lj1995-gpt-sovits-proplus.hf.space/)
[![Image Size](https://img.shields.io/docker/image-size/xxxxrt666/gpt-sovits/latest?style=for-the-badge&amp;logo=docker)](https://hub.docker.com/r/xxxxrt666/gpt-sovits)

[![ç®€ä½“ä¸­æ–‡](https://img.shields.io/badge/ç®€ä½“ä¸­æ–‡-é˜…è¯»æ–‡æ¡£-blue?style=for-the-badge&amp;logo=googledocs&amp;logoColor=white)](https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e)
[![English](https://img.shields.io/badge/English-Read%20Docs-blue?style=for-the-badge&amp;logo=googledocs&amp;logoColor=white)](https://rentry.co/GPT-SoVITS-guide#/)
[![Change Log](https://img.shields.io/badge/Change%20Log-View%20Updates-blue?style=for-the-badge&amp;logo=googledocs&amp;logoColor=white)](https://github.com/RVC-Boss/GPT-SoVITS/blob/main/docs/en/Changelog_EN.md)
[![License](https://img.shields.io/badge/LICENSE-MIT-green.svg?style=for-the-badge&amp;logo=opensourceinitiative)](https://github.com/RVC-Boss/GPT-SoVITS/blob/main/LICENSE)

**English** | [**ä¸­æ–‡ç®€ä½“**](./docs/cn/README.md) | [**æ—¥æœ¬èª**](./docs/ja/README.md) | [**í•œêµ­ì–´**](./docs/ko/README.md) | [**TÃ¼rkÃ§e**](./docs/tr/README.md)

&lt;/div&gt;

---

## Features:

1. **Zero-shot TTS:** Input a 5-second vocal sample and experience instant text-to-speech conversion.

2. **Few-shot TTS:** Fine-tune the model with just 1 minute of training data for improved voice similarity and realism.

3. **Cross-lingual Support:** Inference in languages different from the training dataset, currently supporting English, Japanese, Korean, Cantonese and Chinese.

4. **WebUI Tools:** Integrated tools include voice accompaniment separation, automatic training set segmentation, Chinese ASR, and text labeling, assisting beginners in creating training datasets and GPT/SoVITS models.

**Check out our [demo video](https://www.bilibili.com/video/BV12g4y1m7Uw) here!**

Unseen speakers few-shot fine-tuning demo:

https://github.com/RVC-Boss/GPT-SoVITS/assets/129054828/05bee1fa-bdd8-4d85-9350-80c060ab47fb

**RTF(inference speed) of GPT-SoVITS v2 ProPlus**:
0.028 tested in 4060Ti, 0.014 tested in 4090 (1400words~=4min, inference time is 3.36s), 0.526 in M4 CPU. You can test our [huggingface demo](https://lj1995-gpt-sovits-proplus.hf.space/) (half H200) to experience high-speed inference .

è¯·ä¸è¦å°¬é»‘GPT-SoVITSæ¨ç†é€Ÿåº¦æ…¢ï¼Œè°¢è°¢ï¼

**User guide: [ç®€ä½“ä¸­æ–‡](https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e) | [English](https://rentry.co/GPT-SoVITS-guide#/)**

## Installation

For users in China, you can [click here](https://www.codewithgpu.com/i/RVC-Boss/GPT-SoVITS/GPT-SoVITS-Official) to use AutoDL Cloud Docker to experience the full functionality online.

### Tested Environments

| Python Version | PyTorch Version  | Device        |
| -------------- | ---------------- | ------------- |
| Python 3.10    | PyTorch 2.5.1    | CUDA 12.4     |
| Python 3.11    | PyTorch 2.5.1    | CUDA 12.4     |
| Python 3.11    | PyTorch 2.7.0    | CUDA 12.8     |
| Python 3.9     | PyTorch 2.8.0dev | CUDA 12.8     |
| Python 3.9     | PyTorch 2.5.1    | Apple silicon |
| Python 3.11    | PyTorch 2.7.0    | Apple silicon |
| Python 3.9     | PyTorch 2.2.2    | CPU           |

### Windows

If you are a Windows user (tested with win&gt;=10), you can [download the integrated package](https://huggingface.co/lj1995/GPT-SoVITS-windows-package/resolve/main/GPT-SoVITS-v3lora-20250228.7z?download=true) and double-click on _go-webui.bat_ to start GPT-SoVITS-WebUI.

**Users in China can [download the package here](https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e/dkxgpiy9zb96hob4#KTvnO).**

Install the program by running the following commands:

```pwsh
conda create -n GPTSoVits python=3.10
conda activate GPTSoVits
pwsh -F install.ps1 --Device &lt;CU126|CU128|CPU&gt; --Source &lt;HF|HF-Mirror|ModelScope&gt; [--DownloadUVR5]
```

### Linux

```bash
conda create -n GPTSoVits python=3.10
conda activate GPTSoVits
bash install.sh --device &lt;CU126|CU128|ROCM|CPU&gt; --source &lt;HF|HF-Mirror|ModelScope&gt; [--download-uvr5]
```

### macOS

**Note: The models trained with GPUs on Macs result in significantly lower quality compared to those trained on other devices, so we are temporarily using CPUs instead.**

Install the program by running the following commands:

```bash
conda create -n GPTSoVits python=3.10
conda activate GPTSoVits
bash install.sh --device &lt;MPS|CPU&gt; --source &lt;HF|HF-Mirror|ModelScope&gt; [--download-uvr5]
```

### Install Manually

#### Install Dependences

```bash
conda create -n GPTSoVits python=3.10
conda activate GPTSoVits

pip install -r extra-req.txt --no-deps
pip install -r requirements.txt
```

#### Install FFmpeg

##### Conda Users

```bash
conda activate GPTSoVits
conda install ffmpeg
```

##### Ubuntu/Debian Users

```bash
sudo apt install ffmpeg
sudo apt install libsox-dev
```

##### Windows Users

Download and place [ffmpeg.exe](https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/ffmpeg.exe) and [ffprobe.exe](https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/ffprobe.exe) in the GPT-SoVITS root

Install [Visual Studio 2017](https://aka.ms/vs/17/release/vc_redist.x86.exe)

##### MacOS Users

```bash
brew install ffmpeg
```

### Running GPT-SoVITS with Docker

#### Docker Image Selection

Due to rapid development in the codebase and a slower Docker image release cycle, please:

- Check [Docker Hub](https://hub.docker.com/r/xxxxrt666/gpt-sovits) for the latest available image tags
- Choose an appropriate image tag for your environment
- `Lite` means the Docker image **does not include** ASR models and UVR5 models. You can manually download the UVR5 models, while the program will automatically download the ASR models as needed
- The appropriate architecture image (amd64/arm64) will be automatically pulled during Docker Compose
- Docker Compose will mount **all files** in the current directory. Please switch to the project root directory and **pull the latest code** before using the Docker image
- Optionally, build the image locally using the provided Dockerfile for the most up-to-date changes

#### Environment Variables

- `is_half`: Controls whether half-precision (fp16) is enabled. Set to `true` if your GPU supports it to reduce memory usage.

#### Shared Memory Configuration

On Windows (Docker Desktop), the default shared memory size is small and may cause unexpected behavior. Increase `shm_size` (e.g., to `16g`) in your Docker Compose file based on your available system memory.

#### Choosing a Service

The `docker-compose.yaml` defines two services:

- `GPT-SoVITS-CU126` &amp; `GPT-SoVITS-CU128`: Full version with all features.
- `GPT-SoVITS-CU126-Lite` &amp; `GPT-SoVITS-CU128-Lite`: Lightweight version with reduced dependencies and functionality.

To run a specific service with Docker Compose, use:

```bash
docker compose run --service-ports &lt;GPT-SoVITS-CU126-Lite|GPT-SoVITS-CU128-Lite|GPT-SoVITS-CU126|GPT-SoVITS-CU128&gt;
```

#### Building the Docker Image Locally

If you want to build the image yourself, use:

```bash
bash docker_build.sh --cuda &lt;12.6|12.8&gt; [--lite]
```

#### Accessing the Running Container (Bash Shell)

Once the container is running in the background, you can access it using:

```bash
docker exec -it &lt;GPT-SoVITS-CU126-Lite|GPT-SoVITS-CU128-Lite|GPT-SoVITS-CU126|GPT-SoVITS-CU128&gt; bash
```

## Pretrained Models

**If `install.sh` runs successfully, you may skip No.1,2,3**

**Users in China can [download all these models here](https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e/dkxgpiy9zb96hob4#nVNhX).**

1. Download pretrained models from [GPT-SoVITS Models](https://huggingface.co/lj1995/GPT-SoVITS) and place them in `GPT_SoVITS/pretrained_models`.

2. Download G2PW models from [G2PWModel.zip(HF)](https://huggingface.co/XXXXRT/GPT-SoVITS-Pretrained/resolve/main/G2PWModel.zip)| [G2PWModel.zip(ModelScope)](https://www.modelscope.cn/models/XXXXRT/GPT-SoVITS-Pretrained/resolve/master/G2PWModel.zip), unzip and rename to `G2PWModel`, and then place them in `GPT_SoVITS/text`.(Chinese TTS Only)

3. For UVR5 (Vocals/Accompaniment Separation &amp; Reverberation Removal, additionally), download models from [UVR5 Weights](https://huggingface.co/lj1995/VoiceConversionWebUI/tree/main/uvr5_weights) and place them in `tools/uvr5/uvr5_weights`.

   - If you want to use `bs_roformer` or `mel_band_roformer` models for UVR5, you can manually download the model and corresponding configuration file, and put them in `tools/uvr5/uvr5_weights`. **Rename the model file and configuration file, ensure that the model and configuration files have the same and corresponding names except for the suffix**. In addition, the model and configuration file names **must include `roformer`** in order to be recognized as models of the roformer class.

   - The suggestion is to **directly specify the model type** in the model name and configuration file name, such as `mel_mand_roformer`, `bs_roformer`. If not specified, the features will be compared from the configuration file to determine which type of model it is. For example, the model `bs_roformer_ep_368_sdr_12.9628.ckpt` and its corresponding configuration file `bs_roformer_ep_368_sdr_12.9628.yaml` are a pair, `kim_mel_band_roformer.ckpt` and `kim_mel_band_roformer.yaml` are also a pair.

4. For Chinese ASR (additionally), download models from [Damo ASR Model](https://modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/files), [Damo VAD Model](https://modelscope.cn/models/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch/files), and [Damo Punc Model](https://modelscope.cn/models/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/files) and place them in `tools/asr/models`.

5. For English or Japanese ASR (additionally), download models from [Faster Whisper Large V3](https://huggingface.co/Systran/faster-whisper-large-v3) and place them in `tools/asr/models`. Also, [other models](https://huggingface.co/Systran) may have the similar effect with smaller disk footprint.

## Dataset Format

The TTS annotation .list file format:

```

vocal_path|speaker_name|language|text

```

Language dictionary:

- &#039;zh&#039;: Chinese
- &#039;ja&#039;: Japanese
- &#039;en&#039;: English
- &#039;ko&#039;: Korean
- &#039;yue&#039;: Cantonese

Example:

```

D:\GPT-SoVITS\xxx/xxx.wav|xxx|en|I like playing Genshin.

```

## Finetune and inference

### Open WebUI

#### Integrated Package Users

Double-click `go-webui.bat`or use `go-webui.ps1`
if you want to switch to V1,then double-click`go-webui-v1.bat` or use `go-webui-v1.ps1`

#### Others

```bash
python webui.py &lt;language(optional)&gt;
```

if you want to switch to V1,then

```bash
python webui.py v1 &lt;language(optional)&gt;
```

Or maunally switch version in WebUI

### Finetune

#### Path Auto-filling is now supported

1. Fill in the audio path
2. Slice the audio into small chunks
3. Denoise(optinal)
4. ASR
5. Proofreading ASR transcriptions
6. Go to the next Tab, then finetune the model

### Open Inference WebUI

#### Integrated Package Users

Double-click `go-webui-v2.bat` or use `go-webui-v2.ps1` ,then open the inference webui at `1-GPT-SoVITS-TTS/1C-inference`

#### Others

```bash
python GPT_SoVITS/inference_webui.py &lt;language(optional)&gt;
```

OR

```bash
python webui.py
```

then open the inference webui at `1-GPT-SoVITS-TTS/1C-inference`

## V2 Release Notes

New Features:

1. Support Korean and Cantonese

2. An optimized text frontend

3. Pre-trained model extended from 2k hours to 5k hours

4. Improved synthesis quality for low-quality reference audio

   [more details](&lt;https://github.com/RVC-Boss/GPT-SoVITS/wiki/GPT%E2%80%90SoVITS%E2%80%90v2%E2%80%90features-(%E6%96%B0%E7%89%B9%E6%80%A7)&gt;)

Use v2 from v1 environment:

1. `pip install -r requirements.txt` to update some packages

2. Clone the latest codes from github.

3. Download v2 pretrained models from [huggingface](https://huggingface.co/lj1995/GPT-SoVITS/tree/main/gsv-v2final-pretrained) and put them into `GPT_SoVITS/pretrained_models/gsv-v2final-pretrained`.

   Chinese v2 additional: [G2PWModel.zip(HF)](https://huggingface.co/XXXXRT/GPT-SoVITS-Pretrained/resolve/main/G2PWModel.zip)| [G2PWModel.zip(ModelScope)](https://www.modelscope.cn/models/XXXXRT/GPT-SoVITS-Pretrained/resolve/master/G2PWModel.zip)(Download G2PW models, unzip and rename to `G2PWModel`, and then place them in `GPT_SoVITS/text`.)

## V3 Release Notes

New Features:

1. The timbre similarity is higher, requiring less training data to approximate the target speaker (the timbre similarity is significantly improved using the base model directly without fine-tuning).

2. GPT model is more stable, with fewer repetitions and omissions, and it is easier to generate speech with richer emotional expression.

   [more details](&lt;https://github.com/RVC-Boss/GPT-SoVITS/wiki/GPT%E2%80%90SoVITS%E2%80%90v3v4%E2%80%90features-(%E6%96%B0%E7%89%B9%E6%80%A7)&gt;)

Use v3 from v2 environment:

1. `pip install -r requirements.txt` to update some packages

2. Clone the latest codes from github.

3. Download v3 pretrained models (s1v3.ckpt, s2Gv3.pth and models--nvidia--bigvgan_v2_24khz_100band_256x folder) from [huggingface](https://huggingface.co/lj1995/GPT-SoVITS/tree/main) and put them into `GPT_SoVITS/pretrained_models`.

   additional: for Audio Super Resolution model, you can read [how to download](./tools/AP_BWE_main/24kto48k/readme.txt)

## V4 Release Notes

New Features:

1. Version 4 fixes the issue of metallic artifacts in Version 3 caused by non-integer multiple upsampling, and natively outputs 48k audio to prevent muffled sound (whereas Version 3 only natively outputs 24k audio). The author considers Version 4 a direct replacement for Version 3, though further testing is still needed.
   [more details](&lt;https://github.com/RVC-Boss/GPT-SoVITS/wiki/GPT%E2%80%90SoVITS%E2%80%90v3v4%E2%80%90features-(%E6%96%B0%E7%89%B9%E6%80%A7)&gt;)

Use v4 from v1/v2/v3 environment:

1. `pip install -r requirements.txt` to update some packages

2. Clone the latest codes from github.

3. Download v4 pretrained models (gsv-v4-pretrained/s2v4.ckpt, and gsv-v4-pretrained/vocoder.pth) from [huggingface](https://huggingface.co/lj1995/GPT-SoVITS/tree/main) and put them into `GPT_SoVITS/pretrained_models`.

## V2Pro Release Notes

New Features:

1. Slightly higher VRAM usage than v2, surpassing v4&#039;s performance, with v2&#039;s hardware cost and speed.
   [more details](&lt;https://github.com/RVC-Boss/GPT-SoVITS/wiki/GPT%E2%80%90SoVITS%E2%80%90features-(%E5%90%84%E7%89%88%E6%9C%AC%E7%89%B9%E6%80%A7)&gt;)

2.v1/v2 and the v2Pro series share the same characteristics, while v3/v4 have similar features. For training sets with average audio quality, v1/v2/v2Pro can deliver decent results, but v3/v4 cannot. Additionally, the synthesized tone and timebre of v3/v4 lean more toward the reference audio rather than the overall training set.

Use v2Pro from v1/v2/v3/v4 environment:

1. `pip install -r requirements.txt` to update some packages

2. Clone the latest codes from github.

3. Download v2Pro pretrained models (v2Pro/s2Dv2Pro.pth, v2Pro/s2Gv2Pro.pth, v2Pro/s2Dv2ProPlus.pth, v2Pro/s2Gv2ProPlus.pth, and sv/pretrained_eres2netv2w24s4ep4.ckpt) from [huggingface](https://huggingface.co/lj1995/GPT-SoVITS/tree/main) and put them into `GPT_SoVITS/pretrained_models`.

## Todo List

- [x] **High Priority:**

  - [x] Localization in Japanese and English.
  - [x] User guide.
  - [x] Japanese and English dataset fine tune training.

- [ ] **Features:**
  - [x] Zero-shot voice conversion (5s) / few-shot voice conversion (1min).
  - [x] TTS speaking speed control.
  - [ ] ~~Enhanced TTS emotion control.~~ Maybe use pretrained finetuned preset GPT models for better emotion.
  - [ ] Experiment with changing SoVITS token inputs to probability distribution of GPT vocabs (transformer latent).
  - [x] Improve English and Japanese text frontend.
  - [ ] Develop tiny and larger-sized TTS models.
  - [x] Colab scripts.
  - [x] Try expand training dataset (2k hours -&gt; 10k hours).
  - [x] better sovits base model (enhanced audio quality)
  - [ ] model mix

## (Additional) Method for running from the command line

Use the command line to open the WebUI for UVR5

```bash
python tools/uvr5/webui.py &quot;&lt;infer_device&gt;&quot; &lt;is_half&gt; &lt;webui_port_uvr5&gt;
```

&lt;!-- If you can&#039;t open a browser, follow the format below for UVR processing,This is using mdxnet for audio processing
```
python mdxnet.py --model --input_root --output_vocal --output_ins --agg_level --format --device --is_half_precision
``` --&gt;

This is how the audio segmentation of the dataset is done using the command line

```bash
python audio_slicer.py \
    --input_path &quot;&lt;path_to_original_audio_file_or_directory&gt;&quot; \
    --output_root &quot;&lt;directory_where_subdivided_audio_clips_will_be_saved&gt;&quot; \
    --threshold &lt;volume_threshold&gt; \
    --min_length &lt;minimum_duration_of_each_subclip&gt; \
    --min_interval &lt;shortest_time_gap_between_adjacent_subclips&gt;
    --hop_size &lt;step_size_for_computing_volume_curve&gt;
```

This is how dataset ASR processing is done using the command line(Only Chinese)

```bash
python tools/asr/funasr_asr.py -i &lt;input&gt; -o &lt;output&gt;
```

ASR processing is performed through Faster_Whisper(ASR marking except Chinese)

(No progress bars, GPU performance may cause time delays)

```bash
python ./tools/asr/fasterwhisper_asr.py -i &lt;input&gt; -o &lt;output&gt; -l &lt;language&gt; -p &lt;precision&gt;
```

A custom list save path is enabled

## Credits

Special thanks to the following projects and contributors:

### Theoretical Research

- [ar-vits](https://github.com/innnky/ar-vits)
- [SoundStorm](https://github.com/yangdongchao/SoundStorm/tree/master/soundstorm/s1/AR)
- [vits](https://github.com/jaywalnut310/vits)
- [TransferTTS](https://github.com/hcy71o/TransferTTS/blob/master/models.py#L556)
- [contentvec](https://github.com/auspicious3000/contentvec/)
- [hifi-gan](https://github.com/jik876/hifi-gan)
- [fish-speech](https://github.com/fishaudio/fish-speech/blob/main/tools/llama/generate.py#L41)
- [f5-TTS](https://github.com/SWivid/F5-TTS/blob/main/src/f5_tts/model/backbones/dit.py)
- [shortcut flow matching](https://github.com/kvfrans/shortcut-models/blob/main/targets_shortcut.py)

### Pretrained Models

- [Chinese Speech Pretrain](https://github.com/TencentGameMate/chinese_speech_pretrain)
- [Chinese-Roberta-WWM-Ext-Large](https://huggingface.co/hfl/chinese-roberta-wwm-ext-large)
- [BigVGAN](https://github.com/NVIDIA/BigVGAN)
- [eresnetv2](https://modelscope.cn/models/iic/speech_eres2netv2w24s4ep4_sv_zh-cn_16k-common)

### Text Frontend for Inference

- [paddlespeech zh_normalization](https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/paddlespeech/t2s/frontend/zh_normalization)
- [split-lang](https://github.com/DoodleBears/split-lang)
- [g2pW](https://github.com/GitYCC/g2pW)
- [pypinyin-g2pW](https://github.com/mozillazg/pypinyin-g2pW)
- [paddlespeech g2pw](https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/paddlespeech/t2s/frontend/g2pw)

#

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[leminlimez/Nugget]]></title>
            <link>https://github.com/leminlimez/Nugget</link>
            <guid>https://github.com/leminlimez/Nugget</guid>
            <pubDate>Mon, 15 Dec 2025 00:04:49 GMT</pubDate>
            <description><![CDATA[Unlock the fullest potential of your device]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/leminlimez/Nugget">leminlimez/Nugget</a></h1>
            <p>Unlock the fullest potential of your device</p>
            <p>Language: Python</p>
            <p>Stars: 4,932</p>
            <p>Forks: 285</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre>![Artboard][NuggetLogo]

# Nugget
Unlock your device&#039;s full potential!

Customize your device with animated wallpapers, disable pesky daemons, and more!

Make sure you have installed the [requirements](#requirements) if you are on Windows or Linux.

&gt; [!NOTE]
&gt; Please back up your data before using this Project! Nugget may cause unforeseen problems, so it is better to be safe than sorry. We are not responsible for any damage done to your device.

## Features
&lt;details&gt;
&lt;summary&gt;iOS 17.0 - 26.0+&lt;/summary&gt;

- PosterBoard: Animated wallpapers and descriptors.
  - Community wallpapers can be found [here][WallpapersWebsite]
  - Converting videos to wallpapers
  - Customizing community-made wallpapers via batter files
  - See documentation on the structure of tendies and batter files in [documentation.md](documentation.md)
- Templates: Custom Operations and file editing
  - See documentation on the structure of batter files in [documentation.md](documentation.md)
- Status Bar
  - Change carrier name
  - Change secondary carrier name
  - Enable/Disable the primary or secondary carriers
  - Change the number of WiFi/Cellular bars
  - Change the battery capacity
  - Change battery display detail
  - Change time text
  - Change date text (iPad only)
  - Change breadcrumb text
  - Show numeric WiFi/Cellular strength
  - Hide or show many icons in the status bar
- Springboard Options
  - Set Lock Screen Footnote
  - Set Lock Screen Idle Auto-Lock Time
  - Disable Lock After Respring
  - Disable Screen Dimming While Charging
  - Disable Low Battery Alerts
  - Hide AC Power on Lock Screen
  - Show Supervision Text on Lock Screen
  - Show Dynamic Island in Screenshots
  - Enable AirPlay support for Stage Manager
  - Show Red/Green Authentication Line on Lock Screen (See [this issue](https://github.com/leminlimez/Nugget/issues/656) for what it looks like)
  - Disable Floating Tab Bar on iPads
- Internal Options
  - Disable Liquid Glass (iOS 26.0+)
  - Ignore Liquid Glass App Build Check (iOS 26.0+)
  - Enabling Key Flick (iPad-style keyboard) on iPhones (iOS 26.0-)
  - Build Version in Status Bar
  - Force Right to Left
  - Show Hidden Icons on Home Screen
  - Force Metal HUD Debug
  - iMessage Diagnostics
  - IDS Diagnostics
  - VC Diagnostics
  - App Store Debug Gesture
  - Notes App Debug Mode
  - Show Touches With Debug Info
  - Hide Respring Icon
  - Play Sound on Paste
  - Show Notifications for System Pastes
- Disable Daemons:
  - OTAd
  - UsageTrackingAgent
  - Game Center
  - Screen Time Agent
  - Logs, Dumps, and Crash Reports
  - ATWAKEUP
  - Tipsd
  - VPN
  - Chinese WLAN service
  - HealthKit
  - AirPrint
  - Assistive Touch
  - iCloud
  - Internet Tethering (aka Personal Hotspot)
  - PassBook
  - Spotlight
  - Voice Control
- Risky (Hidden) Options:
  - Disable thermalmonitord
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;iOS 17.0 - 18.1.1&lt;/summary&gt;

- Enable Dynamic Island on any device
- Enable iPhone X gestures on iPhone SEs
- Change Device Model Name (ie what shows in the Settings app)
- Enable Boot Chime
- Enable Charge Limit
- Enable Tap to Wake on unsupported devices (ie iPhone SEs)
- Enable Collision SOS
- Enable Stage Manager
- Disable the Wallpaper Parallax
- Disable Region Restrictions (ie. Shutter Sound)
  - Note: This does not include enabling EU sideloading outside the EU. That will come later.
- Show the Apple Pencil options in Settings app
- Show the Action Button options in Settings app
- Show Internal Storage info (Might cause problems on some devices, use at your own risk)
- EU Enabler (iOS 17.6-)
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;iOS 18.0 - 18.0.1&lt;/summary&gt;

- Feature Flags (iOS 18.1b4-):
  - Enabling lock screen clock animation, lock screen page duplication button, and more!
  - Disabling the new iOS 18 Photos UI (iOS 18.0 betas only, unknown which patched it)
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;iOS 18.0 - 18.1.1&lt;/summary&gt;

- Enable iPhone 16 camera button page in the Settings app
- Enable AOD &amp; AOD Vibrancy on any device
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;iOS 18.1 - 18.1.1&lt;/summary&gt;

- AI Enabler
- Device Spoofing
&lt;/details&gt;

## Requirements:
&lt;details&gt;
&lt;summary&gt;Windows&lt;/summary&gt;
  
  - Either the [Apple Devices (from Microsoft Store)][AppleDevices] App or [iTunes (from Apple website)][iTunes]
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Linux&lt;/summary&gt;

  - [usbmuxd][usbmuxdGitHub]
  - [libimobiledevice][libimobiledeviceGitHub]
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;For Running Python&lt;/summary&gt;

  - [pymobiledevice3][pymobiledevice3GitHub]
  - [PySide6][PySide6Doc]
  - Python 3.8 or newer
&lt;/details&gt;

## Running the Python Program
&gt; [!NOTE]
&gt; It is highly recommended to use a virtual environment:
&gt; ```py
&gt; python3 -m venv .env # only needed once
&gt; ```
macOS/Linux:
```py
source .env/bin/activate
```
Windows:
```py
.env/Scripts/activate.bat
```
Install Packages:
```py
pip3 install -r requirements.txt # only needed once
python3 main_app.py
```
&gt; [!NOTE]
&gt; Depending on your system configuration, use either `python/pip` or `python3/pip3`.

## Getting the File
On iOS 26.1 and below, you may need to get the mobilegestalt file that is specific to your device. To do that, follow these steps:
1. Install the [Shortcuts][ShortcutsApp] app from the iOS app store.
2. Download this shortcut: [Save MobileGestalt][MobilegestaltShortcut]
3. Save the file and share it to your computer.
4. Place it in the same folder as the python file (or specify the path in the program)

## Building
To compile `mainwindow.ui` for Python, run the following command:
```py
pyside6-uic qt/mainwindow.ui -o qt/mainwindow_ui.py
```

To compile the resources file for Python, run the following command:
```py
pyside6-rcc resources.qrc -o resources_rc.py
```

To create and compile languages, you can use the following commands:
```py
pyside6-lupdate gui/main_window.py gui/pages/main/*.py gui/pages/tools/*.py gui/dialogs.py qt/mainwindow.ui devicemanagement/device_manager.py exceptions/*.py tweaks/*.py tweaks/posterboard/*.py tweaks/posterboard/template_options/*.py controllers/*.py -ts translations/Nugget_{language code}.ts # generate/update the language file
pyside6-lrelease translations/Nugget_{language code}.ts -qm translations/Nugget_{language code}.qm # compile to binary
```

The application itself can be compiled by running `compile.py`.

## Sparserestore/BookRestore Info
This uses the sparserestore exploit to write to files outside of the intended restore location, like mobilegestalt. Read the [Getting the File](#getting-the-file) section to learn how to get your mobilegestalt file.

Sparserestore works on all versions iOS 17.0-18.1.1.

BookRestore works on all versions iOS 18.2-26.1.

&gt; [!NOTE]
&gt; **Mobilegestalt and AI Enabler tweaks are not supported on iOS 26.2+.** It will never be supported, do not make issues asking for when it is supported.

## Read More
If you would like to read more about the inner workings of the exploit and iOS restore system, I made a write up which you can read [here][ReadMoreGist].

## Arbitrary Star Graph
&lt;a href=&quot;https://www.star-history.com/#leminlimez/Nugget&amp;Date&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=leminlimez/Nugget&amp;type=Date&amp;theme=dark&quot; /&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=leminlimez/Nugget&amp;type=Date&quot; /&gt;
    &lt;img alt=&quot;Star History&quot; src=&quot;https://api.star-history.com/svg?repos=leminlimez/Nugget&amp;type=Date&quot; /&gt;
  &lt;/picture&gt;
&lt;/a&gt;

## Credits
- Translations crowdsourced using [POEditor][POEditorJoin]. Thank you everyone who assisted in the translation effort!
- [JJTech][JJTechGitHub] for Sparserestore/[TrollRestore][TrollStoreGitHub]
- [Duy Tran][DuyGitHub] and [Huy Nguyen][HuyTwitter] for BookRestore/[bl_sbx][bl_sbxGitHub]
- [PosterRestore][PosterRestoreDiscord] for their help with PosterBoard
  - Special thanks to [dootskyre][dootskyreX], [Middo][MiddoX], [dulark][dularkGitHub], forcequitOS, and pingubow for their work on this. It would not have been possible without them!
  - Thanks to [Snoolie for aar handling][python-aar-stuffGitHub]!
  - Thanks to [SerStars][SerStarsX] for creating [the website][WallpapersWebsite]!
- [disfordottie][disfordottieX] for some global flag features
- [Mikasa-san][Mikasa-sanGitHub] for [Quiet Daemon][QuietDaemonGitHub]
- [sneakyf1shy][sneakyf1shyGitHub] for [AI Eligibility][AIEligibilityGist] (iOS 18.1 beta 4 and below)
- [lrdsnow][lrdsnowGitHub] for [EU Enabler][EUEnablerGitHub]
- [pymobiledevice3][pymobiledevice3GitHub] for restoring and device algorithms.
- [PySide6][PySide6Doc] for the GUI library.

[NuggetLogo]: https://raw.githubusercontent.com/leminlimez/Nugget/refs/heads/main/credits/small_nugget.png
[CowabungaLite]: https://github.com/leminlimez/CowabungaLite
[WallpapersWebsite]: https://cowabun.ga/wallpapers
[AppleDevices]: https://apps.microsoft.com/detail/9np83lwlpz9k
[iTunes]: https://support.apple.com/en-us/106372
[usbmuxdGitHub]: https://github.com/libimobiledevice/usbmuxd
[libimobiledeviceGitHub]: https://github.com/libimobiledevice/libimobiledevice
[ShortcutsApp]: https://apps.apple.com/us/app/shortcuts/id915249334
[MobilegestaltShortcut]: https://www.icloud.com/shortcuts/66bd3c822a0145b98d46cd1c9077e6e5
[ReadMoreGist]: https://gist.github.com/leminlimez/c602c067349140fe979410ef69d39c28

[POEditorJoin]: https://poeditor.com/join/project/UTqpVSE2UD
[JJTechGitHub]: https://github.com/JJTech0130
[TrollStoreGitHub]: https://github.com/JJTech0130/TrollRestore
[PosterRestoreDiscord]: https://discord.gg/gWtzTVhMvh
[dootskyreX]: https://x.com/dootskyre
[MiddoX]: https://x.com/MWRevamped
[dularkGitHub]: https://github.com/dularkian
[SerStarsX]: https://x.com/SerStars_lol
[disfordottieX]: https://x.com/disfordottie
[Mikasa-sanGitHub]: https://github.com/Mikasa-san
[QuietDaemonGitHub]: https://github.com/Mikasa-san/QuietDaemon
[sneakyf1shyGitHub]: https://github.com/f1shy-dev
[lrdsnowGitHub]: https://github.com/Lrdsnow
[EUEnablerGitHub]: https://github.com/Lrdsnow/EUEnabler
[pymobiledevice3GitHub]: https://github.com/doronz88/pymobiledevice3
[PySide6Doc]: https://doc.qt.io/qtforpython-6/
[python-aar-stuffGitHub]: https://github.com/0xilis/python-aar-stuff
[AIEligibilityGist]: https://gist.github.com/f1shy-dev/23b4a78dc283edd30ae2b2e6429129b5
[bl_sbxGitHub]: https://github.com/khanhduytran0/bl_sbx/tree/main
[DuyGitHub]: https://github.com/khanhduytran0
[HuyTwitter]: https://x.com/Little_34306
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[dabeaz-course/python-mastery]]></title>
            <link>https://github.com/dabeaz-course/python-mastery</link>
            <guid>https://github.com/dabeaz-course/python-mastery</guid>
            <pubDate>Mon, 15 Dec 2025 00:04:48 GMT</pubDate>
            <description><![CDATA[Advanced Python Mastery (course by @dabeaz)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/dabeaz-course/python-mastery">dabeaz-course/python-mastery</a></h1>
            <p>Advanced Python Mastery (course by @dabeaz)</p>
            <p>Language: Python</p>
            <p>Stars: 12,774</p>
            <p>Forks: 2,151</p>
            <p>Stars today: 192 stars today</p>
            <h2>README</h2><pre># Advanced Python Mastery

A course by David Beazley (https://www.dabeaz.com)  
Copyright (C) 2007-2024  

## Synopsis

An exercise-driven course on Advanced Python Programming that was
battle-tested several hundred times on the corporate-training circuit
for more than a decade.  Written by David Beazley, author of the
[Python Cookbook, 3rd Edition](https://www.dabeaz.com/cookbook.html) (O&#039;Reilly) and 
[Python Distilled](https://www.dabeaz.com/python-distilled/index.html)
(Addison-Wesley).  Released under a Creative Commons license.  Free of
ads, tracking, pop-ups, newsletters, and AI.

Everything in this course should work with the latest version of
Python, but be aware that the course primarily targets the feature set
of Python 3.6.  As such, certain modern features don&#039;t get coverage. 
Honestly, this shouldn&#039;t affect you much unless you&#039;re trying to write code
that&#039;s freakishly clever.

## Target Audience 

This course is for Python programmers who want to move beyond 
short scripts to writing more sophisticated programs.    To do that,
it helps to better understand the programming techniques used
in popular libraries and frameworks. Thus, this course is mainly 
for programmers who want to build a more complete mental model of the
Python language itself and how it works.  Ultimately, the goal
is to be able to apply this knowledge to your own projects.

## Prerequisites

You already know some Python.  This is not a course for beginners.
For more introductory material, you might consider the
[Practical Python Programming](https://dabeaz-course.github.io/practical-python) course.

## How to Take the Course

To take the course, you should first fork/clone the GitHub repo to your own
machine.

It is assumed that you are working locally in a proper Python
development environment.  That means a proper installation of Python,
an editor/IDE, and whatever other tools that you would normally
install to work on Python.  Due to the use of multiple files and
module imports, the use of Notebooks is not recommended.

The [`PythonMastery.pdf`](PythonMastery.pdf) file contains detailed
presentation slides. Course exercises and suggested timings are
clearly indicated. You&#039;ll want to keep this by your side (I recommend
downloading and viewing it with a local PDF viewer). Start here! 

The [Exercises/](Exercises/index.md) directory has all of the
course exercises. 

The [Solutions/](Solutions/) directory has fully worked out solution code.

The [Data/](Data/) directory has some datafiles used during the course.

The course was originally taught over 4-5 days in an in-person
classroom setting with a mix of lecture and hands-on exercises.
Successful completion of the course will likely require 30-50 hours of
work.  Exercises tend to build upon each other.  Solutions are always
provided in case you get stuck.

## Supplemental Material

The Advanced Python Mastery course often suggested more in-depth tutorials
on selected topics.  These were presented at the PyCon conference and
might be of interest:

* [Generator Tricks for Systems Programmers](https://www.dabeaz.com/generators/)
* [A Curious Course on Coroutines and Concurrency](http://dabeaz.com/coroutines/index.html)
* [Python3 Metaprogramming](https://dabeaz.com/py3meta/index.html)
* [Generators: The Final Frontier](https://dabeaz.com/finalgenerator/index.html)
* [Modules and Packages: Live and Let Die](https://dabeaz.com/modulepackage/index.html)

## Questions and Answers

**Q: Are any videos available?**

**A:** No. You will be able to more quickly read the presentation slides which contain
technical information.  However, the [Python Programming Language: LiveLessons](https://www.safaribooksonline.com/library/view/python-programming-language/9780134217314/) video
available on O&#039;Reilly&#039;s Safari site is closely related to the material in this course.

**Q: Can I use these materials in my own course?**

**A:** Yes. I just kindly ask that you give proper attribution.

**Q: Do you accept bug reports or pull requests?**

**A:** If you&#039;ve found a bug, please report it!  However, I&#039;m not
looking to expand or reorganize the course content with new topics or
exercises.

**Q: Are the presentation slides available in any format other than PDF?**

**A:** No.

**Q: Is there any forum/chat where the course can be discussed?**

**A:** You can use [GitHub discussions](https://github.com/dabeaz-course/python-mastery/discussions) to discuss the course.

**Q: Why wasn&#039;t topic/tool/library X covered?**

**A:** The course was designed to be completed in an intense 4-day
in-person format. It simply isn&#039;t possible to cover absolutely
everything.  As such, the course is focused primarily on the core
Python language, not third party libraries or tooling.

**Q: Why aren&#039;t features like typing, async, or pattern matching covered?**

**A:** Mainly, it&#039;s an issue of calendar timing and scope.  Course
material was primarily developed pre-pandemic and represents Python as
it was at that time. Some topics (e.g., typing or async) are
sufficiently complex that they would be better covered on their own
in a separate course.

**Q: Do you have plans to modernize the course?**

**A:** It is my intention that everything in the course apply to the
latest version of Python.  Unless Python makes backwards-incompatible
changes to the core language, that should hold.  Although the course
doesn&#039;t cover every new features, I won&#039;t rule out future changes.  A
lot depends on my available time and interest however.  So, I make no
promises.

**Q: Why did you release the course?**

**A:** This course was extensively taught pre-pandemic. Post-pandemic,
my teaching has shifted towards projects and CS fundamentals.
However, why let a good course just languish on my computer? 

**Q: How can I help?**

**A:** If you like the course, the best way to support it is to tell
other people about it.

----
`&gt;&gt;&gt;` Advanced Python Mastery  
`...` A course by [dabeaz](https://www.dabeaz.com)  
`...` Copyright 2007-2023  

![](https://i.creativecommons.org/l/by-sa/4.0/88x31.png). This work is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-sa/4.0/)






</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[rendercv/rendercv]]></title>
            <link>https://github.com/rendercv/rendercv</link>
            <guid>https://github.com/rendercv/rendercv</guid>
            <pubDate>Mon, 15 Dec 2025 00:04:47 GMT</pubDate>
            <description><![CDATA[Typst-based CV/resume generator for academics and engineers]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/rendercv/rendercv">rendercv/rendercv</a></h1>
            <p>Typst-based CV/resume generator for academics and engineers</p>
            <p>Language: Python</p>
            <p>Stars: 3,770</p>
            <p>Forks: 346</p>
            <p>Stars today: 379 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;h1&gt;RenderCV&lt;/h1&gt;

_CV/resume generator for academics and engineers_

[![test](https://github.com/rendercv/rendercv/actions/workflows/test.yaml/badge.svg?branch=main)](https://github.com/rendercv/rendercv/actions/workflows/test.yaml)
[![coverage](https://coverage-badge.samuelcolvin.workers.dev/rendercv/rendercv.svg)](https://coverage-badge.samuelcolvin.workers.dev/redirect/rendercv/rendercv)
[![docs](&lt;https://img.shields.io/badge/docs-mkdocs-rgb(0%2C79%2C144)&gt;)](https://docs.rendercv.com)
[![pypi-version](&lt;https://img.shields.io/pypi/v/rendercv?label=PyPI%20version&amp;color=rgb(0%2C79%2C144)&gt;)](https://pypi.python.org/pypi/rendercv)
[![pypi-downloads](&lt;https://img.shields.io/pepy/dt/rendercv?label=PyPI%20downloads&amp;color=rgb(0%2C%2079%2C%20144)&gt;)](https://pypistats.org/packages/rendercv)

&lt;/div&gt;

Write your CV or resume as YAML, then run RenderCV,

```bash
rendercv render John_Doe_CV.yaml
```

and get a PDF with perfect typography. No template wrestling. No broken layouts. Consistent spacing, every time.

With RenderCV, you can:

- Version-control your CV â€” it&#039;s just text.
- Focus on content â€” don&#039;t wory about the formatting.
- Get perfect typography â€” pixel-perfect alignment and spacing, handled for you.

A YAML file like this:

```yaml
cv:
  name: John Doe
  location: San Francisco, CA
  email: john.doe@email.com
  website: https://rendercv.com/
  social_networks:
    - network: LinkedIn
      username: rendercv
    - network: GitHub
      username: rendercv
  sections:
    Welcome to RenderCV:
      - RenderCV reads a CV written in a YAML file, and generates a PDF with professional typography.
      - See the [documentation](https://docs.rendercv.com) for more details.
    education:
      - institution: Princeton University
        area: Computer Science
        degree: PhD
        date:
        start_date: 2018-09
        end_date: 2023-05
        location: Princeton, NJ
        summary:
        highlights:
          - &quot;Thesis: Efficient Neural Architecture Search for Resource-Constrained Deployment&quot;
          - &quot;Advisor: Prof. Sanjeev Arora&quot;
          - NSF Graduate Research Fellowship, Siebel Scholar (Class of 2022)
    ...
```

becomes one of these PDFs. Click on the images to preview.

| [![Classic Theme Example of RenderCV](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/classic.png)](https://github.com/rendercv/rendercv/blob/main/examples/John_Doe_ClassicTheme_CV.pdf)    | [![Engineeringresumes Theme Example of RenderCV](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/engineeringresumes.png)](https://github.com/rendercv/rendercv/blob/main/examples/John_Doe_EngineeringresumesTheme_CV.pdf) | [![Sb2nov Theme Example of RenderCV](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/sb2nov.png)](https://github.com/rendercv/rendercv/blob/main/examples/John_Doe_Sb2novTheme_CV.pdf) |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| [![Moderncv Theme Example of RenderCV](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/moderncv.png)](https://github.com/rendercv/rendercv/blob/main/examples/John_Doe_ModerncvTheme_CV.pdf) | [![Engineeringclassic Theme Example of RenderCV](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/engineeringclassic.png)](https://github.com/rendercv/rendercv/blob/main/examples/John_Doe_EngineeringclassicTheme_CV.pdf) | ![Custom themes can be added.](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/customtheme.png)                                                                                        |


## JSON Schema

RenderCV&#039;s JSON Schema lets you fill out the YAML interactively, with autocompletion and inline documentation.

![JSON Schema of RenderCV](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/json_schema.gif)


## Extensive Design Options

You have full control over every detail.

```yaml
design:
  theme: classic
  page:
    size: us-letter
    top_margin: 0.7in
    bottom_margin: 0.7in
    left_margin: 0.7in
    right_margin: 0.7in
    show_footer: true
    show_top_note: true
  colors:
    body: rgb(0, 0, 0)
    name: rgb(0, 79, 144)
    headline: rgb(0, 79, 144)
    connections: rgb(0, 79, 144)
    section_titles: rgb(0, 79, 144)
    links: rgb(0, 79, 144)
    footer: rgb(128, 128, 128)
    top_note: rgb(128, 128, 128)
  typography:
    line_spacing: 0.6em
    alignment: justified
    date_and_location_column_alignment: right
    font_family: Source Sans 3
  # ...and much more
```

![Design Options of RenderCV](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/design_options.gif)

&gt; [!TIP]
&gt; Want to set up a live preview environment like the one shown above? See [how to set up VS Code for RenderCV](https://docs.rendercv.com/user_guide/how_to/set_up_vs_code_for_rendercv).

## Strict Validation

No surprises. If something&#039;s wrong, you&#039;ll know exactly what and where. If it&#039;s valid, you get a perfect PDF.

![Strict Validation Feature of RenderCV](https://raw.githubusercontent.com/rendercv/rendercv/main/docs/assets/images/validation.gif)


## Any Language

Fill out the locale field for your language.

```yaml
locale:
  language: english
  last_updated: Last updated in
  month: month
  months: months
  year: year
  years: years
  present: present
  month_abbreviations:
    - Jan
    - Feb
    - Mar
  ...
```

## Get Started

Install RenderCV (Requires Python 3.12+):

```
pip install &quot;rendercv[full]&quot;
```

Create a new CV yaml file:

```
rendercv new &quot;John Doe&quot;
```

Edit the YAML, then render:

```
rendercv render &quot;John_Doe_CV.yaml&quot;
```

For more details, see the [user guide](https://docs.rendercv.com/user_guide/).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[opengeos/geoai]]></title>
            <link>https://github.com/opengeos/geoai</link>
            <guid>https://github.com/opengeos/geoai</guid>
            <pubDate>Mon, 15 Dec 2025 00:04:46 GMT</pubDate>
            <description><![CDATA[GeoAI: Artificial Intelligence for Geospatial Data]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/opengeos/geoai">opengeos/geoai</a></h1>
            <p>GeoAI: Artificial Intelligence for Geospatial Data</p>
            <p>Language: Python</p>
            <p>Stars: 2,129</p>
            <p>Forks: 295</p>
            <p>Stars today: 39 stars today</p>
            <h2>README</h2><pre># GeoAI: Artificial Intelligence for Geospatial Data

[![image](https://img.shields.io/pypi/v/geoai-py.svg)](https://pypi.python.org/pypi/geoai-py)
[![image](https://static.pepy.tech/badge/geoai-py)](https://pepy.tech/project/geoai-py)
[![image](https://img.shields.io/conda/vn/conda-forge/geoai.svg)](https://anaconda.org/conda-forge/geoai)
[![Conda Downloads](https://img.shields.io/conda/dn/conda-forge/geoai.svg)](https://anaconda.org/conda-forge/geoai)
[![Conda Recipe](https://img.shields.io/badge/recipe-geoai-green.svg)](https://github.com/conda-forge/geoai-py-feedstock)
[![image](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![image](https://img.shields.io/badge/YouTube-Tutorials-red)](https://tinyurl.com/GeoAI-Tutorials)
[![QGIS](https://img.shields.io/badge/QGIS-plugin-orange.svg)](https://opengeoai.org/qgis_plugin)

[![logo](https://raw.githubusercontent.com/opengeos/geoai/master/docs/assets/logo_rect.png)](https://github.com/opengeos/geoai/blob/master/docs/assets/logo.png)

**A powerful Python package for integrating artificial intelligence with geospatial data analysis and visualization**

## ğŸ“– Introduction

[GeoAI](https://opengeoai.org) is a comprehensive Python package designed to bridge artificial intelligence (AI) and geospatial data analysis, providing researchers and practitioners with intuitive tools for applying machine learning techniques to geographic data. The package offers a unified framework for processing satellite imagery, aerial photographs, and vector data using state-of-the-art deep learning models. GeoAI integrates popular AI frameworks including [PyTorch](https://pytorch.org), [Transformers](https://github.com/huggingface/transformers), [PyTorch Segmentation Models](https://github.com/qubvel-org/segmentation_models.pytorch), and specialized geospatial libraries like [torchange](https://github.com/Z-Zheng/pytorch-change-models), enabling users to perform complex geospatial analyses with minimal code.

The package provides five core capabilities:

1. Interactive and programmatic search and download of remote sensing imagery and geospatial data.
2. Automated dataset preparation with image chips and label generation.
3. Model training for tasks such as classification, detection, and segmentation.
4. Inference pipelines for applying models to new geospatial datasets.
5. Interactive visualization through integration with [Leafmap](https://github.com/opengeos/leafmap/) and [MapLibre](https://github.com/eoda-dev/py-maplibregl).

GeoAI addresses the growing demand for accessible AI tools in geospatial research by providing high-level APIs that abstract complex machine learning workflows while maintaining flexibility for advanced users. The package supports multiple data formats (GeoTIFF, JPEG2000,GeoJSON, Shapefile, GeoPackage) and includes automatic device management for GPU acceleration when available. With over 10 modules and extensive notebook examples, GeoAI serves as both a research tool and educational resource for the geospatial AI community.

## ğŸ“ Statement of Need

The integration of artificial intelligence with geospatial data analysis has become increasingly critical across numerous scientific disciplines, from environmental monitoring and urban planning to disaster response and climate research. However, applying AI techniques to geospatial data presents unique challenges including data preprocessing complexities, specialized model architectures, and the need for domain-specific knowledge in both machine learning and geographic information systems.

Existing solutions often require researchers to navigate fragmented ecosystems of tools, combining general-purpose machine learning libraries with specialized geospatial packages, leading to steep learning curves and reproducibility challenges. While packages like TorchGeo and TerraTorch provide excellent foundational tools for geospatial deep learning, there remains a gap for comprehensive, high-level interfaces that can democratize access to advanced AI techniques for the broader geospatial community.

GeoAI addresses this need by providing a unified, user-friendly interface that abstracts the complexity of integrating multiple AI frameworks with geospatial data processing workflows. It lowers barriers for: (1) geospatial researchers who need accessible AI workflows without deep ML expertise; (2) AI practitioners who want streamlined geospatial preprocessing and domain-specific datasets; and (3) educators seeking reproducible examples and teaching-ready workflows.

The package&#039;s design philosophy emphasizes simplicity without sacrificing functionality, enabling users to perform sophisticated analyses such as building footprint extraction from satellite imagery, land cover classification, and change detection with just a few lines of code. By integrating cutting-edge AI models and providing seamless access to major geospatial data sources, GeoAI significantly lowers the barrier to entry for geospatial AI applications while maintaining the flexibility needed for advanced research applications.

## Citations

If you find GeoAI useful in your research, please consider citing the following paper to support my work. Thank you for your support.

-   Wu, Q. (2025). GeoAI: A Python package for integrating artificial intelligence with geospatial data analysis and visualization. _Journal of Open Source Software_, 9025. [https://doi.org/10.21105/joss.09025](https://github.com/openjournals/joss-papers/blob/joss.09605/joss.09605/10.21105.joss.09605.pdf) (Under Review)

## ğŸš€ Key Features

### ğŸ“Š Advanced Geospatial Data Visualization

-   Interactive multi-layer visualization of vector and raster data stored locally or in cloud storage
-   Customizable styling and symbology
-   Time-series data visualization capabilities

### ğŸ› ï¸ Data Preparation &amp; Processing

-   Streamlined access to satellite and aerial imagery from providers like Sentinel, Landsat, NAIP, and other open datasets
-   Tools for downloading, mosaicking, and preprocessing remote sensing data
-   Automated generation of training datasets with image chips and corresponding labels
-   Vector-to-raster and raster-to-vector conversion utilities optimized for AI workflows
-   Data augmentation techniques specific to geospatial data
-   Support for integrating Overture Maps data and other open datasets for training and validation

### ğŸ–¼ï¸ Image Segmentation

-   Integration with [PyTorch Segmentation Models](https://github.com/qubvel-org/segmentation_models.pytorch) for automatic feature extraction
-   Specialized segmentation algorithms optimized for satellite and aerial imagery
-   Streamlined workflows for segmenting buildings, water bodies, wetlands,solar panels, etc.
-   Export capabilities to standard geospatial formats (GeoJSON, Shapefile, GeoPackage, GeoParquet)

### ğŸ” Image Classification

-   Pre-trained models for land cover and land use classification
-   Transfer learning utilities for fine-tuning models with your own data
-   Multi-temporal classification support for change detection
-   Accuracy assessment and validation tools

### ğŸŒ Additional Capabilities

-   Change detection with AI-enhanced feature extraction
-   Object detection in aerial and satellite imagery
-   Georeferencing utilities for AI model outputs

## ğŸ“¦ Installation

### Using pip

```bash
pip install geoai-py
```

### Using conda

```bash
conda install -c conda-forge geoai
```

### Using mamba

```bash
mamba install -c conda-forge geoai
```

## âš™ï¸ QGIS Plugin

Check out the [QGIS Plugin](https://opengeoai.org/qgis_plugin/) page if you are interested in using GeoAI with QGIS.

[![demo](https://github.com/user-attachments/assets/5aabc3d3-efd1-4011-ab31-2b3f11aab3ed)](https://youtu.be/8-OhlqeoyiY)

## ğŸ“‹ Documentation

Comprehensive documentation is available at [https://opengeoai.org](https://opengeoai.org), including:

-   Detailed API reference
-   Tutorials and example notebooks
-   Contributing guide

## ğŸ“ºÂ Video Tutorials

### GeoAI Made Easy: Learn the Python Package Step-by-Step (Beginner Friendly)

[![intro](https://github.com/user-attachments/assets/7e60ce05-573d-4d0d-9876-5289b87e5136)](https://youtu.be/VIl29Rca6zE&amp;list=PLAxJ4-o7ZoPcvENqwaPa_QwbbkZ5sctZE)

### GeoAI Workshop: Unlocking the Power of GeoAI with Python

[![cover](https://github.com/user-attachments/assets/1c14e651-65b9-41ae-b42d-3ad028b3eeb8)](https://youtu.be/jdK-cleFUkc&amp;list=PLAxJ4-o7ZoPcvENqwaPa_QwbbkZ5sctZE)

### GeoAI Tutorials Playlist

[![cover](https://github.com/user-attachments/assets/3cde9547-ab62-4d70-b23a-3e5ed27c7407)](https://www.youtube.com/playlist?list=PLAxJ4-o7ZoPcvENqwaPa_QwbbkZ5sctZE)

## ğŸ¤ Contributing

We welcome contributions of all kinds! See our [contributing guide](https://opengeoai.org/contributing) for ways to get started.

## ğŸ“„ License

GeoAI is free and open source software, licensed under the MIT License.

## Acknowledgments

We gratefully acknowledge the support of the following organizations:

-   [NASA](https://www.nasa.gov): This research is partially supported by the National Aeronautics and Space Administration (NASA) through Grant No. 80NSSC22K1742, awarded under the [Open Source Tools, Frameworks, and Libraries Program](https://bit.ly/3RVBRcQ).
-   [AmericaView](https://americaview.org): This work is also partially supported by the U.S. Geological Survey through Grant/Cooperative Agreement No. G23AP00683 (GY23-GY27) in collaboration with AmericaView.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[neuraloperator/neuraloperator]]></title>
            <link>https://github.com/neuraloperator/neuraloperator</link>
            <guid>https://github.com/neuraloperator/neuraloperator</guid>
            <pubDate>Mon, 15 Dec 2025 00:04:45 GMT</pubDate>
            <description><![CDATA[Learning in infinite dimension with neural operators.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/neuraloperator/neuraloperator">neuraloperator/neuraloperator</a></h1>
            <p>Learning in infinite dimension with neural operators.</p>
            <p>Language: Python</p>
            <p>Stars: 3,216</p>
            <p>Forks: 791</p>
            <p>Stars today: 21 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[huridocs/pdf-document-layout-analysis]]></title>
            <link>https://github.com/huridocs/pdf-document-layout-analysis</link>
            <guid>https://github.com/huridocs/pdf-document-layout-analysis</guid>
            <pubDate>Mon, 15 Dec 2025 00:04:44 GMT</pubDate>
            <description><![CDATA[A Docker-powered service for PDF document layout analysis. This service provides a powerful and flexible PDF analysis service. The service allows for the segmentation and classification of different parts of PDF pages, identifying the elements such as texts, titles, pictures, tables and so on.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/huridocs/pdf-document-layout-analysis">huridocs/pdf-document-layout-analysis</a></h1>
            <p>A Docker-powered service for PDF document layout analysis. This service provides a powerful and flexible PDF analysis service. The service allows for the segmentation and classification of different parts of PDF pages, identifying the elements such as texts, titles, pictures, tables and so on.</p>
            <p>Language: Python</p>
            <p>Stars: 840</p>
            <p>Forks: 97</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;PDF Document Layout Analysis&lt;/h1&gt;
&lt;p align=&quot;center&quot;&gt;A Docker-powered microservice for intelligent PDF document layout analysis, OCR, and content extraction&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Python-3.10+-blue.svg&quot; alt=&quot;Python Version&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/FastAPI-0.111.1-green.svg&quot; alt=&quot;FastAPI&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Docker-Ready-blue.svg&quot; alt=&quot;Docker&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/GPU-Supported-orange.svg&quot; alt=&quot;GPU Support&quot;&gt;
&lt;/p&gt;


&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Built with â¤ï¸ by &lt;a href=&quot;https://huridocs.org&quot;&gt;HURIDOCS&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;p&gt;
    &lt;a href=&quot;https://github.com/huridocs/pdf-document-layout-analysis&quot;&gt;â­ Star us on GitHub&lt;/a&gt; â€¢
    &lt;a href=&quot;https://hub.docker.com/r/huridocs/pdf-document-layout-analysis&quot;&gt;ğŸ³ Pull from Docker Hub&lt;/a&gt; â€¢
    &lt;a href=&quot;https://huggingface.co/HURIDOCS/pdf-document-layout-analysis&quot;&gt;ğŸ¤— View on Hugging Face&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;



---

## ğŸš€ Overview

This project provides a powerful and flexible PDF analysis microservice built with **Clean Architecture** principles. The service enables OCR, segmentation, and classification of different parts of PDF pages, identifying elements such as texts, titles, pictures, tables, formulas, and more. Additionally, it determines the correct reading order of these identified elements and can convert PDFs to various formats including Markdown and HTML with **automatic translation support** powered by Ollama.

The service offers both a **user-friendly Gradio web interface** for interactive use and a **comprehensive REST API** for programmatic access and integration.

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/huridocs/pdf-document-layout-analysis/main/images/ui.png&quot; alt=&quot;Gradio Web UI&quot; width=&quot;800&quot;/&gt;
  &lt;p&gt;&lt;em&gt;Gradio Web Interface - Easy-to-use UI for PDF analysis, conversion, and translation&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;

---

## ğŸš€ Quick Start

### 1. Start the Service

```bash
make start # or `just start` (https://github.com/casey/just)
```

The service provides two interfaces:
- **ğŸ¨ Web UI (Gradio)**: `http://localhost:7860` - User-friendly interface for all features
- **ğŸ”Œ REST API**: `http://localhost:5060` - Programmatic access for integrations

**See all available commands:**
```bash
make --list
```

**Check service status:**

```bash
curl http://localhost:5060/info
```

### 2. Using the Web UI

Simply open your browser and navigate to `http://localhost:7860` to access the intuitive web interface. The UI provides:

- ğŸ“„ **PDF Analysis** - Upload and analyze PDFs with visual results
- ğŸ”„ **Format Conversion** - Convert to Markdown or HTML
- ğŸŒ **Translation** - Translate documents to multiple languages
- ğŸ‘ï¸ **Visualization** - View segmentation overlays on your PDFs
- ğŸ” **OCR Processing** - Apply OCR to scanned documents
- ğŸ“‘ **TOC Extraction** - Extract table of contents

### 3. Using the REST API

**Analyze a PDF document (VGT model - high accuracy):**
```bash
curl -X POST -F &#039;file=@/path/to/your/document.pdf&#039; http://localhost:5060
```

**Fast analysis (LightGBM models - faster processing):**
```bash
curl -X POST -F &#039;file=@/path/to/your/document.pdf&#039; -F &quot;fast=true&quot; http://localhost:5060
```

### 4. Stop the Service

```bash
make stop
```

&gt; ğŸ’¡ **Tip**: The Web UI at `http://localhost:7860` is the easiest way to get started. For automation and integration, use the REST API at `http://localhost:5060`.

---

## âœ¨ Key Features

- ğŸ¨ **User-Friendly Web UI** - Intuitive Gradio interface for easy PDF processing
- ğŸ” **Advanced PDF Layout Analysis** - Segment and classify PDF content with high accuracy
- ğŸ–¼ï¸ **Visual &amp; Fast Models** - Choose between VGT (Vision Grid Transformer) for accuracy or LightGBM for speed
- ğŸ“ **Multi-format Output** - Export to JSON, Markdown, HTML, and visualize PDF segmentations
- ğŸŒ **Automatic Translation** - Translate documents to multiple languages using Ollama models
- ğŸŒ **OCR Support** - 150+ language support with Tesseract OCR
- ğŸ“Š **Table &amp; Formula Extraction** - Extract tables as HTML and formulas as LaTeX
- ğŸ—ï¸ **Clean Architecture** - Modular, testable, and maintainable codebase
- ğŸ³ **Docker-Ready** - Easy deployment with GPU support
- âš¡ **RESTful API** - Comprehensive API with 10+ endpoints

### ğŸ“¸ Example Results

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/huridocs/pdf-document-layout-analysis/main/images/vgtexample1.png&quot;/&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/huridocs/pdf-document-layout-analysis/main/images/vgtexample2.png&quot;/&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/huridocs/pdf-document-layout-analysis/main/images/vgtexample3.png&quot;/&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/huridocs/pdf-document-layout-analysis/main/images/vgtexample4.png&quot;/&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

### ğŸ”— Project Links

- **GitHub**: [pdf-document-layout-analysis](https://github.com/huridocs/pdf-document-layout-analysis)
- **HuggingFace**: [pdf-document-layout-analysis](https://huggingface.co/HURIDOCS/pdf-document-layout-analysis)
- **DockerHub**: [pdf-document-layout-analysis](https://hub.docker.com/r/huridocs/pdf-document-layout-analysis/)

---

## ğŸ“‹ Table of Contents

- [ğŸš€ Overview](#-overview)
- [ğŸš€ Quick Start](#-quick-start)
- [âœ¨ Key Features](#-key-features)
- [âš™ï¸ Dependencies](#-dependencies)
- [ğŸ“‹ Requirements](#-requirements)
- [ğŸ“š API Reference](#-api-reference)
- [ğŸ’¡ Usage Examples](#-usage-examples)
  - [Translation Features](#translation-features)
- [ğŸ—ï¸ Architecture](#-architecture)
- [ğŸ¤– Models](#-models)
- [ğŸ“Š Data](#-data)
- [ğŸ”§ Development](#-development)
- [ğŸ“ˆ Benchmarks](#-benchmarks)
  - [Performance](#performance)
  - [Speed](#speed)
- [ğŸŒ Installation of More Languages for OCR](#-installation-of-more-languages-for-ocr)
- [ğŸ”— Related Services](#-related-services)
- [ğŸ¤ Contributing](#-contributing)



## âš™ï¸ Dependencies

### Required
- **Docker Desktop 4.25.0+** - [Installation Guide](https://www.docker.com/products/docker-desktop/)
- **Python 3.10+** (for local development)

### Optional
- **NVIDIA Container Toolkit** - [Installation Guide](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) (for GPU support)

## ğŸ“‹ Requirements

### System Requirements
- **RAM**: 2 GB minimum
- **GPU Memory**: 5 GB (optional, will fallback to CPU if unavailable)
- **Disk Space**: 10 GB for models and dependencies
- **CPU**: Multi-core recommended for better performance

### Docker Requirements
- Docker Engine 20.10+
- Docker Compose 2.0+

## ğŸ“š API Reference

The service provides a comprehensive RESTful API with the following endpoints:

### Core Analysis Endpoints

| Endpoint | Method | Description | Parameters |
|----------|--------|-------------|------------|
| `/` | POST | Analyze PDF layout and extract segments | `file`, `fast`, `parse_tables_and_math` |
| `/save_xml/{filename}` | POST | Analyze PDF and save XML output | `file`, `xml_file_name`, `fast` |
| `/get_xml/{filename}` | GET | Retrieve saved XML analysis | `xml_file_name` |

### Content Extraction Endpoints

| Endpoint | Method | Description | Parameters |
|----------|--------|-------------|------------|
| `/text` | POST | Extract text by content types | `file`, `fast`, `types` |
| `/toc` | POST | Extract table of contents | `file`, `fast` |
| `/toc_legacy_uwazi_compatible` | POST | Extract TOC (Uwazi compatible) | `file` |

### Format Conversion Endpoints

| Endpoint | Method | Description | Parameters |
|----------|--------|-------------|------------|
| `/markdown` | POST | Convert PDF to Markdown (includes segmentation data in zip) | `file`, `fast`, `extract_toc`, `dpi`, `output_file`, `target_languages`, `translation_model` |
| `/html` | POST | Convert PDF to HTML (includes segmentation data in zip) | `file`, `fast`, `extract_toc`, `dpi`, `output_file`, `target_languages`, `translation_model` |
| `/visualize` | POST | Visualize segmentation results on the PDF | `file`, `fast` |

### OCR &amp; Utility Endpoints

| Endpoint | Method | Description | Parameters |
|----------|--------|-------------|------------|
| `/ocr` | POST | Apply OCR to PDF | `file`, `language` |
| `/info` | GET | Get service information | - |
| `/` | GET | Health check and system info | - |
| `/error` | GET | Test error handling | - |

### Common Parameters

- **`file`**: PDF file to process (multipart/form-data)
- **`fast`**: Use LightGBM models instead of VGT (boolean, default: false)
- **`parse_tables_and_math`**: Apply OCR to table regions (boolean, default: false) and convert formulas to LaTeX
- **`language`**: OCR language code (string, default: &quot;en&quot;)
- **`types`**: Comma-separated content types to extract (string, default: &quot;all&quot;)
- **`extract_toc`**: Include table of contents at the beginning of the output (boolean, default: false)
- **`dpi`**: Image resolution for conversion (integer, default: 120)
- **`target_languages`**: Comma-separated list of target languages for translation (e.g. &quot;Turkish, Spanish, French&quot;)
- **`translation_model`**: Ollama model to use for translation (string, default: &quot;gpt-oss&quot;)

## ğŸ’¡ Usage Examples

### Basic PDF Analysis

**Standard analysis with VGT model:**
```bash
curl -X POST \
  -F &#039;file=@document.pdf&#039; \
  http://localhost:5060
```

**Fast analysis with LightGBM models:**
```bash
curl -X POST \
  -F &#039;file=@document.pdf&#039; \
  -F &#039;fast=true&#039; \
  http://localhost:5060
```

**Analysis with table and math parsing:**
```bash
curl -X POST \
  -F &#039;file=@document.pdf&#039; \
  -F &#039;parse_tables_and_math=true&#039; \
  http://localhost:5060
```

### Text Extraction

**Extract all text:**
```bash
curl -X POST \
  -F &#039;file=@document.pdf&#039; \
  -F &#039;types=all&#039; \
  http://localhost:5060/text
```

**Extract specific content types:**
```bash
curl -X POST \
  -F &#039;file=@document.pdf&#039; \
  -F &#039;types=title,text,table&#039; \
  http://localhost:5060/text
```

### Format Conversion

**Convert to Markdown:**
```bash
curl -X POST http://localhost:5060/markdown \
  -F &#039;file=@document.pdf&#039; \
  -F &#039;extract_toc=true&#039; \
  -F &#039;output_file=document.md&#039; \
  --output &#039;document.zip&#039;
```

**Convert to HTML:**
```bash
curl -X POST http://localhost:5060/html \
  -F &#039;file=@document.pdf&#039; \
  -F &#039;extract_toc=true&#039; \
  -F &#039;output_file=document.md&#039; \
  --output &#039;document.zip&#039;
```

**Convert to Markdown with Translation:**
```bash
curl -X POST http://localhost:5060/markdown \
  -F &#039;file=@document.pdf&#039; \
  -F &#039;output_file=document.md&#039; \
  -F &#039;target_languages=Turkish, Spanish&#039; \
  -F &#039;translation_model=gpt-oss&#039; \
  --output &#039;document.zip&#039;
```

**Convert to HTML with Translation:**
```bash
curl -X POST http://localhost:5060/html \
  -F &#039;file=@document.pdf&#039; \
  -F &#039;output_file=document.md&#039; \
  -F &#039;target_languages=French, Russian&#039; \
  -F &#039;translation_model=huihui_ai/hunyuan-mt-abliterated&#039; \
  --output &#039;document.zip&#039;
```

&gt; **ğŸ“‹ Segmentation Data &amp; Translations**: Format conversion endpoints automatically include detailed segmentation data in the zip output. The resulting zip file contains:
&gt; - **Original file**: The converted document in the requested format
&gt; - **Segmentation data**: `{filename}_segmentation.json` file with information about each detected document segment:
&gt;   - **Coordinates**: `left`, `top`, `width`, `height`
&gt;   - **Page information**: `page_number`, `page_width`, `page_height` 
&gt;   - **Content**: `text` content and segment `type` (e.g., &quot;Title&quot;, &quot;Text&quot;, &quot;Table&quot;, &quot;Picture&quot;)
&gt; - **Translated files** (if `target_languages` specified): `{filename}_{language}.{extension}` for each target language
&gt; - **Images** (if present): `{filename}_pictures/` directory containing extracted images

### Translation Features

The `/markdown` and `/html` endpoints support automatic translation of the converted content into multiple languages using Ollama models.

**Translation Requirements:**
- The specified translation model must be available in Ollama
- An `output_file` must be specified (translations are only included in zip responses)

**Supported Translation Models:**
- Any Ollama-compatible model (e.g., `gpt-oss`, `llama2`, `mistral`, etc.)
- Models are automatically downloaded if not present locally

**Translation Process:**
1. The service checks if the specified model is available in Ollama
2. If not available, it attempts to download the model using `ollama pull`
3. For each target language, the content is translated while preserving:
   - Original formatting and structure
   - Markdown/HTML syntax
   - Links and references
   - Image references and tables
4. Translated files are named: `{filename}_{language}.{extension}`

_**Note that the quality of translations mostly depends on the models used. When using smaller models, the output may contain many unexpected or undesired elements. For regular users, we aimed for a balance between performance and quality, so we tested with different models with a reasonable size. The results for `gpt-oss` were satisfactory, which is why we set it as the default model. If you need something smaller you can also try `huihui_ai/hunyuan-mt-abliterated`, we saw it gives decent results especially if the text does not have much styling.**_

**Example Translation Output:**
```
document.zip
â”œâ”€â”€ document.md                   # Source text with markdown/html styling
â”œâ”€â”€ document_Spanish.md           # Spanish translation  
â”œâ”€â”€ document_French.md            # French translation
â”œâ”€â”€ document_Turkish.md           # Turkish translation
â”œâ”€â”€ document_segmentation.json    # Segmentation information
â””â”€â”€ document_pictures/       # (if images present)
    â”œâ”€â”€ document_1_1.png
    â””â”€â”€ document_1_2.png
```

### OCR Processing

**OCR in English:**
```bash
curl -X POST \
  -F &#039;file=@scanned_document.pdf&#039; \
  -F &#039;language=en&#039; \
  http://localhost:5060/ocr \
  --output ocr_processed.pdf
```

**OCR in other languages:**
```bash
# French
curl -X POST \
  -F &#039;file=@document_french.pdf&#039; \
  -F &#039;language=fr&#039; \
  http://localhost:5060/ocr \
  --output ocr_french.pdf

# Spanish
curl -X POST \
  -F &#039;file=@document_spanish.pdf&#039; \
  -F &#039;language=es&#039; \
  http://localhost:5060/ocr \
  --output ocr_spanish.pdf
```

### Visualization

**Generate visualization PDF:**
```bash
curl -X POST \
  -F &#039;file=@document.pdf&#039; \
  http://localhost:5060/visualize \
  --output visualization.pdf
```

### Table of Contents Extraction

**Extract structured TOC:**
```bash
curl -X POST \
  -F &#039;file=@document.pdf&#039; \
  http://localhost:5060/toc
```

### XML Storage and Retrieval

**Analyze and save XML:**
```bash
curl -X POST \
  -F &#039;file=@document.pdf&#039; \
  http://localhost:5060/save_xml/my_analysis
```

**Retrieve saved XML:**
```bash
curl http://localhost:5060/get_xml/my_analysis.xml
```

### Service Information

**Get service info and supported languages:**
```bash
curl http://localhost:5060/info
```

**Health check:**
```bash
curl http://localhost:5060/
```

### Response Format

Most endpoints return JSON with segment information:

```json
[
  {
    &quot;left&quot;: 72.0,
    &quot;top&quot;: 84.0,
    &quot;width&quot;: 451.2,
    &quot;height&quot;: 23.04,
    &quot;page_number&quot;: 1,
    &quot;page_width&quot;: 595.32,
    &quot;page_height&quot;: 841.92,
    &quot;text&quot;: &quot;Document Title&quot;,
    &quot;type&quot;: &quot;Title&quot;
  },
  {
    &quot;left&quot;: 72.0,
    &quot;top&quot;: 120.0,
    &quot;width&quot;: 451.2,
    &quot;height&quot;: 200.0,
    &quot;page_number&quot;: 1,
    &quot;page_width&quot;: 595.32,
    &quot;page_height&quot;: 841.92,
    &quot;text&quot;: &quot;This is the main text content...&quot;,
    &quot;type&quot;: &quot;Text&quot;
  }
]
```

### Supported Content Types

- `Caption` - Image and table captions
- `Footnote` - Footnote text
- `Formula` - Mathematical formulas
- `List item` - List items and bullet points
- `Page footer` - Footer content
- `Page header` - Header content
- `Picture` - Images and figures
- `Section header` - Section headings
- `Table` - Table content
- `Text` - Regular text paragraphs
- `Title` - Document and section titles


## ğŸ—ï¸ Architecture

This project follows **Clean Architecture** principles, ensuring separation of concerns, testability, and maintainability. The codebase is organized into distinct layers:

### Directory Structure

```
src/
â”œâ”€â”€ domain/                 # Enterprise Business Rules
â”‚   â”œâ”€â”€ PdfImages.py       # PDF image handling domain logic
â”‚   â”œâ”€â”€ PdfSegment.py      # PDF segment entity
â”‚   â”œâ”€â”€ Prediction.py      # ML prediction entity
â”‚   â””â”€â”€ SegmentBox.py      # Core segment box entity
â”œâ”€â”€ use_cases/             # Application Business Rules
â”‚   â”œâ”€â”€ pdf_analysis/      # PDF analysis use case
â”‚   â”œâ”€â”€ text_extraction/   # Text extraction use case
â”‚   â”œâ”€â”€ toc_extraction/    # Table of contents extraction
â”‚   â”œâ”€â”€ visualization/     # PDF visualization use case
â”‚   â”œâ”€â”€ ocr/              # OCR processing use case
â”‚   â”œâ”€â”€ markdown_conversion/ # Markdown conversion use case (with translation)
â”‚   â””â”€â”€ html_conversion/   # HTML conversion use case (with translation)
â”œâ”€â”€ adapters/              # Interface Adapters
â”‚   â”œâ”€â”€ infrastructure/    # External service adapters
â”‚   â”œâ”€â”€ ml/               # Machine learning model adapters
â”‚   â”œâ”€â”€ storage/          # File storage adapters
â”‚   â””â”€â”€ web/              # Web framework adapters
â”œâ”€â”€ ports/                 # Interface definitions
â”‚   â”œâ”€â”€ services/         # Service interfaces
â”‚   â””â”€â”€ repositories/     # Repository interfaces
â””â”€â”€ drivers/              # Frameworks &amp; Drivers
    â””â”€â”€ web/              # FastAPI application setup
```

### Layer Responsibilities

- **Domain Layer**: Contains core business entities and rules independent of external concerns
- **Use Cases Layer**: Orchestrates domain entities to fulfill specific application requirements
- **Adapters Layer**: Implements interfaces defined by inner layers and adapts external frameworks
- **Drivers Layer**: Contains frameworks, databases, and external agency configurations

### Key Benefits

- ğŸ”„ **Dependency Inversion**: High-level modules don&#039;t depend on low-level modules
- ğŸ§ª **Testability**: Easy to unit test business logic in isolation
- ğŸ”§ **Maintainability**: Changes to external frameworks don&#039;t affect business rules
- ğŸ“ˆ **Scalability**: Easy to add new features without modifying existing code

  
## ğŸ¤– Models

The service offers two complementary model approaches, each optimized for different use cases:

### 1. Vision Grid Transformer (VGT) - High Accuracy Model

**Overview**: A state-of-the-art visual model developed by Alibaba Research Group that &quot;sees&quot; the entire page layout.

**Key Features**:
- ğŸ¯ **High Accuracy**: Best-in-class performance on document layout analysis
- ğŸ‘ï¸ **Visual Understanding**: Analyzes the entire page context including spatial relationships
- ğŸ“Š **Trained on DocLayNet**: Uses the comprehensive [DocLayNet dataset](https://github.com/DS4SD/DocLayNet)
- ğŸ”¬ **Research-Backed**: Based on [Advanced Literate Machinery](https://github.com/AlibabaResearch/AdvancedLiterateMachinery)

**Resource Requirements**:
- GPU: 5GB+ VRAM (recommended)
- CPU: Falls back automatically if GPU unavailable
- Processing Speed: ~1.75 seconds/page (GPU [GTX 1070]) or ~13.5 seconds/page (CPU [i7-8700])

### 2. LightGBM Models - Fast &amp; Efficient

**Overview**: Lightweight ensemble of two specialized models using XML-based features from Poppler.

**Key Features**:
- âš¡ **High Speed**: ~0.42 seconds per page on CPU (i7-8700)
- ğŸ’¾ **Low Resource Usage**: CPU-only, minimal memory footprint
- ğŸ”„ **Dual Model Approach**:
  - **Token Type Classifier**: Identifies content types (title, text, table, etc.)
  - **Segmentation Model**: Determines proper content boundaries
- ğŸ“„ **XML-Based**: Uses Poppler&#039;s PDF-to-XML conversion for feature extraction

**Trade-offs**:
- Slightly lower accuracy compared to VGT
- No visual context understanding
- Excellent for batch processing and resource-constrained environments

### OCR Integration

Both models integrate seamlessly with OCR capabilities:

- **Engine**: [Tesseract OCR](https://github.com/tesseract-ocr/tesseract)
- **Processing**: [ocrmypdf](https://ocrmypdf.readthedocs.io/en/latest/index.html)
- **Languages**: 150+ supported languages
- *

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mindsdb/mindsdb]]></title>
            <link>https://github.com/mindsdb/mindsdb</link>
            <guid>https://github.com/mindsdb/mindsdb</guid>
            <pubDate>Mon, 15 Dec 2025 00:04:43 GMT</pubDate>
            <description><![CDATA[Federated query engine for AI - The only MCP Server you'll ever need]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mindsdb/mindsdb">mindsdb/mindsdb</a></h1>
            <p>Federated query engine for AI - The only MCP Server you'll ever need</p>
            <p>Language: Python</p>
            <p>Stars: 37,874</p>
            <p>Forks: 6,059</p>
            <p>Stars today: 122 stars today</p>
            <h2>README</h2><pre>

&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
	&lt;a href=&quot;https://pypi.org/project/MindsDB/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/MindsDB.svg&quot; alt=&quot;MindsDB Release&quot;&gt;&lt;/a&gt;
	&lt;a href=&quot;https://www.python.org/downloads/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/python-3.10.x%7C%203.11.x%7C%203.12.x%7C%203.13.x-brightgreen.svg&quot; alt=&quot;Python supported&quot;&gt;&lt;/a&gt;
	&lt;a href=&quot;https://hub.docker.com/u/mindsdb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/mindsdb/mindsdb&quot; alt=&quot;Docker pulls&quot;&gt;&lt;/a&gt;

  &lt;br /&gt;
  &lt;br /&gt;

  &lt;a href=&quot;https://trendshift.io/repositories/3068&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/3068&quot; alt=&quot;mindsdb%2Fmindsdb | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

  &lt;a href=&quot;https://github.com/mindsdb/mindsdb&quot;&gt;
    &lt;img src=&quot;/docs/assets/mindsdb_logo.png&quot; alt=&quot;MindsDB&quot; width=&quot;300&quot;&gt;
  &lt;/a&gt;

  &lt;p align=&quot;center&quot;&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://www.mindsdb.com?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Website&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://docs.mindsdb.com?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Docs&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://mindsdb.com/contact&quot;&gt;Contact us for a Demo&lt;/a&gt;
    Â·
    &lt;a href=&quot;https://mindsdb.com/joincommunity?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Community Slack&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

----------------------------------------


MindsDB enables humans, AI, agents, and applications to get highly accurate answers across large scale data sources.

&lt;a href=&quot;https://www.youtube.com/watch?v=MX3OKpnsoLM&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/119e7b82-f901-4214-a26f-ff7c5ad86064&quot; alt=&quot;MindsDB Demo&quot;&gt;
	
&lt;/a&gt;


## Install MindsDB Server 

MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart&#039;s content.

  * [Using Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop). This is the fastest and recommended way to get started and have it all running.
  * [Using Docker](https://docs.mindsdb.com/setup/self-hosted/docker). This is also simple, but gives you more flexibility on how to further customize your server.

[MindsDB has an MCP server built in](https://docs.mindsdb.com/mcp/overview) that enables your MCP applications to connect, unify and respond to questions over large-scale federated dataâ€”spanning databases, data warehouses, and SaaS applications.
 
----------------------------------------

# Core Philosophy: Connect, Unify, Respond

MindsDB&#039;s architecture is built around three fundamental capabilities:

## [Connect](https://docs.mindsdb.com/integrations/data-overview) Your Data

You can connect to hundreds of enterprise [data sources (learn more)](https://docs.mindsdb.com/integrations/data-overview). These integrations allow MindsDB to access data wherever it resides, forming the foundation for all other capabilities.

## [Unify](https://docs.mindsdb.com/mindsdb_sql/overview) Your Data


In many situations, itâ€™s important to be able to prepare and unify data before generating responses from it. MindsDB SQL offers knowledge bases and views that allow indexing and organizing structured and unstructured data as if it were unified in a single system.

* [**KNOWLEDGE BASES**](https://docs.mindsdb.com/mindsdb_sql/knowledge-bases) â€“ Index and organize unstructured data for efficient Q&amp;A.
* [**VIEWS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/view) â€“ Simplify data access by creating unified views across different sources (no-ETL).


Unification of data can be automated using JOBs

* [**JOBS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/jobs) â€“ Schedule synchronization and transformation tasks for real-time processing.


## [Respond](https://docs.mindsdb.com/mindsdb_sql/agents/agent) From Your Data

Chat with Your Data

* [**AGENTS**](https://docs.mindsdb.com/mindsdb_sql/agents/agent) â€“ Configure built-in agents specialized in answering questions over your connected and unified data.
* [**MCP**](https://docs.mindsdb.com/mcp/overview) â€“ Connect to MindsDB through the MCP (Model Context Protocol) for seamless interaction.

----------------------------------------

## ğŸ¤ Contribute

Interested in contributing to MindsDB? Follow our [installation guide for development](https://docs.mindsdb.com/contribute/install?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

You can find our [contribution guide here](https://docs.mindsdb.com/contribute/contribute?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

We welcome suggestions! Feel free to open new issues with your ideas, and weâ€™ll guide you.

This project adheres to a [Contributor Code of Conduct](https://github.com/mindsdb/mindsdb/blob/main/CODE_OF_CONDUCT.md). By participating, you agree to follow its terms.

Also, check out our [community rewards and programs](https://mindsdb.com/community?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

## ğŸ¤ Support

If you find a bug, please submit an [issue on GitHub](https://github.com/mindsdb/mindsdb/issues/new/choose).

Hereâ€™s how you can get community support:

* Ask a question in our [Slack Community](https://mindsdb.com/joincommunity?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).
* Join our [GitHub Discussions](https://github.com/mindsdb/mindsdb/discussions).
* Post on [Stack Overflow](https://stackoverflow.com/questions/tagged/mindsdb) with the MindsDB tag.

For commercial support, please [contact the MindsDB team](https://mindsdb.com/contact?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

## ğŸ’š Current Contributors

&lt;a href=&quot;https://github.com/mindsdb/mindsdb/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contributors-img.web.app/image?repo=mindsdb/mindsdb&quot; /&gt;
&lt;/a&gt;

Generated with [contributors-img](https://contributors-img.web.app).

## ğŸ”” Subscribe for Updates

Join our [Slack community](https://mindsdb.com/joincommunity)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[yt-dlp/yt-dlp]]></title>
            <link>https://github.com/yt-dlp/yt-dlp</link>
            <guid>https://github.com/yt-dlp/yt-dlp</guid>
            <pubDate>Mon, 15 Dec 2025 00:04:42 GMT</pubDate>
            <description><![CDATA[A feature-rich command-line audio/video downloader]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/yt-dlp/yt-dlp">yt-dlp/yt-dlp</a></h1>
            <p>A feature-rich command-line audio/video downloader</p>
            <p>Language: Python</p>
            <p>Stars: 138,266</p>
            <p>Forks: 11,160</p>
            <p>Stars today: 116 stars today</p>
            <h2>README</h2><pre>&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
&lt;div align=&quot;center&quot;&gt;

[![YT-DLP](https://raw.githubusercontent.com/yt-dlp/yt-dlp/master/.github/banner.svg)](#readme)

[![Release version](https://img.shields.io/github/v/release/yt-dlp/yt-dlp?color=brightgreen&amp;label=Download&amp;style=for-the-badge)](#installation &quot;Installation&quot;)
[![PyPI](https://img.shields.io/badge/-PyPI-blue.svg?logo=pypi&amp;labelColor=555555&amp;style=for-the-badge)](https://pypi.org/project/yt-dlp &quot;PyPI&quot;)
[![Donate](https://img.shields.io/badge/_-Donate-red.svg?logo=githubsponsors&amp;labelColor=555555&amp;style=for-the-badge)](Maintainers.md#maintainers &quot;Donate&quot;)
[![Discord](https://img.shields.io/discord/807245652072857610?color=blue&amp;labelColor=555555&amp;label=&amp;logo=discord&amp;style=for-the-badge)](https://discord.gg/H5MNcFW63r &quot;Discord&quot;)
[![Supported Sites](https://img.shields.io/badge/-Supported_Sites-brightgreen.svg?style=for-the-badge)](supportedsites.md &quot;Supported Sites&quot;)
[![License: Unlicense](https://img.shields.io/badge/-Unlicense-blue.svg?style=for-the-badge)](LICENSE &quot;License&quot;)
[![CI Status](https://img.shields.io/github/actions/workflow/status/yt-dlp/yt-dlp/core.yml?branch=master&amp;label=Tests&amp;style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/actions &quot;CI Status&quot;)
[![Commits](https://img.shields.io/github/commit-activity/m/yt-dlp/yt-dlp?label=commits&amp;style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/commits &quot;Commit History&quot;)
[![Last Commit](https://img.shields.io/github/last-commit/yt-dlp/yt-dlp/master?label=&amp;style=for-the-badge&amp;display_timestamp=committer)](https://github.com/yt-dlp/yt-dlp/pulse/monthly &quot;Last activity&quot;)

&lt;/div&gt;
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

yt-dlp is a feature-rich command-line audio/video downloader with support for [thousands of sites](supportedsites.md). The project is a fork of [youtube-dl](https://github.com/ytdl-org/youtube-dl) based on the now inactive [youtube-dlc](https://github.com/blackjack4494/yt-dlc).

&lt;!-- MANPAGE: MOVE &quot;USAGE AND OPTIONS&quot; SECTION HERE --&gt;

&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
* [INSTALLATION](#installation)
    * [Detailed instructions](https://github.com/yt-dlp/yt-dlp/wiki/Installation)
    * [Release Files](#release-files)
    * [Update](#update)
    * [Dependencies](#dependencies)
    * [Compile](#compile)
* [USAGE AND OPTIONS](#usage-and-options)
    * [General Options](#general-options)
    * [Network Options](#network-options)
    * [Geo-restriction](#geo-restriction)
    * [Video Selection](#video-selection)
    * [Download Options](#download-options)
    * [Filesystem Options](#filesystem-options)
    * [Thumbnail Options](#thumbnail-options)
    * [Internet Shortcut Options](#internet-shortcut-options)
    * [Verbosity and Simulation Options](#verbosity-and-simulation-options)
    * [Workarounds](#workarounds)
    * [Video Format Options](#video-format-options)
    * [Subtitle Options](#subtitle-options)
    * [Authentication Options](#authentication-options)
    * [Post-processing Options](#post-processing-options)
    * [SponsorBlock Options](#sponsorblock-options)
    * [Extractor Options](#extractor-options)
    * [Preset Aliases](#preset-aliases)
* [CONFIGURATION](#configuration)
    * [Configuration file encoding](#configuration-file-encoding)
    * [Authentication with netrc](#authentication-with-netrc)
    * [Notes about environment variables](#notes-about-environment-variables)
* [OUTPUT TEMPLATE](#output-template)
    * [Output template examples](#output-template-examples)
* [FORMAT SELECTION](#format-selection)
    * [Filtering Formats](#filtering-formats)
    * [Sorting Formats](#sorting-formats)
    * [Format Selection examples](#format-selection-examples)
* [MODIFYING METADATA](#modifying-metadata)
    * [Modifying metadata examples](#modifying-metadata-examples)
* [EXTRACTOR ARGUMENTS](#extractor-arguments)
* [PLUGINS](#plugins)
    * [Installing Plugins](#installing-plugins)
    * [Developing Plugins](#developing-plugins)
* [EMBEDDING YT-DLP](#embedding-yt-dlp)
    * [Embedding examples](#embedding-examples)
* [CHANGES FROM YOUTUBE-DL](#changes-from-youtube-dl)
    * [New features](#new-features)
    * [Differences in default behavior](#differences-in-default-behavior)
    * [Deprecated options](#deprecated-options)
* [CONTRIBUTING](CONTRIBUTING.md#contributing-to-yt-dlp)
    * [Opening an Issue](CONTRIBUTING.md#opening-an-issue)
    * [Developer Instructions](CONTRIBUTING.md#developer-instructions)
* [WIKI](https://github.com/yt-dlp/yt-dlp/wiki)
    * [FAQ](https://github.com/yt-dlp/yt-dlp/wiki/FAQ)
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;


# INSTALLATION

&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
[![Windows](https://img.shields.io/badge/-Windows_x64-blue.svg?style=for-the-badge&amp;logo=windows)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe)
[![Unix](https://img.shields.io/badge/-Linux/BSD-red.svg?style=for-the-badge&amp;logo=linux)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp)
[![MacOS](https://img.shields.io/badge/-MacOS-lightblue.svg?style=for-the-badge&amp;logo=apple)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos)
[![PyPI](https://img.shields.io/badge/-PyPI-blue.svg?logo=pypi&amp;labelColor=555555&amp;style=for-the-badge)](https://pypi.org/project/yt-dlp)
[![Source Tarball](https://img.shields.io/badge/-Source_tar-green.svg?style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)
[![Other variants](https://img.shields.io/badge/-Other-grey.svg?style=for-the-badge)](#release-files)
[![All versions](https://img.shields.io/badge/-All_Versions-lightgrey.svg?style=for-the-badge)](https://github.com/yt-dlp/yt-dlp/releases)
&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

You can install yt-dlp using [the binaries](#release-files), [pip](https://pypi.org/project/yt-dlp) or one using a third-party package manager. See [the wiki](https://github.com/yt-dlp/yt-dlp/wiki/Installation) for detailed instructions


&lt;!-- MANPAGE: BEGIN EXCLUDED SECTION --&gt;
## RELEASE FILES

#### Recommended

File|Description
:---|:---
[yt-dlp](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp)|Platform-independent [zipimport](https://docs.python.org/3/library/zipimport.html) binary. Needs Python (recommended for **Linux/BSD**)
[yt-dlp.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.exe)|Windows (Win8+) standalone x64 binary (recommended for **Windows**)
[yt-dlp_macos](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos)|Universal MacOS (10.15+) standalone executable (recommended for **MacOS**)

#### Alternatives

File|Description
:---|:---
[yt-dlp_linux](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux)|Linux (glibc 2.17+) standalone x86_64 binary
[yt-dlp_linux.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux.zip)|Unpackaged Linux (glibc 2.17+) x86_64 executable (no auto-update)
[yt-dlp_linux_aarch64](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_aarch64)|Linux (glibc 2.17+) standalone aarch64 binary
[yt-dlp_linux_aarch64.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_aarch64.zip)|Unpackaged Linux (glibc 2.17+) aarch64 executable (no auto-update)
[yt-dlp_linux_armv7l.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_linux_armv7l.zip)|Unpackaged Linux (glibc 2.31+) armv7l executable (no auto-update)
[yt-dlp_musllinux](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux)|Linux (musl 1.2+) standalone x86_64 binary
[yt-dlp_musllinux.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux.zip)|Unpackaged Linux (musl 1.2+) x86_64 executable (no auto-update)
[yt-dlp_musllinux_aarch64](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux_aarch64)|Linux (musl 1.2+) standalone aarch64 binary
[yt-dlp_musllinux_aarch64.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_musllinux_aarch64.zip)|Unpackaged Linux (musl 1.2+) aarch64 executable (no auto-update)
[yt-dlp_x86.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_x86.exe)|Windows (Win8+) standalone x86 (32-bit) binary
[yt-dlp_win_x86.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win_x86.zip)|Unpackaged Windows (Win8+) x86 (32-bit) executable (no auto-update)
[yt-dlp_arm64.exe](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_arm64.exe)|Windows (Win10+) standalone ARM64 binary
[yt-dlp_win_arm64.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win_arm64.zip)|Unpackaged Windows (Win10+) ARM64 executable (no auto-update)
[yt-dlp_win.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_win.zip)|Unpackaged Windows (Win8+) x64 executable (no auto-update)
[yt-dlp_macos.zip](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp_macos.zip)|Unpackaged MacOS (10.15+) executable (no auto-update)

#### Misc

File|Description
:---|:---
[yt-dlp.tar.gz](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)|Source tarball
[SHA2-512SUMS](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS)|GNU-style SHA512 sums
[SHA2-512SUMS.sig](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-512SUMS.sig)|GPG signature file for SHA512 sums
[SHA2-256SUMS](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS)|GNU-style SHA256 sums
[SHA2-256SUMS.sig](https://github.com/yt-dlp/yt-dlp/releases/latest/download/SHA2-256SUMS.sig)|GPG signature file for SHA256 sums

The public key that can be used to verify the GPG signatures is [available here](https://github.com/yt-dlp/yt-dlp/blob/master/public.key)
Example usage:
```
curl -L https://github.com/yt-dlp/yt-dlp/raw/master/public.key | gpg --import
gpg --verify SHA2-256SUMS.sig SHA2-256SUMS
gpg --verify SHA2-512SUMS.sig SHA2-512SUMS
```

#### Licensing

While yt-dlp is licensed under the [Unlicense](LICENSE), many of the release files contain code from other projects with different licenses.

Most notably, the PyInstaller-bundled executables include GPLv3+ licensed code, and as such the combined work is licensed under [GPLv3+](https://www.gnu.org/licenses/gpl-3.0.html).

The zipimport Unix executable (`yt-dlp`) contains [ISC](https://github.com/meriyah/meriyah/blob/main/LICENSE.md) licensed code from [`meriyah`](https://github.com/meriyah/meriyah) and [MIT](https://github.com/davidbonnet/astring/blob/main/LICENSE) licensed code from [`astring`](https://github.com/davidbonnet/astring).

See [THIRD_PARTY_LICENSES.txt](THIRD_PARTY_LICENSES.txt) for more details.

The git repository, the source tarball (`yt-dlp.tar.gz`), the PyPI source distribution and the PyPI built distribution (wheel) only contain code licensed under the [Unlicense](LICENSE).

&lt;!-- MANPAGE: END EXCLUDED SECTION --&gt;

**Note**: The manpages, shell completion (autocomplete) files etc. are available inside the [source tarball](https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp.tar.gz)


## UPDATE
You can use `yt-dlp -U` to update if you are using the [release binaries](#release-files)

If you [installed with pip](https://github.com/yt-dlp/yt-dlp/wiki/Installation#with-pip), simply re-run the same command that was used to install the program

For other third-party package managers, see [the wiki](https://github.com/yt-dlp/yt-dlp/wiki/Installation#third-party-package-managers) or refer to their documentation

&lt;a id=&quot;update-channels&quot;&gt;&lt;/a&gt;

There are currently three release channels for binaries: `stable`, `nightly` and `master`.

* `stable` is the default channel, and many of its changes have been tested by users of the `nightly` and `master` channels.
* The `nightly` channel has releases scheduled to build every day around midnight UTC, for a snapshot of the project&#039;s new patches and changes. This is the **recommended channel for regular users** of yt-dlp. The `nightly` releases are available from [yt-dlp/yt-dlp-nightly-builds](https://github.com/yt-dlp/yt-dlp-nightly-builds/releases) or as development releases of the `yt-dlp` PyPI package (which can be installed with pip&#039;s `--pre` flag).
* The `master` channel features releases that are built after each push to the master branch, and these will have the very latest fixes and additions, but may also be more prone to regressions. They are available from [yt-dlp/yt-dlp-master-builds](https://github.com/yt-dlp/yt-dlp-master-builds/releases).

When using `--update`/`-U`, a release binary will only update to its current channel.
`--update-to CHANNEL` can be used to switch to a different channel when a newer version is available. `--update-to [CHANNEL@]TAG` can also be used to upgrade or downgrade to specific tags from a channel.

You may also use `--update-to &lt;repository&gt;` (`&lt;owner&gt;/&lt;repository&gt;`) to update to a channel on a completely different repository. Be careful with what repository you are updating to though, there is no verification done for binaries from different repositories.

Example usage:

* `yt-dlp --update-to master` switch to the `master` channel and update to its latest release
* `yt-dlp --update-to stable@2023.07.06` upgrade/downgrade to release to `stable` channel tag `2023.07.06`
* `yt-dlp --update-to 2023.10.07` upgrade/downgrade to tag `2023.10.07` if it exists on the current channel
* `yt-dlp --update-to example/yt-dlp@2023.09.24` upgrade/downgrade to the release from the `example/yt-dlp` repository, tag `2023.09.24`

**Important**: Any user experiencing an issue with the `stable` release should install or update to the `nightly` release before submitting a bug report:
```
# To update to nightly from stable executable/binary:
yt-dlp --update-to nightly

# To install nightly with pip:
python -m pip install -U --pre &quot;yt-dlp[default]&quot;
```

When running a yt-dlp version that is older than 90 days, you will see a warning message suggesting to update to the latest version.
You can suppress this warning by adding `--no-update` to your command or configuration file.

## DEPENDENCIES
Python versions 3.10+ (CPython) and 3.11+ (PyPy) are supported. Other versions and implementations may or may not work correctly.

&lt;!-- Python 3.5+ uses VC++14 and it is already embedded in the binary created
&lt;!x-- https://www.microsoft.com/en-us/download/details.aspx?id=26999 --x&gt;
On Windows, [Microsoft Visual C++ 2010 SP1 Redistributable Package (x86)](https://download.microsoft.com/download/1/6/5/165255E7-1014-4D0A-B094-B6A430A6BFFC/vcredist_x86.exe) is also necessary to run yt-dlp. You probably already have this, but if the executable throws an error due to missing `MSVCR100.dll` you need to install it manually.
--&gt;

While all the other dependencies are optional, `ffmpeg`, `ffprobe`, `yt-dlp-ejs` and a supported JavaScript runtime/engine are highly recommended

### Strongly recommended

* [**ffmpeg** and **ffprobe**](https://www.ffmpeg.org) - Required for [merging separate video and audio files](#format-selection), as well as for various [post-processing](#post-processing-options) tasks. License [depends on the build](https://www.ffmpeg.org/legal.html)

    There are bugs in ffmpeg that cause various issues when used alongside yt-dlp. Since ffmpeg is such an important dependency, we provide [custom builds](https://github.com/yt-dlp/FFmpeg-Builds#ffmpeg-static-auto-builds) with patches for some of these issues at [yt-dlp/FFmpeg-Builds](https://github.com/yt-dlp/FFmpeg-Builds). See [the readme](https://github.com/yt-dlp/FFmpeg-Builds#patches-applied) for details on the specific issues solved by these builds

    **Important**: What you need is ffmpeg *binary*, **NOT** [the Python package of the same name](https://pypi.org/project/ffmpeg)

* [**yt-dlp-ejs**](https://github.com/yt-dlp/ejs) - Required for deciphering YouTube n/sig values. Licensed under [Unlicense](https://github.com/yt-dlp/ejs/blob/main/LICENSE), bundles [MIT](https://github.com/davidbonnet/astring/blob/main/LICENSE) and [ISC](https://github.com/meriyah/meriyah/blob/main/LICENSE.md) components.

    A JavaScript runtime/engine like [**deno**](https://deno.land) (recommended), [**node.js**](https://nodejs.org), [**bun**](https://bun.sh), or [**QuickJS**](https://bellard.org/quickjs/) is also required to run yt-dlp-ejs. See [the wiki](https://github.com/yt-dlp/yt-dlp/wiki/EJS).

### Networking
* [**certifi**](https://github.com/certifi/python-certifi)\* - Provides Mozilla&#039;s root certificate bundle. Licensed under [MPLv2](https://github.com/certifi/python-certifi/blob/master/LICENSE)
* [**brotli**](https://github.com/google/brotli)\* or [**brotlicffi**](https://github.com/python-hyper/brotlicffi) - [Brotli](https://en.wikipedia.org/wiki/Brotli) content encoding support. Both licensed under MIT &lt;sup&gt;[1](https://github.com/google/brotli/blob/master/LICENSE) [2](https://github.com/python-hyper/brotlicffi/blob/master/LICENSE) &lt;/sup&gt;
* [**websockets**](https://github.com/aaugustin/websockets)\* - For downloading over websocket. Licensed under [BSD-3-Clause](https://github.com/aaugustin/websockets/blob/main/LICENSE)
* [**requests**](https://github.com/psf/requests)\* - HTTP library. For HTTPS proxy and persistent connections support. Licensed under [Apache-2.0](https://github.com/psf/requests/blob/main/LICENSE)

#### Impersonation

The following provide support for impersonating browser requests. This may be required for some sites that employ TLS fingerprinting.

* [**curl_cffi**](https://github.com/lexiforest/curl_cffi) (recommended) - Python binding for [curl-impersonate](https://github.com/lexiforest/curl-impersonate). Provides impersonation targets for Chrome, Edge and Safari. Licensed under [MIT](https://github.com/lexiforest/curl_cffi/blob/main/LICENSE)
  * Can be installed with the `curl-cffi` extra, e.g. `pip install &quot;yt-dlp[default,curl-cffi]&quot;`
  * Currently included in most builds *except* `yt-dlp` (Unix zipimport binary), `yt-dlp_x86` (Windows 32-bit) and `yt-dlp_musllinux_aarch64`


### Metadata

* [**mutagen**](https://github.com/quodlibet/mutagen)\* - For `--embed-thumbnail` in certain formats. Licensed under [GPLv2+](https://github.com/quodlibet/mutagen/blob/master/COPYING)
* [**AtomicParsley**](https://github.com/wez/atomicparsley) - For `--embed-thumbnail` in `mp4`/`m4a` files when `mutagen`/`ffmpeg` cannot. Licensed under [GPLv2+](https://github.com/wez/atomicparsley/blob/master/COPYING)
* [**xattr**](https://github.com/xattr/xattr), [**pyxattr**](https://github.com/iustin/pyxattr) or [**setfattr**](http://savannah.nongnu.org/projects/attr) - For writing xattr metadata (`--xattrs`) on **Mac** and **BSD**. Licensed under [MIT](https://github.com/xattr/xattr/blob/master/LICENSE.txt), [LGPL2.1](https://github.com/iustin/pyxattr/blob/master/COPYING) and [GPLv2+](http://git.savannah.nongnu.org/cgit/attr.git/tree/doc/COPYING) respectively

### Misc

* [**pycryptodomex**](https://github.com/Legrandin/pycryptodome)\* - For decrypting AES-128 HLS streams and various other data. Licensed under [BSD-2-Clause](https://github.com/Legrandin/pycryptodome/blob/master/LICENSE.rst)
* [**phantomjs**](https://github.com/ariya/phantomjs) - Used in some extractors where JavaScript needs to be run. No longer used for YouTube. To be deprecated in the near future. Licensed under [BSD-3-Clause](https://github.com/ariya/phantomjs/blob/master/LICENSE.BSD)
* [**secretstorage**](https://github.com/mitya57/secretstorage)\* - For `--cookies-from-browser` to access the **Gnome** keyring while decrypting cookies of **Chromium**-based browsers on **Linux**. Licensed under [BSD-3-Clause](https://github.com/mitya57/secretstorage/blob/master/LICENSE)
* Any external downloader that you want to use with `--downloader`

### Deprecated

* [**rtmpdump**](http://rtmpdump.mplayerhq.hu) - For downloading `rtmp` streams. ffmpeg can be used instead with `--downloader ffmpeg`. Licensed under [GPLv2+](http://rtmpdump.mplayerhq.hu)
* [**mplayer**](http://mplayerhq.hu/design7/info.html) or [**mpv

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sinaptik-ai/pandas-ai]]></title>
            <link>https://github.com/sinaptik-ai/pandas-ai</link>
            <guid>https://github.com/sinaptik-ai/pandas-ai</guid>
            <pubDate>Mon, 15 Dec 2025 00:04:41 GMT</pubDate>
            <description><![CDATA[Chat with your database or your datalake (SQL, CSV, parquet). PandasAI makes data analysis conversational using LLMs and RAG.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sinaptik-ai/pandas-ai">sinaptik-ai/pandas-ai</a></h1>
            <p>Chat with your database or your datalake (SQL, CSV, parquet). PandasAI makes data analysis conversational using LLMs and RAG.</p>
            <p>Language: Python</p>
            <p>Stars: 22,811</p>
            <p>Forks: 2,234</p>
            <p>Stars today: 19 stars today</p>
            <h2>README</h2><pre># ![PandasAI](assets/logo.png)

[![Release](https://img.shields.io/pypi/v/pandasai?label=Release&amp;style=flat-square)](https://pypi.org/project/pandasai/)
[![CI](https://github.com/sinaptik-ai/pandas-ai/actions/workflows/ci-core.yml/badge.svg)](https://github.com/sinaptik-ai/pandas-ai/actions/workflows/ci-core.yml/badge.svg)
[![CD](https://github.com/sinaptik-ai/pandas-ai/actions/workflows/cd.yml/badge.svg)](https://github.com/sinaptik-ai/pandas-ai/actions/workflows/cd.yml/badge.svg)
[![Coverage](https://codecov.io/gh/sinaptik-ai/pandas-ai/branch/main/graph/badge.svg)](https://codecov.io/gh/sinaptik-ai/pandas-ai)
[![Discord](https://dcbadge.vercel.app/api/server/kF7FqH2FwS?style=flat&amp;compact=true)](https://discord.gg/KYKj9F2FRH)
[![Downloads](https://static.pepy.tech/badge/pandasai)](https://pepy.tech/project/pandasai) [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1ZnO-njhL7TBOYPZaqvMvGtsjckZKrv2E?usp=sharing)

PandasAI is a Python library that makes it easy to ask questions to your data in natural language. It helps non-technical users to interact with their data in a more natural way, and it helps technical users to save time, and effort when working with data.

# ğŸ”§ Getting started

You can find the full documentation for PandasAI [here](https://docs.pandas-ai.com/).


## ğŸ“š Using the library

### Python Requirements

Python version `3.8+ &lt;=3.11`

### ğŸ“¦ Installation

You can install the PandasAI library using pip or poetry.

With pip:

```bash
pip install pandasai
pip install pandasai-litellm
```

With poetry:

```bash
poetry add pandasai
poetry add pandasai-litellm
```

### ğŸ’» Usage

#### Ask questions

```python
import pandasai as pai
from pandasai_litellm.litellm import LiteLLM

# Initialize LiteLLM with your OpenAI model
llm = LiteLLM(model=&quot;gpt-4.1-mini&quot;, api_key=&quot;YOUR_OPENAI_API_KEY&quot;)

# Configure PandasAI to use this LLM
pai.config.set({
    &quot;llm&quot;: llm
})

# Load your data
df = pai.read_csv(&quot;data/companies.csv&quot;)

response = df.chat(&quot;What is the average revenue by region?&quot;)
print(response)
```

---

Or you can ask more complex questions:

```python
df.chat(
    &quot;What is the total sales for the top 3 countries by sales?&quot;
)
```

```
The total sales for the top 3 countries by sales is 16500.
```

#### Visualize charts

You can also ask PandasAI to generate charts for you:

```python
df.chat(
    &quot;Plot the histogram of countries showing for each one the gdp. Use different colors for each bar&quot;,
)
```

![Chart](assets/histogram-chart.png?raw=true)

#### Multiple DataFrames

You can also pass in multiple dataframes to PandasAI and ask questions relating them.

```python
import pandasai as pai
from pandasai_litellm.litellm import LiteLLM

# Initialize LiteLLM with your OpenAI model
llm = LiteLLM(model=&quot;gpt-4.1-mini&quot;, api_key=&quot;YOUR_OPENAI_API_KEY&quot;)

# Configure PandasAI to use this LLM
pai.config.set({
    &quot;llm&quot;: llm
})

employees_data = {
    &#039;EmployeeID&#039;: [1, 2, 3, 4, 5],
    &#039;Name&#039;: [&#039;John&#039;, &#039;Emma&#039;, &#039;Liam&#039;, &#039;Olivia&#039;, &#039;William&#039;],
    &#039;Department&#039;: [&#039;HR&#039;, &#039;Sales&#039;, &#039;IT&#039;, &#039;Marketing&#039;, &#039;Finance&#039;]
}

salaries_data = {
    &#039;EmployeeID&#039;: [1, 2, 3, 4, 5],
    &#039;Salary&#039;: [5000, 6000, 4500, 7000, 5500]
}

employees_df = pai.DataFrame(employees_data)
salaries_df = pai.DataFrame(salaries_data)


pai.chat(&quot;Who gets paid the most?&quot;, employees_df, salaries_df)
```

```
Olivia gets paid the most.
```

#### Docker Sandbox

You can run PandasAI in a Docker sandbox, providing a secure, isolated environment to execute code safely and mitigate the risk of malicious attacks.

##### Python Requirements

```bash
pip install &quot;pandasai-docker&quot;
```

##### Usage

```python
import pandasai as pai
from pandasai_docker import DockerSandbox
from pandasai_litellm.litellm import LiteLLM

# Initialize LiteLLM with your OpenAI model
llm = LiteLLM(model=&quot;gpt-4.1-mini&quot;, api_key=&quot;YOUR_OPENAI_API_KEY&quot;)

# Configure PandasAI to use this LLM
pai.config.set({
    &quot;llm&quot;: llm
})

# Initialize the sandbox
sandbox = DockerSandbox()
sandbox.start()

employees_data = {
    &#039;EmployeeID&#039;: [1, 2, 3, 4, 5],
    &#039;Name&#039;: [&#039;John&#039;, &#039;Emma&#039;, &#039;Liam&#039;, &#039;Olivia&#039;, &#039;William&#039;],
    &#039;Department&#039;: [&#039;HR&#039;, &#039;Sales&#039;, &#039;IT&#039;, &#039;Marketing&#039;, &#039;Finance&#039;]
}

salaries_data = {
    &#039;EmployeeID&#039;: [1, 2, 3, 4, 5],
    &#039;Salary&#039;: [5000, 6000, 4500, 7000, 5500]
}

employees_df = pai.DataFrame(employees_data)
salaries_df = pai.DataFrame(salaries_data)

pai.chat(&quot;Who gets paid the most?&quot;, employees_df, salaries_df, sandbox=sandbox)

# Don&#039;t forget to stop the sandbox when done
sandbox.stop()
```

```
Olivia gets paid the most.
```

You can find more examples in the [examples](examples) directory.

## ğŸ“œ License

PandasAI is available under the MIT expat license, except for the `pandasai/ee` directory of this repository, which has its [license here](https://github.com/sinaptik-ai/pandas-ai/blob/main/ee/LICENSE).

If you are interested in managed PandasAI Cloud or self-hosted Enterprise Offering, [contact us](https://pandas-ai.com).

## Resources

- [Docs](https://docs.pandas-ai.com/) for comprehensive documentation
- [Examples](examples) for example notebooks
- [Discord](https://discord.gg/KYKj9F2FRH) for discussion with the community and PandasAI team

## ğŸ¤ Contributing

Contributions are welcome! Please check the outstanding issues and feel free to open a pull request.
For more information, please check out the [contributing guidelines](CONTRIBUTING.md).

### Thank you!

[![Contributors](https://contrib.rocks/image?repo=sinaptik-ai/pandas-ai)](https://github.com/sinaptik-ai/pandas-ai/graphs/contributors)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[AUTOMATIC1111/stable-diffusion-webui]]></title>
            <link>https://github.com/AUTOMATIC1111/stable-diffusion-webui</link>
            <guid>https://github.com/AUTOMATIC1111/stable-diffusion-webui</guid>
            <pubDate>Mon, 15 Dec 2025 00:04:40 GMT</pubDate>
            <description><![CDATA[Stable Diffusion web UI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">AUTOMATIC1111/stable-diffusion-webui</a></h1>
            <p>Stable Diffusion web UI</p>
            <p>Language: Python</p>
            <p>Stars: 158,949</p>
            <p>Forks: 29,515</p>
            <p>Stars today: 35 stars today</p>
            <h2>README</h2><pre># Stable Diffusion web UI
A web interface for Stable Diffusion, implemented using Gradio library.

![](screenshot.png)

## Features
[Detailed feature showcase with images](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features):
- Original txt2img and img2img modes
- One click install and run script (but you still must install python and git)
- Outpainting
- Inpainting
- Color Sketch
- Prompt Matrix
- Stable Diffusion Upscale
- Attention, specify parts of text that the model should pay more attention to
    - a man in a `((tuxedo))` - will pay more attention to tuxedo
    - a man in a `(tuxedo:1.21)` - alternative syntax
    - select text and press `Ctrl+Up` or `Ctrl+Down` (or `Command+Up` or `Command+Down` if you&#039;re on a MacOS) to automatically adjust attention to selected text (code contributed by anonymous user)
- Loopback, run img2img processing multiple times
- X/Y/Z plot, a way to draw a 3 dimensional plot of images with different parameters
- Textual Inversion
    - have as many embeddings as you want and use any names you like for them
    - use multiple embeddings with different numbers of vectors per token
    - works with half precision floating point numbers
    - train embeddings on 8GB (also reports of 6GB working)
- Extras tab with:
    - GFPGAN, neural network that fixes faces
    - CodeFormer, face restoration tool as an alternative to GFPGAN
    - RealESRGAN, neural network upscaler
    - ESRGAN, neural network upscaler with a lot of third party models
    - SwinIR and Swin2SR ([see here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/2092)), neural network upscalers
    - LDSR, Latent diffusion super resolution upscaling
- Resizing aspect ratio options
- Sampling method selection
    - Adjust sampler eta values (noise multiplier)
    - More advanced noise setting options
- Interrupt processing at any time
- 4GB video card support (also reports of 2GB working)
- Correct seeds for batches
- Live prompt token length validation
- Generation parameters
     - parameters you used to generate images are saved with that image
     - in PNG chunks for PNG, in EXIF for JPEG
     - can drag the image to PNG info tab to restore generation parameters and automatically copy them into UI
     - can be disabled in settings
     - drag and drop an image/text-parameters to promptbox
- Read Generation Parameters Button, loads parameters in promptbox to UI
- Settings page
- Running arbitrary python code from UI (must run with `--allow-code` to enable)
- Mouseover hints for most UI elements
- Possible to change defaults/mix/max/step values for UI elements via text config
- Tiling support, a checkbox to create images that can be tiled like textures
- Progress bar and live image generation preview
    - Can use a separate neural network to produce previews with almost none VRAM or compute requirement
- Negative prompt, an extra text field that allows you to list what you don&#039;t want to see in generated image
- Styles, a way to save part of prompt and easily apply them via dropdown later
- Variations, a way to generate same image but with tiny differences
- Seed resizing, a way to generate same image but at slightly different resolution
- CLIP interrogator, a button that tries to guess prompt from an image
- Prompt Editing, a way to change prompt mid-generation, say to start making a watermelon and switch to anime girl midway
- Batch Processing, process a group of files using img2img
- Img2img Alternative, reverse Euler method of cross attention control
- Highres Fix, a convenience option to produce high resolution pictures in one click without usual distortions
- Reloading checkpoints on the fly
- Checkpoint Merger, a tab that allows you to merge up to 3 checkpoints into one
- [Custom scripts](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Custom-Scripts) with many extensions from community
- [Composable-Diffusion](https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/), a way to use multiple prompts at once
     - separate prompts using uppercase `AND`
     - also supports weights for prompts: `a cat :1.2 AND a dog AND a penguin :2.2`
- No token limit for prompts (original stable diffusion lets you use up to 75 tokens)
- DeepDanbooru integration, creates danbooru style tags for anime prompts
- [xformers](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers), major speed increase for select cards: (add `--xformers` to commandline args)
- via extension: [History tab](https://github.com/yfszzx/stable-diffusion-webui-images-browser): view, direct and delete images conveniently within the UI
- Generate forever option
- Training tab
     - hypernetworks and embeddings options
     - Preprocessing images: cropping, mirroring, autotagging using BLIP or deepdanbooru (for anime)
- Clip skip
- Hypernetworks
- Loras (same as Hypernetworks but more pretty)
- A separate UI where you can choose, with preview, which embeddings, hypernetworks or Loras to add to your prompt
- Can select to load a different VAE from settings screen
- Estimated completion time in progress bar
- API
- Support for dedicated [inpainting model](https://github.com/runwayml/stable-diffusion#inpainting-with-stable-diffusion) by RunwayML
- via extension: [Aesthetic Gradients](https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients), a way to generate images with a specific aesthetic by using clip images embeds (implementation of [https://github.com/vicgalle/stable-diffusion-aesthetic-gradients](https://github.com/vicgalle/stable-diffusion-aesthetic-gradients))
- [Stable Diffusion 2.0](https://github.com/Stability-AI/stablediffusion) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#stable-diffusion-20) for instructions
- [Alt-Diffusion](https://arxiv.org/abs/2211.06679) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#alt-diffusion) for instructions
- Now without any bad letters!
- Load checkpoints in safetensors format
- Eased resolution restriction: generated image&#039;s dimensions must be a multiple of 8 rather than 64
- Now with a license!
- Reorder elements in the UI from settings screen
- [Segmind Stable Diffusion](https://huggingface.co/segmind/SSD-1B) support

## Installation and Running
Make sure the required [dependencies](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Dependencies) are met and follow the instructions available for:
- [NVidia](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs) (recommended)
- [AMD](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-AMD-GPUs) GPUs.
- [Intel CPUs, Intel GPUs (both integrated and discrete)](https://github.com/openvinotoolkit/stable-diffusion-webui/wiki/Installation-on-Intel-Silicon) (external wiki page)
- [Ascend NPUs](https://github.com/wangshuai09/stable-diffusion-webui/wiki/Install-and-run-on-Ascend-NPUs) (external wiki page)

Alternatively, use online services (like Google Colab):

- [List of Online Services](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services)

### Installation on Windows 10/11 with NVidia-GPUs using release package
1. Download `sd.webui.zip` from [v1.0.0-pre](https://github.com/AUTOMATIC1111/stable-diffusion-webui/releases/tag/v1.0.0-pre) and extract its contents.
2. Run `update.bat`.
3. Run `run.bat`.
&gt; For more details see [Install-and-Run-on-NVidia-GPUs](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs)

### Automatic Installation on Windows
1. Install [Python 3.10.6](https://www.python.org/downloads/release/python-3106/) (Newer version of Python does not support torch), checking &quot;Add Python to PATH&quot;.
2. Install [git](https://git-scm.com/download/win).
3. Download the stable-diffusion-webui repository, for example by running `git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git`.
4. Run `webui-user.bat` from Windows Explorer as normal, non-administrator, user.

### Automatic Installation on Linux
1. Install the dependencies:
```bash
# Debian-based:
sudo apt install wget git python3 python3-venv libgl1 libglib2.0-0
# Red Hat-based:
sudo dnf install wget git python3 gperftools-libs libglvnd-glx
# openSUSE-based:
sudo zypper install wget git python3 libtcmalloc4 libglvnd
# Arch-based:
sudo pacman -S wget git python3
```
If your system is very new, you need to install python3.11 or python3.10:
```bash
# Ubuntu 24.04
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt update
sudo apt install python3.11

# Manjaro/Arch
sudo pacman -S yay
yay -S python311 # do not confuse with python3.11 package

# Only for 3.11
# Then set up env variable in launch script
export python_cmd=&quot;python3.11&quot;
# or in webui-user.sh
python_cmd=&quot;python3.11&quot;
```
2. Navigate to the directory you would like the webui to be installed and execute the following command:
```bash
wget -q https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh
```
Or just clone the repo wherever you want:
```bash
git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui
```

3. Run `webui.sh`.
4. Check `webui-user.sh` for options.
### Installation on Apple Silicon

Find the instructions [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Installation-on-Apple-Silicon).

## Contributing
Here&#039;s how to add code to this repo: [Contributing](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)

## Documentation

The documentation was moved from this README over to the project&#039;s [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki).

For the purposes of getting Google and other search engines to crawl the wiki, here&#039;s a link to the (not for humans) [crawlable wiki](https://github-wiki-see.page/m/AUTOMATIC1111/stable-diffusion-webui/wiki).

## Credits
Licenses for borrowed code can be found in `Settings -&gt; Licenses` screen, and also in `html/licenses.html` file.

- Stable Diffusion - https://github.com/Stability-AI/stablediffusion, https://github.com/CompVis/taming-transformers, https://github.com/mcmonkey4eva/sd3-ref
- k-diffusion - https://github.com/crowsonkb/k-diffusion.git
- Spandrel - https://github.com/chaiNNer-org/spandrel implementing
  - GFPGAN - https://github.com/TencentARC/GFPGAN.git
  - CodeFormer - https://github.com/sczhou/CodeFormer
  - ESRGAN - https://github.com/xinntao/ESRGAN
  - SwinIR - https://github.com/JingyunLiang/SwinIR
  - Swin2SR - https://github.com/mv-lab/swin2sr
- LDSR - https://github.com/Hafiidz/latent-diffusion
- MiDaS - https://github.com/isl-org/MiDaS
- Ideas for optimizations - https://github.com/basujindal/stable-diffusion
- Cross Attention layer optimization - Doggettx - https://github.com/Doggettx/stable-diffusion, original idea for prompt editing.
- Cross Attention layer optimization - InvokeAI, lstein - https://github.com/invoke-ai/InvokeAI (originally http://github.com/lstein/stable-diffusion)
- Sub-quadratic Cross Attention layer optimization - Alex Birch (https://github.com/Birch-san/diffusers/pull/1), Amin Rezaei (https://github.com/AminRezaei0x443/memory-efficient-attention)
- Textual Inversion - Rinon Gal - https://github.com/rinongal/textual_inversion (we&#039;re not using his code, but we are using his ideas).
- Idea for SD upscale - https://github.com/jquesnelle/txt2imghd
- Noise generation for outpainting mk2 - https://github.com/parlance-zz/g-diffuser-bot
- CLIP interrogator idea and borrowing some code - https://github.com/pharmapsychotic/clip-interrogator
- Idea for Composable Diffusion - https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch
- xformers - https://github.com/facebookresearch/xformers
- DeepDanbooru - interrogator for anime diffusers https://github.com/KichangKim/DeepDanbooru
- Sampling in float32 precision from a float16 UNet - marunine for the idea, Birch-san for the example Diffusers implementation (https://github.com/Birch-san/diffusers-play/tree/92feee6)
- Instruct pix2pix - Tim Brooks (star), Aleksander Holynski (star), Alexei A. Efros (no star) - https://github.com/timothybrooks/instruct-pix2pix
- Security advice - RyotaK
- UniPC sampler - Wenliang Zhao - https://github.com/wl-zhao/UniPC
- TAESD - Ollin Boer Bohan - https://github.com/madebyollin/taesd
- LyCORIS - KohakuBlueleaf
- Restart sampling - lambertae - https://github.com/Newbeeer/diffusion_restart_sampling
- Hypertile - tfernd - https://github.com/tfernd/HyperTile
- Initial Gradio script - posted on 4chan by an Anonymous user. Thank you Anonymous user.
- (You)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hesreallyhim/awesome-claude-code]]></title>
            <link>https://github.com/hesreallyhim/awesome-claude-code</link>
            <guid>https://github.com/hesreallyhim/awesome-claude-code</guid>
            <pubDate>Mon, 15 Dec 2025 00:04:39 GMT</pubDate>
            <description><![CDATA[[FOR USERS HAVING PERFORMANCE ISSUES: USE README_BACKUP INSTEAD] A curated list of awesome commands, files, and workflows for Claude Code]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hesreallyhim/awesome-claude-code">hesreallyhim/awesome-claude-code</a></h1>
            <p>[FOR USERS HAVING PERFORMANCE ISSUES: USE README_BACKUP INSTEAD] A curated list of awesome commands, files, and workflows for Claude Code</p>
            <p>Language: Python</p>
            <p>Stars: 18,090</p>
            <p>Forks: 1,021</p>
            <p>Stars today: 46 stars today</p>
            <h2>README</h2><pre>&lt;!--lint disable remark-lint:awesome-badge--&gt;

&lt;div align=&quot;center&quot; id=&quot;awesome-claude-code&quot;&gt;

[![Awesome](https://awesome.re/badge-flat2.svg)](https://awesome.re)

&lt;/div&gt;

&lt;!-- Terminal Header - Theme Adaptive --&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/terminal-header.svg&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/terminal-header-light-anim-lineprint.svg&quot;&gt;
  &lt;img src=&quot;assets/terminal-header-light-anim-lineprint.svg&quot; alt=&quot;Awesome Claude Code Terminal&quot; width=&quot;100%&quot;&gt;
&lt;/picture&gt;

&lt;!-- Generated with https://github.com/denvercoder1/readme-typing-svg --&gt;

&lt;div align=&quot;center&quot;&gt;

&lt;br /&gt;

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/repo-ticker.svg&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/repo-ticker-light.svg&quot;&gt;
  &lt;img src=&quot;assets/repo-ticker-light.svg&quot; alt=&quot;Awesome Claude Code Repo Ticker&quot; width=&quot;100%&quot;&gt;
&lt;/picture&gt;

&lt;/div&gt;

&lt;!--lint enable remark-lint:awesome-badge--&gt;

&lt;br&gt;

&lt;!-- Info Terminal - Theme Adaptive --&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/info-terminal.svg&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/info-terminal-light-vintage.svg&quot;&gt;
  &lt;img src=&quot;assets/info-terminal-light-vintage.svg&quot; alt=&quot;System Info Terminal&quot; width=&quot;100%&quot;&gt;
&lt;/picture&gt;

&lt;!--lint enable remark-lint:awesome-badge--&gt;

&lt;br&gt;

&lt;!-- Intro Terminal - Theme Adaptive --&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/intro-terminal.svg&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/intro-terminal-light-vintage.svg&quot;&gt;
  &lt;img src=&quot;assets/intro-terminal-light-vintage.svg&quot; alt=&quot;About Claude Code&quot; width=&quot;100%&quot; style=&quot;max-width: 900px;&quot;&gt;
&lt;/picture&gt;
&lt;/div&gt;

&lt;!-- Design Credit &amp; Disclaimer - Theme Adaptive --&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/designed-by-badge.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/designed-by-badge-light.svg&quot;&gt;
    &lt;img src=&quot;assets/designed-by-badge-light.svg&quot; alt=&quot;Designed by Claude Code Web&quot; width=&quot;280&quot;&gt;
  &lt;/picture&gt;
  &lt;br&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/disclaimer.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/disclaimer-light.svg&quot;&gt;
    &lt;img src=&quot;assets/disclaimer-light.svg&quot; alt=&quot;Disclaimer: Not affiliated or endorsed by Anthropic PBC. Claude Code is a product of Anthropic.&quot; width=&quot;320&quot;&gt;
  &lt;/picture&gt;
&lt;/div&gt;

&lt;!-- ### Announcements [ğŸ”](#awesome-claude-code)

&lt;details open&gt;
&lt;summary&gt;View Announcements&lt;/summary&gt;

- &lt;details open&gt;
  &lt;summary&gt;2025-11-21 - Claude Code for Web - Breaking the Internet&lt;/summary&gt;

  - I don&#039;t know about you folks, but I&#039;ve been having a romping good time playing around with Claude Code for Web. I&#039;m thinking about adding a category for it, but I&#039;m not sure how many people have access, or if people are interested in that. It&#039;s really pretty awesome (I hired it as a sub-contractor to see if it could spice up the repo a little bit, so it&#039;s been running for like 30 hours, I&#039;m not sure what it&#039;s up to... ğŸ‘€) Anyway, if you&#039;re working with Claude Code on the Web, or you&#039;d like to see it included here, please sound off in the [Discussion thread](https://github.com/hesreallyhim/awesome-claude-code/discussions/308).

  - This was me last week, talking about Output Styles: &quot;I&#039;m going to guarantee that by one week from now, we will have a minimum of **5 Output Styles** in that category.&quot; Well, since then, nobody submitted an Output Style - which makes me almost certain that nobody is reading these announcements, since it was kind of open invitation for anyone to get on the list. _Nevertheless_, I&#039;m a Him of my words, so I went and put together a few of my own, which you can check out below. It&#039;s pretty exciting to be included here, because I&#039;ve been rejected four or five times, which is _really_ embarrassing because it&#039;s my repo...

  &lt;/details&gt;

&lt;/details&gt; --&gt;

&lt;div align=&quot;center&quot;&gt;
  &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;img src=&quot;assets/thinking-asterisk.svg&quot; alt=&quot;*&quot; width=&quot;18&quot; /&gt; &lt;a href=&quot;https://git.io/typing-svg&quot;&gt;&lt;img align=&quot;center&quot; src=&quot;https://readme-typing-svg.demolab.com/?font=Fira+Code&amp;weight=600&amp;duration=3000&amp;pause=100&amp;color=F7080D&amp;width=300&amp;lines=Lollygagging...;Skedaddling...;Bumbershooting...;Widdershinning...;Higgledy-piggledying...;Doodlebugging...;Fiddle-faddling...;Whimwhamming...;Dilly-dallying...;Flapdoodling...;Ballyhooing...;Galumphing...;Razzle-dazzling...;Tiddle-taddling...;Zigzagging...;Twinkletoeing...;Puddle-jumping...;Snicker-snacking...;Jibber-jabbering...;Frabjoussing...;Piffle-puffling...;Whirligigging...;Bibbity-bobbitying...;&quot; /&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

### âš¡ TERMINAL NAVIGATION âš¡

&lt;table&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;
&lt;a href=&quot;#agent-skills-&quot;&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/card-skills.svg&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/card-skills-light-anim-lineprint.svg&quot;&gt;
  &lt;img src=&quot;assets/card-skills-light-anim-lineprint.svg&quot; alt=&quot;Agent Skills&quot; width=&quot;200&quot;/&gt;
&lt;/picture&gt;
&lt;/a&gt;
&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;
&lt;a href=&quot;#workflows-knowledge-guides-&quot;&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/card-workflows.svg&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/card-workflows-light-anim-lineprint.svg&quot;&gt;
  &lt;img src=&quot;assets/card-workflows-light-anim-lineprint.svg&quot; alt=&quot;Workflows&quot; width=&quot;200&quot;/&gt;
&lt;/picture&gt;
&lt;/a&gt;
&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;
&lt;a href=&quot;#tooling-&quot;&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/card-tooling.svg&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/card-tooling-light-anim-lineprint.svg&quot;&gt;
  &lt;img src=&quot;assets/card-tooling-light-anim-lineprint.svg&quot; alt=&quot;Tooling&quot; width=&quot;200&quot;/&gt;
&lt;/picture&gt;
&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;
&lt;a href=&quot;#status-lines-&quot;&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/card-statusline.svg&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/card-statusline-light-anim-lineprint.svg&quot;&gt;
  &lt;img src=&quot;assets/card-statusline-light-anim-lineprint.svg&quot; alt=&quot;Status Lines&quot; width=&quot;200&quot;/&gt;
&lt;/picture&gt;
&lt;/a&gt;
&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;
&lt;a href=&quot;#hooks-&quot;&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/card-custom.svg&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/card-custom-light-anim-lineprint.svg&quot;&gt;
  &lt;img src=&quot;assets/card-custom-light-anim-lineprint.svg&quot; alt=&quot;Hooks&quot; width=&quot;200&quot;/&gt;
&lt;/picture&gt;
&lt;/a&gt;
&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;
&lt;a href=&quot;#slash-commands-&quot;&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/card-commands.svg&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/card-commands-light-anim-lineprint.svg&quot;&gt;
  &lt;img src=&quot;assets/card-commands-light-anim-lineprint.svg&quot; alt=&quot;Slash Commands&quot; width=&quot;200&quot;/&gt;
&lt;/picture&gt;
&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&quot;center&quot;&gt;
&lt;a href=&quot;#claudemd-files-&quot;&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/card-config.svg&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/card-config-light-anim-lineprint.svg&quot;&gt;
  &lt;img src=&quot;assets/card-config-light-anim-lineprint.svg&quot; alt=&quot;CLAUDE.md Files&quot; width=&quot;200&quot;/&gt;
&lt;/picture&gt;
&lt;/a&gt;
&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;
&lt;a href=&quot;#alternative-clients-&quot;&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/card-clients.svg&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/card-clients-light-anim-lineprint.svg&quot;&gt;
  &lt;img src=&quot;assets/card-clients-light-anim-lineprint.svg&quot; alt=&quot;Alternative Clients&quot; width=&quot;200&quot;/&gt;
&lt;/picture&gt;
&lt;/a&gt;
&lt;/td&gt;
&lt;td align=&quot;center&quot;&gt;
&lt;a href=&quot;#official-documentation-&quot;&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/card-docs.svg&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/card-docs-light-anim-lineprint.svg&quot;&gt;
  &lt;img src=&quot;assets/card-docs-light-anim-lineprint.svg&quot; alt=&quot;Documentation&quot; width=&quot;200&quot;/&gt;
&lt;/picture&gt;
&lt;/a&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;/div&gt;

&lt;br&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/makeover-banner.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/makeover-banner-light.svg&quot;&gt;
    &lt;img src=&quot;assets/makeover-banner-light.svg&quot; alt=&quot;EXTREME REPO MAKEOVER BY CLAUDE CODE WEB!&quot; width=&quot;100%&quot; style=&quot;max-width: 900px;&quot;&gt;
  &lt;/picture&gt;
&lt;/div&gt;


&lt;br&gt;

&lt;div align=&quot;left&quot;&gt;

&lt;div style=&quot;overflow-x:auto;white-space:nowrap;text-align:left;&quot;&gt;
&lt;div style=&quot;height:48px;width:400px;overflow:hidden;display:block;&quot;&gt;&lt;!-- Directory Tree Terminal - Theme Adaptive --&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-header.svg&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/toc-header-light-anim-scanline.svg&quot;&gt;
  &lt;img src=&quot;assets/toc-header-light-anim-scanline.svg&quot; alt=&quot;Directory Listing&quot; height=&quot;48&quot; style=&quot;height:48px;max-width:none;&quot;&gt;
&lt;/picture&gt;&lt;/div&gt;
&lt;div style=&quot;height:40px;width:400px;overflow:hidden;display:block;&quot;&gt;
&lt;a href=&quot;#agent-skills-&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-row-skills.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/toc-row-skills-light-anim-scanline.svg&quot;&gt;
    &lt;img src=&quot;assets/toc-row-skills-light-anim-scanline.svg&quot; alt=&quot;Agent Skills&quot; height=&quot;40&quot; style=&quot;height:40px;max-width:none;&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;div style=&quot;height:40px;width:400px;overflow:hidden;display:block;&quot;&gt;
&lt;a href=&quot;#skills-general&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-sub-general.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/toc-sub-general-light-anim-scanline.svg&quot;&gt;
    &lt;img src=&quot;assets/toc-sub-general-light-anim-scanline.svg&quot; alt=&quot;General&quot; height=&quot;40&quot; style=&quot;height:40px;max-width:none;&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;div style=&quot;height:40px;width:400px;overflow:hidden;display:block;&quot;&gt;
&lt;a href=&quot;#workflows--knowledge-guides-&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-row-workflows.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/toc-row-workflows-light-anim-scanline.svg&quot;&gt;
    &lt;img src=&quot;assets/toc-row-workflows-light-anim-scanline.svg&quot; alt=&quot;Workflows &amp; Knowledge Guides&quot; height=&quot;40&quot; style=&quot;height:40px;max-width:none;&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;div style=&quot;height:40px;width:400px;overflow:hidden;display:block;&quot;&gt;
&lt;a href=&quot;#workflows-general&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-sub-general.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/toc-sub-general-light-anim-scanline.svg&quot;&gt;
    &lt;img src=&quot;assets/toc-sub-general-light-anim-scanline.svg&quot; alt=&quot;General&quot; height=&quot;40&quot; style=&quot;height:40px;max-width:none;&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;div style=&quot;height:40px;width:400px;overflow:hidden;display:block;&quot;&gt;
&lt;a href=&quot;#tooling-&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-row-tooling.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/toc-row-tooling-light-anim-scanline.svg&quot;&gt;
    &lt;img src=&quot;assets/toc-row-tooling-light-anim-scanline.svg&quot; alt=&quot;Tooling&quot; height=&quot;40&quot; style=&quot;height:40px;max-width:none;&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;div style=&quot;height:40px;width:400px;overflow:hidden;display:block;&quot;&gt;
&lt;a href=&quot;#tooling-general&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-sub-general.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/toc-sub-general-light-anim-scanline.svg&quot;&gt;
    &lt;img src=&quot;assets/toc-sub-general-light-anim-scanline.svg&quot; alt=&quot;General&quot; height=&quot;40&quot; style=&quot;height:40px;max-width:none;&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;div style=&quot;height:40px;width:400px;overflow:hidden;display:block;&quot;&gt;
&lt;a href=&quot;#ide-integrations-&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-sub-ide.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/toc-sub-ide-light-anim-scanline.svg&quot;&gt;
    &lt;img src=&quot;assets/toc-sub-ide-light-anim-scanline.svg&quot; alt=&quot;IDE Integrations&quot; height=&quot;40&quot; style=&quot;height:40px;max-width:none;&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;div style=&quot;height:40px;width:400px;overflow:hidden;display:block;&quot;&gt;
&lt;a href=&quot;#usage-monitors-&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-sub-monitors.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/toc-sub-monitors-light-anim-scanline.svg&quot;&gt;
    &lt;img src=&quot;assets/toc-sub-monitors-light-anim-scanline.svg&quot; alt=&quot;Usage Monitors&quot; height=&quot;40&quot; style=&quot;height:40px;max-width:none;&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;div style=&quot;height:40px;width:400px;overflow:hidden;display:block;&quot;&gt;
&lt;a href=&quot;#orchestrators-&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-sub-orchestrators.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/toc-sub-orchestrators-light-anim-scanline.svg&quot;&gt;
    &lt;img src=&quot;assets/toc-sub-orchestrators-light-anim-scanline.svg&quot; alt=&quot;Orchestrators&quot; height=&quot;40&quot; style=&quot;height:40px;max-width:none;&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;div style=&quot;height:40px;width:400px;overflow:hidden;display:block;&quot;&gt;
&lt;a href=&quot;#status-lines-&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-row-statusline.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/toc-row-statusline-light-anim-scanline.svg&quot;&gt;
    &lt;img src=&quot;assets/toc-row-statusline-light-anim-scanline.svg&quot; alt=&quot;Status Lines&quot; height=&quot;40&quot; style=&quot;height:40px;max-width:none;&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;div style=&quot;height:40px;width:400px;overflow:hidden;display:block;&quot;&gt;
&lt;a href=&quot;#statusline-general&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-sub-general.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/toc-sub-general-light-anim-scanline.svg&quot;&gt;
    &lt;img src=&quot;assets/toc-sub-general-light-anim-scanline.svg&quot; alt=&quot;General&quot; height=&quot;40&quot; style=&quot;height:40px;max-width:none;&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;div style=&quot;height:40px;width:400px;overflow:hidden;display:block;&quot;&gt;
&lt;a href=&quot;#hooks-&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-row-custom.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/toc-row-custom-light-anim-scanline.svg&quot;&gt;
    &lt;img src=&quot;assets/toc-row-custom-light-anim-scanline.svg&quot; alt=&quot;Hooks&quot; height=&quot;40&quot; style=&quot;height:40px;max-width:none;&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;div style=&quot;height:40px;width:400px;overflow:hidden;display:block;&quot;&gt;
&lt;a href=&quot;#hooks-general&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-sub-general.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/toc-sub-general-light-anim-scanline.svg&quot;&gt;
    &lt;img src=&quot;assets/toc-sub-general-light-anim-scanline.svg&quot; alt=&quot;General&quot; height=&quot;40&quot; style=&quot;height:40px;max-width:none;&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;div style=&quot;height:40px;width:400px;overflow:hidden;display:block;&quot;&gt;
&lt;a href=&quot;#slash-commands-&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-row-commands.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/toc-row-commands-light-anim-scanline.svg&quot;&gt;
    &lt;img src=&quot;assets/toc-row-commands-light-anim-scanline.svg&quot; alt=&quot;Slash-Commands&quot; height=&quot;40&quot; style=&quot;height:40px;max-width:none;&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;div style=&quot;height:40px;width:400px;overflow:hidden;display:block;&quot;&gt;
&lt;a href=&quot;#slash-commands-general&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-sub-general.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/toc-sub-general-light-anim-scanline.svg&quot;&gt;
    &lt;img src=&quot;assets/toc-sub-general-light-anim-scanline.svg&quot; alt=&quot;General&quot; height=&quot;40&quot; style=&quot;height:40px;max-width:none;&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;div style=&quot;height:40px;width:400px;overflow:hidden;display:block;&quot;&gt;
&lt;a href=&quot;#version-control--git-&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-sub-git.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/toc-sub-git-light-anim-scanline.svg&quot;&gt;
    &lt;img src=&quot;assets/toc-sub-git-light-anim-scanline.svg&quot; alt=&quot;Version Control &amp; Git&quot; height=&quot;40&quot; style=&quot;height:40px;max-width:none;&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;div style=&quot;height:40px;width:400px;overflow:hidden;display:block;&quot;&gt;
&lt;a href=&quot;#code-analysis--testing-&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-sub-code-analysis.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/toc-sub-code-analysis-light-anim-scanline.svg&quot;&gt;
    &lt;img src=&quot;assets/toc-sub-code-analysis-light-anim-scanline.svg&quot; alt=&quot;Code Analysis &amp; Testing&quot; height=&quot;40&quot; style=&quot;height:40px;max-width:none;&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;div style=&quot;height:40px;width:400px;overflow:hidden;display:block;&quot;&gt;
&lt;a href=&quot;#context-loading--priming-&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-sub-context.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/toc-sub-context-light-anim-scanline.svg&quot;&gt;
    &lt;img src=&quot;assets/toc-sub-context-light-anim-scanline.svg&quot; alt=&quot;Context Loading &amp; Priming&quot; height=&quot;40&quot; style=&quot;height:40px;max-width:none;&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;div style=&quot;height:40px;width:400px;overflow:hidden;display:block;&quot;&gt;
&lt;a href=&quot;#documentation--changelogs-&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-sub-documentation.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/toc-sub-documentation-light-anim-scanline.svg&quot;&gt;
    &lt;img src=&quot;assets/toc-sub-documentation-light-anim-scanline.svg&quot; alt=&quot;Documentation &amp; Changelogs&quot; height=&quot;40&quot; style=&quot;height:40px;max-width:none;&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;div style=&quot;height:40px;width:400px;overflow:hidden;display:block;&quot;&gt;
&lt;a href=&quot;#ci--deployment-&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-sub-ci.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/toc-sub-ci-light-anim-scanline.svg&quot;&gt;
    &lt;img src=&quot;assets/toc-sub-ci-light-anim-scanline.svg&quot; alt=&quot;CI / Deployment&quot; height=&quot;40&quot; style=&quot;height:40px;max-width:none;&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;div style=&quot;height:40px;width:400px;overflow:hidden;display:block;&quot;&gt;
&lt;a href=&quot;#project--task-management-&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-sub-project-mgmt.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/toc-sub-project-mgmt-light-anim-scanline.svg&quot;&gt;
    &lt;img src=&quot;assets/toc-sub-project-mgmt-light-anim-scanline.svg&quot; alt=&quot;Project &amp; Task Management&quot; height=&quot;40&quot; style=&quot;height:40px;max-width:none;&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;div style=&quot;height:40px;width:400px;overflow:hidden;display:block;&quot;&gt;
&lt;a href=&quot;#miscellaneous-&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-sub-misc.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/toc-sub-misc-light-anim-scanline.svg&quot;&gt;
    &lt;img src=&quot;assets/toc-sub-misc-light-anim-scanline.svg&quot; alt=&quot;Miscellaneous&quot; height=&quot;40&quot; style=&quot;height:40px;max-width:none;&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;div style=&quot;height:40px;width:400px;overflow:hidden;display:block;&quot;&gt;
&lt;a href=&quot;#claudemd-files-&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-row-config.svg&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;assets/toc-row-config-light-anim-scanline.svg&quot;&gt;
    &lt;img src=&quot;assets/toc-row-config-light-anim-scanline.svg&quot; alt=&quot;CLAUDE.md Files&quot; height=&quot;40&quot; style=&quot;height:40px;max-width:none;&quot;&gt;
  &lt;/picture&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;div style=&quot;height:40px;width:400px;overflow:hidden;display:block;&quot;&gt;
&lt;a href=&quot;#language-specific-&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;assets/toc-sub-language.svg&quot;&gt;
    &lt;source med

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>