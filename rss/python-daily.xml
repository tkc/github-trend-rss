<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Fri, 20 Feb 2026 00:06:54 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[RichardAtCT/claude-code-telegram]]></title>
            <link>https://github.com/RichardAtCT/claude-code-telegram</link>
            <guid>https://github.com/RichardAtCT/claude-code-telegram</guid>
            <pubDate>Fri, 20 Feb 2026 00:06:54 GMT</pubDate>
            <description><![CDATA[A powerful Telegram bot that provides remote access to Claude Code, enabling developers to interact with their projects from anywhere with full AI assistance and session persistence.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/RichardAtCT/claude-code-telegram">RichardAtCT/claude-code-telegram</a></h1>
            <p>A powerful Telegram bot that provides remote access to Claude Code, enabling developers to interact with their projects from anywhere with full AI assistance and session persistence.</p>
            <p>Language: Python</p>
            <p>Stars: 941</p>
            <p>Forks: 127</p>
            <p>Stars today: 174 stars today</p>
            <h2>README</h2><pre># Claude Code Telegram Bot

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)

A Telegram bot that gives you remote access to [Claude Code](https://claude.ai/code). Chat naturally with Claude about your projects from anywhere -- no terminal commands needed.

## What is this?

This bot connects Telegram to Claude Code, providing a conversational AI interface for your codebase:

- **Chat naturally** -- ask Claude to analyze, edit, or explain your code in plain language
- **Maintain context** across conversations with automatic session persistence per project
- **Code on the go** from any device with Telegram
- **Receive proactive notifications** from webhooks, scheduled jobs, and CI/CD events
- **Stay secure** with built-in authentication, directory sandboxing, and audit logging

## Quick Start

### Demo

```
You: Can you help me add error handling to src/api.py?

Bot: I&#039;ll analyze src/api.py and add error handling...
     [Claude reads your code, suggests improvements, and can apply changes directly]

You: Looks good. Now run the tests to make sure nothing broke.

Bot: Running pytest...
     All 47 tests passed. The error handling changes are working correctly.
```

### 1. Prerequisites

- **Python 3.10+** -- [Download here](https://www.python.org/downloads/)
- **Poetry** -- Modern Python dependency management
- **Claude Code CLI** -- [Install from here](https://claude.ai/code)
- **Telegram Bot Token** -- Get one from [@BotFather](https://t.me/botfather)

### 2. Install

```bash
git clone https://github.com/RichardAtCT/claude-code-telegram.git
cd claude-code-telegram
make dev
```

### 3. Configure

```bash
cp .env.example .env
# Edit .env with your settings:
```

**Minimum required:**
```bash
TELEGRAM_BOT_TOKEN=1234567890:ABC-DEF1234ghIkl-zyx57W2v1u123ew11
TELEGRAM_BOT_USERNAME=my_claude_bot
APPROVED_DIRECTORY=/Users/yourname/projects
ALLOWED_USERS=123456789  # Your Telegram user ID
```

### 4. Run

```bash
make run          # Production
make run-debug    # With debug logging
```

Message your bot on Telegram to get started.

&gt; **Detailed setup:** See [docs/setup.md](docs/setup.md) for Claude authentication options and troubleshooting.

## Modes

The bot supports two interaction modes:

### Agentic Mode (Default)

The default conversational mode. Just talk to Claude naturally -- no special commands required.

**Commands:** `/start`, `/new`, `/status`, `/verbose`, `/repo`
If `ENABLE_PROJECT_THREADS=true`: `/sync_threads`

```
You: What files are in this project?
Bot: Working... (3s)
     ğŸ“– Read
     ğŸ“‚ LS
     ğŸ’¬ Let me describe the project structure
Bot: [Claude describes the project structure]

You: Add a retry decorator to the HTTP client
Bot: Working... (8s)
     ğŸ“– Read: http_client.py
     ğŸ’¬ I&#039;ll add a retry decorator with exponential backoff
     âœï¸ Edit: http_client.py
     ğŸ’» Bash: poetry run pytest tests/ -v
Bot: [Claude shows the changes and test results]

You: /verbose 0
Bot: Verbosity set to 0 (quiet)
```

Use `/verbose 0|1|2` to control how much background activity is shown:

| Level | Shows |
|-------|-------|
| **0** (quiet) | Final response only (typing indicator stays active) |
| **1** (normal, default) | Tool names + reasoning snippets in real-time |
| **2** (detailed) | Tool names with inputs + longer reasoning text |

#### GitHub Workflow

Claude Code already knows how to use `gh` CLI and `git`. Authenticate on your server with `gh auth login`, then work with repos conversationally:

```
You: List my repos related to monitoring
Bot: [Claude runs gh repo list, shows results]

You: Clone the uptime one
Bot: [Claude runs gh repo clone, clones into workspace]

You: /repo
Bot: ğŸ“¦ uptime-monitor/  â—€
     ğŸ“ other-project/

You: Show me the open issues
Bot: [Claude runs gh issue list]

You: Create a fix branch and push it
Bot: [Claude creates branch, commits, pushes]
```

Use `/repo` to list cloned repos in your workspace, or `/repo &lt;name&gt;` to switch directories (sessions auto-resume).

### Classic Mode

Set `AGENTIC_MODE=false` to enable the full 13-command terminal-like interface with directory navigation, inline keyboards, quick actions, git integration, and session export.

**Commands:** `/start`, `/help`, `/new`, `/continue`, `/end`, `/status`, `/cd`, `/ls`, `/pwd`, `/projects`, `/export`, `/actions`, `/git`  
If `ENABLE_PROJECT_THREADS=true`: `/sync_threads`

```
You: /cd my-web-app
Bot: Directory changed to my-web-app/

You: /ls
Bot: src/  tests/  package.json  README.md

You: /actions
Bot: [Run Tests] [Install Deps] [Format Code] [Run Linter]
```

## Event-Driven Automation

Beyond direct chat, the bot can respond to external triggers:

- **Webhooks** -- Receive GitHub events (push, PR, issues) and route them through Claude for automated summaries or code review
- **Scheduler** -- Run recurring Claude tasks on a cron schedule (e.g., daily code health checks)
- **Notifications** -- Deliver agent responses to configured Telegram chats

Enable with `ENABLE_API_SERVER=true` and `ENABLE_SCHEDULER=true`. See [docs/setup.md](docs/setup.md) for configuration.

## Features

### Working Features

- Conversational agentic mode (default) with natural language interaction
- Classic terminal-like mode with 13 commands and inline keyboards
- Full Claude Code integration with SDK (primary) and CLI (fallback)
- Automatic session persistence per user/project directory
- Multi-layer authentication (whitelist + optional token-based)
- Rate limiting with token bucket algorithm
- Directory sandboxing with path traversal prevention
- File upload handling with archive extraction
- Image/screenshot upload with analysis
- Git integration with safe repository operations
- Quick actions system with context-aware buttons
- Session export in Markdown, HTML, and JSON formats
- SQLite persistence with migrations
- Usage and cost tracking
- Audit logging and security event tracking
- Event bus for decoupled message routing
- Webhook API server (GitHub HMAC-SHA256, generic Bearer token auth)
- Job scheduler with cron expressions and persistent storage
- Notification service with per-chat rate limiting

- Tunable verbose output showing Claude&#039;s tool usage and reasoning in real-time
- Persistent typing indicator so users always know the bot is working

### Planned Enhancements

- Plugin system for third-party extensions

## Configuration

### Required

```bash
TELEGRAM_BOT_TOKEN=...           # From @BotFather
TELEGRAM_BOT_USERNAME=...        # Your bot&#039;s username
APPROVED_DIRECTORY=...           # Base directory for project access
ALLOWED_USERS=123456789          # Comma-separated Telegram user IDs
```

### Common Options

```bash
# Claude
USE_SDK=true                     # Python SDK (default) or CLI subprocess
ANTHROPIC_API_KEY=sk-ant-...     # API key (optional if using CLI auth)
CLAUDE_MAX_COST_PER_USER=10.0    # Spending limit per user (USD)
CLAUDE_TIMEOUT_SECONDS=300       # Operation timeout

# Mode
AGENTIC_MODE=true                # Agentic (default) or classic mode
VERBOSE_LEVEL=1                  # 0=quiet, 1=normal (default), 2=detailed

# Rate Limiting
RATE_LIMIT_REQUESTS=10           # Requests per window
RATE_LIMIT_WINDOW=60             # Window in seconds

# Features (classic mode)
ENABLE_GIT_INTEGRATION=true
ENABLE_FILE_UPLOADS=true
ENABLE_QUICK_ACTIONS=true
```

### Agentic Platform

```bash
# Webhook API Server
ENABLE_API_SERVER=false          # Enable FastAPI webhook server
API_SERVER_PORT=8080             # Server port

# Webhook Authentication
GITHUB_WEBHOOK_SECRET=...        # GitHub HMAC-SHA256 secret
WEBHOOK_API_SECRET=...           # Bearer token for generic providers

# Scheduler
ENABLE_SCHEDULER=false           # Enable cron job scheduler

# Notifications
NOTIFICATION_CHAT_IDS=123,456    # Default chat IDs for proactive notifications
```

### Project Threads Mode

```bash
# Enable strict topic routing by project
ENABLE_PROJECT_THREADS=true

# Mode: private (default) or group
PROJECT_THREADS_MODE=private

# YAML registry file (see config/projects.example.yaml)
PROJECTS_CONFIG_PATH=config/projects.yaml

# Required only when PROJECT_THREADS_MODE=group
PROJECT_THREADS_CHAT_ID=-1001234567890
```

In strict mode, only `/start` and `/sync_threads` work outside mapped project topics.
In private mode, `/start` auto-syncs project topics for your private bot chat.
To use topics with your bot, enable them in BotFather:
`Bot Settings -&gt; Threaded mode`.

&gt; **Full reference:** See [docs/configuration.md](docs/configuration.md) and [`.env.example`](.env.example).

### Finding Your Telegram User ID

Message [@userinfobot](https://t.me/userinfobot) on Telegram -- it will reply with your user ID number.

## Troubleshooting

**Bot doesn&#039;t respond:**
- Check your `TELEGRAM_BOT_TOKEN` is correct
- Verify your user ID is in `ALLOWED_USERS`
- Ensure Claude Code CLI is installed and accessible
- Check bot logs with `make run-debug`

**Claude integration not working:**
- SDK mode (default): Check `claude auth status` or verify `ANTHROPIC_API_KEY`
- CLI mode: Verify `claude --version` and `claude auth status`
- Check `CLAUDE_ALLOWED_TOOLS` includes necessary tools

**High usage costs:**
- Adjust `CLAUDE_MAX_COST_PER_USER` to set spending limits
- Monitor usage with `/status`
- Use shorter, more focused requests

## Security

This bot implements defense-in-depth security:

- **Access Control** -- Whitelist-based user authentication
- **Directory Isolation** -- Sandboxing to approved directories
- **Rate Limiting** -- Request and cost-based limits
- **Input Validation** -- Injection and path traversal protection
- **Webhook Authentication** -- GitHub HMAC-SHA256 and Bearer token verification
- **Audit Logging** -- Complete tracking of all user actions

See [SECURITY.md](SECURITY.md) for details.

## Development

```bash
make dev           # Install all dependencies
make test          # Run tests with coverage
make lint          # Black + isort + flake8 + mypy
make format        # Auto-format code
make run-debug     # Run with debug logging
```

### Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Make changes with tests: `make test &amp;&amp; make lint`
4. Submit a Pull Request

**Code standards:** Python 3.10+, Black formatting (88 chars), type hints required, pytest with &gt;85% coverage.

## License

MIT License -- see [LICENSE](LICENSE).

## Acknowledgments

- [Claude](https://claude.ai) by Anthropic
- [python-telegram-bot](https://github.com/python-telegram-bot/python-telegram-bot)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[freemocap/freemocap]]></title>
            <link>https://github.com/freemocap/freemocap</link>
            <guid>https://github.com/freemocap/freemocap</guid>
            <pubDate>Fri, 20 Feb 2026 00:06:53 GMT</pubDate>
            <description><![CDATA[Free Motion Capture for Everyone ğŸ’€âœ¨]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/freemocap/freemocap">freemocap/freemocap</a></h1>
            <p>Free Motion Capture for Everyone ğŸ’€âœ¨</p>
            <p>Language: Python</p>
            <p>Stars: 5,190</p>
            <p>Forks: 417</p>
            <p>Stars today: 141 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/freemocap/freemocap/assets/15314521/da1af7fe-f808-43dc-8f59-c579715d6593&quot; height=&quot;240&quot; alt=&quot;Project Logo&quot;&gt;
&lt;/p&gt; 


&lt;h3 align=&quot;center&quot;&gt;The FreeMoCap Project&lt;/h3&gt;
&lt;h4 align=&quot;center&quot;&gt; A free-and-open-source, hardware-and-software-agnostic, minimal-cost, research-grade, motion capture
system and platform for decentralized scientific research, education, and training&lt;/h2&gt;


&lt;p align=&quot;center&quot;&gt;

&lt;a href=&quot;https://doi.org/10.5281/zenodo.7233714&quot;&gt;
    &lt;img src=&quot;https://zenodo.org/badge/DOI/10.5281/zenodo.7233714.svg&quot; alt=DOI-via-Zenodo.org&gt;
  &lt;/a&gt;

&lt;a href=&quot;https://github.com/psf/black&quot;&gt;
    &lt;img alt=&quot;https://img.shields.io/badge/code%20style-black-000000.svg&quot; src=&quot;https://img.shields.io/badge/code%20style-black-000000.svg&quot;&gt;
  &lt;/a&gt;

&lt;a href=&quot;https://github.com/freemocap/freemocap/releases/latest&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/github/release/freemocap/freemocap.svg&quot; alt=&quot;Latest Release&quot;&gt;
    &lt;/a&gt;

&lt;a href=&quot;https://github.com/freemocap/freemocap/blob/main/LICENSE&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/license-AGPL-blue.svg&quot; alt=&quot;AGPLv3&quot;&gt;
    &lt;/a&gt;

&lt;a href=&quot;https://github.com/freemocap/freemocap/issues&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/contributions-welcome-ff69b4.svg&quot; alt=&quot;Contributions Welcome&quot;&gt;
    &lt;/a&gt;

&lt;a href=&quot;https://github.com/psf/black&quot;&gt;
    &lt;img alt=&quot;https://img.shields.io/badge/code%20style-black-000000.svg&quot; src=&quot;https://img.shields.io/badge/code%20style-black-000000.svg&quot;&gt;
  &lt;/a&gt;

&lt;a href=&quot;https://discord.gg/SgdnzbHDTG&quot;&gt;
    &lt;img alt=&quot;Discord Community Server&quot; src=&quot;https://dcbadge.vercel.app/api/server/SgdnzbHDTG?style=flat&quot;&gt;
  &lt;/a&gt;


&lt;/p&gt;


https://user-images.githubusercontent.com/15314521/192062522-2a8d9305-f181-4869-a4b9-1aa068e094c9.mp4





--
## QUICKSTART

&gt; [!NOTE] 
&gt; For  detailed installation instructions, see our [official documentation&#039;s Installation page](https://freemocap.github.io/documentation/installation.html#detailed-pip-installation-instructions)


#### 0. Create a a Python 3.10 through 3.12 environment (python3.12 recommended)
#### 1. Install software via [pip](https://pypi.org/project/freemocap/#description):

```
pip install freemocap
```

#### 2. Launch the GUI by entering the command:

```
freemocap
``` 

####  3. A GUI should pop up that looks like this: 

   &lt;img width=&quot;1457&quot; alt=&quot;image&quot; src=&quot;https://github.com/freemocap/freemocap/assets/15314521/90ef7e7b-48f3-4f46-8d4a-5b5bcc3254b3&quot;&gt;

#### 4. Have fun! See the [Beginner Tutorials](https://freemocap.github.io/documentation/your-first-recording.html) on our official docs for detailed instructions.

#### 5. [Join the Discord and let us know how it went!](https://discord.gg/nxv5dNTfKT)



___
## Install/run from source code (i.e. the code in this repo)

Open an [Anaconda-enabled command prompt](https://www.anaconda.org) (or your preferred method of environment management) and enter the following commands:

1) Create a `Python` environment (Recommended version  is `python3.11`)

```bash
conda create -n freemocap-env python=3.11
```

2) Activate that newly created environment

```bash
conda activate freemocap-env
```

3) Clone the repository

```bash
git clone https://github.com/freemocap/freemocap
```

4) Navigate into the newly cloned/downloaded `freemocap` folder

```bash
cd freemocap
```

5) Install the package via the `pyproject.toml` file

```bash
pip install -e .
```

6) Launch the GUI (via the `freemocap.__main__.py` entry point)

```bash
python -m freemocap
```

A GUI should pop up!

___

## Documentation 

Our documentation is hosted at: https://freemocap.github.io/documentation

That site is built using `writerside` from this repository: https://github.com/freemocap/documentation

___



### Contribution Guidelines

Please read our contribution doc: [CONTRIBUTING.md](CONTRIBUTING.md)


## Related

[//]: # (* [project-name]&amp;#40;#&amp;#41; - Project description)

## Maintainers

* [Jon Matthis](https://github.com/jonmatthis)
* [Endurance Idehen](https://github.com/endurance)

## License

This project is licensed under the APGL License - see the [LICENSE](LICENSE) file for details.

If the AGPL does not work for your needs, we are happy to discuss terms to license this software to you with a different
agreement at a price point that increases exponentially as you
move [spiritually](https://www.gnu.org/philosophy/open-source-misses-the-point.en.html) away from the `AGPL`

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[p-e-w/heretic]]></title>
            <link>https://github.com/p-e-w/heretic</link>
            <guid>https://github.com/p-e-w/heretic</guid>
            <pubDate>Fri, 20 Feb 2026 00:06:52 GMT</pubDate>
            <description><![CDATA[Fully automatic censorship removal for language models]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/p-e-w/heretic">p-e-w/heretic</a></h1>
            <p>Fully automatic censorship removal for language models</p>
            <p>Language: Python</p>
            <p>Stars: 8,479</p>
            <p>Forks: 848</p>
            <p>Stars today: 652 stars today</p>
            <h2>README</h2><pre>&lt;img width=&quot;128&quot; height=&quot;128&quot; align=&quot;right&quot; alt=&quot;Logo&quot; src=&quot;https://github.com/user-attachments/assets/df5f2840-2f92-4991-aa57-252747d7182e&quot; /&gt;

# Heretic: Fully automatic censorship removal for language models&lt;br&gt;&lt;br&gt;[![Discord](https://img.shields.io/discord/1447831134212984903?color=5865F2&amp;label=discord&amp;labelColor=black&amp;logo=discord&amp;logoColor=white&amp;style=for-the-badge)](https://discord.gg/gdXc48gSyT) [![Follow us on Hugging Face](https://huggingface.co/datasets/huggingface/badges/resolve/main/follow-us-on-hf-md-dark.svg)](https://huggingface.co/heretic-org)

Heretic is a tool that removes censorship (aka &quot;safety alignment&quot;) from
transformer-based language models without expensive post-training.
It combines an advanced implementation of directional ablation, also known
as &quot;abliteration&quot; ([Arditi et al. 2024](https://arxiv.org/abs/2406.11717),
Lai 2025 ([1](https://huggingface.co/blog/grimjim/projected-abliteration),
[2](https://huggingface.co/blog/grimjim/norm-preserving-biprojected-abliteration))),
with a TPE-based parameter optimizer powered by [Optuna](https://optuna.org/).

This approach enables Heretic to work **completely automatically.** Heretic
finds high-quality abliteration parameters by co-minimizing the number of
refusals and the KL divergence from the original model. This results in a
decensored model that retains as much of the original model&#039;s intelligence
as possible. Using Heretic does not require an understanding of transformer
internals. In fact, anyone who knows how to run a command-line program
can use Heretic to decensor language models.

&lt;img width=&quot;650&quot; height=&quot;715&quot; alt=&quot;Screenshot&quot; src=&quot;https://github.com/user-attachments/assets/d71a5efa-d6be-4705-a817-63332afb2d15&quot; /&gt;

&amp;nbsp;

Running unsupervised with the default configuration, Heretic can produce
decensored models that rival the quality of abliterations created manually
by human experts:

| Model | Refusals for &quot;harmful&quot; prompts | KL divergence from original model for &quot;harmless&quot; prompts |
| :--- | ---: | ---: |
| [google/gemma-3-12b-it](https://huggingface.co/google/gemma-3-12b-it) (original) | 97/100 | 0 *(by definition)* |
| [mlabonne/gemma-3-12b-it-abliterated-v2](https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2) | 3/100 | 1.04 |
| [huihui-ai/gemma-3-12b-it-abliterated](https://huggingface.co/huihui-ai/gemma-3-12b-it-abliterated) | 3/100 | 0.45 |
| **[p-e-w/gemma-3-12b-it-heretic](https://huggingface.co/p-e-w/gemma-3-12b-it-heretic) (ours)** | **3/100** | **0.16** |

The Heretic version, generated without any human effort, achieves the same
level of refusal suppression as other abliterations, but at a much lower
KL divergence, indicating less damage to the original model&#039;s capabilities.
*(You can reproduce those numbers using Heretic&#039;s built-in evaluation functionality,
e.g. `heretic --model google/gemma-3-12b-it --evaluate-model p-e-w/gemma-3-12b-it-heretic`.
Note that the exact values might be platform- and hardware-dependent.
The table above was compiled using PyTorch 2.8 on an RTX 5090.)*

Of course, mathematical metrics and automated benchmarks never tell the whole
story, and are no substitute for human evaluation. Models generated with
Heretic have been well-received by users (links and emphasis added):

&gt; &quot;I was skeptical before, but I just downloaded
&gt; [**GPT-OSS 20B Heretic**](https://huggingface.co/p-e-w/gpt-oss-20b-heretic)
&gt; model and holy shit. It gives properly formatted long responses to sensitive topics,
&gt; using the exact uncensored words that you would expect from an uncensored model,
&gt; produces markdown format tables with details and whatnot. Looks like this is
&gt; the best abliterated version of this model so far...&quot;
&gt; [*(Link to comment)*](https://old.reddit.com/r/LocalLLaMA/comments/1oymku1/heretic_fully_automatic_censorship_removal_for/np6tba6/)

&gt; &quot;[**Heretic GPT 20b**](https://huggingface.co/p-e-w/gpt-oss-20b-heretic)
&gt; seems to be the best uncensored model I have tried yet. It doesn&#039;t destroy a
&gt; the model&#039;s intelligence and it is answering prompts normally would be
&gt; rejected by the base model.&quot;
&gt; [*(Link to comment)*](https://old.reddit.com/r/LocalLLaMA/comments/1oymku1/heretic_fully_automatic_censorship_removal_for/npe9jng/)

&gt; &quot;[[**Qwen3-4B-Instruct-2507-heretic**](https://huggingface.co/p-e-w/Qwen3-4B-Instruct-2507-heretic)]
&gt; Has been the best unquantized abliterated model that I have been able to run on 16gb vram.&quot;
&gt; [*(Link to comment)*](https://old.reddit.com/r/LocalLLaMA/comments/1phjxca/im_calling_these_people_out_right_now/nt06tji/)

Heretic supports most dense models, including many multimodal models, and
several different MoE architectures. It does not yet support SSMs/hybrid models,
models with inhomogeneous layers, and certain novel attention systems.

You can find a small collection of models that have been decensored using Heretic
[on Hugging Face](https://huggingface.co/collections/p-e-w/the-bestiary),
and the community has created and published
[well over 1,000](https://huggingface.co/models?other=heretic)
Heretic models in addition to those.


## Usage

Prepare a Python 3.10+ environment with PyTorch 2.2+ installed as appropriate
for your hardware. Then run:

```
pip install -U heretic-llm
heretic Qwen/Qwen3-4B-Instruct-2507
```

Replace `Qwen/Qwen3-4B-Instruct-2507` with whatever model you want to decensor.

The process is fully automatic and does not require configuration; however,
Heretic has a variety of configuration parameters that can be changed for
greater control. Run `heretic --help` to see available command-line options,
or look at [`config.default.toml`](config.default.toml) if you prefer to use
a configuration file.

At the start of a program run, Heretic benchmarks the system to determine
the optimal batch size to make the most of the available hardware.
On an RTX 3090, with the default configuration, decensoring Llama-3.1-8B-Instruct
takes about 45 minutes. Note that Heretic supports model quantization with
bitsandbytes, which can drastically reduce the amount of VRAM required to process
models. Set the `quantization` option to `bnb_4bit` to enable quantization.

After Heretic has finished decensoring a model, you are given the option to
save the model, upload it to Hugging Face, chat with it to test how well it works,
or any combination of those actions.


## Research features

In addition to its primary function of removing model censorship, Heretic also
provides features designed to support research into the semantics of model internals
(interpretability). To use those features, you need to install Heretic with the
optional `research` extra:

```
pip install -U heretic-llm[research]
```

This gives you access to the following functionality:

### Generate plots of residual vectors by passing `--plot-residuals`

When run with this flag, Heretic will:

1. Compute residual vectors (hidden states) for the first output token,
   for each transformer layer, for both &quot;harmful&quot; and &quot;harmless&quot; prompts.
2. Perform a [PaCMAP projection](https://github.com/YingfanWang/PaCMAP)
   from residual space to 2D-space.
3. Left-right align the projections of &quot;harmful&quot;/&quot;harmless&quot; residuals
   by their geometric medians to make projections for consecutive layers
   more similar. Additionally, PaCMAP is initialized with the previous
   layer&#039;s projections for each new layer, minimizing disruptive transitions.
4. Scatter-plot the projections, generating a PNG image for each layer.
5. Generate an animation showing how residuals transform between layers,
   as an animated GIF.

&lt;img width=&quot;800&quot; height=&quot;600&quot; alt=&quot;Plot of residual vectors&quot; src=&quot;https://github.com/user-attachments/assets/981aa6ed-5ab9-48f0-9abf-2b1a2c430295&quot; /&gt;

See [the configuration file](config.default.toml) for options that allow you
to control various aspects of the generated plots.

Note that PaCMAP is an expensive operation that is performed on the CPU.
For larger models, it can take an hour or more to compute projections
for all layers.

### Print details about residual geometry by passing `--print-residual-geometry`

If you are interested in a quantitative analysis of how residual vectors
for &quot;harmful&quot; and &quot;harmless&quot; prompts relate to each other, this flag gives you
the following table, packed with metrics that can facilitate understanding
the same (for [gemma-3-270m-it](https://huggingface.co/google/gemma-3-270m-it)
in this case):

```
â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Layer â”ƒ S(g,b) â”ƒ S(g*,b*) â”ƒ  S(g,r) â”ƒ S(g*,r*) â”ƒ  S(b,r) â”ƒ S(b*,r*) â”ƒ      |g| â”ƒ     |g*| â”ƒ      |b| â”ƒ     |b*| â”ƒ     |r| â”ƒ    |r*| â”ƒ   Silh â”ƒ
â”¡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚     1 â”‚ 1.0000 â”‚   1.0000 â”‚ -0.4311 â”‚  -0.4906 â”‚ -0.4254 â”‚  -0.4847 â”‚   170.29 â”‚   170.49 â”‚   169.78 â”‚   169.85 â”‚    1.19 â”‚    1.31 â”‚ 0.0480 â”‚
â”‚     2 â”‚ 1.0000 â”‚   1.0000 â”‚  0.4297 â”‚   0.4465 â”‚  0.4365 â”‚   0.4524 â”‚   768.55 â”‚   768.77 â”‚   771.32 â”‚   771.36 â”‚    6.39 â”‚    5.76 â”‚ 0.0745 â”‚
â”‚     3 â”‚ 0.9999 â”‚   1.0000 â”‚ -0.5699 â”‚  -0.5577 â”‚ -0.5614 â”‚  -0.5498 â”‚  1020.98 â”‚  1021.13 â”‚  1013.80 â”‚  1014.71 â”‚   12.70 â”‚   11.60 â”‚ 0.0920 â”‚
â”‚     4 â”‚ 0.9999 â”‚   1.0000 â”‚  0.6582 â”‚   0.6553 â”‚  0.6659 â”‚   0.6627 â”‚  1356.39 â”‚  1356.20 â”‚  1368.71 â”‚  1367.95 â”‚   18.62 â”‚   17.84 â”‚ 0.0957 â”‚
â”‚     5 â”‚ 0.9987 â”‚   0.9990 â”‚ -0.6880 â”‚  -0.6761 â”‚ -0.6497 â”‚  -0.6418 â”‚   766.54 â”‚   762.25 â”‚   731.75 â”‚   732.42 â”‚   51.97 â”‚   45.24 â”‚ 0.1018 â”‚
â”‚     6 â”‚ 0.9998 â”‚   0.9998 â”‚ -0.1983 â”‚  -0.2312 â”‚ -0.1811 â”‚  -0.2141 â”‚  2417.35 â”‚  2421.08 â”‚  2409.18 â”‚  2411.40 â”‚   43.06 â”‚   43.47 â”‚ 0.0900 â”‚
â”‚     7 â”‚ 0.9998 â”‚   0.9997 â”‚ -0.5258 â”‚  -0.5746 â”‚ -0.5072 â”‚  -0.5560 â”‚  3444.92 â”‚  3474.99 â”‚  3400.01 â”‚  3421.63 â”‚   86.94 â”‚   94.38 â”‚ 0.0492 â”‚
â”‚     8 â”‚ 0.9990 â”‚   0.9991 â”‚  0.8235 â”‚   0.8312 â”‚  0.8479 â”‚   0.8542 â”‚  4596.54 â”‚  4615.62 â”‚  4918.32 â”‚  4934.20 â”‚  384.87 â”‚  377.87 â”‚ 0.2278 â”‚
â”‚     9 â”‚ 0.9992 â”‚   0.9992 â”‚  0.5335 â”‚   0.5441 â”‚  0.5678 â”‚   0.5780 â”‚  5322.30 â”‚  5316.96 â”‚  5468.65 â”‚  5466.98 â”‚  265.68 â”‚  267.28 â”‚ 0.1318 â”‚
â”‚    10 â”‚ 0.9974 â”‚   0.9973 â”‚  0.8189 â”‚   0.8250 â”‚  0.8579 â”‚   0.8644 â”‚  5328.81 â”‚  5325.63 â”‚  5953.35 â”‚  5985.15 â”‚  743.95 â”‚  779.74 â”‚ 0.2863 â”‚
â”‚    11 â”‚ 0.9977 â”‚   0.9978 â”‚  0.4262 â”‚   0.4045 â”‚  0.4862 â”‚   0.4645 â”‚  9644.02 â”‚  9674.06 â”‚  9983.47 â”‚  9990.28 â”‚  743.28 â”‚  726.99 â”‚ 0.1576 â”‚
â”‚    12 â”‚ 0.9904 â”‚   0.9907 â”‚  0.4384 â”‚   0.4077 â”‚  0.5586 â”‚   0.5283 â”‚ 10257.40 â”‚ 10368.50 â”‚ 11114.51 â”‚ 11151.21 â”‚ 1711.18 â”‚ 1664.69 â”‚ 0.1890 â”‚
â”‚    13 â”‚ 0.9867 â”‚   0.9874 â”‚  0.4007 â”‚   0.3680 â”‚  0.5444 â”‚   0.5103 â”‚ 12305.12 â”‚ 12423.75 â”‚ 13440.31 â”‚ 13432.47 â”‚ 2386.43 â”‚ 2282.47 â”‚ 0.1293 â”‚
â”‚    14 â”‚ 0.9921 â”‚   0.9922 â”‚  0.3198 â”‚   0.2682 â”‚  0.4364 â”‚   0.3859 â”‚ 16929.16 â”‚ 17080.37 â”‚ 17826.97 â”‚ 17836.03 â”‚ 2365.23 â”‚ 2301.87 â”‚ 0.1282 â”‚
â”‚    15 â”‚ 0.9846 â”‚   0.9850 â”‚  0.1198 â”‚   0.0963 â”‚  0.2913 â”‚   0.2663 â”‚ 16858.58 â”‚ 16949.44 â”‚ 17496.00 â”‚ 17502.88 â”‚ 3077.08 â”‚ 3029.60 â”‚ 0.1611 â”‚
â”‚    16 â”‚ 0.9686 â”‚   0.9689 â”‚ -0.0029 â”‚  -0.0254 â”‚  0.2457 â”‚   0.2226 â”‚ 18912.77 â”‚ 19074.86 â”‚ 19510.56 â”‚ 19559.62 â”‚ 4848.35 â”‚ 4839.75 â”‚ 0.1516 â”‚
â”‚    17 â”‚ 0.9782 â”‚   0.9784 â”‚ -0.0174 â”‚  -0.0381 â”‚  0.1908 â”‚   0.1694 â”‚ 27098.09 â”‚ 27273.00 â”‚ 27601.12 â”‚ 27653.12 â”‚ 5738.19 â”‚ 5724.21 â”‚ 0.1641 â”‚
â”‚    18 â”‚ 0.9184 â”‚   0.9196 â”‚  0.1343 â”‚   0.1430 â”‚  0.5155 â”‚   0.5204 â”‚   190.16 â”‚   190.35 â”‚   219.91 â”‚   220.62 â”‚   87.82 â”‚   87.59 â”‚ 0.1855 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
g = mean of residual vectors for good prompts
g* = geometric median of residual vectors for good prompts
b = mean of residual vectors for bad prompts
b* = geometric median of residual vectors for bad prompts
r = refusal direction for means (i.e., b - g)
r* = refusal direction for geometric medians (i.e., b* - g*)
S(x,y) = cosine similarity of x and y
|x| = L2 norm of x
Silh = Mean silhouette coefficient of residuals for good/bad clusters
```


## How Heretic works

Heretic implements a parametrized variant of directional ablation. For each
supported transformer component (currently, attention out-projection and
MLP down-projection), it identifies the associated matrices in each transformer
layer, and orthogonalizes them with respect to the relevant &quot;refusal direction&quot;,
inhibiting the expression of that direction in the result of multiplications
with that matrix.

Refusal directions are computed for each layer as a difference-of-means between
the first-token residuals for &quot;harmful&quot; and &quot;harmless&quot; example prompts.

The ablation process is controlled by several optimizable parameters:

* `direction_index`: Either the index of a refusal direction, or the special
  value `per layer`, indicating that each layer should be ablated using the
  refusal direction associated with that layer.
* `max_weight`, `max_weight_position`, `min_weight`, and `min_weight_distance`:
  For each component, these parameters describe the shape and position of the
  ablation weight kernel over the layers. The following diagram illustrates this:

&lt;img width=&quot;800&quot; height=&quot;500&quot; alt=&quot;Explanation&quot; src=&quot;https://github.com/user-attachments/assets/82e4b84e-5a82-4faf-b918-ac642f9e4892&quot; /&gt;

&amp;nbsp;

Heretic&#039;s main innovations over existing abliteration systems are:

* The shape of the ablation weight kernel is highly flexible, which, combined with
  automatic parameter optimization, can improve the compliance/quality tradeoff.
  Non-constant ablation weights were previously explored by Maxime Labonne in
  [gemma-3-12b-it-abliterated-v2](https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2).
* The refusal direction index is a float rather than an integer. For non-integral
  values, the two nearest refusal direction vectors are linearly interpolated.
  This unlocks a vast space of additional directions beyond the ones identified
  by the difference-of-means computation, and often enables the optimization
  process to find a better direction than that belonging to any individual layer.
* Ablation parameters are chosen separately for each component. I have found that
  MLP interventions tend to be more damaging to the model than attention interventions,
  so using different ablation weights can squeeze out some extra performance.


## Prior art

I&#039;m aware of the following publicly available implementations of abliteration
techniques:

* [AutoAbliteration](https://huggingface.co/posts/mlabonne/714992455492422)
* [abliterator.py](https://github.com/FailSpy/abliterator)
* [wassname&#039;s Abliterator](https://github.com/wassname/abliterator)
* [ErisForge](https://github.com/Tsadoq/ErisForge)
* [Removing refusals with HF Transformers](https://github.com/Sumandora/remove-refusals-with-transformers)
* [deccp](https://github.com/AUGMXNT/deccp)

Note that Heretic was written from scratch, and does not reuse code from
any of those projects.


## Acknowledgments

The development of Heretic was informed by:

* [The original abliteration paper (Arditi et al. 2024)](https://arxiv.org/abs/2406.11717)
* [Maxime Labonne&#039;s article on abliteration](https://huggingface.co/blog/mlabonne/abliteration),
  as well as some details from the model cards of his own abliterated models (see above)
* Jim Lai&#039;s articles describing [&quot;projected abliteration&quot;](https://huggingface.co/blog/grimjim/projected-abliteration)
  and [&quot;norm-preserving biprojected abliteration&quot;](https://huggingface.co/blog/grimjim/norm-preserving-biprojected-abliteration)


## Citation

If you use Heretic for your research, please cite it using the following BibTeX entry:

```bibtex
@misc{heretic,
  author = {Weidmann, Philipp Emanuel},
  title = {Heretic: Fully automatic censorship removal for language models},
  year = {2025},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/p-e-w/heretic}}
}
```


## License

Copyright &amp;copy; 2025-2026  Philipp Emanuel Weidmann (&lt;pew@worldwidemann.com&gt;) + contributors

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU Affero General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Affero General Public License for more details.

You should have received a copy of the GNU Affero General Public License
along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.

**By contributing to this project, you agree to release your
contributions under the same license.**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[exo-explore/exo]]></title>
            <link>https://github.com/exo-explore/exo</link>
            <guid>https://github.com/exo-explore/exo</guid>
            <pubDate>Fri, 20 Feb 2026 00:06:51 GMT</pubDate>
            <description><![CDATA[Run frontier AI locally.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/exo-explore/exo">exo-explore/exo</a></h1>
            <p>Run frontier AI locally.</p>
            <p>Language: Python</p>
            <p>Stars: 41,611</p>
            <p>Forks: 2,830</p>
            <p>Stars today: 59 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;/docs/imgs/exo-logo-black-bg.jpg&quot;&gt;
  &lt;img alt=&quot;exo logo&quot; src=&quot;/docs/imgs/exo-logo-transparent.png&quot; width=&quot;50%&quot; height=&quot;50%&quot;&gt;
&lt;/picture&gt;

exo: Run frontier AI locally. Maintained by [exo labs](https://x.com/exolabs).

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://discord.gg/TJ4P57arEm&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20Server-5865F2?logo=discord&amp;logoColor=white&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://x.com/exolabs&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/exolabs?style=social&quot; alt=&quot;X&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0.html&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-Apache2.0-blue.svg&quot; alt=&quot;License: Apache-2.0&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;/div&gt;

---

exo connects all your devices into an AI cluster. Not only does exo enable running models larger than would fit on a single device, but with [day-0 support for RDMA over Thunderbolt](https://x.com/exolabs/status/2001817749744476256?s=20), makes models run faster as you add more devices.

## Features

- **Automatic Device Discovery**: Devices running exo automatically discover each other - no manual configuration.
- **RDMA over Thunderbolt**: exo ships with [day-0 support for RDMA over Thunderbolt 5](https://x.com/exolabs/status/2001817749744476256?s=20), enabling 99% reduction in latency between devices.
- **Topology-Aware Auto Parallel**: exo figures out the best way to split your model across all available devices based on a realtime view of your device topology. It takes into account device resources and network latency/bandwidth between each link.
- **Tensor Parallelism**: exo supports sharding models, for up to 1.8x speedup on 2 devices and 3.2x speedup on 4 devices.
- **MLX Support**: exo uses [MLX](https://github.com/ml-explore/mlx) as an inference backend and [MLX distributed](https://ml-explore.github.io/mlx/build/html/usage/distributed.html) for distributed communication.

## Dashboard

exo includes a built-in dashboard for managing your cluster and chatting with models.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/imgs/dashboard-cluster-view.png&quot; alt=&quot;exo dashboard - cluster view showing 4 x M3 Ultra Mac Studio with DeepSeek v3.1 and Kimi-K2-Thinking loaded&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;&lt;em&gt;4 Ã— 512GB M3 Ultra Mac Studio running DeepSeek v3.1 (8-bit) and Kimi-K2-Thinking (4-bit)&lt;/em&gt;&lt;/p&gt;

## Benchmarks

&lt;details&gt;
  &lt;summary&gt;Qwen3-235B (8-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt;
  &lt;img src=&quot;docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-1-qwen3-235b.jpeg&quot; alt=&quot;Benchmark - Qwen3-235B (8-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&quot; width=&quot;80%&quot; /&gt;
  &lt;p&gt;
    &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href=&quot;https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5&quot;&gt;Jeff Geerling: 15 TB VRAM on Mac Studio â€“ RDMA over Thunderbolt 5&lt;/a&gt;
  &lt;/p&gt;
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;DeepSeek v3.1 671B (8-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt;
  &lt;img src=&quot;docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-2-deepseek-3.1-671b.jpeg&quot; alt=&quot;Benchmark - DeepSeek v3.1 671B (8-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&quot; width=&quot;80%&quot; /&gt;
  &lt;p&gt;
    &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href=&quot;https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5&quot;&gt;Jeff Geerling: 15 TB VRAM on Mac Studio â€“ RDMA over Thunderbolt 5&lt;/a&gt;
  &lt;/p&gt;
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;Kimi K2 Thinking (native 4-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt;
  &lt;img src=&quot;docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-3-kimi-k2-thinking.jpeg&quot; alt=&quot;Benchmark - Kimi K2 Thinking (native 4-bit) on 4 Ã— M3 Ultra Mac Studio with Tensor Parallel RDMA&quot; width=&quot;80%&quot; /&gt;
  &lt;p&gt;
    &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href=&quot;https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5&quot;&gt;Jeff Geerling: 15 TB VRAM on Mac Studio â€“ RDMA over Thunderbolt 5&lt;/a&gt;
  &lt;/p&gt;
&lt;/details&gt;

---

## Quick Start

Devices running exo automatically discover each other, without needing any manual configuration. Each device provides an API and a dashboard for interacting with your cluster (runs at `http://localhost:52415`).

There are two ways to run exo:

### Run from Source (macOS)

If you have [Nix](https://nixos.org/) installed, you can skip most of the steps below and run exo directly:

```bash
nix run .#exo
```

**Note:** To accept the Cachix binary cache (and avoid the Xcode Metal ToolChain), add to `/etc/nix/nix.conf`:
```
trusted-users = root    (or your username)
experimental-features = nix-command flakes
```
Then restart the Nix daemon: `sudo launchctl kickstart -k system/org.nixos.nix-daemon`

**Prerequisites:**
- [Xcode](https://developer.apple.com/xcode/) (provides the Metal ToolChain required for MLX compilation)
- [brew](https://github.com/Homebrew/brew) (for simple package management on macOS)

  ```bash
  /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot;
  ```
- [uv](https://github.com/astral-sh/uv) (for Python dependency management)
- [macmon](https://github.com/vladkens/macmon) (for hardware monitoring on Apple Silicon)
- [node](https://github.com/nodejs/node) (for building the dashboard)

  ```bash
  brew install uv macmon node
  ```
- [rust](https://github.com/rust-lang/rustup) (to build Rust bindings, nightly for now)

  ```bash
  curl --proto &#039;=https&#039; --tlsv1.2 -sSf https://sh.rustup.rs | sh
  rustup toolchain install nightly
  ```

Clone the repo, build the dashboard, and run exo:

```bash
# Clone exo
git clone https://github.com/exo-explore/exo

# Build dashboard
cd exo/dashboard &amp;&amp; npm install &amp;&amp; npm run build &amp;&amp; cd ..

# Run exo
uv run exo
```

This starts the exo dashboard and API at http://localhost:52415/


*Please view the section on RDMA to enable this feature on MacOS &gt;=26.2!*


### Run from Source (Linux)

**Prerequisites:**

- [uv](https://github.com/astral-sh/uv) (for Python dependency management)
- [node](https://github.com/nodejs/node) (for building the dashboard) - version 18 or higher
- [rust](https://github.com/rust-lang/rustup) (to build Rust bindings, nightly for now)

**Installation methods:**

**Option 1: Using system package manager (Ubuntu/Debian example):**
```bash
# Install Node.js and npm
sudo apt update
sudo apt install nodejs npm

# Install uv
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install Rust (using rustup)
curl --proto &#039;=https&#039; --tlsv1.2 -sSf https://sh.rustup.rs | sh
rustup toolchain install nightly
```

**Option 2: Using Homebrew on Linux (if preferred):**
```bash
# Install Homebrew on Linux
/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot;

# Install dependencies
brew install uv node

# Install Rust (using rustup)
curl --proto &#039;=https&#039; --tlsv1.2 -sSf https://sh.rustup.rs | sh
rustup toolchain install nightly
```

**Note:** The `macmon` package is macOS-only and not required for Linux.

Clone the repo, build the dashboard, and run exo:

```bash
# Clone exo
git clone https://github.com/exo-explore/exo

# Build dashboard
cd exo/dashboard &amp;&amp; npm install &amp;&amp; npm run build &amp;&amp; cd ..

# Run exo
uv run exo
```

This starts the exo dashboard and API at http://localhost:52415/

**Important note for Linux users:** Currently, exo runs on CPU on Linux. GPU support for Linux platforms is under development. If you&#039;d like to see support for your specific Linux hardware, please [search for existing feature requests](https://github.com/exo-explore/exo/issues) or create a new one.

**Configuration Options:**

- `--no-worker`: Run exo without the worker component. Useful for coordinator-only nodes that handle networking and orchestration but don&#039;t execute inference tasks. This is helpful for machines without sufficient GPU resources but with good network connectivity.

  ```bash
  uv run exo --no-worker
  ```

**File Locations (Linux):**

exo follows the [XDG Base Directory Specification](https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html) on Linux:

- **Configuration files**: `~/.config/exo/` (or `$XDG_CONFIG_HOME/exo/`)
- **Data files**: `~/.local/share/exo/` (or `$XDG_DATA_HOME/exo/`)
- **Cache files**: `~/.cache/exo/` (or `$XDG_CACHE_HOME/exo/`)

You can override these locations by setting the corresponding XDG environment variables.

### macOS App

exo ships a macOS app that runs in the background on your Mac.

&lt;img src=&quot;docs/imgs/macos-app-one-macbook.png&quot; alt=&quot;exo macOS App - running on a MacBook&quot; width=&quot;35%&quot; /&gt;

The macOS app requires macOS Tahoe 26.2 or later.

Download the latest build here: [EXO-latest.dmg](https://assets.exolabs.net/EXO-latest.dmg).

The app will ask for permission to modify system settings and install a new Network profile. Improvements to this are being worked on.

**Custom Namespace for Cluster Isolation:**

The macOS app includes a custom namespace feature that allows you to isolate your exo cluster from others on the same network. This is configured through the `EXO_LIBP2P_NAMESPACE` setting:

- **Use cases**:
  - Running multiple separate exo clusters on the same network
  - Isolating development/testing clusters from production clusters
  - Preventing accidental cluster joining

- **Configuration**: Access this setting in the app&#039;s Advanced settings (or set the `EXO_LIBP2P_NAMESPACE` environment variable when running from source)

The namespace is logged on startup for debugging purposes.

#### Uninstalling the macOS App

The recommended way to uninstall is through the app itself: click the menu bar icon â†’ Advanced â†’ Uninstall. This cleanly removes all system components.

If you&#039;ve already deleted the app, you can run the standalone uninstaller script:

```bash
sudo ./app/EXO/uninstall-exo.sh
```

This removes:
- Network setup LaunchDaemon
- Network configuration script
- Log files
- The &quot;exo&quot; network location

**Note:** You&#039;ll need to manually remove EXO from Login Items in System Settings â†’ General â†’ Login Items.

---

### Enabling RDMA on macOS

RDMA is a new capability added to macOS 26.2. It works on any Mac with Thunderbolt 5 (M4 Pro Mac Mini, M4 Max Mac Studio, M4 Max MacBook Pro, M3 Ultra Mac Studio).

Please refer to the caveats for immediate troubleshooting.

To enable RDMA on macOS, follow these steps:

1. Shut down your Mac.
2. Hold down the power button for 10 seconds until the boot menu appears.
3. Select &quot;Options&quot; to enter Recovery mode.
4. When the Recovery UI appears, open the Terminal from the Utilities menu.
5. In the Terminal, type:
   ```
   rdma_ctl enable
   ```
   and press Enter.
6. Reboot your Mac.

After that, RDMA will be enabled in macOS and exo will take care of the rest.

**Important Caveats**

1. Devices that wish to be part of an RDMA cluster must be connected to all other devices in the cluster.
2. The cables must support TB5.
3. On a Mac Studio, you cannot use the Thunderbolt 5 port next to the Ethernet port.
4. If running from source, please use the script found at `tmp/set_rdma_network_config.sh`, which will disable Thunderbolt Bridge and set dhcp on each RDMA port.
5. RDMA ports may be unable to discover each other on different versions of MacOS. Please ensure that OS versions match exactly (even beta version numbers) on all devices.

---

### Using the API

If you prefer to interact with exo via the API, here is an example creating an instance of a small model (`mlx-community/Llama-3.2-1B-Instruct-4bit`), sending a chat completions request and deleting the instance.

---

**1. Preview instance placements**

The `/instance/previews` endpoint will preview all valid placements for your model.

```bash
curl &quot;http://localhost:52415/instance/previews?model_id=llama-3.2-1b&quot;
```

Sample response:

```json
{
  &quot;previews&quot;: [
    {
      &quot;model_id&quot;: &quot;mlx-community/Llama-3.2-1B-Instruct-4bit&quot;,
      &quot;sharding&quot;: &quot;Pipeline&quot;,
      &quot;instance_meta&quot;: &quot;MlxRing&quot;,
      &quot;instance&quot;: {...},
      &quot;memory_delta_by_node&quot;: {&quot;local&quot;: 729808896},
      &quot;error&quot;: null
    }
    // ...possibly more placements...
  ]
}
```

This will return all valid placements for this model. Pick a placement that you like.
To pick the first one, pipe into `jq`:

```bash
curl &quot;http://localhost:52415/instance/previews?model_id=llama-3.2-1b&quot; | jq -c &#039;.previews[] | select(.error == null) | .instance&#039; | head -n1
```

---

**2. Create a model instance**

Send a POST to `/instance` with your desired placement in the `instance` field (the full payload must match types as in `CreateInstanceParams`), which you can copy from step 1:

```bash
curl -X POST http://localhost:52415/instance \
  -H &#039;Content-Type: application/json&#039; \
  -d &#039;{
    &quot;instance&quot;: {...}
  }&#039;
```


Sample response:

```json
{
  &quot;message&quot;: &quot;Command received.&quot;,
  &quot;command_id&quot;: &quot;e9d1a8ab-....&quot;
}
```

---

**3. Send a chat completion**

Now, make a POST to `/v1/chat/completions` (the same format as OpenAI&#039;s API):

```bash
curl -N -X POST http://localhost:52415/v1/chat/completions \
  -H &#039;Content-Type: application/json&#039; \
  -d &#039;{
    &quot;model&quot;: &quot;mlx-community/Llama-3.2-1B-Instruct-4bit&quot;,
    &quot;messages&quot;: [
      {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is Llama 3.2 1B?&quot;}
    ],
    &quot;stream&quot;: true
  }&#039;
```

---

**4. Delete the instance**

When you&#039;re done, delete the instance by its ID (find it via `/state` or `/instance` endpoints):

```bash
curl -X DELETE http://localhost:52415/instance/YOUR_INSTANCE_ID
```

**Other useful API endpoints*:**

- List all models: `curl http://localhost:52415/models`
- Inspect instance IDs and deployment state: `curl http://localhost:52415/state`

For further details, see:

- API basic documentation in [docs/api.md](docs/api.md).
- API types and endpoints in [src/exo/master/api.py](src/exo/master/api.py).

---

## Benchmarking

The `exo-bench` tool measures model prefill and token generation speed across different placement configurations. This helps you optimize model performance and validate improvements.

**Prerequisites:**
- Nodes should be running with `uv run exo` before benchmarking
- The tool uses the `/bench/chat/completions` endpoint

**Basic usage:**

```bash
uv run bench/exo_bench.py \
  --model Llama-3.2-1B-Instruct-4bit \
  --pp 128,256,512 \
  --tg 128,256
```

**Key parameters:**

- `--model`: Model to benchmark (short ID or HuggingFace ID)
- `--pp`: Prompt size hints (comma-separated integers)
- `--tg`: Generation lengths (comma-separated integers)
- `--max-nodes`: Limit placements to N nodes (default: 4)
- `--instance-meta`: Filter by `ring`, `jaccl`, or `both` (default: both)
- `--sharding`: Filter by `pipeline`, `tensor`, or `both` (default: both)
- `--repeat`: Number of repetitions per configuration (default: 1)
- `--warmup`: Warmup runs per placement (default: 0)
- `--json-out`: Output file for results (default: bench/results.json)

**Example with filters:**

```bash
uv run bench/exo_bench.py \
  --model Llama-3.2-1B-Instruct-4bit \
  --pp 128,512 \
  --tg 128 \
  --max-nodes 2 \
  --sharding tensor \
  --repeat 3 \
  --json-out my-results.json
```

The tool outputs performance metrics including prompt tokens per second (prompt_tps), generation tokens per second (generation_tps), and peak memory usage for each configuration.

---

## Hardware Accelerator Support

On macOS, exo uses the GPU. On Linux, exo currently runs on CPU. We are working on extending hardware accelerator support. If you&#039;d like support for a new hardware platform, please [search for an existing feature request](https://github.com/exo-explore/exo/issues) and add a thumbs up so we know what hardware is important to the community.

---

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines on how to contribute to exo.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[GodsScion/Auto_job_applier_linkedIn]]></title>
            <link>https://github.com/GodsScion/Auto_job_applier_linkedIn</link>
            <guid>https://github.com/GodsScion/Auto_job_applier_linkedIn</guid>
            <pubDate>Fri, 20 Feb 2026 00:06:50 GMT</pubDate>
            <description><![CDATA[Make your job hunt easy by automating your application process with this Auto Applier]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/GodsScion/Auto_job_applier_linkedIn">GodsScion/Auto_job_applier_linkedIn</a></h1>
            <p>Make your job hunt easy by automating your application process with this Auto Applier</p>
            <p>Language: Python</p>
            <p>Stars: 1,627</p>
            <p>Forks: 458</p>
            <p>Stars today: 20 stars today</p>
            <h2>README</h2><pre># LinkedIn AI Auto Job Applier ğŸ¤–
This is an web scraping bot that automates the process of job applications on LinkedIn. It searches for jobs relevant to you, answers all questions in application form, customizes your resume based on the collected job information, such as skills required, description, about company, etc. and applies to the job. Can apply 100+ jobs in less than 1 hour. 


## ğŸ“½ï¸ See it in Action
[![Auto Job Applier demo video](https://github.com/GodsScion/Auto_job_applier_linkedIn/assets/100998531/429f7753-ebb0-499b-bc5e-5b4ee28c4f69)](https://youtu.be/gMbB1fWZDHw)
Click on above image to watch the demo or use this link https://youtu.be/gMbB1fWZDHw


## âœ¨ Content
- [Introduction](#linkedin-ai-auto-job-applier-)
- [Demo Video](#%EF%B8%8F-see-it-in-action)
- [Index](#-content)
- [Install](#%EF%B8%8F-how-to-install)
- [Configure](#-how-to-configure)
- [Contributor Guidelines](#â€-contributor-guidelines)
- [Updates](%EF%B8%8F-major-updates-history)
- [Disclaimer](#-disclaimer)
- [Terms and Conditions](#%EF%B8%8F-terms-and-conditions)
- [License](#%EF%B8%8F-license)
- [Socials](#-socials)
- [Support and Discussions](#-community-support-and-discussions)

&lt;br&gt;

## âš™ï¸ How to install

[![Auto Job Applier setup tutorial video](https://github.com/user-attachments/assets/9e876187-ed3e-4fbf-bd87-4acc145880a2)](https://youtu.be/f9rdz74e1lM?si=4fRBcte0nuvr6tEH)
Click on above image to watch the tutorial for installation and configuration or use this link https://youtu.be/f9rdz74e1lM (Recommended to watch it in 2x speed)

1. [Python 3.10](https://www.python.org/) or above. Visit https://www.python.org/downloads/ to download and install Python, or for windows you could visit Microsoft Store and search for &quot;Python&quot;. **Please make sure Python is added to Path in System Environment Variables**.
2. Install necessary [Undetected Chromedriver](https://pypi.org/project/undetected-chromedriver/), [PyAutoGUI](https://pypi.org/project/PyAutoGUI/) and [Setuptools](https://pypi.org/project/setuptools/) packages. After Python is installed, OPEN a console/terminal or shell, Use below command that uses the [pip](https://pip.pypa.io/en/stable) command-line tool to install these 3 package.
  ```
  pip install undetected-chromedriver pyautogui setuptools openai flask-cors flask
  ```
3. Download and install latest version of [Google Chrome](https://www.google.com/chrome) in it&#039;s default location, visit https://www.google.com/chrome to download it&#039;s installer.
4. Clone the current git repo or download it as a zip file, url to the latest update https://github.com/GodsScion/Auto_job_applier_linkedIn.
5. (Not needed if you set `stealth_mode = True` in `config/settings.py` ) Download and install the appropriate [Chrome Driver](https://googlechromelabs.github.io/chrome-for-testing/) for Google Chrome and paste it in the location Chrome was installed, visit https://googlechromelabs.github.io/chrome-for-testing/ to download.
  &lt;br&gt; &lt;br&gt;
  ***OR*** 
  &lt;br&gt; &lt;br&gt;
  If you are using Windows, click on `windows-setup.bat` available in the `/setup` folder, this will install the latest chromedriver automatically.
6. If you have questions or need help setting it up or to talk in general, join the github server: https://discord.gg/fFp7uUzWCY

[back to index](#-content)

&lt;br&gt;

## ğŸ”§ How to configure
1. Open `personals.py` file in `/config` folder and enter your details like name, phone number, address, etc. Whatever you want to fill in your applications.
2. Open `questions.py` file in `/config` folder and enter your answers for application questions, configure wether you want the bot to pause before submission or pause if it can&#039;t answer unknown questions.
3. Open `search.py` file in `/config` folder and enter your search preferences, job filters, configure the bot as per your needs (these settings decide which jobs to apply for or skip).
4. Open `secrets.py` file in `/config` folder and enter your LinkedIn username, password to login and OpenAI API Key for generation of job tailored resumes and cover letters (This entire step is optional). If you do not provide username or password or leave them as default, it will login with saved profile in browser, if failed will ask you to login manually.
5. Open `settings.py` file in `/config` folder to configure the bot settings like, keep screen awake, click intervals (click intervals are randomized to seem like human behavior), run in background, stealth mode (to avoid bot detection), etc. as per your needs.
6. (Optional) Don&#039;t forget to add you default resume in the location you mentioned in `default_resume_path = &quot;all resumes/default/resume.pdf&quot;` given in `/config/questions.py`. If one is not provided, it will use your previous resume submitted in LinkedIn or (In Development) generate custom resume if OpenAI APT key is provided!
7. Run `runAiBot.py` and see the magic happen.
8. To run the Applied Jobs history UI, run `app.py` and open web browser on `http://localhost:5000`.
8. If you have questions or need help setting it up or to talk in general, join the github server: https://discord.gg/fFp7uUzWCY

[back to index](#-content)

&lt;br&gt;


## ğŸ§‘â€ğŸ’» Contributor Guidelines
Thank you for your efforts and being a part of the community. All contributions are appreciated no matter how small or big. Once you contribute to the code base, your work will be remembered forever.

NOTE: Only Pull request to `community-version` branch will be accepted. Any other requests will be declined by default, especially to main branch.
Once your code is tested, your changes will be merged to the `main` branch in next cycle.

### Code Guidelines
  #### Functions:
  1. All functions or methods are named lower case and snake case
  2. Must have explanation of their purpose. Write explanation surrounded in `&#039;&#039;&#039; Explanation &#039;&#039;&#039;` under the definition `def function() -&gt; None:`. Example:
      ```python
      def function() -&gt; None:
        &#039;&#039;&#039;
        This function does nothing, it&#039;s just an example for explanation placement!
        &#039;&#039;&#039;
      ```
  4. The Types `(str, list, int, list[str], int | float)` for the parameters and returns must be given. Example:
      ```python
      def function(param1: str, param2: list[str], param3: int) -&gt; str:
      ```
  5. Putting all that together some valid examples for function or method declarations would be as follows.
      ```python
      def function_name_in_camel_case(parameter1: driver, parameter2: str) -&gt; list[str] | ValueError:
        &#039;&#039;&#039;
        This function is an example for code guidelines
        &#039;&#039;&#039;
        return [parameter2, parameter2.lower()]
      ```
  6. The hashtag comments on top of functions are optional, which are intended for developers `# Comments for developers`.
      ```python
      # Enter input text function
      def text_input_by_ID(driver: WebDriver, id: str, value: str, time: float=5.0) -&gt; None | Exception:
          &#039;&#039;&#039;
          Enters `value` into the input field with the given `id` if found, else throws NotFoundException.
          - `time` is the max time to wait for the element to be found.
          &#039;&#039;&#039;
          username_field = WebDriverWait(driver, time).until(EC.presence_of_element_located((By.ID, id)))
          username_field.send_keys(Keys.CONTROL + &quot;a&quot;)
          username_field.send_keys(value)
      
      ```
   
  #### Variables
  1. All variables must start with lower case, must be in explainable full words. If someone reads the variable name, it should be easy to understand what the variable stores.
  2. All local variables are camel case. Examples:
      ```python
      jobListingsElement = None
      ```
      ```python
      localBufferTime = 5.5
      ```
  3. All global variables are snake case. Example:
      ```
      total_runs = 1
      ```
  4. Mentioning types are optional.
      ```python
      localBufferTime: float | int = 5.5
      ```
  
  #### Configuration variables
  1. All config variables are treated as global variables. They have some extra guidelines.
  2. Must have variable setting explanation, and examples of valid values. Examples:
      ```python
      # Explanation of what this setting will do, and instructions to enter it correctly
      config_variable = &quot;value1&quot;    #  &lt;Valid values examples, and NOTES&gt; &quot;value1&quot;, &quot;value2&quot;, etc. Don&#039;t forget quotes (&quot;&quot;)
      ```
      ```python
      # Do you want to randomize the search order for search_terms?
      randomize_search_order = False     # True of False, Note: True or False are case-sensitive
      ```
      ```python
      # Avoid applying to jobs if their required experience is above your current_experience. (Set value as -1 if you want to apply to all ignoring their required experience...)
      current_experience = 5             # Integers &gt; -2 (Ex: -1, 0, 1, 2, 3, 4...)
      ```
      ```python
      # Search location, this will be filled in &quot;City, state, or zip code&quot; search box. If left empty as &quot;&quot;, tool will not fill it.
      search_location = &quot;United States&quot;               # Some valid examples: &quot;&quot;, &quot;United States&quot;, &quot;India&quot;, &quot;Chicago, Illinois, United States&quot;, &quot;90001, Los Angeles, California, United States&quot;, &quot;Bengaluru, Karnataka, India&quot;, etc.

      ```
  4. Add the config variable in appropriate `/config/file`.
  5. Every config variable must be validated. Go to `/modules/validator.py` and add it over there. Example:
      For config variable `search_location = &quot;&quot;` found in `/config/search.py`, string validation is added in file `/modules/validator.py` under the method `def validate_search()`.
      ```python
      def validate_search() -&gt; None | ValueError | TypeError:
          &#039;&#039;&#039;
          Validates all variables in the `/config/search.py` file.
          &#039;&#039;&#039;
          check_string(search_location, &quot;search_location&quot;)
      ```

  [back to index](#-content)
  
  ### Attestation
  1. All contributions require proper attestion. Format for attestation:
  ```python
  ##&gt; ------ &lt;Your full name&gt; : &lt;github id&gt; OR &lt;email&gt; - &lt;Type of change&gt; ------
      print(&quot;My contributions ğŸ˜&quot;) # Your code
  ##&lt;
  ```
  2. Examples for proper attestation:
  New feature example
  ```python
  ##&gt; ------ Sai Vignesh Golla : godsscion - Feature ------
  def alert_box(title: str, message: str) -&gt; None:
    &#039;&#039;&#039;
    Shows an alert box with the given `title` and `message`.
    &#039;&#039;&#039;
    from pyautogui import alert
    return alert(title, message)

  ##&lt;
  ```
  
  Bug fix example
  ```python
  def alert_box(title: str, message: str) -&gt; None:
    &#039;&#039;&#039;
    Shows an alert box with the given `title` and `message`.
    &#039;&#039;&#039;
    from pyautogui import alert

  ##&gt; ------ Sai Vignesh Golla : saivigneshgolla@outlook.com - Bug fix ------
    return alert(message, title)
  ##&lt;
  ```

[back to index](#-content)

## ğŸ—“ï¸ Major Updates History:
### Jan 20, 2026
- You can now simultaneously use chrome, while bot continues applying in a new window

### Jul 20, 2024
- Contributions from community have been added
- Better AI support, minor bug fixes

### Nov 28, 2024
- Patched to work for latest changes in Linkedin.
- Users can now select to follow or not follow companies when submitting application.
- Frameworks for future AI Developments have been added.
- AI can now be used to extract skills from job description. 

### Oct 16, 2024
- Framework for OpenAI API and Local LLMs
- Framework for RAG

### Sep 09, 2024
- Smarter Auto-fill for salaries and notice periods
- Robust Search location filter, will work in window mode (No need for full screen)
- Better logic for Select and Radio type questions
- Proper functioning of Decline to answer questions in Equal Employment opportunity questions
- Checkbox questions select fail bug fixed
- Annotations are clearer in instructions for setup

### Sep 07, 2024
- Annotations for developers
- Robust input validations
- Restructured config file
- Fixed pagination bug

### Aug 21, 2024
- Performance improvements (skip clicking on applied jobs and blacklisted companies)
- Stop when easy apply application limit is reached
- Added ability to discard from pause at submission dialogue box
- Added support for address input
- Bug fixed radio questions, added support for physical disability questions
- Added framework for future config file updates

### June 19, 2024
- Major Bug fixes (Text Area type questions)
- Made uploading default resume as not required

### May 15, 2024
- Added functionality for textarea type questions `summary`, `cover_letter`(Summary, Cover letter); checkbox type questions (acknowledgements)
- Added feature to skip irrelevant jobs based on `bad_words` 
- Improved performance for answering questions
- Logic change for masters students skipping
- Change variable names `blacklist_exceptions` -&gt; `about_company_good_words` and `blacklist_words` -&gt; `about_company_bad_words`
- Added session summary for logs
- Added option to turn off &quot;Pause before Submit&quot; until next run

### May 05, 2024
- For questions similar to &quot;What is your current location?&quot;, City posted in Job description will be posted as the answer if `current_city` is left empty in the configuration
- Added option to over write previously saved answers for a question `overwrite_previous_answers`
- Tool will now save previous answer of a question
- Tool will now collect all available options for a Radio type or Select type question
- Major update in answering logic for Easy Apply Application questions
- Added Safe mode option for quick stable launches `safe_mode`

### May 04, 2024
- Added option to fill in &quot;City, state, or zip code&quot; search box `search_location`
- Bug fixes in answering City or location question


[back to index](#-content)

&lt;br&gt;

## ğŸ“œ Disclaimer

**This program is for educational purposes only. By downloading, using, copying, replicating, or interacting with this program or its code, you acknowledge and agree to abide by all the Terms, Conditions, Policies, and Licenses mentioned, which are subject to modification without prior notice. The responsibility of staying informed of any changes or updates bears upon yourself. For the latest Terms &amp; Conditions, Licenses, or Policies, please refer to [Auto Job Applier](https://github.com/GodsScion/Auto_job_applier_linkedIn). Additionally, kindly adhere to and comply with LinkedIn&#039;s terms of service and policies pertaining to web scraping. Usage is at your own risk. The creators and contributors of this program emphasize that they bear no responsibility or liability for any misuse, damages, or legal consequences resulting from its usage.**


## ğŸ›ï¸ Terms and Conditions

Please consider the following:

- **LinkedIn Policies**: LinkedIn has specific policies regarding web scraping and data collection. The responsibility to review and comply with these policies before engaging, interacting, or undertaking any actions with this program bears upon yourself. Be aware of the limitations and restrictions imposed by LinkedIn to avoid any potential violation(s).

- **No Warranties or Guarantees**: This program is provided as-is, without any warranties or guarantees of any kind. The accuracy, reliability, and effectiveness of the program cannot be guaranteed. Use it at your own risk.

- **Disclaimer of Liability**: The creators and contributors of this program shall not be held responsible or liable for any damages or consequences arising from the direct or indirect use, interaction, or actions performed with this program. This includes but is not limited to any legal issues, loss of data, or other damages incurred.

- **Use at Your Own Risk**: It is important to exercise caution and ensure that your usage, interactions, and actions with this program comply with the applicable laws and regulations. Understand the potential risks and consequences associated with web scraping and data collection activities.

- **Chrome Driver**: This program utilizes the Chrome Driver for web scraping. Please review and comply with the terms and conditions specified for [Chrome Driver](https://chromedriver.chromium.org/home).


## âš–ï¸ License

Copyright (C) 2024 Sai Vignesh Golla  &lt;saivigneshgolla@outlook.com&gt;

This program is free software: you can redistribute it and/or modify it under the terms of the GNU Affero General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.

This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License for more details.

You should have received a copy of the GNU Affero General Public License along with this program. If not, see &lt;https://www.gnu.org/licenses/&gt;.

See [AGPLv3 LICENSE](LICENSE) for more info.


&lt;br&gt;

[back to index](#-content)

&lt;br&gt;

## ğŸ§ Socials
- **LinkedIn** : https://www.linkedin.com/in/saivigneshgolla/
- **Email**    : saivigneshgolla@outlook.com
- **X/Twitter**: https://x.com/saivigneshgolla
- **Discord**  : godsscion


## ğŸ™Œ Community Support and Discussions
- **Discord Server** : https://discord.gg/fFp7uUzWCY
alternate link: https://discord.gg/ykfDjRFB
- **GitHub**
    - [All Discussions](https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions)
    - [Announcements](https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions/categories/announcements)
    - [General](https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions/categories/general)
    - [Feature requests or Ideas](https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions/categories/feature-requests-or-ideas)
    - [Polls](https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions/categories/polls)
    - [Community Flex](https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions/categories/community-flex)
    - [Support Q&amp;A](https://github.com/GodsScion/Auto_job_applier_linkedIn/discussions/categories/support-q-a)


#### â„¹ï¸ Version: 26.01.20.5.08

---

[back to the top](#linkedin-ai-auto-job-applier-)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[PostHog/posthog]]></title>
            <link>https://github.com/PostHog/posthog</link>
            <guid>https://github.com/PostHog/posthog</guid>
            <pubDate>Fri, 20 Feb 2026 00:06:49 GMT</pubDate>
            <description><![CDATA[ğŸ¦” PostHog is an all-in-one developer platform for building successful products. We offer product analytics, web analytics, session replay, error tracking, feature flags, experimentation, surveys, data warehouse, a CDP, and an AI product assistant to help debug your code, ship features faster, and keep all your usage and customer data in one stack.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/PostHog/posthog">PostHog/posthog</a></h1>
            <p>ğŸ¦” PostHog is an all-in-one developer platform for building successful products. We offer product analytics, web analytics, session replay, error tracking, feature flags, experimentation, surveys, data warehouse, a CDP, and an AI product assistant to help debug your code, ship features faster, and keep all your usage and customer data in one stack.</p>
            <p>Language: Python</p>
            <p>Stars: 31,325</p>
            <p>Forks: 2,296</p>
            <p>Stars today: 21 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img alt=&quot;posthoglogo&quot; src=&quot;https://user-images.githubusercontent.com/65415371/205059737-c8a4f836-4889-4654-902e-f302b187b6a0.png&quot;&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&#039;https://posthog.com/contributors&#039;&gt;&lt;img alt=&quot;GitHub contributors&quot; src=&quot;https://img.shields.io/github/contributors/posthog/posthog&quot;/&gt;&lt;/a&gt;
  &lt;a href=&#039;http://makeapullrequest.com&#039;&gt;&lt;img alt=&#039;PRs Welcome&#039; src=&#039;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=shields&#039;/&gt;&lt;/a&gt;
  &lt;img alt=&quot;Docker Pulls&quot; src=&quot;https://img.shields.io/docker/pulls/posthog/posthog&quot;/&gt;
  &lt;a href=&quot;https://github.com/PostHog/posthog/commits/master&quot;&gt;&lt;img alt=&quot;GitHub commit activity&quot; src=&quot;https://img.shields.io/github/commit-activity/m/posthog/posthog&quot;/&gt; &lt;/a&gt;
  &lt;a href=&quot;https://github.com/PostHog/posthog/issues?q=is%3Aissue%20state%3Aclosed&quot;&gt;&lt;img alt=&quot;GitHub closed issues&quot; src=&quot;https://img.shields.io/github/issues-closed/posthog/posthog&quot;/&gt; &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://posthog.com/docs&quot;&gt;Docs&lt;/a&gt; - &lt;a href=&quot;https://posthog.com/community&quot;&gt;Community&lt;/a&gt; - &lt;a href=&quot;https://posthog.com/roadmap&quot;&gt;Roadmap&lt;/a&gt; - &lt;a href=&quot;https://posthog.com/why&quot;&gt;Why PostHog?&lt;/a&gt; - &lt;a href=&quot;https://posthog.com/changelog&quot;&gt;Changelog&lt;/a&gt; - &lt;a href=&quot;https://github.com/PostHog/posthog/issues/new?assignees=&amp;labels=bug&amp;template=bug_report.yml&quot;&gt;Bug reports&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=2jQco8hEvTI&quot;&gt;
    &lt;img src=&quot;https://res.cloudinary.com/dmukukwp6/image/upload/demo_thumb_68d0d8d56d&quot; alt=&quot;PostHog Demonstration&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

## PostHog is an all-in-one, open source platform for building successful products

[PostHog](https://posthog.com/) provides every tool you need to build a successful product including:

- [Product Analytics](https://posthog.com/product-analytics): Autocapture or manually instrument event-based analytics to understand user behavior and analyze data with visualization or SQL.
- [Web Analytics](https://posthog.com/web-analytics): Monitor web traffic and user sessions with a GA-like dashboard. Easily monitor conversion, web vitals, and revenue.
- [Session Replays](https://posthog.com/session-replay): Watch real user sessions of interactions with your website or mobile app to diagnose issues and understand user behavior.
- [Feature Flags](https://posthog.com/feature-flags): Safely roll out features to select users or cohorts with feature flags.
- [Experiments](https://posthog.com/experiments): Test changes and measure their statistical impact on goal metrics. Set up experiments with no-code too.
- [Error Tracking](https://posthog.com/error-tracking): Track errors, get alerts, and resolve issues to improve your product.
- [Surveys](https://posthog.com/surveys): Ask anything with our collection of no-code survey templates, or build custom surveys with our survey builder.
- [Data warehouse](https://posthog.com/data-warehouse): Sync data from external tools like Stripe, Hubspot, your data warehouse, and more. Query it alongside your product data.
- [Data pipelines](https://posthog.com/cdp): Run custom filters and transformations on your incoming data. Send it to 25+ tools or any webhook in real time or batch export large amounts to your warehouse.
- [LLM analytics](https://posthog.com/docs/llm-analytics): Capture traces, generations, latency, and cost for your LLM-powered app.
- [Workflows](https://posthog.com/docs/workflows): Create workflows that automate actions or send messages to your users.

Best of all, all of this is free to use with a [generous monthly free tier](https://posthog.com/pricing) for each product. Get started by signing up for [PostHog Cloud US](https://us.posthog.com/signup) or [PostHog Cloud EU](https://eu.posthog.com/signup).

## Table of Contents

- [PostHog is an all-in-one, open source platform for building successful products](#posthog-is-an-all-in-one-open-source-platform-for-building-successful-products)
- [Table of Contents](#table-of-contents)
- [Getting started with PostHog](#getting-started-with-posthog)
  - [PostHog Cloud (Recommended)](#posthog-cloud-recommended)
  - [Self-hosting the open-source hobby deploy (Advanced)](#self-hosting-the-open-source-hobby-deploy-advanced)
- [Setting up PostHog](#setting-up-posthog)
- [Learning more about PostHog](#learning-more-about-posthog)
- [Contributing](#contributing)
- [Open-source vs. paid](#open-source-vs-paid)
- [Weâ€™re hiring!](#were-hiring)

## Getting started with PostHog

### PostHog Cloud (Recommended)

The fastest and most reliable way to get started with PostHog is signing up for free toÂ [PostHog Cloud](https://us.posthog.com/signup) or [PostHog Cloud EU](https://eu.posthog.com/signup). Your first 1 million events, 5k recordings, 1M flag requests, 100k exceptions, and 1500 survey responses are free every month, after which you pay based on usage.

### Self-hosting the open-source hobby deploy (Advanced)

If you want to self-host PostHog, you can deploy a hobby instance in one line on Linux with Docker (recommended 4GB memory):

```bash
/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/posthog/posthog/HEAD/bin/deploy-hobby)&quot;
```

Open source deployments should scale to approximately 100k events per month, after which we recommend [migrating to a PostHog Cloud](https://posthog.com/docs/migrate/migrate-to-cloud).

We _do not_ provide customer support or offer guarantees for open source deployments. See our [self-hosting docs](https://posthog.com/docs/self-host), [troubleshooting guide](https://posthog.com/docs/self-host/deploy/troubleshooting), and [disclaimer](https://posthog.com/docs/self-host/open-source/disclaimer) for more info.

## Setting up PostHog

Once you&#039;ve got a PostHog instance, you can set it up by installing our [JavaScript web snippet](https://posthog.com/docs/getting-started/install?tab=snippet), one of [our SDKs](https://posthog.com/docs/getting-started/install?tab=sdks), or by [using our API](https://posthog.com/docs/getting-started/install?tab=api).

We have SDKs and libraries for popular languages and frameworks like:

| Frontend                                              | Mobile                                                          | Backend                                             |
| ----------------------------------------------------- | --------------------------------------------------------------- | --------------------------------------------------- |
| [JavaScript](https://posthog.com/docs/libraries/js)   | [React Native](https://posthog.com/docs/libraries/react-native) | [Python](https://posthog.com/docs/libraries/python) |
| [Next.js](https://posthog.com/docs/libraries/next-js) | [Android](https://posthog.com/docs/libraries/android)           | [Node](https://posthog.com/docs/libraries/node)     |
| [React](https://posthog.com/docs/libraries/react)     | [iOS](https://posthog.com/docs/libraries/ios)                   | [PHP](https://posthog.com/docs/libraries/php)       |
| [Vue](https://posthog.com/docs/libraries/vue-js)      | [Flutter](https://posthog.com/docs/libraries/flutter)           | [Ruby](https://posthog.com/docs/libraries/ruby)     |

Beyond this, we have docs and guides for [Go](https://posthog.com/docs/libraries/go), [.NET/C#](https://posthog.com/docs/libraries/dotnet), [Django](https://posthog.com/docs/libraries/django), [Angular](https://posthog.com/docs/libraries/angular), [WordPress](https://posthog.com/docs/libraries/wordpress), [Webflow](https://posthog.com/docs/libraries/webflow), and more.

Once you&#039;ve installed PostHog, see our [product docs](https://posthog.com/docs/product-os) for more information on how to set up [product analytics](https://posthog.com/docs/product-analytics/capture-events), [web analytics](https://posthog.com/docs/web-analytics/getting-started), [session replays](https://posthog.com/docs/session-replay/how-to-watch-recordings), [feature flags](https://posthog.com/docs/feature-flags/creating-feature-flags), [experiments](https://posthog.com/docs/experiments/creating-an-experiment), [error tracking](https://posthog.com/docs/error-tracking/installation#setting-up-exception-autocapture), [surveys](https://posthog.com/docs/surveys/installation), [data warehouse](https://posthog.com/docs/cdp/sources), and more.

## Learning more about PostHog

Our code isn&#039;t the only thing that&#039;s open source ğŸ˜³. We also open source our [company handbook](https://posthog.com/handbook) which details our [strategy](https://posthog.com/handbook/why-does-posthog-exist), [ways of working](https://posthog.com/handbook/company/culture), and [processes](https://posthog.com/handbook/team-structure).

Curious about how to make the most of PostHog? We wrote a guide to [winning with PostHog](https://posthog.com/docs/new-to-posthog/getting-hogpilled) which walks you through the basics of [measuring activation](https://posthog.com/docs/new-to-posthog/activation), [tracking retention](https://posthog.com/docs/new-to-posthog/retention), and [capturing revenue](https://posthog.com/docs/new-to-posthog/revenue).

## Contributing

We &lt;3 contributions big and small:

- Vote on features or get early access to beta functionality in our [roadmap](https://posthog.com/roadmap)
- Open a PR (see our instructions on [developing PostHog locally](https://posthog.com/handbook/engineering/developing-locally))
- Submit a [feature request](https://github.com/PostHog/posthog/issues/new?assignees=&amp;labels=enhancement%2C+feature&amp;template=feature_request.yml) or [bug report](https://github.com/PostHog/posthog/issues/new?assignees=&amp;labels=bug&amp;template=bug_report.yml)

For an overview of the codebase structure, see [monorepo layout](docs/internal/monorepo-layout.md) and [products](products/README.md).

## Open-source vs. paid

This repo is available under the [MIT expat license](https://github.com/PostHog/posthog/blob/master/LICENSE), except for the `ee` directory (which has its [license here](https://github.com/PostHog/posthog/blob/master/ee/LICENSE)) if applicable.

Need _absolutely ğŸ’¯% FOSS_? Check out our [posthog-foss](https://github.com/PostHog/posthog-foss) repository, which is purged of all proprietary code and features.

The pricing for our paid plan is completely transparent and available on [our pricing page](https://posthog.com/pricing).

## We&#039;re hiring!

&lt;img src=&quot;https://res.cloudinary.com/dmukukwp6/image/upload/v1/posthog.com/src/components/Home/images/mission-control-hog&quot; alt=&quot;Hedgehog working on a Mission Control Center&quot; width=&quot;350px&quot;/&gt;

Hey! If you&#039;re reading this, you&#039;ve proven yourself as a dedicated README reader.

You might also make a great addition to our team. We&#039;re growing fast [and would love for you to join us](https://posthog.com/careers).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NVIDIA-NeMo/NeMo]]></title>
            <link>https://github.com/NVIDIA-NeMo/NeMo</link>
            <guid>https://github.com/NVIDIA-NeMo/NeMo</guid>
            <pubDate>Fri, 20 Feb 2026 00:06:48 GMT</pubDate>
            <description><![CDATA[A scalable generative AI framework built for researchers and developers working on Large Language Models, Multimodal, and Speech AI (Automatic Speech Recognition and Text-to-Speech)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NVIDIA-NeMo/NeMo">NVIDIA-NeMo/NeMo</a></h1>
            <p>A scalable generative AI framework built for researchers and developers working on Large Language Models, Multimodal, and Speech AI (Automatic Speech Recognition and Text-to-Speech)</p>
            <p>Language: Python</p>
            <p>Stars: 16,776</p>
            <p>Forks: 3,345</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre>[![Project Status: Active -- The project has reached a stable, usable state and is being actively developed.](http://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active)
[![Documentation](https://readthedocs.com/projects/nvidia-nemo/badge/?version=main)](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/)
[![CodeQL](https://github.com/nvidia/nemo/actions/workflows/codeql.yml/badge.svg?branch=main&amp;event=push)](https://github.com/nvidia/nemo/actions/workflows/codeql.yml)
[![NeMo core license and license for collections in this repo](https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg)](https://github.com/NVIDIA/NeMo/blob/master/LICENSE)
[![Release version](https://badge.fury.io/py/nemo-toolkit.svg)](https://badge.fury.io/py/nemo-toolkit)
[![Python version](https://img.shields.io/pypi/pyversions/nemo-toolkit.svg)](https://badge.fury.io/py/nemo-toolkit)
[![PyPi total downloads](https://static.pepy.tech/personalized-badge/nemo-toolkit?period=total&amp;units=international_system&amp;left_color=grey&amp;right_color=brightgreen&amp;left_text=downloads)](https://pepy.tech/project/nemo-toolkit)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

# **NVIDIA NeMo Speech Collection**

## Latest News

&lt;!-- markdownlint-disable --&gt;
&lt;details open&gt;
  &lt;summary&gt;&lt;b&gt;&lt;a href=https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16&gt;NVIDIA-Nemotron-3-Nano-30B-A3B&lt;/a&gt; is out with full reproducible script and recipes! Check out &lt;a href=https://github.com/NVIDIA-NeMo/Megatron-Bridge/tree/nano-v3&gt;NeMo Megatron-Bridge&lt;/a&gt;, &lt;a href=https://github.com/NVIDIA-NeMo/AutoModel/blob/main/examples/llm_finetune/nemotron/nemotron_nano_v3_squad.yaml&gt;NeMo AutoModel&lt;/a&gt;, &lt;a href=https://github.com/NVIDIA-NeMo/RL&gt;NeMo-RL&lt;/a&gt; and &lt;a href=https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo?version=25.11.nemotron_3_nano&gt;NGC container&lt;/a&gt; to try them!&lt;/b&gt; (2025-12-15)
&lt;/details&gt;


&lt;details open&gt;
  &lt;summary&gt;&lt;b&gt;âš ï¸ Pivot notice: This repo has pivoted to focus on audio, speech, and multimodal LLM only. Please refer to &lt;a href=https://github.com/NVIDIA-NeMo&gt;NeMo Framework Github Org&lt;/a&gt; for the complete list of repos under NeMo Framework&lt;/b&gt;&lt;/summary&gt;
  &lt;blockquote&gt;
    NeMo 2.0, with its support for Megatron Core, LLMs, and VLMs became deprecated in 25.11, and replaced by &lt;a href=https://github.com/NVIDIA-NeMo/Megatron-Bridge&gt;NeMo Megatron-Bridge&lt;/a&gt; and &lt;a href=https://github.com/NVIDIA-NeMo/AutoModel&gt;NeMo AutoModel&lt;/a&gt;. More details can be found in the &lt;a href=https://github.com/NVIDIA-NeMo&gt;NeMo Framework GitHub org readme&lt;/a&gt;. (2025-10-10)
    &lt;br&gt;&lt;br&gt;
    &lt;b&gt;The following collections are no longer available&lt;/b&gt;&lt;br&gt;
    &lt;code&gt;avlm&lt;/code&gt; Â· &lt;code&gt;diffusion&lt;/code&gt; Â· &lt;code&gt;llm&lt;/code&gt; Â· &lt;code&gt;multimodal&lt;/code&gt; Â· &lt;code&gt;multimodal-autoregressive&lt;/code&gt; Â· &lt;code&gt;nlp&lt;/code&gt; Â· &lt;code&gt;speechlm&lt;/code&gt; Â· &lt;code&gt;vision&lt;/code&gt; Â· &lt;code&gt;vlm&lt;/code&gt;
  &lt;/blockquote&gt;
&lt;/details&gt;

&lt;details closed&gt;
  &lt;summary&gt;&lt;b&gt;Pretrain and finetune :hugs:Hugging Face models via AutoModel&lt;/b&gt;&lt;/summary&gt;
      NeMo Framework&#039;s latest feature AutoModel enables broad support for :hugs:Hugging Face models, with 25.04 focusing on


- &lt;a href=https://huggingface.co/transformers/v3.5.1/model_doc/auto.html#automodelforcausallm&gt;AutoModelForCausalLM&lt;/a&gt; in the &lt;a href=&quot;https://huggingface.co/models?pipeline_tag=text-generation&amp;sort=trending&quot;&gt;Text Generation&lt;/a&gt; category
- &lt;a href=https://huggingface.co/docs/transformers/main/model_doc/auto#transformers.AutoModelForImageTextToText&gt;AutoModelForImageTextToText&lt;/a&gt; in the &lt;a href=&quot;https://huggingface.co/models?pipeline_tag=image-text-to-text&amp;sort=trending&quot;&gt;Image-Text-to-Text&lt;/a&gt; category

More Details in Blog: &lt;a href=https://developer.nvidia.com/blog/run-hugging-face-models-instantly-with-day-0-support-from-nvidia-nemo-framework&gt;Run Hugging Face Models Instantly with Day-0 Support from NVIDIA NeMo Framework&lt;/a&gt;. Future releases will enable support for more model families such as Video Generation models.(2025-05-19)
&lt;/details&gt;

&lt;details closed&gt;
  &lt;summary&gt;&lt;b&gt;Training on Blackwell using NeMo&lt;/b&gt;&lt;/summary&gt;
      NeMo Framework has added Blackwell support, with &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance_summary.html&gt;performance benchmarks on GB200 &amp; B200&lt;/a&gt;. More optimizations to come in the upcoming releases.(2025-05-19)
&lt;/details&gt;

&lt;details closed&gt;
  &lt;summary&gt;&lt;b&gt;Training Performance on GPU Tuning Guide&lt;/b&gt;&lt;/summary&gt;
      NeMo Framework has published &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance-guide.html&gt;a comprehensive guide for performance tuning to achieve optimal throughput&lt;/a&gt;! (2025-05-19)
&lt;/details&gt;

&lt;details closed&gt;
  &lt;summary&gt;&lt;b&gt;New Models Support&lt;/b&gt;&lt;/summary&gt;
      NeMo Framework has added support for latest community models - &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/vlms/llama4.html&gt;Llama 4&lt;/a&gt;, &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/vision/diffusionmodels/flux.html&gt;Flux&lt;/a&gt;, &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/llama_nemotron.html&gt;Llama Nemotron&lt;/a&gt;, &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/hyena.html#&gt;Hyena &amp; Evo2&lt;/a&gt;, &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/vlms/qwen2vl.html&gt;Qwen2-VL&lt;/a&gt;, &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/qwen2.html&gt;Qwen2.5&lt;/a&gt;, Gemma3, Qwen3-30B&amp;32B.(2025-05-19)
&lt;/details&gt;


&lt;details closed&gt;
  &lt;summary&gt;&lt;b&gt;NeMo Framework 2.0&lt;/b&gt;&lt;/summary&gt;
      We&#039;ve released NeMo 2.0, an update on the NeMo Framework which prioritizes modularity and ease-of-use. Please refer to the &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/index.html&gt;NeMo Framework User Guide&lt;/a&gt; to get started.
&lt;/details&gt;
&lt;details closed&gt;
  &lt;summary&gt;&lt;b&gt;New Cosmos World Foundation Models Support&lt;/b&gt;&lt;/summary&gt;
    &lt;details&gt;
      &lt;summary&gt; &lt;a href=&quot;https://developer.nvidia.com/blog/advancing-physical-ai-with-nvidia-cosmos-world-foundation-model-platform&quot;&gt;Advancing Physical AI with NVIDIA Cosmos World Foundation Model Platform &lt;/a&gt; (2025-01-09)
      &lt;/summary&gt;
        The end-to-end NVIDIA Cosmos platform accelerates world model development for physical AI systems. Built on CUDA, Cosmos combines state-of-the-art world foundation models, video tokenizers, and AI-accelerated data processing pipelines. Developers can accelerate world model development by fine-tuning Cosmos world foundation models or building new ones from the ground up. These models create realistic synthetic videos of environments and interactions, providing a scalable foundation for training complex systems, from simulating humanoid robots performing advanced actions to developing end-to-end autonomous driving models.
        &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/accelerate-custom-video-foundation-model-pipelines-with-new-nvidia-nemo-framework-capabilities/&quot;&gt;
          Accelerate Custom Video Foundation Model Pipelines with New NVIDIA NeMo Framework Capabilities
        &lt;/a&gt; (2025-01-07)
      &lt;/summary&gt;
        The NeMo Framework now supports training and customizing the &lt;a href=&quot;https://github.com/NVIDIA/Cosmos&quot;&gt;NVIDIA Cosmos&lt;/a&gt; collection of world foundation models. Cosmos leverages advanced text-to-world generation techniques to create fluid, coherent video content from natural language prompts.
        &lt;br&gt;&lt;br&gt;
        You can also now accelerate your video processing step using the &lt;a href=&quot;https://developer.nvidia.com/nemo-curator-video-processing-early-access&quot;&gt;NeMo Curator&lt;/a&gt; library, which provides optimized video processing and captioning features that can deliver up to 89x faster video processing when compared to an unoptimized CPU pipeline.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
&lt;/details&gt;
&lt;details closed&gt;
  &lt;summary&gt;&lt;b&gt;Large Language Models and Multimodal Models&lt;/b&gt;&lt;/summary&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/state-of-the-art-multimodal-generative-ai-model-development-with-nvidia-nemo/&quot;&gt;
          State-of-the-Art Multimodal Generative AI Model Development with NVIDIA NeMo
        &lt;/a&gt; (2024-11-06)
      &lt;/summary&gt;
        NVIDIA recently announced significant enhancements to the NeMo platform, focusing on multimodal generative AI models. The update includes NeMo Curator and the Cosmos tokenizer, which streamline the data curation process and enhance the quality of visual data. These tools are designed to handle large-scale data efficiently, making it easier to develop high-quality AI models for various applications, including robotics and autonomous driving. The Cosmos tokenizers, in particular, efficiently map visual data into compact, semantic tokens, which is crucial for training large-scale generative models. The tokenizer is available now on the &lt;a href=https://github.com/NVIDIA/cosmos-tokenizer&gt;NVIDIA/cosmos-tokenizer&lt;/a&gt; GitHub repo and on &lt;a href=https://huggingface.co/nvidia/Cosmos-Tokenizer-CV8x8x8&gt;Hugging Face&lt;/a&gt;.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/llama/index.html#new-llama-3-1-support for more information/&quot;&gt;
        New Llama 3.1 Support
        &lt;/a&gt; (2024-07-23)
      &lt;/summary&gt;
        The NeMo Framework now supports training and customizing the Llama 3.1 collection of LLMs from Meta.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://aws.amazon.com/blogs/machine-learning/accelerate-your-generative-ai-distributed-training-workloads-with-the-nvidia-nemo-framework-on-amazon-eks/&quot;&gt;
          Accelerate your Generative AI Distributed Training Workloads with the NVIDIA NeMo Framework on Amazon EKS
        &lt;/a&gt; (2024-07-16)
      &lt;/summary&gt;
     NVIDIA NeMo Framework now runs distributed training workloads on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. For step-by-step instructions on creating an EKS cluster and running distributed training workloads with NeMo, see the GitHub repository &lt;a href=&quot;https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/2.nemo-launcher/EKS/&quot;&gt; here.&lt;/a&gt;
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-nemo-accelerates-llm-innovation-with-hybrid-state-space-model-support/&quot;&gt;
          NVIDIA NeMo Accelerates LLM Innovation with Hybrid State Space Model Support
        &lt;/a&gt; (2024/06/17)
      &lt;/summary&gt;
     NVIDIA NeMo and Megatron Core now support pre-training and fine-tuning of state space models (SSMs). NeMo also supports training models based on the Griffin architecture as described by Google DeepMind.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
      &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://huggingface.co/models?sort=trending&amp;search=nvidia%2Fnemotron-4-340B&quot;&gt;
          NVIDIA releases 340B base, instruct, and reward models pretrained on a total of 9T tokens.
        &lt;/a&gt; (2024-06-18)
      &lt;/summary&gt;
      See documentation and tutorials for SFT, PEFT, and PTQ with
      &lt;a href=&quot;https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/nemotron/index.html&quot;&gt;
        Nemotron 340B
      &lt;/a&gt;
      in the NeMo Framework User Guide.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-sets-new-generative-ai-performance-and-scale-records-in-mlperf-training-v4-0/&quot;&gt;
          NVIDIA sets new generative AI performance and scale records in MLPerf Training v4.0
        &lt;/a&gt; (2024/06/12)
      &lt;/summary&gt;
      Using NVIDIA NeMo Framework and NVIDIA Hopper GPUs NVIDIA was able to scale to 11,616 H100 GPUs and achieve near-linear performance scaling on LLM pretraining.
      NVIDIA also achieved the highest LLM fine-tuning performance and raised the bar for text-to-image training.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
        &lt;summary&gt;
          &lt;a href=&quot;https://cloud.google.com/blog/products/compute/gke-and-nvidia-nemo-framework-to-train-generative-ai-models&quot;&gt;
            Accelerate your generative AI journey with NVIDIA NeMo Framework on GKE
          &lt;/a&gt; (2024/03/16)
        &lt;/summary&gt;
        An end-to-end walkthrough to train generative AI models on the Google Kubernetes Engine (GKE) using the NVIDIA NeMo Framework is available at https://github.com/GoogleCloudPlatform/nvidia-nemo-on-gke.
        The walkthrough includes detailed instructions on how to set up a Google Cloud Project and pre-train a GPT model using the NeMo Framework.
        &lt;br&gt;&lt;br&gt;
      &lt;/details&gt;
&lt;/details&gt;
&lt;details closed&gt;
  &lt;summary&gt;&lt;b&gt;Speech Recognition&lt;/b&gt;&lt;/summary&gt;
  &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/accelerating-leaderboard-topping-asr-models-10x-with-nvidia-nemo/&quot;&gt;
          Accelerating Leaderboard-Topping ASR Models 10x with NVIDIA NeMo
        &lt;/a&gt; (2024/09/24)
      &lt;/summary&gt;
      NVIDIA NeMo team released a number of inference optimizations for CTC, RNN-T, and TDT models that resulted in up to 10x inference speed-up.
      These models now exceed an inverse real-time factor (RTFx) of 2,000, with some reaching RTFx of even 6,000.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/new-standard-for-speech-recognition-and-translation-from-the-nvidia-nemo-canary-model/&quot;&gt;
          New Standard for Speech Recognition and Translation from the NVIDIA NeMo Canary Model
        &lt;/a&gt; (2024/04/18)
      &lt;/summary&gt;
      The NeMo team just released Canary, a multilingual model that transcribes speech in English, Spanish, German, and French with punctuation and capitalization.
      Canary also provides bi-directional translation, between English and the three other supported languages.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/pushing-the-boundaries-of-speech-recognition-with-nemo-parakeet-asr-models/&quot;&gt;
          Pushing the Boundaries of Speech Recognition with NVIDIA NeMo Parakeet ASR Models
        &lt;/a&gt; (2024/04/18)
      &lt;/summary&gt;
      NVIDIA NeMo, an end-to-end platform for the development of multimodal generative AI models at scale anywhereâ€”on any cloud and on-premisesâ€”released the Parakeet family of automatic speech recognition (ASR) models.
      These state-of-the-art ASR models, developed in collaboration with Suno.ai, transcribe spoken English with exceptional accuracy.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
  &lt;details&gt;
    &lt;summary&gt;
      &lt;a href=&quot;https://developer.nvidia.com/blog/turbocharge-asr-accuracy-and-speed-with-nvidia-nemo-parakeet-tdt/&quot;&gt;
        Turbocharge ASR Accuracy and Speed with NVIDIA NeMo Parakeet-TDT
      &lt;/a&gt; (2024/04/18)
    &lt;/summary&gt;
    NVIDIA NeMo, an end-to-end platform for developing multimodal generative AI models at scale anywhereâ€”on any cloud and on-premisesâ€”recently released Parakeet-TDT.
    This new addition to the â€¯NeMo ASR Parakeet model family boasts better accuracy and 64% greater speed over the previously best model, Parakeet-RNNT-1.1B.
    &lt;br&gt;&lt;br&gt;
  &lt;/details&gt;
&lt;/details&gt;
&lt;!-- markdownlint-enable --&gt;

## Introduction

NVIDIA NeMo Framework is a scalable and cloud-native generative AI
framework built for researchers and PyTorch developers working on Large
Language Models (LLMs), Multimodal Models (MMs), Automatic Speech
Recognition (ASR), Text to Speech (TTS), and Computer Vision (CV)
domains. It is designed to help you efficiently create, customize, and
deploy new generative AI models by leveraging existing code and
pre-trained model checkpoints.

For technical documentation, please see the [NeMo Framework User
Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html).

## What&#039;s New in NeMo 2.0

NVIDIA NeMo 2.0 introduces several significant improvements over its predecessor, NeMo 1.0, enhancing flexibility, performance, and scalability.

- **Python-Based Configuration** - NeMo 2.0 transitions from YAML files to a Python-based configuration, providing more flexibility and control. This shift makes it easier to extend and customize configurations programmatically.

- **Modular Abstractions** - By adopting PyTorch Lightningâ€™s modular abstractions, NeMo 2.0 simplifies adaptation and experimentation. This modular approach allows developers to more easily modify and experiment with different components of their models.

- **Scalability** - NeMo 2.0 seamlessly scaling large-scale experiments across thousands of GPUs using [NeMo-Run](https://github.com/NVIDIA/NeMo-Run), a powerful tool designed to streamline the configuration, execution, and management of machine learning experiments across computing environments.

Overall, these enhancements make NeMo 2.0 a powerful, scalable, and user-friendly framework for AI model development.

### Get Started with NeMo 2.0

- Refer to the [Quickstart](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/quickstart.html) for examples of using NeMo-Run to launch NeMo 2.0 experiments locally and on a slurm cluster.
- For more information about NeMo 2.0, see the [NeMo Framework User Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/index.html).
- For an in-depth exploration of the main features of NeMo 2.0, see the [Feature Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/features/index.html#feature-guide).
- To transition from NeMo 1.0 to 2.0, see the [Migration Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/migration/index.html#migration-guide) for step-by-step instructions.

## Training and Customization

All NeMo models are trained with
[Lightning](https://github.com/Lightning-AI/lightning). Training is
automatically scalable to 1000s of GPUs. You can check the performance benchmarks using the
latest NeMo Framework container [here](https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance_summary.html).

When applicable, NeMo models leverage cutting-edge distributed training
techniques, incorporating [parallelism
strategies](https://docs.nvidia.com/nemo-framework/user-guide/latest/modeloverview.html)
to enable efficient training of very large models. These techniques
include Tensor Parallelism (TP), Pipeline Parallelism (PP), Fully
Sharded Data Parallelism (FSDP), Mixture-of-Experts (MoE), and Mixed
Precision Training with BFloat16 and FP8, as well as others.

In addition to supervised fine-tuning (SFT), NeMo also supports the
latest parameter efficient fine-tuning (PEFT) techniques such as LoRA,
P-Tuning, Adapters, and IA3.

## Speech AI

NeMo ASR and TTS models can be optimized for inference and deployed for
production use cases with [NVIDIA Riva](https://developer.nvidia.com/riva).


## Get Started with NeMo Framework

Getting started with NeMo Framework is easy. State-of-the-art pretrained
NeMo models are freely available on [Hugging Face
Hub](https://huggingface.co/models?library=nemo&amp;sort=downloads&amp;search=nvidia)
and [NVIDIA
NGC](https://catalog.ngc.nvidia.com/models?query=nemo&amp;orderBy=weightPopularDESC).
These models can be used to generate text or images, transcribe audio,
and synthesize speech in just a few lines of code.

We have extensive
[tutorials](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/starthere/tutorials.html)
that can be run on [Google Colab](https://colab.research.google.com) or
with our [NGC NeMo Framework
Container](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo).
We also have
[playbooks](https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html)
for users who want to train NeMo models with the NeMo Framework
Launcher.

For advanced users who want to train NeMo models from scratch or
fine-tune existing NeMo models, we have a full suite of [example
scripts](https://github.com/NVIDIA/NeMo/tree/main/examples) that supp

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/agent-lightning]]></title>
            <link>https://github.com/microsoft/agent-lightning</link>
            <guid>https://github.com/microsoft/agent-lightning</guid>
            <pubDate>Fri, 20 Feb 2026 00:06:47 GMT</pubDate>
            <description><![CDATA[The absolute trainer to light up AI agents.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/agent-lightning">microsoft/agent-lightning</a></h1>
            <p>The absolute trainer to light up AI agents.</p>
            <p>Language: Python</p>
            <p>Stars: 15,041</p>
            <p>Forks: 1,276</p>
            <p>Stars today: 65 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/assets/readme-banner.svg&quot; alt=&quot;Agent-lightning-banner&quot; style=&quot;width:600px&quot;/&gt;
&lt;/p&gt;

# Agent Lightningâš¡

[![Unit Tests](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml)
[![Documentation](https://img.shields.io/badge/GitHub%20Pages-Documentation-blue)](https://microsoft.github.io/agent-lightning/)
[![PyPI version](https://badge.fury.io/py/agentlightning.svg)](https://badge.fury.io/py/agentlightning)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/microsoft/agent-lightning)
[![Discord](https://img.shields.io/badge/Discord-Join-5865F2?logo=discord&amp;logoColor=white)](https://discord.gg/RYk7CdvDR7)

**The absolute trainer to light up AI agents.**

Join our [Discord community](https://discord.gg/RYk7CdvDR7) to connect with other users and contributors.

## âš¡ Core Features

- Turn your agent into an optimizable beast with **ZERO CODE CHANGE** (almost)! ğŸ’¤
- Build with **ANY** agent framework (LangChain, OpenAI Agent SDK, AutoGen, CrewAI, Microsoft Agent Framework...); or even WITHOUT agent framework (Python OpenAI). You name it! ğŸ¤–
- **Selectively** optimize one or more agents in a multi-agent system. ğŸ¯
- Embraces **Algorithms** like Reinforcement Learning, Automatic Prompt Optimization, Supervised Fine-tuning and more. ğŸ¤—

Read more on our [documentation website](https://microsoft.github.io/agent-lightning/).

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/assets/readme-diff.svg&quot; alt=&quot;Agent-Lightning Core Quickstart&quot; style=&quot;width:100%&quot;/&gt;
&lt;/p&gt;

## âš¡ Installation

```bash
pip install agentlightning
```

For the latest nightly build (cutting-edge features), you can install from Test PyPI:

```bash
pip install --upgrade --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ --pre agentlightning
```

Please refer to our [installation guide](https://microsoft.github.io/agent-lightning/stable/tutorials/installation/) for more details.

To start using Agent-lightning, check out our [documentation](https://microsoft.github.io/agent-lightning/) and [examples](./examples).

## âš¡ Articles

- 12/17/2025 [Adopting the Trajectory Level Aggregation for Faster Training](https://agent-lightning.github.io/posts/trajectory_level_aggregation/) Agent-lightning blog.
- 11/4/2025 [Tuning ANY AI agent with Tinker âœ• Agent-lightning](https://medium.com/@yugez/tuning-any-ai-agent-with-tinker-agent-lightning-part-1-1d8c9a397f0e) Medium. See also [Part 2](https://medium.com/@yugez/tuning-any-ai-agent-with-tinker-agent-lightning-part-2-332c5437f0dc).
- 10/22/2025 [No More Retokenization Drift: Returning Token IDs via the OpenAI Compatible API Matters in Agent RL](https://blog.vllm.ai/2025/10/22/agent-lightning.html) vLLM blog. See also [Zhihu writeup](https://zhuanlan.zhihu.com/p/1965067274642785725).
- 8/11/2025 [Training AI Agents to Write and Self-correct SQL with Reinforcement Learning](https://medium.com/@yugez/training-ai-agents-to-write-and-self-correct-sql-with-reinforcement-learning-571ed31281ad) Medium.
- 8/5/2025 [Agent Lightning: Train ANY AI Agents with Reinforcement Learning](https://arxiv.org/abs/2508.03680) arXiv paper.
- 7/26/2025 [We discovered an approach to train any AI agent with RL, with (almost) zero code changes.](https://www.reddit.com/r/LocalLLaMA/comments/1m9m670/we_discovered_an_approach_to_train_any_ai_agent/) Reddit.
- 6/6/2025 [Agent Lightning - Microsoft Research](https://www.microsoft.com/en-us/research/project/agent-lightning/) Project page.

## âš¡ Community Projects

- [DeepWerewolf](https://github.com/af-74413592/DeepWerewolf) â€” A case study of agent RL training for the Chinese Werewolf game built with AgentScope and Agent Lightning.
- [AgentFlow](https://agentflow.stanford.edu/) â€” A modular multi-agent framework that combines planner, executor, verifier, and generator agents with the Flow-GRPO algorithm to tackle long-horizon, sparse-reward tasks.
- [Youtu-Agent](https://github.com/TencentCloudADP/Youtu-agent) â€” Youtu-Agent lets you build and train your agent with ease. Built with [a modified branch](https://github.com/microsoft/agent-lightning/tree/contrib/youtu-agent-lightning) of Agent Lightning, Youtu-Agent has verified up to 128 GPUs RL training on maths/code and search capabilities with steady convergence. Also check [the recipe](https://github.com/TencentCloudADP/youtu-agent/tree/rl/agl) and their blog [*Stop Wrestling with Your Agent RL: How Youtu-Agent Achieved Stable, 128-GPU Scaling Without Breaking a Sweat*](https://spotted-coconut-df8.notion.site/Stop-Wrestling-with-Your-Agent-RL-How-Youtu-Agent-Achieved-Stable-128-GPU-Scaling-Without-Breaking-2ca5e8f089ba80539a98c582b65e0233).

## âš¡ Architecture

Agent Lightning keeps the moving parts to a minimum so you can focus on your idea, not the plumbing. Your agent continues to run as usual; you can still use any agent framework you like; you drop in the lightweight `agl.emit_xxx()` helper, or let the tracer collect every prompt, tool call, and reward. Those events become structured spans that flow into the LightningStore, a central hub that keeps tasks, resources, and traces in sync.

On the other side of the store sits the algorithm you choose, or write yourself. The algorithm reads spans, learns from them, and posts updated resources such as refined prompt templates or new policy weights. The Trainer ties it all together: it streams datasets to runners, ferries resources between the store and the algorithm, and updates the inference engine when improvements land. You can either stop there, or simply let the same loop keep turning.

No rewrites, no lock-in, just a clear path from first rollout to steady improvement.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/assets/readme-architecture.svg&quot; alt=&quot;Agent-lightning Architecture&quot; style=&quot;width:100%&quot;/&gt;
&lt;/p&gt;

## âš¡ CI Status

| Workflow | Status |
|----------|--------|
| CPU Tests | [![tests workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/tests.yml) |
| Full Tests | [![tests summary workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/badge-unit.yml) |
| UI Tests | [![UI Tests](https://github.com/microsoft/agent-lightning/actions/workflows/dashboard.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/dashboard.yml) |
| Examples Integration | [![examples summary workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/badge-examples.yml) |
| Latest Dependency Compatibility | [![latest summary workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/badge-latest.yml) |
| Legacy Examples Compatibility | [![compat summary workflow status](https://github.com/microsoft/agent-lightning/actions/workflows/badge-compat.yml/badge.svg)](https://github.com/microsoft/agent-lightning/actions/workflows/badge-compat.yml) |

## âš¡ Citation

If you find Agent Lightning useful in your research or projects, please cite our paper:

```bibtex
@misc{luo2025agentlightningtrainai,
      title={Agent Lightning: Train ANY AI Agents with Reinforcement Learning},
      author={Xufang Luo and Yuge Zhang and Zhiyuan He and Zilong Wang and Siyun Zhao and Dongsheng Li and Luna K. Qiu and Yuqing Yang},
      year={2025},
      eprint={2508.03680},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.03680},
}
```

## âš¡ Contributing

This project welcomes contributions and suggestions. Start by reading the [Contributing Guide](docs/community/contributing.md) for recommended contribution points, environment setup, branching conventions, and pull request expectations. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## âš¡ Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow [Microsoft&#039;s Trademark &amp; Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#039;s policies.

## âš¡ Responsible AI

This project has been evaluated and certified to comply with the Microsoft Responsible AI Standard. The team will continue to monitor and maintain the repository, addressing any severe issues, including potential harms, if they arise.

## âš¡ License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mlflow/mlflow]]></title>
            <link>https://github.com/mlflow/mlflow</link>
            <guid>https://github.com/mlflow/mlflow</guid>
            <pubDate>Fri, 20 Feb 2026 00:06:46 GMT</pubDate>
            <description><![CDATA[The open source developer platform to build AI agents and models with confidence. Enhance your AI applications with end-to-end tracking, observability, and evaluations, all in one integrated platform.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mlflow/mlflow">mlflow/mlflow</a></h1>
            <p>The open source developer platform to build AI agents and models with confidence. Enhance your AI applications with end-to-end tracking, observability, and evaluations, all in one integrated platform.</p>
            <p>Language: Python</p>
            <p>Stars: 24,317</p>
            <p>Forks: 5,306</p>
            <p>Stars today: 74 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot; style=&quot;border-bottom: none&quot;&gt;
    &lt;a href=&quot;https://mlflow.org/&quot;&gt;
        &lt;img alt=&quot;MLflow logo&quot; src=&quot;https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/logo.svg&quot; width=&quot;200&quot; /&gt;
    &lt;/a&gt;
&lt;/h1&gt;
&lt;h2 align=&quot;center&quot; style=&quot;border-bottom: none&quot;&gt;Open-Source Platform for Productionizing AI&lt;/h2&gt;

MLflow is an open-source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end **experiment tracking**, **observability**, and **evaluations**, all in one integrated platform.

&lt;div align=&quot;center&quot;&gt;

[![Python SDK](https://img.shields.io/pypi/v/mlflow)](https://pypi.org/project/mlflow/)
[![PyPI Downloads](https://img.shields.io/pypi/dm/mlflow)](https://pepy.tech/projects/mlflow)
[![License](https://img.shields.io/github/license/mlflow/mlflow)](https://github.com/mlflow/mlflow/blob/main/LICENSE)
&lt;a href=&quot;https://twitter.com/intent/follow?screen_name=mlflow&quot; target=&quot;_blank&quot;&gt;
&lt;img src=&quot;https://img.shields.io/twitter/follow/mlflow?logo=X&amp;color=%20%23f5f5f5&quot;
      alt=&quot;follow on X(Twitter)&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.linkedin.com/company/mlflow-org/&quot; target=&quot;_blank&quot;&gt;
&lt;img src=&quot;https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&amp;logoColor=fff&quot;
      alt=&quot;follow on LinkedIn&quot;&gt;&lt;/a&gt;
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/mlflow/mlflow)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
   &lt;div&gt;
      &lt;a href=&quot;https://mlflow.org/&quot;&gt;&lt;strong&gt;Website&lt;/strong&gt;&lt;/a&gt; Â·
      &lt;a href=&quot;https://mlflow.org/docs/latest&quot;&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt; Â·
      &lt;a href=&quot;https://github.com/mlflow/mlflow/issues/new/choose&quot;&gt;&lt;strong&gt;Feature Request&lt;/strong&gt;&lt;/a&gt; Â·
      &lt;a href=&quot;https://mlflow.org/blog&quot;&gt;&lt;strong&gt;News&lt;/strong&gt;&lt;/a&gt; Â·
      &lt;a href=&quot;https://www.youtube.com/@mlflowoss&quot;&gt;&lt;strong&gt;YouTube&lt;/strong&gt;&lt;/a&gt; Â·
      &lt;a href=&quot;https://lu.ma/mlflow?k=c&quot;&gt;&lt;strong&gt;Events&lt;/strong&gt;&lt;/a&gt;
   &lt;/div&gt;
&lt;/div&gt;

&lt;br&gt;

## ğŸš€ Installation

To install the MLflow Python package, run the following command:

```
pip install mlflow
```

## ğŸ“¦ Core Components

MLflow is **the only platform that provides a unified solution for all your AI/ML needs**, including LLMs, Agents, Deep Learning, and traditional machine learning.

### ğŸ’¡ For LLM / GenAI Developers

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-tracing.png&quot; alt=&quot;Tracing&quot; width=100%&gt;
    &lt;div align=&quot;center&quot;&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/llms/tracing/index.html&quot;&gt;&lt;strong&gt;ğŸ” Tracing / Observability&lt;/strong&gt;&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
        &lt;div&gt;Trace the internal states of your LLM/agentic applications for debugging quality issues and monitoring performance with ease.&lt;/div&gt;&lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/genai/tracing/quickstart/&quot;&gt;Getting Started â†’&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
    &lt;/div&gt;
    &lt;/td&gt;
    &lt;td&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-llm-eval.png&quot; alt=&quot;LLM Evaluation&quot; width=100%&gt;
    &lt;div align=&quot;center&quot;&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/genai/eval-monitor/&quot;&gt;&lt;strong&gt;ğŸ“Š LLM Evaluation&lt;/strong&gt;&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
        &lt;div&gt;A suite of automated model evaluation tools, seamlessly integrated with experiment tracking to compare across multiple versions.&lt;/div&gt;&lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/genai/eval-monitor/&quot;&gt;Getting Started â†’&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
    &lt;/div&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-prompt.png&quot; alt=&quot;Prompt Management&quot;&gt;
    &lt;div align=&quot;center&quot;&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/genai/prompt-version-mgmt/prompt-registry/&quot;&gt;&lt;strong&gt;ğŸ¤– Prompt Management&lt;/strong&gt;&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
        &lt;div&gt;Version, track, and reuse prompts across your organization, helping maintain consistency and improve collaboration in prompt development.&lt;/div&gt;&lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/genai/prompt-registry/create-and-edit-prompts/&quot;&gt;Getting Started â†’&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
    &lt;/div&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-logged-model.png&quot; alt=&quot;MLflow Hero&quot;&gt;
    &lt;div align=&quot;center&quot;&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/genai/prompt-version-mgmt/version-tracking/&quot;&gt;&lt;strong&gt;ğŸ“¦ App Version Tracking&lt;/strong&gt;&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
        &lt;div&gt;MLflow keeps track of many moving parts in your AI applications, such as models, prompts, tools, and code, with end-to-end lineage.&lt;/div&gt;&lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/genai/version-tracking/quickstart/&quot;&gt;Getting Started â†’&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
    &lt;/div&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

### ğŸ“ For Data Scientists

&lt;table&gt;
  &lt;tr&gt;
    &lt;td colspan=&quot;2&quot; align=&quot;center&quot; &gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-experiment.png&quot; alt=&quot;Tracking&quot; width=50%&gt;
    &lt;div align=&quot;center&quot;&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/ml/tracking/&quot;&gt;&lt;strong&gt;ğŸ“ Experiment Tracking&lt;/strong&gt;&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
        &lt;div&gt;Track your models, parameters, metrics, and evaluation results in ML experiments and compare them using an interactive UI.&lt;/div&gt;&lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/ml/tracking/quickstart/&quot;&gt;Getting Started â†’&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
    &lt;/div&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-model-registry.png&quot; alt=&quot;Model Registry&quot; width=100%&gt;
    &lt;div align=&quot;center&quot;&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/ml/model-registry/&quot;&gt;&lt;strong&gt;ğŸ’¾ Model Registry&lt;/strong&gt;&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
        &lt;div&gt; A centralized model store designed to collaboratively manage the full lifecycle and deployment of machine learning models.&lt;/div&gt;&lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/ml/model-registry/tutorial/&quot;&gt;Getting Started â†’&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
    &lt;/div&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;img src=&quot;https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-deployment.png&quot; alt=&quot;Deployment&quot; width=100%&gt;
    &lt;div align=&quot;center&quot;&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/ml/deployment/&quot;&gt;&lt;strong&gt;ğŸš€ Deployment&lt;/strong&gt;&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
        &lt;div&gt; Tools for seamless model deployment to batch and real-time scoring on platforms like Docker, Kubernetes, Azure ML, and AWS SageMaker.&lt;/div&gt;&lt;br&gt;
        &lt;a href=&quot;https://mlflow.org/docs/latest/ml/deployment/&quot;&gt;Getting Started â†’&lt;/a&gt;
        &lt;br&gt;&lt;br&gt;
    &lt;/div&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

## ğŸŒ Hosting MLflow Anywhere

&lt;div align=&quot;center&quot; &gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-providers.png&quot; alt=&quot;Providers&quot; width=100%&gt;
&lt;/div&gt;

You can run MLflow in many different environments, including local machines, on-premise servers, and cloud infrastructure.

Trusted by thousands of organizations, MLflow is now offered as a managed service by most major cloud providers:

- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/)
- [Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow?view=azureml-api-2)
- [Databricks](https://www.databricks.com/product/managed-mlflow)
- [Nebius](https://nebius.com/services/managed-mlflow)

For hosting MLflow on your own infrastructure, please refer to [this guidance](https://mlflow.org/docs/latest/ml/tracking/#tracking-setup).

## ğŸ—£ï¸ Supported Programming Languages

- [Python](https://pypi.org/project/mlflow/)
- [TypeScript / JavaScript](https://www.npmjs.com/package/mlflow-tracing)
- [Java](https://mvnrepository.com/artifact/org.mlflow/mlflow-client)
- [R](https://cran.r-project.org/web/packages/mlflow/readme/README.html)

## ğŸ”— Integrations

MLflow is natively integrated with many popular machine learning frameworks and GenAI libraries.

![Integrations](https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-integrations.png)

## Usage Examples

### Tracing (Observability) ([Doc](https://mlflow.org/docs/latest/llms/tracing/index.html))

MLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call `mlflow.xyz.autolog()` before running your models. Refer to the documentation for customization and manual instrumentation.

```python
import mlflow
from openai import OpenAI

# Enable tracing for OpenAI
mlflow.openai.autolog()

# Query OpenAI LLM normally
response = OpenAI().chat.completions.create(
    model=&quot;gpt-4o-mini&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hi!&quot;}],
    temperature=0.1,
)
```

Then navigate to the &quot;Traces&quot; tab in the MLflow UI to find the trace records for the OpenAI query.

### Evaluating LLMs, Prompts, and Agents ([Doc](https://mlflow.org/docs/latest/genai/eval-monitor/index.html))

The following example runs automatic evaluation for question-answering tasks with several built-in metrics.

```python
import os
import openai
import mlflow
from mlflow.genai.scorers import Correctness, Guidelines

client = openai.OpenAI(api_key=os.getenv(&quot;OPENAI_API_KEY&quot;))

# 1. Define a simple QA dataset
dataset = [
    {
        &quot;inputs&quot;: {&quot;question&quot;: &quot;Can MLflow manage prompts?&quot;},
        &quot;expectations&quot;: {&quot;expected_response&quot;: &quot;Yes!&quot;},
    },
    {
        &quot;inputs&quot;: {&quot;question&quot;: &quot;Can MLflow create a taco for my lunch?&quot;},
        &quot;expectations&quot;: {
            &quot;expected_response&quot;: &quot;No, unfortunately, MLflow is not a taco maker.&quot;
        },
    },
]


# 2. Define a prediction function to generate responses
def predict_fn(question: str) -&gt; str:
    response = client.chat.completions.create(
        model=&quot;gpt-4o-mini&quot;, messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: question}]
    )
    return response.choices[0].message.content


# 3. Run the evaluation
results = mlflow.genai.evaluate(
    data=dataset,
    predict_fn=predict_fn,
    scorers=[
        # Built-in LLM judge
        Correctness(),
        # Custom criteria using LLM judge
        Guidelines(name=&quot;is_english&quot;, guidelines=&quot;The answer must be in English&quot;),
    ],
)
```

Navigate to the &quot;Evaluations&quot; tab in the MLflow UI to find the evaluation results.

### Tracking Model Training ([Doc](https://mlflow.org/docs/latest/ml/tracking/))

The following example trains a simple regression model with scikit-learn, while enabling MLflow&#039;s [autologging](https://mlflow.org/docs/latest/tracking/autolog.html) feature for experiment tracking.

```python
import mlflow

from sklearn.model_selection import train_test_split
from sklearn.datasets import load_diabetes
from sklearn.ensemble import RandomForestRegressor

# Enable MLflow&#039;s automatic experiment tracking for scikit-learn
mlflow.sklearn.autolog()

# Load the training dataset
db = load_diabetes()
X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)

rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)
# MLflow triggers logging automatically upon model fitting
rf.fit(X_train, y_train)
```

Once the above code finishes, run the following command in a separate terminal and access the MLflow UI via the printed URL. An MLflow **Run** should be automatically created, which tracks the training dataset, hyperparameters, performance metrics, the trained model, dependencies, and even more.

```
mlflow server
```

## ğŸ’­ Support

- For help or questions about MLflow usage (e.g. &quot;how do I do X?&quot;) visit the [documentation](https://mlflow.org/docs/latest).
- In the documentation, you can ask the question to our AI-powered chat bot. Click on the **&quot;Ask AI&quot;** button at the right bottom.
- Join the [virtual events](https://lu.ma/mlflow?k=c) like office hours and meetups.
- To report a bug, file a documentation issue, or submit a feature request, please [open a GitHub issue](https://github.com/mlflow/mlflow/issues/new/choose).
- For release announcements and other discussions, please subscribe to our mailing list (mlflow-users@googlegroups.com)
  or join us on [Slack](https://mlflow.org/slack).

## ğŸ¤ Contributing

We happily welcome contributions to MLflow!

- Submit [bug reports](https://github.com/mlflow/mlflow/issues/new?template=bug_report_template.yaml) and [feature requests](https://github.com/mlflow/mlflow/issues/new?template=feature_request_template.yaml)
- Contribute for [good-first-issues](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) and [help-wanted](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)
- Writing about MLflow and sharing your experience

Please see our [contribution guide](CONTRIBUTING.md) to learn more about contributing to MLflow.

## â­ï¸ Star History

&lt;a href=&quot;https://star-history.com/#mlflow/mlflow&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=mlflow/mlflow&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=mlflow/mlflow&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=mlflow/mlflow&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;

## âœï¸ Citation

If you use MLflow in your research, please cite it using the &quot;Cite this repository&quot; button at the top of the [GitHub repository page](https://github.com/mlflow/mlflow), which will provide you with citation formats including APA and BibTeX.

## ğŸ‘¥ Core Members

MLflow is currently maintained by the following core members with significant contributions from hundreds of exceptionally talented community members.

- [Ben Wilson](https://github.com/BenWilson2)
- [Corey Zumar](https://github.com/dbczumar)
- [Daniel Lok](https://github.com/daniellok-db)
- [Gabriel Fu](https://github.com/gabrielfu)
- [Harutaka Kawamura](https://github.com/harupy)
- [Joel Robin P](https://github.com/joelrobin18)
- [Matt Prahl](https://github.com/mprahl)
- [Serena Ruan](https://github.com/serena-ruan)
- [Tomu Hirata](https://github.com/TomeHirata)
- [Weichen Xu](https://github.com/WeichenXu123)
- [Yuki Watanabe](https://github.com/B-Step62)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[scikit-learn/scikit-learn]]></title>
            <link>https://github.com/scikit-learn/scikit-learn</link>
            <guid>https://github.com/scikit-learn/scikit-learn</guid>
            <pubDate>Fri, 20 Feb 2026 00:06:45 GMT</pubDate>
            <description><![CDATA[scikit-learn: machine learning in Python]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/scikit-learn/scikit-learn">scikit-learn/scikit-learn</a></h1>
            <p>scikit-learn: machine learning in Python</p>
            <p>Language: Python</p>
            <p>Stars: 65,176</p>
            <p>Forks: 26,709</p>
            <p>Stars today: 79 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[getzep/graphiti]]></title>
            <link>https://github.com/getzep/graphiti</link>
            <guid>https://github.com/getzep/graphiti</guid>
            <pubDate>Fri, 20 Feb 2026 00:06:44 GMT</pubDate>
            <description><![CDATA[Build Real-Time Knowledge Graphs for AI Agents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/getzep/graphiti">getzep/graphiti</a></h1>
            <p>Build Real-Time Knowledge Graphs for AI Agents</p>
            <p>Language: Python</p>
            <p>Stars: 22,934</p>
            <p>Forks: 2,259</p>
            <p>Stars today: 37 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.getzep.com/&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73&quot; width=&quot;150&quot; alt=&quot;Zep Logo&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt;
Graphiti
&lt;/h1&gt;
&lt;h2 align=&quot;center&quot;&gt; Build Real-Time Knowledge Graphs for AI Agents&lt;/h2&gt;
&lt;div align=&quot;center&quot;&gt;

[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)
[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)
[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)

![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)
[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?&amp;logo=discord&amp;logoColor=white)](https://discord.com/invite/W8Kw6bsgXQ)
[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)
[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&amp;label=Release&amp;color=limegreen)](https://github.com/getzep/graphiti/releases)

&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://trendshift.io/repositories/12986&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12986&quot; alt=&quot;getzep%2Fgraphiti | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;

:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_

&lt;br /&gt;

&gt; [!TIP]
&gt; Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful
&gt; Knowledge Graph-based memory.

Graphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents
operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti
continuously integrates user interactions, structured and unstructured enterprise data, and external information into a
coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical
queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI
applications.

Use Graphiti to:

- Integrate and maintain dynamic user interactions and business data.
- Facilitate state-based reasoning and task automation for agents.
- Query complex, evolving data with semantic, keyword, and graph-based search methods.

&lt;br /&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;images/graphiti-graph-intro.gif&quot; alt=&quot;Graphiti temporal walkthrough&quot; width=&quot;700px&quot;&gt;
&lt;/p&gt;

&lt;br /&gt;

A knowledge graph is a network of interconnected facts, such as _&quot;Kendra loves Adidas shoes.&quot;_ Each fact is a &quot;triplet&quot;
represented by two entities, or
nodes (&quot;Kendra&quot;, &quot;Adidas shoes&quot;), and their relationship, or edge (&quot;loves&quot;). Knowledge Graphs have been explored
extensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph
while handling changing relationships and maintaining historical context.

## Graphiti and Zep&#039;s Context Engineering Platform.

Graphiti powers the core of [Zep&#039;s context engineering platform](https://www.getzep.com) for AI Agents. Zep
offers agent memory, Graph RAG for dynamic data, and context retrieval and assembly.

Using Graphiti, we&#039;ve demonstrated Zep is
the [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).

Read our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).

We&#039;re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2501.13956&quot;&gt;&lt;img src=&quot;images/arxiv-screenshot.png&quot; alt=&quot;Zep: A Temporal Knowledge Graph Architecture for Agent Memory&quot; width=&quot;700px&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

## Zep vs Graphiti

| Aspect | Zep | Graphiti |
|--------|-----|----------|
| **What they are** | Fully managed platform for context engineering and AI memory | Open-source graph framework |
| **User &amp; conversation management** | Built-in users, threads, and message storage | Build your own |
| **Retrieval &amp; performance** | Pre-configured, production-ready retrieval with sub-200ms performance at scale | Custom implementation required; performance depends on your setup |
| **Developer tools** | Dashboard with graph visualization, debug logs, API logs; SDKs for Python, TypeScript, and Go | Build your own tools |
| **Enterprise features** | SLAs, support, security guarantees | Self-managed |
| **Deployment** | Fully managed or in your cloud | Self-hosted only |

### When to choose which

**Choose Zep** if you want a turnkey, enterprise-grade platform with security, performance, and support baked in.

**Choose Graphiti** if you want a flexible OSS core and you&#039;re comfortable building/operating the surrounding system.

## Why Graphiti?

Traditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for
frequently changing data. Graphiti addresses these challenges by providing:

- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.
- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time
  queries.
- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve
  low-latency queries without reliance on LLM summarization.
- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through
  straightforward Pydantic models.
- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/images/graphiti-intro-slides-stock-2.gif&quot; alt=&quot;Graphiti structured + unstructured demo&quot; width=&quot;700px&quot;&gt;
&lt;/p&gt;

## Graphiti vs. GraphRAG

| Aspect                     | GraphRAG                              | Graphiti                                         |
|----------------------------|---------------------------------------|--------------------------------------------------|
| **Primary Use**            | Static document summarization         | Dynamic data management                          |
| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |
| **Knowledge Structure**    | Entity clusters &amp; community summaries | Episodic data, semantic entities, communities    |
| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |
| **Adaptability**           | Low                                   | High                                             |
| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |
| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |
| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |
| **Custom Entity Types**    | No                                    | Yes, customizable                                |
| **Scalability**            | Moderate                              | High, optimized for large datasets               |

Graphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it
particularly suitable for applications requiring real-time interaction and precise historical queries.

## Installation

Requirements:

- Python 3.10 or higher
- Neo4j 5.26 / FalkorDB 1.1.2 / Kuzu 0.11.2 / Amazon Neptune Database Cluster or Neptune Analytics Graph + Amazon
  OpenSearch Serverless collection (serves as the full text search backend)
- OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)

&gt; [!IMPORTANT]
&gt; Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).
&gt; Using other services may result in incorrect output schemas and ingestion failures. This is particularly
&gt; problematic when using smaller models.

Optional:

- Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)

&gt; [!TIP]
&gt; The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly
&gt; interface to manage Neo4j instances and databases.
&gt; Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:

```bash
docker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest

```

```bash
pip install graphiti-core
```

or

```bash
uv add graphiti-core
```

### Installing with FalkorDB Support

If you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:

```bash
pip install graphiti-core[falkordb]

# or with uv
uv add graphiti-core[falkordb]
```

### Installing with Kuzu Support

If you plan to use Kuzu as your graph database backend, install with the Kuzu extra:

```bash
pip install graphiti-core[kuzu]

# or with uv
uv add graphiti-core[kuzu]
```

### Installing with Amazon Neptune Support

If you plan to use Amazon Neptune as your graph database backend, install with the Amazon Neptune extra:

```bash
pip install graphiti-core[neptune]

# or with uv
uv add graphiti-core[neptune]
```

### You can also install optional LLM providers as extras:

```bash
# Install with Anthropic support
pip install graphiti-core[anthropic]

# Install with Groq support
pip install graphiti-core[groq]

# Install with Google Gemini support
pip install graphiti-core[google-genai]

# Install with multiple providers
pip install graphiti-core[anthropic,groq,google-genai]

# Install with FalkorDB and LLM providers
pip install graphiti-core[falkordb,anthropic,google-genai]

# Install with Amazon Neptune
pip install graphiti-core[neptune]
```

## Default to Low Concurrency; LLM Provider 429 Rate Limit Errors

Graphiti&#039;s ingestion pipelines are designed for high concurrency. By default, concurrency is set low to avoid LLM
Provider 429 Rate Limit Errors. If you find Graphiti slow, please increase concurrency as described below.

Concurrency controlled by the `SEMAPHORE_LIMIT` environment variable. By default, `SEMAPHORE_LIMIT` is set to `10`
concurrent operations to help prevent `429` rate limit errors from your LLM provider. If you encounter such errors, try
lowering this value.

If your LLM provider allows higher throughput, you can increase `SEMAPHORE_LIMIT` to boost episode ingestion
performance.

## Quick Start

&gt; [!IMPORTANT]
&gt; Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your
&gt; environment.
&gt; Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI
&gt; compatible APIs.

For a complete working example, see the [Quickstart Example](./examples/quickstart/README.md) in the examples directory.
The quickstart demonstrates:

1. Connecting to a Neo4j, Amazon Neptune, FalkorDB, or Kuzu database
2. Initializing Graphiti indices and constraints
3. Adding episodes to the graph (both text and structured JSON)
4. Searching for relationships (edges) using hybrid search
5. Reranking search results using graph distance
6. Searching for nodes using predefined search recipes

The example is fully documented with clear explanations of each functionality and includes a comprehensive README with
setup instructions and next steps.

### Running with Docker Compose

You can use Docker Compose to quickly start the required services:

- **Neo4j Docker:**
  ```sh
  docker compose up
  ```
  This will start the Neo4j Docker service and related components.

- **FalkorDB Docker:**
  ```sh
  docker compose --profile falkordb up
  ```
  This will start the FalkorDB Docker service and related components.

## MCP Server

The `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server
allows AI assistants to interact with Graphiti&#039;s knowledge graph capabilities through the MCP protocol.

Key features of the MCP server include:

- Episode management (add, retrieve, delete)
- Entity management and relationship handling
- Semantic and hybrid search capabilities
- Group management for organizing related data
- Graph maintenance operations

The MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant
workflows.

For detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md).

## REST Service

The `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.

Please see the [server README](./server/README.md) for more information.

## Optional Environment Variables

In addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.
If you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables
must be set.

### Database Configuration

Database names are configured directly in the driver constructors:

- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)
- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)

As of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it
to the Graphiti constructor using the `graph_driver` parameter.

#### Neo4j with Custom Database Name

```python
from graphiti_core import Graphiti
from graphiti_core.driver.neo4j_driver import Neo4jDriver

# Create a Neo4j driver with custom database name
driver = Neo4jDriver(
    uri=&quot;bolt://localhost:7687&quot;,
    user=&quot;neo4j&quot;,
    password=&quot;password&quot;,
    database=&quot;my_custom_database&quot;  # Custom database name
)

# Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver)
```

#### FalkorDB with Custom Database Name

```python
from graphiti_core import Graphiti
from graphiti_core.driver.falkordb_driver import FalkorDriver

# Create a FalkorDB driver with custom database name
driver = FalkorDriver(
    host=&quot;localhost&quot;,
    port=6379,
    username=&quot;falkor_user&quot;,  # Optional
    password=&quot;falkor_password&quot;,  # Optional
    database=&quot;my_custom_graph&quot;  # Custom database name
)

# Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver)
```

#### Kuzu

```python
from graphiti_core import Graphiti
from graphiti_core.driver.kuzu_driver import KuzuDriver

# Create a Kuzu driver
driver = KuzuDriver(db=&quot;/tmp/graphiti.kuzu&quot;)

# Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver)
```

#### Amazon Neptune

```python
from graphiti_core import Graphiti
from graphiti_core.driver.neptune_driver import NeptuneDriver

# Create a FalkorDB driver with custom database name
driver = NeptuneDriver(
    host= &lt; NEPTUNE
ENDPOINT &gt;,
aoss_host = &lt; Amazon
OpenSearch
Serverless
Host &gt;,
port = &lt; PORT &gt;  # Optional, defaults to 8182,
         aoss_port = &lt; PORT &gt;  # Optional, defaults to 443
)

driver = NeptuneDriver(host=neptune_uri, aoss_host=aoss_host, port=neptune_port)

# Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver)
```

## Graph Driver Architecture

Graphiti uses a pluggable driver architecture so the core framework is backend-agnostic. All database-specific logic
is encapsulated in driver implementations, allowing you to swap backends or add new ones without modifying the rest of
the framework.

### How Drivers are Integrated

The driver layer is organized into three tiers:

1. **`GraphDriver` ABC** (`graphiti_core/driver/driver.py`) â€” the core interface every backend must implement. It
   defines query execution, session management, index lifecycle, and exposes 11 operations interfaces as `@property`
   accessors.

2. **`GraphProvider` enum** â€” identifies the backend (`NEO4J`, `FALKORDB`, `KUZU`, `NEPTUNE`). Query builders use this
   enum in `match/case` statements to return dialect-specific query strings.

3. **11 Operations ABCs** (`graphiti_core/driver/operations/`) â€” abstract interfaces covering all CRUD and search
   operations for every graph element type:
   - **Node ops:** `EntityNodeOperations`, `EpisodeNodeOperations`, `CommunityNodeOperations`, `SagaNodeOperations`
   - **Edge ops:** `EntityEdgeOperations`, `EpisodicEdgeOperations`, `CommunityEdgeOperations`,
     `HasEpisodeEdgeOperations`, `NextEpisodeEdgeOperations`
   - **Search &amp; maintenance:** `SearchOperations`, `GraphMaintenanceOperations`

Each backend provides a concrete driver class and a matching `operations/` directory with implementations of all 11
ABCs. The key directories and files are shown below (simplified; see source for complete structure):

```
graphiti_core/driver/
â”œâ”€â”€ driver.py                        # GraphDriver ABC, GraphProvider enum
â”œâ”€â”€ query_executor.py                # QueryExecutor protocol
â”œâ”€â”€ record_parsers.py                # Shared record â†’ model conversion
â”œâ”€â”€ operations/                      # 11 operation ABCs
â”‚   â”œâ”€â”€ entity_node_ops.py
â”‚   â”œâ”€â”€ episode_node_ops.py
â”‚   â”œâ”€â”€ community_node_ops.py
â”‚   â”œâ”€â”€ saga_node_ops.py
â”‚   â”œâ”€â”€ entity_edge_ops.py
â”‚   â”œâ”€â”€ episodic_edge_ops.py
â”‚   â”œâ”€â”€ community_edge_ops.py
â”‚   â”œâ”€â”€ has_episode_edge_ops.py
â”‚   â”œâ”€â”€ next_episode_edge_ops.py
â”‚   â”œâ”€â”€ search_ops.py
â”‚   â”œâ”€â”€ graph_ops.py
â”‚   â””â”€â”€ graph_utils.py              # Shared algorithms (e.g., label propagation)
â”œâ”€â”€ graph_operations/                # Legacy graph operations interface
â”œâ”€â”€ search_interface/                # Legacy search interface
â”œâ”€â”€ neo4j_driver.py                  # Neo4jDriver
â”œâ”€â”€ neo4j/operations/                # 11 Neo4j implementations
â”œâ”€â”€ falkordb_driver.py               # FalkorDriver
â”œâ”€â”€ falkordb/operations/             # 11 FalkorDB implementations
â”œâ”€â”€ kuzu_driver.py                   # KuzuDriver
â”œâ”€â”€ kuzu/operations/                 # 11 Kuzu implementations + record_parsers.py
â”œâ”€â”€ neptune_driver.py                # NeptuneDriver
â””â”€â”€ neptune/operations/              # 11 Neptune implementations
```

Operations are decoupled from the driver itself â€” each operation method receives an `executor: QueryExecutor` parameter
(a protocol for running queries) rather than a concrete `GraphDriver`, which makes operations testable and
driver-agnostic. The driver class instantiates all 11 operation classes in its `__init__` and exposes them as
properties. The base `GraphDriver` ABC defines each property with an optional return type (`| None`, defaulting to
`None`); concrete drivers override these to return their implementations:

```python
# In your concrete driver (e.g., Neo4jDriver):
@property
def entity_node_ops(self) -&gt; EntityNodeOperations:
    return self._entity_node_ops
```

Provider-specific query strings are generated by shared query builders in `graphiti_core/models/nodes/node_db_queries.py`
and `graphiti_core/models/edges/edge_db_queries.py`, which use `match/case` on the `GraphProvider` enum to return the
correct dialect for each backend.

### Adding a New Graph Driver

To integrate a new graph database backend, follow these steps:

1. **Add to `GraphProvider`** â€” add your enum value in `graphiti_core/driver/driver.py`:
   ```python
   class GraphProvider(Enum):
       NEO4J = &#039;neo4j&#039;
       FALKORDB = &#039;falkordb&#039;
       KUZU = &#039;kuzu&#039;
       NEPTUNE = &#039;neptune&#039;
       MY_BACKEND = &#039;my_backend&#039;  # New backend
   ```

2. **Create directory structure** â€” create `graphiti_core/driver/&lt;backend&gt;/operations/` with an `__init__.py` exporting
   all 11 operation classes.

3. **Implement `GraphDriver` subclass** â€” create `graphiti_core/driver/&lt;backend&gt;_driver.py`:
   - Set `provider = GraphProvide

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[strands-agents/sdk-python]]></title>
            <link>https://github.com/strands-agents/sdk-python</link>
            <guid>https://github.com/strands-agents/sdk-python</guid>
            <pubDate>Fri, 20 Feb 2026 00:06:43 GMT</pubDate>
            <description><![CDATA[A model-driven approach to building AI agents in just a few lines of code.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/strands-agents/sdk-python">strands-agents/sdk-python</a></h1>
            <p>A model-driven approach to building AI agents in just a few lines of code.</p>
            <p>Language: Python</p>
            <p>Stars: 5,132</p>
            <p>Forks: 656</p>
            <p>Stars today: 8 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;div&gt;
    &lt;a href=&quot;https://strandsagents.com&quot;&gt;
      &lt;img src=&quot;https://strandsagents.com/latest/assets/logo-github.svg&quot; alt=&quot;Strands Agents&quot; width=&quot;55px&quot; height=&quot;105px&quot;&gt;
    &lt;/a&gt;
  &lt;/div&gt;

  &lt;h1&gt;
    Strands Agents
  &lt;/h1&gt;

  &lt;h2&gt;
    A model-driven approach to building AI agents in just a few lines of code.
  &lt;/h2&gt;

  &lt;div align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://github.com/strands-agents/sdk-python/graphs/commit-activity&quot;&gt;&lt;img alt=&quot;GitHub commit activity&quot; src=&quot;https://img.shields.io/github/commit-activity/m/strands-agents/sdk-python&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/strands-agents/sdk-python/issues&quot;&gt;&lt;img alt=&quot;GitHub open issues&quot; src=&quot;https://img.shields.io/github/issues/strands-agents/sdk-python&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/strands-agents/sdk-python/pulls&quot;&gt;&lt;img alt=&quot;GitHub open pull requests&quot; src=&quot;https://img.shields.io/github/issues-pr/strands-agents/sdk-python&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/strands-agents/sdk-python/blob/main/LICENSE&quot;&gt;&lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/github/license/strands-agents/sdk-python&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://pypi.org/project/strands-agents/&quot;&gt;&lt;img alt=&quot;PyPI version&quot; src=&quot;https://img.shields.io/pypi/v/strands-agents&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://python.org&quot;&gt;&lt;img alt=&quot;Python versions&quot; src=&quot;https://img.shields.io/pypi/pyversions/strands-agents&quot;/&gt;&lt;/a&gt;
  &lt;/div&gt;
  
  &lt;p&gt;
    &lt;a href=&quot;https://strandsagents.com/&quot;&gt;Documentation&lt;/a&gt;
    â—† &lt;a href=&quot;https://github.com/strands-agents/samples&quot;&gt;Samples&lt;/a&gt;
    â—† &lt;a href=&quot;https://github.com/strands-agents/sdk-python&quot;&gt;Python SDK&lt;/a&gt;
    â—† &lt;a href=&quot;https://github.com/strands-agents/tools&quot;&gt;Tools&lt;/a&gt;
    â—† &lt;a href=&quot;https://github.com/strands-agents/agent-builder&quot;&gt;Agent Builder&lt;/a&gt;
    â—† &lt;a href=&quot;https://github.com/strands-agents/mcp-server&quot;&gt;MCP Server&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

Strands Agents is a simple yet powerful SDK that takes a model-driven approach to building and running AI agents. From simple conversational assistants to complex autonomous workflows, from local development to production deployment, Strands Agents scales with your needs.

## Feature Overview

- **Lightweight &amp; Flexible**: Simple agent loop that just works and is fully customizable
- **Model Agnostic**: Support for Amazon Bedrock, Anthropic, Gemini, LiteLLM, Llama, Ollama, OpenAI, Writer, and custom providers
- **Advanced Capabilities**: Multi-agent systems, autonomous agents, and streaming support
- **Built-in MCP**: Native support for Model Context Protocol (MCP) servers, enabling access to thousands of pre-built tools

## Quick Start

```bash
# Install Strands Agents
pip install strands-agents strands-agents-tools
```

```python
from strands import Agent
from strands_tools import calculator
agent = Agent(tools=[calculator])
agent(&quot;What is the square root of 1764&quot;)
```

&gt; **Note**: For the default Amazon Bedrock model provider, you&#039;ll need AWS credentials configured and model access enabled for Claude 4 Sonnet in the us-west-2 region. See the [Quickstart Guide](https://strandsagents.com/) for details on configuring other model providers.

## Installation

Ensure you have Python 3.10+ installed, then:

```bash
# Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows use: .venv\Scripts\activate

# Install Strands and tools
pip install strands-agents strands-agents-tools
```

## Features at a Glance

### Python-Based Tools

Easily build tools using Python decorators:

```python
from strands import Agent, tool

@tool
def word_count(text: str) -&gt; int:
    &quot;&quot;&quot;Count words in text.

    This docstring is used by the LLM to understand the tool&#039;s purpose.
    &quot;&quot;&quot;
    return len(text.split())

agent = Agent(tools=[word_count])
response = agent(&quot;How many words are in this sentence?&quot;)
```

**Hot Reloading from Directory:**
Enable automatic tool loading and reloading from the `./tools/` directory:

```python
from strands import Agent

# Agent will watch ./tools/ directory for changes
agent = Agent(load_tools_from_directory=True)
response = agent(&quot;Use any tools you find in the tools directory&quot;)
```

### MCP Support

Seamlessly integrate Model Context Protocol (MCP) servers:

```python
from strands import Agent
from strands.tools.mcp import MCPClient
from mcp import stdio_client, StdioServerParameters

aws_docs_client = MCPClient(
    lambda: stdio_client(StdioServerParameters(command=&quot;uvx&quot;, args=[&quot;awslabs.aws-documentation-mcp-server@latest&quot;]))
)

with aws_docs_client:
   agent = Agent(tools=aws_docs_client.list_tools_sync())
   response = agent(&quot;Tell me about Amazon Bedrock and how to use it with Python&quot;)
```

### Multiple Model Providers

Support for various model providers:

```python
from strands import Agent
from strands.models import BedrockModel
from strands.models.ollama import OllamaModel
from strands.models.llamaapi import LlamaAPIModel
from strands.models.gemini import GeminiModel
from strands.models.llamacpp import LlamaCppModel

# Bedrock
bedrock_model = BedrockModel(
  model_id=&quot;us.amazon.nova-pro-v1:0&quot;,
  temperature=0.3,
  streaming=True, # Enable/disable streaming
)
agent = Agent(model=bedrock_model)
agent(&quot;Tell me about Agentic AI&quot;)

# Google Gemini
gemini_model = GeminiModel(
  client_args={
    &quot;api_key&quot;: &quot;your_gemini_api_key&quot;,
  },
  model_id=&quot;gemini-2.5-flash&quot;,
  params={&quot;temperature&quot;: 0.7}
)
agent = Agent(model=gemini_model)
agent(&quot;Tell me about Agentic AI&quot;)

# Ollama
ollama_model = OllamaModel(
  host=&quot;http://localhost:11434&quot;,
  model_id=&quot;llama3&quot;
)
agent = Agent(model=ollama_model)
agent(&quot;Tell me about Agentic AI&quot;)

# Llama API
llama_model = LlamaAPIModel(
    model_id=&quot;Llama-4-Maverick-17B-128E-Instruct-FP8&quot;,
)
agent = Agent(model=llama_model)
response = agent(&quot;Tell me about Agentic AI&quot;)
```

Built-in providers:
 - [Amazon Bedrock](https://strandsagents.com/latest/user-guide/concepts/model-providers/amazon-bedrock/)
 - [Anthropic](https://strandsagents.com/latest/user-guide/concepts/model-providers/anthropic/)
 - [Gemini](https://strandsagents.com/latest/user-guide/concepts/model-providers/gemini/)
 - [Cohere](https://strandsagents.com/latest/user-guide/concepts/model-providers/cohere/)
 - [LiteLLM](https://strandsagents.com/latest/user-guide/concepts/model-providers/litellm/)
 - [llama.cpp](https://strandsagents.com/latest/user-guide/concepts/model-providers/llamacpp/)
 - [LlamaAPI](https://strandsagents.com/latest/user-guide/concepts/model-providers/llamaapi/)
 - [MistralAI](https://strandsagents.com/latest/user-guide/concepts/model-providers/mistral/)
 - [Ollama](https://strandsagents.com/latest/user-guide/concepts/model-providers/ollama/)
 - [OpenAI](https://strandsagents.com/latest/user-guide/concepts/model-providers/openai/)
 - [SageMaker](https://strandsagents.com/latest/user-guide/concepts/model-providers/sagemaker/)
 - [Writer](https://strandsagents.com/latest/user-guide/concepts/model-providers/writer/)

Custom providers can be implemented using [Custom Providers](https://strandsagents.com/latest/user-guide/concepts/model-providers/custom_model_provider/)

### Example tools

Strands offers an optional strands-agents-tools package with pre-built tools for quick experimentation:

```python
from strands import Agent
from strands_tools import calculator
agent = Agent(tools=[calculator])
agent(&quot;What is the square root of 1764&quot;)
```

It&#039;s also available on GitHub via [strands-agents/tools](https://github.com/strands-agents/tools).

### Bidirectional Streaming

&gt; **âš ï¸ Experimental Feature**: Bidirectional streaming is currently in experimental status. APIs may change in future releases as we refine the feature based on user feedback and evolving model capabilities.

Build real-time voice and audio conversations with persistent streaming connections. Unlike traditional request-response patterns, bidirectional streaming maintains long-running conversations where users can interrupt, provide continuous input, and receive real-time audio responses. Get started with your first BidiAgent by following the [Quickstart](https://strandsagents.com/latest/documentation/docs/user-guide/concepts/experimental/bidirectional-streaming/quickstart) guide. 

**Supported Model Providers:**
- Amazon Nova Sonic (v1, v2)
- Google Gemini Live
- OpenAI Realtime API

**Installation:**

```bash
# Server-side only (no audio I/O dependencies)
pip install strands-agents[bidi]

# With audio I/O support (includes PyAudio dependency)
pip install strands-agents[bidi,bidi-io]
```

**Quick Example:**

```python
import asyncio
from strands.experimental.bidi import BidiAgent
from strands.experimental.bidi.models import BidiNovaSonicModel
from strands.experimental.bidi.io import BidiAudioIO, BidiTextIO
from strands.experimental.bidi.tools import stop_conversation
from strands_tools import calculator

async def main():
    # Create bidirectional agent with Nova Sonic v2
    model = BidiNovaSonicModel()
    agent = BidiAgent(model=model, tools=[calculator, stop_conversation])

    # Setup audio and text I/O (requires bidi-io extra)
    audio_io = BidiAudioIO()
    text_io = BidiTextIO()

    # Run with real-time audio streaming
    # Say &quot;stop conversation&quot; to gracefully end the conversation
    await agent.run(
        inputs=[audio_io.input()],
        outputs=[audio_io.output(), text_io.output()]
    )

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

&gt; **Note**: `BidiAudioIO` and `BidiTextIO` require the `bidi-io` extra. For server-side deployments where audio I/O is handled by clients (browsers, mobile apps), install only `strands-agents[bidi]` and implement custom input/output handlers using the `BidiInput` and `BidiOutput` protocols.

**Configuration Options:**

```python
from strands.experimental.bidi.models import BidiNovaSonicModel

# Configure audio settings and turn detection (v2 only)
model = BidiNovaSonicModel(
    provider_config={
        &quot;audio&quot;: {
            &quot;input_rate&quot;: 16000,
            &quot;output_rate&quot;: 16000,
            &quot;voice&quot;: &quot;matthew&quot;
        },
        &quot;turn_detection&quot;: {
            &quot;endpointingSensitivity&quot;: &quot;MEDIUM&quot;  # HIGH, MEDIUM, or LOW
        },
        &quot;inference&quot;: {
            &quot;max_tokens&quot;: 2048,
            &quot;temperature&quot;: 0.7
        }
    }
)

# Configure I/O devices
audio_io = BidiAudioIO(
    input_device_index=0,  # Specific microphone
    output_device_index=1,  # Specific speaker
    input_buffer_size=10,
    output_buffer_size=10
)

# Text input mode (type messages instead of speaking)
text_io = BidiTextIO()
await agent.run(
    inputs=[text_io.input()],  # Use text input
    outputs=[audio_io.output(), text_io.output()]
)

# Multi-modal: Both audio and text input
await agent.run(
    inputs=[audio_io.input(), text_io.input()],  # Speak OR type
    outputs=[audio_io.output(), text_io.output()]
)
```

## Documentation

For detailed guidance &amp; examples, explore our documentation:

- [User Guide](https://strandsagents.com/)
- [Quick Start Guide](https://strandsagents.com/latest/user-guide/quickstart/)
- [Agent Loop](https://strandsagents.com/latest/user-guide/concepts/agents/agent-loop/)
- [Examples](https://strandsagents.com/latest/examples/)
- [API Reference](https://strandsagents.com/latest/api-reference/agent/)
- [Production &amp; Deployment Guide](https://strandsagents.com/latest/user-guide/deploy/operating-agents-in-production/)

## Contributing â¤ï¸

We welcome contributions! See our [Contributing Guide](CONTRIBUTING.md) for details on:
- Reporting bugs &amp; features
- Development setup
- Contributing via Pull Requests
- Code of Conduct
- Reporting of security issues

## License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>