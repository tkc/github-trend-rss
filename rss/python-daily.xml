<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Thu, 01 May 2025 00:04:42 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[hacksider/Deep-Live-Cam]]></title>
            <link>https://github.com/hacksider/Deep-Live-Cam</link>
            <guid>https://github.com/hacksider/Deep-Live-Cam</guid>
            <pubDate>Thu, 01 May 2025 00:04:42 GMT</pubDate>
            <description><![CDATA[real time face swap and one-click video deepfake with only a single image]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hacksider/Deep-Live-Cam">hacksider/Deep-Live-Cam</a></h1>
            <p>real time face swap and one-click video deepfake with only a single image</p>
            <p>Language: Python</p>
            <p>Stars: 54,926</p>
            <p>Forks: 7,899</p>
            <p>Stars today: 1,686 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;Deep-Live-Cam&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  Real-time face swap and video deepfake with a single click and only a single image.
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/11395&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11395&quot; alt=&quot;hacksider%2FDeep-Live-Cam | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/demo.gif&quot; alt=&quot;Demo GIF&quot; width=&quot;800&quot;&gt;
&lt;/p&gt;

##  Disclaimer

This deepfake software is designed to be a productive tool for the AI-generated media industry. It can assist artists in animating custom characters, creating engaging content, and even using models for clothing design.

We are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to the law and ethics. We may shut down the project or add watermarks if legally required.

- Ethical Use: Users are expected to use this software responsibly and legally. If using a real person&#039;s face, obtain their consent and clearly label any output as a deepfake when sharing online.

- Content Restrictions: The software includes built-in checks to prevent processing inappropriate media, such as nudity, graphic content, or sensitive material.

- Legal Compliance: We adhere to all relevant laws and ethical guidelines. If legally required, we may shut down the project or add watermarks to the output.

- User Responsibility: We are not responsible for end-user actions. Users must ensure their use of the software aligns with ethical standards and legal requirements.

By using this software, you agree to these terms and commit to using it in a manner that respects the rights and dignity of others.

Users are expected to use this software responsibly and legally. If using a real person&#039;s face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user actions.

## Exclusive v2.0 Quick Start - Pre-built (Windows / Nvidia)

  &lt;a href=&quot;https://deeplivecam.net/index.php/quickstart&quot;&gt; &lt;img src=&quot;https://github.com/user-attachments/assets/7d993b32-e3e8-4cd3-bbfb-a549152ebdd5&quot; width=&quot;285&quot; height=&quot;77&quot; /&gt;

##### This is the fastest build you can get if you have a discrete NVIDIA GPU.
 
###### These Pre-builts are perfect for non-technical users or those who don&#039;t have time to, or can&#039;t manually install all the requirements. Just a heads-up: this is an open-source project, so you can also install it manually. This will be 60 days ahead on the open source version.

## TLDR; Live Deepfake in just 3 Clicks
![easysteps](https://github.com/user-attachments/assets/af825228-852c-411b-b787-ffd9aac72fc6)
1. Select a face
2. Select which camera to use
3. Press live!

## Features &amp; Uses - Everything is in real-time

### Mouth Mask

**Retain your original mouth for accurate movement using Mouth Mask**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/ludwig.gif&quot; alt=&quot;resizable-gif&quot;&gt;
&lt;/p&gt;

### Face Mapping

**Use different faces on multiple subjects simultaneously**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/streamers.gif&quot; alt=&quot;face_mapping_source&quot;&gt;
&lt;/p&gt;

### Your Movie, Your Face

**Watch movies with any face in real-time**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/movie.gif&quot; alt=&quot;movie&quot;&gt;
&lt;/p&gt;

### Live Show

**Run Live shows and performances**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/live_show.gif&quot; alt=&quot;show&quot;&gt;
&lt;/p&gt;

### Memes

**Create Your Most Viral Meme Yet**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/meme.gif&quot; alt=&quot;show&quot; width=&quot;450&quot;&gt; 
  &lt;br&gt;
  &lt;sub&gt;Created using Many Faces feature in Deep-Live-Cam&lt;/sub&gt;
&lt;/p&gt;

### Omegle

**Surprise people on Omegle**

&lt;p align=&quot;center&quot;&gt;
  &lt;video src=&quot;https://github.com/user-attachments/assets/2e9b9b82-fa04-4b70-9f56-b1f68e7672d0&quot; width=&quot;450&quot; controls&gt;&lt;/video&gt;
&lt;/p&gt;

## Installation (Manual)

**Please be aware that the installation requires technical skills and is not for beginners. Consider downloading the prebuilt version.**

&lt;details&gt;
&lt;summary&gt;Click to see the process&lt;/summary&gt;

### Installation

This is more likely to work on your computer but will be slower as it utilizes the CPU.

**1. Set up Your Platform**

-   Python (3.10 recommended)
-   pip
-   git
-   [ffmpeg](https://www.youtube.com/watch?v=OlNWCpFdVMA) - ```iex (irm ffmpeg.tc.ht)```
-   [Visual Studio 2022 Runtimes (Windows)](https://visualstudio.microsoft.com/visual-cpp-build-tools/)

**2. Clone the Repository**

```bash
git clone https://github.com/hacksider/Deep-Live-Cam.git
cd Deep-Live-Cam
```

**3. Download the Models**

1. [GFPGANv1.4](https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth)
2. [inswapper\_128\_fp16.onnx](https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx)

Place these files in the &quot;**models**&quot; folder.

**4. Install Dependencies**

We highly recommend using a `venv` to avoid issues.

For Windows:
```bash
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
```

**For macOS:**

Apple Silicon (M1/M2/M3) requires specific setup:

```bash
# Install Python 3.10 (specific version is important)
brew install python@3.10

# Install tkinter package (required for the GUI)
brew install python-tk@3.10

# Create and activate virtual environment with Python 3.10
python3.10 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

** In case something goes wrong and you need to reinstall the virtual environment **

```bash
# Deactivate the virtual environment
rm -rf venv

# Reinstall the virtual environment
python -m venv venv
source venv/bin/activate

# install the dependencies again
pip install -r requirements.txt
```

**Run:** If you don&#039;t have a GPU, you can run Deep-Live-Cam using `python run.py`. Note that initial execution will download models (~300MB).

### GPU Acceleration

**CUDA Execution Provider (Nvidia)**

1. Install [CUDA Toolkit 11.8.0](https://developer.nvidia.com/cuda-11-8-0-download-archive)
2. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu==1.16.3
```

3. Usage:

```bash
python run.py --execution-provider cuda
```

**CoreML Execution Provider (Apple Silicon)**

Apple Silicon (M1/M2/M3) specific installation:

1. Make sure you&#039;ve completed the macOS setup above using Python 3.10.
2. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-silicon
pip install onnxruntime-silicon==1.13.1
```

3. Usage (important: specify Python 3.10):

```bash
python3.10 run.py --execution-provider coreml
```

**Important Notes for macOS:**
- You **must** use Python 3.10, not newer versions like 3.11 or 3.13
- Always run with `python3.10` command not just `python` if you have multiple Python versions installed
- If you get error about `_tkinter` missing, reinstall the tkinter package: `brew reinstall python-tk@3.10`
- If you get model loading errors, check that your models are in the correct folder
- If you encounter conflicts with other Python versions, consider uninstalling them:
  ```bash
  # List all installed Python versions
  brew list | grep python
  
  # Uninstall conflicting versions if needed
  brew uninstall --ignore-dependencies python@3.11 python@3.13
  
  # Keep only Python 3.10
  brew cleanup
  ```

**CoreML Execution Provider (Apple Legacy)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-coreml
pip install onnxruntime-coreml==1.13.1
```

2. Usage:

```bash
python run.py --execution-provider coreml
```

**DirectML Execution Provider (Windows)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-directml
pip install onnxruntime-directml==1.15.1
```

2. Usage:

```bash
python run.py --execution-provider directml
```

**OpenVINO‚Ñ¢ Execution Provider (Intel)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-openvino
pip install onnxruntime-openvino==1.15.0
```

2. Usage:

```bash
python run.py --execution-provider openvino
```
&lt;/details&gt;

## Usage

**1. Image/Video Mode**

-   Execute `python run.py`.
-   Choose a source face image and a target image/video.
-   Click &quot;Start&quot;.
-   The output will be saved in a directory named after the target video.

**2. Webcam Mode**

-   Execute `python run.py`.
-   Select a source face image.
-   Click &quot;Live&quot;.
-   Wait for the preview to appear (10-30 seconds).
-   Use a screen capture tool like OBS to stream.
-   To change the face, select a new source image.

## Tips and Tricks

Check out these helpful guides to get the most out of Deep-Live-Cam:

- [Unlocking the Secrets to the Perfect Deepfake Image](https://deeplivecam.net/index.php/blog/tips-and-tricks/unlocking-the-secrets-to-the-perfect-deepfake-image) - Learn how to create the best deepfake with full head coverage
- [Video Call with DeepLiveCam](https://deeplivecam.net/index.php/blog/tips-and-tricks/video-call-with-deeplivecam) - Make your meetings livelier by using DeepLiveCam with OBS and meeting software
- [Have a Special Guest!](https://deeplivecam.net/index.php/blog/tips-and-tricks/have-a-special-guest) - Tutorial on how to use face mapping to add special guests to your stream
- [Watch Deepfake Movies in Realtime](https://deeplivecam.net/index.php/blog/tips-and-tricks/watch-deepfake-movies-in-realtime) - See yourself star in any video without processing the video
- [Better Quality without Sacrificing Speed](https://deeplivecam.net/index.php/blog/tips-and-tricks/better-quality-without-sacrificing-speed) - Tips for achieving better results without impacting performance
- [Instant Vtuber!](https://deeplivecam.net/index.php/blog/tips-and-tricks/instant-vtuber) - Create a new persona/vtuber easily using Metahuman Creator

Visit our [official blog](https://deeplivecam.net/index.php/blog/tips-and-tricks) for more tips and tutorials.

## Command Line Arguments (Unmaintained)

```
options:
  -h, --help                                               show this help message and exit
  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image
  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video
  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory
  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)
  --keep-fps                                               keep original fps
  --keep-audio                                             keep original audio
  --keep-frames                                            keep temporary frames
  --many-faces                                             process every face
  --map-faces                                              map source target faces
  --mouth-mask                                             mask the mouth region
  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder
  --video-quality [0-51]                                   adjust output video quality
  --live-mirror                                            the live camera display as you see it in the front-facing camera frame
  --live-resizable                                         the live camera frame is resizable
  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB
  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)
  --execution-threads EXECUTION_THREADS                    number of execution threads
  -v, --version                                            show program&#039;s version number and exit
```

Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.

## Press

**We are always open to criticism and are ready to improve, that&#039;s why we didn&#039;t cherry-pick anything.**

 - [*&quot;Deep-Live-Cam goes viral, allowing anyone to become a digital doppelganger&quot;*](https://arstechnica.com/information-technology/2024/08/new-ai-tool-enables-real-time-face-swapping-on-webcams-raising-fraud-concerns/) - Ars Technica
 - [*&quot;Thanks Deep Live Cam, shapeshifters are among us now&quot;*](https://dataconomy.com/2024/08/15/what-is-deep-live-cam-github-deepfake/) - Dataconomy
 - [*&quot;This free AI tool lets you become anyone during video-calls&quot;*](https://www.newsbytesapp.com/news/science/deep-live-cam-ai-impersonation-tool-goes-viral/story) - NewsBytes
 - [*&quot;OK, this viral AI live stream software is truly terrifying&quot;*](https://www.creativebloq.com/ai/ok-this-viral-ai-live-stream-software-is-truly-terrifying) - Creative Bloq
 - [*&quot;Deepfake AI Tool Lets You Become Anyone in a Video Call With Single Photo&quot;*](https://petapixel.com/2024/08/14/deep-live-cam-deepfake-ai-tool-lets-you-become-anyone-in-a-video-call-with-single-photo-mark-zuckerberg-jd-vance-elon-musk/) - PetaPixel
 - [*&quot;Deep-Live-Cam Uses AI to Transform Your Face in Real-Time, Celebrities Included&quot;*](https://www.techeblog.com/deep-live-cam-ai-transform-face/) - TechEBlog
 - [*&quot;An AI tool that &quot;makes you look like anyone&quot; during a video call is going viral online&quot;*](https://telegrafi.com/en/a-tool-that-makes-you-look-like-anyone-during-a-video-call-is-going-viral-on-the-Internet/) - Telegrafi
 - [*&quot;This Deepfake Tool Turning Images Into Livestreams is Topping the GitHub Charts&quot;*](https://decrypt.co/244565/this-deepfake-tool-turning-images-into-livestreams-is-topping-the-github-charts) - Emerge
 - [*&quot;New Real-Time Face-Swapping AI Allows Anyone to Mimic Famous Faces&quot;*](https://www.digitalmusicnews.com/2024/08/15/face-swapping-ai-real-time-mimic/) - Digital Music News
 - [*&quot;This real-time webcam deepfake tool raises alarms about the future of identity theft&quot;*](https://www.diyphotography.net/this-real-time-webcam-deepfake-tool-raises-alarms-about-the-future-of-identity-theft/) - DIYPhotography
 - [*&quot;That&#039;s Crazy, Oh God. That&#039;s Fucking Freaky Dude... That&#039;s So Wild Dude&quot;*](https://www.youtube.com/watch?time_continue=1074&amp;v=py4Tc-Y8BcY) - SomeOrdinaryGamers
 - [*&quot;Alright look look look, now look chat, we can do any face we want to look like chat&quot;*](https://www.youtube.com/live/mFsCe7AIxq8?feature=shared&amp;t=2686) - IShowSpeed

## Credits

-   [ffmpeg](https://ffmpeg.org/): for making video-related operations easy
-   [deepinsight](https://github.com/deepinsight): for their [insightface](https://github.com/deepinsight/insightface) project which provided a well-made library and models. Please be reminded that the [use of the model is for non-commercial research purposes only](https://github.com/deepinsight/insightface?tab=readme-ov-file#license).
-   [havok2-htwo](https://github.com/havok2-htwo): for sharing the code for webcam
-   [GosuDRM](https://github.com/GosuDRM): for the open version of roop
-   [pereiraroland26](https://github.com/pereiraroland26): Multiple faces support
-   [vic4key](https://github.com/vic4key): For supporting/contributing to this project
-   [kier007](https://github.com/kier007): for improving the user experience
-   [qitianai](https://github.com/qitianai): for multi-lingual support
-   and [all developers](https://github.com/hacksider/Deep-Live-Cam/graphs/contributors) behind libraries used in this project.
-   Footnote: Please be informed that the base author of the code is [s0md3v](https://github.com/s0md3v/roop)
-   All the wonderful users who helped make this project go viral by starring the repo ‚ù§Ô∏è

[![Stargazers](https://reporoster.com/stars/hacksider/Deep-Live-Cam)](https://github.com/hacksider/Deep-Live-Cam/stargazers)

## Contributions

![Alt](https://repobeats.axiom.co/api/embed/fec8e29c45dfdb9c5916f3a7830e1249308d20e1.svg &quot;Repobeats analytics image&quot;)

## Stars to the Moon üöÄ

&lt;a href=&quot;https://star-history.com/#hacksider/deep-live-cam&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[QwenLM/Qwen-Agent]]></title>
            <link>https://github.com/QwenLM/Qwen-Agent</link>
            <guid>https://github.com/QwenLM/Qwen-Agent</guid>
            <pubDate>Thu, 01 May 2025 00:04:41 GMT</pubDate>
            <description><![CDATA[Agent framework and applications built upon Qwen>=2.0, featuring Function Calling, Code Interpreter, RAG, and Chrome extension.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/QwenLM/Qwen-Agent">QwenLM/Qwen-Agent</a></h1>
            <p>Agent framework and applications built upon Qwen>=2.0, featuring Function Calling, Code Interpreter, RAG, and Chrome extension.</p>
            <p>Language: Python</p>
            <p>Stars: 7,366</p>
            <p>Forks: 638</p>
            <p>Stars today: 188 stars today</p>
            <h2>README</h2><pre>[‰∏≠Êñá](https://github.com/QwenLM/Qwen-Agent/blob/main/README_CN.md) ÔΩú English

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/qwen_agent/logo-qwen-agent.png&quot; width=&quot;400&quot;/&gt;
&lt;p&gt;
&lt;br&gt;

&lt;p align=&quot;center&quot;&gt;
          üíú &lt;a href=&quot;https://chat.qwen.ai/&quot;&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbspü§ó &lt;a href=&quot;https://huggingface.co/Qwen&quot;&gt;Hugging Face&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbspü§ñ &lt;a href=&quot;https://modelscope.cn/organization/qwen&quot;&gt;ModelScope&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbsp üìë &lt;a href=&quot;https://qwenlm.github.io/&quot;&gt;Blog&lt;/a&gt; &amp;nbsp&amp;nbsp ÔΩú &amp;nbsp&amp;nbspüìñ &lt;a href=&quot;https://qwen.readthedocs.io/&quot;&gt;Documentation&lt;/a&gt;

&lt;br&gt;
üí¨ &lt;a href=&quot;https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png&quot;&gt;WeChat (ÂæÆ‰ø°)&lt;/a&gt;&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbspü´® &lt;a href=&quot;https://discord.gg/CV4E9rpNSD&quot;&gt;Discord&lt;/a&gt;&amp;nbsp&amp;nbsp
&lt;/p&gt;


Qwen-Agent is a framework for developing LLM applications based on the instruction following, tool usage, planning, and
memory capabilities of Qwen.
It also comes with example applications such as Browser Assistant, Code Interpreter, and Custom Assistant.
Now Qwen-Agent plays as the backend of [Qwen Chat](https://chat.qwen.ai/).

# News
* Mar 18, 2025: Support for the `reasoning_content` field; adjust the default [Function Call template](./qwen_agent/llm/fncall_prompts/nous_fncall_prompt.py), which is applicable to the Qwen2.5 series general models and QwQ-32B. If you need to use the old version of the template, please refer to the [example](./examples/function_calling.py) for passing parameters.
* üî•üî•üî•Mar 7, 2025: Added [QwQ-32B Tool-call Demo](./examples/assistant_qwq.py). It supports parallel, multi-step, and multi-turn tool calls.
* Dec 3, 2024: Upgrade GUI to Gradio 5 based. Note: GUI requires Python 3.10 or higher.
* Sep 18, 2024: Added [Qwen2.5-Math Demo](./examples/tir_math.py) to showcase the Tool-Integrated Reasoning capabilities of Qwen2.5-Math. Note: The python executor is not sandboxed and is intended for local testing only, not for production use.

# Getting Started

## Installation

- Install the stable version from PyPI:
```bash
pip install -U &quot;qwen-agent[gui,rag,code_interpreter,mcp]&quot;
# Or use `pip install -U qwen-agent` for the minimal requirements.
# The optional requirements, specified in double brackets, are:
#   [gui] for Gradio-based GUI support;
#   [rag] for RAG support;
#   [code_interpreter] for Code Interpreter support;
#   [mcp] for MCP support.
```

- Alternatively, you can install the latest development version from the source:
```bash
git clone https://github.com/QwenLM/Qwen-Agent.git
cd Qwen-Agent
pip install -e ./&quot;[gui,rag,code_interpreter,mcp]&quot;
# Or `pip install -e ./` for minimal requirements.
```

## Preparation: Model Service

You can either use the model service provided by Alibaba
Cloud&#039;s [DashScope](https://help.aliyun.com/zh/dashscope/developer-reference/quick-start), or deploy and use your own
model service using the open-source Qwen models.

- If you choose to use the model service offered by DashScope, please ensure that you set the environment
variable `DASHSCOPE_API_KEY` to your unique DashScope API key.

- Alternatively, if you prefer to deploy and use your own model service, please follow the instructions provided in the README of Qwen2 for deploying an OpenAI-compatible API service.
Specifically, consult the [vLLM](https://github.com/QwenLM/Qwen2?tab=readme-ov-file#vllm) section for high-throughput GPU deployment or the [Ollama](https://github.com/QwenLM/Qwen2?tab=readme-ov-file#ollama) section for local CPU (+GPU) deployment.
For the QwQ model, it is recommended to add the `--enable-reasoning` and `--reasoning-parser deepseek_r1` parameters when starting the service. **Do not** add the `--enable-auto-tool-choice` and `--tool-call-parser hermes` parameters, as Qwen-Agent will parse the tool outputs from vLLM on its own.

## Developing Your Own Agent

Qwen-Agent offers atomic components, such as LLMs (which inherit from `class BaseChatModel` and come with [function calling](https://github.com/QwenLM/Qwen-Agent/blob/main/examples/function_calling.py)) and Tools (which inherit
from `class BaseTool`), along with high-level components like Agents (derived from `class Agent`).

The following example illustrates the process of creating an agent capable of reading PDF files and utilizing tools, as
well as incorporating a custom tool:

```py
import pprint
import urllib.parse
import json5
from qwen_agent.agents import Assistant
from qwen_agent.tools.base import BaseTool, register_tool
from qwen_agent.utils.output_beautify import typewriter_print


# Step 1 (Optional): Add a custom tool named `my_image_gen`.
@register_tool(&#039;my_image_gen&#039;)
class MyImageGen(BaseTool):
    # The `description` tells the agent the functionality of this tool.
    description = &#039;AI painting (image generation) service, input text description, and return the image URL drawn based on text information.&#039;
    # The `parameters` tell the agent what input parameters the tool has.
    parameters = [{
        &#039;name&#039;: &#039;prompt&#039;,
        &#039;type&#039;: &#039;string&#039;,
        &#039;description&#039;: &#039;Detailed description of the desired image content, in English&#039;,
        &#039;required&#039;: True
    }]

    def call(self, params: str, **kwargs) -&gt; str:
        # `params` are the arguments generated by the LLM agent.
        prompt = json5.loads(params)[&#039;prompt&#039;]
        prompt = urllib.parse.quote(prompt)
        return json5.dumps(
            {&#039;image_url&#039;: f&#039;https://image.pollinations.ai/prompt/{prompt}&#039;},
            ensure_ascii=False)


# Step 2: Configure the LLM you are using.
llm_cfg = {
    # Use the model service provided by DashScope:
    &#039;model&#039;: &#039;qwen-max-latest&#039;,
    &#039;model_server&#039;: &#039;dashscope&#039;,
    # &#039;api_key&#039;: &#039;YOUR_DASHSCOPE_API_KEY&#039;,
    # It will use the `DASHSCOPE_API_KEY&#039; environment variable if &#039;api_key&#039; is not set here.

    # Use a model service compatible with the OpenAI API, such as vLLM or Ollama:
    # &#039;model&#039;: &#039;Qwen2.5-7B-Instruct&#039;,
    # &#039;model_server&#039;: &#039;http://localhost:8000/v1&#039;,  # base_url, also known as api_base
    # &#039;api_key&#039;: &#039;EMPTY&#039;,

    # (Optional) LLM hyperparameters for generation:
    &#039;generate_cfg&#039;: {
        &#039;top_p&#039;: 0.8
    }
}

# Step 3: Create an agent. Here we use the `Assistant` agent as an example, which is capable of using tools and reading files.
system_instruction = &#039;&#039;&#039;After receiving the user&#039;s request, you should:
- first draw an image and obtain the image url,
- then run code `request.get(image_url)` to download the image,
- and finally select an image operation from the given document to process the image.
Please show the image using `plt.show()`.&#039;&#039;&#039;
tools = [&#039;my_image_gen&#039;, &#039;code_interpreter&#039;]  # `code_interpreter` is a built-in tool for executing code.
files = [&#039;./examples/resource/doc.pdf&#039;]  # Give the bot a PDF file to read.
bot = Assistant(llm=llm_cfg,
                system_message=system_instruction,
                function_list=tools,
                files=files)

# Step 4: Run the agent as a chatbot.
messages = []  # This stores the chat history.
while True:
    # For example, enter the query &quot;draw a dog and rotate it 90 degrees&quot;.
    query = input(&#039;\nuser query: &#039;)
    # Append the user query to the chat history.
    messages.append({&#039;role&#039;: &#039;user&#039;, &#039;content&#039;: query})
    response = []
    response_plain_text = &#039;&#039;
    print(&#039;bot response:&#039;)
    for response in bot.run(messages=messages):
        # Streaming output.
        response_plain_text = typewriter_print(response, response_plain_text)
    # Append the bot responses to the chat history.
    messages.extend(response)
```

In addition to using built-in agent implementations such as `class Assistant`, you can also develop your own agent implemetation by inheriting from `class Agent`.

The framework also provides a convenient GUI interface, supporting the rapid deployment of Gradio Demos for Agents.
For example, in the case above, you can quickly launch a Gradio Demo using the following code:

```py
from qwen_agent.gui import WebUI
WebUI(bot).run()  # bot is the agent defined in the above code, we do not repeat the definition here for saving space.
```
Now you can chat with the Agent in the web UI. Please refer to the [examples](https://github.com/QwenLM/Qwen-Agent/blob/main/examples) directory for more usage examples.

# FAQ

## How to Use MCP?

You can select the required tools on the open-source [MCP server website](https://github.com/modelcontextprotocol/servers) and configure the relevant environment.

Example of MCP invocation format:
```
{
    &quot;mcpServers&quot;: {
        &quot;memory&quot;: {
            &quot;command&quot;: &quot;npx&quot;,
            &quot;args&quot;: [&quot;-y&quot;, &quot;@modelcontextprotocol/server-memory&quot;]
        },
        &quot;filesystem&quot;: {
            &quot;command&quot;: &quot;npx&quot;,
            &quot;args&quot;: [&quot;-y&quot;, &quot;@modelcontextprotocol/server-filesystem&quot;, &quot;/path/to/allowed/files&quot;]
        },
        &quot;sqlite&quot; : {
            &quot;command&quot;: &quot;uvx&quot;,
            &quot;args&quot;: [
                &quot;mcp-server-sqlite&quot;,
                &quot;--db-path&quot;,
                &quot;test.db&quot;
            ]
        }
    }
}
```
For more details, you can refer to the [MCP usage example](./examples/assistant_mcp_sqlite_bot.py)

The dependencies required to run this example are as follows:
```
# Node.js (Download and install the latest version from the Node.js official website)
# uv 0.4.18 or higher (Check with uv --version)
# Git (Check with git --version)
# SQLite (Check with sqlite3 --version)

# For macOS users, you can install these components using Homebrew:
brew install uv git sqlite3

# For Windows users, you can install these components using winget:
winget install --id=astral-sh.uv -e
winget install git.git sqlite.sqlite
```
## Do you have function calling (aka tool calling)?

Yes. The LLM classes provide [function calling](https://github.com/QwenLM/Qwen-Agent/blob/main/examples/function_calling.py). Additionally, some Agent classes also are built upon the function calling capability, e.g., FnCallAgent and ReActChat.

## How to do question-answering over super-long documents involving 1M tokens?

We have released [a fast RAG solution](https://github.com/QwenLM/Qwen-Agent/blob/main/examples/assistant_rag.py), as well as [an expensive but competitive agent](https://github.com/QwenLM/Qwen-Agent/blob/main/examples/parallel_doc_qa.py), for doing question-answering over super-long documents. They have managed to outperform native long-context models on two challenging benchmarks while being more efficient, and perform perfectly in the single-needle &quot;needle-in-the-haystack&quot; pressure test involving 1M-token contexts. See the [blog](https://qwenlm.github.io/blog/qwen-agent-2405/) for technical details.

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/assets/qwen_agent/qwen-agent-2405-blog-long-context-results.png&quot; width=&quot;400&quot;/&gt;
&lt;p&gt;

# Application: BrowserQwen

BrowserQwen is a browser assistant built upon Qwen-Agent. Please refer to its [documentation](https://github.com/QwenLM/Qwen-Agent/blob/main/browser_qwen.md) for details.

# Disclaimer

The code interpreter is not sandboxed, and it executes code in your own environment. Please do not ask Qwen to perform dangerous tasks, and do not directly use the code interpreter for production purposes.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[vllm-project/vllm]]></title>
            <link>https://github.com/vllm-project/vllm</link>
            <guid>https://github.com/vllm-project/vllm</guid>
            <pubDate>Thu, 01 May 2025 00:04:40 GMT</pubDate>
            <description><![CDATA[A high-throughput and memory-efficient inference and serving engine for LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/vllm-project/vllm">vllm-project/vllm</a></h1>
            <p>A high-throughput and memory-efficient inference and serving engine for LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 46,287</p>
            <p>Forks: 7,180</p>
            <p>Stars today: 126 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-dark.png&quot;&gt;
    &lt;img alt=&quot;vLLM&quot; src=&quot;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-light.png&quot; width=55%&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;h3 align=&quot;center&quot;&gt;
Easy, fast, and cheap LLM serving for everyone
&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
| &lt;a href=&quot;https://docs.vllm.ai&quot;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://blog.vllm.ai/&quot;&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://arxiv.org/abs/2309.06180&quot;&gt;&lt;b&gt;Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://x.com/vllm_project&quot;&gt;&lt;b&gt;Twitter/X&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://discuss.vllm.ai&quot;&gt;&lt;b&gt;User Forum&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://slack.vllm.ai&quot;&gt;&lt;b&gt;Developer Slack&lt;/b&gt;&lt;/a&gt; |
&lt;/p&gt;

---

*Latest News* üî•
- [2025/04] We hosted [Asia Developer Day](https://www.sginnovate.com/event/limited-availability-morning-evening-slots-remaining-inaugural-vllm-asia-developer-day)! Please find the meetup slides from the vLLM team [here](https://docs.google.com/presentation/d/19cp6Qu8u48ihB91A064XfaXruNYiBOUKrBxAmDOllOo/edit?usp=sharing).
- [2025/03] We hosted [vLLM x Ollama Inference Night](https://lu.ma/vllm-ollama)! Please find the meetup slides from the vLLM team [here](https://docs.google.com/presentation/d/16T2PDD1YwRnZ4Tu8Q5r6n53c5Lr5c73UV9Vd2_eBo4U/edit?usp=sharing).
- [2025/03] We hosted [the first vLLM China Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg)! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1REHvfQMKGnvz6p3Fd23HhSO4c8j5WPGZV0bKYLwnHyQ/edit?usp=sharing).
- [2025/03] We hosted [the East Coast vLLM Meetup](https://lu.ma/7mu4k4xx)! Please find the meetup slides [here](https://docs.google.com/presentation/d/1NHiv8EUFF1NLd3fEYODm56nDmL26lEeXCaDgyDlTsRs/edit#slide=id.g31441846c39_0_0).
- [2025/02] We hosted [the ninth vLLM meetup](https://lu.ma/h7g3kuj9) with Meta! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1jzC_PZVXrVNSFVCW-V4cFXb6pn7zZ2CyP_Flwo05aqg/edit?usp=sharing) and AMD [here](https://drive.google.com/file/d/1Zk5qEJIkTmlQ2eQcXQZlljAx3m9s7nwn/view?usp=sharing). The slides from Meta will not be posted.
- [2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post [here](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html).
- [2025/01] We hosted [the eighth vLLM meetup](https://lu.ma/zep56hui) with Google Cloud! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing), and Google Cloud team [here](https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing).
- [2024/12] vLLM joins [pytorch ecosystem](https://pytorch.org/blog/vllm-joins-pytorch)! Easy, Fast, and Cheap LLM Serving for Everyone!

&lt;details&gt;
&lt;summary&gt;Previous News&lt;/summary&gt;

- [2024/11] We hosted [the seventh vLLM meetup](https://lu.ma/h0qvrajz) with Snowflake! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing), and Snowflake team [here](https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing).
- [2024/10] We have just created a developer slack ([slack.vllm.ai](https://slack.vllm.ai)) focusing on coordinating contributions and discussing features. Please feel free to join us there!
- [2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team [here](https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing). Learn more from the [talks](https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR) from other vLLM contributors and users!
- [2024/09] We hosted [the sixth vLLM meetup](https://lu.ma/87q3nvnh) with NVIDIA! Please find the meetup slides [here](https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing).
- [2024/07] We hosted [the fifth vLLM meetup](https://lu.ma/lp0gyjqr) with AWS! Please find the meetup slides [here](https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing).
- [2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post [here](https://blog.vllm.ai/2024/07/23/llama31.html).
- [2024/06] We hosted [the fourth vLLM meetup](https://lu.ma/agivllm) with Cloudflare and BentoML! Please find the meetup slides [here](https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing).
- [2024/04] We hosted [the third vLLM meetup](https://robloxandvllmmeetup2024.splashthat.com/) with Roblox! Please find the meetup slides [here](https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing).
- [2024/01] We hosted [the second vLLM meetup](https://lu.ma/ygxbpzhl) with IBM! Please find the meetup slides [here](https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing).
- [2023/10] We hosted [the first vLLM meetup](https://lu.ma/first-vllm-meetup) with a16z! Please find the meetup slides [here](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing).
- [2023/08] We would like to express our sincere gratitude to [Andreessen Horowitz](https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/) (a16z) for providing a generous grant to support the open-source development and research of vLLM.
- [2023/06] We officially released vLLM! FastChat-vLLM integration has powered [LMSYS Vicuna and Chatbot Arena](https://chat.lmsys.org) since mid-April. Check out our [blog post](https://vllm.ai).

&lt;/details&gt;

---
## About

vLLM is a fast and easy-to-use library for LLM inference and serving.

Originally developed in the [Sky Computing Lab](https://sky.cs.berkeley.edu) at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.

vLLM is fast with:

- State-of-the-art serving throughput
- Efficient management of attention key and value memory with [**PagedAttention**](https://blog.vllm.ai/2023/06/20/vllm.html)
- Continuous batching of incoming requests
- Fast model execution with CUDA/HIP graph
- Quantizations: [GPTQ](https://arxiv.org/abs/2210.17323), [AWQ](https://arxiv.org/abs/2306.00978), INT4, INT8, and FP8.
- Optimized CUDA kernels, including integration with FlashAttention and FlashInfer.
- Speculative decoding
- Chunked prefill

**Performance benchmark**: We include a performance benchmark at the end of [our blog post](https://blog.vllm.ai/2024/09/05/perf-update.html). It compares the performance of vLLM against other LLM serving engines ([TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM), [SGLang](https://github.com/sgl-project/sglang) and [LMDeploy](https://github.com/InternLM/lmdeploy)). The implementation is under [nightly-benchmarks folder](.buildkite/nightly-benchmarks/) and you can [reproduce](https://github.com/vllm-project/vllm/issues/8176) this benchmark using our one-click runnable script.

vLLM is flexible and easy to use with:

- Seamless integration with popular Hugging Face models
- High-throughput serving with various decoding algorithms, including *parallel sampling*, *beam search*, and more
- Tensor parallelism and pipeline parallelism support for distributed inference
- Streaming outputs
- OpenAI-compatible API server
- Support NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, TPU, and AWS Neuron.
- Prefix caching support
- Multi-lora support

vLLM seamlessly supports most popular open-source models on HuggingFace, including:
- Transformer-like LLMs (e.g., Llama)
- Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)
- Embedding Models (e.g. E5-Mistral)
- Multi-modal LLMs (e.g., LLaVA)

Find the full list of supported models [here](https://docs.vllm.ai/en/latest/models/supported_models.html).

## Getting Started

Install vLLM with `pip` or [from source](https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source):

```bash
pip install vllm
```

Visit our [documentation](https://docs.vllm.ai/en/latest/) to learn more.
- [Installation](https://docs.vllm.ai/en/latest/getting_started/installation.html)
- [Quickstart](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)
- [List of Supported Models](https://docs.vllm.ai/en/latest/models/supported_models.html)

## Contributing

We welcome and value any contributions and collaborations.
Please check out [Contributing to vLLM](https://docs.vllm.ai/en/stable/contributing/overview.html) for how to get involved.

## Sponsors

vLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!

&lt;!-- Note: Please sort them in alphabetical order. --&gt;
&lt;!-- Note: Please keep these consistent with docs/source/community/sponsors.md --&gt;
Cash Donations:
- a16z
- Dropbox
- Sequoia Capital
- Skywork AI
- ZhenFund

Compute Resources:
- AMD
- Anyscale
- AWS
- Crusoe Cloud
- Databricks
- DeepInfra
- Google Cloud
- Intel
- Lambda Lab
- Nebius
- Novita AI
- NVIDIA
- Replicate
- Roblox
- RunPod
- Trainy
- UC Berkeley
- UC San Diego

Slack Sponsor: Anyscale

We also have an official fundraising venue through [OpenCollective](https://opencollective.com/vllm). We plan to use the fund to support the development, maintenance, and adoption of vLLM.

## Citation

If you use vLLM for your research, please cite our [paper](https://arxiv.org/abs/2309.06180):

```bibtex
@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}
```

## Contact Us

- For technical questions and feature requests, please use GitHub [Issues](https://github.com/vllm-project/vllm/issues) or [Discussions](https://github.com/vllm-project/vllm/discussions)
- For discussing with fellow users, please use the [vLLM Forum](https://discuss.vllm.ai)
- coordinating contributions and development, please use [Slack](https://slack.vllm.ai)
- For security disclosures, please use GitHub&#039;s [Security Advisories](https://github.com/vllm-project/vllm/security/advisories) feature
- For collaborations and partnerships, please contact us at [vllm-questions@lists.berkeley.edu](mailto:vllm-questions@lists.berkeley.edu)

## Media Kit

- If you wish to use vLLM&#039;s logo, please refer to [our media kit repo](https://github.com/vllm-project/media-kit).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[simular-ai/Agent-S]]></title>
            <link>https://github.com/simular-ai/Agent-S</link>
            <guid>https://github.com/simular-ai/Agent-S</guid>
            <pubDate>Thu, 01 May 2025 00:04:39 GMT</pubDate>
            <description><![CDATA[Agent S: an open agentic framework that uses computers like a human]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/simular-ai/Agent-S">simular-ai/Agent-S</a></h1>
            <p>Agent S: an open agentic framework that uses computers like a human</p>
            <p>Language: Python</p>
            <p>Stars: 3,525</p>
            <p>Forks: 358</p>
            <p>Stars today: 317 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
  &lt;img src=&quot;images/agent_s.png&quot; alt=&quot;Logo&quot; style=&quot;vertical-align:middle&quot; width=&quot;60&quot;&gt; Agent S2:
  &lt;small&gt;A Compositional Generalist-Specialist Framework for Computer Use Agents&lt;/small&gt;
&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;&amp;nbsp;
  üåê &lt;a href=&quot;https://www.simular.ai/articles/agent-s2-technical-review&quot;&gt;[S2 blog]&lt;/a&gt;&amp;nbsp;
  üìÑ &lt;a href=&quot;https://arxiv.org/abs/2504.00906&quot;&gt;[S2 Paper]&lt;/a&gt;&amp;nbsp;
  üé• &lt;a href=&quot;https://www.youtube.com/watch?v=wUGVQl7c0eg&quot;&gt;[S2 Video]&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&amp;nbsp;
  üåê &lt;a href=&quot;https://www.simular.ai/agent-s&quot;&gt;[S1 blog]&lt;/a&gt;&amp;nbsp;
  üìÑ &lt;a href=&quot;https://arxiv.org/abs/2410.08164&quot;&gt;[S1 Paper (ICLR 2025)]&lt;/a&gt;&amp;nbsp;
  üé• &lt;a href=&quot;https://www.youtube.com/watch?v=OBDE3Knte0g&quot;&gt;[S1 Video]&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&amp;nbsp;
&lt;a href=&quot;https://trendshift.io/repositories/13151&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13151&quot; alt=&quot;simular-ai%2FAgent-S | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://discord.gg/E2XfsK9fPV&quot;&gt;
    &lt;img src=&quot;https://dcbadge.limes.pink/api/server/https://discord.gg/E2XfsK9fPV?style=flat&quot; alt=&quot;Discord&quot;&gt;
  &lt;/a&gt;
  &amp;nbsp;&amp;nbsp;
  &lt;a href=&quot;https://pepy.tech/projects/gui-agents&quot;&gt;
    &lt;img src=&quot;https://static.pepy.tech/badge/gui-agents&quot; alt=&quot;PyPI Downloads&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

## ü•≥ Updates
- [x] **2025/04/01**: Released &lt;a href=&quot;https://arxiv.org/abs/2504.00906&quot;&gt;Agent S2 paper&lt;/a&gt; with new SOTA results on OSWorld, WindowsAgentArena, and AndroidWorld!
- [x] **2025/03/12**: Released Agent S2 along with v0.2.0 of [gui-agents](https://github.com/simular-ai/Agent-S), the new state-of-the-art for computer use agents (CUA), outperforming OpenAI&#039;s CUA/Operator and Anthropic&#039;s Claude 3.7 Sonnet Computer-Use!
- [x] **2025/01/22**: The [Agent S paper](https://arxiv.org/abs/2410.08164) is accepted to ICLR 2025!
- [x] **2025/01/21**: Released v0.1.2 of [gui-agents](https://github.com/simular-ai/Agent-S) library, with support for Linux and Windows!
- [x] **2024/12/05**: Released v0.1.0 of [gui-agents](https://github.com/simular-ai/Agent-S) library, allowing you to use Agent-S for Mac, OSWorld, and WindowsAgentArena with ease!
- [x] **2024/10/10**: Released the [Agent S paper](https://arxiv.org/abs/2410.08164) and codebase!

## Table of Contents

1. [üí° Introduction](#-introduction)
2. [üéØ Current Results](#-current-results)
3. [üõ†Ô∏è Installation &amp; Setup](#%EF%B8%8F-installation--setup) 
4. [üöÄ Usage](#-usage)
5. [ü§ù Acknowledgements](#-acknowledgements)
6. [üí¨ Citation](#-citation)

## üí° Introduction

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;./images/agent_s2_teaser.png&quot; width=&quot;800&quot;&gt;
&lt;/p&gt;

Welcome to **Agent S**, an open-source framework designed to enable autonomous interaction with computers through Agent-Computer Interface. Our mission is to build intelligent GUI agents that can learn from past experiences and perform complex tasks autonomously on your computer. 

Whether you&#039;re interested in AI, automation, or contributing to cutting-edge agent-based systems, we&#039;re excited to have you here!

## üéØ Current Results

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;./images/agent_s2_osworld_result.png&quot; width=&quot;600&quot;&gt;
    &lt;br&gt;
    Results of Agent S2&#039;s Successful Rate (%) on the OSWorld full test set using Screenshot input only.
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;table border=&quot;0&quot; cellspacing=&quot;0&quot; cellpadding=&quot;5&quot;&gt;
    &lt;tr&gt;
      &lt;th&gt;Benchmark&lt;/th&gt;
      &lt;th&gt;Agent S2&lt;/th&gt;
      &lt;th&gt;Previous SOTA&lt;/th&gt;
      &lt;th&gt;Œî improve&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;OSWorld (15 step)&lt;/td&gt;
      &lt;td&gt;27.0%&lt;/td&gt;
      &lt;td&gt;22.7% (UI-TARS)&lt;/td&gt;
      &lt;td&gt;+4.3%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;OSWorld (50 step)&lt;/td&gt;
      &lt;td&gt;34.5%&lt;/td&gt;
      &lt;td&gt;32.6% (OpenAI CUA)&lt;/td&gt;
      &lt;td&gt;+1.9%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;WindowsAgentArena&lt;/td&gt;
      &lt;td&gt;29.8%&lt;/td&gt;
      &lt;td&gt;19.5% (NAVI)&lt;/td&gt;
      &lt;td&gt;+10.3%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;AndroidWorld&lt;/td&gt;
      &lt;td&gt;54.3%&lt;/td&gt;
      &lt;td&gt;46.8% (UI-TARS)&lt;/td&gt;
      &lt;td&gt;+7.5%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;


## üõ†Ô∏è Installation &amp; Setup

&gt; ‚ùó**Warning**‚ùó: If you are on a Linux machine, creating a `conda` environment will interfere with `pyatspi`. As of now, there&#039;s no clean solution for this issue. Proceed through the installation without using `conda` or any virtual environment.

&gt; ‚ö†Ô∏è**Disclaimer**‚ö†Ô∏è: To leverage the full potential of Agent S2, we utilize [UI-TARS](https://github.com/bytedance/UI-TARS) as a grounding model (7B-DPO or 72B-DPO for better performance). They can be hosted locally, or on Hugging Face Inference Endpoints. Our code supports Hugging Face Inference Endpoints. Check out [Hugging Face Inference Endpoints](https://huggingface.co/learn/cookbook/en/enterprise_dedicated_endpoints) for more information on how to set up and query this endpoint. However, running Agent S2 does not require this model, and you can use alternative API based models for visual grounding, such as Claude.

Install the package:
```
pip install gui-agents
```

Set your LLM API Keys and other environment variables. You can do this by adding the following line to your .bashrc (Linux), or .zshrc (MacOS) file. 

```
export OPENAI_API_KEY=&lt;YOUR_API_KEY&gt;
export ANTHROPIC_API_KEY=&lt;YOUR_ANTHROPIC_API_KEY&gt;
export HF_TOKEN=&lt;YOUR_HF_TOKEN&gt;
```

Alternatively, you can set the environment variable in your Python script:

```
import os
os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;&lt;YOUR_API_KEY&gt;&quot;
```

We also support Azure OpenAI, Anthropic, Gemini, Open Router, and vLLM inference. For more information refer to [models.md](models.md).

### Setup Retrieval from Web using Perplexica
Agent S works best with web-knowledge retrieval. To enable this feature, you need to setup Perplexica: 

1. Ensure Docker Desktop is installed and running on your system.

2. Navigate to the directory containing the project files.

   ```bash
    cd Perplexica
    git submodule update --init
   ```

3. Rename the `sample.config.toml` file to `config.toml`. For Docker setups, you need only fill in the following fields:

   - `OPENAI`: Your OpenAI API key. **You only need to fill this if you wish to use OpenAI&#039;s models**.
   - `OLLAMA`: Your Ollama API URL. You should enter it as `http://host.docker.internal:PORT_NUMBER`. If you installed Ollama on port 11434, use `http://host.docker.internal:11434`. For other ports, adjust accordingly. **You need to fill this if you wish to use Ollama&#039;s models instead of OpenAI&#039;s**.
   - `GROQ`: Your Groq API key. **You only need to fill this if you wish to use Groq&#039;s hosted models**.
   - `ANTHROPIC`: Your Anthropic API key. **You only need to fill this if you wish to use Anthropic models**.

     **Note**: You can change these after starting Perplexica from the settings dialog.

   - `SIMILARITY_MEASURE`: The similarity measure to use (This is filled by default; you can leave it as is if you are unsure about it.)

4. Ensure you are in the directory containing the `docker-compose.yaml` file and execute:

   ```bash
   docker compose up -d
   ```
5. Next, export your Perplexica URL. This URL is used to interact with the Perplexica API backend. The port is given by the `config.toml` in your Perplexica directory.

   ```bash
   export PERPLEXICA_URL=http://localhost:{port}/api/search
   ```
6. Our implementation of Agent S incorporates the Perplexica API to integrate a search engine capability, which allows for a more convenient and responsive user experience. If you want to tailor the API to your settings and specific requirements, you may modify the URL and the message of request parameters in  `agent_s/query_perplexica.py`. For a comprehensive guide on configuring the Perplexica API, please refer to [Perplexica Search API Documentation](https://github.com/ItzCrazyKns/Perplexica/blob/master/docs/API/SEARCH.md).
For a more detailed setup and usage guide, please refer to the [Perplexica Repository](https://github.com/ItzCrazyKns/Perplexica.git).

&gt; ‚ùó**Warning**‚ùó: The agent will directly run python code to control your computer. Please use with care.

## üöÄ Usage


&gt; **Note**: Our best configuration uses Claude 3.7 with extended thinking and UI-TARS-72B-DPO. If you are unable to run UI-TARS-72B-DPO due to resource constraints, UI-TARS-7B-DPO can be used as a lighter alternative with minimal performance degradation.

### CLI

Run Agent S2 with a specific model (default is `gpt-4o`):

```sh
agent_s2 \
  --provider &quot;anthropic&quot; \
  --model &quot;claude-3-7-sonnet-20250219&quot; \
  --grounding_model_provider &quot;anthropic&quot; \
  --grounding_model &quot;claude-3-7-sonnet-20250219&quot; \
```

Or use a custom endpoint:

```bash
agent_s2 \
  --provider &quot;anthropic&quot; \
  --model &quot;claude-3-7-sonnet-20250219&quot; \
  --endpoint_provider &quot;huggingface&quot; \
  --endpoint_url &quot;&lt;endpoint_url&gt;/v1/&quot;
```

#### Main Model Settings
- **`--provider`**, **`--model`** 
  - Purpose: Specifies the main generation model
  - Supports: all model providers in [models.md](models.md)
  - Default: `--provider &quot;anthropic&quot; --model &quot;claude-3-7-sonnet-20250219&quot;`

#### Grounding Configuration Options

You can use either Configuration 1 or Configuration 2:

##### **(Default) Configuration 1: API-Based Models**
- **`--grounding_model_provider`**, **`--grounding_model`**
  - Purpose: Specifies the model for visual grounding (coordinate prediction)
  - Supports: all model providers in [models.md](models.md)
  - Default: `--grounding_model_provider &quot;anthropic&quot; --grounding_model &quot;claude-3-7-sonnet-20250219&quot;`
- ‚ùó**Important**‚ùó **`--grounding_model_resize_width`**
  - Purpose:  Some API providers automatically rescale images. Therefore, the generated (x, y) will be relative to the rescaled image dimensions, instead of the original image dimensions.
  - Supports: [Anthropic rescaling](https://docs.anthropic.com/en/docs/build-with-claude/vision#)
  - Tips: If your grounding is inaccurate even for very simple queries, double check your rescaling width is correct for your machine&#039;s resolution.
  - Default: `--grounding_model_resize_width 1366` (Anthropic)

##### **Configuration 2: Custom Endpoint**
- **`--endpoint_provider`**
  - Purpose: Specifies the endpoint provider
  - Supports: HuggingFace TGI, vLLM, Open Router
  - Default: None

- **`--endpoint_url`**
  - Purpose: The URL for your custom endpoint
  - Default: None

&gt; **Note**: Configuration 2 takes precedence over Configuration 1.

This will show a user query prompt where you can enter your query and interact with Agent S2. You can use any model from the list of supported models in [models.md](models.md).

### `gui_agents` SDK

First, we import the necessary modules. `AgentS2` is the main agent class for Agent S2. `OSWorldACI` is our grounding agent that translates agent actions into executable python code.
```
import pyautogui
import io
from gui_agents.s2.agents.agent_s import AgentS2
from gui_agents.s2.agents.grounding import OSWorldACI

# Load in your API keys.
from dotenv import load_dotenv
load_dotenv()

current_platform = &quot;linux&quot;  # &quot;darwin&quot;, &quot;windows&quot;
```

Next, we define our engine parameters. `engine_params` is used for the main agent, and `engine_params_for_grounding` is for grounding. For `engine_params_for_grounding`, we support the Claude, GPT series, and Hugging Face Inference Endpoints.

```
engine_type_for_grounding = &quot;huggingface&quot;

engine_params = {
    &quot;engine_type&quot;: &quot;openai&quot;,
    &quot;model&quot;: &quot;gpt-4o&quot;,
}

if engine_type_for_grounding == &quot;huggingface&quot;:
  engine_params_for_grounding = {
      &quot;engine_type&quot;: &quot;huggingface&quot;,
      &quot;endpoint_url&quot;: &quot;&lt;endpoint_url&gt;/v1/&quot;,
  }
elif engine_type_for_grounding == &quot;claude&quot;:
  engine_params_for_grounding = {
      &quot;engine_type&quot;: &quot;claude&quot;,
      &quot;model&quot;: &quot;claude-3-7-sonnet-20250219&quot;,
  }
elif engine_type_for_grounding == &quot;gpt&quot;:
  engine_params_for_grounding = {
    &quot;engine_type&quot;: &quot;gpt&quot;,
    &quot;model&quot;: &quot;gpt-4o&quot;,
  }
else:
  raise ValueError(&quot;Invalid engine type for grounding&quot;)
```

Then, we define our grounding agent and Agent S2.

```
grounding_agent = OSWorldACI(
    platform=current_platform,
    engine_params_for_generation=engine_params,
    engine_params_for_grounding=engine_params_for_grounding
)

agent = AgentS2(
  engine_params,
  grounding_agent,
  platform=current_platform,
  action_space=&quot;pyautogui&quot;,
  observation_type=&quot;mixed&quot;,
  search_engine=&quot;Perplexica&quot;  # Assuming you have set up Perplexica.
)
```

Finally, let&#039;s query the agent!

```
# Get screenshot.
screenshot = pyautogui.screenshot()
buffered = io.BytesIO() 
screenshot.save(buffered, format=&quot;PNG&quot;)
screenshot_bytes = buffered.getvalue()

obs = {
  &quot;screenshot&quot;: screenshot_bytes,
}

instruction = &quot;Close VS Code&quot;
info, action = agent.predict(instruction=instruction, observation=obs)

exec(action[0])
```

Refer to `gui_agents/s2/cli_app.py` for more details on how the inference loop works.

#### Downloading the Knowledge Base

Agent S2 uses a knowledge base that continually updates with new knowledge during inference. The knowledge base is initially downloaded when initializing `AgentS2`. The knowledge base is stored as assets under our [GitHub Releases](https://github.com/simular-ai/Agent-S/releases). The `AgentS2` initialization will only download the knowledge base for your specified platform and agent version (e.g s1, s2). If you&#039;d like to download the knowledge base programmatically, you can use the following code:

```
download_kb_data(
    version=&quot;s2&quot;,
    release_tag=&quot;v0.2.2&quot;,
    download_dir=&quot;kb_data&quot;,
    platform=&quot;linux&quot;  # &quot;darwin&quot;, &quot;windows&quot;
)
```

This will download Agent S2&#039;s knowledge base for Linux from release tag `v0.2.2` to the `kb_data` directory. Refer to our [GitHub Releases](https://github.com/simular-ai/Agent-S/releases) or release tags that include the knowledge bases.

### OSWorld

To deploy Agent S2 in OSWorld, follow the [OSWorld Deployment instructions](OSWorld.md).

## ü§ù Acknowledgements

We extend our sincere thanks to Tianbao Xie for developing OSWorld and discussing computer use challenges. We also appreciate the engaging discussions with Yujia Qin and Shihao Liang regarding UI-TARS.

## üí¨ Citations

If you find this codebase useful, please cite 

```
@misc{Agent-S2,
      title={Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents}, 
      author={Saaket Agashe and Kyle Wong and Vincent Tu and Jiachen Yang and Ang Li and Xin Eric Wang},
      year={2025},
      eprint={2504.00906},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2504.00906}, 
}
```

```
@inproceedings{Agent-S,
    title={{Agent S: An Open Agentic Framework that Uses Computers Like a Human}},
    author={Saaket Agashe and Jiuzhou Han and Shuyu Gan and Jiachen Yang and Ang Li and Xin Eric Wang},
    booktitle={International Conference on Learning Representations (ICLR)},
    year={2025},
    url={https://arxiv.org/abs/2410.08164}
}
```

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hiyouga/LLaMA-Factory]]></title>
            <link>https://github.com/hiyouga/LLaMA-Factory</link>
            <guid>https://github.com/hiyouga/LLaMA-Factory</guid>
            <pubDate>Thu, 01 May 2025 00:04:38 GMT</pubDate>
            <description><![CDATA[Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hiyouga/LLaMA-Factory">hiyouga/LLaMA-Factory</a></h1>
            <p>Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)</p>
            <p>Language: Python</p>
            <p>Stars: 48,041</p>
            <p>Forks: 5,865</p>
            <p>Stars today: 84 stars today</p>
            <h2>README</h2><pre>![# LLaMA Factory](assets/logo.png)

[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social)](https://github.com/hiyouga/LLaMA-Factory/stargazers)
[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory)](https://github.com/hiyouga/LLaMA-Factory/commits/main)
[![GitHub contributors](https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange)](https://github.com/hiyouga/LLaMA-Factory/graphs/contributors)
[![GitHub workflow](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg)](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml)
[![PyPI](https://img.shields.io/pypi/v/llamafactory)](https://pypi.org/project/llamafactory/)
[![Citation](https://img.shields.io/badge/citation-447-green)](https://scholar.google.com/scholar?cites=12620864006390196564)
[![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)](https://github.com/hiyouga/LLaMA-Factory/pulls)

[![Twitter](https://img.shields.io/twitter/follow/llamafactory_ai)](https://twitter.com/llamafactory_ai)
[![Discord](https://dcbadge.vercel.app/api/server/rKfvV9r9FK?compact=true&amp;style=flat)](https://discord.gg/rKfvV9r9FK)
[![GitCode](https://gitcode.com/zhengyaowei/LLaMA-Factory/star/badge.svg)](https://gitcode.com/zhengyaowei/LLaMA-Factory)

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)
[![Open in DSW](https://gallery.pai-ml.com/assets/open-in-dsw.svg)](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory)
[![Spaces](https://img.shields.io/badge/ü§ó-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/hiyouga/LLaMA-Board)
[![Studios](https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue)](https://modelscope.cn/studios/hiyouga/LLaMA-Board)
[![SageMaker](https://img.shields.io/badge/SageMaker-Open%20in%20AWS-blue)](https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/)

&lt;h3 align=&quot;center&quot;&gt;
    Easily fine-tune 100+ large language models with zero-code &lt;a href=&quot;#quickstart&quot;&gt;CLI&lt;/a&gt; and &lt;a href=&quot;#fine-tuning-with-llama-board-gui-powered-by-gradio&quot;&gt;Web UI&lt;/a&gt;
&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;picture&gt;
        &lt;img alt=&quot;Github trend&quot; src=&quot;https://trendshift.io/api/badge/repositories/4535&quot;&gt;
    &lt;/picture&gt;
&lt;/p&gt;

üëã Join our [WeChat](assets/wechat.jpg) or [NPU user group](assets/wechat_npu.jpg).

\[ English | [‰∏≠Êñá](README_zh.md) \]

**Fine-tuning a large language model can be easy as...**

https://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e

Choose your path:

- **Documentation**: https://llamafactory.readthedocs.io/en/latest/
- **Colab (free)**: https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing
- **Local machine**: Please refer to [usage](#getting-started)
- **PAI-DSW (free trial)**: [Llama3 Example](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory) | [Qwen2-VL Example](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl) | [DeepSeek-R1-Distill Example](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b)
- **Amazon SageMaker**: [Blog](https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/)
- **Easy Dataset**: [Fine-tune on Synthetic Data](https://buaa-act.feishu.cn/wiki/GVzlwYcRFiR8OLkHbL6cQpYin7g)

&gt; [!NOTE]
&gt; Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.

## Table of Contents

- [Features](#features)
- [Benchmark](#benchmark)
- [Changelog](#changelog)
- [Supported Models](#supported-models)
- [Supported Training Approaches](#supported-training-approaches)
- [Provided Datasets](#provided-datasets)
- [Requirement](#requirement)
- [Getting Started](#getting-started)
  - [Installation](#installation)
  - [Data Preparation](#data-preparation)
  - [Quickstart](#quickstart)
  - [Fine-Tuning with LLaMA Board GUI](#fine-tuning-with-llama-board-gui-powered-by-gradio)
  - [Build Docker](#build-docker)
  - [Deploy with OpenAI-style API and vLLM](#deploy-with-openai-style-api-and-vllm)
  - [Download from ModelScope Hub](#download-from-modelscope-hub)
  - [Download from Modelers Hub](#download-from-modelers-hub)
  - [Use W&amp;B Logger](#use-wb-logger)
  - [Use SwanLab Logger](#use-swanlab-logger)
- [Projects using LLaMA Factory](#projects-using-llama-factory)
- [License](#license)
- [Citation](#citation)
- [Acknowledgement](#acknowledgement)

## Features

- **Various models**: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.
- **Integrated methods**: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.
- **Scalable resources**: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.
- **Advanced algorithms**: [GaLore](https://github.com/jiaweizzhao/GaLore), [BAdam](https://github.com/Ledzy/BAdam), [APOLLO](https://github.com/zhuhanqing/APOLLO), [Adam-mini](https://github.com/zyushun/Adam-mini), [Muon](https://github.com/KellerJordan/Muon), DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.
- **Practical tricks**: [FlashAttention-2](https://github.com/Dao-AILab/flash-attention), [Unsloth](https://github.com/unslothai/unsloth), [Liger Kernel](https://github.com/linkedin/Liger-Kernel), RoPE scaling, NEFTune and rsLoRA.
- **Wide tasks**: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.
- **Experiment monitors**: LlamaBoard, TensorBoard, Wandb, MLflow, [SwanLab](https://github.com/SwanHubX/SwanLab), etc.
- **Faster inference**: OpenAI-style API, Gradio UI and CLI with [vLLM worker](https://github.com/vllm-project/vllm) or [SGLang worker](https://github.com/sgl-project/sglang).

### Day-N Support for Fine-Tuning Cutting-Edge Models

| Support Date | Model Name                                                   |
| ------------ | ------------------------------------------------------------ |
| Day 0        | Qwen3 / Qwen2.5-VL / Gemma 3 / InternLM 3 / MiniCPM-o-2.6    |
| Day 1        | Llama 3 / GLM-4 / Mistral Small / PaliGemma2 / Llama 4       |

## Benchmark

Compared to ChatGLM&#039;s [P-Tuning](https://github.com/THUDM/ChatGLM2-6B/tree/main/ptuning), LLaMA Factory&#039;s LoRA tuning offers up to **3.7 times faster** training speed with a better Rouge score on the advertising text generation task. By leveraging 4-bit quantization technique, LLaMA Factory&#039;s QLoRA further improves the efficiency regarding the GPU memory.

![benchmark](assets/benchmark.svg)

&lt;details&gt;&lt;summary&gt;Definitions&lt;/summary&gt;

- **Training Speed**: the number of training samples processed per second during the training. (bs=4, cutoff_len=1024)
- **Rouge Score**: Rouge-2 score on the development set of the [advertising text generation](https://aclanthology.org/D19-1321.pdf) task. (bs=4, cutoff_len=1024)
- **GPU Memory**: Peak GPU memory usage in 4-bit quantized training. (bs=1, cutoff_len=1024)
- We adopt `pre_seq_len=128` for ChatGLM&#039;s P-Tuning and `lora_rank=32` for LLaMA Factory&#039;s LoRA tuning.

&lt;/details&gt;

## Changelog

[25/04/28] We supported fine-tuning the **[Qwen3](https://qwenlm.github.io/blog/qwen3/)** model family.

[25/04/21] We supported the **[Muon](https://github.com/KellerJordan/Muon)** optimizer. See [examples](examples/README.md) for usage. Thank [@tianshijing](https://github.com/tianshijing)&#039;s PR.

[25/04/16] We supported fine-tuning the **[InternVL3](https://huggingface.co/OpenGVLab/InternVL3-8B)** model. See [PR #7258](https://github.com/hiyouga/LLaMA-Factory/pull/7258) to get started.

[25/04/14] We supported fine-tuning the **[GLM-Z1](https://huggingface.co/THUDM/GLM-Z1-9B-0414)** and **[Kimi-VL](https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct)** models.

[25/04/06] We supported fine-tuning the **[Llama 4](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)** model. See [PR #7611](https://github.com/hiyouga/LLaMA-Factory/pull/7611) to get started.

&lt;details&gt;&lt;summary&gt;Full Changelog&lt;/summary&gt;

[25/03/31] We supported fine-tuning the **[Qwen2.5 Omni](https://qwenlm.github.io/blog/qwen2.5-omni/)** model. See [PR #7537](https://github.com/hiyouga/LLaMA-Factory/pull/7537) to get started.

[25/03/15] We supported **[SGLang](https://github.com/sgl-project/sglang)** as inference backend. Try `infer_backend: sglang` to accelerate inference.

[25/03/12] We supported fine-tuning the **[Gemma 3](https://huggingface.co/blog/gemma3)** model.

[25/02/24] Announcing **[EasyR1](https://github.com/hiyouga/EasyR1)**, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.

[25/02/11] We supported saving the **[Ollama](https://github.com/ollama/ollama)** modelfile when exporting the model checkpoints. See [examples](examples/README.md) for usage.

[25/02/05] We supported fine-tuning the **[Qwen2-Audio](Qwen/Qwen2-Audio-7B-Instruct)** and **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** on audio understanding tasks.

[25/01/31] We supported fine-tuning the **[DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)** and **[Qwen2.5-VL](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)** models.

[25/01/15] We supported **[APOLLO](https://arxiv.org/abs/2412.05270)** optimizer. See [examples](examples/README.md) for usage.

[25/01/14] We supported fine-tuning the **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** and **[MiniCPM-V-2.6](https://huggingface.co/openbmb/MiniCPM-V-2_6)** models. Thank [@BUAADreamer](https://github.com/BUAADreamer)&#039;s PR.

[25/01/14] We supported fine-tuning the **[InternLM 3](https://huggingface.co/collections/internlm/)** models. Thank [@hhaAndroid](https://github.com/hhaAndroid)&#039;s PR.

[25/01/10] We supported fine-tuning the **[Phi-4](https://huggingface.co/microsoft/phi-4)** model.

[24/12/21] We supported using **[SwanLab](https://github.com/SwanHubX/SwanLab)** for experiment tracking and visualization. See [this section](#use-swanlab-logger) for details.

[24/11/27] We supported fine-tuning the **[Skywork-o1](https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B)** model and the **[OpenO1](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)** dataset.

[24/10/09] We supported downloading pre-trained models and datasets from the **[Modelers Hub](https://modelers.cn/models)**. See [this tutorial](#download-from-modelers-hub) for usage.

[24/09/19] We supported fine-tuning the **[Qwen2.5](https://qwenlm.github.io/blog/qwen2.5/)** models.

[24/08/30] We supported fine-tuning the **[Qwen2-VL](https://qwenlm.github.io/blog/qwen2-vl/)** models. Thank [@simonJJJ](https://github.com/simonJJJ)&#039;s PR.

[24/08/27] We supported **[Liger Kernel](https://github.com/linkedin/Liger-Kernel)**. Try `enable_liger_kernel: true` for efficient training.

[24/08/09] We supported **[Adam-mini](https://github.com/zyushun/Adam-mini)** optimizer. See [examples](examples/README.md) for usage. Thank [@relic-yuexi](https://github.com/relic-yuexi)&#039;s PR.

[24/07/04] We supported [contamination-free packed training](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing). Use `neat_packing: true` to activate it. Thank [@chuan298](https://github.com/chuan298)&#039;s PR.

[24/06/16] We supported **[PiSSA](https://arxiv.org/abs/2404.02948)** algorithm. See [examples](examples/README.md) for usage.

[24/06/07] We supported fine-tuning the **[Qwen2](https://qwenlm.github.io/blog/qwen2/)** and **[GLM-4](https://github.com/THUDM/GLM-4)** models.

[24/05/26] We supported **[SimPO](https://arxiv.org/abs/2405.14734)** algorithm for preference learning. See [examples](examples/README.md) for usage.

[24/05/20] We supported fine-tuning the **PaliGemma** series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with `paligemma` template for chat completion.

[24/05/18] We supported **[KTO](https://arxiv.org/abs/2402.01306)** algorithm for preference learning. See [examples](examples/README.md) for usage.

[24/05/14] We supported training and inference on the Ascend NPU devices. Check [installation](#installation) section for details.

[24/04/26] We supported fine-tuning the **LLaVA-1.5** multimodal LLMs. See [examples](examples/README.md) for usage.

[24/04/22] We provided a **[Colab notebook](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)** for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check [Llama3-8B-Chinese-Chat](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat) and [Llama3-Chinese](https://huggingface.co/zhichen/Llama3-Chinese) for details.

[24/04/21] We supported **[Mixture-of-Depths](https://arxiv.org/abs/2404.02258)** according to [AstraMindAI&#039;s implementation](https://github.com/astramind-ai/Mixture-of-depths). See [examples](examples/README.md) for usage.

[24/04/16] We supported **[BAdam](https://arxiv.org/abs/2404.02827)** optimizer. See [examples](examples/README.md) for usage.

[24/04/16] We supported **[unsloth](https://github.com/unslothai/unsloth)**&#039;s long-sequence training (Llama-2-7B-56k within 24GB). It achieves **117%** speed and **50%** memory compared with FlashAttention-2, more benchmarks can be found in [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison).

[24/03/31] We supported **[ORPO](https://arxiv.org/abs/2403.07691)**. See [examples](examples/README.md) for usage.

[24/03/21] Our paper &quot;[LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models](https://arxiv.org/abs/2403.13372)&quot; is available at arXiv!

[24/03/20] We supported **FSDP+QLoRA** that fine-tunes a 70B model on 2x24GB GPUs. See [examples](examples/README.md) for usage.

[24/03/13] We supported **[LoRA+](https://arxiv.org/abs/2402.12354)**. See [examples](examples/README.md) for usage.

[24/03/07] We supported **[GaLore](https://arxiv.org/abs/2403.03507)** optimizer. See [examples](examples/README.md) for usage.

[24/03/07] We integrated **[vLLM](https://github.com/vllm-project/vllm)** for faster and concurrent inference. Try `infer_backend: vllm` to enjoy **270%** inference speed.

[24/02/28] We supported weight-decomposed LoRA (**[DoRA](https://arxiv.org/abs/2402.09353)**). Try `use_dora: true` to activate DoRA training.

[24/02/15] We supported **block expansion** proposed by [LLaMA Pro](https://github.com/TencentARC/LLaMA-Pro). See [examples](examples/README.md) for usage.

[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this [blog post](https://qwenlm.github.io/blog/qwen1.5/) for details.

[24/01/18] We supported **agent tuning** for most models, equipping model with tool using abilities by fine-tuning with `dataset: glaive_toolcall_en`.

[23/12/23] We supported **[unsloth](https://github.com/unslothai/unsloth)**&#039;s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try `use_unsloth: true` argument to activate unsloth patch. It achieves **170%** speed in our benchmark, check [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison) for details.

[23/12/12] We supported fine-tuning the latest MoE model **[Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)** in our framework. See hardware requirement [here](#hardware-requirement).

[23/12/01] We supported downloading pre-trained models and datasets from the **[ModelScope Hub](https://modelscope.cn/models)**. See [this tutorial](#download-from-modelscope-hub) for usage.

[23/10/21] We supported **[NEFTune](https://arxiv.org/abs/2310.05914)** trick for fine-tuning. Try `neftune_noise_alpha: 5` argument to activate NEFTune.

[23/09/27] We supported **$S^2$-Attn** proposed by [LongLoRA](https://github.com/dvlab-research/LongLoRA) for the LLaMA models. Try `shift_attn: true` argument to enable shift short attention.

[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See [examples](examples/README.md) for usage.

[23/09/10] We supported **[FlashAttention-2](https://github.com/Dao-AILab/flash-attention)**. Try `flash_attn: fa2` argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.

[23/08/12] We supported **RoPE scaling** to extend the context length of the LLaMA models. Try `rope_scaling: linear` argument in training and `rope_scaling: dynamic` argument at inference to extrapolate the position embeddings.

[23/08/11] We supported **[DPO training](https://arxiv.org/abs/2305.18290)** for instruction-tuned models. See [examples](examples/README.md) for usage.

[23/07/31] We supported **dataset streaming**. Try `streaming: true` and `max_steps: 10000` arguments to load your dataset in streaming mode.

[23/07/29] We released two instruction-tuned 13B models at Hugging Face. See these Hugging Face Repos ([LLaMA-2](https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat) / [Baichuan](https://huggingface.co/hiyouga/Baichuan-13B-sft)) for details.

[23/07/18] We developed an **all-in-one Web UI** for training, evaluation and inference. Try `train_web.py` to fine-tune models in your Web browser. Thank [@KanadeSiina](https://github.com/KanadeSiina) and [@codemayq](https://github.com/codemayq) for their efforts in the development.

[23/07/09] We released **[FastEdit](https://github.com/hiyouga/FastEdit)** ‚ö°ü©π, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow [FastEdit](https://github.com/hiyouga/FastEdit) if you are interested.

[23/06/29] We provided a **reproducible example** of training a chat model using instruction-following datasets, see [Baichuan-7B-sft](https://huggingface.co/hiyouga/Baichuan-7B-sft) for details.

[23/06/22] We aligned the [demo API](src/api_demo.py) with the [OpenAI&#039;s](https://platform.openai.com/docs/api-reference/chat) format where you can insert the fine-tuned model in **arbitrary ChatGPT-based applications**.

[23/06/03] We supported quantized training and inference (aka **[QLoRA](https://github.com/artidoro/qlora)**). See [examples](examples/README.md) for usage.

&lt;/details&gt;

&gt; [!NOTE]
&gt; If you cannot use the latest feature, please pull the latest code and install LLaMA-Factory again.

## Supported Models

| Model                                                             | Model size                       | Template            |
| ----------------------------------------------------------------- | -------------------------------- | ------------------- |
| [Baichuan 2](https://huggingface.co/baichuan-inc)                 | 7B/13B                           | baichuan2           |
| [BLOOM/BLOOMZ](https://huggingface.co/bigscience)                 | 560M/1.1B/1.7B/3B/7.1B/176B      | -                   |
| [ChatGLM3](https://huggingface.co/THUDM)                          | 6B                               | chatglm3            |
| [Command R](https://huggingface.co/CohereForAI)                   | 35B/104B                         | cohere              |
| [DeepSeek (Code/MoE)](https://huggingface.co/deepseek-ai)         | 7B/16B/67B/236B                  | deepseek            |
| [DeepSeek 2.5/3](https://huggingface.co/deepseek-ai)              | 236B/671B                        | deepseek3           |
| [DeepSeek R1 (Distill)](https://huggingface.co/deepseek-ai)       | 1.5B/7B/8B/14B/32B/70B/671B      | deepseekr1          |
| [Falcon](https://huggingface.co/tiiuae)                           | 7B/11B/40B/180B                  | falcon              |
| [Gemma/Gemma 2/CodeGemma](https://huggingface.co/google)          | 2B/7B/9B/27B                     | gemma               |
| [Ge

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Tencent/Hunyuan3D-2]]></title>
            <link>https://github.com/Tencent/Hunyuan3D-2</link>
            <guid>https://github.com/Tencent/Hunyuan3D-2</guid>
            <pubDate>Thu, 01 May 2025 00:04:37 GMT</pubDate>
            <description><![CDATA[High-Resolution 3D Assets Generation with Large Scale Hunyuan3D Diffusion Models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Tencent/Hunyuan3D-2">Tencent/Hunyuan3D-2</a></h1>
            <p>High-Resolution 3D Assets Generation with Large Scale Hunyuan3D Diffusion Models.</p>
            <p>Language: Python</p>
            <p>Stars: 9,220</p>
            <p>Forks: 773</p>
            <p>Stars today: 49 stars today</p>
            <h2>README</h2><pre>[‰∏≠ÊñáÈòÖËØª](README_zh_cn.md)
[Êó•Êú¨Ë™û„ÅßË™≠„ÇÄ](README_ja_jp.md)

&lt;p align=&quot;center&quot;&gt; 
  &lt;img src=&quot;https://github.com/user-attachments/assets/efb402a1-0b09-41e0-a6cb-259d442e76aa&quot;&gt;

&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=https://3d.hunyuan.tencent.com target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage height=22px&gt;&lt;/a&gt;
  &lt;a href=https://huggingface.co/spaces/tencent/Hunyuan3D-2  target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/%F0%9F%A4%97%20Demo-276cb4.svg height=22px&gt;&lt;/a&gt;
  &lt;a href=https://huggingface.co/tencent/Hunyuan3D-2 target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px&gt;&lt;/a&gt;
  &lt;a href=https://3d-models.hunyuan.tencent.com/ target=&quot;_blank&quot;&gt;&lt;img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px&gt;&lt;/a&gt;
  &lt;a href=https://discord.gg/dNBrdrGGMa target=&quot;_blank&quot;&gt;&lt;img src= https://img.shields.io/badge/Discord-white.svg?logo=discord height=22px&gt;&lt;/a&gt;
  &lt;a href=https://arxiv.org/abs/2501.12202 target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px&gt;&lt;/a&gt;
  &lt;a href=https://x.com/txhunyuan target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px&gt;&lt;/a&gt;
 &lt;a href=&quot;#community-resources&quot; target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/Community-lavender.svg?logo=homeassistantcommunitystore height=22px&gt;&lt;/a&gt;
&lt;/div&gt;

[//]: # (  &lt;a href=# target=&quot;_blank&quot;&gt;&lt;img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px&gt;&lt;/a&gt;)

[//]: # (  &lt;a href=# target=&quot;_blank&quot;&gt;&lt;img src= https://img.shields.io/badge/Colab-8f2628.svg?logo=googlecolab height=22px&gt;&lt;/a&gt;)

[//]: # (  &lt;a href=&quot;#&quot;&gt;&lt;img alt=&quot;PyPI - Downloads&quot; src=&quot;https://img.shields.io/pypi/v/mulankit?logo=pypi&quot;  height=22px&gt;&lt;/a&gt;)

&lt;br&gt;

&lt;p align=&quot;center&quot;&gt;
‚Äú Living out everyone‚Äôs imagination on creating and manipulating 3D assets.‚Äù
&lt;/p&gt;

https://github.com/user-attachments/assets/a2cbc5b8-be22-49d7-b1c3-7aa2b20ba460




## üî• News

- Apr 1, 2025: ü§ó Release turbo paint model [Hunyuan3D-Paint-v2-0-Turbo](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-paint-v2-0-turbo), and multiview texture generation pipeline, try it [here](examples/fast_texture_gen_multiview.py)! Stay tuned for our new texture generation model [RomanTex](https://github.com/oakshy/RomanTex) and PBR material generation [MaterialMVP](https://github.com/ZebinHe/MaterialMVP/)! 
- Mar 19, 2025: ü§ó Release turbo model [Hunyuan3D-2-Turbo](https://huggingface.co/tencent/Hunyuan3D-2/), [Hunyuan3D-2mini-Turbo](https://huggingface.co/tencent/Hunyuan3D-2mini/) and [FlashVDM](https://github.com/Tencent/FlashVDM).
- Mar 18, 2025: ü§ó Release multiview shape model [Hunyuan3D-2mv](https://huggingface.co/tencent/Hunyuan3D-2mv) and 0.6B
  shape model [Hunyuan3D-2mini](https://huggingface.co/tencent/Hunyuan3D-2mini).
- Feb 14, 2025: üõ†Ô∏è Release texture enhancement module, please obtain high-definition textures
  via [here](minimal_demo.py)!
- Feb 3, 2025: üêé
  Release [Hunyuan3D-DiT-v2-0-Fast](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0-fast), our
  guidance distillation model that could half the dit inference time, see [here](minimal_demo.py) for usage.
- Jan 27, 2025: üõ†Ô∏è Release Blender addon for Hunyuan3D 2.0, Check it out [here](#blender-addon).
- Jan 23, 2025: üí¨ We thank community members for
  creating [Windows installation tool](https://github.com/YanWenKun/Hunyuan3D-2-WinPortable), ComfyUI support
  with [ComfyUI-Hunyuan3DWrapper](https://github.com/kijai/ComfyUI-Hunyuan3DWrapper)
  and [ComfyUI-3D-Pack](https://github.com/MrForExample/ComfyUI-3D-Pack) and other
  awesome [extensions](#community-resources).
- Jan 21, 2025: üí¨ Enjoy exciting 3D generation on our website [Hunyuan3D Studio](https://3d.hunyuan.tencent.com)!
- Jan 21, 2025: ü§ó Release inference code and pretrained models
  of [Hunyuan3D 2.0](https://huggingface.co/tencent/Hunyuan3D-2). Please give it a try
  via [huggingface space](https://huggingface.co/spaces/tencent/Hunyuan3D-2) and
  our [official site](https://3d.hunyuan.tencent.com)!

&gt; Join our **[Wechat](#)** and **[Discord](https://discord.gg/dNBrdrGGMa)** group to discuss and find help from us.

| Wechat Group                                     | Xiaohongshu                                           | X                                           | Discord                                           |
|--------------------------------------------------|-------------------------------------------------------|---------------------------------------------|---------------------------------------------------|
| &lt;img src=&quot;assets/qrcode/wechat.png&quot;  height=140&gt; | &lt;img src=&quot;assets/qrcode/xiaohongshu.png&quot;  height=140&gt; | &lt;img src=&quot;assets/qrcode/x.png&quot;  height=140&gt; | &lt;img src=&quot;assets/qrcode/discord.png&quot;  height=140&gt; |        




## **Abstract**

We present Hunyuan3D 2.0, an advanced large-scale 3D synthesis system for generating high-resolution textured 3D assets.
This system includes two foundation components: a large-scale shape generation model - Hunyuan3D-DiT, and a large-scale
texture synthesis model - Hunyuan3D-Paint.
The shape generative model, built on a scalable flow-based diffusion transformer, aims to create geometry that properly
aligns with a given condition image, laying a solid foundation for downstream applications.
The texture synthesis model, benefiting from strong geometric and diffusion priors, produces high-resolution and vibrant
texture maps for either generated or hand-crafted meshes.
Furthermore, we build Hunyuan3D-Studio - a versatile, user-friendly production platform that simplifies the re-creation
process of 3D assets. It allows both professional and amateur users to manipulate or even animate their meshes
efficiently.
We systematically evaluate our models, showing that Hunyuan3D 2.0 outperforms previous state-of-the-art models,
including the open-source models and closed-source models in geometry details, condition alignment, texture quality, and
e.t.c.



&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/images/system.jpg&quot;&gt;
&lt;/p&gt;

## ‚òØÔ∏è **Hunyuan3D 2.0**

### Architecture

Hunyuan3D 2.0 features a two-stage generation pipeline, starting with the creation of a bare mesh, followed by the
synthesis of a texture map for that mesh. This strategy is effective for decoupling the difficulties of shape and
texture generation and also provides flexibility for texturing either generated or handcrafted meshes.

&lt;p align=&quot;left&quot;&gt;
  &lt;img src=&quot;assets/images/arch.jpg&quot;&gt;
&lt;/p&gt;

### Performance

We have evaluated Hunyuan3D 2.0 with other open-source as well as close-source 3d-generation methods.
The numerical results indicate that Hunyuan3D 2.0 surpasses all baselines in the quality of generated textured 3D assets
and the condition following ability.

| Model                   | CMMD(‚¨á)   | FID_CLIP(‚¨á) | FID(‚¨á)      | CLIP-score(‚¨Ü) |
|-------------------------|-----------|-------------|-------------|---------------|
| Top Open-source Model1  | 3.591     | 54.639      | 289.287     | 0.787         |
| Top Close-source Model1 | 3.600     | 55.866      | 305.922     | 0.779         |
| Top Close-source Model2 | 3.368     | 49.744      | 294.628     | 0.806         |
| Top Close-source Model3 | 3.218     | 51.574      | 295.691     | 0.799         |
| Hunyuan3D 2.0           | **3.193** | **49.165**  | **282.429** | **0.809**     |

Generation results of Hunyuan3D 2.0:
&lt;p align=&quot;left&quot;&gt;
  &lt;img src=&quot;assets/images/e2e-1.gif&quot;  height=250&gt;
  &lt;img src=&quot;assets/images/e2e-2.gif&quot;  height=250&gt;
&lt;/p&gt;

## üéÅ Models Zoo

It takes 6 GB VRAM for shape generation and 16 GB for shape and texture generation in total.

Hunyuan3D-2mini Series

| Model                       | Description                   | Date       | Size | Huggingface                                                                                      |
|-----------------------------|-------------------------------|------------|------|--------------------------------------------------------------------------------------------------|
| Hunyuan3D-DiT-v2-mini-Turbo | Step Distillation Version     | 2025-03-19 | 0.6B | [Download](https://huggingface.co/tencent/Hunyuan3D-2mini/tree/main/hunyuan3d-dit-v2-mini-turbo) |
| Hunyuan3D-DiT-v2-mini-Fast  | Guidance Distillation Version | 2025-03-18 | 0.6B | [Download](https://huggingface.co/tencent/Hunyuan3D-2mini/tree/main/hunyuan3d-dit-v2-mini-fast)  |
| Hunyuan3D-DiT-v2-mini       | Mini Image to Shape Model     | 2025-03-18 | 0.6B | [Download](https://huggingface.co/tencent/Hunyuan3D-2mini/tree/main/hunyuan3d-dit-v2-mini)       |


Hunyuan3D-2mv Series

| Model                     | Description                    | Date       | Size | Huggingface                                                                                  |
|---------------------------|--------------------------------|------------|------|----------------------------------------------------------------------------------------------| 
| Hunyuan3D-DiT-v2-mv-Turbo | Step Distillation Version      | 2025-03-19 | 1.1B | [Download](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv-turbo) |
| Hunyuan3D-DiT-v2-mv-Fast  | Guidance Distillation Version  | 2025-03-18 | 1.1B | [Download](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv-fast)  |
| Hunyuan3D-DiT-v2-mv       | Multiview Image to Shape Model | 2025-03-18 | 1.1B | [Download](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv)       |

Hunyuan3D-2 Series

| Model                      | Description                 | Date       | Size | Huggingface                                                                               |
|----------------------------|-----------------------------|------------|------|-------------------------------------------------------------------------------------------| 
| Hunyuan3D-DiT-v2-0-Turbo   | Step Distillation Model     | 2025-03-19 | 1.1B | [Download](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0-turbo)   |
| Hunyuan3D-DiT-v2-0-Fast    | Guidance Distillation Model | 2025-02-03 | 1.1B | [Download](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0-fast)    |
| Hunyuan3D-DiT-v2-0         | Image to Shape Model        | 2025-01-21 | 1.1B | [Download](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0)         |
| Hunyuan3D-Paint-v2-0       | Texture Generation Model    | 2025-01-21 | 1.3B | [Download](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-paint-v2-0)       |
| Hunyuan3D-Paint-v2-0-Turbo | Distillation Texure Model   | 2025-04-01 | 1.3B | [Download](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-paint-v2-0-turbo) |
| Hunyuan3D-Delight-v2-0     | Image Delight Model         | 2025-01-21 | 1.3B | [Download](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-delight-v2-0)     | 

## ü§ó Get Started with Hunyuan3D 2.0

Hunyuan3D 2.0 supports Macos, Windows, Linux. You may follow the next steps to use Hunyuan3D 2.0 via:

- [Code](#code-usage)
- [Gradio App](#gradio-app)
- [API Server](#api-server)
- [Blender Addon](#blender-addon)
- [Official Site](#official-site)

### Install Requirements

Please install Pytorch via the [official](https://pytorch.org/) site. Then install the other requirements via

```bash
pip install -r requirements.txt
pip install -e .
# for texture
cd hy3dgen/texgen/custom_rasterizer
python3 setup.py install
cd ../../..
cd hy3dgen/texgen/differentiable_renderer
python3 setup.py install
```

### Code Usage

We designed a diffusers-like API to use our shape generation model - Hunyuan3D-DiT and texture synthesis model -
Hunyuan3D-Paint.

You could assess **Hunyuan3D-DiT** via:

```python
from hy3dgen.shapegen import Hunyuan3DDiTFlowMatchingPipeline

pipeline = Hunyuan3DDiTFlowMatchingPipeline.from_pretrained(&#039;tencent/Hunyuan3D-2&#039;)
mesh = pipeline(image=&#039;assets/demo.png&#039;)[0]
```

The output mesh is a [trimesh object](https://trimesh.org/trimesh.html), which you could save to glb/obj (or other
format) file.

For **Hunyuan3D-Paint**, do the following:

```python
from hy3dgen.texgen import Hunyuan3DPaintPipeline
from hy3dgen.shapegen import Hunyuan3DDiTFlowMatchingPipeline

# let&#039;s generate a mesh first
pipeline = Hunyuan3DDiTFlowMatchingPipeline.from_pretrained(&#039;tencent/Hunyuan3D-2&#039;)
mesh = pipeline(image=&#039;assets/demo.png&#039;)[0]

pipeline = Hunyuan3DPaintPipeline.from_pretrained(&#039;tencent/Hunyuan3D-2&#039;)
mesh = pipeline(mesh, image=&#039;assets/demo.png&#039;)
```

Please visit [examples](examples) folder for more advanced usage, such as **multiview image to 3D generation** and *
*texture generation
for handcrafted mesh**.

### Gradio App

You could also host a [Gradio](https://www.gradio.app/) App in your own computer via:

Standard Version

```bash
# Hunyuan3D-2mini
python3 gradio_app.py --model_path tencent/Hunyuan3D-2mini --subfolder hunyuan3d-dit-v2-mini --texgen_model_path tencent/Hunyuan3D-2 --low_vram_mode
# Hunyuan3D-2mv
python3 gradio_app.py --model_path tencent/Hunyuan3D-2mv --subfolder hunyuan3d-dit-v2-mv --texgen_model_path tencent/Hunyuan3D-2 --low_vram_mode
# Hunyuan3D-2
python3 gradio_app.py --model_path tencent/Hunyuan3D-2 --subfolder hunyuan3d-dit-v2-0 --texgen_model_path tencent/Hunyuan3D-2 --low_vram_mode
```

Turbo Version

```bash
# Hunyuan3D-2mini
python3 gradio_app.py --model_path tencent/Hunyuan3D-2mini --subfolder hunyuan3d-dit-v2-mini-turbo --texgen_model_path tencent/Hunyuan3D-2 --low_vram_mode --enable_flashvdm
# Hunyuan3D-2mv
python3 gradio_app.py --model_path tencent/Hunyuan3D-2mv --subfolder hunyuan3d-dit-v2-mv-turbo --texgen_model_path tencent/Hunyuan3D-2 --low_vram_mode --enable_flashvdm
# Hunyuan3D-2
python3 gradio_app.py --model_path tencent/Hunyuan3D-2 --subfolder hunyuan3d-dit-v2-0-turbo --texgen_model_path tencent/Hunyuan3D-2 --low_vram_mode --enable_flashvdm
```

### API Server

You could launch an API server locally, which you could post web request for Image/Text to 3D, Texturing existing mesh,
and e.t.c.

```bash
python api_server.py --host 0.0.0.0 --port 8080
```

A demo post request for image to 3D without texture.

```bash
img_b64_str=$(base64 -i assets/demo.png)
curl -X POST &quot;http://localhost:8080/generate&quot; \
     -H &quot;Content-Type: application/json&quot; \
     -d &#039;{
           &quot;image&quot;: &quot;&#039;&quot;$img_b64_str&quot;&#039;&quot;,
         }&#039; \
     -o test2.glb
```

### Blender Addon

With an API server launched, you could also directly use Hunyuan3D 2.0 in your blender with
our [Blender Addon](blender_addon.py). Please follow our tutorial to install and use.

https://github.com/user-attachments/assets/8230bfb5-32b1-4e48-91f4-a977c54a4f3e

### Official Site

Don&#039;t forget to visit [Hunyuan3D](https://3d.hunyuan.tencent.com) for quick use, if you don&#039;t want to host yourself.

## üìë Open-Source Plan

- [x] Inference Code
- [x] Model Checkpoints
- [x] Technical Report
- [x] ComfyUI
- [ ] TensorRT Version
- [ ] Finetuning

## üîó BibTeX

If you found this repository helpful, please cite our reports:

```bibtex
@misc{hunyuan3d22025tencent,
    title={Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation},
    author={Tencent Hunyuan3D Team},
    year={2025},
    eprint={2501.12202},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{yang2024hunyuan3d,
    title={Hunyuan3D 1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation},
    author={Tencent Hunyuan3D Team},
    year={2024},
    eprint={2411.02293},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
```

## Community Resources

Thanks for the contributions of community members, here we have these great extensions of Hunyuan3D 2.0:

- [ComfyUI-3D-Pack](https://github.com/MrForExample/ComfyUI-3D-Pack)
- [ComfyUI-Hunyuan3DWrapper](https://github.com/kijai/ComfyUI-Hunyuan3DWrapper)
- [Hunyuan3D-2-for-windows](https://github.com/sdbds/Hunyuan3D-2-for-windows)
- [üì¶ A bundle for running on Windows | Êï¥ÂêàÂåÖ](https://github.com/YanWenKun/Hunyuan3D-2-WinPortable)
- [Hunyuan3D-2GP](https://github.com/deepbeepmeep/Hunyuan3D-2GP)
- [Kaggle Notebook](https://github.com/darkon12/Hunyuan3D-2GP_Kaggle)

## Acknowledgements

We would like to thank the contributors to
the [Trellis](https://github.com/microsoft/TRELLIS),  [DINOv2](https://github.com/facebookresearch/dinov2), [Stable Diffusion](https://github.com/Stability-AI/stablediffusion), [FLUX](https://github.com/black-forest-labs/flux), [diffusers](https://github.com/huggingface/diffusers), [HuggingFace](https://huggingface.co), [CraftsMan3D](https://github.com/wyysf-98/CraftsMan3D),
and [Michelangelo](https://github.com/NeuralCarver/Michelangelo/tree/main) repositories, for their open research and
exploration.

## Star History

&lt;a href=&quot;https://star-history.com/#Tencent/Hunyuan3D-2&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Tencent/Hunyuan3D-2&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=Tencent/Hunyuan3D-2&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=Tencent/Hunyuan3D-2&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pytorch/executorch]]></title>
            <link>https://github.com/pytorch/executorch</link>
            <guid>https://github.com/pytorch/executorch</guid>
            <pubDate>Thu, 01 May 2025 00:04:36 GMT</pubDate>
            <description><![CDATA[On-device AI across mobile, embedded and edge for PyTorch]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pytorch/executorch">pytorch/executorch</a></h1>
            <p>On-device AI across mobile, embedded and edge for PyTorch</p>
            <p>Language: Python</p>
            <p>Stars: 2,784</p>
            <p>Forks: 531</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/source/_static/img/et-logo.png&quot; alt=&quot;Logo&quot; width=&quot;200&quot;&gt;
  &lt;h1 align=&quot;center&quot;&gt;ExecuTorch: A powerful on-device AI Framework&lt;/h1&gt;
&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/pytorch/executorch/graphs/contributors&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/pytorch/executorch?style=for-the-badge&amp;color=blue&quot; alt=&quot;Contributors&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/pytorch/executorch/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/pytorch/executorch?style=for-the-badge&amp;color=blue&quot; alt=&quot;Stargazers&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/Dh43CKSAdc&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Join our Discord community&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pytorch.org/executorch/main/index&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;logoColor=FFE165&amp;style=for-the-badge&quot; alt=&quot;Check out the documentation&quot;&gt;&lt;/a&gt;
  &lt;hr&gt;
&lt;/div&gt;

**ExecuTorch** is an end-to-end solution for on-device inference and training. It powers much of Meta&#039;s on-device AI experiences across Facebook, Instagram, Meta Quest, Ray-Ban Meta Smart Glasses, WhatsApp, and more.

It supports a wide range of models including LLMs (Large Language Models), CV (Computer Vision), ASR (Automatic Speech Recognition), and TTS (Text to Speech).

Platform Support:
- Operating Systems:
  - iOS
  - Mac
  - Android
  - Linux
  - Microcontrollers

- Hardware Acceleration:
  - Apple
  - Arm
  - Cadence
  - MediaTek
  - OpenVINO
  - Qualcomm
  - Vulkan
  - XNNPACK

Key value propositions of ExecuTorch are:

- **Portability:** Compatibility with a wide variety of computing platforms,
  from high-end mobile phones to highly constrained embedded systems and
  microcontrollers.
- **Productivity:** Enabling developers to use the same toolchains and Developer
  Tools from PyTorch model authoring and conversion, to debugging and deployment
  to a wide variety of platforms.
- **Performance:** Providing end users with a seamless and high-performance
  experience due to a lightweight runtime and utilizing full hardware
  capabilities such as CPUs, NPUs, and DSPs.

## Getting Started
To get started you can:

- Visit the [Step by Step Tutorial](https://pytorch.org/executorch/stable/getting-started.html) to get things running locally and deploy a model to a device
- Use this [Colab Notebook](https://colab.research.google.com/drive/1qpxrXC3YdJQzly3mRg-4ayYiOjC6rue3?usp=sharing) to start playing around right away
- Jump straight into LLM use cases by following specific instructions for popular open-source models such as [Llama](examples/models/llama/README.md), [Qwen 3](examples/models/qwen3/README.md), [Phi-4-mini](examples/models/phi_4_mini/README.md), and [Llava](examples/models/llava/README.md)

## Feedback and Engagement

We welcome any feedback, suggestions, and bug reports from the community to help
us improve our technology. Check out the [Discussion Board](https://github.com/pytorch/executorch/discussions) or chat real time with us on [Discord](https://discord.gg/Dh43CKSAdc)

## Contributing

We welcome contributions. To get started review the [guidelines](CONTRIBUTING.md) and chat with us on [Discord](https://discord.gg/Dh43CKSAdc)


## Directory Structure

Please refer to the [Codebase structure](CONTRIBUTING.md#codebase-structure) section of the [Contributing Guidelines](CONTRIBUTING.md) for more details.

## License
ExecuTorch is BSD licensed, as found in the LICENSE file.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[xorbitsai/inference]]></title>
            <link>https://github.com/xorbitsai/inference</link>
            <guid>https://github.com/xorbitsai/inference</guid>
            <pubDate>Thu, 01 May 2025 00:04:35 GMT</pubDate>
            <description><![CDATA[Replace OpenAI GPT with another LLM in your app by changing a single line of code. Xinference gives you the freedom to use any LLM you need. With Xinference, you're empowered to run inference with any open-source language models, speech recognition models, and multimodal models, whether in the cloud, on-premises, or even on your laptop.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/xorbitsai/inference">xorbitsai/inference</a></h1>
            <p>Replace OpenAI GPT with another LLM in your app by changing a single line of code. Xinference gives you the freedom to use any LLM you need. With Xinference, you're empowered to run inference with any open-source language models, speech recognition models, and multimodal models, whether in the cloud, on-premises, or even on your laptop.</p>
            <p>Language: Python</p>
            <p>Stars: 7,698</p>
            <p>Forks: 652</p>
            <p>Stars today: 18 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;./assets/xorbits-logo.png&quot; width=&quot;180px&quot; alt=&quot;xorbits&quot; /&gt;

# Xorbits Inference: Model Serving Made Easy ü§ñ

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://inference.top/&quot;&gt;Xinference Cloud&lt;/a&gt; ¬∑
  &lt;a href=&quot;https://github.com/xorbitsai/enterprise-docs/blob/main/README.md&quot;&gt;Xinference Enterprise&lt;/a&gt; ¬∑
  &lt;a href=&quot;https://inference.readthedocs.io/en/latest/getting_started/installation.html#installation&quot;&gt;Self-hosting&lt;/a&gt; ¬∑
  &lt;a href=&quot;https://inference.readthedocs.io/&quot;&gt;Documentation&lt;/a&gt;
&lt;/p&gt;

[![PyPI Latest Release](https://img.shields.io/pypi/v/xinference.svg?style=for-the-badge)](https://pypi.org/project/xinference/)
[![License](https://img.shields.io/pypi/l/xinference.svg?style=for-the-badge)](https://github.com/xorbitsai/inference/blob/main/LICENSE)
[![Build Status](https://img.shields.io/github/actions/workflow/status/xorbitsai/inference/python.yaml?branch=main&amp;style=for-the-badge&amp;label=GITHUB%20ACTIONS&amp;logo=github)](https://actions-badge.atrox.dev/xorbitsai/inference/goto?ref=main)
[![Discord](https://img.shields.io/badge/join_Discord-5462eb.svg?logo=discord&amp;style=for-the-badge&amp;logoColor=%23f5f5f5)](https://discord.gg/Xw9tszSkr5)
[![Twitter](https://img.shields.io/twitter/follow/xorbitsio?logo=x&amp;style=for-the-badge)](https://twitter.com/xorbitsio)

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;./README.md&quot;&gt;&lt;img alt=&quot;README in English&quot; src=&quot;https://img.shields.io/badge/English-454545?style=for-the-badge&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_zh_CN.md&quot;&gt;&lt;img alt=&quot;ÁÆÄ‰Ωì‰∏≠ÊñáÁâàËá™Ëø∞Êñá‰ª∂&quot; src=&quot;https://img.shields.io/badge/‰∏≠Êñá‰ªãÁªç-d9d9d9?style=for-the-badge&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_ja_JP.md&quot;&gt;&lt;img alt=&quot;Êó•Êú¨Ë™û„ÅÆREADME&quot; src=&quot;https://img.shields.io/badge/Êó•Êú¨Ë™û-d9d9d9?style=for-the-badge&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;/div&gt;
&lt;br /&gt;


Xorbits Inference(Xinference) is a powerful and versatile library designed to serve language, 
speech recognition, and multimodal models. With Xorbits Inference, you can effortlessly deploy 
and serve your or state-of-the-art built-in models using just a single command. Whether you are a 
researcher, developer, or data scientist, Xorbits Inference empowers you to unleash the full 
potential of cutting-edge AI models.

&lt;div align=&quot;center&quot;&gt;
&lt;i&gt;&lt;a href=&quot;https://discord.gg/Xw9tszSkr5&quot;&gt;üëâ Join our Discord community!&lt;/a&gt;&lt;/i&gt;
&lt;/div&gt;

## üî• Hot Topics
### Framework Enhancements
- [Xllamacpp](https://github.com/xorbitsai/xllamacpp): New llama.cpp Python binding, maintained by Xinference team, supports continuous batching and is more production-ready.: [#2997](https://github.com/xorbitsai/inference/pull/2997)
- Distributed inference: running models across workers: [#2877](https://github.com/xorbitsai/inference/pull/2877)
- VLLM enhancement: Shared KV cache across multiple replicas: [#2732](https://github.com/xorbitsai/inference/pull/2732)
- Support Continuous batching for Transformers engine: [#1724](https://github.com/xorbitsai/inference/pull/1724)
- Support MLX backend for Apple Silicon chips: [#1765](https://github.com/xorbitsai/inference/pull/1765)
- Support specifying worker and GPU indexes for launching models: [#1195](https://github.com/xorbitsai/inference/pull/1195)
- Support SGLang backend: [#1161](https://github.com/xorbitsai/inference/pull/1161)
- Support LoRA for LLM and image models: [#1080](https://github.com/xorbitsai/inference/pull/1080)
### New Models
- Built-in support for [Qwen2.5-Omni](https://github.com/QwenLM/Qwen2.5-Omni): [#3279](https://github.com/xorbitsai/inference/pull/3279)
- Built-in support for [Skywork-OR1](https://github.com/SkyworkAI/Skywork-OR1): [#3274](https://github.com/xorbitsai/inference/pull/3274)
- Built-in support for [GLM-4-0414](https://github.com/THUDM/GLM-4): [#3251](https://github.com/xorbitsai/inference/pull/3251)
- Built-in support for [SeaLLMs-v3](https://github.com/DAMO-NLP-SG/DAMO-SeaLLMs): [#3248](https://github.com/xorbitsai/inference/pull/3248)
- Built-in support for [paraformer-zh](https://huggingface.co/funasr/paraformer-zh): [#3236](https://github.com/xorbitsai/inference/pull/3236)
- Built-in support for [InternVL3](https://internvl.github.io/blog/2025-04-11-InternVL-3.0/): [#3235](https://github.com/xorbitsai/inference/pull/3235)
- Built-in support for [MegaTTS3](https://github.com/bytedance/MegaTTS3): [#3224](https://github.com/xorbitsai/inference/pull/3224)
- Built-in support for [Deepseek-VL2](https://github.com/deepseek-ai/DeepSeek-VL2): [#3179](https://github.com/xorbitsai/inference/pull/3179)
### Integrations
- [Dify](https://docs.dify.ai/advanced/model-configuration/xinference): an LLMOps platform that enables developers (and even non-developers) to quickly build useful applications based on large language models, ensuring they are visual, operable, and improvable.
- [FastGPT](https://github.com/labring/FastGPT): a knowledge-based platform built on the LLM, offers out-of-the-box data processing and model invocation capabilities, allows for workflow orchestration through Flow visualization.
- [RAGFlow](https://github.com/infiniflow/ragflow): is an open-source RAG engine based on deep document understanding.
- [MaxKB](https://github.com/1Panel-dev/MaxKB): MaxKB = Max Knowledge Base, it is a chatbot based on Large Language Models (LLM) and Retrieval-Augmented Generation (RAG). 
- [Chatbox](https://chatboxai.app/): a desktop client for multiple cutting-edge LLM models, available on Windows, Mac and Linux.


## Key Features
üåü **Model Serving Made Easy**: Simplify the process of serving large language, speech 
recognition, and multimodal models. You can set up and deploy your models
for experimentation and production with a single command.

‚ö°Ô∏è **State-of-the-Art Models**: Experiment with cutting-edge built-in models using a single 
command. Inference provides access to state-of-the-art open-source models!

üñ• **Heterogeneous Hardware Utilization**: Make the most of your hardware resources with
[ggml](https://github.com/ggerganov/ggml). Xorbits Inference intelligently utilizes heterogeneous
hardware, including GPUs and CPUs, to accelerate your model inference tasks.

‚öôÔ∏è **Flexible API and Interfaces**: Offer multiple interfaces for interacting
with your models, supporting OpenAI compatible RESTful API (including Function Calling API), RPC, CLI 
and WebUI for seamless model management and interaction.

üåê **Distributed Deployment**: Excel in distributed deployment scenarios, 
allowing the seamless distribution of model inference across multiple devices or machines.

üîå **Built-in Integration with Third-Party Libraries**: Xorbits Inference seamlessly integrates
with popular third-party libraries including [LangChain](https://python.langchain.com/docs/integrations/providers/xinference), [LlamaIndex](https://gpt-index.readthedocs.io/en/stable/examples/llm/XinferenceLocalDeployment.html#i-run-pip-install-xinference-all-in-a-terminal-window), [Dify](https://docs.dify.ai/advanced/model-configuration/xinference), and [Chatbox](https://chatboxai.app/).

## Why Xinference
| Feature                                        | Xinference | FastChat | OpenLLM | RayLLM |
|------------------------------------------------|------------|----------|---------|--------|
| OpenAI-Compatible RESTful API                  | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| vLLM Integrations                              | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| More Inference Engines (GGML, TensorRT)        | ‚úÖ | ‚ùå | ‚úÖ | ‚úÖ |
| More Platforms (CPU, Metal)                    | ‚úÖ | ‚úÖ | ‚ùå | ‚ùå |
| Multi-node Cluster Deployment                  | ‚úÖ | ‚ùå | ‚ùå | ‚úÖ |
| Image Models (Text-to-Image)                   | ‚úÖ | ‚úÖ | ‚ùå | ‚ùå |
| Text Embedding Models                          | ‚úÖ | ‚ùå | ‚ùå | ‚ùå |
| Multimodal Models                              | ‚úÖ | ‚ùå | ‚ùå | ‚ùå |
| Audio Models                                   | ‚úÖ | ‚ùå | ‚ùå | ‚ùå |
| More OpenAI Functionalities (Function Calling) | ‚úÖ | ‚ùå | ‚ùå | ‚ùå |

## Using Xinference

- **Cloud &lt;/br&gt;**
We host a [Xinference Cloud](https://inference.top) service for anyone to try with zero setup. 

- **Self-hosting Xinference Community Edition&lt;/br&gt;**
Quickly get Xinference running in your environment with this [starter guide](#getting-started).
Use our [documentation](https://inference.readthedocs.io/) for further references and more in-depth instructions.

- **Xinference for enterprise / organizations&lt;/br&gt;**
We provide additional enterprise-centric features. [send us an email](mailto:business@xprobe.io?subject=[GitHub]Business%20License%20Inquiry) to discuss enterprise needs. &lt;/br&gt;

## Staying Ahead

Star Xinference on GitHub and be instantly notified of new releases.

![star-us](assets/stay_ahead.gif)

## Getting Started

* [Docs](https://inference.readthedocs.io/en/latest/index.html)
* [Built-in Models](https://inference.readthedocs.io/en/latest/models/builtin/index.html)
* [Custom Models](https://inference.readthedocs.io/en/latest/models/custom.html)
* [Deployment Docs](https://inference.readthedocs.io/en/latest/getting_started/using_xinference.html)
* [Examples and Tutorials](https://inference.readthedocs.io/en/latest/examples/index.html)

### Jupyter Notebook

The lightest way to experience Xinference is to try our [Jupyter Notebook on Google Colab](https://colab.research.google.com/github/xorbitsai/inference/blob/main/examples/Xinference_Quick_Start.ipynb).

### Docker 

Nvidia GPU users can start Xinference server using [Xinference Docker Image](https://inference.readthedocs.io/en/latest/getting_started/using_docker_image.html). Prior to executing the installation command, ensure that both [Docker](https://docs.docker.com/get-docker/) and [CUDA](https://developer.nvidia.com/cuda-downloads) are set up on your system.

```bash
docker run --name xinference -d -p 9997:9997 -e XINFERENCE_HOME=/data -v &lt;/on/your/host&gt;:/data --gpus all xprobe/xinference:latest xinference-local -H 0.0.0.0
```

### K8s via helm

Ensure that you have GPU support in your Kubernetes cluster, then install as follows.

```
# add repo
helm repo add xinference https://xorbitsai.github.io/xinference-helm-charts

# update indexes and query xinference versions
helm repo update xinference
helm search repo xinference/xinference --devel --versions

# install xinference
helm install xinference xinference/xinference -n xinference --version 0.0.1-v&lt;xinference_release_version&gt;
```

For more customized installation methods on K8s, please refer to the [documentation](https://inference.readthedocs.io/en/latest/getting_started/using_kubernetes.html).

### Quick Start

Install Xinference by using pip as follows. (For more options, see [Installation page](https://inference.readthedocs.io/en/latest/getting_started/installation.html).)

```bash
pip install &quot;xinference[all]&quot;
```

To start a local instance of Xinference, run the following command:

```bash
$ xinference-local
```

Once Xinference is running, there are multiple ways you can try it: via the web UI, via cURL,
 via the command line, or via the Xinference‚Äôs python client. Check out our [docs]( https://inference.readthedocs.io/en/latest/getting_started/using_xinference.html#run-xinference-locally) for the guide.

![web UI](assets/screenshot.png)

## Getting involved

| Platform                                                                                        | Purpose                                     |
|-------------------------------------------------------------------------------------------------|---------------------------------------------|
| [Github Issues](https://github.com/xorbitsai/inference/issues)                                  | Reporting bugs and filing feature requests. |
| [Discord](https://discord.gg/Xw9tszSkr5) | Collaborating with other Xinference users.  |
| [Twitter](https://twitter.com/xorbitsio)                                                        | Staying up-to-date on new features.         |

## Citation

If this work is helpful, please kindly cite as:

```bibtex
@inproceedings{lu2024xinference,
    title = &quot;Xinference: Making Large Model Serving Easy&quot;,
    author = &quot;Lu, Weizheng and Xiong, Lingfeng and Zhang, Feng and Qin, Xuye and Chen, Yueguo&quot;,
    booktitle = &quot;Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations&quot;,
    month = nov,
    year = &quot;2024&quot;,
    address = &quot;Miami, Florida, USA&quot;,
    publisher = &quot;Association for Computational Linguistics&quot;,
    url = &quot;https://aclanthology.org/2024.emnlp-demo.30&quot;,
    pages = &quot;291--300&quot;,
}
```

## Contributors

&lt;a href=&quot;https://github.com/xorbitsai/inference/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=xorbitsai/inference&quot; /&gt;
&lt;/a&gt;

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=xorbitsai/inference&amp;type=Date)](https://star-history.com/#xorbitsai/inference&amp;Date)</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[meta-llama/PurpleLlama]]></title>
            <link>https://github.com/meta-llama/PurpleLlama</link>
            <guid>https://github.com/meta-llama/PurpleLlama</guid>
            <pubDate>Thu, 01 May 2025 00:04:34 GMT</pubDate>
            <description><![CDATA[Set of tools to assess and improve LLM security.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/meta-llama/PurpleLlama">meta-llama/PurpleLlama</a></h1>
            <p>Set of tools to assess and improve LLM security.</p>
            <p>Language: Python</p>
            <p>Stars: 3,130</p>
            <p>Forks: 527</p>
            <p>Stars today: 64 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/facebookresearch/PurpleLlama/blob/main/logo.png&quot; width=&quot;400&quot;/&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
        ü§ó &lt;a href=&quot;https://huggingface.co/meta-Llama&quot;&gt; Models on Hugging Face&lt;/a&gt;&amp;nbsp | &lt;a href=&quot;https://ai.meta.com/blog/purple-llama-open-trust-safety-generative-ai&quot;&gt; Blog&lt;/a&gt;&amp;nbsp |  &lt;a href=&quot;https://ai.meta.com/llama/purple-llama&quot;&gt;Website&lt;/a&gt;&amp;nbsp | &lt;a href=&quot;https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/&quot;&gt;CyberSec Eval Paper&lt;/a&gt;&amp;nbsp&amp;nbsp | &lt;a href=&quot;https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/&quot;&gt;Llama Guard Paper&lt;/a&gt;&amp;nbsp
&lt;br&gt;

---

# Purple Llama

Purple Llama is an umbrella project that over time will bring together tools
and evals to help the community build responsibly with open generative AI
models. The initial release will include tools and evals for Cyber Security and
Input/Output safeguards but we plan to contribute more in the near future.

## Why purple?

Borrowing a [concept](https://www.youtube.com/watch?v=ab_Fdp6FVDI) from the
cybersecurity world, we believe that to truly mitigate the challenges which
generative AI presents, we need to take both attack (red team) and defensive
(blue team) postures. Purple teaming, composed of both red and blue team
responsibilities, is a collaborative approach to evaluating and mitigating
potential risks and the same ethos applies to generative AI and hence our
investment in Purple Llama will be comprehensive.

## License

Components within the Purple Llama project will be licensed permissively enabling both research and commercial usage.
We believe this is a major step towards enabling community collaboration and standardizing the development and usage of trust and safety tools for generative AI development.
More concretely evals and benchmarks are licensed under the MIT license while any models use the corresponding Llama Community license. See the table below:

| **Component Type** |            **Components**            |                                          **License**                                           |
| :----------------- | :----------------------------------: | :--------------------------------------------------------------------------------------------: |
| Evals/Benchmarks   | Cyber Security Eval (others to come) |                                              MIT                                               |
| Safeguard             |             Llama Guard              | [Llama 2 Community License](https://github.com/facebookresearch/PurpleLlama/blob/main/LICENSE) |
| Safeguard             |             Llama Guard 2            | [Llama 3 Community License](https://github.com/meta-llama/llama3/blob/main/LICENSE) |
| Safeguard             |             Llama Guard 3-8B            | [Llama 3.2 Community License](LICENSE) |
| Safeguard             |             Llama Guard 3-1B            | [Llama 3.2 Community License](LICENSE) |
| Safeguard             |             Llama Guard 3-11B-vision            | [Llama 3.2 Community License](LICENSE) |
| Safeguard             |             Prompt Guard            | [Llama 3.2 Community License](LICENSE) |
| Safeguard          |             Code Shield              | MIT |


## System-Level Safeguards

As we outlined in Llama 3‚Äôs
[Responsible Use Guide](https://ai.meta.com/llama/responsible-use-guide/), we
recommend that all inputs and outputs to the LLM be checked and filtered in
accordance with content guidelines appropriate to the application.

### Llama Guard

Llama Guard 3 consists of a series of high-performance input and output moderation models designed to support developers to detect various common types of violating content.

They were built by fine-tuning Meta-Llama 3.1 and 3.2 models and optimized to support the detection of the MLCommons standard hazards taxonomy, catering to a range of developer use cases.
They support the release of Llama 3.2 capabilities, including 7 new languages, a 128k context window, and image reasoning. Llama Guard 3 models were also optimized to detect helpful cyberattack responses and prevent malicious code output by LLMs to be executed in hosting environments for Llama systems using code interpreters.


### Prompt Guard
Prompt Guard is a powerful tool for protecting LLM powered applications from malicious prompts to ensure their security and integrity.

Categories of prompt attacks include prompt injection and jailbreaking:

* Prompt Injections are inputs that exploit the inclusion of untrusted data from third parties into the context window of a model to get it to execute unintended instructions.
* Jailbreaks are malicious instructions designed to override the safety and security features built into a model.

### Code Shield

Code Shield adds support for inference-time filtering of insecure code produced by LLMs. Code Shield offers mitigation of insecure code suggestions risk, code interpreter abuse prevention, and secure command execution. [CodeShield Example Notebook](https://github.com/meta-llama/PurpleLlama/blob/main/CodeShield/notebook/CodeShieldUsageDemo.ipynb).



## Evals &amp; Benchmarks

### Cybersecurity

#### CyberSec Eval v1
CyberSec Eval v1 was what we believe was the first industry-wide set of cybersecurity safety evaluations for LLMs. These benchmarks are based on industry guidance and standards (e.g., CWE and MITRE ATT&amp;CK) and built in collaboration with our security subject matter experts. We aim to provide tools that will help address some risks outlined in the [White House commitments on developing responsible AI](https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/), including:
* Metrics for quantifying LLM cybersecurity risks.
* Tools to evaluate the frequency of insecure code suggestions.
* Tools to evaluate LLMs to make it harder to generate malicious code or aid in carrying out cyberattacks.

We believe these tools will reduce the frequency of LLMs suggesting insecure AI-generated code and reduce their helpfulness to cyber adversaries. Our initial results show that there are meaningful cybersecurity risks for LLMs, both with recommending insecure code and for complying with malicious requests. See our [Cybersec Eval paper](https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/) for more details.

#### CyberSec Eval 2
CyberSec Eval 2 expands on its predecessor by measuring an LLM‚Äôs propensity to abuse a code interpreter, offensive cybersecurity capabilities, and susceptibility to prompt injection. You can read the paper [here](https://ai.meta.com/research/publications/cyberseceval-2-a-wide-ranging-cybersecurity-evaluation-suite-for-large-language-models/).

You can also check out the ü§ó leaderboard [here](https://huggingface.co/spaces/facebook/CyberSecEval).

#### CyberSec Eval 3
The newly released CyberSec Eval 3 features three additional test suites: visual prompt injection tests, spear phishing capability tests, and autonomous offensive cyber operations tests.

## Getting Started

As part of the [Llama reference system](https://github.com/meta-llama/llama-agentic-system), we‚Äôre integrating a safety layer to facilitate adoption and deployment of these safeguards.
Resources to get started with the safeguards are available in the [Llama-recipe GitHub repository](https://github.com/meta-llama/llama-recipes).

## FAQ

For a running list of frequently asked questions, for not only Purple Llama
components but also generally for Llama models, see the FAQ
[here](https://ai.meta.com/llama/faq/).

## Join the Purple Llama community

See the [CONTRIBUTING](CONTRIBUTING.md) file for how to help out.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[vllm-project/vllm-ascend]]></title>
            <link>https://github.com/vllm-project/vllm-ascend</link>
            <guid>https://github.com/vllm-project/vllm-ascend</guid>
            <pubDate>Thu, 01 May 2025 00:04:33 GMT</pubDate>
            <description><![CDATA[Community maintained hardware plugin for vLLM on Ascend]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/vllm-project/vllm-ascend">vllm-project/vllm-ascend</a></h1>
            <p>Community maintained hardware plugin for vLLM on Ascend</p>
            <p>Language: Python</p>
            <p>Stars: 563</p>
            <p>Forks: 121</p>
            <p>Stars today: 4 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/vllm-project/vllm-ascend/main/docs/source/logos/vllm-ascend-logo-text-dark.png&quot;&gt;
    &lt;img alt=&quot;vllm-ascend&quot; src=&quot;https://raw.githubusercontent.com/vllm-project/vllm-ascend/main/docs/source/logos/vllm-ascend-logo-text-light.png&quot; width=55%&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;h3 align=&quot;center&quot;&gt;
vLLM Ascend Plugin
&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
| &lt;a href=&quot;https://www.hiascend.com/en/&quot;&gt;&lt;b&gt;About Ascend&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://vllm-ascend.readthedocs.io/en/latest/&quot;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://slack.vllm.ai&quot;&gt;&lt;b&gt;#sig-ascend&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://discuss.vllm.ai/c/hardware-support/vllm-ascend-support&quot;&gt;&lt;b&gt;Users Forum&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://tinyurl.com/vllm-ascend-meeting&quot;&gt;&lt;b&gt;Weekly Meeting&lt;/b&gt;&lt;/a&gt; |
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a &gt;&lt;b&gt;English&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;README.zh.md&quot;&gt;&lt;b&gt;‰∏≠Êñá&lt;/b&gt;&lt;/a&gt;
&lt;/p&gt;

---
*Latest News* üî•
- [2025/03] We hosted the [vLLM Beijing Meetup](https://mp.weixin.qq.com/s/VtxO9WXa5fC-mKqlxNUJUQ) with vLLM team! Please find the meetup slides [here](https://drive.google.com/drive/folders/1Pid6NSFLU43DZRi0EaTcPgXsAzDvbBqF).
- [2025/02] vLLM community officially created [vllm-project/vllm-ascend](https://github.com/vllm-project/vllm-ascend) repo for running vLLM seamlessly on the Ascend NPU.
- [2024/12] We are working with the vLLM community to support [[RFC]: Hardware pluggable](https://github.com/vllm-project/vllm/issues/11162).
---
## Overview

vLLM Ascend (`vllm-ascend`) is a community maintained hardware plugin for running vLLM seamlessly on the Ascend NPU.

It is the recommended approach for supporting the Ascend backend within the vLLM community. It adheres to the principles outlined in the [[RFC]: Hardware pluggable](https://github.com/vllm-project/vllm/issues/11162), providing a hardware-pluggable interface that decouples the integration of the Ascend NPU with vLLM.

By using vLLM Ascend plugin, popular open-source models, including Transformer-like, Mixture-of-Expert, Embedding, Multi-modal LLMs can run seamlessly on the Ascend NPU.

## Prerequisites

- Hardware: Atlas 800I A2 Inference series, Atlas A2 Training series
- OS: Linux
- Software:
  * Python &gt;= 3.9, &lt; 3.12
  * CANN &gt;= 8.0.0
  * PyTorch &gt;= 2.5.1, torch-npu &gt;= 2.5.1
  * vLLM (the same version as vllm-ascend)

## Getting Started

Please refer to [QuickStart](https://vllm-ascend.readthedocs.io/en/latest/quick_start.html) and [Installation](https://vllm-ascend.readthedocs.io/en/latest/installation.html) for more details.

## Contributing
See [CONTRIBUTING](https://vllm-ascend.readthedocs.io/en/main/developer_guide/contributing.html) for more details, which is a step-by-step guide to help you set up development environment, build and test.

We welcome and value any contributions and collaborations:
- Please let us know if you encounter a bug by [filing an issue](https://github.com/vllm-project/vllm-ascend/issues)
- Please use [User forum](https://discuss.vllm.ai/c/hardware-support/vllm-ascend-support) for usage questions and help.

## Branch

vllm-ascend has main branch and dev branch.

- **main**: main branchÔºåcorresponds to the vLLM main branch, and is continuously monitored for quality through Ascend CI.
- **vX.Y.Z-dev**: development branch, created with part of new releases of vLLM. For example, `v0.7.3-dev` is the dev branch for vLLM `v0.7.3` version.

Below is maintained branches:

| Branch     | Status       | Note                                 |
|------------|--------------|--------------------------------------|
| main       | Maintained   | CI commitment for vLLM main branch and vLLM 0.8.x branch   |
| v0.7.1-dev | Unmaintained | Only doc fixed is allowed |
| v0.7.3-dev | Maintained   | CI commitment for vLLM 0.7.3 version |

Please refer to [Versioning policy](https://vllm-ascend.readthedocs.io/en/main/developer_guide/versioning_policy.html) for more details.

## Weekly Meeting

- vLLM Ascend Weekly Meeting: https://tinyurl.com/vllm-ascend-meeting
- Wednesday, 15:00 - 16:00 (UTC+8, [Convert to your timezone](https://dateful.com/convert/gmt8?t=15))

## License

Apache License 2.0, as found in the [LICENSE](./LICENSE) file.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sherlock-project/sherlock]]></title>
            <link>https://github.com/sherlock-project/sherlock</link>
            <guid>https://github.com/sherlock-project/sherlock</guid>
            <pubDate>Thu, 01 May 2025 00:04:32 GMT</pubDate>
            <description><![CDATA[Hunt down social media accounts by username across social networks]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sherlock-project/sherlock">sherlock-project/sherlock</a></h1>
            <p>Hunt down social media accounts by username across social networks</p>
            <p>Language: Python</p>
            <p>Stars: 64,098</p>
            <p>Forks: 7,426</p>
            <p>Stars today: 63 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mem0ai/mem0]]></title>
            <link>https://github.com/mem0ai/mem0</link>
            <guid>https://github.com/mem0ai/mem0</guid>
            <pubDate>Thu, 01 May 2025 00:04:31 GMT</pubDate>
            <description><![CDATA[Memory for AI Agents; SOTA in AI Agent Memory, beating OpenAI Memory in accuracy by 26% - https://mem0.ai/research]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mem0ai/mem0">mem0ai/mem0</a></h1>
            <p>Memory for AI Agents; SOTA in AI Agent Memory, beating OpenAI Memory in accuracy by 26% - https://mem0.ai/research</p>
            <p>Language: Python</p>
            <p>Stars: 28,264</p>
            <p>Forks: 2,697</p>
            <p>Stars today: 223 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/mem0ai/mem0&quot;&gt;
    &lt;img src=&quot;docs/images/banner-sm.png&quot; width=&quot;800px&quot; alt=&quot;Mem0 - The Memory Layer for Personalized AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot; style=&quot;display: flex; justify-content: center; gap: 20px; align-items: center;&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/11194&quot; target=&quot;blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/11194&quot; alt=&quot;mem0ai%2Fmem0 | Trendshift&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://mem0.ai&quot;&gt;Learn more&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://mem0.dev/DiG&quot;&gt;Join Discord&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://mem0.dev/demo&quot;&gt;Demo&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://mem0.dev/DiG&quot;&gt;
    &lt;img src=&quot;https://dcbadge.vercel.app/api/server/6PzXDgEjG5?style=flat&quot; alt=&quot;Mem0 Discord&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://pepy.tech/project/mem0ai&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/dm/mem0ai&quot; alt=&quot;Mem0 PyPI - Downloads&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/mem0ai/mem0&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/commit-activity/m/mem0ai/mem0?style=flat-square&quot; alt=&quot;GitHub commit activity&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/mem0ai&quot; target=&quot;blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/mem0ai?color=%2334D058&amp;label=pypi%20package&quot; alt=&quot;Package version&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.npmjs.com/package/mem0ai&quot; target=&quot;blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/npm/v/mem0ai&quot; alt=&quot;Npm package&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.ycombinator.com/companies/mem0&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Y%20Combinator-S24-orange?style=flat-square&quot; alt=&quot;Y Combinator S24&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://mem0.ai/research&quot;&gt;&lt;strong&gt;üìÑ Building Production-Ready AI Agents with Scalable Long-Term Memory ‚Üí&lt;/strong&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;‚ö° +26% Accuracy vs. OpenAI Memory ‚Ä¢ üöÄ 91% Faster ‚Ä¢ üí∞ 90% Fewer Tokens&lt;/strong&gt;
&lt;/p&gt;

##  üî• Research Highlights
- **+26% Accuracy** over OpenAI Memory on the LOCOMO benchmark
- **91% Faster Responses** than full-context, ensuring low-latency at scale
- **90% Lower Token Usage** than full-context, cutting costs without compromise
- [Read the full paper](https://mem0.ai/research)

# Introduction

[Mem0](https://mem0.ai) (&quot;mem-zero&quot;) enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions. It remembers user preferences, adapts to individual needs, and continuously learns over time‚Äîideal for customer support chatbots, AI assistants, and autonomous systems.

### Key Features &amp; Use Cases

**Core Capabilities:**
- **Multi-Level Memory**: Seamlessly retains User, Session, and Agent state with adaptive personalization
- **Developer-Friendly**: Intuitive API, cross-platform SDKs, and a fully managed service option

**Applications:**
- **AI Assistants**: Consistent, context-rich conversations
- **Customer Support**: Recall past tickets and user history for tailored help
- **Healthcare**: Track patient preferences and history for personalized care
- **Productivity &amp; Gaming**: Adaptive workflows and environments based on user behavior

## üöÄ Quickstart Guide &lt;a name=&quot;quickstart&quot;&gt;&lt;/a&gt;

Choose between our hosted platform or self-hosted package:

### Hosted Platform

Get up and running in minutes with automatic updates, analytics, and enterprise security.

1. Sign up on [Mem0 Platform](https://app.mem0.ai)
2. Embed the memory layer via SDK or API keys

### Self-Hosted (Open Source)

Install the sdk via pip:

```bash
pip install mem0ai
```

Install sdk via npm:
```bash
npm install mem0ai
```

### Basic Usage

Mem0 requires an LLM to function, with `gpt-4o-mini` from OpenAI as the default. However, it supports a variety of LLMs; for details, refer to our [Supported LLMs documentation](https://docs.mem0.ai/llms).

First step is to instantiate the memory:

```python
from openai import OpenAI
from mem0 import Memory

openai_client = OpenAI()
memory = Memory()

def chat_with_memories(message: str, user_id: str = &quot;default_user&quot;) -&gt; str:
    # Retrieve relevant memories
    relevant_memories = memory.search(query=message, user_id=user_id, limit=3)
    memories_str = &quot;\n&quot;.join(f&quot;- {entry[&#039;memory&#039;]}&quot; for entry in relevant_memories[&quot;results&quot;])

    # Generate Assistant response
    system_prompt = f&quot;You are a helpful AI. Answer the question based on query and memories.\nUser Memories:\n{memories_str}&quot;
    messages = [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_prompt}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: message}]
    response = openai_client.chat.completions.create(model=&quot;gpt-4o-mini&quot;, messages=messages)
    assistant_response = response.choices[0].message.content

    # Create new memories from the conversation
    messages.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: assistant_response})
    memory.add(messages, user_id=user_id)

    return assistant_response

def main():
    print(&quot;Chat with AI (type &#039;exit&#039; to quit)&quot;)
    while True:
        user_input = input(&quot;You: &quot;).strip()
        if user_input.lower() == &#039;exit&#039;:
            print(&quot;Goodbye!&quot;)
            break
        print(f&quot;AI: {chat_with_memories(user_input)}&quot;)

if __name__ == &quot;__main__&quot;:
    main()
```

For detailed integration steps, see the [Quickstart](https://docs.mem0.ai/quickstart) and [API Reference](https://docs.mem0.ai).

## üîó Integrations &amp; Demos

- **ChatGPT with Memory**: Personalized chat powered by Mem0 ([Live Demo](https://mem0.dev/demo))
- **Browser Extension**: Store memories across ChatGPT, Perplexity, and Claude ([Chrome Extension](https://chromewebstore.google.com/detail/mem0))
- **Langgraph Support**: Build a customer bot with Langgraph + Mem0 ([Guide](https://docs.mem0.ai/integrations/langgraph))
- **CrewAI Integration**: Tailor CrewAI outputs with Mem0 ([Example](https://docs.mem0.ai/integrations/crewai))

## üìö Documentation &amp; Support

- Full docs: https://docs.mem0.ai
- Community: [Discord](https://mem0.dev/DiG) ¬∑ [Twitter](https://x.com/mem0ai)
- Contact: founders@mem0.ai

## ‚öñÔ∏è License

Apache 2.0 ‚Äî see the [LICENSE](LICENSE) file for details.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[myhhub/stock]]></title>
            <link>https://github.com/myhhub/stock</link>
            <guid>https://github.com/myhhub/stock</guid>
            <pubDate>Thu, 01 May 2025 00:04:30 GMT</pubDate>
            <description><![CDATA[stockËÇ°Á•®.Ëé∑ÂèñËÇ°Á•®Êï∞ÊçÆ,ËÆ°ÁÆóËÇ°Á•®ÊåáÊ†á,Á≠πÁ†ÅÂàÜÂ∏É,ËØÜÂà´ËÇ°Á•®ÂΩ¢ÊÄÅ,ÁªºÂêàÈÄâËÇ°,ÈÄâËÇ°Á≠ñÁï•,ËÇ°Á•®È™åËØÅÂõûÊµã,ËÇ°Á•®Ëá™Âä®‰∫§Êòì,ÊîØÊåÅPCÂèäÁßªÂä®ËÆæÂ§á„ÄÇ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/myhhub/stock">myhhub/stock</a></h1>
            <p>stockËÇ°Á•®.Ëé∑ÂèñËÇ°Á•®Êï∞ÊçÆ,ËÆ°ÁÆóËÇ°Á•®ÊåáÊ†á,Á≠πÁ†ÅÂàÜÂ∏É,ËØÜÂà´ËÇ°Á•®ÂΩ¢ÊÄÅ,ÁªºÂêàÈÄâËÇ°,ÈÄâËÇ°Á≠ñÁï•,ËÇ°Á•®È™åËØÅÂõûÊµã,ËÇ°Á•®Ëá™Âä®‰∫§Êòì,ÊîØÊåÅPCÂèäÁßªÂä®ËÆæÂ§á„ÄÇ</p>
            <p>Language: Python</p>
            <p>Stars: 8,457</p>
            <p>Forks: 1,673</p>
            <p>Stars today: 58 stars today</p>
            <h2>README</h2><pre>**InStockËÇ°Á•®Á≥ªÁªü**

InStockËÇ°Á•®Á≥ªÁªüÔºåÊäìÂèñÊØèÊó•ËÇ°Á•®„ÄÅETFÂÖ≥ÈîÆÊï∞ÊçÆÔºåËÆ°ÁÆóËÇ°Á•®ÊäÄÊúØÊåáÊ†á„ÄÅÁ≠πÁ†ÅÂàÜÂ∏ÉÔºåËØÜÂà´KÁ∫øÂêÑÁßçÂΩ¢ÊÄÅÔºåÁªºÂêàÈÄâËÇ°ÔºåÂÜÖÁΩÆÂ§öÁßçÈÄâËÇ°Á≠ñÁï•ÔºåÊîØÊåÅÈÄâËÇ°È™åËØÅÂõûÊµãÔºåÊîØÊåÅËá™Âä®‰∫§ÊòìÔºåÊîØÊåÅÊâπÈáèÊó∂Èó¥ÔºåËøêË°åÈ´òÊïàÔºåÊîØÊåÅPC„ÄÅÂπ≥Êùø„ÄÅÊâãÊú∫ÁßªÂä®ËÆæÂ§áÊòæÁ§∫ÔºåÂêåÊó∂Êèê‰æõDockerÈïúÂÉèÊñπ‰æøÂÆâË£ÖÔºåÊòØÈáèÂåñÊäïËµÑÁöÑÂ•ΩÂ∏ÆÊâã„ÄÇ

The stock system,Capture key data on daily stocks and ETFs, calculate stock technical indicators, chip distribution, Position Cost Distribution(CYQ), identify various K-line forms, comprehensive stock selection, built-in multiple stock selection strategies, support stock selection verification and backtesting, support automatic trading, and support batch time , runs efficiently, supports display on PCs, tablets, and mobile phones, and provides Docker images for easy installation, making it a good helper for quantitative investment.

DockerÈïúÂÉèÔºöhttps://hub.docker.com/r/mayanghua/instock **ÈïúÂÉè‰ºòÂåñÊûÑÂª∫‰ªÖ170M**„ÄÇ

# ÂäüËÉΩ‰ªãÁªç

##  ‰∏ÄÔºöÁªºÂêàÈÄâËÇ°
ÁªºÂêàÈÄâËÇ°ÊîØÊåÅËÇ°Á•®ËåÉÂõ¥„ÄÅÂü∫Êú¨Èù¢„ÄÅÊäÄÊúØÈù¢„ÄÅÊ∂àÊÅØÈù¢„ÄÅ‰∫∫Ê∞îÊåáÊ†á„ÄÅË°åÊÉÖÊï∞ÊçÆÁ≠âÊñπÈù¢ÂÖ±200Â§ö‰∏™‰ø°ÊÅØÊ†èÁõÆËøõË°åËá™Áî±ÁªÑÂêàÈÄâËÇ°„ÄÇÈÄâËÇ°Êù°‰ª∂ÂàÜ‰∏∫‰ª•‰∏ãÂ§ßÁ±ªÔºö
```
1.ËÇ°Á•®ËåÉÂõ¥
Â∏ÇÂú∫„ÄÅ Ë°å‰∏ö„ÄÅÂú∞Âå∫„ÄÅ Ê¶ÇÂøµ„ÄÅ È£éÊ†º„ÄÅÊåáÊï∞Êàê‰ªΩ„ÄÅ ‰∏äÂ∏ÇÊó∂Èó¥„ÄÇ
2.Âü∫Êú¨Èù¢
‰º∞ÂÄºÊåáÊ†á„ÄÅÊØèËÇ°ÊåáÊ†á„ÄÅÁõàÂà©ËÉΩÂäõ„ÄÅÊàêÈïøËÉΩÂäõ„ÄÅËµÑÊú¨ÁªìÊûÑ‰∏éÂÅøÂÄ∫ËÉΩÂäõ„ÄÅËÇ°Êú¨ËÇ°‰∏ú„ÄÇ
3.ÊäÄÊúØÈù¢
MACDÈáëÂèâ„ÄÅKDJÈáëÂèâ„ÄÅÊîæÈáèÁ™ÅÁ†¥„ÄÅ‰Ωé‰ΩçËµÑÈáëÂáÄÊµÅÂÖ•„ÄÅÈ´ò‰ΩçËµÑÈáëÂáÄÊµÅÂá∫„ÄÅÂêë‰∏äÁ™ÅÁ†¥ÂùáÁ∫ø„ÄÅÂùáÁ∫øÂ§öÂ§¥ÊéíÂàó„ÄÅÂùáÁ∫øÁ©∫Â§¥ÊéíÂàó„ÄÅËøûÊ∂®ÊîæÈáè„ÄÅ‰∏ãË∑åÊó†Èáè„ÄÅ‰∏ÄÊ†πÂ§ßÈò≥Á∫ø„ÄÅ‰∏§Ê†πÂ§ßÈò≥Á∫ø„ÄÅÊó≠Êó•‰∏úÂçá„ÄÅÂº∫ÂäøÂ§öÊñπ„ÄÅÁÇÆÊã®‰∫ëËßÅÊó•„ÄÅ‰∏É‰ªôÂ•≥‰∏ãÂá°(‰∏ÉËøûÈò¥)„ÄÅÂÖ´‰ªôËøáÊµ∑(ÂÖ´ËøûÈò≥)„ÄÅ‰πùÈò≥Á•ûÂäü(‰πùËøûÈò≥)„ÄÅÂõõ‰∏≤Èò≥„ÄÅÂ§©ÈáèÊ≥ïÂàô„ÄÅÊîæÈáè‰∏äÊîª„ÄÅÁ©øÂ§¥Á†¥ËÑö„ÄÅÂÄíËΩ¨Èî§Â§¥„ÄÅÂ∞ÑÂáª‰πãÊòü„ÄÅÈªÑÊòè‰πãÊòü„ÄÅÊõôÂÖâÂàùÁé∞„ÄÅË∫´ÊÄÄÂÖ≠Áî≤„ÄÅ‰πå‰∫ëÁõñÈ°∂„ÄÅÊó©Êô®‰πãÊòü„ÄÅÁ™ÑÂπÖÊï¥ÁêÜ„ÄÇ
4.Ê∂àÊÅØÈù¢
ÂÖ¨ÂëäÂ§ß‰∫ã„ÄÅÊú∫ÊûÑÂÖ≥Ê≥®ÊÉÖÂÜµ„ÄÅÊú∫ÊûÑÊåÅËÇ°ÂÆ∂Êï∞„ÄÅÊú∫ÊûÑÊåÅËÇ°ÊØî‰æã„ÄÇ
5.‰∫∫Ê∞îÊåáÊ†á
ËÇ°Âêß‰∫∫Ê∞îÊéíÂêç„ÄÅ‰∫∫Ê∞îÊéíÂêçÂèòÂåñ„ÄÅ‰∫∫Ê∞îÊéíÂêçËøûÊ∂®„ÄÅ‰∫∫Ê∞îÊéíÂêçËøûË∑å„ÄÅ‰∫∫Ê∞îÊéíÂêçÂàõÊñ∞È´ò„ÄÅ‰∫∫Ê∞îÊéíÂêçÂàõÊñ∞‰Ωé„ÄÅÊñ∞ÊôãÁ≤â‰∏ùÂç†ÊØî„ÄÅÈìÅÊùÜÁ≤â‰∏ùÂç†ÊØî„ÄÅ7Êó•ÂÖ≥Ê≥®ÊéíÂêç„ÄÅ‰ªäÊó•ÊµèËßàÊéíÂêç„ÄÇ
6.Ë°åÊÉÖÊï∞ÊçÆ
ËÇ°‰ª∑Ë°®Áé∞„ÄÅÊàê‰∫§ÊÉÖÂÜµ„ÄÅËµÑÈáëÊµÅÂêë„ÄÅË°åÊÉÖÁªüËÆ°„ÄÅÊ≤™Ê∑±ËÇ°ÈÄö„ÄÇ
```
![](img/a3.jpg)
![](img/a1.jpg)

##  ‰∫åÔºöËÇ°Á•®ÊØèÊó•Êï∞ÊçÆ

ÂåÖÊã¨ÊØèÊó•ËÇ°Á•®Êï∞ÊçÆ„ÄÅËÇ°Á•®ËµÑÈáëÊµÅÂêë„ÄÅËÇ°Á•®ÂàÜÁ∫¢ÈÖçÈÄÅ„ÄÅËÇ°Á•®ÈæôËôéÊ¶ú„ÄÅËÇ°Á•®Â§ßÂÆó‰∫§Êòì„ÄÅËÇ°Á•®Âü∫Êú¨Èù¢Êï∞ÊçÆ„ÄÅË°å‰∏öËµÑÈáëÊµÅÂêë„ÄÅÊ¶ÇÂøµËµÑÈáëÊµÅÂêë„ÄÅÊØèÊó•ETFÊï∞ÊçÆ„ÄÇ

ÊäìÂèñAËÇ°Á•®ÊØèÊó•Êï∞ÊçÆÔºå‰∏ªË¶Å‰∏∫‰∏Ä‰∫õÂÖ≥ÈîÆÊï∞ÊçÆÔºåÂêåÊó∂Â∞ÅË£ÖÊäìÂèñÊñπÊ≥ïÔºåÊñπ‰æøÊâ©Â±ïÁ≥ªÁªüËé∑Âèñ‰∏™‰∫∫ÂÖ≥Ê≥®ÁöÑÊï∞ÊçÆ„ÄÇ

![](img/00.jpg)
![](img/12.jpg)
## ‰∏âÔºöËÇ°Á•®ÊåáÊ†áËÆ°ÁÆó
Âü∫‰∫étalib„ÄÅpandas ËÆ°ÁÆóÊåáÊ†áÔºåËÆ°ÁÆóÈ´òÊïàÂáÜÁ°Æ„ÄÇË∞ÉÊï¥‰∏™Âà´ÊåáÊ†áÂÖ¨ÂºèÔºåÁ°Æ‰øùÁªìÊûúÂíåÂêåËä±È°∫„ÄÅÈÄö‰ø°ËææÁªìÊûú‰∏ÄËá¥„ÄÇ
ÊåáÊ†áÔºö

```
1„ÄÅMACD 2„ÄÅKDJ 3„ÄÅBOLL 4„ÄÅTRIXÔºåTRMA 5„ÄÅCR 6„ÄÅSMA 7„ÄÅRSI 
8„ÄÅVRÔºåMAVR 9„ÄÅROC 10„ÄÅDMIÔºå+DIÔºå-DIÔºåDXÔºåADXÔºåADXR 11„ÄÅW&amp;R 
12„ÄÅCCI 13„ÄÅTR„ÄÅATR 14„ÄÅDMA„ÄÅAMA 15„ÄÅOBV 16„ÄÅSAR 17„ÄÅPSY 
18„ÄÅBRAR 19„ÄÅEMV 20„ÄÅBIAS 21„ÄÅTEMA  22„ÄÅMFI 23„ÄÅVWMA
24„ÄÅPPO 25„ÄÅWT 26„ÄÅSupertrend  27„ÄÅDPO  28„ÄÅVHF  29„ÄÅRVI
30„ÄÅFI 31„ÄÅENE 32„ÄÅSTOCHRSI
```

![](img/01.jpg)
![](img/06.jpg)

## ÂõõÔºöÂà§Êñ≠‰π∞ÂÖ•ÂçñÂá∫ÁöÑËÇ°Á•®

Ê†πÊçÆÊåáÊ†áÂà§ÂÆöÂèØËÉΩ‰π∞ÂÖ•ÂçñÂá∫ÁöÑËÇ°Á•®ÔºåÂÖ∑‰ΩìÁ≠õÈÄâÊù°‰ª∂Â¶Ç‰∏ãÔºö


```
KDJ:
1„ÄÅË∂Ö‰π∞Âå∫ÔºöKÂÄºÂú®80‰ª•‰∏äÔºåDÂÄºÂú®70‰ª•‰∏äÔºåJÂÄºÂ§ß‰∫é90Êó∂‰∏∫Ë∂Ö‰π∞„ÄÇ‰∏ÄËà¨ÊÉÖÂÜµ‰∏ãÔºåËÇ°‰ª∑ÊúâÂèØËÉΩ‰∏ãË∑å„ÄÇÊäïËµÑËÄÖÂ∫îË∞®ÊÖéË°å‰∫ãÔºåÂ±ÄÂ§ñ‰∫∫‰∏çÂ∫îÂÜçËøΩÊ∂®ÔºåÂ±ÄÂÜÖ‰∫∫Â∫îÈÄÇÊó∂ÂçñÂá∫„ÄÇ
2„ÄÅË∂ÖÂçñÂå∫ÔºöKÂÄºÂú®20‰ª•‰∏ãÔºåDÂÄºÂú®30‰ª•‰∏ã‰∏∫Ë∂ÖÂçñÂå∫„ÄÇ‰∏ÄËà¨ÊÉÖÂÜµ‰∏ãÔºåËÇ°‰ª∑ÊúâÂèØËÉΩ‰∏äÊ∂®ÔºåÂèçÂºπÁöÑÂèØËÉΩÊÄßÂ¢ûÂ§ß„ÄÇÂ±ÄÂÜÖ‰∫∫‰∏çÂ∫îËΩªÊòìÊäõÂá∫ËÇ°Á•®ÔºåÂ±ÄÂ§ñ‰∫∫ÂèØÂØªÊú∫ÂÖ•Âú∫„ÄÇ
RSI:
1„ÄÅÂΩìÂÖ≠Êó•ÊåáÊ†á‰∏äÂçáÂà∞Ëææ80Êó∂ÔºåË°®Á§∫ËÇ°Â∏ÇÂ∑≤ÊúâË∂Ö‰π∞Áé∞Ë±°ÔºåÂ¶ÇÊûú‰∏ÄÊó¶ÁªßÁª≠‰∏äÂçáÔºåË∂ÖËøá90‰ª•‰∏äÊó∂ÔºåÂàôË°®Á§∫Â∑≤Âà∞‰∏•ÈáçË∂Ö‰π∞ÁöÑË≠¶ÊàíÂå∫ÔºåËÇ°‰ª∑Â∑≤ÂΩ¢ÊàêÂ§¥ÈÉ®ÔºåÊûÅÂèØËÉΩÂú®Áü≠ÊúüÂÜÖÂèçËΩ¨ÂõûËΩ¨„ÄÇ
2„ÄÅÂΩìÂÖ≠Êó•Âº∫Âº±ÊåáÊ†á‰∏ãÈôçËá≥20Êó∂ÔºåË°®Á§∫ËÇ°Â∏ÇÊúâË∂ÖÂçñÁé∞Ë±°ÔºåÂ¶ÇÊûú‰∏ÄÊó¶ÁªßÁª≠‰∏ãÈôçËá≥10‰ª•‰∏ãÊó∂ÂàôË°®Á§∫Â∑≤Âà∞‰∏•ÈáçË∂ÖÂçñÂå∫ÂüüÔºåËÇ°‰ª∑ÊûÅÂèØËÉΩÊúâÊ≠¢Ë∑åÂõûÂçáÁöÑÊú∫‰ºö„ÄÇ
CCI:
1„ÄÅÂΩìCCIÔºûÔπ¢100Êó∂ÔºåË°®ÊòéËÇ°‰ª∑Â∑≤ÁªèËøõÂÖ•ÈùûÂ∏∏ÊÄÅÂå∫Èó¥‚Äî‚ÄîË∂Ö‰π∞Âå∫Èó¥ÔºåËÇ°‰ª∑ÁöÑÂºÇÂä®Áé∞Ë±°Â∫îÂ§öÂä†ÂÖ≥Ê≥®„ÄÇ
2„ÄÅÂΩìCCIÔºúÔπ£100Êó∂ÔºåË°®ÊòéËÇ°‰ª∑Â∑≤ÁªèËøõÂÖ•Âè¶‰∏Ä‰∏™ÈùûÂ∏∏ÊÄÅÂå∫Èó¥‚Äî‚ÄîË∂ÖÂçñÂå∫Èó¥ÔºåÊäïËµÑËÄÖÂèØ‰ª•ÈÄ¢‰ΩéÂê∏Á∫≥ËÇ°Á•®„ÄÇ
CR:
1„ÄÅË∑åÁ©øa„ÄÅb„ÄÅc„ÄÅdÂõõÊù°Á∫øÔºåÂÜçÁî±‰ΩéÁÇπÂêë‰∏äÁà¨Âçá160Êó∂Ôºå‰∏∫Áü≠Á∫øËé∑Âà©ÁöÑ‰∏Ä‰∏™ËâØÊú∫ÔºåÂ∫îÈÄÇÂΩìÂçñÂá∫ËÇ°Á•®„ÄÇ
2„ÄÅCRË∑åËá≥40‰ª•‰∏ãÊó∂ÔºåÊòØÂª∫‰ªìËâØÊú∫„ÄÇ
WR:
1„ÄÅÂΩìÔºÖRÁ∫øËææÂà∞20Êó∂ÔºåÂ∏ÇÂú∫Â§Ñ‰∫éË∂Ö‰π∞Áä∂ÂÜµÔºåËµ∞ÂäøÂèØËÉΩÂç≥Â∞ÜËßÅÈ°∂„ÄÇ
2„ÄÅÂΩìÔºÖRÁ∫øËææÂà∞80Êó∂ÔºåÂ∏ÇÂú∫Â§Ñ‰∫éË∂ÖÂçñÁä∂ÂÜµÔºåËÇ°‰ª∑Ëµ∞ÂäøÈöèÊó∂ÂèØËÉΩËßÅÂ∫ï„ÄÇ
VR:
1„ÄÅËé∑Âà©Âå∫Âüü160Ôºç450Ê†πÊçÆÊÉÖÂÜµËé∑Âà©‰∫ÜÁªì„ÄÇ
2„ÄÅ‰Ωé‰ª∑Âå∫Âüü40Ôºç70ÂèØ‰ª•‰π∞Ëøõ„ÄÇ
```

![](img/05.jpg)

## ‰∫îÔºöKÁ∫øÂΩ¢ÊÄÅËØÜÂà´

Á≤æÂáÜËØÜÂà´61ÁßçKÁ∫øÂΩ¢ÊÄÅÔºåÊîØÊåÅÁî®Êà∑Ëá™ÈÄâÂΩ¢ÊÄÅËØÜÂà´„ÄÇ

ËØÜÂà´ÂΩ¢ÊÄÅ:

```
1„ÄÅ‰∏§Âè™‰πåÈ∏¶2„ÄÅ‰∏âÂè™‰πåÈ∏¶3„ÄÅ‰∏âÂÜÖÈÉ®‰∏äÊ∂®Âíå‰∏ãË∑å4„ÄÅ‰∏âÁ∫øÊâìÂáª5„ÄÅ‰∏âÂ§ñÈÉ®‰∏äÊ∂®Âíå‰∏ãË∑å6„ÄÅÂçóÊñπ‰∏âÊòü7„ÄÅ‰∏â‰∏™ÁôΩÂÖµ8„ÄÅÂºÉÂ©¥
9„ÄÅÂ§ßÊïåÂΩìÂâç10„ÄÅÊçâËÖ∞Â∏¶Á∫ø11„ÄÅËÑ±Á¶ª12„ÄÅÊî∂ÁõòÁº∫ÂΩ±Á∫ø13„ÄÅËóèÂ©¥ÂêûÊ≤°14„ÄÅÂèçÂáªÁ∫ø15„ÄÅ‰πå‰∫ëÂéãÈ°∂16„ÄÅÂçÅÂ≠ó17„ÄÅÂçÅÂ≠óÊòü
18„ÄÅËúªËúìÂçÅÂ≠ó/TÂΩ¢ÂçÅÂ≠ó19„ÄÅÂêûÂô¨Ê®°Âºè20„ÄÅÂçÅÂ≠óÊöÆÊòü  21„ÄÅÊöÆÊòü22„ÄÅÂêë‰∏ä/‰∏ãË∑≥Á©∫Âπ∂ÂàóÈò≥Á∫ø23„ÄÅÂ¢ìÁ¢ëÂçÅÂ≠ó/ÂÄíTÂçÅÂ≠ó
24„ÄÅÈî§Â§¥25„ÄÅ‰∏äÂêäÁ∫ø26„ÄÅÊØçÂ≠êÁ∫ø27„ÄÅÂçÅÂ≠óÂ≠ïÁ∫ø28„ÄÅÈ£éÈ´òÊµ™Â§ßÁ∫ø29„ÄÅÈô∑Èò±30„ÄÅ‰øÆÊ≠£Èô∑Èò±31„ÄÅÂÆ∂È∏Ω32„ÄÅ‰∏âËÉûËÉé‰πåÈ∏¶
33„ÄÅÈ¢àÂÜÖÁ∫ø34„ÄÅÂÄíÈî§Â§¥35„ÄÅÂèçÂÜ≤ÂΩ¢ÊÄÅ36„ÄÅÁî±ËæÉÈïøÁº∫ÂΩ±Á∫øÂÜ≥ÂÆöÁöÑÂèçÂÜ≤ÂΩ¢ÊÄÅ37„ÄÅÊ¢ØÂ∫ï38„ÄÅÈïøËÑöÂçÅÂ≠ó39„ÄÅÈïøËú°ÁÉõ
40„ÄÅÂÖâÂ§¥ÂÖâËÑö/Áº∫ÂΩ±Á∫ø 41„ÄÅÁõ∏Âêå‰Ωé‰ª∑42„ÄÅÈì∫Âû´43„ÄÅÂçÅÂ≠óÊô®Êòü44„ÄÅÊô®Êòü45„ÄÅÈ¢à‰∏äÁ∫ø46„ÄÅÂà∫ÈÄèÂΩ¢ÊÄÅ47„ÄÅÈªÑÂåÖËΩ¶Â§´
48„ÄÅ‰∏äÂçá/‰∏ãÈôç‰∏âÊ≥ï49„ÄÅÂàÜÁ¶ªÁ∫ø50„ÄÅÂ∞ÑÂáª‰πãÊòü51„ÄÅÁü≠Ëú°ÁÉõ52„ÄÅÁ∫∫Èî§53„ÄÅÂÅúÈ°øÂΩ¢ÊÄÅ54„ÄÅÊù°ÂΩ¢‰∏âÊòéÊ≤ª55„ÄÅÊé¢Ê∞¥Á´ø
56„ÄÅË∑≥Á©∫Âπ∂ÂàóÈò¥Èò≥Á∫ø57„ÄÅÊèíÂÖ•58„ÄÅ‰∏âÊòü59„ÄÅÂ•áÁâπ‰∏âÊ≤≥Â∫ä60„ÄÅÂêë‰∏äË∑≥Á©∫ÁöÑ‰∏§Âè™‰πåÈ∏¶61„ÄÅ‰∏äÂçá/‰∏ãÈôçË∑≥Á©∫‰∏âÊ≥ï 
```
ÂΩ¢ÊÄÅËØÜÂà´ÁªìÊûúÔºö
```
Ë¥üÔºöÂá∫Áé∞ÂçñÂá∫‰ø°Âè∑
0ÔºöÊ≤°ÊúâÂá∫Áé∞ËØ•ÂΩ¢ÊÄÅ
Ê≠£ÔºöÂá∫Áé∞‰π∞ÂÖ•‰ø°Âè∑
```
![](img/09.jpg)
![](img/13.jpg)

## ÂÖ≠ÔºöÁ≠πÁ†ÅÂàÜÂ∏É

Á≠πÁ†ÅÂàÜÂ∏ÉÈÄöËøáËÆ°ÁÆó‰∏ÄÂÆöÊó∂Èó¥ËåÉÂõ¥ÂÜÖËÇ°Á•®ÁöÑ:ÊúÄÈ´ò‰ª∑„ÄÅÊúÄ‰Ωé‰ª∑„ÄÅÊàê‰∫§Êï∞ÔºåËæìÂá∫ÂØπÂ∫î‰ª∑Ê†ºÊàê‰∫§Êï∞Âç†Êï¥‰∏™ÊµÅÈÄöÁõòÊØîÂÄºÁöÑÂàÜÂ∏ÉÂõæÂΩ¢„ÄÇËÆ°ÁÆóÈ´òÊïàÂáÜÁ°ÆÔºåÁªìÊûú‰∏é‰∏úÊñπË¥¢ÂØåÁ≠â‰∏ì‰∏öËΩØ‰ª∂ÁöÑ‰∏ÄËá¥ÔºåÁº∫ÁúÅËÆ°ÁÆó210‰∏™‰∫§ÊòìÊó•ÁöÑÊàêÊú¨ÔºåÂèØ‰ª•Ëá™Ë°åËÆæÂÆöÊó∂Èó¥ËåÉÂõ¥„ÄÇ
![](img/06.jpg)

## ‰∏ÉÔºöÁ≠ñÁï•ÈÄâËÇ°

ÂÜÖÁΩÆÊîæÈáè‰∏äÊ∂®„ÄÅÂÅúÊú∫Âù™„ÄÅÂõûË∏©Âπ¥Á∫ø„ÄÅÁ™ÅÁ†¥Âπ≥Âè∞„ÄÅÊîæÈáèË∑åÂÅúÁ≠âÂ§öÁßçÈÄâËÇ°Á≠ñÁï•ÔºåÂêåÊó∂Â∞ÅË£Ö‰∫ÜÁ≠ñÁï•Ê®°ÊùøÔºåÊñπ‰æøÊâ©Â±ïÂÆûÁé∞Ëá™Â∑±ÁöÑÁ≠ñÁï•„ÄÇ


```
1„ÄÅÊîæÈáè‰∏äÊ∂®
    1ÔºâÂΩìÊó•ÊØîÂâç‰∏ÄÂ§©‰∏äÊ∂®Â∞è‰∫é2%ÊàñÊî∂Áõò‰ª∑Â∞è‰∫éÂºÄÁõò‰ª∑„ÄÇ
    2ÔºâÂΩìÊó•Êàê‰∫§È¢ù‰∏ç‰Ωé‰∫é2‰∫ø„ÄÇ
    3ÔºâÂΩìÊó•Êàê‰∫§Èáè/5Êó•Âπ≥ÂùáÊàê‰∫§Èáè&gt;=2„ÄÇ
2„ÄÅÂùáÁ∫øÂ§öÂ§¥
    MA30Âêë‰∏ä
    1Ôºâ30Êó•ÂâçÁöÑ30Êó•ÂùáÁ∫ø&lt;20Êó•ÂâçÁöÑ30Êó•ÂùáÁ∫ø&lt;10Êó•ÂâçÁöÑ30Êó•ÂùáÁ∫ø&lt;ÂΩìÊó•ÁöÑ30Êó•ÂùáÁ∫ø„ÄÇ
    2Ôºâ(ÂΩìÊó•ÁöÑ30Êó•ÂùáÁ∫ø/30Êó•ÂâçÁöÑ30Êó•ÂùáÁ∫ø)&gt;1.2„ÄÇ
3„ÄÅÂÅúÊú∫Âù™
    1ÔºâÊúÄËøë15Êó•ÊúâÊ∂®ÂπÖÂ§ß‰∫é9.5%Ôºå‰∏îÂøÖÈ°ªÊòØÊîæÈáè‰∏äÊ∂®„ÄÇ
    2ÔºâÁ¥ßÊé•ÁöÑ‰∏ã‰∏™‰∫§ÊòìÊó•ÂøÖÈ°ªÈ´òÂºÄÔºåÊî∂Áõò‰ª∑ÂøÖÈ°ª‰∏äÊ∂®Ôºå‰∏î‰∏éÂºÄÁõò‰ª∑‰∏çËÉΩÂ§ß‰∫éÁ≠â‰∫éÁõ∏Â∑Æ3%„ÄÇ
    3ÔºâÊé•‰∏ã2„ÄÅ3‰∏™‰∫§ÊòìÊó•ÂøÖÈ°ªÈ´òÂºÄÔºåÊî∂Áõò‰ª∑ÂøÖÈ°ª‰∏äÊ∂®Ôºå‰∏î‰∏éÂºÄÁõò‰ª∑‰∏çËÉΩÂ§ß‰∫éÁ≠â‰∫éÁõ∏Â∑Æ3%Ôºå‰∏îÊØèÂ§©Ê∂®Ë∑åÂπÖÂú®5%Èó¥„ÄÇ
4„ÄÅÂõûË∏©Âπ¥Á∫ø
    1ÔºâÂàÜ2‰∏™Êó∂Èó¥ÊÆµÔºöÂâçÊÆµ=ÊúÄËøë60‰∫§ÊòìÊó•ÊúÄÈ´òÊî∂Áõò‰ª∑‰πãÂâç‰∫§ÊòìÊó•(ÈïøÂ∫¶&gt;0)ÔºåÂêéÊÆµ=ÊúÄÈ´ò‰ª∑ÂΩìÊó•ÂèäÂêéÈù¢ÁöÑ‰∫§ÊòìÊó•„ÄÇ
    2ÔºâÂâçÊÆµÁî±Âπ¥Á∫ø(250Êó•)‰ª•‰∏ãÂêë‰∏äÁ™ÅÁ†¥„ÄÇ
    3ÔºâÂêéÊÆµÂøÖÈ°ªÂú®Âπ¥Á∫ø‰ª•‰∏äËøêË°åÔºå‰∏îÂêéÊÆµÊúÄ‰Ωé‰ª∑Êó•‰∏éÊúÄÈ´ò‰ª∑Êó•Áõ∏Â∑ÆÂøÖÈ°ªÂú®10-50Êó•Èó¥„ÄÇ
    4ÔºâÂõûË∏©‰º¥ÈöèÁº©ÈáèÔºöÊúÄÈ´ò‰ª∑Êó•‰∫§ÊòìÈáè/ÂêéÊÆµÊúÄ‰Ωé‰ª∑Êó•‰∫§ÊòìÈáè&gt;2,ÂêéÊÆµÊúÄ‰Ωé‰ª∑/ÊúÄÈ´ò‰ª∑&lt;0.8„ÄÇ
5„ÄÅÁ™ÅÁ†¥Âπ≥Âè∞
    1Ôºâ60Êó•ÂÜÖÊüêÊó•Êî∂Áõò‰ª∑&gt;=60Êó•ÂùáÁ∫ø&gt;ÂºÄÁõò‰ª∑„ÄÇ
    2Ôºâ‰∏î„Äê1„ÄëÊîæÈáè‰∏äÊ∂®„ÄÇ
    3Ôºâ‰∏î„Äê1„ÄëÈó¥‰πãÂâçÊó∂Èó¥Ôºå‰ªªÊÑè‰∏ÄÂ§©Êî∂Áõò‰ª∑‰∏é60Êó•ÂùáÁ∫øÂÅèÁ¶ªÂú®-5%~20%‰πãÈó¥„ÄÇ
6„ÄÅÊó†Â§ßÂπÖÂõûÊí§
    1ÔºâÂΩìÊó•Êî∂Áõò‰ª∑ÊØî60Êó•ÂâçÁöÑÊî∂Áõò‰ª∑ÁöÑÊ∂®ÂπÖÂ∞è‰∫é0.6„ÄÇ
    2ÔºâÊúÄËøë60Êó•Ôºå‰∏çËÉΩÊúâÂçïÊó•Ë∑åÂπÖË∂Ö7%„ÄÅÈ´òÂºÄ‰ΩéËµ∞7%„ÄÅ‰∏§Êó•Á¥ØËÆ°Ë∑åÂπÖ10%„ÄÅ‰∏§Êó•È´òÂºÄ‰ΩéËµ∞Á¥ØËÆ°10%„ÄÇ
7„ÄÅÊµ∑Èæü‰∫§ÊòìÊ≥ïÂàô
    ÊúÄÂêé‰∏Ä‰∏™‰∫§ÊòìÊó•Êî∂Â∏Ç‰ª∑‰∏∫ÊåáÂÆöÂå∫Èó¥ÂÜÖÊúÄÈ´ò‰ª∑„ÄÇ
    1ÔºâÂΩìÊó•Êî∂Áõò‰ª∑&gt;=ÊúÄËøë60Êó•ÊúÄÈ´òÊî∂Áõò‰ª∑„ÄÇ
8„ÄÅÈ´òËÄåÁ™ÑÁöÑÊóóÂΩ¢
    1ÔºâÂøÖÈ°ªËá≥Â∞ë‰∏äÂ∏Ç‰∫§Êòì60Êó•„ÄÇ
    2ÔºâÂΩìÊó•Êî∂Áõò‰ª∑/‰πãÂâç24~10Êó•ÁöÑÊúÄ‰Ωé‰ª∑&gt;=1.9„ÄÇ
    3Ôºâ‰πãÂâç24~10Êó•ÂøÖÈ°ªËøûÁª≠‰∏§Â§©Ê∂®ÂπÖÂ§ß‰∫éÁ≠â‰∫é9.5%„ÄÇ
9„ÄÅÊîæÈáèË∑åÂÅú„ÄÇ
    1ÔºâË∑å&gt;9.5%„ÄÇ
    2ÔºâÊàê‰∫§È¢ù‰∏ç‰Ωé‰∫é2‰∫ø„ÄÇ
    3ÔºâÊàê‰∫§ÈáèËá≥Â∞ëÊòØ5Êó•Âπ≥ÂùáÊàê‰∫§ÈáèÁöÑ4ÂÄç„ÄÇ
10„ÄÅ‰ΩéATRÊàêÈïø
    1ÔºâÂøÖÈ°ªËá≥Â∞ë‰∏äÂ∏Ç‰∫§Êòì250Êó•„ÄÇ
    2ÔºâÊúÄËøë10‰∏™‰∫§ÊòìÊó•ÁöÑÊúÄÈ´òÊî∂Áõò‰ª∑ÂøÖÈ°ªÊØîÊúÄËøë10‰∏™‰∫§ÊòìÊó•ÁöÑÊúÄ‰ΩéÊî∂Áõò‰ª∑È´ò1.1ÂÄç„ÄÇ
11„ÄÅËÇ°Á•®Âü∫Êú¨Èù¢ÈÄâËÇ°
    1ÔºâÂ∏ÇÁõàÁéáÂ∞è‰∫éÁ≠â‰∫é20Ôºå‰∏îÂ§ß‰∫é0„ÄÇ
    2ÔºâÂ∏ÇÂáÄÁéáÂ∞è‰∫éÁ≠â‰∫é10„ÄÇ
    3ÔºâÂáÄËµÑ‰∫ßÊî∂ÁõäÁéáÂ§ß‰∫éÁ≠â‰∫é15„ÄÇ
```

![](img/04.jpg)

## ÂÖ´ÔºöÈÄâËÇ°È™åËØÅ


ÂØπÊåáÊ†á„ÄÅÁ≠ñÁï•Á≠âÈÄâÂá∫ÁöÑËÇ°Á•®ËøõË°åÂõûÊµãÔºåÈ™åËØÅÁ≠ñÁï•ÁöÑÊàêÂäüÁéáÔºåÊòØÂê¶ÂèØÁî®„ÄÇ


![](img/05.jpg)

## ‰πùÔºöËá™Âä®‰∫§Êòì

ÊîØÊåÅËá™Âä®‰∫§ÊòìÔºåÂÜÖÁΩÆËá™Âä®ÊâìÊñ∞ËÇ°ÁöÑÁ≠ñÁï•ÂèäÁ§∫‰æãÁ≠ñÁï•ÔºåÁî±‰∫é**Ê∂âÂèäÈáëÈí±**ÔºåËßÑÈÅøÂèØËÉΩÂ≠òÂú®È£éÈô©ÔºåÊ≤°ÊúâÊèê‰æõÂÖ∂‰ªñ‰∫§ÊòìÁ≠ñÁï•„ÄÇ

ÂÖ∑Êúâ‰∫§ÊòìÊó•ÂøóÔºå‰ª•ÂèäÊîØÊåÅ‰∏∫ÊØè‰∏™‰∫§ÊòìÁ≠ñÁï•ÈÖçÁΩÆ‰∫§ÊòìÊó•Âøó„ÄÇ

**ÁâπÂà´ÊèêÈÜí**Ôºö‰∫§ÊòìÊó•10:00ÁÇπ‰ºöËß¶ÂèëÊâìÊñ∞Ôºå‰∏çÊÉ≥ÊâìÊñ∞ÁöÑÂà†Èô§stagging.pyÊàñ‰∏çË¶ÅÂêØÂä®‚Äú‰∫§ÊòìÊúçÂä°‚Äù„ÄÇ

![](img/11.jpg)

## ÂçÅÔºöÂÖ≥Ê≥®ÂäüËÉΩ

ÊîØÊåÅËÇ°Á•®ÂÖ≥Ê≥®ÔºåÂÖ≥Ê≥®ËÇ°Á•®Âú®ÂêÑ‰∏™Ê®°Âùó(Âê´ÊúâÁöÑ)ÁΩÆÈ°∂„ÄÅÊ†áÁ∫¢ÊòæÁ§∫„ÄÇ

## ÂçÅ‰∏ÄÔºöÊîØÊåÅÊâπÈáè


ÂèØ‰ª•ÈÄöËøáÊó∂Èó¥ÊÆµ„ÄÅÊûö‰∏æÊó∂Èó¥„ÄÅÂΩìÂâçÊó∂Èó¥ËøõË°åÊåáÊ†áËÆ°ÁÆó„ÄÅÁ≠ñÁï•ÈÄâËÇ°ÂèäÂõûÊµãÁ≠â„ÄÇÂêåÊó∂ÊîØÊåÅÊô∫ËÉΩËØÜÂà´‰∫§ÊòìÊó•ÔºåÂèØ‰ª•ËæìÂÖ•‰ªªÊÑèÊó•Êúü„ÄÇ

ÂÖ∑‰ΩìÊâßË°åËÆæÁΩÆÂ¶Ç‰∏ãÔºö
```
------Êï¥‰Ωì‰Ωú‰∏öÔºåÊîØÊåÅÊâπÈáè‰Ωú‰∏ö------
ÂΩìÂâçÊó∂Èó¥‰Ωú‰∏ö python execute_daily_job.py
Âçï‰∏™Êó∂Èó¥‰Ωú‰∏ö python execute_daily_job.py 2022-03-01
Êûö‰∏æÊó∂Èó¥‰Ωú‰∏ö python execute_daily_job.py 2022-01-01,2021-02-08,2022-03-12
Âå∫Èó¥Êó∂Èó¥‰Ωú‰∏ö python execute_daily_job.py 2022-01-01 2022-03-01

------ÂçïÂäüËÉΩ‰Ωú‰∏öÔºåÊîØÊåÅÊâπÈáè‰Ωú‰∏öÔºåÂõûÊµãÊï∞ÊçÆËá™Âä®Â°´Ë°•Âà∞ÂΩìÂâç
Âü∫Á°ÄÊï∞ÊçÆÂÆûÊó∂‰Ωú‰∏ö python basic_data_daily_job.py
Âü∫Á°ÄÊï∞ÊçÆÈùûÂÆûÊó∂‰Ωú‰∏ö python basic_data_other_daily_job.py
ÊåáÊ†áÊï∞ÊçÆ‰Ωú‰∏ö python indicators_data_daily_job.py
KÁ∫øÂΩ¢ÊÄÅ‰Ωú‰∏ö klinepattern_data_daily_job.py
Á≠ñÁï•Êï∞ÊçÆ‰Ωú‰∏ö python strategy_data_daily_job.py
ÂõûÊµãÊï∞ÊçÆ python backtest_data_daily_job.py
```

## ÂçÅ‰∫åÔºöÂ≠òÂÇ®ÈááÁî®Êï∞ÊçÆÂ∫ìËÆæËÆ°

Êï∞ÊçÆÂ≠òÂÇ®ÈááÁî®Êï∞ÊçÆÂ∫ìËÆæËÆ°ÔºåËÉΩ‰øùÂ≠òÂéÜÂè≤Êï∞ÊçÆÔºå‰ª•ÂèäÂØπÊï∞ÊçÆËøõË°åÊâ©Â±ïÂàÜÊûê„ÄÅÁªüËÆ°„ÄÅÊåñÊéò„ÄÇÁ≥ªÁªüÂÆûÁé∞Ëá™Âä®ÂàõÂª∫Êï∞ÊçÆÂ∫ì„ÄÅÊï∞ÊçÆË°®ÔºåÂ∞ÅË£Ö‰∫ÜÊâπÈáèÊõ¥Êñ∞„ÄÅÊèíÂÖ•Êï∞ÊçÆÔºåÊñπ‰æø‰∏öÂä°Êâ©Â±ï„ÄÇ

![](img/07.jpg)

## ÂçÅ‰∏âÔºöÂ±ïÁ§∫ÈááÁî®webËÆæËÆ°

ÈááÁî®webËÆæËÆ°ÔºåÂèØËßÜÂåñÂ±ïÁ§∫ÁªìÊûú„ÄÇÂØπÂ±ïÁ§∫ËøõË°åÂ∞ÅË£ÖÔºåÊ∑ªÂä†Êñ∞ÁöÑ‰∏öÂä°Ë°®ÂçïÔºåÂè™ÈúÄË¶ÅÈÖçÁΩÆËßÜÂõæÂ≠óÂÖ∏Â∞±ÂèØËá™Âä®Âá∫Áé∞‰∏öÂä°ÂèØËßÜÂåñÁïåÈù¢ÔºåÊñπ‰æø‰∏öÂä°ÂäüËÉΩÊâ©Â±ï„ÄÇ

## ÂçÅÂõõÔºöËøêË°åÈ´òÊïà


ÈááÁî®Â§öÁ∫øÁ®ã„ÄÅÂçï‰æãÂÖ±‰∫´ËµÑÊ∫êÊúâÊïàÊèêÈ´òËøêÁÆóÊïàÁéá„ÄÇ1Â§©Êï∞ÊçÆÁöÑÊäìÂèñ„ÄÅËÆ°ÁÆóÊåáÊ†á„ÄÅÂΩ¢ÊÄÅËØÜÂà´„ÄÅÁ≠ñÁï•ÈÄâËÇ°„ÄÅÂõûÊµãÁ≠âÂÖ®ÈÉ®‰ªªÂä°ËøêË°åÊó∂Èó¥Â§ßÊ¶Ç4ÂàÜÈíüÔºàÊôÆÈÄöÁ¨îËÆ∞Êú¨ÔºâÔºåËÆ°ÁÆóÂ§©Êï∞Ë∂äÂ§öÊïàÁéáË∂äÈ´ò„ÄÇ


## ÂçÅ‰∫îÔºöÊñπ‰æøË∞ÉËØï

Á≥ªÁªüËøêË°åÁöÑÈáçË¶ÅÊó•ÂøóËÆ∞ÂΩïÂú®stock_execute_job.log(Êï∞ÊçÆÊäìÂèñ„ÄÅÂ§ÑÁêÜ„ÄÅÂàÜÊûê)„ÄÅstock_web.log(webÊúçÂä°)„ÄÅstock_trade.log(‰∫§ÊòìÊúçÂä°)ÔºåÊñπ‰æøË∞ÉËØïÂèëÁé∞ÈóÆÈ¢ò„ÄÇ

![](img/08.jpg)


# ÂÆâË£ÖËØ¥Êòé

Êú¨Á≥ªÁªüÊîØÊåÅWindows„ÄÅLinux„ÄÅMacOSÔºåÂêåÊó∂Êú¨Á≥ªÁªüÂàõÂª∫‰∫ÜDockerÈïúÂÉèÔºåÊåâËá™Â∑±ÈúÄË¶ÅÈÄâÊã©ÂÆâË£ÖÊñπÂºè„ÄÇ

‰∏ãÈù¢ÊåâÂàÜÂ∏∏ËßÑÂÆâË£ÖÊñπÂºè„ÄÅdockerÈïúÂÉèÂÆâË£ÖÊñπÂºèËøõË°å‰∏Ä‰∏ÄËØ¥Êòé„ÄÇ

## ‰∏ÄÔºöÂ∏∏ËßÑÂÆâË£ÖÊñπÂºè

Âª∫ËÆÆwindows‰∏ãÂÆâË£ÖÔºåÊñπ‰æøÊìç‰ΩúÂèä‰ΩøÁî®Á≥ªÁªüÔºåÂêåÊó∂ÂÆâË£Ö‰πüÈùûÂ∏∏ÁÆÄÂçï„ÄÇ

‰ª•‰∏ãÂÆâË£ÖÂèäËøêË°å‰ª•windows‰∏∫‰æãËøõË°å‰ªãÁªç„ÄÇ

### 1.ÂÆâË£Öpython

È°πÁõÆÂºÄÂèë‰ΩøÁî®python 3.11ÔºåÂª∫ËÆÆÊúÄÊñ∞Áâà„ÄÇ

```
Ôºà1ÔºâÂú®ÂÆòÁΩë https://www.python.org/downloads/ ‰∏ãËΩΩÂÆâË£ÖÂåÖÔºå‰∏ÄÈîÆÂÆâË£ÖÂç≥ÂèØÔºåÂÆâË£ÖÂàáËÆ∞ÂãæÈÄâËá™Âä®ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáè„ÄÇ
Ôºà2ÔºâÈÖçÁΩÆÊ∞∏‰πÖÂÖ®Â±ÄÂõΩÂÜÖÈïúÂÉèÂ∫ìÔºàÂõ†‰∏∫ÊúâÂ¢ôÔºåÊó†Ê≥ïÊ≠£Â∏∏ÂÆâË£ÖÂ∫ìÊñá‰ª∂ÔºâÔºåÊâßË°åÂ¶Ç‰∏ãdosÂëΩ‰ª§Ôºö
python pip config --global set  global.index-url https://mirrors.aliyun.com/pypi/simple/
# Â¶ÇÊûú‰Ω†Âè™ÊÉ≥‰∏∫ÂΩìÂâçÁî®Êà∑ËÆæÁΩÆÔºå‰Ω†‰πüÂèØ‰ª•ÂéªÊéâ‰∏ãÈù¢ÁöÑ&quot;--global&quot;ÈÄâÈ°π
```
### 2.ÂÆâË£Ömysql

Âª∫ËÆÆÊúÄÊñ∞Áâà„ÄÇ

```
Âú®ÂÆòÁΩë https://dev.mysql.com/downloads/mysql/ ‰∏ãËΩΩÂÆâË£ÖÂåÖÔºå‰∏ÄÈîÆÂÆâË£ÖÂç≥ÂèØ„ÄÇ
```
### 3.ÂÆâË£Ö TA-Lib ÂÖ±‰∫´ÈùôÊÄÅÂ∫ìÂíåÂ§¥Êñá‰ª∂

ÂÆâË£Ö TA-Lib C/C++ ÂÖ±‰∫´ÈùôÊÄÅÂ∫ìÂíåÂ§¥Êñá‰ª∂

```
https://ta-lib.org/install/ ‰∏ãËΩΩÊúÄÊñ∞ ta-lib ÂÖ±‰∫´ÈùôÊÄÅÂ∫ìÂíåÂ§¥Êñá‰ª∂ÔºåÊåâÁÖßËØ¥ÊòéËøõË°åÂÆâË£Ö„ÄÇ
ÂÆâË£ÖÊñπÂºèÊåâÂÆòÊñπÂª∫ËÆÆÔºå‰ºöÊõ¥ÁÆÄÂçïÔºö
Windows Executable Installer
macOS Homebrew
Linux Debian packages
```

### 4.ÂÆâË£Ö‰æùËµñÂ∫ì

‰æùËµñÂ∫ìÈÉΩÊòØÁõÆÂâçÊúÄÊñ∞ÁâàÊú¨„ÄÇ

a.ÂÆâË£Ö‰æùËµñÂ∫ìÔºö

```
#dosÂàáÊç¢Âà∞Êú¨Á≥ªÁªüÁöÑÊ†πÁõÆÂΩïÔºåÊâßË°å‰∏ãÈù¢ÂëΩ‰ª§Ôºö
python pip install -r requirements.txt
```
b.Ëã•ÊÉ≥ÂçáÁ∫ßÈ°πÁõÆ‰æùËµñÂ∫ìËá≥ÊúÄÊñ∞ÁâàÔºåÂèØ‰ª•ÈÄöËøá‰∏ãÈù¢ÊñπÊ≥ïÔºö

ÂÖàÊâìÂºÄrequirements.txtÔºåÁÑ∂Âêé‰øÆÊîπÊñá‰ª∂‰∏≠ÁöÑ‚Äú==‚Äù‰∏∫‚Äú&gt;=‚ÄùÔºåÊé•ÁùÄÊâßË°å‰∏ãÈù¢ÂëΩ‰ª§Ôºö

```
python pip install -r requirements.txt --upgrade
```

c.Ëã•Êâ©Â±ï‰∫ÜÊú¨È°πÁõÆÔºåÂèØ‰ª•ÈÄöËøá‰∏ãÈù¢ÊñπÊ≥ïÁîüÊàêÈ°πÁõÆ‰æùËµñÔºö

```
#‰ΩøÁî®pipreqsÁîüÊàêÈ°πÁõÆÁõ∏ÂÖ≥‰æùËµñÁöÑrequirements.txt

python pip install pipreqs
# ÂÆâË£ÖpipreqsÔºåËã•ÊúâÂÆâË£ÖÂèØË∑≥Ëøá

python  pipreqs --encoding utf-8 --force ./ 
# Êú¨È°πÁõÆÊòØutf-8ÁºñÁ†Å
```


### 5.ÂÆâË£Ö NavicatÔºàÂèØÈÄâÔºâ

NavicatÂèØ‰ª•Êñπ‰æøÁÆ°ÁêÜÊï∞ÊçÆÂ∫ìÔºå‰ª•ÂèäÂèØ‰ª•ÊâãÂ∑•ÂØπÊï∞ÊçÆËøõË°åÊü•Áúã„ÄÅÂ§ÑÁêÜ„ÄÅÂàÜÊûê„ÄÅÊåñÊéò„ÄÇ

NavicatÊòØ‰∏ÄÂ•óÂèØÂàõÂª∫Â§ö‰∏™ËøûÊé•ÁöÑÊï∞ÊçÆÂ∫ìÁÆ°ÁêÜÂ∑•ÂÖ∑ÔºåÁî®‰ª•Êñπ‰æøÁÆ°ÁêÜ MySQL„ÄÅOracle„ÄÅPostgreSQL„ÄÅSQLite„ÄÅSQL Server„ÄÅMariaDB Âíå MongoDB Á≠â‰∏çÂêåÁ±ªÂûãÁöÑÊï∞ÊçÆÂ∫ì

```
Ôºà1ÔºâÂú®ÂÆòÁΩë https://www.navicat.com.cn/download/navicat-premium ‰∏ãËΩΩÂÆâË£ÖÂåÖÔºå‰∏ÄÈîÆÂÆâË£ÖÂç≥ÂèØ„ÄÇ

Ôºà2ÔºâÁÑ∂Âêé‰∏ãËΩΩÁ†¥Ëß£Ë°•‰∏Å: https://pan.baidu.com/s/18XpTHrm9OiLEl3u6z_uxnw ÊèêÂèñÁ†Å: 8888 ÔºåÁ†¥Ëß£Âç≥ÂèØ„ÄÇ
```
### 6.ÈÖçÁΩÆÊï∞ÊçÆÂ∫ì

‰∏ÄËà¨ÂèØËÉΩ‰ºö‰øÆÊîπÁöÑ‰ø°ÊÅØÊòØ‚ÄùÊï∞ÊçÆÂ∫ìËÆøÈóÆÂØÜÁ†Å‚Äú„ÄÇ

‰øÆÊîπdatabase.pyÁõ∏ÂÖ≥‰ø°ÊÅØ:

```
db_host = &quot;localhost&quot;  # Êï∞ÊçÆÂ∫ìÊúçÂä°‰∏ªÊú∫
db_user = &quot;root&quot;  # Êï∞ÊçÆÂ∫ìËÆøÈóÆÁî®Êà∑
db_password = &quot;root&quot;  # Êï∞ÊçÆÂ∫ìËÆøÈóÆÂØÜÁ†Å
db_port = 3306  # Êï∞ÊçÆÂ∫ìÊúçÂä°Á´ØÂè£
db_charset = &quot;utf8mb4&quot;  # Êï∞ÊçÆÂ∫ìÂ≠óÁ¨¶ÈõÜ
```

### 7.ÂÆâË£ÖËá™Âä®‰∫§ÊòìÔºàÂèØÈÄâÔºâ

```
1.ÂÆâË£Ö‰∫§ÊòìËΩØ‰ª∂
    1.1 ÈÄöÁî®ÂêåËä±È°∫ÂÆ¢Êà∑Á´ØÂà∏ÂïÜÁöÑÂÆ¢Êà∑
        ÈÄöÁî®ÂêåËä±È°∫ÂÆ¢Êà∑Á´Ø:
        https://activity.ths123.com/acmake/cache/1361.html
    1.2 ‰∏ìÁî®ÂêåËä±È°∫ÂÆ¢Êà∑Á´ØÂà∏ÂïÜÁöÑÂÆ¢Êà∑
        Ëá™Ë°åÂéªÂà∏ÂïÜÂÆòÁΩëÊâæÂêåËä±È°∫‰∏ìÁî®Áâà
        ‰æãÂ¶ÇÔºöÂπøÂèëÁöÑ‰∏ãËΩΩÊ†∏Êñ∞Áã¨Á´ãÂßîÊâòÁ´Ø(ÂêåËä±È°∫Áâà):
        http://www.gf.com.cn/softdownload/index?tab=1
2.ÂÆâË£Ötesseract(Ëá™Âä®ËØÜÂà´È™åËØÅÁ†Å)
    Á¨¨‰∏ÄÁßçÊñπÊ≥ï.‰∏ãËΩΩÁºñËØëÂ•ΩÁöÑ
        Âú®‰∏ãÈù¢ÈìæÊé•È°µÔºåÊ†πÊçÆÊìç‰ΩúÁ≥ªÁªüÈÄâÊã©Áõ∏Â∫îÁâàÊú¨
        https://digi.bib.uni-mannheim.de/tesseract/
    Á¨¨‰∫åÁßçÊñπÊ≥ï.Áî®Ê∫êÁ†ÅÁºñËØë
        ‰∏ãËΩΩÊ∫êÁ†ÅÔºöhttps://github.com/tesseract-ocr/tesseract
    Ê≥®ÊÑèÔºö
        ÂÆâË£ÖÂÆåË¶ÅÂ∞ÜÂÆâË£ÖË∑ØÂæÑËÆæÁΩÆÂà∞PATHÁéØÂ¢ÉÂèòÈáèÈáå„ÄÇ
        ‰∏ãÈù¢Êèê‰æõdosÂëΩ‰ª§ËÆæÁΩÆÔºå‰ª•ÁÆ°ÁêÜÂëòË∫´‰ªΩËøêË°åcmdÔºåËæìÂÖ•:
        setx /m PATH &quot;%PATH%;C:\Program Files\Tesseract-OCR&quot;
3.ËÆæÁΩÆ‰∫§ÊòìÈÖçÁΩÆ   
    3.1.‰øÆÊîπtrade_client.json
        &quot;user&quot;: &quot;888888888888&quot;,               #‰∫§ÊòìË¥¶Âè∑
        &quot;password&quot;: &quot;888888&quot;,                 #‰∫§ÊòìÂØÜÁ†Å
        &quot;exe_path&quot;: &quot;C:/gfzqrzrq/xiadan.exe&quot;  #‰∫§ÊòìËΩØ‰ª∂Ë∑ØÂæÑ
    3.2.‰øÆÊîπtrade_service.py
        broker = &#039;gf_client&#039; #ËøôÊòØÂπøÂèë
        ËØ¶ÊÉÖÂèÇÈòÖusage.mdÔºåÈÖçÁΩÆÂØπÂ∫îÂà∏ÂïÜ
```

### 8.ËøêË°åËØ¥Êòé

#### 8.1.ÊâßË°åÊï∞ÊçÆÊäìÂèñ„ÄÅÂ§ÑÁêÜ„ÄÅÂàÜÊûê„ÄÅËØÜÂà´

ÊîØÊåÅÊâπÈáè‰Ωú‰∏öÔºåÂÖ∑‰ΩìÂèÇËßÅrun_job.bat‰∏≠ÁöÑÊ≥®ÈáäËØ¥Êòé„ÄÇ

Âª∫ËÆÆÂ∞ÜÂÖ∂Âä†ÂÖ•Âà∞‰ªªÂä°ËÆ°Âàí‰∏≠ÔºåÂ∑•‰ΩúÊó•ÁöÑÊØèÂ§©17Ôºö00ÊâßË°å„ÄÇ

**Êï∞ÊçÆÊäìÂèñ„ÄÅÂ§ÑÁêÜÂéüÂàôÔºö**

1).ÂºÄÁõòÂç≥Êúâ‰∏îÊó†ÂéÜÂè≤Êï∞ÊçÆÁöÑÔºöÁªºÂêàÈÄâËÇ°„ÄÅÊØèÊó•ËÇ°Á•®Êï∞ÊçÆ„ÄÅËÇ°Á•®ËµÑÈáëÊµÅÂêë„ÄÅËÇ°Á•®ÂàÜÁ∫¢ÈÖçÈÄÅ„ÄÅÈæôËôéÊ¶ú„ÄÅÊØèÊó•ETFÊï∞ÊçÆÔºõ

2).Êî∂ÁõòÂç≥Êúâ‰∏îÊúâÂéÜÂè≤Êï∞ÊçÆÁöÑÔºöËÇ°Á•®ÊåáÊ†áÊï∞ÊçÆ„ÄÅËÇ°Á•®KÁ∫øÂΩ¢ÊÄÅ„ÄÅËÇ°Á•®Á≠ñÁï•Êï∞ÊçÆÔºõ

3).Êî∂ÁõòÂêé1~2Â∞èÊó∂ÊâçÊúâ‰∏îÊúâÂéÜÂè≤Êï∞ÊçÆÁöÑÔºöÂ§ßÂÆó‰∫§Êòì„ÄÇ

ËøêË°årun_job.batÔºå‰ºö‰æùÊçÆ‰∏äÈù¢ÂéüÂàôËé∑ÂèñÂêÑÊ®°ÂùóÂΩìÂâçÊàñÂâç‰∏™‰∫§ÊòìÊó•ÁöÑÊï∞ÊçÆ„ÄÇ

```

ËøêË°å run_job.bat
```
Ëã•ÊÉ≥ÁúãÂºÄÁõòÂêéÁöÑÂΩìÂâçÂÆûÊó∂Êï∞ÊçÆÔºåÂèØ‰ª•ËøêË°å‰∏ãÈù¢ÔºåÂæàÂø´Â§ßÊ¶Ç1ÁßíÔºö

```
#Âü∫Á°ÄÊï∞ÊçÆ‰Ωú‰∏ö 
python basic_data_daily_job.py
```
#### 8.2.ÂêØÂä®webÊúçÂä°

```
ËøêË°å run_web.bat
```
ÂêØÂä®ÊúçÂä°ÂêéÔºåÊâìÂºÄÊµèËßàÂô®ÔºåËæìÂÖ•Ôºöhttp://localhost:9988/ ÔºåÂç≥ÂèØ‰ΩøÁî®Êú¨Á≥ªÁªüÁöÑÂèØËßÜÂåñÂäüËÉΩ„ÄÇ

#### 8.3.ÂêØÂä®‰∫§ÊòìÊúçÂä°

```
ËøêË°å run_trade.bat
```

## ‰∫åÔºödockerÈïúÂÉèÂÆâË£ÖÊñπÂºè

Ê≤°ÊúâdockerÁéØÂ¢ÉÔºåÂèØ‰ª•ÂèÇËÄÉÔºö[VirtualBoxËôöÊãüÊú∫ÂÆâË£ÖUbuntu](https://www.ljjyy.com/archives/2019/10/100590.html)ÔºåÈáåÈù¢‰πü‰ªãÁªç‰∫Üpython„ÄÅdockerÁ≠âÂ∏∏Áî®ËΩØ‰ª∂ÁöÑÂÆâË£ÖÔºåËã•ÊÉ≥Âú®Windows‰∏ãÂÆâË£ÖdockerËá™Ë°åÁôæÂ∫¶„ÄÇ

### 1.ÂÆâË£ÖÊï∞ÊçÆÂ∫ìÈïúÂÉè

Â¶ÇÊûúÂ∑≤ÁªèÊúâMysql„ÄÅmariadbÊï∞ÊçÆÂ∫ìÂèØ‰ª•Ë∑≥ËøáÊú¨Ê≠•„ÄÇ

ËøêË°å‰∏ãÈù¢ÂëΩ‰ª§Ôºö

**ÁâπÂà´ÊèêÈÜíÔºöÊâßË°åÂëΩ‰ª§ÁöÑÁî®Êà∑Ë¶ÅÊúârootÊùÉÈôêÔºåÂÖ∂‰ªñÂëΩ‰ª§‰πüÂ¶ÇÊ≠§„ÄÇ‰æãÂ¶ÇÔºöubuntuÁ≥ªÁªüÂú®ÂëΩ‰ª§ÂâçÂä†‰∏äsudo** Ôºåsudo docker......

```
docker run -d --name InStockDbService \
    -v /data/mariadb/data:/var/lib/instockdb \
    -e MYSQL_ROOT_PASSWORD=root \
    library/mariadb:latest
```

### 2.ÂÆâË£ÖÊú¨Á≥ªÁªüÈïúÂÉè

a.Ëã•Êåâ‰∏äÈù¢„Äê1.ÂÆâË£ÖÊï∞ÊçÆÂ∫ìÈïúÂÉè„ÄëË£ÖÁöÑÊï∞ÊçÆÂ∫ìÔºåËøêË°å‰∏ãÈù¢ÂëΩ‰ª§Ôºö

```
docker run -dit --name InStock --link=InStockDbService \
    -p 9988:9988 \
    -e db_host=InStockDbService \
    mayanghua/instock:latest
```

b.Â∑≤ÁªèÊúâMysql„ÄÅmariadbÊï∞ÊçÆÂ∫ìÔºåËøêË°å‰∏ãÈù¢ÂëΩ‰ª§Ôºö

```
docker run -dit --name InStock \
    -p 9988:9988 \
    -e db_host=localhost \
    -e db_user=root \
    -e db_password=root \
    -e db_database=instockdb \
    -e db_port=3306 \
    mayanghua/instock:latest
```

docker -e ÂèÇÊï∞ËØ¥ÊòéÔºö
```
db_host       # Êï∞ÊçÆÂ∫ìÊúçÂä°‰∏ªÊú∫
db_user       # Êï∞ÊçÆÂ∫ìËÆøÈóÆÁî®Êà∑
db_password   # Êï∞ÊçÆÂ∫ìËÆøÈóÆÂØÜÁ†Å
db_database   # Êï∞ÊçÆÂ∫ìÂêçÁß∞
db_port       # Êï∞ÊçÆÂ∫ìÊúçÂä°Á´ØÂè£
```
ÊåâËá™Â∑±Êï∞ÊçÆÂ∫ìÂÆûÈôÖÊÉÖÂÜµÈÖçÁΩÆÂèÇÊï∞„ÄÇ

### 3. Á≥ªÁªüËøêË°å

ÂêØÂä®ÂÆπÂô®ÂêéÔºå‰ºöËá™Âä®ËøêË°åÔºåÈ¶ñÂÖà‰ºöÂàùÂßãÂåñÊï∞ÊçÆ„ÄÅÂêØÂä®webÊúçÂä°„ÄÇÁÑ∂ÂêéÊØèÂ∞èÊó∂ÊâßË°å‚ÄúÂü∫Á°ÄÊï∞ÊçÆÊäìÂèñ‚ÄùÔºåÊØèÂ§©17:30ÊâßË°åÊâÄÊúâÁöÑÊï∞ÊçÆÊäìÂèñ„ÄÅÂ§ÑÁêÜ„ÄÅÂàÜÊûê„ÄÅËØÜÂà´„ÄÅÂõûÊµã„ÄÇ

ÊâìÂºÄÊµèËßàÂô®ÔºåËæìÂÖ•Ôºöhttp://localhost:9988/ ÔºåÂç≥ÂèØ‰ΩøÁî®Êú¨Á≥ªÁªüÁöÑÂèØËßÜÂåñÂäüËÉΩ„ÄÇ

### 4.ÂéÜÂè≤Êï∞ÊçÆ

ÂéÜÂè≤Êï∞ÊçÆÊäìÂèñ„ÄÅÂ§ÑÁêÜ„ÄÅÂàÜÊûê„ÄÅËØÜÂà´„ÄÅÂõûÊµãÔºåËøêË°å‰∏ãÈù¢ÂëΩ‰ª§Ôºö

```
docker exec -it InStock bash 
cat InStock/instock/bin/run_job.sh
#Êü•Áúãrun_job.shÊ≥®Èáä,Ëá™Â∑±ÈÄâÊã©‰Ωú‰∏ö
------Êï¥‰Ωì‰Ωú‰∏öÔºåÊîØÊåÅÊâπÈáè‰Ωú‰∏ö------
ÂΩìÂâçÊó∂Èó¥‰Ωú‰∏ö python execute_daily_job.py
Âçï‰∏™Êó∂Èó¥‰Ωú‰∏ö python execute_daily_job.py 2022-03-01
Êûö‰∏æÊó∂Èó¥‰Ωú‰∏ö python execute_daily_job.py 2022-01-01,2021-02-08,2022-03-12
Âå∫Èó¥Êó∂Èó¥‰Ωú‰∏ö python execute_daily_job.py 2022-01-01 2022-03-01
------ÂçïÂäüËÉΩ‰Ωú‰∏öÔºåÊîØÊåÅÊâπÈáè‰Ωú‰∏öÔºåÂõûÊµãÊï∞ÊçÆËá™Âä®Â°´Ë°•Âà∞ÂΩìÂâç
ÁªºÂêàÈÄâËÇ°‰Ωú‰∏ö python selection_data_daily_job.py
Âü∫Á°ÄÊï∞ÊçÆÂÆûÊó∂‰Ωú‰∏ö python basic_data_daily_job.py
Âü∫Á°ÄÊï∞ÊçÆÊî∂Áõò2Â∞èÊó∂Âêé‰Ωú‰∏ö python backtest_data_daily_job.py
Âü∫Á°ÄÊï∞ÊçÆÈùûÂÆûÊó∂‰Ωú‰∏ö python basic_data_other_daily_job.py
ÊåáÊ†áÊï∞ÊçÆ‰Ωú‰∏ö python indicators_data_daily_job.py
KÁ∫øÂΩ¢ÊÄÅ‰Ωú‰∏ö klinepattern_data_daily_job.py
Á≠ñÁï•Êï∞ÊçÆ‰Ωú‰∏ö python strategy_data_daily_job.py
ÂõûÊµãÊï∞ÊçÆ python backtest_data_daily_job.py
Á¨¨‰∏ÄÁßçÊñπÊ≥ïÔºö
python execute_daily_job.py 2023-03-01,2023-03-02
Á¨¨‰∫åÁßçÊñπÊ≥ïÔºö
‰øÆÊîπrun_job.shÔºåÁÑ∂ÂêéËøêË°å bash InStock/instock/bin/run_job.sh
```

### 5.Êü•ÁúãÊó•Âøó

ËøêË°å‰∏ãÈù¢ÂëΩ‰ª§Ôºö

```
docker exec -it InStock bash 
cat InStock/instock/log/stock_execute_job.log
cat InStock/instock/log/stock_web.log
```

### 6.dockerÂ∏∏Áî®ÂëΩ‰ª§

```
docker container stop InStock InStockDbService
#ÂÅúÊ≠¢ÂÆπÂô®
docker container prune
#ÂõûÊî∂ÂÆπÂô®
docker rmi mayanghua/instock:latest library/mariadb:latest
#Âà†Èô§ÈïúÂÉè
```

ÂÖ∑‰ΩìÂèÇËßÅÔºö[DockerÂü∫Á°Ä‰πã ‰∫å.ÈïúÂÉèÂèäÂÆπÂô®ÁöÑÂü∫Êú¨Êìç‰Ωú](https://www.ljjyy.com/archives/2018/06/100208.html)

### 7.Ëá™Âä®‰∫§Êòì

ÁõÆÂâçÂè™ÊîØÊåÅwindows„ÄÇÂèÇËÄÉÂ∏∏ËßÑÂÆâË£ÖÊñπÂºè,Âè™ÈúÄÂÆâË£Öpython„ÄÅ‰æùËµñÂ∫ìÔºå**‰∏çÈúÄÂÆâË£Ömysql„ÄÅtalibÁ≠â**„ÄÇ

# ÁâπÂà´Â£∞Êòé

ËÇ°Â∏ÇÊúâÈ£éÈô©ÊäïËµÑÈúÄË∞®ÊÖéÔºåÊú¨Á≥ªÁªüÂè™ËÉΩÁî®‰∫éÂ≠¶‰π†„ÄÅËÇ°Á•®ÂàÜÊûêÔºåÊäïËµÑÁõà‰∫èÊ¶Ç‰∏çË¥üË¥£„ÄÇ

Êú¨Á≥ªÁªü‰∏≠ÁöÑË°®Ê†º‰∏∫Á¨¨‰∏âÊñπÂïÜ‰∏öÊéß‰ª∂Ôºå‰ªÖ‰ΩøÁî®‰∫ÜËØÑ‰º∞ÁâàËøõË°åÂ≠¶‰π†ÂèäÊµãËØï„ÄÇ
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sgl-project/sglang]]></title>
            <link>https://github.com/sgl-project/sglang</link>
            <guid>https://github.com/sgl-project/sglang</guid>
            <pubDate>Thu, 01 May 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[SGLang is a fast serving framework for large language models and vision language models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sgl-project/sglang">sgl-project/sglang</a></h1>
            <p>SGLang is a fast serving framework for large language models and vision language models.</p>
            <p>Language: Python</p>
            <p>Stars: 13,843</p>
            <p>Forks: 1,636</p>
            <p>Stars today: 75 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; id=&quot;sglangtop&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/sgl-project/sglang/main/assets/logo.png&quot; alt=&quot;logo&quot; width=&quot;400&quot; margin=&quot;10px&quot;&gt;&lt;/img&gt;

[![PyPI](https://img.shields.io/pypi/v/sglang)](https://pypi.org/project/sglang)
![PyPI - Downloads](https://img.shields.io/pypi/dm/sglang)
[![license](https://img.shields.io/github/license/sgl-project/sglang.svg)](https://github.com/sgl-project/sglang/tree/main/LICENSE)
[![issue resolution](https://img.shields.io/github/issues-closed-raw/sgl-project/sglang)](https://github.com/sgl-project/sglang/issues)
[![open issues](https://img.shields.io/github/issues-raw/sgl-project/sglang)](https://github.com/sgl-project/sglang/issues)
[![](https://img.shields.io/badge/Gurubase-(experimental)-006BFF)](https://gurubase.io/g/sglang)

&lt;/div&gt;

--------------------------------------------------------------------------------

| [**Blog**](https://lmsys.org/blog/2024-07-25-sglang-llama3/)
| [**Documentation**](https://docs.sglang.ai/)
| [**Join Slack**](https://slack.sglang.ai/)
| [**Join Bi-Weekly Development Meeting**](https://meeting.sglang.ai/)
| [**Roadmap**](https://github.com/sgl-project/sglang/issues/4042)
| [**Slides**](https://github.com/sgl-project/sgl-learning-materials?tab=readme-ov-file#slides) |

## News
- [2025/03] Supercharge DeepSeek-R1 Inference on AMD Instinct MI300X ([AMD blog](https://rocm.blogs.amd.com/artificial-intelligence/DeepSeekR1-Part2/README.html))
- [2025/03] SGLang Joins PyTorch Ecosystem: Efficient LLM Serving Engine ([PyTorch blog](https://pytorch.org/blog/sglang-joins-pytorch/))
- [2025/02] Unlock DeepSeek-R1 Inference Performance on AMD Instinct‚Ñ¢ MI300X GPU ([AMD blog](https://rocm.blogs.amd.com/artificial-intelligence/DeepSeekR1_Perf/README.html))
- [2025/01] üî• SGLang provides day one support for DeepSeek V3/R1 models on NVIDIA and AMD GPUs with DeepSeek-specific optimizations. ([instructions](https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v3), [AMD blog](https://www.amd.com/en/developer/resources/technical-articles/amd-instinct-gpus-power-deepseek-v3-revolutionizing-ai-development-with-sglang.html), [10+ other companies](https://x.com/lmsysorg/status/1887262321636221412))
- [2024/12] üî• v0.4 Release: Zero-Overhead Batch Scheduler, Cache-Aware Load Balancer, Faster Structured Outputs ([blog](https://lmsys.org/blog/2024-12-04-sglang-v0-4/)).
- [2024/09] v0.3 Release: 7x Faster DeepSeek MLA, 1.5x Faster torch.compile, Multi-Image/Video LLaVA-OneVision ([blog](https://lmsys.org/blog/2024-09-04-sglang-v0-3/)).
- [2024/07] v0.2 Release: Faster Llama3 Serving with SGLang Runtime (vs. TensorRT-LLM, vLLM) ([blog](https://lmsys.org/blog/2024-07-25-sglang-llama3/)).

&lt;details&gt;
&lt;summary&gt;More&lt;/summary&gt;

- [2024/10] The First SGLang Online Meetup ([slides](https://github.com/sgl-project/sgl-learning-materials?tab=readme-ov-file#the-first-sglang-online-meetup)).
- [2024/02] SGLang enables **3x faster JSON decoding** with compressed finite state machine ([blog](https://lmsys.org/blog/2024-02-05-compressed-fsm/)).
- [2024/01] SGLang provides up to **5x faster inference** with RadixAttention ([blog](https://lmsys.org/blog/2024-01-17-sglang/)).
- [2024/01] SGLang powers the serving of the official **LLaVA v1.6** release demo ([usage](https://github.com/haotian-liu/LLaVA?tab=readme-ov-file#demo)).

&lt;/details&gt;

## About
SGLang is a fast serving framework for large language models and vision language models.
It makes your interaction with models faster and more controllable by co-designing the backend runtime and frontend language.
The core features include:

- **Fast Backend Runtime**: Provides efficient serving with RadixAttention for prefix caching, zero-overhead CPU scheduler, continuous batching, token attention (paged attention), speculative decoding, tensor parallelism, chunked prefill, structured outputs, quantization (FP8/INT4/AWQ/GPTQ), and multi-lora batching.
- **Flexible Frontend Language**: Offers an intuitive interface for programming LLM applications, including chained generation calls, advanced prompting, control flow, multi-modal inputs, parallelism, and external interactions.
- **Extensive Model Support**: Supports a wide range of generative models (Llama, Gemma, Mistral, QWen, DeepSeek, LLaVA, etc.), embedding models (e5-mistral, gte, mcdse) and reward models (Skywork), with easy extensibility for integrating new models.
- **Active Community**: SGLang is open-source and backed by an active community with industry adoption.

## Getting Started
- [Install SGLang](https://docs.sglang.ai/start/install.html)
- [Quick Start](https://docs.sglang.ai/backend/send_request.html)
- [Backend Tutorial](https://docs.sglang.ai/backend/openai_api_completions.html)
- [Frontend Tutorial](https://docs.sglang.ai/frontend/frontend.html)
- [Contribution Guide](https://docs.sglang.ai/references/contribution_guide.html)

## Benchmark and Performance
Learn more in the release blogs: [v0.2 blog](https://lmsys.org/blog/2024-07-25-sglang-llama3/), [v0.3 blog](https://lmsys.org/blog/2024-09-04-sglang-v0-3/), [v0.4 blog](https://lmsys.org/blog/2024-12-04-sglang-v0-4/)

## Roadmap
[Development Roadmap (2025 H1)](https://github.com/sgl-project/sglang/issues/4042)

## Adoption and Sponsorship
The project has been deployed to large-scale production, generating trillions of tokens every day.
It is supported by the following institutions: AMD, Atlas Cloud, Baseten, Cursor, DataCrunch, Etched, Hyperbolic, Iflytek, Jam &amp; Tea Studios, LinkedIn, LMSYS, Meituan, Nebius, Novita AI, NVIDIA, Oracle, RunPod, Stanford, UC Berkeley, UCLA, xAI, and 01.AI.

&lt;img src=&quot;https://raw.githubusercontent.com/sgl-project/sgl-learning-materials/main/slides/adoption.png&quot; alt=&quot;logo&quot; width=&quot;800&quot; margin=&quot;10px&quot;&gt;&lt;/img&gt;

## Contact Us

For enterprises interested in adopting or deploying SGLang at scale, including technical consulting, sponsorship opportunities, or partnership inquiries, please contact us at contact@sglang.ai.

## Acknowledgment
We learned the design and reused code from the following projects: [Guidance](https://github.com/guidance-ai/guidance), [vLLM](https://github.com/vllm-project/vllm), [LightLLM](https://github.com/ModelTC/lightllm), [FlashInfer](https://github.com/flashinfer-ai/flashinfer), [Outlines](https://github.com/outlines-dev/outlines), and [LMQL](https://github.com/eth-sri/lmql).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[unclecode/crawl4ai]]></title>
            <link>https://github.com/unclecode/crawl4ai</link>
            <guid>https://github.com/unclecode/crawl4ai</guid>
            <pubDate>Thu, 01 May 2025 00:04:28 GMT</pubDate>
            <description><![CDATA[üöÄü§ñ Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper. Don't be shy, join here: https://discord.gg/jP8KfhDhyN]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/unclecode/crawl4ai">unclecode/crawl4ai</a></h1>
            <p>üöÄü§ñ Crawl4AI: Open-source LLM Friendly Web Crawler & Scraper. Don't be shy, join here: https://discord.gg/jP8KfhDhyN</p>
            <p>Language: Python</p>
            <p>Stars: 41,710</p>
            <p>Forks: 3,792</p>
            <p>Stars today: 202 stars today</p>
            <h2>README</h2><pre># üöÄü§ñ Crawl4AI: Open-source LLM Friendly Web Crawler &amp; Scraper.

&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://trendshift.io/repositories/11716&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11716&quot; alt=&quot;unclecode%2Fcrawl4ai | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

[![GitHub Stars](https://img.shields.io/github/stars/unclecode/crawl4ai?style=social)](https://github.com/unclecode/crawl4ai/stargazers)
[![GitHub Forks](https://img.shields.io/github/forks/unclecode/crawl4ai?style=social)](https://github.com/unclecode/crawl4ai/network/members)

[![PyPI version](https://badge.fury.io/py/crawl4ai.svg)](https://badge.fury.io/py/crawl4ai)
[![Python Version](https://img.shields.io/pypi/pyversions/crawl4ai)](https://pypi.org/project/crawl4ai/)
[![Downloads](https://static.pepy.tech/badge/crawl4ai/month)](https://pepy.tech/project/crawl4ai)

&lt;!-- [![Documentation Status](https://readthedocs.org/projects/crawl4ai/badge/?version=latest)](https://crawl4ai.readthedocs.io/) --&gt;
[![License](https://img.shields.io/github/license/unclecode/crawl4ai)](https://github.com/unclecode/crawl4ai/blob/main/LICENSE)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Security: bandit](https://img.shields.io/badge/security-bandit-yellow.svg)](https://github.com/PyCQA/bandit)
[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](code_of_conduct.md)

&lt;/div&gt;

Crawl4AI is the #1 trending GitHub repository, actively maintained by a vibrant community. It delivers blazing-fast, AI-ready web crawling tailored for LLMs, AI agents, and data pipelines. Open source, flexible, and built for real-time performance, Crawl4AI empowers developers with unmatched speed, precision, and deployment ease.  

[‚ú® Check out latest update v0.6.0](#-recent-updates)

üéâ **Version 0.6.0 is now available!** This release candidate introduces World-aware Crawling with geolocation and locale settings, Table-to-DataFrame extraction, Browser pooling with pre-warming, Network and console traffic capture, MCP integration for AI tools, and a completely revamped Docker deployment! [Read the release notes ‚Üí](https://docs.crawl4ai.com/blog)

&lt;details&gt;
&lt;summary&gt;ü§ì &lt;strong&gt;My Personal Story&lt;/strong&gt;&lt;/summary&gt;

My journey with computers started in childhood when my dad, a computer scientist, introduced me to an Amstrad computer. Those early days sparked a fascination with technology, leading me to pursue computer science and specialize in NLP during my postgraduate studies. It was during this time that I first delved into web crawling, building tools to help researchers organize papers and extract information from publications a challenging yet rewarding experience that honed my skills in data extraction.

Fast forward to 2023, I was working on a tool for a project and needed a crawler to convert a webpage into markdown. While exploring solutions, I found one that claimed to be open-source but required creating an account and generating an API token. Worse, it turned out to be a SaaS model charging $16, and its quality didn‚Äôt meet my standards. Frustrated, I realized this was a deeper problem. That frustration turned into turbo anger mode, and I decided to build my own solution. In just a few days, I created Crawl4AI. To my surprise, it went viral, earning thousands of GitHub stars and resonating with a global community.

I made Crawl4AI open-source for two reasons. First, it‚Äôs my way of giving back to the open-source community that has supported me throughout my career. Second, I believe data should be accessible to everyone, not locked behind paywalls or monopolized by a few. Open access to data lays the foundation for the democratization of AI, a vision where individuals can train their own models and take ownership of their information. This library is the first step in a larger journey to create the best open-source data extraction and generation tool the world has ever seen, built collaboratively by a passionate community.

Thank you to everyone who has supported this project, used it, and shared feedback. Your encouragement motivates me to dream even bigger. Join us, file issues, submit PRs, or spread the word. Together, we can build a tool that truly empowers people to access their own data and reshape the future of AI.
&lt;/details&gt;

## üßê Why Crawl4AI?

1. **Built for LLMs**: Creates smart, concise Markdown optimized for RAG and fine-tuning applications.  
2. **Lightning Fast**: Delivers results 6x faster with real-time, cost-efficient performance.  
3. **Flexible Browser Control**: Offers session management, proxies, and custom hooks for seamless data access.  
4. **Heuristic Intelligence**: Uses advanced algorithms for efficient extraction, reducing reliance on costly models.  
5. **Open Source &amp; Deployable**: Fully open-source with no API keys‚Äîready for Docker and cloud integration.  
6. **Thriving Community**: Actively maintained by a vibrant community and the #1 trending GitHub repository.

## üöÄ Quick Start 

1. Install Crawl4AI:
```bash
# Install the package
pip install -U crawl4ai

# For pre release versions
pip install crawl4ai --pre

# Run post-installation setup
crawl4ai-setup

# Verify your installation
crawl4ai-doctor
```

If you encounter any browser-related issues, you can install them manually:
```bash
python -m playwright install --with-deps chromium
```

2. Run a simple web crawl with Python:
```python
import asyncio
from crawl4ai import *

async def main():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url=&quot;https://www.nbcnews.com/business&quot;,
        )
        print(result.markdown)

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

3. Or use the new command-line interface:
```bash
# Basic crawl with markdown output
crwl https://www.nbcnews.com/business -o markdown

# Deep crawl with BFS strategy, max 10 pages
crwl https://docs.crawl4ai.com --deep-crawl bfs --max-pages 10

# Use LLM extraction with a specific question
crwl https://www.example.com/products -q &quot;Extract all product prices&quot;
```

## ‚ú® Features 

&lt;details&gt;
&lt;summary&gt;üìù &lt;strong&gt;Markdown Generation&lt;/strong&gt;&lt;/summary&gt;

- üßπ **Clean Markdown**: Generates clean, structured Markdown with accurate formatting.
- üéØ **Fit Markdown**: Heuristic-based filtering to remove noise and irrelevant parts for AI-friendly processing.
- üîó **Citations and References**: Converts page links into a numbered reference list with clean citations.
- üõ†Ô∏è **Custom Strategies**: Users can create their own Markdown generation strategies tailored to specific needs.
- üìö **BM25 Algorithm**: Employs BM25-based filtering for extracting core information and removing irrelevant content. 
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üìä &lt;strong&gt;Structured Data Extraction&lt;/strong&gt;&lt;/summary&gt;

- ü§ñ **LLM-Driven Extraction**: Supports all LLMs (open-source and proprietary) for structured data extraction.
- üß± **Chunking Strategies**: Implements chunking (topic-based, regex, sentence-level) for targeted content processing.
- üåå **Cosine Similarity**: Find relevant content chunks based on user queries for semantic extraction.
- üîé **CSS-Based Extraction**: Fast schema-based data extraction using XPath and CSS selectors.
- üîß **Schema Definition**: Define custom schemas for extracting structured JSON from repetitive patterns.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üåê &lt;strong&gt;Browser Integration&lt;/strong&gt;&lt;/summary&gt;

- üñ•Ô∏è **Managed Browser**: Use user-owned browsers with full control, avoiding bot detection.
- üîÑ **Remote Browser Control**: Connect to Chrome Developer Tools Protocol for remote, large-scale data extraction.
- üë§ **Browser Profiler**: Create and manage persistent profiles with saved authentication states, cookies, and settings.
- üîí **Session Management**: Preserve browser states and reuse them for multi-step crawling.
- üß© **Proxy Support**: Seamlessly connect to proxies with authentication for secure access.
- ‚öôÔ∏è **Full Browser Control**: Modify headers, cookies, user agents, and more for tailored crawling setups.
- üåç **Multi-Browser Support**: Compatible with Chromium, Firefox, and WebKit.
- üìê **Dynamic Viewport Adjustment**: Automatically adjusts the browser viewport to match page content, ensuring complete rendering and capturing of all elements.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üîé &lt;strong&gt;Crawling &amp; Scraping&lt;/strong&gt;&lt;/summary&gt;

- üñºÔ∏è **Media Support**: Extract images, audio, videos, and responsive image formats like `srcset` and `picture`.
- üöÄ **Dynamic Crawling**: Execute JS and wait for async or sync for dynamic content extraction.
- üì∏ **Screenshots**: Capture page screenshots during crawling for debugging or analysis.
- üìÇ **Raw Data Crawling**: Directly process raw HTML (`raw:`) or local files (`file://`).
- üîó **Comprehensive Link Extraction**: Extracts internal, external links, and embedded iframe content.
- üõ†Ô∏è **Customizable Hooks**: Define hooks at every step to customize crawling behavior.
- üíæ **Caching**: Cache data for improved speed and to avoid redundant fetches.
- üìÑ **Metadata Extraction**: Retrieve structured metadata from web pages.
- üì° **IFrame Content Extraction**: Seamless extraction from embedded iframe content.
- üïµÔ∏è **Lazy Load Handling**: Waits for images to fully load, ensuring no content is missed due to lazy loading.
- üîÑ **Full-Page Scanning**: Simulates scrolling to load and capture all dynamic content, perfect for infinite scroll pages.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üöÄ &lt;strong&gt;Deployment&lt;/strong&gt;&lt;/summary&gt;

- üê≥ **Dockerized Setup**: Optimized Docker image with FastAPI server for easy deployment.
- üîë **Secure Authentication**: Built-in JWT token authentication for API security.
- üîÑ **API Gateway**: One-click deployment with secure token authentication for API-based workflows.
- üåê **Scalable Architecture**: Designed for mass-scale production and optimized server performance.
- ‚òÅÔ∏è **Cloud Deployment**: Ready-to-deploy configurations for major cloud platforms.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üéØ &lt;strong&gt;Additional Features&lt;/strong&gt;&lt;/summary&gt;

- üï∂Ô∏è **Stealth Mode**: Avoid bot detection by mimicking real users.
- üè∑Ô∏è **Tag-Based Content Extraction**: Refine crawling based on custom tags, headers, or metadata.
- üîó **Link Analysis**: Extract and analyze all links for detailed data exploration.
- üõ°Ô∏è **Error Handling**: Robust error management for seamless execution.
- üîê **CORS &amp; Static Serving**: Supports filesystem-based caching and cross-origin requests.
- üìñ **Clear Documentation**: Simplified and updated guides for onboarding and advanced usage.
- üôå **Community Recognition**: Acknowledges contributors and pull requests for transparency.

&lt;/details&gt;

## Try it Now!

‚ú® Play around with this [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1SgRPrByQLzjRfwoRNq1wSGE9nYY_EE8C?usp=sharing)

‚ú® Visit our [Documentation Website](https://docs.crawl4ai.com/)

## Installation üõ†Ô∏è

Crawl4AI offers flexible installation options to suit various use cases. You can install it as a Python package or use Docker.

&lt;details&gt;
&lt;summary&gt;üêç &lt;strong&gt;Using pip&lt;/strong&gt;&lt;/summary&gt;

Choose the installation option that best fits your needs:

### Basic Installation

For basic web crawling and scraping tasks:

```bash
pip install crawl4ai
crawl4ai-setup # Setup the browser
```

By default, this will install the asynchronous version of Crawl4AI, using Playwright for web crawling.

üëâ **Note**: When you install Crawl4AI, the `crawl4ai-setup` should automatically install and set up Playwright. However, if you encounter any Playwright-related errors, you can manually install it using one of these methods:

1. Through the command line:

   ```bash
   playwright install
   ```

2. If the above doesn&#039;t work, try this more specific command:

   ```bash
   python -m playwright install chromium
   ```

This second method has proven to be more reliable in some cases.

---

### Installation with Synchronous Version

The sync version is deprecated and will be removed in future versions. If you need the synchronous version using Selenium:

```bash
pip install crawl4ai[sync]
```

---

### Development Installation

For contributors who plan to modify the source code:

```bash
git clone https://github.com/unclecode/crawl4ai.git
cd crawl4ai
pip install -e .                    # Basic installation in editable mode
```

Install optional features:

```bash
pip install -e &quot;.[torch]&quot;           # With PyTorch features
pip install -e &quot;.[transformer]&quot;     # With Transformer features
pip install -e &quot;.[cosine]&quot;          # With cosine similarity features
pip install -e &quot;.[sync]&quot;            # With synchronous crawling (Selenium)
pip install -e &quot;.[all]&quot;             # Install all optional features
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üê≥ &lt;strong&gt;Docker Deployment&lt;/strong&gt;&lt;/summary&gt;

&gt; üöÄ **Now Available!** Our completely redesigned Docker implementation is here! This new solution makes deployment more efficient and seamless than ever.

### New Docker Features

The new Docker implementation includes:
- **Browser pooling** with page pre-warming for faster response times
- **Interactive playground** to test and generate request code
- **MCP integration** for direct connection to AI tools like Claude Code
- **Comprehensive API endpoints** including HTML extraction, screenshots, PDF generation, and JavaScript execution
- **Multi-architecture support** with automatic detection (AMD64/ARM64)
- **Optimized resources** with improved memory management

### Getting Started

```bash
# Pull and run the latest release candidate
docker pull unclecode/crawl4ai:0.6.0-rN # Use your favorite revision number
docker run -d -p 11235:11235 --name crawl4ai --shm-size=1g unclecode/crawl4ai:0.6.0-rN # Use your favorite revision number

# Visit the playground at http://localhost:11235/playground
```

For complete documentation, see our [Docker Deployment Guide](https://docs.crawl4ai.com/core/docker-deployment/).

&lt;/details&gt;

---

### Quick Test

Run a quick test (works for both Docker options):

```python
import requests

# Submit a crawl job
response = requests.post(
    &quot;http://localhost:11235/crawl&quot;,
    json={&quot;urls&quot;: &quot;https://example.com&quot;, &quot;priority&quot;: 10}
)
task_id = response.json()[&quot;task_id&quot;]

# Continue polling until the task is complete (status=&quot;completed&quot;)
result = requests.get(f&quot;http://localhost:11235/task/{task_id}&quot;)
```

For more examples, see our [Docker Examples](https://github.com/unclecode/crawl4ai/blob/main/docs/examples/docker_example.py). For advanced configuration, environment variables, and usage examples, see our [Docker Deployment Guide](https://docs.crawl4ai.com/basic/docker-deployment/).

&lt;/details&gt;


## üî¨ Advanced Usage Examples üî¨

You can check the project structure in the directory [https://github.com/unclecode/crawl4ai/docs/examples](docs/examples). Over there, you can find a variety of examples; here, some popular examples are shared.

&lt;details&gt;
&lt;summary&gt;üìù &lt;strong&gt;Heuristic Markdown Generation with Clean and Fit Markdown&lt;/strong&gt;&lt;/summary&gt;

```python
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.content_filter_strategy import PruningContentFilter, BM25ContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

async def main():
    browser_config = BrowserConfig(
        headless=True,  
        verbose=True,
    )
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.ENABLED,
        markdown_generator=DefaultMarkdownGenerator(
            content_filter=PruningContentFilter(threshold=0.48, threshold_type=&quot;fixed&quot;, min_word_threshold=0)
        ),
        # markdown_generator=DefaultMarkdownGenerator(
        #     content_filter=BM25ContentFilter(user_query=&quot;WHEN_WE_FOCUS_BASED_ON_A_USER_QUERY&quot;, bm25_threshold=1.0)
        # ),
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url=&quot;https://docs.micronaut.io/4.7.6/guide/&quot;,
            config=run_config
        )
        print(len(result.markdown.raw_markdown))
        print(len(result.markdown.fit_markdown))

if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üñ•Ô∏è &lt;strong&gt;Executing JavaScript &amp; Extract Structured Data without LLMs&lt;/strong&gt;&lt;/summary&gt;

```python
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
import json

async def main():
    schema = {
    &quot;name&quot;: &quot;KidoCode Courses&quot;,
    &quot;baseSelector&quot;: &quot;section.charge-methodology .w-tab-content &gt; div&quot;,
    &quot;fields&quot;: [
        {
            &quot;name&quot;: &quot;section_title&quot;,
            &quot;selector&quot;: &quot;h3.heading-50&quot;,
            &quot;type&quot;: &quot;text&quot;,
        },
        {
            &quot;name&quot;: &quot;section_description&quot;,
            &quot;selector&quot;: &quot;.charge-content&quot;,
            &quot;type&quot;: &quot;text&quot;,
        },
        {
            &quot;name&quot;: &quot;course_name&quot;,
            &quot;selector&quot;: &quot;.text-block-93&quot;,
            &quot;type&quot;: &quot;text&quot;,
        },
        {
            &quot;name&quot;: &quot;course_description&quot;,
            &quot;selector&quot;: &quot;.course-content-text&quot;,
            &quot;type&quot;: &quot;text&quot;,
        },
        {
            &quot;name&quot;: &quot;course_icon&quot;,
            &quot;selector&quot;: &quot;.image-92&quot;,
            &quot;type&quot;: &quot;attribute&quot;,
            &quot;attribute&quot;: &quot;src&quot;
        }
    }
}

    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)

    browser_config = BrowserConfig(
        headless=False,
        verbose=True
    )
    run_config = CrawlerRunConfig(
        extraction_strategy=extraction_strategy,
        js_code=[&quot;&quot;&quot;(async () =&gt; {const tabs = document.querySelectorAll(&quot;section.charge-methodology .tabs-menu-3 &gt; div&quot;);for(let tab of tabs) {tab.scrollIntoView();tab.click();await new Promise(r =&gt; setTimeout(r, 500));}})();&quot;&quot;&quot;],
        cache_mode=CacheMode.BYPASS
    )
        
    async with AsyncWebCrawler(config=browser_config) as crawler:
        
        result = await crawler.arun(
            url=&quot;https://www.kidocode.com/degrees/technology&quot;,
            config=run_config
        )

        companies = json.loads(result.extracted_content)
        print(f&quot;Successfully extracted {len(companies)} companies&quot;)
        print(json.dumps(companies[0], indent=2))


if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;üìö &lt;strong&gt;Extracting Structured Data with LLMs&lt;/strong&gt;&lt;/summary&gt;

```python
import os
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel, Field

class OpenAIModelFee(BaseModel):
    model_name: str = Field(..., description=&quot;Name of the OpenAI model.&quot;)
    input_fee: str = Field(..., description=&quot;Fee for input token for the OpenAI model.&quot;)
    output_fee: str = Field(..., description=&quot;Fee for output token for the OpenAI model.&quot;)

async def main():
    browser_config = BrowserConfig(verbose=True)
    run_config = CrawlerRunConfig(
        word_count_threshold=1,
        extraction_strategy=LLMExtractionStrategy(
            # Here you can use any provider that Litellm library supports, for instance: ollama/qwen2
            # provider=&quot;ollama/qwen2&quot;, api_token=&quot;no-token&quot;, 
            llm_config = LLMConfig(provider=&quot;openai/gpt-4o&quot;, api_token=os.getenv(&#039;OPENAI_API_KEY&#039;)), 
            schema=OpenAIModelFee.schema(),
            extraction_type=&quot;schema&quot;,
            instruction=&quot;&quot;&quot;From the crawled content, extract all mentioned model names along with their fees for input and output tokens. 
            Do not miss any models in the entire content. One extracted model JSON format should look like this: 
            {&quot;model_name&quot;: &quot;GPT-4&quot;, &quot;input_fee&quot;: &quot;US$10.00 / 1M tokens&quot;, &quot;output_fee&quot;: &quot;US$30.00 / 1M tokens&quot;}.&quot;&quot;&quot;
        ),            
     

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[aipotheosis-labs/aci]]></title>
            <link>https://github.com/aipotheosis-labs/aci</link>
            <guid>https://github.com/aipotheosis-labs/aci</guid>
            <pubDate>Thu, 01 May 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[ACI.dev is the open source platform that connects your AI agents to 600+ tool integrations with multi-tenant auth, granular permissions, and access through direct function calling or a unified MCP server.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/aipotheosis-labs/aci">aipotheosis-labs/aci</a></h1>
            <p>ACI.dev is the open source platform that connects your AI agents to 600+ tool integrations with multi-tenant auth, granular permissions, and access through direct function calling or a unified MCP server.</p>
            <p>Language: Python</p>
            <p>Stars: 710</p>
            <p>Forks: 52</p>
            <p>Stars today: 237 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;frontend/public/aci-dev-full-logo.svg&quot; alt=&quot;ACI.dev Logo&quot; width=&quot;100%&quot;&gt;
&lt;/p&gt;

# ACI: Open-Source Infra to Power Unified MCP Servers

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://aci.dev/docs&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Documentation-34a1bf&quot; alt=&quot;Documentation&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/aipotheosis-labs/aci/actions/workflows/devportal.yml&quot;&gt;&lt;img src=&quot;https://github.com/aipotheosis-labs/aci/actions/workflows/devportal.yml/badge.svg&quot; alt=&quot;Dev Portal CI&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/aipotheosis-labs/aci/actions/workflows/backend.yml&quot;&gt;&lt;img src=&quot;https://github.com/aipotheosis-labs/aci/actions/workflows/backend.yml/badge.svg&quot; alt=&quot;Backend CI&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://badge.fury.io/py/aci-sdk&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/aci-sdk.svg&quot; alt=&quot;PyPI version&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://opensource.org/licenses/Apache-2.0&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-Apache_2.0-blue.svg&quot; alt=&quot;License&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.com/invite/UU2XAnfHJh&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join_Chat-7289DA.svg?logo=discord&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://x.com/AipoLabs&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/AipoLabs?style=social&quot; alt=&quot;Twitter Follow&quot;&gt;&lt;/a&gt;

&lt;/p&gt;

&gt; [!NOTE]
&gt; This repo is for the ACI.dev platform. If you&#039;re looking for the **Unified MCP** server built with ACI.dev, see [aci-mcp](https://github.com/aipotheosis-labs/aci-mcp).

ACI.dev is the open-source infrastructure layer for AI-agent tool-use. It gives your agents intent-aware access to 600+ tools with multi-tenant auth, granular permissions, and dynamic tool discovery‚Äîexposed as either direct function calls or through a **Unified Model-Context-Protocol (MCP) server**.

**Example:** Instead of writing separate OAuth flows and API clients for Google Calendar, Slack, and more, use ACI.dev to manage authentication and provide your AI agents with unified, secure function calls. Access these capabilities through our **Unified** [MCP server](https://github.com/aipotheosis-labs/aci-mcp) or via our lightweight [Python SDK](https://github.com/aipotheosis-labs/aci-python-sdk), compatible with any LLM framework.

Build production-ready AI agents without the infrastructure headaches.

![ACI.dev Architecture](frontend/public/aci-architecture-intro.svg)

&lt;p align=&quot;center&quot;&gt;
  Join us on &lt;a href=&quot;https://discord.com/invite/UU2XAnfHJh&quot;&gt;Discord&lt;/a&gt; to help shape the future of Open Source AI Infrastructure.&lt;br/&gt;&lt;br/&gt;
  üåü &lt;strong&gt;Star ACI.dev to stay updated on new releases!&lt;/strong&gt;&lt;br/&gt;&lt;br/&gt;
  &lt;a href=&quot;https://github.com/aipotheosis-labs/aci/stargazers&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/aipotheosis-labs/aci?style=social&quot; alt=&quot;GitHub Stars&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

## üì∫ Demo Video

[ACI.dev **Unified MCP Server** Demo](https://youtu.be/GSR9P53-_7E?feature=shared)

[![ACI.dev Unified MCP Server Demo](frontend/public/umcp-demo-thumbnail.png)](https://youtu.be/GSR9P53-_7E?feature=shared)

## ‚ú® Key Features

- **600+ Pre-built Integrations**: Connect to popular services and apps in minutes.
- **Flexible Access Methods**: Use our unified MCP server or our lightweight SDK for direct function calling.
- **Multi-tenant Authentication**: Built-in OAuth flows and secrets management for both developers and end-users.
- **Enhanced Agent Reliability**: Natural language permission boundaries and dynamic tool discovery.
- **Framework &amp; Model Agnostic**: Works with any LLM framework and agent architecture.
- **100% Open Source**: Everything released under Apache 2.0 (backend, dev portal, integrations).

## üí° Why Use ACI.dev?

ACI.dev solves your critical infrastructure challenges for production-ready AI agents:

- **Authentication at Scale**: Connect multiple users to multiple services securely.
- **Discovery Without Overload**: Find and use the right tools without overwhelming LLM context windows.
- **Natural Language Permissions**: Control agent capabilities with human-readable boundaries.
- **Build Once, Run Anywhere**: No vendor lock-in with our open source, framework-agnostic approach.

## üß∞ Common Use Cases

- **Personal Assistant Chatbots:** Build chatbots that can search the web, manage calendars, send emails, interact with SaaS tools, etc.
- **Research Agent:** Conducts research on specific topics and syncs results to other apps (e.g., Notion, Google Sheets).
- **Outbound Sales Agent:** Automates lead generation, email outreach, and CRM updates.
- **Customer Support Agent:** Provides answers, manages tickets, and performs actions based on customer queries.

## üîó Quick Links

- **Managed Service:** [aci.dev](https://www.aci.dev/)
- **Documentation:** [aci.dev/docs](https://www.aci.dev/docs)
- **Available Tools List:** [aci.dev/tools](https://www.aci.dev/tools)
- **Python SDK:** [github.com/aipotheosis-labs/aci-python-sdk](https://github.com/aipotheosis-labs/aci-python-sdk)
- **Unified MCP Server:** [github.com/aipotheosis-labs/aci-mcp](https://github.com/aipotheosis-labs/aci-mcp)
- **Agent Examples Built with ACI.dev:** [github.com/aipotheosis-labs/aci-agents](https://github.com/aipotheosis-labs/aci-agents)
- **Blog:** [aci.dev/blog](https://www.aci.dev/blog)
- **Community:** [Discord](https://discord.com/invite/UU2XAnfHJh) | [Twitter/X](https://x.com/AipoLabs) | [LinkedIn](https://www.linkedin.com/company/aipotheosis-labs-aipolabs/posts/?feedView=all)

## üíª Getting Started: Local Development

To run the full ACI.dev platform (backend server and frontend portal) locally, follow the individual README files for each component:

- **Backend:** [backend/README.md](backend/README.md)
- **Frontend:** [frontend/README.md](frontend/README.md)

## üëã Contributing

We welcome contributions! Please see our [CONTRIBUTING.md](CONTRIBUTING.md) for more information.

## Integration Requests

Missing any integrations (apps or functions) you need? Please see our [Integration Request Template](.github/ISSUE_TEMPLATE/integration_request.yml) and submit an integration request! Or, if you&#039;re feeling adventurous, you can submit a PR to add the integration yourself!
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[huggingface/lerobot]]></title>
            <link>https://github.com/huggingface/lerobot</link>
            <guid>https://github.com/huggingface/lerobot</guid>
            <pubDate>Thu, 01 May 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[ü§ó LeRobot: Making AI for Robotics more accessible with end-to-end learning]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/huggingface/lerobot">huggingface/lerobot</a></h1>
            <p>ü§ó LeRobot: Making AI for Robotics more accessible with end-to-end learning</p>
            <p>Language: Python</p>
            <p>Stars: 12,990</p>
            <p>Forks: 1,488</p>
            <p>Stars today: 113 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;media/lerobot-logo-thumbnail.png&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;media/lerobot-logo-thumbnail.png&quot;&gt;
    &lt;img alt=&quot;LeRobot, Hugging Face Robotics Library&quot; src=&quot;media/lerobot-logo-thumbnail.png&quot; style=&quot;max-width: 100%;&quot;&gt;
  &lt;/picture&gt;
  &lt;br/&gt;
  &lt;br/&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

[![Tests](https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml/badge.svg?branch=main)](https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml?query=branch%3Amain)
[![Coverage](https://codecov.io/gh/huggingface/lerobot/branch/main/graph/badge.svg?token=TODO)](https://codecov.io/gh/huggingface/lerobot)
[![Python versions](https://img.shields.io/pypi/pyversions/lerobot)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/huggingface/lerobot/blob/main/LICENSE)
[![Status](https://img.shields.io/pypi/status/lerobot)](https://pypi.org/project/lerobot/)
[![Version](https://img.shields.io/pypi/v/lerobot)](https://pypi.org/project/lerobot/)
[![Examples](https://img.shields.io/badge/Examples-green.svg)](https://github.com/huggingface/lerobot/tree/main/examples)
[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v2.1%20adopted-ff69b4.svg)](https://github.com/huggingface/lerobot/blob/main/CODE_OF_CONDUCT.md)
[![Discord](https://dcbadge.vercel.app/api/server/C5P34WJ68S?style=flat)](https://discord.gg/s3KuuzsPFb)

&lt;/div&gt;

&lt;h2 align=&quot;center&quot;&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/huggingface/lerobot/blob/main/examples/12_use_so101.md&quot;&gt;
        Build Your Own SO-101 Robot!&lt;/a&gt;&lt;/p&gt;
&lt;/h2&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;div style=&quot;display: flex; gap: 1rem; justify-content: center; align-items: center;&quot; &gt;
    &lt;img
      src=&quot;media/so101/so101.webp?raw=true&quot;
      alt=&quot;SO-101 follower arm&quot;
      title=&quot;SO-101 follower arm&quot;
      style=&quot;width: 40%;&quot;
    /&gt;
    &lt;img
      src=&quot;media/so101/so101-leader.webp?raw=true&quot;
      alt=&quot;SO-101 leader arm&quot;
      title=&quot;SO-101 leader arm&quot;
      style=&quot;width: 40%;&quot;
    /&gt;
  &lt;/div&gt;


  &lt;p&gt;&lt;strong&gt;Meet the updated SO100, the SO-101 ‚Äì Just ‚Ç¨114 per arm!&lt;/strong&gt;&lt;/p&gt;
  &lt;p&gt;Train it in minutes with a few simple moves on your laptop.&lt;/p&gt;
  &lt;p&gt;Then sit back and watch your creation act autonomously! ü§Ø&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;https://github.com/huggingface/lerobot/blob/main/examples/12_use_so101.md&quot;&gt;
      See the full SO-101 tutorial here.&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;Want to take it to the next level? Make your SO-101 mobile by building LeKiwi!&lt;/p&gt;
  &lt;p&gt;Check out the &lt;a href=&quot;https://github.com/huggingface/lerobot/blob/main/examples/11_use_lekiwi.md&quot;&gt;LeKiwi tutorial&lt;/a&gt; and bring your robot to life on wheels.&lt;/p&gt;

  &lt;img src=&quot;media/lekiwi/kiwi.webp?raw=true&quot; alt=&quot;LeKiwi mobile robot&quot; title=&quot;LeKiwi mobile robot&quot; width=&quot;50%&quot;&gt;
&lt;/div&gt;

&lt;br/&gt;

&lt;h3 align=&quot;center&quot;&gt;
    &lt;p&gt;LeRobot: State-of-the-art AI for real-world robotics&lt;/p&gt;
&lt;/h3&gt;

---

ü§ó LeRobot aims to provide models, datasets, and tools for real-world robotics in PyTorch. The goal is to lower the barrier to entry to robotics so that everyone can contribute and benefit from sharing datasets and pretrained models.

ü§ó LeRobot contains state-of-the-art approaches that have been shown to transfer to the real-world with a focus on imitation learning and reinforcement learning.

ü§ó LeRobot already provides a set of pretrained models, datasets with human collected demonstrations, and simulation environments to get started without assembling a robot. In the coming weeks, the plan is to add more and more support for real-world robotics on the most affordable and capable robots out there.

ü§ó LeRobot hosts pretrained models and datasets on this Hugging Face community page: [huggingface.co/lerobot](https://huggingface.co/lerobot)

#### Examples of pretrained models on simulation environments

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;img src=&quot;media/gym/aloha_act.gif&quot; width=&quot;100%&quot; alt=&quot;ACT policy on ALOHA env&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;media/gym/simxarm_tdmpc.gif&quot; width=&quot;100%&quot; alt=&quot;TDMPC policy on SimXArm env&quot;/&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;media/gym/pusht_diffusion.gif&quot; width=&quot;100%&quot; alt=&quot;Diffusion policy on PushT env&quot;/&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;ACT policy on ALOHA env&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;TDMPC policy on SimXArm env&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;Diffusion policy on PushT env&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

### Acknowledgment

- Thanks to Tony Zhao, Zipeng Fu and colleagues for open sourcing ACT policy, ALOHA environments and datasets. Ours are adapted from [ALOHA](https://tonyzhaozh.github.io/aloha) and [Mobile ALOHA](https://mobile-aloha.github.io).
- Thanks to Cheng Chi, Zhenjia Xu and colleagues for open sourcing Diffusion policy, Pusht environment and datasets, as well as UMI datasets. Ours are adapted from [Diffusion Policy](https://diffusion-policy.cs.columbia.edu) and [UMI Gripper](https://umi-gripper.github.io).
- Thanks to Nicklas Hansen, Yunhai Feng and colleagues for open sourcing TDMPC policy, Simxarm environments and datasets. Ours are adapted from [TDMPC](https://github.com/nicklashansen/tdmpc) and [FOWM](https://www.yunhaifeng.com/FOWM).
- Thanks to Antonio Loquercio and Ashish Kumar for their early support.
- Thanks to [Seungjae (Jay) Lee](https://sjlee.cc/), [Mahi Shafiullah](https://mahis.life/) and colleagues for open sourcing [VQ-BeT](https://sjlee.cc/vq-bet/) policy and helping us adapt the codebase to our repository. The policy is adapted from [VQ-BeT repo](https://github.com/jayLEE0301/vq_bet_official).


## Installation

Download our source code:
```bash
git clone https://github.com/huggingface/lerobot.git
cd lerobot
```

Create a virtual environment with Python 3.10 and activate it, e.g. with [`miniconda`](https://docs.anaconda.com/free/miniconda/index.html):
```bash
conda create -y -n lerobot python=3.10
conda activate lerobot
```

When using `miniconda`, install `ffmpeg` in your environment:
```bash
conda install ffmpeg -c conda-forge
```

&gt; **NOTE:** This usually installs `ffmpeg 7.X` for your platform compiled with the `libsvtav1` encoder. If `libsvtav1` is not supported (check supported encoders with `ffmpeg -encoders`), you can:
&gt;  - _[On any platform]_ Explicitly install `ffmpeg 7.X` using:
&gt;  ```bash
&gt;  conda install ffmpeg=7.1.1 -c conda-forge
&gt;  ```
&gt;  - _[On Linux only]_ Install [ffmpeg build dependencies](https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu#GettheDependencies) and [compile ffmpeg from source with libsvtav1](https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu#libsvtav1), and make sure you use the corresponding ffmpeg binary to your install with `which ffmpeg`.

Install ü§ó LeRobot:
```bash
pip install -e .
```

&gt; **NOTE:** If you encounter build errors, you may need to install additional dependencies (`cmake`, `build-essential`, and `ffmpeg libs`). On Linux, run:
`sudo apt-get install cmake build-essential python3-dev pkg-config libavformat-dev libavcodec-dev libavdevice-dev libavutil-dev libswscale-dev libswresample-dev libavfilter-dev pkg-config`. For other systems, see: [Compiling PyAV](https://pyav.org/docs/develop/overview/installation.html#bring-your-own-ffmpeg)

For simulations, ü§ó LeRobot comes with gymnasium environments that can be installed as extras:
- [aloha](https://github.com/huggingface/gym-aloha)
- [xarm](https://github.com/huggingface/gym-xarm)
- [pusht](https://github.com/huggingface/gym-pusht)

For instance, to install ü§ó LeRobot with aloha and pusht, use:
```bash
pip install -e &quot;.[aloha, pusht]&quot;
```

To use [Weights and Biases](https://docs.wandb.ai/quickstart) for experiment tracking, log in with
```bash
wandb login
```

(note: you will also need to enable WandB in the configuration. See below.)

## Walkthrough

```
.
‚îú‚îÄ‚îÄ examples             # contains demonstration examples, start here to learn about LeRobot
|   ‚îî‚îÄ‚îÄ advanced         # contains even more examples for those who have mastered the basics
‚îú‚îÄ‚îÄ lerobot
|   ‚îú‚îÄ‚îÄ configs          # contains config classes with all options that you can override in the command line
|   ‚îú‚îÄ‚îÄ common           # contains classes and utilities
|   |   ‚îú‚îÄ‚îÄ datasets       # various datasets of human demonstrations: aloha, pusht, xarm
|   |   ‚îú‚îÄ‚îÄ envs           # various sim environments: aloha, pusht, xarm
|   |   ‚îú‚îÄ‚îÄ policies       # various policies: act, diffusion, tdmpc
|   |   ‚îú‚îÄ‚îÄ robot_devices  # various real devices: dynamixel motors, opencv cameras, koch robots
|   |   ‚îî‚îÄ‚îÄ utils          # various utilities
|   ‚îî‚îÄ‚îÄ scripts          # contains functions to execute via command line
|       ‚îú‚îÄ‚îÄ eval.py                 # load policy and evaluate it on an environment
|       ‚îú‚îÄ‚îÄ train.py                # train a policy via imitation learning and/or reinforcement learning
|       ‚îú‚îÄ‚îÄ control_robot.py        # teleoperate a real robot, record data, run a policy
|       ‚îú‚îÄ‚îÄ push_dataset_to_hub.py  # convert your dataset into LeRobot dataset format and upload it to the Hugging Face hub
|       ‚îî‚îÄ‚îÄ visualize_dataset.py    # load a dataset and render its demonstrations
‚îú‚îÄ‚îÄ outputs               # contains results of scripts execution: logs, videos, model checkpoints
‚îî‚îÄ‚îÄ tests                 # contains pytest utilities for continuous integration
```

### Visualize datasets

Check out [example 1](./examples/1_load_lerobot_dataset.py) that illustrates how to use our dataset class which automatically downloads data from the Hugging Face hub.

You can also locally visualize episodes from a dataset on the hub by executing our script from the command line:
```bash
python lerobot/scripts/visualize_dataset.py \
    --repo-id lerobot/pusht \
    --episode-index 0
```

or from a dataset in a local folder with the `root` option and the `--local-files-only` (in the following case the dataset will be searched for in `./my_local_data_dir/lerobot/pusht`)
```bash
python lerobot/scripts/visualize_dataset.py \
    --repo-id lerobot/pusht \
    --root ./my_local_data_dir \
    --local-files-only 1 \
    --episode-index 0
```


It will open `rerun.io` and display the camera streams, robot states and actions, like this:

https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-Date=20240505T172924Z&amp;X-Amz-Expires=300&amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;X-Amz-SignedHeaders=host&amp;actor_id=24889239&amp;key_id=0&amp;repo_id=748713144


Our script can also visualize datasets stored on a distant server. See `python lerobot/scripts/visualize_dataset.py --help` for more instructions.

### The `LeRobotDataset` format

A dataset in `LeRobotDataset` format is very simple to use. It can be loaded from a repository on the Hugging Face hub or a local folder simply with e.g. `dataset = LeRobotDataset(&quot;lerobot/aloha_static_coffee&quot;)` and can be indexed into like any Hugging Face and PyTorch dataset. For instance `dataset[0]` will retrieve a single temporal frame from the dataset containing observation(s) and an action as PyTorch tensors ready to be fed to a model.

A specificity of `LeRobotDataset` is that, rather than retrieving a single frame by its index, we can retrieve several frames based on their temporal relationship with the indexed frame, by setting `delta_timestamps` to a list of relative times with respect to the indexed frame. For example, with `delta_timestamps = {&quot;observation.image&quot;: [-1, -0.5, -0.2, 0]}`  one can retrieve, for a given index, 4 frames: 3 &quot;previous&quot; frames 1 second, 0.5 seconds, and 0.2 seconds before the indexed frame, and the indexed frame itself (corresponding to the 0 entry). See example [1_load_lerobot_dataset.py](examples/1_load_lerobot_dataset.py) for more details on `delta_timestamps`.

Under the hood, the `LeRobotDataset` format makes use of several ways to serialize data which can be useful to understand if you plan to work more closely with this format. We tried to make a flexible yet simple dataset format that would cover most type of features and specificities present in reinforcement learning and robotics, in simulation and in real-world, with a focus on cameras and robot states but easily extended to other types of sensory inputs as long as they can be represented by a tensor.

Here are the important details and internal structure organization of a typical `LeRobotDataset` instantiated with `dataset = LeRobotDataset(&quot;lerobot/aloha_static_coffee&quot;)`. The exact features will change from dataset to dataset but not the main aspects:

```
dataset attributes:
  ‚îú hf_dataset: a Hugging Face dataset (backed by Arrow/parquet). Typical features example:
  ‚îÇ  ‚îú observation.images.cam_high (VideoFrame):
  ‚îÇ  ‚îÇ   VideoFrame = {&#039;path&#039;: path to a mp4 video, &#039;timestamp&#039; (float32): timestamp in the video}
  ‚îÇ  ‚îú observation.state (list of float32): position of an arm joints (for instance)
  ‚îÇ  ... (more observations)
  ‚îÇ  ‚îú action (list of float32): goal position of an arm joints (for instance)
  ‚îÇ  ‚îú episode_index (int64): index of the episode for this sample
  ‚îÇ  ‚îú frame_index (int64): index of the frame for this sample in the episode ; starts at 0 for each episode
  ‚îÇ  ‚îú timestamp (float32): timestamp in the episode
  ‚îÇ  ‚îú next.done (bool): indicates the end of en episode ; True for the last frame in each episode
  ‚îÇ  ‚îî index (int64): general index in the whole dataset
  ‚îú episode_data_index: contains 2 tensors with the start and end indices of each episode
  ‚îÇ  ‚îú from (1D int64 tensor): first frame index for each episode ‚Äî shape (num episodes,) starts with 0
  ‚îÇ  ‚îî to: (1D int64 tensor): last frame index for each episode ‚Äî shape (num episodes,)
  ‚îú stats: a dictionary of statistics (max, mean, min, std) for each feature in the dataset, for instance
  ‚îÇ  ‚îú observation.images.cam_high: {&#039;max&#039;: tensor with same number of dimensions (e.g. `(c, 1, 1)` for images, `(c,)` for states), etc.}
  ‚îÇ  ...
  ‚îú info: a dictionary of metadata on the dataset
  ‚îÇ  ‚îú codebase_version (str): this is to keep track of the codebase version the dataset was created with
  ‚îÇ  ‚îú fps (float): frame per second the dataset is recorded/synchronized to
  ‚îÇ  ‚îú video (bool): indicates if frames are encoded in mp4 video files to save space or stored as png files
  ‚îÇ  ‚îî encoding (dict): if video, this documents the main options that were used with ffmpeg to encode the videos
  ‚îú videos_dir (Path): where the mp4 videos or png images are stored/accessed
  ‚îî camera_keys (list of string): the keys to access camera features in the item returned by the dataset (e.g. `[&quot;observation.images.cam_high&quot;, ...]`)
```

A `LeRobotDataset` is serialised using several widespread file formats for each of its parts, namely:
- hf_dataset stored using Hugging Face datasets library serialization to parquet
- videos are stored in mp4 format to save space
- metadata are stored in plain json/jsonl files

Dataset can be uploaded/downloaded from the HuggingFace hub seamlessly. To work on a local dataset, you can specify its location with the `root` argument if it&#039;s not in the default `~/.cache/huggingface/lerobot` location.

### Evaluate a pretrained policy

Check out [example 2](./examples/2_evaluate_pretrained_policy.py) that illustrates how to download a pretrained policy from Hugging Face hub, and run an evaluation on its corresponding environment.

We also provide a more capable script to parallelize the evaluation over multiple environments during the same rollout. Here is an example with a pretrained model hosted on [lerobot/diffusion_pusht](https://huggingface.co/lerobot/diffusion_pusht):
```bash
python lerobot/scripts/eval.py \
    --policy.path=lerobot/diffusion_pusht \
    --env.type=pusht \
    --eval.batch_size=10 \
    --eval.n_episodes=10 \
    --policy.use_amp=false \
    --policy.device=cuda
```

Note: After training your own policy, you can re-evaluate the checkpoints with:

```bash
python lerobot/scripts/eval.py --policy.path={OUTPUT_DIR}/checkpoints/last/pretrained_model
```

See `python lerobot/scripts/eval.py --help` for more instructions.

### Train your own policy

Check out [example 3](./examples/3_train_policy.py) that illustrate how to train a model using our core library in python, and [example 4](./examples/4_train_policy_with_script.md) that shows how to use our training script from command line.

To use wandb for logging training and evaluation curves, make sure you&#039;ve run `wandb login` as a one-time setup step. Then, when running the training command above, enable WandB in the configuration by adding `--wandb.enable=true`.

A link to the wandb logs for the run will also show up in yellow in your terminal. Here is an example of what they look like in your browser. Please also check [here](./examples/4_train_policy_with_script.md#typical-logs-and-metrics) for the explanation of some commonly used metrics in logs.

![](media/wandb.png)

Note: For efficiency, during training every checkpoint is evaluated on a low number of episodes. You may use `--eval.n_episodes=500` to evaluate on more episodes than the default. Or, after training, you may want to re-evaluate your best checkpoints on more episodes or change the evaluation settings. See `python lerobot/scripts/eval.py --help` for more instructions.

#### Reproduce state-of-the-art (SOTA)

We provide some pretrained policies on our [hub page](https://huggingface.co/lerobot) that can achieve state-of-the-art performances.
You can reproduce their training by loading the config from their run. Simply running:
```bash
python lerobot/scripts/train.py --config_path=lerobot/diffusion_pusht
```
reproduces SOTA results for Diffusion Policy on the PushT task.

## Contribute

If you would like to contribute to ü§ó LeRobot, please check out our [contribution guide](https://github.com/huggingface/lerobot/blob/main/CONTRIBUTING.md).

&lt;!-- ### Add a new dataset

To add a dataset to the hub, you need to login using a write-access token, which can be generated from the [Hugging Face settings](https://huggingface.co/settings/tokens):
```bash
huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential
```

Then point to your raw dataset folder (e.g. `data/aloha_static_pingpong_test_raw`), and push your dataset to the hub with:
```bash
python lerobot/scripts/push_dataset_to_hub.py \
--raw-dir data/aloha_static_pingpong_test_raw \
--out-dir data \
--repo-id lerobot/aloha_static_pingpong_test \
--raw-format aloha_hdf5
```

See `python lerobot/scripts/push_dataset_to_hub.py --help` for more instructions.

If your dataset format is not supported, implement your own in `lerobot/common/datasets/push_dataset_to_hub/${raw_format}_format.py` by copying examples like [pusht_zarr](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/pusht_zarr_format.py), [umi_zarr](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/umi_zarr_format.py), [aloha_hdf5](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/aloha_hdf5_format.py), or [xarm_pkl](https://github.com/huggingface/lerobot/blob/main/lerobot/common/datasets/push_dataset_to_hub/xarm_pkl_format.py). --&gt;


### Add a pretrained policy

Once you have trained a policy you may upload it to the Hugging Face hub using a hub id that looks like `${hf_user}/${repo_name}` (e.g. [lerobot/diffusion_pusht](https://huggingface.co/lerobot/diffusion_pusht)).

You first need to find the checkpoint folder located inside your experiment directory (e.g. `outputs/train/2024-05-05/20-21-12_aloha_act_default/checkpoints/002500`). Within that there is a `pretrained_model` directory which should contain:
- `config.json`: A serialized version of the policy configuration (following the policy&#039;s dataclass config).
- `model.safetensors`: A set of `torch.nn.Module` parameters, saved in [Hugging Face Safetensors](https://huggingface

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>