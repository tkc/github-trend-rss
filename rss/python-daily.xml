<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Fri, 21 Mar 2025 00:04:28 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[subframe7536/maple-font]]></title>
            <link>https://github.com/subframe7536/maple-font</link>
            <guid>https://github.com/subframe7536/maple-font</guid>
            <pubDate>Fri, 21 Mar 2025 00:04:28 GMT</pubDate>
            <description><![CDATA[Maple Mono: Open source monospace font with round corner, ligatures and Nerd-Font for IDE and terminal, fine-grained customization options. Â∏¶ËøûÂ≠óÂíåÊéßÂà∂Âè∞ÂõæÊ†áÁöÑÂúÜËßíÁ≠âÂÆΩÂ≠ó‰ΩìÔºå‰∏≠Ëã±ÊñáÂÆΩÂ∫¶ÂÆåÁæé2:1ÔºåÁªÜÁ≤íÂ∫¶ÁöÑËá™ÂÆö‰πâÈÄâÈ°π]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/subframe7536/maple-font">subframe7536/maple-font</a></h1>
            <p>Maple Mono: Open source monospace font with round corner, ligatures and Nerd-Font for IDE and terminal, fine-grained customization options. Â∏¶ËøûÂ≠óÂíåÊéßÂà∂Âè∞ÂõæÊ†áÁöÑÂúÜËßíÁ≠âÂÆΩÂ≠ó‰ΩìÔºå‰∏≠Ëã±ÊñáÂÆΩÂ∫¶ÂÆåÁæé2:1ÔºåÁªÜÁ≤íÂ∫¶ÁöÑËá™ÂÆö‰πâÈÄâÈ°π</p>
            <p>Language: Python</p>
            <p>Stars: 10,913</p>
            <p>Forks: 229</p>
            <p>Stars today: 829 stars today</p>
            <h2>README</h2><pre>&lt;br&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./img/head.svg&quot; height=&quot;230&quot; alt=&quot;logo&quot;&gt;
&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt; Maple Font &lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
Open source monospace &amp; nerd font with round corners and ligatures.
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/subframe7536/Maple-font/releases&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/v/release/subframe7536/Maple-font?display_name=tag&quot; alt=&quot;release version&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;#install&quot;&gt;install&lt;/a&gt; |
  &lt;a href=&quot;https://github.com/users/subframe7536/projects/1&quot;&gt;what&#039;s next&lt;/a&gt; |
  English |
  &lt;a href=&quot;./README_CN.md&quot;&gt;‰∏≠Êñá&lt;/a&gt;
&lt;/p&gt;

## Preparing for [V7](https://github.com/subframe7536/maple-font/tree/variable), try the new variable font at [latest release](https://github.com/subframe7536/maple-font/releases)

## Features

Inspired by [Source Code Pro](https://github.com/adobe-fonts/source-code-pro), [Fira Code Retina](https://github.com/tonsky/FiraCode), [Sarasa Mono SC Nerd](https://github.com/laishulu/Sarasa-Mono-SC-Nerd) and so on, but:

- üé® **New shape** - such as `@ # $ % &amp;` and new shape of italic style
- ü§ôüèª **More ligatures** - such as `.., ..., /*, /**`
- üì¶ **Small size** - leave only contains Latin, standard set of accents, table control characters and few symbols
- ü¶æ **Better rendering effect** - redesigned it according to Fira Code Retina&#039;s spacing and glyph

  |                           v4                           |                           v5                            |
  | :----------------------------------------------------: | :-----------------------------------------------------: |
  | &lt;img src=&quot;./img/sizechange.gif&quot; height=&quot;200&quot; alt=&quot;v4&quot;&gt; | &lt;img src=&quot;./img/sizechange1.gif&quot; height=&quot;200&quot; alt=&quot;v5&quot;&gt; |
  |     `+` and `=` are not centered at some font-size     |             `+` and `=` are always centered             |

- üóí **More readable** - cursive style, better glyph shape, lower the height of capital letters and numbers, reduce or modify kerning and center operators `+ - * = ^ ~ &lt; &gt;`
- üõ†Ô∏è **More configurable** - enable or disable font features as you want, just make your own font
- ‚ú® See it in [screenshots](#screenshots)



## Install

### V6

| Platform   | Command                                                                          |
| :--------- | :------------------------------------------------------------------------------- |
| macOS      | `brew install --cask font-maple`                                                 |
| Arch Linux | `paru -S ttf-maple`                                                              |
| Others     | Download in [releases](https://github.com/subframe7536/Maple-font/releases/v6.4) |

### V7 Beta

| Platform   | Command                  |
| :--------- | :----------------------- |
| Arch Linux | `paru -S ttf-maple-beta` |


## Notice


Because I don&#039;t have a Mac OS machine, this is the greatest adaption I can do with Mac OS currently, but I can&#039;t test whether it works.

My ability is not enough to solve other problems on Mac OS. I will record the problem and try to solve it, and **PR welcome!**

`Maple Mono NF` now maybe can&#039;t be recognized as Mono, and I try my best but it doesn&#039;t work orz


## Overview

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;./img/base.png&quot; /&gt;&lt;br&gt;
&lt;img src=&quot;./img/ligature.png&quot; /&gt;&lt;br&gt;
&lt;img src=&quot;./img/ligature.gif&quot;/&gt;&lt;br&gt;
multiple ways to get TODO tag&lt;br&gt;
ps: in JetBrains&#039; product, [todo) can&#039;t be properly rendered, so please use todo))&lt;br&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;./img/option.png&quot;/&gt;&lt;br&gt;
&lt;h3 align=&quot;center&quot;&gt;font features are different in V7, see in &lt;a href=&quot;https://github.com/subframe7536/maple-font/tree/variable?tab=readme-ov-file#features&quot;&gt;docs&lt;/h3&gt;&lt;br/&gt;
Compatibility &amp; usage: in &lt;a href=&quot;https://github.com/tonsky/FiraCode#editor-compatibility-list&quot; target=&quot;_blank&quot;&gt;FiraCode README&lt;/a&gt;
&lt;/p&gt;

## Screenshots

Code theme: [vscode-theme-maple](https://github.com/subframe7536/vscode-theme-maple)

~~generate by: [VSCodeSnap](https://github.com/luisllamasbinaburo/VSCodeSnap)~~ Seems deprecated, so I made a new one: [CodeImg](https://github.com/subframe7536/vscode-codeimg)

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Cli (click to expand!)&lt;/b&gt;&lt;/summary&gt;

![](img/code_sample/cli.webp)

&lt;/details&gt;



&lt;details&gt;
&lt;summary&gt;&lt;b&gt;React&lt;/b&gt;&lt;/summary&gt;

![](img/code_sample/react.webp)

&lt;/details&gt;



&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Vue&lt;/b&gt;&lt;/summary&gt;

![](img/code_sample/vue.webp)

&lt;/details&gt;


&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Java&lt;/b&gt;&lt;/summary&gt;

![](img/code_sample/java.webp)

&lt;/details&gt;


&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Go&lt;/b&gt;&lt;/summary&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;img/code_sample/go.webp&quot; width=&quot;540px&quot;/&gt;
&lt;/p&gt;

&lt;/details&gt;


&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Python&lt;/b&gt;&lt;/summary&gt;

![](img/code_sample/python.webp)

&lt;/details&gt;


&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Rust&lt;/b&gt;&lt;/summary&gt;

![](img/code_sample/rust.webp)


&lt;/details&gt;


## Build your own font

See [doc](./source/README.md)

## Donate

If this was helpful to you, please feel free to buy me a coffee

&lt;a href=&quot;https://www.buymeacoffee.com/subframe753&quot;&gt;&lt;img src=&quot;https://img.buymeacoffee.com/button-api/?text=Buy me a coffee&amp;emoji=&amp;slug=subframe753&amp;button_colour=5F7FFF&amp;font_colour=ffffff&amp;font_family=Lato&amp;outline_colour=000000&amp;coffee_colour=FFDD00&quot; /&gt;&lt;/a&gt;

![](img/donate.webp)

## License

SIL Open Font License 1.1
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/qlib]]></title>
            <link>https://github.com/microsoft/qlib</link>
            <guid>https://github.com/microsoft/qlib</guid>
            <pubDate>Fri, 21 Mar 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[Qlib is an AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms. including supervised learning, market dynamics modeling, and RL.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/qlib">microsoft/qlib</a></h1>
            <p>Qlib is an AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms. including supervised learning, market dynamics modeling, and RL.</p>
            <p>Language: Python</p>
            <p>Stars: 17,310</p>
            <p>Forks: 2,924</p>
            <p>Stars today: 149 stars today</p>
            <h2>README</h2><pre>[![Python Versions](https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&amp;logoColor=white)](https://pypi.org/project/pyqlib/#files)
[![Platform](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey)](https://pypi.org/project/pyqlib/#files)
[![PypI Versions](https://img.shields.io/pypi/v/pyqlib)](https://pypi.org/project/pyqlib/#history)
[![Upload Python Package](https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/pyqlib/)
[![Github Actions Test Status](https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main)](https://github.com/microsoft/qlib/actions)
[![Documentation Status](https://readthedocs.org/projects/qlib/badge/?version=latest)](https://qlib.readthedocs.io/en/latest/?badge=latest)
[![License](https://img.shields.io/pypi/l/pyqlib)](LICENSE)
[![Join the chat at https://gitter.im/Microsoft/qlib](https://badges.gitter.im/Microsoft/qlib.svg)](https://gitter.im/Microsoft/qlib?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge)

## :newspaper: **What&#039;s NEW!** &amp;nbsp;   :sparkling_heart: 

Recent released features

### Introducing &lt;a href=&quot;https://github.com/microsoft/RD-Agent&quot;&gt;&lt;img src=&quot;docs/_static/img/rdagent_logo.png&quot; alt=&quot;RD_Agent&quot; style=&quot;height: 2em&quot;&gt;&lt;/a&gt;: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&amp;D

We are excited to announce the release of **RD-Agent**üì¢, a powerful tool that supports automated factor mining and model optimization in quant investment R&amp;D.

RD-Agent is now available on [GitHub](https://github.com/microsoft/RD-Agent), and we welcome your starüåü!

To learn more, please visit our [‚ôæÔ∏èDemo page](https://rdagent.azurewebsites.net/). Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.

We have prepared several demo videos for you:
| Scenario | Demo video (English) | Demo video (‰∏≠Êñá) |
| --                      | ------    | ------    |
| Quant Factor Mining | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=zh) |
| Quant Factor Mining from reports | [Link](https://rdagent.azurewebsites.net/report_factor?lang=en) | [Link](https://rdagent.azurewebsites.net/report_factor?lang=zh) |
| Quant Model Optimization | [Link](https://rdagent.azurewebsites.net/model_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/model_loop?lang=zh) |

***

| Feature | Status |
| --                      | ------    |
| BPQP for End-to-end learning | üìàComing soon!([Under review](https://github.com/microsoft/qlib/pull/1863)) |
| üî•LLM-driven Auto Quant Factoryüî• | üöÄ Released in [‚ôæÔ∏èRD-Agent](https://github.com/microsoft/RD-Agent) on Aug 8, 2024 |
| KRNN and Sandwich models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1414/) on May 26, 2023 |
| Release Qlib v0.9.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.9.0) on Dec 9, 2022 |
| RL Learning Framework | :hammer: :chart_with_upwards_trend: Released on Nov 10, 2022. [#1332](https://github.com/microsoft/qlib/pull/1332), [#1322](https://github.com/microsoft/qlib/pull/1322), [#1316](https://github.com/microsoft/qlib/pull/1316),[#1299](https://github.com/microsoft/qlib/pull/1299),[#1263](https://github.com/microsoft/qlib/pull/1263), [#1244](https://github.com/microsoft/qlib/pull/1244), [#1169](https://github.com/microsoft/qlib/pull/1169), [#1125](https://github.com/microsoft/qlib/pull/1125), [#1076](https://github.com/microsoft/qlib/pull/1076)|
| HIST and IGMTF models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1040) on Apr 10, 2022 |
| Qlib [notebook tutorial](https://github.com/microsoft/qlib/tree/main/examples/tutorial) | üìñ [Released](https://github.com/microsoft/qlib/pull/1037) on Apr 7, 2022 | 
| Ibovespa index data | :rice: [Released](https://github.com/microsoft/qlib/pull/990) on Apr 6, 2022 |
| Point-in-Time database | :hammer: [Released](https://github.com/microsoft/qlib/pull/343) on Mar 10, 2022 |
| Arctic Provider Backend &amp; Orderbook data example | :hammer: [Released](https://github.com/microsoft/qlib/pull/744) on Jan 17, 2022 |
| Meta-Learning-based framework &amp; DDG-DA  | :chart_with_upwards_trend:  :hammer: [Released](https://github.com/microsoft/qlib/pull/743) on Jan 10, 2022 | 
| Planning-based portfolio optimization | :hammer: [Released](https://github.com/microsoft/qlib/pull/754) on Dec 28, 2021 | 
| Release Qlib v0.8.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.8.0) on Dec 8, 2021 |
| ADD model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/704) on Nov 22, 2021 |
| ADARNN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/689) on Nov 14, 2021 |
| TCN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/668) on Nov 4, 2021 |
| Nested Decision Framework | :hammer: [Released](https://github.com/microsoft/qlib/pull/438) on Oct 1, 2021. [Example](https://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/workflow.py) and [Doc](https://qlib.readthedocs.io/en/latest/component/highfreq.html) |
| Temporal Routing Adaptor (TRA) | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/531) on July 30, 2021 |
| Transformer &amp; Localformer | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/508) on July 22, 2021 |
| Release Qlib v0.7.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.7.0) on July 12, 2021 |
| TCTS Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/491) on July 1, 2021 |
| Online serving and automatic model rolling | :hammer:  [Released](https://github.com/microsoft/qlib/pull/290) on May 17, 2021 | 
| DoubleEnsemble Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/286) on Mar 2, 2021 | 
| High-frequency data processing example | :hammer: [Released](https://github.com/microsoft/qlib/pull/257) on Feb 5, 2021  |
| High-frequency trading example | :chart_with_upwards_trend: [Part of code released](https://github.com/microsoft/qlib/pull/227) on Jan 28, 2021  | 
| High-frequency data(1min) | :rice: [Released](https://github.com/microsoft/qlib/pull/221) on Jan 27, 2021 |
| Tabnet Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/205) on Jan 22, 2021 |

Features released before 2021 are not listed here.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/_static/img/logo/1.png&quot; /&gt;
&lt;/p&gt;

Qlib is an open-source, AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms, including supervised learning, market dynamics modeling, and reinforcement learning.

An increasing number of SOTA Quant research works/papers in diverse paradigms are being released in Qlib to collaboratively solve key challenges in quantitative investment. For example, 1) using supervised learning to mine the market&#039;s complex non-linear patterns from rich and heterogeneous financial data, 2) modeling the dynamic nature of the financial market using adaptive concept drift technology, and 3) using reinforcement learning to model continuous investment decisions and assist investors in optimizing their trading strategies.

It contains the full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution. 
For more details, please refer to our paper [&quot;Qlib: An AI-oriented Quantitative Investment Platform&quot;](https://arxiv.org/abs/2009.11189).


&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Frameworks, Tutorial, Data &amp; DevOps&lt;/th&gt;
      &lt;th&gt;Main Challenges &amp; Solutions in Quant Research&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;li&gt;&lt;a href=&quot;#plans&quot;&gt;&lt;strong&gt;Plans&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#framework-of-qlib&quot;&gt;Framework of Qlib&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#quick-start&quot;&gt;Quick Start&lt;/a&gt;&lt;/li&gt;
          &lt;ul dir=&quot;auto&quot;&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#installation&quot;&gt;Installation&lt;/a&gt; &lt;/li&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#data-preparation&quot;&gt;Data Preparation&lt;/a&gt;&lt;/li&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#auto-quant-research-workflow&quot;&gt;Auto Quant Research Workflow&lt;/a&gt;&lt;/li&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#building-customized-quant-research-workflow-by-code&quot;&gt;Building Customized Quant Research Workflow by Code&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
        &lt;li&gt;&lt;a href=&quot;#quant-dataset-zoo&quot;&gt;&lt;strong&gt;Quant Dataset Zoo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#learning-framework&quot;&gt;Learning Framework&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#more-about-qlib&quot;&gt;More About Qlib&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#offline-mode-and-online-mode&quot;&gt;Offline Mode and Online Mode&lt;/a&gt;
        &lt;ul&gt;
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#performance-of-qlib-data-server&quot;&gt;Performance of Qlib Data Server&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
        &lt;li&gt;&lt;a href=&quot;#related-reports&quot;&gt;Related Reports&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#contact-us&quot;&gt;Contact Us&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#contributing&quot;&gt;Contributing&lt;/a&gt;&lt;/li&gt;
      &lt;/td&gt;
      &lt;td valign=&quot;baseline&quot;&gt;
        &lt;li&gt;&lt;a href=&quot;#main-challenges--solutions-in-quant-research&quot;&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/a&gt;
          &lt;ul&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#forecasting-finding-valuable-signalspatterns&quot;&gt;Forecasting: Finding Valuable Signals/Patterns&lt;/a&gt;
              &lt;ul&gt;
                &lt;li type=&quot;disc&quot;&gt;&lt;a href=&quot;#quant-model-paper-zoo&quot;&gt;&lt;strong&gt;Quant Model (Paper) Zoo&lt;/strong&gt;&lt;/a&gt;
                  &lt;ul&gt;
                    &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#run-a-single-model&quot;&gt;Run a Single Model&lt;/a&gt;&lt;/li&gt;
                    &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#run-multiple-models&quot;&gt;Run Multiple Models&lt;/a&gt;&lt;/li&gt;
                  &lt;/ul&gt;
                &lt;/li&gt;
              &lt;/ul&gt;
            &lt;/li&gt;
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#adapting-to-market-dynamics&quot;&gt;Adapting to Market Dynamics&lt;/a&gt;&lt;/li&gt;
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#reinforcement-learning-modeling-continuous-decisions&quot;&gt;Reinforcement Learning: modeling continuous decisions&lt;/a&gt;&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

# Plans
New features under development(order by estimated release time).
Your feedbacks about the features are very important.
&lt;!-- | Feature                        | Status      | --&gt;
&lt;!-- | --                      | ------    | --&gt;

# Framework of Qlib

&lt;div style=&quot;align: center&quot;&gt;
&lt;img src=&quot;docs/_static/img/framework-abstract.jpg&quot; /&gt;
&lt;/div&gt;

The high-level framework of Qlib can be found above(users can find the [detailed framework](https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework) of Qlib&#039;s design when getting into nitty gritty).
The components are designed as loose-coupled modules, and each component could be used stand-alone.

Qlib provides a strong infrastructure to support Quant research. [Data](https://qlib.readthedocs.io/en/latest/component/data.html) is always an important part.
A strong learning framework is designed to support diverse learning paradigms (e.g. [reinforcement learning](https://qlib.readthedocs.io/en/latest/component/rl.html), [supervised learning](https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section)) and patterns at different levels(e.g. [market dynamic modeling](https://qlib.readthedocs.io/en/latest/component/meta.html)).
By modeling the market, [trading strategies](https://qlib.readthedocs.io/en/latest/component/strategy.html) will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be [nested to be optimized and run together](https://qlib.readthedocs.io/en/latest/component/highfreq.html).
At last, a comprehensive [analysis](https://qlib.readthedocs.io/en/latest/component/report.html) will be provided and the model can be [served online](https://qlib.readthedocs.io/en/latest/component/online.html) in a low cost.


# Quick Start

This quick start guide tries to demonstrate
1. It&#039;s very easy to build a complete Quant research workflow and try your ideas with _Qlib_.
2. Though with *public data* and *simple models*, machine learning technologies **work very well** in practical Quant investment.

Here is a quick **[demo](https://terminalizer.com/view/3f24561a4470)** shows how to install ``Qlib``, and run LightGBM with ``qrun``. **But**, please make sure you have already prepared the data following the [instruction](#data-preparation).


## Installation

This table demonstrates the supported Python version of `Qlib`:
|               | install with pip      | install from source  |        plot        |
| ------------- |:---------------------:|:--------------------:|:------------------:|
| Python 3.8    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.9    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.10   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.11   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.12   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |

**Note**: 
1. **Conda** is suggested for managing your Python environment. In some cases, using Python outside of a `conda` environment may result in missing header files, causing the installation failure of certain packages.
2. Please pay attention that installing cython in Python 3.6 will raise some error when installing ``Qlib`` from source. If users use Python 3.6 on their machines, it is recommended to *upgrade* Python to version 3.8 or higher, or use `conda`&#039;s Python to install ``Qlib`` from source.

### Install with pip
Users can easily install ``Qlib`` by pip according to the following command.

```bash
  pip install pyqlib
```

**Note**: pip will install the latest stable qlib. However, the main branch of qlib is in active development. If you want to test the latest scripts or functions in the main branch. Please install qlib with the methods below.

### Install from source
Also, users can install the latest dev version ``Qlib`` by the source code according to the following steps:

* Before installing ``Qlib`` from source, users need to install some dependencies:

  ```bash
  pip install numpy
  pip install --upgrade cython
  ```

* Clone the repository and install ``Qlib`` as follows.
    ```bash
    git clone https://github.com/microsoft/qlib.git &amp;&amp; cd qlib
    pip install .  # `pip install -e .[dev]` is recommended for development. check details in docs/developer/code_standard_and_dev_guide.rst
    ```

**Tips**: If you fail to install `Qlib` or run the examples in your environment,  comparing your steps and the [CI workflow](.github/workflows/test_qlib_from_source.yml) may help you find the problem.

**Tips for Mac**: If you are using Mac with M1, you might encounter issues in building the wheel for LightGBM, which is due to missing dependencies from OpenMP. To solve the problem, install openmp first with ``brew install libomp`` and then run ``pip install .`` to build it successfully. 

## Data Preparation
‚ùó Due to more restrict data security policy. The offical dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.
Here is an example to download the latest data.
```bash
wget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz
mkdir -p ~/.qlib/qlib_data/cn_data
tar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=2
rm -f qlib_bin.tar.gz
```

The official dataset below will resume in short future.


----

Load and prepare data by running the following code:

### Get with module
  ```bash
  # get 1d data
  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

  # get 1min data
  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

  ```

### Get from source

  ```bash
  # get 1d data
  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

  # get 1min data
  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

  ```

This dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in
the same repository.
Users could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)

*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.
We recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.

### Automatic update of daily frequency data (from yahoo finance)
  &gt; This step is *Optional* if users only want to try their models and strategies on history data.
  &gt; 
  &gt; It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.
  &gt;
  &gt; **NOTE**: Users can&#039;t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.
  &gt; 
  &gt; For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)

  * Automatic update of data to the &quot;qlib&quot; directory each trading day(Linux)
      * use *crontab*: `crontab -e`
      * set up timed tasks:

        ```
        * * * * 1-5 python &lt;script path&gt; update_data_to_bin --qlib_data_1d_dir &lt;user data dir&gt;
        ```
        * **script path**: *scripts/data_collector/yahoo/collector.py*

  * Manual update of data
      ```
      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir &lt;user data dir&gt; --trading_date &lt;start date&gt; --end_date &lt;end date&gt;
      ```
      * *trading_date*: start of trading day
      * *end_date*: end of trading day(not included)

### Checking the health of the data
  * We provide a script to check the health of the data, you can run the following commands to check whether the data is healthy or not.
    ```
    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data
    ```
  * Of course, you can also add some parameters to adjust the test results, such as this.
    ```
    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data --missing_data_num 30055 --large_step_threshold_volume 94485 --large_step_threshold_price 20
    ```
  * If you want more information about `check_data_health`, please refer to the [documentation](https://qlib.readthedocs.io/en/latest/component/data.html#checking-the-health-of-the-data).

&lt;!-- 
- Run the initialization code and get stock data:

  ```python
  import qlib
  from qlib.data import D
  from qlib.constant import REG_CN

  # Initialization
  mount_path = &quot;~/.qlib/qlib_data/cn_data&quot;  # target_dir
  qlib.init(mount_path=mount_path, region=REG_CN)

  # Get stock data by Qlib
  # Load trading calendar with the given time range and frequency
  print(D.calendar(start_time=&#039;2010-01-01&#039;, end_time=&#039;2017-12-31&#039;, freq=&#039;day&#039;)[:2])

  # Par

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[localstack/localstack]]></title>
            <link>https://github.com/localstack/localstack</link>
            <guid>https://github.com/localstack/localstack</guid>
            <pubDate>Fri, 21 Mar 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[üíª A fully functional local AWS cloud stack. Develop and test your cloud & Serverless apps offline]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/localstack/localstack">localstack/localstack</a></h1>
            <p>üíª A fully functional local AWS cloud stack. Develop and test your cloud & Serverless apps offline</p>
            <p>Language: Python</p>
            <p>Stars: 58,124</p>
            <p>Forks: 4,117</p>
            <p>Stars today: 25 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
:zap: We are thrilled to announce the release of &lt;a href=&quot;https://blog.localstack.cloud/localstack-release-v-4-1-0/&quot;&gt;LocalStack 4.1&lt;/a&gt; :zap:
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/localstack/localstack/master/docs/localstack-readme-banner.svg&quot; alt=&quot;LocalStack - A fully functional local cloud stack&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://circleci.com/gh/localstack/localstack&quot;&gt;&lt;img alt=&quot;CircleCI&quot; src=&quot;https://img.shields.io/circleci/build/gh/localstack/localstack/master?logo=circleci&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://coveralls.io/github/localstack/localstack?branch=master&quot;&gt;&lt;img alt=&quot;Coverage Status&quot; src=&quot;https://coveralls.io/repos/github/localstack/localstack/badge.svg?branch=master&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/localstack/&quot;&gt;&lt;img alt=&quot;PyPI Version&quot; src=&quot;https://img.shields.io/pypi/v/localstack?color=blue&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://hub.docker.com/r/localstack/localstack&quot;&gt;&lt;img alt=&quot;Docker Pulls&quot; src=&quot;https://img.shields.io/docker/pulls/localstack/localstack&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/localstack&quot;&gt;&lt;img alt=&quot;PyPi downloads&quot; src=&quot;https://static.pepy.tech/badge/localstack&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;#backers&quot;&gt;&lt;img alt=&quot;Backers on Open Collective&quot; src=&quot;https://opencollective.com/localstack/backers/badge.svg&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;#sponsors&quot;&gt;&lt;img alt=&quot;Sponsors on Open Collective&quot; src=&quot;https://opencollective.com/localstack/sponsors/badge.svg&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://img.shields.io/pypi/l/localstack.svg&quot;&gt;&lt;img alt=&quot;PyPI License&quot; src=&quot;https://img.shields.io/pypi/l/localstack.svg&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/psf/black&quot;&gt;&lt;img alt=&quot;Code style: black&quot; src=&quot;https://img.shields.io/badge/code%20style-black-000000.svg&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/astral-sh/ruff&quot;&gt;&lt;img alt=&quot;Ruff&quot; src=&quot;https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/localstack&quot;&gt;&lt;img alt=&quot;Twitter&quot; src=&quot;https://img.shields.io/twitter/url/http/shields.io.svg?style=social&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  LocalStack is a cloud software development framework to develop and test your AWS applications locally.
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;#overview&quot;&gt;Overview&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;#install&quot;&gt;Install&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;#quickstart&quot;&gt;Quickstart&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;#running&quot;&gt;Run&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;#usage&quot;&gt;Usage&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;#releases&quot;&gt;Releases&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;#contributing&quot;&gt;Contributing&lt;/a&gt;
  &lt;br/&gt;
  &lt;a href=&quot;https://docs.localstack.cloud&quot; target=&quot;_blank&quot;&gt;üìñ Docs&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://app.localstack.cloud&quot; target=&quot;_blank&quot;&gt;üíª Pro version&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;https://docs.localstack.cloud/references/coverage/&quot; target=&quot;_blank&quot;&gt;‚òëÔ∏è LocalStack coverage&lt;/a&gt;
&lt;/p&gt;

---

# Overview

[LocalStack](https://localstack.cloud) is a cloud service emulator that runs in a single container on your laptop or in your CI environment. With LocalStack, you can run your AWS applications or Lambdas entirely on your local machine without connecting to a remote cloud provider! Whether you are testing complex CDK applications or Terraform configurations, or just beginning to learn about AWS services, LocalStack helps speed up and simplify your testing and development workflow.

LocalStack supports a growing number of AWS services, like AWS Lambda, S3, DynamoDB, Kinesis, SQS, SNS, and many more! The [Pro version of LocalStack](https://localstack.cloud/pricing) supports additional APIs and advanced features. You can find a comprehensive list of supported APIs on our [‚òëÔ∏è Feature Coverage](https://docs.localstack.cloud/user-guide/aws/feature-coverage/) page.

LocalStack also provides additional features to make your life as a cloud developer easier! Check out LocalStack&#039;s [User Guides](https://docs.localstack.cloud/user-guide/) for more information.

## Install

The quickest way to get started with LocalStack is by using the LocalStack CLI. It enables you to start and manage the LocalStack Docker container directly through your command line. Ensure that your machine has a functional [`docker` environment](https://docs.docker.com/get-docker/) installed before proceeding.

### Brew (macOS or Linux with Homebrew)

Install the LocalStack CLI through our [official LocalStack Brew Tap](https://github.com/localstack/homebrew-tap):

```bash
brew install localstack/tap/localstack-cli
```

### Binary download (macOS, Linux, Windows)

If Brew is not installed on your machine, you can download the pre-built LocalStack CLI binary directly:

- Visit [localstack/localstack-cli](https://github.com/localstack/localstack-cli/releases/latest) and download the latest release for your platform.
- Extract the downloaded archive to a directory included in your `PATH` variable:
    -   For macOS/Linux, use the command: `sudo tar xvzf ~/Downloads/localstack-cli-*-darwin-*-onefile.tar.gz -C /usr/local/bin`

### PyPI (macOS, Linux, Windows)

LocalStack is developed using Python. To install the LocalStack CLI using `pip`, run the following command:

```bash
python3 -m pip install localstack
```

The `localstack-cli` installation enables you to run the Docker image containing the LocalStack runtime. To interact with the local AWS services, you need to install the `awslocal` CLI separately. For installation guidelines, refer to the [`awslocal` documentation](https://docs.localstack.cloud/user-guide/integrations/aws-cli/#localstack-aws-cli-awslocal).

&gt; **Important**: Do not use `sudo` or run as `root` user. LocalStack must be installed and started entirely under a local non-root user. If you have problems with permissions in macOS High Sierra, install with `pip install --user localstack`

## Quickstart

Start LocalStack inside a Docker container by running:

```bash
 % localstack start -d

     __                     _______ __             __
    / /   ____  _________ _/ / ___// /_____ ______/ /__
   / /   / __ \/ ___/ __ `/ /\__ \/ __/ __ `/ ___/ //_/
  / /___/ /_/ / /__/ /_/ / /___/ / /_/ /_/ / /__/ ,&lt;
 /_____/\____/\___/\__,_/_//____/\__/\__,_/\___/_/|_|

- LocalStack CLI: 4.1.0
- Profile: default
- App: https://app.localstack.cloud

[12:00:19] starting LocalStack in Docker mode üê≥               localstack.py:512
           preparing environment                               bootstrap.py:1321
           configuring container                               bootstrap.py:1329
           starting container                                  bootstrap.py:1339
[12:00:20] detaching                                           bootstrap.py:1343
```

You can query the status of respective services on LocalStack by running:

```bash
% localstack status services
‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ Service                  ‚îÉ Status      ‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ acm                      ‚îÇ ‚úî available ‚îÇ
‚îÇ apigateway               ‚îÇ ‚úî available ‚îÇ
‚îÇ cloudformation           ‚îÇ ‚úî available ‚îÇ
‚îÇ cloudwatch               ‚îÇ ‚úî available ‚îÇ
‚îÇ config                   ‚îÇ ‚úî available ‚îÇ
‚îÇ dynamodb                 ‚îÇ ‚úî available ‚îÇ
...
```

To use SQS, a fully managed distributed message queuing service, on LocalStack, run:

```shell
% awslocal sqs create-queue --queue-name sample-queue
{
    &quot;QueueUrl&quot;: &quot;http://sqs.us-east-1.localhost.localstack.cloud:4566/000000000000/sample-queue&quot;
}
```

Learn more about [LocalStack AWS services](https://docs.localstack.cloud/references/coverage/) and using them with LocalStack&#039;s `awslocal` CLI.

## Running

You can run LocalStack through the following options:

- [LocalStack CLI](https://docs.localstack.cloud/getting-started/installation/#localstack-cli)
- [Docker](https://docs.localstack.cloud/getting-started/installation/#docker)
- [Docker Compose](https://docs.localstack.cloud/getting-started/installation/#docker-compose)
- [Helm](https://docs.localstack.cloud/getting-started/installation/#helm)

## Usage

To start using LocalStack, check out our [documentation](https://docs.localstack.cloud).

- [LocalStack Configuration](https://docs.localstack.cloud/references/configuration/)
- [LocalStack in CI](https://docs.localstack.cloud/user-guide/ci/)
- [LocalStack Integrations](https://docs.localstack.cloud/user-guide/integrations/)
- [LocalStack Tools](https://docs.localstack.cloud/user-guide/tools/)
- [Understanding LocalStack](https://docs.localstack.cloud/references/)
- [Frequently Asked Questions](https://docs.localstack.cloud/getting-started/faq/)

To use LocalStack with a graphical user interface, you can use the following UI clients:

* [LocalStack Web Application](https://app.localstack.cloud)
* [LocalStack Desktop](https://docs.localstack.cloud/user-guide/tools/localstack-desktop/)
* [LocalStack Docker Extension](https://docs.localstack.cloud/user-guide/tools/localstack-docker-extension/)

## Releases

Please refer to [GitHub releases](https://github.com/localstack/localstack/releases) to see the complete list of changes for each release. For extended release notes, please refer to the [changelog](https://docs.localstack.cloud/references/changelog/).

## Contributing

If you are interested in contributing to LocalStack:

- Start by reading our [contributing guide](docs/CONTRIBUTING.md).
- Check out our [development environment setup guide](docs/development-environment-setup/README.md).
- Navigate our codebase and [open issues](https://github.com/localstack/localstack/issues).

We are thankful for all the contributions and feedback we receive.

## Get in touch

Get in touch with the LocalStack Team to
report üêû [issues](https://github.com/localstack/localstack/issues/new/choose),
upvote üëç [feature requests](https://github.com/localstack/localstack/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc+),
üôãüèΩ ask [support questions](https://docs.localstack.cloud/getting-started/help-and-support/),
or üó£Ô∏è discuss local cloud development:

- [LocalStack Slack Community](https://localstack.cloud/contact/)
- [LocalStack GitHub Issue tracker](https://github.com/localstack/localstack/issues)

### Contributors

We are thankful to all the people who have contributed to this project.

&lt;a href=&quot;https://github.com/localstack/localstack/graphs/contributors&quot;&gt;&lt;img src=&quot;https://opencollective.com/localstack/contributors.svg?width=890&quot; /&gt;&lt;/a&gt;

### Backers

We are also grateful to all our backers who have donated to the project. You can become a backer on [Open Collective](https://opencollective.com/localstack#backer).

&lt;a href=&quot;https://opencollective.com/localstack#backers&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://opencollective.com/localstack/backers.svg?width=890&quot;&gt;&lt;/a&gt;

### Sponsors

You can also support this project by becoming a sponsor on [Open Collective](https://opencollective.com/localstack#sponsor). Your logo will show up here along with a link to your website.

&lt;a href=&quot;https://opencollective.com/localstack/sponsor/0/website&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://opencollective.com/localstack/sponsor/0/avatar.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://opencollective.com/localstack/sponsor/1/website&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://opencollective.com/localstack/sponsor/1/avatar.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://opencollective.com/localstack/sponsor/2/website&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://opencollective.com/localstack/sponsor/2/avatar.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://opencollective.com/localstack/sponsor/3/website&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://opencollective.com/localstack/sponsor/3/avatar.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://opencollective.com/localstack/sponsor/4/website&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://opencollective.com/localstack/sponsor/4/avatar.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://opencollective.com/localstack/sponsor/5/website&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://opencollective.com/localstack/sponsor/5/avatar.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://opencollective.com/localstack/sponsor/6/website&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://opencollective.com/localstack/sponsor/6/avatar.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://opencollective.com/localstack/sponsor/7/website&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://opencollective.com/localstack/sponsor/7/avatar.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://opencollective.com/localstack/sponsor/8/website&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://opencollective.com/localstack/sponsor/8/avatar.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://opencollective.com/localstack/sponsor/9/website&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://opencollective.com/localstack/sponsor/9/avatar.svg&quot;&gt;&lt;/a&gt;

## License

Copyright (c) 2017-2024 LocalStack maintainers and contributors.

Copyright (c) 2016 Atlassian and others.

This version of LocalStack is released under the Apache License, Version 2.0 (see [LICENSE](LICENSE.txt)). By downloading and using this software you agree to the [End-User License Agreement (EULA)](docs/end_user_license_agreement).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[open-mmlab/Amphion]]></title>
            <link>https://github.com/open-mmlab/Amphion</link>
            <guid>https://github.com/open-mmlab/Amphion</guid>
            <pubDate>Fri, 21 Mar 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[Amphion (/√¶mÀàfa…™…ôn/) is a toolkit for Audio, Music, and Speech Generation. Its purpose is to support reproducible research and help junior researchers and engineers get started in the field of audio, music, and speech generation research and development.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/open-mmlab/Amphion">open-mmlab/Amphion</a></h1>
            <p>Amphion (/√¶mÀàfa…™…ôn/) is a toolkit for Audio, Music, and Speech Generation. Its purpose is to support reproducible research and help junior researchers and engineers get started in the field of audio, music, and speech generation research and development.</p>
            <p>Language: Python</p>
            <p>Stars: 8,801</p>
            <p>Forks: 684</p>
            <p>Stars today: 57 stars today</p>
            <h2>README</h2><pre># Amphion: An Open-Source Audio, Music, and Speech Generation Toolkit

&lt;div&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2312.09911&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-Paper-&lt;COLOR&gt;.svg&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://huggingface.co/amphion&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Amphion-pink&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://modelscope.cn/organization/amphion&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/ModelScope-Amphion-cyan&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://openxlab.org.cn/usercenter/Amphion&quot;&gt;&lt;img src=&quot;https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://discord.com/invite/drhW7ajqAG&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20chat-blue.svg&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;egs/tts/README.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/README-TTS-blue&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;models/vc/vevo/README.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/README-VC-blue&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;models/vc/vevo/README.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/README-AC-blue&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;egs/svc/README.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/README-SVC-blue&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;egs/tta/README.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/README-TTA-blue&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;egs/vocoder/README.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/README-Vocoder-purple&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;egs/metrics/README.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/README-Evaluation-yellow&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/LICENSE-MIT-red&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://trendshift.io/repositories/5469&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/5469&quot; alt=&quot;open-mmlab%2FAmphion | Trendshift&quot; style=&quot;width: 150px; height: 33px;&quot; width=&quot;150&quot; height=&quot;33&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;br&gt;

**Amphion (/√¶mÀàfa…™…ôn/) is a toolkit for Audio, Music, and Speech Generation.** Its purpose is to support reproducible research and help junior researchers and engineers get started in the field of audio, music, and speech generation research and development. Amphion offers a unique feature: **visualizations** of classic models or architectures. We believe that these visualizations are beneficial for junior researchers and engineers who wish to gain a better understanding of the model.

**The North-Star objective of Amphion is to offer a platform for studying the conversion of any inputs into audio.** Amphion is designed to support individual generation tasks, including but not limited to,

- **TTS**: Text to Speech (‚õ≥¬†supported)
- **SVS**: Singing Voice Synthesis (üë®‚Äçüíª¬†developing)
- **VC**: Voice Conversion (‚õ≥¬†supported)
- **AC**: Accent Conversion (‚õ≥¬†supported)
- **SVC**: Singing Voice Conversion (‚õ≥¬†supported)
- **TTA**: Text to Audio (‚õ≥¬†supported)
- **TTM**: Text to Music (üë®‚Äçüíª¬†developing)
- more‚Ä¶

In addition to the specific generation tasks, Amphion includes several **vocoders** and **evaluation metrics**. A vocoder is an important module for producing high-quality audio signals, while evaluation metrics are critical for ensuring consistent metrics in generation tasks. Moreover, Amphion is dedicated to advancing audio generation in real-world applications, such as building **large-scale datasets** for speech synthesis.

## üöÄ¬†News
- **2025/02/26**: We release [***Metis***](https://github.com/open-mmlab/Amphion/tree/main/models/tts/metis), a foundation model for unified speech generation. The system supports zero-shot text-to-speech, voice conversion, target speaker extraction, speech enhancement, and lip-to-speech. [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/pdf/2502.03128) [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-model-yellow)](https://huggingface.co/amphion/metis)
- **2025/02/26**: *The Emilia-Large dataset, featuring over 200,000 hours of data, is now available!!!* Emilia-Large combines the original 101k-hour Emilia dataset (licensed under `CC BY-NC 4.0`) with the brand-new 114k-hour **Emilia-YODAS dataset** (licensed under `CC BY 4.0`). Download at [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow)](https://huggingface.co/datasets/amphion/Emilia-Dataset). Check details at [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2501.15907).
- **2025/01/30**: We release [Amphion v0.2 Technical Report](https://arxiv.org/abs/2501.15442), which provides a comprehensive overview of the Amphion updates in 2024. [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2501.15442)
- **2025/01/23**: [MaskGCT](https://arxiv.org/abs/2409.00750) and [Vevo](https://openreview.net/pdf?id=anQDiQZhDP) got accepted by ICLR 2025! üéâ
- **2024/12/22**: We release the reproduction of **Vevo**, a zero-shot voice imitation framework with controllable timbre and style. Vevo can be applied into a series of speech generation tasks, including VC, TTS, AC, and more. The released pre-trained models are trained on [Emilia](https://huggingface.co/datasets/amphion/Emilia-Dataset) dataset and achieve SOTA zero-shot VC performance. [![arXiv](https://img.shields.io/badge/OpenReview-Paper-COLOR.svg)](https://openreview.net/pdf?id=anQDiQZhDP) [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-model-yellow)](https://huggingface.co/amphion/Vevo) [![WebPage](https://img.shields.io/badge/WebPage-Demo-red)](https://versavoice.github.io/) [![readme](https://img.shields.io/badge/README-Key%20Features-blue)](models/vc/vevo/README.md)
- **2024/10/19**: We release **MaskGCT**, a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision. MaskGCT is trained on [Emilia](https://huggingface.co/datasets/amphion/Emilia-Dataset) dataset and achieves SOTA zero-shot TTS performance.  [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2409.00750) [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-model-yellow)](https://huggingface.co/amphion/maskgct) [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-demo-pink)](https://huggingface.co/spaces/amphion/maskgct) [![ModelScope](https://img.shields.io/badge/ModelScope-space-purple)](https://modelscope.cn/studios/amphion/maskgct) [![ModelScope](https://img.shields.io/badge/ModelScope-model-cyan)](https://modelscope.cn/models/amphion/MaskGCT) [![readme](https://img.shields.io/badge/README-Key%20Features-blue)](models/tts/maskgct/README.md)
- **2024/09/01**: [Amphion](https://arxiv.org/abs/2312.09911), [Emilia](https://arxiv.org/abs/2407.05361) and [DSFF-SVC](https://arxiv.org/abs/2310.11160) got accepted by IEEE SLT 2024! ü§ó
- **2024/08/28**: Welcome to join Amphion&#039;s [Discord channel](https://discord.com/invite/drhW7ajqAG) to stay connected and engage with our communityÔºÅ
- **2024/08/20**: [SingVisio](https://arxiv.org/abs/2402.12660) got accepted by Computers &amp; Graphics, [available here](https://www.sciencedirect.com/science/article/pii/S0097849324001936)! üéâ
- **2024/08/27**: *The Emilia dataset is now publicly available!* Discover the most extensive and diverse speech generation dataset with 101k hours of in-the-wild speech data now at [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow)](https://huggingface.co/datasets/amphion/Emilia-Dataset) or [![OpenDataLab](https://img.shields.io/badge/OpenDataLab-Dataset-blue)](https://opendatalab.com/Amphion/Emilia)! üëëüëëüëë
- **2024/07/01**: Amphion now releases **Emilia**, the first open-source multilingual in-the-wild dataset for speech generation with over 101k hours of speech data, and the **Emilia-Pipe**, the first open-source preprocessing pipeline designed to transform in-the-wild speech data into high-quality training data with annotations for speech generation! [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2407.05361) [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Dataset-yellow)](https://huggingface.co/datasets/amphion/Emilia) [![demo](https://img.shields.io/badge/WebPage-Demo-red)](https://emilia-dataset.github.io/Emilia-Demo-Page/) [![readme](https://img.shields.io/badge/README-Key%20Features-blue)](preprocessors/Emilia/README.md)
- **2024/06/17**: Amphion has a new release for its **VALL-E** model! It uses Llama as its underlying architecture and has better model performance, faster training speed, and more readable codes compared to our first version. [![readme](https://img.shields.io/badge/README-Key%20Features-blue)](egs/tts/VALLE_V2/README.md)
- **2024/03/12**: Amphion now support **NaturalSpeech3 FACodec** and release pretrained checkpoints. [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2403.03100) [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-model-yellow)](https://huggingface.co/amphion/naturalspeech3_facodec) [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-demo-pink)](https://huggingface.co/spaces/amphion/naturalspeech3_facodec) [![readme](https://img.shields.io/badge/README-Key%20Features-blue)](models/codec/ns3_codec/README.md)
- **2024/02/22**: The first Amphion visualization tool, **SingVisio**, release. [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2402.12660) [![openxlab](https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg)](https://openxlab.org.cn/apps/detail/Amphion/SingVisio) [![Video](https://img.shields.io/badge/Video-Demo-orange)](https://drive.google.com/file/d/15097SGhQh-SwUNbdWDYNyWEP--YGLba5/view) [![readme](https://img.shields.io/badge/README-Key%20Features-blue)](egs/visualization/SingVisio/README.md)
- **2023/12/18**: Amphion v0.1 release. [![arXiv](https://img.shields.io/badge/arXiv-Paper-&lt;COLOR&gt;.svg)](https://arxiv.org/abs/2312.09911) [![hf](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Amphion-pink)](https://huggingface.co/amphion) [![youtube](https://img.shields.io/badge/YouTube-Demo-red)](https://www.youtube.com/watch?v=1aw0HhcggvQ) [![readme](https://img.shields.io/badge/README-Key%20Features-blue)](https://github.com/open-mmlab/Amphion/pull/39)
- **2023/11/28**: Amphion alpha release. [![readme](https://img.shields.io/badge/README-Key%20Features-blue)](https://github.com/open-mmlab/Amphion/pull/2)

## ‚≠ê¬†Key Features

### TTS: Text to Speech

- Amphion achieves state-of-the-art performance compared to existing open-source repositories on text-to-speech (TTS) systems. It supports the following models or architectures:
    - [FastSpeech2](https://arxiv.org/abs/2006.04558): A non-autoregressive TTS architecture that utilizes feed-forward Transformer blocks. [![code](https://img.shields.io/badge/README-Code-blue)](egs/tts/FastSpeech2/README.md)
    - [VITS](https://arxiv.org/abs/2106.06103): An end-to-end TTS architecture that utilizes conditional variational autoencoder with adversarial learning [![code](https://img.shields.io/badge/README-Code-blue)](egs/tts/VITS/README.md)
    - [VALL-E](https://arxiv.org/abs/2301.02111): A zero-shot TTS architecture that uses a neural codec language model with discrete codes. [![code](https://img.shields.io/badge/README-Code-blue)](egs/tts/VALLE_V2/README.md)
    - [NaturalSpeech2](https://arxiv.org/abs/2304.09116): An architecture for TTS that utilizes a latent diffusion model to generate natural-sounding voices. [![code](https://img.shields.io/badge/README-Code-blue)](egs/tts/NaturalSpeech2/README.md)
    - [Jets](Jets): An end-to-end TTS model that jointly trains FastSpeech2 and HiFi-GAN with an alignment module. [![code](https://img.shields.io/badge/README-Code-blue)](egs/tts/Jets/README.md)
    - [MaskGCT](https://arxiv.org/abs/2409.00750): A fully non-autoregressive TTS architecture that eliminates the need for explicit alignment information between text and speech supervision. [![code](https://img.shields.io/badge/README-Code-blue)](models/tts/maskgct/README.md)
    - [Vevo-TTS](https://openreview.net/pdf?id=anQDiQZhDP): A zero-shot TTS architecture with controllable timbre and style. It consists of an autoregressive transformer and a flow-matching transformer. [![code](https://img.shields.io/badge/README-Code-blue)](models/vc/vevo/README.md)

### VC: Voice Conversion

Amphion supports the following voice conversion models:

- [Vevo](https://openreview.net/pdf?id=anQDiQZhDP): A zero-shot voice imitation framework with controllable timbre and style. **Vevo-Timbre** conducts the style-preserved voice conversion, and **Vevo-Voice** conducts the style-converted voice conversion. [![code](https://img.shields.io/badge/README-Code-blue)](models/vc/vevo/README.md)
- [FACodec](https://arxiv.org/abs/2403.03100): FACodec decomposes speech into subspaces representing different attributes like content, prosody, and timbre. It can achieve zero-shot voice conversion. [![code](https://img.shields.io/badge/README-Code-blue)](https://huggingface.co/amphion/naturalspeech3_facodec)
- [Noro](https://arxiv.org/abs/2411.19770): A **noise-robust** zero-shot voice conversion system. Noro introduces innovative components tailored for VC using noisy reference speeches, including a dual-branch reference encoding module and a noise-agnostic contrastive speaker loss. [![code](https://img.shields.io/badge/README-Code-blue)](egs/vc/Noro/README.md)

### AC: Accent Conversion

- Amphion supports AC with [Vevo-Style](models/vc/vevo/README.md). Particularly, it can conduct the accent conversion in a zero-shot manner. [![code](https://img.shields.io/badge/README-Code-blue)](models/vc/vevo/README.md)

### SVC: Singing Voice Conversion

- Ampion supports multiple content-based features from various pretrained models, including [WeNet](https://github.com/wenet-e2e/wenet), [Whisper](https://github.com/openai/whisper), and [ContentVec](https://github.com/auspicious3000/contentvec). Their specific roles in SVC has been investigated in our SLT 2024 paper. [![arXiv](https://img.shields.io/badge/arXiv-Paper-&lt;COLOR&gt;.svg)](https://arxiv.org/abs/2310.11160) [![code](https://img.shields.io/badge/README-Code-blue)](egs/svc/MultipleContentsSVC)
- Amphion implements several state-of-the-art model architectures, including diffusion-, transformer-, VAE- and flow-based models. The diffusion-based architecture uses [Bidirectional dilated CNN](https://openreview.net/pdf?id=a-xFK8Ymz5J) as a backend and supports several sampling algorithms such as [DDPM](https://arxiv.org/pdf/2006.11239.pdf), [DDIM](https://arxiv.org/pdf/2010.02502.pdf), and [PNDM](https://arxiv.org/pdf/2202.09778.pdf). Additionally, it supports single-step inference based on the [Consistency Model](https://openreview.net/pdf?id=FmqFfMTNnv). [![code](https://img.shields.io/badge/README-Code-blue)](egs/svc/DiffComoSVC/README.md)

### TTA: Text to Audio

- Amphion supports the TTA with a latent diffusion model. It is designed like [AudioLDM](https://arxiv.org/abs/2301.12503), [Make-an-Audio](https://arxiv.org/abs/2301.12661), and [AUDIT](https://arxiv.org/abs/2304.00830). It is also the official implementation of the text-to-audio generation part of our NeurIPS 2023 paper. [![arXiv](https://img.shields.io/badge/arXiv-Paper-&lt;COLOR&gt;.svg)](https://arxiv.org/abs/2304.00830) [![code](https://img.shields.io/badge/README-Code-blue)](egs/tta/RECIPE.md)

### Vocoder

- Amphion supports various widely-used neural vocoders, including:
    - GAN-based vocoders: [MelGAN](https://arxiv.org/abs/1910.06711), [HiFi-GAN](https://arxiv.org/abs/2010.05646), [NSF-HiFiGAN](https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts), [BigVGAN](https://arxiv.org/abs/2206.04658), [APNet](https://arxiv.org/abs/2305.07952).
    - Flow-based vocoders: [WaveGlow](https://arxiv.org/abs/1811.00002).
    - Diffusion-based vocoders: [Diffwave](https://arxiv.org/abs/2009.09761).
    - Auto-regressive based vocoders: [WaveNet](https://arxiv.org/abs/1609.03499), [WaveRNN](https://arxiv.org/abs/1802.08435v1).
- Amphion provides the official implementation of [Multi-Scale Constant-Q Transform Discriminator](https://arxiv.org/abs/2311.14957) (our ICASSP 2024 paper). It can be used to enhance any architecture GAN-based vocoders during training, and keep the inference stage (such as memory or speed) unchanged. [![arXiv](https://img.shields.io/badge/arXiv-Paper-&lt;COLOR&gt;.svg)](https://arxiv.org/abs/2311.14957) [![code](https://img.shields.io/badge/README-Code-blue)](egs/vocoder/gan/tfr_enhanced_hifigan)

### Evaluation

Amphion provides a comprehensive objective evaluation of the generated audio. [![code](https://img.shields.io/badge/README-Code-blue)](egs/metrics/README.md) 

The supported evaluation metrics contain:

- **F0 Modeling**: F0 Pearson Coefficients, F0 Periodicity Root Mean Square Error, F0 Root Mean Square Error, Voiced/Unvoiced F1 Score, etc.
- **Energy Modeling**: Energy Root Mean Square Error, Energy Pearson Coefficients, etc.
- **Intelligibility**: Character/Word Error Rate, which can be calculated based on [Whisper](https://github.com/openai/whisper) and more.
- **Spectrogram Distortion**: Frechet Audio Distance (FAD), Mel Cepstral Distortion (MCD), Multi-Resolution STFT Distance (MSTFT), Perceptual Evaluation of Speech Quality (PESQ), Short Time Objective Intelligibility (STOI), etc.
- **Speaker Similarity**: Cosine similarity, which can be calculated based on [RawNet3](https://github.com/Jungjee/RawNet), [Resemblyzer](https://github.com/resemble-ai/Resemblyzer), [WeSpeaker](https://github.com/wenet-e2e/wespeaker), [WavLM](https://github.com/microsoft/unilm/tree/master/wavlm) and more.

### Datasets

- Amphion unifies the data preprocess of the open-source datasets including [AudioCaps](https://audiocaps.github.io/), [LibriTTS](https://www.openslr.org/60/), [LJSpeech](https://keithito.com/LJ-Speech-Dataset/), [M4Singer](https://github.com/M4Singer/M4Singer), [Opencpop](https://wenet.org.cn/opencpop/), [OpenSinger](https://github.com/Multi-Singer/Multi-Singer.github.io), [SVCC](http://vc-challenge.org/), [VCTK](https://datashare.ed.ac.uk/handle/10283/3443), and more. The supported dataset list can be seen [here](egs/datasets/README.md) (updating). 
- Amphion (exclusively) supports the [**Emilia**](preprocessors/Emilia/README.md) dataset and its preprocessing pipeline **Emilia-Pipe** for in-the-wild speech data!

### Visualization

Amphion provides visualization tools to interactively illustrate the internal processing mechanism of classic models. This provides an invaluable resource for educational purposes and for facilitating understandable research.

Currently, Amphion supports [SingVisio](egs/visualization/SingVisio/README.md), a visualization tool of the diffusion model for singing voice conversion. [![arXiv](https://img.shields.io/badge/arXiv-Paper-COLOR.svg)](https://arxiv.org/abs/2402.12660) [![openxlab](https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg)](https://openxlab.org.cn/apps/detail/Amphion/SingVisio) [![Video](https://img.shields.io/badge/Video-Demo-orange)](https://drive.google.com/file/d/15097SGhQh-SwUNbdWDYNyWEP--YGLba5/view)


## üìÄ Installation

Amphion can be installed through either Setup Installer or Docker Image.

### Setup Installer

```bash
git clone https://github.com/open-mmlab/Amphion.git
cd Amphion

# Install Python Environment
conda create --name amphion python=3.9.15
conda activate amphion

# Install Python Packages Dependencies
sh env.sh
```

### Docker Image

1. Install [Docker](https://docs.docker.com/get-docker/), [NVIDIA Driver](https://www.nvidia.com/download/index.aspx), [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html), and [CUDA](https://developer.nvidia.com/cuda-downloads).

2. Run the following commands:
```bash
git clone https://github.com/open-mmlab/Amphion.git
cd Amphion

docker pull realamphion/amphion
docker run --runtime=nvidia --gpus all -it -v .:/app realamphion/amphion
```
Mount dataset by argument `-v` is necessary when using Docker. Please refer to [Mount dataset in Docker container](egs/

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[bytedance/LatentSync]]></title>
            <link>https://github.com/bytedance/LatentSync</link>
            <guid>https://github.com/bytedance/LatentSync</guid>
            <pubDate>Fri, 21 Mar 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[Taming Stable Diffusion for Lip Sync!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/bytedance/LatentSync">bytedance/LatentSync</a></h1>
            <p>Taming Stable Diffusion for Lip Sync!</p>
            <p>Language: Python</p>
            <p>Stars: 3,112</p>
            <p>Forks: 465</p>
            <p>Stars today: 46 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;LatentSync&lt;/h1&gt;

&lt;div align=&quot;center&quot;&gt;

[![arXiv](https://img.shields.io/badge/arXiv-Paper-b31b1b)](https://arxiv.org/abs/2412.09262)
[![arXiv](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Model-yellow)](https://huggingface.co/ByteDance/LatentSync-1.5)
[![arXiv](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Space-yellow)](https://huggingface.co/spaces/fffiloni/LatentSync)
&lt;a href=&quot;https://replicate.com/lucataco/latentsync&quot;&gt;&lt;img src=&quot;https://replicate.com/lucataco/latentsync/badge&quot; alt=&quot;Replicate&quot;&gt;&lt;/a&gt;

&lt;/div&gt;

## üî• Updates

- `2025/03/14`: We released **LatentSync 1.5**, which **(1)** improves temporal consistency via adding temporal layer, **(2)** improves performance on Chinese videos and **(3)** reduces the VRAM requirement of the stage2 training to **20 GB** through a series of optimizations. Learn more details [here](docs/changelog_v1.5.md).

## üìñ Introduction

We present *LatentSync*, an end-to-end lip-sync method based on audio-conditioned latent diffusion models without any intermediate motion representation, diverging from previous diffusion-based lip-sync methods based on pixel-space diffusion or two-stage generation. Our framework can leverage the powerful capabilities of Stable Diffusion to directly model complex audio-visual correlations.

## üèóÔ∏è Framework

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;docs/framework.png&quot; width=100%&gt;
&lt;p&gt;

LatentSync uses the [Whisper](https://github.com/openai/whisper) to convert melspectrogram into audio embeddings, which are then integrated into the U-Net via cross-attention layers. The reference and masked frames are channel-wise concatenated with noised latents as the input of U-Net. In the training process, we use a one-step method to get estimated clean latents from predicted noises, which are then decoded to obtain the estimated clean frames. The TREPA, [LPIPS](https://arxiv.org/abs/1801.03924) and [SyncNet](https://www.robots.ox.ac.uk/~vgg/publications/2016/Chung16a/chung16a.pdf) losses are added in the pixel space.

## üé¨ Demo

&lt;table class=&quot;center&quot;&gt;
  &lt;tr style=&quot;font-weight: bolder;text-align:center;&quot;&gt;
        &lt;td width=&quot;50%&quot;&gt;&lt;b&gt;Original video&lt;/b&gt;&lt;/td&gt;
        &lt;td width=&quot;50%&quot;&gt;&lt;b&gt;Lip-synced video&lt;/b&gt;&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;
      &lt;video src=https://github.com/user-attachments/assets/ff3a84da-dc9b-498a-950f-5c54f58dd5c5 controls preload&gt;&lt;/video&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;video src=https://github.com/user-attachments/assets/150e00fd-381e-4421-a478-a9ea3d1212a8 controls preload&gt;&lt;/video&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;
      &lt;video src=https://github.com/user-attachments/assets/32c830a9-4d7d-4044-9b33-b184d8e11010 controls preload&gt;&lt;/video&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;video src=https://github.com/user-attachments/assets/84e4fe9d-b108-44a4-8712-13a012348145 controls preload&gt;&lt;/video&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;
      &lt;video src=https://github.com/user-attachments/assets/7510a448-255a-44ee-b093-a1b98bd3961d controls preload&gt;&lt;/video&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;video src=https://github.com/user-attachments/assets/6150c453-c559-4ae0-bb00-c565f135ff41 controls preload&gt;&lt;/video&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td width=300px&gt;
      &lt;video src=https://github.com/user-attachments/assets/0f7f9845-68b2-4165-bd08-c7bbe01a0e52 controls preload&gt;&lt;/video&gt;
    &lt;/td&gt;
    &lt;td width=300px&gt;
      &lt;video src=https://github.com/user-attachments/assets/c34fe89d-0c09-4de3-8601-3d01229a69e3 controls preload&gt;&lt;/video&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;
      &lt;video src=https://github.com/user-attachments/assets/7ce04d50-d39f-4154-932a-ec3a590a8f64 controls preload&gt;&lt;/video&gt;
    &lt;/td&gt;
    &lt;td&gt;
      &lt;video src=https://github.com/user-attachments/assets/70bde520-42fa-4a0e-b66c-d3040ae5e065 controls preload&gt;&lt;/video&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

(Photorealistic videos are filmed by contracted models, and anime videos are from [VASA-1](https://www.microsoft.com/en-us/research/project/vasa-1/) and [EMO](https://humanaigc.github.io/emote-portrait-alive/))

## üìë Open-source Plan

- [x] Inference code and checkpoints
- [x] Data processing pipeline
- [x] Training code

## üîß Setting up the Environment

Install the required packages and download the checkpoints via:

```bash
source setup_env.sh
```

If the download is successful, the checkpoints should appear as follows:

```
./checkpoints/
|-- latentsync_unet.pt
|-- stable_syncnet.pt
|-- whisper
|   `-- tiny.pt
|-- auxiliary
|   |-- 2DFAN4-cd938726ad.zip
|   |-- i3d_torchscript.pt
|   |-- koniq_pretrained.pkl
|   |-- s3fd-619a316812.pth
|   |-- sfd_face.pth
|   |-- syncnet_v2.model
|   |-- vgg16-397923af.pth
|   `-- vit_g_hybrid_pt_1200e_ssv2_ft.pth
```

These already include all the checkpoints required for latentsync training and inference. If you just want to try inference, you only need to download `latentsync_unet.pt` and `tiny.pt` from our [HuggingFace repo](https://huggingface.co/ByteDance/LatentSync-1.5)

## üöÄ Inference

There are two ways to perform inference, and both require **6.8 GB** of VRAM.

### 1. Gradio App

Run the Gradio app for inference:

```bash
python gradio_app.py
```

### 2. Command Line Interface

Run the script for inference:

```bash
./inference.sh
```

You can change the parameters `inference_steps` and `guidance_scale` to see more results.

## üîÑ Data Processing Pipeline

The complete data processing pipeline includes the following steps:

1. Remove the broken video files.
2. Resample the video FPS to 25, and resample the audio to 16000 Hz.
3. Scene detect via [PySceneDetect](https://github.com/Breakthrough/PySceneDetect).
4. Split each video into 5-10 second segments.
5. Affine transform the faces according to the landmarks detected by [face-alignment](https://github.com/1adrianb/face-alignment), then resize to 256 $\times$ 256.
6. Remove videos with [sync confidence score](https://www.robots.ox.ac.uk/~vgg/publications/2016/Chung16a/chung16a.pdf) lower than 3, and adjust the audio-visual offset to 0.
7. Calculate [hyperIQA](https://openaccess.thecvf.com/content_CVPR_2020/papers/Su_Blindly_Assess_Image_Quality_in_the_Wild_Guided_by_a_CVPR_2020_paper.pdf) score, and remove videos with scores lower than 40.

Run the script to execute the data processing pipeline:

```bash
./data_processing_pipeline.sh
```

You should change the parameter `input_dir` in the script to specify the data directory to be processed. The processed videos will be saved in the `high_visual_quality` directory. Each step will generate a new directory to prevent the need to redo the entire pipeline in case the process is interrupted by an unexpected error.

## üèãÔ∏è‚Äç‚ôÇÔ∏è Training U-Net

Before training, you must process the data as described above and download all the checkpoints. We released a pretrained SyncNet with 94% accuracy on both VoxCeleb2 and HDTF datasets for the supervision of U-Net training. Note that this SyncNet is trained on affine transformed videos, so when using or evaluating this SyncNet, you need to perform affine transformation on the video first (the code of affine transformation is included in the data processing pipeline).

If all the preparations are complete, you can train the U-Net with the following script:

```bash
./train_unet.sh
```

We prepared three UNet configuration files in the ``configs/unet`` directory, each corresponding to a different training setup:

- `stage1.yaml`: Stage1 training, requires **23 GB** VRAM.
- `stage2.yaml`: Stage2 training with optimal performance, requires **30 GB** VRAM.
- `stage2_efficient.yaml`: Efficient Stage 2 training, requires **20 GB** VRAM. It may lead to slight degradation in visual quality and temporal consistency compared with `stage2.yaml`, suitable for users with consumer-grade GPUs, such as the RTX 3090.

Also remember to change the parameters in U-Net config file to specify the data directory, checkpoint save path, and other training hyperparameters.

## üèãÔ∏è‚Äç‚ôÇÔ∏è Training SyncNet

In case you want to train SyncNet on your own datasets, you can run the following script. The data processing pipeline for SyncNet is the same as U-Net. 

```bash
./train_syncnet.sh
```

After `validations_steps` training, the loss charts will be saved in `train_output_dir`. They contain both the training and validation loss. If you want to customize the architecture of SyncNet for different image resolutions and input frame lengths, please follow the [guide](docs/syncnet_arch.md).

## üìä Evaluation

You can evaluate the [sync confidence score](https://www.robots.ox.ac.uk/~vgg/publications/2016/Chung16a/chung16a.pdf) of a generated video by running the following script:

```bash
./eval/eval_sync_conf.sh
```

You can evaluate the accuracy of SyncNet on a dataset by running the following script:

```bash
./eval/eval_syncnet_acc.sh
```

## üôè Acknowledgement

- Our code is built on [AnimateDiff](https://github.com/guoyww/AnimateDiff). 
- Some code are borrowed from [MuseTalk](https://github.com/TMElyralab/MuseTalk), [StyleSync](https://github.com/guanjz20/StyleSync), [SyncNet](https://github.com/joonson/syncnet_python), [Wav2Lip](https://github.com/Rudrabha/Wav2Lip).

Thanks for their generous contributions to the open-source community.

&lt;!-- ## Citation
If you find our repo useful for your research, please consider citing our paper:
```

``` --&gt;</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Azure/azure-sdk-for-python]]></title>
            <link>https://github.com/Azure/azure-sdk-for-python</link>
            <guid>https://github.com/Azure/azure-sdk-for-python</guid>
            <pubDate>Fri, 21 Mar 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[This repository is for active development of the Azure SDK for Python. For consumers of the SDK we recommend visiting our public developer docs at https://learn.microsoft.com/python/azure/ or our versioned developer docs at https://azure.github.io/azure-sdk-for-python.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Azure/azure-sdk-for-python">Azure/azure-sdk-for-python</a></h1>
            <p>This repository is for active development of the Azure SDK for Python. For consumers of the SDK we recommend visiting our public developer docs at https://learn.microsoft.com/python/azure/ or our versioned developer docs at https://azure.github.io/azure-sdk-for-python.</p>
            <p>Language: Python</p>
            <p>Stars: 4,838</p>
            <p>Forks: 2,931</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre># Azure SDK for Python

[![Packages](https://img.shields.io/badge/packages-latest-blue.svg)](https://azure.github.io/azure-sdk/releases/latest/python.html) [![Dependencies](https://img.shields.io/badge/dependency-report-blue.svg)](https://azuresdkartifacts.blob.core.windows.net/azure-sdk-for-python/dependencies/dependencies.html) [![DepGraph](https://img.shields.io/badge/dependency-graph-blue.svg)](https://azuresdkartifacts.blob.core.windows.net/azure-sdk-for-python/dependencies/dependencyGraph/index.html) [![Python](https://img.shields.io/pypi/pyversions/azure-core.svg?maxAge=2592000)](https://pypi.python.org/pypi/azure/) [![Build Status](https://dev.azure.com/azure-sdk/public/_apis/build/status/python/python%20-%20core%20-%20ci?branchName=main)](https://dev.azure.com/azure-sdk/public/_build/latest?definitionId=458&amp;branchName=main)

This repository is for the active development of the Azure SDK for Python. For consumers of the SDK we recommend visiting our [public developer docs](https://docs.microsoft.com/python/azure/) or our versioned [developer docs](https://azure.github.io/azure-sdk-for-python).

## Getting started

For your convenience, each service has a separate set of libraries that you can choose to use instead of one, large Azure package. To get started with a specific library, see the `README.md` (or `README.rst`) file located in the library&#039;s project folder.

You can find service libraries in the `/sdk` directory.

### Prerequisites

The client libraries are supported on Python 3.8 or later. For more details, please read our page on [Azure SDK for Python version support policy](https://github.com/Azure/azure-sdk-for-python/wiki/Azure-SDKs-Python-version-support-policy).

## Packages available

Each service might have a number of libraries available from each of the following categories:
* [Client - New Releases](#client-new-releases)
* [Client - Previous Versions](#client-previous-versions)
* [Management - New Releases](#management-new-releases)
* [Management - Previous Versions](#management-previous-versions)

### Client: New Releases

New wave of packages that we are announcing as **GA** and several that are currently releasing in **preview**. These libraries allow you to use and consume existing resources and interact with them, for example: upload a blob. These libraries share  several core functionalities such as: retries, logging, transport protocols, authentication protocols, etc. that can be found in the [azure-core](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/core/azure-core) library. You can learn more about these libraries by reading guidelines that they follow [here](https://azure.github.io/azure-sdk/python/guidelines/index.html).

You can find the [most up to date list of all of the new packages on our page](https://azure.github.io/azure-sdk/releases/latest/index.html#python)

&gt; NOTE: If you need to ensure your code is ready for production use one of the stable, non-preview libraries.

### Client: Previous Versions

Last stable versions of packages that have been provided for usage with Azure and are production-ready. These libraries provide you with similar functionalities to the Preview ones as they allow you to use and consume existing resources and interact with them, for example: upload a blob. They might not implement the [guidelines](https://azure.github.io/azure-sdk/python/guidelines/index.html) or have the same feature set as the November releases. They do however offer wider coverage of services.

### Management: New Releases
A new set of management libraries that follow the [Azure SDK Design Guidelines for Python](https://azure.github.io/azure-sdk/python/guidelines/) are now available. These new libraries provide a number of core capabilities that are shared amongst all Azure SDKs, including the intuitive Azure Identity library, an HTTP Pipeline with custom policies, error-handling, distributed tracing, and much more.
Documentation and code samples for these new libraries can be found [here](https://aka.ms/azsdk/python/mgmt). In addition, a migration guide that shows how to transition from older versions of libraries is located [here](https://github.com/Azure/azure-sdk-for-python/blob/main/doc/sphinx/mgmt_quickstart.rst#migration-guide).

You can find the [most up to date list of all of the new packages on our page](https://azure.github.io/azure-sdk/releases/latest/mgmt/python.html)

&gt; NOTE: If you need to ensure your code is ready for production use one of the stable, non-preview libraries. Also, if you are experiencing authentication issues with the management libraries after upgrading certain packages, it&#039;s possible that you upgraded to the new versions of SDK without changing the authentication code, please refer to the migration guide mentioned above for proper instructions.

### Management: Previous Versions
For a complete list of management libraries that enable you to provision and manage Azure resources, please [check here](https://azure.github.io/azure-sdk/releases/latest/all/python.html). They might not have the same feature set as the new releases but they do offer wider coverage of services.
Management libraries can be identified by namespaces that start with `azure-mgmt-`, e.g. `azure-mgmt-compute`

## Need help?

* For detailed documentation visit our [Azure SDK for Python documentation](https://aka.ms/python-docs)
* File an issue via [GitHub Issues](https://github.com/Azure/azure-sdk-for-python/issues)
* Check [previous questions](https://stackoverflow.com/questions/tagged/azure+python) or ask new ones on StackOverflow using `azure` and `python` tags.

### Reporting security issues and security bugs

Security issues and bugs should be reported privately, via email, to the Microsoft Security Response Center (MSRC) &lt;secure@microsoft.com&gt;. You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Further information, including the MSRC PGP key, can be found in the [Security TechCenter](https://www.microsoft.com/msrc/faqs-report-an-issue).

## Contributing

For details on contributing to this repository, see the [contributing guide](https://github.com/Azure/azure-sdk-for-python/blob/main/CONTRIBUTING.md).

This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit
https://cla.microsoft.com.

When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repositories using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)
or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.



</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/markitdown]]></title>
            <link>https://github.com/microsoft/markitdown</link>
            <guid>https://github.com/microsoft/markitdown</guid>
            <pubDate>Fri, 21 Mar 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[Python tool for converting files and office documents to Markdown.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/markitdown">microsoft/markitdown</a></h1>
            <p>Python tool for converting files and office documents to Markdown.</p>
            <p>Language: Python</p>
            <p>Stars: 40,579</p>
            <p>Forks: 1,912</p>
            <p>Stars today: 193 stars today</p>
            <h2>README</h2><pre># MarkItDown

[![PyPI](https://img.shields.io/pypi/v/markitdown.svg)](https://pypi.org/project/markitdown/)
![PyPI - Downloads](https://img.shields.io/pypi/dd/markitdown)
[![Built by AutoGen Team](https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue)](https://github.com/microsoft/autogen)

&gt; [!IMPORTANT]
&gt; Breaking changes between 0.0.1 to 0.1.0:
&gt; * Dependencies are now organized into optional feature-groups (further details below). Use `pip install &#039;markitdown[all]~=0.1.0a1&#039;` to have backward-compatible behavior. 
&gt; * The DocumentConverter class interface has changed to read from file-like streams rather than file paths. *No temporary files are created anymore*. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.

MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to [textract](https://github.com/deanmalmgren/textract), but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.

At present, MarkItDown supports:

- PDF
- PowerPoint
- Word
- Excel
- Images (EXIF metadata and OCR)
- Audio (EXIF metadata and speech transcription)
- HTML
- Text-based formats (CSV, JSON, XML)
- ZIP files (iterates over contents)
- Youtube URLs
- EPubs
- ... and more!

## Why Markdown?

Markdown is extremely close to plain text, with minimal markup or formatting, but still
provides a way to represent important document structure. Mainstream LLMs, such as
OpenAI&#039;s GPT-4o, natively &quot;_speak_&quot; Markdown, and often incorporate Markdown into their
responses unprompted. This suggests that they have been trained on vast amounts of
Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions
are also highly token-efficient.

## Installation

To install MarkItDown, use pip: `pip install &#039;markitdown[all]~=0.1.0a1&#039;`. Alternatively, you can install it from the source:

```bash
git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e packages/markitdown[all]
```

## Usage

### Command-Line

```bash
markitdown path-to-file.pdf &gt; document.md
```

Or use `-o` to specify the output file:

```bash
markitdown path-to-file.pdf -o document.md
```

You can also pipe content:

```bash
cat path-to-file.pdf | markitdown
```

### Optional Dependencies
MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the `[all]` option. However, you can also install them individually for more control. For example:

```bash
pip install markitdown[pdf, docx, pptx]
```

will install only the dependencies for PDF, DOCX, and PPTX files.

At the moment, the following optional dependencies are available:

* `[all]` Installs all optional dependencies
* `[pptx]` Installs dependencies for PowerPoint files
* `[docx]` Installs dependencies for Word files
* `[xlsx]` Installs dependencies for Excel files
* `[xls]` Installs dependencies for older Excel files
* `[pdf]` Installs dependencies for PDF files
* `[outlook]` Installs dependencies for Outlook messages
* `[az-doc-intel]` Installs dependencies for Azure Document Intelligence
* `[audio-transcription]` Installs dependencies for audio transcription of wav and mp3 files
* `[youtube-transcription]` Installs dependencies for fetching YouTube video transcription

### Plugins

MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:

```bash
markitdown --list-plugins
```

To enable plugins use:

```bash
markitdown --use-plugins path-to-file.pdf
```

To find available plugins, search GitHub for the hashtag `#markitdown-plugin`. To develop a plugin, see `packages/markitdown-sample-plugin`.

### Azure Document Intelligence

To use Microsoft Document Intelligence for conversion:

```bash
markitdown path-to-file.pdf -o document.md -d -e &quot;&lt;document_intelligence_endpoint&gt;&quot;
```

More information about how to set up an Azure Document Intelligence Resource can be found [here](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0)

### Python API

Basic usage in Python:

```python
from markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert(&quot;test.xlsx&quot;)
print(result.text_content)
```

Document Intelligence conversion in Python:

```python
from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint=&quot;&lt;document_intelligence_endpoint&gt;&quot;)
result = md.convert(&quot;test.pdf&quot;)
print(result.text_content)
```

To use Large Language Models for image descriptions, provide `llm_client` and `llm_model`:

```python
from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model=&quot;gpt-4o&quot;)
result = md.convert(&quot;example.jpg&quot;)
print(result.text_content)
```

### Docker

```sh
docker build -t markitdown:latest .
docker run --rm -i markitdown:latest &lt; ~/your-file.pdf &gt; output.md
```

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

### How to Contribute

You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as &#039;open for contribution&#039; and &#039;open for reviewing&#039; to help facilitate community contributions. These are ofcourse just suggestions and you are welcome to contribute in any way you like.

&lt;div align=&quot;center&quot;&gt;

|            | All                                                          | Especially Needs Help from Community                                                                                                      |
| ---------- | ------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------- |
| **Issues** | [All Issues](https://github.com/microsoft/markitdown/issues) | [Issues open for contribution](https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22) |
| **PRs**    | [All PRs](https://github.com/microsoft/markitdown/pulls)     | [PRs open for reviewing](https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22)              |

&lt;/div&gt;

### Running Tests and Checks

- Navigate to the MarkItDown package:

  ```sh
  cd packages/markitdown
  ```

- Install `hatch` in your environment and run tests:

  ```sh
  pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
  hatch shell
  hatch test
  ```

  (Alternative) Use the Devcontainer which has all the dependencies installed:

  ```sh
  # Reopen the project in Devcontainer and run:
  hatch test
  ```

- Run pre-commit checks before submitting a PR: `pre-commit run --all-files`

### Contributing 3rd-party Plugins

You can also contribute by creating and sharing 3rd party plugins. See `packages/markitdown-sample-plugin` for more details.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
[Microsoft&#039;s Trademark &amp; Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party&#039;s policies.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sqlalchemy/sqlalchemy]]></title>
            <link>https://github.com/sqlalchemy/sqlalchemy</link>
            <guid>https://github.com/sqlalchemy/sqlalchemy</guid>
            <pubDate>Fri, 21 Mar 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[The Database Toolkit for Python]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sqlalchemy/sqlalchemy">sqlalchemy/sqlalchemy</a></h1>
            <p>The Database Toolkit for Python</p>
            <p>Language: Python</p>
            <p>Stars: 10,185</p>
            <p>Forks: 1,492</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NVIDIA/physicsnemo]]></title>
            <link>https://github.com/NVIDIA/physicsnemo</link>
            <guid>https://github.com/NVIDIA/physicsnemo</guid>
            <pubDate>Fri, 21 Mar 2025 00:04:20 GMT</pubDate>
            <description><![CDATA[Open-source deep-learning framework for building, training, and fine-tuning deep learning models using state-of-the-art Physics-ML methods]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NVIDIA/physicsnemo">NVIDIA/physicsnemo</a></h1>
            <p>Open-source deep-learning framework for building, training, and fine-tuning deep learning models using state-of-the-art Physics-ML methods</p>
            <p>Language: Python</p>
            <p>Stars: 1,285</p>
            <p>Forks: 294</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre># NVIDIA PhysicsNeMo

&lt;!-- markdownlint-disable --&gt;

üìù NVIDIA Modulus has been renamed to NVIDIA PhysicsNeMo

[![Project Status: Active - The project has reached a stable, usable state and is being actively developed.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)
[![GitHub](https://img.shields.io/github/license/NVIDIA/physicsnemo)](https://github.com/NVIDIA/physicsnemo/blob/master/LICENSE.txt)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
&lt;!-- markdownlint-enable --&gt;
[**Getting Started**](#getting-started)
| [**Install guide**](#installation)
| [**Contributing Guidelines**](#contributing-to-physicsnemo)
| [**Resources**](#resources)
| [**PhysicsNeMo Migration Guide**](#physicsnemo-migration-guide)
| [**Communication**](#communication)
| [**License**](#license)

## What is PhysicsNeMo?

NVIDIA PhysicsNeMo is an open-source deep-learning framework for building, training,
and fine-tuning deep learning models using state-of-the-art SciML methods for
AI4science and engineering.

PhysicsNeMo provides utilities and optimized pipelines to develop AI models that combine
physics knowledge with data, enabling real-time predictions.

Whether you are exploring the use of Neural operators, GNNs, or transformers or are
interested in Physics-informed Neural Networks or a hybrid approach in between, PhysicsNeMo
provides you with an optimized stack that will enable you to train your models at scale.

&lt;!-- markdownlint-disable --&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=https://raw.githubusercontent.com/NVIDIA/physicsnemo/main/docs/img/value_prop/Knowledge_guided_models.gif alt=&quot;PhysicsNeMo&quot;/&gt;
&lt;/p&gt;
&lt;!-- markdownlint-enable --&gt;

&lt;!-- toc --&gt;

- [More About PhysicsNeMo](#more-about-physicsnemo)
  - [Scalable GPU-optimized training Library](#scalable-gpu-optimized-training-library)
  - [A suite of Physics-Informed ML Models](#a-suite-of-physics-informed-ml-models)
  - [Seamless PyTorch Integration](#seamless-pytorch-integration)
  - [Easy Customization and Extension](#easy-customization-and-extension)
  - [AI4Science Library](#ai4science-library)
    - [Domain Specific Packages](#domain-specific-packages)
- [Who is contributing to PhysicsNeMo](#who-is-using-and-contributing-to-physicsnemo)
- [Why use PhysicsNeMo](#why-are-they-using-physicsnemo)
- [Getting Started](#getting-started)
- [Resources](#resources)
- [Installation](#installation)
- [Contributing](#contributing-to-physicsnemo)
- [Communication](#communication)
- [License](#license)
  
&lt;!-- tocstop --&gt;

## More About PhysicsNeMo

At a granular level, PhysicsNeMo provides a library of a few key components:

&lt;!-- markdownlint-disable --&gt;
Component | Description |
---- | --- |
[**physicsnemo.models**](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.models.html) | A collection of optimized, customizable, and easy-to-use models such as Fourier Neural Operators, Graph Neural Networks, and many more|
[**physicsnemo.datapipes**](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.datapipes.html) | A data pipeline and data loader library, including benchmark datapipes, weather daptapipes, and graph datapipes|
[**physicsnemo.distributed**](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.distributed.html) | A distributed computing library build on top of `torch.distributed` to enable parallel training with just a few steps|
[**physicsnemo.sym.geometry**](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-sym/user_guide/features/csg_and_tessellated_module.html) | A library to handle geometry for DL training using the Constructive Solid Geometry modeling and CAD files in STL format.|
[**physicsnemo.sym.eq**](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-sym/user_guide/features/nodes.html) | A library to use PDEs in your DL training with several implementations of commonly observed equations and easy ways for customization.|
&lt;!-- markdownlint-enable --&gt;

For a complete list, refer to the PhysicsNeMo API documentation for
[PhysicsNeMo Core](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/index.html)
and
[PhysicsNeMo Sym](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-sym/api/api_index.html).

Usually, PhysicsNeMo is used either as:

- A complementary tool to Pytorch when exploring AI for SciML and AI4Science applications.
- A deep learning research platform that provides scale and optimal performance on
NVIDIA GPUs.

Elaborating Further:

### Scalable GPU-optimized training Library

PhysicsNeMo provides a highly optimized and scalable training library for maximizing the
power of NVIDIA GPUs.
[Distributed computing](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.distributed.html)
utilities allow for efficient scaling from a single GPU to multi-node GPU clusters with
a few lines of code, ensuring that large-scale.
physics-informed machine learning (ML) models can be trained quickly and effectively.
The framework includes support for advanced.
[optimization utilities](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.utils.html#module-physicsnemo.utils.capture),
[tailor made datapipes](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.datapipes.html),
[validation utilities](https://github.com/NVIDIA/physicsnemo-sym/tree/main/physicsnemo/sym/eq)
to enhance the end to end training speed.

### A suite of Physics Informed ML Models

PhysicsNeMo offers a comprehensive library of state-of-the-art models specifically designed
for physics-ML applications.
The [Model Zoo](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.models.html#model-zoo)
includes generalizable model architectures such as
[Fourier Neural Operators (FNOs)](physicsnemo/models/fno),
[DeepONet](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-sym/user_guide/neural_operators/deeponet.html),
[Physics-Informed Neural Networks (PINNs)](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-sym/user_guide/foundational/1d_wave_equation.html),
[Graph Neural Networks (GNNs)](physicsnemo/models/gnn_layers),
and generative AI models like [Diffusion Models](physicsnemo/models/diffusion)
as well as domain-specific models such as [Deep Learning Weather Prediction (DLWP)](physicsnemo/models/dlwp)
and [Super Resolution Network (SrNN)](physicsnemo/models/srrn) among others.
These models are optimized for various physics domains, such as computational fluid
dynamics, structural mechanics, and electromagnetics. Users can download, customize, and
build upon these models to suit their specific needs, significantly reducing the time
required to develop high-fidelity simulations.

### Seamless PyTorch Integration

PhysicsNeMo is built on top of PyTorch, providing a familiar and user-friendly experience
for those already proficient with PyTorch.
This includes a simple Python interface and modular design, making it easy to use
PhysicsNeMo with existing PyTorch workflows.
Users can leverage the extensive PyTorch ecosystem, including its libraries and tools
while benefiting from PhysicsNeMo&#039;s specialized capabilities for physics-ML. This seamless
integration ensures users can quickly adopt PhysicsNeMo without a steep learning curve.

For more information, refer [Converting PyTorch Models to PhysicsNeMo Models](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.models.html#converting-pytorch-models-to-physicsnemo-models)

### Easy Customization and Extension

PhysicsNeMo is designed to be highly extensible, allowing users to add new functionality
with minimal effort. The framework provides Pythonic APIs for
defining new physics models, geometries, and constraints, making it easy to extend its
capabilities to new use cases.
The adaptability of PhysicsNeMo is further enhanced by key features such as
[ONNX support](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.deploy.html)
for flexible model deployment,
robust [logging utilities](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.launch.logging.html)
for streamlined error handling,
and efficient
[checkpointing](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.launch.utils.html#module-physicsnemo.launch.utils.checkpoint)
to simplify model loading and saving.

This extensibility ensures that PhysicsNeMo can adapt to the evolving needs of researchers
and engineers, facilitating the development of innovative solutions in the field of physics-ML.

Detailed information on features and capabilities can be found in the [PhysicsNeMo documentation](https://docs.nvidia.com/physicsnemo/index.html#core).

[Reference samples](examples/README.md) cover a broad spectrum of physics-constrained
and data-driven
workflows to suit the diversity of use cases in the science and engineering disciplines.

&gt; [!TIP]
&gt; Have questions about how PhysicsNeMo can assist you? Try our [Experimental] chatbot,
&gt; [PhysicsNeMo Guide](https://chatgpt.com/g/g-PXrBv20SC-modulus-guide), for answers.

### Hello world

You can start using PhysicsNeMo in your PyTorch code as simple as shown here:

```python
python
&gt;&gt;&gt; import torch
&gt;&gt;&gt; from physicsnemo.models.mlp.fully_connected import FullyConnected
&gt;&gt;&gt; model = FullyConnected(in_features=32, out_features=64)
&gt;&gt;&gt; input = torch.randn(128, 32)
&gt;&gt;&gt; output = model(input)
&gt;&gt;&gt; output.shape
torch.Size([128, 64])
```

### AI4Science Library

- [PhysicsNeMo Symbolic](https://github.com/NVIDIA/physicsnemo-sym): This repository of
  algorithms and utilities allows SciML researchers and developers to physics inform model
  training and model validation. It also provides a higher level abstraction
  for domain experts that is native to science and engineering.

#### Domain Specific Packages

The following are packages dedicated for domain experts of specific communities catering
to their unique exploration needs.

- [Earth-2 Studio](https://github.com/NVIDIA/earth2studio): Open source project
  to enable climate researchers and scientists to explore and experiment with
  AI models for weather and climate.

#### Research packages

The following are research packages that get packaged into PhysicsNeMo once they are stable.

- [PhysicsNeMo Makani](https://github.com/NVIDIA/modulus-makani): Experimental library
  designed to enable the research and development of machine-learning based weather and
  climate models.
- [Earth2 Grid](https://github.com/NVlabs/earth2grid): Experimental library with
  utilities for working geographic data defined on various grids.
- [Earth-2 MIP](https://github.com/NVIDIA/earth2mip): Experimental library with
  utilities for model intercomparison for weather and climate models.

## Who is using and contributing to PhysicsNeMo

PhysicsNeMo is an open source project and gets contributions from researchers in
the SciML and AI4science fields. While PhysicsNeMo team works on optimizing the
underlying SW stack, the community collaborates and contributes model architectures,
datasets, and reference applications so we can innovate in the pursuit of
developing generalizable model architectures and algorithms.

Some latest examples of community contributors are [HP Labs 3D Printing team](https://developer.nvidia.com/blog/spotlight-hp-3d-printing-and-nvidia-physicsnemo-collaborate-on-open-source-manufacturing-digital-twin/),
[Stanford Cardiovascular research team](https://developer.nvidia.com/blog/enabling-greater-patient-specific-cardiovascular-care-with-ai-surrogates/),
[UIUC team](https://github.com/NVIDIA/physicsnemo/tree/main/examples/cfd/mhd_pino),
[CMU team](https://github.com/NVIDIA/physicsnemo/tree/main/examples/generative/diffusion)
etc.

Latest examples of research teams using PhysicsNeMo are
[ORNL team](https://arxiv.org/abs/2404.05768),
[TU Munich CFD team](https://www.nvidia.com/en-us/on-demand/session/gtc24-s62237/) etc.

Please navigate to this page for a complete list of research work leveraging PhysicsNeMo.
For a list of enterprises using PhysicsNeMo refer [here](https://developer.nvidia.com/physicsnemo).

Using PhysicsNeMo and interested in showcasing your work on
[NVIDIA Blogs](https://developer.nvidia.com/blog/category/simulation-modeling-design/)?
Fill out this [proposal form](https://forms.gle/XsBdWp3ji67yZAUF7) and we will get back
to you!

## Why are they using PhysicsNeMo

Here are some of the key benefits of PhysicsNeMo for SciML model development:

&lt;!-- markdownlint-disable --&gt;
&lt;img src=&quot;docs/img/value_prop/benchmarking.svg&quot; width=&quot;100&quot;&gt; | &lt;img src=&quot;docs/img/value_prop/recipe.svg&quot; width=&quot;100&quot;&gt; | &lt;img src=&quot;docs/img/value_prop/performance.svg&quot; width=&quot;100&quot;&gt;
---|---|---|
|SciML Benchmarking and validation|Ease of using generalized SciML recipes with heterogenous datasets |Out of the box performance and scalability
|PhysicsNeMo enables researchers to benchmark their AI model against proven architectures for standard benchmark problems with detailed domain-specific validation criteria.|PhysicsNeMo enables researchers to pick from SOTA SciML architectures and use built-in data pipelines for their use case.| PhysicsNeMo provides out-of-the-box performant training pipelines including optimized ETL pipelines for heterogrneous engineering and scientific datasets and out of the box scaling across multi-GPU and multi-node GPUs.
&lt;!-- markdownlint-enable --&gt;

See what your peer SciML researchers are saying about PhysicsNeMo (Coming soon).

## Getting started

The following resources will help you in learning how to use PhysicsNeMo. The best
way is to start with a reference sample and then update it for your own use case.

- [Using PhysicsNeMo with your PyTorch model](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/tutorials/simple_training_example.html#using-custom-models-in-physicsnemo)
- [Using PhysicsNeMo built-in models](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/tutorials/simple_training_example.html#using-built-in-models)
- [Getting started Guide](https://docs.nvidia.com/deeplearning/physicsnemo/getting-started/index.html)
- [Reference Samples](https://github.com/NVIDIA/physicsnemo/blob/main/examples/README.md)
- [User guide Documentation](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/index.html)

## Resources

- [Getting started Webinar](https://www.nvidia.com/en-us/on-demand/session/gtc24-dlit61460/?playlistId=playList-bd07f4dc-1397-4783-a959-65cec79aa985)
- [AI4Science PhysicsNeMo Bootcamp](https://github.com/openhackathons-org/End-to-End-AI-for-Science)
- [PhysicsNeMo Pretrained models](https://catalog.ngc.nvidia.com/models?filters=&amp;orderBy=scoreDESC&amp;query=Modulus&amp;page=&amp;pageSize=)
- [PhysicsNeMo Datasets and Supplementary materials](https://catalog.ngc.nvidia.com/resources?filters=&amp;orderBy=scoreDESC&amp;query=Modulus&amp;page=&amp;pageSize=)
- [Self-paced PhysicsNeMo DLI training](https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+S-OV-04+V1)
- [Deep Learning for Science and Engineering Lecture Series with PhysicsNeMo](https://www.nvidia.com/en-us/on-demand/deep-learning-for-science-and-engineering/)
  - [PhysicsNeMo: purpose and usage](https://www.nvidia.com/en-us/on-demand/session/dliteachingkit-setk5002/)
- [Video Tutorials](https://www.nvidia.com/en-us/on-demand/search/?facet.mimetype[]=event%20session&amp;layout=list&amp;page=1&amp;q=modulus&amp;sort=relevance&amp;sortDir=desc)
  
## Installation

### PyPi

The recommended method for installing the latest version of PhysicsNeMo is using PyPi:

```Bash
pip install nvidia-physicsnemo
```

The installation can be verified by running the hello world example as demonstrated [here](#hello-world).

#### Optional dependencies

PhysicsNeMo has many optional dependencies that are used in specific components.
When using pip, all dependencies used in PhysicsNeMo can be installed with
`pip install nvidia-physicsnemo[all]`. If you are developing PhysicsNeMo, developer dependencies
can be installed using `pip install nvidia-physicsnemo[dev]`. Otherwise, additional dependencies
can be installed on a case by case basis. Detailed information on installing the
optional dependencies can be found in the
[Getting Started Guide](https://docs.nvidia.com/deeplearning/physicsnemo/getting-started/index.html).

### NVCR Container

The recommended PhysicsNeMo docker image can be pulled from the
[NVIDIA Container Registry](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/physicsnemo/containers/physicsnemo)
(refer to the NGC registry for the latest tag):

```Bash
docker pull nvcr.io/nvidia/physicsnemo/physicsnemo:25.03
```

Inside the container, you can clone the PhysicsNeMo git repositories and get
started with the examples. The below command shows the instructions to launch
the physicsnemo container and run examples from this repo.

```bash
docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 --runtime nvidia \
--rm -it nvcr.io/nvidia/physicsnemo/physicsnemo:25.03 bash
git clone https://github.com/NVIDIA/physicsnemo.git
cd physicsnemo/examples/cfd/darcy_fno/
pip install warp-lang # install NVIDIA Warp to run the darcy example
python train_fno_darcy.py
```

For enterprise supported NVAIE container, refer [PhysicsNeMo Secured Feature Branch](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/modulus/containers/modulus-sfb)

## From Source

### Package

For a local build of the PhysicsNeMo Python package from source use:

```Bash
git clone git@github.com:NVIDIA/physicsnemo.git &amp;&amp; cd physicsnemo

pip install --upgrade pip
pip install .
```

### Source Container

To build PhysicsNeMo docker image:

```bash
docker build -t physicsnemo:deploy \
    --build-arg TARGETPLATFORM=linux/amd64 --target deploy -f Dockerfile .
```

Alternatively, you can run `make container-deploy`

To build CI image:

```bash
docker build -t physicsnemo:ci \
    --build-arg TARGETPLATFORM=linux/amd64 --target ci -f Dockerfile .
```

Alternatively, you can run `make container-ci`.

Currently, only `linux/amd64` and `linux/arm64` platforms are supported. If using
`linux/arm64`, some dependencies like `warp-lang` might not install correctly.

## PhysicsNeMo Migration Guide

NVIDIA Modulus has been renamed to NVIDIA PhysicsNeMo. For migration:

- Use `pip install nvidia-physicsnemo` rather than `pip install nvidia-modulus`
  for PyPi wheels.
- Use `nvcr.io/nvidia/physicsnemo/physicsnemo:&lt;tag&gt;` rather than
  `nvcr.io/nvidia/modulus/modulus:&lt;tag&gt;` for Docker containers.
- Replace `nvidia-modulus` by `nvidia-physicsnemo` in your pip requirements
  files (`requirements.txt`, `setup.py`, `setup.cfg`, `pyproject.toml`, etc.)
- In your code, change the import statements from `import modulus` to
  `import physicsnemo`

The old PyPi registry and the NGC container registry will be deprecated soon
and will not receive any bug fixes/updates. The old checkpoints will remain
compatible with these updates.

More details to follow soon.

## Contributing to PhysicsNeMo

PhysicsNeMo is an open source collaboration and its success is rooted in community
contribution to further the field of Physics-ML. Thank you for contributing to the
project so others can build on top of your contribution.

For guidance on contributing to PhysicsNeMo, please refer to the
[contributing guidelines](CONTRIBUTING.md).

## Cite PhysicsNeMo

If PhysicsNeMo helped your research and you would like to cite it, please refer to the [guidelines](https://github.com/NVIDIA/physicsnemo/blob/main/CITATION.cff)

## Communication

- Github Discussions: Discuss new architectures, implementations, Physics-ML research, etc.
- GitHub Issues: Bug reports, feature requests, install issues, etc.
- PhysicsNeMo Forum: The [PhysicsNeMo Forum](https://forums.developer.nvidia.com/t/welcome-to-the-physicsnemo-ml-model-framework-forum/178556)
hosts an audience of new to m

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[andrewyng/aisuite]]></title>
            <link>https://github.com/andrewyng/aisuite</link>
            <guid>https://github.com/andrewyng/aisuite</guid>
            <pubDate>Fri, 21 Mar 2025 00:04:19 GMT</pubDate>
            <description><![CDATA[Simple, unified interface to multiple Generative AI providers]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/andrewyng/aisuite">andrewyng/aisuite</a></h1>
            <p>Simple, unified interface to multiple Generative AI providers</p>
            <p>Language: Python</p>
            <p>Stars: 11,778</p>
            <p>Forks: 1,149</p>
            <p>Stars today: 24 stars today</p>
            <h2>README</h2><pre># aisuite

[![PyPI](https://img.shields.io/pypi/v/aisuite)](https://pypi.org/project/aisuite/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

Simple, unified interface to multiple Generative AI providers.

`aisuite` makes it easy for developers to use multiple LLM through a standardized interface. Using an interface similar to OpenAI&#039;s, `aisuite` makes it easy to interact with the most popular LLMs and compare the results. It is a thin wrapper around python client libraries, and allows creators to seamlessly swap out and test responses from different LLM providers without changing their code. Today, the library is primarily focussed on chat completions. We will expand it cover more use cases in near future.

Currently supported providers are:
- Anthropic
- AWS
- Azure
- Cerebras
- Google
- Groq
- HuggingFace Ollama
- Mistral
- OpenAI
- Sambanova
- Watsonx

To maximize stability, `aisuite` uses either the HTTP endpoint or the SDK for making calls to the provider.

## Installation

You can install just the base `aisuite` package, or install a provider&#039;s package along with `aisuite`.

This installs just the base package without installing any provider&#039;s SDK.

```shell
pip install aisuite
```

This installs aisuite along with anthropic&#039;s library.

```shell
pip install &#039;aisuite[anthropic]&#039;
```

This installs all the provider-specific libraries

```shell
pip install &#039;aisuite[all]&#039;
```

## Set up

To get started, you will need API Keys for the providers you intend to use. You&#039;ll need to
install the provider-specific library either separately or when installing aisuite.

The API Keys can be set as environment variables, or can be passed as config to the aisuite Client constructor.
You can use tools like [`python-dotenv`](https://pypi.org/project/python-dotenv/) or [`direnv`](https://direnv.net/) to set the environment variables manually. Please take a look at the `examples` folder to see usage.

Here is a short example of using `aisuite` to generate chat completion responses from gpt-4o and claude-3-5-sonnet.

Set the API keys.

```shell
export OPENAI_API_KEY=&quot;your-openai-api-key&quot;
export ANTHROPIC_API_KEY=&quot;your-anthropic-api-key&quot;
```

Use the python client.

```python
import aisuite as ai
client = ai.Client()

models = [&quot;openai:gpt-4o&quot;, &quot;anthropic:claude-3-5-sonnet-20240620&quot;]

messages = [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Respond in Pirate English.&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Tell me a joke.&quot;},
]

for model in models:
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.75
    )
    print(response.choices[0].message.content)

```

Note that the model name in the create() call uses the format - `&lt;provider&gt;:&lt;model-name&gt;`.
`aisuite` will call the appropriate provider with the right parameters based on the provider value.
For a list of provider values, you can look at the directory - `aisuite/providers/`. The list of supported providers are of the format - `&lt;provider&gt;_provider.py` in that directory. We welcome  providers adding support to this library by adding an implementation file in this directory. Please see section below for how to contribute.

For more examples, check out the `examples` directory where you will find several notebooks that you can run to experiment with the interface.

## Adding support for a provider

We have made easy for a provider or volunteer to add support for a new platform.

### Naming Convention for Provider Modules

We follow a convention-based approach for loading providers, which relies on strict naming conventions for both the module name and the class name. The format is based on the model identifier in the form `provider:model`.

- The provider&#039;s module file must be named in the format `&lt;provider&gt;_provider.py`.
- The class inside this module must follow the format: the provider name with the first letter capitalized, followed by the suffix `Provider`.

#### Examples

- **Hugging Face**:
  The provider class should be defined as:

  ```python
  class HuggingfaceProvider(BaseProvider)
  ```

  in providers/huggingface_provider.py.
  
- **OpenAI**:
  The provider class should be defined as:

  ```python
  class OpenaiProvider(BaseProvider)
  ```

  in providers/openai_provider.py

This convention simplifies the addition of new providers and ensures consistency across provider implementations.

## Tool Calling

`aisuite` provides a simple abstraction for tool/function calling that works across supported providers. This is in addition to the regular abstraction of passing JSON spec of the tool to the `tools` parameter. The tool calling abstraction makes it easy to use tools with different LLMs without changing your code.

There are two ways to use tools with `aisuite`:

### 1. Manual Tool Handling

This is the default behavior when `max_turns` is not specified.
You can pass tools in the OpenAI tool format:

```python
def will_it_rain(location: str, time_of_day: str):
    &quot;&quot;&quot;Check if it will rain in a location at a given time today.
    
    Args:
        location (str): Name of the city
        time_of_day (str): Time of the day in HH:MM format.
    &quot;&quot;&quot;
    return &quot;YES&quot;

tools = [{
    &quot;type&quot;: &quot;function&quot;,
    &quot;function&quot;: {
        &quot;name&quot;: &quot;will_it_rain&quot;,
        &quot;description&quot;: &quot;Check if it will rain in a location at a given time today&quot;,
        &quot;parameters&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {
                &quot;location&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;Name of the city&quot;
                },
                &quot;time_of_day&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;Time of the day in HH:MM format.&quot;
                }
            },
            &quot;required&quot;: [&quot;location&quot;, &quot;time_of_day&quot;]
        }
    }
}]

response = client.chat.completions.create(
    model=&quot;openai:gpt-4o&quot;,
    messages=messages,
    tools=tools
)
```

### 2. Automatic Tool Execution

When `max_turns` is specified, you can pass a list of callable Python functions as the `tools` parameter. `aisuite` will automatically handle the tool calling flow:

```python
def will_it_rain(location: str, time_of_day: str):
    &quot;&quot;&quot;Check if it will rain in a location at a given time today.
    
    Args:
        location (str): Name of the city
        time_of_day (str): Time of the day in HH:MM format.
    &quot;&quot;&quot;
    return &quot;YES&quot;

client = ai.Client()
messages = [{
    &quot;role&quot;: &quot;user&quot;,
    &quot;content&quot;: &quot;I live in San Francisco. Can you check for weather &quot;
               &quot;and plan an outdoor picnic for me at 2pm?&quot;
}]

# Automatic tool execution with max_turns
response = client.chat.completions.create(
    model=&quot;openai:gpt-4o&quot;,
    messages=messages,
    tools=[will_it_rain],
    max_turns=2  # Maximum number of back-and-forth tool calls
)
print(response.choices[0].message.content)
```

When `max_turns` is specified, `aisuite` will:
1. Send your message to the LLM
2. Execute any tool calls the LLM requests
3. Send the tool results back to the LLM
4. Repeat until the conversation is complete or max_turns is reached

In addition to `response.choices[0].message`, there is an additional field `response.choices[0].intermediate_messages`: which contains the list of all messages including tool interactions used. This can be used to continue the conversation with the model.
For more detailed examples of tool calling, check out the `examples/tool_calling_abstraction.ipynb` notebook.

## License

aisuite is released under the MIT License. You are free to use, modify, and distribute the code for both commercial and non-commercial purposes.

## Contributing

If you would like to contribute, please read our [Contributing Guide](https://github.com/andrewyng/aisuite/blob/main/CONTRIBUTING.md) and join our [Discord](https://discord.gg/T6Nvn8ExSb) server!
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jianchang512/clone-voice]]></title>
            <link>https://github.com/jianchang512/clone-voice</link>
            <guid>https://github.com/jianchang512/clone-voice</guid>
            <pubDate>Fri, 21 Mar 2025 00:04:18 GMT</pubDate>
            <description><![CDATA[A sound cloning tool with a web interface, using your voice or any sound to record audio / ‰∏Ä‰∏™Â∏¶webÁïåÈù¢ÁöÑÂ£∞Èü≥ÂÖãÈöÜÂ∑•ÂÖ∑Ôºå‰ΩøÁî®‰Ω†ÁöÑÈü≥Ëâ≤Êàñ‰ªªÊÑèÂ£∞Èü≥Êù•ÂΩïÂà∂Èü≥È¢ë]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jianchang512/clone-voice">jianchang512/clone-voice</a></h1>
            <p>A sound cloning tool with a web interface, using your voice or any sound to record audio / ‰∏Ä‰∏™Â∏¶webÁïåÈù¢ÁöÑÂ£∞Èü≥ÂÖãÈöÜÂ∑•ÂÖ∑Ôºå‰ΩøÁî®‰Ω†ÁöÑÈü≥Ëâ≤Êàñ‰ªªÊÑèÂ£∞Èü≥Êù•ÂΩïÂà∂Èü≥È¢ë</p>
            <p>Language: Python</p>
            <p>Stars: 8,277</p>
            <p>Forks: 866</p>
            <p>Stars today: 104 stars today</p>
            <h2>README</h2><pre>[English README](./README_EN.md)  / [ÊçêÂä©È°πÁõÆ](https://github.com/jianchang512/pyvideotrans/issues/80) / [Discord](https://discord.gg/7ZWbwKGMcx)

# CVÂ£∞Èü≥ÂÖãÈöÜÂ∑•ÂÖ∑

&gt; Êú¨È°πÁõÆÊâÄÁî®Ê®°Âûã‰∏∫[coqui.ai](https://coqui.ai/)Âá∫ÂìÅÁöÑxtts_v2ÔºåÊ®°ÂûãÂºÄÊ∫êÂçèËÆÆ‰∏∫[Coqui Public Model License 1.0.0](https://coqui.ai/cpml.txt),‰ΩøÁî®Êú¨È°πÁõÆËØ∑ÈÅµÂæ™ËØ•ÂçèËÆÆÔºåÂçèËÆÆÂÖ®ÊñáËßÅ https://coqui.ai/cpml.txt


 ËøôÊòØ‰∏Ä‰∏™Â£∞Èü≥ÂÖãÈöÜÂ∑•ÂÖ∑ÔºåÂèØ‰ΩøÁî®‰ªª‰Ωï‰∫∫Á±ªÈü≥Ëâ≤ÔºåÂ∞Ü‰∏ÄÊÆµÊñáÂ≠óÂêàÊàê‰∏∫‰ΩøÁî®ËØ•Èü≥Ëâ≤ËØ¥ËØùÁöÑÂ£∞Èü≥ÔºåÊàñËÄÖÂ∞Ü‰∏Ä‰∏™Â£∞Èü≥‰ΩøÁî®ËØ•Èü≥Ëâ≤ËΩ¨Êç¢‰∏∫Âè¶‰∏Ä‰∏™Â£∞Èü≥„ÄÇ
 
 ‰ΩøÁî®ÈùûÂ∏∏ÁÆÄÂçïÔºåÊ≤°ÊúâNÂç°GPU‰πüÂèØ‰ª•‰ΩøÁî®Ôºå‰∏ãËΩΩÈ¢ÑÁºñËØëÁâàÊú¨ÔºåÂèåÂáª app.exe ÊâìÂºÄ‰∏Ä‰∏™webÁïåÈù¢ÔºåÈº†Ê†áÁÇπÁÇπÂ∞±ËÉΩÁî®„ÄÇ
 
 ÊîØÊåÅ **‰∏≠„ÄÅËã±„ÄÅÊó•„ÄÅÈü©„ÄÅÊ≥ï„ÄÅÂæ∑„ÄÅÊÑèÁ≠â16ÁßçËØ≠Ë®Ä**ÔºåÂèØÂú®Á∫ø‰ªéÈ∫¶ÂÖãÈ£éÂΩïÂà∂Â£∞Èü≥„ÄÇ
 
 ‰∏∫‰øùËØÅÂêàÊàêÊïàÊûúÔºåÂª∫ËÆÆÂΩïÂà∂Êó∂Èïø5ÁßíÂà∞20ÁßíÔºåÂèëÈü≥Ê∏ÖÊô∞ÂáÜÁ°ÆÔºå‰∏çË¶ÅÂ≠òÂú®ËÉåÊôØÂô™Â£∞„ÄÇ
 
 Ëã±ÊñáÊïàÊûúÂæàÊ£íÔºå‰∏≠ÊñáÊïàÊûúËøòÂáëÂêà„ÄÇ


&gt; **[ËµûÂä©ÂïÜ]**
&gt; 
&gt; [![](https://github.com/user-attachments/assets/5348c86e-2d5f-44c7-bc1b-3cc5f077e710)](https://gpt302.saaslink.net/teRK8Y)
&gt;  [302.AI](https://gpt302.saaslink.net/teRK8Y)ÊòØ‰∏Ä‰∏™ÊåâÈúÄ‰ªòË¥πÁöÑ‰∏ÄÁ´ôÂºèAIÂ∫îÁî®Âπ≥Âè∞ÔºåÂºÄÊîæÂπ≥Âè∞ÔºåÂºÄÊ∫êÁîüÊÄÅ, [302.AIÂºÄÊ∫êÂú∞ÂùÄ](https://github.com/302ai)
&gt; 
&gt; ÈõÜÂêà‰∫ÜÊúÄÊñ∞ÊúÄÂÖ®ÁöÑAIÊ®°ÂûãÂíåÂìÅÁâå/ÊåâÈúÄ‰ªòË¥πÈõ∂ÊúàË¥π/ÁÆ°ÁêÜÂíå‰ΩøÁî®ÂàÜÁ¶ª/ÊâÄÊúâAIËÉΩÂäõÂùáÊèê‰æõAPI/ÊØèÂë®Êé®Âá∫2-3‰∏™Êñ∞Â∫îÁî®


# ËßÜÈ¢ëÊºîÁ§∫


https://github.com/jianchang512/clone-voice/assets/3378335/4e63f2ac-cc68-4324-a4d9-ecf4d4f81acd



![image](https://github.com/jianchang512/clone-voice/assets/3378335/5401a3f8-1623-452b-b0b3-cb2efe87e3d1)




# windowÈ¢ÑÁºñËØëÁâà‰ΩøÁî®ÊñπÊ≥ï(ÂÖ∂‰ªñÁ≥ªÁªüÂèØÊ∫êÁ†ÅÈÉ®ÁΩ≤)

1. [ÁÇπÂáªÊ≠§Â§ÑÊâìÂºÄReleases‰∏ãËΩΩÈ°µÈù¢](https://github.com/jianchang512/clone-voice/releases)Ôºå‰∏ãËΩΩÈ¢ÑÁºñËØëÁâà‰∏ªÊñá‰ª∂(1.7G) Âíå Ê®°Âûã(3G)

2. ‰∏ãËΩΩÂêéËß£ÂéãÂà∞ÊüêÂ§ÑÔºåÊØîÂ¶Ç E:/clone-voice ‰∏ã

3. ÂèåÂáª app.exe ÔºåÁ≠âÂæÖËá™Âä®ÊâìÂºÄwebÁ™óÂè£Ôºå**ËØ∑‰ªîÁªÜÈòÖËØªcmdÁ™óÂè£ÁöÑÊñáÂ≠óÊèêÁ§∫**,Â¶ÇÊúâÈîôËØØÔºåÂùá‰ºöÂú®Ê≠§ÊòæÁ§∫

4. Ê®°Âûã‰∏ãËΩΩÂêéËß£ÂéãÂà∞ËΩØ‰ª∂ÁõÆÂΩï‰∏ãÁöÑ `tts` Êñá‰ª∂Â§πÂÜÖÔºåËß£ÂéãÂêéÊïàÊûúÂ¶ÇÂõæ 

![image](https://github.com/jianchang512/clone-voice/assets/3378335/4b5a60eb-124d-404b-a748-c0a527482e90)

5. ËΩ¨Êç¢Êìç‰ΩúÊ≠•È™§
	
	- ÈÄâÊã©„ÄêÊñáÂ≠ó-&gt;Â£∞Èü≥„ÄëÊåâÈíÆÔºåÂú®ÊñáÊú¨Ê°Ü‰∏≠ËæìÂÖ•ÊñáÂ≠ó„ÄÅÊàñÁÇπÂáªÂØºÂÖ•srtÂ≠óÂπïÊñá‰ª∂ÔºåÁÑ∂ÂêéÁÇπÂáª‚ÄúÁ´ãÂç≥ÂºÄÂßã‚Äù„ÄÇ
	
	- ÈÄâÊã©„ÄêÂ£∞Èü≥-&gt;Â£∞Èü≥„ÄëÊåâÈíÆÔºåÁÇπÂáªÊàñÊãñÊãΩË¶ÅËΩ¨Êç¢ÁöÑÈü≥È¢ëÊñá‰ª∂(mp3/wav/flac)ÔºåÁÑ∂Âêé‰ªé‚ÄúË¶Å‰ΩøÁî®ÁöÑÂ£∞Èü≥Êñá‰ª∂‚Äù‰∏ãÊãâÊ°Ü‰∏≠ÈÄâÊã©Ë¶ÅÂÖãÈöÜÁöÑÈü≥Ëâ≤ÔºåÂ¶ÇÊûúÊ≤°ÊúâÊª°ÊÑèÁöÑÔºå‰πüÂèØ‰ª•ÁÇπÂáª‚ÄúÊú¨Âú∞‰∏ä‰º†‚ÄùÊåâÈíÆÔºåÈÄâÊã©Â∑≤ÂΩïÂà∂Â•ΩÁöÑ5-20sÁöÑwav/mp3/flacÂ£∞Èü≥Êñá‰ª∂„ÄÇÊàñËÄÖÁÇπÂáª‚ÄúÂºÄÂßãÂΩïÂà∂‚ÄùÊåâÈíÆÔºåÂú®Á∫øÂΩïÂà∂‰Ω†Ëá™Â∑±ÁöÑÂ£∞Èü≥5-20sÔºåÂΩïÂà∂ÂÆåÊàêÁÇπÂáª‰ΩøÁî®„ÄÇÁÑ∂ÂêéÁÇπÂáª‚ÄúÁ´ãÂç≥ÂºÄÂßã‚ÄùÊåâÈíÆ
	
6. Â¶ÇÊûúÊú∫Âô®Êã•ÊúâNÂç°GPUÔºåÂπ∂Ê≠£Á°ÆÈÖçÁΩÆ‰∫ÜCUDAÁéØÂ¢ÉÔºåÂ∞ÜËá™Âä®‰ΩøÁî®CUDAÂä†ÈÄü



# Ê∫êÁ†ÅÈÉ®ÁΩ≤(linux mac window)

**Ê∫êÁ†ÅÁâàÈúÄË¶ÅÂú® .env ‰∏≠ HTTP_PROXY=ËÆæÁΩÆ‰ª£ÁêÜ(ÊØîÂ¶Çhttp://127.0.0.1:7890)ÔºåË¶Å‰ªé https://huggingface.co https://github.com ‰∏ãËΩΩÊ®°ÂûãÔºåËÄåËøô‰∏™ÁΩëÂùÄÂõΩÂÜÖÊó†Ê≥ïËÆøÈóÆÔºåÂøÖÈ°ª‰øùËØÅ‰ª£ÁêÜÁ®≥ÂÆöÂèØÈù†ÔºåÂê¶ÂàôÂ§ßÊ®°Âûã‰∏ãËΩΩÂèØËÉΩ‰∏≠ÈÄîÂ§±Ë¥•**

0. Ë¶ÅÊ±Ç python 3.9-&gt;3.11, Âπ∂‰∏îÊèêÂâçÂÆâË£ÖÂ•Ω git-cmd Â∑•ÂÖ∑Ôºå[‰∏ãËΩΩÂú∞ÂùÄ](https://github.com/git-for-windows/git/releases/download/v2.44.0.windows.1/Git-2.44.0-64-bit.exe)
1. ÂàõÂª∫Á©∫ÁõÆÂΩïÔºåÊØîÂ¶Ç E:/clone-voice, Âú®Ëøô‰∏™ÁõÆÂΩï‰∏ãÊâìÂºÄ cmd Á™óÂè£ÔºåÊñπÊ≥ïÊòØÂú∞ÂùÄÊ†è‰∏≠ËæìÂÖ• `cmd`, ÁÑ∂ÂêéÂõûËΩ¶„ÄÇ
‰ΩøÁî®gitÊãâÂèñÊ∫êÁ†ÅÂà∞ÂΩìÂâçÁõÆÂΩï ` git clone git@github.com:jianchang512/clone-voice.git . `
2. ÂàõÂª∫ËôöÊãüÁéØÂ¢É `python -m venv venv`
3. ÊøÄÊ¥ªÁéØÂ¢ÉÔºåwin‰∏ã `E:/clone-voice/venv/scripts/activate`Ôºå
4. ÂÆâË£Ö‰æùËµñ: `pip install -r requirements.txt --no-deps`, 
windows Âíå linux Â¶ÇÊûúË¶ÅÂêØÁî®cudaÂä†ÈÄüÔºåÁªßÁª≠ÊâßË°å `pip uninstall -y torch` Âç∏ËΩΩÔºåÁÑ∂ÂêéÊâßË°å`pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu121`„ÄÇ(ÂøÖÈ°ªÊúâNÂç°Âπ∂‰∏îÈÖçÁΩÆÂ•ΩCUDAÁéØÂ¢É)
5. win‰∏ãËß£Âéã ffmpeg.7zÔºåÂ∞ÜÂÖ∂‰∏≠ÁöÑ`ffmpeg.exe`Âíå`app.py`Âú®Âêå‰∏ÄÁõÆÂΩï‰∏ã, linuxÂíåmac Âà∞ [ffmpegÂÆòÁΩë](https://ffmpeg.org/download.html)‰∏ãËΩΩÂØπÂ∫îÁâàÊú¨ffmpegÔºåËß£ÂéãÂÖ∂‰∏≠ÁöÑ`ffmpeg`Á®ãÂ∫èÂà∞Ê†πÁõÆÂΩï‰∏ãÔºåÂøÖÈ°ªÂ∞ÜÂèØÊâßË°å‰∫åËøõÂà∂Êñá‰ª∂ `ffmpeg` Âíåapp.pyÊîæÂú®Âêå‰∏ÄÁõÆÂΩï‰∏ã„ÄÇ

   ![image](https://github.com/jianchang512/clone-voice/assets/3378335/0c61c8b6-7f7e-475f-8984-47fb87ba58e8)

6. **È¶ñÂÖàËøêË°å**  `python  code_dev.py `ÔºåÂú®ÊèêÁ§∫ÂêåÊÑèÂçèËÆÆÊó∂ÔºåËæìÂÖ• `y`ÔºåÁÑ∂ÂêéÁ≠âÂæÖÊ®°Âûã‰∏ãËΩΩÂÆåÊØï„ÄÇ
   ![](./images/code_dev01.png)
   ![](./images/code_dev02.png)
   
	‰∏ãËΩΩÊ®°ÂûãÈúÄË¶ÅÊåÇÂÖ®Â±Ä‰ª£ÁêÜÔºåÊ®°ÂûãÈùûÂ∏∏Â§ßÔºåÂ¶ÇÊûú‰ª£ÁêÜ‰∏çÂ§üÁ®≥ÂÆöÂèØÈù†ÔºåÂèØËÉΩ‰ºöÈÅáÂà∞ÂæàÂ§öÈîôËØØÔºåÂ§ßÈÉ®ÂàÜÁöÑÈîôËØØÂùáÊòØ‰ª£ÁêÜÈóÆÈ¢òÂØºËá¥„ÄÇ
	
	Â¶ÇÊûúÊòæÁ§∫‰∏ãËΩΩÂ§ö‰∏™Ê®°ÂûãÂùáÊàêÂäü‰∫ÜÔºå‰ΩÜÊúÄÂêéËøòÊòØÊèêÁ§∫‚ÄúDownloading WavLM model‚ÄùÈîôËØØÔºåÂàôÈúÄË¶Å‰øÆÊîπÂ∫ìÂåÖÊñá‰ª∂ `\venv\Lib\site-packages\aiohttp\client.py`, Âú®Â§ßÁ∫¶535Ë°åÈôÑËøëÔºå`if proxy is not None:` ‰∏äÈù¢‰∏ÄË°åÊ∑ªÂä†‰Ω†ÁöÑ‰ª£ÁêÜÂú∞ÂùÄÔºåÊØîÂ¶Ç `proxy=&quot;http://127.0.0.1:10809&quot;`.

7. ‰∏ãËΩΩÂÆåÊØïÂêéÔºåÂÜçÂêØÂä® `python app.py`

8. **„ÄêËÆ≠ÁªÉËØ¥Êòé„Äë** Â¶ÇÊûúÊÉ≥ËÆ≠ÁªÉÔºåÊâßË°å `python train.py`, ËÆ≠ÁªÉÂèÇÊï∞Âú® `param.json`‰∏≠Ë∞ÉÊï¥ÔºåË∞ÉÊï¥ÂêéÈáçÊñ∞ÊâßË°åËÆ≠ÁªÉËÑöÊú¨`python train.py`

8. ÊØèÊ¨°ÂêØÂä®ÈÉΩ‰ºöËøûÊé•Â¢ôÂ§ñÊ£ÄÊµãÊàñÊõ¥Êñ∞Ê®°ÂûãÔºåËØ∑ËÄêÂøÉÁ≠âÂæÖ„ÄÇÂ¶ÇÊûú‰∏çÊÉ≥ÊØèÊ¨°ÂêØÂä®ÈÉΩÊ£ÄÊµãÊàñÊõ¥Êñ∞ÔºåÈúÄÊâãÂä®‰øÆÊîπ‰æùËµñÂåÖ‰∏ãÊñá‰ª∂ÔºåÊâìÂºÄ \venv\Lib\site-packages\TTS\utils\manage.py ,Â§ßÁ∫¶ 389 Ë°åÈôÑËøëÔºådef download_model ÊñπÊ≥ï‰∏≠ÔºåÊ≥®ÈáäÊéâÂ¶Ç‰∏ã‰ª£Á†Å

```
if md5sum is not None:
	md5sum_file = os.path.join(output_path, &quot;hash.md5&quot;)
	if os.path.isfile(md5sum_file):
	    with open(md5sum_file, mode=&quot;r&quot;) as f:
		if not f.read() == md5sum:
		    print(f&quot; &gt; {model_name} has been updated, clearing model cache...&quot;)
		    self.create_dir_and_download_model(model_name, model_item, output_path)
		else:
		    print(f&quot; &gt; {model_name} is already downloaded.&quot;)
	else:
	    print(f&quot; &gt; {model_name} has been updated, clearing model cache...&quot;)
	    self.create_dir_and_download_model(model_name, model_item, output_path)
```

9. Ê∫êÁ†ÅÁâàÂêØÂä®Êó∂ÂèØËÉΩÈ¢ëÁπÅÈÅáÂà∞ÈîôËØØÔºåÂü∫Êú¨ÈÉΩÊòØ‰ª£ÁêÜÈóÆÈ¢òÂØºËá¥Êó†Ê≥ï‰ªéÂ¢ôÂ§ñ‰∏ãËΩΩÊ®°ÂûãÊàñ‰∏ãËΩΩ‰∏≠Êñ≠‰∏çÂÆåÊï¥„ÄÇÂª∫ËÆÆ‰ΩøÁî®Á®≥ÂÆöÁöÑ‰ª£ÁêÜÔºåÂÖ®Â±ÄÂºÄÂêØ„ÄÇÂ¶ÇÊûúÂßãÁªàÊó†Ê≥ïÂÆåÊï¥‰∏ãËΩΩÔºåÂª∫ËÆÆ‰ΩøÁî®È¢ÑÁºñËØëÁâà„ÄÇ




# Â∏∏ËßÅÈóÆÈ¢ò

**Ê®°Âûãxtts‰ªÖÂèØÁî®‰∫éÂ≠¶‰π†Á†îÁ©∂Ôºå‰∏çÂèØÁî®‰∫éÂïÜ‰∏ö**

0. Ê∫êÁ†ÅÁâàÈúÄË¶ÅÂú® .env ‰∏≠ HTTP_PROXY=ËÆæÁΩÆ‰ª£ÁêÜ(ÊØîÂ¶Çhttp://127.0.0.1:7890)ÔºåË¶Å‰ªé https://huggingface.co https://github.com ‰∏ãËΩΩÊ®°ÂûãÔºåËÄåËøô‰∏™ÁΩëÂùÄÂõΩÂÜÖÊó†Ê≥ïËÆøÈóÆÔºåÂøÖÈ°ª‰øùËØÅ‰ª£ÁêÜÁ®≥ÂÆöÂèØÈù†ÔºåÂê¶ÂàôÂ§ßÊ®°Âûã‰∏ãËΩΩÂèØËÉΩ‰∏≠ÈÄîÂ§±Ë¥•

1. ÂêØÂä®ÂêéÈúÄË¶ÅÂÜ∑Âä†ËΩΩÊ®°ÂûãÔºå‰ºöÊ∂àËÄó‰∏Ä‰∫õÊó∂Èó¥ÔºåËØ∑ËÄêÂøÉÁ≠âÂæÖÊòæÁ§∫Âá∫`http://127.0.0.1:9988`Ôºå Âπ∂Ëá™Âä®ÊâìÂºÄÊµèËßàÂô®È°µÈù¢ÂêéÔºåÁ®çÁ≠â‰∏§‰∏âÂàÜÈíüÂêéÂÜçËøõË°åËΩ¨Êç¢

2. ÂäüËÉΩÊúâÔºö

		ÊñáÂ≠óÂà∞ËØ≠Èü≥:Âç≥ËæìÂÖ•ÊñáÂ≠óÔºåÁî®ÈÄâÂÆöÁöÑÈü≥Ëâ≤ÁîüÊàêÂ£∞Èü≥„ÄÇ
		
		Â£∞Èü≥Âà∞Â£∞Èü≥ÔºöÂç≥‰ªéÊú¨Âú∞ÈÄâÊã©‰∏Ä‰∏™Èü≥È¢ëÊñá‰ª∂ÔºåÁî®ÈÄâÂÆöÁöÑÈü≥Ëâ≤ÁîüÊàêÂè¶‰∏Ä‰∏™Èü≥È¢ëÊñá‰ª∂.
		
3. Â¶ÇÊûúÊâìÂºÄÁöÑcmdÁ™óÂè£Âæà‰πÖ‰∏çÂä®ÔºåÈúÄË¶ÅÂú®‰∏äÈù¢Êåâ‰∏ãÂõûËΩ¶ÊâçÁªßÁª≠ËæìÂá∫ÔºåËØ∑Âú®cmdÂ∑¶‰∏äËßíÂõæÊ†á‰∏äÂçïÂáªÔºåÈÄâÊã©‚ÄúÂ±ûÊÄß‚ÄùÔºåÁÑ∂ÂêéÂèñÊ∂à‚ÄúÂø´ÈÄüÁºñËæë‚ÄùÂíå‚ÄúÊèíÂÖ•Ê®°Âºè‚ÄùÁöÑÂ§çÈÄâÊ°Ü

![](./images/3.png)
![](./images/4.png)


4. È¢ÑÁºñËØëÁâà Â£∞Èü≥-Â£∞Èü≥Á∫øÁ®ãÂêØÂä®Â§±Ë¥•

   È¶ñÂÖàÁ°ÆËÆ§Ê®°ÂûãÂ∑≤Ê≠£Á°Æ‰∏ãËΩΩÊîæÁΩÆ„ÄÇttsÊñá‰ª∂Â§πÂÜÖÊúâ3‰∏™Êñá‰ª∂Â§πÔºåÂ¶Ç‰∏ãÂõæ
   ![image](https://github.com/jianchang512/clone-voice/assets/3378335/4b5a60eb-124d-404b-a748-c0a527482e90)

   Â¶ÇÊûúÂ∑≤Ê≠£Á°ÆÊîæÁΩÆ‰∫ÜÔºå‰ΩÜ‰ªçÈîôËØØÔºå[ÁÇπÂáª‰∏ãËΩΩ extra-to-tts_cache.zip](https://github.com/jianchang512/clone-voice/releases/download/v0.0.1/extra-to-tts_cache.zip) ÔºåÂ∞ÜËß£ÂéãÂêéÂæóÂà∞ÁöÑ2‰∏™Êñá‰ª∂ÔºåÂ§çÂà∂Âà∞ËΩØ‰ª∂Ê†πÁõÆÂΩïÁöÑ tts_cache Êñá‰ª∂Â§πÂÜÖ

   Â¶ÇÊûú‰∏äËø∞ÊñπÊ≥ïÊó†ÊïàÔºåÂú® .env Êñá‰ª∂‰∏≠ HTTP_PROXYÂêéÂ°´ÂÜô‰ª£ÁêÜÂú∞ÂùÄÊØîÂ¶Ç `HTTP_PROXY=http://127.0.0.1:7890`ÔºåÂèØËß£ÂÜ≥ËØ•ÈóÆÈ¢òÔºåÂøÖÈ°ªÁ°Æ‰øù‰ª£ÁêÜÁ®≥ÂÆöÔºåÂ°´ÂÜôÁ´ØÂè£Ê≠£Á°Æ

5. ÊèêÁ§∫ ‚ÄúThe text length exceeds the character limit of 182/82 for language‚Äù

   ËøôÊòØÂõ†‰∏∫Áî±Âè•Âè∑ÂàÜÈöîÁöÑÂè•Â≠êÂ§™ÈïøÂØºËá¥ÁöÑÔºåÂª∫ËÆÆÂ∞ÜÂ§™ÈïøÁöÑËØ≠Âè•‰ΩøÁî®Âè•Âè∑ÈöîÂºÄÔºåËÄå‰∏çÊòØÂ§ßÈáè‰ΩøÁî®ÈÄóÂè∑ÔºåÊàñËÄÖ‰Ω†‰πüÂèØ‰ª•ÊâìÂºÄ clone/character.jsonÊñá‰ª∂ÔºåÊâãÂä®‰øÆÊîπÈôêÂà∂
   
6. ÊèêÁ§∫&quot;symbol not found __svml_cosf8_ha&quot;

ÊâìÂºÄÁΩëÈ°µ https://www.dll-files.com/svml_dispmd.dll.html ,ÁÇπÂáªÁ∫¢Ëâ≤&quot;Download&quot;‰∏ãËΩΩÂ≠óÊ†∑Ôºå‰∏ãËΩΩÂêéËß£ÂéãÔºåÂ∞ÜÈáåÈù¢ÁöÑdllÊñá‰ª∂Â§çÂà∂Á≤òË¥¥Âà∞&quot;C:\Windows\System32&quot;
   



# CUDA Âä†ÈÄüÊîØÊåÅ

**ÂÆâË£ÖCUDAÂ∑•ÂÖ∑** [ËØ¶ÁªÜÂÆâË£ÖÊñπÊ≥ï](https://juejin.cn/post/7318704408727519270)

Â¶ÇÊûú‰Ω†ÁöÑÁîµËÑëÊã•Êúâ Nvidia ÊòæÂç°ÔºåÂÖàÂçáÁ∫ßÊòæÂç°È©±Âä®Âà∞ÊúÄÊñ∞ÔºåÁÑ∂ÂêéÂéªÂÆâË£ÖÂØπÂ∫îÁöÑ 
   [CUDA Toolkit 11.8](https://developer.nvidia.com/cuda-downloads)  Âíå  [cudnn for CUDA11.X](https://developer.nvidia.com/rdp/cudnn-archive)„ÄÇ
   
   ÂÆâË£ÖÂÆåÊàêÊàêÔºåÊåâ`Win + R`,ËæìÂÖ• `cmd`ÁÑ∂ÂêéÂõûËΩ¶ÔºåÂú®ÂºπÂá∫ÁöÑÁ™óÂè£‰∏≠ËæìÂÖ•`nvcc --version`,Á°ÆËÆ§ÊúâÁâàÊú¨‰ø°ÊÅØÊòæÁ§∫ÔºåÁ±ª‰ººËØ•Âõæ
   ![image](https://github.com/jianchang512/pyvideotrans/assets/3378335/e68de07f-4bb1-4fc9-bccd-8f841825915a)

   ÁÑ∂ÂêéÁªßÁª≠ËæìÂÖ•`nvidia-smi`,Á°ÆËÆ§ÊúâËæìÂá∫‰ø°ÊÅØÔºåÂπ∂‰∏îËÉΩÁúãÂà∞cudaÁâàÊú¨Âè∑ÔºåÁ±ª‰ººËØ•Âõæ
   ![image](https://github.com/jianchang512/pyvideotrans/assets/3378335/71f1d7d3-07f9-4579-b310-39284734006b)

   ËØ¥ÊòéÂÆâË£ÖÊ≠£Á°ÆÔºåÂèØ‰ª•cudaÂä†ÈÄü‰∫ÜÔºåÂê¶ÂàôÈúÄÈáçÊñ∞ÂÆâË£Ö



# Áõ∏ÂÖ≥ËÅîÈ°πÁõÆ

[ËßÜÈ¢ëÁøªËØëÈÖçÈü≥Â∑•ÂÖ∑:ÁøªËØëÂ≠óÂπïÂπ∂ÈÖçÈü≥](https://github.com/jianchang512/pyvideotrans)

[ËØ≠Èü≥ËØÜÂà´Â∑•ÂÖ∑:Êú¨Âú∞Á¶ªÁ∫øÁöÑËØ≠Èü≥ËØÜÂà´ËΩ¨ÊñáÂ≠óÂ∑•ÂÖ∑](https://github.com/jianchang512/stt)

[‰∫∫Â£∞ËÉåÊôØ‰πêÂàÜÁ¶ª:ÊûÅÁÆÄÁöÑ‰∫∫Â£∞ÂíåËÉåÊôØÈü≥‰πêÂàÜÁ¶ªÂ∑•ÂÖ∑ÔºåÊú¨Âú∞ÂåñÁΩëÈ°µÊìç‰Ωú](https://github.com/jianchang512/vocal-separate)


# [YoutubeÊºîÁ§∫ËßÜÈ¢ë](https://youtu.be/CC227GXOJLk)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ShihuaHuang95/DEIM]]></title>
            <link>https://github.com/ShihuaHuang95/DEIM</link>
            <guid>https://github.com/ShihuaHuang95/DEIM</guid>
            <pubDate>Fri, 21 Mar 2025 00:04:17 GMT</pubDate>
            <description><![CDATA[[CVPR 2025] DEIM: DETR with Improved Matching for Fast Convergence]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ShihuaHuang95/DEIM">ShihuaHuang95/DEIM</a></h1>
            <p>[CVPR 2025] DEIM: DETR with Improved Matching for Fast Convergence</p>
            <p>Language: Python</p>
            <p>Stars: 500</p>
            <p>Forks: 83</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre>&lt;h2 align=&quot;center&quot;&gt;
  DEIM: DETR with Improved Matching for Fast Convergence
&lt;/h2&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://github.com/ShihuaHuang95/DEIM/blob/master/LICENSE&quot;&gt;
        &lt;img alt=&quot;license&quot; src=&quot;https://img.shields.io/badge/LICENSE-Apache%202.0-blue&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2412.04234&quot;&gt;
        &lt;img alt=&quot;arXiv&quot; src=&quot;https://img.shields.io/badge/arXiv-2412.04234-red&quot;&gt;
    &lt;/a&gt;
   &lt;a href=&quot;https://www.shihuahuang.cn/DEIM/&quot;&gt;
        &lt;img alt=&quot;project webpage&quot; src=&quot;https://img.shields.io/badge/Webpage-DEIM-purple&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/ShihuaHuang95/DEIM/pulls&quot;&gt;
        &lt;img alt=&quot;prs&quot; src=&quot;https://img.shields.io/github/issues-pr/ShihuaHuang95/DEIM&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/ShihuaHuang95/DEIM/issues&quot;&gt;
        &lt;img alt=&quot;issues&quot; src=&quot;https://img.shields.io/github/issues/ShihuaHuang95/DEIM?color=olive&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/ShihuaHuang95/DEIM&quot;&gt;
        &lt;img alt=&quot;stars&quot; src=&quot;https://img.shields.io/github/stars/ShihuaHuang95/DEIM&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;mailto:shihuahuang95@gmail.com&quot;&gt;
        &lt;img alt=&quot;Contact Us&quot; src=&quot;https://img.shields.io/badge/Contact-Email-yellow&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    DEIM is an advanced training framework designed to enhance the matching mechanism in DETRs, enabling faster convergence and improved accuracy. It serves as a robust foundation for future research and applications in the field of real-time object detection. 
&lt;/p&gt;

---


&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;http://www.shihuahuang.cn&quot;&gt;Shihua Huang&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;,
  &lt;a href=&quot;https://scholar.google.com/citations?user=tIFWBcQAAAAJ&amp;hl=en&quot;&gt;Zhichao Lu&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;,
  &lt;a href=&quot;https://vinthony.github.io/academic/&quot;&gt;Xiaodong Cun&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;,
  Yongjun Yu&lt;sup&gt;1&lt;/sup&gt;,
  Xiao Zhou&lt;sup&gt;4&lt;/sup&gt;, 
  &lt;a href=&quot;https://xishen0220.github.io&quot;&gt;Xi Shen&lt;/a&gt;&lt;sup&gt;1*&lt;/sup&gt;
&lt;/div&gt;

  
&lt;p align=&quot;center&quot;&gt;
&lt;i&gt;
1. Intellindust AI Lab &amp;nbsp; 2. City University of Hong Kong &amp;nbsp; 3. Great Bay University &amp;nbsp; 4. Hefei Normal University
&lt;/i&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  **üìß Corresponding author:** &lt;a href=&quot;mailto:shenxiluc@gmail.com&quot;&gt;shenxiluc@gmail.com&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://paperswithcode.com/sota/real-time-object-detection-on-coco?p=deim-detr-with-improved-matching-for-fast&quot;&gt;
    &lt;img alt=&quot;sota&quot; src=&quot;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/deim-detr-with-improved-matching-for-fast/real-time-object-detection-on-coco&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;strong&gt;If you like our work, please give us a ‚≠ê!&lt;/strong&gt;
&lt;/p&gt;


&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./figures/teaser_a.png&quot; alt=&quot;Image 1&quot; width=&quot;49%&quot;&gt;
  &lt;img src=&quot;./figures/teaser_b.png&quot; alt=&quot;Image 2&quot; width=&quot;49%&quot;&gt;
&lt;/p&gt;

&lt;/details&gt;

 
  
## üöÄ Updates
- [x] **\[2025.03.12\]** The Object365 Pretrained [DEIM-D-FINE-X](https://drive.google.com/file/d/1RMNrHh3bYN0FfT5ZlWhXtQxkG23xb2xj/view?usp=drive_link) model is released, which achieves 59.5% AP after fine-tuning 24 COCO epochs.
- [x] **\[2025.03.05\]** The Nano DEIM model is released.
- [x] **\[2025.02.27\]** The DEIM paper is accepted to CVPR 2025. Thanks to all co-authors.
- [x] **\[2024.12.26\]** A more efficient implementation of Dense O2O, achieving nearly a 30% improvement in loading speed (See [the pull request](https://github.com/ShihuaHuang95/DEIM/pull/13) for more details). Huge thanks to my colleague [Longfei Liu](https://github.com/capsule2077).
- [x] **\[2024.12.03\]** Release DEIM series. Besides, this repo also supports the re-implmentations of [D-FINE](https://arxiv.org/abs/2410.13842) and [RT-DETR](https://arxiv.org/abs/2407.17140).

## Table of Content
* [1. Model Zoo](https://github.com/ShihuaHuang95/DEIM?tab=readme-ov-file#1-model-zoo)
* [2. Quick start](https://github.com/ShihuaHuang95/DEIM?tab=readme-ov-file#2-quick-start)
* [3. Usage](https://github.com/ShihuaHuang95/DEIM?tab=readme-ov-file#3-usage)
* [4. Tools](https://github.com/ShihuaHuang95/DEIM?tab=readme-ov-file#4-tools)
* [5. Citation](https://github.com/ShihuaHuang95/DEIM?tab=readme-ov-file#5-citation)
* [6. Acknowledgement](https://github.com/ShihuaHuang95/DEIM?tab=readme-ov-file#6-acknowledgement)
  
  
## 1. Model Zoo

### DEIM-D-FINE
| Model | Dataset | AP&lt;sup&gt;D-FINE&lt;/sup&gt; | AP&lt;sup&gt;DEIM&lt;/sup&gt; | #Params | Latency | GFLOPs | config | checkpoint
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: 
**N** | COCO | **42.8** | **43.0** | 4M | 2.12ms | 7 | [yml](./configs/deim_dfine/deim_hgnetv2_n_coco.yml) | [ckpt](https://drive.google.com/file/d/1ZPEhiU9nhW4M5jLnYOFwTSLQC1Ugf62e/view?usp=sharing) |
**S** | COCO | **48.7** | **49.0** | 10M | 3.49ms | 25 | [yml](./configs/deim_dfine/deim_hgnetv2_s_coco.yml) | [ckpt](https://drive.google.com/file/d/1tB8gVJNrfb6dhFvoHJECKOF5VpkthhfC/view?usp=drive_link) |
**M** | COCO | **52.3** | **52.7** | 19M | 5.62ms | 57 | [yml](./configs/deim_dfine/deim_hgnetv2_m_coco.yml) | [ckpt](https://drive.google.com/file/d/18Lj2a6UN6k_n_UzqnJyiaiLGpDzQQit8/view?usp=drive_link) |
**L** | COCO | **54.0** | **54.7** | 31M | 8.07ms | 91 | [yml](./configs/deim_dfine/deim_hgnetv2_l_coco.yml) | [ckpt](https://drive.google.com/file/d/1PIRf02XkrA2xAD3wEiKE2FaamZgSGTAr/view?usp=drive_link) | 
**X** | COCO | **55.8** | **56.5** | 62M | 12.89ms | 202 | [yml](./configs/deim_dfine/deim_hgnetv2_x_coco.yml) | [ckpt](https://drive.google.com/file/d/1dPtbgtGgq1Oa7k_LgH1GXPelg1IVeu0j/view?usp=drive_link) | 


### DEIM-RT-DETRv2
| Model | Dataset | AP&lt;sup&gt;RT-DETRv2&lt;/sup&gt; | AP&lt;sup&gt;DEIM&lt;/sup&gt; | #Params | Latency | GFLOPs | config | checkpoint
| :---: | :---: | :---: | :---: |  :---: | :---: | :---: | :---: | :---: 
**S** | COCO | **47.9** | **49.0** | 20M | 4.59ms | 60 | [yml](./configs/deim_rtdetrv2/deim_r18vd_120e_coco.yml) | [ckpt](https://drive.google.com/file/d/153_JKff6EpFgiLKaqkJsoDcLal_0ux_F/view?usp=drive_link) | 
**M** | COCO | **49.9** | **50.9** | 31M | 6.40ms | 92 | [yml](./configs/deim_rtdetrv2/deim_r34vd_120e_coco.yml) | [ckpt](https://drive.google.com/file/d/1O9RjZF6kdFWGv1Etn1Toml4r-YfdMDMM/view?usp=drive_link) | 
**M*** | COCO | **51.9** | **53.2** | 33M | 6.90ms | 100 | [yml](./configs/deim_rtdetrv2/deim_r50vd_m_60e_coco.yml) | [ckpt](https://drive.google.com/file/d/10dLuqdBZ6H5ip9BbBiE6S7ZcmHkRbD0E/view?usp=drive_link) | 
**L** | COCO | **53.4** | **54.3** | 42M | 9.15ms | 136 | [yml](./configs/deim_rtdetrv2/deim_r50vd_60e_coco.yml) | [ckpt](https://drive.google.com/file/d/1mWknAXD5JYknUQ94WCEvPfXz13jcNOTI/view?usp=drive_link) | 
**X** | COCO | **54.3** | **55.5** | 76M | 13.66ms | 259 | [yml](./configs/deim_rtdetrv2/deim_r101vd_60e_coco.yml) | [ckpt](https://drive.google.com/file/d/1BIevZijOcBO17llTyDX32F_pYppBfnzu/view?usp=drive_link) | 


## 2. Quick start

### Setup

```shell
conda create -n deim python=3.11.9
conda activate deim
pip install -r requirements.txt
```


### Data Preparation

&lt;details&gt;
&lt;summary&gt; COCO2017 Dataset &lt;/summary&gt;

1. Download COCO2017 from [OpenDataLab](https://opendatalab.com/OpenDataLab/COCO_2017) or [COCO](https://cocodataset.org/#download).
1. Modify paths in [coco_detection.yml](./configs/dataset/coco_detection.yml)

    ```yaml
    train_dataloader:
        img_folder: /data/COCO2017/train2017/
        ann_file: /data/COCO2017/annotations/instances_train2017.json
    val_dataloader:
        img_folder: /data/COCO2017/val2017/
        ann_file: /data/COCO2017/annotations/instances_val2017.json
    ```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Custom Dataset&lt;/summary&gt;

To train on your custom dataset, you need to organize it in the COCO format. Follow the steps below to prepare your dataset:

1. **Set `remap_mscoco_category` to `False`:**

    This prevents the automatic remapping of category IDs to match the MSCOCO categories.

    ```yaml
    remap_mscoco_category: False
    ```

2. **Organize Images:**

    Structure your dataset directories as follows:

    ```shell
    dataset/
    ‚îú‚îÄ‚îÄ images/
    ‚îÇ   ‚îú‚îÄ‚îÄ train/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image1.jpg
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image2.jpg
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
    ‚îÇ   ‚îú‚îÄ‚îÄ val/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image1.jpg
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image2.jpg
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
    ‚îî‚îÄ‚îÄ annotations/
        ‚îú‚îÄ‚îÄ instances_train.json
        ‚îú‚îÄ‚îÄ instances_val.json
        ‚îî‚îÄ‚îÄ ...
    ```

    - **`images/train/`**: Contains all training images.
    - **`images/val/`**: Contains all validation images.
    - **`annotations/`**: Contains COCO-formatted annotation files.

3. **Convert Annotations to COCO Format:**

    If your annotations are not already in COCO format, you&#039;ll need to convert them. You can use the following Python script as a reference or utilize existing tools:

    ```python
    import json

    def convert_to_coco(input_annotations, output_annotations):
        # Implement conversion logic here
        pass

    if __name__ == &quot;__main__&quot;:
        convert_to_coco(&#039;path/to/your_annotations.json&#039;, &#039;dataset/annotations/instances_train.json&#039;)
    ```

4. **Update Configuration Files:**

    Modify your [custom_detection.yml](./configs/dataset/custom_detection.yml).

    ```yaml
    task: detection

    evaluator:
      type: CocoEvaluator
      iou_types: [&#039;bbox&#039;, ]

    num_classes: 777 # your dataset classes
    remap_mscoco_category: False

    train_dataloader:
      type: DataLoader
      dataset:
        type: CocoDetection
        img_folder: /data/yourdataset/train
        ann_file: /data/yourdataset/train/train.json
        return_masks: False
        transforms:
          type: Compose
          ops: ~
      shuffle: True
      num_workers: 4
      drop_last: True
      collate_fn:
        type: BatchImageCollateFunction

    val_dataloader:
      type: DataLoader
      dataset:
        type: CocoDetection
        img_folder: /data/yourdataset/val
        ann_file: /data/yourdataset/val/ann.json
        return_masks: False
        transforms:
          type: Compose
          ops: ~
      shuffle: False
      num_workers: 4
      drop_last: False
      collate_fn:
        type: BatchImageCollateFunction
    ```

&lt;/details&gt;


## 3. Usage
&lt;details open&gt;
&lt;summary&gt; COCO2017 &lt;/summary&gt;

1. Training
```shell
CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/deim_dfine/deim_hgnetv2_${model}_coco.yml --use-amp --seed=0
```

&lt;!-- &lt;summary&gt;2. Testing &lt;/summary&gt; --&gt;
2. Testing
```shell
CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/deim_dfine/deim_hgnetv2_${model}_coco.yml --test-only -r model.pth
```

&lt;!-- &lt;summary&gt;3. Tuning &lt;/summary&gt; --&gt;
3. Tuning
```shell
CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --master_port=7777 --nproc_per_node=4 train.py -c configs/deim_dfine/deim_hgnetv2_${model}_coco.yml --use-amp --seed=0 -t model.pth
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt; Customizing Batch Size &lt;/summary&gt;

For example, if you want to double the total batch size when training D-FINE-L on COCO2017, here are the steps you should follow:

1. **Modify your [dataloader.yml](./configs/base/dataloader.yml)** to increase the `total_batch_size`:

    ```yaml
    train_dataloader:
        total_batch_size: 64  # Previously it was 32, now doubled
    ```

2. **Modify your [deim_hgnetv2_l_coco.yml](./configs/deim_dfine/deim_hgnetv2_l_coco.yml)**. Here‚Äôs how the key parameters should be adjusted:

    ```yaml
    optimizer:
    type: AdamW
    params:
        -
        params: &#039;^(?=.*backbone)(?!.*norm|bn).*$&#039;
        lr: 0.000025  # doubled, linear scaling law
        -
        params: &#039;^(?=.*(?:encoder|decoder))(?=.*(?:norm|bn)).*$&#039;
        weight_decay: 0.

    lr: 0.0005  # doubled, linear scaling law
    betas: [0.9, 0.999]
    weight_decay: 0.0001  # need a grid search

    ema:  # added EMA settings
        decay: 0.9998  # adjusted by 1 - (1 - decay) * 2
        warmups: 500  # halved

    lr_warmup_scheduler:
        warmup_duration: 250  # halved
    ```

&lt;/details&gt;


&lt;details&gt;
&lt;summary&gt; Customizing Input Size &lt;/summary&gt;

If you&#039;d like to train **DEIM** on COCO2017 with an input size of 320x320, follow these steps:

1. **Modify your [dataloader.yml](./configs/base/dataloader.yml)**:

    ```yaml

    train_dataloader:
    dataset:
        transforms:
            ops:
                - {type: Resize, size: [320, 320], }
    collate_fn:
        base_size: 320
    dataset:
        transforms:
            ops:
                - {type: Resize, size: [320, 320], }
    ```

2. **Modify your [dfine_hgnetv2.yml](./configs/base/dfine_hgnetv2.yml)**:

    ```yaml
    eval_spatial_size: [320, 320]
    ```

&lt;/details&gt;

## 4. Tools
&lt;details&gt;
&lt;summary&gt; Deployment &lt;/summary&gt;

&lt;!-- &lt;summary&gt;4. Export onnx &lt;/summary&gt; --&gt;
1. Setup
```shell
pip install onnx onnxsim
```

2. Export onnx
```shell
python tools/deployment/export_onnx.py --check -c configs/deim_dfine/deim_hgnetv2_${model}_coco.yml -r model.pth
```

3. Export [tensorrt](https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html)
```shell
trtexec --onnx=&quot;model.onnx&quot; --saveEngine=&quot;model.engine&quot; --fp16
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt; Inference (Visualization) &lt;/summary&gt;


1. Setup
```shell
pip install -r tools/inference/requirements.txt
```


&lt;!-- &lt;summary&gt;5. Inference &lt;/summary&gt; --&gt;
2. Inference (onnxruntime / tensorrt / torch)

Inference on images and videos is now supported.
```shell
python tools/inference/onnx_inf.py --onnx model.onnx --input image.jpg  # video.mp4
python tools/inference/trt_inf.py --trt model.engine --input image.jpg
python tools/inference/torch_inf.py -c configs/deim_dfine/deim_hgnetv2_${model}_coco.yml -r model.pth --input image.jpg --device cuda:0
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt; Benchmark &lt;/summary&gt;

1. Setup
```shell
pip install -r tools/benchmark/requirements.txt
```

&lt;!-- &lt;summary&gt;6. Benchmark &lt;/summary&gt; --&gt;
2. Model FLOPs, MACs, and Params
```shell
python tools/benchmark/get_info.py -c configs/deim_dfine/deim_hgnetv2_${model}_coco.yml
```

2. TensorRT Latency
```shell
python tools/benchmark/trt_benchmark.py --COCO_dir path/to/COCO2017 --engine_dir model.engine
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt; Fiftyone Visualization  &lt;/summary&gt;

1. Setup
```shell
pip install fiftyone
```
4. Voxel51 Fiftyone Visualization ([fiftyone](https://github.com/voxel51/fiftyone))
```shell
python tools/visualization/fiftyone_vis.py -c configs/deim_dfine/deim_hgnetv2_${model}_coco.yml -r model.pth
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt; Others &lt;/summary&gt;

1. Auto Resume Training
```shell
bash reference/safe_training.sh
```

2. Converting Model Weights
```shell
python reference/convert_weight.py model.pth
```
&lt;/details&gt;


## 5. Citation
If you use `DEIM` or its methods in your work, please cite the following BibTeX entries:
&lt;details open&gt;
&lt;summary&gt; bibtex &lt;/summary&gt;

```latex
@misc{huang2024deim,
      title={DEIM: DETR with Improved Matching for Fast Convergence},
      author={Shihua Huang, Zhichao Lu, Xiaodong Cun, Yongjun Yu, Xiao Zhou, and Xi Shen},
      booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
      year={2025},
}
```
&lt;/details&gt;

## 6. Acknowledgement
Our work is built upon [D-FINE](https://github.com/Peterande/D-FINE) and [RT-DETR](https://github.com/lyuwenyu/RT-DETR).

‚ú® Feel free to contribute and reach out if you have any questions! ‚ú®
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>