<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Fri, 13 Jun 2025 00:04:35 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[Shubhamsaboo/awesome-llm-apps]]></title>
            <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
            <guid>https://github.com/Shubhamsaboo/awesome-llm-apps</guid>
            <pubDate>Fri, 13 Jun 2025 00:04:35 GMT</pubDate>
            <description><![CDATA[Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Shubhamsaboo/awesome-llm-apps">Shubhamsaboo/awesome-llm-apps</a></h1>
            <p>Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p>
            <p>Language: Python</p>
            <p>Stars: 36,880</p>
            <p>Forks: 4,280</p>
            <p>Stars today: 1,287 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;http://www.theunwindai.com&quot;&gt;
    &lt;img src=&quot;docs/banner/unwind_black.png&quot; width=&quot;900px&quot; alt=&quot;Unwind AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.linkedin.com/in/shubhamsaboo/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;style=flat-square&quot; alt=&quot;LinkedIn&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/Saboo_Shubham_&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/Shubham_Saboo&quot; alt=&quot;Twitter&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;hr/&gt;

# üåü Awesome LLM Apps

A curated collection of **Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.** This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/9876&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/9876&quot; alt=&quot;Shubhamsaboo%2Fawesome-llm-apps | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## ü§î Why Awesome LLM Apps?

- üí° Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.
- üî• Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp; RAG.
- üéì Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.

## üìÇ Featured AI Projects

### AI Agents

### üå± Starter AI Agents

*   [üéôÔ∏è AI Blog to Podcast Agent](starter_ai_agents/ai_blog_to_podcast_agent/)
*   [‚ù§Ô∏è‚Äçü©π AI Breakup Recovery Agent](starter_ai_agents/ai_breakup_recovery_agent/)
*   [üìä AI Data Analysis Agent](starter_ai_agents/ai_data_analysis_agent/)
*   [ü©ª AI Medical Imaging Agent](starter_ai_agents/ai_medical_imaging_agent/)
*   [üòÇ AI Meme Generator Agent (Browser)](starter_ai_agents/ai_meme_generator_agent_browseruse/)
*   [üéµ AI Music Generator Agent](starter_ai_agents/ai_music_generator_agent/)
*   [üõ´ AI Travel Agent (Local &amp; Cloud)](starter_ai_agents/ai_travel_agent/)
*   [‚ú® Gemini Multimodal Agent](starter_ai_agents/gemini_multimodal_agent_demo/)
*   [üåê Local News Agent (OpenAI Swarm)](starter_ai_agents/local_news_agent_openai_swarm/)
*   [üîÑ Mixture of Agents](starter_ai_agents/mixture_of_agents/)
*   [üìä xAI Finance Agent](starter_ai_agents/xai_finance_agent/)
*   [üîç OpenAI Research Agent](starter_ai_agents/opeani_research_agent/)
*   [üï∏Ô∏è Web Scrapping AI Agent (Local &amp; Cloud)](starter_ai_agents/web_scrapping_ai_agent/)

### üöÄ Advanced AI Agents

*   [üîç AI Deep Research Agent](advanced_ai_agents/single_agent_apps/ai_deep_research_agent/)
*   [üèóÔ∏è AI System Architect Agent](advanced_ai_agents/single_agent_apps/ai_system_architect_r1/)
*   [üéØ AI Lead Generation Agent](advanced_ai_agents/single_agent_apps/ai_lead_generation_agent/)
*   [üí∞ AI Financial Coach Agent](advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/)
*   [üé¨ AI Movie Production Agent](advanced_ai_agents/single_agent_apps/ai_movie_production_agent/)
*   [üìà AI Investment Agent](advanced_ai_agents/single_agent_apps/ai_investment_agent/)
*   [üèãÔ∏è‚Äç‚ôÇÔ∏è AI Health &amp; Fitness Agent](advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/)
*   [üöÄ AI Product Launch Intelligence Agent](advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent)
*   [üóûÔ∏è AI Journalist Agent](advanced_ai_agents/single_agent_apps/ai_journalist_agent/)
*   [üß† AI Mental Wellbeing Agent](advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/)
*   [üìë AI Meeting Agent](advanced_ai_agents/single_agent_apps/ai_meeting_agent/)
*   [üß¨ AI Self-Evolving Agent](advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/)

### üéÆ Autonomous Game Playing Agents

*   [üéÆ AI 3D Pygame Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/)
*   [‚ôú AI Chess Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/)
*   [üé≤ AI Tic-Tac-Toe Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/)

### ü§ù Multi-agent Teams

*   [üß≤ AI Competitor Intelligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/)
*   [üí≤ AI Finance Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/)
*   [üé® AI Game Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/)
*   [üë®‚Äç‚öñÔ∏è AI Legal Agent Team (Cloud &amp; Local)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/)
*   [üíº AI Recruitment Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/)
*   [üë®‚Äçüíº AI Services Agency (CrewAI)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/)
*   [üë®‚Äçüè´ AI Teaching Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/)
*   [üíª Multimodal Coding Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/)
*   [‚ú® Multimodal Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/)

### üó£Ô∏è Voice AI Agents

*   [üó£Ô∏è AI Audio Tour Agent](voice_ai_agents/ai_audio_tour_agent/)
*   [üìû Customer Support Voice Agent](voice_ai_agents/customer_support_voice_agent/)
*   [üîä Voice RAG Agent (OpenAI SDK)](voice_ai_agents/voice_rag_openaisdk/)


### üåê MCP AI Agents

*   [‚ôæÔ∏è Browser MCP Agent](mcp_ai_agents/browser_mcp_agent/)
*   [üêô GitHub MCP Agent](mcp_ai_agents/github_mcp_agent/)
*   [üìë Notion MCP Agent](mcp_ai_agents/notion_mcp_agent) 
*   [üåç AI Travel Planner MCP Agent](mcp_ai_agents/ai_travel_planner_mcp_agent_team)

### üìÄ RAG (Retrieval Augmented Generation)
*   [üîó Agentic RAG](rag_tutorials/agentic_rag/)
*   [üßê Agentic RAG with Reasoning](rag_tutorials/agentic_rag_with_reasoning/)
*   [üì∞ AI Blog Search (RAG)](rag_tutorials/ai_blog_search/)
*   [üîç Autonomous RAG](rag_tutorials/autonomous_rag/)
*   [üîÑ Corrective RAG (CRAG)](rag_tutorials/corrective_rag/)
*   [üêã Deepseek Local RAG Agent](rag_tutorials/deepseek_local_rag_agent/)
*   [ü§î Gemini Agentic RAG](rag_tutorials/gemini_agentic_rag/)
*   [üëÄ Hybrid Search RAG (Cloud)](rag_tutorials/hybrid_search_rag/)
*   [üîÑ Llama 3.1 Local RAG](rag_tutorials/llama3.1_local_rag/)
*   [üñ•Ô∏è Local Hybrid Search RAG](rag_tutorials/local_hybrid_search_rag/)
*   [ü¶ô Local RAG Agent](rag_tutorials/local_rag_agent/)
*   [üß© RAG-as-a-Service](rag_tutorials/rag-as-a-service/)
*   [‚ú® RAG Agent with Cohere](rag_tutorials/rag_agent_cohere/)
*   [‚õìÔ∏è Basic RAG Chain](rag_tutorials/rag_chain/)
*   [üì† RAG with Database Routing](rag_tutorials/rag_database_routing/)
*   [üñºÔ∏è Vision RAG](rag_tutorials/vision_rag/)

### üíæ LLM Apps with Memory Tutorials

*   [üíæ AI ArXiv Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/)
*   [üõ©Ô∏è AI Travel Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/)
*   [üí¨ Llama3 Stateful Chat](advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/)
*   [üìù LLM App with Personalized Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/)
*   [üóÑÔ∏è Local ChatGPT Clone with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/)
*   [üß† Multi-LLM Application with Shared Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/)


### üí¨ Chat with X Tutorials

*   [üí¨ Chat with GitHub (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_github/)
*   [üì® Chat with Gmail](advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/)
*   [üìÑ Chat with PDF (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/)
*   [üìö Chat with Research Papers (ArXiv) (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/)
*   [üìù Chat with Substack](advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/)
*   [üìΩÔ∏è Chat with YouTube Videos](advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/)

### üîß LLM Fine-tuning Tutorials

*   [üîß Llama 3.2 Fine-tuning](advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/)

## üöÄ Getting Started

1. **Clone the repository** 

    ```bash 
    git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
    ```

2. **Navigate to the desired project directory**

    ```bash 
    cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
    ```

3. **Install the required dependencies**

    ```bash
    pip install -r requirements.txt
    ```

4. **Follow the project-specific instructions** in each project&#039;s `README.md` file to set up and run the app.

## ü§ù Contributing to Open Source

Contributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new [GitHub Issue](https://github.com/Shubhamsaboo/awesome-llm-apps/issues) or submit a pull request. Make sure to follow the existing project structure and include a detailed `README.md` for each new app.

### Thank You, Community, for the Support! üôè

[![Star History Chart](https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;type=Date)](https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;Date)

üåü **Don‚Äôt miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jdepoix/youtube-transcript-api]]></title>
            <link>https://github.com/jdepoix/youtube-transcript-api</link>
            <guid>https://github.com/jdepoix/youtube-transcript-api</guid>
            <pubDate>Fri, 13 Jun 2025 00:04:34 GMT</pubDate>
            <description><![CDATA[This is a python API which allows you to get the transcript/subtitles for a given YouTube video. It also works for automatically generated subtitles and it does not require an API key nor a headless browser, like other selenium based solutions do!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jdepoix/youtube-transcript-api">jdepoix/youtube-transcript-api</a></h1>
            <p>This is a python API which allows you to get the transcript/subtitles for a given YouTube video. It also works for automatically generated subtitles and it does not require an API key nor a headless browser, like other selenium based solutions do!</p>
            <p>Language: Python</p>
            <p>Stars: 4,469</p>
            <p>Forks: 501</p>
            <p>Stars today: 324 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;
  ‚ú® YouTube Transcript API ‚ú®
&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=BAENLEW8VUJ6G&amp;source=url&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Donate-PayPal-green.svg&quot; alt=&quot;Donate&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/jdepoix/youtube-transcript-api/actions&quot;&gt;
    &lt;img src=&quot;https://github.com/jdepoix/youtube-transcript-api/actions/workflows/ci.yml/badge.svg?branch=master&quot; alt=&quot;Build Status&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://coveralls.io/github/jdepoix/youtube-transcript-api?branch=master&quot;&gt;
    &lt;img src=&quot;https://coveralls.io/repos/github/jdepoix/youtube-transcript-api/badge.svg?branch=master&quot; alt=&quot;Coverage Status&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;http://opensource.org/licenses/MIT&quot;&gt;
    &lt;img src=&quot;http://img.shields.io/badge/license-MIT-brightgreen.svg?style=flat&quot; alt=&quot;MIT license&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/youtube-transcript-api/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/youtube-transcript-api.svg&quot; alt=&quot;Current Version&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/youtube-transcript-api/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/pyversions/youtube-transcript-api.svg&quot; alt=&quot;Supported Python Versions&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;b&gt;This is a python API which allows you to retrieve the transcript/subtitles for a given YouTube video. It also works for automatically generated subtitles, supports translating subtitles and it does not require a headless browser, like other selenium based solutions do!&lt;/b&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
 Maintenance of this project is made possible by all the &lt;a href=&quot;https://github.com/jdepoix/youtube-transcript-api/graphs/contributors&quot;&gt;contributors&lt;/a&gt; and &lt;a href=&quot;https://github.com/sponsors/jdepoix&quot;&gt;sponsors&lt;/a&gt;. If you&#039;d like to sponsor this project and have your avatar or company logo appear below &lt;a href=&quot;https://github.com/sponsors/jdepoix&quot;&gt;click here&lt;/a&gt;. üíñ
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.searchapi.io&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://www.searchapi.io/press/v1/svg/searchapi_logo_white_h.svg&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://www.searchapi.io/press/v1/svg/searchapi_logo_black_h.svg&quot;&gt;
      &lt;img alt=&quot;SearchAPI&quot; src=&quot;https://www.searchapi.io/press/v1/svg/searchapi_logo_black_h.svg&quot; height=&quot;40px&quot; style=&quot;vertical-align: middle;&quot;&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
  &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
  &lt;a href=&quot;https://supadata.ai&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://supadata.ai/logo-dark.svg&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://supadata.ai/logo-light.svg&quot;&gt;
      &lt;img alt=&quot;supadata&quot; src=&quot;https://supadata.ai/logo-light.svg&quot; height=&quot;40px&quot;&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
  &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
  &lt;a href=&quot;https://www.dumplingai.com&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://www.dumplingai.com/logos/logo-dark.svg&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://www.dumplingai.com/logos/logo-light.svg&quot;&gt;
      &lt;img alt=&quot;Dumpling AI&quot; src=&quot;https://www.dumplingai.com/logos/logo-light.svg&quot; height=&quot;40px&quot; style=&quot;vertical-align: middle;&quot;&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
&lt;/p&gt;

## Install

It is recommended to [install this module by using pip](https://pypi.org/project/youtube-transcript-api/):

```
pip install youtube-transcript-api
```

You can either integrate this module [into an existing application](#api) or just use it via a [CLI](#cli).

## API

The easiest way to get a transcript for a given video is to execute:

```python
from youtube_transcript_api import YouTubeTranscriptApi

ytt_api = YouTubeTranscriptApi()
ytt_api.fetch(video_id)
```

&gt; **Note:** By default, this will try to access the English transcript of the video. If your video has a different 
&gt; language, or you are interested in fetching a transcript in a different language, please read the section below.

&gt; **Note:** Pass in the video ID, NOT the video URL. For a video with the URL `https://www.youtube.com/watch?v=12345` 
&gt; the ID is `12345`.

This will return a `FetchedTranscript` object looking somewhat like this:

```python
FetchedTranscript(
    snippets=[
        FetchedTranscriptSnippet(
            text=&quot;Hey there&quot;,
            start=0.0,
            duration=1.54,
        ),
        FetchedTranscriptSnippet(
            text=&quot;how are you&quot;,
            start=1.54,
            duration=4.16,
        ),
        # ...
    ],
    video_id=&quot;12345&quot;,
    language=&quot;English&quot;,
    language_code=&quot;en&quot;,
    is_generated=False,
)
```

This object implements most interfaces of a `List`:

```python
ytt_api = YouTubeTranscriptApi()
fetched_transcript = ytt_api.fetch(video_id)

# is iterable
for snippet in fetched_transcript:
    print(snippet.text)

# indexable
last_snippet = fetched_transcript[-1]

# provides a length
snippet_count = len(fetched_transcript)
```

If you prefer to handle the raw transcript data you can call `fetched_transcript.to_raw_data()`, which will return 
a list of dictionaries:

```python
[
    {
        &#039;text&#039;: &#039;Hey there&#039;,
        &#039;start&#039;: 0.0,
        &#039;duration&#039;: 1.54
    },
    {
        &#039;text&#039;: &#039;how are you&#039;,
        &#039;start&#039;: 1.54
        &#039;duration&#039;: 4.16
    },
    # ...
]
```
### Retrieve different languages

You can add the `languages` param if you want to make sure the transcripts are retrieved in your desired language 
(it defaults to english).

```python
YouTubeTranscriptApi().fetch(video_id, languages=[&#039;de&#039;, &#039;en&#039;])
```

It&#039;s a list of language codes in a descending priority. In this example it will first try to fetch the german 
transcript (`&#039;de&#039;`) and then fetch the english transcript (`&#039;en&#039;`) if it fails to do so. If you want to find out 
which languages are available first, [have a look at `list()`](#list-available-transcripts).

If you only want one language, you still need to format the `languages` argument as a list

```python
YouTubeTranscriptApi().fetch(video_id, languages=[&#039;de&#039;])
```

### Preserve formatting

You can also add `preserve_formatting=True` if you&#039;d like to keep HTML formatting elements such as `&lt;i&gt;` (italics) 
and `&lt;b&gt;` (bold).

```python
YouTubeTranscriptApi().fetch(video_ids, languages=[&#039;de&#039;, &#039;en&#039;], preserve_formatting=True)
```

### List available transcripts

If you want to list all transcripts which are available for a given video you can call:

```python
ytt_api = YouTubeTranscriptApi()
transcript_list = ytt_api.list(video_id)
```

This will return a `TranscriptList` object which is iterable and provides methods to filter the list of transcripts for 
specific languages and types, like:

```python
transcript = transcript_list.find_transcript([&#039;de&#039;, &#039;en&#039;])
```

By default this module always chooses manually created transcripts over automatically created ones, if a transcript in 
the requested language is available both manually created and generated. The `TranscriptList` allows you to bypass this 
default behaviour by searching for specific transcript types:

```python
# filter for manually created transcripts
transcript = transcript_list.find_manually_created_transcript([&#039;de&#039;, &#039;en&#039;])

# or automatically generated ones
transcript = transcript_list.find_generated_transcript([&#039;de&#039;, &#039;en&#039;])
```

The methods `find_generated_transcript`, `find_manually_created_transcript`, `find_transcript` return `Transcript` 
objects. They contain metadata regarding the transcript:

```python
print(
    transcript.video_id,
    transcript.language,
    transcript.language_code,
    # whether it has been manually created or generated by YouTube
    transcript.is_generated,
    # whether this transcript can be translated or not
    transcript.is_translatable,
    # a list of languages the transcript can be translated to
    transcript.translation_languages,
)
```

and provide the method, which allows you to fetch the actual transcript data:

```python
transcript.fetch()
```

This returns a `FetchedTranscript` object, just like `YouTubeTranscriptApi().fetch()` does.

### Translate transcript

YouTube has a feature which allows you to automatically translate subtitles. This module also makes it possible to 
access this feature. To do so `Transcript` objects provide a `translate()` method, which returns a new translated 
`Transcript` object:

```python
transcript = transcript_list.find_transcript([&#039;en&#039;])
translated_transcript = transcript.translate(&#039;de&#039;)
print(translated_transcript.fetch())
```

### By example
```python
from youtube_transcript_api import YouTubeTranscriptApi

ytt_api = YouTubeTranscriptApi()

# retrieve the available transcripts
transcript_list = ytt_api.list(&#039;video_id&#039;)

# iterate over all available transcripts
for transcript in transcript_list:

    # the Transcript object provides metadata properties
    print(
        transcript.video_id,
        transcript.language,
        transcript.language_code,
        # whether it has been manually created or generated by YouTube
        transcript.is_generated,
        # whether this transcript can be translated or not
        transcript.is_translatable,
        # a list of languages the transcript can be translated to
        transcript.translation_languages,
    )

    # fetch the actual transcript data
    print(transcript.fetch())

    # translating the transcript will return another transcript object
    print(transcript.translate(&#039;en&#039;).fetch())

# you can also directly filter for the language you are looking for, using the transcript list
transcript = transcript_list.find_transcript([&#039;de&#039;, &#039;en&#039;])  

# or just filter for manually created transcripts  
transcript = transcript_list.find_manually_created_transcript([&#039;de&#039;, &#039;en&#039;])  

# or automatically generated ones  
transcript = transcript_list.find_generated_transcript([&#039;de&#039;, &#039;en&#039;])
```

## Working around IP bans (`RequestBlocked` or `IpBlocked` exception)

Unfortunately, YouTube has started blocking most IPs that are known to belong to cloud providers (like AWS, Google Cloud 
Platform, Azure, etc.), which means you will most likely run into `ReuquestBlocked` or `IpBlocked` exceptions when 
deploying your code to any cloud solutions. Same can happen to the IP of your self-hosted solution, if you are doing 
too many requests. You can work around these IP bans using proxies. However, since YouTube will ban static proxies 
after extended use, going for rotating residential proxies provide is the most reliable option.

There are different providers that offer rotating residential proxies, but after testing different 
offerings I have found [Webshare](https://www.webshare.io/?referral_code=w0xno53eb50g) to be the most reliable and have 
therefore integrated it into this module, to make setting it up as easy as possible.

### Using [Webshare](https://www.webshare.io/?referral_code=w0xno53eb50g)

Once you have created a [Webshare account](https://www.webshare.io/?referral_code=w0xno53eb50g) and purchased a 
&quot;Residential&quot; proxy package that suits your workload (make sure NOT to purchase &quot;Proxy Server&quot; or 
&quot;Static Residential&quot;!), open the 
[Webshare Proxy Settings](https://dashboard.webshare.io/proxy/settings?referral_code=w0xno53eb50g) to retrieve 
your &quot;Proxy Username&quot; and &quot;Proxy Password&quot;. Using this information you can initialize the `YouTubeTranscriptApi` as 
follows:

```python
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.proxies import WebshareProxyConfig

ytt_api = YouTubeTranscriptApi(
    proxy_config=WebshareProxyConfig(
        proxy_username=&quot;&lt;proxy-username&gt;&quot;,
        proxy_password=&quot;&lt;proxy-password&gt;&quot;,
    )
)

# all requests done by ytt_api will now be proxied through Webshare
ytt_api.fetch(video_id)
```

Using the `WebshareProxyConfig` will default to using rotating residential proxies and requires no further 
configuration.

Note that [referral links are used here](https://www.webshare.io/?referral_code=w0xno53eb50g) and any purchases 
made through these links will support this Open Source project, which is very much appreciated! üíñüòäüôèüíñ

However, you are of course free to integrate your own proxy solution using the `GenericProxyConfig` class, if you 
prefer using another provider or want to implement your own solution, as covered by the following section.

### Using other Proxy solutions

Alternatively to using [Webshare](#using-webshare), you can set up any generic HTTP/HTTPS/SOCKS proxy using the 
`GenericProxyConfig` class:

```python
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.proxies import GenericProxyConfig

ytt_api = YouTubeTranscriptApi(
    proxy_config=GenericProxyConfig(
        http_url=&quot;http://user:pass@my-custom-proxy.org:port&quot;,
        https_url=&quot;https://user:pass@my-custom-proxy.org:port&quot;,
    )
)

# all requests done by ytt_api will now be proxied using the defined proxy URLs
ytt_api.fetch(video_id)
```

Be aware that using a proxy doesn&#039;t guarantee that you won&#039;t be blocked, as YouTube can always block the IP of your 
proxy! Therefore, you should always choose a solution that rotates through a pool of proxy addresses, if you want to
maximize reliability.

## Overwriting request defaults

When initializing a `YouTubeTranscriptApi` object, it will create a `requests.Session` which will be used for all
HTTP(S) request. This allows for caching cookies when retrieving multiple requests. However, you can optionally pass a
`requests.Session` object into its constructor, if you manually want to share cookies between different instances of
`YouTubeTranscriptApi`, overwrite defaults, set custom headers, specify SSL certificates, etc.

```python
from requests import Session

http_client = Session()

# set custom header
http_client.headers.update({&quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;})

# set path to CA_BUNDLE file
http_client.verify = &quot;/path/to/certfile&quot;

ytt_api = YouTubeTranscriptApi(http_client=http_client)
ytt_api.fetch(video_id)

# share same Session between two instances of YouTubeTranscriptApi
ytt_api_2 = YouTubeTranscriptApi(http_client=http_client)
# now shares cookies with ytt_api
ytt_api_2.fetch(video_id)
```

## Cookie Authentication

Some videos are age restricted, so this module won&#039;t be able to access those videos without some sort of
authentication. Unfortunately, some recent changes to the YouTube API have broken the current implementation of cookie 
based authentication, so this feature is currently not available.

## Using Formatters
Formatters are meant to be an additional layer of processing of the transcript you pass it. The goal is to convert a
`FetchedTranscript` object into a consistent string of a given &quot;format&quot;. Such as a basic text (`.txt`) or even formats 
that have a defined specification such as JSON (`.json`), WebVTT (`.vtt`), SRT (`.srt`), Comma-separated format 
(`.csv`), etc...

The `formatters` submodule provides a few basic formatters, which can be used as is, or extended to your needs:

- JSONFormatter
- PrettyPrintFormatter
- TextFormatter
- WebVTTFormatter
- SRTFormatter

Here is how to import from the `formatters` module.

```python
# the base class to inherit from when creating your own formatter.
from youtube_transcript_api.formatters import Formatter

# some provided subclasses, each outputs a different string format.
from youtube_transcript_api.formatters import JSONFormatter
from youtube_transcript_api.formatters import TextFormatter
from youtube_transcript_api.formatters import WebVTTFormatter
from youtube_transcript_api.formatters import SRTFormatter
```

### Formatter Example
Let&#039;s say we wanted to retrieve a transcript and store it to a JSON file. That would look something like this:

```python
# your_custom_script.py

from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.formatters import JSONFormatter

ytt_api = YouTubeTranscriptApi()
transcript = ytt_api.fetch(video_id)

formatter = JSONFormatter()

# .format_transcript(transcript) turns the transcript into a JSON string.
json_formatted = formatter.format_transcript(transcript)

# Now we can write it out to a file.
with open(&#039;your_filename.json&#039;, &#039;w&#039;, encoding=&#039;utf-8&#039;) as json_file:
    json_file.write(json_formatted)

# Now should have a new JSON file that you can easily read back into Python.
```

**Passing extra keyword arguments**

Since JSONFormatter leverages `json.dumps()` you can also forward keyword arguments into 
`.format_transcript(transcript)` such as making your file output prettier by forwarding the `indent=2` keyword argument.

```python
json_formatted = JSONFormatter().format_transcript(transcript, indent=2)
```

### Custom Formatter Example
You can implement your own formatter class. Just inherit from the `Formatter` base class and ensure you implement the 
`format_transcript(self, transcript: FetchedTranscript, **kwargs) -&gt; str` and 
`format_transcripts(self, transcripts: List[FetchedTranscript], **kwargs) -&gt; str` methods which should ultimately 
return a string when called on your formatter instance.

```python
class MyCustomFormatter(Formatter):
    def format_transcript(self, transcript: FetchedTranscript, **kwargs) -&gt; str:
        # Do your custom work in here, but return a string.
        return &#039;your processed output data as a string.&#039;

    def format_transcripts(self, transcripts: List[FetchedTranscript], **kwargs) -&gt; str:
        # Do your custom work in here to format a list of transcripts, but return a string.
        return &#039;your processed output data as a string.&#039;
```

## CLI

Execute the CLI script using the video ids as parameters and the results will be printed out to the command line:  

```  
youtube_transcript_api &lt;first_video_id&gt; &lt;second_video_id&gt; ...  
```  

The CLI also gives you the option to provide a list of preferred languages:  

```  
youtube_transcript_api &lt;first_video_id&gt; &lt;second_video_id&gt; ... --languages de en  
```

You can also specify if you want to exclude automatically generated or manually created subtitles:

```  
youtube_transcript_api &lt;first_video_id&gt; &lt;second_video_id&gt; ... --languages de en --exclude-generated
youtube_transcript_api &lt;first_video_id&gt; &lt;second_video_id&gt; ... --languages de en --exclude-manually-created
```

If you would prefer to write it into a file or pipe it into another application, you can also output the results as 
json using the following line:  

```  
youtube_transcript_api &lt;first_video_id&gt; &lt;second_video_id&gt; ... --languages de en --format json &gt; transcripts.json
```  

Translating transcripts using the CLI is also possible:

```  
youtube_transcript_api &lt;first_video_id&gt; &lt;second_video_id&gt; ... --languages en --translate de
```  

If you are not sure which languages are available for a given video you can call, to list all available transcripts:

```  
youtube_transcript_api --list-transcripts &lt;first_video_id&gt;
```

If a video&#039;s ID starts with a hyphen you&#039;ll have to mask the hyphen using `\` to prevent the CLI from mistaking it for 
a argument name. For example to get the transcript for the video with the ID `-abc123` run:

```
youtube_transcript_api &quot;\-abc123&quot;
```

### Working around IP bans using the CLI

If you are running into `ReqestBlocked` or `IpBlocked` errors, because YouTube blocks your IP, you can work around this 
using residential proxies as explained in 
[Working around IP bans](#working-around-ip-bans-requestblocked-or-ipblocked-exception). To use
[Webshare &quot;Residential&quot; proxies](https://www.webshare.io/?referral_code=w0xno53eb50g) through the CLI, you will have to 
cre

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[confident-ai/deepeval]]></title>
            <link>https://github.com/confident-ai/deepeval</link>
            <guid>https://github.com/confident-ai/deepeval</guid>
            <pubDate>Fri, 13 Jun 2025 00:04:33 GMT</pubDate>
            <description><![CDATA[The LLM Evaluation Framework]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/confident-ai/deepeval">confident-ai/deepeval</a></h1>
            <p>The LLM Evaluation Framework</p>
            <p>Language: Python</p>
            <p>Stars: 7,450</p>
            <p>Forks: 683</p>
            <p>Stars today: 78 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/confident-ai/deepeval/blob/main/docs/static/img/deepeval.png&quot; alt=&quot;DeepEval Logo&quot; width=&quot;100%&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;h1 align=&quot;center&quot;&gt;The LLM Evaluation Framework&lt;/h1&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://discord.gg/3SEyvpgu2f&quot;&gt;
        &lt;img alt=&quot;discord-invite&quot; src=&quot;https://dcbadge.vercel.app/api/server/3SEyvpgu2f?style=flat&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;h4 align=&quot;center&quot;&gt;
    &lt;p&gt;
        &lt;a href=&quot;https://deepeval.com/docs/getting-started?utm_source=GitHub&quot;&gt;Documentation&lt;/a&gt; |
        &lt;a href=&quot;#-metrics-and-features&quot;&gt;Metrics and Features&lt;/a&gt; |
        &lt;a href=&quot;#-quickstart&quot;&gt;Getting Started&lt;/a&gt; |
        &lt;a href=&quot;#-integrations&quot;&gt;Integrations&lt;/a&gt; |
        &lt;a href=&quot;https://confident-ai.com?utm_source=GitHub&quot;&gt;DeepEval Platform&lt;/a&gt;
    &lt;p&gt;
&lt;/h4&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://github.com/confident-ai/deepeval/releases&quot;&gt;
        &lt;img alt=&quot;GitHub release&quot; src=&quot;https://img.shields.io/github/release/confident-ai/deepeval.svg?color=violet&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://colab.research.google.com/drive/1PPxYEBa6eu__LquGoFFJZkhYgWVYE6kh?usp=sharing&quot;&gt;
        &lt;img alt=&quot;Try Quickstart in Colab&quot; src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/confident-ai/deepeval/blob/master/LICENSE.md&quot;&gt;
        &lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/github/license/confident-ai/deepeval.svg?color=yellow&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=de&quot;&gt;Deutsch&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=es&quot;&gt;Espa√±ol&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=fr&quot;&gt;fran√ßais&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=ja&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=ko&quot;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=pt&quot;&gt;Portugu√™s&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=ru&quot;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | 
    &lt;a href=&quot;https://www.readme-i18n.com/confident-ai/deepeval?lang=zh&quot;&gt;‰∏≠Êñá&lt;/a&gt;
&lt;/p&gt;

**DeepEval** is a simple-to-use, open-source LLM evaluation framework, for evaluating and testing large-language model systems. It is similar to Pytest but specialized for unit testing LLM outputs. DeepEval incorporates the latest research to evaluate LLM outputs based on metrics such as G-Eval, hallucination, answer relevancy, RAGAS, etc., which uses LLMs and various other NLP models that runs **locally on your machine** for evaluation.

Whether your LLM applications are RAG pipelines, chatbots, AI agents, implemented via LangChain or LlamaIndex, DeepEval has you covered. With it, you can easily determine the optimal models, prompts, and architecture to improve your RAG pipeline, agentic workflows, prevent prompt drifting, or even transition from OpenAI to hosting your own Deepseek R1 with confidence.

&gt; [!IMPORTANT]
&gt; Need a place for your DeepEval testing data to live üè°‚ù§Ô∏è? [Sign up to the DeepEval platform](https://confident-ai.com?utm_source=GitHub) to compare iterations of your LLM app, generate &amp; share testing reports, and more.
&gt;
&gt; ![Demo GIF](assets/demo.gif)

&gt; Want to talk LLM evaluation, need help picking metrics, or just to say hi? [Come join our discord.](https://discord.com/invite/3SEyvpgu2f)

&lt;br /&gt;

# üî• Metrics and Features

&gt; ü•≥ You can now share DeepEval&#039;s test results on the cloud directly on [Confident AI](https://confident-ai.com?utm_source=GitHub)&#039;s infrastructure

- Supports both end-to-end and component-level LLM evaluation.
- Large variety of ready-to-use LLM evaluation metrics (all with explanations) powered by **ANY** LLM of your choice, statistical methods, or NLP models that runs **locally on your machine**:
  - G-Eval
  - DAG ([deep acyclic graph](https://deepeval.com/docs/metrics-dag))
  - **RAG metrics:**
    - Answer Relevancy
    - Faithfulness
    - Contextual Recall
    - Contextual Precision
    - Contextual Relevancy
    - RAGAS
  - **Agentic metrics:**
    - Task Completion
    - Tool Correctness
  - **Others:**
    - Hallucination
    - Summarization
    - Bias
    - Toxicity
  - **Conversational metrics:**
    - Knowledge Retention
    - Conversation Completeness
    - Conversation Relevancy
    - Role Adherence
  - etc.
- Build your own custom metrics that are automatically integrated with DeepEval&#039;s ecosystem.
- Generate synthetic datasets for evaluation.
- Integrates seamlessly with **ANY** CI/CD environment.
- [Red team your LLM application](https://deepeval.com/docs/red-teaming-introduction) for 40+ safety vulnerabilities in a few lines of code, including:
  - Toxicity
  - Bias
  - SQL Injection
  - etc., using advanced 10+ attack enhancement strategies such as prompt injections.
- Easily benchmark **ANY** LLM on popular LLM benchmarks in [under 10 lines of code.](https://deepeval.com/docs/benchmarks-introduction?utm_source=GitHub), which includes:
  - MMLU
  - HellaSwag
  - DROP
  - BIG-Bench Hard
  - TruthfulQA
  - HumanEval
  - GSM8K
- [100% integrated with Confident AI](https://confident-ai.com?utm_source=GitHub) for the full evaluation lifecycle:
  - Curate/annotate evaluation datasets on the cloud
  - Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best
  - Fine-tune metrics for custom results
  - Debug evaluation results via LLM traces
  - Monitor &amp; evaluate LLM responses in product to improve datasets with real-world data
  - Repeat until perfection

&gt; [!NOTE]
&gt; Confident AI is the DeepEval platform. Create an account [here.](https://app.confident-ai.com?utm_source=GitHub)

&lt;br /&gt;

# üîå Integrations

- ü¶Ñ LlamaIndex, to [**unit test RAG applications in CI/CD**](https://www.deepeval.com/integrations/frameworks/llamaindex?utm_source=GitHub)
- ü§ó Hugging Face, to [**enable real-time evaluations during LLM fine-tuning**](https://www.deepeval.com/integrations/frameworks/huggingface?utm_source=GitHub)

&lt;br /&gt;

# üöÄ QuickStart

Let&#039;s pretend your LLM application is a RAG based customer support chatbot; here&#039;s how DeepEval can help test what you&#039;ve built.

## Installation

```
pip install -U deepeval
```

## Create an account (highly recommended)

Using the `deepeval` platform will allow you to generate sharable testing reports on the cloud. It is free, takes no additional code to setup, and we highly recommend giving it a try.

To login, run:

```
deepeval login
```

Follow the instructions in the CLI to create an account, copy your API key, and paste it into the CLI. All test cases will automatically be logged (find more information on data privacy [here](https://deepeval.com/docs/data-privacy?utm_source=GitHub)).

## Writing your first test case

Create a test file:

```bash
touch test_chatbot.py
```

Open `test_chatbot.py` and write your first test case to run an **end-to-end** evaluation using DeepEval, which treats your LLM app as a black-box:

```python
import pytest
from deepeval import assert_test
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams

def test_case():
    correctness_metric = GEval(
        name=&quot;Correctness&quot;,
        criteria=&quot;Determine if the &#039;actual output&#039; is correct based on the &#039;expected output&#039;.&quot;,
        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],
        threshold=0.5
    )
    test_case = LLMTestCase(
        input=&quot;What if these shoes don&#039;t fit?&quot;,
        # Replace this with the actual output from your LLM application
        actual_output=&quot;You have 30 days to get a full refund at no extra cost.&quot;,
        expected_output=&quot;We offer a 30-day full refund at no extra costs.&quot;,
        retrieval_context=[&quot;All customers are eligible for a 30 day full refund at no extra costs.&quot;]
    )
    assert_test(test_case, [correctness_metric])
```

Set your `OPENAI_API_KEY` as an environment variable (you can also evaluate using your own custom model, for more details visit [this part of our docs](https://deepeval.com/docs/metrics-introduction#using-a-custom-llm?utm_source=GitHub)):

```
export OPENAI_API_KEY=&quot;...&quot;
```

And finally, run `test_chatbot.py` in the CLI:

```
deepeval test run test_chatbot.py
```

**Congratulations! Your test case should have passed ‚úÖ** Let&#039;s breakdown what happened.

- The variable `input` mimics a user input, and `actual_output` is a placeholder for what your application&#039;s supposed to output based on this input.
- The variable `expected_output` represents the ideal answer for a given `input`, and [`GEval`](https://deepeval.com/docs/metrics-llm-evals) is a research-backed metric provided by `deepeval` for you to evaluate your LLM output&#039;s on any custom custom with human-like accuracy.
- In this example, the metric `criteria` is correctness of the `actual_output` based on the provided `expected_output`.
- All metric scores range from 0 - 1, which the `threshold=0.5` threshold ultimately determines if your test have passed or not.

[Read our documentation](https://deepeval.com/docs/getting-started?utm_source=GitHub) for more information on more options to run end-to-end evaluation, how to use additional metrics, create your own custom metrics, and tutorials on how to integrate with other tools like LangChain and LlamaIndex.

&lt;br /&gt;

## Evaluating Nested Components

If you wish to evaluate individual components within your LLM app, you need to run **component-level** evals - a powerful way to evaluate any component within an LLM system.

Simply trace &quot;components&quot; such as LLM calls, retrievers, tool calls, and agents within your LLM application using the `@observe` decorator to apply metrics on a component-level. Tracing with `deepeval` is non-instrusive (learn more [here](https://deepeval.com/docs/evaluation-llm-tracing#dont-be-worried-about-tracing)) and helps you avoid rewriting your codebase just for evals:

```python
from deepeval.tracing import observe, update_current_span
from deepeval.test_case import LLMTestCase
from deepeval.dataset import Golden
from deepeval.metrics import GEval
from deepeval import evaluate

correctness = GEval(name=&quot;Correctness&quot;, criteria=&quot;Determine if the &#039;actual output&#039; is correct based on the &#039;expected output&#039;.&quot;, evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT])

@observe(metrics=[correctness])
def inner_component():
    # Component can be anything from an LLM call, retrieval, agent, tool use, etc.
    update_current_span(test_case=LLMTestCase(input=&quot;...&quot;, actual_output=&quot;...&quot;))
    return

@observe
def llm_app(input: str):
    inner_component()
    return

evaluate(observed_callback=llm_app, goldens=[Golden(input=&quot;Hi!&quot;)])
```

You can learn everything about component-level evaluations [here.](https://www.deepeval.com/docs/evaluation-component-level-llm-evals)

&lt;br /&gt;

## Evaluating Without Pytest Integration

Alternatively, you can evaluate without Pytest, which is more suited for a notebook environment.

```python
from deepeval import evaluate
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)
test_case = LLMTestCase(
    input=&quot;What if these shoes don&#039;t fit?&quot;,
    # Replace this with the actual output from your LLM application
    actual_output=&quot;We offer a 30-day full refund at no extra costs.&quot;,
    retrieval_context=[&quot;All customers are eligible for a 30 day full refund at no extra costs.&quot;]
)
evaluate([test_case], [answer_relevancy_metric])
```

## Using Standalone Metrics

DeepEval is extremely modular, making it easy for anyone to use any of our metrics. Continuing from the previous example:

```python
from deepeval.metrics import AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase

answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)
test_case = LLMTestCase(
    input=&quot;What if these shoes don&#039;t fit?&quot;,
    # Replace this with the actual output from your LLM application
    actual_output=&quot;We offer a 30-day full refund at no extra costs.&quot;,
    retrieval_context=[&quot;All customers are eligible for a 30 day full refund at no extra costs.&quot;]
)

answer_relevancy_metric.measure(test_case)
print(answer_relevancy_metric.score)
# All metrics also offer an explanation
print(answer_relevancy_metric.reason)
```

Note that some metrics are for RAG pipelines, while others are for fine-tuning. Make sure to use our docs to pick the right one for your use case.

## Evaluating a Dataset / Test Cases in Bulk

In DeepEval, a dataset is simply a collection of test cases. Here is how you can evaluate these in bulk:

```python
import pytest
from deepeval import assert_test
from deepeval.metrics import HallucinationMetric, AnswerRelevancyMetric
from deepeval.test_case import LLMTestCase
from deepeval.dataset import EvaluationDataset

first_test_case = LLMTestCase(input=&quot;...&quot;, actual_output=&quot;...&quot;, context=[&quot;...&quot;])
second_test_case = LLMTestCase(input=&quot;...&quot;, actual_output=&quot;...&quot;, context=[&quot;...&quot;])

dataset = EvaluationDataset(test_cases=[first_test_case, second_test_case])

@pytest.mark.parametrize(
    &quot;test_case&quot;,
    dataset,
)
def test_customer_chatbot(test_case: LLMTestCase):
    hallucination_metric = HallucinationMetric(threshold=0.3)
    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)
    assert_test(test_case, [hallucination_metric, answer_relevancy_metric])
```

```bash
# Run this in the CLI, you can also add an optional -n flag to run tests in parallel
deepeval test run test_&lt;filename&gt;.py -n 4
```

&lt;br/&gt;

Alternatively, although we recommend using `deepeval test run`, you can evaluate a dataset/test cases without using our Pytest integration:

```python
from deepeval import evaluate
...

evaluate(dataset, [answer_relevancy_metric])
# or
dataset.evaluate([answer_relevancy_metric])
```

# LLM Evaluation With Confident AI

The correct LLM evaluation lifecycle is only achievable with [the DeepEval platform](https://confident-ai.com?utm_source=Github). It allows you to:

1. Curate/annotate evaluation datasets on the cloud
2. Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best
3. Fine-tune metrics for custom results
4. Debug evaluation results via LLM traces
5. Monitor &amp; evaluate LLM responses in product to improve datasets with real-world data
6. Repeat until perfection

Everything on Confident AI, including how to use Confident is available [here](https://documentation.confident-ai.com?utm_source=GitHub).

To begin, login from the CLI:

```bash
deepeval login
```

Follow the instructions to log in, create your account, and paste your API key into the CLI.

Now, run your test file again:

```bash
deepeval test run test_chatbot.py
```

You should see a link displayed in the CLI once the test has finished running. Paste it into your browser to view the results!

![Demo GIF](assets/demo.gif)

&lt;br /&gt;

# Contributing

Please read [CONTRIBUTING.md](https://github.com/confident-ai/deepeval/blob/main/CONTRIBUTING.md) for details on our code of conduct, and the process for submitting pull requests to us.

&lt;br /&gt;

# Roadmap

Features:

- [x] Integration with Confident AI
- [x] Implement G-Eval
- [x] Implement RAG metrics
- [x] Implement Conversational metrics
- [x] Evaluation Dataset Creation
- [x] Red-Teaming
- [ ] DAG custom metrics
- [ ] Guardrails

&lt;br /&gt;

# Authors

Built by the founders of Confident AI. Contact jeffreyip@confident-ai.com for all enquiries.

&lt;br /&gt;

# License

DeepEval is licensed under Apache 2.0 - see the [LICENSE.md](https://github.com/confident-ai/deepeval/blob/main/LICENSE.md) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[virattt/ai-hedge-fund]]></title>
            <link>https://github.com/virattt/ai-hedge-fund</link>
            <guid>https://github.com/virattt/ai-hedge-fund</guid>
            <pubDate>Fri, 13 Jun 2025 00:04:32 GMT</pubDate>
            <description><![CDATA[An AI Hedge Fund Team]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/virattt/ai-hedge-fund">virattt/ai-hedge-fund</a></h1>
            <p>An AI Hedge Fund Team</p>
            <p>Language: Python</p>
            <p>Stars: 35,846</p>
            <p>Forks: 6,232</p>
            <p>Stars today: 133 stars today</p>
            <h2>README</h2><pre># AI Hedge Fund

This is a proof of concept for an AI-powered hedge fund.  The goal of this project is to explore the use of AI to make trading decisions.  This project is for **educational** purposes only and is not intended for real trading or investment.

This system employs several agents working together:

1. Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation
2. Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety
3. Bill Ackman Agent - An activist investor, takes bold positions and pushes for change
4. Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption
5. Charlie Munger Agent - Warren Buffett&#039;s partner, only buys wonderful businesses at fair prices
6. Michael Burry Agent - The Big Short contrarian who hunts for deep value
7. Peter Lynch Agent - Practical investor who seeks &quot;ten-baggers&quot; in everyday businesses
8. Phil Fisher Agent - Meticulous growth investor who uses deep &quot;scuttlebutt&quot; research 
9. Rakesh Jhunjhunwala Agent - The Big Bull of India
10. Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential
11. Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price
12. Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals
13. Sentiment Agent - Analyzes market sentiment and generates trading signals
14. Fundamentals Agent - Analyzes fundamental data and generates trading signals
15. Technicals Agent - Analyzes technical indicators and generates trading signals
16. Risk Manager - Calculates risk metrics and sets position limits
17. Portfolio Manager - Makes final trading decisions and generates orders
    
&lt;img width=&quot;1042&quot; alt=&quot;Screenshot 2025-03-22 at 6 19 07 PM&quot; src=&quot;https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4&quot; /&gt;


**Note**: the system simulates trading decisions, it does not actually trade.

[![Twitter Follow](https://img.shields.io/twitter/follow/virattt?style=social)](https://twitter.com/virattt)

## Disclaimer

This project is for **educational and research purposes only**.

- Not intended for real trading or investment
- No investment advice or guarantees provided
- Creator assumes no liability for financial losses
- Consult a financial advisor for investment decisions
- Past performance does not indicate future results

By using this software, you agree to use it solely for learning purposes.

## Table of Contents
- [Setup](#setup)
  - [Using Poetry](#using-poetry)
  - [Using Docker](#using-docker)
- [Usage](#usage)
  - [Running the Hedge Fund](#running-the-hedge-fund)
  - [Running the Backtester](#running-the-backtester)
- [Contributing](#contributing)
- [Feature Requests](#feature-requests)
- [License](#license)

## Setup

### Using Poetry

Clone the repository:
```bash
git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
```

1. Install Poetry (if not already installed):
```bash
curl -sSL https://install.python-poetry.org | python3 -
```

2. Install dependencies:
```bash
poetry install
```

3. Set up your environment variables:
```bash
# Create .env file for your API keys
cp .env.example .env
```

4. Set your API keys:
```bash
# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
# Get your OpenAI API key from https://platform.openai.com/
OPENAI_API_KEY=your-openai-api-key

# For running LLMs hosted by groq (deepseek, llama3, etc.)
# Get your Groq API key from https://groq.com/
GROQ_API_KEY=your-groq-api-key

# For getting financial data to power the hedge fund
# Get your Financial Datasets API key from https://financialdatasets.ai/
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
```

### Using Docker

1. Make sure you have Docker installed on your system. If not, you can download it from [Docker&#039;s official website](https://www.docker.com/get-started).

2. Clone the repository:
```bash
git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
```

3. Set up your environment variables:
```bash
# Create .env file for your API keys
cp .env.example .env
```

4. Edit the .env file to add your API keys as described above.

5. Navigate to the docker directory:
```bash
cd docker
```

6. Build the Docker image:
```bash
# On Linux/Mac:
./run.sh build

# On Windows:
run.bat build
```

**Important**: You must set `OPENAI_API_KEY`, `GROQ_API_KEY`, `ANTHROPIC_API_KEY`, or `DEEPSEEK_API_KEY` for the hedge fund to work.  If you want to use LLMs from all providers, you will need to set all API keys.

Financial data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key.

For any other ticker, you will need to set the `FINANCIAL_DATASETS_API_KEY` in the .env file.

## Usage

### Running the Hedge Fund

#### With Poetry
```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA
```

#### With Docker
**Note**: All Docker commands must be run from the `docker/` directory.

```bash
# Navigate to the docker directory first
cd docker

# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA main

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA main
```

**Example Output:**
&lt;img width=&quot;992&quot; alt=&quot;Screenshot 2025-01-06 at 5 50 17 PM&quot; src=&quot;https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b&quot; /&gt;

You can also specify a `--ollama` flag to run the AI hedge fund using local LLMs.

```bash
# With Poetry:
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --ollama main

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --ollama main
```

You can also specify a `--show-reasoning` flag to print the reasoning of each agent to the console.

```bash
# With Poetry:
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --show-reasoning

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --show-reasoning main

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --show-reasoning main
```

You can optionally specify the start and end dates to make decisions for a specific time period.

```bash
# With Poetry:
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 main

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 main
```

### Running the Backtester

#### With Poetry
```bash
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
```

#### With Docker
**Note**: All Docker commands must be run from the `docker/` directory.

```bash
# Navigate to the docker directory first
cd docker

# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA backtest

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA backtest
```

**Example Output:**
&lt;img width=&quot;941&quot; alt=&quot;Screenshot 2025-01-06 at 5 47 52 PM&quot; src=&quot;https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47&quot; /&gt;


You can optionally specify the start and end dates to backtest over a specific time period.

```bash
# With Poetry:
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 backtest

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01 backtest
```

You can also specify a `--ollama` flag to run the backtester using local LLMs.
```bash
# With Poetry:
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA --ollama

# With Docker (from docker/ directory):
# On Linux/Mac:
./run.sh --ticker AAPL,MSFT,NVDA --ollama backtest

# On Windows:
run.bat --ticker AAPL,MSFT,NVDA --ollama backtest
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

**Important**: Please keep your pull requests small and focused.  This will make it easier to review and merge.

## Feature Requests

If you have a feature request, please open an [issue](https://github.com/virattt/ai-hedge-fund/issues) and make sure it is tagged with `enhancement`.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[OWASP/Nettacker]]></title>
            <link>https://github.com/OWASP/Nettacker</link>
            <guid>https://github.com/OWASP/Nettacker</guid>
            <pubDate>Fri, 13 Jun 2025 00:04:31 GMT</pubDate>
            <description><![CDATA[Automated Penetration Testing Framework - Open-Source Vulnerability Scanner - Vulnerability Management]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OWASP/Nettacker">OWASP/Nettacker</a></h1>
            <p>Automated Penetration Testing Framework - Open-Source Vulnerability Scanner - Vulnerability Management</p>
            <p>Language: Python</p>
            <p>Stars: 4,130</p>
            <p>Forks: 869</p>
            <p>Stars today: 47 stars today</p>
            <h2>README</h2><pre>OWASP Nettacker
=========
[![Build Status](https://github.com/OWASP/Nettacker/actions/workflows/ci_cd.yml/badge.svg?branch=master)](https://github.com/OWASP/Nettacker/actions/workflows/ci_cd.yml/badge.svg?branch=master)
[![Apache License](https://img.shields.io/badge/License-Apache%20v2-green.svg)](https://github.com/OWASP/Nettacker/blob/master/LICENSE)
[![Twitter](https://img.shields.io/badge/Twitter-@iotscan-blue.svg)](https://twitter.com/iotscan)
![GitHub contributors](https://img.shields.io/github/contributors/OWASP/Nettacker)
[![Documentation Status](https://readthedocs.org/projects/nettacker/badge/?version=latest)](https://nettacker.readthedocs.io/en/latest/?badge=latest)
[![repo size ](https://img.shields.io/github/repo-size/OWASP/Nettacker)](https://github.com/OWASP/Nettacker)
[![Docker Pulls](https://img.shields.io/docker/pulls/owasp/nettacker)](https://hub.docker.com/r/owasp/nettacker)


&lt;img src=&quot;https://raw.githubusercontent.com/OWASP/Nettacker/master/nettacker/web/static/img/owasp-nettacker.png&quot; width=&quot;200&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/OWASP/Nettacker/master/nettacker/web/static/img/owasp.png&quot; width=&quot;500&quot;&gt;


**DISCLAIMER**

* ***THIS SOFTWARE WAS CREATED FOR AUTOMATED PENETRATION TESTING AND INFORMATION GATHERING. YOU MUST USE THIS SOFTWARE IN A RESPONSIBLE AND ETHICAL MANNER. DO NOT TARGET SYSTEMS OR APPLICATIONS WITHOUT OBTAINING PERMISSIONS OR CONSENT FROM THE SYSTEM OWNERS OR ADMINISTRATORS. CONTRIBUTORS WILL NOT BE RESPONSIBLE FOR ANY ILLEGAL USAGE.***

![2018-01-19_0-45-07](https://user-images.githubusercontent.com/7676267/35123376-283d5a3e-fcb7-11e7-9b1c-92b78ed4fecc.gif)

OWASP Nettacker project is created to automate information gathering, vulnerability scanning and eventually generating a report for networks, including services, bugs, vulnerabilities, misconfigurations, and other information. This software **will** utilize TCP SYN, ACK, ICMP, and many other protocols in order to detect and bypass Firewall/IDS/IPS devices. By leveraging a unique method in OWASP Nettacker for discovering protected services and devices such as SCADA. It would make a competitive edge compared to other scanners making it one of the best.


* OWASP Page: https://owasp.org/www-project-nettacker/
* Wiki: https://github.com/OWASP/Nettacker/wiki
* Slack: #project-nettacker on https://owasp.slack.com
* Installation: https://github.com/OWASP/Nettacker/wiki/Installation
* Usage: https://github.com/OWASP/Nettacker/wiki/Usage
* GitHub: https://github.com/OWASP/Nettacker
* Docker Image: https://hub.docker.com/r/owasp/nettacker
* How to use the Dockerfile: https://github.com/OWASP/Nettacker/wiki/Installation#docker
* OpenHub: https://www.openhub.net/p/OWASP-Nettacker
* **Donate**: https://owasp.org/donate/?reponame=www-project-nettacker&amp;title=OWASP+Nettacker
* **Read More**: https://www.secologist.com/open-source-projects

____________
Quick Setup &amp; Run
============
```bash
$ docker-compose up -d &amp;&amp; docker exec -it nettacker-nettacker-1 /bin/bash
# poetry run python nettacker.py -i owasp.org -s -m port_scan
```
* Results are accessible from your (https://localhost:5000) or https://nettacker-api.z3r0d4y.com:5000/ (pointed to your localhost)
* The local database is `.data/nettacker.db` (sqlite).
* Default results path is `.data/results`
* `docker-compose` will share your nettacker folder, so you will not lose any data after `docker-compose down`
* To see the API key in you can run `docker logs nettacker_nettacker_1`.
* More details and setup without docker https://github.com/OWASP/Nettacker/wiki/Installation
_____________
Thanks to our awesome contributors
============
![Awesome Contributors](https://contrib.rocks/image?repo=OWASP/Nettacker)

## Adopters

We‚Äôre grateful to the organizations, community projects, and individuals who adopt and rely on OWASP Nettacker for their security workflows.

If you‚Äôre using OWASP Nettacker in your organization or project, we‚Äôd love to hear from you! Feel free to add your details to the [ADOPTERS.md](ADOPTERS.md) file by submitting a pull request or reach out to us via GitHub issues. Let‚Äôs showcase how Nettacker is making a difference in the security community!

 See [ADOPTERS.md](ADOPTERS.md) for details.

_____________

## ***IoT Scanner***
*	Python Multi Thread &amp; Multi Process Network Information Gathering Vulnerability Scanner
*	Service and Device Detection ( SCADA, Restricted Areas, Routers, HTTP Servers, Logins and Authentications, None-Indexed HTTP, Paradox System, Cameras, Firewalls, UTM, WebMails, VPN, RDP, SSH, FTP, TELNET Services, Proxy Servers and Many Devices like Juniper, Cisco, Switches and many more‚Ä¶ ) 
*	Asset Discovery &amp; Network Service Analysis
*	Services Brute Force Testing
*	Services Vulnerability Testing
*	HTTP/HTTPS Crawling, Fuzzing, Information Gathering and ‚Ä¶ 
*	HTML, JSON, CSV and Text Outputs
* API &amp; WebUI
*	This project is at the moment in research and development phase 
* Thanks to Google Summer of Code Initiative and all the students who contributed to this project during their summer breaks: 


&lt;img src=&quot;https://betanews.com/wp-content/uploads/2016/03/vertical-GSoC-logo.jpg&quot; width=&quot;200&quot;&gt;&lt;/img&gt;

_____________
## Stargazers over time

[![Stargazers over time](https://starchart.cc/OWASP/Nettacker.svg)](https://starchart.cc/OWASP/Nettacker)


</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[modelcontextprotocol/servers]]></title>
            <link>https://github.com/modelcontextprotocol/servers</link>
            <guid>https://github.com/modelcontextprotocol/servers</guid>
            <pubDate>Fri, 13 Jun 2025 00:04:30 GMT</pubDate>
            <description><![CDATA[Model Context Protocol Servers]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/modelcontextprotocol/servers">modelcontextprotocol/servers</a></h1>
            <p>Model Context Protocol Servers</p>
            <p>Language: Python</p>
            <p>Stars: 53,370</p>
            <p>Forks: 6,075</p>
            <p>Stars today: 255 stars today</p>
            <h2>README</h2><pre># Model Context Protocol servers

This repository is a collection of *reference implementations* for the [Model Context Protocol](https://modelcontextprotocol.io/) (MCP), as well as references
to community built servers and additional resources.

The servers in this repository showcase the versatility and extensibility of MCP, demonstrating how it can be used to give Large Language Models (LLMs) secure, controlled access to tools and data sources.
Each MCP server is implemented with either the [Typescript MCP SDK](https://github.com/modelcontextprotocol/typescript-sdk) or [Python MCP SDK](https://github.com/modelcontextprotocol/python-sdk).

&gt; Note: Lists in this README are maintained in alphabetical order to minimize merge conflicts when adding new items.

## üåü Reference Servers

These servers aim to demonstrate MCP features and the TypeScript and Python SDKs.

- **[Everything](src/everything)** - Reference / test server with prompts, resources, and tools
- **[Fetch](src/fetch)** - Web content fetching and conversion for efficient LLM usage
- **[Filesystem](src/filesystem)** - Secure file operations with configurable access controls
- **[Git](src/git)** - Tools to read, search, and manipulate Git repositories
- **[Memory](src/memory)** - Knowledge graph-based persistent memory system
- **[Sequential Thinking](src/sequentialthinking)** - Dynamic and reflective problem-solving through thought sequences
- **[Time](src/time)** - Time and timezone conversion capabilities

### Archived

The following reference servers are now archived and can be found at [servers-archived](https://github.com/modelcontextprotocol/servers-archived).

- **[AWS KB Retrieval](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/aws-kb-retrieval-server)** - Retrieval from AWS Knowledge Base using Bedrock Agent Runtime
- **[Brave Search](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/brave-search)** - Web and local search using Brave&#039;s Search API
- **[EverArt](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/everart)** - AI image generation using various models
- **[GitHub](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/github)** - Repository management, file operations, and GitHub API integration
- **[GitLab](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/gitlab)** - GitLab API, enabling project management
- **[Google Drive](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/gdrive)** - File access and search capabilities for Google Drive
- **[Google Maps](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/google-maps)** - Location services, directions, and place details
- **[PostgreSQL](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/postgres)** - Read-only database access with schema inspection
- **[Puppeteer](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/puppeteer)** - Browser automation and web scraping
- **[Redis](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/redis)** - Interact with Redis key-value stores
- **[Sentry](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/sentry)** - Retrieving and analyzing issues from Sentry.io
- **[Slack](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/slack)** - Channel management and messaging capabilities
- **[Sqlite](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/sqlite)** - Database interaction and business intelligence capabilities

## ü§ù Third-Party Servers

### üéñÔ∏è Official Integrations

Official integrations are maintained by companies building production ready MCP servers for their platforms.

- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://www.21st.dev/favicon.ico&quot; alt=&quot;21st.dev Logo&quot; /&gt; **[21st.dev Magic](https://github.com/21st-dev/magic-mcp)** - Create crafted UI components inspired by the best 21st.dev design engineers.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://invoxx-public-bucket.s3.eu-central-1.amazonaws.com/frontend-resources/adfin-logo-small.svg&quot; alt=&quot;Adfin Logo&quot; /&gt; **[Adfin](https://github.com/Adfin-Engineering/mcp-server-adfin)** - The only platform you need to get paid - all payments in one place, invoicing and accounting reconciliations with [Adfin](https://www.adfin.com/).
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://www.agentql.com/favicon/favicon.png&quot; alt=&quot;AgentQL Logo&quot; /&gt; **[AgentQL](https://github.com/tinyfish-io/agentql-mcp)** - Enable AI agents to get structured data from unstructured web with [AgentQL](https://www.agentql.com/).
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://agentrpc.com/favicon.ico&quot; alt=&quot;AgentRPC Logo&quot; /&gt; **[AgentRPC](https://github.com/agentrpc/agentrpc)** - Connect to any function, any language, across network boundaries using [AgentRPC](https://www.agentrpc.com/).
- **[Agentset](https://github.com/agentset-ai/mcp-server)** - RAG for your knowledge base connected to [Agentset](https://agentset.ai).
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://aiven.io/favicon.ico&quot; alt=&quot;Aiven Logo&quot; /&gt; **[Aiven](https://github.com/Aiven-Open/mcp-aiven)** - Navigate your [Aiven projects](https://go.aiven.io/mcp-server) and interact with the PostgreSQL¬Æ, Apache Kafka¬Æ, ClickHouse¬Æ and OpenSearch¬Æ services
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://www.alation.com/resource-center/download/7p3vnbbznfiw/34FMtBTex5ppvs2hNYa9Fc/c877c37e88e5339878658697c46d2d58/Alation-Logo-Bug-Primary.svg&quot; alt=&quot;Alation Logo&quot; /&gt; **[Alation](https://github.com/Alation/alation-ai-agent-sdk)** - Unlock the power of the enterprise Data Catalog by harnessing tools provided by the Alation MCP server.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://www.algolia.com/files/live/sites/algolia-assets/files/icons/algolia-logo-for-favicon.svg&quot; alt=&quot;Algolia Logo&quot; /&gt; **[Algolia MCP](https://github.com/algolia/mcp-node)** Algolia MCP Server exposes a natural language interface to query, inspect, and manage Algolia indices and configs. Useful for monitoring, debugging and optimizing search performance within your agentic workflows. See [demo](https://www.youtube.com/watch?v=UgCOLcDI9Lg).
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://img.alicdn.com/imgextra/i4/O1CN01epkXwH1WLAXkZfV6N_!!6000000002771-2-tps-200-200.png&quot; alt=&quot;Alibaba Cloud AnalyticDB for MySQL Logo&quot; /&gt; **[Alibaba Cloud AnalyticDB for MySQL](https://github.com/aliyun/alibabacloud-adb-mysql-mcp-server)** - Connect to a [AnalyticDB for MySQL](https://www.alibabacloud.com/en/product/analyticdb-for-mysql) cluster for getting database or table metadata, querying and analyzing data.It will be supported to add the openapi for cluster operation in the future.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://github.com/aliyun/alibabacloud-adbpg-mcp-server/blob/master/images/AnalyticDB.png&quot; alt=&quot;Alibaba Cloud AnalyticDB for PostgreSQL Logo&quot; /&gt; **[Alibaba Cloud AnalyticDB for PostgreSQL](https://github.com/aliyun/alibabacloud-adbpg-mcp-server)** - An MCP server to connect to [AnalyticDB for PostgreSQL](https://github.com/aliyun/alibabacloud-adbpg-mcp-server) instances, query and analyze data.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://img.alicdn.com/imgextra/i3/O1CN0101UWWF1UYn3rAe3HU_!!6000000002530-2-tps-32-32.png&quot; alt=&quot;DataWorks Logo&quot; /&gt; **[Alibaba Cloud DataWorks](https://github.com/aliyun/alibabacloud-dataworks-mcp-server)** - A Model Context Protocol (MCP) server that provides tools for AI, allowing it to interact with the [DataWorks](https://www.alibabacloud.com/help/en/dataworks/) Open API through a standardized interface. This implementation is based on the Alibaba Cloud Open API and enables AI agents to perform cloud resources operations seamlessly.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://opensearch-shanghai.oss-cn-shanghai.aliyuncs.com/ouhuang/aliyun-icon.png&quot; alt=&quot;Alibaba Cloud OpenSearch Logo&quot; /&gt; **[Alibaba Cloud OpenSearch](https://github.com/aliyun/alibabacloud-opensearch-mcp-server)** - This MCP server equips AI Agents with tools to interact with [OpenSearch](https://help.aliyun.com/zh/open-search/?spm=5176.7946605.J_5253785160.6.28098651AaYZXC) through a standardized and extensible interface.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://github.com/aliyun/alibaba-cloud-ops-mcp-server/blob/master/image/alibaba-cloud.png&quot; alt=&quot;Alibaba Cloud OPS Logo&quot; /&gt; **[Alibaba Cloud OPS](https://github.com/aliyun/alibaba-cloud-ops-mcp-server)** - Manage the lifecycle of your Alibaba Cloud resources with [CloudOps Orchestration Service](https://www.alibabacloud.com/en/product/oos) and Alibaba Cloud OpenAPI.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://github.com/aliyun/alibabacloud-rds-openapi-mcp-server/blob/main/assets/alibabacloudrds.png&quot; alt=&quot;Alibaba Cloud RDS MySQL Logo&quot; /&gt; **[Alibaba Cloud RDS](https://github.com/aliyun/alibabacloud-rds-openapi-mcp-server)** - An MCP server designed to interact with the Alibaba Cloud RDS OpenAPI, enabling programmatic management of RDS resources via an LLM.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://www.alphavantage.co/logo.png/&quot; alt=&quot;AlphaVantage Logo&quot; /&gt; **[AlphaVantage](https://github.com/calvernaz/alphavantage)** - Connect to 100+ APIs for financial market data, including stock prices, fundamentals, and more from [AlphaVantage](https://www.alphavantage.co)
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://doris.apache.org/images/favicon.ico&quot; alt=&quot;Apache Doris Logo&quot; /&gt; **[Apache Doris](https://github.com/apache/doris-mcp-server)** - MCP Server For [Apache Doris](https://doris.apache.org/), an MPP-based real-time data warehouse.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://iotdb.apache.org/img/logo.svg&quot; alt=&quot;Apache IoTDB Logo&quot; /&gt; **[Apache IoTDB](https://github.com/apache/iotdb-mcp-server)** - MCP Server for [Apache IoTDB](https://github.com/apache/iotdb) database and its tools
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://apify.com/favicon.ico&quot; alt=&quot;Apify Logo&quot; /&gt; **[Apify](https://github.com/apify/actors-mcp-server)** - [Actors MCP Server](https://apify.com/apify/actors-mcp-server): Use 3,000+ pre-built cloud tools to extract data from websites, e-commerce, social media, search engines, maps, and more
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://2052727.fs1.hubspotusercontent-na1.net/hubfs/2052727/cropped-cropped-apimaticio-favicon-1-32x32.png&quot; alt=&quot;APIMatic Logo&quot; /&gt; **[APIMatic MCP](https://github.com/apimatic/apimatic-validator-mcp)** - APIMatic MCP Server is used to validate OpenAPI specifications using [APIMatic](https://www.apimatic.io/). The server processes OpenAPI files and returns validation summaries by leveraging APIMatic&#039;s API.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://apollo-server-landing-page.cdn.apollographql.com/_latest/assets/favicon.png&quot; alt=&quot;Apollo Graph Logo&quot; /&gt; **[Apollo MCP Server](https://github.com/apollographql/apollo-mcp-server/)** - Connect your GraphQL APIs to AI agents
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://phoenix.arize.com/wp-content/uploads/2023/04/cropped-Favicon-32x32.png&quot; alt=&quot;Arize-Phoenix Logo&quot; /&gt; **[Arize Phoenix](https://github.com/Arize-ai/phoenix/tree/main/js/packages/phoenix-mcp)** - Inspect traces, manage prompts, curate datasets, and run experiments using [Arize Phoenix](https://github.com/Arize-ai/phoenix), an open-source AI and LLM observability tool.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://731523176-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2FaVUBXRZbpAgtjYf5HsvO%2Fuploads%2FaRRrVVocXCTr6GkepfCx%2Flogo_color.svg?alt=media&amp;token=3ba24089-0ab2-421f-a9d9-41f2f94f954a&quot; alt=&quot;Armor Logo&quot; /&gt; **[Armor Crypto MCP](https://github.com/armorwallet/armor-crypto-mcp)** - MCP to interface with multiple blockchains, staking, DeFi, swap, bridging, wallet management, DCA, Limit Orders, Coin Lookup, Tracking and more.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://console.asgardeo.io/app/libs/themes/wso2is/assets/images/branding/favicon.ico&quot; alt=&quot;Asgardeo Logo&quot; /&gt; **[Asgardeo](https://github.com/asgardeo/asgardeo-mcp-server)** - MCP server to interact with your [Asgardeo](https://wso2.com/asgardeo) organization through LLM tools.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://www.datastax.com/favicon-32x32.png&quot; alt=&quot;DataStax logo&quot; /&gt; **[Astra DB](https://github.com/datastax/astra-db-mcp)** - Comprehensive tools for managing collections and documents in a [DataStax Astra DB](https://www.datastax.com/products/datastax-astra) NoSQL database with a full range of operations such as create, update, delete, find, and associated bulk actions.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://assets.atlan.com/assets/atlan-a-logo-blue-background.png&quot; alt=&quot;Atlan Logo&quot; /&gt; **[Atlan](https://github.com/atlanhq/agent-toolkit/tree/main/modelcontextprotocol)** - The Atlan Model Context Protocol server allows you to interact with the [Atlan](https://www.atlan.com/) services through multiple tools.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://resources.audiense.com/hubfs/favicon-1.png&quot; alt=&quot;Audiense Logo&quot; /&gt; **[Audiense Insights](https://github.com/AudienseCo/mcp-audiense-insights)** - Marketing insights and audience analysis from [Audiense](https://www.audiense.com/products/audiense-insights) reports, covering demographic, cultural, influencer, and content engagement analysis.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://cdn.auth0.com/website/website/favicons/auth0-favicon.svg&quot; alt=&quot;Auth0 Logo&quot; /&gt; **[Auth0](https://github.com/auth0/auth0-mcp-server)** - MCP server for interacting with your Auth0 tenant, supporting creating and modifying actions, applications, forms, logs, resource servers, and more.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://firstorder.ai/favicon_auth.ico&quot; alt=&quot;Authenticator App Logo&quot; /&gt; **[Authenticator App ¬∑ 2FA](https://github.com/firstorderai/authenticator_mcp)** - A secure MCP (Model Context Protocol) server that enables AI agents to interact with the Authenticator App.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://a0.awsstatic.com/libra-css/images/site/fav/favicon.ico&quot; alt=&quot;AWS Logo&quot; /&gt; **[AWS](https://github.com/awslabs/mcp)** -  Specialized MCP servers that bring AWS best practices directly to your development workflow.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://axiom.co/favicon.ico&quot; alt=&quot;Axiom Logo&quot; /&gt; **[Axiom](https://github.com/axiomhq/mcp-server-axiom)** - Query and analyze your Axiom logs, traces, and all other event data in natural language
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/acom_social_icon_azure&quot; alt=&quot;Microsoft Azure Logo&quot; /&gt; **[Azure](https://github.com/Azure/azure-mcp)** - The Azure MCP Server gives MCP Clients access to key Azure services and tools like Azure Storage, Cosmos DB, the Azure CLI, and more.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://www.bankless.com/favicon.ico&quot; alt=&quot;Bankless Logo&quot; /&gt; **[Bankless Onchain](https://github.com/bankless/onchain-mcp)** - Query Onchain data, like ERC20 tokens, transaction history, smart contract state.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://bicscan.io/favicon.png&quot; alt=&quot;BICScan Logo&quot; /&gt; **[BICScan](https://github.com/ahnlabio/bicscan-mcp)** - Risk score / asset holdings of EVM blockchain address (EOA, CA, ENS) and even domain names.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://web-cdn.bitrise.io/favicon.ico&quot; alt=&quot;Bitrise Logo&quot; /&gt; **[Bitrise](https://github.com/bitrise-io/bitrise-mcp)** - Chat with your builds, CI, and [more](https://bitrise.io/blog/post/chat-with-your-builds-ci-and-more-introducing-the-bitrise-mcp-server).
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://boldsign.com/favicon.ico&quot; alt=&quot;BoldSign Logo&quot; /&gt; **[BoldSign](https://github.com/boldsign/boldsign-mcp)** - Search, request, and manage e-signature contracts effortlessly with [BoldSign](https://boldsign.com/).
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://boost.space/favicon.ico&quot; alt=&quot;Boost.space Logo&quot; /&gt; **[Boost.space](https://github.com/boostspace/boostspace-mcp-server)** - An MCP server integrating with [Boost.space](https://boost.space) for centralized, automated business data from 2000+ sources.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://www.box.com/favicon.ico&quot; alt=&quot;Box Logo&quot; /&gt; **[Box](https://github.com/box-community/mcp-server-box)** - Interact with the Intelligent Content Management platform through Box AI.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://browserbase.com/favicon.ico&quot; alt=&quot;Browserbase Logo&quot; /&gt; **[Browserbase](https://github.com/browserbase/mcp-server-browserbase)** - Automate browser interactions in the cloud (e.g. web navigation, data extraction, form filling, and more)
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://browserstack.wpenginepowered.com/wp-content/themes/browserstack/img/favicons/favicon.ico&quot; alt=&quot;BrowserStack Logo&quot; /&gt; **[BrowserStack](https://github.com/browserstack/mcp-server)** - Access BrowserStack&#039;s [Test Platform](https://www.browserstack.com/test-platform) to debug, write and fix tests, do accessibility testing and more.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://builtwith.com/favicon.ico&quot; alt=&quot;BuiltWith Logo&quot; /&gt; **[BuiltWith](https://github.com/builtwith/mcp)** - Identify the technology stack behind any website.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://portswigger.net/favicon.ico&quot; alt=&quot;PortSwigger Logo&quot; /&gt; **[Burp Suite](https://github.com/PortSwigger/mcp-server)** - MCP Server extension allowing AI clients to connect to [Burp Suite](https://portswigger.net)
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://campertunity.com/assets/icon/favicon.ico&quot; alt=&quot;Campertunity Logo&quot; /&gt; **[Campertunity](https://github.com/campertunity/mcp-server)** - Search campgrounds around the world on campertunity, check availability, and provide booking links.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://play.cartesia.ai/icon.png&quot; alt=&quot;Cartesia logo&quot; /&gt; **[Cartesia](https://github.com/cartesia-ai/cartesia-mcp)** - Connect to the [Cartesia](https://cartesia.ai/) voice platform to perform text-to-speech, voice cloning etc. 
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://www.cashfree.com/favicon.ico&quot; alt=&quot;Cashfree logo&quot; /&gt; **[Cashfree](https://github.com/cashfree/cashfree-mcp)** - [Cashfree Payments](https://www.cashfree.com/) official MCP server.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://www.chargebee.com/static/resources/brand/favicon.png&quot; alt=&quot;Chargebee Logo&quot; /&gt; **[Chargebee](https://github.com/chargebee/agentkit/tree/main/modelcontextprotocol)** - MCP Server that connects AI agents to [Chargebee platform](https://www.chargebee.com).
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://cheqd.io/wp-content/uploads/2023/03/logo_cheqd_favicon.png&quot; alt=&quot;Cheqd Logo&quot; /&gt; **[Cheqd](https://github.com/cheqd/mcp-toolkit)** - Enable AI Agents to be trusted, verified, prevent fraud, protect your reputation, and more through [cheqd&#039;s](https://cheqd.io) Trust Registries and Credentials.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://cdn.chiki.studio/brand/logo.png&quot; alt=&quot;Chiki StudIO Logo&quot; /&gt; **[Chiki StudIO](https://chiki.studio/galimybes/mcp/)** - Create your own configurable MCP servers purely via configuration (no code), with instructions, prompts, and tools support.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://trychroma.com/_next/static/media/chroma-logo.ae2d6e4b.svg&quot; alt=&quot;Chroma Logo&quot; /&gt; **[Chroma](https://github.com/chroma-core/chroma-mcp)** - Embeddings, vector search, document storage, and full-text search with the open-source AI application database
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://www.chronulus.com/favicon/chronulus-logo-blue-on-alpha-square-128x128.ico&quot; alt=&quot;Chronulus AI Logo&quot; /&gt; **[Chronulus AI](https://github.com/ChronulusAI/chronulus-mcp)** - Predict anything with Chronulus AI forecasting and prediction agents.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://circleci.com/favicon.ico&quot; alt=&quot;CircleCI Logo&quot; /&gt; **[CircleCI](https://github.com/CircleCI-Public/mcp-server-circleci)** - Enable AI Agents to fix build failures from CircleCI.
- &lt;img height=&quot;12&quot; width=&quot;12&quot; src=&quot;https://clickhouse.com/favicon.ico&quot; alt=&quot;ClickHouse Logo&quot; /&gt; **[ClickHouse](https://github.com/ClickHouse/mcp-clickhouse)** - Query your [ClickHouse](https://clickhouse.com/) database server.
- &lt;img

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jwohlwend/boltz]]></title>
            <link>https://github.com/jwohlwend/boltz</link>
            <guid>https://github.com/jwohlwend/boltz</guid>
            <pubDate>Fri, 13 Jun 2025 00:04:29 GMT</pubDate>
            <description><![CDATA[Official repository for the Boltz biomolecular interaction models]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jwohlwend/boltz">jwohlwend/boltz</a></h1>
            <p>Official repository for the Boltz biomolecular interaction models</p>
            <p>Language: Python</p>
            <p>Stars: 2,745</p>
            <p>Forks: 407</p>
            <p>Stars today: 94 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;div&gt;&amp;nbsp;&lt;/div&gt;
  &lt;img src=&quot;docs/boltz2_title.png&quot; width=&quot;300&quot;/&gt;
  &lt;img src=&quot;https://model-gateway.boltz.bio/a.png?x-pxid=bce1627f-f326-4bff-8a97-45c6c3bc929d&quot; /&gt;

[Boltz-1](https://doi.org/10.1101/2024.11.19.624167) | [Boltz-2](https://bit.ly/boltz2-pdf) |
[Slack](https://join.slack.com/t/boltz-community/shared_invite/zt-3751cpmn6-kDLgLcQFMOPeUdFIJd4oqQ) &lt;br&gt; &lt;br&gt;
&lt;/div&gt;



![](docs/boltz1_pred_figure.png)


## Introduction

Boltz is a family of models for biomolecular interaction prediction. Boltz-1 was the first fully open source model to approach AlphaFold3 accuracy. Our latest work Boltz-2 is a new biomolecular foundation model that goes beyond AlphaFold3 and Boltz-1 by jointly modeling complex structures and binding affinities, a critical component towards accurate molecular design. Boltz-2 is the first deep learning model to approach the accuracy of physics-based free-energy perturbation (FEP) methods, while running 1000x faster ‚Äî making accurate in silico screening practical for early-stage drug discovery.

All the code and weights are provided under MIT license, making them freely available for both academic and commercial uses. For more information about the model, see the [Boltz-1](https://doi.org/10.1101/2024.11.19.624167) and [Boltz-2](https://bit.ly/boltz2-pdf) technical reports. To discuss updates, tools and applications join our [Slack channel](https://join.slack.com/t/boltz-community/shared_invite/zt-34qg8uink-V1LGdRRUf3avAUVaRvv93w).

## Installation

&gt; Note: we recommend installing boltz in a fresh python environment

Install boltz with PyPI (recommended):

```
pip install boltz -U
```

or directly from GitHub for daily updates:

```
git clone https://github.com/jwohlwend/boltz.git
cd boltz; pip install -e .
```

## Inference

You can run inference using Boltz with:

```
boltz predict input_path --use_msa_server
```

`input_path` should point to a YAML file, or a directory of YAML files for batched processing, describing the biomolecules you want to model and the properties you want to predict (e.g. affinity). To see all available options: `boltz predict --help` and for more information on these input formats, see our [prediction instructions](docs/prediction.md). By default, the `boltz` command will run the latest version of the model.

### Binding Affinity Prediction
There are two main predictions in the affinity output: `affinity_pred_value` and `affinity_probability_binary`. They are trained on largely different datasets, with different supervisions, and should be used in different contexts. The `affinity_probability_binary` field should be used to detect binders from decoys, for example in a hit-discovery stage. It&#039;s value ranges from 0 to 1 and represents the predicted probability that the ligand is a binder. The `affinity_pred_value` aims to measure the specific affinity of different binders and how this changes with small modifications of the molecule. This should be used in ligand optimization stages such as hit-to-lead and lead-optimization. It reports a binding affinity value as `log(IC50)`, derived from an `IC50` measured in `ŒºM`. More details on how to run affinity predictions and parse the output can be found in our [prediction instructions](docs/prediction.md).


## Evaluation

‚ö†Ô∏è **Coming soon: updated evaluation code for Boltz-2!**

To encourage reproducibility and facilitate comparison with other models, on top of the existing Boltz-1 evaluation pipeline, we will soon provide the evaluation scripts and structural predictions for Boltz-2, Boltz-1, Chai-1 and AlphaFold3 on our test benchmark dataset, and our affinity predictions on the FEP+ benchamark, CASP16 and our MF-PCBA test set.

![Affinity test sets evaluations](docs/pearson_plot.png)
![Test set evaluations](docs/plot_test_boltz2.png)


## Training

‚ö†Ô∏è **Coming soon: updated training code for Boltz-2!**

If you&#039;re interested in retraining the model, currently for Boltz-1 but soon for Boltz-2, see our [training instructions](docs/training.md).


## Contributing

We welcome external contributions and are eager to engage with the community. Connect with us on our [Slack channel](https://join.slack.com/t/boltz-community/shared_invite/zt-34qg8uink-V1LGdRRUf3avAUVaRvv93w) to discuss advancements, share insights, and foster collaboration around Boltz-2.

Boltz also runs on Tenstorrent hardware thanks to a [fork](https://github.com/moritztng/tt-boltz) by Moritz Th√ºning.

## License

Our model and code are released under MIT License, and can be freely used for both academic and commercial purposes.


## Cite

If you use this code or the models in your research, please cite the following papers:

```bibtex
@article{passaro2025boltz2,
  author = {Passaro, Saro and Corso, Gabriele and Wohlwend, Jeremy and Reveiz, Mateo and Thaler, Stephan and Somnath, Vignesh Ram and Getz, Noah and Portnoi, Tally and Roy, Julien and Stark, Hannes and Kwabi-Addo, David and Beaini, Dominique and Jaakkola, Tommi and Barzilay, Regina},
  title = {Boltz-2: Towards Accurate and Efficient Binding Affinity Prediction},
  year = {2025},
  doi = {},
  journal = {}
}

@article{wohlwend2024boltz1,
  author = {Wohlwend, Jeremy and Corso, Gabriele and Passaro, Saro and Getz, Noah and Reveiz, Mateo and Leidal, Ken and Swiderski, Wojtek and Atkinson, Liam and Portnoi, Tally and Chinn, Itamar and Silterra, Jacob and Jaakkola, Tommi and Barzilay, Regina},
  title = {Boltz-1: Democratizing Biomolecular Interaction Modeling},
  year = {2024},
  doi = {10.1101/2024.11.19.624167},
  journal = {bioRxiv}
}
```

In addition if you use the automatic MSA generation, please cite:

```bibtex
@article{mirdita2022colabfold,
  title={ColabFold: making protein folding accessible to all},
  author={Mirdita, Milot and Sch{\&quot;u}tze, Konstantin and Moriwaki, Yoshitaka and Heo, Lim and Ovchinnikov, Sergey and Steinegger, Martin},
  journal={Nature methods},
  year={2022},
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[RockChinQ/LangBot]]></title>
            <link>https://github.com/RockChinQ/LangBot</link>
            <guid>https://github.com/RockChinQ/LangBot</guid>
            <pubDate>Fri, 13 Jun 2025 00:04:28 GMT</pubDate>
            <description><![CDATA[ü§© Easy-to-use global IM bot platform designed for the LLM era / ÁÆÄÂçïÊòìÁî®ÁöÑÂ§ßÊ®°ÂûãÂç≥Êó∂ÈÄö‰ø°Êú∫Âô®‰∫∫Âπ≥Âè∞ ‚ö°Ô∏è Bots for QQ / Discord / WeChatÔºà‰ºÅ‰∏öÂæÆ‰ø°„ÄÅ‰∏™‰∫∫ÂæÆ‰ø°Ôºâ/ Telegram / È£û‰π¶ / ÈíâÈíâ / Slack üß© Integrated with ChatGPT„ÄÅDeepSeek„ÄÅDify„ÄÅn8n„ÄÅClaude„ÄÅGoogle Gemini„ÄÅxAI„ÄÅPPIO„ÄÅOllama„ÄÅÈòøÈáå‰∫ëÁôæÁÇº„ÄÅSiliconFlow„ÄÅQwen„ÄÅMoonshot„ÄÅSillyTraven„ÄÅMCP„ÄÅWeClone etc. LLM & Agent]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/RockChinQ/LangBot">RockChinQ/LangBot</a></h1>
            <p>ü§© Easy-to-use global IM bot platform designed for the LLM era / ÁÆÄÂçïÊòìÁî®ÁöÑÂ§ßÊ®°ÂûãÂç≥Êó∂ÈÄö‰ø°Êú∫Âô®‰∫∫Âπ≥Âè∞ ‚ö°Ô∏è Bots for QQ / Discord / WeChatÔºà‰ºÅ‰∏öÂæÆ‰ø°„ÄÅ‰∏™‰∫∫ÂæÆ‰ø°Ôºâ/ Telegram / È£û‰π¶ / ÈíâÈíâ / Slack üß© Integrated with ChatGPT„ÄÅDeepSeek„ÄÅDify„ÄÅn8n„ÄÅClaude„ÄÅGoogle Gemini„ÄÅxAI„ÄÅPPIO„ÄÅOllama„ÄÅÈòøÈáå‰∫ëÁôæÁÇº„ÄÅSiliconFlow„ÄÅQwen„ÄÅMoonshot„ÄÅSillyTraven„ÄÅMCP„ÄÅWeClone etc. LLM & Agent</p>
            <p>Language: Python</p>
            <p>Stars: 11,844</p>
            <p>Forks: 900</p>
            <p>Stars today: 57 stars today</p>
            <h2>README</h2><pre>
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://langbot.app&quot;&gt;
&lt;img src=&quot;https://docs.langbot.app/social.png&quot; alt=&quot;LangBot&quot;/&gt;
&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://trendshift.io/repositories/12901&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12901&quot; alt=&quot;RockChinQ%2FLangBot | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;a href=&quot;https://langbot.app&quot;&gt;È°πÁõÆ‰∏ªÈ°µ&lt;/a&gt; ÔΩú
&lt;a href=&quot;https://docs.langbot.app/zh/insight/guide.html&quot;&gt;ÈÉ®ÁΩ≤ÊñáÊ°£&lt;/a&gt; ÔΩú
&lt;a href=&quot;https://docs.langbot.app/zh/plugin/plugin-intro.html&quot;&gt;Êèí‰ª∂‰ªãÁªç&lt;/a&gt; ÔΩú
&lt;a href=&quot;https://github.com/RockChinQ/LangBot/issues/new?assignees=&amp;labels=%E7%8B%AC%E7%AB%8B%E6%8F%92%E4%BB%B6&amp;projects=&amp;template=submit-plugin.yml&amp;title=%5BPlugin%5D%3A+%E8%AF%B7%E6%B1%82%E7%99%BB%E8%AE%B0%E6%96%B0%E6%8F%92%E4%BB%B6&quot;&gt;Êèê‰∫§Êèí‰ª∂&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
üòéÈ´òÁ®≥ÂÆö„ÄÅüß©ÊîØÊåÅÊâ©Â±ï„ÄÅü¶ÑÂ§öÊ®°ÊÄÅ - Â§ßÊ®°ÂûãÂéüÁîüÂç≥Êó∂ÈÄö‰ø°Êú∫Âô®‰∫∫Âπ≥Âè∞ü§ñ  
&lt;/div&gt;

&lt;br/&gt;

[![Discord](https://img.shields.io/discord/1335141740050649118?logo=discord&amp;labelColor=%20%235462eb&amp;logoColor=%20%23f5f5f5&amp;color=%20%235462eb)](https://discord.gg/wdNEHETs87)
[![QQ Group](https://img.shields.io/badge/%E7%A4%BE%E5%8C%BAQQ%E7%BE%A4-966235608-blue)](https://qm.qq.com/q/JLi38whHum)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/RockChinQ/LangBot)
[![GitHub release (latest by date)](https://img.shields.io/github/v/release/RockChinQ/LangBot)](https://github.com/RockChinQ/LangBot/releases/latest)
&lt;img src=&quot;https://img.shields.io/badge/python-3.10 ~ 3.13 -blue.svg&quot; alt=&quot;python&quot;&gt;
[![star](https://gitcode.com/RockChinQ/LangBot/star/badge.svg)](https://gitcode.com/RockChinQ/LangBot)

[ÁÆÄ‰Ωì‰∏≠Êñá](README.md) / [English](README_EN.md) / [Êó•Êú¨Ë™û](README_JP.md) / (PR for your language)

&lt;/div&gt;

&lt;/p&gt;

&gt; ËøëÊúü GeWeChat È°πÁõÆÂΩíÊ°£ÔºåÊàë‰ª¨Â∑≤ÁªèÈÄÇÈÖç WeChatPad ÂçèËÆÆÁ´ØÔºå‰∏™ÂæÆÊÅ¢Â§çÊ≠£Â∏∏‰ΩøÁî®ÔºåËØ¶ÊÉÖËØ∑Êü•ÁúãÊñáÊ°£„ÄÇ

## ‚ú® ÁâπÊÄß

- üí¨ Â§ßÊ®°ÂûãÂØπËØù„ÄÅAgentÔºöÊîØÊåÅÂ§öÁßçÂ§ßÊ®°ÂûãÔºåÈÄÇÈÖçÁæ§ËÅäÂíåÁßÅËÅäÔºõÂÖ∑ÊúâÂ§öËΩÆÂØπËØù„ÄÅÂ∑•ÂÖ∑Ë∞ÉÁî®„ÄÅÂ§öÊ®°ÊÄÅËÉΩÂäõÔºåÂπ∂Ê∑±Â∫¶ÈÄÇÈÖç [Dify](https://dify.ai)„ÄÇÁõÆÂâçÊîØÊåÅ QQ„ÄÅQQÈ¢ëÈÅì„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°„ÄÅ‰∏™‰∫∫ÂæÆ‰ø°„ÄÅÈ£û‰π¶„ÄÅDiscord„ÄÅTelegram Á≠âÂπ≥Âè∞„ÄÇ
- üõ†Ô∏è È´òÁ®≥ÂÆöÊÄß„ÄÅÂäüËÉΩÂÆåÂ§áÔºöÂéüÁîüÊîØÊåÅËÆøÈóÆÊéßÂà∂„ÄÅÈôêÈÄü„ÄÅÊïèÊÑüËØçËøáÊª§Á≠âÊú∫Âà∂ÔºõÈÖçÁΩÆÁÆÄÂçïÔºåÊîØÊåÅÂ§öÁßçÈÉ®ÁΩ≤ÊñπÂºè„ÄÇÊîØÊåÅÂ§öÊµÅÊ∞¥Á∫øÈÖçÁΩÆÔºå‰∏çÂêåÊú∫Âô®‰∫∫Áî®‰∫é‰∏çÂêåÂ∫îÁî®Âú∫ÊôØ„ÄÇ
- üß© Êèí‰ª∂Êâ©Â±ï„ÄÅÊ¥ªË∑ÉÁ§æÂå∫ÔºöÊîØÊåÅ‰∫ã‰ª∂È©±Âä®„ÄÅÁªÑ‰ª∂Êâ©Â±ïÁ≠âÊèí‰ª∂Êú∫Âà∂ÔºõÈÄÇÈÖç Anthropic [MCP ÂçèËÆÆ](https://modelcontextprotocol.io/)ÔºõÁõÆÂâçÂ∑≤ÊúâÊï∞Áôæ‰∏™Êèí‰ª∂„ÄÇ
- üòª Web ÁÆ°ÁêÜÈù¢ÊùøÔºöÊîØÊåÅÈÄöËøáÊµèËßàÂô®ÁÆ°ÁêÜ LangBot ÂÆû‰æãÔºå‰∏çÂÜçÈúÄË¶ÅÊâãÂä®ÁºñÂÜôÈÖçÁΩÆÊñá‰ª∂„ÄÇ

## üì¶ ÂºÄÂßã‰ΩøÁî®

#### Docker Compose ÈÉ®ÁΩ≤

```bash
git clone https://github.com/RockChinQ/LangBot
cd LangBot
docker compose up -d
```

ËÆøÈóÆ http://localhost:5300 Âç≥ÂèØÂºÄÂßã‰ΩøÁî®„ÄÇ

ËØ¶ÁªÜÊñáÊ°£[Docker ÈÉ®ÁΩ≤](https://docs.langbot.app/zh/deploy/langbot/docker.html)„ÄÇ

#### ÂÆùÂ°îÈù¢ÊùøÈÉ®ÁΩ≤

Â∑≤‰∏äÊû∂ÂÆùÂ°îÈù¢ÊùøÔºåËã•ÊÇ®Â∑≤ÂÆâË£ÖÂÆùÂ°îÈù¢ÊùøÔºåÂèØ‰ª•Ê†πÊçÆ[ÊñáÊ°£](https://docs.langbot.app/zh/deploy/langbot/one-click/bt.html)‰ΩøÁî®„ÄÇ

#### Zeabur ‰∫ëÈÉ®ÁΩ≤

Á§æÂå∫Ë¥°ÁåÆÁöÑ Zeabur Ê®°Êùø„ÄÇ

[![Deploy on Zeabur](https://zeabur.com/button.svg)](https://zeabur.com/zh-CN/templates/ZKTBDH)

#### Railway ‰∫ëÈÉ®ÁΩ≤

[![Deploy on Railway](https://railway.com/button.svg)](https://railway.app/template/yRrAyL?referralCode=vogKPF)

#### ÊâãÂä®ÈÉ®ÁΩ≤

Áõ¥Êé•‰ΩøÁî®ÂèëË°åÁâàËøêË°åÔºåÊü•ÁúãÊñáÊ°£[ÊâãÂä®ÈÉ®ÁΩ≤](https://docs.langbot.app/zh/deploy/langbot/manual.html)„ÄÇ

## üì∏ ÊïàÊûúÂ±ïÁ§∫

&lt;img alt=&quot;bots&quot; src=&quot;https://docs.langbot.app/webui/bot-page.png&quot; width=&quot;450px&quot;/&gt;

&lt;img alt=&quot;bots&quot; src=&quot;https://docs.langbot.app/webui/create-model.png&quot; width=&quot;450px&quot;/&gt;

&lt;img alt=&quot;bots&quot; src=&quot;https://docs.langbot.app/webui/edit-pipeline.png&quot; width=&quot;450px&quot;/&gt;

&lt;img alt=&quot;bots&quot; src=&quot;https://docs.langbot.app/webui/plugin-market.png&quot; width=&quot;450px&quot;/&gt;

&lt;img alt=&quot;ÂõûÂ§çÊïàÊûúÔºàÂ∏¶ÊúâËÅîÁΩëÊèí‰ª∂Ôºâ&quot; src=&quot;https://docs.langbot.app/QChatGPT-0516.png&quot; width=&quot;500px&quot;/&gt;

- WebUI Demo: https://demo.langbot.dev/
    - ÁôªÂΩï‰ø°ÊÅØÔºöÈÇÆÁÆ±Ôºö`demo@langbot.app` ÂØÜÁ†ÅÔºö`langbot123456`
    - Ê≥®ÊÑèÔºö‰ªÖÂ±ïÁ§∫webuiÊïàÊûúÔºåÂÖ¨ÂºÄÁéØÂ¢ÉÔºåËØ∑‰∏çË¶ÅÂú®ÂÖ∂‰∏≠Â°´ÂÖ•ÊÇ®ÁöÑ‰ªª‰ΩïÊïèÊÑü‰ø°ÊÅØ„ÄÇ

## üîå ÁªÑ‰ª∂ÂÖºÂÆπÊÄß

### Ê∂àÊÅØÂπ≥Âè∞

| Âπ≥Âè∞ | Áä∂ÊÄÅ | Â§áÊ≥® |
| --- | --- | --- |
| QQ ‰∏™‰∫∫Âè∑ | ‚úÖ | QQ ‰∏™‰∫∫Âè∑ÁßÅËÅä„ÄÅÁæ§ËÅä |
| QQ ÂÆòÊñπÊú∫Âô®‰∫∫ | ‚úÖ | QQ ÂÆòÊñπÊú∫Âô®‰∫∫ÔºåÊîØÊåÅÈ¢ëÈÅì„ÄÅÁßÅËÅä„ÄÅÁæ§ËÅä |
| ‰ºÅ‰∏öÂæÆ‰ø° | ‚úÖ |  |
| ‰ºÅÂæÆÂØπÂ§ñÂÆ¢Êúç | ‚úÖ |  |
| ‰∏™‰∫∫ÂæÆ‰ø° | ‚úÖ |  |
| ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ | ‚úÖ |  |
| È£û‰π¶ | ‚úÖ |  |
| ÈíâÈíâ | ‚úÖ |  |
| Discord | ‚úÖ |  |
| Telegram | ‚úÖ |  |
| Slack | ‚úÖ |  |
| LINE | üöß |  |
| WhatsApp | üöß |  |

üöß: Ê≠£Âú®ÂºÄÂèë‰∏≠

### Â§ßÊ®°ÂûãËÉΩÂäõ

| Ê®°Âûã | Áä∂ÊÄÅ | Â§áÊ≥® |
| --- | --- | --- |
| [OpenAI](https://platform.openai.com/) | ‚úÖ | ÂèØÊé•ÂÖ•‰ªª‰Ωï OpenAI Êé•Âè£Ê†ºÂºèÊ®°Âûã |
| [DeepSeek](https://www.deepseek.com/) | ‚úÖ |  |
| [Moonshot](https://www.moonshot.cn/) | ‚úÖ |  |
| [Anthropic](https://www.anthropic.com/) | ‚úÖ |  |
| [xAI](https://x.ai/) | ‚úÖ |  |
| [Êô∫Ë∞±AI](https://open.bigmodel.cn/) | ‚úÖ |  |
| [PPIO](https://ppinfra.com/user/register?invited_by=QJKFYD&amp;utm_source=github_langbot) | ‚úÖ | Â§ßÊ®°ÂûãÂíå GPU ËµÑÊ∫êÂπ≥Âè∞ |
| [Google Gemini](https://aistudio.google.com/prompts/new_chat) | ‚úÖ | |
| [Dify](https://dify.ai) | ‚úÖ | LLMOps Âπ≥Âè∞ |
| [Ollama](https://ollama.com/) | ‚úÖ | Êú¨Âú∞Â§ßÊ®°ÂûãËøêË°åÂπ≥Âè∞ |
| [LMStudio](https://lmstudio.ai/) | ‚úÖ | Êú¨Âú∞Â§ßÊ®°ÂûãËøêË°åÂπ≥Âè∞ |
| [GiteeAI](https://ai.gitee.com/) | ‚úÖ | Â§ßÊ®°ÂûãÊé•Âè£ËÅöÂêàÂπ≥Âè∞ |
| [SiliconFlow](https://siliconflow.cn/) | ‚úÖ | Â§ßÊ®°ÂûãËÅöÂêàÂπ≥Âè∞ |
| [ÈòøÈáå‰∫ëÁôæÁÇº](https://bailian.console.aliyun.com/) | ‚úÖ | Â§ßÊ®°ÂûãËÅöÂêàÂπ≥Âè∞, LLMOps Âπ≥Âè∞ |
| [ÁÅ´Â±±ÊñπËàü](https://console.volcengine.com/ark/region:ark+cn-beijing/model?vendor=Bytedance&amp;view=LIST_VIEW) | ‚úÖ | Â§ßÊ®°ÂûãËÅöÂêàÂπ≥Âè∞, LLMOps Âπ≥Âè∞ |
| [ModelScope](https://modelscope.cn/docs/model-service/API-Inference/intro) | ‚úÖ | Â§ßÊ®°ÂûãËÅöÂêàÂπ≥Âè∞ |
| [MCP](https://modelcontextprotocol.io/) | ‚úÖ | ÊîØÊåÅÈÄöËøá MCP ÂçèËÆÆËé∑ÂèñÂ∑•ÂÖ∑ |

### TTS

| Âπ≥Âè∞/Ê®°Âûã | Â§áÊ≥® |
| --- | --- |
| [FishAudio](https://fish.audio/zh-CN/discovery/) | [Êèí‰ª∂](https://github.com/the-lazy-me/NewChatVoice) |
| [Êµ∑Ë±ö AI](https://www.ttson.cn/?source=thelazy) | [Êèí‰ª∂](https://github.com/the-lazy-me/NewChatVoice) |
| [AzureTTS](https://portal.azure.com/) | [Êèí‰ª∂](https://github.com/Ingnaryk/LangBot_AzureTTS) |

### ÊñáÁîüÂõæ

| Âπ≥Âè∞/Ê®°Âûã | Â§áÊ≥® |
| --- | --- |
| ÈòøÈáå‰∫ëÁôæÁÇº | [Êèí‰ª∂](https://github.com/Thetail001/LangBot_BailianTextToImagePlugin)

## üòò Á§æÂå∫Ë¥°ÁåÆ

ÊÑüË∞¢‰ª•‰∏ã[‰ª£Á†ÅË¥°ÁåÆËÄÖ](https://github.com/RockChinQ/LangBot/graphs/contributors)ÂíåÁ§æÂå∫ÈáåÂÖ∂‰ªñÊàêÂëòÂØπ LangBot ÁöÑË¥°ÁåÆÔºö

&lt;a href=&quot;https://github.com/RockChinQ/LangBot/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=RockChinQ/LangBot&quot; /&gt;
&lt;/a&gt;

## üòé ‰øùÊåÅÊõ¥Êñ∞

ÁÇπÂáª‰ªìÂ∫ìÂè≥‰∏äËßí Star Âíå Watch ÊåâÈíÆÔºåËé∑ÂèñÊúÄÊñ∞Âä®ÊÄÅ„ÄÇ

![star gif](https://docs.langbot.app/star.gif)</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Azure/Azure-Sentinel]]></title>
            <link>https://github.com/Azure/Azure-Sentinel</link>
            <guid>https://github.com/Azure/Azure-Sentinel</guid>
            <pubDate>Fri, 13 Jun 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[Cloud-native SIEM for intelligent security analytics for your entire enterprise.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Azure/Azure-Sentinel">Azure/Azure-Sentinel</a></h1>
            <p>Cloud-native SIEM for intelligent security analytics for your entire enterprise.</p>
            <p>Language: Python</p>
            <p>Stars: 5,099</p>
            <p>Forks: 3,227</p>
            <p>Stars today: 4 stars today</p>
            <h2>README</h2><pre>
# Microsoft Sentinel and Microsoft 365 Defender 
Welcome to the unified Microsoft Sentinel and Microsoft 365 Defender repository! This repository contains out of the box detections, exploration queries, hunting queries, workbooks, playbooks and much more to help you get ramped up with Microsoft Sentinel and provide you security content to secure your environment and hunt for threats. The hunting queries also include Microsoft 365 Defender hunting queries for advanced hunting scenarios in both Microsoft 365 Defender and Microsoft Sentinel. You can also submit to [issues](https://github.com/Azure/Azure-Sentinel/issues) for any samples or resources you would like to see here as you onboard to Microsoft Sentinel. This repository welcomes contributions and refer to this repository&#039;s [wiki](https://aka.ms/threathunters) to get started. For questions and feedback, please contact [AzureSentinel@microsoft.com](AzureSentinel@microsoft.com) 

# Resources
* [Microsoft Sentinel documentation](https://go.microsoft.com/fwlink/?linkid=2073774&amp;clcid=0x409)
* [Microsoft 365 Defender documentation](https://docs.microsoft.com/microsoft-365/security/defender/microsoft-365-defender?view=o365-worldwide)
* [Security Community Webinars](https://aka.ms/securitywebinars)
* [Getting started with GitHub](https://help.github.com/en#dotcom)

We value your feedback. Here are some channels to help surface your questions or feedback:
1. General product specific Q&amp;A for SIEM and SOAR - Join in the [Microsoft Sentinel Tech Community conversations](https://techcommunity.microsoft.com/t5/microsoft-sentinel/bd-p/MicrosoftSentinel)
2. General product specific Q&amp;A for XDR - Join in the [Microsoft 365 Defender Tech Community conversations](https://techcommunity.microsoft.com/t5/microsoft-365-defender/bd-p/MicrosoftThreatProtection)
3. Product specific feature requests - Upvote or post new on [Microsoft Sentinel feedback forums](https://feedback.azure.com/d365community/forum/37638d17-0625-ec11-b6e6-000d3a4f07b8)
4. Report product or contribution bugs - File a GitHub Issue using [Bug template](https://github.com/Azure/Azure-Sentinel/issues/new?assignees=&amp;labels=&amp;template=bug_report.md&amp;title=)
5. General feedback on community and contribution process - File a GitHub Issue using [Feature Request template](https://github.com/Azure/Azure-Sentinel/issues/new?assignees=&amp;labels=&amp;template=feature_request.md&amp;title=)


# Contribution guidelines

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

## Add in your new or updated contributions to GitHub
Note: If you are a first time contributor to this repository, [General GitHub Fork the repo guidance](https://docs.github.com/github/getting-started-with-github/fork-a-repo) before cloning or [Specific steps for the Sentinel repo](https://github.com/Azure/Azure-Sentinel/blob/master/GettingStarted.md). 

## General Steps
Brand new or update to a contribution via these methods:
* Submit for review directly on GitHub website 
    * Browse to the folder you want to upload your file to
    * Choose Upload Files and browse to your file. 
    * You will be required to create your own branch and then submit the Pull Request for review.
* Use [GitHub Desktop](https://docs.github.com/en/desktop/overview/getting-started-with-github-desktop) or [Visual Studio](https://visualstudio.microsoft.com/vs/) or [VSCode](https://code.visualstudio.com/?wt.mc_id=DX_841432)
    * [Fork the repo](https://docs.github.com/github/getting-started-with-github/fork-a-repo)  
    * [Clone the repo](https://help.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository)
    * [Create your own branch](https://help.github.com/en/desktop/contributing-to-projects/creating-a-branch-for-your-work)
    * Do your additions/updates in GitHub Desktop
    * Be sure to merge master back to your branch before you push. 
    * [Push your changes to GitHub](https://help.github.com/en/github/using-git/pushing-commits-to-a-remote-repository)

## Pull Request
* After you push your changes, you will need to submit the [Pull Request (PR)](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests)
* Details about the Proposed Changes are required, be sure to include a minimal level of detail so a review can clearly understand the reason for the change and what he change is related to in the code.
* After submission, check the [Pull Request](https://github.com/Azure/Azure-Sentinel/pulls) for comments
* Make changes as suggested and update your branch or explain why no change is needed. Resolve the comment when done.

### Pull Request Detection Template Structure Validation Check
As part of the PR checks we run a structure validation to make sure all required parts of the YAML structure are included.  For Detections, there is a new section that must be included.  See the [contribution guidelines](https://github.com/Azure/Azure-Sentinel/wiki/Contribute-to-Sentinel-GitHub-Community-of-Queries#now-onto-the-how) for more information.  If this section or any other required section is not included, then a validation error will occur similar to the below.
The example is specifically if the YAML is missing the entityMappings section:

```
A total of 1 test files matched the specified pattern.
[xUnit.net 00:00:00.95]     Kqlvalidations.Tests.DetectionTemplateStructureValidationTests.Validate_DetectionTemplates_HaveValidTemplateStructure(detectionsYamlFileName: &quot;ExcessiveBlockedTrafficGeneratedbyUser.yaml&quot;) [FAIL]
  X Kqlvalidations.Tests.DetectionTemplateStructureValidationTests.Validate_DetectionTemplates_HaveValidTemplateStructure(detectionsYamlFileName: &quot;ExcessiveBlockedTrafficGeneratedbyUser.yaml&quot;) [104ms]
  Error Message:
   Expected object to be &lt;null&gt;, but found System.ComponentModel.DataAnnotations.ValidationException with message &quot;An old mapping for entity &#039;AccountCustomEntity&#039; does not have a matching new mapping entry.&quot;
```

### Pull Request KQL Validation Check
As part of the PR checks we run a syntax validation of the KQL queries defined in the template. If this check fails go to Azure Pipeline (by pressing on the errors link on the checks tab in your PR)
![Azurepipeline](.github/Media/Azurepipeline.png)
In the pipeline you can see which test failed and what is the cause:
![Pipeline Tests Tab](.github/Media/PipelineTestsTab.png)

Example error message:
```
A total of 1 test files matched the specified pattern.
[xUnit.net 00:00:01.81]     Kqlvalidations.Tests.KqlValidationTests.Validate_DetectionQueries_HaveValidKql(detectionsYamlFileName: &quot;ExcessiveBlockedTrafficGeneratedbyUser.yaml&quot;) [FAIL]
  X Kqlvalidations.Tests.KqlValidationTests.Validate_DetectionQueries_HaveValidKql(detectionsYamlFileName: &quot;ExcessiveBlockedTrafficGeneratedbyUser.yaml&quot;) [21ms]
  Error Message:
   Template Id:fa0ab69c-7124-4f62-acdd-61017cf6ce89 is not valid Errors:The name &#039;SymantecEndpointProtection&#039; does not refer to any known table, tabular variable or function., Code: &#039;KS204&#039;, Severity: &#039;Error&#039;, Location: &#039;67..93&#039;,The name &#039;SymantecEndpointProtection&#039; does not refer to any known table, tabular variable or function., Code: &#039;KS204&#039;, Severity: &#039;Error&#039;, Location: &#039;289..315&#039;
```
If you are using custom logs table (a table which is not defined on all workspaces by default) you should verify
your table schema is defined in json file in the folder *Azure-Sentinel\\.script\tests\KqlvalidationsTests\CustomTables*

**Example for table tablexyz.json**
```json
{
  &quot;Name&quot;: &quot;tablexyz&quot;,
  &quot;Properties&quot;: [
    {
      &quot;Name&quot;: &quot;SomeDateTimeColumn&quot;,
      &quot;Type&quot;: &quot;DateTime&quot;
    },
    {
      &quot;Name&quot;: &quot;SomeStringColumn&quot;,
      &quot;Type&quot;: &quot;String&quot;
    },
    {
      &quot;Name&quot;: &quot;SomeDynamicColumn&quot;,
      &quot;Type&quot;: &quot;Dynamic&quot;
    }
  ]
}
```
### Run KQL Validation Locally
In order to run the KQL validation before submitting Pull Request in you local machine:
* You need to have **.Net Core 3.1 SDK** installed [How to download .Net](https://dotnet.microsoft.com/download) (Supports all platforms)
* Open Shell and navigate to  `Azure-Sentinel\\.script\tests\KqlvalidationsTests\`
* Execute `dotnet test`

Example of output (in Ubuntu):
```
Welcome to .NET Core 3.1!
---------------------
SDK Version: 3.1.403

Telemetry
---------
The .NET Core tools collect usage data in order to help us improve your experience. The data is anonymous. It is collected by Microsoft and shared with the community. You can opt-out of telemetry by setting the DOTNET_CLI_TELEMETRY_OPTOUT environment variable to &#039;1&#039; or &#039;true&#039; using your favorite shell.

Read more about .NET Core CLI Tools telemetry: https://aka.ms/dotnet-cli-telemetry

----------------
Explore documentation: https://aka.ms/dotnet-docs
Report issues and find source on GitHub: https://github.com/dotnet/core
Find out what&#039;s new: https://aka.ms/dotnet-whats-new
Learn about the installed HTTPS developer cert: https://aka.ms/aspnet-core-https
Use &#039;dotnet --help&#039; to see available commands or visit: https://aka.ms/dotnet-cli-docs
Write your first app: https://aka.ms/first-net-core-app
--------------------------------------------------------------------------------------
Test run for /mnt/c/git/Azure-Sentinel/.script/tests/KqlvalidationsTests/bin/Debug/netcoreapp3.1/Kqlvalidations.Tests.dll(.NETCoreApp,Version=v3.1)
Microsoft (R) Test Execution Command Line Tool Version 16.7.0
Copyright (c) Microsoft Corporation.  All rights reserved.

Starting test execution, please wait...

A total of 1 test files matched the specified pattern.

Test Run Successful.
Total tests: 171
     Passed: 171
 Total time: 25.7973 Seconds
```

### Detection schema validation tests
Similarly to KQL Validation, there is an automatic validation of the schema of a detection.
The schema validation includes the detection&#039;s frequency and period, the detection&#039;s trigger type and threshold, validity of connectors Ids ([valid connectors Ids list](https://github.com/Azure/Azure-Sentinel/blob/master/.script/tests/detectionTemplateSchemaValidation/ValidConnectorIds.json)), etc.
A wrong format or missing attributes will result with an informative check failure, which should guide you through the resolution of the issue, but make sure to look into the format of already approved detection.

### Run Detection Schema Validation Locally
In order to run the KQL validation before submitting Pull Request in you local machine:
* You need to have **.Net Core 3.1 SDK** installed [How to download .Net](https://dotnet.microsoft.com/download) (Supports all platforms)
* Open Shell and navigate to  `Azure-Sentinel\\.script\tests\DetectionTemplateSchemaValidation\`
* Execute `dotnet test`


When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

For information on what you can contribute and further details, refer to the [&quot;get started&quot;](https://github.com/Azure/Azure-Sentinel/wiki#get-started) section on the project&#039;s [wiki](https://aka.ms/threathunters).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sml2h3/ddddocr]]></title>
            <link>https://github.com/sml2h3/ddddocr</link>
            <guid>https://github.com/sml2h3/ddddocr</guid>
            <pubDate>Fri, 13 Jun 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[Â∏¶Â∏¶ÂºüÂºü ÈÄöÁî®È™åËØÅÁ†ÅËØÜÂà´OCR pypiÁâà]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sml2h3/ddddocr">sml2h3/ddddocr</a></h1>
            <p>Â∏¶Â∏¶ÂºüÂºü ÈÄöÁî®È™åËØÅÁ†ÅËØÜÂà´OCR pypiÁâà</p>
            <p>Language: Python</p>
            <p>Stars: 11,989</p>
            <p>Forks: 2,001</p>
            <p>Stars today: 8 stars today</p>
            <h2>README</h2><pre>

# DdddOcr Â∏¶Â∏¶ÂºüÂºüOCRÈÄöÁî®È™åËØÅÁ†ÅÁ¶ªÁ∫øÊú¨Âú∞ËØÜÂà´SDKÂÖçË¥πÂºÄÊ∫êÁâà

DdddOcrÔºåÂÖ∂Áî±‰ΩúËÄÖ‰∏ékerlomzÂÖ±ÂêåÂêà‰ΩúÂÆåÊàêÔºåÈÄöËøáÂ§ßÊâπÈáèÁîüÊàêÈöèÊú∫Êï∞ÊçÆÂêéËøõË°åÊ∑±Â∫¶ÁΩëÁªúËÆ≠ÁªÉÔºåÊú¨Ë∫´Âπ∂ÈùûÈíàÂØπ‰ªª‰Ωï‰∏ÄÂÆ∂È™åËØÅÁ†ÅÂéÇÂïÜËÄåÂà∂‰ΩúÔºåÊú¨Â∫ì‰ΩøÁî®ÊïàÊûúÂÆåÂÖ®Èù†ÁéÑÂ≠¶ÔºåÂèØËÉΩÂèØ‰ª•ËØÜÂà´ÔºåÂèØËÉΩ‰∏çËÉΩËØÜÂà´„ÄÇ

DdddOcr„ÄÅÊúÄÁÆÄ‰æùËµñÁöÑÁêÜÂøµÔºåÂ∞ΩÈáèÂáèÂ∞ëÁî®Êà∑ÁöÑÈÖçÁΩÆÂíå‰ΩøÁî®ÊàêÊú¨ÔºåÂ∏åÊúõÁªôÊØè‰∏Ä‰ΩçÊµãËØïËÄÖÂ∏¶Êù•ËàíÈÄÇÁöÑ‰ΩìÈ™å

È°πÁõÆÂú∞ÂùÄÔºö [ÁÇπÊàë‰º†ÈÄÅ](https://github.com/sml2h3/ddddocr) 

&lt;!-- PROJECT SHIELDS --&gt;

[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]
[![MIT License][license-shield]][license-url]

&lt;!-- PROJECT LOGO --&gt;
&lt;br /&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/shaojintian/Best_README_template/&quot;&gt;
    &lt;img src=&quot;https://cdn.wenanzhe.com/img/logo.png!/crop/700x500a400a500&quot; alt=&quot;Logo&quot;&gt;
  &lt;/a&gt;
  &lt;p align=&quot;center&quot;&gt;
    ‰∏Ä‰∏™ÂÆπÊòì‰ΩøÁî®ÁöÑÈÄöÁî®È™åËØÅÁ†ÅËØÜÂà´pythonÂ∫ì
    &lt;br /&gt;
    &lt;a href=&quot;https://github.com/shaojintian/Best_README_template&quot;&gt;&lt;strong&gt;Êé¢Á¥¢Êú¨È°πÁõÆÁöÑÊñáÊ°£ ¬ª&lt;/strong&gt;&lt;/a&gt;
    &lt;br /&gt;
    &lt;br /&gt;
    ¬∑
    &lt;a href=&quot;https://github.com/sml2h3/ddddocr/issues&quot;&gt;Êä•ÂëäBug&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://github.com/sml2h3/ddddocr/issues&quot;&gt;ÊèêÂá∫Êñ∞ÁâπÊÄß&lt;/a&gt;
  &lt;/p&gt;

&lt;/p&gt;

 
## ÁõÆÂΩï

- [ËµûÂä©Âêà‰ΩúÂïÜ](#ËµûÂä©Âêà‰ΩúÂïÜ)
- [‰∏äÊâãÊåáÂçó](#‰∏äÊâãÊåáÂçó)
  - [ÁéØÂ¢ÉÊîØÊåÅ](#ÁéØÂ¢ÉÊîØÊåÅ)
  - [ÂÆâË£ÖÊ≠•È™§](#ÂÆâË£ÖÊ≠•È™§)
- [Êñá‰ª∂ÁõÆÂΩïËØ¥Êòé](#Êñá‰ª∂ÁõÆÂΩïËØ¥Êòé)
- [È°πÁõÆÂ∫ïÂ±ÇÊîØÊåÅ](#È°πÁõÆÂ∫ïÂ±ÇÊîØÊåÅ)
- [‰ΩøÁî®ÊñáÊ°£](#‰ΩøÁî®ÊñáÊ°£)
  - [Âü∫Á°ÄocrËØÜÂà´ËÉΩÂäõ](#Âü∫Á°ÄocrËØÜÂà´ËÉΩÂäõ)
  - [ÁõÆÊ†áÊ£ÄÊµãËÉΩÂäõ](#ÁõÆÊ†áÊ£ÄÊµãËÉΩÂäõ)
  - [ÊªëÂùóÊ£ÄÊµã](#ÊªëÂùóÊ£ÄÊµã)
  - [OCRÊ¶ÇÁéáËæìÂá∫](#OCRÊ¶ÇÁéáËæìÂá∫)
  - [Ëá™ÂÆö‰πâOCRËÆ≠ÁªÉÊ®°ÂûãÂØºÂÖ•](#Ëá™ÂÆö‰πâOCRËÆ≠ÁªÉÊ®°ÂûãÂØºÂÖ•)
- [ÁâàÊú¨ÊéßÂà∂](#ÁâàÊú¨ÊéßÂà∂)
- [‰ΩúËÄÖ](#‰ΩúËÄÖ)
- [ÊçêËµ†](#ÊçêËµ†)



### ËµûÂä©Âêà‰ΩúÂïÜ

|                                                            | ËµûÂä©Âêà‰ΩúÂïÜ | Êé®ËçêÁêÜÁî±                                                                                             |
|------------------------------------------------------------|------------|--------------------------------------------------------------------------------------------------|
| ![YesCaptcha](https://cdn.wenanzhe.com/img/yescaptcha.png) | [YesCaptcha](https://yescaptcha.com/i/NSwk7i) | Ë∞∑Ê≠åreCaptchaÈ™åËØÅÁ†Å / hCaptchaÈ™åËØÅÁ†Å / funCaptchaÈ™åËØÅÁ†ÅÂïÜ‰∏öÁ∫ßËØÜÂà´Êé•Âè£ [ÁÇπÊàë](https://yescaptcha.com/i/NSwk7i) Áõ¥ËææVIP4 |
| ![Malenia](https://cdn.wenanzhe.com/img/malenia.png!/scale/50)    | [Malenia](https://malenia.iinti.cn/malenia-doc/) | Malenia‰ºÅ‰∏öÁ∫ß‰ª£ÁêÜIPÁΩëÂÖ≥Âπ≥Âè∞/‰ª£ÁêÜIPÂàÜÈîÄËΩØ‰ª∂ |


### ‰∏äÊâãÊåáÂçó

###### ÁéØÂ¢ÉÊîØÊåÅ



| Á≥ªÁªü               | CPU | GPU | ÊúÄÂ§ßÊîØÊåÅpyÁâàÊú¨ | Â§áÊ≥®                                                                 |
|------------------|-----|------|----------|--------------------------------------------------------------------|
| Windows 64‰Ωç      | ‚àö   | ‚àö | 3.12     | ÈÉ®ÂàÜÁâàÊú¨windowsÈúÄË¶ÅÂÆâË£Ö&lt;a href=&quot;https://www.ghxi.com/yxkhj.html&quot;&gt;vcËøêË°åÂ∫ì&lt;/a&gt; |
| Windows 32‰Ωç      | √ó   | √ó | -        |                                                                    |
| Linux 64 / ARM64 | ‚àö   | ‚àö | 3.12     |                                                                    |
| Linux 32         | √ó   | √ó | -        |                                                                    |
| Macos  X64       | ‚àö   | ‚àö | 3.12     | M1/M2/M3...ËäØÁâáÂèÇËÄÉ&lt;a href=&quot;https://github.com/sml2h3/ddddocr/issues/67&quot;&gt;#67&lt;/a&gt;         |

###### **ÂÆâË£ÖÊ≠•È™§**

**i. ‰ªépypiÂÆâË£Ö** 
```sh
pip install ddddocr
```

**ii. ‰ªéÊ∫êÁ†ÅÂÆâË£Ö**
```sh
git clone https://github.com/sml2h3/ddddocr.git
cd ddddocr
python setup.py
```

**ËØ∑ÂãøÁõ¥Êé•Âú®ddddocrÈ°πÁõÆÁöÑÊ†πÁõÆÂΩïÂÜÖÁõ¥Êé•import ddddocr**ÔºåËØ∑Á°Æ‰øù‰Ω†ÁöÑÂºÄÂèëÈ°πÁõÆÁõÆÂΩïÂêçÁß∞‰∏ç‰∏∫ddddocrÔºåÊ≠§‰∏∫Âü∫Á°ÄÂ∏∏ËØÜ„ÄÇ

### Êñá‰ª∂ÁõÆÂΩïËØ¥Êòé
eg:

```
ddddocr 
‚îú‚îÄ‚îÄ MANIFEST.in
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ /ddddocr/
‚îÇ  ‚îÇ‚îÄ‚îÄ __init__.py            ‰∏ª‰ª£Á†ÅÂ∫ìÊñá‰ª∂
‚îÇ  ‚îÇ‚îÄ‚îÄ common.onnx            Êñ∞ocrÊ®°Âûã
‚îÇ  ‚îÇ‚îÄ‚îÄ common_det.onnx        ÁõÆÊ†áÊ£ÄÊµãÊ®°Âûã
‚îÇ  ‚îÇ‚îÄ‚îÄ common_old.onnx        ËÄÅocrÊ®°Âûã
‚îÇ  ‚îÇ‚îÄ‚îÄ logo.png
‚îÇ  ‚îÇ‚îÄ‚îÄ README.md
‚îÇ  ‚îÇ‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ logo.png
‚îî‚îÄ‚îÄ setup.py

```

### È°πÁõÆÂ∫ïÂ±ÇÊîØÊåÅ 

Êú¨È°πÁõÆÂü∫‰∫é[dddd_trainer](https://github.com/sml2h3/dddd_trainer) ËÆ≠ÁªÉÊâÄÂæóÔºåËÆ≠ÁªÉÂ∫ïÂ±ÇÊ°ÜÊû∂‰ΩçpytorchÔºåddddocrÊé®ÁêÜÂ∫ïÂ±ÇÊäµËµñ‰∫é[onnxruntime](https://pypi.org/project/onnxruntime/)ÔºåÊïÖÊú¨È°πÁõÆÁöÑÊúÄÂ§ßÂÖºÂÆπÊÄß‰∏épythonÁâàÊú¨ÊîØÊåÅ‰∏ªË¶ÅÂèñÂÜ≥‰∫é[onnxruntime](https://pypi.org/project/onnxruntime/)„ÄÇ

### ‰ΩøÁî®ÊñáÊ°£

##### i. Âü∫Á°ÄocrËØÜÂà´ËÉΩÂäõ

‰∏ªË¶ÅÁî®‰∫éËØÜÂà´ÂçïË°åÊñáÂ≠óÔºåÂç≥ÊñáÂ≠óÈÉ®ÂàÜÂç†ÊçÆÂõæÁâáÁöÑ‰∏ª‰ΩìÈÉ®ÂàÜÔºå‰æãÂ¶ÇÂ∏∏ËßÅÁöÑËã±Êï∞È™åËØÅÁ†ÅÁ≠âÔºåÊú¨È°πÁõÆÂèØ‰ª•ÂØπ‰∏≠Êñá„ÄÅËã±ÊñáÔºàÈöèÊú∫Â§ßÂ∞èÂÜôorÈÄöËøáËÆæÁΩÆÁªìÊûúËåÉÂõ¥ÂúàÂÆöÂ§ßÂ∞èÂÜôÔºâ„ÄÅÊï∞Â≠ó‰ª•ÂèäÈÉ®ÂàÜÁâπÊÆäÂ≠óÁ¨¶„ÄÇ

```python
# example.py
import ddddocr

ocr = ddddocr.DdddOcr()

image = open(&quot;example.jpg&quot;, &quot;rb&quot;).read()
result = ocr.classification(image)
print(result)
```

Êú¨Â∫ìÂÜÖÁΩÆÊúâ‰∏§Â•óocrÊ®°ÂûãÔºåÈªòËÆ§ÊÉÖÂÜµ‰∏ã‰∏ç‰ºöËá™Âä®ÂàáÊç¢ÔºåÈúÄË¶ÅÂú®ÂàùÂßãÂåñddddocrÁöÑÊó∂ÂÄôÈÄöËøáÂèÇÊï∞ËøõË°åÂàáÊç¢

```python
# example.py
import ddddocr

ocr = ddddocr.DdddOcr(beta=True)  # ÂàáÊç¢‰∏∫Á¨¨‰∫åÂ•óocrÊ®°Âûã

image = open(&quot;example.jpg&quot;, &quot;rb&quot;).read()
result = ocr.classification(image)
print(result)
```

**ÊèêÁ§∫**
ÂØπ‰∫éÈÉ®ÂàÜÈÄèÊòéÈªëËâ≤pngÊ†ºÂºèÂõæÁâáÂæóËØÜÂà´ÊîØÊåÅ: `classification` ÊñπÊ≥ï ‰ΩøÁî® `png_fix` ÂèÇÊï∞ÔºåÈªòËÆ§‰∏∫False

```python
 ocr.classification(image, png_fix=True)
```

**Ê≥®ÊÑè**

‰πãÂâçÂèëÁé∞ÂæàÂ§ö‰∫∫ÂñúÊ¨¢Âú®ÊØèÊ¨°ocrËØÜÂà´ÁöÑÊó∂ÂÄôÈÉΩÈáçÊñ∞ÂàùÂßãÂåñddddocrÔºåÂç≥ÊØèÊ¨°ÈÉΩÊâßË°å```ocr = ddddocr.DdddOcr()```ÔºåËøôÊòØÈîôËØØÁöÑÔºåÈÄöÂ∏∏Êù•ËØ¥Âè™ÈúÄË¶ÅÂàùÂßãÂåñ‰∏ÄÊ¨°Âç≥ÂèØÔºåÂõ†‰∏∫ÊØèÊ¨°ÂàùÂßãÂåñÂíåÂàùÂßãÂåñÂêéÁöÑÁ¨¨‰∏ÄÊ¨°ËØÜÂà´ÈÄüÂ∫¶ÈÉΩÈùûÂ∏∏ÊÖ¢


**ÂèÇËÄÉ‰æãÂõæ**

ÂåÖÊã¨‰∏î‰∏çÈôê‰∫é‰ª•‰∏ãÂõæÁâá

&lt;img src=&quot;https://cdn.wenanzhe.com/img/20210715211733855.png&quot; alt=&quot;captcha&quot; width=&quot;150&quot;&gt;
&lt;img src=&quot;https://cdn.wenanzhe.com/img/78b7f57d-371d-4b65-afb2-d19608ae1892.png&quot; alt=&quot;captcha&quot; width=&quot;150&quot;&gt;
&lt;img src=&quot;https://cdn.wenanzhe.com/img/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20211226142305.png&quot; alt=&quot;captcha&quot; width=&quot;150&quot;&gt;
&lt;img src=&quot;https://cdn.wenanzhe.com/img/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20211226142325.png&quot; alt=&quot;captcha&quot; width=&quot;150&quot;&gt;
&lt;img src=&quot;https://cdn.wenanzhe.com/img/2AMLyA_fd83e1f1800e829033417ae6dd0e0ae0.png&quot; alt=&quot;captcha&quot; width=&quot;150&quot;&gt;
&lt;img src=&quot;https://cdn.wenanzhe.com/img/aabd_181ae81dd5526b8b89f987d1179266ce.jpg&quot; alt=&quot;captcha&quot; width=&quot;150&quot;&gt;
&lt;br /&gt;
&lt;img src=&quot;https://cdn.wenanzhe.com/img/2bghz_b504e9f9de1ed7070102d21c6481e0cf.png&quot; alt=&quot;captcha&quot; width=&quot;150&quot;&gt;
&lt;img src=&quot;https://cdn.wenanzhe.com/img/0000_z4ecc2p65rxc610x.jpg&quot; alt=&quot;captcha&quot; width=&quot;150&quot;&gt;
&lt;img src=&quot;https://cdn.wenanzhe.com/img/2acd_0586b6b36858a4e8a9939db8a7ec07b7.jpg&quot; alt=&quot;captcha&quot; width=&quot;150&quot;&gt;
&lt;img src=&quot;https://cdn.wenanzhe.com/img/2a8r_79074e311d573d31e1630978fe04b990.jpg&quot; alt=&quot;captcha&quot; width=&quot;150&quot;&gt;
&lt;img src=&quot;https://cdn.wenanzhe.com/img/aftf_C2vHZlk8540y3qAmCM.bmp&quot; alt=&quot;captcha&quot; width=&quot;150&quot;&gt;
&lt;img src=&quot;https://cdn.wenanzhe.com/img/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20211226144057.png&quot; alt=&quot;captcha&quot; width=&quot;150&quot;&gt;

##### ii. ÁõÆÊ†áÊ£ÄÊµãËÉΩÂäõ

‰∏ªË¶ÅÁî®‰∫éÂø´ÈÄüÊ£ÄÊµãÂá∫ÂõæÂÉè‰∏≠ÂèØËÉΩÁöÑÁõÆÊ†á‰∏ª‰Ωì‰ΩçÁΩÆÔºåÁî±‰∫éË¢´Ê£ÄÊµãÂá∫ÁöÑÁõÆÊ†á‰∏ç‰∏ÄÂÆö‰∏∫ÊñáÂ≠óÔºåÊâÄ‰ª•Êú¨ÂäüËÉΩ‰ªÖÊèê‰æõÁõÆÊ†áÁöÑbbox‰ΩçÁΩÆ **ÔºàÂú®‚Ω¨Ê†áÊ£ÄÊµã‚æ•ÔºåÊàë‰ª¨ÈÄöÂ∏∏‰Ωø‚Ω§bboxÔºàbounding boxÔºåÁº©ÂÜôÊòØ bboxÔºâÊù•ÊèèËø∞‚Ω¨Ê†á‰ΩçÁΩÆ„ÄÇbboxÊòØ‚ºÄ‰∏™Áü©ÂΩ¢Ê°ÜÔºåÂèØ‰ª•Áî±Áü©ÂΩ¢Â∑¶‰∏ä‚ªÜÁöÑ x Âíå y ËΩ¥ÂùêÊ†á‰∏éÂè≥‰∏ã‚ªÜÁöÑ x Âíå y ËΩ¥ÂùêÊ†áÁ°ÆÂÆöÔºâ** 

Â¶ÇÊûú‰ΩøÁî®ËøáÁ®ã‰∏≠Êó†ÈúÄË∞ÉÁî®ocrÂäüËÉΩÔºåÂèØ‰ª•Âú®ÂàùÂßãÂåñÊó∂ÈÄöËøá‰º†ÂèÇ`ocr=False`ÂÖ≥Èó≠ocrÂäüËÉΩÔºåÂºÄÂêØÁõÆÊ†áÊ£ÄÊµãÈúÄË¶Å‰º†ÂÖ•ÂèÇÊï∞`det=True`

```python
import ddddocr
import cv2

det = ddddocr.DdddOcr(det=True)

with open(&quot;test.jpg&quot;, &#039;rb&#039;) as f:
    image = f.read()

bboxes = det.detection(image)
print(bboxes)

im = cv2.imread(&quot;test.jpg&quot;)

for bbox in bboxes:
    x1, y1, x2, y2 = bbox
    im = cv2.rectangle(im, (x1, y1), (x2, y2), color=(0, 0, 255), thickness=2)

cv2.imwrite(&quot;result.jpg&quot;, im)

```



**ÂèÇËÄÉ‰æãÂõæ**

ÂåÖÊã¨‰∏î‰∏çÈôê‰∫é‰ª•‰∏ãÂõæÁâá

&lt;img src=&quot;https://cdn.wenanzhe.com/img/page1_1.jpg&quot; alt=&quot;captcha&quot; width=&quot;200&quot;&gt;
&lt;img src=&quot;https://cdn.wenanzhe.com/img/page1_2.jpg&quot; alt=&quot;captcha&quot; width=&quot;200&quot;&gt;
&lt;img src=&quot;https://cdn.wenanzhe.com/img/page1_3.jpg&quot; alt=&quot;captcha&quot; width=&quot;200&quot;&gt;
&lt;img src=&quot;https://cdn.wenanzhe.com/img/page1_4.jpg&quot; alt=&quot;captcha&quot; width=&quot;200&quot;&gt;
&lt;br /&gt;
&lt;img src=&quot;https://cdn.wenanzhe.com/img/result.jpg&quot; alt=&quot;captcha&quot; width=&quot;200&quot;&gt;
&lt;img src=&quot;https://cdn.wenanzhe.com/img/result2.jpg&quot; alt=&quot;captcha&quot; width=&quot;200&quot;&gt;
&lt;img src=&quot;https://cdn.wenanzhe.com/img/result4.jpg&quot; alt=&quot;captcha&quot; width=&quot;200&quot;&gt;

##### ‚Ö¢. ÊªëÂùóÊ£ÄÊµã

Êú¨È°πÁõÆÁöÑÊªëÂùóÊ£ÄÊµãÂäüËÉΩÂπ∂ÈùûAIËØÜÂà´ÂÆûÁé∞ÔºåÂùá‰∏∫opencvÂÜÖÁΩÆÁÆóÊ≥ïÂÆûÁé∞„ÄÇÂèØËÉΩÂØπ‰∫éÊà™ÂõæÂÖöÁî®Êà∑Ê≤°ÈÇ£‰πàÂèãÂ•Ω~ÔºåÂ¶ÇÊûú‰ΩøÁî®ËøáÁ®ã‰∏≠Êó†ÈúÄË∞ÉÁî®ocrÂäüËÉΩÊàñÁõÆÊ†áÊ£ÄÊµãÂäüËÉΩÔºåÂèØ‰ª•Âú®ÂàùÂßãÂåñÊó∂ÈÄöËøá‰º†ÂèÇ`ocr=False`ÂÖ≥Èó≠ocrÂäüËÉΩÊàñ`det=False`Êù•ÂÖ≥Èó≠ÁõÆÊ†áÊ£ÄÊµãÂäüËÉΩ

Êú¨ÂäüËÉΩÂÜÖÁΩÆ‰∏§Â•óÁÆóÊ≥ïÂÆûÁé∞ÔºåÈÄÇÁî®‰∫é‰∏§Áßç‰∏çÂêåÊÉÖÂÜµÔºåÂÖ∑‰ΩìËØ∑ÂèÇËÄÉ‰ª•‰∏ãËØ¥Êòé

**a.ÁÆóÊ≥ï1**

ÁÆóÊ≥ï1ÂéüÁêÜÊòØÈÄöËøáÊªëÂùóÂõæÂÉèÁöÑËæπÁºòÂú®ËÉåÊôØÂõæ‰∏≠ËÆ°ÁÆóÊâæÂà∞Áõ∏ÂØπÂ∫îÁöÑÂùë‰ΩçÔºåÂèØ‰ª•ÂàÜÂà´Ëé∑ÂèñÂà∞ÊªëÂùóÂõæÂíåËÉåÊôØÂõæÔºåÊªëÂùóÂõæ‰∏∫ÈÄèÊòéËÉåÊôØÂõæ

ÊªëÂùóÂõæ

&lt;img src=&quot;https://cdn.wenanzhe.com/img/b.png&quot; alt=&quot;captcha&quot; width=&quot;50&quot;&gt;

ËÉåÊôØÂõæ

&lt;img src=&quot;https://cdn.wenanzhe.com/img/a.png&quot; alt=&quot;captcha&quot; width=&quot;350&quot;&gt;

```python
    det = ddddocr.DdddOcr(det=False, ocr=False)
    
    with open(&#039;target.png&#039;, &#039;rb&#039;) as f:
        target_bytes = f.read()
    
    with open(&#039;background.png&#039;, &#039;rb&#039;) as f:
        background_bytes = f.read()
    
    res = det.slide_match(target_bytes, background_bytes)
    
    print(res)
  ```
  Áî±‰∫éÊªëÂùóÂõæÂèØËÉΩÂ≠òÂú®ÈÄèÊòéËæπÊ°ÜÁöÑÈóÆÈ¢òÔºåÂØºËá¥ËÆ°ÁÆóÁªìÊûú‰∏ç‰∏ÄÂÆöÂáÜÁ°ÆÔºåÈúÄË¶ÅËá™Ë°å‰º∞ÁÆóÊªëÂùóÂõæÈÄèÊòéËæπÊ°ÜÁöÑÂÆΩÂ∫¶Áî®‰∫é‰øÆÊ≠£ÂæóÂá∫ÁöÑbbox

  *ÊèêÁ§∫ÔºöÂ¶ÇÊûúÊªëÂùóÊó†ËøáÂ§öËÉåÊôØÈÉ®ÂàÜÔºåÂàôÂèØ‰ª•Ê∑ªÂä†simple_targetÂèÇÊï∞Ôºå ÈÄöÂ∏∏‰∏∫jpgÊàñËÄÖbmpÊ†ºÂºèÁöÑÂõæÁâá*

```python
    slide = ddddocr.DdddOcr(det=False, ocr=False)
    
    with open(&#039;target.jpg&#039;, &#039;rb&#039;) as f:
        target_bytes = f.read()
    
    with open(&#039;background.jpg&#039;, &#039;rb&#039;) as f:
        background_bytes = f.read()
    
    res = slide.slide_match(target_bytes, background_bytes, simple_target=True)
    
    print(res)
  ```

**a.ÁÆóÊ≥ï2**

ÁÆóÊ≥ï2ÊòØÈÄöËøáÊØîËæÉ‰∏§Âº†ÂõæÁöÑ‰∏çÂêå‰πãÂ§ÑËøõË°åÂà§Êñ≠ÊªëÂùóÁõÆÊ†áÂùë‰ΩçÁöÑ‰ΩçÁΩÆ

ÂèÇËÄÉÂõæaÔºåÂ∏¶ÊúâÁõÆÊ†áÂùë‰ΩçÈò¥ÂΩ±ÁöÑÂÖ®Âõæ

&lt;img src=&quot;https://cdn.wenanzhe.com/img/bg.jpg&quot; alt=&quot;captcha&quot; width=&quot;350&quot;&gt;

ÂèÇËÄÉÂõæbÔºåÂÖ®Âõæ

&lt;img src=&quot;https://cdn.wenanzhe.com/img/fullpage.jpg&quot; alt=&quot;captcha&quot; width=&quot;350&quot;&gt;

```python
    slide = ddddocr.DdddOcr(det=False, ocr=False)

    with open(&#039;bg.jpg&#039;, &#039;rb&#039;) as f:
        target_bytes = f.read()
    
    with open(&#039;fullpage.jpg&#039;, &#039;rb&#039;) as f:
        background_bytes = f.read()
    
    img = cv2.imread(&quot;bg.jpg&quot;)
    
    res = slide.slide_comparison(target_bytes, background_bytes)

    print(res)
  ```

##### ‚Ö£. OCRÊ¶ÇÁéáËæìÂá∫

‰∏∫‰∫ÜÊèê‰æõÊõ¥ÁÅµÊ¥ªÁöÑocrÁªìÊûúÊéßÂà∂‰∏éËåÉÂõ¥ÈôêÂÆöÔºåÈ°πÁõÆÊîØÊåÅÂØπocrÁªìÊûúËøõË°åËåÉÂõ¥ÈôêÂÆö„ÄÇ

ÂèØ‰ª•ÈÄöËøáÂú®Ë∞ÉÁî®`classification`ÊñπÊ≥ïÁöÑÊó∂ÂÄô‰º†ÂèÇ`probability=True`ÔºåÊ≠§Êó∂`classification`ÊñπÊ≥ïÂ∞ÜËøîÂõûÂÖ®Â≠óÁ¨¶Ë°®ÁöÑÊ¶ÇÁéá
ÂΩìÁÑ∂‰πüÂèØ‰ª•ÈÄöËøá`set_ranges`ÊñπÊ≥ïËÆæÁΩÆËæìÂá∫Â≠óÁ¨¶ËåÉÂõ¥Êù•ÈôêÂÆöËøîÂõûÁöÑÁªìÊûú„ÄÇ

‚Ö†. `set_ranges` ÊñπÊ≥ïÈôêÂÆöËøîÂõûÂ≠óÁ¨¶ËøîÂõû

Êú¨ÊñπÊ≥ïÊé•Âèó1‰∏™ÂèÇÊï∞ÔºåÂ¶ÇÊûúËæìÂÖ•‰∏∫intÁ±ªÂûã‰∏∫ÂÜÖÁΩÆÁöÑÂ≠óÁ¨¶ÈõÜÈôêÂà∂ÔºåstringÁ±ªÂûãÂàô‰∏∫Ëá™ÂÆö‰πâÁöÑÂ≠óÁ¨¶ÈõÜ

Â¶ÇÊûú‰∏∫intÁ±ªÂûãÔºåËØ∑ÂèÇËÄÉ‰∏ãË°®

| ÂèÇÊï∞ÂÄº | ÊÑè‰πâ                                |
|-----|-----------------------------------|
| 0   | Á∫ØÊï¥Êï∞0-9                            |
| 1   | Á∫ØÂ∞èÂÜôËã±Êñáa-z                          |
| 2   | Á∫ØÂ§ßÂÜôËã±ÊñáA-Z                          |
| 3   | Â∞èÂÜôËã±Êñáa-z + Â§ßÂÜôËã±ÊñáA-Z                 |
| 4   | Â∞èÂÜôËã±Êñáa-z + Êï¥Êï∞0-9                   |
| 5   | Â§ßÂÜôËã±ÊñáA-Z + Êï¥Êï∞0-9                   |
| 6   | Â∞èÂÜôËã±Êñáa-z + Â§ßÂÜôËã±ÊñáA-Z + Êï¥Êï∞0-9         |
| 7   | ÈªòËÆ§Â≠óÁ¨¶Â∫ì - Â∞èÂÜôËã±Êñáa-z - Â§ßÂÜôËã±ÊñáA-Z - Êï¥Êï∞0-9 |

Â¶ÇÊûú‰∏∫stringÁ±ªÂûãËØ∑‰º†ÂÖ•‰∏ÄÊÆµ‰∏çÂåÖÂê´Á©∫Ê†ºÁöÑÊñáÊú¨ÔºåÂÖ∂‰∏≠ÁöÑÊØè‰∏™Â≠óÁ¨¶Âùá‰∏∫‰∏Ä‰∏™ÂæÖÈÄâËØç
Â¶ÇÔºö`&quot;0123456789+-x/=&quot;&quot;`

```python
import ddddocr

ocr = ddddocr.DdddOcr()

image = open(&quot;test.jpg&quot;, &quot;rb&quot;).read()
ocr.set_ranges(&quot;0123456789+-x/=&quot;)
result = ocr.classification(image, probability=True)
s = &quot;&quot;
for i in result[&#039;probability&#039;]:
    s += result[&#039;charsets&#039;][i.index(max(i))]

print(s)

```

##### ‚Ö§. Ëá™ÂÆö‰πâOCRËÆ≠ÁªÉÊ®°ÂûãÂØºÂÖ•

Êú¨È°πÁõÆÊîØÊåÅÂØºÂÖ•Êù•Ëá™‰∫é [dddd_trainer](https://github.com/sml2h3/dddd_trainer) ËøõË°åËá™ÂÆö‰πâËÆ≠ÁªÉÂêéÁöÑÊ®°ÂûãÔºåÂèÇËÄÉÂØºÂÖ•‰ª£Á†Å‰∏∫

```python
import ddddocr

ocr = ddddocr.DdddOcr(det=False, ocr=False, import_onnx_path=&quot;myproject_0.984375_139_13000_2022-02-26-15-34-13.onnx&quot;, charsets_path=&quot;charsets.json&quot;)

with open(&#039;test.jpg&#039;, &#039;rb&#039;) as f:
    image_bytes = f.read()

res = ocr.classification(image_bytes)
print(res)

```

### ÁâàÊú¨ÊéßÂà∂

ËØ•È°πÁõÆ‰ΩøÁî®GitËøõË°åÁâàÊú¨ÁÆ°ÁêÜ„ÄÇÊÇ®ÂèØ‰ª•Âú®repositoryÂèÇÁúãÂΩìÂâçÂèØÁî®ÁâàÊú¨„ÄÇ

### ‰ΩúËÄÖ

sml2h3@gamil.com
 
&lt;img src=&quot;https://cdn.wenanzhe.com/img/mmqrcode1640418911274.png!/scale/50&quot; alt=&quot;wechat&quot; width=&quot;150&quot;&gt;

 *Â•ΩÂèãÊï∞ËøáÂ§ö‰∏ç‰∏ÄÂÆöÈÄöËøáÔºåÊúâÈóÆÈ¢òÂèØ‰ª•Âú®issueËøõË°å‰∫§ÊµÅ*

### ÁâàÊùÉËØ¥Êòé

ËØ•È°πÁõÆÁ≠æÁΩ≤‰∫ÜMIT ÊéàÊùÉËÆ∏ÂèØÔºåËØ¶ÊÉÖËØ∑ÂèÇÈòÖ [LICENSE](https://github.com/sml2h3/ddddocr/blob/master/LICENSE)

### ÊçêËµ† ÔºàÂ¶ÇÊûúÈ°πÁõÆÊúâÂ∏ÆÂä©Âà∞ÊÇ®ÔºåÂèØ‰ª•ÈÄâÊã©ÊçêËµ†‰∏Ä‰∫õË¥πÁî®Áî®‰∫éddddocrÁöÑÂêéÁª≠ÁâàÊú¨Áª¥Êä§ÔºåÊú¨È°πÁõÆÈïøÊúüÁª¥Êä§Ôºâ

&lt;img src=&quot;https://cdn.wenanzhe.com/img/zhifubao.jpg&quot; alt=&quot;captcha&quot; width=&quot;150&quot;&gt;
&lt;img src=&quot;https://cdn.wenanzhe.com/img/weixin.jpg&quot; alt=&quot;captcha&quot; width=&quot;150&quot;&gt;


&lt;!-- links --&gt;
[your-project-path]:sml2h3/ddddocr
[contributors-shield]: https://img.shields.io/github/contributors/sml2h3/ddddocr?style=flat-square
[contributors-url]: https://github.com/shaojintian/Best_README_template/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/sml2h3/ddddocr?style=flat-square
[forks-url]: https://github.com/shaojintian/Best_README_template/network/members
[stars-shield]: https://img.shields.io/github/stars/sml2h3/ddddocr?style=flat-square
[stars-url]: https://github.com/shaojintian/Best_README_template/stargazers
[issues-shield]: https://img.shields.io/github/issues/sml2h3/ddddocr?style=flat-square
[issues-url]: https://img.shields.io/github/issues/sml2h3/ddddocr.svg
[license-shield]: https://img.shields.io/github/license/sml2h3/ddddocr?style=flat-square
[license-url]: https://github.com/sml2h3/ddddocr/blob/master/LICENSE





</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[EleutherAI/lm-evaluation-harness]]></title>
            <link>https://github.com/EleutherAI/lm-evaluation-harness</link>
            <guid>https://github.com/EleutherAI/lm-evaluation-harness</guid>
            <pubDate>Fri, 13 Jun 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[A framework for few-shot evaluation of language models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/EleutherAI/lm-evaluation-harness">EleutherAI/lm-evaluation-harness</a></h1>
            <p>A framework for few-shot evaluation of language models.</p>
            <p>Language: Python</p>
            <p>Stars: 9,243</p>
            <p>Forks: 2,457</p>
            <p>Stars today: 22 stars today</p>
            <h2>README</h2><pre># Language Model Evaluation Harness

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10256836.svg)](https://doi.org/10.5281/zenodo.10256836)

---

## Latest News üì£

- [2025/03] Added support for steering HF models!
- [2025/02] Added [SGLang](https://docs.sglang.ai/) support!
- [2024/09] We are prototyping allowing users of LM Evaluation Harness to create and evaluate on text+image multimodal input, text output tasks, and have just added the `hf-multimodal` and `vllm-vlm` model types and `mmmu` task as a prototype feature. We welcome users to try out this in-progress feature and stress-test it for themselves, and suggest they check out [`lmms-eval`](https://github.com/EvolvingLMMs-Lab/lmms-eval), a wonderful project originally forking off of the lm-evaluation-harness, for a broader range of multimodal tasks, models, and features.
- [2024/07] [API model](docs/API_guide.md) support has been updated and refactored, introducing support for batched and async requests, and making it significantly easier to customize and use for your own purposes. **To run Llama 405B, we recommend using VLLM&#039;s OpenAI-compliant API to host the model, and use the `local-completions` model type to evaluate the model.**
- [2024/07] New Open LLM Leaderboard tasks have been added ! You can find them under the [leaderboard](lm_eval/tasks/leaderboard/README.md) task group.

---

## Announcement

**A new v0.4.0 release of lm-evaluation-harness is available** !

New updates and features include:

- **New Open LLM Leaderboard tasks have been added ! You can find them under the [leaderboard](lm_eval/tasks/leaderboard/README.md) task group.**
- Internal refactoring
- Config-based task creation and configuration
- Easier import and sharing of externally-defined task config YAMLs
- Support for Jinja2 prompt design, easy modification of prompts + prompt imports from Promptsource
- More advanced configuration options, including output post-processing, answer extraction, and multiple LM generations per document, configurable fewshot settings, and more
- Speedups and new modeling libraries supported, including: faster data-parallel HF model usage, vLLM support, MPS support with HuggingFace, and more
- Logging and usability changes
- New tasks including CoT BIG-Bench-Hard, Belebele, user-defined task groupings, and more

Please see our updated documentation pages in `docs/` for more details.

Development will be continuing on the `main` branch, and we encourage you to give us feedback on what features are desired and how to improve the library further, or ask questions, either in issues or PRs on GitHub, or in the [EleutherAI discord](https://discord.gg/eleutherai)!

---

## Overview

This project provides a unified framework to test generative language models on a large number of different evaluation tasks.

**Features:**

- Over 60 standard academic benchmarks for LLMs, with hundreds of subtasks and variants implemented.
- Support for models loaded via [transformers](https://github.com/huggingface/transformers/) (including quantization via [GPTQModel](https://github.com/ModelCloud/GPTQModel) and [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)), [GPT-NeoX](https://github.com/EleutherAI/gpt-neox), and [Megatron-DeepSpeed](https://github.com/microsoft/Megatron-DeepSpeed/), with a flexible tokenization-agnostic interface.
- Support for fast and memory-efficient inference with [vLLM](https://github.com/vllm-project/vllm).
- Support for commercial APIs including [OpenAI](https://openai.com), and [TextSynth](https://textsynth.com/).
- Support for evaluation on adapters (e.g. LoRA) supported in [HuggingFace&#039;s PEFT library](https://github.com/huggingface/peft).
- Support for local models and benchmarks.
- Evaluation with publicly available prompts ensures reproducibility and comparability between papers.
- Easy support for custom prompts and evaluation metrics.

The Language Model Evaluation Harness is the backend for ü§ó Hugging Face&#039;s popular [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard), has been used in [hundreds of papers](https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;authuser=2&amp;cites=15052937328817631261,4097184744846514103,1520777361382155671,17476825572045927382,18443729326628441434,14801318227356878622,7890865700763267262,12854182577605049984,15641002901115500560,5104500764547628290), and is used internally by dozens of organizations including NVIDIA, Cohere, BigScience, BigCode, Nous Research, and Mosaic ML.

## Install

To install the `lm-eval` package from the github repository, run:

```bash
git clone --depth 1 https://github.com/EleutherAI/lm-evaluation-harness
cd lm-evaluation-harness
pip install -e .
```

We also provide a number of optional dependencies for extended functionality. A detailed table is available at the end of this document.

## Basic Usage

### User Guide

A user guide detailing the full list of supported arguments is provided [here](./docs/interface.md), and on the terminal by calling `lm_eval -h`. Alternatively, you can use `lm-eval` instead of `lm_eval`.

A list of supported tasks (or groupings of tasks) can be viewed with `lm-eval --tasks list`. Task descriptions and links to corresponding subfolders are provided [here](./lm_eval/tasks/README.md).

### Hugging Face `transformers`

To evaluate a model hosted on the [HuggingFace Hub](https://huggingface.co/models) (e.g. GPT-J-6B) on `hellaswag` you can use the following command (this assumes you are using a CUDA-compatible GPU):

```bash
lm_eval --model hf \
    --model_args pretrained=EleutherAI/gpt-j-6B \
    --tasks hellaswag \
    --device cuda:0 \
    --batch_size 8
```

Additional arguments can be provided to the model constructor using the `--model_args` flag. Most notably, this supports the common practice of using the `revisions` feature on the Hub to store partially trained checkpoints, or to specify the datatype for running a model:

```bash
lm_eval --model hf \
    --model_args pretrained=EleutherAI/pythia-160m,revision=step100000,dtype=&quot;float&quot; \
    --tasks lambada_openai,hellaswag \
    --device cuda:0 \
    --batch_size 8
```

Models that are loaded via both `transformers.AutoModelForCausalLM` (autoregressive, decoder-only GPT style models) and `transformers.AutoModelForSeq2SeqLM` (such as encoder-decoder models like T5) in Huggingface are supported.

Batch size selection can be automated by setting the  ```--batch_size``` flag to ```auto```. This will perform automatic detection of the largest batch size that will fit on your device. On tasks where there is a large difference between the longest and shortest example, it can be helpful to periodically recompute the largest batch size, to gain a further speedup. To do this, append ```:N``` to above flag to automatically recompute the largest batch size ```N``` times. For example, to recompute the batch size 4 times, the command would be:

```bash
lm_eval --model hf \
    --model_args pretrained=EleutherAI/pythia-160m,revision=step100000,dtype=&quot;float&quot; \
    --tasks lambada_openai,hellaswag \
    --device cuda:0 \
    --batch_size auto:4
```

&gt; [!Note]
&gt; Just like you can provide a local path to `transformers.AutoModel`, you can also provide a local path to `lm_eval` via `--model_args pretrained=/path/to/model`

#### Multi-GPU Evaluation with Hugging Face `accelerate`

We support three main ways of using Hugging Face&#039;s [accelerate üöÄ](https://github.com/huggingface/accelerate) library for multi-GPU evaluation.

To perform *data-parallel evaluation* (where each GPU loads a **separate full copy** of the model), we leverage the `accelerate` launcher as follows:

```bash
accelerate launch -m lm_eval --model hf \
    --tasks lambada_openai,arc_easy \
    --batch_size 16
```

(or via `accelerate launch --no-python lm_eval`).

For cases where your model can fit on a single GPU, this allows you to evaluate on K GPUs K times faster than on one.

**WARNING**: This setup does not work with FSDP model sharding, so in `accelerate config` FSDP must be disabled, or the NO_SHARD FSDP option must be used.

The second way of using `accelerate` for multi-GPU evaluation is when your model is *too large to fit on a single GPU.*

In this setting, run the library *outside the `accelerate` launcher*, but passing `parallelize=True` to `--model_args` as follows:

```bash
lm_eval --model hf \
    --tasks lambada_openai,arc_easy \
    --model_args parallelize=True \
    --batch_size 16
```

This means that your model&#039;s weights will be split across all available GPUs.

For more advanced users or even larger models, we allow for the following arguments when `parallelize=True` as well:

- `device_map_option`: How to split model weights across available GPUs. defaults to &quot;auto&quot;.
- `max_memory_per_gpu`: the max GPU memory to use per GPU in loading the model.
- `max_cpu_memory`: the max amount of CPU memory to use when offloading the model weights to RAM.
- `offload_folder`: a folder where model weights will be offloaded to disk if needed.

The third option is to use both at the same time. This will allow you to take advantage of both data parallelism and model sharding, and is especially useful for models that are too large to fit on a single GPU.

```bash
accelerate launch --multi_gpu --num_processes {nb_of_copies_of_your_model} \
    -m lm_eval --model hf \
    --tasks lambada_openai,arc_easy \
    --model_args parallelize=True \
    --batch_size 16
```

To learn more about model parallelism and how to use it with the `accelerate` library, see the [accelerate documentation](https://huggingface.co/docs/transformers/v4.15.0/en/parallelism)

**Warning: We do not natively support multi-node evaluation using the `hf` model type! Please reference [our GPT-NeoX library integration](https://github.com/EleutherAI/gpt-neox/blob/main/eval.py) for an example of code in which a custom multi-machine evaluation script is written.**

**Note: we do not currently support multi-node evaluations natively, and advise using either an externally hosted server to run inference requests against, or creating a custom integration with your distributed framework [as is done for the GPT-NeoX library](https://github.com/EleutherAI/gpt-neox/blob/main/eval_tasks/eval_adapter.py).**

### Steered Hugging Face `transformers` models

To evaluate a Hugging Face `transformers` model with steering vectors applied, specify the model type as `steered` and provide the path to either a PyTorch file containing pre-defined steering vectors, or a CSV file that specifies how to derive steering vectors from pretrained `sparsify` or `sae_lens` models (you will need to install the corresponding optional dependency for this method).

Specify pre-defined steering vectors:

```python
import torch

steer_config = {
    &quot;layers.3&quot;: {
        &quot;steering_vector&quot;: torch.randn(1, 768),
        &quot;bias&quot;: torch.randn(1, 768),
        &quot;steering_coefficient&quot;: 1,
        &quot;action&quot;: &quot;add&quot;
    },
}
torch.save(steer_config, &quot;steer_config.pt&quot;)
```

Specify derived steering vectors:

```python
import pandas as pd

pd.DataFrame({
    &quot;loader&quot;: [&quot;sparsify&quot;],
    &quot;action&quot;: [&quot;add&quot;],
    &quot;sparse_model&quot;: [&quot;EleutherAI/sae-pythia-70m-32k&quot;],
    &quot;hookpoint&quot;: [&quot;layers.3&quot;],
    &quot;feature_index&quot;: [30],
    &quot;steering_coefficient&quot;: [10.0],
}).to_csv(&quot;steer_config.csv&quot;, index=False)
```

Run the evaluation harness with steering vectors applied:

```bash
lm_eval --model steered \
    --model_args pretrained=EleutherAI/pythia-160m,steer_path=steer_config.pt \
    --tasks lambada_openai,hellaswag \
    --device cuda:0 \
    --batch_size 8
```

### NVIDIA `nemo` models

[NVIDIA NeMo Framework](https://github.com/NVIDIA/NeMo) is a generative AI framework built for researchers and pytorch developers working on language models.

To evaluate a `nemo` model, start by installing NeMo following [the documentation](https://github.com/NVIDIA/NeMo?tab=readme-ov-file#installation). We highly recommended to use the NVIDIA PyTorch or NeMo container, especially if having issues installing Apex or any other dependencies (see [latest released containers](https://github.com/NVIDIA/NeMo/releases)). Please also install the lm evaluation harness library following the instructions in [the Install section](https://github.com/EleutherAI/lm-evaluation-harness/tree/main?tab=readme-ov-file#install).

NeMo models can be obtained through [NVIDIA NGC Catalog](https://catalog.ngc.nvidia.com/models) or in [NVIDIA&#039;s Hugging Face page](https://huggingface.co/nvidia). In [NVIDIA NeMo Framework](https://github.com/NVIDIA/NeMo/tree/main/scripts/nlp_language_modeling) there are conversion scripts to convert the `hf` checkpoints of popular models like llama, falcon, mixtral or mpt to `nemo`.

Run a `nemo` model on one GPU:

```bash
lm_eval --model nemo_lm \
    --model_args path=&lt;path_to_nemo_model&gt; \
    --tasks hellaswag \
    --batch_size 32
```

It is recommended to unpack the `nemo` model to avoid the unpacking inside the docker container - it may overflow disk space. For that you can run:

```bash
mkdir MY_MODEL
tar -xvf MY_MODEL.nemo -c MY_MODEL
```

#### Multi-GPU evaluation with NVIDIA `nemo` models

By default, only one GPU is used. But we do support either data replication or tensor/pipeline parallelism during evaluation, on one node.

1) To enable data replication, set the `model_args` of `devices` to the number of data replicas to run. For example, the command to run 8 data replicas over 8 GPUs is:

```bash
torchrun --nproc-per-node=8 --no-python lm_eval \
    --model nemo_lm \
    --model_args path=&lt;path_to_nemo_model&gt;,devices=8 \
    --tasks hellaswag \
    --batch_size 32
```

1) To enable tensor and/or pipeline parallelism, set the `model_args` of `tensor_model_parallel_size` and/or `pipeline_model_parallel_size`. In addition, you also have to set up `devices` to be equal to the product of `tensor_model_parallel_size` and/or `pipeline_model_parallel_size`. For example, the command to use one node of 4 GPUs with tensor parallelism of 2 and pipeline parallelism of 2 is:

```bash
torchrun --nproc-per-node=4 --no-python lm_eval \
    --model nemo_lm \
    --model_args path=&lt;path_to_nemo_model&gt;,devices=4,tensor_model_parallel_size=2,pipeline_model_parallel_size=2 \
    --tasks hellaswag \
    --batch_size 32
```

Note that it is recommended to substitute the `python` command by `torchrun --nproc-per-node=&lt;number of devices&gt; --no-python` to facilitate loading the model into the GPUs. This is especially important for large checkpoints loaded into multiple GPUs.

Not supported yet: multi-node evaluation and combinations of data replication with tensor or pipeline parallelism.

#### Multi-GPU evaluation with OpenVINO models

Pipeline parallelism during evaluation is supported with OpenVINO models

To enable pipeline parallelism, set the `model_args` of `pipeline_parallel`. In addition, you also have to set up `device` to value `HETERO:&lt;GPU index1&gt;,&lt;GPU index2&gt;` for example `HETERO:GPU.1,GPU.0` For example, the command to use pipeline parallelism of 2 is:

```bash
lm_eval --model openvino \
    --tasks wikitext \
    --model_args pretrained=&lt;path_to_ov_model&gt;,pipeline_parallel=True \
    --device HETERO:GPU.1,GPU.0
```

### Tensor + Data Parallel and Optimized Inference with `vLLM`

We also support vLLM for faster inference on [supported model types](https://docs.vllm.ai/en/latest/models/supported_models.html), especially faster when splitting a model across multiple GPUs. For single-GPU or multi-GPU ‚Äî tensor parallel, data parallel, or a combination of both ‚Äî inference, for example:

```bash
lm_eval --model vllm \
    --model_args pretrained={model_name},tensor_parallel_size={GPUs_per_model},dtype=auto,gpu_memory_utilization=0.8,data_parallel_size={model_replicas} \
    --tasks lambada_openai \
    --batch_size auto
```

To use vllm, do `pip install lm_eval[vllm]`. For a full list of supported vLLM configurations, please reference our [vLLM integration](https://github.com/EleutherAI/lm-evaluation-harness/blob/e74ec966556253fbe3d8ecba9de675c77c075bce/lm_eval/models/vllm_causallms.py) and the vLLM documentation.

vLLM occasionally differs in output from Huggingface. We treat Huggingface as the reference implementation, and provide a [script](./scripts/model_comparator.py) for checking the validity of vllm results against HF.

&gt; [!Tip]
&gt; For fastest performance, we recommend using `--batch_size auto` for vLLM whenever possible, to leverage its continuous batching functionality!

&gt; [!Tip]
&gt; Passing `max_model_len=4096` or some other reasonable default to vLLM through model args may cause speedups or prevent out-of-memory errors when trying to use auto batch size, such as for Mistral-7B-v0.1 which defaults to a maximum length of 32k.

### Tensor + Data Parallel and Fast Offline Batching Inference with `SGLang`

We support SGLang for efficient offline batch inference. Its **[Fast Backend Runtime](https://docs.sglang.ai/index.html)** delivers high performance through optimized memory management and parallel processing techniques. Key features include tensor parallelism, continuous batching, and support for various quantization methods (FP8/INT4/AWQ/GPTQ).

To use SGLang as the evaluation backend, please **install it in advance** via SGLang documents [here](https://docs.sglang.ai/start/install.html#install-sglang).

&gt; [!Tip]
&gt; Due to the installing method of [`Flashinfer`](https://docs.flashinfer.ai/)-- a fast attention kernel library, we don&#039;t include the dependencies of `SGLang` within [pyproject.toml](pyproject.toml). Note that the `Flashinfer` also has some requirements on `torch` version.

SGLang&#039;s server arguments are slightly different from other backends, see [here](https://docs.sglang.ai/backend/server_arguments.html) for more information. We provide an example of the usage here:

```bash
lm_eval --model sglang \
    --model_args pretrained={model_name},dp_size={data_parallel_size},tp_size={tensor_parallel_size},dtype=auto \
    --tasks gsm8k_cot \
    --batch_size auto
```

&gt; [!Tip]
&gt; When encountering out of memory (OOM) errors (especially for multiple-choice tasks), try these solutions:
&gt;
&gt; 1. Use a manual `batch_size`, rather than `auto`.
&gt; 2. Lower KV cache pool memory usage by adjusting `mem_fraction_static` - Add to your model arguments for example `--model_args pretrained=...,mem_fraction_static=0.7`.
&gt; 3. Increase tensor parallel size `tp_size` (if using multiple GPUs).

### Model APIs and Inference Servers

Our library also supports the evaluation of models served via several commercial APIs, and we hope to implement support for the most commonly used performant local/self-hosted inference servers.

To call a hosted model, use:

```bash
export OPENAI_API_KEY=YOUR_KEY_HERE
lm_eval --model openai-completions \
    --model_args model=davinci-002 \
    --tasks lambada_openai,hellaswag
```

We also support using your own local inference server with servers that mirror the OpenAI Completions and ChatCompletions APIs.

```bash
lm_eval --model local-completions --tasks gsm8k --model_args model=facebook/opt-125m,base_url=http://{yourip}:8000/v1/completions,num_concurrent=1,max_retries=3,tokenized_requests=False,batch_size=16
```

Note that for externally hosted models, configs such as `--device` which relate to where to place a local model should not be used and do not function. Just like you can use `--model_args` to pass arbitrary arguments to the model constructor for local models, you can use it to pass arbitrary arguments to the model API for hosted models. See the documentation of the hosting service for information on what arguments they support.

| API or Inference Server                                                                                                   | Implemented?                                                                                            | `--model &lt;xxx&gt;` name                                | Models supported:                                                                   

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[gpustack/gpustack]]></title>
            <link>https://github.com/gpustack/gpustack</link>
            <guid>https://github.com/gpustack/gpustack</guid>
            <pubDate>Fri, 13 Jun 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[Simple, scalable AI model deployment on GPU clusters]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/gpustack/gpustack">gpustack/gpustack</a></h1>
            <p>Simple, scalable AI model deployment on GPU clusters</p>
            <p>Language: Python</p>
            <p>Stars: 2,890</p>
            <p>Forks: 291</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre>&lt;br&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img alt=&quot;GPUStack&quot; src=&quot;https://raw.githubusercontent.com/gpustack/gpustack/main/docs/assets/gpustack-logo.png&quot; width=&quot;300px&quot;/&gt;
&lt;/p&gt;
&lt;br&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://docs.gpustack.ai&quot; target=&quot;_blank&quot;&gt;
        &lt;img alt=&quot;Documentation&quot; src=&quot;https://img.shields.io/badge/Docs-GPUStack-blue?logo=readthedocs&amp;logoColor=white&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;./LICENSE&quot; target=&quot;_blank&quot;&gt;
        &lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/github/license/gpustack/gpustack?logo=github&amp;logoColor=white&amp;label=License&amp;color=blue&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;./docs/assets/wechat-assistant.png&quot; target=&quot;_blank&quot;&gt;
        &lt;img alt=&quot;WeChat&quot; src=&quot;https://img.shields.io/badge/ÂæÆ‰ø°Áæ§-GPUStack-blue?logo=wechat&amp;logoColor=white&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://discord.gg/VXYJzuaqwD&quot; target=&quot;_blank&quot;&gt;
        &lt;img alt=&quot;Discord&quot; src=&quot;https://img.shields.io/badge/Discord-GPUStack-blue?logo=discord&amp;logoColor=white&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://twitter.com/intent/follow?screen_name=gpustack_ai&quot; target=&quot;_blank&quot;&gt;
        &lt;img alt=&quot;Follow on X(Twitter)&quot; src=&quot;https://img.shields.io/twitter/follow/gpustack_ai?logo=X&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;br&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;./README.md&quot;&gt;English&lt;/a&gt; |
  &lt;a href=&quot;./README_CN.md&quot;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; |
  &lt;a href=&quot;./README_JP.md&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt;
&lt;/p&gt;

&lt;br&gt;

![demo](https://raw.githubusercontent.com/gpustack/gpustack/main/docs/assets/gpustack-demo.gif)

GPUStack is an open-source GPU cluster manager for running AI models.

### Key Features

- **Broad GPU Compatibility:** Seamlessly supports GPUs from various vendors across Apple Macs, Windows PCs, and Linux servers.
- **Extensive Model Support:** Supports a wide range of models including LLMs, VLMs, image models, audio models, embedding models, and rerank models.
- **Flexible Inference Backends:** Flexibly integrates with multiple inference backends including llama-box (llama.cpp &amp; stable-diffusion.cpp), vox-box, vLLM and Ascend MindIE.
- **Multi-Version Backend Support:** Run multiple versions of inference backends concurrently to meet the diverse runtime requirements of different models.
- **Distributed Inference:** Supports single-node and multi-node multi-GPU inference, including heterogeneous GPUs across vendors and runtime environments.
- **Scalable GPU Architecture:** Easily scale up by adding more GPUs or nodes to your infrastructure.
- **Robust Model Stability:** Ensures high availability with automatic failure recovery, multi-instance redundancy, and load balancing for inference requests.
- **Intelligent Deployment Evaluation:** Automatically assess model resource requirements, backend and architecture compatibility, OS compatibility, and other deployment-related factors.
- **Automated Scheduling:** Dynamically allocate models based on available resources.
- **Lightweight Python Package:** Minimal dependencies and low operational overhead.
- **OpenAI-Compatible APIs:** Fully compatible with OpenAI‚Äôs API specifications for seamless integration.
- **User &amp; API Key Management:** Simplified management of users and API keys.
- **Real-Time GPU Monitoring:** Track GPU performance and utilization in real time.
- **Token and Rate Metrics:** Monitor token usage and API request rates.

## Installation

### Linux or macOS

GPUStack provides a script to install it as a service on systemd or launchd based systems with default port 80. To install GPUStack using this method, just run:

```bash
curl -sfL https://get.gpustack.ai | sh -s -
```

### Windows

Run PowerShell as administrator (**avoid** using PowerShell ISE), then run the following command to install GPUStack:

```powershell
Invoke-Expression (Invoke-WebRequest -Uri &quot;https://get.gpustack.ai&quot; -UseBasicParsing).Content
```

### Other Installation Methods

For manual installation, docker installation or detailed configuration options, please refer to the [Installation Documentation](https://docs.gpustack.ai/latest/installation/installation-script/).

## Getting Started

1. Run and chat with the **llama3.2** model:

```bash
gpustack chat llama3.2 &quot;tell me a joke.&quot;
```

2. Run and generate an image with the **stable-diffusion-v3-5-large-turbo** model:

&gt; ### üí° Tip
&gt;
&gt; This command downloads the model (~12GB) from Hugging Face. The download time depends on your network speed. Ensure you have enough disk space and VRAM (12GB) to run the model. If you encounter issues, you can skip this step and move to the next one.

```bash
gpustack draw hf.co/gpustack/stable-diffusion-v3-5-large-turbo-GGUF:stable-diffusion-v3-5-large-turbo-Q4_0.gguf \
&quot;A minion holding a sign that says &#039;GPUStack&#039;. The background is filled with futuristic elements like neon lights, circuit boards, and holographic displays. The minion is wearing a tech-themed outfit, possibly with LED lights or digital patterns. The sign itself has a sleek, modern design with glowing edges. The overall atmosphere is high-tech and vibrant, with a mix of dark and neon colors.&quot; \
--sample-steps 5 --show
```

Once the command completes, the generated image will appear in the default viewer. You can experiment with the prompt and CLI options to customize the output.

![Generated Image](https://raw.githubusercontent.com/gpustack/gpustack/main/docs/assets/quickstart-minion.png)

3. Open `http://your_host_ip` in the browser to access the GPUStack UI. Log in to GPUStack with username `admin` and the default password. You can run the following command to get the password for the default setup:

**Linux or macOS**

```bash
cat /var/lib/gpustack/initial_admin_password
```

**Windows**

```powershell
Get-Content -Path &quot;$env:APPDATA\gpustack\initial_admin_password&quot; -Raw
```

4. Click `Playground - Chat` in the navigation menu. Now you can chat with the LLM in the UI playground.

![Playground Screenshot](https://raw.githubusercontent.com/gpustack/gpustack/main/docs/assets/playground-screenshot.png)

5. Click `API Keys` in the navigation menu, then click the `New API Key` button.

6. Fill in the `Name` and click the `Save` button.

7. Copy the generated API key and save it somewhere safe. Please note that you can only see it once on creation.

8. Now you can use the API key to access the OpenAI-compatible API. For example, use curl as the following:

```bash
export GPUSTACK_API_KEY=your_api_key
curl http://your_gpustack_server_url/v1-openai/chat/completions \
  -H &quot;Content-Type: application/json&quot; \
  -H &quot;Authorization: Bearer $GPUSTACK_API_KEY&quot; \
  -d &#039;{
    &quot;model&quot;: &quot;llama3.2&quot;,
    &quot;messages&quot;: [
      {
        &quot;role&quot;: &quot;system&quot;,
        &quot;content&quot;: &quot;You are a helpful assistant.&quot;
      },
      {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;Hello!&quot;
      }
    ],
    &quot;stream&quot;: true
  }&#039;
```

## Supported Platforms

- [x] macOS
- [x] Linux
- [x] Windows

## Supported Accelerators

- [x] NVIDIA CUDA ([Compute Capability](https://developer.nvidia.com/cuda-gpus) 6.0 and above)
- [x] Apple Metal (M-series chips)
- [x] AMD ROCm
- [x] Ascend CANN
- [x] Hygon DTK
- [x] Moore Threads MUSA
- [x] Iluvatar Corex

We plan to support the following accelerators in future releases.

- [ ] Intel oneAPI
- [ ] Qualcomm AI Engine

## Supported Models

GPUStack uses [llama-box](https://github.com/gpustack/llama-box) (bundled [llama.cpp](https://github.com/ggml-org/llama.cpp) and [stable-diffusion.cpp](https://github.com/leejet/stable-diffusion.cpp) server), [vLLM](https://github.com/vllm-project/vllm), [Ascend MindIE](https://www.hiascend.com/en/software/mindie) and [vox-box](https://github.com/gpustack/vox-box) as the backends and supports a wide range of models. Models from the following sources are supported:

1. [Hugging Face](https://huggingface.co/)

2. [ModelScope](https://modelscope.cn/)

3. Local File Path

### Example Models:

| **Category**                     | **Models**                                                                                                                                                                                                                                                                                                                                           |
| -------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Large Language Models(LLMs)**  | [Qwen](https://huggingface.co/models?search=Qwen/Qwen), [LLaMA](https://huggingface.co/meta-llama), [Mistral](https://huggingface.co/mistralai), [DeepSeek](https://huggingface.co/models?search=deepseek-ai/deepseek), [Phi](https://huggingface.co/models?search=microsoft/phi), [Gemma](https://huggingface.co/models?search=Google/gemma)        |
| **Vision Language Models(VLMs)** | [Llama3.2-Vision](https://huggingface.co/models?pipeline_tag=image-text-to-text&amp;search=llama3.2), [Pixtral](https://huggingface.co/models?search=pixtral) , [Qwen2.5-VL](https://huggingface.co/models?search=Qwen/Qwen2.5-VL), [LLaVA](https://huggingface.co/models?search=llava), [InternVL2.5](https://huggingface.co/models?search=internvl2_5) |
| **Diffusion Models**             | [Stable Diffusion](https://huggingface.co/models?search=gpustack/stable-diffusion), [FLUX](https://huggingface.co/models?search=gpustack/flux)                                                                                                                                                                                                       |
| **Embedding Models**             | [BGE](https://huggingface.co/gpustack/bge-m3-GGUF), [BCE](https://huggingface.co/gpustack/bce-embedding-base_v1-GGUF), [Jina](https://huggingface.co/models?search=gpustack/jina-embeddings)                                                                                                                                                         |
| **Reranker Models**              | [BGE](https://huggingface.co/gpustack/bge-reranker-v2-m3-GGUF), [BCE](https://huggingface.co/gpustack/bce-reranker-base_v1-GGUF), [Jina](https://huggingface.co/models?search=gpustack/jina-reranker)                                                                                                                                                |
| **Audio Models**                 | [Whisper](https://huggingface.co/models?search=Systran/faster) (Speech-to-Text), [CosyVoice](https://huggingface.co/models?search=FunAudioLLM/CosyVoice) (Text-to-Speech)                                                                                                                                                                            |

For full list of supported models, please refer to the supported models section in the [inference backends](https://docs.gpustack.ai/latest/user-guide/inference-backends/) documentation.

## OpenAI-Compatible APIs

GPUStack serves the following OpenAI compatible APIs under the `/v1-openai` path:

- [x] [List Models](https://platform.openai.com/docs/api-reference/models/list)
- [x] [Create Completion](https://platform.openai.com/docs/api-reference/completions/create)
- [x] [Create Chat Completion](https://platform.openai.com/docs/api-reference/chat/create)
- [x] [Create Embeddings](https://platform.openai.com/docs/api-reference/embeddings/create)
- [x] [Create Image](https://platform.openai.com/docs/api-reference/images/create)
- [x] [Create Image Edit](https://platform.openai.com/docs/api-reference/images/createEdit)
- [x] [Create Speech](https://platform.openai.com/docs/api-reference/audio/createSpeech)
- [x] [Create Transcription](https://platform.openai.com/docs/api-reference/audio/createTranscription)

For example, you can use the official [OpenAI Python API library](https://github.com/openai/openai-python) to consume the APIs:

```python
from openai import OpenAI
client = OpenAI(base_url=&quot;http://your_gpustack_server_url/v1-openai&quot;, api_key=&quot;your_api_key&quot;)

completion = client.chat.completions.create(
  model=&quot;llama3.2&quot;,
  messages=[
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}
  ]
)

print(completion.choices[0].message)
```

GPUStack users can generate their own API keys in the UI.

## Documentation

Please see the [official docs site](https://docs.gpustack.ai) for complete documentation.

## Build

1. Install Python (version 3.10 to 3.12).

2. Run `make build`.

You can find the built wheel package in `dist` directory.

## Contributing

Please read the [Contributing Guide](./docs/contributing.md) if you&#039;re interested in contributing to GPUStack.

## Join Community

Any issues or have suggestions, feel free to join our [Community](https://discord.gg/VXYJzuaqwD) for support.

## License

Copyright (c) 2024 The GPUStack authors

Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at [LICENSE](./LICENSE) file for details.

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[AtsushiSakai/PythonRobotics]]></title>
            <link>https://github.com/AtsushiSakai/PythonRobotics</link>
            <guid>https://github.com/AtsushiSakai/PythonRobotics</guid>
            <pubDate>Fri, 13 Jun 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[Python sample codes and textbook for robotics algorithms.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/AtsushiSakai/PythonRobotics">AtsushiSakai/PythonRobotics</a></h1>
            <p>Python sample codes and textbook for robotics algorithms.</p>
            <p>Language: Python</p>
            <p>Stars: 25,184</p>
            <p>Forks: 6,796</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre>&lt;img src=&quot;https://github.com/AtsushiSakai/PythonRobotics/raw/master/icon.png?raw=true&quot; align=&quot;right&quot; width=&quot;300&quot; alt=&quot;header pic&quot;/&gt;

# PythonRobotics
![GitHub_Action_Linux_CI](https://github.com/AtsushiSakai/PythonRobotics/workflows/Linux_CI/badge.svg)
![GitHub_Action_MacOS_CI](https://github.com/AtsushiSakai/PythonRobotics/workflows/MacOS_CI/badge.svg)
![GitHub_Action_Windows_CI](https://github.com/AtsushiSakai/PythonRobotics/workflows/Windows_CI/badge.svg)
[![Build status](https://ci.appveyor.com/api/projects/status/sb279kxuv1be391g?svg=true)](https://ci.appveyor.com/project/AtsushiSakai/pythonrobotics)

Python codes and [textbook](https://atsushisakai.github.io/PythonRobotics/index.html) for robotics algorithm.


# Table of Contents
   * [What is this?](#what-is-this)
   * [Requirements](#requirements)
   * [Documentation](#documentation)
   * [How to use](#how-to-use)
   * [Localization](#localization)
      * [Extended Kalman Filter localization](#extended-kalman-filter-localization)
      * [Particle filter localization](#particle-filter-localization)
      * [Histogram filter localization](#histogram-filter-localization)
   * [Mapping](#mapping)
      * [Gaussian grid map](#gaussian-grid-map)
      * [Ray casting grid map](#ray-casting-grid-map)
      * [Lidar to grid map](#lidar-to-grid-map)
      * [k-means object clustering](#k-means-object-clustering)
      * [Rectangle fitting](#rectangle-fitting)
   * [SLAM](#slam)
      * [Iterative Closest Point (ICP) Matching](#iterative-closest-point-icp-matching)
      * [FastSLAM 1.0](#fastslam-10)
   * [Path Planning](#path-planning)
      * [Dynamic Window Approach](#dynamic-window-approach)
      * [Grid based search](#grid-based-search)
         * [Dijkstra algorithm](#dijkstra-algorithm)
         * [A* algorithm](#a-algorithm)
         * [D* algorithm](#d-algorithm)
         * [D* Lite algorithm](#d-lite-algorithm)
         * [Potential Field algorithm](#potential-field-algorithm)
         * [Grid based coverage path planning](#grid-based-coverage-path-planning)
      * [State Lattice Planning](#state-lattice-planning)
         * [Biased polar sampling](#biased-polar-sampling)
         * [Lane sampling](#lane-sampling)
      * [Probabilistic Road-Map (PRM) planning](#probabilistic-road-map-prm-planning)
      * [Rapidly-Exploring Random Trees (RRT)](#rapidly-exploring-random-trees-rrt)
         * [RRT*](#rrt)
         * [RRT* with reeds-shepp path](#rrt-with-reeds-shepp-path)
         * [LQR-RRT*](#lqr-rrt)
      * [Quintic polynomials planning](#quintic-polynomials-planning)
      * [Reeds Shepp planning](#reeds-shepp-planning)
      * [LQR based path planning](#lqr-based-path-planning)
      * [Optimal Trajectory in a Frenet Frame](#optimal-trajectory-in-a-frenet-frame)
   * [Path Tracking](#path-tracking)
      * [move to a pose control](#move-to-a-pose-control)
      * [Stanley control](#stanley-control)
      * [Rear wheel feedback control](#rear-wheel-feedback-control)
      * [Linear‚Äìquadratic regulator (LQR) speed and steering control](#linearquadratic-regulator-lqr-speed-and-steering-control)
      * [Model predictive speed and steering control](#model-predictive-speed-and-steering-control)
      * [Nonlinear Model predictive control with C-GMRES](#nonlinear-model-predictive-control-with-c-gmres)
   * [Arm Navigation](#arm-navigation)
      * [N joint arm to point control](#n-joint-arm-to-point-control)
      * [Arm navigation with obstacle avoidance](#arm-navigation-with-obstacle-avoidance)
   * [Aerial Navigation](#aerial-navigation)
      * [drone 3d trajectory following](#drone-3d-trajectory-following)
      * [rocket powered landing](#rocket-powered-landing)
   * [Bipedal](#bipedal)
      * [bipedal planner with inverted pendulum](#bipedal-planner-with-inverted-pendulum)
   * [License](#license)
   * [Use-case](#use-case)
   * [Contribution](#contribution)
   * [Citing](#citing)
   * [Support](#support)
   * [Sponsors](#sponsors)
      * [JetBrains](#JetBrains)
      * [1Password](#1password)
   * [Authors](#authors)

# What is PythonRobotics?

PythonRobotics is a Python code collection and a [textbook](https://atsushisakai.github.io/PythonRobotics/index.html) of robotics algorithms.

Features:

1. Easy to read for understanding each algorithm&#039;s basic idea.

2. Widely used and practical algorithms are selected.

3. Minimum dependency.

See this documentation 

- [Getting Started ‚Äî PythonRobotics documentation](https://atsushisakai.github.io/PythonRobotics/modules/0_getting_started/1_what_is_python_robotics.html)

or this Youtube video:

- [PythonRobotics project audio overview](https://www.youtube.com/watch?v=uMeRnNoJAfU)

or this paper for more details:

- [\[1808\.10703\] PythonRobotics: a Python code collection of robotics algorithms](https://arxiv.org/abs/1808.10703) ([BibTeX](https://github.com/AtsushiSakai/PythonRoboticsPaper/blob/master/python_robotics.bib))


# Requirements to run the code

For running each sample code:

- [Python 3.13.x](https://www.python.org/)
 
- [NumPy](https://numpy.org/)
 
- [SciPy](https://scipy.org/)
 
- [Matplotlib](https://matplotlib.org/)
 
- [cvxpy](https://www.cvxpy.org/) 

For development:
  
- [pytest](https://pytest.org/) (for unit tests)
  
- [pytest-xdist](https://pypi.org/project/pytest-xdist/) (for parallel unit tests)
  
- [mypy](https://mypy-lang.org/) (for type check)
  
- [sphinx](https://www.sphinx-doc.org/) (for document generation)
  
- [pycodestyle](https://pypi.org/project/pycodestyle/) (for code style check)

# Documentation (Textbook)

This README only shows some examples of this project. 

If you are interested in other examples or mathematical backgrounds of each algorithm, 

You can check the full documentation (textbook) online: [Welcome to PythonRobotics‚Äôs documentation\! ‚Äî PythonRobotics documentation](https://atsushisakai.github.io/PythonRobotics/index.html)

All animation gifs are stored here: [AtsushiSakai/PythonRoboticsGifs: Animation gifs of PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs)

# How to use

1. Clone this repo.

   ```terminal
   git clone https://github.com/AtsushiSakai/PythonRobotics.git
   ```


2. Install the required libraries.

- using conda :

  ```terminal
  conda env create -f requirements/environment.yml
  ```
 
- using pip :

  ```terminal
  pip install -r requirements/requirements.txt
  ```


3. Execute python script in each directory.

4. Add star to this repo if you like it :smiley:. 

# Localization

## Extended Kalman Filter localization

&lt;img src=&quot;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/extended_kalman_filter/animation.gif&quot; width=&quot;640&quot; alt=&quot;EKF pic&quot;&gt;

Reference

- [documentation](https://atsushisakai.github.io/PythonRobotics/modules/2_localization/extended_kalman_filter_localization_files/extended_kalman_filter_localization.html)

## Particle filter localization

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/particle_filter/animation.gif)

This is a sensor fusion localization with Particle Filter(PF).

The blue line is true trajectory, the black line is dead reckoning trajectory,

and the red line is an estimated trajectory with PF.

It is assumed that the robot can measure a distance from landmarks (RFID).

These measurements are used for PF localization.

Reference

- [PROBABILISTIC ROBOTICS](http://www.probabilistic-robotics.org/)


## Histogram filter localization

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Localization/histogram_filter/animation.gif)

This is a 2D localization example with Histogram filter.

The red cross is true position, black points are RFID positions.

The blue grid shows a position probability of histogram filter.  

In this simulation, x,y are unknown, yaw is known.

The filter integrates speed input and range observations from RFID for localization.

Initial position is not needed.

Reference

- [PROBABILISTIC ROBOTICS](http://www.probabilistic-robotics.org/)

# Mapping

## Gaussian grid map

This is a 2D Gaussian grid mapping example.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/gaussian_grid_map/animation.gif)

## Ray casting grid map

This is a 2D ray casting grid mapping example.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/raycasting_grid_map/animation.gif)

## Lidar to grid map

This example shows how to convert a 2D range measurement to a grid map.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/lidar_to_grid_map/animation.gif)

## k-means object clustering

This is a 2D object clustering with k-means algorithm.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/kmeans_clustering/animation.gif)

## Rectangle fitting

This is a 2D rectangle fitting for vehicle detection.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Mapping/rectangle_fitting/animation.gif)


# SLAM

Simultaneous Localization and Mapping(SLAM) examples

## Iterative Closest Point (ICP) Matching

This is a 2D ICP matching example with singular value decomposition.

It can calculate a rotation matrix, and a translation vector between points and points.

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/iterative_closest_point/animation.gif)

Reference

- [Introduction to Mobile Robotics: Iterative Closest Point Algorithm](https://cs.gmu.edu/~kosecka/cs685/cs685-icp.pdf)


## FastSLAM 1.0

This is a feature based SLAM example using FastSLAM 1.0.

The blue line is ground truth, the black line is dead reckoning, the red line is the estimated trajectory with FastSLAM.

The red points are particles of FastSLAM.

Black points are landmarks, blue crosses are estimated landmark positions by FastSLAM.


![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/SLAM/FastSLAM1/animation.gif)


Reference

- [PROBABILISTIC ROBOTICS](http://www.probabilistic-robotics.org/)

- [SLAM simulations by Tim Bailey](http://www-personal.acfr.usyd.edu.au/tbailey/software/slam_simulations.htm)


# Path Planning

## Dynamic Window Approach

This is a 2D navigation sample code with Dynamic Window Approach.

- [The Dynamic Window Approach to Collision Avoidance](https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf)

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DynamicWindowApproach/animation.gif)


## Grid based search

### Dijkstra algorithm

This is a 2D grid based the shortest path planning with Dijkstra&#039;s algorithm.

![PythonRobotics/figure_1.png at master ¬∑ AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/Dijkstra/animation.gif)

In the animation, cyan points are searched nodes.

### A\* algorithm

This is a 2D grid based the shortest path planning with A star algorithm.

![PythonRobotics/figure_1.png at master ¬∑ AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/AStar/animation.gif)

In the animation, cyan points are searched nodes.

Its heuristic is 2D Euclid distance.

### D\* algorithm

This is a 2D grid based the shortest path planning with D star algorithm.

![figure at master ¬∑ nirnayroy/intelligentrobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DStar/animation.gif)

The animation shows a robot finding its path avoiding an obstacle using the D* search algorithm.

Reference

- [D* Algorithm Wikipedia](https://en.wikipedia.org/wiki/D*)

### D\* Lite algorithm

This algorithm finds the shortest path between two points while rerouting when obstacles are discovered. It has been implemented here for a 2D grid.

![D* Lite](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/DStarLite/animation.gif)

The animation shows a robot finding its path and rerouting to avoid obstacles as they are discovered using the D* Lite search algorithm.

Refs:

- [D* Lite](http://idm-lab.org/bib/abstracts/papers/aaai02b.pdf)
- [Improved Fast Replanning for Robot Navigation in Unknown Terrain](http://www.cs.cmu.edu/~maxim/files/dlite_icra02.pdf)

### Potential Field algorithm

This is a 2D grid based path planning with Potential Field algorithm.

![PotentialField](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/PotentialFieldPlanning/animation.gif)

In the animation, the blue heat map shows potential value on each grid.

Reference

- [Robotic Motion Planning:Potential Functions](https://www.cs.cmu.edu/~motionplanning/lecture/Chap4-Potential-Field_howie.pdf)

### Grid based coverage path planning

This is a 2D grid based coverage path planning simulation.

![PotentialField](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/GridBasedSweepCPP/animation.gif)

## State Lattice Planning

This script is a path planning code with state lattice planning.

This code uses the model predictive trajectory generator to solve boundary problem.

Reference 

- [Optimal rough terrain trajectory generation for wheeled mobile robots](https://journals.sagepub.com/doi/pdf/10.1177/0278364906075328)

- [State Space Sampling of Feasible Motions for High-Performance Mobile Robot Navigation in Complex Environments](https://www.frc.ri.cmu.edu/~alonzo/pubs/papers/JFR_08_SS_Sampling.pdf)


### Biased polar sampling

![PythonRobotics/figure_1.png at master ¬∑ AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/BiasedPolarSampling.gif)


### Lane sampling

![PythonRobotics/figure_1.png at master ¬∑ AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/StateLatticePlanner/LaneSampling.gif)

## Probabilistic Road-Map (PRM) planning 

![PRM](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ProbabilisticRoadMap/animation.gif)

This PRM planner uses Dijkstra method for graph search.

In the animation, blue points are sampled points,

Cyan crosses means searched points with Dijkstra method,

The red line is the final path of PRM.

Reference

- [Probabilistic roadmap \- Wikipedia](https://en.wikipedia.org/wiki/Probabilistic_roadmap)

„ÄÄ„ÄÄ

## Rapidly-Exploring Random Trees (RRT)

### RRT\*

![PythonRobotics/figure_1.png at master ¬∑ AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTstar/animation.gif)

This is a path planning code with RRT\*

Black circles are obstacles, green line is a searched tree, red crosses are start and goal positions.

Reference

- [Incremental Sampling-based Algorithms for Optimal Motion Planning](https://arxiv.org/abs/1005.0416)

- [Sampling-based Algorithms for Optimal Motion Planning](https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=bddbc99f97173430aa49a0ada53ab5bade5902fa)

### RRT\* with reeds-shepp path

![Robotics/animation.gif at master ¬∑ AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/RRTStarReedsShepp/animation.gif)

Path planning for a car robot with RRT\* and reeds shepp path planner.

### LQR-RRT\*

This is a path planning simulation with LQR-RRT\*.

A double integrator motion model is used for LQR local planner.

![LQR_RRT](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRRRTStar/animation.gif)

Reference

- [LQR\-RRT\*: Optimal Sampling\-Based Motion Planning with Automatically Derived Extension Heuristics](https://lis.csail.mit.edu/pubs/perez-icra12.pdf)

- [MahanFathi/LQR\-RRTstar: LQR\-RRT\* method is used for random motion planning of a simple pendulum in its phase plot](https://github.com/MahanFathi/LQR-RRTstar)


## Quintic polynomials planning

Motion planning with quintic polynomials.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/QuinticPolynomialsPlanner/animation.gif)

It can calculate a 2D path, velocity, and acceleration profile based on quintic polynomials.

Reference

- [Local Path Planning And Motion Control For Agv In Positioning](https://ieeexplore.ieee.org/document/637936/)

## Reeds Shepp planning

A sample code with Reeds Shepp path planning.

![RSPlanning](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/ReedsSheppPath/animation.gif?raw=true)

Reference

- [15.3.2 Reeds\-Shepp Curves](http://planning.cs.uiuc.edu/node822.html) 

- [optimal paths for a car that goes both forwards and backwards](https://pdfs.semanticscholar.org/932e/c495b1d0018fd59dee12a0bf74434fac7af4.pdf)

- [ghliu/pyReedsShepp: Implementation of Reeds Shepp curve\.](https://github.com/ghliu/pyReedsShepp)


## LQR based path planning

A sample code using LQR based path planning for double integrator model.

![RSPlanning](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/LQRPlanner/animation.gif?raw=true)


## Optimal Trajectory in a Frenet Frame 

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathPlanning/FrenetOptimalTrajectory/animation.gif)

This is optimal trajectory generation in a Frenet Frame.

The cyan line is the target course and black crosses are obstacles.

The red line is the predicted path.

Reference

- [Optimal Trajectory Generation for Dynamic Street Scenarios in a Frenet Frame](https://www.researchgate.net/profile/Moritz_Werling/publication/224156269_Optimal_Trajectory_Generation_for_Dynamic_Street_Scenarios_in_a_Frenet_Frame/links/54f749df0cf210398e9277af.pdf)

- [Optimal trajectory generation for dynamic street scenarios in a Frenet Frame](https://www.youtube.com/watch?v=Cj6tAQe7UCY)


# Path Tracking

## move to a pose control

This is a simulation of moving to a pose control

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/Control/move_to_pose/animation.gif)

Reference

- [P. I. Corke, &quot;Robotics, Vision and Control&quot; \| SpringerLink p102](https://link.springer.com/book/10.1007/978-3-642-20144-8)


## Stanley control

Path tracking simulation with Stanley steering control and PID speed control.

![2](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/stanley_controller/animation.gif)

Reference

- [Stanley: The robot that won the DARPA grand challenge](http://robots.stanford.edu/papers/thrun.stanley05.pdf)

- [Automatic Steering Methods for Autonomous Automobile Path Tracking](https://www.ri.cmu.edu/pub_files/2009/2/Automatic_Steering_Methods_for_Autonomous_Automobile_Path_Tracking.pdf)



## Rear wheel feedback control

Path tracking simulation with rear wheel feedback steering control and PID speed control.

![PythonRobotics/figure_1.png at master ¬∑ AtsushiSakai/PythonRobotics](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/rear_wheel_feedback/animation.gif)

Reference

- [A Survey of Motion Planning and Control Techniques for Self-driving Urban Vehicles](https://arxiv.org/abs/1604.07446)


## Linear‚Äìquadratic regulator (LQR) speed and steering control

Path tracking simulation with LQR speed and steering control.

![3](https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/lqr_speed_steer_control/animation.gif)

Reference

- [Towards fully autonomous driving: Systems and algorithms \- IEEE Conference Publication](https://ieeexplore.ieee.org/document/5940562/)


## Model predictive speed and steering control

Path tracking simulation with iterative linear model predictive speed and steering control.

&lt;img src=&quot;https://github.com/AtsushiSakai/PythonRoboticsGifs/raw/master/PathTracking/model_predictive_speed_and_steer_control/animation.gif&quot; width=&quot;640&quot; alt=&quot;MPC pic&quot;&gt;

Reference

- [documentation](https://atsushisakai.github.io/PythonRobotics/modules/6_path_tracking/model_predictive_speed_and_steering_control/model_predictive_speed_and_steering_control.html)

- [Real\-time Model Predictive Control \(MPC\), ACADO, Python \| Work\-is\-Playing](http:/

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>