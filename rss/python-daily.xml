<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sun, 11 Jan 2026 00:05:40 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[NevaMind-AI/memU]]></title>
            <link>https://github.com/NevaMind-AI/memU</link>
            <guid>https://github.com/NevaMind-AI/memU</guid>
            <pubDate>Sun, 11 Jan 2026 00:05:40 GMT</pubDate>
            <description><![CDATA[Memory infrastructure for LLMs and AI agents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NevaMind-AI/memU">NevaMind-AI/memU</a></h1>
            <p>Memory infrastructure for LLMs and AI agents</p>
            <p>Language: Python</p>
            <p>Stars: 4,260</p>
            <p>Forks: 281</p>
            <p>Stars today: 78 stars today</p>
            <h2>README</h2><pre>![MemU Banner](assets/banner.png)

&lt;div align=&quot;center&quot;&gt;

# MemU

### A Future-Oriented Agentic Memory System

[![PyPI version](https://badge.fury.io/py/memu-py.svg)](https://badge.fury.io/py/memu-py)
[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python 3.13+](https://img.shields.io/badge/python-3.13+-blue.svg)](https://www.python.org/downloads/)
[![Discord](https://img.shields.io/badge/Discord-Join%20Chat-5865F2?logo=discord&amp;logoColor=white)](https://discord.gg/memu)
[![Twitter](https://img.shields.io/badge/Twitter-Follow-1DA1F2?logo=x&amp;logoColor=white)](https://x.com/memU_ai)

&lt;a href=&quot;https://trendshift.io/repositories/17374&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/17374&quot; alt=&quot;NevaMind-AI%2FmemU | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;

---

MemU is an agentic memory framework for LLM and AI agent backends. It receives **multimodal inputs** (conversations, documents, images), extracts them into structured memory, and organizes them into a **hierarchical file system** that supports both **embedding-based (RAG)** and **non-embedding (LLM)** retrieval.

---

## ‚≠êÔ∏è Star the repository

&lt;img width=&quot;100%&quot; src=&quot;https://github.com/NevaMind-AI/memU/blob/main/assets/star.gif&quot; /&gt;
If you find memU useful or interesting, a GitHub Star ‚≠êÔ∏è would be greatly appreciated.

---

MemU is collaborating with four open-source projects to launch the 2026 New Year Challenge. üéâBetween January 8‚Äì18, contributors can submit PRs to memU and earn cash rewards, community recognition, and platform credits. üéÅ[Learn more &amp; get involved](https://discord.gg/KaWy6SBAsx)

## ‚ú® Core Features

| Feature | Description |
|---------|-------------|
| üóÇÔ∏è **Hierarchical File System** | Three-layer architecture: Resource ‚Üí Item ‚Üí Category with full traceability |
| üîç **Dual Retrieval Methods** | RAG (embedding-based) for speed, LLM (non-embedding) for deep semantic understanding |
| üé® **Multimodal Support** | Process conversations, documents, images, audio, and video |
| üîÑ **Self-Evolving Memory** | Memory structure adapts and improves based on usage patterns |

---

## üóÇÔ∏è Hierarchical File System

MemU organizes memory using a **three-layer architecture** inspired by hierarchical storage systems:

&lt;img width=&quot;100%&quot; alt=&quot;structure&quot; src=&quot;assets/structure.png&quot; /&gt;

| Layer | Description | Examples |
|-------|-------------|----------|
| **Resource** | Raw multimodal data warehouse | JSON conversations, text documents, images, videos |
| **Item** | Discrete extracted memory units | Individual preferences, skills, opinions, habits |
| **Category** | Aggregated textual memory with summaries | `preferences.md`, `work_life.md`, `relationships.md` |

**Key Benefits:**
- **Full Traceability**: Track from raw data ‚Üí items ‚Üí categories and back
- **Progressive Summarization**: Each layer provides increasingly abstracted views
- **Flexible Organization**: Categories evolve based on content patterns

---

## üé® Multimodal Support

MemU processes diverse content types into unified memory:

| Modality | Input | Processing |
|----------|-------|------------|
| `conversation` | JSON chat logs | Extract preferences, opinions, habits, relationships |
| `document` | Text files (.txt, .md) | Extract knowledge, skills, facts |
| `image` | PNG, JPG, etc. | Vision model extracts visual concepts and descriptions |
| `video` | Video files | Frame extraction + vision analysis |
| `audio` | Audio files | Transcription + text processing |

All modalities are unified into the same three-layer hierarchy, enabling cross-modal retrieval.

---

## üöÄ Quick Start

### Option 1: Cloud Version

Try MemU instantly without any setup:

üëâ **[memu.so](https://memu.so)** - Hosted cloud service with full API access

For enterprise deployment and custom solutions, contact **info@nevamind.ai**

#### Cloud API (v3)

| Base URL | `https://api.memu.so` |
|----------|----------------------|
| Auth | `Authorization: Bearer YOUR_API_KEY` |

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/v3/memory/memorize` | Register a memorization task |
| `GET` | `/api/v3/memory/memorize/status/{task_id}` | Get task status |
| `POST` | `/api/v3/memory/categories` | List memory categories |
| `POST` | `/api/v3/memory/retrieve` | Retrieve memories (semantic search) |

üìö **[Full API Documentation](https://memu.pro/docs#cloud-version)**

---

### Option 2: Self-Hosted

#### Installation

```bash
pip install -e .
```

#### Basic Example

&gt; **Requirements**: Python 3.13+ and an OpenAI API key

**Test with In-Memory Storage** (no database required):

```bash
export OPENAI_API_KEY=your_api_key
cd tests
python test_inmemory.py
```

**Test with PostgreSQL Storage** (requires pgvector):

```bash
# Start PostgreSQL with pgvector
docker run -d \
  --name memu-postgres \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_DB=memu \
  -p 5432:5432 \
  pgvector/pgvector:pg16

# Run the test
export OPENAI_API_KEY=your_api_key
cd tests
python test_postgres.py
```

Both examples demonstrate the complete workflow:
1. **Memorize**: Process a conversation file and extract structured memory
2. **Retrieve (RAG)**: Fast embedding-based search
3. **Retrieve (LLM)**: Deep semantic understanding search

See [`tests/test_inmemory.py`](tests/test_inmemory.py) and [`tests/test_postgres.py`](tests/test_postgres.py) for the full source code.

---

### Custom LLM and Embedding Providers

MemU supports custom LLM and embedding providers beyond OpenAI. Configure them via `llm_profiles`:

```python
from memu import MemUService

service = MemUService(
    llm_profiles={
        # Default profile for LLM operations
        &quot;default&quot;: {
            &quot;base_url&quot;: &quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,
            &quot;api_key&quot;: &quot;your_api_key&quot;,
            &quot;chat_model&quot;: &quot;qwen3-max&quot;,
            &quot;client_backend&quot;: &quot;sdk&quot;  # &quot;sdk&quot; or &quot;http&quot;
        },
        # Separate profile for embeddings
        &quot;embedding&quot;: {
            &quot;base_url&quot;: &quot;https://api.voyageai.com/v1&quot;,
            &quot;api_key&quot;: &quot;your_voyage_api_key&quot;,
            &quot;embed_model&quot;: &quot;voyage-3.5-lite&quot;
        }
    },
    # ... other configuration
)
```

---

## üìñ Core APIs

### `memorize()` - Extract and Store Memory

Processes input resources and extracts structured memory:

&lt;img width=&quot;100%&quot; alt=&quot;memorize&quot; src=&quot;assets/memorize.png&quot; /&gt;

```python
result = await service.memorize(
    resource_url=&quot;path/to/file.json&quot;,  # File path or URL
    modality=&quot;conversation&quot;,            # conversation | document | image | video | audio
    user={&quot;user_id&quot;: &quot;123&quot;}             # Optional: scope to a user
)

# Returns:
{
    &quot;resource&quot;: {...},      # Stored resource metadata
    &quot;items&quot;: [...],         # Extracted memory items
    &quot;categories&quot;: [...]     # Updated category summaries
}
```

### `retrieve()` - Query Memory

Retrieves relevant memory based on queries. MemU supports **two retrieval strategies**:

&lt;img width=&quot;100%&quot; alt=&quot;retrieve&quot; src=&quot;assets/retrieve.png&quot; /&gt;

#### RAG-based Retrieval (`method=&quot;rag&quot;`)

Fast **embedding vector search** using cosine similarity:

- ‚úÖ **Fast**: Pure vector computation
- ‚úÖ **Scalable**: Efficient for large memory stores
- ‚úÖ **Returns scores**: Each result includes similarity score

#### LLM-based Retrieval (`method=&quot;llm&quot;`)

Deep **semantic understanding** through direct LLM reasoning:

- ‚úÖ **Deep understanding**: LLM comprehends context and nuance
- ‚úÖ **Query rewriting**: Automatically refines query at each tier
- ‚úÖ **Adaptive**: Stops early when sufficient information is found

#### Comparison

| Aspect | RAG | LLM |
|--------|-----|-----|
| **Speed** | ‚ö° Fast | üê¢ Slower |
| **Cost** | üí∞ Low | üí∞üí∞ Higher |
| **Semantic depth** | Medium | Deep |
| **Tier 2 scope** | All items | Only items in relevant categories |
| **Output** | With similarity scores | Ranked by LLM reasoning |

Both methods support:
- **Context-aware rewriting**: Resolves pronouns using conversation history
- **Progressive search**: Categories ‚Üí Items ‚Üí Resources
- **Sufficiency checking**: Stops when enough information is retrieved

#### Usage

```python
result = await service.retrieve(
    queries=[
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: {&quot;text&quot;: &quot;What are their preferences?&quot;}},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: {&quot;text&quot;: &quot;Tell me about work habits&quot;}}
    ],
    where={&quot;user_id&quot;: &quot;123&quot;}  # Optional: scope filter
)

# Returns:
{
    &quot;categories&quot;: [...],     # Relevant categories (with scores for RAG)
    &quot;items&quot;: [...],          # Relevant memory items
    &quot;resources&quot;: [...],      # Related raw resources
    &quot;next_step_query&quot;: &quot;...&quot; # Rewritten query for follow-up (if applicable)
}
```

**Scope Filtering**: Use `where` to filter by user model fields:
- `where={&quot;user_id&quot;: &quot;123&quot;}` - exact match
- `where={&quot;agent_id__in&quot;: [&quot;1&quot;, &quot;2&quot;]}` - match any in list
- Omit `where` to retrieve across all scopes

&gt; üìö **For complete API documentation**, see [SERVICE_API.md](docs/SERVICE_API.md) - includes all methods, CRUD operations, pipeline configuration, and configuration types.

---

## üí° Use Cases

### Example 1: Conversation Memory

Extract and organize memory from multi-turn conversations:

```bash
export OPENAI_API_KEY=your_api_key
python examples/example_1_conversation_memory.py
```

**What it does:**
- Processes multiple conversation JSON files
- Extracts memory items (preferences, habits, opinions, relationships)
- Generates category markdown files (`preferences.md`, `work_life.md`, etc.)

**Best for:** Personal AI assistants, customer support bots, social chatbots

---

### Example 2: Skill Extraction from Logs

Extract skills and lessons learned from agent execution logs:

```bash
export OPENAI_API_KEY=your_api_key
python examples/example_2_skill_extraction.py
```

**What it does:**
- Processes agent logs sequentially
- Extracts actions, outcomes, and lessons learned
- Demonstrates **incremental learning** - memory evolves with each file
- Generates evolving skill guides (`log_1.md` ‚Üí `log_2.md` ‚Üí `skill.md`)

**Best for:** DevOps teams, agent self-improvement, knowledge management

---

### Example 3: Multimodal Memory

Process diverse content types into unified memory:

```bash
export OPENAI_API_KEY=your_api_key
python examples/example_3_multimodal_memory.py
```

**What it does:**
- Processes documents and images together
- Extracts memory from different content types
- Unifies into cross-modal categories (`technical_documentation`, `visual_diagrams`, etc.)

**Best for:** Documentation systems, learning platforms, research tools

---

## üìä Performance

MemU achieves **92.09% average accuracy** on the Locomo benchmark across all reasoning tasks.

&lt;img width=&quot;100%&quot; alt=&quot;benchmark&quot; src=&quot;https://github.com/user-attachments/assets/6fec4884-94e5-4058-ad5c-baac3d7e76d9&quot; /&gt;

View detailed experimental data: [memU-experiment](https://github.com/NevaMind-AI/memU-experiment)

---

## üß© Ecosystem

| Repository | Description | Use Case |
|------------|-------------|----------|
| **[memU](https://github.com/NevaMind-AI/memU)** | Core algorithm engine | Embed AI memory into your product |
| **[memU-server](https://github.com/NevaMind-AI/memU-server)** | Backend service with CRUD, user system, RBAC | Self-host a memory backend |
| **[memU-ui](https://github.com/NevaMind-AI/memU-ui)** | Visual dashboard | Ready-to-use memory console |

**Quick Links:**
- üöÄ [Try MemU Cloud](https://app.memu.so/quick-start)
- üìö [API Documentation](https://memu.pro/docs)
- üí¨ [Discord Community](https://discord.gg/memu)

---

## ü§ù Partners

&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://github.com/TEN-framework/ten-framework&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/113095513?s=200&amp;v=4&quot; alt=&quot;Ten&quot; height=&quot;40&quot; style=&quot;margin: 10px;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://openagents.org&quot;&gt;&lt;img src=&quot;assets/partners/openagents.png&quot; alt=&quot;OpenAgents&quot; height=&quot;40&quot; style=&quot;margin: 10px;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/milvus-io/milvus&quot;&gt;&lt;img src=&quot;https://miro.medium.com/v2/resize:fit:2400/1*-VEGyAgcIBD62XtZWavy8w.png&quot; alt=&quot;Milvus&quot; height=&quot;40&quot; style=&quot;margin: 10px;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://xroute.ai/&quot;&gt;&lt;img src=&quot;assets/partners/xroute.png&quot; alt=&quot;xRoute&quot; height=&quot;40&quot; style=&quot;margin: 10px;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://jaaz.app/&quot;&gt;&lt;img src=&quot;assets/partners/jazz.png&quot; alt=&quot;Jazz&quot; height=&quot;40&quot; style=&quot;margin: 10px;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/Buddie-AI/Buddie&quot;&gt;&lt;img src=&quot;assets/partners/buddie.png&quot; alt=&quot;Buddie&quot; height=&quot;40&quot; style=&quot;margin: 10px;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/bytebase/bytebase&quot;&gt;&lt;img src=&quot;assets/partners/bytebase.png&quot; alt=&quot;Bytebase&quot; height=&quot;40&quot; style=&quot;margin: 10px;&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/LazyAGI/LazyLLM&quot;&gt;&lt;img src=&quot;assets/partners/LazyLLM.png&quot; alt=&quot;LazyLLM&quot; height=&quot;40&quot; style=&quot;margin: 10px;&quot;&gt;&lt;/a&gt;

&lt;/div&gt;

---

## ü§ù How to Contribute

We welcome contributions from the community! Whether you&#039;re fixing bugs, adding features, or improving documentation, your help is appreciated.

### Getting Started

To start contributing to MemU, you&#039;ll need to set up your development environment:

#### Prerequisites
- Python 3.13+
- [uv](https://github.com/astral-sh/uv) (Python package manager)
- Git

#### Setup Development Environment

```bash
# 1. Fork and clone the repository
git clone https://github.com/YOUR_USERNAME/memU.git
cd memU

# 2. Install development dependencies
make install
```

The `make install` command will:
- Create a virtual environment using `uv`
- Install all project dependencies
- Set up pre-commit hooks for code quality checks

#### Running Quality Checks

Before submitting your contribution, ensure your code passes all quality checks:

```bash
make check
```

The `make check` command runs:
- **Lock file verification**: Ensures `pyproject.toml` consistency
- **Pre-commit hooks**: Lints code with Ruff, formats with Black
- **Type checking**: Runs `mypy` for static type analysis
- **Dependency analysis**: Uses `deptry` to find obsolete dependencies

### Contributing Guidelines

For detailed contribution guidelines, code standards, and development practices, please see [CONTRIBUTING.md](CONTRIBUTING.md).

**Quick tips:**
- Create a new branch for each feature or bug fix
- Write clear commit messages
- Add tests for new functionality
- Update documentation as needed
- Run `make check` before pushing

---

## üìÑ License

[Apache License 2.0](LICENSE.txt)

---

## üåç Community

- **GitHub Issues**: [Report bugs &amp; request features](https://github.com/NevaMind-AI/memU/issues)
- **Discord**: [Join the community](https://discord.com/invite/hQZntfGsbJ)
- **X (Twitter)**: [Follow @memU_ai](https://x.com/memU_ai)
- **Contact**: info@nevamind.ai

---

&lt;div align=&quot;center&quot;&gt;

‚≠ê **Star us on GitHub** to get notified about new releases!

&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[MiroMindAI/MiroThinker]]></title>
            <link>https://github.com/MiroMindAI/MiroThinker</link>
            <guid>https://github.com/MiroMindAI/MiroThinker</guid>
            <pubDate>Sun, 11 Jan 2026 00:05:39 GMT</pubDate>
            <description><![CDATA[MiroThinker is an open-source search agent model, built for tool-augmented reasoning and real-world information seeking, aiming to match the deep research experience of OpenAI Deep Research and Gemini Deep Research.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/MiroMindAI/MiroThinker">MiroMindAI/MiroThinker</a></h1>
            <p>MiroThinker is an open-source search agent model, built for tool-augmented reasoning and real-world information seeking, aiming to match the deep research experience of OpenAI Deep Research and Gemini Deep Research.</p>
            <p>Language: Python</p>
            <p>Stars: 4,207</p>
            <p>Forks: 265</p>
            <p>Stars today: 305 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/miro_thinker.png&quot; width=&quot;55%&quot; alt=&quot;MiroThinker&quot; /&gt;
&lt;/div&gt;

&lt;br&gt;

&lt;div align=&quot;center&quot;&gt;

[![DEMO](https://img.shields.io/badge/Demo-FFB300?style=for-the-badge&amp;logo=airplayvideo&amp;logoColor=white)](https://dr.miromind.ai/)
[![MODELS](https://img.shields.io/badge/Models-5EDDD2?style=for-the-badge&amp;logo=huggingface&amp;logoColor=ffffff&amp;labelColor)](https://huggingface.co/collections/miromind-ai/mirothinker-v15)
[![Paper](https://img.shields.io/badge/Paper-B31B1B?style=for-the-badge&amp;logo=arxiv&amp;logoColor=white)](https://arxiv.org/abs/2511.11793)
[![Blog](https://img.shields.io/badge/Blog-4285F4?style=for-the-badge&amp;logo=google-chrome&amp;logoColor=white)](https://miromind.ai/#blog)
[![DATA](https://img.shields.io/badge/Data-0040A1?style=for-the-badge&amp;logo=huggingface&amp;logoColor=ffffff&amp;labelColor)](https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1)

[![GITHUB](https://img.shields.io/badge/Github-24292F?style=for-the-badge&amp;logo=github&amp;logoColor=white)](https://github.com/MiroMindAI)
[![WEBSITE](https://img.shields.io/badge/Website-4285F4?style=for-the-badge&amp;logo=google-chrome&amp;logoColor=white)](https://miromind.ai/)
[![DISCORD](https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&amp;logo=discord&amp;logoColor=white)](https://discord.com/invite/GPqEnkzQZd)
[![WeChat](https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&amp;logo=wechat&amp;logoColor=white)](https://raw.githubusercontent.com/MiroMindAI/MiroThinker/refs/heads/main/assets/miromind_wechat.png)
[![RedNote](https://img.shields.io/badge/RedNote-FF2442?style=for-the-badge&amp;logo=revoltdotchat&amp;logoColor=white)](https://www.xiaohongshu.com/user/profile/5e353bd80000000001000239)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;

### üöÄ [Try our Demo!](https://dr.miromind.ai/)

&lt;/div&gt;

&gt; **MiroThinker** is MiroMind&#039;s Flagship Research Agent Model. It is an open-source search model designed to advance tool-augmented reasoning and information-seeking capabilities, enabling complex real-world research workflows across diverse challenges.

The project currently comprises four key components:

- üí° **MiroThinker**: An open-source search **model** that natively supports tool-assisted reasoning, achieving leading performance across multiple benchmarks (e.g., HLE, HLE-Text-2158, HLE-Text-500, BrowseComp, BrowseComp-ZH, GAIA, XBench-DeepSearch, FutureX, and Frames). See [Quick Start](#-quick-start).
- ü§ñ **MiroFlow**: An open-source research agent framework that offers reproducible state-of-the-art performance across multiple benchmarks. See [MiroFlow](https://github.com/MiroMindAI/MiroFlow) for details.
- üìö **MiroVerse**: A premium open-source training dataset with 147k samples supporting research agent training. See [MiroVerse](https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1) on HuggingFace.
- üîß **MiroTrain / MiroRL**: Training infrastructure that supports stable and efficient training for research agent models. See [MiroTrain](https://github.com/MiroMindAI/MiroTrain) and [MiroRL](https://github.com/MiroMindAI/MiroRL) for details.

## üìã Table of Contents

- üì∞ [News &amp; Updates](#-news--updates)
- üìù [Introduction](#-introduction)
- ‚ú® [Key Features](#-key-features)
- üìà [Performance on Benchmarks](#-performance-on-benchmarks)
- üöÄ [Quick Start](#-quick-start)
- üìä [Benchmark Evaluation](#-benchmark-evaluation)
- üî¨ [Trace Collection](#-trace-collection)
- ‚ùì [FAQ &amp; Troubleshooting](#-faq--troubleshooting)
- üìÑ [License](#-license)
- üôè [Acknowledgments](#-acknowledgments)

## üì∞ News &amp; Updates

- **\[2026-01-05\]** üéâüéâ We release [MiroThinker-v1.5](https://huggingface.co/collections/miromind-ai/mirothinker-v15), a world-leading open-source search agent. [MiroThinker-v1.5-30B](https://huggingface.co/miromind-ai/MiroThinker-v1.5-30B) surpasses Kimi-K2-Thinking on BrowseComp-ZH at much lower cost, using only 1/30 of the parameters. [MiroThinker-v1.5-235B](https://huggingface.co/miromind-ai/MiroThinker-v1.5-235B) scores 39.2% on HLE-Text, 69.8% on BrowseComp, 71.5% on BrowseComp-ZH, and 80.8% on GAIA-Val-165, setting a new state-of-the-art among search agents.
- **\[2025-11-13\]** üéâ [MiroThinker-v1.0](https://huggingface.co/collections/miromind-ai/mirothinker-v10) is now released! Introducing **interactive scaling** as a third dimension of performance improvement, MiroThinker v1.0 supports 256K context window and up to 600 tool calls per task. Available in 8B, 30B, and 72B parameter scales, achieving 37.7%, 47.1%, 55.6%, and 81.9% on HLE-Text, BrowseComp, BrowseComp-ZH, and GAIA-Text-103, respectively. See [Technical Report](https://arxiv.org/abs/2511.11793) for more details.
- **\[2025-09-11\]** MiroThinker-72B-Preview ranked 4th in this week&#039;s FutureX benchmark. See [FutureX](https://futurex-ai.github.io/).

&lt;details&gt;
  &lt;summary&gt;üìú Click to expand older updates&lt;/summary&gt;

- **\[2025-09-08\]** [MiroThinker-v0.2](https://huggingface.co/collections/miromind-ai/mirothinker-v02) is now released, achieving open-source SOTA performance across multiple benchmarks, including HLE (17.8%), HLE-Text-Only (19.1%), BrowseComp-EN (17.2%), BrowseComp-ZH (29.4%), XBench-DeepSearch (56.0%), and Frames (74.8%).
- **\[2025-09-07\]** We supported more benchmarks, including [BrowseComp-ZH](https://arxiv.org/abs/2504.19314), [XBench-DeepSearch](https://xbench.org/agi/aisearch), and [FutureX](https://futurex-ai.github.io/). We plan to add more benchmarks in the future.
- **\[2025-08-22\]** Introducing streamlined deployment options for MiroThinker models with optimized resource usage and faster startup times. Experience the interactive demo: [üöÄ Try Gradio Demo](apps/gradio-demo)
- **\[2025-08-08\]** [MiroThinker-v0.1](https://huggingface.co/collections/miromind-ai/mirothinker-v01-689301b6d0563321862d44a1) released. Models, framework, and data are now fully open-sourced!

&lt;/details&gt;

## üìù Introduction

### MiroThinker-v1.5

MiroThinker v1.5 is the world-leading open-source search agent that advances tool-augmented reasoning through **interactive scaling** ‚Äî training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement, beyond model size and context length.

![image](https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/mirothinker_v1.5_framework.png)

**Key Features**

- üöÄ MiroThinker v1.5 supports a 256K context window, long-horizon reasoning, and deep multi-step analysis.
- üîß Handles up to 400 tool calls per task ‚Äî a substantial improvement over previous open-source research agents.
- üì¶ Released in 30B and 235B parameter scales, accompanied by a comprehensive suite of tools and workflows to flexibly support diverse research settings and compute budgets.

&lt;div align=&quot;center&quot;&gt;

|      Model Name       |         Base Model            | Max Context | Max Tool Calls |                              HF Link                               |
|:---------------------:|:-----------------------------:|:-----------:|:--------------:|:------------------------------------------------------------------:|
| MiroThinker-v1.5-30B  | Qwen3-30B-A3B-Thinking-2507   |    256K     |      400       | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-v1.5-30B) |
| MiroThinker-v1.5-235B | Qwen3-235B-A22B-Thinking-2507 |    256K     |      400       | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-v1.5-235B) |

&lt;/div&gt;

MiroThinker v1.5 demonstrates strong general-research performance across a broad range of benchmarks, achieving¬†39.2%,¬†69.8%, 71.5%, and¬†80.8%¬†on HLE-Text, BrowseComp, BrowseComp-ZH, and GAIA-Val-165, respectively. These results surpass previous open-source agents and set the new world-leading BrowseComp performance.

![image](https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/mirothinker_v1.5_browsecomp.png)

### MiroThinker-v1.0

&lt;details&gt;
  &lt;summary&gt;üì¶ Click to expand MiroThinker-v1.0 details&lt;/summary&gt;

Unlike previous agents that scale only model size or context length, MiroThinker v1.0 introduces **interactive scaling** at the model level, systematically training the model to handle deeper and more frequent agent‚Äìenvironment interactions as a third dimension of performance improvement. Interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories.

![image](https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/MiroThinker_v1.0_Overall.png)

### ‚ú® Key Features

- üöÄ **256K Context Window**: Supports long-horizon reasoning and deep multi-step analysis
- üîß **600 Tool Calls**: Handles up to 600 tool calls per task ‚Äî a substantial improvement over previous open-source research agents
- üì¶ **Multiple Scales**: Released in 8B, 30B, and 72B parameter scales, accompanied by a comprehensive suite of tools and workflows to flexibly support diverse research settings and compute budgets

&lt;div align=&quot;center&quot;&gt;

|      Model Name      |         Base Model          | Max Context | Max Tool Calls |                              HF Link                               |
|:--------------------:|:---------------------------:|:-----------:|:--------------:|:------------------------------------------------------------------:|
| MiroThinker-v1.0-8B  |        Qwen3-8B             |    256K     |      600       | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-v1.0-8B)  |
| MiroThinker-v1.0-30B | Qwen3-30B-A3B-Thinking-2507 |    256K    |      600       | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-v1.0-30B) |
| MiroThinker-v1.0-72B |    Qwen2.5-72B-Instruct     |    256K    |      600       | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-v1.0-72B) |

&lt;/div&gt;

MiroThinker v1.0 demonstrates strong general-research performance across a broad range of benchmarks, achieving **37.7%**, **47.1%**, **55.6%**, and **81.9%** on HLE-Text, BrowseComp, BrowseComp-ZH, and GAIA-Text-103, respectively. These results surpass previous open-source agents and narrow the gap with commercial counterparts such as **GPT-5-high**.

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/MiroThinker_v1.0_Performance_1.png&quot; width=&quot;100%&quot; alt=&quot;MiroThinker&quot; /&gt;
&lt;/div&gt;

&lt;/details&gt;

### MiroThinker-v0.2

&lt;details&gt;
  &lt;summary&gt;üì¶ Click to expand MiroThinker-v0.2 details&lt;/summary&gt;

In this new version, we introduced three key improvements:

- üìö **Richer training data** from both English and Chinese sources, yielding significant gains in benchmark performance and generalization
- üéØ **Unified DPO training** with a single preference dataset across all models
- üìè **Extended context length** from 40k to 64k for more challenging multi-turn tool-use tasks

Compared to v0.1, MiroThinker v0.2 delivers consistent gains across benchmarks. For example, scores improved from **57.3 ‚Üí 64.1** on **GAIA-Text-103** and from **17.0 ‚Üí 29.4** on **BrowseComp-ZH**, reflecting substantial advancements in the model‚Äôs general research agent capabilities.

&lt;div align=&quot;center&quot;&gt;

|        Model Name        |      Base Model       | Max Context |                                HF Link                                 |
|:------------------------:|:---------------------:|:-----------:|:----------------------------------------------------------------------:|
| MiroThinker-4B-SFT-v0.2  |       Qwen3-4B        |    64K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-4B-SFT-v0.2)  |
| MiroThinker-4B-DPO-v0.2  |       Qwen3-4B        |    64K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-4B-DPO-v0.2)  |
| MiroThinker-8B-SFT-v0.2  |       Qwen3-8B        |    64K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-8B-SFT-v0.2)  |
| MiroThinker-8B-DPO-v0.2  |       Qwen3-8B        |    64K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-8B-DPO-v0.2)  |
| MiroThinker-14B-SFT-v0.2 |       Qwen3-14B       |    64K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-14B-SFT-v0.2) |
| MiroThinker-14B-DPO-v0.2 |       Qwen3-14B       |    64K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-14B-DPO-v0.2) |
| MiroThinker-32B-SFT-v0.2 |       Qwen3-32B       |    64K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-32B-SFT-v0.2) |
| MiroThinker-32B-DPO-v0.2 |       Qwen3-32B       |    64K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-32B-DPO-v0.2) |

&lt;/div&gt;

&lt;/details&gt;

### MiroThinker-v0.1

&lt;details&gt;
  &lt;summary&gt;üì¶ Click to expand MiroThinker-v0.1 details&lt;/summary&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/gaia_text_103.png&quot; width=&quot;98%&quot; alt=&quot;MiroFlow Performance on GAIA-Validation&quot; /&gt;
  &lt;p&gt;&lt;strong&gt;Performance of Open-Source Models on GAIA-Validation Benchmark.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;

We have released the **MiroThinker v0.1** series, including both SFT and DPO variants at parameter scales of **8B**, **14B**, and **32B**. Notably, MiroThinker v0.1 achieves **state-of-the-art performance** among open-source models on the [GAIA benchmark](https://huggingface.co/datasets/gaia-benchmark/GAIA), a rigorous evaluation suite for advanced agentic capabilities, demonstrating its strength in long-context, decision-intensive, and real-world task scenarios.

&lt;div align=&quot;center&quot;&gt;

| Model Name                | Base Model | Max Context | HF Link                                                               |
| :-----------------------: |:----------:|:-----------:| :--------------------------------------------------------------------:|
| MiroThinker-8B-SFT-v0.1   |  Qwen3-8B  |    40K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-8B-SFT-v0.1)  |
| MiroThinker-8B-DPO-v0.1   |  Qwen3-8B  |    40K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-8B-DPO-v0.1)  |
| MiroThinker-14B-SFT-v0.1  | Qwen3-14B  |    40K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-14B-SFT-v0.1) |
| MiroThinker-14B-DPO-v0.1  | Qwen3-14B  |    40K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-14B-DPO-v0.1) |
| MiroThinker-32B-SFT-v0.1  | Qwen3-32B  |    40K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-32B-SFT-v0.1) |
| MiroThinker-32B-DPO-v0.1  | Qwen3-32B  |    40K     | [ü§ó link](https://huggingface.co/miromind-ai/MiroThinker-32B-DPO-v0.1) |

&lt;/div&gt;

&lt;/details&gt;

## ‚ú® Key Features

### ü§ñ **MiroThinker-Optimized Framework**

- üîì **Fully Open-Source Agent Framework**: Complete transparency with open framework and open models
- üîó **Tool Integration**: Seamless integration with external tools and APIs
- üìù **Trace Collection**: Comprehensive logging and analysis of agent interactions with elapsed time and estimated completion time displayed in minutes. Ready for SFT and DPO
- üìä **Benchmark Evaluation**: Extensive testing across multiple benchmark datasets

### üìä **Comprehensive Benchmark Suite**

&lt;details open&gt;
  &lt;summary&gt;üìã Click to expand benchmark list&lt;/summary&gt;

- **GAIA Validation**: A benchmark for General AI Assistants. ([paper](https://arxiv.org/abs/2311.12983))
- **GAIA-Text-103**: A subset of GAIA Validation for text-only tasks. ([paper](https://arxiv.org/abs/2505.22648))
- **HLE**: Humanity&#039;s Last Exam. ([paper](https://arxiv.org/abs/2501.14249))
- **HLE-Text-2158**: A subset of HLE for text-only tasks. ([paper](https://arxiv.org/abs/2501.14249))
- **HLE-Text-500**: A subset of HLE for text-only tasks, created by [WebThinker](https://arxiv.org/pdf/2504.21776). ([paper](https://arxiv.org/pdf/2504.21776))
- **BrowseComp-EN**: Web browsing and comprehension tasks. ([paper](https://arxiv.org/abs/2504.12516))
- **BrowseComp-ZH**: A Chinese version of BrowseComp. ([paper](https://arxiv.org/abs/2504.19314))
- **WebWalkerQA**: Web navigation and question answering. ([paper](https://arxiv.org/abs/2501.07572))
- **Frames**: Factuality, Retrieval, And reasoning MEasurement Set. ([paper](https://arxiv.org/abs/2409.12941))
- **XBench-DeepSearch**: A benchmark for deep research agents. ([website](https://xbench.org/agi/aisearch))
- **FutureX**: A live benchmark designed for predicting unknown future. ([website](https://futurex-ai.github.io/))
- **SEAL-0**: A benchmark for evaluating LLMs on conflicting-evidence web questions. ([paper](https://arxiv.org/abs/2506.01062))
- **AIME2025**: American Invitational Mathematics Examination 2025. ([website](https://artificialanalysis.ai/evaluations/aime-2025))
- **DeepSearchQA**: Google&#039;s Deep Search Question Answering benchmark. ([paper](https://arxiv.org/abs/2505.20827))

&lt;/details&gt;

## üìà Performance on Benchmarks

### MiroThinker-v1.5

&gt; To prevent potential information leakage (e.g., searching benchmark answers from HuggingFace), access to HuggingFace has been explicitly disabled in these tools.

&gt; We further perform canary string testing on the tool outputs of all trajectories and disregard any trajectory found to be contaminated, treating it as an incorrect answer.

&lt;div&gt;
  &lt;img src=&quot;https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/mirothinker_v1.5_performance.png&quot; width=&quot;100%&quot; alt=&quot;MiroThinker&quot; /&gt;
&lt;/div&gt;

### MiroThinker-v1.0

&lt;details&gt;
  &lt;summary&gt;üì¶ Click to expand MiroThinker-v1.0 details&lt;/summary&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/108a2105-4e1d-499e-a001-4713a03fd8ac&quot; width=&quot;100%&quot; alt=&quot;MiroThinker&quot; /&gt;
&lt;/div&gt;

&lt;/details&gt;

### MiroThinker-v0.2

&lt;details&gt;
  &lt;summary&gt;üì¶ Click to expand MiroThinker-v0.2 details&lt;/summary&gt;

#### Comparison with SOTA Research Agents

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/MiroThinker_v0.2_Performance_2.png&quot; width=&quot;90%&quot; alt=&quot;MiroThinker&quot; /&gt;
&lt;/div&gt;

#### GAIA Benchmark

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://huggingface.co/datasets/miromind-ai/MiroFlow-Benchmarks/resolve/main/assets/MiroThinker_v0.2_Performance_1.png&quot; width=&quot;80%&quot; alt=&quot;MiroThinker&quot; /&gt;
&lt;/div&gt;

&lt;/details&gt;

### MiroThinker-v0.1

&lt;details&gt;
  &lt;summary&gt;üì¶ Click to expand MiroThinker-v0.1 details&lt;/summary&gt;

#### GAIA Benchmark

&lt;div align=&quot;center&quot;&gt;

| **Method**                   | Text-103&lt;br&gt;Best Pass@1 | Text-103&lt;br&gt;Pass@1 (Avg@8) | Val-165&lt;br&gt;Best Pass@1 | Val-165&lt;br&gt;Pass@1 (Avg@8) |
|------------------------------|:-----------------------:|:--------------------------:|:----------------------:|:-------------------------:|
| **üîπ‚Äî‚Äî 7B/8B Models ‚Äî‚Äî**     |                         |                            |                        |                           |
| Search-o1-7B                 |          17.5           |             -              |           -            |             -             |
| R1-Searcher-7B               |          20.4           |             -              |           -            |             -             |
| WebDancer-7B                 |          31.0           |             -              |           -            |             -             |
| WebSailor-7B                 |          37.9           |             -              |           -            |             -             |
| CK-Pro-8B                    |          40.3           |             -              |          32.7          |             -             |
| **MiroThinker-8B-SFT-v0.1**  |          44.7           |            40.1            |          34.6          |           31.8            |
|     + Commercial Tools       |          46.6           |            42.1            |          37.6          |           33.9            |
| **MiroThinker-8B-DPO-v0.1**  |          46.6           |            44.8            |          37.0          |           35.4            |
|     + Commercial Tools       |        **50.5**         |          **46.7**          |        **38.2**        |         **35.9**          |
| **üîπ‚Äî‚Äî 14B Models ‚Äî‚Äî**       |                         |                            |                        |                           |
| **MiroThinker-14B-SFT-v0.1** |          47.6           |           

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hacksider/Deep-Live-Cam]]></title>
            <link>https://github.com/hacksider/Deep-Live-Cam</link>
            <guid>https://github.com/hacksider/Deep-Live-Cam</guid>
            <pubDate>Sun, 11 Jan 2026 00:05:38 GMT</pubDate>
            <description><![CDATA[real time face swap and one-click video deepfake with only a single image]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hacksider/Deep-Live-Cam">hacksider/Deep-Live-Cam</a></h1>
            <p>real time face swap and one-click video deepfake with only a single image</p>
            <p>Language: Python</p>
            <p>Stars: 77,147</p>
            <p>Forks: 11,279</p>
            <p>Stars today: 119 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;Deep-Live-Cam 2.0.1c&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
  Real-time face swap and video deepfake with a single click and only a single image.
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/11395&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11395&quot; alt=&quot;hacksider%2FDeep-Live-Cam | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/demo.gif&quot; alt=&quot;Demo GIF&quot; width=&quot;800&quot;&gt;
&lt;/p&gt;

##  Disclaimer

This deepfake software is designed to be a productive tool for the AI-generated media industry. It can assist artists in animating custom characters, creating engaging content, and even using models for clothing design.

We are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to the law and ethics. We may shut down the project or add watermarks if legally required.

- Ethical Use: Users are expected to use this software responsibly and legally. If using a real person&#039;s face, obtain their consent and clearly label any output as a deepfake when sharing online.

- Content Restrictions: The software includes built-in checks to prevent processing inappropriate media, such as nudity, graphic content, or sensitive material.

- Legal Compliance: We adhere to all relevant laws and ethical guidelines. If legally required, we may shut down the project or add watermarks to the output.

- User Responsibility: We are not responsible for end-user actions. Users must ensure their use of the software aligns with ethical standards and legal requirements.

By using this software, you agree to these terms and commit to using it in a manner that respects the rights and dignity of others.

Users are expected to use this software responsibly and legally. If using a real person&#039;s face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user actions.

## Exclusive v2.4 Quick Start - Pre-built (Windows/Mac Silicon)

  &lt;a href=&quot;https://deeplivecam.net/index.php/quickstart&quot;&gt; &lt;img src=&quot;media/Download.png&quot; width=&quot;285&quot; height=&quot;77&quot; /&gt;

##### This is the fastest build you can get if you have a discrete NVIDIA or AMD GPU or Mac Silicon, And you&#039;ll receive special priority support.
 
###### These Pre-builts are perfect for non-technical users or those who don&#039;t have time to, or can&#039;t manually install all the requirements. Just a heads-up: this is an open-source project, so you can also install it manually. 

## TLDR; Live Deepfake in just 3 Clicks
![easysteps](https://github.com/user-attachments/assets/af825228-852c-411b-b787-ffd9aac72fc6)
1. Select a face
2. Select which camera to use
3. Press live!

## Features &amp; Uses - Everything is in real-time

### Mouth Mask

**Retain your original mouth for accurate movement using Mouth Mask**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/ludwig.gif&quot; alt=&quot;resizable-gif&quot;&gt;
&lt;/p&gt;

### Face Mapping

**Use different faces on multiple subjects simultaneously**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/streamers.gif&quot; alt=&quot;face_mapping_source&quot;&gt;
&lt;/p&gt;

### Your Movie, Your Face

**Watch movies with any face in real-time**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/movie.gif&quot; alt=&quot;movie&quot;&gt;
&lt;/p&gt;

### Live Show

**Run Live shows and performances**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/live_show.gif&quot; alt=&quot;show&quot;&gt;
&lt;/p&gt;

### Memes

**Create Your Most Viral Meme Yet**

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;media/meme.gif&quot; alt=&quot;show&quot; width=&quot;450&quot;&gt; 
  &lt;br&gt;
  &lt;sub&gt;Created using Many Faces feature in Deep-Live-Cam&lt;/sub&gt;
&lt;/p&gt;

### Omegle

**Surprise people on Omegle**

&lt;p align=&quot;center&quot;&gt;
  &lt;video src=&quot;https://github.com/user-attachments/assets/2e9b9b82-fa04-4b70-9f56-b1f68e7672d0&quot; width=&quot;450&quot; controls&gt;&lt;/video&gt;
&lt;/p&gt;

## Installation (Manual)

**Please be aware that the installation requires technical skills and is not for beginners. Consider downloading the quickstart version.**

&lt;details&gt;
&lt;summary&gt;Click to see the process&lt;/summary&gt;

### Installation

This is more likely to work on your computer but will be slower as it utilizes the CPU.

**1. Set up Your Platform**

-   Python (3.11 recommended)
-   pip
-   git
-   [ffmpeg](https://www.youtube.com/watch?v=OlNWCpFdVMA) - ```iex (irm ffmpeg.tc.ht)```
-   [Visual Studio 2022 Runtimes (Windows)](https://visualstudio.microsoft.com/visual-cpp-build-tools/)

**2. Clone the Repository**

```bash
git clone https://github.com/hacksider/Deep-Live-Cam.git
cd Deep-Live-Cam
```

**3. Download the Models**

1. [GFPGANv1.4](https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth)
2. [inswapper\_128\_fp16.onnx](https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx)

Place these files in the &quot;**models**&quot; folder.

**4. Install Dependencies**

We highly recommend using a `venv` to avoid issues.


For Windows:
```bash
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
```
For Linux:
```bash
# Ensure you use the installed Python 3.10
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

**For macOS:**

Apple Silicon (M1/M2/M3) requires specific setup:

```bash
# Install Python 3.11 (specific version is important)
brew install python@3.11

# Install tkinter package (required for the GUI)
brew install python-tk@3.10

# Create and activate virtual environment with Python 3.11
python3.11 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

** In case something goes wrong and you need to reinstall the virtual environment **

```bash
# Deactivate the virtual environment
rm -rf venv

# Reinstall the virtual environment
python -m venv venv
source venv/bin/activate

# install the dependencies again
pip install -r requirements.txt

# gfpgan and basicsrs issue fix
pip install git+https://github.com/xinntao/BasicSR.git@master
pip uninstall gfpgan -y
pip install git+https://github.com/TencentARC/GFPGAN.git@master
```

**Run:** If you don&#039;t have a GPU, you can run Deep-Live-Cam using `python run.py`. Note that initial execution will download models (~300MB).

### GPU Acceleration

**CUDA Execution Provider (Nvidia)**

1. Install [CUDA Toolkit 12.8.0](https://developer.nvidia.com/cuda-12-8-0-download-archive)
2. Install [cuDNN v8.9.7 for CUDA 12.x](https://developer.nvidia.com/rdp/cudnn-archive) (required for onnxruntime-gpu):
   - Download cuDNN v8.9.7 for CUDA 12.x
   - Make sure the cuDNN bin directory is in your system PATH
3. Install dependencies:

```bash
pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
pip uninstall onnxruntime onnxruntime-gpu
pip install onnxruntime-gpu==1.21.0
```

3. Usage:

```bash
python run.py --execution-provider cuda
```

**CoreML Execution Provider (Apple Silicon)**

Apple Silicon (M1/M2/M3) specific installation:

1. Make sure you&#039;ve completed the macOS setup above using Python 3.10.
2. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-silicon
pip install onnxruntime-silicon==1.13.1
```

3. Usage (important: specify Python 3.10):

```bash
python3.10 run.py --execution-provider coreml
```

**Important Notes for macOS:**
- You **must** use Python 3.10, not newer versions like 3.11 or 3.13
- Always run with `python3.10` command not just `python` if you have multiple Python versions installed
- If you get error about `_tkinter` missing, reinstall the tkinter package: `brew reinstall python-tk@3.10`
- If you get model loading errors, check that your models are in the correct folder
- If you encounter conflicts with other Python versions, consider uninstalling them:
  ```bash
  # List all installed Python versions
  brew list | grep python
  
  # Uninstall conflicting versions if needed
  brew uninstall --ignore-dependencies python@3.11 python@3.13
  
  # Keep only Python 3.11
  brew cleanup
  ```

**CoreML Execution Provider (Apple Legacy)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-coreml
pip install onnxruntime-coreml==1.21.0
```

2. Usage:

```bash
python run.py --execution-provider coreml
```

**DirectML Execution Provider (Windows)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-directml
pip install onnxruntime-directml==1.21.0
```

2. Usage:

```bash
python run.py --execution-provider directml
```

**OpenVINO‚Ñ¢ Execution Provider (Intel)**

1. Install dependencies:

```bash
pip uninstall onnxruntime onnxruntime-openvino
pip install onnxruntime-openvino==1.21.0
```

2. Usage:

```bash
python run.py --execution-provider openvino
```
&lt;/details&gt;

## Usage

**1. Image/Video Mode**

-   Execute `python run.py`.
-   Choose a source face image and a target image/video.
-   Click &quot;Start&quot;.
-   The output will be saved in a directory named after the target video.

**2. Webcam Mode**

-   Execute `python run.py`.
-   Select a source face image.
-   Click &quot;Live&quot;.
-   Wait for the preview to appear (10-30 seconds).
-   Use a screen capture tool like OBS to stream.
-   To change the face, select a new source image.

## Command Line Arguments (Unmaintained)

```
options:
  -h, --help                                               show this help message and exit
  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image
  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video
  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory
  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)
  --keep-fps                                               keep original fps
  --keep-audio                                             keep original audio
  --keep-frames                                            keep temporary frames
  --many-faces                                             process every face
  --map-faces                                              map source target faces
  --mouth-mask                                             mask the mouth region
  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder
  --video-quality [0-51]                                   adjust output video quality
  --live-mirror                                            the live camera display as you see it in the front-facing camera frame
  --live-resizable                                         the live camera frame is resizable
  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB
  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)
  --execution-threads EXECUTION_THREADS                    number of execution threads
  -v, --version                                            show program&#039;s version number and exit
```

Looking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.

## Press

**We are always open to criticism and are ready to improve, that&#039;s why we didn&#039;t cherry-pick anything.**

 - [*&quot;Deep-Live-Cam goes viral, allowing anyone to become a digital doppelganger&quot;*](https://arstechnica.com/information-technology/2024/08/new-ai-tool-enables-real-time-face-swapping-on-webcams-raising-fraud-concerns/) - Ars Technica
 - [*&quot;Thanks Deep Live Cam, shapeshifters are among us now&quot;*](https://dataconomy.com/2024/08/15/what-is-deep-live-cam-github-deepfake/) - Dataconomy
 - [*&quot;This free AI tool lets you become anyone during video-calls&quot;*](https://www.newsbytesapp.com/news/science/deep-live-cam-ai-impersonation-tool-goes-viral/story) - NewsBytes
 - [*&quot;OK, this viral AI live stream software is truly terrifying&quot;*](https://www.creativebloq.com/ai/ok-this-viral-ai-live-stream-software-is-truly-terrifying) - Creative Bloq
 - [*&quot;Deepfake AI Tool Lets You Become Anyone in a Video Call With Single Photo&quot;*](https://petapixel.com/2024/08/14/deep-live-cam-deepfake-ai-tool-lets-you-become-anyone-in-a-video-call-with-single-photo-mark-zuckerberg-jd-vance-elon-musk/) - PetaPixel
 - [*&quot;Deep-Live-Cam Uses AI to Transform Your Face in Real-Time, Celebrities Included&quot;*](https://www.techeblog.com/deep-live-cam-ai-transform-face/) - TechEBlog
 - [*&quot;An AI tool that &quot;makes you look like anyone&quot; during a video call is going viral online&quot;*](https://telegrafi.com/en/a-tool-that-makes-you-look-like-anyone-during-a-video-call-is-going-viral-on-the-Internet/) - Telegrafi
 - [*&quot;This Deepfake Tool Turning Images Into Livestreams is Topping the GitHub Charts&quot;*](https://decrypt.co/244565/this-deepfake-tool-turning-images-into-livestreams-is-topping-the-github-charts) - Emerge
 - [*&quot;New Real-Time Face-Swapping AI Allows Anyone to Mimic Famous Faces&quot;*](https://www.digitalmusicnews.com/2024/08/15/face-swapping-ai-real-time-mimic/) - Digital Music News
 - [*&quot;This real-time webcam deepfake tool raises alarms about the future of identity theft&quot;*](https://www.diyphotography.net/this-real-time-webcam-deepfake-tool-raises-alarms-about-the-future-of-identity-theft/) - DIYPhotography
 - [*&quot;That&#039;s Crazy, Oh God. That&#039;s Fucking Freaky Dude... That&#039;s So Wild Dude&quot;*](https://www.youtube.com/watch?time_continue=1074&amp;v=py4Tc-Y8BcY) - SomeOrdinaryGamers
 - [*&quot;Alright look look look, now look chat, we can do any face we want to look like chat&quot;*](https://www.youtube.com/live/mFsCe7AIxq8?feature=shared&amp;t=2686) - IShowSpeed
 - [*&quot;They do a pretty good job matching poses, expression and even the lighting&quot;*](https://www.youtube.com/watch?v=wnCghLjqv3s&amp;t=551s) - TechLinked (LTT)
 - [*&quot;Als Sean Connery an der Redaktionskonferenz teilnahm&quot;*](https://www.golem.de/news/deepfakes-als-sean-connery-an-der-redaktionskonferenz-teilnahm-2408-188172.html) - Golem.de (German)
 - [*&quot;What the F***! Why do I look like Vinny Jr? I look exactly like Vinny Jr!? No, this shit is crazy! Bro This is F*** Crazy! &quot;*](https://youtu.be/JbUPRmXRUtE?t=3964) - IShowSpeed


## Credits

-   [ffmpeg](https://ffmpeg.org/): for making video-related operations easy
-   [Henry](https://github.com/henryruhs): One of the major contributor in this repo
-   [deepinsight](https://github.com/deepinsight): for their [insightface](https://github.com/deepinsight/insightface) project which provided a well-made library and models. Please be reminded that the [use of the model is for non-commercial research purposes only](https://github.com/deepinsight/insightface?tab=readme-ov-file#license).
-   [havok2-htwo](https://github.com/havok2-htwo): for sharing the code for webcam
-   [GosuDRM](https://github.com/GosuDRM): for the open version of roop
-   [pereiraroland26](https://github.com/pereiraroland26): Multiple faces support
-   [vic4key](https://github.com/vic4key): For supporting/contributing to this project
-   [kier007](https://github.com/kier007): for improving the user experience
-   [qitianai](https://github.com/qitianai): for multi-lingual support
-   and [all developers](https://github.com/hacksider/Deep-Live-Cam/graphs/contributors) behind libraries used in this project.
-   Footnote: Please be informed that the base author of the code is [s0md3v](https://github.com/s0md3v/roop)
-   All the wonderful users who helped make this project go viral by starring the repo ‚ù§Ô∏è

[![Stargazers](https://reporoster.com/stars/hacksider/Deep-Live-Cam)](https://github.com/hacksider/Deep-Live-Cam/stargazers)

## Contributions

![Alt](https://repobeats.axiom.co/api/embed/fec8e29c45dfdb9c5916f3a7830e1249308d20e1.svg &quot;Repobeats analytics image&quot;)

## Stars to the Moon üöÄ

&lt;a href=&quot;https://star-history.com/#hacksider/deep-live-cam&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=hacksider/deep-live-cam&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[PostHog/posthog]]></title>
            <link>https://github.com/PostHog/posthog</link>
            <guid>https://github.com/PostHog/posthog</guid>
            <pubDate>Sun, 11 Jan 2026 00:05:37 GMT</pubDate>
            <description><![CDATA[ü¶î PostHog is an all-in-one developer platform for building successful products. We offer product analytics, web analytics, session replay, error tracking, feature flags, experimentation, surveys, data warehouse, a CDP, and an AI product assistant to help debug your code, ship features faster, and keep all your usage and customer data in one stack.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/PostHog/posthog">PostHog/posthog</a></h1>
            <p>ü¶î PostHog is an all-in-one developer platform for building successful products. We offer product analytics, web analytics, session replay, error tracking, feature flags, experimentation, surveys, data warehouse, a CDP, and an AI product assistant to help debug your code, ship features faster, and keep all your usage and customer data in one stack.</p>
            <p>Language: Python</p>
            <p>Stars: 30,774</p>
            <p>Forks: 2,187</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img alt=&quot;posthoglogo&quot; src=&quot;https://user-images.githubusercontent.com/65415371/205059737-c8a4f836-4889-4654-902e-f302b187b6a0.png&quot;&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&#039;https://posthog.com/contributors&#039;&gt;&lt;img alt=&quot;GitHub contributors&quot; src=&quot;https://img.shields.io/github/contributors/posthog/posthog&quot;/&gt;&lt;/a&gt;
  &lt;a href=&#039;http://makeapullrequest.com&#039;&gt;&lt;img alt=&#039;PRs Welcome&#039; src=&#039;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=shields&#039;/&gt;&lt;/a&gt;
  &lt;img alt=&quot;Docker Pulls&quot; src=&quot;https://img.shields.io/docker/pulls/posthog/posthog&quot;/&gt;
  &lt;a href=&quot;https://github.com/PostHog/posthog/commits/master&quot;&gt;&lt;img alt=&quot;GitHub commit activity&quot; src=&quot;https://img.shields.io/github/commit-activity/m/posthog/posthog&quot;/&gt; &lt;/a&gt;
  &lt;a href=&quot;https://github.com/PostHog/posthog/issues?q=is%3Aissue%20state%3Aclosed&quot;&gt;&lt;img alt=&quot;GitHub closed issues&quot; src=&quot;https://img.shields.io/github/issues-closed/posthog/posthog&quot;/&gt; &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://posthog.com/docs&quot;&gt;Docs&lt;/a&gt; - &lt;a href=&quot;https://posthog.com/community&quot;&gt;Community&lt;/a&gt; - &lt;a href=&quot;https://posthog.com/roadmap&quot;&gt;Roadmap&lt;/a&gt; - &lt;a href=&quot;https://posthog.com/why&quot;&gt;Why PostHog?&lt;/a&gt; - &lt;a href=&quot;https://posthog.com/changelog&quot;&gt;Changelog&lt;/a&gt; - &lt;a href=&quot;https://github.com/PostHog/posthog/issues/new?assignees=&amp;labels=bug&amp;template=bug_report.yml&quot;&gt;Bug reports&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=2jQco8hEvTI&quot;&gt;
    &lt;img src=&quot;https://res.cloudinary.com/dmukukwp6/image/upload/demo_thumb_68d0d8d56d&quot; alt=&quot;PostHog Demonstration&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

## PostHog is an all-in-one, open source platform for building successful products

[PostHog](https://posthog.com/) provides every tool you need to build a successful product including:

- [Product Analytics](https://posthog.com/product-analytics): Autocapture or manually instrument event-based analytics to understand user behavior and analyze data with visualization or SQL.
- [Web Analytics](https://posthog.com/web-analytics): Monitor web traffic and user sessions with a GA-like dashboard. Easily monitor conversion, web vitals, and revenue.
- [Session Replays](https://posthog.com/session-replay): Watch real user sessions of interactions with your website or mobile app to diagnose issues and understand user behavior.
- [Feature Flags](https://posthog.com/feature-flags): Safely roll out features to select users or cohorts with feature flags.
- [Experiments](https://posthog.com/experiments): Test changes and measure their statistical impact on goal metrics. Set up experiments with no-code too.
- [Error Tracking](https://posthog.com/error-tracking): Track errors, get alerts, and resolve issues to improve your product.
- [Surveys](https://posthog.com/surveys): Ask anything with our collection of no-code survey templates, or build custom surveys with our survey builder.
- [Data warehouse](https://posthog.com/data-warehouse): Sync data from external tools like Stripe, Hubspot, your data warehouse, and more. Query it alongside your product data.
- [Data pipelines](https://posthog.com/cdp): Run custom filters and transformations on your incoming data. Send it to 25+ tools or any webhook in real time or batch export large amounts to your warehouse.
- [LLM analytics](https://posthog.com/docs/llm-analytics): Capture traces, generations, latency, and cost for your LLM-powered app.
- [Workflows](https://posthog.com/docs/workflows): Create workflows that automate actions or send messages to your users.

Best of all, all of this is free to use with a [generous monthly free tier](https://posthog.com/pricing) for each product. Get started by signing up for [PostHog Cloud US](https://us.posthog.com/signup) or [PostHog Cloud EU](https://eu.posthog.com/signup).

## Table of Contents

- [PostHog is an all-in-one, open source platform for building successful products](#posthog-is-an-all-in-one-open-source-platform-for-building-successful-products)
- [Table of Contents](#table-of-contents)
- [Getting started with PostHog](#getting-started-with-posthog)
  - [PostHog Cloud (Recommended)](#posthog-cloud-recommended)
  - [Self-hosting the open-source hobby deploy (Advanced)](#self-hosting-the-open-source-hobby-deploy-advanced)
- [Setting up PostHog](#setting-up-posthog)
- [Learning more about PostHog](#learning-more-about-posthog)
- [Contributing](#contributing)
- [Open-source vs. paid](#open-source-vs-paid)
- [We‚Äôre hiring!](#were-hiring)

## Getting started with PostHog

### PostHog Cloud (Recommended)

The fastest and most reliable way to get started with PostHog is signing up for free to¬†[PostHog Cloud](https://us.posthog.com/signup) or [PostHog Cloud EU](https://eu.posthog.com/signup). Your first 1 million events, 5k recordings, 1M flag requests, 100k exceptions, and 1500 survey responses are free every month, after which you pay based on usage.

### Self-hosting the open-source hobby deploy (Advanced)

If you want to self-host PostHog, you can deploy a hobby instance in one line on Linux with Docker (recommended 4GB memory):

```bash
/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/posthog/posthog/HEAD/bin/deploy-hobby)&quot;
```

Open source deployments should scale to approximately 100k events per month, after which we recommend [migrating to a PostHog Cloud](https://posthog.com/docs/migrate/migrate-to-cloud).

We _do not_ provide customer support or offer guarantees for open source deployments. See our [self-hosting docs](https://posthog.com/docs/self-host), [troubleshooting guide](https://posthog.com/docs/self-host/deploy/troubleshooting), and [disclaimer](https://posthog.com/docs/self-host/open-source/disclaimer) for more info.

## Setting up PostHog

Once you&#039;ve got a PostHog instance, you can set it up by installing our [JavaScript web snippet](https://posthog.com/docs/getting-started/install?tab=snippet), one of [our SDKs](https://posthog.com/docs/getting-started/install?tab=sdks), or by [using our API](https://posthog.com/docs/getting-started/install?tab=api).

We have SDKs and libraries for popular languages and frameworks like:

| Frontend                                              | Mobile                                                          | Backend                                             |
| ----------------------------------------------------- | --------------------------------------------------------------- | --------------------------------------------------- |
| [JavaScript](https://posthog.com/docs/libraries/js)   | [React Native](https://posthog.com/docs/libraries/react-native) | [Python](https://posthog.com/docs/libraries/python) |
| [Next.js](https://posthog.com/docs/libraries/next-js) | [Android](https://posthog.com/docs/libraries/android)           | [Node](https://posthog.com/docs/libraries/node)     |
| [React](https://posthog.com/docs/libraries/react)     | [iOS](https://posthog.com/docs/libraries/ios)                   | [PHP](https://posthog.com/docs/libraries/php)       |
| [Vue](https://posthog.com/docs/libraries/vue-js)      | [Flutter](https://posthog.com/docs/libraries/flutter)           | [Ruby](https://posthog.com/docs/libraries/ruby)     |

Beyond this, we have docs and guides for [Go](https://posthog.com/docs/libraries/go), [.NET/C#](https://posthog.com/docs/libraries/dotnet), [Django](https://posthog.com/docs/libraries/django), [Angular](https://posthog.com/docs/libraries/angular), [WordPress](https://posthog.com/docs/libraries/wordpress), [Webflow](https://posthog.com/docs/libraries/webflow), and more.

Once you&#039;ve installed PostHog, see our [product docs](https://posthog.com/docs/product-os) for more information on how to set up [product analytics](https://posthog.com/docs/product-analytics/capture-events), [web analytics](https://posthog.com/docs/web-analytics/getting-started), [session replays](https://posthog.com/docs/session-replay/how-to-watch-recordings), [feature flags](https://posthog.com/docs/feature-flags/creating-feature-flags), [experiments](https://posthog.com/docs/experiments/creating-an-experiment), [error tracking](https://posthog.com/docs/error-tracking/installation#setting-up-exception-autocapture), [surveys](https://posthog.com/docs/surveys/installation), [data warehouse](https://posthog.com/docs/cdp/sources), and more.

## Learning more about PostHog

Our code isn&#039;t the only thing that&#039;s open source üò≥. We also open source our [company handbook](https://posthog.com/handbook) which details our [strategy](https://posthog.com/handbook/why-does-posthog-exist), [ways of working](https://posthog.com/handbook/company/culture), and [processes](https://posthog.com/handbook/team-structure).

Curious about how to make the most of PostHog? We wrote a guide to [winning with PostHog](https://posthog.com/docs/new-to-posthog/getting-hogpilled) which walks you through the basics of [measuring activation](https://posthog.com/docs/new-to-posthog/activation), [tracking retention](https://posthog.com/docs/new-to-posthog/retention), and [capturing revenue](https://posthog.com/docs/new-to-posthog/revenue).

## Contributing

We &lt;3 contributions big and small:

- Vote on features or get early access to beta functionality in our [roadmap](https://posthog.com/roadmap)
- Open a PR (see our instructions on [developing PostHog locally](https://posthog.com/handbook/engineering/developing-locally))
- Submit a [feature request](https://github.com/PostHog/posthog/issues/new?assignees=&amp;labels=enhancement%2C+feature&amp;template=feature_request.yml) or [bug report](https://github.com/PostHog/posthog/issues/new?assignees=&amp;labels=bug&amp;template=bug_report.yml)

## Open-source vs. paid

This repo is available under the [MIT expat license](https://github.com/PostHog/posthog/blob/master/LICENSE), except for the `ee` directory (which has its [license here](https://github.com/PostHog/posthog/blob/master/ee/LICENSE)) if applicable.

Need _absolutely üíØ% FOSS_? Check out our [posthog-foss](https://github.com/PostHog/posthog-foss) repository, which is purged of all proprietary code and features.

The pricing for our paid plan is completely transparent and available on [our pricing page](https://posthog.com/pricing).

## We&#039;re hiring!

&lt;img src=&quot;https://res.cloudinary.com/dmukukwp6/image/upload/v1/posthog.com/src/components/Home/images/mission-control-hog&quot; alt=&quot;Hedgehog working on a Mission Control Center&quot; width=&quot;350px&quot;/&gt;

Hey! If you&#039;re reading this, you&#039;ve proven yourself as a dedicated README reader.

You might also make a great addition to our team. We&#039;re growing fast [and would love for you to join us](https://posthog.com/careers).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[HKUDS/VideoRAG]]></title>
            <link>https://github.com/HKUDS/VideoRAG</link>
            <guid>https://github.com/HKUDS/VideoRAG</guid>
            <pubDate>Sun, 11 Jan 2026 00:05:36 GMT</pubDate>
            <description><![CDATA[[KDD'2026] "VideoRAG: Chat with Your Videos"]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/HKUDS/VideoRAG">HKUDS/VideoRAG</a></h1>
            <p>[KDD'2026] "VideoRAG: Chat with Your Videos"</p>
            <p>Language: Python</p>
            <p>Stars: 2,279</p>
            <p>Forks: 303</p>
            <p>Stars today: 49 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;picture&gt;
      &lt;img src=&quot;cover.png&quot; width=&quot;80%&quot; style=&quot;border: none; box-shadow: none;&quot; alt=&quot;Vimo: Chat with Your Videos&quot;&gt;
  &lt;/picture&gt;
  
  &lt;h1&gt;
    &lt;strong&gt;VideoRAG: Chat with Your Videos&lt;/strong&gt; ‚Ä¢ &lt;strong&gt;Vimo Desktop&lt;/strong&gt;
  &lt;/h1&gt;

  &lt;a href=&quot;https://trendshift.io/repositories/16146&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/16146&quot; alt=&quot;HKUDS%2FVideoRAG | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
  
  &lt;a href=&#039;https://arxiv.org/abs/2502.01549&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/arXiv-2502.01549-b31b1b&#039;&gt;&lt;/a&gt;
  &lt;a href=&#039;https://github.com/HKUDS/VideoRAG/issues/1&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/Áæ§ËÅä-wechat/feishu-green&#039;&gt;&lt;/a&gt;
  &lt;a href=&#039;https://discord.gg/ZzU55kz3&#039;&gt;&lt;img src=&#039;https://discordapp.com/api/guilds/1296348098003734629/widget.png?style=shield&#039;&gt;&lt;/a&gt;
  &lt;a href=&#039;https://www.youtube.com/watch?v=D5vsxcp4QZI&#039;&gt;&lt;img src=&#039;https://img.shields.io/badge/YouTube-Watch%20Demo-red?style=flat&amp;logo=youtube&#039;&gt;&lt;/a&gt;
  [![Blog](https://img.shields.io/badge/Blog-LearnOpenCV-blue?style=flat&amp;logo=data:image/png;base64,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)](https://learnopencv.com/videorag-long-context-video-comprehension/)
  [![Platform](https://img.shields.io/badge/platform-macOS%20|%20Windows%20|%20Linux-lightgrey.svg)]()
  

  **üé¨ Intelligent Video Conversations | Powered by Advanced AI | Extreme Long-Context Processing**

&lt;/div&gt;

&lt;br/&gt;

&lt;img src=&#039;VideoRAG-algorithm/VideoRAG_cover.png&#039; /&gt;

Vimo is a revolutionary desktop application that lets you **chat with your videos** using cutting-edge AI technology. Built on the powerful [VideoRAG framework](https://arxiv.org/abs/2502.01549), Vimo can understand and analyze videos of any length - from short clips to hundreds of hours of content - and answer your questions with remarkable accuracy.

### üé• Watch Vimo in Action

See how Vimo transforms video interaction with intelligent conversations and deep understanding capabilities.

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=D5vsxcp4QZI&quot;&gt;
    &lt;img src=&quot;https://img.youtube.com/vi/D5vsxcp4QZI/maxresdefault.jpg&quot; width=&quot;80%&quot; alt=&quot;Vimo Introduction Video&quot;&gt;
  &lt;/a&gt;
  &lt;p&gt;&lt;em&gt;üëÜ Click to watch the Vimo demo video&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;

## ‚ú® Key Features

### For Everyone
- **Drag &amp; Drop Upload**: Simply drag video files into Vimo
- **Smart Conversations**: Ask questions in natural language
- **Multi-Format Support**: Works with MP4, MKV, AVI, and more
- **Cross-Platform**: Available on macOS, Windows, and Linux

### For Power Users
- **Extreme Long Videos**: Process videos up to hundreds of hours
- **Multi-Video Analysis**: Compare and analyze multiple videos simultaneously
- **Advanced Retrieval**: Find specific moments and scenes with precision
- **Export Capabilities**: Save insights and references for later use

### For Researchers
- **VideoRAG Framework**: Access to cutting-edge retrieval-augmented generation
- **Benchmark Dataset**: LongerVideos benchmark with 134+ hours of content
- **Performance Metrics**: Detailed evaluation against existing methods
- **Extensible Architecture**: Build upon our open-source foundation
  
## üåü Why Vimo?

**For Video Enthusiasts &amp; Professionals:**
- **Effortless Video Analysis**: Upload any video and start asking questions immediately
- **Natural Conversations**: Chat with your videos as if talking to a human expert
- **No Length Limits**: Process everything from 30-second clips to 100+ hour documentaries
- **Deep Understanding**: Combines visual content, audio, and context for comprehensive answers

**For Researchers &amp; Developers:**
- **State-of-the-Art Algorithm**: Built on VideoRAG, featuring graph-driven knowledge indexing
- **Benchmark Performance**: Evaluated on 134+ hours across lectures, documentaries, and entertainment
- **Open Source**: Full access to VideoRAG implementation and research findings
- **Scalable Architecture**: Efficient processing with single GPU (RTX 3090) capability

## üìã Table of Contents

- [üöÄ Quick Start](#-quick-start)
- [‚ú® Key Features](#-key-features)
- [üî¨ VideoRAG Algorithm](#-videorag-algorithm)
- [üõ†Ô∏è Development Setup](#Ô∏è-development-setup)
- [üß™ Benchmarks &amp; Evaluation](#-benchmarks--evaluation)
- [üìñ Citation](#-citation)
- [ü§ù Contributing](#-contributing)
- [üôè Acknowledgement](#-acknowledgement)

## üöÄ Quick Start of Vimo

### Option 1: Download Vimo App (Coming Soon)

&gt; [!NOTE]
&gt; We are preparing the **Beta release** for macOS Apple Silicon first, with Windows and Linux versions coming soon!

&lt;div align=&quot;left&quot;&gt;
  &lt;a href=&quot;https://github.com/HKUDS/Vimo/releases&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Coming%20Soon-Mac%20Download-007ACC?style=for-the-badge&amp;logo=apple&amp;logoColor=white&quot; alt=&quot;Coming Soon - Mac Release&quot; height=&quot;50&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;

### Option 2: Run from Source Code

For detailed setup instructions:

- **Vimo Desktop App**: See [Vimo-desktop](Vimo-desktop) for complete installation and configuration steps

**Quick Overview:**
1. Set up the Python backend environment and start the VideoRAG server
2. Launch the Electron frontend application
3. Start chatting with your videos!

## üî¨ VideoRAG Algorithm

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;VideoRAG-algorithm/VideoRAG.png&quot; alt=&quot;VideoRAG Architecture&quot; width=&quot;80%&quot; /&gt;
&lt;/p&gt;

VideoRAG introduces a novel dual-channel architecture that combines:

- **Graph-Driven Knowledge Indexing**: Multi-modal knowledge graphs for structured video understanding
- **Hierarchical Context Encoding**: Preserves spatiotemporal visual patterns across long sequences  
- **Adaptive Retrieval**: Dynamic retrieval mechanisms optimized for video content
- **Cross-Video Understanding**: Semantic relationship modeling across multiple videos

### Technical Highlights

- **Efficient Processing**: Handle hundreds of hours on a single RTX 3090 (24GB)
- **Structured Indexing**: Distill long videos into concise knowledge representations
- **Multi-Modal Retrieval**: Align textual queries with visual and audio content
- **LongerVideos Benchmark**: 160+ videos, 134+ hours across diverse domains

### Performance Comparison

Our VideoRAG algorithm significantly outperforms existing methods in long-context video understanding:

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;Vimo-desktop/figures/table.png&quot; width=&quot;80%&quot; alt=&quot;Performance Comparison&quot; /&gt;
&lt;/div&gt;

### Experiments and Evaluation

See [VideoRAG-algorithm](VideoRAG-algorithm) for detailed development setup including:
- Conda environment creation
- Model checkpoints download
- Dependencies installation
- Evaluation scripts

## üß™ LongerVideos Benchmark

We created the LongerVideos benchmark to evaluate long-context video understanding:

| Video Type       | #Collections | #Videos | #Queries | Avg. Duration |
|------------------|-------------|---------|----------|---------------|
| **Lectures**     | 12          | 135     | 376      | ~64.3 hours   |
| **Documentaries**| 5           | 12      | 114      | ~28.5 hours   |
| **Entertainment**| 5           | 17      | 112      | ~41.9 hours   |
| **Total**        | 22          | 164     | 602      | ~134.6 hours  |

For detailed evaluation instructions and reproduction scripts, see [VideoRAG-algorithm/reproduce](VideoRAG-algorithm/reproduce).

## üìñ Citation

If you find Vimo or VideoRAG helpful in your research, please cite our paper:

```bibtex
@article{VideoRAG,
  title={VideoRAG: Retrieval-Augmented Generation with Extreme Long-Context Videos},
  author={Ren, Xubin and Xu, Lingrui and Xia, Long and Wang, Shuaiqiang and Yin, Dawei and Huang, Chao},
  journal={arXiv preprint arXiv:2502.01549},
  year={2025}
}
```

## ü§ù Contributing

We welcome contributions from the community! Whether you&#039;re:

- **Reporting bugs** or suggesting features for Vimo
- **Improving VideoRAG algorithms** or adding new capabilities  
- **Enhancing documentation** or creating tutorials
- **Designing UI/UX improvements** for better user experience

Feel free to submit issues and pull requests. Together, we&#039;re building the future of intelligent video interaction!

## üôè Acknowledgement

Vimo builds upon the incredible work of the open-source community:

- **[VideoRAG](https://arxiv.org/abs/2502.01549)**: The core algorithm powering Vimo&#039;s intelligence
- **[nano-graphrag](https://github.com/gusye1234/nano-graphrag)** &amp; **[LightRAG](https://github.com/HKUDS/LightRAG)**: Graph-based retrieval foundations
- **[ImageBind](https://github.com/facebookresearch/ImageBind)**: Multi-modal representation learning
- **[uitars-desktop](https://github.com/bytedance/UI-TARS-desktop)**: Desktop application architecture inspiration

**üåü Transform how you interact with videos. Start your journey with Vimo today!**

---

&lt;div align=&quot;center&quot;&gt;
  &lt;sub&gt;Built with ‚ù§Ô∏è by the VideoRAG@HKUDS team.&lt;/sub&gt;
&lt;/div&gt; 
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[anthropics/claude-agent-sdk-python]]></title>
            <link>https://github.com/anthropics/claude-agent-sdk-python</link>
            <guid>https://github.com/anthropics/claude-agent-sdk-python</guid>
            <pubDate>Sun, 11 Jan 2026 00:05:35 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/anthropics/claude-agent-sdk-python">anthropics/claude-agent-sdk-python</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 4,030</p>
            <p>Forks: 535</p>
            <p>Stars today: 24 stars today</p>
            <h2>README</h2><pre># Claude Agent SDK for Python

Python SDK for Claude Agent. See the [Claude Agent SDK documentation](https://platform.claude.com/docs/en/agent-sdk/python) for more information.

## Installation

```bash
pip install claude-agent-sdk
```

**Prerequisites:**

- Python 3.10+

**Note:** The Claude Code CLI is automatically bundled with the package - no separate installation required! The SDK will use the bundled CLI by default. If you prefer to use a system-wide installation or a specific version, you can:

- Install Claude Code separately: `curl -fsSL https://claude.ai/install.sh | bash`
- Specify a custom path: `ClaudeAgentOptions(cli_path=&quot;/path/to/claude&quot;)`

## Quick Start

```python
import anyio
from claude_agent_sdk import query

async def main():
    async for message in query(prompt=&quot;What is 2 + 2?&quot;):
        print(message)

anyio.run(main)
```

## Basic Usage: query()

`query()` is an async function for querying Claude Code. It returns an `AsyncIterator` of response messages. See [src/claude_agent_sdk/query.py](src/claude_agent_sdk/query.py).

```python
from claude_agent_sdk import query, ClaudeAgentOptions, AssistantMessage, TextBlock

# Simple query
async for message in query(prompt=&quot;Hello Claude&quot;):
    if isinstance(message, AssistantMessage):
        for block in message.content:
            if isinstance(block, TextBlock):
                print(block.text)

# With options
options = ClaudeAgentOptions(
    system_prompt=&quot;You are a helpful assistant&quot;,
    max_turns=1
)

async for message in query(prompt=&quot;Tell me a joke&quot;, options=options):
    print(message)
```

### Using Tools

```python
options = ClaudeAgentOptions(
    allowed_tools=[&quot;Read&quot;, &quot;Write&quot;, &quot;Bash&quot;],
    permission_mode=&#039;acceptEdits&#039;  # auto-accept file edits
)

async for message in query(
    prompt=&quot;Create a hello.py file&quot;,
    options=options
):
    # Process tool use and results
    pass
```

### Working Directory

```python
from pathlib import Path

options = ClaudeAgentOptions(
    cwd=&quot;/path/to/project&quot;  # or Path(&quot;/path/to/project&quot;)
)
```

## ClaudeSDKClient

`ClaudeSDKClient` supports bidirectional, interactive conversations with Claude
Code. See [src/claude_agent_sdk/client.py](src/claude_agent_sdk/client.py).

Unlike `query()`, `ClaudeSDKClient` additionally enables **custom tools** and **hooks**, both of which can be defined as Python functions.

### Custom Tools (as In-Process SDK MCP Servers)

A **custom tool** is a Python function that you can offer to Claude, for Claude to invoke as needed.

Custom tools are implemented in-process MCP servers that run directly within your Python application, eliminating the need for separate processes that regular MCP servers require.

For an end-to-end example, see [MCP Calculator](examples/mcp_calculator.py).

#### Creating a Simple Tool

```python
from claude_agent_sdk import tool, create_sdk_mcp_server, ClaudeAgentOptions, ClaudeSDKClient

# Define a tool using the @tool decorator
@tool(&quot;greet&quot;, &quot;Greet a user&quot;, {&quot;name&quot;: str})
async def greet_user(args):
    return {
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: f&quot;Hello, {args[&#039;name&#039;]}!&quot;}
        ]
    }

# Create an SDK MCP server
server = create_sdk_mcp_server(
    name=&quot;my-tools&quot;,
    version=&quot;1.0.0&quot;,
    tools=[greet_user]
)

# Use it with Claude
options = ClaudeAgentOptions(
    mcp_servers={&quot;tools&quot;: server},
    allowed_tools=[&quot;mcp__tools__greet&quot;]
)

async with ClaudeSDKClient(options=options) as client:
    await client.query(&quot;Greet Alice&quot;)

    # Extract and print response
    async for msg in client.receive_response():
        print(msg)
```

#### Benefits Over External MCP Servers

- **No subprocess management** - Runs in the same process as your application
- **Better performance** - No IPC overhead for tool calls
- **Simpler deployment** - Single Python process instead of multiple
- **Easier debugging** - All code runs in the same process
- **Type safety** - Direct Python function calls with type hints

#### Migration from External Servers

```python
# BEFORE: External MCP server (separate process)
options = ClaudeAgentOptions(
    mcp_servers={
        &quot;calculator&quot;: {
            &quot;type&quot;: &quot;stdio&quot;,
            &quot;command&quot;: &quot;python&quot;,
            &quot;args&quot;: [&quot;-m&quot;, &quot;calculator_server&quot;]
        }
    }
)

# AFTER: SDK MCP server (in-process)
from my_tools import add, subtract  # Your tool functions

calculator = create_sdk_mcp_server(
    name=&quot;calculator&quot;,
    tools=[add, subtract]
)

options = ClaudeAgentOptions(
    mcp_servers={&quot;calculator&quot;: calculator}
)
```

#### Mixed Server Support

You can use both SDK and external MCP servers together:

```python
options = ClaudeAgentOptions(
    mcp_servers={
        &quot;internal&quot;: sdk_server,      # In-process SDK server
        &quot;external&quot;: {                # External subprocess server
            &quot;type&quot;: &quot;stdio&quot;,
            &quot;command&quot;: &quot;external-server&quot;
        }
    }
)
```

### Hooks

A **hook** is a Python function that the Claude Code _application_ (_not_ Claude) invokes at specific points of the Claude agent loop. Hooks can provide deterministic processing and automated feedback for Claude. Read more in [Claude Code Hooks Reference](https://docs.anthropic.com/en/docs/claude-code/hooks).

For more examples, see examples/hooks.py.

#### Example

```python
from claude_agent_sdk import ClaudeAgentOptions, ClaudeSDKClient, HookMatcher

async def check_bash_command(input_data, tool_use_id, context):
    tool_name = input_data[&quot;tool_name&quot;]
    tool_input = input_data[&quot;tool_input&quot;]
    if tool_name != &quot;Bash&quot;:
        return {}
    command = tool_input.get(&quot;command&quot;, &quot;&quot;)
    block_patterns = [&quot;foo.sh&quot;]
    for pattern in block_patterns:
        if pattern in command:
            return {
                &quot;hookSpecificOutput&quot;: {
                    &quot;hookEventName&quot;: &quot;PreToolUse&quot;,
                    &quot;permissionDecision&quot;: &quot;deny&quot;,
                    &quot;permissionDecisionReason&quot;: f&quot;Command contains invalid pattern: {pattern}&quot;,
                }
            }
    return {}

options = ClaudeAgentOptions(
    allowed_tools=[&quot;Bash&quot;],
    hooks={
        &quot;PreToolUse&quot;: [
            HookMatcher(matcher=&quot;Bash&quot;, hooks=[check_bash_command]),
        ],
    }
)

async with ClaudeSDKClient(options=options) as client:
    # Test 1: Command with forbidden pattern (will be blocked)
    await client.query(&quot;Run the bash command: ./foo.sh --help&quot;)
    async for msg in client.receive_response():
        print(msg)

    print(&quot;\n&quot; + &quot;=&quot; * 50 + &quot;\n&quot;)

    # Test 2: Safe command that should work
    await client.query(&quot;Run the bash command: echo &#039;Hello from hooks example!&#039;&quot;)
    async for msg in client.receive_response():
        print(msg)
```

## Types

See [src/claude_agent_sdk/types.py](src/claude_agent_sdk/types.py) for complete type definitions:

- `ClaudeAgentOptions` - Configuration options
- `AssistantMessage`, `UserMessage`, `SystemMessage`, `ResultMessage` - Message types
- `TextBlock`, `ToolUseBlock`, `ToolResultBlock` - Content blocks

## Error Handling

```python
from claude_agent_sdk import (
    ClaudeSDKError,      # Base error
    CLINotFoundError,    # Claude Code not installed
    CLIConnectionError,  # Connection issues
    ProcessError,        # Process failed
    CLIJSONDecodeError,  # JSON parsing issues
)

try:
    async for message in query(prompt=&quot;Hello&quot;):
        pass
except CLINotFoundError:
    print(&quot;Please install Claude Code&quot;)
except ProcessError as e:
    print(f&quot;Process failed with exit code: {e.exit_code}&quot;)
except CLIJSONDecodeError as e:
    print(f&quot;Failed to parse response: {e}&quot;)
```

See [src/claude_agent_sdk/\_errors.py](src/claude_agent_sdk/_errors.py) for all error types.

## Available Tools

See the [Claude Code documentation](https://docs.anthropic.com/en/docs/claude-code/settings#tools-available-to-claude) for a complete list of available tools.

## Examples

See [examples/quick_start.py](examples/quick_start.py) for a complete working example.

See [examples/streaming_mode.py](examples/streaming_mode.py) for comprehensive examples involving `ClaudeSDKClient`. You can even run interactive examples in IPython from [examples/streaming_mode_ipython.py](examples/streaming_mode_ipython.py).

## Migrating from Claude Code SDK

If you&#039;re upgrading from the Claude Code SDK (versions &lt; 0.1.0), please see the [CHANGELOG.md](CHANGELOG.md#010) for details on breaking changes and new features, including:

- `ClaudeCodeOptions` ‚Üí `ClaudeAgentOptions` rename
- Merged system prompt configuration
- Settings isolation and explicit control
- New programmatic subagents and session forking features

## Development

If you&#039;re contributing to this project, run the initial setup script to install git hooks:

```bash
./scripts/initial-setup.sh
```

This installs a pre-push hook that runs lint checks before pushing, matching the CI workflow. To skip the hook temporarily, use `git push --no-verify`.

### Building Wheels Locally

To build wheels with the bundled Claude Code CLI:

```bash
# Install build dependencies
pip install build twine

# Build wheel with bundled CLI
python scripts/build_wheel.py

# Build with specific version
python scripts/build_wheel.py --version 0.1.4

# Build with specific CLI version
python scripts/build_wheel.py --cli-version 2.0.0

# Clean bundled CLI after building
python scripts/build_wheel.py --clean

# Skip CLI download (use existing)
python scripts/build_wheel.py --skip-download
```

The build script:

1. Downloads Claude Code CLI for your platform
2. Bundles it in the wheel
3. Builds both wheel and source distribution
4. Checks the package with twine

See `python scripts/build_wheel.py --help` for all options.

### Release Workflow

The package is published to PyPI via the GitHub Actions workflow in `.github/workflows/publish.yml`. To create a new release:

1. **Trigger the workflow** manually from the Actions tab with two inputs:
   - `version`: The package version to publish (e.g., `0.1.5`)
   - `claude_code_version`: The Claude Code CLI version to bundle (e.g., `2.0.0` or `latest`)

2. **The workflow will**:
   - Build platform-specific wheels for macOS, Linux, and Windows
   - Bundle the specified Claude Code CLI version in each wheel
   - Build a source distribution
   - Publish all artifacts to PyPI
   - Create a release branch with version updates
   - Open a PR to main with:
     - Updated `pyproject.toml` version
     - Updated `src/claude_agent_sdk/_version.py`
     - Updated `src/claude_agent_sdk/_cli_version.py` with bundled CLI version
     - Auto-generated `CHANGELOG.md` entry

3. **Review and merge** the release PR to update main with the new version information

The workflow tracks both the package version and the bundled CLI version separately, allowing you to release a new package version with an updated CLI without code changes.

## License and terms

Use of this SDK is governed by Anthropic&#039;s [Commercial Terms of Service](https://www.anthropic.com/legal/commercial-terms), including when you use it to power products and services that you make available to your own customers and end users, except to the extent a specific component or dependency is covered by a different license as indicated in that component&#039;s LICENSE file.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Arindam200/awesome-ai-apps]]></title>
            <link>https://github.com/Arindam200/awesome-ai-apps</link>
            <guid>https://github.com/Arindam200/awesome-ai-apps</guid>
            <pubDate>Sun, 11 Jan 2026 00:05:34 GMT</pubDate>
            <description><![CDATA[A collection of projects showcasing RAG, agents, workflows, and other AI use cases]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Arindam200/awesome-ai-apps">Arindam200/awesome-ai-apps</a></h1>
            <p>A collection of projects showcasing RAG, agents, workflows, and other AI use cases</p>
            <p>Language: Python</p>
            <p>Stars: 8,433</p>
            <p>Forks: 1,037</p>
            <p>Stars today: 53 stars today</p>
            <h2>README</h2><pre>![Banner](/assets/banner_new.png)

&lt;div align=&quot;center&quot;&gt;

# Awesome AI Apps [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

&lt;a href=&quot;https://trendshift.io/repositories/14662&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14662&quot; alt=&quot;Arindam200%2Fawesome-ai-apps | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;

This repository is a comprehensive collection of **70+ practical examples, tutorials, and recipes** for building powerful LLM-powered applications. From simple chatbots to advanced AI agents, these projects serve as a guide for developers working with various AI frameworks and tools.

## üìã Table of Contents

- [üéì Courses](#-courses)
- [üöÄ Featured AI Apps](#-featured-ai-apps)
  - [üß© Starter Agents](#-starter-agents)
  - [ü™∂ Simple Agents](#-simple-agents)
  - [üóÇÔ∏è MCP Agents](#Ô∏è-mcp-agents)
  - [üß† Memory Agents](#-memory-agents)
  - [üìö RAG Applications](#-rag-applications)
  - [üî¨ Advanced Agents](#-advanced-agents)
- [üì∫ Tutorials &amp; Videos](#-tutorials--videos)
- [üöÄ Getting Started](#getting-started)
- [ü§ù Contributing](#-contributing)

---

&lt;div align=&quot;center&quot;&gt;

## üíé Sponsors

&lt;p align=&quot;center&quot;&gt;
  A huge thank you to our sponsors for their generous support!
&lt;/p&gt;

&lt;table align=&quot;center&quot; cellpadding=&quot;10&quot; style=&quot;width:100%; border-collapse:collapse;&quot;&gt;
  &lt;tr align=&quot;center&quot;&gt;
    &lt;td width=&quot;300&quot; valign=&quot;middle&quot; align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://dub.sh/brightdata&quot; target=&quot;_blank&quot; title=&quot;Visit Bright Data&quot;&gt;
        &lt;img src=&quot;https://mintlify.s3.us-west-1.amazonaws.com/brightdata/logo/light.svg&quot; height=&quot;35&quot; style=&quot;max-width:180px;&quot; alt=&quot;Bright Data - Web Data Platform&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;sub&gt;
        &lt;span style=&quot;white-space:nowrap;&quot;&gt;Web Data Platform&lt;/span&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://dub.sh/brightdata&quot; target=&quot;_blank&quot;&gt;
          &lt;img src=&quot;https://img.shields.io/badge/Visit%20Site-blue?style=flat-square&quot; alt=&quot;Visit Bright Data website&quot;&gt;
        &lt;/a&gt;
      &lt;/sub&gt;
    &lt;/td&gt;
    &lt;td width=&quot;300&quot; valign=&quot;middle&quot; align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://dub.sh/nebius&quot; target=&quot;_blank&quot; title=&quot;Visit Nebius Token Factory&quot;&gt;
        &lt;img src=&quot;./assets/nebius.png&quot; height=&quot;36&quot; style=&quot;max-width:180px;&quot; alt=&quot;Nebius Token Factory&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;sub&gt;
        &lt;span style=&quot;white-space:nowrap;&quot;&gt;AI Inference Provider&lt;/span&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://dub.sh/nebius&quot; target=&quot;_blank&quot;&gt;
          &lt;img src=&quot;https://img.shields.io/badge/Visit%20Site-blue?style=flat-square&quot; alt=&quot;Visit Nebius Token Factory&quot;&gt;
        &lt;/a&gt;
      &lt;/sub&gt;
    &lt;/td&gt;
    &lt;td width=&quot;300&quot; valign=&quot;middle&quot; align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://dub.sh/scrapegraphai&quot; target=&quot;_blank&quot; title=&quot;Visit ScrapeGraphAI on GitHub&quot;&gt;
        &lt;img src=&quot;https://raw.githubusercontent.com/ScrapeGraphAI/ScrapeGraph-AI/main/docs/assets/scrapegraphai_logo.png&quot; height=&quot;44&quot; style=&quot;max-width:180px;&quot; alt=&quot;ScrapeGraphAI - Web Scraping Library&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;sub&gt;
        &lt;span style=&quot;white-space:nowrap;&quot;&gt;AI Web Scraping framework&lt;/span&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://dub.sh/scrapegraphai&quot; target=&quot;_blank&quot;&gt;
          &lt;img src=&quot;https://img.shields.io/badge/Visit%20Site-blue?style=flat-square&quot; alt=&quot;View ScrapeGraphAI on GitHub&quot;&gt;
        &lt;/a&gt;
      &lt;/sub&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr align=&quot;center&quot;&gt;
    &lt;td width=&quot;300&quot; valign=&quot;middle&quot; align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://dub.sh/memorilabs&quot; target=&quot;_blank&quot; title=&quot;Visit Memorilabs&quot;&gt;
        &lt;img src=&quot;assets/memori.png&quot; height=&quot;36&quot; style=&quot;max-width:180px;&quot; alt=&quot;Memori - SQL Native Memory for AI&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;sub&gt;
        &lt;span style=&quot;white-space:nowrap;&quot;&gt;SQL Native Memory for AI&lt;/span&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://dub.sh/memorilabs&quot; target=&quot;_blank&quot;&gt;
          &lt;img src=&quot;https://img.shields.io/badge/Visit%20Site-blue?style=flat-square&quot; alt=&quot;Visit Memorilabs website&quot;&gt;
        &lt;/a&gt;
      &lt;/sub&gt;
    &lt;/td&gt;
    &lt;td width=&quot;300&quot; valign=&quot;middle&quot; align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://dub.sh/copilotkit&quot; target=&quot;_blank&quot; title=&quot;Visit CopilotKit&quot;&gt;
        &lt;img src=&quot;assets/copilot-kit-logo.svg&quot; height=&quot;36&quot; style=&quot;max-width:180px;&quot; alt=&quot;CopilotKit - Agentic Application Platform&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;sub&gt;
        &lt;span style=&quot;white-space:nowrap;&quot;&gt;Agentic Application Platform&lt;/span&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://dub.sh/copilotkit&quot; target=&quot;_blank&quot;&gt;
          &lt;img src=&quot;https://img.shields.io/badge/Visit%20Site-blue?style=flat-square&quot; alt=&quot;Visit CopilotKit website&quot;&gt;
        &lt;/a&gt;
      &lt;/sub&gt;
    &lt;/td&gt;
    &lt;td width=&quot;300&quot; valign=&quot;middle&quot; align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://dub.sh/scalekitt&quot; target=&quot;_blank&quot; title=&quot;Visit ScaleKit&quot;&gt;
        &lt;img src=&quot;assets/scalekit.svg&quot; height=&quot;36&quot; style=&quot;max-width:180px;&quot; alt=&quot;ScaleKit - Auth Stack for AI&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;sub&gt;
        &lt;span style=&quot;white-space:nowrap;&quot;&gt;Auth Stack for AI&lt;/span&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://dub.sh/scalekitt&quot; target=&quot;_blank&quot;&gt;
          &lt;img src=&quot;https://img.shields.io/badge/Visit%20Site-blue?style=flat-square&quot; alt=&quot;Visit ScaleKit website&quot;&gt;
        &lt;/a&gt;
      &lt;/sub&gt;
    &lt;/td&gt;
  &lt;/tr&gt;
  &lt;!-- &lt;tr align=&quot;center&quot;&gt;
    &lt;td width=&quot;200&quot; valign=&quot;middle&quot; align=&quot;center&quot;&gt;
      &lt;a href=&quot;https://okahu.ai&quot; target=&quot;_blank&quot; title=&quot;Visit Okahu&quot;&gt;
        &lt;img src=&quot;assets/okahu.png&quot; height=&quot;36&quot; style=&quot;max-width:180px;&quot; alt=&quot;Okahu - AI Platform&quot;&gt;
      &lt;/a&gt;
      &lt;br&gt;
      &lt;sub&gt;
        &lt;span style=&quot;white-space:nowrap;&quot;&gt;AI Platform&lt;/span&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://okahu.ai&quot; target=&quot;_blank&quot;&gt;
          &lt;img src=&quot;https://img.shields.io/badge/Visit%20Site-blue?style=flat-square&quot; alt=&quot;Visit Okahu website&quot;&gt;
        &lt;/a&gt;
      &lt;/sub&gt;
    &lt;/td&gt;
  &lt;/tr&gt; --&gt;
&lt;/table&gt;

### üíé Become a Sponsor

&lt;p align=&quot;center&quot;&gt;
Interested in sponsoring this project? Feel free to reach out!
&lt;br/&gt;
&lt;a href=&quot;https://dub.sh/arindam-linkedin&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&amp;logo=linkedin&amp;logoColor=white&quot; alt=&quot;LinkedIn&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;mailto:arindammajumder2020@gmail.com&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Email-D14836?style=for-the-badge&amp;logo=gmail&amp;logoColor=white&quot; alt=&quot;Email&quot;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;/div&gt;

---

## üéì Courses

### AWS Strands Course for Beginners

**Comprehensive hands-on course on building AI agents with AWS Strands SDK:**

- [**AWS Strands Course**](course/aws_strands) - Complete 8-lesson course covering agent fundamentals to production patterns
  - **Foundation**: Basic agents, session management, structured output
  - **Integration**: MCP agents, human-in-the-loop patterns
  - **Multi-Agent**: Orchestrator agents, swarm intelligence, graph workflows
  - **Production**: Observability, safety guardrails, and best practices

## üöÄ Featured AI Apps

### üß© Starter Agents

**Quick-start agents for learning and extending different AI frameworks.** _12 projects_

- [Agno HackerNews Analysis](starter_ai_agents/agno_starter) - Agno-based agent for trend analysis on HackerNews
- [OpenAI SDK Starter](starter_ai_agents/openai_agents_sdk) - OpenAI Agents SDK with email helper &amp; haiku writer examples
- [LlamaIndex Task Manager](starter_ai_agents/llamaindex_starter) - LlamaIndex-powered task assistant
- [CrewAI Research Crew](starter_ai_agents/crewai_starter) - Multi-agent research team example
- [PydanticAI Weather Bot](starter_ai_agents/pydantic_starter) - Real-time weather information agent
- [LangChain-LangGraph Starter](starter_ai_agents/langchain_langgraph_starter) - LangChain + LangGraph workflow starter
- [AWS Strands Agent Starter](starter_ai_agents/aws_strands_starter) - Weather report agent using AWS Strands SDK
- [Camel AI Starter](starter_ai_agents/camel_ai_starter) - Performance benchmarking tool comparing various AI models
- [DSPy Starter](starter_ai_agents/dspy_starter) - DSPy framework for building and optimizing AI systems
- [Google ADK Starter](starter_ai_agents/google_adk_starter) - Google Agent Development Kit starter template
- [cagent Starter](starter_ai_agents/cagent_starter) - Open-source customizable multi-agent runtime by Docker
- [Sayna Voice Agent](starter_ai_agents/sayna_starter) - Real-time voice infrastructure with multi-provider STT/TTS (Deepgram, ElevenLabs, Azure, Google) and WebSocket streaming

### ü™∂ Simple Agents

**Straightforward, practical use-cases for everyday AI applications.** _13 projects_

- [Agno AI Examples](simple_ai_agents/agno_ai_examples) - Simple to multi-agent examples with web search &amp; knowledge base
- [Finance Agent](simple_ai_agents/finance_agent) - Real-time stock &amp; market data tracking agent
- [Human-in-the-Loop Agent](simple_ai_agents/human_in_the_loop_agent) - HITL actions for safe AI task execution
- [Newsletter Generator](simple_ai_agents/newsletter_agent) - AI-powered newsletter builder with Firecrawl integration
- [Reasoning Agent](simple_ai_agents/reasoning_agent) - Step-by-step financial reasoning demonstration
- [Agno UI Example](simple_ai_agents/agno_ui_agent) - Interactive UI for web &amp; finance agents
- [Mastra Weather Bot](simple_ai_agents/mastra_ai_weather_agent) - Weather updates using Mastra AI framework
- [Calendar Assistant](simple_ai_agents/cal_scheduling_agent) - Calendar scheduling integration with Cal.com
- [Smart Scheduler Assistant](simple_ai_agents/email_to_calendar_scheduler) - AI-powered Gmail reader and Google Calendar manager
- [Web Automation Agent](simple_ai_agents/browser_agent) - Browser automation agent using Nebius &amp; browser-use
- [Nebius Chat](simple_ai_agents/nebius_chat) - Chat interface for Nebius Token Factory
- [RouteLLM Chat](simple_ai_agents/llm_router) - Intelligent model routing with RouteLLM (GPT-4o-mini vs Nebius Llama) for cost optimization
- [Talk to Your DB](simple_ai_agents/talk_to_db) - Natural language database queries with GibsonAI &amp; LangChain

### üóÇÔ∏è MCP Agents

**Examples using Model Context Protocol for external tool integration.** _11 projects_

- [Doc-MCP](mcp_ai_agents/doc_mcp) - Semantic RAG documentation &amp; Q&amp;A system
- [LangGraph MCP Agent](mcp_ai_agents/langchain_langgraph_mcp_agent) - LangChain ReAct agent with Couchbase integration
- [GitHub MCP Agent](mcp_ai_agents/github_mcp_agent) - Repository insights and analysis via MCP
- [MCP Starter](mcp_ai_agents/mcp_starter) - GitHub repository analyzer starter template
- [Talk to your Docs](mcp_ai_agents/docs_qna_agent) - Documentation Q&amp;A agent with MCP
- [Database MCP Agent](mcp_ai_agents/database_mcp_agent) - Conversational AI agent for managing GibsonAI database projects and schemas
- [Hotel Finder Agent](mcp_ai_agents/hotel_finder_agent) - Hotel search and booking using MCP integration
- [Custom MCP Server](mcp_ai_agents/custom_mcp_server) - Custom MCP server implementation example
- [Couchbase MCP Server](mcp_ai_agents/couchbase_mcp_server) - Couchbase database integration with MCP protocol
- [ScaleKit Exa MCP Security](mcp_ai_agents/scalekit-exa-mcp-security) - Security-focused MCP integration with Exa search
- [Docker E2B MCP Agent](mcp_ai_agents/e2b_docker_mcp_agent) - Secure AI agent for running agents in sandboxed Docker environments via MCP Gateway

### üß† Memory Agents

**Agents with advanced memory capabilities for context retention and personalization.** _12 projects_

- [Agno Memory Agent](memory_agents/agno_memory_agent) - Agno-based agent with persistent memory capabilities
- [arXiv Researcher Agent with Memori](memory_agents/arxiv_researcher_agent_with_memori) - Research assistant using OpenAI Agents and GibsonAI Memori
- [AWS Strands Agent with Memori](memory_agents/aws_strands_agent_with_memori) - AWS Strands agent enhanced with Memori memory system
- [Blog Writing Agent](memory_agents/blog_writing_agent) - Personalized blog writing agent with memory for style consistency
- [Social Media Agent](memory_agents/social_media_agent) - Social media automation agent with memory for brand voice
- [Job Search Agent](memory_agents/job_search_agent) - Job search agent with memory for preference tracking
- [Brand Reputation Monitor](memory_agents/brand_reputation_monitor) - AI-powered brand reputation monitoring with news analysis and sentiment tracking
- [Product Launch Agent](memory_agents/product_launch_agent) - Competitive intelligence tool for analyzing competitor product launches
- [AI Consultant Agent](memory_agents/ai_consultant_agent/) - AI-powered consulting agent using **Memori v3** as long-term memory fabric and **ExaAI** for research
- [Customer Support Voice Agent](memory_agents/customer_support_voice_agent) - Voice-enabled customer support assistant with Memori v3 and Firecrawl for knowledge base management
- [YouTube Trend Agent](memory_agents/youtube_trend_agent) - YouTube channel analysis agent with Memori, Agno, and Exa for trend analysis and video ideas
- [Study Coach Agent](memory_agents/study_coach_agent) - AI-powered study coach with Memori v3 and LangGraph for multi-step verification of understanding

### üìö RAG Applications

**Retrieve-augmented generation examples for document understanding and knowledge bases.** _11 projects_

- [Agentic RAG](rag_apps/agentic_rag) - Agentic RAG implementation with Agno &amp; GPT-5
- [Agentic RAG with Web Search](rag_apps/agentic_rag_with_web_search) - Advanced RAG with CrewAI, Qdrant, and Exa for hybrid search capabilities
- [Resume Optimizer](rag_apps/resume_optimizer) - AI-powered resume optimization and enhancement tool
- [LlamaIndex RAG Starter](rag_apps/llamaIndex_starter) - LlamaIndex + Nebius RAG starter template
- [PDF RAG Analyzer](rag_apps/pdf_rag_analyser) - Multi-PDF chat and analysis system
- [Qwen3 RAG Chat](rag_apps/qwen3_rag) - PDF chatbot interface built with Streamlit
- [Chat with Code](rag_apps/chat_with_code) - Conversational code explorer and documentation assistant
- [Gemma3 OCR](rag_apps/gemma_ocr/) - OCR-based document and image processor using Gemma3 model
- [Nvidia Nemotron OCR](rag_apps/nvidia_ocr/) - OCR-based document and image parsing using Nvidia Nemotron-Nano-V2-12b
- [Contextual AI RAG](rag_apps/contextual_ai_rag) - Enterprise-level RAG with managed datastores and quality evaluation
- [Simple RAG](rag_apps/simple_rag) - Basic RAG implementation with Nebius for quick starts

### üî¨ Advanced Agents

**Complex multi-agent pipelines for production-ready end-to-end workflows.** _14 projects_

- [Deep Researcher](advance_ai_agents/deep_researcher_agent) - Multi-stage research agent with Agno &amp; ScrapeGraph AI
- [Candilyzer](advance_ai_agents/candidate_analyser) - Candidate analysis tool for GitHub/LinkedIn profiles
- [Job Finder](advance_ai_agents/job_finder_agent) - LinkedIn job search automation with Bright Data integration
- [AI Trend Analyzer](advance_ai_agents/trend_analyzer_agent) - AI trend mining and analysis with Google ADK
- [Conference Talk Generator](advance_ai_agents/conference_talk_abstract_generator) - Automated talk abstract generation with Google ADK &amp; Couchbase
- [Finance Service Agent](advance_ai_agents/finance_service_agent) - FastAPI server for stock data and predictions with Agno
- [Price Monitoring Agent](advance_ai_agents/price_monitoring_agent) - Price monitoring and alerting agent powered by CrewAI, Twilio &amp; Nebius
- [Startup Idea Validator Agent](advance_ai_agents/startup_idea_validator_agent) - Agentic workflow to validate and analyze startup ideas
- [Meeting Assistant Agent](advance_ai_agents/meeting_assistant_agent) - Automated meeting notes and task creation from conversations
- [AI Hedgefund](advance_ai_agents/ai-hedgefund) - Agentic workflow for comprehensive financial analysis
- [Smart GTM Agent](advance_ai_agents/smart_gtm_agent) - Go-to-market strategy and competitive analysis agent
- [Conference Agnostic CFP Generator](advance_ai_agents/conference_agnositc_cfp_generator) - Automated conference proposal generation system
- [Car Finder Agent](advance_ai_agents/car_finder_agent) - AI-powered used car recommendation system with CrewAI and MongoDB
- [Content Team Agent](advance_ai_agents/content_team_agent) - SEO content optimization workflow with Agno &amp; SerpAPI for Google AI Search ranking

## üì∫ Tutorials &amp; Videos

### üéì Course Playlists

- [**AWS Strands Course**](https://www.youtube.com/playlist?list=PLMZM1DAlf0Lrc43ZtUXAwYu9DhnqxzRKZ) - Complete 8-lesson course on building AI agents with AWS Strands SDK

### üîß Framework Tutorials

- [**Build with MCP**](https://www.youtube.com/playlist?list=PLMZM1DAlf0Lolxax4L2HS54Me8gn1gkz4) - Model Context Protocol tutorials and examples
- [**Build AI Agents**](https://www.youtube.com/playlist?list=PLMZM1DAlf0LqixhAG9BDk4O_FjqnaogK8) - General AI agent development tutorials
- [**AI Agents, MCP and more...**](https://www.youtube.com/playlist?list=PL2ambAOfYA6-LDz0KpVKu9vJKAqhv0KKI) - Mixed tutorials and project demos

---

&lt;div align=&quot;center&quot;&gt;

## üì• Stay Updated with Daily AI Insight!

Get easy-to-follow weekly tutorials and deep dives on AI, LLMs, and agent frameworks. Perfect for developers who want to learn, build, and stay ahead with new tech. Subscribe our Newsletter!

[![Subscribe to our Newsletter](https://github.com/user-attachments/assets/990d1947-337b-4e87-a7e6-e619ec19dee6)](https://mranand.substack.com/subscribe)

&lt;/div&gt;

---

## Getting Started

### Prerequisites

- **Python 3.10+** (Python 3.11+ recommended for newer projects)
- **Git** for cloning the repository
- **Package Manager**: `pip` or `uv` (recommended for faster installs)
- **API Keys**: Most projects require API keys (see individual project READMEs)

### Quick Start

1. **Clone the repository**

   ```bash
   git clone https://github.com/Arindam200/awesome-ai-apps.git
   cd awesome-ai-apps
   ```

2. **Choose a project** and navigate to its directory

   ```bash
   cd starter_ai_agents/agno_starter  # Example: Start with Agno starter
   ```

3. **Set up environment variables**

   ```bash
   cp .env.example .env  # Copy example environment file
   # Edit .env with your API keys
   ```

4. **Install dependencies**

   ```bash
   # Using pip
   pip install -r requirements.txt

   # OR using uv (recommended - faster)
   uv sync
   # or
   uv pip install -e .
   ```

5. **Run the project**

   ```bash
   python main.py
   # or for Streamlit apps
   streamlit run app.py
   ```

## ü§ù Contributing

We welcome contributions from the community! Here&#039;s how you can help:

- üêõ **Report bugs** or suggest improvements via [GitHub Issues](https://github.com/Arindam200/awesome-ai-apps/issues)
- üí° **Add new projects** - Submit your own AI agent examples
- üìù **Improve documentation** - Help make projects more accessible
- üîß **Fix issues** - Contribute code improvements and bug fixes

**Before contributing:**

- Read our [Contributing Guidelines](CONTRIBUTING.md) for detailed information
- Check existing issues to avoid duplicates
- Follow the project structure and naming conventions
- Ensure your project includes a comprehensive README.md

**Important:** This project follows a [Contributor Code of Conduct](CODE_OF_CONDUCT.md). By participating, you agree to abide by its terms.

## üìú License

This repository is licensed under the [MIT License](./LICENSE). Feel free to use and modify the examples for your projects.

## Thank You for the Support! üôè

[![Star History Chart](https://api.star-history.com/svg?repos=Arindam200/awesome-ai-apps&amp;type=Date)](https://www.star-history.com/#Arindam200/awesome-ai-apps&amp;Date)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[alibaba/ROLL]]></title>
            <link>https://github.com/alibaba/ROLL</link>
            <guid>https://github.com/alibaba/ROLL</guid>
            <pubDate>Sun, 11 Jan 2026 00:05:33 GMT</pubDate>
            <description><![CDATA[An Efficient and User-Friendly Scaling Library for Reinforcement Learning with Large Language Models]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/alibaba/ROLL">alibaba/ROLL</a></h1>
            <p>An Efficient and User-Friendly Scaling Library for Reinforcement Learning with Large Language Models</p>
            <p>Language: Python</p>
            <p>Stars: 2,612</p>
            <p>Forks: 196</p>
            <p>Stars today: 3 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;assets/roll.jpeg&quot; width=&quot;40%&quot; alt=&quot;ROLL Logo&quot;&gt;

# ROLL: Reinforcement Learning Optimization for Large-Scale Learning

&lt;h4&gt;üöÄ An Efficient and User-Friendly Scaling Library for Reinforcement Learning with Large Language Models üöÄ&lt;/h4&gt;

&lt;p&gt;
  &lt;a href=&quot;https://github.com/alibaba/ROLL/blob/main/LICENSE&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/license-Apache%202.0-blue.svg&quot; alt=&quot;License&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/alibaba/ROLL/issues&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/issues/alibaba/ROLL&quot; alt=&quot;GitHub issues&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/alibaba/ROLL/stargazers&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/stars/alibaba/ROLL?style=social&quot; alt=&quot;Repo stars&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://arxiv.org/abs/2506.06122&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=arXiv&amp;message=Paper&amp;color=red&quot;&gt;&lt;/a&gt;
  &lt;!-- ÁªÑÁªá‰∏ªÈ°µÔºöÁÇπÂáªË∑≥ËΩ¨Âà∞ https://github.com/alibaba --&gt;
  &lt;a href=&quot;./assets/roll_wechat.png&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/WeChat-green?logo=wechat&quot; alt=&quot;WeChat QR&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://deepwiki.com/alibaba/ROLL&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://deepwiki.com/badge.svg&quot; alt=&quot;Ask DeepWiki&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;./assets/future_lab.png&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/FutureLab2025?style=social&quot; alt=&quot;X QR&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;/div&gt;

ROLL is an efficient and user-friendly RL library designed for Large Language Models (LLMs) utilizing Large Scale GPU resources. It significantly enhances LLM performance in key areas such as human preference alignment, complex reasoning, and multi-turn agentic interaction scenarios.

Leveraging a multi-role distributed architecture with Ray for flexible resource allocation and heterogeneous task scheduling, ROLL integrates cutting-edge technologies like Megatron-Core, SGLang and vLLM to accelerate model training and inference.



---

## üì¢ News

| üì£   Updates                                                                                                                                                                                                                                                                                                                            |
|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **[01/01/2026]** üéâ Our [Let It Flow: Agentic Crafting on Rock and Roll](https://arxiv.org/abs/2512.24873) report released! Introducing ALE ecosystem and ROME, an open-source agentic model with novel IPA algorithm.   |
| **[11/08/2025]** üéâ Our [ROCK: Reinforcement Open Construction Kit](https://github.com/alibaba/ROCK) released, Explore the new capabilities!.                                                                                                                                                                                           |
| **[10/23/2025]** üéâ Our Papers released, see [Asymmetric Proximal Policy Optimization: mini-critics boost LLM reasoning](https://arxiv.org/abs/2510.01656) and [Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization](https://arxiv.org/abs/2510.13554).                         |
| **[10/14/2025]** üéâ Our Paper released, see [Part II: ROLL Flash -- Accelerating RLVR and Agentic Training with Asynchrony](https://arxiv.org/abs/2510.11345).                                                                                                                                                                          |
| **[09/28/2025]** üéâ Ascend NPU support ‚Äî see [usage guide](https://alibaba.github.io/ROLL/docs/User%20Guides/Hardware%20Support/ascend_usage).                                                                                                                                                                                                  |
| **[09/25/2025]** üéâ Our Paper released, see [RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training](https://arxiv.org/abs/2509.21009)                                                                                                                                                                        |
| **[09/24/2025]** üéâ Support [Wan2_2 Reward FL pipeline](examples/wan2.2-14B-reward_fl_ds/reward_fl_config.yaml). Explore the new capabilities!                                                                                                                                                                                          |
| **[09/23/2025]** üéâ ROLL aligns with GEM environment definition, providing agentic Tool Use training capabilities, [ToolUse docs](docs_roll/docs/English/UserGuide/agentic/Tool_Use.md).                                                                                                                                                |
| **[09/16/2025]** üéâ Qwen3-Next model training is supported, refer to [configuration](examples/qwen3-next-80BA3B-rlvr_megatron/rlvr_config.yaml).                                                                                                                                                                                        |
| **[09/04/2025]** üéâ ROLL supports vLLM dynamic FP8 rollout and remove_padding for acceleration.                                                                                                                                                                                                                                         |
| **[08/28/2025]** üéâ ROLL supports SFT pipeline, refer to [configuration](examples/qwen2.5-7B-sft_megatron/sft_config.yaml).                                                                                                                                                                                                             |
| **[08/13/2025]** üéâ ROLL supports AMD GPUs with out-of-box image docker and Dockerfile and specific yamls under `examples/` directory. Please refer to [Installation](https://alibaba.github.io/ROLL/docs/Getting%20Started/Installation/).                                                                                             |
| **[08/11/2025]** üéâ Our Paper released, see [Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning](https://arxiv.org/abs/2508.08221).                                                                                                                                                                                         |
| **[08/10/2025]** üéâ Agentic RL supports [stepwise learning](examples/qwen2.5-0.5B-agentic/agent_val_frozen_lake_gigpo.yaml), like [GiGPO](https://arxiv.org/abs/2505.10978); Distill supports [VLM](examples/qwen2.5-vl-7B-distill/distill_vl_megatron.yaml). Explore the new capabilities!                                             |
| **[08/06/2025]** üéâ ROLL PPT is now available, [Slides](assets/ROLL%20È´òÊïà‰∏îÁî®Êà∑ÂèãÂ•ΩÁöÑÂ§ßÊ®°ÂûãRLËÆ≠ÁªÉÊ°ÜÊû∂.pdf).                                                                                                                                                                                                                                           |
| **[07/31/2025]** üéâ Refactor agentic rl design. Support agentic rl [async training](examples/qwen2.5-0.5B-agentic/agent_val_frozen_lake_async.yaml). Explore the new capabilities!                                                                                                                                                      |
| **[07/31/2025]** üéâ Support [DistillPipeline](examples/qwen2.5-7B-distill_megatron/run_distill_pipeline.sh)/[DpoPipeline](examples/qwen2.5-3B-dpo_megatron/run_dpo_pipeline.sh). Support [lora](examples/qwen2.5-7B-rlvr_megatron/rlvr_lora_zero3.yaml). Support [GSPO](https://arxiv.org/abs/2507.18071)                               |
| **[06/25/2025]** üéâ Support thread env for env scaling and support [qwen2.5 VL agentic pipeline](examples/qwen2.5-vl-3B-agentic/agentic_val_sokoban.yaml).                                                                                                                                                                              |
| **[06/13/2025]** üéâ Support [Qwen2.5 VL rlvr pipeline](examples/qwen2.5-vl-7B-rlvr/rlvr_megatron.yaml) and upgrade mcore to 0.12 version.                                                                                                                                                                                               |
| **[06/09/2025]** üéâ ROLL tech report is now available! Access the report [here](https://arxiv.org/abs/2506.06122).                                                                                                                                                                                                                      |
| **[06/08/2025]** üéâSupports  Qwen3([8B](examples/qwen3-8B-rlvr_megatron/rlvr_config.yaml)/14B/32B), Qwen3-MoE([30A3](examples/qwen3-30BA3B-rlvr_megatron/rlvr_config.yaml)/[235A22](examples/qwen3-235BA22B-rlvr_megatron/rlvr_config.yaml)), Qwen2.5([7B](examples/qwen2.5-7B-rlvr_megatron/rlvr_config.yaml)/14B/32B/72B) LLM models. |
| **[05/30/2025]** üéâ Training [RLVR](examples/qwen2.5-7B-rlvr_megatron/rlvr_config.yaml) and [Agentic RL](examples/qwen2.5-0.5B-agentic/agent_val_frozen_lake.yaml) with ROLL is now available! Explore the new capabilities.                                                                                                            |
---


## üöÄ Get Started

[Documents](https://alibaba.github.io/ROLL/)

### Quick Start
[Installation](https://alibaba.github.io/ROLL/docs/Getting%20Started/Installation/)  
[Config System Explanation](https://alibaba.github.io/ROLL/docs/User%20Guides/Configuration/config_system)  
[Debugging Guide](https://alibaba.github.io/ROLL/docs/Getting%20Started/Debugging%20Guide/debug_guide)  
[Trackers and Metrics](https://alibaba.github.io/ROLL/docs/User%20Guides/Tracker%20&amp;%20Metrics/trackers_and_metrics)  
[Checkpoint Saving and Resuming Guide](https://alibaba.github.io/ROLL/docs/User%20Guides/Advanced%20Features/checkpoint_and_resume)  
[Converting MCoreAdapter Models to Hugging Face Format](https://alibaba.github.io/ROLL/docs/User%20Guides/Advanced%20Features/megatron_convert_2_hf)  
[Quick Start: Single-Node Deployment Guide](https://alibaba.github.io/ROLL/docs/Getting%20Started/Quick%20Start/single_node_quick_start)  
[Quick Start: Multi-Node Deployment Guide](https://alibaba.github.io/ROLL/docs/Getting%20Started/Quick%20Start/multi_nodes_quick_start)  
[Quick Start: Using Alibaba Cloud Function Compute DevPod for Rapid Development](https://alibaba.github.io/ROLL/docs/Getting%20Started/Quick%20Start/aliyun_serverless_devpod_quick_start)
[Frequently Asked Questions](https://alibaba.github.io/ROLL/docs/Getting%20Started/FAQ/qa_issues)

### UserGuide

#### Pipeline Step by Step
[RLVR Pipeline](https://alibaba.github.io/ROLL/docs/User%20Guides/Pipeline/rlvr_pipeline_start)  
[Agentic Pipeline](https://alibaba.github.io/ROLL/docs/User%20Guides/Pipeline/agentic_pipeline_start)  
[Agentic Comprehensive Guide](https://alibaba.github.io/ROLL/docs/User%20Guides/Pipeline/agent_pipeline_start)  
[Distill Pipeline](https://alibaba.github.io/ROLL/docs/User%20Guides/Pipeline/distill_pipeline_start)

#### Algorithms
[Reinforce++](https://alibaba.github.io/ROLL/docs/User%20Guides/Algorithms/Reinforce_Plus_Plus)  
[TOPR](https://alibaba.github.io/ROLL/docs/User%20Guides/Algorithms/TOPR)  
[GiGPO](https://alibaba.github.io/ROLL/docs/User%20Guides/Agentic/agentic_GiGPO)  
[PPO](https://alibaba.github.io/ROLL/docs/User%20Guides/Algorithms/PPO)  
[Lite PPO](https://alibaba.github.io/ROLL/docs/User%20Guides/Algorithms/LitePPO)  
[GRPO](https://alibaba.github.io/ROLL/docs/User%20Guides/Algorithms/GRPO)  
[GSPO](https://alibaba.github.io/ROLL/docs/User%20Guides/Algorithms/GSPO)  
[RAFT++](https://alibaba.github.io/ROLL/docs/User%20Guides/Algorithms/RAFT_Plus_Plus)  
[StarPO](https://alibaba.github.io/ROLL/docs/User%20Guides/Agentic/agentic_StarPO)   
[RewardFL](https://alibaba.github.io/ROLL/docs/User%20Guides/Algorithms/Reward_FL)

#### Backend
[DeepSeed](https://alibaba.github.io/ROLL/docs/User%20Guides/Configuration/deepspeed)  
[Megatron](https://alibaba.github.io/ROLL/docs/User%20Guides/Configuration/megatron)   
[vLLM](https://alibaba.github.io/ROLL/docs/User%20Guides/Configuration/vllm)  
[SGLang](https://alibaba.github.io/ROLL/docs/User%20Guides/Configuration/sglang)

#### Advanced Features
[Asynchronous Parallel Rollout](https://alibaba.github.io/ROLL/docs/User%20Guides/Advanced%20Features/async_parallel_rollout)  
[Asynchronous Training Feature](https://alibaba.github.io/ROLL/docs/User%20Guides/Advanced%20Features/async_training)  

#### Performance Optimization &amp; Resource Management 
[Resource Config](https://alibaba.github.io/ROLL/docs/User%20Guides/Configuration/device_mapping)   
[GPU Time-Division Multiplexing Control](https://alibaba.github.io/ROLL/docs/User%20Guides/Advanced%20Features/offload_reload_control)  

#### ROLL x Ascend
[Ascend Usage Guide](https://alibaba.github.io/ROLL/docs/User%20Guides/Hardware%20Support/ascend_usage)

---

## ‚ú® Key Features
*   **Multi-task RL Training (RLVR):** Covers mathematics, coding, general reasoning, open-ended Q&amp;A, instruction following, etc.
    *   Flexible `domain_batch_size` distribution control.
    *   **Sample-level asynchronous parallel Rollout**, asynchronous reward calculation, and dynamic sampling.
    *   Asynchronous training under implementation.
*   **Agentic RL:** Multi-turn interaction capabilities for games, multi-turn dialogues, tool use, etc.
    *   Environment-level **asynchronous parallel rollout**.
    *   Supports **asynchronous training**.
    *   Multi-turn interaction rollout supports **local debugging**, improving multi-turn interaction business development efficiency.
    *   Supports **TrajectoryWise (StartPO)** and **StepWise (GiGPO)** training paradigms.
*   **Algorithm-Friendly:** Provides flexible and rich RL strategy configurations by default.
    *   Over 20 rich reinforcement learning strategy options, such as reward normalization, reward clipping, various advantage estimation methods, etc.
    *   Out-of-the-box support for reinforcement learning algorithms, such as **PPO, GRPO, Reinforce++, TOPR, RAFT++, GSPO**, etc.
*   **Rich Training and Inference Engine:** Ray-based multi-role distributed architecture; Strategy abstraction unifies various backends, enabling easy operation from single machines to thousands-of-GPU clusters.
    *   Inference/Generation supports vLLM, SGLang.
    *   Training supports DeepSpeed (ZeRO), Megatron-LM 5D parallelism (mcore-adapter, dp/tp/pp/cp/ep), FSDP under implementation.
    *   Extreme offload/reload capabilities.
    *   Supports [LoRA](https://alibaba.github.io/ROLL/docs/User%20Guides/Configuration/lora) training.
    *   Supports FP8 rollout (FP8 inference for LLM as judge, FP8 rollout with BF16 training under development).
*   **AutoDeviceMapping:** Supports custom device mapping for different roles, flexibly managing colocated and disaggregated deployments.
*   **Observability:** Integrated with SwanLab / WandB / TensorBoard, tracking of performance for each domain and reward type.
*   **Rich Post-training Technical Support:**
    *   Agentic RL LLM &amp; VLM
    *   RLVR LLM &amp; VLM
    *   Distill Pipeline LLM &amp; VLM
    *   DPO Pipeline
    *   SFT Pipeline under development



---

## üîÆ Upcoming Features

We are continuously working to expand ROLL&#039;s capabilities:
* ‚è±Ô∏è **Async RLVR pipeline**: For even more efficient and streamlined asynchronous operations.
* ‚öôÔ∏è **FSDP2**: Integrating the latest Fully Sharded Data Parallel techniques.
* üîç **Support DeepseekV3**: Adding compatibility for the newest Deepseek models.

---

## üèÜ Notable work based on ROLL
- [STAgent](https://arxiv.org/abs/2512.24957): An agentic LLM specialized for spatio-temporal understanding and complex tasks like constrained POI discovery and itinerary planning, featuring hierarchical data curation with 1:10,000 filter ratio and cascaded training (seed SFT + difficulty-aware SFT + RL), achieving strong performance on TravelBench while preserving general capabilities.
- [IPRO](https://arxiv.org/abs/2510.14255): A novel video diffusion framework using reinforcement learning to enhance identity preservation in human-centric I2V generation, optimizing diffusion models with face identity scorer and KL-divergence regularization.
- [TaoSR-SHE](https://arxiv.org/abs/2510.07972): Stepwise Hybrid Examination Reinforcement Learning Framework for Taobao Search Relevance, with SRPO (hybrid reward model + offline verifier), diversified data filtering, and multi-stage curriculum learning.
- [EARL](https://arxiv.org/abs/2510.05943): Efficient Agentic RL Systems for LLMs, introducing a dynamic parallelism selector and a layout-aware data dispatcher to boost throughput, reduce memory and data movement bottlenecks, enabling stable large-scale agentic RL without hard context-length limits.
- [LiveThinking](https://arxiv.org/abs/2510.07685): Real-time reasoning for AI-powered livestreaming by distilling a 670B teacher LLM to a 30B MoE (3B active) via Rejection Sampling Fine-Tuning, then compressing reasoning with GRPO; delivers sub-second latency and ~30x compute reduction, with gains in response correctness (3.3%), helpfulness (21.8%), and GMV in Taobao Live.
- [TaoSR-AGRL](https://www.arxiv.org/abs/2510.08048): Adaptive Guided Reinforcement Learning for LLM-based e-commerce relevance, introducing Rule-aware Reward Shaping and Adaptive Guided Replay to improve long-horizon reasoning, rule adherence, and training stability in Taobao Search; deployed in main search handling hundreds of millions of users.
- [RecGPT](https://www.arxiv.org/abs/2507.22879): a next-generation, LLM-driven framework that places user intent at the core of recommender systems, fostering a more sustainable and mutually beneficial ecosystem.
- [TaoSR1](https://arxiv.org/abs/2508.12365): A novel LLM framework directly deploying Chain-of-Thought (CoT) reasoning for e-commerce query-product relevance prediction, overcoming deployment challenges for superior performance.
- [AIGB-Pearl](https://www.arxiv.org/abs/2509.15927): a novel auto-bidding method that integrates generative planning and policy optimization, utilizing an LLM-enhanced trajectory evaluator to iteratively refine bidding strategies for state-of-the-art advertising performance.
-----

## üôè Citation and Acknowledgement

ROLL is inspired by the design of OpenRLHF, VeRL, Nemo-Aligner, and RAGEN.
The project is developed by Alibaba TAOBAO &amp; TMALL Group and Alibaba Group. The code is distributed under the Apache License (Version 2.0). This product contains various third-party components under other open-source licenses. See the `NOTICE` file for more information.

The following repositories have been used in ROLL, either in their close-to-original form or as an inspiration:

  * [NVIDIA/Megatron-LM](https://github.com/NVIDIA/Megatron-LM)
  * [microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed)
  * [sgl-project/sglang](https://github.com/sgl-project/sglang)
  * [vllm-project/vllm](https://github.com/vllm-project/vllm)
  * [modelscope/DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio)

If you use ROLL in your research or project, please consider citing us:

```bibtex
@article{wang2025reinforcement,
  title={Reinforcement Learning Optimization for Large-Scale Learning: An Efficient and User-Friendly Scaling Library},
  author={Wang, Weixun and Xiong, Shaopan and Chen, Gengru and Gao, Wei and Guo, Sheng and He, Yancheng 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[browser-use/browser-use]]></title>
            <link>https://github.com/browser-use/browser-use</link>
            <guid>https://github.com/browser-use/browser-use</guid>
            <pubDate>Sun, 11 Jan 2026 00:05:32 GMT</pubDate>
            <description><![CDATA[üåê Make websites accessible for AI agents. Automate tasks online with ease.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/browser-use/browser-use">browser-use/browser-use</a></h1>
            <p>üåê Make websites accessible for AI agents. Automate tasks online with ease.</p>
            <p>Language: Python</p>
            <p>Stars: 75,214</p>
            <p>Forks: 8,977</p>
            <p>Stars today: 66 stars today</p>
            <h2>README</h2><pre>&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/user-attachments/assets/2ccdb752-22fb-41c7-8948-857fc1ad7e24&quot;&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/user-attachments/assets/774a46d5-27a0-490c-b7d0-e65fcbbfa358&quot;&gt;
  &lt;img alt=&quot;Shows a black Browser Use Logo in light color mode and a white one in dark color mode.&quot; src=&quot;https://github.com/user-attachments/assets/2ccdb752-22fb-41c7-8948-857fc1ad7e24&quot;  width=&quot;full&quot;&gt;
&lt;/picture&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/user-attachments/assets/9955dda9-ede3-4971-8ee0-91cbc3850125&quot;&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/user-attachments/assets/6797d09b-8ac3-4cb9-ba07-b289e080765a&quot;&gt;
    &lt;img alt=&quot;The AI browser agent.&quot; src=&quot;https://github.com/user-attachments/assets/9955dda9-ede3-4971-8ee0-91cbc3850125&quot;  width=&quot;400&quot;&gt;
    &lt;/picture&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://cloud.browser-use.com&quot;&gt;&lt;img src=&quot;https://media.browser-use.tools/badges/package&quot; height=&quot;48&quot; alt=&quot;Browser-Use Package Download Statistics&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

---

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;#demos&quot;&gt;&lt;img src=&quot;https://media.browser-use.tools/badges/demos&quot; alt=&quot;Demos&quot;&gt;&lt;/a&gt;
&lt;img width=&quot;16&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;
&lt;a href=&quot;https://docs.browser-use.com&quot;&gt;&lt;img src=&quot;https://media.browser-use.tools/badges/docs&quot; alt=&quot;Docs&quot;&gt;&lt;/a&gt;
&lt;img width=&quot;16&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;
&lt;a href=&quot;https://browser-use.com/posts&quot;&gt;&lt;img src=&quot;https://media.browser-use.tools/badges/blog&quot; alt=&quot;Blog&quot;&gt;&lt;/a&gt;
&lt;img width=&quot;16&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;
&lt;a href=&quot;https://browsermerch.com&quot;&gt;&lt;img src=&quot;https://media.browser-use.tools/badges/merch&quot; alt=&quot;Merch&quot;&gt;&lt;/a&gt;
&lt;img width=&quot;100&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;
&lt;a href=&quot;https://github.com/browser-use/browser-use&quot;&gt;&lt;img src=&quot;https://media.browser-use.tools/badges/github&quot; alt=&quot;Github Stars&quot;&gt;&lt;/a&gt;
&lt;img width=&quot;4&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;
&lt;a href=&quot;https://x.com/intent/user?screen_name=browser_use&quot;&gt;&lt;img src=&quot;https://media.browser-use.tools/badges/twitter&quot; alt=&quot;Twitter&quot;&gt;&lt;/a&gt;
&lt;img width=&quot;4 height=&quot;1&quot; alt=&quot;&quot;&gt;
&lt;a href=&quot;https://link.browser-use.com/discord&quot;&gt;&lt;img src=&quot;https://media.browser-use.tools/badges/discord&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt;
&lt;img width=&quot;4&quot; height=&quot;1&quot; alt=&quot;&quot;&gt;
&lt;a href=&quot;https://cloud.browser-use.com&quot;&gt;&lt;img src=&quot;https://media.browser-use.tools/badges/cloud&quot; height=&quot;48&quot; alt=&quot;Browser-Use Cloud&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;/br&gt;

üå§Ô∏è Want to skip the setup? Use our &lt;b&gt;[cloud](https://cloud.browser-use.com)&lt;/b&gt; for faster, scalable, stealth-enabled browser automation!

# ü§ñ LLM Quickstart

1. Direct your favorite coding agent (Cursor, Claude Code, etc) to [Agents.md](https://docs.browser-use.com/llms-full.txt)
2. Prompt away!

&lt;br/&gt;

# üëã Human Quickstart

**1. Create environment with [uv](https://docs.astral.sh/uv/) (Python&gt;=3.11):**
```bash
uv init
```

**2. Install Browser-Use package:**
```bash
#  We ship every day - use the latest version!
uv add browser-use
uv sync
```

**3. Get your API key from [Browser Use Cloud](https://cloud.browser-use.com/new-api-key) and add it to your `.env` file (new signups get $10 free credits):**
```
# .env
BROWSER_USE_API_KEY=your-key
```

**4. Install Chromium browser:**
```bash
uvx browser-use install
```

**5. Run your first agent:**
```python
from browser_use import Agent, Browser, ChatBrowserUse
import asyncio

async def example():
    browser = Browser(
        # use_cloud=True,  # Uncomment to use a stealth browser on Browser Use Cloud
    )

    llm = ChatBrowserUse()

    agent = Agent(
        task=&quot;Find the number of stars of the browser-use repo&quot;,
        llm=llm,
        browser=browser,
    )

    history = await agent.run()
    return history

if __name__ == &quot;__main__&quot;:
    history = asyncio.run(example())
```

Check out the [library docs](https://docs.browser-use.com) and the [cloud docs](https://docs.cloud.browser-use.com) for more!

&lt;br/&gt;

# üî• Deploy on Sandboxes

We handle agents, browsers, persistence, auth, cookies, and LLMs. The agent runs right next to the browser for minimal latency.

```python
from browser_use import Browser, sandbox, ChatBrowserUse
from browser_use.agent.service import Agent
import asyncio

@sandbox()
async def my_task(browser: Browser):
    agent = Agent(task=&quot;Find the top HN post&quot;, browser=browser, llm=ChatBrowserUse())
    await agent.run()

# Just call it like any async function
asyncio.run(my_task())
```

See [Going to Production](https://docs.browser-use.com/production) for more details.

&lt;br/&gt;

# üöÄ Template Quickstart

**Want to get started even faster?** Generate a ready-to-run template:

```bash
uvx browser-use init --template default
```

This creates a `browser_use_default.py` file with a working example. Available templates:
- `default` - Minimal setup to get started quickly
- `advanced` - All configuration options with detailed comments
- `tools` - Examples of custom tools and extending the agent

You can also specify a custom output path:
```bash
uvx browser-use init --template default --output my_agent.py
```

&lt;br/&gt;

# Demos


### üìã Form-Filling
#### Task = &quot;Fill in this job application with my resume and information.&quot;
![Job Application Demo](https://github.com/user-attachments/assets/57865ee6-6004-49d5-b2c2-6dff39ec2ba9)
[Example code ‚Üó](https://github.com/browser-use/browser-use/blob/main/examples/use-cases/apply_to_job.py)


### üçé Grocery-Shopping
#### Task = &quot;Put this list of items into my instacart.&quot;

https://github.com/user-attachments/assets/a6813fa7-4a7c-40a6-b4aa-382bf88b1850

[Example code ‚Üó](https://github.com/browser-use/browser-use/blob/main/examples/use-cases/buy_groceries.py)


### üíª Personal-Assistant.
#### Task = &quot;Help me find parts for a custom PC.&quot;

https://github.com/user-attachments/assets/ac34f75c-057a-43ef-ad06-5b2c9d42bf06

[Example code ‚Üó](https://github.com/browser-use/browser-use/blob/main/examples/use-cases/pcpartpicker.py)


### üí°See [more examples here ‚Üó](https://docs.browser-use.com/examples) and give us a star!

&lt;br/&gt;

## Integrations, hosting, custom tools, MCP, and more on our [Docs ‚Üó](https://docs.browser-use.com)

&lt;br/&gt;

# FAQ

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;What&#039;s the best model to use?&lt;/b&gt;&lt;/summary&gt;

We optimized **ChatBrowserUse()** specifically for browser automation tasks. On avg it completes tasks 3-5x faster than other models with SOTA accuracy.

**Pricing (per 1M tokens):**
- Input tokens: $0.20
- Cached input tokens: $0.02
- Output tokens: $2.00

For other LLM providers, see our [supported models documentation](https://docs.browser-use.com/supported-models).
&lt;/details&gt;


&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Can I use custom tools with the agent?&lt;/b&gt;&lt;/summary&gt;

Yes! You can add custom tools to extend the agent&#039;s capabilities:

```python
from browser_use import Tools

tools = Tools()

@tools.action(description=&#039;Description of what this tool does.&#039;)
def custom_tool(param: str) -&gt; str:
    return f&quot;Result: {param}&quot;

agent = Agent(
    task=&quot;Your task&quot;,
    llm=llm,
    browser=browser,
    tools=tools,
)
```

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Can I use this for free?&lt;/b&gt;&lt;/summary&gt;

Yes! Browser-Use is open source and free to use. You only need to choose an LLM provider (like OpenAI, Google, ChatBrowserUse, or run local models with Ollama).
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;How do I handle authentication?&lt;/b&gt;&lt;/summary&gt;

Check out our authentication examples:
- [Using real browser profiles](https://github.com/browser-use/browser-use/blob/main/examples/browser/real_browser.py) - Reuse your existing Chrome profile with saved logins
- If you want to use temporary accounts with inbox, choose AgentMail
- To sync your auth profile with the remote browser, run `curl -fsSL https://browser-use.com/profile.sh | BROWSER_USE_API_KEY=XXXX sh` (replace XXXX with your API key)

These examples show how to maintain sessions and handle authentication seamlessly.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;How do I solve CAPTCHAs?&lt;/b&gt;&lt;/summary&gt;

For CAPTCHA handling, you need better browser fingerprinting and proxies. Use [Browser Use Cloud](https://cloud.browser-use.com) which provides stealth browsers designed to avoid detection and CAPTCHA challenges.
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;How do I go into production?&lt;/b&gt;&lt;/summary&gt;

Chrome can consume a lot of memory, and running many agents in parallel can be tricky to manage.

For production use cases, use our [Browser Use Cloud API](https://cloud.browser-use.com) which handles:
- Scalable browser infrastructure
- Memory management
- Proxy rotation
- Stealth browser fingerprinting
- High-performance parallel execution
&lt;/details&gt;

&lt;br/&gt;

&lt;div align=&quot;center&quot;&gt;

**Tell your computer what to do, and it gets it done.**

&lt;img src=&quot;https://github.com/user-attachments/assets/06fa3078-8461-4560-b434-445510c1766f&quot; width=&quot;400&quot;/&gt;

[![Twitter Follow](https://img.shields.io/twitter/follow/Magnus?style=social)](https://x.com/intent/user?screen_name=mamagnus00)
&amp;emsp;&amp;emsp;&amp;emsp;
[![Twitter Follow](https://img.shields.io/twitter/follow/Gregor?style=social)](https://x.com/intent/user?screen_name=gregpr07)

&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt; Made with ‚ù§Ô∏è in Zurich and San Francisco &lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[usestrix/strix]]></title>
            <link>https://github.com/usestrix/strix</link>
            <guid>https://github.com/usestrix/strix</guid>
            <pubDate>Sun, 11 Jan 2026 00:05:31 GMT</pubDate>
            <description><![CDATA[Open-source AI agents for penetration testing]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/usestrix/strix">usestrix/strix</a></h1>
            <p>Open-source AI agents for penetration testing</p>
            <p>Language: Python</p>
            <p>Stars: 18,904</p>
            <p>Forks: 1,964</p>
            <p>Stars today: 166 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://strix.ai/&quot;&gt;
    &lt;img src=&quot;.github/logo.png&quot; width=&quot;150&quot; alt=&quot;Strix Logo&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt;Strix&lt;/h1&gt;

&lt;h2 align=&quot;center&quot;&gt;Open-source AI Hackers to secure your Apps&lt;/h2&gt;

&lt;div align=&quot;center&quot;&gt;

[![Python](https://img.shields.io/pypi/pyversions/strix-agent?color=3776AB)](https://pypi.org/project/strix-agent/)
[![PyPI](https://img.shields.io/pypi/v/strix-agent?color=10b981)](https://pypi.org/project/strix-agent/)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)
[![Docs](https://img.shields.io/badge/Docs-docs.strix.ai-10b981.svg)](https://docs.strix.ai)

[![GitHub Stars](https://img.shields.io/github/stars/usestrix/strix)](https://github.com/usestrix/strix)
[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?&amp;logo=discord&amp;logoColor=white)](https://discord.gg/YjKFvEZSdZ)
[![Website](https://img.shields.io/badge/Website-strix.ai-2d3748.svg)](https://strix.ai)

&lt;a href=&quot;https://trendshift.io/repositories/15362&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/15362&quot; alt=&quot;usestrix%2Fstrix | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;


[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/usestrix/strix)

&lt;/div&gt;

&lt;br&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;.github/screenshot.png&quot; alt=&quot;Strix Demo&quot; width=&quot;800&quot; style=&quot;border-radius: 16px;&quot;&gt;
&lt;/div&gt;

&lt;br&gt;

&gt; [!TIP]
&gt; **New!** Strix now integrates seamlessly with GitHub Actions and CI/CD pipelines. Automatically scan for vulnerabilities on every pull request and block insecure code before it reaches production!

---

## ü¶â Strix Overview

Strix are autonomous AI agents that act just like real hackers - they run your code dynamically, find vulnerabilities, and validate them through actual proof-of-concepts. Built for developers and security teams who need fast, accurate security testing without the overhead of manual pentesting or the false positives of static analysis tools.

**Key Capabilities:**

- üîß **Full hacker toolkit** out of the box
- ü§ù **Teams of agents** that collaborate and scale
- ‚úÖ **Real validation** with PoCs, not false positives
- üíª **Developer‚Äëfirst** CLI with actionable reports
- üîÑ **Auto‚Äëfix &amp; reporting** to accelerate remediation


## üéØ Use Cases

- **Application Security Testing** - Detect and validate critical vulnerabilities in your applications
- **Rapid Penetration Testing** - Get penetration tests done in hours, not weeks, with compliance reports
- **Bug Bounty Automation** - Automate bug bounty research and generate PoCs for faster reporting
- **CI/CD Integration** - Run tests in CI/CD to block vulnerabilities before reaching production

---

## üöÄ Quick Start

**Prerequisites:**
- Docker (running)
- An LLM provider key (e.g. [get OpenAI API key](https://platform.openai.com/api-keys) or use a local LLM)

### Installation &amp; First Scan

```bash
# Install Strix
curl -sSL https://strix.ai/install | bash

# Or via pipx
pipx install strix-agent

# Configure your AI provider
export STRIX_LLM=&quot;openai/gpt-5&quot;
export LLM_API_KEY=&quot;your-api-key&quot;

# Run your first security assessment
strix --target ./app-directory
```

&gt; [!NOTE]
&gt; First run automatically pulls the sandbox Docker image. Results are saved to `strix_runs/&lt;run-name&gt;`

## ‚òÅÔ∏è Run Strix in Cloud

Want to skip the local setup, API keys, and unpredictable LLM costs? Run the hosted cloud version of Strix at **[app.strix.ai](https://strix.ai)**.

Launch a scan in just a few minutes‚Äîno setup or configuration required‚Äîand you‚Äôll get:

- **A full pentest report** with validated findings and clear remediation steps
- **Shareable dashboards** your team can use to track fixes over time
- **CI/CD and GitHub integrations** to block risky changes before production
- **Continuous monitoring** so new vulnerabilities are caught quickly

[**Run your first pentest now ‚Üí**](https://strix.ai)

---

## ‚ú® Features

### üõ†Ô∏è Agentic Security Tools

Strix agents come equipped with a comprehensive security testing toolkit:

- **Full HTTP Proxy** - Full request/response manipulation and analysis
- **Browser Automation** - Multi-tab browser for testing of XSS, CSRF, auth flows
- **Terminal Environments** - Interactive shells for command execution and testing
- **Python Runtime** - Custom exploit development and validation
- **Reconnaissance** - Automated OSINT and attack surface mapping
- **Code Analysis** - Static and dynamic analysis capabilities
- **Knowledge Management** - Structured findings and attack documentation

### üéØ Comprehensive Vulnerability Detection

Strix can identify and validate a wide range of security vulnerabilities:

- **Access Control** - IDOR, privilege escalation, auth bypass
- **Injection Attacks** - SQL, NoSQL, command injection
- **Server-Side** - SSRF, XXE, deserialization flaws
- **Client-Side** - XSS, prototype pollution, DOM vulnerabilities
- **Business Logic** - Race conditions, workflow manipulation
- **Authentication** - JWT vulnerabilities, session management
- **Infrastructure** - Misconfigurations, exposed services

### üï∏Ô∏è Graph of Agents

Advanced multi-agent orchestration for comprehensive security testing:

- **Distributed Workflows** - Specialized agents for different attacks and assets
- **Scalable Testing** - Parallel execution for fast comprehensive coverage
- **Dynamic Coordination** - Agents collaborate and share discoveries

---

## üíª Usage Examples

### Basic Usage

```bash
# Scan a local codebase
strix --target ./app-directory

# Security review of a GitHub repository
strix --target https://github.com/org/repo

# Black-box web application assessment
strix --target https://your-app.com
```

### Advanced Testing Scenarios

```bash
# Grey-box authenticated testing
strix --target https://your-app.com --instruction &quot;Perform authenticated testing using credentials: user:pass&quot;

# Multi-target testing (source code + deployed app)
strix -t https://github.com/org/app -t https://your-app.com

# Focused testing with custom instructions
strix --target api.your-app.com --instruction &quot;Focus on business logic flaws and IDOR vulnerabilities&quot;

# Provide detailed instructions through file (e.g., rules of engagement, scope, exclusions)
strix --target api.your-app.com --instruction-file ./instruction.md
```

### ü§ñ Headless Mode

Run Strix programmatically without interactive UI using the `-n/--non-interactive` flag‚Äîperfect for servers and automated jobs. The CLI prints real-time vulnerability findings, and the final report before exiting. Exits with non-zero code when vulnerabilities are found.

```bash
strix -n --target https://your-app.com
```

### üîÑ CI/CD (GitHub Actions)

Strix can be added to your pipeline to run a security test on pull requests with a lightweight GitHub Actions workflow:

```yaml
name: strix-penetration-test

on:
  pull_request:

jobs:
  security-scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v6

      - name: Install Strix
        run: curl -sSL https://strix.ai/install | bash

      - name: Run Strix
        env:
          STRIX_LLM: ${{ secrets.STRIX_LLM }}
          LLM_API_KEY: ${{ secrets.LLM_API_KEY }}

        run: strix -n -t ./ --scan-mode quick
```

### ‚öôÔ∏è Configuration

```bash
export STRIX_LLM=&quot;openai/gpt-5&quot;
export LLM_API_KEY=&quot;your-api-key&quot;

# Optional
export LLM_API_BASE=&quot;your-api-base-url&quot;  # if using a local model, e.g. Ollama, LMStudio
export PERPLEXITY_API_KEY=&quot;your-api-key&quot;  # for search capabilities
export STRIX_REASONING_EFFORT=&quot;high&quot;  # control thinking effort (default: high, quick scan: medium)
```

&gt; [!NOTE]
&gt; Strix automatically saves your configuration to `~/.strix/cli-config.json`, so you don&#039;t have to re-enter it on every run.

**Recommended models for best results:**

- [OpenAI GPT-5](https://openai.com/api/) ‚Äî `openai/gpt-5`
- [Anthropic Claude Sonnet 4.5](https://claude.com/platform/api) ‚Äî `anthropic/claude-sonnet-4-5`
- [Google Gemini 3 Pro Preview](https://cloud.google.com/vertex-ai) ‚Äî `vertex_ai/gemini-3-pro-preview`

See the [LLM Providers documentation](https://docs.strix.ai/llm-providers/overview) for all supported providers including Vertex AI, Bedrock, Azure, and local models.

## üìö Documentation

Full documentation is available at **[docs.strix.ai](https://docs.strix.ai)** ‚Äî including detailed guides for usage, CI/CD integrations, skills, and advanced configuration.

## ü§ù Contributing

We welcome contributions of code, docs, and new skills - check out our [Contributing Guide](https://docs.strix.ai/contributing) to get started or open a [pull request](https://github.com/usestrix/strix/pulls)/[issue](https://github.com/usestrix/strix/issues).

## üë• Join Our Community

Have questions? Found a bug? Want to contribute? **[Join our Discord!](https://discord.gg/YjKFvEZSdZ)**

## üåü Support the Project

**Love Strix?** Give us a ‚≠ê on GitHub!
## üôè Acknowledgements

Strix builds on the incredible work of open-source projects like [LiteLLM](https://github.com/BerriAI/litellm), [Caido](https://github.com/caido/caido), [ProjectDiscovery](https://github.com/projectdiscovery), [Playwright](https://github.com/microsoft/playwright), and [Textual](https://github.com/Textualize/textual). Huge thanks to their maintainers!


&gt; [!WARNING]
&gt; Only test apps you own or have permission to test. You are responsible for using Strix ethically and legally.

&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Lightricks/ComfyUI-LTXVideo]]></title>
            <link>https://github.com/Lightricks/ComfyUI-LTXVideo</link>
            <guid>https://github.com/Lightricks/ComfyUI-LTXVideo</guid>
            <pubDate>Sun, 11 Jan 2026 00:05:30 GMT</pubDate>
            <description><![CDATA[LTX-Video Support for ComfyUI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Lightricks/ComfyUI-LTXVideo">Lightricks/ComfyUI-LTXVideo</a></h1>
            <p>LTX-Video Support for ComfyUI</p>
            <p>Language: Python</p>
            <p>Stars: 2,834</p>
            <p>Forks: 277</p>
            <p>Stars today: 73 stars today</p>
            <h2>README</h2><pre># ComfyUI-LTXVideo

[![GitHub](https://img.shields.io/badge/LTX-Repo-blue?logo=github)](https://github.com/Lightricks/LTX-2)
[![Website](https://img.shields.io/badge/Website-LTX-181717?logo=google-chrome)](https://ltx.io/model)
[![Model](https://img.shields.io/badge/HuggingFace-Model-orange?logo=huggingface)](https://huggingface.co/Lightricks/LTX-2)
[![LTXV Trainer](https://img.shields.io/badge/LTX-Trainer%20Repo-9146FF)](https://github.com/Lightricks/LTX-2/tree/main/packages/ltx-trainer)
[![Demo](https://img.shields.io/badge/Demo-Try%20Now-brightgreen?logo=vercel)](https://app.ltx.studio/ltx-2-playground/i2v)
[![Paper](https://img.shields.io/badge/Paper-arXiv-B31B1B?logo=arxiv)](https://videos.ltx.io/LTX-2/grants/LTX_2_Technical_Report_compressed.pdf)
[![Discord](https://img.shields.io/badge/Join-Discord-5865F2?logo=discord)](https://discord.gg/ltxplatform)


A collection of powerful custom nodes that extend ComfyUI&#039;s capabilities for the LTX-2 video generation model.

LTX-2 is built into ComfyUI core ([see it here](https://github.com/comfyanonymous/ComfyUI/tree/master/comfy/ldm/lightricks)), making it readily accessible to all ComfyUI users. This repository hosts additional nodes and workflows to help you get the most out of LTX-2&#039;s advanced features.

**To learn more about LTX-2** See the [main LTX-2 repository](https://github.com/Lightricks/LTX-2) for model details and additional resources.


## Prerequisites
Before you begin using an LTX-2 workflow in ComfyUI, make sure you have:

* ComfyUI installed (Download here](https://www.comfy.org/download)
* CUDA-compatible GPU with 32GB+ VRAM
* 100GB+ free disk space for models and cache


## Quick Start üöÄ

We recommend using the LTX-2 workflows available in Comfy Manager.

1. Open ComfyUI
2. Click the Manager button (or press Ctrl+M)
3. Select Install Custom Nodes
4. Search for ‚ÄúLTXVideo‚Äù
5. Click Install
6. Wait for installation to complete
7. Restart ComfyUI

The nodes will appear in your node menu under the ‚ÄúLTXVideo‚Äù category. Required models will be downloaded on first use.


## Example Workflows

The ComfyUI-LTXVideo installation includes several example workflows.
You can see them all at:
&#039;&#039;&#039;
ComfyUI/custom_nodes/ComfyUI-LTXVideo/example_workflows/
&#039;&#039;&#039;

* [`Text to video full model`](./example_workflows/LTX-2_T2V_Full_wLora.json)
* [`Text to video distilled model (Fast)`](./example_workflows/LTX-2_T2V_Distilled_wLora.json)
* [`Image to video full model`](./example_workflows/LTX-2_I2V_Full_wLora.json)
* [`Image to video distilled model (Fast)`](./example_workflows/LTX-2_I2V_Distilled_wLora.json)
* [`Video to video detailer`](./example_workflows/LTX-2_V2V_Detailer.json)
* [`IC-LoRA distilled model (depth + human pose + edges)`](./example_workflows/LTX-2_ICLoRA_All_Distilled.json)

## Required Models

Download the following models:

**LTX-2 Model Checkpoint** - Choose and download one of the models to `COMFYUI_ROOT_FOLDER/models/checkpoints` folder.
  * [`ltx-2-19b-dev-fp8.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-dev-fp8.safetensors)
  * [`ltx-2-19b-distilled-fp8.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled-fp8.safetensors)
  * [`ltx-2-19b-dev.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-dev.safetensors)
  * [`ltx-2-19b-distilled.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled.safetensors)

**Spatial Upscaler** - Required for current two-stage pipeline implementations in this repository. Download to `COMFYUI_ROOT_FOLDER/models/latent_upscale_models` folder.
  * [`ltx-2-spatial-upscaler-x2-1.0.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-spatial-upscaler-x2-1.0.safetensors)

**Temporal Upscaler** - Required for current two-stage pipeline implementations in this repository. Download to `COMFYUI_ROOT_FOLDER/models/latent_upscale_models` folder.
  * [`ltx-2-temporal-upscaler-x2-1.0.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-temporal-upscaler-x2-1.0.safetensors)

**Distilled LoRA** - Required for current two-stage pipeline implementations in this repository (except DistilledPipeline and ICLoraPipeline). Download to `COMFYUI_ROOT_FOLDER/models/loras` folder.
  * [`ltx-2-19b-distilled-lora-384.safetensors`](https://huggingface.co/Lightricks/LTX-2/blob/main/ltx-2-19b-distilled-lora-384.safetensors)

**Gemma Text Encoder** Download all files from the repository to `COMFYUI_ROOT_FOLDER/models/text_encoders/gemma-3-12b-it-qat-q4_0-unquantized`.
  * [`Gemma 3`](https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-unquantized)

**LoRAs** Choose and download to `COMFYUI_ROOT_FOLDER/models/loras` folder.
  * [`ltx-2-19b-ic-lora-canny-control.safetensors`](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Canny-Control/blob/main/ltx-2-19b-ic-lora-canny-control.safetensors)
  * [`ltx-2-19b-ic-lora-depth-control.safetensors`](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Depth-Control/blob/main/ltx-2-19b-ic-lora-depth-control.safetensors)
  * [`ltx-2-19b-ic-lora-detailer.safetensors`](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Detailer/blob/main/ltx-2-19b-ic-lora-detailer.safetensors)
  * [`ltx-2-19b-ic-lora-pose-control.safetensors`](https://huggingface.co/Lightricks/LTX-2-19b-IC-LoRA-Pose-Control/blob/main/ltx-2-19b-ic-lora-pose-control.safetensors)
  * [`ltx-2-19b-lora-camera-control-dolly-in.safetensors`](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-In/blob/main/ltx-2-19b-lora-camera-control-dolly-in.safetensors)
  * [`ltx-2-19b-lora-camera-control-dolly-left.safetensors`](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Left/blob/main/ltx-2-19b-lora-camera-control-dolly-left.safetensors)
  * [`ltx-2-19b-lora-camera-control-dolly-out.safetensors`](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Out/blob/main/ltx-2-19b-lora-camera-control-dolly-out.safetensors)
  * [`ltx-2-19b-lora-camera-control-dolly-right.safetensors`](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Right/blob/main/ltx-2-19b-lora-camera-control-dolly-right.safetensors)
  * [`ltx-2-19b-lora-camera-control-jib-down.safetensors`](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Jib-Down/blob/main/ltx-2-19b-lora-camera-control-jib-down.safetensors)
  * [`ltx-2-19b-lora-camera-control-jib-up.safetensors`](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Jib-Up/blob/main/ltx-2-19b-lora-camera-control-jib-up.safetensors)
  * [`ltx-2-19b-lora-camera-control-static.safetensors`](https://huggingface.co/Lightricks/LTX-2-19b-LoRA-Camera-Control-Static/blob/main/ltx-2-19b-lora-camera-control-static.safetensors)


## Advanced Techniques

### Low VRAM
* For systems with low VRAM you can use the model loader nodes from [low_vram_loaders.py](./low_vram_loaders.py). Those nodes ensure the correct order of execution and perform the model offloading such that generation fits in 32 GB VRAM.
* Use --reserve-vram ComfyUI parameter: `python -m main --reserve-vram 5` (or other number in GB).
* For complete information about using LTX-2 models, workflows, and nodes in ComfyUI, please visit our [Open Source documentation](https://docs.ltx.video/open-source-model/integration-tools/comfy-ui).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[EveryInc/compound-engineering-plugin]]></title>
            <link>https://github.com/EveryInc/compound-engineering-plugin</link>
            <guid>https://github.com/EveryInc/compound-engineering-plugin</guid>
            <pubDate>Sun, 11 Jan 2026 00:05:29 GMT</pubDate>
            <description><![CDATA[Official Claude Code compound engineering plugin]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/EveryInc/compound-engineering-plugin">EveryInc/compound-engineering-plugin</a></h1>
            <p>Official Claude Code compound engineering plugin</p>
            <p>Language: Python</p>
            <p>Stars: 4,411</p>
            <p>Forks: 371</p>
            <p>Stars today: 180 stars today</p>
            <h2>README</h2><pre># Compound Engineering Plugin

A Claude Code plugin that makes each unit of engineering work easier than the last.

## Install

```bash
/plugin marketplace add https://github.com/EveryInc/compound-engineering-plugin
/plugin install compound-engineering
```

## Workflow

```
Plan ‚Üí Work ‚Üí Review ‚Üí Compound ‚Üí Repeat
```

| Command | Purpose |
|---------|---------|
| `/workflows:plan` | Turn feature ideas into detailed implementation plans |
| `/workflows:work` | Execute plans with worktrees and task tracking |
| `/workflows:review` | Multi-agent code review before merging |
| `/workflows:compound` | Document learnings to make future work easier |

Each cycle compounds: plans inform future plans, reviews catch more issues, patterns get documented.

## Philosophy

**Each unit of engineering work should make subsequent units easier‚Äînot harder.**

Traditional development accumulates technical debt. Every feature adds complexity. The codebase becomes harder to work with over time.

Compound engineering inverts this. 80% is in planning and review, 20% is in execution:
- Plan thoroughly before writing code
- Review to catch issues and capture learnings
- Codify knowledge so it&#039;s reusable
- Keep quality high so future changes are easy

## Learn More

- [Full component reference](plugins/compound-engineering/README.md) - all agents, commands, skills
- [Compound engineering: how Every codes with agents](https://every.to/chain-of-thought/compound-engineering-how-every-codes-with-agents)
- [The story behind compounding engineering](https://every.to/source-code/my-ai-had-already-fixed-the-code-before-i-saw-it)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NVIDIA-NeMo/NeMo]]></title>
            <link>https://github.com/NVIDIA-NeMo/NeMo</link>
            <guid>https://github.com/NVIDIA-NeMo/NeMo</guid>
            <pubDate>Sun, 11 Jan 2026 00:05:28 GMT</pubDate>
            <description><![CDATA[A scalable generative AI framework built for researchers and developers working on Large Language Models, Multimodal, and Speech AI (Automatic Speech Recognition and Text-to-Speech)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NVIDIA-NeMo/NeMo">NVIDIA-NeMo/NeMo</a></h1>
            <p>A scalable generative AI framework built for researchers and developers working on Large Language Models, Multimodal, and Speech AI (Automatic Speech Recognition and Text-to-Speech)</p>
            <p>Language: Python</p>
            <p>Stars: 16,502</p>
            <p>Forks: 3,278</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre>[![Project Status: Active -- The project has reached a stable, usable state and is being actively developed.](http://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active)
[![Documentation](https://readthedocs.com/projects/nvidia-nemo/badge/?version=main)](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/)
[![CodeQL](https://github.com/nvidia/nemo/actions/workflows/codeql.yml/badge.svg?branch=main&amp;event=push)](https://github.com/nvidia/nemo/actions/workflows/codeql.yml)
[![NeMo core license and license for collections in this repo](https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg)](https://github.com/NVIDIA/NeMo/blob/master/LICENSE)
[![Release version](https://badge.fury.io/py/nemo-toolkit.svg)](https://badge.fury.io/py/nemo-toolkit)
[![Python version](https://img.shields.io/pypi/pyversions/nemo-toolkit.svg)](https://badge.fury.io/py/nemo-toolkit)
[![PyPi total downloads](https://static.pepy.tech/personalized-badge/nemo-toolkit?period=total&amp;units=international_system&amp;left_color=grey&amp;right_color=brightgreen&amp;left_text=downloads)](https://pepy.tech/project/nemo-toolkit)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

# **NVIDIA NeMo Speech Collection**

## Latest News

&lt;!-- markdownlint-disable --&gt;
&lt;details open&gt;
  &lt;summary&gt;&lt;b&gt;&lt;a href=https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16&gt;NVIDIA-Nemotron-3-Nano-30B-A3B&lt;/a&gt; is out with full reproducible script and recipes! Check out &lt;a href=https://github.com/NVIDIA-NeMo/Megatron-Bridge/tree/nano-v3&gt;NeMo Megatron-Bridge&lt;/a&gt;, &lt;a href=https://github.com/NVIDIA-NeMo/AutoModel/blob/main/examples/llm_finetune/nemotron/nemotron_nano_v3_squad.yaml&gt;NeMo AutoModel&lt;/a&gt;, &lt;a href=https://github.com/NVIDIA-NeMo/RL&gt;NeMo-RL&lt;/a&gt; and &lt;a href=https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo?version=25.11.nemotron_3_nano&gt;NGC container&lt;/a&gt; to try them!&lt;/b&gt; (2025-12-15)
&lt;/details&gt;


&lt;details open&gt;
  &lt;summary&gt;&lt;b&gt;‚ö†Ô∏è Pivot notice: This repo will pivot to focus on speech models collections only. Please refer to &lt;a href=https://github.com/NVIDIA-NeMo&gt;NeMo Framework Github Org&lt;/a&gt; for the complete list of repos under NeMo Framework&lt;/b&gt;&lt;/summary&gt;
      NeMo 2.0, with its support for LLMs and VLMs will be deprecated by 25.11, and replaced by &lt;a href=https://github.com/NVIDIA-NeMo/Megatron-Bridge&gt;NeMo Megatron-Bridge&lt;/a&gt; and &lt;a href=https://github.com/NVIDIA-NeMo/AutoModel&gt;NeMo AutoModel&lt;/a&gt;. More details can be found in the &lt;a href=https://github.com/NVIDIA-NeMo&gt;NeMo Framework GitHub org readme&lt;/a&gt;. (2025-10-10)

      Following collections are deprecated and will be removed in a later release, please use previous versions if you are using them:
      - nlp
      - llm
      - vlm
      - vision
&lt;/details&gt;

&lt;details closed&gt;
  &lt;summary&gt;&lt;b&gt;Pretrain and finetune :hugs:Hugging Face models via AutoModel&lt;/b&gt;&lt;/summary&gt;
      NeMo Framework&#039;s latest feature AutoModel enables broad support for :hugs:Hugging Face models, with 25.04 focusing on

  
- &lt;a href=https://huggingface.co/transformers/v3.5.1/model_doc/auto.html#automodelforcausallm&gt;AutoModelForCausalLM&lt;/a&gt; in the &lt;a href=&quot;https://huggingface.co/models?pipeline_tag=text-generation&amp;sort=trending&quot;&gt;Text Generation&lt;/a&gt; category
- &lt;a href=https://huggingface.co/docs/transformers/main/model_doc/auto#transformers.AutoModelForImageTextToText&gt;AutoModelForImageTextToText&lt;/a&gt; in the &lt;a href=&quot;https://huggingface.co/models?pipeline_tag=image-text-to-text&amp;sort=trending&quot;&gt;Image-Text-to-Text&lt;/a&gt; category

More Details in Blog: &lt;a href=https://developer.nvidia.com/blog/run-hugging-face-models-instantly-with-day-0-support-from-nvidia-nemo-framework&gt;Run Hugging Face Models Instantly with Day-0 Support from NVIDIA NeMo Framework&lt;/a&gt;. Future releases will enable support for more model families such as Video Generation models.(2025-05-19)
&lt;/details&gt;

&lt;details closed&gt;
  &lt;summary&gt;&lt;b&gt;Training on Blackwell using NeMo&lt;/b&gt;&lt;/summary&gt;
      NeMo Framework has added Blackwell support, with &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance_summary.html&gt;performance benchmarks on GB200 &amp; B200&lt;/a&gt;. More optimizations to come in the upcoming releases.(2025-05-19)
&lt;/details&gt;

&lt;details closed&gt;
  &lt;summary&gt;&lt;b&gt;Training Performance on GPU Tuning Guide&lt;/b&gt;&lt;/summary&gt;
      NeMo Framework has published &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance-guide.html&gt;a comprehensive guide for performance tuning to achieve optimal throughput&lt;/a&gt;! (2025-05-19)
&lt;/details&gt;

&lt;details closed&gt;
  &lt;summary&gt;&lt;b&gt;New Models Support&lt;/b&gt;&lt;/summary&gt;
      NeMo Framework has added support for latest community models - &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/vlms/llama4.html&gt;Llama 4&lt;/a&gt;, &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/vision/diffusionmodels/flux.html&gt;Flux&lt;/a&gt;, &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/llama_nemotron.html&gt;Llama Nemotron&lt;/a&gt;, &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/hyena.html#&gt;Hyena &amp; Evo2&lt;/a&gt;, &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/vlms/qwen2vl.html&gt;Qwen2-VL&lt;/a&gt;, &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/qwen2.html&gt;Qwen2.5&lt;/a&gt;, Gemma3, Qwen3-30B&amp;32B.(2025-05-19)
&lt;/details&gt;


&lt;details closed&gt;
  &lt;summary&gt;&lt;b&gt;NeMo Framework 2.0&lt;/b&gt;&lt;/summary&gt;
      We&#039;ve released NeMo 2.0, an update on the NeMo Framework which prioritizes modularity and ease-of-use. Please refer to the &lt;a href=https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/index.html&gt;NeMo Framework User Guide&lt;/a&gt; to get started.
&lt;/details&gt;
&lt;details closed&gt;
  &lt;summary&gt;&lt;b&gt;New Cosmos World Foundation Models Support&lt;/b&gt;&lt;/summary&gt;
    &lt;details&gt; 
      &lt;summary&gt; &lt;a href=&quot;https://developer.nvidia.com/blog/advancing-physical-ai-with-nvidia-cosmos-world-foundation-model-platform&quot;&gt;Advancing Physical AI with NVIDIA Cosmos World Foundation Model Platform &lt;/a&gt; (2025-01-09) 
      &lt;/summary&gt; 
        The end-to-end NVIDIA Cosmos platform accelerates world model development for physical AI systems. Built on CUDA, Cosmos combines state-of-the-art world foundation models, video tokenizers, and AI-accelerated data processing pipelines. Developers can accelerate world model development by fine-tuning Cosmos world foundation models or building new ones from the ground up. These models create realistic synthetic videos of environments and interactions, providing a scalable foundation for training complex systems, from simulating humanoid robots performing advanced actions to developing end-to-end autonomous driving models. 
        &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/accelerate-custom-video-foundation-model-pipelines-with-new-nvidia-nemo-framework-capabilities/&quot;&gt;
          Accelerate Custom Video Foundation Model Pipelines with New NVIDIA NeMo Framework Capabilities
        &lt;/a&gt; (2025-01-07)
      &lt;/summary&gt;
        The NeMo Framework now supports training and customizing the &lt;a href=&quot;https://github.com/NVIDIA/Cosmos&quot;&gt;NVIDIA Cosmos&lt;/a&gt; collection of world foundation models. Cosmos leverages advanced text-to-world generation techniques to create fluid, coherent video content from natural language prompts.
        &lt;br&gt;&lt;br&gt;
        You can also now accelerate your video processing step using the &lt;a href=&quot;https://developer.nvidia.com/nemo-curator-video-processing-early-access&quot;&gt;NeMo Curator&lt;/a&gt; library, which provides optimized video processing and captioning features that can deliver up to 89x faster video processing when compared to an unoptimized CPU pipeline.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
&lt;/details&gt;
&lt;details closed&gt;
  &lt;summary&gt;&lt;b&gt;Large Language Models and Multimodal Models&lt;/b&gt;&lt;/summary&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/state-of-the-art-multimodal-generative-ai-model-development-with-nvidia-nemo/&quot;&gt;
          State-of-the-Art Multimodal Generative AI Model Development with NVIDIA NeMo
        &lt;/a&gt; (2024-11-06)
      &lt;/summary&gt;
        NVIDIA recently announced significant enhancements to the NeMo platform, focusing on multimodal generative AI models. The update includes NeMo Curator and the Cosmos tokenizer, which streamline the data curation process and enhance the quality of visual data. These tools are designed to handle large-scale data efficiently, making it easier to develop high-quality AI models for various applications, including robotics and autonomous driving. The Cosmos tokenizers, in particular, efficiently map visual data into compact, semantic tokens, which is crucial for training large-scale generative models. The tokenizer is available now on the &lt;a href=https://github.com/NVIDIA/cosmos-tokenizer&gt;NVIDIA/cosmos-tokenizer&lt;/a&gt; GitHub repo and on &lt;a href=https://huggingface.co/nvidia/Cosmos-Tokenizer-CV8x8x8&gt;Hugging Face&lt;/a&gt;.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/llama/index.html#new-llama-3-1-support for more information/&quot;&gt;
        New Llama 3.1 Support
        &lt;/a&gt; (2024-07-23)
      &lt;/summary&gt;
        The NeMo Framework now supports training and customizing the Llama 3.1 collection of LLMs from Meta.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://aws.amazon.com/blogs/machine-learning/accelerate-your-generative-ai-distributed-training-workloads-with-the-nvidia-nemo-framework-on-amazon-eks/&quot;&gt;
          Accelerate your Generative AI Distributed Training Workloads with the NVIDIA NeMo Framework on Amazon EKS
        &lt;/a&gt; (2024-07-16)
      &lt;/summary&gt;
     NVIDIA NeMo Framework now runs distributed training workloads on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. For step-by-step instructions on creating an EKS cluster and running distributed training workloads with NeMo, see the GitHub repository &lt;a href=&quot;https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/2.nemo-launcher/EKS/&quot;&gt; here.&lt;/a&gt;
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-nemo-accelerates-llm-innovation-with-hybrid-state-space-model-support/&quot;&gt;
          NVIDIA NeMo Accelerates LLM Innovation with Hybrid State Space Model Support
        &lt;/a&gt; (2024/06/17)
      &lt;/summary&gt;
     NVIDIA NeMo and Megatron Core now support pre-training and fine-tuning of state space models (SSMs). NeMo also supports training models based on the Griffin architecture as described by Google DeepMind. 
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
      &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://huggingface.co/models?sort=trending&amp;search=nvidia%2Fnemotron-4-340B&quot;&gt;
          NVIDIA releases 340B base, instruct, and reward models pretrained on a total of 9T tokens.
        &lt;/a&gt; (2024-06-18)
      &lt;/summary&gt;
      See documentation and tutorials for SFT, PEFT, and PTQ with 
      &lt;a href=&quot;https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/nemotron/index.html&quot;&gt;
        Nemotron 340B 
      &lt;/a&gt;
      in the NeMo Framework User Guide.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-sets-new-generative-ai-performance-and-scale-records-in-mlperf-training-v4-0/&quot;&gt;
          NVIDIA sets new generative AI performance and scale records in MLPerf Training v4.0
        &lt;/a&gt; (2024/06/12)
      &lt;/summary&gt;
      Using NVIDIA NeMo Framework and NVIDIA Hopper GPUs NVIDIA was able to scale to 11,616 H100 GPUs and achieve near-linear performance scaling on LLM pretraining. 
      NVIDIA also achieved the highest LLM fine-tuning performance and raised the bar for text-to-image training.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
        &lt;summary&gt;
          &lt;a href=&quot;https://cloud.google.com/blog/products/compute/gke-and-nvidia-nemo-framework-to-train-generative-ai-models&quot;&gt;
            Accelerate your generative AI journey with NVIDIA NeMo Framework on GKE
          &lt;/a&gt; (2024/03/16)
        &lt;/summary&gt;
        An end-to-end walkthrough to train generative AI models on the Google Kubernetes Engine (GKE) using the NVIDIA NeMo Framework is available at https://github.com/GoogleCloudPlatform/nvidia-nemo-on-gke. 
        The walkthrough includes detailed instructions on how to set up a Google Cloud Project and pre-train a GPT model using the NeMo Framework.
        &lt;br&gt;&lt;br&gt;
      &lt;/details&gt;
&lt;/details&gt;
&lt;details closed&gt;
  &lt;summary&gt;&lt;b&gt;Speech Recognition&lt;/b&gt;&lt;/summary&gt;
  &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/accelerating-leaderboard-topping-asr-models-10x-with-nvidia-nemo/&quot;&gt;
          Accelerating Leaderboard-Topping ASR Models 10x with NVIDIA NeMo
        &lt;/a&gt; (2024/09/24)
      &lt;/summary&gt;
      NVIDIA NeMo team released a number of inference optimizations for CTC, RNN-T, and TDT models that resulted in up to 10x inference speed-up. 
      These models now exceed an inverse real-time factor (RTFx) of 2,000, with some reaching RTFx of even 6,000.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/new-standard-for-speech-recognition-and-translation-from-the-nvidia-nemo-canary-model/&quot;&gt;
          New Standard for Speech Recognition and Translation from the NVIDIA NeMo Canary Model
        &lt;/a&gt; (2024/04/18)
      &lt;/summary&gt;
      The NeMo team just released Canary, a multilingual model that transcribes speech in English, Spanish, German, and French with punctuation and capitalization. 
      Canary also provides bi-directional translation, between English and the three other supported languages.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
    &lt;details&gt;
      &lt;summary&gt;
        &lt;a href=&quot;https://developer.nvidia.com/blog/pushing-the-boundaries-of-speech-recognition-with-nemo-parakeet-asr-models/&quot;&gt;
          Pushing the Boundaries of Speech Recognition with NVIDIA NeMo Parakeet ASR Models
        &lt;/a&gt; (2024/04/18)
      &lt;/summary&gt;
      NVIDIA NeMo, an end-to-end platform for the development of multimodal generative AI models at scale anywhere‚Äîon any cloud and on-premises‚Äîreleased the Parakeet family of automatic speech recognition (ASR) models. 
      These state-of-the-art ASR models, developed in collaboration with Suno.ai, transcribe spoken English with exceptional accuracy.
      &lt;br&gt;&lt;br&gt;
    &lt;/details&gt;
  &lt;details&gt;
    &lt;summary&gt;
      &lt;a href=&quot;https://developer.nvidia.com/blog/turbocharge-asr-accuracy-and-speed-with-nvidia-nemo-parakeet-tdt/&quot;&gt;
        Turbocharge ASR Accuracy and Speed with NVIDIA NeMo Parakeet-TDT
      &lt;/a&gt; (2024/04/18)
    &lt;/summary&gt;
    NVIDIA NeMo, an end-to-end platform for developing multimodal generative AI models at scale anywhere‚Äîon any cloud and on-premises‚Äîrecently released Parakeet-TDT. 
    This new addition to the ‚ÄØNeMo ASR Parakeet model family boasts better accuracy and 64% greater speed over the previously best model, Parakeet-RNNT-1.1B.
    &lt;br&gt;&lt;br&gt;
  &lt;/details&gt;
&lt;/details&gt;
&lt;!-- markdownlint-enable --&gt;

## Introduction

NVIDIA NeMo Framework is a scalable and cloud-native generative AI
framework built for researchers and PyTorch developers working on Large
Language Models (LLMs), Multimodal Models (MMs), Automatic Speech
Recognition (ASR), Text to Speech (TTS), and Computer Vision (CV)
domains. It is designed to help you efficiently create, customize, and
deploy new generative AI models by leveraging existing code and
pre-trained model checkpoints.

For technical documentation, please see the [NeMo Framework User
Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html).

## What&#039;s New in NeMo 2.0

NVIDIA NeMo 2.0 introduces several significant improvements over its predecessor, NeMo 1.0, enhancing flexibility, performance, and scalability.

- **Python-Based Configuration** - NeMo 2.0 transitions from YAML files to a Python-based configuration, providing more flexibility and control. This shift makes it easier to extend and customize configurations programmatically.

- **Modular Abstractions** - By adopting PyTorch Lightning‚Äôs modular abstractions, NeMo 2.0 simplifies adaptation and experimentation. This modular approach allows developers to more easily modify and experiment with different components of their models.

- **Scalability** - NeMo 2.0 seamlessly scaling large-scale experiments across thousands of GPUs using [NeMo-Run](https://github.com/NVIDIA/NeMo-Run), a powerful tool designed to streamline the configuration, execution, and management of machine learning experiments across computing environments.

Overall, these enhancements make NeMo 2.0 a powerful, scalable, and user-friendly framework for AI model development.

### Get Started with NeMo 2.0

- Refer to the [Quickstart](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/quickstart.html) for examples of using NeMo-Run to launch NeMo 2.0 experiments locally and on a slurm cluster.
- For more information about NeMo 2.0, see the [NeMo Framework User Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/index.html).
- For an in-depth exploration of the main features of NeMo 2.0, see the [Feature Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/features/index.html#feature-guide).
- To transition from NeMo 1.0 to 2.0, see the [Migration Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemo-2.0/migration/index.html#migration-guide) for step-by-step instructions.

## Training and Customization

All NeMo models are trained with
[Lightning](https://github.com/Lightning-AI/lightning). Training is
automatically scalable to 1000s of GPUs. You can check the performance benchmarks using the
latest NeMo Framework container [here](https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance_summary.html).

When applicable, NeMo models leverage cutting-edge distributed training
techniques, incorporating [parallelism
strategies](https://docs.nvidia.com/nemo-framework/user-guide/latest/modeloverview.html)
to enable efficient training of very large models. These techniques
include Tensor Parallelism (TP), Pipeline Parallelism (PP), Fully
Sharded Data Parallelism (FSDP), Mixture-of-Experts (MoE), and Mixed
Precision Training with BFloat16 and FP8, as well as others.

In addition to supervised fine-tuning (SFT), NeMo also supports the
latest parameter efficient fine-tuning (PEFT) techniques such as LoRA,
P-Tuning, Adapters, and IA3. 

## Speech AI

NeMo ASR and TTS models can be optimized for inference and deployed for
production use cases with [NVIDIA Riva](https://developer.nvidia.com/riva).


## Get Started with NeMo Framework

Getting started with NeMo Framework is easy. State-of-the-art pretrained
NeMo models are freely available on [Hugging Face
Hub](https://huggingface.co/models?library=nemo&amp;sort=downloads&amp;search=nvidia)
and [NVIDIA
NGC](https://catalog.ngc.nvidia.com/models?query=nemo&amp;orderBy=weightPopularDESC).
These models can be used to generate text or images, transcribe audio,
and synthesize speech in just a few lines of code.

We have extensive
[tutorials](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/starthere/tutorials.html)
that can be run on [Google Colab](https://colab.research.google.com) or
with our [NGC NeMo Framework
Container](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo).
We also have
[playbooks](https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html)
for users who want to train NeMo models with the NeMo Framework
Launcher.

For advanced users who want to train NeMo models from scratch or
fine-tune existing NeMo models, we have a full suite of [example
scripts](https://github.com/NVIDIA/NeMo/tree/main/examples) that support
multi-GPU/multi-node training.

## Key Features

- [Multimodal](nemo/collections/multimodal/README.md)
- [Automatic Speech Recognition](nemo/

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[bunkerity/bunkerweb]]></title>
            <link>https://github.com/bunkerity/bunkerweb</link>
            <guid>https://github.com/bunkerity/bunkerweb</guid>
            <pubDate>Sun, 11 Jan 2026 00:05:27 GMT</pubDate>
            <description><![CDATA[üõ°Ô∏è Open-source and next-generation Web Application Firewall (WAF)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/bunkerity/bunkerweb">bunkerity/bunkerweb</a></h1>
            <p>üõ°Ô∏è Open-source and next-generation Web Application Firewall (WAF)</p>
            <p>Language: Python</p>
            <p>Stars: 9,758</p>
            <p>Forks: 557</p>
            <p>Stars today: 17 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
	&lt;img alt=&quot;BunkerWeb logo&quot; src=&quot;https://github.com/bunkerity/bunkerweb/raw/v1.6.7/misc/logo.png&quot; height=100 width=350 /&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
	&lt;img src=&quot;https://img.shields.io/github/v/release/bunkerity/bunkerweb?label=stable&quot; /&gt;
	&lt;img src=&quot;https://img.shields.io/github/v/release/bunkerity/bunkerweb?include_prereleases&amp;label=latest&quot; /&gt;
	&lt;br /&gt;
	&lt;img src=&quot;https://img.shields.io/github/last-commit/bunkerity/bunkerweb&quot; /&gt;
	&lt;img src=&quot;https://img.shields.io/github/issues/bunkerity/bunkerweb&quot;&gt;
	&lt;img src=&quot;https://img.shields.io/github/issues-pr/bunkerity/bunkerweb&quot;&gt;
	&lt;br /&gt;
	&lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/bunkerity/bunkerweb/dev.yml?branch=dev&amp;label=CI%2FCD%20dev&quot; /&gt;
	&lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/bunkerity/bunkerweb/staging.yml?branch=staging&amp;label=CI%2FCD%20staging&quot; /&gt;
	&lt;a href=&quot;https://www.bestpractices.dev/projects/8001&quot;&gt;
		&lt;img src=&quot;https://www.bestpractices.dev/projects/8001/badge&quot;&gt;
	&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
	üåê &lt;a href=&quot;https://www.bunkerweb.io/?utm_campaign=self&amp;utm_source=github&quot;&gt;Website&lt;/a&gt;
	 &amp;#124;
	ü§ù &lt;a href=&quot;https://panel.bunkerweb.io/?utm_campaign=self&amp;utm_source=github&quot;&gt;Panel&lt;/a&gt;
	 &amp;#124;
	üìì &lt;a href=&quot;https://docs.bunkerweb.io/?utm_campaign=self&amp;utm_source=github&quot;&gt;Documentation&lt;/a&gt;
	 &amp;#124;
	üë®‚Äçüíª &lt;a href=&quot;https://demo.bunkerweb.io/?utm_campaign=self&amp;utm_source=github&quot;&gt;Demo&lt;/a&gt;
	 &amp;#124;
	üì± &lt;a href=&quot;https://demo-ui.bunkerweb.io/?utm_campaign=self&amp;utm_source=github&quot;&gt;Demo UI&lt;/a&gt;
	 &amp;#124;
	üß© &lt;a href=&quot;https://github.com/bunkerity/bunkerweb-templates&quot;&gt;Templates&lt;/a&gt;
	 &amp;#124;
	üõ°Ô∏è &lt;a href=&quot;https://github.com/bunkerity/bunkerweb/raw/v1.6.7/examples&quot;&gt;Examples&lt;/a&gt;
	&lt;br/&gt;
	üí¨ &lt;a href=&quot;https://discord.com/invite/fTf46FmtyD&quot;&gt;Chat&lt;/a&gt;
	 &amp;#124;
	üìù &lt;a href=&quot;https://github.com/bunkerity/bunkerweb/discussions&quot;&gt;Forum&lt;/a&gt;
	 &amp;#124;
	üó∫Ô∏è &lt;a href=&quot;https://www.bunkerweb.io/threatmap/?utm_campaign=self&amp;utm_source=github&quot;&gt;Threatmap&lt;/a&gt;
	&amp;#124;
	üìä &lt;a href=&quot;https://status.bunkerweb.io/?utm_campaign=self&amp;utm_source=github&quot;&gt;Status&lt;/a&gt;
	&amp;#124;
	üîé &lt;a href=&quot;https://forms.gle/e3VgymAteYPnwM1j9&quot;&gt;Feedback&lt;/a&gt;
&lt;/p&gt;

&gt; üõ°Ô∏è Make security by default great again!

# BunkerWeb

&lt;p align=&quot;center&quot;&gt;
	&lt;img alt=&quot;Overview banner&quot; src=&quot;https://github.com/bunkerity/bunkerweb/raw/v1.6.7/docs/assets/img/intro-overview.svg&quot; /&gt;
&lt;/p&gt;

BunkerWeb is a next-generation, open-source Web Application Firewall (WAF).

Being a full-featured web server (based on [NGINX](https://nginx.org/) under the hood), it will protect your web services to make them &quot;secure by default.&quot; BunkerWeb integrates seamlessly into your existing environments ([Linux](https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;utm_source=github#linux), [Docker](https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;utm_source=github#docker), [Swarm](https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;utm_source=github#swarm), [Kubernetes](https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;utm_source=github#kubernetes), ‚Ä¶) as a reverse proxy and is fully configurable (don&#039;t panic, there is an [awesome web UI](https://docs.bunkerweb.io/1.6.7/web-ui/?utm_campaign=self&amp;utm_source=github) if you don&#039;t like the CLI) to meet your own use cases. In other words, cybersecurity is no longer a hassle.

BunkerWeb contains primary [security features](https://docs.bunkerweb.io/1.6.7/advanced/?utm_campaign=self&amp;utm_source=github#security-tuning) as part of the core but can be easily extended with additional ones thanks to a [plugin system](https://docs.bunkerweb.io/1.6.7/plugins/?utm_campaign=self&amp;utm_source=github).

## Why BunkerWeb?

https://github.com/user-attachments/assets/c3fed740-28d8-4335-ab05-113a9e815b4f

- **Easy integration into existing environments**: Seamlessly integrate BunkerWeb into various environments such as Linux, Docker, Swarm, Kubernetes, and more. Enjoy a smooth transition and hassle-free implementation.
- **Highly customizable**: Tailor BunkerWeb to your specific requirements with ease. Enable, disable, and configure features effortlessly, allowing you to customize the security settings according to your unique use case.
- **Secure by default**: BunkerWeb provides out-of-the-box, hassle-free minimal security for your web services. Experience peace of mind and enhanced protection right from the start.
- **Awesome web UI**: Take control of BunkerWeb more efficiently with the exceptional web user interface (UI). Navigate settings and configurations effortlessly through a user-friendly graphical interface, eliminating the need for the command-line interface (CLI).
- **Plugin system**: Extend the capabilities of BunkerWeb to meet your own use cases. Seamlessly integrate additional security measures and customize the functionality of BunkerWeb according to your specific requirements.
- **Free as in &quot;freedom&quot;**: BunkerWeb is licensed under the free [AGPLv3 license](https://www.gnu.org/licenses/agpl-3.0.en.html), embracing the principles of freedom and openness. Enjoy the freedom to use, modify, and distribute the software, backed by a supportive community.
- **Professional services**: Get technical support, tailored consulting, and custom development directly from the maintainers of BunkerWeb. Visit the [Bunker Panel](https://panel.bunkerweb.io/?utm_campaign=self&amp;utm_source=github) for more information.

## Security features

A non-exhaustive list of security features:

- **HTTPS** support with transparent **Let&#039;s Encrypt** automation
- **State-of-the-art web security**: HTTP security headers, prevent leaks, TLS hardening, ...
- Integrated **ModSecurity WAF** with the **OWASP Core Rule Set**
- **Automatic ban** of strange behaviors based on HTTP status codes
- Apply **connection and request limits** for clients
- **Block bots** by asking them to solve a **challenge** (e.g., cookie, JavaScript, captcha, hCaptcha, or reCAPTCHA)
- **Block known bad IPs** with external blacklists and DNSBL
- And much more...

Learn more about the core security features in the [security tuning](https://docs.bunkerweb.io/1.6.7/advanced/?utm_campaign=self&amp;utm_source=github#security-tuning) section of the documentation.

## Demo

https://github.com/user-attachments/assets/6fc0e3c1-d353-4a84-bad0-15bf9b6623a5

A demo website protected with BunkerWeb is available at [demo.bunkerweb.io](https://demo.bunkerweb.io/?utm_campaign=self&amp;utm_source=github). Feel free to visit it and perform some security tests.

## Web UI

https://github.com/user-attachments/assets/a3ed56f8-c124-4ca9-b8b3-4be0913b3078

BunkerWeb offers an optional [user interface](web-ui.md) to manage your instances and their configurations. An online read-only demo is available at [demo-ui.bunkerweb.io](https://demo-ui.bunkerweb.io/?utm_campaign=self&amp;utm_source=doc), feel free to test it yourself.

## BunkerWeb Cloud

Don&#039;t want to self-host and manage your own BunkerWeb instance(s)? You might be interested in BunkerWeb Cloud, our fully managed SaaS offering for BunkerWeb.

Order your [BunkerWeb Cloud instance](https://panel.bunkerweb.io/store/bunkerweb-cloud?utm_campaign=self&amp;utm_source=doc) and get access to:

- A fully managed BunkerWeb instance hosted in our cloud
- All BunkerWeb features, including PRO ones
- A monitoring platform with dashboards and alerts
- Technical support to assist you with configuration

If you are interested in the BunkerWeb Cloud offering, don&#039;t hesitate to [contact us](https://panel.bunkerweb.io/contact.php?utm_campaign=self&amp;utm_source=doc) so we can discuss your needs.

## PRO version

Want to quickly test BunkerWeb PRO for one month? Use the code `freetrial` when placing your order on the [BunkerWeb panel](https://panel.bunkerweb.io/store/bunkerweb-pro?utm_campaign=self&amp;utm_source=doc) or by clicking [here](https://panel.bunkerweb.io/cart.php?a=add&amp;pid=19&amp;promocode=freetrial&amp;utm_campaign=self&amp;utm_source=doc) to directly to apply the promo code (will be effective at checkout).

When using BunkerWeb, you have the choice of the version you want to use: open-source or PRO.

Whether it&#039;s enhanced security, an enriched user experience, or technical monitoring, the BunkerWeb PRO version allows you to fully benefit from BunkerWeb and meet your professional needs.

In the documentation or the user interface, PRO features are annotated with a crown &lt;img src=&quot;https://docs.bunkerweb.io/1.6.7/assets/img/pro-icon.svg&quot; alt=&quot;crown pro icon&quot; height=&quot;32px&quot; width=&quot;32px&quot;&gt; to distinguish them from those integrated into the open-source version.

You can upgrade from the open-source version to the PRO one easily and at any time. The process is straightforward:

- Claim your [free trial on the BunkerWeb panel](https://panel.bunkerweb.io/store/bunkerweb-pro?utm_campaign=self&amp;utm_source=doc) by using the `freetrial` promo code at checkout
- Once connected to the client area, copy your PRO license key
- Paste your license key into BunkerWeb using the [web UI](https://docs.bunkerweb.io/1.6.7/web-ui/#upgrade-to-pro) or a [specific setting](https://docs.bunkerweb.io/1.6.7/features/#pro)

Do not hesitate to visit the [BunkerWeb panel](https://panel.bunkerweb.io/knowledgebase?utm_campaign=self&amp;utm_source=doc) or [contact us](https://panel.bunkerweb.io/contact.php?utm_campaign=self&amp;utm_source=doc) if you have any questions regarding the PRO version.

## Professional services

Get the most out of BunkerWeb by getting professional services directly from the maintainers of the project. From technical support to tailored consulting and development, we are here to assist you in the security of your web services.

You will find more information by visiting the [BunkerWeb Panel](https://panel.bunkerweb.io/?utm_campaign=self&amp;utm_source=doc), our dedicated platform for professional services.

Don&#039;t hesitate to [contact us](https://panel.bunkerweb.io/contact.php?utm_campaign=self&amp;utm_source=doc) if you have any questions; we will be more than happy to respond to your needs.

## Ecosystem, community, and resources

Official websites, tools, and resources about BunkerWeb:

- [**Website**](https://www.bunkerweb.io/?utm_campaign=self&amp;utm_source=doc): get more information, news, and articles about BunkerWeb
- [**Panel**](https://panel.bunkerweb.io/?utm_campaign=self&amp;utm_source=doc): dedicated platform to order and manage professional services (e.g., technical support) around BunkerWeb
- [**Documentation**](https://docs.bunkerweb.io): technical documentation of the BunkerWeb solution
- [**Demo**](https://demo.bunkerweb.io/?utm_campaign=self&amp;utm_source=doc): demonstration website of BunkerWeb, don&#039;t hesitate to attempt attacks to test the robustness of the solution
- [**Web UI**](https://demo-ui.bunkerweb.io/?utm_campaign=self&amp;utm_source=doc): online read-only demo of the web UI of BunkerWeb
- [**Threatmap**](https://www.bunkerweb.io/threatmap/?utm_campaign=self&amp;utm_source=doc): live cyber attack blocked by BunkerWeb instances all around the world

Community and social networks:

- [**Discord**](https://discord.com/invite/fTf46FmtyD)
- [**LinkedIn**](https://www.linkedin.com/company/bunkerity/)
- [**Twitter**](https://twitter.com/bunkerity)
- [**Reddit**](https://www.reddit.com/r/BunkerWeb/)

# Concepts

&lt;p align=&quot;center&quot;&gt;
	&lt;img alt=&quot;Concepts banner&quot; src=&quot;https://github.com/bunkerity/bunkerweb/raw/v1.6.7/docs/assets/img/concepts.svg&quot; /&gt;
&lt;/p&gt;

You will find more information about the key concepts of BunkerWeb in the [documentation](https://docs.bunkerweb.io/1.6.7/concepts/?utm_campaign=self&amp;utm_source=github).

## Integrations

The first concept is the integration of BunkerWeb into the target environment. We prefer to use the word &quot;integration&quot; instead of &quot;installation&quot; because one of the goals of BunkerWeb is to integrate seamlessly into existing environments.

The following integrations are officially supported:

- [Docker](https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;utm_source=github#docker)
- [Linux](https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;utm_source=github#linux)
- [Docker autoconf](https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;utm_source=github#docker-autoconf)
- [Kubernetes](https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;utm_source=github#kubernetes)
- [Swarm](https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;utm_source=github#swarm)
- [Microsoft Azure](https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;utm_source=github#microsoft-azure)

## Settings

Once BunkerWeb is integrated into your environment, you will need to configure it to serve and protect your web applications.

The configuration of BunkerWeb is done by using what we call the &quot;settings&quot; or &quot;variables.&quot; Each setting is identified by a name such as `AUTO_LETS_ENCRYPT` or `USE_ANTIBOT`. You can assign values to the settings to configure BunkerWeb.

Here is a dummy example of a BunkerWeb configuration:

```conf
SERVER_NAME=www.example.com
AUTO_LETS_ENCRYPT=yes
USE_ANTIBOT=captcha
REFERRER_POLICY=no-referrer
USE_MODSECURITY=no
USE_GZIP=yes
USE_BROTLI=no
```

## Multisite mode

The multisite mode is a crucial concept to understand when using BunkerWeb. Because the goal is to protect web applications, we intrinsically inherit the concept of &quot;virtual host&quot; or &quot;vhost&quot; (more info [here](https://en.wikipedia.org/wiki/Virtual_hosting)) which makes it possible to serve multiple web applications from a single (or a cluster of) instance.

By default, the multisite mode of BunkerWeb is disabled, which means that only one web application will be served and all the settings will be applied to it. The typical use case is when you have a single application to protect: you don&#039;t have to worry about the multisite, and the default behavior should be the right one for you.

When multisite mode is enabled, BunkerWeb will serve and protect multiple web applications. Each web application is identified by a unique server name and has its own set of settings. The typical use case is when you have multiple applications to protect and you want to use a single (or a cluster depending on the integration) instance of BunkerWeb.

## Custom configurations

Because meeting all the use cases only using the settings is not an option (even with [external plugins](https://docs.bunkerweb.io/1.6.7/plugins/?utm_campaign=self&amp;utm_source=github)), you can use custom configurations to solve your specific challenges.

Under the hood, BunkerWeb uses the notorious NGINX web server, that&#039;s why you can leverage its configuration system for your specific needs. Custom NGINX configurations can be included in different [contexts](https://docs.nginx.com/nginx/admin-guide/basic-functionality/managing-configuration-files/#contexts) like HTTP or server (all servers and/or specific server block).

Another core component of BunkerWeb is the ModSecurity Web Application Firewall: you can also use custom configurations to fix some false positives or add custom rules, for example.

## Database

&lt;p align=&quot;center&quot;&gt;
	&lt;img alt=&quot;Database model&quot; src=&quot;https://github.com/bunkerity/bunkerweb/raw/v1.6.7/docs/assets/img/bunkerweb_db.svg&quot; /&gt;
&lt;/p&gt;

The state of the current configuration of BunkerWeb is stored in a backend database which contains the following data:

- Settings defined for all the services
- Custom configurations
- BunkerWeb instances
- Metadata about job execution
- Cached files

The following backend databases are supported: SQLite, MariaDB, MySQL, and PostgreSQL.

## Scheduler

To make things automagically work together, a dedicated service called the scheduler is in charge of:

- Storing the settings and custom configurations inside the database
- Executing various tasks (called jobs)
- Generating a configuration which is understood by BunkerWeb
- Being the intermediary for other services (like web UI or autoconf)

In other words, the scheduler is the brain of BunkerWeb.

# Setup

&lt;!--## BunkerWeb Cloud

&lt;p align=&quot;center&quot;&gt;
	&lt;img alt=&quot;Docker banner&quot; src=&quot;https://github.com/bunkerity/bunkerweb/raw/v1.6.7/docs/assets/img/bunkerweb-cloud.webp&quot; /&gt;
&lt;/p&gt;

BunkerWeb Cloud is the easiest way to get started with BunkerWeb. It offers you a fully managed BunkerWeb service with no hassle. Think of it like a BunkerWeb-as-a-Service!

You will find more information about BunkerWeb Cloud beta [here](https://www.bunkerweb.io/cloud?utm_campaign=self&amp;utm_source=docs) and you can apply for free [in the BunkerWeb panel](https://panel.bunkerweb.io/store/bunkerweb-cloud?utm_campaign=self&amp;utm_source=docs).
--&gt;
## Linux

&lt;p align=&quot;center&quot;&gt;
	&lt;img alt=&quot;Linux banner&quot; src=&quot;https://github.com/bunkerity/bunkerweb/raw/v1.6.7/docs/assets/img/integration-linux.svg&quot; /&gt;
&lt;/p&gt;

List of supported Linux distros:

- Debian 12 &quot;Bookworm&quot;
- Debian 13 &quot;Trixie&quot;
- Ubuntu 22.04 &quot;Jammy&quot;
- Ubuntu 24.04 &quot;Noble&quot;
- Fedora 42
- Fedora 43
- RHEL 8.10
- RHEL 9.6
- RHEL 10.0

You will find more information in the [Linux section](https://docs.bunkerweb.io/1.5.10/integrations/?utm_campaign=self&amp;utm_source=github#linux) of the documentation.

## Docker

&lt;p align=&quot;center&quot;&gt;
	&lt;img alt=&quot;Docker banner&quot; src=&quot;https://github.com/bunkerity/bunkerweb/raw/v1.6.7/docs/assets/img/integration-docker.svg&quot; /&gt;
&lt;/p&gt;

We provide ready-to-use prebuilt images for x64, x86, armv7, and arm64 platforms on [Docker Hub](https://hub.docker.com/u/bunkerity).

Docker integration key concepts are:

- **Environment variables** to configure BunkerWeb
- **Scheduler** container to store configuration and execute jobs
- **Networks** to expose ports for clients and connect to upstream web services

You will find more information in the [Docker integration section](https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;utm_source=github#docker) of the documentation.

## Docker autoconf

&lt;p align=&quot;center&quot;&gt;
	&lt;img alt=&quot;Docker autoconf banner&quot; src=&quot;https://github.com/bunkerity/bunkerweb/raw/v1.6.7/docs/assets/img/integration-autoconf.svg&quot; /&gt;
&lt;/p&gt;

The downside of using environment variables is that the container needs to be recreated each time there is an update, which is not very convenient. To counter that issue, you can use another image called **autoconf** which will listen for Docker events and automatically reconfigure BunkerWeb in real-time without recreating the container.

Instead of defining environment variables for the BunkerWeb container, you simply add **labels** to your web applications containers and the **autoconf** will &quot;automagically&quot; take care of the rest.

You will find more information in the [Docker autoconf section](https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;utm_source=github#docker-autoconf) of the documentation.

## Kubernetes

&lt;p align=&quot;center&quot;&gt;
	&lt;img alt=&quot;Kubernetes banner&quot; src=&quot;https://github.com/bunkerity/bunkerweb/raw/v1.6.7/docs/assets/img/integration-kubernetes.svg&quot; /&gt;
&lt;/p&gt;

The autoconf acts as an [Ingress controller](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/) and will configure the BunkerWeb instances according to the [Ingress resources](https://kubernetes.io/docs/concepts/services-networking/ingress/). It also monitors other Kubernetes objects like [ConfigMap](https://kubernetes.io/docs/concepts/configuration/configmap/) for custom configurations.

The official [Helm chart](https://helm.sh/) for BunkerWeb is available in the [bunkerity/bunkerweb-helm repository](https://github.com/bunkerity/bunkerweb-helm).

You will find more information in the [Kubernetes section](https://docs.bunkerweb.io/1.6.7/integrations/?utm_campaign=self&amp;utm_source=github#kubernetes) of the documentation.

## Microsoft Azure

&lt;p align=&quot;center&quot;&gt;
	&lt;img alt=&quot;Azure banner&quot; src=&quot;https://github.com/bunkerity/bunkerweb/raw/v1.6.7/docs/assets/img/integration-azure.webp&quot; /&gt;
&lt;/p&gt;

BunkerWeb is referenced in the [Azure Marketplace](https://azuremarketplace.microsoft.com/fr-fr/marketplace/apps/bunkerity.bunkerweb?tab=Overview) and an ARM template is available in the [misc folder](https://github.com/bunkerity/bunkerweb/raw/v1.6.7/misc/integrations/azure-arm-

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[getzep/graphiti]]></title>
            <link>https://github.com/getzep/graphiti</link>
            <guid>https://github.com/getzep/graphiti</guid>
            <pubDate>Sun, 11 Jan 2026 00:05:26 GMT</pubDate>
            <description><![CDATA[Build Real-Time Knowledge Graphs for AI Agents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/getzep/graphiti">getzep/graphiti</a></h1>
            <p>Build Real-Time Knowledge Graphs for AI Agents</p>
            <p>Language: Python</p>
            <p>Stars: 21,823</p>
            <p>Forks: 2,130</p>
            <p>Stars today: 34 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.getzep.com/&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73&quot; width=&quot;150&quot; alt=&quot;Zep Logo&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt;
Graphiti
&lt;/h1&gt;
&lt;h2 align=&quot;center&quot;&gt; Build Real-Time Knowledge Graphs for AI Agents&lt;/h2&gt;
&lt;div align=&quot;center&quot;&gt;

[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)
[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)
[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)

![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)
[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?&amp;logo=discord&amp;logoColor=white)](https://discord.com/invite/W8Kw6bsgXQ)
[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)
[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&amp;label=Release&amp;color=limegreen)](https://github.com/getzep/graphiti/releases)

&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://trendshift.io/repositories/12986&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12986&quot; alt=&quot;getzep%2Fgraphiti | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;

:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_

&lt;br /&gt;

&gt; [!TIP]
&gt; Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful
&gt; Knowledge Graph-based memory.

Graphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents
operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti
continuously integrates user interactions, structured and unstructured enterprise data, and external information into a
coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical
queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI
applications.

Use Graphiti to:

- Integrate and maintain dynamic user interactions and business data.
- Facilitate state-based reasoning and task automation for agents.
- Query complex, evolving data with semantic, keyword, and graph-based search methods.

&lt;br /&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;images/graphiti-graph-intro.gif&quot; alt=&quot;Graphiti temporal walkthrough&quot; width=&quot;700px&quot;&gt;
&lt;/p&gt;

&lt;br /&gt;

A knowledge graph is a network of interconnected facts, such as _&quot;Kendra loves Adidas shoes.&quot;_ Each fact is a &quot;triplet&quot;
represented by two entities, or
nodes (&quot;Kendra&quot;, &quot;Adidas shoes&quot;), and their relationship, or edge (&quot;loves&quot;). Knowledge Graphs have been explored
extensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph
while handling changing relationships and maintaining historical context.

## Graphiti and Zep&#039;s Context Engineering Platform.

Graphiti powers the core of [Zep&#039;s context engineering platform](https://www.getzep.com) for AI Agents. Zep
offers agent memory, Graph RAG for dynamic data, and context retrieval and assembly.

Using Graphiti, we&#039;ve demonstrated Zep is
the [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).

Read our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).

We&#039;re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2501.13956&quot;&gt;&lt;img src=&quot;images/arxiv-screenshot.png&quot; alt=&quot;Zep: A Temporal Knowledge Graph Architecture for Agent Memory&quot; width=&quot;700px&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

## Zep vs Graphiti

| Aspect | Zep | Graphiti |
|--------|-----|----------|
| **What they are** | Fully managed platform for context engineering and AI memory | Open-source graph framework |
| **User &amp; conversation management** | Built-in users, threads, and message storage | Build your own |
| **Retrieval &amp; performance** | Pre-configured, production-ready retrieval with sub-200ms performance at scale | Custom implementation required; performance depends on your setup |
| **Developer tools** | Dashboard with graph visualization, debug logs, API logs; SDKs for Python, TypeScript, and Go | Build your own tools |
| **Enterprise features** | SLAs, support, security guarantees | Self-managed |
| **Deployment** | Fully managed or in your cloud | Self-hosted only |

### When to choose which

**Choose Zep** if you want a turnkey, enterprise-grade platform with security, performance, and support baked in.

**Choose Graphiti** if you want a flexible OSS core and you&#039;re comfortable building/operating the surrounding system.

## Why Graphiti?

Traditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for
frequently changing data. Graphiti addresses these challenges by providing:

- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.
- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time
  queries.
- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve
  low-latency queries without reliance on LLM summarization.
- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through
  straightforward Pydantic models.
- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;/images/graphiti-intro-slides-stock-2.gif&quot; alt=&quot;Graphiti structured + unstructured demo&quot; width=&quot;700px&quot;&gt;
&lt;/p&gt;

## Graphiti vs. GraphRAG

| Aspect                     | GraphRAG                              | Graphiti                                         |
|----------------------------|---------------------------------------|--------------------------------------------------|
| **Primary Use**            | Static document summarization         | Dynamic data management                          |
| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |
| **Knowledge Structure**    | Entity clusters &amp; community summaries | Episodic data, semantic entities, communities    |
| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |
| **Adaptability**           | Low                                   | High                                             |
| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |
| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |
| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |
| **Custom Entity Types**    | No                                    | Yes, customizable                                |
| **Scalability**            | Moderate                              | High, optimized for large datasets               |

Graphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it
particularly suitable for applications requiring real-time interaction and precise historical queries.

## Installation

Requirements:

- Python 3.10 or higher
- Neo4j 5.26 / FalkorDB 1.1.2 / Kuzu 0.11.2 / Amazon Neptune Database Cluster or Neptune Analytics Graph + Amazon
  OpenSearch Serverless collection (serves as the full text search backend)
- OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)

&gt; [!IMPORTANT]
&gt; Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).
&gt; Using other services may result in incorrect output schemas and ingestion failures. This is particularly
&gt; problematic when using smaller models.

Optional:

- Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)

&gt; [!TIP]
&gt; The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly
&gt; interface to manage Neo4j instances and databases.
&gt; Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:

```bash
docker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest

```

```bash
pip install graphiti-core
```

or

```bash
uv add graphiti-core
```

### Installing with FalkorDB Support

If you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:

```bash
pip install graphiti-core[falkordb]

# or with uv
uv add graphiti-core[falkordb]
```

### Installing with Kuzu Support

If you plan to use Kuzu as your graph database backend, install with the Kuzu extra:

```bash
pip install graphiti-core[kuzu]

# or with uv
uv add graphiti-core[kuzu]
```

### Installing with Amazon Neptune Support

If you plan to use Amazon Neptune as your graph database backend, install with the Amazon Neptune extra:

```bash
pip install graphiti-core[neptune]

# or with uv
uv add graphiti-core[neptune]
```

### You can also install optional LLM providers as extras:

```bash
# Install with Anthropic support
pip install graphiti-core[anthropic]

# Install with Groq support
pip install graphiti-core[groq]

# Install with Google Gemini support
pip install graphiti-core[google-genai]

# Install with multiple providers
pip install graphiti-core[anthropic,groq,google-genai]

# Install with FalkorDB and LLM providers
pip install graphiti-core[falkordb,anthropic,google-genai]

# Install with Amazon Neptune
pip install graphiti-core[neptune]
```

## Default to Low Concurrency; LLM Provider 429 Rate Limit Errors

Graphiti&#039;s ingestion pipelines are designed for high concurrency. By default, concurrency is set low to avoid LLM
Provider 429 Rate Limit Errors. If you find Graphiti slow, please increase concurrency as described below.

Concurrency controlled by the `SEMAPHORE_LIMIT` environment variable. By default, `SEMAPHORE_LIMIT` is set to `10`
concurrent operations to help prevent `429` rate limit errors from your LLM provider. If you encounter such errors, try
lowering this value.

If your LLM provider allows higher throughput, you can increase `SEMAPHORE_LIMIT` to boost episode ingestion
performance.

## Quick Start

&gt; [!IMPORTANT]
&gt; Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your
&gt; environment.
&gt; Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI
&gt; compatible APIs.

For a complete working example, see the [Quickstart Example](./examples/quickstart/README.md) in the examples directory.
The quickstart demonstrates:

1. Connecting to a Neo4j, Amazon Neptune, FalkorDB, or Kuzu database
2. Initializing Graphiti indices and constraints
3. Adding episodes to the graph (both text and structured JSON)
4. Searching for relationships (edges) using hybrid search
5. Reranking search results using graph distance
6. Searching for nodes using predefined search recipes

The example is fully documented with clear explanations of each functionality and includes a comprehensive README with
setup instructions and next steps.

### Running with Docker Compose

You can use Docker Compose to quickly start the required services:

- **Neo4j Docker:**
  ```sh
  docker compose up
  ```
  This will start the Neo4j Docker service and related components.

- **FalkorDB Docker:**
  ```sh
  docker compose --profile falkordb up
  ```
  This will start the FalkorDB Docker service and related components.

## MCP Server

The `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server
allows AI assistants to interact with Graphiti&#039;s knowledge graph capabilities through the MCP protocol.

Key features of the MCP server include:

- Episode management (add, retrieve, delete)
- Entity management and relationship handling
- Semantic and hybrid search capabilities
- Group management for organizing related data
- Graph maintenance operations

The MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant
workflows.

For detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md).

## REST Service

The `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.

Please see the [server README](./server/README.md) for more information.

## Optional Environment Variables

In addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.
If you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables
must be set.

### Database Configuration

Database names are configured directly in the driver constructors:

- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)
- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)

As of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it
to the Graphiti constructor using the `graph_driver` parameter.

#### Neo4j with Custom Database Name

```python
from graphiti_core import Graphiti
from graphiti_core.driver.neo4j_driver import Neo4jDriver

# Create a Neo4j driver with custom database name
driver = Neo4jDriver(
    uri=&quot;bolt://localhost:7687&quot;,
    user=&quot;neo4j&quot;,
    password=&quot;password&quot;,
    database=&quot;my_custom_database&quot;  # Custom database name
)

# Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver)
```

#### FalkorDB with Custom Database Name

```python
from graphiti_core import Graphiti
from graphiti_core.driver.falkordb_driver import FalkorDriver

# Create a FalkorDB driver with custom database name
driver = FalkorDriver(
    host=&quot;localhost&quot;,
    port=6379,
    username=&quot;falkor_user&quot;,  # Optional
    password=&quot;falkor_password&quot;,  # Optional
    database=&quot;my_custom_graph&quot;  # Custom database name
)

# Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver)
```

#### Kuzu

```python
from graphiti_core import Graphiti
from graphiti_core.driver.kuzu_driver import KuzuDriver

# Create a Kuzu driver
driver = KuzuDriver(db=&quot;/tmp/graphiti.kuzu&quot;)

# Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver)
```

#### Amazon Neptune

```python
from graphiti_core import Graphiti
from graphiti_core.driver.neptune_driver import NeptuneDriver

# Create a FalkorDB driver with custom database name
driver = NeptuneDriver(
    host= &lt; NEPTUNE
ENDPOINT &gt;,
aoss_host = &lt; Amazon
OpenSearch
Serverless
Host &gt;,
port = &lt; PORT &gt;  # Optional, defaults to 8182,
         aoss_port = &lt; PORT &gt;  # Optional, defaults to 443
)

driver = NeptuneDriver(host=neptune_uri, aoss_host=aoss_host, port=neptune_port)

# Pass the driver to Graphiti
graphiti = Graphiti(graph_driver=driver)
```

## Using Graphiti with Azure OpenAI

Graphiti supports Azure OpenAI for both LLM inference and embeddings using Azure&#039;s OpenAI v1 API compatibility layer.

### Quick Start

```python
from openai import AsyncOpenAI
from graphiti_core import Graphiti
from graphiti_core.llm_client.azure_openai_client import AzureOpenAILLMClient
from graphiti_core.llm_client.config import LLMConfig
from graphiti_core.embedder.azure_openai import AzureOpenAIEmbedderClient

# Initialize Azure OpenAI client using the standard OpenAI client
# with Azure&#039;s v1 API endpoint
azure_client = AsyncOpenAI(
    base_url=&quot;https://your-resource-name.openai.azure.com/openai/v1/&quot;,
    api_key=&quot;your-api-key&quot;,
)

# Create LLM and Embedder clients
llm_client = AzureOpenAILLMClient(
    azure_client=azure_client,
    config=LLMConfig(model=&quot;gpt-5-mini&quot;, small_model=&quot;gpt-5-mini&quot;)  # Your Azure deployment name
)
embedder_client = AzureOpenAIEmbedderClient(
    azure_client=azure_client,
    model=&quot;text-embedding-3-small&quot;  # Your Azure embedding deployment name
)

# Initialize Graphiti with Azure OpenAI clients
graphiti = Graphiti(
    &quot;bolt://localhost:7687&quot;,
    &quot;neo4j&quot;,
    &quot;password&quot;,
    llm_client=llm_client,
    embedder=embedder_client,
)

# Now you can use Graphiti with Azure OpenAI
```

**Key Points:**
- Use the standard `AsyncOpenAI` client with Azure&#039;s v1 API endpoint format: `https://your-resource-name.openai.azure.com/openai/v1/`
- The deployment names (e.g., `gpt-5-mini`, `text-embedding-3-small`) should match your Azure OpenAI deployment names
- See `examples/azure-openai/` for a complete working example

Make sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names.

## Using Graphiti with Google Gemini

Graphiti supports Google&#039;s Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini,
you&#039;ll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.

Install Graphiti:

```bash
uv add &quot;graphiti-core[google-genai]&quot;

# or

pip install &quot;graphiti-core[google-genai]&quot;
```

```python
from graphiti_core import Graphiti
from graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig
from graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig
from graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient

# Google API key configuration
api_key = &quot;&lt;your-google-api-key&gt;&quot;

# Initialize Graphiti with Gemini clients
graphiti = Graphiti(
    &quot;bolt://localhost:7687&quot;,
    &quot;neo4j&quot;,
    &quot;password&quot;,
    llm_client=GeminiClient(
        config=LLMConfig(
            api_key=api_key,
            model=&quot;gemini-2.0-flash&quot;
        )
    ),
    embedder=GeminiEmbedder(
        config=GeminiEmbedderConfig(
            api_key=api_key,
            embedding_model=&quot;embedding-001&quot;
        )
    ),
    cross_encoder=GeminiRerankerClient(
        config=LLMConfig(
            api_key=api_key,
            model=&quot;gemini-2.5-flash-lite&quot;
        )
    )
)

# Now you can use Graphiti with Google Gemini for all components
```

The Gemini reranker uses the `gemini-2.5-flash-lite` model by default, which is optimized for
cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI
reranker, leveraging Gemini&#039;s log probabilities feature to rank passage relevance.

## Using Graphiti with Ollama (Local LLM)

Graphiti supports Ollama for running local LLMs and embedding models via Ollama&#039;s OpenAI-compatible API. This is ideal
for privacy-focused applications or when you want to avoid API costs.

**Note:** Use `OpenAIGenericClient` (not `OpenAIClient`) for Ollama and other OpenAI-compatible providers like LM Studio. The `OpenAIGenericClient` is optimized for local models with a higher default max token limit (16K vs 8K) and full support for structured outputs.

Install the models:

```bash
ollama pull deepseek-r1:7b # LLM
ollama pull nomic-embed-text # embeddings
```

```python
from graphiti_core import Graphiti
from graphiti_core.llm_client.config import LLMConfig
from graphiti_core.llm_client.openai_generic_client import OpenAIGenericClient
from graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig
from graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient

# Configure Ollama LLM client
llm_config = LLMConfig(
    api_key=&quot;ollama&quot;,  # Ollama doesn&#039;t require a real API key, but some placeh

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>