<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sat, 11 Oct 2025 00:04:09 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[microsoft/RD-Agent]]></title>
            <link>https://github.com/microsoft/RD-Agent</link>
            <guid>https://github.com/microsoft/RD-Agent</guid>
            <pubDate>Sat, 11 Oct 2025 00:04:09 GMT</pubDate>
            <description><![CDATA[Research and development (R&D) is crucial for the enhancement of industrial productivity, especially in the AI era, where the core aspects of R&D are mainly focused on data and models. We are committed to automating these high-value generic R&D processes through R&D-Agent, which lets AI drive data-driven AI. üîóhttps://aka.ms/RD-Agent-Tech-Report]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/RD-Agent">microsoft/RD-Agent</a></h1>
            <p>Research and development (R&D) is crucial for the enhancement of industrial productivity, especially in the AI era, where the core aspects of R&D are mainly focused on data and models. We are committed to automating these high-value generic R&D processes through R&D-Agent, which lets AI drive data-driven AI. üîóhttps://aka.ms/RD-Agent-Tech-Report</p>
            <p>Language: Python</p>
            <p>Stars: 8,323</p>
            <p>Forks: 876</p>
            <p>Stars today: 42 stars today</p>
            <h2>README</h2><pre>&lt;h4 align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/_static/logo.png&quot; alt=&quot;RA-Agent logo&quot; style=&quot;width:70%; &quot;&gt;
  
  &lt;a href=&quot;https://rdagent.azurewebsites.net&quot; target=&quot;_blank&quot;&gt;üñ•Ô∏è Live Demo&lt;/a&gt; |
  &lt;a href=&quot;https://rdagent.azurewebsites.net/factor_loop&quot; target=&quot;_blank&quot;&gt;üé• Demo Video&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=JJ4JYO3HscM&amp;list=PLALmKB0_N3_i52fhUmPQiL4jsO354uopR&quot; target=&quot;_blank&quot;&gt;‚ñ∂Ô∏èYouTube&lt;/a&gt;   |
  &lt;a href=&quot;https://rdagent.readthedocs.io/en/latest/index.html&quot; target=&quot;_blank&quot;&gt;üìñ Documentation&lt;/a&gt; |
  &lt;a href=&quot;https://aka.ms/RD-Agent-Tech-Report&quot; target=&quot;_blank&quot;&gt;üìÑ Tech Report&lt;/a&gt; |
  &lt;a href=&quot;#-paperwork-list&quot;&gt; üìÉ Papers &lt;/a&gt;
&lt;/h3&gt;


[![CI](https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml)
[![CodeQL](https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql)
[![Dependabot Updates](https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates)
[![Lint PR Title](https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml)
[![Release.yml](https://github.com/microsoft/RD-Agent/actions/workflows/release.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/release.yml)
[![Platform](https://img.shields.io/badge/platform-Linux-blue)](https://pypi.org/project/rdagent/#files)
[![PyPI](https://img.shields.io/pypi/v/rdagent)](https://pypi.org/project/rdagent/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/rdagent)](https://pypi.org/project/rdagent/)
[![Release](https://img.shields.io/github/v/release/microsoft/RD-Agent)](https://github.com/microsoft/RD-Agent/releases)
[![GitHub](https://img.shields.io/github/license/microsoft/RD-Agent)](https://github.com/microsoft/RD-Agent/blob/main/LICENSE)
[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit)](https://github.com/pre-commit/pre-commit)
[![Checked with mypy](https://www.mypy-lang.org/static/mypy_badge.svg)](http://mypy-lang.org/)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
[![Chat](https://img.shields.io/badge/chat-discord-blue)](https://discord.gg/ybQ97B6Jjy)
[![Documentation Status](https://readthedocs.org/projects/rdagent/badge/?version=latest)](https://rdagent.readthedocs.io/en/latest/?badge=latest)
[![Readthedocs Preview](https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml) &lt;!-- this badge is too long, please place it in the last one to make it pretty --&gt; 
[![arXiv](https://img.shields.io/badge/arXiv-2505.14738-00ff00.svg)](https://arxiv.org/abs/2505.14738)


# üì∞ News
| üóûÔ∏è News        | üìù Description                 |
| --            | ------      |
| NeurIPS 2025 Acceptance | We are thrilled to announce that our paper [R&amp;D-Agent-Quant](https://arxiv.org/abs/2505.15155) has been accepted to NeurIPS 2025 | 
| [Technical Report Release](#overall-technical-report) | Overall framework description and results on MLE-bench | 
| [R&amp;D-Agent-Quant Release](#deep-application-in-diverse-scenarios) | Apply R&amp;D-Agent to quant trading | 
| MLE-Bench Results Released | R&amp;D-Agent currently leads as the [top-performing machine learning engineering agent](#-the-best-machine-learning-engineering-agent) on MLE-bench |
| Support LiteLLM Backend | We now fully support **[LiteLLM](https://github.com/BerriAI/litellm)** as our default backend for integration with multiple LLM providers. |
| General Data Science Agent | [Data Science Agent](https://rdagent.readthedocs.io/en/latest/scens/data_science.html) |
| Kaggle Scenario release | We release **[Kaggle Agent](https://rdagent.readthedocs.io/en/latest/scens/data_science.html)**, try the new features!                  |
| Official WeChat group release  | We created a WeChat group, welcome to join! (üó™[QR Code](https://github.com/microsoft/RD-Agent/issues/880)) |
| Official Discord release  | We launch our first chatting channel in Discord (üó™[![Chat](https://img.shields.io/badge/chat-discord-blue)](https://discord.gg/ybQ97B6Jjy)) |
| First release | **R&amp;D-Agent** is released on GitHub |



# üèÜ The Best Machine Learning Engineering Agent!

[MLE-bench](https://github.com/openai/mle-bench) is a comprehensive benchmark evaluating the performance of AI agents on machine learning engineering tasks. Utilizing datasets from 75 Kaggle competitions, MLE-bench provides robust assessments of AI systems&#039; capabilities in real-world ML engineering scenarios.

R&amp;D-Agent currently leads as the top-performing machine learning engineering agent on MLE-bench:

| Agent | Low == Lite (%) | Medium (%) | High (%) | All (%) |
|---------|--------|-----------|---------|----------|
| R&amp;D-Agent o3(R)+GPT-4.1(D) | 51.52 ¬± 6.9 | 19.3 ¬± 5.5 | 26.67 ¬± 0 | 30.22 ¬± 1.5 |
| R&amp;D-Agent o1-preview | 48.18 ¬± 2.49 | 8.95 ¬± 2.36 | 18.67 ¬± 2.98 | 22.4 ¬± 1.1 |
| AIDE o1-preview | 34.3 ¬± 2.4 | 8.8 ¬± 1.1 | 10.0 ¬± 1.9 | 16.9 ¬± 1.1 |

**Notes:**
- **O3(R)+GPT-4.1(D)**: This version is designed to both reduce average time per loop and leverage a cost-effective combination of backend LLMs by seamlessly integrating Research Agent (o3) with Development Agent (GPT-4.1).
- **AIDE o1-preview**: Represents the previously best public result on MLE-bench as reported in the original MLE-bench paper.
- Average and standard deviation results for R&amp;D-Agent o1-preview is based on a independent of 5 seeds and for R&amp;D-Agent o3(R)+GPT-4.1(D) is based on 6 seeds.
- According to MLE-Bench, the 75 competitions are categorized into three levels of complexity: **Low==Lite** if we estimate that an experienced ML engineer can produce a sensible solution in under 2 hours, excluding the time taken to train any models; **Medium** if it takes between 2 and 10 hours; and **High** if it takes more than 10 hours.

You can inspect the detailed runs of the above results online.
- [R&amp;D-Agent o1-preview detailed runs](https://aka.ms/RD-Agent_MLE-Bench_O1-preview)
- [R&amp;D-Agent o3(R)+GPT-4.1(D) detailed runs](https://aka.ms/RD-Agent_MLE-Bench_O3_GPT41)

For running R&amp;D-Agent on MLE-bench, refer to **[MLE-bench Guide: Running ML Engineering via MLE-bench](https://rdagent.readthedocs.io/en/latest/scens/data_science.html)**

# ü•á The First Data-Centric Quant Multi-Agent Framework!

R&amp;D-Agent for Quantitative Finance, in short **RD-Agent(Q)**, is the first data-centric, multi-agent framework designed to automate the full-stack research and development of quantitative strategies via coordinated factor-model co-optimization.

![image](https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d)

Extensive experiments in real stock markets show that, at a cost under $10, RD-Agent(Q) achieves approximately 2√ó higher ARR than benchmark factor libraries while using over 70% fewer factors. It also surpasses state-of-the-art deep time-series models under smaller resource budgets. Its alternating factor‚Äìmodel optimization further delivers excellent trade-off between predictive accuracy and strategy robustness.

You can learn more details about **RD-Agent(Q)** through the [paper](https://arxiv.org/abs/2505.15155) and reproduce it through the [documentation](https://rdagent.readthedocs.io/en/latest/scens/quant_agent_fin.html).

# Data Science Agent Preview
Check out our demo video showcasing the current progress of our Data Science Agent under development:

https://github.com/user-attachments/assets/3eccbecb-34a4-4c81-bce4-d3f8862f7305

# üåü Introduction
&lt;div align=&quot;center&quot;&gt;
      &lt;img src=&quot;docs/_static/scen.png&quot; alt=&quot;Our focused scenario&quot; style=&quot;width:80%; &quot;&gt;
&lt;/div&gt;

R&amp;D-Agent aims to automate the most critical and valuable aspects of the industrial R&amp;D process, and we begin with focusing on the data-driven scenarios to streamline the development of models and data. 
Methodologically, we have identified a framework with two key components: &#039;R&#039; for proposing new ideas and &#039;D&#039; for implementing them.
We believe that the automatic evolution of R&amp;D will lead to solutions of significant industrial value.


&lt;!-- Tag Cloud --&gt;
R&amp;D is a very general scenario. The advent of R&amp;D-Agent can be your
- üí∞ **Automatic Quant Factory** ([üé•Demo Video](https://rdagent.azurewebsites.net/factor_loop)|[‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=X4DK2QZKaKY&amp;t=6s))
- ü§ñ **Data Mining Agent:** Iteratively proposing data &amp; models ([üé•Demo Video 1](https://rdagent.azurewebsites.net/model_loop)|[‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=dm0dWL49Bc0&amp;t=104s)) ([üé•Demo Video 2](https://rdagent.azurewebsites.net/dmm)|[‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=VIaSTZuoZg4))  and implementing them by gaining knowledge from data.
- ü¶æ **Research Copilot:** Auto read research papers ([üé•Demo Video](https://rdagent.azurewebsites.net/report_model)|[‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=BiA2SfdKQ7o)) / financial reports ([üé•Demo Video](https://rdagent.azurewebsites.net/report_factor)|[‚ñ∂Ô∏èYouTube](https://www.youtube.com/watch?v=ECLTXVcSx-c)) and implement model structures or building datasets.
- ü§ñ **Kaggle Agent:** Auto Model Tuning and Feature Engineering([üé•Demo Video Coming Soon...]()) and implementing them to achieve more in competitions.
- ...

You can click the links above to view the demo. We&#039;re continuously adding more methods and scenarios to the project to enhance your R&amp;D processes and boost productivity. 

Additionally, you can take a closer look at the examples in our **[üñ•Ô∏è Live Demo](https://rdagent.azurewebsites.net/)**.

&lt;div align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://rdagent.azurewebsites.net/&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;docs/_static/demo.png&quot; alt=&quot;Watch the demo&quot; width=&quot;80%&quot;&gt;
    &lt;/a&gt;
&lt;/div&gt;


# ‚ö° Quick start

### RD-Agent currently only supports Linux.

You can try above demos by running the following command:

### üê≥ Docker installation.
Users must ensure Docker is installed before attempting most scenarios. Please refer to the [official üê≥Docker page](https://docs.docker.com/engine/install/) for installation instructions.
Ensure the current user can run Docker commands **without using sudo**. You can verify this by executing `docker run hello-world`.

### üêç Create a Conda Environment
- Create a new conda environment with Python (3.10 and 3.11 are well-tested in our CI):
  ```sh
  conda create -n rdagent python=3.10
  ```
- Activate the environment:
  ```sh
  conda activate rdagent
  ```

### üõ†Ô∏è Install the R&amp;D-Agent

#### For Users
- You can directly install the R&amp;D-Agent package from PyPI:
  ```sh
  pip install rdagent
  ```

#### For Developers
- If you want to try the latest version or contribute to RD-Agent, you can install it from the source and follow the development setup:
  ```sh
  git clone https://github.com/microsoft/RD-Agent
  cd RD-Agent
  make dev
  ```

More details can be found in the [development setup](https://rdagent.readthedocs.io/en/latest/development.html).

### üíä Health check
- rdagent provides a health check that currently checks two things.
  - whether the docker installation was successful.
  - whether the default port used by the [rdagent ui](https://github.com/microsoft/RD-Agent?tab=readme-ov-file#%EF%B8%8F-monitor-the-application-results) is occupied.
  ```sh
  rdagent health_check --no-check-env
  ```


### ‚öôÔ∏è Configuration
- The demos requires following ability:
  - ChatCompletion
  - json_mode
  - embedding query

  You can set your Chat Model and Embedding Model in the following ways:

  &gt; **üî• Attention**: We now provide experimental support for **DeepSeek** models! You can use DeepSeek&#039;s official API for cost-effective and high-performance inference. See the configuration example below for DeepSeek setup.

- **Using LiteLLM (Default)**: We now support LiteLLM as a backend for integration with multiple LLM providers. You can configure in multiple ways:

  **Option 1: Unified API base for both models**

  *Configuration Example: `OpenAI` Setup :*

  ```bash
  cat &lt;&lt; EOF  &gt; .env
  # Set to any model supported by LiteLLM.
  CHAT_MODEL=gpt-4o 
  EMBEDDING_MODEL=text-embedding-3-small
  # Configure unified API base
  OPENAI_API_BASE=&lt;your_unified_api_base&gt;
  OPENAI_API_KEY=&lt;replace_with_your_openai_api_key&gt;
  ```

  *Configuration Example: `Azure OpenAI` Setup :*

  &gt; Before using this configuration, please confirm in advance that your `Azure OpenAI API key` supports `embedded models`.

  ```bash
  cat &lt;&lt; EOF  &gt; .env
  EMBEDDING_MODEL=azure/&lt;Model deployment supporting embedding&gt;
  CHAT_MODEL=azure/&lt;your deployment name&gt;
  AZURE_API_KEY=&lt;replace_with_your_openai_api_key&gt;
  AZURE_API_BASE=&lt;your_unified_api_base&gt;
  AZURE_API_VERSION=&lt;azure api version&gt;
  ```

  **Option 2: Separate API bases for Chat and Embedding models**
  ```bash
  cat &lt;&lt; EOF  &gt; .env
  # Set to any model supported by LiteLLM.
  # Configure separate API bases for chat and embedding
  
  # CHAT MODEL:
  CHAT_MODEL=gpt-4o 
  OPENAI_API_BASE=&lt;your_chat_api_base&gt;
  OPENAI_API_KEY=&lt;replace_with_your_openai_api_key&gt;

  # EMBEDDING MODEL:
  # TAKE siliconflow as an example, you can use other providers.
  # Note: embedding requires litellm_proxy prefix
  EMBEDDING_MODEL=litellm_proxy/BAAI/bge-large-en-v1.5
  LITELLM_PROXY_API_KEY=&lt;replace_with_your_siliconflow_api_key&gt;
  LITELLM_PROXY_API_BASE=https://api.siliconflow.cn/v1
  ```

  *Configuration Example: `DeepSeek` Setup :*

  &gt;Since many users encounter configuration errors when setting up DeepSeek. Here&#039;s a complete working example for DeepSeek Setup:
  ```bash
  cat &lt;&lt; EOF  &gt; .env
  # CHAT MODEL: Using DeepSeek Official API
  CHAT_MODEL=deepseek/deepseek-chat 
  DEEPSEEK_API_KEY=&lt;replace_with_your_deepseek_api_key&gt;

  # EMBEDDING MODEL: Using SiliconFlow for embedding since deepseek has no embedding model.
  # Note: embedding requires litellm_proxy prefix
  EMBEDDING_MODEL=litellm_proxy/BAAI/bge-m3
  LITELLM_PROXY_API_KEY=&lt;replace_with_your_siliconflow_api_key&gt;
  LITELLM_PROXY_API_BASE=https://api.siliconflow.cn/v1
  ```

  Notice: If you are using reasoning models that include thought processes in their responses (such as \&lt;think&gt; tags), you need to set the following environment variable:
  ```bash
  REASONING_THINK_RM=True
  ```

  You can also use a deprecated backend if you only use `OpenAI API` or `Azure OpenAI` directly. For this deprecated setting and more configuration information, please refer to the [documentation](https://rdagent.readthedocs.io/en/latest/installation_and_configuration.html). 



- If your environment configuration is complete, please execute the following commands to check if your configuration is valid. This step is necessary.

  ```bash
  rdagent health_check
  ```

### üöÄ Run the Application

The **[üñ•Ô∏è Live Demo](https://rdagent.azurewebsites.net/)** is implemented by the following commands(each item represents one demo, you can select the one you prefer):

- Run the **Automated Quantitative Trading &amp; Iterative Factors Model Joint Evolution**:  [Qlib](http://github.com/microsoft/qlib) self-loop factor &amp; model proposal and implementation application
  ```sh
  rdagent fin_quant
  ```

- Run the **Automated Quantitative Trading &amp; Iterative Factors Evolution**:  [Qlib](http://github.com/microsoft/qlib) self-loop factor proposal and implementation application
  ```sh
  rdagent fin_factor
  ```

- Run the **Automated Quantitative Trading &amp; Iterative Model Evolution**: [Qlib](http://github.com/microsoft/qlib) self-loop model proposal and implementation application
  ```sh
  rdagent fin_model
  ```

- Run the **Automated Quantitative Trading &amp; Factors Extraction from Financial Reports**:  Run the [Qlib](http://github.com/microsoft/qlib) factor extraction and implementation application based on financial reports
  ```sh
  # 1. Generally, you can run this scenario using the following command:
  rdagent fin_factor_report --report-folder=&lt;Your financial reports folder path&gt;

  # 2. Specifically, you need to prepare some financial reports first. You can follow this concrete example:
  wget https://github.com/SunsetWolf/rdagent_resource/releases/download/reports/all_reports.zip
  unzip all_reports.zip -d git_ignore_folder/reports
  rdagent fin_factor_report --report-folder=git_ignore_folder/reports
  ```

- Run the **Automated Model Research &amp; Development Copilot**: model extraction and implementation application
  ```sh
  # 1. Generally, you can run your own papers/reports with the following command:
  rdagent general_model &lt;Your paper URL&gt;

  # 2. Specifically, you can do it like this. For more details and additional paper examples, use `rdagent general_model -h`:
  rdagent general_model  &quot;https://arxiv.org/pdf/2210.09789&quot;
  ```

- Run the **Automated Medical Prediction Model Evolution**: Medical self-loop model proposal and implementation application

  ```bash
  # Generally, you can run the data science program with the following command:
  rdagent data_science --competition &lt;your competition name&gt;

  # Specifically, you need to create a folder for storing competition files (e.g., competition description file, competition datasets, etc.), and configure the path to the folder in your environment. In addition, you need to use chromedriver when you download the competition descriptors, which you can follow for this specific example:

  # 1. Download the dataset, extract it to the target folder.
  wget https://github.com/SunsetWolf/rdagent_resource/releases/download/ds_data/arf-12-hours-prediction-task.zip
  unzip arf-12-hours-prediction-task.zip -d ./git_ignore_folder/ds_data/

  # 2. Configure environment variables in the `.env` file
  dotenv set DS_LOCAL_DATA_PATH &quot;$(pwd)/git_ignore_folder/ds_data&quot;
  dotenv set DS_CODER_ON_WHOLE_PIPELINE True
  dotenv set DS_IF_USING_MLE_DATA False
  dotenv set DS_SAMPLE_DATA_BY_LLM False
  dotenv set DS_SCEN rdagent.scenarios.data_science.scen.DataScienceScen

  # 3. run the application
  rdagent data_science --competition arf-12-hours-prediction-task
  ```

  **NOTE:** For more information about the dataset, please refer to the [documentation](https://rdagent.readthedocs.io/en/latest/scens/data_science.html).

- Run the **Automated Kaggle Model Tuning &amp; Feature Engineering**:  self-loop model proposal and feature engineering implementation application &lt;br /&gt;
  &gt; Using **tabular-playground-series-dec-2021** as an example. &lt;br /&gt;
  &gt; 1. Register and login on the [Kaggle](https://www.kaggle.com/) website. &lt;br /&gt;
  &gt; 2. Configuring the Kaggle API. &lt;br /&gt;
  &gt; (1) Click on the avatar (usually in the top right corner of the page) -&gt; `Settings` -&gt; `Create New Token`, A file called `kaggle.json` will be downloaded. &lt;br /&gt;
  &gt; (2) Move `kaggle.json` to `~/.config/kaggle/` &lt;br /&gt;
  &gt; (3) Modify the permissions of the kaggle.json file. Reference command: `chmod 600 ~/.config/kaggle/kaggle.json` &lt;br /&gt;
  &gt; 3. Join the competition: Click `Join the competition` -&gt; `I Understand and Accept` at the bottom of the [competition details page](https://www.kaggle.com/competitions/tabular-playground-series-dec-2021/data).
  ```bash
  # Generally, you can run the Kaggle competition program with the following command:
  rdagent data_science --competition &lt;your competition name&gt;

  # 1. Configure environment variables in the `.env` file
  mkdir -p ./git_ignore_folder/ds_data
  dotenv set DS_LOCAL_DATA_PATH &quot;$(pwd)/git_ignore_folder/ds_data&quot;
  dotenv set DS_CODER_ON_WHOLE_PIPELINE True
  dotenv set DS_IF_USING_MLE_DATA True
  dotenv set DS_SAMPLE_DATA_BY_LLM True
  dotenv set DS_SCEN rdagent.scenarios.data_science.scen.KaggleScen

  # 2. run the application
  rdagent data_science --competition tabular-playground-series-dec-2021
  ```

### üñ•Ô∏è Moni

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[MODSetter/SurfSense]]></title>
            <link>https://github.com/MODSetter/SurfSense</link>
            <guid>https://github.com/MODSetter/SurfSense</guid>
            <pubDate>Sat, 11 Oct 2025 00:04:08 GMT</pubDate>
            <description><![CDATA[Open Source Alternative to NotebookLM / Perplexity, connected to external sources such as Search Engines, Slack, Linear, Jira, ClickUp, Confluence, Notion, YouTube, GitHub, Discord and more. Join our discord: https://discord.gg/ejRNvftDp9]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/MODSetter/SurfSense">MODSetter/SurfSense</a></h1>
            <p>Open Source Alternative to NotebookLM / Perplexity, connected to external sources such as Search Engines, Slack, Linear, Jira, ClickUp, Confluence, Notion, YouTube, GitHub, Discord and more. Join our discord: https://discord.gg/ejRNvftDp9</p>
            <p>Language: Python</p>
            <p>Stars: 9,120</p>
            <p>Forks: 701</p>
            <p>Stars today: 334 stars today</p>
            <h2>README</h2><pre>
![new_header](https://github.com/user-attachments/assets/e236b764-0ddc-42ff-a1f1-8fbb3d2e0e65)


&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://discord.gg/ejRNvftDp9&quot;&gt;
&lt;img src=&quot;https://img.shields.io/discord/1359368468260192417&quot; alt=&quot;Discord&quot;&gt;
&lt;/a&gt;
&lt;/div&gt;


# SurfSense
While tools like NotebookLM and Perplexity are impressive and highly effective for conducting research on any topic/query, SurfSense elevates this capability by integrating with your personal knowledge base. It is a highly customizable AI research agent, connected to external sources such as Search Engines (Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar, Luma and more to come.

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/13606&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/13606&quot; alt=&quot;MODSetter%2FSurfSense | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;


# Video 


https://github.com/user-attachments/assets/d9221908-e0de-4b2f-ac3a-691cf4b202da


## Podcast Sample

https://github.com/user-attachments/assets/a0a16566-6967-4374-ac51-9b3e07fbecd7




## Key Features

### üí° **Idea**: 
Have your own highly customizable private NotebookLM and Perplexity integrated with external sources.
### üìÅ **Multiple File Format Uploading Support**
Save content from your own personal files *(Documents, images, videos and supports **50+ file extensions**)* to your own personal knowledge base .
### üîç **Powerful Search**
Quickly research or find anything in your saved content .
### üí¨ **Chat with your Saved Content**
 Interact in Natural Language and get cited answers.
### üìÑ **Cited Answers**
Get Cited answers just like Perplexity.
### üîî **Privacy &amp; Local LLM Support**
Works Flawlessly with Ollama local LLMs.
### üè† **Self Hostable**
Open source and easy to deploy locally.
### üéôÔ∏è Podcasts 
- Blazingly fast podcast generation agent. (Creates a 3-minute podcast in under 20 seconds.)
- Convert your chat conversations into engaging audio content
- Support for local TTS providers (Kokoro TTS)
- Support for multiple TTS providers (OpenAI, Azure, Google Vertex AI)

### üìä **Advanced RAG Techniques**
- Supports 100+ LLM&#039;s
- Supports 6000+ Embedding Models.
- Supports all major Rerankers (Pinecode, Cohere, Flashrank etc)
- Uses Hierarchical Indices (2 tiered RAG setup).
- Utilizes Hybrid Search (Semantic + Full Text Search combined with Reciprocal Rank Fusion).
- RAG as a Service API Backend.

### ‚ÑπÔ∏è **External Sources**
- Search Engines (Tavily, LinkUp)
- Slack
- Linear
- Jira
- ClickUp
- Confluence
- Notion
- Gmail
- Youtube Videos
- GitHub
- Discord
- Airtable
- Google Calendar
- Luma
- and more to come.....

## üìÑ **Supported File Extensions**

&gt; **Note**: File format support depends on your ETL service configuration. LlamaCloud supports 50+ formats, Unstructured supports 34+ core formats, and Docling (core formats, local processing, privacy-focused, no API key).

### Documents &amp; Text
**LlamaCloud**: `.pdf`, `.doc`, `.docx`, `.docm`, `.dot`, `.dotm`, `.rtf`, `.txt`, `.xml`, `.epub`, `.odt`, `.wpd`, `.pages`, `.key`, `.numbers`, `.602`, `.abw`, `.cgm`, `.cwk`, `.hwp`, `.lwp`, `.mw`, `.mcw`, `.pbd`, `.sda`, `.sdd`, `.sdp`, `.sdw`, `.sgl`, `.sti`, `.sxi`, `.sxw`, `.stw`, `.sxg`, `.uof`, `.uop`, `.uot`, `.vor`, `.wps`, `.zabw`

**Unstructured**: `.doc`, `.docx`, `.odt`, `.rtf`, `.pdf`, `.xml`, `.txt`, `.md`, `.markdown`, `.rst`, `.html`, `.org`, `.epub`

**Docling**: `.pdf`, `.docx`, `.html`, `.htm`, `.xhtml`, `.adoc`, `.asciidoc`

### Presentations
**LlamaCloud**: `.ppt`, `.pptx`, `.pptm`, `.pot`, `.potm`, `.potx`, `.odp`, `.key`

**Unstructured**: `.ppt`, `.pptx`

**Docling**: `.pptx`

### Spreadsheets &amp; Data
**LlamaCloud**: `.xlsx`, `.xls`, `.xlsm`, `.xlsb`, `.xlw`, `.csv`, `.tsv`, `.ods`, `.fods`, `.numbers`, `.dbf`, `.123`, `.dif`, `.sylk`, `.slk`, `.prn`, `.et`, `.uos1`, `.uos2`, `.wk1`, `.wk2`, `.wk3`, `.wk4`, `.wks`, `.wq1`, `.wq2`, `.wb1`, `.wb2`, `.wb3`, `.qpw`, `.xlr`, `.eth`

**Unstructured**: `.xls`, `.xlsx`, `.csv`, `.tsv`

**Docling**: `.xlsx`, `.csv`

### Images
**LlamaCloud**: `.jpg`, `.jpeg`, `.png`, `.gif`, `.bmp`, `.svg`, `.tiff`, `.webp`, `.html`, `.htm`, `.web`

**Unstructured**: `.jpg`, `.jpeg`, `.png`, `.bmp`, `.tiff`, `.heic`

**Docling**: `.jpg`, `.jpeg`, `.png`, `.bmp`, `.tiff`, `.tif`, `.webp`

### Audio &amp; Video *(Always Supported)*
`.mp3`, `.mpga`, `.m4a`, `.wav`, `.mp4`, `.mpeg`, `.webm`

### Email &amp; Communication
**Unstructured**: `.eml`, `.msg`, `.p7s`

### üîñ Cross Browser Extension
- The SurfSense extension can be used to save any webpage you like.
- Its main usecase is to save any webpages protected beyond authentication.



## FEATURE REQUESTS AND FUTURE


**SurfSense is actively being developed.** While it&#039;s not yet production-ready, you can help us speed up the process.

Join the [SurfSense Discord](https://discord.gg/ejRNvftDp9) and help shape the future of SurfSense!

## üöÄ Roadmap

Stay up to date with our development progress and upcoming features!  
Check out our public roadmap and contribute your ideas or feedback:

**View the Roadmap:** [SurfSense Roadmap on GitHub Projects](https://github.com/users/MODSetter/projects/2)

## How to get started?

### Installation Options

SurfSense provides two installation methods:

1. **[Docker Installation](https://www.surfsense.net/docs/docker-installation)** - The easiest way to get SurfSense up and running with all dependencies containerized.
   - Includes pgAdmin for database management through a web UI
   - Supports environment variable customization via `.env` file
   - Flexible deployment options (full stack or core services only)
   - No need to manually edit configuration files between environments
   - See [Docker Setup Guide](DOCKER_SETUP.md) for detailed instructions
   - For deployment scenarios and options, see [Deployment Guide](DEPLOYMENT_GUIDE.md)

2. **[Manual Installation (Recommended)](https://www.surfsense.net/docs/manual-installation)** - For users who prefer more control over their setup or need to customize their deployment.

Both installation guides include detailed OS-specific instructions for Windows, macOS, and Linux.

Before installation, make sure to complete the [prerequisite setup steps](https://www.surfsense.net/docs/) including:
- PGVector setup
- **File Processing ETL Service** (choose one):
  - Unstructured.io API key (supports 34+ formats)
  - LlamaIndex API key (enhanced parsing, supports 50+ formats)
  - Docling (local processing, no API key required, supports PDF, Office docs, images, HTML, CSV)
- Other required API keys

## Screenshots

**Research Agent** 

![updated_researcher](https://github.com/user-attachments/assets/e22c5d86-f511-4c72-8c50-feba0c1561b4)

**Search Spaces** 

![search_spaces](https://github.com/user-attachments/assets/e254c38c-f937-44b6-9e9d-770db583d099)

**Manage Documents** 
![documents](https://github.com/user-attachments/assets/7001e306-eb06-4009-89c6-8fadfdc3fc4d)

**Podcast Agent** 
![podcasts](https://github.com/user-attachments/assets/6cb82ffd-9e14-4172-bc79-67faf34c4c1c)


**Agent Chat** 

![git_chat](https://github.com/user-attachments/assets/bb352d52-1c6d-4020-926b-722d0b98b491)

**Browser Extension**

![ext1](https://github.com/user-attachments/assets/1f042b7a-6349-422b-94fb-d40d0df16c40)

![ext2](https://github.com/user-attachments/assets/a9b9f1aa-2677-404d-b0a0-c1b2dddf24a7)


## Tech Stack


 ### **BackEnd** 

-  **FastAPI**: Modern, fast web framework for building APIs with Python
  
-  **PostgreSQL with pgvector**: Database with vector search capabilities for similarity searches

-  **SQLAlchemy**: SQL toolkit and ORM (Object-Relational Mapping) for database interactions

-  **Alembic**: A database migrations tool for SQLAlchemy.

-  **FastAPI Users**: Authentication and user management with JWT and OAuth support

-  **LangGraph**: Framework for developing AI-agents.
  
-  **LangChain**: Framework for developing AI-powered applications.

-  **LLM Integration**: Integration with LLM models through LiteLLM

-  **Rerankers**: Advanced result ranking for improved search relevance

-  **Hybrid Search**: Combines vector similarity and full-text search for optimal results using Reciprocal Rank Fusion (RRF)

-  **Vector Embeddings**: Document and text embeddings for semantic search

-  **pgvector**: PostgreSQL extension for efficient vector similarity operations

-  **Chonkie**: Advanced document chunking and embedding library
 - Uses `AutoEmbeddings` for flexible embedding model selection
 -  `LateChunker` for optimized document chunking based on embedding model&#039;s max sequence length


  
---
 ### **FrontEnd**

-  **Next.js 15.2.3**: React framework featuring App Router, server components, automatic code-splitting, and optimized rendering.

-  **React 19.0.0**: JavaScript library for building user interfaces.

-  **TypeScript**: Static type-checking for JavaScript, enhancing code quality and developer experience.
- **Vercel AI SDK Kit UI Stream Protocol**: To create scalable chat UI.

-  **Tailwind CSS 4.x**: Utility-first CSS framework for building custom UI designs.

-  **Shadcn**: Headless components library.

-  **Lucide React**: Icon set implemented as React components.

-  **Framer Motion**: Animation library for React.

-  **Sonner**: Toast notification library.

-  **Geist**: Font family from Vercel.

-  **React Hook Form**: Form state management and validation.

-  **Zod**: TypeScript-first schema validation with static type inference.

-  **@hookform/resolvers**: Resolvers for using validation libraries with React Hook Form.

-  **@tanstack/react-table**: Headless UI for building powerful tables &amp; datagrids.


 ### **DevOps**

-  **Docker**: Container platform for consistent deployment across environments
  
-  **Docker Compose**: Tool for defining and running multi-container Docker applications

-  **pgAdmin**: Web-based PostgreSQL administration tool included in Docker setup


### **Extension** 
 Manifest v3 on Plasmo

## Future Work
- Add More Connectors.
- Patch minor bugs.
- Document Podcasts



## Contribute 

Contributions are very welcome! A contribution can be as small as a ‚≠ê or even finding and creating issues.
Fine-tuning the Backend is always desired.

For detailed contribution guidelines, please see our [CONTRIBUTING.md](CONTRIBUTING.md) file.

## Star History

&lt;a href=&quot;https://www.star-history.com/#MODSetter/SurfSense&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=MODSetter/SurfSense&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;

---
---
&lt;p align=&quot;center&quot;&gt;
    &lt;img 
      src=&quot;https://github.com/user-attachments/assets/329c9bc2-6005-4aed-a629-700b5ae296b4&quot; 
      alt=&quot;Catalyst Project&quot; 
      width=&quot;200&quot;
    /&gt;
&lt;/p&gt;

---
---

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[WECENG/ticket-purchase]]></title>
            <link>https://github.com/WECENG/ticket-purchase</link>
            <guid>https://github.com/WECENG/ticket-purchase</guid>
            <pubDate>Sat, 11 Oct 2025 00:04:07 GMT</pubDate>
            <description><![CDATA[Â§ßÈ∫¶Ëá™Âä®Êä¢Á•®ÔºåÊîØÊåÅ‰∫∫Âëò„ÄÅÂüéÂ∏Ç„ÄÅÊó•ÊúüÂú∫Ê¨°„ÄÅ‰ª∑Ê†ºÈÄâÊã©]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/WECENG/ticket-purchase">WECENG/ticket-purchase</a></h1>
            <p>Â§ßÈ∫¶Ëá™Âä®Êä¢Á•®ÔºåÊîØÊåÅ‰∫∫Âëò„ÄÅÂüéÂ∏Ç„ÄÅÊó•ÊúüÂú∫Ê¨°„ÄÅ‰ª∑Ê†ºÈÄâÊã©</p>
            <p>Language: Python</p>
            <p>Stars: 4,424</p>
            <p>Forks: 583</p>
            <p>Stars today: 46 stars today</p>
            <h2>README</h2><pre># Â§ßÈ∫¶Êä¢Á•®ËÑöÊú¨ V1.0
### ÁâπÂæÅ

- Ëá™Âä®Êó†Âª∂Êó∂Êä¢Á•®
- ÊîØÊåÅ‰∫∫Âëò„ÄÅÂüéÂ∏Ç„ÄÅÊó•ÊúüÂú∫Ê¨°„ÄÅ‰ª∑Ê†ºÈÄâÊã©

## ÂäüËÉΩ‰ªãÁªç
ÈÄöËøáseleniumÊâìÂºÄÈ°µÈù¢ËøõË°åÁôªÂΩïÔºåÊ®°ÊãüÁî®Êà∑Ë¥≠Á•®ÊµÅÁ®ãËá™Âä®Ë¥≠Á•®

ÂÖ∂ÊµÅÁ®ãÂõæÂ¶Ç‰∏ã:

&lt;img src=&quot;img/Â§ßÈ∫¶Êä¢Á•®ÊµÅÁ®ã.png&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;

## ÂáÜÂ§áÂ∑•‰Ωú
### 1. ÈÖçÁΩÆÁéØÂ¢É

#### 1.1ÂÆâË£Öpython3ÁéØÂ¢É

**Windows**

1. ËÆøÈóÆPythonÂÆòÊñπÁΩëÁ´ôÔºöhttps://www.python.org/downloads/windows/
2. ‰∏ãËΩΩÊúÄÊñ∞ÁöÑPython 3.9+ÁâàÊú¨ÁöÑÂÆâË£ÖÁ®ãÂ∫è„ÄÇ
3. ËøêË°åÂÆâË£ÖÁ®ãÂ∫è„ÄÇ
4. Âú®ÂÆâË£ÖÁ®ãÂ∫è‰∏≠ÔºåÁ°Æ‰øùÂãæÈÄâ &quot;Add Python X.X to PATH&quot; ÈÄâÈ°πÔºåËøôÂ∞ÜËá™Âä®Â∞ÜPythonÊ∑ªÂä†Âà∞Á≥ªÁªüÁéØÂ¢ÉÂèòÈáè‰∏≠ÔºåÊñπ‰æøÂú®ÂëΩ‰ª§Ë°å‰∏≠‰ΩøÁî®Python„ÄÇ
5. ÂÆåÊàêÂÆâË£ÖÂêéÔºå‰Ω†ÂèØ‰ª•Âú®ÂëΩ‰ª§ÊèêÁ§∫Á¨¶ÊàñPowerShell‰∏≠ËæìÂÖ• `python3` Êù•ÂêØÂä®PythonËß£ÈáäÂô®„ÄÇ

**macOS**

1. ‰Ω†ÂèØ‰ª•‰ΩøÁî®HomebrewÊù•ÂÆâË£ÖPython 3„ÄÇ

   - ÂÆâË£ÖHomebrewÔºàÂ¶ÇÊûúÊú™ÂÆâË£ÖÔºâÔºöÊâìÂºÄÁªàÁ´ØÂπ∂ËøêË°å‰ª•‰∏ãÂëΩ‰ª§Ôºö

     ```shell
     /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot;
     ```

   - ÂÆâË£ÖPython 3ÔºöËøêË°å‰ª•‰∏ãÂëΩ‰ª§Êù•ÂÆâË£ÖPython 3Ôºö

     ```shell
     brew install python@3
     ```

#### 1.2 ÂÆâË£ÖÊâÄÈúÄË¶ÅÁöÑÁéØÂ¢É

Âú®ÂëΩ‰ª§Á™óÂè£ËæìÂÖ•Â¶Ç‰∏ãÊåá‰ª§

```shell
pip3 install selenium
```

#### 1.3 ‰∏ãËΩΩgoogle chromeÊµèËßàÂô®

‰∏ãËΩΩÂú∞ÂùÄ: https://www.google.cn/intl/zh-CN/chrome/?brand=YTUH&amp;gclid=Cj0KCQjwj5mpBhDJARIsAOVjBdoV_1sBwdqKGHV3rUU1vJmNKZdy5QNzbRT8F5O0-_jq1WHXurE8a7MaAkWrEALw_wcB&amp;gclsrc=aw.ds

### 2. ‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂

Âú®ËøêË°åÁ®ãÂ∫è‰πãÂâçÔºåÈúÄË¶ÅÂÖà‰øÆÊîπ`config.json`Êñá‰ª∂„ÄÇËØ•Êñá‰ª∂Áî®‰∫éÊåáÂÆöÁî®Êà∑ÈúÄË¶ÅÊä¢Á•®ÁöÑÁõ∏ÂÖ≥‰ø°ÊÅØÔºåÂåÖÊã¨ÊºîÂî±‰ºöÁöÑÂú∫Ê¨°„ÄÅËßÇÊºîÁöÑ‰∫∫Âëò„ÄÅÂüéÂ∏Ç„ÄÅÊó•Êúü„ÄÅ‰ª∑Ê†ºÁ≠â„ÄÇÊñá‰ª∂ÁªìÊûúÂ¶Ç‰∏ãÂõæÊâÄÁ§∫Ôºö

&lt;img src=&quot;img/config_json.png&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;

#### 2.1 Êñá‰ª∂ÂÜÖÂÆπËØ¥Êòé

- `index_url`‰∏∫Â§ßÈ∫¶ÁΩëÁöÑÂú∞ÂùÄÔºå**Êó†ÈúÄ‰øÆÊîπ**
- `login_url`‰∏∫Â§ßÈ∫¶ÁΩëÁöÑÁôªÂΩïÂú∞ÂùÄÔºå**Êó†ÈúÄ‰øÆÊîπ**
- `target_url`‰∏∫Áî®Êà∑ÈúÄË¶ÅÊä¢ÁöÑÊºîÂî±‰ºöÁ•®ÁöÑÁõÆÊ†áÂú∞ÂùÄÔºå**ÂæÖ‰øÆÊîπ**
- `users`‰∏∫ËßÇÊºî‰∫∫ÁöÑÂßìÂêçÔºå**ËßÇÊºî‰∫∫ÈúÄË¶ÅÁî®Êà∑Âú®ÊâãÊú∫Â§ßÈ∫¶APP‰∏≠ÂÖàÂ°´ÂÜôÂ•ΩÔºåÁÑ∂ÂêéÂÜçÂ°´ÂÖ•ËØ•ÈÖçÁΩÆÊñá‰ª∂‰∏≠**Ôºå**ÂæÖ‰øÆÊîπ**
- `city`‰∏∫ÂüéÂ∏ÇÔºå**Â¶ÇÊûúÁî®Êà∑ÈúÄË¶ÅÊä¢ÁöÑÊºîÂî±‰ºöÁ•®ÈúÄË¶ÅÈÄâÊã©ÂüéÂ∏ÇÔºåËØ∑ÊääÂüéÂ∏ÇÂ°´ÂÖ•Ê≠§Â§Ñ„ÄÇÂ¶ÇÊó†ÈúÄÈÄâÊã©ÔºåÂàô‰∏çÂ°´**
- `date`‰∏∫Âú∫Ê¨°Êó•ÊúüÔºå**ÂæÖ‰øÆÊîπÔºåÂèØÂ§öÈÄâ**
- `price`‰∏∫Á•®Ê°£ÁöÑ‰ª∑Ê†ºÔºå**ÂæÖ‰øÆÊîπÔºåÂèØÂ§öÈÄâ**
- `if_commit_order`‰∏∫ÊòØÂê¶Ë¶ÅËá™Âä®Êèê‰∫§ËÆ¢ÂçïÔºå**ÊîπÊàê true**
- if_listen‰∏∫ÊòØÂê¶ÂõûÊµÅÁõëÂê¨Ôºå**ÊîπÊàêtrue**



#### 2.2 Á§∫‰æãËØ¥Êòé

ËøõÂÖ•Â§ßÈ∫¶ÁΩëhttps://www.damai.cn/ÔºåÈÄâÊã©‰Ω†ÈúÄË¶ÅÊä¢Á•®ÁöÑÊºîÂî±‰ºö„ÄÇÂÅáËÆæÂ¶Ç‰∏ãÂõæÊâÄÁ§∫Ôºö

&lt;img src=&quot;img/example.png&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;

Êé•‰∏ãÊù•ÊåâÁÖß‰∏ãÂõæÁöÑÊ†áÊ≥®ÂØπÈÖçÁΩÆÊñá‰ª∂ËøõË°å‰øÆÊîπÔºö

&lt;img src=&quot;img/example_detail.png&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;

ÊúÄÁªà`config.json`ÁöÑÊñá‰ª∂ÂÜÖÂÆπÂ¶Ç‰∏ãÔºö

```json
{
  &quot;index_url&quot;: &quot;https://www.damai.cn/&quot;,
  &quot;login_url&quot;: &quot;https://passport.damai.cn/login?ru=https%3A%2F%2Fwww.damai.cn%2F&quot;,
  &quot;target_url&quot;: &quot;https://detail.damai.cn/item.htm?spm=a2oeg.home.card_0.ditem_1.591b23e1JQGWHg&amp;id=740680932762&quot;,
  &quot;users&quot;: [
    &quot;ÂêçÂ≠ó1&quot;,
    &quot;ÂêçÂ≠ó2&quot;
  ],
  &quot;city&quot;: &quot;ÂπøÂ∑û&quot;,
  &quot;date&quot;: &quot;2023-10-28&quot;,
  &quot;price&quot;: &quot;1039&quot;,
  &quot;if_listen&quot;:true,
  &quot;if_commit_order&quot;: true
}
```



### 3.ËøêË°åÁ®ãÂ∫è

ËøêË°åÁ®ãÂ∫èÂºÄÂßãÊä¢Á•®ÔºåËøõÂÖ•ÂëΩ‰ª§Á™óÂè£ÔºåÊâßË°åÂ¶Ç‰∏ãÂëΩ‰ª§Ôºö

```shell
cd damai
python3 damai.py
```



# Â§ßÈ∫¶appÊä¢Á•®

Â§ßÈ∫¶appÊä¢Á•®ËÑöÊú¨ÈúÄË¶Å‰æùËµñappiumÔºåÂõ†Ê≠§ÈúÄË¶ÅÁé∞Âú®ÂÆâË£Öappium server&amp;clientÁéØÂ¢ÉÔºåÊ≠•È™§Â¶Ç‰∏ãÔºö

## appium server

### ‰∏ãËΩΩ

- ÂÖàÂÆâË£ÖÂ•ΩnodeÁéØÂ¢ÉÔºàÂÖ∑Â§ánpmÔºânodeÁâàÊú¨Âè∑18.0.0

- ÂÖà‰∏ãËΩΩÂπ∂ÂÆâË£ÖÂ•Ωandroid sdkÔºåÂπ∂ÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáèÔºàappium serverËøêË°åÈúÄ‰æùËµñandroid sdk)

- ‰∏ãËΩΩappium

  ```shell
  npm install -g appium
  ```

- Êü•ÁúãappiumÊòØÂê¶ÂÆâË£ÖÊàêÂäü

  ```shell
  appium -v
  ```

- ‰∏ãËΩΩUiAutomator2È©±Âä®

  ```shell
  npm install appium-uiautomator2-driver
  ```

‚Äã		ÂèØËÉΩ‰ºöÈÅáÂà∞Â¶Ç‰∏ãÈîôËØØÔºö

```tex
‚ûú  xcode git:(master) ‚úó npm install appium-uiautomator2-driver

npm ERR! code 1
npm ERR! path /Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/appium-chromedriver
npm ERR! command failed
npm ERR! command sh -c node install-npm.js
npm ERR! [11:57:54] Error installing Chromedriver: Request failed with status code 404
npm ERR! [11:57:54] AxiosError: Request failed with status code 404
npm ERR!     at settle (/Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/axios/lib/core/settle.js:19:12)
npm ERR!     at IncomingMessage.handleStreamEnd (/Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver/node_modules/axios/lib/adapters/http.js:572:11)
npm ERR!     at IncomingMessage.emit (node:events:539:35)
npm ERR!     at endReadableNT (node:internal/streams/readable:1344:12)
npm ERR!     at processTicksAndRejections (node:internal/process/task_queues:82:21)
npm ERR! [11:57:54] Downloading Chromedriver can be skipped by setting the&#039;APPIUM_SKIP_CHROMEDRIVER_INSTALL&#039; environment variable.

npm ERR! A complete log of this run can be found in:
npm ERR!     /Users/chenweicheng/.npm/_logs/2023-10-26T03_57_35_950Z-debug-0.log
```

‚Äã		Ëß£ÂÜ≥ÂäûÊ≥ïÔºàÊ∑ªÂä†ÁéØÂ¢ÉÂèòÈáèÔºåÈîôËØØÂéüÂõ†ÊòØÊ≤°ÊúâÊâæÂà∞chromeÊµèËßàÂô®È©±Âä®ÔºåÂøΩÁï•Âç≥ÂèØÔºâ

```shell
export APPIUM_SKIP_CHROMEDRIVER_INSTALL=true
```

### ÂêØÂä®

ÂêØÂä®appium serverÂπ∂‰ΩøÁî®uiautomator2È©±Âä®

```shell
appium --use-plugins uiautomator2
```

ÂêØÂä®ÊàêÂäüÂ∞ÜÂá∫Áé∞Â¶Ç‰∏ã‰ø°ÊÅØÔºö

```
[Appium] Welcome to Appium v2.2.1 (REV 2176894a5be5da17a362bf3f20678641a78f4b69)
[Appium] Non-default server args:
[Appium] {
[Appium]   usePlugins: [
[Appium]     &#039;uiautomator2&#039;
[Appium]   ]
[Appium] }
[Appium] Attempting to load driver uiautomator2...
[Appium] Requiring driver at /Users/chenweicheng/Documents/xcode/node_modules/appium-uiautomator2-driver
[Appium] Appium REST http interface listener started on http://0.0.0.0:4723
[Appium] You can provide the following URLs in your client code to connect to this server:
[Appium] 	http://127.0.0.1:4723/ (only accessible from the same host)
[Appium] 	http://172.31.102.45:4723/
[Appium] 	http://198.18.0.1:4723/
[Appium] Available drivers:
[Appium]   - uiautomator2@2.32.3 (automationName &#039;UiAutomator2&#039;)
[Appium] No plugins have been installed. Use the &quot;appium plugin&quot; command to install the one(s) you want to use.
```

ÂÖ∂‰∏≠`[Appium] 	http://127.0.0.1:4723/ (only accessible from the same host)
[Appium] 	http://172.31.102.45:4723/
[Appium] 	http://198.18.0.1:4723/`‰∏∫appium serverËøûÊé•Âú∞ÂùÄ



## appium client

- ÂÖà‰∏ãËΩΩÂπ∂ÂÆâË£ÖÂ•Ωpython3Âíåpip3

- ÂÆâË£Ö

  ```shell
  pip3 install appium-python-client
  ```

- Âú®‰ª£Á†Å‰∏≠ÂºïÂÖ•Âπ∂‰ΩøÁî®appium

  ```python
  from appium import webdriver
  from appium.options.common.base import AppiumOptions
  
  device_app_info = AppiumOptions()
  device_app_info.set_capability(&#039;platformName&#039;, &#039;Android&#039;)
  device_app_info.set_capability(&#039;platformVersion&#039;, &#039;10&#039;)
  device_app_info.set_capability(&#039;deviceName&#039;, &#039;YourDeviceName&#039;)
  device_app_info.set_capability(&#039;appPackage&#039;, &#039;cn.damai&#039;)
  device_app_info.set_capability(&#039;appActivity&#039;, &#039;.launcher.splash.SplashMainActivity&#039;)
  device_app_info.set_capability(&#039;unicodeKeyboard&#039;, True)
  device_app_info.set_capability(&#039;resetKeyboard&#039;, True)
  device_app_info.set_capability(&#039;noReset&#039;, True)
  device_app_info.set_capability(&#039;newCommandTimeout&#039;, 6000)
  device_app_info.set_capability(&#039;automationName&#039;, &#039;UiAutomator2&#039;)
  
  # ËøûÊé•appium serverÔºåserverÂú∞ÂùÄÊü•ÁúãappiumÂêØÂä®‰ø°ÊÅØ
  driver = webdriver.Remote(&#039;http://127.0.0.1:4723&#039;, options=device_app_info)
  
  ```

- ÂêØÂä®ËÑöÊú¨Á®ãÂ∫è

  ```shell
  cd damai_appium
  python3 damai_appium.py
  ```

  

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[bytedance/deer-flow]]></title>
            <link>https://github.com/bytedance/deer-flow</link>
            <guid>https://github.com/bytedance/deer-flow</guid>
            <pubDate>Sat, 11 Oct 2025 00:04:06 GMT</pubDate>
            <description><![CDATA[DeerFlow is a community-driven Deep Research framework, combining language models with tools like web search, crawling, and Python execution, while contributing back to the open-source community.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/bytedance/deer-flow">bytedance/deer-flow</a></h1>
            <p>DeerFlow is a community-driven Deep Research framework, combining language models with tools like web search, crawling, and Python execution, while contributing back to the open-source community.</p>
            <p>Language: Python</p>
            <p>Stars: 17,414</p>
            <p>Forks: 2,264</p>
            <p>Stars today: 27 stars today</p>
            <h2>README</h2><pre># ü¶å DeerFlow

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![DeepWiki](https://img.shields.io/badge/DeepWiki-bytedance%2Fdeer--flow-blue.svg?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAyCAYAAAAnWDnqAAAAAXNSR0IArs4c6QAAA05JREFUaEPtmUtyEzEQhtWTQyQLHNak2AB7ZnyXZMEjXMGeK/AIi+QuHrMnbChYY7MIh8g01fJoopFb0uhhEqqcbWTp06/uv1saEDv4O3n3dV60RfP947Mm9/SQc0ICFQgzfc4CYZoTPAswgSJCCUJUnAAoRHOAUOcATwbmVLWdGoH//PB8mnKqScAhsD0kYP3j/Yt5LPQe2KvcXmGvRHcDnpxfL2zOYJ1mFwrryWTz0advv1Ut4CJgf5uhDuDj5eUcAUoahrdY/56ebRWeraTjMt/00Sh3UDtjgHtQNHwcRGOC98BJEAEymycmYcWwOprTgcB6VZ5JK5TAJ+fXGLBm3FDAmn6oPPjR4rKCAoJCal2eAiQp2x0vxTPB3ALO2CRkwmDy5WohzBDwSEFKRwPbknEggCPB/imwrycgxX2NzoMCHhPkDwqYMr9tRcP5qNrMZHkVnOjRMWwLCcr8ohBVb1OMjxLwGCvjTikrsBOiA6fNyCrm8V1rP93iVPpwaE+gO0SsWmPiXB+jikdf6SizrT5qKasx5j8ABbHpFTx+vFXp9EnYQmLx02h1QTTrl6eDqxLnGjporxl3NL3agEvXdT0WmEost648sQOYAeJS9Q7bfUVoMGnjo4AZdUMQku50McCcMWcBPvr0SzbTAFDfvJqwLzgxwATnCgnp4wDl6Aa+Ax283gghmj+vj7feE2KBBRMW3FzOpLOADl0Isb5587h/U4gGvkt5v60Z1VLG8BhYjbzRwyQZemwAd6cCR5/XFWLYZRIMpX39AR0tjaGGiGzLVyhse5C9RKC6ai42ppWPKiBagOvaYk8lO7DajerabOZP46Lby5wKjw1HCRx7p9sVMOWGzb/vA1hwiWc6jm3MvQDTogQkiqIhJV0nBQBTU+3okKCFDy9WwferkHjtxib7t3xIUQtHxnIwtx4mpg26/HfwVNVDb4oI9RHmx5WGelRVlrtiw43zboCLaxv46AZeB3IlTkwouebTr1y2NjSpHz68WNFjHvupy3q8TFn3Hos2IAk4Ju5dCo8B3wP7VPr/FGaKiG+T+v+TQqIrOqMTL1VdWV1DdmcbO8KXBz6esmYWYKPwDL5b5FA1a0hwapHiom0r/cKaoqr+27/XcrS5UwSMbQAAAABJRU5ErkJggg==)](https://deepwiki.com/bytedance/deer-flow)

&lt;!-- DeepWiki badge generated by https://deepwiki.ryoppippi.com/ --&gt;

[English](./README.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](./README_zh.md) | [Êó•Êú¨Ë™û](./README_ja.md) | [Deutsch](./README_de.md) | [Espa√±ol](./README_es.md) | [–†—É—Å—Å–∫–∏–π](./README_ru.md) | [Portuguese](./README_pt.md)

&gt; Originated from Open Source, give back to Open Source.

**DeerFlow** (**D**eep **E**xploration and **E**fficient **R**esearch **Flow**) is a community-driven Deep Research framework that builds upon the incredible work of the open source community. Our goal is to combine language models with specialized tools for tasks like web search, crawling, and Python code execution, while giving back to the community that made this possible.

Currently, DeerFlow has officially entered the [FaaS Application Center of Volcengine](https://console.volcengine.com/vefaas/region:vefaas+cn-beijing/market). Users can experience it online through the [experience link](https://console.volcengine.com/vefaas/region:vefaas+cn-beijing/market/deerflow/?channel=github&amp;source=deerflow) to intuitively feel its powerful functions and convenient operations. At the same time, to meet the deployment needs of different users, DeerFlow supports one-click deployment based on Volcengine. Click the [deployment link](https://console.volcengine.com/vefaas/region:vefaas+cn-beijing/application/create?templateId=683adf9e372daa0008aaed5c&amp;channel=github&amp;source=deerflow) to quickly complete the deployment process and start an efficient research journey.


Please visit [our official website](https://deerflow.tech/) for more details.

## Demo

### Video

&lt;https://github.com/user-attachments/assets/f3786598-1f2a-4d07-919e-8b99dfa1de3e&gt;

In this demo, we showcase how to use DeerFlow to:

- Seamlessly integrate with MCP services
- Conduct the Deep Research process and produce a comprehensive report with images
- Create podcast audio based on the generated report

### Replays

- [How tall is Eiffel Tower compared to tallest building?](https://deerflow.tech/chat?replay=eiffel-tower-vs-tallest-building)
- [What are the top trending repositories on GitHub?](https://deerflow.tech/chat?replay=github-top-trending-repo)
- [Write an article about Nanjing&#039;s traditional dishes](https://deerflow.tech/chat?replay=nanjing-traditional-dishes)
- [How to decorate a rental apartment?](https://deerflow.tech/chat?replay=rental-apartment-decoration)
- [Visit our official website to explore more replays.](https://deerflow.tech/#case-studies)

---

## üìë Table of Contents

- [üöÄ Quick Start](#quick-start)
- [üåü Features](#features)
- [üèóÔ∏è Architecture](#architecture)
- [üõ†Ô∏è Development](#development)
- [üê≥ Docker](#docker)
- [üó£Ô∏è Text-to-Speech Integration](#text-to-speech-integration)
- [üìö Examples](#examples)
- [‚ùì FAQ](#faq)
- [üìú License](#license)
- [üíñ Acknowledgments](#acknowledgments)
- [‚≠ê Star History](#star-history)

## Quick Start

DeerFlow is developed in Python, and comes with a web UI written in Node.js. To ensure a smooth setup process, we recommend using the following tools:

### Recommended Tools

- **[`uv`](https://docs.astral.sh/uv/getting-started/installation/):**
  Simplify Python environment and dependency management. `uv` automatically creates a virtual environment in the root directory and installs all required packages for you‚Äîno need to manually install Python environments.

- **[`nvm`](https://github.com/nvm-sh/nvm):**
  Manage multiple versions of the Node.js runtime effortlessly.

- **[`pnpm`](https://pnpm.io/installation):**
  Install and manage dependencies of Node.js project.

### Environment Requirements

Make sure your system meets the following minimum requirements:

- **[Python](https://www.python.org/downloads/):** Version `3.12+`
- **[Node.js](https://nodejs.org/en/download/):** Version `22+`

### Installation

```bash
# Clone the repository
git clone https://github.com/bytedance/deer-flow.git
cd deer-flow

# Install dependencies, uv will take care of the python interpreter and venv creation, and install the required packages
uv sync

# Configure .env with your API keys
# Tavily: https://app.tavily.com/home
# Brave_SEARCH: https://brave.com/search/api/
# volcengine TTS: Add your TTS credentials if you have them
cp .env.example .env

# See the &#039;Supported Search Engines&#039; and &#039;Text-to-Speech Integration&#039; sections below for all available options

# Configure conf.yaml for your LLM model and API keys
# Please refer to &#039;docs/configuration_guide.md&#039; for more details
# For local development, you can use Ollama or other local models
cp conf.yaml.example conf.yaml

# Install marp for ppt generation
# https://github.com/marp-team/marp-cli?tab=readme-ov-file#use-package-manager
brew install marp-cli
```

Optionally, install web UI dependencies via [pnpm](https://pnpm.io/installation):

```bash
cd deer-flow/web
pnpm install
```

### Configurations

Please refer to the [Configuration Guide](docs/configuration_guide.md) for more details.

&gt; [!NOTE]
&gt; Before you start the project, read the guide carefully, and update the configurations to match your specific settings and requirements.

### Console UI

The quickest way to run the project is to use the console UI.

```bash
# Run the project in a bash-like shell
uv run main.py
```

### Web UI

This project also includes a Web UI, offering a more dynamic and engaging interactive experience.

&gt; [!NOTE]
&gt; You need to install the dependencies of web UI first.

```bash
# Run both the backend and frontend servers in development mode
# On macOS/Linux
./bootstrap.sh -d

# On Windows
bootstrap.bat -d
```
&gt; [!Note]
&gt; By default, the backend server binds to 127.0.0.1 (localhost) for security reasons. If you need to allow external connections (e.g., when deploying on Linux server), you can modify the server host to 0.0.0.0 in the bootstrap script(uv run server.py --host 0.0.0.0).
&gt; Please ensure your environment is properly secured before exposing the service to external networks.

Open your browser and visit [`http://localhost:3000`](http://localhost:3000) to explore the web UI.

Explore more details in the [`web`](./web/) directory.

## Supported Search Engines

### Web Search

DeerFlow supports multiple search engines that can be configured in your `.env` file using the `SEARCH_API` variable:

- **Tavily** (default): A specialized search API for AI applications
  - Requires `TAVILY_API_KEY` in your `.env` file
  - Sign up at: https://app.tavily.com/home

- **DuckDuckGo**: Privacy-focused search engine
  - No API key required

- **Brave Search**: Privacy-focused search engine with advanced features
  - Requires `BRAVE_SEARCH_API_KEY` in your `.env` file
  - Sign up at: https://brave.com/search/api/

- **Arxiv**: Scientific paper search for academic research
  - No API key required
  - Specialized for scientific and academic papers

- **Searx/SearxNG**: Self-hosted metasearch engine
  - Requires `SEARX_HOST` to be set in the `.env` file
  - Supports connecting to either Searx or SearxNG

To configure your preferred search engine, set the `SEARCH_API` variable in your `.env` file:

```bash
# Choose one: tavily, duckduckgo, brave_search, arxiv
SEARCH_API=tavily
```

### Private Knowledgebase

DeerFlow support private knowledgebase such as ragflow and vikingdb, so that you can use your private documents to answer questions.

- **[RAGFlow](https://ragflow.io/docs/dev/)**Ôºöopen source RAG engine
   ```
   # examples in .env.example
   RAG_PROVIDER=ragflow
   RAGFLOW_API_URL=&quot;http://localhost:9388&quot;
   RAGFLOW_API_KEY=&quot;ragflow-xxx&quot;
   RAGFLOW_RETRIEVAL_SIZE=10
   RAGFLOW_CROSS_LANGUAGES=English,Chinese,Spanish,French,German,Japanese,Korean
   ```

## Features

### Core Capabilities

- ü§ñ **LLM Integration**
  - It supports the integration of most models through [litellm](https://docs.litellm.ai/docs/providers).
  - Support for open source models like Qwen, you need to read the [configuration](docs/configuration_guide.md) for more details.
  - OpenAI-compatible API interface
  - Multi-tier LLM system for different task complexities

### Tools and MCP Integrations

- üîç **Search and Retrieval**
  - Web search via Tavily, Brave Search and more
  - Crawling with Jina
  - Advanced content extraction
  - Support for private knowledgebase

- üìÉ **RAG Integration**

  - Supports mentioning files from [RAGFlow](https://github.com/infiniflow/ragflow) within the input box. [Start up RAGFlow server](https://ragflow.io/docs/dev/).

- üîó **MCP Seamless Integration**
  - Expand capabilities for private domain access, knowledge graph, web browsing and more
  - Facilitates integration of diverse research tools and methodologies

### Human Collaboration

- üß† **Human-in-the-loop**
  - Supports interactive modification of research plans using natural language
  - Supports auto-acceptance of research plans

- üìù **Report Post-Editing**
  - Supports Notion-like block editing
  - Allows AI refinements, including AI-assisted polishing, sentence shortening, and expansion
  - Powered by [tiptap](https://tiptap.dev/)

### Content Creation

- üéôÔ∏è **Podcast and Presentation Generation**
  - AI-powered podcast script generation and audio synthesis
  - Automated creation of simple PowerPoint presentations
  - Customizable templates for tailored content

## Architecture

DeerFlow implements a modular multi-agent system architecture designed for automated research and code analysis. The system is built on LangGraph, enabling a flexible state-based workflow where components communicate through a well-defined message passing system.

![Architecture Diagram](./assets/architecture.png)

&gt; See it live at [deerflow.tech](https://deerflow.tech/#multi-agent-architecture)

The system employs a streamlined workflow with the following components:

1. **Coordinator**: The entry point that manages the workflow lifecycle

   - Initiates the research process based on user input
   - Delegates tasks to the planner when appropriate
   - Acts as the primary interface between the user and the system

2. **Planner**: Strategic component for task decomposition and planning

   - Analyzes research objectives and creates structured execution plans
   - Determines if enough context is available or if more research is needed
   - Manages the research flow and decides when to generate the final report

3. **Research Team**: A collection of specialized agents that execute the plan:

   - **Researcher**: Conducts web searches and information gathering using tools like web search engines, crawling and even MCP services.
   - **Coder**: Handles code analysis, execution, and technical tasks using Python REPL tool.
     Each agent has access to specific tools optimized for their role and operates within the LangGraph framework

4. **Reporter**: Final stage processor for research outputs
   - Aggregates findings from the research team
   - Processes and structures the collected information
   - Generates comprehensive research reports

## Text-to-Speech Integration

DeerFlow now includes a Text-to-Speech (TTS) feature that allows you to convert research reports to speech. This feature uses the volcengine TTS API to generate high-quality audio from text. Features like speed, volume, and pitch are also customizable.

### Using the TTS API

You can access the TTS functionality through the `/api/tts` endpoint:

```bash
# Example API call using curl
curl --location &#039;http://localhost:8000/api/tts&#039; \
--header &#039;Content-Type: application/json&#039; \
--data &#039;{
    &quot;text&quot;: &quot;This is a test of the text-to-speech functionality.&quot;,
    &quot;speed_ratio&quot;: 1.0,
    &quot;volume_ratio&quot;: 1.0,
    &quot;pitch_ratio&quot;: 1.0
}&#039; \
--output speech.mp3
```

## Development

### Testing

Run the test suite:

```bash
# Run all tests
make test

# Run specific test file
pytest tests/integration/test_workflow.py

# Run with coverage
make coverage
```

### Code Quality

```bash
# Run linting
make lint

# Format code
make format
```

### Debugging with LangGraph Studio

DeerFlow uses LangGraph for its workflow architecture. You can use LangGraph Studio to debug and visualize the workflow in real-time.

#### Running LangGraph Studio Locally

DeerFlow includes a `langgraph.json` configuration file that defines the graph structure and dependencies for the LangGraph Studio. This file points to the workflow graphs defined in the project and automatically loads environment variables from the `.env` file.

##### Mac

```bash
# Install uv package manager if you don&#039;t have it
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install dependencies and start the LangGraph server
uvx --refresh --from &quot;langgraph-cli[inmem]&quot; --with-editable . --python 3.12 langgraph dev --allow-blocking
```

##### Windows / Linux

```bash
# Install dependencies
pip install -e .
pip install -U &quot;langgraph-cli[inmem]&quot;

# Start the LangGraph server
langgraph dev
```

After starting the LangGraph server, you&#039;ll see several URLs in the terminal:

- API: http://127.0.0.1:2024
- Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024
- API Docs: http://127.0.0.1:2024/docs

Open the Studio UI link in your browser to access the debugging interface.

#### Using LangGraph Studio

In the Studio UI, you can:

1. Visualize the workflow graph and see how components connect
2. Trace execution in real-time to see how data flows through the system
3. Inspect the state at each step of the workflow
4. Debug issues by examining inputs and outputs of each component
5. Provide feedback during the planning phase to refine research plans

When you submit a research topic in the Studio UI, you&#039;ll be able to see the entire workflow execution, including:

- The planning phase where the research plan is created
- The feedback loop where you can modify the plan
- The research and writing phases for each section
- The final report generation

### Enabling LangSmith Tracing

DeerFlow supports LangSmith tracing to help you debug and monitor your workflows. To enable LangSmith tracing:

1. Make sure your `.env` file has the following configurations (see `.env.example`):

   ```bash
   LANGSMITH_TRACING=true
   LANGSMITH_ENDPOINT=&quot;https://api.smith.langchain.com&quot;
   LANGSMITH_API_KEY=&quot;xxx&quot;
   LANGSMITH_PROJECT=&quot;xxx&quot;
   ```

2. Start tracing and visualize the graph locally with LangSmith by running:
   ```bash
   langgraph dev
   ```

This will enable trace visualization in LangGraph Studio and send your traces to LangSmith for monitoring and analysis.

### Checkpointing
1. Postgres and MonogDB implementation of LangGraph checkpoint saver.
2. In-memory store is used to caching the streaming messages before persisting to database, If finish_reason is &quot;stop&quot; or &quot;interrupt&quot;, it triggers persistence.
3. Supports saving and loading checkpoints for workflow execution.
4. Supports saving chat stream events for replaying conversations.

*Note: About langgraph issue #5557* 
The latest langgraph-checkpoint-postgres-2.0.23 have checkpointing issue, you can check the open issue:&quot;TypeError: Object of type HumanMessage is not JSON serializable&quot;  [https://github.com/langchain-ai/langgraph/issues/5557].

To use postgres checkpoint you should install langgraph-checkpoint-postgres-2.0.21

*Note: About psycopg dependencies* 
Please read the following document before using postgres:  https://www.psycopg.org/psycopg3/docs/basic/install.html

BY default, psycopg needs libpq to be installed on your system. If you don&#039;t have libpq installed, you can install psycopg with the `binary` extra to include a statically linked version of libpq mannually:

```bash
pip install psycopg[binary]
```
This will install a self-contained package with all the libraries needed, but binary not supported for all platform, you check the supported platform : https://pypi.org/project/psycopg-binary/#files

if not supported, you can select local-installation: https://www.psycopg.org/psycopg3/docs/basic/install.html#local-installation


The default database and collection will be automatically created if not exists.
Default database: checkpoing_db
Default collection: checkpoint_writes_aio (langgraph checkpoint writes)
Default collection: checkpoints_aio (langgraph checkpoints)
Default collection: chat_streams (chat stream events for replaying conversations)

You need to set the following environment variables in your `.env` file:

```bash
# Enable LangGraph checkpoint saver, supports MongoDB, Postgres
LANGGRAPH_CHECKPOINT_SAVER=true
# Set the database URL for saving checkpoints
LANGGRAPH_CHECKPOINT_DB_URL=&quot;mongodb://localhost:27017/&quot;
#LANGGRAPH_CHECKPOINT_DB_URL=postgresql://localhost:5432/postgres
```

## Docker

You can also run this project with Docker.

First, you need read the [configuration](docs/configuration_guide.md) below. Make sure `.env`, `.conf.yaml` files are ready.

Second, to build a Docker image of your own web server:

```bash
docker build -t deer-flow-api .
```

Final, start up a docker container running the web server:

```bash
# Replace deer-flow-api-app with your preferred container name
# Start the server then bind to localhost:8000
docker run -d -t -p 127.0.0.1:8000:8000 --env-file .env --name deer-flow-api-app deer-flow-api

# stop the server
docker stop deer-flow-api-app
```

### Docker Compose (include both backend and frontend)

DeerFlow provides a docker-compose setup to easily run both the backend and frontend together:

```bash
# building docker image
docker compose build

# start the server
docker compose up
```

&gt; [!WARNING]
&gt; If you want to deploy the deer flow into production environments, please add authentication to the website and evaluate your security check of the MCPServer and Python Repl. 

## Examples

The following examples demonstrate the capabilities of DeerFlow:

### Research Reports

1. **OpenAI Sora Report** - Analysis of OpenAI&#039;s Sora AI tool

   - Discusses features, access, prompt engineering, limitations, and ethical considerations
   - [View full report](examples/openai_sora_report.md)

2. **Google&#039;s Agent to Agent Protocol Report** - Overview of Google&#039;s Agent to Agent (A2A) protocol

   - Discusses its role in AI agent communication and its relationship with Anthropic&#039;s Model Context Protocol (MCP)
   - [View full report](examples/what_is_agent_to_agent_protocol.md)

3. **What is MCP?** - A comprehensive analysis of the term &quot;MCP&quot; across multiple contexts

   - Explores Model Context Protocol in AI, Monocalcium Phosphate in chemistry, and Micro-channel Plate in 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[vllm-project/vllm]]></title>
            <link>https://github.com/vllm-project/vllm</link>
            <guid>https://github.com/vllm-project/vllm</guid>
            <pubDate>Sat, 11 Oct 2025 00:04:05 GMT</pubDate>
            <description><![CDATA[A high-throughput and memory-efficient inference and serving engine for LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/vllm-project/vllm">vllm-project/vllm</a></h1>
            <p>A high-throughput and memory-efficient inference and serving engine for LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 59,782</p>
            <p>Forks: 10,607</p>
            <p>Stars today: 70 stars today</p>
            <h2>README</h2><pre>&lt;!-- markdownlint-disable MD001 MD041 --&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-dark.png&quot;&gt;
    &lt;img alt=&quot;vLLM&quot; src=&quot;https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-light.png&quot; width=55%&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;h3 align=&quot;center&quot;&gt;
Easy, fast, and cheap LLM serving for everyone
&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
| &lt;a href=&quot;https://docs.vllm.ai&quot;&gt;&lt;b&gt;Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://blog.vllm.ai/&quot;&gt;&lt;b&gt;Blog&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://arxiv.org/abs/2309.06180&quot;&gt;&lt;b&gt;Paper&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://x.com/vllm_project&quot;&gt;&lt;b&gt;Twitter/X&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://discuss.vllm.ai&quot;&gt;&lt;b&gt;User Forum&lt;/b&gt;&lt;/a&gt; | &lt;a href=&quot;https://slack.vllm.ai&quot;&gt;&lt;b&gt;Developer Slack&lt;/b&gt;&lt;/a&gt; |
&lt;/p&gt;

---
Join us at the [PyTorch Conference, October 22-23](https://events.linuxfoundation.org/pytorch-conference/) and [Ray Summit, November 3-5](https://www.anyscale.com/ray-summit/2025) in San Francisco for our latest updates on vLLM and to meet the vLLM team! Register now for the largest vLLM community events of the year!

---

*Latest News* üî•

- [2025/09] We hosted [vLLM Toronto Meetup](https://luma.com/e80e0ymm) focused on tackling inference at scale and speculative decoding with speakers from NVIDIA and Red Hat! Please find the meetup slides [here](https://docs.google.com/presentation/d/1IYJYmJcu9fLpID5N5RbW_vO0XLo0CGOR14IXOjB61V8/edit?usp=sharing).
- [2025/08] We hosted [vLLM Shenzhen Meetup](https://mp.weixin.qq.com/s/k8ZBO1u2_2odgiKWH_GVTQ) focusing on the ecosystem around vLLM! Please find the meetup slides [here](https://drive.google.com/drive/folders/1Ua2SVKVSu-wp5vou_6ElraDt2bnKhiEA).
- [2025/08] We hosted [vLLM Singapore Meetup](https://www.sginnovate.com/event/vllm-sg-meet). We shared V1 updates, disaggregated serving and MLLM speedups with speakers from Embedded LLM, AMD, WekaIO, and A*STAR. Please find the meetup slides [here](https://drive.google.com/drive/folders/1ncf3GyqLdqFaB6IeB834E5TZJPLAOiXZ?usp=sharing).
- [2025/08] We hosted [vLLM Shanghai Meetup](https://mp.weixin.qq.com/s/pDmAXHcN7Iqc8sUKgJgGtg) focusing on building, developing, and integrating with vLLM! Please find the meetup slides [here](https://drive.google.com/drive/folders/1OvLx39wnCGy_WKq8SiVKf7YcxxYI3WCH).
- [2025/05] vLLM is now a hosted project under PyTorch Foundation! Please find the announcement [here](https://pytorch.org/blog/pytorch-foundation-welcomes-vllm/).
- [2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post [here](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html).

&lt;details&gt;
&lt;summary&gt;Previous News&lt;/summary&gt;

- [2025/08] We hosted [vLLM Korea Meetup](https://luma.com/cgcgprmh) with Red Hat and Rebellions! We shared the latest advancements in vLLM along with project spotlights from the vLLM Korea community. Please find the meetup slides [here](https://drive.google.com/file/d/1bcrrAE1rxUgx0mjIeOWT6hNe2RefC5Hm/view).
- [2025/08] We hosted [vLLM Beijing Meetup](https://mp.weixin.qq.com/s/dgkWg1WFpWGO2jCdTqQHxA) focusing on large-scale LLM deployment! Please find the meetup slides [here](https://drive.google.com/drive/folders/1Pid6NSFLU43DZRi0EaTcPgXsAzDvbBqF) and the recording [here](https://www.chaspark.com/#/live/1166916873711665152).
- [2025/05] We hosted [NYC vLLM Meetup](https://lu.ma/c1rqyf1f)! Please find the meetup slides [here](https://docs.google.com/presentation/d/1_q_aW_ioMJWUImf1s1YM-ZhjXz8cUeL0IJvaquOYBeA/edit?usp=sharing).
- [2025/04] We hosted [Asia Developer Day](https://www.sginnovate.com/event/limited-availability-morning-evening-slots-remaining-inaugural-vllm-asia-developer-day)! Please find the meetup slides from the vLLM team [here](https://docs.google.com/presentation/d/19cp6Qu8u48ihB91A064XfaXruNYiBOUKrBxAmDOllOo/edit?usp=sharing).
- [2025/03] We hosted [vLLM x Ollama Inference Night](https://lu.ma/vllm-ollama)! Please find the meetup slides from the vLLM team [here](https://docs.google.com/presentation/d/16T2PDD1YwRnZ4Tu8Q5r6n53c5Lr5c73UV9Vd2_eBo4U/edit?usp=sharing).
- [2025/03] We hosted [the first vLLM China Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg)! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1REHvfQMKGnvz6p3Fd23HhSO4c8j5WPGZV0bKYLwnHyQ/edit?usp=sharing).
- [2025/03] We hosted [the East Coast vLLM Meetup](https://lu.ma/7mu4k4xx)! Please find the meetup slides [here](https://docs.google.com/presentation/d/1NHiv8EUFF1NLd3fEYODm56nDmL26lEeXCaDgyDlTsRs/edit#slide=id.g31441846c39_0_0).
- [2025/02] We hosted [the ninth vLLM meetup](https://lu.ma/h7g3kuj9) with Meta! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1jzC_PZVXrVNSFVCW-V4cFXb6pn7zZ2CyP_Flwo05aqg/edit?usp=sharing) and AMD [here](https://drive.google.com/file/d/1Zk5qEJIkTmlQ2eQcXQZlljAx3m9s7nwn/view?usp=sharing). The slides from Meta will not be posted.
- [2025/01] We hosted [the eighth vLLM meetup](https://lu.ma/zep56hui) with Google Cloud! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing), and Google Cloud team [here](https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing).
- [2024/12] vLLM joins [pytorch ecosystem](https://pytorch.org/blog/vllm-joins-pytorch)! Easy, Fast, and Cheap LLM Serving for Everyone!
- [2024/11] We hosted [the seventh vLLM meetup](https://lu.ma/h0qvrajz) with Snowflake! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing), and Snowflake team [here](https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing).
- [2024/10] We have just created a developer slack ([slack.vllm.ai](https://slack.vllm.ai)) focusing on coordinating contributions and discussing features. Please feel free to join us there!
- [2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team [here](https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing). Learn more from the [talks](https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR) from other vLLM contributors and users!
- [2024/09] We hosted [the sixth vLLM meetup](https://lu.ma/87q3nvnh) with NVIDIA! Please find the meetup slides [here](https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing).
- [2024/07] We hosted [the fifth vLLM meetup](https://lu.ma/lp0gyjqr) with AWS! Please find the meetup slides [here](https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing).
- [2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post [here](https://blog.vllm.ai/2024/07/23/llama31.html).
- [2024/06] We hosted [the fourth vLLM meetup](https://lu.ma/agivllm) with Cloudflare and BentoML! Please find the meetup slides [here](https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing).
- [2024/04] We hosted [the third vLLM meetup](https://robloxandvllmmeetup2024.splashthat.com/) with Roblox! Please find the meetup slides [here](https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing).
- [2024/01] We hosted [the second vLLM meetup](https://lu.ma/ygxbpzhl) with IBM! Please find the meetup slides [here](https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing).
- [2023/10] We hosted [the first vLLM meetup](https://lu.ma/first-vllm-meetup) with a16z! Please find the meetup slides [here](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing).
- [2023/08] We would like to express our sincere gratitude to [Andreessen Horowitz](https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/) (a16z) for providing a generous grant to support the open-source development and research of vLLM.
- [2023/06] We officially released vLLM! FastChat-vLLM integration has powered [LMSYS Vicuna and Chatbot Arena](https://chat.lmsys.org) since mid-April. Check out our [blog post](https://vllm.ai).

&lt;/details&gt;

---

## About

vLLM is a fast and easy-to-use library for LLM inference and serving.

Originally developed in the [Sky Computing Lab](https://sky.cs.berkeley.edu) at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.

vLLM is fast with:

- State-of-the-art serving throughput
- Efficient management of attention key and value memory with [**PagedAttention**](https://blog.vllm.ai/2023/06/20/vllm.html)
- Continuous batching of incoming requests
- Fast model execution with CUDA/HIP graph
- Quantizations: [GPTQ](https://arxiv.org/abs/2210.17323), [AWQ](https://arxiv.org/abs/2306.00978), [AutoRound](https://arxiv.org/abs/2309.05516), INT4, INT8, and FP8
- Optimized CUDA kernels, including integration with FlashAttention and FlashInfer
- Speculative decoding
- Chunked prefill

vLLM is flexible and easy to use with:

- Seamless integration with popular Hugging Face models
- High-throughput serving with various decoding algorithms, including *parallel sampling*, *beam search*, and more
- Tensor, pipeline, data and expert parallelism support for distributed inference
- Streaming outputs
- OpenAI-compatible API server
- Support for NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, and TPU. Additionally, support for diverse hardware plugins such as Intel Gaudi, IBM Spyre and Huawei Ascend.
- Prefix caching support
- Multi-LoRA support

vLLM seamlessly supports most popular open-source models on HuggingFace, including:

- Transformer-like LLMs (e.g., Llama)
- Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)
- Embedding Models (e.g., E5-Mistral)
- Multi-modal LLMs (e.g., LLaVA)

Find the full list of supported models [here](https://docs.vllm.ai/en/latest/models/supported_models.html).

## Getting Started

Install vLLM with `pip` or [from source](https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source):

```bash
pip install vllm
```

Visit our [documentation](https://docs.vllm.ai/en/latest/) to learn more.

- [Installation](https://docs.vllm.ai/en/latest/getting_started/installation.html)
- [Quickstart](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)
- [List of Supported Models](https://docs.vllm.ai/en/latest/models/supported_models.html)

## Contributing

We welcome and value any contributions and collaborations.
Please check out [Contributing to vLLM](https://docs.vllm.ai/en/latest/contributing/index.html) for how to get involved.

## Sponsors

vLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!

&lt;!-- Note: Please sort them in alphabetical order. --&gt;
&lt;!-- Note: Please keep these consistent with docs/community/sponsors.md --&gt;
Cash Donations:

- a16z
- Dropbox
- Sequoia Capital
- Skywork AI
- ZhenFund

Compute Resources:

- Alibaba Cloud
- AMD
- Anyscale
- AWS
- Crusoe Cloud
- Databricks
- DeepInfra
- Google Cloud
- Intel
- Lambda Lab
- Nebius
- Novita AI
- NVIDIA
- Replicate
- Roblox
- RunPod
- Trainy
- UC Berkeley
- UC San Diego
- Volcengine

Slack Sponsor: Anyscale

We also have an official fundraising venue through [OpenCollective](https://opencollective.com/vllm). We plan to use the fund to support the development, maintenance, and adoption of vLLM.

## Citation

If you use vLLM for your research, please cite our [paper](https://arxiv.org/abs/2309.06180):

```bibtex
@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}
```

## Contact Us

&lt;!-- --8&lt;-- [start:contact-us] --&gt;
- For technical questions and feature requests, please use GitHub [Issues](https://github.com/vllm-project/vllm/issues)
- For discussing with fellow users, please use the [vLLM Forum](https://discuss.vllm.ai)
- For coordinating contributions and development, please use [Slack](https://slack.vllm.ai)
- For security disclosures, please use GitHub&#039;s [Security Advisories](https://github.com/vllm-project/vllm/security/advisories) feature
- For collaborations and partnerships, please contact us at [vllm-questions@lists.berkeley.edu](mailto:vllm-questions@lists.berkeley.edu)
&lt;!-- --8&lt;-- [end:contact-us] --&gt;

## Media Kit

- If you wish to use vLLM&#039;s logo, please refer to [our media kit repo](https://github.com/vllm-project/media-kit)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Zie619/n8n-workflows]]></title>
            <link>https://github.com/Zie619/n8n-workflows</link>
            <guid>https://github.com/Zie619/n8n-workflows</guid>
            <pubDate>Sat, 11 Oct 2025 00:04:04 GMT</pubDate>
            <description><![CDATA[all of the workflows of n8n i could find (also from the site itself)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Zie619/n8n-workflows">Zie619/n8n-workflows</a></h1>
            <p>all of the workflows of n8n i could find (also from the site itself)</p>
            <p>Language: Python</p>
            <p>Stars: 35,841</p>
            <p>Forks: 2,907</p>
            <p>Stars today: 186 stars today</p>
            <h2>README</h2><pre># ‚ö° N8N Workflow Collection &amp; Documentation

A professionally organized collection of **2,057 n8n workflows** with a lightning-fast documentation system that provides instant search, analysis, and browsing capabilities.

&gt; **‚ö†Ô∏è IMPORTANT NOTICE (Aug 14, 2025):** Repository history has been rewritten due to DMCA compliance. If you have a fork or local clone, please see [Issue 85](https://github.com/Zie619/n8n-workflows/issues/85) for instructions on syncing your copy.
&gt; 
## Support My Work

[![Buy Me a Coffee](https://img.shields.io/badge/-Buy%20Me%20a%20Coffee-ffdd00?logo=buy-me-a-coffee&amp;logoColor=black&amp;style=flat)](https://www.buymeacoffee.com/zie619)

If you&#039;d like to say thanks, consider buying me a coffee‚Äîyour support helps me keep improving this project!

## üöÄ **NEW: Public Search Interface &amp; High-Performance Documentation**

**üåê [Browse workflows online](https://zie619.github.io/n8n-workflows) - No installation required!**

**Or run locally for development with 100x performance improvement:**

### Option 1: Online Search (Recommended for Users)
**üîó Visit: [zie619.github.io/n8n-workflows](https://zie619.github.io/n8n-workflows)**
- ‚ö° **Instant access** - No setup required
- üîç **Search 2,057+ workflows** directly in browser
- üì± **Mobile-friendly** interface
- üè∑Ô∏è **Category filtering** across 15 categories
- üì• **Direct download** of workflow JSON files

### Option 2: Local Development System
```bash
# Install dependencies
pip install -r requirements.txt

# Start the fast API server
python run.py

# Open in browser
http://localhost:8000
```

**Features:**
- ‚ö° **Sub-100ms response times** with SQLite FTS5 search
- üîç **Instant full-text search** with advanced filtering
- üì± **Responsive design** - works perfectly on mobile
- üåô **Dark/light themes** with system preference detection
- üìä **Live statistics** - 365 unique integrations, 29,445 total nodes
- üéØ **Smart categorization** by trigger type and complexity
- üéØ **Use case categorization** by service name mapped to categories
- üìÑ **On-demand JSON viewing** and download
- üîó **Mermaid diagram generation** for workflow visualization
- üîÑ **Real-time workflow naming** with intelligent formatting

### Performance Comparison

| Metric | Old System | New System | Improvement |
|--------|------------|------------|-------------|
| **File Size** | 71MB HTML | &lt;100KB | **700x smaller** |
| **Load Time** | 10+ seconds | &lt;1 second | **10x faster** |
| **Search** | Client-side only | Full-text with FTS5 | **Instant** |
| **Memory Usage** | ~2GB RAM | &lt;50MB RAM | **40x less** |
| **Mobile Support** | Poor | Excellent | **Fully responsive** |

---

## üìÇ Repository Organization

### Workflow Collection
- **2,057 workflows** with meaningful, searchable names
- **365 unique integrations** across popular platforms
- **29,445 total nodes** with professional categorization
- **Quality assurance** - All workflows analyzed and categorized

### Advanced Naming System ‚ú®
Our intelligent naming system converts technical filenames into readable titles:
- **Before**: `2051_Telegram_Webhook_Automation_Webhook.json`
- **After**: `Telegram Webhook Automation`
- **100% meaningful names** with smart capitalization
- **Automatic integration detection** from node analysis

### Use Case Category ‚ú®

The search interface includes a dropdown filter that lets you browse 2,057+ workflows by category.

The system includes an automated categorization feature that organizes workflows by service categories to make them easier to discover and filter.

### How Categorization Works

1. **Run the categorization script**
   ```
   python create_categories.py
   ```

2. **Service Name Recognition**
   The script analyzes each workflow JSON filename to identify recognized service names (e.g., &quot;Twilio&quot;, &quot;Slack&quot;, &quot;Gmail&quot;, etc.)

3. **Category Mapping**
   Each recognized service name is matched to its corresponding category using the definitions in `context/def_categories.json`. For example:
   - Twilio ‚Üí Communication &amp; Messaging
   - Gmail ‚Üí Communication &amp; Messaging  
   - Airtable ‚Üí Data Processing &amp; Analysis
   - Salesforce ‚Üí CRM &amp; Sales

4. **Search Categories Generation**
   The script produces a `search_categories.json` file that contains the categorized workflow data

5. **Filter Interface**
   Users can then filter workflows by category in the search interface, making it easier to find workflows for specific use cases

### Available Categories

The categorization system includes the following main categories:
- AI Agent Development
- Business Process Automation
- Cloud Storage &amp; File Management
- Communication &amp; Messaging
- Creative Content &amp; Video Automation
- Creative Design Automation
- CRM &amp; Sales
- Data Processing &amp; Analysis
- E-commerce &amp; Retail
- Financial &amp; Accounting
- Marketing &amp; Advertising Automation
- Project Management
- Social Media Management
- Technical Infrastructure &amp; DevOps
- Web Scraping &amp; Data Extraction

### Contribute Categories

You can help expand the categorization by adding more service-to-category mappings (e.g., Twilio ‚Üí Communication &amp; Messaging) in context/defs_categories.json.

Many workflow JSON files are conveniently named with the service name, often separated by underscores (_).


---

## üõ† Usage Instructions

### Option 1: Modern Fast System (Recommended)
```bash
# Clone repository
git clone &lt;repo-url&gt;
cd n8n-workflows

# Install Python dependencies
pip install -r requirements.txt

# Start the documentation server
python run.py

# Browse workflows at http://localhost:8000
# - Instant search across 2,057 workflows
# - Professional responsive interface
# - Real-time workflow statistics
```

### Option 2: Development Mode
```bash
# Start with auto-reload for development
python run.py --dev

# Or specify custom host/port
python run.py --host 0.0.0.0 --port 3000

# Force database reindexing
python run.py --reindex
```

### Import Workflows into n8n
```bash
# Use the Python importer (recommended)
python import_workflows.py

# Or manually import individual workflows:
# 1. Open your n8n Editor UI
# 2. Click menu (‚ò∞) ‚Üí Import workflow
# 3. Choose any .json file from the workflows/ folder
# 4. Update credentials/webhook URLs before running
```

---

## üìä Workflow Statistics

### Current Collection Stats
- **Total Workflows**: 2,057 automation workflows
- **Active Workflows**: 215 (10.5% active rate)
- **Total Nodes**: 29,528 (avg 14.4 nodes per workflow)
- **Unique Integrations**: 367 different services and APIs
- **Database**: SQLite with FTS5 full-text search

### Trigger Distribution
- **Complex**: 832 workflows (40.4%) - Multi-trigger systems
- **Webhook**: 521 workflows (25.3%) - API-triggered automations  
- **Manual**: 478 workflows (23.2%) - User-initiated workflows
- **Scheduled**: 226 workflows (11.0%) - Time-based executions

### Complexity Analysis
- **Low (‚â§5 nodes)**: ~35% - Simple automations
- **Medium (6-15 nodes)**: ~45% - Standard workflows
- **High (16+ nodes)**: ~20% - Complex enterprise systems

### Popular Integrations
Top services by usage frequency:
- **Communication**: Telegram, Discord, Slack, WhatsApp
- **Cloud Storage**: Google Drive, Google Sheets, Dropbox
- **Databases**: PostgreSQL, MySQL, MongoDB, Airtable
- **AI/ML**: OpenAI, Anthropic, Hugging Face
- **Development**: HTTP Request, Webhook, GraphQL

---

## üîç Advanced Search Features

### Smart Search Categories
Our system automatically categorizes workflows into 15 main categories:

#### Available Categories:
- **AI Agent Development**: OpenAI, Anthropic, Hugging Face, CalcsLive
- **Business Process Automation**: Workflow utilities, scheduling, data processing
- **Cloud Storage &amp; File Management**: Google Drive, Dropbox, OneDrive, Box
- **Communication &amp; Messaging**: Telegram, Discord, Slack, WhatsApp, Email
- **Creative Content &amp; Video Automation**: YouTube, Vimeo, content creation
- **Creative Design Automation**: Canva, Figma, image processing
- **CRM &amp; Sales**: Salesforce, HubSpot, Pipedrive, customer management
- **Data Processing &amp; Analysis**: Database operations, analytics, data transformation
- **E-commerce &amp; Retail**: Shopify, Stripe, PayPal, online stores
- **Financial &amp; Accounting**: Financial tools, payment processing, accounting
- **Marketing &amp; Advertising Automation**: Email marketing, campaigns, lead generation
- **Project Management**: Jira, Trello, Asana, task management
- **Social Media Management**: LinkedIn, Twitter/X, Facebook, Instagram
- **Technical Infrastructure &amp; DevOps**: GitHub, deployment, monitoring
- **Web Scraping &amp; Data Extraction**: HTTP requests, webhooks, data collection

### API Usage Examples
```bash
# Search workflows by text
curl &quot;http://localhost:8000/api/workflows?q=telegram+automation&quot;

# Filter by trigger type and complexity
curl &quot;http://localhost:8000/api/workflows?trigger=Webhook&amp;complexity=high&quot;

# Find all messaging workflows
curl &quot;http://localhost:8000/api/workflows/category/messaging&quot;

# Get database statistics
curl &quot;http://localhost:8000/api/stats&quot;

# Browse available categories
curl &quot;http://localhost:8000/api/categories&quot;
```

---

## üèó Technical Architecture

### Modern Stack
- **SQLite Database** - FTS5 full-text search with 365 indexed integrations
- **FastAPI Backend** - RESTful API with automatic OpenAPI documentation
- **Responsive Frontend** - Modern HTML5 with embedded CSS/JavaScript
- **Smart Analysis** - Automatic workflow categorization and naming

### Key Features
- **Change Detection** - MD5 hashing for efficient re-indexing
- **Background Processing** - Non-blocking workflow analysis
- **Compressed Responses** - Gzip middleware for optimal speed
- **Error Handling** - Graceful degradation and comprehensive logging
- **Mobile Optimization** - Touch-friendly interface design

### Database Performance
```sql
-- Optimized schema for lightning-fast queries
CREATE TABLE workflows (
    id INTEGER PRIMARY KEY,
    filename TEXT UNIQUE,
    name TEXT,
    active BOOLEAN,
    trigger_type TEXT,
    complexity TEXT,
    node_count INTEGER,
    integrations TEXT,  -- JSON array of 365 unique services
    description TEXT,
    file_hash TEXT,     -- MD5 for change detection
    analyzed_at TIMESTAMP
);

-- Full-text search with ranking
CREATE VIRTUAL TABLE workflows_fts USING fts5(
    filename, name, description, integrations, tags,
    content=&#039;workflows&#039;, content_rowid=&#039;id&#039;
);
```

---

## üîß Setup &amp; Requirements

### System Requirements
- **Python 3.7+** - For running the documentation system
- **Modern Browser** - Chrome, Firefox, Safari, Edge
- **50MB Storage** - For SQLite database and indexes
- **n8n Instance** - For importing and running workflows

### Installation
```bash
# Clone repository
git clone &lt;repo-url&gt;
cd n8n-workflows

# Install dependencies
pip install -r requirements.txt

# Start documentation server
python run.py

# Access at http://localhost:8000
```

### Development Setup
```bash
# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate  # Linux/Mac
# or .venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Run with auto-reload for development
python api_server.py --reload

# Force database reindexing
python workflow_db.py --index --force
```

---

## üìã Naming Convention

### Intelligent Formatting System
Our system automatically converts technical filenames to user-friendly names:

```bash
# Automatic transformations:
2051_Telegram_Webhook_Automation_Webhook.json ‚Üí &quot;Telegram Webhook Automation&quot;
0250_HTTP_Discord_Import_Scheduled.json ‚Üí &quot;HTTP Discord Import Scheduled&quot;  
0966_OpenAI_Data_Processing_Manual.json ‚Üí &quot;OpenAI Data Processing Manual&quot;
```

### Technical Format
```
[ID]_[Service1]_[Service2]_[Purpose]_[Trigger].json
```

### Smart Capitalization Rules
- **HTTP** ‚Üí HTTP (not Http)
- **API** ‚Üí API (not Api)  
- **webhook** ‚Üí Webhook
- **automation** ‚Üí Automation
- **scheduled** ‚Üí Scheduled

---

## üöÄ API Documentation

### Core Endpoints
- `GET /` - Main workflow browser interface
- `GET /api/stats` - Database statistics and metrics
- `GET /api/workflows` - Search with filters and pagination
- `GET /api/workflows/{filename}` - Detailed workflow information
- `GET /api/workflows/{filename}/download` - Download workflow JSON
- `GET /api/workflows/{filename}/diagram` - Generate Mermaid diagram

### Advanced Search
- `GET /api/workflows/category/{category}` - Search by service category
- `GET /api/categories` - List all available categories
- `GET /api/integrations` - Get integration statistics
- `POST /api/reindex` - Trigger background reindexing

### Response Examples
```json
// GET /api/stats
{
  &quot;total&quot;: 2053,
  &quot;active&quot;: 215,
  &quot;inactive&quot;: 1838,
  &quot;triggers&quot;: {
    &quot;Complex&quot;: 831,
    &quot;Webhook&quot;: 519,
    &quot;Manual&quot;: 477,
    &quot;Scheduled&quot;: 226
  },
  &quot;total_nodes&quot;: 29445,
  &quot;unique_integrations&quot;: 365
}
```

---

## ü§ù Contributing

**üéâ This project solves [Issue #84](https://github.com/Zie619/n8n-workflows/issues/84) - providing online access to workflows without requiring local setup!**

### Adding New Workflows
1. **Export workflow** as JSON from n8n
2. **Name descriptively** following the established pattern: `[ID]_[Service]_[Purpose]_[Trigger].json`
3. **Add to workflows/** directory (create service folder if needed)
4. **Remove sensitive data** (credentials, personal URLs)
5. **Add tags** for better searchability (calculation, automation, etc.)
6. **GitHub Actions automatically** updates the public search interface

### Quality Standards
- ‚úÖ Workflow must be functional and tested
- ‚úÖ Remove all credentials and sensitive data
- ‚úÖ Follow naming convention for consistency
- ‚úÖ Verify compatibility with recent n8n versions
- ‚úÖ Include meaningful description or comments
- ‚úÖ Add relevant tags for search optimization

### Custom Node Workflows
- ‚úÖ Include npm package links in descriptions
- ‚úÖ Document custom node requirements
- ‚úÖ Add installation instructions
- ‚úÖ Use descriptive tags (like CalcsLive example)

### Reindexing (for local development)
```bash
# Force database reindexing after adding workflows
python run.py --reindex

# Or update search index only
python scripts/generate_search_index.py
```

---

## ‚ö†Ô∏è Important Notes

### Security &amp; Privacy
- **Review before use** - All workflows shared as-is for educational purposes
- **Update credentials** - Replace API keys, tokens, and webhooks
- **Test safely** - Verify in development environment first
- **Check permissions** - Ensure proper access rights for integrations

### Compatibility
- **n8n Version** - Compatible with n8n 1.0+ (most workflows)
- **Community Nodes** - Some workflows may require additional node installations
- **API Changes** - External services may have updated their APIs since creation
- **Dependencies** - Verify required integrations before importing

---

## üìö Resources &amp; References

### Workflow Sources
This comprehensive collection includes workflows from:
- **Official n8n.io** - Documentation and community examples
- **GitHub repositories** - Open source community contributions  
- **Blog posts &amp; tutorials** - Real-world automation patterns
- **User submissions** - Tested and verified workflows
- **Enterprise use cases** - Business process automations

### Learn More
- [n8n Documentation](https://docs.n8n.io/) - Official documentation
- [n8n Community](https://community.n8n.io/) - Community forum and support
- [Workflow Templates](https://n8n.io/workflows/) - Official template library
- [Integration Docs](https://docs.n8n.io/integrations/) - Service-specific guides

---

## üèÜ Project Achievements

### Repository Transformation
- **2,053 workflows** professionally organized and named
- **365 unique integrations** automatically detected and categorized
- **100% meaningful names** (improved from basic filename patterns)
- **Zero data loss** during intelligent renaming process
- **Advanced search** with 15 service categories

### Performance Revolution
- **Sub-100ms search** with SQLite FTS5 full-text indexing
- **Instant filtering** across 29,445 workflow nodes
- **Mobile-optimized** responsive design for all devices
- **Real-time statistics** with live database queries
- **Professional interface** with modern UX principles

### System Reliability
- **Robust error handling** with graceful degradation
- **Change detection** for efficient database updates
- **Background processing** for non-blocking operations
- **Comprehensive logging** for debugging and monitoring
- **Production-ready** with proper middleware and security

---

*This repository represents the most comprehensive and well-organized collection of n8n workflows available, featuring cutting-edge search technology and professional documentation that makes workflow discovery and usage a delightful experience.*

**üéØ Perfect for**: Developers, automation engineers, business analysts, and anyone looking to streamline their workflows with proven n8n automations.

---

[‰∏≠Êñá](./README_ZH.md)


</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[jiji262/douyin-downloader]]></title>
            <link>https://github.com/jiji262/douyin-downloader</link>
            <guid>https://github.com/jiji262/douyin-downloader</guid>
            <pubDate>Sat, 11 Oct 2025 00:04:03 GMT</pubDate>
            <description><![CDATA[ÊäñÈü≥ÊâπÈáè‰∏ãËΩΩÂ∑•ÂÖ∑ÔºåÂéªÊ∞¥Âç∞ÔºåÊîØÊåÅËßÜÈ¢ë„ÄÅÂõæÈõÜ„ÄÅÂêàÈõÜ„ÄÅÈü≥‰πê(ÂéüÂ£∞)„ÄÇÂÖçË¥πÔºÅÂÖçË¥πÔºÅÂÖçË¥πÔºÅ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/jiji262/douyin-downloader">jiji262/douyin-downloader</a></h1>
            <p>ÊäñÈü≥ÊâπÈáè‰∏ãËΩΩÂ∑•ÂÖ∑ÔºåÂéªÊ∞¥Âç∞ÔºåÊîØÊåÅËßÜÈ¢ë„ÄÅÂõæÈõÜ„ÄÅÂêàÈõÜ„ÄÅÈü≥‰πê(ÂéüÂ£∞)„ÄÇÂÖçË¥πÔºÅÂÖçË¥πÔºÅÂÖçË¥πÔºÅ</p>
            <p>Language: Python</p>
            <p>Stars: 5,256</p>
            <p>Forks: 776</p>
            <p>Stars today: 60 stars today</p>
            <h2>README</h2><pre># ÊäñÈü≥‰∏ãËΩΩÂô® - Êó†Ê∞¥Âç∞ÊâπÈáè‰∏ãËΩΩÂ∑•ÂÖ∑

![douyin-downloader](https://socialify.git.ci/jiji262/douyin-downloader/image?custom_description=%E6%8A%96%E9%9F%B3%E6%89%B9%E9%87%8F%E4%B8%8B%E8%BD%BD%E5%B7%A5%E5%85%B7%EF%BC%8C%E5%8E%BB%E6%B0%B4%E5%8D%B0%EF%BC%8C%E6%94%AF%E6%8C%81%E8%A7%86%E9%A2%91%E3%80%81%E5%9B%BE%E9%9B%86%E3%80%81%E5%90%88%E9%9B%86%E3%80%81%E9%9F%B3%E4%B9%90%28%E5%8E%9F%E5%A3%B0%29%E3%80%82%0A%E5%85%8D%E8%B4%B9%EF%BC%81%E5%85%8D%E8%B4%B9%EF%BC%81%E5%85%8D%E8%B4%B9%EF%BC%81&amp;description=1&amp;font=Jost&amp;forks=1&amp;logo=https%3A%2F%2Fraw.githubusercontent.com%2Fjiji262%2Fdouyin-downloader%2Frefs%2Fheads%2Fmain%2Fimg%2Flogo.png&amp;name=1&amp;owner=1&amp;pattern=Circuit+Board&amp;pulls=1&amp;stargazers=1&amp;theme=Light)

‰∏Ä‰∏™ÂäüËÉΩÂº∫Â§ßÁöÑÊäñÈü≥ÂÜÖÂÆπÊâπÈáè‰∏ãËΩΩÂ∑•ÂÖ∑ÔºåÊîØÊåÅËßÜÈ¢ë„ÄÅÂõæÈõÜ„ÄÅÈü≥‰πê„ÄÅÁõ¥Êí≠Á≠âÂ§öÁßçÂÜÖÂÆπÁ±ªÂûãÁöÑ‰∏ãËΩΩ„ÄÇÊèê‰æõ‰∏§‰∏™ÁâàÊú¨ÔºöV1.0ÔºàÁ®≥ÂÆöÁâàÔºâÂíå V2.0ÔºàÂ¢ûÂº∫ÁâàÔºâ„ÄÇ

## üìã ÁõÆÂΩï

- [Âø´ÈÄüÂºÄÂßã](#-Âø´ÈÄüÂºÄÂßã)
- [ÁâàÊú¨ËØ¥Êòé](#-ÁâàÊú¨ËØ¥Êòé)
- [V1.0 ‰ΩøÁî®ÊåáÂçó](#-v10-‰ΩøÁî®ÊåáÂçó)
- [V2.0 ‰ΩøÁî®ÊåáÂçó](#-v20-‰ΩøÁî®ÊåáÂçó)
- [Cookie ÈÖçÁΩÆÂ∑•ÂÖ∑](#-cookie-ÈÖçÁΩÆÂ∑•ÂÖ∑)
- [ÊîØÊåÅÁöÑÈìæÊé•Á±ªÂûã](#-ÊîØÊåÅÁöÑÈìæÊé•Á±ªÂûã)
- [Â∏∏ËßÅÈóÆÈ¢ò](#-Â∏∏ËßÅÈóÆÈ¢ò)
- [Êõ¥Êñ∞Êó•Âøó](#-Êõ¥Êñ∞Êó•Âøó)

## ‚ö° Âø´ÈÄüÂºÄÂßã

![qun](./img/fuye.jpg)

### ÁéØÂ¢ÉË¶ÅÊ±Ç

- **Python 3.9+**
- **Êìç‰ΩúÁ≥ªÁªü**ÔºöWindows„ÄÅmacOS„ÄÅLinux

### ÂÆâË£ÖÊ≠•È™§

1. **ÂÖãÈöÜÈ°πÁõÆ**
```bash
git clone https://github.com/jiji262/douyin-downloader.git
cd douyin-downloader
```

2. **ÂÆâË£Ö‰æùËµñ**
```bash
pip install -r requirements.txt
```

3. **ÈÖçÁΩÆ Cookie**ÔºàÈ¶ñÊ¨°‰ΩøÁî®ÈúÄË¶ÅÔºâ
```bash
# ÊñπÂºè1ÔºöËá™Âä®Ëé∑ÂèñÔºàÊé®ËçêÔºâ
python cookie_extractor.py

# ÊñπÂºè2ÔºöÊâãÂä®Ëé∑Âèñ
python get_cookies_manual.py
```

## üì¶ ÁâàÊú¨ËØ¥Êòé

### V1.0 (DouYinCommand.py) - Á®≥ÂÆöÁâà
- ‚úÖ **ÁªèËøáÈ™åËØÅ**ÔºöÁ®≥ÂÆöÂèØÈù†ÔºåÁªèËøáÂ§ßÈáèÊµãËØï
- ‚úÖ **ÁÆÄÂçïÊòìÁî®**ÔºöÈÖçÁΩÆÊñá‰ª∂È©±Âä®Ôºå‰ΩøÁî®ÁÆÄÂçï
- ‚úÖ **ÂäüËÉΩÂÆåÊï¥**ÔºöÊîØÊåÅÊâÄÊúâÂÜÖÂÆπÁ±ªÂûã‰∏ãËΩΩ
- ‚úÖ **Âçï‰∏™ËßÜÈ¢ë‰∏ãËΩΩ**ÔºöÂÆåÂÖ®Ê≠£Â∏∏Â∑•‰Ωú
- ‚ö†Ô∏è **ÈúÄË¶ÅÊâãÂä®ÈÖçÁΩÆ**ÔºöÈúÄË¶ÅÊâãÂä®Ëé∑ÂèñÂíåÈÖçÁΩÆ Cookie

### V2.0 (downloader.py) - Â¢ûÂº∫Áâà
- üöÄ **Ëá™Âä® Cookie ÁÆ°ÁêÜ**ÔºöÊîØÊåÅËá™Âä®Ëé∑ÂèñÂíåÂà∑Êñ∞ Cookie
- üöÄ **Áªü‰∏ÄÂÖ•Âè£**ÔºöÊï¥ÂêàÊâÄÊúâÂäüËÉΩÂà∞Âçï‰∏ÄËÑöÊú¨
- üöÄ **ÂºÇÊ≠•Êû∂ÊûÑ**ÔºöÊÄßËÉΩÊõ¥‰ºòÔºåÊîØÊåÅÂπ∂Âèë‰∏ãËΩΩ
- üöÄ **Êô∫ËÉΩÈáçËØï**ÔºöËá™Âä®ÈáçËØïÂíåÈîôËØØÊÅ¢Â§ç
- üöÄ **Â¢ûÈáè‰∏ãËΩΩ**ÔºöÊîØÊåÅÂ¢ûÈáèÊõ¥Êñ∞ÔºåÈÅøÂÖçÈáçÂ§ç‰∏ãËΩΩ
- ‚ö†Ô∏è **Âçï‰∏™ËßÜÈ¢ë‰∏ãËΩΩ**ÔºöÁõÆÂâç API ËøîÂõûÁ©∫ÂìçÂ∫îÔºàÂ∑≤Áü•ÈóÆÈ¢òÔºâ
- ‚úÖ **Áî®Êà∑‰∏ªÈ°µ‰∏ãËΩΩ**ÔºöÂÆåÂÖ®Ê≠£Â∏∏Â∑•‰Ωú

## üéØ V1.0 ‰ΩøÁî®ÊåáÂçó

### ÈÖçÁΩÆÊñá‰ª∂ËÆæÁΩÆ

1. **ÁºñËæëÈÖçÁΩÆÊñá‰ª∂**
```bash
cp config.example.yml config.yml
# ÁºñËæë config.yml Êñá‰ª∂
```

2. **ÈÖçÁΩÆÁ§∫‰æã**
```yaml
# ‰∏ãËΩΩÈìæÊé•
link:
  - https://v.douyin.com/xxxxx/                    # Âçï‰∏™ËßÜÈ¢ë
  - https://www.douyin.com/user/xxxxx              # Áî®Êà∑‰∏ªÈ°µ
  - https://www.douyin.com/collection/xxxxx        # ÂêàÈõÜ

# ‰øùÂ≠òË∑ØÂæÑ
path: ./Downloaded/

# CookieÈÖçÁΩÆÔºàÂøÖÂ°´Ôºâ
cookies:
  msToken: YOUR_MS_TOKEN_HERE
  ttwid: YOUR_TTWID_HERE
  odin_tt: YOUR_ODIN_TT_HERE
  passport_csrf_token: YOUR_PASSPORT_CSRF_TOKEN_HERE
  sid_guard: YOUR_SID_GUARD_HERE

# ‰∏ãËΩΩÈÄâÈ°π
music: True    # ‰∏ãËΩΩÈü≥‰πê
cover: True    # ‰∏ãËΩΩÂ∞ÅÈù¢
avatar: True   # ‰∏ãËΩΩÂ§¥ÂÉè
json: True     # ‰øùÂ≠òJSONÊï∞ÊçÆ

# ‰∏ãËΩΩÊ®°Âºè
mode:
  - post       # ‰∏ãËΩΩÂèëÂ∏ÉÁöÑ‰ΩúÂìÅ
  # - like     # ‰∏ãËΩΩÂñúÊ¨¢ÁöÑ‰ΩúÂìÅ
  # - mix      # ‰∏ãËΩΩÂêàÈõÜ

# ‰∏ãËΩΩÊï∞ÈáèÔºà0Ë°®Á§∫ÂÖ®ÈÉ®Ôºâ
number:
  post: 0      # ÂèëÂ∏É‰ΩúÂìÅÊï∞Èáè
  like: 0      # ÂñúÊ¨¢‰ΩúÂìÅÊï∞Èáè
  allmix: 0    # ÂêàÈõÜÊï∞Èáè
  mix: 0       # Âçï‰∏™ÂêàÈõÜÂÜÖ‰ΩúÂìÅÊï∞Èáè

# ÂÖ∂‰ªñËÆæÁΩÆ
thread: 5      # ‰∏ãËΩΩÁ∫øÁ®ãÊï∞
database: True # ‰ΩøÁî®Êï∞ÊçÆÂ∫ìËÆ∞ÂΩï
```

### ËøêË°åÁ®ãÂ∫è

```bash
# ‰ΩøÁî®ÈÖçÁΩÆÊñá‰ª∂ËøêË°å
python DouYinCommand.py

# ÊàñËÄÖ‰ΩøÁî®ÂëΩ‰ª§Ë°åÂèÇÊï∞
python DouYinCommand.py --cmd False
```

### ‰ΩøÁî®Á§∫‰æã

```bash
# ‰∏ãËΩΩÂçï‰∏™ËßÜÈ¢ë
# Âú® config.yml ‰∏≠ËÆæÁΩÆ link ‰∏∫Âçï‰∏™ËßÜÈ¢ëÈìæÊé•
python DouYinCommand.py

# ‰∏ãËΩΩÁî®Êà∑‰∏ªÈ°µ
# Âú® config.yml ‰∏≠ËÆæÁΩÆ link ‰∏∫Áî®Êà∑‰∏ªÈ°µÈìæÊé•
python DouYinCommand.py

# ‰∏ãËΩΩÂêàÈõÜ
# Âú® config.yml ‰∏≠ËÆæÁΩÆ link ‰∏∫ÂêàÈõÜÈìæÊé•
python DouYinCommand.py
```

## üöÄ V2.0 ‰ΩøÁî®ÊåáÂçó

### ÂëΩ‰ª§Ë°å‰ΩøÁî®

```bash
# ‰∏ãËΩΩÂçï‰∏™ËßÜÈ¢ëÔºàÈúÄË¶ÅÂÖàÈÖçÁΩÆ CookieÔºâ
python downloader.py -u &quot;https://v.douyin.com/xxxxx/&quot;

# ‰∏ãËΩΩÁî®Êà∑‰∏ªÈ°µÔºàÊé®ËçêÔºâ
python downloader.py -u &quot;https://www.douyin.com/user/xxxxx&quot;

# Ëá™Âä®Ëé∑Âèñ Cookie Âπ∂‰∏ãËΩΩ
python downloader.py --auto-cookie -u &quot;https://www.douyin.com/user/xxxxx&quot;

# ÊåáÂÆö‰øùÂ≠òË∑ØÂæÑ
python downloader.py -u &quot;ÈìæÊé•&quot; --path &quot;./my_videos/&quot;

# ‰ΩøÁî®ÈÖçÁΩÆÊñá‰ª∂
python downloader.py --config
```

### ÈÖçÁΩÆÊñá‰ª∂‰ΩøÁî®

1. **ÂàõÂª∫ÈÖçÁΩÆÊñá‰ª∂**
```bash
cp config.example.yml config_simple.yml
```

2. **ÈÖçÁΩÆÁ§∫‰æã**
```yaml
# ‰∏ãËΩΩÈìæÊé•
link:
  - https://www.douyin.com/user/xxxxx

# ‰øùÂ≠òË∑ØÂæÑ
path: ./Downloaded/

# Ëá™Âä® Cookie ÁÆ°ÁêÜ
auto_cookie: true

# ‰∏ãËΩΩÈÄâÈ°π
music: true
cover: true
avatar: true
json: true

# ‰∏ãËΩΩÊ®°Âºè
mode:
  - post

# ‰∏ãËΩΩÊï∞Èáè
number:
  post: 10

# Â¢ûÈáè‰∏ãËΩΩ
increase:
  post: false

# Êï∞ÊçÆÂ∫ì
database: true
```

3. **ËøêË°åÁ®ãÂ∫è**
```bash
python downloader.py --config
```

### ÂëΩ‰ª§Ë°åÂèÇÊï∞

```bash
python downloader.py [ÈÄâÈ°π] [ÈìæÊé•...]

ÈÄâÈ°πÔºö
  -u, --url URL          ‰∏ãËΩΩÈìæÊé•
  -p, --path PATH        ‰øùÂ≠òË∑ØÂæÑ
  -c, --config           ‰ΩøÁî®ÈÖçÁΩÆÊñá‰ª∂
  --auto-cookie          Ëá™Âä®Ëé∑Âèñ Cookie
  --cookies COOKIES      ÊâãÂä®ÊåáÂÆö Cookie
  -h, --help            ÊòæÁ§∫Â∏ÆÂä©‰ø°ÊÅØ
```

## üç™ Cookie ÈÖçÁΩÆÂ∑•ÂÖ∑

### 1. cookie_extractor.py - Ëá™Âä®Ëé∑ÂèñÂ∑•ÂÖ∑

**ÂäüËÉΩ**Ôºö‰ΩøÁî® Playwright Ëá™Âä®ÊâìÂºÄÊµèËßàÂô®ÔºåËá™Âä®Ëé∑Âèñ Cookie

**‰ΩøÁî®ÊñπÂºè**Ôºö
```bash
# ÂÆâË£Ö Playwright
pip install playwright
playwright install chromium

# ËøêË°åËá™Âä®Ëé∑Âèñ
python cookie_extractor.py
```

**ÁâπÁÇπ**Ôºö
- ‚úÖ Ëá™Âä®ÊâìÂºÄÊµèËßàÂô®
- ‚úÖ ÊîØÊåÅÊâ´Á†ÅÁôªÂΩï
- ‚úÖ Ëá™Âä®Ê£ÄÊµãÁôªÂΩïÁä∂ÊÄÅ
- ‚úÖ Ëá™Âä®‰øùÂ≠òÂà∞ÈÖçÁΩÆÊñá‰ª∂
- ‚úÖ ÊîØÊåÅÂ§öÁßçÁôªÂΩïÊñπÂºè

**‰ΩøÁî®Ê≠•È™§**Ôºö
1. ËøêË°å `python cookie_extractor.py`
2. ÈÄâÊã©ÊèêÂèñÊñπÂºèÔºàÊé®ËçêÈÄâÊã©1Ôºâ
3. Âú®ÊâìÂºÄÁöÑÊµèËßàÂô®‰∏≠ÂÆåÊàêÁôªÂΩï
4. Á®ãÂ∫èËá™Âä®ÊèêÂèñÂπ∂‰øùÂ≠ò Cookie

### 2. get_cookies_manual.py - ÊâãÂä®Ëé∑ÂèñÂ∑•ÂÖ∑

**ÂäüËÉΩ**ÔºöÈÄöËøáÊµèËßàÂô®ÂºÄÂèëËÄÖÂ∑•ÂÖ∑ÊâãÂä®Ëé∑Âèñ Cookie

**‰ΩøÁî®ÊñπÂºè**Ôºö
```bash
python get_cookies_manual.py
```

**ÁâπÁÇπ**Ôºö
- ‚úÖ Êó†ÈúÄÂÆâË£Ö Playwright
- ‚úÖ ËØ¶ÁªÜÁöÑÊìç‰ΩúÊïôÁ®ã
- ‚úÖ ÊîØÊåÅ Cookie È™åËØÅ
- ‚úÖ Ëá™Âä®‰øùÂ≠òÂà∞ÈÖçÁΩÆÊñá‰ª∂
- ‚úÖ ÊîØÊåÅÂ§á‰ªΩÂíåÊÅ¢Â§ç

**‰ΩøÁî®Ê≠•È™§**Ôºö
1. ËøêË°å `python get_cookies_manual.py`
2. ÈÄâÊã©&quot;Ëé∑ÂèñÊñ∞ÁöÑCookie&quot;
3. ÊåâÁÖßÊïôÁ®ãÂú®ÊµèËßàÂô®‰∏≠Ëé∑Âèñ Cookie
4. Á≤òË¥¥ Cookie ÂÜÖÂÆπ
5. Á®ãÂ∫èËá™Âä®Ëß£ÊûêÂπ∂‰øùÂ≠ò

### Cookie Ëé∑ÂèñÊïôÁ®ã

#### ÊñπÊ≥ï‰∏ÄÔºöÊµèËßàÂô®ÂºÄÂèëËÄÖÂ∑•ÂÖ∑

1. ÊâìÂºÄÊµèËßàÂô®ÔºåËÆøÈóÆ [ÊäñÈü≥ÁΩëÈ°µÁâà](https://www.douyin.com)
2. ÁôªÂΩï‰Ω†ÁöÑÊäñÈü≥Ë¥¶Âè∑
3. Êåâ `F12` ÊâìÂºÄÂºÄÂèëËÄÖÂ∑•ÂÖ∑
4. ÂàáÊç¢Âà∞ `Network` Ê†áÁ≠æÈ°µ
5. Âà∑Êñ∞È°µÈù¢ÔºåÊâæÂà∞‰ªªÊÑèËØ∑Ê±Ç
6. Âú®ËØ∑Ê±ÇÂ§¥‰∏≠ÊâæÂà∞ `Cookie` Â≠óÊÆµ
7. Â§çÂà∂‰ª•‰∏ãÂÖ≥ÈîÆ cookie ÂÄºÔºö
   - `msToken`
   - `ttwid`
   - `odin_tt`
   - `passport_csrf_token`
   - `sid_guard`

#### ÊñπÊ≥ï‰∫åÔºö‰ΩøÁî®Ëá™Âä®Â∑•ÂÖ∑

```bash
# Êé®Ëçê‰ΩøÁî®Ëá™Âä®Â∑•ÂÖ∑
python cookie_extractor.py
```

## üìã ÊîØÊåÅÁöÑÈìæÊé•Á±ªÂûã

### üé¨ ËßÜÈ¢ëÂÜÖÂÆπ
- **Âçï‰∏™ËßÜÈ¢ëÂàÜ‰∫´ÈìæÊé•**Ôºö`https://v.douyin.com/xxxxx/`
- **Âçï‰∏™ËßÜÈ¢ëÁõ¥Èìæ**Ôºö`https://www.douyin.com/video/xxxxx`
- **ÂõæÈõÜ‰ΩúÂìÅ**Ôºö`https://www.douyin.com/note/xxxxx`

### üë§ Áî®Êà∑ÂÜÖÂÆπ
- **Áî®Êà∑‰∏ªÈ°µ**Ôºö`https://www.douyin.com/user/xxxxx`
  - ÊîØÊåÅ‰∏ãËΩΩÁî®Êà∑ÂèëÂ∏ÉÁöÑÊâÄÊúâ‰ΩúÂìÅ
  - ÊîØÊåÅ‰∏ãËΩΩÁî®Êà∑ÂñúÊ¨¢ÁöÑ‰ΩúÂìÅÔºàÈúÄË¶ÅÊùÉÈôêÔºâ

### üìö ÂêàÈõÜÂÜÖÂÆπ
- **Áî®Êà∑ÂêàÈõÜ**Ôºö`https://www.douyin.com/collection/xxxxx`
- **Èü≥‰πêÂêàÈõÜ**Ôºö`https://www.douyin.com/music/xxxxx`

### üî¥ Áõ¥Êí≠ÂÜÖÂÆπ
- **Áõ¥Êí≠Èó¥**Ôºö`https://live.douyin.com/xxxxx`

## üîß Â∏∏ËßÅÈóÆÈ¢ò

### Q: ‰∏∫‰ªÄ‰πàÂçï‰∏™ËßÜÈ¢ë‰∏ãËΩΩÂ§±Ë¥•Ôºü
**A**: 
- V1.0ÔºöËØ∑Ê£ÄÊü• Cookie ÊòØÂê¶ÊúâÊïàÔºåÁ°Æ‰øùÂåÖÂê´ÂøÖË¶ÅÁöÑÂ≠óÊÆµ
- V2.0ÔºöÁõÆÂâçÂ∑≤Áü•ÈóÆÈ¢òÔºåAPI ËøîÂõûÁ©∫ÂìçÂ∫îÔºåÂª∫ËÆÆ‰ΩøÁî®Áî®Êà∑‰∏ªÈ°µ‰∏ãËΩΩ

### Q: Cookie ËøáÊúüÊÄé‰πàÂäûÔºü
**A**: 
- ‰ΩøÁî® `python cookie_extractor.py` ÈáçÊñ∞Ëé∑Âèñ
- Êàñ‰ΩøÁî® `python get_cookies_manual.py` ÊâãÂä®Ëé∑Âèñ

### Q: ‰∏ãËΩΩÈÄüÂ∫¶ÊÖ¢ÊÄé‰πàÂäûÔºü
**A**: 
- Ë∞ÉÊï¥ `thread` ÂèÇÊï∞Â¢ûÂä†Âπ∂ÂèëÊï∞
- Ê£ÄÊü•ÁΩëÁªúËøûÊé•
- ÈÅøÂÖçÂêåÊó∂‰∏ãËΩΩËøáÂ§öÂÜÖÂÆπ

### Q: Â¶Ç‰ΩïÊâπÈáè‰∏ãËΩΩÔºü
**A**: 
- V1.0ÔºöÂú® `config.yml` ‰∏≠Ê∑ªÂä†Â§ö‰∏™ÈìæÊé•
- V2.0Ôºö‰ΩøÁî®ÂëΩ‰ª§Ë°å‰º†ÂÖ•Â§ö‰∏™ÈìæÊé•Êàñ‰ΩøÁî®ÈÖçÁΩÆÊñá‰ª∂

### Q: ÊîØÊåÅÂì™‰∫õÊ†ºÂºèÔºü
**A**: 
- ËßÜÈ¢ëÔºöMP4 Ê†ºÂºèÔºàÊó†Ê∞¥Âç∞Ôºâ
- ÂõæÁâáÔºöJPG Ê†ºÂºè
- Èü≥È¢ëÔºöMP3 Ê†ºÂºè
- Êï∞ÊçÆÔºöJSON Ê†ºÂºè

## üìù Êõ¥Êñ∞Êó•Âøó

### V2.0 (2025-08)
- ‚úÖ **Áªü‰∏ÄÂÖ•Âè£**ÔºöÊï¥ÂêàÊâÄÊúâÂäüËÉΩÂà∞ `downloader.py`
- ‚úÖ **Ëá™Âä® Cookie ÁÆ°ÁêÜ**ÔºöÊîØÊåÅËá™Âä®Ëé∑ÂèñÂíåÂà∑Êñ∞
- ‚úÖ **ÂºÇÊ≠•Êû∂ÊûÑ**ÔºöÊÄßËÉΩ‰ºòÂåñÔºåÊîØÊåÅÂπ∂Âèë‰∏ãËΩΩ
- ‚úÖ **Êô∫ËÉΩÈáçËØï**ÔºöËá™Âä®ÈáçËØïÂíåÈîôËØØÊÅ¢Â§ç
- ‚úÖ **Â¢ûÈáè‰∏ãËΩΩ**ÔºöÊîØÊåÅÂ¢ûÈáèÊõ¥Êñ∞
- ‚úÖ **Áî®Êà∑‰∏ªÈ°µ‰∏ãËΩΩ**ÔºöÂÆåÂÖ®Ê≠£Â∏∏Â∑•‰Ωú
- ‚ö†Ô∏è **Âçï‰∏™ËßÜÈ¢ë‰∏ãËΩΩ**ÔºöAPI ËøîÂõûÁ©∫ÂìçÂ∫îÔºàÂ∑≤Áü•ÈóÆÈ¢òÔºâ

### V1.0 (2024-12)
- ‚úÖ **Á®≥ÂÆöÂèØÈù†**ÔºöÁªèËøáÂ§ßÈáèÊµãËØïÈ™åËØÅ
- ‚úÖ **ÂäüËÉΩÂÆåÊï¥**ÔºöÊîØÊåÅÊâÄÊúâÂÜÖÂÆπÁ±ªÂûã
- ‚úÖ **Âçï‰∏™ËßÜÈ¢ë‰∏ãËΩΩ**ÔºöÂÆåÂÖ®Ê≠£Â∏∏Â∑•‰Ωú
- ‚úÖ **ÈÖçÁΩÆÊñá‰ª∂È©±Âä®**ÔºöÁÆÄÂçïÊòìÁî®
- ‚úÖ **Êï∞ÊçÆÂ∫ìÊîØÊåÅ**ÔºöËÆ∞ÂΩï‰∏ãËΩΩÂéÜÂè≤

## ‚öñÔ∏è Ê≥ïÂæãÂ£∞Êòé

- Êú¨È°πÁõÆ‰ªÖ‰æõ**Â≠¶‰π†‰∫§ÊµÅ**‰ΩøÁî®
- ËØ∑ÈÅµÂÆàÁõ∏ÂÖ≥Ê≥ïÂæãÊ≥ïËßÑÂíåÂπ≥Âè∞ÊúçÂä°Êù°Ê¨æ
- ‰∏çÂæóÁî®‰∫éÂïÜ‰∏öÁî®ÈÄîÊàñ‰æµÁäØ‰ªñ‰∫∫ÊùÉÁõä
- ‰∏ãËΩΩÂÜÖÂÆπËØ∑Â∞äÈáçÂéü‰ΩúËÄÖÁâàÊùÉ

## ü§ù Ë¥°ÁåÆÊåáÂçó

Ê¨¢ËøéÊèê‰∫§ Issue Âíå Pull RequestÔºÅ

### Êä•ÂëäÈóÆÈ¢ò
- ‰ΩøÁî® [Issues](https://github.com/jiji262/douyin-downloader/issues) Êä•Âëä bug
- ËØ∑Êèê‰æõËØ¶ÁªÜÁöÑÈîôËØØ‰ø°ÊÅØÂíåÂ§çÁé∞Ê≠•È™§

### ÂäüËÉΩÂª∫ËÆÆ
- Âú® Issues ‰∏≠ÊèêÂá∫Êñ∞ÂäüËÉΩÂª∫ËÆÆ
- ËØ¶ÁªÜÊèèËø∞ÂäüËÉΩÈúÄÊ±ÇÂíå‰ΩøÁî®Âú∫ÊôØ

## üìÑ ËÆ∏ÂèØËØÅ

Êú¨È°πÁõÆÈááÁî® [MIT License](LICENSE) ÂºÄÊ∫êËÆ∏ÂèØËØÅ„ÄÇ

---

&lt;div align=&quot;center&quot;&gt;

**Â¶ÇÊûúËøô‰∏™È°πÁõÆÂØπ‰Ω†ÊúâÂ∏ÆÂä©ÔºåËØ∑Áªô‰∏™ ‚≠ê Star ÊîØÊåÅ‰∏Ä‰∏ãÔºÅ**

[üêõ Êä•ÂëäÈóÆÈ¢ò](https://github.com/jiji262/douyin-downloader/issues) ‚Ä¢ [üí° ÂäüËÉΩÂª∫ËÆÆ](https://github.com/jiji262/douyin-downloader/issues) ‚Ä¢ [üìñ Êü•ÁúãÊñáÊ°£](https://github.com/jiji262/douyin-downloader/wiki)

Made with ‚ù§Ô∏è by [jiji262](https://github.com/jiji262)

&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[THUDM/slime]]></title>
            <link>https://github.com/THUDM/slime</link>
            <guid>https://github.com/THUDM/slime</guid>
            <pubDate>Sat, 11 Oct 2025 00:04:02 GMT</pubDate>
            <description><![CDATA[slime is an LLM post-training framework for RL Scaling.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/THUDM/slime">THUDM/slime</a></h1>
            <p>slime is an LLM post-training framework for RL Scaling.</p>
            <p>Language: Python</p>
            <p>Stars: 2,098</p>
            <p>Forks: 201</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre># slime

[‰∏≠ÊñáÁâà](./README_zh.md)

[![Documentation](https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat)](https://thudm.github.io/slime/)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/THUDM/slime)

**slime** is an LLM post-training framework for RL scaling, providing two core capabilities:

1.  **High-Performance Training**: Supports efficient training in various modes by connecting Megatron with SGLang;
2.  **Flexible Data Generation**: Enables arbitrary training data generation workflows through custom data generation interfaces and server-based engines.

slime is the RL-framework behind [GLM-4.5](https://z.ai/blog/glm-4.5) and [GLM-4.6](https://z.ai/blog/glm-4.6) and apart from models from Z.ai, we also supports the following models:
- Qwen3 series (Qwen3Next, Qwen3MoE, Qwen3), Qwen2.5 series;
- DeepSeek V3 series (DeepSeek V3, V3.1, DeepSeek R1);
- Llama 3.

## Blogs

- Our vision: [slime: An SGLang-Native Post-Training Framework for RL Scaling](https://lmsys.org/blog/2025-07-09-slime/).
- Our ideas on agentic training: [Agent-Oriented Design: An Asynchronous and Decoupled Framework for Agentic RL](https://www.notion.so/Agent-Oriented-Design-An-Asynchronous-and-Decoupled-Framework-for-Agentic-RL-2278e692d081802cbdd5d37cef76a547)
- v0.1.0 release note: [v0.1.0: Redefining High-Performance RL Training Frameworks](https://thudm.github.io/slime/blogs/release_v0.1.0.html)

## Table of Contents

- [Architecture Overview](#architecture-overview)
- [Quick Start](#quick-start)
- [Projects Built with slime](#projects-built-with-slime)
- [Arguments Walkthrough](#arguments-walkthrough)
- [Developer Guide](#developer-guide)
- [FAQ &amp; Acknowledgements](#faq--acknowledgements)

## Architecture Overview

![arch](./imgs/arch.png)

**Module Descriptions**:

- **training (Megatron)**: Responsible for the main training process, reads data from the Data Buffer, and synchronizes parameters to the rollout module after training.
- **rollout (SGLang + router)**: Generates new data (including rewards/verifier outputs) and stores it in the Data Buffer.
- **data buffer**: A bridge module that manages prompt initialization, custom data, and rollout generation methods.

## Quick Start

For a comprehensive quick start guide covering environment setup, data preparation, training startup, and key code analysis, please refer to:
- [Quick Start Guide](./docs/en/get_started/quick_start.md)

We also provide examples for some use cases not covered in the quick start guide; please check [examples](examples/).

## Projects Built upon slime

slime has powered several novel research projects and production systems. Here are some notable examples:

### ‚ö° TritonForge: Agentic RL Training Framework for Kernel Generation

[**TritonForge**](https://github.com/RLsys-Foundation/TritonForge) leverages slime&#039;s SFT &amp; RL capabilities to train LLMs that automatically generate optimized GPU kernels. By using a two-stage training approach‚Äîsupervised fine-tuning followed by reinforcement learning with multi-turn compilation feedback‚ÄîTritonForge achieves remarkable results in converting PyTorch operations into high-performance Triton kernels.

### üöÄ APRIL: Accelerating RL Training with Active Partial Rollouts

[**APRIL**](https://github.com/RLsys-Foundation/APRIL) introduces a system-level optimization that seamlessly integrates with slime to accelerate the rollout generation phase in RL training. By intelligently over-provisioning requests and actively managing partial completions, APRIL addresses the long-tail generation bottleneck that typically consumes over 90% of RL training time.

These projects showcase slime&#039;s versatility‚Äîfrom training code-generation models to optimizing RL training systems‚Äîmaking it a powerful foundation for both research and production deployments.

## Arguments Walkthrough

Arguments in slime are divided into three categories:

1.  **Megatron arguments**: slime reads all arguments set in Megatron via `PYTHONPATH`. You can configure Megatron by passing arguments like `--tensor-model-parallel-size 2`.
2.  **SGLang arguments**: All arguments for the installed SGLang are supported. These arguments must be prefixed with `--sglang-`. For example, `--mem-fraction-static` should be passed as `--sglang-mem-fraction-static`.
3.  **slime-specific arguments**: Please refer to: [slime/utils/arguments.py](slime/utils/arguments.py)

For complete usage instructions, please refer to the [Usage Documentation](docs/en/get_started/usage.md).

## Developer Guide

- **Contributions are welcome\!** If you have suggestions for new features, performance tuning, or feedback on user experience, feel free to submit an Issue or PR üòä

- Use [pre-commit](https://pre-commit.com/) to ensure code style consistency for your commits:

```bash
apt install pre-commit -y
pre-commit install

# run pre-commit to ensure code style consistency
pre-commit run --all-files --show-diff-on-failure --color=always
```

- For debugging tips, please refer to the [Debugging Guide](docs/en/developer_guide/debug.md)

## FAQ &amp; Acknowledgements

- For frequently asked questions, please see the [Q\&amp;A](docs/en/get_started/qa.md)
- Special thanks to the following projects &amp; communities: SGLang, Megatron‚ÄëLM, mbridge, OpenRLHF, veRL, Pai-Megatron-Patch and others.
- To quote slime, please use:

```bibtext
@misc{slime_github,
  author       = {Zilin Zhu and Chengxing Xie and Xin Lv and slime Contributors},
  title        = {slime: An LLM post-training framework for RL Scaling},
  year         = {2025},
  howpublished = {\url{https://github.com/THUDM/slime}},
  note         = {GitHub repository. Corresponding author: Xin Lv},
  urldate      = {2025-06-19}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[dataease/SQLBot]]></title>
            <link>https://github.com/dataease/SQLBot</link>
            <guid>https://github.com/dataease/SQLBot</guid>
            <pubDate>Sat, 11 Oct 2025 00:04:01 GMT</pubDate>
            <description><![CDATA[üî• Âü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü„ÄÇText-to-SQL Generation via LLMs using RAG.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/dataease/SQLBot">dataease/SQLBot</a></h1>
            <p>üî• Âü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü„ÄÇText-to-SQL Generation via LLMs using RAG.</p>
            <p>Language: Python</p>
            <p>Stars: 3,705</p>
            <p>Forks: 369</p>
            <p>Stars today: 33 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://resource-fit2cloud-com.oss-cn-hangzhou.aliyuncs.com/sqlbot/sqlbot.png&quot; alt=&quot;SQLBot&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;
&lt;h3 align=&quot;center&quot;&gt;Âü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/dataease/SQLBot/releases/latest&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/dataease/SQLBot&quot; alt=&quot;Latest release&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/dataease/SQLBot&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/dataease/SQLBot?color=%231890FF&amp;style=flat-square&quot; alt=&quot;Stars&quot;&gt;&lt;/a&gt;    
  &lt;a href=&quot;https://hub.docker.com/r/dataease/SQLbot&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/dataease/sqlbot?label=downloads&quot; alt=&quot;Download&quot;&gt;&lt;/a&gt;&lt;br/&gt;

&lt;/p&gt;
&lt;hr/&gt;

SQLBot ÊòØ‰∏ÄÊ¨æÂü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü„ÄÇSQLBot ÁöÑ‰ºòÂäøÂåÖÊã¨Ôºö

- **ÂºÄÁÆ±Âç≥Áî®**: Âè™ÈúÄÈÖçÁΩÆÂ§ßÊ®°ÂûãÂíåÊï∞ÊçÆÊ∫êÂç≥ÂèØÂºÄÂêØÈóÆÊï∞‰πãÊóÖÔºåÈÄöËøáÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÁªìÂêàÊù•ÂÆûÁé∞È´òË¥®ÈáèÁöÑ text2sqlÔºõ
- **Êòì‰∫éÈõÜÊàê**: ÊîØÊåÅÂø´ÈÄüÂµåÂÖ•Âà∞Á¨¨‰∏âÊñπ‰∏öÂä°Á≥ªÁªüÔºå‰πüÊîØÊåÅË¢´ n8n„ÄÅMaxKB„ÄÅDify„ÄÅCoze Á≠â AI Â∫îÁî®ÂºÄÂèëÂπ≥Âè∞ÈõÜÊàêË∞ÉÁî®ÔºåËÆ©ÂêÑÁ±ªÂ∫îÁî®Âø´ÈÄüÊã•ÊúâÊô∫ËÉΩÈóÆÊï∞ËÉΩÂäõÔºõ
- **ÂÆâÂÖ®ÂèØÊéß**: Êèê‰æõÂü∫‰∫éÂ∑•‰ΩúÁ©∫Èó¥ÁöÑËµÑÊ∫êÈöîÁ¶ªÊú∫Âà∂ÔºåËÉΩÂ§üÂÆûÁé∞ÁªÜÁ≤íÂ∫¶ÁöÑÊï∞ÊçÆÊùÉÈôêÊéßÂà∂„ÄÇ

## Â∑•‰ΩúÂéüÁêÜ

&lt;img width=&quot;1105&quot; height=&quot;577&quot; alt=&quot;system-arch&quot; src=&quot;https://github.com/user-attachments/assets/462603fc-980b-4b8b-a6d4-a821c070a048&quot; /&gt;

## Âø´ÈÄüÂºÄÂßã

### ÂÆâË£ÖÈÉ®ÁΩ≤

ÂáÜÂ§á‰∏ÄÂè∞ Linux ÊúçÂä°Âô®ÔºåÂÆâË£ÖÂ•Ω [Docker](https://docs.docker.com/get-docker/)ÔºåÊâßË°å‰ª•‰∏ã‰∏ÄÈîÆÂÆâË£ÖËÑöÊú¨Ôºö

```bash
docker run -d \
  --name sqlbot \
  --restart unless-stopped \
  -p 8000:8000 \
  -p 8001:8001 \
  -v ./data/sqlbot/excel:/opt/sqlbot/data/excel \
  -v ./data/sqlbot/file:/opt/sqlbot/data/file \
  -v ./data/sqlbot/images:/opt/sqlbot/images \
  -v ./data/sqlbot/logs:/opt/sqlbot/app/logs \
  -v ./data/postgresql:/var/lib/postgresql/data \
  --privileged=true \
  dataease/sqlbot
```

‰Ω†‰πüÂèØ‰ª•ÈÄöËøá [1Panel Â∫îÁî®ÂïÜÂ∫ó](https://apps.fit2cloud.com/1panel) Âø´ÈÄüÈÉ®ÁΩ≤ SQLBot„ÄÇ

Â¶ÇÊûúÊòØÂÜÖÁΩëÁéØÂ¢ÉÔºå‰Ω†ÂèØ‰ª•ÈÄöËøá [Á¶ªÁ∫øÂÆâË£ÖÂåÖÊñπÂºè](https://community.fit2cloud.com/#/products/sqlbot/downloads) ÈÉ®ÁΩ≤ SQLBot„ÄÇ

### ËÆøÈóÆÊñπÂºè

- Âú®ÊµèËßàÂô®‰∏≠ÊâìÂºÄ: http://&lt;‰Ω†ÁöÑÊúçÂä°Âô®IP&gt;:8000/
- Áî®Êà∑Âêç: admin
- ÂØÜÁ†Å: SQLBot@123456

### ËÅîÁ≥ªÊàë‰ª¨

Â¶Ç‰Ω†ÊúâÊõ¥Â§öÈóÆÈ¢òÔºåÂèØ‰ª•Âä†ÂÖ•Êàë‰ª¨ÁöÑÊäÄÊúØ‰∫§ÊµÅÁæ§‰∏éÊàë‰ª¨‰∫§ÊµÅ„ÄÇ

&lt;img width=&quot;180&quot; height=&quot;180&quot; alt=&quot;contact_me_qr&quot; src=&quot;https://github.com/user-attachments/assets/2594ff29-5426-4457-b051-279855610030&quot; /&gt;

## UI Â±ïÁ§∫

  &lt;tr&gt;
    &lt;img alt=&quot;q&amp;a&quot; src=&quot;https://github.com/user-attachments/assets/55526514-52f3-4cfe-98ec-08a986259280&quot;   /&gt;
  &lt;/tr&gt;

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=dataease/sqlbot&amp;type=Date)](https://www.star-history.com/#dataease/sqlbot&amp;Date)

## È£ûËá¥‰∫ëÊóó‰∏ãÁöÑÂÖ∂‰ªñÊòéÊòüÈ°πÁõÆ

- [DataEase](https://github.com/dataease/dataease/) - ‰∫∫‰∫∫ÂèØÁî®ÁöÑÂºÄÊ∫ê BI Â∑•ÂÖ∑
- [1Panel](https://github.com/1panel-dev/1panel/) - Áé∞‰ª£Âåñ„ÄÅÂºÄÊ∫êÁöÑ Linux ÊúçÂä°Âô®ËøêÁª¥ÁÆ°ÁêÜÈù¢Êùø
- [MaxKB](https://github.com/1panel-dev/MaxKB/) - Âº∫Â§ßÊòìÁî®ÁöÑ‰ºÅ‰∏öÁ∫ßÊô∫ËÉΩ‰ΩìÂπ≥Âè∞
- [JumpServer](https://github.com/jumpserver/jumpserver/) - ÂπøÂèóÊ¨¢ËøéÁöÑÂºÄÊ∫êÂ†°ÂûíÊú∫
- [Cordys CRM](https://github.com/1Panel-dev/CordysCRM) - Êñ∞‰∏Ä‰ª£ÁöÑÂºÄÊ∫ê AI CRM Á≥ªÁªü
- [Halo](https://github.com/halo-dev/halo/) - Âº∫Â§ßÊòìÁî®ÁöÑÂºÄÊ∫êÂª∫Á´ôÂ∑•ÂÖ∑
- [MeterSphere](https://github.com/metersphere/metersphere/) - Êñ∞‰∏Ä‰ª£ÁöÑÂºÄÊ∫êÊåÅÁª≠ÊµãËØïÂ∑•ÂÖ∑

## License

Êú¨‰ªìÂ∫ìÈÅµÂæ™ [FIT2CLOUD Open Source License](LICENSE) ÂºÄÊ∫êÂçèËÆÆÔºåËØ•ËÆ∏ÂèØËØÅÊú¨Ë¥®‰∏äÊòØ GPLv3Ôºå‰ΩÜÊúâ‰∏Ä‰∫õÈ¢ùÂ§ñÁöÑÈôêÂà∂„ÄÇ

‰Ω†ÂèØ‰ª•Âü∫‰∫é SQLBot ÁöÑÊ∫ê‰ª£Á†ÅËøõË°å‰∫åÊ¨°ÂºÄÂèëÔºå‰ΩÜÊòØÈúÄË¶ÅÈÅµÂÆà‰ª•‰∏ãËßÑÂÆöÔºö

- ‰∏çËÉΩÊõøÊç¢Âíå‰øÆÊîπ SQLBot ÁöÑ Logo ÂíåÁâàÊùÉ‰ø°ÊÅØÔºõ
- ‰∫åÊ¨°ÂºÄÂèëÂêéÁöÑË°çÁîü‰ΩúÂìÅÂøÖÈ°ªÈÅµÂÆà GPL V3 ÁöÑÂºÄÊ∫ê‰πâÂä°„ÄÇ

Â¶ÇÈúÄÂïÜ‰∏öÊéàÊùÉÔºåËØ∑ËÅîÁ≥ª support@fit2cloud.com „ÄÇ
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[intruder-io/autoswagger]]></title>
            <link>https://github.com/intruder-io/autoswagger</link>
            <guid>https://github.com/intruder-io/autoswagger</guid>
            <pubDate>Sat, 11 Oct 2025 00:04:00 GMT</pubDate>
            <description><![CDATA[Autoswagger by Intruder - detect API auth weaknesses]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/intruder-io/autoswagger">intruder-io/autoswagger</a></h1>
            <p>Autoswagger by Intruder - detect API auth weaknesses</p>
            <p>Language: Python</p>
            <p>Stars: 1,470</p>
            <p>Forks: 139</p>
            <p>Stars today: 32 stars today</p>
            <h2>README</h2><pre># [Autoswagger](https://www.intruder.io/research/broken-authorization-apis-autoswagger) by [Intruder](https://intruder.io/)
&lt;a href=&quot;https://intruder.io/&quot;&gt;
  &lt;img width=&quot;966&quot; alt=&quot;output&quot; src=&quot;https://github.com/user-attachments/assets/e502abaf-426c-4fab-ad60-d7b5dcd730d8&quot; /&gt;
&lt;/a&gt;
&lt;br&gt;  
&lt;br&gt;  

**[Autoswagger](https://www.intruder.io/research/broken-authorization-apis-autoswagger)** is a command-line tool designed to discover, parse, and test for unauthenticated endpoints using **Swagger/OpenAPI** documentation. It helps identify potential security issues in unprotected endpoints of APIs, such as PII leaks and common secret exposures.

**Please note that this initial release of Autoswagger is by no means complete, and there are some types of specification which the tool does not currently handle. Please feel free to use it as you wish, and extend its detection capabilities or add detection regexes to cover your specific use-case!**

---

## Table of Contents
1. [Introduction](#introduction)
2. [Key Features](#key-features)
3. [Installation &amp; Usage](#installation--usage)
4. [Discovery Phases](#discovery-phases)
5. [Endpoint Testing](#endpoint-testing)
6. [PII Detection](#pii-detection)
7. [Output Examples](#output)
8. [Stats &amp; Reporting](#stats--reporting)
9. [Acknowledgments](#acknowledgments)

---

## Introduction

Autoswagger automates the process of finding **OpenAPI/Swagger** specifications, extracting API endpoints, and systematically testing them for **PII** exposure, **secrets**, and large or interesting responses. It leverages **Presidio** for PII recognition and **regex** for sensitive key/token detection.

---

## Key Features

- **Multiple Discovery Phases**  
  Discovers OpenAPI specs in three ways:
  1. **Direct Spec**: If a full URL with a path ending in `.json`, `.yaml`, or `.yml` is provided, parse that file directly.  
  2. **Swagger UI**: Parse known paths of Swagger UI (e.g. `/swagger-ui.html`), and extract spec from HTML or JavaScript.  
  3. **Direct Spec by Bruteforce**: Attempt discovery using common OpenAPI schema locations (`/swagger.json`, `/openapi.json`, etc.). Only attempt this if 1. and 2. did not yield a result.

- **Parallel Endpoint Testing**  
  Multi-threaded concurrent testing of many endpoints, respecting a configurable rate limit (`-rate`).

- **Brute-Force of Parameter Values**  
  If `-b` or `--brute` is used, try using various data types with a few example values in an attempt to bypass parameter-specific validations.

- **Presidio PII Detection**  
  Check output for phone numbers, emails, addresses, and names (with context validation to reduce false positives). Also parse CSV rows and naive ‚Äúkey: value‚Äù lines.

- **Secrets Detection**  
  Leverages a set of regex patterns to detect tokens, keys, and debugging artifacts (like environment variables).

- **Command Line or JSON Output**  
  In default mode, displays results in a table. With `-json`, output a JSON structure. `-product` mode filters output to only show those that contain PII, secrets, or large responses.


---

## Installation &amp; Usage

1. **Clone** or **download** the repository containing Autoswagger.
   ```bash
   git clone git@github.com:intruder-io/autoswagger.git
   ```


2. **Install dependencies** (e.g., using Python 3.7+):
   ```bash
   pip install -r requirements.txt
   ```

   (It&#039;s recommended to use a virtual environment for this: `python3 -m venv venv;source venv/bin/activate`)

3. **Check installation, show help:**
  ```bash
  python3 autoswagger.py -h
  ```



## Flags 

| Flag                 | Description                                                                                                 |
|----------------------|-------------------------------------------------------------------------------------------------------------|
| `urls`               | List of base URLs or direct spec URLs.                                                                       |
| `-v, --verbose`      | Enables verbose logging. Creates a log file under `~/.autoswagger/logs`.                                     |
| `-risk`              | Includes non-GET methods (POST, PUT, PATCH, DELETE) in testing.                                              |
| `-all`               | Includes 200 and 404 endpoints in output (excludes 401/403).                                                 |
| `-product`           | Outputs only endpoints with PII or large responses, in JSON format.                                          |
| `-stats`             | Displays scan statistics (e.g. requests, RPS, hosts with PII).                                               |
| `-rate &lt;N&gt;`          | Throttles requests to N requests per second. Default is 30. Use 0 to disable rate limiting.                  |
| `-b, --brute`        | Enables brute-forcing of parameter values (multiple test combos).                                            |
| `-json`              | Outputs results in JSON format instead of a Rich table in default mode.                                      |


## Help

```


      /   | __  __/ /_____  ______      ______ _____ _____ ____  _____
     / /| |/ / / / __/ __ \/ ___/ | /| / / __ `/ __ `/ __ `/ _ \/ ___/
    / ___ / /_/ / /_/ /_/ (__  )| |/ |/ / /_/ / /_/ / /_/ /  __/ /
    /_/  |_\__,_/\__/\____/____/ |__/|__/_\__,_/\__, /\__, /\___/_/
                                              /____//____/
                              https://intruder.io
                          Find unauthenticated endpoints

usage: autoswagger.py [-h] [-v] [-risk] [-all] [-product] [-stats] [-rate RATE] [-b] [-json] [urls ...]

Autoswagger: Detect unauthenticated access control issues via Swagger/OpenAPI documentation.

positional arguments:
  urls           Base URL(s) or spec URL(s) of the target API(s)

options:
  -h, --help     show this help message and exit
  -v, --verbose  Enable verbose output
  -risk          Include non-GET requests in testing
  -all           Include all HTTP status codes in the results, excluding 401 and 403
  -product       Output all endpoints in JSON, flagging those that contain PII or have large responses.
  -stats         Display scan statistics. Included in JSON if -product or -json is used.
  -rate RATE     Set the rate limit in requests per second (default: 30). Use 0 to disable rate limiting.
  -b, --brute    Enable exhaustive testing of parameter values.
  -json          Output results in JSON format in default mode.

Example usage:
  python autoswagger.py https://api.example.com -v

```
## Discovery Phases

1. **Direct Spec**  
   If a provided URL ends with `.json/.yaml/.yml`, Autoswagger **directly** attempts to parse the OpenAPI schema.

2. **Swagger-UI Detection**  
   - Tries known UI paths (e.g., `/swagger-ui.html`).
   - If found, parses the HTML or local JavaScript files for a `swagger.json` or `openapi.json`.
   - Can detect embedded configs like `window.swashbuckleConfig`.

3. **Direct Spec by Bruteforce**  
   - If no spec is found so far, Autoswagger attempts a list of default endpoints like `/swagger.json`, `/openapi.json`, etc.
   - Stops when a valid spec is discovered or none are found.

---

## Endpoint Testing

1. **Collect Endpoints**  
   After loading a spec, Autoswagger extracts each path and method under the `paths` key.

2. **HTTP Methods**  
   - By default, tests `GET` only.  
   - Use `-risk` to include other methods (`POST`, `PUT`, `PATCH`, `DELETE`).

3. **Parameter Values**  
   - Fill path/query parameters with defaults or values to enumerate.  
   - Optionally builds request bodies from the spec‚Äôs `requestBody` (OpenAPI 3) or body parameters (Swagger 2).

4. **Rate Limiting &amp; Concurrency**  
   - Supports threading with a cap on requests per second (`-rate`).  
   - Each endpoint is tested in a dedicated job.

5. **Response Analysis**  
   - Decodes responses, checks for PII, secrets, and large content.  
   - Logs relevant findings.

---

## PII Detection

1. **Presidio-Based Analysis**  
   - Searches for phone numbers, emails, addresses, names.  
   - Context-based scanning (e.g., CSV headers, key-value lines).

2. **Secrets &amp; Debug Info**  
   - TruffleHog-like regex checks for API keys, tokens, environment variables.  
   - Merges any matches into the PII data structure for final reporting.

3. **Large Response Check**  
   - Flags responses with 100+ JSON elements or large XML structures as ‚Äúinteresting.‚Äù  
   - Also checks raw size threshold (e.g., &gt;100k bytes).

---

## Output

By default, output is shown in a table.

- `-json` produces JSON objects, grouping results by endpoint.
- `-product` filters down to only ‚Äúinteresting‚Äù endpoints (PII, large responses and responses with secrets).

---

## Interpreting Results

For most use cases, interpreting results involves looking at the output (endpoints resulting in Status Code 200s), and paying particular attention to endpoints which are marked as &#039;PII or Secret Detected&#039;. These endpoints are the ones that contain impactful exposures, but they should be manually checked to confirm. You may also wish to look at other 200s that do not contain PII, and determine whether it&#039;s intended for these endpoints to be public or not.

Simple GET endpoints can be triaged using command line tools like curl, but we would recommend using your usual API testing suite (tools such as Postman or Burp Suite) to replay requests and read responses to confirm whether an exposure is present.

---

## Stats &amp; Reporting

- `-stats` appends or prints overall statistics, such as:
  - Hosts with valid specs
  - Hosts with PII
  - Total requests sent, average RPS
  - Percentage of endpoints responding with 2xx or 4xx
  - Shown in either a Rich table in default mode or embedded in JSON if `-json` or `-product` is used.

---

## Acknowledgments

Autoswagger is maintained and owned by **[Intruder](https://intruder.io/)**. It was primarily developed by Cale Anderson

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mem0ai/mem0]]></title>
            <link>https://github.com/mem0ai/mem0</link>
            <guid>https://github.com/mem0ai/mem0</guid>
            <pubDate>Sat, 11 Oct 2025 00:03:59 GMT</pubDate>
            <description><![CDATA[Universal memory layer for AI Agents; Announcing OpenMemory MCP - local and secure memory management.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mem0ai/mem0">mem0ai/mem0</a></h1>
            <p>Universal memory layer for AI Agents; Announcing OpenMemory MCP - local and secure memory management.</p>
            <p>Language: Python</p>
            <p>Stars: 41,076</p>
            <p>Forks: 4,366</p>
            <p>Stars today: 96 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/mem0ai/mem0&quot;&gt;
    &lt;img src=&quot;docs/images/banner-sm.png&quot; width=&quot;800px&quot; alt=&quot;Mem0 - The Memory Layer for Personalized AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot; style=&quot;display: flex; justify-content: center; gap: 20px; align-items: center;&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/11194&quot; target=&quot;blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/11194&quot; alt=&quot;mem0ai%2Fmem0 | Trendshift&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://mem0.ai&quot;&gt;Learn more&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://mem0.dev/DiG&quot;&gt;Join Discord&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://mem0.dev/demo&quot;&gt;Demo&lt;/a&gt;
  ¬∑
  &lt;a href=&quot;https://mem0.dev/openmemory&quot;&gt;OpenMemory&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://mem0.dev/DiG&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Discord-%235865F2.svg?&amp;logo=discord&amp;logoColor=white&quot; alt=&quot;Mem0 Discord&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://pepy.tech/project/mem0ai&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/dm/mem0ai&quot; alt=&quot;Mem0 PyPI - Downloads&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/mem0ai/mem0&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/commit-activity/m/mem0ai/mem0?style=flat-square&quot; alt=&quot;GitHub commit activity&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://pypi.org/project/mem0ai&quot; target=&quot;blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/mem0ai?color=%2334D058&amp;label=pypi%20package&quot; alt=&quot;Package version&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.npmjs.com/package/mem0ai&quot; target=&quot;blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/npm/v/mem0ai&quot; alt=&quot;Npm package&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.ycombinator.com/companies/mem0&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Y%20Combinator-S24-orange?style=flat-square&quot; alt=&quot;Y Combinator S24&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://mem0.ai/research&quot;&gt;&lt;strong&gt;üìÑ Building Production-Ready AI Agents with Scalable Long-Term Memory ‚Üí&lt;/strong&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;‚ö° +26% Accuracy vs. OpenAI Memory ‚Ä¢ üöÄ 91% Faster ‚Ä¢ üí∞ 90% Fewer Tokens&lt;/strong&gt;
&lt;/p&gt;

##  üî• Research Highlights
- **+26% Accuracy** over OpenAI Memory on the LOCOMO benchmark
- **91% Faster Responses** than full-context, ensuring low-latency at scale
- **90% Lower Token Usage** than full-context, cutting costs without compromise
- [Read the full paper](https://mem0.ai/research)

# Introduction

[Mem0](https://mem0.ai) (&quot;mem-zero&quot;) enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions. It remembers user preferences, adapts to individual needs, and continuously learns over time‚Äîideal for customer support chatbots, AI assistants, and autonomous systems.

### Key Features &amp; Use Cases

**Core Capabilities:**
- **Multi-Level Memory**: Seamlessly retains User, Session, and Agent state with adaptive personalization
- **Developer-Friendly**: Intuitive API, cross-platform SDKs, and a fully managed service option

**Applications:**
- **AI Assistants**: Consistent, context-rich conversations
- **Customer Support**: Recall past tickets and user history for tailored help
- **Healthcare**: Track patient preferences and history for personalized care
- **Productivity &amp; Gaming**: Adaptive workflows and environments based on user behavior

## üöÄ Quickstart Guide &lt;a name=&quot;quickstart&quot;&gt;&lt;/a&gt;

Choose between our hosted platform or self-hosted package:

### Hosted Platform

Get up and running in minutes with automatic updates, analytics, and enterprise security.

1. Sign up on [Mem0 Platform](https://app.mem0.ai)
2. Embed the memory layer via SDK or API keys

### Self-Hosted (Open Source)

Install the sdk via pip:

```bash
pip install mem0ai
```

Install sdk via npm:
```bash
npm install mem0ai
```

### Basic Usage

Mem0 requires an LLM to function, with `gpt-4o-mini` from OpenAI as the default. However, it supports a variety of LLMs; for details, refer to our [Supported LLMs documentation](https://docs.mem0.ai/components/llms/overview).

First step is to instantiate the memory:

```python
from openai import OpenAI
from mem0 import Memory

openai_client = OpenAI()
memory = Memory()

def chat_with_memories(message: str, user_id: str = &quot;default_user&quot;) -&gt; str:
    # Retrieve relevant memories
    relevant_memories = memory.search(query=message, user_id=user_id, limit=3)
    memories_str = &quot;\n&quot;.join(f&quot;- {entry[&#039;memory&#039;]}&quot; for entry in relevant_memories[&quot;results&quot;])

    # Generate Assistant response
    system_prompt = f&quot;You are a helpful AI. Answer the question based on query and memories.\nUser Memories:\n{memories_str}&quot;
    messages = [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_prompt}, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: message}]
    response = openai_client.chat.completions.create(model=&quot;gpt-4o-mini&quot;, messages=messages)
    assistant_response = response.choices[0].message.content

    # Create new memories from the conversation
    messages.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: assistant_response})
    memory.add(messages, user_id=user_id)

    return assistant_response

def main():
    print(&quot;Chat with AI (type &#039;exit&#039; to quit)&quot;)
    while True:
        user_input = input(&quot;You: &quot;).strip()
        if user_input.lower() == &#039;exit&#039;:
            print(&quot;Goodbye!&quot;)
            break
        print(f&quot;AI: {chat_with_memories(user_input)}&quot;)

if __name__ == &quot;__main__&quot;:
    main()
```

For detailed integration steps, see the [Quickstart](https://docs.mem0.ai/quickstart) and [API Reference](https://docs.mem0.ai/api-reference).

## üîó Integrations &amp; Demos

- **ChatGPT with Memory**: Personalized chat powered by Mem0 ([Live Demo](https://mem0.dev/demo))
- **Browser Extension**: Store memories across ChatGPT, Perplexity, and Claude ([Chrome Extension](https://chromewebstore.google.com/detail/onihkkbipkfeijkadecaafbgagkhglop?utm_source=item-share-cb))
- **Langgraph Support**: Build a customer bot with Langgraph + Mem0 ([Guide](https://docs.mem0.ai/integrations/langgraph))
- **CrewAI Integration**: Tailor CrewAI outputs with Mem0 ([Example](https://docs.mem0.ai/integrations/crewai))

## üìö Documentation &amp; Support

- Full docs: https://docs.mem0.ai
- Community: [Discord](https://mem0.dev/DiG) ¬∑ [Twitter](https://x.com/mem0ai)
- Contact: founders@mem0.ai

## Citation

We now have a paper you can cite:

```bibtex
@article{mem0,
  title={Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory},
  author={Chhikara, Prateek and Khant, Dev and Aryan, Saket and Singh, Taranjeet and Yadav, Deshraj},
  journal={arXiv preprint arXiv:2504.19413},
  year={2025}
}
```

## ‚öñÔ∏è License

Apache 2.0 ‚Äî see the [LICENSE](https://github.com/mem0ai/mem0/blob/main/LICENSE) file for details.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[hiyouga/LLaMA-Factory]]></title>
            <link>https://github.com/hiyouga/LLaMA-Factory</link>
            <guid>https://github.com/hiyouga/LLaMA-Factory</guid>
            <pubDate>Sat, 11 Oct 2025 00:03:58 GMT</pubDate>
            <description><![CDATA[Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/hiyouga/LLaMA-Factory">hiyouga/LLaMA-Factory</a></h1>
            <p>Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)</p>
            <p>Language: Python</p>
            <p>Stars: 59,934</p>
            <p>Forks: 7,354</p>
            <p>Stars today: 56 stars today</p>
            <h2>README</h2><pre>![# LLaMA Factory](assets/logo.png)

[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social)](https://github.com/hiyouga/LLaMA-Factory/stargazers)
[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory)](https://github.com/hiyouga/LLaMA-Factory/commits/main)
[![GitHub contributors](https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange)](https://github.com/hiyouga/LLaMA-Factory/graphs/contributors)
[![GitHub workflow](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg)](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml)
[![PyPI](https://img.shields.io/pypi/v/llamafactory)](https://pypi.org/project/llamafactory/)
[![Citation](https://img.shields.io/badge/citation-840-green)](https://scholar.google.com/scholar?cites=12620864006390196564)
[![Docker Pulls](https://img.shields.io/docker/pulls/hiyouga/llamafactory)](https://hub.docker.com/r/hiyouga/llamafactory/tags)

[![Twitter](https://img.shields.io/twitter/follow/llamafactory_ai)](https://twitter.com/llamafactory_ai)
[![Discord](assets/thirdparty/discord.svg)](https://discord.gg/rKfvV9r9FK)

[![Open in Colab](assets/thirdparty/colab.svg)](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)
[![Open in DSW](assets/thirdparty/dsw.svg)](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory)
[![Open in Lab4ai](assets/thirdparty/lab4ai.svg)](https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&amp;utm_source=LLaMA-Factory)
[![Open in Online](assets/thirdparty/online.svg)](https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory)
[![Open in Spaces](https://img.shields.io/badge/ü§ó-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/hiyouga/LLaMA-Board)
[![Open in Studios](https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue)](https://modelscope.cn/studios/hiyouga/LLaMA-Board)
[![Open in Novita](https://img.shields.io/badge/Novita-Deploy%20Template-blue)](https://novita.ai/templates-library/105981?sharer=88115474-394e-4bda-968e-b88e123d0c47)

### Used by [Amazon](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/), [NVIDIA](https://developer.nvidia.com/rtx/ai-toolkit), [Aliyun](https://help.aliyun.com/zh/pai/use-cases/fine-tune-a-llama-3-model-with-llama-factory), etc.

&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;

### Supporters ‚ù§Ô∏è

| &lt;div style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://warp.dev/llama-factory&quot;&gt;&lt;img alt=&quot;Warp sponsorship&quot; width=&quot;400&quot; src=&quot;assets/sponsors/warp.jpg&quot;&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://warp.dev/llama-factory&quot; style=&quot;font-size:larger;&quot;&gt;Warp, the agentic terminal for developers&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://warp.dev/llama-factory&quot;&gt;Available for MacOS, Linux, &amp; Windows&lt;/a&gt; | &lt;a href=&quot;https://serpapi.com&quot;&gt;&lt;img alt=&quot;SerpAPI sponsorship&quot; width=&quot;250&quot; src=&quot;assets/sponsors/serpapi.svg&quot;&gt; &lt;/a&gt; |
| ---- | ---- |

----

### Easily fine-tune 100+ large language models with zero-code [CLI](#quickstart) and [Web UI](#fine-tuning-with-llama-board-gui-powered-by-gradio)

![GitHub Trend](https://trendshift.io/api/badge/repositories/4535)

&lt;/div&gt;

üëã Join our [WeChat](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/main.jpg), [NPU](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/npu.jpg), [Lab4AI](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/lab4ai.jpg), [LLaMA Factory Online](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/online.jpg) user group.

\[ English | [‰∏≠Êñá](README_zh.md) \]

**Fine-tuning a large language model can be easy as...**

https://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e

Choose your path:

- **Documentation (WIP)**: https://llamafactory.readthedocs.io/en/latest/
- **Documentation (AMD GPU)**: https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html
- **Colab (free)**: https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing
- **Local machine**: Please refer to [usage](#getting-started)
- **PAI-DSW (free trial)**: https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory
- **Alaya NeW (cloud GPU deal)**: https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory
- **Official Course**: https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&amp;utm_source=LLaMA-Factory
- **LLaMA Factory Online**: https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory

&gt; [!NOTE]
&gt; Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.

## Table of Contents

- [Features](#features)
- [Blogs](#blogs)
- [Changelog](#changelog)
- [Supported Models](#supported-models)
- [Supported Training Approaches](#supported-training-approaches)
- [Provided Datasets](#provided-datasets)
- [Requirement](#requirement)
- [Getting Started](#getting-started)
  - [Installation](#installation)
  - [Data Preparation](#data-preparation)
  - [Quickstart](#quickstart)
  - [Fine-Tuning with LLaMA Board GUI](#fine-tuning-with-llama-board-gui-powered-by-gradio)
  - [LLaMA Factory Online](#llama-factory-online)
  - [Build Docker](#build-docker)
  - [Deploy with OpenAI-style API and vLLM](#deploy-with-openai-style-api-and-vllm)
  - [Download from ModelScope Hub](#download-from-modelscope-hub)
  - [Download from Modelers Hub](#download-from-modelers-hub)
  - [Use W&amp;B Logger](#use-wb-logger)
  - [Use SwanLab Logger](#use-swanlab-logger)
- [Projects using LLaMA Factory](#projects-using-llama-factory)
- [License](#license)
- [Citation](#citation)
- [Acknowledgement](#acknowledgement)

## Features

- **Various models**: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.
- **Integrated methods**: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.
- **Scalable resources**: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.
- **Advanced algorithms**: [GaLore](https://github.com/jiaweizzhao/GaLore), [BAdam](https://github.com/Ledzy/BAdam), [APOLLO](https://github.com/zhuhanqing/APOLLO), [Adam-mini](https://github.com/zyushun/Adam-mini), [Muon](https://github.com/KellerJordan/Muon), [OFT](https://github.com/huggingface/peft/tree/main/src/peft/tuners/oft), DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.
- **Practical tricks**: [FlashAttention-2](https://github.com/Dao-AILab/flash-attention), [Unsloth](https://github.com/unslothai/unsloth), [Liger Kernel](https://github.com/linkedin/Liger-Kernel), RoPE scaling, NEFTune and rsLoRA.
- **Wide tasks**: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.
- **Experiment monitors**: LlamaBoard, TensorBoard, Wandb, MLflow, [SwanLab](https://github.com/SwanHubX/SwanLab), etc.
- **Faster inference**: OpenAI-style API, Gradio UI and CLI with [vLLM worker](https://github.com/vllm-project/vllm) or [SGLang worker](https://github.com/sgl-project/sglang).

### Day-N Support for Fine-Tuning Cutting-Edge Models

| Support Date | Model Name                                                           |
| ------------ | -------------------------------------------------------------------- |
| Day 0        | Qwen3 / Qwen2.5-VL / Gemma 3 / GLM-4.1V / InternLM 3 / MiniCPM-o-2.6 |
| Day 1        | Llama 3 / GLM-4 / Mistral Small / PaliGemma2 / Llama 4               |

## Blogs

- üí° [Easy Dataset √ó LLaMA Factory: Enabling LLMs to Efficiently Learn Domain Knowledge](https://buaa-act.feishu.cn/wiki/GVzlwYcRFiR8OLkHbL6cQpYin7g) (English)
- [Fine-tune a mental health LLM using LLaMA-Factory](https://www.lab4ai.cn/project/detail?id=25cce32ec131497b9e06a93336a0817f&amp;type=project&amp;utm_source=LLaMA-Factory) (Chinese)
- [Fine-tune GPT-OSS for Role-Playing using LLaMA-Factory](https://docs.llamafactory.com.cn/docs/documents/best-practice/gptroleplay/?utm_source=LLaMA-Factory) (Chinese)
- [A One-Stop Code-Free Model Reinforcement Learning and Deployment Platform based on LLaMA-Factory and EasyR1](https://aws.amazon.com/cn/blogs/china/building-llm-model-hub-based-on-llamafactory-and-easyr1/) (Chinese)
- [How Apoidea Group enhances visual information extraction from banking documents with multimodal models using LLaMA-Factory on Amazon SageMaker HyperPod](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/) (English)

&lt;details&gt;&lt;summary&gt;All Blogs&lt;/summary&gt;

- [Fine-tune Llama3.1-70B for Medical Diagnosis using LLaMA-Factory](https://docs.alayanew.com/docs/documents/bestPractice/bigModel/llama70B/?utm_source=LLaMA-Factory) (Chinese)
- [Fine-tune Qwen2.5-VL for Autonomous Driving using LLaMA-Factory](https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory) (Chinese)
- [LLaMA Factory: Fine-tuning the DeepSeek-R1-Distill-Qwen-7B Model for News Classifier](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b) (Chinese)
- [A One-Stop Code-Free Model Fine-Tuning \&amp; Deployment Platform based on SageMaker and LLaMA-Factory](https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/) (Chinese)
- [LLaMA Factory Multi-Modal Fine-Tuning Practice: Fine-Tuning Qwen2-VL for Personal Tourist Guide](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl) (Chinese)
- [LLaMA Factory: Fine-tuning Llama3 for Role-Playing](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory) (Chinese)

&lt;/details&gt;

## Changelog

[25/08/22] We supported **[OFT](https://arxiv.org/abs/2306.07280)** and **[OFTv2](https://arxiv.org/abs/2506.19847)**. See [examples](examples/README.md) for usage.

[25/08/20] We supported fine-tuning the **[Intern-S1-mini](https://huggingface.co/internlm/Intern-S1-mini)** models. See [PR #8976](https://github.com/hiyouga/LLaMA-Factory/pull/8976) to get started.

[25/08/06] We supported fine-tuning the **[GPT-OSS](https://github.com/openai/gpt-oss)** models. See [PR #8826](https://github.com/hiyouga/LLaMA-Factory/pull/8826) to get started.

&lt;details&gt;&lt;summary&gt;Full Changelog&lt;/summary&gt;

[25/07/02] We supported fine-tuning the **[GLM-4.1V-9B-Thinking](https://github.com/THUDM/GLM-4.1V-Thinking)** model.

[25/04/28] We supported fine-tuning the **[Qwen3](https://qwenlm.github.io/blog/qwen3/)** model family.

[25/04/21] We supported the **[Muon](https://github.com/KellerJordan/Muon)** optimizer. See [examples](examples/README.md) for usage. Thank [@tianshijing](https://github.com/tianshijing)&#039;s PR.

[25/04/16] We supported fine-tuning the **[InternVL3](https://huggingface.co/OpenGVLab/InternVL3-8B)** model. See [PR #7258](https://github.com/hiyouga/LLaMA-Factory/pull/7258) to get started.

[25/04/14] We supported fine-tuning the **[GLM-Z1](https://huggingface.co/THUDM/GLM-Z1-9B-0414)** and **[Kimi-VL](https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct)** models.

[25/04/06] We supported fine-tuning the **[Llama 4](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)** model. See [PR #7611](https://github.com/hiyouga/LLaMA-Factory/pull/7611) to get started.

[25/03/31] We supported fine-tuning the **[Qwen2.5 Omni](https://qwenlm.github.io/blog/qwen2.5-omni/)** model. See [PR #7537](https://github.com/hiyouga/LLaMA-Factory/pull/7537) to get started.

[25/03/15] We supported **[SGLang](https://github.com/sgl-project/sglang)** as inference backend. Try `infer_backend: sglang` to accelerate inference.

[25/03/12] We supported fine-tuning the **[Gemma 3](https://huggingface.co/blog/gemma3)** model.

[25/02/24] Announcing **[EasyR1](https://github.com/hiyouga/EasyR1)**, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.

[25/02/11] We supported saving the **[Ollama](https://github.com/ollama/ollama)** modelfile when exporting the model checkpoints. See [examples](examples/README.md) for usage.

[25/02/05] We supported fine-tuning the **[Qwen2-Audio](Qwen/Qwen2-Audio-7B-Instruct)** and **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** on audio understanding tasks.

[25/01/31] We supported fine-tuning the **[DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)** and **[Qwen2.5-VL](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)** models.

[25/01/15] We supported **[APOLLO](https://arxiv.org/abs/2412.05270)** optimizer. See [examples](examples/README.md) for usage.

[25/01/14] We supported fine-tuning the **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** and **[MiniCPM-V-2.6](https://huggingface.co/openbmb/MiniCPM-V-2_6)** models. Thank [@BUAADreamer](https://github.com/BUAADreamer)&#039;s PR.

[25/01/14] We supported fine-tuning the **[InternLM 3](https://huggingface.co/collections/internlm/)** models. Thank [@hhaAndroid](https://github.com/hhaAndroid)&#039;s PR.

[25/01/10] We supported fine-tuning the **[Phi-4](https://huggingface.co/microsoft/phi-4)** model.

[24/12/21] We supported using **[SwanLab](https://github.com/SwanHubX/SwanLab)** for experiment tracking and visualization. See [this section](#use-swanlab-logger) for details.

[24/11/27] We supported fine-tuning the **[Skywork-o1](https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B)** model and the **[OpenO1](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)** dataset.

[24/10/09] We supported downloading pre-trained models and datasets from the **[Modelers Hub](https://modelers.cn/models)**. See [this tutorial](#download-from-modelers-hub) for usage.

[24/09/19] We supported fine-tuning the **[Qwen2.5](https://qwenlm.github.io/blog/qwen2.5/)** models.

[24/08/30] We supported fine-tuning the **[Qwen2-VL](https://qwenlm.github.io/blog/qwen2-vl/)** models. Thank [@simonJJJ](https://github.com/simonJJJ)&#039;s PR.

[24/08/27] We supported **[Liger Kernel](https://github.com/linkedin/Liger-Kernel)**. Try `enable_liger_kernel: true` for efficient training.

[24/08/09] We supported **[Adam-mini](https://github.com/zyushun/Adam-mini)** optimizer. See [examples](examples/README.md) for usage. Thank [@relic-yuexi](https://github.com/relic-yuexi)&#039;s PR.

[24/07/04] We supported [contamination-free packed training](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing). Use `neat_packing: true` to activate it. Thank [@chuan298](https://github.com/chuan298)&#039;s PR.

[24/06/16] We supported **[PiSSA](https://arxiv.org/abs/2404.02948)** algorithm. See [examples](examples/README.md) for usage.

[24/06/07] We supported fine-tuning the **[Qwen2](https://qwenlm.github.io/blog/qwen2/)** and **[GLM-4](https://github.com/THUDM/GLM-4)** models.

[24/05/26] We supported **[SimPO](https://arxiv.org/abs/2405.14734)** algorithm for preference learning. See [examples](examples/README.md) for usage.

[24/05/20] We supported fine-tuning the **PaliGemma** series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with `paligemma` template for chat completion.

[24/05/18] We supported **[KTO](https://arxiv.org/abs/2402.01306)** algorithm for preference learning. See [examples](examples/README.md) for usage.

[24/05/14] We supported training and inference on the Ascend NPU devices. Check [installation](#installation) section for details.

[24/04/26] We supported fine-tuning the **LLaVA-1.5** multimodal LLMs. See [examples](examples/README.md) for usage.

[24/04/22] We provided a **[Colab notebook](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)** for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check [Llama3-8B-Chinese-Chat](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat) and [Llama3-Chinese](https://huggingface.co/zhichen/Llama3-Chinese) for details.

[24/04/21] We supported **[Mixture-of-Depths](https://arxiv.org/abs/2404.02258)** according to [AstraMindAI&#039;s implementation](https://github.com/astramind-ai/Mixture-of-depths). See [examples](examples/README.md) for usage.

[24/04/16] We supported **[BAdam](https://arxiv.org/abs/2404.02827)** optimizer. See [examples](examples/README.md) for usage.

[24/04/16] We supported **[unsloth](https://github.com/unslothai/unsloth)**&#039;s long-sequence training (Llama-2-7B-56k within 24GB). It achieves **117%** speed and **50%** memory compared with FlashAttention-2, more benchmarks can be found in [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison).

[24/03/31] We supported **[ORPO](https://arxiv.org/abs/2403.07691)**. See [examples](examples/README.md) for usage.

[24/03/21] Our paper &quot;[LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models](https://arxiv.org/abs/2403.13372)&quot; is available at arXiv!

[24/03/20] We supported **FSDP+QLoRA** that fine-tunes a 70B model on 2x24GB GPUs. See [examples](examples/README.md) for usage.

[24/03/13] We supported **[LoRA+](https://arxiv.org/abs/2402.12354)**. See [examples](examples/README.md) for usage.

[24/03/07] We supported **[GaLore](https://arxiv.org/abs/2403.03507)** optimizer. See [examples](examples/README.md) for usage.

[24/03/07] We integrated **[vLLM](https://github.com/vllm-project/vllm)** for faster and concurrent inference. Try `infer_backend: vllm` to enjoy **270%** inference speed.

[24/02/28] We supported weight-decomposed LoRA (**[DoRA](https://arxiv.org/abs/2402.09353)**). Try `use_dora: true` to activate DoRA training.

[24/02/15] We supported **block expansion** proposed by [LLaMA Pro](https://github.com/TencentARC/LLaMA-Pro). See [examples](examples/README.md) for usage.

[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this [blog post](https://qwenlm.github.io/blog/qwen1.5/) for details.

[24/01/18] We supported **agent tuning** for most models, equipping model with tool using abilities by fine-tuning with `dataset: glaive_toolcall_en`.

[23/12/23] We supported **[unsloth](https://github.com/unslothai/unsloth)**&#039;s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try `use_unsloth: true` argument to activate unsloth patch. It achieves **170%** speed in our benchmark, check [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison) for details.

[23/12/12] We supported fine-tuning the latest MoE model **[Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)** in our framework. See hardware requirement [here](#hardware-requirement).

[23/12/01] We supported downloading pre-trained models and datasets from the **[ModelScope Hub](https://modelscope.cn/models)**. See [this tutorial](#download-from-modelscope-hub) for usage.

[23/10/21] We supported **[NEFTune](https://arxiv.org/abs/2310.05914)** trick for fine-tuning. Try `neftune_noise_alpha: 5` argument to activate NEFTune.

[23/09/27] We supported **$S^2$-Attn** proposed by [LongLoRA](https://github.com/dvlab-research/LongLoRA) for the LLaMA models. Try `shift_attn: true` argument to enable shift short attention.

[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See [examples](examples/README.md) for usage.

[23/09/10] We supported **[FlashAttention-2](https://github.com/Dao-AILab/flash-attention)**. Try `flash_attn: fa2` argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.

[23/08/12] We supported **RoPE scaling** to extend the context length of the LLaMA models. Try `rope_scaling: linear` argument in training and `rope_scaling: dynamic` argument at inference to extrapo

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[OpenMind/OM1]]></title>
            <link>https://github.com/OpenMind/OM1</link>
            <guid>https://github.com/OpenMind/OM1</guid>
            <pubDate>Sat, 11 Oct 2025 00:03:57 GMT</pubDate>
            <description><![CDATA[Modular AI runtime for robots]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenMind/OM1">OpenMind/OM1</a></h1>
            <p>Modular AI runtime for robots</p>
            <p>Language: Python</p>
            <p>Stars: 702</p>
            <p>Forks: 141</p>
            <p>Stars today: 31 stars today</p>
            <h2>README</h2><pre>![OM_Banner_X2 (1)](https://github.com/user-attachments/assets/853153b7-351a-433d-9e1a-d257b781f93c)

&lt;p align=&quot;center&quot;&gt;  &lt;a href=&quot;https://arxiv.org/abs/2412.18588&quot;&gt;Technical Paper&lt;/a&gt; |  &lt;a href=&quot;https://docs.openmind.org/&quot;&gt;Documentation&lt;/a&gt; |  &lt;a href=&quot;https://x.com/openmind_agi&quot;&gt;X&lt;/a&gt; | &lt;a href=&quot;https://discord.gg/VUjpg4ef5n&quot;&gt;Discord&lt;/a&gt; &lt;/p&gt;

**OpenMind&#039;s OM1 is a modular AI runtime that empowers developers to create and deploy multimodal AI agents across digital environments and physical robots**, including Humanoids, Phone Apps, websites, Quadrupeds, and educational robots such as TurtleBot 4. OM1 agents can process diverse inputs like web data, social media, camera feeds, and LIDAR, while enabling physical actions including motion, autonomous navigation, and natural conversations. The goal of OM1 is to make it easy to create highly capable human-focused robots, that are easy to upgrade and (re)configure to accommodate different physical form factors.

## Capabilities of OM1

* **Modular Architecture**: Designed with Python for simplicity and seamless integration.
* **Data Input**: Easily handles new data and sensors.
* **Hardware Support via Plugins**: Supports new hardware through plugins for API endpoints and specific robot hardware connections to `ROS2`, `Zenoh`, and `CycloneDDS`. (We recommend `Zenoh` for all new development).
* **Web-Based Debugging Display**: Monitor the system in action with WebSim (available at http://localhost:8000/) for easy visual debugging.
* **Pre-configured Endpoints**: Supports Voice-to-Speech, OpenAI‚Äôs `gpt-4o`, DeepSeek, and multiple Visual Language Models (VLMs) with pre-configured endpoints for each service.

## Architecture Overview
  ![Artboard 1@4x 1 (1)](https://github.com/user-attachments/assets/14e9b916-4df7-4700-9336-2983c85be311)

## Getting Started - Hello World

To get started with OM1, let&#039;s run the Spot agent. Spot uses your webcam to capture and label objects. These text captions are then sent to `OpenAI 4o`, which returns `movement`, `speech` and `face` action commands. These commands are displayed on WebSim along with basic timing and other debugging information.

### Package Management and VENV

You will need the [`uv` package manager](https://docs.astral.sh/uv/getting-started/installation/).

### Clone the Repo

```bash
git clone https://github.com/openmind/OM1.git
cd OM1
git submodule update --init
uv venv
```

### Install Dependencies

For MacOS  
```bash
brew install portaudio ffmpeg
```

For Linux  
```bash
sudo apt-get update
sudo apt-get install portaudio19-dev python-dev ffmpeg
```

### Obtain an OpenMind API Key

Obtain your API Key at [OpenMind Portal](https://portal.openmind.org/). Copy it to `config/spot.json5`, replacing the `openmind_free` placeholder. Or, `cp env.example .env` and add your key to the `.env`. 

### Launching OM1

Run
```bash
uv run src/run.py spot
```

After launching OM1, the Spot agent will interact with you and perform (simulated) actions. For more help connecting OM1 to your robot hardware, see [getting started](https://docs.openmind.org/getting-started).

## What&#039;s Next?

* Try out some [examples](https://docs.openmind.org/examples)
* Add new `inputs` and `actions`.
* Design custom agents and robots by creating your own `json5` config files with custom combinations of inputs and actions.
* Change the system prompts in the configuration files (located in `/config/`) to create new behaviors.

## Interfacing with New Robot Hardware

OM1 assumes that robot hardware provides a high-level SDK that accepts elemental movement and action commands such as `backflip`, `run`, `gently pick up the red apple`, `move(0.37, 0, 0)`, and `smile`. An example is provided in `actions/move_safe/connector/ros2.py`:

```python
...
elif output_interface.action == &quot;shake paw&quot;:
    if self.sport_client:
        self.sport_client.Hello()
...
```

If your robot hardware does not yet provide a suitable HAL (hardware abstraction layer), traditional robotics approaches such as RL (reinforcement learning) in concert with suitable simulation environments (Unity, Gazebo), sensors (such as hand mounted ZED depth cameras), and custom VLAs will be needed for you to create one. It is further assumed that your HAL accepts motion trajectories, provides battery and thermal management/monitoring, and calibrates and tunes sensors such as IMUs, LIDARs, and magnetometers. 

OM1 can interface with your HAL via USB, serial, ROS2, CycloneDDS, Zenoh, or websockets. For an example of an advanced humanoid HAL, please see [Unitree&#039;s C++ SDK](https://github.com/unitreerobotics/unitree_sdk2/blob/adee312b081c656ecd0bb4e936eed96325546296/example/g1/high_level/g1_loco_client_example.cpp#L159). Frequently, a HAL, especially ROS2 code, will be dockerized and can then interface with OM1 through DDS middleware or websockets.   

## Recommended Development Platforms

OM1 is developed on:

* Jetson AGX Orin 64GB (running Ubuntu 22.04 and JetPack 6.1)
* Mac Studio with Apple M2 Ultra with 48 GB unified memory (running MacOS Sequoia)
* Mac Mini with Apple M4 Pro with 48 GB unified memory (running MacOS Sequoia)
* Generic Linux machines (running Ubuntu 22.04)

OM1 _should_ run on other platforms (such as Windows) and microcontrollers such as the Raspberry Pi 5 16GB.


## Full Autonomy Guidance

We&#039;re excited to introduce **full autonomy mode**, where three services work together in a loop without manual intervention:

- **om1**
- **unitree_go2_ros2_sdk** ‚Äì A ROS 2 package that provides SLAM (Simultaneous Localization and Mapping) capabilities for the Unitree Go2 robot using an RPLiDAR sensor, the SLAM Toolbox and the Nav2 stack.
- **om1-avatar** ‚Äì A modern React-based frontend application that provides the user interface and avatar display system for OM1 robotics software.

## Intro to Backpack?
From research to real-world autonomy, a platform that learns, moves, and builds with you.
We&#039;ll shortly be releasing the **BOM** and details on **DIY** for the it. 
Stay tuned!

Clone the following repos -
- https://github.com/OpenMind/OM1.git
- https://github.com/OpenMind/unitree_go2_ros2_sdk.git
- https://github.com/OpenMind/OM1-avatar.git

## Starting the system
To start all services, run the following commands:
- For OM1

Setup the API key

For Bash: vim ~/.bashrc or ~/.bash_profile.

For Zsh: vim ~/.zshrc.

Add 

```bash 
export OM_API_KEY=&quot;your_api_key&quot;
```

Update the docker-compose file. Replace &quot;unitree_go2_autonomy_advance&quot; with the agent you want to run.
```bash
command: [&quot;unitree_go2_autonomy_advance&quot;]
```

```bash
cd OM1
docker-compose up om1 -d --no-build
```
- For unitree_go2_ros2_sdk
```bash
cd unitree_go2_ros2_sdk
docker-compose up orchestrator -d --no-build
docker-compose up om1_sensor -d --no-build
docker-compose up watchdog -d --no-build
```
- For OM1-avatar
```bash
cd OM1-avatar
docker-compose up om1_avatar -d --no-build
```
## Detailed Documentation

More detailed documentation can be accessed at [docs.openmind.org](https://docs.openmind.org/).

## Contributing

Please make sure to read the [Contributing Guide](./CONTRIBUTING.md) before making a pull request.

## License

This project is licensed under the terms of the MIT License, which is a permissive free software license that allows users to freely use, modify, and distribute the software. The MIT License is a widely used and well-established license that is known for its simplicity and flexibility. By using the MIT License, this project aims to encourage collaboration, modification, and distribution of the software.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[mindsdb/mindsdb]]></title>
            <link>https://github.com/mindsdb/mindsdb</link>
            <guid>https://github.com/mindsdb/mindsdb</guid>
            <pubDate>Sat, 11 Oct 2025 00:03:56 GMT</pubDate>
            <description><![CDATA[AI Analytics and Knowledge Engine for RAG over large-scale, heterogeneous data. - The only MCP Server you'll ever need]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mindsdb/mindsdb">mindsdb/mindsdb</a></h1>
            <p>AI Analytics and Knowledge Engine for RAG over large-scale, heterogeneous data. - The only MCP Server you'll ever need</p>
            <p>Language: Python</p>
            <p>Stars: 36,376</p>
            <p>Forks: 5,850</p>
            <p>Stars today: 47 stars today</p>
            <h2>README</h2><pre>

&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
	&lt;a href=&quot;https://pypi.org/project/MindsDB/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/MindsDB.svg&quot; alt=&quot;MindsDB Release&quot;&gt;&lt;/a&gt;
	&lt;a href=&quot;https://www.python.org/downloads/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/python-3.10.x%7C%203.11.x-brightgreen.svg&quot; alt=&quot;Python supported&quot;&gt;&lt;/a&gt;
	&lt;a href=&quot;https://hub.docker.com/u/mindsdb&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/mindsdb/mindsdb&quot; alt=&quot;Docker pulls&quot;&gt;&lt;/a&gt;

  &lt;br /&gt;
  &lt;br /&gt;

  &lt;a href=&quot;https://trendshift.io/repositories/3068&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/3068&quot; alt=&quot;mindsdb%2Fmindsdb | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

  &lt;a href=&quot;https://github.com/mindsdb/mindsdb&quot;&gt;
    &lt;img src=&quot;/docs/assets/mindsdb_logo.png&quot; alt=&quot;MindsDB&quot; width=&quot;300&quot;&gt;
  &lt;/a&gt;

  &lt;p align=&quot;center&quot;&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://www.mindsdb.com?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Website&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://docs.mindsdb.com?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Docs&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://mindsdb.com/contact&quot;&gt;Contact us for a Demo&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://mindsdb.com/joincommunity?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo&quot;&gt;Community Slack&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

----------------------------------------


MindsDB enables humans, AI, agents, and applications to get highly accurate answers across large scale data sources.

&lt;a href=&quot;https://www.youtube.com/watch?v=MX3OKpnsoLM&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/119e7b82-f901-4214-a26f-ff7c5ad86064&quot; alt=&quot;MindsDB Demo&quot;&gt;
	
&lt;/a&gt;


## Install MindsDB Server 

MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart&#039;s content.

  * [Using Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop). This is the fastest and recommended way to get started and have it all running.
  * [Using Docker](https://docs.mindsdb.com/setup/self-hosted/docker). This is also simple, but gives you more flexibility on how to further customize your server.

[MindsDB has an MCP server built in](https://docs.mindsdb.com/mcp/overview) that enables your MCP applications to connect, unify and respond to questions over large-scale federated data‚Äîspanning databases, data warehouses, and SaaS applications.
 
----------------------------------------

# Core Philosophy: Connect, Unify, Respond

MindsDB&#039;s architecture is built around three fundamental capabilities:

## [Connect](https://docs.mindsdb.com/integrations/data-overview) Your Data

You can connect to hundreds of enterprise [data sources (learn more)](https://docs.mindsdb.com/integrations/data-overview). These integrations allow MindsDB to access data wherever it resides, forming the foundation for all other capabilities.

## [Unify](https://docs.mindsdb.com/mindsdb_sql/overview) Your Data


In many situations, it‚Äôs important to be able to prepare and unify data before generating responses from it. MindsDB SQL offers knowledge bases and views that allow indexing and organizing structured and unstructured data as if it were unified in a single system.

* [**KNOWLEDGE BASES**](https://docs.mindsdb.com/mindsdb_sql/knowledge-bases) ‚Äì Index and organize unstructured data for efficient Q&amp;A.
* [**VIEWS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/view) ‚Äì Simplify data access by creating unified views across different sources (no-ETL).


Unification of data can be automated using JOBs

* [**JOBS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/jobs) ‚Äì Schedule synchronization and transformation tasks for real-time processing.


## [Respond](https://docs.mindsdb.com/mindsdb_sql/agents/agent) From Your Data

Chat with Your Data

* [**AGENTS**](https://docs.mindsdb.com/mindsdb_sql/agents/agent) ‚Äì Configure built-in agents specialized in answering questions over your connected and unified data.
* [**MCP**](https://docs.mindsdb.com/mcp/overview) ‚Äì Connect to MindsDB through the MCP (Model Context Protocol) for seamless interaction.

----------------------------------------

## ü§ù Contribute

Interested in contributing to MindsDB? Follow our [installation guide for development](https://docs.mindsdb.com/contribute/install?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

You can find our [contribution guide here](https://docs.mindsdb.com/contribute/contribute?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

We welcome suggestions! Feel free to open new issues with your ideas, and we‚Äôll guide you.

This project adheres to a [Contributor Code of Conduct](https://github.com/mindsdb/mindsdb/blob/main/CODE_OF_CONDUCT.md). By participating, you agree to follow its terms.

Also, check out our [community rewards and programs](https://mindsdb.com/community?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

## ü§ç Support

If you find a bug, please submit an [issue on GitHub](https://github.com/mindsdb/mindsdb/issues/new/choose).

Here‚Äôs how you can get community support:

* Ask a question in our [Slack Community](https://mindsdb.com/joincommunity?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).
* Join our [GitHub Discussions](https://github.com/mindsdb/mindsdb/discussions).
* Post on [Stack Overflow](https://stackoverflow.com/questions/tagged/mindsdb) with the MindsDB tag.

For commercial support, please [contact the MindsDB team](https://mindsdb.com/contact?utm_medium=community&amp;utm_source=github&amp;utm_campaign=mindsdb%20repo).

## üíö Current Contributors

&lt;a href=&quot;https://github.com/mindsdb/mindsdb/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contributors-img.web.app/image?repo=mindsdb/mindsdb&quot; /&gt;
&lt;/a&gt;

Generated with [contributors-img](https://contributors-img.web.app).

## üîî Subscribe for Updates

Join our [Slack community](https://mindsdb.com/joincommunity)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[open-compass/VLMEvalKit]]></title>
            <link>https://github.com/open-compass/VLMEvalKit</link>
            <guid>https://github.com/open-compass/VLMEvalKit</guid>
            <pubDate>Sat, 11 Oct 2025 00:03:55 GMT</pubDate>
            <description><![CDATA[Open-source evaluation toolkit of large multi-modality models (LMMs), support 220+ LMMs, 80+ benchmarks]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/open-compass/VLMEvalKit">open-compass/VLMEvalKit</a></h1>
            <p>Open-source evaluation toolkit of large multi-modality models (LMMs), support 220+ LMMs, 80+ benchmarks</p>
            <p>Language: Python</p>
            <p>Stars: 3,158</p>
            <p>Forks: 516</p>
            <p>Stars today: 7 stars today</p>
            <h2>README</h2><pre>![LOGO](http://opencompass.openxlab.space/utils/MMLB.jpg)

&lt;b&gt;A Toolkit for Evaluating Large Vision-Language Models. &lt;/b&gt;

[![][github-contributors-shield]][github-contributors-link] ‚Ä¢ [![][github-forks-shield]][github-forks-link] ‚Ä¢ [![][github-stars-shield]][github-stars-link] ‚Ä¢ [![][github-issues-shield]][github-issues-link] ‚Ä¢ [![][github-license-shield]][github-license-link]

English | [ÁÆÄ‰Ωì‰∏≠Êñá](/docs/zh-CN/README_zh-CN.md) | [Êó•Êú¨Ë™û](/docs/ja/README_ja.md)

&lt;a href=&quot;https://rank.opencompass.org.cn/leaderboard-multimodal&quot;&gt;üèÜ OC Learderboard &lt;/a&gt; ‚Ä¢
&lt;a href=&quot;#%EF%B8%8F-quickstart&quot;&gt;üèóÔ∏èQuickstart &lt;/a&gt; ‚Ä¢
&lt;a href=&quot;#-datasets-models-and-evaluation-results&quot;&gt;üìäDatasets &amp; Models &lt;/a&gt; ‚Ä¢
&lt;a href=&quot;#%EF%B8%8F-development-guide&quot;&gt;üõ†Ô∏èDevelopment &lt;/a&gt;

&lt;a href=&quot;https://huggingface.co/spaces/opencompass/open_vlm_leaderboard&quot;&gt;ü§ó HF Leaderboard&lt;/a&gt; ‚Ä¢
&lt;a href=&quot;https://huggingface.co/datasets/VLMEval/OpenVLMRecords&quot;&gt;ü§ó Evaluation Records&lt;/a&gt; ‚Ä¢
&lt;a href=&quot;https://huggingface.co/spaces/opencompass/openvlm_video_leaderboard&quot;&gt;ü§ó HF Video Leaderboard&lt;/a&gt; ‚Ä¢

&lt;a href=&quot;https://discord.gg/evDT4GZmxN&quot;&gt;üîä Discord&lt;/a&gt; ‚Ä¢
&lt;a href=&quot;https://www.arxiv.org/abs/2407.11691&quot;&gt;üìù Report&lt;/a&gt; ‚Ä¢
&lt;a href=&quot;#-the-goal-of-vlmevalkit&quot;&gt;üéØGoal &lt;/a&gt; ‚Ä¢
&lt;a href=&quot;#%EF%B8%8F-citation&quot;&gt;üñäÔ∏èCitation &lt;/a&gt;
&lt;/div&gt;

**VLMEvalKit** (the python package name is **vlmeval**) is an **open-source evaluation toolkit** of **large vision-language models (LVLMs)**. It enables **one-command evaluation** of LVLMs on various benchmarks, without the heavy workload of data preparation under multiple repositories. In VLMEvalKit, we adopt **generation-based evaluation** for all LVLMs, and provide the evaluation results obtained with both **exact matching** and **LLM-based answer extraction**.

## Recent Codebase Changes
- **[2025-09-12]** **Major Update: Improved Handling for Models with Thinking Mode**

    A new feature in [PR 1229](https://github.com/open-compass/VLMEvalKit/pull/1175) that improves support for models with thinking mode. VLMEvalKit now allows for the use of a custom `split_thinking` function. **We strongly recommend this for models with thinking mode to ensure the accuracy of evaluation**.  To use this new functionality, please enable the following settings: `SPLIT_THINK=True`. By default, the function will parse content within `&lt;think&gt;...&lt;/think&gt;` tags and store it in the `thinking` key of the output. For more advanced customization, you can also create a `split_think` function for model. Please see the InternVL implementation for an example.
- **[2025-09-12]** **Major Update: Improved Handling for Long Response(More than 16k/32k)**

    A new feature in [PR 1229](https://github.com/open-compass/VLMEvalKit/pull/1175) that improves support for models with long response outputs. VLMEvalKit can now save prediction files in TSV format. **Since individual cells in an `.xlsx` file are limited to 32,767 characters, we strongly recommend using this feature for models that generate long responses (e.g., exceeding 16k or 32k tokens) to prevent data truncation.**. To use this new functionality, please enable the following settings: `PRED_FORMAT=tsv`.
- **[2025-08-04]** In [PR 1175](https://github.com/open-compass/VLMEvalKit/pull/1175), we refine the `can_infer_option` and `can_infer_text`, which increasingly route the evaluation to LLM choice extractors and empirically leads to slight performance improvement for MCQ benchmarks.

## üÜï News
- **[2025-07-07]** Supported [**SeePhys**](https://seephys.github.io/), which is a ‚Äãfull spectrum multimodal benchmark for evaluating physics reasoning across different knowledge levels. thanks to [**Quinn777**](https://github.com/Quinn777) üî•üî•üî•
- **[2025-07-02]** Supported [**OvisU1**](https://huggingface.co/AIDC-AI/Ovis-U1-3B), thanks to [**liyang-7**](https://github.com/liyang-7) üî•üî•üî•
- **[2025-06-16]** Supported [**PhyX**](https://phyx-bench.github.io/), a benchmark aiming to assess capacity for physics-grounded reasoning in visual scenarios. üî•üî•üî•
- **[2025-05-24]** To facilitate faster evaluations for large-scale or thinking models, **VLMEvalKit supports multi-node distributed inference** using **LMDeploy**  (supports *InternVL Series, QwenVL Series, LLaMa4*) or **VLLM**(supports *QwenVL Series, LLaMa4*). You can activate this feature by adding the ```use_lmdeploy``` or ```use_vllm``` flag to your custom model configuration in [config.py](vlmeval/config.py) . Leverage these tools to significantly speed up your evaluation workflows üî•üî•üî•
- **[2025-05-24]** Supported Models: **InternVL3 Series, Gemini-2.5-Pro, Kimi-VL, LLaMA4, NVILA, Qwen2.5-Omni, Phi4, SmolVLM2, Grok, SAIL-VL-1.5, WeThink-Qwen2.5VL-7B, Bailingmm, VLM-R1, Taichu-VLR**. Supported Benchmarks: **HLE-Bench, MMVP, MM-AlignBench, Creation-MMBench, MM-IFEval, OmniDocBench, OCR-Reasoning, EMMA, ChaXivÔºåMedXpertQA, Physics, MSEarthMCQ, MicroBench, MMSci, VGRP-Bench, wildDoc, TDBench, VisuLogic, CVBench, LEGO-Puzzles, Video-MMLU, QBench-Video, MME-CoT, VLM2Bench, VMCBench, MOAT, Spatial457 Benchmark**. Please refer to [**VLMEvalKit Features**](https://aicarrier.feishu.cn/wiki/Qp7wwSzQ9iK1Y6kNUJVcr6zTnPe?table=tblsdEpLieDoCxtb) for more details. Thanks to all contributors üî•üî•üî•
- **[2025-02-20]** Supported Models: **InternVL2.5 Series, Qwen2.5VL Series, QVQ-72B, Doubao-VL, Janus-Pro-7B, MiniCPM-o-2.6, InternVL2-MPO, LLaVA-CoT, Hunyuan-Standard-Vision, Ovis2, Valley, SAIL-VL, Ross, Long-VITA, EMU3, SmolVLM**. Supported Benchmarks: **MMMU-Pro, WeMath, 3DSRBench, LogicVista, VL-RewardBench, CC-OCR, CG-Bench, CMMMU, WorldSense**. Thanks to all contributors üî•üî•üî•
- **[2024-12-11]** Supported [**NaturalBench**](https://huggingface.co/datasets/BaiqiL/NaturalBench), a vision-centric VQA benchmark (NeurIPS&#039;24) that challenges vision-language models with simple questions about natural imagery.
- **[2024-12-02]** Supported [**VisOnlyQA**](https://github.com/psunlpgroup/VisOnlyQA/), a benchmark for evaluating the visual perception capabilities üî•üî•üî•
- **[2024-11-26]** Supported [**Ovis1.6-Gemma2-27B**](https://huggingface.co/AIDC-AI/Ovis1.6-Gemma2-27B), thanks to [**runninglsy**](https://github.com/runninglsy) üî•üî•üî•
- **[2024-11-25]** Create a new flag `VLMEVALKIT_USE_MODELSCOPE`. By setting this environment variable, you can download the video benchmarks supported from [**modelscope**](https://www.modelscope.cn) üî•üî•üî•

## üèóÔ∏è QuickStart

See [[QuickStart](/docs/en/Quickstart.md) | [Âø´ÈÄüÂºÄÂßã](/docs/zh-CN/Quickstart.md)] for a quick start guide.

## üìä Datasets, Models, and Evaluation Results

### Evaluation Results

**The performance numbers on our official multi-modal leaderboards can be downloaded from here!**

[**OpenVLM Leaderboard**](https://huggingface.co/spaces/opencompass/open_vlm_leaderboard): [**Download All DETAILED Results**](http://opencompass.openxlab.space/assets/OpenVLM.json).

Check **Supported Benchmarks** Tab in [**VLMEvalKit Features**](https://aicarrier.feishu.cn/wiki/Qp7wwSzQ9iK1Y6kNUJVcr6zTnPe?table=tblsdEpLieDoCxtb) to view all supported image &amp; video benchmarks (70+).

Check **Supported LMMs** Tab in [**VLMEvalKit Features**](https://aicarrier.feishu.cn/wiki/Qp7wwSzQ9iK1Y6kNUJVcr6zTnPe?table=tblsdEpLieDoCxtb) to view all supported LMMs, including commercial APIs, open-source models, and more (200+).

**Transformers Version Recommendation:**

Note that some VLMs may not be able to run under certain transformer versions, we recommend the following settings to evaluate each VLM:

- **Please use** `transformers==4.33.0` **for**: `Qwen series`, `Monkey series`, `InternLM-XComposer Series`, `mPLUG-Owl2`, `OpenFlamingo v2`, `IDEFICS series`, `VisualGLM`, `MMAlaya`, `ShareCaptioner`, `MiniGPT-4 series`, `InstructBLIP series`, `PandaGPT`, `VXVERSE`.
- **Please use** `transformers==4.36.2` **for**: `Moondream1`.
- **Please use** `transformers==4.37.0` **for**: `LLaVA series`, `ShareGPT4V series`, `TransCore-M`, `LLaVA (XTuner)`, `CogVLM Series`, `EMU2 Series`, `Yi-VL Series`, `MiniCPM-[V1/V2]`, `OmniLMM-12B`, `DeepSeek-VL series`, `InternVL series`, `Cambrian Series`, `VILA Series`, `Llama-3-MixSenseV1_1`, `Parrot-7B`, `PLLaVA Series`.
- **Please use** `transformers==4.40.0` **for**: `IDEFICS2`, `Bunny-Llama3`, `MiniCPM-Llama3-V2.5`, `360VL-70B`, `Phi-3-Vision`, `WeMM`.
- **Please use** `transformers==4.42.0` **for**: `AKI`.
- **Please use** `transformers==4.44.0` **for**: `Moondream2`, `H2OVL series`.
- **Please use** `transformers==4.45.0` **for**: `Aria`.
- **Please use** `transformers==latest` **for**: `LLaVA-Next series`, `PaliGemma-3B`, `Chameleon series`, `Video-LLaVA-7B-HF`, `Ovis series`, `Mantis series`, `MiniCPM-V2.6`, `OmChat-v2.0-13B-sinlge-beta`, `Idefics-3`, `GLM-4v-9B`, `VideoChat2-HD`, `RBDash_72b`, `Llama-3.2 series`, `Kosmos series`.

**Torchvision Version Recommendation:**

Note that some VLMs may not be able to run under certain torchvision versions, we recommend the following settings to evaluate each VLM:

- **Please use** `torchvision&gt;=0.16` **for**: `Moondream series` and `Aria`

**Flash-attn Version Recommendation:**

Note that some VLMs may not be able to run under certain flash-attention versions, we recommend the following settings to evaluate each VLM:

- **Please use** `pip install flash-attn --no-build-isolation` **for**: `Aria`

```python
# Demo
from vlmeval.config import supported_VLM
model = supported_VLM[&#039;idefics_9b_instruct&#039;]()
# Forward Single Image
ret = model.generate([&#039;assets/apple.jpg&#039;, &#039;What is in this image?&#039;])
print(ret)  # The image features a red apple with a leaf on it.
# Forward Multiple Images
ret = model.generate([&#039;assets/apple.jpg&#039;, &#039;assets/apple.jpg&#039;, &#039;How many apples are there in the provided images? &#039;])
print(ret)  # There are two apples in the provided images.
```

## üõ†Ô∏è Development Guide

To develop custom benchmarks, VLMs, or simply contribute other codes to **VLMEvalKit**, please refer to [[Development_Guide](/docs/en/Development.md) | [ÂºÄÂèëÊåáÂçó](/docs/zh-CN/Development.md)].

**Call for contributions**

To promote the contribution from the community and share the corresponding credit (in the next report update):

- All Contributions will be acknowledged in the report.
- Contributors with 3 or more major contributions (implementing an MLLM, benchmark, or major feature) can join the author list of [VLMEvalKit Technical Report](https://www.arxiv.org/abs/2407.11691) on ArXiv. Eligible contributors can create an issue or dm kennyutc in [VLMEvalKit Discord Channel](https://discord.com/invite/evDT4GZmxN).

Here is a [contributor list](/docs/en/Contributors.md) we curated based on the records.

## üéØ The Goal of VLMEvalKit

**The codebase is designed to:**

1. Provide an **easy-to-use**, **opensource evaluation toolkit** to make it convenient for researchers &amp; developers to evaluate existing LVLMs and make evaluation results **easy to reproduce**.
2. Make it easy for VLM developers to evaluate their own models. To evaluate the VLM on multiple supported benchmarks, one just need to **implement a single `generate_inner()` function**, all other workloads (data downloading, data preprocessing, prediction inference, metric calculation) are handled by the codebase.

**The codebase is not designed to:**

1. Reproduce the exact accuracy number reported in the original papers of all **3rd party benchmarks**. The reason can be two-fold:
   1. VLMEvalKit uses **generation-based evaluation** for all VLMs (and optionally with **LLM-based answer extraction**). Meanwhile, some benchmarks may use different approaches (SEEDBench uses PPL-based evaluation, *eg.*). For those benchmarks, we compare both scores in the corresponding result. We encourage developers to support other evaluation paradigms in the codebase.
   2. By default, we use the same prompt template for all VLMs to evaluate on a benchmark. Meanwhile, **some VLMs may have their specific prompt templates** (some may not covered by the codebase at this time). We encourage VLM developers to implement their own prompt template in VLMEvalKit, if that is not covered currently. That will help to improve the reproducibility.

## üñäÔ∏è Citation

If you find this work helpful, please consider to **starüåü** this repo. Thanks for your support!

[![Stargazers repo roster for @open-compass/VLMEvalKit](https://reporoster.com/stars/open-compass/VLMEvalKit)](https://github.com/open-compass/VLMEvalKit/stargazers)

If you use VLMEvalKit in your research or wish to refer to published OpenSource evaluation results, please use the following BibTeX entry and the BibTex entry corresponding to the specific VLM / benchmark you used.

```bib
@inproceedings{duan2024vlmevalkit,
  title={Vlmevalkit: An open-source toolkit for evaluating large multi-modality models},
  author={Duan, Haodong and Yang, Junming and Qiao, Yuxuan and Fang, Xinyu and Chen, Lin and Liu, Yuan and Dong, Xiaoyi and Zang, Yuhang and Zhang, Pan and Wang, Jiaqi and others},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
  pages={11198--11201},
  year={2024}
}
```

&lt;p align=&quot;right&quot;&gt;&lt;a href=&quot;#top&quot;&gt;üîùBack to top&lt;/a&gt;&lt;/p&gt;

[github-contributors-link]: https://github.com/open-compass/VLMEvalKit/graphs/contributors
[github-contributors-shield]: https://img.shields.io/github/contributors/open-compass/VLMEvalKit?color=c4f042&amp;labelColor=black&amp;style=flat-square
[github-forks-link]: https://github.com/open-compass/VLMEvalKit/network/members
[github-forks-shield]: https://img.shields.io/github/forks/open-compass/VLMEvalKit?color=8ae8ff&amp;labelColor=black&amp;style=flat-square
[github-issues-link]: https://github.com/open-compass/VLMEvalKit/issues
[github-issues-shield]: https://img.shields.io/github/issues/open-compass/VLMEvalKit?color=ff80eb&amp;labelColor=black&amp;style=flat-square
[github-license-link]: https://github.com/open-compass/VLMEvalKit/blob/main/LICENSE
[github-license-shield]: https://img.shields.io/github/license/open-compass/VLMEvalKit?color=white&amp;labelColor=black&amp;style=flat-square
[github-stars-link]: https://github.com/open-compass/VLMEvalKit/stargazers
[github-stars-shield]: https://img.shields.io/github/stars/open-compass/VLMEvalKit?color=ffcb47&amp;labelColor=black&amp;style=flat-square
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[MetaCubeX/mihomo]]></title>
            <link>https://github.com/MetaCubeX/mihomo</link>
            <guid>https://github.com/MetaCubeX/mihomo</guid>
            <pubDate>Sat, 11 Oct 2025 00:03:54 GMT</pubDate>
            <description><![CDATA[A simple Python Pydantic model for Honkai: Star Rail parsed data from the Mihomo API.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/MetaCubeX/mihomo">MetaCubeX/mihomo</a></h1>
            <p>A simple Python Pydantic model for Honkai: Star Rail parsed data from the Mihomo API.</p>
            <p>Language: Python</p>
            <p>Stars: 23,525</p>
            <p>Forks: 3,335</p>
            <p>Stars today: 26 stars today</p>
            <h2>README</h2><pre># mihomo
A simple python pydantic model (type hint and autocompletion support) for Honkai: Star Rail parsed data from the Mihomo API.

API url: https://api.mihomo.me/sr_info_parsed/{UID}?lang={LANG}

## Installation
```
pip install -U git+https://github.com/KT-Yeh/mihomo.git
```

## Usage

### Basic
There are two parsed data formats:
- V1:
  - URL: https://api.mihomo.me/sr_info_parsed/800333171?lang=en&amp;version=v1
  - Fetching: use `client.fetch_user_v1(800333171)`
  - Data model: `mihomo.models.v1.StarrailInfoParsedV1`
  - All models defined in `mihomo/models/v1` directory.
- V2: 
  - URL: https://api.mihomo.me/sr_info_parsed/800333171?lang=en
  - Fetching: use `client.fetch_user(800333171)`
  - Data model: `mihomo.models.StarrailInfoParsed`
  - All models defined in `mihomo/models` directory.

If you don&#039;t want to use `client.get_icon_url` to get the image url everytime, you can use `client.fetch_user(800333171, replace_icon_name_with_url=True)` to get the parsed data with asset urls.

### Example
```py
import asyncio

from mihomo import Language, MihomoAPI
from mihomo.models import StarrailInfoParsed
from mihomo.models.v1 import StarrailInfoParsedV1

client = MihomoAPI(language=Language.EN)


async def v1():
    data: StarrailInfoParsedV1 = await client.fetch_user_v1(800333171)

    print(f&quot;Name: {data.player.name}&quot;)
    print(f&quot;Level: {data.player.level}&quot;)
    print(f&quot;Signature: {data.player.signature}&quot;)
    print(f&quot;Achievements: {data.player_details.achievements}&quot;)
    print(f&quot;Characters count: {data.player_details.characters}&quot;)
    print(f&quot;Profile picture url: {client.get_icon_url(data.player.icon)}&quot;)
    for character in data.characters:
        print(&quot;-----------&quot;)
        print(f&quot;Name: {character.name}&quot;)
        print(f&quot;Rarity: {character.rarity}&quot;)
        print(f&quot;Level: {character.level}&quot;)
        print(f&quot;Avatar url: {client.get_icon_url(character.icon)}&quot;)
        print(f&quot;Preview url: {client.get_icon_url(character.preview)}&quot;)
        print(f&quot;Portrait url: {client.get_icon_url(character.portrait)}&quot;)


async def v2():
    data: StarrailInfoParsed = await client.fetch_user(800333171, replace_icon_name_with_url=True)

    print(f&quot;Name: {data.player.name}&quot;)
    print(f&quot;Level: {data.player.level}&quot;)
    print(f&quot;Signature: {data.player.signature}&quot;)
    print(f&quot;Profile picture url: {data.player.avatar.icon}&quot;)
    for character in data.characters:
        print(&quot;-----------&quot;)
        print(f&quot;Name: {character.name}&quot;)
        print(f&quot;Rarity: {character.rarity}&quot;)
        print(f&quot;Portrait url: {character.portrait}&quot;)

asyncio.run(v1())
asyncio.run(v2())
```

### Tools
`from mihomo import tools`
#### Remove Duplicate Character
```py
    data = await client.fetch_user(800333171)
    data = tools.remove_duplicate_character(data)
```

#### Merge Character Data
```py
    old_data = await client.fetch_user(800333171)

    # Change characters in game and wait for the API to refresh
    # ...

    new_data = await client.fetch_user(800333171)
    data = tools.merge_character_data(new_data, old_data)
```

### Data Persistence
Take pickle and json as an example
```py
import pickle
import zlib
from mihomo import MihomoAPI, Language, StarrailInfoParsed

client = MihomoAPI(language=Language.EN)
data = await client.fetch_user(800333171)

# Save
pickle_data = zlib.compress(pickle.dumps(data))
print(len(pickle_data))
json_data = data.json(by_alias=True, ensure_ascii=False)
print(len(json_data))

# Load
data_from_pickle = pickle.loads(zlib.decompress(pickle_data))
data_from_json = StarrailInfoParsed.parse_raw(json_data)
print(type(data_from_pickle))
print(type(data_from_json))
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NanmiCoder/MediaCrawler]]></title>
            <link>https://github.com/NanmiCoder/MediaCrawler</link>
            <guid>https://github.com/NanmiCoder/MediaCrawler</guid>
            <pubDate>Sat, 11 Oct 2025 00:03:53 GMT</pubDate>
            <description><![CDATA[Â∞èÁ∫¢‰π¶Á¨îËÆ∞ | ËØÑËÆ∫Áà¨Ëô´„ÄÅÊäñÈü≥ËßÜÈ¢ë | ËØÑËÆ∫Áà¨Ëô´„ÄÅÂø´ÊâãËßÜÈ¢ë | ËØÑËÆ∫Áà¨Ëô´„ÄÅB Á´ôËßÜÈ¢ë ÔΩú ËØÑËÆ∫Áà¨Ëô´„ÄÅÂæÆÂçöÂ∏ñÂ≠ê ÔΩú ËØÑËÆ∫Áà¨Ëô´„ÄÅÁôæÂ∫¶Ë¥¥ÂêßÂ∏ñÂ≠ê ÔΩú ÁôæÂ∫¶Ë¥¥ÂêßËØÑËÆ∫ÂõûÂ§çÁà¨Ëô´ | Áü•‰πéÈóÆÁ≠îÊñáÁ´†ÔΩúËØÑËÆ∫Áà¨Ëô´]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NanmiCoder/MediaCrawler">NanmiCoder/MediaCrawler</a></h1>
            <p>Â∞èÁ∫¢‰π¶Á¨îËÆ∞ | ËØÑËÆ∫Áà¨Ëô´„ÄÅÊäñÈü≥ËßÜÈ¢ë | ËØÑËÆ∫Áà¨Ëô´„ÄÅÂø´ÊâãËßÜÈ¢ë | ËØÑËÆ∫Áà¨Ëô´„ÄÅB Á´ôËßÜÈ¢ë ÔΩú ËØÑËÆ∫Áà¨Ëô´„ÄÅÂæÆÂçöÂ∏ñÂ≠ê ÔΩú ËØÑËÆ∫Áà¨Ëô´„ÄÅÁôæÂ∫¶Ë¥¥ÂêßÂ∏ñÂ≠ê ÔΩú ÁôæÂ∫¶Ë¥¥ÂêßËØÑËÆ∫ÂõûÂ§çÁà¨Ëô´ | Áü•‰πéÈóÆÁ≠îÊñáÁ´†ÔΩúËØÑËÆ∫Áà¨Ëô´</p>
            <p>Language: Python</p>
            <p>Stars: 37,747</p>
            <p>Forks: 8,650</p>
            <p>Stars today: 40 stars today</p>
            <h2>README</h2><pre># üî• MediaCrawler - Ëá™Â™í‰ΩìÂπ≥Âè∞Áà¨Ëô´ üï∑Ô∏è

&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;
   &lt;sup&gt;Special thanks to:&lt;/sup&gt;
   &lt;br&gt;
   &lt;br&gt;
   &lt;a href=&quot;https://go.warp.dev/MediaCrawler&quot;&gt;
      &lt;img alt=&quot;Warp sponsorship&quot; width=&quot;400&quot; src=&quot;https://github.com/warpdotdev/brand-assets/blob/main/Github/Sponsor/Warp-Github-LG-02.png?raw=true&quot;&gt;
   &lt;/a&gt;

### [Warp is built for coding with multiple AI agents](https://go.warp.dev/MediaCrawler)


&lt;/div&gt;
&lt;hr&gt;

&lt;div align=&quot;center&quot;&gt;

&lt;a href=&quot;https://trendshift.io/repositories/8291&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://trendshift.io/api/badge/repositories/8291&quot; alt=&quot;NanmiCoder%2FMediaCrawler | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
&lt;/a&gt;

[![GitHub Stars](https://img.shields.io/github/stars/NanmiCoder/MediaCrawler?style=social)](https://github.com/NanmiCoder/MediaCrawler/stargazers)
[![GitHub Forks](https://img.shields.io/github/forks/NanmiCoder/MediaCrawler?style=social)](https://github.com/NanmiCoder/MediaCrawler/network/members)
[![GitHub Issues](https://img.shields.io/github/issues/NanmiCoder/MediaCrawler)](https://github.com/NanmiCoder/MediaCrawler/issues)
[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/NanmiCoder/MediaCrawler)](https://github.com/NanmiCoder/MediaCrawler/pulls)
[![License](https://img.shields.io/github/license/NanmiCoder/MediaCrawler)](https://github.com/NanmiCoder/MediaCrawler/blob/main/LICENSE)
[![‰∏≠Êñá](https://img.shields.io/badge/üá®üá≥_‰∏≠Êñá-ÂΩìÂâç-blue)](README.md)
[![English](https://img.shields.io/badge/üá∫üá∏_English-Available-green)](README_en.md)
[![Espa√±ol](https://img.shields.io/badge/üá™üá∏_Espa√±ol-Available-green)](README_es.md)
&lt;/div&gt;



&gt; **ÂÖçË¥£Â£∞ÊòéÔºö**
&gt; 
&gt; Â§ßÂÆ∂ËØ∑‰ª•Â≠¶‰π†‰∏∫ÁõÆÁöÑ‰ΩøÁî®Êú¨‰ªìÂ∫ì‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏èÔºå[Áà¨Ëô´ËøùÊ≥ïËøùËßÑÁöÑÊ°à‰ª∂](https://github.com/HiddenStrawberry/Crawler_Illegal_Cases_In_China)  &lt;br&gt;
&gt;
&gt;Êú¨‰ªìÂ∫ìÁöÑÊâÄÊúâÂÜÖÂÆπ‰ªÖ‰æõÂ≠¶‰π†ÂíåÂèÇËÄÉ‰πãÁî®ÔºåÁ¶ÅÊ≠¢Áî®‰∫éÂïÜ‰∏öÁî®ÈÄî„ÄÇ‰ªª‰Ωï‰∫∫ÊàñÁªÑÁªá‰∏çÂæóÂ∞ÜÊú¨‰ªìÂ∫ìÁöÑÂÜÖÂÆπÁî®‰∫éÈùûÊ≥ïÁî®ÈÄîÊàñ‰æµÁäØ‰ªñ‰∫∫ÂêàÊ≥ïÊùÉÁõä„ÄÇÊú¨‰ªìÂ∫ìÊâÄÊ∂âÂèäÁöÑÁà¨Ëô´ÊäÄÊúØ‰ªÖÁî®‰∫éÂ≠¶‰π†ÂíåÁ†îÁ©∂Ôºå‰∏çÂæóÁî®‰∫éÂØπÂÖ∂‰ªñÂπ≥Âè∞ËøõË°åÂ§ßËßÑÊ®°Áà¨Ëô´ÊàñÂÖ∂‰ªñÈùûÊ≥ïË°å‰∏∫„ÄÇÂØπ‰∫éÂõ†‰ΩøÁî®Êú¨‰ªìÂ∫ìÂÜÖÂÆπËÄåÂºïËµ∑ÁöÑ‰ªª‰ΩïÊ≥ïÂæãË¥£‰ªªÔºåÊú¨‰ªìÂ∫ì‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªª„ÄÇ‰ΩøÁî®Êú¨‰ªìÂ∫ìÁöÑÂÜÖÂÆπÂç≥Ë°®Á§∫ÊÇ®ÂêåÊÑèÊú¨ÂÖçË¥£Â£∞ÊòéÁöÑÊâÄÊúâÊù°Ê¨æÂíåÊù°‰ª∂„ÄÇ
&gt;
&gt; ÁÇπÂáªÊü•ÁúãÊõ¥‰∏∫ËØ¶ÁªÜÁöÑÂÖçË¥£Â£∞Êòé„ÄÇ[ÁÇπÂáªË∑≥ËΩ¨](#disclaimer)




## üìñ È°πÁõÆÁÆÄ‰ªã

‰∏Ä‰∏™ÂäüËÉΩÂº∫Â§ßÁöÑ**Â§öÂπ≥Âè∞Ëá™Â™í‰ΩìÊï∞ÊçÆÈááÈõÜÂ∑•ÂÖ∑**ÔºåÊîØÊåÅÂ∞èÁ∫¢‰π¶„ÄÅÊäñÈü≥„ÄÅÂø´Êâã„ÄÅBÁ´ô„ÄÅÂæÆÂçö„ÄÅË¥¥Âêß„ÄÅÁü•‰πéÁ≠â‰∏ªÊµÅÂπ≥Âè∞ÁöÑÂÖ¨ÂºÄ‰ø°ÊÅØÊäìÂèñ„ÄÇ

### üîß ÊäÄÊúØÂéüÁêÜ

- **Ê†∏ÂøÉÊäÄÊúØ**ÔºöÂü∫‰∫é [Playwright](https://playwright.dev/) ÊµèËßàÂô®Ëá™Âä®ÂåñÊ°ÜÊû∂ÁôªÂΩï‰øùÂ≠òÁôªÂΩïÊÄÅ
- **Êó†ÈúÄJSÈÄÜÂêë**ÔºöÂà©Áî®‰øùÁïôÁôªÂΩïÊÄÅÁöÑÊµèËßàÂô®‰∏ä‰∏ãÊñáÁéØÂ¢ÉÔºåÈÄöËøá JS Ë°®ËææÂºèËé∑ÂèñÁ≠æÂêçÂèÇÊï∞
- **‰ºòÂäøÁâπÁÇπ**ÔºöÊó†ÈúÄÈÄÜÂêëÂ§çÊùÇÁöÑÂä†ÂØÜÁÆóÊ≥ïÔºåÂ§ßÂπÖÈôç‰ΩéÊäÄÊúØÈó®Êßõ

## ‚ú® ÂäüËÉΩÁâπÊÄß
| Âπ≥Âè∞   | ÂÖ≥ÈîÆËØçÊêúÁ¥¢ | ÊåáÂÆöÂ∏ñÂ≠êIDÁà¨Âèñ | ‰∫åÁ∫ßËØÑËÆ∫ | ÊåáÂÆöÂàõ‰ΩúËÄÖ‰∏ªÈ°µ | ÁôªÂΩïÊÄÅÁºìÂ≠ò | IP‰ª£ÁêÜÊ±† | ÁîüÊàêËØÑËÆ∫ËØç‰∫ëÂõæ |
| ------ | ---------- | -------------- | -------- | -------------- | ---------- | -------- | -------------- |
| Â∞èÁ∫¢‰π¶ | ‚úÖ          | ‚úÖ              | ‚úÖ        | ‚úÖ              | ‚úÖ          | ‚úÖ        | ‚úÖ              |
| ÊäñÈü≥   | ‚úÖ          | ‚úÖ              | ‚úÖ        | ‚úÖ              | ‚úÖ          | ‚úÖ        | ‚úÖ              |
| Âø´Êâã   | ‚úÖ          | ‚úÖ              | ‚úÖ        | ‚úÖ              | ‚úÖ          | ‚úÖ        | ‚úÖ              |
| B Á´ô   | ‚úÖ          | ‚úÖ              | ‚úÖ        | ‚úÖ              | ‚úÖ          | ‚úÖ        | ‚úÖ              |
| ÂæÆÂçö   | ‚úÖ          | ‚úÖ              | ‚úÖ        | ‚úÖ              | ‚úÖ          | ‚úÖ        | ‚úÖ              |
| Ë¥¥Âêß   | ‚úÖ          | ‚úÖ              | ‚úÖ        | ‚úÖ              | ‚úÖ          | ‚úÖ        | ‚úÖ              |
| Áü•‰πé   | ‚úÖ          | ‚úÖ              | ‚úÖ        | ‚úÖ              | ‚úÖ          | ‚úÖ        | ‚úÖ              |



### üöÄ MediaCrawlerPro ÈáçÁ£ÖÂèëÂ∏ÉÔºÅ

&gt; ‰∏ìÊ≥®‰∫éÂ≠¶‰π†ÊàêÁÜüÈ°πÁõÆÁöÑÊû∂ÊûÑËÆæËÆ°Ôºå‰∏ç‰ªÖ‰ªÖÊòØÁà¨Ëô´ÊäÄÊúØÔºåPro ÁâàÊú¨ÁöÑ‰ª£Á†ÅËÆæËÆ°ÊÄùË∑ØÂêåÊ†∑ÂÄºÂæóÊ∑±ÂÖ•Â≠¶‰π†ÔºÅ

[MediaCrawlerPro](https://github.com/MediaCrawlerPro) Áõ∏ËæÉ‰∫éÂºÄÊ∫êÁâàÊú¨ÁöÑÊ†∏ÂøÉ‰ºòÂäøÔºö

#### üéØ Ê†∏ÂøÉÂäüËÉΩÂçáÁ∫ß
- ‚úÖ **Êñ≠ÁÇπÁª≠Áà¨ÂäüËÉΩ**ÔºàÈáçÁÇπÁâπÊÄßÔºâ
- ‚úÖ **Â§öË¥¶Âè∑ + IP‰ª£ÁêÜÊ±†ÊîØÊåÅ**ÔºàÈáçÁÇπÁâπÊÄßÔºâ
- ‚úÖ **ÂéªÈô§ Playwright ‰æùËµñ**Ôºå‰ΩøÁî®Êõ¥ÁÆÄÂçï
- ‚úÖ **ÂÆåÊï¥ Linux ÁéØÂ¢ÉÊîØÊåÅ**

#### üèóÔ∏è Êû∂ÊûÑËÆæËÆ°‰ºòÂåñ
- ‚úÖ **‰ª£Á†ÅÈáçÊûÑ‰ºòÂåñ**ÔºåÊõ¥ÊòìËØªÊòìÁª¥Êä§ÔºàËß£ËÄ¶ JS Á≠æÂêçÈÄªËæëÔºâ
- ‚úÖ **‰ºÅ‰∏öÁ∫ß‰ª£Á†ÅË¥®Èáè**ÔºåÈÄÇÂêàÊûÑÂª∫Â§ßÂûãÁà¨Ëô´È°πÁõÆ
- ‚úÖ **ÂÆåÁæéÊû∂ÊûÑËÆæËÆ°**ÔºåÈ´òÊâ©Â±ïÊÄßÔºåÊ∫êÁ†ÅÂ≠¶‰π†‰ª∑ÂÄºÊõ¥Â§ß

#### üéÅ È¢ùÂ§ñÂäüËÉΩ
- ‚úÖ **Ëá™Â™í‰ΩìËßÜÈ¢ë‰∏ãËΩΩÂô®Ê°åÈù¢Á´Ø**ÔºàÈÄÇÂêàÂ≠¶‰π†ÂÖ®Ê†àÂºÄÂèëÔºâ
- ‚úÖ **Â§öÂπ≥Âè∞È¶ñÈ°µ‰ø°ÊÅØÊµÅÊé®Ëçê**ÔºàHomeFeedÔºâ
- [ ] **Âü∫‰∫éËá™Â™í‰ΩìÂπ≥Âè∞ÁöÑAI AgentÊ≠£Âú®ÂºÄÂèë‰∏≠ üöÄüöÄ**

ÁÇπÂáªÊü•ÁúãÔºö[MediaCrawlerPro È°πÁõÆ‰∏ªÈ°µ](https://github.com/MediaCrawlerPro) Êõ¥Â§ö‰ªãÁªç


## üöÄ Âø´ÈÄüÂºÄÂßã

&gt; üí° **ÂºÄÊ∫ê‰∏çÊòìÔºåÂ¶ÇÊûúËøô‰∏™È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåËØ∑Áªô‰∏™ ‚≠ê Star ÊîØÊåÅ‰∏Ä‰∏ãÔºÅ**

## üìã ÂâçÁΩÆ‰æùËµñ

### üöÄ uv ÂÆâË£ÖÔºàÊé®ËçêÔºâ

Âú®ËøõË°å‰∏ã‰∏ÄÊ≠•Êìç‰Ωú‰πãÂâçÔºåËØ∑Á°Æ‰øùÁîµËÑë‰∏äÂ∑≤ÁªèÂÆâË£Ö‰∫Ü uvÔºö

- **ÂÆâË£ÖÂú∞ÂùÄ**Ôºö[uv ÂÆòÊñπÂÆâË£ÖÊåáÂçó](https://docs.astral.sh/uv/getting-started/installation)
- **È™åËØÅÂÆâË£Ö**ÔºöÁªàÁ´ØËæìÂÖ•ÂëΩ‰ª§ `uv --version`ÔºåÂ¶ÇÊûúÊ≠£Â∏∏ÊòæÁ§∫ÁâàÊú¨Âè∑ÔºåËØÅÊòéÂ∑≤ÁªèÂÆâË£ÖÊàêÂäü
- **Êé®ËçêÁêÜÁî±**Ôºöuv ÊòØÁõÆÂâçÊúÄÂº∫ÁöÑ Python ÂåÖÁÆ°ÁêÜÂ∑•ÂÖ∑ÔºåÈÄüÂ∫¶Âø´„ÄÅ‰æùËµñËß£ÊûêÂáÜÁ°Æ

### üü¢ Node.js ÂÆâË£Ö

È°πÁõÆ‰æùËµñ Node.jsÔºåËØ∑ÂâçÂæÄÂÆòÁΩë‰∏ãËΩΩÂÆâË£ÖÔºö

- **‰∏ãËΩΩÂú∞ÂùÄ**Ôºöhttps://nodejs.org/en/download/
- **ÁâàÊú¨Ë¶ÅÊ±Ç**Ôºö&gt;= 16.0.0

### üì¶ Python ÂåÖÂÆâË£Ö

```shell
# ËøõÂÖ•È°πÁõÆÁõÆÂΩï
cd MediaCrawler

# ‰ΩøÁî® uv sync ÂëΩ‰ª§Êù•‰øùËØÅ python ÁâàÊú¨ÂíåÁõ∏ÂÖ≥‰æùËµñÂåÖÁöÑ‰∏ÄËá¥ÊÄß
uv sync
```

### üåê ÊµèËßàÂô®È©±Âä®ÂÆâË£Ö

```shell
# ÂÆâË£ÖÊµèËßàÂô®È©±Âä®
uv run playwright install
```

&gt; **üí° ÊèêÁ§∫**ÔºöMediaCrawler ÁõÆÂâçÂ∑≤ÁªèÊîØÊåÅ‰ΩøÁî® playwright ËøûÊé•‰Ω†Êú¨Âú∞ÁöÑ Chrome ÊµèËßàÂô®‰∫ÜÔºå‰∏Ä‰∫õÂõ†‰∏∫ Webdriver ÂØºËá¥ÁöÑÈóÆÈ¢òËøéÂàÉËÄåËß£‰∫Ü„ÄÇ
&gt;
&gt; ÁõÆÂâçÂºÄÊîæ‰∫Ü `xhs` Âíå `dy` Ëøô‰∏§‰∏™‰ΩøÁî® CDP ÁöÑÊñπÂºèËøûÊé•Êú¨Âú∞ÊµèËßàÂô®ÔºåÂ¶ÇÊúâÈúÄË¶ÅÔºåÊü•Áúã `config/base_config.py` ‰∏≠ÁöÑÈÖçÁΩÆÈ°π„ÄÇ

## üöÄ ËøêË°åÁà¨Ëô´Á®ãÂ∫è

```shell
# È°πÁõÆÈªòËÆ§ÊòØÊ≤°ÊúâÂºÄÂêØËØÑËÆ∫Áà¨ÂèñÊ®°ÂºèÔºåÂ¶ÇÈúÄËØÑËÆ∫ËØ∑Âú® config/base_config.py ‰∏≠ÁöÑ ENABLE_GET_COMMENTS ÂèòÈáè‰øÆÊîπ
# ‰∏Ä‰∫õÂÖ∂‰ªñÊîØÊåÅÈ°πÔºå‰πüÂèØ‰ª•Âú® config/base_config.py Êü•ÁúãÂäüËÉΩÔºåÂÜôÁöÑÊúâ‰∏≠ÊñáÊ≥®Èáä

# ‰ªéÈÖçÁΩÆÊñá‰ª∂‰∏≠ËØªÂèñÂÖ≥ÈîÆËØçÊêúÁ¥¢Áõ∏ÂÖ≥ÁöÑÂ∏ñÂ≠êÂπ∂Áà¨ÂèñÂ∏ñÂ≠ê‰ø°ÊÅØ‰∏éËØÑËÆ∫
uv run main.py --platform xhs --lt qrcode --type search

# ‰ªéÈÖçÁΩÆÊñá‰ª∂‰∏≠ËØªÂèñÊåáÂÆöÁöÑÂ∏ñÂ≠êIDÂàóË°®Ëé∑ÂèñÊåáÂÆöÂ∏ñÂ≠êÁöÑ‰ø°ÊÅØ‰∏éËØÑËÆ∫‰ø°ÊÅØ
uv run main.py --platform xhs --lt qrcode --type detail

# ÊâìÂºÄÂØπÂ∫îAPPÊâ´‰∫åÁª¥Á†ÅÁôªÂΩï

# ÂÖ∂‰ªñÂπ≥Âè∞Áà¨Ëô´‰ΩøÁî®Á§∫‰æãÔºåÊâßË°å‰∏ãÈù¢ÁöÑÂëΩ‰ª§Êü•Áúã
uv run main.py --help
```

&lt;details&gt;
&lt;summary&gt;üîó &lt;strong&gt;‰ΩøÁî® Python ÂéüÁîü venv ÁÆ°ÁêÜÁéØÂ¢ÉÔºà‰∏çÊé®ËçêÔºâ&lt;/strong&gt;&lt;/summary&gt;

#### ÂàõÂª∫Âπ∂ÊøÄÊ¥ª Python ËôöÊãüÁéØÂ¢É

&gt; Â¶ÇÊûúÊòØÁà¨ÂèñÊäñÈü≥ÂíåÁü•‰πéÔºåÈúÄË¶ÅÊèêÂâçÂÆâË£Ö nodejs ÁéØÂ¢ÉÔºåÁâàÊú¨Â§ß‰∫éÁ≠â‰∫éÔºö`16` Âç≥ÂèØ

```shell
# ËøõÂÖ•È°πÁõÆÊ†πÁõÆÂΩï
cd MediaCrawler

# ÂàõÂª∫ËôöÊãüÁéØÂ¢É
# ÊàëÁöÑ python ÁâàÊú¨ÊòØÔºö3.9.6Ôºårequirements.txt ‰∏≠ÁöÑÂ∫ìÊòØÂü∫‰∫éËøô‰∏™ÁâàÊú¨ÁöÑ
# Â¶ÇÊûúÊòØÂÖ∂‰ªñ python ÁâàÊú¨ÔºåÂèØËÉΩ requirements.txt ‰∏≠ÁöÑÂ∫ì‰∏çÂÖºÂÆπÔºåÈúÄËá™Ë°åËß£ÂÜ≥
python -m venv venv

# macOS &amp; Linux ÊøÄÊ¥ªËôöÊãüÁéØÂ¢É
source venv/bin/activate

# Windows ÊøÄÊ¥ªËôöÊãüÁéØÂ¢É
venv\Scripts\activate
```

#### ÂÆâË£Ö‰æùËµñÂ∫ì

```shell
pip install -r requirements.txt
```

#### ÂÆâË£Ö playwright ÊµèËßàÂô®È©±Âä®

```shell
playwright install
```

#### ËøêË°åÁà¨Ëô´Á®ãÂ∫èÔºàÂéüÁîüÁéØÂ¢ÉÔºâ

```shell
# È°πÁõÆÈªòËÆ§ÊòØÊ≤°ÊúâÂºÄÂêØËØÑËÆ∫Áà¨ÂèñÊ®°ÂºèÔºåÂ¶ÇÈúÄËØÑËÆ∫ËØ∑Âú® config/base_config.py ‰∏≠ÁöÑ ENABLE_GET_COMMENTS ÂèòÈáè‰øÆÊîπ
# ‰∏Ä‰∫õÂÖ∂‰ªñÊîØÊåÅÈ°πÔºå‰πüÂèØ‰ª•Âú® config/base_config.py Êü•ÁúãÂäüËÉΩÔºåÂÜôÁöÑÊúâ‰∏≠ÊñáÊ≥®Èáä

# ‰ªéÈÖçÁΩÆÊñá‰ª∂‰∏≠ËØªÂèñÂÖ≥ÈîÆËØçÊêúÁ¥¢Áõ∏ÂÖ≥ÁöÑÂ∏ñÂ≠êÂπ∂Áà¨ÂèñÂ∏ñÂ≠ê‰ø°ÊÅØ‰∏éËØÑËÆ∫
python main.py --platform xhs --lt qrcode --type search

# ‰ªéÈÖçÁΩÆÊñá‰ª∂‰∏≠ËØªÂèñÊåáÂÆöÁöÑÂ∏ñÂ≠êIDÂàóË°®Ëé∑ÂèñÊåáÂÆöÂ∏ñÂ≠êÁöÑ‰ø°ÊÅØ‰∏éËØÑËÆ∫‰ø°ÊÅØ
python main.py --platform xhs --lt qrcode --type detail

# ÊâìÂºÄÂØπÂ∫îAPPÊâ´‰∫åÁª¥Á†ÅÁôªÂΩï

# ÂÖ∂‰ªñÂπ≥Âè∞Áà¨Ëô´‰ΩøÁî®Á§∫‰æãÔºåÊâßË°å‰∏ãÈù¢ÁöÑÂëΩ‰ª§Êü•Áúã
python main.py --help
```

&lt;/details&gt;


## üíæ Êï∞ÊçÆ‰øùÂ≠ò

ÊîØÊåÅÂ§öÁßçÊï∞ÊçÆÂ≠òÂÇ®ÊñπÂºèÔºö
- **CSV Êñá‰ª∂**ÔºöÊîØÊåÅ‰øùÂ≠òÂà∞ CSV ‰∏≠Ôºà`data/` ÁõÆÂΩï‰∏ãÔºâ
- **JSON Êñá‰ª∂**ÔºöÊîØÊåÅ‰øùÂ≠òÂà∞ JSON ‰∏≠Ôºà`data/` ÁõÆÂΩï‰∏ãÔºâ
- **Êï∞ÊçÆÂ∫ìÂ≠òÂÇ®**
  - ‰ΩøÁî®ÂèÇÊï∞ `--init_db` ËøõË°åÊï∞ÊçÆÂ∫ìÂàùÂßãÂåñÔºà‰ΩøÁî®`--init_db`Êó∂‰∏çÈúÄË¶ÅÊê∫Â∏¶ÂÖ∂‰ªñoptionalÔºâ
  - **SQLite Êï∞ÊçÆÂ∫ì**ÔºöËΩªÈáèÁ∫ßÊï∞ÊçÆÂ∫ìÔºåÊó†ÈúÄÊúçÂä°Âô®ÔºåÈÄÇÂêà‰∏™‰∫∫‰ΩøÁî®ÔºàÊé®ËçêÔºâ
    1. ÂàùÂßãÂåñÔºö`--init_db sqlite`
    2. Êï∞ÊçÆÂ≠òÂÇ®Ôºö`--save_data_option sqlite`
  - **MySQL Êï∞ÊçÆÂ∫ì**ÔºöÊîØÊåÅÂÖ≥Á≥ªÂûãÊï∞ÊçÆÂ∫ì MySQL ‰∏≠‰øùÂ≠òÔºàÈúÄË¶ÅÊèêÂâçÂàõÂª∫Êï∞ÊçÆÂ∫ìÔºâ
    1. ÂàùÂßãÂåñÔºö`--init_db mysql`
    2. Êï∞ÊçÆÂ≠òÂÇ®Ôºö`--save_data_option db`Ôºàdb ÂèÇÊï∞‰∏∫ÂÖºÂÆπÂéÜÂè≤Êõ¥Êñ∞‰øùÁïôÔºâ


### ‰ΩøÁî®Á§∫‰æãÔºö
```shell
# ÂàùÂßãÂåñ SQLite Êï∞ÊçÆÂ∫ìÔºà‰ΩøÁî®&#039;--init_db&#039;Êó∂‰∏çÈúÄË¶ÅÊê∫Â∏¶ÂÖ∂‰ªñoptionalÔºâ
uv run main.py --init_db sqlite
# ‰ΩøÁî® SQLite Â≠òÂÇ®Êï∞ÊçÆÔºàÊé®Ëçê‰∏™‰∫∫Áî®Êà∑‰ΩøÁî®Ôºâ
uv run main.py --platform xhs --lt qrcode --type search --save_data_option sqlite
```
```shell
# ÂàùÂßãÂåñ MySQL Êï∞ÊçÆÂ∫ì
uv run main.py --init_db mysql
# ‰ΩøÁî® MySQL Â≠òÂÇ®Êï∞ÊçÆÔºà‰∏∫ÈÄÇÈÖçÂéÜÂè≤Êõ¥Êñ∞ÔºådbÂèÇÊï∞ËøõË°åÊ≤øÁî®Ôºâ
uv run main.py --platform xhs --lt qrcode --type search --save_data_option db
```


[üöÄ MediaCrawlerPro ÈáçÁ£ÖÂèëÂ∏É üöÄÔºÅÊõ¥Â§öÁöÑÂäüËÉΩÔºåÊõ¥Â•ΩÁöÑÊû∂ÊûÑËÆæËÆ°ÔºÅ](https://github.com/MediaCrawlerPro)

---

### üí∞ ËµûÂä©ÂïÜÂ±ïÁ§∫

&lt;a href=&quot;https://www.swiftproxy.net/?ref=nanmi&quot;&gt;
&lt;img src=&quot;docs/static/images/img_5.png&quot;&gt;
&lt;br&gt;
Swiftproxy - 90M+ ÂÖ®ÁêÉÈ´òË¥®ÈáèÁ∫ØÂáÄ‰ΩèÂÆÖIPÔºåÊ≥®ÂÜåÂèØÈ¢ÜÂÖçË¥π 500MB ÊµãËØïÊµÅÈáèÔºåÂä®ÊÄÅÊµÅÈáè‰∏çËøáÊúüÔºÅ
&gt; ‰∏ìÂ±ûÊäòÊâ£Á†ÅÔºö**GHB5** Á´ã‰∫´‰πùÊäò‰ºòÊÉ†ÔºÅ
&lt;/a&gt;

&lt;br&gt;
&lt;br&gt;

&lt;a href=&quot;https://h.wandouip.com&quot;&gt;
&lt;img src=&quot;docs/static/images/img_8.jpg&quot;&gt;
&lt;br&gt;
Ë±åË±ÜHTTPËá™Ëê•ÂçÉ‰∏áÁ∫ßIPËµÑÊ∫êÊ±†ÔºåIPÁ∫ØÂáÄÂ∫¶‚â•99.8%ÔºåÊØèÊó•‰øùÊåÅIPÈ´òÈ¢ëÊõ¥Êñ∞ÔºåÂø´ÈÄüÂìçÂ∫îÔºåÁ®≥ÂÆöËøûÊé•ÔºåÊª°Ë∂≥Â§öÁßç‰∏öÂä°Âú∫ÊôØÔºåÊîØÊåÅÊåâÈúÄÂÆöÂà∂ÔºåÊ≥®ÂÜåÂÖçË¥πÊèêÂèñ10000ip„ÄÇ
&lt;/a&gt;



### ü§ù Êàê‰∏∫ËµûÂä©ËÄÖ

Êàê‰∏∫ËµûÂä©ËÄÖÔºåÂèØ‰ª•Â∞ÜÊÇ®ÁöÑ‰∫ßÂìÅÂ±ïÁ§∫Âú®ËøôÈáåÔºåÊØèÂ§©Ëé∑ÂæóÂ§ßÈáèÊõùÂÖâÔºÅ

**ËÅîÁ≥ªÊñπÂºè**Ôºö
- ÂæÆ‰ø°Ôºö`yzglan`
- ÈÇÆÁÆ±Ôºö`relakkes@gmail.com`


## ü§ù Á§æÂå∫‰∏éÊîØÊåÅ

### üí¨ ‰∫§ÊµÅÁæ§ÁªÑ
- **ÂæÆ‰ø°‰∫§ÊµÅÁæ§**Ôºö[ÁÇπÂáªÂä†ÂÖ•](https://nanmicoder.github.io/MediaCrawler/%E5%BE%AE%E4%BF%A1%E4%BA%A4%E6%B5%81%E7%BE%A4.html)

### üìö ÊñáÊ°£‰∏éÊïôÁ®ã
- **Âú®Á∫øÊñáÊ°£**Ôºö[MediaCrawler ÂÆåÊï¥ÊñáÊ°£](https://nanmicoder.github.io/MediaCrawler/)
- **Áà¨Ëô´ÊïôÁ®ã**Ôºö[CrawlerTutorial ÂÖçË¥πÊïôÁ®ã](https://github.com/NanmiCoder/CrawlerTutorial)
  

# ÂÖ∂‰ªñÂ∏∏ËßÅÈóÆÈ¢òÂèØ‰ª•Êü•ÁúãÂú®Á∫øÊñáÊ°£
&gt; 
&gt; Âú®Á∫øÊñáÊ°£ÂåÖÂê´‰ΩøÁî®ÊñπÊ≥ï„ÄÅÂ∏∏ËßÅÈóÆÈ¢ò„ÄÅÂä†ÂÖ•È°πÁõÆ‰∫§ÊµÅÁæ§Á≠â„ÄÇ
&gt; [MediaCrawlerÂú®Á∫øÊñáÊ°£](https://nanmicoder.github.io/MediaCrawler/)
&gt; 

# ‰ΩúËÄÖÊèê‰æõÁöÑÁü•ËØÜÊúçÂä°
&gt; Â¶ÇÊûúÊÉ≥Âø´ÈÄüÂÖ•Èó®ÂíåÂ≠¶‰π†ËØ•È°πÁõÆÁöÑ‰ΩøÁî®„ÄÅÊ∫êÁ†ÅÊû∂ÊûÑËÆæËÆ°Á≠â„ÄÅÂ≠¶‰π†ÁºñÁ®ãÊäÄÊúØ„ÄÅ‰∫¶ÊàñËÄÖÊÉ≥‰∫ÜËß£MediaCrawlerProÁöÑÊ∫ê‰ª£Á†ÅËÆæËÆ°ÂèØ‰ª•Áúã‰∏ãÊàëÁöÑÁü•ËØÜ‰ªòË¥πÊ†èÁõÆ„ÄÇ

[‰ΩúËÄÖÁöÑÁü•ËØÜ‰ªòË¥πÊ†èÁõÆ‰ªãÁªç](https://nanmicoder.github.io/MediaCrawler/%E7%9F%A5%E8%AF%86%E4%BB%98%E8%B4%B9%E4%BB%8B%E7%BB%8D.html)


---

## ‚≠ê Star Ë∂ãÂäøÂõæ

Â¶ÇÊûúËøô‰∏™È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåËØ∑Áªô‰∏™ ‚≠ê Star ÊîØÊåÅ‰∏Ä‰∏ãÔºåËÆ©Êõ¥Â§öÁöÑ‰∫∫ÁúãÂà∞ MediaCrawlerÔºÅ

[![Star History Chart](https://api.star-history.com/svg?repos=NanmiCoder/MediaCrawler&amp;type=Date)](https://star-history.com/#NanmiCoder/MediaCrawler&amp;Date)



## üìö ÂèÇËÄÉ

- **Â∞èÁ∫¢‰π¶ÂÆ¢Êà∑Á´Ø**Ôºö[ReaJason ÁöÑ xhs ‰ªìÂ∫ì](https://github.com/ReaJason/xhs)
- **Áü≠‰ø°ËΩ¨Âèë**Ôºö[SmsForwarder ÂèÇËÄÉ‰ªìÂ∫ì](https://github.com/pppscn/SmsForwarder)
- **ÂÜÖÁΩëÁ©øÈÄèÂ∑•ÂÖ∑**Ôºö[ngrok ÂÆòÊñπÊñáÊ°£](https://ngrok.com/docs/)


# ÂÖçË¥£Â£∞Êòé
&lt;div id=&quot;disclaimer&quot;&gt; 

## 1. È°πÁõÆÁõÆÁöÑ‰∏éÊÄßË¥®
Êú¨È°πÁõÆÔºà‰ª•‰∏ãÁÆÄÁß∞‚ÄúÊú¨È°πÁõÆ‚ÄùÔºâÊòØ‰Ωú‰∏∫‰∏Ä‰∏™ÊäÄÊúØÁ†îÁ©∂‰∏éÂ≠¶‰π†Â∑•ÂÖ∑ËÄåÂàõÂª∫ÁöÑÔºåÊó®Âú®Êé¢Á¥¢ÂíåÂ≠¶‰π†ÁΩëÁªúÊï∞ÊçÆÈááÈõÜÊäÄÊúØ„ÄÇÊú¨È°πÁõÆ‰∏ìÊ≥®‰∫éËá™Â™í‰ΩìÂπ≥Âè∞ÁöÑÊï∞ÊçÆÁà¨ÂèñÊäÄÊúØÁ†îÁ©∂ÔºåÊó®Âú®Êèê‰æõÁªôÂ≠¶‰π†ËÄÖÂíåÁ†îÁ©∂ËÄÖ‰Ωú‰∏∫ÊäÄÊúØ‰∫§ÊµÅ‰πãÁî®„ÄÇ

## 2. Ê≥ïÂæãÂêàËßÑÊÄßÂ£∞Êòé
Êú¨È°πÁõÆÂºÄÂèëËÄÖÔºà‰ª•‰∏ãÁÆÄÁß∞‚ÄúÂºÄÂèëËÄÖ‚ÄùÔºâÈÉëÈáçÊèêÈÜíÁî®Êà∑Âú®‰∏ãËΩΩ„ÄÅÂÆâË£ÖÂíå‰ΩøÁî®Êú¨È°πÁõÆÊó∂Ôºå‰∏•Ê†ºÈÅµÂÆà‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÁõ∏ÂÖ≥Ê≥ïÂæãÊ≥ïËßÑÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫é„Ää‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÁΩëÁªúÂÆâÂÖ®Ê≥ï„Äã„ÄÅ„Ää‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÂèçÈó¥Ë∞çÊ≥ï„ÄãÁ≠âÊâÄÊúâÈÄÇÁî®ÁöÑÂõΩÂÆ∂Ê≥ïÂæãÂíåÊîøÁ≠ñ„ÄÇÁî®Êà∑Â∫îËá™Ë°åÊâøÊãÖ‰∏ÄÂàáÂõ†‰ΩøÁî®Êú¨È°πÁõÆËÄåÂèØËÉΩÂºïËµ∑ÁöÑÊ≥ïÂæãË¥£‰ªª„ÄÇ

## 3. ‰ΩøÁî®ÁõÆÁöÑÈôêÂà∂
Êú¨È°πÁõÆ‰∏•Á¶ÅÁî®‰∫é‰ªª‰ΩïÈùûÊ≥ïÁõÆÁöÑÊàñÈùûÂ≠¶‰π†„ÄÅÈùûÁ†îÁ©∂ÁöÑÂïÜ‰∏öË°å‰∏∫„ÄÇÊú¨È°πÁõÆ‰∏çÂæóÁî®‰∫é‰ªª‰ΩïÂΩ¢ÂºèÁöÑÈùûÊ≥ï‰æµÂÖ•‰ªñ‰∫∫ËÆ°ÁÆóÊú∫Á≥ªÁªüÔºå‰∏çÂæóÁî®‰∫é‰ªª‰Ωï‰æµÁäØ‰ªñ‰∫∫Áü•ËØÜ‰∫ßÊùÉÊàñÂÖ∂‰ªñÂêàÊ≥ïÊùÉÁõäÁöÑË°å‰∏∫„ÄÇÁî®Êà∑Â∫î‰øùËØÅÂÖ∂‰ΩøÁî®Êú¨È°πÁõÆÁöÑÁõÆÁöÑÁ∫ØÂ±û‰∏™‰∫∫Â≠¶‰π†ÂíåÊäÄÊúØÁ†îÁ©∂Ôºå‰∏çÂæóÁî®‰∫é‰ªª‰ΩïÂΩ¢ÂºèÁöÑÈùûÊ≥ïÊ¥ªÂä®„ÄÇ

## 4. ÂÖçË¥£Â£∞Êòé
ÂºÄÂèëËÄÖÂ∑≤Â∞ΩÊúÄÂ§ßÂä™ÂäõÁ°Æ‰øùÊú¨È°πÁõÆÁöÑÊ≠£ÂΩìÊÄßÂèäÂÆâÂÖ®ÊÄßÔºå‰ΩÜ‰∏çÂØπÁî®Êà∑‰ΩøÁî®Êú¨È°πÁõÆÂèØËÉΩÂºïËµ∑ÁöÑ‰ªª‰ΩïÂΩ¢ÂºèÁöÑÁõ¥Êé•ÊàñÈó¥Êé•ÊçüÂ§±ÊâøÊãÖË¥£‰ªª„ÄÇÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÁî±‰∫é‰ΩøÁî®Êú¨È°πÁõÆËÄåÂØºËá¥ÁöÑ‰ªª‰ΩïÊï∞ÊçÆ‰∏¢Â§±„ÄÅËÆæÂ§áÊçüÂùè„ÄÅÊ≥ïÂæãËØâËÆºÁ≠â„ÄÇ

## 5. Áü•ËØÜ‰∫ßÊùÉÂ£∞Êòé
Êú¨È°πÁõÆÁöÑÁü•ËØÜ‰∫ßÊùÉÂΩíÂºÄÂèëËÄÖÊâÄÊúâ„ÄÇÊú¨È°πÁõÆÂèóÂà∞Ëëó‰ΩúÊùÉÊ≥ïÂíåÂõΩÈôÖËëó‰ΩúÊùÉÊù°Á∫¶‰ª•ÂèäÂÖ∂‰ªñÁü•ËØÜ‰∫ßÊùÉÊ≥ïÂæãÂíåÊù°Á∫¶ÁöÑ‰øùÊä§„ÄÇÁî®Êà∑Âú®ÈÅµÂÆàÊú¨Â£∞ÊòéÂèäÁõ∏ÂÖ≥Ê≥ïÂæãÊ≥ïËßÑÁöÑÂâçÊèê‰∏ãÔºåÂèØ‰ª•‰∏ãËΩΩÂíå‰ΩøÁî®Êú¨È°πÁõÆ„ÄÇ

## 6. ÊúÄÁªàËß£ÈáäÊùÉ
ÂÖ≥‰∫éÊú¨È°πÁõÆÁöÑÊúÄÁªàËß£ÈáäÊùÉÂΩíÂºÄÂèëËÄÖÊâÄÊúâ„ÄÇÂºÄÂèëËÄÖ‰øùÁïôÈöèÊó∂Êõ¥ÊîπÊàñÊõ¥Êñ∞Êú¨ÂÖçË¥£Â£∞ÊòéÁöÑÊùÉÂà©ÔºåÊÅï‰∏çÂè¶Ë°åÈÄöÁü•„ÄÇ
&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>