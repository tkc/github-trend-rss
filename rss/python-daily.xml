<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Mon, 01 Sep 2025 00:05:07 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[QuentinFuxa/WhisperLiveKit]]></title>
            <link>https://github.com/QuentinFuxa/WhisperLiveKit</link>
            <guid>https://github.com/QuentinFuxa/WhisperLiveKit</guid>
            <pubDate>Mon, 01 Sep 2025 00:05:07 GMT</pubDate>
            <description><![CDATA[Real-time & local speech-to-text, translation, and speaker diarization. With server & web UI.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/QuentinFuxa/WhisperLiveKit">QuentinFuxa/WhisperLiveKit</a></h1>
            <p>Real-time & local speech-to-text, translation, and speaker diarization. With server & web UI.</p>
            <p>Language: Python</p>
            <p>Stars: 4,440</p>
            <p>Forks: 411</p>
            <p>Stars today: 445 stars today</p>
            <h2>README</h2><pre>&lt;h1 align=&quot;center&quot;&gt;WhisperLiveKit&lt;/h1&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/QuentinFuxa/WhisperLiveKit/refs/heads/main/demo.png&quot; alt=&quot;WhisperLiveKit Demo&quot; width=&quot;730&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;b&gt;Real-time, Fully Local Speech-to-Text with Speaker Identification&lt;/b&gt;&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://pypi.org/project/whisperlivekit/&quot;&gt;&lt;img alt=&quot;PyPI Version&quot; src=&quot;https://img.shields.io/pypi/v/whisperlivekit?color=g&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://pepy.tech/project/whisperlivekit&quot;&gt;&lt;img alt=&quot;PyPI Downloads&quot; src=&quot;https://static.pepy.tech/personalized-badge/whisperlivekit?period=total&amp;units=international_system&amp;left_color=grey&amp;right_color=brightgreen&amp;left_text=installations&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://pypi.org/project/whisperlivekit/&quot;&gt;&lt;img alt=&quot;Python Versions&quot; src=&quot;https://img.shields.io/badge/python-3.9--3.13-dark_green&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/QuentinFuxa/WhisperLiveKit/blob/main/LICENSE&quot;&gt;&lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/badge/License-MIT/Dual Licensed-dark_green&quot;&gt;&lt;/a&gt;
&lt;/p&gt;


Real-time speech transcription directly to your browser, with a ready-to-use backend+server and a simple frontend. ✨

#### Powered by Leading Research:

- [SimulStreaming](https://github.com/ufal/SimulStreaming) (SOTA 2025) - Ultra-low latency transcription with AlignAtt policy
- [WhisperStreaming](https://github.com/ufal/whisper_streaming) (SOTA 2023) - Low latency transcription with LocalAgreement policy
- [Streaming Sortformer](https://arxiv.org/abs/2507.18446) (SOTA 2025) - Advanced real-time speaker diarization
- [Diart](https://github.com/juanmc2005/diart) (SOTA 2021) - Real-time speaker diarization
- [Silero VAD](https://github.com/snakers4/silero-vad) (2024) - Enterprise-grade Voice Activity Detection


&gt; **Why not just run a simple Whisper model on every audio batch?** Whisper is designed for complete utterances, not real-time chunks. Processing small segments loses context, cuts off words mid-syllable, and produces poor transcription. WhisperLiveKit uses state-of-the-art simultaneous speech research for intelligent buffering and incremental processing.


### Architecture

&lt;img alt=&quot;Architecture&quot; src=&quot;https://raw.githubusercontent.com/QuentinFuxa/WhisperLiveKit/refs/heads/main/architecture.png&quot; /&gt;

*The backend supports multiple concurrent users. Voice Activity Detection reduces overhead when no voice is detected.*

### Installation &amp; Quick Start

```bash
pip install whisperlivekit
```

&gt;  **FFmpeg is required** and must be installed before using WhisperLiveKit
&gt; 
&gt; | OS | How to install |
&gt; |-----------|-------------|
&gt;  | Ubuntu/Debian | `sudo apt install ffmpeg` |
&gt; | MacOS | `brew install ffmpeg` |
&gt; | Windows | Download .exe from https://ffmpeg.org/download.html and add to PATH |

#### Quick Start
1. **Start the transcription server:**
   ```bash
   whisperlivekit-server --model base --language en
   ```

2. **Open your browser** and navigate to `http://localhost:8000`. Start speaking and watch your words appear in real-time!


&gt; - See [tokenizer.py](https://github.com/QuentinFuxa/WhisperLiveKit/blob/main/whisperlivekit/simul_whisper/whisper/tokenizer.py) for the list of all available languages.
&gt; - For HTTPS requirements, see the **Parameters** section for SSL configuration options.

 

#### Optional Dependencies

| Optional | `pip install` |
|-----------|-------------|
| **Speaker diarization with Sortformer** | `git+https://github.com/NVIDIA/NeMo.git@main#egg=nemo_toolkit[asr]` |
| Speaker diarization with Diart | `diart` |
| Original Whisper backend | `whisper` |
| Improved timestamps backend | `whisper-timestamped` |
| Apple Silicon optimization backend | `mlx-whisper` |
| OpenAI API backend | `openai` |

See  **Parameters &amp; Configuration** below on how to use them.



### Usage Examples

**Command-line Interface**: Start the transcription server with various options:

```bash
# Use better model than default (small)
whisperlivekit-server --model large-v3

# Advanced configuration with diarization and language
whisperlivekit-server --host 0.0.0.0 --port 8000 --model medium --diarization --language fr
```


**Python API Integration**: Check [basic_server](https://github.com/QuentinFuxa/WhisperLiveKit/blob/main/whisperlivekit/basic_server.py) for a more complete example of how to use the functions and classes.

```python
from whisperlivekit import TranscriptionEngine, AudioProcessor, parse_args
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse
from contextlib import asynccontextmanager
import asyncio

transcription_engine = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global transcription_engine
    transcription_engine = TranscriptionEngine(model=&quot;medium&quot;, diarization=True, lan=&quot;en&quot;)
    yield

app = FastAPI(lifespan=lifespan)

async def handle_websocket_results(websocket: WebSocket, results_generator):
    async for response in results_generator:
        await websocket.send_json(response)
    await websocket.send_json({&quot;type&quot;: &quot;ready_to_stop&quot;})

@app.websocket(&quot;/asr&quot;)
async def websocket_endpoint(websocket: WebSocket):
    global transcription_engine

    # Create a new AudioProcessor for each connection, passing the shared engine
    audio_processor = AudioProcessor(transcription_engine=transcription_engine)    
    results_generator = await audio_processor.create_tasks()
    results_task = asyncio.create_task(handle_websocket_results(websocket, results_generator))
    await websocket.accept()
    while True:
        message = await websocket.receive_bytes()
        await audio_processor.process_audio(message)        
```

**Frontend Implementation**: The package includes an HTML/JavaScript implementation [here](https://github.com/QuentinFuxa/WhisperLiveKit/blob/main/whisperlivekit/web/live_transcription.html). You can also import it using `from whisperlivekit import get_inline_ui_html` &amp; `page = get_inline_ui_html()`


## Parameters &amp; Configuration

An important list of parameters can be changed. But what *should* you change?
- the `--model` size. List and recommandations [here](https://github.com/QuentinFuxa/WhisperLiveKit/blob/main/available_models.md)
- the `--language`.  List [here](https://github.com/QuentinFuxa/WhisperLiveKit/blob/main/whisperlivekit/simul_whisper/whisper/tokenizer.py). If you use `auto`, the model attempts to detect the language automatically, but it tends to bias towards English.
- the `--backend` ? you can switch to `--backend faster-whisper` if  `simulstreaming` does not work correctly or if you prefer to avoid the dual-license requirements.
- `--warmup-file`, if you have one
- `--host`, `--port`, `--ssl-certfile`, `--ssl-keyfile`, if you set up a server
- `--diarization`, if you want to use it.

The rest I don&#039;t recommend. But below are your options.

| Parameter | Description | Default |
|-----------|-------------|---------|
| `--model` | Whisper model size. | `small` |
| `--language` | Source language code or `auto` | `auto` |
| `--task` | `transcribe` or `translate` | `transcribe` |
| `--backend` | Processing backend | `simulstreaming` |
| `--min-chunk-size` | Minimum audio chunk size (seconds) | `1.0` |
| `--no-vac` | Disable Voice Activity Controller | `False` |
| `--no-vad` | Disable Voice Activity Detection | `False` |
| `--warmup-file` | Audio file path for model warmup | `jfk.wav` |
| `--host` | Server host address | `localhost` |
| `--port` | Server port | `8000` |
| `--ssl-certfile` | Path to the SSL certificate file (for HTTPS support) | `None` |
| `--ssl-keyfile` | Path to the SSL private key file (for HTTPS support) | `None` |


| WhisperStreaming backend options | Description | Default |
|-----------|-------------|---------|
| `--confidence-validation` | Use confidence scores for faster validation | `False` |
| `--buffer_trimming` | Buffer trimming strategy (`sentence` or `segment`) | `segment` |


| SimulStreaming backend options | Description | Default |
|-----------|-------------|---------|
| `--frame-threshold` | AlignAtt frame threshold (lower = faster, higher = more accurate) | `25` |
| `--beams` | Number of beams for beam search (1 = greedy decoding) | `1` |
| `--decoder` | Force decoder type (`beam` or `greedy`) | `auto` |
| `--audio-max-len` | Maximum audio buffer length (seconds) | `30.0` |
| `--audio-min-len` | Minimum audio length to process (seconds) | `0.0` |
| `--cif-ckpt-path` | Path to CIF model for word boundary detection | `None` |
| `--never-fire` | Never truncate incomplete words | `False` |
| `--init-prompt` | Initial prompt for the model | `None` |
| `--static-init-prompt` | Static prompt that doesn&#039;t scroll | `None` |
| `--max-context-tokens` | Maximum context tokens | `None` |
| `--model-path` | Direct path to .pt model file. Download it if not found | `./base.pt` |
| `--preloaded-model-count` | Optional. Number of models to preload in memory to speed up loading (set up to the expected number of concurrent users) | `1` |

| Diarization options | Description | Default |
|-----------|-------------|---------|
| `--diarization` | Enable speaker identification | `False` |
| `--diarization-backend` |  `diart` or `sortformer` | `sortformer` |
| `--segmentation-model` | Hugging Face model ID for Diart segmentation model. [Available models](https://github.com/juanmc2005/diart/tree/main?tab=readme-ov-file#pre-trained-models) | `pyannote/segmentation-3.0` |
| `--embedding-model` | Hugging Face model ID for Diart embedding model. [Available models](https://github.com/juanmc2005/diart/tree/main?tab=readme-ov-file#pre-trained-models) | `speechbrain/spkrec-ecapa-voxceleb` |


&gt; For diarization using Diart, you need access to pyannote.audio models:
&gt; 1. [Accept user conditions](https://huggingface.co/pyannote/segmentation) for the `pyannote/segmentation` model
&gt; 2. [Accept user conditions](https://huggingface.co/pyannote/segmentation-3.0) for the `pyannote/segmentation-3.0` model
&gt; 3. [Accept user conditions](https://huggingface.co/pyannote/embedding) for the `pyannote/embedding` model
&gt;4. Login with HuggingFace: `huggingface-cli login`

### 🚀 Deployment Guide

To deploy WhisperLiveKit in production:
 
1. **Server Setup**: Install production ASGI server &amp; launch with multiple workers
   ```bash
   pip install uvicorn gunicorn
   gunicorn -k uvicorn.workers.UvicornWorker -w 4 your_app:app
   ```

2. **Frontend**: Host your customized version of the `html` example &amp; ensure WebSocket connection points correctly

3. **Nginx Configuration** (recommended for production):
    ```nginx    
   server {
       listen 80;
       server_name your-domain.com;
        location / {
            proxy_pass http://localhost:8000;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection &quot;upgrade&quot;;
            proxy_set_header Host $host;
    }}
    ```

4. **HTTPS Support**: For secure deployments, use &quot;wss://&quot; instead of &quot;ws://&quot; in WebSocket URL

## 🐋 Docker

Deploy the application easily using Docker with GPU or CPU support.

### Prerequisites
- Docker installed on your system
- For GPU support: NVIDIA Docker runtime installed

### Quick Start

**With GPU acceleration (recommended):**
```bash
docker build -t wlk .
docker run --gpus all -p 8000:8000 --name wlk wlk
```

**CPU only:**
```bash
docker build -f Dockerfile.cpu -t wlk .
docker run -p 8000:8000 --name wlk wlk
```

### Advanced Usage

**Custom configuration:**
```bash
# Example with custom model and language
docker run --gpus all -p 8000:8000 --name wlk wlk --model large-v3 --language fr
```

### Memory Requirements
- **Large models**: Ensure your Docker runtime has sufficient memory allocated


#### Customization

- `--build-arg` Options:
  - `EXTRAS=&quot;whisper-timestamped&quot;` - Add extras to the image&#039;s installation (no spaces). Remember to set necessary container options!
  - `HF_PRECACHE_DIR=&quot;./.cache/&quot;` - Pre-load a model cache for faster first-time start
  - `HF_TKN_FILE=&quot;./token&quot;` - Add your Hugging Face Hub access token to download gated models

## 🔮 Use Cases
Capture discussions in real-time for meeting transcription, help hearing-impaired users follow conversations through accessibility tools, transcribe podcasts or videos automatically for content creation, transcribe support calls with speaker identification for customer service...
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[laramies/theHarvester]]></title>
            <link>https://github.com/laramies/theHarvester</link>
            <guid>https://github.com/laramies/theHarvester</guid>
            <pubDate>Mon, 01 Sep 2025 00:05:06 GMT</pubDate>
            <description><![CDATA[E-mails, subdomains and names Harvester - OSINT]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/laramies/theHarvester">laramies/theHarvester</a></h1>
            <p>E-mails, subdomains and names Harvester - OSINT</p>
            <p>Language: Python</p>
            <p>Stars: 13,718</p>
            <p>Forks: 2,270</p>
            <p>Stars today: 214 stars today</p>
            <h2>README</h2><pre>![theHarvester](https://github.com/laramies/theHarvester/blob/master/theHarvester-logo.webp)

![TheHarvester CI](https://github.com/laramies/theHarvester/workflows/TheHarvester%20Python%20CI/badge.svg) ![TheHarvester Docker Image CI](https://github.com/laramies/theHarvester/workflows/TheHarvester%20Docker%20Image%20CI/badge.svg)
[![Rawsec&#039;s CyberSecurity Inventory](https://inventory.raw.pm/img/badges/Rawsec-inventoried-FF5050_flat_without_logo.svg)](https://inventory.raw.pm/)

About
-----
theHarvester is a simple to use, yet powerful tool designed to be used during the reconnaissance stage of a red
team assessment or penetration test. It performs open source intelligence (OSINT) gathering to help determine
a domain&#039;s external threat landscape. The tool gathers names, emails, IPs, subdomains, and URLs by using
multiple public resources that include:

Install and dependencies
------------------------
* Python 3.12 or higher.
* https://github.com/laramies/theHarvester/wiki/Installation

Install uv:
   ```bash
   curl -LsSf https://astral.sh/uv/install.sh | sh
   ```

Clone the repository:
   ```bash
   git clone https://github.com/laramies/theHarvester
   cd theHarvester
   ```

Install dependencies and create a virtual environment:
   ```bash
   uv sync
   ```

Run theHarvester:
   ```bash
   uv run theHarvester
   ```

## Development

To install development dependencies:
```bash
uv sync --extra dev
```

To run tests:
```bash
uv run pytest
```

To run linting and formatting:
```bash
uv run ruff check
```
```bash
uv run ruff format
```

Passive modules
---------------

* baidu: Baidu search engine (https://www.baidu.com)

* bevigil: CloudSEK BeVigil scans mobile application for OSINT assets (https://bevigil.com/osint-api)

* brave: Brave search engine - now uses official Brave Search API (https://api-dashboard.search.brave.com)

* bufferoverun: Fast domain name lookups for TLS certificates in IPv4 space (https://tls.bufferover.run)

* builtwith: Find out what websites are built with (https://builtwith.com)

* censys: Uses certificates searches to enumerate subdomains and gather emails (https://censys.io)

* certspotter: Cert Spotter monitors Certificate Transparency logs (https://sslmate.com/certspotter)

* criminalip: Specialized Cyber Threat Intelligence (CTI) search engine (https://www.criminalip.io)

* crtsh: Comodo Certificate search (https://crt.sh)

* dehashed: Take your data security to the next level is (https://dehashed.com)

* dnsdumpster: Domain research tool that can discover hosts related to a domain (https://dnsdumpster.com)

* duckduckgo: DuckDuckGo search engine (https://duckduckgo.com)

* fullhunt: Next-generation attack surface security platform (https://fullhunt.io)

* github-code: GitHub code search engine (https://www.github.com)

* hackertarget: Online vulnerability scanners and network intelligence to help organizations (https://hackertarget.com)

* haveibeenpwned: Check if your email address is in a data breach (https://haveibeenpwned.com)

* hunter: Hunter search engine (https://hunter.io)

* hunterhow: Internet search engines for security researchers (https://hunter.how)

* intelx: Intelx search engine (https://intelx.io)

* leaklookup: Data breach search engine (https://leak-lookup.com)

* netlas: A Shodan or Censys competitor (https://app.netlas.io)

* onyphe: Cyber defense search engine (https://www.onyphe.io)

* otx: AlienVault open threat exchange (https://otx.alienvault.com)

* pentesttools: Cloud-based toolkit for offensive security testing, focused on web applications and network penetration testing (https://pentest-tools.com)

* projecdiscovery: Actively collects and maintains internet-wide assets data, to enhance research and analyse changes around DNS for better insights (https://chaos.projectdiscovery.io)

* rapiddns: DNS query tool which make querying subdomains or sites of a same IP easy (https://rapiddns.io)

* rocketreach: Access real-time verified personal/professional emails, phone numbers, and social media links (https://rocketreach.co)

* securityscorecard: helps TPRM and SOC teams detect, prioritize, and remediate vendor risk across their entire supplier ecosystem at scale (https://securityscorecard.com)

* securityTrails: Security Trails search engine, the world&#039;s largest repository of historical DNS data (https://securitytrails.com)

* -s, --shodan: Shodan search engine will search for ports and banners from discovered hosts (https://shodan.io)

* subdomaincenter: A subdomain finder tool used to find subdomains of a given domain (https://www.subdomain.center)

* subdomainfinderc99: A subdomain finder is a tool used to find the subdomains of a given domain (https://subdomainfinder.c99.nl)

* threatminer: Data mining for threat intelligence (https://www.threatminer.org)

* tomba: Tomba search engine (https://tomba.io)

* urlscan: A sandbox for the web that is a URL and website scanner (https://urlscan.io)

* venacus: Venacus search engine (https://venacus.com)

* virustotal: Domain search (https://www.virustotal.com)

* whoisxml: Subdomain search (https://subdomains.whoisxmlapi.com/api/pricing)

* yahoo: Yahoo search engine (https://www.yahoo.com)

* zoomeye: China&#039;s version of Shodan (https://www.zoomeye.org)

Active modules
--------------
* DNS brute force: dictionary brute force enumeration
* Screenshots: Take screenshots of subdomains that were found

Modules that require an API key
-------------------------------
Documentation to setup API keys can be found at - https://github.com/laramies/theHarvester/wiki/Installation#api-keys

* bevigil - 50 free queries/month, 1k queries/month $50
* brave - Free plan available, Pro plans for higher limits
* bufferoverun - 100 free queries/month, 10k/month $25
* builtwith - 50 free queries ever, $2950/yr
* censys - 500 credits $100
* criminalip - 100 free queries/month, 700k/month $59
* dehashed - 500 credts $15, 5k credits $150
* dnsdumpster - 50 free querries/day, $49
* fullhunt - 50 free queries, 200 queries $29/month, 500 queries $59/month 
* github-code
* haveibeenpwned - 10 email searches/min $4.50, 50 email searches/min $22
* hunter - 50 credits/month free, 12k credits/yr $34
* hunterhow - 10k free API results per 30 days, 50k API results per 30 days $10
* intelx
* leaklookup - 20 credits $10, 50 credits $20, 140 credits $50, 300 credits $100
* netlas - 50 free requests/day, 1k requests $49, 10k requests $249
* onyphe - 10M results/month $587
* pentesttools - 5 assets netsec $95/month, 5 assets webnetsec $140/month
* projecdiscovery - requires work email. Free monthly discovery and vulnerability scans on sign-up email domain, enterprise $
* rocketreach - 100 email lookups/month $48, 250 email lookups/month $108
* securityscorecard
* securityTrails - 50 free queries/month, 20k queries/month $500
* shodan - Freelancer $69 month, Small Business $359 month
* tomba - 25 searches/month free, 1k searches/month $39, 5k searches/month $89
* venacus - 1 search/day free, 10 searches/day $12, 30 searches/day $36
* whoisxml - 2k queries $50, 5k queries $105
* zoomeye - 5 results/day free, 30/results/day $190/yr

Comments, bugs, and requests
----------------------------
* [![Twitter Follow](https://img.shields.io/twitter/follow/laramies.svg?style=social&amp;label=Follow)](https://twitter.com/laramies) Christian Martorella @laramies
  cmartorella@edge-security.com
* [![Twitter Follow](https://img.shields.io/twitter/follow/NotoriousRebel1.svg?style=social&amp;label=Follow)](https://twitter.com/NotoriousRebel1) Matthew Brown @NotoriousRebel1
* [![Twitter Follow](https://img.shields.io/twitter/follow/jay_townsend1.svg?style=social&amp;label=Follow)](https://twitter.com/jay_townsend1) Jay &quot;L1ghtn1ng&quot; Townsend @jay_townsend1

Main contributors
-----------------
* [![Twitter Follow](https://img.shields.io/twitter/follow/NotoriousRebel1.svg?style=social&amp;label=Follow)](https://twitter.com/NotoriousRebel1) Matthew Brown @NotoriousRebel1
* [![Twitter Follow](https://img.shields.io/twitter/follow/jay_townsend1.svg?style=social&amp;label=Follow)](https://twitter.com/jay_townsend1) Jay &quot;L1ghtn1ng&quot; Townsend @jay_townsend1
* [![Twitter Follow](https://img.shields.io/twitter/follow/discoverscripts.svg?style=social&amp;label=Follow)](https://twitter.com/discoverscripts) Lee Baird @discoverscripts


Thanks
------
* John Matherly - Shodan project
* Ahmed Aboul Ela - subdomain names dictionaries (big and small)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[TheAlgorithms/Python]]></title>
            <link>https://github.com/TheAlgorithms/Python</link>
            <guid>https://github.com/TheAlgorithms/Python</guid>
            <pubDate>Mon, 01 Sep 2025 00:05:05 GMT</pubDate>
            <description><![CDATA[All Algorithms implemented in Python]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/TheAlgorithms/Python">TheAlgorithms/Python</a></h1>
            <p>All Algorithms implemented in Python</p>
            <p>Language: Python</p>
            <p>Stars: 205,938</p>
            <p>Forks: 47,648</p>
            <p>Stars today: 437 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;!-- Title: --&gt;
  &lt;a href=&quot;https://github.com/TheAlgorithms/&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/TheAlgorithms/website/1cd824df116b27029f17c2d1b42d81731f28a920/public/logo.svg&quot; height=&quot;100&quot;&gt;
  &lt;/a&gt;
  &lt;h1&gt;&lt;a href=&quot;https://github.com/TheAlgorithms/&quot;&gt;The Algorithms&lt;/a&gt; - Python&lt;/h1&gt;

&lt;!-- Labels: --&gt;
  &lt;!-- First row: --&gt;
  &lt;a href=&quot;https://gitpod.io/#https://github.com/TheAlgorithms/Python&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;Gitpod Ready-to-Code&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/TheAlgorithms/Python/blob/master/CONTRIBUTING.md&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/static/v1.svg?label=Contributions&amp;message=Welcome&amp;color=0059b3&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;Contributions Welcome&quot;&gt;
  &lt;/a&gt;
  &lt;img src=&quot;https://img.shields.io/github/repo-size/TheAlgorithms/Python.svg?label=Repo%20size&amp;style=flat-square&quot; height=&quot;20&quot;&gt;
  &lt;a href=&quot;https://the-algorithms.com/discord&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/discord/808045925556682782.svg?logo=discord&amp;colorB=7289DA&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;Discord chat&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://gitter.im/TheAlgorithms/community&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/Chat-Gitter-ff69b4.svg?label=Chat&amp;logo=gitter&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;Gitter chat&quot;&gt;
  &lt;/a&gt;

  &lt;!-- Second row: --&gt;
  &lt;br&gt;
  &lt;a href=&quot;https://github.com/TheAlgorithms/Python/actions&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/TheAlgorithms/Python/build.yml?branch=master&amp;label=CI&amp;logo=github&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;GitHub Workflow Status&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/pre-commit/pre-commit&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&amp;logoColor=white&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;pre-commit&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://docs.astral.sh/ruff/formatter/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/static/v1?label=code%20style&amp;message=ruff&amp;color=black&amp;style=flat-square&quot; height=&quot;20&quot; alt=&quot;code style: black&quot;&gt;
  &lt;/a&gt;

&lt;!-- Short description: --&gt;
  &lt;h3&gt;All algorithms implemented in Python - for education 📚&lt;/h3&gt;
&lt;/div&gt;

Implementations are for learning purposes only. They may be less efficient than the implementations in the Python standard library. Use them at your discretion.

## 🚀 Getting Started

📋 Read through our [Contribution Guidelines](CONTRIBUTING.md) before you contribute.

## 🌐 Community Channels

We are on [Discord](https://the-algorithms.com/discord) and [Gitter](https://gitter.im/TheAlgorithms/community)! Community channels are a great way for you to ask questions and get help. Please join us!

## 📜 List of Algorithms

See our [directory](DIRECTORY.md) for easier navigation and a better overview of the project.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[paperless-ngx/paperless-ngx]]></title>
            <link>https://github.com/paperless-ngx/paperless-ngx</link>
            <guid>https://github.com/paperless-ngx/paperless-ngx</guid>
            <pubDate>Mon, 01 Sep 2025 00:05:04 GMT</pubDate>
            <description><![CDATA[A community-supported supercharged document management system: scan, index and archive all your documents]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/paperless-ngx/paperless-ngx">paperless-ngx/paperless-ngx</a></h1>
            <p>A community-supported supercharged document management system: scan, index and archive all your documents</p>
            <p>Language: Python</p>
            <p>Stars: 31,324</p>
            <p>Forks: 1,915</p>
            <p>Stars today: 123 stars today</p>
            <h2>README</h2><pre>[![ci](https://github.com/paperless-ngx/paperless-ngx/workflows/ci/badge.svg)](https://github.com/paperless-ngx/paperless-ngx/actions)
[![Crowdin](https://badges.crowdin.net/paperless-ngx/localized.svg)](https://crowdin.com/project/paperless-ngx)
[![Documentation Status](https://img.shields.io/github/deployments/paperless-ngx/paperless-ngx/github-pages?label=docs)](https://docs.paperless-ngx.com)
[![codecov](https://codecov.io/gh/paperless-ngx/paperless-ngx/branch/main/graph/badge.svg?token=VK6OUPJ3TY)](https://codecov.io/gh/paperless-ngx/paperless-ngx)
[![Chat on Matrix](https://matrix.to/img/matrix-badge.svg)](https://matrix.to/#/%23paperlessngx%3Amatrix.org)
[![demo](https://cronitor.io/badges/ve7ItY/production/W5E_B9jkelG9ZbDiNHUPQEVH3MY.svg)](https://demo.paperless-ngx.com)

&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://github.com/paperless-ngx/paperless-ngx/blob/main/resources/logo/web/png/White%20logo%20-%20no%20background.png&quot; width=&quot;50%&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png&quot; width=&quot;50%&quot;&gt;
    &lt;img src=&quot;https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png&quot; width=&quot;50%&quot;&gt;
  &lt;/picture&gt;
&lt;/p&gt;

&lt;!-- omit in toc --&gt;

# Paperless-ngx

Paperless-ngx is a document management system that transforms your physical documents into a searchable online archive so you can keep, well, _less paper_.

Paperless-ngx is the official successor to the original [Paperless](https://github.com/the-paperless-project/paperless) &amp; [Paperless-ng](https://github.com/jonaswinkler/paperless-ng) projects and is designed to distribute the responsibility of advancing and supporting the project among a team of people. [Consider joining us!](#community-support)

Thanks to the generous folks at [DigitalOcean](https://m.do.co/c/8d70b916d462), a demo is available at [demo.paperless-ngx.com](https://demo.paperless-ngx.com) using login `demo` / `demo`. _Note: demo content is reset frequently and confidential information should not be uploaded._

- [Features](#features)
- [Getting started](#getting-started)
- [Contributing](#contributing)
  - [Community Support](#community-support)
  - [Translation](#translation)
  - [Feature Requests](#feature-requests)
  - [Bugs](#bugs)
- [Related Projects](#related-projects)
- [Important Note](#important-note)

&lt;p align=&quot;right&quot;&gt;This project is supported by:&lt;br/&gt;
  &lt;a href=&quot;https://m.do.co/c/8d70b916d462&quot; style=&quot;padding-top: 4px; display: block;&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_white.svg&quot; width=&quot;140px&quot;&gt;
      &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg&quot; width=&quot;140px&quot;&gt;
      &lt;img src=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_black_.svg&quot; width=&quot;140px&quot;&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
&lt;/p&gt;

# Features

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards-dark.png&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png&quot;&gt;
&lt;/picture&gt;

A full list of [features](https://docs.paperless-ngx.com/#features) and [screenshots](https://docs.paperless-ngx.com/#screenshots) are available in the [documentation](https://docs.paperless-ngx.com/).

# Getting started

The easiest way to deploy paperless is `docker compose`. The files in the [`/docker/compose` directory](https://github.com/paperless-ngx/paperless-ngx/tree/main/docker/compose) are configured to pull the image from the GitHub container registry.

If you&#039;d like to jump right in, you can configure a `docker compose` environment with our install script:

```bash
bash -c &quot;$(curl -L https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/install-paperless-ngx.sh)&quot;
```

More details and step-by-step guides for alternative installation methods can be found in [the documentation](https://docs.paperless-ngx.com/setup/#installation).

Migrating from Paperless-ng is easy, just drop in the new docker image! See the [documentation on migrating](https://docs.paperless-ngx.com/setup/#migrating-to-paperless-ngx) for more details.

&lt;!-- omit in toc --&gt;

### Documentation

The documentation for Paperless-ngx is available at [https://docs.paperless-ngx.com](https://docs.paperless-ngx.com/).

# Contributing

If you feel like contributing to the project, please do! Bug fixes, enhancements, visual fixes etc. are always welcome. If you want to implement something big: Please start a discussion about that! The [documentation](https://docs.paperless-ngx.com/development/) has some basic information on how to get started.

## Community Support

People interested in continuing the work on paperless-ngx are encouraged to reach out here on github and in the [Matrix Room](https://matrix.to/#/#paperless:matrix.org). If you would like to contribute to the project on an ongoing basis there are multiple [teams](https://github.com/orgs/paperless-ngx/people) (frontend, ci/cd, etc) that could use your help so please reach out!

## Translation

Paperless-ngx is available in many languages that are coordinated on Crowdin. If you want to help out by translating paperless-ngx into your language, please head over to https://crowdin.com/project/paperless-ngx, and thank you! More details can be found in [CONTRIBUTING.md](https://github.com/paperless-ngx/paperless-ngx/blob/main/CONTRIBUTING.md#translating-paperless-ngx).

## Feature Requests

Feature requests can be submitted via [GitHub Discussions](https://github.com/paperless-ngx/paperless-ngx/discussions/categories/feature-requests), you can search for existing ideas, add your own and vote for the ones you care about.

## Bugs

For bugs please [open an issue](https://github.com/paperless-ngx/paperless-ngx/issues) or [start a discussion](https://github.com/paperless-ngx/paperless-ngx/discussions) if you have questions.

# Related Projects

Please see [the wiki](https://github.com/paperless-ngx/paperless-ngx/wiki/Related-Projects) for a user-maintained list of related projects and software that is compatible with Paperless-ngx.

# Important Note

&gt; Document scanners are typically used to scan sensitive documents like your social insurance number, tax records, invoices, etc. **Paperless-ngx should never be run on an untrusted host** because information is stored in clear text without encryption. No guarantees are made regarding security (but we do try!) and you use the app at your own risk.
&gt; **The safest way to run Paperless-ngx is on a local server in your own home with backups in place**.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[All-Hands-AI/OpenHands]]></title>
            <link>https://github.com/All-Hands-AI/OpenHands</link>
            <guid>https://github.com/All-Hands-AI/OpenHands</guid>
            <pubDate>Mon, 01 Sep 2025 00:05:03 GMT</pubDate>
            <description><![CDATA[🙌 OpenHands: Code Less, Make More]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/All-Hands-AI/OpenHands">All-Hands-AI/OpenHands</a></h1>
            <p>🙌 OpenHands: Code Less, Make More</p>
            <p>Language: Python</p>
            <p>Stars: 62,959</p>
            <p>Forks: 7,523</p>
            <p>Stars today: 101 stars today</p>
            <h2>README</h2><pre>&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./docs/static/img/logo.png&quot; alt=&quot;Logo&quot; width=&quot;200&quot;&gt;
  &lt;h1 align=&quot;center&quot;&gt;OpenHands: Code Less, Make More&lt;/h1&gt;
&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/All-Hands-AI/OpenHands/graphs/contributors&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/All-Hands-AI/OpenHands?style=for-the-badge&amp;color=blue&quot; alt=&quot;Contributors&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/All-Hands-AI/OpenHands/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/All-Hands-AI/OpenHands?style=for-the-badge&amp;color=blue&quot; alt=&quot;Stargazers&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/All-Hands-AI/OpenHands/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/All-Hands-AI/OpenHands?style=for-the-badge&amp;color=blue&quot; alt=&quot;MIT License&quot;&gt;&lt;/a&gt;
  &lt;br/&gt;
  &lt;a href=&quot;https://join.slack.com/t/openhands-ai/shared_invite/zt-3847of6xi-xuYJIPa6YIPg4ElbDWbtSA&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Slack-Join%20Us-red?logo=slack&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Join our Slack community&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/ESHStjSjD4&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20Us-purple?logo=discord&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Join our Discord community&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/All-Hands-AI/OpenHands/blob/main/CREDITS.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Project-Credits-blue?style=for-the-badge&amp;color=FFE165&amp;logo=github&amp;logoColor=white&quot; alt=&quot;Credits&quot;&gt;&lt;/a&gt;
  &lt;br/&gt;
  &lt;a href=&quot;https://docs.all-hands.dev/usage/getting-started&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;logoColor=FFE165&amp;style=for-the-badge&quot; alt=&quot;Check out the documentation&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://arxiv.org/abs/2407.16741&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Paper%20on%20Arxiv-000?logoColor=FFE165&amp;logo=arxiv&amp;style=for-the-badge&quot; alt=&quot;Paper on Arxiv&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1wOUdFCMyY6Nt0AIqF705KN4JKOWgeI4wUGUP60krXXs/edit?gid=0#gid=0&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Benchmark%20score-000?logoColor=FFE165&amp;logo=huggingface&amp;style=for-the-badge&quot; alt=&quot;Evaluation Benchmark Score&quot;&gt;&lt;/a&gt;

  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=de&quot;&gt;Deutsch&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=es&quot;&gt;Español&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=fr&quot;&gt;français&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=ja&quot;&gt;日本語&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=ko&quot;&gt;한국어&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=pt&quot;&gt;Português&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=ru&quot;&gt;Русский&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/All-Hands-AI/OpenHands?lang=zh&quot;&gt;中文&lt;/a&gt;

  &lt;hr&gt;
&lt;/div&gt;

Welcome to OpenHands (formerly OpenDevin), a platform for software development agents powered by AI.

OpenHands agents can do anything a human developer can: modify code, run commands, browse the web,
call APIs, and yes—even copy code snippets from StackOverflow.

Learn more at [docs.all-hands.dev](https://docs.all-hands.dev), or [sign up for OpenHands Cloud](https://app.all-hands.dev) to get started.

&gt; [!IMPORTANT]
&gt; Using OpenHands for work? We&#039;d love to chat! Fill out
&gt; [this short form](https://docs.google.com/forms/d/e/1FAIpQLSet3VbGaz8z32gW9Wm-Grl4jpt5WgMXPgJ4EDPVmCETCBpJtQ/viewform)
&gt; to join our Design Partner program, where you&#039;ll get early access to commercial features and the opportunity to provide input on our product roadmap.

![App screenshot](./docs/static/img/screenshot.png)

## ☁️ OpenHands Cloud
The easiest way to get started with OpenHands is on [OpenHands Cloud](https://app.all-hands.dev),
which comes with $20 in free credits for new users.

## 💻 Running OpenHands Locally

### Option 1: CLI Launcher (Recommended)

The easiest way to run OpenHands locally is using the CLI launcher with [uv](https://docs.astral.sh/uv/). This provides better isolation from your current project&#039;s virtual environment and is required for OpenHands&#039; default MCP servers.

**Install uv** (if you haven&#039;t already):

See the [uv installation guide](https://docs.astral.sh/uv/getting-started/installation/) for the latest installation instructions for your platform.

**Launch OpenHands**:
```bash
# Launch the GUI server
uvx --python 3.12 --from openhands-ai openhands serve

# Or launch the CLI
uvx --python 3.12 --from openhands-ai openhands
```

You&#039;ll find OpenHands running at [http://localhost:3000](http://localhost:3000) (for GUI mode)!

### Option 2: Docker

&lt;details&gt;
&lt;summary&gt;Click to expand Docker command&lt;/summary&gt;

You can also run OpenHands directly with Docker:

```bash
docker pull docker.all-hands.dev/all-hands-ai/runtime:0.54-nikolaik

docker run -it --rm --pull=always \
    -e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.54-nikolaik \
    -e LOG_ALL_EVENTS=true \
    -v /var/run/docker.sock:/var/run/docker.sock \
    -v ~/.openhands:/.openhands \
    -p 3000:3000 \
    --add-host host.docker.internal:host-gateway \
    --name openhands-app \
    docker.all-hands.dev/all-hands-ai/openhands:0.54
```

&lt;/details&gt;

&gt; **Note**: If you used OpenHands before version 0.44, you may want to run `mv ~/.openhands-state ~/.openhands` to migrate your conversation history to the new location.

&gt; [!WARNING]
&gt; On a public network? See our [Hardened Docker Installation Guide](https://docs.all-hands.dev/usage/runtimes/docker#hardened-docker-installation)
&gt; to secure your deployment by restricting network binding and implementing additional security measures.

### Getting Started

When you open the application, you&#039;ll be asked to choose an LLM provider and add an API key.
[Anthropic&#039;s Claude Sonnet 4](https://www.anthropic.com/api) (`anthropic/claude-sonnet-4-20250514`)
works best, but you have [many options](https://docs.all-hands.dev/usage/llms).

See the [Running OpenHands](https://docs.all-hands.dev/usage/installation) guide for
system requirements and more information.

## 💡 Other ways to run OpenHands

&gt; [!WARNING]
&gt; OpenHands is meant to be run by a single user on their local workstation.
&gt; It is not appropriate for multi-tenant deployments where multiple users share the same instance. There is no built-in authentication, isolation, or scalability.
&gt;
&gt; If you&#039;re interested in running OpenHands in a multi-tenant environment, check out the source-available, commercially-licensed
&gt; [OpenHands Cloud Helm Chart](https://github.com/all-Hands-AI/OpenHands-cloud)

You can [connect OpenHands to your local filesystem](https://docs.all-hands.dev/usage/runtimes/docker#connecting-to-your-filesystem),
interact with it via a [friendly CLI](https://docs.all-hands.dev/usage/how-to/cli-mode),
run OpenHands in a scriptable [headless mode](https://docs.all-hands.dev/usage/how-to/headless-mode),
or run it on tagged issues with [a github action](https://docs.all-hands.dev/usage/how-to/github-action).

Visit [Running OpenHands](https://docs.all-hands.dev/usage/installation) for more information and setup instructions.

If you want to modify the OpenHands source code, check out [Development.md](https://github.com/All-Hands-AI/OpenHands/blob/main/Development.md).

Having issues? The [Troubleshooting Guide](https://docs.all-hands.dev/usage/troubleshooting) can help.

## 📖 Documentation

To learn more about the project, and for tips on using OpenHands,
check out our [documentation](https://docs.all-hands.dev/usage/getting-started).

There you&#039;ll find resources on how to use different LLM providers,
troubleshooting resources, and advanced configuration options.

## 🤝 How to Join the Community

OpenHands is a community-driven project, and we welcome contributions from everyone. We do most of our communication
through Slack, so this is the best place to start, but we also are happy to have you contact us on Discord or Github:

- [Join our Slack workspace](https://join.slack.com/t/openhands-ai/shared_invite/zt-3847of6xi-xuYJIPa6YIPg4ElbDWbtSA) - Here we talk about research, architecture, and future development.
- [Join our Discord server](https://discord.gg/ESHStjSjD4) - This is a community-run server for general discussion, questions, and feedback.
- [Read or post Github Issues](https://github.com/All-Hands-AI/OpenHands/issues) - Check out the issues we&#039;re working on, or add your own ideas.

See more about the community in [COMMUNITY.md](./COMMUNITY.md) or find details on contributing in [CONTRIBUTING.md](./CONTRIBUTING.md).

## 📈 Progress

See the monthly OpenHands roadmap [here](https://github.com/orgs/All-Hands-AI/projects/1) (updated at the maintainer&#039;s meeting at the end of each month).

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://star-history.com/#All-Hands-AI/OpenHands&amp;Date&quot;&gt;
    &lt;img src=&quot;https://api.star-history.com/svg?repos=All-Hands-AI/OpenHands&amp;type=Date&quot; width=&quot;500&quot; alt=&quot;Star History Chart&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

## 📜 License

Distributed under the MIT License. See [`LICENSE`](./LICENSE) for more information.

## 🙏 Acknowledgements

OpenHands is built by a large number of contributors, and every contribution is greatly appreciated! We also build upon other open source projects, and we are deeply thankful for their work.

For a list of open source projects and licenses used in OpenHands, please see our [CREDITS.md](./CREDITS.md) file.

## 📚 Cite

```
@inproceedings{
  wang2025openhands,
  title={OpenHands: An Open Platform for {AI} Software Developers as Generalist Agents},
  author={Xingyao Wang and Boxuan Li and Yufan Song and Frank F. Xu and Xiangru Tang and Mingchen Zhuge and Jiayi Pan and Yueqi Song and Bowen Li and Jaskirat Singh and Hoang H. Tran and Fuqiang Li and Ren Ma and Mingzhang Zheng and Bill Qian and Yanjun Shao and Niklas Muennighoff and Yizhe Zhang and Binyuan Hui and Junyang Lin and Robert Brennan and Hao Peng and Heng Ji and Graham Neubig},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
  url={https://openreview.net/forum?id=OJd3ayDDoF}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[chubin/cheat.sh]]></title>
            <link>https://github.com/chubin/cheat.sh</link>
            <guid>https://github.com/chubin/cheat.sh</guid>
            <pubDate>Mon, 01 Sep 2025 00:05:02 GMT</pubDate>
            <description><![CDATA[the only cheat sheet you need]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/chubin/cheat.sh">chubin/cheat.sh</a></h1>
            <p>the only cheat sheet you need</p>
            <p>Language: Python</p>
            <p>Stars: 40,117</p>
            <p>Forks: 1,862</p>
            <p>Stars today: 42 stars today</p>
            <h2>README</h2><pre>

![cheat.sh logo](http://cheat.sh/files/big-logo-v2-fixed.png)

Unified access to the best community driven cheat sheets repositories of the world.

Let&#039;s imagine for a moment that there is such a thing as an ideal cheat sheet.
What should it look like?
What features should it have?

* **Concise** — It should only contain the things you need, and nothing else.
* **Fast** — It should be possible to use it instantly.
* **Comprehensive** — It should contain answers for every possible question.
* **Universal** — It should be available everywhere, anytime, without any preparations.
* **Unobtrusive** — It should not distract you from your main task.
* **Tutoring** — It should help you to learn the subject.
* **Inconspicuous** — It should be possible to use it completely unnoticed.

Such a thing exists! It&#039;s easy to [install](#installation) and there&#039;s even [auto-complete](#tab-completion).


## Features

**cheat.sh**

* Has a simple curl/browser/editor interface.
* Covers 56 programming languages, several DBMSes, and more than 1000 most important UNIX/Linux commands.
* Provides access to the best community driven cheat sheets repositories in the world, on par with StackOverflow.
* Available everywhere, no installation needed, but can be installed for offline usage.
* Ultrafast, returns answers within 100 ms, as a rule.
* Has a convenient command line client, `cht.sh`, that is very advantageous and helpful, though not mandatory.
* Can be used directly from code editors, without opening a browser and not switching your mental context.
* Supports a special stealth mode where it can be used fully invisibly without ever touching a key and making sounds.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&#039;https://cheat.sh/files/demo-curl.gif&#039;/&gt;
&lt;/p&gt;

## Contents

* [Features](#features)
* [Usage](#usage)
* [Command line client, cht.sh](#command-line-client-chtsh)
  * [Installation](#installation)
  * [Client usage](#client-usage)
  * [Tab-completion](#tab-completion)
    - [Bash Tab completion](#bash-tab-completion)
    - [ZSH Tab completion](#zsh-tab-completion)
  * [Stealth mode](#stealth-mode)
  * [Windows command line client](#windows-command-line-client)
* [Self-Hosting](#self-hosting)
  * [Docker](#docker)
* [Editors integration](#editors-integration)
  * [Vim](#vim)
  * [Emacs](#emacs)
  * [Visual Studio Code](#visual-studio-code)
  * [Sublime](#sublime)
  * [IntelliJ IDEA](#intellij-idea)
  * [QT Creator](#qtcreator)
* [Special pages](#special-pages)
* [Search](#search)
* [Programming languages cheat sheets](#programming-languages-cheat-sheets)
* [Cheat sheets sources](#cheat-sheets-sources)
* [How to contribute](#how-to-contribute)
  * [How to edit a cheat sheet](#how-to-edit-a-cheat-sheet)
  * [How to add a cheat sheet](#how-to-add-a-cheat-sheet)
  * [How to add a cheat sheet repository](#how-to-add-a-cheat-sheet-repository)

## Usage

To get a cheat sheet for a UNIX/Linux command from a command line, query the service using `curl` or any other HTTP/HTTPS client
specifying the name of the command in the query:

```
    curl cheat.sh/tar
    curl cht.sh/curl
    curl https://cheat.sh/rsync
    curl https://cht.sh/tr
```
As you can see, you can use both HTTPS and HTTP to access the service, and both the long (cheat.sh) and the short (cht.sh) service names.

Here `tar`, `curl`, `rsync`, and `tr` are names of the UNIX/Linux commands you want to get cheat sheets for.

If you don&#039;t know the name of the command you need, you can search for it using the `~KEYWORD` notation.
For example, to see how you can make `snapshots` of a filesystem/volume/something else:
```
    curl cht.sh/~snapshot
```

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&#039;https://cheat.sh/files/cht.sh-url-structure.png&#039;/&gt;
&lt;/p&gt;

The programming language cheat sheets are located in special namespaces dedicated to them.

```
    curl cht.sh/go/Pointers
    curl cht.sh/scala/Functions
    curl cht.sh/python/lambda
```

To get the list of available programming language cheat sheets, use the special query `:list`:

```
    curl cht.sh/go/:list
```

Almost each programming language has a special page named `:learn`
that describes the language basics (that&#039;s a direct mapping from the *&quot;Learn X in Y&quot;* project).
It could be a good starting point if you&#039;ve just started learning a language.

If there is no cheat sheet for a programming language query (and it is almost always the case),
it is generated on the fly, based on available cheat sheets and answers on StackOverflow.
Of course, there is no guarantee that the returned cheat sheet will be a 100% hit, but it is almost always exactly what you are looking for.

Try these (and your own) queries to get the impression of that, what the answers look like:
```
    curl cht.sh/go/reverse+a+list
    curl cht.sh/python/random+list+elements
    curl cht.sh/js/parse+json
    curl cht.sh/lua/merge+tables
    curl cht.sh/clojure/variadic+function
```

If you don&#039;t like an answer for your queries, you can pick another one. For that, repeat the query with an additional parameter `/1`, `/2` etc. appended:

```
    curl cht.sh/python/random+string
    curl cht.sh/python/random+string/1
    curl cht.sh/python/random+string/2
```

Cheat sheets are formatted as code of the queried programming language (at least we are trying our best to do so)
so they can be pasted into a program in this language directly. Text comments, if there are any, are formatted according to the language syntax.

```lua
    $ curl cht.sh/lua/table+keys
    -- lua: retrieve list of keys in a table

    local keyset={}
    local n=0

    for k,v in pairs(tab) do
      n=n+1
      keyset[n]=k
    end

    --[[
       [ Note that you cannot guarantee any order in keyset. If you want the
       [ keys in sorted order, then sort keyset with table.sort(keyset).
       [ 
       [ [lhf] [so/q/12674345] [cc by-sa 3.0]
       ]]

```

If you don&#039;t need text comments in the answer, you can eliminate them
using a special option `\?Q`:
```lua
    $ curl cht.sh/lua/table+keys\?Q
    local keyset={}
    local n=0

    for k,v in pairs(tab) do
      n=n+1
      keyset[n]=k
    end
```

And if you don&#039;t need syntax highlighting, switch it off using `\?T`.
You can combine the options together:

```
    curl cht.sh/go/reverse+a+list\?Q
    curl cht.sh/python/random+list+elements\?Q
    curl cht.sh/js/parse+json\?Q
    curl cht.sh/lua/merge+tables\?QT
    curl cht.sh/clojure/variadic+function\?QT
```

Full list of all options described below and in `/:help`.

Try your own queries. Follow these rules:

1. Try to be more specific (`/python/append+file` is better than `/python/file` and `/python/append`).
2. Ask practical question if possible (yet theoretical question are possible too).
3. Ask programming language questions only; specify the name of the programming language as the section name.
4. Separate words with `+` instead of spaces.
5. Do not use special characters, they are ignored anyway.
6. If you want to eliminate cheat sheets containing some word, add it to the query with `+-`: `python/multiply+matrices+-numpy`

Read more about the programming languages queries below.

----

## Command line client, cht.sh

The cheat.sh service has its own command line client (`cht.sh`) that
has several useful features compared to querying the service directly with `curl`:

* Special shell mode with a persistent queries context and readline support.
* Queries history.
* Clipboard integration.
* Tab completion support for shells (bash, fish, zsh).
* Stealth mode.

### Installation

To install the client:

```bash
PATH_DIR=&quot;$HOME/bin&quot;  # or another directory on your $PATH
mkdir -p &quot;$PATH_DIR&quot;
curl https://cht.sh/:cht.sh &gt; &quot;$PATH_DIR/cht.sh&quot;
chmod +x &quot;$PATH_DIR/cht.sh&quot;
```

or to install it globally (for all users):

```bash
curl -s https://cht.sh/:cht.sh | sudo tee /usr/local/bin/cht.sh &amp;&amp; sudo chmod +x /usr/local/bin/cht.sh
```

Note: The package &quot;rlwrap&quot; is a required dependency to run in shell mode. Install this using `sudo apt install rlwrap`

### Client usage

Now, you can use `cht.sh` instead of `curl`, and write your queries in more natural way,
with spaces instead of `+`:

```
    $ cht.sh go reverse a list
    $ cht.sh python random list elements
    $ cht.sh js parse json
```

It is even more convenient to start the client in a special shell mode:
```
    $ cht.sh --shell
    cht.sh&gt; go reverse a list
```

If all your queries are about the same language, you can change the context
and spare repeating the programming language name:
```
    $ cht.sh --shell
    cht.sh&gt; cd go
    cht.sh/go&gt; reverse a list
```
or even start the client in this context:
```
    $ cht.sh --shell go
    cht.sh/go&gt; reverse a list
    ...
    cht.sh/go&gt; join a list
    ...
```

If you want to change the context, you can do it with the `cd` command,
or if you want do a single query for some other language, just prepend it with `/`:

```
    $ cht.sh --shell go
    ...
    cht.sh/go&gt; /python dictionary comprehension
    ...
```

If you want to copy the last answer into the clipboard, you can
use the `c` (`copy`) command, or `C` (`ccopy`, without comments).

```
    cht.sh/python&gt; append file
    #  python - How do you append to a file?

    with open(&quot;test.txt&quot;, &quot;a&quot;) as myfile:
        myfile.write(&quot;appended text&quot;)
    cht.sh/python&gt; C
    copy: 2 lines copied to the selection
```

Type `help` for other internal `cht.sh` commands.

```
	cht.sh&gt; help
	help    - show this help
	hush    - do not show the &#039;help&#039; string at start anymore
	cd LANG - change the language context
	copy    - copy the last answer in the clipboard (aliases: yank, y, c)
	ccopy   - copy the last answer w/o comments (cut comments; aliases: cc, Y, C)
	exit    - exit the cheat shell (aliases: quit, ^D)
	id [ID] - set/show an unique session id (&quot;reset&quot; to reset, &quot;remove&quot; to remove)
	stealth - stealth mode (automatic queries for selected text)
	update  - self update (only if the scriptfile is writeable)
	version - show current cht.sh version
	/:help  - service help
	QUERY   - space separated query staring (examples are below)
				  cht.sh&gt; python zip list
				  cht.sh/python&gt; zip list
				  cht.sh/go&gt; /python zip list
```

The `cht.sh` client has its configuration file which is located at `~/.cht.sh/cht.sh.conf`
(location of the file can be overridden by the environment variable `CHTSH_CONF`).
Use it to specify query options that you would use with each query.
For example, to switch syntax highlighting off create the file with the following
content:

```bash
CHTSH_QUERY_OPTIONS=&quot;T&quot;
```

Or if you want to use a special syntax highlighting theme:

```bash
CHTSH_QUERY_OPTIONS=&quot;style=native&quot;
```

(`curl cht.sh/:styles-demo` to see all supported styles).

Other cht.sh configuration parameters:

```bash
CHTSH_CURL_OPTIONS=&quot;-A curl&quot;        # curl options used for cht.sh queries
CHTSH_URL=https://cht.sh            # URL of the cheat.sh server
```

### Tab completion


#### Bash Tab completion

To activate tab completion support for `cht.sh`, add the `:bash_completion` script to your `~/.bashrc`:

```bash
    curl https://cheat.sh/:bash_completion &gt; ~/.bash.d/cht.sh
    . ~/.bash.d/cht.sh
    # and add . ~/.bash.d/cht.sh to ~/.bashrc
```

#### ZSH Tab completion

To activate tab completion support for `cht.sh`, add the `:zsh` script to the *fpath* in your `~/.zshrc`:

```zsh
    curl https://cheat.sh/:zsh &gt; ~/.zsh.d/_cht
    echo &#039;fpath=(~/.zsh.d/ $fpath)&#039; &gt;&gt; ~/.zshrc
    # Open a new shell to load the plugin
```

----

### Stealth mode

Being used fully unnoticed is one of the most important property of any cheat sheet.

cheat.sh can be used completely unnoticed too. The cheat.sh client, `cht.sh`, has
a special mode, called **stealth mode**. Using that, you don&#039;t even need to touch your
keyboard to open a cheat sheet.

In this mode, as soon as you select some text with the mouse (and thus adding it
into the selection buffer of X Window System or into the clipboard) it&#039;s used
as a query string for cheat.sh, and the correspondent cheat sheet is automatically shown.

Let&#039;s imagine, that you are having an online interview, where your interviewer asks you
some questions using a shared document (say Google Docs) and you are supposed
to write your coding answers there (it&#039;s possible too that you&#039;ll type in the questions
on your own, just to show to the interviewer that you&#039;ve heard it right).

When using the stealth mode of `cht.sh`, the only thing you need to do in order to see
a cheat sheet for some question, is to select the question using the mouse.
If you don&#039;t want any text in the answers and the only thing you need is code,
use the `Q` option when starting the stealth mode.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&#039;https://cheat.sh/files/stealth-mode.gif&#039;/&gt;
&lt;/p&gt;

```
You: Hi!                                            | $ cht.sh --shell python
She: Hi!                                            | cht.sh/python&gt; stealth Q
She: Are you ready for a small interview?           | stealth: you are in the stealth mode; select any text
She: Just a couple of questions                     | stealth: selections longer than 5 words are ignored
She: We will talk about python                      | stealth: query arguments: ?Q
She: Let&#039;s start from something simple.             | stealth: use ^C to leave this mode
She: Do you know how to reverse a list in python?   |
You: Sure                                           |
You: (selecting &quot;reverse a list&quot;)                   | stealth: reverse a list
                                                    | reverse_lst = lst[::-1]
You: lst[::-1]?                                     |
She: Good.                                          |
She: Do you know how to chain a list of lists?      |
You: (selecting &quot;chain a list of lists&quot;)            | stealth: chain a list of lists
                                                    | import itertools
                                                    | a = [[&quot;a&quot;,&quot;b&quot;], [&quot;c&quot;]]
                                                    | print list(itertools.chain.from_iterable(a))
You: May I use external modules?                    |
She: What module do you want to use?                |
You: itertools                                      |
She: Yes, you may use it                            |
You: Ok, then:                                      |
You: itertools.chain.from_iterable(a)               |
She: Good. Let&#039;s try something harder.              |
She: What about quicksort implementation?           |
You: (selecting &quot;quicksort implementation&quot;)         | stealth: quicksort implementation
You: Let me think about it.                         | (some big and clumsy lowlevel implementation shown)
You: Well...(starting typing it in)                 | def sort(array=[12,4,5,6,7,3,1,15]):
                                                    |     less = []
She: (seeing your ugly pascal style)                |     equal = []
She: Could you write it more concise?               |     greater = []
                                                    |     if len(array) &gt; 1:
You: What do you mean?                              |         pivot = array[0]
                                                    |         for x in array:
She: I mean,                                        |             if x &lt; pivot: less.append(x)
She: do you really need all these ifs and fors?     |             if x == pivot: equal.append(x)
She: Could you maybe just use filter instead?       |             if x &gt; pivot: greater.append(x)
                                                    |         return sort(less)+equal+sort(greater)
You: quicksort with filter?                         |     else:
                                                    |         return array
She: Yes                                            |
You: (selecting &quot;quicksort with filter&quot;)            | stealth: quicksort with filter
You: Ok, I will try.                                | return qsort(filter(lt, L[1:]))+[pivot] \
You: Something like this?                           |     +qsort(filter(ge, L[1:]))
You: qsort(filter(lt, L[1:]))+[pivot] \             |
       + qsort(filter(ge, L[1:]))                   |
                                                    |
She: Yes! Perfect! Exactly what I wanted to see!    |
                                                    |

```

Of course, this is just for fun, and you should never cheat in your coding interviews,
because you know what happens when you do.

![when you lie in your interview](http://cheat.sh/files/when-you-lie-katze.png)

### Windows command line client

You can access cheat.sh from Windows command line too.

Use cheat.sh command line client for that: [`cht.exe`](https://github.com/tpanj/cht.exe).
It supports:

* output colorization;
* command line options;
* its own configuration file.

You can also use [`scoop`](https://github.com/lukesampson/scoop) command-line installer for Windows to get it:
```batch
scoop install cht
```

----

## Self-Hosting

### Docker

Currently, the easiest way to get a self-hosted instance running is by using
the `docker-compose.yml` file.

    docker-compose up

This builds and runs the image with baked in cheatsheets and starts the app
and a Redis instance to back it, making the service available at
http://localhost:8002 This is currently an early implementation and should
probably not be used for anything outside of internal/dev/personal use right
now.

## Editors integration

You can use *cheat.sh* directly from the editor
(*Emacs*, *Sublime*, *Vim*, and *Visual Studio Code* are currently supported;
not all features are supported by all plugins though; see below).
Instead of opening your browser, googling, browsing Stack Overflow
and eventually copying the code snippets you need into the clipboard
and later pasting them into the editor,
you can achieve the same instantly and without leaving the editor at all!

Here is what it looks like in Vim:

1. If you have a question while editing a program, you can just type
your question directly in the buffer and press `&lt;leader&gt;KK`. You will get
the answer to your question in pager. (with `&lt;leader&gt;KB` you&#039;ll get the answer
in a separate buffer).

2. If you like the answer, you can manually paste it from the buffer or
the pager, or if you are lazy you can use `&lt;leader&gt;KP` to paste it below/under
your question (or replace you question using `&lt;leader&gt;KR`). If you want the
answer without the comments, `&lt;leader&gt;KC` replays the last query
toggling them.

If you use some static analysis plugin such as *syntastic* (for Vim), you can use
its warning and error messages as cheat.sh queries: place the cursor on the problem line
and press `&lt;leader&gt;KE`: explanation for the warning will be opened in a new buffer.

Features supported by cheat.sh plugins for different editors:

|Feature            |Emacs|Sublime|Vim|VSCode|IDEA|QtCreator|
|-------------------|-----|-------|---|------|----|---------|
|Command queries    |✓    |✓      |✓  |✓     |✓   |✓        |
|Queries from buffer|     |       |✓  |✓     |    |✓        |
|Toggle comments    |     |       |✓  |✓     |✓   |✓        |
|Prev/next answer   |     |       |✓  |✓     |✓   |✓        |
|Multiple answers   |     |✓      |   |      |✓   |         |
|Warnings as queries|     |       |✓  |      |    |         |
|Queries history    |     |       |✓  |✓     |    |         |
|Session id         |     |       |✓  |      |    |         |
|Configurable server|✓    |       |✓  |✓     |    |✓        |

### Vim

* [cheat.sh-vim](https://github.com/dbeniamine/cheat.sh-vim) — Vim support

Here is Vim configuration example:

```vim
&quot; some configuration above ...

let mapleader=&quot; &quot;

call vundle#begin()
Bundle &#039;gmarik/vundle&#039;
Bundle &#039;scrooloose/syntastic&#039;
Bundle &#039;dbeniamine/cheat.sh-vim&#039;
call vundle#end()

let g:syntastic_javascript_checkers = [ &#039;jshint&#039; ]
let g:syntastic_ocaml_checkers = [&#039;merlin&#039;]
let g:syntastic_python_checkers = [&#039;pylint&#039;]
let g:syntastic_shell_checkers = [&#039;shellcheck&#039;]

&quot; some configuration below ...
```

In this

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/qlib]]></title>
            <link>https://github.com/microsoft/qlib</link>
            <guid>https://github.com/microsoft/qlib</guid>
            <pubDate>Mon, 01 Sep 2025 00:05:01 GMT</pubDate>
            <description><![CDATA[Qlib is an AI-oriented Quant investment platform that aims to use AI tech to empower Quant Research, from exploring ideas to implementing productions. Qlib supports diverse ML modeling paradigms, including supervised learning, market dynamics modeling, and RL, and is now equipped with https://github.com/microsoft/RD-Agent to automate R&D process.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/qlib">microsoft/qlib</a></h1>
            <p>Qlib is an AI-oriented Quant investment platform that aims to use AI tech to empower Quant Research, from exploring ideas to implementing productions. Qlib supports diverse ML modeling paradigms, including supervised learning, market dynamics modeling, and RL, and is now equipped with https://github.com/microsoft/RD-Agent to automate R&D process.</p>
            <p>Language: Python</p>
            <p>Stars: 29,482</p>
            <p>Forks: 4,560</p>
            <p>Stars today: 124 stars today</p>
            <h2>README</h2><pre>[![Python Versions](https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&amp;logoColor=white)](https://pypi.org/project/pyqlib/#files)
[![Platform](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey)](https://pypi.org/project/pyqlib/#files)
[![PypI Versions](https://img.shields.io/pypi/v/pyqlib)](https://pypi.org/project/pyqlib/#history)
[![Upload Python Package](https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/pyqlib/)
[![Github Actions Test Status](https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main)](https://github.com/microsoft/qlib/actions)
[![Documentation Status](https://readthedocs.org/projects/qlib/badge/?version=latest)](https://qlib.readthedocs.io/en/latest/?badge=latest)
[![License](https://img.shields.io/pypi/l/pyqlib)](LICENSE)
[![Join the chat at https://gitter.im/Microsoft/qlib](https://badges.gitter.im/Microsoft/qlib.svg)](https://gitter.im/Microsoft/qlib?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge)

## :newspaper: **What&#039;s NEW!** &amp;nbsp;   :sparkling_heart: 

Recent released features

### Introducing &lt;a href=&quot;https://github.com/microsoft/RD-Agent&quot;&gt;&lt;img src=&quot;docs/_static/img/rdagent_logo.png&quot; alt=&quot;RD_Agent&quot; style=&quot;height: 2em&quot;&gt;&lt;/a&gt;: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&amp;D

We are excited to announce the release of **RD-Agent**📢, a powerful tool that supports automated factor mining and model optimization in quant investment R&amp;D.

RD-Agent is now available on [GitHub](https://github.com/microsoft/RD-Agent), and we welcome your star🌟!

To learn more, please visit our [♾️Demo page](https://rdagent.azurewebsites.net/). Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.

We have prepared several demo videos for you:
| Scenario | Demo video (English) | Demo video (中文) |
| --                      | ------    | ------    |
| Quant Factor Mining | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=zh) |
| Quant Factor Mining from reports | [Link](https://rdagent.azurewebsites.net/report_factor?lang=en) | [Link](https://rdagent.azurewebsites.net/report_factor?lang=zh) |
| Quant Model Optimization | [Link](https://rdagent.azurewebsites.net/model_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/model_loop?lang=zh) |

- 📃**Paper**: [R&amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization](https://arxiv.org/abs/2505.15155)
- 👾**Code**: https://github.com/microsoft/RD-Agent/
```BibTeX
@misc{li2025rdagentquant,
    title={R\&amp;D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},
    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},
    year={2025},
    eprint={2505.15155},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}
```
![image](https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d)

***

| Feature | Status |
| --                      | ------    |
| [R&amp;D-Agent-Quant](https://arxiv.org/abs/2505.15155) Published | Apply R&amp;D-Agent to Qlib for quant trading | 
| BPQP for End-to-end learning | 📈Coming soon!([Under review](https://github.com/microsoft/qlib/pull/1863)) |
| 🔥LLM-driven Auto Quant Factory🔥 | 🚀 Released in [♾️RD-Agent](https://github.com/microsoft/RD-Agent) on Aug 8, 2024 |
| KRNN and Sandwich models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1414/) on May 26, 2023 |
| Release Qlib v0.9.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.9.0) on Dec 9, 2022 |
| RL Learning Framework | :hammer: :chart_with_upwards_trend: Released on Nov 10, 2022. [#1332](https://github.com/microsoft/qlib/pull/1332), [#1322](https://github.com/microsoft/qlib/pull/1322), [#1316](https://github.com/microsoft/qlib/pull/1316),[#1299](https://github.com/microsoft/qlib/pull/1299),[#1263](https://github.com/microsoft/qlib/pull/1263), [#1244](https://github.com/microsoft/qlib/pull/1244), [#1169](https://github.com/microsoft/qlib/pull/1169), [#1125](https://github.com/microsoft/qlib/pull/1125), [#1076](https://github.com/microsoft/qlib/pull/1076)|
| HIST and IGMTF models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1040) on Apr 10, 2022 |
| Qlib [notebook tutorial](https://github.com/microsoft/qlib/tree/main/examples/tutorial) | 📖 [Released](https://github.com/microsoft/qlib/pull/1037) on Apr 7, 2022 | 
| Ibovespa index data | :rice: [Released](https://github.com/microsoft/qlib/pull/990) on Apr 6, 2022 |
| Point-in-Time database | :hammer: [Released](https://github.com/microsoft/qlib/pull/343) on Mar 10, 2022 |
| Arctic Provider Backend &amp; Orderbook data example | :hammer: [Released](https://github.com/microsoft/qlib/pull/744) on Jan 17, 2022 |
| Meta-Learning-based framework &amp; DDG-DA  | :chart_with_upwards_trend:  :hammer: [Released](https://github.com/microsoft/qlib/pull/743) on Jan 10, 2022 | 
| Planning-based portfolio optimization | :hammer: [Released](https://github.com/microsoft/qlib/pull/754) on Dec 28, 2021 | 
| Release Qlib v0.8.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.8.0) on Dec 8, 2021 |
| ADD model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/704) on Nov 22, 2021 |
| ADARNN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/689) on Nov 14, 2021 |
| TCN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/668) on Nov 4, 2021 |
| Nested Decision Framework | :hammer: [Released](https://github.com/microsoft/qlib/pull/438) on Oct 1, 2021. [Example](https://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/workflow.py) and [Doc](https://qlib.readthedocs.io/en/latest/component/highfreq.html) |
| Temporal Routing Adaptor (TRA) | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/531) on July 30, 2021 |
| Transformer &amp; Localformer | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/508) on July 22, 2021 |
| Release Qlib v0.7.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.7.0) on July 12, 2021 |
| TCTS Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/491) on July 1, 2021 |
| Online serving and automatic model rolling | :hammer:  [Released](https://github.com/microsoft/qlib/pull/290) on May 17, 2021 | 
| DoubleEnsemble Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/286) on Mar 2, 2021 | 
| High-frequency data processing example | :hammer: [Released](https://github.com/microsoft/qlib/pull/257) on Feb 5, 2021  |
| High-frequency trading example | :chart_with_upwards_trend: [Part of code released](https://github.com/microsoft/qlib/pull/227) on Jan 28, 2021  | 
| High-frequency data(1min) | :rice: [Released](https://github.com/microsoft/qlib/pull/221) on Jan 27, 2021 |
| Tabnet Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/205) on Jan 22, 2021 |

Features released before 2021 are not listed here.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/_static/img/logo/1.png&quot; /&gt;
&lt;/p&gt;

Qlib is an open-source, AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms, including supervised learning, market dynamics modeling, and reinforcement learning.

An increasing number of SOTA Quant research works/papers in diverse paradigms are being released in Qlib to collaboratively solve key challenges in quantitative investment. For example, 1) using supervised learning to mine the market&#039;s complex non-linear patterns from rich and heterogeneous financial data, 2) modeling the dynamic nature of the financial market using adaptive concept drift technology, and 3) using reinforcement learning to model continuous investment decisions and assist investors in optimizing their trading strategies.

It contains the full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution. 
For more details, please refer to our paper [&quot;Qlib: An AI-oriented Quantitative Investment Platform&quot;](https://arxiv.org/abs/2009.11189).


&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;Frameworks, Tutorial, Data &amp; DevOps&lt;/th&gt;
      &lt;th&gt;Main Challenges &amp; Solutions in Quant Research&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;
        &lt;li&gt;&lt;a href=&quot;#plans&quot;&gt;&lt;strong&gt;Plans&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#framework-of-qlib&quot;&gt;Framework of Qlib&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#quick-start&quot;&gt;Quick Start&lt;/a&gt;&lt;/li&gt;
          &lt;ul dir=&quot;auto&quot;&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#installation&quot;&gt;Installation&lt;/a&gt; &lt;/li&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#data-preparation&quot;&gt;Data Preparation&lt;/a&gt;&lt;/li&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#auto-quant-research-workflow&quot;&gt;Auto Quant Research Workflow&lt;/a&gt;&lt;/li&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#building-customized-quant-research-workflow-by-code&quot;&gt;Building Customized Quant Research Workflow by Code&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
        &lt;li&gt;&lt;a href=&quot;#quant-dataset-zoo&quot;&gt;&lt;strong&gt;Quant Dataset Zoo&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#learning-framework&quot;&gt;Learning Framework&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#more-about-qlib&quot;&gt;More About Qlib&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#offline-mode-and-online-mode&quot;&gt;Offline Mode and Online Mode&lt;/a&gt;
        &lt;ul&gt;
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#performance-of-qlib-data-server&quot;&gt;Performance of Qlib Data Server&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
        &lt;li&gt;&lt;a href=&quot;#related-reports&quot;&gt;Related Reports&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#contact-us&quot;&gt;Contact Us&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&quot;#contributing&quot;&gt;Contributing&lt;/a&gt;&lt;/li&gt;
      &lt;/td&gt;
      &lt;td valign=&quot;baseline&quot;&gt;
        &lt;li&gt;&lt;a href=&quot;#main-challenges--solutions-in-quant-research&quot;&gt;Main Challenges &amp;amp; Solutions in Quant Research&lt;/a&gt;
          &lt;ul&gt;
            &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#forecasting-finding-valuable-signalspatterns&quot;&gt;Forecasting: Finding Valuable Signals/Patterns&lt;/a&gt;
              &lt;ul&gt;
                &lt;li type=&quot;disc&quot;&gt;&lt;a href=&quot;#quant-model-paper-zoo&quot;&gt;&lt;strong&gt;Quant Model (Paper) Zoo&lt;/strong&gt;&lt;/a&gt;
                  &lt;ul&gt;
                    &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#run-a-single-model&quot;&gt;Run a Single Model&lt;/a&gt;&lt;/li&gt;
                    &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#run-multiple-models&quot;&gt;Run Multiple Models&lt;/a&gt;&lt;/li&gt;
                  &lt;/ul&gt;
                &lt;/li&gt;
              &lt;/ul&gt;
            &lt;/li&gt;
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#adapting-to-market-dynamics&quot;&gt;Adapting to Market Dynamics&lt;/a&gt;&lt;/li&gt;
          &lt;li type=&quot;circle&quot;&gt;&lt;a href=&quot;#reinforcement-learning-modeling-continuous-decisions&quot;&gt;Reinforcement Learning: modeling continuous decisions&lt;/a&gt;&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
      &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

# Plans
New features under development(order by estimated release time).
Your feedbacks about the features are very important.
&lt;!-- | Feature                        | Status      | --&gt;
&lt;!-- | --                      | ------    | --&gt;

# Framework of Qlib

&lt;div style=&quot;align: center&quot;&gt;
&lt;img src=&quot;docs/_static/img/framework-abstract.jpg&quot; /&gt;
&lt;/div&gt;

The high-level framework of Qlib can be found above(users can find the [detailed framework](https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework) of Qlib&#039;s design when getting into nitty gritty).
The components are designed as loose-coupled modules, and each component could be used stand-alone.

Qlib provides a strong infrastructure to support Quant research. [Data](https://qlib.readthedocs.io/en/latest/component/data.html) is always an important part.
A strong learning framework is designed to support diverse learning paradigms (e.g. [reinforcement learning](https://qlib.readthedocs.io/en/latest/component/rl.html), [supervised learning](https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section)) and patterns at different levels(e.g. [market dynamic modeling](https://qlib.readthedocs.io/en/latest/component/meta.html)).
By modeling the market, [trading strategies](https://qlib.readthedocs.io/en/latest/component/strategy.html) will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be [nested to be optimized and run together](https://qlib.readthedocs.io/en/latest/component/highfreq.html).
At last, a comprehensive [analysis](https://qlib.readthedocs.io/en/latest/component/report.html) will be provided and the model can be [served online](https://qlib.readthedocs.io/en/latest/component/online.html) in a low cost.


# Quick Start

This quick start guide tries to demonstrate
1. It&#039;s very easy to build a complete Quant research workflow and try your ideas with _Qlib_.
2. Though with *public data* and *simple models*, machine learning technologies **work very well** in practical Quant investment.

Here is a quick **[demo](https://terminalizer.com/view/3f24561a4470)** shows how to install ``Qlib``, and run LightGBM with ``qrun``. **But**, please make sure you have already prepared the data following the [instruction](#data-preparation).


## Installation

This table demonstrates the supported Python version of `Qlib`:
|               | install with pip      | install from source  |        plot        |
| ------------- |:---------------------:|:--------------------:|:------------------:|
| Python 3.8    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.9    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.10   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.11   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |
| Python 3.12   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |

**Note**: 
1. **Conda** is suggested for managing your Python environment. In some cases, using Python outside of a `conda` environment may result in missing header files, causing the installation failure of certain packages.
2. Please pay attention that installing cython in Python 3.6 will raise some error when installing ``Qlib`` from source. If users use Python 3.6 on their machines, it is recommended to *upgrade* Python to version 3.8 or higher, or use `conda`&#039;s Python to install ``Qlib`` from source.

### Install with pip
Users can easily install ``Qlib`` by pip according to the following command.

```bash
  pip install pyqlib
```

**Note**: pip will install the latest stable qlib. However, the main branch of qlib is in active development. If you want to test the latest scripts or functions in the main branch. Please install qlib with the methods below.

### Install from source
Also, users can install the latest dev version ``Qlib`` by the source code according to the following steps:

* Before installing ``Qlib`` from source, users need to install some dependencies:

  ```bash
  pip install numpy
  pip install --upgrade cython
  ```

* Clone the repository and install ``Qlib`` as follows.
    ```bash
    git clone https://github.com/microsoft/qlib.git &amp;&amp; cd qlib
    pip install .  # `pip install -e .[dev]` is recommended for development. check details in docs/developer/code_standard_and_dev_guide.rst
    ```

**Tips**: If you fail to install `Qlib` or run the examples in your environment,  comparing your steps and the [CI workflow](.github/workflows/test_qlib_from_source.yml) may help you find the problem.

**Tips for Mac**: If you are using Mac with M1, you might encounter issues in building the wheel for LightGBM, which is due to missing dependencies from OpenMP. To solve the problem, install openmp first with ``brew install libomp`` and then run ``pip install .`` to build it successfully. 

## Data Preparation
❗ Due to more restrict data security policy. The official dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.
Here is an example to download the latest data.
```bash
wget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz
mkdir -p ~/.qlib/qlib_data/cn_data
tar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1
rm -f qlib_bin.tar.gz
```

The official dataset below will resume in short future.


----

Load and prepare data by running the following code:

### Get with module
  ```bash
  # get 1d data
  python -m qlib.cli.data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

  # get 1min data
  python -m qlib.cli.data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

  ```

### Get from source

  ```bash
  # get 1d data
  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn

  # get 1min data
  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min

  ```

This dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in
the same repository.
Users could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)

*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.
We recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.

### Automatic update of daily frequency data (from yahoo finance)
  &gt; This step is *Optional* if users only want to try their models and strategies on history data.
  &gt; 
  &gt; It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.
  &gt;
  &gt; **NOTE**: Users can&#039;t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.
  &gt; 
  &gt; For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)

  * Automatic update of data to the &quot;qlib&quot; directory each trading day(Linux)
      * use *crontab*: `crontab -e`
      * set up timed tasks:

        ```
        * * * * 1-5 python &lt;script path&gt; update_data_to_bin --qlib_data_1d_dir &lt;user data dir&gt;
        ```
        * **script path**: *scripts/data_collector/yahoo/collector.py*

  * Manual update of data
      ```
      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir &lt;user data dir&gt; --trading_date &lt;start date&gt; --end_date &lt;end date&gt;
      ```
      * *trading_date*: start of trading day
      * *end_date*: end of trading day(not included)

### Checking the health of the data
  * We provide a script to check the health of the data, you can run the following commands to check whether the data is healthy or not.
    ```
    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data
    ```
  * Of course, you can also add some parameters to adjust the test results, such as this.
    ```
    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_dat

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[RVC-Project/Retrieval-based-Voice-Conversion-WebUI]]></title>
            <link>https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI</link>
            <guid>https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI</guid>
            <pubDate>Mon, 01 Sep 2025 00:05:00 GMT</pubDate>
            <description><![CDATA[Easily train a good VC model with voice data <= 10 mins!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI">RVC-Project/Retrieval-based-Voice-Conversion-WebUI</a></h1>
            <p>Easily train a good VC model with voice data <= 10 mins!</p>
            <p>Language: Python</p>
            <p>Stars: 31,762</p>
            <p>Forks: 4,455</p>
            <p>Stars today: 26 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;h1&gt;Retrieval-based-Voice-Conversion-WebUI&lt;/h1&gt;
一个基于VITS的简单易用的变声框架&lt;br&gt;&lt;br&gt;

[![madewithlove](https://img.shields.io/badge/made_with-%E2%9D%A4-red?style=for-the-badge&amp;labelColor=orange
)](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI)

&lt;img src=&quot;https://counter.seku.su/cmoe?name=rvc&amp;theme=r34&quot; /&gt;&lt;br&gt;

[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&amp;logo=googlecolab&amp;color=525252)](https://colab.research.google.com/github/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/Retrieval_based_Voice_Conversion_WebUI.ipynb)
[![Licence](https://img.shields.io/badge/LICENSE-MIT-green.svg?style=for-the-badge)](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/LICENSE)
[![Huggingface](https://img.shields.io/badge/🤗%20-Spaces-yellow.svg?style=for-the-badge)](https://huggingface.co/lj1995/VoiceConversionWebUI/tree/main/)

[![Discord](https://img.shields.io/badge/RVC%20Developers-Discord-7289DA?style=for-the-badge&amp;logo=discord&amp;logoColor=white)](https://discord.gg/HcsmBBGyVk)

[**更新日志**](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/blob/main/docs/Changelog_CN.md) | [**常见问题解答**](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/wiki/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E8%A7%A3%E7%AD%94) | [**AutoDL·5毛钱训练AI歌手**](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/wiki/Autodl%E8%AE%AD%E7%BB%83RVC%C2%B7AI%E6%AD%8C%E6%89%8B%E6%95%99%E7%A8%8B) | [**对照实验记录**](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/wiki/Autodl%E8%AE%AD%E7%BB%83RVC%C2%B7AI%E6%AD%8C%E6%89%8B%E6%95%99%E7%A8%8B](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/wiki/%E5%AF%B9%E7%85%A7%E5%AE%9E%E9%AA%8C%C2%B7%E5%AE%9E%E9%AA%8C%E8%AE%B0%E5%BD%95)) | [**在线演示**](https://modelscope.cn/studios/FlowerCry/RVCv2demo)

[**English**](./docs/en/README.en.md) | [**中文简体**](./README.md) | [**日本語**](./docs/jp/README.ja.md) | [**한국어**](./docs/kr/README.ko.md) ([**韓國語**](./docs/kr/README.ko.han.md)) | [**Français**](./docs/fr/README.fr.md) | [**Türkçe**](./docs/tr/README.tr.md) | [**Português**](./docs/pt/README.pt.md)

&lt;/div&gt;

&gt; 底模使用接近50小时的开源高质量VCTK训练集训练，无版权方面的顾虑，请大家放心使用

&gt; 请期待RVCv3的底模，参数更大，数据更大，效果更好，基本持平的推理速度，需要训练数据量更少。

&lt;table&gt;
   &lt;tr&gt;
		&lt;td align=&quot;center&quot;&gt;训练推理界面&lt;/td&gt;
		&lt;td align=&quot;center&quot;&gt;实时变声界面&lt;/td&gt;
	&lt;/tr&gt;
  &lt;tr&gt;
		&lt;td align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/assets/129054828/092e5c12-0d49-4168-a590-0b0ef6a4f630&quot;&gt;&lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;&lt;img src=&quot;https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/assets/129054828/730b4114-8805-44a1-ab1a-04668f3c30a6&quot;&gt;&lt;/td&gt;
	&lt;/tr&gt;
	&lt;tr&gt;
		&lt;td align=&quot;center&quot;&gt;go-web.bat&lt;/td&gt;
		&lt;td align=&quot;center&quot;&gt;go-realtime-gui.bat&lt;/td&gt;
	&lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align=&quot;center&quot;&gt;可以自由选择想要执行的操作。&lt;/td&gt;
		&lt;td align=&quot;center&quot;&gt;我们已经实现端到端170ms延迟。如使用ASIO输入输出设备，已能实现端到端90ms延迟，但非常依赖硬件驱动支持。&lt;/td&gt;
	&lt;/tr&gt;
&lt;/table&gt;

## 简介
本仓库具有以下特点
+ 使用top1检索替换输入源特征为训练集特征来杜绝音色泄漏
+ 即便在相对较差的显卡上也能快速训练
+ 使用少量数据进行训练也能得到较好结果(推荐至少收集10分钟低底噪语音数据)
+ 可以通过模型融合来改变音色(借助ckpt处理选项卡中的ckpt-merge)
+ 简单易用的网页界面
+ 可调用UVR5模型来快速分离人声和伴奏
+ 使用最先进的[人声音高提取算法InterSpeech2023-RMVPE](#参考项目)根绝哑音问题。效果最好（显著地）但比crepe_full更快、资源占用更小
+ A卡I卡加速支持

点此查看我们的[演示视频](https://www.bilibili.com/video/BV1pm4y1z7Gm/) !

## 环境配置
以下指令需在 Python 版本大于3.8的环境中执行。  

### Windows/Linux/MacOS等平台通用方法
下列方法任选其一。
#### 1. 通过 pip 安装依赖
1. 安装Pytorch及其核心依赖，若已安装则跳过。参考自: https://pytorch.org/get-started/locally/
```bash
pip install torch torchvision torchaudio
```
2. 如果是 win 系统 + Nvidia Ampere 架构(RTX30xx)，根据 #21 的经验，需要指定 pytorch 对应的 cuda 版本
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117
```
3. 根据自己的显卡安装对应依赖
- N卡
```bash
pip install -r requirements.txt
```
- A卡/I卡
```bash
pip install -r requirements-dml.txt
```
- A卡ROCM(Linux)
```bash
pip install -r requirements-amd.txt
```
- I卡IPEX(Linux)
```bash
pip install -r requirements-ipex.txt
```

#### 2. 通过 poetry 来安装依赖
安装 Poetry 依赖管理工具，若已安装则跳过。参考自: https://python-poetry.org/docs/#installation
```bash
curl -sSL https://install.python-poetry.org | python3 -
```

通过 Poetry 安装依赖时，python 建议使用 3.7-3.10 版本，其余版本在安装 llvmlite==0.39.0 时会出现冲突
```bash
poetry init -n
poetry env use &quot;path to your python.exe&quot;
poetry run pip install -r requirments.txt
```

### MacOS
可以通过 `run.sh` 来安装依赖
```bash
sh ./run.sh
```

## 其他预模型准备
RVC需要其他一些预模型来推理和训练。

你可以从我们的[Hugging Face space](https://huggingface.co/lj1995/VoiceConversionWebUI/tree/main/)下载到这些模型。

### 1. 下载 assets
以下是一份清单，包括了所有RVC所需的预模型和其他文件的名称。你可以在`tools`文件夹找到下载它们的脚本。

- ./assets/hubert/hubert_base.pt

- ./assets/pretrained 

- ./assets/uvr5_weights

想使用v2版本模型的话，需要额外下载

- ./assets/pretrained_v2

### 2. 安装 ffmpeg
若ffmpeg和ffprobe已安装则跳过。

#### Ubuntu/Debian 用户
```bash
sudo apt install ffmpeg
```
#### MacOS 用户
```bash
brew install ffmpeg
```
#### Windows 用户
下载后放置在根目录。
- 下载[ffmpeg.exe](https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/ffmpeg.exe)

- 下载[ffprobe.exe](https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/ffprobe.exe)

### 3. 下载 rmvpe 人声音高提取算法所需文件

如果你想使用最新的RMVPE人声音高提取算法，则你需要下载音高提取模型参数并放置于RVC根目录。

- 下载[rmvpe.pt](https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/rmvpe.pt)

#### 下载 rmvpe 的 dml 环境(可选, A卡/I卡用户)

- 下载[rmvpe.onnx](https://huggingface.co/lj1995/VoiceConversionWebUI/blob/main/rmvpe.onnx)

### 4. AMD显卡Rocm(可选, 仅Linux)

如果你想基于AMD的Rocm技术在Linux系统上运行RVC，请先在[这里](https://rocm.docs.amd.com/en/latest/deploy/linux/os-native/install.html)安装所需的驱动。

若你使用的是Arch Linux，可以使用pacman来安装所需驱动：
````
pacman -S rocm-hip-sdk rocm-opencl-sdk
````
对于某些型号的显卡，你可能需要额外配置如下的环境变量（如：RX6700XT）：
````
export ROCM_PATH=/opt/rocm
export HSA_OVERRIDE_GFX_VERSION=10.3.0
````
同时确保你的当前用户处于`render`与`video`用户组内：
````
sudo usermod -aG render $USERNAME
sudo usermod -aG video $USERNAME
````

## 开始使用
### 直接启动
使用以下指令来启动 WebUI
```bash
python infer-web.py
```

若先前使用 Poetry 安装依赖，则可以通过以下方式启动WebUI
```bash
poetry run python infer-web.py
```

### 使用整合包
下载并解压`RVC-beta.7z`
#### Windows 用户
双击`go-web.bat`
#### MacOS 用户
```bash
sh ./run.sh
```
### 对于需要使用IPEX技术的I卡用户(仅Linux)
```bash
source /opt/intel/oneapi/setvars.sh
```

## 参考项目
+ [ContentVec](https://github.com/auspicious3000/contentvec/)
+ [VITS](https://github.com/jaywalnut310/vits)
+ [HIFIGAN](https://github.com/jik876/hifi-gan)
+ [Gradio](https://github.com/gradio-app/gradio)
+ [FFmpeg](https://github.com/FFmpeg/FFmpeg)
+ [Ultimate Vocal Remover](https://github.com/Anjok07/ultimatevocalremovergui)
+ [audio-slicer](https://github.com/openvpi/audio-slicer)
+ [Vocal pitch extraction:RMVPE](https://github.com/Dream-High/RMVPE)
  + The pretrained model is trained and tested by [yxlllc](https://github.com/yxlllc/RMVPE) and [RVC-Boss](https://github.com/RVC-Boss).

## 感谢所有贡献者作出的努力
&lt;a href=&quot;https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI/graphs/contributors&quot; target=&quot;_blank&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=RVC-Project/Retrieval-based-Voice-Conversion-WebUI&quot; /&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[llamastack/llama-stack]]></title>
            <link>https://github.com/llamastack/llama-stack</link>
            <guid>https://github.com/llamastack/llama-stack</guid>
            <pubDate>Mon, 01 Sep 2025 00:04:59 GMT</pubDate>
            <description><![CDATA[Composable building blocks to build Llama Apps]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/llamastack/llama-stack">llamastack/llama-stack</a></h1>
            <p>Composable building blocks to build Llama Apps</p>
            <p>Language: Python</p>
            <p>Stars: 8,019</p>
            <p>Forks: 1,142</p>
            <p>Stars today: 11 stars today</p>
            <h2>README</h2><pre># Llama Stack

[![PyPI version](https://img.shields.io/pypi/v/llama_stack.svg)](https://pypi.org/project/llama_stack/)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-stack)](https://pypi.org/project/llama-stack/)
[![License](https://img.shields.io/pypi/l/llama_stack.svg)](https://github.com/meta-llama/llama-stack/blob/main/LICENSE)
[![Discord](https://img.shields.io/discord/1257833999603335178?color=6A7EC2&amp;logo=discord&amp;logoColor=ffffff)](https://discord.gg/llama-stack)
[![Unit Tests](https://github.com/meta-llama/llama-stack/actions/workflows/unit-tests.yml/badge.svg?branch=main)](https://github.com/meta-llama/llama-stack/actions/workflows/unit-tests.yml?query=branch%3Amain)
[![Integration Tests](https://github.com/meta-llama/llama-stack/actions/workflows/integration-tests.yml/badge.svg?branch=main)](https://github.com/meta-llama/llama-stack/actions/workflows/integration-tests.yml?query=branch%3Amain)

[**Quick Start**](https://llama-stack.readthedocs.io/en/latest/getting_started/index.html) | [**Documentation**](https://llama-stack.readthedocs.io/en/latest/index.html) | [**Colab Notebook**](./docs/getting_started.ipynb) | [**Discord**](https://discord.gg/llama-stack)


### ✨🎉 Llama 4 Support  🎉✨
We released [Version 0.2.0](https://github.com/meta-llama/llama-stack/releases/tag/v0.2.0) with support for the Llama 4 herd of models released by Meta.

&lt;details&gt;

&lt;summary&gt;👋 Click here to see how to run Llama 4 models on Llama Stack &lt;/summary&gt;

\
*Note you need 8xH100 GPU-host to run these models*

```bash
pip install -U llama_stack

MODEL=&quot;Llama-4-Scout-17B-16E-Instruct&quot;
# get meta url from llama.com
llama model download --source meta --model-id $MODEL --meta-url &lt;META_URL&gt;

# start a llama stack server
INFERENCE_MODEL=meta-llama/$MODEL llama stack build --run --template meta-reference-gpu

# install client to interact with the server
pip install llama-stack-client
```
### CLI
```bash
# Run a chat completion
MODEL=&quot;Llama-4-Scout-17B-16E-Instruct&quot;

llama-stack-client --endpoint http://localhost:8321 \
inference chat-completion \
--model-id meta-llama/$MODEL \
--message &quot;write a haiku for meta&#039;s llama 4 models&quot;

ChatCompletionResponse(
    completion_message=CompletionMessage(content=&quot;Whispers in code born\nLlama&#039;s gentle, wise heartbeat\nFuture&#039;s soft unfold&quot;, role=&#039;assistant&#039;, stop_reason=&#039;end_of_turn&#039;, tool_calls=[]),
    logprobs=None,
    metrics=[Metric(metric=&#039;prompt_tokens&#039;, value=21.0, unit=None), Metric(metric=&#039;completion_tokens&#039;, value=28.0, unit=None), Metric(metric=&#039;total_tokens&#039;, value=49.0, unit=None)]
)
```
### Python SDK
```python
from llama_stack_client import LlamaStackClient

client = LlamaStackClient(base_url=f&quot;http://localhost:8321&quot;)

model_id = &quot;meta-llama/Llama-4-Scout-17B-16E-Instruct&quot;
prompt = &quot;Write a haiku about coding&quot;

print(f&quot;User&gt; {prompt}&quot;)
response = client.inference.chat_completion(
    model_id=model_id,
    messages=[
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt},
    ],
)
print(f&quot;Assistant&gt; {response.completion_message.content}&quot;)
```
As more providers start supporting Llama 4, you can use them in Llama Stack as well. We are adding to the list. Stay tuned!


&lt;/details&gt;

### 🚀 One-Line Installer 🚀

To try Llama Stack locally, run:

```bash
curl -LsSf https://github.com/meta-llama/llama-stack/raw/main/scripts/install.sh | bash
```

### Overview

Llama Stack standardizes the core building blocks that simplify AI application development. It codifies best practices across the Llama ecosystem. More specifically, it provides

- **Unified API layer** for Inference, RAG, Agents, Tools, Safety, Evals, and Telemetry.
- **Plugin architecture** to support the rich ecosystem of different API implementations in various environments, including local development, on-premises, cloud, and mobile.
- **Prepackaged verified distributions** which offer a one-stop solution for developers to get started quickly and reliably in any environment.
- **Multiple developer interfaces** like CLI and SDKs for Python, Typescript, iOS, and Android.
- **Standalone applications** as examples for how to build production-grade AI applications with Llama Stack.

&lt;div style=&quot;text-align: center;&quot;&gt;
  &lt;img
    src=&quot;https://github.com/user-attachments/assets/33d9576d-95ea-468d-95e2-8fa233205a50&quot;
    width=&quot;480&quot;
    title=&quot;Llama Stack&quot;
    alt=&quot;Llama Stack&quot;
  /&gt;
&lt;/div&gt;

### Llama Stack Benefits
- **Flexible Options**: Developers can choose their preferred infrastructure without changing APIs and enjoy flexible deployment choices.
- **Consistent Experience**: With its unified APIs, Llama Stack makes it easier to build, test, and deploy AI applications with consistent application behavior.
- **Robust Ecosystem**: Llama Stack is already integrated with distribution partners (cloud providers, hardware vendors, and AI-focused companies) that offer tailored infrastructure, software, and services for deploying Llama models.

By reducing friction and complexity, Llama Stack empowers developers to focus on what they do best: building transformative generative AI applications.

### API Providers
Here is a list of the various API providers and available distributions that can help developers get started easily with Llama Stack.
Please checkout for [full list](https://llama-stack.readthedocs.io/en/latest/providers/index.html)

| API Provider Builder | Environments | Agents | Inference | VectorIO | Safety | Telemetry | Post Training | Eval | DatasetIO |
|:--------------------:|:------------:|:------:|:---------:|:--------:|:------:|:---------:|:-------------:|:----:|:--------:|
|    Meta Reference    | Single Node | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |
|      SambaNova       | Hosted | | ✅ | | ✅ | | | | |
|       Cerebras       | Hosted | | ✅ | | | | | | |
|      Fireworks       | Hosted | ✅ | ✅ | ✅ | | | | | |
|     AWS Bedrock      | Hosted | | ✅ | | ✅ | | | | |
|       Together       | Hosted | ✅ | ✅ | | ✅ | | | | |
|         Groq         | Hosted | | ✅ | | | | | | |
|        Ollama        | Single Node | | ✅ | | | | | | |
|         TGI          | Hosted/Single Node | | ✅ | | | | | | |
|      NVIDIA NIM      | Hosted/Single Node | | ✅ | | ✅ | | | | |
|       ChromaDB       | Hosted/Single Node | | | ✅ | | | | | |
|        Milvus        | Hosted/Single Node | | | ✅ | | | | | |
|        Qdrant        | Hosted/Single Node | | | ✅ | | | | | |
|       Weaviate       | Hosted/Single Node | | | ✅ | | | | | |
|      SQLite-vec      | Single Node | | | ✅ | | | | | |
|      PG Vector       | Single Node | | | ✅ | | | | | |
|  PyTorch ExecuTorch  | On-device iOS | ✅ | ✅ | | | | | | |
|         vLLM         | Single Node | | ✅ | | | | | | |
|        OpenAI        | Hosted | | ✅ | | | | | | |
|      Anthropic       | Hosted | | ✅ | | | | | | |
|        Gemini        | Hosted | | ✅ | | | | | | |
|       WatsonX        | Hosted | | ✅ | | | | | | |
|     HuggingFace      | Single Node | | | | | | ✅ | | ✅ |
|      TorchTune       | Single Node | | | | | | ✅ | | |
|     NVIDIA NEMO      | Hosted | | ✅ | ✅ | | | ✅ | ✅ | ✅ |
|        NVIDIA        | Hosted | | | | | | ✅ | ✅ | ✅ |

&gt; **Note**: Additional providers are available through external packages. See [External Providers](https://llama-stack.readthedocs.io/en/latest/providers/external.html) documentation.

### Distributions

A Llama Stack Distribution (or &quot;distro&quot;) is a pre-configured bundle of provider implementations for each API component. Distributions make it easy to get started with a specific deployment scenario - you can begin with a local development setup (eg. ollama) and seamlessly transition to production (eg. Fireworks) without changing your application code.
Here are some of the distributions we support:

|               **Distribution**                |                                                                    **Llama Stack Docker**                                                                     |                                                 Start This Distribution                                                  |
|:---------------------------------------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------:|:------------------------------------------------------------------------------------------------------------------------:|
|                Starter Distribution                 |           [llamastack/distribution-starter](https://hub.docker.com/repository/docker/llamastack/distribution-starter/general)           |      [Guide](https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/starter.html)      |
|                Meta Reference                 |           [llamastack/distribution-meta-reference-gpu](https://hub.docker.com/repository/docker/llamastack/distribution-meta-reference-gpu/general)           |      [Guide](https://llama-stack.readthedocs.io/en/latest/distributions/self_hosted_distro/meta-reference-gpu.html)      |
|                   PostgreSQL                  |                [llamastack/distribution-postgres-demo](https://hub.docker.com/repository/docker/llamastack/distribution-postgres-demo/general)                |                  |

### Documentation

Please checkout our [Documentation](https://llama-stack.readthedocs.io/en/latest/index.html) page for more details.

* CLI references
    * [llama (server-side) CLI Reference](https://llama-stack.readthedocs.io/en/latest/references/llama_cli_reference/index.html): Guide for using the `llama` CLI to work with Llama models (download, study prompts), and building/starting a Llama Stack distribution.
    * [llama (client-side) CLI Reference](https://llama-stack.readthedocs.io/en/latest/references/llama_stack_client_cli_reference.html): Guide for using the `llama-stack-client` CLI, which allows you to query information about the distribution.
* Getting Started
    * [Quick guide to start a Llama Stack server](https://llama-stack.readthedocs.io/en/latest/getting_started/index.html).
    * [Jupyter notebook](./docs/getting_started.ipynb) to walk-through how to use simple text and vision inference llama_stack_client APIs
    * The complete Llama Stack lesson [Colab notebook](https://colab.research.google.com/drive/1dtVmxotBsI4cGZQNsJRYPrLiDeT0Wnwt) of the new [Llama 3.2 course on Deeplearning.ai](https://learn.deeplearning.ai/courses/introducing-multimodal-llama-3-2/lesson/8/llama-stack).
    * A [Zero-to-Hero Guide](https://github.com/meta-llama/llama-stack/tree/main/docs/zero_to_hero_guide) that guide you through all the key components of llama stack with code samples.
* [Contributing](CONTRIBUTING.md)
    * [Adding a new API Provider](https://llama-stack.readthedocs.io/en/latest/contributing/new_api_provider.html) to walk-through how to add a new API provider.

### Llama Stack Client SDKs

|  **Language** |  **Client SDK** | **Package** |
| :----: | :----: | :----: |
| Python |  [llama-stack-client-python](https://github.com/meta-llama/llama-stack-client-python) | [![PyPI version](https://img.shields.io/pypi/v/llama_stack_client.svg)](https://pypi.org/project/llama_stack_client/)
| Swift  | [llama-stack-client-swift](https://github.com/meta-llama/llama-stack-client-swift) | [![Swift Package Index](https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Fmeta-llama%2Fllama-stack-client-swift%2Fbadge%3Ftype%3Dswift-versions)](https://swiftpackageindex.com/meta-llama/llama-stack-client-swift)
| Typescript   | [llama-stack-client-typescript](https://github.com/meta-llama/llama-stack-client-typescript) | [![NPM version](https://img.shields.io/npm/v/llama-stack-client.svg)](https://npmjs.org/package/llama-stack-client)
| Kotlin | [llama-stack-client-kotlin](https://github.com/meta-llama/llama-stack-client-kotlin) | [![Maven version](https://img.shields.io/maven-central/v/com.llama.llamastack/llama-stack-client-kotlin)](https://central.sonatype.com/artifact/com.llama.llamastack/llama-stack-client-kotlin)

Check out our client SDKs for connecting to a Llama Stack server in your preferred language, you can choose from [python](https://github.com/meta-llama/llama-stack-client-python), [typescript](https://github.com/meta-llama/llama-stack-client-typescript), [swift](https://github.com/meta-llama/llama-stack-client-swift), and [kotlin](https://github.com/meta-llama/llama-stack-client-kotlin) programming languages to quickly build your applications.

You can find more example scripts with client SDKs to talk with the Llama Stack server in our [llama-stack-apps](https://github.com/meta-llama/llama-stack-apps/tree/main/examples) repo.


## 🌟 GitHub Star History
## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=meta-llama/llama-stack&amp;type=Date)](https://www.star-history.com/#meta-llama/llama-stack&amp;Date)

## ✨ Contributors

Thanks to all of our amazing contributors!

&lt;a href=&quot;https://github.com/meta-llama/llama-stack/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=meta-llama/llama-stack&quot; /&gt;
&lt;/a&gt;</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[OpenBMB/MiniCPM-V]]></title>
            <link>https://github.com/OpenBMB/MiniCPM-V</link>
            <guid>https://github.com/OpenBMB/MiniCPM-V</guid>
            <pubDate>Mon, 01 Sep 2025 00:04:58 GMT</pubDate>
            <description><![CDATA[MiniCPM-V 4.5: A GPT-4o Level MLLM for Single Image, Multi Image and High-FPS Video Understanding on Your Phone]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenBMB/MiniCPM-V">OpenBMB/MiniCPM-V</a></h1>
            <p>MiniCPM-V 4.5: A GPT-4o Level MLLM for Single Image, Multi Image and High-FPS Video Understanding on Your Phone</p>
            <p>Language: Python</p>
            <p>Stars: 20,742</p>
            <p>Forks: 1,525</p>
            <p>Stars today: 110 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;./assets/minicpm_v_and_minicpm_o_title.png&quot; width=&quot;500em&quot; &gt;&lt;/img&gt; 

**A GPT-4o Level MLLM for Single Image, Multi Image and High-FPS Video Understanding on Your Phone**

  &lt;strong&gt;[中文](./README_zh.md) |
  English&lt;/strong&gt;



&lt;span style=&quot;display: inline-flex; align-items: center; margin-right: 2px;&quot;&gt;
  &lt;img src=&quot;./assets/wechat.png&quot; alt=&quot;WeChat&quot; style=&quot;margin-right: 4px;&quot;&gt;
  &lt;a href=&quot;docs/wechat.md&quot; target=&quot;_blank&quot;&gt; WeChat&lt;/a&gt; &amp;nbsp;|
&lt;/span&gt;
&amp;nbsp;
&lt;span style=&quot;display: inline-flex; align-items: center; margin-left: -8px;&quot;&gt;
&lt;img src=&quot;./assets/discord.png&quot; alt=&quot;Discord&quot; style=&quot;margin-right: 4px;&quot;&gt;
  &lt;a href=&quot;https://discord.gg/rftuRMbqzf&quot; target=&quot;_blank&quot;&gt; Discord&lt;/a&gt; &amp;nbsp;
&lt;/span&gt;



&lt;p align=&quot;center&quot;&gt;
   MiniCPM-V 4.5 &lt;a href=&quot;https://huggingface.co/openbmb/MiniCPM-V-4_5&quot;&gt;🤗&lt;/a&gt; &lt;a href=&quot;http://101.126.42.235:30910/&quot;&gt;🤖&lt;/a&gt; | MiniCPM-o 2.6 &lt;a href=&quot;https://huggingface.co/openbmb/MiniCPM-o-2_6&quot;&gt;🤗&lt;/a&gt;  &lt;a href=&quot;https://minicpm-omni-webdemo-us.modelbest.cn/&quot;&gt; 🤖&lt;/a&gt; | &lt;a href=&quot;https://github.com/OpenSQZ/MiniCPM-V-Cookbook&quot;&gt;🍳 Cookbook&lt;/a&gt; | 
  📄 Technical Report (Coming Soon)
&lt;/p&gt;

&lt;/div&gt;

**MiniCPM-V** is a series of efficient end-side multimodal LLMs (MLLMs), which accept images, videos and text as inputs and deliver high-quality text outputs. **MiniCPM-o** additionally takes audio as inputs and provides high-quality speech outputs in an end-to-end fashion. Since February 2024, we have released 7 versions of the model, aiming to achieve **strong performance and efficient deployment**. The most notable models in the series currently include:


- **MiniCPM-V 4.5**: 🔥🔥🔥 The latest and most capable model in the MiniCPM-V series. With a total of 8B parameters, this model **outperforms GPT-4o-latest, Gemini-2.0 Pro, and Qwen2.5-VL 72B** in vision-language capabilities, making it the most performant on-device multimodal model in the open-source community. This version brings **new features including efficient high-FPS and long video understanding (up to 96x compression rate for video tokens), controllable hybrid fast/deep thinking, strong handwritten OCR and complex table/document parsing**. It also advances MiniCPM-V&#039;s popular features such as trustworthy behavior, multilingual support and end-side deployability. 

- **MiniCPM-o 2.6**: ⭐️⭐️⭐️ The most capable model in the MiniCPM-o series. With a total of 8B parameters, this end-to-end model **achieves comparable performance to GPT-4o-202405 in vision, speech, and multimodal live streaming**, making it one of the most versatile and performant models in the open-source community. For the new voice mode, MiniCPM-o 2.6 **supports bilingual real-time speech conversation with configurable voices**, and also allows for fun capabilities such as emotion/speed/style control, end-to-end voice cloning, role play, etc. Due to its superior token density, MiniCPM-o 2.6 can for the first time **support multimodal live streaming on end-side devices** such as iPad.




## News &lt;!-- omit in toc --&gt;

#### 📌 Pinned

* [2025.08.26] 🔥🔥🔥 We open-source MiniCPM-V 4.5, which outperforms GPT-4o-latest, Gemini-2.0 Pro, and Qwen2.5-VL 72B. It advances popular capabilities of MiniCPM-V, and brings useful new features. Try it now!

* [2025.08.01] ⭐️⭐️⭐️ We open-sourced the [MiniCPM-V &amp; o Cookbook](https://github.com/OpenSQZ/MiniCPM-V-CookBook)! It provides comprehensive guides for diverse user scenarios, paired with our new [Docs Site](https://minicpm-o.readthedocs.io/en/latest/index.html) for smoother onboarding.

* [2025.06.20] ⭐️⭐️⭐️ Our official [Ollama repository](https://ollama.com/openbmb) is released. Try our latest models with [one click](https://ollama.com/openbmb/minicpm-o2.6)！

* [2025.03.01] 🚀🚀🚀 RLAIF-V, the alignment technique of MiniCPM-o, is accepted by CVPR 2025 Highlights！The [code](https://github.com/RLHF-V/RLAIF-V), [dataset](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset), [paper](https://arxiv.org/abs/2405.17220) are open-sourced!

* [2025.01.24] 📢📢📢 MiniCPM-o 2.6 technical report is released! See [here](https://openbmb.notion.site/MiniCPM-o-2-6-A-GPT-4o-Level-MLLM-for-Vision-Speech-and-Multimodal-Live-Streaming-on-Your-Phone-185ede1b7a558042b5d5e45e6b237da9).

* [2025.01.19] 📢 **ATTENTION!** We are currently working on merging MiniCPM-o 2.6 into the official repositories of llama.cpp, Ollama, and vllm. Until the merge is complete, please USE OUR LOCAL FORKS of [llama.cpp](https://github.com/OpenBMB/llama.cpp/blob/minicpm-omni/examples/llava/README-minicpmo2.6.md), [Ollama](https://github.com/OpenBMB/ollama/blob/minicpm-v2.6/examples/minicpm-v2.6/README.md), and [vllm](https://github.com/OpenBMB/MiniCPM-o?tab=readme-ov-file#efficient-inference-with-llamacpp-ollama-vllm). **Using the official repositories before the merge may lead to unexpected issues**.

* [2025.01.19] ⭐️⭐️⭐️ MiniCPM-o tops GitHub Trending and reaches top-2 on Hugging Face Trending!

* [2025.01.17] We have updated the usage of MiniCPM-o 2.6 int4 quantization version and resolved the model initialization error. Click [here](https://huggingface.co/openbmb/MiniCPM-o-2_6-int4) and try it now!

* [2025.01.13] 🔥🔥🔥 We open-source MiniCPM-o 2.6, which matches GPT-4o-202405 on vision, speech and multimodal live streaming. It advances popular capabilities of MiniCPM-V 2.6, and supports various new fun features. Try it now!

* [2024.08.17] 🚀🚀🚀 MiniCPM-V 2.6 is now fully supported by [official](https://github.com/ggerganov/llama.cpp) llama.cpp! GGUF models of various sizes are available [here](https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf).

* [2024.08.06] 🔥🔥🔥 We open-source MiniCPM-V 2.6, which outperforms GPT-4V on single image, multi-image and video understanding. It advances popular features of MiniCPM-Llama3-V 2.5, and can support real-time video understanding on iPad. Try it now!

* [2024.08.03] MiniCPM-Llama3-V 2.5 technical report is released! See [here](https://arxiv.org/abs/2408.01800).

* [2024.05.23] 🔥🔥🔥 MiniCPM-V tops GitHub Trending and Hugging Face Trending! Our demo, recommended by Hugging Face Gradio’s official account, is available [here](https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5). Come and try it out!

&lt;br&gt;

&lt;details&gt; 
&lt;summary&gt;Click to view more news.&lt;/summary&gt;

* [2025.08.02] 🚀🚀🚀 We open-source MiniCPM-V 4.0, which outperforms GPT-4.1-mini-20250414 in image understanding. It advances popular features of MiniCPM-V 2.6, and largely improves the efficiency. We also open-source the iOS App on iPhone and iPad. Try it now!

* [2025.01.23] 💡💡💡 MiniCPM-o 2.6 is now supported by [Align-Anything](https://github.com/PKU-Alignment/align-anything), a framework by PKU-Alignment Team for aligning any-to-any modality large models with human intentions. It supports DPO and SFT fine-tuning on both vision and audio. Try it now!

* [2024.08.15] We now also support multi-image SFT. For more details, please refer to the [document](https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune).
* [2024.08.14] MiniCPM-V 2.6 now also supports [fine-tuning](https://github.com/modelscope/ms-swift/issues/1613) with the SWIFT framework!
* [2024.08.10] 🚀🚀🚀 MiniCPM-Llama3-V 2.5 is now fully supported by [official](https://github.com/ggerganov/llama.cpp) llama.cpp! GGUF models of various sizes are available [here](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf).

* [2024.07.19] MiniCPM-Llama3-V 2.5 supports vLLM now! See [here](#inference-with-vllm).

* [2024.06.03] Now, you can run MiniCPM-Llama3-V 2.5 on multiple low VRAM GPUs(12 GB or 16 GB) by distributing the model&#039;s layers across multiple GPUs. For more details, check this [link](https://github.com/OpenBMB/MiniCPM-V/blob/main/docs/inference_on_multiple_gpus.md).
* [2024.05.28] 🚀🚀🚀 MiniCPM-Llama3-V 2.5 now fully supports its feature in llama.cpp and Ollama! Please pull the latest code **of our provided forks** ([llama.cpp](https://github.com/OpenBMB/llama.cpp/blob/minicpm-v2.5/examples/minicpmv/README.md), [Ollama](https://github.com/OpenBMB/ollama/tree/minicpm-v2.5/examples/minicpm-v2.5)). GGUF models in various sizes are available [here](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf/tree/main). MiniCPM-Llama3-V 2.5 series is **not supported by the official repositories yet**, and we are working hard to merge PRs. Please stay tuned!

* [2024.05.28] 💫 We now support LoRA fine-tuning for MiniCPM-Llama3-V 2.5, using only 2 V100 GPUs! See more statistics [here](https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune#model-fine-tuning-memory-usage-statistics).

* [2024.05.25] MiniCPM-Llama3-V 2.5 now supports streaming outputs and customized system prompts. Try it [here](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5#usage)!
* [2024.05.24] We release the MiniCPM-Llama3-V 2.5 [gguf](https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf), which supports [llama.cpp](#inference-with-llamacpp) inference and provides a 6~8 token/s smooth decoding on mobile phones. Try it now!

* [2024.05.23] 🔍 We&#039;ve released a comprehensive comparison between Phi-3-vision-128k-instruct and MiniCPM-Llama3-V 2.5, including benchmark evaluations, multilingual capabilities, and inference efficiency 🌟📊🌍🚀. Click [here](./docs/compare_with_phi-3_vision.md) to view more details.

* [2024.05.20] We open-soure MiniCPM-Llama3-V 2.5, it has improved OCR capability and supports 30+ languages, representing the first end-side MLLM achieving GPT-4V level performance! We provide [efficient inference](#deployment-on-mobile-phone) and [simple fine-tuning](./finetune/readme.md). Try it now!
* [2024.04.23] MiniCPM-V-2.0 supports vLLM now! Click [here](#inference-with-vllm) to view more details.
* [2024.04.18] We create a HuggingFace Space to host the demo of MiniCPM-V 2.0 at [here](https://huggingface.co/spaces/openbmb/MiniCPM-V-2)!
* [2024.04.17] MiniCPM-V-2.0 supports deploying [WebUI Demo](#webui-demo) now!
* [2024.04.15] MiniCPM-V-2.0 now also supports [fine-tuning](https://github.com/modelscope/swift/blob/main/docs/source/Multi-Modal/minicpm-v-2最佳实践.md) with the SWIFT framework!
* [2024.04.12] We open-source MiniCPM-V 2.0, which achieves comparable performance with Gemini Pro in understanding scene text and outperforms strong Qwen-VL-Chat 9.6B and Yi-VL 34B on &lt;a href=&quot;https://rank.opencompass.org.cn/leaderboard-multimodal&quot;&gt;OpenCompass&lt;/a&gt;, a comprehensive evaluation over 11 popular benchmarks. Click &lt;a href=&quot;https://openbmb.vercel.app/minicpm-v-2&quot;&gt;here&lt;/a&gt; to view the MiniCPM-V 2.0 technical blog.
* [2024.03.14] MiniCPM-V now supports [fine-tuning](https://github.com/modelscope/swift/blob/main/docs/source/Multi-Modal/minicpm-v最佳实践.md) with the SWIFT framework. Thanks to [Jintao](https://github.com/Jintao-Huang) for the contribution！
* [2024.03.01] MiniCPM-V can now be deployed on Mac!
* [2024.02.01] We open-source MiniCPM-V and OmniLMM-12B, which support efficient end-side deployment and powerful multimodal capabilities correspondingly.
&lt;/details&gt; 


## Contents &lt;!-- omit in toc --&gt;


- [MiniCPM-V 4.5](#minicpm-v-45)
- [MiniCPM-o 2.6](#minicpm-o-26)
- [MiniCPM-V \&amp; o Cookbook](#minicpm-v--o-cookbook)
- [Chat with Our Demo on Gradio 🤗](#chat-with-our-demo-on-gradio-)
- [Inference](#inference)
  - [Model Zoo](#model-zoo)
  - [Multi-turn Conversation](#multi-turn-conversation)
    - [Chat with Multiple Images](#chat-with-multiple-images)
    - [In-context Few-shot Learning](#in-context-few-shot-learning)
    - [Chat with Video](#chat-with-video)
    - [Speech and Audio Mode](#speech-and-audio-mode)
    - [Multimodal Live Streaming](#multimodal-live-streaming)
  - [Inference on Multiple GPUs](#inference-on-multiple-gpus)
  - [Inference on Mac](#inference-on-mac)
  - [Efficient Inference with llama.cpp, Ollama, vLLM](#efficient-inference-with-llamacpp-ollama-vllm)
- [Fine-tuning](#fine-tuning)
- [Awesome work using MiniCPM-V \&amp; MiniCPM-o](#awesome-work-using-minicpm-v--minicpm-o)
- [FAQs](#faqs)
- [Limitations](#limitations)


## MiniCPM-V 4.5

**MiniCPM-V 4.5** is the latest and most capable model in the MiniCPM-V series. The model is built on Qwen3-8B and SigLIP2-400M with a total of 8B parameters. It exhibits a significant performance improvement over previous MiniCPM-V and MiniCPM-o models, and introduces new useful features. Notable features of MiniCPM-V 4.5 include:

- 🔥 **State-of-the-art Vision-Language Capability.**
  MiniCPM-V 4.5 achieves an average score of 77.0 on OpenCompass, a comprehensive evaluation of 8 popular benchmarks. **With only 8B parameters, it surpasses widely used proprietary models like GPT-4o-latest, Gemini-2.0 Pro, and strong open-source models like Qwen2.5-VL 72B** for vision-language capabilities, making it the most performant MLLM under 30B parameters.

- 🎬 **Efficient High-FPS and Long Video Understanding.** Powered by a new unified 3D-Resampler over images and videos, MiniCPM-V 4.5 can now achieve 96x compression rate for video tokens, where 6 448x448 video frames can be jointly compressed into 64 video tokens (normally 1,536 tokens for most MLLMs). This means that the model can perceive significantly more video frames without increasing the LLM inference cost. This brings state-of-the-art high-FPS (up to 10FPS) video understanding and long video understanding capabilities on Video-MME, LVBench, MLVU, MotionBench, FavorBench, etc., efficiently.

- ⚙️ **Controllable Hybrid Fast/Deep Thinking.** MiniCPM-V 4.5 supports both fast thinking for efficient frequent usage with competitive performance, and deep thinking for more complex problem solving. To cover efficiency and performance trade-offs in different user scenarios, this fast/deep thinking mode can be switched in a highly controlled fashion.

- 💪 **Strong OCR, Document Parsing and Others.**
Based on [LLaVA-UHD](https://arxiv.org/pdf/2403.11703) architecture, MiniCPM-V 4.5 can process high-resolution images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344), using 4x fewer visual tokens than most MLLMs. The model achieves **leading performance on OCRBench, surpassing proprietary models such as GPT-4o-latest and Gemini 2.5**. It also achieves state-of-the-art performance for PDF document parsing capability on OmniDocBench among general MLLMs. Based on the latest [RLAIF-V](https://github.com/RLHF-V/RLAIF-V/) and [VisCPM](https://github.com/OpenBMB/VisCPM) techniques, it features **trustworthy behaviors**, outperforming GPT-4o-latest on MMHal-Bench, and supports **multilingual capabilities** in more than 30 languages.


-  💫  **Easy Usage.**
MiniCPM-V 4.5 can be easily used in various ways: (1) [llama.cpp](https://github.com/tc-mb/llama.cpp/blob/Support-MiniCPM-V-4.5/docs/multimodal/minicpmv4.5.md) and [ollama](https://github.com/tc-mb/ollama/tree/MIniCPM-V) support for efficient CPU inference on local devices, (2) [int4](https://huggingface.co/openbmb/MiniCPM-V-4_5-int4), [GGUF](https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf) and [AWQ](https://github.com/tc-mb/AutoAWQ) format quantized models in 16 sizes, (3) [SGLang](https://github.com/tc-mb/sglang/tree/main) and [vLLM](#efficient-inference-with-llamacpp-ollama-vllm) support for high-throughput and memory-efficient inference, (4) fine-tuning on new domains and tasks with [Transformers](https://github.com/tc-mb/transformers/tree/main) and [LLaMA-Factory](./docs/llamafactory_train_and_infer.md), (5) quick [local WebUI demo](#chat-with-our-demo-on-gradio), (6) optimized [local iOS app](https://github.com/tc-mb/MiniCPM-o-demo-iOS) on iPhone and iPad, and (7) online web demo on [server](http://101.126.42.235:30910/). See our [Cookbook](https://github.com/OpenSQZ/MiniCPM-V-CookBook) for full usage!


### Key Techniques &lt;!-- omit in toc --&gt;


&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;./assets/minicpm-v-4dot5-framework.png&quot; , width=100%&gt;
&lt;/div&gt;

- **Architechture: Unified 3D-Resampler for High-density Video Compression.** MiniCPM-V 4.5 introduces a 3D-Resampler that overcomes the performance-efficiency trade-off in video understanding. By grouping and jointly compressing up to 6 consecutive video frames into just 64 tokens (the same token count used for a single image in MiniCPM-V series), MiniCPM-V 4.5 achieves a 96× compression rate for video tokens. This allows the model to process more video frames without additional LLM computational cost, enabling high-FPS video and long video understanding. The architecture supports unified encoding for images, multi-image inputs, and videos, ensuring seamless capability and knowledge transfer.

- **Pre-training: Unified Learning for OCR and Knowledge from Documents.** Existing MLLMs learn OCR capability and knowledge from documents in isolated training approaches. We observe that the essential difference between these two training approaches is the visibility of the text in images. By dynamically corrupting text regions in documents with varying noise levels and asking the model to reconstruct the text, the model learns to adaptively and properly switch between accurate text recognition (when text is visible) and multimodal context-based knowledge reasoning (when text is heavily obscured). This eliminates reliance on error-prone document parsers in knowledge learning from documents, and prevents hallucinations from over-augmented OCR data, resulting in top-tier OCR and multimodal knowledge performance with minimal engineering overhead.

- **Post-training: Hybrid Fast/Deep Thinking with Multimodal RL.** MiniCPM-V 4.5 offers a balanced reasoning experience through two switchable modes: fast thinking for efficient daily use and deep thinking for complex tasks. Using a new hybrid reinforcement learning method, the model jointly optimizes both modes, significantly enhancing fast-mode performance without compromising deep-mode capability. Incorporated with [RLPR](https://github.com/OpenBMB/RLPR) and [RLAIF-V](https://github.com/RLHF-V/RLAIF-V), it generalizes robust reasoning skills from broad multimodal data while effectively reducing hallucinations.

### Evaluation  &lt;!-- omit in toc --&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./assets/radar_minicpm_v45.png&quot;, width=60%&gt;
&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;./assets/minicpmv_4_5_evaluation_result.png&quot; , width=80%&gt;
&lt;/div&gt;


### Inference Efficiency 


**OpenCompass**
&lt;div align=&quot;left&quot;&gt;
&lt;table style=&quot;margin: 0px auto;&quot;&gt;
    &lt;thead&gt;
            &lt;tr&gt;
              &lt;th align=&quot;left&quot;&gt;Model&lt;/th&gt;
              &lt;th&gt;Size&lt;/th&gt;
              &lt;th&gt;Avg Score ↑&lt;/th&gt;
              &lt;th&gt;Total Inference Time ↓&lt;/th&gt;
            &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody align=&quot;center&quot;&gt;
        &lt;tr&gt;
            &lt;td nowrap=&quot;nowrap&quot; align=&quot;left&quot;&gt;GLM-4.1V-9B-Thinking&lt;/td&gt;
            &lt;td&gt;10.3B&lt;/td&gt;
            &lt;td&gt;76.6&lt;/td&gt;
            &lt;td&gt;17.5h&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td nowrap=&quot;nowrap&quot; align=&quot;left&quot;&gt;MiMo-VL-7B-RL&lt;/td&gt;
            &lt;td&gt;8.3B&lt;/td&gt;
            &lt;td&gt;76.4&lt;/td&gt;
            &lt;td&gt;11h&lt;/td&gt;
        &lt;/tr&gt;
        &lt;tr&gt;
            &lt;td nowrap=&quot;nowrap&quot; align=&quot;left&quot;&gt;MiniCPM-V 4.5&lt;/td&gt;
            &lt;td&gt;8.7B&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;77.0&lt;/td&gt;
            &lt;td&gt;&lt;b&gt;7.5h&lt;/td&gt;
        &lt;/tr&gt;
    &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

**Video-MME**

&lt;div align=&quot;left&quot;&gt;
&lt;table style=&quot;margin: 0px auto;&quot;&gt;
    &lt;thead&gt;
          &lt;tr&gt;
              &lt;th align=&quot;left&quot;&gt;Model&lt;/th&gt;
              &lt;th&gt;Size&lt;/th&gt;
              &lt;th&gt;Avg Score ↑&lt;/th&gt;
              &lt;th&gt;Total Inference Time ↓&lt;/th&gt;
              &lt;th&gt;GPU Mem ↓&lt;/th&gt;
          &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody align=&quot;center&quot;&gt;
          &lt;tr&gt;
              &lt;td nowrap=&quot;nowrap&quot; align=&quot;left&quot;&gt;Qwen2.5-VL-7B-Instruct&lt;/td&gt;
              &lt;td&gt;8.3B&lt;/td&gt;
              &lt;td&gt;71.6&lt;/td&gt;
              &lt;td&gt;3h&lt;/td&gt;
              &lt;td&gt;60G&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
              &lt;td nowrap=&quot;nowrap&quot; align=&quot;left&quot;&gt;GLM-4.1V-9B-Thinking&lt;/td&gt;
              &lt;td&gt;10.3B&lt;/td&gt;
              &lt;td&gt;&lt;b&gt;73.6&lt;/td&gt;
              &lt;td&gt;2.63h&lt;/td&gt;
              &lt;td&gt;32G&lt;/td&gt;
          &lt;/tr&gt;
          &lt;tr&gt;
              &lt;td nowrap=&quot;nowrap&quot; align=&quot;left&quot;&gt;MiniCPM-V 4.5&lt;/td&gt;
              &lt;td&gt;8.7B&lt;/td&gt;
              &lt;td&gt;

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pydantic/pydantic-ai]]></title>
            <link>https://github.com/pydantic/pydantic-ai</link>
            <guid>https://github.com/pydantic/pydantic-ai</guid>
            <pubDate>Mon, 01 Sep 2025 00:04:57 GMT</pubDate>
            <description><![CDATA[Agent Framework / shim to use Pydantic with LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pydantic/pydantic-ai">pydantic/pydantic-ai</a></h1>
            <p>Agent Framework / shim to use Pydantic with LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 12,014</p>
            <p>Forks: 1,184</p>
            <p>Stars today: 49 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://ai.pydantic.dev/&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://ai.pydantic.dev/img/pydantic-ai-dark.svg&quot;&gt;
      &lt;img src=&quot;https://ai.pydantic.dev/img/pydantic-ai-light.svg&quot; alt=&quot;Pydantic AI&quot;&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;em&gt;Agent Framework / shim to use Pydantic with LLMs&lt;/em&gt;
&lt;/div&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml?query=branch%3Amain&quot;&gt;&lt;img src=&quot;https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml/badge.svg?event=push&quot; alt=&quot;CI&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://coverage-badge.samuelcolvin.workers.dev/redirect/pydantic/pydantic-ai&quot;&gt;&lt;img src=&quot;https://coverage-badge.samuelcolvin.workers.dev/pydantic/pydantic-ai.svg&quot; alt=&quot;Coverage&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pypi.python.org/pypi/pydantic-ai&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/pydantic-ai.svg&quot; alt=&quot;PyPI&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/pydantic/pydantic-ai&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/pyversions/pydantic-ai.svg&quot; alt=&quot;versions&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/pydantic/pydantic-ai/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/pydantic/pydantic-ai.svg?v&quot; alt=&quot;license&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://logfire.pydantic.dev/docs/join-slack/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Slack-Join%20Slack-4A154B?logo=slack&quot; alt=&quot;Join Slack&quot; /&gt;&lt;/a&gt;
&lt;/div&gt;

---

**Documentation**: [ai.pydantic.dev](https://ai.pydantic.dev/)

---

Pydantic AI is a Python agent framework designed to make it less painful to build production grade applications with Generative AI.

FastAPI revolutionized web development by offering an innovative and ergonomic design, built on the foundation of [Pydantic Validation](https://docs.pydantic.dev).

Similarly, virtually every agent framework and LLM library in Python uses Pydantic Validation, yet when we began to use LLMs in [Pydantic Logfire](https://pydantic.dev/logfire), we couldn&#039;t find anything that gave us the same feeling.

We built Pydantic AI with one simple aim: to bring that FastAPI feeling to GenAI app development.

## Why use Pydantic AI

- **Built by the Pydantic Team**
  Built by the team behind [Pydantic Validation](https://docs.pydantic.dev/latest/) (the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more).

- **Model-agnostic**
  Supports OpenAI, Anthropic, Gemini, Deepseek, Ollama, Groq, Cohere, and Mistral, and there is a simple interface to implement support for [other models](https://ai.pydantic.dev/models/).

- **Pydantic Logfire Integration**
  Seamlessly [integrates](https://ai.pydantic.dev/logfire/) with [Pydantic Logfire](https://pydantic.dev/logfire) for real-time debugging, performance monitoring, and behavior tracking of your LLM-powered applications.

- **Type-safe**
  Designed to make [type checking](https://ai.pydantic.dev/agents/#static-type-checking) as powerful and informative as possible for you.

- **Python-centric Design**
  Leverages Python&#039;s familiar control flow and agent composition to build your AI-driven projects, making it easy to apply standard Python best practices you&#039;d use in any other (non-AI) project.

- **Structured Responses**
  Harnesses the power of [Pydantic Validation](https://docs.pydantic.dev/latest/) to [validate and structure](https://ai.pydantic.dev/output/#structured-output) model outputs, ensuring responses are consistent across runs.

- **Dependency Injection System**
  Offers an optional [dependency injection](https://ai.pydantic.dev/dependencies/) system to provide data and services to your agent&#039;s [system prompts](https://ai.pydantic.dev/agents/#system-prompts), [tools](https://ai.pydantic.dev/tools/) and [output validators](https://ai.pydantic.dev/output/#output-validator-functions).
  This is useful for testing and eval-driven iterative development.

- **Streamed Responses**
  Provides the ability to [stream](https://ai.pydantic.dev/output/#streamed-results) LLM outputs continuously, with immediate validation, ensuring rapid and accurate outputs.

- **Graph Support**
  [Pydantic Graph](https://ai.pydantic.dev/graph) provides a powerful way to define graphs using typing hints, this is useful in complex applications where standard control flow can degrade to spaghetti code.

## Hello World Example

Here&#039;s a minimal example of Pydantic AI:

```python
from pydantic_ai import Agent

# Define a very simple agent including the model to use, you can also set the model when running the agent.
agent = Agent(
    &#039;google-gla:gemini-1.5-flash&#039;,
    # Register a static system prompt using a keyword argument to the agent.
    # For more complex dynamically-generated system prompts, see the example below.
    system_prompt=&#039;Be concise, reply with one sentence.&#039;,
)

# Run the agent synchronously, conducting a conversation with the LLM.
# Here the exchange should be very short: Pydantic AI will send the system prompt and the user query to the LLM,
# the model will return a text response. See below for a more complex run.
result = agent.run_sync(&#039;Where does &quot;hello world&quot; come from?&#039;)
print(result.output)
&quot;&quot;&quot;
The first known use of &quot;hello, world&quot; was in a 1974 textbook about the C programming language.
&quot;&quot;&quot;
```

_(This example is complete, it can be run &quot;as is&quot;)_

Not very interesting yet, but we can easily add &quot;tools&quot;, dynamic system prompts, and structured responses to build more powerful agents.

## Tools &amp; Dependency Injection Example

Here is a concise example using Pydantic AI to build a support agent for a bank:

**(Better documented example [in the docs](https://ai.pydantic.dev/#tools-dependency-injection-example))**

```python
from dataclasses import dataclass

from pydantic import BaseModel, Field
from pydantic_ai import Agent, RunContext

from bank_database import DatabaseConn


# SupportDependencies is used to pass data, connections, and logic into the model that will be needed when running
# system prompt and tool functions. Dependency injection provides a type-safe way to customise the behavior of your agents.
@dataclass
class SupportDependencies:
    customer_id: int
    db: DatabaseConn


# This pydantic model defines the structure of the output returned by the agent.
class SupportOutput(BaseModel):
    support_advice: str = Field(description=&#039;Advice returned to the customer&#039;)
    block_card: bool = Field(description=&quot;Whether to block the customer&#039;s card&quot;)
    risk: int = Field(description=&#039;Risk level of query&#039;, ge=0, le=10)


# This agent will act as first-tier support in a bank.
# Agents are generic in the type of dependencies they accept and the type of output they return.
# In this case, the support agent has type `Agent[SupportDependencies, SupportOutput]`.
support_agent = Agent(
    &#039;openai:gpt-4o&#039;,
    deps_type=SupportDependencies,
    # The response from the agent will, be guaranteed to be a SupportOutput,
    # if validation fails the agent is prompted to try again.
    output_type=SupportOutput,
    system_prompt=(
        &#039;You are a support agent in our bank, give the &#039;
        &#039;customer support and judge the risk level of their query.&#039;
    ),
)


# Dynamic system prompts can make use of dependency injection.
# Dependencies are carried via the `RunContext` argument, which is parameterized with the `deps_type` from above.
# If the type annotation here is wrong, static type checkers will catch it.
@support_agent.system_prompt
async def add_customer_name(ctx: RunContext[SupportDependencies]) -&gt; str:
    customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)
    return f&quot;The customer&#039;s name is {customer_name!r}&quot;


# `tool` let you register functions which the LLM may call while responding to a user.
# Again, dependencies are carried via `RunContext`, any other arguments become the tool schema passed to the LLM.
# Pydantic is used to validate these arguments, and errors are passed back to the LLM so it can retry.
@support_agent.tool
async def customer_balance(
        ctx: RunContext[SupportDependencies], include_pending: bool
) -&gt; float:
    &quot;&quot;&quot;Returns the customer&#039;s current account balance.&quot;&quot;&quot;
    # The docstring of a tool is also passed to the LLM as the description of the tool.
    # Parameter descriptions are extracted from the docstring and added to the parameter schema sent to the LLM.
    balance = await ctx.deps.db.customer_balance(
        id=ctx.deps.customer_id,
        include_pending=include_pending,
    )
    return balance


...  # In a real use case, you&#039;d add more tools and a longer system prompt


async def main():
    deps = SupportDependencies(customer_id=123, db=DatabaseConn())
    # Run the agent asynchronously, conducting a conversation with the LLM until a final response is reached.
    # Even in this fairly simple case, the agent will exchange multiple messages with the LLM as tools are called to retrieve an output.
    result = await support_agent.run(&#039;What is my balance?&#039;, deps=deps)
    # The `result.output` will be validated with Pydantic to guarantee it is a `SupportOutput`. Since the agent is generic,
    # it&#039;ll also be typed as a `SupportOutput` to aid with static type checking.
    print(result.output)
    &quot;&quot;&quot;
    support_advice=&#039;Hello John, your current account balance, including pending transactions, is $123.45.&#039; block_card=False risk=1
    &quot;&quot;&quot;

    result = await support_agent.run(&#039;I just lost my card!&#039;, deps=deps)
    print(result.output)
    &quot;&quot;&quot;
    support_advice=&quot;I&#039;m sorry to hear that, John. We are temporarily blocking your card to prevent unauthorized transactions.&quot; block_card=True risk=8
    &quot;&quot;&quot;
```

## Next Steps

To try Pydantic AI yourself, follow the instructions [in the examples](https://ai.pydantic.dev/examples/).

Read the [docs](https://ai.pydantic.dev/agents/) to learn more about building applications with Pydantic AI.

Read the [API Reference](https://ai.pydantic.dev/api/agent/) to understand Pydantic AI&#039;s interface.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[freqtrade/freqtrade]]></title>
            <link>https://github.com/freqtrade/freqtrade</link>
            <guid>https://github.com/freqtrade/freqtrade</guid>
            <pubDate>Mon, 01 Sep 2025 00:04:56 GMT</pubDate>
            <description><![CDATA[Free, open source crypto trading bot]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/freqtrade/freqtrade">freqtrade/freqtrade</a></h1>
            <p>Free, open source crypto trading bot</p>
            <p>Language: Python</p>
            <p>Stars: 41,742</p>
            <p>Forks: 8,498</p>
            <p>Stars today: 33 stars today</p>
            <h2>README</h2><pre># ![freqtrade](https://raw.githubusercontent.com/freqtrade/freqtrade/develop/docs/assets/freqtrade_poweredby.svg)

[![Freqtrade CI](https://github.com/freqtrade/freqtrade/actions/workflows/ci.yml/badge.svg?branch=develop)](https://github.com/freqtrade/freqtrade/actions/)
[![DOI](https://joss.theoj.org/papers/10.21105/joss.04864/status.svg)](https://doi.org/10.21105/joss.04864)
[![Coverage Status](https://coveralls.io/repos/github/freqtrade/freqtrade/badge.svg?branch=develop&amp;service=github)](https://coveralls.io/github/freqtrade/freqtrade?branch=develop)
[![Documentation](https://readthedocs.org/projects/freqtrade/badge/)](https://www.freqtrade.io)

Freqtrade is a free and open source crypto trading bot written in Python. It is designed to support all major exchanges and be controlled via Telegram or webUI. It contains backtesting, plotting and money management tools as well as strategy optimization by machine learning.

![freqtrade](https://raw.githubusercontent.com/freqtrade/freqtrade/develop/docs/assets/freqtrade-screenshot.png)

## Disclaimer

This software is for educational purposes only. Do not risk money which
you are afraid to lose. USE THE SOFTWARE AT YOUR OWN RISK. THE AUTHORS
AND ALL AFFILIATES ASSUME NO RESPONSIBILITY FOR YOUR TRADING RESULTS.

Always start by running a trading bot in Dry-run and do not engage money
before you understand how it works and what profit/loss you should
expect.

We strongly recommend you to have coding and Python knowledge. Do not
hesitate to read the source code and understand the mechanism of this bot.

## Supported Exchange marketplaces

Please read the [exchange specific notes](docs/exchanges.md) to learn about eventual, special configurations needed for each exchange.

- [X] [Binance](https://www.binance.com/)
- [X] [Bitmart](https://bitmart.com/)
- [X] [BingX](https://bingx.com/invite/0EM9RX)
- [X] [Bybit](https://bybit.com/)
- [X] [Gate.io](https://www.gate.io/ref/6266643)
- [X] [HTX](https://www.htx.com/)
- [X] [Hyperliquid](https://hyperliquid.xyz/) (A decentralized exchange, or DEX)
- [X] [Kraken](https://kraken.com/)
- [X] [OKX](https://okx.com/)
- [X] [MyOKX](https://okx.com/) (OKX EEA)
- [ ] [potentially many others](https://github.com/ccxt/ccxt/). _(We cannot guarantee they will work)_

### Supported Futures Exchanges (experimental)

- [X] [Binance](https://www.binance.com/)
- [X] [Gate.io](https://www.gate.io/ref/6266643)
- [X] [Hyperliquid](https://hyperliquid.xyz/) (A decentralized exchange, or DEX)
- [X] [OKX](https://okx.com/)
- [X] [Bybit](https://bybit.com/)

Please make sure to read the [exchange specific notes](docs/exchanges.md), as well as the [trading with leverage](docs/leverage.md) documentation before diving in.

### Community tested

Exchanges confirmed working by the community:

- [X] [Bitvavo](https://bitvavo.com/)
- [X] [Kucoin](https://www.kucoin.com/)

## Documentation

We invite you to read the bot documentation to ensure you understand how the bot is working.

Please find the complete documentation on the [freqtrade website](https://www.freqtrade.io).

## Features

- [x] **Based on Python 3.11+**: For botting on any operating system - Windows, macOS and Linux.
- [x] **Persistence**: Persistence is achieved through sqlite.
- [x] **Dry-run**: Run the bot without paying money.
- [x] **Backtesting**: Run a simulation of your buy/sell strategy.
- [x] **Strategy Optimization by machine learning**: Use machine learning to optimize your buy/sell strategy parameters with real exchange data.
- [X] **Adaptive prediction modeling**: Build a smart strategy with FreqAI that self-trains to the market via adaptive machine learning methods. [Learn more](https://www.freqtrade.io/en/stable/freqai/)
- [x] **Whitelist crypto-currencies**: Select which crypto-currency you want to trade or use dynamic whitelists.
- [x] **Blacklist crypto-currencies**: Select which crypto-currency you want to avoid.
- [x] **Builtin WebUI**: Builtin web UI to manage your bot.
- [x] **Manageable via Telegram**: Manage the bot with Telegram.
- [x] **Display profit/loss in fiat**: Display your profit/loss in fiat currency.
- [x] **Performance status report**: Provide a performance status of your current trades.

## Quick start

Please refer to the [Docker Quickstart documentation](https://www.freqtrade.io/en/stable/docker_quickstart/) on how to get started quickly.

For further (native) installation methods, please refer to the [Installation documentation page](https://www.freqtrade.io/en/stable/installation/).

## Basic Usage

### Bot commands

```
usage: freqtrade [-h] [-V]
                 {trade,create-userdir,new-config,show-config,new-strategy,download-data,convert-data,convert-trade-data,trades-to-ohlcv,list-data,backtesting,backtesting-show,backtesting-analysis,edge,hyperopt,hyperopt-list,hyperopt-show,list-exchanges,list-markets,list-pairs,list-strategies,list-hyperoptloss,list-freqaimodels,list-timeframes,show-trades,test-pairlist,convert-db,install-ui,plot-dataframe,plot-profit,webserver,strategy-updater,lookahead-analysis,recursive-analysis}
                 ...

Free, open source crypto trading bot

positional arguments:
  {trade,create-userdir,new-config,show-config,new-strategy,download-data,convert-data,convert-trade-data,trades-to-ohlcv,list-data,backtesting,backtesting-show,backtesting-analysis,edge,hyperopt,hyperopt-list,hyperopt-show,list-exchanges,list-markets,list-pairs,list-strategies,list-hyperoptloss,list-freqaimodels,list-timeframes,show-trades,test-pairlist,convert-db,install-ui,plot-dataframe,plot-profit,webserver,strategy-updater,lookahead-analysis,recursive-analysis}
    trade               Trade module.
    create-userdir      Create user-data directory.
    new-config          Create new config
    show-config         Show resolved config
    new-strategy        Create new strategy
    download-data       Download backtesting data.
    convert-data        Convert candle (OHLCV) data from one format to
                        another.
    convert-trade-data  Convert trade data from one format to another.
    trades-to-ohlcv     Convert trade data to OHLCV data.
    list-data           List downloaded data.
    backtesting         Backtesting module.
    backtesting-show    Show past Backtest results
    backtesting-analysis
                        Backtest Analysis module.
    hyperopt            Hyperopt module.
    hyperopt-list       List Hyperopt results
    hyperopt-show       Show details of Hyperopt results
    list-exchanges      Print available exchanges.
    list-markets        Print markets on exchange.
    list-pairs          Print pairs on exchange.
    list-strategies     Print available strategies.
    list-hyperoptloss   Print available hyperopt loss functions.
    list-freqaimodels   Print available freqAI models.
    list-timeframes     Print available timeframes for the exchange.
    show-trades         Show trades.
    test-pairlist       Test your pairlist configuration.
    convert-db          Migrate database to different system
    install-ui          Install FreqUI
    plot-dataframe      Plot candles with indicators.
    plot-profit         Generate plot showing profits.
    webserver           Webserver module.
    strategy-updater    updates outdated strategy files to the current version
    lookahead-analysis  Check for potential look ahead bias.
    recursive-analysis  Check for potential recursive formula issue.

options:
  -h, --help            show this help message and exit
  -V, --version         show program&#039;s version number and exit
```

### Telegram RPC commands

Telegram is not mandatory. However, this is a great way to control your bot. More details and the full command list on the [documentation](https://www.freqtrade.io/en/latest/telegram-usage/)

- `/start`: Starts the trader.
- `/stop`: Stops the trader.
- `/stopentry`: Stop entering new trades.
- `/status &lt;trade_id&gt;|[table]`: Lists all or specific open trades.
- `/profit [&lt;n&gt;]`: Lists cumulative profit from all finished trades, over the last n days.
- `/profit_long [&lt;n&gt;]`: Lists cumulative profit from all finished long trades, over the last n days.
- `/profit_short [&lt;n&gt;]`: Lists cumulative profit from all finished short trades, over the last n days.
- `/forceexit &lt;trade_id&gt;|all`: Instantly exits the given trade (Ignoring `minimum_roi`).
- `/fx &lt;trade_id&gt;|all`: Alias to `/forceexit`
- `/performance`: Show performance of each finished trade grouped by pair
- `/balance`: Show account balance per currency.
- `/daily &lt;n&gt;`: Shows profit or loss per day, over the last n days.
- `/help`: Show help message.
- `/version`: Show version.


## Development branches

The project is currently setup in two main branches:

- `develop` - This branch has often new features, but might also contain breaking changes. We try hard to keep this branch as stable as possible.
- `stable` - This branch contains the latest stable release. This branch is generally well tested.
- `feat/*` - These are feature branches, which are being worked on heavily. Please don&#039;t use these unless you want to test a specific feature.

## Support

### Help / Discord

For any questions not covered by the documentation or for further information about the bot, or to simply engage with like-minded individuals, we encourage you to join the Freqtrade [discord server](https://discord.gg/p7nuUNVfP7).

### [Bugs / Issues](https://github.com/freqtrade/freqtrade/issues?q=is%3Aissue)

If you discover a bug in the bot, please
[search the issue tracker](https://github.com/freqtrade/freqtrade/issues?q=is%3Aissue)
first. If it hasn&#039;t been reported, please
[create a new issue](https://github.com/freqtrade/freqtrade/issues/new/choose) and
ensure you follow the template guide so that the team can assist you as
quickly as possible.

For every [issue](https://github.com/freqtrade/freqtrade/issues/new/choose) created, kindly follow up and mark satisfaction or reminder to close issue when equilibrium ground is reached.

--Maintain github&#039;s [community policy](https://docs.github.com/en/site-policy/github-terms/github-community-code-of-conduct)--

### [Feature Requests](https://github.com/freqtrade/freqtrade/labels/enhancement)

Have you a great idea to improve the bot you want to share? Please,
first search if this feature was not [already discussed](https://github.com/freqtrade/freqtrade/labels/enhancement).
If it hasn&#039;t been requested, please
[create a new request](https://github.com/freqtrade/freqtrade/issues/new/choose)
and ensure you follow the template guide so that it does not get lost
in the bug reports.

### [Pull Requests](https://github.com/freqtrade/freqtrade/pulls)

Feel like the bot is missing a feature? We welcome your pull requests!

Please read the
[Contributing document](https://github.com/freqtrade/freqtrade/blob/develop/CONTRIBUTING.md)
to understand the requirements before sending your pull-requests.

Coding is not a necessity to contribute - maybe start with improving the documentation?
Issues labeled [good first issue](https://github.com/freqtrade/freqtrade/labels/good%20first%20issue) can be good first contributions, and will help get you familiar with the codebase.

**Note** before starting any major new feature work, *please open an issue describing what you are planning to do* or talk to us on [discord](https://discord.gg/p7nuUNVfP7) (please use the #dev channel for this). This will ensure that interested parties can give valuable feedback on the feature, and let others know that you are working on it.

**Important:** Always create your PR against the `develop` branch, not `stable`.

## Requirements

### Up-to-date clock

The clock must be accurate, synchronized to a NTP server very frequently to avoid problems with communication to the exchanges.

### Minimum hardware required

To run this bot we recommend you a cloud instance with a minimum of:

- Minimal (advised) system requirements: 2GB RAM, 1GB disk space, 2vCPU

### Software requirements

- [Python &gt;= 3.11](http://docs.python-guide.org/en/latest/starting/installation/)
- [pip](https://pip.pypa.io/en/stable/installing/)
- [git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)
- [TA-Lib](https://ta-lib.github.io/ta-lib-python/)
- [virtualenv](https://virtualenv.pypa.io/en/stable/installation.html) (Recommended)
- [Docker](https://www.docker.com/products/docker) (Recommended)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[iperov/DeepFaceLab]]></title>
            <link>https://github.com/iperov/DeepFaceLab</link>
            <guid>https://github.com/iperov/DeepFaceLab</guid>
            <pubDate>Mon, 01 Sep 2025 00:04:55 GMT</pubDate>
            <description><![CDATA[DeepFaceLab is the leading software for creating deepfakes.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/iperov/DeepFaceLab">iperov/DeepFaceLab</a></h1>
            <p>DeepFaceLab is the leading software for creating deepfakes.</p>
            <p>Language: Python</p>
            <p>Stars: 18,534</p>
            <p>Forks: 670</p>
            <p>Stars today: 42 stars today</p>
            <h2>README</h2><pre>&lt;table align=&quot;center&quot; border=&quot;0&quot;&gt;

&lt;tr&gt;&lt;td colspan=2 align=&quot;center&quot;&gt;

# DeepFaceLab  

&lt;a href=&quot;https://arxiv.org/abs/2005.05535&quot;&gt;

&lt;img src=&quot;https://static.arxiv.org/static/browse/0.3.0/images/icons/favicon.ico&quot; width=14&gt;&lt;/img&gt;
https://arxiv.org/abs/2005.05535&lt;/a&gt;

&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan=2 align=&quot;center&quot;&gt;

&lt;p align=&quot;center&quot;&gt;

![](doc/logo_tensorflow.png)
![](doc/logo_cuda.png)
![](doc/logo_directx.png)

&lt;/p&gt;

DeepFaceLab is used by such popular youtube channels as

|![](doc/tiktok_icon.png) [deeptomcruise](https://www.tiktok.com/@deeptomcruise)|![](doc/tiktok_icon.png) [1facerussia](https://www.tiktok.com/@1facerussia)|![](doc/tiktok_icon.png) [arnoldschwarzneggar](https://www.tiktok.com/@arnoldschwarzneggar)
|---|---|---|

|![](doc/tiktok_icon.png) [mariahcareyathome?](https://www.tiktok.com/@mariahcareyathome?)|![](doc/tiktok_icon.png) [diepnep](https://www.tiktok.com/@diepnep)|![](doc/tiktok_icon.png) [mr__heisenberg](https://www.tiktok.com/@mr__heisenberg)|![](doc/tiktok_icon.png) [deepcaprio](https://www.tiktok.com/@deepcaprio)
|---|---|---|---|

|![](doc/youtube_icon.png) [VFXChris Ume](https://www.youtube.com/channel/UCGf4OlX_aTt8DlrgiH3jN3g/videos)|![](doc/youtube_icon.png) [Sham00k](https://www.youtube.com/channel/UCZXbWcv7fSZFTAZV4beckyw/videos)|
|---|---|

|![](doc/youtube_icon.png) [Collider videos](https://www.youtube.com/watch?v=A91P2qtPT54&amp;list=PLayt6616lBclvOprvrC8qKGCO-mAhPRux)|![](doc/youtube_icon.png) [iFake](https://www.youtube.com/channel/UCC0lK2Zo2BMXX-k1Ks0r7dg/videos)|![](doc/youtube_icon.png) [NextFace](https://www.youtube.com/channel/UCFh3gL0a8BS21g-DHvXZEeQ/videos)|
|---|---|---|

|![](doc/youtube_icon.png) [Futuring Machine](https://www.youtube.com/channel/UCC5BbFxqLQgfnWPhprmQLVg)|![](doc/youtube_icon.png) [RepresentUS](https://www.youtube.com/channel/UCRzgK52MmetD9aG8pDOID3g)|![](doc/youtube_icon.png) [Corridor Crew](https://www.youtube.com/c/corridorcrew/videos)|
|---|---|---|

|![](doc/youtube_icon.png) [DeepFaker](https://www.youtube.com/channel/UCkHecfDTcSazNZSKPEhtPVQ)|![](doc/youtube_icon.png) [DeepFakes in movie](https://www.youtube.com/c/DeepFakesinmovie/videos)|
|---|---|

|![](doc/youtube_icon.png) [DeepFakeCreator](https://www.youtube.com/channel/UCkNFhcYNLQ5hr6A6lZ56mKA)|![](doc/youtube_icon.png) [Jarkan](https://www.youtube.com/user/Jarkancio/videos)|
|---|---|

&lt;/td&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;td colspan=2 align=&quot;center&quot;&gt;

# What can I do using DeepFaceLab?

&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan=2 align=&quot;center&quot;&gt;

## Replace the face

&lt;img src=&quot;doc/replace_the_face.jpg&quot; align=&quot;center&quot;&gt;

&lt;/td&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;td colspan=2 align=&quot;center&quot;&gt;

## De-age the face

&lt;/td&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;td align=&quot;center&quot; width=&quot;50%&quot;&gt;

&lt;img src=&quot;doc/deage_0_1.jpg&quot; align=&quot;center&quot;&gt;

&lt;/td&gt;
&lt;td align=&quot;center&quot; width=&quot;50%&quot;&gt;

&lt;img src=&quot;doc/deage_0_2.jpg&quot; align=&quot;center&quot;&gt;

&lt;/td&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;td colspan=2 align=&quot;center&quot;&gt;

![](doc/youtube_icon.png) https://www.youtube.com/watch?v=Ddx5B-84ebo

&lt;/td&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;td colspan=2 align=&quot;center&quot;&gt;

## Replace the head

&lt;/td&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;td align=&quot;center&quot; width=&quot;50%&quot;&gt;

&lt;img src=&quot;doc/head_replace_1_1.jpg&quot; align=&quot;center&quot;&gt;

&lt;/td&gt;
&lt;td align=&quot;center&quot; width=&quot;50%&quot;&gt;

&lt;img src=&quot;doc/head_replace_1_2.jpg&quot; align=&quot;center&quot;&gt;

&lt;/td&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;td colspan=2 align=&quot;center&quot;&gt;

![](doc/youtube_icon.png) https://www.youtube.com/watch?v=RTjgkhMugVw

&lt;/td&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;td colspan=2 align=&quot;center&quot;&gt;

# Native resolution progress

&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan=2 align=&quot;center&quot;&gt;

&lt;img src=&quot;doc/deepfake_progress.png&quot; align=&quot;center&quot;&gt;

&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan=2 align=&quot;center&quot;&gt;

&lt;img src=&quot;doc/make_everything_ok.png&quot; align=&quot;center&quot;&gt;

Unfortunately, there is no &quot;make everything ok&quot; button in DeepFaceLab. You should spend time studying the workflow and growing your skills. A skill in programs such as *AfterEffects* or *Davinci Resolve* is also desirable.

&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan=2 align=&quot;center&quot;&gt;

## Mini tutorial

&lt;a href=&quot;https://www.youtube.com/watch?v=kOIMXt8KK8M&quot;&gt;

&lt;img src=&quot;doc/mini_tutorial.jpg&quot; align=&quot;center&quot;&gt;

&lt;/a&gt;

&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td colspan=2 align=&quot;center&quot;&gt;

## Releases

&lt;/td&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;td align=&quot;right&quot;&gt;
&lt;a href=&quot;https://tinyurl.com/2p9cvt25&quot;&gt;Windows (magnet link)&lt;/a&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;Last release. Use torrent client to download.&lt;/td&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;td align=&quot;right&quot;&gt;
&lt;a href=&quot;https://mega.nz/folder/Po0nGQrA#dbbttiNWojCt8jzD4xYaPw&quot;&gt;Windows (Mega.nz)&lt;/a&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;Contains new and prev releases.&lt;/td&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;td align=&quot;right&quot;&gt;
&lt;a href=&quot;https://disk.yandex.ru/d/7i5XTKIKVg5UUg&quot;&gt;Windows (yandex.ru)&lt;/a&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;Contains new and prev releases.&lt;/td&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;td align=&quot;right&quot;&gt;
&lt;a href=&quot;https://github.com/nagadit/DeepFaceLab_Linux&quot;&gt;Linux (github)&lt;/a&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;by @nagadit&lt;/td&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;td align=&quot;right&quot;&gt;
&lt;a href=&quot;https://github.com/elemantalcode/dfl&quot;&gt;CentOS Linux (github)&lt;/a&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;May be outdated. By @elemantalcode&lt;/td&gt;&lt;/tr&gt;

&lt;/table&gt;

&lt;table align=&quot;center&quot; border=&quot;0&quot;&gt;

&lt;tr&gt;&lt;td colspan=2 align=&quot;center&quot;&gt;

### Communication groups

&lt;/td&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;td align=&quot;right&quot;&gt;
&lt;a href=&quot;https://discord.gg/rxa7h9M6rH&quot;&gt;Discord&lt;/a&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;Official discord channel. English / Russian.&lt;/td&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;td colspan=2 align=&quot;center&quot;&gt;

## Related works

&lt;/td&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;td align=&quot;right&quot;&gt;
&lt;a href=&quot;https://github.com/iperov/DeepFaceLive&quot;&gt;DeepFaceLive&lt;/a&gt;
&lt;/td&gt;&lt;td align=&quot;center&quot;&gt;Real-time face swap for PC streaming or video calls&lt;/td&gt;&lt;/tr&gt;

&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;

&lt;table align=&quot;center&quot; border=&quot;0&quot;&gt;

&lt;tr&gt;&lt;td colspan=2 align=&quot;center&quot;&gt;

## How I can help the project?

&lt;/td&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;td colspan=2 align=&quot;center&quot;&gt;

### Star this repo

&lt;/td&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;td colspan=2 align=&quot;center&quot;&gt;

Register github account and push &quot;Star&quot; button.

&lt;/td&gt;&lt;/tr&gt;

&lt;/table&gt;

&lt;table align=&quot;center&quot; border=&quot;0&quot;&gt;
&lt;tr&gt;&lt;td colspan=2 align=&quot;center&quot;&gt;

## Meme zone

&lt;/td&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;td align=&quot;center&quot; width=&quot;50%&quot;&gt;

&lt;img src=&quot;doc/meme1.jpg&quot; align=&quot;center&quot;&gt;

&lt;/td&gt;

&lt;td align=&quot;center&quot; width=&quot;50%&quot;&gt;

&lt;img src=&quot;doc/meme2.jpg&quot; align=&quot;center&quot;&gt;

&lt;/td&gt;&lt;/tr&gt;

&lt;tr&gt;&lt;td colspan=2 align=&quot;center&quot;&gt;

&lt;sub&gt;#deepfacelab #faceswap #face-swap #deep-learning #deeplearning #deep-neural-networks #deepface #deep-face-swap #neural-networks #neural-nets #tensorflow #cuda #nvidia&lt;/sub&gt;

&lt;/td&gt;&lt;/tr&gt;



&lt;/table&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/RD-Agent]]></title>
            <link>https://github.com/microsoft/RD-Agent</link>
            <guid>https://github.com/microsoft/RD-Agent</guid>
            <pubDate>Mon, 01 Sep 2025 00:04:54 GMT</pubDate>
            <description><![CDATA[Research and development (R&D) is crucial for the enhancement of industrial productivity, especially in the AI era, where the core aspects of R&D are mainly focused on data and models. We are committed to automating these high-value generic R&D processes through R&D-Agent, which lets AI drive data-driven AI. 🔗https://aka.ms/RD-Agent-Tech-Report]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/RD-Agent">microsoft/RD-Agent</a></h1>
            <p>Research and development (R&D) is crucial for the enhancement of industrial productivity, especially in the AI era, where the core aspects of R&D are mainly focused on data and models. We are committed to automating these high-value generic R&D processes through R&D-Agent, which lets AI drive data-driven AI. 🔗https://aka.ms/RD-Agent-Tech-Report</p>
            <p>Language: Python</p>
            <p>Stars: 7,267</p>
            <p>Forks: 744</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre>&lt;h4 align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/_static/logo.png&quot; alt=&quot;RA-Agent logo&quot; style=&quot;width:70%; &quot;&gt;
  
  &lt;a href=&quot;https://rdagent.azurewebsites.net&quot; target=&quot;_blank&quot;&gt;🖥️ Live Demo&lt;/a&gt; |
  &lt;a href=&quot;https://rdagent.azurewebsites.net/factor_loop&quot; target=&quot;_blank&quot;&gt;🎥 Demo Video&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=JJ4JYO3HscM&amp;list=PLALmKB0_N3_i52fhUmPQiL4jsO354uopR&quot; target=&quot;_blank&quot;&gt;▶️YouTube&lt;/a&gt;   |
  &lt;a href=&quot;https://rdagent.readthedocs.io/en/latest/index.html&quot; target=&quot;_blank&quot;&gt;📖 Documentation&lt;/a&gt; |
  &lt;a href=&quot;https://aka.ms/RD-Agent-Tech-Report&quot; target=&quot;_blank&quot;&gt;📄 Tech Report&lt;/a&gt; |
  &lt;a href=&quot;#-paperwork-list&quot;&gt; 📃 Papers &lt;/a&gt;
&lt;/h3&gt;


[![CI](https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/ci.yml)
[![CodeQL](https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/github-code-scanning/codeql)
[![Dependabot Updates](https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/dependabot/dependabot-updates)
[![Lint PR Title](https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/pr.yml)
[![Release.yml](https://github.com/microsoft/RD-Agent/actions/workflows/release.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/release.yml)
[![Platform](https://img.shields.io/badge/platform-Linux-blue)](https://pypi.org/project/rdagent/#files)
[![PyPI](https://img.shields.io/pypi/v/rdagent)](https://pypi.org/project/rdagent/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/rdagent)](https://pypi.org/project/rdagent/)
[![Release](https://img.shields.io/github/v/release/microsoft/RD-Agent)](https://github.com/microsoft/RD-Agent/releases)
[![GitHub](https://img.shields.io/github/license/microsoft/RD-Agent)](https://github.com/microsoft/RD-Agent/blob/main/LICENSE)
[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit)](https://github.com/pre-commit/pre-commit)
[![Checked with mypy](https://www.mypy-lang.org/static/mypy_badge.svg)](http://mypy-lang.org/)
[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)
[![Chat](https://img.shields.io/badge/chat-discord-blue)](https://discord.gg/ybQ97B6Jjy)
[![Documentation Status](https://readthedocs.org/projects/rdagent/badge/?version=latest)](https://rdagent.readthedocs.io/en/latest/?badge=latest)
[![Readthedocs Preview](https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml/badge.svg)](https://github.com/microsoft/RD-Agent/actions/workflows/readthedocs-preview.yml) &lt;!-- this badge is too long, please place it in the last one to make it pretty --&gt; 
[![arXiv](https://img.shields.io/badge/arXiv-2505.14738-00ff00.svg)](https://arxiv.org/abs/2505.14738)



# 🏆 The Best Machine Learning Engineering Agent!

[MLE-bench](https://github.com/openai/mle-bench) is a comprehensive benchmark evaluating the performance of AI agents on machine learning engineering tasks. Utilizing datasets from 75 Kaggle competitions, MLE-bench provides robust assessments of AI systems&#039; capabilities in real-world ML engineering scenarios.

R&amp;D-Agent currently leads as the top-performing machine learning engineering agent on MLE-bench:

| Agent | Low == Lite (%) | Medium (%) | High (%) | All (%) |
|---------|--------|-----------|---------|----------|
| R&amp;D-Agent o1-preview | 48.18 ± 2.49 | 8.95 ± 2.36 | 18.67 ± 2.98 | 22.4 ± 1.1 |
| R&amp;D-Agent o3(R)+GPT-4.1(D) | 51.52 ± 6.21 | 7.89 ± 3.33 | 16.67 ± 3.65 | 22.45 ± 2.45 |
| AIDE o1-preview | 34.3 ± 2.4 | 8.8 ± 1.1 | 10.0 ± 1.9 | 16.9 ± 1.1 |

**Notes:**
- **O3(R)+GPT-4.1(D)**: This version is designed to both reduce average time per loop and leverage a cost-effective combination of backend LLMs by seamlessly integrating Research Agent (o3) with Development Agent (GPT-4.1).
- **AIDE o1-preview**: Represents the previously best public result on MLE-bench as reported in the original MLE-bench paper.
- Average and standard deviation results for R&amp;D-Agent o1-preview is based on a independent of 5 seeds and for R&amp;D-Agent o3(R)+GPT-4.1(D) is based on 6 seeds.
- According to MLE-Bench, the 75 competitions are categorized into three levels of complexity: **Low==Lite** if we estimate that an experienced ML engineer can produce a sensible solution in under 2 hours, excluding the time taken to train any models; **Medium** if it takes between 2 and 10 hours; and **High** if it takes more than 10 hours.

You can inspect the detailed runs of the above results online.
- [R&amp;D-Agent o1-preview detailed runs](https://aka.ms/RD-Agent_MLE-Bench_O1-preview)
- [R&amp;D-Agent o3(R)+GPT-4.1(D) detailed runs](https://aka.ms/RD-Agent_MLE-Bench_O3_GPT41)

For running R&amp;D-Agent on MLE-bench, refer to **[MLE-bench Guide: Running ML Engineering via MLE-bench](https://rdagent.readthedocs.io/en/latest/scens/data_science.html)**

# 🥇 The First Data-Centric Quant Multi-Agent Framework!

R&amp;D-Agent for Quantitative Finance, in short **RD-Agent(Q)**, is the first data-centric, multi-agent framework designed to automate the full-stack research and development of quantitative strategies via coordinated factor-model co-optimization.

![image](https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d)

Extensive experiments in real stock markets show that, at a cost under $10, RD-Agent(Q) achieves approximately 2× higher ARR than benchmark factor libraries while using over 70% fewer factors. It also surpasses state-of-the-art deep time-series models under smaller resource budgets. Its alternating factor–model optimization further delivers excellent trade-off between predictive accuracy and strategy robustness.

You can learn more details about **RD-Agent(Q)** through the [paper](https://arxiv.org/abs/2505.15155) and reproduce it through the [documentation](https://rdagent.readthedocs.io/en/latest/scens/quant_agent_fin.html).

# 📰 News
| 🗞️ News        | 📝 Description                 |
| --            | ------      |
| [Technical Report Release](#overall-technical-report) | Overall framework description and results on MLE-bench | 
| [R&amp;D-Agent-Quant Release](#deep-application-in-diverse-scenarios) | Apply R&amp;D-Agent to quant trading | 
| MLE-Bench Results Released | R&amp;D-Agent currently leads as the [top-performing machine learning engineering agent](#-the-best-machine-learning-engineering-agent) on MLE-bench |
| Support LiteLLM Backend | We now fully support **[LiteLLM](https://github.com/BerriAI/litellm)** as our default backend for integration with multiple LLM providers. |
| General Data Science Agent | [Data Science Agent](https://rdagent.readthedocs.io/en/latest/scens/data_science.html) |
| Kaggle Scenario release | We release **[Kaggle Agent](https://rdagent.readthedocs.io/en/latest/scens/data_science.html)**, try the new features!                  |
| Official WeChat group release  | We created a WeChat group, welcome to join! (🗪[QR Code](https://github.com/microsoft/RD-Agent/issues/880)) |
| Official Discord release  | We launch our first chatting channel in Discord (🗪[![Chat](https://img.shields.io/badge/chat-discord-blue)](https://discord.gg/ybQ97B6Jjy)) |
| First release | **R&amp;D-Agent** is released on GitHub |



# Data Science Agent Preview
Check out our demo video showcasing the current progress of our Data Science Agent under development:

https://github.com/user-attachments/assets/3eccbecb-34a4-4c81-bce4-d3f8862f7305

# 🌟 Introduction
&lt;div align=&quot;center&quot;&gt;
      &lt;img src=&quot;docs/_static/scen.png&quot; alt=&quot;Our focused scenario&quot; style=&quot;width:80%; &quot;&gt;
&lt;/div&gt;

R&amp;D-Agent aims to automate the most critical and valuable aspects of the industrial R&amp;D process, and we begin with focusing on the data-driven scenarios to streamline the development of models and data. 
Methodologically, we have identified a framework with two key components: &#039;R&#039; for proposing new ideas and &#039;D&#039; for implementing them.
We believe that the automatic evolution of R&amp;D will lead to solutions of significant industrial value.


&lt;!-- Tag Cloud --&gt;
R&amp;D is a very general scenario. The advent of R&amp;D-Agent can be your
- 💰 **Automatic Quant Factory** ([🎥Demo Video](https://rdagent.azurewebsites.net/factor_loop)|[▶️YouTube](https://www.youtube.com/watch?v=X4DK2QZKaKY&amp;t=6s))
- 🤖 **Data Mining Agent:** Iteratively proposing data &amp; models ([🎥Demo Video 1](https://rdagent.azurewebsites.net/model_loop)|[▶️YouTube](https://www.youtube.com/watch?v=dm0dWL49Bc0&amp;t=104s)) ([🎥Demo Video 2](https://rdagent.azurewebsites.net/dmm)|[▶️YouTube](https://www.youtube.com/watch?v=VIaSTZuoZg4))  and implementing them by gaining knowledge from data.
- 🦾 **Research Copilot:** Auto read research papers ([🎥Demo Video](https://rdagent.azurewebsites.net/report_model)|[▶️YouTube](https://www.youtube.com/watch?v=BiA2SfdKQ7o)) / financial reports ([🎥Demo Video](https://rdagent.azurewebsites.net/report_factor)|[▶️YouTube](https://www.youtube.com/watch?v=ECLTXVcSx-c)) and implement model structures or building datasets.
- 🤖 **Kaggle Agent:** Auto Model Tuning and Feature Engineering([🎥Demo Video Coming Soon...]()) and implementing them to achieve more in competitions.
- ...

You can click the links above to view the demo. We&#039;re continuously adding more methods and scenarios to the project to enhance your R&amp;D processes and boost productivity. 

Additionally, you can take a closer look at the examples in our **[🖥️ Live Demo](https://rdagent.azurewebsites.net/)**.

&lt;div align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://rdagent.azurewebsites.net/&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;docs/_static/demo.png&quot; alt=&quot;Watch the demo&quot; width=&quot;80%&quot;&gt;
    &lt;/a&gt;
&lt;/div&gt;


# ⚡ Quick start

### RD-Agent currently only supports Linux.

You can try above demos by running the following command:

### 🐳 Docker installation.
Users must ensure Docker is installed before attempting most scenarios. Please refer to the [official 🐳Docker page](https://docs.docker.com/engine/install/) for installation instructions.
Ensure the current user can run Docker commands **without using sudo**. You can verify this by executing `docker run hello-world`.

### 🐍 Create a Conda Environment
- Create a new conda environment with Python (3.10 and 3.11 are well-tested in our CI):
  ```sh
  conda create -n rdagent python=3.10
  ```
- Activate the environment:
  ```sh
  conda activate rdagent
  ```

### 🛠️ Install the R&amp;D-Agent

#### For Users
- You can directly install the R&amp;D-Agent package from PyPI:
  ```sh
  pip install rdagent
  ```

#### For Developers
- If you want to try the latest version or contribute to RD-Agent, you can install it from the source and follow the development setup:
  ```sh
  git clone https://github.com/microsoft/RD-Agent
  cd RD-Agent
  make dev
  ```

More details can be found in the [development setup](https://rdagent.readthedocs.io/en/latest/development.html).

### 💊 Health check
- rdagent provides a health check that currently checks two things.
  - whether the docker installation was successful.
  - whether the default port used by the [rdagent ui](https://github.com/microsoft/RD-Agent?tab=readme-ov-file#%EF%B8%8F-monitor-the-application-results) is occupied.
  ```sh
  rdagent health_check --no-check-env
  ```


### ⚙️ Configuration
- The demos requires following ability:
  - ChatCompletion
  - json_mode
  - embedding query

  You can set your Chat Model and Embedding Model in the following ways:

  &gt; **🔥 Attention**: We now provide experimental support for **DeepSeek** models! You can use DeepSeek&#039;s official API for cost-effective and high-performance inference. See the configuration example below for DeepSeek setup.

- **Using LiteLLM (Default)**: We now support LiteLLM as a backend for integration with multiple LLM providers. You can configure in multiple ways:

  **Option 1: Unified API base for both models**

  *Configuration Example: `OpenAI` Setup :*

  ```bash
  cat &lt;&lt; EOF  &gt; .env
  # Set to any model supported by LiteLLM.
  CHAT_MODEL=gpt-4o 
  EMBEDDING_MODEL=text-embedding-3-small
  # Configure unified API base
  OPENAI_API_BASE=&lt;your_unified_api_base&gt;
  OPENAI_API_KEY=&lt;replace_with_your_openai_api_key&gt;
  ```

  *Configuration Example: `Azure OpenAI` Setup :*

  &gt; Before using this configuration, please confirm in advance that your `Azure OpenAI API key` supports `embedded models`.

  ```bash
  cat &lt;&lt; EOF  &gt; .env
  EMBEDDING_MODEL=azure/&lt;Model deployment supporting embedding&gt;
  CHAT_MODEL=azure/&lt;your deployment name&gt;
  AZURE_API_KEY=&lt;replace_with_your_openai_api_key&gt;
  AZURE_API_BASE=&lt;your_unified_api_base&gt;
  AZURE_API_VERSION=&lt;azure api version&gt;
  ```

  **Option 2: Separate API bases for Chat and Embedding models**
  ```bash
  cat &lt;&lt; EOF  &gt; .env
  # Set to any model supported by LiteLLM.
  # Configure separate API bases for chat and embedding
  
  # CHAT MODEL:
  CHAT_MODEL=gpt-4o 
  OPENAI_API_BASE=&lt;your_chat_api_base&gt;
  OPENAI_API_KEY=&lt;replace_with_your_openai_api_key&gt;

  # EMBEDDING MODEL:
  # TAKE siliconflow as an example, you can use other providers.
  # Note: embedding requires litellm_proxy prefix
  EMBEDDING_MODEL=litellm_proxy/BAAI/bge-large-en-v1.5
  LITELLM_PROXY_API_KEY=&lt;replace_with_your_siliconflow_api_key&gt;
  LITELLM_PROXY_API_BASE=https://api.siliconflow.cn/v1
  ```

  *Configuration Example: `DeepSeek` Setup :*

  &gt;Since many users encounter configuration errors when setting up DeepSeek. Here&#039;s a complete working example for DeepSeek Setup:
  ```bash
  cat &lt;&lt; EOF  &gt; .env
  # CHAT MODEL: Using DeepSeek Official API
  CHAT_MODEL=deepseek/deepseek-chat 
  DEEPSEEK_API_KEY=&lt;replace_with_your_deepseek_api_key&gt;

  # EMBEDDING MODEL: Using SiliconFlow for embedding since deepseek has no embedding model.
  # Note: embedding requires litellm_proxy prefix
  EMBEDDING_MODEL=litellm_proxy/BAAI/bge-m3
  LITELLM_PROXY_API_KEY=&lt;replace_with_your_siliconflow_api_key&gt;
  LITELLM_PROXY_API_BASE=https://api.siliconflow.cn/v1
  ```

  Notice: If you are using reasoning models that include thought processes in their responses (such as \&lt;think&gt; tags), you need to set the following environment variable:
  ```bash
  REASONING_THINK_RM=True
  ```

  You can also use a deprecated backend if you only use `OpenAI API` or `Azure OpenAI` directly. For this deprecated setting and more configuration information, please refer to the [documentation](https://rdagent.readthedocs.io/en/latest/installation_and_configuration.html). 



- If your environment configuration is complete, please execute the following commands to check if your configuration is valid. This step is necessary.

  ```bash
  rdagent health_check
  ```

### 🚀 Run the Application

The **[🖥️ Live Demo](https://rdagent.azurewebsites.net/)** is implemented by the following commands(each item represents one demo, you can select the one you prefer):

- Run the **Automated Quantitative Trading &amp; Iterative Factors Model Joint Evolution**:  [Qlib](http://github.com/microsoft/qlib) self-loop factor &amp; model proposal and implementation application
  ```sh
  rdagent fin_quant
  ```

- Run the **Automated Quantitative Trading &amp; Iterative Factors Evolution**:  [Qlib](http://github.com/microsoft/qlib) self-loop factor proposal and implementation application
  ```sh
  rdagent fin_factor
  ```

- Run the **Automated Quantitative Trading &amp; Iterative Model Evolution**: [Qlib](http://github.com/microsoft/qlib) self-loop model proposal and implementation application
  ```sh
  rdagent fin_model
  ```

- Run the **Automated Quantitative Trading &amp; Factors Extraction from Financial Reports**:  Run the [Qlib](http://github.com/microsoft/qlib) factor extraction and implementation application based on financial reports
  ```sh
  # 1. Generally, you can run this scenario using the following command:
  rdagent fin_factor_report --report-folder=&lt;Your financial reports folder path&gt;

  # 2. Specifically, you need to prepare some financial reports first. You can follow this concrete example:
  wget https://github.com/SunsetWolf/rdagent_resource/releases/download/reports/all_reports.zip
  unzip all_reports.zip -d git_ignore_folder/reports
  rdagent fin_factor_report --report-folder=git_ignore_folder/reports
  ```

- Run the **Automated Model Research &amp; Development Copilot**: model extraction and implementation application
  ```sh
  # 1. Generally, you can run your own papers/reports with the following command:
  rdagent general_model &lt;Your paper URL&gt;

  # 2. Specifically, you can do it like this. For more details and additional paper examples, use `rdagent general_model -h`:
  rdagent general_model  &quot;https://arxiv.org/pdf/2210.09789&quot;
  ```

- Run the **Automated Medical Prediction Model Evolution**: Medical self-loop model proposal and implementation application

  ```bash
  # Generally, you can run the data science program with the following command:
  rdagent data_science --competition &lt;your competition name&gt;

  # Specifically, you need to create a folder for storing competition files (e.g., competition description file, competition datasets, etc.), and configure the path to the folder in your environment. In addition, you need to use chromedriver when you download the competition descriptors, which you can follow for this specific example:

  # 1. Download the dataset, extract it to the target folder.
  wget https://github.com/SunsetWolf/rdagent_resource/releases/download/ds_data/arf-12-hours-prediction-task.zip
  unzip arf-12-hours-prediction-task.zip -d ./git_ignore_folder/ds_data/

  # 2. Configure environment variables in the `.env` file
  dotenv set DS_LOCAL_DATA_PATH &quot;$(pwd)/git_ignore_folder/ds_data&quot;
  dotenv set DS_CODER_ON_WHOLE_PIPELINE True
  dotenv set DS_IF_USING_MLE_DATA False
  dotenv set DS_SAMPLE_DATA_BY_LLM False
  dotenv set DS_SCEN rdagent.scenarios.data_science.scen.DataScienceScen

  # 3. run the application
  rdagent data_science --competition arf-12-hours-prediction-task
  ```

  **NOTE:** For more information about the dataset, please refer to the [documentation](https://rdagent.readthedocs.io/en/latest/scens/data_science.html).

- Run the **Automated Kaggle Model Tuning &amp; Feature Engineering**:  self-loop model proposal and feature engineering implementation application &lt;br /&gt;
  &gt; Using **tabular-playground-series-dec-2021** as an example. &lt;br /&gt;
  &gt; 1. Register and login on the [Kaggle](https://www.kaggle.com/) website. &lt;br /&gt;
  &gt; 2. Configuring the Kaggle API. &lt;br /&gt;
  &gt; (1) Click on the avatar (usually in the top right corner of the page) -&gt; `Settings` -&gt; `Create New Token`, A file called `kaggle.json` will be downloaded. &lt;br /&gt;
  &gt; (2) Move `kaggle.json` to `~/.config/kaggle/` &lt;br /&gt;
  &gt; (3) Modify the permissions of the kaggle.json file. Reference command: `chmod 600 ~/.config/kaggle/kaggle.json` &lt;br /&gt;
  &gt; 3. Join the competition: Click `Join the competition` -&gt; `I Understand and Accept` at the bottom of the [competition details page](https://www.kaggle.com/competitions/tabular-playground-series-dec-2021/data).
  ```bash
  # Generally, you can run the Kaggle competition program with the following command:
  rdagent data_science --competition &lt;your competition name&gt;

  # 1. Configure environment variables in the `.env` file
  mkdir -p ./git_ignore_folder/ds_data
  dotenv set DS_LOCAL_DATA_PATH &quot;$(pwd)/git_ignore_folder/ds_data&quot;
  dotenv set DS_CODER_ON_WHOLE_PIPELINE True
  dotenv set DS_IF_USING_MLE_DATA True
  dotenv set DS_SAMPLE_DATA_BY_LLM True
  dotenv set DS_SCEN rdagent.scenarios.data_science.scen.KaggleScen

  # 2. run the application
  rdagent data_science --competition tabular-playground-series-dec-2021
  ```

### 🖥️ Monitor the Application Results
- You can run the following command for our demo program to see the run logs.

  ```sh
  rdagent ui --port 19899 --log-dir &lt;y

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pwndbg/pwndbg]]></title>
            <link>https://github.com/pwndbg/pwndbg</link>
            <guid>https://github.com/pwndbg/pwndbg</guid>
            <pubDate>Mon, 01 Sep 2025 00:04:53 GMT</pubDate>
            <description><![CDATA[Exploit Development and Reverse Engineering with GDB & LLDB Made Easy]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pwndbg/pwndbg">pwndbg/pwndbg</a></h1>
            <p>Exploit Development and Reverse Engineering with GDB & LLDB Made Easy</p>
            <p>Language: Python</p>
            <p>Stars: 9,169</p>
            <p>Forks: 1,069</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre>![repository-open-graph](https://github.com/pwndbg/pwndbg/assets/150354584/77b2e438-898f-416f-a989-4bef30759627)
# pwndbg

[![license](https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000)](https://choosealicense.com/licenses/mit/)
[![Tests](https://github.com/pwndbg/pwndbg/actions/workflows/tests.yml/badge.svg?branch=dev&amp;event=push)](https://github.com/pwndbg/pwndbg/actions/workflows/tests.yml)
[![codecov.io](https://codecov.io/github/pwndbg/pwndbg/branch/dev/badge.svg?token=i1cBPFVCav)](https://app.codecov.io/github/pwndbg/pwndbg/tree/dev)
[![Discord](https://img.shields.io/discord/843809097920413717?label=Discord&amp;style=plastic)](https://discord.gg/x47DssnGwm)

`pwndbg` (/paʊnˈdiˌbʌɡ/) is a GDB and LLDB plug-in that makes debugging suck less,
with a focus on features needed by low-level software developers, hardware hackers,
reverse-engineers and exploit developers.

It has a boatload of features, see our [Features page](https://pwndbg.re/pwndbg/latest/features/)
and [CHEATSHEET][CHEATSHEET] (feel free to print it!). If you have any questions you may read the
[documentation](https://pwndbg.re/pwndbg/latest/) or asks us in our [Discord server](https://discord.gg/x47DssnGwm).

[CHEATSHEET]: https://pwndbg.re/pwndbg/dev/CHEATSHEET.pdf

## Why?

Vanilla GDB and LLDB are terrible to use for reverse engineering and exploit development.
Typing `x/30gx $rsp` or navigating cumbersome LLDB commands is not fun and often provides
minimal information. The year is 2025, and core debuggers still lack many user-friendly
features such as a robust hexdump command. WinDbg users are completely lost when they
occasionally need to bump into GDB or LLDB.

Pwndbg is a Python module which can be loaded into GDB or run as a REPL interface for LLDB.
It provides a suite of utilities and enhancements that fill the gaps left by these debuggers,
smoothing out rough edges and making them more user-friendly.

## Installation

See [installation instructions](https://pwndbg.re/pwndbg/latest/setup).

## What about ...?

Many past ([gdbinit][gdbinit], [PEDA][PEDA]) and present projects ([GEF][GEF],
[bata24/GEF][bata24/GEF]) offer great features, but are hard to extend and are packaged
as large single files ([103KB][gdbinit2], [195KB][peda.py], [423KB][gef.py],
[4.12MB][bata24/gef.py]). Pwndbg aims to replace them with a faster, cleaner, and
more robust implementation.

[gdbinit]: https://github.com/gdbinit/Gdbinit
[gdbinit2]: https://github.com/gdbinit/Gdbinit/blob/master/gdbinit
[PEDA]: https://github.com/longld/peda
[peda.py]: https://github.com/longld/peda/blob/master/peda.py
[GEF]: https://github.com/hugsy/gef
[gef.py]: https://github.com/hugsy/gef/blob/main/gef.py
[bata24/GEF]: https://github.com/bata24/gef
[bata24/gef.py]: https://github.com/bata24/gef/blob/dev/gef.py

## When to Use GDB or LLDB?

Pwndbg supports both GDB and LLDB, and each debugger has its own strengths.
Here&#039;s a quick guide to help you decide which one to use:

| Use Case                                        | Supported Debugger   |
|-------------------------------------------------|----------------------|
| Debugging Linux binaries or ELF files           | **GDB**, **LLDB**    |
| Debugging Mach-O binaries on macOS              | **LLDB**             |
| Linux kernel debugging (qemu-system)            | **GDB**, **LLDB**    |
| Linux user-space emulation (qemu-user)          | **GDB**              |
| Embedded debugging (ARM Cortex M* or RISC-V/32) | **GDB**, **LLDB**    |

Pwndbg ensures a consistent experience across both, so switching between them is seamless.
&gt; The LLDB implementation in pwndbg is still in early-stage and may contain bugs or limitations.&lt;br/&gt;
&gt; Known issues are tracked in [GitHub Issues][lldb_tracker].
&gt;
&gt; If you encounter any problems, feel free to report them or discuss on our [Discord server](https://discord.gg/x47DssnGwm).

[lldb_tracker]: https://github.com/pwndbg/pwndbg/issues?q=is%3Aissue%20state%3Aopen%20label%3A%22LLDB%20Port%22

### Compatibility Table
| Feature     | Supported Version               | Notes                                |
|-------------|---------------------------------|--------------------------------------|
| pwndbg-gdb  | - Python 3.10+ &lt;br/&gt;- GDB 12.1+ | Battle-tested on Ubuntu 22.04/24.04  |
| pwndbg-lldb | - Python 3.12+ &lt;br/&gt;- LLDB 19+  | Experimental/early-stage support     |
| qemu-user   | QEMU 8.1+                       | vFile API is needed for vmmap        |
| qemu-system | QEMU 6.2+                       | Supported version since ubuntu 22.04 |


## Contributing
Pull requests are welcome ❤️. Check out the [Contributing Guide](https://pwndbg.re/pwndbg/dev/contributing/).

## Acknowledgements
Pwndbg was originally created by [Zach Riggle](https://github.com/zachriggle), who is no longer with us. We want to thank Zach for all of his contributions to pwndbg and the wider security community.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[inventree/InvenTree]]></title>
            <link>https://github.com/inventree/InvenTree</link>
            <guid>https://github.com/inventree/InvenTree</guid>
            <pubDate>Mon, 01 Sep 2025 00:04:52 GMT</pubDate>
            <description><![CDATA[Open Source Inventory Management System]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/inventree/InvenTree">inventree/InvenTree</a></h1>
            <p>Open Source Inventory Management System</p>
            <p>Language: Python</p>
            <p>Stars: 5,676</p>
            <p>Forks: 1,050</p>
            <p>Stars today: 207 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/images/logo/inventree.png&quot; alt=&quot;InvenTree logo&quot; width=&quot;200&quot; height=&quot;auto&quot; /&gt;
  &lt;h1&gt;InvenTree&lt;/h1&gt;
  &lt;p&gt;Open Source Inventory Management System &lt;/p&gt;

&lt;!-- Badges --&gt;
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/license/MIT)![GitHub tag (latest SemVer)](https://img.shields.io/github/v/tag/inventree/inventree)
![CI](https://github.com/inventree/inventree/actions/workflows/qc_checks.yaml/badge.svg)
[![Documentation Status](https://readthedocs.org/projects/inventree/badge/?version=latest)](https://inventree.readthedocs.io/en/latest/?badge=latest)
![Docker Build](https://github.com/inventree/inventree/actions/workflows/docker.yaml/badge.svg)
[![Netlify Status](https://api.netlify.com/api/v1/badges/9bbb2101-0a4d-41e7-ad56-b63fb6053094/deploy-status)](https://app.netlify.com/sites/inventree/deploys)
[![Performance Testing](https://dev.azure.com/InvenTree/InvenTree%20test%20statistics/_apis/build/status%2Fmatmair.InvenTree?branchName=testing)](https://dev.azure.com/InvenTree/InvenTree%20test%20statistics/_build/latest?definitionId=3&amp;branchName=testing)

[![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/7179/badge)](https://bestpractices.coreinfrastructure.org/projects/7179)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/inventree/InvenTree/badge)](https://securityscorecards.dev/viewer/?uri=github.com/inventree/InvenTree)
[![Maintainability Rating](https://sonarcloud.io/api/project_badges/measure?project=inventree_InvenTree&amp;metric=sqale_rating)](https://sonarcloud.io/summary/new_code?id=inventree_InvenTree)

[![codecov](https://codecov.io/gh/inventree/InvenTree/graph/badge.svg?token=9DZRGUUV7B)](https://codecov.io/gh/inventree/InvenTree)
[![Crowdin](https://badges.crowdin.net/inventree/localized.svg)](https://crowdin.com/project/inventree)
![GitHub commit activity](https://img.shields.io/github/commit-activity/m/inventree/inventree)
[![Docker Pulls](https://img.shields.io/docker/pulls/inventree/inventree)](https://hub.docker.com/r/inventree/inventree)

[![GitHub Org&#039;s stars](https://img.shields.io/github/stars/inventree?style=social)](https://github.com/inventree/InvenTree/)
[![Twitter Follow](https://img.shields.io/twitter/follow/inventreedb?style=social)](https://twitter.com/inventreedb)
[![Subreddit subscribers](https://img.shields.io/reddit/subreddit-subscribers/inventree?style=social)](https://www.reddit.com/r/InvenTree/)
[![Mastdon](https://img.shields.io/badge/dynamic/json?label=Mastodon&amp;query=followers_count&amp;url=https%3A%2F%2Fchaos.social%2Fapi%2Fv1%2Faccounts%2Flookup%3Facct=InvenTree&amp;logo=mastodon&amp;style=social)](https://chaos.social/@InvenTree)

&lt;h4&gt;
    &lt;a href=&quot;https://demo.inventree.org/&quot;&gt;View Demo&lt;/a&gt;
  &lt;span&gt; · &lt;/span&gt;
    &lt;a href=&quot;https://docs.inventree.org/en/latest/&quot;&gt;Documentation&lt;/a&gt;
  &lt;span&gt; · &lt;/span&gt;
    &lt;a href=&quot;https://github.com/inventree/InvenTree/issues/new?template=bug_report.md&amp;title=[BUG]&quot;&gt;Report Bug&lt;/a&gt;
  &lt;span&gt; · &lt;/span&gt;
    &lt;a href=&quot;https://github.com/inventree/InvenTree/issues/new?template=feature_request.md&amp;title=[FR]&quot;&gt;Request Feature&lt;/a&gt;
  &lt;/h4&gt;
&lt;/div&gt;

&lt;!-- About the Project --&gt;
## :star2: About the Project

InvenTree is an open-source Inventory Management System which provides powerful low-level stock control and part tracking. The core of the InvenTree system is a Python/Django database backend which provides an admin interface (web-based) and a REST API for interaction with external interfaces and applications. A powerful plugin system provides support for custom applications and extensions.

Check out [our website](https://inventree.org) for more details.

&lt;!-- Roadmap --&gt;
### :compass: Roadmap

Want to see what we are working on? Check out the [roadmap tag](https://github.com/inventree/InvenTree/issues?q=is%3Aopen+is%3Aissue+label%3Aroadmap) and [horizon milestone](https://github.com/inventree/InvenTree/milestone/42).

&lt;!-- Integration --&gt;
### :hammer_and_wrench: Integration

InvenTree is designed to be **extensible**, and provides multiple options for **integration** with external applications or addition of custom plugins:

* [InvenTree API](https://docs.inventree.org/en/latest/api/)
* [Python module](https://docs.inventree.org/en/latest/api/python/)
* [Plugin interface](https://docs.inventree.org/en/latest/plugins/)
* [Third party tools](https://docs.inventree.org/en/latest/plugins/integrate/)

&lt;!-- TechStack --&gt;
### :space_invader: Tech Stack

&lt;details&gt;
  &lt;summary&gt;Server&lt;/summary&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.python.org/&quot;&gt;Python&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.djangoproject.com/&quot;&gt;Django&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.django-rest-framework.org/&quot;&gt;DRF&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://django-q.readthedocs.io/&quot;&gt;Django Q&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://docs.allauth.org/&quot;&gt;Django-Allauth&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Database&lt;/summary&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.postgresql.org/&quot;&gt;PostgreSQL&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.mysql.com/&quot;&gt;MySQL&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://www.sqlite.org/&quot;&gt;SQLite&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://redis.io/&quot;&gt;Redis&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;Client&lt;/summary&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://react.dev/&quot;&gt;React&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://lingui.dev/&quot;&gt;Lingui&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://reactrouter.com/&quot;&gt;React Router&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://tanstack.com/query/&quot;&gt;TanStack Query&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://github.com/pmndrs/zustand&quot;&gt;Zustand&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://mantine.dev/&quot;&gt;Mantine&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://icflorescu.github.io/mantine-datatable/&quot;&gt;Mantine Data Table&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://codemirror.net/&quot;&gt;CodeMirror&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;DevOps&lt;/summary&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;https://hub.docker.com/r/inventree/inventree&quot;&gt;Docker&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://crowdin.com/project/inventree&quot;&gt;Crowdin&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://app.codecov.io/gh/inventree/InvenTree&quot;&gt;Codecov&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://sonarcloud.io/project/overview?id=inventree_InvenTree&quot;&gt;SonarCloud&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://packager.io/gh/inventree/InvenTree&quot;&gt;Packager.io&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/details&gt;

&lt;!-- Getting Started --&gt;
## 	:toolbox: Deployment / Getting Started

There are several options to deploy InvenTree.

&lt;div align=&quot;center&quot;&gt;&lt;h4&gt;
    &lt;a href=&quot;https://docs.inventree.org/en/latest/start/docker/&quot;&gt;Docker&lt;/a&gt;
    &lt;span&gt; · &lt;/span&gt;
    &lt;a href=&quot;https://inventree.org/digitalocean&quot;&gt;&lt;img src=&quot;https://www.deploytodo.com/do-btn-blue-ghost.svg&quot; alt=&quot;Deploy to DO&quot; width=&quot;auto&quot; height=&quot;40&quot; /&gt;&lt;/a&gt;
    &lt;span&gt; · &lt;/span&gt;
    &lt;a href=&quot;https://docs.inventree.org/en/latest/start/install/&quot;&gt;Bare Metal&lt;/a&gt;
&lt;/h4&gt;&lt;/div&gt;

Single line install - read [the docs](https://docs.inventree.org/en/latest/start/installer/) for supported distros and details about the function:
```bash
wget -qO install.sh https://get.inventree.org &amp;&amp; bash install.sh
```

Refer to the [getting started guide](https://docs.inventree.org/en/latest/start/install/) for a full set of installation and setup instructions.

&lt;!-- Mobile App --&gt;
## 	:iphone: Mobile App

InvenTree is supported by a [companion mobile app](https://docs.inventree.org/app/) which allows users access to stock control information and functionality.

&lt;div align=&quot;center&quot;&gt;&lt;h4&gt;
    &lt;a href=&quot;https://play.google.com/store/apps/details?id=inventree.inventree_app&quot;&gt;Android Play Store&lt;/a&gt;
     &lt;span&gt; · &lt;/span&gt;
    &lt;a href=&quot;https://apps.apple.com/au/app/inventree/id1581731101#?platform=iphone&quot;&gt;Apple App Store&lt;/a&gt;
&lt;/h4&gt;&lt;/div&gt;

&lt;!-- Security --&gt;
## :lock: Code of Conduct &amp; Security Policy

The InvenTree project team is committed to providing a safe and welcoming environment for all users. Please read our [Code of Conduct](CODE_OF_CONDUCT.md) for more information.

InvenTree is following industry best practices for security. Our security policy is included [in this repo](SECURITY.md). We provide dedicated security pages on [our documentation site](https://docs.inventree.org/en/latest/security/).

&lt;!-- Contributing --&gt;
## :wave: Contributing

Contributions are welcomed and encouraged. Please help to make this project even better! Refer to the [contribution page](https://docs.inventree.org/en/latest/develop/contributing/).

&lt;!-- Translation --&gt;
## :scroll: Translation

Native language translation of the InvenTree web application is [community contributed via crowdin](https://crowdin.com/project/inventree). **Contributions are welcomed and encouraged**.

&lt;!-- Sponsor --&gt;
## :money_with_wings: Sponsor

If you use InvenTree and find it to be useful, please consider [sponsoring the project](https://github.com/sponsors/inventree).

&lt;!-- Acknowledgments --&gt;
## :gem: Acknowledgements

We want to acknowledge [PartKeepr](https://github.com/partkeepr/PartKeepr) as a valuable predecessor and inspiration.
Find a full list of used third-party libraries in the license information dialog of your instance.

## :heart: Support

&lt;p&gt;This project is supported by the following sponsors:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/MartinLoeper&quot;&gt;&lt;img src=&quot;https://github.com/MartinLoeper.png&quot; width=&quot;60px&quot; alt=&quot;Martin Löper&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/lippoliv&quot;&gt;&lt;img src=&quot;https://github.com/lippoliv.png&quot; width=&quot;60px&quot; alt=&quot;Oliver Lippert&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/lfg-seth&quot;&gt;&lt;img src=&quot;https://github.com/lfg-seth.png&quot; width=&quot;60px&quot; alt=&quot;Seth Smith&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/snorkrat&quot;&gt;&lt;img src=&quot;https://github.com/snorkrat.png&quot; width=&quot;60px&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/spacequest-ltd&quot;&gt;&lt;img src=&quot;https://github.com/spacequest-ltd.png&quot; width=&quot;60px&quot; alt=&quot;SpaceQuest Ltd&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/appwrite&quot;&gt;&lt;img src=&quot;https://github.com/appwrite.png&quot; width=&quot;60px&quot; alt=&quot;Appwrite&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/PricelessToolkit&quot;&gt;&lt;img src=&quot;https://github.com/PricelessToolkit.png&quot; width=&quot;60px&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/cabottech&quot;&gt;&lt;img src=&quot;https://github.com/cabottech.png&quot; width=&quot;60px&quot; alt=&quot;Cabot Technologies&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/markus-k&quot;&gt;&lt;img src=&quot;https://github.com/markus-k.png&quot; width=&quot;60px&quot; alt=&quot;Markus Kasten&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/jefffhaynes&quot;&gt;&lt;img src=&quot;https://github.com/jefffhaynes.png&quot; width=&quot;60px&quot; alt=&quot;Jeff Haynes&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/dnviti&quot;&gt;&lt;img src=&quot;https://github.com/dnviti.png&quot; width=&quot;60px&quot; alt=&quot;Daniele Viti&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/Islendur&quot;&gt;&lt;img src=&quot;https://github.com/Islendur.png&quot; width=&quot;60px&quot; alt=&quot;Islendur&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/Gibeon-NL&quot;&gt;&lt;img src=&quot;https://github.com/Gibeon-NL.png&quot; width=&quot;60px&quot; alt=&quot;Gibeon-NL&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/Motrac-Research-Engineering&quot;&gt;&lt;img src=&quot;https://github.com/Motrac-Research-Engineering.png&quot; width=&quot;60px&quot; alt=&quot;Motrac Research&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/trytuna&quot;&gt;&lt;img src=&quot;https://github.com/trytuna.png&quot; width=&quot;60px&quot; alt=&quot;Timo Scrappe&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/ATLAS2246&quot;&gt;&lt;img src=&quot;https://github.com/ATLAS2246.png&quot; width=&quot;60px&quot; alt=&quot;ATLAS2246&quot; /&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/Kedarius&quot;&gt;&lt;img src=&quot;https://github.com/Kedarius.png&quot; width=&quot;60px&quot; alt=&quot;Radek Hladik&quot; /&gt;&lt;/a&gt;

&lt;/p&gt;

&lt;p&gt;With ongoing resources provided by:&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://depot.dev?utm_source=inventree&quot;&gt;&lt;img src=&quot;https://depot.dev/badges/built-with-depot.svg&quot; alt=&quot;Built with Depot&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://inventree.org/digitalocean&quot;&gt;
    &lt;img src=&quot;https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg&quot; width=&quot;201px&quot; alt=&quot;Servers by Digital Ocean&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.netlify.com&quot;&gt; &lt;img src=&quot;https://www.netlify.com/v3/img/components/netlify-color-bg.svg&quot; alt=&quot;Deploys by Netlify&quot; /&gt; &lt;/a&gt;
  &lt;a href=&quot;https://crowdin.com&quot;&gt; &lt;img src=&quot;https://crowdin.com/images/crowdin-logo.svg&quot; alt=&quot;Translation by Crowdin&quot; /&gt; &lt;/a&gt; &lt;br&gt;
&lt;/p&gt;


&lt;!-- License --&gt;
## :warning: License

Distributed under the [MIT](https://choosealicense.com/licenses/mit/) License. See [LICENSE.txt](https://github.com/inventree/InvenTree/blob/master/LICENSE) for more information.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Johnserf-Seed/f2]]></title>
            <link>https://github.com/Johnserf-Seed/f2</link>
            <guid>https://github.com/Johnserf-Seed/f2</guid>
            <pubDate>Mon, 01 Sep 2025 00:04:51 GMT</pubDate>
            <description><![CDATA[High-speed downloader for multiple platforms]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Johnserf-Seed/f2">Johnserf-Seed/f2</a></h1>
            <p>High-speed downloader for multiple platforms</p>
            <p>Language: Python</p>
            <p>Stars: 1,431</p>
            <p>Forks: 238</p>
            <p>Stars today: 110 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/Johnserf-Seed/f2/raw/main/docs/public/f2-logo-with-shadow-svg@0.5x.svg&quot; alt=&quot;Logo&quot;&gt;
&lt;/p&gt;

[![Downloads](https://pepy.tech/badge/f2/month)](https://pepy.tech/project/f2)
[![PyPI version](https://badge.fury.io/py/f2.svg)](https://badge.fury.io/py/f2)
[![Dev Branch](https://badgen.net/badge/branch/v0.0.1.7-pw2/blue)](https://github.com/Johnserf-Seed/f2/tree/v0.0.1.7-pw2)
[![Discord](https://img.shields.io/discord/1146473603450282004?label=Discord)](https://discord.gg/3PhtPmgHf8)
[![codecov](https://codecov.io/gh/Johnserf-Seed/f2/graph/badge.svg?token=T9DH4QPZSS)](https://codecov.io/gh/Johnserf-Seed/f2)
[![TikHub](https://img.shields.io/badge/%E8%B5%9E%E5%8A%A9%E5%95%86-TikHub-orange?style=flat-square&amp;logo=tiktok)](https://beta-web.tikhub.io/users/signup?referral_code=6hLcGD94)
[![APACHE-2.0](https://img.shields.io/github/license/johnserf-seed/f2)](https://github.com/Johnserf-Seed/f2/blob/main/LICENSE)


[简体中文 readme](https://github.com/Johnserf-Seed/f2/blob/main/README.md) • [English readme](https://github.com/Johnserf-Seed/f2/blob/main/README.en.md)

`F2` 是一个 [Python](https://pypi.org/project/f2/) 库，提供多平台的作品下载与接口数据处理。支持 `DouYin`、`TikTok`、`Twitter`、`WeiBo` 等平台，且方便适配更多平台。

&lt;img src=&#039;https://github.com/user-attachments/assets/92a70f27-c93f-422e-ba9a-040060323654&#039;&gt;

## 🚀 快速入门

### ⚙️ 安装

- [必备条件](https://f2.wiki/install#%E5%BF%85%E5%A4%87%E6%9D%A1%E4%BB%B6)
- [包管理器安装](https://f2.wiki/install#%E5%8C%85%E7%AE%A1%E7%90%86%E5%99%A8%E5%AE%89%E8%A3%85)
- [编译安装](https://f2.wiki/install#%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85)

### ⚡ 快速使用

- [启动和运行](https://f2.wiki/quick-start#%E5%90%AF%E5%8A%A8%E5%92%8C%E8%BF%90%E8%A1%8C)
- [下一步是什么？](https://f2.wiki/quick-start#%E4%B8%8B%E4%B8%80%E6%AD%A5%E6%98%AF%E4%BB%80%E4%B9%88)

### 📋 配置文件

- [主配置文件](https://f2.wiki/site-config#%E4%B8%BB%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6)
- [初始化配置文件](https://f2.wiki/site-config#%E5%88%9D%E5%A7%8B%E5%8C%96%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6)
- [自定义配置文件](https://f2.wiki/site-config#%E8%87%AA%E5%AE%9A%E4%B9%89%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6)
- [配置Cookie](https://f2.wiki/site-config#%E9%85%8D%E7%BD%AECookie)
- [配置文件的位置](https://f2.wiki/site-config#%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E7%9A%84%E4%BD%8D%E7%BD%AE)
- [下一步是什么？](https://f2.wiki/site-config#%E4%B8%8B%E4%B8%80%E6%AD%A5%E6%98%AF%E4%BB%80%E4%B9%88)

### 💻 命令行

- [CLI临时配置](https://f2.wiki/cli#cli%E4%B8%B4%E6%97%B6%E9%85%8D%E7%BD%AE)
- [拓展](https://f2.wiki/cli#%E6%8B%93%E5%B1%95)
- [应用命令行](https://f2.wiki/cli#%E5%BA%94%E7%94%A8%E5%91%BD%E4%BB%A4%E8%A1%8C)

### 📚 进阶用法

- [DouYin 批量采集直播流](https://f2.wiki/advance-guide#%E6%89%B9%E9%87%8F%E9%87%87%E9%9B%86%E7%9B%B4%E6%92%AD%E6%B5%81)
- [DouYin 直播弹幕转发](https://f2.wiki/advance-guide#%E7%9B%B4%E6%92%AD%E5%BC%B9%E5%B9%95%E8%BD%AC%E5%8F%91)
- 文档还在进一步更新中...

## 🧐 FAQ

- [常见问题](https://f2.wiki/faq)

## 👏 团队

- [团队介绍](https://f2.wiki/team)

### 📘 开发指南

- [开发者必看](https://f2.wiki/guide/what-is-f2)

### 📝 API示例

- [使用示例](https://f2.wiki/guide/api-examples)

### 🧩 开发者接口

- [Bark](https://f2.wiki/guide/apps/bark/)
- [DouYin](https://f2.wiki/guide/apps/douyin/)
- [TikTok](https://f2.wiki/guide/apps/tiktok/)
- [Twitter](https://f2.wiki/guide/apps/twitter/)
- [WeiBo](https://f2.wiki/guide/apps/weibo/)

### 🖥️ 命令行指引

- [Bark](https://f2.wiki/guide/apps/bark/cli)
- [DouYin](https://f2.wiki/guide/apps/douyin/cli)
- [TikTok](https://f2.wiki/guide/apps/tiktok/cli)
- [Twitter](https://f2.wiki/guide/apps/twitter/cli)
- [WeiBo](https://f2.wiki/guide/apps/weibo/cli)

## ✨ 新变化

当下载或升级到 `F2` 的不同版本时，请注意以下关键的版本更新。

&lt;details&gt;
  &lt;summary&gt;🛠️ v0.0.1.7-pw2&lt;/summary&gt;

  - 🚀 **新增 Bark 应用支持**
    请前往 App Store 下载 [Bark](https://apps.apple.com/cn/app/id1403753865)，并在 [F2 配置文件](https://f2.wiki/site-config#%E4%B8%BB%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6) 中完成相关配置。
  - 🛡️ **开放 `ab` 算法**
    已开源满血版 `ab` 算法，支持自定义 `UA`。请确保自定义 `UA` 符合规范。
  - 📡 **新增直播弹幕转发功能**
    支持 `douyin` 与 `tiktok` 直播弹幕转发，请根据 [WSS 配置指南](https://f2.wiki/guide/what-is-f2#wss%E9%85%8D%E7%BD%AE) 完成相应参数配置。
  - 🔔 **启用通知推送**
    如需启用应用通知推送，请在 [F2 配置文件](https://f2.wiki/site-config#%E4%B8%BB%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6) 中设置 `enable_bark` 参数为 `true`。
  - 📄 **更多变更详情**
    请查看完整的 [ChangeLog](https://github.com/Johnserf-Seed/f2/blob/main/CHANGELOG.md#0017---2024-12-31)。
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;📌 v0.0.1.6-pw2&lt;/summary&gt;

  - 🛠️ **配置文件格式已更新**
    如果你使用旧版配置文件，请注意进行迁移。
  - 🌍 **时区标准化**
    所有时间戳的默认时区已设置为 `UTC/GMT+08:00`。
  - 📁 **文件格式调整**
    - `douyin` 直播流文件名调整为 `flv`。
    - 图集格式调整回 `webp`。
  - 🔄 **错误修复**
    修复了 `tiktok` 视频地址 `403` 错误。
    👉 [了解更多解决方案](https://f2.wiki/faq#tiktok-403-forbidden)
  - 🛡️ **算法优化**
    `douyin` 现在默认使用 `ab` 算法进行请求。（满血版 `ab` 算法即将开源）
  - 📄 **更多变更详情**
    👉 [查看 ChangeLog](https://github.com/Johnserf-Seed/f2/blob/main/CHANGELOG.md#0016---2024-05-04)
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;📡 v0.0.1.5-pw2&lt;/summary&gt;

  - 🛡️ **自定义 UA 支持**
    `XBogus` 参数现在支持自定义 `UA`，请确保 `UA` 符合规范。
  - 📊 **数据库重建**
    重建后的数据库包含接口的原始数据。
    👉 如需保留旧记录，请注意迁移或备份。
  - 🔄 **返回类型统一**
    所有 `fetch` 方法的返回类型已统一为过滤器类型，请注意相关变化。
  - 🛠️ **新功能**
    添加了 `_to_raw` 方法，可将过滤器转换为原始接口数据。
  - 📝 **文件名模板更新**
    如果文件名不符合新规范，将抛出异常，请检查并调整。
  - 🔗 **链接解析修复**
    修复了 `douyin` 合集页链接无法解析的问题。
    👉 [了解更多](#抖音合集作品)
  - 📄 **更多变更详情**
    👉 [查看 ChangeLog](https://github.com/Johnserf-Seed/f2/blob/main/CHANGELOG.md#0015---2024-04-04)
&lt;/details&gt;

## 📑 文档

`F2` 的目标是提供一个简单易用的接口，让用户可以快速获取作品数据。
在 `preview` 版本中很多功能没有完善，如果你发现了问题，请在 `F2` 项目中提交 `issue`。
[项目文档](https://f2.wiki/) 还在完善中，存在滞后的情况，请保持关注。

## 🗓️ Todo

- 将在 `0.0.1.8` 版本中添加 `BiliBili` &amp; `NetEaseMusic` 支持。
- 将在 `0.0.1.8` 版本中维护更多的 `API` 与 `CLI` 功能。
- 优化 `F2` 的 `CLI` 体验。
- 添加 `Socket` 代理支持。
- 添加 `Cookie` 池，`Proxy` 池，`User-Agent` 池等支持。
- 添加 `F2` 的 `WebAPI` 版本。
- [更多计划](https://github.com/Johnserf-Seed/f2/discussions/203)

## 🐛 更新

[ChangeLog](https://github.com/Johnserf-Seed/f2/blob/main/CHANGELOG.md)

## 💡 应用&amp;功能

功能状态：🟢代表已经实现，🟡代表正在实现，🟤代表暂时不实现，🔵代表未来实现，🔴代表将会弃用。
账号状态：⚪代表未知，🟣代表需要登录（无视自己账号隐私设置），⚫代表不需要登录（游客状态能看到的）。

完整的功能列表请查看 [API文档](https://f2.wiki/guide/api-examples)。

&lt;details&gt;
  &lt;summary&gt; 📠 Bark &lt;/summary&gt;

  |功能|账号状态|接口|功能状态|
  |---|---|---|---|
  |发送通知（GET）|⚪|`fetch_bark_notification`|🟢|
  |发送通知（POST）|⚪|`post_bark_notification`|🟢|
  |发送加密通知|⚪|`cipher_bark_notification`|🟢|

  |工具类|类名|接口|功能状态|
  |---|---|---|---|
  | 管理客户端配置     | `ClientConfManager` |   -    |  🟢  |
  | 生成随机数字字节   | - | `generate_numeric_bytes` |  🟢  |
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt; 📸 DouYin &lt;/summary&gt;

  - 🟣 表示需要登录才可以下载仅自己可见的作品、收藏作品、收藏夹作品或点赞作品等。（登录后无视自己的私密设置、可获取个性化内容）
  - ⚫ 表示不需要登录下载公开的作品、收藏夹作品、点赞作品等。（仅下载他人公开可见作品与页面）

  |功能|账号状态|接口|功能状态|
  |---|---|---|---|
  |用户信息|🟣⚫|`fetch_user_profile`|🟢|
  |单个作品（视频、图集、日常）|🟣⚫|`fetch_one_video`|🟢|
  |live图集|🟣⚫|`fetch_one_video`|🟢|
  |主页作品|🟣⚫|`fetch_user_post_videos`|🟢|
  |点赞作品|🟣⚫|`fetch_user_like_videos`|🟢|
  |收藏夹作品|🟣⚫|`fetch_user_collects_videos`|🟢|
  |收藏作品|🟣|`fetch_user_collection_videos`|🟢|
  |收藏原声|🟣|`fetch_user_music_collection`|🟢|
  |收藏合集|🟣|`fetch_user_mix_collection`|🔵|
  |收藏短剧|🟣|`fetch_user_series_collection`|🟤|
  |合集作品|⚫|`fetch_user_mix_videos`|🟢|
  |首页推荐作品|🟣⚫|`fetch_user_feed_videos`|🟢|
  |相似推荐作品|⚫|`fetch_related_videos`|🟢|
  |直播间信息（流下载）|⚫|`fetch_user_live_videos`、`fetch_user_live_videos_by_room_id`|🟢|
  |直播间弹幕负载|⚫|`fetch_live_im`|🟢|
  |直播间弹幕|⚫|`fetch_live_danmaku`|🟢|
  |查询用户基本信息|🟣⚫|`fetch_query_user`|🟢|
  |关注用户开播|🟣⚫|`fetch_user_following_lives`|🟢|
  |关注用户信息|🟣⚫|`fetch_user_following`|🟢|
  |粉丝用户信息|🟣⚫|`fetch_user_follower`|🟢|
  |关注用户作品|🟣⚫|`fetch_user_following_videos`|🟤|
  |粉丝用户作品|🟣⚫|`fetch_user_follower_videos`|🟤|
  |朋友作品|🟣|`fetch_friend_feed_videos`|🟢|
  |增加播放量|🟣⚫|`fetch_post_stats`|🟢|
  |搜索视频|⚫|`fetch_search_videos`|🔵|
  |搜索用户|⚫|`fetch_search_users`|🔵|
  |搜索直播|⚫|`fetch_search_lives`|🔵|
  |猜你想搜（相关搜索）|⚫|`fetch_search_suggest`|🟤|
  |抖音热点|⚫|`fetch_hot_search`|🟤|
  |作品评论|🟣⚫|`fetch_video_comments`|🔵|
  |观看历史|🟣|`fetch_user_history_read`|🟤|
  |稍后再看|🟣|`fetch_user_watch_later`|🟤|
  |...|...|...|...|

  |工具类|类名|接口|功能状态|
  |---|---|---|---|
  | 管理客户端配置        | `ClientConfManager`    |                              |  🟢  |
  | 生成真实msToken      | `TokenManager`         | `gen_real_msToken`           |  🟢  |
  | 生成虚假msToken      | `TokenManager`         | `gen_false_msToken`          |  🟢  |
  | 生成ttwid           | `TokenManager`         | `gen_ttwid`                  |  🟢  |
  | 生成webid           | `TokenManager`         | `gen_webid`                  |  🟢  |
  | 生成verify_fp       | `VerifyFpManager`      | `gen_verify_fp`              |  🟢  |
  | 生成s_v_web_id      | `VerifyFpManager`      | `gen_s_v_web_id`             |  🟢  |
  | 生成直播signature    | `DouyinWebcastSignature` | `get_signature`            |  🟢  |
  | 使用接口地址生成Xb参数 | `XBogusManager`        | `str_2_endpoint`             |  🟢  |
  | 使用接口模型生成Xb参数 | `XBogusManager`        | `model_2_endpoint`           |  🟢  |
  | 使用接口地址生成Ab参数 | `ABogusManager`        | `str_2_endpoint`             |  🟢  |
  | 使用接口模型生成Ab参数 | `ABogusManager`        | `model_2_endpoint`           |  🟢  |
  | 提取单个用户id       | `SecUserIdFetcher`     | `get_sec_user_id`            |  🟢  |
  | 提取列表用户id       | `SecUserIdFetcher`     | `get_all_sec_user_id`        |  🟢  |
  | 提取单个作品id       | `AwemeIdFetcher`       | `get_aweme_id`               |  🟢  |
  | 提取列表作品id       | `AwemeIdFetcher`       | `get_all_aweme_id`           |  🟢  |
  | 提取单个合集id       | `MixIdFetcher`         | `get_mix_id`                 |  🟢  |
  | 提取列表合集id       | `MixIdFetcher`         | `get_all_mix_id`             |  🟢  |
  | 提取单个直播间号      | `WebCastIdFetcher`     | `get_webcast_id`             |  🟢  |
  | 提取列表直播间号      | `WebCastIdFetcher`     | `get_all_webcast_id`         |  🟢  |
  | 全局格式化文件名      | -                      | `format_file_name`           |  🟢  |
  | 创建用户目录         | -                      | `create_user_folder`         |  🟢  |
  | 重命名用户目录        | -                      | `rename_user_folder`         |  🟢  |
  | 创建或重命名用户目录   | -                      | `create_or_rename_user_folder` | 🟢  |
  | json歌词转lrc歌词    | -                      | `json_2_lrc`                 |  🟢  |
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt; 🎶 TikTok &lt;/summary&gt;

  - 🟣 表示需要登录才可以下载仅自己可见的作品、收藏作品、收藏夹作品或点赞作品等。（登录后无视自己的私密设置、可获取个性化内容）
  - ⚫ 表示不需要登录下载公开的作品、收藏夹作品、点赞作品等。（仅下载他人公开可见作品与页面）

  |功能|账号状态|接口|功能状态|
  |---|---|---|---|
  |用户信息|🟣⚫|`fetch_user_profile`|🟢|
  |单个作品|🟣⚫|`fetch_one_video`|🟢|
  |主页作品|🟣⚫|`fetch_user_post_videos`|🟢|
  |点赞作品|🟣⚫|`fetch_user_like_videos`|🟢|
  |收藏作品|🟣⚫|`fetch_user_collect_videos`|🟢|
  |播放列表|🟣⚫|`fetch_play_list`|🟢|
  |播放列表作品|🟣⚫|`fetch_user_mix_videos`|🟢|
  |作品搜索|🟣⚫|`fetch_search_videos`|🟢|
  |直播间信息（流下载）|⚫|`fetch_user_live_videos`|🟢|
  |直播间弹幕负载|⚫|`fetch_live_im`|🟢|
  |直播间弹幕|⚫|`fetch_live_danmaku`|🟢|
  |检查开播|🟣⚫|`fetch_check_live_alive`|🟢|
  |...|...|...|...|

  |工具类|类名|接口|功能状态|
  |---|---|---|---|
  | 管理客户端配置     | `ClientConfManager`   |                  |  🟢  |
  | 生成真实msToken    | `TokenManager`     | `gen_real_msToken`   |  🟢  |
  | 生成虚假msToken     | `TokenManager`     | `gen_false_msToken`  |  🟢  |
  | 生成ttwid          | `TokenManager`     | `gen_ttwid`          |  🟢  |
  | 生成odin_tt        | `TokenManager`      | `gen_odin_tt`        |  🟢  |
  | 使用接口地址生成Xb参数 | `XBogusManager`    | `str_2_endpoint`    |  🟢  |
  | 使用接口模型生成Xb参数 | `XBogusManager`    | `model_2_endpoint`   |  🟢  |
  | 提取单个用户id       | `SecUserIdFetcher` | `get_secuid`         |  🟢  |
  | 提取列表用户id       | `SecUserIdFetcher` | `get_all_secuid`     |  🟢  |
  | 提取单个用户唯一id    | `SecUserIdFetcher` | `get_uniqueid`        |  🟢  |
  | 提取列表用户唯一id    | `SecUserIdFetcher` | `get_all_uniqueid`    |  🟢  |
  | 提取列表用户id       | `SecUserIdFetcher` | `get_all_secUid`   |  🟢  |
  | 提取单个作品id       | `AwemeIdFetcher`   | `get_aweme_id`          |  🟢  |
  | 提取列表作品id       | `AwemeIdFetcher`   | `get_all_aweme_id`      |  🟢  |
  | 生成deviceId       | `DeviceIdManager`  | `gen_device_id`        |  🟢  |
  | 生成devideId列表   | `DeviceIdManager`  | `gen_device_ids`   |  🟢  |
  | 全局格式化文件名      | -                | `format_file_name`      |  🟢  |
  | 创建用户目录         | -                | `create_user_folder`    |  🟢  |
  | 重命名用户目录       | -                | `rename_user_folder`     |  🟢  |
  | 创建或重命名用户目录  | -                | `create_or_rename_user_folder` |   🟢   |
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt; 🐦 Twitter &lt;/summary&gt;

  |功能|账号状态|接口|功能状态|
  |---|---|---|---|
  |推文详情|🟣⚫|`fetch_tweet_detail`|🟢|
  |用户信息|🟣⚫|`fetch_user_profile`|🟢|
  |主页推文|🟣⚫|`fetch_post_tweet`|🟢|
  |喜欢推文|🟣|`fetch_like_tweet`|🟢|
  |收藏推文|🟣|`fetch_bookmark_tweet`|🟢|

  |工具类|类名|接口|功能状态|
  |---|---|---|---|
  |  管理客户端配置     | `ClientConfManager`  |                          |  🟢  |
  |  提取用户唯一ID     | `UniqueIdFetcher`   | `get_unique_id`          |  🟢  |
  |  提取列表用户唯一ID  | `UniqueIdFetcher`   | `get_all_unique_ids`    |  🟢  |
  |  提取推文ID        | `TweetIdFetcher`    | `get_tweet_id`           |  🟢  |
  |  提取列表推文ID     | `TweetIdFetcher`   | `get_all_tweet_ids`       |  🟢  |
  |  全局格式化文件名    | -                  | `format_file_name`        |  🟢  |
  |  创建用户目录       | -                  | `create_user_folder`       |  🟢  |
  |  重命名用户目录      | -                 | `rename_user_folder`       |  🟢  |
  |  创建或重命名用户目录 | -                 | `create_or_rename_user_folder` |  🟢  |
  |  提取推文文案       | -                  | `extract_desc`              |  🟢  |
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt; 📱 WeiBo &lt;/summary&gt;

  |功能|账号状态|接口|功能状态|
  |---|---|---|---|
  |用户信息|🟣⚫|`fetch_user_info`|🟢|
  |用户详情|🟣⚫|`fetch_user_detail`|🟢|
  |主页微博|🟣⚫|`fetch_user_weibo`|🟢|
  |微博详情|🟣⚫|`fetch_weibo_detail`|🟢|

  |工具类|类名|接口|功能状态|
  |---|---|---|---|
  | 管理客户端配置       | `ClientConfManager`       |                               |  🟢  |
  | 生成访客 Cookie      | `VisitorManager`          | `gen_visitor`                 |  🟢  |
  | 提取微博 ID          | `WeiboIdFetcher`          | `get_weibo_id`                |  🟢  |
  | 提取列表微博 ID      | `WeiboIdFetcher`          | `get_all_weibo_id`            |  🟢  |
  | 提取微博用户 ID      | `WeiboUidFetcher`         | `get_weibo_uid`               |  🟢  |
  | 提取列表微博用户 ID  | `WeiboUidFetcher`         | `get_all_weibo_uid`           |  🟢  |
  | 提取微博用户昵称     | `WeiboScreenNameFetcher`  | `get_weibo_screen_name`       |  🟢  |
  | 提取列表微博用户昵称 | `WeiboScreenNameFetcher`  | `get_all_weibo_screen_name`   |  🟢  |
  | 全局格式化文件名     | -                         | `format_file_name`            |  🟢  |
  | 创建用户目录         | -                         | `create_user_folder`          |  🟢  |
  | 重命名用户目录       | -                         | `rename_user_folder`          |  🟢  |
  | 创建或重命名用户目录 | -                         | `create_or_rename_user_folder`|  🟢  |
  | 提取微博文案         | -                         | `extract_desc`                |  🟢  |
&lt;/details&gt;

## 📸 截图

&lt;details&gt;
  &lt;summary&gt; 🎬 Bark &lt;/summary&gt;

  ### 发送通知（GET）

  &lt;img src=&#039;https://github.com/user-attachments/assets/9c977737-c172-420a-9d7f-6f05bc843957&#039;&gt;

  &lt;img src=&#039;https://github.com/user-attachments/assets/9a59f9d6-cc8a-48bb-9a75-d075a9e13498&#039;&gt;

  ### 发送通知（POST）

  &lt;img src=&#039;https://github.com/user-attachments/assets/7a3e9054-a156-4be7-bf93-90d0963f8390&#039;&gt;

  &lt;img src=&#039;https://github.com/user-attachments/assets/9a59f9d6-cc8a-48bb-9a75-d075a9e13498&#039;&gt;

  ### 发送加密通知

  &lt;img src=&#039;https://github.com/user-attachments/assets/8a3cb67d-e1b6-40d3-b4c1-543d098eb481&#039;&gt;

  &lt;img src=&#039;https://github.com/user-attachments/assets/9a59f9d6-cc8a-48bb-9a75-d075a9e13498&#039;&gt;
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt; 🎬 DouYin &lt;/summary&gt;

  ### 抖音单个作品

  &lt;img src=&#039;https://github.com/user-attachments/assets/01b62283-322a-42f1-96d7-bc1431cc0e1b&#039;&gt;

  ### 抖音主页作品

  &lt;img src=&#039;https://github.com/user-attachments/assets/6353a309-9f8d-4284-9f90-267d683ac9cd&#039;&gt;

  ### 抖音点赞作品

  &lt;img src=&#039;https://github.com/user-attachments/assets/018d2b6f-d874-41f9-9c20-e1a17bdf16e0&#039;&gt;

  ### 抖音收藏作品

  &lt;img src=&#039;https://github.com/user-attachments/assets/0334b1b0-1b61-4cbb-b47a-0ad0ce82e315&#039;&gt;

  ### 抖音收藏夹作品

  &lt;img src=&#039;https://github.com/user-attachments/assets/ef9fc0e4-5d4f-4ad6-9fa1-e305f9b60c83&#039;&gt;

  ### 抖音收藏原声

  &lt;img src=&#039;https://github.com/user-attachments/assets/414e20eb-0837-48b5-8a7b-622e4d0aafe1&#039;&gt;

  ### 抖音合集作品

  支持合集里任意作品链接解析
  &lt;img src=&#039;https://github.com/user-attachments/assets/4cd85a9f-d684-4c02-8106-fce567f05f0b&#039;&gt;

  合集链接解析
  &lt;img src=&#039;https://github.com/user-attachments/assets/04a3553b-93f8-4f99-a9f2-689bef881899&#039;&gt;

  ### 抖音直播录制

  单个直播录制
  &lt;img src=&#039;https://github.com/user-attachments/assets/63c31ad3-3026-4ae8-8fc1-752ba2117915&#039;&gt;

  批量直播录制
  &lt;img src=&#039;https://github.com/user-attachments/assets/6e8caaa7-2bfe-4542-b896-ae4f3c70877f&#039;&gt;

  ### 抖音相关推荐

  &lt;img src=&quot;https://github.com/user-attachments/assets/c64731a1-5383-4810-af15-8125330856a8&quot;&gt;

  ### 抖音好友作品

  &lt;img src=&quot;https://github.com/user-attachments/assets/c73feec7-f158-4eb1-be3b-9a957507ef45&quot;&gt;

  ### 抖音直播弹幕

  &lt;video src=&quot;https://github.com/Johnserf-Seed/f2/assets/40727745/500d1eaf-59ba-44ba-849b-666c0ddf8469&quot; width=&quot;70%&quot; height=&quot;auto&quot; autoplay loop style=&quot;border-radius: 8px; overflow: hidden;&quot;&gt;&lt;/video&gt;
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt; 🎬 TikTok &lt;/summary&gt;

  ### TikTok单个作品

  &lt;img src=&#039;https://github.com/user-attachments/assets/1ddee9ec-cd4c-4dc0-81a0-970e5d1cb831&#039;&gt;

  ### TikTok主页作品

  &lt;img src=&#039;https://github.com/user-attachments/assets/a45f2186-cf0a-4c21-8502-267147936e06&#039;&gt;

  ### TikTok点赞作品

  &lt;img src=&#039;https://github.com/user-attachments/assets/11e8e70f-ee32-422e-a79d-f65276b12652&#039;&gt;

  ### TikTok收藏作品

  &lt;img src=&#039;https://github.com/user-attachments/assets/bcd584fa-8c8c-4846-8be0-3c7d2c85ec5d&#039;&gt;

  ### TikTok播放列表作品

  &lt;img src=&#039;https://github.com/user-attachments/assets/c7ba8740-b9a0-4a8d-98cc-ae8fab2393e2&#039;&gt;

  ### TikTok作品搜索
  &lt;img src=&#039;https://github.com/user-attachments/assets/c69b76e5-b168-4966-af5d-c9325af2015e&#039;&gt;

  ### TikTok直播弹幕

  &lt;video src=&quot;https://github.com/Johnserf-Seed/f2/assets/40727745/500d1eaf-59ba-44ba-849b-666c0ddf8469&quot; width=&quot;70%&quot; height=&quot;auto&quot; autoplay loop style=&quot;border-radius: 8px; overflow: hidden;&quot;&gt;&lt;/video&gt;
  ps. 懒得录了，放的douyin的弹幕，效果一样的。
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt; 🎬 Twitter &lt;/summary&gt;

  ### x单个推文

  &lt;img src=&#039;https://github.com/user-attachments/assets/5858e19f-e4e6-4279-a1a4-56ac2878afde&#039;&gt;

  ### x主页推文

  &lt;img src=&#039;https://github.com/user-attachments/assets/43f9665e-3086-4078-a093-59a8081bb77c&#039;&gt;

  ### x喜欢推文

  &lt;img src=&#039;https://github.com/user-attachments/assets/c8b592f6-84a5-4a7d-b8df-9a8f2e25abb0&#039;&gt;

  ### x收藏推文

  &lt;img src=&#039;https://github.com/user-attachments/assets/e4aa7adf-52de-4a7c-a1f4-f61423047ac8&#039;&gt;
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt; 🎬 WeiBo &lt;/summary&gt;

  ### WeiBo单个微博

  &lt;img src=&#039;https://github.com/user-attachments/assets/4038766d-d601-42a9-8f35-6243b3744bd7&#039;&gt;

  ### WeiBo主页微博

  &lt;img src=&#039;https://github.com/user-attachments/assets/e02c5007-4c17-4d97-a648-6aabb328a618&#039;&gt;
&lt;/details&gt;

## 📦 结构

&lt;details&gt;
  &lt;summary&gt;📁 项目目录&lt;/summary&gt;

  ```bash
  .
  |___.coverage
  |___.github
  | |___dependabot.yml
  | |___ISSUE_TEMPLATE
  | | |___ask-question.md
  | | |___bug-report.md
  | | |___feature_request.md
  | |___workflows
  | | |___Codecov.yml
  | | |___deploy.yml
  | | |___issue_similarity.yml
  |___.gitignore
  |___.vscode
  | |___launch.json
  | |___settings.json
  |___babel.cfg
  |___CHANGELOG.md
  |___CNAME
  |___CODE_OF_CONDUCT.md
  |___CONTRIBUTING.en.md
  |___CONTRIBUTING.md
  |___CONTRIBUTORS.en.md
  |___CONTRIBUTORS.md
  |___coverage.xml
  |___docs
  | |___.vitepress
  | | |___config.mts
  | | |___theme
  | | | |___index.ts
  | | | |___Layout.vue
  | | | |___styles
  | | | | |___vars.css
  | |___advance-guide.md
  | |___cli.md
  | |___en
  | | |___advance-guide.md
  | | |___api-examples.md
  | | |___cli.md
  | | |___guide
  | | | |___api-examples.md
  | | | |___apps
  | | | | |___bark
  | | | | | |___cli.md
  | | | | | |___index.md
  | | | | |___douyin
  | | | | | |___cli.md
  | | | | | |___index.md
  | | | | |___f2
  | | | | | |___cli.md
  | | | | | |___index.md
  | | | | |___tiktok
  | | | | |

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[SylphAI-Inc/AdalFlow]]></title>
            <link>https://github.com/SylphAI-Inc/AdalFlow</link>
            <guid>https://github.com/SylphAI-Inc/AdalFlow</guid>
            <pubDate>Mon, 01 Sep 2025 00:04:50 GMT</pubDate>
            <description><![CDATA[AdalFlow: The library to build & auto-optimize LLM applications.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/SylphAI-Inc/AdalFlow">SylphAI-Inc/AdalFlow</a></h1>
            <p>AdalFlow: The library to build & auto-optimize LLM applications.</p>
            <p>Language: Python</p>
            <p>Stars: 3,637</p>
            <p>Forks: 336</p>
            <p>Stars today: 21 stars today</p>
            <h2>README</h2><pre>
&lt;!-- &lt;h4 align=&quot;center&quot;&gt;
    &lt;img alt=&quot;AdalFlow logo&quot; src=&quot;docs/source/_static/images/adalflow-logo.png&quot; style=&quot;width: 100%;&quot;&gt;
&lt;/h4&gt; --&gt;



&lt;h4 align=&quot;center&quot;&gt;
    &lt;img alt=&quot;AdalFlow logo&quot; src=&quot;https://raw.githubusercontent.com/SylphAI-Inc/AdalFlow/main/docs/source/_static/images/adalflow-logo.png&quot; style=&quot;width: 100%;&quot;&gt;
&lt;/h4&gt;

&lt;h2&gt;
    &lt;p align=&quot;center&quot;&gt;
     ⚡ AdalFlow is a PyTorch-like library to build and auto-optimize any LM workflows, from Chatbots, RAG,  to Agents. ⚡
    &lt;/p&gt;
&lt;/h2&gt;




&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://colab.research.google.com/drive/1_YnD4HshzPRARvishoU4IA-qQuX9jHrT?usp=sharing&quot;&gt;
        &lt;img alt=&quot;Try Quickstart in Colab&quot; src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;h4 align=&quot;center&quot;&gt;
    &lt;p&gt;
        &lt;a href=&quot;https://adalflow.sylph.ai/&quot;&gt;View Documentation&lt;/a&gt;
        &lt;!-- &lt;a href=&quot;https://adalflow.sylph.ai/apis/components/components.model_client.html&quot;&gt;Models&lt;/a&gt; |
        &lt;a href=&quot;https://adalflow.sylph.ai/apis/components/components.retriever.html&quot;&gt;Retrievers&lt;/a&gt; |
        &lt;a href=&quot;https://adalflow.sylph.ai/apis/components/components.agent.html&quot;&gt;Agents&lt;/a&gt; |
        &lt;a href=&quot;https://adalflow.sylph.ai/tutorials/evaluation.html&quot;&gt; LLM evaluation&lt;/a&gt; |
        &lt;a href=&quot;https://adalflow.sylph.ai/use_cases/question_answering.html&quot;&gt;Trainer &amp; Optimizers&lt;/a&gt; --&gt;
    &lt;p&gt;
&lt;/h4&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://pypi.org/project/adalflow/&quot;&gt;
        &lt;img alt=&quot;PyPI Version&quot; src=&quot;https://img.shields.io/pypi/v/adalflow?style=flat-square&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://pypi.org/project/adalflow/&quot;&gt;
        &lt;img alt=&quot;PyPI Downloads&quot; src=&quot;https://static.pepy.tech/badge/adalflow&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://pypi.org/project/adalflow/&quot;&gt;
        &lt;img alt=&quot;PyPI Downloads&quot; src=&quot;https://static.pepy.tech/badge/adalflow/month&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://star-history.com/#SylphAI-Inc/AdalFlow&quot;&gt;
        &lt;img alt=&quot;GitHub stars&quot; src=&quot;https://img.shields.io/github/stars/SylphAI-Inc/AdalFlow?style=flat-square&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/SylphAI-Inc/AdalFlow/issues&quot;&gt;
        &lt;img alt=&quot;Open Issues&quot; src=&quot;https://img.shields.io/github/issues-raw/SylphAI-Inc/AdalFlow?style=flat-square&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://opensource.org/license/MIT&quot;&gt;
        &lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/github/license/SylphAI-Inc/AdalFlow&quot;&gt;
    &lt;/a&gt;
      &lt;a href=&quot;https://discord.gg/ezzszrRZvT&quot;&gt;
        &lt;img alt=&quot;discord-invite&quot; src=&quot;https://dcbadge.limes.pink/api/server/ezzszrRZvT?style=flat&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;!-- &lt;h4&gt;
&lt;p align=&quot;center&quot;&gt;
For AI researchers, product teams, and software engineers who want to learn the AI way.
&lt;/p&gt;
&lt;/h4&gt; --&gt;

&lt;!-- &lt;h4&gt;
&lt;p align=&quot;center&quot;&gt;
AdalFlow is a PyTorch-like library to build and auto-optimize any LM workflows, from Chatbots, RAG,  to Agents.
&lt;/p&gt;
&lt;/h4&gt; --&gt;




&lt;!-- &lt;a href=&quot;https://colab.research.google.com/drive/1PPxYEBa6eu__LquGoFFJZkhYgWVYE6kh?usp=sharing&quot;&gt;
        &lt;img alt=&quot;Try Quickstart in Colab&quot; src=&quot;https://colab.research.google.com/assets/colab-badge.svg&quot;&gt;
    &lt;/a&gt; --&gt;

&lt;!-- &lt;a href=&quot;https://pypistats.org/packages/lightrag&quot;&gt;
&lt;img alt=&quot;PyPI Downloads&quot; src=&quot;https://img.shields.io/pypi/dm/lightRAG?style=flat-square&quot;&gt;
&lt;/a&gt; --&gt;

# Why AdalFlow

1. **100% Open-source Agents SDK**: Lightweight and requires no additional API to setup ``Human-in-the-Loop`` and ``Tracing`` Functionalities.
2. **Say goodbye to manual prompting**: AdalFlow provides a unified auto-differentiative framework for both zero-shot optimization and few-shot prompt optimization. Our research, ``LLM-AutoDiff`` and ``Learn-to-Reason Few-shot In Context Learning``, achieve the highest accuracy among all auto-prompt optimization libraries.
3. **Switch your LLM app to any model via a config**:  AdalFlow provides `Model-agnostic` building blocks for LLM task pipelines, ranging from RAG, Agents to classical NLP tasks.

&lt;!-- &lt;p align=&quot;center&quot; style=&quot;background-color: #f0f0f0;&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/SylphAI-Inc/AdalFlow/main/docs/source/_static/images/classification_training_map.png&quot; style=&quot;width: 80%;&quot; alt=&quot;AdalFlow Auto-optimization&quot;&gt;
&lt;/p&gt; --&gt;

&lt;p align=&quot;center&quot; style=&quot;background-color: #f0f0f0;&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/SylphAI-Inc/AdalFlow/main/docs/source/_static/images/classification_opt_prompt.png&quot; alt=&quot;AdalFlow Optimized Prompt&quot; style=&quot;width: 80%;&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot; style=&quot;background-color: #f0f0f0;&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/SylphAI-Inc/AdalFlow/main/docs/source/_static/images/adalflow_tracing_mlflow.png&quot; alt=&quot;AdalFlow MLflow Integration&quot; style=&quot;width: 80%;&quot;&gt;
&lt;/p&gt;

&lt;!-- Among all libraries, AdalFlow achieved the highest accuracy with manual prompting (starting at 82%) and the highest accuracy after optimization. --&gt;
&lt;!-- &lt;p align=&quot;center&quot; style=&quot;background-color: #f0f0f0;&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/SylphAI-Inc/AdalFlow/main/docs/source/_static/images/classification_opt_prompt.png&quot; alt=&quot;AdalFlow Optimized Prompt&quot; style=&quot;width: 80%;&quot;&gt;
&lt;/p&gt; --&gt;

View [Documentation](https://adalflow.sylph.ai)


# Quick Start


Install AdalFlow with pip:

```bash
pip install adalflow
```

## Hello World Agent Example

```python
from adalflow import Agent, Runner
from adalflow.components.model_client.openai_client import OpenAIClient
from adalflow.core.types import (
    ToolCallActivityRunItem, 
    RunItemStreamEvent,
    ToolCallRunItem,
    ToolOutputRunItem,
    FinalOutputItem
)
import asyncio

# Define tools
def calculator(expression: str) -&gt; str:
    &quot;&quot;&quot;Evaluate a mathematical expression.&quot;&quot;&quot;
    try:
        result = eval(expression)
        return f&quot;The result of {expression} is {result}&quot;
    except Exception as e:
        return f&quot;Error: {e}&quot;

async def web_search(query: str=&quot;what is the weather in SF today?&quot;) -&gt; str:
    &quot;&quot;&quot;Web search on query.&quot;&quot;&quot;
    await asyncio.sleep(0.5)
    return &quot;San Francisco will be mostly cloudy today with some afternoon sun, reaching about 67 °F (20 °C).&quot;

def counter(limit: int):
    &quot;&quot;&quot;A counter that counts up to a limit.&quot;&quot;&quot;
    final_output = []
    for i in range(1, limit + 1):
        stream_item = f&quot;Count: {i}/{limit}&quot;
        final_output.append(stream_item)
        yield ToolCallActivityRunItem(data=stream_item)
    yield final_output

# Create agent with tools
agent = Agent(
    name=&quot;MyAgent&quot;,
    tools=[calculator, web_search, counter],
    model_client=OpenAIClient(),
    model_kwargs={&quot;model&quot;: &quot;gpt-4o&quot;, &quot;temperature&quot;: 0.3},
    max_steps=5
)

runner = Runner(agent=agent)
```

### 1. Synchronous Call Mode

```python
# Sync call - returns RunnerResult with complete execution history
result = runner.call(
    prompt_kwargs={&quot;input_str&quot;: &quot;Calculate 15 * 7 + 23 and count to 5&quot;}
)

print(result.answer)
# Output: The result of 15 * 7 + 23 is 128. The counter counted up to 5: 1, 2, 3, 4, 5.

# Access step history
for step in result.step_history:
    print(f&quot;Step {step.step}: {step.function.name} -&gt; {step.observation}&quot;)
# Output:
# Step 0: calculator -&gt; The result of 15 * 7 + 23 is 128
# Step 1: counter -&gt; [&#039;Count: 1/5&#039;, &#039;Count: 2/5&#039;, &#039;Count: 3/5&#039;, &#039;Count: 4/5&#039;, &#039;Count: 5/5&#039;]
```

### 2. Asynchronous Call Mode

```python
# Async call - similar output structure to sync call
result = await runner.acall(
    prompt_kwargs={&quot;input_str&quot;: &quot;What&#039;s the weather in SF and calculate 42 * 3&quot;}
)

print(result.answer)
# Output: San Francisco will be mostly cloudy today with some afternoon sun, reaching about 67 °F (20 °C). 
#         The result of 42 * 3 is 126.
```

### 3. Async Streaming Mode

```python
# Async streaming - real-time event processing
streaming_result = runner.astream(
    prompt_kwargs={&quot;input_str&quot;: &quot;Calculate 100 + 50 and count to 3&quot;},
)

# Process streaming events in real-time
async for event in streaming_result.stream_events():
    if isinstance(event, RunItemStreamEvent):
        if isinstance(event.item, ToolCallRunItem):
            print(f&quot;🔧 Calling: {event.item.data.name}&quot;)
        elif isinstance(event.item, ToolCallActivityRunItem):
            print(f&quot;📝 Activity: {event.item.data}&quot;)
        elif isinstance(event.item, ToolOutputRunItem):
            print(f&quot;✅ Output: {event.item.data.output}&quot;)
        elif isinstance(event.item, FinalOutputItem):
            print(f&quot;🎯 Final: {event.item.data.answer}&quot;)

# Output:
# 🔧 Calling: calculator
# ✅ Output: The result of 100 + 50 is 150
# 🔧 Calling: counter
# 📝 Activity: Count: 1/3
# 📝 Activity: Count: 2/3
# 📝 Activity: Count: 3/3
# ✅ Output: [&#039;Count: 1/3&#039;, &#039;Count: 2/3&#039;, &#039;Count: 3/3&#039;]
# 🎯 Final: The result of 100 + 50 is 150. Counted to 3 successfully.
```

_Set your `OPENAI_API_KEY` environment variable to run these examples._

**Try the full Agent tutorial in Colab:** [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SylphAI-Inc/AdalFlow/blob/main/notebooks/agents/agent_tutorial.ipynb)

&lt;!-- Please refer to the [full installation guide](https://adalflow.sylph.ai/get_started/installation.html) for more details.
[Package changelog](https://github.com/SylphAI-Inc/AdalFlow/blob/main/adalflow/CHANGELOG.md). --&gt;
View [Quickstart](https://colab.research.google.com/drive/1_YnD4HshzPRARvishoU4IA-qQuX9jHrT?usp=sharing): Learn How `AdalFlow` optimizes LM workflows end-to-end in 15 mins.

Go to [Documentation](https://adalflow.sylph.ai) for tracing, human-in-the-loop, and more.


&lt;!-- * Try the [Building Quickstart](https://colab.research.google.com/drive/1TKw_JHE42Z_AWo8UuRYZCO2iuMgyslTZ?usp=sharing) in Colab to see how AdalFlow can build the task pipeline, including Chatbot, RAG, agent, and structured output.
* Try the [Optimization Quickstart](https://colab.research.google.com/github/SylphAI-Inc/AdalFlow/blob/main/notebooks/qas/adalflow_object_count_auto_optimization.ipynb) to see how AdalFlow can optimize the task pipeline. --&gt;



# Research
[Jan 2025] [Auto-Differentiating Any LLM Workflow: A Farewell to Manual Prompting](https://arxiv.org/abs/2501.16673)
- LLM Applications as auto-differentiation graphs
- Token-efficient and better performance than DsPy


# Collaborations

We work closely with the [**VITA Group** at University of Texas at Austin](https://vita-group.github.io/), under the leadership of [Dr. Atlas Wang](https://www.ece.utexas.edu/people/faculty/atlas-wang), who provides valuable support in driving project initiatives.

 &lt;!-- alongside [Dr. Junyuan Hong](https://jyhong.gitlab.io/),  --&gt;
For collaboration, contact [Li Yin](https://www.linkedin.com/in/li-yin-ai/).

# Hiring

We are looking for a Dev Rel to help us build the community and support our users. If you are interested, please contact [Li Yin](https://www.linkedin.com/in/li-yin-ai/).



&lt;!-- ## Light, Modular, and Model-Agnostic Task Pipeline

LLMs are like water; AdalFlow help you quickly shape them into any applications, from GenAI applications such as chatbots, translation, summarization, code generation, RAG, and autonomous agents to classical NLP tasks like text classification and named entity recognition.

AdalFlow has two fundamental, but powerful, base classes: `Component` for the pipeline and `DataClass` for data interaction with LLMs.
The result is a library with minimal abstraction, providing developers with *maximum customizability*.

You have full control over the prompt template, the model you use, and the output parsing for your task pipeline.


&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/SylphAI-Inc/AdalFlow/main/docs/source/_static/images/AdalFlow_task_pipeline.png&quot; alt=&quot;AdalFlow Task Pipeline&quot;&gt;
&lt;/p&gt;

Many providers and models accessible via the same interface:

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/SylphAI-Inc/AdalFlow/main/docs/source/_static/images/multi-providers.png&quot; alt=&quot;AdalFlow Model Providers&quot;&gt;
&lt;/p&gt;

[All available model providers](https://adalflow.sylph.ai/apis/components/components.model_client.html)




Further reading: [How We Started](https://www.linkedin.com/posts/li-yin-ai_both-ai-research-and-engineering-use-pytorch-activity-7189366364694892544-Uk1U?utm_source=share&amp;utm_medium=member_desktop),[Design Philosophy](https://adalflow.sylph.ai/tutorials/lightrag_design_philosophy.html) and [Class hierarchy](https://adalflow.sylph.ai/tutorials/class_hierarchy.html).



## Unified Framework for Auto-Optimization


To optimize your pipeline, simply define a ``Parameter`` and pass it to AdalFlow&#039;s ``Generator``.
You use `PROMPT` for prompt tuning via textual gradient descent and `DEMO` for few-shot demonstrations.
We let you **diagnose**, **visualize**, **debug**, and **train** your pipeline.


### **Trainable Task Pipeline**

Just define it as a ``Parameter`` and pass it to AdalFlow&#039;s ``Generator``.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/SylphAI-Inc/AdalFlow/main/docs/source/_static/images/Trainable_task_pipeline.png&quot; alt=&quot;AdalFlow Trainable Task Pipeline&quot;&gt;
&lt;/p&gt;

### **AdalComponent &amp; Trainer**

``AdalComponent`` acts as the &#039;interpreter&#039;  between task pipeline and the trainer, defining training and validation steps, optimizers, evaluators, loss functions, backward engine for textual gradients or tracing the demonstrations, the teacher generator.

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/SylphAI-Inc/AdalFlow/main/docs/source/_static/images/trainer.png&quot; alt=&quot;AdalFlow AdalComponent &amp; Trainer&quot;&gt;

&lt;/p&gt;
 --&gt;


# Documentation

AdalFlow full documentation available at [adalflow.sylph.ai](https://adalflow.sylph.ai/):
&lt;!-- - [How We Started](https://www.linkedin.com/posts/li-yin-ai_both-ai-research-and-engineering-use-pytorch-activity-7189366364694892544-Uk1U?utm_source=share&amp;utm_medium=member_desktop)
- [Introduction](https://adalflow.sylph.ai/)
- [Full installation guide](https://adalflow.sylph.ai/get_started/installation.html)
- [Design philosophy](https://adalflow.sylph.ai/tutorials/lightrag_design_philosophy.html)
- [Class hierarchy](https://adalflow.sylph.ai/tutorials/class_hierarchy.html)
- [Tutorials](https://adalflow.sylph.ai/tutorials/index.html)
- [Supported Models](https://adalflow.sylph.ai/apis/components/components.model_client.html)
- [Supported Retrievers](https://adalflow.sylph.ai/apis/components/components.retriever.html)
- [API reference](https://adalflow.sylph.ai/apis/index.html) --&gt;


# AdalFlow: A Tribute to Ada Lovelace


AdalFlow is named in honor of [Ada Lovelace](https://en.wikipedia.org/wiki/Ada_Lovelace), the pioneering female mathematician who first recognized that machines could go beyond mere calculations. As a team led by a female founder, we aim to inspire more women to pursue careers in AI.

# Community &amp; Contributors

The AdalFlow is a community-driven project, and we welcome everyone to join us in building the future of LLM applications.

Join our [Discord](https://discord.gg/ezzszrRZvT) community to ask questions, share your projects, and get updates on AdalFlow.

To contribute, please read our [Contributor Guide](https://adalflow.sylph.ai/contributor/index.html).

# Contributors

[![contributors](https://contrib.rocks/image?repo=SylphAI-Inc/AdalFlow&amp;max=2000)](https://github.com/SylphAI-Inc/AdalFlow/graphs/contributors)

# Acknowledgements

Many existing works greatly inspired AdalFlow library! Here is a non-exhaustive list:

- 📚 [PyTorch](https://github.com/pytorch/pytorch/) for design philosophy and design pattern of ``Component``, ``Parameter``, ``Sequential``.
- 📚 [Micrograd](https://github.com/karpathy/micrograd): A tiny autograd engine for our auto-differentiative architecture.
- 📚 [Text-Grad](https://github.com/zou-group/textgrad) for the ``Textual Gradient Descent`` text optimizer.
- 📚 [DSPy](https://github.com/stanfordnlp/dspy) for inspiring the ``__{input/output}__fields`` in our ``DataClass`` and the bootstrap few-shot optimizer.
- 📚 [OPRO](https://github.com/google-deepmind/opro) for adding past text instructions along with its accuracy in the text optimizer.
- 📚 [PyTorch Lightning](https://github.com/Lightning-AI/pytorch-lightning) for the ``AdalComponent`` and ``Trainer``.

&lt;!-- # Citation

```bibtex

@software{Yin2024AdalFlow,
  author = {Li Yin},
  title = {{AdalFlow: The Library for Large Language Model (LLM) Applications}},
  month = {7},
  year = {2024},
  doi = {10.5281/zenodo.12639531},
  url = {https://github.com/SylphAI-Inc/AdalFlow}
}
``` --&gt;

&lt;!-- # Star History

[![Star History Chart](https://api.star-history.com/svg?repos=SylphAI-Inc/AdalFlow&amp;type=Date)](https://star-history.com/#SylphAI-Inc/AdalFlow&amp;Date) --&gt;
&lt;!--
&lt;a href=&quot;https://trendshift.io/repositories/11559&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11559&quot; alt=&quot;SylphAI-Inc%2FAdalFlow | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt; --&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>