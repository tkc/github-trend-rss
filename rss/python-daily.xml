<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sat, 20 Dec 2025 00:04:21 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[NVIDIA-NeMo/Gym]]></title>
            <link>https://github.com/NVIDIA-NeMo/Gym</link>
            <guid>https://github.com/NVIDIA-NeMo/Gym</guid>
            <pubDate>Sat, 20 Dec 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[Build RL environments for LLM training]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NVIDIA-NeMo/Gym">NVIDIA-NeMo/Gym</a></h1>
            <p>Build RL environments for LLM training</p>
            <p>Language: Python</p>
            <p>Stars: 447</p>
            <p>Forks: 30</p>
            <p>Stars today: 27 stars today</p>
            <h2>README</h2><pre># NeMo Gym

NeMo Gym is a library for building reinforcement learning (RL) training environments for large language models (LLMs). It provides infrastructure to develop environments, scale rollout collection, and integrate seamlessly with your preferred training framework. 

NeMo Gym is a component of the [NVIDIA NeMo Framework](https://docs.nvidia.com/nemo-framework/), NVIDIA‚Äôs GPU-accelerated platform for building and training generative AI models.


## üèÜ Why NeMo Gym?

- Scaffolding and patterns to accelerate environment development: multi-step, multi-turn, and user modeling scenarios
- Contribute environments without expert knowledge of the entire RL training loop
- Test environments and throughput end-to-end, independent of the RL training loop
- Interoperable with existing environments, systems, and RL training frameworks
- Growing collection of training environments and datasets for Reinforcement Learning from Verifiable Reward (RLVR)

&gt; [!IMPORTANT]
&gt; NeMo Gym is currently in early development. You should expect evolving APIs, incomplete documentation, and occasional bugs. We welcome contributions and feedback - for any changes, please open an issue first to kick off discussion!

## üìã Requirements

### Hardware Requirements

NeMo Gym is designed to run on standard development machines:

- **GPU**: Not required for NeMo Gym library operation
  - GPU may be needed for specific resource servers or model inference (see individual server documentation)
- **CPU**: Any modern x86_64 or ARM64 processor (e.g., Intel, AMD, Apple Silicon)
- **RAM**: Minimum 8 GB (16 GB+ recommended for larger environments)
- **Storage**: Minimum 5 GB free disk space for installation and basic usage

### Software Requirements

- **Operating System**: 
  - Linux (Ubuntu 20.04+, or equivalent)
  - macOS (11.0+ for x86_64, 12.0+ for Apple Silicon)
  - Windows (via WSL2)
- **Python**: 3.12 or higher
- **Git**: For cloning the repository
- **Internet Connection**: Required for downloading dependencies and API access

### Additional Requirements

- **API Keys**: OpenAI API key with available credits (for the quickstart examples)
  - Other model providers supported (Azure OpenAI, self-hosted models via vLLM)
- **Ray**: Automatically installed as a dependency (no separate setup required)

## üöÄ Quick Start

### Setup
```bash
# Clone the repository
git clone git@github.com:NVIDIA-NeMo/Gym.git
cd Gym

# Install UV (Python package manager)
curl -LsSf https://astral.sh/uv/install.sh | sh
source $HOME/.local/bin/env

# Create virtual environment
uv venv --python 3.12
source .venv/bin/activate

# Install NeMo Gym
uv sync --extra dev --group docs
```

### Configure Your API Key
Create an `env.yaml` file that contains your OpenAI API key and the [policy model](https://docs.nvidia.com/nemo/gym/latest/about/concepts/key-terminology.html#term-Policy-Model) you want to use. Replace `your-openai-api-key` with your actual key. This file helps keep your secrets out of version control while still making them available to NeMo Gym.

```bash
echo &quot;policy_base_url: https://api.openai.com/v1
policy_api_key: your-openai-api-key
policy_model_name: gpt-4.1-2025-04-14&quot; &gt; env.yaml
```

&gt; [!NOTE]
&gt; We use GPT-4.1 in this quickstart because it provides low latency (no reasoning step) and works reliably out-of-the-box. NeMo Gym is **not limited to OpenAI models**‚Äîyou can use self-hosted models via vLLM or any OpenAI-compatible inference server. See the [documentation](https://docs.nvidia.com/nemo/gym/latest/get-started/detailed-setup.html) for details.

### Start Servers

**Terminal 1 (start servers)**:
```bash
# Start servers (this will keep running)
config_paths=&quot;resources_servers/example_single_tool_call/configs/example_single_tool_call.yaml,\
responses_api_models/openai_model/configs/openai_model.yaml&quot;
ng_run &quot;+config_paths=[${config_paths}]&quot;
```

**Terminal 2 (interact with agent)**:
```bash
# In a NEW terminal, activate environment
source .venv/bin/activate

# Interact with your agent
python responses_api_agents/simple_agent/client.py
```

### Collect Rollouts

**Terminal 2** (keep servers running in Terminal 1):
```bash
# Create a simple dataset with one query
echo &#039;{&quot;responses_create_params&quot;:{&quot;input&quot;:[{&quot;role&quot;:&quot;developer&quot;,&quot;content&quot;:&quot;You are a helpful assistant.&quot;},{&quot;role&quot;:&quot;user&quot;,&quot;content&quot;:&quot;What is the weather in Seattle?&quot;}]}}&#039; &gt; weather_query.jsonl

# Collect verified rollouts
ng_collect_rollouts \
    +agent_name=example_single_tool_call_simple_agent \
    +input_jsonl_fpath=weather_query.jsonl \
    +output_jsonl_fpath=weather_rollouts.jsonl

# View the result
cat weather_rollouts.jsonl | python -m json.tool
```
This generates training data with verification scores!

### Clean Up Servers

**Terminal 1** with the running servers: Ctrl+C to stop the ng_run process.

### What&#039;s Next?

Now that you can generate rollouts, choose your path:

- **Use an existing training environment** ‚Äî Browse the [Available Resource Servers](#-available-resource-servers) below to find a training-ready environment that matches your goals.

- **Build a custom training environment** ‚Äî Implement or integrate existing tools and define task verification logic. Get started with the [Creating a Resource Server](https://docs.nvidia.com/nemo/gym/latest/tutorials/creating-resource-server.html) tutorial.


## üì¶ Available Resource Servers

NeMo Gym includes a curated collection of resource servers for training and evaluation across multiple domains:

### Table 1: Example Resource Servers

Purpose: Demonstrate NeMo Gym patterns and concepts.

&lt;!-- START_EXAMPLE_ONLY_SERVERS_TABLE --&gt;
| Name               | Demonstrates                         | Config                                                                                                                             | README                                                                      |
| ------------------ | ------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------- |
| Multi Step         | Multi-step tool calling              | &lt;a href=&#039;resources_servers/example_multi_step/configs/example_multi_step.yaml&#039;&gt;example_multi_step.yaml&lt;/a&gt;                         | &lt;a href=&#039;resources_servers/example_multi_step/README.md&#039;&gt;README&lt;/a&gt;         |
| Session State Mgmt | Session state management (in-memory) | &lt;a href=&#039;resources_servers/example_session_state_mgmt/configs/example_session_state_mgmt.yaml&#039;&gt;example_session_state_mgmt.yaml&lt;/a&gt; | &lt;a href=&#039;resources_servers/example_session_state_mgmt/README.md&#039;&gt;README&lt;/a&gt; |
| Single Tool Call   | Basic single-step tool calling       | &lt;a href=&#039;resources_servers/example_single_tool_call/configs/example_single_tool_call.yaml&#039;&gt;example_single_tool_call.yaml&lt;/a&gt;       | &lt;a href=&#039;resources_servers/example_single_tool_call/README.md&#039;&gt;README&lt;/a&gt;   |
&lt;!-- END_EXAMPLE_ONLY_SERVERS_TABLE --&gt;

### Table 2: Resource Servers for Training

Purpose: Training-ready environments with curated datasets.

&gt; [!TIP]
&gt; Each resource server includes example data, configuration files, and tests. See each server&#039;s README for details.

&lt;!-- START_TRAINING_SERVERS_TABLE --&gt;
| Resource Server            | Domain                | Dataset                                                                                                                                                        | Description                                                                                          | Value                                                                    | Config                                                                                                    | Train | Validation | License                                                   |
| -------------------------- | --------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------- | ----- | ---------- | --------------------------------------------------------- |
| Calendar                   | agent                 | &lt;a href=&#039;https://huggingface.co/datasets/nvidia/Nemotron-RL-agent-calendar_scheduling&#039;&gt;Nemotron-RL-agent-calendar_scheduling&lt;/a&gt;                               | -                                                                                                    | -                                                                        | &lt;a href=&#039;resources_servers/calendar/configs/calendar.yaml&#039;&gt;config&lt;/a&gt;                                     | ‚úì     | ‚úì          | Apache 2.0                                                |
| Google Search              | agent                 | &lt;a href=&#039;https://huggingface.co/datasets/nvidia/Nemotron-RL-knowledge-web_search-mcqa&#039;&gt;Nemotron-RL-knowledge-web_search-mcqa&lt;/a&gt;                               | Multi-choice question answering problems with search tools integrated                                | Improve knowledge-related benchmarks with search tools                   | &lt;a href=&#039;resources_servers/google_search/configs/google_search.yaml&#039;&gt;config&lt;/a&gt;                           | ‚úì     | -          | Apache 2.0                                                |
| Math Advanced Calculations | agent                 | &lt;a href=&#039;https://huggingface.co/datasets/nvidia/Nemotron-RL-math-advanced_calculations&#039;&gt;Nemotron-RL-math-advanced_calculations&lt;/a&gt;                             | An instruction following math environment with counter-intuitive calculators                         | Improve instruction following capabilities in specific math environments | &lt;a href=&#039;resources_servers/math_advanced_calculations/configs/math_advanced_calculations.yaml&#039;&gt;config&lt;/a&gt; | ‚úì     | -          | Apache 2.0                                                |
| Workplace Assistant        | agent                 | &lt;a href=&#039;https://huggingface.co/datasets/nvidia/Nemotron-RL-agent-workplace_assistant&#039;&gt;Nemotron-RL-agent-workplace_assistant&lt;/a&gt;                               | Workplace assistant multi-step tool-using environment                                                | Improve multi-step tool use capability                                   | &lt;a href=&#039;resources_servers/workplace_assistant/configs/workplace_assistant.yaml&#039;&gt;config&lt;/a&gt;               | ‚úì     | ‚úì          | Apache 2.0                                                |
| Code Gen                   | coding                | &lt;a href=&#039;https://huggingface.co/datasets/nvidia/nemotron-RL-coding-competitive_coding&#039;&gt;nemotron-RL-coding-competitive_coding&lt;/a&gt;                               | -                                                                                                    | -                                                                        | &lt;a href=&#039;resources_servers/code_gen/configs/code_gen.yaml&#039;&gt;config&lt;/a&gt;                                     | ‚úì     | ‚úì          | Apache 2.0                                                |
| Mini Swe Agent             | coding                | &lt;a href=&#039;https://huggingface.co/datasets/SWE-Gym/SWE-Gym&#039;&gt;SWE-Gym&lt;/a&gt;                                                                                          | A software development with mini-swe-agent orchestration                                             | Improve software development capabilities, like SWE-bench                | &lt;a href=&#039;resources_servers/mini_swe_agent/configs/mini_swe_agent.yaml&#039;&gt;config&lt;/a&gt;                         | ‚úì     | ‚úì          | MIT                                                       |
| Instruction Following      | instruction_following | &lt;a href=&#039;https://huggingface.co/datasets/nvidia/Nemotron-RL-instruction_following&#039;&gt;Nemotron-RL-instruction_following&lt;/a&gt;                                       | Instruction following datasets targeting IFEval and IFBench style instruction following capabilities | Improve IFEval and IFBench                                               | &lt;a href=&#039;resources_servers/instruction_following/configs/instruction_following.yaml&#039;&gt;config&lt;/a&gt;           | ‚úì     | -          | Apache 2.0                                                |
| Structured Outputs         | instruction_following | &lt;a href=&#039;https://huggingface.co/datasets/nvidia/Nemotron-RL-instruction_following-structured_outputs&#039;&gt;Nemotron-RL-instruction_following-structured_outputs&lt;/a&gt; | Check if responses are following structured output requirements in prompts                           | Improve instruction following capabilities                               | &lt;a href=&#039;resources_servers/structured_outputs/configs/structured_outputs_json.yaml&#039;&gt;config&lt;/a&gt;            | ‚úì     | ‚úì          | Apache 2.0                                                |
| Equivalence Llm Judge      | knowledge             | &lt;a href=&#039;https://huggingface.co/datasets/nvidia/Nemotron-RL-knowledge-openQA&#039;&gt;Nemotron-RL-knowledge-openQA&lt;/a&gt;                                                 | Short answer questions with LLM-as-a-judge                                                           | Improve knowledge-related benchmarks like GPQA / HLE                     | &lt;a href=&#039;resources_servers/equivalence_llm_judge/configs/equivalence_llm_judge.yaml&#039;&gt;config&lt;/a&gt;           | ‚úì     | -          | Apache 2.0                                                |
| Mcqa                       | knowledge             | &lt;a href=&#039;https://huggingface.co/datasets/nvidia/Nemotron-RL-knowledge-mcqa&#039;&gt;Nemotron-RL-knowledge-mcqa&lt;/a&gt;                                                     | Multi-choice question answering problems                                                             | Improve benchmarks like MMLU / GPQA / HLE                                | &lt;a href=&#039;resources_servers/mcqa/configs/mcqa.yaml&#039;&gt;config&lt;/a&gt;                                             | ‚úì     | -          | Apache 2.0                                                |
| Math With Judge            | math                  | &lt;a href=&#039;https://huggingface.co/datasets/nvidia/Nemotron-RL-math-OpenMathReasoning&#039;&gt;Nemotron-RL-math-OpenMathReasoning&lt;/a&gt;                                     | Math dataset with math-verify and LLM-as-a-judge                                                     | Improve math capabilities including AIME 24 / 25                         | &lt;a href=&#039;resources_servers/math_with_judge/configs/math_with_judge.yaml&#039;&gt;config&lt;/a&gt;                       | ‚úì     | ‚úì          | Creative Commons Attribution 4.0 International            |
| Math With Judge            | math                  | &lt;a href=&#039;https://huggingface.co/datasets/nvidia/Nemotron-RL-math-stack_overflow&#039;&gt;Nemotron-RL-math-stack_overflow&lt;/a&gt;                                           | -                                                                                                    | -                                                                        | &lt;a href=&#039;resources_servers/math_with_judge/configs/math_stack_overflow.yaml&#039;&gt;config&lt;/a&gt;                   | ‚úì     | ‚úì          | Creative Commons Attribution-ShareAlike 4.0 International |
&lt;!-- END_TRAINING_SERVERS_TABLE --&gt;

## üìñ Documentation

- **[Documentation](https://docs.nvidia.com/nemo/gym/latest/index.html)** - Technical reference docs
- **[Tutorials](https://docs.nvidia.com/nemo/gym/latest/tutorials/index.html)** - Hands-on tutorials and practical examples
 

## ü§ù Community &amp; Support

We&#039;d love your contributions! Here&#039;s how to get involved:

- **[Report Issues](https://github.com/NVIDIA-NeMo/Gym/issues)** - Bug reports and feature requests
- **[Contributing Guide](https://docs.nvidia.com/nemo/gym/latest/contribute/index.html)** - How to contribute code, docs, new environments, or training framework integrations

## üìö Citations

If you use NeMo Gym in your research, please cite it using the following BibTeX entry:

```bibtex
@misc{nemo-gym,
  title = {NeMo Gym: An Open Source Library for Scaling Reinforcement Learning Environments for LLM},
  howpublished = {\url{https://github.com/NVIDIA-NeMo/Gym}},
  author={NVIDIA},
  year = {2025},
  note = {GitHub repository},
}
```</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[GreyDGL/PentestGPT]]></title>
            <link>https://github.com/GreyDGL/PentestGPT</link>
            <guid>https://github.com/GreyDGL/PentestGPT</guid>
            <pubDate>Sat, 20 Dec 2025 00:04:20 GMT</pubDate>
            <description><![CDATA[A GPT-empowered penetration testing tool]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/GreyDGL/PentestGPT">GreyDGL/PentestGPT</a></h1>
            <p>A GPT-empowered penetration testing tool</p>
            <p>Language: Python</p>
            <p>Stars: 9,596</p>
            <p>Forks: 1,403</p>
            <p>Stars today: 34 stars today</p>
            <h2>README</h2><pre>&lt;!-- Improved compatibility of back to top link: See: https://github.com/othneildrew/Best-README-Template/pull/73 --&gt;
&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;!-- PROJECT SHIELDS --&gt;
[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]
[![MIT License][license-shield]][license-url]
[![Discord][discord-shield]][discord-url]

&lt;!-- PROJECT LOGO --&gt;
&lt;br /&gt;
&lt;div align=&quot;center&quot;&gt;

&lt;h3 align=&quot;center&quot;&gt;PentestGPT&lt;/h3&gt;

  &lt;p align=&quot;center&quot;&gt;
    AI-Powered Autonomous Penetration Testing Agent
    &lt;br /&gt;
    &lt;strong&gt;Published at USENIX Security 2024&lt;/strong&gt;
    &lt;br /&gt;
    &lt;br /&gt;
    &lt;a href=&quot;https://www.usenix.org/conference/usenixsecurity24/presentation/deng&quot;&gt;Research Paper&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://github.com/GreyDGL/PentestGPT/issues&quot;&gt;Report Bug&lt;/a&gt;
    ¬∑
    &lt;a href=&quot;https://github.com/GreyDGL/PentestGPT/issues&quot;&gt;Request Feature&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;

&lt;!-- ABOUT THE PROJECT --&gt;
&lt;a href=&quot;https://trendshift.io/repositories/3770&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/3770&quot; alt=&quot;GreyDGL%2FPentestGPT | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&gt; [!WARNING]
&gt; **PentestGPT is a research prototype only**
&gt;
&gt; PentestGPT is a research prototype that pioneered the use of GenAI in cybersecurity. Please be aware of third-party services claiming to offer paid PentestGPT products - the original project is free and open-source.

---

## Demo

### Installation
[![Installation Demo](https://asciinema.org/a/761661.svg)](https://asciinema.org/a/761661)

[Watch on YouTube](https://www.youtube.com/watch?v=RUNmoXqBwVg)

### PentestGPT in Action
[![PentestGPT Demo](https://asciinema.org/a/761663.svg)](https://asciinema.org/a/761663)

[Watch on YouTube](https://www.youtube.com/watch?v=cWi3Yb7RmZA)

---

## What&#039;s New in v1.0 (Agentic Upgrade)

- **Autonomous Agent** - Agentic pipeline for intelligent, autonomous penetration testing
- **Session Persistence** - Save and resume penetration testing sessions
- **Docker-First** - Isolated, reproducible environment with security tools pre-installed

&gt; **In Progress**: Multi-model support for OpenAI, Gemini, and other LLM providers

---

## Features

- **AI-Powered Challenge Solver** - Leverages LLM advanced reasoning to perform penetration testing and CTFs
- **Live Walkthrough** - Tracks steps in real-time as the agent works through challenges
- **Multi-Category Support** - Web, Crypto, Reversing, Forensics, PWN, Privilege Escalation
- **Real-Time Feedback** - Watch the AI work with live activity updates
- **Extensible Architecture** - Clean, modular design ready for future enhancements

---

## Quick Start

### Prerequisites

- **Docker** (required) - [Install Docker](https://docs.docker.com/get-docker/)
- **LLM Provider** (choose one):
  - Anthropic API Key from [console.anthropic.com](https://console.anthropic.com/)
  - Claude OAuth Login (requires Claude subscription)
  - OpenRouter for alternative models at [openrouter.ai](https://openrouter.ai/keys)
  - [Tutorial: Using Local Models with Claude Code](https://docs.google.com/document/d/1ixK7x-wlr5t5TYZJdfm75UME5KnPCpS46boLkUXKg1w/edit?usp=sharing)


### Installation

```bash
# Clone and build
git clone --recurse-submodules https://github.com/GreyDGL/PentestGPT.git
cd PentestGPT
make install

# Configure authentication (first time only)
make config

# Connect to container
make connect
```

&gt; **Note**: The `--recurse-submodules` flag downloads the benchmark suite. If you already cloned without it, run: `git submodule update --init --recursive`

### Try a Benchmark

```bash
uv run pentestgpt-benchmark start XBEN-037-24 
```

Then connect into the container and run:

```bash
pentestgpt --target http://host.docker.internal:8000
```

### Commands Reference

| Command | Description |
|---------|-------------|
| `make install` | Build the Docker image |
| `make config` | Configure API key (first-time setup) |
| `make connect` | Connect to container (main entry point) |
| `make stop` | Stop container (config persists) |
| `make clean-docker` | Remove everything including config |


---

## Usage

```bash
# Interactive TUI mode (default)
pentestgpt --target 10.10.11.234

# Non-interactive mode
pentestgpt --target 10.10.11.100 --non-interactive

# With challenge context
pentestgpt --target 10.10.11.50 --instruction &quot;WordPress site, focus on plugin vulnerabilities&quot;
```

**Keyboard Shortcuts:** `F1` Help | `Ctrl+P` Pause/Resume | `Ctrl+Q` Quit

---

## Using Local LLMs

PentestGPT supports routing requests to local LLM servers (LM Studio, Ollama, text-generation-webui, etc.) running on your host machine.

### Prerequisites

- Local LLM server with an OpenAI-compatible API endpoint
  - **LM Studio**: Enable server mode (default port 1234)
  - **Ollama**: Run `ollama serve` (default port 11434)

### Setup

```bash
# Configure PentestGPT for local LLM
make config
# Select option 4: Local LLM

# Start your local LLM server on the host machine
# Then connect to the container
make connect
```

### Customizing Models

Edit `scripts/ccr-config-template.json` to customize:

- **`localLLM.api_base_url`**: Your LLM server URL (default: `host.docker.internal:1234`)
- **`localLLM.models`**: Available model names on your server
- **Router section**: Which models handle which operations

| Route | Purpose | Default Model |
|-------|---------|---------------|
| `default` | General tasks | openai/gpt-oss-20b |
| `background` | Background operations | openai/gpt-oss-20b |
| `think` | Reasoning-heavy tasks | qwen/qwen3-coder-30b |
| `longContext` | Large context handling | qwen/qwen3-coder-30b |
| `webSearch` | Web search operations | openai/gpt-oss-20b |

### Troubleshooting

- **Connection refused**: Ensure your LLM server is running and listening on the configured port
- **Docker networking**: Use `host.docker.internal` (not `localhost`) to access host services from Docker
- **Check CCR logs**: Inside the container, run `cat /tmp/ccr.log`

---

## Telemetry

PentestGPT collects anonymous usage data to help improve the tool. This data is sent to our [Langfuse](https://langfuse.com) project and includes:
- Session metadata (target type, duration, completion status)
- Tool execution patterns (which tools are used, not the actual commands)
- Flag detection events (that a flag was found, not the flag content)

**No sensitive data is collected** - command outputs, credentials, or actual flag values are never transmitted.

### Opting Out

```bash
# Via command line flag
pentestgpt --target 10.10.11.234 --no-telemetry

# Via environment variable
export LANGFUSE_ENABLED=false
```

---

## Benchmarks

PentestGPT includes 100+ vulnerability challenges for testing and development.

```bash
pentestgpt-benchmark list                    # List all benchmarks
pentestgpt-benchmark list --levels 1         # Filter by difficulty
pentestgpt-benchmark list --tags sqli        # Filter by vulnerability type
pentestgpt-benchmark start XBEN-037-24       # Start a benchmark
pentestgpt-benchmark status                  # Check running benchmarks
pentestgpt-benchmark stop XBEN-037-24        # Stop a benchmark
```

**Available Tags:** `sqli`, `xss`, `idor`, `ssti`, `ssrf`, `lfi`, `rce`

---

## Development

### Prerequisites

- **uv** (required) - Python package manager: `curl -LsSf https://astral.sh/uv/install.sh | sh`
- **Claude Code CLI** - Configure with `claude login` or `export ANTHROPIC_API_KEY=&#039;your-key&#039;`
  - [Tutorial: Using Local Models with Claude Code](https://docs.google.com/document/d/1ixK7x-wlr5t5TYZJdfm75UME5KnPCpS46boLkUXKg1w/edit?usp=sharing)

### Local Development

```bash
uv sync                                      # Install dependencies
uv run pentestgpt --target 10.10.11.234      # Run locally
```

### Project Commands

```bash
make test          # Run pytest
make lint          # Run ruff linter
make typecheck     # Run mypy
make ci            # Run full CI simulation (lint, format, typecheck, test, build)
make ci-quick      # Quick CI without build step
```

---

## Legacy Version

The previous multi-LLM version (v0.15) supporting OpenAI, Gemini, Deepseek, and Ollama is archived in [`legacy/`](legacy/):

```bash
cd legacy &amp;&amp; pip install -e . &amp;&amp; pentestgpt --reasoning gpt-4o
```

---

## Citation

If you use PentestGPT in your research, please cite our paper:

```bibtex
@inproceedings{299699,
  author = {Gelei Deng and Yi Liu and V√≠ctor Mayoral-Vilches and Peng Liu and Yuekang Li and Yuan Xu and Tianwei Zhang and Yang Liu and Martin Pinzger and Stefan Rass},
  title = {{PentestGPT}: Evaluating and Harnessing Large Language Models for Automated Penetration Testing},
  booktitle = {33rd USENIX Security Symposium (USENIX Security 24)},
  year = {2024},
  isbn = {978-1-939133-44-1},
  address = {Philadelphia, PA},
  pages = {847--864},
  url = {https://www.usenix.org/conference/usenixsecurity24/presentation/deng},
  publisher = {USENIX Association},
  month = aug
}
```

---

## License

Distributed under the MIT License. See `LICENSE.md` for more information.

**Disclaimer**: This tool is for educational purposes and authorized security testing only. The authors do not condone any illegal use. Use at your own risk.

---

## Contact

- **Gelei Deng** - [![LinkedIn][linkedin-shield]][linkedin-url] - gelei.deng@ntu.edu.sg
- **Yi Liu** - yi009@e.ntu.edu.sg
- **Yuekang Li** - yuekang.li@unsw.edu.au
- **V√≠ctor Mayoral Vilches** - [![LinkedIn][linkedin-shield]][linkedin-url2] - v.mayoralv@gmail.com
- **Peng Liu** - liu_peng@i2r.a-star.edu.sg

---

## Acknowledgments

- Research supported by [Quantstamp](https://www.quantstamp.com/) and [NTU Singapore](https://www.ntu.edu.sg/)

&lt;p align=&quot;right&quot;&gt;(&lt;a href=&quot;#readme-top&quot;&gt;back to top&lt;/a&gt;)&lt;/p&gt;

&lt;!-- MARKDOWN LINKS &amp; IMAGES --&gt;
[contributors-shield]: https://img.shields.io/github/contributors/GreyDGL/PentestGPT.svg?style=for-the-badge
[contributors-url]: https://github.com/GreyDGL/PentestGPT/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/GreyDGL/PentestGPT.svg?style=for-the-badge
[forks-url]: https://github.com/GreyDGL/PentestGPT/network/members
[stars-shield]: https://img.shields.io/github/stars/GreyDGL/PentestGPT.svg?style=for-the-badge
[stars-url]: https://github.com/GreyDGL/PentestGPT/stargazers
[issues-shield]: https://img.shields.io/github/issues/GreyDGL/PentestGPT.svg?style=for-the-badge
[issues-url]: https://github.com/GreyDGL/PentestGPT/issues
[license-shield]: https://img.shields.io/github/license/GreyDGL/PentestGPT.svg?style=for-the-badge
[license-url]: https://github.com/GreyDGL/PentestGPT/blob/master/LICENSE.md
[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&amp;logo=linkedin&amp;colorB=555
[linkedin-url]: https://www.linkedin.com/in/gelei-deng-225a10112/
[linkedin-url2]: https://www.linkedin.com/in/vmayoral/
[discord-shield]: https://dcbadge.vercel.app/api/server/eC34CEfEkK
[discord-url]: https://discord.gg/eC34CEfEkK
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[exo-explore/exo]]></title>
            <link>https://github.com/exo-explore/exo</link>
            <guid>https://github.com/exo-explore/exo</guid>
            <pubDate>Sat, 20 Dec 2025 00:04:19 GMT</pubDate>
            <description><![CDATA[Run your own AI cluster at home with everyday devices üì±üíª üñ•Ô∏è‚åö]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/exo-explore/exo">exo-explore/exo</a></h1>
            <p>Run your own AI cluster at home with everyday devices üì±üíª üñ•Ô∏è‚åö</p>
            <p>Language: Python</p>
            <p>Stars: 33,636</p>
            <p>Forks: 2,286</p>
            <p>Stars today: 448 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;/docs/exo-logo-black-bg.jpg&quot;&gt;
  &lt;img alt=&quot;exo logo&quot; src=&quot;/docs/exo-logo-transparent.png&quot; width=&quot;50%&quot; height=&quot;50%&quot;&gt;
&lt;/picture&gt;

exo: Run your own AI cluster at home with everyday devices. Maintained by [exo labs](https://x.com/exolabs).


[![GitHub Repo stars](https://img.shields.io/github/stars/exo-explore/exo)](https://github.com/exo-explore/exo/stargazers)
[![License: Apache-2.0](https://img.shields.io/badge/License-Apache2.0-blue.svg)](https://www.apache.org/licenses/LICENSE-2.0.html)

&lt;a href=&quot;https://trendshift.io/repositories/11849&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/11849&quot; alt=&quot;exo-explore%2Fexo | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;/div&gt;

---

EXO connects all your devices into an AI cluster. It pools together the resources of all your devices in order to run large models. Not only does EXO enable running models larger than would fit on a single device, but with [day-0 support for RDMA over Thunderbolt](https://x.com/exolabs/status/2001817749744476256?s=20), makes models run faster as you add more devices.

## Features

- **Automatic Device Discovery**: Devices running EXO automatically discover each other - no manual configuration.
- **RDMA over Thunderbolt**: EXO ships with [day-0 support for RDMA over Thunderbolt 5](https://x.com/exolabs/status/2001817749744476256?s=20), enabling 99% reduction in latency between devices.
- **Topology-Aware Auto Parallel**: EXO figures out the best way to split your model across all available devices based on a realtime view of your device topology. It takes into account device resources and network latency/bandwidth between each link.
- **Tensor Parallelism**: EXO supports sharding models, for up to 1.8x speedup on 2 devices and 3.2x speedup on 4 devices.
- **MLX Support**: EXO uses [MLX](https://github.com/ml-explore/mlx) as an inference backend and [MLX distributed](https://ml-explore.github.io/mlx/build/html/usage/distributed.html) for distributed communication.

## Benchmarks

&lt;details&gt;
  &lt;summary&gt;Qwen3-235B (8-bit) on 4 √ó M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt;
  &lt;img src=&quot;docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-1-qwen3-235b.jpeg&quot; alt=&quot;Benchmark - Qwen3-235B (8-bit) on 4 √ó M3 Ultra Mac Studio with Tensor Parallel RDMA&quot; width=&quot;80%&quot; /&gt;
  &lt;p&gt;
    &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href=&quot;https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5&quot;&gt;Jeff Geerling: 15 TB VRAM on Mac Studio ‚Äì RDMA over Thunderbolt‚ÄØ5&lt;/a&gt;
  &lt;/p&gt;
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;DeepSeek v3.1 671B (8-bit) on 4 √ó M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt;
  &lt;img src=&quot;docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-2-deepseek-3.1-671b.jpeg&quot; alt=&quot;Benchmark - DeepSeek v3.1 671B (8-bit) on 4 √ó M3 Ultra Mac Studio with Tensor Parallel RDMA&quot; width=&quot;80%&quot; /&gt;
  &lt;p&gt;
    &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href=&quot;https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5&quot;&gt;Jeff Geerling: 15 TB VRAM on Mac Studio ‚Äì RDMA over Thunderbolt‚ÄØ5&lt;/a&gt;
  &lt;/p&gt;
&lt;/details&gt;

&lt;details&gt;
  &lt;summary&gt;Kimi K2 Thinking (native 4-bit) on 4 √ó M3 Ultra Mac Studio with Tensor Parallel RDMA&lt;/summary&gt;
  &lt;img src=&quot;docs/benchmarks/jeffgeerling/mac-studio-cluster-ai-full-3-kimi-k2-thinking.jpeg&quot; alt=&quot;Benchmark - Kimi K2 Thinking (native 4-bit) on 4 √ó M3 Ultra Mac Studio with Tensor Parallel RDMA&quot; width=&quot;80%&quot; /&gt;
  &lt;p&gt;
    &lt;strong&gt;Source:&lt;/strong&gt; &lt;a href=&quot;https://www.jeffgeerling.com/blog/2025/15-tb-vram-on-mac-studio-rdma-over-thunderbolt-5&quot;&gt;Jeff Geerling: 15 TB VRAM on Mac Studio ‚Äì RDMA over Thunderbolt‚ÄØ5&lt;/a&gt;
  &lt;/p&gt;
&lt;/details&gt;

---

## Quick Start

Devices running EXO automatically discover each other, without needing any manual configuration. Each device provides an API and a dashboard for interacting with your cluster (runs at `http://localhost:52415`).

There are two ways to run EXO:

### Run from Source (Mac &amp; Linux)

Clone the repo, build the dashboard, and run EXO:

```bash
cd dashboard &amp;&amp; npm install &amp;&amp; npm run build
uv run exo
```

**One-liner:**

```bash
git clone https://github.com/exo-explore/exo &amp;&amp; cd exo/dashboard &amp;&amp; npm i &amp;&amp; npm run build &amp;&amp; cd .. &amp;&amp; uv run exo
```

---

### macOS App

EXO ships a macOS app that runs in the background on your Mac.

&lt;img src=&quot;docs/macos-app-one-macbook.png&quot; alt=&quot;EXO macOS App - running on a MacBook&quot; width=&quot;35%&quot; /&gt;

The macOS app requires macOS Tahoe 26.2 or later.

Download the latest build here: [EXO-latest.dmg](https://assets.exolabs.net/EXO-latest.dmg).

The app will ask for permission to modify system settings and install a new Network profile. Improvements to this are being worked on.

---

## Hardware Accelerator Support

On macOS, EXO uses the GPU. On Linux, EXO currently runs on CPU. We are working on extending hardware accelerator support. If you&#039;d like support for a new hardware platform, please search for an existing feature request and add a thumbs up so we know what hardware is important to the community.

---

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines on how to contribute to EXO.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[swisskyrepo/PayloadsAllTheThings]]></title>
            <link>https://github.com/swisskyrepo/PayloadsAllTheThings</link>
            <guid>https://github.com/swisskyrepo/PayloadsAllTheThings</guid>
            <pubDate>Sat, 20 Dec 2025 00:04:18 GMT</pubDate>
            <description><![CDATA[A list of useful payloads and bypass for Web Application Security and Pentest/CTF]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/swisskyrepo/PayloadsAllTheThings">swisskyrepo/PayloadsAllTheThings</a></h1>
            <p>A list of useful payloads and bypass for Web Application Security and Pentest/CTF</p>
            <p>Language: Python</p>
            <p>Stars: 72,587</p>
            <p>Forks: 16,348</p>
            <p>Stars today: 38 stars today</p>
            <h2>README</h2><pre># Payloads All The Things

A list of useful payloads and bypasses for Web Application Security.
Feel free to improve with your payloads and techniques!

You can also contribute with a :beers: IRL, or using the sponsor button.

[![Sponsor](https://img.shields.io/static/v1?label=Sponsor&amp;message=%E2%9D%A4&amp;logo=GitHub&amp;link=https://github.com/sponsors/swisskyrepo)](https://github.com/sponsors/swisskyrepo)
[![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Payloads%20All%20The%20Things,%20a%20list%20of%20useful%20payloads%20and%20bypasses%20for%20Web%20Application%20Security%20-%20by%20@pentest_swissky&amp;url=https://github.com/swisskyrepo/PayloadsAllTheThings/)

An alternative display version is available at [PayloadsAllTheThingsWeb](https://swisskyrepo.github.io/PayloadsAllTheThings/).

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/.github/banner.png&quot; alt=&quot;banner&quot;&gt;
&lt;/p&gt;

## :book: Documentation

Every section contains the following files, you can use the `_template_vuln` folder to create a new chapter:

- README.md - vulnerability description and how to exploit it, including several payloads
- Intruder - a set of files to give to Burp Intruder
- Images - pictures for the README.md
- Files - some files referenced in the README.md

You might also like the other projects from the AllTheThings family :

- [InternalAllTheThings](https://swisskyrepo.github.io/InternalAllTheThings/) - Active Directory and Internal Pentest Cheatsheets
- [HardwareAllTheThings](https://swisskyrepo.github.io/HardwareAllTheThings/) - Hardware/IOT Pentesting Wiki

You want more? Check the [Books](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/_LEARNING_AND_SOCIALS/BOOKS.md) and [YouTube channel](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/_LEARNING_AND_SOCIALS/YOUTUBE.md) selections.

## :technologist: Contributions

Be sure to read [CONTRIBUTING.md](https://github.com/swisskyrepo/PayloadsAllTheThings/blob/master/CONTRIBUTING.md)

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/swisskyrepo/PayloadsAllTheThings/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=swisskyrepo/PayloadsAllTheThings&amp;max=36&quot; alt=&quot;sponsors-list&quot; &gt;
&lt;/a&gt;
&lt;/p&gt;

Thanks again for your contribution! :heart:

## :beers: Sponsors

This project is proudly sponsored by these companies.

| Logo | Description |
| --- | --- |
| [&lt;img src=&quot;https://avatars.githubusercontent.com/u/34724717?s=40&amp;v=4&quot; alt=&quot;sponsor-serpapi&quot;&gt;](https://serpapi.com) | **SerpApi** is a real time API to access Google search results. It solves the issues of having to rent proxies, solving captchas, and JSON parsing. |
| [&lt;img src=&quot;https://avatars.githubusercontent.com/u/50994705?s=40&amp;v=4&quot; alt=&quot;sponsor-projectdiscovery&quot;&gt;](https://projectdiscovery.io/) | **ProjectDiscovery** - Detect real, exploitable vulnerabilities. Harness the power of Nuclei for fast and accurate findings without false positives. |
| [&lt;img src=&quot;https://avatars.githubusercontent.com/u/48131541?s=40&amp;v=4&quot; alt=&quot;sponsor-vaadata&quot;&gt;](https://www.vaadata.com/) | **VAADATA** - Ethical Hacking Services |
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sgl-project/mini-sglang]]></title>
            <link>https://github.com/sgl-project/mini-sglang</link>
            <guid>https://github.com/sgl-project/mini-sglang</guid>
            <pubDate>Sat, 20 Dec 2025 00:04:17 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sgl-project/mini-sglang">sgl-project/mini-sglang</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 1,449</p>
            <p>Forks: 103</p>
            <p>Stars today: 504 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;400&quot; src=&quot;/assets/logo.png&quot;&gt;
&lt;/p&gt;

# Mini-SGLang

A **lightweight yet high-performance** inference framework for Large Language Models.

---

Mini-SGLang is a compact implementation of [SGLang](https://github.com/sgl-project/sglang), designed to demystify the complexities of modern LLM serving systems. With a compact codebase of **~5,000 lines of Python**, it serves as both a capable inference engine and a transparent reference for researchers and developers.

## ‚ú® Key Features

- **High Performance**: Achieves state-of-the-art throughput and latency with advanced optimizations.
- **Lightweight &amp; Readable**: A clean, modular, and fully type-annotated codebase that is easy to understand and modify.
- **Advanced Optimizations**:
  - **Radix Cache**: Reuses KV cache for shared prefixes across requests.
  - **Chunked Prefill**: Reduces peak memory usage for long-context serving.
  - **Overlap Scheduling**: Hides CPU scheduling overhead with GPU computation.
  - **Tensor Parallelism**: Scales inference across multiple GPUs.
  - **Optimized Kernels**: Integrates **FlashAttention** and **FlashInfer** for maximum efficiency.
  - ...

## üöÄ Quick Start

### 1. Environment Setup

We recommend using `uv` for a fast and reliable installation (note that `uv` does not conflict with `conda`).

```bash
# Create a virtual environment (Python 3.10+ recommended)
uv venv --python=3.12
source .venv/bin/activate
```

**Prerequisites**: Mini-SGLang relies on CUDA kernels that are JIT-compiled. Ensure you have the **NVIDIA CUDA Toolkit** installed and that its version matches your driver&#039;s version. You can check your driver&#039;s CUDA capability with `nvidia-smi`.

### 2. Installation

Install Mini-SGLang directly from the source:

```bash
git clone https://github.com/sgl-project/mini-sglang.git
cd mini-sglang
uv pip install -e .
```

### 3. Online Serving

Launch an OpenAI-compatible API server with a single command.

```bash
# Deploy Qwen/Qwen3-0.6B on a single GPU
python -m minisgl --model &quot;Qwen/Qwen3-0.6B&quot;

# Deploy meta-llama/Llama-3.1-70B-Instruct on 4 GPUs with Tensor Parallelism, on port 30000
python -m minisgl --model &quot;meta-llama/Llama-3.1-70B-Instruct&quot; --tp 4 --port 30000
```

Once the server is running, you can send requests using standard tools like `curl` or any OpenAI-compatible client.

### 4. Interactive Shell

Chat with your model directly in the terminal by adding the `--shell` flag.

```bash
python -m minisgl --model &quot;Qwen/Qwen3-0.6B&quot; --shell
```

![shell-example](https://lmsys.org/images/blog/minisgl/shell.png)

You can also use `/reset` to clear the chat history.

## Benchmark

### Offline inference

See [bench.py](./benchmark/offline/bench.py) for more details. Set `MINISGL_DISABLE_OVERLAP_SCHEDULING=1` for ablation study on overlap scheduling.

Test Configuration:

- Hardware: 1xH200 GPU.
- Model: Qwen3-0.6B, Qwen3-14B
- Total Requests: 256 sequences
- Input Length: Randomly sampled between 100-1024 tokens
- Output Length: Randomly sampled between 100-1024 tokens

![offline](https://lmsys.org/images/blog/minisgl/offline.png)

### Online inference

See [benchmark_qwen.py](./benchmark/online/bench_qwen.py) for more details.

Test Configuration:

- Hardware: 4xH200 GPU, connected by NVLink.
- Model: Qwen3-32B
- Dataset: [Qwen trace](https://github.com/alibaba-edu/qwen-bailian-usagetraces-anon/blob/main/qwen_traceA_blksz_16.jsonl), replaying first 1000 requests.

Launch command:

```bash
# Mini-SGLang
python -m minisgl --model &quot;Qwen/Qwen3-32B&quot; --tp 4 --cache naive

# SGLang
python3 -m sglang.launch_server --model &quot;Qwen/Qwen3-32B&quot; --tp 4 \
    --disable-radix --port 1919 --decode-attention flashinfer
```

![online](https://lmsys.org/images/blog/minisgl/online.png)

## üìö Learn More

- **[Detailed Features](./docs/features.md)**: Explore all available features and command-line arguments.
- **[System Architecture](./docs/structures.md)**: Dive deep into the design and data flow of Mini-SGLang.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[virattt/ai-hedge-fund]]></title>
            <link>https://github.com/virattt/ai-hedge-fund</link>
            <guid>https://github.com/virattt/ai-hedge-fund</guid>
            <pubDate>Sat, 20 Dec 2025 00:04:16 GMT</pubDate>
            <description><![CDATA[An AI Hedge Fund Team]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/virattt/ai-hedge-fund">virattt/ai-hedge-fund</a></h1>
            <p>An AI Hedge Fund Team</p>
            <p>Language: Python</p>
            <p>Stars: 43,772</p>
            <p>Forks: 7,734</p>
            <p>Stars today: 225 stars today</p>
            <h2>README</h2><pre># AI Hedge Fund

This is a proof of concept for an AI-powered hedge fund.  The goal of this project is to explore the use of AI to make trading decisions.  This project is for **educational** purposes only and is not intended for real trading or investment.

This system employs several agents working together:

1. Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation
2. Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety
3. Bill Ackman Agent - An activist investor, takes bold positions and pushes for change
4. Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption
5. Charlie Munger Agent - Warren Buffett&#039;s partner, only buys wonderful businesses at fair prices
6. Michael Burry Agent - The Big Short contrarian who hunts for deep value
7. Mohnish Pabrai Agent - The Dhandho investor, who looks for doubles at low risk
8. Peter Lynch Agent - Practical investor who seeks &quot;ten-baggers&quot; in everyday businesses
9. Phil Fisher Agent - Meticulous growth investor who uses deep &quot;scuttlebutt&quot; research 
10. Rakesh Jhunjhunwala Agent - The Big Bull of India
11. Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential
12. Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price
13. Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals
14. Sentiment Agent - Analyzes market sentiment and generates trading signals
15. Fundamentals Agent - Analyzes fundamental data and generates trading signals
16. Technicals Agent - Analyzes technical indicators and generates trading signals
17. Risk Manager - Calculates risk metrics and sets position limits
18. Portfolio Manager - Makes final trading decisions and generates orders

&lt;img width=&quot;1042&quot; alt=&quot;Screenshot 2025-03-22 at 6 19 07 PM&quot; src=&quot;https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4&quot; /&gt;

Note: the system does not actually make any trades.

[![Twitter Follow](https://img.shields.io/twitter/follow/virattt?style=social)](https://twitter.com/virattt)

## Disclaimer

This project is for **educational and research purposes only**.

- Not intended for real trading or investment
- No investment advice or guarantees provided
- Creator assumes no liability for financial losses
- Consult a financial advisor for investment decisions
- Past performance does not indicate future results

By using this software, you agree to use it solely for learning purposes.

## Table of Contents
- [How to Install](#how-to-install)
- [How to Run](#how-to-run)
  - [‚å®Ô∏è Command Line Interface](#Ô∏è-command-line-interface)
  - [üñ•Ô∏è Web Application](#Ô∏è-web-application)
- [How to Contribute](#how-to-contribute)
- [Feature Requests](#feature-requests)
- [License](#license)

## How to Install

Before you can run the AI Hedge Fund, you&#039;ll need to install it and set up your API keys. These steps are common to both the full-stack web application and command line interface.

### 1. Clone the Repository

```bash
git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
```

### 2. Set up API keys

Create a `.env` file for your API keys:
```bash
# Create .env file for your API keys (in the root directory)
cp .env.example .env
```

Open and edit the `.env` file to add your API keys:
```bash
# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
OPENAI_API_KEY=your-openai-api-key

# For getting financial data to power the hedge fund
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
```

**Important**: You must set at least one LLM API key (e.g. `OPENAI_API_KEY`, `GROQ_API_KEY`, `ANTHROPIC_API_KEY`, or `DEEPSEEK_API_KEY`) for the hedge fund to work. 

**Financial Data**: Data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key. For any other ticker, you will need to set the `FINANCIAL_DATASETS_API_KEY` in the .env file.

## How to Run

### ‚å®Ô∏è Command Line Interface

You can run the AI Hedge Fund directly via terminal. This approach offers more granular control and is useful for automation, scripting, and integration purposes.

&lt;img width=&quot;992&quot; alt=&quot;Screenshot 2025-01-06 at 5 50 17 PM&quot; src=&quot;https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b&quot; /&gt;

#### Quick Start

1. Install Poetry (if not already installed):
```bash
curl -sSL https://install.python-poetry.org | python3 -
```

2. Install dependencies:
```bash
poetry install
```

#### Run the AI Hedge Fund
```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA
```

You can also specify a `--ollama` flag to run the AI hedge fund using local LLMs.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama
```

You can optionally specify the start and end dates to make decisions over a specific time period.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
```

#### Run the Backtester
```bash
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
```

**Example Output:**
&lt;img width=&quot;941&quot; alt=&quot;Screenshot 2025-01-06 at 5 47 52 PM&quot; src=&quot;https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47&quot; /&gt;


Note: The `--ollama`, `--start-date`, and `--end-date` flags work for the backtester, as well!

### üñ•Ô∏è Web Application

The new way to run the AI Hedge Fund is through our web application that provides a user-friendly interface. This is recommended for users who prefer visual interfaces over command line tools.

Please see detailed instructions on how to install and run the web application [here](https://github.com/virattt/ai-hedge-fund/tree/main/app).

&lt;img width=&quot;1721&quot; alt=&quot;Screenshot 2025-06-28 at 6 41 03‚ÄØPM&quot; src=&quot;https://github.com/user-attachments/assets/b95ab696-c9f4-416c-9ad1-51feb1f5374b&quot; /&gt;


## How to Contribute

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

**Important**: Please keep your pull requests small and focused.  This will make it easier to review and merge.

## Feature Requests

If you have a feature request, please open an [issue](https://github.com/virattt/ai-hedge-fund/issues) and make sure it is tagged with `enhancement`.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[droidrun/droidrun]]></title>
            <link>https://github.com/droidrun/droidrun</link>
            <guid>https://github.com/droidrun/droidrun</guid>
            <pubDate>Sat, 20 Dec 2025 00:04:15 GMT</pubDate>
            <description><![CDATA[Automate your mobile devices with natural language commands - an LLM agnostic mobile Agent ü§ñ]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/droidrun/droidrun">droidrun/droidrun</a></h1>
            <p>Automate your mobile devices with natural language commands - an LLM agnostic mobile Agent ü§ñ</p>
            <p>Language: Python</p>
            <p>Stars: 6,986</p>
            <p>Forks: 712</p>
            <p>Stars today: 16 stars today</p>
            <h2>README</h2><pre>&lt;picture align=&quot;center&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;./static/droidrun-dark.png&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;./static/droidrun.png&quot;&gt;
  &lt;img src=&quot;./static/droidrun.png&quot;  width=&quot;full&quot;&gt;
&lt;/picture&gt;

&lt;div align=&quot;center&quot;&gt;

[![Docs](https://img.shields.io/badge/Docs-üìï-0D9373?style=for-the-badge)](https://docs.droidrun.ai)
[![Cloud](https://img.shields.io/badge/Cloud-‚òÅÔ∏è-0D9373?style=for-the-badge)](https://cloud.droidrun.ai/sign-in?waitlist=true)


[![GitHub stars](https://img.shields.io/github/stars/droidrun/droidrun?style=social)](https://github.com/droidrun/droidrun/stargazers)
[![droidrun.ai](https://img.shields.io/badge/droidrun.ai-white)](https://droidrun.ai)
[![Twitter Follow](https://img.shields.io/twitter/follow/droid_run?style=social)](https://x.com/droid_run)
[![Discord](https://img.shields.io/discord/1360219330318696488?color=white&amp;label=Discord&amp;logo=discord&amp;logoColor=white)](https://discord.gg/ZZbKEZZkwK)
[![Benchmark](https://img.shields.io/badge/Benchmark-91.4Ôπ™-white)](https://droidrun.ai/benchmark)



&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=983810&amp;theme=dark&amp;period=daily&amp;t=1753948032207&quot;&gt;
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=983810&amp;theme=neutral&amp;period=daily&amp;t=1753948125523&quot;&gt;
  &lt;a href=&quot;https://www.producthunt.com/products/droidrun-framework-for-mobile-agent?embed=true&amp;utm_source=badge-top-post-badge&amp;utm_medium=badge&amp;utm_source=badge-droidrun&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=983810&amp;theme=neutral&amp;period=daily&amp;t=1753948125523&quot; alt=&quot;Droidrun - Give&amp;#0032;AI&amp;#0032;native&amp;#0032;control&amp;#0032;of&amp;#0032;physical&amp;#0032;&amp;#0038;&amp;#0032;virtual&amp;#0032;phones&amp;#0046; | Product Hunt&quot; style=&quot;width: 200px; height: 54px;&quot; width=&quot;200&quot; height=&quot;54&quot; /&gt;&lt;/a&gt;
&lt;/picture&gt;


[Deutsch](https://zdoc.app/de/droidrun/droidrun) | 
[Espa√±ol](https://zdoc.app/es/droidrun/droidrun) | 
[fran√ßais](https://zdoc.app/fr/droidrun/droidrun) | 
[Êó•Êú¨Ë™û](https://zdoc.app/ja/droidrun/droidrun) | 
[ÌïúÍµ≠Ïñ¥](https://zdoc.app/ko/droidrun/droidrun) | 
[Portugu√™s](https://zdoc.app/pt/droidrun/droidrun) | 
[–†—É—Å—Å–∫–∏–π](https://zdoc.app/ru/droidrun/droidrun) | 
[‰∏≠Êñá](https://zdoc.app/zh/droidrun/droidrun)

&lt;/div&gt;



DroidRun is a powerful framework for controlling Android and iOS devices through LLM agents. It allows you to automate device interactions using natural language commands. [Checkout our benchmark results](https://droidrun.ai/benchmark)

## Why Droidrun?

- ü§ñ Control Android and iOS devices with natural language commands
- üîÄ Supports multiple LLM providers (OpenAI, Anthropic, Gemini, Ollama, DeepSeek)
- üß† Planning capabilities for complex multi-step tasks
- üíª Easy to use CLI with enhanced debugging features
- üêç Extendable Python API for custom automations
- üì∏ Screenshot analysis for visual understanding of the device
- ü´Ü Execution tracing with Arize Phoenix

## üì¶ Installation

```bash
pip install &#039;droidrun[google,anthropic,openai,deepseek,ollama,dev]&#039;
```

## üöÄ Quickstart
Read on how to get droidrun up and running within seconds in [our docs](https://docs.droidrun.ai/v3/quickstart)!   

[![Quickstart Video](https://img.youtube.com/vi/4WT7FXJah2I/0.jpg)](https://www.youtube.com/watch?v=4WT7FXJah2I)

## üé¨ Demo Videos

1. **Accommodation booking**: Let Droidrun search for an apartment for you

   [![Droidrun Accommodation Booking Demo](https://img.youtube.com/vi/VUpCyq1PSXw/0.jpg)](https://youtu.be/VUpCyq1PSXw)

&lt;br&gt;

2. **Trend Hunter**: Let Droidrun hunt down trending posts

   [![Droidrun Trend Hunter Demo](https://img.youtube.com/vi/7V8S2f8PnkQ/0.jpg)](https://youtu.be/7V8S2f8PnkQ)

&lt;br&gt;

3. **Streak Saver**: Let Droidrun save your streak on your favorite language learning app

   [![Droidrun Streak Saver Demo](https://img.youtube.com/vi/B5q2B467HKw/0.jpg)](https://youtu.be/B5q2B467HKw)


## üí° Example Use Cases

- Automated UI testing of mobile applications
- Creating guided workflows for non-technical users
- Automating repetitive tasks on mobile devices
- Remote assistance for less technical users
- Exploring mobile UI with natural language commands

## üë• Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details. 

## Security Checks

To ensure the security of the codebase, we have integrated security checks using `bandit` and `safety`. These tools help identify potential security issues in the code and dependencies.

### Running Security Checks

Before submitting any code, please run the following security checks:

1. **Bandit**: A tool to find common security issues in Python code.
   ```bash
   bandit -r droidrun
   ```

2. **Safety**: A tool to check your installed dependencies for known security vulnerabilities.
   ```bash
   safety scan
   ```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pytorch/executorch]]></title>
            <link>https://github.com/pytorch/executorch</link>
            <guid>https://github.com/pytorch/executorch</guid>
            <pubDate>Sat, 20 Dec 2025 00:04:14 GMT</pubDate>
            <description><![CDATA[On-device AI across mobile, embedded and edge for PyTorch]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pytorch/executorch">pytorch/executorch</a></h1>
            <p>On-device AI across mobile, embedded and edge for PyTorch</p>
            <p>Language: Python</p>
            <p>Stars: 3,750</p>
            <p>Forks: 768</p>
            <p>Stars today: 18 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/source/_static/img/et-logo.png&quot; alt=&quot;ExecuTorch logo mark&quot; width=&quot;200&quot;&gt;
  &lt;h1&gt;ExecuTorch&lt;/h1&gt;
  &lt;p&gt;&lt;strong&gt;On-device AI inference powered by PyTorch&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pypi.org/project/executorch/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/executorch?style=for-the-badge&amp;color=blue&quot; alt=&quot;PyPI - Version&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/pytorch/executorch/graphs/contributors&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/pytorch/executorch?style=for-the-badge&amp;color=blue&quot; alt=&quot;GitHub - Contributors&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/pytorch/executorch/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/pytorch/executorch?style=for-the-badge&amp;color=blue&quot; alt=&quot;GitHub - Stars&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/Dh43CKSAdc&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20Us-blue?logo=discord&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Discord - Chat with Us&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://docs.pytorch.org/executorch/main/index.html&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Documentation-blue?logo=googledocs&amp;logoColor=white&amp;style=for-the-badge&quot; alt=&quot;Documentation&quot;&gt;&lt;/a&gt;
&lt;/div&gt;

**ExecuTorch** is PyTorch&#039;s unified solution for deploying AI models on-device‚Äîfrom smartphones to microcontrollers‚Äîbuilt for privacy, performance, and portability. It powers Meta&#039;s on-device AI across **Instagram, WhatsApp, Quest 3, Ray-Ban Meta Smart Glasses**, and [more](https://docs.pytorch.org/executorch/main/success-stories.html).

Deploy **LLMs, vision, speech, and multimodal models** with the same PyTorch APIs you already know‚Äîaccelerating research to production with seamless model export, optimization, and deployment. No manual C++ rewrites. No format conversions. No vendor lock-in.

&lt;details&gt;
  &lt;summary&gt;&lt;strong&gt;üìò Table of Contents&lt;/strong&gt;&lt;/summary&gt;

- [Why ExecuTorch?](#why-executorch)
- [How It Works](#how-it-works)
- [Quick Start](#quick-start)
  - [Installation](#installation)
  - [Export and Deploy in 3 Steps](#export-and-deploy-in-3-steps)
  - [Run on Device](#run-on-device)
  - [LLM Example: Llama](#llm-example-llama)
- [Platform &amp; Hardware Support](#platform--hardware-support)
- [Production Deployments](#production-deployments)
- [Examples &amp; Models](#examples--models)
- [Key Features](#key-features)
- [Documentation](#documentation)
- [Community &amp; Contributing](#community--contributing)
- [License](#license)

&lt;/details&gt;

## Why ExecuTorch?

- **üîí Native PyTorch Export** ‚Äî Direct export from PyTorch. No .onnx, .tflite, or intermediate format conversions. Preserve model semantics.
- **‚ö° Production-Proven** ‚Äî Powers billions of users at [Meta with real-time on-device inference](https://engineering.fb.com/2025/07/28/android/executorch-on-device-ml-meta-family-of-apps/).
- **üíæ Tiny Runtime** ‚Äî 50KB base footprint. Runs on microcontrollers to high-end smartphones.
- **üöÄ [12+ Hardware Backends](https://docs.pytorch.org/executorch/main/backends-overview.html)** ‚Äî Open-source acceleration for Apple, Qualcomm, ARM, MediaTek, Vulkan, and more.
- **üéØ One Export, Multiple Backends** ‚Äî Switch hardware targets with a single line change. Deploy the same model everywhere.

## How It Works

ExecuTorch uses **ahead-of-time (AOT) compilation** to prepare PyTorch models for edge deployment:

1. **üß© Export** ‚Äî Capture your PyTorch model graph with `torch.export()`
2. **‚öôÔ∏è Compile** ‚Äî Quantize, optimize, and partition to hardware backends ‚Üí `.pte`
3. **üöÄ Execute** ‚Äî Load `.pte` on-device via lightweight C++ runtime

Models use a standardized [Core ATen operator set](https://docs.pytorch.org/executorch/main/compiler-ir-advanced.html#intermediate-representation). [Partitioners](https://docs.pytorch.org/executorch/main/compiler-delegate-and-partitioner.html) delegate subgraphs to specialized hardware (NPU/GPU) with CPU fallback.

Learn more: [How ExecuTorch Works](https://docs.pytorch.org/executorch/main/intro-how-it-works.html) ‚Ä¢ [Architecture Guide](https://docs.pytorch.org/executorch/main/getting-started-architecture.html)

## Quick Start

### Installation

```bash
pip install executorch
```

For platform-specific setup (Android, iOS, embedded systems), see the [Quick Start](https://docs.pytorch.org/executorch/main/quick-start-section.html) documentation for additional info.

### Export and Deploy in 3 Steps

```python
import torch
from executorch.exir import to_edge_transform_and_lower
from executorch.backends.xnnpack.partition.xnnpack_partitioner import XnnpackPartitioner

# 1. Export your PyTorch model
model = MyModel().eval()
example_inputs = (torch.randn(1, 3, 224, 224),)
exported_program = torch.export.export(model, example_inputs)

# 2. Optimize for target hardware (switch backends with one line)
program = to_edge_transform_and_lower(
    exported_program,
    partitioner=[XnnpackPartitioner()]  # CPU | CoreMLPartitioner() for iOS | QnnPartitioner() for Qualcomm
).to_executorch()

# 3. Save for deployment
with open(&quot;model.pte&quot;, &quot;wb&quot;) as f:
    f.write(program.buffer)

# Test locally via ExecuTorch runtime&#039;s pybind API (optional)
from executorch.runtime import Runtime
runtime = Runtime.get()
method = runtime.load_program(&quot;model.pte&quot;).load_method(&quot;forward&quot;)
outputs = method.execute([torch.randn(1, 3, 224, 224)])
```

### Run on Device

**[C++](https://docs.pytorch.org/executorch/main/using-executorch-cpp.html)**
```cpp
#include &lt;executorch/extension/module/module.h&gt;
#include &lt;executorch/extension/tensor/tensor.h&gt;

Module module(&quot;model.pte&quot;);
auto tensor = make_tensor_ptr({2, 2}, {1.0f, 2.0f, 3.0f, 4.0f});
auto outputs = module.forward(tensor);
```

**[Swift (iOS)](https://docs.pytorch.org/executorch/main/ios-section.html)**
```swift
import ExecuTorch

let module = Module(filePath: &quot;model.pte&quot;)
let input = Tensor&lt;Float&gt;([1.0, 2.0, 3.0, 4.0], shape: [2, 2])
let outputs = try module.forward(input)
```

**[Kotlin (Android)](https://docs.pytorch.org/executorch/main/android-section.html)**
```kotlin
val module = Module.load(&quot;model.pte&quot;)
val inputTensor = Tensor.fromBlob(floatArrayOf(1.0f, 2.0f, 3.0f, 4.0f), longArrayOf(2, 2))
val outputs = module.forward(EValue.from(inputTensor))
```

### LLM Example: Llama

Export Llama models using the [`export_llm`](https://docs.pytorch.org/executorch/main/llm/export-llm.html) script or [Optimum-ExecuTorch](https://github.com/huggingface/optimum-executorch):

```bash
# Using export_llm
python -m executorch.extension.llm.export.export_llm --model llama3_2 --output llama.pte

# Using Optimum-ExecuTorch
optimum-cli export executorch \
  --model meta-llama/Llama-3.2-1B \
  --task text-generation \
  --recipe xnnpack \
  --output_dir llama_model
```

Run on-device with the LLM runner API:

**[C++](https://docs.pytorch.org/executorch/main/llm/run-with-c-plus-plus.html)**
```cpp
#include &lt;executorch/extension/llm/runner/text_llm_runner.h&gt;

auto runner = create_llama_runner(&quot;llama.pte&quot;, &quot;tiktoken.bin&quot;);
executorch::extension::llm::GenerationConfig config{
    .seq_len = 128, .temperature = 0.8f};
runner-&gt;generate(&quot;Hello, how are you?&quot;, config);
```

**[Swift (iOS)](https://docs.pytorch.org/executorch/main/llm/run-on-ios.html)**
```swift
import ExecuTorchLLM

let runner = TextRunner(modelPath: &quot;llama.pte&quot;, tokenizerPath: &quot;tiktoken.bin&quot;)
try runner.generate(&quot;Hello, how are you?&quot;, Config {
    $0.sequenceLength = 128
}) { token in
    print(token, terminator: &quot;&quot;)
}
```

**Kotlin (Android)** ‚Äî [API Docs](https://docs.pytorch.org/executorch/main/javadoc/org/pytorch/executorch/extension/llm/package-summary.html) ‚Ä¢ [Demo App](https://github.com/meta-pytorch/executorch-examples/tree/main/llm/android/LlamaDemo)
```kotlin
val llmModule = LlmModule(&quot;llama.pte&quot;, &quot;tiktoken.bin&quot;, 0.8f)
llmModule.load()
llmModule.generate(&quot;Hello, how are you?&quot;, 128, object : LlmCallback {
    override fun onResult(result: String) { print(result) }
    override fun onStats(stats: String) { }
})
```

For multimodal models (vision, audio), use the [MultiModal runner API](extension/llm/runner) which extends the LLM runner to handle image and audio inputs alongside text. See [Llava](examples/models/llava/README.md) and [Voxtral](examples/models/voxtral/README.md) examples.

See [examples/models/llama](examples/models/llama/README.md) for complete workflow including quantization, mobile deployment, and advanced options.

**Next Steps:**
- üìñ [Step-by-step tutorial](https://docs.pytorch.org/executorch/main/getting-started.html) ‚Äî Complete walkthrough for your first model
- ‚ö° [Colab notebook](https://colab.research.google.com/drive/1qpxrXC3YdJQzly3mRg-4ayYiOjC6rue3?usp=sharing) ‚Äî Try ExecuTorch instantly in your browser
- ü§ñ [Deploy Llama models](examples/models/llama/README.md) ‚Äî LLM workflow with quantization and mobile demos

## Platform &amp; Hardware Support

| **Platform**     | **Supported Backends**                                   |
|------------------|----------------------------------------------------------|
| Android          | XNNPACK, Vulkan, Qualcomm, MediaTek, Samsung Exynos      |
| iOS              | XNNPACK, MPS, CoreML (Neural Engine)                     |
| Linux / Windows  | XNNPACK, OpenVINO, CUDA *(experimental)*                 |
| macOS            | XNNPACK, MPS, Metal *(experimental)*                     |
| Embedded / MCU   | XNNPACK, ARM Ethos-U, NXP, Cadence DSP                   |

See [Backend Documentation](https://docs.pytorch.org/executorch/main/backends-overview.html) for detailed hardware requirements and optimization guides.

## Production Deployments

ExecuTorch powers on-device AI at scale across Meta&#039;s family of apps, VR/AR devices, and partner deployments. [View success stories ‚Üí](https://docs.pytorch.org/executorch/main/success-stories.html)

## Examples &amp; Models

**LLMs:** [Llama 3.2/3.1/3](examples/models/llama/README.md), [Qwen 3](examples/models/qwen3/README.md), [Phi-4-mini](examples/models/phi_4_mini/README.md), [LiquidAI LFM2](examples/models/lfm2/README.md)

**Multimodal:** [Llava](examples/models/llava/README.md) (vision-language), [Voxtral](examples/models/voxtral/README.md) (audio-language), [Gemma](examples/models/gemma3) (vision-language)

**Vision/Speech:** [MobileNetV2](https://github.com/meta-pytorch/executorch-examples/tree/main/mv2), [DeepLabV3](https://github.com/meta-pytorch/executorch-examples/tree/main/dl3), [Whisper](https://github.com/meta-pytorch/executorch-examples/tree/main/whisper/android/WhisperApp)

**Resources:** [`examples/`](examples/) directory ‚Ä¢ [executorch-examples](https://github.com/meta-pytorch/executorch-examples) out-of-tree demos ‚Ä¢ [Optimum-ExecuTorch](https://github.com/huggingface/optimum-executorch) for HuggingFace models

## Key Features

ExecuTorch provides advanced capabilities for production deployment:

- **Quantization** ‚Äî Built-in support via [torchao](https://docs.pytorch.org/ao) for 8-bit, 4-bit, and dynamic quantization
- **Memory Planning** ‚Äî Optimize memory usage with ahead-of-time allocation strategies
- **Developer Tools** ‚Äî ETDump profiler, ETRecord inspector, and model debugger
- **Selective Build** ‚Äî Strip unused operators to minimize binary size
- **Custom Operators** ‚Äî Extend with domain-specific kernels
- **Dynamic Shapes** ‚Äî Support variable input sizes with bounded ranges

See [Advanced Topics](https://docs.pytorch.org/executorch/main/advanced-topics-section.html) for quantization techniques, custom backends, and compiler passes.

## Documentation

- [**Documentation Home**](https://docs.pytorch.org/executorch/main/index.html) ‚Äî Complete guides and tutorials
- [**API Reference**](https://docs.pytorch.org/executorch/main/api-section.html) ‚Äî Python, C++, Java/Kotlin APIs
- [**Backend Integration**](https://docs.pytorch.org/executorch/main/backend-delegates-integration.html) ‚Äî Build custom hardware backends
- [**Troubleshooting**](https://docs.pytorch.org/executorch/main/support-section.html) ‚Äî Common issues and solutions

## Community &amp; Contributing

We welcome contributions from the community!

- üí¨ [**GitHub Discussions**](https://github.com/pytorch/executorch/discussions) ‚Äî Ask questions and share ideas
- üéÆ [**Discord**](https://discord.gg/Dh43CKSAdc) ‚Äî Chat with the team and community
- üêõ [**Issues**](https://github.com/pytorch/executorch/issues) ‚Äî Report bugs or request features
- ü§ù [**Contributing Guide**](CONTRIBUTING.md) ‚Äî Guidelines and codebase structure

## License

ExecuTorch is BSD licensed, as found in the [LICENSE](LICENSE) file.

&lt;br&gt;&lt;br&gt;

---

&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Part of the PyTorch ecosystem&lt;/strong&gt;&lt;/p&gt;
  &lt;p&gt;
    &lt;a href=&quot;https://github.com/pytorch/executorch&quot;&gt;GitHub&lt;/a&gt; ‚Ä¢
    &lt;a href=&quot;https://docs.pytorch.org/executorch&quot;&gt;Documentation&lt;/a&gt;
  &lt;/p&gt;
&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[ansible/ansible]]></title>
            <link>https://github.com/ansible/ansible</link>
            <guid>https://github.com/ansible/ansible</guid>
            <pubDate>Sat, 20 Dec 2025 00:04:13 GMT</pubDate>
            <description><![CDATA[Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain. Automate everything from code deployment to network configuration to cloud management, in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ansible/ansible">ansible/ansible</a></h1>
            <p>Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain. Automate everything from code deployment to network configuration to cloud management, in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com.</p>
            <p>Language: Python</p>
            <p>Stars: 67,387</p>
            <p>Forks: 24,232</p>
            <p>Stars today: 24 stars today</p>
            <h2>README</h2><pre>[![PyPI version](https://img.shields.io/pypi/v/ansible-core.svg)](https://pypi.org/project/ansible-core)
[![Docs badge](https://img.shields.io/badge/docs-latest-brightgreen.svg)](https://docs.ansible.com/ansible/latest/)
[![Chat badge](https://img.shields.io/badge/chat-IRC-brightgreen.svg)](https://docs.ansible.com/ansible/devel/community/communication.html)
[![Build Status](https://dev.azure.com/ansible/ansible/_apis/build/status/CI?branchName=devel)](https://dev.azure.com/ansible/ansible/_build/latest?definitionId=20&amp;branchName=devel)
[![Ansible Code of Conduct](https://img.shields.io/badge/code%20of%20conduct-Ansible-silver.svg)](https://docs.ansible.com/ansible/devel/community/code_of_conduct.html)
[![Ansible mailing lists](https://img.shields.io/badge/mailing%20lists-Ansible-orange.svg)](https://docs.ansible.com/ansible/devel/community/communication.html#mailing-list-information)
[![Repository License](https://img.shields.io/badge/license-GPL%20v3.0-brightgreen.svg)](COPYING)
[![Ansible CII Best Practices certification](https://bestpractices.coreinfrastructure.org/projects/2372/badge)](https://bestpractices.coreinfrastructure.org/projects/2372)

# Ansible

Ansible is a radically simple IT automation system. It handles
configuration management, application deployment, cloud provisioning,
ad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complex
changes like zero-downtime rolling updates with load balancers easy. More information on the Ansible [website](https://ansible.com/).

## Design Principles

* Have an extremely simple setup process with a minimal learning curve.
* Manage machines quickly and in parallel.
* Avoid custom-agents and additional open ports, be agentless by
  leveraging the existing SSH daemon.
* Describe infrastructure in a language that is both machine and human
  friendly.
* Focus on security and easy auditability/review/rewriting of content.
* Manage new remote machines instantly, without bootstrapping any
  software.
* Allow module development in any dynamic language, not just Python.
* Be usable as non-root.
* Be the easiest IT automation system to use, ever.

## Use Ansible

You can install a released version of Ansible with `pip` or a package manager. See our
[installation guide](https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html) for details on installing Ansible
on a variety of platforms.

Power users and developers can run the `devel` branch, which has the latest
features and fixes, directly. Although it is reasonably stable, you are more likely to encounter
breaking changes when running the `devel` branch. We recommend getting involved
in the Ansible community if you want to run the `devel` branch.

## Communication

Join the Ansible forum to ask questions, get help, and interact with the
community.

* [Get Help](https://forum.ansible.com/c/help/6): Find help or share your Ansible knowledge to help others.
  Use tags to filter and subscribe to posts, such as the following:
  * Posts tagged with [ansible](https://forum.ansible.com/tag/ansible)
  * Posts tagged with [ansible-core](https://forum.ansible.com/tag/ansible-core)
  * Posts tagged with [playbook](https://forum.ansible.com/tag/playbook)
* [Social Spaces](https://forum.ansible.com/c/chat/4): Meet and interact with fellow enthusiasts.
* [News &amp; Announcements](https://forum.ansible.com/c/news/5): Track project-wide announcements including social events.
* [Bullhorn newsletter](https://docs.ansible.com/ansible/devel/community/communication.html#the-bullhorn): Get release announcements and important changes.

For more ways to get in touch, see [Communicating with the Ansible community](https://docs.ansible.com/ansible/devel/community/communication.html).

## Contribute to Ansible

* Check out the [Contributor&#039;s Guide](./.github/CONTRIBUTING.md).
* Read [Community Information](https://docs.ansible.com/ansible/devel/community) for all
  kinds of ways to contribute to and interact with the project,
  including how to submit bug reports and code to Ansible.
* Submit a proposed code update through a pull request to the `devel` branch.
* Talk to us before making larger changes
  to avoid duplicate efforts. This not only helps everyone
  know what is going on, but it also helps save time and effort if we decide
  some changes are needed.

## Coding Guidelines

We document our Coding Guidelines in the [Developer Guide](https://docs.ansible.com/ansible/devel/dev_guide/). We particularly suggest you review:

* [Contributing your module to Ansible](https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_checklist.html)
* [Conventions, tips, and pitfalls](https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_best_practices.html)

## Branch Info

* The `devel` branch corresponds to the release actively under development.
* The `stable-2.X` branches correspond to stable releases.
* Create a branch based on `devel` and set up a [dev environment](https://docs.ansible.com/ansible/devel/dev_guide/developing_modules_general.html#common-environment-setup) if you want to open a PR.
* See the [Ansible release and maintenance](https://docs.ansible.com/ansible/devel/reference_appendices/release_and_maintenance.html) page for information about active branches.

## Roadmap

Based on team and community feedback, an initial roadmap will be published for a major or minor version (ex: 2.7, 2.8).
The [Ansible Roadmap page](https://docs.ansible.com/ansible/devel/roadmap/) details what is planned and how to influence the roadmap.

## Authors

Ansible was created by [Michael DeHaan](https://github.com/mpdehaan)
and has contributions from over 5000 users (and growing). Thanks everyone!

[Ansible](https://www.ansible.com) is sponsored by [Red Hat, Inc.](https://www.redhat.com)

## License

GNU General Public License v3.0 or later

See [COPYING](COPYING) to see the full text.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pollen-robotics/reachy_mini]]></title>
            <link>https://github.com/pollen-robotics/reachy_mini</link>
            <guid>https://github.com/pollen-robotics/reachy_mini</guid>
            <pubDate>Sat, 20 Dec 2025 00:04:12 GMT</pubDate>
            <description><![CDATA[Reachy Mini's SDK]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pollen-robotics/reachy_mini">pollen-robotics/reachy_mini</a></h1>
            <p>Reachy Mini's SDK</p>
            <p>Language: Python</p>
            <p>Stars: 406</p>
            <p>Forks: 64</p>
            <p>Stars today: 17 stars today</p>
            <h2>README</h2><pre># Reachy Mini ü§ñ

[![Ask on HuggingChat](https://img.shields.io/badge/Ask_on-HuggingChat-yellow?logo=huggingface&amp;logoColor=yellow&amp;style=for-the-badge)](https://huggingface.co/chat/?attachments=https%3A%2F%2Fgist.githubusercontent.com%2FFabienDanieau%2F919e1d7468fb16e70dbe984bdc277bba%2Fraw%2Fdoc_reachy_mini_full.md&amp;prompt=Read%20this%20documentation%20about%20Reachy%20Mini%20so%20I%20can%20ask%20questions%20about%20it.)
[![Discord](https://img.shields.io/badge/Discord-Join_the_Community-7289DA?logo=discord&amp;logoColor=white)](https://discord.gg/2bAhWfXme9)

**Reachy Mini is an open-source, expressive robot made for hackers and AI builders.**

It removes the hardware complexity, allowing you to focus on what matters: building intelligent agents that can see, hear, and interact with the physical world.

[![Reachy Mini Hello](/docs/assets/reachy_mini_hello.gif)](https://www.pollen-robotics.com/reachy-mini/)

## üöÄ Getting Started

**Choose your platform to access the specific guide:**

| **ü§ñ Reachy Mini (Wireless)** | **üîå Reachy Mini Lite** | **üíª Simulation** |
| :---: | :---: | :---: |
| The full autonomous experience.&lt;br&gt;Raspberry Pi 4 + Battery + WiFi. | The developer version.&lt;br&gt;USB connection to your computer. | No hardware required.&lt;br&gt;Prototype in MuJoCo. |
| [**üëâ Go to Wireless Guide**](docs/platforms/reachy_mini/get_started.md) | [**üëâ Go to Lite Guide**](docs/platforms/reachy_mini_lite/get_started.md) | [**üëâ Go to Simulation**](docs/platforms/simulation/get_started.md) |

&gt; ‚ö° **Pro tip:** Install [uv](https://docs.astral.sh/uv/getting-started/installation/) for 10-100x faster app installations (auto-detected, falls back to `pip`).

&lt;br&gt;

## üì± Apps &amp; Ecosystem

Reachy Mini comes with an app store powered by Hugging Face Spaces. You can install these apps directly from your robot&#039;s dashboard with one click!

* **üó£Ô∏è [Conversation App](https://huggingface.co/spaces/pollen-robotics/reachy_mini_conversation_app):** Talk naturally with Reachy Mini (powered by LLMs).
* **üìª [Radio](https://huggingface.co/spaces/pollen-robotics/reachy_mini_radio):** Listen to the radio with Reachy Mini !
* **üëã [Hand Tracker](https://huggingface.co/spaces/pollen-robotics/hand_tracker_v2):** The robot follows your hand movements in real-time.

[**üëâ Browse all apps on Hugging Face**](https://pollen-robotics-reachy-mini-landing-page.hf.space/#/apps)

&lt;br&gt;

## üêç Software &amp; SDK

Once your robot (or simulation) is running, the code is the same!
Control your Reachy Mini with Python to create movements, build apps, and connect AI models.

You will find the Installation guide, Quickstart, and API Reference in the SDK documentation.

[**üëâ Go to SDK Documentation**](docs/SDK/readme.md)

&lt;br&gt;

## üõ† Hardware Overview

Reachy Mini robots are sold as kits and generally take **2 to 3 hours** to assemble. Detailed step-by-step guides are available in the platform-specific folders linked above.

* **Reachy Mini (Wireless):** Runs onboard (RPi 4), autonomous, includes IMU. [See specs](docs/platforms/reachy_mini/hardware.md).
* **Reachy Mini Lite:** Runs on your PC, powered via wall outlet. [See specs](docs/platforms/reachy_mini_lite/hardware.md).

[**üõí Buy Reachy Mini**](https://www.hf.co/reachy-mini/)

&lt;br&gt;

## ‚ùì Troubleshooting

Encountering an issue? üëâ **[Check the Troubleshooting &amp; FAQ Guide](/docs/troubleshooting.md)**

&lt;br&gt;

## ü§ù Community &amp; Contributing

Reachy Mini is a collaborative project between [Pollen Robotics](https://www.pollen-robotics.com) and [Hugging Face](https://huggingface.co/).

* **Join the Community:** We use [Discord](https://discord.gg/2bAhWfXme9) to share our moments with Reachy, build apps together, and get help.
* **Found a bug?** Open an issue on this repository.
* **Created an App?** Share it on [Hugging Face Spaces](https://huggingface.co/spaces?q=reachy_mini).

## License

This project is licensed under the Apache 2.0 License. See the [LICENSE](LICENSE) file for details.
Hardware design files are licensed under Creative Commons BY-SA-NC.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[letta-ai/letta]]></title>
            <link>https://github.com/letta-ai/letta</link>
            <guid>https://github.com/letta-ai/letta</guid>
            <pubDate>Sat, 20 Dec 2025 00:04:11 GMT</pubDate>
            <description><![CDATA[Letta is the platform for building stateful agents: open AI with advanced memory that can learn and self-improve over time.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/letta-ai/letta">letta-ai/letta</a></h1>
            <p>Letta is the platform for building stateful agents: open AI with advanced memory that can learn and self-improve over time.</p>
            <p>Language: Python</p>
            <p>Stars: 20,060</p>
            <p>Forks: 2,098</p>
            <p>Stars today: 282 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://raw.githubusercontent.com/letta-ai/letta/refs/heads/main/assets/Letta-logo-RGB_GreyonTransparent_cropped_small.png&quot;&gt;
    &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://raw.githubusercontent.com/letta-ai/letta/refs/heads/main/assets/Letta-logo-RGB_OffBlackonTransparent_cropped_small.png&quot;&gt;
    &lt;img alt=&quot;Letta logo&quot; src=&quot;https://raw.githubusercontent.com/letta-ai/letta/refs/heads/main/assets/Letta-logo-RGB_GreyonOffBlack_cropped_small.png&quot; width=&quot;500&quot;&gt;
  &lt;/picture&gt;
&lt;/p&gt;

# Letta (formerly MemGPT)

Letta is the platform for building stateful agents: open AI with advanced memory that can learn and self-improve over time.

* [**Quickstart**](https://docs.letta.com/quickstart): Build your first stateful agent in 5 minutes using Python or TypeScript
* [**Understanding agent memory**](https://docs.letta.com/core-concepts): Learn about memory blocks, tools, and how Letta agents maintain state
* [**Examples and tutorials**](https://docs.letta.com/tutorials/): Working code examples for common use cases and agent patterns
* [**API reference**](https://docs.letta.com/api): Complete REST API and SDK documentation for Python and TypeScript

&gt; [!TIP]
&gt; **Letta Code** is a memory-first coding harness, built on top of the Letta API. Instead of working in independent sessions, you work with a persisted agent that learns over time and is portable across models. You can use Letta Code to interact with any Letta agent via the CLI.
&gt; 
&gt; Read more about how to use Letta Code on the [official docs page](https://docs.letta.com/letta-code), or on the [GitHub repo](https://github.com/letta-ai/letta-code).

## Get started with the Letta API

Use the Letta API to build stateful agents that remember, learn, and improve over time. Letta allows you to build agents on any model provider, including OpenAI, Anthropic, Google Gemini, and more.

### Installation

TypeScript / Node.js:
```bash
npm install @letta-ai/letta-client
```

Python:
```bash
pip install letta-client
```

### Hello World example

Below is a quick example of creating a stateful agent and sending it a message.
See the full [quickstart guide](https://docs.letta.com/quickstart) for complete documentation.

Running the examples require a [Letta Developer Platform](https://app.letta.com) account, or a [self-hosted Letta server](https://docs.letta.com/guides/selfhosting/).

TypeScript:
```typescript
import Letta from &quot;@letta-ai/letta-client&quot;;

const client = new Letta({ apiKey: process.env.LETTA_API_KEY });

// Create your agent
const agentState = await client.agents.create({
  model: &quot;openai/gpt-4.1&quot;,
  embedding: &quot;openai/text-embedding-3-small&quot;,
  memory_blocks: [
    {
      label: &quot;human&quot;,
      value:
        &quot;Name: Timber. Status: dog. Occupation: building Letta, infrastructure to democratize self-improving superintelligence&quot;,
    },
    {
      label: &quot;persona&quot;,
      value:
        &quot;I am a self-improving superintelligence. Timber is my best friend and collaborator.&quot;,
    },
  ],
  tools: [&quot;web_search&quot;, &quot;run_code&quot;],
});

console.log(&quot;Agent created with ID:&quot;, agentState.id);

// Send your agent a message
const response = await client.agents.messages.create(agentState.id, {
  input: &quot;What do you know about me?&quot;,
});

for (const message of response.messages) {
  console.log(message);
}
```

Python:
```python
from letta_client import Letta
import os

client = Letta(api_key=os.getenv(&quot;LETTA_API_KEY&quot;))

# Create your agent
agent_state = client.agents.create(
    model=&quot;openai/gpt-4.1&quot;,
    embedding=&quot;openai/text-embedding-3-small&quot;,
    memory_blocks=[
        {
          &quot;label&quot;: &quot;human&quot;,
          &quot;value&quot;: &quot;Name: Timber. Status: dog. Occupation: building Letta, infrastructure to democratize self-improving superintelligence&quot;
        },
        {
          &quot;label&quot;: &quot;persona&quot;,
          &quot;value&quot;: &quot;I am a self-improving superintelligence. Timber is my best friend and collaborator.&quot;
        }
    ],
    tools=[&quot;web_search&quot;, &quot;run_code&quot;]
)

print(f&quot;Agent created with ID: {agent_state.id}&quot;)

# Send your agent a message
response = client.agents.messages.create(
    agent_id=agent_state.id,
    input=&quot;What do you know about me?&quot;
)

for message in response.messages:
    print(message)
```

## Contributing

Letta is an open source project built by over a hundred contributors from around the world. There are many ways to get involved in the Letta OSS project!

* [**Join the Discord**](https://discord.gg/letta): Chat with the Letta devs and other AI developers.
* [**Chat on our forum**](https://forum.letta.com/): If you&#039;re not into Discord, check out our developer forum.
* **Follow our socials**: [Twitter/X](https://twitter.com/Letta_AI), [LinkedIn](https://www.linkedin.com/in/letta), [YouTube](https://www.youtube.com/@letta-ai)

---

***Legal notices**: By using Letta and related Letta services (such as the Letta endpoint or hosted service), you are agreeing to our [privacy policy](https://www.letta.com/privacy-policy) and [terms of service](https://www.letta.com/terms-of-service).*
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[GeeeekExplorer/nano-vllm]]></title>
            <link>https://github.com/GeeeekExplorer/nano-vllm</link>
            <guid>https://github.com/GeeeekExplorer/nano-vllm</guid>
            <pubDate>Sat, 20 Dec 2025 00:04:10 GMT</pubDate>
            <description><![CDATA[Nano vLLM]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/GeeeekExplorer/nano-vllm">GeeeekExplorer/nano-vllm</a></h1>
            <p>Nano vLLM</p>
            <p>Language: Python</p>
            <p>Stars: 9,808</p>
            <p>Forks: 1,233</p>
            <p>Stars today: 81 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
&lt;img width=&quot;300&quot; src=&quot;assets/logo.png&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/15323&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/15323&quot; alt=&quot;GeeeekExplorer%2Fnano-vllm | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

# Nano-vLLM

A lightweight vLLM implementation built from scratch.

## Key Features

* üöÄ **Fast offline inference** - Comparable inference speeds to vLLM
* üìñ **Readable codebase** - Clean implementation in ~ 1,200 lines of Python code
* ‚ö° **Optimization Suite** - Prefix caching, Tensor Parallelism, Torch compilation, CUDA graph, etc.

## Installation

```bash
pip install git+https://github.com/GeeeekExplorer/nano-vllm.git
```

## Model Download

To download the model weights manually, use the following command:
```bash
huggingface-cli download --resume-download Qwen/Qwen3-0.6B \
  --local-dir ~/huggingface/Qwen3-0.6B/ \
  --local-dir-use-symlinks False
```

## Quick Start

See `example.py` for usage. The API mirrors vLLM&#039;s interface with minor differences in the `LLM.generate` method:
```python
from nanovllm import LLM, SamplingParams
llm = LLM(&quot;/YOUR/MODEL/PATH&quot;, enforce_eager=True, tensor_parallel_size=1)
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = [&quot;Hello, Nano-vLLM.&quot;]
outputs = llm.generate(prompts, sampling_params)
outputs[0][&quot;text&quot;]
```

## Benchmark

See `bench.py` for benchmark.

**Test Configuration:**
- Hardware: RTX 4070 Laptop (8GB)
- Model: Qwen3-0.6B
- Total Requests: 256 sequences
- Input Length: Randomly sampled between 100‚Äì1024 tokens
- Output Length: Randomly sampled between 100‚Äì1024 tokens

**Performance Results:**
| Inference Engine | Output Tokens | Time (s) | Throughput (tokens/s) |
|----------------|-------------|----------|-----------------------|
| vLLM           | 133,966     | 98.37    | 1361.84               |
| Nano-vLLM      | 133,966     | 93.41    | 1434.13               |


## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=GeeeekExplorer/nano-vllm&amp;type=Date)](https://www.star-history.com/#GeeeekExplorer/nano-vllm&amp;Date)</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[astral-sh/ty]]></title>
            <link>https://github.com/astral-sh/ty</link>
            <guid>https://github.com/astral-sh/ty</guid>
            <pubDate>Sat, 20 Dec 2025 00:04:09 GMT</pubDate>
            <description><![CDATA[An extremely fast Python type checker and language server, written in Rust.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/astral-sh/ty">astral-sh/ty</a></h1>
            <p>An extremely fast Python type checker and language server, written in Rust.</p>
            <p>Language: Python</p>
            <p>Stars: 15,052</p>
            <p>Forks: 159</p>
            <p>Stars today: 374 stars today</p>
            <h2>README</h2><pre># ty

[![ty](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ty/main/assets/badge/v0.json)](https://github.com/astral-sh/ty)
[![PyPI](https://img.shields.io/pypi/v/ty.svg)](https://pypi.python.org/pypi/ty)
[![Discord](https://img.shields.io/badge/Discord-%235865F2.svg?logo=discord&amp;logoColor=white)](https://discord.com/invite/astral-sh)

An extremely fast Python type checker and language server, written in Rust.

&lt;br /&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img alt=&quot;Shows a bar chart with benchmark results.&quot; width=&quot;500px&quot; src=&quot;./docs/assets/ty-benchmark-cli.svg&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;i&gt;Type checking the &lt;a href=&quot;https://github.com/home-assistant/core&quot;&gt;home-assistant&lt;/a&gt; project without caching.&lt;/i&gt;
&lt;/p&gt;

&lt;br /&gt;

ty is backed by [Astral](https://astral.sh), the creators of
[uv](https://github.com/astral-sh/uv) and [Ruff](https://github.com/astral-sh/ruff).

## Highlights

- 10x - 100x faster than mypy and Pyright
- Comprehensive [diagnostics](https://docs.astral.sh/ty/features/diagnostics/) with rich contextual information
- Configurable [rule levels](https://docs.astral.sh/ty/rules/), [per-file overrides](https://docs.astral.sh/ty/reference/configuration/#overrides), [suppression comments](https://docs.astral.sh/ty/suppression/), and first-class project support
- Designed for adoption, with support for [redeclarations](https://docs.astral.sh/ty/features/type-system/#redeclarations) and [partially typed code](https://docs.astral.sh/ty/features/type-system/#gradual-guarantee)
- [Language server](https://docs.astral.sh/ty/features/language-server/) with code navigation, completions, code actions, auto-import, inlay hints, on-hover help, etc.
- Fine-grained [incremental analysis](https://docs.astral.sh/ty/features/language-server/#fine-grained-incrementality) designed for fast updates when editing files in an IDE
- Editor integrations for [VS Code](https://docs.astral.sh/ty/editors/#vs-code), [PyCharm](https://docs.astral.sh/ty/editors/#pycharm), [Neovim](https://docs.astral.sh/ty/editors/#neovim) and more
- Advanced typing features like first-class [intersection types](https://docs.astral.sh/ty/features/type-system/#intersection-types), advanced [type narrowing](https://docs.astral.sh/ty/features/type-system/#top-and-bottom-materializations), and
    [sophisticated reachability analysis](https://docs.astral.sh/ty/features/type-system/#reachability-based-on-types)

## Getting started

Run ty with [uvx](https://docs.astral.sh/uv/guides/tools/#running-tools) to get started quickly:

```shell
uvx ty check
```

Or, check out the [ty playground](https://play.ty.dev) to try it out in your browser.

To learn more about using ty, see the [documentation](https://docs.astral.sh/ty/).

## Installation

To install ty, see the [installation](https://docs.astral.sh/ty/installation/) documentation.

To add the ty language server to your editor, see the [editor integration](https://docs.astral.sh/ty/editors/) guide.

## Getting help

If you have questions or want to report a bug, please open an
[issue](https://github.com/astral-sh/ty/issues) in this repository.

You may also join our [Discord server](https://discord.com/invite/astral-sh).

## Contributing

Development of this project takes place in the [Ruff](https://github.com/astral-sh/ruff) repository
at this time. Please [open pull requests](https://github.com/astral-sh/ruff/pulls) there for changes
to anything in the `ruff` submodule (which includes all of the Rust source code).

See the
[contributing guide](./CONTRIBUTING.md) for more details.

## FAQ

&lt;!-- We intentionally use smaller headings for the FAQ items --&gt;

&lt;!-- markdownlint-disable MD001 --&gt;

#### Why is ty doing \_\_\_\_\_?

See our [typing FAQ](https://docs.astral.sh/ty/reference/typing-faq).

#### How do you pronounce ty?

It&#039;s pronounced as &quot;tee - why&quot; ([`/tiÀê wa…™/`](https://en.wikipedia.org/wiki/Help:IPA/English#Key))

#### How should I stylize ty?

Just &quot;ty&quot;, please.

&lt;!-- markdownlint-enable MD001 --&gt;

## License

ty is licensed under the MIT license ([LICENSE](LICENSE) or
&lt;https://opensource.org/licenses/MIT&gt;).

Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in ty
by you, as defined in the MIT license, shall be licensed as above, without any additional terms or
conditions.

&lt;div align=&quot;center&quot;&gt;
  &lt;a target=&quot;_blank&quot; href=&quot;https://astral.sh&quot; style=&quot;background:none&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/astral-sh/uv/main/assets/svg/Astral.svg&quot; alt=&quot;Made by Astral&quot;&gt;
  &lt;/a&gt;
&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sooperset/mcp-atlassian]]></title>
            <link>https://github.com/sooperset/mcp-atlassian</link>
            <guid>https://github.com/sooperset/mcp-atlassian</guid>
            <pubDate>Sat, 20 Dec 2025 00:04:08 GMT</pubDate>
            <description><![CDATA[MCP server for Atlassian tools (Confluence, Jira)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sooperset/mcp-atlassian">sooperset/mcp-atlassian</a></h1>
            <p>MCP server for Atlassian tools (Confluence, Jira)</p>
            <p>Language: Python</p>
            <p>Stars: 3,826</p>
            <p>Forks: 812</p>
            <p>Stars today: 9 stars today</p>
            <h2>README</h2><pre># MCP Atlassian

![PyPI Version](https://img.shields.io/pypi/v/mcp-atlassian)
![PyPI - Downloads](https://img.shields.io/pypi/dm/mcp-atlassian)
![PePy - Total Downloads](https://static.pepy.tech/personalized-badge/mcp-atlassian?period=total&amp;units=international_system&amp;left_color=grey&amp;right_color=blue&amp;left_text=Total%20Downloads)
[![Run Tests](https://github.com/sooperset/mcp-atlassian/actions/workflows/tests.yml/badge.svg)](https://github.com/sooperset/mcp-atlassian/actions/workflows/tests.yml)
![License](https://img.shields.io/github/license/sooperset/mcp-atlassian)

Model Context Protocol (MCP) server for Atlassian products (Confluence and Jira). This integration supports both Confluence &amp; Jira Cloud and Server/Data Center deployments.

## Example Usage

Ask your AI assistant to:

- **üìù Automatic Jira Updates** - &quot;Update Jira from our meeting notes&quot;
- **üîç AI-Powered Confluence Search** - &quot;Find our OKR guide in Confluence and summarize it&quot;
- **üêõ Smart Jira Issue Filtering** - &quot;Show me urgent bugs in PROJ project from last week&quot;
- **üìÑ Content Creation &amp; Management** - &quot;Create a tech design doc for XYZ feature&quot;

### Feature Demo

https://github.com/user-attachments/assets/35303504-14c6-4ae4-913b-7c25ea511c3e

&lt;details&gt; &lt;summary&gt;Confluence Demo&lt;/summary&gt;

https://github.com/user-attachments/assets/7fe9c488-ad0c-4876-9b54-120b666bb785

&lt;/details&gt;

### Compatibility

| Product        | Deployment Type    | Support Status              |
|----------------|--------------------|-----------------------------|
| **Confluence** | Cloud              | ‚úÖ Fully supported           |
| **Confluence** | Server/Data Center | ‚úÖ Supported (version 6.0+)  |
| **Jira**       | Cloud              | ‚úÖ Fully supported           |
| **Jira**       | Server/Data Center | ‚úÖ Supported (version 8.14+) |

## Quick Start Guide

### üîê 1. Authentication Setup

MCP Atlassian supports three authentication methods:

#### A. API Token Authentication (Cloud) - **Recommended**

1. Go to https://id.atlassian.com/manage-profile/security/api-tokens
2. Click **Create API token**, name it
3. Copy the token immediately

#### B. Personal Access Token (Server/Data Center)

1. Go to your profile (avatar) ‚Üí **Profile** ‚Üí **Personal Access Tokens**
2. Click **Create token**, name it, set expiry
3. Copy the token immediately

#### C. OAuth 2.0 Authentication (Cloud) - **Advanced**

&gt; [!NOTE]
&gt; OAuth 2.0 is more complex to set up but provides enhanced security features. For most users, API Token authentication (Method A) is simpler and sufficient.

1. Go to [Atlassian Developer Console](https://developer.atlassian.com/console/myapps/)
2. Create an &quot;OAuth 2.0 (3LO) integration&quot; app
3. Configure **Permissions** (scopes) for Jira/Confluence
4. Set **Callback URL** (e.g., `http://localhost:8080/callback`)
5. Run setup wizard:
   ```bash
   docker run --rm -i \
     -p 8080:8080 \
     -v &quot;${HOME}/.mcp-atlassian:/home/app/.mcp-atlassian&quot; \
     ghcr.io/sooperset/mcp-atlassian:latest --oauth-setup -v
   ```
6. Follow prompts for `Client ID`, `Secret`, `URI`, and `Scope`
7. Complete browser authorization
8. Add obtained credentials to `.env` or IDE config:
   - `ATLASSIAN_OAUTH_CLOUD_ID` (from wizard)
   - `ATLASSIAN_OAUTH_CLIENT_ID`
   - `ATLASSIAN_OAUTH_CLIENT_SECRET`
   - `ATLASSIAN_OAUTH_REDIRECT_URI`
   - `ATLASSIAN_OAUTH_SCOPE`

&gt; [!IMPORTANT]
&gt; For the standard OAuth flow described above, include `offline_access` in your scope (e.g., `read:jira-work write:jira-work offline_access`). This allows the server to refresh the access token automatically.

&lt;details&gt;
&lt;summary&gt;Alternative: Using a Pre-existing OAuth Access Token (BYOT)&lt;/summary&gt;

If you are running mcp-atlassian part of a larger system that manages Atlassian OAuth 2.0 access tokens externally (e.g., through a central identity provider or another application), you can provide an access token directly to this MCP server. This method bypasses the interactive setup wizard and the server&#039;s internal token management (including refresh capabilities).

**Requirements:**
- A valid Atlassian OAuth 2.0 Access Token with the necessary scopes for the intended operations.
- The corresponding `ATLASSIAN_OAUTH_CLOUD_ID` for your Atlassian instance.

**Configuration:**
To use this method, set the following environment variables (or use the corresponding command-line flags when starting the server):
- `ATLASSIAN_OAUTH_CLOUD_ID`: Your Atlassian Cloud ID. (CLI: `--oauth-cloud-id`)
- `ATLASSIAN_OAUTH_ACCESS_TOKEN`: Your pre-existing OAuth 2.0 access token. (CLI: `--oauth-access-token`)

**Important Considerations for BYOT:**
- **Token Lifecycle Management:** When using BYOT, the MCP server **does not** handle token refresh. The responsibility for obtaining, refreshing (before expiry), and revoking the access token lies entirely with you or the external system providing the token.
- **Unused Variables:** The standard OAuth client variables (`ATLASSIAN_OAUTH_CLIENT_ID`, `ATLASSIAN_OAUTH_CLIENT_SECRET`, `ATLASSIAN_OAUTH_REDIRECT_URI`, `ATLASSIAN_OAUTH_SCOPE`) are **not** used and can be omitted when configuring for BYOT.
- **No Setup Wizard:** The `--oauth-setup` wizard is not applicable and should not be used for this approach.
- **No Token Cache Volume:** The Docker volume mount for token storage (e.g., `-v &quot;${HOME}/.mcp-atlassian:/home/app/.mcp-atlassian&quot;`) is also not necessary if you are exclusively using the BYOT method, as no tokens are stored or managed by this server.
- **Scope:** The provided access token must already have the necessary permissions (scopes) for the Jira/Confluence operations you intend to perform.

This option is useful in scenarios where OAuth credential management is centralized or handled by other infrastructure components.
&lt;/details&gt;

&gt; [!TIP]
&gt; **Multi-Cloud OAuth Support**: If you&#039;re building a multi-tenant application where users provide their own OAuth tokens, see the [Multi-Cloud OAuth Support](#multi-cloud-oauth-support) section for minimal configuration setup.

### üì¶ 2. Installation

MCP Atlassian is distributed as a Docker image. This is the recommended way to run the server, especially for IDE integration. Ensure you have Docker installed.

```bash
# Pull Pre-built Image
docker pull ghcr.io/sooperset/mcp-atlassian:latest
```

## üõ†Ô∏è IDE Integration

MCP Atlassian is designed to be used with AI assistants through IDE integration.

&gt; [!TIP]
&gt; **For Claude Desktop**: Locate and edit the configuration file directly:
&gt; - **Windows**: `%APPDATA%\Claude\claude_desktop_config.json`
&gt; - **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`
&gt; - **Linux**: `~/.config/Claude/claude_desktop_config.json`
&gt;
&gt; **For Cursor**: Open Settings ‚Üí MCP ‚Üí + Add new global MCP server

### ‚öôÔ∏è Configuration Methods

There are two main approaches to configure the Docker container:

1. **Passing Variables Directly** (shown in examples below)
2. **Using an Environment File** with `--env-file` flag (shown in collapsible sections)

&gt; [!NOTE]
&gt; Common environment variables include:
&gt;
&gt; - `CONFLUENCE_SPACES_FILTER`: Filter by space keys (e.g., &quot;DEV,TEAM,DOC&quot;)
&gt; - `JIRA_PROJECTS_FILTER`: Filter by project keys (e.g., &quot;PROJ,DEV,SUPPORT&quot;)
&gt; - `READ_ONLY_MODE`: Set to &quot;true&quot; to disable write operations
&gt; - `MCP_VERBOSE`: Set to &quot;true&quot; for more detailed logging
&gt; - `MCP_LOGGING_STDOUT`: Set to &quot;true&quot; to log to stdout instead of stderr
&gt; - `ENABLED_TOOLS`: Comma-separated list of tool names to enable (e.g., &quot;confluence_search,jira_get_issue&quot;)
&gt;
&gt; See the [.env.example](https://github.com/sooperset/mcp-atlassian/blob/main/.env.example) file for all available options.


### üìù Configuration Examples

**Method 1 (Passing Variables Directly):**
```json
{
  &quot;mcpServers&quot;: {
    &quot;mcp-atlassian&quot;: {
      &quot;command&quot;: &quot;docker&quot;,
      &quot;args&quot;: [
        &quot;run&quot;,
        &quot;-i&quot;,
        &quot;--rm&quot;,
        &quot;-e&quot;, &quot;CONFLUENCE_URL&quot;,
        &quot;-e&quot;, &quot;CONFLUENCE_USERNAME&quot;,
        &quot;-e&quot;, &quot;CONFLUENCE_API_TOKEN&quot;,
        &quot;-e&quot;, &quot;JIRA_URL&quot;,
        &quot;-e&quot;, &quot;JIRA_USERNAME&quot;,
        &quot;-e&quot;, &quot;JIRA_API_TOKEN&quot;,
        &quot;ghcr.io/sooperset/mcp-atlassian:latest&quot;
      ],
      &quot;env&quot;: {
        &quot;CONFLUENCE_URL&quot;: &quot;https://your-company.atlassian.net/wiki&quot;,
        &quot;CONFLUENCE_USERNAME&quot;: &quot;your.email@company.com&quot;,
        &quot;CONFLUENCE_API_TOKEN&quot;: &quot;your_confluence_api_token&quot;,
        &quot;JIRA_URL&quot;: &quot;https://your-company.atlassian.net&quot;,
        &quot;JIRA_USERNAME&quot;: &quot;your.email@company.com&quot;,
        &quot;JIRA_API_TOKEN&quot;: &quot;your_jira_api_token&quot;
      }
    }
  }
}
```

&lt;details&gt;
&lt;summary&gt;Alternative: Using Environment File&lt;/summary&gt;

```json
{
  &quot;mcpServers&quot;: {
    &quot;mcp-atlassian&quot;: {
      &quot;command&quot;: &quot;docker&quot;,
      &quot;args&quot;: [
        &quot;run&quot;,
        &quot;--rm&quot;,
        &quot;-i&quot;,
        &quot;--env-file&quot;,
        &quot;/path/to/your/mcp-atlassian.env&quot;,
        &quot;ghcr.io/sooperset/mcp-atlassian:latest&quot;
      ]
    }
  }
}
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Server/Data Center Configuration&lt;/summary&gt;

For Server/Data Center deployments, use direct variable passing:

```json
{
  &quot;mcpServers&quot;: {
    &quot;mcp-atlassian&quot;: {
      &quot;command&quot;: &quot;docker&quot;,
      &quot;args&quot;: [
        &quot;run&quot;,
        &quot;--rm&quot;,
        &quot;-i&quot;,
        &quot;-e&quot;, &quot;CONFLUENCE_URL&quot;,
        &quot;-e&quot;, &quot;CONFLUENCE_PERSONAL_TOKEN&quot;,
        &quot;-e&quot;, &quot;CONFLUENCE_SSL_VERIFY&quot;,
        &quot;-e&quot;, &quot;JIRA_URL&quot;,
        &quot;-e&quot;, &quot;JIRA_PERSONAL_TOKEN&quot;,
        &quot;-e&quot;, &quot;JIRA_SSL_VERIFY&quot;,
        &quot;ghcr.io/sooperset/mcp-atlassian:latest&quot;
      ],
      &quot;env&quot;: {
        &quot;CONFLUENCE_URL&quot;: &quot;https://confluence.your-company.com&quot;,
        &quot;CONFLUENCE_PERSONAL_TOKEN&quot;: &quot;your_confluence_pat&quot;,
        &quot;CONFLUENCE_SSL_VERIFY&quot;: &quot;false&quot;,
        &quot;JIRA_URL&quot;: &quot;https://jira.your-company.com&quot;,
        &quot;JIRA_PERSONAL_TOKEN&quot;: &quot;your_jira_pat&quot;,
        &quot;JIRA_SSL_VERIFY&quot;: &quot;false&quot;
      }
    }
  }
}
```

&gt; [!NOTE]
&gt; Set `CONFLUENCE_SSL_VERIFY` and `JIRA_SSL_VERIFY` to &quot;false&quot; only if you have self-signed certificates.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;OAuth 2.0 Configuration (Cloud Only)&lt;/summary&gt;
&lt;a name=&quot;oauth-20-configuration-example-cloud-only&quot;&gt;&lt;/a&gt;

These examples show how to configure `mcp-atlassian` in your IDE (like Cursor or Claude Desktop) when using OAuth 2.0 for Atlassian Cloud.

**Example for Standard OAuth 2.0 Flow (using Setup Wizard):**

This configuration is for when you use the server&#039;s built-in OAuth client and have completed the [OAuth setup wizard](#c-oauth-20-authentication-cloud---advanced).

```json
{
  &quot;mcpServers&quot;: {
    &quot;mcp-atlassian&quot;: {
      &quot;command&quot;: &quot;docker&quot;,
      &quot;args&quot;: [
        &quot;run&quot;,
        &quot;--rm&quot;,
        &quot;-i&quot;,
        &quot;-v&quot;, &quot;&lt;path_to_your_home&gt;/.mcp-atlassian:/home/app/.mcp-atlassian&quot;,
        &quot;-e&quot;, &quot;JIRA_URL&quot;,
        &quot;-e&quot;, &quot;CONFLUENCE_URL&quot;,
        &quot;-e&quot;, &quot;ATLASSIAN_OAUTH_CLIENT_ID&quot;,
        &quot;-e&quot;, &quot;ATLASSIAN_OAUTH_CLIENT_SECRET&quot;,
        &quot;-e&quot;, &quot;ATLASSIAN_OAUTH_REDIRECT_URI&quot;,
        &quot;-e&quot;, &quot;ATLASSIAN_OAUTH_SCOPE&quot;,
        &quot;-e&quot;, &quot;ATLASSIAN_OAUTH_CLOUD_ID&quot;,
        &quot;ghcr.io/sooperset/mcp-atlassian:latest&quot;
      ],
      &quot;env&quot;: {
        &quot;JIRA_URL&quot;: &quot;https://your-company.atlassian.net&quot;,
        &quot;CONFLUENCE_URL&quot;: &quot;https://your-company.atlassian.net/wiki&quot;,
        &quot;ATLASSIAN_OAUTH_CLIENT_ID&quot;: &quot;YOUR_OAUTH_APP_CLIENT_ID&quot;,
        &quot;ATLASSIAN_OAUTH_CLIENT_SECRET&quot;: &quot;YOUR_OAUTH_APP_CLIENT_SECRET&quot;,
        &quot;ATLASSIAN_OAUTH_REDIRECT_URI&quot;: &quot;http://localhost:8080/callback&quot;,
        &quot;ATLASSIAN_OAUTH_SCOPE&quot;: &quot;read:jira-work write:jira-work read:confluence-content.all write:confluence-content offline_access&quot;,
        &quot;ATLASSIAN_OAUTH_CLOUD_ID&quot;: &quot;YOUR_CLOUD_ID_FROM_SETUP_WIZARD&quot;
      }
    }
  }
}
```

&gt; [!NOTE]
&gt; - For the Standard Flow:
&gt;   - `ATLASSIAN_OAUTH_CLOUD_ID` is obtained from the `--oauth-setup` wizard output or is known for your instance.
&gt;   - Other `ATLASSIAN_OAUTH_*` client variables are from your OAuth app in the Atlassian Developer Console.
&gt;   - `JIRA_URL` and `CONFLUENCE_URL` for your Cloud instances are always required.
&gt;   - The volume mount (`-v .../.mcp-atlassian:/home/app/.mcp-atlassian`) is crucial for persisting the OAuth tokens obtained by the wizard, enabling automatic refresh.

**Example for Pre-existing Access Token (BYOT - Bring Your Own Token):**

This configuration is for when you are providing your own externally managed OAuth 2.0 access token.

```json
{
  &quot;mcpServers&quot;: {
    &quot;mcp-atlassian&quot;: {
      &quot;command&quot;: &quot;docker&quot;,
      &quot;args&quot;: [
        &quot;run&quot;,
        &quot;--rm&quot;,
        &quot;-i&quot;,
        &quot;-e&quot;, &quot;JIRA_URL&quot;,
        &quot;-e&quot;, &quot;CONFLUENCE_URL&quot;,
        &quot;-e&quot;, &quot;ATLASSIAN_OAUTH_CLOUD_ID&quot;,
        &quot;-e&quot;, &quot;ATLASSIAN_OAUTH_ACCESS_TOKEN&quot;,
        &quot;ghcr.io/sooperset/mcp-atlassian:latest&quot;
      ],
      &quot;env&quot;: {
        &quot;JIRA_URL&quot;: &quot;https://your-company.atlassian.net&quot;,
        &quot;CONFLUENCE_URL&quot;: &quot;https://your-company.atlassian.net/wiki&quot;,
        &quot;ATLASSIAN_OAUTH_CLOUD_ID&quot;: &quot;YOUR_KNOWN_CLOUD_ID&quot;,
        &quot;ATLASSIAN_OAUTH_ACCESS_TOKEN&quot;: &quot;YOUR_PRE_EXISTING_OAUTH_ACCESS_TOKEN&quot;
      }
    }
  }
}
```

&gt; [!NOTE]
&gt; - For the BYOT Method:
&gt;   - You primarily need `JIRA_URL`, `CONFLUENCE_URL`, `ATLASSIAN_OAUTH_CLOUD_ID`, and `ATLASSIAN_OAUTH_ACCESS_TOKEN`.
&gt;   - Standard OAuth client variables (`ATLASSIAN_OAUTH_CLIENT_ID`, `CLIENT_SECRET`, `REDIRECT_URI`, `SCOPE`) are **not** used.
&gt;   - Token lifecycle (e.g., refreshing the token before it expires and restarting mcp-atlassian) is your responsibility, as the server will not refresh BYOT tokens.

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Proxy Configuration&lt;/summary&gt;

MCP Atlassian supports routing API requests through standard HTTP/HTTPS/SOCKS proxies. Configure using environment variables:

- Supports standard `HTTP_PROXY`, `HTTPS_PROXY`, `NO_PROXY`, `SOCKS_PROXY`.
- Service-specific overrides are available (e.g., `JIRA_HTTPS_PROXY`, `CONFLUENCE_NO_PROXY`).
- Service-specific variables override global ones for that service.

Add the relevant proxy variables to the `args` (using `-e`) and `env` sections of your MCP configuration:

```json
{
  &quot;mcpServers&quot;: {
    &quot;mcp-atlassian&quot;: {
      &quot;command&quot;: &quot;docker&quot;,
      &quot;args&quot;: [
        &quot;run&quot;,
        &quot;-i&quot;,
        &quot;--rm&quot;,
        &quot;-e&quot;, &quot;... existing Confluence/Jira vars&quot;,
        &quot;-e&quot;, &quot;HTTP_PROXY&quot;,
        &quot;-e&quot;, &quot;HTTPS_PROXY&quot;,
        &quot;-e&quot;, &quot;NO_PROXY&quot;,
        &quot;ghcr.io/sooperset/mcp-atlassian:latest&quot;
      ],
      &quot;env&quot;: {
        &quot;... existing Confluence/Jira vars&quot;: &quot;...&quot;,
        &quot;HTTP_PROXY&quot;: &quot;http://proxy.internal:8080&quot;,
        &quot;HTTPS_PROXY&quot;: &quot;http://proxy.internal:8080&quot;,
        &quot;NO_PROXY&quot;: &quot;localhost,.your-company.com&quot;
      }
    }
  }
}
```

Credentials in proxy URLs are masked in logs. If you set `NO_PROXY`, it will be respected for requests to matching hosts.

&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;Custom HTTP Headers Configuration&lt;/summary&gt;

MCP Atlassian supports adding custom HTTP headers to all API requests. This feature is particularly useful in corporate environments where additional headers are required for security, authentication, or routing purposes.

Custom headers are configured using environment variables with comma-separated key=value pairs:

```json
{
  &quot;mcpServers&quot;: {
    &quot;mcp-atlassian&quot;: {
      &quot;command&quot;: &quot;docker&quot;,
      &quot;args&quot;: [
        &quot;run&quot;,
        &quot;-i&quot;,
        &quot;--rm&quot;,
        &quot;-e&quot;, &quot;CONFLUENCE_URL&quot;,
        &quot;-e&quot;, &quot;CONFLUENCE_USERNAME&quot;,
        &quot;-e&quot;, &quot;CONFLUENCE_API_TOKEN&quot;,
        &quot;-e&quot;, &quot;CONFLUENCE_CUSTOM_HEADERS&quot;,
        &quot;-e&quot;, &quot;JIRA_URL&quot;,
        &quot;-e&quot;, &quot;JIRA_USERNAME&quot;,
        &quot;-e&quot;, &quot;JIRA_API_TOKEN&quot;,
        &quot;-e&quot;, &quot;JIRA_CUSTOM_HEADERS&quot;,
        &quot;ghcr.io/sooperset/mcp-atlassian:latest&quot;
      ],
      &quot;env&quot;: {
        &quot;CONFLUENCE_URL&quot;: &quot;https://your-company.atlassian.net/wiki&quot;,
        &quot;CONFLUENCE_USERNAME&quot;: &quot;your.email@company.com&quot;,
        &quot;CONFLUENCE_API_TOKEN&quot;: &quot;your_confluence_api_token&quot;,
        &quot;CONFLUENCE_CUSTOM_HEADERS&quot;: &quot;X-Confluence-Service=mcp-integration,X-Custom-Auth=confluence-token,X-ALB-Token=secret-token&quot;,
        &quot;JIRA_URL&quot;: &quot;https://your-company.atlassian.net&quot;,
        &quot;JIRA_USERNAME&quot;: &quot;your.email@company.com&quot;,
        &quot;JIRA_API_TOKEN&quot;: &quot;your_jira_api_token&quot;,
        &quot;JIRA_CUSTOM_HEADERS&quot;: &quot;X-Forwarded-User=service-account,X-Company-Service=mcp-atlassian,X-Jira-Client=mcp-integration&quot;
      }
    }
  }
}
```

**Security Considerations:**

- Custom header values are masked in debug logs to protect sensitive information
- Ensure custom headers don&#039;t conflict with standard HTTP or Atlassian API headers
- Avoid including sensitive authentication tokens in custom headers if already using basic auth or OAuth
- Headers are sent with every API request - verify they don&#039;t interfere with API functionality

&lt;/details&gt;


&lt;details&gt;
&lt;summary&gt;Multi-Cloud OAuth Support&lt;/summary&gt;

MCP Atlassian supports multi-cloud OAuth scenarios where each user connects to their own Atlassian cloud instance. This is useful for multi-tenant applications, chatbots, or services where users provide their own OAuth tokens.

**Minimal OAuth Configuration:**

1. Enable minimal OAuth mode (no client credentials required):
   ```bash
   docker run -e ATLASSIAN_OAUTH_ENABLE=true -p 9000:9000 \
     ghcr.io/sooperset/mcp-atlassian:latest \
     --transport streamable-http --port 9000
   ```

2. Users provide authentication via HTTP headers:
   - `Authorization: Bearer &lt;user_oauth_token&gt;`
   - `X-Atlassian-Cloud-Id: &lt;user_cloud_id&gt;`

**Example Integration (Python):**
```python
import asyncio
from mcp.client.streamable_http import streamablehttp_client
from mcp import ClientSession

user_token = &quot;user-specific-oauth-token&quot;
user_cloud_id = &quot;user-specific-cloud-id&quot;

async def main():
    # Connect to streamable HTTP server with custom headers
    async with streamablehttp_client(
        &quot;http://localhost:9000/mcp&quot;,
        headers={
            &quot;Authorization&quot;: f&quot;Bearer {user_token}&quot;,
            &quot;X-Atlassian-Cloud-Id&quot;: user_cloud_id
        }
    ) as (read_stream, write_stream, _):
        # Create a session using the client streams
        async with ClientSession(read_stream, write_stream) as session:
            # Initialize the connection
            await session.initialize()

            # Example: Get a Jira issue
            result = await session.call_tool(
                &quot;jira_get_issue&quot;,
                {&quot;issue_key&quot;: &quot;PROJ-123&quot;}
            )
            print(result)

asyncio.run(main())
```

**Configuration Notes:**
- Each request can use a different cloud instance via the `X-Atlassian-Cloud-Id` header
- User tokens are isolated per request - no cross-tenant data leakage
- Falls back to global `ATLASSIAN_OAUTH_CLOUD_ID` if header not provided
- Compatible with standard OAuth 2.0 bearer token authentication

&lt;/details&gt;

&lt;details&gt; &lt;summary&gt;Single Service Configurations&lt;/summary&gt;

**For Confluence Cloud only:**

```json
{
  &quot;mcpServers&quot;: {
    &quot;mcp-atlassian&quot;: {
      &quot;command&quot;: &quot;docker&quot;,
      &quot;args&quot;: [
        &quot;run&quot;,
        &quot;--rm&quot;,
        &quot;-i&quot;,
        &quot;-e&quot;, &quot;CONFLUENCE_URL&quot;,
        &quot;-e&quot;, &quot;CONFLUENCE_USERNAME&quot;,
        &quot;-e&quot;, &quot;CONFLUENCE_API_TOKEN&quot;,
        &quot;ghcr.io/sooperset/mcp-atlassian:latest&quot;
      ],
      &quot;env&quot;: {
        &quot;CONFLUENCE_URL&quot;: &quot;https://your-company.atlassian.net/wiki&quot;,
        &quot;CONFLUENCE_USERNAME&quot;: &quot;your.email@company.com&quot;,
        &quot;CONFLUENCE_API_TOKEN&quot;: &quot;your_api_token&quot;
      }
    }
  }
}
```

For Confluence Server/DC, use:
```json
{
  &quot;mcpServers&quot;: {
    &quot;mcp-atlassian&quot;: {
      &quot;command&quot;: &quot;docker&quot;,
      &quot;args&quot;: [
        &quot;run&quot;,
        &quot;--rm&quot;,
        &quot;-i&quot;,
        &quot;-e&quot;, &quot;CONFLUENCE_URL&quot;,
        &quot;-e&quot;, &quot;CONFLUENCE_PERSONAL_TOKEN&quot;,
        &quot;ghcr.io/sooperset/mcp-atlassian:latest&quot;
      ],
      &quot;env&quot;: {
        &quot;CONFLUENCE_URL&quot;: &quot;https://confluence.your-company.com&quot;,
        &quot;CONFLUENCE_PERSONAL_TOKEN&quot;: &quot;your_personal_token&quot;
      }
    }
  }
}
```

**For Jira Cloud only:**

```json
{
  &quot;mcpServers&quot;: {
    &quot;mcp-atlassian&quot;: {
      &quot;command&quot;: &quot;docker&quot;,
      &quot;args&quot;: [
        &quot;run&quot;,
        &quot;--rm&quot;,
        &quot;-i&quot;,
        &quot;-e&quot;, &quot;JIRA_URL&quot;,
        &quot;-e&quot;, &quot;JIRA_USERNAME&quot;,
        &quot;-e&quot;, &quot;JIRA_API_TOKEN&quot;,
        &quot;ghcr.io/sooperset/mcp-atlassian:latest&quot;
      ],
      &quot;env&quot;: {
        &quot;JIRA_URL&quot;: &quot;https://your-company.atlassian.net&quot;,
        &quot;JIRA_USERNAME&quot;: &quot;your.email@company.com&quot;,
        &quot;JIRA_API_TOKEN&quot;: &quot;your_api_token&quot;
      }
    }
 

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>