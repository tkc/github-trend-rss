<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sat, 16 Aug 2025 00:04:22 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[tadata-org/fastapi_mcp]]></title>
            <link>https://github.com/tadata-org/fastapi_mcp</link>
            <guid>https://github.com/tadata-org/fastapi_mcp</guid>
            <pubDate>Sat, 16 Aug 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/tadata-org/fastapi_mcp">tadata-org/fastapi_mcp</a></h1>
            <p>Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!</p>
            <p>Language: Python</p>
            <p>Stars: 8,692</p>
            <p>Forks: 680</p>
            <p>Stars today: 222 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/tadata-org/fastapi_mcp&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/7e44e98b-a0ba-4aff-a68a-4ffee3a6189c&quot; alt=&quot;fastapi-to-mcp&quot; height=100/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;span style=&quot;font-size: 0.85em; font-weight: normal;&quot;&gt;Built by &lt;a href=&quot;https://tadata.com&quot;&gt;Tadata&lt;/a&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;h1 align=&quot;center&quot;&gt;
  FastAPI-MCP
&lt;/h1&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/14064&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14064&quot; alt=&quot;tadata-org%2Ffastapi_mcp | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;

[![PyPI version](https://img.shields.io/pypi/v/fastapi-mcp?color=%2334D058&amp;label=pypi%20package)](https://pypi.org/project/fastapi-mcp/)
[![Python Versions](https://img.shields.io/pypi/pyversions/fastapi-mcp.svg)](https://pypi.org/project/fastapi-mcp/)
[![FastAPI](https://img.shields.io/badge/FastAPI-009485.svg?logo=fastapi&amp;logoColor=white)](#)
[![CI](https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml/badge.svg)](https://github.com/tadata-org/fastapi_mcp/actions/workflows/ci.yml)
[![Coverage](https://codecov.io/gh/tadata-org/fastapi_mcp/branch/main/graph/badge.svg)](https://codecov.io/gh/tadata-org/fastapi_mcp)

&lt;/div&gt;


&lt;p align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/tadata-org/fastapi_mcp&quot;&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/b205adc6-28c0-4e3c-a68b-9c1a80eb7d0c&quot; alt=&quot;fastapi-mcp-usage&quot; height=&quot;400&quot;/&gt;&lt;/a&gt;&lt;/p&gt;


## Features

- **Authentication** built in, using your existing FastAPI dependencies!

- **FastAPI-native:** Not just another OpenAPI -&gt; MCP converter

- **Zero/Minimal configuration** required - just point it at your FastAPI app and it works

- **Preserving schemas** of your request models and response models

- **Preserve documentation** of all your endpoints, just as it is in Swagger

- **Flexible deployment** - Mount your MCP server to the same app, or deploy separately

- **ASGI transport** - Uses FastAPI&#039;s ASGI interface directly for efficient communication


## Hosted Solution

If you prefer a managed hosted solution check out [tadata.com](https://tadata.com).

## Installation

We recommend using [uv](https://docs.astral.sh/uv/), a fast Python package installer:

```bash
uv add fastapi-mcp
```

Alternatively, you can install with pip:

```bash
pip install fastapi-mcp
```

## Basic Usage

The simplest way to use FastAPI-MCP is to add an MCP server directly to your FastAPI application:

```python
from fastapi import FastAPI
from fastapi_mcp import FastApiMCP

app = FastAPI()

mcp = FastApiMCP(app)

# Mount the MCP server directly to your FastAPI app
mcp.mount()
```

That&#039;s it! Your auto-generated MCP server is now available at `https://app.base.url/mcp`.

## Documentation, Examples and Advanced Usage

FastAPI-MCP provides [comprehensive documentation](https://fastapi-mcp.tadata.com/). Additionaly, check out the [examples directory](examples) for code samples demonstrating these features in action.

## FastAPI-first Approach

FastAPI-MCP is designed as a native extension of FastAPI, not just a converter that generates MCP tools from your API. This approach offers several key advantages:

- **Native dependencies**: Secure your MCP endpoints using familiar FastAPI `Depends()` for authentication and authorization

- **ASGI transport**: Communicates directly with your FastAPI app using its ASGI interface, eliminating the need for HTTP calls from the MCP to your API

- **Unified infrastructure**: Your FastAPI app doesn&#039;t need to run separately from the MCP server (though [separate deployment](https://fastapi-mcp.tadata.com/advanced/deploy#deploying-separately-from-original-fastapi-app) is also supported)

This design philosophy ensures minimum friction when adding MCP capabilities to your existing FastAPI services.


## Development and Contributing

Thank you for considering contributing to FastAPI-MCP! We encourage the community to post Issues and create Pull Requests.

Before you get started, please see our [Contribution Guide](CONTRIBUTING.md).

## Community

Join [MCParty Slack community](https://join.slack.com/t/themcparty/shared_invite/zt-30yxr1zdi-2FG~XjBA0xIgYSYuKe7~Xg) to connect with other MCP enthusiasts, ask questions, and share your experiences with FastAPI-MCP.

## Requirements

- Python 3.10+ (Recommended 3.12)
- uv

## License

MIT License. Copyright (c) 2025 Tadata Inc.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[budtmo/docker-android]]></title>
            <link>https://github.com/budtmo/docker-android</link>
            <guid>https://github.com/budtmo/docker-android</guid>
            <pubDate>Sat, 16 Aug 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[Android in docker solution with noVNC supported and video recording]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/budtmo/docker-android">budtmo/docker-android</a></h1>
            <p>Android in docker solution with noVNC supported and video recording</p>
            <p>Language: Python</p>
            <p>Stars: 12,045</p>
            <p>Forks: 1,440</p>
            <p>Stars today: 221 stars today</p>
            <h2>README</h2><pre>
&lt;p align=&quot;center&quot;&gt;
  &lt;img id=&quot;header&quot; src=&quot;./images/logo_docker-android.png&quot; /&gt;
&lt;/p&gt;

[![Paypal Donate](https://img.shields.io/badge/paypal-donate-blue.svg)](http://paypal.me/budtmo) [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com) [![codecov](https://codecov.io/gh/budtmo/docker-android/branch/master/graph/badge.svg)](https://codecov.io/gh/budtmo/docker-android) [![Join the chat at https://gitter.im/budtmo/docker-android](https://badges.gitter.im/budtmo/docker-android.svg)](https://gitter.im/budtmo/docker-android?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge) [![GitHub release](https://img.shields.io/github/release/budtmo/docker-android.svg)](https://github.com/budtmo/docker-android/releases)

Docker-Android is a docker image built to be used for everything related to Android. It can be used for Application development and testing (native, web and hybrid-app).

Advantages of using this project
--------------------------------
1. Emulator with different device profile and skins, such as Samsung Galaxy S6, LG Nexus 4, HTC Nexus One and more.
2. Support vnc to be able to see what happen inside docker container
3. Support log sharing feature where all logs can be accessed from web-UI
4. Ability to control emulator from outside container by using adb connect
5. Integrated with other cloud solutions, e.g. [Genymotion Cloud](https://www.genymotion.com/cloud/)
6. It can be used to build Android project
7. It can be used to run unit and UI-Test with different test-frameworks, e.g. Appium, Espresso, etc.

List of Docker-Images
---------------------
|Android   |API   |Image with latest release version   |Image with specific release version|
|:---|:---|:---|:---|
|9.0|28|budtmo/docker-android:emulator_9.0|budtmo/docker-android:emulator_9.0_&lt;release_version&gt;|
|10.0|29|budtmo/docker-android:emulator_10.0|budtmo/docker-android:emulator_10.0_&lt;release_version&gt;|
|11.0|30|budtmo/docker-android:emulator_11.0|budtmo/docker-android:emulator_11.0_&lt;release_version&gt;|
|12.0|32|budtmo/docker-android:emulator_12.0|budtmo/docker-android:emulator_12.0_&lt;release_version&gt;|
|13.0|33|budtmo/docker-android:emulator_13.0|budtmo/docker-android:emulator_13.0_&lt;release_version&gt;|
|14.0|34|budtmo/docker-android:emulator_14.0|budtmo/docker-android:emulator_14.0_&lt;release_version&gt;|
|-|-|budtmo/docker-android:genymotion|budtmo/docker-android:genymotion_&lt;release_version&gt;|

List of Devices
---------------

Type   | Device Name
-----  | -----
Phone  | Samsung Galaxy S10
Phone  | Samsung Galaxy S9
Phone  | Samsung Galaxy S8
Phone  | Samsung Galaxy S7 Edge
Phone  | Samsung Galaxy S7
Phone  | Samsung Galaxy S6
Phone  | Nexus 4
Phone  | Nexus 5
Phone  | Nexus One
Phone  | Nexus S
Tablet | Nexus 7
Tablet | Pixel C

Requirements
------------

1. Docker is installed on your system.

Quick Start
-----------

1. If you use ***Ubuntu OS*** on your host machine, you can skip this step. For ***OSX*** and ***Windows OS*** user, you need to use Virtual Machine that support Virtualization with Ubuntu OS because the image can be run under ***Ubuntu OS only***.

2. Your machine should support virtualization. To check if the virtualization is enabled is:
    ```
    sudo apt install cpu-checker
    kvm-ok
    ```

3. Run Docker-Android container
    ```
    docker run -d -p 6080:6080 -e EMULATOR_DEVICE=&quot;Samsung Galaxy S10&quot; -e WEB_VNC=true --device /dev/kvm --name android-container budtmo/docker-android:emulator_11.0
    ```

4. Open ***http://localhost:6080*** to see inside running container.

5. To check the status of the emulator
    ```
    docker exec -it android-container cat device_status
    ```

Persisting data
-----------

The default behaviour is to destroy the emulated device on container restart. To persist data, you need to mount a volume at `/home/androidusr`:
    ```
    docker run -v data:/home/androidusr budtmo/docker-android:emulator_11.0
    ```

WSL2 Hardware acceleration (Windows 11 only)
-----------

Credit goes to [Guillaume - The Parallel Interface blog](https://www.paralint.com/2022/11/find-new-modified-and-unversioned-subversion-files-on-windows)

[Microsoft - Advanced settings configuration in WSL](https://learn.microsoft.com/en-us/windows/wsl/wsl-config)


1. Add yourself to the `kvm` usergroup.
    ```
    sudo usermod -a -G kvm ${USER}
    ```

2. Add necessary flags to `/etc/wsl2.conf` to their respective sections.
    ```
    [boot]
    command = /bin/bash -c &#039;chown -v root:kvm /dev/kvm &amp;&amp; chmod 660 /dev/kvm&#039;

    [wsl2]
    nestedVirtualization=true
    ```
3. Restart WSL2 via CMD prompt or Powershell
    ```
    wsl --shutdown
    ```


`command = /bin/bash -c &#039;chown -v root:kvm /dev/kvm &amp;&amp; chmod 660 /dev/kvm&#039;` sets `/dev/kvm` to `kvm` usergroup rather than the default `root` usergroup on WSL2 startup.

`nestedVirtualization` flag is only available to Windows 11.

Use-Cases
---------

1. [Build Android project](./documentations/USE_CASE_BUILD_ANDROID_PROJECT.md)
2. [UI-Test with Appium](./documentations/USE_CASE_APPIUM.md)
3. [Control Android emulator on host machine](./documentations/USE_CASE_CONTROL_EMULATOR.md)
4. [SMS Simulation](./documentations/USE_CASE_SMS.md)
5. [Jenkins](./documentations/USE_CASE_JENKINS.md)
6. [Deploying on cloud (Azure, AWS, GCP)](./documentations/USE_CASE_CLOUD.md)

Custom-Configurations
---------------------

This [document](./documentations/CUSTOM_CONFIGURATIONS.md) contains information about configurations that can be used to enable some features, e.g. log-sharing, etc.

Genymotion
----------

&lt;p align=&quot;center&quot;&gt;
  &lt;img id=&quot;geny&quot; src=&quot;./images/logo_genymotion_and_dockerandroid.png&quot; /&gt;
&lt;/p&gt;

For you who do not have ressources to maintain the simulator or to buy machines or need different device profiles, you can give a try by using [Genymotion SAAS](https://cloud.geny.io/). Docker-Android is [integrated with Genymotion](https://www.genymotion.com/blog/partner_tag/docker/) on different cloud services, e.g. Genymotion SAAS, AWS, GCP, Alibaba Cloud. Please follow [this document](./documentations/THIRD_PARTY_GENYMOTION.md) for more detail.

Emulator Skins
--------------
The Emulator skins are taken from [Android Studio IDE](https://developer.android.com/studio) and [Samsung Developer Website](https://developer.samsung.com/)

USERS
-----

&lt;a href=&quot;https://lookerstudio.google.com/s/iGaemHJqQvg&quot;&gt;
  &lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;./images/docker-android_users.png&quot; alt=&quot;docker-android-users&quot; width=&quot;800&quot; height=&quot;600&quot;&gt;
  &lt;/p&gt;
&lt;/a&gt;

PRO VERSION
-----------

Due to high requests for help and to be able to actively maintain the projects, the creator has decided to create docker-android-pro. Docker-Android-Pro is a sponsor based project which mean that the docker image of pro-version can be pulled only by [active sponsor](https://github.com/sponsors/budtmo).

The differences between normal version and pro version are:

|Feature   |Normal   |Pro   |Comment|
|:---|:---|:---|:---|
|user-behavior-analytics|Yes|No|-|
|proxy|No|Yes|Set up company proxy on Android emulator on fly|
|language|No|Yes|Set up language on Android emulator on fly|
|Newer Android version|No|Yes|Support other newer Android version e.g. Android 15, Android 16, etc|
|root-privileged|No|Yes|Able to run command with security privileged|
|headless-mode|No|Yes|Save resources by using headless mode|
|Selenium 4.x integration|No|Yes|Running Appium UI-Tests againt one (Selenium Hub) endpoint for Android- and iOS emulator(s) / device(s)|
|multiple Android-Simulators|No|Yes (soon)|Save resources by having multiple Android-Simulators on one docker-container|
|Google Play Store|No|Yes (soon)|-|
|Video Recording|No|Yes (soon)|Helpful for debugging|

This [document](./documentations/DOCKER-ANDROID-PRO.md) contains detail information about how to use docker-android-pro.

LICENSE
-------
See [License](LICENSE.md)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[manycore-research/SpatialLM]]></title>
            <link>https://github.com/manycore-research/SpatialLM</link>
            <guid>https://github.com/manycore-research/SpatialLM</guid>
            <pubDate>Sat, 16 Aug 2025 00:04:20 GMT</pubDate>
            <description><![CDATA[SpatialLM: Training Large Language Models for Structured Indoor Modeling]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/manycore-research/SpatialLM">manycore-research/SpatialLM</a></h1>
            <p>SpatialLM: Training Large Language Models for Structured Indoor Modeling</p>
            <p>Language: Python</p>
            <p>Stars: 3,703</p>
            <p>Forks: 284</p>
            <p>Stars today: 44 stars today</p>
            <h2>README</h2><pre># SpatialLM

&lt;!-- markdownlint-disable first-line-h1 --&gt;
&lt;!-- markdownlint-disable html --&gt;
&lt;!-- markdownlint-disable no-duplicate-header --&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;figures/logo_light.png#gh-light-mode-only&quot; width=&quot;60%&quot; alt=&quot;SpatialLM&quot; /&gt;
  &lt;img src=&quot;figures/logo_dark.png#gh-dark-mode-only&quot; width=&quot;60%&quot; alt=&quot;SpatialLM&quot; /&gt;
&lt;/div&gt;
&lt;hr style=&quot;margin-top: 0; margin-bottom: 8px;&quot;&gt;
&lt;div align=&quot;center&quot; style=&quot;margin-top: 0; padding-top: 0; line-height: 1;&quot;&gt;
    &lt;a href=&quot;https://manycore-research.github.io/SpatialLM&quot; target=&quot;_blank&quot; style=&quot;margin: 2px;&quot;&gt;&lt;img alt=&quot;Project&quot;
    src=&quot;https://img.shields.io/badge/üåê%20Website-SpatialLM-ffc107?color=42a5f5&amp;logoColor=white&quot; style=&quot;display: inline-block; vertical-align: middle;&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2506.07491&quot; target=&quot;_blank&quot; style=&quot;margin: 2px;&quot;&gt;&lt;img alt=&quot;arXiv&quot;
    src=&quot;https://img.shields.io/badge/arXiv-Techreport-b31b1b?logo=arxiv&amp;logoColor=white&quot; style=&quot;display: inline-block; vertical-align: middle;&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://github.com/manycore-research/SpatialLM&quot; target=&quot;_blank&quot; style=&quot;margin: 2px;&quot;&gt;&lt;img alt=&quot;GitHub&quot;
    src=&quot;https://img.shields.io/badge/GitHub-SpatialLM-24292e?logo=github&amp;logoColor=white&quot; style=&quot;display: inline-block; vertical-align: middle;&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;div align=&quot;center&quot; style=&quot;line-height: 1;&quot;&gt;
    &lt;a href=&quot;https://huggingface.co/manycore-research/SpatialLM1.1-Qwen-0.5B&quot; target=&quot;_blank&quot; style=&quot;margin: 2px;&quot;&gt;&lt;img alt=&quot;Hugging Face&quot;
    src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-SpatialLM-ffc107?color=ffc107&amp;logoColor=white&quot; style=&quot;display: inline-block; vertical-align: middle;&quot;/&gt;&lt;/a&gt;
    &lt;a href=&quot;https://huggingface.co/datasets/manycore-research/SpatialLM-Testset&quot; target=&quot;_blank&quot; style=&quot;margin: 2px;&quot;&gt;&lt;img alt=&quot;Dataset&quot;
    src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Dataset-Testset-ffc107?color=ffc107&amp;logoColor=white&quot; style=&quot;display: inline-block; vertical-align: middle;&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

## ‚ú® News

- [Jun, 2025] Check out our new models: [SpatialLM1.1-Llama-1B](https://huggingface.co/manycore-research/SpatialLM1.1-Llama-1B) and [SpatialLM1.1-Qwen-0.5B](https://huggingface.co/manycore-research/SpatialLM1.1-Qwen-0.5B), now available on Hugging Face. SpatialLM1.1 doubles the point cloud resolution, incorporates a more powerful point cloud encoder [Sonata](https://xywu.me/sonata/) and supports detection with user-specified categories.
- [Jun, 2025] SpatialLM [Technical Report](https://arxiv.org/abs/2506.07491) is now on arXiv.
- [Mar, 2025] We&#039;re excited to release the [SpatialLM-Llama-1B](https://huggingface.co/manycore-research/SpatialLM-Llama-1B) and [SpatialLM-Qwen-0.5B](https://huggingface.co/manycore-research/SpatialLM-Qwen-0.5B) on Hugging Face.
- [Mar, 2025] Initial release of SpatialLM!

## Introduction

SpatialLM is a 3D large language model designed to process 3D point cloud data and generate structured 3D scene understanding outputs. These outputs include architectural elements like walls, doors, windows, and oriented object bounding boxes with their semantic categories. Unlike previous methods that require specialized equipment for data collection, SpatialLM can handle point clouds from diverse sources such as monocular video sequences, RGBD images, and LiDAR sensors. This multimodal architecture effectively bridges the gap between unstructured 3D geometric data and structured 3D representations, offering high-level semantic understanding. It enhances spatial reasoning capabilities for applications in embodied robotics, autonomous navigation, and other complex 3D scene analysis tasks.

&lt;div align=&quot;center&quot;&gt;
  &lt;video src=&quot;https://github.com/user-attachments/assets/c0218d6a-f676-41f8-ae76-bba228866306&quot; poster=&quot;figures/cover.png&quot;&gt; &lt;/video&gt;
  &lt;p&gt;&lt;i&gt;SpatialLM reconstructs 3D layout from a monocular RGB video with MASt3R-SLAM. Results aligned to video with GT cameras for visualization.&lt;/i&gt;&lt;/p&gt;
&lt;/div&gt;

## SpatialLM Models

&lt;div align=&quot;center&quot;&gt;

|       **Model**        | **Download**                                                                      |
| :--------------------: | --------------------------------------------------------------------------------- |
| SpatialLM1.1-Llama-1B  | [ü§ó HuggingFace](https://huggingface.co/manycore-research/SpatialLM1.1-Llama-1B)  |
| SpatialLM1.1-Qwen-0.5B | [ü§ó HuggingFace](https://huggingface.co/manycore-research/SpatialLM1.1-Qwen-0.5B) |
| SpatialLM1.0-Llama-1B  | [ü§ó HuggingFace](https://huggingface.co/manycore-research/SpatialLM-Llama-1B)     |
| SpatialLM1.0-Qwen-0.5B | [ü§ó HuggingFace](https://huggingface.co/manycore-research/SpatialLM-Qwen-0.5B)    |

&lt;/div&gt;

## Usage

### Installation

Tested with the following environment:

- Python 3.11
- Pytorch 2.4.1
- CUDA Version 12.4

```bash
# clone the repository
git clone https://github.com/manycore-research/SpatialLM.git
cd SpatialLM

# create a conda environment with cuda 12.4
conda create -n spatiallm python=3.11
conda activate spatiallm
conda install -y -c nvidia/label/cuda-12.4.0 cuda-toolkit conda-forge::sparsehash

# Install dependencies with poetry
pip install poetry &amp;&amp; poetry config virtualenvs.create false --local
poetry install
# SpatialLM1.0 dependency
poe install-torchsparse # Building wheel for torchsparse will take a while
# SpatialLM1.1 dependency
poe install-sonata # Building wheel for flash-attn will take a while
```

### Inference

In the current version of SpatialLM, input point clouds are considered axis-aligned where the z-axis is the up axis. This orientation is crucial for maintaining consistency in spatial understanding and scene interpretation across different datasets and applications.
Example preprocessed point clouds, reconstructed from RGB videos using [MASt3R-SLAM](https://github.com/rmurai0610/MASt3R-SLAM), are available in [SpatialLM-Testset](#spatiallm-testset).

Download an example point cloud:

```bash
huggingface-cli download manycore-research/SpatialLM-Testset pcd/scene0000_00.ply --repo-type dataset --local-dir .
```

Run inference:

```bash
python inference.py --point_cloud pcd/scene0000_00.ply --output scene0000_00.txt --model_path manycore-research/SpatialLM1.1-Qwen-0.5B
```

### Detection with user-specified categories

SpatialLM1.1 supports object detection conditioned on user-specified categories by leveraging the flexibility of LLMs.

SpatialLM1.1 offers three variants of structured indoor modeling tasks:

- **Structured Reconstruction**: Detect walls, doors, windows, boxes.
- **Layout Estimation**: Detect walls, doors, windows.
- **3D Object Detection**: Detect boxes.

For tasks that include object box estimation, you can specify a subset of the 59 furniture categories, and the model will only predict objects within those specified categories. For example:

```bash
python inference.py --point_cloud pcd/scene0000_00.ply --output scene0000_00.txt --model_path manycore-research/SpatialLM1.1-Qwen-0.5B --detect_type object --category bed nightstand
```

### Visualization

Use `rerun` to visualize the point cloud and the predicted structured 3D layout output:

```bash
# Convert the predicted layout to Rerun format
python visualize.py --point_cloud pcd/scene0000_00.ply --layout scene0000_00.txt --save scene0000_00.rrd

# Visualize the point cloud and the predicted layout
rerun scene0000_00.rrd
```

### Evaluation

To evaluate the performance of SpatialLM, we provide `eval.py` script that reports the benchmark results on the SpatialLM-Testset in the table below in section [Benchmark Results](#benchmark-results).

Download the testset:

```bash
huggingface-cli download manycore-research/SpatialLM-Testset --repo-type dataset --local-dir SpatialLM-Testset
```

Run evaluation:

```bash
# Run inference on the PLY point clouds in folder SpatialLM-Testset/pcd with SpatialLM1.1-Qwen-0.5B model
python inference.py --point_cloud SpatialLM-Testset/pcd --output SpatialLM-Testset/pred --model_path manycore-research/SpatialLM1.1-Qwen-0.5B

# Evaluate the predicted layouts
python eval.py --metadata SpatialLM-Testset/test.csv --gt_dir SpatialLM-Testset/layout --pred_dir SpatialLM-Testset/pred --label_mapping SpatialLM-Testset/benchmark_categories.tsv
```

### Example using a custom video

We provide an example of how to use our model to estimate scene layout starting from a RGB video with the newly released [SLAM3R](https://github.com/PKU-VCL-3DV/SLAM3R) in [EXAMPLE.md](EXAMPLE.md). These steps work for MASt3R-SLAM, and other reconstruction methods as well.

## SpatialLM Testset

We provide a test set of 107 preprocessed point clouds, reconstructed from RGB videos using [MASt3R-SLAM](https://github.com/rmurai0610/MASt3R-SLAM). SpatialLM-Testset is quite challenging compared to prior clean RGBD scans datasets due to the noises and occlusions in the point clouds reconstructed from monocular RGB videos.

&lt;div align=&quot;center&quot;&gt;

|    **Dataset**    | **Download**                                                                       |
| :---------------: | ---------------------------------------------------------------------------------- |
| SpatialLM-Testset | [ü§ó Datasets](https://huggingface.co/datasets/manycore-research/SpatialLM-TestSet) |

&lt;/div&gt;

## Benchmark Results

### Layout Estimation

Layout estimation focuses on predicting architectural elements, i.e., walls, doors, and windows, within an indoor scene. We evaluated this task on the [Structured3D](https://structured3d-dataset.org) dataset. For [RoomFormer](https://github.com/ywyue/RoomFormer), we directly downloaded the model checkpoint. SceneScript and SpatialLM were first trained on our dataset, and further fine-tuned on Structured3D.

&lt;div align=&quot;center&quot;&gt;

|   **Method**    | **RoomFormer** | **SceneScript (finetuned)** | **SpatialLM1.1-Qwen-0.5B (finetuned)** |
| :-------------: | :------------: | :-------------------------: | :------------------------------------: |
| **F1 @.25 IoU** |      70.4      |            83.1             |                  86.5                  |
| **F1 @.5 IoU**  |      67.2      |            80.8             |                  84.6                  |

&lt;/div&gt;

### 3D Object Detection

We evaluate 3D object detection on [ScanNet](http://www.scan-net.org) with annotations of 18 object categories. For [V-DETR](https://github.com/V-DETR/V-DETR), we directly download the model checkpoint. SceneScript and SpatialLM were first trained on our dataset, and further fine-tuned on ScanNet.

&lt;div align=&quot;center&quot;&gt;

|   **Method**    | **V-DETR** | **SceneScript (finetuned)** | **SpatialLM1.1-Qwen-0.5B (finetuned)** |
| :-------------: | :--------: | :-------------------------: | :------------------------------------: |
| **F1 @.25 IoU** |    65.1    |            49.1             |                  65.6                  |
| **F1 @.5 IoU**  |    56.8    |            36.8             |                  52.6                  |

&lt;/div&gt;

### Zero-shot Detection on Videos

Zero-shot detection results on the challenging SpatialLM-Testset are reported in the following table:

&lt;div align=&quot;center&quot;&gt;

|   **Method**    | **SpatialLM1.1-Llama-1B** | **SpatialLM1.1-Qwen-0.5B** |
| :-------------: | :-----------------------: | :------------------------: |
|   **Layout**    |   **F1 @.25 IoU (2D)**    |    **F1 @.25 IoU (2D)**    |
|      wall       |           68.9            |            68.2            |
|      door       |           46.3            |            43.1            |
|     window      |           43.8            |            47.4            |
|                 |                           |                            |
|   **Objects**   |   **F1 @.25 IoU (3D)**    |    **F1 @.25 IoU (2D)**    |
|     curtain     |           34.9            |            37.0            |
|   nightstand    |           62.8            |            67.0            |
|   chandelier    |           53.5            |            36.8            |
|    wardrobe     |           29.4            |            39.6            |
|       bed       |           96.8            |            95.2            |
|      sofa       |           66.9            |            69.1            |
|      chair      |           20.8            |            32.3            |
|     cabinet     |           15.2            |            11.2            |
|  dining table   |           40.7            |            24.2            |
|     plants      |           29.5            |            26.3            |
|   tv cabinet    |           34.4            |            27.3            |
|  coffee table   |           56.4            |            64.9            |
|   side table    |           14.6            |            9.7             |
| air conditioner |           16.7            |            24.0            |
|     dresser     |           46.7            |            46.7            |
|      stool      |           17.6            |            30.8            |
|  refrigerator   |            0.0            |            16.7            |
|    painting     |           34.9            |            38.2            |
|     carpet      |           40.3            |            24.1            |
|       tv        |           16.0            |            18.0            |

&lt;/div&gt;

### Result Visualizations

&lt;div align=&quot;center&quot;&gt;

|                                                            Layout Estimation                                                            |                                                          Object Detection                                                          |                                                       Zero-shot Reconstruction                                                        |
| :-------------------------------------------------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------------------------------------: |
|                                                  ![Structured3D](./figures/stru3d.jpg)                                                  |                                                 ![ScanNet](./figures/scannet.jpg)                                                  |                                                 ![Zero-shot](./figures/zeroshot.jpg)                                                  |
| [Structured3D Results](https://manycore-research-azure.kujiale.com/manycore-research/SpatialLM/supplementary/visualization_layout.html) | [ScanNet Results](https://manycore-research-azure.kujiale.com/manycore-research/SpatialLM/supplementary/visualization_object.html) | [Zeroshot Results](https://manycore-research-azure.kujiale.com/manycore-research/SpatialLM/supplementary/visualization_zeroshot.html) |

&lt;/div&gt;

## License

SpatialLM-Llama-1B is derived from Llama3.2-1B-Instruct, which is licensed under the Llama3.2 license.
SpatialLM-Qwen-0.5B is derived from the Qwen-2.5 series, originally licensed under the Apache 2.0 License.

SpatialLM1.0 are built upon the SceneScript point cloud encoder, licensed under the CC-BY-NC-4.0 License. TorchSparse, utilized in this project, is licensed under the MIT License.

SpatialLM1.1 are built upon Sonata point cloud encoder, model weight is licensed under the CC-BY-NC-4.0 License. Code built on Pointcept is licensed under the Apache 2.0 License.

## Citation

If you find this work useful, please consider citing:

```bibtex
@article{SpatialLM,
    title         = {SpatialLM: Training Large Language Models for Structured Indoor Modeling},
    author        = {Mao, Yongsen and Zhong, Junhao and Fang, Chuan and Zheng, Jia and Tang, Rui and Zhu, Hao and Tan, Ping and Zhou, Zihan},
    journal       = {arXiv preprint},
    year          = {2025},
    eprint        = {2506.07491},
    archivePrefix = {arXiv},
    primaryClass  = {cs.CV}
}
```

## Acknowledgements

We would like to thank the following projects that made this work possible:

[Llama3.2](https://github.com/meta-llama) | [Qwen2.5](https://github.com/QwenLM/Qwen2.5) | [Transformers](https://github.com/huggingface/transformers) | [SceneScript](https://github.com/facebookresearch/scenescript) | [TorchSparse](https://github.com/mit-han-lab/torchsparse) | [Sonata](https://xywu.me/sonata/) | [Pointcept](https://github.com/Pointcept/Pointcept)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/magentic-ui]]></title>
            <link>https://github.com/microsoft/magentic-ui</link>
            <guid>https://github.com/microsoft/magentic-ui</guid>
            <pubDate>Sat, 16 Aug 2025 00:04:19 GMT</pubDate>
            <description><![CDATA[A research prototype of a human-centered web agent]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/magentic-ui">microsoft/magentic-ui</a></h1>
            <p>A research prototype of a human-centered web agent</p>
            <p>Language: Python</p>
            <p>Stars: 7,238</p>
            <p>Forks: 739</p>
            <p>Stars today: 105 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;docs/img/magui-readme-logo.svg&quot; alt=&quot;Magentic-UI Logo&quot;&gt;


_Automate your web tasks while you stay in control_

[![image](https://img.shields.io/pypi/v/magentic_ui.svg)](https://pypi.python.org/pypi/magentic_ui)
[![image](https://img.shields.io/pypi/l/magentic_ui.svg)](https://pypi.python.org/pypi/magentic_ui)
![Python Versions](https://img.shields.io/badge/python-3.10%20%7C%203.11%20%7C%203.12%20%7C%203.13-blue)

&lt;/div&gt;

---

Magentic-UI is a **research prototype** of a human-centered interface powered by a multi-agent system that can browse and perform actions on the web, generate and execute code, and generate and analyze files.

  https://github.com/user-attachments/assets/7975fc26-1a18-4acb-8bf9-321171eeade7

## üöÄ Quick Start

Here&#039;s how you can get started with Magentic-UI:

```bash
# 1. Setup environment
python3 -m venv .venv
source .venv/bin/activate
pip install magentic-ui --upgrade

# 2. Set your API key
export OPENAI_API_KEY=&quot;your-api-key-here&quot;

# 3. Launch Magentic-UI
magentic-ui --port 8081
```

Then open &lt;http://localhost:8081&gt; in your browser to interact with Magentic-UI!

&gt; **Prerequisites**: Requires Docker and Python 3.10+. Windows users should use WSL2. See [detailed installation](#Ô∏è-installation) for more info.

## ‚ú® What&#039;s New

- **File Upload Support**: Upload any file through the UI for analysis or modification
- **MCP Agents**: Extend capabilities with your favorite MCP servers
- **Easier Installation**: We have uploaded our docker containers to GHCR so you no longer need to build any containers! Installation time now is much quicker.

## Alternative Usage Options

**Without Docker** (limited functionality: no code execution):
```bash
magentic-ui --run-without-docker --port 8081
```

**Command Line Interface**:
```bash
magentic-cli --work-dir PATH/TO/STORE/DATA
```

**Custom LLM Clients**:
```bash
# Azure
pip install magentic-ui[azure]

# Ollama (local models)
pip install magentic-ui[ollama]
```

You can then pass a config file to the `magentic-ui` command (&lt;a href=&quot;#model-client-configuration&quot;&gt; client config&lt;/a&gt;) or change the model client inside the UI settings.

For further details on installation please read the   &lt;a href=&quot;#Ô∏è-installation&quot;&gt;üõ†Ô∏è Installation&lt;/a&gt; section. For common installation issues and their solutions, please refer to the [troubleshooting document](TROUBLESHOOTING.md). See advanced usage instructions with the command `magentic-ui --help`. 


## Quick Navigation:
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;#-how-it-works&quot;&gt;üü™ How it Works&lt;/a&gt; &amp;nbsp;|&amp;nbsp;
  &lt;a href=&quot;#Ô∏è-installation&quot;&gt;üõ†Ô∏è Installation&lt;/a&gt; &amp;nbsp;|&amp;nbsp;
  &lt;a href=&quot;#troubleshooting&quot;&gt;‚ö†Ô∏è Troubleshooting&lt;/a&gt; &amp;nbsp;|&amp;nbsp; 
  &lt;a href=&quot;#contributing&quot;&gt;ü§ù Contributing&lt;/a&gt; &amp;nbsp;|&amp;nbsp;
  &lt;a href=&quot;#license&quot;&gt;üìÑ License&lt;/a&gt;
&lt;/p&gt;

---

## üü™ How it Works
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./docs/img/magenticui_running.png&quot; alt=&quot;Magentic-UI&quot; height=&quot;400&quot;&gt;
&lt;/p&gt;

Magentic-UI is especially useful for web tasks that require actions on the web (e.g., filling a form, customizing a food order), deep navigation through websites not indexed by search engines (e.g., filtering flights, finding a link from a personal site) or tasks that need web navigation and code execution (e.g., generate a chart from online data).

The interface of Magentic-UI is displayed in the screenshot above and consists of two panels. The left side panel is the sessions navigator where users can create new sessions to solve new tasks, switch between sessions and check on session progress with the session status indicators (üî¥ needs input, ‚úÖ task done, ‚Ü∫ task in progress).

The right-side panel displays the session selected. This is where you can type your query to Magentic-UI alongside any file attachments and observe detailed task progress as well as  interact with the agents. The session display itself is split in two panels: the left side is where Magentic-UI presents the plan, task progress and asks for action approvals, the right side is a browser view where you can see web agent actions in real time and interact with the browser. Finally, at the top of the session display is a progress bar that updates as Magentic-UI makes progress.


The example below shows a step by step user interaction with Magentic-UI:

&lt;!-- Screenshots --&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;docs/img/magui-landing.png&quot; alt=&quot;Magentic-UI Landing&quot; width=&quot;45%&quot; style=&quot;margin:10px;&quot;&gt;
  &lt;img src=&quot;docs/img/magui-coplanning.png&quot; alt=&quot;Co-Planning UI&quot; width=&quot;45%&quot; style=&quot;margin:10px;&quot;&gt;
  &lt;img src=&quot;docs/img/magui-cotasking.png&quot; alt=&quot;Co-Tasking UI&quot; width=&quot;45%&quot; style=&quot;margin:10px;&quot;&gt;
  &lt;img src=&quot;docs/img/magui-actionguard.png&quot; alt=&quot;Action Guard UI&quot; width=&quot;45%&quot; style=&quot;margin:10px;&quot;&gt;
&lt;/p&gt;


What differentiates Magentic-UI from other browser use offerings is its transparent and controllable interface that allows for efficient human-in-the-loop involvement. Magentic-UI is built using [AutoGen](https://github.com/microsoft/autogen) and provides a platform to study human-agent interaction and experiment with web agents. Key features include:

- üßë‚Äçü§ù‚Äçüßë **Co-Planning**: Collaboratively create and approve step-by-step plans using chat and the plan editor.
- ü§ù **Co-Tasking**: Interrupt and guide the task execution using the web browser directly or through chat. Magentic-UI can also ask for clarifications and help when needed.
- üõ°Ô∏è **Action Guards**: Sensitive actions are only executed with explicit user approvals.
- üß† **Plan Learning and Retrieval**: Learn from previous runs to improve future task automation and save them in a plan gallery. Automatically or manually retrieve saved plans in future tasks.
- üîÄ **Parallel Task Execution**: You can run multiple tasks in parallel and session status indicators will let you know when Magentic-UI needs your input or has completed the task.

&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=wOs-5SR8xOc&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.youtube.com/vi/wOs-5SR8xOc/maxresdefault.jpg&quot; alt=&quot;Watch the demo video&quot; width=&quot;600&quot;/&gt;
  &lt;/a&gt;
  &lt;br&gt;
  ‚ñ∂Ô∏è &lt;em&gt; Click to watch a video and learn more about Magentic-UI &lt;/em&gt;
&lt;/div&gt;


### Autonomous Evaluation

To evaluate its autonomous capabilities, Magentic-UI has been tested against several benchmarks when running with o4-mini: [GAIA](https://huggingface.co/datasets/gaia-benchmark/GAIA) test set (42.52%), which assesses general AI assistants across reasoning, tool use, and web interaction tasks ; [AssistantBench](https://huggingface.co/AssistantBench) test set (27.60%), focusing on realistic, time-consuming web tasks; [WebVoyager](https://github.com/MinorJerry/WebVoyager) (82.2%), measuring end-to-end web navigation in real-world scenarios; and [WebGames](https://webgames.convergence.ai/) (45.5%), evaluating general-purpose web-browsing agents through interactive challenges.
To reproduce these experimental results, please see the following [instructions](experiments/eval/README.md).



If you&#039;re interested in reading more checkout our [technical report](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/07/magentic-ui-report.pdf) and [blog post](https://www.microsoft.com/en-us/research/blog/magentic-ui-an-experimental-human-centered-web-agent/).

## üõ†Ô∏è Installation
### Pre-Requisites

**Note**: If you&#039;re using Windows, we highly recommend using [WSL2](https://docs.microsoft.com/en-us/windows/wsl/install) (Windows Subsystem for Linux).

1. If running on **Windows** or **Mac** you should use [Docker Desktop](https://www.docker.com/products/docker-desktop/) or if inside WSL2 you can install Docker directly inside WSL [docker in WSL2 guide](https://gist.github.com/dehsilvadeveloper/c3bdf0f4cdcc5c177e2fe9be671820c7). If running on **Linux**, you should use [Docker Engine](https://docs.docker.com/engine/install/). 

If using Docker Desktop, make sure it is set up to use WSL2:
    - Go to Settings &gt; Resources &gt; WSL Integration
    - Enable integration with your development distro You can find more detailed instructions about this step [here](https://docs.microsoft.com/en-us/windows/wsl/tutorials/wsl-containers).



2. During the Installation step, you will need to set up your `OPENAI_API_KEY`. To use other models, review the [Model Client Configuration](#model-client-configuration) section below.

3. You need at least [Python 3.10](https://www.python.org/downloads/) installed.


If you are on Windows, we recommend to run Magentic-UI inside [WSL2](https://docs.microsoft.com/en-us/windows/wsl/install) (Windows Subsystem for Linux) for correct Docker and file path compatibility.



### PyPI Installation

Magentic-UI is available on PyPI. We recommend using a virtual environment to avoid conflicts with other packages.

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install magentic-ui
```

Alternatively, if you use [`uv`](https://docs.astral.sh/uv/getting-started/installation/) for dependency management, you can install Magentic-UI with:

```bash
uv venv --python=3.12 .venv
. .venv/bin/activate
uv pip install magentic-ui
```


### Running Magentic-UI

To run Magentic-UI, make sure that Docker is running, then run the following command:

```bash
magentic-ui --port 8081
```

&gt;**Note**: Running this command for the first time will pull two docker images required for the Magentic-UI agents. If you encounter problems, you can build them directly with the following command:
```bash
cd docker
sh build-all.sh
```

If you face issues with Docker, please refer to the [TROUBLESHOOTING.md](TROUBLESHOOTING.md) document.

Once the server is running, you can access the UI at &lt;http://localhost:8081&gt;.


### Configuration

#### Model Client Configuration

If you want to use a different OpenAI key, or if you want to configure use with Azure OpenAI or Ollama, you can do so inside the UI by navigating to settings (top right icon) and changing model configuration. Another option is to pass a yaml config file when you start Magentic-UI which will override any settings in the UI:

```bash
magentic-ui --port 8081 --config config.yaml
```

Where the `config.yaml` should look as follows with an AutoGen model client configuration:

```yaml
gpt4o_client: &amp;gpt4o_client
    provider: OpenAIChatCompletionClient
    config:
      model: gpt-4o-2024-08-06
      api_key: null
      base_url: null
      max_retries: 5

orchestrator_client: *gpt4o_client
coder_client: *gpt4o_client
web_surfer_client: *gpt4o_client
file_surfer_client: *gpt4o_client
action_guard_client: *gpt4o_client
plan_learning_client: *gpt4o_client
```
You can change the client for each of the agents using the config file and use AzureOpenAI (`AzureOpenAIChatCompletionClient`), Ollama and other clients.

#### MCP Server Configuration

You can also extend Magentic-UI&#039;s capabilities by adding custom &quot;McpAgents&quot; to the multi-agent team. Each McpAgent can have access to one or more MCP Servers. You can specify these agents via the `mcp_agent_configs` parameter in your `config.yaml`.

For example, here&#039;s an agent called &quot;airbnb_surfer&quot; that has access to the OpenBnb MCP Server running locally via Stdio.

```yaml
mcp_agent_configs:
  - name: airbnb_surfer
    description: &quot;The airbnb_surfer has direct access to AirBnB.&quot;
    model_client: 
      provider: OpenAIChatCompletionClient
      config:
        model: gpt-4.1-2025-04-14
      max_retries: 10
    system_message: |-
      You are AirBnb Surfer, a helpful digital assistant that can help users acces AirBnB.

      You have access to a suite of tools provided by the AirBnB API. Use those tools to satisfy the users requests.
    reflect_on_tool_use: false
    mcp_servers:
      - server_name: AirBnB
        server_params:
          type: StdioServerParams
          command: npx
          args:
            - -y
            - &quot;@openbnb/mcp-server-airbnb&quot;
            - --ignore-robots-txt
```

Under the hood, each `McpAgent` is just a `autogen_agentchat.agents.AssistantAgent` with the set of MCP Servers exposed as an `AggregateMcpWorkbench` which is simply a named collection of `autogen_ext.tools.mcp.McpWorkbench` objects (one per MCP Server).

Currently the supported MCP Server types are `autogen_ext.tools.mcp.StdioServerParams` and `autogen_ext.tools.mcp.SseServerParams`.

### Building Magentic-UI from source

This step is primarily for users seeking to make modifications to the code, are having trouble with the pypi installation or want the latest code before a pypi version release.

#### 1. Make sure the above prerequisites are installed, and that Docker is running.

#### 2. Clone the repository to your local machine:

```bash
git clone https://github.com/microsoft/magentic-ui.git
cd magentic-ui
```

#### 3. Install Magentic-UI&#039;s dependencies with uv or your favorite package manager:

```bash
# install uv through https://docs.astral.sh/uv/getting-started/installation/
uv venv --python=3.12 .venv
uv sync --all-extras
source .venv/bin/activate
```

#### 4. Build the frontend:

First make sure to install node:

```bash
# install nvm to install node
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.1/install.sh | bash
nvm install node
```

Then install the frontend:

```bash
cd frontend
npm install -g gatsby-cli
npm install --global yarn
yarn install
yarn build
```

#### 5. Run Magentic-UI, as usual.

```bash
magentic-ui --port 8081
```


#### Running the UI from source

If you are making changes to the source code of the UI, you can run the frontend in development mode so that it will automatically update when you make changes for faster development.

1. Open a separate terminal and change directory to the frontend

```bash
cd frontend
```

2. Create a `.env.development` file.

```bash
cp .env.default .env.development
```

3. Launch frontend server

```bash
npm run start
```

4. Then run the UI:

```bash
magentic-ui --port 8081
```

The frontend from source will be available at &lt;http://localhost:8000&gt;, and the compiled frontend will be available at &lt;http://localhost:8081&gt;.




## Troubleshooting


If you were unable to get Magentic-UI running, do not worry! The first step is to make sure you have followed the steps outlined above, particularly with the [pre-requisites](#pre-requisites).

For common issues and their solutions, please refer to the [TROUBLESHOOTING.md](TROUBLESHOOTING.md) file in this repository. If you do not see your problem there, please open a `GitHub Issue`. 

## Contributing

This project welcomes contributions and suggestions. For information about contributing to Magentic-UI, please see our [CONTRIBUTING.md](CONTRIBUTING.md) guide, which includes current issues to be resolved and other forms of contributing.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information, see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## License

Microsoft, and any contributors, grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT). See the [LICENSE](LICENSE) file.

Microsoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation
may be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.
The licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.
Microsoft&#039;s general trademark guidelines can be found at &lt;http://go.microsoft.com/fwlink/?LinkID=254653&gt;.

Any use of third-party trademarks or logos are subject to those third-party&#039;s policies.

Privacy information can be found at &lt;https://go.microsoft.com/fwlink/?LinkId=521839&gt;

Microsoft and any contributors reserve all other rights, whether under their respective copyrights, patents, or trademarks, whether by implication, estoppel, or otherwise.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[datalab-to/marker]]></title>
            <link>https://github.com/datalab-to/marker</link>
            <guid>https://github.com/datalab-to/marker</guid>
            <pubDate>Sat, 16 Aug 2025 00:04:18 GMT</pubDate>
            <description><![CDATA[Convert PDF to markdown + JSON quickly with high accuracy]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/datalab-to/marker">datalab-to/marker</a></h1>
            <p>Convert PDF to markdown + JSON quickly with high accuracy</p>
            <p>Language: Python</p>
            <p>Stars: 27,722</p>
            <p>Forks: 1,816</p>
            <p>Stars today: 108 stars today</p>
            <h2>README</h2><pre># Marker

Marker converts documents to markdown, JSON, chunks, and HTML quickly and accurately.

- Converts PDF, image, PPTX, DOCX, XLSX, HTML, EPUB files in all languages
- Formats tables, forms, equations, inline math, links, references, and code blocks
- Extracts and saves images
- Removes headers/footers/other artifacts
- Extensible with your own formatting and logic
- Does structured extraction, given a JSON schema (beta)
- Optionally boost accuracy with LLMs (and your own prompt)
- Works on GPU, CPU, or MPS

## Performance

&lt;img src=&quot;data/images/overall.png&quot; width=&quot;800px&quot;/&gt;

Marker benchmarks favorably compared to cloud services like Llamaparse and Mathpix, as well as other open source tools.

The above results are running single PDF pages serially.  Marker is significantly faster when running in batch mode, with a projected throughput of 25 pages/second on an H100.

See [below](#benchmarks) for detailed speed and accuracy benchmarks, and instructions on how to run your own benchmarks.

## Hybrid Mode

For the highest accuracy, pass the `--use_llm` flag to use an LLM alongside marker.  This will do things like merge tables across pages, handle inline math, format tables properly, and extract values from forms.  It can use any gemini or ollama model.  By default, it uses `gemini-2.0-flash`.  See [below](#llm-services) for details.

Here is a table benchmark comparing marker, gemini flash alone, and marker with use_llm:

&lt;img src=&quot;data/images/table.png&quot; width=&quot;400px&quot;/&gt;

As you can see, the use_llm mode offers higher accuracy than marker or gemini alone.

## Examples

| PDF | File type | Markdown                                                                                                                     | JSON                                                                                                   |
|-----|-----------|------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|
| [Think Python](https://greenteapress.com/thinkpython/thinkpython.pdf) | Textbook | [View](https://github.com/VikParuchuri/marker/blob/master/data/examples/markdown/thinkpython/thinkpython.md)                 | [View](https://github.com/VikParuchuri/marker/blob/master/data/examples/json/thinkpython.json)         |
| [Switch Transformers](https://arxiv.org/pdf/2101.03961.pdf) | arXiv paper | [View](https://github.com/VikParuchuri/marker/blob/master/data/examples/markdown/switch_transformers/switch_trans.md) | [View](https://github.com/VikParuchuri/marker/blob/master/data/examples/json/switch_trans.json) |
| [Multi-column CNN](https://arxiv.org/pdf/1804.07821.pdf) | arXiv paper | [View](https://github.com/VikParuchuri/marker/blob/master/data/examples/markdown/multicolcnn/multicolcnn.md)                 | [View](https://github.com/VikParuchuri/marker/blob/master/data/examples/json/multicolcnn.json)         |

# Commercial usage

I want marker to be as widely accessible as possible, while still funding my development/training costs.  Research and personal usage is always okay, but there are some restrictions on commercial usage.

The weights for the models are licensed `cc-by-nc-sa-4.0`, but I will waive that for any organization under \$2M USD in gross revenue in the most recent 12-month period AND under \$2M in lifetime VC/angel funding raised. You also must not be competitive with the [Datalab API](https://www.datalab.to/).  If you want to remove the GPL license requirements (dual-license) and/or use the weights commercially over the revenue limit, check out the options [here](https://www.datalab.to).

# Hosted API

There&#039;s a hosted API for marker available [here](https://www.datalab.to/):

- Supports PDF, image, PPT, PPTX, DOC, DOCX, XLS, XLSX, HTML, EPUB files
- 1/4th the price of leading cloud-based competitors
- Fast - ~15s for a 250 page PDF
- Supports LLM mode
- High uptime (99.99%)

# Community

[Discord](https://discord.gg//KuZwXNGnfH) is where we discuss future development.

# Installation

You&#039;ll need python 3.10+ and [PyTorch](https://pytorch.org/get-started/locally/).

Install with:

```shell
pip install marker-pdf
```

If you want to use marker on documents other than PDFs, you will need to install additional dependencies with:

```shell
pip install marker-pdf[full]
```

# Usage

First, some configuration:

- Your torch device will be automatically detected, but you can override this.  For example, `TORCH_DEVICE=cuda`.
- Some PDFs, even digital ones, have bad text in them.  Set `--force_ocr` to force OCR on all lines, or the `strip_existing_ocr` to keep all digital text, and strip out any existing OCR text.
- If you care about inline math, set `force_ocr` to convert inline math to LaTeX.

## Interactive App

I&#039;ve included a streamlit app that lets you interactively try marker with some basic options.  Run it with:

```shell
pip install streamlit streamlit-ace
marker_gui
```

## Convert a single file

```shell
marker_single /path/to/file.pdf
```

You can pass in PDFs or images.

Options:
- `--page_range TEXT`: Specify which pages to process. Accepts comma-separated page numbers and ranges. Example: `--page_range &quot;0,5-10,20&quot;` will process pages 0, 5 through 10, and page 20.
- `--output_format [markdown|json|html|chunks]`: Specify the format for the output results.
- `--output_dir PATH`: Directory where output files will be saved. Defaults to the value specified in settings.OUTPUT_DIR.
- `--paginate_output`: Paginates the output, using `\n\n{PAGE_NUMBER}` followed by `-` * 48, then `\n\n` 
- `--use_llm`: Uses an LLM to improve accuracy.  You will need to configure the LLM backend - see [below](#llm-services).
- `--force_ocr`: Force OCR processing on the entire document, even for pages that might contain extractable text.  This will also format inline math properly.
- `--block_correction_prompt`: if LLM mode is active, an optional prompt that will be used to correct the output of marker.  This is useful for custom formatting or logic that you want to apply to the output.
- `--strip_existing_ocr`: Remove all existing OCR text in the document and re-OCR with surya.
- `--redo_inline_math`: If you want the absolute highest quality inline math conversion, use this along with `--use_llm`.
- `--disable_image_extraction`: Don&#039;t extract images from the PDF.  If you also specify `--use_llm`, then images will be replaced with a description.
- `--debug`: Enable debug mode for additional logging and diagnostic information.
- `--processors TEXT`: Override the default processors by providing their full module paths, separated by commas. Example: `--processors &quot;module1.processor1,module2.processor2&quot;`
- `--config_json PATH`: Path to a JSON configuration file containing additional settings.
- `config --help`: List all available builders, processors, and converters, and their associated configuration.  These values can be used to build a JSON configuration file for additional tweaking of marker defaults.
- `--converter_cls`: One of `marker.converters.pdf.PdfConverter` (default) or `marker.converters.table.TableConverter`.  The `PdfConverter` will convert the whole PDF, the `TableConverter` will only extract and convert tables.
- `--llm_service`: Which llm service to use if `--use_llm` is passed.  This defaults to `marker.services.gemini.GoogleGeminiService`.
- `--help`: see all of the flags that can be passed into marker.  (it supports many more options then are listed above)

The list of supported languages for surya OCR is [here](https://github.com/VikParuchuri/surya/blob/master/surya/recognition/languages.py).  If you don&#039;t need OCR, marker can work with any language.

## Convert multiple files

```shell
marker /path/to/input/folder
```

- `marker` supports all the same options from `marker_single` above.
- `--workers` is the number of conversion workers to run simultaneously.  This is automatically set by default, but you can increase it to increase throughput, at the cost of more CPU/GPU usage.  Marker will use 5GB of VRAM per worker at the peak, and 3.5GB average.

## Convert multiple files on multiple GPUs

```shell
NUM_DEVICES=4 NUM_WORKERS=15 marker_chunk_convert ../pdf_in ../md_out
```

- `NUM_DEVICES` is the number of GPUs to use.  Should be `2` or greater.
- `NUM_WORKERS` is the number of parallel processes to run on each GPU.

## Use from python

See the `PdfConverter` class at `marker/converters/pdf.py` function for additional arguments that can be passed.

```python
from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.output import text_from_rendered

converter = PdfConverter(
    artifact_dict=create_model_dict(),
)
rendered = converter(&quot;FILEPATH&quot;)
text, _, images = text_from_rendered(rendered)
```

`rendered` will be a pydantic basemodel with different properties depending on the output type requested.  With markdown output (default), you&#039;ll have the properties `markdown`, `metadata`, and `images`.  For json output, you&#039;ll have `children`, `block_type`, and `metadata`.

### Custom configuration

You can pass configuration using the `ConfigParser`.  To see all available options, do `marker_single --help`.

```python
from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.config.parser import ConfigParser

config = {
    &quot;output_format&quot;: &quot;json&quot;,
    &quot;ADDITIONAL_KEY&quot;: &quot;VALUE&quot;
}
config_parser = ConfigParser(config)

converter = PdfConverter(
    config=config_parser.generate_config_dict(),
    artifact_dict=create_model_dict(),
    processor_list=config_parser.get_processors(),
    renderer=config_parser.get_renderer(),
    llm_service=config_parser.get_llm_service()
)
rendered = converter(&quot;FILEPATH&quot;)
```

### Extract blocks

Each document consists of one or more pages.  Pages contain blocks, which can themselves contain other blocks.  It&#039;s possible to programmatically manipulate these blocks.  

Here&#039;s an example of extracting all forms from a document:

```python
from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.schema import BlockTypes

converter = PdfConverter(
    artifact_dict=create_model_dict(),
)
document = converter.build_document(&quot;FILEPATH&quot;)
forms = document.contained_blocks((BlockTypes.Form,))
```

Look at the processors for more examples of extracting and manipulating blocks.

## Other converters

You can also use other converters that define different conversion pipelines:

### Extract tables

The `TableConverter` will only convert and extract tables:

```python
from marker.converters.table import TableConverter
from marker.models import create_model_dict
from marker.output import text_from_rendered

converter = TableConverter(
    artifact_dict=create_model_dict(),
)
rendered = converter(&quot;FILEPATH&quot;)
text, _, images = text_from_rendered(rendered)
```

This takes all the same configuration as the PdfConverter.  You can specify the configuration `force_layout_block=Table` to avoid layout detection and instead assume every page is a table.  Set `output_format=json` to also get cell bounding boxes.

You can also run this via the CLI with 
```shell
marker_single FILENAME --use_llm --force_layout_block Table --converter_cls marker.converters.table.TableConverter --output_format json
```

### OCR Only

If you only want to run OCR, you can also do that through the `OCRConverter`.  Set `--keep_chars` to keep individual characters and bounding boxes.

```python
from marker.converters.ocr import OCRConverter
from marker.models import create_model_dict

converter = OCRConverter(
    artifact_dict=create_model_dict(),
)
rendered = converter(&quot;FILEPATH&quot;)
```

This takes all the same configuration as the PdfConverter.

You can also run this via the CLI with 
```shell
marker_single FILENAME --converter_cls marker.converters.ocr.OCRConverter
```

### Structured Extraction (beta)

You can run structured extraction via the `ExtractionConverter`.  This requires an llm service to be setup first (see [here](#llm-services) for details).  You&#039;ll get a JSON output with the extracted values.

```python
from marker.converters.extraction import ExtractionConverter
from marker.models import create_model_dict
from marker.config.parser import ConfigParser
from pydantic import BaseModel

class Links(BaseModel):
    links: list[str]
    
schema = Links.model_json_schema()
config_parser = ConfigParser({
    &quot;page_schema&quot;: schema
})

converter = ExtractionConverter(
    artifact_dict=create_model_dict(),
    config=config_parser.generate_config_dict(),
    llm_service=config_parser.get_llm_service(),
)
rendered = converter(&quot;FILEPATH&quot;)
```

Rendered will have an `original_markdown` field.  If you pass this back in next time you run the converter, as the `existing_markdown` config key, you can skip re-parsing the document.

# Output Formats

## Markdown

Markdown output will include:

- image links (images will be saved in the same folder)
- formatted tables
- embedded LaTeX equations (fenced with `$$`)
- Code is fenced with triple backticks
- Superscripts for footnotes

## HTML

HTML output is similar to markdown output:

- Images are included via `img` tags
- equations are fenced with `&lt;math&gt;` tags
- code is in `pre` tags

## JSON

JSON output will be organized in a tree-like structure, with the leaf nodes being blocks.  Examples of leaf nodes are a single list item, a paragraph of text, or an image.

The output will be a list, with each list item representing a page.  Each page is considered a block in the internal marker schema.  There are different types of blocks to represent different elements.  

Pages have the keys:

- `id` - unique id for the block.
- `block_type` - the type of block. The possible block types can be seen in `marker/schema/__init__.py`.  As of this writing, they are [&quot;Line&quot;, &quot;Span&quot;, &quot;FigureGroup&quot;, &quot;TableGroup&quot;, &quot;ListGroup&quot;, &quot;PictureGroup&quot;, &quot;Page&quot;, &quot;Caption&quot;, &quot;Code&quot;, &quot;Figure&quot;, &quot;Footnote&quot;, &quot;Form&quot;, &quot;Equation&quot;, &quot;Handwriting&quot;, &quot;TextInlineMath&quot;, &quot;ListItem&quot;, &quot;PageFooter&quot;, &quot;PageHeader&quot;, &quot;Picture&quot;, &quot;SectionHeader&quot;, &quot;Table&quot;, &quot;Text&quot;, &quot;TableOfContents&quot;, &quot;Document&quot;]
- `html` - the HTML for the page.  Note that this will have recursive references to children.  The `content-ref` tags must be replaced with the child content if you want the full html.  You can see an example of this at `marker/output.py:json_to_html`.  That function will take in a single block from the json output, and turn it into HTML.
- `polygon` - the 4-corner polygon of the page, in (x1,y1), (x2,y2), (x3, y3), (x4, y4) format.  (x1,y1) is the top left, and coordinates go clockwise.
- `children` - the child blocks.

The child blocks have two additional keys:

- `section_hierarchy` - indicates the sections that the block is part of.  `1` indicates an h1 tag, `2` an h2, and so on.
- `images` - base64 encoded images.  The key will be the block id, and the data will be the encoded image.

Note that child blocks of pages can have their own children as well (a tree structure).

```json
{
      &quot;id&quot;: &quot;/page/10/Page/366&quot;,
      &quot;block_type&quot;: &quot;Page&quot;,
      &quot;html&quot;: &quot;&lt;content-ref src=&#039;/page/10/SectionHeader/0&#039;&gt;&lt;/content-ref&gt;&lt;content-ref src=&#039;/page/10/SectionHeader/1&#039;&gt;&lt;/content-ref&gt;&lt;content-ref src=&#039;/page/10/Text/2&#039;&gt;&lt;/content-ref&gt;&lt;content-ref src=&#039;/page/10/Text/3&#039;&gt;&lt;/content-ref&gt;&lt;content-ref src=&#039;/page/10/Figure/4&#039;&gt;&lt;/content-ref&gt;&lt;content-ref src=&#039;/page/10/SectionHeader/5&#039;&gt;&lt;/content-ref&gt;&lt;content-ref src=&#039;/page/10/SectionHeader/6&#039;&gt;&lt;/content-ref&gt;&lt;content-ref src=&#039;/page/10/TextInlineMath/7&#039;&gt;&lt;/content-ref&gt;&lt;content-ref src=&#039;/page/10/TextInlineMath/8&#039;&gt;&lt;/content-ref&gt;&lt;content-ref src=&#039;/page/10/Table/9&#039;&gt;&lt;/content-ref&gt;&lt;content-ref src=&#039;/page/10/SectionHeader/10&#039;&gt;&lt;/content-ref&gt;&lt;content-ref src=&#039;/page/10/Text/11&#039;&gt;&lt;/content-ref&gt;&quot;,
      &quot;polygon&quot;: [[0.0, 0.0], [612.0, 0.0], [612.0, 792.0], [0.0, 792.0]],
      &quot;children&quot;: [
        {
          &quot;id&quot;: &quot;/page/10/SectionHeader/0&quot;,
          &quot;block_type&quot;: &quot;SectionHeader&quot;,
          &quot;html&quot;: &quot;&lt;h1&gt;Supplementary Material for &lt;i&gt;Subspace Adversarial Training&lt;/i&gt; &lt;/h1&gt;&quot;,
          &quot;polygon&quot;: [
            [217.845703125, 80.630859375], [374.73046875, 80.630859375],
            [374.73046875, 107.0],
            [217.845703125, 107.0]
          ],
          &quot;children&quot;: null,
          &quot;section_hierarchy&quot;: {
            &quot;1&quot;: &quot;/page/10/SectionHeader/1&quot;
          },
          &quot;images&quot;: {}
        },
        ...
        ]
    }


```

## Chunks

Chunks format is similar to JSON, but flattens everything into a single list instead of a tree.  Only the top level blocks from each page show up. It also has the full HTML of each block inside, so you don&#039;t need to crawl the tree to reconstruct it.  This enable flexible and easy chunking for RAG.

## Metadata

All output formats will return a metadata dictionary, with the following fields:

```json
{
    &quot;table_of_contents&quot;: [
      {
        &quot;title&quot;: &quot;Introduction&quot;,
        &quot;heading_level&quot;: 1,
        &quot;page_id&quot;: 0,
        &quot;polygon&quot;: [...]
      }
    ], // computed PDF table of contents
    &quot;page_stats&quot;: [
      {
        &quot;page_id&quot;:  0, 
        &quot;text_extraction_method&quot;: &quot;pdftext&quot;,
        &quot;block_counts&quot;: [(&quot;Span&quot;, 200), ...]
      },
      ...
    ]
}
```

# LLM Services

When running with the `--use_llm` flag, you have a choice of services you can use:

- `Gemini` - this will use the Gemini developer API by default.  You&#039;ll need to pass `--gemini_api_key` to configuration.
- `Google Vertex` - this will use vertex, which can be more reliable.  You&#039;ll need to pass `--vertex_project_id`.  To use it, set `--llm_service=marker.services.vertex.GoogleVertexService`.
- `Ollama` - this will use local models.  You can configure `--ollama_base_url` and `--ollama_model`. To use it, set `--llm_service=marker.services.ollama.OllamaService`.
- `Claude` - this will use the anthropic API.  You can configure `--claude_api_key`, and `--claude_model_name`.  To use it, set `--llm_service=marker.services.claude.ClaudeService`.
- `OpenAI` - this supports any openai-like endpoint. You can configure `--openai_api_key`, `--openai_model`, and `--openai_base_url`. To use it, set `--llm_service=marker.services.openai.OpenAIService`.
- `Azure OpenAI` - this uses the Azure OpenAI service. You can configure `--azure_endpoint`, `--azure_api_key`, and `--deployment_name`. To use it, set `--llm_service=marker.services.azure_openai.AzureOpenAIService`.

These services may have additional optional configuration as well - you can see it by viewing the classes.

# Internals

Marker is easy to extend.  The core units of marker are:

- `Providers`, at `marker/providers`.  These provide information from a source file, like a PDF.
- `Builders`, at `marker/builders`.  These generate the initial document blocks and fill in text, using info from the providers.
- `Processors`, at `marker/processors`.  These process specific blocks, for example the table formatter is a processor.
- `Renderers`, at `marker/renderers`. These use the blocks to render output.
- `Schema`, at `marker/schema`.  The classes for all the block types.
- `Converters`, at `marker/converters`.  They run the whole end to end pipeline.

To customize processing behavior, override the `processors`.  To add new output formats, write a new `renderer`.  For additional input formats, write a new `provider.`

Processors and renderers can be directly passed into the base `PDFConverter`, so you can specify your own custom processing easily.

## API server

There is a very simple API server you can run like this:

```shell
pip install -U uvicorn fastapi python-multipart
marker_server --port 8001
```

This will start a fastapi server that you can access at `localhost:8001`.  You can go to `localhost:8001/docs` to see the endpoint options.

You can send requests like this:

```
import requests
import json

post_data = {
    &#039;filepath&#039;: &#039;FILEPATH&#039;,
    # Add other params here
}

reque

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[microsoft/markitdown]]></title>
            <link>https://github.com/microsoft/markitdown</link>
            <guid>https://github.com/microsoft/markitdown</guid>
            <pubDate>Sat, 16 Aug 2025 00:04:17 GMT</pubDate>
            <description><![CDATA[Python tool for converting files and office documents to Markdown.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/markitdown">microsoft/markitdown</a></h1>
            <p>Python tool for converting files and office documents to Markdown.</p>
            <p>Language: Python</p>
            <p>Stars: 71,706</p>
            <p>Forks: 3,910</p>
            <p>Stars today: 327 stars today</p>
            <h2>README</h2><pre># MarkItDown

[![PyPI](https://img.shields.io/pypi/v/markitdown.svg)](https://pypi.org/project/markitdown/)
![PyPI - Downloads](https://img.shields.io/pypi/dd/markitdown)
[![Built by AutoGen Team](https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue)](https://github.com/microsoft/autogen)

&gt; [!TIP]
&gt; MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See [markitdown-mcp](https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp) for more information.

&gt; [!IMPORTANT]
&gt; Breaking changes between 0.0.1 to 0.1.0:
&gt; * Dependencies are now organized into optional feature-groups (further details below). Use `pip install &#039;markitdown[all]&#039;` to have backward-compatible behavior. 
&gt; * convert\_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.
&gt; * The DocumentConverter class interface has changed to read from file-like streams rather than file paths. *No temporary files are created anymore*. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.

MarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to [textract](https://github.com/deanmalmgren/textract), but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.

MarkItDown currently supports the conversion from:

- PDF
- PowerPoint
- Word
- Excel
- Images (EXIF metadata and OCR)
- Audio (EXIF metadata and speech transcription)
- HTML
- Text-based formats (CSV, JSON, XML)
- ZIP files (iterates over contents)
- Youtube URLs
- EPubs
- ... and more!

## Why Markdown?

Markdown is extremely close to plain text, with minimal markup or formatting, but still
provides a way to represent important document structure. Mainstream LLMs, such as
OpenAI&#039;s GPT-4o, natively &quot;_speak_&quot; Markdown, and often incorporate Markdown into their
responses unprompted. This suggests that they have been trained on vast amounts of
Markdown-formatted text, and understand it well. As a side benefit, Markdown conventions
are also highly token-efficient.

## Prerequisites
MarkItDown requires Python 3.10 or higher. It is recommended to use a virtual environment to avoid dependency conflicts.

With the standard Python installation, you can create and activate a virtual environment using the following commands:

```bash
python -m venv .venv
source .venv/bin/activate
```

If using `uv`, you can create a virtual environment with:

```bash
uv venv --python=3.12 .venv
source .venv/bin/activate
# NOTE: Be sure to use &#039;uv pip install&#039; rather than just &#039;pip install&#039; to install packages in this virtual environment
```

If you are using Anaconda, you can create a virtual environment with:

```bash
conda create -n markitdown python=3.12
conda activate markitdown
```

## Installation

To install MarkItDown, use pip: `pip install &#039;markitdown[all]&#039;`. Alternatively, you can install it from the source:

```bash
git clone git@github.com:microsoft/markitdown.git
cd markitdown
pip install -e &#039;packages/markitdown[all]&#039;
```

## Usage

### Command-Line

```bash
markitdown path-to-file.pdf &gt; document.md
```

Or use `-o` to specify the output file:

```bash
markitdown path-to-file.pdf -o document.md
```

You can also pipe content:

```bash
cat path-to-file.pdf | markitdown
```

### Optional Dependencies
MarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the `[all]` option. However, you can also install them individually for more control. For example:

```bash
pip install &#039;markitdown[pdf, docx, pptx]&#039;
```

will install only the dependencies for PDF, DOCX, and PPTX files.

At the moment, the following optional dependencies are available:

* `[all]` Installs all optional dependencies
* `[pptx]` Installs dependencies for PowerPoint files
* `[docx]` Installs dependencies for Word files
* `[xlsx]` Installs dependencies for Excel files
* `[xls]` Installs dependencies for older Excel files
* `[pdf]` Installs dependencies for PDF files
* `[outlook]` Installs dependencies for Outlook messages
* `[az-doc-intel]` Installs dependencies for Azure Document Intelligence
* `[audio-transcription]` Installs dependencies for audio transcription of wav and mp3 files
* `[youtube-transcription]` Installs dependencies for fetching YouTube video transcription

### Plugins

MarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:

```bash
markitdown --list-plugins
```

To enable plugins use:

```bash
markitdown --use-plugins path-to-file.pdf
```

To find available plugins, search GitHub for the hashtag `#markitdown-plugin`. To develop a plugin, see `packages/markitdown-sample-plugin`.

### Azure Document Intelligence

To use Microsoft Document Intelligence for conversion:

```bash
markitdown path-to-file.pdf -o document.md -d -e &quot;&lt;document_intelligence_endpoint&gt;&quot;
```

More information about how to set up an Azure Document Intelligence Resource can be found [here](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0)

### Python API

Basic usage in Python:

```python
from markitdown import MarkItDown

md = MarkItDown(enable_plugins=False) # Set to True to enable plugins
result = md.convert(&quot;test.xlsx&quot;)
print(result.text_content)
```

Document Intelligence conversion in Python:

```python
from markitdown import MarkItDown

md = MarkItDown(docintel_endpoint=&quot;&lt;document_intelligence_endpoint&gt;&quot;)
result = md.convert(&quot;test.pdf&quot;)
print(result.text_content)
```

To use Large Language Models for image descriptions, provide `llm_client` and `llm_model`:

```python
from markitdown import MarkItDown
from openai import OpenAI

client = OpenAI()
md = MarkItDown(llm_client=client, llm_model=&quot;gpt-4o&quot;)
result = md.convert(&quot;example.jpg&quot;)
print(result.text_content)
```

### Docker

```sh
docker build -t markitdown:latest .
docker run --rm -i markitdown:latest &lt; ~/your-file.pdf &gt; output.md
```

## Contributing

This project welcomes contributions and suggestions. Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

### How to Contribute

You can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as &#039;open for contribution&#039; and &#039;open for reviewing&#039; to help facilitate community contributions. These are ofcourse just suggestions and you are welcome to contribute in any way you like.

&lt;div align=&quot;center&quot;&gt;

|            | All                                                          | Especially Needs Help from Community                                                                                                      |
| ---------- | ------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------- |
| **Issues** | [All Issues](https://github.com/microsoft/markitdown/issues) | [Issues open for contribution](https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22) |
| **PRs**    | [All PRs](https://github.com/microsoft/markitdown/pulls)     | [PRs open for reviewing](https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22)              |

&lt;/div&gt;

### Running Tests and Checks

- Navigate to the MarkItDown package:

  ```sh
  cd packages/markitdown
  ```

- Install `hatch` in your environment and run tests:

  ```sh
  pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/
  hatch shell
  hatch test
  ```

  (Alternative) Use the Devcontainer which has all the dependencies installed:

  ```sh
  # Reopen the project in Devcontainer and run:
  hatch test
  ```

- Run pre-commit checks before submitting a PR: `pre-commit run --all-files`

### Contributing 3rd-party Plugins

You can also contribute by creating and sharing 3rd party plugins. See `packages/markitdown-sample-plugin` for more details.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft
trademarks or logos is subject to and must follow
[Microsoft&#039;s Trademark &amp; Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party&#039;s policies.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[instaloader/instaloader]]></title>
            <link>https://github.com/instaloader/instaloader</link>
            <guid>https://github.com/instaloader/instaloader</guid>
            <pubDate>Sat, 16 Aug 2025 00:04:16 GMT</pubDate>
            <description><![CDATA[Download pictures (or videos) along with their captions and other metadata from Instagram.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/instaloader/instaloader">instaloader/instaloader</a></h1>
            <p>Download pictures (or videos) along with their captions and other metadata from Instagram.</p>
            <p>Language: Python</p>
            <p>Stars: 10,570</p>
            <p>Forks: 1,369</p>
            <p>Stars today: 71 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[dreammis/social-auto-upload]]></title>
            <link>https://github.com/dreammis/social-auto-upload</link>
            <guid>https://github.com/dreammis/social-auto-upload</guid>
            <pubDate>Sat, 16 Aug 2025 00:04:15 GMT</pubDate>
            <description><![CDATA[Ëá™Âä®Âåñ‰∏ä‰º†ËßÜÈ¢ëÂà∞Á§æ‰∫§Â™í‰ΩìÔºöÊäñÈü≥„ÄÅÂ∞èÁ∫¢‰π¶„ÄÅËßÜÈ¢ëÂè∑„ÄÅtiktok„ÄÅyoutube„ÄÅbilibili]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/dreammis/social-auto-upload">dreammis/social-auto-upload</a></h1>
            <p>Ëá™Âä®Âåñ‰∏ä‰º†ËßÜÈ¢ëÂà∞Á§æ‰∫§Â™í‰ΩìÔºöÊäñÈü≥„ÄÅÂ∞èÁ∫¢‰π¶„ÄÅËßÜÈ¢ëÂè∑„ÄÅtiktok„ÄÅyoutube„ÄÅbilibili</p>
            <p>Language: Python</p>
            <p>Stars: 5,784</p>
            <p>Forks: 990</p>
            <p>Stars today: 30 stars today</p>
            <h2>README</h2><pre># social-auto-upload

`social-auto-upload` ÊòØ‰∏Ä‰∏™Âº∫Â§ßÁöÑËá™Âä®ÂåñÂ∑•ÂÖ∑ÔºåÊó®Âú®Â∏ÆÂä©ÂÜÖÂÆπÂàõ‰ΩúËÄÖÂíåËøêËê•ËÄÖÈ´òÊïàÂú∞Â∞ÜËßÜÈ¢ëÂÜÖÂÆπ‰∏ÄÈîÆÂèëÂ∏ÉÂà∞Â§ö‰∏™ÂõΩÂÜÖÂ§ñ‰∏ªÊµÅÁ§æ‰∫§Â™í‰ΩìÂπ≥Âè∞„ÄÇ
È°πÁõÆÂÆûÁé∞‰∫ÜÂØπ `ÊäñÈü≥`„ÄÅ`Bilibili`„ÄÅ`Â∞èÁ∫¢‰π¶`„ÄÅ`Âø´Êâã`„ÄÅ`ËßÜÈ¢ëÂè∑`„ÄÅ`ÁôæÂÆ∂Âè∑` ‰ª•Âèä `TikTok` Á≠âÂπ≥Âè∞ÁöÑËßÜÈ¢ë‰∏ä‰º†„ÄÅÂÆöÊó∂ÂèëÂ∏ÉÁ≠âÂäüËÉΩ„ÄÇ
ÁªìÂêàÂêÑÂπ≥Âè∞ `uploader` Ê®°ÂùóÔºåÊÇ®ÂèØ‰ª•ËΩªÊùæÈÖçÁΩÆÂíåÊâ©Â±ïÊîØÊåÅÁöÑÂπ≥Âè∞ÔºåÂπ∂ÈÄöËøáÁ§∫‰æãËÑöÊú¨Âø´ÈÄü‰∏äÊâã„ÄÇ

&lt;img src=&quot;media/show/tkupload.gif&quot; alt=&quot;tiktok show&quot; width=&quot;800&quot;/&gt;

## ÁõÆÂΩï

- [üí° ÂäüËÉΩÁâπÊÄß](#üí°ÂäüËÉΩÁâπÊÄß)
- [üöÄ ÊîØÊåÅÁöÑÂπ≥Âè∞](#üöÄÊîØÊåÅÁöÑÂπ≥Âè∞)
- [üíæ ÂÆâË£ÖÊåáÂçó](#üíæÂÆâË£ÖÊåáÂçó)
- [üèÅ Âø´ÈÄüÂºÄÂßã](#üèÅÂø´ÈÄüÂºÄÂßã)
- [üêá È°πÁõÆËÉåÊôØ](#üêáÈ°πÁõÆËÉåÊôØ)
- [üìÉ ËØ¶ÁªÜÊñáÊ°£](#üìÉËØ¶ÁªÜÊñáÊ°£)
- [üêæ ‰∫§ÊµÅ‰∏éÊîØÊåÅ](#üêæ‰∫§ÊµÅ‰∏éÊîØÊåÅ)
- [ü§ù Ë¥°ÁåÆÊåáÂçó](#ü§ùË¥°ÁåÆÊåáÂçó)
- [üìú ËÆ∏ÂèØËØÅ](#üìúËÆ∏ÂèØËØÅ)
- [‚≠ê Star History](#‚≠êStar-History)

## üí°ÂäüËÉΩÁâπÊÄß

### Â∑≤ÊîØÊåÅÂπ≥Âè∞

-   **ÂõΩÂÜÖÂπ≥Âè∞**:
    -   [x] ÊäñÈü≥
    -   [x] ËßÜÈ¢ëÂè∑
    -   [x] Bilibili
    -   [x] Â∞èÁ∫¢‰π¶
    -   [x] Âø´Êâã
    -   [x] ÁôæÂÆ∂Âè∑
-   **ÂõΩÂ§ñÂπ≥Âè∞**:
    -   [x] TikTok

### Ê†∏ÂøÉÂäüËÉΩ

-   [x] ÂÆöÊó∂‰∏ä‰º† (Cron Job / Scheduled Upload)
-   [ ] Cookie ÁÆ°ÁêÜ (ÈÉ®ÂàÜÂÆûÁé∞ÔºåÊåÅÁª≠‰ºòÂåñ‰∏≠)
-   [ ] ÂõΩÂ§ñÂπ≥Âè∞ Proxy ËÆæÁΩÆ (ÈÉ®ÂàÜÂÆûÁé∞)

### ËÆ°ÂàíÊîØÊåÅ‰∏éÂºÄÂèë‰∏≠

-   **Âπ≥Âè∞Êâ©Â±ï**:
    -   [ ] YouTube
-   **ÂäüËÉΩÂ¢ûÂº∫**:
    -   [x] Êõ¥ÊòìÁî®ÁöÑÁâàÊú¨ (GUI / CLI ‰∫§‰∫í‰ºòÂåñ)
    -   [x] API Â∞ÅË£Ö
    -   [ ] Docker ÈÉ®ÁΩ≤
    -   [ ] Ëá™Âä®Âåñ‰∏ä‰º† (Êõ¥Êô∫ËÉΩÁöÑË∞ÉÂ∫¶Á≠ñÁï•)
    -   [ ] Â§öÁ∫øÁ®ã/ÂºÇÊ≠•‰∏ä‰º†‰ºòÂåñ
    -   [ ] Slack/Ê∂àÊÅØÊé®ÈÄÅÈÄöÁü•

---

## üöÄÊîØÊåÅÁöÑÂπ≥Âè∞

Êú¨È°πÁõÆÈÄöËøáÂêÑÂπ≥Âè∞ÂØπÂ∫îÁöÑ `uploader` Ê®°ÂùóÂÆûÁé∞ËßÜÈ¢ë‰∏ä‰º†ÂäüËÉΩ„ÄÇÊÇ®ÂèØ‰ª•Âú® `examples` ÁõÆÂΩï‰∏ãÊâæÂà∞ÂêÑ‰∏™Âπ≥Âè∞ÁöÑ‰ΩøÁî®Á§∫‰æãËÑöÊú¨„ÄÇ

ÊØè‰∏™Á§∫‰æãËÑöÊú¨Â±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÈÖçÁΩÆÂíåË∞ÉÁî®Áõ∏Â∫îÁöÑ uploader„ÄÇ

## üíæÂÆâË£ÖÊåáÂçó

1.  **ÂÖãÈöÜÈ°πÁõÆ**:
    ```bash
    git clone https://github.com/dreammis/social-auto-upload.git
    cd social-auto-upload
    ```

2.  **ÂÆâË£Ö‰æùËµñ**:
    Âª∫ËÆÆÂú®ËôöÊãüÁéØÂ¢É‰∏≠ÂÆâË£Ö‰æùËµñ„ÄÇ
    ```bash
    conda create -n social-auto-upload python=3.10
    conda activate social-auto-upload
    # ÊåÇËΩΩÊ∏ÖÂçéÈïúÂÉè or ÂëΩ‰ª§Ë°å‰ª£ÁêÜ
    pip install -r requirements.txt
    ```

3.  **ÂÆâË£Ö Playwright ÊµèËßàÂô®È©±Âä®**:
    ```bash
    playwright install chromium firefox
    ```
    Ê†πÊçÆÊÇ®ÁöÑÈúÄÊ±ÇÔºåËá≥Â∞ëÈúÄË¶ÅÂÆâË£Ö `chromium`„ÄÇ`firefox` ‰∏ªË¶ÅÁî®‰∫é TikTok ‰∏ä‰º†ÔºàÊóßÁâàÔºâ„ÄÇ

4.  **‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂**:
    Â§çÂà∂ `conf.example.py` Âπ∂ÈáçÂëΩÂêç‰∏∫ `conf.py`„ÄÇ
    Âú® `conf.py` ‰∏≠ÔºåÊÇ®ÈúÄË¶ÅÈÖçÁΩÆ‰ª•‰∏ãÂÜÖÂÆπÔºö
    -   `LOCAL_CHROME_PATH`: Êú¨Âú∞ Chrome ÊµèËßàÂô®ÁöÑË∑ØÂæÑÔºåÊØîÂ¶Ç `C:\Program Files\Google\Chrome\Application\chrome.exe` ‰øùÂ≠ò„ÄÇ
    
    **‰∏¥Êó∂Ëß£ÂÜ≥ÊñπÊ°à**

    ÈúÄË¶ÅÂú®Ê†πÁõÆÂΩïÂàõÂª∫ `cookiesFile` Âíå `videoFile` ‰∏§‰∏™Êñá‰ª∂Â§πÔºåÂàÜÂà´ÊòØ Â≠òÂÇ®cookieÊñá‰ª∂ Âíå Â≠òÂÇ®‰∏ä‰º†Êñá‰ª∂ ÁöÑÊñá‰ª∂Â§π

5.  **ÈÖçÁΩÆÊï∞ÊçÆÂ∫ì**:
    Â¶ÇÊûú db/database.db Êñá‰ª∂‰∏çÂ≠òÂú®ÔºåÊÇ®ÂèØ‰ª•ËøêË°å‰ª•‰∏ãÂëΩ‰ª§Êù•ÂàùÂßãÂåñÊï∞ÊçÆÂ∫ìÔºö
    ```bash
    cd db
    python createTable.py
    ```
    Ê≠§ÂëΩ‰ª§Â∞ÜÂàùÂßãÂåñ SQLite Êï∞ÊçÆÂ∫ì„ÄÇ

6.  **ÂêØÂä®ÂêéÁ´ØÈ°πÁõÆ**:
    ```bash
    python sau_backend.py
    ```
    ÂêéÁ´ØÈ°πÁõÆÂ∞ÜÂú® `http://localhost:5409` ÂêØÂä®„ÄÇ

7.  **ÂêØÂä®ÂâçÁ´ØÈ°πÁõÆ**:
    ```bash
    cd sau_frontend
    npm install
    npm run dev
    ```
    ÂâçÁ´ØÈ°πÁõÆÂ∞ÜÂú® `http://localhost:5173` ÂêØÂä®ÔºåÂú®ÊµèËßàÂô®‰∏≠ÊâìÂºÄÊ≠§ÈìæÊé•Âç≥ÂèØËÆøÈóÆ„ÄÇ


&gt; ÈùûÁ®ãÂ∫èÂëòÁî®Êà∑ÂèØ‰ª•ÂèÇËÄÉÔºö[Êñ∞ÊâãÁ∫ßÊïôÁ®ã](https://juejin.cn/post/7372114027840208911)


## üèÅÂø´ÈÄüÂºÄÂßã

1.  **ÂáÜÂ§á Cookie**: 
    Â§ßÂ§öÊï∞Âπ≥Âè∞ÈúÄË¶ÅÁôªÂΩïÂêéÁöÑ Cookie ‰ø°ÊÅØÊâçËÉΩËøõË°åÊìç‰Ωú„ÄÇËØ∑ÂèÇÁÖß examples ÁõÆÂΩï‰∏ãÂêÑ `get_xxx_cookie.py` ËÑöÊú¨Ôºà‰æãÂ¶Ç get_douyin_cookie.py, get_ks_cookie.pyÔºâÁöÑËØ¥ÊòéÔºåËøêË°åËÑöÊú¨‰ª•ÁîüÊàêÂπ∂‰øùÂ≠ò Cookie Êñá‰ª∂ÔºàÈÄöÂ∏∏Âú® `cookies/[PLATFORM]_uploader/account.json`Ôºâ„ÄÇ

2.  **ÂáÜÂ§áËßÜÈ¢ëÊñá‰ª∂**: 
    Â∞ÜÈúÄË¶Å‰∏ä‰º†ÁöÑËßÜÈ¢ëÊñá‰ª∂ÔºàÈÄöÂ∏∏‰∏∫ `.mp4` Ê†ºÂºèÔºâÊîæÁΩÆÂú® videos ÁõÆÂΩï‰∏ã„ÄÇ
    ÈÉ®ÂàÜÂπ≥Âè∞ÊîØÊåÅËßÜÈ¢ëÂ∞ÅÈù¢ÔºåÂèØ‰ª•Â∞ÜÂ∞ÅÈù¢ÂõæÁâáÔºà‰æãÂ¶Ç `.png` Ê†ºÂºèÔºå‰∏éËßÜÈ¢ëÂêåÂêçÔºâ‰πüÊîæÂú®Ê≠§ÁõÆÂΩï„ÄÇ
    Â¶ÇÊûúÈúÄË¶Å‰∏ä‰º†Ê†áÈ¢òÂèäÊ†áÁ≠æÔºåËØ∑Âú®ËßÜÈ¢ëÊñá‰ª∂ÊóÅËæπÂàõÂª∫‰∏Ä‰∏™ÂêåÂêçÁöÑ `.txt` Êñá‰ª∂ÔºåÂÜÖÂÆπ‰∏∫Ê†áÈ¢òÂíåÊ†áÁ≠æÔºå‰ª•Êç¢Ë°åÂàÜÈöî„ÄÇ

3.  **‰øÆÊîπÂπ∂ËøêË°åÁ§∫‰æãËÑöÊú¨**:
    ÊâìÂºÄ examples ÁõÆÂΩï‰∏≠ÊÇ®ÊÉ≥‰ΩøÁî®ÁöÑÂπ≥Âè∞ÁöÑ‰∏ä‰º†ËÑöÊú¨Ôºà‰æãÂ¶Ç upload_video_to_douyin.pyÔºâ„ÄÇ
    -   Ê†πÊçÆËÑöÊú¨ÂÜÖÁöÑÊ≥®ÈáäÂíåËØ¥ÊòéÔºåÁ°ÆËÆ§ Cookie Êñá‰ª∂Ë∑ØÂæÑ„ÄÅËßÜÈ¢ëÊñá‰ª∂Ë∑ØÂæÑÁ≠âÈÖçÁΩÆÊòØÂê¶Ê≠£Á°Æ„ÄÇ
    -   ÊÇ®ÂèØ‰ª•‰øÆÊîπËÑöÊú¨‰ª•ÈÄÇÂ∫îÊÇ®ÁöÑÂÖ∑‰ΩìÈúÄÊ±ÇÔºå‰æãÂ¶ÇÊâπÈáè‰∏ä‰º†„ÄÅËá™ÂÆö‰πâÊ†áÈ¢ò„ÄÅÊ†áÁ≠æÁ≠â„ÄÇ

4.  **ÊâßË°å‰∏ä‰º†**:
    ËøêË°å‰øÆÊîπÂêéÁöÑÁ§∫‰æãËÑöÊú¨Ôºå‰æãÂ¶ÇÔºö
    ```bash
    python examples/upload_video_to_douyin.py
    ```

## üêáÈ°πÁõÆËÉåÊôØ

ËØ•È°πÁõÆÊúÄÂàùÊòØÊàë‰∏™‰∫∫Áî®‰∫éËá™Âä®ÂåñÁÆ°ÁêÜÁ§æ‰∫§Â™í‰ΩìËßÜÈ¢ëÂèëÂ∏ÉÁöÑÂ∑•ÂÖ∑„ÄÇÊàëÁöÑ‰∏ªË¶ÅÂèëÂ∏ÉÁ≠ñÁï•ÊòØÊèêÂâç‰∏ÄÂ§©ËÆæÁΩÆÂÆöÊó∂ÂèëÂ∏ÉÔºåÂõ†Ê≠§È°πÁõÆ‰∏≠ÂæàÂ§öÂÆöÊó∂ÂèëÂ∏ÉÁõ∏ÂÖ≥ÁöÑÈÄªËæëÊòØÂü∫‰∫é‚ÄúÁ¨¨‰∫åÂ§©‚ÄùÁöÑÊó∂Èó¥ËøõË°åËÆ°ÁÆóÁöÑ„ÄÇ

Â¶ÇÊûúÊÇ®ÈúÄË¶ÅÁ´ãÂç≥ÂèëÂ∏ÉÊàñÂÖ∂‰ªñÂÆöÂà∂ÂåñÁöÑÂèëÂ∏ÉÁ≠ñÁï•ÔºåÊ¨¢ËøéÁ†îÁ©∂Ê∫êÁ†ÅÊàñÂú®Á§æÂå∫ÊèêÈóÆ„ÄÇ

## üìÉËØ¶ÁªÜÊñáÊ°£

Êõ¥ËØ¶ÁªÜÁöÑÊñáÊ°£ÂíåËØ¥ÊòéÔºåËØ∑Êü•ÁúãÔºö[social-auto-upload ÂÆòÊñπÊñáÊ°£](https://sap-doc.nasdaddy.com/)

## üêæ‰∫§ÊµÅ‰∏éÊîØÊåÅ

[‚òï Donate as u like](https://www.buymeacoffee.com/hysn2001m) - Â¶ÇÊûúÊÇ®ËßâÂæóËøô‰∏™È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåÂèØ‰ª•ËÄÉËôëËµûÂä©„ÄÇ

Â¶ÇÊûúÊÇ®‰πüÊòØÁã¨Á´ãÂºÄÂèëËÄÖ„ÄÅÊäÄÊúØÁà±Â•ΩËÄÖÔºåÂØπ #ÊäÄÊúØÂèòÁé∞ #AIÂàõ‰∏ö #Ë∑®Â¢ÉÁîµÂïÜ #Ëá™Âä®ÂåñÂ∑•ÂÖ∑ #ËßÜÈ¢ëÂàõ‰Ωú Á≠âËØùÈ¢òÊÑüÂÖ¥Ë∂£ÔºåÊ¨¢ËøéÂä†ÂÖ•Á§æÁæ§‰∫§ÊµÅ„ÄÇ

### Creator

&lt;table&gt;
    &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://sap-doc.nasdaddy.com/&quot;&gt;
            &lt;img src=&quot;media/mp.jpg&quot; width=&quot;200px&quot; alt=&quot;NasDaddyÂÖ¨‰ºóÂè∑&quot;/&gt;
            &lt;br /&gt;
            &lt;sub&gt;&lt;b&gt;ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑&lt;/b&gt;&lt;/sub&gt;
        &lt;/a&gt;
        &lt;br /&gt;
        &lt;a href=&quot;https://github.com/dreammis/social-auto-upload/commits?author=dreammis&quot; title=&quot;Code&quot;&gt;üíª&lt;/a&gt;
        &lt;br /&gt;
        ÂÖ≥Ê≥®ÂÖ¨‰ºóÂè∑ÔºåÂêéÂè∞ÂõûÂ§ç `‰∏ä‰º†` Ëé∑ÂèñÂä†Áæ§ÊñπÂºè
    &lt;/td&gt;
    &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://sap-doc.nasdaddy.com/&quot;&gt;
            &lt;img src=&quot;media/QR.png&quot; width=&quot;200px&quot; alt=&quot;ËµûËµèÁ†Å/ÂÖ•Áæ§ÂºïÂØº&quot;/&gt;
            &lt;br /&gt;
            &lt;sub&gt;&lt;b&gt;‰∫§ÊµÅÁæ§ (ÈÄöËøáÂÖ¨‰ºóÂè∑Ëé∑Âèñ)&lt;/b&gt;&lt;/sub&gt;
        &lt;/a&gt;
        &lt;br /&gt;
        &lt;a href=&quot;https://sap-doc.nasdaddy.com/&quot; title=&quot;Documentation&quot;&gt;üìñ&lt;/a&gt;
        &lt;br /&gt;
        Â¶ÇÊûúÊÇ®ËßâÂæóÈ°πÁõÆÊúâÁî®ÔºåÂèØ‰ª•ËÄÉËôëÊâìËµèÊîØÊåÅ‰∏Ä‰∏ã
    &lt;/td&gt;
&lt;/table&gt;

### Active Core Team

&lt;table&gt;
    &lt;td align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://leedebug.github.io/&quot;&gt;
            &lt;img src=&quot;media/edan-qrcode.png&quot; width=&quot;200px&quot; alt=&quot;Edan Lee&quot;/&gt;
            &lt;br /&gt;
            &lt;sub&gt;&lt;b&gt;Edan Lee&lt;/b&gt;&lt;/sub&gt;
        &lt;/a&gt;
        &lt;br /&gt;
        &lt;a href=&quot;https://github.com/dreammis/social-auto-upload/commits?author=LeeDebug&quot; title=&quot;Code&quot;&gt;üíª&lt;/a&gt;
        &lt;a href=&quot;https://leedebug.github.io/&quot; title=&quot;Documentation&quot;&gt;üìñ&lt;/a&gt;
        &lt;br /&gt;
        Â∞ÅË£Ö‰∫Ü api Êé•Âè£Âíå web ÂâçÁ´ØÁÆ°ÁêÜÁïåÈù¢
        &lt;br /&gt;
        ÔºàËØ∑Ê≥®ÊòéÊù•ÊÑèÔºöËøõÁæ§„ÄÅÂ≠¶‰π†„ÄÅ‰ºÅ‰∏öÂí®ËØ¢Á≠âÔºâ
    &lt;/td&gt;
&lt;/table&gt;

## ü§ùË¥°ÁåÆÊåáÂçó

Ê¨¢ËøéÂêÑÁßçÂΩ¢ÂºèÁöÑË¥°ÁåÆÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÔºö

-   Êèê‰∫§ BugÊä•Âëä Âíå FeatureËØ∑Ê±Ç„ÄÇ
-   ÊîπËøõ‰ª£Á†Å„ÄÅÊñáÊ°£„ÄÇ
-   ÂàÜ‰∫´‰ΩøÁî®ÁªèÈ™åÂíåÊïôÁ®ã„ÄÇ

Â¶ÇÊûúÊÇ®Â∏åÊúõË¥°ÁåÆ‰ª£Á†ÅÔºåËØ∑ÈÅµÂæ™‰ª•‰∏ãÊ≠•È™§Ôºö

1.  Fork Êú¨‰ªìÂ∫ì„ÄÇ
2.  ÂàõÂª∫‰∏Ä‰∏™Êñ∞ÁöÑÂàÜÊîØ (`git checkout -b feature/YourFeature` Êàñ `bugfix/YourBugfix`)„ÄÇ
3.  Êèê‰∫§ÊÇ®ÁöÑÊõ¥Êîπ (`git commit -m &#039;Add some feature&#039;`)„ÄÇ
4.  PushÂà∞ÊÇ®ÁöÑÂàÜÊîØ (`git push origin feature/YourFeature`)„ÄÇ
5.  ÂàõÂª∫‰∏Ä‰∏™ Pull Request„ÄÇ

## üìúËÆ∏ÂèØËØÅ

Êú¨È°πÁõÆÊöÇÊó∂ÈááÁî® [MIT License](LICENSE) ÂºÄÊ∫êËÆ∏ÂèØËØÅ„ÄÇ

## ‚≠êStar-History

&gt; Â¶ÇÊûúËøô‰∏™È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåËØ∑Áªô‰∏Ä‰∏™ ‚≠ê Star ‰ª•Ë°®Á§∫ÊîØÊåÅÔºÅ

[![Star History Chart](https://api.star-history.com/svg?repos=dreammis/social-auto-upload&amp;type=Date)](https://star-history.com/#dreammis/social-auto-upload&amp;Date)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[pathwaycom/pathway]]></title>
            <link>https://github.com/pathwaycom/pathway</link>
            <guid>https://github.com/pathwaycom/pathway</guid>
            <pubDate>Sat, 16 Aug 2025 00:04:14 GMT</pubDate>
            <description><![CDATA[Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/pathwaycom/pathway">pathwaycom/pathway</a></h1>
            <p>Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.</p>
            <p>Language: Python</p>
            <p>Stars: 31,529</p>
            <p>Forks: 881</p>
            <p>Stars today: 393 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://pathway.com/&quot;&gt;
    &lt;img src=&quot;https://pathway.com/logo-light.svg&quot;/&gt;
  &lt;/a&gt;
  &lt;br /&gt;&lt;br /&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/10388&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/10388&quot; alt=&quot;pathwaycom%2Fpathway | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
  &lt;br /&gt;&lt;br /&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml&quot;&gt;
        &lt;img src=&quot;https://github.com/pathwaycom/pathway/actions/workflows/ubuntu_test.yml/badge.svg&quot; alt=&quot;ubuntu&quot;/&gt;
        &lt;br&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/actions/workflows/release.yml&quot;&gt;
        &lt;img src=&quot;https://github.com/pathwaycom/pathway/actions/workflows/release.yml/badge.svg&quot; alt=&quot;Last release&quot;/&gt;&lt;/a&gt;
        &lt;a href=&quot;https://badge.fury.io/py/pathway&quot;&gt;&lt;img src=&quot;https://badge.fury.io/py/pathway.svg&quot; alt=&quot;PyPI version&quot; height=&quot;18&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://badge.fury.io/py/pathway&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/badge/pathway&quot; alt=&quot;PyPI downloads&quot; height=&quot;18&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://github.com/pathwaycom/pathway/blob/main/LICENSE.txt&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/license-BSL-green&quot; alt=&quot;License: BSL&quot;/&gt;&lt;/a&gt;
      &lt;br&gt;
        &lt;a href=&quot;https://discord.gg/pathway&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/discord/1042405378304004156?logo=discord&quot;
            alt=&quot;chat on Discord&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://twitter.com/intent/follow?screen_name=pathway_com&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/twitter/follow/pathwaycom&quot;
            alt=&quot;follow on Twitter&quot;&gt;&lt;/a&gt;
        &lt;a href=&quot;https://linkedin.com/company/pathway&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/pathway-0077B5?style=social&amp;logo=linkedin&quot; alt=&quot;follow on LinkedIn&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://github.com/dylanhogg/awesome-python/blob/main/README.md&quot;&gt;
      &lt;img src=&quot;https://awesome.re/badge.svg&quot; alt=&quot;Awesome Python&quot;&gt;&lt;/a&gt;
      &lt;a href=&quot;https://gurubase.io/g/pathway&quot;&gt;
      &lt;img src=&quot;https://img.shields.io/badge/Gurubase-Ask%20Pathway%20Guru-006BFF&quot; alt=&quot;Pathway Guru&quot;&gt;&lt;/a&gt;
    &lt;br&gt;
    &lt;a href=&quot;#getting-started&quot;&gt;Getting Started&lt;/a&gt; |
    &lt;a href=&quot;#deployment&quot;&gt;Deployment&lt;/a&gt; |
    &lt;a href=&quot;#resources&quot;&gt;Documentation and Support&lt;/a&gt; |
    &lt;a href=&quot;https://pathway.com/blog/&quot;&gt;Blog&lt;/a&gt; |
    &lt;a href=&quot;#license&quot;&gt;License&lt;/a&gt;

  
&lt;/p&gt;

# Pathway&lt;a id=&quot;pathway&quot;&gt; Live Data Framework&lt;/a&gt;

[Pathway](https://pathway.com) is a Python ETL framework for stream processing, real-time analytics, LLM pipelines, and RAG.

Pathway comes with an **easy-to-use Python API**, allowing you to seamlessly integrate your favorite Python ML libraries.
Pathway code is versatile and robust: **you can use it in both development and production environments, handling both batch and streaming data effectively**.
The same code can be used for local development, CI/CD tests, running batch jobs, handling stream replays, and processing data streams.

Pathway is powered by a **scalable Rust engine** based on Differential Dataflow and performs incremental computation.
Your Pathway code, despite being written in Python, is run by the Rust engine, enabling multithreading, multiprocessing, and distributed computations.
All the pipeline is kept in memory and can be easily deployed with **Docker and Kubernetes**.

You can install Pathway with pip:
```
pip install -U pathway
```

For any questions, you will find the community and team behind the project [on Discord](https://discord.com/invite/pathway).

## Use-cases and templates

Ready to see what Pathway can do?

[Try one of our easy-to-run examples](https://pathway.com/developers/templates)!

Available in both notebook and docker formats, these ready-to-launch examples can be launched in just a few clicks. Pick one and start your hands-on experience with Pathway today!

### Event processing and real-time analytics pipelines
With its unified engine for batch and streaming and its full Python compatibility, Pathway makes data processing as easy as possible. It&#039;s the ideal solution for a wide range of data processing pipelines, including:

- [Showcase: Real-time ETL.](https://pathway.com/developers/templates/kafka-etl)
- [Showcase: Event-driven pipelines with alerting.](https://pathway.com/developers/templates/realtime-log-monitoring)
- [Showcase: Realtime analytics.](https://pathway.com/developers/templates/linear_regression_with_kafka/)
- [Docs: Switch from batch to streaming.](https://pathway.com/developers/user-guide/connecting-to-data/switch-from-batch-to-streaming)



### AI Pipelines

Pathway provides dedicated LLM tooling to build live LLM and RAG pipelines. Wrappers for most common LLM services and utilities are included, making working with LLMs and RAGs pipelines incredibly easy. Check out our [LLM xpack documentation](https://pathway.com/developers/user-guide/llm-xpack/overview).

Don&#039;t hesitate to try one of our runnable examples featuring LLM tooling.
You can find such examples [here](https://pathway.com/developers/user-guide/llm-xpack/llm-examples).

  - [Template: Unstructured data to SQL on-the-fly.](https://pathway.com/developers/templates/unstructured-to-structured/)
  - [Template: Private RAG with Ollama and Mistral AI](https://pathway.com/developers/templates/private-rag-ollama-mistral)
  - [Template: Adaptive RAG](https://pathway.com/developers/templates/adaptive-rag)
  - [Template: Multimodal RAG with gpt-4o](https://pathway.com/developers/templates/multimodal-rag)

## Features

- **A wide range of connectors**: Pathway comes with connectors that connect to external data sources such as Kafka, GDrive, PostgreSQL, or SharePoint. Its Airbyte connector allows you to connect to more than 300 different data sources. If the connector you want is not available, you can build your own custom connector using Pathway Python connector.
- **Stateless and stateful transformations**: Pathway supports stateful transformations such as joins, windowing, and sorting. It provides many transformations directly implemented in Rust. In addition to the provided transformation, you can use any Python function. You can implement your own or you can use any Python library to process your data.
- **Persistence**: Pathway provides persistence to save the state of the computation. This allows you to restart your pipeline after an update or a crash. Your pipelines are in good hands with Pathway!
- **Consistency**: Pathway handles the time for you, making sure all your computations are consistent. In particular, Pathway manages late and out-of-order points by updating its results whenever new (or late, in this case) data points come into the system. The free version of Pathway gives the &quot;at least once&quot; consistency while the enterprise version provides the &quot;exactly once&quot; consistency.
- **Scalable Rust engine**: with Pathway Rust engine, you are free from the usual limits imposed by Python. You can easily do multithreading, multiprocessing, and distributed computations.
- **LLM helpers**: Pathway provides an LLM extension with all the utilities to integrate LLMs with your data pipelines (LLM wrappers, parsers, embedders, splitters), including an in-memory real-time Vector Index, and integrations with LLamaIndex and LangChain. You can quickly build and deploy RAG applications with your live documents.


## Getting started&lt;a id=&quot;getting-started&quot;&gt;&lt;/a&gt;

### Installation&lt;a id=&quot;installation&quot;&gt;&lt;/a&gt;

Pathway requires Python 3.10 or above.

You can install the current release of Pathway using `pip`:

```
$ pip install -U pathway
```

‚ö†Ô∏è Pathway is available on MacOS and Linux. Users of other systems should run Pathway on a Virtual Machine.


### Example: computing the sum of positive values in real time.&lt;a id=&quot;example&quot;&gt;&lt;/a&gt;

```python
import pathway as pw

# Define the schema of your data (Optional)
class InputSchema(pw.Schema):
  value: int

# Connect to your data using connectors
input_table = pw.io.csv.read(
  &quot;./input/&quot;,
  schema=InputSchema
)

#Define your operations on the data
filtered_table = input_table.filter(input_table.value&gt;=0)
result_table = filtered_table.reduce(
  sum_value = pw.reducers.sum(filtered_table.value)
)

# Load your results to external systems
pw.io.jsonlines.write(result_table, &quot;output.jsonl&quot;)

# Run the computation
pw.run()
```

Run Pathway [in Google Colab](https://colab.research.google.com/drive/1aBIJ2HCng-YEUOMrr0qtj0NeZMEyRz55?usp=sharing).

You can find more examples [here](https://github.com/pathwaycom/pathway/tree/main/examples).


## Deployment&lt;a id=&quot;deployment&quot;&gt;&lt;/a&gt;

### Locally&lt;a id=&quot;running-pathway-locally&quot;&gt;&lt;/a&gt;

To use Pathway, you only need to import it:

```python
import pathway as pw
```

Now, you can easily create your processing pipeline, and let Pathway handle the updates. Once your pipeline is created, you can launch the computation on streaming data with a one-line command:

```python
pw.run()
```

You can then run your Pathway project (say, `main.py`) just like a normal Python script: `$ python main.py`.
Pathway comes with a monitoring dashboard that allows you to keep track of the number of messages sent by each connector and the latency of the system. The dashboard also includes log messages. 

&lt;img src=&quot;https://d14l3brkh44201.cloudfront.net/pathway-dashboard.png&quot; width=&quot;1326&quot; alt=&quot;Pathway dashboard&quot;/&gt;

Alternatively, you can use the pathway&#039;ish version:

```
$ pathway spawn python main.py
```

Pathway natively supports multithreading.
To launch your application with 3 threads, you can do as follows:
```
$ pathway spawn --threads 3 python main.py
```

To jumpstart a Pathway project, you can use our [cookiecutter template](https://github.com/pathwaycom/cookiecutter-pathway).


### Docker&lt;a id=&quot;docker&quot;&gt;&lt;/a&gt;

You can easily run Pathway using docker.

#### Pathway image

You can use the [Pathway docker image](https://hub.docker.com/r/pathwaycom/pathway), using a Dockerfile:

```dockerfile
FROM pathwaycom/pathway:latest

WORKDIR /app

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD [ &quot;python&quot;, &quot;./your-script.py&quot; ]
```

You can then build and run the Docker image:

```console
docker build -t my-pathway-app .
docker run -it --rm --name my-pathway-app my-pathway-app
```

#### Run a single Python script

When dealing with single-file projects, creating a full-fledged `Dockerfile`
might seem unnecessary. In such scenarios, you can execute a
Python script directly using the Pathway Docker image. For example:

```console
docker run -it --rm --name my-pathway-app -v &quot;$PWD&quot;:/app pathwaycom/pathway:latest python my-pathway-app.py
```

#### Python docker image

You can also use a standard Python image and install Pathway using pip with a Dockerfile:

```dockerfile
FROM --platform=linux/x86_64 python:3.10

RUN pip install -U pathway
COPY ./pathway-script.py pathway-script.py

CMD [&quot;python&quot;, &quot;-u&quot;, &quot;pathway-script.py&quot;]
```

### Kubernetes and cloud&lt;a id=&quot;k8s&quot;&gt;&lt;/a&gt;

Docker containers are ideally suited for deployment on the cloud with Kubernetes.
If you want to scale your Pathway application, you may be interested in our Pathway for Enterprise.
Pathway for Enterprise is specially tailored towards end-to-end data processing and real time intelligent analytics.
It scales using distributed computing on the cloud and supports distributed Kubernetes deployment, with external persistence setup.

You can easily deploy Pathway using services like Render: see [how to deploy Pathway in a few clicks](https://pathway.com/developers/user-guide/deployment/render-deploy/).

If you are interested, don&#039;t hesitate to [contact us](mailto:contact@pathway.com) to learn more.

## Performance&lt;a id=&quot;performance&quot;&gt;&lt;/a&gt;

Pathway is made to outperform state-of-the-art technologies designed for streaming and batch data processing tasks, including: Flink, Spark, and Kafka Streaming. It also makes it possible to implement a lot of algorithms/UDF&#039;s in streaming mode which are not readily supported by other streaming frameworks (especially: temporal joins, iterative graph algorithms, machine learning routines).

If you are curious, here are [some benchmarks to play with](https://github.com/pathwaycom/pathway-benchmarks).

&lt;img src=&quot;https://github.com/pathwaycom/pathway-benchmarks/raw/main/images/bm-wordcount-lineplot.png&quot; width=&quot;1326&quot; alt=&quot;WordCount Graph&quot;/&gt;

## Documentation and Support&lt;a id=&quot;resources&quot;&gt;&lt;/a&gt;

The entire documentation of Pathway is available at [pathway.com/developers/](https://pathway.com/developers/user-guide/introduction/welcome), including the [API Docs](https://pathway.com/developers/api-docs/pathway).

If you have any question, don&#039;t hesitate to [open an issue on GitHub](https://github.com/pathwaycom/pathway/issues), join us on [Discord](https://discord.com/invite/pathway), or send us an email at [contact@pathway.com](mailto:contact@pathway.com).

## License&lt;a id=&quot;license&quot;&gt;&lt;/a&gt;

Pathway is distributed on a [BSL 1.1 License](https://github.com/pathwaycom/pathway/blob/main/LICENSE.txt) which allows for unlimited non-commercial use, as well as use of the Pathway package [for most commercial purposes](https://pathway.com/license/), free of charge. Code in this repository automatically converts to Open Source (Apache 2.0 License) after 4 years. Some [public repos](https://github.com/pathwaycom) which are complementary to this one (examples, libraries, connectors, etc.) are licensed as Open Source, under the MIT license.


## Contribution guidelines&lt;a id=&quot;contribution-guidelines&quot;&gt;&lt;/a&gt;

If you develop a library or connector which you would like to integrate with this repo, we suggest releasing it first as a separate repo on a MIT/Apache 2.0 license. 

For all concerns regarding core Pathway functionalities, Issues are encouraged. For further information, don&#039;t hesitate to engage with Pathway&#039;s [Discord community](https://discord.gg/pathway).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[public-apis/public-apis]]></title>
            <link>https://github.com/public-apis/public-apis</link>
            <guid>https://github.com/public-apis/public-apis</guid>
            <pubDate>Sat, 16 Aug 2025 00:04:13 GMT</pubDate>
            <description><![CDATA[A collective list of free APIs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/public-apis/public-apis">public-apis/public-apis</a></h1>
            <p>A collective list of free APIs</p>
            <p>Language: Python</p>
            <p>Stars: 361,422</p>
            <p>Forks: 37,928</p>
            <p>Stars today: 169 stars today</p>
            <h2>README</h2><pre># Try Public APIs for free
The Public APIs repository is manually curated by community members like you and folks working at [APILayer](https://apilayer.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo). It includes an extensive list of public APIs from many domains that you can use for your own products. Consider it a treasure trove of APIs well-managed by the community over the years.

&lt;br &gt;

&lt;p&gt;
    &lt;a href=&quot;https://apilayer.com&quot;&gt;
        &lt;div&gt;
            &lt;img src=&quot;.github/cs1586-APILayerLogoUpdate2022-LJ_v2-HighRes.png&quot; width=&quot;100%&quot; alt=&quot;APILayer Logo&quot; /&gt;
        &lt;/div&gt;
    &lt;/a&gt;
  &lt;/p&gt;

APILayer is the fastest way to integrate APIs into any product. Explore [APILayer APIs](https://apilayer.com/products/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo) here for your next project.

Join our [Discord server](https://discord.com/invite/hgjA78638n/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo) to get updates, ask questions, get answers, random community calls, and more.

&lt;br &gt;

## APILayer APIs
| API | Description | Call this API |
|:---|:---|:---|
| [IPstack](https://ipstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Locate and Identify Website Visitors by IP Address | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-55145132-244c-448c-8e6f-8780866e4862?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-55145132-244c-448c-8e6f-8780866e4862%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Marketstack](https://marketstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Free, easy-to-use REST API interface delivering worldwide stock market data in JSON format | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-9cbac391-3611-4f50-9bfd-d24ae41c97c1%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Weatherstack](https://weatherstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Retrieve instant, accurate weather information for any location in the world in lightweight JSON format | [&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-276c4312-f682-425d-b6b1-0f82c0a7f2b3%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Numverify](https://numverify.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers ) | Global Phone Number Validation &amp; Lookup JSON API |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-0760d25e-b802-412e-b0e4-26e5ca3b9ffa%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Fixer](https://fixer.io/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Fixer is a simple and lightweight API for current and historical foreign exchange (forex) rates. |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d?action=collection%2Ffork&amp;source=rip_markdown&amp;collection-url=entityId%3D10131015-0d9c66b3-5f1a-42ed-a5ca-379217bd629d%26entityType%3Dcollection%26workspaceId%3D2b7498b6-6d91-4fa8-817f-608441fe42a8)|
| [Aviationstack](https://avaitionstack.com/?utm_source=Github&amp;utm_medium=Referral&amp;utm_campaign=Public-apis-repo-Best-sellers) | Free, real-time flight status and global Aviation data API |[&lt;img src=&quot;https://run.pstmn.io/button.svg&quot; alt=&quot;Run In Postman&quot; style=&quot;width: 128px; height: 32px;&quot;&gt;](https://god.gw.postman.com/run-collection/10131015-72ee0d35-018e-4370-a2b6-a66d3ebd5b5a?action=collection/fork)|

&lt;br &gt;

## Learn more about Public APIs

&lt;strong&gt;Get Involved&lt;/strong&gt;

* [Contributing Guide](CONTRIBUTING.md)
* [API for this project](https://github.com/davemachado/public-api)
* [Issues](https://github.com/public-apis/public-apis/issues)
* [Pull Requests](https://github.com/public-apis/public-apis/pulls)
* [LICENSE](LICENSE) 

&lt;br /&gt;

## Index

* [Animals](#animals)
* [Anime](#anime)
* [Anti-Malware](#anti-malware)
* [Art &amp; Design](#art--design)
* [Authentication &amp; Authorization](#authentication--authorization)
* [Blockchain](#blockchain)
* [Books](#books)
* [Business](#business)
* [Calendar](#calendar)
* [Cloud Storage &amp; File Sharing](#cloud-storage--file-sharing)
* [Continuous Integration](#continuous-integration)
* [Cryptocurrency](#cryptocurrency)
* [Currency Exchange](#currency-exchange)
* [Data Validation](#data-validation)
* [Development](#development)
* [Dictionaries](#dictionaries)
* [Documents &amp; Productivity](#documents--productivity)
* [Email](#email)
* [Entertainment](#entertainment)
* [Environment](#environment)
* [Events](#events)
* [Finance](#finance)
* [Food &amp; Drink](#food--drink)
* [Games &amp; Comics](#games--comics)
* [Geocoding](#geocoding)
* [Government](#government)
* [Health](#health)
* [Jobs](#jobs)
* [Machine Learning](#machine-learning)
* [Music](#music)
* [News](#news)
* [Open Data](#open-data)
* [Open Source Projects](#open-source-projects)
* [Patent](#patent)
* [Personality](#personality)
* [Phone](#phone)
* [Photography](#photography)
* [Programming](#programming)
* [Science &amp; Math](#science--math)
* [Security](#security)
* [Shopping](#shopping)
* [Social](#social)
* [Sports &amp; Fitness](#sports--fitness)
* [Test Data](#test-data)
* [Text Analysis](#text-analysis)
* [Tracking](#tracking)
* [Transportation](#transportation)
* [URL Shorteners](#url-shorteners)
* [Vehicle](#vehicle)
* [Video](#video)
* [Weather](#weather)
&lt;br &gt;

### Animals
API | Description | Auth | HTTPS | CORS 
|:---|:---|:---|:---|:---|
| [AdoptAPet](https://www.adoptapet.com/public/apis/pet_list.html) | Resource to help get pets adopted | `apiKey` | Yes | Yes |
| [Axolotl](https://theaxolotlapi.netlify.app/) | Collection of axolotl pictures and facts | No | Yes | No |
| [Cat Facts](https://alexwohlbruck.github.io/cat-facts/) | Daily cat facts | No | Yes | No | |
| [Cataas](https://cataas.com/) | Cat as a service (cats pictures and gifs) | No | Yes | No |
| [Cats](https://docs.thecatapi.com/) | Pictures of cats from Tumblr | `apiKey` | Yes | No |
| [Dog Facts](https://dukengn.github.io/Dog-facts-API/) | Random dog facts | No | Yes | Yes |
| [Dog Facts](https://kinduff.github.io/dog-api/) | Random facts of Dogs | No | Yes | Yes |
| [Dogs](https://dog.ceo/dog-api/) | Based on the Stanford Dogs Dataset | No | Yes | Yes |
| [eBird](https://documenter.getpostman.com/view/664302/S1ENwy59) | Retrieve recent or notable birding observations within a region | `apiKey` | Yes | No |
| [FishWatch](https://www.fishwatch.gov/developers) | Information and pictures about individual fish species | No | Yes | Yes |
| [HTTP Cat](https://http.cat/) | Cat for every HTTP Status | No | Yes | Yes |
| [HTTP Dog](https://http.dog/) | Dogs for every HTTP response status code | No | Yes | Yes |
| [IUCN](http://apiv3.iucnredlist.org/api/v3/docs) | IUCN Red List of Threatened Species | `apiKey` | No | No |
| [MeowFacts](https://github.com/wh-iterabb-it/meowfacts) | Get random cat facts | No | Yes | No |
| [Movebank](https://github.com/movebank/movebank-api-doc) | Movement and Migration data of animals | No | Yes | Yes |
| [Petfinder](https://www.petfinder.com/developers/) | Petfinder is dedicated to helping pets find homes, another resource to get pets adopted | `apiKey` | Yes | Yes |
| [PlaceBear](https://placebear.com/) | Placeholder bear pictures | No | Yes | Yes |
| [PlaceDog](https://place.dog) | Placeholder Dog pictures | No | Yes | Yes |
| [PlaceKitten](https://placekitten.com/) | Placeholder Kitten pictures | No | Yes | Yes |
| [RandomDog](https://random.dog/woof.json) | Random pictures of dogs | No | Yes | Yes |
| [RandomDuck](https://random-d.uk/api) | Random pictures of ducks | No | Yes | No |
| [RandomFox](https://randomfox.ca/floof/) | Random pictures of foxes | No | Yes | No |
| [RescueGroups](https://userguide.rescuegroups.org/display/APIDG/API+Developers+Guide+Home) | Adoption | No | Yes | Unknown |
| [Shibe.Online](http://shibe.online/) | Random pictures of Shiba Inu, cats or birds | No | Yes | Yes |
| [The Dog](https://thedogapi.com/) | A public service all about Dogs, free to use when making your fancy new App, Website or Service | `apiKey` | Yes | No |
| [xeno-canto](https://xeno-canto.org/explore/api) | Bird recordings | No | Yes | Unknown |
| [Zoo Animals](https://zoo-animal-api.herokuapp.com/) | Facts and pictures of zoo animals | No | Yes | Yes |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;

### Anime
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AniAPI](https://aniapi.com/docs/) | Anime discovery, streaming &amp; syncing with trackers | `OAuth` | Yes | Yes |
| [AniDB](https://wiki.anidb.net/HTTP_API_Definition) | Anime Database | `apiKey` | No | Unknown |
| [AniList](https://github.com/AniList/ApiV2-GraphQL-Docs) | Anime discovery &amp; tracking | `OAuth` | Yes | Unknown |
| [AnimeChan](https://github.com/RocktimSaikia/anime-chan) | Anime quotes (over 10k+) | No | Yes | No |
| [AnimeFacts](https://chandan-02.github.io/anime-facts-rest-api/) | Anime Facts (over 100+) | No | Yes | Yes |
| [AnimeNewsNetwork](https://www.animenewsnetwork.com/encyclopedia/api.php) | Anime industry news | No | Yes | Yes |
| [Catboy](https://catboys.com/api) | Neko images, funny GIFs &amp; more | No | Yes | Yes |
| [Danbooru Anime](https://danbooru.donmai.us/wiki_pages/help:api) | Thousands of anime artist database to find good anime art | `apiKey` | Yes | Yes |
| [Jikan](https://jikan.moe) | Unofficial MyAnimeList API | No | Yes | Yes |
| [Kitsu](https://kitsu.docs.apiary.io/) | Anime discovery platform | `OAuth` | Yes | Yes |
| [MangaDex](https://api.mangadex.org/docs.html) | Manga Database and Community | `apiKey` | Yes | Unknown |
| [Mangapi](https://rapidapi.com/pierre.carcellermeunier/api/mangapi3/) | Translate manga pages from one language to another | `apiKey` | Yes | Unknown |
| [MyAnimeList](https://myanimelist.net/clubs.php?cid=13727) | Anime and Manga Database and Community | `OAuth` | Yes | Unknown |
| [NekosBest](https://docs.nekos.best) | Neko Images &amp; Anime roleplaying GIFs | No | Yes | Yes |
| [Shikimori](https://shikimori.one/api/doc) | Anime discovery, tracking, forum, rates | `OAuth` | Yes | Unknown |
| [Studio Ghibli](https://ghibliapi.herokuapp.com) | Resources from Studio Ghibli films | No | Yes | Yes |
| [Trace Moe](https://soruly.github.io/trace.moe-api/#/) | A useful tool to get the exact scene of an anime from a screenshot | No | Yes | No |
| [Waifu.im](https://waifu.im/docs) | Get waifu pictures from an archive of over 4000 images and multiple tags | No | Yes | Yes |
| [Waifu.pics](https://waifu.pics/docs) | Image sharing platform for anime images | No | Yes | No |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;

### Anti-Malware
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [AbuseIPDB](https://docs.abuseipdb.com/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [AlienVault Open Threat Exchange (OTX)](https://otx.alienvault.com/api) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [CAPEsandbox](https://capev2.readthedocs.io/en/latest/usage/api.html) | Malware execution and analysis | `apiKey` | Yes | Unknown |
| [Google Safe Browsing](https://developers.google.com/safe-browsing/) | Google Link/Domain Flagging | `apiKey` | Yes | Unknown |
| [MalDatabase](https://maldatabase.com/api-doc.html) | Provide malware datasets and threat intelligence feeds | `apiKey` | Yes | Unknown |
| [MalShare](https://malshare.com/doc.php) | Malware Archive / file sourcing | `apiKey` | Yes | No |
| [MalwareBazaar](https://bazaar.abuse.ch/api/) | Collect and share malware samples | `apiKey` | Yes | Unknown |
| [Metacert](https://metacert.com/) | Metacert Link Flagging | `apiKey` | Yes | Unknown |
| [NoPhishy](https://rapidapi.com/Amiichu/api/exerra-phishing-check/) | Check links to see if they&#039;re known phishing attempts | `apiKey` | Yes | Yes |
| [Phisherman](https://phisherman.gg/) | IP/domain/URL reputation | `apiKey` | Yes | Unknown |
| [Scanii](https://docs.scanii.com/) | Simple REST API that can scan submitted documents/files for the presence of threats | `apiKey` | Yes | Yes |
| [URLhaus](https://urlhaus-api.abuse.ch/) | Bulk queries and Download Malware Samples | No | Yes | Yes |
| [URLScan.io](https://urlscan.io/about-api/) | Scan and Analyse URLs | `apiKey` | Yes | Unknown |
| [VirusTotal](https://www.virustotal.com/en/documentation/public-api/) | VirusTotal File/URL Analysis | `apiKey` | Yes | Unknown |
| [Web of Trust](https://support.mywot.com/hc/en-us/sections/360004477734-API-) | IP/domain/URL reputation | `apiKey` | Yes | Unknown | 

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Art &amp; Design
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [Am√©thyste](https://api.amethyste.moe/) | Generate images for Discord users | `apiKey` | Yes | Unknown |
| [Art Institute of Chicago](https://api.artic.edu/docs/) | Art | No | Yes | Yes |
| [Colormind](http://colormind.io/api-access/) | Color scheme generator | No | No | Unknown |
| [ColourLovers](http://www.colourlovers.com/api) | Get various patterns, palettes and images | No | No | Unknown |
| [Cooper Hewitt](https://collection.cooperhewitt.org/api) | Smithsonian Design Museum | `apiKey` | Yes | Unknown |
| [Dribbble](https://developer.dribbble.com) | Discover the world‚Äôs top designers &amp; creatives | `OAuth` | Yes | Unknown |
| [EmojiHub](https://github.com/cheatsnake/emojihub) | Get emojis by categories and groups | No | Yes | Yes |
| [Europeana](https://pro.europeana.eu/resources/apis/search) | European Museum and Galleries content | `apiKey` | Yes | Unknown |
| [Harvard Art Museums](https://github.com/harvardartmuseums/api-docs) | Art | `apiKey` | No | Unknown |
| [Icon Horse](https://icon.horse) | Favicons for any website, with fallbacks | No | Yes | Yes |
| [Iconfinder](https://developer.iconfinder.com) | Icons | `apiKey` | Yes | Unknown |
| [Icons8](https://img.icons8.com/) | Icons (find &quot;search icon&quot; hyperlink in page) | No | Yes | Unknown |
| [Lordicon](https://lordicon.com/) | Icons with predone Animations | No | Yes | Yes |
| [Metropolitan Museum of Art](https://metmuseum.github.io/) | Met Museum of Art | No | Yes | No |
| [Noun Project](http://api.thenounproject.com/index.html) | Icons | `OAuth` | No | Unknown |
| [PHP-Noise](https://php-noise.com/) | Noise Background Image Generator | No | Yes | Yes |
| [Pixel Encounter](https://pixelencounter.com/api) | SVG Icon Generator | No | Yes | No |
| [Rijksmuseum](https://data.rijksmuseum.nl/object-metadata/api/) | RijksMuseum Data | `apiKey` | Yes | Unknown |
| [Word Cloud](https://wordcloudapi.com/) | Easily create word clouds | `apiKey` | Yes | Unknown |
| [xColors](https://x-colors.herokuapp.com/) | Generate &amp; convert colors | No | Yes | Yes |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Authentication &amp; Authorization
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [Auth0](https://auth0.com) | Easy to implement, adaptable authentication and authorization platform | `apiKey` | Yes | Yes |
| [GetOTP](https://otp.dev/en/docs/) | Implement OTP flow quickly | `apiKey` | Yes | No |
| [Micro User Service](https://m3o.com/user) | User management and authentication | `apiKey` | Yes | No |
| [MojoAuth](https://mojoauth.com) | Secure and modern passwordless authentication platform | `apiKey` | Yes | Yes |
| [SAWO Labs](https://sawolabs.com) | Simplify login and improve user experience by integrating passwordless authentication in your app | `apiKey` | Yes | Yes |
| [Stytch](https://stytch.com/) | User infrastructure for modern applications | `apiKey` | Yes | No |
| [Warrant](https://warrant.dev/) | APIs for authorization and access control | `apiKey` | Yes | Yes |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Blockchain
| API | Description | Auth | HTTPS | CORS |
|---|:---|:---|:---|:---|
| [Bitquery](https://graphql.bitquery.io/ide) | Onchain GraphQL APIs &amp; DEX APIs | `apiKey` | Yes | Yes |
| [Chainlink](https://chain.link/developer-resources) | Build hybrid smart contracts with Chainlink | No | Yes | Unknown |
| [Chainpoint](https://tierion.com/chainpoint/) | Chainpoint is a global network for anchoring data to the Bitcoin blockchain | No | Yes | Unknown |
| [Covalent](https://www.covalenthq.com/docs/api/) | Multi-blockchain data aggregator platform | `apiKey` | Yes | Unknown |
| [Etherscan](https://etherscan.io/apis) | Ethereum explorer API | `apiKey` | Yes | Yes |
| [Helium](https://docs.helium.com/api/blockchain/introduction/) | Helium is a global, distributed network of Hotspots that create public, long-range wireless coverage | No | Yes | Unknown |
| [Nownodes](https://nownodes.io/) | Blockchain-as-a-service solution that provides high-quality connection via API | `apiKey` | Yes | Unknown |
| [Steem](https://developers.steem.io/) | Blockchain-based blogging and social media website | No | No | No |
| [The Graph](https://thegraph.com) | Indexing protocol for querying networks like Ethereum with GraphQL | `apiKey` | Yes | Unknown |
| [Walltime](https://walltime.info/api.html) | To retrieve Walltime&#039;s market info | No | Yes | Unknown |
| [Watchdata](https://docs.watchdata.io) | Provide simple and reliable API access to Ethereum blockchain | `apiKey` | Yes | Unknown |

**[‚¨Ü Back to Index](#index)**
&lt;br &gt;
&lt;br &gt;
### Books
API | Description | Auth | HTTPS | CORS |
|:---|:---|:---|:---|:---|
| [A B√≠blia Digital](https://www.abibliadigital.com.br/en) | Do not worry about managing the multiple versions of the Bible | `apiKey` | Yes | No |
| [Bhagavad Gita](https://docs.bhagavadgitaapi.in) | Open Source Shrimad Bhagavad Gita API including 21+ authors translation in Sanskrit/English/Hindi | `apiKey` | Yes | Yes |
| [Bhagavad Gita](https://bhagavadgita.io/api) | Bhagavad Gita text | `OAuth` | Yes | Yes |
| [Bhagavad Gita telugu](https://gita-api.vercel.app) | Bhagavad Gita API in telugu and odia languages | No | Yes | Yes |
| [Bible-api](https://bible-api.com/) | Free Bible API with multiple languages | No | Yes | Yes |
| [British National Bibliography](http://bnb.data.bl.uk/) | Books | No | No | Unknown |
| [Crossref Metadata Search](https://github.com/CrossRef/rest-api-doc) | Books &amp; Articles Metadata | No | Yes | Unknown |
| [Ganjoor](https://api.ganjoor.net) | Classic Persian poetry works including access to related manuscripts, recitations and music tracks | `OAuth` | Yes | Yes |
| [Google Books](https://developers.google.com/books/) | Books | `OAuth` | Yes | Unknown |
| [GurbaniNow](https://github.com/GurbaniNow/api) | Fast and Accurate Gurbani RESTful API | No | Yes | Unknown |
| [Gutendex](https://gutendex.com/) | Web-API for fetching data from Project Gutenberg Books Library | No | Yes | Unknown |
| [Open Library](https://openlibrary.org/developers/api) | Books, book covers and related data | No | Yes | No |
| [Penguin Publishing](http://www.penguinrandomhouse.biz/webservices/rest/) | Books, book covers and related data | No | Yes | Yes |
| [PoetryDB](https://github.com/thundercomb/poetrydb#readme) | Enables you to get instant data from our vast p

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Azure/Azure-Sentinel]]></title>
            <link>https://github.com/Azure/Azure-Sentinel</link>
            <guid>https://github.com/Azure/Azure-Sentinel</guid>
            <pubDate>Sat, 16 Aug 2025 00:04:12 GMT</pubDate>
            <description><![CDATA[Cloud-native SIEM for intelligent security analytics for your entire enterprise.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Azure/Azure-Sentinel">Azure/Azure-Sentinel</a></h1>
            <p>Cloud-native SIEM for intelligent security analytics for your entire enterprise.</p>
            <p>Language: Python</p>
            <p>Stars: 5,232</p>
            <p>Forks: 3,300</p>
            <p>Stars today: 2 stars today</p>
            <h2>README</h2><pre>
# Microsoft Sentinel and Microsoft 365 Defender 
Welcome to the unified Microsoft Sentinel and Microsoft 365 Defender repository! This repository contains out of the box detections, exploration queries, hunting queries, workbooks, playbooks and much more to help you get ramped up with Microsoft Sentinel and provide you security content to secure your environment and hunt for threats. The hunting queries also include Microsoft 365 Defender hunting queries for advanced hunting scenarios in both Microsoft 365 Defender and Microsoft Sentinel. You can also submit to [issues](https://github.com/Azure/Azure-Sentinel/issues) for any samples or resources you would like to see here as you onboard to Microsoft Sentinel. This repository welcomes contributions and refer to this repository&#039;s [wiki](https://aka.ms/threathunters) to get started. For questions and feedback, please contact [AzureSentinel@microsoft.com](AzureSentinel@microsoft.com) 

# Resources
* [Microsoft Sentinel documentation](https://go.microsoft.com/fwlink/?linkid=2073774&amp;clcid=0x409)
* [Microsoft 365 Defender documentation](https://docs.microsoft.com/microsoft-365/security/defender/microsoft-365-defender?view=o365-worldwide)
* [Security Community Webinars](https://aka.ms/securitywebinars)
* [Getting started with GitHub](https://help.github.com/en#dotcom)

We value your feedback. Here are some channels to help surface your questions or feedback:
1. General product specific Q&amp;A for SIEM and SOAR - Join in the [Microsoft Sentinel Tech Community conversations](https://techcommunity.microsoft.com/t5/microsoft-sentinel/bd-p/MicrosoftSentinel)
2. General product specific Q&amp;A for XDR - Join in the [Microsoft 365 Defender Tech Community conversations](https://techcommunity.microsoft.com/t5/microsoft-365-defender/bd-p/MicrosoftThreatProtection)
3. Product specific feature requests - Upvote or post new on [Microsoft Sentinel feedback forums](https://feedback.azure.com/d365community/forum/37638d17-0625-ec11-b6e6-000d3a4f07b8)
4. Report product or contribution bugs - File a GitHub Issue using [Bug template](https://github.com/Azure/Azure-Sentinel/issues/new?assignees=&amp;labels=&amp;template=bug_report.md&amp;title=)
5. General feedback on community and contribution process - File a GitHub Issue using [Feature Request template](https://github.com/Azure/Azure-Sentinel/issues/new?assignees=&amp;labels=&amp;template=feature_request.md&amp;title=)


# Contribution guidelines

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.microsoft.com.

## Add in your new or updated contributions to GitHub
Note: If you are a first time contributor to this repository, [General GitHub Fork the repo guidance](https://docs.github.com/github/getting-started-with-github/fork-a-repo) before cloning or [Specific steps for the Sentinel repo](https://github.com/Azure/Azure-Sentinel/blob/master/GettingStarted.md). 

## General Steps
Brand new or update to a contribution via these methods:
* Submit for review directly on GitHub website 
    * Browse to the folder you want to upload your file to
    * Choose Upload Files and browse to your file. 
    * You will be required to create your own branch and then submit the Pull Request for review.
* Use [GitHub Desktop](https://docs.github.com/en/desktop/overview/getting-started-with-github-desktop) or [Visual Studio](https://visualstudio.microsoft.com/vs/) or [VSCode](https://code.visualstudio.com/?wt.mc_id=DX_841432)
    * [Fork the repo](https://docs.github.com/github/getting-started-with-github/fork-a-repo)  
    * [Clone the repo](https://help.github.com/en/github/creating-cloning-and-archiving-repositories/cloning-a-repository)
    * [Create your own branch](https://help.github.com/en/desktop/contributing-to-projects/creating-a-branch-for-your-work)
    * Do your additions/updates in GitHub Desktop
    * Be sure to merge master back to your branch before you push. 
    * [Push your changes to GitHub](https://help.github.com/en/github/using-git/pushing-commits-to-a-remote-repository)

## Pull Request
* After you push your changes, you will need to submit the [Pull Request (PR)](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests)
* Details about the Proposed Changes are required, be sure to include a minimal level of detail so a review can clearly understand the reason for the change and what he change is related to in the code.
* After submission, check the [Pull Request](https://github.com/Azure/Azure-Sentinel/pulls) for comments
* Make changes as suggested and update your branch or explain why no change is needed. Resolve the comment when done.

### Pull Request Detection Template Structure Validation Check
As part of the PR checks we run a structure validation to make sure all required parts of the YAML structure are included.  For Detections, there is a new section that must be included.  See the [contribution guidelines](https://github.com/Azure/Azure-Sentinel/wiki/Contribute-to-Sentinel-GitHub-Community-of-Queries#now-onto-the-how) for more information.  If this section or any other required section is not included, then a validation error will occur similar to the below.
The example is specifically if the YAML is missing the entityMappings section:

```
A total of 1 test files matched the specified pattern.
[xUnit.net 00:00:00.95]     Kqlvalidations.Tests.DetectionTemplateStructureValidationTests.Validate_DetectionTemplates_HaveValidTemplateStructure(detectionsYamlFileName: &quot;ExcessiveBlockedTrafficGeneratedbyUser.yaml&quot;) [FAIL]
  X Kqlvalidations.Tests.DetectionTemplateStructureValidationTests.Validate_DetectionTemplates_HaveValidTemplateStructure(detectionsYamlFileName: &quot;ExcessiveBlockedTrafficGeneratedbyUser.yaml&quot;) [104ms]
  Error Message:
   Expected object to be &lt;null&gt;, but found System.ComponentModel.DataAnnotations.ValidationException with message &quot;An old mapping for entity &#039;AccountCustomEntity&#039; does not have a matching new mapping entry.&quot;
```

### Pull Request KQL Validation Check
As part of the PR checks we run a syntax validation of the KQL queries defined in the template. If this check fails go to Azure Pipeline (by pressing on the errors link on the checks tab in your PR)
![Azurepipeline](.github/Media/Azurepipeline.png)
In the pipeline you can see which test failed and what is the cause:
![Pipeline Tests Tab](.github/Media/PipelineTestsTab.png)

Example error message:
```
A total of 1 test files matched the specified pattern.
[xUnit.net 00:00:01.81]     Kqlvalidations.Tests.KqlValidationTests.Validate_DetectionQueries_HaveValidKql(detectionsYamlFileName: &quot;ExcessiveBlockedTrafficGeneratedbyUser.yaml&quot;) [FAIL]
  X Kqlvalidations.Tests.KqlValidationTests.Validate_DetectionQueries_HaveValidKql(detectionsYamlFileName: &quot;ExcessiveBlockedTrafficGeneratedbyUser.yaml&quot;) [21ms]
  Error Message:
   Template Id:fa0ab69c-7124-4f62-acdd-61017cf6ce89 is not valid Errors:The name &#039;SymantecEndpointProtection&#039; does not refer to any known table, tabular variable or function., Code: &#039;KS204&#039;, Severity: &#039;Error&#039;, Location: &#039;67..93&#039;,The name &#039;SymantecEndpointProtection&#039; does not refer to any known table, tabular variable or function., Code: &#039;KS204&#039;, Severity: &#039;Error&#039;, Location: &#039;289..315&#039;
```
If you are using custom logs table (a table which is not defined on all workspaces by default) you should verify
your table schema is defined in json file in the folder *Azure-Sentinel\\.script\tests\KqlvalidationsTests\CustomTables*

**Example for table tablexyz.json**
```json
{
  &quot;Name&quot;: &quot;tablexyz&quot;,
  &quot;Properties&quot;: [
    {
      &quot;Name&quot;: &quot;SomeDateTimeColumn&quot;,
      &quot;Type&quot;: &quot;DateTime&quot;
    },
    {
      &quot;Name&quot;: &quot;SomeStringColumn&quot;,
      &quot;Type&quot;: &quot;String&quot;
    },
    {
      &quot;Name&quot;: &quot;SomeDynamicColumn&quot;,
      &quot;Type&quot;: &quot;Dynamic&quot;
    }
  ]
}
```
### Run KQL Validation Locally
In order to run the KQL validation before submitting Pull Request in you local machine:
* You need to have **.Net Core 3.1 SDK** installed [How to download .Net](https://dotnet.microsoft.com/download) (Supports all platforms)
* Open Shell and navigate to  `Azure-Sentinel\\.script\tests\KqlvalidationsTests\`
* Execute `dotnet test`

Example of output (in Ubuntu):
```
Welcome to .NET Core 3.1!
----------------------
SDK Version: 3.1.403

Telemetry
---------
The .NET Core tools collect usage data in order to help us improve your experience. The data is anonymous. It is collected by Microsoft and shared with the community. You can opt-out of telemetry by setting the DOTNET_CLI_TELEMETRY_OPTOUT environment variable to &#039;1&#039; or &#039;true&#039; using your favorite shell.

Read more about .NET Core CLI Tools telemetry: https://aka.ms/dotnet-cli-telemetry

----------------
Explore documentation: https://aka.ms/dotnet-docs
Report issues and find source on GitHub: https://github.com/dotnet/core
Find out what&#039;s new: https://aka.ms/dotnet-whats-new
Learn about the installed HTTPS developer cert: https://aka.ms/aspnet-core-https
Use &#039;dotnet --help&#039; to see available commands or visit: https://aka.ms/dotnet-cli-docs
Write your first app: https://aka.ms/first-net-core-app
--------------------------------------------------------------------------------------
Test run for /mnt/c/git/Azure-Sentinel/.script/tests/KqlvalidationsTests/bin/Debug/netcoreapp3.1/Kqlvalidations.Tests.dll(.NETCoreApp,Version=v3.1)
Microsoft (R) Test Execution Command Line Tool Version 16.7.0
Copyright (c) Microsoft Corporation.  All rights reserved.

Starting test execution, please wait...

A total of 1 test files matched the specified pattern.

Test Run Successful.
Total tests: 171
     Passed: 171
 Total time: 25.7973 Seconds
```

### Detection schema validation tests
Similarly to KQL Validation, there is an automatic validation of the schema of a detection.
The schema validation includes the detection&#039;s frequency and period, the detection&#039;s trigger type and threshold, validity of connectors Ids ([valid connectors Ids list](https://github.com/Azure/Azure-Sentinel/blob/master/.script/tests/detectionTemplateSchemaValidation/ValidConnectorIds.json)), etc.
A wrong format or missing attributes will result with an informative check failure, which should guide you through the resolution of the issue, but make sure to look into the format of already approved detection.

### Run Detection Schema Validation Locally
In order to run the KQL validation before submitting Pull Request in you local machine:
* You need to have **.Net Core 3.1 SDK** installed [How to download .Net](https://dotnet.microsoft.com/download) (Supports all platforms)
* Open Shell and navigate to  `Azure-Sentinel\\.script\tests\DetectionTemplateSchemaValidation\`
* Execute `dotnet test`


When you submit a pull request, a CLA-bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

For information on what you can contribute and further details, refer to the [&quot;get started&quot;](https://github.com/Azure/Azure-Sentinel/wiki#get-started) section on the project&#039;s [wiki](https://aka.ms/threathunters). 
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[langchain-ai/open_deep_research]]></title>
            <link>https://github.com/langchain-ai/open_deep_research</link>
            <guid>https://github.com/langchain-ai/open_deep_research</guid>
            <pubDate>Sat, 16 Aug 2025 00:04:11 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/langchain-ai/open_deep_research">langchain-ai/open_deep_research</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 7,795</p>
            <p>Forks: 1,030</p>
            <p>Stars today: 142 stars today</p>
            <h2>README</h2><pre># üî¨ Open Deep Research

&lt;img width=&quot;1388&quot; height=&quot;298&quot; alt=&quot;full_diagram&quot; src=&quot;https://github.com/user-attachments/assets/12a2371b-8be2-4219-9b48-90503eb43c69&quot; /&gt;

Deep research has broken out as one of the most popular agent applications. This is a simple, configurable, fully open source deep research agent that works across many model providers, search tools, and MCP servers. It&#039;s performance is on par with many popular deep research agents ([see Deep Research Bench leaderboard](https://huggingface.co/spaces/Ayanami0730/DeepResearch-Leaderboard)).

&lt;img width=&quot;817&quot; height=&quot;666&quot; alt=&quot;Screenshot 2025-07-13 at 11 21 12‚ÄØPM&quot; src=&quot;https://github.com/user-attachments/assets/052f2ed3-c664-4a4f-8ec2-074349dcaa3f&quot; /&gt;

### üî• Recent Updates

**August 7, 2025**: Added GPT-5 and updated the Deep Research Bench evaluation w/ GPT-5 results.

**August 2, 2025**: Achieved #6 ranking on the [Deep Research Bench Leaderboard](https://huggingface.co/spaces/Ayanami0730/DeepResearch-Leaderboard) with an overall score of 0.4344. 

**July 30, 2025**: Read about the evolution from our original implementations to the current version in our [blog post](https://rlancemartin.github.io/2025/07/30/bitter_lesson/).

**July 16, 2025**: Read more in our [blog](https://blog.langchain.com/open-deep-research/) and watch our [video](https://www.youtube.com/watch?v=agGiWUpxkhg) for a quick overview.

### üöÄ Quickstart

1. Clone the repository and activate a virtual environment:
```bash
git clone https://github.com/langchain-ai/open_deep_research.git
cd open_deep_research
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

2. Install dependencies:
```bash
uv sync
# or
uv pip install -r pyproject.toml
```

3. Set up your `.env` file to customize the environment variables (for model selection, search tools, and other configuration settings):
```bash
cp .env.example .env
```

4. Launch agent with the LangGraph server locally:

```bash
# Install dependencies and start the LangGraph server
uvx --refresh --from &quot;langgraph-cli[inmem]&quot; --with-editable . --python 3.11 langgraph dev --allow-blocking
```

This will open the LangGraph Studio UI in your browser.

```
- üöÄ API: http://127.0.0.1:2024
- üé® Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024
- üìö API Docs: http://127.0.0.1:2024/docs
```

Ask a question in the `messages` input field and click `Submit`. Select different configuration in the &quot;Manage Assistants&quot; tab.

### ‚öôÔ∏è Configurations

#### LLM :brain:

Open Deep Research supports a wide range of LLM providers via the [init_chat_model() API](https://python.langchain.com/docs/how_to/chat_models_universal_init/). It uses LLMs for a few different tasks. See the below model fields in the [configuration.py](https://github.com/langchain-ai/open_deep_research/blob/main/src/open_deep_research/configuration.py) file for more details. This can be accessed via the LangGraph Studio UI. 

- **Summarization** (default: `openai:gpt-4.1-mini`): Summarizes search API results
- **Research** (default: `openai:gpt-4.1`): Power the search agent
- **Compression** (default: `openai:gpt-4.1`): Compresses research findings
- **Final Report Model** (default: `openai:gpt-4.1`): Write the final report

&gt; Note: the selected model will need to support [structured outputs](https://python.langchain.com/docs/integrations/chat/) and [tool calling](https://python.langchain.com/docs/how_to/tool_calling/).

&gt; Note: For OpenRouter: Follow [this guide](https://github.com/langchain-ai/open_deep_research/issues/75#issuecomment-2811472408) and for local models via Ollama  see [setup instructions](https://github.com/langchain-ai/open_deep_research/issues/65#issuecomment-2743586318).

#### Search API :mag:

Open Deep Research supports a wide range of search tools. By default it uses the [Tavily](https://www.tavily.com/) search API. Has full MCP compatibility and work native web search for Anthropic and OpenAI. See the `search_api` and `mcp_config` fields in the [configuration.py](https://github.com/langchain-ai/open_deep_research/blob/main/src/open_deep_research/configuration.py) file for more details. This can be accessed via the LangGraph Studio UI. 

#### Other 

See the fields in the [configuration.py](https://github.com/langchain-ai/open_deep_research/blob/main/src/open_deep_research/configuration.py) for various other settings to customize the behavior of Open Deep Research. 

### üìä Evaluation

Open Deep Research is configured for evaluation with [Deep Research Bench](https://huggingface.co/spaces/Ayanami0730/DeepResearch-Leaderboard). This benchmark has 100 PhD-level research tasks (50 English, 50 Chinese), crafted by domain experts across 22 fields (e.g., Science &amp; Tech, Business &amp; Finance) to mirror real-world deep-research needs. It has 2 evaluation metrics, but the leaderboard is based on the RACE score. This uses LLM-as-a-judge (Gemini) to evaluate research reports against a golden set of reports compiled by experts across a set of metrics.

#### Usage

&gt; Warning: Running across the 100 examples can cost ~$20-$100 depending on the model selection.

The dataset is available on [LangSmith via this link](https://smith.langchain.com/public/c5e7a6ad-fdba-478c-88e6-3a388459ce8b/d). To kick off evaluation, run the following command:

```bash
# Run comprehensive evaluation on LangSmith datasets
python tests/run_evaluate.py
```

This will provide a link to a LangSmith experiment, which will have a name `YOUR_EXPERIMENT_NAME`. Once this is done, extract the results to a JSONL file that can be submitted to the Deep Research Bench.

```bash
python tests/extract_langsmith_data.py --project-name &quot;YOUR_EXPERIMENT_NAME&quot; --model-name &quot;you-model-name&quot; --dataset-name &quot;deep_research_bench&quot;
```

This creates `tests/expt_results/deep_research_bench_model-name.jsonl` with the required format. Move the generated JSONL file to a local clone of the Deep Research Bench repository and follow their [Quick Start guide](https://github.com/Ayanami0730/deep_research_bench?tab=readme-ov-file#quick-start) for evaluation submission.

#### Results 

| Name | Commit | Summarization | Research | Compression | Total Cost | Total Tokens | RACE Score | Experiment |
|------|--------|---------------|----------|-------------|------------|--------------|------------|------------|
| GPT-5 | [ca3951d](https://github.com/langchain-ai/open_deep_research/pull/168/commits) | openai:gpt-4.1-mini | openai:gpt-5 | openai:gpt-4.1 |  | 204,640,896 | 0.4943 | [Link](https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/6e4766ca-613c-4bda-8bde-f64f0422bbf3/compare?selectedSessions=4d5941c8-69ce-4f3d-8b3e-e3c99dfbd4cc&amp;baseline=undefined) |
| Defaults | [6532a41](https://github.com/langchain-ai/open_deep_research/commit/6532a4176a93cc9bb2102b3d825dcefa560c85d9) | openai:gpt-4.1-mini | openai:gpt-4.1 | openai:gpt-4.1 | $45.98 | 58,015,332 | 0.4309 | [Link](https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/6e4766ca-6[‚Ä¶]ons=cf4355d7-6347-47e2-a774-484f290e79bc&amp;baseline=undefined) |
| Claude Sonnet 4 | [f877ea9](https://github.com/langchain-ai/open_deep_research/pull/163/commits/f877ea93641680879c420ea991e998b47aab9bcc) | openai:gpt-4.1-mini | anthropic:claude-sonnet-4-20250514 | openai:gpt-4.1 | $187.09 | 138,917,050 | 0.4401 | [Link](https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/6e4766ca-6[‚Ä¶]ons=04f6002d-6080-4759-bcf5-9a52e57449ea&amp;baseline=undefined) |
| Deep Research Bench Submission | [c0a160b](https://github.com/langchain-ai/open_deep_research/commit/c0a160b57a9b5ecd4b8217c3811a14d8eff97f72) | openai:gpt-4.1-nano | openai:gpt-4.1 | openai:gpt-4.1 | $87.83 | 207,005,549 | 0.4344 | [Link](https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/6e4766ca-6[‚Ä¶]ons=e6647f74-ad2f-4cb9-887e-acb38b5f73c0&amp;baseline=undefined) |

### üöÄ Deployments and Usage

#### LangGraph Studio

Follow the [quickstart](#-quickstart) to start LangGraph server locally and test the agent out on LangGraph Studio.

#### Hosted deployment
 
You can easily deploy to [LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/#deployment-options). 

#### Open Agent Platform

Open Agent Platform (OAP) is a UI from which non-technical users can build and configure their own agents. OAP is great for allowing users to configure the Deep Researcher with different MCP tools and search APIs that are best suited to their needs and the problems that they want to solve.

We&#039;ve deployed Open Deep Research to our public demo instance of OAP. All you need to do is add your API Keys, and you can test out the Deep Researcher for yourself! Try it out [here](https://oap.langchain.com)

You can also deploy your own instance of OAP, and make your own custom agents (like Deep Researcher) available on it to your users.
1. [Deploy Open Agent Platform](https://docs.oap.langchain.com/quickstart)
2. [Add Deep Researcher to OAP](https://docs.oap.langchain.com/setup/agents)

### Legacy Implementations üèõÔ∏è

The `src/legacy/` folder contains two earlier implementations that provide alternative approaches to automated research. They are less performant than the current implementation, but provide alternative ideas understanding the different approaches to deep research.

#### 1. Workflow Implementation (`legacy/graph.py`)
- **Plan-and-Execute**: Structured workflow with human-in-the-loop planning
- **Sequential Processing**: Creates sections one by one with reflection
- **Interactive Control**: Allows feedback and approval of report plans
- **Quality Focused**: Emphasizes accuracy through iterative refinement

#### 2. Multi-Agent Implementation (`legacy/multi_agent.py`)  
- **Supervisor-Researcher Architecture**: Coordinated multi-agent system
- **Parallel Processing**: Multiple researchers work simultaneously
- **Speed Optimized**: Faster report generation through concurrency
- **MCP Support**: Extensive Model Context Protocol integration
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sqlfluff/sqlfluff]]></title>
            <link>https://github.com/sqlfluff/sqlfluff</link>
            <guid>https://github.com/sqlfluff/sqlfluff</guid>
            <pubDate>Sat, 16 Aug 2025 00:04:10 GMT</pubDate>
            <description><![CDATA[A modular SQL linter and auto-formatter with support for multiple dialects and templated code.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sqlfluff/sqlfluff">sqlfluff/sqlfluff</a></h1>
            <p>A modular SQL linter and auto-formatter with support for multiple dialects and templated code.</p>
            <p>Language: Python</p>
            <p>Stars: 9,110</p>
            <p>Forks: 861</p>
            <p>Stars today: 8 stars today</p>
            <h2>README</h2><pre>![SQLFluff](https://raw.githubusercontent.com/sqlfluff/sqlfluff/main/images/sqlfluff-wide.png)

# The SQL Linter for Humans

[![PyPi Version](https://img.shields.io/pypi/v/sqlfluff.svg?style=flat-square&amp;logo=PyPi)](https://pypi.org/project/sqlfluff/)
[![PyPi License](https://img.shields.io/pypi/l/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)
[![PyPi Python Versions](https://img.shields.io/pypi/pyversions/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)
[![PyPi Status](https://img.shields.io/pypi/status/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)
[![PyPi Downloads](https://img.shields.io/pypi/dm/sqlfluff?style=flat-square)](https://pypi.org/project/sqlfluff/)

[![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/sqlfluff/sqlfluff/.github/workflows/ci-tests.yml?logo=github&amp;style=flat-square)](https://github.com/sqlfluff/sqlfluff/actions/workflows/ci-tests.yml?query=branch%3Amain)
[![ReadTheDocs](https://img.shields.io/readthedocs/sqlfluff?style=flat-square&amp;logo=Read%20the%20Docs)](https://sqlfluff.readthedocs.io)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/psf/black)
[![Docker Pulls](https://img.shields.io/docker/pulls/sqlfluff/sqlfluff?logo=docker&amp;style=flat-square)](https://hub.docker.com/r/sqlfluff/sqlfluff)
[![Gurubase](https://img.shields.io/badge/Gurubase-Ask%20SQLFluff%20Guru-006BFF?style=flat-square)](https://gurubase.io/g/sqlfluff)

**SQLFluff** is a dialect-flexible and configurable SQL linter. Designed
with [ELT](https://www.techtarget.com/searchdatamanagement/definition/Extract-Load-Transform-ELT) applications in mind, **SQLFluff** also works with Jinja templating
and dbt. **SQLFluff** will auto-fix most linting errors, allowing you to focus
your time on what matters.

## Table of Contents

1. [Dialects Supported](#dialects-supported)
2. [Templates Supported](#templates-supported)
3. [VS Code Extension](#vs-code-extension)
4. [Getting Started](#getting-started)
5. [Documentation](#documentation)
6. [Releases](#releases)
7. [SQLFluff on Slack](#sqlfluff-on-slack)
8. [SQLFluff on Twitter](#sqlfluff-on-twitter)
9. [Contributing](#contributing)
10. [Sponsors](#sponsors)

## Dialects Supported

Although SQL is reasonably consistent in its implementations, there are several
different dialects available with variations of syntax and grammar. **SQLFluff**
currently supports the following SQL dialects (though perhaps not in full):

- ANSI SQL - this is the base version and on occasion may not strictly follow
  the ANSI/ISO SQL definition
- [Athena](https://aws.amazon.com/athena/)
- [BigQuery](https://cloud.google.com/bigquery/)
- [ClickHouse](https://clickhouse.com/)
- [Databricks](https://databricks.com/) (note: this extends the `sparksql` dialect with
  [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html) syntax).
- [Db2](https://www.ibm.com/analytics/db2)
- [Doris](https://doris.apache.org/)
- [DuckDB](https://duckdb.org/)
- [Exasol](https://www.exasol.com/)
- [Greenplum](https://greenplum.org/)
- [Hive](https://hive.apache.org/)
- [Impala](https://impala.apache.org/)
- [MariaDB](https://www.mariadb.com/)
- [Materialize](https://materialize.com/)
- [MySQL](https://www.mysql.com/)
- [Oracle](https://docs.oracle.com/en/database/oracle/oracle-database/21/sqlrf/index.html)
- [PostgreSQL](https://www.postgresql.org/) (aka Postgres)
- [Redshift](https://docs.aws.amazon.com/redshift/index.html)
- [Snowflake](https://www.snowflake.com/)
- [SOQL](https://developer.salesforce.com/docs/atlas.en-us.soql_sosl.meta/soql_sosl/sforce_api_calls_soql.htm)
- [SparkSQL](https://spark.apache.org/docs/latest/)
- [SQLite](https://www.sqlite.org/)
- [StarRocks](https://www.starrocks.io)
- [Teradata](https://www.teradata.com/)
- [Transact-SQL](https://docs.microsoft.com/en-us/sql/t-sql/language-reference) (aka T-SQL)
- [Trino](https://trino.io/)
- [Vertica](https://www.vertica.com/)

We aim to make it easy to expand on the support of these dialects and also
add other, currently unsupported, dialects. Please [raise issues](https://github.com/sqlfluff/sqlfluff/issues)
(or upvote any existing issues) to let us know of demand for missing support.

Pull requests from those that know the missing syntax or dialects are especially
welcomed and are the question way for you to get support added. We are happy
to work with any potential contributors on this to help them add this support.
Please raise an issue first for any large feature change to ensure it is a good
fit for this project before spending time on this work.

## Templates Supported

SQL itself does not lend itself well to [modularity](https://docs.getdbt.com/docs/viewpoint#section-modularity),
so to introduce some flexibility and reusability it is often [templated](https://en.wikipedia.org/wiki/Template_processor)
as discussed more in [our modularity documentation](https://docs.sqlfluff.com/en/stable/perma/modularity.html).

**SQLFluff** supports the following templates:

- [Jinja](https://jinja.palletsprojects.com/) (aka Jinja2)
- SQL placeholders (e.g. SQLAlchemy parameters)
- [Python format strings](https://docs.python.org/3/library/string.html#format-string-syntax)
- [dbt](https://www.getdbt.com/) (requires plugin)

Again, please raise issues if you wish to support more templating languages/syntaxes.

## VS Code Extension

We also have a VS Code extension:

- [Github Repository](https://github.com/sqlfluff/vscode-sqlfluff)
- [Extension in VS Code marketplace](https://marketplace.visualstudio.com/items?itemName=dorzey.vscode-sqlfluff)

# Getting Started

To get started, install the package and run `sqlfluff lint` or `sqlfluff fix`.

```shell
$ pip install sqlfluff
$ echo &quot;  SELECT a  +  b FROM tbl;  &quot; &gt; test.sql
$ sqlfluff lint test.sql --dialect ansi
== [test.sql] FAIL
L:   1 | P:   1 | LT01 | Expected only single space before &#039;SELECT&#039; keyword.
                       | Found &#039;  &#039;. [layout.spacing]
L:   1 | P:   1 | LT02 | First line should not be indented.
                       | [layout.indent]
L:   1 | P:   1 | LT13 | Files must not begin with newlines or whitespace.
                       | [layout.start_of_file]
L:   1 | P:  11 | LT01 | Expected only single space before binary operator &#039;+&#039;.
                       | Found &#039;  &#039;. [layout.spacing]
L:   1 | P:  14 | LT01 | Expected only single space before naked identifier.
                       | Found &#039;  &#039;. [layout.spacing]
L:   1 | P:  27 | LT01 | Unnecessary trailing whitespace at end of file.
                       | [layout.spacing]
L:   1 | P:  27 | LT12 | Files must end with a single trailing newline.
                       | [layout.end_of_file]
All Finished üìú üéâ!
```

Alternatively, you can use the [**Official SQLFluff Docker Image**](https://hub.docker.com/r/sqlfluff/sqlfluff)
or have a play using [**SQLFluff online**](https://online.sqlfluff.com/).

For full [CLI usage](https://docs.sqlfluff.com/en/stable/perma/cli.html) and
[rules reference](https://docs.sqlfluff.com/en/stable/perma/rules.html), see
[the SQLFluff docs](https://docs.sqlfluff.com/en/stable/).

# Documentation

For full documentation visit [docs.sqlfluff.com](https://docs.sqlfluff.com/en/stable/).
This documentation is generated from this repository so please raise
[issues](https://github.com/sqlfluff/sqlfluff/issues) or pull requests
for any additions, corrections, or clarifications.

# Releases

**SQLFluff** adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html),
so breaking changes should be restricted to major versions releases. Some
elements (such as the python API) are in a less stable state and may see more
significant changes more often. For details on breaking changes and how
to migrate between versions, see our
[release notes](https://docs.sqlfluff.com/en/latest/perma/releasenotes.html). See the
[changelog](CHANGELOG.md) for more details. If you would like to join in, please
consider [contributing](CONTRIBUTING.md).

New releases are made monthly. For more information, visit
[Releases](https://github.com/sqlfluff/sqlfluff/releases).

# SQLFluff on Slack

We have a fast-growing community
[on Slack](https://join.slack.com/t/sqlfluff/shared_invite/zt-2qtu36kdt-OS4iONPbQ3aCz2DIbYJdWg),
come and join us!

# SQLFluff on Twitter

Follow us [on Twitter @SQLFluff](https://twitter.com/SQLFluff) for announcements
and other related posts.

# Contributing

We are grateful to all our [contributors](https://github.com/sqlfluff/sqlfluff/graphs/contributors).
There is a lot to do in this project, and we are just getting started.

If you want to understand more about the architecture of **SQLFluff**, you can
find [more here](https://docs.sqlfluff.com/en/latest/perma/architecture.html).

If you would like to contribute, check out the
[open issues on GitHub](https://github.com/sqlfluff/sqlfluff/issues). You can also see
the guide to [contributing](CONTRIBUTING.md).

# Sponsors

&lt;img src=&quot;images/datacoves.png&quot; alt=&quot;Datacoves&quot; width=&quot;150&quot;/&gt;&lt;br&gt;
The turnkey analytics stack, find out more at [Datacoves.com](https://datacoves.com/).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[coleam00/Archon]]></title>
            <link>https://github.com/coleam00/Archon</link>
            <guid>https://github.com/coleam00/Archon</guid>
            <pubDate>Sat, 16 Aug 2025 00:04:09 GMT</pubDate>
            <description><![CDATA[Beta release of Archon OS - the knowledge and task management backbone for AI coding assistants.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/coleam00/Archon">coleam00/Archon</a></h1>
            <p>Beta release of Archon OS - the knowledge and task management backbone for AI coding assistants.</p>
            <p>Language: Python</p>
            <p>Stars: 5,901</p>
            <p>Forks: 1,235</p>
            <p>Stars today: 318 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./archon-ui-main/public/archon-main-graphic.png&quot; alt=&quot;Archon Main Graphic&quot; width=&quot;853&quot; height=&quot;422&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;em&gt;Power up your AI coding assistants with your own custom knowledge base and task management as an MCP server&lt;/em&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;#quick-start&quot;&gt;Quick Start&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;#whats-included&quot;&gt;What&#039;s Included&lt;/a&gt; ‚Ä¢
  &lt;a href=&quot;#architecture&quot;&gt;Architecture&lt;/a&gt;
&lt;/p&gt;

---

## üéØ What is Archon?

&gt; Archon is currently in beta! Expect things to not work 100%, and please feel free to share any feedback and contribute with fixes/new features!

Archon is the **command center** for AI coding assistants. For you, it&#039;s a sleek interface to manage knowledge, context, and tasks for your projects. For the AI coding assistant(s), it&#039;s a **Model Context Protocol (MCP) server** to collaborate on and leverage the same knowledge, context, and tasks. Connect Claude Code, Kiro, Cursor, Windsurf, etc. to give your AI agents access to:

- **Your documentation** (crawled websites, uploaded PDFs/docs)
- **Smart search capabilities** with advanced RAG strategies  
- **Task management** integrated with your knowledge base
- **Real-time updates** as you add new content and collaborate with your coding assistant on tasks
- **Much more** coming soon to build Archon into an integrated environment for all context engineering

This new vision for Archon replaces the old one (the agenteer). Archon used to be the AI agent that builds other agents, and now you can use Archon to do that and more.

&gt; It doesn&#039;t matter what you&#039;re building or if it&#039;s a new/existing codebase - Archon&#039;s knowledge and task management capabilities will improve the output of **any** AI driven coding.

## üîó Important Links

- **[GitHub Discussions](https://github.com/coleam00/Archon/discussions)** - Join the conversation and share ideas about Archon
- **[Contributing Guide](CONTRIBUTING.md)** - How to get involved and contribute to Archon
- **[Introduction Video](https://youtu.be/8pRc_s2VQIo)** - Getting Started Guide and Vision for Archon
- **[Dynamous AI Mastery](https://dynamous.ai)** - The birthplace of Archon - come join a vibrant community of other early AI adopters all helping each other transform their careers and businesses!

## Quick Start

### Prerequisites
- [Docker Desktop](https://www.docker.com/products/docker-desktop/)
- [Supabase](https://supabase.com/) account (free tier or local Supabase both work)
- [OpenAI API key](https://platform.openai.com/api-keys) (Gemini and Ollama are supported too!)

### Setup Instructions

1. **Clone Repository**:
   ```bash
   git clone https://github.com/coleam00/archon.git
   cd archon
   ```

2. **Environment Configuration**:
   ```bash
   cp .env.example .env
   # Edit .env and add your Supabase credentials:
   # SUPABASE_URL=https://your-project.supabase.co
   # SUPABASE_SERVICE_KEY=your-service-key-here
   ```

   NOTE: Supabase introduced a new type of service key but use the legacy one (the longer one).

   OPTIONAL: If you want to enable the reranking RAG strategy, uncomment lines 20-22 in `python\requirements.server.txt`. This will significantly increase the size of the Archon Server container which is why it&#039;s off by default.

3. **Database Setup**: In your [Supabase project](https://supabase.com/dashboard) SQL Editor, copy, paste, and execute the contents of `migration/complete_setup.sql`

4. **Start Services**:
   ```bash
   docker-compose up --build -d
   ```
   
   This starts the core microservices:
   - **Server**: Core API and business logic (Port: 8181)
   - **MCP Server**: Protocol interface for AI clients (Port: 8051)
   - **Agents (coming soon!)**: AI operations and streaming (Port: 8052)
   - **UI**: Web interface (Port: 3737)

   Ports are configurable in your .env as well!

5. **Configure API Keys**:
   - Open http://localhost:3737
   - Go to **Settings** ‚Üí Select your LLM/embedding provider and set the API key (OpenAI is default)
   - Test by uploading a document or crawling a website

## üîÑ Database Reset (Start Fresh if Needed)

If you need to completely reset your database and start fresh:

&lt;details&gt;
&lt;summary&gt;‚ö†Ô∏è &lt;strong&gt;Reset Database - This will delete ALL data for Archon!&lt;/strong&gt;&lt;/summary&gt;

1. **Run Reset Script**: In your Supabase SQL Editor, run the contents of `migration/RESET_DB.sql`
   
   ‚ö†Ô∏è WARNING: This will delete all Archon specific tables and data! Nothing else will be touched in your DB though.

2. **Rebuild Database**: After reset, run `migration/complete_setup.sql` to create all the tables again.

3. **Restart Services**:
   ```bash
   docker-compose up -d
   ```

4. **Reconfigure**: 
   - Select your LLM/embedding provider and set the API key again
   - Re-upload any documents or re-crawl websites

The reset script safely removes all tables, functions, triggers, and policies with proper dependency handling.

&lt;/details&gt;

## ‚ö° Quick Test

Once everything is running:

1. **Test Web Crawling**: Go to http://localhost:3737 ‚Üí Knowledge Base ‚Üí &quot;Crawl Website&quot; ‚Üí Enter a doc URL (such as https://ai.pydantic.dev/llms-full.txt)
2. **Test Document Upload**: Knowledge Base ‚Üí Upload a PDF
3. **Test Projects**: Projects ‚Üí Create a new project and add tasks
4. **Integrate with your AI coding assistant**: MCP Dashboard ‚Üí Copy connection config for your AI coding assistant

## üìö Documentation

### Core Services

| Service | Container Name | Default URL | Purpose |
|---------|---------------|-------------|---------|
| **Web Interface** | archon-ui | http://localhost:3737 | Main dashboard and controls |
| **API Service** | archon-server | http://localhost:8181 | Web crawling, document processing |
| **MCP Server** | archon-mcp | http://localhost:8051 | Model Context Protocol interface |
| **Agents Service** | archon-agents | http://localhost:8052 | AI/ML operations, reranking |

## What&#039;s Included

### üß† Knowledge Management
- **Smart Web Crawling**: Automatically detects and crawls entire documentation sites, sitemaps, and individual pages
- **Document Processing**: Upload and process PDFs, Word docs, markdown files, and text documents with intelligent chunking
- **Code Example Extraction**: Automatically identifies and indexes code examples from documentation for enhanced search
- **Vector Search**: Advanced semantic search with contextual embeddings for precise knowledge retrieval
- **Source Management**: Organize knowledge by source, type, and tags for easy filtering

### ü§ñ AI Integration  
- **Model Context Protocol (MCP)**: Connect any MCP-compatible client (Claude Code, Cursor, even non-AI coding assistants like Claude Desktop)
- **10 MCP Tools**: Comprehensive yet simple set of tools for RAG queries, task management, and project operations
- **Multi-LLM Support**: Works with OpenAI, Ollama, and Google Gemini models
- **RAG Strategies**: Hybrid search, contextual embeddings, and result reranking for optimal AI responses
- **Real-time Streaming**: Live responses from AI agents with progress tracking

### üìã Project &amp; Task Management
- **Hierarchical Projects**: Organize work with projects, features, and tasks in a structured workflow
- **AI-Assisted Creation**: Generate project requirements and tasks using integrated AI agents  
- **Document Management**: Version-controlled documents with collaborative editing capabilities
- **Progress Tracking**: Real-time updates and status management across all project activities

### üîÑ Real-time Collaboration
- **WebSocket Updates**: Live progress tracking for crawling, processing, and AI operations
- **Multi-user Support**: Collaborative knowledge building and project management
- **Background Processing**: Asynchronous operations that don&#039;t block the user interface
- **Health Monitoring**: Built-in service health checks and automatic reconnection

## Architecture

### Microservices Structure

Archon uses true microservices architecture with clear separation of concerns:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Frontend UI   ‚îÇ    ‚îÇ  Server (API)   ‚îÇ    ‚îÇ   MCP Server    ‚îÇ    ‚îÇ Agents Service  ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ  React + Vite   ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ    FastAPI +    ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ    Lightweight  ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   PydanticAI    ‚îÇ
‚îÇ  Port 3737      ‚îÇ    ‚îÇ    SocketIO     ‚îÇ    ‚îÇ    HTTP Wrapper ‚îÇ    ‚îÇ   Port 8052     ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ    Port 8181    ‚îÇ    ‚îÇ    Port 8051    ‚îÇ    ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                        ‚îÇ                        ‚îÇ                        ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ                        ‚îÇ
                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
                         ‚îÇ    Database     ‚îÇ               ‚îÇ
                         ‚îÇ                 ‚îÇ               ‚îÇ
                         ‚îÇ    Supabase     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ    PostgreSQL   ‚îÇ
                         ‚îÇ    PGVector     ‚îÇ
                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Service Responsibilities

| Service | Location | Purpose | Key Features |
|---------|----------|---------|--------------|
| **Frontend** | `archon-ui-main/` | Web interface and dashboard | React, TypeScript, TailwindCSS, Socket.IO client |
| **Server** | `python/src/server/` | Core business logic and APIs | FastAPI, service layer, Socket.IO broadcasts, all ML/AI operations |
| **MCP Server** | `python/src/mcp/` | MCP protocol interface | Lightweight HTTP wrapper, 10 MCP tools, session management |
| **Agents** | `python/src/agents/` | PydanticAI agent hosting | Document and RAG agents, streaming responses |

### Communication Patterns

- **HTTP-based**: All inter-service communication uses HTTP APIs
- **Socket.IO**: Real-time updates from Server to Frontend  
- **MCP Protocol**: AI clients connect to MCP Server via SSE or stdio
- **No Direct Imports**: Services are truly independent with no shared code dependencies

### Key Architectural Benefits

- **Lightweight Containers**: Each service contains only required dependencies
- **Independent Scaling**: Services can be scaled independently based on load
- **Development Flexibility**: Teams can work on different services without conflicts
- **Technology Diversity**: Each service uses the best tools for its specific purpose

## üîß Configuring Custom Ports &amp; Hostname

By default, Archon services run on the following ports:
- **Archon-UI**: 3737
- **Archon-Server**: 8181  
- **Archon-MCP**: 8051
- **Archon-Agents**: 8052
- **Archon-Docs**: 3838 (optional)

### Changing Ports

To use custom ports, add these variables to your `.env` file:

```bash
# Service Ports Configuration
ARCHON_UI_PORT=3737
ARCHON_SERVER_PORT=8181
ARCHON_MCP_PORT=8051
ARCHON_AGENTS_PORT=8052
ARCHON_DOCS_PORT=3838
```

Example: Running on different ports:
```bash
ARCHON_SERVER_PORT=8282
ARCHON_MCP_PORT=8151
```

### Configuring Hostname

By default, Archon uses `localhost` as the hostname. You can configure a custom hostname or IP address by setting the `HOST` variable in your `.env` file:

```bash
# Hostname Configuration
HOST=localhost  # Default

# Examples of custom hostnames:
HOST=192.168.1.100     # Use specific IP address
HOST=archon.local      # Use custom domain
HOST=myserver.com      # Use public domain
```

This is useful when:
- Running Archon on a different machine and accessing it remotely
- Using a custom domain name for your installation
- Deploying in a network environment where `localhost` isn&#039;t accessible

After changing hostname or ports:
1. Restart Docker containers: `docker-compose down &amp;&amp; docker-compose up -d`
2. Access the UI at: `http://${HOST}:${ARCHON_UI_PORT}`
3. Update your AI client configuration with the new hostname and MCP port

## üîß Development

For development with hot reload:

```bash
# Backend services (with auto-reload)
docker-compose up archon-server archon-mcp archon-agents --build

# Frontend (with hot reload) 
cd archon-ui-main &amp;&amp; npm run dev

# Documentation (with hot reload)
cd docs &amp;&amp; npm start
```

**Note**: The backend services are configured with `--reload` flag in their uvicorn commands and have source code mounted as volumes for automatic hot reloading when you make changes.

## üìÑ License

Archon Community License (ACL) v1.2 - see [LICENSE](LICENSE) file for details.

**TL;DR**: Archon is free, open, and hackable. Run it, fork it, share it - just don&#039;t sell it as-a-service without permission.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>