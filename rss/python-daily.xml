<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Tue, 08 Jul 2025 00:04:25 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[commaai/openpilot]]></title>
            <link>https://github.com/commaai/openpilot</link>
            <guid>https://github.com/commaai/openpilot</guid>
            <pubDate>Tue, 08 Jul 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/commaai/openpilot">commaai/openpilot</a></h1>
            <p>openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.</p>
            <p>Language: Python</p>
            <p>Stars: 54,683</p>
            <p>Forks: 9,893</p>
            <p>Stars today: 229 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; style=&quot;text-align: center;&quot;&gt;

&lt;h1&gt;openpilot&lt;/h1&gt;

&lt;p&gt;
  &lt;b&gt;openpilot is an operating system for robotics.&lt;/b&gt;
  &lt;br&gt;
  Currently, it upgrades the driver assistance system in 300+ supported cars.
&lt;/p&gt;

&lt;h3&gt;
  &lt;a href=&quot;https://docs.comma.ai&quot;&gt;Docs&lt;/a&gt;
  &lt;span&gt; · &lt;/span&gt;
  &lt;a href=&quot;https://docs.comma.ai/contributing/roadmap/&quot;&gt;Roadmap&lt;/a&gt;
  &lt;span&gt; · &lt;/span&gt;
  &lt;a href=&quot;https://github.com/commaai/openpilot/blob/master/docs/CONTRIBUTING.md&quot;&gt;Contribute&lt;/a&gt;
  &lt;span&gt; · &lt;/span&gt;
  &lt;a href=&quot;https://discord.comma.ai&quot;&gt;Community&lt;/a&gt;
  &lt;span&gt; · &lt;/span&gt;
  &lt;a href=&quot;https://comma.ai/shop&quot;&gt;Try it on a comma 3X&lt;/a&gt;
&lt;/h3&gt;

Quick start: `bash &lt;(curl -fsSL openpilot.comma.ai)`

[![openpilot tests](https://github.com/commaai/openpilot/actions/workflows/selfdrive_tests.yaml/badge.svg)](https://github.com/commaai/openpilot/actions/workflows/selfdrive_tests.yaml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![X Follow](https://img.shields.io/twitter/follow/comma_ai)](https://x.com/comma_ai)
[![Discord](https://img.shields.io/discord/469524606043160576)](https://discord.comma.ai)

&lt;/div&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/NmBfgOanCyk&quot; title=&quot;Video By Greer Viau&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/2f7112ae-f748-4f39-b617-fabd689c3772&quot;&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/VHKyqZ7t8Gw&quot; title=&quot;Video By Logan LeGrand&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/92351544-2833-40d7-9e0b-7ef7ae37ec4c&quot;&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/SUIZYzxtMQs&quot; title=&quot;A drive to Taco Bell&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/05ceefc5-2628-439c-a9b2-89ce77dc6f63&quot;&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;


Using openpilot in a car
------

To use openpilot in a car, you need four things:
1. **Supported Device:** a comma 3/3X, available at [comma.ai/shop](https://comma.ai/shop/comma-3x).
2. **Software:** The setup procedure for the comma 3/3X allows users to enter a URL for custom software. Use the URL `openpilot.comma.ai` to install the release version.
3. **Supported Car:** Ensure that you have one of [the 275+ supported cars](docs/CARS.md).
4. **Car Harness:** You will also need a [car harness](https://comma.ai/shop/car-harness) to connect your comma 3/3X to your car.

We have detailed instructions for [how to install the harness and device in a car](https://comma.ai/setup). Note that it&#039;s possible to run openpilot on [other hardware](https://blog.comma.ai/self-driving-car-for-free/), although it&#039;s not plug-and-play.

### Branches
| branch           | URL                                    | description                                                                         |
|------------------|----------------------------------------|-------------------------------------------------------------------------------------|
| `release3`         | openpilot.comma.ai                      | This is openpilot&#039;s release branch.                                                 |
| `release3-staging` | openpilot-test.comma.ai                | This is the staging branch for releases. Use it to get new releases slightly early. |
| `nightly`          | openpilot-nightly.comma.ai             | This is the bleeding edge development branch. Do not expect this to be stable.      |
| `nightly-dev`      | installer.comma.ai/commaai/nightly-dev | Same as nightly, but includes experimental development features for some cars.      |
| `secretgoodopenpilot` | installer.comma.ai/commaai/secretgoodopenpilot | This is a preview branch from the autonomy team where new driving models get merged earlier than master. |

To start developing openpilot
------

openpilot is developed by [comma](https://comma.ai/) and by users like you. We welcome both pull requests and issues on [GitHub](http://github.com/commaai/openpilot).

* Join the [community Discord](https://discord.comma.ai)
* Check out [the contributing docs](docs/CONTRIBUTING.md)
* Check out the [openpilot tools](tools/)
* Code documentation lives at https://docs.comma.ai
* Information about running openpilot lives on the [community wiki](https://github.com/commaai/openpilot/wiki)

Want to get paid to work on openpilot? [comma is hiring](https://comma.ai/jobs#open-positions) and offers lots of [bounties](https://comma.ai/bounties) for external contributors.

Safety and Testing
----

* openpilot observes [ISO26262](https://en.wikipedia.org/wiki/ISO_26262) guidelines, see [SAFETY.md](docs/SAFETY.md) for more details.
* openpilot has software-in-the-loop [tests](.github/workflows/selfdrive_tests.yaml) that run on every commit.
* The code enforcing the safety model lives in panda and is written in C, see [code rigor](https://github.com/commaai/panda#code-rigor) for more details.
* panda has software-in-the-loop [safety tests](https://github.com/commaai/panda/tree/master/tests/safety).
* Internally, we have a hardware-in-the-loop Jenkins test suite that builds and unit tests the various processes.
* panda has additional hardware-in-the-loop [tests](https://github.com/commaai/panda/blob/master/Jenkinsfile).
* We run the latest openpilot in a testing closet containing 10 comma devices continuously replaying routes.

&lt;details&gt;
&lt;summary&gt;MIT Licensed&lt;/summary&gt;

openpilot is released under the MIT license. Some parts of the software are released under other licenses as specified.

Any user of this software shall indemnify and hold harmless Comma.ai, Inc. and its directors, officers, employees, agents, stockholders, affiliates, subcontractors and customers from and against all allegations, claims, actions, suits, demands, damages, liabilities, obligations, losses, settlements, judgments, costs and expenses (including without limitation attorneys’ fees and costs) which arise out of, relate to or result from any use of this software by user.

**THIS IS ALPHA QUALITY SOFTWARE FOR RESEARCH PURPOSES ONLY. THIS IS NOT A PRODUCT.
YOU ARE RESPONSIBLE FOR COMPLYING WITH LOCAL LAWS AND REGULATIONS.
NO WARRANTY EXPRESSED OR IMPLIED.**
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;User Data and comma Account&lt;/summary&gt;

By default, openpilot uploads the driving data to our servers. You can also access your data through [comma connect](https://connect.comma.ai/). We use your data to train better models and improve openpilot for everyone.

openpilot is open source software: the user is free to disable data collection if they wish to do so.

openpilot logs the road-facing cameras, CAN, GPS, IMU, magnetometer, thermal sensors, crashes, and operating system logs.
The driver-facing camera and microphone are only logged if you explicitly opt-in in settings.

By using openpilot, you agree to [our Privacy Policy](https://comma.ai/privacy). You understand that use of this software or its related services will generate certain types of user data, which may be logged and stored at the sole discretion of comma. By accepting this agreement, you grant an irrevocable, perpetual, worldwide right to comma for the use of this data.
&lt;/details&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[topoteretes/cognee]]></title>
            <link>https://github.com/topoteretes/cognee</link>
            <guid>https://github.com/topoteretes/cognee</guid>
            <pubDate>Tue, 08 Jul 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[Memory for AI Agents in 5 lines of code]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/topoteretes/cognee">topoteretes/cognee</a></h1>
            <p>Memory for AI Agents in 5 lines of code</p>
            <p>Language: Python</p>
            <p>Stars: 6,227</p>
            <p>Forks: 480</p>
            <p>Stars today: 58 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/topoteretes/cognee&quot;&gt;
    &lt;img src=&quot;https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/dev/assets/cognee-logo-transparent.png&quot; alt=&quot;Cognee Logo&quot; height=&quot;60&quot;&gt;
  &lt;/a&gt;

  &lt;br /&gt;

  cognee - Memory for AI Agents in 5 lines of code

  &lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=1bezuvLwJmw&amp;t=2s&quot;&gt;Demo&lt;/a&gt;
  .
  &lt;a href=&quot;https://cognee.ai&quot;&gt;Learn more&lt;/a&gt;
  ·
  &lt;a href=&quot;https://discord.gg/NQPKmU5CCg&quot;&gt;Join Discord&lt;/a&gt;
  ·
  &lt;a href=&quot;https://www.reddit.com/r/AIMemory/&quot;&gt;Join r/AIMemory&lt;/a&gt;
  .
  &lt;a href=&quot;https://www.docs.cognee.ai&quot;&gt;Docs&lt;/a&gt;
  .
  &lt;a href=&quot;https://github.com/topoteretes/cognee-community&quot;&gt;cognee community repo&lt;/a&gt;
  &lt;/p&gt;


  [![GitHub forks](https://img.shields.io/github/forks/topoteretes/cognee.svg?style=social&amp;label=Fork&amp;maxAge=2592000)](https://GitHub.com/topoteretes/cognee/network/)
  [![GitHub stars](https://img.shields.io/github/stars/topoteretes/cognee.svg?style=social&amp;label=Star&amp;maxAge=2592000)](https://GitHub.com/topoteretes/cognee/stargazers/)
  [![GitHub commits](https://badgen.net/github/commits/topoteretes/cognee)](https://GitHub.com/topoteretes/cognee/commit/)
  [![Github tag](https://badgen.net/github/tag/topoteretes/cognee)](https://github.com/topoteretes/cognee/tags/)
  [![Downloads](https://static.pepy.tech/badge/cognee)](https://pepy.tech/project/cognee)
  [![License](https://img.shields.io/github/license/topoteretes/cognee?colorA=00C586&amp;colorB=000000)](https://github.com/topoteretes/cognee/blob/main/LICENSE)
  [![Contributors](https://img.shields.io/github/contributors/topoteretes/cognee?colorA=00C586&amp;colorB=000000)](https://github.com/topoteretes/cognee/graphs/contributors)
  &lt;a href=&quot;https://github.com/sponsors/topoteretes&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Sponsor-❤️-ff69b4.svg&quot; alt=&quot;Sponsor&quot;&gt;&lt;/a&gt;

&lt;p&gt;
  &lt;a href=&quot;https://www.producthunt.com/posts/cognee?embed=true&amp;utm_source=badge-top-post-badge&amp;utm_medium=badge&amp;utm_souce=badge-cognee&quot; target=&quot;_blank&quot; style=&quot;display:inline-block; margin-right:10px;&quot;&gt;
    &lt;img src=&quot;https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=946346&amp;theme=light&amp;period=daily&amp;t=1744472480704&quot; alt=&quot;cognee - Memory&amp;#0032;for&amp;#0032;AI&amp;#0032;Agents&amp;#0032;&amp;#0032;in&amp;#0032;5&amp;#0032;lines&amp;#0032;of&amp;#0032;code | Product Hunt&quot; width=&quot;250&quot; height=&quot;54&quot; /&gt;
  &lt;/a&gt;

  &lt;a href=&quot;https://trendshift.io/repositories/13955&quot; target=&quot;_blank&quot; style=&quot;display:inline-block;&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/13955&quot; alt=&quot;topoteretes%2Fcognee | Trendshift&quot; width=&quot;250&quot; height=&quot;55&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;



**🚀 We are launching Cognee SaaS: Sign up [here](https://www.cognee.ai/waitlist) for the hosted beta!**

Build dynamic memory for Agents and replace RAG using scalable, modular ECL (Extract, Cognify, Load) pipelines.

More on [use-cases](https://docs.cognee.ai/use-cases) and [evals](https://github.com/topoteretes/cognee/tree/main/evals)

  &lt;p align=&quot;center&quot;&gt;
  🌐 Available Languages
  :
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://www.readme-i18n.com/topoteretes/cognee?lang=de&quot;&gt;Deutsch&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/topoteretes/cognee?lang=es&quot;&gt;Español&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/topoteretes/cognee?lang=fr&quot;&gt;français&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/topoteretes/cognee?lang=ja&quot;&gt;日本語&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/topoteretes/cognee?lang=ko&quot;&gt;한국어&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/topoteretes/cognee?lang=pt&quot;&gt;Português&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/topoteretes/cognee?lang=ru&quot;&gt;Русский&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/topoteretes/cognee?lang=zh&quot;&gt;中文&lt;/a&gt;
  &lt;/p&gt;


&lt;div style=&quot;text-align: center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_benefits.png&quot; alt=&quot;Why cognee?&quot; width=&quot;50%&quot; /&gt;
&lt;/div&gt;
&lt;/div&gt;


## Features

- Interconnect and retrieve your past conversations, documents, images and audio transcriptions
- Replaces RAG systems and reduces developer effort, and cost.
- Load data to graph and vector databases using only Pydantic
- Manipulate your data while ingesting from 30+ data sources

## Get Started

Get started quickly with a Google Colab  &lt;a href=&quot;https://colab.research.google.com/drive/1jHbWVypDgCLwjE71GSXhRL3YxYhCZzG1?usp=sharing&quot;&gt;notebook&lt;/a&gt; , &lt;a href=&quot;https://deepnote.com/workspace/cognee-382213d0-0444-4c89-8265-13770e333c02/project/cognee-demo-78ffacb9-5832-4611-bb1a-560386068b30/notebook/Notebook-1-75b24cda566d4c24ab348f7150792601?utm_source=share-modal&amp;utm_medium=product-shared-content&amp;utm_campaign=notebook&amp;utm_content=78ffacb9-5832-4611-bb1a-560386068b30&quot;&gt;Deepnote notebook&lt;/a&gt; or  &lt;a href=&quot;https://github.com/topoteretes/cognee-starter&quot;&gt;starter repo&lt;/a&gt;


## Contributing
Your contributions are at the core of making this a true open source project. Any contributions you make are **greatly appreciated**. See [`CONTRIBUTING.md`](CONTRIBUTING.md) for more information.





## 📦 Installation

You can install Cognee using either **pip**, **poetry**, **uv** or any other python package manager.
Cognee supports Python 3.8 to 3.12

### With pip

```bash
pip install cognee
```

## Local Cognee installation

You can install the local Cognee repo using **pip**, **poetry** and **uv**.
For local pip installation please make sure your pip version is above version 21.3.

### with UV with all optional dependencies

```bash
uv sync --all-extras
```

## 💻 Basic Usage

### Setup

```
import os
os.environ[&quot;LLM_API_KEY&quot;] = &quot;YOUR OPENAI_API_KEY&quot;

```

You can also set the variables by creating .env file, using our &lt;a href=&quot;https://github.com/topoteretes/cognee/blob/main/.env.template&quot;&gt;template.&lt;/a&gt;
To use different LLM providers, for more info check out our &lt;a href=&quot;https://docs.cognee.ai&quot;&gt;documentation&lt;/a&gt;


### Simple example

This script will run the default pipeline:

```python
import cognee
import asyncio


async def main():
    # Add text to cognee
    await cognee.add(&quot;Natural language processing (NLP) is an interdisciplinary subfield of computer science and information retrieval.&quot;)

    # Generate the knowledge graph
    await cognee.cognify()

    # Query the knowledge graph
    results = await cognee.search(&quot;Tell me about NLP&quot;)

    # Display the results
    for result in results:
        print(result)


if __name__ == &#039;__main__&#039;:
    asyncio.run(main())

```
Example output:
```
  Natural Language Processing (NLP) is a cross-disciplinary and interdisciplinary field that involves computer science and information retrieval. It focuses on the interaction between computers and human language, enabling machines to understand and process natural language.

```

## Our paper is out! &lt;a href=&quot;https://arxiv.org/abs/2505.24478&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;Read here&lt;/a&gt;

&lt;div style=&quot;text-align: center&quot;&gt;
  &lt;img src=&quot;assets/cognee-paper.png&quot; alt=&quot;cognee paper&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

## Cognee UI

You can also cognify your files and query using cognee UI.

&lt;img src=&quot;assets/cognee-ui-2.webp&quot; width=&quot;100%&quot; alt=&quot;Cognee UI 2&quot;&gt;&lt;/a&gt;

Try cognee UI out locally [here](https://docs.cognee.ai/how-to-guides/cognee-ui).

## Understand our architecture

&lt;div style=&quot;text-align: center&quot;&gt;
  &lt;img src=&quot;assets/cognee_diagram.png&quot; alt=&quot;cognee concept diagram&quot; width=&quot;100%&quot; /&gt;
&lt;/div&gt;



## Demos

1. What is AI memory:

[Learn about cognee](https://github.com/user-attachments/assets/8b2a0050-5ec4-424c-b417-8269971503f0)

2. Simple GraphRAG demo

[Simple GraphRAG demo](https://github.com/user-attachments/assets/d80b0776-4eb9-4b8e-aa22-3691e2d44b8f)

3. cognee with Ollama

[cognee with local models](https://github.com/user-attachments/assets/8621d3e8-ecb8-4860-afb2-5594f2ee17db)


## Code of Conduct

We are committed to making open source an enjoyable and respectful experience for our community. See &lt;a href=&quot;https://github.com/topoteretes/cognee/blob/main/CODE_OF_CONDUCT.md&quot;&gt;&lt;code&gt;CODE_OF_CONDUCT&lt;/code&gt;&lt;/a&gt; for more information.

## 💫 Contributors

&lt;a href=&quot;https://github.com/topoteretes/cognee/graphs/contributors&quot;&gt;
  &lt;img alt=&quot;contributors&quot; src=&quot;https://contrib.rocks/image?repo=topoteretes/cognee&quot;/&gt;
&lt;/a&gt;


## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=topoteretes/cognee&amp;type=Date)](https://star-history.com/#topoteretes/cognee&amp;Date)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[megadose/toutatis]]></title>
            <link>https://github.com/megadose/toutatis</link>
            <guid>https://github.com/megadose/toutatis</guid>
            <pubDate>Tue, 08 Jul 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[Toutatis is a tool that allows you to extract information from instagrams accounts such as e-mails, phone numbers and more]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/megadose/toutatis">megadose/toutatis</a></h1>
            <p>Toutatis is a tool that allows you to extract information from instagrams accounts such as e-mails, phone numbers and more</p>
            <p>Language: Python</p>
            <p>Stars: 2,790</p>
            <p>Forks: 418</p>
            <p>Stars today: 23 stars today</p>
            <h2>README</h2><pre># Toutatis
👋 Hi there! For any professional inquiries or collaborations, please reach out to me at:
megadose@protonmail.com

📧 Preferably, use your professional email for correspondence. Let&#039;s keep it short and sweet, and all in English!

Toutatis is a tool that allows you to extract information from instagrams accounts such as e-mails, phone numbers and more &lt;/br&gt;
For BTC Donations : 1FHDM49QfZX6pJmhjLE5tB2K6CaTLMZpXZ
## 💡 Prerequisite
[Python 3](https://www.python.org/downloads/release/python-370/)

## 🛠️ Installation
### With PyPI

```pip install toutatis```

### With Github

```bash
git clone https://github.com/megadose/toutatis.git
cd toutatis/
python3 setup.py install
```

## 📚 Usage:

### Find information from a username

```
toutatis -u username -s instagramsessionid
```

### Find information from an Instagram ID

```
toutatis -i instagramID -s instagramsessionid
```

## 📈 Example

```
Informations about     : xxxusernamexxx
Full Name              : xxxusernamesxx | userID : 123456789
Verified               : False | Is buisness Account : False
Is private Account     : False
Follower               : xxx | Following : xxx
Number of posts        : x
Number of tag in posts : x
External url           : http://example.com
IGTV posts             : x
Biography              : example biography
Public Email           : public@example.com
Public Phone           : +00 0 00 00 00 00
Obfuscated email       : me********s@examplemail.com
Obfuscated phone       : +00 0xx xxx xx 00
------------------------
Profile Picture        : https://scontent-X-X.cdninstagram.com/
```

## 📚 To retrieve the sessionID
![](https://files.catbox.moe/1rfi6j.png)

## Thank you to :

- [EyupErgin](https://github.com/eyupergin)
- [yazeed44](https://github.com/yazeed44)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[WEIFENG2333/VideoCaptioner]]></title>
            <link>https://github.com/WEIFENG2333/VideoCaptioner</link>
            <guid>https://github.com/WEIFENG2333/VideoCaptioner</guid>
            <pubDate>Tue, 08 Jul 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[🎬 卡卡字幕助手 | VideoCaptioner - 基于 LLM 的智能字幕助手 - 视频字幕生成、断句、校正、字幕翻译全流程处理！- A powered tool for easy and efficient video subtitling.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/WEIFENG2333/VideoCaptioner">WEIFENG2333/VideoCaptioner</a></h1>
            <p>🎬 卡卡字幕助手 | VideoCaptioner - 基于 LLM 的智能字幕助手 - 视频字幕生成、断句、校正、字幕翻译全流程处理！- A powered tool for easy and efficient video subtitling.</p>
            <p>Language: Python</p>
            <p>Stars: 8,100</p>
            <p>Forks: 650</p>
            <p>Stars today: 107 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./docs/images/logo.png&quot;alt=&quot;VideoCaptioner Logo&quot; width=&quot;100&quot;&gt;
  &lt;p&gt;卡卡字幕助手&lt;/p&gt;
  &lt;h1&gt;VideoCaptioner&lt;/h1&gt;
  &lt;p&gt;一款基于大语言模型(LLM)的视频字幕处理助手，支持语音识别、字幕断句、优化、翻译全流程处理&lt;/p&gt;

  简体中文 / [正體中文](./docs/README_TW.md) / [English](./docs/README_EN.md) / [日本語](./docs/README_JA.md)
  
&lt;/div&gt;

## 📖 项目介绍

卡卡字幕助手（VideoCaptioner）操作简单且无需高配置，支持网络调用和本地离线（支持调用GPU）两种方式进行语音识别，利用可用通过大语言模型进行字幕智能断句、校正、翻译，字幕视频全流程一键处理！为视频配上效果惊艳的字幕。

最新版本已经支持 VAD 、 人声分离、 字级时间戳 批量字幕等实用功能

- 🎯 无需GPU即可使用强大的语音识别引擎，生成精准字幕
- ✂️ 基于 LLM 的智能分割与断句，字幕阅读更自然流畅
- 🔄 AI字幕多线程优化与翻译，调整字幕格式、表达更地道专业
- 🎬 支持批量视频字幕合成，提升处理效率
- 📝 直观的字幕编辑查看界面，支持实时预览和快捷编辑
- 🤖 消耗模型 Token 少，且内置基础 LLM 模型，保证开箱即用

## 📸 界面预览

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://h1.appinn.me/file/1731487405884_main.png&quot; alt=&quot;软件界面预览&quot; width=&quot;90%&quot; style=&quot;border-radius: 5px;&quot;&gt;
&lt;/div&gt;

![页面预览](https://h1.appinn.me/file/1731487410170_preview1.png)
![页面预览](https://h1.appinn.me/file/1731487410832_preview2.png)


## 🧪 测试

全流程处理一个14分钟1080P的 [B站英文 TED 视频](https://www.bilibili.com/video/BV1jT411X7Dz)，调用本地 Whisper 模型进行语音识别，使用 `gpt-4o-mini` 模型优化和翻译为中文，总共消耗时间约 **4 分钟**。

 近后台计算，模型优化和翻译消耗费用不足 ￥0.01（以OpenAI官方价格为计算）

具体字幕和视频合成的效果的测试结果图片，请参考 [TED视频测试](./docs/test.md)


## 🚀 快速开始

### Windows 用户

软件较为轻量，打包大小不足 60M,已集成所有必要环境，下载后可直接运行。

1. 从 [Release](https://github.com/WEIFENG2333/VideoCaptioner/releases) 页面下载最新版本的可执行程序。或者：[蓝奏盘下载](https://wwwm.lanzoue.com/ii14G2pdsbej)

2. 打开安装包进行安装

3. LLM API 配置，（用于字幕断句、校正），可使用 [✨本项目的中转站 ](https://api.videocaptioner.cn) 

4. 翻译配置，选择是否启用翻译，翻译服务（默认使用微软翻译，质量一般，推荐使用大模型翻译）

5. 语音识别配置（默认使用B接口，中英以外的语言请使用本地转录）

6. 拖拽视频文件到软件窗口，即可全自动处理

提示：每一个步骤均支持单独处理，均支持文件拖拽。软件具体模型选择和参数配置说明，请查看下文。

&lt;details&gt;
&lt;summary&gt;MacOS 用户&lt;/summary&gt;
 
 
由于本人缺少 Mac，所以没法测试和打包，暂无法提供 MacOS 的可执行程序。

Mac 用户请自行使用下载源码和安装 python 依赖运行。（本地 Whisper 功能暂不支持 MacOS）

1. 安装 ffmpeg 和 Aria2 下载工具
```bash
brew install ffmpeg
brew install aria2
brew install python@3.**
```

2. 克隆项目
```bash
git clone https://github.com/WEIFENG2333/VideoCaptioner.git
cd VideoCaptioner
```

3. 安装依赖
```bash
python3.** -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

4. 运行程序
```bash
python main.py
```
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;Docker 部署（beta）&lt;/summary&gt;

目前本项目streamlit应用因为项目重构过，Docker不可以使用。欢迎各位PR贡献新代码。

### 1. 克隆项目

```bash
git clone https://github.com/WEIFENG2333/VideoCaptioner.git
cd VideoCaptioner

```

### 2. 构建镜像

```bash
docker build -t video-captioner .
```

### 3. 运行容器

使用自定义API配置运行：
```bash
docker run -d \
  -p 8501:8501 \
  -v $(pwd)/temp:/app/temp \
  -e OPENAI_BASE_URL=&quot;你的API地址&quot; \
  -e OPENAI_API_KEY=&quot;你的API密钥&quot; \
  --name video-captioner \
  video-captioner
```

### 4. 访问应用

打开浏览器访问：`http://localhost:8501`

### 注意事项

- 容器内已预装ffmpeg等必要依赖
- 如需使用其他模型，请通过环境变量配置

&lt;/details&gt;

## ⚙️ 基本配置

### 1. LLM API 配置说明

LLM 大模型是用来字幕段句、字幕优化、以及字幕翻译（如果选择了LLM 大模型翻译）。

| 配置项 | 说明 |
|--------|------|
| SiliconCloud | [SiliconCloud 官网](https://cloud.siliconflow.cn/i/onCHcaDx)配置方法请参考[配置文档](./docs/llm_config.md)&lt;br&gt;该并发较低，建议把线程设置为5以下。 |
| DeepSeek | [DeepSeek 官网](https://platform.deepseek.com)，建议使用 `deepseek-v3` 模型，&lt;br&gt;官方网站最近服务好像并不太稳定。 |
| Ollama本地 | [Ollama 官网](https://ollama.com) |
| 内置公益模型 | 内置基础大语言模型（`gpt-4o-mini`）(公益服务不稳定，强烈建议请使用自己的模型API) |
| OpenAI兼容接口 | 如果有其他服务商的API，可直接在软件中填写。base_url 和api_key |

注：如果用的 API 服务商不支持高并发，请在软件设置中将“线程数”调低，避免请求错误。

---

如果希望高并发⚡️，或者希望在在软件内使用使用 OpenAI 或者 Claude 等优质大模型进行字幕校正和翻译。

可使用本项目的✨LLM API中转站✨： [https://api.videocaptioner.cn](https://api.videocaptioner.cn)

其支持高并发，性价比极高，且有国内外大量模型可挑选。

注册获取key之后，设置中按照下面配置：

BaseURL: `https://api.videocaptioner.cn/v1`

API-key: `个人中心-API 令牌页面自行获取。`

💡 模型选择建议 (本人在各质量层级中精选出的高性价比模型)： 

 - 高质量之选： `claude-3-5-sonnet-20241022` (耗费比例：3) 

 - 较高质量之选： `gemini-2.0-flash`、`deepseek-chat` (耗费比例：1) 

 - 中质量之选： `gpt-4o-mini`、`gemini-1.5-flash` (耗费比例：0.15) 

本站支持超高并发，软件中线程数直接拉满即可~ 处理速度非常快~

更详细的API配置教程：[中转站配置配置](./docs/llm_config.md#中转站配置)

---

## 2. 翻译配置

| 配置项 | 说明 |
|--------|------|
| LLM 大模型翻译 | 🌟 翻译质量最好的选择。使用 AI 大模型进行翻译,能更好理解上下文,翻译更自然。需要在设置中配置 LLM API(比如 OpenAI、DeepSeek 等) |
| DeepLx 翻译 |  翻译较可靠。基于 DeepL 翻译, 需要要配置自己的后端接口。 |
| 微软翻译 | 使用微软的翻译服务, 速度非常快 |
| 谷歌翻译 | 谷歌的翻译服务,速度快,但需要能访问谷歌的网络环境 |

推荐使用 `LLM 大模型翻译` ，翻译质量最好。


### 3. 语音识别接口说明

| 接口名称 | 支持语言 | 运行方式 | 说明 |
|---------|---------|---------|------|
| B接口 | 仅支持中文、英文 | 在线 | 免费、速度较快 |
| J接口 | 仅支持中文、英文 | 在线 | 免费、速度较快 |
| WhisperCpp | 中文、日语、韩语、英文等 99 种语言，外语效果较好 | 本地 | （实际使用不稳定）需要下载转录模型&lt;br&gt;中文建议medium以上模型&lt;br&gt;英文等使用较小模型即可达到不错效果。 |
| fasterWhisper 👍 | 中文、英文等多99种语言，外语效果优秀，时间轴更准确 | 本地 | （🌟极力推荐🌟）需要下载程序和转录模型&lt;br&gt;支持CUDA,速度更快，转录准确。&lt;br&gt;超级准确的时间戳字幕。&lt;br&gt;建议优先使用 |


### 4. 本地 Whisper 语音识别模型

Whisper 版本有 WhisperCpp 和 fasterWhisper（推荐） 两种，后者效果更好，都需要自行在软件内下载模型。

| 模型 | 磁盘空间 | 内存占用 | 说明 |
|------|----------|----------|------|
| Tiny | 75 MiB | ~273 MB | 转录很一般，仅用于测试 |
| Small | 466 MiB | ~852 MB | 英文识别效果已经不错 |
| Medium | 1.5 GiB | ~2.1 GB | 中文识别建议至少使用此版本 |
| Large-v2 👍 | 2.9 GiB | ~3.9 GB | 效果好，配置允许情况推荐使用 |
| Large-v3 | 2.9 GiB | ~3.9 GB | 社区反馈可能会出现幻觉/字幕重复问题 |

推荐模型: `Large-v2` 稳定且质量较好。

注：以上模型国内网络可直接在软件内下载。


### 5. 文稿匹配

- 在&quot;字幕优化与翻译&quot;页面，包含&quot;文稿匹配&quot;选项，支持以下**一种或者多种**内容，辅助校正字幕和翻译:

| 类型 | 说明 | 填写示例 |
|------|------|------|
| 术语表 | 专业术语、人名、特定词语的修正对照表 | 机器学习-&gt;Machine Learning&lt;br&gt;马斯克-&gt;Elon Musk&lt;br&gt;打call -&gt; 应援&lt;br&gt;图灵斑图&lt;br&gt;公交车悖论 |
| 原字幕文稿 | 视频的原有文稿或相关内容 | 完整的演讲稿、课程讲义等 |
| 修正要求 | 内容相关的具体修正要求 | 统一人称代词、规范专业术语等&lt;br&gt;填写**内容相关**的要求即可，[示例参考](https://github.com/WEIFENG2333/VideoCaptioner/issues/59#issuecomment-2495849752) |

- 如果需要文稿进行字幕优化辅助，全流程处理时，先填写文稿信息，再进行开始任务处理
- 注意: 使用上下文参数量不高的小型LLM模型时，建议控制文稿内容在1千字内，如果使用上下文较大的模型，则可以适当增加文稿内容。

无特殊需求，一般不填写。



### 6. Cookie 配置说明

如果使用URL下载功能时，如果遇到以下情况:
1. 下载视频网站需要登录信息才可以下载；
2. 只能下载较低分辨率的视频；
3. 网络条件较差时需要验证；

- 请参考 [Cookie 配置说明](./docs/get_cookies.md) 获取Cookie信息，并将cookies.txt文件放置到软件安装目录的 `AppData` 目录下，即可正常下载高质量视频。

## 💡 软件流程介绍

程序简单的处理流程如下:
```
语音识别转录 -&gt; 字幕断句(可选) -&gt; 字幕优化翻译(可选) -&gt; 字幕视频合成
```

## ✨ 软件主要功能

软件利用大语言模型(LLM)在理解上下文方面的优势，对语音识别生成的字幕进一步处理。有效修正错别字、统一专业术语，让字幕内容更加准确连贯，为用户带来出色的观看体验！

#### 1. 多平台视频下载与处理
- 支持国内外主流视频平台（B站、Youtube、小红书、TikTok、X、西瓜视频、抖音等）
- 自动提取视频原有字幕处理

#### 2. 专业的语音识别引擎
- 提供多种接口在线识别，效果媲美剪映（免费、高速）
- 支持本地Whisper模型（保护隐私、可离线）

#### 3. 字幕智能纠错
- 自动优化专业术语、代码片段和数学公式格式
- 上下文进行断句优化，提升阅读体验
- 支持文稿提示，使用原有文稿或者相关提示优化字幕断句

#### 4. 高质量字幕翻译
- 结合上下文的智能翻译，确保译文兼顾全文
- 通过Prompt指导大模型反思翻译，提升翻译质量
- 使用序列模糊匹配算法、保证时间轴完全一致

#### 5. 字幕样式调整
- 丰富的字幕样式模板（科普风、新闻风、番剧风等等）
- 多种格式字幕视频（SRT、ASS、VTT、TXT）

针对小白用户，对一些软件内的选项说明：

#### 1. 语音转录页面

- `VAD过滤`：开启后，VAD（语音活动检测）将过滤无人声的语音片段，从而减少幻觉现象。建议保持默认开启状态。如果不懂，其他VAD选项建议直接保持默认即可。

- `音频分离`：开启后，使用MDX-Net进行降噪处理，能够有效分离人声和背景音乐，从而提升音频质量。建议只在嘈杂的视频中开启。

#### 2. 字幕优化与翻译页面

- `智能断句`：开启后，全流程处理时生成字级时间戳，然后通过LLM大模型进行断句，从而在视频有更完美的观看体验。有按照句子断句和按照语义断句两种模式。可根据自己的需求配置。

- `字幕校正`：开启后，会通过LLM大模型对字幕内容进行校正(如：英文单词大小写、标点符号、错别字、数学公式和代码的格式等)，提升字幕的质量。

- `反思翻译`：开启后，会通过LLM大模型进行反思翻译，提升翻译的质量。相应的会增加请求的时间和消耗的Token。(选项在 设置页-LLM大模型翻译-反思翻译 中开启。)

- `文稿提示`：填写后，这部分也将作为提示词发送给大模型，辅助字幕优化和翻译。

#### 3. 字幕视频合成页面

- `视频合成`：开启后，会根据合成字幕视频；关闭将跳过视频合成的流程。


- `软字幕`：开启后，字幕不会烧录到视频中，处理速度极快。但是软字幕需要一些播放器（如PotPlayer）支持才可以进行显示播放。而且软字幕的样式不是软件内调整的字幕样式，而是播放器默认的白色样式。


安装软件的主要目录结构说明如下：
```
VideoCaptioner/
├── runtime/                    # 运行环境目录
├── resources/               # 软件资源文件目录（二进制程序、图标等,以及下载的faster-whisper程序）
├── work-dir/               # 工作目录，处理完成的视频和字幕文件保存在这里
├── AppData/                    # 应用数据目录
    ├── cache/              # 缓存目录，缓存转录、大模型请求的数据。
    ├── models/              # 存放 Whisper 模型文件
    ├── logs/               # 日志目录，记录软件运行状态
    ├── settings.json          # 存储用户设置
    └──  cookies.txt           # 视频平台的 cookie 信息（下载高清视频时需要）
└── VideoCaptioner.exe      # 主程序执行文件
```

## 📝 说明

1. 字幕断句的质量对观看体验至关重要。软件能将逐字字幕智能重组为符合自然语言习惯的段落，并与视频画面完美同步。

2. 在处理过程中，仅向大语言模型发送文本内容，不包含时间轴信息，这大大降低了处理开销。

3. 在翻译环节，我们采用吴恩达提出的&quot;翻译-反思-翻译&quot;方法论。这种迭代优化的方式确保了翻译的准确性。

4. 填入 YouTube 链接时进行处理时，会自动下载视频的字幕，从而省去转录步骤，极大地节省操作时间。

## 🤝 贡献指南

作者是一名大三学生，个人能力和项目都还有许多不足，项目也在不断完善中，如果在使用过程遇到的Bug，欢迎提交 [Issue](https://github.com/WEIFENG2333/VideoCaptioner/issues) 和 Pull Request 帮助改进项目。

## 更新日志

&lt;details&gt;
&lt;summary&gt;2025.02.07&lt;/summary&gt;
### Bug 修复与其他改进
- 修复谷歌翻译语言不正确的问题。
- 修部微软翻译不准确的问题。
- 修复运行设备不选择cuda时显示报 winError的错误
- 修复合成失败的问题
- 修复ass单语字幕没有内容的问题
&lt;/details&gt;


&lt;details&gt;
&lt;summary&gt;2024.2.06&lt;/summary&gt;

### 核心功能增强
- 完整重构代码架构，优化整体性能
- 字幕优化与翻译功能模块分离，提供更灵活的处理选项
- 新增批量处理功能：支持批量字幕、批量转录、批量字幕视频合成
- 全面优化 UI 界面与交互细节

### AI 模型与翻译升级
- 扩展 LLM 支持：新增 SiliconCloud、DeepSeek、Ollama、Gemini、ChatGLM 等模型
- 集成多种翻译服务：DeepLx、Bing、Google、LLM
- 新增 faster-whisper-large-v3-turbo 模型支持
- 新增多种 VAD（语音活动检测）方法
- 支持自定义反思翻译开关
- 字幕断句支持语义/句子两种模式
- 字幕断句、优化、翻译提示词的优化
- 字幕、转录缓存机制的优化
- 优化中文字幕自动换行功能
- 新增竖屏字幕样式
- 改进字幕时间轴切换机制，消除闪烁问题

### Bug 修复与其他改进
- 修复 Whisper API 无法使用问题
- 新增多种字幕视频格式支持
- 修复部分情况转录错误的问题
- 优化视频工作目录结构
- 新增日志查看功能
- 新增泰语、德语等语言的字幕优化
- 修复诸多Bug...

&lt;/details&gt;


&lt;details&gt;
&lt;summary&gt;2024.12.07&lt;/summary&gt;

- 新增 Faster-whisper 支持，音频转字幕质量更优
- 支持Vad语音断点检测，大大减少幻觉现象
- 支持人声音分离，分离视频背景噪音
- 支持关闭视频合成
- 新增字幕最大长度设置
- 新增字幕末尾标点去除设置
- 优化和翻译的提示词优化
- 优化LLM字幕断句错误的情况 
- 修复音频转换格式不一致问题

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;2024.11.23&lt;/summary&gt;

- 新增 Whisper-v3 模型支持，大幅提升语音识别准确率
- 优化字幕断句算法，提供更自然的阅读体验 
- 修复检测模型可用性时的稳定性问题
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;2024.11.20&lt;/summary&gt;

- 支持自定义调节字幕位置和样式
- 新增字幕优化和翻译过程的实时日志查看
- 修复使用 API 时的自动翻译问题
- 优化视频工作目录结构,提升文件管理效率
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;2024.11.17&lt;/summary&gt;

- 支持双语/单语字幕灵活导出
- 新增文稿匹配提示对齐功能
- 修复字幕导入时的稳定性问题
- 修复非中文路径下载模型的兼容性问题
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;2024.11.13&lt;/summary&gt;

- 新增 Whisper API 调用支持
- 支持导入 cookie.txt 下载各大视频平台资源
- 字幕文件名自动与视频保持一致
- 软件主页新增运行日志实时查看
- 统一和完善软件内部功能
&lt;/details&gt;


## 💖 支持作者

如果觉得项目对你有帮助，可以给项目点个Star，这将是对我最大的鼓励和支持！

&lt;details&gt;
&lt;summary&gt;捐助支持&lt;/summary&gt;
&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;./docs/images/alipay.jpg&quot; alt=&quot;支付宝二维码&quot; width=&quot;30%&quot;&gt;
  &lt;img src=&quot;./docs/images/wechat.jpg&quot; alt=&quot;微信二维码&quot; width=&quot;30%&quot;&gt;
&lt;/div&gt;
&lt;/details&gt;

## ⭐ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=WEIFENG2333/VideoCaptioner&amp;type=Date)](https://star-history.com/#WEIFENG2333/VideoCaptioner&amp;Date)


</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Alibaba-NLP/WebAgent]]></title>
            <link>https://github.com/Alibaba-NLP/WebAgent</link>
            <guid>https://github.com/Alibaba-NLP/WebAgent</guid>
            <pubDate>Tue, 08 Jul 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[🌐 WebAgent for Information Seeking bulit by Tongyi Lab: WebWalker & WebDancer & WebSailor https://arxiv.org/pdf/2507.02592]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Alibaba-NLP/WebAgent">Alibaba-NLP/WebAgent</a></h1>
            <p>🌐 WebAgent for Information Seeking bulit by Tongyi Lab: WebWalker & WebDancer & WebSailor https://arxiv.org/pdf/2507.02592</p>
            <p>Language: Python</p>
            <p>Stars: 1,597</p>
            <p>Forks: 118</p>
            <p>Stars today: 188 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;h2&gt;WebAgent for Information Seeking bulit by Tongyi Lab, Alibaba Group &lt;img src=&quot;./assets/tongyi.png&quot; width=&quot;30px&quot; style=&quot;display:inline;&quot;&gt;&lt;/h2&gt;

&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/14217&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14217&quot; 
alt=&quot;Alibaba-NLP%2FWebAgent | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
🤗 &lt;a href=&quot;https://huggingface.co/Alibaba-NLP/WebSailor&quot; target=&quot;_blank&quot;&gt;WebSailor&lt;/a&gt; ｜
🤗 &lt;a href=&quot;https://huggingface.co/Alibaba-NLP/WebDancer-32B&quot; target=&quot;_blank&quot;&gt;WebDancer-QwQ-32B&lt;/a&gt;  | 
🤗 &lt;a href=&quot;https://huggingface.co/datasets/callanwu/WebWalkerQA&quot; target=&quot;_blank&quot;&gt;WebWalkerQA&lt;/a&gt;

&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/roadmap.png&quot; width=&quot;100%&quot; height=&quot;50%&quot; /&gt;
&lt;/p&gt;
&lt;/div&gt;

&gt; You can check the paper of [WebDancer](https://arxiv.org/pdf/2505.22648) and [WebWalker](https://arxiv.org/pdf/2501.07572) and [WebSailor](https://arxiv.org/pdf/2507.02592).

&gt; 💥 💥 💥 Stay tuned for more updates! We are working on building native agentic model based on the Browser and more open-domain environments!

- [**WebSailor**](WebSailor) (Preprint 2025) - WebSailor: Navigating Super-human Reasoning for Web Agent
- [**WebDancer**](WebDancer) (Preprint 2025) - WebDancer: Towards Autonomous Information Seeking Agency
- [**WebWalker**](WebWalker) (ACL 2025) - WebWalker: Benchmarking LLMs in Web Traversal

## 📰News and Updates

- `2025.07.03` 🔥🔥🔥We release **WebSailor**, an agentic search model specialized in performing extremely complex information seeking tasks, achieving open-source SOTA on some of the most difficult browsing benchmarks. **WebSailor** topped the HuggingFace [daily papers](https://huggingface.co/papers/2507.02592).
- `2025.06.23` 🔥🔥🔥The model, interactive demo, and some of the data of **WebDancer** have been open-sourced. You&#039;re welcome to try them out!
- `2025.05.29` 🔥🔥🔥We release **WebDancer**, a native agentic search model towards autonomous information seeking agency and _Deep Research_-like model.
- `2025.05.15` **WebWalker** is accepted by ACL 2025 main conference.
- `2025.01.14` We release **WebWalker**, a benchmark for LLMs in web traversal and a multi-agent framework for information seeking.

## 💎Results Showcase

&lt;div align=&quot;center&quot;&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;assets/result.png&quot; width=&quot;800%&quot; height=&quot;50%&quot; /&gt;
&lt;/p&gt;
&lt;/div&gt;

## ⛵️Features for WebSailor

- A complete post-training methodology enabling models to engage in extended thinking and information seeking, ultimately allowing them to successfully complete extremely complex tasks previously considered unsolvable.
- Introduces **SailorFog-QA**, a scalable QA benchmark with high uncertainty and difficulty, curated with a novel data synthesis method through graph sampling and information obfuscation. Example SailorFog-QA data samples can be found at: [`WebSailor/dataset/sailorfog-QA.jsonl`](WebSailor/dataset/sailorfog-QA.jsonl)
- Effective post-training pipeline consisting of (1) high-quality reconstruction of concise reasoning from expert trajectories for clean supervision, (2) a two-stage training process involving an RFT cold start stage, followed by **Duplicating Sampling Policy Optimization (DUPO)**, an efficient agentic RL algorithm excelling in effectiveness and efficiency.
- WebSailor-72B significantly outperforms all open-source agents and frameworks while closing the performance gap with leading proprietary systems, achieving a score of **12.0%** on BrowseComp-en, **30.1%** on BrowseComp-zh, and **55.4%** on GAIA.
- **The checkpoint is coming soon.**

## 🌐Features for WebDancer

- Native agentic search reasoning model using ReAct framework towards autonomous information seeking agency and _Deep Research_-like model.
- We introduce a four-stage training paradigm comprising **browsing data construction, trajectory sampling, supervised fine-tuning for effective cold start, and reinforcement learning for improved generalization**, enabling the agent to autonomously acquire autonomous search and reasoning skills.
- Our data-centric approach integrates trajectory-level supervision fine-tuning and reinforcement learning (DAPO) to develop a scalable pipeline for **training agentic systems** via SFT or RL.
- WebDancer achieves a Pass@3 score of 64.1% on GAIA and 62.0% on WebWalkerQA.

## 🚀 Quick Start

You need to enter the [`WebDancer`](WebDancer) folder for the following commands.

### Step 0: Set Up the Environment

```bash
conda create -n webdancer python=3.12
pip install -r requirements.txt
```

### Step 1: Deploy the Model

Download the WebDancer model from [🤗 HuggingFace](https://huggingface.co/Alibaba-NLP/WebDancer-32B) and deploy it using the provided scripts with [sglang](https://github.com/sgl-project/sglang).

```bash
cd scripts
bash deploy_model.sh WebDancer_PATH
```

&gt; **Note:** Replace `WebDancer_PATH` with the actual path to the downloaded model.

### Step 2: Run the Demo

Edit the following keys in [`WebDancer/scripts/run_demo.sh`](WebDancer/scripts/run_demo.sh):

- `GOOGLE_SEARCH_KEY`, you can get it from [here](https://serpapi.com/).
- `JINA_API_KEY`, you can get it from [here](https://jina.ai/api-dashboard/).
- `DASHSCOPE_API_KEY`, you can get it from [here](https://dashscope.aliyun.com/).

Then, launch the demo with Gradio to interact with the WebDancer model:

```bash
cd scripts
bash run_demo.sh
```

## 🎥 WebSailor Demos

We provide demos for BrowseComp-en, BrowseComp-zh and Daily Use. Our model can complete highly difficult and uncertain tasks requiring massive information acquisition and complex reasoning.

&lt;div align=&quot;center&quot;&gt;
    &lt;h3&gt;BrowseComp-en&lt;/h3&gt;
    &lt;video src= &quot;https://github.com/user-attachments/assets/2dc0b03a-c241-4f70-bf11-92fda28020fa&quot;/&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;h3&gt;BrowseComp-zh&lt;/h3&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/f9aed746-ffc8-4b76-b135-715ec0eab544&quot; /&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;h3&gt;Daily Use&lt;/h3&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/1299c5a8-cee3-4a70-b68b-c5d227cf8055&quot; /&gt;
&lt;/div&gt;

## 🎥 WebDancer Demos

We provide demos for WebWalkerQA, GAIA and Daily Use.
Our model can execute the long-horizon tasks with **multiple steps** and **complex reasoning**, such as web traversal, information seeking and question answering.

&lt;div align=&quot;center&quot;&gt;
    &lt;h3&gt;WebWalkerQA&lt;/h3&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/0bbaf55b-897e-4c57-967d-a6e8bbd2167e&quot; /&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;h3&gt;GAIA&lt;/h3&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/935c668e-6169-4712-9c04-ac80f0531872&quot; /&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
    &lt;h3&gt;Daily Use&lt;/h3&gt;
    &lt;video src=&quot;https://github.com/user-attachments/assets/d1d5b533-4009-478b-bd87-96b86389327d&quot; /&gt;
&lt;/div&gt;

## 📃License

The content of this project itself is licensed under [LICENSE](LICENSE).

## 🚩Citation

If this work is helpful, please kindly cite as:

```bigquery
@misc{li2025websailor,
      title={WebSailor: Navigating Super-human Reasoning for Web Agent},
      author={Kuan Li and Zhongwang Zhang and Huifeng Yin and Liwen Zhang and Litu Ou and Jialong Wu and Wenbiao Yin and Baixuan Li and Zhengwei Tao and Xinyu Wang and Weizhou Shen and Junkai Zhang and Dingchu Zhang and Xixi Wu and Yong Jiang and Ming Yan and Pengjun Xie and Fei Huang and Jingren Zhou},
      year={2025},
      eprint={2507.02592},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.02592},
}
@misc{wu2025webdancer,
      title={WebDancer: Towards Autonomous Information Seeking Agency},
      author={Jialong Wu and Baixuan Li and Runnan Fang and Wenbiao Yin and Liwen Zhang and Zhengwei Tao and Dingchu Zhang and Zekun Xi and Yong Jiang and Pengjun Xie and Fei Huang and Jingren Zhou},
      year={2025},
      eprint={2505.22648},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.22648},
}
@misc{wu2025webwalker,
      title={WebWalker: Benchmarking LLMs in Web Traversal},
      author={Jialong Wu and Wenbiao Yin and Yong Jiang and Zhenglin Wang and Zekun Xi and Runnan Fang and Deyu Zhou and Pengjun Xie and Fei Huang},
      year={2025},
      eprint={2501.07572},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.07572},
}
```

The repo is contributed by [Jialong Wu](https://callanwu.github.io/). If you have any questions, please feel free to contact via wujialongml@gmail.com or create an issue.

## 🌟Misc

&lt;div align=&quot;center&quot;&gt;

[![Star History Chart](https://api.star-history.com/svg?repos=Alibaba-NLP/WebAgent&amp;type=Date)](https://www.star-history.com/#Alibaba-NLP/WebAgent&amp;Date)

&lt;/div&gt;

## 🚩Talent Recruitment

🔥🔥🔥We are hiring! Research intern positions are open (based in Hangzhou、Beijing、Shanghai)

📚 **Research Area**：Web Agent, Search Agent, Agent RL, MultiAgent RL, Agentic RAG

☎️ **Contact**：[yongjiang.jy@alibaba-inc.com]()
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Free-TV/IPTV]]></title>
            <link>https://github.com/Free-TV/IPTV</link>
            <guid>https://github.com/Free-TV/IPTV</guid>
            <pubDate>Tue, 08 Jul 2025 00:04:20 GMT</pubDate>
            <description><![CDATA[M3U Playlist for free TV channels]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Free-TV/IPTV">Free-TV/IPTV</a></h1>
            <p>M3U Playlist for free TV channels</p>
            <p>Language: Python</p>
            <p>Stars: 6,418</p>
            <p>Forks: 1,110</p>
            <p>Stars today: 15 stars today</p>
            <h2>README</h2><pre>Free TV
=======

This is an M3U playlist for free TV channels around the World.

Either free locally (over the air):

[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/us.svg&quot; width=&quot;24&quot;&gt;](lists/usa.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ca.svg&quot; width=&quot;24&quot;&gt;](lists/canada.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/gb.svg&quot; width=&quot;24&quot;&gt;](lists/uk.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ie.svg&quot; width=&quot;24&quot;&gt;](lists/ireland.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/au.svg&quot; width=&quot;24&quot;&gt;](lists/australia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/in.svg&quot; width=&quot;24&quot;&gt;](lists/india.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/jp.svg&quot; width=&quot;24&quot;&gt;](lists/japan.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/cn.svg&quot; width=&quot;24&quot;&gt;](lists/china.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/hk.svg&quot; width=&quot;24&quot;&gt;](lists/hong_kong.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/mo.svg&quot; width=&quot;24&quot;&gt;](lists/macau.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/tw.svg&quot; width=&quot;24&quot;&gt;](lists/taiwan.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/kp.svg&quot; width=&quot;24&quot;&gt;](lists/north_korea.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/kr.svg&quot; width=&quot;24&quot;&gt;](lists/korea.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/dk.svg&quot; width=&quot;24&quot;&gt;](lists/denmark.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/fo.svg&quot; width=&quot;24&quot;&gt;](lists/faroe_islands.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/gl.svg&quot; width=&quot;24&quot;&gt;](lists/greenland.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/fi.svg&quot; width=&quot;24&quot;&gt;](lists/finland.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/is.svg&quot; width=&quot;24&quot;&gt;](lists/iceland.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/no.svg&quot; width=&quot;24&quot;&gt;](lists/norway.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/se.svg&quot; width=&quot;24&quot;&gt;](lists/sweden.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ee.svg&quot; width=&quot;24&quot;&gt;](lists/estonia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/lv.svg&quot; width=&quot;24&quot;&gt;](lists/latvia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/lt.svg&quot; width=&quot;24&quot;&gt;](lists/lithuania.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/be.svg&quot; width=&quot;24&quot;&gt;](lists/belgium.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/nl.svg&quot; width=&quot;24&quot;&gt;](lists/netherlands.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/lu.svg&quot; width=&quot;24&quot;&gt;](lists/luxembourg.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/de.svg&quot; width=&quot;24&quot;&gt;](lists/germany.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/at.svg&quot; width=&quot;24&quot;&gt;](lists/austria.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ch.svg&quot; width=&quot;24&quot;&gt;](lists/switzerland.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/pl.svg&quot; width=&quot;24&quot;&gt;](lists/poland.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/cz.svg&quot; width=&quot;24&quot;&gt;](lists/czech_republic.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/sk.svg&quot; width=&quot;24&quot;&gt;](lists/slovakia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/hu.svg&quot; width=&quot;24&quot;&gt;](lists/hungary.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ro.svg&quot; width=&quot;24&quot;&gt;](lists/romania.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/md.svg&quot; width=&quot;24&quot;&gt;](lists/moldova.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/bg.svg&quot; width=&quot;24&quot;&gt;](lists/bulgaria.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/fr.svg&quot; width=&quot;24&quot;&gt;](lists/france.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/it.svg&quot; width=&quot;24&quot;&gt;](lists/italy.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/pt.svg&quot; width=&quot;24&quot;&gt;](lists/portugal.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/es.svg&quot; width=&quot;24&quot;&gt;](lists/spain.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ru.svg&quot; width=&quot;24&quot;&gt;](lists/russia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/by.svg&quot; width=&quot;24&quot;&gt;](lists/belarus.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ua.svg&quot; width=&quot;24&quot;&gt;](lists/ukraine.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/am.svg&quot; width=&quot;24&quot;&gt;](lists/armenia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/az.svg&quot; width=&quot;24&quot;&gt;](lists/azerbaijan.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ge.svg&quot; width=&quot;24&quot;&gt;](lists/georgia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ba.svg&quot; width=&quot;24&quot;&gt;](lists/bosnia_and_herzegovina.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/hr.svg&quot; width=&quot;24&quot;&gt;](lists/croatia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/me.svg&quot; width=&quot;24&quot;&gt;](lists/montenegro.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/mk.svg&quot; width=&quot;24&quot;&gt;](lists/north_macedonia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/rs.svg&quot; width=&quot;24&quot;&gt;](lists/serbia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/si.svg&quot; width=&quot;24&quot;&gt;](lists/slovenia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/al.svg&quot; width=&quot;24&quot;&gt;](lists/albania.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/xk.svg&quot; width=&quot;24&quot;&gt;](lists/kosovo.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/gr.svg&quot; width=&quot;24&quot;&gt;](lists/greece.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/cy.svg&quot; width=&quot;24&quot;&gt;](lists/cyprus.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ad.svg&quot; width=&quot;24&quot;&gt;](lists/andorra.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/mt.svg&quot; width=&quot;24&quot;&gt;](lists/malta.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/mc.svg&quot; width=&quot;24&quot;&gt;](lists/monaco.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/sm.svg&quot; width=&quot;24&quot;&gt;](lists/san_marino.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ir.svg&quot; width=&quot;24&quot;&gt;](lists/iran.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/iq.svg&quot; width=&quot;24&quot;&gt;](lists/iraq.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/il.svg&quot; width=&quot;24&quot;&gt;](lists/israel.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/qa.svg&quot; width=&quot;24&quot;&gt;](lists/qatar.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/tr.svg&quot; width=&quot;24&quot;&gt;](lists/turkey.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ae.svg&quot; width=&quot;24&quot;&gt;](lists/united_arab_emirates.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ar.svg&quot; width=&quot;24&quot;&gt;](lists/argentina.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/cr.svg&quot; width=&quot;24&quot;&gt;](lists/costa_rica.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/do.svg&quot; width=&quot;24&quot;&gt;](lists/dominican_republic.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/mx.svg&quot; width=&quot;24&quot;&gt;](lists/mexico.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/py.svg&quot; width=&quot;24&quot;&gt;](lists/paraguay.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/pe.svg&quot; width=&quot;24&quot;&gt;](lists/peru.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/ve.svg&quot; width=&quot;24&quot;&gt;](lists/venezuela.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/br.svg&quot; width=&quot;24&quot;&gt;](lists/brazil.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/tt.svg&quot; width=&quot;24&quot;&gt;](lists/trinidad.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/td.svg&quot; width=&quot;24&quot;&gt;](lists/chad.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/so.svg&quot; width=&quot;24&quot;&gt;](lists/somalia.md)
[&lt;img src=&quot;https://hatscripts.github.io/circle-flags/flags/id.svg&quot; width=&quot;24&quot;&gt;](lists/indonesia.md)

Or free on the Internet:

- Plex TV
- Pluto TV (English, Spanish, French, Italian)
- Redbox Live TV
- Roku TV
- Samsung TV Plus
- Youtube live channels

To use it point your IPTV player to https://raw.githubusercontent.com/Free-TV/IPTV/master/playlist.m3u8.

Philosophy
==========

The main goals for this playlist are listed below.

**Quality over quantity**

The less channels we support the better.

- All channels should work well.
- As much as possible channels should be in HD, not SD.
- Only one URL per channel (no +1, no alternate feeds, no regional declinations)

**Only free channels**

If a channel is normally only available via commercial subscriptions it has nothing to do in this playlist. If on the other hand it is provided for free to everybody in a particular country, then it should be in this playlist.

- No paid channels
- Only channels which are officially provided for free (via DVB-S, DVB-T, analog, etc..)

**Only mainstream channels**

This is a playlist for everybody.

- No adult channels
- No channels dedicated to any particular religion
- No channels dedicated to any particular political party
- No channels made for a country and funded by a different country

Feed sources
============

It can be quite hard to find up to date URLs, here&#039;s a list of sources:

- https://github.com/iptv-org/iptv/tree/master/streams
- Youtube: As long as the channel is live and its URL doesn&#039;t change (check the age of the stream, the number of viewers..)
- Dailymotion: Same criteria as for youtube

Format
======

The m3u8 playlist is generated by `make_playlist.py`, using the `.md` files located in `lists`.

Each .md file represesnts a group. The `&lt;h1&gt;` line is used as the group title.

Only channels which URL column starts with `[&gt;]` are included in the playlist.

Channels which are not in HD are marked with an `Ⓢ`.

Channels which use GeoIP blocking are marked with a `Ⓖ`.

Channels which are live Youtube channels are marked with a `Ⓨ`.

Issues
======

Only create issues for bugs and feature requests.

Do not create issues to add/edit or to remove channels. If you want to add/edit/remove channels, create a pull request directly.

Pull Requests
=============

**Only modify .md files**

If your Pull Request modifies channels, only modify .md files. Do not modify m3u8 files in your pull request.

**Adding a new Channel**

To add a new channel, make a Pull Request.

- In your Pull Request you need to provide information to show that the channel is free.
- Use imgur.com to host the channel logo and point to it.
- If you have a valid stream, add it and put `[&gt;]` in front of it.
- If you don&#039;t have an stream for the channel, add `[x]()` in the url column and place your channel in the Invalid category.
- If you have a stream but it doesn&#039;t work well, put the channel in the Invalid category and put `[x]` in front of the url.
- If you&#039;re adding geoblocked URLs specify it in your PR and specify which country they&#039;re working in. The PR will only be merged if these URLs can be tested.

**Removing a Channel**

To remove a channel, make a Pull Request.

In your Pull Request you need to provide information to show that the channel is only available via a private paid subscription.

Note: Public taxes (whether national or regional, whether called TV License or not) do not constitute a private paid subscription.

If a stream is broken, simply move the channel to the invalid category and replace `[&gt;]` with `[x]` in the url column.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[joeyism/linkedin_scraper]]></title>
            <link>https://github.com/joeyism/linkedin_scraper</link>
            <guid>https://github.com/joeyism/linkedin_scraper</guid>
            <pubDate>Tue, 08 Jul 2025 00:04:19 GMT</pubDate>
            <description><![CDATA[A library that scrapes Linkedin for user data]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/joeyism/linkedin_scraper">joeyism/linkedin_scraper</a></h1>
            <p>A library that scrapes Linkedin for user data</p>
            <p>Language: Python</p>
            <p>Stars: 2,956</p>
            <p>Forks: 727</p>
            <p>Stars today: 287 stars today</p>
            <h2>README</h2><pre># Linkedin Scraper

Scrapes Linkedin User Data

[Linkedin Scraper](#linkedin-scraper)
* [Installation](#installation)
* [Setup](#setup)
* [Usage](#usage)
  + [Sample Usage](#sample-usage)
  + [User Scraping](#user-scraping)
  + [Company Scraping](#company-scraping)
  + [Job Scraping](#job-scraping)
  + [Job Search Scraping](#job-search-scraping)
  + [Scraping sites where login is required first](#scraping-sites-where-login-is-required-first)
  + [Scraping sites and login automatically](#scraping-sites-and-login-automatically)
* [API](#api)
  + [Person](#person)
    - [`linkedin_url`](#linkedin_url)
    - [`name`](#name)
    - [`about`](#about)
    - [`experiences`](#experiences)
    - [`educations`](#educations)
    - [`interests`](#interests)
    - [`accomplishment`](#accomplishment)
    - [`company`](#company)
    - [`job_title`](#job_title)
    - [`driver`](#driver)
    - [`scrape`](#scrape)
    - [`scrape(close_on_complete=True)`](#scrapeclose_on_completetrue)
  + [Company](#company)
    - [`linkedin_url`](#linkedin_url-1)
    - [`name`](#name-1)
    - [`about_us`](#about_us)
    - [`website`](#website)
    - [`headquarters`](#headquarters)
    - [`founded`](#founded)
    - [`company_type`](#company_type)
    - [`company_size`](#company_size)
    - [`specialties`](#specialties)
    - [`showcase_pages`](#showcase_pages)
    - [`affiliated_companies`](#affiliated_companies)
    - [`driver`](#driver-1)
    - [`get_employees`](#get_employees)
    - [`scrape(close_on_complete=True)`](#scrapeclose_on_completetrue-1)
* [Contribution](#contribution)

## Installation

```bash
pip3 install --user linkedin_scraper
```

Version **2.0.0** and before is called `linkedin_user_scraper` and can be installed via `pip3 install --user linkedin_user_scraper`

## Setup
First, you must set your chromedriver location by

```bash
export CHROMEDRIVER=~/chromedriver
```

## Sponsor
Message me if you&#039;d like to sponsor me

## Usage
To use it, just create the class.

### Sample Usage
```python
from linkedin_scraper import Person, actions
from selenium import webdriver
driver = webdriver.Chrome()

email = &quot;some-email@email.address&quot;
password = &quot;password123&quot;
actions.login(driver, email, password) # if email and password isnt given, it&#039;ll prompt in terminal
person = Person(&quot;https://www.linkedin.com/in/joey-sham-aa2a50122&quot;, driver=driver)
```

**NOTE**: The account used to log-in should have it&#039;s language set English to make sure everything works as expected.

### User Scraping
```python
from linkedin_scraper import Person
person = Person(&quot;https://www.linkedin.com/in/andre-iguodala-65b48ab5&quot;)
```

### Company Scraping
```python
from linkedin_scraper import Company
company = Company(&quot;https://ca.linkedin.com/company/google&quot;)
```

### Job Scraping
```python
from linkedin_scraper import Job, actions
from selenium import webdriver

driver = webdriver.Chrome()
email = &quot;some-email@email.address&quot;
password = &quot;password123&quot;
actions.login(driver, email, password) # if email and password isnt given, it&#039;ll prompt in terminal
input(&quot;Press Enter&quot;)
job = Job(&quot;https://www.linkedin.com/jobs/collections/recommended/?currentJobId=3456898261&quot;, driver=driver, close_on_complete=False)
```

### Job Search Scraping
```python
from linkedin_scraper import JobSearch, actions
from selenium import webdriver

driver = webdriver.Chrome()
email = &quot;some-email@email.address&quot;
password = &quot;password123&quot;
actions.login(driver, email, password) # if email and password isnt given, it&#039;ll prompt in terminal
input(&quot;Press Enter&quot;)
job_search = JobSearch(driver=driver, close_on_complete=False, scrape=False)
# job_search contains jobs from your logged in front page:
# - job_search.recommended_jobs
# - job_search.still_hiring
# - job_search.more_jobs

job_listings = job_search.search(&quot;Machine Learning Engineer&quot;) # returns the list of `Job` from the first page
```

### Scraping sites where login is required first
1. Run `ipython` or `python`
2. In `ipython`/`python`, run the following code (you can modify it if you need to specify your driver)
3. 
```python
from linkedin_scraper import Person
from selenium import webdriver
driver = webdriver.Chrome()
person = Person(&quot;https://www.linkedin.com/in/andre-iguodala-65b48ab5&quot;, driver = driver, scrape=False)
```
4. Login to Linkedin
5. [OPTIONAL] Logout of Linkedin
6. In the same `ipython`/`python` code, run
```python
person.scrape()
```

The reason is that LinkedIn has recently blocked people from viewing certain profiles without having previously signed in. So by setting `scrape=False`, it doesn&#039;t automatically scrape the profile, but Chrome will open the linkedin page anyways. You can login and logout, and the cookie will stay in the browser and it won&#039;t affect your profile views. Then when you run `person.scrape()`, it&#039;ll scrape and close the browser. If you want to keep the browser on so you can scrape others, run it as 

**NOTE**: For version &gt;= `2.1.0`, scraping can also occur while logged in. Beware that users will be able to see that you viewed their profile.

```python
person.scrape(close_on_complete=False)
``` 
so it doesn&#039;t close.

### Scraping sites and login automatically
From verison **2.4.0** on, `actions` is a part of the library that allows signing into Linkedin first. The email and password can be provided as a variable into the function. If not provided, both will be prompted in terminal.

```python
from linkedin_scraper import Person, actions
from selenium import webdriver
driver = webdriver.Chrome()
email = &quot;some-email@email.address&quot;
password = &quot;password123&quot;
actions.login(driver, email, password) # if email and password isnt given, it&#039;ll prompt in terminal
person = Person(&quot;https://www.linkedin.com/in/andre-iguodala-65b48ab5&quot;, driver=driver)
```


## API

### Person
A Person object can be created with the following inputs:

```python
Person(linkedin_url=None, name=None, about=[], experiences=[], educations=[], interests=[], accomplishments=[], company=None, job_title=None, driver=None, scrape=True)
```
#### `linkedin_url`
This is the linkedin url of their profile

#### `name`
This is the name of the person

#### `about`
This is the small paragraph about the person

#### `experiences`
This is the past experiences they have. A list of `linkedin_scraper.scraper.Experience`

#### `educations`
This is the past educations they have. A list of `linkedin_scraper.scraper.Education`

#### `interests`
This is the interests they have. A list of `linkedin_scraper.scraper.Interest`

#### `accomplishment`
This is the accomplishments they have. A list of `linkedin_scraper.scraper.Accomplishment`

#### `company`
This the most recent company or institution they have worked at. 

#### `job_title`
This the most recent job title they have. 

#### `driver`
This is the driver from which to scraper the Linkedin profile. A driver using Chrome is created by default. However, if a driver is passed in, that will be used instead.

For example
```python
driver = webdriver.Chrome()
person = Person(&quot;https://www.linkedin.com/in/andre-iguodala-65b48ab5&quot;, driver = driver)
```

#### `scrape`
When this is **True**, the scraping happens automatically. To scrape afterwards, that can be run by the `scrape()` function from the `Person` object.


#### `scrape(close_on_complete=True)`
This is the meat of the code, where execution of this function scrapes the profile. If *close_on_complete* is True (which it is by default), then the browser will close upon completion. If scraping of other profiles are desired, then you might want to set that to false so you can keep using the same driver.

 


### Company

```python
Company(linkedin_url=None, name=None, about_us=None, website=None, phone=None, headquarters=None, founded=None, company_type=None, company_size=None, specialties=None, showcase_pages=[], affiliated_companies=[], driver=None, scrape=True, get_employees=True)
```

#### `linkedin_url`
This is the linkedin url of their profile

#### `name`
This is the name of the company

#### `about_us`
The description of the company

#### `website`
The website of the company

#### `phone`
The phone of the company

#### `headquarters`
The headquarters location of the company

#### `founded`
When the company was founded

#### `company_type`
The type of the company

#### `company_size`
How many people are employeed at the company

#### `specialties`
What the company specializes in

#### `showcase_pages`
Pages that the company owns to showcase their products

#### `affiliated_companies`
Other companies that are affiliated with this one

#### `driver`
This is the driver from which to scraper the Linkedin profile. A driver using Chrome is created by default. However, if a driver is passed in, that will be used instead.

#### `get_employees`
Whether to get all the employees of company

For example
```python
driver = webdriver.Chrome()
company = Company(&quot;https://ca.linkedin.com/company/google&quot;, driver=driver)
```


#### `scrape(close_on_complete=True)`
This is the meat of the code, where execution of this function scrapes the company. If *close_on_complete* is True (which it is by default), then the browser will close upon completion. If scraping of other companies are desired, then you might want to set that to false so you can keep using the same driver.

## Contribution

&lt;a href=&quot;https://www.buymeacoffee.com/joeyism&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://www.buymeacoffee.com/assets/img/custom_images/orange_img.png&quot; alt=&quot;Buy Me A Coffee&quot; style=&quot;height: 41px !important;width: 174px !important;box-shadow: 0px 3px 2px 0px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 3px 2px 0px rgba(190, 190, 190, 0.5) !important;&quot; &gt;&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Shubhamsaboo/awesome-llm-apps]]></title>
            <link>https://github.com/Shubhamsaboo/awesome-llm-apps</link>
            <guid>https://github.com/Shubhamsaboo/awesome-llm-apps</guid>
            <pubDate>Tue, 08 Jul 2025 00:04:18 GMT</pubDate>
            <description><![CDATA[Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Shubhamsaboo/awesome-llm-apps">Shubhamsaboo/awesome-llm-apps</a></h1>
            <p>Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p>
            <p>Language: Python</p>
            <p>Stars: 48,676</p>
            <p>Forks: 5,585</p>
            <p>Stars today: 315 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;http://www.theunwindai.com&quot;&gt;
    &lt;img src=&quot;docs/banner/unwind_black.png&quot; width=&quot;900px&quot; alt=&quot;Unwind AI&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.linkedin.com/in/shubhamsaboo/&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/-Follow%20Shubham%20Saboo-blue?logo=linkedin&amp;style=flat-square&quot; alt=&quot;LinkedIn&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/Saboo_Shubham_&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/Shubham_Saboo&quot; alt=&quot;Twitter&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=de&quot;&gt;Deutsch&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=es&quot;&gt;Español&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=fr&quot;&gt;français&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ja&quot;&gt;日本語&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ko&quot;&gt;한국어&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=pt&quot;&gt;Português&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=ru&quot;&gt;Русский&lt;/a&gt; | 
  &lt;a href=&quot;https://www.readme-i18n.com/Shubhamsaboo/awesome-llm-apps?lang=zh&quot;&gt;中文&lt;/a&gt;
&lt;/p&gt;

&lt;hr/&gt;

# 🌟 Awesome LLM Apps

A curated collection of **Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.** This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/9876&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/9876&quot; alt=&quot;Shubhamsaboo%2Fawesome-llm-apps | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## 🤔 Why Awesome LLM Apps?

- 💡 Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.
- 🔥 Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp; RAG.
- 🎓 Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.

## 📂 Featured AI Projects

### AI Agents

### 🌱 Starter AI Agents

*   [🎙️ AI Blog to Podcast Agent](starter_ai_agents/ai_blog_to_podcast_agent/)
*   [❤️‍🩹 AI Breakup Recovery Agent](starter_ai_agents/ai_breakup_recovery_agent/)
*   [📊 AI Data Analysis Agent](starter_ai_agents/ai_data_analysis_agent/)
*   [🩻 AI Medical Imaging Agent](starter_ai_agents/ai_medical_imaging_agent/)
*   [😂 AI Meme Generator Agent (Browser)](starter_ai_agents/ai_meme_generator_agent_browseruse/)
*   [🎵 AI Music Generator Agent](starter_ai_agents/ai_music_generator_agent/)
*   [🛫 AI Travel Agent (Local &amp; Cloud)](starter_ai_agents/ai_travel_agent/)
*   [✨ Gemini Multimodal Agent](starter_ai_agents/gemini_multimodal_agent_demo/)
*   [🌐 Local News Agent (OpenAI Swarm)](starter_ai_agents/local_news_agent_openai_swarm/)
*   [🔄 Mixture of Agents](starter_ai_agents/mixture_of_agents/)
*   [📊 xAI Finance Agent](starter_ai_agents/xai_finance_agent/)
*   [🔍 OpenAI Research Agent](starter_ai_agents/opeani_research_agent/)
*   [🕸️ Web Scrapping AI Agent (Local &amp; Cloud)](starter_ai_agents/web_scrapping_ai_agent/)

### 🚀 Advanced AI Agents

*   [🔍 AI Deep Research Agent](advanced_ai_agents/single_agent_apps/ai_deep_research_agent/)
*   [🤝 AI Consultant Agent](advanced_ai_agents/single_agent_apps/ai_consultant_agent)
*   [🏗️ AI System Architect Agent](advanced_ai_agents/single_agent_apps/ai_system_architect_r1/)
*   [🎯 AI Lead Generation Agent](advanced_ai_agents/single_agent_apps/ai_lead_generation_agent/)
*   [💰 AI Financial Coach Agent](advanced_ai_agents/multi_agent_apps/ai_financial_coach_agent/)
*   [🎬 AI Movie Production Agent](advanced_ai_agents/single_agent_apps/ai_movie_production_agent/)
*   [📈 AI Investment Agent](advanced_ai_agents/single_agent_apps/ai_investment_agent/)
*   [🏋️‍♂️ AI Health &amp; Fitness Agent](advanced_ai_agents/single_agent_apps/ai_health_fitness_agent/)
*   [🚀 AI Product Launch Intelligence Agent](advanced_ai_agents/multi_agent_apps/product_launch_intelligence_agent)
*   [🗞️ AI Journalist Agent](advanced_ai_agents/single_agent_apps/ai_journalist_agent/)
*   [🧠 AI Mental Wellbeing Agent](advanced_ai_agents/multi_agent_apps/ai_mental_wellbeing_agent/)
*   [📑 AI Meeting Agent](advanced_ai_agents/single_agent_apps/ai_meeting_agent/)
*   [🧬 AI Self-Evolving Agent](advanced_ai_agents/multi_agent_apps/ai_Self-Evolving_agent/)
*   [🎧 AI Social Media News and Podcast Agent](advanced_ai_agents/multi_agent_apps/ai_news_and_podcast_agents/)

### 🎮 Autonomous Game Playing Agents

*   [🎮 AI 3D Pygame Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_3dpygame_r1/)
*   [♜ AI Chess Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_chess_agent/)
*   [🎲 AI Tic-Tac-Toe Agent](advanced_ai_agents/autonomous_game_playing_agent_apps/ai_tic_tac_toe_agent/)

### 🤝 Multi-agent Teams

*   [🧲 AI Competitor Intelligence Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_competitor_intelligence_agent_team/)
*   [💲 AI Finance Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_finance_agent_team/)
*   [🎨 AI Game Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_game_design_agent_team/)
*   [👨‍⚖️ AI Legal Agent Team (Cloud &amp; Local)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_legal_agent_team/)
*   [💼 AI Recruitment Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_recruitment_agent_team/)
*   [👨‍💼 AI Services Agency (CrewAI)](advanced_ai_agents/multi_agent_apps/agent_teams/ai_services_agency/)
*   [👨‍🏫 AI Teaching Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/ai_teaching_agent_team/)
*   [💻 Multimodal Coding Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_coding_agent_team/)
*   [✨ Multimodal Design Agent Team](advanced_ai_agents/multi_agent_apps/agent_teams/multimodal_design_agent_team/)
*   [🌏 AI Travel Planner Agent Team](/advanced_ai_agents/multi_agent_apps/agent_teams/ai_travel_planner_agent_team/)

### 🗣️ Voice AI Agents

*   [🗣️ AI Audio Tour Agent](voice_ai_agents/ai_audio_tour_agent/)
*   [📞 Customer Support Voice Agent](voice_ai_agents/customer_support_voice_agent/)
*   [🔊 Voice RAG Agent (OpenAI SDK)](voice_ai_agents/voice_rag_openaisdk/)


### 🌐 MCP AI Agents

*   [♾️ Browser MCP Agent](mcp_ai_agents/browser_mcp_agent/)
*   [🐙 GitHub MCP Agent](mcp_ai_agents/github_mcp_agent/)
*   [📑 Notion MCP Agent](mcp_ai_agents/notion_mcp_agent) 
*   [🌍 AI Travel Planner MCP Agent](mcp_ai_agents/ai_travel_planner_mcp_agent_team)

### 📀 RAG (Retrieval Augmented Generation)
*   [🔗 Agentic RAG](rag_tutorials/agentic_rag/)
*   [🧐 Agentic RAG with Reasoning](rag_tutorials/agentic_rag_with_reasoning/)
*   [📰 AI Blog Search (RAG)](rag_tutorials/ai_blog_search/)
*   [🔍 Autonomous RAG](rag_tutorials/autonomous_rag/)
*   [🔄 Corrective RAG (CRAG)](rag_tutorials/corrective_rag/)
*   [🐋 Deepseek Local RAG Agent](rag_tutorials/deepseek_local_rag_agent/)
*   [🤔 Gemini Agentic RAG](rag_tutorials/gemini_agentic_rag/)
*   [👀 Hybrid Search RAG (Cloud)](rag_tutorials/hybrid_search_rag/)
*   [🔄 Llama 3.1 Local RAG](rag_tutorials/llama3.1_local_rag/)
*   [🖥️ Local Hybrid Search RAG](rag_tutorials/local_hybrid_search_rag/)
*   [🦙 Local RAG Agent](rag_tutorials/local_rag_agent/)
*   [🧩 RAG-as-a-Service](rag_tutorials/rag-as-a-service/)
*   [✨ RAG Agent with Cohere](rag_tutorials/rag_agent_cohere/)
*   [⛓️ Basic RAG Chain](rag_tutorials/rag_chain/)
*   [📠 RAG with Database Routing](rag_tutorials/rag_database_routing/)
*   [🖼️ Vision RAG](rag_tutorials/vision_rag/)

### 💾 LLM Apps with Memory Tutorials

*   [💾 AI ArXiv Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_arxiv_agent_memory/)
*   [🛩️ AI Travel Agent with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/ai_travel_agent_memory/)
*   [💬 Llama3 Stateful Chat](advanced_llm_apps/llm_apps_with_memory_tutorials/llama3_stateful_chat/)
*   [📝 LLM App with Personalized Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/llm_app_personalized_memory/)
*   [🗄️ Local ChatGPT Clone with Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/local_chatgpt_with_memory/)
*   [🧠 Multi-LLM Application with Shared Memory](advanced_llm_apps/llm_apps_with_memory_tutorials/multi_llm_memory/)


### 💬 Chat with X Tutorials

*   [💬 Chat with GitHub (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_github/)
*   [📨 Chat with Gmail](advanced_llm_apps/chat_with_X_tutorials/chat_with_gmail/)
*   [📄 Chat with PDF (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_pdf/)
*   [📚 Chat with Research Papers (ArXiv) (GPT &amp; Llama3)](advanced_llm_apps/chat_with_X_tutorials/chat_with_research_papers/)
*   [📝 Chat with Substack](advanced_llm_apps/chat_with_X_tutorials/chat_with_substack/)
*   [📽️ Chat with YouTube Videos](advanced_llm_apps/chat_with_X_tutorials/chat_with_youtube_videos/)

### 🔧 LLM Fine-tuning Tutorials

*   [🔧 Llama 3.2 Fine-tuning](advanced_llm_apps/llm_finetuning_tutorials/llama3.2_finetuning/)

## 🚀 Getting Started

1. **Clone the repository** 

    ```bash 
    git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git 
    ```

2. **Navigate to the desired project directory**

    ```bash 
    cd awesome-llm-apps/starter_ai_agents/ai_travel_agent
    ```

3. **Install the required dependencies**

    ```bash
    pip install -r requirements.txt
    ```

4. **Follow the project-specific instructions** in each project&#039;s `README.md` file to set up and run the app.

## 🤝 Contributing to Open Source

Contributions are welcome! If you have any ideas, improvements, or new apps to add, please create a new [GitHub Issue](https://github.com/Shubhamsaboo/awesome-llm-apps/issues) or submit a pull request. Make sure to follow the existing project structure and include a detailed `README.md` for each new app.

### Thank You, Community, for the Support! 🙏

[![Star History Chart](https://api.star-history.com/svg?repos=Shubhamsaboo/awesome-llm-apps&amp;type=Date)](https://star-history.com/#Shubhamsaboo/awesome-llm-apps&amp;Date)

🌟 **Don’t miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.**
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>