<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Sat, 21 Feb 2026 00:07:17 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2026, GitHub</copyright>
        <item>
            <title><![CDATA[PostHog/posthog]]></title>
            <link>https://github.com/PostHog/posthog</link>
            <guid>https://github.com/PostHog/posthog</guid>
            <pubDate>Sat, 21 Feb 2026 00:07:17 GMT</pubDate>
            <description><![CDATA[ü¶î PostHog is an all-in-one developer platform for building successful products. We offer product analytics, web analytics, session replay, error tracking, feature flags, experimentation, surveys, data warehouse, a CDP, and an AI product assistant to help debug your code, ship features faster, and keep all your usage and customer data in one stack.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/PostHog/posthog">PostHog/posthog</a></h1>
            <p>ü¶î PostHog is an all-in-one developer platform for building successful products. We offer product analytics, web analytics, session replay, error tracking, feature flags, experimentation, surveys, data warehouse, a CDP, and an AI product assistant to help debug your code, ship features faster, and keep all your usage and customer data in one stack.</p>
            <p>Language: Python</p>
            <p>Stars: 31,541</p>
            <p>Forks: 2,306</p>
            <p>Stars today: 46 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img alt=&quot;posthoglogo&quot; src=&quot;https://user-images.githubusercontent.com/65415371/205059737-c8a4f836-4889-4654-902e-f302b187b6a0.png&quot;&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&#039;https://posthog.com/contributors&#039;&gt;&lt;img alt=&quot;GitHub contributors&quot; src=&quot;https://img.shields.io/github/contributors/posthog/posthog&quot;/&gt;&lt;/a&gt;
  &lt;a href=&#039;http://makeapullrequest.com&#039;&gt;&lt;img alt=&#039;PRs Welcome&#039; src=&#039;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=shields&#039;/&gt;&lt;/a&gt;
  &lt;img alt=&quot;Docker Pulls&quot; src=&quot;https://img.shields.io/docker/pulls/posthog/posthog&quot;/&gt;
  &lt;a href=&quot;https://github.com/PostHog/posthog/commits/master&quot;&gt;&lt;img alt=&quot;GitHub commit activity&quot; src=&quot;https://img.shields.io/github/commit-activity/m/posthog/posthog&quot;/&gt; &lt;/a&gt;
  &lt;a href=&quot;https://github.com/PostHog/posthog/issues?q=is%3Aissue%20state%3Aclosed&quot;&gt;&lt;img alt=&quot;GitHub closed issues&quot; src=&quot;https://img.shields.io/github/issues-closed/posthog/posthog&quot;/&gt; &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://posthog.com/docs&quot;&gt;Docs&lt;/a&gt; - &lt;a href=&quot;https://posthog.com/community&quot;&gt;Community&lt;/a&gt; - &lt;a href=&quot;https://posthog.com/roadmap&quot;&gt;Roadmap&lt;/a&gt; - &lt;a href=&quot;https://posthog.com/why&quot;&gt;Why PostHog?&lt;/a&gt; - &lt;a href=&quot;https://posthog.com/changelog&quot;&gt;Changelog&lt;/a&gt; - &lt;a href=&quot;https://github.com/PostHog/posthog/issues/new?assignees=&amp;labels=bug&amp;template=bug_report.yml&quot;&gt;Bug reports&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.youtube.com/watch?v=2jQco8hEvTI&quot;&gt;
    &lt;img src=&quot;https://res.cloudinary.com/dmukukwp6/image/upload/demo_thumb_68d0d8d56d&quot; alt=&quot;PostHog Demonstration&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

## PostHog is an all-in-one, open source platform for building successful products

[PostHog](https://posthog.com/) provides every tool you need to build a successful product including:

- [Product Analytics](https://posthog.com/product-analytics): Autocapture or manually instrument event-based analytics to understand user behavior and analyze data with visualization or SQL.
- [Web Analytics](https://posthog.com/web-analytics): Monitor web traffic and user sessions with a GA-like dashboard. Easily monitor conversion, web vitals, and revenue.
- [Session Replays](https://posthog.com/session-replay): Watch real user sessions of interactions with your website or mobile app to diagnose issues and understand user behavior.
- [Feature Flags](https://posthog.com/feature-flags): Safely roll out features to select users or cohorts with feature flags.
- [Experiments](https://posthog.com/experiments): Test changes and measure their statistical impact on goal metrics. Set up experiments with no-code too.
- [Error Tracking](https://posthog.com/error-tracking): Track errors, get alerts, and resolve issues to improve your product.
- [Surveys](https://posthog.com/surveys): Ask anything with our collection of no-code survey templates, or build custom surveys with our survey builder.
- [Data warehouse](https://posthog.com/data-warehouse): Sync data from external tools like Stripe, Hubspot, your data warehouse, and more. Query it alongside your product data.
- [Data pipelines](https://posthog.com/cdp): Run custom filters and transformations on your incoming data. Send it to 25+ tools or any webhook in real time or batch export large amounts to your warehouse.
- [LLM analytics](https://posthog.com/docs/llm-analytics): Capture traces, generations, latency, and cost for your LLM-powered app.
- [Workflows](https://posthog.com/docs/workflows): Create workflows that automate actions or send messages to your users.

Best of all, all of this is free to use with a [generous monthly free tier](https://posthog.com/pricing) for each product. Get started by signing up for [PostHog Cloud US](https://us.posthog.com/signup) or [PostHog Cloud EU](https://eu.posthog.com/signup).

## Table of Contents

- [PostHog is an all-in-one, open source platform for building successful products](#posthog-is-an-all-in-one-open-source-platform-for-building-successful-products)
- [Table of Contents](#table-of-contents)
- [Getting started with PostHog](#getting-started-with-posthog)
  - [PostHog Cloud (Recommended)](#posthog-cloud-recommended)
  - [Self-hosting the open-source hobby deploy (Advanced)](#self-hosting-the-open-source-hobby-deploy-advanced)
- [Setting up PostHog](#setting-up-posthog)
- [Learning more about PostHog](#learning-more-about-posthog)
- [Contributing](#contributing)
- [Open-source vs. paid](#open-source-vs-paid)
- [We‚Äôre hiring!](#were-hiring)

## Getting started with PostHog

### PostHog Cloud (Recommended)

The fastest and most reliable way to get started with PostHog is signing up for free to¬†[PostHog Cloud](https://us.posthog.com/signup) or [PostHog Cloud EU](https://eu.posthog.com/signup). Your first 1 million events, 5k recordings, 1M flag requests, 100k exceptions, and 1500 survey responses are free every month, after which you pay based on usage.

### Self-hosting the open-source hobby deploy (Advanced)

If you want to self-host PostHog, you can deploy a hobby instance in one line on Linux with Docker (recommended 4GB memory):

```bash
/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/posthog/posthog/HEAD/bin/deploy-hobby)&quot;
```

Open source deployments should scale to approximately 100k events per month, after which we recommend [migrating to a PostHog Cloud](https://posthog.com/docs/migrate/migrate-to-cloud).

We _do not_ provide customer support or offer guarantees for open source deployments. See our [self-hosting docs](https://posthog.com/docs/self-host), [troubleshooting guide](https://posthog.com/docs/self-host/deploy/troubleshooting), and [disclaimer](https://posthog.com/docs/self-host/open-source/disclaimer) for more info.

## Setting up PostHog

Once you&#039;ve got a PostHog instance, you can set it up by installing our [JavaScript web snippet](https://posthog.com/docs/getting-started/install?tab=snippet), one of [our SDKs](https://posthog.com/docs/getting-started/install?tab=sdks), or by [using our API](https://posthog.com/docs/getting-started/install?tab=api).

We have SDKs and libraries for popular languages and frameworks like:

| Frontend                                              | Mobile                                                          | Backend                                             |
| ----------------------------------------------------- | --------------------------------------------------------------- | --------------------------------------------------- |
| [JavaScript](https://posthog.com/docs/libraries/js)   | [React Native](https://posthog.com/docs/libraries/react-native) | [Python](https://posthog.com/docs/libraries/python) |
| [Next.js](https://posthog.com/docs/libraries/next-js) | [Android](https://posthog.com/docs/libraries/android)           | [Node](https://posthog.com/docs/libraries/node)     |
| [React](https://posthog.com/docs/libraries/react)     | [iOS](https://posthog.com/docs/libraries/ios)                   | [PHP](https://posthog.com/docs/libraries/php)       |
| [Vue](https://posthog.com/docs/libraries/vue-js)      | [Flutter](https://posthog.com/docs/libraries/flutter)           | [Ruby](https://posthog.com/docs/libraries/ruby)     |

Beyond this, we have docs and guides for [Go](https://posthog.com/docs/libraries/go), [.NET/C#](https://posthog.com/docs/libraries/dotnet), [Django](https://posthog.com/docs/libraries/django), [Angular](https://posthog.com/docs/libraries/angular), [WordPress](https://posthog.com/docs/libraries/wordpress), [Webflow](https://posthog.com/docs/libraries/webflow), and more.

Once you&#039;ve installed PostHog, see our [product docs](https://posthog.com/docs/product-os) for more information on how to set up [product analytics](https://posthog.com/docs/product-analytics/capture-events), [web analytics](https://posthog.com/docs/web-analytics/getting-started), [session replays](https://posthog.com/docs/session-replay/how-to-watch-recordings), [feature flags](https://posthog.com/docs/feature-flags/creating-feature-flags), [experiments](https://posthog.com/docs/experiments/creating-an-experiment), [error tracking](https://posthog.com/docs/error-tracking/installation#setting-up-exception-autocapture), [surveys](https://posthog.com/docs/surveys/installation), [data warehouse](https://posthog.com/docs/cdp/sources), and more.

## Learning more about PostHog

Our code isn&#039;t the only thing that&#039;s open source üò≥. We also open source our [company handbook](https://posthog.com/handbook) which details our [strategy](https://posthog.com/handbook/why-does-posthog-exist), [ways of working](https://posthog.com/handbook/company/culture), and [processes](https://posthog.com/handbook/team-structure).

Curious about how to make the most of PostHog? We wrote a guide to [winning with PostHog](https://posthog.com/docs/new-to-posthog/getting-hogpilled) which walks you through the basics of [measuring activation](https://posthog.com/docs/new-to-posthog/activation), [tracking retention](https://posthog.com/docs/new-to-posthog/retention), and [capturing revenue](https://posthog.com/docs/new-to-posthog/revenue).

## Contributing

We &lt;3 contributions big and small:

- Vote on features or get early access to beta functionality in our [roadmap](https://posthog.com/roadmap)
- Open a PR (see our instructions on [developing PostHog locally](https://posthog.com/handbook/engineering/developing-locally))
- Submit a [feature request](https://github.com/PostHog/posthog/issues/new?assignees=&amp;labels=enhancement%2C+feature&amp;template=feature_request.yml) or [bug report](https://github.com/PostHog/posthog/issues/new?assignees=&amp;labels=bug&amp;template=bug_report.yml)

For an overview of the codebase structure, see [monorepo layout](docs/internal/monorepo-layout.md) and [products](products/README.md).

## Open-source vs. paid

This repo is available under the [MIT expat license](https://github.com/PostHog/posthog/blob/master/LICENSE), except for the `ee` directory (which has its [license here](https://github.com/PostHog/posthog/blob/master/ee/LICENSE)) if applicable.

Need _absolutely üíØ% FOSS_? Check out our [posthog-foss](https://github.com/PostHog/posthog-foss) repository, which is purged of all proprietary code and features.

The pricing for our paid plan is completely transparent and available on [our pricing page](https://posthog.com/pricing).

## We&#039;re hiring!

&lt;img src=&quot;https://res.cloudinary.com/dmukukwp6/image/upload/v1/posthog.com/src/components/Home/images/mission-control-hog&quot; alt=&quot;Hedgehog working on a Mission Control Center&quot; width=&quot;350px&quot;/&gt;

Hey! If you&#039;re reading this, you&#039;ve proven yourself as a dedicated README reader.

You might also make a great addition to our team. We&#039;re growing fast [and would love for you to join us](https://posthog.com/careers).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[anthropics/claude-plugins-official]]></title>
            <link>https://github.com/anthropics/claude-plugins-official</link>
            <guid>https://github.com/anthropics/claude-plugins-official</guid>
            <pubDate>Sat, 21 Feb 2026 00:07:16 GMT</pubDate>
            <description><![CDATA[Official, Anthropic-managed directory of high quality Claude Code Plugins.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/anthropics/claude-plugins-official">anthropics/claude-plugins-official</a></h1>
            <p>Official, Anthropic-managed directory of high quality Claude Code Plugins.</p>
            <p>Language: Python</p>
            <p>Stars: 7,865</p>
            <p>Forks: 774</p>
            <p>Stars today: 75 stars today</p>
            <h2>README</h2><pre># Claude Code Plugins Directory

A curated directory of high-quality plugins for Claude Code.

&gt; **‚ö†Ô∏è Important:** Make sure you trust a plugin before installing, updating, or using it. Anthropic does not control what MCP servers, files, or other software are included in plugins and cannot verify that they will work as intended or that they won&#039;t change. See each plugin&#039;s homepage for more information.

## Structure

- **`/plugins`** - Internal plugins developed and maintained by Anthropic
- **`/external_plugins`** - Third-party plugins from partners and the community

## Installation

Plugins can be installed directly from this marketplace via Claude Code&#039;s plugin system.

To install, run `/plugin install {plugin-name}@claude-plugin-directory`

or browse for the plugin in `/plugin &gt; Discover`

## Contributing

### Internal Plugins

Internal plugins are developed by Anthropic team members. See `/plugins/example-plugin` for a reference implementation.

### External Plugins

Third-party partners can submit plugins for inclusion in the marketplace. External plugins must meet quality and security standards for approval. To submit a new plugin, use the [plugin directory submission form](https://clau.de/plugin-directory-submission).

## Plugin Structure

Each plugin follows a standard structure:

```
plugin-name/
‚îú‚îÄ‚îÄ .claude-plugin/
‚îÇ   ‚îî‚îÄ‚îÄ plugin.json      # Plugin metadata (required)
‚îú‚îÄ‚îÄ .mcp.json            # MCP server configuration (optional)
‚îú‚îÄ‚îÄ commands/            # Slash commands (optional)
‚îú‚îÄ‚îÄ agents/              # Agent definitions (optional)
‚îú‚îÄ‚îÄ skills/              # Skill definitions (optional)
‚îî‚îÄ‚îÄ README.md            # Documentation
```

## License

Please see each linked plugin for the relevant LICENSE file.

## Documentation

For more information on developing Claude Code plugins, see the [official documentation](https://code.claude.com/docs/en/plugins).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[google-research/timesfm]]></title>
            <link>https://github.com/google-research/timesfm</link>
            <guid>https://github.com/google-research/timesfm</guid>
            <pubDate>Sat, 21 Feb 2026 00:07:15 GMT</pubDate>
            <description><![CDATA[TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google-research/timesfm">google-research/timesfm</a></h1>
            <p>TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.</p>
            <p>Language: Python</p>
            <p>Stars: 8,840</p>
            <p>Forks: 731</p>
            <p>Stars today: 404 stars today</p>
            <h2>README</h2><pre># TimesFM

TimesFM (Time Series Foundation Model) is a pretrained time-series foundation
model developed by Google Research for time-series forecasting.

*   Paper:
    [A decoder-only foundation model for time-series forecasting](https://arxiv.org/abs/2310.10688),
    ICML 2024.
*   All checkpoints:
    [TimesFM Hugging Face Collection](https://huggingface.co/collections/google/timesfm-release-66e4be5fdb56e960c1e482a6).
*   [Google Research blog](https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/).
*   [TimesFM in BigQuery](https://cloud.google.com/bigquery/docs/timesfm-model):
    an official Google product.

This open version is not an officially supported Google product.

**Latest Model Version:** TimesFM 2.5

**Archived Model Versions:**

-   1.0 and 2.0: relevant code archived in the sub directory `v1`. You can `pip
    install timesfm==1.3.0` to install an older version of this package to load
    them.

## Update - Oct. 29, 2025

Added back the covariate support through XReg for TimesFM 2.5.


## Update - Sept. 15, 2025

TimesFM 2.5 is out!

Comparing to TimesFM 2.0, this new 2.5 model:

-   uses 200M parameters, down from 500M.
-   supports up to 16k context length, up from 2048.
-   supports continuous quantile forecast up to 1k horizon via an optional 30M
    quantile head.
-   gets rid of the `frequency` indicator.
-   has a couple of new forecasting flags.

Along with the model upgrade we have also upgraded the inference API. This repo
will be under construction over the next few weeks to

1.  add support for an upcoming Flax version of the model (faster inference).
2.  add back covariate support.
3.  populate more docstrings, docs and notebook.

### Install

1.  Clone the repository:
    ```shell
    git clone https://github.com/google-research/timesfm.git
    cd timesfm
    ```

2.  Create a virtual environment and install dependencies using `uv`:
    ```shell
    # Create a virtual environment
    uv venv
    
    # Activate the environment
    source .venv/bin/activate
    
    # Install the package in editable mode with torch
    uv pip install -e .[torch]
    # Or with flax
    uv pip install -e .[flax]
    # Or XReg is needed
    uv pip install -e .[xreg]
    ```

3. [Optional] Install your preferred `torch` / `jax` backend based on your OS and accelerators
(CPU, GPU, TPU or Apple Silicon).:

-   [Install PyTorch](https://pytorch.org/get-started/locally/).
-   [Install Jax](https://docs.jax.dev/en/latest/installation.html#installation)
    for Flax.

### Code Example

```python
import torch
import numpy as np
import timesfm

torch.set_float32_matmul_precision(&quot;high&quot;)

model = timesfm.TimesFM_2p5_200M_torch.from_pretrained(&quot;google/timesfm-2.5-200m-pytorch&quot;)

model.compile(
    timesfm.ForecastConfig(
        max_context=1024,
        max_horizon=256,
        normalize_inputs=True,
        use_continuous_quantile_head=True,
        force_flip_invariance=True,
        infer_is_positive=True,
        fix_quantile_crossing=True,
    )
)
point_forecast, quantile_forecast = model.forecast(
    horizon=12,
    inputs=[
        np.linspace(0, 1, 100),
        np.sin(np.linspace(0, 20, 67)),
    ],  # Two dummy inputs
)
point_forecast.shape  # (2, 12)
quantile_forecast.shape  # (2, 12, 10): mean, then 10th to 90th quantiles.
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[roboflow/trackers]]></title>
            <link>https://github.com/roboflow/trackers</link>
            <guid>https://github.com/roboflow/trackers</guid>
            <pubDate>Sat, 21 Feb 2026 00:07:14 GMT</pubDate>
            <description><![CDATA[Trackers gives you clean, modular re-implementations of leading multi-object tracking algorithms released under the permissive Apache 2.0 license. You combine them with any detection model you already use.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/roboflow/trackers">roboflow/trackers</a></h1>
            <p>Trackers gives you clean, modular re-implementations of leading multi-object tracking algorithms released under the permissive Apache 2.0 license. You combine them with any detection model you already use.</p>
            <p>Language: Python</p>
            <p>Stars: 2,719</p>
            <p>Forks: 263</p>
            <p>Stars today: 133 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
    &lt;img width=&quot;200&quot; src=&quot;https://raw.githubusercontent.com/roboflow/trackers/refs/heads/release/stable/docs/assets/logo-trackers-violet.svg&quot; alt=&quot;trackers logo&quot;&gt;
    &lt;h1&gt;trackers&lt;/h1&gt;
    &lt;p&gt;Plug-and-play multi-object tracking for any detection model.&lt;/p&gt;

[![version](https://badge.fury.io/py/trackers.svg)](https://badge.fury.io/py/trackers)
[![downloads](https://img.shields.io/pypi/dm/trackers)](https://pypistats.org/packages/trackers)
[![license](https://img.shields.io/badge/license-Apache%202.0-blue)](https://github.com/roboflow/trackers/blob/release/stable/LICENSE.md)
[![python-version](https://img.shields.io/pypi/pyversions/trackers)](https://badge.fury.io/py/trackers)
[![hf space](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/Roboflow/Trackers)
[![colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-track-objects-with-bytetrack-tracker.ipynb)
[![discord](https://img.shields.io/discord/1159501506232451173?logo=discord&amp;label=discord&amp;labelColor=fff&amp;color=5865f2&amp;link=https%3A%2F%2Fdiscord.gg%2FGbfgXGJ8Bk)](https://discord.gg/GbfgXGJ8Bk)

&lt;/div&gt;

## Try It

No install needed. Try trackers in your browser with our [Hugging Face Playground](https://huggingface.co/spaces/roboflow/trackers).

## Install

```bash
pip install trackers
```

&lt;details&gt;
&lt;summary&gt;install from source&lt;/summary&gt;

```bash
pip install git+https://github.com/roboflow/trackers.git
```

&lt;/details&gt;

https://github.com/user-attachments/assets/eef9b00a-cfe4-40f7-a495-954550e3ef1f

## Track from CLI

Point at a video, webcam, RTSP stream, or image directory. Get tracked output.

Use our [interactive command builder](https://trackers.roboflow.com/develop/learn/track) to configure your tracking pipeline.

```bash
trackers track \
    --source video.mp4 \
    --output output.mp4 \
    --model rfdetr-medium \
    --tracker bytetrack \
    --show-labels \
    --show-trajectories
```

## Track from Python

Plug trackers into your existing detection pipeline. Works with any detector.

```python
import cv2
import supervision as sv
from inference import get_model
from trackers import ByteTrackTracker

model = get_model(model_id=&quot;rfdetr-medium&quot;)
tracker = ByteTrackTracker()

label_annotator = sv.LabelAnnotator()
trajectory_annotator = sv.TrajectoryAnnotator()

cap = cv2.VideoCapture(&quot;video.mp4&quot;)
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    result = model.infer(frame)[0]
    detections = sv.Detections.from_inference(result)
    tracked = tracker.update(detections)

    frame = label_annotator.annotate(frame, tracked)
    frame = trajectory_annotator.annotate(frame, tracked)
```

## Evaluate

Benchmark your tracker against ground truth with standard MOT metrics.

```bash
trackers eval \
    --gt-dir data/gt \
    --tracker-dir data/trackers \
    --metrics CLEAR HOTA Identity
```

```
Sequence                        MOTA    HOTA    IDF1  IDSW
----------------------------------------------------------
MOT17-02-FRCNN                75.600  62.300  72.100    42
MOT17-04-FRCNN                78.200  65.100  74.800    31
----------------------------------------------------------
COMBINED                      75.033  62.400  72.033    73
```

## Algorithms

Clean, modular implementations of leading trackers. See the [tracker comparison](https://trackers.roboflow.com/develop/trackers/comparison/) for detailed benchmarks.

|                   Algorithm                   |  MOT17   | SportsMOT | SoccerNet |
| :-------------------------------------------: | :------: | :-------: | :-------: |
|   [SORT](https://arxiv.org/abs/1602.00763)    |   58.4   |   70.9    |   81.6    |
| [ByteTrack](https://arxiv.org/abs/2110.06864) | **60.1** | **73.0**  | **84.0**  |
|  [OC-SORT](https://arxiv.org/abs/2203.14360)  |    ‚Äî     |     ‚Äî     |     ‚Äî     |
| [BoT-SORT](https://arxiv.org/abs/2206.14651)  |    ‚Äî     |     ‚Äî     |     ‚Äî     |
|  [McByte](https://arxiv.org/abs/2506.01373)   |    ‚Äî     |     ‚Äî     |     ‚Äî     |

## Contributing

We welcome contributions. Read our [contributor guidelines](https://github.com/roboflow/trackers/blob/release/stable/CONTRIBUTING.md) to get started.

## License

The code is released under the [Apache 2.0 license](https://github.com/roboflow/trackers/blob/release/stable/LICENSE).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[huggingface/skills]]></title>
            <link>https://github.com/huggingface/skills</link>
            <guid>https://github.com/huggingface/skills</guid>
            <pubDate>Sat, 21 Feb 2026 00:07:13 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/huggingface/skills">huggingface/skills</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 1,417</p>
            <p>Forks: 130</p>
            <p>Stars today: 33 stars today</p>
            <h2>README</h2><pre># Hugging Face Skills

Hugging Face Skills are definitions for AI/ML tasks like dataset creation, model training, and evaluation. They are interoperable with all major coding agent tools like OpenAI Codex, Anthropic&#039;s Claude Code, Google DeepMind&#039;s Gemini CLI, and Cursor.

The Skills in this repository follow the standardized format [Agent Skill](https://agentskills.io/home) format.

## How do Skills work?

In practice, skills are self-contained folders that package instructions, scripts, and resources together for an AI agent to use on a specific use case. Each folder includes a `SKILL.md` file with YAML frontmatter (name and description) followed by the guidance your coding agent follows while the skill is active. 

&gt; [!NOTE]
&gt; &#039;Skills&#039; is actually an Anthropic term used within Claude AI and Claude Code and not adopted by other agent tools, but we love it! OpenAI Codex uses an `AGENTS.md` file to define the instructions for your coding agent. Google Gemini uses &#039;extensions&#039; to define the instructions for your coding agent in a `gemini-extension.json` file. **This repo is compatible with all of them, and more!**

&gt; [!TIP]
&gt; If your agent doesn&#039;t support skills, you can use [`agents/AGENTS.md`](agents/AGENTS.md) directly as a fallback.

## Installation

Hugging Face skills are compatible with Claude Code, Codex, Gemini CLI, and Cursor.

### Claude Code

1. Register the repository as a plugin marketplace:  
   
```
/plugin marketplace add huggingface/skills
```

2. To install a skill, run:  
   
```
/plugin install &lt;skill-name&gt;@huggingface/skills
```

For example:  

```
/plugin install hugging-face-cli@huggingface/skills
```

### Codex

1. Codex will identify the skills via the `AGENTS.md` file. You can verify the instructions are loaded with:

```
codex --ask-for-approval never &quot;Summarize the current instructions.&quot;
```

2. For more details, see the [Codex AGENTS guide](https://developers.openai.com/codex/guides/agents-md).

### Gemini CLI

1. This repo includes `gemini-extension.json` to integrate with the Gemini CLI.

2. Install locally:  

```
gemini extensions install . --consent
```

or use the GitHub URL:

```
gemini extensions install https://github.com/huggingface/skills.git --consent
```

4. See [Gemini CLI extensions docs](https://geminicli.com/docs/extensions/#installing-an-extension) for more help.

### Cursor

This repository includes Cursor plugin manifests:

- `.cursor-plugin/plugin.json`
- `.mcp.json` (configured with the Hugging Face MCP server URL)

Install from repository URL (or local checkout) via the Cursor plugin flow.

For contributors, regenerate manifests with:

```bash
./scripts/publish.sh
```

## Skills

This repository contains a few skills to get you started. You can also contribute your own skills to the repository.

### Available skills

&lt;!-- This table is auto-generated by scripts/generate_agents.py. Do not edit manually. --&gt;
&lt;!-- BEGIN_SKILLS_TABLE --&gt;
| Name | Description | Documentation |
|------|-------------|---------------|
| `hugging-face-cli` | Execute Hugging Face Hub operations using the hf CLI. Download models/datasets, upload files, manage repos, and run cloud compute jobs. | [SKILL.md](skills/hugging-face-cli/SKILL.md) |
| `hugging-face-datasets` | Create and manage datasets on Hugging Face Hub. Supports initializing repos, defining configs/system prompts, streaming row updates, and SQL-based dataset querying/transformation. | [SKILL.md](skills/hugging-face-datasets/SKILL.md) |
| `hugging-face-evaluation` | Add and manage evaluation results in Hugging Face model cards. Supports extracting eval tables from README content, importing scores from Artificial Analysis API, and running custom evaluations with vLLM/lighteval. | [SKILL.md](skills/hugging-face-evaluation/SKILL.md) |
| `hugging-face-jobs` | Run compute jobs on Hugging Face infrastructure. Execute Python scripts, manage scheduled jobs, and monitor job status. | [SKILL.md](skills/hugging-face-jobs/SKILL.md) |
| `hugging-face-model-trainer` | Train or fine-tune language models using TRL on Hugging Face Jobs infrastructure. Covers SFT, DPO, GRPO and reward modeling training methods, plus GGUF conversion for local deployment. Includes hardware selection, cost estimation, Trackio monitoring, and Hub persistence. | [SKILL.md](skills/hugging-face-model-trainer/SKILL.md) |
| `hugging-face-paper-publisher` | Publish and manage research papers on Hugging Face Hub. Supports creating paper pages, linking papers to models/datasets, claiming authorship, and generating professional markdown-based research articles. | [SKILL.md](skills/hugging-face-paper-publisher/SKILL.md) |
| `hugging-face-tool-builder` | Build reusable scripts for Hugging Face API operations. Useful for chaining API calls or automating repeated tasks. | [SKILL.md](skills/hugging-face-tool-builder/SKILL.md) |
| `hugging-face-trackio` | Track and visualize ML training experiments with Trackio. Log metrics via Python API and retrieve them via CLI. Supports real-time dashboards synced to HF Spaces. | [SKILL.md](skills/hugging-face-trackio/SKILL.md) |
&lt;!-- END_SKILLS_TABLE --&gt;

### Using skills in your coding agent

Once a skill is installed, mention it directly while giving your coding agent instructions:

- &quot;Use the HF LLM trainer skill to estimate the GPU memory needed for a 70B model run.&quot;
- &quot;Use the HF model evaluation skill to launch `run_eval_job.py` on the latest checkpoint.&quot;
- &quot;Use the HF dataset creator skill to draft new few-shot classification templates.&quot;
- &quot;Use the HF paper publisher skill to index my arXiv paper and link it to my model.&quot;

Your coding agent automatically loads the corresponding `SKILL.md` instructions and helper scripts while it completes the task.

### Contribute or customize a skill

1. Copy one of the existing skill folders (for example, `hf-datasets/`) and rename it.
2. Update the new folder&#039;s `SKILL.md` frontmatter:
   ```markdown
   ---
   name: my-skill-name
   description: Describe what the skill does and when to use it
   ---

   # Skill Title
   Guidance + examples + guardrails
   ```
3. Add or edit supporting scripts, templates, and documents referenced by your instructions.
4. Add an entry to `.claude-plugin/marketplace.json` with a concise, human-readable description.
5. Run:
   ```bash
   ./scripts/publish.sh
   ```
   to regenerate and validate all generated metadata.
6. Reinstall or reload the skill bundle in your coding agent so the updated folder is available.

### Marketplace

The `.claude-plugin/marketplace.json` file lists skills with human-readable descriptions for the plugin marketplace. The CI validates that skill names and paths match between `SKILL.md` files and `marketplace.json`, but descriptions are maintained separately: `SKILL.md` descriptions guide when Claude activates the skill, while marketplace descriptions are written for humans browsing available skills.

### Additional references
- Browse the latest instructions, scripts, and templates directly at [huggingface/skills](https://github.com/huggingface/skills).
- Review Hugging Face documentation for the specific libraries or workflows you reference inside each skill.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[databricks-solutions/ai-dev-kit]]></title>
            <link>https://github.com/databricks-solutions/ai-dev-kit</link>
            <guid>https://github.com/databricks-solutions/ai-dev-kit</guid>
            <pubDate>Sat, 21 Feb 2026 00:07:12 GMT</pubDate>
            <description><![CDATA[Databricks Toolkit for Coding Agents provided by Field Engineering]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/databricks-solutions/ai-dev-kit">databricks-solutions/ai-dev-kit</a></h1>
            <p>Databricks Toolkit for Coding Agents provided by Field Engineering</p>
            <p>Language: Python</p>
            <p>Stars: 539</p>
            <p>Forks: 94</p>
            <p>Stars today: 45 stars today</p>
            <h2>README</h2><pre># Databricks AI Dev Kit

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Databricks-Certified%20Gold%20Project-FFD700?style=for-the-badge&amp;logo=databricks&amp;logoColor=black&quot; alt=&quot;Databricks Certified Gold Project&quot;&gt;
&lt;/p&gt;

---

## Overview

AI-Driven Development (vibe coding) on Databricks just got a whole lot better. The **AI Dev Kit** gives your AI coding assistant (Claude Code, Cursor, Windsurf, etc.) the trusted sources it needs to build faster and smarter on Databricks.

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;databricks-tools-core/docs/architecture.svg&quot; alt=&quot;Architecture&quot; width=&quot;700&quot;&gt;
&lt;/p&gt;

---

## What Can I Build?

- **Spark Declarative Pipelines** (streaming tables, CDC, SCD Type 2, Auto Loader)
- **Databricks Jobs** (scheduled workflows, multi-task DAGs)
- **AI/BI Dashboards** (visualizations, KPIs, analytics)
- **Unity Catalog** (tables, volumes, governance)
- **Genie Spaces** (natural language data exploration)
- **Knowledge Assistants** (RAG-based document Q&amp;A)
- **MLflow Experiments** (evaluation, scoring, traces)
- **Model Serving** (deploy ML models and AI agents to endpoints)
- **Databricks Apps** (full-stack web applications)
- ...and more

---

## Choose Your Own Adventure

| Adventure                        | Best For | Start Here |
|----------------------------------|----------|------------|
| :star: [**Install AI Dev Kit**](#install-in-existing-project) | **Start here!** Follow quick install instructions to add to your existing project folder | [Quick Start (install)](#install-in-existing-project)
| [**Visual Builder App**](#visual-builder-app) | Web-based UI for Databricks development | `databricks-builder-app/` |
| [**Core Library**](#core-library) | Building custom integrations (LangChain, OpenAI, etc.) | `pip install` |
| [**Skills Only**](databricks-skills/) | Provide Databricks patterns and best practices (without MCP functions) | Install skills |
| [**MCP Tools Only**](databricks-mcp-server/) | Just executable actions (no guidance) | Register MCP server |
---

## Quick Start

### Prerequisites

- [uv](https://github.com/astral-sh/uv) - Python package manager
- [Databricks CLI](https://docs.databricks.com/aws/en/dev-tools/cli/) - Command line interface for Databricks
- AI coding environment
  - [Claude Code](https://claude.ai/code)
  - [Cursor](https://cursor.com)


### Install in existing project
By default this will install at a project level rather than a user level. This is often a good fit, but requires you to run your client from the exact directory that was used for the install.  
_Note: Project configuration files can be re-used in other projects. You find these configs under .claude or .cursor_

#### Mac / Linux

**Basic installation** (uses DEFAULT profile, project scope)

```bash
bash &lt;(curl -sL https://raw.githubusercontent.com/databricks-solutions/ai-dev-kit/main/install.sh)
```

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Advanced Options&lt;/strong&gt; (click to expand)&lt;/summary&gt;

**Global installation with force reinstall**

```bash
bash &lt;(curl -sL https://raw.githubusercontent.com/databricks-solutions/ai-dev-kit/main/install.sh) --global --force
```

**Specify profile and force reinstall**

```bash
bash &lt;(curl -sL https://raw.githubusercontent.com/databricks-solutions/ai-dev-kit/main/install.sh) --profile DEFAULT --force
```

**Install for specific tools only**

```bash
bash &lt;(curl -sL https://raw.githubusercontent.com/databricks-solutions/ai-dev-kit/main/install.sh) --tools cursor
```

&lt;/details&gt;

**Next steps:** Respond to interactive prompts and follow the on-screen instructions.
- Note: Cursor and Copilot require updating settings manually after install.

#### Windows (PowerShell)

**Basic installation** (uses DEFAULT profile, project scope)

```powershell
irm https://raw.githubusercontent.com/databricks-solutions/ai-dev-kit/main/install.ps1 | iex
```

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Advanced Options&lt;/strong&gt; (click to expand)&lt;/summary&gt;

**Download script first**

```powershell
irm https://raw.githubusercontent.com/databricks-solutions/ai-dev-kit/main/install.ps1 -OutFile install.ps1
```

**Global installation with force reinstall**

```powershell
.\install.ps1 -Global -Force
```

**Specify profile and force reinstall**

```powershell
.\install.ps1 -Profile DEFAULT -Force
```

**Install for specific tools only**

```powershell
.\install.ps1 -Tools cursor
```

&lt;/details&gt;

**Next steps:** Respond to interactive prompts and follow the on-screen instructions.
- Note: Cursor and Copilot require updating settings manually after install.


### Visual Builder App

Full-stack web application with chat UI for Databricks development:

```bash
cd ai-dev-kit/databricks-builder-app
./scripts/setup.sh
# Follow instructions to start the app
```


### Core Library

Use `databricks-tools-core` directly in your Python projects:

```python
from databricks_tools_core.sql import execute_sql

results = execute_sql(&quot;SELECT * FROM my_catalog.schema.table LIMIT 10&quot;)
```

Works with LangChain, OpenAI Agents SDK, or any Python framework. See [databricks-tools-core/](databricks-tools-core/) for details.

---

## What&#039;s Included

| Component | Description |
|-----------|-------------|
| [`databricks-tools-core/`](databricks-tools-core/) | Python library with high-level Databricks functions |
| [`databricks-mcp-server/`](databricks-mcp-server/) | MCP server exposing 50+ tools for AI assistants |
| [`databricks-skills/`](databricks-skills/) | 19 markdown skills teaching Databricks patterns |
| [`databricks-builder-app/`](databricks-builder-app/) | Full-stack web app with Claude Code integration |

---

## Star History

&lt;a href=&quot;https://star-history.com/#databricks-solutions/ai-dev-kit&amp;Date&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=databricks-solutions/ai-dev-kit&amp;type=Date&amp;theme=dark&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=databricks-solutions/ai-dev-kit&amp;type=Date&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=databricks-solutions/ai-dev-kit&amp;type=Date&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;

---

## License

(c) 2026 Databricks, Inc. All rights reserved.

The source in this project is provided subject to the [Databricks License](https://databricks.com/db-license-source). See [LICENSE.md](LICENSE.md) for details.

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Third-Party Licenses&lt;/strong&gt;&lt;/summary&gt;

| Package | Version | License | Project URL |
|---------|---------|---------|-------------|
| [fastmcp](https://github.com/jlowin/fastmcp) | ‚â•0.1.0 | MIT | https://github.com/jlowin/fastmcp |
| [mcp](https://github.com/modelcontextprotocol/python-sdk) | ‚â•1.0.0 | MIT | https://github.com/modelcontextprotocol/python-sdk |
| [sqlglot](https://github.com/tobymao/sqlglot) | ‚â•20.0.0 | MIT | https://github.com/tobymao/sqlglot |
| [sqlfluff](https://github.com/sqlfluff/sqlfluff) | ‚â•3.0.0 | MIT | https://github.com/sqlfluff/sqlfluff |
| [litellm](https://github.com/BerriAI/litellm) | ‚â•1.0.0 | MIT | https://github.com/BerriAI/litellm |
| [pymupdf](https://github.com/pymupdf/PyMuPDF) | ‚â•1.24.0 | AGPL-3.0 | https://github.com/pymupdf/PyMuPDF |
| [claude-agent-sdk](https://github.com/anthropics/claude-code) | ‚â•0.1.19 | MIT | https://github.com/anthropics/claude-code |
| [fastapi](https://github.com/fastapi/fastapi) | ‚â•0.115.8 | MIT | https://github.com/fastapi/fastapi |
| [uvicorn](https://github.com/encode/uvicorn) | ‚â•0.34.0 | BSD-3-Clause | https://github.com/encode/uvicorn |
| [httpx](https://github.com/encode/httpx) | ‚â•0.28.0 | BSD-3-Clause | https://github.com/encode/httpx |
| [sqlalchemy](https://github.com/sqlalchemy/sqlalchemy) | ‚â•2.0.41 | MIT | https://github.com/sqlalchemy/sqlalchemy |
| [alembic](https://github.com/sqlalchemy/alembic) | ‚â•1.16.1 | MIT | https://github.com/sqlalchemy/alembic |
| [asyncpg](https://github.com/MagicStack/asyncpg) | ‚â•0.30.0 | Apache-2.0 | https://github.com/MagicStack/asyncpg |
| [greenlet](https://github.com/python-greenlet/greenlet) | ‚â•3.0.0 | MIT | https://github.com/python-greenlet/greenlet |
| [psycopg2-binary](https://github.com/psycopg/psycopg2) | ‚â•2.9.11 | LGPL-3.0 | https://github.com/psycopg/psycopg2 |

&lt;/details&gt;

---

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Acknowledgments&lt;/strong&gt;&lt;/summary&gt;

MCP Databricks Command Execution API from [databricks-exec-code](https://github.com/databricks-solutions/databricks-exec-code-mcp) by Natyra Bajraktari and Henryk Borzymowski.

&lt;/details&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[freemocap/freemocap]]></title>
            <link>https://github.com/freemocap/freemocap</link>
            <guid>https://github.com/freemocap/freemocap</guid>
            <pubDate>Sat, 21 Feb 2026 00:07:11 GMT</pubDate>
            <description><![CDATA[Free Motion Capture for Everyone üíÄ‚ú®]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/freemocap/freemocap">freemocap/freemocap</a></h1>
            <p>Free Motion Capture for Everyone üíÄ‚ú®</p>
            <p>Language: Python</p>
            <p>Stars: 5,557</p>
            <p>Forks: 441</p>
            <p>Stars today: 497 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://github.com/freemocap/freemocap/assets/15314521/da1af7fe-f808-43dc-8f59-c579715d6593&quot; height=&quot;240&quot; alt=&quot;Project Logo&quot;&gt;
&lt;/p&gt; 


&lt;h3 align=&quot;center&quot;&gt;The FreeMoCap Project&lt;/h3&gt;
&lt;h4 align=&quot;center&quot;&gt; A free-and-open-source, hardware-and-software-agnostic, minimal-cost, research-grade, motion capture
system and platform for decentralized scientific research, education, and training&lt;/h2&gt;


&lt;p align=&quot;center&quot;&gt;

&lt;a href=&quot;https://doi.org/10.5281/zenodo.7233714&quot;&gt;
    &lt;img src=&quot;https://zenodo.org/badge/DOI/10.5281/zenodo.7233714.svg&quot; alt=DOI-via-Zenodo.org&gt;
  &lt;/a&gt;

&lt;a href=&quot;https://github.com/psf/black&quot;&gt;
    &lt;img alt=&quot;https://img.shields.io/badge/code%20style-black-000000.svg&quot; src=&quot;https://img.shields.io/badge/code%20style-black-000000.svg&quot;&gt;
  &lt;/a&gt;

&lt;a href=&quot;https://github.com/freemocap/freemocap/releases/latest&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/github/release/freemocap/freemocap.svg&quot; alt=&quot;Latest Release&quot;&gt;
    &lt;/a&gt;

&lt;a href=&quot;https://github.com/freemocap/freemocap/blob/main/LICENSE&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/license-AGPL-blue.svg&quot; alt=&quot;AGPLv3&quot;&gt;
    &lt;/a&gt;

&lt;a href=&quot;https://github.com/freemocap/freemocap/issues&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/contributions-welcome-ff69b4.svg&quot; alt=&quot;Contributions Welcome&quot;&gt;
    &lt;/a&gt;

&lt;a href=&quot;https://github.com/psf/black&quot;&gt;
    &lt;img alt=&quot;https://img.shields.io/badge/code%20style-black-000000.svg&quot; src=&quot;https://img.shields.io/badge/code%20style-black-000000.svg&quot;&gt;
  &lt;/a&gt;

&lt;a href=&quot;https://discord.gg/SgdnzbHDTG&quot;&gt;
    &lt;img alt=&quot;Discord Community Server&quot; src=&quot;https://dcbadge.vercel.app/api/server/SgdnzbHDTG?style=flat&quot;&gt;
  &lt;/a&gt;


&lt;/p&gt;


https://user-images.githubusercontent.com/15314521/192062522-2a8d9305-f181-4869-a4b9-1aa068e094c9.mp4





--
## QUICKSTART

&gt; [!NOTE] 
&gt; For  detailed installation instructions, see our [official documentation&#039;s Installation page](https://freemocap.github.io/documentation/installation.html#detailed-pip-installation-instructions)


#### 0. Create a a Python 3.10 through 3.12 environment (python3.12 recommended)
#### 1. Install software via [pip](https://pypi.org/project/freemocap/#description):

```
pip install freemocap
```

#### 2. Launch the GUI by entering the command:

```
freemocap
``` 

####  3. A GUI should pop up that looks like this: 

   &lt;img width=&quot;1457&quot; alt=&quot;image&quot; src=&quot;https://github.com/freemocap/freemocap/assets/15314521/90ef7e7b-48f3-4f46-8d4a-5b5bcc3254b3&quot;&gt;

#### 4. Have fun! See the [Beginner Tutorials](https://freemocap.github.io/documentation/your-first-recording.html) on our official docs for detailed instructions.

#### 5. [Join the Discord and let us know how it went!](https://discord.gg/nxv5dNTfKT)



___
## Install/run from source code (i.e. the code in this repo)

Open an [Anaconda-enabled command prompt](https://www.anaconda.org) (or your preferred method of environment management) and enter the following commands:

1) Create a `Python` environment (Recommended version  is `python3.11`)

```bash
conda create -n freemocap-env python=3.11
```

2) Activate that newly created environment

```bash
conda activate freemocap-env
```

3) Clone the repository

```bash
git clone https://github.com/freemocap/freemocap
```

4) Navigate into the newly cloned/downloaded `freemocap` folder

```bash
cd freemocap
```

5) Install the package via the `pyproject.toml` file

```bash
pip install -e .
```

6) Launch the GUI (via the `freemocap.__main__.py` entry point)

```bash
python -m freemocap
```

A GUI should pop up!

___

## Documentation 

Our documentation is hosted at: https://freemocap.github.io/documentation

That site is built using `writerside` from this repository: https://github.com/freemocap/documentation

___



### Contribution Guidelines

Please read our contribution doc: [CONTRIBUTING.md](CONTRIBUTING.md)


## Related

[//]: # (* [project-name]&amp;#40;#&amp;#41; - Project description)

## Maintainers

* [Jon Matthis](https://github.com/jonmatthis)
* [Endurance Idehen](https://github.com/endurance)

## License

This project is licensed under the APGL License - see the [LICENSE](LICENSE) file for details.

If the AGPL does not work for your needs, we are happy to discuss terms to license this software to you with a different
agreement at a price point that increases exponentially as you
move [spiritually](https://www.gnu.org/philosophy/open-source-misses-the-point.en.html) away from the `AGPL`

</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Crosstalk-Solutions/unifi-toolkit]]></title>
            <link>https://github.com/Crosstalk-Solutions/unifi-toolkit</link>
            <guid>https://github.com/Crosstalk-Solutions/unifi-toolkit</guid>
            <pubDate>Sat, 21 Feb 2026 00:07:10 GMT</pubDate>
            <description><![CDATA[A suite of tools for UniFi network management]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Crosstalk-Solutions/unifi-toolkit">Crosstalk-Solutions/unifi-toolkit</a></h1>
            <p>A suite of tools for UniFi network management</p>
            <p>Language: Python</p>
            <p>Stars: 246</p>
            <p>Forks: 22</p>
            <p>Stars today: 59 stars today</p>
            <h2>README</h2><pre># UI Toolkit

A comprehensive suite of tools for UniFi network management and monitoring.

&gt; **Note:** This project is not affiliated with, endorsed by, or sponsored by Ubiquiti Inc. UniFi is a trademark of Ubiquiti Inc.

&lt;img width=&quot;1094&quot; height=&quot;748&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/a167fc5c-9db5-48f2-8b43-0dfdab3b08a8&quot; /&gt;

## Features

### Dashboard
Real-time system status including:
- **Gateway Info** - Model, firmware, uptime
- **Resource Usage** - CPU and RAM utilization
- **Network Health** - WAN, LAN, WLAN, VPN status with diagnostic reasons
- **Connected Clients** - Wired and wireless counts
- **WAN Status** - IP, ISP, latency, uptime (supports multi-WAN)

### Wi-Fi Stalker
Track specific Wi-Fi client devices through your UniFi infrastructure.
- Device tracking by MAC address
- Roaming detection between access points
- Connection history with timestamps
- Block/unblock devices directly from the UI
- Blocked device indicator in device list
- Webhook alerts (Slack, Discord, n8n) for connect, disconnect, roam, block, and unblock events

&lt;img width=&quot;1355&quot; height=&quot;702&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/383d3c84-1b24-480a-bbaf-e72c47953b85&quot; /&gt;

### Threat Watch
Monitor IDS/IPS security events from your UniFi gateway.
- Real-time event monitoring
- Threat categorization and analysis
- Top attackers and targets
- Webhook alerts (Slack, Discord, n8n)

&lt;img width=&quot;1359&quot; height=&quot;468&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/7bfec7f7-bdf6-4ae2-af0e-143dcd982d4a&quot; /&gt;

### Network Pulse
Real-time network monitoring dashboard.
- Gateway status (model, firmware, uptime, WAN)
- Device counts (total clients, wired, wireless, APs, switches)
- Chart.js visualizations (clients by band, clients by SSID, top bandwidth)
- Clickable AP cards with detailed client views
- WebSocket-powered live updates

&lt;img width=&quot;1895&quot; height=&quot;957&quot; alt=&quot;image&quot; src=&quot;https://github.com/user-attachments/assets/ca6f0df5-8657-4c2a-ad16-8807aa21bcac&quot; /&gt;

### UI Product Selector *(External)*
Build the perfect UniFi network at [uiproductselector.com](https://uiproductselector.com)

---

## Quick Start

### Requirements
- **Docker** (recommended) or Python 3.9-3.12
- **Ubuntu 22.04/24.04** (or other Linux)
- Access to UniFi Controller

### Local Deployment (LAN Only)

No authentication, access via `http://localhost:8000`

**Prerequisites:** Install Docker first - see [docs/INSTALLATION.md](docs/INSTALLATION.md#option-a-docker-installation-recommended)

```bash
# Clone and setup
git clone https://github.com/Crosstalk-Solutions/unifi-toolkit.git
cd unifi-toolkit
./setup.sh  # Select 1 for Local

# Start
docker compose up -d
```

Access at **http://localhost:8000**

### Production Deployment (Internet-Facing)

Authentication enabled, HTTPS with Let&#039;s Encrypt via Caddy

**Prerequisites:** Install Docker first - see [docs/INSTALLATION.md](docs/INSTALLATION.md#option-a-docker-installation-recommended)

```bash
# Clone and setup
git clone https://github.com/Crosstalk-Solutions/unifi-toolkit.git
cd unifi-toolkit
./setup.sh  # Select 2 for Production
# Enter: domain name, admin username, password

# Open firewall ports
sudo ufw allow 80/tcp &amp;&amp; sudo ufw allow 443/tcp

# Start with HTTPS
docker compose --profile production up -d
```

Access at **https://your-domain.com**

---

## Documentation

| Guide | Description |
|-------|-------------|
| [INSTALLATION.md](docs/INSTALLATION.md) | Complete installation guide with troubleshooting |
| [SYNOLOGY.md](docs/SYNOLOGY.md) | Synology NAS Container Manager setup |
| [QNAP Guide](https://github.com/Crosstalk-Solutions/unifi-toolkit/issues/29) | QNAP Container Station setup (community) |
| [Unraid Guide](docs/UNRAID.md) | Unraid Community apps Setup |
| [QUICKSTART.md](docs/QUICKSTART.md) | 5-minute quick start reference |

---

## Common Commands

| Action | Command |
|--------|---------|
| Start (local) | `docker compose up -d` |
| Start (production) | `docker compose --profile production up -d` |
| Stop | `docker compose down` |
| View logs | `docker compose logs -f` |
| Restart | `docker compose restart` |
| Reset password | `./reset_password.sh` |
| Update | `./upgrade.sh` |

---

## Configuration

### Setup Wizard (Recommended)

Run the interactive setup wizard:

```bash
./setup.sh
```

The wizard will:
- Generate encryption key
- Configure deployment mode (local/production)
- Set up authentication (production only)
- Create your `.env` file

### Manual Configuration

Copy and edit the example configuration:

```bash
cp .env.example .env
```

#### Required Settings

| Variable | Description |
|----------|-------------|
| `ENCRYPTION_KEY` | Encrypts stored credentials (auto-generated by setup wizard) |

#### Deployment Settings (Production Only)

| Variable | Description |
|----------|-------------|
| `DEPLOYMENT_TYPE` | `local` or `production` |
| `DOMAIN` | Your domain name (e.g., `toolkit.example.com`) |
| `AUTH_USERNAME` | Admin username |
| `AUTH_PASSWORD_HASH` | Bcrypt password hash (generated by setup wizard) |

#### UniFi Controller Settings

Configure via `.env` or the web UI (web UI takes precedence):

| Variable | Description |
|----------|-------------|
| `UNIFI_CONTROLLER_URL` | Controller URL (e.g., `https://192.168.1.1`) |
| `UNIFI_USERNAME` | Username (legacy controllers) |
| `UNIFI_PASSWORD` | Password (legacy controllers) |
| `UNIFI_API_KEY` | API key (UniFi OS: UDM, UCG, Cloud Key) |
| `UNIFI_SITE_ID` | Site ID from URL, not friendly name (default: `default`). For multi-site, use ID from `/manage/site/{id}/...` |
| `UNIFI_VERIFY_SSL` | SSL verification (default: `false`) |

#### Tool Settings

| Variable | Description |
|----------|-------------|
| `STALKER_REFRESH_INTERVAL` | Device refresh interval in seconds (default: `60`) |

---

## Security

### Authentication

- **Local mode**: No authentication (trusted LAN only)
- **Production mode**: Session-based authentication with bcrypt password hashing
- **Rate limiting**: 5 failed login attempts = 5 minute lockout

### HTTPS

Production deployments use Caddy for automatic HTTPS:
- Let&#039;s Encrypt certificates (auto-renewed)
- HTTP to HTTPS redirect
- Security headers (HSTS, X-Frame-Options, etc.)

### Multi-Site Networking

When managing multiple UniFi sites, always use site-to-site VPN:

```
‚úÖ RECOMMENDED: VPN Connection
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  UI Toolkit      ‚îÇ‚óÑ‚îÄ‚îÄVPN‚îÄ‚îÄ‚ñ∫‚îÇ  Remote UniFi    ‚îÇ
‚îÇ  Server          ‚îÇ         ‚îÇ  Controller      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚ùå AVOID: Direct Internet Exposure
Never expose UniFi controllers via port forwarding
```

**VPN Options:** UniFi Site-to-Site, WireGuard, Tailscale, IPSec

---

## Troubleshooting

### Can&#039;t connect to UniFi controller
- Set `UNIFI_VERIFY_SSL=false` for self-signed certificates
- UniFi OS devices (UDM, UCG) require an API key, not username/password
- Verify network connectivity to controller

### Device not showing as online
- Wait 60 seconds for the next refresh cycle
- Verify MAC address format is correct
- Confirm device is connected in UniFi dashboard

### Let&#039;s Encrypt certificate fails
- Verify DNS A record points to your server
- Ensure ports 80 and 443 are open
- Check Caddy logs: `docker compose logs caddy`

### Rate limited on login
- Wait 5 minutes for lockout to expire
- Use `./reset_password.sh` if you forgot your password

### Docker issues
- Verify `.env` exists and contains `ENCRYPTION_KEY`
- Check logs: `docker compose logs -f`
- Pull latest image: `docker compose pull &amp;&amp; docker compose up -d`

---

## Running with Python (Alternative to Docker)

```bash
# Clone repository
git clone https://github.com/Crosstalk-Solutions/unifi-toolkit.git
cd unifi-toolkit

# Create virtual environment (Python 3.9-3.12 only, NOT 3.13+)
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Run setup wizard
./setup.sh

# Start application
python run.py
```

---

## Project Structure

```
unifi-toolkit/
‚îú‚îÄ‚îÄ app/                    # Main application
‚îÇ   ‚îú‚îÄ‚îÄ main.py            # FastAPI entry point
‚îÇ   ‚îú‚îÄ‚îÄ routers/           # API routes (auth, config)
‚îÇ   ‚îú‚îÄ‚îÄ static/            # CSS, images
‚îÇ   ‚îî‚îÄ‚îÄ templates/         # HTML templates
‚îú‚îÄ‚îÄ tools/                 # Individual tools
‚îÇ   ‚îú‚îÄ‚îÄ wifi_stalker/      # Wi-Fi Stalker tool
‚îÇ   ‚îú‚îÄ‚îÄ threat_watch/      # Threat Watch tool
‚îÇ   ‚îî‚îÄ‚îÄ network_pulse/     # Network Pulse tool
‚îú‚îÄ‚îÄ shared/                # Shared infrastructure
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Settings management
‚îÇ   ‚îú‚îÄ‚îÄ database.py        # SQLAlchemy setup
‚îÇ   ‚îú‚îÄ‚îÄ unifi_client.py    # UniFi API wrapper
‚îÇ   ‚îî‚îÄ‚îÄ crypto.py          # Credential encryption
‚îú‚îÄ‚îÄ docs/                  # Documentation
‚îú‚îÄ‚îÄ data/                  # Database (created at runtime)
‚îú‚îÄ‚îÄ setup.sh               # Setup wizard
‚îú‚îÄ‚îÄ upgrade.sh             # Upgrade script
‚îú‚îÄ‚îÄ reset_password.sh      # Password reset utility
‚îú‚îÄ‚îÄ Caddyfile              # Reverse proxy config
‚îú‚îÄ‚îÄ docker-compose.yml     # Docker configuration
‚îî‚îÄ‚îÄ requirements.txt       # Python dependencies
```

---

## Development

### Running Tests

The project includes a comprehensive test suite covering authentication, caching, configuration, and encryption.

```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Run all tests
pytest tests/ -v

# Run specific test file
pytest tests/test_auth.py -v

# Run with coverage
pytest tests/ --cov=shared --cov=app -v
```

**Test modules:**
- `tests/test_auth.py` - Authentication, session management, rate limiting (22 tests)
- `tests/test_cache.py` - In-memory caching with TTL expiration (18 tests)
- `tests/test_config.py` - Pydantic settings and environment variables (13 tests)
- `tests/test_crypto.py` - Fernet encryption for credentials (15 tests)

---

## Support

- **Community**: [#unifi-toolkit on Discord](https://discord.com/invite/crosstalksolutions)
- **Issues**: [GitHub Issues](https://github.com/Crosstalk-Solutions/unifi-toolkit/issues)
- **Documentation**: [docs/](docs/)

### Buy Me a Coffee

If you find UI Toolkit useful, consider supporting development:

[![ko-fi](https://ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/crosstalk)

---

## Credits

Developed by [Crosstalk Solutions](https://www.crosstalksolutions.com/)

- YouTube: [@CrosstalkSolutions](https://www.youtube.com/@CrosstalkSolutions)

---

## License

MIT License
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[fastapi/fastapi]]></title>
            <link>https://github.com/fastapi/fastapi</link>
            <guid>https://github.com/fastapi/fastapi</guid>
            <pubDate>Sat, 21 Feb 2026 00:07:09 GMT</pubDate>
            <description><![CDATA[FastAPI framework, high performance, easy to learn, fast to code, ready for production]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/fastapi/fastapi">fastapi/fastapi</a></h1>
            <p>FastAPI framework, high performance, easy to learn, fast to code, ready for production</p>
            <p>Language: Python</p>
            <p>Stars: 95,391</p>
            <p>Forks: 8,715</p>
            <p>Stars today: 47 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://fastapi.tiangolo.com&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/logo-margin/logo-teal.png&quot; alt=&quot;FastAPI&quot;&gt;&lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
    &lt;em&gt;FastAPI framework, high performance, easy to learn, fast to code, ready for production&lt;/em&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://github.com/fastapi/fastapi/actions?query=workflow%3ATest+event%3Apush+branch%3Amaster&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://github.com/fastapi/fastapi/actions/workflows/test.yml/badge.svg?event=push&amp;branch=master&quot; alt=&quot;Test&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://coverage-badge.samuelcolvin.workers.dev/redirect/fastapi/fastapi&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://coverage-badge.samuelcolvin.workers.dev/fastapi/fastapi.svg&quot; alt=&quot;Coverage&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://pypi.org/project/fastapi&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/v/fastapi?color=%2334D058&amp;label=pypi%20package&quot; alt=&quot;Package version&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://pypi.org/project/fastapi&quot; target=&quot;_blank&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/pypi/pyversions/fastapi.svg?color=%2334D058&quot; alt=&quot;Supported Python versions&quot;&gt;
&lt;/a&gt;
&lt;/p&gt;

---

**Documentation**: &lt;a href=&quot;https://fastapi.tiangolo.com&quot; target=&quot;_blank&quot;&gt;https://fastapi.tiangolo.com&lt;/a&gt;

**Source Code**: &lt;a href=&quot;https://github.com/fastapi/fastapi&quot; target=&quot;_blank&quot;&gt;https://github.com/fastapi/fastapi&lt;/a&gt;

---

FastAPI is a modern, fast (high-performance), web framework for building APIs with Python based on standard Python type hints.

The key features are:

* **Fast**: Very high performance, on par with **NodeJS** and **Go** (thanks to Starlette and Pydantic). [One of the fastest Python frameworks available](#performance).
* **Fast to code**: Increase the speed to develop features by about 200% to 300%. *
* **Fewer bugs**: Reduce about 40% of human (developer) induced errors. *
* **Intuitive**: Great editor support. &lt;dfn title=&quot;also known as auto-complete, autocompletion, IntelliSense&quot;&gt;Completion&lt;/dfn&gt; everywhere. Less time debugging.
* **Easy**: Designed to be easy to use and learn. Less time reading docs.
* **Short**: Minimize code duplication. Multiple features from each parameter declaration. Fewer bugs.
* **Robust**: Get production-ready code. With automatic interactive documentation.
* **Standards-based**: Based on (and fully compatible with) the open standards for APIs: &lt;a href=&quot;https://github.com/OAI/OpenAPI-Specification&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;OpenAPI&lt;/a&gt; (previously known as Swagger) and &lt;a href=&quot;https://json-schema.org/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;JSON Schema&lt;/a&gt;.

&lt;small&gt;* estimation based on tests conducted by an internal development team, building production applications.&lt;/small&gt;

## Sponsors

&lt;!-- sponsors --&gt;
### Keystone Sponsor

&lt;a href=&quot;https://fastapicloud.com&quot; target=&quot;_blank&quot; title=&quot;FastAPI Cloud. By the same team behind FastAPI. You code. We Cloud.&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/fastapicloud.png&quot;&gt;&lt;/a&gt;

### Gold and Silver Sponsors

&lt;a href=&quot;https://blockbee.io?ref=fastapi&quot; target=&quot;_blank&quot; title=&quot;BlockBee Cryptocurrency Payment Gateway&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/blockbee.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://github.com/scalar/scalar/?utm_source=fastapi&amp;utm_medium=website&amp;utm_campaign=main-badge&quot; target=&quot;_blank&quot; title=&quot;Scalar: Beautiful Open-Source API References from Swagger/OpenAPI files&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/scalar.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.propelauth.com/?utm_source=fastapi&amp;utm_campaign=1223&amp;utm_medium=mainbadge&quot; target=&quot;_blank&quot; title=&quot;Auth, user management and more for your B2B product&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/propelauth.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://zuplo.link/fastapi-gh&quot; target=&quot;_blank&quot; title=&quot;Zuplo: Deploy, Secure, Document, and Monetize your FastAPI&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/zuplo.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://liblab.com?utm_source=fastapi&quot; target=&quot;_blank&quot; title=&quot;liblab - Generate SDKs from FastAPI&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/liblab.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://docs.render.com/deploy-fastapi?utm_source=deploydoc&amp;utm_medium=referral&amp;utm_campaign=fastapi&quot; target=&quot;_blank&quot; title=&quot;Deploy &amp; scale any full-stack web app on Render. Focus on building apps, not infra.&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/render.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.coderabbit.ai/?utm_source=fastapi&amp;utm_medium=badge&amp;utm_campaign=fastapi&quot; target=&quot;_blank&quot; title=&quot;Cut Code Review Time &amp; Bugs in Half with CodeRabbit&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/coderabbit.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://subtotal.com/?utm_source=fastapi&amp;utm_medium=sponsorship&amp;utm_campaign=open-source&quot; target=&quot;_blank&quot; title=&quot;The Gold Standard in Retail Account Linking&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/subtotal.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://docs.railway.com/guides/fastapi?utm_medium=integration&amp;utm_source=docs&amp;utm_campaign=fastapi&quot; target=&quot;_blank&quot; title=&quot;Deploy enterprise applications at startup speed&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/railway.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://serpapi.com/?utm_source=fastapi_website&quot; target=&quot;_blank&quot; title=&quot;SerpApi: Web Search API&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/serpapi.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.greptile.com/?utm_source=fastapi&amp;utm_medium=sponsorship&amp;utm_campaign=fastapi_sponsor_page&quot; target=&quot;_blank&quot; title=&quot;Greptile: The AI Code Reviewer&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/greptile.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://databento.com/?utm_source=fastapi&amp;utm_medium=sponsor&amp;utm_content=display&quot; target=&quot;_blank&quot; title=&quot;Pay as you go for market data&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/databento.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://speakeasy.com/editor?utm_source=fastapi+repo&amp;utm_medium=github+sponsorship&quot; target=&quot;_blank&quot; title=&quot;SDKs for your API | Speakeasy&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/speakeasy.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.svix.com/&quot; target=&quot;_blank&quot; title=&quot;Svix - Webhooks as a service&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/svix.svg&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.stainlessapi.com/?utm_source=fastapi&amp;utm_medium=referral&quot; target=&quot;_blank&quot; title=&quot;Stainless | Generate best-in-class SDKs&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/stainless.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.permit.io/blog/implement-authorization-in-fastapi?utm_source=github&amp;utm_medium=referral&amp;utm_campaign=fastapi&quot; target=&quot;_blank&quot; title=&quot;Fine-Grained Authorization for FastAPI&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/permit.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.interviewpal.com/?utm_source=fastapi&amp;utm_medium=open-source&amp;utm_campaign=dev-hiring&quot; target=&quot;_blank&quot; title=&quot;InterviewPal - AI Interview Coach for Engineers and Devs&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/interviewpal.png&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://dribia.com/en/&quot; target=&quot;_blank&quot; title=&quot;Dribia - Data Science within your reach&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/sponsors/dribia.png&quot;&gt;&lt;/a&gt;

&lt;!-- /sponsors --&gt;

&lt;a href=&quot;https://fastapi.tiangolo.com/fastapi-people/#sponsors&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Other sponsors&lt;/a&gt;

## Opinions

&quot;_[...] I&#039;m using **FastAPI** a ton these days. [...] I&#039;m actually planning to use it for all of my team&#039;s **ML services at Microsoft**. Some of them are getting integrated into the core **Windows** product and some **Office** products._&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Kabir Khan - &lt;strong&gt;Microsoft&lt;/strong&gt; &lt;a href=&quot;https://github.com/fastapi/fastapi/pull/26&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_We adopted the **FastAPI** library to spawn a **REST** server that can be queried to obtain **predictions**. [for Ludwig]_&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Piero Molino, Yaroslav Dudin, and Sai Sumanth Miryala - &lt;strong&gt;Uber&lt;/strong&gt; &lt;a href=&quot;https://eng.uber.com/ludwig-v0-2/&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_**Netflix** is pleased to announce the open-source release of our **crisis management** orchestration framework: **Dispatch**! [built with **FastAPI**]_&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Kevin Glisson, Marc Vilanova, Forest Monsen - &lt;strong&gt;Netflix&lt;/strong&gt; &lt;a href=&quot;https://netflixtechblog.com/introducing-dispatch-da4b8a2a8072&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_I‚Äôm over the moon excited about **FastAPI**. It‚Äôs so fun!_&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Brian Okken - &lt;strong&gt;&lt;a href=&quot;https://pythonbytes.fm/episodes/show/123/time-to-right-the-py-wrongs?time_in_sec=855&quot; target=&quot;_blank&quot;&gt;Python Bytes&lt;/a&gt; podcast host&lt;/strong&gt; &lt;a href=&quot;https://x.com/brianokken/status/1112220079972728832&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_Honestly, what you&#039;ve built looks super solid and polished. In many ways, it&#039;s what I wanted **Hug** to be - it&#039;s really inspiring to see someone build that._&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Timothy Crosley - &lt;strong&gt;&lt;a href=&quot;https://github.com/hugapi/hug&quot; target=&quot;_blank&quot;&gt;Hug&lt;/a&gt; creator&lt;/strong&gt; &lt;a href=&quot;https://news.ycombinator.com/item?id=19455465&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_If you&#039;re looking to learn one **modern framework** for building REST APIs, check out **FastAPI** [...] It&#039;s fast, easy to use and easy to learn [...]_&quot;

&quot;_We&#039;ve switched over to **FastAPI** for our **APIs** [...] I think you&#039;ll like it [...]_&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Ines Montani - Matthew Honnibal - &lt;strong&gt;&lt;a href=&quot;https://explosion.ai&quot; target=&quot;_blank&quot;&gt;Explosion AI&lt;/a&gt; founders - &lt;a href=&quot;https://spacy.io&quot; target=&quot;_blank&quot;&gt;spaCy&lt;/a&gt; creators&lt;/strong&gt; &lt;a href=&quot;https://x.com/_inesmontani/status/1144173225322143744&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt; - &lt;a href=&quot;https://x.com/honnibal/status/1144031421859655680&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

&quot;_If anyone is looking to build a production Python API, I would highly recommend **FastAPI**. It is **beautifully designed**, **simple to use** and **highly scalable**, it has become a **key component** in our API first development strategy and is driving many automations and services such as our Virtual TAC Engineer._&quot;

&lt;div style=&quot;text-align: right; margin-right: 10%;&quot;&gt;Deon Pillsbury - &lt;strong&gt;Cisco&lt;/strong&gt; &lt;a href=&quot;https://www.linkedin.com/posts/deonpillsbury_cisco-cx-python-activity-6963242628536487936-trAp/&quot; target=&quot;_blank&quot;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&lt;/div&gt;

---

## FastAPI mini documentary

There&#039;s a &lt;a href=&quot;https://www.youtube.com/watch?v=mpR8ngthqiE&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;FastAPI mini documentary&lt;/a&gt; released at the end of 2025, you can watch it online:

&lt;a href=&quot;https://www.youtube.com/watch?v=mpR8ngthqiE&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://fastapi.tiangolo.com/img/fastapi-documentary.jpg&quot; alt=&quot;FastAPI Mini Documentary&quot;&gt;&lt;/a&gt;

## **Typer**, the FastAPI of CLIs

&lt;a href=&quot;https://typer.tiangolo.com&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://typer.tiangolo.com/img/logo-margin/logo-margin-vector.svg&quot; style=&quot;width: 20%;&quot;&gt;&lt;/a&gt;

If you are building a &lt;abbr title=&quot;Command Line Interface&quot;&gt;CLI&lt;/abbr&gt; app to be used in the terminal instead of a web API, check out &lt;a href=&quot;https://typer.tiangolo.com/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;**Typer**&lt;/a&gt;.

**Typer** is FastAPI&#039;s little sibling. And it&#039;s intended to be the **FastAPI of CLIs**. ‚å®Ô∏è üöÄ

## Requirements

FastAPI stands on the shoulders of giants:

* &lt;a href=&quot;https://www.starlette.dev/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Starlette&lt;/a&gt; for the web parts.
* &lt;a href=&quot;https://docs.pydantic.dev/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Pydantic&lt;/a&gt; for the data parts.

## Installation

Create and activate a &lt;a href=&quot;https://fastapi.tiangolo.com/virtual-environments/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;virtual environment&lt;/a&gt; and then install FastAPI:

&lt;div class=&quot;termy&quot;&gt;

```console
$ pip install &quot;fastapi[standard]&quot;

---&gt; 100%
```

&lt;/div&gt;

**Note**: Make sure you put `&quot;fastapi[standard]&quot;` in quotes to ensure it works in all terminals.

## Example

### Create it

Create a file `main.py` with:

```Python
from fastapi import FastAPI

app = FastAPI()


@app.get(&quot;/&quot;)
def read_root():
    return {&quot;Hello&quot;: &quot;World&quot;}


@app.get(&quot;/items/{item_id}&quot;)
def read_item(item_id: int, q: str | None = None):
    return {&quot;item_id&quot;: item_id, &quot;q&quot;: q}
```

&lt;details markdown=&quot;1&quot;&gt;
&lt;summary&gt;Or use &lt;code&gt;async def&lt;/code&gt;...&lt;/summary&gt;

If your code uses `async` / `await`, use `async def`:

```Python hl_lines=&quot;7  12&quot;
from fastapi import FastAPI

app = FastAPI()


@app.get(&quot;/&quot;)
async def read_root():
    return {&quot;Hello&quot;: &quot;World&quot;}


@app.get(&quot;/items/{item_id}&quot;)
async def read_item(item_id: int, q: str | None = None):
    return {&quot;item_id&quot;: item_id, &quot;q&quot;: q}
```

**Note**:

If you don&#039;t know, check the _&quot;In a hurry?&quot;_ section about &lt;a href=&quot;https://fastapi.tiangolo.com/async/#in-a-hurry&quot; target=&quot;_blank&quot;&gt;`async` and `await` in the docs&lt;/a&gt;.

&lt;/details&gt;

### Run it

Run the server with:

&lt;div class=&quot;termy&quot;&gt;

```console
$ fastapi dev main.py

 ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FastAPI CLI - Development mode ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
 ‚îÇ                                                     ‚îÇ
 ‚îÇ  Serving at: http://127.0.0.1:8000                  ‚îÇ
 ‚îÇ                                                     ‚îÇ
 ‚îÇ  API docs: http://127.0.0.1:8000/docs               ‚îÇ
 ‚îÇ                                                     ‚îÇ
 ‚îÇ  Running in development mode, for production use:   ‚îÇ
 ‚îÇ                                                     ‚îÇ
 ‚îÇ  fastapi run                                        ‚îÇ
 ‚îÇ                                                     ‚îÇ
 ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ

INFO:     Will watch for changes in these directories: [&#039;/home/user/code/awesomeapp&#039;]
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [2248755] using WatchFiles
INFO:     Started server process [2248757]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
```

&lt;/div&gt;

&lt;details markdown=&quot;1&quot;&gt;
&lt;summary&gt;About the command &lt;code&gt;fastapi dev main.py&lt;/code&gt;...&lt;/summary&gt;

The command `fastapi dev` reads your `main.py` file, detects the **FastAPI** app in it, and starts a server using &lt;a href=&quot;https://www.uvicorn.dev&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Uvicorn&lt;/a&gt;.

By default, `fastapi dev` will start with auto-reload enabled for local development.

You can read more about it in the &lt;a href=&quot;https://fastapi.tiangolo.com/fastapi-cli/&quot; target=&quot;_blank&quot;&gt;FastAPI CLI docs&lt;/a&gt;.

&lt;/details&gt;

### Check it

Open your browser at &lt;a href=&quot;http://127.0.0.1:8000/items/5?q=somequery&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/items/5?q=somequery&lt;/a&gt;.

You will see the JSON response as:

```JSON
{&quot;item_id&quot;: 5, &quot;q&quot;: &quot;somequery&quot;}
```

You already created an API that:

* Receives HTTP requests in the _paths_ `/` and `/items/{item_id}`.
* Both _paths_ take `GET` &lt;em&gt;operations&lt;/em&gt; (also known as HTTP _methods_).
* The _path_ `/items/{item_id}` has a _path parameter_ `item_id` that should be an `int`.
* The _path_ `/items/{item_id}` has an optional `str` _query parameter_ `q`.

### Interactive API docs

Now go to &lt;a href=&quot;http://127.0.0.1:8000/docs&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/docs&lt;/a&gt;.

You will see the automatic interactive API documentation (provided by &lt;a href=&quot;https://github.com/swagger-api/swagger-ui&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;Swagger UI&lt;/a&gt;):

![Swagger UI](https://fastapi.tiangolo.com/img/index/index-01-swagger-ui-simple.png)

### Alternative API docs

And now, go to &lt;a href=&quot;http://127.0.0.1:8000/redoc&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/redoc&lt;/a&gt;.

You will see the alternative automatic documentation (provided by &lt;a href=&quot;https://github.com/Rebilly/ReDoc&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;ReDoc&lt;/a&gt;):

![ReDoc](https://fastapi.tiangolo.com/img/index/index-02-redoc-simple.png)

## Example upgrade

Now modify the file `main.py` to receive a body from a `PUT` request.

Declare the body using standard Python types, thanks to Pydantic.

```Python hl_lines=&quot;2  7-10 23-25&quot;
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()


class Item(BaseModel):
    name: str
    price: float
    is_offer: bool | None = None


@app.get(&quot;/&quot;)
def read_root():
    return {&quot;Hello&quot;: &quot;World&quot;}


@app.get(&quot;/items/{item_id}&quot;)
def read_item(item_id: int, q: str | None = None):
    return {&quot;item_id&quot;: item_id, &quot;q&quot;: q}


@app.put(&quot;/items/{item_id}&quot;)
def update_item(item_id: int, item: Item):
    return {&quot;item_name&quot;: item.name, &quot;item_id&quot;: item_id}
```

The `fastapi dev` server should reload automatically.

### Interactive API docs upgrade

Now go to &lt;a href=&quot;http://127.0.0.1:8000/docs&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/docs&lt;/a&gt;.

* The interactive API documentation will be automatically updated, including the new body:

![Swagger UI](https://fastapi.tiangolo.com/img/index/index-03-swagger-02.png)

* Click on the button &quot;Try it out&quot;, it allows you to fill the parameters and directly interact with the API:

![Swagger UI interaction](https://fastapi.tiangolo.com/img/index/index-04-swagger-03.png)

* Then click on the &quot;Execute&quot; button, the user interface will communicate with your API, send the parameters, get the results and show them on the screen:

![Swagger UI interaction](https://fastapi.tiangolo.com/img/index/index-05-swagger-04.png)

### Alternative API docs upgrade

And now, go to &lt;a href=&quot;http://127.0.0.1:8000/redoc&quot; class=&quot;external-link&quot; target=&quot;_blank&quot;&gt;http://127.0.0.1:8000/redoc&lt;/a&gt;.

* The alternative documentation will also reflect the new query parameter and body:

![ReDoc](https://fastapi.tiangolo.com/img/index/index-06-redoc-02.png)

### Recap

In summary, you declare **once** the types of parameters, body, etc. as function parameters.

You do that with standard modern Python types.

You don&#039;t have to learn a new syntax, the methods or classes of a specific library, etc.

Just standard **Python**.

For example, for an `int`:

```Python
item_id: int
```

or for a more complex `Item` model:

```Python
item: Item
```

...and with that single declaration you get:

* Editor support, including:
    * Completion.
    * Type checks.
* Validation of data:
    * Automatic and clear errors when the data is invalid.
    * Validation even for deeply nested JSON objects.
* &lt;dfn title=&quot;also known as: serialization, parsing, marshalling&quot;&gt;Conversion&lt;/dfn&gt; of input data: coming from the network to Python data and types. Reading from:
    * JSON.
    * Path parameters.
    * Query parameters.
    * Cookies.
    * Headers.
    * Forms.
    * Files.
* &lt;dfn title=&quot;also known as: serialization, parsing, marshalling&quot;&gt;Conversion&lt;/dfn&gt; of output data: converting from Python data and types to network data (as JSON):
    * Convert Python types (`str`, `int`, `float`, `bool`, `list`, etc).
    * `datetime` objects.
    * `UUID` objects.
    * Database models.
    * ...and many more.
* Automatic interactive API documentation, including 2 alternative user interfaces:
    * Swagger UI.
    * ReDoc.

---

Coming back to the previous code example, **FastAPI** will:

* Validate that there is an `item_id` in the path for `GET` and `PUT` requests.
* Validate that the `item_id` is of type `int` for `GET` and `PUT` requests.
    * If it is not, the client will see a useful, clear error.
* Check if there is an optional query parameter named `q` (as in `http://127.0.0.1:8000/items/foo?q=somequery`) for `GET` requests.
    * As the `q` parameter is declared with `= None`, it is optional.
    * Without the `None` it would be required (as is the body in the case with `PUT`).
* For `PUT` requests to `/items/{item_id}`, read the body as JSON:
    * Check that it has a required attribute `name` that should be a `str`.
    * Check that it has a required attribute `price` that has to be a `float`.
    * Check that it has an optional attribute `is_offer`, that should be a `bool`, if present.
    * All this would also work for deeply n

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[RichardAtCT/claude-code-telegram]]></title>
            <link>https://github.com/RichardAtCT/claude-code-telegram</link>
            <guid>https://github.com/RichardAtCT/claude-code-telegram</guid>
            <pubDate>Sat, 21 Feb 2026 00:07:08 GMT</pubDate>
            <description><![CDATA[A powerful Telegram bot that provides remote access to Claude Code, enabling developers to interact with their projects from anywhere with full AI assistance and session persistence.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/RichardAtCT/claude-code-telegram">RichardAtCT/claude-code-telegram</a></h1>
            <p>A powerful Telegram bot that provides remote access to Claude Code, enabling developers to interact with their projects from anywhere with full AI assistance and session persistence.</p>
            <p>Language: Python</p>
            <p>Stars: 1,216</p>
            <p>Forks: 155</p>
            <p>Stars today: 445 stars today</p>
            <h2>README</h2><pre># Claude Code Telegram Bot

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)

A Telegram bot that gives you remote access to [Claude Code](https://claude.ai/code). Chat naturally with Claude about your projects from anywhere -- no terminal commands needed.

## What is this?

This bot connects Telegram to Claude Code, providing a conversational AI interface for your codebase:

- **Chat naturally** -- ask Claude to analyze, edit, or explain your code in plain language
- **Maintain context** across conversations with automatic session persistence per project
- **Code on the go** from any device with Telegram
- **Receive proactive notifications** from webhooks, scheduled jobs, and CI/CD events
- **Stay secure** with built-in authentication, directory sandboxing, and audit logging

## Quick Start

### Demo

```
You: Can you help me add error handling to src/api.py?

Bot: I&#039;ll analyze src/api.py and add error handling...
     [Claude reads your code, suggests improvements, and can apply changes directly]

You: Looks good. Now run the tests to make sure nothing broke.

Bot: Running pytest...
     All 47 tests passed. The error handling changes are working correctly.
```

### 1. Prerequisites

- **Python 3.10+** -- [Download here](https://www.python.org/downloads/)
- **Poetry** -- Modern Python dependency management
- **Claude Code CLI** -- [Install from here](https://claude.ai/code)
- **Telegram Bot Token** -- Get one from [@BotFather](https://t.me/botfather)

### 2. Install

```bash
git clone https://github.com/RichardAtCT/claude-code-telegram.git
cd claude-code-telegram
make dev
```

### 3. Configure

```bash
cp .env.example .env
# Edit .env with your settings:
```

**Minimum required:**
```bash
TELEGRAM_BOT_TOKEN=1234567890:ABC-DEF1234ghIkl-zyx57W2v1u123ew11
TELEGRAM_BOT_USERNAME=my_claude_bot
APPROVED_DIRECTORY=/Users/yourname/projects
ALLOWED_USERS=123456789  # Your Telegram user ID
```

### 4. Run

```bash
make run          # Production
make run-debug    # With debug logging
```

Message your bot on Telegram to get started.

&gt; **Detailed setup:** See [docs/setup.md](docs/setup.md) for Claude authentication options and troubleshooting.

## Modes

The bot supports two interaction modes:

### Agentic Mode (Default)

The default conversational mode. Just talk to Claude naturally -- no special commands required.

**Commands:** `/start`, `/new`, `/status`, `/verbose`, `/repo`
If `ENABLE_PROJECT_THREADS=true`: `/sync_threads`

```
You: What files are in this project?
Bot: Working... (3s)
     üìñ Read
     üìÇ LS
     üí¨ Let me describe the project structure
Bot: [Claude describes the project structure]

You: Add a retry decorator to the HTTP client
Bot: Working... (8s)
     üìñ Read: http_client.py
     üí¨ I&#039;ll add a retry decorator with exponential backoff
     ‚úèÔ∏è Edit: http_client.py
     üíª Bash: poetry run pytest tests/ -v
Bot: [Claude shows the changes and test results]

You: /verbose 0
Bot: Verbosity set to 0 (quiet)
```

Use `/verbose 0|1|2` to control how much background activity is shown:

| Level | Shows |
|-------|-------|
| **0** (quiet) | Final response only (typing indicator stays active) |
| **1** (normal, default) | Tool names + reasoning snippets in real-time |
| **2** (detailed) | Tool names with inputs + longer reasoning text |

#### GitHub Workflow

Claude Code already knows how to use `gh` CLI and `git`. Authenticate on your server with `gh auth login`, then work with repos conversationally:

```
You: List my repos related to monitoring
Bot: [Claude runs gh repo list, shows results]

You: Clone the uptime one
Bot: [Claude runs gh repo clone, clones into workspace]

You: /repo
Bot: üì¶ uptime-monitor/  ‚óÄ
     üìÅ other-project/

You: Show me the open issues
Bot: [Claude runs gh issue list]

You: Create a fix branch and push it
Bot: [Claude creates branch, commits, pushes]
```

Use `/repo` to list cloned repos in your workspace, or `/repo &lt;name&gt;` to switch directories (sessions auto-resume).

### Classic Mode

Set `AGENTIC_MODE=false` to enable the full 13-command terminal-like interface with directory navigation, inline keyboards, quick actions, git integration, and session export.

**Commands:** `/start`, `/help`, `/new`, `/continue`, `/end`, `/status`, `/cd`, `/ls`, `/pwd`, `/projects`, `/export`, `/actions`, `/git`  
If `ENABLE_PROJECT_THREADS=true`: `/sync_threads`

```
You: /cd my-web-app
Bot: Directory changed to my-web-app/

You: /ls
Bot: src/  tests/  package.json  README.md

You: /actions
Bot: [Run Tests] [Install Deps] [Format Code] [Run Linter]
```

## Event-Driven Automation

Beyond direct chat, the bot can respond to external triggers:

- **Webhooks** -- Receive GitHub events (push, PR, issues) and route them through Claude for automated summaries or code review
- **Scheduler** -- Run recurring Claude tasks on a cron schedule (e.g., daily code health checks)
- **Notifications** -- Deliver agent responses to configured Telegram chats

Enable with `ENABLE_API_SERVER=true` and `ENABLE_SCHEDULER=true`. See [docs/setup.md](docs/setup.md) for configuration.

## Features

### Working Features

- Conversational agentic mode (default) with natural language interaction
- Classic terminal-like mode with 13 commands and inline keyboards
- Full Claude Code integration with SDK (primary) and CLI (fallback)
- Automatic session persistence per user/project directory
- Multi-layer authentication (whitelist + optional token-based)
- Rate limiting with token bucket algorithm
- Directory sandboxing with path traversal prevention
- File upload handling with archive extraction
- Image/screenshot upload with analysis
- Git integration with safe repository operations
- Quick actions system with context-aware buttons
- Session export in Markdown, HTML, and JSON formats
- SQLite persistence with migrations
- Usage and cost tracking
- Audit logging and security event tracking
- Event bus for decoupled message routing
- Webhook API server (GitHub HMAC-SHA256, generic Bearer token auth)
- Job scheduler with cron expressions and persistent storage
- Notification service with per-chat rate limiting

- Tunable verbose output showing Claude&#039;s tool usage and reasoning in real-time
- Persistent typing indicator so users always know the bot is working

### Planned Enhancements

- Plugin system for third-party extensions

## Configuration

### Required

```bash
TELEGRAM_BOT_TOKEN=...           # From @BotFather
TELEGRAM_BOT_USERNAME=...        # Your bot&#039;s username
APPROVED_DIRECTORY=...           # Base directory for project access
ALLOWED_USERS=123456789          # Comma-separated Telegram user IDs
```

### Common Options

```bash
# Claude
ANTHROPIC_API_KEY=sk-ant-...     # API key (optional if using CLI auth)
CLAUDE_MAX_COST_PER_USER=10.0    # Spending limit per user (USD)
CLAUDE_TIMEOUT_SECONDS=300       # Operation timeout

# Mode
AGENTIC_MODE=true                # Agentic (default) or classic mode
VERBOSE_LEVEL=1                  # 0=quiet, 1=normal (default), 2=detailed

# Rate Limiting
RATE_LIMIT_REQUESTS=10           # Requests per window
RATE_LIMIT_WINDOW=60             # Window in seconds

# Features (classic mode)
ENABLE_GIT_INTEGRATION=true
ENABLE_FILE_UPLOADS=true
ENABLE_QUICK_ACTIONS=true
```

### Agentic Platform

```bash
# Webhook API Server
ENABLE_API_SERVER=false          # Enable FastAPI webhook server
API_SERVER_PORT=8080             # Server port

# Webhook Authentication
GITHUB_WEBHOOK_SECRET=...        # GitHub HMAC-SHA256 secret
WEBHOOK_API_SECRET=...           # Bearer token for generic providers

# Scheduler
ENABLE_SCHEDULER=false           # Enable cron job scheduler

# Notifications
NOTIFICATION_CHAT_IDS=123,456    # Default chat IDs for proactive notifications
```

### Project Threads Mode

```bash
# Enable strict topic routing by project
ENABLE_PROJECT_THREADS=true

# Mode: private (default) or group
PROJECT_THREADS_MODE=private

# YAML registry file (see config/projects.example.yaml)
PROJECTS_CONFIG_PATH=config/projects.yaml

# Required only when PROJECT_THREADS_MODE=group
PROJECT_THREADS_CHAT_ID=-1001234567890
```

In strict mode, only `/start` and `/sync_threads` work outside mapped project topics.
In private mode, `/start` auto-syncs project topics for your private bot chat.
To use topics with your bot, enable them in BotFather:
`Bot Settings -&gt; Threaded mode`.

&gt; **Full reference:** See [docs/configuration.md](docs/configuration.md) and [`.env.example`](.env.example).

### Finding Your Telegram User ID

Message [@userinfobot](https://t.me/userinfobot) on Telegram -- it will reply with your user ID number.

## Troubleshooting

**Bot doesn&#039;t respond:**
- Check your `TELEGRAM_BOT_TOKEN` is correct
- Verify your user ID is in `ALLOWED_USERS`
- Ensure Claude Code CLI is installed and accessible
- Check bot logs with `make run-debug`

**Claude integration not working:**
- SDK mode (default): Check `claude auth status` or verify `ANTHROPIC_API_KEY`
- CLI mode: Verify `claude --version` and `claude auth status`
- Check `CLAUDE_ALLOWED_TOOLS` includes necessary tools

**High usage costs:**
- Adjust `CLAUDE_MAX_COST_PER_USER` to set spending limits
- Monitor usage with `/status`
- Use shorter, more focused requests

## Security

This bot implements defense-in-depth security:

- **Access Control** -- Whitelist-based user authentication
- **Directory Isolation** -- Sandboxing to approved directories
- **Rate Limiting** -- Request and cost-based limits
- **Input Validation** -- Injection and path traversal protection
- **Webhook Authentication** -- GitHub HMAC-SHA256 and Bearer token verification
- **Audit Logging** -- Complete tracking of all user actions

See [SECURITY.md](SECURITY.md) for details.

## Development

```bash
make dev           # Install all dependencies
make test          # Run tests with coverage
make lint          # Black + isort + flake8 + mypy
make format        # Auto-format code
make run-debug     # Run with debug logging
```

### Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Make changes with tests: `make test &amp;&amp; make lint`
4. Submit a Pull Request

**Code standards:** Python 3.10+, Black formatting (88 chars), type hints required, pytest with &gt;85% coverage.

## License

MIT License -- see [LICENSE](LICENSE).

## Acknowledgments

- [Claude](https://claude.ai) by Anthropic
- [python-telegram-bot](https://github.com/python-telegram-bot/python-telegram-bot)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[sherlock-project/sherlock]]></title>
            <link>https://github.com/sherlock-project/sherlock</link>
            <guid>https://github.com/sherlock-project/sherlock</guid>
            <pubDate>Sat, 21 Feb 2026 00:07:07 GMT</pubDate>
            <description><![CDATA[Hunt down social media accounts by username across social networks]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/sherlock-project/sherlock">sherlock-project/sherlock</a></h1>
            <p>Hunt down social media accounts by username across social networks</p>
            <p>Language: Python</p>
            <p>Stars: 72,981</p>
            <p>Forks: 8,649</p>
            <p>Stars today: 97 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Comfy-Org/ComfyUI]]></title>
            <link>https://github.com/Comfy-Org/ComfyUI</link>
            <guid>https://github.com/Comfy-Org/ComfyUI</guid>
            <pubDate>Sat, 21 Feb 2026 00:07:06 GMT</pubDate>
            <description><![CDATA[The most powerful and modular diffusion model GUI, api and backend with a graph/nodes interface.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Comfy-Org/ComfyUI">Comfy-Org/ComfyUI</a></h1>
            <p>The most powerful and modular diffusion model GUI, api and backend with a graph/nodes interface.</p>
            <p>Language: Python</p>
            <p>Stars: 103,726</p>
            <p>Forks: 11,842</p>
            <p>Stars today: 92 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

# ComfyUI
**The most powerful and modular visual AI engine and application.**


[![Website][website-shield]][website-url]
[![Dynamic JSON Badge][discord-shield]][discord-url]
[![Twitter][twitter-shield]][twitter-url]
[![Matrix][matrix-shield]][matrix-url]
&lt;br&gt;
[![][github-release-shield]][github-release-link]
[![][github-release-date-shield]][github-release-link]
[![][github-downloads-shield]][github-downloads-link]
[![][github-downloads-latest-shield]][github-downloads-link]

[matrix-shield]: https://img.shields.io/badge/Matrix-000000?style=flat&amp;logo=matrix&amp;logoColor=white
[matrix-url]: https://app.element.io/#/room/%23comfyui_space%3Amatrix.org
[website-shield]: https://img.shields.io/badge/ComfyOrg-4285F4?style=flat
[website-url]: https://www.comfy.org/
&lt;!-- Workaround to display total user from https://github.com/badges/shields/issues/4500#issuecomment-2060079995 --&gt;
[discord-shield]: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2Fcomfyorg%3Fwith_counts%3Dtrue&amp;query=%24.approximate_member_count&amp;logo=discord&amp;logoColor=white&amp;label=Discord&amp;color=green&amp;suffix=%20total
[discord-url]: https://www.comfy.org/discord
[twitter-shield]: https://img.shields.io/twitter/follow/ComfyUI
[twitter-url]: https://x.com/ComfyUI

[github-release-shield]: https://img.shields.io/github/v/release/comfyanonymous/ComfyUI?style=flat&amp;sort=semver
[github-release-link]: https://github.com/comfyanonymous/ComfyUI/releases
[github-release-date-shield]: https://img.shields.io/github/release-date/comfyanonymous/ComfyUI?style=flat
[github-downloads-shield]: https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/total?style=flat
[github-downloads-latest-shield]: https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/latest/total?style=flat&amp;label=downloads%40latest
[github-downloads-link]: https://github.com/comfyanonymous/ComfyUI/releases

![ComfyUI Screenshot](https://github.com/user-attachments/assets/7ccaf2c1-9b72-41ae-9a89-5688c94b7abe)
&lt;/div&gt;

ComfyUI lets you design and execute advanced stable diffusion pipelines using a graph/nodes/flowchart based interface. Available on Windows, Linux, and macOS.

## Get Started

#### [Desktop Application](https://www.comfy.org/download)
- The easiest way to get started.
- Available on Windows &amp; macOS.

#### [Windows Portable Package](#installing)
- Get the latest commits and completely portable.
- Available on Windows.

#### [Manual Install](#manual-install-windows-linux)
Supports all operating systems and GPU types (NVIDIA, AMD, Intel, Apple Silicon, Ascend).

## [Examples](https://comfyanonymous.github.io/ComfyUI_examples/)
See what ComfyUI can do with the [example workflows](https://comfyanonymous.github.io/ComfyUI_examples/).

## Features
- Nodes/graph/flowchart interface to experiment and create complex Stable Diffusion workflows without needing to code anything.
- Image Models
   - SD1.x, SD2.x ([unCLIP](https://comfyanonymous.github.io/ComfyUI_examples/unclip/))
   - [SDXL](https://comfyanonymous.github.io/ComfyUI_examples/sdxl/), [SDXL Turbo](https://comfyanonymous.github.io/ComfyUI_examples/sdturbo/)
   - [Stable Cascade](https://comfyanonymous.github.io/ComfyUI_examples/stable_cascade/)
   - [SD3 and SD3.5](https://comfyanonymous.github.io/ComfyUI_examples/sd3/)
   - Pixart Alpha and Sigma
   - [AuraFlow](https://comfyanonymous.github.io/ComfyUI_examples/aura_flow/)
   - [HunyuanDiT](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_dit/)
   - [Flux](https://comfyanonymous.github.io/ComfyUI_examples/flux/)
   - [Lumina Image 2.0](https://comfyanonymous.github.io/ComfyUI_examples/lumina2/)
   - [HiDream](https://comfyanonymous.github.io/ComfyUI_examples/hidream/)
   - [Qwen Image](https://comfyanonymous.github.io/ComfyUI_examples/qwen_image/)
   - [Hunyuan Image 2.1](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_image/)
   - [Flux 2](https://comfyanonymous.github.io/ComfyUI_examples/flux2/)
   - [Z Image](https://comfyanonymous.github.io/ComfyUI_examples/z_image/)
- Image Editing Models
   - [Omnigen 2](https://comfyanonymous.github.io/ComfyUI_examples/omnigen/)
   - [Flux Kontext](https://comfyanonymous.github.io/ComfyUI_examples/flux/#flux-kontext-image-editing-model)
   - [HiDream E1.1](https://comfyanonymous.github.io/ComfyUI_examples/hidream/#hidream-e11)
   - [Qwen Image Edit](https://comfyanonymous.github.io/ComfyUI_examples/qwen_image/#edit-model)
- Video Models
   - [Stable Video Diffusion](https://comfyanonymous.github.io/ComfyUI_examples/video/)
   - [Mochi](https://comfyanonymous.github.io/ComfyUI_examples/mochi/)
   - [LTX-Video](https://comfyanonymous.github.io/ComfyUI_examples/ltxv/)
   - [Hunyuan Video](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/)
   - [Wan 2.1](https://comfyanonymous.github.io/ComfyUI_examples/wan/)
   - [Wan 2.2](https://comfyanonymous.github.io/ComfyUI_examples/wan22/)
   - [Hunyuan Video 1.5](https://docs.comfy.org/tutorials/video/hunyuan/hunyuan-video-1-5)
- Audio Models
   - [Stable Audio](https://comfyanonymous.github.io/ComfyUI_examples/audio/)
   - [ACE Step](https://comfyanonymous.github.io/ComfyUI_examples/audio/)
- 3D Models
   - [Hunyuan3D 2.0](https://docs.comfy.org/tutorials/3d/hunyuan3D-2)
- Asynchronous Queue system
- Many optimizations: Only re-executes the parts of the workflow that changes between executions.
- Smart memory management: can automatically run large models on GPUs with as low as 1GB vram with smart offloading.
- Works even if you don&#039;t have a GPU with: ```--cpu``` (slow)
- Can load ckpt and safetensors: All in one checkpoints or standalone diffusion models, VAEs and CLIP models.
- Safe loading of ckpt, pt, pth, etc.. files.
- Embeddings/Textual inversion
- [Loras (regular, locon and loha)](https://comfyanonymous.github.io/ComfyUI_examples/lora/)
- [Hypernetworks](https://comfyanonymous.github.io/ComfyUI_examples/hypernetworks/)
- Loading full workflows (with seeds) from generated PNG, WebP and FLAC files.
- Saving/Loading workflows as Json files.
- Nodes interface can be used to create complex workflows like one for [Hires fix](https://comfyanonymous.github.io/ComfyUI_examples/2_pass_txt2img/) or much more advanced ones.
- [Area Composition](https://comfyanonymous.github.io/ComfyUI_examples/area_composition/)
- [Inpainting](https://comfyanonymous.github.io/ComfyUI_examples/inpaint/) with both regular and inpainting models.
- [ControlNet and T2I-Adapter](https://comfyanonymous.github.io/ComfyUI_examples/controlnet/)
- [Upscale Models (ESRGAN, ESRGAN variants, SwinIR, Swin2SR, etc...)](https://comfyanonymous.github.io/ComfyUI_examples/upscale_models/)
- [GLIGEN](https://comfyanonymous.github.io/ComfyUI_examples/gligen/)
- [Model Merging](https://comfyanonymous.github.io/ComfyUI_examples/model_merging/)
- [LCM models and Loras](https://comfyanonymous.github.io/ComfyUI_examples/lcm/)
- Latent previews with [TAESD](#how-to-show-high-quality-previews)
- Works fully offline: core will never download anything unless you want to.
- Optional API nodes to use paid models from external providers through the online [Comfy API](https://docs.comfy.org/tutorials/api-nodes/overview) disable with: `--disable-api-nodes`
- [Config file](extra_model_paths.yaml.example) to set the search paths for models.

Workflow examples can be found on the [Examples page](https://comfyanonymous.github.io/ComfyUI_examples/)

## Release Process

ComfyUI follows a weekly release cycle targeting Monday but this regularly changes because of model releases or large changes to the codebase. There are three interconnected repositories:

1. **[ComfyUI Core](https://github.com/comfyanonymous/ComfyUI)**
   - Releases a new stable version (e.g., v0.7.0) roughly every week.
   - Starting from v0.4.0 patch versions will be used for fixes backported onto the current stable release.
   - Minor versions will be used for releases off the master branch.
   - Patch versions may still be used for releases on the master branch in cases where a backport would not make sense.
   - Commits outside of the stable release tags may be very unstable and break many custom nodes.
   - Serves as the foundation for the desktop release

2. **[ComfyUI Desktop](https://github.com/Comfy-Org/desktop)**
   - Builds a new release using the latest stable core version

3. **[ComfyUI Frontend](https://github.com/Comfy-Org/ComfyUI_frontend)**
   - Weekly frontend updates are merged into the core repository
   - Features are frozen for the upcoming core release
   - Development continues for the next release cycle

## Shortcuts

| Keybind                            | Explanation                                                                                                        |
|------------------------------------|--------------------------------------------------------------------------------------------------------------------|
| `Ctrl` + `Enter`                      | Queue up current graph for generation                                                                              |
| `Ctrl` + `Shift` + `Enter`              | Queue up current graph as first for generation                                                                     |
| `Ctrl` + `Alt` + `Enter`                | Cancel current generation                                                                                          |
| `Ctrl` + `Z`/`Ctrl` + `Y`                 | Undo/Redo                                                                                                          |
| `Ctrl` + `S`                          | Save workflow                                                                                                      |
| `Ctrl` + `O`                          | Load workflow                                                                                                      |
| `Ctrl` + `A`                          | Select all nodes                                                                                                   |
| `Alt `+ `C`                           | Collapse/uncollapse selected nodes                                                                                 |
| `Ctrl` + `M`                          | Mute/unmute selected nodes                                                                                         |
| `Ctrl` + `B`                           | Bypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)            |
| `Delete`/`Backspace`                   | Delete selected nodes                                                                                              |
| `Ctrl` + `Backspace`                   | Delete the current graph                                                                                           |
| `Space`                              | Move the canvas around when held and moving the cursor                                                             |
| `Ctrl`/`Shift` + `Click`                 | Add clicked node to selection                                                                                      |
| `Ctrl` + `C`/`Ctrl` + `V`                  | Copy and paste selected nodes (without maintaining connections to outputs of unselected nodes)                     |
| `Ctrl` + `C`/`Ctrl` + `Shift` + `V`          | Copy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes) |
| `Shift` + `Drag`                       | Move multiple selected nodes at the same time                                                                      |
| `Ctrl` + `D`                           | Load default graph                                                                                                 |
| `Alt` + `+`                          | Canvas Zoom in                                                                                                     |
| `Alt` + `-`                          | Canvas Zoom out                                                                                                    |
| `Ctrl` + `Shift` + LMB + Vertical drag | Canvas Zoom in/out                                                                                                 |
| `P`                                  | Pin/Unpin selected nodes                                                                                           |
| `Ctrl` + `G`                           | Group selected nodes                                                                                               |
| `Q`                                 | Toggle visibility of the queue                                                                                     |
| `H`                                  | Toggle visibility of history                                                                                       |
| `R`                                  | Refresh graph                                                                                                      |
| `F`                                  | Show/Hide menu                                                                                                      |
| `.`                                  | Fit view to selection (Whole graph when nothing is selected)                                                        |
| Double-Click LMB                   | Open node quick search palette                                                                                     |
| `Shift` + Drag                       | Move multiple wires at once                                                                                        |
| `Ctrl` + `Alt` + LMB                   | Disconnect all wires from clicked slot                                                                             |

`Ctrl` can also be replaced with `Cmd` instead for macOS users

# Installing

## Windows Portable

There is a portable standalone build for Windows that should work for running on Nvidia GPUs or for running on your CPU only on the [releases page](https://github.com/comfyanonymous/ComfyUI/releases).

### [Direct link to download](https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z)

Simply download, extract with [7-Zip](https://7-zip.org) or with the windows explorer on recent windows versions and run. For smaller models you normally only need to put the checkpoints (the huge ckpt/safetensors files) in: ComfyUI\models\checkpoints but many of the larger models have multiple files. Make sure to follow the instructions to know which subfolder to put them in ComfyUI\models\

If you have trouble extracting it, right click the file -&gt; properties -&gt; unblock

The portable above currently comes with python 3.13 and pytorch cuda 13.0. Update your Nvidia drivers if it doesn&#039;t start.

#### Alternative Downloads:

[Experimental portable for AMD GPUs](https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_amd.7z)

[Portable with pytorch cuda 12.8 and python 3.12](https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia_cu128.7z).

[Portable with pytorch cuda 12.6 and python 3.12](https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia_cu126.7z) (Supports Nvidia 10 series and older GPUs).

#### How do I share models between another UI and ComfyUI?

See the [Config file](extra_model_paths.yaml.example) to set the search paths for models. In the standalone windows build you can find this file in the ComfyUI directory. Rename this file to extra_model_paths.yaml and edit it with your favorite text editor.


## [comfy-cli](https://docs.comfy.org/comfy-cli/getting-started)

You can install and start ComfyUI using comfy-cli:
```bash
pip install comfy-cli
comfy install
```

## Manual Install (Windows, Linux)

Python 3.14 works but some custom nodes may have issues. The free threaded variant works but some dependencies will enable the GIL so it&#039;s not fully supported.

Python 3.13 is very well supported. If you have trouble with some custom node dependencies on 3.13 you can try 3.12

torch 2.4 and above is supported but some features and optimizations might only work on newer versions. We generally recommend using the latest major version of pytorch with the latest cuda version unless it is less than 2 weeks old.

### Instructions:

Git clone this repo.

Put your SD checkpoints (the huge ckpt/safetensors files) in: models/checkpoints

Put your VAE in: models/vae


### AMD GPUs (Linux)

AMD users can install rocm and pytorch with pip if you don&#039;t have it already installed, this is the command to install the stable version:

```pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm7.1```

This is the command to install the nightly with ROCm 7.1 which might have some performance improvements:

```pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm7.1```


### AMD GPUs (Experimental: Windows and Linux), RDNA 3, 3.5 and 4 only.

These have less hardware support than the builds above but they work on windows. You also need to install the pytorch version specific to your hardware.

RDNA 3 (RX 7000 series):

```pip install --pre torch torchvision torchaudio --index-url https://rocm.nightlies.amd.com/v2/gfx110X-all/```

RDNA 3.5 (Strix halo/Ryzen AI Max+ 365):

```pip install --pre torch torchvision torchaudio --index-url https://rocm.nightlies.amd.com/v2/gfx1151/```

RDNA 4 (RX 9000 series):

```pip install --pre torch torchvision torchaudio --index-url https://rocm.nightlies.amd.com/v2/gfx120X-all/```

### Intel GPUs (Windows and Linux)

Intel Arc GPU users can install native PyTorch with torch.xpu support using pip. More information can be found [here](https://pytorch.org/docs/main/notes/get_start_xpu.html)

1. To install PyTorch xpu, use the following command:

```pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/xpu```

This is the command to install the Pytorch xpu nightly which might have some performance improvements:

```pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/xpu```

### NVIDIA

Nvidia users should install stable pytorch using this command:

```pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu130```

This is the command to install pytorch nightly instead which might have performance improvements.

```pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu130```

#### Troubleshooting

If you get the &quot;Torch not compiled with CUDA enabled&quot; error, uninstall torch with:

```pip uninstall torch```

And install it again with the command above.

### Dependencies

Install the dependencies by opening your terminal inside the ComfyUI folder and:

```pip install -r requirements.txt```

After this you should have everything installed and can proceed to running ComfyUI.

### Others:

#### Apple Mac silicon

You can install ComfyUI in Apple Mac silicon (M1 or M2) with any recent macOS version.

1. Install pytorch nightly. For instructions, read the [Accelerated PyTorch training on Mac](https://developer.apple.com/metal/pytorch/) Apple Developer guide (make sure to install the latest pytorch nightly).
1. Follow the [ComfyUI manual installation](#manual-install-windows-linux) instructions for Windows and Linux.
1. Install the ComfyUI [dependencies](#dependencies). If you have another Stable Diffusion UI [you might be able to reuse the dependencies](#i-already-have-another-ui-for-stable-diffusion-installed-do-i-really-have-to-install-all-of-these-dependencies).
1. Launch ComfyUI by running `python main.py`

&gt; **Note**: Remember to add your models, VAE, LoRAs etc. to the corresponding Comfy folders, as discuss

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[666ghj/MiroFish]]></title>
            <link>https://github.com/666ghj/MiroFish</link>
            <guid>https://github.com/666ghj/MiroFish</guid>
            <pubDate>Sat, 21 Feb 2026 00:07:05 GMT</pubDate>
            <description><![CDATA[A Simple and Universal Swarm Intelligence Engine, Predicting Anything. ÁÆÄÊ¥ÅÈÄöÁî®ÁöÑÁæ§‰ΩìÊô∫ËÉΩÂºïÊìéÔºåÈ¢ÑÊµã‰∏áÁâ©]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/666ghj/MiroFish">666ghj/MiroFish</a></h1>
            <p>A Simple and Universal Swarm Intelligence Engine, Predicting Anything. ÁÆÄÊ¥ÅÈÄöÁî®ÁöÑÁæ§‰ΩìÊô∫ËÉΩÂºïÊìéÔºåÈ¢ÑÊµã‰∏áÁâ©</p>
            <p>Language: Python</p>
            <p>Stars: 4,191</p>
            <p>Forks: 514</p>
            <p>Stars today: 109 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;./static/image/MiroFish_logo_compressed.jpeg&quot; alt=&quot;MiroFish Logo&quot; width=&quot;75%&quot;/&gt;

ÁÆÄÊ¥ÅÈÄöÁî®ÁöÑÁæ§‰ΩìÊô∫ËÉΩÂºïÊìéÔºåÈ¢ÑÊµã‰∏áÁâ©
&lt;/br&gt;
&lt;em&gt;A Simple and Universal Swarm Intelligence Engine, Predicting Anything&lt;/em&gt;

&lt;a href=&quot;https://www.shanda.com/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/shanda_logo.png&quot; alt=&quot;666ghj%2MiroFish | Shanda&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;

[![GitHub Stars](https://img.shields.io/github/stars/666ghj/MiroFish?style=flat-square)](https://github.com/666ghj/MiroFish/stargazers)
[![GitHub Watchers](https://img.shields.io/github/watchers/666ghj/MiroFish?style=flat-square)](https://github.com/666ghj/MiroFish/watchers)
[![GitHub Forks](https://img.shields.io/github/forks/666ghj/MiroFish?style=flat-square)](https://github.com/666ghj/MiroFish/network)
[![GitHub Issues](https://img.shields.io/github/issues/666ghj/MiroFish?style=flat-square)](https://github.com/666ghj/MiroFish/issues)
[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/666ghj/MiroFish?style=flat-square)](https://github.com/666ghj/MiroFish/pulls)

[![GitHub License](https://img.shields.io/github/license/666ghj/MiroFish?style=flat-square)](https://github.com/666ghj/MiroFish/blob/main/LICENSE)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/666ghj/MiroFish)
[![Docker](https://img.shields.io/badge/Docker-Build-2496ED?style=flat-square&amp;logo=docker&amp;logoColor=white)](https://hub.docker.com/)
[![Version](https://img.shields.io/badge/version-v0.1.0-green.svg?style=flat-square)](https://github.com/666ghj/MiroFish)

[English](./README-EN.md) | [‰∏≠ÊñáÊñáÊ°£](./README.md)

&lt;/div&gt;

## ‚ö° È°πÁõÆÊ¶ÇËø∞

**MiroFish** ÊòØ‰∏ÄÊ¨æÂü∫‰∫éÂ§öÊô∫ËÉΩ‰ΩìÊäÄÊúØÁöÑÊñ∞‰∏Ä‰ª£ AI È¢ÑÊµãÂºïÊìé„ÄÇÈÄöËøáÊèêÂèñÁé∞ÂÆû‰∏ñÁïåÁöÑÁßçÂ≠ê‰ø°ÊÅØÔºàÂ¶ÇÁ™ÅÂèëÊñ∞Èóª„ÄÅÊîøÁ≠ñËçâÊ°à„ÄÅÈáëËûç‰ø°Âè∑ÔºâÔºåËá™Âä®ÊûÑÂª∫Âá∫È´ò‰øùÁúüÁöÑÂπ≥Ë°åÊï∞Â≠ó‰∏ñÁïå„ÄÇÂú®Ê≠§Á©∫Èó¥ÂÜÖÔºåÊàêÂçÉ‰∏ä‰∏á‰∏™ÂÖ∑Â§áÁã¨Á´ã‰∫∫Ê†º„ÄÅÈïøÊúüËÆ∞ÂøÜ‰∏éË°å‰∏∫ÈÄªËæëÁöÑÊô∫ËÉΩ‰ΩìËøõË°åËá™Áî±‰∫§‰∫í‰∏éÁ§æ‰ºöÊºîÂåñ„ÄÇ‰Ω†ÂèØÈÄèËøá„Äå‰∏äÂ∏ùËßÜËßí„ÄçÂä®ÊÄÅÊ≥®ÂÖ•ÂèòÈáèÔºåÁ≤æÂáÜÊé®ÊºîÊú™Êù•Ëµ∞Âêë‚Äî‚Äî**ËÆ©Êú™Êù•Âú®Êï∞Â≠óÊ≤ôÁõò‰∏≠È¢ÑÊºîÔºåÂä©ÂÜ≥Á≠ñÂú®ÁôæÊàòÊ®°ÊãüÂêéËÉúÂá∫**„ÄÇ

&gt; ‰Ω†Âè™ÈúÄÔºö‰∏ä‰º†ÁßçÂ≠êÊùêÊñôÔºàÊï∞ÊçÆÂàÜÊûêÊä•ÂëäÊàñËÄÖÊúâË∂£ÁöÑÂ∞èËØ¥ÊïÖ‰∫ãÔºâÔºåÂπ∂Áî®Ëá™ÁÑ∂ËØ≠Ë®ÄÊèèËø∞È¢ÑÊµãÈúÄÊ±Ç&lt;/br&gt;
&gt; MiroFish Â∞ÜËøîÂõûÔºö‰∏Ä‰ªΩËØ¶Â∞ΩÁöÑÈ¢ÑÊµãÊä•ÂëäÔºå‰ª•Âèä‰∏Ä‰∏™ÂèØÊ∑±Â∫¶‰∫§‰∫íÁöÑÈ´ò‰øùÁúüÊï∞Â≠ó‰∏ñÁïå

### Êàë‰ª¨ÁöÑÊÑøÊôØ

MiroFish Ëá¥Âäõ‰∫éÊâìÈÄ†Êò†Â∞ÑÁé∞ÂÆûÁöÑÁæ§‰ΩìÊô∫ËÉΩÈïúÂÉèÔºåÈÄöËøáÊçïÊçâ‰∏™‰Ωì‰∫íÂä®ÂºïÂèëÁöÑÁæ§‰ΩìÊ∂åÁé∞ÔºåÁ™ÅÁ†¥‰º†ÁªüÈ¢ÑÊµãÁöÑÂ±ÄÈôêÔºö

- **‰∫éÂÆèËßÇ**ÔºöÊàë‰ª¨ÊòØÂÜ≥Á≠ñËÄÖÁöÑÈ¢ÑÊºîÂÆûÈ™åÂÆ§ÔºåËÆ©ÊîøÁ≠ñ‰∏éÂÖ¨ÂÖ≥Âú®Èõ∂È£éÈô©‰∏≠ËØïÈîô
- **‰∫éÂæÆËßÇ**ÔºöÊàë‰ª¨ÊòØ‰∏™‰∫∫Áî®Êà∑ÁöÑÂàõÊÑèÊ≤ôÁõòÔºåÊó†ËÆ∫ÊòØÊé®ÊºîÂ∞èËØ¥ÁªìÂ±ÄËøòÊòØÊé¢Á¥¢ËÑëÊ¥ûÔºåÁöÜÂèØÊúâË∂£„ÄÅÂ•ΩÁé©„ÄÅËß¶ÊâãÂèØÂèä

‰ªé‰∏•ËÇÉÈ¢ÑÊµãÂà∞Ë∂£Âë≥‰ªøÁúüÔºåÊàë‰ª¨ËÆ©ÊØè‰∏Ä‰∏™Â¶ÇÊûúÈÉΩËÉΩÁúãËßÅÁªìÊûúÔºåËÆ©È¢ÑÊµã‰∏áÁâ©Êàê‰∏∫ÂèØËÉΩ„ÄÇ

## üì∏ Á≥ªÁªüÊà™Âõæ

&lt;div align=&quot;center&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src=&quot;./static/image/Screenshot/ËøêË°åÊà™Âõæ1.png&quot; alt=&quot;Êà™Âõæ1&quot; width=&quot;100%&quot;/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&quot;./static/image/Screenshot/ËøêË°åÊà™Âõæ2.png&quot; alt=&quot;Êà™Âõæ2&quot; width=&quot;100%&quot;/&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src=&quot;./static/image/Screenshot/ËøêË°åÊà™Âõæ3.png&quot; alt=&quot;Êà™Âõæ3&quot; width=&quot;100%&quot;/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&quot;./static/image/Screenshot/ËøêË°åÊà™Âõæ4.png&quot; alt=&quot;Êà™Âõæ4&quot; width=&quot;100%&quot;/&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src=&quot;./static/image/Screenshot/ËøêË°åÊà™Âõæ5.png&quot; alt=&quot;Êà™Âõæ5&quot; width=&quot;100%&quot;/&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&quot;./static/image/Screenshot/ËøêË°åÊà™Âõæ6.png&quot; alt=&quot;Êà™Âõæ6&quot; width=&quot;100%&quot;/&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;

## üé¨ ÊºîÁ§∫ËßÜÈ¢ë

### 1. Ê≠¶Ê±âÂ§ßÂ≠¶ËàÜÊÉÖÊé®ÊºîÈ¢ÑÊµã + MiroFishÈ°πÁõÆËÆ≤Ëß£

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://www.bilibili.com/video/BV1VYBsBHEMY/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/Ê≠¶Â§ßÊ®°ÊãüÊºîÁ§∫Â∞ÅÈù¢.png&quot; alt=&quot;MiroFish Demo Video&quot; width=&quot;75%&quot;/&gt;&lt;/a&gt;

ÁÇπÂáªÂõæÁâáÊü•Áúã‰ΩøÁî®ÂæÆËàÜBettaFishÁîüÊàêÁöÑ„ÄäÊ≠¶Â§ßËàÜÊÉÖÊä•Âëä„ÄãËøõË°åÈ¢ÑÊµãÁöÑÂÆåÊï¥ÊºîÁ§∫ËßÜÈ¢ë
&lt;/div&gt;

### 2. „ÄäÁ∫¢Ê•ºÊ¢¶„ÄãÂ§±‰º†ÁªìÂ±ÄÊé®ÊºîÈ¢ÑÊµã

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://www.bilibili.com/video/BV1cPk3BBExq&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/Á∫¢Ê•ºÊ¢¶Ê®°ÊãüÊé®ÊºîÂ∞ÅÈù¢.jpg&quot; alt=&quot;MiroFish Demo Video&quot; width=&quot;75%&quot;/&gt;&lt;/a&gt;

ÁÇπÂáªÂõæÁâáÊü•ÁúãÂü∫‰∫é„ÄäÁ∫¢Ê•ºÊ¢¶„ÄãÂâç80ÂõûÊï∞ÂçÅ‰∏áÂ≠óÔºåMiroFishÊ∑±Â∫¶È¢ÑÊµãÂ§±‰º†ÁªìÂ±Ä
&lt;/div&gt;

&gt; **ÈáëËûçÊñπÂêëÊé®ÊºîÈ¢ÑÊµã**„ÄÅ**Êó∂ÊîøË¶ÅÈóªÊé®ÊºîÈ¢ÑÊµã**Á≠âÁ§∫‰æãÈôÜÁª≠Êõ¥Êñ∞‰∏≠...

## üîÑ Â∑•‰ΩúÊµÅÁ®ã

1. **ÂõæË∞±ÊûÑÂª∫**ÔºöÁé∞ÂÆûÁßçÂ≠êÊèêÂèñ &amp; ‰∏™‰Ωì‰∏éÁæ§‰ΩìËÆ∞ÂøÜÊ≥®ÂÖ• &amp; GraphRAGÊûÑÂª∫
2. **ÁéØÂ¢ÉÊê≠Âª∫**ÔºöÂÆû‰ΩìÂÖ≥Á≥ªÊäΩÂèñ &amp; ‰∫∫ËÆæÁîüÊàê &amp; ÁéØÂ¢ÉÈÖçÁΩÆAgentÊ≥®ÂÖ•‰ªøÁúüÂèÇÊï∞
3. **ÂºÄÂßãÊ®°Êãü**ÔºöÂèåÂπ≥Âè∞Âπ∂Ë°åÊ®°Êãü &amp; Ëá™Âä®Ëß£ÊûêÈ¢ÑÊµãÈúÄÊ±Ç &amp; Âä®ÊÄÅÊõ¥Êñ∞Êó∂Â∫èËÆ∞ÂøÜ
4. **Êä•ÂëäÁîüÊàê**ÔºöReportAgentÊã•Êúâ‰∏∞ÂØåÁöÑÂ∑•ÂÖ∑ÈõÜ‰∏éÊ®°ÊãüÂêéÁéØÂ¢ÉËøõË°åÊ∑±Â∫¶‰∫§‰∫í
5. **Ê∑±Â∫¶‰∫íÂä®**Ôºö‰∏éÊ®°Êãü‰∏ñÁïå‰∏≠ÁöÑ‰ªªÊÑè‰∏Ä‰ΩçËøõË°åÂØπËØù &amp; ‰∏éReportAgentËøõË°åÂØπËØù

## üöÄ Âø´ÈÄüÂºÄÂßã

### ‰∏Ä„ÄÅÊ∫êÁ†ÅÈÉ®ÁΩ≤ÔºàÊé®ËçêÔºâ

#### ÂâçÁΩÆË¶ÅÊ±Ç

| Â∑•ÂÖ∑ | ÁâàÊú¨Ë¶ÅÊ±Ç | ËØ¥Êòé | ÂÆâË£ÖÊ£ÄÊü• |
|------|---------|------|---------|
| **Node.js** | 18+ | ÂâçÁ´ØËøêË°åÁéØÂ¢ÉÔºåÂåÖÂê´ npm | `node -v` |
| **Python** | ‚â•3.11, ‚â§3.12 | ÂêéÁ´ØËøêË°åÁéØÂ¢É | `python --version` |
| **uv** | ÊúÄÊñ∞Áâà | Python ÂåÖÁÆ°ÁêÜÂô® | `uv --version` |

#### 1. ÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáè

```bash
# Â§çÂà∂Á§∫‰æãÈÖçÁΩÆÊñá‰ª∂
cp .env.example .env

# ÁºñËæë .env Êñá‰ª∂ÔºåÂ°´ÂÖ•ÂøÖË¶ÅÁöÑ API ÂØÜÈí•
```

**ÂøÖÈúÄÁöÑÁéØÂ¢ÉÂèòÈáèÔºö**

```env
# LLM APIÈÖçÁΩÆÔºàÊîØÊåÅ OpenAI SDK Ê†ºÂºèÁöÑ‰ªªÊÑè LLM APIÔºâ
# Êé®Ëçê‰ΩøÁî®ÈòøÈáåÁôæÁÇºÂπ≥Âè∞qwen-plusÊ®°ÂûãÔºöhttps://bailian.console.aliyun.com/
# Ê≥®ÊÑèÊ∂àËÄóËæÉÂ§ßÔºåÂèØÂÖàËøõË°åÂ∞è‰∫é40ËΩÆÁöÑÊ®°ÊãüÂ∞ùËØï
LLM_API_KEY=your_api_key
LLM_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1
LLM_MODEL_NAME=qwen-plus

# Zep Cloud ÈÖçÁΩÆ
# ÊØèÊúàÂÖçË¥πÈ¢ùÂ∫¶Âç≥ÂèØÊîØÊíëÁÆÄÂçï‰ΩøÁî®Ôºöhttps://app.getzep.com/
ZEP_API_KEY=your_zep_api_key
```

#### 2. ÂÆâË£Ö‰æùËµñ

```bash
# ‰∏ÄÈîÆÂÆâË£ÖÊâÄÊúâ‰æùËµñÔºàÊ†πÁõÆÂΩï + ÂâçÁ´Ø + ÂêéÁ´ØÔºâ
npm run setup:all
```

ÊàñËÄÖÂàÜÊ≠•ÂÆâË£ÖÔºö

```bash
# ÂÆâË£Ö Node ‰æùËµñÔºàÊ†πÁõÆÂΩï + ÂâçÁ´ØÔºâ
npm run setup

# ÂÆâË£Ö Python ‰æùËµñÔºàÂêéÁ´ØÔºåËá™Âä®ÂàõÂª∫ËôöÊãüÁéØÂ¢ÉÔºâ
npm run setup:backend
```

#### 3. ÂêØÂä®ÊúçÂä°

```bash
# ÂêåÊó∂ÂêØÂä®ÂâçÂêéÁ´ØÔºàÂú®È°πÁõÆÊ†πÁõÆÂΩïÊâßË°åÔºâ
npm run dev
```

**ÊúçÂä°Âú∞ÂùÄÔºö**
- ÂâçÁ´ØÔºö`http://localhost:3000`
- ÂêéÁ´Ø APIÔºö`http://localhost:5001`

**ÂçïÁã¨ÂêØÂä®Ôºö**

```bash
npm run backend   # ‰ªÖÂêØÂä®ÂêéÁ´Ø
npm run frontend  # ‰ªÖÂêØÂä®ÂâçÁ´Ø
```

### ‰∫å„ÄÅDocker ÈÉ®ÁΩ≤

```bash
# 1. ÈÖçÁΩÆÁéØÂ¢ÉÂèòÈáèÔºàÂêåÊ∫êÁ†ÅÈÉ®ÁΩ≤Ôºâ
cp .env.example .env

# 2. ÊãâÂèñÈïúÂÉèÂπ∂ÂêØÂä®
docker compose up -d
```

ÈªòËÆ§‰ºöËØªÂèñÊ†πÁõÆÂΩï‰∏ãÁöÑ `.env`ÔºåÂπ∂Êò†Â∞ÑÁ´ØÂè£ `3000ÔºàÂâçÁ´ØÔºâ/5001ÔºàÂêéÁ´ØÔºâ`

&gt; Âú® `docker-compose.yml` ‰∏≠Â∑≤ÈÄöËøáÊ≥®ÈáäÊèê‰æõÂä†ÈÄüÈïúÂÉèÂú∞ÂùÄÔºåÂèØÊåâÈúÄÊõøÊç¢

## üì¨ Êõ¥Â§ö‰∫§ÊµÅ

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;./static/image/QQÁæ§.png&quot; alt=&quot;QQ‰∫§ÊµÅÁæ§&quot; width=&quot;60%&quot;/&gt;
&lt;/div&gt;

&amp;nbsp;

MiroFishÂõ¢ÈòüÈïøÊúüÊãõÂãüÂÖ®ËÅå/ÂÆû‰π†ÔºåÂ¶ÇÊûú‰Ω†ÂØπÂ§öAgentÂ∫îÁî®ÊÑüÂÖ¥Ë∂£ÔºåÊ¨¢ËøéÊäïÈÄíÁÆÄÂéÜËá≥Ôºö**mirofish@shanda.com**

## üìÑ Ëá¥Ë∞¢

**MiroFish ÂæóÂà∞‰∫ÜÁõõÂ§ßÈõÜÂõ¢ÁöÑÊàòÁï•ÊîØÊåÅÂíåÂ≠µÂåñÔºÅ**

MiroFish ÁöÑ‰ªøÁúüÂºïÊìéÁî± **[OASIS](https://github.com/camel-ai/oasis)** È©±Âä®ÔºåÊàë‰ª¨Ë°∑ÂøÉÊÑüË∞¢ CAMEL-AI Âõ¢ÈòüÁöÑÂºÄÊ∫êË¥°ÁåÆÔºÅ

## üìà È°πÁõÆÁªüËÆ°

&lt;a href=&quot;https://www.star-history.com/#666ghj/MiroFish&amp;type=date&amp;legend=top-left&quot;&gt;
 &lt;picture&gt;
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=666ghj/MiroFish&amp;type=date&amp;theme=dark&amp;legend=top-left&quot; /&gt;
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=666ghj/MiroFish&amp;type=date&amp;legend=top-left&quot; /&gt;
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=666ghj/MiroFish&amp;type=date&amp;legend=top-left&quot; /&gt;
 &lt;/picture&gt;
&lt;/a&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[django/django]]></title>
            <link>https://github.com/django/django</link>
            <guid>https://github.com/django/django</guid>
            <pubDate>Sat, 21 Feb 2026 00:07:04 GMT</pubDate>
            <description><![CDATA[The Web framework for perfectionists with deadlines.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/django/django">django/django</a></h1>
            <p>The Web framework for perfectionists with deadlines.</p>
            <p>Language: Python</p>
            <p>Stars: 86,900</p>
            <p>Forks: 33,654</p>
            <p>Stars today: 24 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[Future-House/paper-qa]]></title>
            <link>https://github.com/Future-House/paper-qa</link>
            <guid>https://github.com/Future-House/paper-qa</guid>
            <pubDate>Sat, 21 Feb 2026 00:07:03 GMT</pubDate>
            <description><![CDATA[High accuracy RAG for answering questions from scientific documents with citations]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Future-House/paper-qa">Future-House/paper-qa</a></h1>
            <p>High accuracy RAG for answering questions from scientific documents with citations</p>
            <p>Language: Python</p>
            <p>Stars: 8,179</p>
            <p>Forks: 826</p>
            <p>Stars today: 19 stars today</p>
            <h2>README</h2><pre># PaperQA2

&lt;!-- pyml disable-num-lines 6 line-length --&gt;

[![GitHub](https://img.shields.io/badge/GitHub-black?logo=github&amp;logoColor=white)](https://github.com/Future-House/paper-qa)
[![PyPI version](https://badge.fury.io/py/paper-qa.svg)](https://badge.fury.io/py/paper-qa)
[![tests](https://github.com/Future-House/paper-qa/actions/workflows/tests.yml/badge.svg)](https://github.com/Future-House/paper-qa)
![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)
![PyPI Python Versions](https://img.shields.io/pypi/pyversions/paper-qa)

PaperQA2 is a package for doing high-accuracy retrieval augmented generation (RAG) on PDFs, text files, Microsoft Office documents, and source code files,
with a focus on the scientific literature.
See our [recent 2024 paper](https://paper.wikicrow.ai)
to see examples of PaperQA2&#039;s superhuman performance in scientific tasks like
question answering, summarization, and contradiction detection.

&lt;!--TOC--&gt;

---

**Table of Contents**

- [Quickstart](#quickstart)
  - [Example Output](#example-output)
- [What is PaperQA2](#what-is-paperqa2)
  - [PaperQA2 vs PaperQA](#paperqa2-vs-paperqa)
  - [PaperQA2 Goes CalVer in December 2025](#paperqa2-goes-calver-in-december-2025)
  - [What&#039;s New in Version 5 (aka PaperQA2)?](#whats-new-in-version-5-aka-paperqa2)
  - [What&#039;s New in December 2025?](#whats-new-in-december-2025)
  - [PaperQA2 Algorithm](#paperqa2-algorithm)
- [Installation](#installation)
- [CLI Usage](#cli-usage)
  - [Bundled Settings](#bundled-settings)
  - [Rate Limits](#rate-limits)
- [Library Usage](#library-usage)
  - [Agentic Adding/Querying Documents](#agentic-addingquerying-documents)
  - [Manual (No Agent) Adding/Querying Documents](#manual-no-agent-addingquerying-documents)
  - [Async](#async)
  - [Choosing Model](#choosing-model)
    - [Locally Hosted](#locally-hosted)
  - [Embedding Model](#embedding-model)
    - [Specifying the Embedding Model](#specifying-the-embedding-model)
    - [Local Embedding Models (Sentence Transformers)](#local-embedding-models-sentence-transformers)
  - [Adjusting number of sources](#adjusting-number-of-sources)
  - [Using Code or HTML](#using-code-or-html)
  - [Multimodal Support](#multimodal-support)
  - [Using External DB/Vector DB and Caching](#using-external-dbvector-db-and-caching)
  - [Creating Index](#creating-index)
    - [Manifest Files](#manifest-files)
  - [Reusing Index](#reusing-index)
  - [Using Clients Directly](#using-clients-directly)
- [Settings Cheatsheet](#settings-cheatsheet)
- [Where do I get papers?](#where-do-i-get-papers)
- [Callbacks](#callbacks)
  - [Caching Embeddings](#caching-embeddings)
- [Customizing Prompts](#customizing-prompts)
  - [Pre and Post Prompts](#pre-and-post-prompts)
- [FAQ](#faq)
  - [How come I get different results than your papers?](#how-come-i-get-different-results-than-your-papers)
  - [How is this different from LlamaIndex or LangChain?](#how-is-this-different-from-llamaindex-or-langchain)
  - [Can I save or load?](#can-i-save-or-load)
- [Reproduction](#reproduction)
- [Citation](#citation)

---

&lt;!--TOC--&gt;

## Quickstart

In this example we take a folder of research paper PDFs,
magically get their metadata - including citation counts with a retraction check,
then parse and cache PDFs into a full-text search index,
and finally answer the user question with an LLM agent.

```bash
pip install paper-qa
mkdir my_papers
curl -o my_papers/PaperQA2.pdf https://arxiv.org/pdf/2409.13740
cd my_papers
pqa ask &#039;What is PaperQA2?&#039;
```

### Example Output

Question: Has anyone designed neural networks that compute with proteins or DNA?

&gt; The claim that neural networks have been designed to compute with DNA is supported by multiple sources.
&gt; The work by Qian, Winfree, and Bruck demonstrates the use of DNA strand displacement cascades
&gt; to construct neural network components, such as artificial neurons and associative memories,
&gt; using a DNA-based system (Qian2011Neural pages 1-2, Qian2011Neural pages 15-16, Qian2011Neural pages 54-56).
&gt; This research includes the implementation of a 3-bit XOR gate and a four-neuron Hopfield associative memory,
&gt; showcasing the potential of DNA for neural network computation.
&gt; Additionally, the application of deep learning techniques to genomics,
&gt; which involves computing with DNA sequences, is well-documented.
&gt; Studies have applied convolutional neural networks (CNNs) to predict genomic features such as
&gt; transcription factor binding and DNA accessibility (Eraslan2019Deep pages 4-5, Eraslan2019Deep pages 5-6).
&gt; These models leverage DNA sequences as input data,
&gt; effectively using neural networks to compute with DNA.
&gt; While the provided excerpts do not explicitly mention protein-based neural network computation,
&gt; they do highlight the use of neural networks in tasks related to protein sequences,
&gt; such as predicting DNA-protein binding (Zeng2016Convolutional pages 1-2).
&gt; However, the primary focus remains on DNA-based computation.

## What is PaperQA2

PaperQA2 is engineered to be the best agentic RAG model for working with scientific papers.
Here are some features:

- A simple interface to get good answers with grounded responses containing in-text citations.
- State-of-the-art implementation including document metadata-awareness
  in embeddings and LLM-based re-ranking and contextual summarization (RCS).
- Support for agentic RAG, where a language agent can iteratively refine queries and answers.
- Automatic redundant fetching of paper metadata,
  including citation and journal quality data from multiple providers.
- A usable full-text search engine for a local repository of PDF/text files.
- A robust interface for customization, with default support for all [LiteLLM][LiteLLM providers] models.

[LiteLLM providers]: https://docs.litellm.ai/docs/providers
[LiteLLM general docs]: https://docs.litellm.ai/docs/

By default, it uses [OpenAI embeddings](https://platform.openai.com/docs/guides/embeddings)
and [models](https://platform.openai.com/docs/models) with a Numpy vector DB to embed and search documents.
However, you can easily use other closed-source, open-source models or embeddings (see details below).

PaperQA2 depends on some awesome libraries/APIs that make our repo possible.
Here are some in no particular order:

1. [Semantic Scholar](https://www.semanticscholar.org/)
2. [Crossref](https://www.crossref.org/)
3. [Unpaywall](https://unpaywall.org/)
4. [Pydantic](https://docs.pydantic.dev/latest/)
5. [tantivy](https://github.com/quickwit-oss/tantivy)
6. [LiteLLM][LiteLLM general docs]
7. [pybtex](https://pybtex.org/)

### PaperQA2 vs PaperQA

We&#039;ve been working hard on fundamental upgrades for a while
and mostly followed [SemVer](https://semver.org/), until [December 2025](#paperqa2-goes-calver-in-december-2025).
Meaning we&#039;ve incremented the major version number on each breaking change.
This brings us to the current major version number v5.
So why call is the repo now called PaperQA2?
We wanted to remark on the fact though that we&#039;ve
exceeded human performance on [many important metrics](https://paper.wikicrow.ai).
So we arbitrarily call version 5 and onward PaperQA2,
and versions before it as PaperQA1 to denote the significant change in performance.
We recognize that we are challenged at naming and counting at FutureHouse,
so we reserve the right at any time to arbitrarily change the name to PaperCrow.

### PaperQA2 Goes CalVer in December 2025

Prior to December 2025 we used [semantic versioning](https://semver.org/).
This eventually led to confusion in two ways:

1. Developers: should we major version bump based on
   settings or fundamental system capabilities?
   What if a bug fix requires breaking changes to the agent&#039;s behaviors?
2. Speaking: should one use terminology from our publications
   (e.g. [PaperQA1](https://arxiv.org/abs/2312.07559),
   [PaperQA2](https://arxiv.org/abs/2409.13740))
   or the Git tags (e.g. v5) from this repo/package?
   When someone says &quot;PaperQA&quot; -- what version do they mean?

To resolve these confusions, in December 2025,
we moved to [calendar versioning](https://calver.org/).
The developer burden is diminished because
we&#039;re basically removing guarantees of backwards compatibility across releases
(as CalVer is [ZeroVer](https://0ver.org/) bound to dates).
It solves the &quot;speaking&quot; issue because Git tags are now
quite different from publication terminology (e.g. PaperQA2 vs `v2025.12.17`).
When someone says &quot;PaperQA&quot; it will just refer to the system,
not a particular snapshot of agentic behaviors.
When someone says &quot;PaperQA2&quot; it will refer to `paper-qa&gt;=5`,
which applies to both SemVer tags `v5.0.0` and the new CalVer tags `v2025.12.17`.

This switch is backwards compatible for version 5&#039;s SemVer,
as the year 2025 is strictly greater than major version 5.

### What&#039;s New in Version 5 (aka PaperQA2)?

Version 5 added:

- A CLI `pqa`
- Agentic workflows invoking tools for
  paper search, gathering evidence, and generating an answer
- Removed much of the statefulness from the `Docs` object
- A migration to LiteLLM for compatibility with many LLM providers
  as well as centralized rate limits and cost tracking
- A bundled set of configurations (read [this section here](#bundled-settings)))
  containing known-good hyperparameters

Note that `Docs` objects pickled from prior versions of `PaperQA` are incompatible with version 5,
and will need to be rebuilt.
Also, our minimum Python version was increased to Python 3.11.

### What&#039;s New in December 2025?

The last four months since version `5.29.1` have seen many changes:

- New modalities: tables, figures, non-English languages, math equations
- More and better readers
  - Two new _model-based_ PDF readers: [Docling](packages/paper-qa-docling)
    and [Nvidia nemotron-parse](packages/paper-qa-nemotron)
  - All PDF readers now can parse images and tables, report page numbers,
    support DPI
  - A reader for Microsoft Office data types
- Multimodal contextual summarization
  - Media objects are also passed to the `summary_llm` during creation
  - Media objects&#039; embedding space is enhanced using an `enrichment_llm` prompt
- Simpler and performant HTTP stack
  - Consolidation from `aiohttp` and `httpx` to just `httpx`
  - Integration with [`httpx-aiohttp`](https://github.com/karpetrosyan/httpx-aiohttp) for performance
- `Context` relevance is simplified and some assumptions were removed
- Many minor features such as
  retrying `Context` creation upon invalid JSON,
  compatibility with fall 2025&#039;s frontier LLMs,
  and improved prompt templates
- Multiple fixes in metadata processing via Semantic Scholar and OpenAlex,
  and metadata processing
  (e.g. incorrectly inferring identical document IDs for main text and SI)
- Completed the deprecations accrued over the past year

### PaperQA2 Algorithm

To understand PaperQA2, let&#039;s start with the pieces of the underlying algorithm.
The default workflow of PaperQA2 is as follows:

| Phase                  | PaperQA2 Actions                                                          |
| ---------------------- | ------------------------------------------------------------------------- |
| **1. Paper Search**    | - Get candidate papers from LLM-generated keyword query                   |
|                        | - Chunk, embed, and add candidate papers to state                         |
| **2. Gather Evidence** | - Embed query into vector                                                 |
|                        | - Rank top _k_ document chunks in current state                           |
|                        | - Create scored summary of each chunk in the context of the current query |
|                        | - Use LLM to re-score and select most relevant summaries                  |
| **3. Generate Answer** | - Put best summaries into prompt with context                             |
|                        | - Generate answer with prompt                                             |

The tools can be invoked in any order by a language agent.
For example, an LLM agent might do a narrow and broad search,
or using different phrasing for the gather evidence step from the generate answer step.

## Installation

For a non-development setup,
install PaperQA2 (aka version 5) from [PyPI](https://pypi.org/project/paper-qa/).
Note version 5 requires Python 3.11+.

```bash
pip install paper-qa&gt;=5
```

For development setup,
please refer to the [CONTRIBUTING.md](CONTRIBUTING.md) file.

PaperQA2 uses an LLM to operate,
so you&#039;ll need to either set an appropriate [API key environment variable][LiteLLM providers]
(i.e. `export OPENAI_API_KEY=sk-...`)
or set up an open source LLM server (i.e. using [llamafile](https://github.com/Mozilla-Ocho/llamafile).
Any LiteLLM compatible model can be configured to use with PaperQA2.

If you need to index a large set of papers (100+),
you will likely want an API key for both
[Crossref](https://www.crossref.org/documentation/metadata-plus/metadata-plus-keys/)
and [Semantic Scholar](https://www.semanticscholar.org/product/api#api-key),
which will allow you to avoid hitting public rate limits using these metadata services.
Those can be exported as `CROSSREF_API_KEY` and `SEMANTIC_SCHOLAR_API_KEY` variables.

## CLI Usage

The fastest way to test PaperQA2 is via the CLI. First navigate to a directory with some papers and use the `pqa` cli:

```bash
pqa ask &#039;What is PaperQA2?&#039;
```

You will see PaperQA2 index your local PDF files,
gathering the necessary metadata for each of them
(using [Crossref](https://www.crossref.org/) and [Semantic Scholar](https://www.semanticscholar.org/)),
search over that index, then break the files into chunked evidence contexts,
rank them, and ultimately generate an answer.
The next time this directory is queried,
your index will already be built (save for any differences detected, like new added papers),
so it will skip the indexing and chunking steps.

All prior answers will be indexed and stored,
you can view them by querying via the `search` subcommand,
or access them yourself in your `PQA_HOME` directory,
which defaults to `~/.pqa/`.

```bash
pqa -i &#039;answers&#039; search &#039;ranking and contextual summarization&#039;
```

PaperQA2 is highly configurable, when running from the command line,
`pqa --help` shows all options and short descriptions.
For example to run with a higher temperature:

```bash
pqa --temperature 0.5 ask &#039;What is PaperQA2?&#039;
```

You can view all settings with `pqa view`.
Another useful thing is to change to other templated settings - for example
`fast` is a setting that answers more quickly
and you can see it with `pqa -s fast view`

Maybe you have some new settings you want to save? You can do that with

```bash
pqa -s my_new_settings --temperature 0.5 --llm foo-bar-5 save
```

and then you can use it with

```bash
pqa -s my_new_settings ask &#039;What is PaperQA2?&#039;
```

If you run `pqa` with a command which requires a new indexing,
say if you change the default chunk_size,
a new index will automatically be created for you.

```bash
pqa --parsing.chunk_size 5000 ask &#039;What is PaperQA2?&#039;
```

You can also use `pqa` to do full-text search with use of LLMs view the search command.
For example, let&#039;s save the index from a directory and give it a name:

```bash
pqa -i nanomaterials index
```

Now I can search for papers about thermoelectrics:

```bash
pqa -i nanomaterials search thermoelectrics
```

or I can use the normal ask

```bash
pqa -i nanomaterials ask &#039;Are there nm scale features in thermoelectric materials?&#039;
```

Both the CLI and module have pre-configured settings based on prior performance and our publications,
they can be invoked as follows:

```bash
pqa --settings &lt;setting name&gt; \
    ask &#039;Are there nm scale features in thermoelectric materials?&#039;
```

### Bundled Settings

Inside [`src/paperqa/configs`](src/paperqa/configs) we bundle known useful settings:

| Setting Name | Description                                                                                                                  |
| ------------ | ---------------------------------------------------------------------------------------------------------------------------- |
| high_quality | Highly performant, relatively expensive (due to having `evidence_k` = 15) query using a `ToolSelector` agent.                |
| fast         | Setting to get answers cheaply and quickly.                                                                                  |
| wikicrow     | Setting to emulate the Wikipedia article writing used in our WikiCrow publication.                                           |
| contracrow   | Setting to find contradictions in papers, your query should be a claim that needs to be flagged as a contradiction (or not). |
| debug        | Setting useful solely for debugging, but not in any actual application beyond debugging.                                     |
| tier1_limits | Settings that match OpenAI rate limits for each tier, you can use `tier&lt;1-5&gt;_limits` to specify the tier.                    |

### Rate Limits

If you are hitting rate limits, say with the OpenAI Tier 1 plan, you can add them into PaperQA2.
For each OpenAI tier, a pre-built setting exists to limit usage.

```bash
pqa --settings &#039;tier1_limits&#039; ask &#039;What is PaperQA2?&#039;
```

This will limit your system to use the [tier1_limits](src/paperqa/configs/tier1_limits.json),
and slow down your queries to accommodate.

You can also specify them manually with any rate limit string that matches the specification in
the [limits](https://limits.readthedocs.io/en/stable/quickstart.html#rate-limit-string-notation) module:

```bash
pqa --summary_llm_config &#039;{&quot;rate_limit&quot;: {&quot;gpt-4o-2024-11-20&quot;: &quot;30000 per 1 minute&quot;}}&#039; \
    ask &#039;What is PaperQA2?&#039;
```

Or by adding into a `Settings` object, if calling imperatively:

```python
from paperqa import Settings, ask

answer_response = ask(
    &quot;What is PaperQA2?&quot;,
    settings=Settings(
        llm_config={&quot;rate_limit&quot;: {&quot;gpt-4o-2024-11-20&quot;: &quot;30000 per 1 minute&quot;}},
        summary_llm_config={&quot;rate_limit&quot;: {&quot;gpt-4o-2024-11-20&quot;: &quot;30000 per 1 minute&quot;}},
    ),
)
```

## Library Usage

PaperQA2&#039;s full workflow can be accessed via Python directly:

```python
from paperqa import Settings, ask

answer_response = ask(
    &quot;What is PaperQA2?&quot;,
    settings=Settings(temperature=0.5, paper_directory=&quot;my_papers&quot;),
)
```

Please see our [installation docs](#installation) for how to install the package from PyPI.

### Agentic Adding/Querying Documents

The answer object has the following attributes:
`formatted_answer`, `answer` (answer alone), `question` , and `context` (the summaries of passages found for answer).
`ask` will use the `SearchPapers` tool, which will query a local index of files,
you can specify this location via the `Settings` object:

```python
from paperqa import Settings, ask

answer_response = ask(
    &quot;What is PaperQA2?&quot;,
    settings=Settings(
        temperature=0.5, agent={&quot;index&quot;: {&quot;paper_directory&quot;: &quot;my_papers&quot;}}
    ),
)
```

`ask` is just a convenience wrapper around the real entrypoint,
which can be accessed if you&#039;d like to run concurrent asynchronous workloads:

```python
from paperqa import Settings, agent_query

answer_response = await agent_query(
    query=&quot;What is PaperQA2?&quot;,
    settings=Settings(
        temperature=0.5, agent={&quot;index&quot;: {&quot;paper_directory&quot;: &quot;my_papers&quot;}}
    ),
)
```

The default agent will use an LLM based agent,
but you can also specify a `&quot;fake&quot;` agent to use a hard coded call path of
search -&gt; gather evidence -&gt; answer to reduce token usage.

### Manual (No Agent) Adding/Querying Documents

Normally via agent execution, the agent invokes the search tool,
which adds documents to the `Docs` object for you behind the scenes.
However, if you prefer fine-grained control,
you can directly interact with the `Docs` object.

Note that manually adding and querying `Docs` does not impact performance.
It just removes the automation associated with a

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>