<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for python - Python Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for python.</description>
        <lastBuildDate>Wed, 10 Dec 2025 00:04:32 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[microsoft/VibeVoice]]></title>
            <link>https://github.com/microsoft/VibeVoice</link>
            <guid>https://github.com/microsoft/VibeVoice</guid>
            <pubDate>Wed, 10 Dec 2025 00:04:32 GMT</pubDate>
            <description><![CDATA[Open-Source Frontier Voice AI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/VibeVoice">microsoft/VibeVoice</a></h1>
            <p>Open-Source Frontier Voice AI</p>
            <p>Language: Python</p>
            <p>Stars: 16,264</p>
            <p>Forks: 1,792</p>
            <p>Stars today: 2,638 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

## ğŸ™ï¸ VibeVoice: Open-Source Frontier Voice AI
[![Project Page](https://img.shields.io/badge/Project-Page-blue?logo=microsoft)](https://microsoft.github.io/VibeVoice)
[![Hugging Face](https://img.shields.io/badge/HuggingFace-Collection-orange?logo=huggingface)](https://huggingface.co/collections/microsoft/vibevoice-68a2ef24a875c44be47b034f)
[![Technical Report](https://img.shields.io/badge/Technical-Report-red?logo=adobeacrobatreader)](https://arxiv.org/pdf/2508.19205)


&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt;
&lt;picture&gt;
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;Figures/VibeVoice_logo_white.png&quot;&gt;
  &lt;img src=&quot;Figures/VibeVoice_logo.png&quot; alt=&quot;VibeVoice Logo&quot; width=&quot;300&quot;&gt;
&lt;/picture&gt;
&lt;/div&gt;

&lt;div align=&quot;left&quot;&gt;

&lt;h3&gt;ğŸ“° News&lt;/h3&gt;

&lt;img src=&quot;https://img.shields.io/badge/Status-New-brightgreen?style=flat&quot; alt=&quot;New&quot; /&gt;
&lt;img src=&quot;https://img.shields.io/badge/Feature-Realtime_TTS-blue?style=flat&amp;logo=soundcharts&quot; alt=&quot;Realtime TTS&quot; /&gt;

&lt;strong&gt;2025-12-03: ğŸ“£ We open-sourced &lt;a href=&quot;docs/vibevoice-realtime-0.5b.md&quot;&gt;&lt;strong&gt;VibeVoiceâ€‘Realtimeâ€‘0.5B&lt;/strong&gt;&lt;/a&gt;, a realâ€‘time textâ€‘toâ€‘speech model that supports streaming text input and robust long-form speech generation. Try it on [Colab](https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb).&lt;/strong&gt;

&lt;strong&gt;2025-12-09: ğŸ“£ Weâ€™ve added experimental speakers in nine languages (DE, FR, IT, JP, KR, NL, PL, PT, ES) for explorationâ€”welcome to try them out and share your feedback.&lt;/strong&gt;

To mitigate deepfake risks and ensure low latency for the first speech chunk, voice prompts are provided in an embedded format. For users requiring voice customization, please reach out to our team. We will also be expanding the range of available speakers.
&lt;br&gt;

https://github.com/user-attachments/assets/0901d274-f6ae-46ef-a0fd-3c4fba4f76dc

&gt; (Launch your own realtime demo via the websocket example in [Usage](docs/vibevoice-realtime-0.5b.md#usage-1-launch-real-time-websocket-demo)).

&lt;/div&gt;

2025-09-05: VibeVoice is an open-source research framework intended to advance collaboration in the speech synthesis community. After release, we discovered instances where the tool was used in ways inconsistent with the stated intent. Since responsible use of AI is one of Microsoftâ€™s guiding principles, we have disabled this repo until we are confident that out-of-scope use is no longer possible.


### Overview

VibeVoice is a novel framework designed for generating **expressive**, **long-form**, **multi-speaker** conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking.

VibeVoice currently includes two model variants:

- **Long-form multi-speaker model**: Synthesizes conversational/single-speaker speech up to **90 minutes** with up to **4 distinct speakers**, surpassing the typical 1â€“2 speaker limits of many prior models.
- **[Realtime streaming TTS model](docs/vibevoice-realtime-0.5b.md)**: Produces initial audible speech in ~**300 ms** and supports **streaming text input** for single-speaker **real-time** speech generation; designed for low-latency generation.

A core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a [next-token diffusion](https://arxiv.org/abs/2412.08635) framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.


&lt;p align=&quot;left&quot;&gt;
  &lt;img src=&quot;Figures/MOS-preference.png&quot; alt=&quot;MOS Preference Results&quot; height=&quot;260px&quot;&gt;
  &lt;img src=&quot;Figures/VibeVoice.jpg&quot; alt=&quot;VibeVoice Overview&quot; height=&quot;250px&quot; style=&quot;margin-right: 10px;&quot;&gt;
&lt;/p&gt;


### ğŸµ Demo Examples


**Video Demo**

We produced this video with [Wan2.2](https://github.com/Wan-Video/Wan2.2). We sincerely appreciate the Wan-Video team for their great work.

**English**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/0967027c-141e-4909-bec8-091558b1b784

&lt;/div&gt;


**Chinese**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/322280b7-3093-4c67-86e3-10be4746c88f

&lt;/div&gt;

**Cross-Lingual**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/838d8ad9-a201-4dde-bb45-8cd3f59ce722

&lt;/div&gt;

**Spontaneous Singing**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/6f27a8a5-0c60-4f57-87f3-7dea2e11c730

&lt;/div&gt;


**Long Conversation with 4 people**
&lt;div align=&quot;center&quot;&gt;

https://github.com/user-attachments/assets/a357c4b6-9768-495c-a576-1618f6275727

&lt;/div&gt;

For more examples, see the [Project Page](https://microsoft.github.io/VibeVoice).



## Risks and limitations

While efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release).
Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.

English and Chinese only: Transcripts in languages other than English or Chinese may result in unexpected audio outputs.

Non-Speech Audio: The model focuses solely on speech synthesis and does not handle background noise, music, or other sound effects.

Overlapping Speech: The current model does not explicitly model or generate overlapping speech segments in conversations.

We do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.

## Star History

![Star History Chart](https://api.star-history.com/svg?repos=Microsoft/vibevoice&amp;type=date&amp;legend=top-left)</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[NVIDIA/cutile-python]]></title>
            <link>https://github.com/NVIDIA/cutile-python</link>
            <guid>https://github.com/NVIDIA/cutile-python</guid>
            <pubDate>Wed, 10 Dec 2025 00:04:31 GMT</pubDate>
            <description><![CDATA[cuTile is a programming model for writing parallel kernels for NVIDIA GPUs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/NVIDIA/cutile-python">NVIDIA/cutile-python</a></h1>
            <p>cuTile is a programming model for writing parallel kernels for NVIDIA GPUs</p>
            <p>Language: Python</p>
            <p>Stars: 1,193</p>
            <p>Forks: 57</p>
            <p>Stars today: 178 stars today</p>
            <h2>README</h2><pre>&lt;!--- SPDX-FileCopyrightText: Copyright (c) &lt;2025&gt; NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved. --&gt;
&lt;!--- SPDX-License-Identifier: Apache-2.0 --&gt;

cuTile Python
=============

cuTile Python is a programming language for NVIDIA GPUs. The official documentation can be found
on [docs.nvidia.com](https://docs.nvidia.com/cuda/cutile-python),
or built from source located in the [docs](docs/) folder.

Installing from PyPI
--------------------
cuTile Python is published on [PyPI](https://pypi.org/) under the
[cuda-tile](https://pypi.org/project/cuda-tile/) package name and can be installed with `pip`:
```
pip install cuda-tile
```
Currently, the [CUDA Toolkit 13.1+](https://developer.nvidia.com/cuda-downloads) is required
and needs to be installed separately.

Building from Source
--------------------
cuTile is written mostly in Python, but includes a C++ extension which needs to be built.
You will need:
- A C++17-capable compiler, such as GNU C++ or MSVC;
- CMake 3.18+;
- GNU Make on Linux or msbuild on Windows;
- Python 3.10+ with development headers (`venv` module is recommended but optional);
- [CUDA Toolkit 13.1+](https://developer.nvidia.com/cuda-downloads)

On an Ubuntu system, the first four dependencies can be installed with APT:
```
sudo apt-get update &amp;&amp; sudo apt-get install build-essential cmake python3-dev python3-venv
```

The CMakeLists.txt script will also automatically download
the [DLPack](https://github.com/dmlc/dlpack) dependency from GitHub.
If you wish to disable this behavior and provide your own copy of DLPack,
set the `CUDA_TILE_CMAKE_DLPACK_PATH` environment variable to a local path
to the DLPack source tree.

Unless you are already using a Python virtual environment, it is recommended to create one
in order to avoid installing cuTile globally:

```
python3 -m venv env
source env/bin/activate
```

Once the build dependencies are in place, the simplest way to build cuTile is to install it
in editable mode by running the following command in the source root directory:

```
pip install -e .
```

This will create the `build` directory and invoke the CMake-based build process.
In editable mode, the compiled extension module will be placed in the build directory,
and then a symbolic link to it will be created in the source directory.
This makes sure that the `pip install -e .` command above is needed only once, and recompiling
the extension after making changes to the C++ code can be done with `make -C build`
which is much faster. This logic is defined in [setup.py](./setup.py).

Running Tests
-------------
cuTile uses the [pytest](https://pytest.org) framework for testing.
Tests have extra dependencies, such as PyTorch, which can be installed with
```
pip install -r test/requirements.txt
```

The tests are located in the [test/](test/) directory. To run a specific test file,
for example `test_copy.py`, use the following command:
```
pytest test/test_copy.py
```

Copyright and License Information
---------------------------------
Copyright Â© 2025 NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved.

cuTile-Python is under Apache 2.0 license. See the [LICENSES](LICENSES/) folder for the full license text.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[google/adk-samples]]></title>
            <link>https://github.com/google/adk-samples</link>
            <guid>https://github.com/google/adk-samples</guid>
            <pubDate>Wed, 10 Dec 2025 00:04:30 GMT</pubDate>
            <description><![CDATA[A collection of sample agents built with Agent Development Kit (ADK)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/google/adk-samples">google/adk-samples</a></h1>
            <p>A collection of sample agents built with Agent Development Kit (ADK)</p>
            <p>Language: Python</p>
            <p>Stars: 7,041</p>
            <p>Forks: 2,009</p>
            <p>Stars today: 76 stars today</p>
            <h2>README</h2><pre># Agent Development Kit (ADK) Samples

[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](LICENSE)

&lt;img src=&quot;https://github.com/google/adk-docs/blob/main/docs/assets/agent-development-kit.png&quot; alt=&quot;Agent Development Kit Logo&quot; width=&quot;150&quot;&gt;

Welcome to the ADK Sample Agents repository! This collection provides ready-to-use agents built on top of the [Agent Development Kit](https://google.github.io/adk-docs/), designed to accelerate your development process. These agents cover a range of common use cases and complexities, from simple conversational bots to complex multi-agent workflows.

## âœ¨ Getting Started
This repo contains ADK sample agents for **Python**, **Go** and **Java.** Navigate to the **[Python](python/)**, **[Go](go/)**, and **[Java](java/)** subfolders to see language-specific setup instructions, and learn more about the available sample agents.

&gt; [!IMPORTANT]
&gt; The agents in this repository are built using the **Agent Development Kit (ADK)**. Before you can run any of the samples, you must have the ADK installed. For instructions, please refer to the [**ADK Installation Guide**](https://google.github.io/adk-docs/get-started).

To learn more, check out the [ADK Documentation](https://google.github.io/adk-docs/), and the GitHub repositories for each language:
- [ADK Python](https://github.com/google/adk-python)
- [ADK Go](https://github.com/google/adk-go)
- [ADK Java](https://github.com/google/adk-java)

## ğŸŒ³ Repository Structure
```bash
â”œâ”€â”€ go
â”‚Â Â  â”œâ”€â”€ agents
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ llm-auditor
â”‚Â Â  â””â”€â”€ README.md
â”œâ”€â”€ java
â”‚Â Â  â”œâ”€â”€ agents
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ software-bug-assistant
â”‚Â Â  â”‚Â Â  â””â”€â”€ time-series-forecasting
â”‚Â Â  â””â”€â”€ README.md
â”œâ”€â”€ python
â”‚Â Â  â”œâ”€â”€ agents
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ academic-research
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ antom-payment
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ blog-writer
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ brand-search-optimization
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ camel
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ customer-service
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ data-engineering
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ data-science
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ financial-advisor
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ fomc-research
â”‚   â”‚   â”œâ”€â”€ gemini-fullstack
â”‚   â”‚   â”œâ”€â”€ deep-search
â”‚   â”‚   â”œâ”€â”€ google-trends-agent
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ image-scoring
â”‚   â”‚   â”œâ”€â”€ llm-auditor
â”‚   â”‚   â”œâ”€â”€ machine-learning-engineering
â”‚   â”‚   â”œâ”€â”€ marketing-agency
â”‚   â”‚   â”œâ”€â”€ medical-pre-authorization
â”‚   â”‚   â”œâ”€â”€ personalized-shopping
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ plumber-data-engineering-assistant
â”‚   â”‚   â”œâ”€â”€ RAG
â”‚   â”‚   â”œâ”€â”€ realtime-conversational-agent
â”‚   â”‚   â”œâ”€â”€ safety-plugins
â”‚   â”‚   â”œâ”€â”€ short-movie-agents
â”‚   â”‚   â”œâ”€â”€ software-bug-assistant
â”‚   â”‚   â”œâ”€â”€ travel-concierge
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â””â”€â”€ README.md
â””â”€â”€ README.md
```

## â„¹ï¸ Getting help

If you have any questions or if you found any problems with this repository, please report through [GitHub issues](https://github.com/google/adk-samples/issues).

## ğŸ¤ Contributing

We welcome contributions from the community! Whether it&#039;s bug reports, feature requests, documentation improvements, or code contributions, please see our [**Contributing Guidelines**](https://github.com/google/adk-samples/blob/main/CONTRIBUTING.md) to get started.

## ğŸ“„ License

This project is licensed under the Apache 2.0 License - see the [LICENSE](https://github.com/google/adk-samples/blob/main/LICENSE) file for details.

## Disclaimers

This is not an officially supported Google product. This project is not eligible for the [Google Open Source Software Vulnerability Rewards Program](https://bughunters.google.com/open-source-security).

This project is intended for demonstration purposes only. It is not intended for use in a production environment.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[zhu-xlab/GlobalBuildingAtlas]]></title>
            <link>https://github.com/zhu-xlab/GlobalBuildingAtlas</link>
            <guid>https://github.com/zhu-xlab/GlobalBuildingAtlas</guid>
            <pubDate>Wed, 10 Dec 2025 00:04:29 GMT</pubDate>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/zhu-xlab/GlobalBuildingAtlas">zhu-xlab/GlobalBuildingAtlas</a></h1>
            <p></p>
            <p>Language: Python</p>
            <p>Stars: 1,112</p>
            <p>Forks: 119</p>
            <p>Stars today: 143 stars today</p>
            <h2>README</h2><pre># GlobalBuildingAtlas

## Introduction
In this project, we provide the level of detail 1 (LoD1) data of buildings across the globe.

A overview of the dataset is illustrated bellow:

&lt;img src=&quot;figures/overview.png&quot; width=&quot;800&quot;&gt;


## Access to the Data
### Web Feature Service (WFS)
A WFS is provided so that one can access the data using other websites or GIS softwares such as QGIS and ArcGIS.

Url: `https://tubvsig-so2sat-vm1.srv.mwn.de/geoserver/ows?`

### Web Viewer
A web interface for viewing the data is available at: [website](https://tubvsig-so2sat-vm1.srv.mwn.de)

### Full Data Download
The full data can be downloaded from [mediaTUM](https://mediatum.ub.tum.de/1782307)

## Development Code
### Global Building Polygon Generation using Satellite Data (Sec. 4.3)
For codes related to building map extraction, regularization, polygonization, and simplification, i.e., generating building polygons from satellite images (Sec. 4.3.2, Sec. 4.3.3, and Sec. 4.3.4), please refer to `./im2bf`.

### Global Building Height Estimation (Sec. 4.4)
1. For codes related to monocular height estimation using HTC-DC Net (Sec. 4.4.2), please refer to `./im2bh`.
2. For codes related to the global inference and uncertainty quantification (Sec. 4.4.3), please refer to `./infer_height`

### Global LoD1 Building Model Generation (Sec. 4.5)
1. For codes related to quality-guided building polygon fusion (Sec. 4.5.1), please refer to `./fuse_bf`.
2. For codes related to LoD1 building model generation (Sec. 4.5.2), please refer to `./make_lod1`.

## Visualization Code
For codes to reproduce the plots in the manuscript, please refer to `./make_plots`.

## Code License
MIT with Commons Clause (no commercial use allowed). See [LICENSE](https://github.com/zhu-xlab/GlobalBuildingAtlas/blob/main/LICENSE).

## How to cite
If you find this dataset helpful in your work, please cite the following paper.
```
@Article{essd-17-6647-2025,
AUTHOR = {Zhu, X. X. and Chen, S. and Zhang, F. and Shi, Y. and Wang, Y.},
TITLE = {GlobalBuildingAtlas: an open global and complete dataset of building polygons, heights and LoD1 3D models},
JOURNAL = {Earth System Science Data},
VOLUME = {17},
YEAR = {2025},
NUMBER = {12},
PAGES = {6647--6668},
URL = {https://essd.copernicus.org/articles/17/6647/2025/},
DOI = {10.5194/essd-17-6647-2025}
}
```
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[666ghj/BettaFish]]></title>
            <link>https://github.com/666ghj/BettaFish</link>
            <guid>https://github.com/666ghj/BettaFish</guid>
            <pubDate>Wed, 10 Dec 2025 00:04:28 GMT</pubDate>
            <description><![CDATA[å¾®èˆ†ï¼šäººäººå¯ç”¨çš„å¤šAgentèˆ†æƒ…åˆ†æåŠ©æ‰‹ï¼Œæ‰“ç ´ä¿¡æ¯èŒ§æˆ¿ï¼Œè¿˜åŸèˆ†æƒ…åŸè²Œï¼Œé¢„æµ‹æœªæ¥èµ°å‘ï¼Œè¾…åŠ©å†³ç­–ï¼ä»0å®ç°ï¼Œä¸ä¾èµ–ä»»ä½•æ¡†æ¶ã€‚]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/666ghj/BettaFish">666ghj/BettaFish</a></h1>
            <p>å¾®èˆ†ï¼šäººäººå¯ç”¨çš„å¤šAgentèˆ†æƒ…åˆ†æåŠ©æ‰‹ï¼Œæ‰“ç ´ä¿¡æ¯èŒ§æˆ¿ï¼Œè¿˜åŸèˆ†æƒ…åŸè²Œï¼Œé¢„æµ‹æœªæ¥èµ°å‘ï¼Œè¾…åŠ©å†³ç­–ï¼ä»0å®ç°ï¼Œä¸ä¾èµ–ä»»ä½•æ¡†æ¶ã€‚</p>
            <p>Language: Python</p>
            <p>Stars: 31,994</p>
            <p>Forks: 6,148</p>
            <p>Stars today: 464 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

&lt;img src=&quot;static/image/logo_compressed.png&quot; alt=&quot;BettaFish Logo&quot; width=&quot;100%&quot;&gt;

&lt;a href=&quot;https://trendshift.io/repositories/15286&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/15286&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

&lt;a href=&quot;https://aihubmix.com/?aff=8Ds9&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_aihubmix.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;&amp;ensp;
&lt;a href=&quot;https://lioncc.ai/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_loincc.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;&amp;ensp;
&lt;a href=&quot;https://share.302.ai/P66Qe3&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_302ai.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;

&lt;a href=&quot;https://open.anspire.cn/?share_code=3E1FUOUH&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_anspire.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;50&quot;/&gt;&lt;/a&gt;

[![GitHub Stars](https://img.shields.io/github/stars/666ghj/BettaFish?style=flat-square)](https://github.com/666ghj/BettaFish/stargazers)
[![GitHub Watchers](https://img.shields.io/github/watchers/666ghj/BettaFish?style=flat-square)](https://github.com/666ghj/BettaFish/watchers)
[![GitHub Forks](https://img.shields.io/github/forks/666ghj/BettaFish?style=flat-square)](https://github.com/666ghj/BettaFish/network)
[![GitHub Issues](https://img.shields.io/github/issues/666ghj/BettaFish?style=flat-square)](https://github.com/666ghj/BettaFish/issues)
[![GitHub Pull Requests](https://img.shields.io/github/issues-pr/666ghj/BettaFish?style=flat-square)](https://github.com/666ghj/BettaFish/pulls)

[![GitHub License](https://img.shields.io/github/license/666ghj/BettaFish?style=flat-square)](https://github.com/666ghj/BettaFish/blob/main/LICENSE)
[![Version](https://img.shields.io/badge/version-v1.2.1-green.svg?style=flat-square)](https://github.com/666ghj/BettaFish)
[![Docker](https://img.shields.io/badge/Docker-Build-2496ED?style=flat-square&amp;logo=docker&amp;logoColor=white)](https://hub.docker.com/)



[English](./README-EN.md) | [ä¸­æ–‡æ–‡æ¡£](./README.md)

&lt;/div&gt;

## âš¡ é¡¹ç›®æ¦‚è¿°

â€œ**å¾®èˆ†**â€ æ˜¯ä¸€ä¸ªä»0å®ç°çš„åˆ›æ–°å‹ å¤šæ™ºèƒ½ä½“ èˆ†æƒ…åˆ†æç³»ç»Ÿï¼Œå¸®åŠ©å¤§å®¶ç ´é™¤ä¿¡æ¯èŒ§æˆ¿ï¼Œè¿˜åŸèˆ†æƒ…åŸè²Œï¼Œé¢„æµ‹æœªæ¥èµ°å‘ï¼Œè¾…åŠ©å†³ç­–ã€‚ç”¨æˆ·åªéœ€åƒèŠå¤©ä¸€æ ·æå‡ºåˆ†æéœ€æ±‚ï¼Œæ™ºèƒ½ä½“å¼€å§‹å…¨è‡ªåŠ¨åˆ†æ å›½å†…å¤–30+ä¸»æµç¤¾åª’ ä¸ æ•°ç™¾ä¸‡æ¡å¤§ä¼—è¯„è®ºã€‚

&gt; â€œå¾®èˆ†â€è°éŸ³â€œå¾®é±¼â€ï¼ŒBettaFishæ˜¯ä¸€ç§ä½“å‹å¾ˆå°ä½†éå¸¸å¥½æ–—ã€æ¼‚äº®çš„é±¼ï¼Œå®ƒè±¡å¾ç€â€œå°è€Œå¼ºå¤§ï¼Œä¸ç•æŒ‘æˆ˜â€

æŸ¥çœ‹ç³»ç»Ÿä»¥â€œæ­¦æ±‰å¤§å­¦èˆ†æƒ…â€ä¸ºä¾‹ï¼Œç”Ÿæˆçš„ç ”ç©¶æŠ¥å‘Šï¼š[æ­¦æ±‰å¤§å­¦å“ç‰Œå£°èª‰æ·±åº¦åˆ†ææŠ¥å‘Š](./final_reports/final_report__20250827_131630.html)

æŸ¥çœ‹ç³»ç»Ÿä»¥â€œæ­¦æ±‰å¤§å­¦èˆ†æƒ…â€ä¸ºä¾‹ï¼Œä¸€æ¬¡å®Œæ•´è¿è¡Œçš„è§†é¢‘ï¼š[è§†é¢‘-æ­¦æ±‰å¤§å­¦å“ç‰Œå£°èª‰æ·±åº¦åˆ†ææŠ¥å‘Š](https://www.bilibili.com/video/BV1TH1WBxEWN/?vd_source=da3512187e242ce17dceee4c537ec7a6#reply279744466833)

ä¸ä»…ä»…ä½“ç°åœ¨æŠ¥å‘Šè´¨é‡ä¸Šï¼Œç›¸æ¯”åŒç±»äº§å“ï¼Œæˆ‘ä»¬æ‹¥æœ‰ğŸš€å…­å¤§ä¼˜åŠ¿ï¼š

1. **AIé©±åŠ¨çš„å…¨åŸŸç›‘æ§**ï¼šAIçˆ¬è™«é›†ç¾¤7x24å°æ—¶ä¸é—´æ–­ä½œä¸šï¼Œå…¨é¢è¦†ç›–å¾®åšã€å°çº¢ä¹¦ã€æŠ–éŸ³ã€å¿«æ‰‹ç­‰10+å›½å†…å¤–å…³é”®ç¤¾åª’ã€‚ä¸ä»…å®æ—¶æ•è·çƒ­ç‚¹å†…å®¹ï¼Œæ›´èƒ½ä¸‹é’»è‡³æµ·é‡ç”¨æˆ·è¯„è®ºï¼Œè®©æ‚¨å¬åˆ°æœ€çœŸå®ã€æœ€å¹¿æ³›çš„å¤§ä¼—å£°éŸ³ã€‚

2. **è¶…è¶ŠLLMçš„å¤åˆåˆ†æå¼•æ“**ï¼šæˆ‘ä»¬ä¸ä»…ä¾èµ–è®¾è®¡çš„5ç±»ä¸“ä¸šAgentï¼Œæ›´èåˆäº†å¾®è°ƒæ¨¡å‹ã€ç»Ÿè®¡æ¨¡å‹ç­‰ä¸­é—´ä»¶ã€‚é€šè¿‡å¤šæ¨¡å‹ååŒå·¥ä½œï¼Œç¡®ä¿äº†åˆ†æç»“æœçš„æ·±åº¦ã€å‡†åº¦ä¸å¤šç»´è§†è§’ã€‚

3. **å¼ºå¤§çš„å¤šæ¨¡æ€èƒ½åŠ›**ï¼šçªç ´å›¾æ–‡é™åˆ¶ï¼Œèƒ½æ·±åº¦è§£ææŠ–éŸ³ã€å¿«æ‰‹ç­‰çŸ­è§†é¢‘å†…å®¹ï¼Œå¹¶ç²¾å‡†æå–ç°ä»£æœç´¢å¼•æ“ä¸­çš„å¤©æ°”ã€æ—¥å†ã€è‚¡ç¥¨ç­‰ç»“æ„åŒ–å¤šæ¨¡æ€ä¿¡æ¯å¡ç‰‡ï¼Œè®©æ‚¨å…¨é¢æŒæ¡èˆ†æƒ…åŠ¨æ€ã€‚

4. **Agentâ€œè®ºå›â€åä½œæœºåˆ¶**ï¼šä¸ºä¸åŒAgentèµ‹äºˆç‹¬ç‰¹çš„å·¥å…·é›†ä¸æ€ç»´æ¨¡å¼ï¼Œå¼•å…¥è¾©è®ºä¸»æŒäººæ¨¡å‹ï¼Œé€šè¿‡â€œè®ºå›â€æœºåˆ¶è¿›è¡Œé“¾å¼æ€ç»´ç¢°æ’ä¸è¾©è®ºã€‚è¿™ä¸ä»…é¿å…äº†å•ä¸€æ¨¡å‹çš„æ€ç»´å±€é™ä¸äº¤æµå¯¼è‡´çš„åŒè´¨åŒ–ï¼Œæ›´å‚¬ç”Ÿå‡ºæ›´é«˜è´¨é‡çš„é›†ä½“æ™ºèƒ½ä¸å†³ç­–æ”¯æŒã€‚

5. **å…¬ç§åŸŸæ•°æ®æ— ç¼èåˆ**ï¼šå¹³å°ä¸ä»…åˆ†æå…¬å¼€èˆ†æƒ…ï¼Œè¿˜æä¾›é«˜å®‰å…¨æ€§çš„æ¥å£ï¼Œæ”¯æŒæ‚¨å°†å†…éƒ¨ä¸šåŠ¡æ•°æ®åº“ä¸èˆ†æƒ…æ•°æ®æ— ç¼é›†æˆã€‚æ‰“é€šæ•°æ®å£å’ï¼Œä¸ºå‚ç›´ä¸šåŠ¡æä¾›â€œå¤–éƒ¨è¶‹åŠ¿+å†…éƒ¨æ´å¯Ÿâ€çš„å¼ºå¤§åˆ†æèƒ½åŠ›ã€‚

6. **è½»é‡åŒ–ä¸é«˜æ‰©å±•æ€§æ¡†æ¶**ï¼šåŸºäºçº¯Pythonæ¨¡å—åŒ–è®¾è®¡ï¼Œå®ç°è½»é‡åŒ–ã€ä¸€é”®å¼éƒ¨ç½²ã€‚ä»£ç ç»“æ„æ¸…æ™°ï¼Œå¼€å‘è€…å¯è½»æ¾é›†æˆè‡ªå®šä¹‰æ¨¡å‹ä¸ä¸šåŠ¡é€»è¾‘ï¼Œå®ç°å¹³å°çš„å¿«é€Ÿæ‰©å±•ä¸æ·±åº¦å®šåˆ¶ã€‚

**å§‹äºèˆ†æƒ…ï¼Œè€Œä¸æ­¢äºèˆ†æƒ…**ã€‚â€œå¾®èˆ†â€çš„ç›®æ ‡ï¼Œæ˜¯æˆä¸ºé©±åŠ¨ä¸€åˆ‡ä¸šåŠ¡åœºæ™¯çš„ç®€æ´é€šç”¨çš„æ•°æ®åˆ†æå¼•æ“ã€‚

&gt; ä¸¾ä¸ªä¾‹å­. ä½ åªéœ€ç®€å•ä¿®æ”¹Agentå·¥å…·é›†çš„apiå‚æ•°ä¸promptï¼Œå°±å¯ä»¥æŠŠä»–å˜æˆä¸€ä¸ªé‡‘èé¢†åŸŸçš„å¸‚åœºåˆ†æç³»ç»Ÿ
&gt;
&gt; é™„ä¸€ä¸ªæ¯”è¾ƒæ´»è·ƒçš„Lç«™é¡¹ç›®è®¨è®ºå¸–ï¼šhttps://linux.do/t/topic/1009280
&gt;
&gt; æŸ¥çœ‹Lç«™ä½¬å‹åšçš„æµ‹è¯„ [å¼€æºé¡¹ç›®(å¾®èˆ†)ä¸manus|minimax|ChatGPTå¯¹æ¯”](https://linux.do/t/topic/1148040)

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;static/image/system_schematic.png&quot; alt=&quot;banner&quot; width=&quot;800&quot;&gt;

å‘Šåˆ«ä¼ ç»Ÿçš„æ•°æ®çœ‹æ¿ï¼Œåœ¨â€œå¾®èˆ†â€ï¼Œä¸€åˆ‡ç”±ä¸€ä¸ªç®€å•çš„é—®é¢˜å¼€å§‹ï¼Œæ‚¨åªéœ€åƒå¯¹è¯ä¸€æ ·ï¼Œæå‡ºæ‚¨çš„åˆ†æéœ€æ±‚
&lt;/div&gt;

## ğŸª„ èµåŠ©å•†

LLMæ¨¡å‹APIèµåŠ©ï¼š&lt;a href=&quot;https://aihubmix.com/?aff=8Ds9&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_aihubmix.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;

&lt;details&gt;
&lt;summary&gt;æœ‰èµåŠ©LLMç®—åŠ›ç¦åˆ©ï¼ç¼–ç¨‹æ‹¼è½¦codecodex.aiï¼›ç¼–ç¨‹ç®—åŠ›VibeCodingAPI.aiï¼š&lt;/a&gt;&lt;span style=&quot;margin-left: 10px&quot;&gt;&lt;a href=&quot;https://codecodex.ai/&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_loincc.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;&lt;/summary&gt;

1. æ‰€ç½—é—¨åšå®¢LionCC.aiå·²æ›´æ–°ã€ŠBettaFish å¾®èˆ†ç³»ç»Ÿ - LionCC API éƒ¨ç½²é…ç½®å®Œå…¨æŒ‡å—ã€‹æ­£åœ¨äºŒå¼€ä¼˜åŒ–ä¸€é”®éƒ¨ç½²å’Œäº‘æœåŠ¡å™¨è°ƒç”¨æ–¹æ¡ˆã€‚
2. VibeCodingapi.aiç‹®å­ç®—åŠ›å¹³å°å·²ç»é€‚é…ã€ŠBettaFish å¾®èˆ†ç³»ç»Ÿã€‹æ‰€æœ‰LLMæ¨¡å‹å«claude codeå’Œopenai codexå’Œgemini cliç¼–ç¨‹å¼€å‘ä¸‰å·¨å¤´ç®—åŠ›ã€‚é¢åº¦ä»·æ ¼ï¼Œåªè¦ä¸€æ¯”ä¸€ï¼ˆ100å…ƒç­‰äº100ç¾åˆ€é¢åº¦ï¼‰
3. Codecodex.aiç‹®å­ç¼–ç¨‹æ‹¼è½¦ç³»ç»Ÿï¼Œå·²å®ç°æ— IPé—¨æ§›ç»•è¿‡claude codeå’Œopenai codexå°é”ï¼ŒæŒ‰å®˜æ–¹éƒ¨ç½²æ•™ç¨‹ååˆ‡æ¢BASE_URLè°ƒç”¨åœ°å€å’ŒToken keyè°ƒç”¨å¯†é’¥å³å¯ä½¿ç”¨æœ€å¼ºç¼–ç¨‹æ¨¡å‹ã€‚

æ‰€ç½—é—¨LionCCèµåŠ©BettaFish å¾®èˆ†ç¦åˆ©ï¼šæ‰“å¼€codecodex.aiç‹®å­ç¼–ç¨‹é¢‘é“æ‰«ç åŠ å…¥å¾®ä¿¡ç¤¾ç¾¤ï¼Œæ³¨å†ŒVibeCodingapi.aiç‹®å­ç®—åŠ›ï¼Œç»Ÿä¸€é€20åˆ€APIé¢åº¦ï¼ˆä»…é™å‰ä¸€åƒåï¼‰
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;æŒ‰ç”¨é‡ä»˜è´¹çš„ä¼ä¸šçº§AIèµ„æºå¹³å°ï¼Œæä¾›å¸‚åœºä¸Šå…¨é¢çš„AIæ¨¡å‹å’ŒAPIï¼Œä»¥åŠå¤šç§åœ¨çº¿AIåº”ç”¨ï¼š&lt;/a&gt;&lt;span style=&quot;margin-left: 10px&quot;&gt;&lt;a href=&quot;https://share.302.ai/P66Qe3&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_302ai.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;40&quot;/&gt;&lt;/a&gt;&lt;/summary&gt;
&lt;img src=&quot;static/image/banner_302ai_ch.jpg&quot; alt=&quot;banner&quot;&gt;302.AIæ˜¯ä¸€ä¸ªæŒ‰ç”¨é‡ä»˜è´¹çš„ä¼ä¸šçº§AIèµ„æºå¹³å°ï¼Œæä¾›å¸‚åœºä¸Šæœ€æ–°ã€æœ€å…¨é¢çš„AIæ¨¡å‹å’ŒAPIï¼Œä»¥åŠå¤šç§å¼€ç®±å³ç”¨çš„åœ¨çº¿AIåº”ç”¨ã€‚
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;AIè”ç½‘æœç´¢ã€æ–‡ä»¶è§£æåŠç½‘é¡µå†…å®¹æŠ“å–ç­‰æ™ºèƒ½ä½“æ ¸å¿ƒèƒ½åŠ›æä¾›å•†ï¼š&lt;/a&gt;&lt;span style=&quot;margin-left: 10px&quot;&gt;&lt;a href=&quot;https://open.anspire.cn/?share_code=3E1FUOUH&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;./static/image/logo_anspire.png&quot; alt=&quot;666ghj%2FBettaFish | Trendshift&quot; height=&quot;50&quot;/&gt;&lt;/a&gt;&lt;/summary&gt;
å®‰æ€æ´¾å¼€æ”¾å¹³å°(Anspire Open)æ˜¯é¢å‘æ™ºèƒ½ä½“æ—¶ä»£çš„é¢†å…ˆçš„åŸºç¡€è®¾æ–½æä¾›å•†ã€‚æˆ‘ä»¬ä¸ºå¼€å‘è€…æä¾›æ„å»ºå¼ºå¤§æ™ºèƒ½ä½“æ‰€éœ€çš„æ ¸å¿ƒèƒ½åŠ›æ ˆï¼Œç°å·²ä¸Šçº¿AIè”ç½‘æœç´¢ã€å¤šç‰ˆæœ¬ï¼Œæå…·ç«äº‰åŠ›çš„ä»·æ ¼ã€‘ã€æ–‡ä»¶è§£æã€é™å…ã€‘åŠç½‘é¡µå†…å®¹æŠ“å–ã€é™å…ã€‘ã€äº‘ç«¯æµè§ˆå™¨è‡ªåŠ¨åŒ–ï¼ˆAnspire Browser Agentï¼‰ã€å†…æµ‹ã€‘ã€å¤šè½®æ”¹å†™ç­‰æœåŠ¡ï¼ŒæŒç»­ä¸ºæ™ºèƒ½ä½“è¿æ¥å¹¶æ“ä½œå¤æ‚çš„æ•°å­—ä¸–ç•Œæä¾›åšå®åŸºç¡€ã€‚å¯æ— ç¼é›†æˆè‡³Difyã€Cozeã€å…ƒå™¨ç­‰ä¸»æµæ™ºèƒ½ä½“å¹³å°ã€‚é€šè¿‡é€æ˜ç‚¹æ•°è®¡è´¹ä½“ç³»ä¸æ¨¡å—åŒ–è®¾è®¡ï¼Œä¸ºä¼ä¸šæä¾›é«˜æ•ˆã€ä½æˆæœ¬çš„å®šåˆ¶åŒ–æ”¯æŒï¼ŒåŠ é€Ÿæ™ºèƒ½åŒ–å‡çº§è¿›ç¨‹ã€‚
&lt;/details&gt;

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### æ•´ä½“æ¶æ„å›¾

**Insight Agent** ç§æœ‰æ•°æ®åº“æŒ–æ˜ï¼šç§æœ‰èˆ†æƒ…æ•°æ®åº“æ·±åº¦åˆ†æAIä»£ç†

**Media Agent** å¤šæ¨¡æ€å†…å®¹åˆ†æï¼šå…·å¤‡å¼ºå¤§å¤šæ¨¡æ€èƒ½åŠ›çš„AIä»£ç†

**Query Agent** ç²¾å‡†ä¿¡æ¯æœç´¢ï¼šå…·å¤‡å›½å†…å¤–ç½‘é¡µæœç´¢èƒ½åŠ›çš„AIä»£ç†

**Report Agent** æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆï¼šå†…ç½®æ¨¡æ¿çš„å¤šè½®æŠ¥å‘Šç”ŸæˆAIä»£ç†

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;static/image/framework.png&quot; alt=&quot;banner&quot; width=&quot;800&quot;&gt;
&lt;/div&gt;

### ä¸€æ¬¡å®Œæ•´åˆ†ææµç¨‹

| æ­¥éª¤ | é˜¶æ®µåç§° | ä¸»è¦æ“ä½œ | å‚ä¸ç»„ä»¶ | å¾ªç¯ç‰¹æ€§ |
|------|----------|----------|----------|----------|
| 1 | ç”¨æˆ·æé—® | Flaskä¸»åº”ç”¨æ¥æ”¶æŸ¥è¯¢ | Flaskä¸»åº”ç”¨ | - |
| 2 | å¹¶è¡Œå¯åŠ¨ | ä¸‰ä¸ªAgentåŒæ—¶å¼€å§‹å·¥ä½œ | Query Agentã€Media Agentã€Insight Agent | - |
| 3 | åˆæ­¥åˆ†æ | å„Agentä½¿ç”¨ä¸“å±å·¥å…·è¿›è¡Œæ¦‚è§ˆæœç´¢ | å„Agent + ä¸“å±å·¥å…·é›† | - |
| 4 | ç­–ç•¥åˆ¶å®š | åŸºäºåˆæ­¥ç»“æœåˆ¶å®šåˆ†å—ç ”ç©¶ç­–ç•¥ | å„Agentå†…éƒ¨å†³ç­–æ¨¡å— | - |
| 5-N | **å¾ªç¯é˜¶æ®µ** | **è®ºå›åä½œ + æ·±åº¦ç ”ç©¶** | **ForumEngine + æ‰€æœ‰Agent** | **å¤šè½®å¾ªç¯** |
| 5.1 | æ·±åº¦ç ”ç©¶ | å„AgentåŸºäºè®ºå›ä¸»æŒäººå¼•å¯¼è¿›è¡Œä¸“é¡¹æœç´¢ | å„Agent + åæ€æœºåˆ¶ + è®ºå›å¼•å¯¼ | æ¯è½®å¾ªç¯ |
| 5.2 | è®ºå›åä½œ | ForumEngineç›‘æ§Agentå‘è¨€å¹¶ç”Ÿæˆä¸»æŒäººå¼•å¯¼ | ForumEngine + LLMä¸»æŒäºº | æ¯è½®å¾ªç¯ |
| 5.3 | äº¤æµèåˆ | å„Agentæ ¹æ®è®¨è®ºè°ƒæ•´ç ”ç©¶æ–¹å‘ | å„Agent + forum_readerå·¥å…· | æ¯è½®å¾ªç¯ |
| N+1 | ç»“æœæ•´åˆ | Report Agentæ”¶é›†æ‰€æœ‰åˆ†æç»“æœå’Œè®ºå›å†…å®¹ | Report Agent | - |
| N+2 | IRä¸­é—´è¡¨ç¤º | åŠ¨æ€é€‰æ‹©æ¨¡æ¿å’Œæ ·å¼ï¼Œå¤šè½®ç”Ÿæˆå…ƒæ•°æ®ï¼Œè£…è®¢ä¸ºIRä¸­é—´è¡¨ç¤º | Report Agent + æ¨¡æ¿å¼•æ“ | - |
| N+3 | æŠ¥å‘Šç”Ÿæˆ | åˆ†å—è¿›è¡Œè´¨é‡æ£€æµ‹ï¼ŒåŸºäºIRæ¸²æŸ“æˆäº¤äº’å¼ HTML æŠ¥å‘Š | Report Agent + è£…è®¢å¼•æ“ | - |

### é¡¹ç›®ä»£ç ç»“æ„æ ‘

```
BettaFish/
â”œâ”€â”€ QueryEngine/                            # å›½å†…å¤–æ–°é—»å¹¿åº¦æœç´¢Agent
â”‚   â”œâ”€â”€ agent.py                            # Agentä¸»é€»è¾‘ï¼Œåè°ƒæœç´¢ä¸åˆ†ææµç¨‹
â”‚   â”œâ”€â”€ llms/                               # LLMæ¥å£å°è£…
â”‚   â”œâ”€â”€ nodes/                              # å¤„ç†èŠ‚ç‚¹ï¼šæœç´¢ã€æ ¼å¼åŒ–ã€æ€»ç»“ç­‰
â”‚   â”œâ”€â”€ tools/                              # å›½å†…å¤–æ–°é—»æœç´¢å·¥å…·é›†
â”‚   â”œâ”€â”€ utils/                              # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ state/                              # çŠ¶æ€ç®¡ç†
â”‚   â”œâ”€â”€ prompts/                            # æç¤ºè¯æ¨¡æ¿
â”‚   â””â”€â”€ ...
â”œâ”€â”€ MediaEngine/                            # å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£Agent
â”‚   â”œâ”€â”€ agent.py                            # Agentä¸»é€»è¾‘ï¼Œå¤„ç†è§†é¢‘/å›¾ç‰‡ç­‰å¤šæ¨¡æ€å†…å®¹
â”‚   â”œâ”€â”€ llms/                               # LLMæ¥å£å°è£…
â”‚   â”œâ”€â”€ nodes/                              # å¤„ç†èŠ‚ç‚¹ï¼šæœç´¢ã€æ ¼å¼åŒ–ã€æ€»ç»“ç­‰
â”‚   â”œâ”€â”€ tools/                              # å¤šæ¨¡æ€æœç´¢å·¥å…·é›†
â”‚   â”œâ”€â”€ utils/                              # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ state/                              # çŠ¶æ€ç®¡ç†
â”‚   â”œâ”€â”€ prompts/                            # æç¤ºè¯æ¨¡æ¿
â”‚   â””â”€â”€ ...
â”œâ”€â”€ InsightEngine/                          # ç§æœ‰æ•°æ®åº“æŒ–æ˜Agent
â”‚   â”œâ”€â”€ agent.py                            # Agentä¸»é€»è¾‘ï¼Œåè°ƒæ•°æ®åº“æŸ¥è¯¢ä¸åˆ†æ
â”‚   â”œâ”€â”€ llms/                               # LLMæ¥å£å°è£…
â”‚   â”‚   â””â”€â”€ base.py                         # ç»Ÿä¸€çš„OpenAIå…¼å®¹å®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ nodes/                              # å¤„ç†èŠ‚ç‚¹ï¼šæœç´¢ã€æ ¼å¼åŒ–ã€æ€»ç»“ç­‰
â”‚   â”‚   â”œâ”€â”€ base_node.py                    # åŸºç¡€èŠ‚ç‚¹ç±»
â”‚   â”‚   â”œâ”€â”€ search_node.py                  # æœç´¢èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ formatting_node.py              # æ ¼å¼åŒ–èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ report_structure_node.py        # æŠ¥å‘Šç»“æ„èŠ‚ç‚¹
â”‚   â”‚   â””â”€â”€ summary_node.py                 # æ€»ç»“èŠ‚ç‚¹
â”‚   â”œâ”€â”€ tools/                              # æ•°æ®åº“æŸ¥è¯¢å’Œåˆ†æå·¥å…·é›†
â”‚   â”‚   â”œâ”€â”€ keyword_optimizer.py            # Qwenå…³é”®è¯ä¼˜åŒ–ä¸­é—´ä»¶
â”‚   â”‚   â”œâ”€â”€ search.py                       # æ•°æ®åº“æ“ä½œå·¥å…·é›†ï¼ˆè¯é¢˜æœç´¢ã€è¯„è®ºè·å–ç­‰ï¼‰
â”‚   â”‚   â””â”€â”€ sentiment_analyzer.py           # æƒ…æ„Ÿåˆ†æé›†æˆå·¥å…·
â”‚   â”œâ”€â”€ utils/                              # å·¥å…·å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ config.py                       # é…ç½®ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ db.py                           # SQLAlchemyå¼‚æ­¥å¼•æ“ä¸åªè¯»æŸ¥è¯¢å°è£…
â”‚   â”‚   â””â”€â”€ text_processing.py              # æ–‡æœ¬å¤„ç†å·¥å…·
â”‚   â”œâ”€â”€ state/                              # çŠ¶æ€ç®¡ç†
â”‚   â”‚   â””â”€â”€ state.py                        # AgentçŠ¶æ€å®šä¹‰
â”‚   â”œâ”€â”€ prompts/                            # æç¤ºè¯æ¨¡æ¿
â”‚   â”‚   â””â”€â”€ prompts.py                      # å„ç±»æç¤ºè¯
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ReportEngine/                           # å¤šè½®æŠ¥å‘Šç”ŸæˆAgent
â”‚   â”œâ”€â”€ agent.py                            # æ€»è°ƒåº¦å™¨ï¼šæ¨¡æ¿é€‰æ‹©â†’å¸ƒå±€â†’ç¯‡å¹…â†’ç« èŠ‚â†’æ¸²æŸ“
â”‚   â”œâ”€â”€ flask_interface.py                  # Flask/SSEå…¥å£ï¼Œç®¡ç†ä»»åŠ¡æ’é˜Ÿä¸æµå¼äº‹ä»¶
â”‚   â”œâ”€â”€ llms/                               # OpenAIå…¼å®¹LLMå°è£…
â”‚   â”‚   â””â”€â”€ base.py                         # ç»Ÿä¸€çš„æµå¼/é‡è¯•å®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ core/                               # æ ¸å¿ƒåŠŸèƒ½ï¼šæ¨¡æ¿è§£æã€ç« èŠ‚å­˜å‚¨ã€æ–‡æ¡£è£…è®¢
â”‚   â”‚   â”œâ”€â”€ template_parser.py              # Markdownæ¨¡æ¿åˆ‡ç‰‡ä¸slugç”Ÿæˆ
â”‚   â”‚   â”œâ”€â”€ chapter_storage.py              # ç« èŠ‚runç›®å½•ã€manifestä¸rawæµå†™å…¥
â”‚   â”‚   â””â”€â”€ stitcher.py                     # Document IRè£…è®¢å™¨ï¼Œè¡¥é½é”šç‚¹/å…ƒæ•°æ®
â”‚   â”œâ”€â”€ ir/                                 # æŠ¥å‘Šä¸­é—´è¡¨ç¤ºï¼ˆIRï¼‰å¥‘çº¦ä¸æ ¡éªŒ
â”‚   â”‚   â”œâ”€â”€ schema.py                       # å—/æ ‡è®°Schemaå¸¸é‡å®šä¹‰
â”‚   â”‚   â””â”€â”€ validator.py                    # ç« èŠ‚JSONç»“æ„æ ¡éªŒå™¨
â”‚   â”œâ”€â”€ nodes/                              # å…¨æµç¨‹æ¨ç†èŠ‚ç‚¹
â”‚   â”‚   â”œâ”€â”€ base_node.py                    # èŠ‚ç‚¹åŸºç±»+æ—¥å¿—/çŠ¶æ€é’©å­
â”‚   â”‚   â”œâ”€â”€ template_selection_node.py      # æ¨¡æ¿å€™é€‰æ”¶é›†ä¸LLMç­›é€‰
â”‚   â”‚   â”œâ”€â”€ document_layout_node.py         # æ ‡é¢˜/ç›®å½•/ä¸»é¢˜è®¾è®¡
â”‚   â”‚   â”œâ”€â”€ word_budget_node.py             # ç¯‡å¹…è§„åˆ’ä¸ç« èŠ‚æŒ‡ä»¤ç”Ÿæˆ
â”‚   â”‚   â””â”€â”€ chapter_generation_node.py      # ç« èŠ‚çº§JSONç”Ÿæˆ+æ ¡éªŒ
â”‚   â”œâ”€â”€ prompts/                            # æç¤ºè¯åº“ä¸Schemaè¯´æ˜
â”‚   â”‚   â””â”€â”€ prompts.py                      # æ¨¡æ¿é€‰æ‹©/å¸ƒå±€/ç¯‡å¹…/ç« èŠ‚æç¤ºè¯
â”‚   â”œâ”€â”€ renderers/                          # IRæ¸²æŸ“å™¨
â”‚   â”‚   â”œâ”€â”€ html_renderer.py                # Document IRâ†’äº¤äº’å¼HTML
â”‚   â”‚   â”œâ”€â”€ pdf_renderer.py                 # HTMLâ†’PDFå¯¼å‡ºï¼ˆWeasyPrintï¼‰
â”‚   â”‚   â”œâ”€â”€ pdf_layout_optimizer.py         # PDFå¸ƒå±€ä¼˜åŒ–å™¨
â”‚   â”‚   â””â”€â”€ chart_to_svg.py                 # å›¾è¡¨è½¬SVGå·¥å…·
â”‚   â”œâ”€â”€ state/                              # ä»»åŠ¡/å…ƒæ•°æ®çŠ¶æ€æ¨¡å‹
â”‚   â”‚   â””â”€â”€ state.py                        # ReportStateä¸åºåˆ—åŒ–å·¥å…·
â”‚   â”œâ”€â”€ utils/                              # é…ç½®ä¸è¾…åŠ©å·¥å…·
â”‚   â”‚   â”œâ”€â”€ config.py                       # Pydantic Settingsä¸æ‰“å°åŠ©æ‰‹
â”‚   â”‚   â”œâ”€â”€ dependency_check.py             # ä¾èµ–æ£€æŸ¥å·¥å…·
â”‚   â”‚   â”œâ”€â”€ json_parser.py                  # JSONè§£æå·¥å…·
â”‚   â”‚   â”œâ”€â”€ chart_validator.py              # å›¾è¡¨æ ¡éªŒå·¥å…·
â”‚   â”‚   â””â”€â”€ chart_repair_api.py             # å›¾è¡¨ä¿®å¤API
â”‚   â”œâ”€â”€ report_template/                    # Markdownæ¨¡æ¿åº“
â”‚   â”‚   â”œâ”€â”€ ä¼ä¸šå“ç‰Œå£°èª‰åˆ†ææŠ¥å‘Š.md
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ForumEngine/                            # è®ºå›å¼•æ“ï¼šAgentåä½œæœºåˆ¶
â”‚   â”œâ”€â”€ monitor.py                          # æ—¥å¿—ç›‘æ§å’Œè®ºå›ç®¡ç†æ ¸å¿ƒ
â”‚   â”œâ”€â”€ llm_host.py                         # è®ºå›ä¸»æŒäººLLMæ¨¡å—
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ MindSpider/                             # ç¤¾äº¤åª’ä½“çˆ¬è™«ç³»ç»Ÿ
â”‚   â”œâ”€â”€ main.py                             # çˆ¬è™«ä¸»ç¨‹åºå…¥å£
â”‚   â”œâ”€â”€ config.py                           # çˆ¬è™«é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ BroadTopicExtraction/               # è¯é¢˜æå–æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ main.py                         # è¯é¢˜æå–ä¸»ç¨‹åº
â”‚   â”‚   â”œâ”€â”€ database_manager.py             # æ•°æ®åº“ç®¡ç†å™¨
â”‚   â”‚   â”œâ”€â”€ get_today_news.py               # ä»Šæ—¥æ–°é—»è·å–
â”‚   â”‚   â””â”€â”€ topic_extractor.py              # è¯é¢˜æå–å™¨
â”‚   â”œâ”€â”€ DeepSentimentCrawling/              # æ·±åº¦èˆ†æƒ…çˆ¬å–æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ main.py                         # æ·±åº¦çˆ¬å–ä¸»ç¨‹åº
â”‚   â”‚   â”œâ”€â”€ keyword_manager.py              # å…³é”®è¯ç®¡ç†å™¨
â”‚   â”‚   â”œâ”€â”€ platform_crawler.py             # å¹³å°çˆ¬è™«ç®¡ç†
â”‚   â”‚   â””â”€â”€ MediaCrawler/                   # ç¤¾åª’çˆ¬è™«æ ¸å¿ƒ
â”‚   â”‚       â”œâ”€â”€ main.py
â”‚   â”‚       â”œâ”€â”€ config/                     # å„å¹³å°é…ç½®
â”‚   â”‚       â”œâ”€â”€ media_platform/             # å„å¹³å°çˆ¬è™«å®ç°
â”‚   â”‚       â””â”€â”€ ...
â”‚   â””â”€â”€ schema/                             # æ•°æ®åº“ç»“æ„å®šä¹‰
â”‚       â”œâ”€â”€ db_manager.py                   # æ•°æ®åº“ç®¡ç†å™¨
â”‚       â”œâ”€â”€ init_database.py                # æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬
â”‚       â”œâ”€â”€ mindspider_tables.sql           # æ•°æ®åº“è¡¨ç»“æ„SQL
â”‚       â”œâ”€â”€ models_bigdata.py               # å¤§è§„æ¨¡åª’ä½“èˆ†æƒ…è¡¨çš„SQLAlchemyæ˜ å°„
â”‚       â””â”€â”€ models_sa.py                    # DailyTopic/Taskç­‰æ‰©å±•è¡¨ORMæ¨¡å‹
â”œâ”€â”€ SentimentAnalysisModel/                 # æƒ…æ„Ÿåˆ†ææ¨¡å‹é›†åˆ
â”‚   â”œâ”€â”€ WeiboSentiment_Finetuned/           # å¾®è°ƒBERT/GPT-2æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ BertChinese-Lora/               # BERTä¸­æ–‡LoRAå¾®è°ƒ
â”‚   â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”‚   â”œâ”€â”€ predict.py
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â””â”€â”€ GPT2-Lora/                      # GPT-2 LoRAå¾®è°ƒ
â”‚   â”‚       â”œâ”€â”€ train.py
â”‚   â”‚       â”œâ”€â”€ predict.py
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”œâ”€â”€ WeiboMultilingualSentiment/         # å¤šè¯­è¨€æƒ…æ„Ÿåˆ†æ
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”œâ”€â”€ predict.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ WeiboSentiment_SmallQwen/           # å°å‚æ•°Qwen3å¾®è°ƒ
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”œâ”€â”€ predict_universal.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ WeiboSentiment_MachineLearning/     # ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•
â”‚       â”œâ”€â”€ train.py
â”‚       â”œâ”€â”€ predict.py
â”‚       â””â”€â”€ ...
â”œâ”€â”€ SingleEngineApp/                        # å•ç‹¬Agentçš„Streamlitåº”ç”¨
â”‚   â”œâ”€â”€ query_engine_streamlit_app.py       # QueryEngineç‹¬ç«‹åº”ç”¨
â”‚   â”œâ”€â”€ media_engine_streamlit_app.py       # MediaEngineç‹¬ç«‹åº”ç”¨
â”‚   â””â”€â”€ insight_engine_streamlit_app.py     # InsightEngineç‹¬ç«‹åº”ç”¨
â”œâ”€â”€ query_engine_streamlit_reports/         # QueryEngineå•åº”ç”¨è¿è¡Œè¾“å‡º
â”œâ”€â”€ media_engine_streamlit_reports/         # MediaEngineå•åº”ç”¨è¿è¡Œè¾“å‡º
â”œâ”€â”€ insight_engine_streamlit_reports/       # InsightEngineå•åº”ç”¨è¿è¡Œè¾“å‡º
â”œâ”€â”€ templates/                              # Flaskå‰ç«¯æ¨¡æ¿
â”‚   â””â”€â”€ index.html                          # ä¸»ç•Œé¢HTML
â”œâ”€â”€ static/                                 # é™æ€èµ„æº
â”‚   â””â”€â”€ image/                              # å›¾ç‰‡èµ„æº
â”‚       â”œâ”€â”€ logo_compressed.png
â”‚       â”œâ”€â”€ framework.png
â”‚       â””â”€â”€ ...
â”œâ”€â”€ logs/                                   # è¿è¡Œæ—¥å¿—ç›®å½•
â”œâ”€â”€ final_reports/                          # æœ€ç»ˆç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶
â”‚   â”œâ”€â”€ ir/                                 # æŠ¥å‘ŠIR JSONæ–‡ä»¶
â”‚   â””â”€â”€ *.html                              # æœ€ç»ˆHTMLæŠ¥å‘Š
â”œâ”€â”€ utils/                                  # é€šç”¨å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ forum_reader.py                     # Agenté—´è®ºå›é€šä¿¡å·¥å…·
â”‚   â”œâ”€â”€ github_issues.py                    # ç»Ÿä¸€ç”ŸæˆGitHub Issueé“¾æ¥ä¸é”™è¯¯æç¤º
â”‚   â””â”€â”€ retry_helper.py                     # ç½‘ç»œè¯·æ±‚é‡è¯•æœºåˆ¶å·¥å…·
â”œâ”€â”€ tests/                                  # å•å…ƒæµ‹è¯•ä¸é›†æˆæµ‹è¯•
â”‚   â”œâ”€â”€ run_tests.py                        # pytestå…¥å£è„šæœ¬
â”‚   â”œâ”€â”€ test_monitor.py                     # ForumEngineç›‘æ§å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ test_report_engine_sanitization.py  # ReportEngineå®‰å…¨æ€§æµ‹è¯•
â”‚   â””â”€â”€ ...
â”œâ”€â”€ app.py                                  # Flaskä¸»åº”ç”¨å…¥å£
â”œâ”€â”€ config.py                               # å…¨å±€é…ç½®æ–‡ä»¶
â”œâ”€â”€ .env.example                            # ç¯å¢ƒå˜é‡ç¤ºä¾‹æ–‡ä»¶
â”œâ”€â”€ docker-compose.yml                      # Dockerå¤šæœåŠ¡ç¼–æ’é…ç½®
â”œâ”€â”€ Dockerfile                              # Dockeré•œåƒæ„å»ºæ–‡ä»¶
â”œâ”€â”€ requirements.txt                        # Pythonä¾èµ–åŒ…æ¸…å•
â”œâ”€â”€ regenerate_latest_pdf.py                # PDFé‡æ–°ç”Ÿæˆå·¥å…·è„šæœ¬
â”œâ”€â”€ report_engine_only.py                   # Report Engineå‘½ä»¤è¡Œç‰ˆæœ¬
â”œâ”€â”€ README.md                               # ä¸­æ–‡è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ README-EN.md                            # è‹±æ–‡è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ CONTRIBUTING.md                         # ä¸­æ–‡è´¡çŒ®æŒ‡å—
â”œâ”€â”€ CONTRIBUTING-EN.md                      # è‹±æ–‡è´¡çŒ®æŒ‡å—
â””â”€â”€ LICENSE                                 # GPL-2.0å¼€æºè®¸å¯è¯
```

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆDockerï¼‰

### 1. å¯åŠ¨é¡¹ç›®

å¤åˆ¶ä¸€ä»½ `.env.example` æ–‡ä»¶ï¼Œå‘½åä¸º `.env` ï¼Œå¹¶æŒ‰éœ€é…ç½® `.env` æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤åœ¨åå°å¯åŠ¨æ‰€æœ‰æœåŠ¡ï¼š

```bash
docker compose up -d
```

&gt; **æ³¨ï¼šé•œåƒæ‹‰å–é€Ÿåº¦æ…¢**ï¼Œåœ¨åŸ `docker-compose.yml` æ–‡ä»¶ä¸­ï¼Œæˆ‘ä»¬å·²ç»é€šè¿‡**æ³¨é‡Š**çš„æ–¹å¼æä¾›äº†å¤‡ç”¨é•œåƒåœ°å€ä¾›æ‚¨æ›¿æ¢

### 2. é…ç½®è¯´æ˜

#### æ•°æ®åº“é…ç½®ï¼ˆPostgreSQLï¼‰

è¯·æŒ‰ç…§ä»¥ä¸‹å‚æ•°é…ç½®æ•°æ®åº“è¿æ¥ä¿¡æ¯ï¼Œä¹Ÿæ”¯æŒMysqlå¯è‡ªè¡Œä¿®æ”¹ï¼š

| é…ç½®é¡¹ | å¡«å†™å€¼ | è¯´æ˜ |
| :--- | :--- | :--- |
| `DB_HOST` | `db` | æ•°æ®åº“æœåŠ¡åç§° (å¯¹åº” `docker-compose.yml` ä¸­çš„æœåŠ¡å) |
| `DB_PORT` | `5432` | é»˜è®¤ PostgreSQL ç«¯å£ |
| `DB_USER` | `bettafish` | æ•°æ®åº“ç”¨æˆ·å |
| `DB_PASSWORD` | `bettafish` | æ•°æ®åº“å¯†ç  |
| `DB_NAME` | `bettafish` | æ•°æ®åº“åç§° |
| **å…¶ä»–** | **ä¿æŒé»˜è®¤** | æ•°æ®åº“è¿æ¥æ± ç­‰å…¶ä»–å‚æ•°è¯·ä¿æŒé»˜è®¤è®¾ç½®ã€‚ |

#### å¤§æ¨¡å‹é…ç½®

&gt; æˆ‘ä»¬æ‰€æœ‰ LLM è°ƒç”¨ä½¿ç”¨ OpenAI çš„ API æ¥å£æ ‡å‡†

åœ¨å®Œæˆæ•°æ®åº“é…ç½®åï¼Œè¯·æ­£å¸¸é…ç½®**æ‰€æœ‰å¤§æ¨¡å‹ç›¸å…³çš„å‚æ•°**ï¼Œç¡®ä¿ç³»ç»Ÿèƒ½å¤Ÿè¿æ¥åˆ°æ‚¨é€‰æ‹©çš„å¤§æ¨¡å‹æœåŠ¡ã€‚

å®Œæˆä¸Šè¿°æ‰€æœ‰é…ç½®å¹¶ä¿å­˜åï¼Œç³»ç»Ÿå³å¯æ­£å¸¸è¿è¡Œã€‚

## ğŸ”§ æºç å¯åŠ¨æŒ‡å—

&gt; å¦‚æœä½ æ˜¯åˆæ¬¡å­¦ä¹ ä¸€ä¸ªAgentç³»ç»Ÿçš„æ­å»ºï¼Œå¯ä»¥ä»ä¸€ä¸ªéå¸¸ç®€å•çš„demoå¼€å§‹ï¼š[Deep Search Agent Demo](https://github.com/666ghj/DeepSearchAgent-Demo)

### ç¯å¢ƒè¦æ±‚

- **æ“ä½œç³»ç»Ÿ**: Windowsã€Linuxã€MacOS
- **Pythonç‰ˆæœ¬**: 3.9+
- **Conda**: Anacondaæˆ–Miniconda
- **æ•°æ®åº“**: PostgreSQLï¼ˆæ¨èï¼‰æˆ–MySQL
- **å†…å­˜**: å»ºè®®2GBä»¥ä¸Š

### 1. åˆ›å»ºç¯å¢ƒ

#### å¦‚æœä½¿ç”¨Conda

```bash
# åˆ›å»ºcondaç¯å¢ƒ
conda create -n your_conda_name python=3.11
conda activate your_conda_name
```

#### å¦‚æœä½¿ç”¨uv

```bash
# åˆ›å»ºuvç¯å¢ƒ
uv venv --python 3.11 # åˆ›å»º3.11ç¯å¢ƒ
```

### 2. å®‰è£… PDF å¯¼å‡ºæ‰€éœ€ç³»ç»Ÿä¾èµ–ï¼ˆå¯é€‰ï¼‰

è¿™éƒ¨åˆ†æœ‰è¯¦ç»†çš„é…ç½®è¯´æ˜ï¼š[é…ç½®æ‰€éœ€ä¾èµ–](./static/Partial%20README%20for%20PDF%20Exporting/README.md)

### 3. å®‰è£…ä¾èµ–åŒ…

&gt; å¦‚æœè·³è¿‡äº†æ­¥éª¤2ï¼Œweasyprintåº“å¯èƒ½æ— æ³•å®‰è£…ï¼ŒPDFåŠŸèƒ½å¯èƒ½æ— æ³•æ­£å¸¸ä½¿ç”¨ã€‚

```bash
# åŸºç¡€ä¾èµ–å®‰è£…
pip install -r requirements.txt

# uvç‰ˆæœ¬å‘½ä»¤ï¼ˆæ›´å¿«é€Ÿå®‰è£…ï¼‰
uv pip install -r requirements.txt
# å¦‚æœä¸æƒ³ä½¿ç”¨æœ¬åœ°æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼ˆç®—åŠ›éœ€æ±‚å¾ˆå°ï¼Œé»˜è®¤å®‰è£…cpuç‰ˆæœ¬ï¼‰ï¼Œå¯ä»¥å°†è¯¥æ–‡ä»¶ä¸­çš„&quot;æœºå™¨å­¦ä¹ &quot;éƒ¨åˆ†æ³¨é‡Šæ‰å†æ‰§è¡ŒæŒ‡ä»¤
```

### 4. å®‰è£…Playwrightæµè§ˆå™¨é©±åŠ¨

```bash
# å®‰è£…æµè§ˆå™¨é©±åŠ¨ï¼ˆç”¨äºçˆ¬è™«åŠŸèƒ½ï¼‰
playwright install chromium
```

### 5. é…ç½®LLMä¸æ•°æ®åº“

å¤åˆ¶ä¸€ä»½é¡¹ç›®æ ¹ç›®å½• `.env.example` æ–‡ä»¶ï¼Œå‘½åä¸º `.env`

ç¼–è¾‘ `.env` æ–‡ä»¶ï¼Œå¡«å…¥æ‚¨çš„APIå¯†é’¥ï¼ˆæ‚¨ä¹Ÿå¯ä»¥é€‰æ‹©è‡ªå·±çš„æ¨¡å‹ã€æœç´¢ä»£ç†ï¼Œè¯¦æƒ…è§æ ¹ç›®å½•.env.exampleæ–‡ä»¶å†…æˆ–æ ¹ç›®å½•config.pyä¸­çš„è¯´æ˜ï¼‰ï¼š

```yml
# ====================== æ•°æ®åº“é…ç½® ======================
# æ•°æ®åº“ä¸»æœºï¼Œä¾‹å¦‚localhost æˆ– 127.0.0.1
DB_HOST=your_db_host
# æ•°æ®åº“ç«¯å£å·ï¼Œé»˜è®¤ä¸º3306
DB_PORT=3306
# æ•°æ®åº“ç”¨æˆ·å
DB_USER=your_db_user
# æ•°æ®åº“å¯†ç 
DB_PASSWORD=your_db_password
# æ•°æ®åº“åç§°
DB_NAME=your_db_name
# æ•°æ®åº“å­—ç¬¦é›†ï¼Œæ¨èutf8mb4ï¼Œå…¼å®¹emoji
DB_CHARSET=utf8mb4
# æ•°æ®åº“ç±»å‹postgresqlæˆ–mysql
DB_DIALECT=postgresql
# æ•°æ®åº“ä¸éœ€è¦åˆå§‹åŒ–ï¼Œæ‰§è¡Œapp.pyæ—¶ä¼šè‡ªåŠ¨æ£€æµ‹

# ====================== LLMé…ç½® ======================
# æ‚¨å¯ä»¥æ›´æ”¹æ¯ä¸ªéƒ¨åˆ†LLMä½¿ç”¨çš„APIï¼Œåªè¦å…¼å®¹OpenAIè¯·æ±‚æ ¼å¼éƒ½å¯ä»¥
# é…ç½®æ–‡ä»¶å†…éƒ¨ç»™äº†æ¯ä¸€ä¸ªAgentçš„æ¨èLLMï¼Œåˆæ¬¡éƒ¨ç½²è¯·å…ˆå‚è€ƒæ¨èè®¾ç½®

# Insight Agent
INSIGHT_ENGINE_API_KEY=
INSIGHT_ENGINE_BASE_URL=
INSIGHT_ENGINE_MODEL_NAME=

# Media Agent
...
```

### 6. å¯åŠ¨ç³»ç»Ÿ

#### 6.1 å®Œæ•´ç³»ç»Ÿå¯åŠ¨ï¼ˆæ¨èï¼‰

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ï¼Œæ¿€æ´»condaç¯å¢ƒ
conda activate your_conda_name

# å¯åŠ¨ä¸»åº”ç”¨å³å¯
python app.py
```

uv ç‰ˆæœ¬å¯åŠ¨å‘½ä»¤ 
```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ï¼Œæ¿€æ´»uvç¯å¢ƒ
.venv\Scripts\activate

# å¯åŠ¨ä¸»åº”ç”¨å³å¯
python app.py
```

&gt; æ³¨1ï¼šä¸€æ¬¡è¿è¡Œç»ˆæ­¢åï¼Œstreamlit appå¯èƒ½ç»“æŸå¼‚å¸¸ä»ç„¶å ç”¨ç«¯å£ï¼Œæ­¤æ—¶æœç´¢å ç”¨ç«¯å£çš„è¿›ç¨‹killæ‰å³å¯

&gt; æ³¨2ï¼šæ•°æ®çˆ¬å–éœ€è¦å•ç‹¬æ“ä½œï¼Œè§6.3æŒ‡å¼•

è®¿é—® http://localhost:5000 å³å¯ä½¿ç”¨å®Œæ•´ç³»ç»Ÿ

#### 6.2 å•ç‹¬å¯åŠ¨æŸä¸ªAgent

```bash
# å¯åŠ¨QueryEngine
streamlit run SingleEngineApp/query_engine_streamlit_app.py --server.port 8503

# å¯åŠ¨MediaEngine  
streamlit run SingleEngineApp/media_engine_streamlit_app.py --server.port 8502

# å¯åŠ¨InsightEngine
streamlit run SingleEngineApp/insight_engine_streamlit_app.py --server.port 8501
```

#### 6.3 çˆ¬è™«ç³»ç»Ÿå•ç‹¬ä½¿ç”¨

è¿™éƒ¨åˆ†æœ‰è¯¦ç»†çš„é…ç½®æ–‡æ¡£ï¼š[MindSpiderä½¿ç”¨è¯´æ˜](./MindSpider/README.md)

&lt;div align=&quot;center&quot;&gt;
&lt;img src=&quot;MindSpider\img\example.png&quot; alt=&quot;banner&quot; width=&quot;600&quot;&gt;

MindSpider è¿è¡Œç¤ºä¾‹
&lt;/div&gt;

```bash
# è¿›å…¥çˆ¬è™«ç›®å½•
cd MindSpider

# é¡¹ç›®åˆå§‹åŒ–
python main.py --setup

# è¿è¡Œè¯é¢˜æå–ï¼ˆè·å–çƒ­ç‚¹æ–°é—»å’Œå…³é”®è¯ï¼‰
python main.py --broad-topic

# è¿è¡Œå®Œæ•´çˆ¬è™«æµç¨‹
python main.py --complete --date 2024-01-20

# ä»…è¿è¡Œè¯é¢˜æå–
python main.py --broad-topic --date 2024-01-20

# ä»…è¿è¡Œæ·±åº¦çˆ¬å–
python main.py --deep-sentiment --platforms xhs dy wb
```

#### 6.4 å‘½ä»¤è¡ŒæŠ¥å‘Šç”Ÿæˆå·¥å…·

è¯¥å·¥å…·ä¼šè·³è¿‡ä¸‰ä¸ªåˆ†æå¼•æ“çš„è¿è¡Œé˜¶æ®µï¼Œç›´æ¥è¯»å–å®ƒä»¬çš„æœ€æ–°æ—¥å¿—æ–‡ä»¶ï¼Œå¹¶åœ¨æ— éœ€ Web ç•Œé¢çš„æƒ…å†µä¸‹ç”Ÿæˆç»¼åˆæŠ¥å‘Šï¼ˆåŒæ—¶çœç•¥æ–‡ä»¶å¢é‡æ ¡éªŒæ­¥éª¤ï¼‰ã€‚é€šå¸¸ç”¨äºå¯¹æŠ¥å‘Šç”Ÿæˆç»“æœä¸æ»¡æ„ã€éœ€è¦å¿«é€Ÿé‡è¯•çš„åœºæ™¯ï¼Œæˆ–åœ¨è°ƒè¯• Report Engine æ—¶å¯ç”¨ã€‚

```bash
# åŸºæœ¬ä½¿ç”¨ï¼ˆè‡ªåŠ¨ä»æ–‡ä»¶åæå–ä¸»é¢˜ï¼‰
python report_engine_only.py

# æŒ‡å®šæŠ¥å‘Šä¸»é¢˜
python report_engine_only.py --query &quot;åœŸæœ¨å·¥ç¨‹è¡Œä¸šåˆ†æ&quot;

# è·³è¿‡PDFç”Ÿæˆï¼ˆå³ä½¿ç³»ç»Ÿæ”¯æŒï¼‰
python report_engine_only.py --skip-pdf

# æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
python report_engine_only.py --verbose

# æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯
python report_engine_only.py --help
```

**åŠŸèƒ½è¯´æ˜ï¼š**

1. **è‡ªåŠ¨æ£€æŸ¥ä¾èµ–**ï¼šç¨‹åºä¼šè‡ªåŠ¨æ£€æŸ¥PDFç”Ÿæˆæ‰€éœ€çš„ç³»ç»Ÿä¾èµ–ï¼Œå¦‚æœç¼ºå¤±ä¼šç»™å‡ºå®‰è£…æç¤º
2. **è·å–æœ€æ–°æ–‡ä»¶**ï¼šè‡ªåŠ¨ä»ä¸‰ä¸ªå¼•æ“ç›®å½•ï¼ˆ`insight_engine_streamlit_reports`ã€`media_engine_streamlit_reports`ã€`query_engine_streamlit_reports`ï¼‰è·å–æœ€æ–°çš„åˆ†ææŠ¥å‘Š
3. **æ–‡ä»¶ç¡®è®¤**ï¼šæ˜¾ç¤ºæ‰€æœ‰é€‰æ‹©çš„æ–‡ä»¶åã€è·¯å¾„å’Œä¿®æ”¹æ—¶é—´ï¼Œç­‰å¾…ç”¨æˆ·ç¡®è®¤ï¼ˆé»˜è®¤è¾“å…¥ `y` ç»§ç»­ï¼Œè¾“å…¥ `n` é€€å‡ºï¼‰
4. **ç›´æ¥ç”ŸæˆæŠ¥å‘Š**ï¼šè·³è¿‡æ–‡ä»¶å¢åŠ å®¡æ ¸ç¨‹åºï¼Œç›´æ¥è°ƒç”¨Report Engineç”Ÿæˆç»¼åˆæŠ¥å‘Š
5. **è‡ªåŠ¨ä¿å­˜æ–‡ä»¶**ï¼š
   - HTMLæŠ¥å‘Šä¿å­˜åˆ° `final_reports/` ç›®å½•
   - PDFæŠ¥å‘Šï¼ˆå¦‚æœæœ‰ä¾èµ–ï¼‰ä¿å­˜åˆ° `final_reports/pdf/` ç›®å½•
   - æ–‡ä»¶å‘½åæ ¼å¼ï¼š`final_report_{ä¸»é¢˜}_{æ—¶é—´æˆ³}.html/pdf`

**æ³¨æ„äº‹é¡¹ï¼š**

- ç¡®ä¿ä¸‰ä¸ªå¼•æ“ç›®å½•ä¸­è‡³å°‘æœ‰ä¸€ä¸ªåŒ…å«`.md`æŠ¥å‘Šæ–‡ä»¶
- å‘½ä»¤è¡Œå·¥å…·ä¸Webç•Œé¢ç›¸äº’ç‹¬ç«‹ï¼Œä¸ä¼šç›¸äº’å½±å“
- PDFç”Ÿæˆéœ€è¦å®‰è£…ç³»ç»Ÿä¾èµ–ï¼Œè¯¦è§ä¸Šæ–‡&quot;å®‰è£… PDF å¯¼å‡ºæ‰€éœ€ç³»ç»Ÿä¾èµ–&quot;éƒ¨åˆ†

## âš™ï¸ é«˜çº§é…ç½®ï¼ˆå·²è¿‡æ—¶ï¼Œå·²ç»ç»Ÿä¸€ä¸ºé¡¹ç›®æ ¹ç›®å½•.envæ–‡ä»¶ç®¡ç†ï¼Œå…¶ä»–å­agentè‡ªåŠ¨ç»§æ‰¿æ ¹ç›®å½•é…ç½®ï¼‰

### ä¿®æ”¹å…³é”®å‚æ•°

#### Agenté…ç½®å‚æ•°

æ¯ä¸ªAgentéƒ½æœ‰ä¸“é—¨çš„é…ç½®æ–‡ä»¶ï¼Œå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼Œä¸‹é¢æ˜¯éƒ¨åˆ†ç¤ºä¾‹ï¼š

```python
# QueryEngine/utils/config.py
class Config:
    max_reflections = 2           # åæ€è½®æ¬¡
    max_search_results = 15       # æœ€å¤§æœç´¢ç»“æœæ•°
    max_content_length = 8000     # æœ€å¤§å†…å®¹é•¿åº¦
    
# MediaEngine/utils/config.py  
class Config:
    comprehensive_search_limit = 10  # ç»¼åˆæœç´¢é™åˆ¶
    web_search_limit = 15           # ç½‘é¡µæœç´¢é™åˆ¶
  

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[srbhr/Resume-Matcher]]></title>
            <link>https://github.com/srbhr/Resume-Matcher</link>
            <guid>https://github.com/srbhr/Resume-Matcher</guid>
            <pubDate>Wed, 10 Dec 2025 00:04:27 GMT</pubDate>
            <description><![CDATA[Improve your resumes with Resume Matcher. Get insights, keyword suggestions and tune your resumes to job descriptions.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/srbhr/Resume-Matcher">srbhr/Resume-Matcher</a></h1>
            <p>Improve your resumes with Resume Matcher. Get insights, keyword suggestions and tune your resumes to job descriptions.</p>
            <p>Language: Python</p>
            <p>Stars: 24,307</p>
            <p>Forks: 4,531</p>
            <p>Stars today: 45 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;

[![Resume Matcher](assets/page_2.png)](https://www.resumematcher.fyi)

# Resume Matcher

[ğ™¹ğš˜ğš’ğš— ğ™³ğš’ğšœğšŒğš˜ğš›ğš](https://dsc.gg/resume-matcher) âœ¦ [ğš†ğšğš‹ğšœğš’ğšğš](https://resumematcher.fyi) âœ¦ [ğ™·ğš˜ğš  ğšğš˜ ğ™¸ğš—ğšœğšğšŠğš•ğš•](#how-to-install) âœ¦ [ğ™²ğš˜ğš—ğšğš›ğš’ğš‹ğšğšğš˜ğš›ğšœ](#contributors) âœ¦ [ğ™³ğš˜ğš—ğšŠğšğš](#support-the-development-by-donating) âœ¦ [ğšƒğš ğš’ğšğšğšğš›/ğš‡](https://twitter.com/ssrbhr) âœ¦ [ğ™»ğš’ğš—ğš”ğšğšğ™¸ğš—](https://www.linkedin.com/company/resume-matcher/)

**Stop getting auto-rejected by ATS bots.** Resume Matcher is the AI-powered platform that reverse-engineers hiring algorithms to show you exactly how to tailor your resume. Get the keywords, formatting, and insights that actually get you past the first screen and into human hands.

Hoping to make this, **VS Code for making resumes**.

&lt;/div&gt;

&lt;br&gt;

&lt;div align=&quot;center&quot;&gt;

![Stars](https://img.shields.io/github/stars/srbhr/Resume-Matcher?labelColor=black&amp;style=for-the-badge&amp;color=c20a71)
![Apache 2.0](https://img.shields.io/github/license/srbhr/Resume-Matcher?labelColor=black&amp;style=for-the-badge&amp;color=c20a71) ![Forks](https://img.shields.io/github/forks/srbhr/Resume-Matcher?labelColor=black&amp;style=for-the-badge&amp;color=c20a71) ![version](https://img.shields.io/badge/Version-0.1%20Veridis%20Quo-FFF?labelColor=black&amp;logo=LinkedIn&amp;style=for-the-badge&amp;color=c20a71)

[![Discord](https://img.shields.io/discord/1122069176962531400?labelColor=black&amp;logo=discord&amp;logoColor=c20a71&amp;style=for-the-badge&amp;color=c20a71)](https://dsc.gg/resume-matcher) [![Website](https://img.shields.io/badge/website-Resume%20Matcher-FFF?labelColor=black&amp;style=for-the-badge&amp;color=c20a71)](https://resumematcher.fyi) [![LinkedIn](https://img.shields.io/badge/LinkedIn-Resume%20Matcher-FFF?labelColor=black&amp;logo=LinkedIn&amp;style=for-the-badge&amp;color=c20a71)](https://www.linkedin.com/company/resume-matcher/)

&lt;a href=&quot;https://trendshift.io/repositories/565&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/565&quot; alt=&quot;srbhr%2FResume-Matcher | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;

![Vercel OSS Program](https://vercel.com/oss/program-badge.svg)

&lt;/div&gt;

&gt; \[!IMPORTANT]
&gt;
&gt; This project is in active development. New features are being added continuously, and we welcome contributions from the community. There are some breaking changes on the `main` branch. If you have any suggestions or feature requests, please feel free to open an issue on GitHub or discuss it on our [Discord](https://dsc.gg/resume-matcher) server.

## Getting started with Resume Matcher

Resume Matcher is designed to help you optimize your resume with the aim to highlight your skills and experience in a way that resonates with potential employers.

We&#039;re actively working on improving the platform, building towards a **VS Code for making resumes**, and adding new features. The best way to stay updated is to join the Discord discussion and be part of the active development community.

&gt; Join our [Discord](https://dsc.gg/resume-matcher) community ğŸ‘‡
[![Discord](assets/resume_matcher_discord.png)](https://dsc.gg/resume-matcher)

&gt; Follow us on [LinkedIn](https://www.linkedin.com/company/resume-matcher/) âœ¨
[![LinkedIn](assets/resume_matcher_linkedin.png)](https://www.linkedin.com/company/resume-matcher/)

&gt; â­ Star Resume Matcher to support the development and get updates on GitHub.
![Star Resume Matcher](assets/star_resume_matcher.png)

## Key Features

![resume_matcher_features](assets/resume_matcher_features.png)

- **Works locally**: No need to upload your resume to a server. Everything runs on your machine with open source AI models by Ollama.
- **ATS Compatibility**: Get a detailed analysis of your resume&#039;s compatibility with ATS systems.
- **Instant Match Score**: Upload resume &amp; job description for a quick match score and key improvement areas.
- **Keyword Optimizer**: Align your resume with job keywords and identify critical content gaps.
- **Guided Improvements**: Get clear suggestions to make your resume stand out.

### Roadmap

If you have any suggestions or feature requests, please feel free to open an issue on GitHub. And discuss it on our [Discord](https://dsc.gg/resume-matcher) server.

- Visual keyword highlighting.
- AI Canvas, which can help to craft impactful, metric-driven resume content.
- Multi-job description optimization.

## How to Install

![Installation](assets/how_to_install_resumematcher.png)

Follow the instructions in the [SETUP.md](SETUP.md) file to set up the project locally. The setup script will install all the necessary dependencies and configure your environment.

The project is built using:

- FastAPI for the backend.
- Next.js for the frontend.
- Ollama for local AI model serving.
- Tailwind CSS for styling.
- SQLite for the database.

| Technology   | Info/Version                               |
|--------------|---------------------------------------|
| Python      | 3.12+                   |
| Next.js      | 15+                   |
| Ollama       |        0.6.7        |

## Join Us and Contribute

![how to contribute](assets/how_to_contribute.png)

We welcome contributions from everyone! Whether you&#039;re a developer, designer, or just someone who wants to help out. All the contributors are listed in the [about page](https://resumematcher.fyi/about) on our website and on the GitHub Readme here.

Check out the roadmap if you would like to work on the features that are planned for the future. If you have any suggestions or feature requests, please feel free to open an issue on GitHub and discuss it on our [Discord](https://dsc.gg/resume-matcher) server.

## Contributors

![Contributors](assets/contributors.png)

&lt;a href=&quot;https://github.com/srbhr/Resume-Matcher/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=srbhr/Resume-Matcher&quot; /&gt;
&lt;/a&gt;

## Support the Development by Donating

![donate](assets/supporting_resume_matcher.png)

If you would like to support the development of Resume Matcher, you can do so by donating. Your contributions will help us keep the project alive and continue adding new features.

| Platform  | Link                                   |
|-----------|----------------------------------------|
| GitHub    | [![GitHub Sponsors](https://img.shields.io/github/sponsors/srbhr?style=for-the-badge&amp;color=c20a71&amp;labelColor=black&amp;logo=github)](https://github.com/sponsors/srbhr) |
| Buy Me a Coffee | [![BuyMeACoffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=for-the-badge&amp;logo=buy-me-a-coffee&amp;color=c20a72&amp;logoColor=white)](https://www.buymeacoffee.com/srbhr) |

&lt;details&gt;
  &lt;summary&gt;&lt;kbd&gt;Star History&lt;/kbd&gt;&lt;/summary&gt;
  &lt;picture&gt;
    &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=srbhr/resume-matcher&amp;theme=dark&amp;type=Date&quot;&gt;
    &lt;img width=&quot;100%&quot; src=&quot;https://api.star-history.com/svg?repos=srbhr/resume-matcher&amp;theme=dark&amp;type=Date&quot;&gt;
  &lt;/picture&gt;
&lt;/details&gt;

## Resume Matcher is a part of [Vercel Open Source Program](https://vercel.com/oss)

![Vercel OSS Program](https://vercel.com/oss/program-badge.svg)
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[anthropics/claude-quickstarts]]></title>
            <link>https://github.com/anthropics/claude-quickstarts</link>
            <guid>https://github.com/anthropics/claude-quickstarts</guid>
            <pubDate>Wed, 10 Dec 2025 00:04:26 GMT</pubDate>
            <description><![CDATA[A collection of projects designed to help developers quickly get started with building deployable applications using the Claude API]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/anthropics/claude-quickstarts">anthropics/claude-quickstarts</a></h1>
            <p>A collection of projects designed to help developers quickly get started with building deployable applications using the Claude API</p>
            <p>Language: Python</p>
            <p>Stars: 11,726</p>
            <p>Forks: 2,048</p>
            <p>Stars today: 320 stars today</p>
            <h2>README</h2><pre># Claude Quickstarts

Claude Quickstarts is a collection of projects designed to help developers quickly get started with building  applications using the Claude API. Each quickstart provides a foundation that you can easily build upon and customize for your specific needs.

## Getting Started

To use these quickstarts, you&#039;ll need an Claude API key. If you don&#039;t have one yet, you can sign up for free at [console.anthropic.com](https://console.anthropic.com).

## Available Quickstarts

### Customer Support Agent

A customer support agent powered by Claude. This project demonstrates how to leverage Claude&#039;s natural language understanding and generation capabilities to create an AI-assisted customer support system with access to a knowledge base.

[Go to Customer Support Agent Quickstart](./customer-support-agent)

### Financial Data Analyst

A financial data analyst powered by Claude. This project demonstrates how to leverage Claude&#039;s capabilities with interactive data visualization to analyze financial data via chat.

[Go to Financial Data Analyst Quickstart](./financial-data-analyst)

### Computer Use Demo

An environment and tools that Claude can use to control a desktop computer. This project demonstrates how to leverage the computer use capabilities of Claude, including support for the latest `computer_use_20251124` tool version with zoom actions.

[Go to Computer Use Demo Quickstart](./computer-use-demo)

### Autonomous Coding Agent

An autonomous coding agent powered by the Claude Agent SDK. This project demonstrates a two-agent pattern (initializer + coding agent) that can build complete applications over multiple sessions, with progress persisted via git and a feature list that the agent works through incrementally.

[Go to Autonomous Coding Agent Quickstart](./autonomous-coding)

## General Usage

Each quickstart project comes with its own README and setup instructions. Generally, you&#039;ll follow these steps:

1. Clone this repository
2. Navigate to the specific quickstart directory
3. Install the required dependencies
4. Set up your Claude API key as an environment variable
5. Run the quickstart application

## Explore Further

To deepen your understanding of working with Claude and the Claude API, check out these resources:

- [Claude API Documentation](https://docs.claude.com)
- [Claude Cookbooks](https://github.com/anthropics/claude-cookbooks) - A collection of code snippets and guides for common tasks
- [Claude API Fundamentals Course](https://github.com/anthropics/courses/tree/master/anthropic_api_fundamentals)

## Contributing

We welcome contributions to the Claude Quickstarts repository! If you have ideas for new quickstart projects or improvements to existing ones, please open an issue or submit a pull request.

## Community and Support

- Join our [Anthropic Discord community](https://www.anthropic.com/discord) for discussions and support
- Check out the [Anthropic support documentation](https://support.anthropic.com) for additional help

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[commaai/openpilot]]></title>
            <link>https://github.com/commaai/openpilot</link>
            <guid>https://github.com/commaai/openpilot</guid>
            <pubDate>Wed, 10 Dec 2025 00:04:25 GMT</pubDate>
            <description><![CDATA[openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/commaai/openpilot">commaai/openpilot</a></h1>
            <p>openpilot is an operating system for robotics. Currently, it upgrades the driver assistance system on 300+ supported cars.</p>
            <p>Language: Python</p>
            <p>Stars: 59,289</p>
            <p>Forks: 10,493</p>
            <p>Stars today: 39 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot; style=&quot;text-align: center;&quot;&gt;

&lt;h1&gt;openpilot&lt;/h1&gt;

&lt;p&gt;
  &lt;b&gt;openpilot is an operating system for robotics.&lt;/b&gt;
  &lt;br&gt;
  Currently, it upgrades the driver assistance system in 300+ supported cars.
&lt;/p&gt;

&lt;h3&gt;
  &lt;a href=&quot;https://docs.comma.ai&quot;&gt;Docs&lt;/a&gt;
  &lt;span&gt; Â· &lt;/span&gt;
  &lt;a href=&quot;https://docs.comma.ai/contributing/roadmap/&quot;&gt;Roadmap&lt;/a&gt;
  &lt;span&gt; Â· &lt;/span&gt;
  &lt;a href=&quot;https://github.com/commaai/openpilot/blob/master/docs/CONTRIBUTING.md&quot;&gt;Contribute&lt;/a&gt;
  &lt;span&gt; Â· &lt;/span&gt;
  &lt;a href=&quot;https://discord.comma.ai&quot;&gt;Community&lt;/a&gt;
  &lt;span&gt; Â· &lt;/span&gt;
  &lt;a href=&quot;https://comma.ai/shop&quot;&gt;Try it on a comma 3X&lt;/a&gt;
&lt;/h3&gt;

Quick start: `bash &lt;(curl -fsSL openpilot.comma.ai)`

[![openpilot tests](https://github.com/commaai/openpilot/actions/workflows/tests.yaml/badge.svg)](https://github.com/commaai/openpilot/actions/workflows/tests.yaml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![X Follow](https://img.shields.io/twitter/follow/comma_ai)](https://x.com/comma_ai)
[![Discord](https://img.shields.io/discord/469524606043160576)](https://discord.comma.ai)

&lt;/div&gt;

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/NmBfgOanCyk&quot; title=&quot;Video By Greer Viau&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/2f7112ae-f748-4f39-b617-fabd689c3772&quot;&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/VHKyqZ7t8Gw&quot; title=&quot;Video By Logan LeGrand&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/92351544-2833-40d7-9e0b-7ef7ae37ec4c&quot;&gt;&lt;/a&gt;&lt;/td&gt;
    &lt;td&gt;&lt;a href=&quot;https://youtu.be/SUIZYzxtMQs&quot; title=&quot;A drive to Taco Bell&quot;&gt;&lt;img src=&quot;https://github.com/commaai/openpilot/assets/8762862/05ceefc5-2628-439c-a9b2-89ce77dc6f63&quot;&gt;&lt;/a&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;


Using openpilot in a car
------

To use openpilot in a car, you need four things:
1. **Supported Device:** a comma 3X, available at [comma.ai/shop](https://comma.ai/shop/comma-3x).
2. **Software:** The setup procedure for the comma 3X allows users to enter a URL for custom software. Use the URL `openpilot.comma.ai` to install the release version.
3. **Supported Car:** Ensure that you have one of [the 275+ supported cars](docs/CARS.md).
4. **Car Harness:** You will also need a [car harness](https://comma.ai/shop/car-harness) to connect your comma 3X to your car.

We have detailed instructions for [how to install the harness and device in a car](https://comma.ai/setup). Note that it&#039;s possible to run openpilot on [other hardware](https://blog.comma.ai/self-driving-car-for-free/), although it&#039;s not plug-and-play.


### Branches

Running `master` and other branches directly is supported, but it&#039;s recommended to run one of the following prebuilt branches:

| comma four branch      | comma 3X branch        | URL                                    | description                                                                         |
|------------------------|------------------------|----------------------------------------|-------------------------------------------------------------------------------------|
| `release-mici`         | `release-tizi`         | openpilot.comma.ai                     | This is openpilot&#039;s release branch.                                                 |
| `release-mici-staging` | `release-tizi-staging` | openpilot-test.comma.ai                | This is the staging branch for releases. Use it to get new releases slightly early. |
| `nightly`              | `nightly`              | openpilot-nightly.comma.ai             | This is the bleeding edge development branch. Do not expect this to be stable.      |
| `nightly-dev`          | `nightly-dev`          | installer.comma.ai/commaai/nightly-dev | Same as nightly, but includes experimental development features for some cars.      |

To start developing openpilot
------

openpilot is developed by [comma](https://comma.ai/) and by users like you. We welcome both pull requests and issues on [GitHub](http://github.com/commaai/openpilot).

* Join the [community Discord](https://discord.comma.ai)
* Check out [the contributing docs](docs/CONTRIBUTING.md)
* Check out the [openpilot tools](tools/)
* Code documentation lives at https://docs.comma.ai
* Information about running openpilot lives on the [community wiki](https://github.com/commaai/openpilot/wiki)

Want to get paid to work on openpilot? [comma is hiring](https://comma.ai/jobs#open-positions) and offers lots of [bounties](https://comma.ai/bounties) for external contributors.

Safety and Testing
----

* openpilot observes [ISO26262](https://en.wikipedia.org/wiki/ISO_26262) guidelines, see [SAFETY.md](docs/SAFETY.md) for more details.
* openpilot has software-in-the-loop [tests](.github/workflows/tests.yaml) that run on every commit.
* The code enforcing the safety model lives in panda and is written in C, see [code rigor](https://github.com/commaai/panda#code-rigor) for more details.
* panda has software-in-the-loop [safety tests](https://github.com/commaai/panda/tree/master/tests/safety).
* Internally, we have a hardware-in-the-loop Jenkins test suite that builds and unit tests the various processes.
* panda has additional hardware-in-the-loop [tests](https://github.com/commaai/panda/blob/master/Jenkinsfile).
* We run the latest openpilot in a testing closet containing 10 comma devices continuously replaying routes.

&lt;details&gt;
&lt;summary&gt;MIT Licensed&lt;/summary&gt;

openpilot is released under the MIT license. Some parts of the software are released under other licenses as specified.

Any user of this software shall indemnify and hold harmless Comma.ai, Inc. and its directors, officers, employees, agents, stockholders, affiliates, subcontractors and customers from and against all allegations, claims, actions, suits, demands, damages, liabilities, obligations, losses, settlements, judgments, costs and expenses (including without limitation attorneysâ€™ fees and costs) which arise out of, relate to or result from any use of this software by user.

**THIS IS ALPHA QUALITY SOFTWARE FOR RESEARCH PURPOSES ONLY. THIS IS NOT A PRODUCT.
YOU ARE RESPONSIBLE FOR COMPLYING WITH LOCAL LAWS AND REGULATIONS.
NO WARRANTY EXPRESSED OR IMPLIED.**
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;User Data and comma Account&lt;/summary&gt;

By default, openpilot uploads the driving data to our servers. You can also access your data through [comma connect](https://connect.comma.ai/). We use your data to train better models and improve openpilot for everyone.

openpilot is open source software: the user is free to disable data collection if they wish to do so.

openpilot logs the road-facing cameras, CAN, GPS, IMU, magnetometer, thermal sensors, crashes, and operating system logs.
The driver-facing camera and microphone are only logged if you explicitly opt-in in settings.

By using openpilot, you agree to [our Privacy Policy](https://comma.ai/privacy). You understand that use of this software or its related services will generate certain types of user data, which may be logged and stored at the sole discretion of comma. By accepting this agreement, you grant an irrevocable, perpetual, worldwide right to comma for the use of this data.
&lt;/details&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[infiniflow/ragflow]]></title>
            <link>https://github.com/infiniflow/ragflow</link>
            <guid>https://github.com/infiniflow/ragflow</guid>
            <pubDate>Wed, 10 Dec 2025 00:04:24 GMT</pubDate>
            <description><![CDATA[RAGFlow is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/infiniflow/ragflow">infiniflow/ragflow</a></h1>
            <p>RAGFlow is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs</p>
            <p>Language: Python</p>
            <p>Stars: 69,177</p>
            <p>Forks: 7,513</p>
            <p>Stars today: 187 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://demo.ragflow.io/&quot;&gt;
&lt;img src=&quot;web/src/assets/logo-with-text.svg&quot; width=&quot;520&quot; alt=&quot;ragflow logo&quot;&gt;
&lt;/a&gt;
&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;./README.md&quot;&gt;&lt;img alt=&quot;README in English&quot; src=&quot;https://img.shields.io/badge/English-DBEDFA&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_zh.md&quot;&gt;&lt;img alt=&quot;ç®€ä½“ä¸­æ–‡ç‰ˆè‡ªè¿°æ–‡ä»¶&quot; src=&quot;https://img.shields.io/badge/ç®€ä½“ä¸­æ–‡-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_tzh.md&quot;&gt;&lt;img alt=&quot;ç¹é«”ç‰ˆä¸­æ–‡è‡ªè¿°æ–‡ä»¶&quot; src=&quot;https://img.shields.io/badge/ç¹é«”ä¸­æ–‡-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_ja.md&quot;&gt;&lt;img alt=&quot;æ—¥æœ¬èªã®README&quot; src=&quot;https://img.shields.io/badge/æ—¥æœ¬èª-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_ko.md&quot;&gt;&lt;img alt=&quot;í•œêµ­ì–´&quot; src=&quot;https://img.shields.io/badge/í•œêµ­ì–´-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_id.md&quot;&gt;&lt;img alt=&quot;Bahasa Indonesia&quot; src=&quot;https://img.shields.io/badge/Bahasa Indonesia-DFE0E5&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;./README_pt_br.md&quot;&gt;&lt;img alt=&quot;PortuguÃªs(Brasil)&quot; src=&quot;https://img.shields.io/badge/PortuguÃªs(Brasil)-DFE0E5&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://x.com/intent/follow?screen_name=infiniflowai&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/twitter/follow/infiniflow?logo=X&amp;color=%20%23f5f5f5&quot; alt=&quot;follow on X(Twitter)&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://demo.ragflow.io&quot; target=&quot;_blank&quot;&gt;
        &lt;img alt=&quot;Static Badge&quot; src=&quot;https://img.shields.io/badge/Online-Demo-4e6b99&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://hub.docker.com/r/infiniflow/ragflow&quot; target=&quot;_blank&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/docker/pulls/infiniflow/ragflow?label=Docker%20Pulls&amp;color=0db7ed&amp;logo=docker&amp;logoColor=white&amp;style=flat-square&quot; alt=&quot;docker pull infiniflow/ragflow:v0.22.1&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/infiniflow/ragflow/releases/latest&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/github/v/release/infiniflow/ragflow?color=blue&amp;label=Latest%20Release&quot; alt=&quot;Latest Release&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://github.com/infiniflow/ragflow/blob/main/LICENSE&quot;&gt;
        &lt;img height=&quot;21&quot; src=&quot;https://img.shields.io/badge/License-Apache--2.0-ffffff?labelColor=d4eaf7&amp;color=2e6cc4&quot; alt=&quot;license&quot;&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://deepwiki.com/infiniflow/ragflow&quot;&gt;
        &lt;img alt=&quot;Ask DeepWiki&quot; src=&quot;https://deepwiki.com/badge.svg&quot;&gt;
    &lt;/a&gt;
&lt;/p&gt;

&lt;h4 align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://ragflow.io/docs/dev/&quot;&gt;Document&lt;/a&gt; |
  &lt;a href=&quot;https://github.com/infiniflow/ragflow/issues/4214&quot;&gt;Roadmap&lt;/a&gt; |
  &lt;a href=&quot;https://twitter.com/infiniflowai&quot;&gt;Twitter&lt;/a&gt; |
  &lt;a href=&quot;https://discord.gg/NjYzJD3GM3&quot;&gt;Discord&lt;/a&gt; |
  &lt;a href=&quot;https://demo.ragflow.io&quot;&gt;Demo&lt;/a&gt;
&lt;/h4&gt;

&lt;div align=&quot;center&quot; style=&quot;margin-top:20px;margin-bottom:20px;&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/ragflow-octoverse.png&quot; width=&quot;1200&quot;/&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://trendshift.io/repositories/9064&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/9064&quot; alt=&quot;infiniflow%2Fragflow | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/div&gt;

&lt;details open&gt;
&lt;summary&gt;&lt;b&gt;ğŸ“• Table of Contents&lt;/b&gt;&lt;/summary&gt;

- ğŸ’¡ [What is RAGFlow?](#-what-is-ragflow)
- ğŸ® [Demo](#-demo)
- ğŸ“Œ [Latest Updates](#-latest-updates)
- ğŸŒŸ [Key Features](#-key-features)
- ğŸ” [System Architecture](#-system-architecture)
- ğŸ¬ [Get Started](#-get-started)
- ğŸ”§ [Configurations](#-configurations)
- ğŸ”§ [Build a Docker image](#-build-a-docker-image)
- ğŸ”¨ [Launch service from source for development](#-launch-service-from-source-for-development)
- ğŸ“š [Documentation](#-documentation)
- ğŸ“œ [Roadmap](#-roadmap)
- ğŸ„ [Community](#-community)
- ğŸ™Œ [Contributing](#-contributing)

&lt;/details&gt;

## ğŸ’¡ What is RAGFlow?

[RAGFlow](https://ragflow.io/) is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs. It offers a streamlined RAG workflow adaptable to enterprises of any scale. Powered by a converged context engine and pre-built agent templates, RAGFlow enables developers to transform complex data into high-fidelity, production-ready AI systems with exceptional efficiency and precision.

## ğŸ® Demo

Try our demo at [https://demo.ragflow.io](https://demo.ragflow.io).

&lt;div align=&quot;center&quot; style=&quot;margin-top:20px;margin-bottom:20px;&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/chunking.gif&quot; width=&quot;1200&quot;/&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/agentic-dark.gif&quot; width=&quot;1200&quot;/&gt;
&lt;/div&gt;

## ğŸ”¥ Latest Updates

- 2025-11-19 Supports Gemini 3 Pro.
- 2025-11-12 Supports data synchronization from Confluence, S3, Notion, Discord, Google Drive.
- 2025-10-23 Supports MinerU &amp; Docling as document parsing methods.
- 2025-10-15 Supports orchestrable ingestion pipeline.
- 2025-08-08 Supports OpenAI&#039;s latest GPT-5 series models.
- 2025-08-01 Supports agentic workflow and MCP.
- 2025-05-23 Adds a Python/JavaScript code executor component to Agent.
- 2025-05-05 Supports cross-language query.
- 2025-03-19 Supports using a multi-modal model to make sense of images within PDF or DOCX files.

## ğŸ‰ Stay Tuned

â­ï¸ Star our repository to stay up-to-date with exciting new features and improvements! Get instant notifications for new
releases! ğŸŒŸ

&lt;div align=&quot;center&quot; style=&quot;margin-top:20px;margin-bottom:20px;&quot;&gt;
&lt;img src=&quot;https://github.com/user-attachments/assets/18c9707e-b8aa-4caf-a154-037089c105ba&quot; width=&quot;1200&quot;/&gt;
&lt;/div&gt;

## ğŸŒŸ Key Features

### ğŸ­ **&quot;Quality in, quality out&quot;**

- [Deep document understanding](./deepdoc/README.md)-based knowledge extraction from unstructured data with complicated
  formats.
- Finds &quot;needle in a data haystack&quot; of literally unlimited tokens.

### ğŸ± **Template-based chunking**

- Intelligent and explainable.
- Plenty of template options to choose from.

### ğŸŒ± **Grounded citations with reduced hallucinations**

- Visualization of text chunking to allow human intervention.
- Quick view of the key references and traceable citations to support grounded answers.

### ğŸ” **Compatibility with heterogeneous data sources**

- Supports Word, slides, excel, txt, images, scanned copies, structured data, web pages, and more.

### ğŸ›€ **Automated and effortless RAG workflow**

- Streamlined RAG orchestration catered to both personal and large businesses.
- Configurable LLMs as well as embedding models.
- Multiple recall paired with fused re-ranking.
- Intuitive APIs for seamless integration with business.

## ğŸ” System Architecture

&lt;div align=&quot;center&quot; style=&quot;margin-top:20px;margin-bottom:20px;&quot;&gt;
&lt;img src=&quot;https://github.com/user-attachments/assets/31b0dd6f-ca4f-445a-9457-70cb44a381b2&quot; width=&quot;1000&quot;/&gt;
&lt;/div&gt;

## ğŸ¬ Get Started

### ğŸ“ Prerequisites

- CPU &gt;= 4 cores
- RAM &gt;= 16 GB
- Disk &gt;= 50 GB
- Docker &gt;= 24.0.0 &amp; Docker Compose &gt;= v2.26.1
- [gVisor](https://gvisor.dev/docs/user_guide/install/): Required only if you intend to use the code executor (sandbox) feature of RAGFlow.

&gt; [!TIP]
&gt; If you have not installed Docker on your local machine (Windows, Mac, or Linux), see [Install Docker Engine](https://docs.docker.com/engine/install/).

### ğŸš€ Start up the server

1. Ensure `vm.max_map_count` &gt;= 262144:

   &gt; To check the value of `vm.max_map_count`:
   &gt;
   &gt; ```bash
   &gt; $ sysctl vm.max_map_count
   &gt; ```
   &gt;
   &gt; Reset `vm.max_map_count` to a value at least 262144 if it is not.
   &gt;
   &gt; ```bash
   &gt; # In this case, we set it to 262144:
   &gt; $ sudo sysctl -w vm.max_map_count=262144
   &gt; ```
   &gt;
   &gt; This change will be reset after a system reboot. To ensure your change remains permanent, add or update the
   &gt; `vm.max_map_count` value in **/etc/sysctl.conf** accordingly:
   &gt;
   &gt; ```bash
   &gt; vm.max_map_count=262144
   &gt; ```
   &gt;
2. Clone the repo:

   ```bash
   $ git clone https://github.com/infiniflow/ragflow.git
   ```
3. Start up the server using the pre-built Docker images:

&gt; [!CAUTION]
&gt; All Docker images are built for x86 platforms. We don&#039;t currently offer Docker images for ARM64.
&gt; If you are on an ARM64 platform, follow [this guide](https://ragflow.io/docs/dev/build_docker_image) to build a Docker image compatible with your system.

&gt; The command below downloads the `v0.22.1` edition of the RAGFlow Docker image. See the following table for descriptions of different RAGFlow editions. To download a RAGFlow edition different from `v0.22.1`, update the `RAGFLOW_IMAGE` variable accordingly in **docker/.env** before using `docker compose` to start the server.

```bash
   $ cd ragflow/docker
  
   # git checkout v0.22.1
   # Optional: use a stable tag (see releases: https://github.com/infiniflow/ragflow/releases)
   # This step ensures the **entrypoint.sh** file in the code matches the Docker image version.
   
   # Use CPU for DeepDoc tasks:
   $ docker compose -f docker-compose.yml up -d

   # To use GPU to accelerate DeepDoc tasks:
   # sed -i &#039;1i DEVICE=gpu&#039; .env
   # docker compose -f docker-compose.yml up -d
```

&gt; Note: Prior to `v0.22.0`, we provided both images with embedding models and slim images without embedding models. Details as follows:

| RAGFlow image tag | Image size (GB) | Has embedding models? | Stable?                  |
| ----------------- | --------------- | --------------------- | ------------------------ |
| v0.21.1           | &amp;approx;9       | âœ”ï¸                    | Stable release           |
| v0.21.1-slim      | &amp;approx;2       | âŒ                    | Stable release           |

&gt; Starting with `v0.22.0`, we ship only the slim edition and no longer append the **-slim** suffix to the image tag.

4. Check the server status after having the server up and running:

   ```bash
   $ docker logs -f docker-ragflow-cpu-1
   ```

   _The following output confirms a successful launch of the system:_

   ```bash

         ____   ___    ______ ______ __
        / __ \ /   |  / ____// ____// /____  _      __
       / /_/ // /| | / / __ / /_   / // __ \| | /| / /
      / _, _// ___ |/ /_/ // __/  / // /_/ /| |/ |/ /
     /_/ |_|/_/  |_|\____//_/    /_/ \____/ |__/|__/

    * Running on all addresses (0.0.0.0)
   ```

   &gt; If you skip this confirmation step and directly log in to RAGFlow, your browser may prompt a `network anormal`
   &gt; error because, at that moment, your RAGFlow may not be fully initialized.
   &gt;
5. In your web browser, enter the IP address of your server and log in to RAGFlow.

   &gt; With the default settings, you only need to enter `http://IP_OF_YOUR_MACHINE` (**sans** port number) as the default
   &gt; HTTP serving port `80` can be omitted when using the default configurations.
   &gt;
6. In [service_conf.yaml.template](./docker/service_conf.yaml.template), select the desired LLM factory in `user_default_llm` and update
   the `API_KEY` field with the corresponding API key.

   &gt; See [llm_api_key_setup](https://ragflow.io/docs/dev/llm_api_key_setup) for more information.
   &gt;

   _The show is on!_

## ğŸ”§ Configurations

When it comes to system configurations, you will need to manage the following files:

- [.env](./docker/.env): Keeps the fundamental setups for the system, such as `SVR_HTTP_PORT`, `MYSQL_PASSWORD`, and
  `MINIO_PASSWORD`.
- [service_conf.yaml.template](./docker/service_conf.yaml.template): Configures the back-end services. The environment variables in this file will be automatically populated when the Docker container starts. Any environment variables set within the Docker container will be available for use, allowing you to customize service behavior based on the deployment environment.
- [docker-compose.yml](./docker/docker-compose.yml): The system relies on [docker-compose.yml](./docker/docker-compose.yml) to start up.

&gt; The [./docker/README](./docker/README.md) file provides a detailed description of the environment settings and service
&gt; configurations which can be used as `${ENV_VARS}` in the [service_conf.yaml.template](./docker/service_conf.yaml.template) file.

To update the default HTTP serving port (80), go to [docker-compose.yml](./docker/docker-compose.yml) and change `80:80`
to `&lt;YOUR_SERVING_PORT&gt;:80`.

Updates to the above configurations require a reboot of all containers to take effect:

&gt; ```bash
&gt; $ docker compose -f docker-compose.yml up -d
&gt; ```

### Switch doc engine from Elasticsearch to Infinity

RAGFlow uses Elasticsearch by default for storing full text and vectors. To switch to [Infinity](https://github.com/infiniflow/infinity/), follow these steps:

1. Stop all running containers:

   ```bash
   $ docker compose -f docker/docker-compose.yml down -v
   ```

&gt; [!WARNING]
&gt; `-v` will delete the docker container volumes, and the existing data will be cleared.

2. Set `DOC_ENGINE` in **docker/.env** to `infinity`.
3. Start the containers:

   ```bash
   $ docker compose -f docker-compose.yml up -d
   ```

&gt; [!WARNING]
&gt; Switching to Infinity on a Linux/arm64 machine is not yet officially supported.

## ğŸ”§ Build a Docker image

This image is approximately 2 GB in size and relies on external LLM and embedding services.

```bash
git clone https://github.com/infiniflow/ragflow.git
cd ragflow/
docker build --platform linux/amd64 -f Dockerfile -t infiniflow/ragflow:nightly .
```

## ğŸ”¨ Launch service from source for development

1. Install `uv` and `pre-commit`, or skip this step if they are already installed:

   ```bash
   pipx install uv pre-commit
   ```
2. Clone the source code and install Python dependencies:

   ```bash
   git clone https://github.com/infiniflow/ragflow.git
   cd ragflow/
   uv sync --python 3.12 # install RAGFlow dependent python modules
   uv run download_deps.py
   pre-commit install
   ```
3. Launch the dependent services (MinIO, Elasticsearch, Redis, and MySQL) using Docker Compose:

   ```bash
   docker compose -f docker/docker-compose-base.yml up -d
   ```

   Add the following line to `/etc/hosts` to resolve all hosts specified in **docker/.env** to `127.0.0.1`:

   ```
   127.0.0.1       es01 infinity mysql minio redis sandbox-executor-manager
   ```
4. If you cannot access HuggingFace, set the `HF_ENDPOINT` environment variable to use a mirror site:

   ```bash
   export HF_ENDPOINT=https://hf-mirror.com
   ```
5. If your operating system does not have jemalloc, please install it as follows:

   ```bash
   # Ubuntu
   sudo apt-get install libjemalloc-dev
   # CentOS
   sudo yum install jemalloc
   # OpenSUSE
   sudo zypper install jemalloc
   # macOS
   sudo brew install jemalloc
   ```
6. Launch backend service:

   ```bash
   source .venv/bin/activate
   export PYTHONPATH=$(pwd)
   bash docker/launch_backend_service.sh
   ```
7. Install frontend dependencies:

   ```bash
   cd web
   npm install
   ```
8. Launch frontend service:

   ```bash
   npm run dev
   ```

   _The following output confirms a successful launch of the system:_

   ![](https://github.com/user-attachments/assets/0daf462c-a24d-4496-a66f-92533534e187)
9. Stop RAGFlow front-end and back-end service after development is complete:

   ```bash
   pkill -f &quot;ragflow_server.py|task_executor.py&quot;
   ```

## ğŸ“š Documentation

- [Quickstart](https://ragflow.io/docs/dev/)
- [Configuration](https://ragflow.io/docs/dev/configurations)
- [Release notes](https://ragflow.io/docs/dev/release_notes)
- [User guides](https://ragflow.io/docs/dev/category/guides)
- [Developer guides](https://ragflow.io/docs/dev/category/developers)
- [References](https://ragflow.io/docs/dev/category/references)
- [FAQs](https://ragflow.io/docs/dev/faq)

## ğŸ“œ Roadmap

See the [RAGFlow Roadmap 2025](https://github.com/infiniflow/ragflow/issues/4214)

## ğŸ„ Community

- [Discord](https://discord.gg/NjYzJD3GM3)
- [Twitter](https://twitter.com/infiniflowai)
- [GitHub Discussions](https://github.com/orgs/infiniflow/discussions)

## ğŸ™Œ Contributing

RAGFlow flourishes via open-source collaboration. In this spirit, we embrace diverse contributions from the community.
If you would like to be a part, review our [Contribution Guidelines](https://ragflow.io/docs/dev/contributing) first.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[virattt/ai-hedge-fund]]></title>
            <link>https://github.com/virattt/ai-hedge-fund</link>
            <guid>https://github.com/virattt/ai-hedge-fund</guid>
            <pubDate>Wed, 10 Dec 2025 00:04:23 GMT</pubDate>
            <description><![CDATA[An AI Hedge Fund Team]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/virattt/ai-hedge-fund">virattt/ai-hedge-fund</a></h1>
            <p>An AI Hedge Fund Team</p>
            <p>Language: Python</p>
            <p>Stars: 42,580</p>
            <p>Forks: 7,570</p>
            <p>Stars today: 14 stars today</p>
            <h2>README</h2><pre># AI Hedge Fund

This is a proof of concept for an AI-powered hedge fund.  The goal of this project is to explore the use of AI to make trading decisions.  This project is for **educational** purposes only and is not intended for real trading or investment.

This system employs several agents working together:

1. Aswath Damodaran Agent - The Dean of Valuation, focuses on story, numbers, and disciplined valuation
2. Ben Graham Agent - The godfather of value investing, only buys hidden gems with a margin of safety
3. Bill Ackman Agent - An activist investor, takes bold positions and pushes for change
4. Cathie Wood Agent - The queen of growth investing, believes in the power of innovation and disruption
5. Charlie Munger Agent - Warren Buffett&#039;s partner, only buys wonderful businesses at fair prices
6. Michael Burry Agent - The Big Short contrarian who hunts for deep value
7. Mohnish Pabrai Agent - The Dhandho investor, who looks for doubles at low risk
8. Peter Lynch Agent - Practical investor who seeks &quot;ten-baggers&quot; in everyday businesses
9. Phil Fisher Agent - Meticulous growth investor who uses deep &quot;scuttlebutt&quot; research 
10. Rakesh Jhunjhunwala Agent - The Big Bull of India
11. Stanley Druckenmiller Agent - Macro legend who hunts for asymmetric opportunities with growth potential
12. Warren Buffett Agent - The oracle of Omaha, seeks wonderful companies at a fair price
13. Valuation Agent - Calculates the intrinsic value of a stock and generates trading signals
14. Sentiment Agent - Analyzes market sentiment and generates trading signals
15. Fundamentals Agent - Analyzes fundamental data and generates trading signals
16. Technicals Agent - Analyzes technical indicators and generates trading signals
17. Risk Manager - Calculates risk metrics and sets position limits
18. Portfolio Manager - Makes final trading decisions and generates orders

&lt;img width=&quot;1042&quot; alt=&quot;Screenshot 2025-03-22 at 6 19 07 PM&quot; src=&quot;https://github.com/user-attachments/assets/cbae3dcf-b571-490d-b0ad-3f0f035ac0d4&quot; /&gt;

Note: the system does not actually make any trades.

[![Twitter Follow](https://img.shields.io/twitter/follow/virattt?style=social)](https://twitter.com/virattt)

## Disclaimer

This project is for **educational and research purposes only**.

- Not intended for real trading or investment
- No investment advice or guarantees provided
- Creator assumes no liability for financial losses
- Consult a financial advisor for investment decisions
- Past performance does not indicate future results

By using this software, you agree to use it solely for learning purposes.

## Table of Contents
- [How to Install](#how-to-install)
- [How to Run](#how-to-run)
  - [âŒ¨ï¸ Command Line Interface](#ï¸-command-line-interface)
  - [ğŸ–¥ï¸ Web Application](#ï¸-web-application)
- [How to Contribute](#how-to-contribute)
- [Feature Requests](#feature-requests)
- [License](#license)

## How to Install

Before you can run the AI Hedge Fund, you&#039;ll need to install it and set up your API keys. These steps are common to both the full-stack web application and command line interface.

### 1. Clone the Repository

```bash
git clone https://github.com/virattt/ai-hedge-fund.git
cd ai-hedge-fund
```

### 2. Set up API keys

Create a `.env` file for your API keys:
```bash
# Create .env file for your API keys (in the root directory)
cp .env.example .env
```

Open and edit the `.env` file to add your API keys:
```bash
# For running LLMs hosted by openai (gpt-4o, gpt-4o-mini, etc.)
OPENAI_API_KEY=your-openai-api-key

# For getting financial data to power the hedge fund
FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key
```

**Important**: You must set at least one LLM API key (e.g. `OPENAI_API_KEY`, `GROQ_API_KEY`, `ANTHROPIC_API_KEY`, or `DEEPSEEK_API_KEY`) for the hedge fund to work. 

**Financial Data**: Data for AAPL, GOOGL, MSFT, NVDA, and TSLA is free and does not require an API key. For any other ticker, you will need to set the `FINANCIAL_DATASETS_API_KEY` in the .env file.

## How to Run

### âŒ¨ï¸ Command Line Interface

You can run the AI Hedge Fund directly via terminal. This approach offers more granular control and is useful for automation, scripting, and integration purposes.

&lt;img width=&quot;992&quot; alt=&quot;Screenshot 2025-01-06 at 5 50 17 PM&quot; src=&quot;https://github.com/user-attachments/assets/e8ca04bf-9989-4a7d-a8b4-34e04666663b&quot; /&gt;

#### Quick Start

1. Install Poetry (if not already installed):
```bash
curl -sSL https://install.python-poetry.org | python3 -
```

2. Install dependencies:
```bash
poetry install
```

#### Run the AI Hedge Fund
```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA
```

You can also specify a `--ollama` flag to run the AI hedge fund using local LLMs.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --ollama
```

You can optionally specify the start and end dates to make decisions over a specific time period.

```bash
poetry run python src/main.py --ticker AAPL,MSFT,NVDA --start-date 2024-01-01 --end-date 2024-03-01
```

#### Run the Backtester
```bash
poetry run python src/backtester.py --ticker AAPL,MSFT,NVDA
```

**Example Output:**
&lt;img width=&quot;941&quot; alt=&quot;Screenshot 2025-01-06 at 5 47 52 PM&quot; src=&quot;https://github.com/user-attachments/assets/00e794ea-8628-44e6-9a84-8f8a31ad3b47&quot; /&gt;


Note: The `--ollama`, `--start-date`, and `--end-date` flags work for the backtester, as well!

### ğŸ–¥ï¸ Web Application

The new way to run the AI Hedge Fund is through our web application that provides a user-friendly interface. This is recommended for users who prefer visual interfaces over command line tools.

Please see detailed instructions on how to install and run the web application [here](https://github.com/virattt/ai-hedge-fund/tree/main/app).

&lt;img width=&quot;1721&quot; alt=&quot;Screenshot 2025-06-28 at 6 41 03â€¯PM&quot; src=&quot;https://github.com/user-attachments/assets/b95ab696-c9f4-416c-9ad1-51feb1f5374b&quot; /&gt;


## How to Contribute

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

**Important**: Please keep your pull requests small and focused.  This will make it easier to review and merge.

## Feature Requests

If you have a feature request, please open an [issue](https://github.com/virattt/ai-hedge-fund/issues) and make sure it is tagged with `enhancement`.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[wasserth/TotalSegmentator]]></title>
            <link>https://github.com/wasserth/TotalSegmentator</link>
            <guid>https://github.com/wasserth/TotalSegmentator</guid>
            <pubDate>Wed, 10 Dec 2025 00:04:22 GMT</pubDate>
            <description><![CDATA[Tool for robust segmentation of >100 important anatomical structures in CT and MR images]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/wasserth/TotalSegmentator">wasserth/TotalSegmentator</a></h1>
            <p>Tool for robust segmentation of >100 important anatomical structures in CT and MR images</p>
            <p>Language: Python</p>
            <p>Stars: 2,316</p>
            <p>Forks: 381</p>
            <p>Stars today: 1 star today</p>
            <h2>README</h2><pre># TotalSegmentator

Tool for segmentation of most major anatomical structures in any CT or MR image. It was trained on a wide range of different CT and MR images (different scanners, institutions, protocols,...) and therefore works well on most images. A large part of the training dataset can be downloaded here: [CT dataset](https://doi.org/10.5281/zenodo.6802613) (1228 subjects) and [MR dataset](https://zenodo.org/doi/10.5281/zenodo.11367004) (616 subjects). You can also try the tool online at [totalsegmentator.com](https://totalsegmentator.com/) or as [3D Slicer extension](https://github.com/lassoan/SlicerTotalSegmentator).

**ANNOUNCEMENT: We created a platform where anyone can help annotate more data to further improve TotalSegmentator: [TotalSegmentator Annotation Platform](https://annotate.totalsegmentator.com).**  
  
**ANNOUNCEMENT: We created web applications for [abdominal organ volume](https://compute.totalsegmentator.com/volume-report/), [Evans index](https://compute.totalsegmentator.com/evans-index/), and [aorta diameter](https://compute.totalsegmentator.com/aorta-report/).**

Main classes for CT and MR:
![Alt text](resources/imgs/overview_classes_v2.png)

TotalSegmentator supports a lot more structures. See [subtasks](#subtasks) or [here](https://backend.totalsegmentator.com/find-task/) for more details.

Created by the department of [Research and Analysis at University Hospital Basel](https://www.unispital-basel.ch/en/radiologie-nuklearmedizin/forschung-radiologie-nuklearmedizin).
If you use it please cite our [Radiology AI paper](https://pubs.rsna.org/doi/10.1148/ryai.230024) ([free preprint](https://arxiv.org/abs/2208.05868)). If you use it for MR images please cite the [TotalSegmentator MRI Radiology paper](https://pubs.rsna.org/doi/10.1148/radiol.241613) ([free preprint](https://arxiv.org/abs/2405.19492)). Please also cite [nnUNet](https://github.com/MIC-DKFZ/nnUNet) since TotalSegmentator is heavily based on it.


### Installation

TotalSegmentator works on Ubuntu, Mac, and Windows and on CPU and GPU.

Install dependencies:
* Python &gt;= 3.9
* [PyTorch](http://pytorch.org/) &gt;= 2.0.0

Optionally:
* if you use the option `--preview` you have to install xvfb (`apt-get install xvfb`) and fury (`pip install fury`)


Install Totalsegmentator
```bash
pip install TotalSegmentator
```


### Usage
For CT images:
```bash
TotalSegmentator -i ct.nii.gz -o segmentations
```
For MR images:
```bash
TotalSegmentator -i mri.nii.gz -o segmentations --task total_mr
```
&gt; Note: A Nifti file or a folder (or zip file) with all DICOM slices of one patient is allowed as input.

&gt; Note: If you run on CPU use the option `--fast` or `--roi_subset` to greatly improve runtime.

&gt; Note: This is not a medical device and is not intended for clinical usage. However, it is part of several FDA-approved products, where it has been certified as a component of the overall system.


### Subtasks

![Alt text](resources/imgs/overview_subclasses_2.png)

Next to the default task (`total`) there are more subtasks with more classes. If the taskname ends with `_mr` it works for MR images, otherwise for CT images.

Openly available for any usage (Apache-2.0 license):
* **total**: default task containing 117 main classes (see [here](https://github.com/wasserth/TotalSegmentator#class-details) for a list of classes)
* **total_mr**: default task containing 50 main classes on MR images (see [here](https://github.com/wasserth/TotalSegmentator#class-details) for a list of classes)
* **lung_vessels**: lung_vessels (cite [paper](https://www.sciencedirect.com/science/article/pii/S0720048X22001097)), lung_trachea_bronchia
* **body**: body, body_trunc, body_extremities, skin
* **body_mr**: body_trunc, body_extremities (for MR images)
* **vertebrae_mr**: sacrum, vertebrae_L5, vertebrae_L4, vertebrae_L3, vertebrae_L2, vertebrae_L1, vertebrae_T12, vertebrae_T11, vertebrae_T10, vertebrae_T9, vertebrae_T8, vertebrae_T7, vertebrae_T6, vertebrae_T5, vertebrae_T4, vertebrae_T3, vertebrae_T2, vertebrae_T1, vertebrae_C7, vertebrae_C6, vertebrae_C5, vertebrae_C4, vertebrae_C3, vertebrae_C2, vertebrae_C1 (for CT this is part of the `total` task)
* **cerebral_bleed**: intracerebral_hemorrhage (cite [paper](https://www.mdpi.com/2077-0383/12/7/2631))*
* **hip_implant**: hip_implant*
* **pleural_pericard_effusion**: pleural_effusion (cite [paper](http://dx.doi.org/10.1097/RLI.0000000000000869)), pericardial_effusion (cite [paper](http://dx.doi.org/10.3390/diagnostics12051045))*
* **head_glands_cavities**: eye_left, eye_right, eye_lens_left, eye_lens_right, optic_nerve_left, optic_nerve_right, parotid_gland_left, parotid_gland_right, submandibular_gland_right, submandibular_gland_left, nasopharynx, oropharynx, hypopharynx, nasal_cavity_right, nasal_cavity_left, auditory_canal_right, auditory_canal_left, soft_palate, hard_palate (cite [paper](https://www.mdpi.com/2072-6694/16/2/415))
* **head_muscles**: masseter_right, masseter_left, temporalis_right, temporalis_left, lateral_pterygoid_right, lateral_pterygoid_left, medial_pterygoid_right, medial_pterygoid_left, tongue, digastric_right, digastric_left
* **headneck_bones_vessels**: larynx_air, thyroid_cartilage, hyoid, cricoid_cartilage, zygomatic_arch_right, zygomatic_arch_left, styloid_process_right, styloid_process_left, internal_carotid_artery_right, internal_carotid_artery_left, internal_jugular_vein_right, internal_jugular_vein_left (cite [paper](https://www.mdpi.com/2072-6694/16/2/415))
* **headneck_muscles**: sternocleidomastoid_right, sternocleidomastoid_left, superior_pharyngeal_constrictor, middle_pharyngeal_constrictor, inferior_pharyngeal_constrictor, trapezius_right, trapezius_left, platysma_right, platysma_left, levator_scapulae_right, levator_scapulae_left, anterior_scalene_right, anterior_scalene_left, middle_scalene_right, middle_scalene_left, posterior_scalene_right, posterior_scalene_left, sterno_thyroid_right, sterno_thyroid_left, thyrohyoid_right, thyrohyoid_left, prevertebral_right, prevertebral_left (cite [paper](https://www.mdpi.com/2072-6694/16/2/415))
* **liver_vessels**: liver_vessels, liver_tumor (cite [paper](https://arxiv.org/abs/1902.09063))*
* **oculomotor_muscles**: skull, eyeball_right, lateral_rectus_muscle_right, superior_oblique_muscle_right, levator_palpebrae_superioris_right, superior_rectus_muscle_right, medial_rectus_muscle_left, inferior_oblique_muscle_right, inferior_rectus_muscle_right, optic_nerve_left, eyeball_left, lateral_rectus_muscle_left, superior_oblique_muscle_left, levator_palpebrae_superioris_left, superior_rectus_muscle_left, medial_rectus_muscle_right, inferior_oblique_muscle_left, inferior_rectus_muscle_left, optic_nerve_right*
* **lung_nodules**: lung, lung_nodules (provided by [BLUEMIND AI](https://bluemind.co/): Fitzjalen R., Aladin M., Nanyan G.) (trained on 1353 subjects, partly from LIDC-IDRI)
* **kidney_cysts**: kidney_cyst_left, kidney_cyst_right (strongly improved accuracy compared to kidney_cysts inside of `total` task)
* **breasts**: breast
* **liver_segments**: liver_segment_1, liver_segment_2, liver_segment_3, liver_segment_4, liver_segment_5, liver_segment_6, liver_segment_7, liver_segment_8 (Couinaud segments) (cite [paper](https://doi.org/10.1007/978-3-030-32692-0_32))*
* **liver_segments_mr**: liver_segment_1, liver_segment_2, liver_segment_3, liver_segment_4, liver_segment_5, liver_segment_6, liver_segment_7, liver_segment_8 (for MR images) (Couinaud segments)*
* **craniofacial_structures**: mandible, teeth_lower, skull, head, sinus_maxillary, sinus_frontal, teeth_upper
* **abdominal_muscles**: pectoralis_major_right, pectoralis_major_left, rectus_abdominis_right, rectus_abdominis_left, serratus_anterior_right, serratus_anterior_left, latissimus_dorsi_right, latissimus_dorsi_left, trapezius_right, trapezius_left, external_oblique_right, external_oblique_left, internal_oblique_right, internal_oblique_left, erector_spinae_right, erector_spinae_left, transversospinalis_right, transversospinalis_left, psoas_major_right, psoas_major_left, quadratus_lumborum_right, quadratus_lumborum_left (cite [paper](https://doi.org/10.1101/2025.01.13.25319967)) (only segments within T4-L4)*
* **teeth**: &quot;lower_jawbone&quot;, &quot;upper_jawbone&quot;, &quot;left_inferior_alveolar_canal&quot;, &quot;right_inferior_alveolar_canal&quot;, &quot;left_maxillary_sinus&quot;, &quot;right_maxillary_sinus&quot;, &quot;pharynx&quot;, &quot;bridge&quot;, &quot;crown&quot;, &quot;implant&quot;, &quot;upper_right_central_incisor_fdi11&quot;, &quot;upper_right_lateral_incisor_fdi12&quot;, &quot;upper_right_canine_fdi13&quot;, &quot;upper_right_first_premolar_fdi14&quot;, &quot;upper_right_second_premolar_fdi15&quot;, &quot;upper_right_first_molar_fdi16&quot;, &quot;upper_right_second_molar_fdi17&quot;, &quot;upper_right_third_molar_fdi18&quot;, &quot;upper_left_central_incisor_fdi21&quot;, &quot;upper_left_lateral_incisor_fdi22&quot;, &quot;upper_left_canine_fdi23&quot;, &quot;upper_left_first_premolar_fdi24&quot;, &quot;upper_left_second_premolar_fdi25&quot;, &quot;upper_left_first_molar_fdi26&quot;, &quot;upper_left_second_molar_fdi27&quot;, &quot;upper_left_third_molar_fdi28&quot;, &quot;lower_left_central_incisor_fdi31&quot;, &quot;lower_left_lateral_incisor_fdi32&quot;, &quot;lower_left_canine_fdi33&quot;, &quot;lower_left_first_premolar_fdi34&quot;, &quot;lower_left_second_premolar_fdi35&quot;, &quot;lower_left_first_molar_fdi36&quot;, &quot;lower_left_second_molar_fdi37&quot;, &quot;lower_left_third_molar_fdi38&quot;, &quot;lower_right_central_incisor_fdi41&quot;, &quot;lower_right_lateral_incisor_fdi42&quot;, &quot;lower_right_canine_fdi43&quot;, &quot;lower_right_first_premolar_fdi44&quot;, &quot;lower_right_second_premolar_fdi45&quot;, &quot;lower_right_first_molar_fdi46&quot;, &quot;lower_right_second_molar_fdi47&quot;, &quot;lower_right_third_molar_fdi48&quot;, &quot;left_mandibular_incisive_canal_fdi103&quot;, &quot;right_mandibular_incisive_canal_fdi104&quot;, &quot;lingual_canal&quot;, &quot;upper_right_central_incisor_pulp_fdi111&quot;, &quot;upper_right_lateral_incisor_pulp_fdi112&quot;, &quot;upper_right_canine_pulp_fdi113&quot;, &quot;upper_right_first_premolar_pulp_fdi114&quot;, &quot;upper_right_second_premolar_pulp_fdi115&quot;, &quot;upper_right_first_molar_pulp_fdi116&quot;, &quot;upper_right_second_molar_pulp_fdi117&quot;, &quot;upper_right_third_molar_pulp_fdi118&quot;, &quot;upper_left_central_incisor_pulp_fdi121&quot;, &quot;upper_left_lateral_incisor_pulp_fdi122&quot;, &quot;upper_left_canine_pulp_fdi123&quot;, &quot;upper_left_first_premolar_pulp_fdi124&quot;, &quot;upper_left_second_premolar_pulp_fdi125&quot;, &quot;upper_left_first_molar_pulp_fdi126&quot;, &quot;upper_left_second_molar_pulp_fdi127&quot;, &quot;upper_left_third_molar_pulp_fdi128&quot;, &quot;lower_left_central_incisor_pulp_fdi131&quot;, &quot;lower_left_lateral_incisor_pulp_fdi132&quot;, &quot;lower_left_canine_pulp_fdi133&quot;, &quot;lower_left_first_premolar_pulp_fdi134&quot;, &quot;lower_left_second_premolar_pulp_fdi135&quot;, &quot;lower_left_first_molar_pulp_fdi136&quot;, &quot;lower_left_second_molar_pulp_fdi137&quot;, &quot;lower_left_third_molar_pulp_fdi138&quot;, &quot;lower_right_central_incisor_pulp_fdi141&quot;, &quot;lower_right_lateral_incisor_pulp_fdi142&quot;, &quot;lower_right_canine_pulp_fdi143&quot;, &quot;lower_right_first_premolar_pulp_fdi144&quot;, &quot;lower_right_second_premolar_pulp_fdi145&quot;, &quot;lower_right_first_molar_pulp_fdi146&quot;, &quot;lower_right_second_molar_pulp_fdi147&quot;, &quot;lower_right_third_molar_pulp_fdi148&quot; (based on the ToothFairy3 dataset, cite [paper](https://openaccess.thecvf.com/content/CVPR2025/html/Bolelli_Segmenting_Maxillofacial_Structures_in_CBCT_Volumes_CVPR_2025_paper.html))
* **trunk_cavities**: abdominal_cavity, thoracic_cavity, pericardium, mediastinum

*: These models are not trained on the full totalsegmentator dataset but on some small other datasets. Therefore, expect them to work less robustly.

Available with a license (free licenses available for non-commercial usage [here](https://backend.totalsegmentator.com/license-academic/). For a commercial license contact jakob.wasserthal@usb.ch):
* **heartchambers_highres**: myocardium, atrium_left, ventricle_left, atrium_right, ventricle_right, aorta, pulmonary_artery (trained on sub-millimeter resolution)
* **appendicular_bones**: patella, tibia, fibula, tarsal, metatarsal, phalanges_feet, ulna, radius, carpal, metacarpal, phalanges_hand
* **appendicular_bones_mr**: patella, tibia, fibula, tarsal, metatarsal, phalanges_feet, ulna, radius (for MR images)
* **tissue_types**: subcutaneous_fat, torso_fat, skeletal_muscle
* **tissue_types_mr**: subcutaneous_fat, torso_fat, skeletal_muscle (for MR images)
* **tissue_4_types**: subcutaneous_fat, torso_fat, skeletal_muscle, intermuscular_fat (in contrast to `tissue_types` skeletal_muscle is split into two classes: muscle and fat)
* **brain_structures**: brainstem, subarachnoid_space, venous_sinuses, septum_pellucidum, cerebellum, caudate_nucleus, lentiform_nucleus, insular_cortex, internal_capsule, ventricle, central_sulcus, frontal_lobe, parietal_lobe, occipital_lobe, temporal_lobe, thalamus (NOTE: this is for CT) (cite [paper](https://doi.org/10.1148/ryai.2020190183) as our model is partly based on this)
* **vertebrae_body**: vertebral body of all vertebrae (without the vertebral arch), intervertebral_discs (for MR this is part of the `total_mr` task)
* **face**: face_region (for anonymization)
* **face_mr**: face_region (for anonymization)
* **thigh_shoulder_muscles**: quadriceps_femoris_left, quadriceps_femoris_right, thigh_medial_compartment_left, thigh_medial_compartment_right, thigh_posterior_compartment_left, thigh_posterior_compartment_right, sartorius_left, sartorius_right, deltoid, supraspinatus, infraspinatus, subscapularis, coracobrachial, trapezius, pectoralis_minor, serratus_anterior, teres_major, triceps_brachii
* **thigh_shoulder_muscles_mr**: quadriceps_femoris_left, quadriceps_femoris_right, thigh_medial_compartment_left, thigh_medial_compartment_right, thigh_posterior_compartment_left, thigh_posterior_compartment_right, sartorius_left, sartorius_right, deltoid, supraspinatus, infraspinatus, subscapularis, coracobrachial, trapezius, pectoralis_minor, serratus_anterior, teres_major, triceps_brachii (for MR images)
* **coronary_arteries**: coronary_arteries (also works on non-contrast images)

Usage:
```bash
TotalSegmentator -i ct.nii.gz -o segmentations -ta &lt;task_name&gt;
```

Confused by all the structures and tasks? Check [this](https://backend.totalsegmentator.com/find-task/) to search through available structures and tasks.

The mapping from label ID to class name can be found [here](https://github.com/wasserth/TotalSegmentator/blob/master/totalsegmentator/map_to_binary.py).

If you have a nnU-Net model for some structures not supported yet, you can contribute it. This will enable all TotalSegmentator users to easily use it and at the same time increase the reach of your work by more people citing your paper. Contact jakob.wasserthal@usb.ch.

Thank you to [INGEDATA](https://www.ingedata.ai/) for providing a team of radiologists to support some of the data annotations.


### Advanced settings
* `--device`: Choose `cpu` or `gpu` or `gpu:X (e.g., gpu:1 -&gt; cuda:1)`
* `--fast`: For faster runtime and less memory requirements use this option. It will run a lower resolution model (3mm instead of 1.5mm).
* `--roi_subset`: Takes a space-separated list of class names (e.g. `spleen colon brain`) and only predicts those classes. Saves a lot of runtime and memory. Might be less accurate especially for small classes (e.g. prostate).
* `--robust_crop`: For some tasks and for roi_subset a 6mm low resolution model is used to crop to the region of interest. Sometimes this model is incorrect, which leads to artifacts like segmentations being cut off. robust_crop will use a better but slower 3mm model instead.
* `--preview`: This will generate a 3D rendering of all classes, giving you a quick overview if the segmentation worked and where it failed (see `preview.png` in output directory).
* `--ml`: This will save one nifti file containing all labels instead of one file for each class. Saves runtime during saving of nifti files. (see [here](https://github.com/wasserth/TotalSegmentator#class-details) for index to class name mapping).
* `--statistics`: This will generate a file `statistics.json` with volume (in mmÂ³) and mean intensity of each class.
* `--radiomics`: This will generate a file `statistics_radiomics.json` with the radiomics features of each class. You have to install pyradiomics to use this (`pip install pyradiomics`).
* `--output_type`: This will output the segmentation as DICOM. Supported are `dicom_seg` requires (`pip install highdicom`) and `dicom_rtstruct` requires (`pip install rt_utils`).


### Other commands
If you want to know which contrast phase a CT image is you can use the following command (requires `pip install xgboost`). More details can be found [here](resources/contrast_phase_prediction.md):
```bash
totalseg_get_phase -i ct.nii.gz -o contrast_phase.json
```

If you want to know which modality (CT or MR) an image is you can use the following command (requires `pip install xgboost`). 
```bash
totalseg_get_modality -i image.nii.gz -o modality.json
```

If you want to combine some subclasses (e.g. lung lobes) into one binary mask (e.g. entire lung) you can use the following command:
```bash
totalseg_combine_masks -i totalsegmentator_output_dir -o combined_mask.nii.gz -m lungcomm 
```

If you want to calculate the [Evans index](https://radiopaedia.org/articles/evans-index-2) you can use the following command:
```bash
totalseg_evans_index -i ct_skull.nii.gz -o evans_index.json -p evans_index.png
```

Normally weights are automatically downloaded when running TotalSegmentator. If you want to download the weights with an extra command (e.g. when building a docker container) use this:
```bash
totalseg_download_weights -t &lt;task_name&gt;
```
This will download them to `~/.totalsegmentator/nnunet/results`. You can change this path by doing `export TOTALSEG_HOME_DIR=/new/path/.totalsegmentator`. If your machine has no internet, then download on another machine with internet and copy `~/.totalsegmentator` to the machine without internet.

After acquiring a license number for the non-open tasks you can set it with the following command:
```bash
totalseg_set_license -l aca_12345678910
```

You can output the softmax probabilities. This will give you a `.npz` file you can load with numpy. The geometry
might not be identical to your input image. There will also be a `.pkl` output file with geometry
information. This does not work well for the `total` task since this is based on multiple models.
```bash
TotalSegmentator -i ct.nii.gz -o seg -ta lung_nodules --save_probabilities probs.npz
```

If you do not have internet access on the machine you want to run TotalSegmentator on:
1. Install TotalSegmentator [and set up the license] on a machine with internet.
2. Run TotalSegmentator for one subject on this machine. This will download the weights and save them to `~/.totalsegmentator`.
3. Copy the folder `~/.totalsegmentator` from this machine to the machine without internet.
4. TotalSegmentator should now work also on the machine without internet.


### Web applications
We provide the following web applications to easily process your images:
* [TotalSegmentator](https://totalsegmentator.com/): Run totalsegmentator on your own images via a simple web interface.
* [TotalSegmentator Annotation Platform](https://annotate.totalsegmentator.com/): Help annotate more data to further improve TotalSegmentator.
* [Volume Report](https://compute.totalsegmentator.com/volume-report/): Get the volume of abdominal organs + tissue und bone density. Also show percentile in population.
* [Evans Index](https://compute.totalsegmentator.com/evans-index/): Compute the Evans index.
* [Aorta Report](https://compute.totalsegmentator.com/aorta-report/): Analyse the diameter along the aorta.


### Run via docker
We also provide a docker container which can be used the following way
```bash
docker run --gpus &#039;device=0&#039; --shm-size=16G -v /absolute/path/to/my/data/directory:/tmp wasserth/totalsegmentator:2.11.0 TotalSegmentator -i /tmp/ct.nii.gz -o /tmp/segmentations
```


### Resource Requirements
Totalsegmentator has the following runtime and memory requirements (using an Nvidia RTX 3090 GPU):
(1.5mm is the normal model and 3mm is the `--fast` model. With v2 the runtimes have increased a bit since
we added more classes.)

![Alt text](resources/imgs/runtime_table.png)

If you want to reduce memo

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[OpenHands/OpenHands]]></title>
            <link>https://github.com/OpenHands/OpenHands</link>
            <guid>https://github.com/OpenHands/OpenHands</guid>
            <pubDate>Wed, 10 Dec 2025 00:04:21 GMT</pubDate>
            <description><![CDATA[ğŸ™Œ OpenHands: AI-Driven Development]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/OpenHands/OpenHands">OpenHands/OpenHands</a></h1>
            <p>ğŸ™Œ OpenHands: AI-Driven Development</p>
            <p>Language: Python</p>
            <p>Stars: 65,517</p>
            <p>Forks: 8,029</p>
            <p>Stars today: 28 stars today</p>
            <h2>README</h2><pre>&lt;a name=&quot;readme-top&quot;&gt;&lt;/a&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/OpenHands/docs/main/openhands/static/img/logo.png&quot; alt=&quot;Logo&quot; width=&quot;200&quot;&gt;
  &lt;h1 align=&quot;center&quot; style=&quot;border-bottom: none&quot;&gt;OpenHands: AI-Driven Development&lt;/h1&gt;
&lt;/div&gt;


&lt;div align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/OpenHands/OpenHands/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/LICENSE-MIT-20B2AA?style=for-the-badge&quot; alt=&quot;MIT License&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1wOUdFCMyY6Nt0AIqF705KN4JKOWgeI4wUGUP60krXXs/edit?gid=811504672#gid=811504672&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/SWEBench-72.8-00cc00?logoColor=FFE165&amp;style=for-the-badge&quot; alt=&quot;Benchmark Score&quot;&gt;&lt;/a&gt;
  &lt;br/&gt;
  &lt;a href=&quot;https://docs.openhands.dev/sdk&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Documentation-000?logo=googledocs&amp;logoColor=FFE165&amp;style=for-the-badge&quot; alt=&quot;Check out the documentation&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://arxiv.org/abs/2511.03690&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Paper-000?logoColor=FFE165&amp;logo=arxiv&amp;style=for-the-badge&quot; alt=&quot;Tech Report&quot;&gt;&lt;/a&gt;


  &lt;!-- Keep these links. Translations will automatically update with the README. --&gt;
  &lt;a href=&quot;https://www.readme-i18n.com/OpenHands/OpenHands?lang=de&quot;&gt;Deutsch&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/OpenHands/OpenHands?lang=es&quot;&gt;EspaÃ±ol&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/OpenHands/OpenHands?lang=fr&quot;&gt;franÃ§ais&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/OpenHands/OpenHands?lang=ja&quot;&gt;æ—¥æœ¬èª&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/OpenHands/OpenHands?lang=ko&quot;&gt;í•œêµ­ì–´&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/OpenHands/OpenHands?lang=pt&quot;&gt;PortuguÃªs&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/OpenHands/OpenHands?lang=ru&quot;&gt;Ğ ÑƒÑÑĞºĞ¸Ğ¹&lt;/a&gt; |
  &lt;a href=&quot;https://www.readme-i18n.com/OpenHands/OpenHands?lang=zh&quot;&gt;ä¸­æ–‡&lt;/a&gt;

&lt;/div&gt;

&lt;hr&gt;

ğŸ™ŒÂ Welcome to OpenHands, a [community](COMMUNITY.md) focused on AI-driven development. Weâ€™d love for you to [join us on Slack](https://dub.sh/openhands).

There are a few ways to work with OpenHands:

### OpenHands Software Agent SDK
The SDK is a composable Python library that contains all of our agentic tech. It&#039;s the engine that powers everything else below.

Define agents in code, then run them locally, or scale to 1000s of agents in the cloud.

[Check out the docs](https://docs.openhands.dev/sdk) or [view the source](https://github.com/OpenHands/software-agent-sdk/)

### OpenHands CLI
The CLI is the easiest way to start using OpenHands. The experience will be familiar to anyone who has worked
with e.g. Claude Code or Codex. You can power it with Claude, GPT, or any other LLM.

[Check out the docs](https://docs.openhands.dev/openhands/usage/run-openhands/cli-mode) or [view the source](https://github.com/OpenHands/OpenHands-CLI)

### OpenHands Local GUI
Use the Local GUI for running agents on your laptop. It comes with a REST API and a single-page React application.
The experience will be familiar to anyone who has used Devin or Jules.

[Check out the docs](https://docs.openhands.dev/openhands/usage/run-openhands/local-setup) or view the source in this repo.

### OpenHands Cloud
This is a deployment of OpenHands GUI, running on hosted infrastructure.

You can try it with a free $10 credit by [signing in with your GitHub account](https://app.all-hands.dev).

OpenHands Cloud comes with source-available features and integrations:
- Integrations with Slack, Jira, and Linear
- Multi-user support
- RBAC and permissions
- Collaboration features (e.g., conversation sharing)

### OpenHands Enterprise
Large enterprises can work with us to self-host OpenHands Cloud in their own VPC, via Kubernetes.
OpenHands Enterprise can also work with the CLI and SDK above.

OpenHands Enterprise is source-available--you can see all the source code here in the enterprise/ directory,
but you&#039;ll need to purchase a license if you want to run it for more than one month.

Enterprise contracts also come with extended support and access to our research team.

Learn more at [openhands.dev/enterprise](https://openhands.dev/enterprise)

### Everything Else

Check out our [Product Roadmap](https://github.com/orgs/openhands/projects/1), and feel free to
[open up an issue](https://github.com/OpenHands/OpenHands/issues) if there&#039;s something you&#039;d like to see!

You might also be interested in our [evaluation infrastructure](https://github.com/OpenHands/benchmarks), our [chrome extension](https://github.com/OpenHands/openhands-chrome-extension/), or our [Theory-of-Mind module](https://github.com/OpenHands/ToM-SWE).

All our work is available under the MIT license, except for the `enterprise/` directory in this repository (see the [enterprise license](enterprise/LICENSE) for details).
The core `openhands` and `agent-server` Docker images are fully MIT-licensed as well.

If you need help with anything, or just want to chat, [come find us on Slack](https://dub.sh/openhands).
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[CVHub520/X-AnyLabeling]]></title>
            <link>https://github.com/CVHub520/X-AnyLabeling</link>
            <guid>https://github.com/CVHub520/X-AnyLabeling</guid>
            <pubDate>Wed, 10 Dec 2025 00:04:20 GMT</pubDate>
            <description><![CDATA[Effortless data labeling with AI support from Segment Anything and other awesome models.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/CVHub520/X-AnyLabeling">CVHub520/X-AnyLabeling</a></h1>
            <p>Effortless data labeling with AI support from Segment Anything and other awesome models.</p>
            <p>Language: Python</p>
            <p>Stars: 7,310</p>
            <p>Forks: 816</p>
            <p>Stars today: 23 stars today</p>
            <h2>README</h2><pre>&lt;div align=&quot;center&quot;&gt;
  &lt;p&gt;
    &lt;a href=&quot;https://github.com/CVHub520/X-AnyLabeling/&quot; target=&quot;_blank&quot;&gt;
      &lt;img alt=&quot;X-AnyLabeling&quot; height=&quot;200px&quot; src=&quot;https://github.com/user-attachments/assets/0714a182-92bd-4b47-b48d-1c5d7c225176&quot;&gt;&lt;/a&gt;
  &lt;/p&gt;

[English](README.md) | [ç®€ä½“ä¸­æ–‡](README_zh-CN.md)

&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;a href=&quot;./LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-LGPL%20v3-blue.svg&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/CVHub520/X-AnyLabeling?color=ffa&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/x-anylabeling-cvhub?logo=pypi&amp;logoColor=white&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/python-3.10+-aff.svg&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/downloads/CVHub520/X-AnyLabeling/total?label=downloads&quot;&gt;&lt;/a&gt;
    &lt;a href=&quot;https://modelscope.cn/collections/X-AnyLabeling-7b0e1798bcda43&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/modelscope-X--AnyLabeling-6750FF?link=https%3A%2F%2Fmodelscope.cn%2Fcollections%2FX-AnyLabeling-7b0e1798bcda43&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

![](https://user-images.githubusercontent.com/18329471/234640541-a6a65fbc-d7a5-4ec3-9b65-55305b01a7aa.png)

&lt;img src=&quot;https://github.com/user-attachments/assets/8b5f290a-dddf-410c-a004-21e5a7bcd1cc&quot; width=&quot;100%&quot; /&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Auto-Training&lt;/strong&gt;&lt;/summary&gt;

&lt;video src=&quot;https://github.com/user-attachments/assets/c0ab2056-2743-4a2c-ba93-13f478d3481e&quot; width=&quot;100%&quot; controls&gt;
&lt;/video&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Auto-Labeling&lt;/strong&gt;&lt;/summary&gt;

&lt;video src=&quot;https://github.com/user-attachments/assets/f517fa94-c49c-4f05-864e-96b34f592079&quot; width=&quot;100%&quot; controls&gt;
&lt;/video&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Detect Anything&lt;/strong&gt;&lt;/summary&gt;

&lt;img src=&quot;https://github.com/user-attachments/assets/7f43bcec-96fd-48d1-bd36-9e5a440a66f6&quot; width=&quot;100%&quot; /&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Segment Anything&lt;/strong&gt;&lt;/summary&gt;

&lt;img src=&quot;https://github.com/user-attachments/assets/208dc9ed-b8c9-4127-9e5b-e76f53892f03&quot; width=&quot;100%&quot; /&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Promptable Concept Grounding&lt;/strong&gt;&lt;/summary&gt;

&lt;video src=&quot;https://github.com/user-attachments/assets/52cbdb5d-cc60-4be5-826f-903ea4330ca8&quot; width=&quot;100%&quot; controls&gt;
&lt;/video&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;VQA&lt;/strong&gt;&lt;/summary&gt;

&lt;video src=&quot;https://github.com/user-attachments/assets/53adcff4-b962-41b7-a408-3afecd8d8c82&quot; width=&quot;100%&quot; controls&gt;
&lt;/video&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Chatbot&lt;/strong&gt;&lt;/summary&gt;

&lt;img src=&quot;https://github.com/user-attachments/assets/56c9a20b-c836-47aa-8b54-bad5bb99b735&quot; width=&quot;100%&quot; /&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;strong&gt;Image Classifier&lt;/strong&gt;&lt;/summary&gt;

&lt;video src=&quot;https://github.com/user-attachments/assets/0652adfb-48a4-4219-9b18-16ff5ce31be0&quot; width=&quot;100%&quot; controls&gt;
&lt;/video&gt;
&lt;/details&gt;

## ğŸ¥³ What&#039;s New

- Add support for [Segment Anything 3](./examples/grounding/sam3/README.md) model with text and visual promptable segmentation (#1207)
- Add TinyObj mode for Segment Anything Model to improve small object detection accuracy in high-resolution images by local cropping (#1193)
- For more details, please refer to the [CHANGELOG](./CHANGELOG.md)

## X-AnyLabeling

**X-AnyLabeling** is a powerful annotation tool that integrates an AI engine for fast and automatic labeling. It&#039;s designed for multi-modal data engineers, offering industrial-grade solutions for complex tasks.

&lt;img src=&quot;https://github.com/user-attachments/assets/632e629b-0dec-407b-95a6-728052e1dd7b&quot; width=&quot;100%&quot; /&gt;

Also, we highly recommend trying out [X-AnyLabeling-Server](https://github.com/CVHub520/X-AnyLabeling-Server), a simple, lightweight, and extensible framework that enables remote inference capabilities for X-AnyLabeling.

## Features

&lt;img src=&quot;https://github.com/user-attachments/assets/c65db18f-167b-49e8-bea3-fcf4b43a8ffd&quot; width=&quot;100%&quot; /&gt;

- Supports remote inference service.
- Processes both `images` and `videos`.
- Accelerates inference with `GPU` support.
- Allows custom models and secondary development.
- Supports one-click inference for all images in the current task.
- Supports import/export for formats like COCO, VOC, YOLO, DOTA, MOT, MASK, PPOCR, MMGD, VLM-R1.
- Handles tasks like `classification`, `detection`, `segmentation`, `caption`, `rotation`, `tracking`, `estimation`, `ocr`, `vqa`, `grounding` and so on.
- Supports diverse annotation styles: `polygons`, `rectangles`, `rotated boxes`, `circles`, `lines`, `points`, and annotations for `text detection`, `recognition`, and `KIE`.

### Model library

&lt;img src=&quot;https://github.com/user-attachments/assets/7da2da2e-f182-4a1b-85f6-bfd0dfcc6a1b&quot; width=&quot;100%&quot; /&gt;

| **Task Category** | **Supported Models** |
| :--- | :--- |
| ğŸ–¼ï¸ Image Classification | YOLOv5-Cls, YOLOv8-Cls, YOLO11-Cls, InternImage, PULC |
| ğŸ¯ Object Detection | YOLOv5/6/7/8/9/10, YOLO11/12, YOLOX, YOLO-NAS, D-FINE, DAMO-YOLO, Gold_YOLO, RT-DETR, RF-DETR, DEIMv2 |
| ğŸ–Œï¸ Instance Segmentation | YOLOv5-Seg, YOLOv8-Seg, YOLO11-Seg, Hyper-YOLO-Seg, RF-DETR-Seg |
| ğŸƒ Pose Estimation | YOLOv8-Pose, YOLO11-Pose, DWPose, RTMO |
| ğŸ‘£ Tracking | Bot-SORT, ByteTrack |
| ğŸ”„ Rotated Object Detection | YOLOv5-Obb, YOLOv8-Obb, YOLO11-Obb |
| ğŸ“ Depth Estimation | Depth Anything |
| ğŸ§© Segment Anything | SAM 1/2/3, SAM-HQ, SAM-Med2D, EdgeSAM, EfficientViT-SAM, MobileSAM |
| âœ‚ï¸ Image Matting | RMBG 1.4/2.0 |
| ğŸ’¡ Proposal | UPN |
| ğŸ·ï¸ Tagging | RAM, RAM++ |
| ğŸ“„ OCR | PP-OCRv4, PP-OCRv5 |
| ğŸ—£ï¸ Vision Foundation Models | Florence2 |
| ğŸ‘ï¸ Vision Language Models | Qwen3-VL, Gemini, ChatGPT |
| ğŸ›£ï¸ Land Detection | CLRNet |
| ğŸ“ Grounding | CountGD, GeCO, Grounding DINO, YOLO-World, YOLOE |
| ğŸ“š Other | ğŸ‘‰ [model_zoo](./docs/en/model_zoo.md) ğŸ‘ˆ |

## Docs

0. [Remote Inference Service](https://github.com/CVHub520/X-AnyLabeling-Server)
1. [Installation &amp; Quickstart](./docs/en/get_started.md)
2. [Usage](./docs/en/user_guide.md)
3. [Command Line Interface](./docs/en/cli.md)
4. [Customize a model](./docs/en/custom_model.md)
5. [Chatbot](./docs/en/chatbot.md)
6. [VQA](./docs/en/vqa.md)
7. [Multi-class Image Classifier](./docs/en/image_classifier.md)

&lt;img src=&quot;https://github.com/user-attachments/assets/0d67311c-f441-44b6-9ee0-932f25f51b1c&quot; width=&quot;100%&quot; /&gt;

## Examples

- [Classification](./examples/classification/)
  - [Image-Level](./examples/classification/image-level/README.md)
  - [Shape-Level](./examples/classification/shape-level/README.md)
- [Detection](./examples/detection/)
  - [HBB Object Detection](./examples/detection/hbb/README.md)
  - [OBB Object Detection](./examples/detection/obb/README.md)
- [Segmentation](./examples/segmentation/README.md)
  - [Instance Segmentation](./examples/segmentation/instance_segmentation/)
  - [Binary Semantic Segmentation](./examples/segmentation/binary_semantic_segmentation/)
  - [Multiclass Semantic Segmentation](./examples/segmentation/multiclass_semantic_segmentation/)
- [Description](./examples/description/)
  - [Tagging](./examples/description/tagging/README.md)
  - [Captioning](./examples/description/captioning/README.md)
- [Estimation](./examples/estimation/)
  - [Pose Estimation](./examples/estimation/pose_estimation/README.md)
  - [Depth Estimation](./examples/estimation/depth_estimation/README.md)
- [OCR](./examples/optical_character_recognition/)
  - [Text Recognition](./examples/optical_character_recognition/text_recognition/)
  - [Key Information Extraction](./examples/optical_character_recognition/key_information_extraction/README.md)
- [MOT](./examples/multiple_object_tracking/README.md)
  - [Tracking by HBB Object Detection](./examples/multiple_object_tracking/README.md)
  - [Tracking by OBB Object Detection](./examples/multiple_object_tracking/README.md)
  - [Tracking by Instance Segmentation](./examples/multiple_object_tracking/README.md)
  - [Tracking by Pose Estimation](./examples/multiple_object_tracking/README.md)
- [iVOS](./examples/interactive_video_object_segmentation/README.md)
- [Matting](./examples/matting/)
  - [Image Matting](./examples/matting/image_matting/README.md)
- [Vision-Language](./examples/vision_language/)
  - [Florence 2](./examples/vision_language/florence2/README.md)
- [Counting](./examples/counting/)
  - [GeCo](./examples/counting/geco/README.md)
- [Grounding](./examples/grounding/)
  - [YOLOE](./examples/grounding/yoloe/README.md)
  - [SAM 3](./examples/grounding/sam3/README.md)
- [Training](./examples/training/)
  - [Ultralytics](./examples/training/ultralytics/README.md)


## Contribute

We believe in open collaboration! **Xâ€‘AnyLabeling** continues to grow with the support of the community. Whether you&#039;re fixing bugs, improving documentation, or adding new features, your contributions make a real impact.

To get started, please read our [Contributing Guide](./CONTRIBUTING.md) and make sure to agree to the [Contributor License Agreement (CLA)](./CLA.md) before submitting a pull request.

If you find this project helpful, please consider giving it a â­ï¸ star! Have questions or suggestions? Open an [issue](https://github.com/CVHub520/X-AnyLabeling/issues) or email us at cv_hub@163.com.

A huge thank you ğŸ™ to everyone helping to make Xâ€‘AnyLabeling better.

## License

This project is licensed under the [GPL-3.0 license](./LICENSE) and is completely open source and free. The original intention is to enable more developers, researchers, and enterprises to conveniently use this AI application platform, promoting the development of the entire industry. We encourage everyone to use it freely (including commercial use), and you can also add features based on this project and commercialize it, but you must retain the brand identity and indicate the source project address.

Additionally, to understand the ecosystem and usage of X-AnyLabeling, if you use this project for academic, research, teaching, or enterprise purposes, please fill out the [registration form](https://forms.gle/MZCKhU7UJ4TRSWxR7). This registration is only for statistical purposes and will not incur any fees. We will strictly keep all information confidential.

X-AnyLabeling is independently developed and maintained by an individual. If this project has been helpful to you, we welcome your support through the donation links below to help sustain the project&#039;s continued development. Your support is the greatest encouragement! If you have any questions about the project or would like to collaborate, please feel free to contact via WeChat: ww10874 or email provided above.

## Sponsors

- [buy-me-a-coffee](https://ko-fi.com/cvhub520)
- [Wechat/Alipay](https://github.com/CVHub520/X-AnyLabeling/blob/main/README_zh-CN.md#%E8%B5%9E%E5%8A%A9)

## Acknowledgement

I extend my heartfelt thanks to the developers and contributors of [AnyLabeling](https://github.com/vietanhdev/anylabeling), [LabelMe](https://github.com/wkentaro/labelme), [LabelImg](https://github.com/tzutalin/labelImg), [roLabelImg](https://github.com/cgvict/roLabelImg), [PPOCRLabel](https://github.com/PFCCLab/PPOCRLabel) and [CVAT](https://github.com/opencv/cvat), whose work has been crucial to the success of this project.

## Citing

If you use this software in your research, please cite it as below:

```
@misc{X-AnyLabeling,
  year = {2023},
  author = {Wei Wang},
  publisher = {Github},
  organization = {CVHub},
  journal = {Github repository},
  title = {Advanced Auto Labeling Solution with Added Features},
  howpublished = {\url{https://github.com/CVHub520/X-AnyLabeling}}
}
```

---

![Star History Chart](https://api.star-history.com/svg?repos=CVHub520/X-AnyLabeling&amp;type=Date)

&lt;div align=&quot;center&quot;&gt;&lt;a href=&quot;#top&quot;&gt;ğŸ” Back to Top&lt;/a&gt;&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[BeehiveInnovations/pal-mcp-server]]></title>
            <link>https://github.com/BeehiveInnovations/pal-mcp-server</link>
            <guid>https://github.com/BeehiveInnovations/pal-mcp-server</guid>
            <pubDate>Wed, 10 Dec 2025 00:04:19 GMT</pubDate>
            <description><![CDATA[The power of Claude Code / GeminiCLI / CodexCLI + [Gemini / OpenAI / OpenRouter / Azure / Grok / Ollama / Custom Model / All Of The Above] working as one.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/BeehiveInnovations/pal-mcp-server">BeehiveInnovations/pal-mcp-server</a></h1>
            <p>The power of Claude Code / GeminiCLI / CodexCLI + [Gemini / OpenAI / OpenRouter / Azure / Grok / Ollama / Custom Model / All Of The Above] working as one.</p>
            <p>Language: Python</p>
            <p>Stars: 10,367</p>
            <p>Forks: 872</p>
            <p>Stars today: 77 stars today</p>
            <h2>README</h2><pre># PAL MCP: Many Workflows. One Context.

&lt;div align=&quot;center&quot;&gt;

  &lt;em&gt;Your AI&#039;s PAL â€“ a Provider Abstraction Layer&lt;/em&gt;&lt;br /&gt;
  &lt;sub&gt;&lt;a href=&quot;docs/name-change.md&quot;&gt;Formerly known as Zen MCP&lt;/a&gt;&lt;/sub&gt;

  [PAL in action](https://github.com/user-attachments/assets/0d26061e-5f21-4ab1-b7d0-f883ddc2c3da)

ğŸ‘‰ **[Watch more examples](#-watch-tools-in-action)**

### Your CLI + Multiple Models = Your AI Dev Team

**Use the ğŸ¤– CLI you love:**  
[Claude Code](https://www.anthropic.com/claude-code) Â· [Gemini CLI](https://github.com/google-gemini/gemini-cli) Â· [Codex CLI](https://github.com/openai/codex) Â· [Qwen Code CLI](https://qwenlm.github.io/qwen-code-docs/) Â· [Cursor](https://cursor.com) Â· _and more_

**With multiple models within a single prompt:**  
Gemini Â· OpenAI Â· Anthropic Â· Grok Â· Azure Â· Ollama Â· OpenRouter Â· DIAL Â· On-Device Model

&lt;/div&gt;

---

## ğŸ†• Now with CLI-to-CLI Bridge

The new **[`clink`](docs/tools/clink.md)** (CLI + Link) tool connects external AI CLIs directly into your workflow:

- **Connect external CLIs** like [Gemini CLI](https://github.com/google-gemini/gemini-cli), [Codex CLI](https://github.com/openai/codex), and [Claude Code](https://www.anthropic.com/claude-code) directly into your workflow
- **CLI Subagents** - Launch isolated CLI instances from _within_ your current CLI! Claude Code can spawn Codex subagents, Codex can spawn Gemini CLI subagents, etc. Offload heavy tasks (code reviews, bug hunting) to fresh contexts while your main session&#039;s context window remains unpolluted. Each subagent returns only final results.
- **Context Isolation** - Run separate investigations without polluting your primary workspace
- **Role Specialization** - Spawn `planner`, `codereviewer`, or custom role agents with specialized system prompts
- **Full CLI Capabilities** - Web search, file inspection, MCP tool access, latest documentation lookups
- **Seamless Continuity** - Sub-CLIs participate as first-class members with full conversation context between tools

```bash
# Codex spawns Codex subagent for isolated code review in fresh context
clink with codex codereviewer to audit auth module for security issues
# Subagent reviews in isolation, returns final report without cluttering your context as codex reads each file and walks the directory structure

# Consensus from different AI models â†’ Implementation handoff with full context preservation between tools
Use consensus with gpt-5 and gemini-pro to decide: dark mode or offline support next
Continue with clink gemini - implement the recommended feature
# Gemini receives full debate context and starts coding immediately
```

ğŸ‘‰ **[Learn more about clink](docs/tools/clink.md)**

---

## Why PAL MCP?

**Why rely on one AI model when you can orchestrate them all?**

A Model Context Protocol server that supercharges tools like [Claude Code](https://www.anthropic.com/claude-code), [Codex CLI](https://developers.openai.com/codex/cli), and IDE clients such
as [Cursor](https://cursor.com) or the [Claude Dev VS Code extension](https://marketplace.visualstudio.com/items?itemName=Anthropic.claude-vscode). **PAL MCP connects your favorite AI tool
to multiple AI models** for enhanced code analysis, problem-solving, and collaborative development.

### True AI Collaboration with Conversation Continuity

PAL supports **conversation threading** so your CLI can **discuss ideas with multiple AI models, exchange reasoning, get second opinions, and even run collaborative debates between models** to help you reach deeper insights and better solutions.

Your CLI always stays in control but gets perspectives from the best AI for each subtask. Context carries forward seamlessly across tools and models, enabling complex workflows like: code reviews with multiple models â†’ automated planning â†’ implementation â†’ pre-commit validation.

&gt; **You&#039;re in control.** Your CLI of choice orchestrates the AI team, but you decide the workflow. Craft powerful prompts that bring in Gemini Pro, GPT 5, Flash, or local offline models exactly when needed.

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Reasons to Use PAL MCP&lt;/b&gt;&lt;/summary&gt;

A typical workflow with Claude Code as an example:

1. **Multi-Model Orchestration** - Claude coordinates with Gemini Pro, O3, GPT-5, and 50+ other models to get the best analysis for each task

2. **Context Revival Magic** - Even after Claude&#039;s context resets, continue conversations seamlessly by having other models &quot;remind&quot; Claude of the discussion

3. **Guided Workflows** - Enforces systematic investigation phases that prevent rushed analysis and ensure thorough code examination

4. **Extended Context Windows** - Break Claude&#039;s limits by delegating to Gemini (1M tokens) or O3 (200K tokens) for massive codebases

5. **True Conversation Continuity** - Full context flows across tools and models - Gemini remembers what O3 said 10 steps ago

6. **Model-Specific Strengths** - Extended thinking with Gemini Pro, blazing speed with Flash, strong reasoning with O3, privacy with local Ollama

7. **Professional Code Reviews** - Multi-pass analysis with severity levels, actionable feedback, and consensus from multiple AI experts

8. **Smart Debugging Assistant** - Systematic root cause analysis with hypothesis tracking and confidence levels

9. **Automatic Model Selection** - Claude intelligently picks the right model for each subtask (or you can specify)

10. **Vision Capabilities** - Analyze screenshots, diagrams, and visual content with vision-enabled models

11. **Local Model Support** - Run Llama, Mistral, or other models locally for complete privacy and zero API costs

12. **Bypass MCP Token Limits** - Automatically works around MCP&#039;s 25K limit for large prompts and responses

**The Killer Feature:** When Claude&#039;s context resets, just ask to &quot;continue with O3&quot; - the other model&#039;s response magically revives Claude&#039;s understanding without re-ingesting documents!

#### Example: Multi-Model Code Review Workflow

1. `Perform a codereview using gemini pro and o3 and use planner to generate a detailed plan, implement the fixes and do a final precommit check by continuing from the previous codereview`
2. This triggers a [`codereview`](docs/tools/codereview.md) workflow where Claude walks the code, looking for all kinds of issues
3. After multiple passes, collects relevant code and makes note of issues along the way
4. Maintains a `confidence` level between `exploring`, `low`, `medium`, `high` and `certain` to track how confidently it&#039;s been able to find and identify issues
5. Generates a detailed list of critical -&gt; low issues
6. Shares the relevant files, findings, etc with **Gemini Pro** to perform a deep dive for a second [`codereview`](docs/tools/codereview.md)
7. Comes back with a response and next does the same with o3, adding to the prompt if a new discovery comes to light
8. When done, Claude takes in all the feedback and combines a single list of all critical -&gt; low issues, including good patterns in your code. The final list includes new findings or revisions in case Claude misunderstood or missed something crucial and one of the other models pointed this out
9. It then uses the [`planner`](docs/tools/planner.md) workflow to break the work down into simpler steps if a major refactor is required
10. Claude then performs the actual work of fixing highlighted issues
11. When done, Claude returns to Gemini Pro for a [`precommit`](docs/tools/precommit.md) review

All within a single conversation thread! Gemini Pro in step 11 _knows_ what was recommended by O3 in step 7! Taking that context
and review into consideration to aid with its final pre-commit review.

**Think of it as Claude Code _for_ Claude Code.** This MCP isn&#039;t magic. It&#039;s just **super-glue**.

&gt; **Remember:** Claude stays in full control â€” but **YOU** call the shots.
&gt; PAL is designed to have Claude engage other models only when needed â€” and to follow through with meaningful back-and-forth.
&gt; **You&#039;re** the one who crafts the powerful prompt that makes Claude bring in Gemini, Flash, O3 â€” or fly solo.
&gt; You&#039;re the guide. The prompter. The puppeteer.
&gt; #### You are the AI - **Actually Intelligent**.
&lt;/details&gt;

#### Recommended AI Stack

&lt;details&gt;
&lt;summary&gt;For Claude Code Users&lt;/summary&gt;

For best results when using [Claude Code](https://claude.ai/code):  

- **Sonnet 4.5** - All agentic work and orchestration
- **Gemini 3.0 Pro** OR **GPT-5-Pro** - Deep thinking, additional code reviews, debugging and validations, pre-commit analysis
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;For Codex Users&lt;/summary&gt;

For best results when using [Codex CLI](https://developers.openai.com/codex/cli):  

- **GPT-5 Codex Medium** - All agentic work and orchestration
- **Gemini 3.0 Pro** OR **GPT-5-Pro** - Deep thinking, additional code reviews, debugging and validations, pre-commit analysis
&lt;/details&gt;

## Quick Start (5 minutes)

**Prerequisites:** Python 3.10+, Git, [uv installed](https://docs.astral.sh/uv/getting-started/installation/)

**1. Get API Keys** (choose one or more):
- **[OpenRouter](https://openrouter.ai/)** - Access multiple models with one API
- **[Gemini](https://makersuite.google.com/app/apikey)** - Google&#039;s latest models
- **[OpenAI](https://platform.openai.com/api-keys)** - O3, GPT-5 series
- **[Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/)** - Enterprise deployments of GPT-4o, GPT-4.1, GPT-5 family
- **[X.AI](https://console.x.ai/)** - Grok models
- **[DIAL](https://dialx.ai/)** - Vendor-agnostic model access
- **[Ollama](https://ollama.ai/)** - Local models (free)

**2. Install** (choose one):

**Option A: Clone and Automatic Setup** (recommended)
```bash
git clone https://github.com/BeehiveInnovations/pal-mcp-server.git
cd pal-mcp-server

# Handles everything: setup, config, API keys from system environment. 
# Auto-configures Claude Desktop, Claude Code, Gemini CLI, Codex CLI, Qwen CLI
# Enable / disable additional settings in .env
./run-server.sh  
```

**Option B: Instant Setup with [uvx](https://docs.astral.sh/uv/getting-started/installation/)**
```json
// Add to ~/.claude/settings.json or .mcp.json
// Don&#039;t forget to add your API keys under env
{
  &quot;mcpServers&quot;: {
    &quot;pal&quot;: {
      &quot;command&quot;: &quot;bash&quot;,
      &quot;args&quot;: [&quot;-c&quot;, &quot;for p in $(which uvx 2&gt;/dev/null) $HOME/.local/bin/uvx /opt/homebrew/bin/uvx /usr/local/bin/uvx uvx; do [ -x \&quot;$p\&quot; ] &amp;&amp; exec \&quot;$p\&quot; --from git+https://github.com/BeehiveInnovations/pal-mcp-server.git pal-mcp-server; done; echo &#039;uvx not found&#039; &gt;&amp;2; exit 1&quot;],
      &quot;env&quot;: {
        &quot;PATH&quot;: &quot;/usr/local/bin:/usr/bin:/bin:/opt/homebrew/bin:~/.local/bin&quot;,
        &quot;GEMINI_API_KEY&quot;: &quot;your-key-here&quot;,
        &quot;DISABLED_TOOLS&quot;: &quot;analyze,refactor,testgen,secaudit,docgen,tracer&quot;,
        &quot;DEFAULT_MODEL&quot;: &quot;auto&quot;
      }
    }
  }
}
```

**3. Start Using!**
```
&quot;Use pal to analyze this code for security issues with gemini pro&quot;
&quot;Debug this error with o3 and then get flash to suggest optimizations&quot;
&quot;Plan the migration strategy with pal, get consensus from multiple models&quot;
&quot;clink with cli_name=\&quot;gemini\&quot; role=\&quot;planner\&quot; to draft a phased rollout plan&quot;
```

ğŸ‘‰ **[Complete Setup Guide](docs/getting-started.md)** with detailed installation, configuration for Gemini / Codex / Qwen, and troubleshooting
ğŸ‘‰ **[Cursor &amp; VS Code Setup](docs/getting-started.md#ide-clients)** for IDE integration instructions
ğŸ“º **[Watch tools in action](#-watch-tools-in-action)** to see real-world examples

## Provider Configuration

PAL activates any provider that has credentials in your `.env`. See `.env.example` for deeper customization.

## Core Tools

&gt; **Note:** Each tool comes with its own multi-step workflow, parameters, and descriptions that consume valuable context window space even when not in use. To optimize performance, some tools are disabled by default. See [Tool Configuration](#tool-configuration) below to enable them.

**Collaboration &amp; Planning** *(Enabled by default)*
- **[`clink`](docs/tools/clink.md)** - Bridge requests to external AI CLIs (Gemini planner, codereviewer, etc.)
- **[`chat`](docs/tools/chat.md)** - Brainstorm ideas, get second opinions, validate approaches. With capable models (GPT-5 Pro, Gemini 3.0 Pro), generates complete code / implementation
- **[`thinkdeep`](docs/tools/thinkdeep.md)** - Extended reasoning, edge case analysis, alternative perspectives
- **[`planner`](docs/tools/planner.md)** - Break down complex projects into structured, actionable plans
- **[`consensus`](docs/tools/consensus.md)** - Get expert opinions from multiple AI models with stance steering

**Code Analysis &amp; Quality**
- **[`debug`](docs/tools/debug.md)** - Systematic investigation and root cause analysis
- **[`precommit`](docs/tools/precommit.md)** - Validate changes before committing, prevent regressions
- **[`codereview`](docs/tools/codereview.md)** - Professional reviews with severity levels and actionable feedback
- **[`analyze`](docs/tools/analyze.md)** *(disabled by default - [enable](#tool-configuration))* - Understand architecture, patterns, dependencies across entire codebases

**Development Tools** *(Disabled by default - [enable](#tool-configuration))*
- **[`refactor`](docs/tools/refactor.md)** - Intelligent code refactoring with decomposition focus
- **[`testgen`](docs/tools/testgen.md)** - Comprehensive test generation with edge cases
- **[`secaudit`](docs/tools/secaudit.md)** - Security audits with OWASP Top 10 analysis
- **[`docgen`](docs/tools/docgen.md)** - Generate documentation with complexity analysis

**Utilities**
- **[`apilookup`](docs/tools/apilookup.md)** - Forces current-year API/SDK documentation lookups in a sub-process (saves tokens within the current context window), prevents outdated training data responses
- **[`challenge`](docs/tools/challenge.md)** - Prevent &quot;You&#039;re absolutely right!&quot; responses with critical analysis
- **[`tracer`](docs/tools/tracer.md)** *(disabled by default - [enable](#tool-configuration))* - Static analysis prompts for call-flow mapping

&lt;details&gt;
&lt;summary&gt;&lt;b id=&quot;tool-configuration&quot;&gt;ğŸ‘‰ Tool Configuration&lt;/b&gt;&lt;/summary&gt;

### Default Configuration

To optimize context window usage, only essential tools are enabled by default:

**Enabled by default:**
- `chat`, `thinkdeep`, `planner`, `consensus` - Core collaboration tools
- `codereview`, `precommit`, `debug` - Essential code quality tools
- `apilookup` - Rapid API/SDK information lookup
- `challenge` - Critical thinking utility

**Disabled by default:**
- `analyze`, `refactor`, `testgen`, `secaudit`, `docgen`, `tracer`

### Enabling Additional Tools

To enable additional tools, remove them from the `DISABLED_TOOLS` list:

**Option 1: Edit your .env file**
```bash
# Default configuration (from .env.example)
DISABLED_TOOLS=analyze,refactor,testgen,secaudit,docgen,tracer

# To enable specific tools, remove them from the list
# Example: Enable analyze tool
DISABLED_TOOLS=refactor,testgen,secaudit,docgen,tracer

# To enable ALL tools
DISABLED_TOOLS=
```

**Option 2: Configure in MCP settings**
```json
// In ~/.claude/settings.json or .mcp.json
{
  &quot;mcpServers&quot;: {
    &quot;pal&quot;: {
      &quot;env&quot;: {
        // Tool configuration
        &quot;DISABLED_TOOLS&quot;: &quot;refactor,testgen,secaudit,docgen,tracer&quot;,
        &quot;DEFAULT_MODEL&quot;: &quot;pro&quot;,
        &quot;DEFAULT_THINKING_MODE_THINKDEEP&quot;: &quot;high&quot;,
        
        // API configuration
        &quot;GEMINI_API_KEY&quot;: &quot;your-gemini-key&quot;,
        &quot;OPENAI_API_KEY&quot;: &quot;your-openai-key&quot;,
        &quot;OPENROUTER_API_KEY&quot;: &quot;your-openrouter-key&quot;,
        
        // Logging and performance
        &quot;LOG_LEVEL&quot;: &quot;INFO&quot;,
        &quot;CONVERSATION_TIMEOUT_HOURS&quot;: &quot;6&quot;,
        &quot;MAX_CONVERSATION_TURNS&quot;: &quot;50&quot;
      }
    }
  }
}
```

**Option 3: Enable all tools**
```json
// Remove or empty the DISABLED_TOOLS to enable everything
{
  &quot;mcpServers&quot;: {
    &quot;pal&quot;: {
      &quot;env&quot;: {
        &quot;DISABLED_TOOLS&quot;: &quot;&quot;
      }
    }
  }
}
```

**Note:**
- Essential tools (`version`, `listmodels`) cannot be disabled
- After changing tool configuration, restart your Claude session for changes to take effect
- Each tool adds to context window usage, so only enable what you need

&lt;/details&gt;

## ğŸ“º Watch Tools In Action

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Chat Tool&lt;/b&gt; - Collaborative decision making and multi-turn conversations&lt;/summary&gt;

**Picking Redis vs Memcached:**

[Chat Redis or Memcached_web.webm](https://github.com/user-attachments/assets/41076cfe-dd49-4dfc-82f5-d7461b34705d)

**Multi-turn conversation with continuation:**

[Chat With Gemini_web.webm](https://github.com/user-attachments/assets/37bd57ca-e8a6-42f7-b5fb-11de271e95db)

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Consensus Tool&lt;/b&gt; - Multi-model debate and decision making&lt;/summary&gt;

**Multi-model consensus debate:**

[PAL Consensus Debate](https://github.com/user-attachments/assets/76a23dd5-887a-4382-9cf0-642f5cf6219e)

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;PreCommit Tool&lt;/b&gt; - Comprehensive change validation&lt;/summary&gt;

**Pre-commit validation workflow:**

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://github.com/user-attachments/assets/584adfa6-d252-49b4-b5b0-0cd6e97fb2c6&quot; width=&quot;950&quot;&gt;
&lt;/div&gt;

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;API Lookup Tool&lt;/b&gt; - Current vs outdated API documentation&lt;/summary&gt;

**Without PAL - outdated APIs:**

[API without PAL](https://github.com/user-attachments/assets/01a79dc9-ad16-4264-9ce1-76a56c3580ee)

**With PAL - current APIs:**

[API with PAL](https://github.com/user-attachments/assets/5c847326-4b66-41f7-8f30-f380453dce22)

&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;&lt;b&gt;Challenge Tool&lt;/b&gt; - Critical thinking vs reflexive agreement&lt;/summary&gt;

**Without PAL:**

![without_pal@2x](https://github.com/user-attachments/assets/64f3c9fb-7ca9-4876-b687-25e847edfd87)

**With PAL:**

![with_pal@2x](https://github.com/user-attachments/assets/9d72f444-ba53-4ab1-83e5-250062c6ee70)

&lt;/details&gt;

## Key Features

**AI Orchestration**
- **Auto model selection** - Claude picks the right AI for each task
- **Multi-model workflows** - Chain different models in single conversations
- **Conversation continuity** - Context preserved across tools and models
- **[Context revival](docs/context-revival.md)** - Continue conversations even after context resets

**Model Support**
- **Multiple providers** - Gemini, OpenAI, Azure, X.AI, OpenRouter, DIAL, Ollama
- **Latest models** - GPT-5, Gemini 3.0 Pro, O3, Grok-4, local Llama
- **[Thinking modes](docs/advanced-usage.md#thinking-modes)** - Control reasoning depth vs cost
- **Vision support** - Analyze images, diagrams, screenshots

**Developer Experience**
- **Guided workflows** - Systematic investigation prevents rushed analysis
- **Smart file handling** - Auto-expand directories, manage token limits
- **Web search integration** - Access current documentation and best practices
- **[Large prompt support](docs/advanced-usage.md#working-with-large-prompts)** - Bypass MCP&#039;s 25K token limit

## Example Workflows

**Multi-model Code Review:**
```
&quot;Perform a codereview using gemini pro and o3, then use planner to create a fix strategy&quot;
```
â†’ Claude reviews code systematically â†’ Consults Gemini Pro â†’ Gets O3&#039;s perspective â†’ Creates unified action plan

**Collaborative Debugging:**
```
&quot;Debug this race condition with max thinking mode, then validate the fix with precommit&quot;
```
â†’ Deep investigation â†’ Expert analysis â†’ Solution implementation â†’ Pre-commit validation

**Architecture Planning:**
```
&quot;Plan our microservices migration, get consensus from pro and o3 on the approach&quot;
```
â†’ Structured planning â†’ Multiple expert opinions â†’ Consensus building â†’ Implementation roadmap

ğŸ‘‰ **[Advanced Usage Guide](docs/advanced-usage.md)** for complex workflows, model configuration, and power-user features

## Quick Links

**ğŸ“– Documentation**
- [Docs Overview](docs/index.md) - High-level map of major guides
- [Getting Started](docs/getting-started.md) - Complete setup guide
- [Tools Reference](docs/tools/) - All tools with examples
- [Advanced Usage](docs/advanced-usage.md) - Power user features
- [Configuration](docs/configuration.md) - Environment variables, restrictions
- [Adding Providers](docs/adding_provi

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
        <item>
            <title><![CDATA[modelscope/DiffSynth-Studio]]></title>
            <link>https://github.com/modelscope/DiffSynth-Studio</link>
            <guid>https://github.com/modelscope/DiffSynth-Studio</guid>
            <pubDate>Wed, 10 Dec 2025 00:04:18 GMT</pubDate>
            <description><![CDATA[Enjoy the magic of Diffusion models!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/modelscope/DiffSynth-Studio">modelscope/DiffSynth-Studio</a></h1>
            <p>Enjoy the magic of Diffusion models!</p>
            <p>Language: Python</p>
            <p>Stars: 10,945</p>
            <p>Forks: 1,029</p>
            <p>Stars today: 24 stars today</p>
            <h2>README</h2><pre># DiffSynth-Studio

&lt;a href=&quot;https://github.com/modelscope/DiffSynth-Studio&quot;&gt;&lt;img src=&quot;.github/workflows/logo.gif&quot; title=&quot;Logo&quot; style=&quot;max-width:100%;&quot; width=&quot;55&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://trendshift.io/repositories/10946&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/10946&quot; alt=&quot;modelscope%2FDiffSynth-Studio | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;&lt;/p&gt;

[![PyPI](https://img.shields.io/pypi/v/DiffSynth)](https://pypi.org/project/DiffSynth/)
[![license](https://img.shields.io/github/license/modelscope/DiffSynth-Studio.svg)](https://github.com/modelscope/DiffSynth-Studio/blob/master/LICENSE)
[![open issues](https://isitmaintained.com/badge/open/modelscope/DiffSynth-Studio.svg)](https://github.com/modelscope/DiffSynth-Studio/issues)
[![GitHub pull-requests](https://img.shields.io/github/issues-pr/modelscope/DiffSynth-Studio.svg)](https://GitHub.com/modelscope/DiffSynth-Studio/pull/)
[![GitHub latest commit](https://badgen.net/github/last-commit/modelscope/DiffSynth-Studio)](https://GitHub.com/modelscope/DiffSynth-Studio/commit/)

[åˆ‡æ¢åˆ°ä¸­æ–‡ç‰ˆ](./README_zh.md)

## Introduction

Welcome to the magical world of Diffusion models! DiffSynth-Studio is an open-source Diffusion model engine developed and maintained by the [ModelScope Community](https://www.modelscope.cn/). We hope to foster technological innovation through framework construction, aggregate the power of the open-source community, and explore the boundaries of generative model technology!

DiffSynth currently includes two open-source projects:
* [DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio): Focused on aggressive technical exploration, targeting academia, and providing cutting-edge model capability support.
* [DiffSynth-Engine](https://github.com/modelscope/DiffSynth-Engine): Focused on stable model deployment, targeting industry, and providing higher computational performance and more stable features.

[DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio) and [DiffSynth-Engine](https://github.com/modelscope/DiffSynth-Engine) are the core engines of the ModelScope AIGC zone. Welcome to experience our carefully crafted productized features:

* ModelScope AIGC Zone (for Chinese users): https://modelscope.cn/aigc/home
* ModelScope Civision (for global users): https://modelscope.ai/civision/home

&gt; DiffSynth-Studio Documentation: [ä¸­æ–‡ç‰ˆ](/docs/zh/README.md)ã€[English version](/docs/en/README.md)

We believe that a well-developed open-source code framework can lower the threshold for technical exploration. We have achieved many [interesting technologies](#innovative-achievements) based on this codebase. Perhaps you also have many wild ideas, and with DiffSynth-Studio, you can quickly realize these ideas. For this reason, we have prepared detailed documentation for developers. We hope that through these documents, developers can understand the principles of Diffusion models, and we look forward to expanding the boundaries of technology together with you.

## Update History

&gt; DiffSynth-Studio has undergone major version updates, and some old features are no longer maintained. If you need to use old features, please switch to the [last historical version](https://github.com/modelscope/DiffSynth-Studio/tree/afd101f3452c9ecae0c87b79adfa2e22d65ffdc3) before the major version update.

&gt; Currently, the development personnel of this project are limited, with most of the work handled by [Artiprocher](https://github.com/Artiprocher). Therefore, the progress of new feature development will be relatively slow, and the speed of responding to and resolving issues is limited. We apologize for this and ask developers to understand.

- **December 9, 2025** We release a wild model based on DiffSynth-Studio 2.0: [Qwen-Image-i2L](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-i2L) (Image-to-LoRA). This model takes an image as input and outputs a LoRA. Although this version still has significant room for improvement in terms of generalization, detail preservation, and other aspects, we are open-sourcing these models to inspire more innovative research.

- **December 4, 2025** DiffSynth-Studio 2.0 released! Many new features online
  - [Documentation](/docs/en/README.md) online: Our documentation is still continuously being optimized and updated
  - [VRAM Management](/docs/en/Pipeline_Usage/VRAM_management.md) module upgraded, supporting layer-level disk offload, releasing both memory and VRAM simultaneously
  - New model support
    - Z-Image Turbo: [Model](https://www.modelscope.ai/models/Tongyi-MAI/Z-Image-Turbo), [Documentation](/docs/en/Model_Details/Z-Image.md), [Code](/examples/z_image/)
    - FLUX.2-dev: [Model](https://www.modelscope.cn/models/black-forest-labs/FLUX.2-dev), [Documentation](/docs/en/Model_Details/FLUX2.md), [Code](/examples/flux2/)
  - Training framework upgrade
    - [Split Training](/docs/zh/Training/Split_Training.md): Supports automatically splitting the training process into two stages: data processing and training (even for training ControlNet or any other model). Computations that do not require gradient backpropagation, such as text encoding and VAE encoding, are performed during the data processing stage, while other computations are handled during the training stage. Faster speed, less VRAM requirement.
    - [Differential LoRA Training](/docs/zh/Training/Differential_LoRA.md): This is a training technique we used in [ArtAug](https://www.modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1), now available for LoRA training of any model.
    - [FP8 Training](/docs/zh/Training/FP8_Precision.md): FP8 can be applied to any non-training model during training, i.e., models with gradients turned off or gradients that only affect LoRA weights.

&lt;details&gt;
&lt;summary&gt;More&lt;/summary&gt;

- **November 4, 2025** Supported the [ByteDance/Video-As-Prompt-Wan2.1-14B](https://modelscope.cn/models/ByteDance/Video-As-Prompt-Wan2.1-14B) model, which is trained based on Wan 2.1 and supports generating corresponding actions based on reference videos.

- **October 30, 2025** Supported the [meituan-longcat/LongCat-Video](https://www.modelscope.cn/models/meituan-longcat/LongCat-Video) model, which supports text-to-video, image-to-video, and video continuation. This model uses the Wan framework for inference and training in this project.

- **October 27, 2025** Supported the [krea/krea-realtime-video](https://www.modelscope.cn/models/krea/krea-realtime-video) model, adding another member to the Wan model ecosystem.

- **September 23, 2025** [DiffSynth-Studio/Qwen-Image-EliGen-Poster](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen-Poster) released! This model was jointly developed and open-sourced by us and Taobao Experience Design Team. Built upon Qwen-Image, the model is specifically designed for e-commerce poster scenarios, supporting precise partition layout control. Please refer to [our sample code](./examples/qwen_image/model_inference/Qwen-Image-EliGen-Poster.py).

- **September 9, 2025** Our training framework supports various training modes. Currently adapted for Qwen-Image, in addition to the standard SFT training mode, Direct Distill is now supported. Please refer to [our sample code](./examples/qwen_image/model_training/lora/Qwen-Image-Distill-LoRA.sh). This feature is experimental, and we will continue to improve it to support more comprehensive model training functions.

- **August 28, 2025** We support Wan2.2-S2V, an audio-driven cinematic video generation model. See [./examples/wanvideo/](./examples/wanvideo/).

- **August 21, 2025** [DiffSynth-Studio/Qwen-Image-EliGen-V2](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen-V2) released! Compared to the V1 version, the training dataset has been changed to [Qwen-Image-Self-Generated-Dataset](https://www.modelscope.cn/datasets/DiffSynth-Studio/Qwen-Image-Self-Generated-Dataset), so the generated images better conform to Qwen-Image&#039;s own image distribution and style. Please refer to [our sample code](./examples/qwen_image/model_inference_low_vram/Qwen-Image-EliGen-V2.py).

- **August 21, 2025** We open-sourced the [DiffSynth-Studio/Qwen-Image-In-Context-Control-Union](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-In-Context-Control-Union) structural control LoRA model, adopting the In Context technical route, supporting multiple categories of structural control conditions, including canny, depth, lineart, softedge, normal, and openpose. Please refer to [our sample code](./examples/qwen_image/model_inference/Qwen-Image-In-Context-Control-Union.py).

- **August 20, 2025** We open-sourced the [DiffSynth-Studio/Qwen-Image-Edit-Lowres-Fix](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Edit-Lowres-Fix) model, improving the editing effect of Qwen-Image-Edit on low-resolution image inputs. Please refer to [our sample code](./examples/qwen_image/model_inference/Qwen-Image-Edit-Lowres-Fix.py)

- **August 19, 2025** ğŸ”¥ Qwen-Image-Edit open-sourced, welcome a new member to the image editing model family!

- **August 18, 2025** We trained and open-sourced the Qwen-Image inpainting ControlNet model [DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Inpaint). The model structure adopts a lightweight design. Please refer to [our sample code](./examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Inpaint.py).

- **August 15, 2025** We open-sourced the [Qwen-Image-Self-Generated-Dataset](https://www.modelscope.cn/datasets/DiffSynth-Studio/Qwen-Image-Self-Generated-Dataset) dataset. This is an image dataset generated using the Qwen-Image model, containing 160,000 `1024 x 1024` images. It includes general, English text rendering, and Chinese text rendering subsets. We provide annotations for image descriptions, entities, and structural control images for each image. Developers can use this dataset to train Qwen-Image models&#039; ControlNet and EliGen models. We aim to promote technological development through open-sourcing!

- **August 13, 2025** We trained and open-sourced the Qwen-Image ControlNet model [DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Depth](https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Depth). The model structure adopts a lightweight design. Please refer to [our sample code](./examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Depth.py).

- **August 12, 2025** We trained and open-sourced the Qwen-Image ControlNet model [DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Canny](https://modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Blockwise-ControlNet-Canny). The model structure adopts a lightweight design. Please refer to [our sample code](./examples/qwen_image/model_inference/Qwen-Image-Blockwise-ControlNet-Canny.py).

- **August 11, 2025** We open-sourced the distilled acceleration model [DiffSynth-Studio/Qwen-Image-Distill-LoRA](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Distill-LoRA) for Qwen-Image, following the same training process as [DiffSynth-Studio/Qwen-Image-Distill-Full](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Distill-Full), but the model structure has been modified to LoRA, thus being better compatible with other open-source ecosystem models.

- **August 7, 2025** We open-sourced the entity control LoRA model [DiffSynth-Studio/Qwen-Image-EliGen](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-EliGen) for Qwen-Image. Qwen-Image-EliGen can achieve entity-level controlled text-to-image generation. Technical details can be found in [the paper](https://arxiv.org/abs/2501.01097). Training dataset: [EliGenTrainSet](https://www.modelscope.cn/datasets/DiffSynth-Studio/EliGenTrainSet).

- **August 5, 2025** We open-sourced the distilled acceleration model [DiffSynth-Studio/Qwen-Image-Distill-Full](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-Distill-Full) for Qwen-Image, achieving approximately 5x acceleration.

- **August 4, 2025** ğŸ”¥ Qwen-Image open-sourced, welcome a new member to the image generation model family!

- **August 1, 2025** [FLUX.1-Krea-dev](https://www.modelscope.cn/models/black-forest-labs/FLUX.1-Krea-dev) open-sourced, a text-to-image model focused on aesthetic photography. We provided comprehensive support in a timely manner, including low VRAM layer-by-layer offload, LoRA training, and full training. For more details, please refer to [./examples/flux/](./examples/flux/).

- **July 28, 2025** Wan 2.2 open-sourced. We provided comprehensive support in a timely manner, including low VRAM layer-by-layer offload, FP8 quantization, sequence parallelism, LoRA training, and full training. For more details, please refer to [./examples/wanvideo/](./examples/wanvideo/).

- **July 11, 2025** We propose Nexus-Gen, a unified framework that combines the language reasoning capabilities of Large Language Models (LLMs) with the image generation capabilities of diffusion models. This framework supports seamless image understanding, generation, and editing tasks.
  - Paper: [Nexus-Gen: Unified Image Understanding, Generation, and Editing via Prefilled Autoregression in Shared Embedding Space](https://arxiv.org/pdf/2504.21356)
  - GitHub Repository: https://github.com/modelscope/Nexus-Gen
  - Model: [ModelScope](https://www.modelscope.cn/models/DiffSynth-Studio/Nexus-GenV2), [HuggingFace](https://huggingface.co/modelscope/Nexus-GenV2)
  - Training Dataset: [ModelScope Dataset](https://www.modelscope.cn/datasets/DiffSynth-Studio/Nexus-Gen-Training-Dataset)
  - Online Experience: [ModelScope Nexus-Gen Studio](https://www.modelscope.cn/studios/DiffSynth-Studio/Nexus-Gen)

- **June 15, 2025** ModelScope&#039;s official evaluation framework [EvalScope](https://github.com/modelscope/evalscope) now supports text-to-image generation evaluation. Please refer to the [best practices](https://evalscope.readthedocs.io/zh-cn/latest/best_practice/t2i_eval.html) guide to try it out.

- **March 25, 2025** Our new open-source project [DiffSynth-Engine](https://github.com/modelscope/DiffSynth-Engine) is now open-sourced! Focused on stable model deployment, targeting industry, providing better engineering support, higher computational performance, and more stable features.

- **March 31, 2025** We support InfiniteYou, a face feature preservation method for FLUX. More details can be found in [./examples/InfiniteYou/](./examples/InfiniteYou/).

- **March 13, 2025** We support HunyuanVideo-I2V, the image-to-video generation version of Tencent&#039;s open-source HunyuanVideo. More details can be found in [./examples/HunyuanVideo/](./examples/HunyuanVideo/).

- **February 25, 2025** We support Wan-Video, a series of state-of-the-art video synthesis models open-sourced by Alibaba. See [./examples/wanvideo/](./examples/wanvideo/).

- **February 17, 2025** We support [StepVideo](https://modelscope.cn/models/stepfun-ai/stepvideo-t2v/summary)! Advanced video synthesis model! See [./examples/stepvideo](./examples/stepvideo/).

- **December 31, 2024** We propose EliGen, a new framework for entity-level controlled text-to-image generation, supplemented with an inpainting fusion pipeline, extending its capabilities to image inpainting tasks. EliGen can seamlessly integrate existing community models such as IP-Adapter and In-Context LoRA, enhancing their versatility. For more details, see [./examples/EntityControl](./examples/EntityControl/).
  - Paper: [EliGen: Entity-Level Controlled Image Generation with Regional Attention](https://arxiv.org/abs/2501.01097)
  - Model: [ModelScope](https://www.modelscope.cn/models/DiffSynth-Studio/Eligen), [HuggingFace](https://huggingface.co/modelscope/EliGen)
  - Online Experience: [ModelScope EliGen Studio](https://www.modelscope.cn/studios/DiffSynth-Studio/EliGen)
  - Training Dataset: [EliGen Train Set](https://www.modelscope.cn/datasets/DiffSynth-Studio/EliGenTrainSet)

- **December 19, 2024** We implemented advanced VRAM management for HunyuanVideo, enabling video generation with resolutions of 129x720x1280 on 24GB VRAM or 129x512x384 on just 6GB VRAM. More details can be found in [./examples/HunyuanVideo/](./examples/HunyuanVideo/).

- **December 18, 2024** We propose ArtAug, a method to improve text-to-image models through synthesis-understanding interaction. We trained an ArtAug enhancement module for FLUX.1-dev in LoRA format. This model incorporates the aesthetic understanding of Qwen2-VL-72B into FLUX.1-dev, thereby improving the quality of generated images.
  - Paper: https://arxiv.org/abs/2412.12888
  - Example: https://github.com/modelscope/DiffSynth-Studio/tree/main/examples/ArtAug
  - Model: [ModelScope](https://www.modelscope.cn/models/DiffSynth-Studio/ArtAug-lora-FLUX.1dev-v1), [HuggingFace](https://huggingface.co/ECNU-CILab/ArtAug-lora-FLUX.1dev-v1)
  - Demo: [ModelScope](https://modelscope.cn/aigc/imageGeneration?tab=advanced&amp;versionId=7228&amp;modelType=LoRA&amp;sdVersion=FLUX_1&amp;modelUrl=modelscope%3A%2F%2FDiffSynth-Studio%2FArtAug-lora-FLUX.1dev-v1%3Frevision%3Dv1.0), HuggingFace (coming soon)

- **October 25, 2024** We provide extensive FLUX ControlNet support. This project supports many different ControlNet models and can be freely combined, even if their structures are different. Additionally, ControlNet models are compatible with high-resolution optimization and partition control technologies, enabling very powerful controllable image generation. See [`./examples/ControlNet/`](./examples/ControlNet/).

- **October 8, 2024** We released extended LoRAs based on CogVideoX-5B and ExVideo. You can download this model from [ModelScope](https://modelscope.cn/models/ECNU-CILab/ExVideo-CogVideoX-LoRA-129f-v1) or [HuggingFace](https://huggingface.co/ECNU-CILab/ExVideo-CogVideoX-LoRA-129f-v1).

- **August 22, 2024** This project now supports CogVideoX-5B. See [here](/examples/video_synthesis/). We provide several interesting features for this text-to-video model, including:
  - Text-to-video
  - Video editing
  - Self super-resolution
  - Video interpolation

- **August 22, 2024** We implemented an interesting brush feature that supports all text-to-image models. Now you can create stunning images with the assistance of AI using the brush!
  - Use it in our [WebUI](#usage-in-webui).

- **August 21, 2024** DiffSynth-Studio now supports FLUX.
  - Enable CFG and high-resolution inpainting to improve visual quality. See [here](/examples/image_synthesis/README.md)
  - LoRA, ControlNet, and other addon models will be released soon.

- **June 21, 2024** We propose ExVideo, a post-training fine-tuning technique aimed at enhancing the capabilities of video generation models. We extended Stable Video Diffusion to achieve long video generation of up to 128 frames.
  - [Project Page](https://ecnu-cilab.github.io/ExVideoProjectPage/)
  - Source code has been released in this repository. See [`examples/ExVideo`](./examples/ExVideo/).
  - Model has been released at [HuggingFace](https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1) and [ModelScope](https://modelscope.cn/models/ECNU-CILab/ExVideo-SVD-128f-v1).
  - Technical report has been released at [arXiv](https://arxiv.org/abs/2406.14130).
  - You can try ExVideo in this [demo](https://huggingface.co/spaces/modelscope/ExVideo-SVD-128f-v1)!

- **June 13, 2024** DiffSynth Studio has migrated to ModelScope. The development team has also transitioned from &quot;me&quot; to &quot;us&quot;. Of course, I will still participate in subsequent development and maintenance work.

- **January 29, 2024** We propose Diffutoon, an excellent cartoon coloring solution.
  - [Project Page](https://ecnu-cilab.github.io/DiffutoonProjectPage/)
  - Source code has been released in this project.
  - Technical report (IJCAI 2024) has been released at [arXiv](https://arxiv.org/abs/2401.16224).

- **December 8, 2023** We decided to i

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>Python</category>
        </item>
    </channel>
</rss>