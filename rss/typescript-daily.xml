<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>GitHub Trending Today for typescript - TypeScript Daily</title>
        <link>https://github.com/trending</link>
        <description>The most popular GitHub repositories today for typescript.</description>
        <lastBuildDate>Sat, 09 Aug 2025 00:04:55 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>GitHub Trending RSS Generator</generator>
        <language>en</language>
        <copyright>All rights reserved 2025, GitHub</copyright>
        <item>
            <title><![CDATA[browserbase/stagehand]]></title>
            <link>https://github.com/browserbase/stagehand</link>
            <guid>https://github.com/browserbase/stagehand</guid>
            <pubDate>Sat, 09 Aug 2025 00:04:55 GMT</pubDate>
            <description><![CDATA[The AI Browser Automation Framework]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/browserbase/stagehand">browserbase/stagehand</a></h1>
            <p>The AI Browser Automation Framework</p>
            <p>Language: TypeScript</p>
            <p>Stars: 15,902</p>
            <p>Forks: 950</p>
            <p>Stars today: 311 stars today</p>
            <h2>README</h2><pre>&lt;div id=&quot;toc&quot; align=&quot;center&quot; style=&quot;margin-bottom: 0;&quot;&gt;
  &lt;ul style=&quot;list-style: none; margin: 0; padding: 0;&quot;&gt;
    &lt;a href=&quot;https://stagehand.dev&quot;&gt;
      &lt;picture&gt;
        &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;media/dark_logo.png&quot; /&gt;
        &lt;img alt=&quot;Stagehand&quot; src=&quot;media/light_logo.png&quot; width=&quot;200&quot; style=&quot;margin-right: 30px;&quot; /&gt;
      &lt;/picture&gt;
    &lt;/a&gt;
  &lt;/ul&gt;
&lt;/div&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;The AI Browser Automation Framework&lt;/strong&gt;&lt;br&gt;
  &lt;a href=&quot;https://docs.stagehand.dev&quot;&gt;Read the Docs&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/browserbase/stagehand/tree/main?tab=MIT-1-ov-file#MIT-1-ov-file&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;media/dark_license.svg&quot; /&gt;
      &lt;img alt=&quot;MIT License&quot; src=&quot;media/light_license.svg&quot; /&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://join.slack.com/t/stagehand-dev/shared_invite/zt-38khc8iv5-T2acb50_0OILUaX7lxeBOg&quot;&gt;
    &lt;picture&gt;
      &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;media/dark_slack.svg&quot; /&gt;
      &lt;img alt=&quot;Slack Community&quot; src=&quot;media/light_slack.svg&quot; /&gt;
    &lt;/picture&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
	&lt;a href=&quot;https://trendshift.io/repositories/12122&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/12122&quot; alt=&quot;browserbase%2Fstagehand | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
If you&#039;re looking for the Python implementation, you can find it 
&lt;a href=&quot;https://github.com/browserbase/stagehand-python&quot;&gt; here&lt;/a&gt;
&lt;/p&gt;

&lt;div align=&quot;center&quot; style=&quot;display: flex; align-items: center; justify-content: center; gap: 4px; margin-bottom: 0;&quot;&gt;
  &lt;b&gt;Vibe code&lt;/b&gt;
  &lt;span style=&quot;font-size: 1.05em;&quot;&gt; Stagehand with &lt;/span&gt;
  &lt;a href=&quot;https://director.ai&quot; style=&quot;display: flex; align-items: center;&quot;&gt;
    &lt;span&gt;Director&lt;/span&gt;
  &lt;/a&gt;
  &lt;span&gt; &lt;/span&gt;
  &lt;picture&gt;
    &lt;img alt=&quot;Director&quot; src=&quot;media/director_icon.svg&quot; width=&quot;25&quot; /&gt;
  &lt;/picture&gt;
&lt;/div&gt;

## Why Stagehand?

Most existing browser automation tools either require you to write low-level code in a framework like Selenium, Playwright, or Puppeteer, or use high-level agents that can be unpredictable in production. By letting developers choose what to write in code vs. natural language, Stagehand is the natural choice for browser automations in production.

1. **Choose when to write code vs. natural language**: use AI when you want to navigate unfamiliar pages, and use code ([Playwright](https://playwright.dev/)) when you know exactly what you want to do.

2. **Preview and cache actions**: Stagehand lets you preview AI actions before running them, and also helps you easily cache repeatable actions to save time and tokens.

3. **Computer use models with one line of code**: Stagehand lets you integrate SOTA computer use models from OpenAI and Anthropic into the browser with one line of code.

## Example

Here&#039;s how to build a sample browser automation with Stagehand:

&lt;div align=&quot;center&quot;&gt;
  &lt;div style=&quot;max-width:300px;&quot;&gt;
    &lt;img src=&quot;/media/github_demo.gif&quot; alt=&quot;See Stagehand in Action&quot;&gt;
  &lt;/div&gt;
&lt;/div&gt;

```typescript
// Use Playwright functions on the page object
const page = stagehand.page;
await page.goto(&quot;https://github.com/browserbase&quot;);

// Use act() to execute individual actions
await page.act(&quot;click on the stagehand repo&quot;);

// Use Computer Use agents for larger actions
const agent = stagehand.agent({
    provider: &quot;openai&quot;,
    model: &quot;computer-use-preview&quot;,
});
await agent.execute(&quot;Get to the latest PR&quot;);

// Use extract() to read data from the page
const { author, title } = await page.extract({
  instruction: &quot;extract the author and title of the PR&quot;,
  schema: z.object({
    author: z.string().describe(&quot;The username of the PR author&quot;),
    title: z.string().describe(&quot;The title of the PR&quot;),
  }),
});
```

## Documentation

Visit [docs.stagehand.dev](https://docs.stagehand.dev) to view the full documentation.

## Getting Started

Start with Stagehand with one line of code, or check out our [Quickstart Guide](https://docs.stagehand.dev/get_started/quickstart) for more information:

```bash
npx create-browser-app
```

&lt;div align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://www.loom.com/share/f5107f86d8c94fa0a8b4b1e89740f7a7&quot;&gt;
      &lt;p&gt;Watch Anirudh demo create-browser-app to create a Stagehand project!&lt;/p&gt;
    &lt;/a&gt;
    &lt;a href=&quot;https://www.loom.com/share/f5107f86d8c94fa0a8b4b1e89740f7a7&quot;&gt;
      &lt;img style=&quot;max-width:300px;&quot; src=&quot;https://cdn.loom.com/sessions/thumbnails/f5107f86d8c94fa0a8b4b1e89740f7a7-ec3f428b6775ceeb-full-play.gif&quot;&gt;
    &lt;/a&gt;
  &lt;/div&gt;

### Build and Run from Source

```bash
git clone https://github.com/browserbase/stagehand.git
cd stagehand
pnpm install
pnpm playwright install
pnpm run build
pnpm run example # run the blank script at ./examples/example.ts
pnpm run example 2048 # run the 2048 example at ./examples/2048.ts
```

Stagehand is best when you have an API key for an LLM provider and Browserbase credentials. To add these to your project, run:

```bash
cp .env.example .env
nano .env # Edit the .env file to add API keys
```

## Contributing

&gt; [!NOTE]  
&gt; We highly value contributions to Stagehand! For questions or support, please join our [Slack community](https://join.slack.com/t/stagehand-dev/shared_invite/zt-38khc8iv5-T2acb50_0OILUaX7lxeBOg).

At a high level, we&#039;re focused on improving reliability, speed, and cost in that order of priority. If you&#039;re interested in contributing, we strongly recommend reaching out to [Miguel Gonzalez](https://x.com/miguel_gonzf) or [Paul Klein](https://x.com/pk_iv) in our [Slack community](https://join.slack.com/t/stagehand-dev/shared_invite/zt-38khc8iv5-T2acb50_0OILUaX7lxeBOg) before starting to ensure that your contribution aligns with our goals.

For more information, please see our [Contributing Guide](https://docs.stagehand.dev/examples/contributing).

## Acknowledgements

This project heavily relies on [Playwright](https://playwright.dev/) as a resilient backbone to automate the web. It also would not be possible without the awesome techniques and discoveries made by [tarsier](https://github.com/reworkd/tarsier), [gemini-zod](https://github.com/jbeoris/gemini-zod), and [fuji-web](https://github.com/normal-computing/fuji-web).

We&#039;d like to thank the following people for their major contributions to Stagehand:
- [Paul Klein](https://github.com/pkiv)
- [Anirudh Kamath](https://github.com/kamath)
- [Sean McGuire](https://github.com/seanmcguire12)
- [Miguel Gonzalez](https://github.com/miguelg719)
- [Sameel Arif](https://github.com/sameelarif)
- [Filip Michalsky](https://github.com/filip-michalsky)
- [Jeremy Press](https://x.com/jeremypress)
- [Navid Pour](https://github.com/navidpour)

## License

Licensed under the MIT License.

Copyright 2025 Browserbase, Inc.
</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[backstage/backstage]]></title>
            <link>https://github.com/backstage/backstage</link>
            <guid>https://github.com/backstage/backstage</guid>
            <pubDate>Sat, 09 Aug 2025 00:04:54 GMT</pubDate>
            <description><![CDATA[Backstage is an open framework for building developer portals]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/backstage/backstage">backstage/backstage</a></h1>
            <p>Backstage is an open framework for building developer portals</p>
            <p>Language: TypeScript</p>
            <p>Stars: 31,019</p>
            <p>Forks: 6,716</p>
            <p>Stars today: 29 stars today</p>
            <h2>README</h2><pre>[![headline](docs/assets/headline.png)](https://backstage.io/)

# [Backstage](https://backstage.io)

English \| [한국어](README-ko_kr.md) \| [中文版](README-zh_Hans.md) \| [Français](README-fr_FR.md)

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![CNCF Status](https://img.shields.io/badge/cncf%20status-incubation-blue.svg)](https://www.cncf.io/projects)
[![Discord](https://img.shields.io/discord/687207715902193673?logo=discord&amp;label=Discord&amp;color=5865F2&amp;logoColor=white)](https://discord.gg/backstage-687207715902193673)
![Code style](https://img.shields.io/badge/code_style-prettier-ff69b4.svg)
[![Codecov](https://img.shields.io/codecov/c/github/backstage/backstage)](https://codecov.io/gh/backstage/backstage)
[![](https://img.shields.io/github/v/release/backstage/backstage)](https://github.com/backstage/backstage/releases)
[![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/7678/badge)](https://bestpractices.coreinfrastructure.org/projects/7678)
[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/backstage/backstage/badge)](https://securityscorecards.dev/viewer/?uri=github.com/backstage/backstage)

## What is Backstage?

[Backstage](https://backstage.io/) is an open source framework for building developer portals. Powered by a centralized software catalog, Backstage restores order to your microservices and infrastructure and enables your product teams to ship high-quality code quickly without compromising autonomy.

Backstage unifies all your infrastructure tooling, services, and documentation to create a streamlined development environment from end to end.

![software-catalog](docs/assets/header.png)

Out of the box, Backstage includes:

- [Backstage Software Catalog](https://backstage.io/docs/features/software-catalog/) for managing all your software such as microservices, libraries, data pipelines, websites, and ML models
- [Backstage Software Templates](https://backstage.io/docs/features/software-templates/) for quickly spinning up new projects and standardizing your tooling with your organization’s best practices
- [Backstage TechDocs](https://backstage.io/docs/features/techdocs/) for making it easy to create, maintain, find, and use technical documentation, using a &quot;docs like code&quot; approach
- Plus, a growing ecosystem of [open source plugins](https://github.com/backstage/backstage/tree/master/plugins) that further expand Backstage’s customizability and functionality

Backstage was created by Spotify but is now hosted by the [Cloud Native Computing Foundation (CNCF)](https://www.cncf.io) as an Incubation level project. For more information, see the [announcement](https://backstage.io/blog/2022/03/16/backstage-turns-two#out-of-the-sandbox-and-into-incubation).

## Project roadmap

For information about the detailed project roadmap including delivered milestones, see [the Roadmap](https://backstage.io/docs/overview/roadmap).

## Getting Started

To start using Backstage, see the [Getting Started documentation](https://backstage.io/docs/getting-started).

## Documentation

The documentation of Backstage includes:

- [Main documentation](https://backstage.io/docs)
- [Software Catalog](https://backstage.io/docs/features/software-catalog/)
- [Architecture](https://backstage.io/docs/overview/architecture-overview) ([Decisions](https://backstage.io/docs/architecture-decisions/))
- [Designing for Backstage](https://backstage.io/docs/dls/design)
- [Storybook - UI components](https://backstage.io/storybook)

## Community

To engage with our community, you can use the following resources:

- [Discord chatroom](https://discord.gg/backstage-687207715902193673) - Get support or discuss the project
- [Contributing to Backstage](https://github.com/backstage/backstage/blob/master/CONTRIBUTING.md) - Start here if you want to contribute
- [RFCs](https://github.com/backstage/backstage/labels/rfc) - Help shape the technical direction
- [FAQ](https://backstage.io/docs/faq) - Frequently Asked Questions
- [Code of Conduct](CODE_OF_CONDUCT.md) - This is how we roll
- [Adopters](ADOPTERS.md) - Companies already using Backstage
- [Blog](https://backstage.io/blog/) - Announcements and updates
- [Newsletter](https://spoti.fi/backstagenewsletter) - Subscribe to our email newsletter
- [Backstage Community Sessions](https://github.com/backstage/community) - Join monthly meetups and explore Backstage community
- Give us a star ⭐️ - If you are using Backstage or think it is an interesting project, we would love a star ❤️

## Governance

See the [GOVERNANCE.md](https://github.com/backstage/community/blob/main/GOVERNANCE.md) document in the [backstage/community](https://github.com/backstage/community) repository.

## License

Copyright 2020-2025 © The Backstage Authors. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our Trademark Usage page: https://www.linuxfoundation.org/trademark-usage

Licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0

## Security

Please report sensitive security issues using Spotify&#039;s [bug-bounty program](https://hackerone.com/spotify) rather than GitHub.

For further details, see our complete [security release process](SECURITY.md).
</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[nestjs/nest]]></title>
            <link>https://github.com/nestjs/nest</link>
            <guid>https://github.com/nestjs/nest</guid>
            <pubDate>Sat, 09 Aug 2025 00:04:53 GMT</pubDate>
            <description><![CDATA[A progressive Node.js framework for building efficient, scalable, and enterprise-grade server-side applications with TypeScript/JavaScript 🚀]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/nestjs/nest">nestjs/nest</a></h1>
            <p>A progressive Node.js framework for building efficient, scalable, and enterprise-grade server-side applications with TypeScript/JavaScript 🚀</p>
            <p>Language: TypeScript</p>
            <p>Stars: 72,125</p>
            <p>Forks: 7,971</p>
            <p>Stars today: 23 stars today</p>
            <h2>README</h2><pre>README not available. Either the repository does not have a README or it could not be accessed.</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[simstudioai/sim]]></title>
            <link>https://github.com/simstudioai/sim</link>
            <guid>https://github.com/simstudioai/sim</guid>
            <pubDate>Sat, 09 Aug 2025 00:04:52 GMT</pubDate>
            <description><![CDATA[Sim is an open-source AI agent workflow builder. Sim Studio's interface is a lightweight, intuitive way to quickly build and deploy LLMs that connect with your favorite tools.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/simstudioai/sim">simstudioai/sim</a></h1>
            <p>Sim is an open-source AI agent workflow builder. Sim Studio's interface is a lightweight, intuitive way to quickly build and deploy LLMs that connect with your favorite tools.</p>
            <p>Language: TypeScript</p>
            <p>Stars: 7,128</p>
            <p>Forks: 935</p>
            <p>Stars today: 152 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;apps/sim/public/static/sim.png&quot; alt=&quot;Sim Logo&quot; width=&quot;500&quot;/&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-Apache%202.0-blue.svg&quot; alt=&quot;License: Apache-2.0&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://discord.gg/Hr4UWYEcTT&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discord-Join%20Server-7289DA?logo=discord&amp;logoColor=white&quot; alt=&quot;Discord&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://x.com/simdotai&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/simstudioai?style=social&quot; alt=&quot;Twitter&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/simstudioai/sim/pulls&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg&quot; alt=&quot;PRs welcome&quot;&gt;&lt;/a&gt;
  &lt;a href=&quot;https://docs.sim.ai&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Docs-visit%20documentation-blue.svg&quot; alt=&quot;Documentation&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;strong&gt;Sim&lt;/strong&gt; is a lightweight, user-friendly platform for building AI agent workflows.
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;apps/sim/public/static/demo.gif&quot; alt=&quot;Sim Demo&quot; width=&quot;800&quot;/&gt;
&lt;/p&gt;

## Getting Started

1. Use our [cloud-hosted version](https://sim.ai)
2. Self-host using one of the methods below

## Self-Hosting Options

### Option 1: NPM Package (Simplest)

The easiest way to run Sim locally is using our [NPM package](https://www.npmjs.com/package/simstudio?activeTab=readme):

```bash
npx simstudio
```

After running these commands, open [http://localhost:3000/](http://localhost:3000/) in your browser.

#### Options

- `-p, --port &lt;port&gt;`: Specify the port to run Sim on (default: 3000)
- `--no-pull`: Skip pulling the latest Docker images

#### Requirements

- Docker must be installed and running on your machine

### Option 2: Docker Compose

```bash
# Clone the repository
git clone https://github.com/simstudioai/sim.git

# Navigate to the project directory
cd sim

# Start Sim
docker compose -f docker-compose.prod.yml up -d
```

Access the application at [http://localhost:3000/](http://localhost:3000/)

#### Using Local Models with Ollama

Run Sim with local AI models using [Ollama](https://ollama.ai) - no external APIs required:

```bash
# Start with GPU support (automatically downloads gemma3:4b model)
docker compose -f docker-compose.ollama.yml --profile setup up -d

# For CPU-only systems:
docker compose -f docker-compose.ollama.yml --profile cpu --profile setup up -d
```

Wait for the model to download, then visit [http://localhost:3000](http://localhost:3000). Add more models with:
```bash
docker compose -f docker-compose.ollama.yml exec ollama ollama pull llama3.1:8b
```

### Option 3: Dev Containers

1. Open VS Code with the [Remote - Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)
2. Open the project and click &quot;Reopen in Container&quot; when prompted
3. Run `bun run dev:full` in the terminal or use the `sim-start` alias
   - This starts both the main application and the realtime socket server

### Option 4: Manual Setup

**Requirements:**
- [Bun](https://bun.sh/) runtime
- PostgreSQL 12+ with [pgvector extension](https://github.com/pgvector/pgvector) (required for AI embeddings)

**Note:** Sim uses vector embeddings for AI features like knowledge bases and semantic search, which requires the `pgvector` PostgreSQL extension.

1. Clone and install dependencies:

```bash
git clone https://github.com/simstudioai/sim.git
cd sim
bun install
```

2. Set up PostgreSQL with pgvector:

You need PostgreSQL with the `vector` extension for embedding support. Choose one option:

**Option A: Using Docker (Recommended)**
```bash
# Start PostgreSQL with pgvector extension
docker run --name simstudio-db \
  -e POSTGRES_PASSWORD=your_password \
  -e POSTGRES_DB=simstudio \
  -p 5432:5432 -d \
  pgvector/pgvector:pg17
```

**Option B: Manual Installation**
- Install PostgreSQL 12+ and the pgvector extension
- See [pgvector installation guide](https://github.com/pgvector/pgvector#installation)

3. Set up environment:

```bash
cd apps/sim
cp .env.example .env  # Configure with required variables (DATABASE_URL, BETTER_AUTH_SECRET, BETTER_AUTH_URL)
```

Update your `.env` file with the database URL:
```bash
DATABASE_URL=&quot;postgresql://postgres:your_password@localhost:5432/simstudio&quot;
```

4. Set up the database:

```bash
bunx drizzle-kit migrate 
```

5. Start the development servers:

**Recommended approach - run both servers together (from project root):**

```bash
bun run dev:full
```

This starts both the main Next.js application and the realtime socket server required for full functionality.

**Alternative - run servers separately:**

Next.js app (from project root):
```bash
bun run dev
```

Realtime socket server (from `apps/sim` directory in a separate terminal):
```bash
cd apps/sim
bun run dev:sockets
```

## Tech Stack

- **Framework**: [Next.js](https://nextjs.org/) (App Router)
- **Runtime**: [Bun](https://bun.sh/)
- **Database**: PostgreSQL with [Drizzle ORM](https://orm.drizzle.team)
- **Authentication**: [Better Auth](https://better-auth.com)
- **UI**: [Shadcn](https://ui.shadcn.com/), [Tailwind CSS](https://tailwindcss.com)
- **State Management**: [Zustand](https://zustand-demo.pmnd.rs/)
- **Flow Editor**: [ReactFlow](https://reactflow.dev/)
- **Docs**: [Fumadocs](https://fumadocs.vercel.app/)
- **Monorepo**: [Turborepo](https://turborepo.org/)
- **Realtime**: [Socket.io](https://socket.io/)
- **Background Jobs**: [Trigger.dev](https://trigger.dev/)

## Contributing

We welcome contributions! Please see our [Contributing Guide](.github/CONTRIBUTING.md) for details.

## License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

&lt;p align=&quot;center&quot;&gt;Made with ❤️ by the Sim Team&lt;/p&gt;</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[devlikeapro/waha]]></title>
            <link>https://github.com/devlikeapro/waha</link>
            <guid>https://github.com/devlikeapro/waha</guid>
            <pubDate>Sat, 09 Aug 2025 00:04:51 GMT</pubDate>
            <description><![CDATA[WAHA - WhatsApp HTTP API (REST API) that you can configure in a click! 3 engines: WEBJS (browser based), NOWEB (websocket nodejs), GOWS (websocket go)]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/devlikeapro/waha">devlikeapro/waha</a></h1>
            <p>WAHA - WhatsApp HTTP API (REST API) that you can configure in a click! 3 engines: WEBJS (browser based), NOWEB (websocket nodejs), GOWS (websocket go)</p>
            <p>Language: TypeScript</p>
            <p>Stars: 3,563</p>
            <p>Forks: 727</p>
            <p>Stars today: 212 stars today</p>
            <h2>README</h2><pre># WAHA

&lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;./logo.png&quot; style=&#039;border-radius: 50%&#039; width=&#039;150&#039;/&gt;
&lt;/p&gt;

**WAHA** - **W**hats**A**pp **H**TTP **A**PI (REST API) that you can install on your own server and run in less than 5 minutes!

[![Docker Pulls](https://img.shields.io/docker/pulls/devlikeapro/waha)](https://hub.docker.com/r/devlikeapro/waha)

- Documentation: [https://waha.devlike.pro/](https://waha.devlike.pro/)
- Dashboard Example: [https://waha.devlike.pro/dashboard](https://waha.devlike.pro/dashboard)
- Swagger Example: [https://waha.devlike.pro/swagger](https://waha.devlike.pro/swagger)

# Tables of Contents

&lt;!-- toc --&gt;

- [Quick start](#quick-start)
  * [Requirements](#requirements)
  * [Send your first message](#send-your-first-message)
    + [1. Download image](#1-download-image)
    + [2. Run WhatsApp HTTP API](#2-run-whatsapp-http-api)
    + [3. Start a new session](#3-start-a-new-session)
    + [4. Get and scan QR](#4-get-and-scan-qr)
    + [5. Get the screenshot](#5-get-the-screenshot)
    + [6. Send a text message](#6-send-a-text-message)
  * [What is next?](#what-is-next)
- [Development](#development)
  * [Start the project](#start-the-project)

&lt;!-- tocstop --&gt;

# Quick start

## Requirements

Only thing that you must have - installed docker. Please follow the original
instruction &lt;a href=&quot;https://docs.docker.com/get-docker/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;how to install docker -&gt;&lt;/a&gt;.

When you are ready - come back and follows the below steps to send the first text message to WhatsApp via HTTP API!

## Send your first message

Let&#039;s go over steps that allow you to send your first text message via WhatsApp HTTP API!

### 1. Download image

Assuming you have installed [Docker](https://docs.docker.com/get-docker/), let&#039;s download the image.


```bash
docker pull devlikeapro/waha
```


```bash
docker login -u devlikeapro -p {KEY}
docker pull devlikeapro/waha-plus
docker logout
```

Read more about how to get `PASSWORD` for [**➕ WAHA Plus**](https://waha.devlike.pro/docs/how-to/waha-plus/)

### 2. Run WhatsApp HTTP API

Run WhatsApp HTTP API:

```bash
docker run -it --rm -p 3000:3000/tcp --name waha devlikeapro/waha

# It prints logs and the last line must be
# WhatsApp HTTP API is running on: http://[::1]:3000
```

Open the link in your browser [http://localhost:3000/](http://localhost:3000/) and you&#039;ll see API documentation
(Swagger).


### 3. Start a new session

To start a new session you should have your mobile phone with installed WhatsApp application close to you.

Please go and read how what we&#039;ll need to a bit
later:
&lt;a href=&quot;https://faq.whatsapp.com/381777293328336/?helpref=hc_fnav&quot; target=&quot;_blank&quot;&gt;
How to log in - the instruction on WhatsApp site
&lt;/a&gt;

When your ready - find `POST /api/sessions`, click on **Try it out**, then **Execute** a bit below.


The example payload:
```json
{
  &quot;name&quot;: &quot;default&quot;
}
```


By using the request with `name` values you can start multiple session (WhatsApp accounts) inside the single docker container in Plus


### 4. Get and scan QR

Find `GET /api/screenshot` and execute it, it shows you QR code.


**Scan the QR with your cell phone&#039;s WhatsApp app.**


### 5. Get the screenshot

Execute `GET /api/screenshot` after a few seconds after scanning the QR - it&#039;ll show you the screenshot of you Whatsapp
instance. If you can get the actual screenshot - then you&#039;re ready to start sending messages!


### 6. Send a text message

Let&#039;s send a text message - find `POST /api/sendText`  in [swagger](http://localhost:3000/) and change `chatId` this
way: use a phone international phone number without `+` symbol and add `@c.us` at the end.

For phone number `12132132131` the `chatId` is  `12132132131@c.us`.

The example payload:
```json
{
  &quot;chatId&quot;: &quot;12132132130@c.us&quot;,
  &quot;text&quot;: &quot;Hi there!&quot;,
  &quot;session&quot;: &quot;default&quot;
}
```

Also, you can use `curl` and send POST request like this:

```bash
# Phone without +
export PHONE=12132132130
curl -d &quot;{\&quot;chatId\&quot;: \&quot;${PHONE}@c.us\&quot;, \&quot;text\&quot;: \&quot;Hello from WhatsApp HTTP API\&quot; }&quot; -H &quot;Content-Type: application/json&quot; -X POST http://localhost:3000/api/sendText
```

## What is next?
[Go and read the full documentation!](https://waha.devlike.pro/docs/overview/introduction/)

# Development

## Start the project
1. Clone the repository
2. Make sure you&#039;re using node&gt;=22 (check [.nvmrc](/.nvmrc) to get the version)
3. Run the following commands:
```bash
# Install dependencies
yarn install
# Fetch and compile proto files
yarn gows:proto
# Run
yarn start
# open http://localhost:3000
```</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[mui/mui-x]]></title>
            <link>https://github.com/mui/mui-x</link>
            <guid>https://github.com/mui/mui-x</guid>
            <pubDate>Sat, 09 Aug 2025 00:04:50 GMT</pubDate>
            <description><![CDATA[MUI X: Build complex and data-rich applications using a growing list of advanced React components, like the Data Grid, Date and Time Pickers, Charts, and more!]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/mui/mui-x">mui/mui-x</a></h1>
            <p>MUI X: Build complex and data-rich applications using a growing list of advanced React components, like the Data Grid, Date and Time Pickers, Charts, and more!</p>
            <p>Language: TypeScript</p>
            <p>Stars: 5,399</p>
            <p>Forks: 1,532</p>
            <p>Stars today: 12 stars today</p>
            <h2>README</h2><pre>&lt;!-- markdownlint-disable-next-line --&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://mui.com/x/&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;&lt;img width=&quot;150&quot; height=&quot;133&quot; src=&quot;https://mui.com/static/logo.svg&quot; alt=&quot;MUI X logo&quot;&gt;&lt;/a&gt;
&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt;MUI X&lt;/h1&gt;

&lt;div align=&quot;center&quot;&gt;

[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/mui/mui-x/blob/HEAD/LICENSE)
[![npm latest package](https://img.shields.io/npm/v/@mui/x-data-grid/latest.svg)](https://www.npmjs.com/package/@mui/x-data-grid)
[![npm downloads](https://img.shields.io/npm/dm/@mui/x-data-grid.svg)](https://www.npmjs.com/package/@mui/x-data-grid)
[![GitHub branch status](https://img.shields.io/github/checks-status/mui/mui-x/HEAD)](https://github.com/mui/mui-x/commits/HEAD/)
[![Coverage status](https://img.shields.io/codecov/c/github/mui/mui-x.svg)](https://codecov.io/gh/mui/mui-x/)
[![Follow on X](https://img.shields.io/twitter/follow/MUI_X_.svg?label=follow+MUI+X)](https://x.com/MUI_X_)
[![Renovate status](https://img.shields.io/badge/renovate-enabled-brightgreen.svg)](https://github.com/mui/mui-x/issues/2081)
[![Average time to resolve an issue](https://isitmaintained.com/badge/resolution/mui/mui-x.svg)](https://isitmaintained.com/project/mui/mui-x &#039;Average time to resolve an issue&#039;)
[![Open Collective backers and sponsors](https://img.shields.io/opencollective/all/mui-org)](https://opencollective.com/mui-org)
[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/6293/badge)](https://www.bestpractices.dev/projects/6293)

&lt;/div&gt;

[MUI X](https://mui.com/x/) is a suite of advanced React UI components for a wide range of complex use cases.
Each component provides best-in-class UX and DX, with sophisticated UX workflows for data-rich applications.
Components include the Data Grid, Date and Time Pickers, Charts, and Tree View.

MUI X extends the core functionality of [Material UI](https://github.com/mui/material-ui/), but the advanced components also stand on their own and can be fully customized to meet the needs of any design system.

MUI X is **open-core**: [Community](#community-plan) components are MIT-licensed and free forever, while more advanced features and components require a [Pro](#pro-plan) or [Premium](#premium-plan) commercial license.
See [Licensing](#licensing) for more information.

## Documentation

Get started in the [MUI X documentation](https://mui.com/x/introduction/).

- [Data Grid](https://mui.com/x/react-data-grid/)
- [Date and Time Pickers](https://mui.com/x/react-date-pickers/)
- [Charts](https://mui.com/x/react-charts/)
- [Tree View](https://mui.com/x/react-tree-view/)

### Installation

- [Data Grid installation](https://mui.com/x/react-data-grid/quickstart/#installation)
- [Date and Time Pickers installation](https://mui.com/x/react-date-pickers/quickstart/#installation)
- [Charts installation](https://mui.com/x/react-charts/quickstart/#installation)
- [Tree View installation](https://mui.com/x/react-tree-view/quickstart/#installation)

## Licensing

The MUI X team has been building MIT-licensed React components since 2014, starting with Material UI, and we&#039;re committed to the continued advancement of our open-source libraries.
Anything we release under an MIT license will remain MIT-licensed forever.
Learn more about [our stewardship ethos](https://mui-org.notion.site/Stewardship-542a2226043d4f4a96dfb429d16cf5bd).

We offer commercial licenses to developers who need the most advanced components and features that can&#039;t reasonably be maintained by the open-source community alone.
These licenses make it possible for us to support a full-time staff of engineers.

Rest assured that when we release features commercially, it&#039;s only because we believe you won&#039;t find a better MIT-licensed alternative anywhere else.

See the [Licensing page](https://mui.com/x/introduction/licensing/) for complete details.

### Plans

#### Community plan

The free Community version of MUI X contains components and features that we believe are maintainable by contributions from the open-source community.
It&#039;s published under an [MIT license](https://www.tldrlegal.com/license/mit-license) and it&#039;s [free forever](https://mui-org.notion.site/Stewardship-542a2226043d4f4a96dfb429d16cf5bd#20f609acab4441cf9346614119fbbac1).

- [`@mui/x-data-grid`](https://www.npmjs.com/package/@mui/x-data-grid)
- [`@mui/x-date-pickers`](https://www.npmjs.com/package/@mui/x-date-pickers)
- [`@mui/x-charts`](https://www.npmjs.com/package/@mui/x-charts)
- [`@mui/x-tree-view`](https://www.npmjs.com/package/@mui/x-tree-view)

#### Pro plan

MUI X Pro expands on the Community version with more advanced features and functionality.
The Data Grid Pro comes with multi-filtering, multi-sorting, column resizing, and column pinning; you also gain access to the Date and Time Range Picker components, advanced Charts, and drag-and-drop reordering for the Tree View.

The Pro version is available under a commercial license—visit the [Pricing page](https://mui.com/pricing/) for details.

- [`@mui/x-data-grid-pro`](https://www.npmjs.com/package/@mui/x-data-grid-pro)
- [`@mui/x-date-pickers-pro`](https://www.npmjs.com/package/@mui/x-date-pickers-pro)
- [`@mui/x-charts-pro`](https://www.npmjs.com/package/@mui/x-charts-pro)
- [`@mui/x-tree-view-pro`](https://www.npmjs.com/package/@mui/x-tree-view-pro)

#### Premium plan

MUI X Premium unlocks the most advanced features of the Data Grid, including row grouping and Excel exporting, as well as everything offered in the Pro plan.

The Premium version is available under a commercial license—visit the [Pricing page](https://mui.com/pricing/) for details.

- [`@mui/x-data-grid-premium`](https://www.npmjs.com/package/@mui/x-data-grid-premium)
  &lt;!-- TODO: CHARTS-PREMIUM: uncomment when ready --&gt;
  &lt;!-- - [`@mui/x-charts-premium`](https://www.npmjs.com/package/@mui/x-charts-premium) --&gt;

## Support

From community guidance to critical business support, we&#039;re here to help.
Read the [Support guide](https://mui.com/x/introduction/support/) for details.

## Contributing

Read the [Contributing guide](/CONTRIBUTING.md) to learn about our development process, how to propose bug fixes and improvements, and how to build and test your changes.

Contributing to MUI X is about more than just issues and pull requests!
There are many other ways to [support MUI X](https://mui.com/material-ui/getting-started/faq/#mui-is-an-awesome-organization-how-can-i-support-it) beyond contributing to the code base.

## Changelog

The [changelog](https://github.com/mui/mui-x/releases) is regularly updated to reflect what&#039;s changed in each new release.

## Roadmap

Future plans and high-priority features and enhancements can be found in the [roadmap](https://mui.com/x/introduction/roadmap/).

## Security

For details on supported versions and contact information for reporting security issues, please refer to the [security policy](https://github.com/mui/mui-x/security/policy).
</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[prisma/prisma]]></title>
            <link>https://github.com/prisma/prisma</link>
            <guid>https://github.com/prisma/prisma</guid>
            <pubDate>Sat, 09 Aug 2025 00:04:49 GMT</pubDate>
            <description><![CDATA[Next-generation ORM for Node.js & TypeScript | PostgreSQL, MySQL, MariaDB, SQL Server, SQLite, MongoDB and CockroachDB]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/prisma/prisma">prisma/prisma</a></h1>
            <p>Next-generation ORM for Node.js & TypeScript | PostgreSQL, MySQL, MariaDB, SQL Server, SQLite, MongoDB and CockroachDB</p>
            <p>Language: TypeScript</p>
            <p>Stars: 43,454</p>
            <p>Forks: 1,829</p>
            <p>Stars today: 155 stars today</p>
            <h2>README</h2><pre>![Prisma](https://i.imgur.com/h6UIYTu.png)

&lt;div align=&quot;center&quot;&gt;
  &lt;h1&gt;Prisma&lt;/h1&gt;
  &lt;a href=&quot;https://www.npmjs.com/package/prisma&quot;&gt;&lt;img src=&quot;https://img.shields.io/npm/v/prisma.svg?style=flat&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/prisma/prisma/blob/main/CONTRIBUTING.md&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/prisma/prisma/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-Apache%202-blue&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://pris.ly/discord&quot;&gt;&lt;img alt=&quot;Discord&quot; src=&quot;https://img.shields.io/discord/937751382725886062?label=Discord&quot;&gt;&lt;/a&gt;
  &lt;br /&gt;
  &lt;br /&gt;
  &lt;a href=&quot;https://www.prisma.io/docs/getting-started/quickstart&quot;&gt;Quickstart&lt;/a&gt;
  &lt;span&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp;&lt;/span&gt;
  &lt;a href=&quot;https://www.prisma.io/&quot;&gt;Website&lt;/a&gt;
  &lt;span&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp;&lt;/span&gt;
  &lt;a href=&quot;https://www.prisma.io/docs/&quot;&gt;Docs&lt;/a&gt;
  &lt;span&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp;&lt;/span&gt;
  &lt;a href=&quot;https://github.com/prisma/prisma-examples/&quot;&gt;Examples&lt;/a&gt;
  &lt;span&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp;&lt;/span&gt;
  &lt;a href=&quot;https://www.prisma.io/blog&quot;&gt;Blog&lt;/a&gt;
  &lt;span&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp;&lt;/span&gt;
  &lt;a href=&quot;https://pris.ly/discord?utm_source=github&amp;utm_medium=prisma&amp;utm_content=repo_readme&quot;&gt;Discord&lt;/a&gt;
  &lt;span&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp;&lt;/span&gt;
  &lt;a href=&quot;https://pris.ly/x?utm_source=github&amp;utm_medium=prisma&amp;utm_content=repo_readme&quot;&gt;Twitter&lt;/a&gt;
  &lt;span&gt;&amp;nbsp;&amp;nbsp;•&amp;nbsp;&amp;nbsp;&lt;/span&gt;
  &lt;a href=&quot;https://pris.ly/youtube?utm_source=github&amp;utm_medium=prisma&amp;utm_content=repo_readme&quot;&gt;Youtube&lt;/a&gt;
  &lt;br /&gt;
  &lt;hr /&gt;
&lt;/div&gt;

## What is Prisma?

Prisma ORM is a **next-generation ORM** that consists of these tools:

- [**Prisma Client**](https://www.prisma.io/docs/concepts/components/prisma-client): Auto-generated and type-safe query builder for Node.js &amp; TypeScript
- [**Prisma Migrate**](https://www.prisma.io/docs/concepts/components/prisma-migrate): Declarative data modeling &amp; migration system
- [**Prisma Studio**](https://github.com/prisma/studio): GUI to view and edit data in your database

Prisma Client can be used in _any_ Node.js or TypeScript backend application (including serverless applications and microservices). This can be a [REST API](https://www.prisma.io/docs/concepts/overview/prisma-in-your-stack/rest), a [GraphQL API](https://www.prisma.io/docs/concepts/overview/prisma-in-your-stack/graphql), a gRPC API, or anything else that needs a database.

**If you need a database to use with Prisma ORM, check out [Prisma Postgres](https://www.prisma.io/docs/getting-started/quickstart-prismaPostgres?utm_source=github&amp;utm_medium=prisma-readme) or if you are looking for our MCP Server, head [here](https://github.com/prisma/mcp).**

## Getting started

### Quickstart (5min)

The fastest way to get started with Prisma is by following the quickstart guides. You can choose either of two databases:

- [Prisma Postgres](https://www.prisma.io/docs/getting-started/quickstart-prismaPostgres)
- [SQLite](https://www.prisma.io/docs/getting-started/quickstart-sqlite)

### Bring your own database

If you already have your own database, you can follow these guides:

- [Add Prisma to an existing project](https://www.prisma.io/docs/getting-started/setup-prisma/add-to-existing-project/relational-databases-typescript-postgresql)
- [Set up a new project with Prisma from scratch](https://www.prisma.io/docs/getting-started/setup-prisma/start-from-scratch/relational-databases-typescript-postgresql)

## How Prisma ORM works

This section provides a high-level overview of how Prisma ORM works and its most important technical components. For a more thorough introduction, visit the [Prisma documentation](https://www.prisma.io/docs/).

### The Prisma schema

Every project that uses a tool from the Prisma toolkit starts with a [Prisma schema file](https://www.prisma.io/docs/concepts/components/prisma-schema). The Prisma schema allows developers to define their _application models_ in an intuitive data modeling language. It also contains the connection to a database and defines a _generator_:

```prisma
// Data source
datasource db {
  provider = &quot;postgresql&quot;
  url      = env(&quot;DATABASE_URL&quot;)
}

// Generator
generator client {
  provider = &quot;prisma-client-js&quot;
}

// Data model
model Post {
  id        Int     @id @default(autoincrement())
  title     String
  content   String?
  published Boolean @default(false)
  author    User?   @relation(fields:  [authorId], references: [id])
  authorId  Int?
}

model User {
  id    Int     @id @default(autoincrement())
  email String  @unique
  name  String?
  posts Post[]
}
```

In this schema, you configure three things:

- **Data source**: Specifies your database connection (via an environment variable)
- **Generator**: Indicates that you want to generate Prisma Client
- **Data model**: Defines your application models

---

### The Prisma data model

On this page, the focus is on the data model. You can learn more about [Data sources](https://www.prisma.io/docs/reference/tools-and-interfaces/prisma-schema/data-sources) and [Generators](https://www.prisma.io/docs/reference/tools-and-interfaces/prisma-schema/generators) on the respective docs pages.

#### Functions of Prisma models

The data model is a collection of [models](https://www.prisma.io/docs/concepts/components/prisma-schema/data-model#defining-models). A model has two major functions:

- Represent a table in the underlying database
- Provide the foundation for the queries in the Prisma Client API

#### Getting a data model

There are two major workflows for &quot;getting&quot; a data model into your Prisma schema:

- Generate the data model from [introspecting](https://www.prisma.io/docs/concepts/components/introspection) a database
- Manually writing the data model and mapping it to the database with [Prisma Migrate](https://www.prisma.io/docs/concepts/components/prisma-migrate)

Once the data model is defined, you can [generate Prisma Client](https://www.prisma.io/docs/concepts/components/prisma-client/generating-prisma-client) which will expose CRUD and more queries for the defined models. If you&#039;re using TypeScript, you&#039;ll get full type-safety for all queries (even when only retrieving the subsets of a model&#039;s fields).

---

### Accessing your database with Prisma Client

#### Generating Prisma Client

The first step when using Prisma Client is installing its npm package:

```
npm install @prisma/client
```

Note that the installation of this package invokes the `prisma generate` command which reads your Prisma schema and _generates_ the Prisma Client code. The code will be located in `node_modules/.prisma/client`, which is exported by `node_modules/@prisma/client/index.d.ts`.

After you change your data model, you&#039;ll need to manually re-generate Prisma Client to ensure the code inside `node_modules/.prisma/client` gets updated:

```
npx prisma generate
```

Refer to the documentation for more information about [&quot;generating the Prisma client&quot;](https://www.prisma.io/docs/concepts/components/prisma-client/generating-prisma-client).

#### Using Prisma Client to send queries to your database

Once the Prisma Client is generated, you can import it in your code and send queries to your database. This is what the setup code looks like.

##### Import and instantiate Prisma Client

You can import and instantiate Prisma Client as follows:

```ts
import { PrismaClient } from &#039;@prisma/client&#039;

const prisma = new PrismaClient()
```

or

```js
const { PrismaClient } = require(&#039;@prisma/client&#039;)

const prisma = new PrismaClient()
```

Now you can start sending queries via the generated Prisma Client API, here are a few sample queries. Note that all Prisma Client queries return _plain old JavaScript objects_.

Learn more about the available operations in the [Prisma Client docs](https://www.prisma.io/docs/concepts/components/prisma-client) or watch this [demo video](https://www.youtube.com/watch?v=LggrE5kJ75I&amp;list=PLn2e1F9Rfr6k9PnR_figWOcSHgc_erDr5&amp;index=4) (2 min).

##### Retrieve all `User` records from the database

```ts
const allUsers = await prisma.user.findMany()
```

##### Include the `posts` relation on each returned `User` object

```ts
const allUsers = await prisma.user.findMany({
  include: { posts: true },
})
```

##### Filter all `Post` records that contain `&quot;prisma&quot;`

```ts
const filteredPosts = await prisma.post.findMany({
  where: {
    OR: [{ title: { contains: &#039;prisma&#039; } }, { content: { contains: &#039;prisma&#039; } }],
  },
})
```

##### Create a new `User` and a new `Post` record in the same query

```ts
const user = await prisma.user.create({
  data: {
    name: &#039;Alice&#039;,
    email: &#039;alice@prisma.io&#039;,
    posts: {
      create: { title: &#039;Join us for Prisma Day 2021&#039; },
    },
  },
})
```

##### Update an existing `Post` record

```ts
const post = await prisma.post.update({
  where: { id: 42 },
  data: { published: true },
})
```

#### Usage with TypeScript

Note that when using TypeScript, the result of this query will be _statically typed_ so that you can&#039;t accidentally access a property that doesn&#039;t exist (and any typos are caught at compile-time). Learn more about leveraging Prisma Client&#039;s generated types on the [Advanced usage of generated types](https://www.prisma.io/docs/concepts/components/prisma-client/advanced-usage-of-generated-types) page in the docs.

## Community

Prisma has a large and supportive [community](https://www.prisma.io/community) of enthusiastic application developers. You can join us on [Discord](https://pris.ly/discord) and here on [GitHub](https://github.com/prisma/prisma/discussions).

## Badges

[![Made with Prisma](http://made-with.prisma.io/dark.svg)](https://prisma.io) [![Made with Prisma](http://made-with.prisma.io/indigo.svg)](https://prisma.io)

Built something awesome with Prisma? 🌟 Show it off with these [badges](https://github.com/prisma/presskit?tab=readme-ov-file#badges), perfect for your readme or website.

```
[![Made with Prisma](http://made-with.prisma.io/dark.svg)](https://prisma.io)
```

```
[![Made with Prisma](http://made-with.prisma.io/indigo.svg)](https://prisma.io)
```

## Security

If you have a security issue to report, please contact us at [security@prisma.io](mailto:security@prisma.io?subject=[GitHub]%20Prisma%202%20Security%20Report%20).

## Support

### Ask a question about Prisma

You can ask questions and initiate [discussions](https://github.com/prisma/prisma/discussions/) about Prisma-related topics in the `prisma` repository on GitHub.

👉 [**Ask a question**](https://github.com/prisma/prisma/discussions/new)

### Create a bug report for Prisma

If you see an error message or run into an issue, please make sure to create a bug report! You can find [best practices for creating bug reports](https://www.prisma.io/docs/guides/other/troubleshooting-orm/creating-bug-reports) (like including additional debugging output) in the docs.

👉 [**Create bug report**](https://pris.ly/prisma-prisma-bug-report)

### Submit a feature request

If Prisma currently doesn&#039;t have a certain feature, be sure to check out the [roadmap](https://www.prisma.io/docs/more/roadmap) to see if this is already planned for the future.

If the feature on the roadmap is linked to a GitHub issue, please make sure to leave a 👍 reaction on the issue and ideally a comment with your thoughts about the feature!

👉 [**Submit feature request**](https://github.com/prisma/prisma/issues/new?assignees=&amp;labels=&amp;template=feature_request.md&amp;title=)

## Contributing

Refer to our [contribution guidelines](https://github.com/prisma/prisma/blob/main/CONTRIBUTING.md) and [Code of Conduct for contributors](https://github.com/prisma/prisma/blob/main/CODE_OF_CONDUCT.md).

## Tests Status

- Prisma Tests Status:
  [![Prisma Tests Status](https://github.com/prisma/prisma/workflows/CI/badge.svg)](https://github.com/prisma/prisma/actions/workflows/test.yml?query=branch%3Amain)
- Ecosystem Tests Status:
  [![Ecosystem Tests Status](https://github.com/prisma/ecosystem-tests/workflows/test/badge.svg)](https://github.com/prisma/ecosystem-tests/actions/workflows/test.yaml?query=branch%3Adev)
</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[humanlayer/12-factor-agents]]></title>
            <link>https://github.com/humanlayer/12-factor-agents</link>
            <guid>https://github.com/humanlayer/12-factor-agents</guid>
            <pubDate>Sat, 09 Aug 2025 00:04:48 GMT</pubDate>
            <description><![CDATA[What are the principles we can use to build LLM-powered software that is actually good enough to put in the hands of production customers?]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/humanlayer/12-factor-agents">humanlayer/12-factor-agents</a></h1>
            <p>What are the principles we can use to build LLM-powered software that is actually good enough to put in the hands of production customers?</p>
            <p>Language: TypeScript</p>
            <p>Stars: 12,092</p>
            <p>Forks: 812</p>
            <p>Stars today: 127 stars today</p>
            <h2>README</h2><pre># 12-Factor Agents - Principles for building reliable LLM applications

&lt;div align=&quot;center&quot;&gt;
&lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/Code-Apache%202.0-blue.svg&quot; alt=&quot;Code License: Apache 2.0&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://creativecommons.org/licenses/by-sa/4.0/&quot;&gt;
        &lt;img src=&quot;https://img.shields.io/badge/Content-CC%20BY--SA%204.0-lightgrey.svg&quot; alt=&quot;Content License: CC BY-SA 4.0&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://humanlayer.dev/discord&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/chat-discord-5865F2&quot; alt=&quot;Discord Server&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.youtube.com/watch?v=8kMaTybvDUw&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/aidotengineer-conf_talk_(17m)-white&quot; alt=&quot;YouTube
Deep Dive&quot;&gt;&lt;/a&gt;
&lt;a href=&quot;https://www.youtube.com/watch?v=yxJDyQ8v6P0&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/badge/youtube-deep_dive-crimson&quot; alt=&quot;YouTube
Deep Dive&quot;&gt;&lt;/a&gt;
    
&lt;/div&gt;

&lt;p&gt;&lt;/p&gt;

*In the spirit of [12 Factor Apps](https://12factor.net/)*.  *The source for this project is public at https://github.com/humanlayer/12-factor-agents, and I welcome your feedback and contributions. Let&#039;s figure this out together!*

&gt; [!TIP]
&gt; Missed the AI Engineer World&#039;s Fair? [Catch the talk here](https://www.youtube.com/watch?v=8kMaTybvDUw)
&gt;
&gt; Looking for Context Engineering? [Jump straight to factor 3](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-03-own-your-context-window.md)
&gt;
&gt; Want to contribute to `npx/uvx create-12-factor-agent` - check out [the discussion thread](https://github.com/humanlayer/12-factor-agents/discussions/61)


&lt;img referrerpolicy=&quot;no-referrer-when-downgrade&quot; src=&quot;https://static.scarf.sh/a.png?x-pxid=2acad99a-c2d9-48df-86f5-9ca8061b7bf9&quot; /&gt;

&lt;a href=&quot;#visual-nav&quot;&gt;&lt;img width=&quot;907&quot; alt=&quot;Screenshot 2025-04-03 at 2 49 07 PM&quot; src=&quot;https://github.com/user-attachments/assets/23286ad8-7bef-4902-b371-88ff6a22e998&quot; /&gt;&lt;/a&gt;


Hi, I&#039;m Dex. I&#039;ve been [hacking](https://youtu.be/8bIHcttkOTE) on [AI agents](https://theouterloop.substack.com) for [a while](https://humanlayer.dev). 


**I&#039;ve tried every agent framework out there**, from the plug-and-play crew/langchains to the &quot;minimalist&quot; smolagents of the world to the &quot;production grade&quot; langraph, griptape, etc. 

**I&#039;ve talked to a lot of really strong founders**, in and out of YC, who are all building really impressive things with AI. Most of them are rolling the stack themselves. I don&#039;t see a lot of frameworks in production customer-facing agents.

**I&#039;ve been surprised to find** that most of the products out there billing themselves as &quot;AI Agents&quot; are not all that agentic. A lot of them are mostly deterministic code, with LLM steps sprinkled in at just the right points to make the experience truly magical.

Agents, at least the good ones, don&#039;t follow the [&quot;here&#039;s your prompt, here&#039;s a bag of tools, loop until you hit the goal&quot;](https://www.anthropic.com/engineering/building-effective-agents#agents) pattern. Rather, they are comprised of mostly just software. 

So, I set out to answer:

&gt; ### **What are the principles we can use to build LLM-powered software that is actually good enough to put in the hands of production customers?**

Welcome to 12-factor agents. As every Chicago mayor since Daley has consistently plastered all over the city&#039;s major airports, we&#039;re glad you&#039;re here.

*Special thanks to [@iantbutler01](https://github.com/iantbutler01), [@tnm](https://github.com/tnm), [@hellovai](https://www.github.com/hellovai), [@stantonk](https://www.github.com/stantonk), [@balanceiskey](https://www.github.com/balanceiskey), [@AdjectiveAllison](https://www.github.com/AdjectiveAllison), [@pfbyjy](https://www.github.com/pfbyjy), [@a-churchill](https://www.github.com/a-churchill), and the SF MLOps community for early feedback on this guide.*

## The Short Version: The 12 Factors

Even if LLMs [continue to get exponentially more powerful](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-10-small-focused-agents.md#what-if-llms-get-smarter), there will be core engineering techniques that make LLM-powered software more reliable, more scalable, and easier to maintain.

- [How We Got Here: A Brief History of Software](https://github.com/humanlayer/12-factor-agents/blob/main/content/brief-history-of-software.md)
- [Factor 1: Natural Language to Tool Calls](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-01-natural-language-to-tool-calls.md)
- [Factor 2: Own your prompts](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-02-own-your-prompts.md)
- [Factor 3: Own your context window](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-03-own-your-context-window.md)
- [Factor 4: Tools are just structured outputs](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-04-tools-are-structured-outputs.md)
- [Factor 5: Unify execution state and business state](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-05-unify-execution-state.md)
- [Factor 6: Launch/Pause/Resume with simple APIs](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-06-launch-pause-resume.md)
- [Factor 7: Contact humans with tool calls](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-07-contact-humans-with-tools.md)
- [Factor 8: Own your control flow](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-08-own-your-control-flow.md)
- [Factor 9: Compact Errors into Context Window](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-09-compact-errors.md)
- [Factor 10: Small, Focused Agents](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-10-small-focused-agents.md)
- [Factor 11: Trigger from anywhere, meet users where they are](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-11-trigger-from-anywhere.md)
- [Factor 12: Make your agent a stateless reducer](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-12-stateless-reducer.md)

### Visual Nav

|    |    |    |
|----|----|-----|
|[![factor 1](https://github.com/humanlayer/12-factor-agents/blob/main/img/110-natural-language-tool-calls.png)](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-01-natural-language-to-tool-calls.md) | [![factor 2](https://github.com/humanlayer/12-factor-agents/blob/main/img/120-own-your-prompts.png)](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-02-own-your-prompts.md) | [![factor 3](https://github.com/humanlayer/12-factor-agents/blob/main/img/130-own-your-context-building.png)](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-03-own-your-context-window.md) |
|[![factor 4](https://github.com/humanlayer/12-factor-agents/blob/main/img/140-tools-are-just-structured-outputs.png)](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-04-tools-are-structured-outputs.md) | [![factor 5](https://github.com/humanlayer/12-factor-agents/blob/main/img/150-unify-state.png)](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-05-unify-execution-state.md) | [![factor 6](https://github.com/humanlayer/12-factor-agents/blob/main/img/160-pause-resume-with-simple-apis.png)](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-06-launch-pause-resume.md) |
| [![factor 7](https://github.com/humanlayer/12-factor-agents/blob/main/img/170-contact-humans-with-tools.png)](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-07-contact-humans-with-tools.md) | [![factor 8](https://github.com/humanlayer/12-factor-agents/blob/main/img/180-control-flow.png)](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-08-own-your-control-flow.md) | [![factor 9](https://github.com/humanlayer/12-factor-agents/blob/main/img/190-factor-9-errors-static.png)](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-09-compact-errors.md) |
| [![factor 10](https://github.com/humanlayer/12-factor-agents/blob/main/img/1a0-small-focused-agents.png)](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-10-small-focused-agents.md) | [![factor 11](https://github.com/humanlayer/12-factor-agents/blob/main/img/1b0-trigger-from-anywhere.png)](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-11-trigger-from-anywhere.md) | [![factor 12](https://github.com/humanlayer/12-factor-agents/blob/main/img/1c0-stateless-reducer.png)](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-12-stateless-reducer.md) |

## How we got here

For a deeper dive on my agent journey and what led us here, check out [A Brief History of Software](https://github.com/humanlayer/12-factor-agents/blob/main/content/brief-history-of-software.md) - a quick summary here:

### The promise of agents

We&#039;re gonna talk a lot about Directed Graphs (DGs) and their Acyclic friends, DAGs. I&#039;ll start by pointing out that...well...software is a directed graph. There&#039;s a reason we used to represent programs as flow charts.

![010-software-dag](https://github.com/humanlayer/12-factor-agents/blob/main/img/010-software-dag.png)

### From code to DAGs

Around 20 years ago, we started to see DAG orchestrators become popular. We&#039;re talking classics like [Airflow](https://airflow.apache.org/), [Prefect](https://www.prefect.io/), some predecessors, and some newer ones like ([dagster](https://dagster.io/), [inggest](https://www.inngest.com/), [windmill](https://www.windmill.dev/)). These followed the same graph pattern, with the added benefit of observability, modularity, retries, administration, etc.

![015-dag-orchestrators](https://github.com/humanlayer/12-factor-agents/blob/main/img/015-dag-orchestrators.png)

### The promise of agents

I&#039;m not the first [person to say this](https://youtu.be/Dc99-zTMyMg?si=bcT0hIwWij2mR-40&amp;t=73), but my biggest takeaway when I started learning about agents, was that you get to throw the DAG away. Instead of software engineers coding each step and edge case, you can give the agent a goal and a set of transitions:

![025-agent-dag](https://github.com/humanlayer/12-factor-agents/blob/main/img/025-agent-dag.png)

And let the LLM make decisions in real time to figure out the path

![026-agent-dag-lines](https://github.com/humanlayer/12-factor-agents/blob/main/img/026-agent-dag-lines.png)

The promise here is that you write less software, you just give the LLM the &quot;edges&quot; of the graph and let it figure out the nodes. You can recover from errors, you can write less code, and you may find that LLMs find novel solutions to problems.


### Agents as loops

As we&#039;ll see later, it turns out this doesn&#039;t quite work.

Let&#039;s dive one step deeper - with agents you&#039;ve got this loop consisting of 3 steps:

1. LLM determines the next step in the workflow, outputting structured json (&quot;tool calling&quot;)
2. Deterministic code executes the tool call
3. The result is appended to the context window 
4. Repeat until the next step is determined to be &quot;done&quot;

```python
initial_event = {&quot;message&quot;: &quot;...&quot;}
context = [initial_event]
while True:
  next_step = await llm.determine_next_step(context)
  context.append(next_step)

  if (next_step.intent === &quot;done&quot;):
    return next_step.final_answer

  result = await execute_step(next_step)
  context.append(result)
```

Our initial context is just the starting event (maybe a user message, maybe a cron fired, maybe a webhook, etc), and we ask the llm to choose the next step (tool) or to determine that we&#039;re done.

Here&#039;s a multi-step example:

[![027-agent-loop-animation](https://github.com/humanlayer/12-factor-agents/blob/main/img/027-agent-loop-animation.gif)](https://github.com/user-attachments/assets/3beb0966-fdb1-4c12-a47f-ed4e8240f8fd)

&lt;details&gt;
&lt;summary&gt;&lt;a href=&quot;https://github.com/humanlayer/12-factor-agents/blob/main/img/027-agent-loop-animation.gif&quot;&gt;GIF Version&lt;/a&gt;&lt;/summary&gt;

![027-agent-loop-animation](https://github.com/humanlayer/12-factor-agents/blob/main/img/027-agent-loop-animation.gif)]

&lt;/details&gt;

## Why 12-factor agents?

At the end of the day, this approach just doesn&#039;t work as well as we want it to.

In building HumanLayer, I&#039;ve talked to at least 100 SaaS builders (mostly technical founders) looking to make their existing product more agentic. The journey usually goes something like:

1. Decide you want to build an agent
2. Product design, UX mapping, what problems to solve
3. Want to move fast, so grab $FRAMEWORK and *get to building*
4. Get to 70-80% quality bar 
5. Realize that 80% isn&#039;t good enough for most customer-facing features
6. Realize that getting past 80% requires reverse-engineering the framework, prompts, flow, etc.
7. Start over from scratch

&lt;details&gt;
&lt;summary&gt;Random Disclaimers&lt;/summary&gt;

**DISCLAIMER**: I&#039;m not sure the exact right place to say this, but here seems as good as any: **this in BY NO MEANS meant to be a dig on either the many frameworks out there, or the pretty dang smart people who work on them**. They enable incredible things and have accelerated the AI ecosystem. 

I hope that one outcome of this post is that agent framework builders can learn from the journeys of myself and others, and make frameworks even better. 

Especially for builders who want to move fast but need deep control.

**DISCLAIMER 2**: I&#039;m not going to talk about MCP. I&#039;m sure you can see where it fits in.

**DISCLAIMER 3**: I&#039;m using mostly typescript, for [reasons](https://www.linkedin.com/posts/dexterihorthy_llms-typescript-aiagents-activity-7290858296679313408-Lh9e?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAA4oHTkByAiD-wZjnGsMBUL_JT6nyyhOh30) but all this stuff works in python or any other language you prefer. 


Anyways back to the thing...

&lt;/details&gt;

### Design Patterns for great LLM applications

After digging through hundreds of AI libriaries and working with dozens of founders, my instinct is this:

1. There are some core things that make agents great
2. Going all in on a framework and building what is essentially a greenfield rewrite may be counter-productive
3. There are some core principles that make agents great, and you will get most/all of them if you pull in a framework
4. BUT, the fastest way I&#039;ve seen for builders to get high-quality AI software in the hands of customers is to take small, modular concepts from agent building, and incorporate them into their existing product
5. These modular concepts from agents can be defined and applied by most skilled software engineers, even if they don&#039;t have an AI background

&gt; #### The fastest way I&#039;ve seen for builders to get good AI software in the hands of customers is to take small, modular concepts from agent building, and incorporate them into their existing product


## The 12 Factors (again)


- [How We Got Here: A Brief History of Software](https://github.com/humanlayer/12-factor-agents/blob/main/content/brief-history-of-software.md)
- [Factor 1: Natural Language to Tool Calls](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-01-natural-language-to-tool-calls.md)
- [Factor 2: Own your prompts](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-02-own-your-prompts.md)
- [Factor 3: Own your context window](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-03-own-your-context-window.md)
- [Factor 4: Tools are just structured outputs](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-04-tools-are-structured-outputs.md)
- [Factor 5: Unify execution state and business state](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-05-unify-execution-state.md)
- [Factor 6: Launch/Pause/Resume with simple APIs](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-06-launch-pause-resume.md)
- [Factor 7: Contact humans with tool calls](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-07-contact-humans-with-tools.md)
- [Factor 8: Own your control flow](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-08-own-your-control-flow.md)
- [Factor 9: Compact Errors into Context Window](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-09-compact-errors.md)
- [Factor 10: Small, Focused Agents](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-10-small-focused-agents.md)
- [Factor 11: Trigger from anywhere, meet users where they are](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-11-trigger-from-anywhere.md)
- [Factor 12: Make your agent a stateless reducer](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-12-stateless-reducer.md)

## Honorable Mentions / other advice

- [Factor 13: Pre-fetch all the context you might need](https://github.com/humanlayer/12-factor-agents/blob/main/content/appendix-13-pre-fetch.md)

## Related Resources

- Contribute to this guide [here](https://github.com/humanlayer/12-factor-agents)
- [I talked about a lot of this on an episode of the Tool Use podcast](https://youtu.be/8bIHcttkOTE) in March 2025
- I write about some of this stuff at [The Outer Loop](https://theouterloop.substack.com)
- I do [webinars about Maximizing LLM Performance](https://github.com/hellovai/ai-that-works/tree/main) with [@hellovai](https://github.com/hellovai)
- We build OSS agents with this methodology under [got-agents/agents](https://github.com/got-agents/agents)
- We ignored all our own advice and built a [framework for running distributed agents in kubernetes](https://github.com/humanlayer/kubechain)
- Other links from this guide:
  - [12 Factor Apps](https://12factor.net)
  - [Building Effective Agents (Anthropic)](https://www.anthropic.com/engineering/building-effective-agents#agents)
  - [Prompts are Functions](https://thedataexchange.media/baml-revolution-in-ai-engineering/ )
  - [Library patterns: Why frameworks are evil](https://tomasp.net/blog/2015/library-frameworks/)
  - [The Wrong Abstraction](https://sandimetz.com/blog/2016/1/20/the-wrong-abstraction)
  - [Mailcrew Agent](https://github.com/dexhorthy/mailcrew)
  - [Mailcrew Demo Video](https://www.youtube.com/watch?v=f_cKnoPC_Oo)
  - [Chainlit Demo](https://x.com/chainlit_io/status/1858613325921480922)
  - [TypeScript for LLMs](https://www.linkedin.com/posts/dexterihorthy_llms-typescript-aiagents-activity-7290858296679313408-Lh9e)
  - [Schema Aligned Parsing](https://www.boundaryml.com/blog/schema-aligned-parsing)
  - [Function Calling vs Structured Outputs vs JSON Mode](https://www.vellum.ai/blog/when-should-i-use-function-calling-structured-outputs-or-json-mode)
  - [BAML on GitHub](https://github.com/boundaryml/baml)
  - [OpenAI JSON vs Function Calling](https://docs.llamaindex.ai/en/stable/examples/llm/openai_json_vs_function_calling/)
  - [Outer Loop Agents](https://theouterloop.substack.com/p/openais-realtime-api-is-a-step-towards)
  - [Airflow](https://airflow.apache.org/)
  - [Prefect](https://www.prefect.io/)
  - [Dagster](https://dagster.io/)
  - [Inngest](https://www.inngest.com/)
  - [Windmill](https://www.windmill.dev/)
  - [The AI Agent Index (MIT)](https://aiagentindex.mit.edu/)
  - [NotebookLM on Finding Model Capability Boundaries](https://open.substack.com/pub/swyx/p/notebooklm?selection=08e1187c-cfee-4c63-93c9-71216640a5f8)

## Contributors

Thanks to everyone who has contributed to 12-factor agents!

[&lt;img src=&quot;https://avatars.githubusercontent.com/u/3730605?v=4&amp;s=80&quot; width=&quot;80px&quot; alt=&quot;dexhorthy&quot; /&gt;](https://github.com/dexhorthy) [&lt;img src=&quot;https://avatars.githubusercontent.com/u/50557586?v=4&amp;s=80&quot; width=&quot;80px&quot; alt=&quot;Sypherd&quot; /&gt;](https://github.com/Sypherd) [&lt;img src=&quot;https://avatars.githubusercontent.com/u/66259401?v=4&amp;s=80&quot; width=&quot;80px&quot; alt=&quot;tofaramususa&quot; /&gt;](https://github.com/tofaramususa) [&lt;img src=&quot;https://avatars.githubusercontent.com/u/18105223?v=4&amp;s=80&quot; width=&quot;80px&quot; alt=&quot;a-churchill&quot; /&gt;](https://github.com/a-church

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[danny-avila/LibreChat]]></title>
            <link>https://github.com/danny-avila/LibreChat</link>
            <guid>https://github.com/danny-avila/LibreChat</guid>
            <pubDate>Sat, 09 Aug 2025 00:04:47 GMT</pubDate>
            <description><![CDATA[Enhanced ChatGPT Clone: Features Agents, DeepSeek, Anthropic, AWS, OpenAI, Responses API, Azure, Groq, o1, GPT-5, Mistral, OpenRouter, Vertex AI, Gemini, Artifacts, AI model switching, message search, Code Interpreter, langchain, DALL-E-3, OpenAPI Actions, Functions, Secure Multi-User Auth, Presets, open-source for self-hosting. Active project.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/danny-avila/LibreChat">danny-avila/LibreChat</a></h1>
            <p>Enhanced ChatGPT Clone: Features Agents, DeepSeek, Anthropic, AWS, OpenAI, Responses API, Azure, Groq, o1, GPT-5, Mistral, OpenRouter, Vertex AI, Gemini, Artifacts, AI model switching, message search, Code Interpreter, langchain, DALL-E-3, OpenAPI Actions, Functions, Secure Multi-User Auth, Presets, open-source for self-hosting. Active project.</p>
            <p>Language: TypeScript</p>
            <p>Stars: 28,881</p>
            <p>Forks: 5,304</p>
            <p>Stars today: 35 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://librechat.ai&quot;&gt;
    &lt;img src=&quot;client/public/assets/logo.svg&quot; height=&quot;256&quot;&gt;
  &lt;/a&gt;
  &lt;h1 align=&quot;center&quot;&gt;
    &lt;a href=&quot;https://librechat.ai&quot;&gt;LibreChat&lt;/a&gt;
  &lt;/h1&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://discord.librechat.ai&quot;&gt; 
    &lt;img
      src=&quot;https://img.shields.io/discord/1086345563026489514?label=&amp;logo=discord&amp;style=for-the-badge&amp;logoWidth=20&amp;logoColor=white&amp;labelColor=000000&amp;color=blueviolet&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://www.youtube.com/@LibreChat&quot;&gt; 
    &lt;img
      src=&quot;https://img.shields.io/badge/YOUTUBE-red.svg?style=for-the-badge&amp;logo=youtube&amp;logoColor=white&amp;labelColor=000000&amp;logoWidth=20&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://docs.librechat.ai&quot;&gt; 
    &lt;img
      src=&quot;https://img.shields.io/badge/DOCS-blue.svg?style=for-the-badge&amp;logo=read-the-docs&amp;logoColor=white&amp;labelColor=000000&amp;logoWidth=20&quot;&gt;
  &lt;/a&gt;
  &lt;a aria-label=&quot;Sponsors&quot; href=&quot;https://github.com/sponsors/danny-avila&quot;&gt;
    &lt;img
      src=&quot;https://img.shields.io/badge/SPONSORS-brightgreen.svg?style=for-the-badge&amp;logo=github-sponsors&amp;logoColor=white&amp;labelColor=000000&amp;logoWidth=20&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;a href=&quot;https://railway.app/template/b5k2mn?referralCode=HI9hWz&quot;&gt;
  &lt;img src=&quot;https://railway.app/button.svg&quot; alt=&quot;Deploy on Railway&quot; height=&quot;30&quot;&gt;
&lt;/a&gt;
&lt;a href=&quot;https://zeabur.com/templates/0X2ZY8&quot;&gt;
  &lt;img src=&quot;https://zeabur.com/button.svg&quot; alt=&quot;Deploy on Zeabur&quot; height=&quot;30&quot;/&gt;
&lt;/a&gt;
&lt;a href=&quot;https://template.cloud.sealos.io/deploy?templateName=librechat&quot;&gt;
  &lt;img src=&quot;https://raw.githubusercontent.com/labring-actions/templates/main/Deploy-on-Sealos.svg&quot; alt=&quot;Deploy on Sealos&quot; height=&quot;30&quot;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.librechat.ai/docs/translation&quot;&gt;
    &lt;img 
      src=&quot;https://img.shields.io/badge/dynamic/json.svg?style=for-the-badge&amp;color=2096F3&amp;label=locize&amp;query=%24.translatedPercentage&amp;url=https://api.locize.app/badgedata/4cb2598b-ed4d-469c-9b04-2ed531a8cb45&amp;suffix=%+translated&quot; 
      alt=&quot;Translation Progress&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;


# ✨ Features

- 🖥️ **UI &amp; Experience** inspired by ChatGPT with enhanced design and features

- 🤖 **AI Model Selection**:  
  - Anthropic (Claude), AWS Bedrock, OpenAI, Azure OpenAI, Google, Vertex AI, OpenAI Responses API (incl. Azure)
  - [Custom Endpoints](https://www.librechat.ai/docs/quick_start/custom_endpoints): Use any OpenAI-compatible API with LibreChat, no proxy required
  - Compatible with [Local &amp; Remote AI Providers](https://www.librechat.ai/docs/configuration/librechat_yaml/ai_endpoints):
    - Ollama, groq, Cohere, Mistral AI, Apple MLX, koboldcpp, together.ai,
    - OpenRouter, Perplexity, ShuttleAI, Deepseek, Qwen, and more

- 🔧 **[Code Interpreter API](https://www.librechat.ai/docs/features/code_interpreter)**: 
  - Secure, Sandboxed Execution in Python, Node.js (JS/TS), Go, C/C++, Java, PHP, Rust, and Fortran
  - Seamless File Handling: Upload, process, and download files directly
  - No Privacy Concerns: Fully isolated and secure execution

- 🔦 **Agents &amp; Tools Integration**:  
  - **[LibreChat Agents](https://www.librechat.ai/docs/features/agents)**:
    - No-Code Custom Assistants: Build specialized, AI-driven helpers without coding  
    - Flexible &amp; Extensible: Use MCP Servers, tools, file search, code execution, and more  
    - Compatible with Custom Endpoints, OpenAI, Azure, Anthropic, AWS Bedrock, Google, Vertex AI, Responses API, and more
    - [Model Context Protocol (MCP) Support](https://modelcontextprotocol.io/clients#librechat) for Tools

- 🔍 **Web Search**:  
  - Search the internet and retrieve relevant information to enhance your AI context
  - Combines search providers, content scrapers, and result rerankers for optimal results
  - **[Learn More →](https://www.librechat.ai/docs/features/web_search)**

- 🪄 **Generative UI with Code Artifacts**:  
  - [Code Artifacts](https://youtu.be/GfTj7O4gmd0?si=WJbdnemZpJzBrJo3) allow creation of React, HTML, and Mermaid diagrams directly in chat

- 🎨 **Image Generation &amp; Editing**
  - Text-to-image and image-to-image with [GPT-Image-1](https://www.librechat.ai/docs/features/image_gen#1--openai-image-tools-recommended)
  - Text-to-image with [DALL-E (3/2)](https://www.librechat.ai/docs/features/image_gen#2--dalle-legacy), [Stable Diffusion](https://www.librechat.ai/docs/features/image_gen#3--stable-diffusion-local), [Flux](https://www.librechat.ai/docs/features/image_gen#4--flux), or any [MCP server](https://www.librechat.ai/docs/features/image_gen#5--model-context-protocol-mcp)
  - Produce stunning visuals from prompts or refine existing images with a single instruction

- 💾 **Presets &amp; Context Management**:  
  - Create, Save, &amp; Share Custom Presets  
  - Switch between AI Endpoints and Presets mid-chat
  - Edit, Resubmit, and Continue Messages with Conversation branching  
  - [Fork Messages &amp; Conversations](https://www.librechat.ai/docs/features/fork) for Advanced Context control

- 💬 **Multimodal &amp; File Interactions**:  
  - Upload and analyze images with Claude 3, GPT-4.5, GPT-4o, o1, Llama-Vision, and Gemini 📸  
  - Chat with Files using Custom Endpoints, OpenAI, Azure, Anthropic, AWS Bedrock, &amp; Google 🗃️

- 🌎 **Multilingual UI**:  
  - English, 中文, Deutsch, Español, Français, Italiano, Polski, Português Brasileiro
  - Русский, 日本語, Svenska, 한국어, Tiếng Việt, 繁體中文, العربية, Türkçe, Nederlands, עברית

- 🧠 **Reasoning UI**:  
  - Dynamic Reasoning UI for Chain-of-Thought/Reasoning AI models like DeepSeek-R1

- 🎨 **Customizable Interface**:  
  - Customizable Dropdown &amp; Interface that adapts to both power users and newcomers

- 🗣️ **Speech &amp; Audio**:  
  - Chat hands-free with Speech-to-Text and Text-to-Speech  
  - Automatically send and play Audio  
  - Supports OpenAI, Azure OpenAI, and Elevenlabs

- 📥 **Import &amp; Export Conversations**:  
  - Import Conversations from LibreChat, ChatGPT, Chatbot UI  
  - Export conversations as screenshots, markdown, text, json

- 🔍 **Search &amp; Discovery**:  
  - Search all messages/conversations

- 👥 **Multi-User &amp; Secure Access**:
  - Multi-User, Secure Authentication with OAuth2, LDAP, &amp; Email Login Support
  - Built-in Moderation, and Token spend tools

- ⚙️ **Configuration &amp; Deployment**:  
  - Configure Proxy, Reverse Proxy, Docker, &amp; many Deployment options  
  - Use completely local or deploy on the cloud

- 📖 **Open-Source &amp; Community**:  
  - Completely Open-Source &amp; Built in Public  
  - Community-driven development, support, and feedback

[For a thorough review of our features, see our docs here](https://docs.librechat.ai/) 📚

## 🪶 All-In-One AI Conversations with LibreChat

LibreChat brings together the future of assistant AIs with the revolutionary technology of OpenAI&#039;s ChatGPT. Celebrating the original styling, LibreChat gives you the ability to integrate multiple AI models. It also integrates and enhances original client features such as conversation and message search, prompt templates and plugins.

With LibreChat, you no longer need to opt for ChatGPT Plus and can instead use free or pay-per-call APIs. We welcome contributions, cloning, and forking to enhance the capabilities of this advanced chatbot platform.

[![Watch the video](https://raw.githubusercontent.com/LibreChat-AI/librechat.ai/main/public/images/changelog/v0.7.6.gif)](https://www.youtube.com/watch?v=ilfwGQtJNlI)

Click on the thumbnail to open the video☝️

---

## 🌐 Resources

**GitHub Repo:**
  - **RAG API:** [github.com/danny-avila/rag_api](https://github.com/danny-avila/rag_api)
  - **Website:** [github.com/LibreChat-AI/librechat.ai](https://github.com/LibreChat-AI/librechat.ai)

**Other:**
  - **Website:** [librechat.ai](https://librechat.ai)
  - **Documentation:** [librechat.ai/docs](https://librechat.ai/docs)
  - **Blog:** [librechat.ai/blog](https://librechat.ai/blog)

---

## 📝 Changelog

Keep up with the latest updates by visiting the releases page and notes:
- [Releases](https://github.com/danny-avila/LibreChat/releases)
- [Changelog](https://www.librechat.ai/changelog) 

**⚠️ Please consult the [changelog](https://www.librechat.ai/changelog) for breaking changes before updating.**

---

## ⭐ Star History

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://star-history.com/#danny-avila/LibreChat&amp;Date&quot;&gt;
    &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=danny-avila/LibreChat&amp;type=Date&amp;theme=dark&quot; onerror=&quot;this.src=&#039;https://api.star-history.com/svg?repos=danny-avila/LibreChat&amp;type=Date&#039;&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://trendshift.io/repositories/4685&quot; target=&quot;_blank&quot; style=&quot;padding: 10px;&quot;&gt;
    &lt;img src=&quot;https://trendshift.io/api/badge/repositories/4685&quot; alt=&quot;danny-avila%2FLibreChat | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot;/&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://runacap.com/ross-index/q1-24/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; style=&quot;margin-left: 20px;&quot;&gt;
    &lt;img style=&quot;width: 260px; height: 56px&quot; src=&quot;https://runacap.com/wp-content/uploads/2024/04/ROSS_badge_white_Q1_2024.svg&quot; alt=&quot;ROSS Index - Fastest Growing Open-Source Startups in Q1 2024 | Runa Capital&quot; width=&quot;260&quot; height=&quot;56&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;

---

## ✨ Contributions

Contributions, suggestions, bug reports and fixes are welcome!

For new features, components, or extensions, please open an issue and discuss before sending a PR.

If you&#039;d like to help translate LibreChat into your language, we&#039;d love your contribution! Improving our translations not only makes LibreChat more accessible to users around the world but also enhances the overall user experience. Please check out our [Translation Guide](https://www.librechat.ai/docs/translation).

---

## 💖 This project exists in its current state thanks to all the people who contribute

&lt;a href=&quot;https://github.com/danny-avila/LibreChat/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=danny-avila/LibreChat&quot; /&gt;
&lt;/a&gt;

---

## 🎉 Special Thanks

We thank [Locize](https://locize.com) for their translation management tools that support multiple languages in LibreChat.

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://locize.com&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;
    &lt;img src=&quot;https://github.com/user-attachments/assets/d6b70894-6064-475e-bb65-92a9e23e0077&quot; alt=&quot;Locize Logo&quot; height=&quot;50&quot;&gt;
  &lt;/a&gt;
&lt;/p&gt;
</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[Budibase/budibase]]></title>
            <link>https://github.com/Budibase/budibase</link>
            <guid>https://github.com/Budibase/budibase</guid>
            <pubDate>Sat, 09 Aug 2025 00:04:46 GMT</pubDate>
            <description><![CDATA[Create business apps and automate workflows in minutes. Supports PostgreSQL, MySQL, MariaDB, MSSQL, MongoDB, Rest API, Docker, K8s, and more 🚀 No code / Low code platform..]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/Budibase/budibase">Budibase/budibase</a></h1>
            <p>Create business apps and automate workflows in minutes. Supports PostgreSQL, MySQL, MariaDB, MSSQL, MongoDB, Rest API, Docker, K8s, and more 🚀 No code / Low code platform..</p>
            <p>Language: TypeScript</p>
            <p>Stars: 25,260</p>
            <p>Forks: 1,817</p>
            <p>Stars today: 40 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.budibase.com&quot;&gt;
    &lt;img alt=&quot;Budibase&quot; src=&quot;https://res.cloudinary.com/daog6scxm/image/upload/v1696515725/Branding/Assets/Symbol/RGB/Full%20Colour/Budibase_Symbol_RGB_FullColour_cbqvha_1_z5cwq2.svg&quot; width=&quot;60&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;
&lt;h1 align=&quot;center&quot;&gt;
  Budibase
&lt;/h1&gt;

&lt;h3 align=&quot;center&quot;&gt;
  The low code platform you&#039;ll enjoy using
&lt;/h3&gt;
&lt;p align=&quot;center&quot;&gt;
  Budibase is an open-source low-code platform that saves engineers 100s of hours building forms, portals, and approval apps, securely.
&lt;/p&gt;

&lt;h3 align=&quot;center&quot;&gt;
 🤖 🎨 🚀
&lt;/h3&gt;
&lt;br&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img alt=&quot;Budibase design ui&quot; src=&quot;https://res.cloudinary.com/daog6scxm/image/upload/v1680181644/ui/homepage-design-ui_sizp7b.png&quot;&gt;
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://github.com/Budibase/budibase/releases&quot;&gt;
    &lt;img alt=&quot;GitHub all releases&quot; src=&quot;https://img.shields.io/github/downloads/Budibase/budibase/total&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://github.com/Budibase/budibase/releases&quot;&gt;
    &lt;img alt=&quot;GitHub release (latest by date)&quot; src=&quot;https://img.shields.io/github/v/release/Budibase/budibase&quot;&gt;
  &lt;/a&gt;
  &lt;a href=&quot;https://twitter.com/intent/follow?screen_name=budibase&quot;&gt;
    &lt;img src=&quot;https://img.shields.io/twitter/follow/budibase?style=social&quot; alt=&quot;Follow @budibase&quot; /&gt;
  &lt;/a&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg&quot; alt=&quot;Code of conduct&quot; /&gt;
  &lt;a href=&quot;https://codecov.io/gh/Budibase/budibase&quot;&gt;
    &lt;img src=&quot;https://codecov.io/gh/Budibase/budibase/graph/badge.svg?token=E8W2ZFXQOH&quot;/&gt;
  &lt;/a&gt;
&lt;/p&gt;

&lt;h3 align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://account.budibase.app/register&quot;&gt;Get started - we host (Budibase Cloud)&lt;/a&gt;
  &lt;span&gt; · &lt;/span&gt;
  &lt;a href=&quot;https://docs.budibase.com/docs/hosting-methods&quot;&gt;Get started - you host (Docker, K8s, DO)&lt;/a&gt;
  &lt;span&gt; · &lt;/span&gt;
  &lt;a href=&quot;https://docs.budibase.com/docs&quot;&gt;Docs&lt;/a&gt;
  &lt;span&gt; · &lt;/span&gt;
  &lt;a href=&quot;https://github.com/Budibase/budibase/discussions?discussions_q=category%3AIdeas&quot;&gt;Feature request&lt;/a&gt;
  &lt;span&gt; · &lt;/span&gt;
  &lt;a href=&quot;https://github.com/Budibase/budibase/issues&quot;&gt;Report a bug&lt;/a&gt;
  &lt;span&gt; · &lt;/span&gt;
  Support: &lt;a href=&quot;https://github.com/Budibase/budibase/discussions&quot;&gt;Discussions&lt;/a&gt;
&lt;/h3&gt;

&lt;br /&gt;&lt;br /&gt;

## ✨ Features

### Build and ship real software

Unlike other platforms, with Budibase you build and ship single page applications. Budibase applications have performance baked in and can be designed responsively, providing users with a great experience.
&lt;br /&gt;&lt;br /&gt;

### Open source and extensible

Budibase is open-source - licensed as GPL v3. This should fill you with confidence that Budibase will always be around. You can also code against Budibase or fork it and make changes as you please, providing a developer-friendly experience.
&lt;br /&gt;&lt;br /&gt;

### Load data or start from scratch

Budibase pulls data from multiple sources, including MongoDB, CouchDB, PostgreSQL, MariaDB, MySQL, Airtable, S3, DynamoDB, or a REST API. And unlike other platforms, with Budibase you can start from scratch and create business apps with no data sources. [Request new datasources](https://github.com/Budibase/budibase/discussions?discussions_q=category%3AIdeas).

&lt;p align=&quot;center&quot;&gt;
  &lt;img alt=&quot;Budibase data&quot; src=&quot;https://res.cloudinary.com/daog6scxm/image/upload/v1680281798/ui/data_klbuna.png&quot;&gt;
&lt;/p&gt;
&lt;br /&gt;&lt;br /&gt;

### Design and build apps with powerful pre-made components

Budibase comes out of the box with beautifully designed, powerful components which you can use like building blocks to build your UI. We also expose many of your favourite CSS styling options so you can go that extra creative mile. [Request new component](https://github.com/Budibase/budibase/discussions?discussions_q=category%3AIdeas).

&lt;p align=&quot;center&quot;&gt;
  &lt;img alt=&quot;Budibase design&quot; src=&quot;https://res.cloudinary.com/daog6scxm/image/upload/v1675437167/ui/form_2x_mbli8y.png&quot;&gt;
&lt;/p&gt;
&lt;br /&gt;&lt;br /&gt;

### Automate processes, integrate with other tools and connect to webhooks

Save time by automating manual processes and workflows. From connecting to webhooks to automating emails, simply tell Budibase what to do and let it work for you. You can easily [create new automations for Budibase here](https://github.com/Budibase/automations) or [Request new automation](https://github.com/Budibase/budibase/discussions?discussions_q=category%3AIdeas).
&lt;br /&gt;&lt;br /&gt;

### Integrate with your favorite tools

Budibase integrates with a number of popular tools allowing you to build apps that perfectly fit your stack.

&lt;p align=&quot;center&quot;&gt;
  &lt;img alt=&quot;Budibase integrations&quot; src=&quot;https://res.cloudinary.com/daog6scxm/image/upload/v1680195228/ui/automate_fg9z07.png&quot;&gt;
&lt;/p&gt;
&lt;br /&gt;&lt;br /&gt;

### Deploy with confidence and security

Budibase is made to scale. With Budibase, you can self-host on your own infrastructure and globally manage users, onboarding, SMTP, apps, groups, theming and more. You can also provide users/groups with an app portal and disseminate user management to the group manager.

- Checkout the promo video: https://youtu.be/xoljVpty_Kw

&lt;br /&gt;

---

&lt;br /&gt;

## Budibase Public API

As with anything that we build in Budibase, our new public API is simple to use, flexible, and introduces new extensibility. To summarize, the Budibase API enables:

- Budibase as a backend
- Interoperability

#### Docs

You can learn more about the Budibase API at the following places:

- [General documentation](https://docs.budibase.com/docs/public-api): Learn how to get your API key, how to use spec, and how to use Postman
- [Interactive API documentation](https://docs.budibase.com/reference/appcreate) : Learn how to interact with the API

&lt;br /&gt;&lt;br /&gt;

## 🏁 Get started

Deploy Budibase using Docker, Kubernetes, and Digital Ocean on your existing infrastructure. Or use Budibase Cloud if you don&#039;t need to self-host and would like to get started quickly.

### [Get started with self-hosting Budibase](https://docs.budibase.com/docs/hosting-methods)

- [Docker - single ARM compatible image](https://docs.budibase.com/docs/docker)
- [Docker Compose](https://docs.budibase.com/docs/docker-compose)
- [Kubernetes](https://docs.budibase.com/docs/kubernetes-k8s)
- [Digital Ocean](https://docs.budibase.com/docs/digitalocean)
- [Portainer](https://docs.budibase.com/docs/portainer)

### [Get started with Budibase Cloud](https://budibase.com)

&lt;br /&gt;&lt;br /&gt;

## 🎓 Learning Budibase

The Budibase documentation [lives here](https://docs.budibase.com/docs).
&lt;br /&gt;

&lt;br /&gt;&lt;br /&gt;

## 💬 Community

If you have a question or would like to talk with other Budibase users and join our community, please hop over to [Github discussions](https://github.com/Budibase/budibase/discussions)

&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;

## ❗ Code of conduct

Budibase is dedicated to providing everyone a welcoming, diverse, and harassment-free experience. We expect everyone in the Budibase community to abide by our [**Code of Conduct**](https://github.com/Budibase/budibase/blob/HEAD/docs/CODE_OF_CONDUCT.md). Please read it.
&lt;br /&gt;

&lt;br /&gt;&lt;br /&gt;

## 🙌 Contributing to Budibase

From opening a bug report to creating a pull request: every contribution is appreciated and welcomed. If you&#039;re planning to implement a new feature or change the API, please create an issue first. This way, we can ensure your work is not in vain.
Environment setup instructions are available [here](https://github.com/Budibase/budibase/tree/HEAD/docs/CONTRIBUTING.md).

### Not Sure Where to Start?

A good place to start contributing is by looking for the [good first issue](https://github.com/Budibase/budibase/labels/good%20first%20issue) tag.

### How the repository is organized

Budibase is a monorepo managed by lerna. Lerna manages the building and publishing of the budibase packages. At a high level, here are the packages that make up Budibase.

- [packages/builder](https://github.com/Budibase/budibase/tree/HEAD/packages/builder) - contains code for the budibase builder client-side svelte application.

- [packages/client](https://github.com/Budibase/budibase/tree/HEAD/packages/client) - A module that runs in the browser responsible for reading JSON definition and creating living, breathing web apps from it.

- [packages/server](https://github.com/Budibase/budibase/tree/HEAD/packages/server) - The budibase server. This Koa app is responsible for serving the JS for the builder and budibase apps, as well as providing the API for interaction with the database and file system.

For more information, see [CONTRIBUTING.md](https://github.com/Budibase/budibase/blob/HEAD/docs/CONTRIBUTING.md)

&lt;br /&gt;&lt;br /&gt;

## 📝 License

Budibase is open-source, licensed as [GPL v3](https://www.gnu.org/licenses/gpl-3.0.en.html). The client and component libraries are licensed as [MPL](https://directory.fsf.org/wiki/License:MPL-2.0) - so the apps you build can be licensed however you like.

&lt;br /&gt;&lt;br /&gt;

## ⭐ Stargazers over time

[![Stargazers over time](https://starchart.cc/Budibase/budibase.svg)](https://starchart.cc/Budibase/budibase)

If you are having issues between updates of the builder, please use the guide [here](https://github.com/Budibase/budibase/blob/HEAD/docs/CONTRIBUTING.md#troubleshooting) to clear down your environment.

&lt;br /&gt;&lt;br /&gt;

## Contributors ✨

Thanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):

&lt;a href=&quot;https://github.com/Budibase/budibase/graphs/contributors&quot;&gt;
  &lt;img src=&quot;https://contrib.rocks/image?repo=Budibase/budibase&quot; /&gt;
&lt;/a&gt;

Made with [contrib.rocks](https://contrib.rocks).
</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[ItzCrazyKns/Perplexica]]></title>
            <link>https://github.com/ItzCrazyKns/Perplexica</link>
            <guid>https://github.com/ItzCrazyKns/Perplexica</guid>
            <pubDate>Sat, 09 Aug 2025 00:04:45 GMT</pubDate>
            <description><![CDATA[Perplexica is an AI-powered search engine. It is an Open source alternative to Perplexity AI]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/ItzCrazyKns/Perplexica">ItzCrazyKns/Perplexica</a></h1>
            <p>Perplexica is an AI-powered search engine. It is an Open source alternative to Perplexity AI</p>
            <p>Language: TypeScript</p>
            <p>Stars: 23,434</p>
            <p>Forks: 2,446</p>
            <p>Stars today: 38 stars today</p>
            <h2>README</h2><pre># 🚀 Perplexica - An AI-powered search engine 🔎 &lt;!-- omit in toc --&gt;

&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt;
   &lt;sup&gt;Special thanks to:&lt;/sup&gt;
   &lt;br&gt;
   &lt;br&gt;
   &lt;a href=&quot;https://www.warp.dev/perplexica&quot;&gt;
      &lt;img alt=&quot;Warp sponsorship&quot; width=&quot;400&quot; src=&quot;https://github.com/user-attachments/assets/775dd593-9b5f-40f1-bf48-479faff4c27b&quot;&gt;
   &lt;/a&gt;

### [Warp, the AI Devtool that lives in your terminal](https://www.warp.dev/perplexica)

[Available for MacOS, Linux, &amp; Windows](https://www.warp.dev/perplexica)

&lt;/div&gt;

&lt;hr/&gt;

[![Discord](https://dcbadge.limes.pink/api/server/26aArMy8tT?style=flat)](https://discord.gg/26aArMy8tT)

![preview](.assets/perplexica-screenshot.png?)

## Table of Contents &lt;!-- omit in toc --&gt;

- [Overview](#overview)
- [Preview](#preview)
- [Features](#features)
- [Installation](#installation)
  - [Getting Started with Docker (Recommended)](#getting-started-with-docker-recommended)
  - [Non-Docker Installation](#non-docker-installation)
  - [Ollama Connection Errors](#ollama-connection-errors)
- [Using as a Search Engine](#using-as-a-search-engine)
- [Using Perplexica&#039;s API](#using-perplexicas-api)
- [Expose Perplexica to a network](#expose-perplexica-to-network)
- [One-Click Deployment](#one-click-deployment)
- [Upcoming Features](#upcoming-features)
- [Support Us](#support-us)
  - [Donations](#donations)
- [Contribution](#contribution)
- [Help and Support](#help-and-support)

## Overview

Perplexica is an open-source AI-powered searching tool or an AI-powered search engine that goes deep into the internet to find answers. Inspired by Perplexity AI, it&#039;s an open-source option that not just searches the web but understands your questions. It uses advanced machine learning algorithms like similarity searching and embeddings to refine results and provides clear answers with sources cited.

Using SearxNG to stay current and fully open source, Perplexica ensures you always get the most up-to-date information without compromising your privacy.

Want to know more about its architecture and how it works? You can read it [here](https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/architecture/README.md).

## Preview

![video-preview](.assets/perplexica-preview.gif)

## Features

- **Local LLMs**: You can make use local LLMs such as Llama3 and Mixtral using Ollama.
- **Two Main Modes:**
  - **Copilot Mode:** (In development) Boosts search by generating different queries to find more relevant internet sources. Like normal search instead of just using the context by SearxNG, it visits the top matches and tries to find relevant sources to the user&#039;s query directly from the page.
  - **Normal Mode:** Processes your query and performs a web search.
- **Focus Modes:** Special modes to better answer specific types of questions. Perplexica currently has 6 focus modes:
  - **All Mode:** Searches the entire web to find the best results.
  - **Writing Assistant Mode:** Helpful for writing tasks that do not require searching the web.
  - **Academic Search Mode:** Finds articles and papers, ideal for academic research.
  - **YouTube Search Mode:** Finds YouTube videos based on the search query.
  - **Wolfram Alpha Search Mode:** Answers queries that need calculations or data analysis using Wolfram Alpha.
  - **Reddit Search Mode:** Searches Reddit for discussions and opinions related to the query.
- **Current Information:** Some search tools might give you outdated info because they use data from crawling bots and convert them into embeddings and store them in a index. Unlike them, Perplexica uses SearxNG, a metasearch engine to get the results and rerank and get the most relevant source out of it, ensuring you always get the latest information without the overhead of daily data updates.
- **API**: Integrate Perplexica into your existing applications and make use of its capibilities.

It has many more features like image and video search. Some of the planned features are mentioned in [upcoming features](#upcoming-features).

## Installation

There are mainly 2 ways of installing Perplexica - With Docker, Without Docker. Using Docker is highly recommended.

### Getting Started with Docker (Recommended)

1. Ensure Docker is installed and running on your system.
2. Clone the Perplexica repository:

   ```bash
   git clone https://github.com/ItzCrazyKns/Perplexica.git
   ```

3. After cloning, navigate to the directory containing the project files.

4. Rename the `sample.config.toml` file to `config.toml`. For Docker setups, you need only fill in the following fields:

   - `OPENAI`: Your OpenAI API key. **You only need to fill this if you wish to use OpenAI&#039;s models**.
   - `OLLAMA`: Your Ollama API URL. You should enter it as `http://host.docker.internal:PORT_NUMBER`. If you installed Ollama on port 11434, use `http://host.docker.internal:11434`. For other ports, adjust accordingly. **You need to fill this if you wish to use Ollama&#039;s models instead of OpenAI&#039;s**.
   - `GROQ`: Your Groq API key. **You only need to fill this if you wish to use Groq&#039;s hosted models**.
   - `ANTHROPIC`: Your Anthropic API key. **You only need to fill this if you wish to use Anthropic models**.
   - `Gemini`: Your Gemini API key. **You only need to fill this if you wish to use Google&#039;s models**.
   - `DEEPSEEK`: Your Deepseek API key. **Only needed if you want Deepseek models.**
   - `AIMLAPI`: Your AI/ML API key. **Only needed if you want to use AI/ML API models and embeddings.**

     **Note**: You can change these after starting Perplexica from the settings dialog.

   - `SIMILARITY_MEASURE`: The similarity measure to use (This is filled by default; you can leave it as is if you are unsure about it.)

5. Ensure you are in the directory containing the `docker-compose.yaml` file and execute:

   ```bash
   docker compose up -d
   ```

6. Wait a few minutes for the setup to complete. You can access Perplexica at http://localhost:3000 in your web browser.

**Note**: After the containers are built, you can start Perplexica directly from Docker without having to open a terminal.

### Non-Docker Installation

1. Install SearXNG and allow `JSON` format in the SearXNG settings.
2. Clone the repository and rename the `sample.config.toml` file to `config.toml` in the root directory. Ensure you complete all required fields in this file.
3. After populating the configuration run `npm i`.
4. Install the dependencies and then execute `npm run build`.
5. Finally, start the app by running `npm run start`

**Note**: Using Docker is recommended as it simplifies the setup process, especially for managing environment variables and dependencies.

See the [installation documentation](https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/installation) for more information like updating, etc.

### Ollama Connection Errors

If you&#039;re encountering an Ollama connection error, it is likely due to the backend being unable to connect to Ollama&#039;s API. To fix this issue you can:

1. **Check your Ollama API URL:** Ensure that the API URL is correctly set in the settings menu.
2. **Update API URL Based on OS:**

   - **Windows:** Use `http://host.docker.internal:11434`
   - **Mac:** Use `http://host.docker.internal:11434`
   - **Linux:** Use `http://&lt;private_ip_of_host&gt;:11434`

   Adjust the port number if you&#039;re using a different one.

3. **Linux Users - Expose Ollama to Network:**

   - Inside `/etc/systemd/system/ollama.service`, you need to add `Environment=&quot;OLLAMA_HOST=0.0.0.0:11434&quot;`. (Change the port number if you are using a different one.) Then reload the systemd manager configuration with `systemctl daemon-reload`, and restart Ollama by `systemctl restart ollama`. For more information see [Ollama docs](https://github.com/ollama/ollama/blob/main/docs/faq.md#setting-environment-variables-on-linux)

   - Ensure that the port (default is 11434) is not blocked by your firewall.

## Using as a Search Engine

If you wish to use Perplexica as an alternative to traditional search engines like Google or Bing, or if you want to add a shortcut for quick access from your browser&#039;s search bar, follow these steps:

1. Open your browser&#039;s settings.
2. Navigate to the &#039;Search Engines&#039; section.
3. Add a new site search with the following URL: `http://localhost:3000/?q=%s`. Replace `localhost` with your IP address or domain name, and `3000` with the port number if Perplexica is not hosted locally.
4. Click the add button. Now, you can use Perplexica directly from your browser&#039;s search bar.

## Using Perplexica&#039;s API

Perplexica also provides an API for developers looking to integrate its powerful search engine into their own applications. You can run searches, use multiple models and get answers to your queries.

For more details, check out the full documentation [here](https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/API/SEARCH.md).

## Expose Perplexica to network

Perplexica runs on Next.js and handles all API requests. It works right away on the same network and stays accessible even with port forwarding.

## One-Click Deployment

[![Deploy to Sealos](https://raw.githubusercontent.com/labring-actions/templates/main/Deploy-on-Sealos.svg)](https://usw.sealos.io/?openapp=system-template%3FtemplateName%3Dperplexica)
[![Deploy to RepoCloud](https://d16t0pc4846x52.cloudfront.net/deploylobe.svg)](https://repocloud.io/details/?app_id=267)
[![Run on ClawCloud](https://raw.githubusercontent.com/ClawCloud/Run-Template/refs/heads/main/Run-on-ClawCloud.svg)](https://template.run.claw.cloud/?referralCode=U11MRQ8U9RM4&amp;openapp=system-fastdeploy%3FtemplateName%3Dperplexica)

## Upcoming Features

- [x] Add settings page
- [x] Adding support for local LLMs
- [x] History Saving features
- [x] Introducing various Focus Modes
- [x] Adding API support
- [x] Adding Discover
- [ ] Finalizing Copilot Mode

## Support Us

If you find Perplexica useful, consider giving us a star on GitHub. This helps more people discover Perplexica and supports the development of new features. Your support is greatly appreciated.

### Donations

We also accept donations to help sustain our project. If you would like to contribute, you can use the following options to donate. Thank you for your support!

| Ethereum                                              |
| ----------------------------------------------------- |
| Address: `0xB025a84b2F269570Eb8D4b05DEdaA41D8525B6DD` |

## Contribution

Perplexica is built on the idea that AI and large language models should be easy for everyone to use. If you find bugs or have ideas, please share them in via GitHub Issues. For more information on contributing to Perplexica you can read the [CONTRIBUTING.md](CONTRIBUTING.md) file to learn more about Perplexica and how you can contribute to it.

## Help and Support

If you have any questions or feedback, please feel free to reach out to us. You can create an issue on GitHub or join our Discord server. There, you can connect with other users, share your experiences and reviews, and receive more personalized help. [Click here](https://discord.gg/EFwsmQDgAu) to join the Discord server. To discuss matters outside of regular support, feel free to contact me on Discord at `itzcrazykns`.

Thank you for exploring Perplexica, the AI-powered search engine designed to enhance your search experience. We are constantly working to improve Perplexica and expand its capabilities. We value your feedback and contributions which help us make Perplexica even better. Don&#039;t forget to check back for updates and new features!
</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[openai/openai-node]]></title>
            <link>https://github.com/openai/openai-node</link>
            <guid>https://github.com/openai/openai-node</guid>
            <pubDate>Sat, 09 Aug 2025 00:04:44 GMT</pubDate>
            <description><![CDATA[Official JavaScript / TypeScript library for the OpenAI API]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/openai/openai-node">openai/openai-node</a></h1>
            <p>Official JavaScript / TypeScript library for the OpenAI API</p>
            <p>Language: TypeScript</p>
            <p>Stars: 9,579</p>
            <p>Forks: 1,172</p>
            <p>Stars today: 13 stars today</p>
            <h2>README</h2><pre># OpenAI TypeScript and JavaScript API Library

[![NPM version](&lt;https://img.shields.io/npm/v/openai.svg?label=npm%20(stable)&gt;)](https://npmjs.org/package/openai) ![npm bundle size](https://img.shields.io/bundlephobia/minzip/openai) [![JSR Version](https://jsr.io/badges/@openai/openai)](https://jsr.io/@openai/openai)

This library provides convenient access to the OpenAI REST API from TypeScript or JavaScript.

It is generated from our [OpenAPI specification](https://github.com/openai/openai-openapi) with [Stainless](https://stainlessapi.com/).

To learn how to use the OpenAI API, check out our [API Reference](https://platform.openai.com/docs/api-reference) and [Documentation](https://platform.openai.com/docs).

## Installation

```sh
npm install openai
```

### Installation from JSR

```sh
deno add jsr:@openai/openai
npx jsr add @openai/openai
```

These commands will make the module importable from the `@openai/openai` scope. You can also [import directly from JSR](https://jsr.io/docs/using-packages#importing-with-jsr-specifiers) without an install step if you&#039;re using the Deno JavaScript runtime:

```ts
import OpenAI from &#039;jsr:@openai/openai&#039;;
```

## Usage

The full API of this library can be found in [api.md file](api.md) along with many [code examples](https://github.com/openai/openai-node/tree/master/examples).

The primary API for interacting with OpenAI models is the [Responses API](https://platform.openai.com/docs/api-reference/responses). You can generate text from the model with the code below.

```ts
import OpenAI from &#039;openai&#039;;

const client = new OpenAI({
  apiKey: process.env[&#039;OPENAI_API_KEY&#039;], // This is the default and can be omitted
});

const response = await client.responses.create({
  model: &#039;gpt-4o&#039;,
  instructions: &#039;You are a coding assistant that talks like a pirate&#039;,
  input: &#039;Are semicolons optional in JavaScript?&#039;,
});

console.log(response.output_text);
```

The previous standard (supported indefinitely) for generating text is the [Chat Completions API](https://platform.openai.com/docs/api-reference/chat). You can use that API to generate text from the model with the code below.

```ts
import OpenAI from &#039;openai&#039;;

const client = new OpenAI({
  apiKey: process.env[&#039;OPENAI_API_KEY&#039;], // This is the default and can be omitted
});

const completion = await client.chat.completions.create({
  model: &#039;gpt-4o&#039;,
  messages: [
    { role: &#039;developer&#039;, content: &#039;Talk like a pirate.&#039; },
    { role: &#039;user&#039;, content: &#039;Are semicolons optional in JavaScript?&#039; },
  ],
});

console.log(completion.choices[0].message.content);
```

## Streaming responses

We provide support for streaming responses using Server Sent Events (SSE).

```ts
import OpenAI from &#039;openai&#039;;

const client = new OpenAI();

const stream = await client.responses.create({
  model: &#039;gpt-4o&#039;,
  input: &#039;Say &quot;Sheep sleep deep&quot; ten times fast!&#039;,
  stream: true,
});

for await (const event of stream) {
  console.log(event);
}
```

## File uploads

Request parameters that correspond to file uploads can be passed in many different forms:

- `File` (or an object with the same structure)
- a `fetch` `Response` (or an object with the same structure)
- an `fs.ReadStream`
- the return value of our `toFile` helper

```ts
import fs from &#039;fs&#039;;
import OpenAI, { toFile } from &#039;openai&#039;;

const client = new OpenAI();

// If you have access to Node `fs` we recommend using `fs.createReadStream()`:
await client.files.create({ file: fs.createReadStream(&#039;input.jsonl&#039;), purpose: &#039;fine-tune&#039; });

// Or if you have the web `File` API you can pass a `File` instance:
await client.files.create({ file: new File([&#039;my bytes&#039;], &#039;input.jsonl&#039;), purpose: &#039;fine-tune&#039; });

// You can also pass a `fetch` `Response`:
await client.files.create({ file: await fetch(&#039;https://somesite/input.jsonl&#039;), purpose: &#039;fine-tune&#039; });

// Finally, if none of the above are convenient, you can use our `toFile` helper:
await client.files.create({
  file: await toFile(Buffer.from(&#039;my bytes&#039;), &#039;input.jsonl&#039;),
  purpose: &#039;fine-tune&#039;,
});
await client.files.create({
  file: await toFile(new Uint8Array([0, 1, 2]), &#039;input.jsonl&#039;),
  purpose: &#039;fine-tune&#039;,
});
```

## Webhook Verification

Verifying webhook signatures is _optional but encouraged_.

For more information about webhooks, see [the API docs](https://platform.openai.com/docs/guides/webhooks).

### Parsing webhook payloads

For most use cases, you will likely want to verify the webhook and parse the payload at the same time. To achieve this, we provide the method `client.webhooks.unwrap()`, which parses a webhook request and verifies that it was sent by OpenAI. This method will throw an error if the signature is invalid.

Note that the `body` parameter must be the raw JSON string sent from the server (do not parse it first). The `.unwrap()` method will parse this JSON for you into an event object after verifying the webhook was sent from OpenAI.

```ts
import { headers } from &#039;next/headers&#039;;
import OpenAI from &#039;openai&#039;;

const client = new OpenAI({
  webhookSecret: process.env.OPENAI_WEBHOOK_SECRET, // env var used by default; explicit here.
});

export async function webhook(request: Request) {
  const headersList = headers();
  const body = await request.text();

  try {
    const event = client.webhooks.unwrap(body, headersList);

    switch (event.type) {
      case &#039;response.completed&#039;:
        console.log(&#039;Response completed:&#039;, event.data);
        break;
      case &#039;response.failed&#039;:
        console.log(&#039;Response failed:&#039;, event.data);
        break;
      default:
        console.log(&#039;Unhandled event type:&#039;, event.type);
    }

    return Response.json({ message: &#039;ok&#039; });
  } catch (error) {
    console.error(&#039;Invalid webhook signature:&#039;, error);
    return new Response(&#039;Invalid signature&#039;, { status: 400 });
  }
}
```

### Verifying webhook payloads directly

In some cases, you may want to verify the webhook separately from parsing the payload. If you prefer to handle these steps separately, we provide the method `client.webhooks.verifySignature()` to _only verify_ the signature of a webhook request. Like `.unwrap()`, this method will throw an error if the signature is invalid.

Note that the `body` parameter must be the raw JSON string sent from the server (do not parse it first). You will then need to parse the body after verifying the signature.

```ts
import { headers } from &#039;next/headers&#039;;
import OpenAI from &#039;openai&#039;;

const client = new OpenAI({
  webhookSecret: process.env.OPENAI_WEBHOOK_SECRET, // env var used by default; explicit here.
});

export async function webhook(request: Request) {
  const headersList = headers();
  const body = await request.text();

  try {
    client.webhooks.verifySignature(body, headersList);

    // Parse the body after verification
    const event = JSON.parse(body);
    console.log(&#039;Verified event:&#039;, event);

    return Response.json({ message: &#039;ok&#039; });
  } catch (error) {
    console.error(&#039;Invalid webhook signature:&#039;, error);
    return new Response(&#039;Invalid signature&#039;, { status: 400 });
  }
}
```

## Handling errors

When the library is unable to connect to the API,
or if the API returns a non-success status code (i.e., 4xx or 5xx response),
a subclass of `APIError` will be thrown:

&lt;!-- prettier-ignore --&gt;
```ts
const job = await client.fineTuning.jobs
  .create({ model: &#039;gpt-4o&#039;, training_file: &#039;file-abc123&#039; })
  .catch(async (err) =&gt; {
    if (err instanceof OpenAI.APIError) {
      console.log(err.request_id);
      console.log(err.status); // 400
      console.log(err.name); // BadRequestError
      console.log(err.headers); // {server: &#039;nginx&#039;, ...}
    } else {
      throw err;
    }
  });
```

Error codes are as follows:

| Status Code | Error Type                 |
| ----------- | -------------------------- |
| 400         | `BadRequestError`          |
| 401         | `AuthenticationError`      |
| 403         | `PermissionDeniedError`    |
| 404         | `NotFoundError`            |
| 422         | `UnprocessableEntityError` |
| 429         | `RateLimitError`           |
| &gt;=500       | `InternalServerError`      |
| N/A         | `APIConnectionError`       |

## Request IDs

&gt; For more information on debugging requests, see [these docs](https://platform.openai.com/docs/api-reference/debugging-requests)

All object responses in the SDK provide a `_request_id` property which is added from the `x-request-id` response header so that you can quickly log failing requests and report them back to OpenAI.

```ts
const completion = await client.chat.completions.create({
  messages: [{ role: &#039;user&#039;, content: &#039;Say this is a test&#039; }],
  model: &#039;gpt-4o&#039;,
});
console.log(completion._request_id); // req_123
```

You can also access the Request ID using the `.withResponse()` method:

```ts
const { data: stream, request_id } = await openai.chat.completions
  .create({
    model: &#039;gpt-4&#039;,
    messages: [{ role: &#039;user&#039;, content: &#039;Say this is a test&#039; }],
    stream: true,
  })
  .withResponse();
```

## Realtime API Beta

The Realtime API enables you to build low-latency, multi-modal conversational experiences. It currently supports text and audio as both input and output, as well as [function calling](https://platform.openai.com/docs/guides/function-calling) through a `WebSocket` connection.

```ts
import { OpenAIRealtimeWebSocket } from &#039;openai/beta/realtime/websocket&#039;;

const rt = new OpenAIRealtimeWebSocket({ model: &#039;gpt-4o-realtime-preview-2024-12-17&#039; });

rt.on(&#039;response.text.delta&#039;, (event) =&gt; process.stdout.write(event.delta));
```

For more information see [realtime.md](realtime.md).

## Microsoft Azure OpenAI

To use this library with [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/overview), use the `AzureOpenAI`
class instead of the `OpenAI` class.

&gt; [!IMPORTANT]
&gt; The Azure API shape slightly differs from the core API shape which means that the static types for responses / params
&gt; won&#039;t always be correct.

```ts
import { AzureOpenAI } from &#039;openai&#039;;
import { getBearerTokenProvider, DefaultAzureCredential } from &#039;@azure/identity&#039;;

const credential = new DefaultAzureCredential();
const scope = &#039;https://cognitiveservices.azure.com/.default&#039;;
const azureADTokenProvider = getBearerTokenProvider(credential, scope);

const openai = new AzureOpenAI({ azureADTokenProvider });

const result = await openai.chat.completions.create({
  model: &#039;gpt-4o&#039;,
  messages: [{ role: &#039;user&#039;, content: &#039;Say hello!&#039; }],
});

console.log(result.choices[0]!.message?.content);
```

### Retries

Certain errors will be automatically retried 2 times by default, with a short exponential backoff.
Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict,
429 Rate Limit, and &gt;=500 Internal errors will all be retried by default.

You can use the `maxRetries` option to configure or disable this:

&lt;!-- prettier-ignore --&gt;
```js
// Configure the default for all requests:
const client = new OpenAI({
  maxRetries: 0, // default is 2
});

// Or, configure per-request:
await client.chat.completions.create({ messages: [{ role: &#039;user&#039;, content: &#039;How can I get the name of the current day in JavaScript?&#039; }], model: &#039;gpt-4o&#039; }, {
  maxRetries: 5,
});
```

### Timeouts

Requests time out after 10 minutes by default. You can configure this with a `timeout` option:

&lt;!-- prettier-ignore --&gt;
```ts
// Configure the default for all requests:
const client = new OpenAI({
  timeout: 20 * 1000, // 20 seconds (default is 10 minutes)
});

// Override per-request:
await client.chat.completions.create({ messages: [{ role: &#039;user&#039;, content: &#039;How can I list all files in a directory using Python?&#039; }], model: &#039;gpt-4o&#039; }, {
  timeout: 5 * 1000,
});
```

On timeout, an `APIConnectionTimeoutError` is thrown.

Note that requests which time out will be [retried twice by default](#retries).

## Request IDs

&gt; For more information on debugging requests, see [these docs](https://platform.openai.com/docs/api-reference/debugging-requests)

All object responses in the SDK provide a `_request_id` property which is added from the `x-request-id` response header so that you can quickly log failing requests and report them back to OpenAI.

```ts
const response = await client.responses.create({ model: &#039;gpt-4o&#039;, input: &#039;testing 123&#039; });
console.log(response._request_id); // req_123
```

You can also access the Request ID using the `.withResponse()` method:

```ts
const { data: stream, request_id } = await openai.responses
  .create({
    model: &#039;gpt-4o&#039;,
    input: &#039;Say this is a test&#039;,
    stream: true,
  })
  .withResponse();
```

## Auto-pagination

List methods in the OpenAI API are paginated.
You can use the `for await … of` syntax to iterate through items across all pages:

```ts
async function fetchAllFineTuningJobs(params) {
  const allFineTuningJobs = [];
  // Automatically fetches more pages as needed.
  for await (const fineTuningJob of client.fineTuning.jobs.list({ limit: 20 })) {
    allFineTuningJobs.push(fineTuningJob);
  }
  return allFineTuningJobs;
}
```

Alternatively, you can request a single page at a time:

```ts
let page = await client.fineTuning.jobs.list({ limit: 20 });
for (const fineTuningJob of page.data) {
  console.log(fineTuningJob);
}

// Convenience methods are provided for manually paginating:
while (page.hasNextPage()) {
  page = await page.getNextPage();
  // ...
}
```

## Realtime API Beta

The Realtime API enables you to build low-latency, multi-modal conversational experiences. It currently supports text and audio as both input and output, as well as [function calling](https://platform.openai.com/docs/guides/function-calling) through a `WebSocket` connection.

```ts
import { OpenAIRealtimeWebSocket } from &#039;openai/beta/realtime/websocket&#039;;

const rt = new OpenAIRealtimeWebSocket({ model: &#039;gpt-4o-realtime-preview-2024-12-17&#039; });

rt.on(&#039;response.text.delta&#039;, (event) =&gt; process.stdout.write(event.delta));
```

For more information see [realtime.md](realtime.md).

## Microsoft Azure OpenAI

To use this library with [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/overview), use the `AzureOpenAI`
class instead of the `OpenAI` class.

&gt; [!IMPORTANT]
&gt; The Azure API shape slightly differs from the core API shape which means that the static types for responses / params
&gt; won&#039;t always be correct.

```ts
import { AzureOpenAI } from &#039;openai&#039;;
import { getBearerTokenProvider, DefaultAzureCredential } from &#039;@azure/identity&#039;;

const credential = new DefaultAzureCredential();
const scope = &#039;https://cognitiveservices.azure.com/.default&#039;;
const azureADTokenProvider = getBearerTokenProvider(credential, scope);

const openai = new AzureOpenAI({
  azureADTokenProvider,
  apiVersion: &#039;&lt;The API version, e.g. 2024-10-01-preview&gt;&#039;,
});

const result = await openai.chat.completions.create({
  model: &#039;gpt-4o&#039;,
  messages: [{ role: &#039;user&#039;, content: &#039;Say hello!&#039; }],
});

console.log(result.choices[0]!.message?.content);
```

For more information on support for the Azure API, see [azure.md](azure.md).

## Advanced Usage

### Accessing raw Response data (e.g., headers)

The &quot;raw&quot; `Response` returned by `fetch()` can be accessed through the `.asResponse()` method on the `APIPromise` type that all methods return.
This method returns as soon as the headers for a successful response are received and does not consume the response body, so you are free to write custom parsing or streaming logic.

You can also use the `.withResponse()` method to get the raw `Response` along with the parsed data.
Unlike `.asResponse()` this method consumes the body, returning once it is parsed.

&lt;!-- prettier-ignore --&gt;
```ts
const client = new OpenAI();

const httpResponse = await client.responses
  .create({ model: &#039;gpt-4o&#039;, input: &#039;say this is a test.&#039; })
  .asResponse();

// access the underlying web standard Response object
console.log(httpResponse.headers.get(&#039;X-My-Header&#039;));
console.log(httpResponse.statusText);

const { data: modelResponse, response: raw } = await client.responses
  .create({ model: &#039;gpt-4o&#039;, input: &#039;say this is a test.&#039; })
  .withResponse();
console.log(raw.headers.get(&#039;X-My-Header&#039;));
console.log(modelResponse);
```

### Logging

&gt; [!IMPORTANT]
&gt; All log messages are intended for debugging only. The format and content of log messages
&gt; may change between releases.

#### Log levels

The log level can be configured in two ways:

1. Via the `OPENAI_LOG` environment variable
2. Using the `logLevel` client option (overrides the environment variable if set)

```ts
import OpenAI from &#039;openai&#039;;

const client = new OpenAI({
  logLevel: &#039;debug&#039;, // Show all log messages
});
```

Available log levels, from most to least verbose:

- `&#039;debug&#039;` - Show debug messages, info, warnings, and errors
- `&#039;info&#039;` - Show info messages, warnings, and errors
- `&#039;warn&#039;` - Show warnings and errors (default)
- `&#039;error&#039;` - Show only errors
- `&#039;off&#039;` - Disable all logging

At the `&#039;debug&#039;` level, all HTTP requests and responses are logged, including headers and bodies.
Some authentication-related headers are redacted, but sensitive data in request and response bodies
may still be visible.

#### Custom logger

By default, this library logs to `globalThis.console`. You can also provide a custom logger.
Most logging libraries are supported, including [pino](https://www.npmjs.com/package/pino), [winston](https://www.npmjs.com/package/winston), [bunyan](https://www.npmjs.com/package/bunyan), [consola](https://www.npmjs.com/package/consola), [signale](https://www.npmjs.com/package/signale), and [@std/log](https://jsr.io/@std/log). If your logger doesn&#039;t work, please open an issue.

When providing a custom logger, the `logLevel` option still controls which messages are emitted, messages
below the configured level will not be sent to your logger.

```ts
import OpenAI from &#039;openai&#039;;
import pino from &#039;pino&#039;;

const logger = pino();

const client = new OpenAI({
  logger: logger.child({ name: &#039;OpenAI&#039; }),
  logLevel: &#039;debug&#039;, // Send all messages to pino, allowing it to filter
});
```

### Making custom/undocumented requests

This library is typed for convenient access to the documented API. If you need to access undocumented
endpoints, params, or response properties, the library can still be used.

#### Undocumented endpoints

To make requests to undocumented endpoints, you can use `client.get`, `client.post`, and other HTTP verbs.
Options on the client, such as retries, will be respected when making these requests.

```ts
await client.post(&#039;/some/path&#039;, {
  body: { some_prop: &#039;foo&#039; },
  query: { some_query_arg: &#039;bar&#039; },
});
```

#### Undocumented request params

To make requests using undocumented parameters, you may use `// @ts-expect-error` on the undocumented
parameter. This library doesn&#039;t validate at runtime that the request matches the type, so any extra values you
send will be sent as-is.

```ts
client.chat.completions.create({
  // ...
  // @ts-expect-error baz is not yet public
  baz: &#039;undocumented option&#039;,
});
```

For requests with the `GET` verb, any extra params will be in the query, all other requests will send the
extra param in the body.

If you want to explicitly send an extra argument, you can do so with the `query`, `body`, and `headers` request
options.

#### Undocumented response properties

To access undocumented response properties, you may access the response object with `// @ts-expect-error` on
the response object, or cast the response object to the requisite type. Like the request params, we do not
validate or strip extra properties from the response from the API.

### Customizing the fetch client

If you want to use a different `fetch` function, you can either polyfill the global:

```ts
import fetch from &#039;my-fetch&#039;;

globalThis.fetch = fetch;
```

Or pass it to the client:

```ts
import OpenAI from &#039;openai&#039;;
import fetch from &#039;my-fetch&#039;;

const client = new OpenAI({ fetch });
```

### Fetch options

If you want to set custom `fetch` options without overriding the `fetch` function, y

... [README content truncated due to size. Visit the repository for the complete README] ...</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[web-infra-dev/midscene]]></title>
            <link>https://github.com/web-infra-dev/midscene</link>
            <guid>https://github.com/web-infra-dev/midscene</guid>
            <pubDate>Sat, 09 Aug 2025 00:04:43 GMT</pubDate>
            <description><![CDATA[Your AI Operator for Web, Android, Automation & Testing.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/web-infra-dev/midscene">web-infra-dev/midscene</a></h1>
            <p>Your AI Operator for Web, Android, Automation & Testing.</p>
            <p>Language: TypeScript</p>
            <p>Stars: 9,843</p>
            <p>Forks: 689</p>
            <p>Stars today: 10 stars today</p>
            <h2>README</h2><pre>&lt;p align=&quot;center&quot;&gt;
  &lt;img alt=&quot;Midscene.js&quot;  width=&quot;260&quot; src=&quot;https://github.com/user-attachments/assets/f60de3c1-dd6f-4213-97a1-85bf7c6e79e4&quot;&gt;
&lt;/p&gt;

&lt;h1 align=&quot;center&quot;&gt;Midscene.js&lt;/h1&gt;
&lt;div align=&quot;center&quot;&gt;

English | [简体中文](./README.zh.md)

&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;
  Open-source AI Operator for Web, Android, Automation &amp; Testing
&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;a href=&quot;https://www.npmjs.com/package/@midscene/web&quot;&gt;&lt;img src=&quot;https://img.shields.io/npm/v/@midscene/web?style=flat-square&amp;color=00a8f0&quot; alt=&quot;npm version&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://huggingface.co/ByteDance-Seed/UI-TARS-1.5-7B&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97-UI%20TARS%20Models-yellow&quot; alt=&quot;hugging face model&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://npm-compare.com/@midscene/web/#timeRange=THREE_YEARS&quot;&gt;&lt;img src=&quot;https://img.shields.io/npm/dm/@midscene/web.svg?style=flat-square&amp;color=00a8f0&quot; alt=&quot;downloads&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://github.com/web-infra-dev/midscene/blob/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-MIT-blue.svg?style=flat-square&amp;color=00a8f0&quot; alt=&quot;License&quot; /&gt;
  &lt;a href=&quot;https://discord.gg/2JyBHxszE4&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/1328277792730779648?style=flat-square&amp;color=7289DA&amp;label=Discord&amp;logo=discord&amp;logoColor=white&quot; alt=&quot;discord&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://x.com/midscene_ai&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/midscene_ai?style=flat-square&quot; alt=&quot;twitter&quot; /&gt;&lt;/a&gt;
  &lt;a href=&quot;https://deepwiki.com/web-infra-dev/midscene&quot;&gt;
    &lt;img alt=&quot;Ask DeepWiki.com&quot; src=&quot;https://devin.ai/assets/deepwiki-badge.png&quot; style=&quot;height: 18px; vertical-align: middle;&quot; /&gt;
  &lt;/a&gt;
&lt;/p&gt;

## Showcases

| Instruction  | Video |
| :---:  | :---: |
| Use JS code to drive task orchestration, collect information about Jay Chou&#039;s concert, and write it into Google Docs (By UI-TARS model)   | &lt;video src=&quot;https://github.com/user-attachments/assets/75474138-f51f-4c54-b3cf-46d61d059999&quot; height=&quot;300&quot; /&gt;        |
| Control Maps App on Android (By Qwen-2.5-VL model)   | &lt;video src=&quot;https://github.com/user-attachments/assets/1f5bab0e-4c28-44e1-b378-a38809b05a00&quot; height=&quot;300&quot; /&gt;        |
| Using midscene mcp to browse the page (https://www.saucedemo.com/), perform login, add products, place orders, and finally generate test cases based on mcp execution steps and playwright example | &lt;video src=&quot;https://github.com/user-attachments/assets/a95ca353-e50c-4091-85ba-e542f576b6be&quot; height=&quot;300&quot; /&gt;   |

## 💡 Features

### Write Automation with Natural Language
- Describe your goals and steps, and Midscene will plan and operate the user interface for you.
- Use Javascript SDK or YAML to write your automation script.

### Web &amp; Mobile App
- **Web Automation 🖥️**: Either integrate with [Puppeteer](https://midscenejs.com/integrate-with-puppeteer.html), [Playwright](https://midscenejs.com/integrate-with-playwright.html) or use [Bridge Mode](https://midscenejs.com/bridge-mode-by-chrome-extension.html) to control your desktop browser.
- **Android Automation 📱**: Use [Javascript SDK](https://midscenejs.com/integrate-with-android.html) with adb to control your local Android device.

### Tools
- **Visual Reports for Debugging 🎞️**: Through our test reports and Playground, you can easily understand, replay and debug the entire process.
- [**Caching for Efficiency 🔄**](https://midscenejs.com/caching.html): Replay your script with cache and get the result faster.
- [**MCP 🔗**](https://midscenejs.com/mcp.html): Allows other MCP Clients to directly use Midscene&#039;s capabilities.

### Three kinds of APIs
- [Interaction API 🔗](https://midscenejs.com/api.html#interaction-methods): interact with the user interface.
- [Data Extraction API 🔗](https://midscenejs.com/api.html#data-extraction): extract data from the user interface and dom.
- [Utility API 🔗](https://midscenejs.com/api.html#more-apis): utility functions like `aiAssert()`, `aiLocate()`, `aiWaitFor()`.

## 👉 Zero-code Quick Experience

- **[Chrome Extension](https://midscenejs.com/quick-experience.html)**: Start in-browser experience immediately through [the Chrome Extension](https://midscenejs.com/quick-experience.html), without writing any code.
- **[Android Playground](https://midscenejs.com/quick-experience-with-android.html)**: There is also a built-in Android playground to control your local Android device.

## ✨ Model Choices

Midscene.js supports both multimodal LLMs like `gpt-4o`, and visual-language models like `Qwen2.5-VL`, `Doubao-1.5-thinking-vision-pro`, `gemini-2.5-pro` and `UI-TARS`. 

Visual-language models are recommended for UI automation.

Read more about [Choose a model](https://midscenejs.com/choose-a-model)

## 💡 Two Styles of Automation

### Auto Planning

Midscene will automatically plan the steps and execute them. It may be slower and heavily rely on the quality of the AI model.

```javascript
await aiAction(&#039;click all the records one by one. If one record contains the text &quot;completed&quot;, skip it&#039;);
```

### Workflow Style

Split complex logic into multiple steps to improve the stability of the automation code.

```javascript
const recordList = await agent.aiQuery(&#039;string[], the record list&#039;)
for (const record of recordList) {
  const hasCompleted = await agent.aiBoolean(`check if the record contains the text &quot;completed&quot;`)
  if (!hasCompleted) {
    await agent.aiTap(record)
  }
}
```

&gt; For more details about the workflow style, please refer to [Blog - Use JavaScript to Optimize the AI Automation Code](https://midscenejs.com/blog-programming-practice-using-structured-api.html)


## 👀 Comparing to other projects

There are so many UI automation tools out there, and each one seems to be all-powerful. What&#039;s special about Midscene.js?

* **Debugging Experience**: You will soon realize that debugging and maintaining automation scripts is the real challenge. No matter how magical the demo looks, ensuring stability over time requires careful debugging. Midscene.js offers a visualized report file, a built-in playground, and a Chrome Extension to simplify the debugging process. These are the tools most developers truly need, and we&#039;re continually working to improve the debugging experience.

* **Open Source, Free, Deploy as you want**: Midscene.js is an open-source project. It&#039;s decoupled from any cloud service and model provider, you can choose either public or private deployment. There is always a suitable plan for your business.

* **Integrate with Javascript**: You can always bet on Javascript 😎

## 📄 Resources 

* Home Page and Documentation: [https://midscenejs.com](https://midscenejs.com/)
* Sample Projects: [https://github.com/web-infra-dev/midscene-example](https://github.com/web-infra-dev/midscene-example)
* API Reference: [https://midscenejs.com/api.html](https://midscenejs.com/api.html)
* GitHub: [https://github.com/web-infra-dev/midscene](https://github.com/web-infra-dev/midscene)

## 🤝 Community

* [Discord](https://discord.gg/2JyBHxszE4)
* [Follow us on X](https://x.com/midscene_ai)
* [Lark Group(飞书交流群)](https://applink.larkoffice.com/client/chat/chatter/add_by_link?link_token=291q2b25-e913-411a-8c51-191e59aab14d)


## 📝 Credits

We would like to thank the following projects:

- [Rsbuild](https://github.com/web-infra-dev/rsbuild) for the build tool.
- [UI-TARS](https://github.com/bytedance/ui-tars) for the open-source agent model UI-TARS.
- [Qwen2.5-VL](https://github.com/QwenLM/Qwen2.5-VL) for the open-source VL model Qwen2.5-VL.
- [scrcpy](https://github.com/Genymobile/scrcpy) and [yume-chan](https://github.com/yume-chan) allow us to control Android devices with browser.
- [appium-adb](https://github.com/appium/appium-adb) for the javascript bridge of adb.
- [YADB](https://github.com/ysbing/YADB) for the yadb tool which improves the performance of text input.
- [Puppeteer](https://github.com/puppeteer/puppeteer) for browser automation and control.
- [Playwright](https://github.com/microsoft/playwright) for browser automation and control and testing.

## 📝 Citation

If you use Midscene.js in your research or project, please cite:

```bibtex
@software{Midscene.js,
  author = {Xiao Zhou, Tao Yu, YiBing Lin},
  title = {Midscene.js: Your AI Operator for Web, Android, Automation &amp; Testing.},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/web-infra-dev/midscene}
}
```


## 📝 License

Midscene.js is [MIT licensed](https://github.com/web-infra-dev/midscene/blob/main/LICENSE).

---

&lt;div align=&quot;center&quot;&gt;
  If this project helps you or inspires you, please give us a ⭐️
&lt;/div&gt;
</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[shadcn-ui/ui]]></title>
            <link>https://github.com/shadcn-ui/ui</link>
            <guid>https://github.com/shadcn-ui/ui</guid>
            <pubDate>Sat, 09 Aug 2025 00:04:42 GMT</pubDate>
            <description><![CDATA[A set of beautifully-designed, accessible components and a code distribution platform. Works with your favorite frameworks. Open Source. Open Code.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/shadcn-ui/ui">shadcn-ui/ui</a></h1>
            <p>A set of beautifully-designed, accessible components and a code distribution platform. Works with your favorite frameworks. Open Source. Open Code.</p>
            <p>Language: TypeScript</p>
            <p>Stars: 92,565</p>
            <p>Forks: 6,461</p>
            <p>Stars today: 75 stars today</p>
            <h2>README</h2><pre># shadcn/ui

Accessible and customizable components that you can copy and paste into your apps. Free. Open Source. **Use this to build your own component library**.

![hero](apps/www/public/og.jpg)

## Documentation

Visit http://ui.shadcn.com/docs to view the documentation.

## Contributing

Please read the [contributing guide](/CONTRIBUTING.md).

## License

Licensed under the [MIT license](https://github.com/shadcn/ui/blob/main/LICENSE.md).
</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[musistudio/claude-code-router]]></title>
            <link>https://github.com/musistudio/claude-code-router</link>
            <guid>https://github.com/musistudio/claude-code-router</guid>
            <pubDate>Sat, 09 Aug 2025 00:04:41 GMT</pubDate>
            <description><![CDATA[Use Claude Code as the foundation for coding infrastructure, allowing you to decide how to interact with the model while enjoying updates from Anthropic.]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/musistudio/claude-code-router">musistudio/claude-code-router</a></h1>
            <p>Use Claude Code as the foundation for coding infrastructure, allowing you to decide how to interact with the model while enjoying updates from Anthropic.</p>
            <p>Language: TypeScript</p>
            <p>Stars: 11,522</p>
            <p>Forks: 851</p>
            <p>Stars today: 358 stars today</p>
            <h2>README</h2><pre># Claude Code Router

[中文版](README_zh.md)

&gt; A powerful tool to route Claude Code requests to different models and customize any request.

![](blog/images/claude-code.png)

## ✨ Features

- **Model Routing**: Route requests to different models based on your needs (e.g., background tasks, thinking, long context).
- **Multi-Provider Support**: Supports various model providers like OpenRouter, DeepSeek, Ollama, Gemini, Volcengine, and SiliconFlow.
- **Request/Response Transformation**: Customize requests and responses for different providers using transformers.
- **Dynamic Model Switching**: Switch models on-the-fly within Claude Code using the `/model` command.
- **GitHub Actions Integration**: Trigger Claude Code tasks in your GitHub workflows.
- **Plugin System**: Extend functionality with custom transformers.

## 🚀 Getting Started

### 1. Installation

First, ensure you have [Claude Code](https://docs.anthropic.com/en/docs/claude-code/quickstart) installed:

```shell
npm install -g @anthropic-ai/claude-code
```

Then, install Claude Code Router:

```shell
npm install -g @musistudio/claude-code-router
```

### 2. Configuration

Create and configure your `~/.claude-code-router/config.json` file. For more details, you can refer to `config.example.json`.

The `config.json` file has several key sections:

- **`PROXY_URL`** (optional): You can set a proxy for API requests, for example: `&quot;PROXY_URL&quot;: &quot;http://127.0.0.1:7890&quot;`.
- **`LOG`** (optional): You can enable logging by setting it to `true`. The log file will be located at `$HOME/.claude-code-router.log`.
- **`APIKEY`** (optional): You can set a secret key to authenticate requests. When set, clients must provide this key in the `Authorization` header (e.g., `Bearer your-secret-key`) or the `x-api-key` header. Example: `&quot;APIKEY&quot;: &quot;your-secret-key&quot;`.
- **`HOST`** (optional): You can set the host address for the server. If `APIKEY` is not set, the host will be forced to `127.0.0.1` for security reasons to prevent unauthorized access. Example: `&quot;HOST&quot;: &quot;0.0.0.0&quot;`.
- **`NON_INTERACTIVE_MODE`** (optional): When set to `true`, enables compatibility with non-interactive environments like GitHub Actions, Docker containers, or other CI/CD systems. This sets appropriate environment variables (`CI=true`, `FORCE_COLOR=0`, etc.) and configures stdin handling to prevent the process from hanging in automated environments. Example: `&quot;NON_INTERACTIVE_MODE&quot;: true`.

- **`Providers`**: Used to configure different model providers.
- **`Router`**: Used to set up routing rules. `default` specifies the default model, which will be used for all requests if no other route is configured.
- **`API_TIMEOUT_MS`**: Specifies the timeout for API calls in milliseconds.

Here is a comprehensive example:

```json
{
  &quot;APIKEY&quot;: &quot;your-secret-key&quot;,
  &quot;PROXY_URL&quot;: &quot;http://127.0.0.1:7890&quot;,
  &quot;LOG&quot;: true,
  &quot;API_TIMEOUT_MS&quot;: 600000,
  &quot;NON_INTERACTIVE_MODE&quot;: false,
  &quot;Providers&quot;: [
    {
      &quot;name&quot;: &quot;openrouter&quot;,
      &quot;api_base_url&quot;: &quot;https://openrouter.ai/api/v1/chat/completions&quot;,
      &quot;api_key&quot;: &quot;sk-xxx&quot;,
      &quot;models&quot;: [
        &quot;google/gemini-2.5-pro-preview&quot;,
        &quot;anthropic/claude-sonnet-4&quot;,
        &quot;anthropic/claude-3.5-sonnet&quot;,
        &quot;anthropic/claude-3.7-sonnet:thinking&quot;
      ],
      &quot;transformer&quot;: {
        &quot;use&quot;: [&quot;openrouter&quot;]
      }
    },
    {
      &quot;name&quot;: &quot;deepseek&quot;,
      &quot;api_base_url&quot;: &quot;https://api.deepseek.com/chat/completions&quot;,
      &quot;api_key&quot;: &quot;sk-xxx&quot;,
      &quot;models&quot;: [&quot;deepseek-chat&quot;, &quot;deepseek-reasoner&quot;],
      &quot;transformer&quot;: {
        &quot;use&quot;: [&quot;deepseek&quot;],
        &quot;deepseek-chat&quot;: {
          &quot;use&quot;: [&quot;tooluse&quot;]
        }
      }
    },
    {
      &quot;name&quot;: &quot;ollama&quot;,
      &quot;api_base_url&quot;: &quot;http://localhost:11434/v1/chat/completions&quot;,
      &quot;api_key&quot;: &quot;ollama&quot;,
      &quot;models&quot;: [&quot;qwen2.5-coder:latest&quot;]
    },
    {
      &quot;name&quot;: &quot;gemini&quot;,
      &quot;api_base_url&quot;: &quot;https://generativelanguage.googleapis.com/v1beta/models/&quot;,
      &quot;api_key&quot;: &quot;sk-xxx&quot;,
      &quot;models&quot;: [&quot;gemini-2.5-flash&quot;, &quot;gemini-2.5-pro&quot;],
      &quot;transformer&quot;: {
        &quot;use&quot;: [&quot;gemini&quot;]
      }
    },
    {
      &quot;name&quot;: &quot;volcengine&quot;,
      &quot;api_base_url&quot;: &quot;https://ark.cn-beijing.volces.com/api/v3/chat/completions&quot;,
      &quot;api_key&quot;: &quot;sk-xxx&quot;,
      &quot;models&quot;: [&quot;deepseek-v3-250324&quot;, &quot;deepseek-r1-250528&quot;],
      &quot;transformer&quot;: {
        &quot;use&quot;: [&quot;deepseek&quot;]
      }
    },
    {
      &quot;name&quot;: &quot;modelscope&quot;,
      &quot;api_base_url&quot;: &quot;https://api-inference.modelscope.cn/v1/chat/completions&quot;,
      &quot;api_key&quot;: &quot;&quot;,
      &quot;models&quot;: [&quot;Qwen/Qwen3-Coder-480B-A35B-Instruct&quot;, &quot;Qwen/Qwen3-235B-A22B-Thinking-2507&quot;],
      &quot;transformer&quot;: {
        &quot;use&quot;: [
          [
            &quot;maxtoken&quot;,
            {
              &quot;max_tokens&quot;: 65536
            }
          ],
          &quot;enhancetool&quot;
        ],
        &quot;Qwen/Qwen3-235B-A22B-Thinking-2507&quot;: {
          &quot;use&quot;: [&quot;reasoning&quot;]
        }
      }
    },
    {
      &quot;name&quot;: &quot;dashscope&quot;,
      &quot;api_base_url&quot;: &quot;https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions&quot;,
      &quot;api_key&quot;: &quot;&quot;,
      &quot;models&quot;: [&quot;qwen3-coder-plus&quot;],
      &quot;transformer&quot;: {
        &quot;use&quot;: [
          [
            &quot;maxtoken&quot;,
            {
              &quot;max_tokens&quot;: 65536
            }
          ],
          &quot;enhancetool&quot;
        ]
      }
    },
    {
      &quot;name&quot;: &quot;aihubmix&quot;,
      &quot;api_base_url&quot;: &quot;https://aihubmix.com/v1/chat/completions&quot;,
      &quot;api_key&quot;: &quot;sk-&quot;,
      &quot;models&quot;: [
        &quot;Z/glm-4.5&quot;,
        &quot;claude-opus-4-20250514&quot;,
        &quot;gemini-2.5-pro&quot;
      ]
    }
  ],
  &quot;Router&quot;: {
    &quot;default&quot;: &quot;deepseek,deepseek-chat&quot;,
    &quot;background&quot;: &quot;ollama,qwen2.5-coder:latest&quot;,
    &quot;think&quot;: &quot;deepseek,deepseek-reasoner&quot;,
    &quot;longContext&quot;: &quot;openrouter,google/gemini-2.5-pro-preview&quot;,
    &quot;longContextThreshold&quot;: 60000,
    &quot;webSearch&quot;: &quot;gemini,gemini-2.5-flash&quot;
  }
}
```

### 3. Running Claude Code with the Router

Start Claude Code using the router:

```shell
ccr code
```

&gt; **Note**: After modifying the configuration file, you need to restart the service for the changes to take effect:
&gt;
&gt; ```shell
&gt; ccr restart
&gt; ```

### 4. UI Mode (Beta)

For a more intuitive experience, you can use the UI mode to manage your configuration:

```shell
ccr ui
```

This will open a web-based interface where you can easily view and edit your `config.json` file.

![UI](/blog/images/ui.png)

&gt; **Note**: The UI mode is currently in beta. 100% vibe coding: including project initialization, I just created a folder and a project.md document, and all code was generated by ccr + qwen3-coder + gemini(webSearch). 
If you encounter any issues, please submit an issue on GitHub.

#### Providers

The `Providers` array is where you define the different model providers you want to use. Each provider object requires:

- `name`: A unique name for the provider.
- `api_base_url`: The full API endpoint for chat completions.
- `api_key`: Your API key for the provider.
- `models`: A list of model names available from this provider.
- `transformer` (optional): Specifies transformers to process requests and responses.

#### Transformers

Transformers allow you to modify the request and response payloads to ensure compatibility with different provider APIs.

- **Global Transformer**: Apply a transformer to all models from a provider. In this example, the `openrouter` transformer is applied to all models under the `openrouter` provider.
  ```json
  {
    &quot;name&quot;: &quot;openrouter&quot;,
    &quot;api_base_url&quot;: &quot;https://openrouter.ai/api/v1/chat/completions&quot;,
    &quot;api_key&quot;: &quot;sk-xxx&quot;,
    &quot;models&quot;: [
      &quot;google/gemini-2.5-pro-preview&quot;,
      &quot;anthropic/claude-sonnet-4&quot;,
      &quot;anthropic/claude-3.5-sonnet&quot;
    ],
    &quot;transformer&quot;: { &quot;use&quot;: [&quot;openrouter&quot;] }
  }
  ```
- **Model-Specific Transformer**: Apply a transformer to a specific model. In this example, the `deepseek` transformer is applied to all models, and an additional `tooluse` transformer is applied only to the `deepseek-chat` model.

  ```json
  {
    &quot;name&quot;: &quot;deepseek&quot;,
    &quot;api_base_url&quot;: &quot;https://api.deepseek.com/chat/completions&quot;,
    &quot;api_key&quot;: &quot;sk-xxx&quot;,
    &quot;models&quot;: [&quot;deepseek-chat&quot;, &quot;deepseek-reasoner&quot;],
    &quot;transformer&quot;: {
      &quot;use&quot;: [&quot;deepseek&quot;],
      &quot;deepseek-chat&quot;: { &quot;use&quot;: [&quot;tooluse&quot;] }
    }
  }
  ```

- **Passing Options to a Transformer**: Some transformers, like `maxtoken`, accept options. To pass options, use a nested array where the first element is the transformer name and the second is an options object.
  ```json
  {
    &quot;name&quot;: &quot;siliconflow&quot;,
    &quot;api_base_url&quot;: &quot;https://api.siliconflow.cn/v1/chat/completions&quot;,
    &quot;api_key&quot;: &quot;sk-xxx&quot;,
    &quot;models&quot;: [&quot;moonshotai/Kimi-K2-Instruct&quot;],
    &quot;transformer&quot;: {
      &quot;use&quot;: [
        [
          &quot;maxtoken&quot;,
          {
            &quot;max_tokens&quot;: 16384
          }
        ]
      ]
    }
  }
  ```

**Available Built-in Transformers:**

- `Anthropic`:If you use only the `Anthropic` transformer, it will preserve the original request and response parameters(you can use it to connect directly to an Anthropic endpoint).
- `deepseek`: Adapts requests/responses for DeepSeek API.
- `gemini`: Adapts requests/responses for Gemini API.
- `openrouter`: Adapts requests/responses for OpenRouter API. It can also accept a `provider` routing parameter to specify which underlying providers OpenRouter should use. For more details, refer to the [OpenRouter documentation](https://openrouter.ai/docs/features/provider-routing). See an example below:
  ```json
    &quot;transformer&quot;: {
      &quot;use&quot;: [&quot;openrouter&quot;],
      &quot;moonshotai/kimi-k2&quot;: {
        &quot;use&quot;: [
          [
            &quot;openrouter&quot;,
            {
              &quot;provider&quot;: {
                &quot;only&quot;: [&quot;moonshotai/fp8&quot;]
              }
            }
          ]
        ]
      }
    }
  ```
- `groq`: Adapts requests/responses for groq API.
- `maxtoken`: Sets a specific `max_tokens` value.
- `tooluse`: Optimizes tool usage for certain models via `tool_choice`.
- `gemini-cli` (experimental): Unofficial support for Gemini via Gemini CLI [gemini-cli.js](https://gist.github.com/musistudio/1c13a65f35916a7ab690649d3df8d1cd).
- `reasoning`: Used to process the `reasoning_content` field.
- `sampling`: Used to process sampling information fields such as `temperature`, `top_p`, `top_k`, and `repetition_penalty`.
- `enhancetool`: Adds a layer of error tolerance to the tool call parameters returned by the LLM (this will cause the tool call information to no longer be streamed).
- `cleancache`: Clears the `cache_control` field from requests.
- `vertex-gemini`: Handles the Gemini API using Vertex authentication.

**Custom Transformers:**

You can also create your own transformers and load them via the `transformers` field in `config.json`.

```json
{
  &quot;transformers&quot;: [
    {
      &quot;path&quot;: &quot;$HOME/.claude-code-router/plugins/gemini-cli.js&quot;,
      &quot;options&quot;: {
        &quot;project&quot;: &quot;xxx&quot;
      }
    }
  ]
}
```

#### Router

The `Router` object defines which model to use for different scenarios:

- `default`: The default model for general tasks.
- `background`: A model for background tasks. This can be a smaller, local model to save costs.
- `think`: A model for reasoning-heavy tasks, like Plan Mode.
- `longContext`: A model for handling long contexts (e.g., &gt; 60K tokens).
- `longContextThreshold` (optional): The token count threshold for triggering the long context model. Defaults to 60000 if not specified.
- `webSearch`: Used for handling web search tasks and this requires the model itself to support the feature. If you&#039;re using openrouter, you need to add the `:online` suffix after the model name.

You can also switch models dynamically in Claude Code with the `/model` command:
`/model provider_name,model_name`
Example: `/model openrouter,anthropic/claude-3.5-sonnet`

#### Custom Router

For more advanced routing logic, you can specify a custom router script via the `CUSTOM_ROUTER_PATH` in your `config.json`. This allows you to implement complex routing rules beyond the default scenarios.

In your `config.json`:

```json
{
  &quot;CUSTOM_ROUTER_PATH&quot;: &quot;$HOME/.claude-code-router/custom-router.js&quot;
}
```

The custom router file must be a JavaScript module that exports an `async` function. This function receives the request object and the config object as arguments and should return the provider and model name as a string (e.g., `&quot;provider_name,model_name&quot;`), or `null` to fall back to the default router.

Here is an example of a `custom-router.js` based on `custom-router.example.js`:

```javascript
// $HOME/.claude-code-router/custom-router.js

/**
 * A custom router function to determine which model to use based on the request.
 *
 * @param {object} req - The request object from Claude Code, containing the request body.
 * @param {object} config - The application&#039;s config object.
 * @returns {Promise&lt;string|null&gt;} - A promise that resolves to the &quot;provider,model_name&quot; string, or null to use the default router.
 */
module.exports = async function router(req, config) {
  const userMessage = req.body.messages.find((m) =&gt; m.role === &quot;user&quot;)?.content;

  if (userMessage &amp;&amp; userMessage.includes(&quot;explain this code&quot;)) {
    // Use a powerful model for code explanation
    return &quot;openrouter,anthropic/claude-3.5-sonnet&quot;;
  }

  // Fallback to the default router configuration
  return null;
};
```

##### Subagent Routing

For routing within subagents, you must specify a particular provider and model by including `&lt;CCR-SUBAGENT-MODEL&gt;provider,model&lt;/CCR-SUBAGENT-MODEL&gt;` at the **beginning** of the subagent&#039;s prompt. This allows you to direct specific subagent tasks to designated models.

**Example:**

```
&lt;CCR-SUBAGENT-MODEL&gt;openrouter,anthropic/claude-3.5-sonnet&lt;/CCR-SUBAGENT-MODEL&gt;
Please help me analyze this code snippet for potential optimizations...
```

## 🤖 GitHub Actions

Integrate Claude Code Router into your CI/CD pipeline. After setting up [Claude Code Actions](https://docs.anthropic.com/en/docs/claude-code/github-actions), modify your `.github/workflows/claude.yaml` to use the router:

```yaml
name: Claude Code

on:
  issue_comment:
    types: [created]
  # ... other triggers

jobs:
  claude:
    if: |
      (github.event_name == &#039;issue_comment&#039; &amp;&amp; contains(github.event.comment.body, &#039;@claude&#039;)) ||
      # ... other conditions
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: read
      issues: read
      id-token: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Prepare Environment
        run: |
          curl -fsSL https://bun.sh/install | bash
          mkdir -p $HOME/.claude-code-router
          cat &lt;&lt; &#039;EOF&#039; &gt; $HOME/.claude-code-router/config.json
          {
            &quot;log&quot;: true,
            &quot;NON_INTERACTIVE_MODE&quot;: true,
            &quot;OPENAI_API_KEY&quot;: &quot;${{ secrets.OPENAI_API_KEY }}&quot;,
            &quot;OPENAI_BASE_URL&quot;: &quot;https://api.deepseek.com&quot;,
            &quot;OPENAI_MODEL&quot;: &quot;deepseek-chat&quot;
          }
          EOF
        shell: bash

      - name: Start Claude Code Router
        run: |
          nohup ~/.bun/bin/bunx @musistudio/claude-code-router@1.0.8 start &amp;
        shell: bash

      - name: Run Claude Code
        id: claude
        uses: anthropics/claude-code-action@beta
        env:
          ANTHROPIC_BASE_URL: http://localhost:3456
        with:
          anthropic_api_key: &quot;any-string-is-ok&quot;
```

&gt; **Note**: When running in GitHub Actions or other automation environments, make sure to set `&quot;NON_INTERACTIVE_MODE&quot;: true` in your configuration to prevent the process from hanging due to stdin handling issues.

This setup allows for interesting automations, like running tasks during off-peak hours to reduce API costs.

## 📝 Further Reading

- [Project Motivation and How It Works](blog/en/project-motivation-and-how-it-works.md)
- [Maybe We Can Do More with the Router](blog/en/maybe-we-can-do-more-with-the-route.md)

## ❤️ Support &amp; Sponsoring

If you find this project helpful, please consider sponsoring its development. Your support is greatly appreciated!

[![ko-fi](https://ko-fi.com/img/githubbutton_sm.svg)](https://ko-fi.com/F1F31GN2GM)

[Paypal](https://paypal.me/musistudio1999)

&lt;table&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;img src=&quot;/blog/images/alipay.jpg&quot; width=&quot;200&quot; alt=&quot;Alipay&quot; /&gt;&lt;/td&gt;
    &lt;td&gt;&lt;img src=&quot;/blog/images/wechat.jpg&quot; width=&quot;200&quot; alt=&quot;WeChat Pay&quot; /&gt;&lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

### Our Sponsors

A huge thank you to all our sponsors for their generous support!


- [AIHubmix](https://aihubmix.com/)
- @Simon Leischnig
- [@duanshuaimin](https://github.com/duanshuaimin)
- [@vrgitadmin](https://github.com/vrgitadmin)
- @\*o
- [@ceilwoo](https://github.com/ceilwoo)
- @\*说
- @\*更
- @K\*g
- @R\*R
- [@bobleer](https://github.com/bobleer)
- @\*苗
- @\*划
- [@Clarence-pan](https://github.com/Clarence-pan)
- [@carter003](https://github.com/carter003)
- @S\*r
- @\*晖
- @\*敏
- @Z\*z
- @\*然
- [@cluic](https://github.com/cluic)
- @\*苗
- [@PromptExpert](https://github.com/PromptExpert)
- @\*应
- [@yusnake](https://github.com/yusnake)
- @\*飞
- @董\*
- @\*汀
- @\*涯
- @\*:-）
- @\*\*磊
- @\*琢
- @\*成
- @Z\*o
- @\*琨
- [@congzhangzh](https://github.com/congzhangzh)
- @\*\_
- @Z\*m
- @*鑫
- @c\*y
- @\*昕
- [@witsice](https://github.com/witsice)
- @b\*g
- @\*亿
- @\*辉
- @JACK 

(If your name is masked, please contact me via my homepage email to update it with your GitHub username.)
</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
        <item>
            <title><![CDATA[microsoft/vscode]]></title>
            <link>https://github.com/microsoft/vscode</link>
            <guid>https://github.com/microsoft/vscode</guid>
            <pubDate>Sat, 09 Aug 2025 00:04:40 GMT</pubDate>
            <description><![CDATA[Visual Studio Code]]></description>
            <content:encoded><![CDATA[
            <h1><a href="https://github.com/microsoft/vscode">microsoft/vscode</a></h1>
            <p>Visual Studio Code</p>
            <p>Language: TypeScript</p>
            <p>Stars: 175,515</p>
            <p>Forks: 34,237</p>
            <p>Stars today: 70 stars today</p>
            <h2>README</h2><pre># Visual Studio Code - Open Source (&quot;Code - OSS&quot;)

[![Feature Requests](https://img.shields.io/github/issues/microsoft/vscode/feature-request.svg)](https://github.com/microsoft/vscode/issues?q=is%3Aopen+is%3Aissue+label%3Afeature-request+sort%3Areactions-%2B1-desc)
[![Bugs](https://img.shields.io/github/issues/microsoft/vscode/bug.svg)](https://github.com/microsoft/vscode/issues?utf8=✓&amp;q=is%3Aissue+is%3Aopen+label%3Abug)
[![Gitter](https://img.shields.io/badge/chat-on%20gitter-yellow.svg)](https://gitter.im/Microsoft/vscode)

## The Repository

This repository (&quot;`Code - OSS`&quot;) is where we (Microsoft) develop the [Visual Studio Code](https://code.visualstudio.com) product together with the community. Not only do we work on code and issues here, we also publish our [roadmap](https://github.com/microsoft/vscode/wiki/Roadmap), [monthly iteration plans](https://github.com/microsoft/vscode/wiki/Iteration-Plans), and our [endgame plans](https://github.com/microsoft/vscode/wiki/Running-the-Endgame). This source code is available to everyone under the standard [MIT license](https://github.com/microsoft/vscode/blob/main/LICENSE.txt).

## Visual Studio Code

&lt;p align=&quot;center&quot;&gt;
  &lt;img alt=&quot;VS Code in action&quot; src=&quot;https://user-images.githubusercontent.com/35271042/118224532-3842c400-b438-11eb-923d-a5f66fa6785a.png&quot;&gt;
&lt;/p&gt;

[Visual Studio Code](https://code.visualstudio.com) is a distribution of the `Code - OSS` repository with Microsoft-specific customizations released under a traditional [Microsoft product license](https://code.visualstudio.com/License/).

[Visual Studio Code](https://code.visualstudio.com) combines the simplicity of a code editor with what developers need for their core edit-build-debug cycle. It provides comprehensive code editing, navigation, and understanding support along with lightweight debugging, a rich extensibility model, and lightweight integration with existing tools.

Visual Studio Code is updated monthly with new features and bug fixes. You can download it for Windows, macOS, and Linux on [Visual Studio Code&#039;s website](https://code.visualstudio.com/Download). To get the latest releases every day, install the [Insiders build](https://code.visualstudio.com/insiders).

## Contributing

There are many ways in which you can participate in this project, for example:

* [Submit bugs and feature requests](https://github.com/microsoft/vscode/issues), and help us verify as they are checked in
* Review [source code changes](https://github.com/microsoft/vscode/pulls)
* Review the [documentation](https://github.com/microsoft/vscode-docs) and make pull requests for anything from typos to additional and new content

If you are interested in fixing issues and contributing directly to the code base,
please see the document [How to Contribute](https://github.com/microsoft/vscode/wiki/How-to-Contribute), which covers the following:

* [How to build and run from source](https://github.com/microsoft/vscode/wiki/How-to-Contribute)
* [The development workflow, including debugging and running tests](https://github.com/microsoft/vscode/wiki/How-to-Contribute#debugging)
* [Coding guidelines](https://github.com/microsoft/vscode/wiki/Coding-Guidelines)
* [Submitting pull requests](https://github.com/microsoft/vscode/wiki/How-to-Contribute#pull-requests)
* [Finding an issue to work on](https://github.com/microsoft/vscode/wiki/How-to-Contribute#where-to-contribute)
* [Contributing to translations](https://aka.ms/vscodeloc)

## Feedback

* Ask a question on [Stack Overflow](https://stackoverflow.com/questions/tagged/vscode)
* [Request a new feature](CONTRIBUTING.md)
* Upvote [popular feature requests](https://github.com/microsoft/vscode/issues?q=is%3Aopen+is%3Aissue+label%3Afeature-request+sort%3Areactions-%2B1-desc)
* [File an issue](https://github.com/microsoft/vscode/issues)
* Connect with the extension author community on [GitHub Discussions](https://github.com/microsoft/vscode-discussions/discussions) or [Slack](https://aka.ms/vscode-dev-community)
* Follow [@code](https://twitter.com/code) and let us know what you think!

See our [wiki](https://github.com/microsoft/vscode/wiki/Feedback-Channels) for a description of each of these channels and information on some other available community-driven channels.

## Related Projects

Many of the core components and extensions to VS Code live in their own repositories on GitHub. For example, the [node debug adapter](https://github.com/microsoft/vscode-node-debug) and the [mono debug adapter](https://github.com/microsoft/vscode-mono-debug) repositories are separate from each other. For a complete list, please visit the [Related Projects](https://github.com/microsoft/vscode/wiki/Related-Projects) page on our [wiki](https://github.com/microsoft/vscode/wiki).

## Bundled Extensions

VS Code includes a set of built-in extensions located in the [extensions](extensions) folder, including grammars and snippets for many languages. Extensions that provide rich language support (code completion, Go to Definition) for a language have the suffix `language-features`. For example, the `json` extension provides coloring for `JSON` and the `json-language-features` extension provides rich language support for `JSON`.

## Development Container

This repository includes a Visual Studio Code Dev Containers / GitHub Codespaces development container.

* For [Dev Containers](https://aka.ms/vscode-remote/download/containers), use the **Dev Containers: Clone Repository in Container Volume...** command which creates a Docker volume for better disk I/O on macOS and Windows.
  * If you already have VS Code and Docker installed, you can also click [here](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/vscode) to get started. This will cause VS Code to automatically install the Dev Containers extension if needed, clone the source code into a container volume, and spin up a dev container for use.

* For Codespaces, install the [GitHub Codespaces](https://marketplace.visualstudio.com/items?itemName=GitHub.codespaces) extension in VS Code, and use the **Codespaces: Create New Codespace** command.

Docker / the Codespace should have at least **4 Cores and 6 GB of RAM (8 GB recommended)** to run full build. See the [development container README](.devcontainer/README.md) for more information.

## Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## License

Copyright (c) Microsoft Corporation. All rights reserved.

Licensed under the [MIT](LICENSE.txt) license.
</pre>
          ]]></content:encoded>
            <category>TypeScript</category>
        </item>
    </channel>
</rss>